{
  "id": "http://arxiv.org/abs/1910.01055v6",
  "title": "QuaRL: Quantization for Fast and Environmentally Sustainable Reinforcement Learning",
  "authors": [
    "Srivatsan Krishnan",
    "Maximilian Lam",
    "Sharad Chitlangia",
    "Zishen Wan",
    "Gabriel Barth-Maron",
    "Aleksandra Faust",
    "Vijay Janapa Reddi"
  ],
  "abstract": "Deep reinforcement learning continues to show tremendous potential in\nachieving task-level autonomy, however, its computational and energy demands\nremain prohibitively high. In this paper, we tackle this problem by applying\nquantization to reinforcement learning. To that end, we introduce a novel\nReinforcement Learning (RL) training paradigm, \\textit{ActorQ}, to speed up\nactor-learner distributed RL training. \\textit{ActorQ} leverages 8-bit\nquantized actors to speed up data collection without affecting learning\nconvergence. Our quantized distributed RL training system, \\textit{ActorQ},\ndemonstrates end-to-end speedups \\blue{between 1.5 $\\times$ and 5.41$\\times$},\nand faster convergence over full precision training on a range of tasks\n(Deepmind Control Suite) and different RL algorithms (D4PG, DQN). Furthermore,\nwe compare the carbon emissions (Kgs of CO2) of \\textit{ActorQ} versus standard\nreinforcement learning \\blue{algorithms} on various tasks. Across various\nsettings, we show that \\textit{ActorQ} enables more environmentally friendly\nreinforcement learning by achieving \\blue{carbon emission improvements between\n1.9$\\times$ and 3.76$\\times$} compared to training RL-agents in full-precision.\nWe believe that this is the first of many future works on enabling\ncomputationally energy-efficient and sustainable reinforcement learning. The\nsource code is available here for the public to use:\n\\url{https://github.com/harvard-edge/QuaRL}.",
  "text": "Published in Transactions on Machine Learning Research (07/2022)\nQuaRL: Quantization for Fast and Environmentally\nSustainable Reinforcement Learning\nSrivatsan Krishnan∗\nsrivatsan@seas.harvard.edu\nHarvard University\nMaximilian Lam∗\nmaxlam@g.harvard.edu\nHarvard University\nSharad Chitlangia∗†\nchitshar@amazon.com\nAmazon Advertising\nZishen Wan‡\nzishenwan@gatech.edu\nGeorgia Institute of Technology\nGaberial Barth-Maron\ngabrielbm@google.com\nDeepMind\nAleksandra Faust\nsandrafaust@google.com\nGoogle Research, Brain\nVijay Janapa Reddi\nvj@eecs.harvard.edu\nHarvard University\nReviewed on OpenReview: https: // openreview. net/ forum? id= xwWsiFmUEs\nAbstract\nDeep reinforcement learning continues to show tremendous potential in achieving task-\nlevel autonomy, however, its computational and energy demands remain prohibitively\nhigh. In this paper, we tackle this problem by applying quantization to reinforcement\nlearning. To that end, we introduce a novel Reinforcement Learning (RL) training\nparadigm, ActorQ, to speed up actor-learner distributed RL training.\nActorQ\nleverages 8-bit quantized actors to speed up data collection without aﬀecting learning\nconvergence. Our quantized distributed RL training system, ActorQ, demonstrates\nend-to-end speedups between 1.5 × and 5.41×, and faster convergence over full\nprecision training on a range of tasks (Deepmind Control Suite) and diﬀerent RL\nalgorithms (D4PG, DQN). Furthermore, we compare the carbon emissions (Kgs of\nCO2) of ActorQ versus standard reinforcement learning algorithms on various tasks.\nAcross various settings, we show that ActorQ enables more environmentally friendly\nreinforcement learning by achieving carbon emission improvements between 1.9×\nand 3.76× compared to training RL-agents in full-precision. We believe that this\nis the ﬁrst of many future works on enabling computationally energy-eﬃcient and\n∗Equal Contribution\n†Work done when Sharad was a visiting undergraduate student at Harvard.\n‡Work done when Zishen was a graduate student at Harvard.\n1\narXiv:1910.01055v6  [cs.LG]  14 Nov 2022\nPublished in Transactions on Machine Learning Research (07/2022)\nsustainable reinforcement learning. The source code is available here for the public\nto use: https://github.com/harvard-edge/QuaRL.\n1\nIntroduction\nDeep reinforcement learning has attained signiﬁcant achievements in various ﬁelds (Bellemare et al.,\n2013; Kempka et al., 2016; Kalashnikov et al., 2018; Silver et al., 2016; 2017; OpenAI, 2018; Chiang\net al., 2019; OpenAI et al., 2019). Despite its promise, one of its limiting factors is long training times,\nand the current approach to speed up RL training on complex and diﬃcult tasks involves distributed\ntraining (Espeholt et al., 2019; Nair et al., 2015; Babaeizadeh et al., 2016). Although distributed\nRL training has demonstrated signiﬁcant potential in reducing training times (Hoﬀman et al., 2020;\nEspeholt et al., 2018), this approach also leads to increased energy consumption and greater carbon\nemissions. Recently, work by (Wu et al., 2021) indicates that improving hardware utilization using\nquantization and other performance optimizations can reduce the carbon footprint of training and\ninference of large recommendation models by 20% every six months. In this same vein, we believe\nreinforcement learning can also beneﬁt from these optimization techniques, particularly quantization,\nto reduce training time, improve hardware utilization and and reduce carbon emissions.\nIn this paper, we tackle the following research question – How can we speed up RL training without\nsigniﬁcantly increasing its carbon emissions? To systematically tackle this problem, we introduce\nActorQ. In ActorQ, we use quantization to reduce the computation and communication costs\nof various components of a distributed RL training system. Based on the characterization of\ncore components of distributed RL training, we ﬁnd that majority of the time is spent on actor\npolicy inference, followed by the learner’s gradient calculation, model update, and ﬁnally, the\ncommunication) cost between actors and learners (Figure 1). Thus, to obtain signiﬁcant speedups\nand reduce carbon emissions, we ﬁrst need to lower the overhead of performing actor inference. To\nachieve this in ActorQ, we employ quantized policy (neural network) inference, a simple yet eﬀective\noptimization technique to lower neural network inference’s compute and memory costs.\nApplying quantization to RL is non-trivial and diﬀerent from traditional neural network quantization.\nDue to the inherent feedback loop in RL between the agent and the environment, the errors made\nat one state might propagate to subsequent states, suggesting that actor’s policies might be more\nchallenging to quantize than traditional neural network applications. Despite signiﬁcant research\non quantization for neural networks in supervised learning, there is little prior work studying the\neﬀects of quantizing the actor’s policy in reinforcement learning. To bridge this gap in literature,\nwe benchmark the eﬀects of quantized policy during rollouts in popular RL algorithms namely\nDQN (Mnih et al., 2013), PPO (Schulman et al., 2017), DDPG (Lillicrap et al., 2015), A2C (Mnih\net al., 2016). Our benchmarking study shows that RL policies are resilient to quantization error,\nand the error does not propagate due to the feedback loop between the actor and the environment.\nAlthough our benchmarking study shows the potential beneﬁts of applying quantization (i.e., no\nerror propagation), we still need to apply quantization during RL training to realize real-world\nspeedups. Applying quantization during RL training may seem diﬃcult due to the myriad of\ndiﬀerent learning algorithms (Lillicrap et al., 2015; Mnih et al., 2016; Barth-Maron et al., 2018) and\nthe complexity of these optimization procedures. Luckily, most popular RL training algorithms can\nbe formulated using the actor-learner training paradigm such as Ape-X (Horgan et al., 2018) and\nACME (Hoﬀman et al., 2020). ActorQ leverages the fact that many RL training procedures can be\nformulated in a general actor-learning paradigm in order to apply quantization speciﬁcally during\nactor’s policy inference and policy broadcast between learners to actors during the RL training.\nTo demonstrate the beneﬁts of our approach, we choose two RL algorithms, namely DQN (Mnih\n2\nPublished in Transactions on Machine Learning Research (07/2022)\nCarbon (Kgs)\n(a) Carbon breakdown\nint8\nfp32\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nRuntime (s)\nActors\nLearner\nBroadcast\n(b) Time breakdown\nFigure 1: (a) Breakdown of carbon emissions between using a quantized and non-quantized policy\nin RL training for MountainCar. (b) Wall clock time for ‘Actors’, ‘Learner’, and ‘Broadcast’ for\nover a window of 1000 steps in ACME (Hoﬀman et al., 2020)\net al., 2013) and D4PG (Barth-Maron et al., 2018) from Deepmind’s ACME framework (Hoﬀman\net al., 2020) and show speedups in training time and reduced carbon emissions while maintaining\nconvergence.\nIn summary, our fundamental contributions are as follows:\n• We introduce ActorQ, to speed up distributed reinforcement learning training. ActorQ\noperates by quantizing the actor’s policy, thereby speeding up experience collection. ActorQ\nachieves between 1.5 × and 5.41× speedup on a variety of tasks from the Deepmind\ncontrol suite (Tassa et al., 2018) and OpenAI gym (Brockman et al., 2016) compared to its\nfull-precision counterparts.\n• Using our ActorQ framework, we further explore opportunities to identify various bottlenecks\nin distributed RL training. We show that quantization can also minimize communication\noverheads between actors and learners and reduce reinforcement learning time.\n• Finally, by quantizing the policy weights and communication between actors and learners,\nwe show a reduction in carbon emissions between 1.9 × and 3.76× versus full precision\npolicies, thus paving the way towards sustainable reinforcement learning.\n• To address lack of benchmarks in applying quantization for RL in the literature, we extensively\nbenchmark quantized policies on standard tasks (Atari, Gym), algorithms (A2C, DQN,\nD4PG, PPO), and policy architecture (MLPs, CNNs). We demonstrate little to no loss in\nmean return, especially in the context of using quantized policy for rollouts.\n2\nRelated Work\nBoth quantization and reinforcement learning in isolation have been the subject of much research\nin recent years. However, to the best of our knowledge, they have seldom been applied together\n(as surveyed in a broad range of quantization and RL papers) to improve RL eﬃciency. Below we\nprovide an overview of related works in both quantization and reinforcement learning and discuss\ntheir contributions and signiﬁcance in relation to our paper.\n2.1\nQuantization\nQuantizing a neural network reduces the precision of neural network weights, reducing memory\ntransfer times and enabling the use of fast low-precision compute operations. Innovations in both\npost-training quantization (Krishnamoorthi, 2018; Banner et al., 2018; Zhao et al., 2019; Tambe\n3\nPublished in Transactions on Machine Learning Research (07/2022)\nMetrics\nA3C\nCULE\nRay\nGorila\nSeed-RL\nACME\nActor-Q\n(Our Work)\nMethod\nDistributed\nDistributed\nDistributed\nDistributed\nDistributed\nDistributed\nDistributed/Standalone\nIncrease in Speed-up\nDecrease in\nCarbon Emission\nDecrease in Energy\nDecrease in\nCommunication Cost\nFramework Agnostic\nTable 1: Comparison of prior works on speeding-up RL training wrt to speed-up (lower training\ntimes), energy, and carbon emissions. Previous works compared include Nvidia’s CULE (Dalton\net al., 2019), Ray (Moritz et al., 2018), Gorila (Nair et al., 2015), Seed-RL (Espeholt et al., 2019)\nand ACME (Hoﬀman et al., 2020)\net al., 2020) and quantization aware training (Dong et al., 2019; Hubara et al., 2018; Choi et al.,\n2018) demonstrate that neural networks may be quantized to very low precision without accuracy\nloss, suggesting that quantization has immense potential for producing eﬃcient deployable models.\nIn the context of speeding up training, research has also shown that quantization can yield signiﬁcant\nperformance boosts. For example, prior work on half or mixed precision training (Sun et al., 2019;\nDas et al., 2018) demonstrates that using half-precision operators may signiﬁcantly reduce compute\nand memory requirements while achieving adequate convergence.\nAlthough much research has been conducted on quantization and machine learning, the primary\ntargets of quantization are applications in the image classiﬁcation and natural language processing\ndomains. Quantization as applied to reinforcement learning has been absent in the literature.\n2.2\nReinforcement Learning & Distributed Reinforcement Learning Training\nSigniﬁcant work on reinforcement learning range from training algorithms (Mnih et al., 2013;\nLevine et al., 2015) to environments (Brockman et al., 2016; Bellemare et al., 2013; Tassa et al.,\n2018) to systems improvements (Petrenko et al., 2020; Hoﬀman et al., 2020). From a system\noptimization perspective, reinforcement learning poses a unique opportunity compared to traditional\nsupervised machine learning algorithms as training a policy is markedly diﬀerent from standard\nneural network training involving pure backpropagation (as employed in learning image classiﬁcation\nor language models). Notably, reinforcement learning training involves repeatedly executing policies\non environments (experience generation), communicating across various components (in this case\nbetween devices performing rollouts and the main device updating the policy), and ﬁnally, learning\na policy based on the training samples obtained from experience generation. Experience generation\nis trivially parallelizable by employing multiple devices that perform rollouts simultaneously, and\nvarious recent research in distributed and parallel reinforcement learning training (Kapturowski et al.,\n2018; Moritz et al., 2018; Nair et al., 2015) leverages this to accelerate training. One signiﬁcant work\nis the Deepmind ACME reinforcement learning framework (Hoﬀman et al., 2020), which enables\nscalable training to many processors or nodes on a single machine.\n3\nActorQ: Quantization for Reinforcement Learning\nIn this section, we introduce ActorQ a quantization method for improving the run time eﬃciency\nof actor-learner training. We ﬁrst provide a high-level overview of the ActorQ system. Then, we\n4\nPublished in Transactions on Machine Learning Research (07/2022)\nActor\nEnvironment\nPolicy\nLearner\nWeights\nOptimize\n<State, Action, Rewards>\n<Policy>\nActor\nEnvironment\nPolicy\nActor\nEnvironment\nPolicy\nActor\nEnvironment\nPolicy\n(a) Tradition RL training setup.\nActor \nQ(Policy)\nEnvironment\nLearner \nWeights\nOptimize\nQuantizer \nload_state_dict\nWeights\nQuantize\nPolicy\nQuantized \nPolicy\nState \nAction \nReturns \n<Quantized Policy>\n<State, Action, Rewards>\n<Policy>\nLegend\nComputation\nBroadcast\n(b) ActorQ RL training setup.\nFigure 2: (a) Traditional RL training setup. In a non-distributed RL training scenario, the number\nof actors is 1, and all the components are run on the same machine. In distributed RL training\nsetup (e.g., ACME), the actors are distributed across multiple CPUs. (b) ActorQ system setup.\nIn ActorQ, we add a quantizer block to the RL-training loop. The learner performs full-precision\npolicy training in GPU. Before the policy is broadcasted to the actors, the quantizer block quantizes\nthe policy. The quantized policy reduces the communication of the updated policy between the\nGPU learner and CPU actors. The actors run rollouts on the quantized policy for the experience\ngeneration. The learner and actors are instrumented with carbon monitoring APIs (Henderson\net al., 2020) to quantify the impact of carbon emission with and without quantization.\ncharacterize the eﬀects of quantization on diﬀerent reinforcement learning algorithms. Lastly, we\napply quantization to a distributed RL training framework to show speed-ups on a real system.\nOur results demonstrate that apart from reducing training time, ActorQ also leads to lower carbon\nemissions, thus paving the way towards sustainable reinforcement learning research.\n3.1\nActorQ System Architecture\nTraditional RL training can be formulated within context of an actor- learner training paradigm (Hor-\ngan et al., 2018) as shown in Fig. 2a. In this framework, one or multiple actors collect new training\nexamples based on its current policy and relays them to the learner (or replay buﬀer), which utilizes\nthe generated experience to update the weights of its own policy. Periodically, the learner broadcasts\nits policy to the actors which update their internal policy with the learner’s, synchronizing the\nmodels. In the non-distributed formulation, the actor, learner, and replay buﬀer reside in a single\nmachine. However, in the case of the distributed RL, the actor, learner, and the replay buﬀer run on\ndiﬀerent compute nodes, constantly communicating with each other. We refer to this communication\nbetween the actor/learner components as broadcast.\nActorQ introduces quantization in the actor-learner reinforcement learning framework to speed\nup training. There are three main components in ActorQ system namely, ‘Actors’, ‘Learner’, and\n‘Quantizer’ as shown in Figure 2. During training, each actor instance performs rollouts and initially\nuses a randomly initialized policy for decision making. At each step, the actors broadcast the\nenvironment state, action, and the reward for a given state to the learner. The learner uses this\ninformation to optimize the policy. Periodically, the learner broadcasts the updated policy to the\nactors, who then use the updated policy to perform future rollouts.\nThere are two main performance bottlenecks in reinforcement learning training. First, each actor\nuses a neural network policy to generate an action. Thus, how fast it can perform rollouts depends\non the policy’s inference latency. Second, the learner broadcasts the policy periodically to all the\n5\nPublished in Transactions on Machine Learning Research (07/2022)\nactors. Broadcasting of the entire policy network to all actors can cause communication overheads\nand slow down training.\nIn ActorQ, we use quantization to reduce these bottlenecks and achieve end-to-end speed-up. To\nsystematically integrate quantization into the training algorithm, we ﬁrst perform a study to\ncharacterize the eﬀects of applying post-training quantization and quantization aware training to\nvarious RL algorithms and environments.\nIn ActorQ, all actors use a quantized policy to perform rollouts. Additionally, the broadcasted\npolicy is also quantized. Note that ActorQ maintains all learner computation in full precision\nto maintain learning convergence; further note that the learner is signiﬁcantly faster than the\nactors due to the utilization of hardware acceleration (e.g., GPU) to perform batched updates\nduring policy optimization (Espeholt et al., 2019). Meanwhile, actors are stuck performing rollouts\nthat require repeated executions of single-example (non-batched) inference, and the diﬀerence in\nhardware utilization between the actors’ rollouts and the learner’s policy optimizations, based on\nour characterization, is one of the key areas leading to ineﬃciency. This motivates us to ﬁrst apply\nquantization to the actors’ inference and then to the policy broadcast (e.g: communication) between\nlearners to actors.\nWhile simple, ActorQ distinguishes from traditional quantized neural network training as the\ninference-only role of actors enables the use of low precision (≤8 bit) operators to speed up training.\nThis is unlike traditional quantized neural network training, which must utilize more complex\nalgorithms like loss scaling Das et al. (2018), specialized numerical representations Sun et al. (2019);\nWang et al. (2018), stochastic rounding Wang et al. (2018) to attain convergence. This adds extra\ncomplexity and may also limit speedup and, in many cases, this is still limited to half-precision\noperations due to convergence issues.\n3.2\nEﬀects of Quantization on Reinforcement Learning\nAs a ﬁrst step towards developing ActorQ, we ﬁrst perform experiments to benchmark the eﬀects of\nquantization on RL. Insights gained from these experiments will help verify that quantization can\nbe applied to learning without signiﬁcantly degrading quality. To this end, we apply post-training\nquantization (PTQ) to just the policies of various reinforcement learning agents: we take an RL\npolicy fully trained in fp32 (ﬂoating point precision) and apply post-training quantization to it,\nevaluating the impact of quantization on the resulting model. Performing this experiment allows\nus to understand whether the drift caused by quantization error at each policy step aﬀects RL\nreward quality, and to what extent it impacts rewards with diﬀerent degrees (i.e., number of bits) of\nquantization.\nPost-Training Quantization (PTQ)\nThe post-training quantization is performed using standard uniform aﬃne quantization (Krish-\nnamoorthi, 2018) deﬁned as follows:\nQn(W) = round(W\nδ )\nwhere\nδ = |min(W, 0)| + |max(W, 0)|\n2n\nDequantization is deﬁned as\nD(Wq, δ) = δ(Wq)\n6\nPublished in Transactions on Machine Learning Research (07/2022)\nAlgorithm →\nA2C\nDQN\nPPO\nDDPG\nDatatype →\nfp32\nfp16\nint8\nKL-div\nfp32\nfp16\nint8\nKL-div\nfp32\nfp16\nint8\nKL-div\nfp32\nfp16\nint8\nKL-div\nEnvironment ↓\nRwd\nRwd\nRwd\nRwd\nRwd\nRwd\nRwd\nRwd\nRwd\nRwd\nRwd\nRwd\nBreakout\n379\n371\n350\n0.00262\n214\n217\n78\n0.06045\n400\n400\n368\n0.08892\nSpaceInvaders\n717\n667\n634\n0.06066\n586\n625\n509\n0.01353\n698\n662\n684\n0.08115\nBeamRiders\n3087\n3060\n2793\n0.00993\n925\n823\n721\n0.09787\n1655\n1820\n1697\n0.0186\nMsPacman\n1915\n1915\n2045\n0.17536\n1433\n1429\n2024\n0.01531\n1735\n1735\n1845\n0.11978\nQbert\n5002\n5002\n5611\n0.01957\n641\n641\n616\n0.01995\n15010\n15010\n14425\n0.02573\nSeaquest\n782\n756\n753\n0.03358\n1709\n1885\n1582\n0.02536\n1782\n1784\n1795\n0.02991\nCartpole\n500\n500\n500\n0.00113\n500\n500\n500\n0.1019\n500\n500\n500\n0.00566\nPong\n20\n20\n19\n0.01528\n21\n21\n21\n0.1257\n20\n20\n20\n0.01\nWalker2D\n399\n422\n442\n0.05371\n2274\n2273\n2268\n0.03\n1890\n1929\n1866\n0.0376\nHalfCheetah\n2199\n2215\n2208\n0.13427\n3026\n3062\n3080\n0.02\n2553\n2551\n2473\n0.0763\nBipedalWalker\n230\n240\n226\n0.0252\n304\n280\n291\n0.03\n98\n90\n83\n0.0134\nMountainCar\n94\n94\n94\n0.03705\n92\n92\n92\n0.07\n92\n92\n92\n0.0651\nTable 2: Post-training quantization error for DQN, DDPG, PPO, and A2C algorithm on Atari\nand Gym. Quantization down to int8 yields similar episodic rewards to full precision baseline. To\nmeasure the policy distribution shift, we use KL-divergence. We measure KL-divergence between\nthe fp32 policy and the int8 policy. The action distribution for the fp32 policy and int8 is shown in\nthe appendix.\nIn our study, policies are trained in standard full precision. Once trained, we quantize them to\nfp16 and int8 ﬁrst and dequantization them to simulate quantization error. For convolutional\nneural networks, we use per-channel quantization, which applies Qn to each channel of convolutions\nindividually. Also, all layers of the policy are quantized to the same precision level.\nWe apply the PTQ to Atari arcade learning (Bellemare et al., 2013), OpenAI gym environ-\nments (Brockman et al., 2016) and diﬀerent RL algorithms namely A2C (Mnih et al., 2016),\nDQN (Mnih et al., 2013), PPO (Schulman et al., 2017), and DDPG (Lillicrap et al., 2015). We\ntrain a three-layer convolutional neural network for all Atari arcade learning. For openAI Gym\nenvironments, we train neural networks with two hidden layers of size 64. In PTQ, unless otherwise\nnoted, both weights and activations are quantized to the same precision.\nTable 2 shows the rewards attained by policies quantized via post-training quantization in. The\nmean of int8 and fp16 relative errors ranges between 2% and 5% of the full precision model, which\nindicates that policies can be represented in 8/16 bits precision without much quality loss.\nIn a few cases (e.g., MsPacman for PPO), post-training quantization yields better scores than the\nfull precision policy. We believe that quantization injected an amount of noise that was small enough\nto maintain a good policy and large enough to regularize model behavior; this supports some of the\nresults seen by Louizos et al. (2018); Bishop (1995); Hirose et al. (2018).\nTo quantify the shift in policy distribution, we use KL-divergence (Kullback & Leibler, 1951).\nTable 2 shows the KL-divergence for the fp32 policy and int8 (quantized) policy. Across all the\nevaluated algorithms (both on-policy and oﬀ-policy), we observe that the KL-divergence is very small,\nsuggesting that the quantization of policy does not change the inherent distribution signiﬁcantly.\nThe eﬀect of small KL-divergence is also reﬂected in the minimal degradation in mean return.\nWe also visualize the action distribution of both the fp32 policy and int8. Fig. 3 shows the action\ndistribution for two on-policy algorithms, namely A2C and PPO, for the Walker2D environment.\n7\nPublished in Transactions on Machine Learning Research (07/2022)\nfp32 policy\nint8 policy\nFrequency\n(a) A2C.\nfp32 policy\nint8 policy\nFrequency\n(b) PPO.\nFigure 3: Small variation in action distribution for A2C and PPO in WalkerStand environment\nfor fp32 and int8 policies for the same observation. We run each policy for 5000 steps. The small\nchange in action distribution for quantized policy suggests safe exploration without signiﬁcantly\ndegrading the rewards.\nWe observe a small variation in the action distribution, suggesting that quantizing the policy allows\nthe agent to perform a small safe exploration compared to the fp32 policy. This small variation is\nconsistent with other environments and RL algorithms. Appendix C shows the action distributions\nfor other environments.\nBased on this study, we observe that the quantization of RL policy does not cause a signiﬁcant loss\nin reward compared to an fp32 policy. Appendix A shows the results of applying quantization aware\ntraining (QAT) with RL. Since QAT uses fake quantization nodes to estimate the statistics of the\nparameter distributions, it allows for more aggressive quantizations. Our study on QAT suggests\n(See Appendix A) that we can safely quantize the policy up to 5-bits without signiﬁcantly aﬀecting\nthe agent’s rewards. However, to fully utilize the gains from aggressive quantization, we need native\nhardware support to run computations lower than 8-bits. Commonly used hardware accelerators\nused in RL training support native int8 computations. To that end, in ActorQ, we use PTQ for the\nquantizer block. Also, it is important to note that ActorQ uses simple uniform aﬃne quantization\nto demonstrate how to apply quantization in RL policies; however, we can easily swap the quantizer\nfunction to include other quantization techniques (Krishnamoorthi, 2018).\nIn summary, at int8, we see minimal degradation in rewards. Hence, when designing quantizer block\nin ActorQ, we quantize the policy to int8. By using int8 quantization and leveraging the native\nint8 computation support in hardware, we achieve end-to-end speed-up in reinforcement learning\ntraining.\n4\nExperimental Setup\nWe evaluate the ActorQ system for speeding up distributed quantized reinforcement learning across\nvarious tasks in Deepmind Control Suite (Tassa et al., 2018). Overall, we show that: (1) we see\nsigniﬁcant speedup (between 1.5 × and 5.41 ×) in training reinforcement learning policies using\nActorQ; (2) convergence is maintained even when actors perform int8 quantized inference; (3) Using\nActorQ, we lower the carbon emissions between 1.9× and 3.76× compared to training without\nquantization.\n4.1\nActorQ Experimental Setup\nWe evaluate ActorQ on a range of environments from the Deepmind Control Suite (Tassa et al.,\n2018). We choose the environments to cover a wide range of diﬃculties to determine the eﬀects of\n8\nPublished in Transactions on Machine Learning Research (07/2022)\nTask\nAlgorithm\nSteps Trained\nModel Pull Frequency (Steps)\nCartpole Balance\nD4PG\n40000\n1000\nWalker Stand\nD4PG\n40000\n1000\nHopper Stand\nD4PG\n100000\n1000\nReacher Hard\nD4PG\n70000\n1000\nCheetah Run\nD4PG\n200000\n1000\nFinger Spin\nD4PG\n200000\n1000\nHumanoid Stand\nD4PG\n500000\n100\nHumanoid Walk\nD4PG\n700000\n100\nCartpole\nDQN\n60000\n1000\nAcrobot\nDQN\n100000\n1000\nMountainCar\nDQN\n200000\n100\nTable 3: Tasks evaluated using ActorQ range from easy to diﬃcult, along with the steps trained for\ncorresponding tasks, with how frequently the model is pulled on the actor side.\nquantization on both easy and diﬃcult tasks. The diﬃculty of the Deepmind Control Suite tasks\nis determined by (Hoﬀman et al., 2020). Table 3 lists the environments we tested on with their\ncorresponding diﬃculty and number of steps trained. Each episode has a maximum length of 1000\nsteps, so the maximum reward for each task is 1000 (though this may not always be attainable).\nPolicy architectures are fully connected networks with three hidden layers of size 2048. We apply\na Gaussian noise layer to the output of the policy network on the actor to encourage exploration;\nsigma is uniformly assigned between 0 and 0.2 according to the actor being executed. On the learner\nside, the critic network is a three-layer hidden network with a hidden size of 512. We train policies\nusing D4PG (Barth-Maron et al., 2018) on continuous control environments and DQN (Mnih et al.,\n2013) on discrete control environments. We chose D4PG as it was the best learning algorithm in\n(Tassa et al., 2018; Hoﬀman et al., 2020), and DQN is a widely used and standard reinforcement\nlearning algorithm. An example submitted by an actor is sampled 16 times before being removed\nfrom the replay buﬀer (spi=16) (lower spi is typically better as it minimizes model staleness (Fedus\net al., 2020)).\nAll the experiments are run in a distributed fashion to leverage multiple CPU cores and a GPU.\nA V100 GPU is used on the learner, while the actors are mapped to the CPU (1 core for each\nactor). We run each experiment and average over at least three runs to compute the running mean\n(window=10) of the aggregated runs.\n4.2\nMeasuring Carbon Emissions\nFor measuring the carbon emission for the run, we use the experiment-impact-tracker proposed\nin prior JMLR work (Henderson et al., 2020).1 We instrument the ActorQ system with carbon\nmonitor APIs to measure the energy and carbon emissions for each training experiment in ActorQ.\nTo measure the improvement (i.e., lowering of carbon emissions) when using the int8 policy, we take\nthe ratio of carbon emission when using the fp32 policy and int8 policy as:\n1https://github.com/Breakend/experiment-impact-tracker\n9\nPublished in Transactions on Machine Learning Research (07/2022)\nTime to Convergence (s)\nCarbon Emissions (Kgs)\nCarbon Emission\nImprovement\nTask\nMean\nReturn\nfp32\nint8\nSpeedup\nfp32\n(Co2fp32)\nint8\n(Co2int8)\nCo2fp32/Co2int8\nCartpole Balance\n941.22\n870.91\n279\n3.12×\n0.359\n0.15\n2.39\nWalker Stand\n947.74\n871.32\n534.37\n1.63×\n0.67\n0.178\n3.76\nHopper Stand\n836.41\n2660.41\n1699.17\n1.57×\n0.34\n0.17\n2.00\nReacher Hard\n948.12\n1597\n875.34\n1.82×\n0.35\n0.18\n1.94\nCheetah Run\n732.31\n2517.3\n891.84\n2.82×\n0.263\n0.12\n2.19\nFinger Spin\n810.32\n3256.56\n1065.52\n3.06×\n0.361\n0.19\n1.90\nHumanoid Stand\n884.89\n13964.92\n9302.82\n1.51×\n0.55\n0.27\n2.04\nHumanoid Walk\n649.91\n17990.66\n6223.35\n2.89×\n0.56\n0.278\n2.01\nCartpole (Gym)\n198.22\n963.67\n260.1\n3.70×\n0.188\n0.089\n2.11\nMountain Car (Gym)\n-120.62\n2861.8\n1284.32\n2.22×\n0.21\n0.098\n2.14\nAcrobot (Gym\n-107.45\n912.24\n168.44\n5.41×\n0.198\n0.097\n2.04\nTable 4: ActorQ time, speedups, carbon emission improvements to 95% reward on select tasks\nfrom DeepMind Control Suite and Gym. 8 bit inference yields between 1.5 × and 5.41 × speedup\nover full precision training. Likewise, int8 policy inference yields between 1.9× and 3.76× less\ncarbon emission compared to fp32 policy. We use D4PG on DeepMind Control Suite environments\n(non-gym), DQN on gym environments.\nCo2Improvment = CO2fp32\nCO2int8\n,\nwhere, Co2fp32 and Co2int8 are the carbon emissions when using fp32 and int8 policy respectively.\n5\nResults\n5.1\nSpeedup, Convergence, and Carbon Emissions\nWe focus our results on three main areas: end-to-end speed-ups for training, model convergence\nduring training and environmental sustainability from a carbon emissions perspective.\nEnd-to-End Speedups. We show end-to-end training speedups with ActorQ in Figure(s) 4 and\n6. Across nearly all tasks, we see signiﬁcant speedups with both 8-bit inference. Additionally, to\nimprove readability, we estimate the 95% percentile of the maximum attained score by fp32 and\nmeasure time to this reward level for fp32, int8, and compute corresponding speedups. This is\nshown in Table 4. Note that Table 4 does not take into account cases where fp16 or int8 achieve a\nhigher score than fp32.\nConvergence. We show the episode reward versus total actor steps convergence plots using ActorQ\nin Figure(s) 5 and 6. Data shows that broadly, convergence is maintained even with 8-bit actors\nacross both easy and diﬃcult tasks. On Cheetah, Run and Reacher, Hard, 8-bit ActorQ achieve\neven slightly faster convergence, and we believe this may have happened as quantization introduces\nnoise which could be seen as exploration.\nCarbon Emissions. Table 4 also shows the carbon emissions for various task in openAI gym\nand Deepmind Control Suite. We compare the carbon emissions of a policy running in fp32 and\n10\nPublished in Transactions on Machine Learning Research (07/2022)\n0.2\n0.4\n0.6\n0.8\n1.0\nTime (s)\n1e6\n500\n600\n700\n800\n900\n1000\nReturn\nq=32\nq=8\n(a) Cartpole Balance\n0\n1\n2\n3\n4\nTime (s)\n1e6\n200\n400\n600\n800\nReturn\nq=32\nq=8\n(b) Cheetah Run\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nTime (s)\n1e6\n200\n400\n600\n800\n1000\nReturn\nq=32\nq=8\n(c) Walker Stand\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTime (s)\n1e6\n0\n200\n400\n600\n800\nReturn\nq=32\nq=8\n(d) Hopper Stand\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nTime (s)\n1e6\n0\n200\n400\n600\n800\n1000\nReturn\nq=32\nq=8\n(e) Reacher Hard\n0\n1\n2\n3\n4\n5\nTime (s)\n1e6\n100\n200\n300\n400\n500\n600\n700\n800\n900\nReturn\nq=32\nq=8\n(f) Finger Spin\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime (s)\n1e7\n0\n200\n400\n600\n800\nReturn\nq=32\nq=8\n(g) Humanoid Stand\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nTime (s)\n1e7\n0\n100\n200\n300\n400\n500\n600\n700\nReturn\nq=32\nq=8\n(h) Humanoid Walk\nFigure 4: End-to-end speedups of ActorQ across various Deepmind Control Suite tasks using 8 bit\nand 32 bit inference. int8 training yields signiﬁcant end-to-end training speedups over the fp32\nbaseline. The x-axis denotes the wall-clock time and y-axis denotes the reward. Training uses the\nD4PG algorithm.\n0\n2\n4\n6\n8\nSteps\n1e6\n400\n500\n600\n700\n800\n900\n1000\nReturn\nq=32\nq=8\n(a) Cartpole Balance\n0\n1\n2\n3\n4\nSteps\n1e7\n200\n400\n600\n800\nReturn\nq=32\nq=8\n(b) Cheetah Run\n0\n2\n4\n6\n8\nSteps\n1e6\n200\n400\n600\n800\n1000\nReturn\nq=32\nq=8\n(c) Walker Stand\n0.0\n0.5\n1.0\n1.5\n2.0\nSteps\n1e7\n0\n200\n400\n600\n800\nReturn\nq=32\nq=8\n(d) Hopper Stand\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nSteps\n1e7\n0\n200\n400\n600\n800\n1000\nReturn\nq=32\nq=8\n(e) Reacher Hard\n0\n1\n2\n3\n4\nSteps\n1e7\n100\n200\n300\n400\n500\n600\n700\n800\n900\nReturn\nq=32\nq=8\n(f) Finger Spin\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps\n1e8\n0\n200\n400\n600\n800\nReturn\nq=32\nq=8\n(g) Humanoid Stand\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nSteps\n1e8\n0\n100\n200\n300\n400\n500\n600\n700\nReturn\nq=32\nq=8\n(h) Humanoid Walk\nFigure 5: Convergence of ActorQ across various Deepmind Control Suite tasks using int8 and fp32\ninference. int8 quantized training attains the same or better convergence than full precision training.\nTraining uses the D4PG algorithm.\nint8. We observe that quantization of policies reduces the carbon emissions anywhere from 1.9× to\n3.76× depending upon the task. As RL systems are scaled to run on 1000’s distributed CPU cores\nand accelerators (GPU/TPU), the absolute carbon reduction (measured in Kgs of CO2) can be\nsigniﬁcant.\n11\nPublished in Transactions on Machine Learning Research (07/2022)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nSteps\n1e6\n80\n100\n120\n140\n160\n180\n200\nReturn\nq=32\nq=8\n(a) Cartpole(Conv)\n0\n1\n2\n3\n4\n5\n6\n7\nSteps\n1e6\n220\n200\n180\n160\n140\n120\n100\nReturn\nq=32\nq=8\n(b) MntnCar(Conv)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nSteps\n1e6\n500\n400\n300\n200\n100\nReturn\nq=32\nq=8\n(c) Acrobot(Conv)\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime (s)\n1e6\n80\n100\n120\n140\n160\n180\n200\nReturn\nq=32\nq=8\n(d) Cartpole(Spd)\n0\n1\n2\n3\n4\n5\n6\n7\nTime (s)\n1e6\n200\n180\n160\n140\n120\n100\nReturn\nq=32\nq=8\n(e) MntnCar(Spd)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nTime (s)\n1e6\n400\n350\n300\n250\n200\n150\n100\nReturn\nq=32\nq=8\n(f) Acrobot(Spd)\nFigure 6: Convergence (Conv.) and end-to-end speedups (Spd) of ActorQ across various Gym tasks\nusing int8 and fp32 inference. int8 training yields signiﬁcant end-to-end training speedups over the\nfull precision baseline. For the speed-up plots (d-f), the x-axis denotes wall-clock time. Training\nuses the DQN algorithm.\n5.2\nCommunication vs Computation\nThe frequency of model pulls on actors is a hyperparameter and may have impacts on convergence\nas it aﬀects the staleness of policies being used to populate the replay buﬀer; this has been witnessed\nin both prior research (Fedus et al., 2020) and our experiment with the hyperparameter. Figure 7a\nshows that a higher update frequency of 100 can help in faster convergence compared to an update\nfrequency of 1000 for the Humanoid stand task. This hyperparameter has system-level implications\nsince a higher update frequency can increase the communication cost (policy broadcast from learner\nto actors). In contrast, a lower update frequency can increase the computation cost since it will\ntake more steps to converge, increasing the computation cost. Thus, to understand the tradeoﬀof\nquantization concerning this hyperparameter, we explore the eﬀects of quantization of communication\nversus computation in both communication and computation-heavy setups.\nTo quantize communication, we quantize policy weights to int8 and compress them by packing\nthem into a matrix, thus, reducing the memory of model broadcasts by 4×. Naturally, quantizing\ncommunication would be more beneﬁcial in the communication heavy scenario, and quantizing\ncompute would yield relatively more gains in the computation-heavy scenario.\nFigure 8 shows an ablation plot of the gains of quantization on both communication and compu-\ntation in a communication heavy scenario (frequency=30) versus a computation-heavy scenario\n(frequency=300).\nThe ﬁgures show that in a communication heavy scenario (Figure 8a), quantizing communication\nmay yield up to 30% speedup; conversely, in a computation-heavy scenario (Figure 8b) quantizing\ncommunication has little impact as the overhead is dominated by computation. Therefore, we\nbelieve that communication would incur higher costs on a networked cluster as actors scale.\n12\nPublished in Transactions on Machine Learning Research (07/2022)\n0\n100000\n200000\n300000\n400000\nSteps\n0\n200\n400\n600\n800\nReturn\nupdate freq=100\nupdate freq=1000\n(a) Humanoid Stand.\nSteps\nReturn\n(b) Walker Stand.\nFigure 7: Studying the eﬀect of training with more frequent actor pulls in Humanoid stand and\nWalker Stand. We observe that the more frequent actor pulls learns faster than with less frequent\nactor pulls and demonstrates model pull frequency aﬀects staleness of actor policies and may have\nan eﬀect on training.\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nTime (s)\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nReturn\nq=32,q_c=32\nq=32,q_c=8\nq=8\n(a) Communication Heavy (Update Freq=30)\n200\n400\n600\n800\n1000\n1200\nTime (s)\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nReturn\nq=32,q_c=32\nq=32,q_c=8\nq=8\n(b) Computation Heavy (Update Freq=300)\nFigure 8: Eﬀects of quantizing communication versus computation in compute heavy and communi-\ncation heavy training scenarios. q is the precision of inference; q_c is the precision of communication.\nNote q=8 implicitly quantizes communication to 8 bits. Experiment run on the walker stand task,\nusing the D4PG algorithm.\n5.3\nRationale for Why Quantization of Actors Speed-Up RL Training\nWe further break down the various components contributing to runtime on a single actor to\nunderstand how quantization of actor’s policy inference speeds up training. Runtime components are\nbroken down into: step time, pull time, deserialize time, and load_state_dict time. Step time is the\ntime spent performing neural network policy inference. Pull time is the time between querying the\nReverb queue (DeepMind, 2020) for a model and receiving the serialized models’ weights; deserialize\ntime is the time spent to deserialize the serialized model dictionary; load_state_dict time is the\ntime to call PyTorch load_state_dict (used for loading and storing the policy).2\nFigure 9a shows the relative breakdown of the component runtimes with fp32 (denoted as q=32),\nfp16 (denoted as q=16), and int8 (denoted as q=8) quantized inference in the communication heavy\nscenario. It is communication heavy because every 30 steps (Update Freq=30), the learner’s policy\nis updated to each actors. This causes increased communication between the learner and actor\n2https://bit.ly/pytorch_api\n13\nPublished in Transactions on Machine Learning Research (07/2022)\nq=32\nq=16\nq=8\n0\n2\n4\n6\n8\n10\nTime (s)\nStep Time\nPull Time\nDeserialize Time\nload_state_dict time\n(a) Communication Heavy (Update Freq=30)\nq=32\nq=16\nq=8\n0\n2\n4\n6\n8\n10\nTime (s)\nStep Time\nPull Time\nDeserialize Time\nload_state_dict time\n(b) Computation Heavy (Update Freq=300)\nFigure 9: Breakdown of components for quantized and non-quantized training over 1000 steps.\ncomponents. As shown, step time is the main bottleneck, and quantization of the actor’s policy\nsigniﬁcantly speed-up each roll-out, speeding up the overall training. Figure 9b shows the cost\nbreakdown in the computation heavy scenario. While quantization speeds up the step time (Actor’s\npolicy inference), the pull time is also signiﬁcantly reduced since there is less communication (Update\nFreq=300) happening between learner and actor. Quantization can still reduce the deserialize time\nand load_state_dict time since quantization lowers the memory footprint compared to the fp32\nscenario.\nIn int8 and fp16 quantized training, the cost of PyTorch load_state_dict is signiﬁcantly higher.\nAn investigation shows that the cost of loading a quantized PyTorch model is spent repacking the\nweights from Python object into C data. Int8 weight repacking is noticeably faster than fp16 weight\nrepacking due to fewer memory accesses. The cost of model loading suggests that additional speed\ngains can be achieved by serializing the packed C data structure and reducing the cost of weight\npacking.\n6\nDiscussion & Future Work\nTo the best of our knowledge, our work is one of the ﬁrst to experimentally and quantitatively\ndemonstrated that quantization may be eﬀectively applied to many facets of reinforcement learning,\nfrom obtaining high quality and eﬃcient quantized policies, to reducing training times and eliminating\ncarbon emissions. More speciﬁcally, we have shown that reinforcement learning policies may be\nquantized down to 4-5 bits (See Appendix A) without signiﬁcantly aﬀecting their performance;\nbased on this result, we have developed a simple but eﬀective method for speeding up reinforcement\nlearning training, ActorQ, which achieves between 1.5× and 5.41× speedup over non quantized\ntraining, with a reduction in carbon emissions between 1.9× and 3.76×. In the future, alternative\nand more competitive methods are likely to emerge.\nThe computational requirements for RL training are growing (Espeholt et al., 2019) (Espeholt\net al., 2018). Training OpenAI Five to play Dota 2 required a scaled-up version of Proximal\nPolicy Optimization running on 512 GPUs and 51200 CPU cores (Berner et al., 2019). As we\nscale RL training to more thousands of cores and GPUs, even a 50% improvement as we have\nexperimentally demonstrated (Table 4) will result in enormous savings in absolute dollar cost,\nenergy, and carbon emissions. We believe that quantization, which is already a standard technique\napplied to non-reinforcement learning neural network models, will likewise be a critical technique\nin optimizing the performance of reinforcement learning policies. Our paper demonstrates that,\nlike for standard neural networks, quantization yields signiﬁcant beneﬁts for reinforcement learning\n14\nPublished in Transactions on Machine Learning Research (07/2022)\nwhile maintaining accuracy. We believe our work is a ﬁrst step towards quantization being applied\nto reinforcement learning to achieve eﬃcient, and environmentally sustainable training.\nSeveral design decisions in our system warrant further discussion and research. In our design\nof the quantizer in ActorQ, we relied on simple uniform quantization, however, we believe other\nforms of aggressive quantization / compression (Park et al., 2016; Polino et al., 2018; Tambe et al.,\n2020; Lam et al., 2021) can also be applied (e.g., distillation, sparsiﬁcation, etc.). Applying more\naggressive quantization / compression methods may yield additional beneﬁts to the performance\n/ accuracy tradeoﬀobtained by the trained policies. Additionally, in order to achieve tangible\nspeedups from quantization, the underlying hardware system must support quantized operations\nat the machine level. However, with increased hardware support for neural network execution,\nwe believe that devices in the future will exhibit an increasing amount of hardware support for\nquantized operations (Jouppi et al., 2017). Finally, in ActorQ, we primarily focused on quantizing\nactor neural network execution and the communication between actors and learners (as these were\nthe biggest computational bottlenecks), however, we believe that the learner’s policy can similarly\nbe quantized to achieve further performance beneﬁts.\n7\nConclusion\nTo the best of our knowledge, we are the ﬁrst to apply quantization to not only speed up RL\ntraining and inference, but also reduce RL carbon emissions. We experimentally demonstrate that\nstandard quantization methods can quantize policies down to ≤8 bits with little quality loss. We\npresent ActorQ to attain signiﬁcant speedups and reductions in carbon emissions over full precision\ntraining. Our results demonstrate that quantization has considerable potential in speeding up both\nreinforcement learning inference and training while reducing carbon emissions. Future work includes\nextending the results to networked clusters to evaluate further the impacts of communication and\napplying quantization to reinforcement learning to diﬀerent application scenarios such as the edge.\nAcknowledgements\nThe authors will like to thank Google for providing GCP research credits. We would also like to\nthank anonymous reviewers for their valuable feedback on the manuscript.\nReferences\nMohammad Babaeizadeh, Iuri Frosio, Stephen Tyree, Jason Clemons, and Jan Kautz. Reinforcement\nlearning through asynchronous advantage actor-critic on a gpu. arXiv preprint arXiv:1611.06256,\n2016.\nRon Banner, Yury Nahshan, Elad Hoﬀer, and Daniel Soudry. Post-training 4-bit quantization of\nconvolution networks for rapid-deployment, 2018.\nGabriel Barth-Maron, Matthew W. Hoﬀman, David Budden, Will Dabney, Dan Horgan, Dhruva TB,\nAlistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic\npolicy gradients. 2018.\nM. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An\nevaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279,\njun 2013.\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, Rafal Józefowicz,\n15\nPublished in Transactions on Machine Learning Research (07/2022)\nScott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pondé de Oliveira Pinto,\nJonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever,\nJie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning.\nCoRR, abs/1912.06680, 2019. URL http://arxiv.org/abs/1912.06680.\nC. M. Bishop. Training with noise is equivalent to tikhonov regularization. Neural Computation, 7\n(1):108–116, Jan 1995. doi: 10.1162/neco.1995.7.1.108.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/abs/\n1606.01540.\nHao-Tien Lewis Chiang, Aleksandra Faust, Marek Fiser, and Anthony Francis. Learning navigation\nbehaviors end-to-end with autorl. IEEE Robotics and Automation Letters, 4(2):2007–2014, April\n2019. ISSN 2377-3766. doi: 10.1109/LRA.2019.2899918.\nJungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini-\nvasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural\nnetworks. 2018.\nSteven Dalton, Iuri Frosio, and Michael Garland. Gpu-accelerated atari emulation for reinforcement\nlearning, 2019.\nDipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth Avancha,\nKunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas,\nAlexander Heinecke, Pradeep Dubey, Jesus Corbal, Nikita Shustrov, Roma Dubtsov, Evarist\nFomenko, and Vadim Pirogov. Mixed precision training of convolutional neural networks using\ninteger operations. International Conference on Learning Representations (ICLR), 2018.\nDeepMind. Reverb. https://github.com/deepmind/reverb, 2020.\nZhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Hawq: Hes-\nsian aware quantization of neural networks with mixed-precision. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pp. 293–302, 2019.\nLasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam\nDoron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl\nwith importance weighted actor-learner architectures. In International Conference on Machine\nLearning (ICML), 2018.\nLasse Espeholt, Raphaël Marinier, Piotr Stanczyk, Ke Wang, and Marcin Michalski. Seed rl:\nScalable and eﬃcient deep-rl with accelerated central inference. 2019.\nWilliam Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark\nRowland, and Will Dabney. Revisiting fundamentals of experience replay. In International\nConference on Machine Learning (ICML), 2020.\nPeter Henderson, Jieru Hu, Joshua Romoﬀ, Emma Brunskill, Dan Jurafsky, and Joelle Pineau.\nTowards the systematic reporting of the energy and carbon footprints of machine learning. Journal\nof Machine Learning Research, 21(248):1–43, 2020. URL http://jmlr.org/papers/v21/20-312.\nhtml.\n16\nPublished in Transactions on Machine Learning Research (07/2022)\nKazutoshi Hirose, Ryota Uematsu, Kota Ando, Kodai Ueyoshi, Masayuki Ikebe, Tetsuya Asai,\nMasato Motomura, and Shinya Takamaeda-Yamazaki. Quantization error-based regularization\nfor hardware-aware neural network training. Nonlinear Theory and Its Applications, IEICE, 9(4):\n453–465, 2018. doi: 10.1587/nolta.9.453.\nMatt Hoﬀman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara\nNorman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, et al. Acme: A research\nframework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020.\nDan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt,\nand David Silver. Distributed prioritized experience replay. In International Conference on\nLearning Representations (ICLR), 2018.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized\nneural networks: Training neural networks with low precision weights and activations. Journal of\nMachine Learning Research, 18(187):1–30, 2018. URL http://jmlr.org/papers/v18/16-456.\nhtml.\nNorman Jouppi, Al Borchers, Rick Boyle, Pierre-luc Cantin, Cliﬀord Chao, Chris Clark, Jeremy\nCoriell, Mike Daley, Matt Dau, Jeﬀrey Dean, Ben Gelb, CliﬀYoung, Tara Ghaemmaghami,\nRajendra Gottipati, William Gulland, Robert Hagmann, C. Ho, Doug Hogberg, John Hu, and\nNan Boden. In-datacenter performance analysis of a tensor processing unit. pp. 1–12, 06 2017.\ndoi: 10.1145/3079856.3080246.\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre\nQuillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Qt-opt:\nScalable deep reinforcement learning for vision-based robotic manipulation. CoRR, abs/1806.10293,\n2018. URL http://arxiv.org/abs/1806.10293.\nSteven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent\nexperience replay in distributed reinforcement learning. In International Conference on Learning\nRepresentations (ICLR), 2018.\nMichał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jaśkowski. Vizdoom:\nA doom-based ai research platform for visual reinforcement learning. In 2016 IEEE Conference\non Computational Intelligence and Games (CIG), pp. 1–8. IEEE, 2016.\nRaghuraman Krishnamoorthi. Quantizing deep convolutional networks for eﬃcient inference: A\nwhitepaper. arXiv preprint arXiv:1806.08342, 2018.\nSolomon Kullback and Richard A Leibler. On information and suﬃciency. The annals of mathematical\nstatistics, 22(1):79–86, 1951.\nMaximilian Lam, Zachary Yedidia, Colby R Banbury, and Vijay Janapa Reddi. Precision batching:\nBitserial decomposition for eﬃcient neural network inference on gpus. In 2021 30th International\nConference on Parallel Architectures and Compilation Techniques (PACT), pp. 129–141, 2021.\ndoi: 10.1109/PACT52795.2021.00017.\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep\nvisuomotor policies, 2015.\n17\nPublished in Transactions on Machine Learning Research (07/2022)\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\nChristos Louizos, Matthias Reisser, Tijmen Blankevoort, Efstratios Gavves, and Max Welling.\nRelaxed quantization for discretized neural networks. International Conference on Learning\nRepresentations (ICLR), 2018.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning, 2013.\nVolodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap,\nTim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\nlearning. CoRR, abs/1602.01783, 2016. URL http://arxiv.org/abs/1602.01783.\nPhilipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang,\nMelih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. Ray: A distributed framework\nfor emerging {AI} applications. In 13th {USENIX} Symposium on Operating Systems Design\nand Implementation ({OSDI} 18), pp. 561–577, 2018.\nArun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria,\nVedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, et al. Massively\nparallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296, 2015.\nOpenAI. Openai ﬁve. https://blog.openai.com/openai-five/, 2018.\nOpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur\nPetron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas\nTezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei\nZhang. Solving rubik’s cube with a robot hand, 2019.\nJongsoo Park, Sheng R. Li, Wei Wen, Hai Li, Yiran Chen, and Pradeep Dubey. Holistic spar-\nsecnn: Forging the trident of accuracy, speed, and size. International Conference on Learning\nRepresentations (ICLR), 2016.\nAleksei Petrenko, Zhehui Huang, Tushar Kumar, Gaurav Sukhatme, and Vladlen Koltun. Sample\nfactory: Egocentric 3d control from pixels at 100000 fps with asynchronous reinforcement learning.\nIn International Conference on Machine Learning (ICML), 2020.\nAntonio Polino, Razvan Pascanu, and Dan Alistarh.\nModel compression via distillation and\nquantization. International Conference on Learning Representations (ICLR), 2018.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nDavid Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre, George van den\nDriessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander\nDieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap,\nMadeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the\ngame of go with deep neural networks and tree search.\nNature, 529:484–503, 2016.\nURL\nhttp://www.nature.com/nature/journal/v529/n7587/full/nature16961.html.\n18\nPublished in Transactions on Machine Learning Research (07/2022)\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go\nwithout human knowledge. Nature, 550(7676):354, 2017.\nXiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalak-\nshmi (Viji) Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit\nﬂoating point (hfp8) training and inference for deep neural networks. In Advances in Neural\nInformation Processing Systems 32. 2019.\nThierry Tambe, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush,\nDavid Brooks, and Gu-Yeon Wei.\nAlgorithm-hardware co-design of adaptive ﬂoating-point\nencodings for resilient deep learning inference. In 2020 57th ACM/IEEE Design Automation\nConference (DAC), pp. 1–6. IEEE, 2020.\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David\nBudden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv\npreprint arXiv:1801.00690, 2018.\nNaigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training\ndeep neural networks with 8-bit ﬂoating point numbers. In Advances in Neural Information\nProcessing Systems, 2018.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng,\nGloria Chang, Fiona Aga Behram, James Huang, Charles Bai, et al. Sustainable ai: Environmental\nimplications, challenges and opportunities. arXiv preprint arXiv:2111.00364, 2021.\nRitchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving Neural Network\nQuantization without Retraining using Outlier Channel Splitting. International Conference on\nMachine Learning (ICML), pp. 7543–7552, June 2019.\n19\nPublished in Transactions on Machine Learning Research (07/2022)\nAppendix\nA\nQuantization Aware Training (QAT)\nTo understand how aggressively (i.e., number of bits) we can quantify the RL policies, we use\nquantization aware training (QAT). In QAT, the RL policy weights and activations are passed\nthrough the quantization function Qn during inference; during backpropagation the straight-through\nestimator is used as the gradient of Qn (Krishnamoorthi, 2018)\n∇W Qn(W) = I\nNote that quantization aware training does not speed up training as all operations are still performed\nin ﬂoating-point. Quantization aware training is used primarily to train a model with simulated\nquantized weights and activations to evaluate the reward loss (if any) for a given RL task. Native\nhardware and library support for sub int8 quantization can speed policy inference time with quantized\nexecution.\nReward\n450\n500\n550\nFp PTQ8-bit7-bit6-bit5-bit4-bit3-bit2-bit\nCartpole\n Fp 8* 8  7  6  5  4  3  2 \n550\n500\n450\nCartpole\nReward\n       A2C\n       PPO\nbit\nReward\n0\n200\n400\nFp PTQ8-bit 7-bit 6-bit 5-bit 4-bit 3-bit 2-bit\nBreakOut\nBreakOut\nReward\n       A2C\n       PPO\nbit\n400\n  \n200\n  \n0\n Fp 8* 8  7  6  5  4  3  2 \nReward\n0\n1000\n2000\nFp PTQ8-bit7-bit6-bit5-bit4-bit3-bit2-bit\nSeaQuest\n2000\n  \n1000\n  \n0\nSHD4XHVW\nReward\nbit\n       A2C\n       PPO\n Fp 8* 8  7  6  5  4  3  2 \nReward\n−20\n0\n20\nFp PTQ8-bit 7-bit 6-bit 5-bit 4-bit 3-bit 2-bit\nPong\n20\n0\n-20\nPong\nReward\n       A2C\n       PPO\nbit\n Fp 8* 8  7  6  5  4  3  2 \nReward\n0\n1000\n2000\nFp PTQ8-bit7-bit6-bit5-bit4-bit3-bit2-bit\nMsPacman\n2000\n1000\n0\nMsPacman\nReward\n       A2C\n       PPO\nbit\n Fp 8* 8  7  6  5  4  3  2 \nReward\n0\n5,000\n10,000\n15,000\nFp PTQ8-bit7-bit6-bit5-bit4-bit3-bit2-bit\nQBert\n15000\n10000\n5000\n0\nQBert\nReward\n       A2C\n       PPO\nbit\n Fp 8* 8  7  6  5  4  3  2 \nReward\n0\n1000\n2000\n3000\nFp PTQ8-bit7-bit6-bit5-bit4-bit3-bit2-bit\nBeamRider\nBeamRider\nReward\n       A2C\n       PPO\nbit\n3000\n2000\n1000\n0  Fp 8* 8  7  6  5  4  3  2 \nReward\n500\n1000\nFp PTQ8-bit7-bit6-bit5-bit4-bit3-bit2-bit\nSpaceInvader\n1000\n500\n0\nSpaceInvader\nReward\n       A2C\n       PPO\nbit\n Fp 8* 8  7  6  5  4  3  2 \nReward\n−100\n0\n100\nFp PTQ 8-bit 7-bit 6-bit 5-bit 4-bit 3-bit 2-bit\nMountainCar\nMountainCar\n Fp 8*  8   7   6   5   4  3   2 \n100\n0\n-100\nReward\n Fp 8* 8  7  6  5  4  3  2 \n100\n0\n-100\nMountainCar\nReward\n       DDPG\nbit\nReward\n0\n1000\n2000\nFp PTQ 8-bit 7-bit 6-bit 5-bit 4-bit 3-bit 2-bit\nWalker2DBulletEnv 0\nFp 8* 8 7 6 5 4 3 2\n2000\n000\n0\n Fp 8* 8  7  6  5  4  3  2 \n2000\n1000\n0\nWalker2D\nReward\n       DDPG\nbit\nReward\n0\n2000\nFp PTQ8-bit7-bit6-bit5-bit4-bit3-bit2-bit\n Fp 8* 8   7  6   5  4  3   2 \n2000\n0\nReward\n   \n \n Fp 8* 8  7  6  5  4  3  2 \n2000\n  \n0\nHalfCheetah\nReward\n       DDPG\nbit\n   \n   \nReward\n−100\n0\n100\nFp PTQ8-bit 7-bit 6-bit 5-bit 4-bit 3-bit 2-bit\nBiPedalWalker v2\n100\n0\n-100\nFp 8* 8 7 6\n5 4\n3 2\nReward\n Fp 8* 8  7  6  5  4  3  2 \n100\n0\n-100\nBiPedalWalker\nReward\n       DDPG\nbit\n   \n    \nFigure 10: Quantization aware training of PPO, A2C, and DDPG algorithms on OpenAI gym,\nAtari, and PyBullet. FP denotes fp32 and 8* is achieved by int8 post-training quantization.\nWe present rewards for policies quantized via quantization aware training on multiple environments\nand training algorithms in Figure 10. We train a three-layer convolutional neural network for all\nAtari arcade learning. For openAI Gym environments, we train neural networks with two hidden\nlayers of size 64. We train all agents for 10 Million steps. Since in QAT, the trainer inserts fake\nquantization node is the neural network graph (policy in the context of RL), and the statistics\n20\nPublished in Transactions on Machine Learning Research (07/2022)\nof weights distribution during training are collected before the weights can be quantized. The\nquantization delay is a hyperparameter (Krishnamoorthi, 2018) that controls when the quantization\nis applied to the weights. One can consider this the “warm-up” period to collect the policy weight\ndistribution statistics before applying quantization. We tried diﬀerent values of quantization delay\nand used 5M since that gave us the best results.\nOur results (see Fig. 10) suggest that generally, the performance relative to the full precision baseline\nis maintained until 5/6-bit quantization, after which there is a drop in reward, suggesting that for\nseveral of these environments, we can quantize the policy to 5-bits. However, to achieve measurable\nspeed-up during training, there is a need for native hardware and library support (e.g., Nvidia\nCuda/Intel MKL/ ARM Neon support in TensorFlow, Pytorch, and JAX frameworks).\nBroadly, at 8-bits, we see no degradation in rewards. Hence, when applying quantization in ActorQ,\nwe quantize the policy to 8-bits. By quantizing the policy at 8-bits and leveraging the native\n8-bit computation support in hardware, we achieve end-to-end speed-up in reinforcement learning\ntraining.\n2\n4\n6\n8\nNumber of actors\n10000\n15000\n20000\n25000\nWall clock time (s)\nfp32\nint8\nFigure 11: Wall clock time for training DQN with varying number of actors. The actor policies are\ntrained in full precision (fp32) and int8. Beneﬁts of quantization is agnostic to RL training setup\nand is eﬀective for both single actor RL (non-parallel version) as well as distributed RL training\n(number of actors greater than one).\nB\nParallelization and Scaling\nIn this paper, we show that applying quantization to reinforcement learning training improves\ntraining speed and lowers carbon emissions. Though in Table 1 explicitly compares with distributed\nRL training, we believe quantization can also improve single actor training.\nThe cost of RL training can be formulated as\nC = cb.na + ca.na −co.np\nWhere , na: number of agents\nnp: number of processors\ncb : cost for broadcasting (computed as a diﬀerence between quantized and non-quantized )\nca: cost for actor inference (computed as a diﬀerence between quantized and non-quantized )\nco: cost of overhead when more than one processor is used.\nOur proposed method oﬀers O(n) saving where n is the number of actors. Of course, this formula\nworks only in the ideal cases, and will downgrade in real computer systems that run other processes\n21\nPublished in Transactions on Machine Learning Research (07/2022)\n(a) BeamRider.\n(b) Breakout.\n(c) CartPole.\n(d) MsPacman.\n(e) Pong.\n(f) Qbert.\n(g) Seaquest.\n(h) SpaceInvaders.\nFigure 12: Variation in the action distribution for DQN between fp32 policy and quantized policy\n(int8) on several environments evalauted in Table 2.\nIn the legends, ‘action’ and ‘action_q‘’\ncorresponds to the actions taken by the fp32 and int8 policies.\nand are subject to network traﬃc. Still these slowdowns are scaling with the number of processors,\nleaving overall linear improvement with diﬀerent scalers depending on the number of processors\nused. Fig. 11 demonstrates the linear relationship between training time (savings) as number of\nactors scale. It is also important to note that in the case of single actor (non-distributed scenario),\nquantization of actor’s policy still improves the training speed since ca is minimized due to quantized\ninference of actor’s policy.\nC\nAction Distribution Visualization\nIn this section, we include the variation in the action distribution between fp32 policy and quantized\npolicy (int8) for PPO and DQN for several environments evaluated in Table 2.\nMethodology for visualizing action distributions. To visualize the action distribution varia-\ntion between int8 and fp32 policy, we load both the policies for rollouts. First, for each algorithm\nand task (environment) combination, we run a rollout of 5000 steps. The same observation is passed\nto the int8 and fp32 policies. Finally, the actions for both policies are logged and visualized.\nAcross diﬀerent RL algorithms and tasks, we observe that the variation of the action between the\nfp32 policy and int8 policy is very small. Also, since the expected mean return between fp32 and\nint8 policies is very similar, it suggests that quantization of the policies facilitates safe exploration.\n22\nPublished in Transactions on Machine Learning Research (07/2022)\n(a) Breakout.\n(b) CartPole.\n(c) MsPacman.\n(d) Pong.\n(e) Qbert.\n(f) Seaquest.\n(g) SpaceInvaders.\n(h) SpaceInvaders.\nFigure 13: Variation in the action distribution for PPO between fp32 policy and quantized policy\n(int8) on several environments evalauted in Table 2.\nIn the legends, ‘action’ and ‘action_q‘’\ncorresponds to the actions taken by the fp32 and int8 policies.\nAlso, from a hardware utilization perspective, quantized computations (int8) are faster and more\nenergy-eﬃcient than fp32 computation. Hence, quantization is a simple yet eﬀective strategy to\nimprove the training speed of RL sustainably (i.e., lower carbon emissions).\n23\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2019-10-02",
  "updated": "2022-11-14"
}