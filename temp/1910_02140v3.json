{
  "id": "http://arxiv.org/abs/1910.02140v3",
  "title": "Discounted Reinforcement Learning Is Not an Optimization Problem",
  "authors": [
    "Abhishek Naik",
    "Roshan Shariff",
    "Niko Yasui",
    "Hengshuai Yao",
    "Richard S. Sutton"
  ],
  "abstract": "Discounted reinforcement learning is fundamentally incompatible with function\napproximation for control in continuing tasks. It is not an optimization\nproblem in its usual formulation, so when using function approximation there is\nno optimal policy. We substantiate these claims, then go on to address some\nmisconceptions about discounting and its connection to the average reward\nformulation. We encourage researchers to adopt rigorous optimization\napproaches, such as maximizing average reward, for reinforcement learning in\ncontinuing tasks.",
  "text": "arXiv:1910.02140v3  [cs.AI]  27 Nov 2019\nDiscounted Reinforcement Learning\nIs Not an Optimization Problem\nAbhishek Naik1,2∗Roshan Shariff1\nNiko Yasui1\nHengshuai Yao2\nRichard S. Sutton1\n{anaik1,rshariff,yasui,rsutton}@ualberta.ca\nhengshuai.yao@huawei.com\n1Department of Computing Science\n2Huawei Technologies\nUniversity of Alberta\nEdmonton, Canada\nAbstract\nDiscounted reinforcement learning is fundamentally incompatible with function\napproximation for control in continuing tasks. It is not an optimization problem in\nits usual formulation, so when using function approximation there is no optimal\npolicy. We substantiate these claims, then go on to address some misconceptions\nabout discounting and its connection to the average reward formulation. We en-\ncourage researchers to adopt rigorous optimization approaches, such as maximiz-\ning average reward, for reinforcement learning in continuing tasks.\n1\nIntroduction\nReinforcement learning (RL) is a paradigm in which an agent learns to interact with an environment\nin order to maximize reward [Sutton and Barto, 2018]. Many interesting RL problems concern\ncontinuing tasks in which the agent lives and learns over a single lifetime, rather than experiencing\na sequence of distinct episodes. Some examples of continuing tasks include routing internet packets,\nmanaging the inventory in a warehouse, and controlling the temperature in a data center.\nIn continuing tasks, it is common practice to value immediate rewards more highly than rewards\nfurther in the future — this is called temporal discounting. One reason is that the sum of future\nrewards (which is ﬁnite for episodic tasks) can grow to inﬁnity as the agent and environment con-\ntinually interact. With discounting the sum of the inﬁnitely many future rewards remains bounded\neven when the length of the interaction does not.1\nIn this paper, we take the position that this common practice has serious conceptual ﬂaws and should\nbe carefully reconsidered. The straightforward formulation of discounted reinforcement learning is\nfundamentally incompatible with large-scale reinforcement learning in continuing tasks. Moreover,\nthese issues are inherent to the very idea of discounting and cannot be avoided by minor modiﬁca-\ntions of the problem formulation.\nFor continuing tasks, we will ﬁrst argue that discounted reinforcement learning is not an optimiza-\ntion problem, and that this makes it incompatible with function approximation. A standard optimiza-\ntion problem is deﬁned by a set of feasible solutions and an objective function that describes the\nquality of each feasible solution with a real number. The feasible solutions in RL are policies and,\nat least for episodic tasks, the natural objective function is the sum of rewards over an episode. With\ncontinuing tasks, however, optimality under discounting is not deﬁned as maximizing an objective\n∗This work was partially done during an internship at Huawei Technologies.\n1The notion of discounting discussed here is one that leads to a ﬁnite sum of future rewards. This occurs with\ngeometric discounting, for instance, but not hyperbolic discounting [Fedus et al., 2019], which is applicable\nonly in ﬁnite-horizon episodic problems.\nOptimization Foundations for Reinforcement Learning Workshop at NeurIPS 2019, Vancouver, Canada.\nfunction. Instead, a policy is usually considered optimal if, in every state, it achieves a higher dis-\ncounted sum of future rewards than any other policy — for a ﬁxed discount rate γ, an optimal policy\nπ∗satisﬁes\nvγ\nπ∗(s) ≥vγ\nπ(s),\nfor all states s and policies π.\n(1)\nThese inequalities produce a partial order on the set of policies: some pairs of policies may be\nincomparable with each other, because each achieves a higher value in some states but a lower value\nin others. This is not a problem with tabular representations, which can represent any possible\npolicy — in their foundational work, Bellman and Dreyfus [1959] proved the existence of a policy\nthat maximizes value at every state simultaneously. With function approximation, on the other hand,\na partial ordering is not enough to identify an optimal policy.\n1.1\nThere is no optimal representable policy with discounting and function approximation\nIn many RL problems the state or action spaces are so large that policies cannot be represented as\na table of action probabilities for each state. In such domains we often resort to a compact policy\nrepresentation that cannot represent every possible policy. In most cases the optimal policy deﬁned\nby (1) will no longer be representable, so the agent cannot hope to learn it. Instead, the agent should\nﬁnd the best representable policy.\nHowever, without an objective function there is usually no representable policy that is unambigu-\nously better than all the other representable policies. In general, for every representable policy there\nwill be another policy that has a higher value in some states but a lower value in others, so the partial\norder will not be able to identify any policy as optimal — the notion of “best representable policy”\nis not well-deﬁned. See Singh et al. [1994] for illustrative examples.\nIf we had an explicit objective function we would be able to compare any two policies, creating a\ntotal order over the policy space. Regardless of the choice of policy representation, the total ordering\nprovided by an explicit objective function would guarantee the existence of an optimal representable\npolicy. This is why there is no issue with deﬁning optimal policies for episodic tasks, where the sum\nof rewards is a natural and explicit objective function.\nIn the rest of this paper we will address the question of choosing an appropriate objective function\nfor continuing tasks. We will ﬁnd that discounting is hard to incorporate into a meaningful objective\nfunction. Many common choices do not capture the continuing nature of the task, whereas others\nare formally equivalent to undiscounted objectives. Indeed, we will be forced to conclude that\ndiscounting is not just a technical problem with the above deﬁnition of optimality. Rather, there\nis a fundamental mismatch between continuing tasks and discounting that cannot be ignored when\nfunction approximation is required.\nWe now argue that function approximation and continuing tasks are indispensable for reinforcement\nlearning, which makes this problem setting important.\n1.2\nContinuing control with function approximation is the problem that matters for AI\nReinforcement learning with tabular representations has served us well to build intuition, construct\nalgorithms, and analyze their properties, but the world we live in is too large to be represented\nby tables of values or action probabilities. Furthermore, it is crucial to generalize across similar\nstates and actions. Function approximation serves both these purposes and is necessary for agents\nto compactly represent complex worlds and make sense of their complexity.\nContinuing tasks are those in which the agent-environmentinteraction does not naturally break down\ninto episodes. Lifelong learning follows this paradigm, because an agent living and learning over a\nlifetime does not usually have the ability to reset to a pre-deﬁned initial state. Additionally, many\ninteresting RL tasks being studied today involve extremely long episodes and the challenges that\ncome with them, such as sparse rewards and credit assignment over long time horizons. Such tasks\nare episodic in name only, and should be thought of as continuing tasks in spirit. In particular,\nsolutions that fundamentally rely on episodes are likely to fare worse than those that fully embrace\nthe continuing task setting.\nFunction approximation is necessary in large-scale reinforcement learning, whereas discounting is\nnot. We believe that whole-heartedly adopting optimization approaches for control in continuing\ntasks (for example, average reward) is the most appropriate direction for our discipline.\n2\n1.3\nPaper organization\nAfter covering some background in Section 2, we describe an alternative continuing task formula-\ntion in Section 3 which has a well-deﬁned objective — maximizing the average reward per step —\nmaking it suitable for use with function approximation. We discuss its equivalence with the time-\naverage of the discounted value function, and the resulting misconception that common discounted\nalgorithms maximize the average reward irrespective of the choice of the discount factor. We sum-\nmarize the arguments in Section 4 and give pointers to the existing literature involving the average\nreward formulation.\n2\nBackground\nA Markov decision process (MDP) consists of a ﬁnite set of states S and a ﬁnite set of actions A\nthat can be performed in those states. When the agent takes an action in a state, the agent transitions\nto another state and receives a scalar reinforcement signal called the reward. Formally, an MDP\ncan be represented by M = (S, A, P, r, µ0), where upon performing action a in state s the agent\nreceives reward r(s, a) in expectation and transitions to state s′ with probability Pa(s, s′). Note that\nthe transition probabilities and rewards are independent of the agent’s history before reaching state\ns (the Markov assumption). The agent’s initial state is distributed according to µ0.\nA policy π : S × A →[0, 1] gives, for every state in an MDP, a probability distribution over actions\nto be taken in that state. A policy π acting in an MDP (S, A, P, r, µ0) produces a Markov reward\nprocess (S, Pπ, rπ, µ0), where Pπ : S × S →[0, 1] is the transition probability and rπ : S →R is\nthe reward function:\nPπ(s, s′) =\nÕ\na∈A\nπ(s, a) Pa(s, s′),\nrπ(s) =\nÕ\na∈A\nπ(s, a) r(s, a).\nAverage reward\nThe average reward of a policy is an intuitively straightforward quantity — it is the reward the agent\nreceives on average every time step as it acts in an environment. We deﬁne the average reward\nmore formally as follows. Under certain mild conditions, a Markov reward process has a so-called\nstationary distribution over states, dπ(s), that satisﬁes\ndπ(s′) =\nÕ\ns∈S\ndπ(s) Pπ(s, s′)\nfor every s′ ∈S.\nIn other words, if the agent’s state is distributed according to the stationary distribution and it acts\naccording to its policy, then its next state will also follow the stationary distribution. Furthermore,\nunder additional mild conditions, the state distribution converges to dπ(s) over the long run regard-\nless of the starting state s0:\ndπ(s) = lim\nT→∞Pr(ST = s),\nwhere S0 = s0 and St+1 ∼Pπ(St, · ) for all time steps t.\nIn continuing tasks, dπ(s) measures the fraction of time the agent spends in state s. Frequently\nvisited states have higher values of dπ, and if dπ(s) = 0 then s is called a transient state: it is only\nvisited a ﬁnite number of times and never again.\nThe average reward of a policy is simply deﬁned as the average one-step reward, weighted by the\nproportion of time spent in each state while following that policy:\n¯r(π) =\nÕ\ns∈S\ndπ(s) rπ(s).\n(2)\nFor the purposes of this paper, we only consider stationary Markov policies, which do not change\nwith time and depend only on the current state, not previous history. For more details on MDPs and\naverage reward, refer to Puterman [1994].\n3\n3\nAn Optimization Objective for Continuing Tasks\nWe would like to ﬁnd an objective for continuing tasks that is analogous to the sum of rewards in\nepisodic tasks. Such an objective should be a function that quantiﬁes the performance of each policy\nby a single number, so that any policy can be compared with any other and our goal of ﬁnding the\nbest policy amongst any class of policies becomes well-deﬁned.\nThe founding principle of reinforcement learning is that an agent acts to maximize reward not just\nin the present, but also over its future lifetime, which is potentially inﬁnite in continuing tasks. The\noptimality deﬁnition of (1) captures this idea — vγ\nπ(s) represents the future reward (though not\ninﬁnitely far in the future) which the agent seeks to maximize at every state s.\nA natural objective function, therefore, is the weighted average of vγ\nπ(s) with each state weighted by\nµ(s) — maximizing Í\ns µ(s) vγ\nπ(s). The choice of µ determines which states the agent prefers when it\ncannot act optimally in every state simultaneously. We could contemplate evaluating policies using\ntheir discounted value from the start state (setting µ = µ0, the initial state distribution), but this gives\nundue importance to the early part of an agent’s lifetime, going against the never-ending nature of\nthe task. Indeed, the start states may never be revisited in a continuing task, so they do not deserve\nspecial treatment. Moreover, the states experienced by a long-lived agent might not even be close\nto the start states, so it seems nonsensical to ask the agent to maximize short-term reward at those\nstates.\nThe objective can be made more meaningful in two ways — either by making µ representative of the\nstates the agent will actually experience over its lifetime, or by using a longer-term future reward.\nThe ﬁrst corresponds to setting µ = dπ; we saw in Section 2 that dπ(s) is proportional to how\nfrequently the agent visits state s. The second alternative corresponds to increasing the discount rate\nγ →1. We will see in Sections 3.1 and 3.2 that both these alternatives are, in fact, equivalent to the\naverage reward objective, and give the optimization problem\narg max\nπ ∈Π\nÕ\ns∈S\ndπ(s) rπ(s)\n≡\narg max\nπ ∈Π\n¯r(π),\n(using (2))\nwhere Π is the set of representable policies. This objective function captures a core tenet of rein-\nforcement learning — that the agent’s actions should cause it to visit states where it can earn large\nrewards. In continuing tasks, the agent should ﬁnd policies that cause it to frequently visit these\nhighly rewarding states.\nAnother alternative is to set µ to be an arbitrary weighting of states that is neither the initial state\ndistribution nor the stationary distribution.2 If we let γ →1, then the choice of µ is unimportant and\nthe objective is still the average reward (see Section 3.2). For a ﬁxed γ < 1, on the other hand, we\nare maximizing the short-term future reward at a set of states that must somehow be communicated\nto the agent (just like the reward, this is additional information that the agent would not otherwise\nobserve). In other words, the agent is being evaluated not on the continuing MDP, but rather on\nanother MDP with the same state transition probabilities P but a different start state distribution µ.\nSuch an objective function could be useful in certain applications, but it represents an extension of\nthe RL framework (similarly to off-policy objectives) and is beyond the scope of this paper.\n3.1\nThe average reward objective in terms of discounted values\nInterestingly, the average reward ¯r(π) has several equivalent formulations. While we deﬁned it as an\naverage of the one-step reward weighted by the stationary distribution, it is equivalent to a weighted\naverage of the discounted values:\nÕ\ns∈S\ndπ(s) rπ(s) ≡(1 −γ)\nÕ\ns∈S\ndπ(s) vγ\nπ(s),\nfor all 0 ≤γ < 1.\n(3)\nIntuitively, if the initial state is sampled from dπ, then all future states will have the same distribution\nand the expected reward on every future time step will be ¯r(π). Effectively, the stationary distribution\nallows any weighted average over time (in this case, the normalized value function (1−γ)vγ\nπ(s)) to be\nreplaced with a weighted average over states [Singh et al., 1994; Sutton and Barto, 2018]. Indeed, the\n2White [2017] discusses weightings based on the stationary distribution but with an additional “interest\nfunction”. For the purposes of this paper, these are generalizations of average reward.\n4\nsame equivalence holds for any state-independent temporal discounting scheme (not just geometric)\nas long the discount factors sum to a constant; the (1 −γ) factor is replaced by the reciprocal of\nthat constant. This equivalence does not hold for hyperbolic discounting [Fedus et al., 2019], for\nexample, because the sum of discount factors diverges.\nTheoretically, the deﬁnition in (3) implies that the policy that maximizes the average of future dis-\ncounted values (weighted by the stationary distribution, but with any discount factor) also maximizes\naverage reward. The discount rate thus becomes a hyper-parameter of the algorithm, rather than a\nparameter specifying the optimization objective [Sutton and Barto, 2018].\nGreedily maximizing discounted future value does not maximize average reward\nAt this point, it would be easy to conclude that any algorithm that maximizes discounted value must\nalso maximize average reward regardless of the discount factor, which seems absurd on the face of\nit. We should indeed be skeptical — common algorithms like Q-learning or Sarsa estimate an action-\nvalue function Qγ(s, a) and behave (close to) greedily with respect to it. Such a local greediﬁcation\nwill not in general correspond to maximizing the average discounted value over time.\nAs an example, consider the two-choice MDP in Figure 1. State 0 is the only state with a choice\nbetween two actions: left and right. Choosing left leads to an immediate reward of +1, and right\nleads to a delayed reward of +2. For small discount factors, the immediate reward from going left\nappears much more appealing than the discounted delayed reward. When the discount factor is\nincreased sufﬁciently, the right action becomes more appealing instead. For algorithms based on the\nlocal greediﬁcation operator, the discount factor is not just a hyper-parameter of the algorithm — it\nactually changes the problem being solved.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n+1\nleft\nright\n+2\nFigure 1:\nThe two-choice MDP. This is a continuing MDP with only one action in every state\nexcept state 0. Action left in state 0 gives an immediate reward of +1 and action right leads to a\ndelayed reward of +2 after ﬁve time steps.\nActing greedily with respect to a (discounted) value function is ultimately a solution method, not\na problem deﬁnition. We have previously argued that the discounted objective does not deﬁne a\nmeaningful optimization problem under function approximation. Although we cannot rule out the\npossibility that discounting may form part of an algorithm for control in continuing tasks,3 the danger\nlies in naively transplanting algorithms from the episodic discounted setting to continuing tasks just\nbecause the discounted future rewards are ﬁnite in both cases. Discounting should not be the only\n(or even the ﬁrst) algorithmic approach we consider when we set out to solve continuing RL tasks.\n3.2\nIncreasing γ →1 does not solve the problem\nInspired by the previous example, one might consider using a discounted algorithm but taking the\nlimit as γ →1. Depending on how this limit is interpreted, however, it is either equivalent to the\naverage reward or algorithmically impractical.\nConsider an optimal policy arg maxπ limγ→1−(1 −γ)vγ\nπ(s0). This objective does indeed avoid the\nissues that come with a ﬁnite (discounted) time horizon. Furthermore, it is bounded, being the\nweighted average of bounded rewards. Unsurprisingly, Bishop et al. [2014] show that this quantity\nis equivalent to average reward; it is independent of the start state s0.\n3For example, an algorithm might estimate vγ\nπ and maximize the right-hand side of the equivalence (3).\nBecause of that equivalence, γ would simply be a tuning parameter of the algorithm. It would have no effect\non the objective being maximized, average reward. [Sutton and Barto, 2018, §10.4]\n5\nUnfortunately, that objective function contains a limit, making it hard to optimize in practice. In-\nstead, algorithms ﬁnd optimal policies for increasing discount factors — they exchange the maxi-\nmization with the limit — limγ→1−arg maxπ(1 −γ)vγ\nπ(s0) for some start state s0.\nIn theory, the limiting policy does indeed exist and maximizes average reward; it is called the\nBlackwell-optimal policy π∗. In fact, this policy satisﬁes the optimality criterion (1) for every state\ns as long as γ ≥γ∗, a critical discount rate that depends on the particular MDP. Intuitively, in\nany given MDP, making a decision that is optimal over a sufﬁciently large horizon is equivalent to\noptimizing average reward. Crucially, this horizon depends on the MDP and how long it takes for\nactions to have consequences (in terms of reward), which is not known beforehand. The critical\ndiscount rate is related to the maximum mixing time of the MDP. [Puterman, 1994]\nThere are two major obstacles to exploiting this theory to use discounted-value algorithms as\naverage-value algorithms for real-world RL problems. First, most algorithms that learn discounted\nvalue functions become increasingly unstable as γ increases to 1. Second, it is hard to estimate the\ncritical discount factor, because it is intimately tied to the unknown dynamics of the environment.\nDenis [2019] discusses these issues more thoroughly. Thus, to ensure we ﬁnd an optimal policy we\ncannot settle for any ﬁxed γ < 1, forcing us into the γ ≈1 regime where our algorithms become\nunusable. Algorithms that optimize the average reward objective do not rely on a discount factor\nand automatically make decisions at the appropriate time horizon.\n3.3\nMaximizing average reward is algorithmically feasible\nThe average reward formulation has been long studied, and there are several dynamic programming\nalgorithms for ﬁnding optimal average reward policies; see Puterman [1994] and Bertsekas [2005].\nSchwartz [1993] proposed the ﬁrst reinforcement learning algorithm to maximize the undiscounted\naverage reward, variants of which were reviewed by Mahadevan [1996]. Of particular interest is\nRVI Q-learning [Abounadi et al., 2001], a simple Q-learning-like algorithm with convergence guar-\nantees. Furthermore, policy gradient methods are especially promising since the gradient of the\naverage reward objective has an elegant closed-form expression [Sutton et al., 2000]. Konda and\nTsitsiklis [1999] have proposed an actor-critic method for average reward.\n4\nConclusions\nDiscounting in continuing reinforcement learning tasks raises serious conceptual problems that be-\ncome especially acute in the presence of function approximation. The key messages from this paper\nare as follows:\n• Discounted reinforcement learning is not an optimization problem. In continuing tasks, the\nusual formulation does not correspond to the maximization of any objective function over a set\nof policies.\n• Function approximation is therefore incompatible with discounting. Not being an optimiza-\ntion problem, the best representable policy is not well-deﬁned in continuing tasks.\n• Average reward reinforcement learning is an optimization problem. There is always an\noptimal representable policy even with function approximation.\n• Greedily maximizing discounted future value does not maximize average reward. Common\nalgorithms like Sarsa or Q-learning do not optimize the average reward, and they ﬁnd different\npolicies depending on the discount factor.\n• Increasing γ →1 does not solve the problem. The discount factor must be increased beyond\na critical point which is problem-speciﬁc and cannot be determined a priori. Moreover, most\nalgorithms become increasingly unstable as γ approaches unity.\nThere are many open problems in average reward RL. While several relatively simple convergent\nalgorithms exist, further advances are needed in fundamental model-free and model-based learning\nas well as topics like multi-step learning, off-policy learning, and options. Many of the recent\nadvances in optimization can be applied to improve algorithms for average reward.\n6\nAcknowledgements\nThe authors were supported by the Natural Sciences and Engineering Research Council of Canada\n(NSERC), Alberta Innovates, and DeepMind. The authors also wish to thank Muhammad Zaheer,\nKirby Banman, Samuel Sokota, and Martha White for valuable feedback on earlier drafts of the\nwork.\nReferences\nAbounadi, J., Bertsekas, D. P., and Borkar, V. S. (2001). Learning algorithms for Markov deci-\nsion processes with average cost. SIAM Journal on Control and Optimization, 40(3):681–698.\ndoi:10.1137/S0363012999361974.\nBellman, R. and Dreyfus, S. (1959). Function approximations and dynamic programming. Mathe-\nmatical Tables and Other Aids to Computation, 13(68):247–251. doi:10.2307/2002797.\nBertsekas, D. P. (2005). Dynamic Programming and Optimal Control, vol. II. Athena Scientiﬁc.\nBishop, C. J., Feinberg, E. A., and Zhang, J. (2014).\nExamples concerning Abel and\nCesàro limits.\nJournal of Mathematical Analysis and Applications, 420(2):1654–1661.\ndoi:10.1016/j.jmaa.2014.06.017.\nDenis, N. (2019). Issues concerning realizability of Blackwell optimal policies in reinforcement\nlearning. arXiv e-prints. 1905.08293.\nFedus, W., Gelada, C., Bengio, Y., Bellemare, M. G., and Larochelle, H. (2019). Hyperbolic dis-\ncounting and learning over multiple horizons. arXiv e-prints. 1902.06865.\nKonda, V. R. and Tsitsiklis, J. N. (1999). Actor-critic algorithms. In NeurIPS.\nURL http://papers.neurips.cc/paper/1786-actor-critic-algorithms/\nMahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and em-\npirical results. Machine Learning, 22(1-3):159–195. doi:10.1007/BF00114727.\nPuterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.\nJohn Wiley & Sons, Inc., New York, NY, USA, 1st ed.\nSchwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In\nICML. doi:10.1016/B978-1-55860-307-3.50045-9.\nSingh, S. P., Jaakkola, T. S., and Jordan, M. I. (1994). Learning without state-estimation in partially\nobservable Markovian decision processes. In ICML. doi:10.1016/B978-1-55860-335-6.50042-8.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press, 2nd\ned.\nSutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000). Policy gradient methods for\nreinforcement learning with function approximation. In NeurIPS.\nURL\nhttp://papers.neurips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-\nfunction-approximation/\nWhite, M. (2017). Unifying task speciﬁcation in reinforcement learning. In ICML.\nURL http://proceedings.mlr.press/v70/white17a.html\n7\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2019-10-04",
  "updated": "2019-11-27"
}