{
  "id": "http://arxiv.org/abs/1910.03016v4",
  "title": "Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?",
  "authors": [
    "Simon S. Du",
    "Sham M. Kakade",
    "Ruosong Wang",
    "Lin F. Yang"
  ],
  "abstract": "Modern deep learning methods provide effective means to learn good\nrepresentations. However, is a good representation itself sufficient for sample\nefficient reinforcement learning? This question has largely been studied only\nwith respect to (worst-case) approximation error, in the more classical\napproximate dynamic programming literature. With regards to the statistical\nviewpoint, this question is largely unexplored, and the extant body of\nliterature mainly focuses on conditions which permit sample efficient\nreinforcement learning with little understanding of what are necessary\nconditions for efficient reinforcement learning.\n  This work shows that, from the statistical viewpoint, the situation is far\nsubtler than suggested by the more traditional approximation viewpoint, where\nthe requirements on the representation that suffice for sample efficient RL are\neven more stringent. Our main results provide sharp thresholds for\nreinforcement learning methods, showing that there are hard limitations on what\nconstitutes good function approximation (in terms of the dimensionality of the\nrepresentation), where we focus on natural representational conditions relevant\nto value-based, model-based, and policy-based learning. These lower bounds\nhighlight that having a good (value-based, model-based, or policy-based)\nrepresentation in and of itself is insufficient for efficient reinforcement\nlearning, unless the quality of this approximation passes certain hard\nthresholds. Furthermore, our lower bounds also imply exponential separations on\nthe sample complexity between 1) value-based learning with perfect\nrepresentation and value-based learning with a good-but-not-perfect\nrepresentation, 2) value-based learning and policy-based learning, 3)\npolicy-based learning and supervised learning and 4) reinforcement learning and\nimitation learning.",
  "text": "Published as a conference paper at ICLR 2020\nIS A GOOD REPRESENTATION SUFFICIENT FOR SAM-\nPLE EFFICIENT REINFORCEMENT LEARNING?\nSimon S. Du\nInstitute for Advanced Study\nssdu@ias.edu\nSham M. Kakade\nUniversity of Washington, Seattle\nsham@cs.washington.edu\nRuosong Wang\nCarnegie Mellon University\nruosongw@andrew.cmu.edu\nLin F. Yang\nUniversity of California, Los Angles\nlinyang@ee.ucla.edu\nABSTRACT\nModern deep learning methods provide effective means to learn good represen-\ntations. However, is a good representation itself sufﬁcient for sample efﬁcient\nreinforcement learning? This question has largely been studied only with respect\nto (worst-case) approximation error, in the more classical approximate dynamic\nprogramming literature. With regards to the statistical viewpoint, this question is\nlargely unexplored, and the extant body of literature mainly focuses on conditions\nwhich permit sample efﬁcient reinforcement learning with little understanding of\nwhat are necessary conditions for efﬁcient reinforcement learning.\nThis work shows that, from the statistical viewpoint, the situation is far subtler\nthan suggested by the more traditional approximation viewpoint, where the re-\nquirements on the representation that sufﬁce for sample efﬁcient RL are even more\nstringent. Our main results provide sharp thresholds for reinforcement learning\nmethods, showing that there are hard limitations on what constitutes good function\napproximation (in terms of the dimensionality of the representation), where we\nfocus on natural representational conditions relevant to value-based, model-based,\nand policy-based learning. These lower bounds highlight that having a good (value-\nbased, model-based, or policy-based) representation in and of itself is insufﬁcient\nfor efﬁcient reinforcement learning, unless the quality of this approximation passes\ncertain hard thresholds. Furthermore, our lower bounds also imply exponential\nseparations on the sample complexity between 1) value-based learning with perfect\nrepresentation and value-based learning with a good-but-not-perfect representation,\n2) value-based learning and policy-based learning, 3) policy-based learning and\nsupervised learning and 4) reinforcement learning and imitation learning.\n1\nINTRODUCTION\nModern reinforcement learning (RL) problems are often challenging due to the huge state space.\nTo tackle this challenge, function approximation schemes are often employed to provide a compact\nrepresentation, so that reinforcement learning can generalize across states. A common paradigm\nis to ﬁrst use a feature extractor to transform the raw input to features (a succinct representation)\nand then apply a linear predictor on top of the features. Traditionally, the feature extractor is often\nhandcrafted (Sutton & Barto, 2018), while more modern methods often train a deep neural network\nto extract features. The hope of this paradigm is that, if there exists a good low dimensional (linear)\nrepresentation, then efﬁcient reinforcement learning is possible.\nEmpirically, combining various RL function approximation algorithms with neural networks for\nfeature extraction has lead to tremendous successes on various tasks (Mnih et al., 2015; Schulman\net al., 2015; 2017). A major problem, however, is that these methods often require a large amount of\nsamples to learn a good policy. For example, deep Q-network requires millions of samples to solve\ncertain Atari games (Mnih et al., 2015). Here, one may wonder if there are fundamental statistical\n1\narXiv:1910.03016v4  [cs.LG]  28 Feb 2020\nPublished as a conference paper at ICLR 2020\nlimitations on such methods, and, if so, under what conditions it would be possible to efﬁciently learn\na good policy?\nIn the supervised learning context, it is well-known that empirical risk minimization is a statistically\nefﬁcient method when using a low-complexity hypothesis space (Shalev-Shwartz & Ben-David,\n2014), e.g. a hypothesis space with bounded VC dimension. For example, polynomial number\nof samples sufﬁce for learning a near-optimal d-dimensional linear classiﬁer, even in the agnostic\nsetting1. In contrast, in the more challenging RL setting, we seek to understand if efﬁcient learning\nis possible (say from a sample complexity perspective) when we have access to an accurate (and\ncompact) parametric representation — e.g. our policy class contains a near-optimal policy or our\nhypothesis class accurately approximates the optimal value function. In particular, this work focuses\non the following question:\nIs a good representation sufﬁcient for sample-efﬁcient reinforcement learning?\nThis question has largely been studied only with respect to approximation error in the more classical\napproximate dynamic programming literature, where it is known that algorithms are stable to certain\nworst-case approximation errors. With regards to sample efﬁciency, this question is largely unexplored,\nwhere the extant body of literature mainly focuses on conditions which are sufﬁcient for efﬁcient\nreinforcement learning though there is little understanding of what are necessary conditions for\nefﬁcient reinforcement learning. In reinforcement learning, there is no direct analogue of empirical\nrisk minimization as in the supervised learning context, and it is not evident what are the statistical\nlimits of learning based on properties of our underlying hypothesis class (which may be value-based,\npolicy-based, or model-based).\nMany recent works have provided polynomial upper bounds under various sufﬁcient conditions, and\nin what follows we list a few examples. For value-based learning, the work of Wen & Van Roy (2013)\nshowed that for deterministic systems2, if the optimal Q-function can be perfectly predicted by linear\nfunctions of the given features, then the agent can learn the optimal policy exactly with polynomial\nnumber of samples. Recent work (Jiang et al., 2017) further showed that if certain complexity\nmeasure called Bellman rank is bounded, then the agent can learn a near-optimal policy efﬁciently.\nFor policy-based learning, Agarwal et al. (2019) gave polynomial upper bounds which depend on a\nparameter that measures the difference between the initial distribution and the distribution induced by\nthe optimal policy.\nOur Contributions. This paper gives, perhaps surprisingly, strong negative results to this question.\nThe main results are exponential lower bounds in terms of planning horizon H for value-based, model-\nbased, and policy-based algorithms with given good representations3. Notably, the requirements\non the representation that sufﬁce for sample efﬁcient RL are even more stringent than the more\ntraditional approximation viewpoint. A comprehensive summary of previous upper bounds and our\nlower bounds is given in Table 1, and here we brieﬂy summarize our hardness results.\n1. For value-based learning, we show even if Q-functions of all policies can be approximated\nby linear functions of the given representation with approximation error δ = Ω\n\u0010q\nH\nd\n\u0011\nwhere d is the dimension of the representation and H is the planning horizon, then the agent\nstill needs to sample exponential number of trajectories to ﬁnd a near-optimal policy.\n2. For model-based learning, we show even if the transition matrix and the reward function\ncan be approximated by linear functions of the given representation with approximation\nerror δ = Ω\n\u0010q\nH\nd\n\u0011\n(in ℓ∞sense), the agent still needs to sample exponential number of\ntrajectories to ﬁnd a near-optimal policy.\n3. We show even if optimal policy can be perfectly predicted by a linear function of the given\nrepresentation with a strictly positive margin, the agent still requires exponential number of\ntrajectories to ﬁnd a near-optimal policy.\n1Here we only study the sample complexity and ignore the computational complexity.\n2MDPs where both reward and transition are deterministic.\n3 Our results can be easily extend to inﬁnite horizon MDPs with discount factors by replacing the planning\nhorizon H with\n1\n1−γ , where γ is the discount factor. We omit the discussion on discount MDPs for simplicity.\n2\nPublished as a conference paper at ICLR 2020\nThese lower bounds hold even in deterministic systems and even if the agent knows the transition\nmodel. Note these negative results apply to the case where the Q-function, the model, or the optimal\npolicy can be predicted well by a linear function of the given representation. Since the class of linear\nfunctions is a strict subset of many more complicated function classes, including neural networks in\nparticular, our negative results imply lower bounds for these more complex function classes as well.\nOur results highlight the following conceptual insights:\n• The requirements on the representation that sufﬁce for sample efﬁcient RL are signiﬁcantly\nmore stringent than the more traditional approximation viewpoint; our statistical lower\nbounds show that there are hard thresholds on the worst-case approximation quality of the\nrepresentation which are not necessary from the approximation viewpoint.\n• Since our lower bounds apply even when the agent knows the transition model, the hardness\nis not due to the difﬁculty of exploration in the standard sense. The unknown reward function\nis sufﬁcient to make the problem exponentially difﬁcult.\n• Our lower bounds are not due to the agent’s inability to perform efﬁcient supervised learning,\nsince our assumptions do admit polynomial sample complexity upper bounds if the data\ndistribution is ﬁxed.\n• Our lower bounds are not pathological in nature and suggest that these concerns may arise\nin practice. In a precise sense, almost all feature extractors induce a hard MDP instance in\nour construction (see Section 4.4).\nInstead, one interpretation is that the hardness is due to a distribution mismatch in the following sense:\nthe agent does not know which distribution to use for minimizing a (supervised) learning error (see\nKakade (2003) for discussion), and even a known transition model is not information-theoretically\nsufﬁcient to reduce the sample complexity.\nFurthermore, our work implies several interesting exponential separations on the sample complexity\nbetween: 1) value-based learning with perfect representation and value-based learning with a good-\nbut-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based\nlearning and supervised learning and 4) reinforcement learning and imitation learning. We provide\nmore details in Section 5.\n2\nRELATED WORK\nA summary of previous upper bounds, together with lower bounds proved in this paper, is provided\nin Table 1. Some key assumptions are formally stated in Section 3 and Section 4. Our lower bounds\nhighlight that classical complexity measures in supervised learning including small approximation\nerror and margin, and standard assumptions in reinforcement learning including optimality gap\nand deterministic systems, are not enough for efﬁcient RL with function approximation. We need\nadditional assumptions, e.g., ones used in previous upper bounds, for efﬁcient RL.\n2.1\nPREVIOUS LOWER BOUNDS\nExisting exponential lower bounds, to our knowledge, construct unstructured MDPs with an ex-\nponentially large state space and reduce a bandit problem with exponentially many arms to an\nMDP (Krishnamurthy et al., 2016; Sun et al., 2017). However, these lower bounds cannot apply to\nMDPs whose transition models, value functions, or policies can be approximated with some natural\nfunction classes, e.g., linear functions, neural networks, etc. The current paper gives the ﬁrst set of\nlower bounds for RL with linear function approximation (and thus also hold for super classes of\nlinear functions such as neural networks).\n2.2\nPREVIOUS UPPER BOUNDS\nWe divide previous algorithms (with provable guarantees) into three classes: those that utilize\nuncertainty-based bonuses (e.g. UCB variants or Thompson sampling variants); approximate dynamic\nprogramming variants (which often make assumptions with respect to concentrability coefﬁcients);\nand direct policy search-based methods (such as conserve policy iteration (CPI, see Kakade (2003)) or\npolicy gradient methods, which make assumptions with respect to distribution mismatch coefﬁcients).\n3\nPublished as a conference paper at ICLR 2020\nQuery Oracle\nRL\nGenerative Model\nKnown Transition\nPrevious Upper Bounds\nExact linear Q∗+ DetMDP (Wen & Van Roy, 2013)\n✓\n✓\n✓\nExact linear Q∗+ Bellman-Rank (Jiang et al., 2017)\n✓\n✓\n✓\nExact Linear Q∗+ Low Var + Gap (Du et al., 2019a)\n✓\n✓\n✓\nExact Linear Q∗+ Gap (Open Problem / Theorem C.1)\n?\n✓\n✓\nExact Linear Qπ for all π (Open Problem / Theorem D.1)\n?\n✓\n✓\nApprox. Linear Qπ for all π +\nConcentratability (Munos, 2005; Antos et al., 2008)\n✓×\n✓\n✓\nApprox. Linear Qπ for all π +\nBounded Dist Mismatch Coeff (Kakade & Langford, 2002)\n✓×\n✓\n✓\nLower Bounds (this work)\nApprox Linear Q∗(Theorem 4.1)\n×\n×\n×\nApprox Linear Qπ for all π (Theorem 4.1)\n×\n×\n×\nℓ∞Approx Linear MDP (Theorem 4.2)\n×\n×\n×\nExact Linear π∗+ Margin + Gap + DetMDP (Theorem 4.3)\n×\n×\n×\nExact Linear Q∗(Open Problem)\n?\n?\n?\nTable 1: Summary of theoretical results on reinforcement learning with linear function approximation.\nSee Section 2 for discussion on this table. RL, Generative Model, Known Transition are deﬁned\nin Section 3.3. Exact linear Q∗: Assumption 4.1 with δ = 0. Approx linear Q∗: Assumption 4.1\nwith δ = Ω\n\u0010q\nH\nd\n\u0011\n. Exact linear π∗: Assumption 4.4. Margin: Assumption 4.5. Exact Linear Qπ\nfor all π: Assumption 4.2 with δ = 0. Approximate Linear Qπ for all π: Assumption 4.2 with\nδ = Ω\n\u0010q\nH\nd\n\u0011\n. DetMDP: deterministic system deﬁned in Section 3.1. Bellman-rank: Deﬁnition 5 in\nJiang et al. (2017). Low Var: Assumption 1 in Du et al. (2019b). Gap: Assumption 3.1. Bounded\nDistribution Mismatch Coefﬁcient: Deﬁnition 3.3 in Agarwal et al. (2019). ℓ∞Approx Linear\nMDP: Assumption 4.3 with δ = Ω\n\u0010q\nH\nd\n\u0011\n. ✓: there exists an algorithm with polynomial sample\ncomplexity to ﬁnd a near-optimal policy. ✓×: requires certain condition on the initial distribution. ×:\nexponential number of samples is required. ?: open problem.\n4\nPublished as a conference paper at ICLR 2020\nThe ﬁrst class of methods include those based on witness rank, Belman rank, and the Eluder dimension,\nwhile the latter two classes of algorithms make assumptions either on concentrability coefﬁcients or\non distribution mismatch coefﬁcients (see Agarwal et al. (2019); Scherrer (2014) for discussions).\nUncertainty bonus-based algorithms. Now we discuss existing theoretical results on value-based\nlearning with function approximation. The most relevant work is Wen & Van Roy (2013) which\nshowed in deterministic systems, if the optimal Q-function is within a pre-speciﬁed function class\nwhich has bounded Eluder dimension, for which the class of linear functions is a special case, then the\nagent can learn the optimal policy using polynomial number of samples. This result has recently been\ngeneralized by Du et al. (2019a) which can deal with stochastic reward and low variance transition\nbut requires strictly positive optimality gap. As we listed in Table 1, it is an open problem whether\nthe condition that the optimal Q-function is linear itself is sufﬁcient for efﬁcient RL.\nLi et al. (2011) proposed a Q-learning algorithm which requires the Know-What-It-Knows oracle.\nHowever, it is in general unknown how to implement such oracle in practice. Jiang et al. (2017)\nproposed the concept of Bellman Rank to characterize the sample complexity of value-based learning\nmethods and gave an algorithm that has polynomial sample complexity in terms of the Bellman Rank,\nthough the proposed algorithm is not computationally efﬁcient. Bellman rank is bounded for a wide\nrange of problems, including MDP with small number of hidden states, linear MDP, LQR, etc. Later\nwork gave computationally efﬁcient algorithms for certain special cases (Dann et al., 2018; Du et al.,\n2019a; Yang & Wang, 2019b; Jin et al., 2019). Recently, Witness rank, a generalization of Bellman\nrank to model-based methods, is studied in Sun et al. (2019).\nApproximate dynamic programming-based algorithms. We now discuss approximate dynamic\nprogramming-based results characterized in terms of the concentrability coefﬁcient. While classical\napproximate dynamic programming results typically require ℓ∞-bounded errors, the notion of\nconcentrability (originally due to (Munos, 2005)) permits sharper bounds in terms of average-case\nfunction approximation error, provided that the concentrability coefﬁcient is bounded (e.g. see\nMunos (2005); Szepesv´ari & Munos (2005); Antos et al. (2008); Geist et al. (2019)). Under the\nassumption that this problem-dependent parameter is bounded, Munos (2005); Szepesv´ari & Munos\n(2005) and Antos et al. (2008) proved sample complexity and error bounds for approximate dynamic\nprogramming methods when there is a data collection policy (under which value-function ﬁtting\noccurs) that induces a ﬁnite concentrability coefﬁcient. The assumption that the concentrability\ncoefﬁcient is ﬁnite is in fact quite limiting. See Chen & Jiang (2019) which provides a more detailed\ndiscussion on this quantity.\nDirect policy search-based algorithms.\nStronger guarantees over approximate dynamic\nprogramming-based algrithm can be obtained with direct policy search-based methods, where instead\nof having a bounded concentrability coefﬁcient, one only needs to have a bounded distribution\nmismatch coefﬁcient. The latter assumption requires the agent to have access to a “good” initial state\ndistribution (e.g. a measure which has coverage over where an optimal policy tends to visit); note\nthat this assumption does not make restrictions over the class of MDPs. There are two classes of algo-\nrithms that fall into this category. First, there is Conservative Policy Iteration (Kakade & Langford,\n2002), along with Policy Search by Dynamic Programming (PSDP) (Bagnell et al., 2004), and other\nboosting-style of policy search-based methods Scherrer & Geist (2014); Scherrer (2014), which have\nguarantees in terms of bounded distribution mismatch ratio. Second, more recently, Agarwal et al.\n(2019) showed that policy gradient styles of algorithms also have comparable guarantees.\nRecent extensions. Subsequent to this work, the work by Van Roy & Dong (2019) and Lattimore &\nSzepesvari (2019) made notable contributions to the misspeciﬁed linear bandit problem. In particular,\nboth papers found that Theorem 4.1 in our paper can be extended to the misspeciﬁed linear bandit\nproblem and gave upper bounds for this problem showing that our lower bound has tight dependency\non δ and d. Lattimore & Szepesvari (2019) further gave an upper bound for the setting where the\nQ-functions of all policies can be approximated by linear functions with small approximation errors\nand the agent can interact with the environment using a generative model. This upper bound also\ndemonstrates that our lower bound has tight dependency on δ and d.\n3\nPRELIMINARIES\nThroughout this paper, for a given integer H, we use [H] to denote the set {0, 1, . . . , H −1}.\n5\nPublished as a conference paper at ICLR 2020\n3.1\nEPISODIC REINFORCEMENT LEARNING\nLet M = (S, A, H, P, R) be an Markov Decision Process (MDP) where S is the state space,\nA is the action space whose size is bounded by a constant, H ∈Z+ is the planning horizon,\nP : S ×A →△(S) is the transition function which takes a state-action pair and returns a distribution\nover states and R : S × A →△(R) is the reward distribution. Without loss of generality, we assume\na ﬁxed initial state s04. A policy π : S →△(A) prescribes a distribution over actions for each\nstate. The policy π induces a (random) trajectory s0, a0, r0, s1, a1, r1, . . . , sH−1, aH−1, rH−1 where\na0 ∼π(s0), r0 ∼R(s0, a0), s1 ∼P(s0, a0), a1 ∼π(s1), etc. To streamline our analysis, for each\nh ∈[H], we use Sh ⊆S to denote the set of states at level h, and we assume Sh do not intersect\nwith each other. We also assume PH−1\nh=0 rh ∈[0, 1] almost surely. Our goal is to ﬁnd a policy π that\nmaximizes the expected total reward E\nhPH−1\nh=0 rh | π\ni\n. We use π∗to denote the optimal policy. We\nsay a policy π is ε-optimal if E\nhPH−1\nh=0 rh | π\ni\n≥E\nhPH−1\nh=0 rh | π∗i\n−ε.\nIn this paper we prove lower bounds for deterministic systems, i.e., MDPs with deterministic\ntransition P, deterministic reward R. In this setting, P and R can be regarded as functions instead\nof distributions. Since deterministic systems are special cases of general stochastic MDPs, lower\nbounds proved in this paper still hold for more general MDPs.\n3.2\nQ-FUNCTION AND OPTIMALITY GAP\nAn important concept in RL is the Q-function. Given a policy π, a level h ∈[H] and a state-action\npair (s, a) ∈Sh × A, the Q-function is deﬁned as Qπ\nh(s, a) = E\nhPH−1\nh′=h rh′ | sh = s, ah = a, π\ni\n.\nFor simplicity, we denote Q∗\nh(s, a) = Qπ∗\nh (s, a). In addition to these deﬁnitions, we list below an\nimportant assumption, the optimality gap assumption, which is widely used in reinforcement learning\nand bandit literature. To state the assumption, we ﬁrst deﬁne the function gap : S × A →R as\ngap(s, a) = arg maxa′∈A Q∗(s, a′) −Q∗(s, a). Now we formally state the assumption.\nAssumption 3.1 (Optimality Gap). There exists ρ > 0 such that ρ ≤gap(s, a) for all (s, a) ∈S ×A\nwith gap(s, a) > 0.\nHere, ρ is the smallest reward-to-go difference between the best set of actions and the rest. Recently,\nDu et al. (2019b) gave a provably efﬁcient Q-learning algorithm based on this assumption and\nSimchowitz & Jamieson (2019) showed that with this condition, the agent only incurs logarithmic\nregret in the tabular setting.\n3.3\nQUERY MODELS\nHere we discuss three possible query oracles interacting with the MDP.\n• RL: The most basic and weakest query oracle for MDP is the standard reinforcement\nlearning query oracle where the agent can only interact with the MDP by choosing actions\nand observe the next state and the reward.\n• Generative Model: A stronger query model assumes the agent can transit to any\nstate (Kearns & Singh, 2002; Kakade, 2003; Sidford et al., 2018). This query model\nis available in certain robotic applications where one can control the robot to reach the target\nstate.\n• Known Transition: The strongest query model considered is that the agent can not only\ntransit to any state, but also knows the whole transition function. In this model, only the\nreward is unknown.\nIn this paper, we will prove lower bounds for the strongest Known Transition query oracle. Therefore,\nour lower bounds also apply to RL and Generative Model query oracles.\n4Some papers assume the initial state is sampled from a distribution P1. Note this is equivalent to assuming\na ﬁxed initial state s0, by setting P(s0, a) = P1 for all a ∈A and now our state s1 is equivalent to the initial\nstate in their assumption.\n6\nPublished as a conference paper at ICLR 2020\n4\nMAIN RESULTS\nIn this section we formally present our lower bounds. We also discuss proof ideas in Section 4.4.\n4.1\nLOWER BOUND FOR VALUE-BASED LEARNING\nWe ﬁrst present our lower bound for value-based learning. A common assumption is that the Q-\nfunction can be predicted well by a linear function of the given features (representation) (Bertsekas\n& Tsitsiklis, 1996). Formally, the agent is given a feature extractor φ : S × A →Rd which can be\nhand-crafted or a pre-trained neural network that transforms a state-action pair to a d-dimensional\nembedding. The following assumption states that the given feature extractor can be used to predict\nthe Q-function with approximation error at most δ using a linear function.\nAssumption 4.1. There exists δ > 0 and θ0, θ1, . . . , θH−1 ∈Rd such that for any h ∈[H] and any\n(s, a) ∈Sh × A, |Q∗\nh (s, a) −⟨θh, φ (s, a)⟩| ≤δ.\nHere δ is the approximation error, which indicates the quality of the representation. If δ = 0, then\nQ-function can be perfectly predicted by a linear function of φ (·, ·). In general, δ becomes smaller as\nwe increase the dimension of φ, since larger dimension usually has more expressive power. When the\nfeature extractor is strong enough, previous papers (Chen & Jiang, 2019; Farahmand, 2011) assume\nthat linear functions of φ can approximate the Q-function of any policy.\nAssumption 4.2 (Policy Completeness). There exists δ > 0, such that for any h ∈[H] and any\npolicy π, there exists θπ\nh ∈Rd such that for any (s, a) ∈Sh × A, |Qπ\nh (s, a) −⟨θh, φ (s, a)⟩| ≤δ.\nIn the theoretical reinforcement learning literature, Assumption 4.2 is often called the (approximate)\npolicy completeness assumption. This assumption is crucial in proving polynomial sample complexity\nguarantee for value iteration type of algorithms (Chen & Jiang, 2019; Farahmand, 2011).\nThe following theorem shows when δ = Ω\n\u0010q\nH\nd\n\u0011\n, the agent needs to sample exponential number of\ntrajectories to ﬁnd a near-optimal policy.\nTheorem 4.1 (Exponential Lower Bound for Value-based Learning). There exists a family of MDPs\nwith |A| = 2 and a feature extractor φ that satisfy Assumption 4.2, such that any algorithm that\nreturns a 1/2-optimal policy with probability 0.9 needs to sample Ω\n\u0000min{|S|, 2H, exp(dδ2/16)}\n\u0001\ntrajectories.\nNote this lower bound also applies to MDPs that satisfy Assumption 4.1, since Assumption 4.2\nis strictly stronger. We would like to emphasize that since linear functions is a subclass of more\ncomplicated function classes, e.g., neural networks, our lower bound also holds for these function\nclasses. Moreover, in many scenarios, the feature extractor φ is the last layer of a neural network.\nModern neural networks are often over-parameterized, which makes d large. In this case, d is much\nlarger than H. Thus, our lower bound holds even if the representation has small approximation\nerror. Furthermore, the assumption that |A| = 2 is only for simplicity. Our lower bound can be\neasily generalized to the case that |A| > 2, in which case the sample complexity lower bound is\nΩ\n\u0000min{|S|, |A|H, exp(dδ2/16)}\n\u0001\n.\n4.2\nLOWER BOUND FOR MODEL-BASED LEARNING\nHere we present our lower bound for model-based learning. Recently, Yang & Wang (2019b)\nproposed the linear transition assumption which was later studied in Yang & Wang (2019a); Jin et al.\n(2019). Again, we assume the agent is given a feature extractor φ : S × A →Rd, and now we state\nthe assumption formally as follow.\nAssumption 4.3 (Approximate Linear MDP). There exists δ > 0, β0, β1, . . . , βH−1 ∈Rd\nand ψ : S →Rd such that for any h ∈[H −1], (s, a) ∈Sh × A and s′ ∈Sh+1,\n|P (s′ | s, a) −⟨ψ(s′), φ (s, a)⟩| ≤δ and |E[R(s, a)] −⟨βh, φ(s, a)⟩| ≤δ.\nIt has been shown in Yang & Wang (2019b;a); Jin et al. (2019) if ∥P (· | s, a) −⟨ψ(·), φ (s, a)⟩∥1 is\nbounded, then the problem admits an algorithm with polynomial sample complexity. Now we show\nthat when δ = Ω\n\u0010q\nH\nd\n\u0011\nin Assumption 4.3, the agent needs exponential number of samples to ﬁnd\na near-optimal policy.\n7\nPublished as a conference paper at ICLR 2020\nTheorem 4.2 (Exponential Lower Bound for Linear Transition Model). There exists a family of\nMDPs with |A| = 2 and a feature extractor φ that satisfy Assumption 4.3, such that any algorithm that\nreturns a 1/2-optimal policy with probability 0.9 needs to sample Ω\n\u0000min{|S|, 2H, exp(dδ2/16)}\n\u0001\ntrajectories.\nAgain, our lower bound can be easily generalized to the case that |A| > 2.\nWe do note that an ℓ∞approximation for a transition matrix may be a weak condition. Under the\nstronger condition that the transition matrix can be approximated well under the total variational\ndistance, there exists polynomial sample complexity upper bounds that can tolerate approximation\nerrors (Yang & Wang, 2019b;a; Jin et al., 2019).\n4.3\nLOWER BOUND FOR POLICY-BASED LEARNING\nNext we present our lower bound for policy-based learning. This class of methods use function\napproximation on the policy and use optimization techniques, e.g., policy gradient, to ﬁnd the optimal\npolicy. In this paper, we focus on linear policies on top of a given representation. A linear policy\nπ is a policy of the form π(sh) = arg maxa∈A ⟨θh, φ(sh, a)⟩where sh ∈Sh, φ (·, ·) is a given\nfeature extractor and θh ∈Rd is the linear coefﬁcient. Note that applying policy gradient on softmax\nparameterization of the policy is indeed trying to ﬁnd the optimal policy among linear policies.\nSimilar to value-based learning, a natural assumption for policy-based learning is that the optimal\npolicy is realizable5, i.e., the optimal policy is linear.\nAssumption 4.4. For any h ∈[H], there exists θh ∈Rd that satisﬁes for any s ∈Sh, we have\nπ∗(s) ∈arg maxa ⟨θh, φ (s, a)⟩.\nHere we discuss another assumption. For learning a linear classiﬁer in the supervised learning setting,\none can reduce the sample complexity signiﬁcantly if the optimal linear classiﬁer has a margin.\nAssumption 4.5. We assume φ (s, a) ∈Rd satisﬁes ∥φ(s, a)∥2 = 1 for any (s, a) ∈S × A. For any\nh ∈[H], there exists θh ∈Rd with ∥θh∥2 = 1 and △> 0 such that for any s ∈Sh, there is a unique\noptimal action π∗(s), and for any a ̸= π∗(s), ⟨θh, φ (s, π∗(s))⟩−⟨θh, φ (s, a)⟩≥△.\nHere we restrict the linear coefﬁcients and features to have unit norm for normalization. Note that\nAssumption 4.5 is strictly stronger than Assumption 4.4. Now we present our result for linear policy.\nTheorem 4.3 (Exponential Lower Bound for Policy-based Learning). There exists an absolute\nconstant △0, such that for any △≤△0, there exists a family of MDPs with |A| = 2 and a\nfeature extractor φ that satisfy Assumption 3.1 with ρ =\n1\n2 min{H,d} and Assumption 4.5, such\nthat any algorithm that returns a 1/4-optimal policy with probability at least 0.9 needs to sample\nΩ\n\u0000min{2H, 2d}\n\u0001\ntrajectories.\nAgain, our lower bound can be easily generalized to the case that |A| > 2.\nCompared with Theorem 4.1, Theorem 4.3 is even more pessimistic, in the sense that even with\nperfect representation with benign properties (gap and margin), the agent still needs to sample\nexponential number of samples. It also suggests that policy-based learning could be very different\nfrom supervised learning.\n4.4\nPROOF IDEAS\nThe binary tree hard instance.\nAll our lower bound are proved based on reductions from the\nfollowing hard instance. In this instance, both the transition P and the reward R are deterministic.\nThere are H levels of states, which form a full binary tree of depth H. There are 2h states in level h,\nand thus 2H −1 states in total. Among all the 2H−1 states in level H −1, there is only one state with\nreward R = 1, and for all other states in the MDP, the corresponding reward value R = 0. Intuitively,\nto ﬁnd a 1/2-optimal policy for such MDPs, the agent must enumerate all possible states in level\nH −1 to ﬁnd the state with reward R = 1. Doing so intrinsically induces a sample complexity of\nΩ(2H). This intuition is formalized in Theorem A.1 using Yao’s minimax principle (Yao, 1977).\n5 Unlike value-based learning, it is hard to deﬁne completeness on the policy-based learning with function\napproximation, since not all policy has the arg max form.\n8\nPublished as a conference paper at ICLR 2020\nLower bound for value-based and model-based learning\nWe now show how to construct a set of\nfeatures so that Assumption 4.1-4.3 hold. Our main idea is to the utilize the following fact regarding\nthe identity matrix: ε-rank(I2H) ≤O(H/ε2). Here for a matrix A ∈Rn×n, its ε-rank (a.k.a\napproximate rank) is deﬁned to be min{rank(B) : B ∈Rn×n, ∥A −B∥∞≤ε}, where we use\n∥· ∥∞to denote the entry-wise ℓ∞norm of a matrix. The upper bound ε-rank(In) ≤O(log n/ε2)\nwas ﬁrst proved in Alon (2009) using the Johnson-Lindenstrauss Lemma (Johnson & Lindenstrauss,\n1984), and we also provide a proof in Lemma A.1. The concept of ε-rank has wide applications in\ntheoretical computer science (Alon, 2009; Barak et al., 2011; Alon et al., 2013; 2014; Chen & Wang,\n2019), but to our knowledge, this is the ﬁrst time that it appears in reinforcement learning.\nThis fact can be alternatively stated as follow: there exists Φ ∈R2H×O(H/ε2) such that ∥I2H −\nΦΦ⊤∥∞≤ε. We interpret each row of Φ as the feature of a state in the binary tree. By construction\nof Φ, now features of states in the binary tree have a nice property that (i) each feature vector has\napproximately unit norm and (ii) different feature vector are nearly orthogonal. Using this set of\nfeatures, we can now show that Assumption 4.1-4.3 hold. Here we prove Assumption 4.1 holds as\nan example and prove other assumptions also hold in the appendix. To prove Assumption 4.1, we\nnote that in the binary tree hard instance, for each level h, only a single state satisﬁes Q∗= 1, and all\nother states satisfy Q∗= 0. We simply take θh to be the feature of the state with Q∗= 1. Since all\nfeature vectors are nearly orthogonal, Assumption 4.1 holds.\nSince the above fact regarding the ε-rank of the identity matrix can be proved by simply taking each\nrow of Φ to be a random unit vector, our lower bound reveals another intriguing (yet pessimistic)\naspect of Assumption 4.1-4.3: for the binary tree instance, almost all feature extractors induce a\nhard MDP instance. This again suggests that a good representation itself may not necessarily lead to\nefﬁcient RL and additional assumptions (e.g. on the reward distribution) could be crucial.\nLower bound for policy-based learning.\nIt is straightfoward to construct a set of feature vectors\nfor the binary tree instance so that Assumption 4.4 holds, even if d = 1. We set φ(s, a) to be +1 if\na = a1 and −1 if a = a2. For each level h, for the unique state s in level h with Q∗= 1, we set θh\nto be 1 if π∗(s) = a1 and −1 if π∗(s) = a2. With this construction, Assumption 4.4 holds.\nTo prove that the lower bound under Assumption 4.5, we use a new reward function for states in\nlevel H −1 in the binary tree instance above so that there exists a unique optimal action for each\nstate in the MDP. See Figure 2 for an example with H = 3 levels of states. Another nice property\nof the new reward function is that for all states s we always have π∗(s) = a1. Now, we deﬁne\n2H−1 different new MDPs as follow: for each state in level H −1, we change its original reward\n(deﬁned in Figure 2) to 1. An exponential sample complexity lower bound for these MDPs can be\nproved using the same argument as the original binary tree hard instance, and now we show this set\nof MDPs satisfy Assumption 4.5. We ﬁrst show in Lemma A.2 that there exists a set N ⊆Sd−1 with\n|N| = (1/△)Ω(d), so that for each p ∈N, there exists a hyperplane L that separates p and N \\ {p},\nand all vectors in N have distance at least △to L. Equivalently, for each p ∈N,we can always deﬁne\na linear function fp so that fp(p) ≥△and fp(q) ≤−△for all q ∈N \\ {p}. This can be proved\nusing standard lower bounds on the size of ε-nets. Now we simply use vectors in N as features of\nstates. By construction of the reward function, for each level h, there could only be two possible\ncases for the optimal policy π∗. I.e., either π∗(s) = a1 for all states in level h, or π∗(s) = a2 for a\nunique state s and π∗(s′) = a1 for all s ̸= s′. In both cases, we can easily deﬁne a linear function\nwith margin △to implement the optimal policy π∗, and thus Assumption 4.5 holds. Notice that in\nthis proof, we critically relies on d = Θ(H), so that we can utilize the curse of dimensionality to\nconstruct a large set of vectors as features.\n5\nSEPARATIONS\nPerfect representation vs. good-but-not-perfect representation. For value-based learning in\ndeterministic systems, Wen & Van Roy (2013) showed polynomial sample complexity upper bound\nwhen the representation can perfectly predict the Q-function. In contrast, if the representation is only\nable to approximate the Q-function, then the agent requires exponential number of trajectories. This\nexponential separation demonstrates a provable exponential beneﬁt of better representation.\nValue-based learning vs. policy-based learning. Note that if the optimal Q-function can be\nperfectly predicted by the provided representation, then the optimal policy can also be perfectly\n9\nPublished as a conference paper at ICLR 2020\npredicted using the same representation. Since Wen & Van Roy (2013) showed polynomial sample\ncomplexity upper bound when the representation can perfectly predict the Q-function, our lower\nbound on policy-based learning, which applies to perfect representations, thus demonstrates that the\nability of predicting the Q-function is much stronger than that of predicting the optimal policy.\nSupervised learning vs. reinforcement learning. For policy-based learning, if the planning horizon\nH = 1, the problem becomes learning a linear classiﬁer, for which there are polynomial sample\ncomplexity upper bounds. For policy-based learning, the agent needs to learn H linear classiﬁers\nsequentially. Our lower bound on policy-based learning shows the sample complexity dependency on\nH is exponential.\nImitation learning vs. reinforcement learning. In imitation learning (IL), the agent can observe\ntrajectories induced by the optimal policy (expert). If the optimal policy is linear in the given\nrepresentation, it can be shown that the simple behavior cloning algorithm only requires polynomial\nnumber of samples to ﬁnd a near-optimal policy (Ross et al., 2011). Our Theorem 4.3 shows if the\nagent cannot observe expert’s behavior, then it requires exponential number of samples. Therefore,\nour lower bound shows there is an exponential separation between policy-based RL and IL when\nfunction approximation is used.\n6\nACKNOWLEDGMENTS\nThe authors would like to thank Yuping Luo, Wenlong Mou, Martin Wainwright, Mengdi Wang and\nYifan Wu for insightful discussions. Also, the authors would also like to gratefully acknowledge\nBenjamin Van Roy, Shi Dong, Tor Lattimore and Csaba Szepesv´ari for sharing a draft of their\nwork and their comments. Simon S. Du is supported by NSF grant DMS-1638352 and the Infosys\nMembership. Sham M. Kakade acknowledges funding from the Washington Research Foundation\nFund for Innovation in Data-Intensive Discovery; the NSF award CCF 1740551; and the ONR award\nN00014-18-1-2247. Ruosong Wang is supported in part by NSF IIS1763562, AFRL CogDeCON\nFA875018C0014, and DARPA SAGAMORE HR00111990016. Part of this work was done while\nSimon S. Du was visiting Google Brain Princeton and Ruosong Wang was visiting Princeton\nUniversity.\nREFERENCES\nAlekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation\nwith policy gradient methods in markov decision processes. arXiv preprint arXiv:1908.00261,\n2019.\nNoga Alon. Perturbed identity matrices have high rank: Proof and applications. Combinatorics,\nProbability and Computing, 18(1-2):3–15, 2009.\nNoga Alon, Troy Lee, Adi Shraibman, and Santosh Vempala. The approximate rank of a matrix\nand its algorithmic applications: approximate rank. In Proceedings of the forty-ﬁfth annual ACM\nsymposium on Theory of computing, pp. 675–684. ACM, 2013.\nNoga Alon, Troy Lee, and Adi Shraibman. The cover number of a matrix and its algorithmic\napplications. Approximation, Randomization, and Combinatorial Optimization. Algorithms and\nTechniques, pp. 34, 2014.\nAndr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Learning near-optimal policies with bellman-\nresidual minimization based ﬁtted policy iteration and a single sample path. Machine Learning, 71\n(1):89–129, 2008.\nBaruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. Journal of\nComputer and System Sciences, 74(1):97–114, 2008.\nJ. A. Bagnell, Sham M Kakade, Jeff G. Schneider, and Andrew Y. Ng. Policy search by dynamic\nprogramming. In S. Thrun, L. K. Saul, and B. Sch¨olkopf (eds.), Advances in Neural Information\nProcessing Systems 16, pp. 831–838. MIT Press, 2004.\n10\nPublished as a conference paper at ICLR 2020\nBoaz Barak, Zeev Dvir, Amir Yehudayoff, and Avi Wigderson. Rank bounds for design matrices\nwith applications to combinatorial geometry and locally correctable codes. In Proceedings of the\nforty-third annual ACM symposium on Theory of computing, pp. 519–528. ACM, 2011.\nDimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, volume 5. Athena Scientiﬁc\nBelmont, MA, 1996.\nJinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.\narXiv preprint arXiv:1905.00360, 2019.\nLijie Chen and Ruosong Wang. Classical algorithms from quantum and arthur-merlin communication\nprotocols. 10th Innovations in Theoretical Computer Science, 2019.\nChristoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E\nSchapire. On polynomial time PAC reinforcement learning with rich observations. arXiv preprint\narXiv:1803.00606, 2018.\nSanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of johnson and lindenstrauss.\nRandom Structures & Algorithms, 22(1):60–65, 2003.\nSimon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dud´ık, and John Lang-\nford. Provably efﬁcient RL with rich observations via latent state decoding. arXiv preprint\narXiv:1901.09018, 2019a.\nSimon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably efﬁcient Q-learning with func-\ntion approximation via distribution shift error checking oracle. arXiv preprint arXiv:1906.06321,\n2019b.\nAmir-massoud Farahmand. Regularization in reinforcement learning. 2011.\nMatthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision\nprocesses. arXiv preprint arXiv:1901.11275, 2019.\nNan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Con-\ntextual decision processes with low bellman rank are PAC-learnable. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70, pp. 1704–1713. JMLR. org, 2017.\nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement\nlearning with linear function approximation. arXiv preprint arXiv:1907.05388, 2019.\nWilliam B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.\nContemporary mathematics, 26(189-206):1, 1984.\nSham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In\nICML, volume 2, pp. 267–274, 2002.\nSham Machandranath Kakade. On the sample complexity of reinforcement learning. PhD thesis,\nUniversity of College London, 2003.\nMichael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Mach.\nLearn., 49(2-3):209–232, November 2002. ISSN 0885-6125. doi: 10.1023/A:1017984413808.\nURL https://doi.org/10.1023/A:1017984413808.\nAkshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich\nobservations. In Advances in Neural Information Processing Systems, pp. 1840–1848, 2016.\nTor Lattimore and Csaba Szepesvari. Learning with good feature representations in bandits and in rl\nwith a generative model. arXiv preprint arXiv:1911.07676, 2019.\nLihong Li, Michael L Littman, Thomas J Walsh, and Alexander L Strehl. Knows what it knows: a\nframework for self-aware learning. Machine learning, 82(3):399–443, 2011.\nGG Lorentz. Metric entropy and approximation. Bulletin of the American Mathematical Society, 72\n(6):903–937, 1966.\n11\nPublished as a conference paper at ICLR 2020\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):529, 2015.\nR´emi Munos. Error bounds for approximate value iteration. In Proceedings of the National Conference\non Artiﬁcial Intelligence, volume 20, pp. 1006. Menlo Park, CA; Cambridge, MA; London; AAAI\nPress; MIT Press; 1999, 2005.\nSt´ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured\nprediction to no-regret online learning. In Proceedings of the fourteenth international conference\non artiﬁcial intelligence and statistics, pp. 627–635, 2011.\nBruno Scherrer. Approximate policy iteration schemes: A comparison. In Proceedings of the 31st\nInternational Conference on International Conference on Machine Learning - Volume 32, ICML’14.\nJMLR.org, 2014.\nBruno Scherrer and Matthieu Geist. Local policy search in a convex space and conservative policy\niteration as boosted policy search. In Joint European Conference on Machine Learning and\nKnowledge Discovery in Databases, pp. 35–50. Springer, 2014.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\npolicy optimization. In International conference on machine learning, pp. 1889–1897, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nS. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algo-\nrithms. Understanding Machine Learning: From Theory to Algorithms. Cambridge University\nPress, 2014. ISBN 9781107057135. URL https://books.google.com/books?id=\nttJkAwAAQBAJ.\nAaron Sidford, Mengdi Wang, Xian Wu, Lin F Yang, and Yinyu Ye. Near-optimal time and sample\ncomplexities for solving discounted markov decision process with a generative model. arXiv\npreprint arXiv:1806.01492, 2018.\nMax Simchowitz and Kevin Jamieson. Non-asymptotic gap-dependent regret bounds for tabular\nMDPs. 05 2019.\nWen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply\naggrevated: Differentiable imitation learning for sequential prediction. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70, pp. 3309–3318. JMLR. org, 2017.\nWen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based\nrl in contextual decision processes: Pac bounds and exponential improvements over model-free\napproaches. In Conference on Learning Theory, pp. 2898–2933, 2019.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nCsaba Szepesv´ari and R´emi Munos. Finite time bounds for sampling based ﬁtted value iteration. In\nProceedings of the 22nd international conference on Machine learning, pp. 880–887. ACM, 2005.\nBenjamin Van Roy and Shi Dong. Comments on the du-kakade-wang-yang lower bounds. arXiv\npreprint arXiv:1911.07910, 2019.\nZheng Wen and Benjamin Van Roy. Efﬁcient exploration and value function generalization in\ndeterministic systems. In Advances in Neural Information Processing Systems, pp. 3021–3029,\n2013.\nLin F. Yang and Mengdi Wang. Reinforcement leaning in feature space: Matrix bandit, kernels, and\nregret bound. arXiv preprint arXiv:1905.10389, 2019a.\nLin F. Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.\nIn International Conference on Machine Learning, pp. 6995–7004, 2019b.\nAndrew Chi-Chin Yao. Probabilistic computations: Toward a uniﬁed measure of complexity. In 18th\nAnnual Symposium on Foundations of Computer Science (sfcs 1977), pp. 222–227. IEEE, 1977.\n12\nPublished as a conference paper at ICLR 2020\nA\nPROOFS OF LOWER BOUNDS\nIn this section we present our lower bounds. It will also be useful to deﬁne the value function of a\ngiven state s ∈Sh as V π\nh (s) = E\nhPH−1\nh′=h rh′ | sh = s, π\ni\n. For simplicity, we denote V ∗\nh = V π∗\nh (s).\nThroughout the appendix, for the Q-function Qπ\nh and Q∗\nh and the value function V π\nh and V ∗\nh , we may\nomit h from the subscript when it is clear from the context.\nWe ﬁrst introduce the INDEX-QUERY problem, which will be useful in our lower bound arguments.\nDeﬁnition A.1 (INDEX-QUERY). In the INDQn problem, there is an underlying integer i∗∈[n].\nThe algorithm sequentially (and adaptively) outputs guesses i ∈[n] and queries whether i = i∗. The\ngoal is to output i∗, using as few queries as possible.\nDeﬁnition A.2 (δ-correct algorithms). For a real number δ ∈(0, 1), we say a randomized algorithm\nA is δ-correct for INDQn, if for any underlying integer i∗∈[n], with probability at least 1 −δ, A\noutputs i∗.\nThe following theorem states the query complexity of INDQn for 0.1-correct algorithms, whose proof\nis provided in Section B.1.\nTheorem A.1. Any 0.1-correct algorithm A for INDQn requires at least 0.9n queries in the worst\ncase.\nA.1\nPROOF OF LOWER BOUND FOR VALUE-BASED LEARNING\nIn this section we prove Theorem 4.1. We need the following existential result, whose proof is\nprovided in Section B.2.\nLemma A.1. For any n > 2, there exists a set of vectors P = {p0, p1, . . . , pn−1} ⊂Rd with\nd = ⌈8 ln n/ε2⌉such that\n1. ∥pi∥2 = 1 for all 0 ≤i ≤n −1;\n2. |⟨pi, pj⟩| ≤ε for any 0 ≤i, j ≤n −1 with i ̸= j.\nNow we give the construction of the hard MDP instances. We ﬁrst deﬁne the transitions and the\nreward functions. In the hard instances, both the rewards and the transitions are deterministic. There\nare H levels of states, and level h ∈[H] contains 2h distinct states. Thus we have |S| = 2H −1. If\n|S| > 2H −1 we simply add dummy states to the state space S. We use s0, s1, . . . , s2H−2 to name\nthese states. Here, s0 is the unique state in level h = 0, s1 and s2 are the two states in level h = 1,\ns3, s4, s5 and s6 are the four states in level h = 2, etc. There are two different actions, a1 and a2, in\nthe MDPs. For a state si in level h with h < H −1, playing action a1 transits state si to state s2i+1\nand playing action a2 transits state si to state s2i+2, where s2i+1 and s2i+2 are both states in level\nh + 1. See Figure 1 for an example with H = 3.\nIn our hard instances, r(s, a) = 0 for all (s, a) pairs except for a unique state s in level H −2 and a\nunique action a ∈{a1, a2}. It is convenient to deﬁne r(s′) = r(s, a), if playing action a transits s to\ns′. For our hard instances, we have r(s) = 1 for a unique node s in level H −1 and r(s) = 0 for all\nother nodes.\nNow we deﬁne the features map φ(·, ·). Here we assume d ≥2 · ⌈8 ln 2 · H/δ2⌉, and otherwise we\ncan simply decrease the planning horizon so that d ≥2 · ⌈8 ln 2 · H/δ2⌉. We invoke Lemma A.1 to\nget a set P = {p0, p1, . . . , p2H−1} ⊂Rd/2. For each state si, φ(si, a1) ∈Rd is deﬁned to be [pi; 0],\nand φ(si, a2) ∈Rd is deﬁned to be [0; pi]. This ﬁnishes the deﬁnition of the MDPs. We now show\nthat no matter which state s in level H −1 satisﬁes r(s) = 1, the resulting MDP always satisﬁes\nAssumption 4.2.\nVerifying Assumption 4.2.\nBy construction, for each level h ∈[H], there is a unique state sh\nin level h and action ah ∈{a1, a2}, such that Q∗(sh, ah) = 1. For all other (s, a) pairs such that\ns ̸= sh or a ̸= ah, it is satisﬁed that Q∗(s, a) = 0. For a given level h and policy π, we take θπ\nh to be\nQπ(sh, ah) · φ(sh, ah). Now we show that |Qπ(s, a) −⟨θπ\nh, φ(s, a)⟩| ≤δ for all states s in level h\nand a ∈{a1, a2}.\n13\nPublished as a conference paper at ICLR 2020\ns0\ns1\ns3\nQ∗(s1, a1) = 0\ns4\nQ∗(s1, a2) = 0\nQ∗(s0, a1) = 0\ns2\ns5\nQ∗(s2, a1) = 1\ns6\nQ∗(s2, a2) = 0\nQ∗(s0, a2) = 1\nFigure 1: An example with H = 3. For this example, we have r(s5) = 1 and r(s) = 0 for all other\nstates s. The unique state s5 which satisﬁes r(s) = 1 is marked as dash in the ﬁgure. The induced\nQ∗function is marked on the edges.\nCase I: a ̸= ah. In this case, we have Qπ(s, a) = 0 and ⟨θπ\nh, φ(s, a)⟩= 0, since θπ\nh and φ(s, a) do\nnot have a common non-zero coordinate.\nCase II: a = ah and s ̸= sh.\nIn this case, by the second property of P in Lemma A.1 and the fact\nthat Qπ(sh, ah) ≤1, we have |⟨θπ\nh, φ(s, a)⟩| ≤δ. Meanwhile, we have Qπ(s, a) = 0.\nCase III: a = ah and s = sh. In this case, we have ⟨θπ\nh, φ(s, a)⟩= Qπ(sh, ah).\nFinally, we prove any algorithm that solves these MDP instances and succeeds with probability\nat least 0.9 needs to sample at least\n9\n20 · 2H trajectories. We do so by providing a reduction from\nINDQ2H−1 to solving MDPs. Suppose we have an algorithm for solving these MDPs, we show that\nsuch an algorithm can be transformed to solve INDQ2H−1. For a speciﬁc choice of i∗in INDQ2H−1,\nthere is a corresponding MDP instance with\nr(s) =\n\u001a1\nif s = si∗+2H−1−1\n0\notherwise\n.\nNotice that for all MDPs that we are considering, the transition and features are always the same.\nThus, the only thing that the learner needs to learn by interacting with the environment is the reward\nvalue. Since the reward value is non-zero only for states in level H −1, each time the algorithm for\nsolving MDP samples a trajectory that ends at state si where si is a state in level H −1, we query\nwhether i∗= i −2H−1 + 1 or not in INDQ2H−1, and return reward value 1 if i∗= i −2H−1 + 1\nand 0 otherwise. If the algorithm is guaranteed to return a 1/2-optimal policy, then it must be able to\nﬁnd i∗.\nA.2\nPROOF OF LOWER BOUND FOR MODEL-BASED LEARNING\nProof of Theorem 4.2. We use the same construction as in the proof of Theorem 4.1. Note we\njust need to verify that the construction satisﬁes Assumption 4.3. By construction, for all h ∈\n{1, 2, . . . , H −1}, for each state s′ in level h, there exists a unique (s, a) pair such that playing action\na transits s to s′, and we take ψ(s′) = φ(s, a). We also take βh = 0 for h ∈{0, 1, . . . , H −4, H −3}\nand βH−2 = φ(s, a) where (s, a) is the unique pair with R(s, a) = 1. Now, according to the design\nof φ(·, ·) and Lemma A.1, Assumption 4.3 is satisﬁed.\nA.3\nPROOF OF LOWER BOUND FOR POLICY-BASED LEARNING\nIn this section, we present our hardness results for linear policy learning. We ﬁrst prove a weaker\nlower bound which only satisﬁes Assumption 4.4, and then prove Theoerem 4.3.\n14\nPublished as a conference paper at ICLR 2020\nWarmup: Lower Bound for Linear Policy Without Margin.\nTo present the hardness results, we\nﬁrst give the construction of the hard instances. The transitions and rewards functions of these MDP\ninstances are exactly the same as those in Section A.1. The main difference is in the deﬁnition of the\nfeature map φ(·, ·). For this lower bound, we deﬁne φ(s, a) = 1 ∈R if a = a1 and φ(s, a) = −1\nif a = a2. By construction, these MDPs satisfy Assumption 3.1 with ρ = 1. We now show that\nno matter which state s in level H −1 satisﬁes r(s) = 16, the resulting MDP always satisﬁes\nAssumption 4.4.\nVerifying Assumption 4.4.\nRecall that for each level h ∈[H], there is a unique state sh in level h\nand action ah ∈{a1, a2}, such that Q∗(sh, ah) = 1. For all other (s, a) pairs such that s ̸= sh or\na ̸= ah, it is satisﬁed that Q∗(s, a) = 0. We simply take θh to be 1 if ah = a1, and take θh to be −1\nif ah = a2.\nUsing the same lower bound argument (by reducing INDEX-QUERY to MDPs), we have the\nfollowing theorem.\nTheorem A.2. There exists a family of MDPs and a feature map φ (·, ·) that satisfy Assumption 4.4\nwith d = 1 and Assumption 3.1 with ρ = 1, such that any algorithm that returns a 1/2-optimal policy\nwith probability at least 0.9 needs to sample Ω\n\u00002H\u0001\ntrajectories.\nProof of Theoerem 4.3\nNow we prove Theoerem 4.3. In order to prove Theoerem 4.3, we need\nthe following geometric lemma whose proof is provided in Section B.3.\nLemma A.2. Let d ∈N+ be a positive integer and ϵ ∈(0, 1) be a real number. Then there exists a\nset of points N ⊂Sd−1 with size |N| = Ω(1/ϵd/2) such that for every point x ∈N,\ninf\ny∈conv(N \\{x}) ∥x −y∥2 ≥ϵ/2.\n(1)\nNow we are ready to prove Theorem 4.3. In the proof we assume H = d, since otherwise we can\ntake H and d to be min{H, d} by decreasing the planning horizon H or adding dummy dimensions\nto the feature extractor φ.\nProof of Theorem 4.3. We deﬁne a set of 2H−1 deterministic MDPs. The transitions of these hard\ninstances are exactly the same as those in Section A.1. The main difference is in the deﬁnition of the\nfeature map φ(·, ·) and the reward function. Again in the hard instances, r(s, a) = 0 for all s in the\nﬁrst H −2 levels. Using the terminology in Section A.1, we have r(s) = 0 for all states in the ﬁrst\nH −1 levels. Now we deﬁne r(s) for states s in level H −1. We do so by recursively deﬁning the\noptimal value function V ∗(·). The initial state s0 in level 0 satisﬁes V ∗(s0) = 1/2. For each state si\nin the ﬁrst H −2 levels, we have V ∗(s2i+1) = V ∗(si) and V ∗(s2i+2) = V ∗(si) −1/2H. For each\nstate si in the level h = H −2, we have r(s2i+1) = V ∗(si) and r(s2i+2) = V ∗(si) −1/2H. This\nimplies that ρ = 1/2H. In fact, this implies a stronger property that each state has a unique optimal\naction. See Figure 2 for an example with H = 3.\nTo deﬁne 2H−1 different MDPs, for each state s in level H −1 of the MDP deﬁned above, we deﬁne a\nnew MDP by changing r(s) from its original value to 1. This also affects the deﬁnition of the optimal\nV function for states in the ﬁrst H −1 levels. In particular, for each level i ∈{0, 1, 2, . . . , H −2},\nwe have changed the V value of a unique state in level i from its original value (at most 1/2) to 1. By\ndoing so we have deﬁned 2H−1 different MDPs. See Figure 3 for an example with H = 3.\nNow we deﬁne the feature function φ(·, ·). We invoke Lemma A.2 with ϵ = 8△and d = H/2 −1.\nSince △is sufﬁciently small, we have |N| ≥2H. We use P = {p0, p2, . . . , p2H−1} ⊂RH/2−1 to\ndenote an arbitrary subset of N with cardinality 2H. By Lemma A.2, for any p ∈P, the distance\nbetween p and the convex hull of P \\ {p} is at least 4△. Thus, there exists a hyperplane L which\nseparates p and P \\ {p}, and for all points q ∈P, the distance between q and L is at least 2△.\nEquivalently, for each point p ∈P, there exists np ∈RH/2−1 and op ∈R such that ∥np∥2 = 1,\n|op| ≤1 and the linear function fp(q) = ⟨q, np⟩+ op satisﬁes fp(p) ≥2△and fp(q) ≤−2△\nfor all q ∈P \\ {p}. Given the set P = {p0, p2, . . . , p2H−1} ⊂RH/2−1, we construct a new set\n6Recall that r(s′) = r(s, a), if playing action a transits s to s′. Moreover, for the instances in Section A.1,\nwe have r(s) = 1 for a unique node s in level H −1 and r(s) = 0 for all other nodes.\n15\nPublished as a conference paper at ICLR 2020\nV ∗(s0) = 1/2\nV ∗(s1) = 1/2\nr(s3) = 1/2\nr(s4) = 1/3\nV ∗(s2) = 1/3\nr(s5) = 1/3\nr(s6) = 1/6\nFigure 2: An example with H = 3.\nV ∗(s0) = 1\nV ∗(s1) = 1/2\nr(s3) = 1/2\nr(s4) = 1/3\nV ∗(s2) = 1\nr(s5) = 1\nr(s6) = 1/6\nFigure 3: An example with H = 3. Here we deﬁne a new MDP by changing r(s5) from its original\nvalue 1/3 to 1. This also affects the value of V (s2) and V (s0).\n16\nPublished as a conference paper at ICLR 2020\nP = {p0, p2, . . . , p2H−1} ⊂RH/2, where pi = [pi; 1] ∈RH/2. Thus ∥pi∥2 =\n√\n2 for all pi ∈P.\nClearly, for each p ∈P, there exists a vector ωp ∈RH/2 such that ⟨ωp, p⟩≥2△and ⟨ωp, q⟩≤−2△\nfor all q ∈P \\ {p}. It is also clear that ∥ωp∥2 ≤\n√\n2. We take φ(si, a1) = [0; pi] ∈RH and\nφ(si, a2) = [pi; 0] ∈RH.\nWe now show that all the 2H−1 MDPs constructed above satisfy the linear policy assumption. Namely,\nwe show that for any state s in level H −1, after changing r(s) to be 1, the resulting MDP satisﬁes the\nlinear policy assumption. As in Section A.1, for each level h ∈[H], there is a unique state sh in level\nh and action ah ∈{a1, a2}, such that Q∗(sh, ah) = 1. For all other (s, a) pairs such that s ̸= sh or\na ̸= ah, it is satisﬁed that Q∗(s, a) = 0. For each level h, if ah = a1, then we take (θh)H/2 = 1 and\n(θh)H = −1, and all other entries in θh are zeros. If ah = a2, we use p to denote the vector formed\nby the ﬁrst H/2 coordinates of φ(sh, a2). By construction, we have p ∈P. We take θh = [ωp; 0] in\nthis case. In any case, we have ∥θh∥2 ≤\n√\n2. Now for each level h, if ah = a1, then for all states s in\nlevel h, we have π∗(s) = a1. In this case, ⟨φ(s, a1), θh⟩= 1 and ⟨φ(s, a2), θh⟩= −1 for all states\nin level h, and thus Assumption 4.5 is satisﬁed. If ah = a2, then π∗(sh) = a2 and π∗(s) = a1 for\nall states s ̸= sh in level h. By construction, we have ⟨θh, φ(s, a1)⟩= 0 for all states s in level h,\nsince θh and φ(s, a1) do not have a common non-zero entry. We also have ⟨θh, φ(sh, a2)⟩≥2△\nand ⟨θh, φ(s, a2)⟩≤−2△for all states s ̸= sh in level h. Finally, we normalize all θh and φ(s, a)\nso that they all have unit norm. Since ∥φ(s, a)∥2 =\n√\n2 for all (s, a) pairs before normalization,\nAssumption 4.5 is still satisﬁed after normalization.\nFinally, we prove any algorithm that solves these MDP instances and succeeds with probability\nat least 0.9 needs to sample at least Ω(2H) trajectories. We do so by providing a reduction from\nINDQ2H−1 to solving MDPs. Suppose we have an algorithm for solving these MDPs, we show that\nsuch an algorithm can be transformed to solve INDQ2H−1. For a speciﬁc choice of i∗in INDQ2H−1,\nthere is a corresponding MDP instance with\nr(s) =\n\u001a1\nif s = si∗+2H−1−1\nthe original (recursively deﬁned) value\notherwise\n.\nNotice that for all MDPs that we are considering, the transition and features are always the same.\nThus, the only thing that the learner needs to learn by interacting with the environment is the reward\nvalue. Since the reward value is non-zero only for states in level H −1, each time the algorithm for\nsolving MDP samples a trajectory that ends at state si where si is a state in level H −1, we query\nwhether i∗= i −2H−1 + 1 or not in INDQ2H−1, and return reward value 1 if i∗= i −2H−1 + 1\nand it original reward value otherwise. If the algorithm is guaranteed to return a 1/4-optimal policy,\nthen it must be able to ﬁnd i∗.\nB\nTECHNICAL PROOFS\nB.1\nPROOF OF THEOREM A.1\nProof. The proof is a straightforward application of Yao’s minimax principle Yao (1977). We provide\nthe full proof for completeness.\nConsider an input distribution where i∗is drawn uniformly at random from [n]. Suppose there is\na 0.1-correct algorithm for INDQn with worst-case query complexity T such that T < 0.9n. By\naveraging, there is a deterministic algorithm A′ with worst-case query complexity T, such that\nPr\ni∼[n][A′ correctly outputs i when i∗= i] ≥0.9.\nWe may assume that the sequence of queries made by A′ is ﬁxed. This is because (i) A′ is deterministic\nand (ii) before A′ correctly guesses i∗, all responses that A′ receives are the same (i.e., all guesses\nare incorrect). We use S = {s1, s2, . . . , sm} to denote the sequence of queries made by A′. Notice\nthat m is the worst-case query complexity of A′. Suppose m < 0.9n, there exist 0.1n distinct i ∈[n]\nsuch that A′ will never guess i, and will be incorrect if i∗equals i, which implies\nPr\ni∼[n][A′ correctly outputs i when i∗= i] < 0.9.\n17\nPublished as a conference paper at ICLR 2020\nB.2\nPROOF OF LEMMA A.1\nWe need the following tail inequality for random unit vectors, which will be useful for the proof of\nLemma A.1.\nLemma B.1 (Lemma 2.2 in Dasgupta & Gupta (2003)). For a random unit vector u in Rd and β > 1,\nwe have\nPr\n\u0002\nu2\n1 ≥β/d\n\u0003\n≤exp((1 + ln β −β)/2).\nIn particular, when β ≥6,we have\nPr\n\u0002\nu2\n1 > β/d\n\u0003\n≤exp(−β/4).\nProof of Lemma A.1. Let Q = {q1, q2, . . . , qn} be a set of n independent random unit vectors in Rd\nwith d = ⌈8 ln n/ε2⌉. We will prove that with probability at least 1/2, Q satisﬁes the two desired\nproperties as stated in Lemma A.1. This implies the existence of such set P.\nIt is clear that ∥qi∥2 = 1 for all i ∈[n], since each qi is drawn from the unit sphere. We now prove\nthat for any i, j ∈[n] with i ̸= j, with probability at least 1 −\n1\nn2 , we have |⟨qi, qj⟩| ≤ε. Notice that\nthis is sufﬁcient to prove the lemma, since by a union bound over all the\n\u0000n\n2\n\u0001\n= n(n −1)/2 possible\npairs of (i, j), this implies that Q satisﬁes the two desired properties with probability at least 1/2.\nNow, we prove that for two independent random unit vectors u and v in Rd with d = ⌈8 ln n/ε2⌉,\nwith probability at least 1 −1\nn2 , |⟨u, v⟩| ≤ε. By rotational invariance, we assume that v is a standard\nbasis vector. I.e., we assume v1 = 1 and vi = 0 for all 1 < i ≤d. Notice that now ⟨u, v⟩is the\nmagnitude of the ﬁrst coordinate of u. We ﬁnish the proof by invoking Lemma B.1 and taking\nβ = 8 ln n > 6.\nB.3\nPROOF OF LEMMA A.2\nProof of Lemma A.2. Consider a √ϵ-packing N with size Ω(1/ϵd/2) on the d-dimensional unit\nsphere Sd−1 (for the existence of such a packing, see, e.g., Lorentz (1966)). Let o be the origin. For\ntwo points x, x′ ∈Rd, we denote |xx′| := ∥x −x′∥2 the length of the line segment between x, x′.\nNote that every two points x, x′ ∈N satisfy |xx′| ≥√ϵ.\nTo prove the lemma, it sufﬁces to show that N satisﬁes the property equation 1. Consider a point\nx ∈N, let A be a hyperplane that is perpendicular to x (notice that x is a also a vector) and separates\nx and every other points in N. We let the distance between x and A be the largest possible, i.e., A\ncontains a point in N\\{x}. Since x is on the unit sphere and N is a √ϵ-packing, we have that x is at\nleast √ϵ away from every point on the spherical cap not containing x, deﬁned by the cutting plane A.\nMore formally, let b be the intersection point of the line segment ox and A. Then\n∀y ∈\n\b\ny′ ∈Sd−s : ⟨b, y′⟩≤∥b∥2\n2\n\t\n:\n∥x −y∥2 ≥√ϵ.\nIndeed, by symmetry, ∀y ∈{y′ ∈Sd−1 : ⟨b, y′⟩≤∥b∥2\n2\n\t\n,\n∥x −y∥2 ≥∥x −z∥2 ≥√ϵ.\nwhere z ∈N ∩A. Notice that the distance between x and the convex hull of N\\{x} is lower\nbounded by the distance between x and A, which is given by |bx|. Consider the triangles deﬁned by\nx, z, o, b. We have bz ⊥ox (note that bz lies inside A). By Pythagorean theorem, we have\n|bz|2 + |bx|2 = |xz|2;\n|bx| + |bo| = |xo| = 1;\n|bz|2 + |bo|2 = |oz|2 = 1.\nSolve the above three equations for |bx|, we have\n|bx| = |xz|2/2 ≥ϵ/2\nas desired.\n18\nPublished as a conference paper at ICLR 2020\nC\nEXACT LINEAR Q∗+ GAP IN GENERATIVE MODEL\nIn this section we present and prove the following theorem.\nTheorem C.1. Under Assumption 3.1, Assumption 4.2 and Generative Model query model, the\nagent can ﬁnd the optimal π∗with poly\n\u0010\nd, H, 1\nρ, log\n\u0000 1\nδ\n\u0001\u0011\nqueries with probability 1 −δ for a given\nfailure probability δ > 0,\nProof of Theorem C.1. We ﬁrst describe the algorithm. For each level, the agent ﬁrst construct\na barycentric spanner Λh ≜\n\b\nφ(s1\nh, a1\nh), . . . φ(sd\nh, ad\nh)\n\t\n⊂Φh ≜{φ (s, a)}s∈Sh,a∈A (Awerbuch\n& Kleinberg, 2008). We have the property that any φ(s, a) with sh ∈Sh, a ∈A, we have\nc1\ns,a, . . . , cd\ns,a ∈[−1, 1] such that φ(s, a) = Pd\ni=1 ci\ns,aφ(si\nh, ai\nh).\nThe algorithm learns the optimal policy from h = H −1, . . . , 0. At any level h, we assume the agent\nhas learned the optimal policy π∗\nh′ at level h′ = h + 1, . . . , H −1.\nNow we present a procedure to show how to learn the optimal policy at level h. At level h, the\nagent queries every vector φ(si\nh, ai\nh) in Λh for poly(d, 1\nρ, log\n\u0000 H\nδ\n\u0001\n) times and uses π∗\nh+1, . . . , π∗\nH\nas the roll-out to get the on-the-go reward. Note by the deﬁnition of π∗and Q∗, the on-the-go\nreward is an unbiased sample of Q∗(si\nh, ai\nh). We denote bQ(si\nh, ai\nh) the average of these on-the-go\nrewards. By Hoeffding inequality, it is easy to show with probability 1 −δ\nH , for all i = 1, . . . , d,\n\f\f\f bQ(si\nh, ai\nh) −Q∗(si\nh, ai\nh)\n\f\f\f ≤poly\n\u0000 1\nd, ρ\n\u0001\n. Now we deﬁne our estimated Q∗at level h as follow: for\nany (s, a) ∈Sh × A, bQ (s, a) = Pd\ni=1 ci\ns,a bQ(si\nh, ai\nh). By the boundedness property of cs,a, we\nknow for any (s, a) ∈Sh × A, bQ (s, a) −Q∗(s, a) < ρ\n2. Note this implies the policy induced by bQ\nis the same as π∗. Therefore by induction we ﬁnish the proof.\nD\nLINEAR Qπ FOR ALL π IN GENERATIVE MODEL\nIn this section we present and prove the following theorem.\nTheorem D.1. Under Assumption 4.2 with δ = 0, in the Generative Model query model, there is\nan algorithm that ﬁnds an ϵ-optimal policy ˆπ using poly\n\u0000d, H, 1\nϵ\n\u0001\ntrajectories with probability 0.99.\nProof of Theorem D.1. The algorithm is the same as the one in Theorem C.1 We only need to change\nthe analysis. Suppose we are learning at level h and we have learned policies πh+1, . . . , πH−1 for\nlevel h + 1, h + 2, . . . , H −1, respectively. Because we use the roll-out policy πh+1 ◦· · · ◦πH−1,\nby Assumption 4.2 and the property of barycentric spanner, using the same argument in the proof of\nTheorem C.1, we know with probability 1 −0.01/H, we can learn a policy πh with poly\n\u0000d, H, 1\nϵ\n\u0001\nsamples such that for any s ∈Sh, we know πh is only sub-optimal by ϵ\nH from the ˜πh where ˜πh is the\noptimal policy at level h such that πh+1 ◦· · · ◦πH−1 is the ﬁxed roll-out policy.\nNow we can bound the sub-optimality of ˆπ ≜π0 ◦· · · ◦πH−1:\nV π0◦π1◦···◦πH−1 (s1) −V π∗\n0◦π∗\n1◦···◦π∗\nH−1 (s1)\n= V π0◦π1◦···◦πH−1 (s1) −V ˜π0◦π1◦···◦πH−1 (s1)\n+V ˜π0◦π1◦···◦πH−1 (s1) −V π∗\n0◦π1◦···◦πH−1(s1)\n+V π∗\n0◦π1◦···◦πH−1(s1) −V π∗\n0◦π∗\n1◦···◦π∗\nH−1 (s1) .\nThe ﬁrst term is at least −ϵ\nH by our estimation bound, The second term is positive by deﬁnition of ˜π0.\nWe can just recursively apply this argument to obtain\nV π0◦π1◦···◦πH−1 (s1) −V π∗\n0◦π∗\n1◦···◦π∗\nH−1 (s1)\n≥V π∗\n0◦π1◦···◦πH−1(s1) −V π∗\n0◦π∗\n1◦···◦π∗\nH−1 (s1) −ϵ\nH .\n≥V π∗\n0◦π∗\n1◦···◦πH−1(s1) −V π∗\n0◦π∗\n1◦···◦π∗\nH−1 (s1) −2ϵ\nH .\n19\nPublished as a conference paper at ICLR 2020\n≥. . .\n≥−ϵ.\n20\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "math.OC",
    "stat.ML"
  ],
  "published": "2019-10-07",
  "updated": "2020-02-28"
}