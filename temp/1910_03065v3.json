{
  "id": "http://arxiv.org/abs/1910.03065v3",
  "title": "Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations",
  "authors": [
    "Oana-Maria Camburu",
    "Brendan Shillingford",
    "Pasquale Minervini",
    "Thomas Lukasiewicz",
    "Phil Blunsom"
  ],
  "abstract": "To increase trust in artificial intelligence systems, a promising research\ndirection consists of designing neural models capable of generating natural\nlanguage explanations for their predictions. In this work, we show that such\nmodels are nonetheless prone to generating mutually inconsistent explanations,\nsuch as \"Because there is a dog in the image\" and \"Because there is no dog in\nthe [same] image\", exposing flaws in either the decision-making process of the\nmodel or in the generation of the explanations. We introduce a simple yet\neffective adversarial framework for sanity checking models against the\ngeneration of inconsistent natural language explanations. Moreover, as part of\nthe framework, we address the problem of adversarial attacks with full target\nsequences, a scenario that was not previously addressed in sequence-to-sequence\nattacks. Finally, we apply our framework on a state-of-the-art neural natural\nlanguage inference model that provides natural language explanations for its\npredictions. Our framework shows that this model is capable of generating a\nsignificant number of inconsistent explanations.",
  "text": "arXiv:1910.03065v3  [cs.CL]  2 May 2020\nMake Up Your Mind! Adversarial Generation of\nInconsistent Natural Language Explanations\nOana-Maria Camburu1,4\nBrendan Shillingford1,2\nPasquale Minervini3\nThomas Lukasiewicz1,4\nPhil Blunsom1,2\n1 University of Oxford\n2 DeepMind, London\n3 University College London\n4 Alan Turing Institute, London\nfirstname.lastname@cs.ox.ac.uk\np.minervini@ucl.ac.uk\nAbstract\nTo increase trust in artiﬁcial intelligence sys-\ntems, a promising research direction consists\nof designing neural models capable of generat-\ning natural language explanations for their pre-\ndictions. In this work, we show that such mod-\nels are nonetheless prone to generating mu-\ntually inconsistent explanations, such as “Be-\ncause there is a dog in the image.” and “Be-\ncause there is no dog in the [same] image.”,\nexposing ﬂaws in either the decision-making\nprocess of the model or in the generation of\nthe explanations. We introduce a simple yet ef-\nfective adversarial framework for sanity check-\ning models against the generation of incon-\nsistent natural language explanations. More-\nover, as part of the framework, we address\nthe problem of adversarial attacks with full\ntarget sequences, a scenario that was not pre-\nviously addressed in sequence-to-sequence at-\ntacks. Finally, we apply our framework on a\nstate-of-the-art neural natural language infer-\nence model that provides natural language ex-\nplanations for its predictions. Our framework\nshows that this model is capable of generating\na signiﬁcant number of inconsistent explana-\ntions.\n1\nIntroduction\nIn order to explain the predictions produced by\naccurate yet black-box neural models, a growing\nnumber of works propose extending these mod-\nels with natural language explanation generation\nmodules, thus obtaining models that explain them-\nselves in human language (Hendricks et al., 2016;\nCamburu et al., 2018; Park et al., 2018; Kim et al.,\n2018; Ling et al., 2017).\nIn this work, we ﬁrst draw attention to the fact\nthat such models, while appealing, are nonethe-\nless prone to generating inconsistent explanations.\nWe deﬁne two explanations to be inconsistent if\nthey provide contradictory arguments about the in-\nstances and predictions that they aim to explain.\nFor example, consider a visual question answering\n(VQA) task (Park et al., 2018) and two instances\nwhere the image is the same but the questions are\ndifferent, say “Is there an animal in the image?”\nand “Can you see a Husky in the image?”.\nIf\nfor the ﬁrst instance a model predicts “Yes.” and\ngenerates the explanation “Because there is a dog\nin the image.”, while for the second instance the\nsame model predicts “No.” and generates the ex-\nplanation “Because there is no dog in the image.”,\nthen the model is producing inconsistent explana-\ntions.\nInconsistent explanations reveal at least one of\nthe following undesired behaviors: (i) at least one\nof the explanations is not faithfully describing the\ndecision mechanism of the model, or (ii) the model\nrelied on a faulty decision mechanism for at least\none of the instances. Note that, for a pair of incon-\nsistent explanations, further investigation would\nbe needed to conclude which of these two behav-\niors is the actual one (and might vary for each in-\nstance). Indeed, a pair of inconsistent explanations\ndoes not necessarily imply at least one unfaithful\nexplanation. In our previous example, if the im-\nage contains a dog, it is possible that the model\nidentiﬁes the dog when it processes the image to-\ngether with the ﬁrst question, and that the model\ndoes not identify the dog when it processes the im-\nage together with the second question, hence both\nexplanations would faithfully reﬂect the decision\nmechanism of the model even if they are inconsis-\ntent. Similarly, a pair of inconsistent explanations\ndoes not necessarily imply that the model relies\non a faulty decision mechanism, because the ex-\nplanations may not faithfully describe the decision\nmechanism of the model. We here will not inves-\ntigate the problem of identifying which of the two\nundesired behaviors is true for a pair of inconsis-\nPREMISE: A guy in a red jacket is snowboarding in midair.\nORIGINAL HYPOTHESIS: A guy is outside in the snow.\nPREDICTED LABEL: entailment\nORIGINAL EXPLANATION: Snowboarding is done outside.\nREVERSE HYPOTHESIS: The guy is outside.\nPREDICTED LABEL: contradiction\nREVERSE EXPLANATION: Snowboarding is not done outside.\nPREMISE: A man talks to two guards as he holds a drink.\nORIGINAL HYPOTHESIS: The prisoner is talking to two guards in the\nprison cafeteria.\nPREDICTED LABEL: neutral\nORIGINAL EXPLANATION: The man is not necessarily a prisoner.\nREVERSE HYPOTHESIS: A prisoner talks to two guards.\nPREDICTED LABEL: entailment\nREVERSE EXPLANATION: A man is a prisoner.\nPREMISE: Two women and a man are sitting down eating and drinking various items.\nORIGINAL HYPOTHESIS: Three women are shopping at the mall.\nPREDICTED LABEL: contradiction\nORIGINAL EXPLANATION: There are either two women and a man or\nthree women.\nREVERSE HYPOTHESIS: Three women are sitting down eating.\nPREDICTED LABEL: neutral\nREVERSE EXPLANATION: Two women and a man are three women.\nTable 1: Examples of detected inconsistent explanations – the reverse hypotheses generated by our method (right)\nare realistic.\ntent explanations.\nIn this work, we introduce a framework for\nchecking if models are robust against generating\ninconsistent natural language explanations. Given\na model m that produces natural language ex-\nplanations for its predictions, and an instance x,\nour framework aims to generate inputs ˆx that\ncause the model to produce explanations that are\ninconsistent with the explanation produced for\nx.\nThus, our framework falls under the cate-\ngory of adversarial methods, i.e., searching for in-\nputs that cause a model to produce undesired an-\nswers (Biggio et al., 2013; Szegedy et al., 2014).\nAs part of our framework, we address the\nproblem of adversarial attacks with full target se-\nquences, a scenario that has not been previously\naddressed in sequence-to-sequence attacks, and\nwhich can be useful for other areas, such as di-\nalog systems. Finally, we apply our framework\non a state-of-the-art neural natural language in-\nference model that generates natural language ex-\nplanations for its decisions (Camburu et al., 2018).\nWe show that this model can generate a signiﬁcant\nnumber of inconsistent explanations.\n2\nMethod\nGiven a model m that can jointly produce predic-\ntions and natural language explanations, we pro-\npose a framework that, for any given instance x,\nattempts to generate new instances for which the\nmodel produces explanations that are inconsistent\nwith the explanation produced for x; we refer to\nthe latter as em(x).\nWe approach the problem in two high-level\nsteps. Given an instance x, (A) we create a list of\nexplanations that are inconsistent with the explana-\ntion generated by the model on x, and (B) given an\ninconsistent explanation from the list created in A,\nwe ﬁnd an input that causes the model to generate\nthis precise inconsistent explanation.\nSetup.\nOur setup has three desired properties\nthat make it different from commonly researched\nadversarial settings in natural language process-\ning:\n• At step (B), the model has to generate a full\ntarget sequence: the goal is to generate the ex-\nact explanation that was identiﬁed at step (A)\nas inconsistent with the explanation em(x).\n• Adversarial inputs do not have to be a para-\nphrase or a small perturbation of the original\ninput, since our objective is to generate incon-\nsistent explanations rather than incorrect pre-\ndictions — these can eventually happen as a\nbyproduct.\n• Adversarial inputs have to be realistic to the\ntask at hand.\nTo our knowledge, this work is the ﬁrst to tackle\nthis problem setting, especially due to the chal-\nlenging requirement of generating a full target se-\nquence — see Section 4 for comparison with exist-\ning works.\nContext-dependent inconsistencies.\nIn certain\ntasks, instances consist of a context (such as an\nimage or a paragraph), and some assessment to be\nmade about the context (such as a question or a\nhypothesis). Since explanations may refer (some-\ntimes implicitly) to the context, the assessment\nof whether two explanations are inconsistent may\nalso depend on it. For example, in VQA, the incon-\nsistency of the two explanations “Because there is\na dog in the image.” and “Because there is no dog\nin the image.” depends on the image. However, if\nthe image is the same, the two explanations are\ninconsistent regardless of which questions were\nasked on that image.\nFor such a reason, given an instance x, we dif-\nferentiate between parts of the instance that will\nremain ﬁxed in our method (referred to as con-\ntext parts and denoted as xc) and parts of the in-\nstance that our method will vary in order to obtain\ninconsistencies (referred to as variable parts and\ndenoted as xv). Hence, x = (xc, xv). In our VQA\nexample, xc is the image, and xv is the question.\nStand-alone inconsistencies.\nFurthermore, we\nnote that there are cases for which explanations\nare inconsistent regardless of the input. For ex-\nample, explanations formed purely of background\nknowledge such as “A woman is a person.” and\n“A woman is not a person.”1 are always inconsis-\ntent (and sometimes outrageous), regardless of the\ninstances that lead to them. For these cases, our\nmethod can treat the whole input as variable, i.e.,\nxc = ∅and ˆxv = x.\nSteps.\nOur adversarial framework consists of the\nfollowing steps:\n1. Reverse the explanation generator module of\nmodel m by training a REVEXPL model to map\nfrom the generated explanation and the context\npart of the input to the variable part of the input,\ni.e., REVEXPL(xc, em(x)) = xv.\n2. For each explanation e = em(x):\n(a) Create a list of statements that are inconsis-\ntent with e, we call it Ie.\n(b) Query REVEXPL on each ˆe ∈Ie and the\ncontext xc. Get the new variable part ˆxv =\nREVEXPL(xc,ˆe) of a reverse input ˆx =\n(xc, ˆxv), which may cause the m to produce\ninconsistent explanations.\n(c) Query m on each reverse input to get a re-\nverse explanation em(ˆx).\n(d) Check if each reverse explanation em(ˆx) is\nindeed inconsistent with e by checking if\nem(ˆx) ∈Ie.\nTo execute step (2a), note that explanations are by\nnature logical sentences. Hence, for any task, one\nmay deﬁne a set of logical rules to transform an\nexplanation into an inconsistent counterpart, such\n1Which was generated by the model in our experiments.\nas negation or replacement of task-essential tokens\nwith task-speciﬁc antonyms. For example, in ex-\nplanations for self-driving cars (Kim et al., 2018),\none can replace “green light” with “red light”, or\n“the road is empty” with “the road is crowded”\n(which are task-speciﬁc antonyms), to get incon-\nsistent (and hazardous) explanations such as “The\ncar accelerates because there is a red light.”.\nAnother strategy to obtain inconsistent explana-\ntions consists of swapping explanations from mu-\ntually exclusive labels. For example, assume a rec-\nommender system predicts that movie X is a bad\nrecommendation for user Y “because X is a horror\nmovie.”, implying that user Y does not like horror\nmovies. If it also predicts that movie Z is a good\nrecommendation to the same user Y “because Z is\na horror movie.”, then we have an inconsistency,\nas the latter would imply that user Y likes horror\nmovies.\nWhile this step requires a degree of speciﬁc ad-\njustment to the task at hand, we consider it a small\nprice to pay to ensure that the deployed system is\ncoherent. Also, note that this step can eventually\nbe automated, for example, by training a neural\nnetwork to generate task-speciﬁc inconsistencies\nafter crowd-sourcing a dataset of inconsistent ex-\nplanations for a task at hand — we leave this as\nfuture work.\nFinally, to execute step (2d), our framework cur-\nrently checks for an exact string match between a\nreverse explanation and any of the inconsistent ex-\nplanations created at step (2a). Alternatively, one\ncan train a model to identify if a pair of explana-\ntions forms an inconsistency, which we also leave\nas future work.\n3\nExperiments\nWe consider the task of natural language inference\n(NLI) (Bowman et al., 2015), which consists of de-\ntecting whether a pair of sentences, called premise\nand hypothesis, are in a relation of: entailment, if\nthe premise entails the hypothesis; contradiction,\nif the premise contradicts the hypothesis; or neu-\ntral, if neither entailment nor contradiction holds.\nFor example, a pair with premise “Two doctors\nperform surgery on patient.” and hypothesis “Two\ndoctors are performing surgery on a man.” consti-\ntutes a neutral pair.\nThe SNLI corpus (Bowman et al., 2015) of ∼\n570K such human-written instances enabled a\nplethora of works on this task (Rockt¨aschel et al.,\n2015; Munkhdalai and Yu, 2016; Liu et al., 2016).\nRecently, Camburu et al. (2018) augmented SNLI\nwith crowd-sourced free-form explanations of the\nground-truth label, called e-SNLI. An explanation\nfrom e-SNLI for the neutral pair above is “Not ev-\nery patient is a man.”.\nTheir\nbest\nmodel\nfor\ngenerating\nexpla-\nnations,\ncalled\nEXPLAINTHENPREDIC-\nTATTENTION\n(hereafter\ncalled\nETPA),\nis\na\nsequence-to-sequence\nattention\nmodel\nthat\nuses\ntwo\nbidirectional\nLSTM\nnet-\nworks\n(Hochreiter and Schmidhuber,\n1997)\nfor encoding the premise and hypothesis, and an\nLSTM decoder for generating the explanation\nwhile separately attending over the tokens of\nthe premise and hypothesis. Subsequently, they\npredict the label solely based on the explanation\nvia a separately trained network, which maps an\nexplanation to a label.\nWe show that our framework is able to make\nETPA2 generate a signiﬁcant number of inconsis-\ntent explanations. We highlight that our ﬁnal goal\nis not a label attack, even if, for this particular\nmodel in which the label is predicted solely from\nthe explanation, we implicitly also have a label at-\ntack with high probability.3\nIn our experiments, we set xc as the premise\n(as this represents the given context in this task)\nand xv as the hypothesis.\nHowever, note that\ndue to the nature of SNLI for which decisions are\nbased mostly on commonsense knowledge, the ex-\nplanations are most of the time independent of the\npremise, such as “A dog is an animal.” — hence,\nit would be possible to also reverse the premise\nand not just the hypothesis; we leave this as future\nwork.\nFor the REVEXPL model, we use the same\nneural architecture and hyperparameters used by\nCamburu et al. (2018) for ETPA. REVEXPL takes\nas input a premise-explanation pair, and produce a\nhypothesis. Our trained REVEXPL model is able\nto reconstruct exactly the same (according to string\nmatching) hypothesis with 32.78% test accuracy.\nCreating Ie.\nTo execute step (2a), we employ\nnegation and swapping explanations. For negation,\nwe simply remove the tokens “not” and “n’t” if\nthey are present. If these tokens appear more than\n2We\nuse\nthe\npretrained\nmodel\nfrom\nhttps://github.com/OanaMariaCamburu/e-SNLI.\n3Their Explanation-to-Label component had 96.83% test\naccuracy.\nonce in an explanation, we create multiple incon-\nsistencies by removing only one occurrence at a\ntime. We do not attempt to add negation tokens,\nas this may result in grammatically incorrect sen-\ntences.\nFor swapping explanations, we note that the ex-\nplanations in e-SNLI largely follow a set of label-\nspeciﬁc templates. This is a natural consequence\nof the task and the SNLI dataset and not a require-\nment in the collection of the e-SNLI. For exam-\nple, annotators often used “One cannot X and Y\nsimultaneously.” to explain a contradiction, “Just\nbecause X, doesn’t mean Y.” for neutral, or “X im-\nplies Y.” for entailment. Since any two labels are\nmutually exclusive, transforming an explanation\nfrom one template to a template of another label\nshould automatically create an inconsistency. For\nexample, for the explanation of the contradiction\n“One cannot eat and sleep simultaneously.”, we\nmatch X to “eat” and Y to “sleep”, and create the\ninconsistent explanation “Eat implies sleep.” us-\ning the entailment template “X implies Y.”. Thus,\nfor each label, we created a list of the most used\ntemplates that we manually identiﬁed among e-\nSNLI, which can be found in Appendix A. A run-\nning example of creating inconsistent explanations\nby swapping is given in Appendix A.1.\nIf there is no negation and no template match,\nwe discarded the instance. In our experiments, we\nonly discarded 2.6% of the SNLI test set.\nWe note that this procedure may result in gram-\nmatically or semantically incorrect inconsistent ex-\nplanations. However, as we will see below, our\nREVEXPL performed well in generating correct\nand relevant reverse hypotheses even when its in-\nput explanations were not correct. This is not sur-\nprising, because REVEXPL has been trained to out-\nput ground-truth hypotheses.\nThe rest of the steps follow as described in (2b)\n- (2d).\nResults and discussion.\nWe identiﬁed a total\nof 1044 pairs of inconsistent explanations starting\nfrom the SNLI test set, which contains 9824 in-\nstances. First, we noticed that there are, on av-\nerage, 1.93 ± 1.77 distinct reverse hypotheses giv-\ning rise to a pair of inconsistent explanation. Since\nthe hypotheses are distinct, each of these instances\nis a separate valid adversarial inputs. However, if\none is strictly interested in the number of distinct\npairs of inconsistent explanations, then, after elim-\ninating duplications, we obtain 540 pairs of such\ninconsistencies.\nSecondly, since the generation of natural lan-\nguage is always best evaluated by humans, we\nmanually annotated 100 random distinct pairs. We\nfound that 82% of the reverse hypotheses form re-\nalistic instances together with the premise.\nWe\nalso found that the majority of the unrealistic in-\nstances are due to a repetition of a token in the\nhypothesis. For example, “A kid is riding a helmet\nwith a helmet on training.” is a generated reverse\nhypothesis which is just one token away from a\nperfectly valid hypothesis.\nGiven our estimation of 82% to be inconsisten-\ncies caused by realistic reverse hypotheses, we\nobtained a total of ∼443 distinct pairs of incon-\nsistent explanations.\nWhile this means that our\nprocedure only has a success rate of ∼4.51%, it\nis nonetheless alarming that this very simple and\nunder-optimized adversarial framework detects a\nsigniﬁcant number of inconsistencies on a model\ntrained on ∼570K examples. In Table 1, we see\nthree examples of detected inconsistencies. More\nexamples can be found in Appendix B.\nManual scanning.\nWe were curious to what ex-\ntent one can ﬁnd inconsistencies via a brute-force\nmanual scanning.\nWe performed three such ex-\nperiments, with no success. On the contrary, we\nnoticed a good level of robustness against incon-\nsistencies when scanning through the generic ad-\nversarial hypotheses introduced by Carmona et al.\n(2018). The details are in Appendix C.\n4\nRelated Work\nAn increasing\namount\nof\nwork\nfocuses\non\nproviding natural language, free-form explana-\ntions (Camburu et al., 2018; Kim et al., 2018;\nPark et al., 2018; Hendricks et al., 2016) as a\nmore comprehensive and user-friendly alterna-\ntive to other forms of explainability, such as\nfeature-based explanations (Ribeiro et al., 2016;\nLundberg and Lee, 2017). In this work, we bring\nawareness to the risk of generating inconsistent ex-\nplanations. Similarly, Hendricks et al. (2017) iden-\ntify the risk of mentioning attributes from a strong\nclass prior without any evidence being present in\nthe input.\nGenerating adversarial examples.\nGenerating\nadversarial examples is an active research area in\nnatural language processing (Zhang et al., 2019;\nWang et al., 2019).\nHowever, most works build\non the requirement that the adversarial input\nshould be a small perturbation of an original\ninput (Belinkov and Bisk, 2017; Hosseini et al.,\n2017; Cheng et al., 2018), or should be preserv-\ning the semantics of the original input (Iyyer et al.,\n2018). Our setup does not have this requirement,\nand any pair of task-realistic inputs that causes the\nmodel to produce inconsistent explanations suf-\nﬁces. Most importantly, to our knowledge, no pre-\nvious adversarial attack for sequence-to-sequence\nmodels generates full target sequences.\nFor in-\nstance, Cheng et al. (2018) require the presence\nof pre-deﬁned tokens anywhere in the target se-\nquence: they only test with up to 3 required tokens,\nand their success rate dramatically drops from\n99% for 1 token to 37% for 3 tokens for the task of\nsummarization. Similarly, Zhao et al. (2018) pro-\nposed an adversarial framework for adding and re-\nmoving tokens in the target sequence for the task\nof machine translation.\nOur scenario would re-\nquire as many tokens as the desired adversarial\nexplanation, and we also additionally need them\nto be in a given order, thus tackling a much chal-\nlenging task. Finally, Minervini and Riedel (2018)\nattempted to ﬁnd inputs where a model trained on\nSNLI violates a set of logical constraints. How-\never, their method needs to enumerate and evalu-\nate a potentially very large set of perturbations of\nthe inputs. Besides the computational overhead, it\nalso may easily generating ungrammatical inputs.\nMoreover, their scenario does not address the ques-\ntion of automatically producing undesired (incon-\nsistent) sequences.\n5\nSummary and Outlook\nWe drew attention that models generating natural\nlanguage explanations are prone to producing in-\nconsistent explanations. This concern is general\nand can have a large practical impact. For exam-\nple, users would likely not accept a self-driving\ncar if its explanation module is prone to state\nthat “The car accelerates because there are peo-\nple crossing the intersection.”. We introduced a\ngeneric framework for identifying such inconsis-\ntencies and showed that the best existing model on\ne-SNLI can generate a signiﬁcant number of incon-\nsistencies. Future work will focus on developing\nmore advanced procedures for detecting inconsis-\ntencies, and on building robust models that do not\ngenerate inconsistencies.\nAcknowledgments.\nThis work was supported\nby a JP Morgan PhD Fellowship, the Alan Turing\nInstitute under the EPSRC grant EP/N510129/1,\nthe EPSRC grant EP/R013667/1, the AXA Re-\nsearch Fund, and the EU Horizon 2020 Re-\nsearch and Innovation Programme under the grant\n875160.\nReferences\nYonatan Belinkov and Yonatan Bisk. 2017. Synthetic\nand natural noise both break neural machine transla-\ntion. CoRR, abs/1711.02173.\nBattista Biggio, Igino Corona, Davide Maiorca, Blaine\nNelson, Nedim Srndic, Pavel Laskov, Giorgio Giac-\ninto, and Fabio Roli. 2013. Evasion attacks against\nmachine learning at test time. In ECML/PKDD (3),\nvolume 8190 of LNCS, pages 387–402. Springer.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nCoRR, abs/1508.05326.\nOana-Maria Camburu,\nTim Rockt¨aschel,\nThomas\nLukasiewicz, and Phil Blunsom. 2018. e-SNLI: Nat-\nural language inference with natural language expla-\nnations. In NeurIPS, pages 9560–9572.\nVicente Iv´an S´anchez Carmona, Jeff Mitchell, and Se-\nbastian Riedel. 2018. Behavior analysis of NLI mod-\nels: Uncovering the inﬂuence of three factors on ro-\nbustness. In NAACL-HLT, pages 1975–1985. Asso-\nciation for Computational Linguistics.\nMinhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen,\nand Cho-Jui Hsieh. 2018. Seq2Sick: Evaluating the\nrobustness of sequence-to-sequence models with ad-\nversarial examples. CoRR, abs/1803.01128.\nLisa\nAnne\nHendricks,\nZeynep\nAkata,\nMarcus\nRohrbach, Jeff Donahue, Bernt Schiele, and Trevor\nDarrell. 2016. Generating visual explanations. In\nECCV (4), volume 9908 of LNCS, pages 3–19.\nSpringer.\nLisa Anne Hendricks, Ronghang Hu, Trevor Darrell,\nand Zeynep Akata. 2017. Grounding visual expla-\nnations (extended abstract). CoRR, abs/1711.06465.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory.\nNeural Comput., 9(8):1735–\n1780.\nHossein Hosseini, Baicen Xiao, and Radha Pooven-\ndran. 2017. Deceiving Google’s cloud video intel-\nligence API built for summarizing videos. In CVPR\nWorkshops, pages 1305–1309. IEEE Computer Soci-\nety.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks.\nCoRR, abs/1804.06059.\nJinkyu Kim, Anna Rohrbach, Trevor Darrell, John F.\nCanny, and Zeynep Akata. 2018. Textual explana-\ntions for self-driving vehicles. In ECCV (2), volume\n11206 of LNCS, pages 577–593. Springer.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. CoRR, abs/1705.04146.\nYang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.\n2016.\nLearning natural language inference us-\ning bidirectional LSTM model and inner-attention.\nCoRR, abs/1605.09090.\nScott M. Lundberg and Su-In Lee. 2017.\nA uniﬁed\napproach to interpreting model predictions. In Ad-\nvances in Neural Information Processing Systems\n30, pages 4765–4774. Curran Associates, Inc.\nPasquale Minervini and Sebastian Riedel. 2018. Adver-\nsarially regularising neural NLI models to integrate\nlogical background knowledge.\nIn CoNLL, pages\n65–74. Association for Computational Linguistics.\nTsendsuren Munkhdalai and Hong Yu. 2016. Neural\nsemantic encoders. CoRR, abs/1607.04315.\nDong Huk Park, Lisa Anne Hendricks, Zeynep Akata,\nAnna Rohrbach, Bernt Schiele, Trevor Darrell, and\nMarcus Rohrbach. 2018. Multimodal explanations:\nJustifying decisions and pointing to the evidence.\nCoRR, abs/1802.08129.\nMarco T´ulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. “Why should I trust you?”: Explain-\ning the predictions of any classiﬁer. In KDD, pages\n1135–1144. ACM.\nTim Rockt¨aschel, Edward Grefenstette, Karl Moritz\nHermann, Tom´as Kocisk´y, and Phil Blunsom. 2015.\nReasoning about entailment with neural attention.\nCoRR, abs/1509.06664.\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever,\nJoan Bruna, Dumitru Erhan, Ian J. Goodfellow, and\nRob Fergus. 2014. Intriguing properties of neural\nnetworks. In ICLR (Poster).\nWenqi Wang, Benxiao Tang, Run Wang, Lina Wang,\nand Aoshuang Ye. 2019. A survey on adversarial\nattacks and defenses in text. CoRR, abs/1902.07285.\nWei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi,\nand Chenliang Li. 2019. Adversarial attacks on deep\nlearning models in natural language processing: A\nsurvey.\nZhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.\nGenerating natural adversarial examples. In ICLR\n(Poster). OpenReview.net.\nA\ne-SNLI Explanations Templates\nBelow we present the list of templates that we\nmanually found to match most of the e-SNLI ex-\nplanations (Camburu et al., 2018). We recall that\nduring the collection of the dataset Camburu et al.\n(2018) did not impose any template, they were a\nnatural consequence of the task and SNLI dataset.\nHere, “subphrase1/subphrase2/...” means that\na separate template is to be considered for each\nof the subphrases. X and Y are the key elements\nthat we want to identify and use in the other tem-\nplates in order to create inconsistencies. “[...]” is\na placeholder for any string, and its value is not\nrelevant. Subphrases placed between round paren-\nthesis (for example, “(the)” or “(if)”) are optional,\nand two distinct templates are formed one with and\none without that subphrase.\nEntailment Templates\n• X is/are a type of Y\n• X implies Y\n• X is/are the same as Y\n• X is a rephrasing of Y\n• X is a another form of Y\n• X is synonymous with Y\n• X and Y are synonyms/synonymous\n• X and Y is/are the same thing\n• (if) X , then Y\n• X so Y\n• X must be Y\n• X has/have to be Y\n• X is/are Y\nNeutral Templates\n• not all X are Y\n• not every X is Y\n• just because X does not/n’t mean/imply Y\n• X is/are not necessarily Y\n• X does not/n’t have to be Y\n• X does not/n’t imply/mean Y\nContradiction Templates\n• ([...]) cannot/can not/ca n’t (be) X and Y at\nthe same time/simultaneously\n• ([...]) cannot/can not/ca n’t (be) X and at the\nsame time Y\n• X is/are not (the) same as Y\n• ([...]) is/are either X or Y\n• X is/are not Y\n• X is/are the opposite of Y\n•\n([...])\ncannot/can not/ca n’t (be) X if\n(is/are) Y\n• X is/are different than Y\n• X and Y are different ([...])\nA.1\nRunning Example for Creating\nInconsistencies by Swapping between\nTemplates of Explanations\nConsider the explanation e =“Dog is a type of an-\nimal.” which may arise from a model explaining\nthe instance x = (premise: “A dog is in the park.”,\nhypothesis: “An animal is in the park.”). We iden-\ntify that e matches the template “X is/are a type of\nY” with X = “dog” (we convert to lowercase) and\nY = “animal”. We generate the list Ie by replacing\nX and Y in each of the neutral and contradictory\ntemplates listed above with the exception of those\nthat contain “[...]” in order to avoid guessing the\nplaceholder. We obtain Ie as:\n• not all dog are animal\n• not every dog is animal\n• just because dog does not/n’t mean/imply an-\nimal\n• dog is/are not necessarily animal\n• dog does not/n’t have to be animal\n• dog does not/n’t imply/mean animal\n• cannot/can not/ca n’t (be) dog and animal at\nthe same time/simultaneously\n•\ncannot/can not/ca n’t (be) dog and at the\nsame time animal\n• dog is/are not (the) same as animal\n• is/are either dog or animal\n• dog is/are not animal\n• dog is/are the opposite of animal\n• cannot/can not/ca n’t (be) dog if (is/are) ani-\nmal\n• dog is/are different than animal\n• dog and animal are different\nB\nMore Examples of Detected\nInconsistencies\nIn Table 2, we provide more examples of inconsis-\ntent explanations detected with our method.\nC\nManual Scanning\nWe performed three experiments of manually scan-\nning. First, we manually analyzed the ﬁrst 50 in-\nstances in the test set without ﬁnding any incon-\nsistency. However, these examples were involving\ndifferent concepts, thus decreasing the likelihood\nof ﬁnding inconsistencies.\nTo account for this,\nin our second experiment, we constructed three\ngroups around the concepts of woman, prisoner,\nand snowboarding, by simply selecting the expla-\nnations in the test set containing these words. We\nselected these concepts, because our framework\ndetected inconsistencies about them — examples\nare listed in Table 1 and Table 2.\nFor woman, we obtained 1150 examples in the\ntest set, and we looked at a random sample of 20,\namong which we did not ﬁnd any inconsistency.\nFor snowboarding, we found 16 examples in the\ntest set and again no inconsistency among them.\nFor prisoner, we only found one instance in the\ntest set, so we had no way to ﬁnd out that the\nmodel is inconsistent with respect to this concept\nsimply by scanning the test set.\nWe only looked at the test set for a fair compari-\nson with our method that was only applied on this\nset.\nHowever, we highlight that, even if the manual\nscanning would have been successful, it should not\nbe regarded as a proper baseline, since it does not\nbring the same beneﬁts as our framework. Indeed,\nmanual scanning requires considerable human ef-\nfort to look over a large set of explanations in order\nto ﬁnd if any two are inconsistent. Even a group of\nonly 50 explanations required us a non-negligible\namount of time. Moreover, restricting ourselves to\nthe instances in the original dataset would clearly\nbe less effective than being able to generate new in-\nstances from the dataset’s distribution. Our frame-\nwork addresses these issues and directly provides\npairs of inconsistent explanations.\nNonetheless,\nwe considered this experiment useful for illustrat-\ning that the explanation module does not provide\ninconsistent explanations in a frequent manner.\nIn our third experiment of manual scanning,\nwe experimented with a few manually created hy-\npotheses from Carmona et al. (2018), which had\nbeen shown to induce confusion at the label level.\nWe were pleased to notice a good level of robust-\nness against inconsistencies. For example, for the\nneutral pair (premise: “A bird is above water.”, hy-\npothesis: “A swan is above water.”), we get the\nexplanation “Not all birds are a swan.”, while\nwhen interchanging bird with swan, i.e., for the\npair (premise: “A swan is above water.”, hypothe-\nsis: “A bird is above water.”), ETPA generates the\nexplanation “A swan is a bird.”, showing a good\nunderstanding of the relationship between the en-\ntities “swan” and “bird”. Similarly, interchanging\n“child” with “toddler” in (premise: “A small child\nwatches the outside world through a window.”, hy-\npothesis: “A small toddler watches the outside\nworld through a window.”) does not confuse the\nmodel, which outputs “Not every child is a tod-\ndler.” and “A toddler is a small child.”, respec-\ntively. Further investigation on whether the model\ncan be tricked on concepts where it seems to ex-\nhibit robustness, such as toddler or swan, are left\nfor future work.\nPREMISE: Biker riding through the forest.\nORIGINAL HYPOTHESIS: Man riding motorcycle on highway.\nPREDICTED LABEL: contradiction\nORIGINAL EXPLANATION: Biker and man are different.\nREVERSE HYPOTHESIS: A man rides his bike through the forest.\nPREDICTED LABEL: entailment\nREVERSE EXPLANATION: A biker is a man.\nPREMISE: A hockey player in helmet.\nORIGINAL HYPOTHESIS: They are playing hockey\nPREDICTED LABEL: entailment\nORIGINAL EXPLANATION: A hockey player in helmet is playing hockey.\nREVERSE HYPOTHESIS: A man is playing hockey.\nPREDICTED LABEL: neutral\nREVERSE EXPLANATION: A hockey player in helmet doesn’t imply playing\nhockey.\nPREMISE: A blond woman speaks with a group of young dark-haired female students carrying pieces of paper.\nORIGINAL HYPOTHESIS: A blond speaks with a group of young\ndark-haired woman students carrying pieces of paper.\nPREDICTED LABEL: entailment\nORIGINAL EXPLANATION: A woman is a female.\nREVERSE HYPOTHESIS:The students are all female.\nPREDICTED LABEL: neutral\nREVERSE EXPLANATION: The woman is not necessarily female.\nPREMISE: The sun breaks through the trees as a child rides a swing.\nORIGINAL HYPOTHESIS: A child rides a swing in the daytime.\nPREDICTED LABEL: entailment\nORIGINAL EXPLANATION: The sun is in the daytime.\nREVERSE HYPOTHESIS: The sun is in the daytime.\nPREDICTED LABEL: neutral\nREVERSE EXPLANATION: The sun is not necessarily in the daytime.\nPREMISE: A family walking with a soldier.\nORIGINAL HYPOTHESIS: A group of people strolling.\nPREDICTED LABEL: entailment\nORIGINAL EXPLANATION: A family is a group of people.\nREVERSE HYPOTHESIS: A group of people walking down a street.\nPREDICTED LABEL: contradiction\nREVERSE EXPLANATION: A family is not a group of people.\nTable 2: More examples of inconsistent explanations detected with our method.\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2019-10-07",
  "updated": "2020-05-02"
}