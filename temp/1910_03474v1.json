{
  "id": "http://arxiv.org/abs/1910.03474v1",
  "title": "Fine-grained Sentiment Classification using BERT",
  "authors": [
    "Manish Munikar",
    "Sushil Shakya",
    "Aakash Shrestha"
  ],
  "abstract": "Sentiment classification is an important process in understanding people's\nperception towards a product, service, or topic. Many natural language\nprocessing models have been proposed to solve the sentiment classification\nproblem. However, most of them have focused on binary sentiment classification.\nIn this paper, we use a promising deep learning model called BERT to solve the\nfine-grained sentiment classification task. Experiments show that our model\noutperforms other popular models for this task without sophisticated\narchitecture. We also demonstrate the effectiveness of transfer learning in\nnatural language processing in the process.",
  "text": "Fine-grained Sentiment Classiﬁcation using BERT\nManish Munikar∗, Sushil Shakya† and Aakash Shrestha‡\nDepartment of Electronics and Computer Engineering\nPulchowk Campus, Institute of Engineering, Tribhuvan University\nLalitpur, Nepal\n∗070bct520@ioe.edu.np, †070bct547@ioe.edu.np, ‡070bct501@ioe.edu.np\nAbstract—Sentiment classiﬁcation is an important process in\nunderstanding people’s perception towards a product, service,\nor topic. Many natural language processing models have been\nproposed to solve the sentiment classiﬁcation problem. However,\nmost of them have focused on binary sentiment classiﬁcation.\nIn this paper, we use a promising deep learning model called\nBERT to solve the ﬁne-grained sentiment classiﬁcation task.\nExperiments show that our model outperforms other popular\nmodels for this task without sophisticated architecture. We also\ndemonstrate the effectiveness of transfer learning in natural\nlanguage processing in the process.\nIndex Terms—sentiment classiﬁcation, natural language pro-\ncessing, language model, pretraining\nI. INTRODUCTION\nSentiment classiﬁcation is a form of text classiﬁcation in\nwhich a piece of text has to be classiﬁed into one of the\npredeﬁned sentiment classes. It is a supervised machine learn-\ning problem. In binary sentiment classiﬁcation, the possible\nclasses are positive and negative. In ﬁne-grained sentiment\nclassiﬁcation, there are ﬁve classes (very negative, negative,\nneutral, positive, and very positive). Fig 1 shows a black-box\nview of a ﬁne-grained sentiment classiﬁer model.\nReview\ntext\nSentiment label\n(0, 1, 2, 3, 4)\nSentiment\nClassiﬁer\nFig. 1. High-level black-box view of a sentiment classiﬁer showing its input\nand output.\nSentiment classiﬁcation model, like any other machine\nlearning model, requires its input to be a ﬁxed-sized vector\nof numbers. Therefore, we need to convert a text—sequence\nof words represented as ASCII or Unicode—into a ﬁxed-\nsized vector that encodes the meaningful information of the\ntext. Many statistical and deep learning NLP models have\nbeen proposed just for that. Recently, there has been an\nexplosion of developments in NLP as well as other deep\nlearning architectures.\nWhile transfer learning (pretraining and ﬁnetuning) has\nbecome the de-facto standard in computer vision, NLP is yet\nto utilize this concept fully. However, neural language models\nsuch as word vectors [1], paragraph vectors [2], and GloVe [3]\nhave started the transfer learning revolution in NLP. Recently,\nGoogle researchers published BERT (Bidirectional Encoder\nRepresentations from Transformers) [4], a deep bidirectional\nlanguage model based on the Transformer architecture [5],\nand advanced the state-of-the-art in many popular NLP tasks.\nIn this paper, we use the pretrained BERT model and ﬁne-\ntune it for the ﬁne-grained sentiment classiﬁcation task on the\nStanford Sentiment Treebank (SST) dataset.\nThe rest of the paper is organized into six sections. In\nSection II, we mention our motivation for this work. In\nSection III, we discuss related works. In Section IV, we\ndescribe the dataset we performed our experiments on. We\nexplain our model architecture and methodology in detail\nin Section V. Then we present and analyze our results in\nSection VI. Finally, we provide our concluding remarks in\nSection VII.\nII. MOTIVATION\nWe have been working on replicating the different research\npaper results for sentiment analysis, especially on the ﬁne-\ngrained Stanford Sentiment Treebank (SST) dataset. After the\npopularity of BERT, researchers have tried to use it on different\nNLP tasks, including binary sentiment classiﬁcation on SST-2\n(binary) dataset, and they were able to obtain state-of-the-art\nresults as well. But we haven’t yet found any experimentation\ndone using BERT on the SST-5 (ﬁne-grained) dataset. Because\nBERT is so powerful, fast, and easy to use for downstream\ntasks, it is likely to give promising results in SST-5 dataset\nas well. This became the main motivation for pursuing this\nwork.\nIII. RELATED WORK\nSentiment classiﬁcation is one of the most popular tasks in\nNLP, and so there has been a lot of research and progress\nin solving this task accurately. Most of the approaches have\nfocused on binary sentiment classiﬁcation, most probably\nbecause there are large public datasets for it such as the IMDb\nmovie review dataset [6]. In this section, we only discuss some\nsigniﬁcant deep learning NLP approaches applied to sentiment\nclassiﬁcation.\nThe ﬁrst step in sentiment classiﬁcation of a text is the\nembedding, where a text is converted into a ﬁxed-size vector.\nSince the number of words in the vocabulary after tokenization\nand stemming is limited, researchers ﬁrst tackled the problem\nof learning word embeddings. The ﬁrst promising language\narXiv:1910.03474v1  [cs.CL]  4 Oct 2019\nmodel was proposed by Mikolov et al. [1]. They trained con-\ntinuous semantic representation of words from large unlabeled\ntext that could be ﬁne-tuned for downstream tasks. Pennington\net al. [3] used a co-occurrence matrix and only trained on non-\nzero elements to efﬁciently learn semantic word embeddings.\nBojanowski et al. [7] broke words into character n-grams for\nsmaller vocabulary size and fast training.\nThe next step is to combine a variable number of word\nvectors into a single ﬁxed-size document vector. The trivial\nway is to take the sum or the average, but they don’t lose\nthe ordering information of words and thus don’t give good\nresults. Tai et al. [8] used recursive neural networks to compute\nvector representation of sentences by utilizing the intrinsic\ntree structure of natural language sentences. Socher et al. [9]\nintroduced a tensor-based compositionaity function for better\ninteraction between child nodes in recursive networks. They\nalso introduced the Stanford Sentiment Treebank (SST) dataset\nfor ﬁne-grained sentiment classiﬁcation. Tai et al. [10] applied\nvarious forms of long short-term memory (LSTM) networks\nand Kim [11] applied convolutional neural networks (CNN)\ntowards sentiment classiﬁcation.\nAll of the approaches mentioned above are context-free, i.e.,\nthey generate single word embedding for each word in the\nvocabulary. For instance, “bank“ would have the same repre-\nsentation in “bank deposit“ and “river bank“. Recent language\nmodel research has been trying to train contextual embeddings.\nPeters et al. [12] extracted context-sensitive features from left-\nto-right and right-to-left LSTM-based language model. Devlin\net al. [4] proposed BERT (Bidirectional Encoder Represen-\ntations from Transformers), an attention-based Transformer\narchitecture [5], to train deep bidirectional representations\nfrom unlabeled texts. Their architecture not only obtains state-\nof-the-art results on many NLP tasks but also allows a high\ndegree of parallelism since it is not based on sequential or\nrecurrent connections.\nIV. DATASET\nStanford Sentiment Treebank (SST) [9] is one of the most\npopular publicly available datasets for ﬁne-grained sentiment\nclassiﬁcation task. It contains 11,855 one-sentence movie\nreviews extracted from Rotten Tomatoes. Not only that, each\nsentence is also parsed by the Stanford constituency parser\n[13] into a tree structure with the whole sentence as the root\nnode and the individual words as leaf nodes. Moreover, each\nnode is labeled by at least three humans. In total, SST contains\n215,154 unique manually labeled texts of varying lengths. Fig\n2 shows a sample review from the SST dataset in a parse-\ntree structure with all its nodes labeled. Therefore, this dataset\ncan be used to train models to learn the sentiment of words,\nphrases, and sentences together.\nThere are ﬁve sentiment labels in SST: 0 (very negative),\n1 (negative), 2 (neutral), 3 (positive), and 4 (very positive). If\nwe only consider positivity and negativity, we get the binary\nSST-2 dataset. If we consider all ﬁve labels, we get SST-5. For\nthis research, we evaluate the performance of various models\n–\n0\n0\nThis\n0\nﬁlm\n–\n–\n–\n0\ndoes\n0\nn’t\n0\n+\ncare\n+\n0\nabout\n+\n+\n+\n+\n+\ncleverness\n0\n,\n0\nwit\n0\nor\n+\n0\n0\nany\n0\n0\nother\n+\nkind\n+\n0\nof\n+\n+\nintelligent\n+ +\nhumor\n0\n.\nFig. 2. A sample sentence from the SST dataset. (Source: Adapted from [9].)\non all nodes as well as on just the root nodes, and on both\nSST-2 and SST-5.\nV. METHODOLOGY\nSentiment classiﬁcation takes a natural language text as\ninput and outputs a sentiment score ∈{0, 1, 2, 3, 4}. Our\nmethod has three stages from input sentence to output score,\nwhich are described below. We use pretrained BERT model\nto build a sentiment classiﬁer. Therefore, in this section, we\nbrieﬂy explain BERT and then describe our model architecture.\nA. BERT\nBERT (Bidirectional Encoder Representations from Trans-\nformers is an embedding layer designed to train deep bidi-\nrectional representations from unlabeled texts by jointly con-\nditioning on both left and right context in all layers. It is\npretrained from a large unsupervised text corpus (such as\nWikipedia dump or BookCorpus) using the following objec-\ntives:\n• Masked word prediction: In this task, 15% of the words\nin the input sequence are masked out, the entire sequence\nis fed to a deep bidirectional Transfomer [5] encoder, and\nthen the model learns to predict the masked words.\n• Next sentence prediction: To learn the relationship be-\ntween sentences, BERT takes two sentences A and B as\ninputs and learns to classify whether B actually follows\nA or is it just a random sentence.\nUnlike traditional sequential or recurrent models, the atten-\ntion architecture processes the whole input sequence at once,\nenabling all input tokens to be processed in parallel. The layers\nof BERT architecture are visualized in Fig 3. Pretrained BERT\nmodel can be ﬁne-tuned with just one additional layer to obtain\nstate-of-the-art results in a wide range of NLP tasks [4].\nThere are two variants for BERT models: BERTBASE and\nBERTLARGE. The difference between them is listed in Table I.\n1) Input format: BERT requires its input token sequence\nto have a certain format. First token of every sequence\nshould be [CLS] (classiﬁcation token) and there should be\na [SEP] token (separation token) after every sentence. The\noutput embedding corresponding to the [CLS] token is the\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\n...\n...\n T1\nT2\nTN\n...\n E1\nE2\n EN\n...\nFig. 3. BERT Architecture, where En is the n-th token in the input sequence,\nTrm is the Transformer block, and Tn is the corresponding output embedding.\n(Source: Adapted from [4].)\nTABLE I\nBERTBASE VS. BERTLARGE.\nBERTBASE\nBERTLARGE\nNo. of layers (Transformer blocks)\n12\n24\nNo. of hidden units\n768\n1024\nNo. of self-attention heads\n12\n16\nTotal trainable parameters\n110M\n340M\nsequence embedding that can be used for classifying the whole\nsequence.\nB. Preprocessing\nWe perform the following preprocessing steps on the review\ntext before we feed them into out model.\n1) Canonicalization: First, we remove all the digits, punc-\ntuation symbols and accent marks, and convert everything to\nlowercase.\n2) Tokenization: We then tokenize the text using the Word-\nPiece tokenizer [14]. It breaks the words down to their preﬁx,\nroot, and sufﬁx to handle unseen words better. For example,\nplaying →play + ##ing.\n3) Special token addition: Finally, we add the [CLS] and\n[SEP] tokens at the appropriate positions.\nC. Proposed Architecture\nWe build a simple architecture with just a dropout regular-\nization [15] and a softmax classiﬁer layers on top of pretrained\nBERT layer to demonstrate that BERT can produce great re-\nsults even without any sophisticated task-speciﬁc architecture.\nFig 4 shows the overall architecture of our model. There\nare four main stages. The ﬁrst is the proprocessing step as\ndescribed earlier. Then we compute the sequence embedding\nfrom BERT. We then apply dropout with a probability factor\nof 0.1 to regularize and prevent overﬁtting. Dropout is only\napplied in training phase and not in inference phase. Finally,\nthe softmax classiﬁcation layer will output the probabilities\nof the input text belonging to each of the class labels such\nPreprocessing\nReview text\nBERT Embedding\nDropout\nSoftmax Classiﬁer\nSentiment label\n(0, 1, 2, 3, 4)\nFig. 4. Proposed architecture for ﬁne-grained sentiment classiﬁcation.\nthat the sum of the probabilities is 1. The softmax layer is\njust a fully connected neural network layer with the softmax\nactivation function. The softmax function σ : RK →RK is\ngiven in (1).\nσ(z)i =\nezi\nPK\nj=1 ezj for i = 1, . . . , K\n(1)\nwhere z = (z1, . . . , zK) ∈RK is the intermediate output of\nthe softmax layer (also called logits). The output node with\nthe highest probability is then chosen as the predicted label\nfor the input.\nVI. EXPERIMENTS AND RESULTS\nIn this section, we discuss the results of our model and\ncompare with it some of the popular models that solve the\nsame problem, i.e., sentiment classiﬁcation on the SST dataset.\nA. Comparison Models\n1) Word embeddings: In this method, the word vectors\npretrained on large text corpus such as Wikipedia dump are\naveraged to get the document vector, which is then fed to the\nsentiment classiﬁer to compute the sentiment score.\n2) Recursive networks: Various types of recursive neural\nnetworks (RNN) have been applied on SST [9]. We compare\nour results with the standard RNN and the more sophisticated\nRNTN. Both of them were trained on SST from scratch,\nwithout pretraining.\n3) Recurrent networks: Sophisticated recurrent networks\nsuch as left-to-right and bidrectional LSTM networks have\nalso been applied on SST [10].\n4) Convolutional networks: In this approach, the input\nsequences were passed through a 1-dimensional convolutional\nneural network as feature extractors [11].\nTABLE II\nACCURACY (%) OF OUR MODELS ON SST DATASET COMPARED TO\nOTHER MODELS.1\nModel\nSST-2\nSST-5\nAll\nRoot\nAll\nRoot\nAvg word vectors [9]\n85.1\n80.1\n73.3\n32.7\nRNN [8]\n86.1\n82.4\n79.0\n43.2\nRNTN [9]\n87.6\n85.4\n80.7\n45.7\nParagraph vectors [2]\n–\n87.8\n–\n48.7\nLSTM [10]\n–\n84.9\n–\n46.4\nBiLSTM [10]\n–\n87.5\n–\n49.1\nCNN [11]\n–\n87.2\n–\n48.0\nBERTBASE\n94.0\n91.2\n83.9\n53.2\nBERTLARGE\n94.7\n93.1\n84.2\n55.5\n1 Some values are blank in “All” columns because the original authors\nof those paper did not publish their result on all phrases.\nB. Evaluation Metric\nSince the dataset has roughly balanced number of samples\nof all classes, we directly use the accuracy measure to evaluate\nthe performance of our model and compare it with other\nmodels. The accuracy is deﬁned simply as follows:\naccuracy = number of correct predictions\ntotal number of predictions\n∈[0, 1]\n(2)\nC. Results\nThe result and comparisons are shown in Table II. It shows\nthe accuracy of various models on SST-2 and SST-5. It\nincludes results for all phrases as well as for just the root\n(whole review). We can see that our model, despite being a\nsimple architecture, performs better in terms of accuracy than\nmany popular and sophisticated NLP models.\nVII. CONCLUSION\nIn this paper, we used the pretrained BERT model and ﬁne-\ntuned it for the ﬁne-grained sentiment classiﬁcation task on the\nSST dataset. Even with such a simple downstream architecture,\nour model was able to outperform complicated architectures\nlike recursive, recurrent, and convolutional neural networks.\nThus, we have demonstrated the transfer learning capability in\nNLP enabled by deep contextual language models like BERT.\nACKNOWLEDGMENT\nWe would like to express our gratitude towards Prof. Dr.\nShashidhar Ram Joshi for his invaluable advice and guidance\non this paper. We also thank all the helpers and reviewers for\ntheir valuable input to this work.\nREFERENCES\n[1] T. Mikolov, K. Chen, G. S. Corrado, and J. Dean, “Efﬁcient estimation of\nword representations in vector space,” CoRR, vol. abs/1301.3781, 2013.\n[2] Q. Le and T. Mikolov, “Distributed representations of sentences and\ndocuments,” in International Conference on Machine Learning, 2014,\npp. 1188–1196.\n[3] J. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors\nfor word representation,” in Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), 2014,\npp. 1532–1543.\n[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of\ndeep bidirectional transformers for language understanding,” in NAACL-\nHLT, 2018.\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems, 2017, pp. 5998–6008.\n[6] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts,\n“Learning word vectors for sentiment analysis,” in Proceedings of the\n49th Annual Meeting of the Association for Computational Linguistics:\nHuman Language Technologies.\nAssociation for Computational Lin-\nguistics, June 2011, pp. 142–150.\n[7] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word\nvectors with subword information,” Transactions of the Association for\nComputational Linguistics, vol. 5, pp. 135–146, 2017.\n[8] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning,\n“Semi-supervised recursive autoencoders for predicting sentiment dis-\ntributions,” in Proceedings of the Conference on Empirical Methods\nin Natural Language Processing.\nAssociation for Computational\nLinguistics, 2011, pp. 151–161.\n[9] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng,\nand C. Potts, “Recursive deep models for semantic compositionality\nover a sentiment treebank,” in Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), 2013,\npp. 1631–1642.\n[10] K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic rep-\nresentations from tree-structured long short-term memory networks,”\nin Proceedings of the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 1: Long Papers), 2015, pp.\n1556–1566.\n[11] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,”\narXiv preprint arXiv:1408.5882, 2014.\n[12] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations,” arXiv\npreprint arXiv:1802.05365, 2018.\n[13] D. Chen and C. Manning, “A fast and accurate dependency parser using\nneural networks,” in Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2014, pp. 740–750.\n[14] M. Schuster and K. Nakajima, “Japanese and korean voice search,” in\n2012 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2012, pp. 5149–5152.\n[15] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: A simple way to prevent neural networks from over-\nﬁtting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp.\n1929–1958, 2014.\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-10-04",
  "updated": "2019-10-04"
}