{
  "id": "http://arxiv.org/abs/1910.03806v1",
  "title": "Is Multilingual BERT Fluent in Language Generation?",
  "authors": [
    "Samuel Rönnqvist",
    "Jenna Kanerva",
    "Tapio Salakoski",
    "Filip Ginter"
  ],
  "abstract": "The multilingual BERT model is trained on 104 languages and meant to serve as\na universal language model and tool for encoding sentences. We explore how well\nthe model performs on several languages across several tasks: a diagnostic\nclassification probing the embeddings for a particular syntactic property, a\ncloze task testing the language modelling ability to fill in gaps in a\nsentence, and a natural language generation task testing for the ability to\nproduce coherent text fitting a given context. We find that the currently\navailable multilingual BERT model is clearly inferior to the monolingual\ncounterparts, and cannot in many cases serve as a substitute for a well-trained\nmonolingual model. We find that the English and German models perform well at\ngeneration, whereas the multilingual model is lacking, in particular, for\nNordic languages.",
  "text": "Is Multilingual BERT Fluent in Language Generation?\nSamuel R¨onnqvist∗Jenna Kanerva∗Tapio Salakoski Filip Ginter∗\nTurkuNLP\nDepartment of Future Technologies\nUniversity of Turku, Finland\n{saanro,jmnybl,sala,figint}@utu.fi\nAbstract\nThe multilingual BERT model is trained\non 104 languages and meant to serve as a\nuniversal language model and tool for en-\ncoding sentences. We explore how well\nthe model performs on several languages\nacross several tasks: a diagnostic classiﬁ-\ncation probing the embeddings for a par-\nticular syntactic property, a cloze task test-\ning the language modelling ability to ﬁll in\ngaps in a sentence, and a natural language\ngeneration task testing for the ability to\nproduce coherent text ﬁtting a given con-\ntext. We ﬁnd that the currently available\nmultilingual BERT model is clearly infe-\nrior to the monolingual counterparts, and\ncannot in many cases serve as a substitute\nfor a well-trained monolingual model. We\nﬁnd that the English and German models\nperform well at generation, whereas the\nmultilingual model is lacking, in particu-\nlar, for Nordic languages.1\n1\nIntroduction\nThe language representation model BERT (Bidi-\nrectional Encoder Representations from Trans-\nformers) has been shown to achieve state-of-the-\nart performance when ﬁne-tuned on a range of\ndownstream tasks related to language understand-\ning (Devlin et al., 2018), and recently also lan-\nguage generation.\nIn addition to downstream\napplications, many recent studies have explored\nmore directly how various types of linguistic in-\nformation is captured in BERT’s representations.\nHowever, all such studies we are aware of\nare conducted for English using the monolin-\n∗The marked authors contributed equally to this paper.\n1The code of the experiments in the paper is available at:\nhttps://github.com/TurkuNLP/bert-eval\ngual BERT model as the availability of pre-\ntrained BERT models for other languages is ex-\ntremely scarce. For the vast majority of languages,\nthe only option is the multilingual BERT model\ntrained jointly on 104 languages. In “coffee break”\ndiscussions, it is often mentioned that the multi-\nlingual BERT model lags behind the monolingual\nmodels in terms of quality and cannot serve as a\ndrop-in replacement.\nIn this paper, we therefore set out to test\nthe multilingual model on several tasks and sev-\neral languages (primarily Nordic), to establish\nwhether, and to what extent this is the case, as well\nas to establish at least an order-of-magnitude ex-\npectation of the performance of the present mul-\ntilingual BERT model on these tasks and lan-\nguages. It must be stressed that this paper deals\nwith the particular multilingual model distributed\nby the BERT creators, rather than the more general\nquestion of comparison of the multilingual and\nmonolingual training schedule.\nStudying those\nquestions would necessitate training multilingual\nBERT models with resource requirements far be-\nyond those at our disposal.\nWe put a particular focus on the natural lan-\nguage generation (NLG) task, which we hypoth-\nesize requires a deeper understanding of the lan-\nguage in question on the side of the model. We\ntake English and German, for which monolingual\nversions of BERT are available, as reference lan-\nguages, in order to compare how they perform in\nthe mono- vs. multilingual settings. Furthermore,\nwe perform experiments with the Nordic lan-\nguages of Danish, Finnish, Norwegian (Bokm˚al\nand Nynorsk) and Swedish, with in-depth eval-\nuations on Finnish and Swedish, as well as the\nabovementioned two reference languages.\n2\nRelated Work\nA BERT model is comprised of several layers of\nstacked Transformer networks\n(Vaswani et al.,\n2017), each providing representations of both the\ninput sequence and its individual tokens.\nThe\nmodel incorporates a tokenizer that splits an input\nsentence into words, or subword units for words\nor word forms that are relatively infrequent in the\ntraining data.\nSeveral recent studies have explored how BERT\ncaptures linguistic information in English, and\nhow it is distributed across layers. (Tenney et al.,\n2019; Jawahar et al., 2019; Clark et al., 2019)\nA particular line of inquiry has focused on how\nmuch hierarchical understanding of a language\nand knowledge of the syntactic structure is cap-\ntured in the word representations of the monolin-\ngual English BERT model. In Goldberg (2019),\nthe BERT models are shown to perform well on\ncapturing several syntactic phenomena of the En-\nglish language.\nThe paper shows the model to\nfavor the correct subject-verb agreement over the\nwrong one even if the input is crafted to mislead\nthe model with agreement attractors, i.e. an inter-\nvening subordinate clause with opposite number\nof the subject. BERT is also shown to perform\nwell on the agreement task even if tokens are ran-\ndomly substituted from the same part-of-speech\ncategory, making the input semantically meaning-\nless while preserving the syntactic structure.\nSimilarly, Ettinger (2019) evaluates the BERT\nmodel\non\nseveral\nEnglish\npsycholinguistic\ndatasets, where the model is shown generally\nbeing able to distinguish a good completion\nfrom a bad one, while still failing in some more\ncomplex categories, for example being insensitive\nto negation.\nLin et al. (2019) uses a diagnostic classiﬁer to\nstudy to which extent syntactic or positional infor-\nmation can be predicted from the English BERT\nembeddings, and how this information is carried\nthrough the different layers.\nThe multilingual BERT model is studied in the\ncontext of zero-shot cross-lingual transfer, where\nit is shown to perform competitively to other trans-\nfer models. (Pires et al., 2019; Wu and Dredze,\n2019)\nText generation with BERT is introduced by\nWang and Cho (2019), who demonstrate several\ndifferent algorithms to generate language with a\nBERT model. They demonstrate that BERT even\nthough not being trained on an explicit language\ngeneration objective, is capable of generating co-\nherent, varied language.\nLanguage\nBERT\nTest acc.\nBaseline\nEnglish\nmono\n86.03\n54.93\nmulti\n87.82\n54.44\nGerman\nmono\n97.27\n69.61\nmulti\n95.29\n69.19\nDanish\nmulti\n89.96\n53.25\nFinnish\nmulti\n93.20\n50.54\nNor. (Bokm˚al)\nmulti\n93.67\n56.19\nNor. (Nynorsk)\nmulti\n94.44\n53.18\nSwedish\nmulti\n93.00\n62.09\nTable 1: Diagnostic classiﬁer results. Auxiliary\nclassiﬁcation task accuracies and majority class\nbaselines for all languages.\n3\nExperiments\nWe evaluate the BERT models on 6 languages,\nEnglish, German, Swedish, Finnish, Danish, and\nNorwegian (Bokm˚al and Nynorsk), and three dif-\nferent tasks. In addition to automatic metrics, the\ngenerated output is manually evaluated for En-\nglish, German, Swedish, and Finnish, the four lan-\nguages that at least one of the authors is ﬂuent in,\nand therefore comfortable evaluating. For English\nand German there are monolingual BERT mod-\nels available, which we use as references to eval-\nuate the performance of the multilingual BERT\nmodel.2 We further compare performance among\nthese languages and the four Nordic languages in\norder to assess its utility for such relatively low-\nresource languages. In all evaluation tasks, we use\ndata from the Universal Dependencies (UD) ver\n2.4 treebanks (Nivre et al., 2016, 2019) for the lan-\nguages in question.3\n3.1\nDiagnostic Classiﬁer\nAs an initial experiment, we train a diagnostic\nclassiﬁer to predict whether an auxiliary is the\nmain auxiliary of its sentence, in order to assess\nhow well the BERT encodings represent elemen-\ntary linguistic information including hierarchical\nunderstanding of a sentence. The task is inspired\nby Lin et al. (2019) who use it as one way of\ntesting what kind of linguistic knowledge BERT\n2For multilingual and English monolingual experi-\nments we used the ofﬁcial models by the original BERT\nauthors, namely bert-base-multilingual-cased\nand bert-base-uncased.\nFor German monolin-\ngual experiments we use the model provided by Deepset\n(bert-base-german-cased).\n3Treebanks are English-EWT, German-HDT (part a),\nSwedish-Talbanken, Finnish-TDT, Danish-DDT, Norwegian-\nBokmaal, and Norwegian-Nynorsk.\nis able to encode. Speciﬁcally, they use it as a\nproxy for assessing whether BERT has a hierar-\nchical representation of sentences, as it is neces-\nsary information for differentiating between main\nand subordinate clause or coordinate clause auxil-\niaries.\nAll words marked with the part-of-speech tag\nAUX in the treebank data are taken as prediction\ncandidates, where the target is a binary classiﬁca-\ntion as to whether the auxiliary is dependent on\nthe root token of the sentence or not. The input of\nthe classiﬁer is the ﬁnal-layer BERT embedding\nfor the auxiliary. In case the auxiliary token is\ntokenized into multiple subword units, each sub-\nword representation is fed as a separate instance,\nand thus classiﬁed independently. We expect each\nsubword embedding to encode the relevant knowl-\nedge of both the whole word and its function in the\nsentence.\nThe classiﬁer consists of 768 input units corre-\nsponding to the BERT base embedding size and a\nsoftmax layer. The model is trained separately for\nall languages and available BERT model conﬁgu-\nrations, using treebank training sections, and SGD\notimizer for 50 epochs. For improved comparabil-\nity, the train set size is capped for all languages to\nthat of the smallest treebank (Swedish), for which\nwe were able to extract 3031 training examples.\nThe treebank test sets yield 1002–1217 examples,\nwith the exception of Danish with 515 examples.\nThe results evaluated on the treebank test sets\nare listed in Table 1, where we measure subword\nclassiﬁcation accuracy. The majority class base-\nline frequencies are listed as reference; they tend\nto be relatively balanced, although somewhat tilted\ntowards main auxiliaries. There is a notable 2 per-\ncentage point decrease for German with multilin-\ngual BERT, whereas English exhibits a 1.8 point\nincrease. Comparison between languages is prob-\nlematic, but we observe that all perform relatively\nwell on the task.4\nAlbeit our results not being\ndirectly comparable with Lin et al., our ﬁndings\nare in line with their work, indicating BERT being\nable to encode hierarchical sentence information\nin all languages, and most interestingly, the same\nholds also for the multilingual BERT model.\n4The slight variation in baseline between models for the\nsame language is likely inﬂuenced by differing tokenization.\nMono\nMulti\nEnglish\n45.92\n33.94\nGerman\n43.93\n28.10\nSwedish\n22.30\nFinnish\n14.56\nDanish\n25.07\nNorwegian (Bokm˚al)\n25.21\nNorwegian (Nynorsk)\n22.28\nTable 2: Results for the cloze test in terms of sub-\nword predictions accuracy.\n3.2\nCloze Test\nMoving towards natural language generation, and\nto evaluate the BERT models with respect to their\noriginal training objective, we employ a cloze test,\nwhere words are randomly masked and predicted\nback. We mask a random 15% of words in each\nsentence, and, in case a word is composed of sev-\neral subwords, all subwords are masked for an eas-\nier and more meaningful evaluation. All masked\npositions are predicted at once in the same man-\nner as done in the BERT pretraining (i.e. no itera-\ntive prediction of one position per time step). As a\nsource of sentences, we use the training sections of\nthe treebanks, limited to sentences of 5–50 tokens\nin length.\nThe results are shown in Table 2, where we\nmeasure subword level prediction accuracy, i.e.\nhow many times the model gives the highest con-\nﬁdence score for the original subword.\nOver-\nall, we ﬁnd that the multilingual model substan-\ntially lags behind the monolingual variants (at 15–\n34% vs. 44–45% accuracy), even though the per-\nformance at worst is far from trivial.\nWe also\nobserve a notable difference in performance of\nthe multilingual model across the languages, be-\ning able to correctly predict between 15% and\n34% of the masked subwords. English and Ger-\nman score highest also in the multilingual set-\nting, whereas the Scandinavian languages perform\nsomewhat worse, but similarly among themselves.\nFinnish stands out as the most challenging.\nIn order to gain a better understanding of the\npredictions, we perform a manual evaluation on\nfour languages to observe whether the model is\nable to ﬁll the gaps with plausible predictions\nalthough differing from the original. We manually\ncategorize each predicted word into one of the\nfollowing categories:\nmatch\nmismatch\ncopy\ngibb\nEng\nmono\n88%\n9%\n1%\n1%\nmulti\n72%\n15%\n8%\n6%\nGer\nmono\n82%\n12%\n1%\n5%\nmulti\n69%\n15%\n6%\n10%\nFin\nmulti\n42%\n15%\n3%\n39%\nSwe\nmulti\n56%\n19%\n2%\n23%\nTable 3: Manual evaluation of words generated in\nthe cloze test.\n• match: A real word ﬁtting the context both\ngrammatically and semantically\n• mismatch: A real word that does not ﬁt the\ncontext\n• copy: An unnatural repetition of a word ap-\npearing in the nearby context\n• gibberish:\nSubwords do not form a real\nword, or the prediction forms a meaningless\nsequence of tokens (e.g. sequence of punctu-\nation tokens)\nAn example prediction of each category is given\nin Figure 1 and the evaluation results are summa-\nrized in Table 3. These even further demonstrate\nthe capability of the monolingual models, with\n82–88% of the generated words ﬁtting the context\nboth syntactically and semantically, i.e. being an\nacceptable substitution for the masked word in the\ngiven context.\nBy contrast, the matches decrease for German\nand English, using the multilingual model, to 69%\nand 72% respectively. Finnish and Swedish per-\nform signiﬁcantly worse, with match rates at 42%\nand 56%. The evaluation is based on 50–100 sen-\ntences per language and model, and about 100–\n200 predicted words in each case.\nThe other categories display similar trends: the\nsemantically or syntactically mismatching words\nincrease for the multilingual model, and in partic-\nular the amount of gibberish surges for the Nordic\nlanguages.\nThe fact that prediction in Finnish\nexhibits almost twice as much gibberish as in\nSwedish is likely inﬂuenced by the morphological\nrichness of Finnish, resulting in words to gener-\nally be composed of more subword units and the\nlikelihood of predicting non-existent words being\nhigher. An interesting trend for the two Nordic\nlanguages, especially strongly seen in Finnish, is\nthe predictions mostly falling into two distinct\non-top\noff-top\ncopy\ngibb\nEng\nmono\n50%\n21%\n5%\n24%\nmulti\n7%\n2%\n38%\n53%\nGer\nmono\n67%\n28%\n3%\n2%\nmulti\n17%\n13%\n48%\n22%\nFin\nmulti\n19%\n2%\n37%\n43%\nSwe\nmulti\n10%\n5%\n47%\n37%\nTable 4: Manual evaluation of generated text from\nthe mono- and multilingual models.\nThe cate-\ngories are, in order, on-topic original text, off-\ntopic original text, copy of the context, and gib-\nberish. N is 55–60 for all tests.\nends of the evaluation scale, 42% being perfectly\nacceptable substitutions, while 39% being gibber-\nish. The likely explanation noticed during man-\nual evaluation is the model being quite capable\npredicting natural output in the place of masked\nfunction words, while completely failing to pre-\ndict anything reasonable for masked content words\nforming longer subword sequences.\nExamples of the model predictions in this\ntask are given in Figure 2 for English, German,\nSwedish and Finnish, generated using both mono-\nlingual and multilingual models.\n4\nSentence Generation\nTo evaluate and compare the text generation abili-\nties of the models, we employ the method recently\nintroduced by Wang and Cho (2019) which en-\nables BERT to be used for text generation.5 In par-\nticular, we use the Gibbs-sampling-based method,\nreported in the paper to give the best results. In\nthis method, a sequence of [MASK] symbols is\ngenerated and BERT is used for a number of itera-\ntions to generate new subwords at random individ-\nual positions of this sequence until the maximum\nnumber of iterations (500 by default), or conver-\ngence are reached. This method is shown by Wang\nand Cho to produce varied output of good quality,\neven though not entirely competitive with the fa-\nmous GPT-2 model (Radford et al., 2019). Most\nimportantly for our objective, this method allows\nus to probe the model’s ability to generate longer\nsequences of the language and to compare the rela-\ntive differences between the monolingual and mul-\ntilingual pre-trained BERT models.\n5Note that some of the underlying assumptions of this pa-\nper were later corrected by the authors http://tiny.cc/\ncho-correction\nLabels\nGenerated\nmatch\nQuestion[ing∼about] the sinking of the Titanic?\nmismatch\nThose [they∼ones] are quite small.\nmatch, copy\nI [felt∼understand] that it is a [process∼competitive] process...\ngibberish\nA full [- of and∼substantive] reconciliation of cash and funding accounts\nFigure 1: Example generation of each category used in the manual evaluation of Cloze task predic-\ntions. Examples are generated by the English multilingual model. The format of the masked words is\n[predicted∼gold]. Examples for other languages and models are shown in Figure 2.\nLang\nModel\nGeneration\nEng\nmono\nregarding [the∼those] rumors about [people∼wolves] living in yellowstone prior to\nthe ofﬁcial reintroduction?\nmulti\nRegarding [the∼those] rumors about [thes∼wolves] living in Yellowstone prior to\nthe ofﬁcial reintroduction?\nmono\nwe [went∼got] to [work∼talking] and he got me set up and i [just∼test] drove\nwith craig and i fell head over heels for this car [and∼all] i kept saying, ”[but∼was]\ni gotta have it [.∼!]”\nmulti\nWe [went∼got] to [Craig∼talking] and he got me set up and I [went∼test]\ndrove with Craig and I fell head over heels for this car [and∼all] I kept saying,\n”[And∼was] I gotta have it [.∼!]”\nGer\nmono\n[Voraussetzung∼Kennzeichen] f¨ur eine [intensivere∼krankhafte] Nutzung des\nInternets sei unter anderem ein deutlicher R¨uckzug [aus∼aus] dem sozialen Leben.\nmulti\n[Ein Vorsetzung∼Kennzeichen] f¨ur eine [gewise∼krankhafte] Nutzung des Inter-\nnets sei unter anderem ein deutlicher R¨uckzug [aus∼aus] dem sozialen Leben.\nmono\n”[Es∼Das]\nist\neine\nRevolution\nf¨ur\ndie\nmobile\nKommunikation”,\nmeint\n[Professor∼Vizepr¨asident] Mike Zaﬁrovski.\nmulti\n”[Es∼Das] ist eine Revolution f¨ur die mobile Kommunikation”, meint\n[der -er∼Vizepr¨asident] Mike Zaﬁrovski.\nFin\nmulti\nStokessa Gallagher [oli∼pelasi] enimm¨akseen laiturina, joka ei ollut h¨anen\n[val¨aa¨aa∼lempipaikkojaan].\nmulti\nNykyhetki laajenee vauhdilla, joka [johtaa∼saa] tulevaisuuden kutistumaan l¨ahes\nmenneisyyden kaltaiseksi [. .ksi . . , ,∼makrok¨a¨api¨oksi] [joka∼joka] vieritt¨a¨a\nkvarkkia alas leskenlehden ter¨a¨a salaiseen maailmaansa.\nSwe\nmulti\nMen du [m˚aste∼kan] f˚a ett givande grepp p˚a staden [fr˚an∼och] dess [, och∼milj¨o]\nocks˚a fr˚an andra utg˚angspunkter.\nmulti\n˚Ar 1951 [stod∼gjorde] den engelske [psnologen .∼l¨akaren] J. Bowlby f¨or WHO:s\n[forsknings f¨or en∼r¨akning] en sammanst¨allning av dittills gjorda unders¨okningar\n¨over hur [barn¨ar barn∼sp¨ada] och sm˚a barn, som f¨or n˚agon tid helt skilts\n[fr˚an∼fr˚an] sin mor, utvecklas.\nFigure 2: Example generations of the cloze prediction task for English, German, Finnish and Swedish.\nThe format of the masked words is [predicted∼gold].\nModel\nJudgement\nGenerated text in context\nMono\non-topic\nIt came out better than I even imagined . how did this tattoo artist come up\nwith the idea of a quality tattoo ? I would highly recommend this shop to\nanyone looking to get a quality tattoo done .\nMulti\ncopy\nIt came out better than I even imagined .\n. . this shop to anyone looking\nto get a quality tattoo done . I would highly recommend this shop to anyone\nlooking to get a quality tattoo done .\nMono\non-topic\nHalbleiter-Riese National hatte die zwei Jahre zuvor akquirierte Chipschmiede\nCyrix k¨urzlich an den taiwanischen Chipsatzproduzenten VIA weiter verkauft\n. VIA ist die weltweit t¨atige Tochter von Cyrix ( GM ) . 130-Nanometer-\nChipfertigung l¨auft an\nMulti\ncopy\nHalbleiter-Riese National hatte die zwei Jahre zuvor akquirierte Chipschmiede\nCyrix k¨urzlich an den taiwanischen Chipsatzproduzenten VIA weiter verkauft\n. 128 - Nanometer - Chipfertigung l¨auft an - an - an . 130-Nanometer-\nChipfertigung l¨auft an\nMulti\non-topic\nKeskuspankki sitoi Islannin kruunun kurssin euroon kaksi p¨aiv¨a¨a sitten , jol-\nloin eurolla sai 131 kruunua . 1900 - luvun alkuvuonna eurolla sai 140 kru-\nunua . K¨ayt¨ann¨oss¨a t¨am¨a merkitsi vakavaa iskua Islannin taloudelle .\nMulti\non-topic\nI stadsmilj¨on utg¨or parker och gr¨onomr˚aden en viktig del i v˚ara dagar . Stallar\ni naturen utg¨or ocks˚a en viktig del i v˚ara dagar . Men naturen l˚ag i omedel-\nbar n¨arhet , och stallar och ladug˚ardar var l˚angt in p˚a 1800-talet vanliga i den\nagrara svenska sm˚astaden\nFigure 3: Example sentence generations (in bold) together with the manual quality judgements and the\ncontext provided in generation, for English, German, Finnish and Swedish.\nFor each language, we randomly sample 30\ndocuments from the Universal Dependencies ver-\nsion 2.4 training data, and from each document we\nrandomly select 2 sentences. For each of these\nsentences, we provide on input the preceding and\nfollowing sentence as the left and right context for\nthe model. Between these contexts, we use the\nparallel-sequential method of Wang and Cho to\ngenerate text which is as long, in terms of subword\ncount, as the original sentence, restricting never-\ntheless to a minimum of 5 subwords and a maxi-\nmum of 15 subwords. The maximum of 15 sub-\nwords was selected in preliminary experiments,\nas for considerably longer sequences, the model\nstarts deviating from the seeded context and often\nfails to even stick to the language of the seed, ow-\ning to the fact that BERT is not trained to deal with\nlong sequences of consecutive masked positions.\nSubsequently, we manually evaluate the gener-\nated texts in context, and classify them into the\nfollowing categories:\n• on-topic: original, intelligible sentence or\nphrase without excessive errors, essentially\nﬁtting the context\n• off-topic: original, intelligible sentence or\nphrase without excessive errors, not ﬁtting\nthe context\n• copy: unoriginal text composed for the most\npart of verbatim copied sections of the con-\ntext, often containing grammatical and ﬂow\nerrors\n• gibberish: unintelligible sequence of words\nand characters, text with excessive grammat-\nical and ﬂow errors\nThe results of the evaluation are shown in Ta-\nble 4. A comparison against an existing monolin-\ngual model is possible only for English and Ger-\nman.\nBoth for English and German, there is a\nstriking difference, where the monolingual models\ngenerate a substantially larger proportion of orig-\ninal on-topic text, compared to the multilingual\nmodel which, for the most part, copies sections of\nthe context or produces gibberish. Especially for\nGerman, the monolingual model generates a sub-\njectively very good output, with next to no copy-\ning and gibberish. For Finnish and Swedish, we\ncan only report on the multilingual model, show-\ning the same tendencies to copy or produce gibber-\nish as for English and German. Overall, the results\nin Table 4 demonstrate that the multilingual model\nis clearly inferior to the monolingual counterparts\nand unsuitable for the generation task.\nFigure 3 lists a few examples of generated sen-\ntences for the four languages and the available\nmodels. For English and German it illustrates the\ncomparably worse performance of the multilin-\ngual model, as the generation is mostly copying\nfrom the context rather than creating original and\nﬂuent text that ﬁts the context. For Finnish and\nSwedish, it shows cases where the generation has\nbeen able to ﬁll in sentences that are correct and\nthat to some extent relates to the context.\n5\nDiscussion and Conclusions\nIn this paper, we set out to establish whether the\nmultilingual BERT model, as distributed, is of suf-\nﬁcient quality to be considered an effective sub-\nstitute for a dedicated, monolingual model for the\ngiven language.\nWe tested the model on three\ntasks of increasing difﬁculty: a simple syntactic\nclassiﬁcation task, a cloze test, and full text gen-\neration. We found that the multilingual model no-\ntably lags behind the available monolingual mod-\nels and the gap opens as the complexity of the\ntask increases. While on the syntactic classiﬁca-\ntion task, all models perform comparatively well,\nin the cloze test there is a notable difference. In\nthe full text generation the multilingual model out-\nputs are practically useless, while the monolingual\nmodels produce very good, and in the case of Ger-\nman rather impressive output. We can also observe\nmajor differences across languages in the multilin-\ngual model where, for instance, in the cloze test\nthe model is considerably more likely to produce\ngibberish in Finnish than e.g. in German. It is not\nclear, however, to what extent this reﬂects the sim-\nple fact that Finnish has fewer “easy” functional\nwords, providing for a harder task.\nThese results allow us to conclude that the cur-\nrent multilingual BERT model as distributed is\nnot able to substitute a well-trained monolingual\nmodel in more challenging tasks. This, however,\nis unlikely due to the multilinguality of the model,\nrather, we believe it is due to the simple fact that\neach language is a mere 1/100th of the training\ndata and training effort of the model.\nIn other\nwords, the model seems undertrained w.r.t. to in-\ndividual languages. This is, for example, hinted\nat in the text generation task where the multilin-\ngual model mostly copies from the context or pro-\nduces gibberish, while the monolingual models\nproduce a considerably higher proportion of orig-\ninal text. Intuitively, this would ﬁt a pattern where\none would expect the model, as it is being trained,\nto ﬁrst produce gibberish, then learn to understand\nand copy, and ﬁnally learn to generate.\nThe primary practical conclusion of this paper\nis that it is indeed necessary to invest the neces-\nsary computational effort to produce well-trained\nBERT models for other languages instead of re-\nlying on the present multilingual model as dis-\ntributed. We also established baseline results on\nseveral tasks across several languages, allowing a\nbetter intuitive estimation of the applicability of\nthe multilingual model in different situations.\nAcknowledgments\nWe gratefully acknowledge the support of the\nGoogle Digital News Innovation Fund, Academy\nof Finland, CSC – IT Center for Science, and the\nNVIDIA Corporation GPU Grant Program.\nReferences\nKevin\nClark,\nUrvashi\nKhandelwal,\nOmer\nLevy,\nand\nChristopher\nD.\nManning.\n2019.\nhttps://www.aclweb.org/anthology/W19-4828\nWhat does BERT look at? an analysis of BERT’s at-\ntention. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 276–286, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAllyson Ettinger. 2019. What bert is not: Lessons from\na new suite of psycholinguistic diagnostics for lan-\nguage models. arXiv preprint arXiv:1907.13528.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. arXiv preprint arXiv:1901.05287.\nGanesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah.\n2019. https://www.aclweb.org/anthology/P19-1356\nWhat does BERT learn about the structure of lan-\nguage?\nIn Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3651–3657, Florence, Italy. Association\nfor Computational Linguistics.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside bert’s linguistic knowl-\nedge. In Proceedings of the Second BlackboxNLP\nWorkshop on Analyzing and Interpreting Neural\nNetworks for NLP.\nJoakim Nivre, Mitchell Abrams, ˇZeljko Agi´c, Lars\nAhrenberg, Gabriel˙e Aleksandraviˇci¯ut˙e, Lene An-\ntonsen,\nKatya Aplonova,\nMaria Jesus Aranz-\nabe, Gashaw Arutie, Masayuki Asahara, et al.\n2019.\nhttp://hdl.handle.net/11234/1-2988 Univer-\nsal dependencies 2.4. LINDAT/CLARIN digital li-\nbrary at the Institute of Formal and Applied Linguis-\ntics ( ´UFAL), Faculty of Mathematics and Physics,\nCharles University.\nJoakim Nivre, Marie-Catherine De Marneffe, Filip\nGinter, Yoav Goldberg, Jan Hajic, Christopher D\nManning, Ryan McDonald, Slav Petrov, Sampo\nPyysalo, Natalia Silveira, et al. 2016. Universal de-\npendencies v1: A multilingual treebank collection.\nIn Proceedings of the Tenth International Confer-\nence on Language Resources and Evaluation (LREC\n2016), pages 1659–1666.\nTelmo Pires,\nEva Schlinger,\nand Dan Garrette.\n2019. https://www.aclweb.org/anthology/P19-1493\nHow multilingual is multilingual BERT?\nIn Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nhttps://www.aclweb.org/anthology/P19-1452 BERT\nrediscovers the classical NLP pipeline. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4593–4601,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nAshish\nVaswani,\nNoam\nShazeer,\nNiki\nParmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ\nukasz\nKaiser,\nand\nIllia\nPolosukhin.\n2017.\nhttp://papers.nips.cc/paper/7181-attention-is-all-\nyou-need.pdf Attention is all you need. In I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 30, pages\n5998–6008. Curran Associates, Inc.\nAlex\nWang\nand\nKyunghyun\nCho.\n2019.\nhttp://arxiv.org/abs/1902.04094 Bert has a mouth,\nand it must speak: Bert as a markov random ﬁeld\nlanguage model.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nbert. arXiv preprint arXiv:1904.09077.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2019-10-09",
  "updated": "2019-10-09"
}