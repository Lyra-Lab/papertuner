{
  "id": "http://arxiv.org/abs/1910.04281v2",
  "title": "Integrating Behavior Cloning and Reinforcement Learning for Improved Performance in Dense and Sparse Reward Environments",
  "authors": [
    "Vinicius G. Goecks",
    "Gregory M. Gremillion",
    "Vernon J. Lawhern",
    "John Valasek",
    "Nicholas R. Waytowich"
  ],
  "abstract": "This paper investigates how to efficiently transition and update policies,\ntrained initially with demonstrations, using off-policy actor-critic\nreinforcement learning. It is well-known that techniques based on Learning from\nDemonstrations, for example behavior cloning, can lead to proficient policies\ngiven limited data. However, it is currently unclear how to efficiently update\nthat policy using reinforcement learning as these approaches are inherently\noptimizing different objective functions. Previous works have used loss\nfunctions, which combine behavior cloning losses with reinforcement learning\nlosses to enable this update. However, the components of these loss functions\nare often set anecdotally, and their individual contributions are not well\nunderstood. In this work, we propose the Cycle-of-Learning (CoL) framework that\nuses an actor-critic architecture with a loss function that combines behavior\ncloning and 1-step Q-learning losses with an off-policy pre-training step from\nhuman demonstrations. This enables transition from behavior cloning to\nreinforcement learning without performance degradation and improves\nreinforcement learning in terms of overall performance and training time.\nAdditionally, we carefully study the composition of these combined losses and\ntheir impact on overall policy learning. We show that our approach outperforms\nstate-of-the-art techniques for combining behavior cloning and reinforcement\nlearning for both dense and sparse reward scenarios. Our results also suggest\nthat directly including the behavior cloning loss on demonstration data helps\nto ensure stable learning and ground future policy updates.",
  "text": "Integrating Behavior Cloning and Reinforcement Learning for\nImproved Performance in Dense and Sparse Reward\nEnvironments\nVinicius G. Goecks\nTexas A&M University\nUS Army Research Laboratory\nCollege Station, Texas\nvinicius.goecks@tamu.edu\nGregory M. Gremillion\nUS Army Research Laboratory\nAdelphi, Maryland\ngregory.m.gremillion.civ@mail.mil\nVernon J. Lawhern\nUS Army Research Laboratory\nAberdeen, Maryland\nvernon.j.lawhern.civ@mail.mil\nJohn Valasek\nTexas A&M University\nCollege Station, Texas\nvalasek@tamu.edu\nNicholas R. Waytowich\nUS Army Research Laboratory\nColumbia University\nAberdeen, Maryland\nnicholas.r.waytowich.civ@mail.mil\nABSTRACT\nThis paper investigates how to efficiently transition and update poli-\ncies, trained initially with demonstrations, using off-policy actor-\ncritic reinforcement learning. It is well-known that techniques\nbased on Learning from Demonstrations, for example behavior\ncloning, can lead to proficient policies given limited data. How-\never, it is currently unclear how to efficiently update that policy\nusing reinforcement learning as these approaches are inherently\noptimizing different objective functions. Previous works have used\nloss functions, which combine behavior cloning losses with re-\ninforcement learning losses to enable this update. However, the\ncomponents of these loss functions are often set anecdotally, and\ntheir individual contributions are not well understood. In this work,\nwe propose the Cycle-of-Learning (CoL) framework that uses an\nactor-critic architecture with a loss function that combines behav-\nior cloning and 1-step Q-learning losses with an off-policy pre-\ntraining step from human demonstrations. This enables transition\nfrom behavior cloning to reinforcement learning without perfor-\nmance degradation and improves reinforcement learning in terms\nof overall performance and training time. Additionally, we carefully\nstudy the composition of these combined losses and their impact\non overall policy learning. We show that our approach outperforms\nstate-of-the-art techniques for combining behavior cloning and\nreinforcement learning for both dense and sparse reward scenar-\nios. Our results also suggest that directly including the behavior\ncloning loss on demonstration data helps to ensure stable learning\nand ground future policy updates.\nCCS CONCEPTS\n• Computing methodologies →Learning from demonstra-\ntions; Reinforcement learning; Artificial intelligence; • Human-\ncentered computing →Human computer interaction (HCI); Inter-\naction design process and methods;\nProc. of the 19th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2020), B. An, N. Yorke-Smith, A. El Fallah Seghrouchni, G. Sukthankar (eds.), May\n9–13, 2020, Auckland, New Zealand. © 2020 International Foundation for Autonomous\nAgents and Multiagent Systems (www.ifaamas.org). All rights reserved.\nKEYWORDS\nHuman-robot/agent interaction; Agent-based analysis of human\ninteraction; Machine learning for robotics; Reinforcement Learning\nACM Reference Format:\nVinicius G. Goecks, Gregory M. Gremillion, Vernon J. Lawhern, John Valasek,\nand Nicholas R. Waytowich. 2020. Integrating Behavior Cloning and Rein-\nforcement Learning for Improved Performance in Dense and Sparse Reward\nEnvironments. In Proc. of the 19th International Conference on Autonomous\nAgents and Multiagent Systems (AAMAS 2020), Auckland, New Zealand, May\n9–13, 2020, IFAAMAS, 9 pages.\n1\nINTRODUCTION\nReinforcement Learning (RL) has yielded many recent successes\nin solving complex tasks that meet and exceed the capabilities of\nhuman counterparts, demonstrated in video game environments\n[19], robotic manipulators [1], and various open-source simulated\nscenarios [17]. However, these RL approaches are sample inefficient\nand slow to converge to this impressive behavior, limited signifi-\ncantly by the need to explore potential strategies through trial and\nerror, which produces initial performance significantly worse than\nhuman counterparts. The resultant behavior that is initially ran-\ndom and slow to reach proficiency is poorly suited for real-world\napplications such as physically embodied ground and air vehicles,\nor in scenarios where sufficient capability must be achieved in short\ntime spans. In such situations, the random exploration of the state\nspace of an untrained agent can result in unsafe behaviors and\ncatastrophic failure of a physical system, potentially resulting in\nunacceptable damage or downtime. Similarly, slow convergence of\nthe agent’s performance requires exceedingly many interactions\nwith the environment, which is often prohibitively difficult or in-\nfeasible for physical systems that are subject to energy constraints,\ncomponent failures, and operation in dynamic or adverse environ-\nments. These sample efficiency pitfalls of RL are exacerbated even\nfurther when trying to learn in the presence of sparse rewards,\noften leading to cases where RL can fail to learn entirely.\nOne approach for overcoming these limitations is to utilize\ndemonstrations of desired behavior from a human data source\n(or potentially some other agent) to initialize the learning agent\narXiv:1910.04281v2  [cs.LG]  3 Apr 2020\nto a significantly higher level of performance than is yielded by\na randomly initialized agent. This is often termed Learning from\nDemonstrations (LfD) [2], which is a subset of imitation learning\nthat seeks to train a policy to imitate the desired behavior of an-\nother policy or agent. LfD leverages data (in the form of state-action\ntuples) collected from a demonstrator for supervised learning, and\ncan be used to produce an agent with qualitatively similar behavior\nin a relatively short training time and with limited data. This type of\nLfD, called Behavior Cloning (BC), attempts to learn a mapping be-\ntween the state-action pairs contained in the set of demonstrations\nto mimic the behavior of the demonstrator. LfD also encompasses\nother learning modalities such as inverse reinforcement learning\n(IRL), where it is desired to learn the reward function that the agent,\nor demonstrator, is optimizing to perform the task [25].\nThough BC techniques do allow for the relatively rapid learning\nof behaviors that are comparable to that of the demonstrator, they\nare limited by the quality and quantity of the demonstrations pro-\nvided and are only improved by providing additional, high-quality\ndemonstrations. In addition, BC is plagued by the distributional\ndrift problem in which a mismatch between the learned policy distri-\nbution of states and the distribution of states in the training set can\ncause errors that propagate over time and lead to catastrophic fail-\nures. By combining BC with subsequent RL, it is possible to address\nthe drawbacks of either approach, initializing a significantly more\ncapable and safer agent than with random initialization, while also\nallowing for further self-improvement without needing to collect\nadditional data from a human demonstrator. However, it is currently\nunclear how to effectively update a policy initially trained with BC\nusing RL as these approaches are inherently optimizing different\nobjective functions. Previous works have used loss functions that\ncombine BC losses with RL losses to enable this update, however,\nthe components of these loss functions are often set anecdotally\nand their individual contributions are not well understood.\nIn this work, we propose the Cycle-of-Learning (CoL) framework,\nwhich uses an actor-critic architecture with a loss function that\ncombines behavior cloning and 1-step Q-learning losses with an\noff-policy algorithm, and a pre-training step to learn from human\ndemonstrations. Unlike previous approaches to combine BC with\nRL, such as Rajeswaran et al. [24], our approach uses an actor-critic\narchitecture to learn both a policy and value function from the\nhuman demonstration data, which we show, speeds up learning.\nAdditionally, we perform a detailed component analysis of our\nmethod to investigate the individual contributions of pre-training,\ncombined losses, and sampling methods of the demonstration data\nand their effects on transferring from BC to RL. To summarize, the\nmain contribution of this work are:\n• We introduce an actor-critic based method, that combines\npre-training as well as combined loss functions to learn both\na policy and value function from demonstrations, to enable\ntransition from behavior cloning to reinforcement learning.\n• We show that our method can transfer from BC to RL with-\nout performance degradation while improving upon existing\nstate-of-the-art BC to RL algorithms in terms of overall per-\nformance and training time.\n• We perform a detailed analysis to investigate the contribu-\ntions of the individual components in our method.\nOur results show that our approach outperforms BC, Deep Deter-\nministic Policy Gradients (DDPG), and Demonstration Augmented\nPolicy Gradient (DAPG) in two different application domains for\nboth dense- and sparse-reward settings. Our results also suggest\nthat directly including the behavior cloning loss on demonstration\ndata helps to ensure stable learning and ground future policy up-\ndates, and that a pre-training step enables the policy to start at a\nperformance level greater than behavior cloning.\n2\nPRELIMINARIES\nWe adopt the standard Markov Decision Process (MDP) formulation\nfor sequential decision making [31], which is defined as a tuple\n(S,A,R, P,γ), where S is the set of states, A is the set of actions,\nR(s,a) is the reward function, P(s′|s,a) is the transition probability\nfunction and γ is a discount factor. At each state s ∈S, the agent\ntakes an action a ∈A, receives a reward R(s,a) and arrives at\nstate s′ as determined by P(s′|s,a). The goal is to learn a behavior\npolicy π which maximizes the expected discounted total reward.\nThis is formalized by the Q-function, sometimes referred to as the\nstate-action value function:\nQπ (s,a) = Eat ∼π\n\"+∞\nÕ\nt=0\nγ tR(st,at )\n#\ntaking the expectation over trajectories obtained by executing the\npolicy π starting at s0 = s and a0 = a.\nHere we focus on actor-critic methods which seek to maximize\nJ(θ) = Es∼µ[Qπ(.|θ)(s, π(s|θ))]\nwith respect to parameters θ and an initial state distribution µ.\nThe Deep Deterministic Policy Gradient (DDPG) [17] is an off-\npolicy actor-critic reinforcement learning algorithm for continuous\naction spaces, which calculates the gradient of the Q-function with\nrespect to the action to train the policy. DDPG makes use of a replay\nbuffer to store past state-action transitions and target networks to\nstabilize Q-learning [19]. Since DDPG is an off-policy algorithm, it\nallows for the use of arbitrary data, such as demonstrations from\nanother source, to update the policy. A demonstration trajectory is\na tuple (s,a,r,s′) of state s, action a, the reward r = R(s,a) and the\ntransition state s′ collected from a demonstrator’s policy. In most\ncases these demonstrations are from a human observer, although in\nprinciple these demonstrations can come from any existing agent\nor policy.\n3\nRELATED WORK\nSeveral works have shown the efficacy of combining behavior\ncloning with reinforcement learning across a variety of tasks. One\nof the earliest works in this area was by Schaal [27], who studied\ndemonstration learning and model-based reinforcement learning\nand their application to classical tasks such as cart-pole. Similarly,\nAtkeson and Schaal [3], in a robotic arm swinging up a pendulum\ntask, trained a dynamics model and reward function from human\ndemonstrations to learn a policy and improve it with reinforcement\nlearning. Kim et al. [14] used expert samples to constrain the approx-\nimate policy iteration step and learn a value function, parametrized\nby linear radial basis functions (RBF), with convex optimization.\nRecent work by Hester et al. [9], known as Deep Q-learning from\nDemonstrations (DQfD), combined behavior cloning with deep Q-\nlearning [19] to learn policies for Atari games by leveraging a loss\nfunction that combines a large-margin supervised learning loss\nfunction, 1-step Q-learning loss, and an n-step Q-learning loss func-\ntion that helps ensure the network satisfies the Bellman equation.\nThis work was extended to continuous action spaces by Večerík\net al. [33] with DDPG from Demonstrations (DDPGfD), who pro-\nposed an extension of DDPG [17] that uses human demonstrations,\nand applied their approach to object manipulation tasks for both\nsimulated and real robotic environments. The loss functions for\nthese methods include the n-step Q-learning loss, which is known\nto require on-policy data to accurately estimate. Similar work by\nNair et al. [21] combined behavior cloning-based demonstration\nlearning, goal-based reinforcement learning, and DDPG for robotic\nmanipulation of objects in a simulated environment.\nThe Normalized Actor Critic [6] uses principles from maximum\nentropy reinforcement learning [8] and proposes a learning objec-\ntive which better normalizes the Q-function learned from demon-\nstration data. In addition they proposed a single unified loss func-\ntion as opposed to a combined loss function of supervised and\nreinforcement losses and showed superior performance versus ex-\nisting works in a Minecraft task and two 3D driving tasks. Policy\nOptimization with Demonstrations (POfD) [13] specifies a demon-\nstration learning approach using an adversarial learning objective,\nseeking to minimize the difference between the learned policy and\nthe demonstration policy when the reward signal is sparse, an\napproach similar in nature to Generative Adversarial Imitation\nLearning (GAIL) [11].\nA method that is very similar to ours is the Demonstration Aug-\nmented Policy Gradient (DAPG) [24], a policy-gradient method\nthat uses behavior cloning as a pre-training step together with\nan augmented loss function with a heuristic weight function that\ninterpolates between the policy gradient loss, computed using the\nNatural Policy Gradient [12], and behavior cloning loss. They apply\ntheir approach across four different robotic manipulations tasks\nusing a 24 Degree-of-Freedom (DoF) robotic hand in a simulator\nand show that DAPG outperforms DDPGfD [33] across all tasks.\nTheir work also showed that behavior cloning combined with Nat-\nural Policy Gradient performed very similarly to DAPG for three\nof the four tasks considered, showcasing the importance of using a\nbehavior cloning loss both in pre-training and policy training.\nIn summary, when compared to the main related literature, the\nCycle-of-Learning (CoL) algorithm differs from existing algorithms\nin several ways. First, CoL uses an actor-critic architecture, as op-\nposed to the policy gradient algorithm proposed by DAPG [24]. The\nactor-critic architecture allows the integration of additional human\ninteraction modalities during training as, for example, evaluative\nfeedback to update the critic and human interventions to update the\nactor; Second, CoL introduces a pre-training phase the combined\nloss function and the expert demonstrations are used to train the\nactor and critic network before interacting with the environment,\nwhich is not present on state-of-the-art works such as Nair et al.\n[21] and Pohlen et al. [22]. Third, CoL learns in continuous action-\nspace environments as opposed to discrete action-spaces as was\ndone in Hester et al. [9], Večerík et al. [33], and Pohlen et al. [22].\n4\nPROPOSED APPROACH\nThe Cycle-of-Learning (CoL) framework is a method for leverag-\ning multiple modalities of human input to improve the training of\nRL agents. These modalities can include human demonstrations,\ni.e. human-provided exemplar behaviors, human interventions, i.e.\ninterdictions in agent behavior with subsequent partial demonstra-\ntions, and human evaluations, i.e. sparse indications of the quality\nof agent behavior. These individual mechanisms of human inter-\naction have been previously shown to provide various benefits in\nlearning performance and efficiency [7, 16, 18, 26, 34]. The success-\nful integration of these disparate techniques, which would leverage\ntheir complementary characteristics, requires a learning architec-\nture that allows for optimization of common objective functions\nand consistent representations. An actor-critic framework with\na combined loss function, as presented in this work, is such an\narchitecture.\nIn this paper, we focus on extending the Cycle-of-Learning frame-\nwork to tackle the known issue of transitioning BC policies to RL by\nutilizing an actor-critic architecture with a combined BC+RL loss\nfunction and pre-training phase for continuous state-action spaces,\nthat can learn in both dense- and sparse-reward environments. The\nmain advantage of our method is the use of an off-policy, actor-\ncritic architecture to pre-train both a policy and value function,\nas well as continued re-use of demonstration data during agent\ntraining, which reduces the amount of interactions needed between\nthe agent and environment. This is an important aspect especially\nfor robotic applications or real-world systems where interactions\ncan be costly.\nThe combined loss function consists of the following compo-\nnents: an expert behavior cloning loss that drives the actor’s actions\ntoward previous human trajectories, 1-step return Q-learning loss\nto propagate values of human trajectories to previous states, the\nactor loss, and a L2 regularization loss on the actor and critic to\nstabilize performance and prevent over-fitting during training. The\nimplementation of each loss component and their combination are\ndefined as follows:\n• Expert behavior cloning loss (LBC): Given expert demon-\nstration subset DE of continuous states and actions sE and\naE visited by the expert during a task demonstration over T\ntime steps\nDE =\nn\nsE\n0 ,aE\n0 ,sE\n1 ,aE\n1 , ...,sE\nT ,aE\nT\no\n,\n(1)\na behavior cloning loss (mean squared error) from demon-\nstration data LBC can be written as\nLBC(θπ ) = 1\n2\n\u0010\nπ(st |θπ ) −aE\nt )\n\u00112\n(2)\nin order to minimize the difference between the actions pre-\ndicted by the actor network π(st ), parametrized by θπ , and\nthe expert actions aEt for a given state vector st .\n• 1-step return Q-learning loss (L1): The 1-step return R1\ncan be written in terms of the critic network Q, parametrized\nby θQ, as\nR1 = rt + γQ(st+1, π(st+1|θπ )|θQ).\n(3)\nIn order to satisfy the Bellman equation, we minimize the\ndifference between the predicted Q-value and the observed\nreturn from the 1-step roll-out for a batch of sampled states\ns:\nLQ1(θQ) = 1\n2\n\u0000R1 −Q(s, π(s|θπ )|θQ)\u00012 .\n(4)\n• Actor Q-loss (LA): It is assumed that the critic function Q\nis differentiable with respect to the action. Since we want to\nmaximize the Q-values for the current state, the actor loss\nbecame the negative of the Q-values predicted by the critic\nfor a batch of sampled states s:\nLA(θπ ) = −Q(s, π(s|θπ )|θQ).\n(5)\n• L2 regularization (LL2): We also add a L2 regularization\nterm for the actor and critic weights to prevent overfitting\nand control model complexity:\nLL2(θπ ) = θT\nπ θπ ,\n(6)\nLL2(θQ) = θT\nQθQ .\n(7)\nCombining the above loss functions for the Cycle-of-Learning\nbecomes\nLCoL(θQ,θπ ) = λBC LBC(θπ ) + λALA(θπ )\n+ λQ1LQ1(θQ) + λL2Q LL2(θQ) + λL2π LL2(θπ ).\n(8)\nOur approach starts by collecting contiguous trajectories from\nexpert policies and stores the current and subsequent state-actions\npairs, reward received, and task completion signal in a permanent\nexpert memory buffer DE. During the pre-training phase, the agent\nsamples a batch of trajectories from the expert memory buffer DE\ncontaining expert trajectories to perform updates on the actor and\ncritic networks using the same combined loss function (Equations\n8). This procedure shapes the actor and critic initial distributions\nto be closer to the expert trajectories and eases the transition from\npolicies learned through expert demonstration to reinforcement\nlearning.\nAfter the pre-training phase, the policy is allowed to roll-out and\ncollect its first on-policy samples, which are stored in a separate\nfirst-in-first-out memory buffer with only the agent’s samples. After\ncollecting a given number of on-policy samples, the agent samples\na batch of trajectories comprising 25% of samples from the expert\nmemory buffer and 75% from the agent’s memory buffer. This fixed\nratio guarantees that each gradient update is grounded by expert\ntrajectories. We opted to use a fixed buffer ratio as in the Ape-X\nDQfD [22], one of the extensions of DQfD [9], which is claimed\nto be the first RL algorithm to solve the first level of Montezuma\nRevenge, a challenging ATARI task with sparse rewards. In our\nexperiments, showed in Section 5, we also compared this fixed\nbuffer ratio with the traditional Prioritized Experience Replay (PER)\nmethod and showed that the fixed buffer ratio outperforms PER\nin the sparse reward scenario. If a human demonstrator is used,\nthey can intervene at any time the agent is executing their policy,\nand add this new trajectories to the expert memory buffer. Samples\ncollected by the agent are added to the agent memory buffer, as\nusual.\nAfter sampling a batch of trajectories from the expert and agent\nbuffers, we perform model updates using the CoL combined loss.\nThis process is repeated after each interaction with the environment.\nAlgorithm 1 Cycle-of-Learning (CoL): Transitioning from Demon-\nstrations to Reinforcement Learning\n1: Input:\nEnvironment env, training steps T, data collection steps M,\nbatch size N, pre-training steps L, CoL hyperparameters\nλQ1, λBC, λA, λL2Q, λL2π , τ, and expert trajectories DE (if\navailable).\n2: Output:\nTrained actor π(s|θπ ) and critic Q(s, π |θQ) networks.\n3: Randomly initialize:\nActor network π(s|θπ ) and its target π ′(s|θπ ′) weights.\nCritic network Q(s, π |θQ) and its target Q′(s, π ′|θQ′)\nweights.\n4: Initialize empty agent and expert replay buffers R and RE.\n5: Load R and RE with expert trajectories DE, if available.\n6: for pre-training steps = 1, ..., L do\n7:\nCall TrainUpdate() procedure.\n8: for training steps = 1, ..., T do\n9:\nReset env and receive initial state s0.\n10:\nfor data collection steps = 1, ..., M do\n11:\nSelect action at = π(st |θπ ) according to policy.\n12:\nPerform action at and observe reward rt and next state\nst+1.\n13:\nStore transition (st,at,rt,st+1) in R.\n14:\nif End of episode then\n15:\nReset env and receive initial state s0.\n16:\nCall TrainUpdate() procedure.\n17: procedure TrainUpdate()\n18:\nif Pre-training then\n19:\nRandomly sample N transitions (si,ai,ri,si+1) from the\nexpert replay buffer RE.\n20:\nelse\n21:\nRandomly sample N ∗0.25 transitions (si,ai,ri,si+1)\nfrom the expert replay buffer RE and N ∗0.75 transitions from\nthe agent replay buffer R.\n22:\nCompute LQ1(θQ), LBC(θπ ), LA(θπ ), LL2(θQ), LL2(θπ )\n23:\nUpdate actor and critic networks according to Equation 8.\n24:\nUpdate target networks:\nθπ ′ ←τθπ + (1 −τ)θπ ′,\nθQ′ ←τθQ + (1 −τ)θQ′.\nThe proposed method is summarized in the pseudocode shown in\nAlgorithm 1.\n5\nEXPERIMENTAL SETUP AND RESULTS\n5.1\nExperimental Setup\nAs described in the previous sections, in our approach, the Cycle-\nof-Learning (CoL), we collect contiguous trajectories from expert\npolicies and store them in a permanent memory buffer. The policy is\nallowed to roll-out and is trained with a combined loss from a mix of\ndemonstration and agent data, stored in a separate first-in-first-out\nbuffer. We validate our approach in three environments with con-\ntinuous observation- and action-space: LunarLanderContinuous-v2\n[4] (dense and sparse reward cases) and a custom quadrotor landing\ntask [7] implemented using Microsoft AirSim [29].\nThe dense reward case of LunarLanderContinuous-v2 is the\nstandard environment provided by OpenAI Gym library [4]: the\nstate space consists of a eight-dimensional continuous vector with\ninertial states of the lander, the action space consists of a two-\ndimensional continuous vector controlling main and side thrusters,\nand the reward is given at every step based on the relative motion\nof the lander with respect to the landing pad (bonus reward is given\nwhen the landing is completed successfully). The sparse reward\ncase is a custom modification with the same reward scheme and\nstate-action space, however the reward is stored during the policy\nroll-out and is only given to the agent when the episode ends and\nis zero otherwise. The custom quadrotor landing task is a modified\nversion of the environment proposed by Goecks et al. [7], imple-\nmented using Microsoft AirSim [29], which consists of landing a\nquadrotor on a static landing pad in a simulated gusty environment,\nas seen in Figure 1. The state space consists of a fifteen-dimensional\ncontinuous vector with inertial states of the quadrotor and visual\nfeatures that represent the landing pad image-frame position and\nradius as seen by a downward-facing camera. The action space\nis a four-dimensional continuous vector that sends velocity com-\nmands for throttle, roll, pitch, and yaw. Wind is modeled as noise\napplied directly to the actions commanded by the agent and follows\na temporal-based, instead of distance-based, discrete wind gust\nmodel [20] with 65% probability of encountering a wind gust at\neach time step. This was done to induce additional stochasticity\nin the environment. The gust duration is uniformly sampled to\nlast between one to three real time seconds and can be imparted\nin any direction, with maximum velocity of half of what can be\ncommanded by the agent along each axis. This task has a sparse-\nreward scheme (reward R is given at the end of the episode, and\nis zero otherwise) based on the relative distance rrel between the\nquadrotor and the center of the landing pad at the final time step\nof the episode:\nR =\n1\n1 + r2\nrel\n.\nAlthough the goals of the two tasks are similar, the environments\nare different in terms of state- and action-spaces and physics (2d\nvs 3d, with and without wind effects). Additionally, in these envi-\nronments it was possible to collect human demonstrations of the\ntask. We studied using standard benchmark environments as, for\nexample, MuJoCo and PyBullet locomotion tasks, however, due to\nthe nature of the tasks and number of controls, collecting human\ndemonstrations were not feasible.\nThe hyperparameters used in CoL for each environment, and\nhow to tune them properly, are described in the project page avail-\nable online1.\nThe baselines that we compare our approach to are Deep De-\nterministic Policy Gradient (DDPG) [17, 30], Demonstration Aug-\nmented Policy Gradient (DAPG) [24], and traditional behavior\ncloning (BC). For the DDPG baseline we used an open-source im-\nplementation by Stable Baselines [10]. The hyperparameters used\nconcur with the original DDPG publication [17]: actor and critic\nnetworks with 2 hidden layers with 400 and 300 units respectively,\n1Cycle-of-Learning project page: https://vggoecks.com/cycle-of-learning/.\nFigure 1: Screenshot of AirSim environment and landing\ntask. Inset image in lower right corner: downward-facing\ncamera view used for extracting the position and radius of\nthe landing pad, which is part of the state space.\noptimized using Adam [15] with learning rate of 10−4 for the actor\nand 10−3 for the critic, discount factor of γ = 0.99, trained with\nminibatch size of 64, and replay buffer size of 106. Exploration noise\nwas added to the action following an Ornstein-Uhlenbeck process\n[32] with mean of 0.15 and standard deviation of 0.2. For the DAPG\nbaseline we used an official release of the DAPG codebase from\nthe authors 2. The policy is represented by a deep neural network\nwith three hidden layers of 128 units each, pre-trained with behav-\nior cloning for 100 epochs, with a batch size of 32 samples, and\nlearning rate of 10−3, λ0 = 0.01, and λ1 = 0.99. The BC policies are\ntrained by minimizing the mean squared error between the expert\ndemonstrations and the output of the model. The policies consist\nof a fully-connected neural network with 3 hidden layers with 128\nunits each and exponential linear unit (ELU) activation function\n[5]. The BC policy was evaluated for 100 episodes which was used\nto calculate the mean and standard error of the performance of the\npolicy.\nAll baselines that rely on demonstrations, namely BC, DAPG,\nand CoL, use the same human trajectories collected in the Lunar-\nLanderContinuous-v2 and custom Microsoft AirSim environment.\n5.2\nExperimental Results\nThe comparative performances of the CoL against the baseline\nmethods (BC, DDPG and DAPG) for the LunarLanderContinuous-\nv2 environment are presented via their training curves in Figure\n2a, using the standard dense reward. The mean reward of the BC\npre-trained from the human demonstrations is also shown for ref-\nerence, and its standard error is shown by the shaded band. The\nCoL reward initializes to values at or above the BC and steadily\nimproves throughout the reinforcement learning phase. Conversely,\nthe DDPG RL baseline initially returns rewards lower than the BC\nand slowly improves until its performance reaches similar levels\nto the CoL after approximately one million steps. However, this\nbaseline never performs as consistently as the CoL and eventually\nbegins to diverge, losing much of its performance gains after about\n2DAPG implementation: https://github.com/aravindr93/hand_dapg [23].\n(a)\n(b)\n(c)\nFigure 2: Comparison of CoL, BC, DDPG, and DAPG for 3 random seeds (bold line representing the mean and shaded area the\nstandard error) in the (a) dense- and (b) sparse-reward LunarLanderContinuous-v2 environment, and the (c) sparse-reward\nMicrosoft AirSim quadrotor landing environment.\nfour million steps. The DAPG baseline initial performance, simi-\nlar to the CoL, surpasses behavior cloning due to the pre-training\nphase and slowly converges to a high score, although slower than\nthe CoL.\nWhen using sparse rewards, meaning the rewards generated by\nthe LunarLanderContinuous-v2 environment are provided only at\nthe last time step of each episode, the performance improvement of\nthe CoL relative to the DDPG and DAPG baselines is even greater\n(Figure 2b). The performance of the CoL is qualitatively similar dur-\ning training to that of the dense case, with an initial reward roughly\nequal to or greater than that of the BC and a consistently increas-\ning reward. Conversely, the performance of the DDPG baseline is\ngreatly diminished for the sparse reward case, yielding effectively\nno improvement throughout the whole training period. The train-\ning of the DAPG does not deteriorate when compared to the dense\nreward case, however, the performance does not match CoL for the\nspecified training time.\nThe results for the more realistic and challenging AirSim quadro-\ntor landing environment (Figure 2c) illustrate a similar trend. The\nCoL initially returns rewards above the BC, DDPG, and DAPG base-\nlines and steadily increases its performance, with DAPG converging\nat end to a similar level of performance. The DDPG baseline practi-\ncally never succeeds and subsequently fails to learn a viable policy,\nwhile displaying greater variance in performance when compared\nto CoL and DAPG. Noting that successfully landing on the target\nwould generate a sparse episode reward of approximately 0.64, it\nis clear that these baseline algorithms, with exception of DAPG,\nrarely generate a satisfactory trajectory for the duration of training.\n5.3\nComponent Analysis\nSeveral component analyses were performed to evaluate the impact\nof each of the critical elements of the CoL on learning. These respec-\ntively include the effects of pre-training, the combined loss function,\nand the sample composition of the experience replay buffer. The re-\nsults of each analysis are shown in Figures 3-5 and are summarized\nin Table 1.\n5.3.1\nEffects of Pre-Training. To determine the effects of pre-\ntraining on performance we compare the standard CoL against an\nimplementation without this pre-training phase, where the number\nof pre-training steps L = 0, denoted as CoL-PT. The complete com-\nbined loss, as seen in Equations 8 is used during the reinforcement\nlearning phase. This condition assesses the impact on learning\nperformance of not pre-training the agent, while still using the\ncombined loss in the RL phase. As seen in Figure 3, this condition\ndiffers from the baseline CoL in its initial performance being worse,\ni.e. significantly below the BC, but does reach similar rewards after\nseveral hundred thousand steps, exhibiting the same consistent\nresponse during training thereafter. Effectively, this highlights that\nthe benefit of pre-training is improved initial response and signifi-\ncant speed gain in reaching steady-state performance level, without\nqualitatively impacting the long-term training behavior.\n5.3.2\nEffects of Combined Loss. To determine the effects of the\ncombined loss function on performance we compare the standard\nCoL against two alternate learning implementations: 1) the CoL\nwithout the behavioral cloning expert loss on the actor (λBC := 0)\nduring both pre-training and RL phases, denoted as CoL-BC, and\n2) standard BC followed by DDPG using standard loss functions,\ndenoted as BC+DDPG. For the implementation of the CoL without\nthe behavior cloning loss (CoL-BC), the critic loss remains the same\nas in Equation 8 for both training phases. This condition assesses\nthe impact on learning performance of the behavior cloning loss\ncomponent LBC, given otherwise consistent loss functions in both\npre-training and RL phases. As seen in Figure 4, this condition\n(purple, dashed) improves upon the CoL-PT condition (Figure 3) in\nits initial reward return and similarly achieves comparable perfor-\nmance to the baseline CoL in the first few hundred thousand steps,\nbut then steadily deteriorates as training continues, with several\ncatastrophic losses in performance. This result makes clear that the\nbehavioral cloning loss is an essential component of the combined\nloss function toward maintaining performance throughout training,\nanchoring the learning to some previously demonstrated behaviors\nthat are sufficiently proficient.\nTable 1: Method Comparison on LunarLanderContinuous-v2 environment, dense-reward case\nMethod\nPre-Training Loss\nTraining Loss\nBuffer Type\nAverage Reward\nCoL\nLQ1 + LA + LBC\nLQ1 + LA + LBC\nFixed Ratio\n261.80 ± 22.53\nCoL-PT\nNone\nLQ1 + LA + LBC\nFixed Ratio\n253.24 ± 46.50\nCoL+PER\nLQ1 + LA + LBC\nLQ1 + LA + LBC\nPER\n245.24 ± 37.66\nDAPG\nLBC\nAugmented Policy Gradient\nNone\n127.99 ± 37.28\nDDPG\nNone\nLQ1 + LA\nUniform\n152.98 ± 69.45\nBC\nLBC\nNone\nNone\n-48.83 ± 27.68*\nBC+DDPG\nLBC\nLQ1 + LA\nUniform\n-57.38 ± 50.11\nCoL-BC\nLQ 1 + LA\nLQ1 + LA\nFixed Ratio\n-105.65 ± 196.85\nSummary of learning methods. Enumerated for each method are all non-zero loss components\n(excluding regularization), buffer type, and average and standard error of the reward throughout\ntraining (after pre-training) across the three seeds, evaluated with dense reward in LunarLander-\nContinuous-v2 environment. ∗For BC, these values are computed from 100 evaluation trajectories of\nthe final pre-trained agent.\nFigure 3: Effects of the pre-training phase in the Cycle-\nof-Learning. Results for 3 random seeds (bold line rep-\nresenting the mean and shaded area the standard error)\nshowing component analysis in LunarLanderContinuous-\nv2 environment comparing pre-trained Cycle-of-Learning\n(CoL curve) against the Cycle-of-Learning without the pre-\ntraining phase (CoL-PT curve) and the behavior cloning (BC)\nbaseline.\nThe second of these comparative implementations that illustrate\nthe effects of the combined loss is the behavior cloning with subse-\nquent DDPG (BC+DDPG) condition, which utilized standard loss\nfunctions (Equations 2, 4, and 5) rather than the CoL combined\nloss in both phases (Equation 8). Pre-training of the actor with BC\nuses only the regression loss, as seen in Equation 2. DDPG utilizes\nstandard loss functions for the actor and critic, as seen in Lillicrap\net al. [17]. The BC+DDPG condition assesses the impact on learning\nperformance of standardized loss functions rather than our com-\nbined loss functions across both training phases. The BC+DDPG\ncondition (Figure 4; red, dashed) produces initial rewards below the\nBC response and subsequently improves in performance only to an\naverage level similar to that of the BC and is much less stable in its\nFigure 4: Effects of the combined loss in the Cycle-of-\nLearning. Results for 3 random seeds (bold line represent-\ning the mean and shaded area the standard error) showing\ncomponent analysis in LunarLanderContinuous-v2 environ-\nment comparing complete Cycle-of-Learning (CoL), CoL\nwithout the expert behavior cloning loss (CoL-BC), and pre-\ntraining with BC followed by DDPG without combined loss\n(BC+DDPG).\nresponse throughout training, as indicated by the wide standard\nerror band. This result indicates that simply sequencing standard\nBC and RL algorithms results in significantly worse performance\nand stability even after millions of training steps, emphasizing the\nvalue of a consistent combined loss function across all training\nphases.\n5.3.3\nEffects of Human Experience Replay Sampling. To deter-\nmine the effects of the different sampling techniques of the ex-\nperience replay buffer on performance we compare the standard\nCoL, which utilizes a fixed ratio buffer of samples comprising 25%\nexpert data and 75% agent data, against an implementation with\nFigure 5: Effects of human experience replay sampling in\nthe Cycle-of-Learning. Results for 3 random seeds (bold line\nrepresenting the mean and shaded area the standard error)\nshowing ablation study in LunarLanderContinuous-v2 envi-\nronment, dense (D) and sparse (S) reward cases, comparing\ncomplete Cycle-of-Learning (CoL) trained with fixed ratio of\nexpert and agent samples and complete Cycle-of-Learning\nusing Prioritized Experience Replay (CoL+PER) with a vari-\nable ratio of expert and agent samples ranked based on their\ntemporal difference error.\nPrioritized Experience Replay (PER) [28], with a data buffer prior-\nitized by the magnitude of each transition’s temporal difference\n(TD) error, denoted as CoL+PER. The comparative performance of\nthese implementations, for both the dense- (D) and sparse-reward\n(S) cases of the LunarLanderContinuous-v2 scenario, are shown in\nFigure 5. For the dense-reward condition, there is no significant\ndifference in the learning performance between the fixed ratio and\nPER buffers. However, for the sparse-reward case of the CoL+PER\nimplementation, the learning breaks down after approximately 1.3\nmillion training steps, resulting in a significantly decreased per-\nformance thereafter. This result illustrates that the fixed sampling\nratio for the replay buffer in the standard CoL is a more robust\nmechanism of incorporating experience data, particularly in sparse-\nreward environments, likely because it grounds performance to\ndemonstrated human behavior throughout training.\n6\nDISCUSSION AND CONCLUSION\nIn this work, we present a novel method for combining behavior\ncloning with reinforcement learning using an actor-critic architec-\nture that implements a combined loss function and a demonstration-\nbased pre-training phase. We compare our approach against state-\nof-the-art baselines, including BC, DDPG, and DAPG, and demon-\nstrate the superiority of our method in terms of learning speed,\nstability, and performance with respect to these baselines. This is\nshown in the OpenAI Gym LunarLanderContinuous-v2 and the\nhigh-fidelity Microsoft AirSim quadrotor simulation environments\nin both dense and sparse reward settings. This result is especially\nnoticeable in the AirSim landing task (Figure 2c), an environment\ndesigned to exhibit a high degree of stochasticity. The BC and DDPG\nbaselines fail to converge to an effective and stable policy after five\nmillion training steps on the LunarLanderContinuous-v2 environ-\nment with dense reward and the modified version with a sparse\nreward signal. DAPG, although successful in both LunarLander-\nContinuous-v2 environments and the custom AirSim landing task,\nconverges at a slower rate when compared to the proposed method\nand starts the training at a lower performance value after pre-\ntraining with demonstration data. Conversely, our method, CoL,\nis able to quickly achieve high performance without degradation,\nsurpassing both behavior cloning and reinforcement learning algo-\nrithms alone, in both dense and sparse reward cases. Additionally,\nwe demonstrate through separate analyses of several components\nof our architecture that pre-training, the use of a combined loss\nfunction, and a fixed ratio of human-generated experience are criti-\ncal to the performance improvements. This component analysis also\nindicated that simply sequencing standard behavior cloning and re-\ninforcement learning algorithms does not produce these gains and\nhighlighted the importance of grounding the training to the demon-\nstrated data by using a fixed ratio of expert and agent trajectories\nin the experience replay buffer.\n6.1\nFuture Work\nFuture work will investigate how to effectively integrate multiple\nforms of human feedback into an efficient human-in-the-loop RL\nsystem capable of rapidly adapting autonomous systems in dynam-\nically changing environments. Actor-critic methods, such as the\nCoL method proposed in this paper, provide an interesting opportu-\nnity to integrate different human feedback modalities as additional\nlearning signals at different stages of policy learning [35]. For ex-\nample, existing works have shown the utility of leveraging human\ninterventions [7, 26], and specifically learning a predictive model of\nwhat actions to ignore at every time step [36], which could be used\nto improve the quality of the actor’s policy. Deep reinforcement\nlearning with human evaluative feedback has also been shown to\nquickly train policies across a variety of domains [18, 34] and can be\na particularly useful approach when the human is unable to provide\na demonstration of desired behavior but can articulate when desired\nbehavior is achieved. Further, the capability our approach provides,\ntransitioning from a limited number of human demonstrations to\na baseline behavior cloning agent and subsequent improvement\nthrough reinforcement learning without significant losses in per-\nformance, is largely motivated by the goal of human-in-the-loop\nlearning on physical robotic systems. Thus, our aim is to integrate\nthis method onto such systems and demonstrate rapid, safe, and\nstable learning from limited human interaction.\nACKNOWLEDGMENTS\nResearch was sponsored by the U.S. Army Research Laboratory and\nwas accomplished under Cooperative Agreement Number W911NF-\n18-2-0134. The views and conclusions contained in this document\nare those of the authors and should not be interpreted as represent-\ning the official policies, either expressed or implied, of the Army\nResearch Laboratory or the U.S. Government. The U.S. Government\nis authorized to reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation herein.\nREFERENCES\n[1] Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob\nMcGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex\nRay, et al. 2018.\nLearning dexterous in-hand manipulation.\narXiv preprint\narXiv:1808.00177 (2018).\n[2] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. 2009. A\nsurvey of robot learning from demonstration. Robotics and autonomous systems\n57, 5 (2009), 469–483.\n[3] Christopher G Atkeson and Stefan Schaal. 1997. Robot learning from demonstra-\ntion. In ICML, Vol. 97. Citeseer, 12–20.\n[4] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schul-\nman, Jie Tang, and Wojciech Zaremba. 2016.\nOpenai gym.\narXiv preprint\narXiv:1606.01540 (2016).\n[5] Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. 2015. Fast and\naccurate deep network learning by exponential linear units (elus). arXiv preprint\narXiv:1511.07289 (2015).\n[6] Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. 2018.\nReinforcement Learning from Imperfect Demonstrations. CoRR abs/1802.05313\n(2018). arXiv:1802.05313 http://arxiv.org/abs/1802.05313\n[7] Vinicius G. Goecks, Gregory M. Gremillion, Vernon J. Lawhern, John Valasek,\nand Nicholas R. Waytowich. 2018. Efficiently Combining Human Demonstrations\nand Interventions for Safe Training of Autonomous Systems in Real-Time. CoRR\nabs/1810.11545 (2018). arXiv:1810.11545 http://arxiv.org/abs/1810.11545\n[8] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft\nActor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a\nStochastic Actor. In Proceedings of the 35th International Conference on Machine\nLearning (Proceedings of Machine Learning Research), Jennifer Dy and Andreas\nKrause (Eds.), Vol. 80. PMLR, StockholmsmÃďssan, Stockholm Sweden, 1861–\n1870. http://proceedings.mlr.press/v80/haarnoja18b.html\n[9] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal\nPiot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et al. 2018. Deep\nq-learning from demonstrations. In Thirty-Second AAAI Conference on Artificial\nIntelligence.\n[10] Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Rene Traore,\nPrafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plap-\npert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. 2018. Stable\nBaselines. https://github.com/hill-a/stable-baselines. (2018).\n[11] Jonathan Ho and Stefano Ermon. 2016. Generative adversarial imitation learning.\nIn Advances in neural information processing systems. 4565–4573.\n[12] Sham Kakade. 2001. A Natural Policy Gradient. In Proceedings of the 14th Interna-\ntional Conference on Neural Information Processing Systems: Natural and Synthetic\n(NIPS’01). MIT Press, Cambridge, MA, USA, 1531–1538. http://dl.acm.org/citation.\ncfm?id=2980539.2980738\n[13] Bingyi Kang, Zequn Jie, and Jiashi Feng. 2018. Policy optimization with demon-\nstrations. In International Conference on Machine Learning. 2474–2483.\n[14] Beomjoon Kim, Amir-massoud Farahmand, Joelle Pineau, and Doina Precup.\n2013. Learning from Limited Demonstrations. In Advances in Neural Information\nProcessing Systems 26, C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and\nK. Q. Weinberger (Eds.). Curran Associates, Inc., 2859–2867. http://papers.nips.\ncc/paper/4918-learning-from-limited-demonstrations.pdf\n[15] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[16] W Bradley Knox and Peter Stone. 2009. Interactively shaping agents via human\nreinforcement: The TAMER framework. In Proceedings of the fifth international\nconference on Knowledge capture. ACM, 9–16.\n[17] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,\nYuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with\ndeep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).\n[18] James MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, Guan Wang, David L\nRoberts, Matthew E Taylor, and Michael L Littman. 2017. Interactive learning\nfrom policy-dependent human feedback. In Proceedings of the 34th International\nConference on Machine Learning-Volume 70. JMLR. org, 2285–2294.\n[19] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei a Rusu, Joel Veness,\nMarc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen\nKing, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015.\nHuman-level control through deep reinforcement learning. Nature 518, 7540\n(2015), 529–533.\n[20] D Moorhouse and R Woodcock. 1980. US Military Specification MIL–F–8785C.\nUS Department of Defense (1980).\n[21] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. 2018. Over-\ncoming Exploration in Reinforcement Learning with Demonstrations. In 2018\nIEEE International Conference on Robotics and Automation (ICRA). 6292–6299.\nhttps://doi.org/10.1109/ICRA.2018.8463162\n[22] Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan,\nDavid Budden, Gabriel Barth-Maron, Hado van Hasselt, John Quan, Mel Vecerík,\nMatteo Hessel, Rémi Munos, and Olivier Pietquin. 2018. Observe and Look\nFurther: Achieving Consistent Performance on Atari. CoRR abs/1805.11593\n(2018). arXiv:1805.11593 http://arxiv.org/abs/1805.11593\n[23] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schul-\nman, Emanuel Todorov, and Sergey Levine. 2018. DAPG for Dexterous Hand\nManipulation. https://github.com/aravindr93/hand_dapg. (2018).\n[24] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John\nSchulman, Emanuel Todorov, and Sergey Levine. 2018.\nLearning Complex\nDexterous Manipulation with Deep Reinforcement Learning and Demonstra-\ntions. In Proceedings of Robotics: Science and Systems. Pittsburgh, Pennsylvania.\nhttps://doi.org/10.15607/RSS.2018.XIV.049\n[25] S Russell. 1998. Learning agents for uncertain environments (extended abstract).\nConference on Computational Learning Theory (COLT) (1998), 1–3.\n[26] William Saunders, Girish Sastry, Andreas Stuhlmueller, and Owain Evans. 2018.\nTrial without error: Towards safe reinforcement learning via human interven-\ntion. In Proceedings of the 17th International Conference on Autonomous Agents\nand MultiAgent Systems. International Foundation for Autonomous Agents and\nMultiagent Systems, 2067–2069.\n[27] Stefan Schaal. 1996. Learning from Demonstration. In Proceedings of the 9th Inter-\nnational Conference on Neural Information Processing Systems (NIPS’96). MIT Press,\nCambridge, MA, USA, 1040–1046. http://dl.acm.org/citation.cfm?id=2998981.\n2999127\n[28] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. 2015. Prioritized\nExperience Replay. (2015). arXiv:cs.LG/1511.05952\n[29] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. 2017. AirSim:\nHigh-Fidelity Visual and Physical Simulation for Autonomous Vehicles. In Field\nand Service Robotics. arXiv:arXiv:1705.05065 https://arxiv.org/abs/1705.05065\n[30] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and\nMartin Riedmiller. 2014. Deterministic policy gradient algorithms.\n[31] Richard Sutton and Andrew Barto. 1998. Reinforcement Learning: An Introduction.\nMIT Press.\n[32] George E Uhlenbeck and Leonard S Ornstein. 1930. On the theory of the Brownian\nmotion. Physical review 36, 5 (1930), 823.\n[33] Matej Večerík, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal\nPiot, Nicolas Heess, Thomas Rothörl, Thomas Lampe, and Martin Riedmiller.\n2017. Leveraging demonstrations for deep reinforcement learning on robotics\nproblems with sparse rewards. arXiv preprint arXiv:1707.08817 (2017).\n[34] Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone. 2018.\nDeep tamer: Interactive agent shaping in high-dimensional state spaces. In Thirty-\nSecond AAAI Conference on Artificial Intelligence.\n[35] Nicholas R. Waytowich, Vinicius G. Goecks, and Vernon J. Lawhern. 2018.\nCycle-of-Learning for Autonomous Systems from Human Interaction. CoRR\nabs/1808.09572v1 (2018). arXiv:1808.09572v1 https://arxiv.org/abs/1808.09572v1\n[36] Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Man-\nnor. 2018. Learn What Not to Learn: Action Elimination with Deep Reinforcement\nLearning. In Advances in Neural Information Processing Systems 31, S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.).\nCurran Associates, Inc., 3562–3573.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-10-09",
  "updated": "2020-04-03"
}