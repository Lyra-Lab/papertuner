{
  "id": "http://arxiv.org/abs/1910.05915v1",
  "title": "Knowledge-guided Unsupervised Rhetorical Parsing for Text Summarization",
  "authors": [
    "Shengluan Hou",
    "Ruqian Lu"
  ],
  "abstract": "Automatic text summarization (ATS) has recently achieved impressive\nperformance thanks to recent advances in deep learning and the availability of\nlarge-scale corpora. To make the summarization results more faithful, this\npaper presents an unsupervised approach that combines rhetorical structure\ntheory, deep neural model and domain knowledge concern for ATS. This\narchitecture mainly contains three components: domain knowledge base\nconstruction based on representation learning, attentional encoder-decoder\nmodel for rhetorical parsing and subroutine-based model for text summarization.\nDomain knowledge can be effectively used for unsupervised rhetorical parsing\nthus rhetorical structure trees for each document can be derived. In the\nunsupervised rhetorical parsing module, the idea of translation was adopted to\nalleviate the problem of data scarcity. The subroutine-based summarization\nmodel purely depends on the derived rhetorical structure trees and can generate\ncontent-balanced results. To evaluate the summary results without golden\nstandard, we proposed an unsupervised evaluation metric, whose hyper-parameters\nwere tuned by supervised learning. Experimental results show that, on a\nlarge-scale Chinese dataset, our proposed approach can obtain comparable\nperformances compared with existing methods.",
  "text": "Knowledge-guided Unsupervised Rhetorical Parsing for Text Summarization\nShengluan Houa,b,∗, Ruqian Lua,c\naInstitute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China\nbUniversity of Chinese Academy of Sciences, Beijing 100049, China\ncAcademy of Mathematics and Systems Sciences & Key Lab of MADIS, Chinese Academy of Sciences, Beijing 100190, China\nAbstract\nAutomatic text summarization (ATS) has recently achieved impressive performance thanks to recent advances in\ndeep learning and the availability of large-scale corpora. To make the summarization results more faithful, this\npaper presents an unsupervised approach that combines rhetorical structure theory, deep neural model and domain\nknowledge concern for ATS. This architecture mainly contains three components: domain knowledge base construc-\ntion based on representation learning, attentional encoder-decoder model for rhetorical parsing and subroutine-based\nmodel for text summarization. Domain knowledge can be eﬀectively used for unsupervised rhetorical parsing thus\nrhetorical structure trees for each document can be derived. In the unsupervised rhetorical parsing module, the idea of\ntranslation was adopted to alleviate the problem of data scarcity. The subroutine-based summarization model purely\ndepends on the derived rhetorical structure trees and can generate content-balanced results. To evaluate the summary\nresults without golden standard, we proposed an unsupervised evaluation metric, whose hyper-parameters were tuned\nby supervised learning. Experimental results show that, on a large-scale Chinese dataset, our proposed approach can\nobtain comparable performances compared with existing methods.\nKeywords: Automatic text summarization, Rhetorical structure theory, Domain knowledge base, Attentional\nencoder-decoder, Natural language processing\n1. Introduction\nAutomatic text summarization (ATS) aims to produce a condensed representation while keeping the salient el-\nements from one or a group of topic-related documents, which is a potential research area receiving considerable\nattentions from academia to industry. With the amounts of data are being generated in the Web age, ATS plays an in-\ncreasingly important role in addressing the problem of how to acquire information and knowledge in a fast, reliable and\neﬃcient way. Generally, ATS can be categorized into two types: extractive summarization and abstractive counterpart\n[1, 12]. Extractive text summarization approaches directly extract salient words, sentences, or other granularities of\ntexts to produce the summary. Conversely, abstractive models paraphrase the salient contents using NLG techniques.\nAbstractive methods concerns the generation of new sentences, new phrases while retaining the same meaning as\nthe same source documents have, which are more complex than extractive ones. Extractive approaches are now the\nmainstream ones. According to the number of input documents, ATS can also be classiﬁed into single-document\nsummarization (SDS) and multi-document summarization (MDS) methods.\nIn this paper, we focus on extractive SDS task. The key to SDS is how to score the salience of candidate text\nsummary units (i.e. sentences, clauses, ect). There are lexical chain based approaches [19], classical machine learning\napproaches [10], graph-based unsupervised methods [43] etc. With the recent advances in deep learning, ATS has\nbeneﬁted much from these new ideas and gained considerable improvements [7, 44, 56, 61]. These approaches are\ndescribed in more detail in Section 2. To make the results more faithful and coherent, we incorporate discourse\nstructures into summarization generation.\n∗Corresponding author\nEmail addresses: houshengluan1989@163.com (Shengluan Hou), rqlu@math.ac.cn (Ruqian Lu)\nPreprint submitted to Elsevier\nOctober 15, 2019\narXiv:1910.05915v1  [cs.CL]  14 Oct 2019\nDiscourse structure theories involve understanding the part-whole nature of textual documents. The task of rhetor-\nical parsing, for example, involves understanding how two text spans are related to each other in the context. As Web\nmining extracts latent knowledge from the Web content [57], rhetorical parsing reveals the meaningful knowledge out\nof vast amounts of documents and help in improving many NLP applications. The theoretical foundation is rhetorical\nstructure theory (RST) [39], which a comprehensive theory of discourse organization. RST investigates how clauses,\nsentences and even larger text spans connect together into a whole. RST assumes that discourse is not merely a col-\nlection of random utterances but the discourse units connect to each other as a whole in a logical and topological way.\nRST explains text coherence by postulating a hierarchical, connected tree-structure (denote as RS-tree) for a given text\n[8], in which every part has a role, a function to play, with respect to other parts in the text. RST has been empirically\nproved useful for improving the performance of NLP tasks that need to combine meanings of larger text units, such\nas single-document summarization [17, 36], QA and chabot [11] and text classiﬁcation [22, 26].\nOur proposed model can beneﬁt from RST for faithful results, which mainly consists of three components: do-\nmain knowledge base construction based on representation learning, attentional encoder-decoder model for rhetorical\nparsing and subroutine-based model for text summarization. The ﬁrst component extracts domain keywords based\non representation learning. The domain keywords contain three types: acting agents, major inﬂuence factors and\ndynamics of a domain. The second component leverages the output of the ﬁrst component for RS-tree construction,\nwhich will be fed to the third component for summary generation.\nOur aim is Chinese-oriented text summarization. To alleviate the problem of data scarcity, we leverage the labeled\nEnglish data RST-DT [6] and map texts of English and Chinese into the same latent space, from which the rhetorical\nrelation between two Chinese text spans can be determined. Furthermore, the last component extracts summary texts\nfrom the derived RS-trees. The generated summary from our subroutine-based text summarization model can be\nalways balanced between nucleus and satellite subtrees.\nThe contributions of this work can be concluded as follows:\n• We ﬁrst proposed an unsupervised Chinese-oriented rhetorical parsing method. Existing rhetorical parsing\nmethods are English-oriented, supervised methods that often trained on RST-DT, a human-annotated discourse\ntreebank of WSJ articles under the framework of RST. Our proposed method leverages the idea of translation\nand embeds the Chinese and English texts in the same latent space. In this way, the rhetorical relations between\nChinese text spans can be determined by the the rhetorical relations in RST-DT.\n• Domain knowledge was utilized in the rhetorical parsing procedure, which was constructed based on represen-\ntation learning. Domain knowledge was used in two aspects: one for discourse segmentation and the other one\nfor guiding rhetorical structure inference. Furthermore, attention mechanism was adopted in rhetorical parsing\nthus the attention weights enable our model has the ability to focus on relevant and drown-out irrelevant parts\nof the input.\n• Diﬀerent from the majority of literature, our subroutine-based summarization model is purely based on the\ngenerated rhetorical structure. The basic processing unit is elementary discourse unit (EDU), which is rela-\ntive shorter than sentence. Thus the generated summary can be more informative. This model is based on\n‘importance ﬁrst’ principle, each time the ‘currently’ most important EDU from the rhetorical structure will\nbe selected one by one mechanically. The ‘importance ﬁrst’ principle makes the selection of EDUs alternated\nbetween nucleus and satellite subtrees. Thus the generated summary can always be balanced.\n• We also proposed an unsupervised summarization evaluation metric. This evaluation metric considers many\naspects of how faithful a generated summary is. To make this evaluation metric more eﬀective, the hyper-\nparameters were tuned by supervised learning on the golden standard of DUC2002.\nThe remainder of this paper is organized as follows. Section 2 reviews some related works, including approaches\nabout domain knowledge, rhetorical parsing and automatic text summarization. Section 3 is domain knowledge base\nconstruction based on representation learning. The large-scale Chinese dataset and experimental results on it will also\nbe given. The unsupervised rhetorical parsing approach is elaborated in Section 4, in which the idea of translation\nand attention mechanism were adopted. Section 5 is about the subroutine-based text summarization. An unsupervised\nsummarization evaluation metric and experimental results are shown in Section 6. The paper is concluded with a brief\nsummary and an outlook for further research in Section 7.\n2\n2. Related Works\nIn this section, we brieﬂy review some related works. In Subsection 2.1, we will ﬁrst discuss works about domain\nknowledge. Subsection 2.2 then introduces rhetorical structure theory, which is an important theoretical foundation\nof our work. Finally, the latest and classical approaches of automatic text summarization will be described in Subsec-\ntion 2.3.\n2.1. Domain Knowledge\nKnowledge is power. Domain knowledge plays a signiﬁcant role in many NLP tasks. For instance, the knowledge\ngraph (KG) is a knowledge base proposed by Google to enhance its search engine’s results with information gathered\nfrom a variety of sources. Li and Mao [32] proposed an eﬀective way of combing human knowledge and information\nfrom data for CNN to achieve better performance. They presented K-CNN: a knowledge-oriented CNN for causal\nrelation extraction. In K-CNN, the convolutional ﬁlters are automatically generated based on WordNet and FrameNet.\nThe data-oriented channel is used to learn other important features of causal relation from the data. Lu et al. [38]\nstudied the concepts of big knowledge, big-knowledge system and big-knowledge engineering. Ten massiveness\ncharacteristics for big knowledge and big-knowledge systems are deﬁned and explored. Zheng [60] explored how to\nenable humans to use big knowledge correctly and eﬀectively in biomedical domain. There are also some knowledge-\nbased text summarization methods, we refer to [14, 52].\nDomain knowledge keyword extraction is deﬁned as the task that automatically identiﬁes a set of the terms that\nbest describe the domain of documents [46]. Generally, domain keyword extraction approaches can be divided into\ntwo categories as unsupervised methods and supervised methods. TF-IDF is one of the simplest unsupervised ap-\nproaches. The top-k high TF-IDF value words are chosen as keywords. Until now, TF-IDF remains a strong unsuper-\nvised baseline [42]. TextRank [43] is another typical unsupervised method, which formulates keyword extraction as\n“recommendation”. The supervised methods often take keyword extraction as classiﬁcation problems [4]. However,\na number of annotated dataset is needed, which is limited for unlabeled data. Kong et al. [25] constructed a Chinese\nsentiment lexicon using representation learning. A skip-gram model was built to predict word embeddings accord-\ning to the context words and their composing characters, whose outputs were then fed into a Random Forest (RF)\nclassiﬁer. Words of the same polarity were then grouped together to form the sentiment lexicon.\nWith regard to KG, YAGO is automatically extracted from Wikipedia and other sources. YAGO2 [18] contains\n447 million facts about 9.8 million entities, in which an article in Wikipedia becomes an entity. DBpedia [29] extracts\nfact triples from 111 diﬀerent language versions of Wikipedia. To tackle the problem of low recall for pattern-based\napproaches, Angeli et al. [2] leveraged dependency parsing tree for relation triple extraction. They constructed a few\npatterns for canonically structured sentences, and shift the focus to a classiﬁer which learns to extract self-contained\nclauses from long sentences. On the other hand, the key idea of KG embedding is to embed components of a KG into\ncontinuous vector spaces and thus to simplify the manipulation while preserving the inherent structure of the KG [54].\nTypical methods contain TransE [5], TransH [55], TransR [35], etc. KG embedding has been applied to and beneﬁts\na wide variety of downstream NLP tasks such as KG completion, question answering, and so on.\n2.2. Rhetorical Structure Theory\nRhetorical structure theory [39] is a comprehensive theory of text organization. With more and more attentions on\nthis theory, RST has been applied to many high-level NLP applications since Marcu’s earlier works on RST parsing\nand applications on text summarization [41]. RST is now one of the most popular theories for discourse analysis.\nCentral to RST is rhetorical relation, which exists between two neighboring text units. The interpretation of\nhow text spans are semantically related to each other described by rhetorical relations is crucial to retrieve important\ninformation from documents.There are two types of rhetorical relations: mononuclear relations and multi-nuclear\nrelations. In the former ones, one of the text spans is more important than the other one, which play the role of\nnucleus and satellite respectively. One the other hand, all text spans are equally salient in multi-nuclear relations,\nwhich are all play the role of nucleus. Nucleus and satellite play diﬀerent roles to the writer’s purpose. In general, what\nnucleus of a rhetorical relation expresses is more essential than what satellite expresses; The nucleus is comprehensible\nindependent of the satellite, but not vice versa.\nAccording to RST, the minimum processing unit is EDU. EDU acts as a syntactic constituent that has independent\nsemantics. In this sense, an EDU corresponds to a clause or a simple sentence. RST explains text coherence by\n3\npostulating a hierarchical, connected tree-structure (i.e. RS-tree) for a given text. In the RS-tree, each leaf node\ncorresponds to an EDU. Each internal node corresponds to a larger text span which captures the rhetorical relation\nbetween its two children.\nRhetorical parsing aims to generate EDU sequences and RS-trees for given documents. It involves ﬁnding roles\nfor every granularity of text spans and rhetorical relations that hold between them. There are rule-based methods,\ntraditional machine learning methods and deep learning methods. LeThanh et al. [30] used syntactic information\nand cue phrases to segment sentences and integrated constraints about textual adjacency and textual organization to\ngenerate best RS-trees. Toﬁloski et al. [53] presented a syntactic rules and lexical rules based discourse segmenter\n(SLSeg). Soricut and Marcu’s SPADE model [51] used two probabilistic models for sentence-level analysis, one for\nsegmentation and the other for RS-tree building. After that, most research focused on SVM-based discourse analysis.\nThey regarded relation identiﬁcation as classiﬁcation problem [9, 16]. Joty et al. [24] ﬁrst used Dynamic Conditional\nRandom Field (DCRF) for sentence-level discourse analysis, and then proposed a two stage rhetorical parser. Recent\nadvances in deep learning led to further progress in rhetorical parsing. DPLP [21] is a representation learning method,\nwhose main idea is to project lexical features into a latent space. DPLP constructs RS-trees in a shift-reduce way.\nA multi-class linear SVM classiﬁer was learned to decide whether shift or reduce operation would be taken. Li et\nal.’s recursive method [31] contains two components. The ﬁrst is to obtain the distributed representation for sentences\nusing recursive convolution based on its syntactic tree. The second component contains two classiﬁers, one is used\nfor determining whether two adjacent nodes should be merged. If so, the other one selects the appropriate rhetorical\nrelation to the new merged subtree.\n2.3. Automatic Text Summarization\nAutomatic text summarization has spurred a surge of research and experimentation since its remarkable eﬀect in\nmodern Web age. With the fast development of deep learning technologies, many eﬀorts applied encoder-decoder\nmodels into ATS. The usage of attention mechanism into text summarization was ﬁrst brought to prominence by Rush\net al. [49]. This attentional encoder-decoder abstractive model was trained on large-scale Gigaword1 dataset. Its\nvariants and further improvements include [34, 44] et al. Neural extractive methods are also popular, such as pointer\nnetwork-based models [7, 50], SummaaRuNNer [44], SWAP-NET [20], etc. Most of the extractive models are trained\non CNN/DM2 dataset. However, large-scale dataset is necessary for these neural models since they are purely data-\ndriven. For MDS, parallel data is scarce and costly to obtain. To tackle this predicament, Lebanoﬀet al. presented\nPG-MMR [28], an adaptation method from single to MDS, to generate abstract summaries from multiple documents.\nThis method is new, but also aﬀected by the data source and data scale.\nBesides the above deep learning-based approaches, there are also other solutions, such as traditional machine\nlearning-based methods, optimization-based methods, graph-based ones, etc. ATS was taken as an optimization prob-\nlem in [13], the ILP-based method did exact inference under a maximum coverage model. Other traditional machine\nlearning-based methods take TF-IDF, n-gram, the position and others as features to extract summary sentences. For\nmore details, we refer to [1, 12]. Graph-based methods have become increasingly prevalent and far-reaching since\ntheir easy implementation and relative good performance, such as Textrank [43]. Another representative of unsu-\npervised algorithm is SummCoder [23], whose summary sentence selection module contains three metrics: sentence\ncontent relevance is measured by a deep auto-encoder network, sentence novelty is measured by sentence similarity\nbased on sentence embeddings and sentence position relevance is derived by a hand-designed score function.\nThe authors of RST have long speculated that the nuclei in RS-tree constitute an adequate summarization of the\ntext. It was ﬁrst validated by Marcu [40]. Louis et al. [36] proved that the structure features (i.e. position in the\nglobal structure of the whole text) of RS-tree are the most useful feature to compute the salience of text spans. Hirao\net al. [17] treated summary generation as a tree knapsack problem. They transformed an RS-tree into a dependency-\nbased discourse tree (DEP-DT), which can be directly used to take tree trimming method for text summarization. For\nMDS, to address redundancy problem, Zahri et al. [58] used RS-trees for cluster-based MDS. They utilized rhetorical\nrelations that exist between two sentences to group similar sentences into multiple clusters to identify themes of\ncommon information, from which candidate summary sentences were extracted. In this paper we propose further\ncontribution to this approach, focusing on unsupervised extractive summarization.\n1https://catalog.ldc.upenn.edu/ldc2003t05\n2https://github.com/deepmind/rc-data\n4\n3. Domain Knowledge Base Construction Based on Representation Learning\nDomain knowledge plays a signiﬁcant role in many NLP tasks. At present, most of the existing knowledge bases\nare in the form of knowledge graph, such as YAGO, DBpedia, etc, which generally consists of entity and relation\ntriples. The knowledge triples in a KG are composed of two entities along with their relation, which are in the form\nof < e1, r1, e2 >, where e1 and e2 are entities that often nouns or noun phrases, r1 is the relation between e1 and e2.\nHowever, knowledge keywords for a domain are also indispensable. For a domain, knowledge keywords can provide\na panorama for this domain. In this section, we propose a framework of constructing domain knowledge base on the\nbasis of representation learning. Our proposed domain knowledge contains three types of keywords: acting agents,\nmajor inﬂuence factors and dynamics of a domain.\nWe deﬁne the domain as:\nDeﬁnition 1 (Domain). A domain is a particular area of human knowledge. Such as education, ﬁnance, et al.\nFor a domain, keywords can be regarded as the knowledge generalization of the full text in a corresponding\nliterature and help readers to quickly grasp the core idea, core technique, or core methodology, etc. In general, two\ndiﬀerent domains have diﬀerent knowledge keywords, but maybe with some common knowledge keywords. The\ndeﬁnition of domain knowledge keyword is given in Deﬁnition 2.\nDeﬁnition 2 (Domain Knowledge Keyword, DKK). A domain knowledge keyword is a basic and characteristic el-\nement of this domain, which is represented by a word or phrase and is often referred to when talking about some\naspects of this domain. DKK can generalize the main topics of domain texts.\nExample 1. “Teacher”, “Student”, “Professor”, “Teach”, “Learn”, “Library”, “Course”, “Doctoral” are DKKs\nof the domain “Education”.\nTwo diﬀerent domains may share some of their DKKs (e.g. “Library” may be a DKK of some other domains), but\nnever share their whole sets of DKKs. The less the size of the shared DKKs, the more are the two domains diﬀerent\nfrom each other.\nWe argue that besides nouns, verbs and adjectives (adverbs) also serve as the key components. In the above\nexample, “Teacher” is a noun, “Teach” is a verb, and “Doctoral” is an adjective. In fact, these three types of keywords\nconstitute the main types of DKKs. For each domain, we construct domain knowledge from large-scale texts. The\nDKB in this work is composed of a set of triples containing domain keywords.\nDeﬁnition 3 (Domain Knowledge Base, DKB). For a domain, the DKB can be represented by a triple:\n< A, P, T >\n(1)\nwhere\n• A denotes nouns and named entities, each of which represents the acting agents of this domain;\n• P acts as the major inﬂuence factors of this domain, which are nouns;\n• T denotes the concepts about dynamics of this domain, each of which is often adjective or adverb.\nThese three types of keywords constitute a full DKB for a domain.\nOur goal is to construct the DKB for each domain in a fast and eﬃcient manner. Traditional methods can obtain\nhigh accuracy but with low recall, it also need much eﬀorts when used to a new domain. On the other hand, the more\nand more popular word embedding methods have the pros of robust and eﬃcient. It can be trained on large scale\ndataset without any other extra resource. We leverage the representation learning from word embedding methods for\nDKB construction.\nDeﬁnition 4 (Domain Knowledge Base Construction, DKBC). Given a large set of documents that consists of texts\nfor several domains {D1, D2, · · · , Dt}, DKBC aims to extract a DKB from each domain texts Di(1 ≤i ≤t), the\nconstructed DKB is in the form of (1).\n5\nSpecially, for each domain Di, given the corresponding documents {d1, d2, · · · , dk}, DKBC can automatically\ngenerate three types of DKKs as deﬁned in Deﬁnition 2. All generated DKKs can constitute the DKB (denote as\nDKBi) in the form of (1) such that:\n• If wm ∈DKBi, then wm ∈DICTi, where DICTi is the vocabulary taken from Di;\n• Suppose DKBi =< Ai, Pi, Ti >, wp ∈Ai, wq ∈Pi, then p , q.\nTo obtain better results, our model is an integration of three diﬀerent models. The ﬁrst one is representation\nlearning based model, which we call VWRank. The other two models are TF-IDF model and TextRank model. TF-\nIDF is an important indicator of the word’s saliency. TextRank is an “recommendation” strategy for voting salient\nwords.\n3.1. The Architecture of VWRank\nOur DKBC model utilizes representation learning from word embedding approaches. We use the improved word\nrepresentation learning with sememes method, called SE-WRL [45]. The sememe knowledge base they used is\nHownet [59]. SE-WRL provides diﬀerent strategies, among which SE-WRL-SAT achieved the best performance\naccording to their original paper. SE-WRL-SAT learns the original word embeddings for context words, but sememe\nembeddings for target words.\nFor each domain, we use SE-WRL-SAT to learn word representations. Then we deﬁne the similarity between two\ncandidate words cw1 = (x1\n1, x1\n2, · · · , x1\nn) and cw2 = (x2\n1, x2\n2, · · · , x2\nn) as consine distance:\nS im(cwi, cw j) =\ncwi · cw j\n||cwi|| ∗||cw j||\n=\nnP\nk=1\n(x1\nk ∗x2\nk)\nr nP\nk=1\n(x1\nk)2 ∗\nr nP\nk=1\n(x2\nk)2\n(2)\nObviously, the following two properties hold:\n• S im(cwi, cwj) = S im(cw j, cwi);\n• S im(cwi, cwj) ∈[−1, 1].\nMotivated by TextRank, the score of candidate keyword cwi can be computed as:\nS core(cwi) = (1 −d) + d ∗\nX\ncwj∈S (cwi)\nS im(cw j, cwi)\nP\ncwk∈S (cwj)\nS im(cw j, cwk)S core(cw j)\n(3)\nwhere d ∈(0, 1) is a damping factor, which has the role of integrating into the model the probability of jumping from\na given candidate word to another random candidate word. S (cwi) and S (cw j) are two sets of candidate words that cwi\nand cw j similar with, respectively. The similarity between two candidate keywords is computed by (2). After several\niterations, the S core(cwi) can converge to a ﬁxed value.\n3.2. Model Integration\nBesides VWRank, in a domain, we also calculate the TF-IDF and TextRank values for candidate words in each\ndomain. We denote the high score candidate keywords of VWRank, TF-IDF and TextRank as Cvw, Cti and Ctr. The\nﬁnal score of a candidate keyword is computed as:\nS core(cwi) = α ∗I(Cwv, cwi) + β ∗I(Cti, cwi) + γ ∗I(Ctr, cwi)\n(4)\nwhere α, β and γ are harmonic coeﬃcients, I(·) is the indicator function such that\nI(C, cw) =\n( 1, if cw ∈C\n0, else\n(5)\n6\nTable 1: Excerpts of url and its corresponding domain\nUrl\nDomain\nhttp://www.xinhuanet.com/world/\n国际(World)\nhttp://news.china.com/zh_cn/international/\n国际(World)\nhttp://finance.sina.com.cn/\n财经(Finance)\nhttp://sports.china.com/\n体育(Sports)\nhttp://china.soufun.com/\n房产(House)\n· · ·\n· · ·\nTable 2: The statistics of documents in each domain\nDomain\nSports\nIT\nMilitary\nOlympic\nCulture\nHouse\nDomestic\nEntertainment\n#Docs\n323,861\n22,033\n17,607\n74,374\n7,212\n22,381\n2,454\n93,949\nAvg. #Sens.\n16.49\n20.58\n19.27\n17.09\n23.49\n17.73\n19.46\n16.63\nAvg. #Words.\n334.65\n493.37\n447.82\n385.83\n505.17\n229.97\n490.63\n372.77\nDomain\nAuto\nFinance\nLady\nHealth\nEducation\nSociety\nWorld\nTotal\n#Docs\n44,462\n263,575\n72,970\n5,712\n53,197\n2,698\n2,566\n1,009,231\nAvg. #Sens.\n18.48\n22.21\n15.49\n20.04\n21.22\n20.60\n14.25\n18.55\nAvg. #Words.\n413.14\n492.01\n277.30\n332.22\n418.76\n419.98\n333.31\n391.75\nThen the ﬁnal DKKs are composed of candidate keywords that further ﬁltered by the value of:\nptt(cwi) = The number of documents that cwi presents\nThe number of all documents in this domain\n(6)\nFinally, all selected DKKs will be organized in hierarchies by their semantic in Hownet.\n3.3. Dataset: SogouCA\nSogouCA3 is a large-scale Chinese corpus, which is crawled and provided by Sogou Labs from dozens of Chinese\nnews websites, including news reports and reviews.\nEach document in SogouCA contains ﬁelds of “url”, “docno”, “contenttitle”, and “content”. Leveraging “url”\ninformation, we can categorize documents into corresponding domains. Excerpts of “url” and its corresponding\ndomain are shown in Table 1.\nAfter that, we collected texts for 15 domains. We did preprocessing including delete empty or very short lines,\nignore extreme long lines, etc. The statistics of documents in each domain are listed in Table 2, from which we can\nsee that most of the domains contain tens of thousands of documents.\nUnlike English, to manipulate text at the word level, word segmentation is needed for Chinese text processing. We\nused HanLP [15] for Chinese word segmentation, part-of-speech tagging and named entity recognition (NER), which\nis a Chinese natural language processing tool.\n3.4. Experimental Results of DKBC\nWe have ﬁnished the DKBC for 15 domains according to the above methods. The derived 15 DKBs can be used\nfor other NLP applications. The statistics of DKB in each domain are shown in Table 3.\nFig. 1 shows some DKKs of “Finance” domain in Chinese.\nThe DKKs contains keywords such as “ 中国\n(China)”(Agent), “ 投资者(investor)”(Phenomenon), “ 风险(risk)”(Phenomenon), “ 上涨(increase)”(Tendency),\netc. These words can provide a panorama for domain “Finance”.\n3https://www.sogou.com/labs/resource/ca.php\n7\nTable 3: The statistics of DKB in each domain\nDomain\nSports\nIT\nMilitary\nOlympic\nCulture\nHouse\nDomestic\nEntertainment\n#Agents\n2,687\n622\n561\n1,706\n179\n308\n126\n1,318\n#Phenomenons\n6,162\n1,580\n1,186\n3,416\n439\n996\n267\n4,422\n#Tendencies\n5,290\n1,004\n767\n2,316\n230\n564\n141\n3,448\nDomain\nAuto\nFinance\nLady\nHealth\nEducation\nSociety\nWorld\nTotal\n#Agents\n533\n2,856\n637\n56\n902\n93\n130\n12,723\n#Phenomenons\n1,999\n5,469\n3,504\n413\n2,298\n278\n204\n32,633\n#Tendencies\n1,440\n3,308\n3,011\n247\n1,579\n135\n139\n23,619\nFig. 1. The DKKs of “Finance” domain\nOn the other hand, in the domain of “IT” (Fig. 2), some DKKs are “ 微软(Microsoft)”(Agent), “ 排行(Rank-\ning)”(Phenomenon), “ 市场(market)”(Phenomenon), “ 上市(Be listed)”(Tendency), etc. These two examples can\nvalidate the eﬀectiveness of our method.\n4. Unsupervised Rhetorical Parsing\nRhetorical structure theory was proposed as a way to attribute structure to text, which often represents a text as\na tree structure. It is characterized by rhetorical relations, which reﬂect the semantic and functional judgments about\nthe text spans they connect. We ﬁrst give a formal deﬁnition of rhetorical structure tree.\nDeﬁnition 5 (Rhetorical Structure Tree). Rhetorical Structure Tree (RS-tree) is a tree representation of a document\nunder the framework of RST. The leaf nodes of a RS-tree are EDUs. Each internal node is characterized by a rhetorical\nrelation and corresponds to a contiguous text span. The siblings are connected via a rhetorical relation such that in\nmost cases one is nucleus and the other is satellite. The siblings are both nucleus when they are connected by a\nmulti-nuclear relation.\nIn Deﬁnition 5, EDU is the minimal textual unit of an RS-tree, which means that it can’t be split into smaller\ntext spans. EDU acts as a syntactic constituent that has independent semantics. In this sense, an EDU functionally\ncorresponds to a simple sentence or a clause in a complex sentence.\nDeﬁnition 6 (Rhetorical Parsing). Rhetorical Parsing, also called RST analysis, RST parsing, or rhetorical anal-\nysis, is a procedure of generating EDU sequences and deriving RS-trees for given texts. It involves segmenting\ndiscourse into EDUs and ﬁnding roles for every granularity of text spans (EDUs, sentences, paragraphs and even\nlarger spans) and rhetorical relations that hold between them.\nAs depicted in Deﬁnition 6, rhetorical parsing contains two steps: discourse segmentation and RS-tree construc-\ntion. An example RS-tree for a given Chinese text is shown in Fig. 3. The leaf nodes numbered with digits are four\n8\nFig. 2. The DKKs of “IT” domain\nFig. 3. An example of RS-tree\nEDUs. The internal nodes corresponds to text spans are characterized by rhetorical relations (such as Joint and Elab-\noration). The arrow from A to B denotes A and B are satellite and nucleus respectively in the sense of that relation.\nThey are both nuclei when A and B have multi-nuclear relation. Horizontal lines correspond to text spans, and vertical\nlines identify text spans which are nuclei.\n4.1. Discourse Segmentation based on Domain Knowledge Base\nLeveraging domain knowledge, we segment each document in each domain into EDU sequence. According to\nDeﬁnition 5, EDU functionally corresponds to a simple sentence or a clause. We ﬁrst segment a text into paragraphs\nand further sentences by punctuations. Then DKB is used for segmenting sentences into EDUs. Concretely, for a\ndomain, given its DKB Kd and domain texts Td, for each text td\ni ∈Td, Algorithm 1 is the detailed segmentation\nalgorithm.\nAlgorithm 1 Discourse Segmentation for a Domain Text\nInput: A document td\ni ; DKB Kd.\nOutput: EDU sequences S edu.\n1: Segment td\ni into sentence sequence S by punctuations (line break for segmenting into paragraphs, period, question\nmark, etc for segmenting into sentences).\n2: for each sentence sj in S do\n3:\nScan sj and match their words against the domain keywords in Kd;\n4:\nIf the domain keywords of a clause have the form “A+P+T” or “P+T”, then put it into S edu.\n5: end for\n6: Output the EDUs in S edu according to their order in the original text.\n9\nFig. 4. The architecture of rhetorical relation identiﬁcation\nAfter Algorithm 1, each derived EDU is a part of a sentence or clause, characterizing the domain relatedness of\nits elements. Moreover, most EDUs have the form of “A+P+T” with respect to domain keywords. For the form of\n“P+T”, we borrow a agent keyword from the nearest neighbor EDU to form a complete triple. After that, each EDU\nhas a DKB triple < a, p, t >.\n4.2. Rhetorical Structure Theory Discourse Treebank\nFor RS-tree construction, existing models contain classical machine learning-based methods and deep learning-\nbased methods, almost all of which are supervised methods. These approaches were trained on Rhetorical Structure\nTheory Discourse Treebank (RST-DT) [6]. RST-DT was developed as a human-annotated discourse level corpus with\nRS-trees for 385 English-written Wall Street Journal texts. These texts were manually annotated by the professional\nlanguage analysts grounded in the framework of RST. There are 78 ﬁne-grained rhetorical relations that grouped\ninto 18 coarse-grained relation categories. In the existing approaches, the latter 18 categories are often used for\ntraining and testing. Since there exist multi-nuclear relations, non-binary relations are often converted into a cascade\nof right-branching binary relations for convenience. In RST-DT, there are 21,789 EDUs and 21,404 text pairs that are\ncharacterized by rhetorical relations.\n4.3. Attentional Encoder-decoder Model for RS-tree Construction\nThe objective of RS-tree construction is to ﬁnd rhetorical relations between two adjacent text spans(including\nEDUs). Then the RS-tree can be constructed in a bottom-up way. For our Chinese-oriented rhetorical parsing work,\nthere is no human-annotated Chinese-oriented discourse treebank like RST-DT in English. Motivated by the basic\nideas of recent progress in unsupervised machine translation [27], we propose to leverage RST-DT and embed Chinese\ntext spans and English text spans into the same latent space. Thus the rhetorical relation between two Chinese text\nspans can be derived by the the rhetorical relations in RST-DT. Our work is unsupervised since there’s no labeled\nChinese dataset is used. The architecture of rhetorical relation identiﬁcation is shown in Fig. 4.\nThe unsupervised rhetorical parsing model we propose is composed of two encoders, an decoder and two clas-\nsiﬁers. The translation encoder is responsible for encoding Chinese and English texts into a latent space and the\nDKB encoder is used for representing domain keyword sequence. The attention-based decoder 1 and attention-based\ndecoder 2 are the same decoder with same parameters, whose only diﬀerence is the choice of lookup tables when\napplying them to diﬀerent languages. The two classiﬁers are used for rhetorical relation identiﬁcation.\nIn Fig. 4, the components in dotted box are to constrain the model can map text pair from Chinese (English) to\nEnglish (Chinese). Suppose < L1T1, L1T2 > is English (Chinese) text pair, the output of attention-based decoder 1\n10\nis Chinese (English) < L2T1, L2T2 >, which then will be input to the translation encoder. The output of attention-\nbased decoder 2 is English (Chinese) < L1T1′, L1T2′ >. The object of this procedure is to learn a mapping such that\ntranslations are close in the same latent space. The translation loss function is:\nLtrans =\nX\n[∆(< L1T1, L1T2 >, < L1T1′, L1T2′ >) + ∆(< L2T1, L2T2 >, < L2T1′, L2T2′ >)]\n(7)\nwhere ∆is the sum of token-level cross-entropy losses.\nThe second objective of our model is to train two classiﬁers. When < L1T1, L1T2 > is English text pair, we\ndenote < DKB(L1T1), DKB(L1T2) > as its domain keyword sequence. The concatenation of the two encoders’\nhidden states is fed into two classiﬁers. The attention-based span classiﬁer is used for determining whether L1T1 and\nL1T2 should be merged into a new subtree and if so the attention-based relation classiﬁer is used to assign which\nrelation and which role should be labeled to the merged node and its two children respectively. The loss function used\nfor classiﬁcation is also cross entropy loss.\nIn this work, we our propsed model is based on the sequence-to-sequence model with attention [3]. These two\nencoders are both bidirectional-GRU which returns a sequence of hidden states whereas the decoder is also an GRU,\nwhich takes as input the previous hidden state, the current word and a context vector given by a weighted sum over\nthe encoder states.\nThe ﬁnal loss function is:\nL = λtransLtrans + λclasLclas\n(8)\nwhere λtrans and λclas are hyper-parameters, Lclas is classiﬁcation loss.\nFor inference, the input is Chinese EDU text pair along with their domain keywords and the output is (1): whether\nthey can be merged into a subtree; (2) If so, which rhetorical relation and which role should be labeled to the merged\nnode. Then the text of the merged node and its neighboring node’s text will form the new input text pair. Loop this\nstep until a RS-tree for a document has been constructed.\n5. Subroutine-based Model for Automatic Text Summarization\nIn this section, we present a subroutine-based model for automatic text summarization, which has been introduce\nin our previous paper [37]. Diﬀerent from the majority of literature, our subroutine-based summarization model is\npurely based on the generated RS-tree from Section 4.\nThe basic processing unit is EDU, which is relative shorter than sentence. Thus the generated summary can be\nmore informative than summary that composed of sentences. The summarization algorithm is based on ‘importance\nﬁrst’ principle, each time the ‘currently’ most important EDU from RS-tree will be selected one by one mechanically.\nIn this way we can obtain a hierarchy of diﬀerent summarizations level-wise from simple to complex by adding one\nmore EDU at each level. There two ways of controlling the complexity of summarized result: either by specifying the\nword length limit or the rate of text reduction.\nWhen going to produce a summary, the summarization model traverses the RS-tree in a nucleus preference way.\nThat is: (1) a nucleus node is always preferred over its sibling satellite node; (2) if node A is preferred over node B,\nthen all child nodes of A are preferred over B; (3) the selection of EDUs should be alternated between the left and\nright subtrees of the root node whenever both subtrees are not empty. Whenever a leaf node (EDU) is traversed, the\ntext unit represented by it will be put to the ﬁnal summary. In our model, a nucleus node is always preferred over its\nsibling satellite node. Thus the generated summaries will always content-balanced.\nAll details of this subroutine-based text summarization algorithm are clariﬁed in Algorithm 2. The nuc(x) and\nsat(x) mean the nucleus resp. satellite child nodes of x.\nflip(x) is the ﬂip-ﬂop function with flip(0) = 1 and\nflip(1) = 0. d f p and sf p are pointers pointing to the entry of subroutine d finding resp. sfinding. zp. is a formal\nparameter for storing a pointer. For example, after the subroutine call d finding(0, R(T), sat(R(T)), d f p) it is zp = d f p\nin the subroutine body of d finding.\n6. Experimental Results\n6.1. Training Details about Unsupervised Rhetorical Parsing\nThe training of our unsupervised rhetorical parsing was carried out on SogouCA and RST-DT datasets. We used\na mini-batch stochastic gradient descent (SGD) algorithm together with Adam [48] with initial learning rate 0.001\n11\nAlgorithm 2 Subroutine-based Text Summarization\nInput: The RS-tree R(T) of a document; Summary length/cadence ratio r; j = 1; k = 1.\nOutput: The generated EDU sequences Redu.\n1: If R(T) is a leaf node then put R(T) into Redu and goto step 6, else call d finding(0, R(T), sat(R(T)), d f p).\n2: Subroutine d finding(w, x, y, zp): Case: w = 0 →If j = 0 then goto step 6; If nuc(x) is a non-leaf node then call\nd finding(w, nuc(x), y, zp) else put nuc(x) into Redu, call index and call zp.( flip(w), nuc(x), y, sf p); Case: w = 1\n→If k = 0 then goto step 6; If y is a leaf node then put y into Redu, k := 0, if j = 0 then goto step 6 else call index,\ncall zp.( flip(w), x, y, sf p), else If nuc(y) is non-leaf node then call d finding(w, x, nuc(y), zp) else put nuc(y) into\nRedu, call index, call zp.( flip(w), x, nuc(y), sf p).\n3: Subroutine ufinding(w, x, y, zp): Case: w = 0 →If j = 0 then goto step 6 else call zp.( flip(w), parent(x), y, sf p);\nCase: w = 1 →If k = 0 then goto step 6 else call zp.( flip(w), x, parent(y), sf p).\n4: Subroutine sfinding(w, x, y, zp):\nCase:\nw\n=\n0 →If j\n=\n0 then call sfinding( flip(w), x, y, sf p); If\nparent(x) = R(T) then j := 0, if k = 0 then goto step 6 else call sfinding( flip(w), x, y, sf p); If sibling(x)\nhas been travelled then call ufinding(w, x, y, sf p) else if sibling(x) is a leaf node then put sibling(x) into\nRedu, call index, call ufinding(w, x, y, sf p), else call d finding(w, sibling(x), y, sf p); Case:\nw = 1 →If\nk = 0 then call sfinding( flip(w), x, y, sf p); If parent(y) = R(T) then k := 0, if j = 0 then goto step\n6 else call sfinding( flip(w), x, y, sf p); If sibling(y) has been travelled then call ufinding(w, x, y, sf p) else\nIf sibling(y) is a leaf node then put sibling(y) into Redu, call index, call ufinding(w, x, y, sf p), else call\nd finding(w, x, sibling(y), sf p).\n5: Subroutine index: If the word length of Redu satisﬁes r then goto step 6.\n6: Sort the EDUs in Redu according to their order in the original text.\n7: return Redu.\nto train this model. In each epoch, the training data in each batch are the mixture of Chinese and English text pairs.\nWe used Textrank for DKBC of RST-DT. The size of word embedding for both language and GRU hidden state\ndimensions are set to 100 and 300 respectively. For two decoders, texts are generated using greedy decoding.\n6.2. Unsupervised Quantitative Evaluation Metric\nThe commonly used evaluation metric for text summarization is ROUGE [33]. ROUGE evaluates n-gram co-\noccurrences between summary pairs. It works by comparing an automatically produced summary against a set of ref-\nerence summaries. The reference summaries are typically human produced, which are expensive and time-consuming.\nIt is even more diﬃcult when facing large amounts of texts in the big data age. There is no reference summary as\ngolden standard in our selected dataset (i.e. SogouCA). To build a quantization standard, we propose an unsupervised\nevaluation metric. We consider that a faithful summary should:\n1. Overlaps with title in three aspects: n-gram, domain knowledge keywords and named entities;\n2. Contains more domain knowledge keywords than other non-summary texts;\n3. Contains more named entities than other non-summary texts;\n4. The similarities between two summary texts should be lower in case of redundancy.\nFormally, for a document d in domain D, whose title is t, the faithful score of a generated summary s is computed\nas:\nS core(s) = w · [ROUGE(t, s), Countdkb(t, s)\nCountdkb(d) , Countent(t, s)\nCountent(d) ,\nP\ne1,e2∈s ROGUE(e1, e2)\nCountedu(s)\n, Countdkb(s)\nCountdkb(d), Countent(s)\nCountent(d)] + b\n(9)\nwhere w ∈RN, b is a scalar.\nROUGE(a, b) denotes the ROUGE score between text a and b.\nCountdkb(a, b)\n(Countent(a, b)) denotes the number of domain keywords (named entities) that a and b both have.\nCountdkb(x)\n(Countent(x)) denotes the number of domain keywords (named entities) that x has. Countedu(s) denotes the num-\nber of EDUs in s. To make the score more objective, the hyper-parameters [w, b] were learned using linear regression\non DUC20024 dataset. In the training step, the faithful score for each golden standard is set to 100.\n4https://www-nlpir.nist.gov/projects/duc/data/2002_data.html\n12\nTable 4: Evaluation results on SogouCA dataset\nApproaches\n10%\n20%\n50\n100\nLead\n62.9\n72.8\n63.2\n73.6\nTextRank\n65.2\n76.6\n66.1\n75.4\nILP\n66.7\n79.5\n66.9\n79.7\nSummCoder\n68.8\n82.3\n69.0\n82.5\nOurs\n71.1\n85.7\n72.6\n86.3\n6.3. Results and Analysis\nFor each generated RS-tree, we applied Algorithm 2 for summary generation. In what follows, we present the\nresults using our method and our comparison to previous works. Since our model is unsupervised, we compare it with\nexisting unsupervised single-document summarization methods. The baselines include:\n• Lead selects the leading sentences in the document until length limit to form a summary, which is often used\nas an oﬃcial baseline of DUC.\n• TextRank is a graph-based text summarization model. It represents the document as a graph in which sentences\nare nodes and the edges between two sentences are connected based on the similarity between them.\n• ILP is a text summarization technique which utilizes Integer Linear Program (ILP) for inference under a maxi-\nmum coverage model.\n• SummCoder is an unsupervised framework for extracting sentences based on deep auto-encoders.\nWe generated four versions of summary (word length limit=100, 200 and the rate of text reduction=10%, 20%).\nTable 4 shows the faithful score of our method and baseline approaches. Our proposed framework outperforms many\nof the existing text summarizers on SogouCA dataset in terms of our proposed faithful score such as ILP, graph-based\napproaches.\nThe ﬁnal summaries obtained from a sample SogouCA document by each summarizer (i.e. Lead, TextRank, ILP,\nSummCoder, and Ours) with word length limit 100 are shown in Table 5. From the summaries, it can be observed that\nthe result generated from our method is more informative than other methods. Our result can summarize the results\nof others. The summary generated by ILP is similar to that generated by SummCoder but it is diﬀerent from those\ngenerated by TextRank.\n7. Concluding Remarks\nIn this paper, we proposed a novel unsupervised rhetorical parsing architecture for single-document extractive\nsummarization. The proposed approach mainly contains three parts: domain knowledge base construction, Chinese-\noriented rhetorical parsing and level-wise extractive summarization. To the best of our knowledge, this is the ﬁrst\nstudy to adopt translation idea for rhetorical parsing.\nFirstly, we proposed a domain knowledge base construction model based on representation learning. The learned\nDKB can provide a panorama for a domain, which has two important roles for rhetorical parsing. One is discourse\nsegmentation, and the other one is guiding rhetorical relation identiﬁcation. In the unsupervised rhetorical parsing\nmodel, we leveraged the idea of translation, and designed a novel attention-based sequence-to-sequence model for\nrhetorical relation identiﬁcation. Then the subroutine-based ATS model can accept diﬀerent word length limit or\nsummarization ratio and provide content-balanced results based on RS-tree. To evaluate our generated summary\nresults in an unsupervised way, we presented a faithful score, whose hyper-parameters were learned on DUC2002\ndataset.\nDirections for future work are many and varied. One of challenges left for the future is to further improve the\nperformance of rhetorical parsing. Such as introducing attribute grammar into the deep neural model. Another\nimportant further work would be to utilize RS-tree for multi-document summarization.\n13\nTable 5: Case study with summary length=100\nTitle\n空客称争取以合作方式参与中国大飞机项目\nAirbus said it is seeking to participate in China’s large aircraft project in a cooperative manner.\nLead\n新华网天津5月30日电空中客车中国公司总裁博龙在天津接受新华社记者独家采访时说，空客正与中方合作\n伙伴商议，争取以合作方式参与中国大飞机项目。对于中国正在研发的大飞机项目，空客正与中方合作伙\n伴商议争取以合作方式参与该项目。\nXinhuanet Tianjin, May 30th – Bolong, the president of Airbus China, said in an exclusive interview with Xinhua\nNews Agency in Tianjin that Airbus is negotiating with Chinese partners to participate in China’s large aircraft project\nin a cooperative manner. For the China’s developing large aircraft project, Airbus is negotiating with Chinese partners\nto participate in the project in a cooperative manner.\nTextRank\n空中客车中国有限公司企业资讯部提供的情况:早在1999年，空中客车公司与中国航空工业第一集团公司签\n署协议，计划分阶段向中国转让A320系列飞机机翼制造技术和生产线，目标是到2007年底使中国能够为空\n中客车在英国布劳顿和北威尔士的工厂制造A320系列飞机完整的机翼结构。\nAirbus China Ltd. Corporate Information Department provided: As early as 1999, Airbus and China Aviation Industry\nFirst Group signed an agreement to transfer the A320 series aircraft wing manufacturing technology and production\nline to China in stages, with the goal of At the end of 2007, China was able to manufacture the complete wing structure\nof the A320 family of aircraft for Airbus’ plants in Broughton and North Wales, England.\nILP\n空客正与中方合作伙伴商议争取以合作方式参与该项目。中国作为世界航空市场增长最快的国家，已成为\n空中客车和波音全球的竞争焦点。通过该中心，中国已承担空中客车于2005年10月6日正式发起的、最新\n的A350飞机项目5%的工作份额。\nAirbus is negotiating with Chinese partners to participate in the project in a cooperative manner. As the fastest growing\ncountry in the world aviation market, China has become the focus of competition for Airbus and Boeing worldwide.\nThrough the center, China has undertaken a 5% share of the latest A350 aircraft project oﬃcially launched by Airbus\non October 6, 2005.\nSummCoder\n空客正与中方合作伙伴商议争取以合作方式参与该项目。双方均不断加大在华采购、投资和技术合作，双\n方在最新机型上的重要零部件生产，也都有中国参与。博龙认为中国的大飞机之路困难而漫长，“空客花了\n近40年才取得今天的成就，现在我们拥有实力雄厚的工业基地。”\nAirbus is negotiating with Chinese partners to participate in the project in a cooperative manner. Both parties have\nalso participated in the production of important parts and components on the latest models. Bolong holds that China’s\nbig plane is diﬃcult and long. “Airbus has spent nearly 40 years to achieve today’s achievements, and now we have a\nstrong industrial base.”\nOurs\n空客正与中方合作伙伴商议，争取以合作方式参与中国大飞机项目。中国作为世界航空市场增长最快的国\n家，已成为空中客车和波音全球的竞争焦点。双方均不断加大在华采购、投资和技术合作，双方在最新机\n型上的重要零部件生产，也都有中国参与。\nAirbus is negotiating with Chinese partners to participate in China’s large aircraft project in a cooperative manner. As\nthe fastest growing country in the world aviation market, China has become the focus of competition for Airbus and\nBoeing worldwide. Both parties have continuously increased procurement, investment and technical cooperation in\nChina. Both parties have also participated in the production of important parts and components on the latest models.\nAcknowledgments\nThe authors would like to thank the developers of Pytorch [47]. This work was supported by the National Key\nResearch and Development Program of China under grant 2016YFB1000902; and the National Natural Science Foun-\n14\ndation of China (No. 61232015, 61472412, and 61621003).\nReferences\nReferences\n[1] Aggarwal, C.C., 2018. Text summarization, in: Machine Learning for Text. Springer, pp. 361–380.\n[2] Angeli, G., Premkumar, M.J.J., Manning, C.D., 2015. Leveraging linguistic structure for open domain information extraction, in: Proceedings\nof the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pp. 344–354.\n[3] Bahdanau, D., Cho, K., Bengio, Y., 2015. Neural machine translation by jointly learning to align and translate, in: International Conference\non Learning Representations 2015.\n[4] Bharti, S.K., Babu, K.S., Pradhan, A., 2017. Automatic keyword extraction for text summarization in multi-document e-newspapers articles.\nEuropean Journal of Advances in Engineering and Technology 4, 410–427.\n[5] Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., Yakhnenko, O., 2013. Translating embeddings for modeling multi-relational data, in:\nAdvances in neural information processing systems, pp. 2787–2795.\n[6] Carlson, L., Marcu, D., Okurowski, M.E., 2003. Building a discourse-tagged corpus in the framework of rhetorical structure theory, in:\nCurrent and new directions in discourse and dialogue. Springer, pp. 85–112.\n[7] Cheng, J., Lapata, M., 2016. Neural summarization by extracting sentences and words, in: Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 484–494.\n[8] Das, D., 2014. Signalling of Coherence Relations in Discourse. Ph.D. thesis. Simon Fraser University.\n[9] Feng, V.W., Hirst, G., 2012. Text-level discourse parsing with rich linguistic features, in: Proceedings of the 50th Annual Meeting of the\nAssociation for Computational Linguistics: Long Papers-Volume 1, Association for Computational Linguistics. pp. 60–68.\n[10] Ferreira, R., de Souza Cabral, L., Lins, R.D., e Silva, G.P., Freitas, F., Cavalcanti, G.D., Lima, R., Simske, S.J., Favaro, L., 2013. Assessing\nsentence scoring techniques for extractive text summarization. Expert systems with applications 40, 5755–5764.\n[11] Galitsky, B., Ilvovsky, D., 2017. Chatbot with a discourse structure-driven dialogue management, in: Proceedings of the Software Demon-\nstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics, pp. 87–90.\n[12] Gambhir, M., Gupta, V., 2017. Recent automatic text summarization techniques: a survey. Artiﬁcial Intelligence Review 47, 1–66.\n[13] Gillick, D., Favre, B., 2009. A scalable global model for summarization, in: Proceedings of the Workshop on Integer Linear Programming\nfor Natural Langauge Processing, pp. 10–18.\n[14] Goldstein, A., Shahar, Y., 2016. An automated knowledge-based textual summarization system for longitudinal, multivariate clinical data.\nJournal of biomedical informatics 61, 159–175.\n[15] He, H., 2014. HanLP: Han Language Processing. URL: https://github.com/hankcs/HanLP.\n[16] Hernault, H., Prendinger, H., Ishizuka, M., et al., 2010. Hilda: A discourse parser using support vector machine classiﬁcation. Dialogue &\nDiscourse 1.\n[17] Hirao, T., Yoshida, Y., Nishino, M., Yasuda, N., Nagata, M., 2013. Single-document summarization as a tree knapsack problem, in: Proceed-\nings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1515–1520.\n[18] Hoﬀart, J., Suchanek, F.M., Berberich, K., Weikum, G., 2013. Yago2: A spatially and temporally enhanced knowledge base from wikipedia.\nArtiﬁcial Intelligence 194, 28–61.\n[19] Hou, S., Huang, Y., Fei, C., Zhang, S., Lu, R., 2017. Holographic lexical chain and its application in chinese text summarization, in:\nAsia-Paciﬁc Web (APWeb) and Web-Age Information Management (WAIM) Joint Conference on Web and Big Data, Springer. pp. 266–281.\n[20] Jadhav, A., Rajan, V., 2018. Extractive summarization with swap-net: Sentences and words from alternating pointer networks, in: Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 142–151.\n[21] Ji, Y., Eisenstein, J., 2014. Representation learning for text-level discourse parsing, in: Proceedings of the 52nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 13–24.\n[22] Ji, Y., Smith, N.A., 2017. Neural discourse structure for text categorization, in: Proceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 996–1005.\n[23] Joshi, A., Fidalgo, E., Alegre, E., Fern´andez-Robles, L., 2019. Summcoder: An unsupervised framework for extractive text summarization\nbased on deep auto-encoders. Expert Systems with Applications .\n[24] Joty, S., Carenini, G., Ng, R., Mehdad, Y., 2013. Combining intra-and multi-sentential rhetorical parsing for document-level discourse\nanalysis, in: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 486–\n496.\n[25] Kong, L., Li, C., Ge, J., Yang, Y., Zhang, F., Luo, B., 2018. Construction of microblog-speciﬁc chinese sentiment lexicon based on represen-\ntation learning, in: Paciﬁc Rim International Conference on Artiﬁcial Intelligence, Springer. pp. 204–216.\n[26] Kraus, M., Feuerriegel, S., 2019. Sentiment analysis based on rhetorical structure theory: Learning deep neural networks from discourse\ntrees. Expert Systems with Applications 118, 65–79.\n[27] Lample, G., Conneau, A., Denoyer, L., Ranzato, M., 2018. Unsupervised machine translation using monolingual corpora only, in: Interna-\ntional Conference on Learning Representations (ICLR).\n[28] Lebanoﬀ, L., Song, K., Liu, F., 2018. Adapting the neural encoder-decoder framework from single to multi-document summarization, in:\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4131–4141.\n[29] Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas, D., Mendes, P.N., Hellmann, S., Morsey, M., Van Kleef, P., Auer, S., et al.,\n2015. Dbpedia–a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web 6, 167–195.\n[30] Lethanh, H., Abeysinghe, G., Huyck, C., 2004. Generating discourse structures for written texts, in: International Conference on Computa-\ntional Linguistics, p. 329.\n15\n[31] Li, J., Li, R., Hovy, E., 2014. Recursive deep models for discourse parsing, in: Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pp. 2061–2069.\n[32] Li, P., Mao, K., 2019. Knowledge-oriented convolutional neural network for causal relation extraction from natural language texts. Expert\nSystems with Applications 115, 512–523.\n[33] Lin, C.Y., 2004. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out .\n[34] Lin, J., SUN, X., Ma, S., Su, Q., 2018. Global encoding for abstractive summarization, in: Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers), pp. 163–169.\n[35] Lin, Y., Liu, Z., Sun, M., Liu, Y., Zhu, X., 2015. Learning entity and relation embeddings for knowledge graph completion, in: Twenty-ninth\nAAAI conference on artiﬁcial intelligence.\n[36] Louis, A., Joshi, A., Nenkova, A., 2010. Discourse indicators for content selection in summarization, in: Proceedings of the 11th Annual\nMeeting of the Special Interest Group on Discourse and Dialogue, Association for Computational Linguistics. pp. 147–156.\n[37] Lu, R., Hou, S., Wang, C., Huang, Y., Fei, C., Zhang, S., Submitted for publication. Attributed rhetorical structure grammar for domain text\nsummarization. Knowledge and Information Systems .\n[38] Lu, R., Jin, X., Zhang, S., Qiu, M., Wu, X., 2018. A study on big knowledge and its engineering issues. IEEE Transactions on Knowledge\nand Data Engineering .\n[39] Mann, W.C., Thompson, S.A., 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary\nJournal for the Study of Discourse 8, 243–281.\n[40] Marcu, D., 1997. From discourse structures to text summaries. Intelligent Scalable Text Summarization .\n[41] Marcu, D., 2000. The theory and practice of discourse parsing and summarization. MIT press.\n[42] Marujo, L., Ling, W., Trancoso, I., Dyer, C., Black, A.W., Gershman, A., de Matos, D.M., Neto, J., Carbonell, J., 2015. Automatic keyword\nextraction on twitter, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 637–643.\n[43] Mihalcea, R., Tarau, P., 2004. Textrank: Bringing order into text, in: Proceedings of the 2004 conference on empirical methods in natural\nlanguage processing.\n[44] Nallapati, R., Zhai, F., Zhou, B., 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of\ndocuments, in: Thirty-First AAAI Conference on Artiﬁcial Intelligence.\n[45] Niu, Y., Xie, R., Liu, Z., Sun, M., 2017. Improved word representation learning with sememes, in: Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2049–2058.\n[46] Onan, A., Koruko˘glu, S., Bulut, H., 2016. Ensemble of keyword extraction methods and classiﬁers in text classiﬁcation. Expert Systems with\nApplications 57, 232–247.\n[47] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A., 2017. Automatic\ndiﬀerentiation in pytorch .\n[48] Ruder, S., 2016. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747 .\n[49] Rush, A.M., Chopra, S., Weston, J., 2015. A neural attention model for abstractive sentence summarization, in: Proceedings of the 2015\nConference on Empirical Methods in Natural Language Processing, pp. 379–389.\n[50] See, A., Liu, P.J., Manning, C.D., 2017. Get to the point: Summarization with pointer-generator networks, in: Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1073–1083.\n[51] Soricut, R., Marcu, D., 2003. Sentence level discourse parsing using syntactic and lexical information, in: Proceedings of the 2003 Conference\nof the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pp. 149–156.\n[52] Timofeyev, A., Choi, B., 2018. Building a knowledge based summarization system for text data mining, in: International Cross-Domain\nConference for Machine Learning and Knowledge Extraction, Springer. pp. 118–133.\n[53] Toﬁloski, M., Brooke, J., Taboada, M., 2009. A syntactic and lexical-based discourse segmenter, in: Proceedings of the ACL-IJCNLP 2009\nConference Short Papers, Association for Computational Linguistics. pp. 77–80.\n[54] Wang, Q., Mao, Z., Wang, B., Guo, L., 2017. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on\nKnowledge and Data Engineering 29, 2724–2743.\n[55] Wang, Z., Zhang, J., Feng, J., Chen, Z., 2014. Knowledge graph embedding by translating on hyperplanes, in: Twenty-Eighth AAAI\nconference on artiﬁcial intelligence.\n[56] Wu, Y., Hu, B., 2018. Learning to extract coherent summary via deep reinforcement learning, in: Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence.\n[57] Yaqoob, I., Hashem, I.A.T., Gani, A., Mokhtar, S., Ahmed, E., Anuar, N.B., Vasilakos, A.V., 2016. Big data: From beginning to future.\nInternational Journal of Information Management 36, 1231–1247.\n[58] Zahri, N.A.H., Fukumoto, F., Suguru, M., Lynn, O.B., 2015. Exploiting rhetorical relations to multiple documents text summarization.\nInternational Journal of Network Security & Its Applications 7, 1.\n[59] Zhendong, D., Qiang, D., 2006. Hownet and the computation of meaning (with Cd-rom). World Scientiﬁc.\n[60] Zheng, L., 2018. Applications of Big Knowledge Summarization. Ph.D. thesis. New Jersey Institute of Technology.\n[61] Zhou, Q., Yang, N., Wei, F., Huang, S., Zhou, M., Zhao, T., 2018. Neural document summarization by jointly learning to score and select\nsentences, in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n654–663.\n16\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2019-10-14",
  "updated": "2019-10-14"
}