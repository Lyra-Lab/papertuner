{
  "id": "http://arxiv.org/abs/1910.07370v1",
  "title": "Evolution of transfer learning in natural language processing",
  "authors": [
    "Aditya Malte",
    "Pratik Ratadiya"
  ],
  "abstract": "In this paper, we present a study of the recent advancements which have\nhelped bring Transfer Learning to NLP through the use of semi-supervised\ntraining. We discuss cutting-edge methods and architectures such as BERT, GPT,\nELMo, ULMFit among others. Classically, tasks in natural language processing\nhave been performed through rule-based and statistical methodologies. However,\nowing to the vast nature of natural languages these methods do not generalise\nwell and failed to learn the nuances of language. Thus machine learning\nalgorithms such as Naive Bayes and decision trees coupled with traditional\nmodels such as Bag-of-Words and N-grams were used to usurp this problem.\nEventually, with the advent of advanced recurrent neural network architectures\nsuch as the LSTM, we were able to achieve state-of-the-art performance in\nseveral natural language processing tasks such as text classification and\nmachine translation. We talk about how Transfer Learning has brought about the\nwell-known ImageNet moment for NLP. Several advanced architectures such as the\nTransformer and its variants have allowed practitioners to leverage knowledge\ngained from unrelated task to drastically fasten convergence and provide better\nperformance on the target task. This survey represents an effort at providing a\nsuccinct yet complete understanding of the recent advances in natural language\nprocessing using deep learning in with a special focus on detailing transfer\nlearning and its potential advantages.",
  "text": "Evolution of Transfer Learning in Natural Language\nProcessing\nAditya Malte †\nDept. of Computer Engineering,\nPune Institute of Computer Technology,\nMaharashtra, India.\nPratik Ratadiya †\nDept. of Computer Engineering,\nPune Institute of Computer Technology,\nMaharashtra, India.\nAbstract—In this paper, we present a study of the recent\nadvancements which have helped bring Transfer Learning to\nNLP through the use of semi-supervised training. We discuss\ncutting-edge methods and architectures such as BERT, GPT,\nELMo, ULMFit among others. Classically, tasks in natural\nlanguage processing have been performed through rule-based\nand statistical methodologies. However, owing to the vast nature\nof natural languages these methods do not generalise well and\nfailed to learn the nuances of language. Thus machine learning\nalgorithms such as Naive Bayes and decision trees coupled with\ntraditional models such as Bag-of-Words and N-grams were used\nto usurp this problem. Eventually, with the advent of advanced\nrecurrent neural network architectures such as the LSTM, we\nwere able to achieve state-of-the-art performance in several\nnatural language processing tasks such as text classiﬁcation\nand machine translation. We talk about how Transfer Learning\nhas brought about the well-known ImageNet moment for NLP.\nSeveral advanced architectures such as the Transformer and its\nvariants have allowed practitioners to leverage knowledge gained\nfrom unrelated task to drastically fasten convergence and provide\nbetter performance on the target task. This survey represents\nan effort at providing a succinct yet complete understanding of\nthe recent advances in natural language processing using deep\nlearning in with a special focus on detailing transfer learning\nand its potential advantages.\nIndex Terms—Natural language processing, transfer learning,\nself attention, language modeling\nI. INTRODUCTION\nNatural Language processing - the science of how to\nmake computers effectively process natural text- has recently\nwitnessed rapid advancements thanks to increased processing\npower, data and better algorithms. It forms the heart of several\nuse cases such as opinion mining, conversational agents and\nmachine translation among others. Traditionally, NLP tasks\nwere achieved through rule-based systems, in essence, a set\nof manually crafted rules that determined the behaviour of\nthe system. Examples include rule-based machine translation\nwhere linguists iteratively framed new rules to make the\ntranslations more accurate.\nHowever, owing to the vast and heuristic nature of natural\nlanguage, machine learning gained a stronger ground in per-\nforming NLP tasks. Machine learning models such as SVM,\nNaive Bayes and random forests found use in tasks such as\nsentiment analysis, spam detection and hate speech detection.\nOn the other hand, Natural Language Generation(NLG) tasks\nsuch as machine translation, question-answering, abstractive\nsummarization were achieved through models such as the\nTransformer and Seq2Seq architectures.\nTwo important breakthroughs that provided signiﬁcant im-\npetus to the NLP and NLG domain were the arrival of\nTransfer Learning and rapid improvements in the performance\nof Language models. We† feel it necessary to discuss these\nconcepts before moving further:\nA. Language Modeling\nLanguage modeling is an NLP task where the model has\nto predict the succeeding word given the previous words\nof the sequence as context. This task requires the language\nmodel(LM) to learn the nuances and inter-dependencies among\nthe various words of the language. Standard benchmark\ndatasets for the language modeling tasks include the wikitext\ndataset, the BookCorpus dataset [1] and the 1B word [2]\nbenchmark. Perplexity is generally used as a metric to evaluate\nthe performance of language models. Perplexity is deﬁned as\nfollows:\nN\nv\nu\nu\nt\nN\nY\ni=1\n1\nP(wi|w1...wi−1)\n(1)\nGiven a sequence of N words of the corpus, w1w2...wN,\nP(wi|w1...wi−1) is the probability assigned by the language\nmodel to word wi given wi−1 preceding words of the se-\nquence.\nA lower perplexity generally signals towards a better per-\nforming language model as it indicates a lower entropy in the\ngenerated text. Language modeling is used for tasks such as\nnext word prediction, text auto-completion and checking lin-\nguistic acceptability. The concept of semi-supervised learning\nin NLP allows us to understand why and how language mod-\neling has played a fundamental role in various architectures\nthat have allowed for transfer learning in NLP.\nB. Transfer Learning\nTraditionally, NLP models were trained after random initial-\nization of the model parameters(also called weights). Transfer\nLearning, a technique where a neural network is ﬁne-tuned on\na speciﬁc task after being pre-trained on a general task allowed\ndeep learning models to converge faster and with relatively\n†Equal contribution of both the authors\narXiv:1910.07370v1  [cs.CL]  16 Oct 2019\nlower requirements of ﬁne-tuning data. Historically, transfer\nlearning has been mainly associated with the ﬁne-tuning of\ndeep neural networks trained on the ImageNet dataset [3] for\nother computer vision tasks. However, with recent advances in\nnatural language processing, it has become possible to perform\ntransfer learning in this domain as well.\nIn this survey, we seek to discuss the recent strides made in\nTransfer learning, Language modeling and natural language\ngenerations through advancements in algorithms and tech-\nniques. Transfer learning can be used for applications where\nthere is lack of a large training set. The target dataset should\nideally be related to the pre-training dataset for effective\ntransfer learning. This type of training is generally referred\nto as semi-supervised training where the neural network is\nﬁrst trained as a language model on a general dataset fol-\nlowed by supervised training on a labelled training dataset\nthus establishing a dependence of supervised ﬁne-tuning on\nunsupervised language modeling.\nThe paper is structured as follows- Section II of the pa-\nper elaborates on the various algorithms and architectures\nthat have serve as a base on which more advanced models\nhave been built upon. Section III provides information about\nthe transformer architecture which drastically improved the\nprospects of using transfer learning for NLP tasks. Section IV\nthen goes on to discuss the evolution of language modeling\nand transfer learning through models such as BERT, ElMo,\nUlMFit and so on. We conclude our survey and suggest future\nimprovements in section V.\nFig. 1: Developments in Transfer Learning and Language\nModeling\nII. BACKGROUND\nA. Vanilla RNNs\nMachine learning models have been widely been used for an\narray of supervised as well as unsupervised learning tasks such\nas regression, classiﬁcation, clustering and recommendation\nmodelling. Markov models such as the Multi-layer Perceptron\nNetwork, vector machines and logistic regression, however,\ndid not perform well in sequence modeling tasks such as\ntext classiﬁcation, language modeling and tasks based on time\nseries forecasting. These models suffered from an inability to\nretain information throughout the sequence and treated each\ninput independently. In essence, the lack of a memory element\nprecluded these models from performing well on sequence\nmodelling tasks.\nRecurrent Neural Networks [4] or RNNs attempted to redress\nthis shortcoming by introducing loops within the network, thus\nallowing the retention of information.\nFig. 2: Recurrent Neural Network [7]\nht = φ(Wht−1 + Uxt + b)\n(2)\nAs shown in Fig.2 and the corresponding equation, the\ncurrent hidden state of the neuron can be modelled as a\nfunction of the hidden state of the previous neuron st−1, the\ncurrent input xt, weight matrices\nU, W and bias b. These\nweights of the network are then updated through a training\nalgorithm called Backpropagation Through Time(BPTT) [5].\nBPTT is, in essence, the backpropagation algorithm with\nsome modiﬁcations. The network is propagated for each time\nstep- an operation that is often referred to as unrolling the\nRNN. The parameters of the neural network remain the same\nthroughout the unrolling operation of the RNN. Corresponding\nerrors concerning the predicted output and the ground truth are\nthen calculated for each time step. Gradients of the error with\nrespect to all parameters are then calculated and accumulated\nusing the backpropagation algorithm. It is only after the\nunrolling is complete that all the parameters of the RNN are\nupdated by using this accumulated error gradient.\nVanilla RNNs were successful in a wide variety of tasks\nincluding speech recognition, translation and language mod-\nelling. Despite their initial success, vanilla RNNs were only\nable to model short term dependencies. They failed to model\nlong term dependencies, primarily because the information\nwas often ”forgotten” after the unit activations were multiplied\nseveral times by small numbers. Further, they suffered from\nvarious issues while training such as the vanishing gradient\nproblem (the error gradient being used for weight updation\nreducing to very low values) and the exploding gradient prob-\nlem. Thus, successfully training and applying vanilla RNNs\nwas a challenging task.\nB. Long Short Term Memory\nThe problem of ’long-term’ dependencies faced by earlier\nrecurrent neural networks was solved by designing a special\nkind of RNN architecture called the LSTM(Long Short Term\nMemory) [6]. They were designed to keep track of information\nfor the extended number of timesteps. LSTMs have an overall\nchain-based architecture similar to RNNs but the crucial\ndifference is the improvements in the internal node structure.\nWhile a node in RNN consists of a single neural layer, there\nare four layers connected uniquely in LSTMs as shown in the\nﬁgure.\nFig. 3: Chain structure of LSTMs [7]\nThe key feature of LSTMs is the information carrier con-\nnection present at the top known as the ’cell state’. It proves\nto be useful to carry information along longer distances with\nonly minor linear operations taking place at each node.The\nability to add or delete certain information of the cell state\nat each node is provided by structures called as gates. LSTM\nhas three such gates, each comprising of a sigmoid neural net\nlayer and a pointwise multiplier.\nThe ’forget gate’ decides what information is to be retained\nin the cell state by using a sigmoid layer which outputs a\nvalue between 0 and 1(0 indicates ’forget everything’ while 1\nindicates ’retain completely’). The ’input gate’ layer is used\nto determine new information which is to be added to the cell\nstate. It involves deciding which values to be updated and the\nnew candidates to do so. The previous cell state values and\nthe new candidate values are then combined to get the ﬁnal\nnew cell state. The output to be forwarded by the node is\ndecided by combining the values of current cell state with the\nresults of the ’output layer’. The output is generally supporting\ninformation relevant to the previous word. Mathematically, the\noperations performed using the three gates can be expressed\nas:\nft = σ(Wf.[ht−1, xt] + bf)\n(3)\nit = σ(Wi.[ht−1, xt] + bi)\n(4)\neCt = tanh(Wc.[ht−1, xt] + bC)\n(5)\nCt = ft ∗Ct−1 + it ∗eCt\n(6)\not = σ(Wo.[ht−1, xt] + bo)\n(7)\nht = ot ∗tanh(Ct)\n(8)\nwhere ft,it,ot indicate outputs of the forget gate, input gate\nand output gate respectively. W and b indicate the weights and\nbias. Ct−1 indicates the previous cell state, eCt represents the\nnew candidate values and the current cell state is shown by\nCt.\nThe advantage of using LSTM is that they offer more\ncontrol in a network than the conventional recurrent networks.\nThe system is more sophisticated and can retain information\nover longer timesteps. However, the added gates lead to more\ncomputation requirement and thus LSTMs tend to be slower.\nC. Gated Recurrent Units\nGated Recurrent Units [8] or GRUs introduced by Cho, et al.\nin 2014 are a curtailed variation of LSTMs designed to reduce\nthe computation issues of the latter. The forget and input gates\nin LSTMs are combined into a single ’update gate’. The cell\nstate and hidden states are also merged together and computed\nusing a single ’reset gate’. The operations now performed are\nas follows:\nzt = σ(Wz.[ht−1, xt])\n(9)\nrt = σ(Wr.[ht−1, xt])\n(10)\neht = tanh(Wc.[rt ∗ht−1, xt])\n(11)\nht = (1 −zt) ∗ht−1 + zt ∗eht\n(12)\nFig. 4: Internal structure of GRUs [7]\nGRUs have the advantage of being able to control the ﬂow\nof information without having an explicit memory unit, unlike\nLSTMs. It exposes the hidden content of the node without any\ncontrol. The performance is almost on par with LSTM but with\nefﬁcient computation. However, with large data LSTMs with\nhigher expressiveness may lead to better results.\nD. Average SGD Weight Dropped(AWD)-LSTM\nAWD-LSTM [9], despite its relatively simple 3-layer LSTM\narchitecture was proven to be highly effective for Language\nModeling tasks. It employed a novel algorithm called Drop-\nConnect to mediate the problem of overﬁtting that had been\ninherent in the RNN architecture. Besides, the authors used\nNon-monotonically Triggered ASGD(NTASGD) algorithm to\noptimize the network.\nDropconnect Algorithm Neural networks, prone to overﬁt-\nting, traditionally utilised Dropout as regularization to prevent\noverﬁtting. Dropout, an algorithm that randomly(with a prob-\nability p) ignore units’ activations during the training phase\nallows for the regularization of a neural network. By diminish-\ning the probability of neurons developing inter-dependencies,\nit increases the individual power of a neuron and thus reduces\noverﬁtting. However, dropout has not been able to provide\ncommensurate results in case of the RNN architectures. In\nessence, it inhibits the RNN’s capability of developing long\nterm dependencies as there is loss of information caused due\nto randomly ignoring units activations.\nTo this end, the drop connect algorithm randomly drops\nweights instead of neuron activations. It does so by ran-\ndomly(with probability 1-p) setting weights of the neural\nnetwork to zero during the training phase. Thus redressing\nthe issue of information loss in the Recurrent Neural Network\nwhile still performing regularization.\nNon-monotonically\nTriggered\nASGD\n(NT-ASGD)\nStochastic gradient descent has been demonstrated to offer\ngood performance for language modeling tasks through saddle\npoint avoidance and linear convergence. Thus, the authors go\non to investigate a variant of SGD- averaged SGD. Averaged\nSGD-almost identical to vanilla SGD- differs in the fact that\nan averaging of the weights(which are cached) is performed\nafter a threshold number of iterations T is over.\nWhile theoretically able to control the effects of noise,\naveraged SGD has found little use while training neural\nnetworks. This has been mainly attributed by the author to am-\nbiguous guidelines regarding the tuning of hyperparameters:\nlearning rate scheduler and averaging trigger. A commonly\nused strategy while using the SGD optimizer is to reduce the\nlearning rate by a ﬁxed quantity when the validation error\nworsens or fails to improve. Similarly, one may perform the\naveraging operation after validation error worsens. The Non-\nmonotonically triggered ASGD employs a similar technique.\nIt differs in the fact that, instead of performing averaging\nwhen the validation error worsens NT-ASGD performs the\naveraging operation if the validation error fails to improve.\nNT-ASGD introduces two new hyperparameters-the logging\ninterval L and the non-monotone interval n. Consequently, the\nauthors found that keeping n = 5 provided good performance\nin general. Better results were achieved compared to SGD\nwhile training their model.\nE. Seq2Seq Architecture\nWe take the example of neural machine translation to ex-\nplain the working of the Attention Mechanism and advantages\nthat it provides.\nThe Seq2Seq architecture [10] has been used to perform\na wide variety of tasks including Neural Machine Transla-\ntion(NMT), Abstractive summarization and chatbot systems.\nThe traditional Seq2Seq architecture consists of an encoder\nRNN(LSTM/GRU) followed by a decoder RNN. The encoder\nencoded the given sequence into a ﬁxed-length vector. The\ndecoder generated the output sequence after taking the ﬁxed-\nlength vector as source hidden state. While giving signiﬁcant\nimprovements in the domains of neural machine translation,\nencoding the context of complex and long sequences into\na single vector impeded the performance of the network.\nThis was since a ﬁxed-length vector was often incapable\nof effectively encoding the context of the given sequence.\nConsequently, this led to the birth of the Attention Mechanism,\na novel technique that allowed the neural network to identify\nwhich input tokens are relevant to a corresponding target token\nin the output.\nF. Attention Mechanism\nInstead of encoding a single vector to represent the se-\nquence, the attention mechanism [11] computes a context\nvector for all tokens in the input sequence for each token in\nthe output. The decoder computes a relevancy score for all\ntokens on the input side. These scores are then normalized\nby performing a softmax operation to obtain the Attention\nweights. These weights are then used to perform a weighted\nsum of the encoder’s hidden states, thus obtaining the Context\nVector ct.\nαts =\nexp(score(ht, hs))\nPS\ns′=1 exp(score(ht, hs′)))\n(13)\nct =\nX\ns\nαtshs\n(14)\nat = f(ct, ht) = tanh(Wc[ct; ht])\n(15)\nA hyperbolic tangent operation is performed on the concate-\nnation of the context vector and the target hidden state to get\nthe Attention vector at. This attention vector generally pro-\nvides a better representation of the sequence than traditional\nﬁxed-sized vector methods.\nBy identifying the relevant input tokens while generating\nthe output token, Attention mechanism is able to redress\nthe problem of compressing the context of the text into a\nﬁxed-sized vector. Using this mechanism Bahdanu et. al. [11]\nwere able to achieve state-of-the-art performance in machine\ntranslation tasks.\nIII. THE TRANSFORMER ARCHITECTURE\nOwing to the signiﬁcant improvements gained due to\nthe Attention mechanism, Vaswani et. al. [12] proposed the\nTransformer architecture. The Transformer achieved new\nstate-of-the-art results in various tasks such as machine\ntranslation, entailment and so on. As shown in Fig.5,\nthe Transformer consists of an encoder and a decoder.\nFurthermore, the encoder consists of a Multi-Head Attention\nFig. 5: Architecture of the Transformer\nlayer, residual connections, normalization layer and a generic\nfeed-forward Layer. The decoder is almost identical to\nthe encoder but contains a certain ”Masked” Multi-Head\nAttention layer.\nEncoder: The encoder takes as input the input embedding\nthat is added with the positional encoding. The positional\nencoding allows for the retention of position and order related\ninformation. The authors employ the following equations to\ncompute the positional encoding:\nPE(pos,2i) = sin(pos/10000(2i/dmodel))\n(16)\nPE(pos,2i+1) = cos(pos/10000(2i/dmodel))\n(17)\nWhere pos is the position of the word in the sequence and\ni is the dimension. This positional encoding is added to the\ninput embedding. It is then followed by a residual connection\nR, deﬁned as follows:\nR(x) = LayerNorm(x + MultiHeadedAttention(x))\n(18)\nWhere x is the value of the input embedding added with\npositional encoding. Thus we are effectively able to encode\nthe semantic and position-related information using the input\nand positional encodings.\nDecoder: As previously stated, the decoder is almost identical\nto the Encoder but for the ”Masked” Multi-Headed Attention\nLayer.\nScaled dot-Product Attention: While generating embeddings\nfor each word in the input token, the authors made use of\nthe self attention mechanism. Self attention, similar to vanilla\nattention, allows the Transformer to identify words in the input\nsequence that were relevant to the current token. Speciﬁcally,\nthe authors made use of Scaled dot-Product Attention.\nAttention (Q, K, V ) = softmax(QKT\n√dk\n)V\n(19)\nAs shown above, the Scaled dot-Product Attention takes\nthree vectors as input- the key, value and query.As shown,\nwe perform a weighted average of the value vector V . The\nweights are assigned by using a ”compatibility function” to\nﬁnd .\n• Embedding E is the output value of previous hidden layer\n• Query, Q = EWq\n• Key, Q = EWk\n• Value, Q = EWv\nWhere Wq, Wk and Wv are weight matrices. A dot product\nof the query vector Q and key vector K is performed with\na scaling factor 1/\n√\ndk. The scaling factor is used to avoid\nthe softmax input falling in a range where the output falls to\na negligible value. A softmax operation is performed on the\noutput of softmax. The ﬁnal vector is multiplied with the value\nvector through a matrix multiplication operation to obtain the\nﬁnal attention scores.\nFig. 6: Multi-Headed and Scaled Dot-Product Attention\nMulti-Headed Attention: Additionally, to reduce the number\nof operations to compute Attention scores, the authors make\nuse of Multi-Headed Attention. Multi-Head Attention splits the\nvector space into ’n’ parts. These divisions are then passed to\n’n’ Attention Heads to perform the Self-Attention operation,\nthe results of these operations are then concatenated. In\naddition to reducing the number of operations, Multi-Headed\nAttention allows the model to ”jointly attend to information\nfrom different representation subspaces at different positions”.\nMasked Multi-Headed Attention: The Masked Multi-\nHeaded Attention is similar to the Multi-Headed Attention\nbut performs an additional ”masking” operations. The decoder\nis allowed to attend to only the previous positions while\ncomputing self-attention passing the output embedding. This\nwould result in the transformer being able to attend to the\nsubsequent positions and consequently the output prediction\nin the sequence. To prevent this undesirable phenomenon, all\nsubsequent positions are set to −∞before computing the self-\nattention that is passed to higher layers.\nFinally, a softmax layer is used as the to compute output word\nprobabilities. The word probabilities are in the form of a vector\nthat has a size equal to the size of the vocabulary of the training\nset.\nBased on the Attention mechanism and without using any\nrecurrence mechanism, the Transformer effectively supplanted\nthe Recurrent Neural Network Architectures-LSTM, GRU,\netc.-as the state-of-the-art in several NLP tasks. It is used\nfor a wide variety of tasks including machine translation and\nconstituency parsing.\nIV. EVOLUTION OF TRANSFER LEARNING AND\nLANGUAGE MODELING\nA. ULMFIT\nUniversal Language Model Fine-tuning (ULMFit) [13]- a\nmethod to ﬁne-tune a pre-trained language model- was one\nof the forerunners of inductive transfer learning in NLP. By\ndeﬁnition, the language modeling task entails that given a\nsequence of tokens, the model has to predict the likelihood\nof the next token based on the sequence. The ULMFit method\nachieved good results by using the then state-of-the-art AWD-\nLSTM for their experiments. The proposed network was a sim-\nple 3-layer neural network without any attention mechanism,\nskip connections etc. ULMFit allowed for transfer learning by\nemploying the following three steps in the stated order:\n1) Generic Pretraining of the Language Model: The\nauthors pre-trained their language model on WikiText-\n103– a large general-purpose dataset that consists of\n28,595 preprocessed articles and 103 million words. This\nstep allowed the language model to capture the general\nproperties of the given language. Also, while computa-\ntionally expensive, this task need to be performed only\nonce.\n2) Fine-tuning the Language Model on the Target task:\nThis step allowed the language model to capture the\ninherent nuances of the target task,thus allowing better\nperformance. Furthermore, this step can be performed on\na relatively smaller dataset, thus requiring relatively less\ncomputational power. The authors then proposed two\nnovel methods- Discriminative Fine-tuning and Slanted\nTriangular Learning rates to perform this task.\nDiscriminative ﬁne-tuning - Different layers of the\nmodel extract different features from the text. Thus,\nthe use of different learning rates for different layers\nseemed apt. To this end, the the authors formulated\nDiscriminative Fine-Tuning, a variation of the SGD\noptimizer-as follows:\nθl\nt = θl\nt−1 −ηl · ∇θlJ(θ)\n(20)\nWhere, θ is the weight, t is the iteration, l is the layer,\n∇θlJ(θ) is the error gradient, η is the learning rate and\nJ indicates the error function.\nThe authors found that choosing a value of learning\nrate for the last layers and then setting the learning rate\nof lower layers by the relation ηl−1 = ηl/2.6 worked\nwell.\nSlanted Triangular Learning Rates: The Slanted\nTriangular Learning Rate is deﬁned by the author as\nfollows: by using a high initial learning rate, the model\nwould quickly converge to an appropriate region in the\nhyperspace. The learning rate is then linearly decayed\nin order to improve the parameters on target task at a\nﬁne rate. The authors deﬁned three new equations:\ncut = [T · cutfrac]\np =\n\n\n\nt/cut, t < cut\n1 −\nt −cut\ncut·(1/cutfrac −1), otherwise\n\n\n\nηt = ηmax · 1 + p·(ratio −1)\nratio\nwhere T is the number of iterations, cutfrac the frac-\ntion of iterations that the LR is increased and p is\nthe number of iterations that the LR is going to be\ndecreased or has been decreased. Additionally, ratio\ndeﬁnes etamin/etamax and cut is the cutoff iteration\nwhere the model switches from an increasing LR to a\ndecreasing LR.\n3) Fine-Tuning the Classiﬁer on Target Task To perform\ntask-speciﬁc classiﬁcation, two linear blocks initialized\nfrom scratch are added to the language model. The\nauthor follows standard practices such as Batch Normal-\nization and Dropout to perform regularization. Besides,\nthe ReLu activation function is used similar to those\nused in Computer Vision models.\nConcat Pooling: To preserve information contained in\nfew words, the input provided to this classiﬁer is a\nconcatenation of the last hidden layers and the average\nand max pooled output of the previous hidden layers.\nFor this purpose, the author sought to concatenate as\nmany hidden layers as would ﬁt in the GPU memory.\nThe concatenation hc is as given below:\nhc = concatenate(hT , maxpool(H), averagepool(H))\n(21)\nwhere H = {h1, h2..hT }\nGradual Unfreezing: Keeping all parameters trainable,\ni.e. performing the updation of all parameters during\ntraining would lead to a rapid loss in information learnt\nduring the pre-training phase. To tackle this, the authors\nhave gradually ”unfrozen” the layers. In essence, the\nauthors start by unfreezing the last layers and then\nperform ﬁne-tuning. They repeat this process until all\nlayers of the AWD-LSTM have not been trained.\nBidirectional LM: The authors train both a forward\nLM as well as a backward LM. Consequently averaging\nthe predictions given by both the Language Models.\nOne can apply transfer learning using ULMFit by using pre-\ntrained models trained on datasets such as the Wikitext 103\ndata. Fine-tuning is performed by adding training the network\non the target task by using supervised learning\nB. Embeddings from Language Models(ELMo)\nTraditional word embeddings involve assigning a unique\nvector to each word. These word embeddings are ﬁxed and\nindependent of the context in which the words are being\nused. Peters et. al came up with a new word representation\ncalled ”Embeddings from Language Models(ELMo)”, [14]\nin which the tokens beings assigned to each word were\na function of the entire sentence of which the word as a\npart of. These embeddings are obtained from the internal\nlayers of a deep bidirectional LSTM that is trained with a\ncoupled language model objective (biLM) on a large text\ncorpus. These representations are more elaborate as they are\ndependent on all of the internal layers of the biLM. The\nword representations are computed on top of two-layer biLMs\nwith character convolutions as a linear function of the internal\nnetwork states. For a given set of tokens, the biLM computes\ntheir probability by taking into consideration the logarithmic\nlikelihood of both the previous words(forward LM) as well as\nthe future words(backward LM) and maximizing it.\np(t1, t2, ..., tN) =\nN\nX\nk=1\n(logp(tk|t1, ..., tk−1; Θx, −→\nΘ LSTM, Θs)+\nlogp(tk|tk+1, ..., tN; Θx, ←−\nΘ LSTM, Θs))\n(22)\nwhere Θs and Θx indicate the parameters for the softmax\nlayer and the token representations in the forward and back-\nward layer respectively. The ELMo model uses the intermedi-\nate layer representation of the biLM. ELMo combines all the\nlayers of the biLM representation into a single vector ELMOk\nto be accommodated later in the ﬁne-tuning task. Generally,\nfor a given task of obtaining word embeddings in the language\nmodeling phase, we ﬁnd the obtained weightings of all biLM\nlayers:\nELMotask\nk\n= γtask\nL\nX\nj=0\nstask\nj\nhLM\nk,j\n(23)\nwhere stask are the weights and γtask is a scalar quantity.\nstask is obtained after normalizing the weights and passing\nthem through a softmax layer. γtask, on the other hand,\nallows us to scale the ELMo vector. γtask plays an important\nrole in the optimization process. The layer representation\nhLM\nk,j = [−→h LM\nk,j ; ←−h LM\nk,j ] for each biLSTM layer wiz. combina-\ntion of forward context representations and backward context\nrepresentations.\nSuch a deep representation helps ELMo to trace two im-\nportant factors (1) Complex characteristics like the syntax and\nsemantics of the words being used and (2) Their variations\nacross linguistic contexts. For any supervised task, the weights\nof the biLM are frozen. ELMOk is then concatenated with\nit and the obtained representations are then forwarded to the\ntask-speciﬁc architecture. A pretrained biLM can be used to\nobtain representations for any tasks and with ﬁne-tuning, they\nhave shown a decrease in perplexity thereby beneﬁting from\ntransfer learning.\nThe addition of ELMo embeddings to existing models has\nhelped them process more useful information from the sen-\ntences and thus enhanced the performances in many applica-\ntions like question answering, semantic role labelling, named\nentity extraction and many more.\nELMo provides an enhancement to the traditional word\nembedding techniques by incorporating context while gener-\nating the same. The vector generated by ELMo can be used\nfor a general downstream task. This is sometimes done by\npassing the generated word embeddings to another neural\nnetwork(eg. LSTM) which is then trained on the target task.\nFurthermore, concatenation of ELMo embeddings with other\nword embeddings is also done to provide better performance.\nC. OpenAI Transformer\nAlthough there is availability of large text corpora, labelled\ndata is tough to ﬁnd and manually labelling the data is an\nequally tedious task. Radford et al. at OpenAI proposed a\nsemi-supervised approach called Generative Pre-training(GPT)\n[15] which involved unsupervised pre-tuning of the model\nand then task-speciﬁc supervised ﬁne-tuning for language\nunderstanding tasks. The Transformer is used as the base\nmodel for this purpose. The unsupervised learning helps to\nset the initial parameters of the model based on a language\nmodeling objective. The subsequent supervised learning helps\nthe parameters adjust to the target task.\nInitially, a multi-layer transformer decoder is used to pro-\nduce an output distribution over the target tokens based on a\nmulti-headed self-attention mechanism.\nh0 = UWc + Wp\n(24)\nhl = transformer block(hl−1)∀i ∈[1, n]\n(25)\nP(u) = softmax(hnW T\nc )\n(26)\nwhere hi is the transformer layer’s activations, Wc is the\ntoken embedding matrix and Wp is the position embedding\nmatrix. The supervised learning task then obtains the ﬁnal\ntransformer block activations hl\nm which are passed through\na softmax layer to predict output label y:\nP(y|x1, ..., xm) = softmax(hm\nl Wy)\n(27)\nto maximize\nL2(C) =\nX\n(x,y)\nlogP(y|x1, ..., xn)\n(28)\nwhere C indicates the labeled dataset with each sequence\nconsisting of tokens x1, x2, ..xm. During transfer learning, the\ninput is converted into a single contiguous sequence of tokens\nso as to ﬁt the pre-trained model.\nOpenAI transformer improvises on generative pre-training\nto improve performance on tasks by providing a better start\nthan random initialization. A single model is able to produce\nquality results with minimum task-speciﬁc customization or\nhyperparameter tuning, thereby showing its robustness. This\narchitecture model was able to outperform other approaches\non tasks like natural language inference, question answering,\nsentence similarity etc.\nD. Bidirectional Encoder Represenation from Transform-\ners(BERT)\nBERT [16] -proposed by J.Devlin et al.- is a novel ap-\nproach to incorporate bidirectionality in a single Transformer\nmodel. A particularly challenging task, direct approaches to\nincorporating bidirectionality in Transformer models fail since\ndirect bidirectional conditioning would allow the words to\nsee themselves in the light of context from multiple layers,\nthereby ruling out the possibility of using it as a Language\nModel. In essence, it was traditionally only possible to train\na unidirectional encoder- a left-right or a right-left model.\nHowever, bidirectional models that could see the complete\nsequence context would inherently be more powerful than\nunidirectional models or a concatenation of two unidirectional\nmodels-left-right and right-left. To this end, the authors trained\ntheir model on two unsupervised prediction tasks:\nMasked LM To overcome the challenges posed while ap-\nplying of bi-directionality in Transformers, J.Devlin proposed\nmasking of random tokens in the sequence. The Transformer\nwas trained such that it had to predict only the words that had\nbeen masked while being able to view the whole sequence.\nWordPiece Tokenization is used to generate the sequence of\ntokens where rare words are split into sub-tokens. Masking\nof 15% of the Wordpiece Tokens is performed. Masking\nessentially replace the words with [MASK] tokens. How-\never, instead of always replacing the selected words with a\n[MASK] token, the data generator employs the following\napproach:\n• Replace the word with [MASK] token 80% of the time\n• Replace the word with another random word 10% of time\n• Keep the word as it is 10% of the time\nPerforming prediction on only 15% of all words instead of\nperforming prediction on all words would entail that BERT\nwould be much slower to converge. However, BERT showed\nimmediate improvements in absolute accuracy while converg-\ning at a slightly slower pace than traditional unidirectional\nleft-right models.\nNext Sentence Prediction This task entails predicting\nwhether the ﬁrst sequence provided immediately precedes the\nnext. This task allows the Transformer to perform better on\nseveral downstream tasks such as question-answering, Natural\nLanguage Inference that involve understanding the relationship\nbetween two input sequences. The dataset so used for training\nhad a balanced 50/50 distribution created as follows: choosing\nan actual pair of neighbouring sentences for positive examples\nand a random choice of the second sentence for the negative\nexamples. The input sequence for this pair classiﬁcation task\nis generated as:\n[CLS] < Sentence A > [SEP] < Sentence B > [SEP],\nwhere sentences A and B are two sentences after performing\nthe masking operations. The [CLS] token is the ﬁrst token used\nto obtain a ﬁxed vector representation that is consequently\nused for classiﬁcation and [SEP] is used to separate the two\ninput sequence. The authors were able to achieve an impressive\naccuracy of 97-98% in the next sentence prediction task.\nPre-Training\nProcedure The authors have used the\nBooksCorpus and the English Wikipedia as pretraining data.\nThey have used two variations of BERT- BERTBASE(12-\nlayer) and BERTLARGE(24 layers)- that primarily differ in\ntheir depth. The maximum length of the input sequence is\nrestricted to 512 tokens. All subsequent tokens in the sequence\nare neglected. A dropout value of 0.1 is used as regularization.\nFurthermore, the authors have made use of the GELU instead\nof Relu as activation function. GELU- Gaussian Error Linear\nunits has been shown to provide improvements compared to\nReLU and eLu.\nTraining of the models was performed on TPUs, speciﬁcally\nBERTBASE was trained on 16 TPU chips for 4 days.\nBERTLARGE was trained on 64 TPU chips, also for 4 days.\nFine-Tuning Procedure The pre-trained BERT can be ﬁne-\ntuned on a relatively small dataset and requires lesser pro-\ncessing power. BERT was able to improve upon the previous\nstate-of-the-art in several tasks involving natural language\ninference, question answering, semantic similarity, linguistic\nacceptability among other tasks. The pattern of the input and\noutput sequence varies depending on the type of the task. The\ntasks can be broadly divided into four categories:\n• Single Sentence Classiﬁcation Tasks: These tasks are per-\nformed by adding layers on the classiﬁcation embedding\n[CLS] and passing the input sequence preceded by the\n[CLS] token.\n• Sentence Pair Classiﬁcation Tasks The two sentences are\npassed to BERT after being separated by the [SEP]\ntoken. Classiﬁcation can be performed by adding layers\nto the [CLS\n• Question Answering Tasks\n• Single Sentence Tagging Tasks\nSubsequently, two multilingual BERT models-uncased and\ncased-for over 102 languages were released. Furthermore,\nOpenAI released the GPT2 [17], essentially BERT trained as\na language model on a very large amount of data.\nE. Universal sentence encoder\nThe amount of supervised training data present for language\nprocessing tasks is limited. The use of pre-trained word\nembeddings has proved to be useful in this case as they\nperform a limited amount of transfer learning. Daniel et al.\n[18] proposed a new approach which involved direct encoding\nof sentences instead of words into vectors. The sentence\nencoded vectors are found to require minimal task-speciﬁc\ndata to produce good results. The encoder models are available\nin two architectures taking into consideration the two primal\nchallenges of training transfer learning models wiz. complexity\nand accuracy.\n1) Transformer based architecture: The ﬁrst model makes\nuse of the transformer architecture to construct sentence em-\nbeddings. The encoding subgraph of the transformer architec-\nture is used for this purpose. Attention mechanism is used\nto ﬁnd context-based word representations which are then\nconverted into a ﬁxed-length sentence encoding vector. The\ninput to the transformer model is a lower case Penn Treebank\n3(PTB) [19] tokenized string and a 512-dimensional sentence\nembedding is produced as the output. A single encoding\nmodel is trained over multiple tasks to make it as general\nas possible. This model achieves superior accuracy over the\nother architecture but at the expense of increased computation\nrequirement and complexity.\n2) Deep Averaging Network(DAN) architecture: In this\nmodel, the input embeddings for words and bi-grams are aver-\naged and then passed through a deep neural network. The input\nand output format are same as that of the transformer encoder.\nMultitask learning, similar to the transformer model encoder\nmodel is used for training purpose. The main advantage of\nthis model is that it performs the required operations in linear\ntime.\nThe main difference between the transformer and DAN\nencoder models is of time complexity(O(n2) and O(n) re-\nspectively). The memory requirement for the transformer\nmodel increases quickly with increase in sentence length while\nthat of DAN model remains constant. The trade-off between\ncomplexity and accuracy should be noted when deciding a\nparticular architecture for a given task.\nThe unsupervised learning data used for training in both the\ncases included Wikipedia, web news and discussion forums.\nAugmentation is performed using training on supervised data\nfrom the Stanford Natural Language Inference(SNLI) corpus\nwhich improved the performance further. The universal sen-\ntence encoder can be used for a variety of transfer tasks includ-\ning sentiment analysis, sentence classiﬁcation, text similarity\netc. For determining a pairwise semantic similarity between\ntwo sentences, the similarity of the sentence embeddings\nproduced by the encoder can be calculated and converted into\nangular distance to get the ﬁnal result.\nsim(u, v) = (1 −arccos(\nu.v\n||u||||v||)/π)\n(29)\nThe sentence embeddings outperform the results of word\nembeddings on the fore mentioned tasks. However, combining\nword and sentence embeddings for transfer learning produced\nthe best overall results. Universal sentence encoder assists the\nmost when limited training data is available for the transfer\ntask.\nThe Universal Sentence Encoder can be used for down-\nstream tasks bypassing the generated embedding to a classiﬁer\nsuch as an SVM or another deep neural network.\nF. Transformer-XL\nThe Transformer-XL [20]was able to model very long-range\ndependencies. It did so by overcoming one limitation of the\nvanilla Transformer- ﬁxed-length context. Vanilla Transform-\ners were incapable of accommodating a very long sequence\nowing to this limitation. Hence they resorted to alternatives\nsuch as splitting the corpus into segments which could be\nmanaged by the Transformers. This led to loss of context\namong individual segments despite being part of the same\ncorpus. However, the Transformer-XL was able to take the\nentire large corpus as input, thus preserving this contextual\ninformation. In essence a vanilla Transformer, it relied on\ntwo novel techniques-Recurrence mechanism and Positional\nEncoding- to provide the improvement.\n1) Recurrence mechanism: Instead of training the Trans-\nformer on individual segments of the corpus(without regards\nto previous context), the authors propose caching the hidden\nsequence state computed from the previous segments. Con-\nsequently, the model computes self attention(and other opera-\ntions) on the current hidden/input state as well as these cached\nhidden states. The number of states cached during training\nis limited due to memory limitations of the GPU. However,\nduring inference, the authors can increase the number of\ncached hidden states used to model long-term dependency.\n2) Relative Positional Encoding: An inherent problem with\nusing the said Recurrence mechanism is preserving relative\npositional information while reusing cached states. The authors\novercame this problem by incorporating Relative Positional\nEncoding in the Attention mechanism(instead of hidden states)\nof the Transformer. They do so by encoding information re-\ngarding the positional embedding dynamically in the Attention\nscores themselves. The distance of the key vectors for the\nquery vector is the temporal information that is encoded in the\nAttention scores. In essence, computing attention is facilitated\nas temporal distance is still available to the model while still\npreserving previous states. Furthermore, information regarding\nthe absolute position can be recovered recursively.\nThe Transformer-XL was able to surpass the state-of-the-art\nresults for language modelling tasks on the enwiki8 and text8\ndatasets.\nG. XLNet\nThe BERT model proposed by Authors et. al, was an\nAuto Encoding(AE) model that suffered from the following\nproblems:\n• The use of [MASK] tokens during the pre-training phase\nled to a discrepancy as these tokens were absent during\nthe ﬁne-tuning phase.\n• The model neglected inherent dependencies among two\nor more [MASK] tokens, thus leading to sub-optimal\nperformance.\nA new model, the XLNet [21] was able to overcome these\ndifﬁculties by using a modiﬁcation of general autoregressive\npretraining.\nGeneralized Autoregressive Pretraining Phase Instead\nof using unidirectional language modeling or bidirectional\nmasked language model to predict tokens, the paper proposed\npassing all permutations of a given sequence to the model\nand predicting a particular token missing from this sequence.\nDespite random re-ordering of the sequence, order-related\ninformation remains preserved as positional encodings of the\ntokens remained the same for all permutations of the input\nsequence.\nUse of this modiﬁed form of pretraining helped overcome\nthe two main challenges posed by the BERT architecture.\nAlong with that, the XLNet incorporated Transformer-XL into\nits core architecture. This allowed for better modeling of long-\ndependencies compared to BERT. Through the use of these\ntwo major modiﬁcations, the XLNet provided new state-of-\nthe-art results in 18 natural language processing tasks.\nSigniﬁcant gains were observed compared to BERT espe-\ncially in tasks such as machine reading comprehension which\nrequired modeling of long-range dependencies. The authors\nattribute this improvement mainly to the use of Transformer-\nXL in the XLNet architecture. The XLNet, similar to BERT,\ncan be used for a wide range of single sentence, sentence pair\nand reading comprehension tasks among others.\nV. CONCLUSION\nWe have thus provided a lucid summary of recent advances\nin the domain of transfer learning in the domain of natural\nlanguage processing. We hope that this survey would help\nthe reader gain a quick and profound understanding of this\ndomain. Recent advances in this domain, despite being a\nstep forward, come with their challenges. Speciﬁcally, large\narchitectures such as the BERT, XLNet and Transformer-XL\nmake training and deployment difﬁcult owing to the large\namount of processing power required. Furthermore, employing\nlarge and opaque models impedes upon the explainability\naspect of the same, thus making one question their deploy-\nment in the real-world. Thirdly, while newer models can\nprovide improvements over their predecessors, the lack of\na standard benchmark dataset for pre-training these models\nmakes one question whether these improvements were due\nto an architectural innovation or simply because the said\nmodel was pre-trained on larger amount of data. Take for\ninstance, it is difﬁcult to gauge whether the XLNet model\nbettered upon the BERT model because of an architectural\nimprovement or because it was pre-trained on a larger corpus.\nThus, there is a need to decide upon a standard pre-training\ndataset to remove this ambiguity. Lighter models such as the\nDistilBERT and ALBERT are a step in the right direction and\ncould potentially help bridge the gap between performance\nand processing power. On the other hand, innovations brought\nabout during the training phase, such as in the RoBERTa model\nmight help seek out better performance using the same model\narchitecture.\nREFERENCES\n[1] Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov,\nRaquel Urtasun, Antonio Torralba, Sanja Fidler,\n”Aligning\nBooks and Movies: Towards Story-like Visual Explanations\nby Watching Movies and Reading Books”, arXiv:1506.06724\n[cs.CV]\n[2] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten\nBrants, Phillipp Koehn, Tony Robinson,\n”One Billion Word\nBenchmark for Measuring Progress in Statistical Language\nModeling”, arXiv:1312.3005 [cs.CL]\n[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nLi Fei-Fei,\n”ImageNet: A large-scale hierarchical image\ndatabase”, 2009 IEEE Conference on Computer Vision and\nPattern Recognition\n[4] Zachary C. Lipton, John Berkowitz, Charles Elkan, ”A Critical\nReview of Recurrent Neural Networks for Sequence Learning”,\narXiv:1506.00019 [cs.LG]\n[5] P.J. Werbos, ”Backpropagation through time: what it does and\nhow to do it”, Proceedings of the IEEE, Volume: 78 , Issue: 10\n, Oct 1990\n[6] Sepp\nHochreiter,Jurgen\nSchmidhuber,\n”Long\nshort-term\nmemory”, Neural Computation 9(8):1735-1780, 1997\n[7] Christopher\nOlah,\n”Understanding\nLSTM\nNetworks”,\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/\n[8] K. Cho, B. van Merrienboer, D. Bahdanau, Y. Bengio,\n”On\nthe properties of neural machine translation: Encoder-decoder\napproaches”, arXiv preprint arXiv:1409.1259, 2014\n[9] Stephen\nMerity,\nNitish\nShirish\nKeskar,\nRichard\nSocher,\n”Regularizing\nand\nOptimizing\nLSTM\nLanguage\nModels”,\narXiv:1708.02182 [cs.CL]\n[10] Ilya Sutskever, Oriol Vinyals, Quoc V. Le,\n”Sequence to\nSequence Learning with Neural Networks”, arXiv:1409.3215\n[cs.CL]\n[11] Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, ”Neural\nMachine Translation by Jointly Learning to Align and Translate”,\narXiv:1409.0473 [cs.CL]\n[12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin,\n”Attention is All you Need”, Advances in Neural Information\nProcessing Systems 30 (NIPS 2017)\n[13] Jeremy Howard, Sebastian Ruder, ”Universal Language Model\nFine-tuning for Text Classiﬁcation”, arXiv:1801.06146 [cs.CL]\n[14] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,\nChristopher Clark, Kenton Lee, Luke Zettlemoyer,\n”Deep\nContextualized Word Representations”, Volume: Proceedings\nof the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies, Volume 1, June 2018\n[15] Alec Radford, Kartik Narsimhan, Tim Salimans, Ilya Sutskever\n”Improving\nLanguage\nUnderstanding\nby\nGenerative\nPre-\nTraining”,2018\n[16] Jacob\nDevlin,\nMing-Wei\nChang,\nKenton\nLee,\nKristina\nToutanova,\n”BERT:\nPre-training\nof\nDeep\nBidirectional\nTransformers for Language Understanding”, arXiv:1810.04805\n[cs.CL]\n[17] Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David\nand Amodei, Dario and Sutskever, Ilya, ”Language Models are\nUnsupervised Multitask Learners”, 2019\n[18] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole\nLimtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-\nCespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian\nStrope,\nRay\nKurzweil,\n”Universal\nSentence\nEncoder”,\narXiv:1803.11175 [cs.CL]\n[19] Marcus, Mitchell P. and Marcinkiewicz, Mary Ann and\nSantorini, Beatrice,\n”Building a Large Annotated Corpus\nof English: The Penn Treebank”, Computational Linguistics\nVolume 19 Issue 2, 0891-2017, June 1993.\n[20] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,\nQuoc\nV.\nLe,\nRuslan\nSalakhutdinov,\n”Transformer-XL:\nAttentive Language Models Beyond a Fixed-Length Context”,\narXiv:1901.02860 [cs.LG]\n[21] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan\nSalakhutdinov, Quoc V. Le, ”XLNet: Generalized Autoregressive\nPretraining for Language Understanding”, arXiv:1906.08237\n[cs.CL]\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-10-16",
  "updated": "2019-10-16"
}