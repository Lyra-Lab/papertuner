{
  "id": "http://arxiv.org/abs/1910.08207v1",
  "title": "Unsupervised Multi-Task Feature Learning on Point Clouds",
  "authors": [
    "Kaveh Hassani",
    "Mike Haley"
  ],
  "abstract": "We introduce an unsupervised multi-task model to jointly learn point and\nshape features on point clouds. We define three unsupervised tasks including\nclustering, reconstruction, and self-supervised classification to train a\nmulti-scale graph-based encoder. We evaluate our model on shape classification\nand segmentation benchmarks. The results suggest that it outperforms prior\nstate-of-the-art unsupervised models: In the ModelNet40 classification task, it\nachieves an accuracy of 89.1% and in ShapeNet segmentation task, it achieves an\nmIoU of 68.2 and accuracy of 88.6%.",
  "text": "Unsupervised Multi-Task Feature Learning on Point Clouds\nKaveh Hassani\nAutodesk AI Lab\nToronto, Canada\nkaveh.hassani@autodesk.com\nMike Haley\nAutodesk AI Lab\nSan Francisco, USA\nmike.haley@autodesk.com\nAbstract\nWe introduce an unsupervised multi-task model to jointly\nlearn point and shape features on point clouds. We deﬁne\nthree unsupervised tasks including clustering, reconstruc-\ntion, and self-supervised classiﬁcation to train a multi-scale\ngraph-based encoder. We evaluate our model on shape clas-\nsiﬁcation and segmentation benchmarks. The results sug-\ngest that it outperforms prior state-of-the-art unsupervised\nmodels: In the ModelNet40 classiﬁcation task, it achieves\nan accuracy of 89.1% and in ShapeNet segmentation task,\nit achieves an mIoU of 68.2 and accuracy of 88.6%.\n1. Introduction\nPoint clouds are sparse order-invariant sets of interact-\ning points deﬁned in a coordinate space and sampled from\nsurface of objects to capture their spatial-semantic informa-\ntion. They are the output of 3D sensors such as LiDAR\nscanners and RGB-D cameras, and are used in applications\nsuch as human-computer interactions [21], self-driving cars\n[51], and robotics [60]. Their sparse nature makes them\ncomputationally efﬁcient and less sensitive to noise com-\npared to volumetric and multi-view representations.\nClassic methods craft salient geometric features on point\nclouds to capture their local or global statistical properties.\nIntrinsic features such as wave kernel signature (WKS) [6],\nheat kernel signature (HKS) [7], multi-scale Gaussian cur-\nvature [66], and global point signature [57]; and extrin-\nsic features such as persistent point feature histograms [59]\nand fast point feature histograms [58] are examples of such\nfeatures. These features cannot address semantic tasks re-\nquired by modern applications and hence are replaced by\nthe unparalleled representation capacity of deep models.\nFeeding point clouds to deep models, however, is not\ntrivial. Standard deep models operate on regular-structured\ninputs such as grids (images and volumetric data) and\nsequences (speech and text) whereas point clouds are\npermutation-invariant and irregular in nature. One can ras-\nterize the point clouds into voxels [81, 43, 53] but it de-\nmands excessive time and memory, and suffers from infor-\nmation loss and quantization artifacts [78].\nSome recent deep models can directly consume point\nclouds and learn to perform various tasks such as classiﬁ-\ncation [78], semantic segmentation [89, 17], part segmen-\ntation [78], image-point cloud translation [19], object de-\ntection and region proposal [97], consolidation and surface\nreconstruction [92, 45, 47], registration [16, 74, 34], gener-\nation [68, 67], and up-sampling [93]. These models achieve\npromising results thanks to their feature learning capabili-\nties. However, to successfully learn such features, they re-\nquire large amounts of labeled data.\nA few works explore unsupervised feature learning on\npoint sets using autoencoders [88, 13, 39, 96, 2, 16] and\ngenerative models, e.g., generative adversarial networks\n(GAN) [68, 67, 2], variational autoencoders (VAE) [20],\nand Gaussian mixture models (GMM) [2]. Despite their\ngood feature learning capabilities, they suffer from not\nhaving access to supervisory signals and targeting a sin-\ngle task.\nThese shortcomings can be addressed by self-\nsupervised learning and multi-task learning, respectively.\nSelf-supervised learning deﬁnes a pretext task using only\nthe information present in the data to provide a surrogate su-\npervisory signal whereas multi-task learning uses the com-\nmonalities across tasks by jointly learning them [95].\nWe\nintroduce\na\nmulti-task\nmodel\nthat\nexploits\nthree regimes of unsupervised learning including self-\nsupervision, autoencoding, and clustering as its target tasks\nto jointly learn point and shape features. Inspired by [9, 22],\nwe show that leveraging joint clustering and self-supervised\nclassiﬁcation along with enforcing reconstruction achieves\npromising results while avoiding trivial solutions. The key\ncontributions of our work are as follows:\n• We introduce a multi-scale graph-based encoder for\npoint clouds and train it within an unsupervised multi-\ntask learning setting.\n• We exhaustively evaluate our model under various\nlearning settings on ModelNet40 shape classiﬁcation\nand ShapeNetPart segmentation tasks.\narXiv:1910.08207v1  [cs.CV]  18 Oct 2019\n• We show that our model achieves state-of-the-art re-\nsults w.r.t prior unsupervised models and narrows the\ngap between unsupervised and supervised models.\n2. Related Work\n2.1. Deep Learning on Point Clouds\nPointNet [52] is an MLP that learns point features inde-\npendently and aggregates them into a shape feature. Point-\nNet++ [54] deﬁnes multi-scale regions and uses PointNet\nto learn their features and then hierarchically aggregates\nthem. Models based on KD-trees [37, 94, 20] spatially par-\ntition the points using kd-trees and then recursively aggre-\ngate them. RNNs [31, 89, 17, 41] are applied to point clouds\nby the assumption that “order matters” [72] and achieve\npromising results on semantic segmentation tasks but the\nquality of the learned features is not clear.\nCNN models introduce non-Euclidean convolutions to\noperate on point sets. A few models such as RGCNN [70],\nSyncSpecCNN [91] and Local Spectral GCNN [75] operate\non spectral domain. These models tend to be computation-\nally expensive. Spatial CNNs learn point features by aggre-\ngating the contributions of neighbor points. Pointwise con-\nvolution [30], Edge convolution [78] , Spider convolution\n[84], sparse convolution [65, 25], Monte Carlo convolu-\ntion [27], parametric continuous convolution [76], feature-\nsteered graph convolution [71], point-set convolution [63],\nχ-convolution [40], and spherical convolution [38] are ex-\namples of these models. Spatial models provide strong lo-\ncalized ﬁlters but struggle to learn global structures [70].\nA few works train generative models on point sets. Mul-\ntiresolution VAE [20] introduces a VAE with multiresolu-\ntion convolution and deconvolution layers. PointGrow [68]\nis an auto-regressive model that can generate point clouds\nfrom scratch or conditioned on given semantic contexts. It\nis shown that GMMs trained on PointNet features achieve\nbetter performance compared to GANs [2].\nA few recent works explore representation learning us-\ning autoencoders. A simple autoencoder based on Point-\nNet is shown to achieve good results on various tasks [2].\nFoldingNet [88] uses an encoder with graph pooling and\nMLP layers and introduces a decoder of folding operations\nthat deform a 2D grid onto the underlying object surface.\nPPF-FoldNet [13] projects the points into point pair feature\n(PPF) space and then applies a PointNet encoder and a Fold-\ningNet decoder to reconstruct that space. AtlasNet[26] ex-\ntends the FoldingNet to multiple grid patches whereas SO-\nNet [39] aggregates the point features into SOM node fea-\ntures to encode the spatial distributions. PointCapsNet [96]\nintroduces an autoencoder based on dynamic routing to ex-\ntract latent capsules and a few MLPs that generate multiple\npoint patches from the latent capsules with distinct grids.\n2.2. Self-Supervised Learning\nSelf-supervised learning deﬁnes a proxy task on unla-\nbeled data and uses the pseudo-labels of that task to provide\nthe model with supervisory signals. It is used in machine vi-\nsion with proxy tasks such as predicting arrow of time [79],\nmissing pixels [50], position of patches [14], image rota-\ntions [23], synthetic artifacts [33], image clusters [9], cam-\nera transformation in consecutive frames [3], rearranging\nshufﬂed patches [48], video colourization [73], and track-\ning of image patches[77] and has demonstrated promising\nresults in learning and transferring visual features.\nThe main challenge in self-supervised learning is to de-\nﬁne tasks that relate most to the down-stream tasks that use\nthe learned features [33]. Unsupervised learning, e.g., den-\nsity estimation and clustering, on the other hand, is not do-\nmain speciﬁc [9]. Deep clustering [4, 44, 86, 28, 83, 22,\n61, 87, 29] models are recently proposed to learn cluster-\nfriendly features by jointly optimizing a clustering loss with\na network-speciﬁc loss. A few recent works combine these\ntwo approaches and deﬁne deep clustering as a surrogate\ntask for self-supervised learning. It is shown that alternat-\ning between clustering the latent representation and predict-\ning the cluster assignments achieves state-of-the-art results\nin visual feature learning[9, 22].\n2.3. Multi-Task Learning\nMulti-task learning leverages the commonalities across\nrelevant tasks to enhance the performance over those tasks\n[95, 18].\nIt learns a shared feature with adequate ex-\npressive power to capture the useful information across\nthe tasks. Multi-task learning has been successfully used\nin machine vision applications such as image classiﬁca-\ntion [42], image segmentation [12], video captioning [49],\nand activity recognition [85]. A few works explore self-\nsupervised multi-task learning to learn high level visual fea-\ntures [15, 55]. Our approach is relevant to these models\nexcept we use self-supervised tasks in addition to other un-\nsupervised tasks such as clustering and autoencoding.\n3. Methodology\nAssume a training set S = [s1, s2, ..., sN] of N point\nsets where a point set si = {pi\n1, pi\n2, ..., pi\nM} is an order-\ninvariant set of M points and each point pi\nj ∈Rdin. In the\nsimplest case pi\nj = (xi\nj, yi\nj, zi\nj) only contains coordinates,\nbut can extend to carry other features, e.g., normals. We\ndeﬁne an encoder Eθ : S 7−→Z that maps input point\nsets from RM×din into the latent space Z ∈Rdz such that\ndz ≫din. For each point pi\nj, the encoder ﬁrst learns a point\n(local) feature zi\nj ∈Rdz and then aggregates them into a\nshape (global) feature Zi ∈Rdz. It basically projects the\ninput points to a feature subspace with higher dimension\nto encode richer local information than the original space.\nAny parametric non-linear function parametrized by θ can\nbe used as the encoder. To learn θ in unsupervised multi-\ntask fashion, we deﬁne three parametric functions on the\nlatent variable Z as follows:\nClustering function Γc : Z 7−→Y maps the latent\nvariable into K categories Y = [y1, y2, ..., yn] such that\nyi ∈{0, 1}K and yT\nn 1k = 1. This function encourages\nthe encoder to generate features that are clustering-friendly\nby pushing similar samples in the feature space closer and\npushing dissimilar ones away. It also provides the model\nwith pseudo-labels for self-supervised learning through its\nhard cluster assignments.\nClassiﬁer function fψ : Z 7−→ˆY predicts the cluster\nassignments of the latent variable such that the predictions\ncorrespond to the hard clusters assignments of Γc. In other\nwords, fψ maps the latent variable into K predicted cate-\ngories ˆY = [ˆy1, ˆy2, ..., ˆyn] such that ˆyi ∈{0, 1}K. This\nfunction uses the pseudo-labels generated by the clustering\nfunction, i.e., cluster assignments, as its proxy train data.\nThe difference between the cluster assignments and the pre-\ndicted cluster assignments provides the supervisory signals.\nDecoder function gφ : Z 7−→ˆS reconstructs the orig-\ninal point set from the latent variable, i.e., maps the latent\nvariable Z ∈Rdz to a point set ˆS ∈RM×din. Training\na deep model with a clustering loss collapses the features\ninto a single cluster [9]. Some heuristics such as penalizing\nthe minimal number of points per cluster and randomly re-\nassigning empty clusters are introduced to prevent this. We\nintroduce the decoder function to prevent the model from\nconverging to trivial solutions.\n3.1. Training\nThe model alternates between clustering the latent vari-\nable Z to generate pseudo-labels Y for self-supervised\nlearning, and learning the model parameters by jointly pre-\ndicting the pseudo-labels ˆY and reconstructing the input\npoint set ˆS. Assuming K-means clustering, the model learns\na centroid matrix C ∈Rdz×K and cluster assignments yn\nby optimizing the following objective clustering [9]:\nmin\n{C,θ}\n1\nN\nN\nX\nn=1\nmin\nyn∈{0,1}K ∥zn −Cyn∥2\n2\n(1)\nwhere zn = Eθ(sn) and yT\nn 1k = 1. The centroid matrix is\ninitialized randomly. It is noteworthy that: (i) when assign-\ning cluster labels, the centroid matrix is ﬁxed, and (ii) the\ncentroid matrix is updated epoch-wise and not batch-wise\nto prevent the learning process from diverging.\nFor the classiﬁcation function, we minimize the cross-\nentropy loss between the cluster assignments and the pre-\ndicted cluster assignments as follows.\nmin\n{θ,ψ}\n−1\nN\nN\nX\nn=1\nyn log ˆyn\n(2)\nwhere yn = Γc(zn) and ˆyn = fψ(zn) are the cluster assign-\nments and the predicted cluster assignments, respectively.\nWe use Chamfer distance to measure the difference be-\ntween the original point cloud and its reconstruction. Cham-\nfer distance is differentiable with respect to points and is\ncomputationally efﬁcient.\nIt is computed by ﬁnding the\nnearest neighbor of each point of the original space in the\nreconstructed space and vice versa, and summing up their\nEuclidean distances. Hence, we optimize the decoding loss\nas follows.\nmin\n{θ,φ}\n1\n2NM\nN\nX\nn=1\nM\nX\nm=1\nmin\nˆp∈ˆsn ∥pn\nm −ˆp∥2\n2 + min\np∈sn ∥ˆpn\nm −p∥2\n2\n(3)\nwhere ˆsn = gφ(zn) and, sn and ˆsn are the original and\nreconstructed point sets, respectively. N and M denote the\nnumber of point sets in the train set and the number of points\nin each point set, respectively.\nLet’s denote the clustering, classiﬁcation, and decoding\nobjectives by LΓ, Lf, and Lg, respectively. we deﬁne the\nmulti-task objective as a linear combination of these objec-\ntives: L = αLΓ + βLf + γLg and train the model based on\nthat. The training process is shown in Algorithm 1.\nWe ﬁrst randomly initialize the model parameters and as-\nsume an arbitrary upper bound for the number of clusters.\nWe show through experiments that the model converges to a\nﬁxed number of clusters by emptying some of the clusters.\nThis is especially favorable when the true number of cate-\ngories is unknown. We then randomly select K point sets\nfrom the training data and feed them to the randomly ini-\ntialized encoder and set the extracted features as the initial\ncentroids. Afterwards we optimize the model parameters\nw.r.t the multi-task objective using mini-batch stochastic\ngradient descent. Updating the centroids with the same fre-\nquency as the network parameters can destabilize the train-\ning. Therefore, we aggregate the learned features and the\ncluster assignments within each epoch and update the cen-\ntroids after an epoch is completed.\n3.2. Architecture\nInspired by Inception [69] and Dynamic Graph CNN\n(DGCNN) [78] architectures, we introduce a graph-based\narchitecture shown in Figure 1 which consists of an en-\ncoder and three task-speciﬁc decoders. The encoder uses\na series of graph convolution, convolution, and pooling lay-\ners in a multi-scale fashion to learn point and shape fea-\ntures from an input point cloud jittered by Gaussian noise.\nFor each point, it extracts three intermediate features by ap-\nplying graph convolutions on three neighborhood radii and\nconcatenates them with the input point feature and its con-\nvolved feature. The ﬁrst three features encode the interac-\ntions between each point and its neighbors where as the last\ntwo features encode the information about each point. The\nAlgorithm 1: Unsupervised Multi-task training algo-\nrithm.\n1 θ, φ, ψ ←−Random()\nInitial parameters\n2 K ←−KUB\nUpper bound #clusters\n3 C ←−Eθ (Choice (S, K))\nInitial centroids\n4 for epoch in epochs do\n5\nwhile epoch not completed do\n6\nForward pass\n7\nSx ←−Sample(S)\nMini-batch\n8\nZx ←−Eθ (Sx)\nEncoding\n9\nYx ←−Γc (Zx)\nCluster assignment\n10\nˆYx ←−fψ (Zx)\nCluster prediction\n11\nˆSx ←−gφ (Zx)\nDecoding\n12\n(Z, Y) ←−Aggregate (Zx, Y)\n13\nBackwards pass\n14\n▽θ,φ,ψ(αLΓ(Zx, C; θ)+ Compute gradients\n15\nβLf(Yx, ˆYx; θ, ψ)+\n16\nγLg(Sx, ˆSx; θ, φ))\n17\nUpdate(θ, φ, ψ)\nUpdate with gradients\n18\nend\n19\nC ←−Update(Z, Y)\nUpdate centroids\n20 end\nconcatenation of the intermediate features is then passed\nthrough a few convolution and pooling layers to learn an-\nother level of intermediate features. These point-wise fea-\ntures are then pooled and fed to an MLP to learn the ﬁnal\nshape feature. They are also concatenated with the shape\nfeature to represent the ﬁnal point features. Similar to [78],\nwe deﬁne the graph convolution as follows:\nzi =\nX\npk∈N(pi)\nhθ ([pi ∥pk −pi])\n(4)\nwhere zi is the learned feature for point pi based on its\nneighbor contributions, pk ∈N(pi) are the k nearest points\nto the pi in Euclidean space, hθ is a nonlinear function pa-\nrameterized by θ and ∥is the concatenation operator. We\nuse a shared MLP for hθ. The reason to use both pi and\npk −pi is to encode both global information (pi) and local\ninteractions (pk −pi) of each point.\nTo perform the target tasks, i.e., clustering, classiﬁcation,\nand autoencoding, we use the following. For clustering, we\nuse a standard implementation of K-means to cluster the\nshape features. For self-supervised classiﬁcation, we feed\nthe shape features to an MLP to predict the category of the\nshape (i.e., cluster assignment by the clustering module).\nAnd for the autoencoding task, we use an MLP to recon-\nstruct the original point cloud from the shape feature. This\nMLP is denoising and reconstructs the original point cloud\nbefore the addition of the Gaussian noise. All these models\nalong with the encoder are trained jointly and end-to-end.\nNote that all these tasks are deﬁned on the shape features.\nBecause a shape feature is an aggregation of its correspond-\ning point features, learning a good shape feature pushes the\nmodel to learn good point features too.\n4. Experiments\n4.1. Implementation Details\nWe optimize the network using Adam [36] with an initial\nlearning rate of 0.003 and batch size of 40. The learning rate\nis scheduled to decrease by 0.8 every 50 epochs. We apply\nbatch-normalization [32] and ReLU activation to each layer\nand use dropout [64] with p = 0.5. To normalize the task\nweights to the same scale, we set the weights of clustering\n( α), classiﬁcation (β), and reconstruction(γ) to 0.005, 1.0,\n500, respectively. For graph convolutions, we use neighbor-\nhood radii of 15, 20, and 25 (as suggested in [78]) and for\nnormal convolutions we use 1×1 kernels. We set the up-\nper bound number of clusters (KUB) to 500. We also set\nthe size of the MLPs in prediction and reconstruction tasks\nto [2048, 1024, 500] and [2048, 1024, 6144], respectively.\nNote that the size of the last layers correspond to the upper\nbound number of clusters (500) and the reconstruction size\n(6144: 2048×3). Following [2] we set the shape and point\nfeature sizes to 512 and 1024, respectively.\nFor preprocessing and augmentation we follow [52, 78]\nand uniformly sample 2048 points and normalize them to\na unit sphere. We also apply point-wise Gaussian noise of\nN ∼(0, 0.01) and shape-wise random rotations between\n[-180, 180] degrees along z-axis and random rotations be-\ntween [-20, +20] degrees along x and y axes.\nThe model is implemented with Tensorﬂow [1] on a\nNvidia DGX-1 server with 8 Volta V100 GPUs. We used\nsynchronous parallel training by distributing the training\nmini-batches over all GPUs and averaging the gradients to\nupdate the model parameters. With this setting, our model\ntakes 830s on average to train one epoch on the ShapeNet\n(i.e, ∼55k samples of size 2048×3). We train the model\nfor 500 epochs. At test time, it takes 8ms on an input point\ncloud with size 2048×3.\n4.2. Pre-training for Transfer Learning\nFollowing the experimental protocol introduced in [2],\nwe pre-train the model across all categories of the ShapeNet\ndataset [10] (i.e., 57,000 models across 55 categories) , and\nthen transfer the trained model to two down-stream tasks\nincluding shape classiﬁcation and part segmentation. After\npre-training the model, we freeze its weights and do not\nﬁne-tune it for the down-stream tasks.\nFollowing [9], we use Normalized Mutual Information\n(NMI) to measure the correlation between cluster assign-\nments and the categories without leaking the category in-\nClustering Task\nPrediction Task\nShared Multi-scale Graph-Based Encoder\nKNN\nK=15\nGraph\nConv\nMax\nMean\n1x1\nConv\n64\nConcat\nConcat\n1x1\nConv\n64\n1x1\nConv\n64\nKNN\nK=20\nGraph\n Conv\nMax\nMean\n1x1\nConv\n64\nConcat\n1x1\nConv\n64\nKNN\nK=25\nGraph\n Conv\nMax\nMean\n1x1\nConv\n64\nConcat\n1x1\nConv\n64\n1x1\nConv\n256\nMax\nMean\n1x1\nConv\n512\nConcat\nMLP\n(1024, 512)\nConcat\nΣ\nCross-Entropy\nLoss\nMulti-Task Loss\nMLP\nReconstruction Task \nChamfer\nLoss\nMLP\nK-Means\nLoss\nK-Means\nΣ\nGaussian Noise\nPoint Cloud\nPoint Feature\nShape Feature\nFigure 1. Proposed Architecture for unsupervised multi-task feature learning on point clouds. It consists of a multi-scale graph-based\nencoder that generates point and shape features for an input point cloud and three task decoders that jointly provide the architecture with a\nmulti-task loss.\nformation to the model. This measure gives insight on the\ncapability of the model in predicting category level informa-\ntion without observing the ground-truth labels. The model\nreaches an NMI of 0.68 and 0.62 on the train and validation\nsets, respectively which suggests that the learned features\nare progressively encoding category-wise information.\nWe also observe that the model converges to 88 clus-\nters (from the initial 500 clusters) which is 33 more clusters\ncompared to the number of ShapeNet categories. This is\nconsistent with the observation that “some amount of over-\nsegmentation is beneﬁcial” [9]. The model empties more\nthan 80% of the clusters but does not converge to the trivial\nsolution of one cluster. We also trained our model on the\n10 largest ShapeNet categories to investigate the clustering\nbehavior where the model converged to 17 clusters. This\nconﬁrms that model converges to a ﬁxed number of clusters\nwhich is less than the initial upper bound assumption and is\nmore than the actual number of categories in the data.\nTo investigate the dynamics of the learned features, we\nselected the 10 largest ShapeNet categories and randomly\nsampled 200 shapes from each category. The evolution of\nthe features of the sampled shapes visualized using t-SNE\n(Figure 2) suggests that the learned features progressively\ndemonstrate clustering-friendly behavior along the training\nepochs.\n4.3. Shape Classiﬁcation\nTo evaluate the performance of the model on shape fea-\nture learning, we follow the experimental protocol in [2] and\nreport the classiﬁcation accuracy on transfer learning from\nthe ShapeNet dataset [10] to the ModelNet40 dataset [82]\n(i.e., 13,834 models across 40 categories divided to 9,843\nand 3,991 train and test samples, respectively). Similar to\n[2], we extract the shape features of the ModelNet40 sam-\nples from the pre-trained model without any ﬁne-tuning,\ntrain a linear SVM on them, and report the classiﬁcation\naccuracy. This approach is a common practice in evaluat-\ning unsupervised visual feature learning [9] and provides in-\nsight about the effectiveness of the learned features in clas-\nsiﬁcation tasks.\nResults shown in Table 1 suggest that our model achieves\nstate-of-the-art accuracy on the ModelNet40 shape classiﬁ-\ncation task compared to other unsupervised feature learning\nmodels. It is noteworthy that the reported result is without\nany hyper-parameter tuning. With random hyper-parameter\nsearch, we observed an 0.4 absolute increase in the accu-\nracy (i.e., 89.5%). The results also suggest that the unsu-\npervised model is competitive with the supervised models.\nError analysis reveals that the misclassiﬁcations occur be-\ntween geometrically similar shapes. For example, the three\nmost frequent misclassiﬁcations are between (table, desk),\n(nightstand, dresser), and (ﬂowerpot, plant) categories. A\nsimilar observation is reported in [2] and it is suggested that\nstronger supervision signals may be required to learn subtle\ndetails that discriminate these categories.\nTo further investigate the quality of the learned shape\nfeatures, we evaluated them in a zero-shot setting. For this\npurpose, we cluster the learned features using agglomera-\nepoch 1\nepoch 100\nepoch 250\nepoch 500\nFigure 2. Evolution of the learned features along the training epochs (visualized using t-SNE) showing progressive clustering-friendly\nbehavior.\nUnsupervised transfer learning\nSupervised learning\nModel\nAccuracy\nModel\nAccuracy\nSPH [35]\n68.2\nPointNet [52]\n89.2\nLFD [11]\n75.5\nPointNet++ [54]\n90.7\nT-L Network [24]\n74.4\nPointCNN [30]\n86.1\nVConv-DAE [62]\n75.5\nDGCNN [78]\n92.2\n3D-GAN [80]\n83.3\nKCNet [63]\n91.0\nLatent-GAN [2]\n85.7\nKDNet [37]\n91.8\nMRTNet-VAE [20]\n86.4\nMRTNet [20]\n91.7\nFoldingNet [88]\n88.4\nSpecGCN [75]\n91.5\nPointCapsNet [96]\n88.9\nOurs\n89.1\nTable 1. Left: Accuracy of classiﬁcation by transfer learning from\nthe ShapeNet on the ModelNet40 data. Right: Classiﬁcation ac-\ncuracy of supervised learning on the ModelNet40 data. Our model\nnarrows the gap with supervised models.\ntive hierarchical clustering (AHC) [46] and then align the\nassigned cluster labels with the ground truth labels (Mod-\nelNet40 categories) based on majority voting within each\ncluster. The results suggest that the model achieves 68.88%\naccuracy on the shape classiﬁcation task with zero supervi-\nsion. This result is consistent with the observed NMI be-\ntween cluster assignments and ground truth labels in the\nShapeNet dataset.\n4.4. Part Segmentation\nPart segmentation is a ﬁne-grained point-wise classiﬁca-\ntion task where the goal is to predict the part category label\nof each point in a given shape. We evaluate the learned point\nfeatures on the ShapeNetPart dataset [90], which contains\n16,881 objects from 16 categories (12149 train, 2874 test,\nand 1858 validation). Each object consists of 2 to 6 parts\nwith total of 50 distinct parts among all categories. Fol-\nlowing [52], we use mean Intersection-over-Union (mIoU)\nas the evaluation metric computed by averaging the IoUs\nof different parts occurring in a shape. We also report part\nclassiﬁcation accuracy.\nModel\n1% of train data\n5% of train data\nAccuracy\nIoU\nAccuracy\nIoU\nSO-Net[39]\n78.0\n64.0\n84.0\n69.0\nPointCapsNet[96]\n85.0\n67.0\n86.0\n70.0\nOurs\n88.6\n68.2\n93.7\n77.7\nTable 2. Results on semi-supervised ShapeNetPart segmentation\ntask.\nFollowing [96], we randomly sample 1% and 5% of the\nShapeNetPart train set to evaluate the point features in a\nsemi-supervised setting. We use the same pre-trained model\nto extract the point features of the sampled training data,\nalong with validation and test samples without any ﬁne-\ntuning. We then train a 4-layer MLP [2048, 4096, 1024,\n50] on the sampled training sets and evaluate it on all test\ndata.\nResults shown in Table 2 suggest that our model\nachieves state-of-the-art accuracy and mIoU on ShapeNet-\nPart segmentation task compared to other unsupervised fea-\nture learning models. Also comparisons between our model\n(trained on 5% of the training data) and the fully supervised\nmodels are shown in Table 3. The results suggest that our\nmodel achieves an mIoU which is only 8% less than the\nbest supervised model and hence narrows the gap with su-\npervised models.\nWe also performed intrinsic evaluations to investigate\nthe consistency of the learned point features within each\ncategory. We sampled a few shapes from each category,\nstacked their point features, and reduced the feature dimen-\nsion from 1024 to 512 using PCA. We then co-clustered the\nfeatures using the AHC method. The result of co-clustering\non the airplane category is shown in Figure 3. We observed\na similar consistent behavior over all categories. We also\nused AHC and hierarchical density-based spatial clustering\n(HDBSCAN) [8] methods to cluster the point features of\neach shape. We aligned the assigned cluster labels with the\nground truth labels based on majority voting within each\ncluster. A few sample shapes along with their ground truth\nModel\n%train Cat.\nIns. Aero Bag Cap Car Chair Ear Guitar Knife Lamp Laptop Motor Mug Pistol Rocket Skate Table\ndata mIoU mIoU\nphone\nboard\nPointNet [52]\n80.4\n83.7 83.4 78.7 82.5 74.9 89.6\n73.0\n91.5\n85.9 80.8\n95.3\n65.2 93.0 81.2\n57.9\n72.8 80.6\nPointNet++ [54]\n81.9\n85.1 82.4 79.0 87.7 77.3 90.8\n71.8\n91.0\n85.9 83.7\n95.3\n71.6 94.1 81.3\n58.7\n76.4 82.6\nDGCNN [78]\n82.3\n85.1 84.2 83.7 84.4 77.1 90.9\n78.5\n91.5\n87.3 82.9\n96.0\n67.8 93.3 82.6\n59.7\n75.5 82.0\nKCNet [63]\n82.2\n84.7 82.8 81.5 86.4 77.6 90.3\n76.8\n91.0\n87.2 84.5\n95.5\n69.2 94.4 81.6\n60.1\n75.2 81.3\nRSNet [31]\n81.4\n84.9 82.7 86.4 84.1 78.2 90.4\n69.3\n91.4\n87.0 83.5\n95.4\n66.0 92.6 81.8\n56.1\n75.8 82.2\nSynSpecCNN [91] 100% 82.0\n84.7 81.6 81.7 81.9 75.2 90.2\n74.9\n93.0\n86.1 84.7\n95.6\n66.7 92.7 81.6\n60.6\n82.9 82.1\nRGCNN [70]\n79.5\n84.3 80.2 82.8 92.6 75.3 89.2\n73.7\n91.3\n88.4 83.3\n96.0\n63.9 95.7 60.9\n44.6\n72.9 80.4\nSpiderCNN [84]\n82.4\n85.3 83.5 81.0 87.2 77.5 90.7\n76.8\n91.1\n87.3 83.3\n95.8\n70.2 93.5 82.7\n59.7\n75.8 82.8\nSPLATNet [65]\n83.7\n85.4 83.2 84.3 89.1 80.3 90.7\n75.5\n92.1\n87.1 83.9\n96.3\n75.6 95.8 83.8\n64.0\n75.5 81.8\nFCPN [56]\n84.0\n84.0 84.0 82.8 86.4 88.3 83.3\n73.6\n93.4\n87.4 77.4\n97.7\n81.4 95.8 87.7\n68.4\n83.6 73.4\nOurs\n5%\n72.1\n77.7 78.4 67.7 78.2 66.2 85.5\n52.6\n87.7\n81.6 76.3\n93.7\n56.1 80.1 70.9\n44.7\n60.7 73.0\nTable 3. Comparison between our semi-supervised model and supervised models on ShapeNetPart segmentation task. Average mIoU over\ninstances (Ins.) and categories (Cat.) are reported.\nFigure 3. Co-clustering of the learned point features within the Air-\nplane category using hierarchical clustering which demonstrates\nthe consistency of the learned point features within the category.\npart labels, predicted part labels by the trained MLP, AHC,\nand HDBSCAN clustering are illustrated in Figure 4. As\nshown, HDBSCAN clustering results in a decent segmenta-\ntion of the learned features in a fully unsupervised setting.\n4.5. Ablation Study\nWe ﬁrst investigate the effectiveness of the graph-based\nencoder on the shape classiﬁcation task. In the ﬁrst experi-\nment, we replace the encoder with a PointNet [52] encoder\nand keep the multi-task decoders.\nWe train and test the\nnetwork with the same transfer learning protocol which re-\nsults in a classiﬁcation accuracy of 86.2%. Compared to\nthe graph-based encoder with accuracy of 89.1%, this sug-\ngests that our encoder learns better features and hence con-\ntributes to the state-of-the-art results that we achieve. To\ninvestigate the effectiveness of the multi-task learning, we\ncompare our result against the results reported on a PointNet\nEncoder\nDecoder\nAccuracy\nPointNet\nReconstruction\n85.7\nPointNet\nMulti-Task\n86.2\nOurs\nReconstruction\n86.7\nOurs\nMulti-Task\n89.1\nTable 4. Effect of encoder and multi-task learning on accuracy on\nthe ModelNet40.\nautoencoder (i.e., single reconstruction decoder) [2] which\nachieves classiﬁcation accuracy of 85.7%. This suggests\nthat using multi-task learning improves the quality of the\nlearned features. The summary of the results is shown in\nTable 4.\nWe also investigate the effect of different tasks on the\nquality of the learned features by masking the task losses\nand training and testing the model on each conﬁguration.\nThe results shown in Table 5 suggest that the reconstruc-\ntion task has the highest impact on the performance. This is\nbecause contrary to [9], we are not applying any heuristics\nto avoid trivial solutions and hence when the reconstruction\ntask is masked both clustering and classiﬁcation tasks tend\nto collapse the features to one cluster which results in de-\ngraded feature learning.\nMoreover, the results suggest that masking the cross-\nentropy loss degrades the accuracy to 87.6% (absolute de-\ncrease of 1.5%) whereas masking the k-means loss has a\nless adverse effect (degraded loss of 88.3%, i.e., absolute\ndecrease of 0.8%). This implies that the cross-entropy loss\n(classiﬁer) plays a more important role than the clustering\nloss. Furthermore, the results indicate that having both K-\nmeans and cross-entropy losses along with the reconstruc-\ntion task yields the best result (i.e., accuracy of 89.1%).\nThis may seems counter-intuitive as one may assume that\nusing the clustering pseudo-labels to learn a classiﬁcation\nfunction would push the classiﬁer to replicate the K-means\nbehavior and hence the k-means loss will be redundant.\nGround Truth\nHDBSCAN \nClustering\nAgglomerative  \nClustering\nMLP trained on \n 1% of data\nFigure 4. A few sample shapes along with their ground truth part labels, predicted part labels by the trained MLP on 1% of the training\ndata, and predicted part labels by AHC and HDBSCAN methods.\nClassiﬁcation Reconstruction Clustering Overall\nTask\nTask\nTask\nAccuracy\n√\n×\n×\n22.8\n×\n√\n×\n86.7\n×\n×\n√\n6.9\n√\n√\n×\n88.3\n√\n×\n√\n15.2\n×\n√\n√\n87.6\n√\n√\n√\n89.1\nTable 5. Effect of tasks on the accuracy of classiﬁcation on the\nModelNet40.\nHowever, we think this is not the case because the classi-\nﬁer introduces non-linearity to the feature space by learn-\ning non-linear boundaries to approximate the predictions of\nthe linear K-means model which in turn affects the clus-\ntering outcomes in the following epoch. K-means loss on\nthe other hand, pushes the features in the same cluster to\na closer space while pushing the features of other clusters\naway.\nFinally, we report some of our failed experiments:\n• We tried K-Means++ [5] to warm-start the cluster cen-\ntroids. We did not observe any signiﬁcant improve-\nment over the randomly selected centroids.\n• We tried soft parameter sharing between the decoder\nand classiﬁer models. We observed that this destabi-\nlizes the model and hence we isolated them.\n• Similar to [78], we tried stacking more graph convo-\nlution layers and recomputing the input adjacency to\neach layer based on the feature space of its predeces-\nsor layer. We observed that this has an adverse effect\non both classiﬁcation and segmentation tasks.\n5. Conclusion\nWe proposed an unsupervised multi-task learning ap-\nproach to learn point and shape features on point clouds\nwhich uses three unsupervised tasks including clustering,\nautoencoding, and self-supervised classiﬁcation to train a\nmulti-scale graph-based encoder. We exhaustively evalu-\nated our model on point cloud classiﬁcation and segmenta-\ntion benchmarks. The results suggest that the learned fea-\ntures outperform prior state-of-the-art models in unsuper-\nvised representation learning. For example, in ModelNet40\nshape classiﬁcation tasks, our model achieved the state-of-\nthe-art (among unsupervised models) accuracy of 89.1%\nwhich is also competitive with supervised models. In the\nShapeNetPart segmentation task, it achieved mIoU of 77.7\nwhich is only 8% less than the state-of-the-art supervised\nmodel. For future directions, we are planning to: (i) intro-\nduce more powerful decoders to enhance the quality of the\nlearned features, (ii) investigate the effect of other features\nsuch as normals and geodesics, and (iii) adapt the model to\nperform semantic segmentation tasks too.\nReferences\n[1] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-\nmawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow:\na system for large-scale machine learning.\n[2] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas. Learning representations and generative\nmodels for 3d point clouds. 2018.\n[3] Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning\nto see by moving. In The IEEE International Conference on\nComputer Vision (ICCV), pages 37–45, December 2015.\n[4] Elie Aljalbout, Vladimir Golkov, Yawar Siddiqui, and Daniel\nCremers. Clustering with deep learning: Taxonomy and new\nmethods. arXiv preprint arXiv:1801.07648, 2018.\n[5] David Arthur and Sergei Vassilvitskii. K-means++: The ad-\nvantages of careful seeding.\nIn Proceedings of the Eigh-\nteenth Annual ACM-SIAM Symposium on Discrete Algo-\nrithms, pages 1027–1035, 2007.\n[6] Mathieu Aubry, Ulrich Schlickewei, and Daniel Cremers.\nThe wave kernel signature: A quantum mechanical approach\nto shape analysis. In 2011 IEEE International Conference\non Computer Vision Workshops (ICCV Workshops), pages\n1626–1633, Nov 2011.\n[7] Michael M. Bronstein and Iasonas Kokkinos. Scale-invariant\nheat kernel signatures for non-rigid shape recognition. In\n2010 IEEE Computer Society Conference on Computer Vi-\nsion and Pattern Recognition, pages 1704–1711, June 2010.\n[8] Ricardo JGB Campello, Davoud Moulavi, and J¨org Sander.\nDensity-based clustering based on hierarchical density esti-\nmates. In Paciﬁc-Asia conference on knowledge discovery\nand data mining, pages 160–172. Springer, 2013.\n[9] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning\nof visual features. In The European Conference on Computer\nVision (ECCV), pages 132–149, September 2018.\n[10] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015.\n[11] Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming\nOuhyoung. On visual similarity based 3d model retrieval. In\nComputer graphics forum, volume 22, pages 223–232. Wi-\nley Online Library, 2003.\n[12] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-\nmantic segmentation via multi-task network cascades.\nIn\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3150–3158, 2016.\n[13] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppf-foldnet:\nUnsupervised learning of rotation invariant 3d local descrip-\ntors.\nIn The European Conference on Computer Vision\n(ECCV), September 2018.\n[14] Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsu-\npervised visual representation learning by context prediction.\nIn The IEEE International Conference on Computer Vision\n(ICCV), pages 1422–1430, December 2015.\n[15] Carl Doersch and Andrew Zisserman.\nMulti-task self-\nsupervised visual learning. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 2051–2060,\n2017.\n[16] Gil Elbaz, Tamar Avraham, and Anath Fischer.\n3d point\ncloud registration for localization using a deep neural net-\nwork auto-encoder. In The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 4631–4640,\nJuly 2017.\n[17] Francis Engelmann, Theodora Kontogianni, Alexander Her-\nmans, and Bastian Leibe. Exploring spatial context for 3d\nsemantic segmentation of point clouds. In The IEEE Inter-\nnational Conference on Computer Vision (ICCV) Workshops,\npages 716–724, Oct 2017.\n[18] Andreas Argyriou Theodoros Evgeniou and Massimiliano\nPontil.\nMulti-task feature learning.\nIn Advances in Neu-\nral Information Processing Systems 19: Proceedings of the\n2006 Conference, volume 19, page 41. MIT Press, 2007.\n[19] Haoqiang Fan, Hao Su, and Leonidas J. Guibas. A point\nset generation network for 3d object reconstruction from a\nsingle image. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 605–613, July 2017.\n[20] Matheus Gadelha, Rui Wang, and Subhransu Maji. Multires-\nolution tree networks for 3d point cloud processing. In The\nEuropean Conference on Computer Vision (ECCV), pages\n103–118, September 2018.\n[21] Liuhao Ge, Yujun Cai, Junwu Weng, and Junsong Yuan.\nHand pointnet: 3d hand pose estimation using point sets.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 8417–8426, June 2018.\n[22] Kamran Ghasedi Dizaji, Amirhossein Herandi, Cheng Deng,\nWeidong Cai, and Heng Huang. Deep clustering via joint\nconvolutional autoencoder embedding and relative entropy\nminimization.\nIn The IEEE International Conference on\nComputer Vision (ICCV), pages 5736–5745, Oct 2017.\n[23] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. In International Conference on Learning Representa-\ntions (ICLR), 2018.\n[24] Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Ab-\nhinav Gupta. Learning a predictable and generative vector\nrepresentation for objects. In European Conference on Com-\nputer Vision, pages 484–499. Springer, 2016.\n[25] Benjamin Graham, Martin Engelcke, and Laurens van der\nMaaten. 3d semantic segmentation with submanifold sparse\nconvolutional networks. In The IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 9224–\n9232, June 2018.\n[26] Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan\nRussell, and Mathieu Aubry.\nAtlasnet: A papier-mˆach´e\napproach to learning 3d surface generation.\nIn The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), June 2018.\n[27] Pedro Hermosilla, Tobias Ritschel, Pere-Pau V´azquez, `Alvar\nVinacua, and Timo Ropinski. Monte carlo convolution for\nlearning on non-uniformly sampled point clouds.\nACM\nTrans. Graph., 37(6):235:1–235:12, 2018.\n[28] John R Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji\nWatanabe.\nDeep clustering: Discriminative embeddings\nfor segmentation and separation.\nIn 2016 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 31–35, March 2016.\n[29] C. Hsu and C. Lin.\nCnn-based joint clustering and rep-\nresentation learning with feature drift compensation for\nlarge-scale image data. IEEE Transactions on Multimedia,\n20(2):421–429, Feb 2018.\n[30] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Point-\nwise convolutional neural networks. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n984–993, June 2018.\n[31] Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Re-\ncurrent slice networks for 3d segmentation of point clouds.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2626–2635, June 2018.\n[32] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In Francis Bach and David Blei, editors, Pro-\nceedings of the 32nd International Conference on Machine\nLearning, Proceedings of Machine Learning Research, pages\n448–456, Jul 2015.\n[33] Simon Jenni and Paolo Favaro. Self-supervised feature learn-\ning by learning to spot artifacts. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n2733–2742, June 2018.\n[34] Felix Jremo Lawin, Martin Danelljan, Fahad Shahbaz Khan,\nPer-Erik Forssn, and Michael Felsberg.\nDensity adaptive\npoint set registration. In The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 3829–3837,\nJune 2018.\n[35] Michael Kazhdan,\nThomas Funkhouser,\nand Szymon\nRusinkiewicz.\nRotation invariant spherical harmonic rep-\nresentation of 3d shape descriptors. In Proceedings of the\n2003 Eurographics/ACM SIGGRAPH Symposium on Ge-\nometry Processing, SGP ’03, pages 156–164, Aire-la-Ville,\nSwitzerland, Switzerland, 2003. Eurographics Association.\n[36] Diederik P Kingma and Jimmy Lei Ba. Adam: Amethod\nfor stochastic optimization. In International Conference on\nLearning Representation (ICLR), 2014.\n[37] Roman Klokov and Victor Lempitsky. Escape from cells:\nDeep kd-networks for the recognition of 3d point cloud mod-\nels. In The IEEE International Conference on Computer Vi-\nsion (ICCV), pages 863–872, Oct 2017.\n[38] Haun Lei, Naveed Akhtar, and Ajmal Mian. Spherical con-\nvolutional neural network for 3d point clouds. arXiv preprint\narXiv:1805.07872, 2018.\n[39] Jiaxin Li, Ben M. Chen, and Gim Hee Lee. So-net: Self-\norganizing network for point cloud analysis. In The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 19397–9406, June 2018.\n[40] Yangyan Li, Rui Bu, Mingchao Sun, and Baoquan Chen.\nPointcnn: Convolution on x-transformed points. In Advances\nin Neural Information Processing Systems 32, pages 828–\n838. Curran Associates, Inc., 2018.\n[41] Xinhai Liu, Zhizhong Han, Yu-Shen Liu, and Matthias\nZwicker. Point2sequence: Learning the shape representa-\ntion of 3d point clouds with an attention-based sequence to\nsequence network. arXiv preprint arXiv:1811.02565, 2018.\n[42] Yong Luo, Yonggang Wen, Dacheng Tao, Jie Gui, and Chao\nXu. Large margin multi-modal multi-task feature extraction\nfor image classiﬁcation. IEEE Transactions on Image Pro-\ncessing, 25(1):414–427, 2015.\n[43] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-\nvolutional neural network for real-time object recognition.\nIn 2015 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 922–928, Sept 2015.\n[44] Erxue Min, Xifeng Guo, Qiang Liu, Gen Zhang, Jianjing\nCui, and Jun Long. A survey of clustering with deep learn-\ning: From the perspective of network architecture.\nIEEE\nAccess, 6:39501–39514, 2018.\n[45] Christian Mostegel, Rudolf Prettenthaler, Friedrich Fraun-\ndorfer, and Horst Bischof. Scalable surface reconstruction\nfrom point clouds with extreme scale and density diversity.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 904–913, July 2017.\n[46] Daniel M¨ullner. Modern hierarchical, agglomerative cluster-\ning algorithms. arXiv preprint arXiv:1109.2378, 2011.\n[47] Liangliang Nan and Peter Wonka. Polyﬁt: Polygonal surface\nreconstruction from point clouds. In The IEEE International\nConference on Computer Vision (ICCV), pages 2353–2361,\nOct 2017.\n[48] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In Euro-\npean Conference on Computer Vision (ECCV), pages 69–84.\nSpringer, 2016.\n[49] Ramakanth Pasunuru and Mohit Bansal. Multi-task video\ncaptioning with video and entailment generation.\nIn Pro-\nceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n1273–1283, 2017.\n[50] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A. Efros.\nContext encoders: Feature\nlearning by inpainting. In The IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 2536–\n2544, June 2016.\n[51] Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, and\nLeonidas J. Guibas.\nFrustum pointnets for 3d object de-\ntection from rgb-d data. In The IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 918–\n927, June 2018.\n[52] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas.\nPointnet: Deep learning on point sets for 3d classiﬁcation\nand segmentation. In The IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 652–660, July\n2017.\n[53] Charles R. Qi, Hao Su, Matthias Niessner, Angela Dai,\nMengyuan Yan, and Leonidas J. Guibas.\nVolumetric and\nmulti-view cnns for object classiﬁcation on 3d data. In The\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 5648–5656, June 2016.\n[54] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R.\nGarnett, editors, Advances in Neural Information Process-\ning Systems 30, pages 5099–5108. Curran Associates, Inc.,\n2017.\n[55] Zhongzheng Ren and Yong Jae Lee.\nCross-domain self-\nsupervised multi-task feature learning using synthetic im-\nagery. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 762–771, 2018.\n[56] Dario Rethage, Johanna Wald, Jurgen Sturm, Nassir Navab,\nand Federico Tombari. Fully-convolutional point networks\nfor large-scale point clouds. In The European Conference on\nComputer Vision (ECCV), pages 596–611, September 2018.\n[57] Raif M Rustamov. Laplace-beltrami eigenfunctions for de-\nformation invariant shape representation. In Proceedings of\nthe ﬁfth Eurographics symposium on Geometry processing,\npages 225–233. Eurographics Association, 2007.\n[58] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast\npoint feature histograms (fpfh) for 3d registration. In 2009\nIEEE International Conference on Robotics and Automation,\npages 3212–3217, May 2009.\n[59] Radu Bogdan Rusu, Nico Blodow, Zoltan C. Marton, and\nMichael Beetz. Aligning point cloud views using persistent\nfeature histograms. In 2008 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems (IROS), pages 3384–\n3391, Sept 2008.\n[60] Radu Bogdan Rusu, Zoltan Csaba Marton, Nico Blodow, Mi-\nhai Dolha, and Michael Beetz. Towards 3d point cloud based\nobject maps for household environments. Robotics and Au-\ntonomous Systems, 56(11):927–941, 2008.\n[61] Uri Shaham, Kelly Stanton, Henry Li, Ronen Basri, Boaz\nNadler, and Yuval Kluger. Spectralnet: Spectral clustering\nusing deep neural networks. In International Conference on\nLearning Representations (ICLR), 2018.\n[62] Abhishek Sharma, Oliver Grau, and Mario Fritz. Vconv-dae:\nDeep volumetric shape learning without object labels.\nIn\nEuropean Conference on Computer Vision, pages 236–250.\nSpringer, 2016.\n[63] Yiru Shen, Chen Feng, Yaoqing Yang, and Dong Tian. Min-\ning point cloud local structures by kernel correlation and\ngraph pooling. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 4548–4557, June\n2018.\n[64] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: A simple\nway to prevent neural networks from overﬁtting. Journal of\nMachine Learning Research, 15:1929–1958, 2014.\n[65] Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji,\nEvangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz.\nSplatnet: Sparse lattice networks for point cloud processing.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2530–2539, June 2018.\n[66] Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A con-\ncise and provably informative multi-scale signature based\non heat diffusion. Computer Graphics Forum, 28(5):1383–\n1392, 2009.\n[67] Yongbin Sun, Yue Wang, Ziwei Liu, Joshua E. Siegel,\nand Sanjay Sarma.\nPoint cloud gan.\narXiv preprint\narXiv:1810.05591, 2018.\n[68] Yongbin Sun, Yue Wang, Ziwei Liu, Joshua E. Siegel,\nand Sanjay Sarma.\nPointgrow: Autoregressively learned\npoint cloud generation with self-attention.\narXiv preprint\narXiv:1810.05591, 2018.\n[69] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2015.\n[70] Gusi Te, Wei Hu, Amin Zheng, and Zongming Guo. Rgcnn:\nRegularized graph cnn for point cloud segmentation. In Pro-\nceedings of the 26th ACM International Conference on Mul-\ntimedia, pages 746–754. ACM, 2018.\n[71] Nitika Verma, Edmond Boyer, and Jakob Verbeek. Feast-\nnet: Feature-steered graph convolutions for 3d shape analy-\nsis. In The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2598–2606, June 2018.\n[72] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Or-\nder matters: Sequence to sequence for sets. In International\nConference on Learning Representation (ICLR), 2015.\n[73] Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio\nGuadarrama, and Kevin Murphy. Tracking emerges by col-\norizing videos. In The European Conference on Computer\nVision (ECCV), pages 391–408, September 2018.\n[74] Jayakorn Vongkulbhisal, Beat Irastorza Ugalde, Fernando\nDe la Torre, and Joo P. Costeira. Inverse composition dis-\ncriminative optimization for point cloud registration. In The\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 2993–3001, June 2018.\n[75] Chu Wang, Babak Samari, and Kaleem Siddiqi. Local spec-\ntral graph convolution for point set feature learning. In The\nEuropean Conference on Computer Vision (ECCV), pages\n52–66, September 2018.\n[76] Shenlong Wang,\nSimon Suo,\nWei-Chiu Ma,\nAndrei\nPokrovsky, and Raquel Urtasun. Deep parametric continu-\nous convolutional neural networks. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n2589–2597, June 2018.\n[77] Xiaolong Wang and Abhinav Gupta. Unsupervised learning\nof visual representations using videos. In The IEEE Interna-\ntional Conference on Computer Vision (ICCV), pages 2794–\n2802, December 2015.\n[78] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon.\nDynamic\ngraph cnn for learning on point clouds.\narXiv preprint\narXiv:1801.07829, 2018.\n[79] Donglai Wei, Joseph J. Lim, Andrew Zisserman, and\nWilliam T. Freeman. Learning and using the arrow of time.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 8052–8060, June 2018.\n[80] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of ob-\nject shapes via 3d generative-adversarial modeling. In D. D.\nLee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,\neditors, Advances in Neural Information Processing Systems\n29, pages 82–90. Curran Associates, Inc., 2016.\n[81] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets:\nA deep representation for volumetric shapes.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1912–1920, June 2015.\n[82] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets:\nA deep representation for volumetric shapes.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1912–1920, June 2015.\n[83] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised\ndeep embedding for clustering analysis. In Proceedings of\nThe 33rd International Conference on Machine Learning\n(ICML), volume 48, pages 478–487. PMLR, June 2016.\n[84] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao.\nSpidercnn: Deep learning on point sets with parameterized\nconvolutional ﬁlters. In The European Conference on Com-\nputer Vision (ECCV), pages 87–102, September 2018.\n[85] Yan Yan, Elisa Ricci, Gaowen Liu, and Nicu Sebe. Ego-\ncentric daily activity recognition via multitask clustering.\nIEEE Transactions on Image Processing, 24(10):2984–2995,\n2015.\n[86] Bo Yang, Xiao Fu, Nicholas D. Sidiropoulos, and Mingyi\nHong. Towards k-means-friendly spaces: Simultaneous deep\nlearning and clustering. In Proceedings of the 34th Interna-\ntional Conference on Machine Learning (ICML), volume 70,\npages 3861–3870. PMLR, August 2017.\n[87] Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsuper-\nvised learning of deep representations and image clusters.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5147–5156, June 2016.\n[88] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Fold-\ningnet: Point cloud auto-encoder via deep grid deformation.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 206–215, June 2018.\n[89] Xiaoqing Ye, Jiamao Li, Hexiao Huang, Liang Du, and Xi-\naolin Zhang. 3d recurrent neural networks with context fu-\nsion for point cloud semantic segmentation. In The Euro-\npean Conference on Computer Vision (ECCV), pages 403–\n417, September 2018.\n[90] Li Yi, Vladimir G. Kim, Duygu Ceylan, I-Chao Shen,\nMengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Shef-\nfer, and Leonidas Guibas.\nA scalable active framework\nfor region annotation in 3d shape collections. ACM Trans.\nGraph., 35(6):210:1–210:12, Nov. 2016.\n[91] Li Yi, Hao Su, Xingwen Guo, and Leonidas J. Guibas. Sync-\nspeccnn: Synchronized spectral cnn for 3d shape segmenta-\ntion. In The IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 2282–2290, July 2017.\n[92] Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and\nPheng-Ann Heng. Ec-net: an edge-aware point set consoli-\ndation network. In The European Conference on Computer\nVision (ECCV), pages 386–402, September 2018.\n[93] Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and\nPheng-Ann Heng. Pu-net: Point cloud upsampling network.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2790–2799, June 2018.\n[94] Wei Zeng and Theo Gevers. 3dcontextnet: K-d tree guided\nhierarchical learning of point clouds using local and global\ncontextual cues. In The European Conference on Computer\nVision (ECCV) Workshops, September 2018.\n[95] Yu Zhang and Qiang Yang. A survey on multi-task learning.\narXiv preprint arXiv:1707.08114, 2017.\n[96] Yongheng Zhao, Tolga Birdal, Haowen Deng, and Federico\nTombari. 3d point capsule networks. In The IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\nJune 2019.\n[97] Yin Zhou and Oncel Tuzel.\nVoxelnet: End-to-end learn-\ning for point cloud based 3d object detection. In The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 4490–4499, June 2018.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2019-10-18",
  "updated": "2019-10-18"
}