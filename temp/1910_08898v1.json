{
  "id": "http://arxiv.org/abs/1910.08898v1",
  "title": "Moving Indoor: Unsupervised Video Depth Learning in Challenging Environments",
  "authors": [
    "Junsheng Zhou",
    "Yuwang Wang",
    "Kaihuai Qin",
    "Wenjun Zeng"
  ],
  "abstract": "Recently unsupervised learning of depth from videos has made remarkable\nprogress and the results are comparable to fully supervised methods in outdoor\nscenes like KITTI. However, there still exist great challenges when directly\napplying this technology in indoor environments, e.g., large areas of\nnon-texture regions like white wall, more complex ego-motion of handheld\ncamera, transparent glasses and shiny objects. To overcome these problems, we\npropose a new optical-flow based training paradigm which reduces the difficulty\nof unsupervised learning by providing a clearer training target and handles the\nnon-texture regions. Our experimental evaluation demonstrates that the result\nof our method is comparable to fully supervised methods on the NYU Depth V2\nbenchmark. To the best of our knowledge, this is the first quantitative result\nof purely unsupervised learning method reported on indoor datasets.",
  "text": "Moving Indoor: Unsupervised Video Depth Learning in Challenging\nEnvironments\nJunsheng Zhou1,*, Yuwang Wang2, Kaihuai Qin1, and Wenjun Zeng2\n1Tsinghua University, Beijing, China\nzhoujs17@mails.tsinghua.edu.cn, qkh-dcs@mail.tsinghua.edu.cn\n2Microsoft Research, Beijing, China\nyuwwan, wezeng@microsoft.com\nAbstract\nRecently unsupervised learning of depth from videos has\nmade remarkable progress and the results are comparable\nto fully supervised methods in outdoor scenes like KITTI.\nHowever, there still exist great challenges when directly ap-\nplying this technology in indoor environments, e.g., large\nareas of non-texture regions like white wall, more com-\nplex ego-motion of handheld camera, transparent glasses\nand shiny objects. To overcome these problems, we pro-\npose a new optical-Ô¨Çow based training paradigm which re-\nduces the difÔ¨Åculty of unsupervised learning by providing a\nclearer training target and handles the non-texture regions.\nOur experimental evaluation demonstrates that the result of\nour method is comparable to fully supervised methods on\nthe NYU Depth V2 benchmark. To the best of our knowl-\nedge, this is the Ô¨Årst quantitative result of purely unsuper-\nvised learning method reported on indoor datasets.\n1. Introduction\nReconstructing the structures of scenes from RGB im-\nages has long been a popular research topic. Depth estima-\ntion is an important step to reconstruct the scenes. It is easy\nfor human to perceive the depth of scenes from a RGB im-\nage since we have prior knowledge about the sizes of com-\nmon objects and their layouts, but it is difÔ¨Åcult for computer\nto estimate accurate depth map from a single image due\nto scale ambiguity. Classical methods like Structure-from-\nMotion and Stereo Matching [14, 18] were proposed and\nachieved plausible results. In recent years, due to the surge\nof deep learning, convolutional neural network was intro-\nduced to predict depth from monocular RGB images for its\n*Work done as an intern at MSRA.\npowerful capacity of feature extraction,which leads to great\nprogress in this Ô¨Åeld. These methods [31, 11, 10], treat the\nneural networks as blackbox with strong Ô¨Åtting ability and\nuse the collected ground-truth annotation to supervise the\ntraining. However, these fully supervised methods are lim-\nited by the huge demand of training samples.\nRecently unsupervised depth learning [48, 17, 47] has\nbeen proposed and attracted more and more interests. This\nmethod is similar to traditional Structure-from-Motion that\nleverages the disparity information contained in videos to\nsupervise the networks‚Äô training. The key idea is novel view\nsynthesis based on simultaneously estimated depth of scene\nand ego-motion of camera. The appearance difference be-\ntween the synthesized view and real view is used as super-\nvisory signal for the entire training pipeline. These unsu-\npervised methods do not need ground-truth annotation and\nachieve remarkable results on driving scenes like KITTI and\nCityscapes.\nHowever, there still exist great challenges when directly\napplying this technology in indoor environments. In our\nexperiments, we observed that the same model and same\ntraining setting, which is able to achieve state-of-the-art per-\nformance on KITTI, soon collapse when training on indoor\ndatasets NYU V2[39] and Scannet [8]. The reason is that\nindoor environments are more complicated than urban driv-\ning scenes. The main problems can be summarized below:\n1) Large areas of non-texture regions. Unlike fully su-\npervised methods in which each pixel has ground-truth su-\npervision, the supervisory signal of unsupervised learning\nonly comes from the appearance difference between images\nthemselves. Non-texture regions seriously hinder the train-\ning since in these regions the photometric loss is always\nclose to zero. However we observed that there are consid-\nerable amount of images which have more than 50% non-\ntexture areas in indoor datasets. White wall and carpets are\narXiv:1910.08898v1  [cs.CV]  20 Oct 2019\nfairly common non-texture objects.\n2) More complex ego-motion of handheld camera. Gen-\nerally indoor datasets are collected by handheld cameras,\nwhich means that the ego-motion of consecutive frames is\nmore complex than driving scenes where the cars are mostly\njust moving forward. Especially, we can not infer the depth\nof scene theoretically from sequences of pure rotation. The\nexistence of large amount of training samples with pure ro-\ntation will overwhelm the entire training process.\nThese reveal that the existing training pipeline needs\nto be revamped to be applied in more general scenes. In\nthis paper, we propose a new optical-Ô¨Çow based training\nparadigm, which focuses on the most important part, i.e.,\nthe supervisory signal of unsupervised depth learning. This\nnew pipeline uses the optical Ô¨Çow results generated by a\nÔ¨Çow estimation network as supervision and it is easier for\nthe training to converge.\nThe key component is a spe-\ncially designed network which is responsible for estimating\nthe optical Ô¨Çow between consecutive frames in a sparse-to-\ndense propagation manner. This unsupervised Ô¨Çow network\nis able to handle the non-texture regions and produce plau-\nsible optical Ô¨Çow results. Then this network can be used\nas a teacher simultaneously training the DepthNet and the\nPoseNet. We also improve the existing PoseNet and make\nit easier to learn the complex ego-motion of handheld cam-\nera.\nOur evaluation on NYU Depth V2 benchmark demon-\nstrates that the result of our method is comparable to fully\nsupervised methods.\n2. Related Work\nSupervised Depth Estimation\nEstimating the depth\nfrom a single image has been long studied. Recently due\nto the success of deep learning [27, 28], many networks for\ndepth estimation have been proposed [11, 10, 29, 31, 13,\n24]. Eigen et al. [11] applied multi-scale networks which\nÔ¨Årst estimate coarse depth by a coarse-scale network and\nreÔ¨Åne it by another network. CRF is also introduced to this\ntask and used as a postprocessing module in a model [45].\nFu et al. [13] treated the depth estimation as a classiÔ¨Åcation\nproblem instead of a regression problem. All these methods\nhave powerful capacity and achieve very good performance\non both indoor datasets like NYU V2 [39], Scannet [8] and\noutdoor datasets like KITTI [15], Make3D [37, 38]. How-\never these methods rely on large-scale datasets with depth\nlabels.\nUnsupervised Depth Learning\nTo get rid of the need\nof ground-truth depth annotation, unsupervised depth learn-\ning methods have been proposed. These methods leverage\neither stereo images [17, 46] or videos [48, 47] as training\ndata. Godard et al. [17] Ô¨Årst proposed to use the left-right\nconsistency of stereo images to train a depth estimation net-\nwork. Zhou et al. [48] applied two networks which jointly\nestimate the depth and ego-motion of camera to learn the\ndepth from videos.\nWang et al. [42] discarded the pose\nnetwork and directly computed the pose by visual odom-\netry method. Casser et al. [3] leveraged the additional in-\nstance segmentation masks to model the dynamic objects.\nThese methods have achieved tremendous success in out-\ndoor scenes like KITTI [15] and CityScapes [7]. However,\nonly some of these works demonstrated the sample pre-\ndicted results of indoor scenes using the networks trained\non KITTI, and there is no quantitative result reported on\ntypical indoor datasets. In our experiments, we also faced\ngreat challenges when directly using the previous methods\nto train on indoor scenes.\nUnsupervised Optical Flow Learning\nBased on the\nsame photometric supervisory signal as unsupervised depth\nlearning, unsupervised optical Ô¨Çow learning methods have\nalso been proposed.\nYu et al. [21] and Ren et al. [34]\nproposed FlowNet-based [19] architecture for unsupervised\noptical Ô¨Çow learning. Meister et al. [33] proposed a Bidi-\nrectional Census Loss to handle occlusion/disocclusion. Al-\nthough these methods perform well on synthetic datasets,\nnon-texture regions, dynamic objects and occlusion are still\nintractable problems.\nOur new Ô¨Çow network is differ-\nent from previous architecture, which leverages the sparse\nÔ¨Çow seeds generated by traditional feature matching meth-\nods and progressively propagates them to the entire image.\nNon-texture regions are handled well by this mean.\n3. Approach\n3.1. Overall Pipeline\nAs shown in Figure 1 (a), previous unsupervised depth\nlearning pipeline generally consists of two modules: Depth-\nNet and PoseNet. During training, both networks simulta-\nneously estimate the depth of the scene and the ego-motion\nof the camera. Once the depth map of RGB image is es-\ntimated, we can backproject the pixels on image plane to\n3D coordinates with known camera intrinsic. Then with the\nestimated motion the 3D point cloud can be transformed to\nanother view. This view transformation can be formulated\nas below:\nps‚àí‚Üít = KTt‚àí‚ÜísDt(pt)K‚àí1pt\n(1)\nwhere K denotes the camera intrinsic, Tt‚àí‚Üís denotes the\nestimated transformation matrix from view t to view s, Dt\ndenotes the estimated depth, pt and ps‚àí‚Üít denote the homo-\ngeneous coordinates of a pixel in view t and view s respec-\ntively. Then we can obtain the 2D rigid Ô¨Çow from view t to\nview s:\nft‚àí‚Üís(pt) = ps‚àí‚Üít ‚àípt\n(2)\nOnce the rigid Ô¨Çow between two views is estimated, we can\nsynthesize the image Is‚àí‚Üít by differentiable inverse warp-\nDepthNet \nPoseNet \nùë∑ùíêùíîùíÜùíï‚Üíùíï‚àíùüè \nùë∑ùíêùíîùíÜùíï‚Üíùíï+ùüè \n(6 DoF) \n \nInverse \nWarping \nSynthesized  \nTarget Views \n Target Views \nComparing \nSupervisory Signal \nRigid Flow \nDepth \n Target View \n Target View & \nSource Views \n Source Views \n(a) \nDepthNet \nPoseNet \nùë∑ùíêùíîùíÜùíï‚Üíùíï‚àíùüè \nùë∑ùíêùíîùíÜùíï‚Üíùíï+ùüè \n(6 DoF) \n \nComparing \nRigid Flow \nOptical Flow \nSF-Net \nOptical Flow \nSupervisory Signal \n Target View \nDepth \n Target View & \nSource Views \n(b) \nFigure 1. Overview of previous pipeline (a) and our pipeline (b). The supervisory signal of previous pipeline is based on appearance\nmatching of two images, which is unstable and suffers from non-texture regions. We use the optical Ô¨Çow results generated by our Ô¨Çow\nestimation network SF-Net as supervision and modify the input of PoseNet.\ning [20] from source view. The appearance difference be-\ntween the synthesized image Is‚àí‚Üít and the real image It is\nused as supervisory signal for the entire pipeline.\nAs shown in the red frame in Figure 1 (a), the photomet-\nric loss computed based on the synthesized image and the\nreal image supervises the training of both networks. This is\nequivalent to Ô¨Ånding the best match point in source view\nfor each pixel in target view, which is similar to Stereo\nMatching. Once the rigid Ô¨Çow is predicted perfectly , the\nsynthesized image matches the real image completely (if\nthere is no occlusion and dynamic object). However, this\nappearance-based supervisory signal is indirect and suscep-\ntible to non-texture regions.\nMore speciÔ¨Åcally, for each\npixel there is no explicit target position that it should match\nin another view. The optimization target of this proxy ob-\njective is just to minimize the appearance difference be-\ntween two images. This paradigm is difÔ¨Åcult to work in\nindoor environments since there is a lot of non-texture stuff\nin our daily scenes, where the appearance difference is al-\nways close to zero, which can not provide valid and strong\nsignal for the training.\nTo overcome the problem, we Ô¨Årst look back to the entire\npipeline. It can be separated into two stages: 1) composing\nthe rigid Ô¨Çow based on jointly estimated depth and pose.\n2) using the rigid Ô¨Çow to synthesize novel image and com-\nputing the loss (corresponding to the left and right parts in\nFigure 1 (a)). The objective function can be brieÔ¨Çy written\nas:\nL = |It ‚àíIs‚àí‚Üít|\n(3)\nwhere It denotes target view image and Is‚àí‚Üít denotes syn-\nthesized image.\nOur main contribution is that instead of using this indi-\nrect proxy supervision, we provide an explicit optical Ô¨Çow\ntarget to supervise the estimated rigid Ô¨Çow as shown in Fig-\nure 1 (b). The optical Ô¨Çow target is obtained by a sparse-to-\ndense Ô¨Çow estimation network (named hereafter as SF-Net),\nwhich will be elaborated on in the next subsection. There-\nfore the objective function is modiÔ¨Åed as:\nL = |ft‚àí‚Üís(pt) ‚àíf ‚Ä≤\nt‚àí‚Üís(pt)|\n(4)\nwhere f ‚Ä≤\nt‚àí‚Üís(pt) denotes the optical Ô¨Çow result from SF-\nNet. This modiÔ¨Åcation is signiÔ¨Åcant since it transforms the\nunsupervised learning to ‚Äúfully supervised‚Äù learning and\nreduces the difÔ¨Åculty of training.\nThe words ‚Äúfully su-\n(a) One of input images \n(b) Sparse seeds \n(c) Outlier \n(d) 1K iterations \n(e) 40K iterations \n(f) 400K iterations \nFigure 2. Illustration of our SF-Net. (a) One of input images. (b)\nSparse seeds generated by SURF [1]. The sizes of sparse points\nhave been enlarged for better visualization. (c) The green frame\nin (b), which represents an outlier (blue point) in sparse seeds.\n(d)(e)(f) Visualization of a training sample at different stages. Our\nSF-Net works in a sparse-to-dense propagation manner which pro-\ngressively propagates the sparse Ô¨Çow from textured regions to non-\ntexture regions. The negative effects induced by outliers are also\nsuppressed during training.\npervised‚Äù here only mean more explicit supervisory signal\nwhile this signal is still obtained unsupervisedly. Photo-\nmetric loss is not able to penalize the incorrect prediction in\nnon-texture regions, i.e., the supervisory signal fails in those\nareas (common in indoor scenes). By contrast, the optical\nÔ¨Çow generated by SF-Net provides unique target for each\npixel, which is a very strong supervisory signal. Our ex-\nperiments show that this new pipeline is able to handle the\nindoor environments with large areas of non-texture regions\nand achieve plausible result.\n3.2. SF-Net\nThe core of this new training paradigm is the part of SF-\nNet. Before the introduction of the SF-Net, we Ô¨Årst explain\nthe principle of unsupervised optical Ô¨Çow learning. The key\nidea is novel view synthesis, which is the same as unsuper-\nvised depth learning. The network predicts a Ô¨Çow map for\nimage It in target view and use this dense Ô¨Çow to synthesize\nthe image Is‚àí‚Üít by differentiable inverse warping [20] from\nsource view. Then the appearance difference between the\nsynthesized image Is‚àí‚Üít and the real target image It is used\nas supervisory signal for the training.\nHowever, since both unsupervised depth learning and\noptical Ô¨Çow learning leverage the same proxy supervisory\nsignal, they also suffer from the same problems.\nPrevi-\nous methods usually impose smoothness constraint on the\nÔ¨Çow prediction. The reason is that it is easy for the network\nto produce correct result in textured region like corner and\nborder, we expect this correct prediction to guide its neigh-\nborhood where the prediction is incorrect. But in indoor\nenvironments where non-texture regions cover large areas,\nthe correct prediction may be overwhelmed by the incorrect\npredictions on the contrary.\nIn order to overcome this problem, we propose an ac-\ntive propagation approach. Instead of only using the weak\nsmoothness constraint, we actively propagate the sparse ini-\ntial seeds at textured regions to the entire image as shown\nin Figure 2. The key idea is that we do not need to gener-\nate the dense Ô¨Çow maps from scratch since we can leverage\nthe traditional feature matching algorithms like SURF [1]\nto generate sparse corresponding points. The displacement\nof the corresponding points (Figure 2 (b)) are considered\nas initial Ô¨Çow seeds and propagated to the entire region. In\naddition, the network is able to suppress the mismatches\nexisting in the corresponding points during training. In this\nway, plausible results can also be generated in the interior\nof non-texture regions.\nAs for the method of propagation, we adopt the archi-\ntecture of CSPN proposed by Cheng et al.[5, 6] for its ef-\nÔ¨Åciency. This method diffuses the information of the cen-\nter pixel to its eight neighborhood iteratively in the form\nof convolution. Our adapted network is a very simple and\ncommon encoder-decoder architecture, which takes stacked\nRGB images and sparse seeds as input and outputs two re-\nsults: one is the coarse optical Ô¨Çow F0 and another is the\ntransformation kernels ÀÜKi,j with k2 ‚àí1 channels, where\nk denotes the kernel size. Then the coarse optical Ô¨Çow is\nreÔ¨Åned iteratively as in [5, 6]:\nFi,j,t+1 =\n(k‚àí1)/2\nX\na,b=‚àí(k‚àí1)/2\nKi,j(a, b) ‚äôFi‚àía,j‚àíb,t\n(5)\nwhere ‚äôdenotes element-wise product and:\nKi,j(a, b) =\nÀÜKi,j(a, b)\nP\na,bÃ∏=0 | ÀÜKi,j(a, b)|\n(6)\nKi,j(0, 0) = 1 ‚àí\nX\na,bÃ∏=0\nKi,j(a, b)\n(7)\nBefore each propagating operation, we Ô¨Åx the sparse seeds\nin order to guarantee that our propagated Ô¨Çows have the ex-\nact same value at those valid pixels in the sparse Ô¨Çow map:\nFi,j,t+1 = (1 ‚àími,j)Fi,j,t+1 + mi,jF s\ni,j\n(8)\nwhere F s\ni,j denotes the sparse Ô¨Çow with empty positions\nÔ¨Ålled with zero, mi,j is an indicator for the availability of\nsparse Ô¨Çow at (i, j). In our training setting, the kernel size\nk is set to 3 and the max steps of iterations are 16.\n3.3. PoseNet\nThe PoseNet is also an important component of the un-\nsupervised depth learning pipeline, which is an application\nof deep learning in visual odometry and responsible for es-\ntimating the pose in 6 degrees of freedom (DoF) between\ntwo images. In driving scenes like KITTI the poses are\nfairly simple and the cars in most images are just driving\nforward. But in indoor environments, the images are typ-\nically collected by handheld cameras, which means more\ncomplex ego-motion and raises the difÔ¨Åculty of learning for\nPoseNet.\nThere are also some approaches proposed to get rid of\nthe PoseNet. For instance, Mahjourian et al. [32] used It-\nerative Closest Point (ICP) [2, 4, 36] to compute a trans-\nformation that minimizes point-to-point distances between\ncorresponding points. Wang et al. [42] used direct visual\nodometry (DVO) [40] to obtain camera pose from predicted\ndepth and images. But in our experiments these network-\nfree methods collapsed during training on indoor datasets\nsince they rely on good initial depth with low level of noise.\nCollapse means all the predictions of depth converge to a\nconstant value. The estimation of poses can also be consid-\nered as a Perspective-n-Point (PNP) problem because we\nhave predicted depth and dense matching simultaneously.\nWe tried to implement the EPNP [25] algorithm on GPU\nto directly compute camera pose from predicted depth and\nÔ¨Çow in real time but the training collapsed again. These tri-\nals indicate that the use of PoseNet is necessary since the\nprediction of PoseNet is based on the entire dataset statisti-\ncally and it is not likely to be dominated by individual train-\ning samples.\nTherefore we rethink the working principle of the\nPoseNet. How does this blackbox estimate the pose from\nstacked RGB images? A reasonable speculation is that it\nÔ¨Årst Ô¨Ånds corresponding points in two images internally and\ninfers the pose from the displacements of the corresponding\npoints according to an unknown rule. However, there is no\nneed for the PoseNet to match the pixels again since we al-\nready have the optical Ô¨Çow result, i.e., the dense matching.\nSo we propose to use the Ô¨Çow result produced by SF-Net as\nthe input of PoseNet instead of RGB images.\nThis modiÔ¨Åcation is equivalent to separate the unsuper-\nvised pose estimation into two stages: estimating the op-\ntical Ô¨Çow between two frames Ô¨Årst and then inferring the\npose based on the Ô¨Çow. It is unnecessary to do this separa-\ntion if there exist ground-truth labels. But it enhances the\ninterpretability of the PoseNet and reduces the difÔ¨Åculty of\nunsupervised learning. The signiÔ¨Åcant improvement in the\nexperimental results reported in Section 4.2 also supports\nour speculation.\n3.4. Pure Rotation\nThere still exists a problem that should not be ignored:\npure rotation. This problem does not exist in driving scenes\nsince previous methods generally remove static frames dur-\ning preprocessing but is common in indoor datasets. Theo-\nretically image pairs with pure rotation do not contain depth\ninformation and are harmful to the training. It is crucial to\nÔ¨Årst Ô¨Ålter out the training samples with pure rotation. It\ncan be derived that the relationship between corresponding\npoints with pure rotation can be Ô¨Åtted by a homography ma-\ntrix H:\nH = KRK‚àí1\n(9)\nwhere K denotes the camera intrinsic and R denotes the\nrotation matrix.\nSo for each image pair, we use the dense optical Ô¨Çow\ngenerated by SF-Net to compute its homography matrix\nwith RANSAC [12]. If the ratio of outliers is lower than\na pre-set threshold (20% in our setting, which means more\nthan 80% pixels can be Ô¨Åtted by a homography matrix), we\nconsider the pose of this image pair as pure rotation and dis-\ncard it. After this Ô¨Åltering process, about 30% of the images\nin NYU V2 are discarded.\n3.5. Loss Function\n3.5.1\nLoss Function of SF-Net\nFor the training of SF-Net, we use photometric loss and\nsmoothness loss.\nPhotometric Loss\nThis loss function computes the ap-\npearance difference between two images.\nWe adopt the\nsame setting as [17] which combines L1 loss and the Struc-\ntural Similarity (SSIM) [44]. Besides per-pixel minimum\ntrick proposed by [16] is also adopted which is aimed to\nhandle occlusion/disocclusion. It can be written as:\nLph =\nX\np\nmin\ns (Œ±SSIM(It, Is‚àí‚Üít)+(1‚àíŒ±)|It(p)‚àíIs‚àí‚Üít(p)|)\n(10)\nwhere p indexes over pixel coordinates, s denotes the index\nof source views, Œ± is set to 0.5.\nSmoothness Loss\nAlthough SF-Net adopts a sparse-\nto-dense training scheme, we also use edge-aware Ô¨Çow\nsmoothness loss to suppress the mismatches in initial seeds:\nLsmooth =\nX\np\n|‚àáF(p)| ¬∑\n\u0010\ne‚àí|‚àáI(p)|\u0011T\n(11)\nwhere ‚àáis the vector differential operator, and T denotes\nthe transpose of image gradient weighting. So the total loss\nof SF-Net is:\nL = Œª1Lph + Œª2Lsmooth\n(12)\n3.5.2\nLoss Function of DepthNet and PoseNet\nFor the training of DepthNet and PoseNet, the loss func-\ntion consists of four terms.\nInput \nGeoNet \nOurs \nGT \nFigure 3. Qualitative comparison between GeoNet [47], ours and ground-truth depth. We directly use the original code of GeoNet, which\nis able to achieve state-of-the-art performance on KITTI, but collapsed during training on NYU V2 [39].\nMethod\nSupervision\nAccuracy metric\nError metric\nŒ¥ < 1.25\nŒ¥ < 1.252\nŒ¥ < 1.253\nrel\nlog10\nrms\nMake3D [38]\n‚úì\n0.447\n0.745\n0.897\n0.349\n-\n1.214\nDepth Transfer [22]\n‚úì\n-\n-\n-\n0.35\n0.131\n1.2\nLiu et al. [31]\n‚úì\n-\n-\n-\n0.335\n0.127\n1.06\nLadicky et al. [23]\n‚úì\n0.542\n0.829\n0.941\n-\n-\n-\nLi et al. [26]\n‚úì\n0.621\n0.886\n0.968\n0.232\n0.094\n0.821\nWang et al. [43]\n‚úì\n0.605\n0.890\n0.970\n0.220\n-\n0.824\nRoy et al. [35]\n‚úì\n-\n-\n-\n0.187\n-\n0.744\nLiu et al. [30]\n‚úì\n0.650\n0.906\n0.976\n0.213\n0.087\n0.759\nLi et al. [29]\n‚úì\n0.788\n0.958\n0.991\n0.143\n0.063\n0.635\nMS-CRF [45]\n‚úì\n0.811\n0.954\n0.987\n0.121\n0.052\n0.586\nDORN [13]\n‚úì\n0.828\n0.965\n0.992\n0.115\n0.051\n0.509\nOurs (baseline‚Ä†)\n√ó\n0.511\n0.779\n0.904\n0.331\n0.127\n1.000\nOurs\n√ó\n0.674\n0.900\n0.968\n0.208\n0.086\n0.712\nTable 1. Comparison to existing methods on NYU V2 [39]. ‚Ä† denotes the model that collapsed during training. Since we adopt the scale\nnormalization proposed by [42] to avoid the shrinking of depth, all the predictions are normalized to 1 meter when the model collapses.\nAll the other methods in the table are fully supervised by depth annotations.\nRigid Flow Loss\nThis term directly uses the optical\nÔ¨Çow result produced by SF-Net as supervision of the synth-\nsized rigid Ô¨Çow. We use berHu [24] norm || ¬∑ ||Œ¥ to measure\nthe deviation:\nLflow = ||ft‚àí‚Üís(pt) ‚àíf ‚Ä≤\nt‚àí‚Üís(pt)||Œ¥\n(13)\nThe other two terms Ls depth and Ls normal are smooth-\nness constraints and are similar to Equation 11. But they are\nimposed on the predicted depth and corresponding normal\nMethod\nSupervision\nFiltering of\nAccuracy metric\nError metric\nPure Rotation\nd1\nd2\nd3\nrel\nlog10\nrms\nDepthNet + R-PoseNet ‚Ä†\nRGB\n√ó\n0.511\n0.779\n0.904\n0.331\n0.127\n1.000\nDepthNet + R-PoseNet ‚Ä†\nRGB\n‚úì\n0.511\n0.779\n0.904\n0.331\n0.127\n1.000\nDepthNet + F-PoseNet ‚Ä†\nFlow (w/o propagation)\n√ó\n0.511\n0.779\n0.904\n0.331\n0.127\n1.000\nDepthNet + F-PoseNet\nFlow (w/o propagation)\n‚úì\n0.596\n0.862\n0.951\n0.257\n0.102\n0.841\nDepthNet + R-PoseNet\nFlow (w/ propagation)\n‚úì\n0.578\n0.836\n0.938\n0.273\n0.108\n0.910\nDepthNet + F-PoseNet\nFlow (w/ propagation)\n‚úì\n0.674\n0.900\n0.968\n0.208\n0.086\n0.712\nTable 2. Evaluation of each component on NYU V2‚Äôs test split. ‚Ä† denotes the model that collapsed during training. R-PoseNet denotes the\nPoseNet with RGB images input and F-PoseNet denotes the PoseNet with Ô¨Çow input. Œ¥ < 1.25, Œ¥ < 1.252, Œ¥ < 1.253 are abbreviated to\nd1, d2, d3 due to space limitation.\nMethod\nDataset\nAccuracy\nError\nd1\nrel\nsq rel rms\nOurs(baseline)‚Ä†\nS\n0.631\n0.238 0.190 0.570\nOurs(F-PoseNet)‚Ä†\nS\n0.631\n0.238 0.190 0.570\nOurs(F-Sup+R-PoseNet)\nS\n0.682\n0.206 0.134 0.491\nOurs(F-Sup+F-PoseNet)\nS\n0.710\n0.190 0.124 0.465\nOurs(baseline)\nK\n0.833\n0.132 1.016 5.506\nOurs(F-Sup)\nK\n0.837\n0.136 1.110 5.327\nOurs(F-PoseNet)\nK\n0.836\n0.130 1.001 5.294\nTable 3. Evaluation of depth on Scannet and KITTI datasets.\nK:KITTI, S:Scannet, ‚Ä†:collapse, F-sup:Ô¨Çow supervision.\n(a) \n(b)  \n(c) \n(d) \nFigure 4. (a) A typical image with large areas of non-texture re-\ngions. (b) GT Ô¨Çow. (c) Flow without propagation. (d) Flow with\npropagation. Previous unsupervised Ô¨Çow learning methods only\nimpose smoothness constraint on non-texture regions, which are\nprone to consider these areas as static (red circle in (c)).\nwhich is computed from the depth. The last term Lph is\nthe same as Equation 10. So the total loss of DepthNet and\nPoseNet is\nL = Œª1Lflow + Œª2Lph + Œª3Ls depth + Œª4Ls normal (14)\nMethod\nw/o Propagation\nw/ Propagation\nPWC-Net\nEPE\n7.409\n3.602\n3.279\nTable 4. Average endpoint error (EPE) Ô¨Çow results on NYU vali-\ndation split. The result of PWC-Net is directly generated by PWC-\nNet [41] pretrained on FlyingChairs [9] with supervision but with-\nout Ô¨Ånetuning on NYU V2. The result obtained by SF-Net with\npropagation is close to supervised result obtained by PWC-Net.\n4. Experiments\n4.1. Comparisons on NYU V2 Dataset\nThe NYU Depth v2 [39] dataset contains 582 indoor\nvideo scenes taken with a Microsoft Kinect camera and the\ntraining split contains 283 scenes (about 230K images). To\ntrain the DepthNet and PoseNet, we Ô¨Årst use the method\nmentioned in Section 3.4 to Ô¨Ålter out the image pairs with\npure rotation. About 30% images are discarded and Ô¨Ånally\nwe use about 180K images for training. We Ô¨Åx the length\nof training image sequences to be 3 frames for all the three\nnetworks, and treat the central frame as the target view and\nthe ¬±10 frames as the source views. We only use raw RGB\nimage sequences for training and the images are resized to\n192 √ó 256. Visualization is shown in Figure 3 and quanti-\ntative evaluation is reported in Table 1. The baseline model\nonly consists of DepthNet and PoseNet with RGB images\ninput and does not use Ô¨Çow as supervision.\n4.2. Ablation Study\nIn this subsection, we individually evaluate the effects of\nthe four components in our pipeline: 1) The Ô¨Çow supervi-\nsion. 2) The Ô¨Åltering of pure rotation. 3) The propagation\ndesign for the SF-Net. 4) The input of the PoseNet.\nAs shown in Table 2, the results of rows 3,6 and rows\n4,5 indicate that the Ô¨Çow supervision and Ô¨Åltering of pure\nrotation are indispensable in order to get rid of the collapse\nphenomenon. It also shows that both the F-PoseNet and\nthe propagation of SF-Net improve the performance signif-\nicantly. The result without propagation means the SF-Net\ntakes RGB images as input and only output optical Ô¨Çow,\nOne of \nInput Images \nSparse \nSeeds \nPredicted \nFlow \nGT* \nFigure 5. Visualization of the results generated by SF-Net on NYU V2. NYU depth dataset does not contain optical Ô¨Çow annotations.\n* means that the Ô¨Çow results are computed with ground-truth depth by the method mentioned in Section 4.2. Our SF-Net handles the\nnon-texture regions (green circles) well. The sizes of sparse points have been enlarged for better visualization.\nand the other parts in the network remains the same.\nNYU depth dataset does not contain optical Ô¨Çow annota-\ntions. But we can compute the rigid Ô¨Çow from the ground-\ntruth depth. We Ô¨Årst use the EPNP [25] algorithm to solve\nthe pose between two images with depth annotations and\nsparse matchings. The sparse matchings are also obtained\nby SURF [1]. Then the estimated pose and ground-truth\ndepth can be used to compose the rigid Ô¨Çow. We consider\nthese estimated rigid Ô¨Çows as ground-truth Ô¨Çow labels.\nIn order to quantiÔ¨Åcationally evaluate the performance of\nSF-Net, we randomly choose 1000 image pairs as validation\nset and exclude them during training. The quantitative result\nis shown in Table 4 and qualitative comparison is shown in\nFigure 4. With the help of the propagation of sparse seeds,\nSF-Net handles the non-texture regions well and achieves\nreasonably good results as shown in Figure 5.\n4.3. Evaluations on Scannet and KITTI\nAs shown in Table 3, we also evaluate our method on\nScannet and KITTI datasets.\nSince Scannet is a large\ndataset, we train and test our model on a subset of Scan-\nnet for efÔ¨Åciency, which contains 40 scenes and about 70K\nimages. The Ô¨Çow supervision is proposed to address the non\ntexture problems in indoor scenarios, but has its own limi-\ntation. When applied on KITTI, the accuracy of the Ô¨Çow\nbecomes the bottleneck. However, F-PoseNet still has gain.\n5. Conclusion\nIn this paper, we propose a new unsupervised depth\nlearning framework which reduces the difÔ¨Åculty for the net-\nwork to learn and is able to work indoor. We also propose a\nsparse-to-dense unsupervised Ô¨Çow estimation network that\naddresses the intractable non-texture area problem. More\nimportantly, the results of our approach demonstrate that\nthe technology of unsupervised depth learning is not only\nable to work in driving scenes but also has the capacity to\nbe applied in more general scenes. This is an important step\ntowards exploring the innumerable videos available on the\nInternet for the training of deep learning.\nLimitation\nAlthough our approach is able to handle\nthe non-texture regions in most cases, it relies on the prop-\nagation of sparse seeds. In some special situations where\nnon-texture areas are extremely large and the identiÔ¨Åed cor-\nresponding key points are very sparse, it is difÔ¨Åcult for our\nmodel to predict correct results.\nIn addition, the gener-\nated Ô¨Çows usually have blurry boundary. When these blurry\nÔ¨Çows are used to supervise the training of DepthNet, the ob-\ntained depth maps have blurrier boundary. This also limits\nthe performance of our model. We will address these prob-\nlems in the future work.\nReferences\n[1] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool.\nSurf:\nSpeeded up robust features. In European conference on com-\nputer vision, pages 404‚Äì417. Springer, 2006.\n[2] Paul J Besl and Neil D McKay. Method for registration of\n3-d shapes. In Sensor Fusion IV: Control Paradigms and\nData Structures, volume 1611, pages 586‚Äì607. International\nSociety for Optics and Photonics, 1992.\n[3] Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia\nAngelova. Unsupervised learning of depth and ego-motion:\nA structured approach. In Thirty-Third AAAI Conference on\nArtiÔ¨Åcial Intelligence (AAAI-19), 2019.\n[4] Yang Chen and G¬¥erard Medioni. Object modelling by regis-\ntration of multiple range images. Image and vision comput-\ning, 10(3):145‚Äì155, 1992.\n[5] Xinjing Cheng, Peng Wang, and Ruigang Yang. Depth es-\ntimation via afÔ¨Ånity learned with convolutional spatial prop-\nagation network. In European Conference on Computer Vi-\nsion, pages 108‚Äì125. Springer, Cham, 2018.\n[6] Xinjing Cheng, Peng Wang, and Ruigang Yang. Learning\ndepth with convolutional spatial propagation network. 2018.\n[7] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld,\nMarkus Enzweiler,\nRodrigo Benenson,\nUwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene understanding. In CVPR,\npages 3213‚Äì3223, 2016.\n[8] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie√üner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes.\nIn\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 5828‚Äì5839, 2017.\n[9] A. Dosovitskiy, P. Fischer, E. Ilg, P. H¬®ausser, C. Hazƒ±rbas¬∏, V.\nGolkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet:\nLearning optical Ô¨Çow with convolutional networks. In IEEE\nInternational Conference on Computer Vision (ICCV), 2015.\n[10] David Eigen and Rob Fergus. Predicting depth, surface nor-\nmals and semantic labels with a common multi-scale convo-\nlutional architecture. In ICCV, 2015.\n[11] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map\nprediction from a single image using a multi-scale deep net-\nwork. In NIPS, 2014.\n[12] Martin A Fischler and Robert C Bolles.\nRandom sample\nconsensus: a paradigm for model Ô¨Åtting with applications to\nimage analysis and automated cartography. Communications\nof the ACM, 24(6):381‚Äì395, 1981.\n[13] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-\nmanghelich, and Dacheng Tao. Deep ordinal regression net-\nwork for monocular depth estimation. In CVPR, 2018.\n[14] Yasutaka Furukawa, Carlos Hern¬¥andez, et al.\nMulti-view\nstereo: A tutorial. Foundations and Trends¬Æ in Computer\nGraphics and Vision, 9(1-2):1‚Äì148, 2015.\n[15] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In CVPR, 2012.\n[16] Cl¬¥ement Godard, Oisin Mac Aodha, and Gabriel Brostow.\nDigging into self-supervised monocular depth estimation.\narXiv preprint arXiv:1806.01260, 2018.\n[17] Cl¬¥ement Godard, Oisin Mac Aodha, and Gabriel J Bros-\ntow.\nUnsupervised monocular depth estimation with left-\nright consistency. In CVPR, 2017.\n[18] Richard Hartley and Andrew Zisserman. Multiple view ge-\nometry in computer vision.\nCambridge university press,\n2003.\n[19] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,\nAlexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-\ntion of optical Ô¨Çow estimation with deep networks. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 2462‚Äì2470, 2017.\n[20] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. In NIPS, 2015.\n[21] J Yu Jason, Adam W Harley, and Konstantinos G Derpanis.\nBack to basics: Unsupervised learning of optical Ô¨Çow via\nbrightness constancy and motion smoothness. In European\nConference on Computer Vision, pages 3‚Äì10. Springer, 2016.\n[22] Kevin Karsch, Ce Liu, and Sing Bing Kang. Depth transfer:\nDepth extraction from video using non-parametric sampling.\nPAMI, 2014.\n[23] Lubor Ladicky, Jianbo Shi, and Marc Pollefeys.\nPulling\nthings out of perspective. In CVPR, 2014.\n[24] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-\nerico Tombari, and Nassir Navab. Deeper depth prediction\nwith fully convolutional residual networks.\nIn 3D Vision\n(3DV), 2016.\n[25] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua.\nEpnp: An accurate o (n) solution to the pnp problem. Inter-\nnational journal of computer vision, 81(2):155, 2009.\n[26] Bo Li, Chunhua Shen, Yuchao Dai, Anton Van Den Hen-\ngel, and Mingyi He. Depth and surface normal estimation\nfrom monocular images using regression on deep features\nand hierarchical crfs. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 1119‚Äì\n1127, 2015.\n[27] Hongyang Li, Bo Dai, Shaoshuai Shi, Wanli Ouyang, and\nXiaogang Wang. Feature Intertwiner for Object Detection.\nIn ICLR, 2019.\n[28] Hongyang Li, David Eigen, Samuel Dodge, Matt Zeiler, and\nXiaogang Wang. Finding Task-relevant Features for Few-\nshot Learning by Category Traversal. In CVPR, 2019.\n[29] Jun Li, Reinhard Klein, and Angela Yao. A two-streamed\nnetwork for estimating Ô¨Åne-scaled depth maps from single\nrgb images. In Proceedings of the IEEE International Con-\nference on Computer Vision, pages 3372‚Äì3380, 2017.\n[30] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid.\nLearning depth from single monocular images using deep\nconvolutional neural Ô¨Åelds.\nIEEE transactions on pattern\nanalysis and machine intelligence, 38(10):2024‚Äì2039, 2016.\n[31] Miaomiao Liu,\nMathieu Salzmann,\nand Xuming He.\nDiscrete-continuous depth estimation from a single image.\nIn CVPR, 2014.\n[32] Reza Mahjourian, Martin Wicke, and Anelia Angelova. Un-\nsupervised learning of depth and ego-motion from monocu-\nlar video using 3d geometric constraints. In CVPR, 2018.\n[33] Simon Meister, Junhwa Hur, and Stefan Roth. UnÔ¨Çow: Un-\nsupervised learning of optical Ô¨Çow with a bidirectional cen-\nsus loss. In Thirty-Second AAAI Conference on ArtiÔ¨Åcial In-\ntelligence, 2018.\n[34] Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang,\nand Hongyuan Zha. Unsupervised deep learning for optical\nÔ¨Çow estimation. In Thirty-First AAAI Conference on ArtiÔ¨Å-\ncial Intelligence, 2017.\n[35] Anirban Roy and Sinisa Todorovic. Monocular depth esti-\nmation using neural regression forest. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 5506‚Äì5514, 2016.\n[36] Szymon Rusinkiewicz and Marc Levoy. EfÔ¨Åcient variants of\nthe icp algorithm. In 3dim, page 145. IEEE, 2001.\n[37] Ashutosh Saxena, Sung H Chung, and Andrew Y Ng. Learn-\ning depth from single monocular images. In NIPS, 2006.\n[38] Ashutosh Saxena, Min Sun, and Andrew Y Ng. Make3d:\nLearning 3d scene structure from a single still image. PAMI,\n2009.\n[39] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\nFergus.\nIndoor segmentation and support inference from\nrgbd images. In European Conference on Computer Vision,\npages 746‚Äì760. Springer, 2012.\n[40] Frank Steinbr¬®ucker, J¬®urgen Sturm, and Daniel Cremers.\nReal-time visual odometry from dense rgb-d images. In 2011\nIEEE International Conference on Computer Vision Work-\nshops (ICCV Workshops), pages 719‚Äì722. IEEE, 2011.\n[41] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nPWC-Net: CNNs for optical Ô¨Çow using pyramid, warping,\nand cost volume. 2018.\n[42] Chaoyang Wang, Jos¬¥e Miguel Buenaposada, Rui Zhu, and\nSimon Lucey. Learning depth from monocular videos using\ndirect methods. In CVPR, 2018.\n[43] Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian\nPrice, and Alan L Yuille. Towards uniÔ¨Åed depth and seman-\ntic prediction from a single image. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 2800‚Äì2809, 2015.\n[44] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004.\n[45] Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, and\nNicu Sebe. Multi-scale continuous crfs as sequential deep\nnetworks for monocular depth estimation. In CVPR, 2017.\n[46] Zhenheng Yang, Peng Wang, Yang Wang, Wei Xu, and Ram\nNevatia. Every pixel counts: Unsupervised geometry learn-\ning with holistic 3d motion understanding. In ECCV, 2018.\n[47] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learn-\ning of dense depth, optical Ô¨Çow and camera pose. In CVPR,\n2018.\n[48] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G\nLowe. Unsupervised learning of depth and ego-motion from\nvideo. In CVPR, 2017.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-10-20",
  "updated": "2019-10-20"
}