{
  "id": "http://arxiv.org/abs/1910.09281v2",
  "title": "Dealing with Sparse Rewards in Reinforcement Learning",
  "authors": [
    "Joshua Hare"
  ],
  "abstract": "Successfully navigating a complex environment to obtain a desired outcome is\na difficult task, that up to recently was believed to be capable only by\nhumans. This perception has been broken down over time, especially with the\nintroduction of deep reinforcement learning, which has greatly increased the\ndifficulty of tasks that can be automated. However, for traditional\nreinforcement learning agents this requires an environment to be able to\nprovide frequent extrinsic rewards, which are not known or accessible for many\nreal-world environments. This project aims to explore and contrast existing\nreinforcement learning solutions that circumnavigate the difficulties of an\nenvironment that provide sparse rewards. Different reinforcement solutions will\nbe implemented over a several video game environments with varying difficulty\nand varying frequency of rewards, as to properly investigate the applicability\nof these solutions. This project introduces a novel reinforcement learning\nsolution by combining aspects of two existing state of the art sparse reward\nsolutions, curiosity driven exploration and unsupervised auxiliary tasks.",
  "text": "University of Sheﬃeld\nDealing with Sparse Rewards in\nReinforcement Learning\nJoshua Hare\nSupervisor: Eleni Vasilaki\nA report submitted in fulﬁlment of the requirements\nfor the degree of MSc in Advanced Computer Science\nin the\nDepartment of Computer Science\nSeptember 11, 2019\narXiv:1910.09281v2  [cs.LG]  11 Nov 2019\nDeclaration\nAll sentences or passages quoted in this report from other people’s work have been speciﬁcally\nacknowledged by clear cross-referencing to author, work and page(s). Any illustrations that are\nnot the work of the author of this report have been used with the explicit permission of the originator\nand are speciﬁcally acknowledged. I understand that failure to do this amounts to plagiarism and\nwill be considered grounds for failure in this project and the degree examination as a whole.\nName: Joshua Hare\nSignature: Joshua Hare\nDate: 15/05/2019\ni\nAbstract\nSuccessfully navigating a complex environment to obtain a desired outcome is a diﬃcult task,\nthat up to recently was believed to be capable only by humans. This perception has been broken\ndown over time, especially with the introduction of deep reinforcement learning, which has greatly\nincreased the diﬃculty of tasks that can be automated. However, for traditional reinforcement\nlearning agents this requires an environment to be able to provide frequent extrinsic rewards,\nwhich are not known or accessible for many real-world environments. This project aims to explore\nand contrast existing reinforcement learning solutions that circumnavigate the diﬃculties of an\nenvironment that provide sparse rewards. Diﬀerent reinforcement solutions will be implemented\nover a several video game environments with varying diﬃculty and varying frequency of rewards,\nas to properly investigate the applicability of these solutions.\nThis project introduces a novel\nreinforcement learning solution by combining aspects of two existing state of the art sparse reward\nsolutions, curiosity driven exploration and unsupervised auxiliary tasks.\nii\nContents\n1\nIntroduction\n1\n1.1\nAims and Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n1.2\nOverview of the Report\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2\nReinforcement Learning Background\n3\n2.1\nMarkov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nDiscrete Markov Decision Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.3\nReinforcement Learning in Markov Decision Processes . . . . . . . . . . . . . . . . .\n3\n2.4\nDynamic Programming\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.5\nModel Free Reinforcement Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.5.1\nMonte Carlo\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.5.2\nTemporal Diﬀerence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.5.3\nPolicy Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3\nNeural Networks\n11\n3.1\nMulti-Layer-Perceptron\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.2\nConvolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.3\nRecurrent Neural Networks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.4\nActivation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.5\nLoss Functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.6\nOptimisation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.6.1\nMomentum based SGD\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.6.2\nRMSProp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.6.3\nAdam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4\nDeep Reinforcement Learning\n18\n4.1\nTraditional Deep Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.1.1\nDeep Q Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.1.2\nAdvantage Actor Critic\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n4.1.3\nSynchronous DDQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.1.4\nProximal Policy Optimisation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.2\nDeep Reinforcement Solutions to Sparse Extrinsic Rewards\n. . . . . . . . . . . . . .\n23\n4.2.1\nCuriosity Driven Exploration via Next-State Prediction\n. . . . . . . . . . . .\n23\n4.2.2\nUnsupervised Auxiliary Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\niii\nCONTENTS\niv\n4.2.3\nRandom Network Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.2.4\nHindsight Experience Replay . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n5\nEvaluation and Testing\n26\n5.1\nEnvironments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.1.1\nClassic Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.1.2\nAtari 2600 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n6\nImplementation and Analysis\n29\n6.1\nSoftware and Hardware\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n6.2\nScalable Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n6.3\nEncoder Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n6.4\nA2C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n6.5\nSynchronous DDQN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n6.6\nPPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n6.7\nUnsupervised Auxiliary tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n6.7.1\nOriginal Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n6.7.2\nCustom A2C-CNN implementation (UNREAL-A2C2) . . . . . . . . . . . . .\n32\n6.8\nRND . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n6.8.1\nIntrinsic Reward and Value Function . . . . . . . . . . . . . . . . . . . . . . .\n33\n6.8.2\nTarget and Predictor networks\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n6.9\nICM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n6.10 Random Network Distillation with Auxiliary Learning RANDAL . . . . . . . . . . .\n34\n6.11 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n7\nResults and Discussion\n36\n7.1\nTesting and Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n7.2\nAtari Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n7.2.1\nBaselines\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n7.2.2\nλ-Return and PPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n7.2.3\nSparse Reward Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n7.3\nClassic Control Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n7.3.1\nAdam Hyperparameter Search\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n7.3.2\nAdam vs RMSProp\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n7.3.3\nλ-Return and PPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n7.3.4\nSparse Reward Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n8\nConclusions and Further Work\n46\n8.1\nFurther Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nList of Figures\n2.1\nGreedy policy iteration for state-action value function\n. . . . . . . . . . . . . . . . .\n6\n3.1\nVisual representation of stochastic gradient optimisers . . . . . . . . . . . . . . . . .\n16\n4.1\nDeep Q Learning algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n4.2\nAdvantage Actor-Critic (A2C) algorithm . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.3\nProximal Policy Optimisation (PPO) algorithm . . . . . . . . . . . . . . . . . . . . .\n22\n4.4\nOverview of the UNREAL agent\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n7.1\nAtari baselines for A2C vs Sync-DDQN\n. . . . . . . . . . . . . . . . . . . . . . . . .\n37\n7.2\nAtari experiments for PPO-GAE vs A2C-GAE and Sync-DDQN(λ)\n. . . . . . . . .\n38\n7.3\nAtari experiments for RND vs UNREAL-A2C2 vs RANDAL agents\n. . . . . . . . .\n40\n7.4\nClassic Control A2C Adam hyperparameter search . . . . . . . . . . . . . . . . . . .\n41\n7.5\nClassic Control Sync-DDQN Adam hyperparameter search . . . . . . . . . . . . . . .\n42\n7.6\nClassic Control baselines Adam vs RMSProp for A2C and Sync-DQQN agents\n. . .\n43\n7.7\nClassic Control A2C-GAE vs Sync-DDQN(λ) vs PPO . . . . . . . . . . . . . . . . .\n44\n7.8\nClassic Control experiments for RND vs UNREAL-A2C2 vs RANDAL agents . . . .\n45\nList of Tables\n7.1\nA2C Atari baseline hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n7.2\nSynchronous Double DQN Atari baseline hyperparameters . . . . . . . . . . . . . . .\n37\n7.3\nPPO Atari hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n7.4\nRND Atari hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n7.5\nUNREAL-A2C2 Atari hyperparameters\n. . . . . . . . . . . . . . . . . . . . . . . . .\n39\n7.6\nRND Atari hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n7.7\nPPO Classic Control hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\nv\nChapter 1\nIntroduction\nReinforcement learning is a multidisciplinary ﬁeld combining aspects from psychology, neuroscience,\nmathematics and computer science, where an agent learns to interact with a environment by taking\nactions and receiving rewards. This is inspired from observations of chemical reward signals found\nin brains of humans and many other animal [1], that dictate the behaviours of these animals. Video\ngames have been a common benchmark for reinforcement learning (RL) throughout the existence of\nRL. From Samuel’s checkers player [2] in 1959 to the game of Go [3], Atari-2600 games [4], to most\nrecently Deepmind’s Starcraft II [5], which has over 108 possible available actions that the agent can\ntake. The preference for Video games in RL, is easily understandable, as they provide an enclosed\nsimulatory environment, often with easily accessible, quantiﬁable rewards that may not be available\nor diﬃcult to measure in a real world environment. As it is common for many RL algorithms to\ntake millions of training examples to eﬀectively learn to interact with its environment, video games\nare useful in that they are computational simulations. This means that the environment is able to\nrun faster than real-time, and many simulations can be performed simultaneously.\nHowever, many modern approaches to RL such as Deep Q learning [6] require, as stated previously,\nmillions of examples in order to optimise due to some fundamental diﬃculties of the task. One\nof these is that often RL agents have no prior knowledge of the environment in this case video\ngames, as often video games are modelled on and include real world objects such as ladders, and\nphysics such as gravity.\nFrom [7] it was shown that humans perform signiﬁcantly worse when\nprior knowledge is masked, and similarly, reinforcement agents perform better when given prior\ninformation about the environment.\nAnother fundamental diﬃculty of RL is that of a sparse reward signal provided by an environment.\nA Sparse reward signal is series of rewards produced by the agent interacting with the environment,\nwhere most of the rewards received are non-positive. This makes it extremely diﬃcult for reinforcement\nlearning algorithms to connect a long series of actions to a distant future reward. For extremely\nsparse rewards, the agent may never ﬁnd a reward signal, and thus will never learn how to perform\na given task. Human agents are not hindered by sparse reward signals and in fact are capable\nof achieving tasks over a entire lifetime with little to no reward through intrinsic motivation [8],\ntraditional RL algorithm’s learning is severely inhibited by sparse rewards.\nThis means that in order to automate a speciﬁc tasks with traditional reinforcement learning,\n1\nCHAPTER 1. INTRODUCTION\n2\nthe rewards received by the agent have to be carefully designed in order to maximise optimal\nbehaviour.\nThis is a problem as it requires that researchers and developers take time to fully\nunderstand the dynamics or optimal behaviour for an environment a priori. This takes time to\nimplement, or worse the dynamics or optimal behaviour may unknown to the designers, so the\nrewards given to the agent may provide sub-optimal or non-functioning behaviour. By having an\nagent that can learn from sparse rewards, it allows designers to provide more abstract and long\nterm goals, as well as being able tackle more complex tasks.\nThis project seeks to explore and contrast solutions to the inhibiting eﬀect sparse rewards on RL\nagents, their implementations can be found on the following GitHub repository\nhttps://github.com/jhare96/reinforcement-learning/.\n1.1\nAims and Objectives\nIn this project the overall aims are:\n• Use reinforcement learning to successfully implement a working agent capable of playing video\ngames\n• Explore and contrast existing reinforcement methods to deal with sparse reward signals\n• Implement existing solutions for sparse reward reinforcement learning on a series of increasing\ndiﬃculty tasks\n1.2\nOverview of the Report\nThis report begins with a literature survey starting with chapter 2, which is a detailed introduction\nto reinforcement learning, speciﬁcally with the use of Markov Decision Processes. This chapter\nintroduces some key terminology for RL and also introduces the mathematical framework used\nto evaluate and create RL algorithms as well the algorithms themselves. The literature survey\ncontinues on to introduce neural networks in chapter 3, ﬁnally a ﬁnal chapter 4 on deep neural\nnetworks use in RL, including state of the art neural network implementations related to solving\nRL with sparse rewards.\nChapter 5 contains a brief description of the environments used to evaluate the agents implemented\nin this project. Following this is chapter 6 dedicated to a detailed explanation on the implementations\nof the RL agents tested in this project, as well as preliminary analysis on their expected performance.\nThe results of the experiments performed are presented in chapter 7 and ﬁnally the conclusions of\nthe project are found in chapter 8\nChapter 2\nReinforcement Learning Background\n2.1\nMarkov Chains\nA Markov chain is a probabilistic model that connects are series of states s in the set S via\ntransition probabilities, i.e. moving from one state to another [9]. A state is some representation\nof a model, this could be a known behaviour or variable, or an unobserved or partially observed\nhidden representation of a system (Hidden Markov Model). The transition probabilities between\nthe states in the Markov Chain hold the Markov property, that is the transition between the next\nstate of a system st+1 is only dependent on the current state st irrespective of the previous state\nst−1 [10].\np(st+1|s0, s1, . . . , st) = p(st+1|st)\n(2.1)\n2.2\nDiscrete Markov Decision Processes\nA Discrete Markov Decision Process (MDP) is a decision making process characterised by a discrete\nstate and time Markov chain. In a MDP the set of states S are a representation of the environment\ne.g. game piece conﬁguration on a board game [11]. In an MDP at each time step a decision is\nmade by taking an action at from a set of actions available in state s, at ∈As. Taking the action\nat produces a state transition modelled by the probability distribution Pa\nss′ = P(st+1 = s′|st =\ns, at = a) [12], following each state transition a real reward is received rt 1 ∈R characterised by\nthe reward model rt = Ra\ns = (rt|st = s, at = a).\n2.3\nReinforcement Learning in Markov Decision Processes\nReinforcement learning is an optimisation process in which an agent learns a policy π (a strategy\nin which determines what decisions will be made), by interacting with an environment such that\nit maximises the expected future reward. For reinforcement learning in an MDP the optimisation\nprocess is to learn the optimal policy π ≈π∗which maximises the cumulative expected future\n1The reward after transitioning from state st to st+1 is sometimes denoted as rt+1 [12]. The reasoning for the\nchoice of rt is to be consistent with the modern deep reinforcement learning papers [6],[13],[14],[15], which use this\nnotation.\n3\nCHAPTER 2. REINFORCEMENT LEARNING BACKGROUND\n4\nreward [12], known as the return2 R. The return at time t, Rt is the expected future reward from\nstate st onwards indeﬁnitely for continuous tasks or until the ﬁnal state sT for episodic tasks.\nRt =\n∞\nX\nk=0\nrt+k\n(2.2)\nA policy π maps the state s onto a probability distribution over the available actions in As [12].\nThe probability for each action a ∈As is denoted by the term π(a|s). Value functions provide an\nestimation of the value of being in state s or the value of taking an action a in s under a certain\npolicy π. Commonly used in reinforcement learning are the value functions V π and Qπ, which\ndenote the value and state-action value functions respectively.\nThe value function for a given state s under π is given by the expected future reward of from\nstarting at s and following the policy π until the terminal state. Often a discount factor γ ∈[0, 1]\nis used dampen the reward signal from distant future actions so that the agent prioritises more\nimmediate rewards (with the idea that immediate rewards are better than distant ones) and to\ndeal with any uncertainty in the model [16]. Similarly the state-action value function is given by\nthe expected future reward starting from s taking action a then following π onwards. The value\nfunction and state-action value function are given by:\nV π(s) = Eπ[Rt|st = s] = Eπ\n\" ∞\nX\nk=0\nγkrt+k\n\f\f\f\f\f st = s\n#\n(2.3)\nQπ(s, a) = Eπ[Rt|st = s, at = a] = Eπ\n\" ∞\nX\nk=0\nγkrt+k\n\f\f\f\f\f st = s, at = a\n#\n(2.4)\nIn order to solve reinforcement learning, the optimal policy π∗is to be found, which is to say to\nﬁnd the strategy which yields the highest reward over all the states. A policy π is considered better\nthan another policy π′ if it’s value function is greater over all states [12], V π(s) > V π′(s) ∀s ∈S.\nTherefore, this can be extended to deﬁne the optimal policy:\nπ∗= arg max\nπ\nV π(s) ∀s ∈S\n(2.5)\nSince V ∗is the maximum expected reward from starting from state s then it must be the\nmaximum of the expected reward over all possible available actions [12] V ∗(s) = maxa∈As Q∗(s, a).\nHence, the optimal policy can be written in terms of the optimal state-action value function:\nπ∗= arg max\na\nQ∗(s, a) ∀s ∈S\n(2.6)\n2.4\nDynamic Programming\nDynamic programming is technique developed by Bellman to recursively solve a structural search\nproblem (in this case a the optimal policy for a MDP), by storing the values of computed variables\nthat reoccur during the recursive process [17]. From equations (2.3) & (2.4) it can be seen that\n2Also referred to as Gt in [12]\nCHAPTER 2. REINFORCEMENT LEARNING BACKGROUND\n5\nthe cumulative expected reward Rt = P∞\nk=0 rt+k is calculated at each time step t, which can be\ncomputationally demanding for large MDPs.\nUsing γ0 = 1 (2.3) can be expanded [18]:\nV π(s) = Eπ\n\u0002\nrt + γrt+1 + γ2rt+2 + . . . γKrt+K\n\f\f st = s\n\u0003\nUsing Deﬁnition of Rt 2.2\n= Eπ [rt + γRt+1 | st = s]\n(2.7)\nconditioning on the next state s′ using law of total expectation\n=\nX\ns′∈S\nP(st+1 = s′|st = s)Eπ\n\u0002\nrt + γRt+1\n\f\f st = s, st+1 = s′\u0003\nthen condition on the action a\nX\na\nP(a|s)\nX\ns′∈S\nP(s′|s, a)Eπ\n\u0002\nrt + γRt+1\n\f\f s, s′, a\n\u0003\nthen using addition theorem of expectations\nX\na\nP(a|s)\nX\ns′∈S\nP(s′|s, a)\n\u0002\nEπ\n\u0002\nrt\n\f\f s, s′, a\n\u0003\n+ Eπ\n\u0002\nRt+1\n\f\f s, s′, a\n\u0003\u0003\nusing the Markov property and deﬁnition of Reward model\nX\na\nP(a|s)\nX\ns′∈S\nP(s′|s, a)\n\u0002\nRa\ns + Eπ\n\u0002\nRt+1\n\f\f s′\u0003\u0003\nThus the value function can be expressed recursively with the reward and transition model:\nV π(s) =\nX\na\nπ(a|s)\nX\ns′∈S\nPa\nss′\n\u0002\nRa\ns + γV π(s′)\n\u0003\n(2.8)\nA similar derivation can be done conditioning on the next state s′ and next action a′ or from (2.8)\nusing:\nV π(s) =\nX\na∈A\nπ(a|s)Qπ(s, a)\n=⇒Qπ(s, a) =\nX\ns′∈S\nPa\nss′\n\u0002\nRa\ns + γV π(s′)\n\u0003\nQπ(s, a) =\nX\ns′∈S\nPa\nss′\n\"\nRa\ns + γ\nX\na′∈A\nπ(a′|s′)Qπ(s′, a′)\n#\n(2.9)\nThese are known as the Bellman equations for stochastic processes.\nDynamic program requires a known MDP in order to estimate the optimal value function hence\nupdate the policy. The policy can be updated iteratively by computing the value function of a\npolicy π as according to the Bellman equations given above, then updating the policy using (2.5)\nor (2.6) depending on the value function used.\nCHAPTER 2. REINFORCEMENT LEARNING BACKGROUND\n6\nGreedy Policy Iteration for State-Action Value Function\nInitialise Q(s, a), π(s)\nrepeat\n1. Policy Evaluation\nrepeat\nfor each s ∈S do\nfor each a ∈A do\nQπ\nold = Qπ\nQπ(s, a) ←\nX\ns′∈S\nPa\nss′\n\"\nRa\ns + γ\nX\na′∈A\nπ(a′|s′)Qπ(s′, a′)\n#\nend for\nend for\nuntil Qπ\nold −Qπ < ϑ\n▷Check For Convergence\n2. Policy Update\nfor each s ∈S do\nπold(s) ←π(s)\nπ(s) = arg maxaQπ(s, a)\nend for\nuntil π ≈πold\n▷Found stable policy\nFigure 2.1: Greedy policy iteration for state-action value function algorithm, adapted from [12]\n2.5\nModel Free Reinforcement Learning\nRather than learning the optimal policy for a known MDP structure, model-free reinforcement\nlearning attempts to ﬁnd the value function of being in state s directly from experience. This is\na crucial step for applying reinforcement learning to real world problems, as most environments\nhave an unknown MDP or too large state-action space to computationally model. The cost of\nthis approach is the agent has a less eﬃcient learning process, i.e. many more samples required to\noptimally converge.\n2.5.1\nMonte Carlo\nIn Monte Carlo reinforcement learning is a model free method which learns the value function for\nepisodic tasks. An episodic task is a sequence of sequential experiences ⟨st, at, rt, ⟩that always\nhave a terminal state i.e. from s0, . . . , sT . Most games can be deﬁned as episodic tasks, an example\nbeing a game of chess always has a terminal state (a ﬁnal board-piece conﬁguration) after the game\nhas ﬁnished through a win or draw. The reason Monte Carlo reinforcement learning can only learn\nfrom episodic tasks is that it approximates the expectation of future rewards in the value functions\n(2.8) and (2.9) with an average of Rt for all states visited, over each episode. The future reward\nRt can be determined online with dynamic programming however, the average state count can\nCHAPTER 2. REINFORCEMENT LEARNING BACKGROUND\n7\nonly be determined at the end of the episode. The average will tend to the true expectation as\nthe number of samples approaches inﬁnity due to the central limit theorem. Two common ways\nthe average is calculated in Monte Carlo methods are the ﬁrst visit evaluation and the every visit\nevaluation. In the ﬁrst visit approach, the value function for a state s is calculated only once per\nsample (episode) and is averaged over all samples. The every visit approach averages the value\nof the state for every visit including multiple visits per episode [12]. The Value function can be\nexpressed via V (s) = S(s)/N(s), where S(s) is the sum of sample value estimates from state s and\nN(s) is the number of times the agent has visited state s.\nThe value function can be updated iteratively in Monte Carlo learning, as the mean of a series can\nbe computed iteratively. For a dynamic problem it may be useful to calculate the running mean\nwhilst forgetting older episodes, this is by introducing a dynamic variable α [12]:\nV (st) ←V (St) + α (Rt −V (St))\n(2.10)\nA disadvantage Monte Carlo method introduces is the inability to learn continuous tasks and the\ninability to update the value function before the end of an episode. This is not an issue for short\ntime length episodes, but can greatly increase the time taken to ﬁnd the optimal policy.\n2.5.2\nTemporal Diﬀerence\nTemporal Diﬀerence combines the model free learning from the Monte Carlo method with the\nbootstrapping (expected future reward value estimation) from Dynamic Programming. This allows\npolicy learning during episodes or continuous tasks, without the need to model the transition\nprobabilities Pa\nss′. In Temporal Diﬀerence learning, the optimal value function is found by iteratively\nupdating the value function with the temporal error between the value predicted at V (st) and the\nfuture reward Rt = P\nk γkrt+k similar to Monte Carlo evaluation (2.10). This is done by sampling\nfrom the expected future reward from (2.7) and using current value function estimate instead of\nthe true value function, as seen in Dynamic Programming [12]. The temporal error is then:\nδt = rt + γV (st+1) −V st)\n(2.11)\nV (s) can be iteratively updated using the temporal error δt, this is known as TD(0):\nV (s) ←V (s) + α [rt + γV (st+1) −V (st)]\n(2.12)\nUsing the state-action value function in (2.12) results in the on-policy method commonly known\nas SARSA (state, action, reward, state, action)\nQ(s, a) ←Q(s, a) + α [rt + γQ(st+1, at+1) −Q(st, at)]\n(2.13)\nA bridge between Temporal Diﬀerence and Monte Carlo methods can be extended ﬁrst with an\nn-step Temporal Diﬀerence error TD(n) which is deﬁned as the temporal error between state V (st)\nCHAPTER 2. REINFORCEMENT LEARNING BACKGROUND\n8\nand V (st+n) deﬁned using the n-step future return R(n)\nt\n[12]:\nR(n)\nt\n= rt + γrt+1 + . . . + γn−1rt+n−1 + γnV (st+n)\n(2.14)\nδ(n)\nt\n= R(n)\nt\n−V (st)\n(2.15)\nThe optimal number of steps n depends on the length of the task and the number of states, ﬁnding\na general solution to this problem is done by further extending the bridge to Monte Carlo is the\nTD(λ) approach. This is where all the expected rewards (2.14) are computed from n = 0, . . . T and\ncombined geometrically via [19]:\nRλ\nt = (1 −λ)\n∞\nX\nn=1\nλn−1R(n)\nt\n(2.16)\nThe λ approach can also be applied to SARSA, SARSA(λ).\nQ Learning(an oﬀ-policy version of SARSA) was developed by Watkins [20].\nHere instead of\napproximating the true state-action value function for a given policy Qπ(s, a) with the current\nestimate, the optimal state-action value function is approximated Q∗(s, a).\nQ(s, a) ←Q(s, a) + α\nh\nrt + γ max\na\nQ(st+1, a) −Q(st, at)\ni\n(2.17)\nQ Learning suﬀers from a maximisation bias, that is that it overestimates the values in the\nstate-action function Q(s, a) i.e.\nwith a positive bias.\nA solution was proposed in [21] to use\ntwo state-action value functions, one to predict the next action to be taken and another to predict\nthe value estimation of the action. (2.17) then becomes:\nQA(s, a) ←QA(s, a) + α\n\u0002\nrt + γQB(s′, arg maxaQA(s′, a)) −QA(s, a)\n\u0003\nQB(s, a) ←QB(s, a) + α\n\u0002\nrt + γQA(s′, arg maxaQB(s′, a)) −QB(s, a)\n\u0003\n(2.18)\nBoth QA(s, a) and QB(s, a) are used as the to select the action at time t and at each optimisation\nstep either A or B are updated via random selection.\nQ Learning also suﬀers from poor exploration as the approximation to optimal value is being used\nas the policy hence often limiting the exploration to states of known rewards. A solution to this is\nto use a decaying ε - greedy policy to enable initial exploration, then use optimal policy once the\nstates have been suﬃciently explored.\nε −greedy\n(\nat = random a ∈A\nwith probability ε\nat = maxa Q∗(s, a)\nwith probability 1 −ε\n(2.19)\n2.5.3\nPolicy Gradient\nRather than learning the optimal policy through the maximisation over value function approximations\n2.5 or 2.6 (which can be expensive for high dimensional state-actions spaces), the policy can be\nlearnt directly under the condition that the policy is parametrised by a diﬀerentiable function. The\nCHAPTER 2. REINFORCEMENT LEARNING BACKGROUND\n9\npolicy is learnt by calculating the gradient of the current policy estimate ∇π(a|s, θ) [12]. This\nallows us to learn, stochastic policies, continuous action policies and should theoretically provide\nmore stable convergence properties [16] over value-based methods such as Q-Learning. However\nlearning the policy directly can reduce the sample eﬃciency of the agent taking longer to converge\nto the optimal policy.\nThe policy gradient theorem shows that maximising the gradient of the value function is equivalent\nto maximising the gradient of the policy multiplied by the state-action value function all under the\nexpectation of the policy [12]. This gives the gradient objective function J(θ) (with the objective\nof maximising the expected future reward) as\n∇θJ(θ) = Eπθ [Qπ(s, a)∇θ log π(a|s, θ)]\n(2.20)\nUsing the deﬁnition of Qπ(s, a) 2.4\n∇θJ(θ) = Eπθ [Rt∇θ log π(a|s, θ)]\n(2.21)\nAs Qπ(s, a) is not dependent on θ a baseline metric b(st) can be subtracted from equations 2.20 &\n2.21 as long as it is constant w.r.t. a.\n∇θJ(θ) = Eπθ [(Rt −b(s))∇θ log π(a|s, θ)]\n(2.22)\nActor Critic\nThe Actor-Critic family of policy gradient methods involves combining the baseline policy gradient\nmethod with the bootstrapping method from temporal diﬀerence. For the Advantage Actor-Critic\nthe baseline function used is the Advantage function\nA(st, at) = Q(st, at) −V (st) = rt + γV (st+1) −V (st).\n(2.23)\nExpressing the Advantage function in terms of only V π allows for the use of a single set of parameters\nto deﬁne the advantage function and reducing the dimensionality of the function being learnt. This\nhas the beneﬁts of being easier to learn as well as decreasing computational complexity, especially\nfor large |A| [22].\n∇θJ(θ) = Eπθ [(rt + γV (st+1) −V (st))∇θ log π(at|st, θ)]\nwhere at ∼π\n= Eπθ [δt∇θ log π(at|st, θ)]\n(2.24)\nBy using V (s) as a baseline as well as to predict the TD error, value function provides the estimate\nof the quality of the action taken by the policy. Hence, it is said to be acting as a critic to the\npolicy (the actor), thus giving the name Actor-Critic.\nGeneralised Advantage Estimation\nJust as the TD n-step returns estimator can be combined in TD(λ), the advantage function\nestimation can be calculated similarly.\nFollowing 2.23, the n-step advantage estimator can be\nCHAPTER 2. REINFORCEMENT LEARNING BACKGROUND\n10\ndeﬁned as\nAn\nt =\nn−1\nX\nl=0\nγlδt+l =\nn−1\nX\nl=0\nγlrt+l + γnV (st+n) −V (st)\n(2.25)\nFollowing the derivation from [22], Aλ\nt can be derived by ﬁrst exponentially averaging each n-step\nadvantage estimate\nAλ\nt = (1 −λ)\n\u0010\nA(1)\nt\n+ λA(2)\nt\n+ λ2A(3)\nt\n+ . . .\n\u0011\nAλ\nt = (1 −λ)\n\u0000δt + λ(δt + γδt+1) + λ2(δt + γδt+1 + γ2δt+2) + . . .\n\u0001\nFactoring by δt\nAλ\nt = (1 −λ)\n\u0000δt(λ + λ2 + . . .) + γδt+1(λ2 + λ3 + . . .) + . . .\n\u0001\nUsing the geometric series solution\nn−1\nX\nk=0\nark = a\n\u00121 −rn\n1 −r\n\u0013\nAλ\nt = (1 −λ)\n\u0012\nδt\n\u0012\n1\n1 −λ\n\u0013\n+ γδt+1\n\u0012\nλ\n1 −λ\n\u0013\n+ γ2δt+2\n\u0012 λ2\n1 −λ\n\u0013\n+ . . .\n\u0013\nthen\nAλ\nt =\n∞\nX\nl=0\n(λγ)lδt+l\n(2.26)\nChapter 3\nNeural Networks\nArtiﬁcial neural networks are neurologically inspired computational frameworks, which simulate\nneuron activity as a weighted summation over a series of connected input neurons.\nThere are\nmany diﬀerent neural networks topologies that are commonly used, but most consist of a series of\nconnected groups of neurons known as layers, which are fed stimulatory input through an input\nlayer. An output layer (most often the ﬁnal layer of the network) provides an estimation of some\ntarget value that is attempted to being modelled by the network.\nNeural networks were ﬁrst\ndeveloped the in the 1940s [23], however have seen a large increase in usage in recent years, due\nto increased computational power of modern computers. This allows larger, hence more powerful\nnetworks to be trained within a reasonable time frame.\n3.1\nMulti-Layer-Perceptron\nInspired by the perceptron algorithm Hinton et al created a feed-forward multi-layer perceptron\n(MLP) [24]. MLPs consist of distinct layers of nodes, where each node’s value is a summation of the\nprevious layer l −1 with the synaptic weights of the current layer wl\nij under a non-linear activation\nfunction g. MLPs can approximate any function with an inﬁnitely sized hidden layer [25] and can\nbe trained to minimise a given loss function via the backpropagation algorithm [26]. Non-linear\nactivation functions are a key process of the MLP’s ability to approximate any function and they\nare required to be diﬀerentiable in order to update the weights via backpropagation. The value of\na node of a hidden layer l is given by:\nhl\ni = g\n\nX\nj\nwl\nijhl−1\nj\n\n= g\n\u0010\nW lhl−1\u0011\nwhere W ∈Rn×m\n(3.1)\nA loss function is deﬁned by the ﬁnal output layer ˆy and the target value y given by the model\n(weights) θ, L(ˆy, y|θ). As the output layer is a composite function the weights can be updated by\ndiﬀerentiating the weights w.r.t the loss function using the chain rule.\n∂L\n∂wl\nij\n= ∂L\n∂zl\ni\n∂zl\ni\n∂wl\nij\n= ∂L\n∂hl\ni\n∂hl\ni\n∂zl\ni\n∂zl\ni\n∂wl\nij\n(3.2)\n11\nCHAPTER 3. NEURAL NETWORKS\n12\nwhere z = P\nj wl\nijhl−1\nj\n. As the loss w.r.t the hl is dependent on the next layer ∂L\n∂hl\ni can be expressed\nrecursively:\n∂L\n∂wl\nij\n=\n\nX\nj\n∂L\n∂zl+1\nj\n∂zl+1\nj\n∂hl\ni\n\n∂hl\ni\n∂zl\nj\n∂zl\ni\n∂wl\nij\nSince ∂zl\ni\n∂wl\nij\n= hl−1\ni\nand ∂hl\ni\n∂zl\nj\n= ∂g(zl\ni)\n∂zl\nj\n=\n\nX\nj\n∂L\n∂zl+1\nj\n∂zl+1\nj\n∂hl\ni\n\n∂g(zl\ni)\n∂zl\nj\nhl−1\ni\n(3.3)\n3.2\nConvolutional Neural Networks\nConvolutional neural networks (CNN) are a visual cortex inspired neural network similar to the\nMLP as described above. They diﬀer by the replacing the linear summation by a convolutional\noperation across the input. The convolution operation can be physically interpreted as an area\noverlap between two functions [27], in the context of neural networks it can be loosely interpreted\nas comparing a signal (image in video games) to a set of learnt ﬁlters which have a feature\nrepresentation of diﬀerent components of a signal. Using a convolutional layer allows the network\nto learn spatially or temporally invariant features across an input signal as the same ﬁlter is used\nacross diﬀerent sections of the input. In the context of images, this is equivalent to learning speciﬁc\nfeatures, for example the features of a car regardless of the location of the car within the input.\nThe convolution operation is deﬁned by,\nf(t) ∗g(t) =\nZ ∞\n−∞\nf(τ)g(t −τ)dτ\nFor continuous functions\nx[m] ∗y[m] =\n∞\nX\nn=−∞\nx[n]y[m −n]\nFor discrete functions\n(3.4)\nThe hidden layer component hl\nij for a 3D fully convolutional layer with a single ﬁlter is given by,\nhl\nij = g\n\u0010\nwl\nijk ∗hl−1\nij\n\u0011\n= g\n X\nx\nX\ny\nX\nz\nwl\nx,y,zhl−1\ni−x,j−y,k−z\n!\nfor a singular k × k × c ﬁlter wl\nijk.\n(3.5)\n3.3\nRecurrent Neural Networks\nCNN and MLP are both examples of feed-forward neural network architectures. Speciﬁcally, these\nare both discrete acyclic graphs, meaning that each node in the graph is not connected to itself via\nany path. This reduces the capability of these networks for modelling sequential data, when the\nfeatures x ∈Rn are fed into the network at each time step t. This is because, in acyclic graphs\nthere in no modelled relation between each pass of the network. Recurrent Neural Networks (RNN),\nsolve this problem by combining the hidden state ht of the feature xt, with the hidden state ht−1\nof the previous feature xt−1. The hidden layer at time t for a single-hidden layer RNN is given by\nCHAPTER 3. NEURAL NETWORKS\n13\nthe equation,\nht = g (Wx xt + Wh ht−1)\nwhere Wx ∈Rm×n, Wh ∈Rm×m.\n(3.6)\nThe output yt is deﬁned by an additional MLP layer, and can be produced every time-step in a\nmany-to-many conﬁguration or at the ﬁnal time-step T in a many-to-one conﬁguration. Another\nconﬁguration includes that of one-to-many where the output yt−1 is fed into 3.6 instead of the\nprevious hidden state ht−1.\nAs the hidden state ht hence output yt is conditioned of the previous step, the backpropagation\nalgorithms ﬂows backwards through the time steps T →t0, hence is known as backpropagation\nthrough time. However the drawbacks of this simple recurrent network is that of the vanishing\ngradients problem, where the gradients decrease in size from T to t0, hence simple RNNs can\nstruggle to learn from long sequential data. This problem is reduced by the introduction of Long\nTerm Short Memory (LSTM) recurrent networks which use multiple gates, in order to learn to\nextract more useful long-term important features.\n3.4\nActivation Functions\nMany diﬀerent non-linear activation functions are used for artiﬁcial neural networks and bring\ndiﬀerent strengths and weakness depending on the usecase. A brief description of each activation\nalong with the relevant formula is given below.\nThe hyperbolic and logistic activation function families have been a popular choice throughout\nthe history of neural networks. The two most common choices however are the tanh and sigmoid\nfunctions, these are given by the equations,\ng(x) = tanh(x) = (ex −e−x)\n(ex + e−x)\ntanh\n(3.7)\ng(x) = σ(x) =\n1\n1 + e−x\nsigmoid.\n(3.8)\nThe tanh and sigmoid function constrain the value of x to within the boundaries [0, 1], [−1, 1]\nrespectively. This can lead to vanishing gradients [28] for deeper neural networks when training\nwith backpropagation. This is because the gradient is restricted to between the values of [0, 1],\nsince backpropagation uses the chain rule, these gradients can quickly tend towards zero resulting\nin very small weight updates, as well as numerical error eﬀects dominating the weight updates.\nRectiﬁed linear unit (ReLU) is perhaps the most popular activation function due to its simplicity,\nspeed and often performance increase. It is given by the equation,\ng(x) =\n(\n0\nfor x ≤0\nx\nfor x > 0\nReLU.\n(3.9)\nThe performance increase has often been attributed to the increase in sparsity of the connections\nthat carry any non-zero value, this has been observed to be between 50%-80% in [29] for deep\nCHAPTER 3. NEURAL NETWORKS\n14\nneural networks. Sparsity of neural connectivity is seen as a more biologically plausible model, as\nit is estimated that neurons in the human brain have a sparsity of between 90%-99% [29]. This\nintuitively makes sense as for random weight initialisation with mean zero should produce 50%\nsparsity, as the sets of positive and negative real numbers are equal in size. The vanishing gradient\nproblem is negated as the gradient of the ReLU function is either 1 for positive x or zero otherwise\n∂g(x)\n∂x\n=\n(\n0\nfor x ≤0\n1\nfor x > 0\n, hence the gradients do not reduce in size over multiple layers. However the downside is that\nReLU units can enter a value such that the neuron is never activated again, this is known as the\ndying ReLU problem and is solved by replacing the zero value in 3.9 by some function of x. For\nexample it is replaced by αx in the leaky ReLU and a(ex−1) in the Exponential Linear Unit (ELU)\nfunctions.\ng(x) =\n(\nαx\nfor x ≤0\nx\nfor x > 0\nleaky ReLU.\n(3.10)\ng(x) =\n(\na(ex −1)\nfor x ≤0\nx\nfor x > 0\nELU.\n(3.11)\nThe softmax activation function produces a normalised probability distribution over each feature in\nan input vector x ∈Rd →z ∈Rd, such that the probability of zi is is proportional to the exponent\nof the input value xi [9]. This is explicitly calculated by the formula,\ng(xi) =\nexi\nP\nk∈d\nexk\nsoftmax.\n(3.12)\nAs the nature of this activation function this is generally used on the ﬁnal layer of the network for\nmultinomial logistic regression.\n3.5\nLoss Functions\nIn order to calculate the gradients a distance metric between the desired output y ∈RN and the\nactual output ˆy ∈RN must be deﬁned. Mean squared error is a common distance metric which\npenalises the diﬀerence between two vectors by the mean of the squared distance between each\nfeature\nMSE = ||y −ˆy||2 = 1\nN\nN\nX\ni=0\n(yi −ˆyi)2.\n(3.13)\nA better comparison between two probability distributions than the mean squared error of the\ndistributions is the cross entropy.\nEntropy is a measurement of uncertainty in a probability\ndistribution.\nEntropy is highest when a distribution is uniform and lowest as the distribution\napproaches the Dirac delta function. For example the uncertainty is high for the outcome of a fair\nCHAPTER 3. NEURAL NETWORKS\n15\nsided die roll, hence it has high entropy. For a discrete probability distribution entropy is given by,\nH(x) = −\nX\ni\np(xi) log p(xi).\n(3.14)\nInformation theory states that a message contains high information when the outcome is unpredictable\n[30], as you have gained information about the outcome. Information is lowest when the outcome\nis known and predictable, as you already had the information about what the outcome would be.\nEntropy then can be seen as the amount of information in a signal. The Kullback-Leibler (KL)\ndivergence can be seen as the amount of information needed to encode the information in the\nprobability distribution p with another distribution q [31], it is lowest when q = p and hence can\nbe seen as an error between these two probability distributions. Formally this is the expected log\ndiﬀerence between the two distributions,\nDKL(p||q) = Ex [log p −log q] =\nX\ni\np(xi) log p(xi) −p(xi) log q(xi) =\nX\ni\np(xi) log\n\u0012p(xi)\nq(xi)\n\u0013\n.\n(3.15)\nHowever this is not a true distance metric because it is not symmetrical DKL(p||q) ̸= DKL(q||p),\nnor does it satisfy the triangle equality [32].\nThe second term in equation 3.15, −P\ni p(xi) log q(xi) is also known as the cross entropy between\np and q. For a ﬁxed p, for example the labels for a classiﬁcation task, minimising the cross entropy\nis equivalent to minimising the KL-divergence. Minimising both the mean squared error and cross\nentropy are equivalent to maximising the likelihood of the data given the parameters [9].\n3.6\nOptimisation Methods\nAs backpropagation is a gradient based optimisation process, that minimises the parameters in some\nloss space L. Using gradient based optimisation allows the parameters to approach at least a local\nminimum for convex problems as well as stationary points for non-convex problems [33]. As neural\nnetworks require very large amount of data points compared to other Machine Learning algorithms,\nupdating the gradient based on the average over all data points in a data set is impracticable. Thus,\nStochastic Gradient Descent (SGD) methods are often used to increase training speed.\nUsing SGD methods is especially important as all data points cannot be accessed at any time, as\nit is being continually generated by the agent. SGD, however requires heavy tuning in order to\nﬁnd a optimal learning rate η for diﬀerent tasks, such that the parameters do not get stuck in local\nminima and preferable one that converges with as few updates as needed. Because of this modiﬁed\nSGD algorithms are often used throughout machine learning, especially with neural networks.\nCHAPTER 3. NEURAL NETWORKS\n16\nFigure 3.1: Visual representation of stochastic gradient optimisers\n3.6.1\nMomentum based SGD\nIn order to reduce stochasticity of the SGD updates, the concept of momentum of gradient updates\nwas ﬁrst used in [34]. Here the analogy is that of a particle rolling down a hill, it experiences a force\npulling on it which increases its momentum. When the gradients are steeper the particle accelerates,\nand as the hill ﬂattens it decelerates. This stabilises the direction of the gradient updates as the\nmomentum makes it less susceptible to large deviations caused by a single minibatch update. If\nthe optimiser has a large enough momentum it can cause it to escape suboptimal local minima.\nHowever, too large momentum can cause the optimiser to overshoot minima thus being slower to\noptimally converge, as seen in Figure 3.1.\n3.6.2\nRMSProp\nRoot Mean Squared Propagation (RMSProp) [35], adapts the magnitude of the update for each\nparameter by dividing by a running average of the root mean squared gradient v = ¯W 2, this allows\ndiﬀerent features to be learnt at diﬀerent scales, thus reducing the need to ﬁnely tune the learning\nrate. For a neural network layer weighting W, the update rule can be written as\nvt ←βvt−1 + (1 −β)\n\u0012 ∂J\n∂Wt\n\u00132\nWt+1 ←Wt −\nη\np\nvt\n\u0012 ∂J\n∂Wt\n\u00132\n.\n(3.16)\nWhere β is the decay rate of the average.\nCHAPTER 3. NEURAL NETWORKS\n17\n3.6.3\nAdam\nAdaptive Moment Estimation (Adam) [36] combines the adaptive magnitude updates from RMSProp\n3.16 with momentum based gradient descent [36], with bias correction for momentum and adaptive\nmoment (mean square of the gradient)\nmt ←βmmt−1 + (1 −βm)\n\u0012 ∂J\n∂Wt\n\u0013\nmomentum\nvt ←βvvt−1 + (1 −βv)\n\u0012 ∂J\n∂Wt\n\u00132\nadaptive moment\nbmt ←mt/\n\u00001 −βt\nm\n\u0001\nbias correction for momentum\nˆvt ←vt/\n\u00001 −βt\nv\n\u0001\nbias correction for adaptive moment\nWt+1 ←Wt −η\nbmt\np\nbvt + ϵ\n(3.17)\nWhere βt\nm, βt\nv are the decay rates with exponent of the number of updates t, and ϵ is a small scalar\nvalue to prevent division by zero [36].\nChapter 4\nDeep Reinforcement Learning\nIn video games it is common for the set of states S to be given as a pixel array of the game screen\ns ∈Rh×w×c, for a single colour frame of size 84 × 84 there are 842 × (255)3 ≈1 × 1011 diﬀerent\nimage combinations. Most video games will likely only use a tiny fraction of this image space, but\nit is still impractical to model the value functions for each state separately. A solution is to use\na diﬀerentiable function to model V (s), this is known as value function approximation [12]. Deep\nReinforcement Learning refers to using a deep neural network as a non-linear value function or\npolicy approximator.\n4.1\nTraditional Deep Reinforcement Learning\n4.1.1\nDeep Q Learning\nThe Deep Q learning algorithm proposed by [6] is essentially the Q learning algorithm described\npreviously (2.17), with two additional key features that stabilise the policy iterations for the use\nof neural networks.\nThe two additional features are experience replay, and ﬁxed target policy\niteration. If using a gradient based iterative optimisation process as done via back-propagation in\nneural networks, it is important that the data is independent and identically distributed (i.i.d.).\nThis is done as to avoid sampling bias from correlated inputs, which can cause the gradient to get\nstuck in a non-optimal local maxima (as we are performing gradient ascent to maximise the expected\nfuture reward). Experience replay is a method to help decorrelate the sequential experiences gained\nfrom dynamic programming and model free reinforcement learning methods. This is done by storing\nexperiences, a tuple of ⟨st, at, rt, st+1⟩into a list of experiences known as the replay memory. Batch\nsamples can be drawn randomly from the replay memory which provide ∼i.i.d. for a large replay\nlength. Deep Q Learning also solves the partially observable MDP problem by providing the input\nof the neural network with 4 concatenated previous states (greyscaled images for CNN).\nThe Deep Q Learning algorithm is expressed below :\n4.1.2\nAdvantage Actor Critic\nAsynchronous Advantage Actor-Critic (A3C) [37] is an algorithm where multiple actors interact and\nlearn from their own environment asynchronously. Each actor has it’s own local policy parameters\n18\nCHAPTER 4. DEEP REINFORCEMENT LEARNING\n19\nDeep Q Learning Algorithm with Experience Replay and ϵ - greedy Policy for Atari\nEnvironments\nInitialize replay memory D to capacity N\nInitialize action-value function Q with random weights\nk - number of frames to stack\nfor episode 1 . . . M do\ns1 = {x1, x1, ...}\nstack initial frame k-times\nfor time t . . . T do\nWith probability ε select a random action at\notherwise at = maxa Q∗(st, a|θ)\nExecute action at in emulator and observe reward rt and image xt+1\nst+1 = {xt−k+1, ..., xt, xt+1} and add experience to D ←⟨st, at, rt, st+1⟩\nSample random minibatch of transitions ⟨sj, aj, rj, sj+1⟩from D\nSet yj =\n\u001a rj\nfor terminal sj+1\nrj + γ maxa′ Q (sj+1, a′|θ)\nfor non-terminal sj+1\n∇θiLi (θi) = Eπ [(r + γ maxa′ Q(s′, a′|θi−1) −Q (s, a|θi)) ∇θiQ (s, a|θi) | s, a, s′]\nEvery C steps θi−1 = θi\nend for\nend for\nFigure 4.1: Deep Q Learning algorithm, adapted from [6]\nwhich it uses to calculate the gradients of the policy and value function over n-steps, this is\ndone using TD(n). The gradients calculated by each actor are asynchronously applied to shared\nglobal parameters, and after each n-step rollout the local parameters are updated from the global\nparameters.\nRunning multiple environments allows the agent to learn from many uncorrelated\nexperiences, thus reducing the impact on-policy training has on local optimal convergence. Thus,\nis seen as a replacement for an experience replay buﬀer. The greatest beneﬁt of the A3C and other\nasynchronous method is the reduced training time over single actor algorithms, especially when\nusing distributed computing [38].\nA2C [39] is a synchronous version of A3C, where multiple actors interact synchronously using\na single shared policy-network. Running these environments synchronously removes the need to\ncorrect for any oﬀ-policy correction and allows for greater GPU utilisation.\nCHAPTER 4. DEEP REINFORCEMENT LEARNING\n20\nn-step Synchronous Advantage Actor-Critic (A2C) Algorithm\nRandomly initialize policy π and value function V with parameters θ, θv respectively\nCreate Ne environments\nn - number of steps to calculate TD over\nRn - n-step return\nrepeat\nfor actor 1 . . . Ne do\nfor time t . . . t + n do\nSample actions at ∼π(a|st, θ)\nExecute actions at and observe rewards rt and next states st+1\nend for\nend for\nRn\nt =\n(\n0\nfor terminal st\nV (st|θv)\nfor non-terminal st\nBootstrap from last state\nfor t + n, t + n −1, ...t do\nRn\nt =\n(\nrt\nfor terminal st\nrt + γRn\nt+1\nfor non-terminal st\nend for\n∇θL(θ) = Eπ,∀e∈Ne [∇θ log π(a|s, θ)(Rn −V (s|θv) + βH∇θH(π(s, θ))]\n∇θvL(θv) = Eπ,∀e∈Ne\nh\n∂(Rn−V (s|θv))2\n∂θv\ni\nuntil tmax\nFigure 4.2: A2C algorithm adapted from [37] and [40]\n4.1.3\nSynchronous DDQN\nIn [37] the asynchronous multi-actor framework was also extended to include versions which use\nstate-action value functions, e.g. SARSA and Q-Learning. Synchronous n-step Double DQN (Sync\nDDQN), is a synchronous version of the async DQN algorithm proposed in [37], with Double DQN\nparametrisation following [41]. The network used to provide the value of action was a ﬁxed copy\nof the online network θ. The online network was used to select the action at under the epsilon\ngreedy policy. The value used to bootstrap the estimated future return from 4.2 is then replaced by\nQ(st, arg maxa Q(st, a|θi) |θi−1), where θi−1 is a periodic copy of θ updated every C steps similar\nto 4.1.\nCHAPTER 4. DEEP REINFORCEMENT LEARNING\n21\n4.1.4\nProximal Policy Optimisation\nTrust Regions\nIn reinforcement learning the training data is dynamically produced by the agent. This means\ntoo large policy update can signiﬁcantly alter the behaviour of the agent. Too large an update\ncan cause the agent to move into parameter space where it converges on sub-optimal behaviour or\nworse, it no longer receives any rewards from the environment. A safer method to update the policy\nis to stick within a trusted region of the current policy as to not cause too dramatic updates. In\nTrust Region Policy Optimisation TPRO, [42] the surrogate objective function for a speciﬁc state,\naction pair st, at is the policy scaled by the policy of this pair w.r.t. the old policy parameters θold,\npenalised by the KL-divergence between the old policy and the new policy. This is given by the\nequation\nmaximize\nθ\nEt\n\u0014 π (at|st, θ)\nπ (at|st, θold)At −β KL (π (·|st, θold) || π (·|st, θ))\n\u0015\n[43].\n(4.1)\nβ is a constant that controls the strength of the KL-divergence penalty. The original derived\nquantity leads to a too small an update [42], thus requires tuning for each task, or requires further\ncomplexity by adaptively changing the strength throughout training [43]. In [42] they overcome this\nby optimising the surrogate objective Et\nh\nπ(at|st,θ)\nπ(at|st,θold)At\ni\nunder the constraint that the KL-divergence\nis less than some scalar value δ, instead of being penalised by it.\nThis requires second order\noptimisation methods introducing even further complexity.\nPPO\nProximal Policy Optimisation PPO [43], is a ﬁrst order optimisation method that is used to calculate\na lower-bound for the policy update. This is done in the form of clipping the policy ratio. The\nminimum value between the unclipped surrogate policy objective, and the clipped surrogate policy\nobjective is chosen to perform the update on θ.\nmaximize\nθ\nEt\n\u0014\nmin\n\u0012 π(at|st, θ)\nπ(at|st, θold)At, clip\n\u0012 π(at|st, θ)\nπ(at|st, θold), 1 −ϵ, 1 + ϵ\n\u0013\nAt\n\u0013\u0015\n[43].\n(4.2)\nPPO is optimised over several epochs for each n-step sample, no theoretical justiﬁcation is given\nin [43] for this. This is a problem for policy gradient methods as after the ﬁrst epoch the updated\nparameters are longer the parameters that produced the current training data, and thus further\nepochs are oﬀ-policy updates.\nThe use of multiple updates could be justiﬁed by claiming the\nparameter updates are small enough as they constrained by the old policy and the clipping bound,\nsuch that the new policy is never too far from the old policy.\nThus, reducing the chances of\nsub-optimal convergence.\nCHAPTER 4. DEEP REINFORCEMENT LEARNING\n22\nProximal Policy Optimisation (PPO) Algorithm\nRandomly initialize policy π and value function V with parameters θ, θv respectively\nCreate Ne environments\nn - number of steps to calculate TD over\nθold ←θ\nrepeat\nfor actor 1 . . . Ne do\nfor time t . . . t + n do\nSample actions at ∼π(a|st, θ)\nExecute actions at and observe rewards rt and next states st+1\nend for\nend for\nCompute Advantages A and Returns R\nfor epoch 1, . . . , K do\nfor minibatch 1, . . . , N do\nsample minibatch transitions randomly without replacement (sj, aj, Aj, Rj)\nr(θ) =\nπ(aj|sj,θ)\nπ(aj|sj,θold)\n∇θL(θ) = E [∇θ min (r(θ)Aj, clip (r(θ), 1 −ϵ, 1 + ϵ) Aj) + βH∇θH(π(s, θ))]\n∇θvL(θv) = Eπ\nh\n∂(Rn−V (s|θv))2\n∂θv\ni\nend for\nend for\nθold ←θ\nuntil tmax\nFigure 4.3: Proximal Policy Optimisation algorithm adapted from [43]\nAs stated previously the Q Learning algorithm suﬀers from a maximisation bias and poor exploration.\nTo improve on [6], methods such as TD(n) and Double Q Learning as previously explained can be\nimplemented along with additional reinforcement learning methods such as Duelling Q Networks,\nA3C and prioritised experience replay. These provide signiﬁcant gains on the initial DQN algorithm\nas shown in [37] and [44]. Although, even after combining all these deep reinforcement learning\nmethods, deep reinforcement learning still suﬀers from sampling ineﬃciency requiring up to 50\nmillion state samples (200 million frames) in order to to converge on an policy equivalent to\nhuman-level performance in Atari-2600 games.\nFurthermore, all of these improvements fail to\nachieve near human level performance in sparse extrinsic reward games such as MontezumaRevenge\nor PrivateEye. 200 million frames is equivalent to playing ∼38 days in real-time, playing at 60fps.\nAs stated earlier this is extremely ineﬃcient learning when contrasted against humans who are\noften able to perform well in both dense and sparse reward games after a single or few attempts.\nCHAPTER 4. DEEP REINFORCEMENT LEARNING\n23\n4.2\nDeep Reinforcement Solutions to Sparse Extrinsic Rewards\n4.2.1\nCuriosity Driven Exploration via Next-State Prediction\nCuriosity driven exploration in deep reinforcement learning as seen in [8] attempts to solve the\nlack of frequent rewards by introducing a intrinsic reward function. The intrinsic reward function\nprovides rewards when the agent experiences novel states. In order to avoid the agent from seeking\nstates of high stochasticity which the agent cannot model e.g. leaves ﬂuttering in a breeze [8], the\nagent learns an exploration model only for features that the agent can interact with. The intrinsic\nreward is calculated by intrinsic curiosity model (ICM) in [8]. The ICM consists of two models\nthe inverse dynamics model which tries to predict the action at given an encoded version of states\nφ(st) and φ(st+1) and the forward model that tries to predict the next encoded state φ(st+1) using\nthe actual action taken by the agent at and the encoded state φ(st). In [15] the intrinsic reward it\nis given by the mean squared error it = |ˆφ(st) −φ(st+1)|2. By jointly optimising the inverse and\nforward model, the agent gains no beneﬁt of trying to model any part of the environment that isn’t\naﬀected by the action taken [8]. However, this introduces a problem as parts of the environment\nthat are not immediately aﬀected by the agent’s policy, may be important in the future [15].\n4.2.2\nUnsupervised Auxiliary Tasks\nThe UNsupervised REinforcement and Auxiliary Learning (UNREAL) [45] agent increases the\nsample eﬃciency of the A3C agent by extracting additional reward signals from the environment.\nFigure 4.4: Overview of the UNREAL agent, image taken from [45] with authorial permission. This\nﬁgure shows the auxiliary tasks (b) Pixel Control, (c) Reward Prediction and (d) Value Function\nReplay for the UNREAL agent.\nCHAPTER 4. DEEP REINFORCEMENT LEARNING\n24\nOne additional task is that of pixel control (PC), where an auxiliary policy Qaux is trained to\nmaximise the pixel intensity across diﬀerent sections of the image input over an n-step period as seen\nin Figure 4.4(b). The auxiliary state-action function is trained by minimising the mean-squared\nerror of the between the predicted n-step pixel change given as\nLQaux = E\n\"\u0012\nRn\nt + γn max\na′\nQaux \u0000s′, a′|θi−1\n\u0001\n−Qaux(s, a|θi)\n\u00132#\n[45].\n(4.3)\nThe other auxiliary tasks are that of reward prediction (RP), where the agent tries to predict\nthe reward received only for the next step given three preceding frames Figure 4.4(c), and ﬁnally\nthe value function replay (VR) which further trains the value function V (st) from a replay buﬀer\n4.4(d). All three auxiliary tasks are trained oﬀ-policy where input state sequences are sampled\nfrom a small experience replay buﬀer. As the sequences sampled have diﬀerent starting points to\nthe online rollouts, this allows the agent to learn new temporal relations between states [45]. Here\nthe three auxiliary tasks use the agents’ existing neural network to jointly optimise the network for\nall tasks. The total loss function is then,\nL = LA3C + λPCLQaux + λV RLV R + λRP LRP\n(4.4)\nThe auxiliary tasks do not directly aﬀect the policy π, but rather shape the features in the encoder\nnetwork which could then aﬀect the agent’s behaviour.\nHowever, Unlike the curiosity driven\nexploration the pixel control task learns to maximise the expected change in pixels, this could\npossibly indirectly lead to the agent seeking states of high stochasticity if LQaux is too large, as it\nwill bias the encoded features towards those that change the pixel intensity the most.\n4.2.3\nRandom Network Distillation\nThe novelty of a state can be also measured by the number of counts the agent has been in\nthat state, in [46] this is combined with intrinsic motivation by providing an additional reward\nsignal which is inversely proportional to the count. This method was simpliﬁed in [14] via using\na Random Network Distillation (RND). RND consists of a base policy πθ, as well as randomly\ninitialised target and predictor networks f and ˆf respectively. The target network is ﬁxed after\ninitialisation and the predictor network is trained to predict the output of the target network for\nthe next state st+1. The intrinsic reward is deﬁned as the mean squared error between the two\noutputs it = | ˆf(st+1) −f(st+1)|2 and the predictor network is trained by minimising this loss [14].\nBy using a lower dimensional representation this should reduce the eﬀect of the agent seeking states\nof high stochasticity as mentioned earlier.\nMinimising the distance between f and ˆf implicitly learns the count of the number times a state has\nbeen visited, as the predictor network cannot predict the target network output with greater than\nrandom probability without ﬁrst visiting the state. As the target network f output is deterministic\ni.e. constant, this means the output is learnable and the loss can approach at least a local minima\nthrough gradient descent. This is notably diﬀerent to a forward dynamics model, where for certain\nnon-stochastic environments such as simplistic physics based environments the next state can be\npredicted without necessarily having to have explicitly visited that state, as the forward dynamics\nCHAPTER 4. DEEP REINFORCEMENT LEARNING\n25\nmodel learns the true transition model Pa\nss′. This is likened to the agent getting bored in [8], as\nthe outcome becomes more predictable hence less novel.\n4.2.4\nHindsight Experience Replay\nAuxiliary Tasks and curiosity driven exploration both attempt to explore unknown states by\nmaximising the statistical novelty of a state compared to states the agent previously experienced.\nThis only will work if there is a variation amongst the states being explored, however in some tasks\nthe states lack diversity hence the ICM and pixel control policies will not suﬃciently be able to\nexplore the state space [47]. This is especially crucial for large state spaces with very few rewards.\nThe proposed solution uses every episode the agent performs to provide a reward even if the agent\ndoes not achieve the said goal. This is done by creating virtual goals which use the ﬁnal the state\nthe agent ended up in as the desired outcome. An example given in [47] is using a robotic arm to\nmoving an object to a speciﬁc coordinate (x,y). For a large x,y plane randomly sampling a speciﬁc\nlocation would be statistically negligible, so after every episode the agent updates it’s value function\nby ”pretending” that the ﬁnal (x,y) coordinate was the desired outcome. Hence, the agent learns\nto move the object to any speciﬁed coordinate.\nChapter 5\nEvaluation and Testing\n5.1\nEnvironments\nIn order to test the eﬃcacy of the algorithms discussed above, a suitable MDP environment must\nbe used. Several environments were used to provide a variety of environment complexities and task\ndiﬃculties, but crucially a diﬀerence in sparsity of the extrinsic reward received. The environments\ninclude that of classic control games {CartPole, Acrobot, MountainCar} and a small selection\nof Atari games {SpaceInvaders, Freeway, MontezumaRevenge} were chosen to meet the above\nrequirements, as well as their popularity so that there were plenty of reference results in the\nliterature. All environments were executed using OpenAI’s gym toolkit.\n5.1.1\nClassic Control\nThese simplistic environments represent the state of the environment with a vector s ∈Rn, with\neach feature corresponding to a physical variable of the system that the agent needs to learn to\ncontrol in order to gain rewards. The CartPole-v1 game consists of an unstable inverted pendulum\n(pole) attached to a cart and the goal is to keep the pole upright. The agent can move the cart\nleft or right via applying a discrete force of ±1 along the horizontal axis of the cart. The game\nﬁnishes when the pole moves greater than 15°either side of the upright position, moving ±2.4 units\nfrom the centre or after 500 steps. The agent receives a reward +1 for each step until the episode\nis ﬁnished [48].\nThe Acrobot-v1 environment consists of a double pendulum in which the goal is to try to swing\nthe tip of the lower pendulum to a speciﬁc target height. The agent can move the pendulum by\napplying a torque of {+1, 0, -1} to the joining link. Every step the agent receives a negative reward\nother than the target state [49].\nThe ﬁnal control task is MountainCar-v0, where the goal is to reach the top of the mountain from\nthe bottom of the valley. However, the car does not have enough power to directly drive up the\nside, therefore in order to solve the problem, the agent must gain momentum from oscillating either\nup slide of the mountain. The agent receives a -1 reward for every step the agent is not at the top\nand the episode terminates at 200 steps. Therefore the agent will receive a -200 reward unless it\n26\nCHAPTER 5. EVALUATION AND TESTING\n27\nlearns to reach the top in under 200 moves [50].\nAt ﬁrst glance these as all these tasks contain a dense extrinsic reward signal as they produce a\nnon neutral at each step, so one might expect that they should be solved easily. However, under\nfurther inspection in Acrobot and MountainCar tasks, the rewards contain very little information\nabout the state-action value, as all actions are considered equally bad regardless of how close the\nagent is to the optimal behaviour. An agent must suﬃciently explore the state space in order to\nachieve the a higher reward, hence learn from it.\n5.1.2\nAtari 2600\nFor the Atari 2600 games, OpenAI provides wrapper to the Arcade Learning Environment (ALE).\nThe ALE produces a 210x160x3 RGB image for each frame, however many frames are duplicates or\nare very similar so it has been a common practise since [6] to only consider the nth successive frames,\nrepeating the same action n times. This helps reduce the computational cost by concatenating n\nsteps in the MDP onto a single step. Another preprocessing step performed is to convert the images\nto greyscale and scale and crop the image to a size 84x84. The ﬁnal preprocessing step for the Atari\nenvironments is to concatenate the current frame with the previous k frames into a single image.\nThis solves the partially observable MDP problem, providing additional temporal information into\nthe state (such as velocity of the ball in pong) that would not be present in a static frame. This\nallows the use of convolutional neural networks to be used as the non-linear state-value function\napproximator.\nAs the environment is deterministic a random amount of ‘no-op’ (neutral) actions were taken\nat the start of each episode, this changes the initial starting state, and is done to prevent the agent\n‘memorising’ good actions from frequently visited target states. Sometimes an loss of life, done or\nterminal ﬂag is used to increase the agent’s learning eﬃciency and, performance. This means when\nbootstrapping for the estimate of the expected future return, a loss of life is treated as the terminal\nstate. The reasoning being that using the ﬂag will allow the agent to quickly learn losing a life is to\nbe avoided. Another commonly used practise is to cap the maximum number of steps an agent can\ntake in the environment before the episode terminates. This penalises idle behaviour, thus should\nmake the agent more eﬃcient. The exact environment parameters are given in 7.1\nIn SpaceInvaders a series of waves of aliens zig-zag down the screen ﬁring at the agent, the\nagent’s goal is to shoot the aliens before they reach a certain threshold height. The game increases\nin diﬃculty as the aliens increase their speed of descent as their numbers go down. a life is lost\nwhen the aliens reach the threshold and the game is ended when all 3 lives are lost or the player\nhas defeated all waves of aliens.\nIn the game of Freeway the goal is for the player to reach the other side of the road without being\nhit by the oncoming traﬃc. The player receives a reward of +1 when reaching the other side and 0\notherwise. In the game MontezumaRevenge the goal is to explore temple rooms and collect amulets.\nNavigating the diﬀerent rooms requires the collecting objects such as keys found throughout the\nlevel, as well as stringing together a series of complex movements to avoid falling and traps.\nCHAPTER 5. EVALUATION AND TESTING\n28\nThe games were chosen for varying sparsity of rewards as according to [46], SpaceInvaders provide a\nhuman-optimal environment with relatively dense reward feedback and Freeway and MontezumaRevenge\nprovide the sparse reward environments with easy and diﬃcult search respectively.\nChapter 6\nImplementation and Analysis\n6.1\nSoftware and Hardware\nPython was the language of choice for implementing these algorithms as it is widely used in the\nreinforcement learning community, such as OpenAI’s baselines [40] allowing for better troubleshooting\nand debugging. Also, Python is one the most common used API for neural network packages, such\nas the popular Tensorﬂow and Pytorch packages. All neural networks were implemented using\nTensorﬂow’s python API. Tensorﬂow is a Machine Learning framework that allows users to build\ngraphical models using ‘tensors’, Tensorﬂow’s internal representation of a generalised matrix 1,\nand provides built-in automatic diﬀerentiation, which automatically handles gradient calculation\nand backpropagation. And all the experiments were ran on either a single machine with a 8-core,\n16-thread AMD RyzenTM 1800x with a single NVIDIA® GeForce® RTX 2070 GPU and 48GB of\nDDR4 RAM, or, on a single GPU-node on the Sheﬃeld Advanced Research Computer (ShARC)\nhigh performance computing system, which contained a NVIDIA® Tesla K80 with a Intel® XeonTM\nE5-2630 v3 CPU. All references to computational speed and timing are those observed using the\nsingle machine setup.\n6.2\nScalable Reinforcement Learning\nIn order to perform the Atari experiments over 200 million frames, it is crucial that the implementation\nuse multiple workers to run the environments, these methods scale number of steps taken in the\nenvironment per second sub linearly. Many diﬀerent scalable frameworks were attempted such as\nA3C and IMPALA [38], however due to Tensorﬂow’s incompatibility with python’s multiprocessing\nlibrary these methods required multithreading, or using Tensorﬂow’s distributed training framework.\nDue python’s Global Interpretable Lock, multithreading is not truly parallel, only concurrent\nand attempted implementations using Tensorﬂow’s distributed training for a single machine were\nrelatively slow compared to results shown in the literature [38]. The A2C framework, however\nwas observed to increase the number of steps per second by a factor of 10, from ∼250 to ∼\n2300-2700 steps per second. The A2C framework reduces the need or multiple copies of the network\nby running a single step in multiple environment in parallel.\nThe implementation of running\n1Note that these tensors are distinct in notation and properties from the tensors in tensor calculus [51]\n29\nCHAPTER 6. IMPLEMENTATION AND ANALYSIS\n30\nmultiple environments synchronously in parallel was adapted from OpenAI’s implementation [40],\nand achieved this by running an individual environment in a separate process using python’s\nmultiprocessing library.\nHaving the environments run synchronously reduces the overhead of\npassing gradients between processes as well as the need to account for oﬀ-policy corrections.\nHowever, the most crucial beneﬁt is the increased eﬃciency of the use GPU when using a single\nmachine. This is as value estimations can be ran in a single batch decreasing the time spent of\ntransferring data onto the GPU, which is a large bottleneck for GPU applications.\n6.3\nEncoder Neural Networks\nAll implementations used neural networks to encode the state st onto a high dimensional feature\nvector, which then would be used to predict the value function of policy distribution from. In order\nto be consistent across experiments, every agent used the same encoder networks architectures.\nFor the Classic Control tasks the encoder network was two MLP layers of with 64 hidden units\neach, under the ReLU activation function.\nFor the Atari experiments a CNN encoder network was used, as in [13], as it is a commonly used\nto benchmark to test diﬀerent algorithms on. This CNN is parametrised as following, ﬁrst a 2D\nconvolutional layer with ﬁlter size of [8, 8] of with 32 output channels and stride [4, 4], following a 2D\nconvolutional layer with ﬁlter size [4, 4], 64 output channels and stride [2, 2]. A ﬁnal convolutional\nlayer with 64 output channels, ﬁlter size [3, 3] and stride [1, 1]. The resulting convolutional layer was\nﬂattened to a vector and a ﬁnal fully connected MLP layer with 512 hidden units. All layers were\nunder the ReLU activation function, and all convolutional layers had the ’VALID’ padding. The\nobservational input range for the policy CNN encoder is scaled from [0, 255] to [0, 1] for each pixel\nby diving by 255 to avoid to large ReLU activations as it is unbounded in the positive direction\nequation 3.9.\nBoth encoder networks use the Glorot uniform weight initialisation and zero bias initialisation.\n6.4\nA2C\nFor the Advantage-Actor Critic A2C network, The actor and the critic share the same encoder\nnetwork with the value function and unnormalised log probabilities (logits) being linear projections\n(via a fully connected MLP layer with no activation) from the encoded feature vector. To get\nthe policy distribution π, the softmax activation function was applied to the logits, resulting in a\npolicy distribution over the discrete set of available actions. The gradient of the entropy over the\npolicy distribution βH∇θH(π(s, θ)) is added to the loss function following [37] in order to provide\nregularisation of the policy such that is doesn’t converge on a sub optimal policy. By maximising the\nentropy, the uniformity of the probability distribution increases, this should increases exploration\nas previously unlikely actions are sampled with greater frequency. The strength of the entropy\nmaximisation is controlled by the parameter βH [37].\nCHAPTER 6. IMPLEMENTATION AND ANALYSIS\n31\n6.5\nSynchronous DDQN\nFor the Synchronous n-step DDQN network, the state-action value function was a linear projection\nfrom the encoder network. Here double Q-learning was used to reduce maximisation bias.\n6.6\nPPO\nThe PPO implementation uses the exact same model as the A2C model mentioned above.\nA\nseparate network for the old policy parameters θold is not used, instead the policies for each rollout\nare stored and then passed into the network during backpropagation.\n6.7\nUnsupervised Auxiliary tasks\n6.7.1\nOriginal Implementation\nA3C Policy\nIn [45] the A3C agent is parametrised by an encoded CNN-LSTM network, which takes in an RGB\ncolour image of size [84, 84] as it’s input state st for time t. It then encodes this onto a feature\nvector using a small two layer CNN following the original DQN paper implementation [6]. The\nCNN-encoded state is concatenated with the previous reward and action is then fed into a LSTM\nlayer with 256 hidden units, from this the policy and value function are projected as described in\nthe A2C implementation.\nPixel Control\nFor The pixel reward for each time step the image at time t is cropped to a the central region of\nsize 80 × 80 pixels, and subdivided into 20 × 20 cells of size 4 × 4. The pixel reward for time t is\nthen deﬁned by the absolute diﬀerence between the mean over all colour channels and all pixels for\neach cell, of the input image st and the previous image st−1.\nIn order to produce the Qaux estimate of expected pixel change from each subdivided cell region,\na MLP layer with relu is added the LSTM output layer, this is then reshaped to size 32 × 7 × 7.\nA deconvolutional layer with 32 output channels, ﬁlter size [3, 3] and stride one produces a 32 × 9\nfeature map. Two deconvolutional layers with output channels 1, number of actions are applied\nto the 9 × 9 feature map concurrently, both with ﬁlter size [2, 2]. This produces a value function\nand advantage function estimate which is used to produce the Qaux estimate via duelling neural\nnetworks given by [45]\nQaux(s, a|θ) = ReLU\n \nV (s|θ) + A(s, a|θ) −1\n|A|\nX\na′∈A\nA(s, a′|θ)\n!\n[52].\n(6.1)\nReward Prediction\nFor the reward prediction a reward rt is sampled from the experience replay in a skewed manner\nsuch that the probability mass for non-zero rewards is equal 0.5. The three proceeding images\nCHAPTER 6. IMPLEMENTATION AND ANALYSIS\n32\n[st−2, st−1, st] 2 are passed into the CNN encoder, the ﬂattened output features across all three\nimages are concatenated into a single feature vector. This is then passed into a MLP layer with\n128 hidden units with the ReLU activation function. This is then projected via another MLP layer\nonto a categorical distribution using the softmax function. The categories of this distribution are\npositive negative and neutral rewards, which should be easier to learn than the exact value of rt,\nthis gradient is calculated via cross entropy loss.\nValue Replay\nA small experience replay buﬀer stores the most recent 2000 states for each actor, the gradient of\nthe value function replay task is calculated via mean squared error in the same fashion as the base\nA3C agent.\n6.7.2\nCustom A2C-CNN implementation (UNREAL-A2C2)\nA2C Policy\nIn order to be a more accurate comparison to the base A2C agent and for greater GPU utilisation,\nan CNN-only A2C version of [45] is implemented.\nIn order to get this to work the following\nchanges were made, ﬁrst the states are converted to greyscale and stacked to the match base A2C\nand Sync-DDQN implementations. In order to be more comparable to the A2C experiments the\naction-reward concatenation to the encoded state before the value and policy was not performed,\nas this provides extra temporal information that may improve performance.\nPixel Control\nThe pixel control reward at each time step is then altered to be deﬁned as the absolute mean\ndiﬀerence across the greyscale subdivided region of the most recent image xt in the concatenated\nstate st = [xt−3, xt−2, xt−1, xt]\nrPC\nt\n= |subdivided(xt) −subdivided(xt−1)|.\nOther changes, made to the pixel control task were to not perform the cropping operation and\nsubdivide the image into 21 × 21 cells.\nThe only change this requires to the deconvolutional\nnetwork is to increase the number of size of the MLP layer before the ﬁrst feat map from size\n32 × 7 × 7 to 32 × 8 × 8. Finally the greyscale images were normalised between the values of (0,1)\nusing a simple min-max normalisation [53] in order to avoid manually ﬁtting the λPC term per\nenvironment. However, this does introduce a small non-stationarity aﬀect into calculating the pixel\nrewards as the maximum pixel value of the dataset may increase over the course of training. This\nis less pronounced for than it would be for per-channel RGB min max normalisation, as well as\nbeing more stationary that per-pixel z-score image normalisation. This is because in episodic video\ngames tasks many start in a speciﬁc location, and when progressing to diﬀerent areas often means a\nchange in environment textures, thus possibly changing the normalisation parameters signiﬁcantly.\nWhereas the change in maximum pixel intensity for a greyscaled images is expected to be negligible.\n2as rt is reward from st →st+1\nCHAPTER 6. IMPLEMENTATION AND ANALYSIS\n33\nReward Prediction\nThe reward prediction task is the same as the original implementation as described above but,\nwithout the frame concatenation as the state already provides necessary temporal information.\nInstead of sampling rewards from all workers, the worker with the highest reward is greedily\nchosen to reduce the chance of sparse rewards going unmissed, as well as increasing computational\neﬃciency.\nBecause of the noticeable modiﬁcations to the original UNREAL agent’s algorithm, to distinguish\nbetween the two algorithms, the custom agent used is hence referred to as the UNREAL A2C-CNN\nor UNREAL-A2C2 agent.\n6.8\nRND\n6.8.1\nIntrinsic Reward and Value Function\nFollowing [14], The base policy is a PPO policy with an additional separate critic is used to calculate\nthe value of the intrinsic rewards Vi(s). The weighted combination of the extrinsic and intrinsic\nadvantages is used to update the policy, and the discount factor for the extrinsic reward is increased\nto 0.999 as to match [14]. The intrinsic rewards are non-episodic, as [15] argues that this increases\nexploration, and they are normalised so that the scale is independent of the observational space [14].\nThis is achieved by dividing the intrinsic reward by the running estimate of the standard deviation\nof the intrinsic return. However the intrinsic rewards are non-episodic so the true return cannot\nbe calculated as it goes on indeﬁnitely. It is also non-stationary as it is produced by a changing\nparametrised function, so using the intrinsic value estimate to approximate the return is suspected\nto be an unstable normalisation technique. So the normalisation used in [54] is used, where the\nintrinsic reward is normalised by the standard deviation of the inversely discount rewards as the\npast rewards are calculable. This can be explicitly deﬁned as,\nit =\n\f\f\ff(st+1) −ˆf(st+1)\n\f\f\f\n2\nσ\n\" t\nX\nk=0\nγt−kit\n#\n.\nIn practise the running estimate of the standard deviation is updated after every n-step rollout and\nintrinsic return is calculated online.\n6.8.2\nTarget and Predictor networks\nAs the target network is ﬁxed, it cannot adjust to the scale of the observational inputs, thus\nnormalisation techniques are required in order ensure the output scale is consistent across diﬀerent\nenvironments [14]. The input to the predictor and target networks is the ﬁnal image xt in the\nstate st = [xt−3, xt−2, xt−1, xt], this is then normalised by subtracting the running estimate of the\nper-pixel mean and dividing it by the running estimate of the per-pixel standard deviation and\nﬁnally being clipped between the range -5, 5. However, This introduces non-stationarity to target\nCHAPTER 6. IMPLEMENTATION AND ANALYSIS\n34\nnetwork output thus further non-stationarity to the intrinsic reward. The running estimates of\nthe observational mean and standard deviations are initialised by running a random agent for 50\nrollouts for each actor and continually updated during training.\nThe target and predictor network are both separate models from the policy as to ensure bias free\nreward prediction, they both use the same convolutional encoder architecture as the policy network.\nThe target network has a ﬁnal MLP layer of size 512 with no activation after the encoder. For\nthe predictor network after the convolutional encoder it has two successive MLP layers of size 512,\nboth with the ReLU activation followed by a ﬁnal MLP layer with size 512 with no activation. The\ntarget and predictor networks use a leaky ReLU activations for the convolutional layers, as well\nas all layers being initialised with orthogonal initialisation with gain\n√\n2. This was not done to\nincrease performance but to ensure the output of the target and prediction networks were of the\nsame magnitude as the [54] implementation, so that the hyperparameters taken from [14] are valid.\n6.9\nICM\nFor curious agents using the next-state prediction, the ICM model is used to provide the additional\nreward for the PPO policy with separate intrinsic value function. The ICM is parametrised by\nadditional separate encoder network using the same architecture and weight initialisation as the\nbase policy encoder, but replacing the observational scaling with the observational normalisation as\ndescribed in the RND implementation. The output from the encoder φ(st) is concatenated the with\naction at and fed into the forward model. The forward model consists of one fully connected MLP\nlayer with ReLU activation, with the number of hidden units the same size of the encoded state,\ni.e. 512 and 64 for the CNN and MLP encoder respectively. A following MLP with no activation\nand same size of the previous layer creates the predicted next state ˆφ(st+1) [8].\nFor the inverse model the encoded state and next state φ(st) and φ(st+1) are concatenated into a\nsingle feature vector and passed into a MLP layer with ReLU activation and size 512 and 64 for\nthe CNN and MLP respectively. This is fed into a ﬁnal MLP layer with no activation that maps\nonto the unnormalised logit probability distributions over the available actions producing ˆat. The\ninverse model is trained as a classiﬁcation task using cross entropy between the predicted action\nˆat and actual action at taken at time t, as well the forward model being trained via mean squared\nerror\n\f\f\fˆφ(st+1) −φ(st+1)\n\f\f\f\n2\n[8]. Unlike the [15] implementation the forward model and the inverse\nmodel gradients are both passed back to the encoder model as in the original implementation [8],\nwith weighting βLforward + (1 −β)Linverse, with β = 0.2.\n6.10\nRandom Network Distillation with Auxiliary Learning RANDAL\nRAndom Network Distillation with Auxiliary Learning (RANDAL) is a novel method of combining\ncuriosity driven exploration with RND and auxiliary tasks was implemented as following.\nA\nbase PPO policy with RND is combined with all auxiliary tasks as described in UNREAL-A2C2\nimplementation, in order to stop the intrinsic reward from decreasing too rapidly and thus exploring\nless, a separate intrinsic value function is used as in the RND agent. This allows the value replay\nCHAPTER 6. IMPLEMENTATION AND ANALYSIS\n35\nbe performed exclusively on the extrinsic value function, preserving the rate of change in it. As the\nintrinsic reward is non-stationary, the reward prediction is done only on the sign (+, −, 0) of the\nextrinsic reward, this is done in hopes to increase stability as extrinsic rewards are stationary as\nwell as being able to continue to use the simpliﬁed categorical reward rather than the exact reward\nvalue. The target and predictor networks are identical to the RND implementation.\n6.11\nAnalysis\nFor the baseline architectures of Sync-DDQN and A2C, it is expected that A2C agents shall\noutperform Sync-DDQN agents as it is a policy gradient method. As previously mentioned policy\ngradient methods provide are theorised to have more stable convergence properties, thus should ﬁnd\na more optimal than the value based Sync DDQN agents. However, the A2C algorithm is expected\nto perform worse on sparse reward tasks as it suﬀers for poor exploration, e.g. A2C will probably\nnever learn to navigate MountainCar, Freeway or MontezumaRevenge. Again this is because it is\nan on-policy algorithm using the estimate of the optimal policy π(a|s, θ) to sample actions from\neach turn. The expectation of the Sync DDQN agents is that they should slightly underperform\ncompared to the A2C for dense reward tasks but should perform better for tasks that beneﬁt from\nrandom exploration, i.e. should outperform A2C on easy search sparse reward tasks, for the chosen\nenvironments this includes the Freeway, Acrobot and MountainCar tasks.\nPPO agents should outperform A2C and Sync-DDQN agents on all tasks, as the multiple epoch\nminibatch training should ensure sparse rewards do not get lost in the gradient update, as well as\nthe value function estimate converging to the true value of the policy quicker than the single update\nalgorithms. This reiteration of the value function is comparable to the value function replay in the\nUNREAL architecture, thus the performance of the PPO agent should be closer to the UNREAL\nagents than the A2C and Sync-DDQN agents.\nFor the curious agents, the expectation is that the ICM and RND agents should perform equally\nwell for high dimensional complex inputs, such as image observations in Atari games. This is because\nthe forward model will not converge too quickly, providing similar amounts of rewards as the RND\nagents. For low dimensional non-complex environments, ICM agents are expected the magnitude\nof the intrinsic rewards is expected to decrease quickly as the observations are easily predictable,\neven for unseen states.\nChapter 7\nResults and Discussion\n7.1\nTesting and Validation\nFor the Atari experiments the number of frames skipped (actions repeated) was 4, meaning each\nenvironment step is equivalent to 4 frames. The number of frames stacked per state was 4, the\nexperiments were ran over 50 million steps, or 200 million frames as according to [6] and the episode\nterminated after 4500 steps as to match [14]. The number of ‘no-op’ actions was a random number\nbetween 0 and 7 steps, or 0-28 frames. The agent was validated every 1 million steps and the score\nwas averaged over 50 episodes. During validation the episode was terminated after 10,000 steps\napproximately 11 minutes of real-time play to avoid games such as MontezumaRevenge carrying\non indeﬁnitely if the agent is stationary.\nFor the Classic Control experiments the ran over 2 million steps, the agent was validated following\nthe same procedure as the Atari experiments except validated every 40,000 steps.\n7.2\nAtari Experiments\nDue to the computational complexity and limited time of this project, no hyperparameter search\nwas completed for the Atari domains, and the hyperparameters were taken from existing work in\nthe literature. All agents use a gradients calculated with a clipping by normalisation value of 0.5\n[55] to help stabilise gradient updates. All ﬁgures show the mean score over 3 random starts, ±\nthe standard deviation of the runs.\n7.2.1\nBaselines\nThe selected environments were all run for both the Synchronous n-step Double DQN and the A2C\narchitectures as to provide a baseline to compare the more advanced algorithms to. As [43] and [14]\nuses Adam optimisation, in order for the baselines A2C and Sync-DDQN to be a fairer comparison,\nAdam is used for both of these agents, with ϵ = 1 × 10−8 and βm = 0.9, βv = 0.999. Although,\nRMSProp is used in the original implementation [37].\n36\nCHAPTER 7. RESULTS AND DISCUSSION\n37\nHyperparameter\nValue\nOptimiser\nAdam\nLearning rate\n1 × 10−3\nNumber of actors\n32\nEntropy coeﬃcient\n0.011\nValue coeﬃcient\n0.52\nn-step period\n51\nDiscount factor γ\n0.991\nGradient norm clipping\n0.52\nTable\n7.1:\nA2C\nAtari\nbaseline\nhyperparameters, taken from 1[37], 2[40]\nHyperparameter\nValue\nOptimiser\nAdam\nLearning rate\n1 × 10−3\nNumber of actors\n32\nTarget Network Period\n10, 000 steps 3\nInitial ε\n11\nFinal ε\n0.011\nεtest\n0.011\nn-step period\n51\nDiscount factor γ\n0.991\nGradient norm clipping\n0.52\nTable 7.2: Synchronous Double DQN Atari\nbaseline hyperparameters, taken from 1[37],\n2[40], 3[41]\nIn the Atari experiments for the value based Double DQN the exploration rate ε was linearly\nannealed over 2 million steps to provide suﬃcient exploration.\nFigure 7.1: Atari Baselines, results show a mean validation score ± standard deviation over 3\nrandom parameter initialisations run of the respective models for the number of steps in millions.\nThe ﬁgure shows that the policy gradient A2C leads to better convergence for dense reward tasks\nhowever suﬀers from poor exploration and thus cannot learn to solve sparse reward tasks. This\nis contrasted against the Sync-DDQN which can solve the easy search sparse task Freeway with\nrandom search, however fails to learn from the more complex MontezumaRevenge sparse reward\ntask.\n7.2.2\nλ-Return and PPO\nIn order to better connect the future rewards to the value of the current state V (st), the n-step\nperiod was increased to 20 to match the [45] implementation, and to avoid having to manually ﬁt\nthe n-step period for each experiment the λ-return from equation 2.16 and GAE from equation\n2.26 were used for the value-based and policy based methods respectively, with a value of λ = 0.95\ntaken from [22]. For the PPO experiments, the n-step period was further increased to 128 to match\nthe value used in [15] and [14].\nCHAPTER 7. RESULTS AND DISCUSSION\n38\nHyperparameter\nValue\nOptimiser\nAdam1\nLearning rate\n1 × 10−4\nNumber of actors\n32\nEntropy coeﬃcient\n0.011\nValue coeﬃcient\n0.5\nn-step period\n1282\nNumber of epochs\n42\nNumber of minibatches\n42\nDiscount factor γ\n0.991\nPPO clip range\n[0.9, 1.1]2\nGradient norm clipping\n0.53\nTable 7.3: PPO Atari hyperparameters, taken from 1[43], 2[14] 3[40]\nFigure 7.2: Atari experiments for PPO with GAE vs A2C with GAE vs Sync-DDQN(λ). The\ncombination of increased n-step period and use of λ return, signiﬁcantly increase performance in\nthe dense reward environment over the initial TD(5) return however, show no performance gains for\nthe sparse reward tasks. Surprisingly the PPO agent does not outperform the A2C and Sync-DDQN\nagents on the easy search sparse reward task, where it was expected to outperform on all three\ntasks according to the results obtained in [43].\n7.2.3\nSparse Reward Solutions\nThe three sparse reward solutions being tested are, the UNREAL-A2C2 agent, the RND with\nPPO, and the novel RANDAL with PPO. All agents use the respective n-step period from their\nbase policies in section 7.2.2 and all use GAE to improve performance. Due to the signiﬁcantly\nslower algorithm implementation and time constraints of the project, a curiosity driven agent using\nthe ICM was not tested for the Atari experiments.\nCHAPTER 7. RESULTS AND DISCUSSION\n39\nHyperparameter\nValue\nOptimiser\nAdam1\nLearning rate\n1 × 10−4 (1)\nNumber of actors\n32\nEntropy coeﬃcient\n0.0011\nValue coeﬃcient\n0.5\nn-step period\n1281\nNumber of epochs\n41\nNumber of minibatches\n41\nExtrinsic advantage coeﬃcient\n2.01\nIntrinsic advantage coeﬃcient\n1.01\nExtrinsic discount factor γe\n0.9991\nIntrinsic discount factor γi\n0.991\nPPO clip range\n[0.9, 1.1]1\nGradient norm clipping\n0.5\nTable 7.4:\nRND Atari hyperparameters,\ntaken from 1[14]\nHyperparameter\nValue\nOptimiser\nAdam\nLearning rate\n1 × 10−3 (1)\nNumber of actors\n32\nEntropy coeﬃcient\n0.0011\nValue coeﬃcient\n0.51\nn-step period\n201\nDiscount factor γ\n0.991\nGradient norm clipping\n0.5\nReward prediction coeﬃcient\n11\nValue replay coeﬃcient\n11\nPixel Control coeﬃcient\n1\nReplay length per actor\n20001\nTable\n7.5:\nUNREAL-A2C2\nAtari\nhyperparameters taken from 1[45]\nHyperparameter\nValue\nOptimiser\nAdam1\nLearning rate\n1 × 10−4\nNumber of actors\n32\nEntropy coeﬃcient\n0.001\nValue coeﬃcient\n0.5\nn-step period\n128\nNumber of epochs\n4\nNumber of minibatches\n4\nExtrinsic advantage coeﬃcient\n2.0\nIntrinsic advantage coeﬃcient\n1.0\nExtrinsic discount factor γe\n0.999\nIntrinsic discount factor γi\n0.99\nPPO clip range\n[0.9, 1.1]\nGradient norm clipping\n0.5\nReward prediction coeﬃcient\n1\nValue replay coeﬃcient\n1\nPixel Control coeﬃcient\n1\nReplay length per actor\n2000\nTable 7.6: RANDAL Atari hyperparameters\nCHAPTER 7. RESULTS AND DISCUSSION\n40\nFigure 7.3: Atari experiments for RND vs UNREAL-A2C2 vs RANDAL agents. All agents are\nsuperior in all environments over the traditional reinforcement learning agents and are equal\nin the dense reward task.\nHowever, the RND and RANDAL excel in the sparse reward tasks\nsigniﬁcantly outperforming the UNREAL-A2C2 agent in both the easy and hard search, sparse\nreward environments. RANDAL shows a less stable mean score than the RND, with large spikes\nin performance in successive validation scores, however shows a lower variance between the runs\nfor both sparse reward tasks.\nThe UNREAL-A2C2 agent shows increased sample eﬃciency over the original implementation\n[45, Figure 6] for MontezumaRevenge achieving the score of 500 at around 25 million steps as\nopposed to 50 million in [45, Figure 6]. However, the UNREAL-A2C2 agent’s performance plateaus\nat this score whilst the original implementation increases approximately linearly. The diﬀerence\nbetween these two agents could due to one of many changes. As using GAE is shown to generally\nincrease performance [22], the suspect list is narrowed down to the inclusion of the previous action\nand reward to the policy as well as the use of a LSTM network.\nA clear parallel between these two methods can be seen when contrasting them from a\nmodel-based/model-free viewpoint. Both RND and UNREAL agents can be said to utilise environment\nmodels to increase performance, as UNREAL agents learn a reward model p(rt|st), unconditioned\non the action i.e.\np(rt|st) =\nX\na∈A\np(a|st)(rt|st, a) =\nX\na∈A\np(a|st)Ra\ns\n(7.1)\nand RND agents learn a random feature model of the environment which can be said to be learning a\nsimplistic density model of regions explored by the agent. This interplay between model-based and\nmodel-free reinforcement learning, seems to show increased sample eﬃciency and performance over\npurely model-free agents, again this is empirically supported by [56] as well as in Figure 7.3. This\ntheory is also backed by biological plausibility as model-based learning is said to be ‘ubiquitous’ in\nanimal brains [57].\nQualitative analysis of trained agents show that all sparse reward solutions explore the same 3\nrooms in MontezumaRevenge, and the diﬀerence in the score is that the RND and RANDAL agents\nlearn to use a collected sword to kill any enemy resulting in a large score increase. An interesting\nobservation was that after the agents had reached the maximum total score, that they were capable\nof achieving, the RND agent aimlessly wanders the surrounding rooms and the UNREAL-A2C2\nand RANDAL agents spend the remainder of the episode transitioning between two rooms. These\ncan be explained as the RND agent is trying to maximise the intrinsic reward for the rest of the\nepisode and the UNREAL-A2C2 agent being ‘interested’ in the rooms transitions as they provide\nlarge pixel changes. The RANDAL agent shows a mix between these two behaviours, ﬁrst moving\nCHAPTER 7. RESULTS AND DISCUSSION\n41\nback to the ﬁrst room then for the remainder of the episode, it transitions between two rooms.\n7.3\nClassic Control Experiments\nFor the Classic Control Experiments the ﬁgures show the mean score ± the standard deviation for\n5 diﬀerent random starts.\n7.3.1\nAdam Hyperparameter Search\nInitial results from the Classic Control environments using hyperparameters from Table 7.1 and\nTable 7.2 and Adam optimisation, showed relatively poor performance, so a small hyperparameter\nsearch was completed. For the A2C algorithm a random search was performed for 50 combinations\nof the entropy and learning rate with diﬀerent random starts. The hyperparameters were sampled\nfrom a LogUniform(10−3, 1) and LogUniform(10−5, 10−2) for the entropy and learning rate respectively.\nFigure 7.4: Classic Control A2C hyperparameter search, results show the Adam learning rate vs\nentropy with the colour representing the average of the last 5 validation scores of the agent, as this\nincludes a measure of stability of the agent. It can be seen that learning rates that are optimal for\nboth CartPole and Acrobot lie between the values of 2 × 10−4 and 4 × 10−4 and optimal entropy\nlies between 5 × 10−3 and 2 × 10−2.\nThe learning rates in Figure 7.4 have a larger aﬀect on performance than the entropy coeﬃcient,\nthis is more pronounced in the Acrobot results where a large range of entropy values provide\ngood performance. However even large entropy regularisation wasn’t enough to for any the agents\nsampled to achieve any score in the sparse reward MountainCar task.\nFor the Classic Control Sync-DDQN agents, the exploration rate was linearly annealed over 80,000\nsteps, as to be proportional to the Atari experiments i.e. linearly annealed over 1/25th of training.\nThese were done to try to keep the hyper-parameters consistent or proportional across all tasks as\nto reduce the number of variables that could be aﬀecting the results. The agents are validated\nusing a εtest of value 0.01 for all Sync-DDQN agents.\nThe learning rate is sampled from a\nLogUniform(5 × 10−5, 10−2) distribution and the ﬁnal ε is chosen to be either 0.1 or 0.01 with\nequal probability.\nCHAPTER 7. RESULTS AND DISCUSSION\n42\nFigure 7.5: Classic Control Sync-DDQN hyperparameter search, results show the Adam learning\nrate vs ﬁnal ε value over 30 samples, with the colour representing the average of the last 5 validation\nscores of the agent. The results are less clear than 7.4, however it can be seen that both the CartPole\nand MountainCar agents suﬀer from poor exploration, as higher performance is seen for the higher\nﬁnal ε value for CartPole and the MountainCar agents perform poorly, when they were expected\nto be able solve the task through suﬃcient random search.\n7.3.2\nAdam vs RMSProp\nFor the A2C agent the learning rate and entropy coeﬃcient were selected from the results shown in\nFigure 7.4, to perform well over both the CartPole and Acrobot environments. These values were\nselected to be 2 × 10−4 and 0.005 for the learning rate and entropy coeﬃcient respectively.\nFor the Sync-DDQN agent the only values shared that perform well in Figure 7.5 for both\nCartPole and Acrobot. This is where the learning rate of 2 × 10−4 and ﬁnal exploration value\nε = 0.1. As mentioned In Figure 7.5, the annealing the learning rate over 80,000 steps does not\nseem to provide enough exploration, so the number of steps were increased, so that the ε was\nannealed from 1 to 0.1 over half of training i.e. 1 million steps.\nThe initial optimised Adam results, showed large dips in performance around the 500,000 step\nmark for both CartPole and Acrobot experiments, where the gradients have signiﬁcantly overshot\na local maximum, causing performance to drastically decrease. This was expected to be due to the\nmomentum from the Adam optimiser causing the weights to be ‘slingshot’ further from the global\nmaxima. To test this, the RMSProp optimiser which has no momentum was used to compare the\nresults, for the RMSProp agents the learning rates and entropy coeﬃcient hyperparameters taken\nfrom the literature, as seen in Table 7.1 and Table 7.2, and the ϵ = 1 × 10−5, βv = 0.9\nCHAPTER 7. RESULTS AND DISCUSSION\n43\n(a) Optimised Adam Agents\n(b) RMSProp Agents\nFigure 7.6: Classic Control baselines Adam vs RMSProp for A2C and Sync-DQQN agents. The\nresults show that for low dimensional RL problems, RMSProp optimisation is signiﬁcantly more\nstable and has faster convergence than Adam.\nThe results also show that the policy gradient\nmethod is more stable than the value based method as the rewards ﬂuctuate less over the course\nof training, matching up with the theory.\nHowever an unexpected result is the MountainCar\nenvironment which was expected to be solvable for the Sync DDQN through increased exploration\nover the initial results in Figure 7.5.\nUnlike the in Atari Freeway environment as seen in Figure 7.1, both algorithms are unable to\nsuccessfully navigate the MountainCar environment in Figure 7.6. Even with suﬃcient exploration\nfor the Sync-DDQN agents the sparsity of the reward provides it very diﬃcult for the agent to\nconnect the series of actions to the reward received when reaching the target.\n7.3.3\nλ-Return and PPO\nFor the Sync-DDQN(λ) and A2C-GAE agents all hyperparameters were kept constant as increasing\nthe step size decreased sample eﬃciency and performance, due to n=20 being much closer to the\nlength of the episode (200 steps) in the Classic Control environments, compared to the Atari\nexperiments. For simplicity and fairness, the PPO agents share many of the same hyperparameters\nof the A2C agents, and can be seen in Table 7.7\nCHAPTER 7. RESULTS AND DISCUSSION\n44\nHyperparameter\nValue\nOptimiser\nRMSProp\nLearning rate\n1 × 10−3\nNumber of actors\n32\nEntropy coeﬃcient\n0.01\nValue coeﬃcient\n0.5\nn-step period\n5\nNumber of epochs\n4\nNumber of minibatches\n1\nDiscount factor γ\n0.99\nPPO clip range\n[0.9, 1.1]\nGradient norm clipping\n0.5\nTable 7.7: PPO Classic Control hyperparameters\nFigure 7.7: Classic Control A2C-GAE vs Sync-DDQN(λ) vs PPO. The results show that the GAE\nslightly decreases sample eﬃciency for the A2C Acrobot agents, and Sync-DDQN CartPole agents.\nPPO, show greater sample eﬃciency and stability than all A2C and Sync-DDQN agents, achieving\na perfect score from 0.5 million steps onwards for the CartPole environment.\nThe results from Figure 7.7 can be explained by the Rλ and GAE, being more complex functions\nto model, thus slightly decreasing the learning speed of the agents.\n7.3.4\nSparse Reward Solutions\nDue to the nature of the pixel control task, the UNREAL-A2C and RANDAL agents only use the\nvalue replay and reward prediction tasks. All PPO-based agents (RANDAL, RND, ICM) use the\nsame base policy hyperparameters as the Classic Control PPO agents seen in Table 7.7, with the\nγe and γi and other algorithm speciﬁc hyperparameters from Tables 7.4 and 7.6.\nCHAPTER 7. RESULTS AND DISCUSSION\n45\nFigure 7.8: Classic Control experiments for RND vs UNREAL-A2C2 vs RANDAL agents. The\nresults surprisingly show no signiﬁcant gain over the PPO agents, and the UNREAL-A2C2 Acrobot\nresults show the importance of the aﬀect that bad initialisation can have on deep RL agents.\nSurprisingly Figure 7.8, shows that auxiliary reward prediction and curiosity driven exploration\nare not enough for agents to suﬃciently explore the MountainCar environment, this could be due\nto the normalisation techniques in curiosity agents or the intrinsic rewards are not enough to\nincentivise the agent to explore diﬀerent methods of hill climbing before the parameters get stuck\nin a suboptimal stationary point.\nThe CartPole and Acrobot environments are clearly limiting benchmarks, easily solved by traditional\nagents, thus providing little information on the eﬀectiveness on more complex algorithms. The\nMountainCar environment provides a diﬃcult benchmark to reach for neural network parametrised\nagents. This is as neural networks often work better on higher dimensional data and require large\namounts of data to converge compared to other models. The frequency of the reward is too small for\nthese agents to suﬃciently learn, another explanation is that the neural network agents are being\nstuck in a steep local stationary point where even after experiencing positive reward the update is\nnot large enough to escape this point. One way to solve this problem would be to use a form of\nprioritised experience replay with large initial random exploration, to ensure the agent experiences\nenough rewards earlier on on the training.\nChapter 8\nConclusions and Further Work\nLarge performance gains in deep learning have often been attributed to increased computational\npower, however fundamental diﬃculties of sparse reward reinforcement learning have not been\nsolved brute strength so far.\nInstead it has often relied on more intelligent exploration and\nexploitation algorithms, inspired by solutions found in nature, as reinforcement learning has had\nsigniﬁcant input from the ﬁelds of neuroscience and psychology.\nThis report has shown that the combination and interplay of model-based and model-free\nreinforcement learning techniques in non-traditional ways, often increases sample eﬃciency and\nperformance. This report has also shown that speciﬁcally ﬁnding reinforcement learning solutions\nthat work well across multiple sparse reward tasks, generally increases performance across all tasks.\n8.1\nFurther Work\nWith the increased in performance of the UNREAL-A2C2, RND and novel RANDAL agents in\nsparse tasks, further work could be done to combine diﬀerent aspects of these algorithms. For\nthe UNREAL-A2C2 further investigation could be done to determine the compare the diﬀerences\nbetween this implementation vs the original, such as the use of GAE, Adam vs RMSprop optimisers,\nand the impact the feeding temporal information such as previous actions and rewards.\nFor the RANDAL agents a hyperparameter search could be done to optimise the weighting of\nthe auxiliary tasks with the intrinsic reward. Combining intrinsic and extrinsic rewards for the\nreward prediction task is also an avenue for further exploration for the RANDAL architecture.\nFollowing the successes of the interplay between model-based and model-free learning a proposed\ndirection is to combine model-based imagination used in [56] with intrinsic motivation and auxiliary\ntasks.\n46\nBibliography\n[1] R. A. Wise and P.-P. Rompre, “Brain dopamine and reward,” Annual Review of Psychology,\nvol. 40, no. 1, pp. 191–225, 1989. PMID: 2648975.\n[2] C. Sammut and G. I. Webb, eds., Samuel’s Checkers Player, pp. 881–881.\nBoston, MA:\nSpringer US, 2010.\n[3] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert,\nL. Baker, M. Lai, A. Bolton, et al., “Mastering the game of go without human knowledge,”\nNature, vol. 550, no. 7676, p. 354, 2017.\n[4] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The arcade learning environment:\nAn evaluation platform for general agents,” CoRR, vol. abs/1207.4708, 2012.\n[5] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets, M. Yeo, A. Makhzani,\nH. K¨uttler, J. Agapiou, J. Schrittwieser, et al., “Starcraft ii: A new challenge for reinforcement\nlearning,” arXiv preprint arXiv:1708.04782, 2017.\n[6] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A.\nRiedmiller, “Playing atari with deep reinforcement learning,” CoRR, vol. abs/1312.5602, 2013.\n[7] R. Dubey, P. Agrawal, D. Pathak, T. L. Griﬃths, and A. A. Efros, “Investigating human priors\nfor playing video games,” in ICML, 2018.\n[8] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven exploration by\nself-supervised prediction,” in ICML, 2017.\n[9] C. M. Bishop, Pattern recognition and machine learning. springer, 2006.\n[10] P. A. Gagniuc, Markov chains: from theory to implementation and experimentation. John\nWiley & Sons, 2017.\n[11] C. C. White, Markov decision processesMarkov decision processes, pp. 484–486. Boston, MA:\nSpringer US, 2001.\n[12] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. The MIT Press,\nsecond ed., 2018.\n[13] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,\nM. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep\nreinforcement learning,” Nature, vol. 518, no. 7540, p. 529, 2015.\n47\nBIBLIOGRAPHY\n48\n[14] Y. Burda, H. Edwards, A. J. Storkey, and O. Klimov, “Exploration by random network\ndistillation,” CoRR, vol. abs/1810.12894, 2018.\n[15] Y. Burda, H. Edwards, D. Pathak, A. J. Storkey, T. Darrell, and A. A. Efros, “Large-scale\nstudy of curiosity-driven learning,” CoRR, vol. abs/1808.04355, 2018.\n[16] D. Silver, “Lecture 2:\nMarkov decision process.” http://www0.cs.ucl.ac.uk/staff/d.\nsilver/web/Teaching_files/MDP.pdf.\n[17] R. Bellman et al., “The theory of dynamic programming,” Bulletin of the American\nMathematical Society, vol. 60, no. 6, pp. 503–515, 1954.\n[18] E. Vasilaki, “Lecture notes in adaptive intelligence,” Department of Computer Science,\nUniversity of Sheﬃeld, 2019.\n[19] D. Silver, “Lecture 4: Model free prediction.” http://www0.cs.ucl.ac.uk/staff/d.silver/\nweb/Teaching_files/MC-TD.pdf.\n[20] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Machine Learning, vol. 8, pp. 279–292, May\n1992.\n[21] H. V. Hasselt, “Double q-learning,” in Advances in Neural Information Processing Systems\n23 (J. D. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, eds.),\npp. 2613–2621, Curran Associates, Inc., 2010.\n[22] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-dimensional continuous\ncontrol using generalized advantage estimation,” arXiv preprint arXiv:1506.02438, 2015.\n[23] R. Morris, “D.o. hebb: The organization of behavior, wiley: New york; 1949,” Brain Research\nBulletin, vol. 50, no. 5, p. 437, 1999.\n[24] D. E. Rumelhart, G. E. Hinton, R. J. Williams, et al., “Learning representations by\nback-propagating errors,” Cognitive modeling, vol. 5, no. 3, p. 1, 1988.\n[25] K. Hornik, “Approximation capabilities of multilayer feedforward networks,” Neural Networks,\nvol. 4, no. 2, pp. 251 – 257, 1991.\n[26] Y. LeCun, “A theoretical framework for back-propagation,” in Proceedings of the 1988\nConnectionist Models Summer School (D. Touretzky, G. Hinton, and T. Sejnowski, eds.),\n(CMU, Pittsburgh, Pa), pp. 21–28, Morgan Kaufmann, 1988.\n[27] W. Hossack, “Fourier transform,” 2019.\n[28] I. H. Witten, E. Frank, M. A. Hall, and C. J. Pal, “Chapter 10 - deep learning,” in Data\nMining (Fourth Edition) (I. H. Witten, E. Frank, M. A. Hall, and C. J. Pal, eds.), pp. 417 –\n466, Morgan Kaufmann, fourth edition ed., 2017.\n[29] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer neural networks,” in Proceedings\nof the fourteenth international conference on artiﬁcial intelligence and statistics, pp. 315–323,\n2011.\nBIBLIOGRAPHY\n49\n[30] C. E. Shannon, “A mathematical theory of communication,” Bell system technical journal,\nvol. 27, no. 3, pp. 379–423, 1948.\n[31] J. Shore and R. Johnson, “Axiomatic derivation of the principle of maximum entropy and\nthe principle of minimum cross-entropy,” IEEE Transactions on Information Theory, vol. 26,\npp. 26–37, January 1980.\n[32] J. Shore and R. Johnson, “Properties of cross-entropy minimization,” IEEE Transactions on\nInformation Theory, vol. 27, no. 4, pp. 472–482, 1981.\n[33] C. D. Sa,\n“Non-convex optimization.” http://www.cs.cornell.edu/courses/cs6787/\n2017fa/Lecture7.pdf.\n[34] D. E. Rumelhart, G. E. Hinton, R. J. Williams, et al., “Learning representations by\nback-propagating errors,” Cognitive modeling, vol. 5, no. 3, p. 1, 1988.\n[35] G. Hinton, N. Srivastava, and K. Swersky, “Neural networks for machine learning lecture 6a\noverview of mini-batch gradient descent,” Cited on, vol. 14, p. 8, 2012.\n[36] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint\narXiv:1412.6980, 2014.\n[37] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver,\nand K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” CoRR,\nvol. abs/1602.01783, 2016.\n[38] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu,\nT. Harley, I. Dunning, S. Legg, and K. Kavukcuoglu, “IMPALA: scalable distributed deep-rl\nwith importance weighted actor-learner architectures,” CoRR, vol. abs/1802.01561, 2018.\n[39] Y. Wu, E. Mansimov, S. Liao, R. B. Grosse, and J. Ba, “Scalable trust-region method for deep\nreinforcement learning using kronecker-factored approximation,” CoRR, vol. abs/1708.05144,\n2017.\n[40] P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor,\nY. Wu, and P. Zhokhov, “Openai baselines.” https://github.com/openai/baselines, 2017.\n[41] H. van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with double q-learning,”\nCoRR, vol. abs/1509.06461, 2015.\n[42] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust region policy\noptimization,” CoRR, vol. abs/1502.05477, 2015.\n[43] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy\noptimization algorithms,” CoRR, vol. abs/1707.06347, 2017.\n[44] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan,\nB. Piot, M. G. Azar, and D. Silver, “Rainbow: Combining improvements in deep reinforcement\nlearning,” CoRR, vol. abs/1710.02298, 2017.\nBIBLIOGRAPHY\n50\n[45] M. Jaderberg,\nV. Mnih,\nW. M. Czarnecki,\nT. Schaul,\nJ. Z. Leibo,\nD. Silver,\nand\nK. Kavukcuoglu,\n“Reinforcement learning with unsupervised auxiliary tasks,”\nCoRR,\nvol. abs/1611.05397, 2016.\n[46] M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos, “Unifying\ncount-based exploration and intrinsic motivation,” CoRR, vol. abs/1606.01868, 2016.\n[47] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,\nP. Abbeel, and W. Zaremba, “Hindsight experience replay,” CoRR, vol. abs/1707.01495, 2017.\n[48] “Cartpole-v1 documentation.” https://gym.openai.com/envs/CartPole-v1.\nAccessed:\n2019-08-25.\n[49] “Acrobot-v1\ndocumentation.”\nhttps://gym.openai.com/envs/Acrobot-v1.\nAccessed:\n2019-08-25.\n[50] “Mountaincar-v0\ndocumentation.”\nhttps://gym.openai.com/envs/MountainCar-v0.\nAccessed: 2019-08-25.\n[51] S. Laue, M. Mitterreiter, and J. Giesen, “Computing higher order derivatives of matrix and\ntensor expressions,” in Advances in Neural Information Processing Systems, pp. 2750–2759,\n2018.\n[52] Z. Wang, N. de Freitas, and M. Lanctot, “Dueling network architectures for deep reinforcement\nlearning,” CoRR, vol. abs/1511.06581, 2015.\n[53] S. Aksoy and R. M. Haralick, “Feature normalization and likelihood-based similarity measures\nfor image retrieval,” Pattern Recognition Letters, vol. 22, no. 5, pp. 563 – 582, 2001.\nImage/Video Indexing and Retrieval.\n[54] Y. Burda, H. Edwards, A. J. Storkey, and O. Klimov, “Random network distillation.” https:\n//github.com/openai/random-network-distillation, 2019.\n[55] “Clip by normalised gradient.” https://www.tensorflow.org/api_docs/python/tf/clip_\nby_norm, 2019.\n[56] G.\nKalweit\nand\nJ.\nBoedecker,\n“Uncertainty-driven\nimagination\nfor\ncontinuous\ndeep\nreinforcement learning,” in Proceedings of the 1st Annual Conference on Robot Learning\n(S. Levine, V. Vanhoucke, and K. Goldberg, eds.), vol. 78 of Proceedings of Machine Learning\nResearch, pp. 195–206, PMLR, 13–15 Nov 2017.\n[57] “The ubiquity of model-based reinforcement learning,” Current Opinion in Neurobiology,\nvol. 22, no. 6, pp. 1075 – 1081, 2012. Decision making.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-10-21",
  "updated": "2019-11-11"
}