{
  "id": "http://arxiv.org/abs/1910.11914v3",
  "title": "On the convergence of projective-simulation-based reinforcement learning in Markov decision processes",
  "authors": [
    "Walter L. Boyajian",
    "Jens Clausen",
    "Lea M. Trenkwalder",
    "Vedran Dunjko",
    "Hans J. Briegel"
  ],
  "abstract": "In recent years, the interest in leveraging quantum effects for enhancing\nmachine learning tasks has significantly increased. Many algorithms speeding up\nsupervised and unsupervised learning were established. The first framework in\nwhich ways to exploit quantum resources specifically for the broader context of\nreinforcement learning were found is projective simulation. Projective\nsimulation presents an agent-based reinforcement learning approach designed in\na manner which may support quantum walk-based speed-ups. Although classical\nvariants of projective simulation have been benchmarked against common\nreinforcement learning algorithms, very few formal theoretical analyses have\nbeen provided for its performance in standard learning scenarios. In this\npaper, we provide a detailed formal discussion of the properties of this model.\nSpecifically, we prove that one version of the projective simulation model,\nunderstood as a reinforcement learning approach, converges to optimal behavior\nin a large class of Markov decision processes. This proof shows that a\nphysically-inspired approach to reinforcement learning can guarantee to\nconverge.",
  "text": "arXiv:1910.11914v3  [cs.LG]  12 Nov 2020\nOn the convergence of projective-simulation-based reinforcement learning in Markov\ndecision processes\nW. L. Boyajian,1 J. Clausen,1 L. M. Trenkwalder,1 V. Dunjko,2, 3 and H. J. Briegel1, 4\n1Institute for Theoretical Physics, University of Innsbruck, 6020 Innsbruck, Austria\n2Max Planck Institute of Quantum Optics, 85748 Garching, Germany\n3LIACS, Leiden University, Niels Bohrweg 1, 2333 CA Leiden, The Netherlands\n4Department of Philosophy, University of Konstanz, 78457 Konstanz, Germany\n(Dated: November 13, 2020)\nAbstract.\nIn recent years, the interest in leveraging quantum eﬀects for enhancing machine\nlearning tasks has signiﬁcantly increased. Many algorithms speeding up supervised and unsuper-\nvised learning were established. The ﬁrst framework in which ways to exploit quantum resources\nspeciﬁcally for the broader context of reinforcement learning were found is projective simulation.\nProjective simulation presents an agent-based reinforcement learning approach designed in a man-\nner which may support quantum walk-based speed-ups. Although classical variants of projective\nsimulation have been benchmarked against common reinforcement learning algorithms, very few\nformal theoretical analyses have been provided for its performance in standard learning scenarios.\nIn this paper, we provide a detailed formal discussion of the properties of this model. Speciﬁcally, we\nprove that one version of the projective simulation model, understood as a reinforcement learning\napproach, converges to optimal behavior in a large class of Markov decision processes. This proof\nshows that a physically-inspired approach to reinforcement learning can guarantee to converge.\nKeywords: Reinforcement Learning; Convergence Proof; Projective Simulation; Markov Decision Process;\nPhysics-inspired Artiﬁcial Intelligence.\nI.\nINTRODUCTION\nIn the past decade, quantum information science es-\ntablished itself as a fruitful research ﬁeld that leverages\nquantum eﬀects to enhance communication and informa-\ntion processing tasks bookNielsen, Bennett1995. The re-\nsults and insights gained inspired further investigations\nwhich more recently contributed to the emergence of the\nﬁeld quantum machine learning [3–5]. The aim of this\nnew ﬁeld is twofold. On the one hand, machine learn-\ning methods are developed to further our understanding\nand control of physical systems and, on the other hand,\nquantum information processing is employed to enhance\ncertain aspects of machine learning. A learning frame-\nwork that features in both aspects of quantum machine\nlearning is projective simulation (PS). In particular, PS\ncan be seen as a platform for the design of autonomous\n(quantum) learning agents [6].\nThe development of projective simulation is not moti-\nvated by the aim of designing ever-faster computer algo-\nrithms. Projective simulation is a tool for understanding\nvarious aspects of learning, where agents are viewed from\nthe perspective of realizable entities such as robots or\nbiological systems interacting with an unknown environ-\nment. In this embodied approach, the agent’s perception\nis inﬂuenced by its sensors, its actions are limited by its\nphysical capabilities and its memory is altered by its in-\nteraction with an environment. The deliberation process\nof the agent can be described by a random walk process\non the memory structure and it is their quantum counter-\npart, quantum random walks, that oﬀers a direct route\nto the quantization of the deliberation and learning pro-\ncess. Thereby, PS not only allows us to study learning in\nthe quantum domain it also oﬀers speed-ups in a variety\nof learning settings [7, 8].\nProjective simulation can be used to solve reinforce-\nment learning (RL) problems as well. Taken as a classi-\ncal RL approach, the PS has proven to be a successful\ntool for learning how to design quantum experiments [9].\nIn [9] PS was used to design experiments that generate\nhigh-dimensional multipartite entangled photonic states.\nThe ability of PS to learn and adapt to an unknown en-\nvironment was further used for optimizing and adapting\nquantum error correction codes [10]. In a quite diﬀerent\ncontext, PS is used to model complex skill acquisition in\nrobotics [11, 12].\nAlthough PS has been shown suitable for a number\nof applications, it is a fair question of just how well it\ndoes, compared to other models, or compared to theo-\nretical optima. However, the empirical evaluation of a\nmodel through simulations, and analytically proving the\nproperties of the same model are fundamentally distinct\nmatters. For example, in many applications, empirical\nconvergence can be reached even if the conditions for\ntheoretical convergence are not met. In any real-world\napplication, such as learning to play the game of Go,\nconvergence to optimal performance, even though it is\ntheoretically feasible, is not reached due to the size of\nthe state space, which for the game of Go consists of\n10170 states. This, however, is not worrying in practice\nwhere the goal is to create a well-performing and fast\nalgorithm without the goal of full convergence or theo-\nretical guarantees. In numerical investigations of various\ntextbook problems, it was shown that PS demonstrates\na competitive performance with respect to standard RL\nmethods [13–16]. In this work, we complement those re-\nsults by comparing PS with other RL approaches from\n2\na theoretical perspective. Speciﬁcally, we analyze if PS\nconverges to an optimal solution, as other methods, like\nQ-learning and SARSA, have been proven to in environ-\nments which are describable by MDPs [17–20]. Specif-\nically, we analyze if PS converges to an optimal solu-\ntion. Other methods, like Q-learning and SARSA, have\nalready been proven to converge in environments which\nare describable by Markov Decision Processes (MDPs)\nOne should notice, however, that Q-learning and SARSA\nare methods equipped with update rules explicitly de-\nsigned for such problems. PS, in contrast, was designed\nwith a broader set of time-varying and partially observ-\nable learning environments in mind. For this reason, it\nis capable of solving tasks that a direct (naive) imple-\nmentation of Q-learning and SARSA fail to learn as they\nare designed to obtain a time-independent optimal policy\n[20, 21], examples can be found in [15]. Thus, it would\nbe unlikely for a PS agent to exactly realize the same\noptimality with respect to the discounted inﬁnite hori-\nzon reward ﬁgures of merit (for which Q-learning was\ndesigned) without any further adjustment to the model.\nNonetheless, in this work, we analyze the properties of\nPS taken as a pure MDP solving RL algorithm. We show\nthat a slightly modiﬁed PS variant recovers the notion of\nstate-action values as a function of its internal parame-\nters, while preserving the main characteristics that make\nPS stand out from other RL algorithms, such as the lo-\ncality of the update rules. As we show, this new variant\nis suitable for episodic MDPs, and we can prove conver-\ngence to the optimal strategy for a range of solutions. In\nthe process, we connect the modiﬁed PS model with the\nbasic PS model, which allows us to partially explain and\nunderstand the empirical performance and successes of\nPS reported in previous experimental works.\nThis paper is organized as follows: We quickly recap\nthe main concepts of RL theory [26] in Sec. II concerning\nMDPs that will be used by us during the rest of this pa-\nper before we present the PS model in Sec. III. In Sec. IV,\nwe begin by introducing the adaption to PS needed for\nthe convergence proof, which will be followed by the con-\nvergence proof that is based on a well-known theorem in\nstochastic approximation theory. In the Appendix of the\npaper, we provide a detailed exposition of RL methods\nwhich introduces the necessary concepts for the analy-\nsis, with a broader perspective on RL theory in mind.\nAdditionally, after discussing multiple variants of the PS\nupdate rules and their implications, we present an exten-\nsive investigation of the similarities and diﬀerence of PS\nto standard RL methods.\nII.\nMARKOV DECISION PROCESSES\nA.\nPolicy and discounted return\nIn the RL framework [21], an RL problem is a gen-\neral concept that encompasses the learning of an agent\nthrough the interaction with an environment with the\ngoal of maximizing some precisely deﬁned ﬁgure of merit\nsuch as a reward function. In a discrete-time framework,\nthe agent-environment interaction can be modeled as fol-\nlows. At every time step t, the agent perceives the en-\nvironmental state St. Then the agent chooses an action\nAt to execute upon the environment. The environment\ncompletes the cycle by signaling to the agent a new state\nSt+1 and a reward Rt+1. The variables Rt, St and At are,\nin general, random variables, where Rt can take values\nrt ∈R, while St and At take values sampled from sets\nS = {s1, s2, . . . } and A = {a1, a2, . . . } respectively. For\nsimplicity, we assume in the following that these two sets\nare ﬁnite and rt is bounded for all time steps t.\nA particularly important set of RL problems are those\nwhere the environment satisﬁes the Markovian property.\nThese problems can be modeled by Markov Decision Pro-\ncesses (MDPs). In an MDP, the probabilities of transi-\ntions and rewards are given by the set of probabilities\np(s′, r | s, a) := Pr{St+1 = s′, Rt+1 = r | St = s, At = a}.\n(1)\nAt every time step, the agent chooses an action as\nthe result of some internal function that takes as input\nthe current state of the environment. Thus, formally, an\nagent maps states into actions, which is captured by the\nso-called policy of the agent. Mathematically, the policy\n(at a certain time step t) can be deﬁned as the set of\nprobabilities\nπ(a | s) := Pr{At = a | St = s}.\n(2)\nThe successive modiﬁcation of these probabilities, π=πt,\nthrough the experience with the environment constitutes\nthe learning that the agent undergoes in order to achieve\na goal. In an MDP, the notion of goal can be formalized\nby introducing a new random variable\nGt(γdis) :=\n∞\nX\nk=0\nγk\ndisRt+k+1,\n(3)\ncalled the discounted return, where γdis ∈[0, 1] is the dis-\ncount parameter. The case with γdis = 1 is reserved for\nepisodic tasks, where the agent-environment interaction\nnaturally terminates at some ﬁnite time. The discounted\nreturn at some time-step t consists of the sum of all re-\nwards received after t, discounted by how far in the future\nthey are received. The solution to the MDP is the pol-\nicy that maximizes the expected return starting from any\nstate s, called the optimal policy.\nA particular set of RL problems, we will consider in\nthis work, are the so-called episodic tasks.\nIn these,\nthe agent-environment interactions naturally break into\nepisodes, e.g. an agent playing some card game, or try-\ning to escape from a maze.\nNote that while in some\nepisodic problems the objective could be to ﬁnish the\nepisode with the fewest possible actions (e.g. escaping a\nmaze), in general, the optimal solution is not necessarily\nrelated to ending the episode. A notion of episodic MDP\ncan be easily incorporated into the theoretical formalism\n3\nrecalled above, by including a set ST ⊂S, of so-called\nterminal or absorbing states. These states are charac-\nterized by the fact that transitions from a terminal state\nlead back to the same state with unit probability and\nzero reward. In episodic MDPs, the goal for the agent is\nto maximize the expected discounted return per episode.\nIt should be noted that the concept of absorbing states\nis a theoretical construct introduced to include the con-\ncept of episodic and non-episodic MDPs into a single for-\nmalism. In a practical implementation, however, after\nreaching a terminal state, an agent would be reset to\nsome initial state, which could be a predeﬁned state or\nchosen at random for instance. While such a choice could\nhave an impact on learning rates, it is irrelevant regard-\ning the optimal policy. For this reason, in the following,\nwe do not make any assumption about the choice of the\ninitial states. We will assume, however, that the environ-\nment signals the ﬁnalization of the episode to the agent.\nB.\nValue functions and optimal policy\nThe concept of an optimal policy is closely intertwined\nwith that of value functions. The value vπ(s) of a state\ns ∈S under a certain policy π is deﬁned as the expected\nreturn after state s is visited, i.e. it is the value\nvπ(s) := Eπ {Gt | St = s} .\n(4)\nIt has been proven for ﬁnite MDPs that there exists at\nleast one policy, called the optimal policy π∗, which max-\nimizes over the space of policies vπ(s) ∀s simultaneously,\ni.e.\nv∗(s) = max\nπ\n{vπ(s)} , ∀s ∈S,\n(5)\nwhere v∗denotes the value functions associated to the\noptimal policy.\nValue functions can also be deﬁned for state-action\npairs. The so-called Q-value of a pair (s, a), for a cer-\ntain policy π, is deﬁned as the expected return received\nby the agent following the execution of action a while\nin state s, and sticking to the policy π afterwards. The\nQ-values of the optimal policy, or optimal Q-values can\nbe written in terms of the optimal state value functions\nas\nq∗(s, a) = r(s, a)+γdis E {v∗(St+1)|St = s, At = a} , (6)\nwhere\nr(s, a) = E {Rt+1 | St = s, At = a} .\n(7)\nThe relevance of Q-values is evidenced by noting that\ngiven the set of all q∗values, an optimal policy can be\nderived straightforwardly as\nπ∗(s) = arg max\na′\n{q∗(s, a′)} .\n(8)\n(Note the notational diﬀerence in the arguments to dis-\ntinguish between the stochastic policy Eq. (2), which re-\nturns a probability, and the deterministic policy Eq. (8),\nwhich returns an action.) For this reason, standard RL\nmethods achieve an optimal policy in an indirect way, as\na function of the internal parameters of the model, which\nare those which are updated through the learning of the\nmodel, and which in the limit converge to the q∗values.\nA similar procedure will be used by us in Sec. IV, where\nwe discuss the convergence of PS to the optimal policy\nof MDPs.\nC.\nQ-learning and SARSA\nQ-learning and SARSA are two prominent algorithms\nthat capture an essential idea of RL: online learning in an\nunknown environment. They are particularly designed\nto solve Markovian environments and their prominence\ncan in part be ascribed to the theoretical results that\nprove their convergence in MDPs. In both algorithms,\nlearning is achieved by estimating the action value func-\ntion qπ(s, a) for every state action pair for a given policy\nπ. This estimate is described be the Q-value which is\nassigned to each state-action pair.\nThe update of the\nQ-value is given by:\nQt+1(st, at) = (1 −α)Qt(st, at) + α(Rt+1\n+ γdisf(Qt(st+1, at+1)).\n(9)\nThe learning rate α describes how fast a new estimate of\nthe Q-value overwrites the previous estimate. In SARSA,\nthe function f is the identity, so that the Q-value is not\nonly updated by the reward Rt+1 but also with the Q-\nvalue of the next state-action pair along the policy π.\nThus, SARSA is an on-policy algorithm, as described in\nApp A. In Q-learning, on the other hand, the function\nf = maxat+1 takes the maximal Q-value of the next state.\nThis algorithm is an oﬀ-policy algorithm due to sampling\nof the next action independently from the update of the\nQ-values.\nIII.\nPROJECTIVE SIMULATION\nProjective Simulation (PS) is a physically inspired\nframework for artiﬁcial intelligence introduced in [6]. The\ncore of the model is a particular kind of memory called\nEpisodic and Compositional Memory (ECM) composed\nof a stochastic network of interconnected units, called\nclips (cf. Fig.2 in [6]). Clips represent either percepts\nor actions experienced in the past, or in more general\nversions of the model, combinations of those. The archi-\ntecture of ECM, representing deliberation as a random\nwalk in a network of clips, together with the possibility of\ncombining clips and thus creating structures within the\nnetwork, allows for modeling incipient forms of creativity\n4\n[11, 22]. Additionally, the deliberation process leading\nfrom percepts to actions has a physical interpretation in\nPS. Visiting any environmental state activates a corre-\nsponding percept clip in the ECM. This activation can\nbe interpreted as an excitation, which then propagates\nstochastically through the network in the form of a ran-\ndom walk. The underlying dynamics have the potential\nto be implementable by real physical processes, thus re-\nlating the model to embodied agents including systems\nwhich exploit quantum eﬀects, as has been explored in\n[23, 24].\nPS can be used as an RL approach, where the action,\nthe percept and the reward are used to update the ECM\nstructure. In general, the PS framework enables to lever-\nage complex graph structures to enhance learning. For\nexample, generalization can be implemented through ma-\nnipulation of the ECM topology so that the RL agent is\ncapable of learning in scenarios it would otherwise fail to\nlearn [14]. However, this generalization mechanism is not\nnecessary for solving MDP environments.\nBefore we discuss the ECM for solving MDPs in detail,\nwe need to emphasis the diﬀerence between the state of\nthe environment and the percept the agent receives. In\nan algorithm speciﬁcally designed to solve MDPs, the\nstate contain suﬃcient information of the environment\nsuch that the corresponding transition function fulﬁlls\nthe Markov property. We will refer to this type of state\nas Markov state. This assumption on the state space can\ngenerally not be made in most realistic learning scenarios\nbut it can be generalized to partially observable MDPs\nwhere the Markovian dynamics are hidden. In a partially\nobservable environment, the input of the learning algo-\nrithm is an observation that is linked to a Markov state\nvia a, from the perspective of the algorithm, unknown\nprobability distribution.\nA percept, as introduced in the PS model, further gen-\neralizes the concept of such an observation. Here, the\npercept does not necessarily have to be connected to an\nunderlying Markov state contrary to the observation in\npartially observable MDPs. This distinction might not\nseem necessary for learning in a classical environment\nbut plays a signiﬁcant role when one considers quantum\nsystems that cannot be described with hidden variable\nmodels. In this work, since we focus on MDPs, we will\nequate the percepts an agent receives and the state of\nthe MDP. In the following, both are denoted by s. Fur-\nthermore, we will not emphasize the diﬀerence between\nthe percept and its corresponding percept clip, assuming\nthere is a one-to-one correspondence between percept and\npercept clip. The same holds for the actions and their\ncorresponding action clips.\nThe ECM structure used to solve MDPs consists of\none layer of percept clips that is fully connected with a\nlayer of action clips.\nEach edge represents a state ac-\ntion pair (s, a) which is assigned a real-valued weight (or\nhopping value) h= h(s, a) and a non-negative glow value\ng = g(s, a). While the weight h determines the proba-\nbility of transition between a percept clip and an action\nclip, the glow variable g measures how ’susceptible’ this\nweight h is to future rewards from the environment. In\n(10), heq is an (arbitrarily given) equilibrium value, and\nλt+1 is the reward received immediately after action at,\nin accordance with the time-indexing conventions in [21]\nas shown in Fig. 1.\nA variety of diﬀerent update rules are discussed in\nApp D and compared with other RL methods in App E.\nIn the following, we will focus on the standard update\nused in [6, 13, 15]. The update rules for the h-value and\nthe glow value are given by:\nht+1(s, a) = ht(s, a) −γ(ht(s, a) −heq)\n+gt(s, a)λt+1\n(10)\ngt(s, a) = (1 −δ(s,a),(st,at))(1 −η)gt−1(s, a)\n+δ(s,a),(st,at)\n(11)\nst\nλt\nat\nst+1\nλt+1\nFIG. 1: Transition from time step t to t + 1, (t = 0, 1, 2, . . .),\nvia the agent’s decision at, where s and λ denote environment\nstate and reward (λ0 = 0), respectively (adapted from [21]).\nThe update of the h-value consists, in the language\nused in the context of Master equations, of a gain and\na loss term. The parameter for the loss term is called\ndamping parameter and is denoted by γ ∈[0, 1]. The pa-\nrameter for the gain term is called glow parameter [15]\nand is denoted by η ∈[0, 1]. In particular, η = 1 recov-\ners the original PS as introduced in [6]. Finally, δt :=\nδs,stδa,at = δ(s,a),(st,at) denotes the Kronecker delta sym-\nbol, which becomes 1 if the respective (s, a)-pair is vis-\nited at cycle t, and is otherwise 0. The agent’s policy\nis deﬁned as the set of all conditional probabilities (i.e.,\ntransition probabilities in the ECM clip network)\npij = p(aj|si) = Π(hij)\nκi\n,\nκi =\nX\nj\nΠ(hij),\n(12)\nof selecting action aj when in state si and is here de-\nscribed in terms of some given function Π. Examples of\nΠ which have been used or discussed in the context of\nPS are an identity function [6, 15],\nΠ(x) = x,\n(13)\nif x is non-negative, and an exponential function leading\nto the well-known softmax policy [13] if a normalization\nfactor is added\nΠ(x) = eβx,\n(14)\nwhere β ≥0 is a real-valued parameter.\n5\nIV.\nCONVERGENCE OF PS IN EPISODIC\nMDPS\nIn previous works, it has been shown numerically that\nthe basic version of a PS-agent is capable of learning the\noptimal strategy in a variety of textbook RL problems.\nThe PS model with standard update rules, however, does\nnot necessarily converge in all MDP settings. This ver-\nsion of the PS is thoroughly analyzed in App. D and\nApp. E. As recalled in Sec. II, in MDPs optimality can\nbe deﬁned in terms of the optimal policy. In this section,\nwe present a modiﬁed version of the PS that has been\ndesigned exclusively to tackle this kind of problem. We\nconsider arbitrary episodic MDPs, and derive an analyti-\ncal proof of convergence. In this version, the policy func-\ntion depends on the normalized ˜h values, which, as we\nshow, later behave similarly as state-action values, and\nin fact, in episodic MDPs, they converge to the optimal\nq∗values for a range of discount parameters.\nA.\nProjective Simulation for solving MDPs\nIn the following, we introduce a new variant of PS\naimed at solving episodic MDPs.\nIn those problems,\nthere is a well-deﬁned notion of optimality, given by the\noptimal policy. As described above, the basic PS con-\nstitutes a direct policy method (see also App. A). Find-\ning the optimal policy of an MDP by policy exploration\nseems a rather diﬃcult task. However, as other methods\nhave proven, ﬁnding the optimal q∗values can be done\nwith relatively simple algorithms, and the optimal policy\ncan be derived from the q∗values in a straightforward\nmanner.\nMotivated by this, we add a new local vari-\nable to the ECM-network in order to recover a notion of\nstate-action values while maintaining the locality of the\nmodel.\nFor this version we consider “ﬁrst-visit” glow, deﬁned\nas follows [27]. The glow of any given edge is set to one\nwhenever that edge is visited for the ﬁrst time during an\nepisode and in any other circumstance it is damped by\na factor (1 −η), even if the same edge is visited again\nduring the same episode.\nIn addition, the entire glow\nmatrix is reset to zero at the end of an episode. We thus\nwrite the updates as\nht+1(s, a) = ht(s, a) + λt+1gt(s, a)\n(15)\ngt(s, a) = (1 −η)gt−1(s, a) + δ(s,a),(s,a)first−visit(16)\nNt+1(s, a) = Nt(s, a) + δ(s,a),(s,a)first−visit\n(17)\nHere, the update for h is the same as in Eq. (10),\nbut given that the MDPs are time-independent environ-\nments, γ has been set to zero. We add a matrix N to the\nstandard PS, which counts the number of episodes dur-\ning which each entry of h has been updated. The idea\nbehind these updates is that the ratios\n˜ht(s, a) := ht(s, a)\nN + 1\n(18)\nresemble state-action values.\nTo gain some intuition\nabout this, note that h-values associated to visited edges\nwill accumulate during a single episode a sum of rewards\nof the form\nλt + (1 −η)λt+1 + (1 −η)2λt+2 + . . . ,\n(19)\nwhich gets truncated at the time step the episode ends.\nHence, the normalized ˜h values become averages of sam-\npled discounted rewards (see App E 0 a). Later we show\nthat paired with the right policy and glow coeﬃcient the\n˜h values converge to the optimal q∗values.\nInstead of considering a policy function of the h-values\nas in Eq. (12), here we will consider a policy function\ngiven by\npi,j = Π(˜hi,j)\nci\n,\nci =\nX\nj\nΠ(˜hi,j),\n(20)\nfor a certain function Π(·). Given that the ˜h-values are,\nin general, not diverging with time (in fact they are\nbounded in the case of bounded rewards) a linear func-\ntion, as in Eq. (13), would fail to converge to a deter-\nministic policy. A solution for that is to use a softmax\nfunction as in Eq. (14), where the free coeﬃcient β is\nmade time-dependent. By letting β diverge with time,\nthe policy can become deterministic in the limit.\nSimilarly to Monte Carlo methods, which may be\nequipped with a variety of update rules, giving rise to\nﬁrst-visit or many-visit Monte Carlo methods, the choice\nof the glow update rule is to some extent arbitrary but\nmay depend on the physical implementation of PS and\nthe ECM. For example, instead of Eq. (15), one could\nuse the accumulating glow update, given in Eq. (D9). In\nthat case, one simply needs to change the update rule\nof N, given in Eq. (17) in such a way that every visit\nof the edge is counted, instead of only ﬁrst visits. In-\ntuitively, both pairs of update rules have similar eﬀects,\nin the sense that in both cases ˜h(s, a) equals an average\nof sampled discounted returns starting from the time a\nstate-action pair (s, a) was visited. However, while for\nﬁrst-visit glow we were able to prove convergence, that\nis not the case for accumulating glow. Therefore, when\nreferring to this version of PS in the following, we assume\nupdate rules given by (15)-(17).\nB.\nConvergence to the optimal policy\nThe convergence of ˜h values to q∗values can be proven\nby a standard approach used in the literature to prove, for\nexample, the convergence of RL methods like Q-Learning\nand SARSA, or prediction methods like TD(λ). In the\nremainder of the paper, we will use interchangeably the\nboldface notation e to denote a state-action pair as well\nas the explicit notation (s, a) whenever convenience dic-\ntates. Denoting by ˜hm(e) the ˜h-value of edge e at the\nend of episode m, we deﬁne the family of random vari-\nables ∆m(e) := ˜hm(e) −q∗(e). We want to show that\n6\nin the limit of large m, ∆m(e) converges to zero for all\ne. Moreover, it is desirable that such convergence occurs\nin a strong sense, i.e. with probability one. We show\nthat by following the standard approach of constructing\nan update rule for ∆m(e) which satisﬁes the conditions\nof the following theorem [28]\nTheorem 1. A random iterative process ∆m+1(x) =\n[1 −αm(x)] ∆m(x) + αm(x)Fm(x), x ∈X converges to\nzero with probability one (w.p.1) if the following proper-\nties hold:\n1. the set of possible states X is ﬁnite.\n2. 0 ≤αm(x) ≤1, P\nm αm(x) = ∞, P\nm α2\nm(x) <\n∞w.p.1, where the probability is over the learning\nrates αm(x).\n3. ∥E{Fm(·)|Pm}∥W ≤κ∥∆m(·)∥W + cm, where κ ∈\n[0, 1) and cm converges to zero w.p.1\n4. Var{Fm(x)|Pm} ≤K(1 + ∥∆m(·)∥W)2, where K is\nsome constant.\nHere Pm is the past of the process at step m, and the no-\ntation ∥·∥W denotes some ﬁxed weighted maximum norm.\nIn addition to ∆m(e) meeting the conditions of the the-\norem, the policy function must also satisfy two speciﬁc\nrequirements. First of all, it must be greedy with respect\nto the ˜h-values (at least in the limit of m to inﬁnity).\nIn that way, provided that the ˜h-values converge to the\noptimal q∗values, the policy becomes automatically an\noptimal policy. Additionally, to guarantee that all ∆m\nkeep being periodically updated, the policy must guar-\nantee inﬁnite exploration. A policy that satisﬁes these\ntwo properties is called GLIE [18], standing for Greedy\nin the Limit and Inﬁnite Exploration. Adapting the re-\nsults from [18] for PS and episodic environments, we can\nshow (see App. G) that a softmax policy function deﬁned\nby\nπm(a|s, ˜hm) =\nexp\nh\nβm˜hm(s, a)\ni\nP\na′∈A exp\nh\nβm˜hm(s, a′)\ni\n(21)\nis GLIE, provided that βm →m→∞∞and βm ≤C ln(m),\nwhere C is a constant depending on η and |S|. While\nthe ﬁrst condition on βm guarantees that the policy is\ngreedy in the limit, the second one guarantees that the\nagent will keep exploring all state-action pairs inﬁnitely\noften. In this particular example, we have considered βm\nto depend exclusively on the episode index.\nBy doing\nso, the policy remains local, because βm can be updated\nusing exclusively the signal of the environment indicating\nthe ﬁnalization of the episode. Note however that the\nchoice of the policy function, as far as it is GLIE, has\nno impact on the convergence proof. We are now in a\nposition to state our main result about the convergence\nof PS-agents in the form of the following theorem.\nTheorem 2. For any ﬁnite episodic MDP with a dis-\ncount factor of γdis, the policy resulting from the new up-\ndates converges with probability one to the optimal policy,\nprovided that\n1. rewards are bounded,\n2. 0 ≤γdis ≤1/3 , where γdis = 1 −η,\n3. the policy is a GLIE function of the ˜h-values.\nNote that we have restricted the range of values γdis\ncan take. The reason for that is related to the way the\nh-values are updated in PS. In Q-Learning and SARSA,\nwhere the γdis parameter of the MDP is directly included\nin the algorithm, every time an action is taken its corre-\nsponding Q-value is updated by a sum of a single reward\nand a discounted bootstrapping term.\nGiven that the\nPS updates do not use bootstrapping, that term is “re-\nplaced” by a discounted sum of rewards.\nDue to this\ndiﬀerence, the contraction property (Condition 3 in The-\norem 1) is not so straightforward to prove forcing us to\nconsider smaller values of γdis. However, this condition\non the γdis parameter is not a fundamental restriction of\nthe PS model, but merely a result of how convergence is\nproven in this work.\nC.\nEnvironments without terminal states\nIn Theorem 2, we have considered exclusively episodic\nMDPs.\nHowever, it is still possible for these environ-\nments to have an optimal policy which does not drive\nthe agent to any terminal states. This observation sug-\ngests that the scope of problems solvable by PS can be\nfurther extended to a subset of non-episodic MDPs.\nGiven any non-episodic MDP, one can construct an\nepisodic MDP from it by adding one single terminal state\nsT and one single transition leading to it with non-zero\nprobability, i.e.\nby deﬁning pT = Pr(sT |s, a) ̸= 0 for\nsome arbitrary pair (s, a). Thus, while the original non-\nepisodic MDP falls outside the scope of Theorem 2, PS\ncould be used to tackle the non-episodic MDP. Anyway,\nin general, these two problems might have diﬀerent solu-\ntions, i.e. diﬀerent optimal policies. However, given that\nboth the pair (s, a) for which pT ̸= 0 and the value of pT\nare arbitrary, by properly choosing them, the diﬀerence\nbetween the two optimal policies could become negligi-\nble or non-existent. That could be done easily having\nsome useful knowledge about the solution of the original\nMDP. Consider for instance a grid world, where multiple\nrewards are placed randomly around some central area\nof grid cells. Even without knowing the exact optimal\npolicy, one can correctly guess that there will be an opti-\nmal cyclic path about the center of the world yielding the\nmaximum expected discounted return. Hence, adding a\nterminal state in some remote corner of the world would\nvery likely leave the optimal policy unchanged.\n7\nD.\nProof of Theorem 2\nIn this section, we discuss the core of the proof of Theo-\nrem 2, leaving for App. F the most involved calculations.\nGiven that the policy is a greedy-in-the-limit-function of\nthe ˜hm values, the proof of Theorem 2 follows if we show\nthat\n∆m(e) := ˜hm(e) −q∗(e)\n(22)\nconverges to zero with probability one. In order to do\nso, we show that ∆m(e) obeys an update rule of the\nform given in Theorem 1 and the four conditions of the\ntheorem are satisﬁed.\nWe begin by deriving an update rule for the h-values\nbetween episodes. In the case where an edge e is not vis-\nited during the m-th episode, its corresponding h-value\nis left unchanged, i.e. hm(e) = hm−1(e). Otherwise, due\nto the decreasing value of the glow during the episode,\nin the m-th episode, the h(e) value will accumulate a\ndiscounted sum of rewards given by\nDm(e) =\nTm\nX\nt=tm(e)\n¯ηt−tm(e)λt,\n(23)\nwhere tm(e) and Tm are the times at which the ﬁrst visit\nto e during episode m occurred and at which the episode\nﬁnished, respectively, and ¯η = 1−η. Therefore, in general\nhm(e) = hm−1(e) + χm(e)Dm(e), where χm(e) is given\nby\nχm(e) =\n(\n1\nif e is visited during the m-th episode,\n0\notherwise.\n(24)\nWe denote, respectively, by Nm(e) and ˜hm(e) the N-\nvalue and ˜h-value associated to edge e at the end of\nepisode m. Thus, we have that ˜hm(e) = hm(e)/[Nm(e)+\n1] and it obeys an update rule of the form\n˜hm(e) =\n1\nNm(e) + 1\nn\u0002\nNm−1(e) + 1\n\u0003˜hm−1(e)\n+ χm(e)Dm(e)\no\n.\n(25)\nNoting that the variables Nm(e) can be written in terms\nof χm(e) as the sum Nm(e) = Pm\nj=1 χj(e), it follows\nfrom Eq. (25) that the variable ∆m(e) given in Eq. (22)\nsatisﬁes the recursive relation\n∆m(e) = [1 −αm(e)] ∆m−1(e) + αm(e)Fm(e),\n(26)\nwhere the ratios\nαm(e) :=\nχm(e)\nNm(e) + 1,\n(27)\nplay the role of learning rates, and Fm(e) is deﬁned as\nFm(e) := χm(e) (Dm(e) −q∗(e)) .\n(28)\nThe update rule in Eq. (26) is exactly of the form given\nin Theorem 1. Therefore we are left with showing that\nαm(e) satisﬁes Condition 2 in Theorem 1, and Fm(e) sat-\nisﬁes Conditions 3 and 4. Below we describe the general\nprocedure to prove that, while most of the details can be\nfound in App. F.\nThe fact that αm(e) satisﬁes Condition 2 in Theorem\n1 follows from noting that P\nm αm(e) = P\nn 1/n and\nP\nn α2\nm(e) = P\nn 1/n2, which are, respectively, a diver-\ngent and a convergent series.\nRegarding Condition 3,\nnote that by tweaking the free glow parameter in such a\nway that ¯η = γdis, the variable Dm(e) becomes a trun-\ncated sample of the discounted return G(e, γdis) given in\nEq. (3). Thus, ˜h values undergo a similar update to that\nfound in SARSA, with the diﬀerence that instead of a\nbootstrapping term an actual sample of rewards is used.\nDue to these similarities we can use the same techniques\nused in the proof of convergence of RL methods [18, 19]\nand show that\n∥E{Fm(·)|Pm}∥W ≤f(γdis)∥∆m(·)∥W + cm,\n(29)\nwhere cm converges to zero w.p.1 and f(γdis) =\n2γdis\n1−γdis .\nThis equation satisﬁes Condition 3 in Theorem 1 as far\nas f(γdis) < 1, which occurs for γdis < 1/3.\nFinally, Condition 4 in Theorem 1 follows from the fact\nthat rewards are bounded.\nThis implies that ˜h-values\nand, in turn, the variance of Fm(e) are bounded as well.\nThis concludes the proof of Theorem 2.\nV.\nCONCLUSION\nIn this work, we studied the convergence of a variant\nof PS applied to episodic MDPs. Given that MDPs have\na clear deﬁnition of a goal, characterized by the optimal\npolicy, we took the approach of adapting the PS model\nto deal with this kind of problem speciﬁcally. The ﬁrst\nvisit glow-version of PS presented in this work, internally\nrecovers a certain notion of state-action values, while pre-\nserving the locality of the parameter updates, crucial to\nguarantee a physical implementation of the model by sim-\nple means. We have shown that with this model a PS-\nagent achieves optimal behavior in episodic MDPs, for a\nrange of discount parameters. This proof and the theo-\nretical analysis of the PS update rules shed light on how\nPS, or, more precisely, its policy, behaves in a general RL\nproblem.\nThe PS updates that alter the h-values at every time\nstep asynchronously pose a particular challenge for prov-\ning convergence.\nTo deal with that, we analyzed the\nsubsequence of internal parameters at the times when\nepisodes end, thus recovering a synchronous update. We\ncould then apply techniques from stochastic approxima-\ntion theory to prove that the internal parameters of PS\nconverge to the optimal q values, similarly as in the con-\nvergence proofs of other RL methods.\nWe have also chosen a speciﬁc glow update rule, which\nwe have called ﬁrst-visit glow. While other glow updates,\n8\nlike accumulating or replacing glow, show the same be-\nhavior at an intuitive level, trying to prove the conver-\ngence with those updates has proven to be more cumber-\nsome. Therefore, from a practical point of view, several\nglow mechanisms could be potentially utilized, but con-\nvergence in the limit is, at the moment, only guaranteed\nfor ﬁrst-visit glow.\nAlthough only episodic MDPs fall within the scope of\nour theorem, no constraints are imposed on the nature\nof the optimal policy. Hence, episodic problems where\nthe optimal policy completely avoids terminal states (i.e.\nthe probability that an agent reaches a terminal state by\nfollowing that policy is strictly zero) can also be consid-\nered. Furthermore, the agent could be equipped with any\npolicy, as far as the GLIE condition is satisﬁed. In this\npaper, we provided a particular example of a GLIE policy\nfunction, in the form of a softmax function with a global\nparameter, which depends exclusively on the episode in-\ndex. In this particular case, the policy is compatible with\nlocal updates, in the sense that the probabilities to take\nan action given a state can be computed locally.\nVI.\nACKNOWLEDGEMENTS\nThis work was supported, in part, by the Austrian\nScience Fund through the projects SFB FoQus F4212,\nSFB BeyondC F7102, and DK ALM W1259, and in part\nby the Dutch Research Council (NWO/OCW), through\nthe Quantum Software Consortium programme (project\nnumber 024.003.037).\nAppendix A: A review of RL methods\nThe following section is meant as a concise overview\nof standard RL methods, which we distilled and adapted\nfrom [21], to provide the necessary background before\nwhich the PS will we be discussed in Sections D and E.\nFor details we refer the reader to Ref. [21].\nAmong the model-free and gradient-based approaches\nwe can broadly distinguish between value function-based\nmethods which are gradient-descent with respect to a so-\ncalled temporal diﬀerence (TD) error and direct policy\nmethods which are gradient-ascent with respect to the\nexpected return as shown in Fig. 2.\nIn what follows, we focus on actor-critic methods be-\ncause they exhibit an “all in one” structure from which\nthe other approaches can be deduced by simpliﬁcations.\nTo make it short and provide an overall picture, the so-\ncalled update rules for a single time step are listed in (A1)\nvalue function\nbased\nactor-critic\ndirect policy\nFIG. 2: Some types of gradient-based solution methods for\nRL-problems.\nValue function-based methods are gradient-\ndescent with respect to a TD-error, whereas direct policy\nmethods are gradient-ascent with respect to the expected\nreturn. Actor-critic methods are depicted as their intersec-\ntion since they combine both approaches. Being understood\nas “parametric” methods, this ﬁgure corresponds to the left\nbranch of Fig. 3 in [24].\nand will be explained in the remainder of this section.\nδ ←R + γdis u′ −u\n(episodic),\n(A1a)\nδ ←R −¯R + u′ −u\n(continuing),\n(A1b)\nzu ←γdis λu\ntra zu + Γ ∇u,\n(A1c)\nθu ←θu + αu δ zu,\n(A1d)\nzπ ←γdis λπ\ntra zπ + Γ ∇ln π,\n(A1e)\nθπ ←θπ + απ δ zπ,\n(A1f)\nΓ ←γdis Γ\n(episodic),\n(A1g)\n¯R ←\n¯R + ηδ\n(continuing).\n(A1h)\nIn (A1), we have two players: an actor [the policy π =\nπ(A|S, θπ)] and a critic [the value function u = u(S, θu)],\nhence all corresponding quantities are distinguished by\ntheir respective superscript. The value function is pa-\nrameterized by a weight vector θ. The vector z is referred\nto as the eligibility trace.\nThe update equations for the actor are given by (A1e)–\n(A1f) and the updates for the critic are (A1c)–(A1d),\nwhere the ∇:= ∂\n∂θ denote the gradients with respect to\nthe θ vectors. These two sets of updates are identical\nexcept for the natural logarithm ln π of the policy taken\nin (A1e). This logarithm is a consequence of the policy\ngradient theorem and makes the gradient of the actual\nperformance measure to be maximized (value of the start\nstate of an episode or average rate of reward per time step\nfor continuing problems, see below) independent of the\nderivative of the state distribution (that is, the eﬀect of\nthe policy on the state distribution), which depends on\nthe environment and is unknown.\nEqs. (A1) describe approximative methods, since they\napply function approximation as a scalable way of\ngeneralizing from a state space much larger then the\nmemory capacity of the agent. The tabular case can be\n9\nrecovered from this as a special exact case, in the sense\nthat all encountered states and actions are represented\nindividually. The function approximations are hidden in\nthe gradients ∇u and ∇ln π in (A1c) and (A1e) and\ncan be done in linear fashion [by scalar products θ · x\nwith feature basis vectors x(S) or x(S, A)] or in nonlin-\near fashion (e.g. by neural networks with θ as connec-\ntion weights)[29].\nNote that the two parametrizations\nθu and θπ are entirely unrelated (and hence diﬀerent-\ndimensional in general). In App. B, we show in the ex-\nample of SARSA, how tabular methods can be recovered\nfrom (A1).\nIn (A1), we can use a state value function, u=u(S, θu),\nbecause the policy is taken care of separately. Without\nit, i.e., when we only know the value u(s) of the state\ns we are in, we would require an environment model\np(s′, r|s, a) to decide on an action a. To remain model-\nfree, we would then have to apply an action value func-\ntion u = u(s, a, θu) instead, from which could obtain the\nbest action by search for argmaxau.\nEqs. (A1) contain six meta parameters: η > 0 and the\ntwo α > 0 are step sizes, γdis ∈[0, 1] is the discount-rate\nparameter, and the two λtra ∈[0, 1] are trace-decay rates\nthat allow to vary the degree of bootstrapping, which\ndenotes the updating of estimates by using other existing\nestimates (cf. App. C). In (A1), these existing estimates\ninvolve the current values u′ of subsequent (i.e., one time\nstep later) states or state-action pairs, which enter the\nTD-error δ in either (A1a) or (A1b) together with the\nreward R. Choosing λtra is thus a possibility to inter-\npolate between the fully bootstrapping original one-step\nTD methods which are recovered for λtra=0, and Monte\nCarlo (i.e., non-bootstrapping) methods, which are ob-\ntained in the limit λtra = 1. Monte Carlo methods rely\nexclusively on actual complete returns Gt received. In a\nstrict sense, they update oﬀ-line, i.e., they store a whole\nepisode S0, A0, R1, . . . , ST −1, AT −1, RT , ST in a separate\nmemory and only at the end of an episode the respec-\ntive current estimates are updated in reverse time order\nt = T −1, T −2, . . . , 0 making use of the fact that Gt =\nRt+1 + γdisGt+1. In contrast to the updates (A1), which\nare are done online (i.e., are incremental step-by-step),\nstrict Monte Carlo methods are thus incremental in an\nepisode-by-episode sense, and are consequently only de-\nﬁned for the episodic case. Consequently, even for λtra\n= 1 the online updates (A1) approximate Monte Carlo\nmethods only for inﬁnitesimally small step sizes α.\nIn continuing problems, the interaction between agent\nand environment goes on forever without termination or\nstart states. Discounting is here useful in the tabular case\nbut problematic when used with function approximation,\nwhere the states cannot be clearly distinguished anymore.\nAn alternative then is to use the average rate of reward r\n:=limT →∞1\nT\nPT\nt=1 E(Rt)=limt→∞E(Rt), i.e., the aver-\nage reward per time step (assuming ergodicity). E(Rt) is\nthe respective expected reward and (3) is replaced with\nthe diﬀerential return Gt :=P∞\nk=0(Rt+k+1 −r) . In (A1),\nwe thus set γdis = 1 in such a case and apply (A1b) and\n(A1h) instead of (A1a) and (A1g), respectively. ¯R is the\ncurrent estimate of the average reward r.\nTo actually run (A1), ¯R and θ can be initialized arbi-\ntrarily (e.g. to 0). At the beginning of each episode, z\nis initialized to 0, and Γ is initialized to 1. At the end of\neach episode, the value of a terminal state is set to 0.\nEqs. (A1) are on-policy and must be distinguished\nfrom oﬀ-policy methods (such as Q-learning) which\ntrain on a distribution of transitions that is diﬀerent\nfrom the one corresponding to the targeted (desired) be-\nhavior and thus free what the agent is actually doing\n(behavior policy) from what it should do (target pol-\nicy). While this is not fundamentally required for basic\nproblems that can be treated with model-free learning, it\nbecomes essential in (model-based) prediction and plan-\nning, where it allows parallel learning of many target\npolicies from the (necessarily one) realized behavior pol-\nicy. Combining function approximation, bootstrapping,\nand oﬀ-policy updates may lead to instability and diver-\ngence.\nAt ﬁrst glance, (A1) look elaborate and one might\nwonder why, for instance, value function-based methods\nshould not suﬃce. The short answer is that this depends\non the type and intricacy of the problem. To be more\nspeciﬁc, one reason is that the expected return of state-\naction-pairs, that value functions estimate, typically con-\ntains more information than needed to make decisions.\nFor example, a transformation of u which leaves the or-\nder of its values unchanged (such as multiplication with a\npositive constant) has no eﬀect on the respective optimal\ndecision. As a consequence, value function-based meth-\nods are too strict, since u are well deﬁned and one needs\nto separately decide on a policy in order to convert these\nvalue estimates to action selection probabilities. If, for\nexample a so-called softmax policy (14) is used, a choice\nand schedule (i.e., time-dependence) of the so-called in-\nverse temperature parameter β has to be made. In con-\ntrast, direct policy methods, for instance, internally work\nwith numerical preferences h(s, a, θπ) whose actual val-\nues do not have to represent anything meaningful and\nare thus free to evolve in parameter space.\nAppendix B: Recovering SARSA from actor-critic\nmethods\nTo recover a pure action value method from the actor-\ncritic methods (A1), we restrict attention to (A1a),\n(A1c), and (A1d), set Γ = 1 and ignore the remaining\nupdates. For u we choose an action value function u =\nu(S, A, θu) which we name as q. Suppressing the super-\nscript u but adding time step indices, this gives for the\nscalar TD-error signal\nδTD\nt\n= Rt+1 + γdisqt(St+1, At+1) −qt(St, At),\n(B1)\n10\nwith which the remaining updates describe SARSA(λtra)\nwith function approximation,\nθt+1 = θt + αδTD\nt\nzt,\n(B2)\nzt+1 = γdisλtrazt + ∇qt+1(St+1, At+1).\n(B3)\nIn the tabular case, q becomes a table (matrix) with en-\ntries q(Si, Aj). The components of the parameter vector\nθ are identiﬁed with just these entries, so that the gradi-\nent becomes a table of Kronecker delta symbols\n∇q(S, A)|t = ∂q(S, A)\n∂q(St, At) = (δS,StδA,At) =: δt.\n(B4)\nTo clarify, the bold δt has the same dimension as q (i.e.,\nit is a matrix) with the single entry corresponding to\n(St, At) (i.e., the state-action-pair visited at time t) be-\ning equal to 1 and all other entries being 0 and must\nbe distinguished from the non-bold δt used throughout\nSec. III, which refers to a single given state-action-pair,\nand is 1 (0), if this pair is (not) visited at time t. With\nthe bold δt, the updates (B2)–(B3) reduce to tabular\nSARSA(λtra),\nqt+1 = qt + αδTD\nt\nzt,\n(B5)\nzt+1 = γdisλtrazt + δt+1,\n(B6)\nwhere z is here called an accumulating eligibility trace\nand also has the same dimension as q (i.e., it is also a\nmatrix). Hence (B5) updates all entries of q. For λtra\n= 0, only the respective (i.e., visited) single entry of q is\nupdated,\nqt+1(St, At) = qt(St, At) + αδTD\nt\n,\n(B7)\nwhich corresponds to conventional one-step SARSA.\nAppendix C: Notes on eligibility trace vectors\nWe can here only outline a sketch. Let us focus on\nvalue function methods, where for simplicity of notation\nwe restrict attention to state values. Action value meth-\nods can be covered analogously by referring to (s, a)-pairs\ninstead of states s. We may consider the mean squared\nvalue error\nV E(θ) :=\nX\ns\nµ(s) [u(s) −ˆv(s, θ)]2\n(C1)\nbetween the true value function u(s) and a current es-\ntimate ˆv(s, θ) of it. µ(s) can be any probability distri-\nbution, but is typically chosen to be the fraction of time\nspent in s under on-policy training in which case it is\ncalled “on-policy distribution” (or stationary distribution\nin continuing tasks). Ideally, one would ﬁnd a global op-\ntimum θ∗such that V E(θ∗) ≤V E(θ) ∀θ. The problem\nis that u(St) is unknown, hence we substitute a so-called\nupdate target (“backed-up” value) Ut as a random ap-\nproximation of the true value u(St) and apply stochastic\ngradient descent\nθt+1 = θt −1\n2αt∇[Ut −ˆv(St, θt)]2\n(C2)\n= θt + αt [Ut −ˆv(St, θt)] ∇ˆv(St, θt).\n(C3)\nIf Ut is an unbiased estimate of u(St), i.e., E [Ut|St = s]\n= u(s) ∀t, then convergence to a local optimum (i.e.,\nthe above inequality holds in a neighborhood of θ∗) fol-\nlows under the stochastic approximation conditions for\nthe step-size parameter αt > 0,\n∞\nX\nt=0\nαt = ∞,\n∞\nX\nt=0\nα2\nt < ∞.\n(C4)\nOne possible choice for Ut is the λ-return\nGλ\nt := (1 −λ)\n∞\nX\nn=1\nλn−1\ntra Gt:t+n\n(C5)\nas a mixture (λtra ∈[0, 1]) of n-step returns at time t,\nGt:t+n :=\nn−1\nX\nk=0\nγk\ndisRt+k+1 + γnˆv(St+n, θt+n−1).\n(C6)\nReferring to episodic tasks with (random) termination\ntime T , i.e., t ≤T −n in (C6), and Gt:t+n = Gt:T = Gt\nfor n ≥T −t, one can decompose\nGλtra\nt\n= (1 −λtra)\nT −t−1\nX\nn=1\nλn−1\ntra Gt:t+n + λT −t−1\ntra\nGt, (C7)\nin order to demonstrate the TD(0)-limit Gλ\nt\nλtra→0\n→\nGt:t+1\nof recovering the one-step return and the Monte Carlo\nlimit Gλ\nt\nλtra→1\n→\nGt of recovering the conventional (full)\nreturn (3).\nThe incremental updates (C3) refer to time t but in-\nvolve via (C6) information that is only obtained after t.\nThus we must rearrange these updates such that they\nare postponed to later times following a state visit. This\nis accomplished by eligibility trace vector updates such\nas (A1c). To see this, one may sum up all increments\n(A1d) over an episode [with (A1c) substituted in explicit\nform] and compare this with the sum of all increments\n(C3) [with Ut = Gλ\nt ] over an episode. Both total incre-\nments are equal if we neglect the inﬂuence of the change\nof θ on the target Ut. This approximation holds for suf-\nﬁciently small αt, but strictly speaking, the bootstrap-\nping involved in Gλ\nt , namely its dependence on θ via\nˆv(St+n, θt+n−1) renders the transition from (C2) to (C3)\nonly a (“semi-gradient”) approximation.\nWhile a Monte Carlo target Ut=Gt is by deﬁnition un-\nbiased, Monte Carlo methods are plagued with high vari-\nance and hence slow convergence. In contrast, bootstrap-\nping reduces memory resources, is more data-eﬃcient,\n11\nand often results in faster learning, but introduces bias.\nThis is reminiscent of the “bias-variance tradeoﬀ” which\noriginally refers to supervised learning. While in the lat-\nter case the “trading” is realized by methods such as\n“regularization” or “boosting” and “bagging”, in the con-\ntext of RL considered here, choosing λtra serves a similar\nfunction.\nAppendix D: Choice of glow update\nIn the following, diﬀerent types of glow updates are dis-\ncussed, which are useful for the comparison with other\nRL methods in App. E. We will focus on two types of\nglow, which we refer to as replacing and accumulating\nglow, motivated by the respective eligibility traces of the\nsame name, which were originally introduced as tabular\nversions of the vector z and represent in the methods dis-\ncussed in App. A the counterpart of glow introduced in\nSec. III. While replacing glow deﬁned in Equation (11)\nis the version applied in all works on PS so far, accu-\nmulating glow deﬁned in Equation (D9) and ﬁrst-visit\nglow deﬁned in Equation (15) are introduced to simplify\nthe expressions and analysis.\nFrom the perspective of\nthe methods considered in App. A, accumulating glow is\nmore similar to the z-updates (A1c) and (A1e) than re-\nplacing glow, for which such a generalization is not clear.\nAs far as the choice of an eligibility trace in tabular RL\nmethods is concerned, the tabular case of (A1) yields\naccumulating traces as is shown in (B5)–(B6) in the ex-\nample of SARSA.\n1.\nReplacing glow\nIn what follows, we discuss the update rules for replac-\ning glow, which is helpful for the comparison with other\nRL methods. We consider the h-value of an arbitrarily\ngiven single (s, a)-pair at time t. It is determined by the\nsequence of rewards λj+1 (j = 0, . . . , t −1) and times\nof visits, which can be described using the sequence of\nKronecker delta symbols δj. It is convenient to deﬁne\nthe parameters\n¯η := 1 −η,\n(D1a)\n¯γ := 1 −γ,\n(D1b)\nχ :=\n¯η\n¯γ .\n(D1c)\nWith them, we can express the recursion (10) combined\nwith (11) in explicit form at time step t (see App. D 3 for\ndetails),\nht = hres\nt\n+ hexp\nt\n.\n(D2)\nThe ﬁrst term\nhres\nt\n= ¯γth0 +\n\u00001 −¯γt\u0001\nheq\n(D3)\ndescribes a transition from the initial h0 to the asymp-\ntotic value heq.\nIn particular, hres\nt\n= h0 holds exactly\nfor ¯γ = 1 or for h0 = heq, and otherwise hres\nt\n≈heq holds\nasymptotically for times long enough such that ¯γt ≪1.\nThis reward-independent term hres\nt\nis always present in\n(D2), and (D2) reduces to it if the agent never receives\nany reward, i.e., if the agent is at “rest”.\n[Note that\nin case of an exponential policy function (14), hres\nt\nhas\nno eﬀect on the policy, but this is not of concern for the\npresent discussion.] The second term in (D2) encodes the\nagent’s “experience” and is determined by the history of\nvisits and rewards. Let us refer to time step t + 1 for\nconvenience,\nhexp\nt+1 =\nt\nX\nk=l1\nh\n¯γt−k¯ηk−l(k)i\nλk+1\n(D4a)\n=\njt\nX\nj=1\n¯γt−ljG\nh\nlj : min(lj+1 −1, t), χ\ni\n(D4b)\n=\n\n\njt\nX\nj=1\n¯γt−lj −\njt\nX\nj=2\nχlj−lj−1\n\nG(lj : t, χ). (D4c)\nHere, lj, j = 1, . . . , jt, are the times at which the respec-\ntive edge is visited, i.e., 1 ≤l1 ≤l2 ≤. . .≤ljt ≤t, and jt is\nthus the number of visits up to time t. In (D4a), l(k) is\nthe time of the last visit with respect to time step k, i.e.,\nif lj ≤k < lj+1, then l(k) = lj. Consequently, k −l(k) is\nthe number of steps that have passed at time k since the\nlast visit occurred. In (D4b) we have deﬁned a truncated\ndiscounted return\nG (t : t + T, χ) :=\nT\nX\nk=0\nχkλt+k+1,\n(D5)\nwhich obeys\nG(t1 : t2, χ) = λt1+1 + χ G(t1 + 1 : t2, χ)\n(D6)\nand more generally\nG(t1 : t3, χ) = G(t1 : t2 −1, χ) + χt2−t1G(t2 : t3, χ).\n(D7)\nNote that in (D5), discounting starts at the respective t\nand not at t = 0, hence\nt2\nX\nk=t1\nχkλk+1 = χt1G(t1 : t2, χ).\n(D8)\n2.\nAccumulating glow\nIn the following, we introduce accumulating glow,\nwhich is deﬁned by the following update:\ngt+1 = (1 −η)gt + δt+1,\n(D9)\nwhere each visit adds a value of 1 to the current glow\nvalue of the respective edge. Writing the recursion (10)\n12\ncombined with (D9) instead of (11) in explicit form yields\nhexp\nt+1 =\nt\nX\nk=0\nh\n¯γt−k\n(lj≤k)\nX\nj=1\n¯ηk−lji\nλk+1\n(D10a)\n=\njt\nX\nj=1\n¯γt−ljG(lj : t, χ)\n(D10b)\ninstead of (D4).\nThe diﬀerence is that the subtracted\nsum in (D4c), which represents “multiple re-visits” is not\nincluded in (D10b).\n3.\nDerivation of the explicit expressions for the\nh-value\nWriting the recursion (10) in explicit form gives\nhn = hres\nn + hexp\nn ,\n(D11)\nwhich corresponds to (D2) with hres\nn\ngiven by (D3) and\nhexp\nn\n=\nn−1\nX\nk=0\n¯γn−k−1gkλk+1.\n(D12)\na.\nReplacing glow\nThe recursion (11) for replacing glow yields the explicit\nexpression\ngn = ¯ηng0\nn\nY\nj=1\n¯δj +\nn\nX\nk=1\n¯ηn−kδk\nn\nY\nj=k+1\n¯δj,\n(D13)\nwhere we have deﬁned\n¯δj := 1 −δj\n(D14)\nfor convenience. Setting g0 = 0 gives\ngn =\nn\nX\nk=1\n¯ηn−k∆(k, n),\n(D15)\nwhere\n∆(k, n) := δk\nn\nY\nj=k+1\n¯δj\n(D16)\nis 1 if the last visit occurred at step k [k ≤n, i.e., by\ndeﬁnition ∆(n, n):= 1] and 0 otherwise. Together with\n(D12) we obtain after renaming n as t + 1\nhexp\nt+1 =\nt\nX\nk=0\n¯γt−kgkλk+1\n(D17)\n=\nt\nX\nk=0\nk\nX\nl=1\n¯γt−k¯ηk−l∆(l, k)λk+1.\n(D18)\nRearranging summation (Pt\nk=0\nPk\nl=1 =Pt\nl=1\nPt\nk=l), ap-\nplying (D5) together with (D7) and (D8), and resolving\nthe Kronecker delta symbols then gives (D4).\nb.\nAccumulating glow\nSimilarly, the recursion (D9) for accumulating glow\nyields the explicit expression\ngn = ¯ηng0 +\nn\nX\nk=1\n¯ηn−kδk,\n(D19)\nwhere we set again g0=0. Together with (D12) we obtain\nafter renaming n as t + 1 the same expression as (D17),\nfrom which (E9) and (E10) follow.\nIn (E10) we have\nagain rearranged summation and applied (D5) together\nwith (D8). Resolving the Kronecker delta symbols then\ngives (D10).\n4.\nOrder in glow updating\nNote that in the updating of the replacing edge glow\napplied in [15], the glow value of visited edges is ﬁrst reset\nto one, followed by a damping of all edges by multiplying\ng with ¯η. In contrast, the recursion (11) for replacing\nglow applies the damping ﬁrst and then resets the glow\nvalue of visited edges to 1.\nWe may understand (11)\nas ﬁrst making up for the damping of the previous step\nand then do the actual resetting of the present step. As\nan embedding description we may deﬁne an s-ordered\nreplacing glow update\ngt+1 = sδt+1 + ¯η¯δt+1gt\n(D20)\ngeneralizing (11), where s is a real valued ordering pa-\nrameter. s = ¯η describes the case of 1. resetting and 2.\ndamping as done in [13, 15], whereas s = 1 describes the\ncase of 1. damping and 2. resetting as done in (11). In\nexplicit form, (D20) yields\ngn = ¯ηng0\nn\nY\nj=1\n¯δj + s\nn\nX\nk=1\n¯ηn−kδk\nn\nY\nj=k+1\n¯δj\n(D21)\ninstead of (D13). Analogously, an s-ordered accumulat-\ning glow update\ngt+1 = sδt+1 + ¯ηgt\n(D22)\ngeneralizes (D9) by describing 1.\nincrementing and 2.\ndamping for s = ¯η and 1. damping and 2. incrementing\nfor s = 1 as done in (D9). The explicit form of (D22) is\ngn = ¯ηng0 + s\nn\nX\nk=1\n¯ηn−kδk\n(D23)\ninstead of (D19). The only diﬀerence is the extra factor s\nin the second term in (D21) and (D23) compared to (D13)\nand (D19), respectively, with which the hexp\nt+1 in (D17)\n(which holds for both types of glow) and hence in (D4)\nand (D10) would have to be multiplied. The diﬀerence is\ntherefore minor and irrelevant for our considerations.\n13\nAppendix E: Comparative Analysis of Projective\nSimulation and other RL methods\nA speciﬁc contribution, which the PS-updates have to\noﬀer to RL consists in supplementing the usual forward\ndiscounting with a backward discounting enabled by the\ndamping of the (s, a)-pair values, which amounts to a\ngeneralization of the standard notion of return. On the\nother hand, the incompatibility of discounting with func-\ntion approximation mentioned in App. A may also extend\nto damping.\nIn the following discussion, we want to analyze the dif-\nference between PS and other RL methods.\nThe ﬁrst\nobservation is that neither (D4) nor (D10) update aver-\nages, instead they add “double-discounted” rewards. In\nwhat follows, ﬁrst we show in App. E 0 a how averaging\ncan be implemented before we show in App. E 0 b some\nsimple eﬀects of forward and backward discounting, as-\nsuming that averaging is carried out. Averaging will not\nbe integrated into the PS, as we do not want to give up\nthe simplicity of the PS updates. This discussion merely\nserves as a thorough analysis of the diﬀerences between\nPS and methods that use averaging.\nIn the language of App. A, the basic PS updates (10)\nconstitute a tabular model-free on-policy online learning\nmethod.\nIn the analysis in App. E 0 d, we show that\namong the methods in App. A, it is tabular SARSA(λ)\ndeﬁned in (B5)–(B6), which comes closest to (10), be-\ncause it has an eligibility value z(s, a) ascribed to each\n(s, a)-pair that is the counterpart of the respective glow\nvalue g(s, a) and a trace decay parameter λ, which may\nbe “meta-learned” (i.e., optimized). Thus, in App.E 0 d\nwe analyze the diﬀerences and similarities between PS\nand SARSA.\na.\nImplementing a temporal averaging\nIn this section, we show how temporal averaging can\nbe integrated by adding to the h- and g-value a third\nvariable N =N(s, a) to each (s, a)-pair, which counts the\nnumber of visits by updating it according to\nNt+1 = Nt + δt+1,\n(E1)\nwhich is formally the same update as (D9) for undamped\naccumulating glow. With it, we could consider the nor-\nmalized ˜ht = ht/Nt and initialize with N0 = 1 to avoid\ndivision by zero, so that explicitly Nt = N0 + Pt\nk=1 δk =\njt + 1. Alternatively, we can integrate the normalization\ninto the update rule f, ht+1=Nt+1˜ht+1=f(ht)=f(Nt˜ht)\nby replacing (10) with\n˜ht+1 = αt+1\nαt\n˜ht −γ\n\u0012αt+1\nαt\n˜ht −αt+1heq\n\u0013\n+ αt+1gtλt+1\n≈˜ht −γ(˜ht −αtheq) + αtgtλt+1,\n(E2)\nαt+1 =\nαt\n1 + αtδt+1\n,\n(E3)\nwhere the approximation (E2) holds as soon as αt ≪\n1. Instead of N, we thus then keep for each (s, a)-pair a\nseparate time-dependent learning rate αt=N −1\nt\n=αt(s, a)\nand update it according to (E3).\nFor accumulating glow, (D10b) sums over all visits j\nthe backward-discounted returns that follow these visits\nup to the present t, and ˜hexp\nt+1 thus becomes (for large t)\nan estimate of the average backward-discounted return\nthat follows a visit. In contrast, the ﬁrst-visit counter-\npart (15) only depends on the time l1 of the ﬁrst visit,\nwhich is analytically more easily analyzed in an episodic\nenvironment, where after each episode, the glow values\nof all (s, a)-pairs are reset to zero.\nThe updates involving (E1) or (E3) may be read as a\nlaborious reinvention of an online approximation of an\nevery-visit Monte Carlo method, but provide the follow-\ning insight: For the action value methods in the context\nof Sec. A, the learning rate can in practice (especially\nwhen dealing with deterministic environments) often be\nkept constant rather than gradually decreasing it, where\nthe precise value of this constant doesn’t matter.\nFor\nour updates of ˜h, omitting the correction by N or α and\nworking with the original h should work reasonably well,\ntoo, in such problems.\nb.\nEﬀect of double discounting on a temporal average\nAs an elementary illustration of the eﬀect of forward\ndiscounting via ¯η and backward discounting via ¯γ on\nagent learning consider a weighted arithmetic mean\n¯x(t) =\nPt\nk=1 wkxk\nPt\nk=1 wk\n(E4)\nof random samples xk with variable but known weights\nwk≥0 (w1>0). If the samples are drawn in succession x1,\nx2, . . ., then the average can be updated incrementally,\n¯x(t) = (1 −αt)¯x(t−1) + αtxt,\n(E5)\nwith a “learning rate”\nαt =\nwt\nPt\nk=1 wk\n,\n(E6)\nwhich in general ﬂuctuates within αt ∈[0, 1) depending\non the weight sequence w1, w2, . . .. (Note that an incre-\nmental formulation\nαt+1 =\n\u0012\n1 +\nwt\nwt+1αt\n\u0013−1\n(E7)\nwould require that wk >0 holds ∀k.) Of particular inter-\nest for our discussion is an exponential choice of weights\nwk = wk,\nαt = 1 −w−1\n1 −w−t .\n(E8)\nIn (E8) we can distinguish the following cases:\n14\n(a) For w = 1 all samples are given equal weight and\nthe learning rate αt = t−1 t→∞\n→0 decays to zero in a man-\nner of (C4). In the special case, when the xk are drawn\nfrom i.i.d. random variables Xk ≡X with variance σ2(X)\n= σ2, the total variance σ2\n(t) = t−1σ2 of\nPt\nk=1 wkXk\nPt\nk=1 wk\nvan-\nishes with growing t and ¯x(t) converges to the expecta-\ntion value E(X). In the context of agent learning, we\nmay interpret ¯x(t) as the agent’s experience (e.g., a cur-\nrent value estimate of some given state-action-pair). If\nafter some longer time t ≫1, the environment statistics\nchanges (Xk ≡X no longer holds), the average ¯x(t) will\nstart to change only slowly.\n(b) The case w < 1 in (E8) corresponds to the eﬀect\nof a discounting from the beginning of learning towards\nthe present t by the factor ¯ηk. The learning rate αt\nt≫1\n≈\n(w−1 −1)wt t→∞\n→0 decays to zero exponentially and the\nagent will cease to learn anything after some ﬁnite time\nperiod of the order ∆t ≈−(ln w)−1 due to decay of the\nweights in (E8). After that time, the agent will behave\nsolely according to this early experience “imprinted” into\nit.\n(c) The case w > 1 in (E8) corresponds to the eﬀect\nof discounting from the present t towards the past by a\ndamping factor ¯γt−k. The learning rate αt\nt→∞\n→1 −w−1\nconverges to a positive constant and the agent remains\n“ﬂuid” in its ability to react to changes in the environ-\nment statistics. On the other hand, since it’s remembered\nexperience reaches only a time period of the order ∆t ≈\n(ln w)−1 from the present into the past, such an agent will\njust “chase the latest trend” without learning anything\nproperly.\nIn the special case, when the xk are drawn from i.i.d.\nrandom variables Xk ≡X with variance σ2(X) = σ2, the\ntotal variance σ2\n(t)\nt≫1\n≈\n|w−1|\nw+1 σ2 of\nPt\nk=1 wkXk\nPt\nk=1 wk\nconverges\nto a positive constant in both cases (b) and (c), that\nis, when w ̸= 1. The diﬀerence is that in case (c), the\nweighted mean ¯x(t) keeps ﬂuctuating, whereas in case\n(b), this variance has been “crystallized” into a bias ¯x(t)\n−E(X) of the early experience ¯x(t) which is ﬁxed by the\nsamples in (E4) with respect to the actual E(X).\nc.\nDescription of a formal ensemble average\nWe restrict attention to the simpler accumulating glow\n(D10), which we rewrite with the Kronecker delta sym-\nbols kept explicitly,\nhexp\nt+1 =\nt\nX\nk=0\nk\nX\nl=1\n¯γt−k¯ηk−lλk+1δl\n(E9)\n=\nt\nX\nl=1\n¯γt−lG(l : t, χ)δl\n(E10)\n(see App. D 3 a for the corresponding expressions describ-\ning replacing glow). Each δl samples the “occupation” of\nthe given (s, a)-pair at time l, whose probability is given\nby pl(s, a). For an ensemble of independent agents run-\nning in parallel, we can thus replace the δl with these\nprobabilities and write\n\nhexp\nt+1\n\u000b\nens =\nt\nX\nl=1\n¯γt−lG(l : t, χ)pl\n(E11)\n=\n\n¯γt−lG(l : t, χ)\n\u000b\nl=1,...,t .\n(E12)\nWhile (E10) sums for all times l the respective backward-\ndiscounted return ¯γt−lG(l : t, χ) from that time under the\ncondition that the edge was visited, the ensemble average\n(E11) performs an average with respect to the pl over all\ntimes l up to the present t. The problem is that we do\nnot know the distribution pl, which itself is aﬀected by\nboth the environment and the agent’s policy.\nWhat we can do, however, is to consider the average\nreturn that follows a visit at given time l. The average\nnumber nl = Npl of visits per unit of time at time l is\nfor an ensemble of size N just given by pl, with which\nwe normalize each summand in (E11). The sum over all\ntimes l of these average returns per visit with respect to\ntime l can then be written as\n^\n\nhexp\nt+1\n\u000b\nens =\n1\nN\nt\nX\nl=1\n¯γt−lG(l : t, χ)pl\npl\n(E13)\n=\n1\nN\nt\nX\nl=1\n¯γt−lG(l : t, χ)\n(E14)\n=\n1\nN\n¯γt\n1 −¯η\n\u0002\nG(0 : t, ¯γ−1) −G(0 : t, χ)\n\u0003\n.(E15)\nThe normalization in (E13) is analogous to the one that\nmotivated the logarithm in (A1e) as discussed in App. A\nand E 0 b: it makes the expression independent of the\nstate distribution pl. It also reveals that what we have\ncalled “double discounting”, i.e., the convolution (E14)\nof the return G(l : t, χ) with the exponential ¯γl amounts\nto the diﬀerence (E15) between two returns from the be-\nginning t = 0.\nFor a single agent in Sec. E 0 a, there cannot be more\nthan one visit at each time l. We therefore had to take\nthe sum hexp\nt+1 at time t, and divide it by the total (cu-\nmulative) number of visits Nt that occurred up to this\ntime, to get an estimate ˜ht = ht/Nt of the average re-\nturn per visit. One possibility to implement (E13) for\na single agent consists in training the agent “oﬀ-policy”\nby separating exploration and exploitation, which can be\ndone by choosing the softmax policy (14). During peri-\nods of exploration (e.g. if the agent is not needed), we\nchoose a small β , whereas for exploitation, we temporar-\nily disable the updating (10) and switch to a large β. By\nlarge (small) we mean values of β such that for all x =\nhij encountered in (14), the argument of the exponen-\ntial obeys βx ≫1 (βx ≪1). For graphs that have for\nsymmetry reasons the property that pl ≡p for a random\nwalker (note that for ergodic MDPs the pl eventually be-\ncome independent of the initial conditions), we should be\n15\nable to realize (E13) during the periods of exploration.\nIt is clear, that this is impractical for all but small ﬁnite\nMDPs.\nd.\nRelation of the PS-updates to other RL methods\nIn this section, we compare PS to the standard RL\nmethods presented in App. A. One may interpret the PS-\nupdates (10) as implementing a direct policy method. On\nthe other hand, these updates do not involve gradients.\nTo draw connections between PS and direct policy meth-\nods, we consider the gradient ∇p of the probability pij =\np(aj|si) of selecting action aj in state si, i.e., one element\nof the policy π and restrict to our case (12), i.e., pij =\nΠ(hij)\nκi\n. As in the derivation (B4) of tabular SARSA, we\nidentify the components of the parameter vector θ (with\nrespect to which we want to determine the gradient) with\nthe edges hkl. The gradient of p thus becomes a matrix,\nwhose element kl reads\n(∇pij)kl = ∂pij\n∂hkl\n= Π′(hil)\nκi\n\u0014\nδkiδlj −Π(hij)\nκi\nδki\n\u0015\n,\n(E16)\nwhere Π′(x) = dΠ(x)\ndx . To obtain ∇ln p, we just multi-\nply this with the factor p−1\nij =\nκi\nΠ(hij). The term δkiδlj\nis also present in the PS-update, where it corresponds\nto the strengthening of a visited edge. The subtracted\nsecond term proportional to δki represents a weakening\nof all edges connecting si, which is not present in the\nPS-update.\nWith (E16) we can now compare the PS-updates (10)\nwith the methods in Fig. 2.\nAmong the action value\nmethods, it is tabular SARSA(λ) deﬁned in (B5)–(B6),\nwhich resembles the PS-updates most. Let us rewrite the\nSARSA-updates in the notation used within this. After\nrenaming the reward R as λ, the action value function q\nas h, the eligibility vector z as (matrix) g, the discount\nrate γ as γdis, and the trace-decay rate λ as λtra for clar-\nity, the TD-error (B1) reads\nδTD\nt\n= λt+1 + γdisht(st+1, at+1) −ht(st, at),\n(E17)\nwith which (B5)–(B6) for SARSA(λtra) become\nht+1 = ht + αδTD\nt\ngt,\n(E18)\ngt+1 = γdisλtragt + δt+1,\n(E19)\nwhere δt is a matrix of Kronecker deltas describing which\nof the (s, a)-pairs has been visited at time t. A tabular\ndirect policy method follows in the same way from (A1a),\n(A1e), and (A1f) (setting again Γ = 1): (E17) and (E18)\nremain identical, merely (E19) is replaced with\ngt+1 = γdisλtragt + (∇ln p)t+1,\n(E20)\nwhere for ∇ln p we substitute (E16) together with the ex-\ntra factor as explained in the fext following (E16). While\n(E19) recovers the update (D9) for accumulating glow\n[(D9) considers a given (s, a)-pair, (E19) the whole ma-\ntrix], (E20) is in fact even more complex than (E19).\nOne important diﬀerence is the presence of the term\nht(st+1, at+1) in (E17) which persists even if we disable\nbootstrapping by setting λtra = 1. We can also rewrite\nSARSA in the “local” fashion of the PS-updates (10),\nwhich we here repeat as\nht+1 = ht + λt+1gt −γht + γheq\n(E21)\nfor convenience. To rewrite SARSA(λtra) in the form of\n(E21), we proceed as in the justiﬁcation of accumulating\ntraces outlined in App. C. First, we sum all increments in\n(E18) up to some time T , i.e., hT = h0 + α PT −1\nt=0 δTD\nt\ngt,\nthen rewrite the term involving ht(st+1, at+1) in δTD\nt\nas\nPT −1\nt=0 ht(st+1, at+1)gt = PT\nt=1 ht−1(st, at)gt−1 and sub-\nstitute gt−1 =\ngt−δt\nγdisλtra . If we now ignore (a) the change\nof the h-values over a single time step (which holds for\nsmall α), ht−1(st, at)≈ht(st, at), and (b) ignore the shift\nof argument in the summation (i.e. ignore the ﬁrst and\nlast sum term), then identifying each term referring to a\ngiven t in the sum over all increments with an individual\nupdate leads to a “PS-style” form of SARSA(λtra),\nht+1 = ht + αλt+1gt −αht(st, at)\n\u0002\nλ−1\ntraδt + (1 −λ−1\ntra)gt\n\u0003\n,\n(E22)\nin which γdis no longer appears [it remains in (E19)].\nWe can simplify Equation (E22) if we disable boot-\nstrapping by setting λtra = 1, so that it becomes even\nmore similar to PS. On the other hand, if we take into\naccount that PS does not use averaging, PS carries some\nsimilarities to (an online approximation of) Monte Carlo\napproaches. The type of glow then determines the cor-\nresponding type of Monte Carlo method.\nFor exam-\nple, using replacing glow relates it more to ﬁrst-visit\nMonte Carlo, whereas accumulating glow relates it more\nto every-visit Monte Carlo.\nAppendix F: Mathematical details of the\nconvergence proof\nIn this appendix, we provide the mathematical details\nwe skipped during the proof of Theorem 2 in Sec. IV.\nWe are left with showing that αm(e), given in Eq. (27)\nsatisﬁes Condition 2 in Theorem 1, and Fm(e), given in\nEq. (28), satisﬁes Conditions 3 and 4. From these, Con-\ndition 3 is the most involving, and to some extent is the\ncore of the proof. Condition 4 follows trivially under our\nassumption of bounded rewards. One can easily see that\nbounded rewards imply that ˜h values are upper and lower\nbounded. Given that optimal Q-values are bounded as\nwell it follows that Var{Fm(e)|Pm} ≤K′ for some con-\nstant K′. The remaining two properties are proven in\nthe following.\n16\n1.\nProving that αm(e) satisﬁes Condition 2 in\nTheorem 1\nLet us recall that\nαm(e) :=\nχm(e)\nNm(e) + 1 =\nχm(e)\nPm\nj=1 χj(e) + 1,\n(F1)\nwhere the χm(e) are given by\nχm(e) =\n(\n1\nif e was visited during the mth episode,\n0\notherwise.\n(F2)\nDue to the fact that the policy guarantees inﬁnite explo-\nration, we know that the number of non-zero terms from\nthe sequence Q1 := {αm(e)}1≤m<∞is inﬁnite. Thus let\nQ2 := {˜αn(e)}1≤n<∞be the subsequence of Q1 obtained\nby removing all zero elements, and relabeling the remain-\ning ones as 1, 2, 3, etc. Clearly, we have that\n∞\nX\nm=1\nαm(e) =\n∞\nX\nn=1\n˜αn(e),\n(F3)\n∞\nX\nm=1\n[αm(e)]2 =\n∞\nX\nn=1\n[˜αn(e)]2.\n(F4)\nFurthermore, it is trivial to see that the non-zero terms\n˜αn(e) = 1/(n + 1).\nGiven that P\nn 1/n = ∞and\nP\nn 1/n2 < ∞, it follows that\n∞\nX\nm=1\nαm(e) = ∞,\n(F5)\n∞\nX\nm=1\nα2\nm(e) < ∞,\n(F6)\nas we wanted to prove.\n2.\nContraction of Fm(e)\nIn this part of the appendix we show that in the case\nwhere the glow parameter of the PS model ¯η is set equal\nto the discount parameter γdis associated to the MDP,\nFm(e) := Dm(e) −q∗(e) satisﬁes\n∥E{Fm(·)|Pm}∥W ≤f(γdis)∥∆m(·)∥W + cm,\n(F7)\nwhere ∆m(e) := ˜hm(e) −q∗(e), f(γdis) =\n2γdis\n1−γdis and cm\nconverges to zero w.p.1.\nIn the update rule for ∆m(e) given in Eq. (26), Fm(e)\nappears multiplied by the learning rate coeﬃcient αm(e).\nGiven that αm(e) = 0 in the case where χm(e) = 0, we\ncan, w.l.o.g., deﬁne Fm(e) = 0 for that case.\nThis is\nmade explicit by the factor χm(e) in the deﬁnition of\nFm(e) given in Eq. (28), which for ¯η = γ leads to\nFm(e) = χm(e)\n\n\nTm−tm(e)\nX\nj=0\nγjλtm(e)+j −q∗(e)\n\n. (F8)\nFollowing this deﬁnition of Fm(e) we have that\nE{Fm(e)|Pm} = E{Fm(e)|χm(e) = 1, Pm}.\n(F9)\nTo simplify the notation, in the following we will al-\nways assume that e has been visited at least once dur-\ning episode m, but for simplicity in our notation we omit\nwriting the condition on χm(e) = 1 in the expected value.\nThe past of the process at episode m, Pm, includes ev-\nery state, action, and reward received by the agent from\nt = 0 until the beginning of the episode m. In particular\nit determines the set of ˜hm values, which in turn deter-\nmine the policy at the beginning of the mth episode. For\nthe clarity of this proof we will ﬁrst consider the case\nwhere the policy is kept unchanged during episodes and\nonly updated at the beginning of a new episode, shown\nthat Eq. (F7) holds under those assumptions. Later on\nwe relax that condition and show that the diﬀerences\naccumulated by the policy during the episode converge\nto zero with probability one.\nThis allows us to prove\nEq. (F7) also in the case where the policy is updated\nevery time step.\na.\nConstant policies during the episodes\nGiven that the number of time steps required by an\nagent to hit a terminal state is unbounded, the number\nof terms in Fm(e) could be arbitrarily large. Therefore,\nwe construct a family of truncated versions of Fm(e),\nwhere the maximum number of terms is upper bounded.\nLet us deﬁne\nF (k)\nm (e) :=\nk\nX\nj=0\nΘ(Tm −tm(e) −j)γj\ndisλtm(e)+j\n+Θ(Tm −tm(e) −k)γt+1\ndis ˜hm(Sk+1, Ak+1) −q∗(e),\n(F10)\nwhere Θ(l) = 1 for l ≥0 and it is zero otherwise, and\nwe have deﬁned ˜h(sT , a) = 0, for all terminal states\nsT [30]. Comparing Eqs. (F8) and (F10) one can see that\nF (k)\nm (e) = Fm(e) in the case where the agent takes less\nthan k time steps to ﬁnish the episode since e is visited\nfor the ﬁrst time during the mth episode, i.e. whenever\nTm −tm(e) < k. Considering that the policy guarantees\ninﬁnite exploration, the probability of not reaching a ter-\nminal state after k time steps (during a single episode)\ndecays exponentially with k, and therefore\nE{Fm(e)|Pm} = lim\nk→∞E{F (k)\nm (e)|Pm}.\n(F11)\nIn the following we construct an upper bound for\nE{F (k)\nm (e)|Pm} that holds for all k, and hence due to\nEq. (F11) it also bounds E{Fm(e)|Pm}. We can write\nF (k)\nm (e) in the following form\n17\nE{F (k)\nm (e)|Pm} =r(e) +\nk\nX\nl=1\nX\ne(1),...e(l)\nPr\nh\ne, e(1), . . . , e(l)|Pm\ni\nγl\ndisr(e(l))\n+\nX\ne(1),...,e(k+1)\nPr\nh\ne, e(1), . . . , e(k+1)|Pm\ni\nγk+1\ndis ˜hm(e(k+1)) −q∗(e),\n(F12)\nwhere\nwe\nused\nthe\nshort-hand\nnotation\nPr\n\u0002\ne, e(1), . . . , e(l)|Pm\n\u0003\nto denote the probability of\nan agent following the sequence of state-action pairs\ne, e(1), . . . , e(l).\nGiven that, for the moment, we are\nconsidering constant policies during episodes,\nthese\nprobabilities only depend on the episode index m.\nIn\naddition, in order to have a simpler expression, w.l.o.g.\nwe assume that transitions from a terminal state return\nwith probability one to a terminal state with zero\nreward.\nNote that this assumption together with the\nfact that ˜hm(sT , a) = 0 for all terminal states sT , allows\nus to write the summations in Eq. (F12) over all edges,\nincluding those associated to terminal states.\nThe following step consists in writing E{F (k)\nm (e)|Pm}\nas a recursive relation in k. Given that the policies are\nkept constant during episodes they satisfy that\nPr\nh\ne, e(1), . . . e(k+1)|Pm\ni\n=\nPr\nh\ne, e(1), . . . , e(k)|Pm\ni\nPr\nh\ne(k), e(k+1)|Pm\ni\n.\n(F13)\nPlugging this equation into Eq. (F12) and adding cancel-\ning terms we end up with the expression\nE{F (k)\nm (e)|Pm} =r(e) +\nk−1\nX\nl=1\nX\ne(1),...,e(l)\nPr\nh\ne, e(1), . . . , e(l)|Pm\ni\nγl\ndisr(e(l))\n+\nX\ne(1),...,e(k)\nPr\nh\ne, e(1), . . . , e(k)|Pm\ni\nγk\ndis˜hm(e(k)) −q∗(e)\n+\nX\ne(1),...,e(k)\nPr\nh\ne, e(1), . . . , e(k)|Pm\ni\nγk\ndis\nn\n−˜hm(e(k)) + q∗(e(k))\n+ r(e(k)) +\nX\ne(k+1)\nPr[e(k), e(k+1)|Pm] γdis˜hm(e(k+1)) −q∗(e(k))\no\n.\n(F14)\nComparing Eqs. (F12) and (F14) one can see that\nthe ﬁrst two lines in the previous equation equal\nE{F (k−1)\nm\n(e)|Pm}. Furthermore, in the third and fourth\nlines, the quantities within curly brackets correspond to\nthe deﬁnition of ∆m(e(k)) and E{F (0)\nm (e(k))|Pm} respec-\ntively. Hence | E{F (k)\nm (e)|Pm}| obeys the following recur-\nsive relation\n\f\f\fE{F (k)\nm (e)|Pm}\n\f\f\f ≤\n\f\f\fE{F (k−1)\nm\n(e)|Pm}\n\f\f\f\n+ γk\ndis∥∆m(·)∥W\n+ γk\ndis\n\r\r\rE{F (0)\nm (·)|Pm}\n\r\r\r\nW.\n(F15)\nBy iterating the previous equation we achieve the follow-\ning bound\n\f\f\fE{F (k)\nm (e)|Pm}\n\f\f\f\n≤\nk\nX\nl=0\nγl\ndis\n\r\r\rE{F (0)\nm (·)|Pm}\n\r\r\r\nW +\nk\nX\nl=1\nγl\ndis∥∆m(·)∥W\n≤\n1\n1 −γdis\n\u0010\r\r\rE{F (0)\nm (·)|Pm}\n\r\r\r\nW + γdis∥∆m(·)∥W\n\u0011\n,\n(F16)\nwhere we have used the relation P∞\nl=0 γl\ndis =\n1\n1−γdis to\nobtain a bound that is independent of both e and k.\nNotice that F (0)\nm (e) corresponds to the kind of update\nterm encountered in the single-step algorithm of SARSA.\nIt has been proven in [18] as part of the convergence proof\nof the SARSA method that F (0)\nm\nsatisﬁes\n\r\r\rE{F (0)\nm (·)|Pm}\n\r\r\r\nW ≤γdis∥∆m(·)∥W + dm,\n(F17)\n18\nwhere dm converges to zero w.p.1. as m →∞.\nHere\nwe recall from [18] the main mathematical steps to prove\nEq. (F17) as they will be useful later, when we consider\nthe general scenario with time-dependent policies. The\nexpected value of F (0)\nm (e) can be written explicitly as\nE{F (0)\nm (s, a)|Pm}\n= r(s, a) + γdis\nX\ns′\nPr(s′|s, a)\nX\na′\nPr (a′|s′, Pm) ˜hm(s′, a′)\n−q∗(s, a)\n= fm(s, a) + γdis\nX\ns′\nPr(s′|s, a)gm(s′),\n(F18)\nwhere we have deﬁned\nfm(s, a) = r(s, a) + γdis\nX\ns′\nPr (s′|s, a) max\nb {˜hm(s′, b)}\n−q∗(s, a),\ngm(s′) =\nX\na′\nPr (a′|s′, Pm) ˜hm(s′, a′) −max\nb {˜hm(s′, b)}.\n(F19)\nThe ﬁrst term given above corresponds to the update\nterm encountered in Q-learning algorithms and it has\nbeen proven to be bounded by\n|fm(s, a)| ≤γdis∥∆m(·)∥W, ∀s, a.\n(F20)\nIn order to bound gm(s), let us recall that the policy\n(under the assumption that it is kept constant during an\nepisode) is given by\nPr(a|s, Pm) =\nexp[βm˜hm(s, a)]\nP\nb exp[βm˜hm(s, b)]\n.\n(F21)\nBy simple algebraic manipulation, one can see that\ngm(s) < na\nβm\n, ∀s,\n(F22)\nwhere na = ||A|| is the number of actions (assumed ﬁ-\nnite). Due to the fact that βm →∞as m →∞it follows\nthat gm(s) converges to zero w.p.1. Eq. (F17) is recov-\nered by plugging Eqs. (F22) and (F20) into Eq. (F18) and\ndeﬁning dm = γdisna/βm. Finally, replacing Eq. (F17)\ninto Eq. (F16) we obtain the desired bound\n\r\r\rE{F (k)\nm (·)|Pm}\n\r\r\r\nW ≤\n2γdis\n1 −γdis\n∥∆m(·)∥W + cm\n∀k,\n(F23)\nwhere cm ≡\n1\n1−γdis dm also converges to zero w.p.1.\nEq. (F23) proves the contraction property for the case\nwhere the policy is not updated during episodes. Below\nwe discuss the general case, where the policy may change\nevery time step. As we will see the only diﬀerence with\nrespect to the case discussed above is that an additional\nterm must be added to the right hand side of Eq. (F23).\nSince this additional term converges to zero w.p.1 the\ncontraction property also holds in that case.\nb.\nPolicy update at every time step\nWhen on line updates are considered the policy might\nchange every time step. Therefore, the probabilities in\nEq. (F14) no longer depend exclusively on Pm but rather\nalso on the rewards received by the agent during the\nepisode.\nHowever, since most of this probabilities are\ntaken as common factors and upper bounded by one, one\ncan verify that most of the previous derivations still hold\nin the case where policies are changed every time step. In\nfact the only point where the previous derivations have\nto be generalized is in Eq. (F18). A time-dependent gen-\neralization of gm(s) can be deﬁned by\ng′(t)\nm (s) =\nX\na\nπ(t)\nm (a|s)˜hm(s, a) −max\nb {˜hm(s, b)}, (F24)\nwhere t could be any time step in the interval Im =\n[Tm + 1, Tm+1], i.e. the interval of time steps between\nthe beginning and the end of episode m, and the policy\nis now given by\nπ(t)\nm (a | s) =\nexp[βm˜h(t)(s, a)]\nP\nb∈A exp[βm˜h(t)(s, b)]\n.\n(F25)\nIt is easy to verify that, similarly as in Eq. (F22), if ∀t ∈\nIm, g′(t)\nm (s) is upper bounded by a sequence converging\nto zero, Eq. (F17) also holds for time-dependent policies\nand hence Eq. (F23) too.\nNote that the only diﬀerence between Eqs. (F21) and\n(F25) is that in the former, the policy depends on the\n˜hm values while in the latter on the ˜h(t), with t ∈Im.\nThe diﬀerence between these two, however, tends to zero\nw.p.1 as the number of episodes increase.\nGiven that\nrewards are bounded in the sense that R(t) ≤BR, ∀t for\ncertain constant BR, we have that\n|˜ht(e)−˜hm(e)| ≤\n1\nNm + 1\n\u0014\n|˜hm(e)| +\nBR\n1 −γdis\n\u0015\n≤\nC\nNm + 1,\n(F26)\nwhere C is a constant. Since Nm →∞w.p.1, the previ-\nous diﬀerence converges to zero.\nTo exploit Eq. (F26), we bound |g(t)\nm (s)| in the follow-\ning way\n|g(t)\nm (s)| ≤| max\na {˜hm(s, a)} −max\na {˜h(t)(s, a)}|\n+ | max\na {˜h(t)(s, a)} −\nX\na\nπ(t)(a|s)˜h(t)(s, a)|\n+\nX\na\nπ(t)(a|s)|˜h(t)(s, a) −˜hm(s, a)|.\n(F27)\nIt follows from Eq. (F26) that the ﬁrst and third line in\nthe equation above are each upper bounded by C/(Nm +\n1). Furthermore, the second term can be upper bounded\nby na/βm in the exact same way as in Eq. (F22). Thus,\n|g′(t)\nm | ≤na\nβm\n+ 2\nC\nNm + 1,\n(F28)\n19\nwhich converges to zero w.p.1.\nThis implies that\nEq. (F18) also holds in the general case where the policy\nis updated every time step. This completes the proof of\nEq. (F23).\nAppendix G: GLIE policy\nIn this appendix, we consider a policy given by the\nprobabilities\nPr(t)\nm (a | s) =\nexp[βm˜h(t)(s, a)]\nP\nb∈A exp[βm˜h(t)(s, b)]\n,\n(G1)\nand derive conditions on the coeﬃcients βm in order to\nguarantee that this policy is GLIE on the ˜h-values. We\nclosely follow the derivation provided in [18], and there-\nfore will omit most of the details. Here, however, we fo-\ncus on episodic tasks, in contrast to the continuous time\ntasks considered in [18]. Moreover, the coeﬃcients βm\nwill depend exclusively on the episode index m, instead\nof the state s. In this way we preserve a local model in\nthe sense deﬁned in the main text, where the β coeﬃcient\ncan be updated using exclusively environmental signals\n(in this case, the end of an episode).\nLetting βm →∞is enough to guarantee that the pol-\nicy is greedy in the limit. However, the speed at which\nβm grows as a function of m must be upper bounded in\norder to additionally guarantee inﬁnite exploration. In\nthe following we derive such a bound.\nLet us denote as Prm(s, a) the probability that during\nepisode m the state-action pair (s, a) is visited at least\nonce. Hence, inﬁnite exploration occurs if ∀s, a\n∞\nX\nm=1\nPrm(s, a) = ∞.\n(G2)\nConsidering that P∞\nm=1 c/m = ∞for any constant c, as\na consequence of the Borel Cantelli lemma we have that\na suﬃcient condition for Eq. (G2) is given by\nPrm(s, a) ≥c/m,\n(G3)\nfor some constant c. Therefore we would like to pick a\nbound on βm in such a way that Eq. (G3) is satisﬁed.\nLet us denote by Prm(s) the probability that during\nepisode m state s is visited at least once and let\npmin(m) =\nmin\na,s,t∈Im\nn\nPr(t)\nm (a|s)\no\n(G4)\nbe the minimum probability to take any action at any\ntime step during the mth episode. It follows that\nPrm(s, a) ≥Prm(s)pmin(m).\n(G5)\nThe ﬁrst factor can in turn be bounded by a function\nof pmin(m) by noting the following. In a communicat-\ning MDP, any state can be reached from any other non-\nterminal state with nonzero probability.\nThat means\nthat independently of the initial state of the mth episode,\nthere exists a sequence of actions that lead to any state\ns with some nonzero probability p0. Such probability is\nconstant and it is given by a product of transition prob-\nabilities of the MDP. In the worst case scenario, it could\nhappen that s can only be reached by taking a speciﬁc\nsequence of actions that leads the agent to visit all non\nterminal states before reaching s. Hence Prm(s) can be\nbounded by the product of probabilities to pick those ac-\ntions weighted by the transition probabilities of such a\nsequence. Given that the probability to take any action\nis in turn lower bounded by pmin(m), we conclude that\nPrm(s) ≥p0 [pmin(m)]ns−1 ,\n(G6)\nwhere ns = ||S|| −||ST || is the number of non terminal\nstates.\nIn order to bound pmin(m) note that\nPr(t)\nm (a|s) ≥1\nna\nexp\n\u0014\n−max\na,b\nn\n˜h(t)(a|s) −˜h(t)(b|s)\no\nβm\n\u0015\n≥1\nna\nexp\n\u0002\n−2B˜hβm\n\u0003\n,\n(G7)\nwhere B˜h ≥˜h(t)(s, a), ∀s, a is an upper bound that exists\nbecause the rewards are bounded. Hence, it follows that\npmin(m) ≥exp\n\u0002\n−2B˜hβm\n\u0003\n/na. Replacing this inequality\nin Eq. (G6) and using Eq. (G5) we get that\nPrm(s, a) ≥p0 pns\nmin(m)\n≥p0\nnns\na\nexp\n\u0002\n−2nsB˜hβm\n\u0003\n.\n(G8)\nTherefore, by choosing βm in such a way that\nβm ≤\n1\n2nsB˜h\nln(m),\n(G9)\nEq. (G3) holds, and thus the policy is guaranteed to pre-\nserve inﬁnite exploration of all state-action pairs.\nReferences\n20\n[1] M. A. Nielsen and I. L. Chuang, Quantum Computation\nand Quantum Information. Cambridge: Cambridge Uni-\nversity Press, 2000.\n[2] C. H. Bennett and D. P. DiVincenzo, “Towards an en-\ngineering era?,” Nature, vol. 377, no. 6548, pp. 389–390,\n1995.\n[3] M. Schuld, I. Sinayskiy, and F. Petruccione, “The quest\nfor a quantum neural network,” Quantum Information\nProcessing, vol. 13, pp. 2567–2586, Nov. 2014.\n[4] J. Biamonte, P. Wittek, N. Pancotti, P. R. N., Wiebe,\nand S. Lloyd, “Quantum machine learning,” vol. 549, 11\n2016.\n[5] V. Dunjko and H. Briegel, “Machine learning & artiﬁcial\nintelligence in the quantum domain: a review of recent\nprogress,” Reports on Progress in Physics, vol. 81, no. 7,\n2018.\n[6] H. J. Briegel and G. D. las Cuevas, “Projective simula-\ntion for artiﬁcial intelligence,” Sci. Rep., vol. 2, p. 400,\n2012.\n[7] G. Paparo, V. Dunjko, A. Makmal, M. A. Martin-\nDelgado, and H. J. Briegel, “Quantum speed-up for ac-\ntive learning agents,” Phys. Rev. X, vol. 4, p. 031002,\n2014.\n[8] T. Sriarunothai, S. W¨olk, G. S. Giri, N. Friis, V. Dun-\njko, H. J. Briegel, and C. Wunderlich, “Speeding-up the\ndecision making of a learning agent using an ion trap\nquantum processor,” Quantum Science and Technology,\nvol. 4, no. 1, p. 015014, 2018.\n[9] A. A. Melnikov, H. Poulsen Nautrup, M. Krenn, V. Dun-\njko, M. Tiersch, A. Zeilinger, and H. J. Briegel, “Active\nlearning machine learns to create new quantum experi-\nments,” Proc. Natl. Acad. Sci. U.S.A., vol. 115, p. 1221,\n2018.\n[10] H. P. Nautrup, N. Delfosse, V. Dunjko, H. J. Briegel, and\nN. Friis, “Optimizing Quantum Error Correction Codes\nwith Reinforcement Learning,” Quantum, vol. 3, p. 215,\nDec. 2019.\n[11] S. Hangl, V. Dunjko, H. J. Briegel, and J. Piater, “Skill\nlearning by autonomous robotic playing using active\nlearning and exploratory behavior composition,” Fron-\ntiers in Robotics and AI, vol. 7, p. 42, 2020.\n[12] S. Hangl, E. Ugur, S. Szedmak, and J. Piater, “Robotic\nplaying for hierarchical complex skill learning,” in Proc.\nIEEE/RSJ Int. Conf. Intell. Robots Syst., pp. 2799–2804,\n2016.\n[13] A. A. Melnikov, A. Makmal, and H. J. Briegel, “Bench-\nmarking Projective Simulation in Navigation Problems,”\nIEEE Access, vol. 6, pp. 64639–64648, 2018.\n[14] A. A. Melnikov, A. Makmal, V. Dunjko, and H. J.\nBriegel, “Projective simulation with generalization,” Sci.\nRep., vol. 7, p. 14430, 2017.\n[15] J. Mautner, A. Makmal, D. Manzano, M. Tiersch,\nand H. J. Briegel, “Projective Simulation for Classical\nLearning Agents: A Comprehensive Investigation,” New\nGener. Comput., vol. 33, p. 69, 2015.\n[16] A. Makmal, A. A. Melnikov, V. Dunjko, and H. J.\nBriegel, “Meta-learning within Projective Simulation,”\nIEEE Access, vol. 4, p. 2110, 2016.\n[17] P. Dayan and T. J. Sejnowski, “TD(λ) converges with\nprobability 1,” Machine Learning, vol. 14, no. 3, pp. 295–\n301, 1994.\n[18] S. Singh, T. Jaakkola, M. L. Littman, and C. Szepesv´ari,\n“Convergence\nresults\nfor\nsingle-step\non-policy\nreinforcement-learning\nalgorithms,”\nMachine\nlearn-\ning, vol. 38, no. 3, pp. 287–308, 2000.\n[19] T. Jaakkola, M. I. Jordan, and S. P. Singh, “Conver-\ngence of stochastic iterative dynamic programming al-\ngorithms,” in Advances in neural information processing\nsystems, pp. 703–710, 1994.\n[20] C. J. Watkins and P. Dayan, “Q-learning,” Machine\nlearning, vol. 8, no. 3-4, pp. 279–292, 1992.\n[21] R. S. Sutton and A. G. Barto, Reinforcement Learning:\nAn Introduction. Cambridge, MA: MIT Press, second ed.,\n2018.\n[22] H. J. Briegel, “On creative machines and the physical\norigins of freedom,” Sci. Rep., vol. 2, p. 522, 2012.\n[23] V. Dunjko, J. M. Taylor, and H. J. Briegel, “Quantum-\nenhanced machine learning,” Phys. Rev. Lett., vol. 117,\np. 130501, 2016.\n[24] J. Clausen and H. J. Briegel, “Quantum machine learning\nwith glow for episodic tasks and decision games,” Phys.\nRev. A, vol. 97, p. 022303, 2018.\n[25] A. Dvoretzky et al., “On stochastic approximation,” in\nProceedings of the Third Berkeley Symposium on Math-\nematical Statistics and Probability, Volume 1: Contri-\nbutions to the Theory of Statistics, The Regents of the\nUniversity of California, 1956.\n[26] We will follow the notation introduced in [21] closely.\n[27] We can assume without loss of generality that the envi-\nronment signals the ﬁnalization of the episode. Thus the\nﬁrst visits to an edge can be determined locally. More-\nover, the same signal can be used to reset the glow values\nlocally.\n[28] This theorem is a known result in the ﬁeld of Stochastic\nApproximation. While the ﬁrst version of the theorem\nwas presented in [25], it can be found in many forms\nin the literature. The version presented here, where the\ncontraction property has been relaxed by allowing a noise\nthat tends to zero, has been presented in [18].\n[29] These examples are not exhaustive. In a wider sense, one\nmay also mention decision trees with θ deﬁning the split\npoints and leaf values.\n[30] Since episodes end when a terminal state is reached, the\nPS agent does not need to store h-values associated with\nterminal states, thus we can deﬁne them equal to zero.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "quant-ph",
    "stat.ML"
  ],
  "published": "2019-10-25",
  "updated": "2020-11-12"
}