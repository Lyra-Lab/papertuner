{
  "id": "http://arxiv.org/abs/1910.12024v1",
  "title": "SUPER Learning: A Supervised-Unsupervised Framework for Low-Dose CT Image Reconstruction",
  "authors": [
    "Zhipeng Li",
    "Siqi Ye",
    "Yong Long",
    "Saiprasad Ravishankar"
  ],
  "abstract": "Recent years have witnessed growing interest in machine learning-based models\nand techniques for low-dose X-ray CT (LDCT) imaging tasks. The methods can\ntypically be categorized into supervised learning methods and unsupervised or\nmodel-based learning methods. Supervised learning methods have recently shown\nsuccess in image restoration tasks. However, they often rely on large training\nsets. Model-based learning methods such as dictionary or transform learning do\nnot require large or paired training sets and often have good generalization\nproperties, since they learn general properties of CT image sets. Recent works\nhave shown the promising reconstruction performance of methods such as\nPWLS-ULTRA that rely on clustering the underlying (reconstructed) image patches\ninto a learned union of transforms. In this paper, we propose a new\nSupervised-UnsuPERvised (SUPER) reconstruction framework for LDCT image\nreconstruction that combines the benefits of supervised learning methods and\n(unsupervised) transform learning-based methods such as PWLS-ULTRA that involve\nhighly image-adaptive clustering. The SUPER model consists of several layers,\neach of which includes a deep network learned in a supervised manner and an\nunsupervised iterative method that involves image-adaptive components. The\nSUPER reconstruction algorithms are learned in a greedy manner from training\ndata. The proposed SUPER learning methods dramatically outperform both the\nconstituent supervised learning-based networks and iterative algorithms for\nLDCT, and use much fewer iterations in the iterative reconstruction modules.",
  "text": "SUPER Learning: A Supervised-Unsupervised Framework for Low-Dose CT\nImage Reconstruction ∗\nZhipeng Li1\nSiqi Ye1\nYong Long1†\nSaiprasad Ravishankar2\n1University of Michigan - Shanghai Jiao Tong University Joint Institute,\nShanghai Jiao Tong University‡, Shanghai, China\n2Departments of Computational Mathematics, Science and Engineering,\nand Biomedical Engineering, Michigan State University, East Lansing, MI, USA\n{zhipengli, yesiqi, yong.long}@sjtu.edu.cn, ravisha3@msu.edu\nAbstract\nRecent years have witnessed growing interest in machine\nlearning-based models and techniques for low-dose X-ray\nCT (LDCT) imaging tasks. The methods can typically be\ncategorized into supervised learning methods and unsuper-\nvised or model-based learning methods. Supervised learn-\ning methods have recently shown success in image restora-\ntion tasks. However, they often rely on large training sets.\nModel-based learning methods such as dictionary or trans-\nform learning do not require large or paired training sets\nand often have good generalization properties, since they\nlearn general properties of CT image sets. Recent works\nhave shown the promising reconstruction performance of\nmethods such as PWLS-ULTRA that rely on clustering the\nunderlying (reconstructed) image patches into a learned\nunion of transforms.\nIn this paper, we propose a new\nSupervised-UnsuPERvised (SUPER) reconstruction frame-\nwork for LDCT image reconstruction that combines the\nbeneﬁts of supervised learning methods and (unsupervised)\ntransform learning-based methods such as PWLS-ULTRA\nthat involve highly image-adaptive clustering. The SUPER\nmodel consists of several layers, each of which includes a\ndeep network learned in a supervised manner and an un-\nsupervised iterative method that involves image-adaptive\ncomponents.\nThe SUPER reconstruction algorithms are\nlearned in a greedy manner from training data. The pro-\nposed SUPER learning methods dramatically outperform\nboth the constituent supervised learning-based networks\nand iterative algorithms for LDCT, and use much fewer it-\n∗Copyright (c) 2019 IEEE. Personal use of this material is permit-\nted.Permission from IEEE must be obtained for all other uses, in any cur-\nrent or future media, including reprinting/republishing this material for ad-\nvertising or promotional purposes, creating new collective works, for resale\nor redistribution to servers or lists, or reuse of any copyrighted component\nof this work in other works.\n†Yong Long is the corresponding author.\n‡This work was supported by NSFC (61501292).\nerations in the iterative reconstruction modules.\n1. Introduction\nX-ray computed tomography (CT) is a popular imaging\nmodality in many clinical and industrial applications. There\nhas been particular interest in CT imaging with low X-ray\ndose levels that would reduce the potential risks to patients\nfrom radiation. However, image reconstruction at low X-\nray dose levels is challenging. Conventional X-ray CT im-\nage reconstruction methods include analytical methods, and\nmodel-based iterative reconstruction (MBIR) methods. A\nclassical analytical method is the ﬁltered back-projection\n(FBP) method [1]. That can be degraded excessively by\nnoise and streak artifacts in low-dose situations [2,3].\nMBIR methods incorporate the system physics, statis-\ntical model of measurements, and typically certain sim-\nple prior information of the unknown object [4]. A typ-\nical method of this kind is the penalized weighted-least\nsquares (PWLS) method, for which the cost function in-\ncludes a weighted quadratic data-ﬁdelity term that models\nthe measurement statistics, and a penalty term called a regu-\nlarizer that models the prior information [5–7]. For PWLS,\nvarious optimization approaches and regularization designs\nhave been exploited with efﬁciency and convergence guar-\nantees.\nAdopting appropriate prior knowledge of images for\nMBIR approaches is also important to improve CT recon-\nstruction. More recently, with the availability of data sets of\nCT images, methods based on big-data priors have gained\ninterest, such as dictionary learning-based techniques [8].\nThe dictionary can be either pre-learned from training data,\nor adaptively learned with the reconstruction. In particu-\nlar, the synthesis dictionary learning approaches represent a\nsignal or image patch as a sparse linear combination of the\natoms or columns of a learned dictionary, and have obtained\npromising results in many applications [9–11]. However,\narXiv:1910.12024v1  [cs.LG]  26 Oct 2019\nthe dictionary learning based MBIR approaches are often\ncomputationally expensive due to expensive sparse coding\n(where typically NP-hard problems are optimized for esti-\nmating sparse coefﬁcients). Different from synthesis dictio-\nnary learning, sparsifying transform (a generalized analysis\ndictionary model) learning techniques efﬁciently adapt an\noperator to approximately sparsify signals in transform do-\nmains, and the corresponding transform sparse coding prob-\nlem can be solved exactly and cheaply by thresholding [12].\nSparsifying transform learning techniques including using\ndoubly-sparse transforms and unions of transforms have\nbeen applied to image reconstruction and obtained promis-\ning results [13–15].\nVery recently, there has been growing interest in deep\nlearning approaches for medical imaging problems [16–21].\nIn the LDCT image reconstruction ﬁeld, typical deep learn-\ning methods learn the reconstruction mapping from large\ndatasets of pairs of (low-dose and regular-dose) scans.\nThese methods include image-domain learning, sensor-\ndomain learning, and hybrid-domain learning. For example,\na particular image-domain learning approach is the FBP-\nConvNet scheme [19] that solves the normal-convolutional\ninverse problems by applying a (learned) CNN after the di-\nrect inversion that encapsulates the system physics. The\nimage-domain learning approaches can have many varia-\ntions. For example, instead of directly working in the image\ndomain, one can transform the images to a speciﬁc domain\nand learn in such domain the relationship between training\npairs. Kang et al. [20] designed a neural network that learns\na mapping between contourlet transform coefﬁcients of the\nlow-dose input and its high-dose counterpart. This work\nwas later extended to learn a wavelet domain residual net-\nwork (WavResNet) [21].\nIn the sensor-domain deep learning category, W¨urﬂet\nal. [22] proposed an end-to-end neural network for low-dose\nCT that maps the sinogram to the reconstructed image by\nmapping the ﬁltered back-projection algorithm to a basic\nneural network. This allows one to take into account the\nartifacts in the sensor-domain, e.g., the scatter and beam-\nhardening artifacts, and compensate them in the learning\nprocess. Another framework named the Automated Trans-\nform by Manifold Approximation (AUTOMAP) [23] learns\na direct mapping from the measurement domain to image\ndomain. However, due to the high memory requirements\nfor storing fully connected layers, it is challenging for AU-\nTOMAP to handle large scale reconstruction tasks such\nas CT image reconstruction. Hybrid-domain learning ap-\nproaches exploit data-ﬁdelity terms in the neural network\narchitecture. The Learned ISTA (LISTA) [24] was one of\nthe earliest work of this kind.\nLISTA unfolds the itera-\ntive soft-thresholding (ISTA) algorithm [25], and learns the\nweight matrices and the sparsifying soft-thresholding op-\nerator.\nLater, Yang et al. [26] proposed an ADMM-Net\nwhich unfolds the alternating direction method of multipli-\ners algorithm for image reconstruction. Each step of the\nalgorithm is mapped to a neural network module.\nThis\nidea was then extended to a learnable primal-dual approach\nbased CNN [27]. These methods fall in the class of physics-\ndriven deep learning methods [28–30]. Hybrid-domain ap-\nproaches also include a type that applies a plug-and-play\nmodel. He et al. [31] applied the plug-and-play model to\nthe ADMM algorithm and unfolded it into a deep recon-\nstruction network, so that each network module is learnable\nand replaceable.\nMost deep learning algorithms are learned in a super-\nvised manner (using task-speciﬁc cost functions) and re-\nquire large training sets. However, in CT imaging, it is often\ndifﬁcult to acquire large datasets of training image pairs.\nEven though in the AAPM X-ray CT Low-Dose Grand\nChallenge, both regular-dose and the matched quarter-\ndose images were provided, only the regular-dose images\nwere reconstructed from real scans, while the matched\nquarter-dose images were synthesized by adding noise to\nthe regular-dose sinogram data. Therefore, training with\nsmaller number of paired data (and yet generalizing) or\nwithout reference data is highly conducive for CT image re-\nconstruction. Moreover, different machine learning (as well\nas conventional) approaches such as dictionary or transform\nlearning and deep learning use different types of big-data\npriors and are advantageous in different ways. For example,\ntransform learning approaches learn general image proper-\nties and features in an unsupervised or model-based man-\nner, and can easily and effectively adapt to speciﬁc image\ninstances.\nIn this work, we propose a new image reconstruction\nframework for LDCT dubbed Supervised-UnsuPERvised\n(SUPER) learning. The algorithm architecture involves in-\nterconnected supervised (deep network) and unsupervised\n(iterative reconstruction) modules over many layers. The\narchitecture enables effectively leveraging different kinds\nof big data learned priors for CT reconstruction. For ex-\nample, we used FBPConvNet [19] as the supervised (deep)\nlearned module and PWLS-ULTRA [14] as the unsuper-\nvised module with a pre-learned union of transforms, which\nprovided both high quality image reconstruction and image-\nadaptive clustering. The proposed SUPER learning used\nrelatively small training sets and dramatically outperformed\nboth deep learning and transform/dictionary learning by ef-\nfectively combining task-speciﬁc and image or instance-\nspeciﬁc adaptivity. The proposed framework is generaliz-\nable to include various constituent modules including non-\nlearning based algorithms, as shown in our experiments.\n2. Proposed Model and Algorithms\nHere, we present the proposed reconstruction model, its\nmotivations and interpretations, example architectures, and\nSupervised \nModule 1 \nIterative \nModule 1 \nSupervised \nModule 2 \nIterative \nModule 2 \nSupervised \nModule M \nIterative \nModule M \n𝒙(𝟎)   \n𝒙 𝑴\nSUPER Layer 1 \nSUPER Layer 2 \nSUPER Layer M \nFigure 1: Overall structure of the proposed reconstruction framework.\ntraining method.\n2.1. Overview of the SUPER Model\nWe propose a novel efﬁcient physics-driven learning\nframework for CT reconstruction that effectively combines\nthe beneﬁts of supervised (deep) learning and unsupervised\niterative reconstruction methods. The proposed reconstruc-\ntion architecture takes an initial image as input and pro-\ncesses it through multiple “super” layers (Fig. 1).\nEach\nsuper layer consists of a network learned in a supervised\nmanner (supervised module) and an iterative reconstruc-\ntion method (iterative module) in sequence. The supervised\nmodule is different in each super layer, i.e., the weights\nin the supervised module are not shared among the su-\nper layers. Importantly, this module is learned in a super-\nvised manner (e.g., to minimize reconstruction error) to re-\nmove artifacts and noise. The iterative module on the other\nhand iteratively optimizes a regularized image reconstruc-\ntion problem using image-adaptive priors or regularizers\n(e.g., the patches of the underlying image can be clustered\nand sparsiﬁed in a learned union of transforms or dictionar-\nies [14]). The iterative algorithm is run for a ﬁxed number\nof iterations in each super layer.\nWhile the supervised module removes image noise and\nartifacts using a single learned network, the iterative mod-\nule could adapt various image-speciﬁc features in an MBIR\nsetup to further improve image quality and remove artifacts.\nImportantly, the iterative module is not learned in a su-\npervised manner. The SUPER model in Fig. 1 is ﬂexible\nand could use various architectures for the supervised mod-\nule (e.g., FBPConvNet [19], WavResNet [21], etc.) and\na variety of iterative data-adaptive methods (e.g., PWLS-\nULTRA [14]). The model could be potentially used in a\nvariety of imaging as well as other applications.\n2.2. Interpretations and Generalization\nThe SUPER model enables combining different kinds\nof machine learned models and priors in a common recon-\nstruction framework. While the supervised module could\nbe a deep convolutional network learned from a big dataset\nto optimize task-speciﬁc performance metrics, the iterative\nmodule could exploit models learned from images using cri-\nteria such as sparsity, manifold properties, etc. For exam-\nple, an operator could be pre-learned from CT images or\npatches to approximately sparsify them (a.k.a. transform\nlearning [12]) and used to construct the regularizer for the\niterative module. Such image-based learned operators are\nnot typically task-sensitive and often generalize readily to\ndifferent settings and can help delineate or reconstruct vari-\nous image features.\nThe proposed SUPER model can also help combine\nglobal adaptivity and image-speciﬁc adaptivity to obtain\nthe best of both worlds.\nWhile each supervised module\nis learned from a dataset and ﬁxed during reconstruction,\nthe iterative module could optimize novel and speciﬁc fea-\ntures for each image being reconstructed (during training\nand testing) and thus capture the diversity of images and\nenable highly adaptive reconstructions. For example, dur-\ning training and testing, the iterative module could cluster\nimage patches differently for each image [14], or even learn\nnovel models such as dictionaries for each image [32].\nAnother interpretation of the SUPER model arises from\nthe perspective of iterative reconstruction.\nMany recent\nstate-of-the-art MBIR schemes involve complex noncon-\nvex optimization and priors, wherein the initialization of\nthe algorithm is typically quite important, and better initial-\nizations can lead to better reconstructions. In the SUPER\nmodel, the iterative module is “initialized” with a different\nimage (i.e., output of the corresponding supervised module)\nin each super layer. If the output of the supervised module\nimproves in quality over layers, the iterative module will\nsee increasingly better initializations and could thus provide\nbetter quality outputs over layers. Moreover, the parameters\nof the iterative module could also be varied over layers to\nprovide optimal bias-noise trade-offs. Thus, the SUPER\nmodel could be viewed as minimizing nonconvex costs in\nsequence with better initializations and parameters.\nThe proposed SUPER model for LDCT reconstruction\ncan be generalized to incorporate a variety of iterative\nand MBIR techniques in the iterative modules.\nFor ex-\nample, conventional techniques such as PWLS-EP (edge-\npreserving hyperbola regularizer) [33] could be used in the\niterative module. PWLS-EP is a non-adaptive method that\npenalizes the differences between neighboring pixels in the\nreconstruction. We show later that combining PWLS-EP\nwith supervised learning in the SUPER model boosts the\nperformance of both methods. Note that we do not run the\nPWLS-EP modules to near convergence (as it involves a\nstrictly convex problem and a unique minimum) but only\nfor multiple iterations.\n2.3. Examples of SUPER Architectures\nWe now discuss some example SUPER models and their\nproperties.\nTo illustrate the proposed approach, in this\nwork, we focused on the recent FBPConvNet [19] for the\nsupervised module. For the iterative module, we chose the\nconventional PWLS-EP approach that uses a hand-crafted\nprior (edge-preserving hyperbola regularizer) as well as the\nlearning and clustering-based PWLS-ULTRA [14]. Our ex-\nperiments later show that combining such supervised and it-\nerative methods improves image quality over the constituent\nmethods. In the following, we further discuss the chosen ar-\nchitectures.\n2.3.1\nSupervised Module\nWe work with FBPConvNet, which is a CNN-based image-\ndomain denoising architecture, originally designed for\nsparse-view CT. The backbone of FBPConvNet is a U-net\n[34] like CNN that takes noisy images reconstructed by the\nFBP method (from low-dose scans) as input. The neural\nnetwork is trained so that its outputs closely match the ref-\nerence high-quality (true) images, e.g., in an ℓ2 norm or root\nmean squared error (RMSE) sense.\nThe traditional U-net uses a multilevel decomposition,\nand employs a dyadic scale decomposition based on max\npooling. Thus, the effective ﬁlter size in the middle layers\nis larger than that in the early and late layers. This scheme is\nimportant because the ﬁlters corresponding to the Hessian\nof the data ﬁdelity term in (1) may have noncompact sup-\nport. Similar to U-net, FBPConvNet employs multichannel\nﬁlters, which is the standard approach in CNNs, to increase\nthe capacity of the network. Compared with the traditional\nU-net, FBPConvNet adopts a residual learning strategy to\nlearn the difference between the input and output.\n2.3.2\nIterative Module\nThe iterative module optimizes an MBIR problem that es-\ntimates the linear attenuation coefﬁcients x ∈RNp from\nthe measurements y ∈RNd. The typical PWLS approach\ninvolves a cost function of the following form:\nx = arg min\nx≥0\n∥y −Ax∥2\nW + βR(x),\n(1)\nwhere A ∈RNd×Np is the CT system matrix, W is the\nweighting matrix related to the measurements (capturing\nmeasurement statistics), R(x) is the regularizer, and β is\na positive scalar controlling the balance between the data-\nﬁdelity term and the regularizer. In this paper, we used the\nunsupervised learning-based PWLS-ULTRA as well as the\nconventional PWLS-EP for (1).\nFor PWLS-EP, the regularizer R(x) can be written as\nR(x) = PNp\nj=1\nP\nk∈Nj κjκkϕ(xj −xk), where xj is the\njth pixel of x, Nj is the neighborhood of the jth pixel,\nand κj and κk are analytically determined weights that en-\ncourage resolution uniformity [33]. The potential function\nϕ(t) is often chosen as ϕ(t) = δ2(|t/δ| −log(1 + |t/δ|)),\nfor δ > 0. PWLS-EP enforces approximate sparsity of gra-\ndients of the image, a non-adaptive prior.\nPWLS-ULTRA pre-learns a union of sparsifying trans-\nforms from a dataset of image patches, and uses the learned\nmodel during reconstruction [14].\nThe formulation for\nlearning the union of sparsifying transforms is as follows:\nmin\n{Ωk,Zi,Ck}\nK\nX\nk=1\nX\ni∈Ck\n{∥ΩkXi −Zi∥2\n2 + η∥Zi∥0}\n+\nK\nX\nk=1\nλkQ(Ωk), s.t. Ck ∈G.\n(2)\nHere, Ωk, Ck, and Zi represent the learned transform for\nthe kth class, the set of indices of patches belonging to\nthe kth class, and the sparse coefﬁcient of the ith training\nsignal or patch Xi (N training signals assumed), respec-\ntively. Each signal is grouped with a corresponding best\nmatched sparsifying transform in (2). The set G is the set\nof all possible partitions of [1 : N] into K disjoint sub-\nsets. To avoid trivial solutions for Ωk, the penalty terms\nQ(Ωk) ≜∥Ωk∥2\nF −log | det Ωk| for 1 ≤k ≤K are\nused that also control the condition number of the trans-\nforms. Parameters η and λk = λ0\nP\ni∈Ck ∥Xi∥2\n2 are pos-\nitive weighting factors with λ0 > 0 [14]. Problem (2) is\nsolved efﬁciently using alternating optimization [14].\nWith the pre-learned transforms {Ωk}, the regularizer\nR(x) for image reconstruction is as follows:\nR(x) ≜\nmin\n{zj,Ck}\nK\nX\nk=1\nX\nj∈Ck\nτj\n\b\n∥ΩkPjx −zj∥2\n2 + γ2∥zj∥0\n\t\n,\nwhere {τj} are patch-based weights to encourage uniform\nspatial resolution or uniform noise (see [14]), Pj ∈Rl×Np\nis a patch extraction operator that extracts the jth patch from\nx. zj is the sparse coefﬁcient for the jth patch, and γ > 0\nis a parameter controlling sparsity.\nThe PWLS-ULTRA problem is efﬁciently solved by al-\nternating between updating x (image update step), and solv-\ning for {zj, Ck} (sparse coding and clustering step). In the\nimage update step, where {zj, Ck} are ﬁxed, the subprob-\nlem is quadratic with non-negativity constraints, and can be\nsolved using fast iterative algorithms such as the relaxed\nlinearized augmented Lagrangian method with ordered-\nsubsets (relaxed OS-LALM) [35, 36]. The sparse coding\nand clustering step with ﬁxed x is solved exactly [32], with\nthe optimal class assignment ˆkj for each patch given as\narg min\n1≤k≤K\n∥ΩkPjx −Hγ(ΩkPjx)∥2\n2 + γ2∥Hγ(ΩkPjx)∥0.\nThe corresponding optimal ˆzj = Hγ(ΩˆkjPjx),\nwhere\nHγ(·) is the hard-thresholding operator that sets elements\nsmaller than γ to zero.\nThe hard-thresholding can be\nviewed as the non-smooth nonlinearity in PWLS-ULTRA.\nThe clustering could vary from image to image and iteration\nto iteration in the PWLS-ULTRA algorithm.\n2.4. Training and Implementation\nWe propose to train the SUPER model layer-by-layer\n(sequentially) from a dataset of pairs of low-dose and\nregular-dose CT measurements.\nFor example, the FBP\nmethod can be used to obtain reconstructions from the mea-\nsurements. The initial low-dose reconstructed images are\nthen used as inputs to the ﬁrst supervised module, which\nis trained to minimize the reconstruction error (RMSE) at\nits output, with respect to the regular-dose reconstructions.\nThe initial images are then passed through the trained net-\nwork, following which the iterative algorithm in the ﬁrst it-\nerative module is run for each training image (in parallel) to\nproduce iterative reconstructions. The iterative reconstruc-\ntions serve as inputs to the subsequent supervised model,\nwhich is trained to minimize reconstruction error. The sub-\nsequent supervised modules are thus learned sequentially.\nOnce trained, the SUPER model is readily implemented\nfor test data by passing initial reconstructions sequentially\nthrough the supervised learned networks and iterative algo-\nrithms.\n3. Experiments\nHere, we ﬁrst describe our experimental setup, training\nprocedures, and evaluation metrics. Then we present re-\nsults for the learned SUPER-ULTRA model, and compare\nthese with those obtained by each individual module of SU-\nPER, i.e., FBPConvNet and PWLS-ULTRA. We also tested\nthe generalized SUPER model that replaces the unsuper-\nvised learning-based PWLS-ULTRA with the non-adaptive\nPWLS-EP. This scheme is dubbed FBPConvNet+EP.\n3.1. Experimental Setup\nWe used regular-dose CT images of two patients from\nthe Mayo Clinics dataset established for “the 2016 NIH-\nAAPM-Mayo Clinic Low Dose CT Grand Challenge” [37],\nto evaluate the performance of the proposed SUPER learn-\ning. The two patient datasets (L067 and L096) contain 224\nand 330 real in-vivo slice images, respectively. We sim-\nulated low-dose CT measurements yl from the provided\nregular-dose images with GE 2D LightSpeed fan-beam CT\ngeometry corresponding to a monoenergetic source.\nWe\nprojected the regular-dose images x∗to sinograms and\nadded Poisson and additive Gaussian noise to them as fol-\nlows:\nyli = Poisson{I0e−[Ax∗]i} + N{0, σ2}.\n(3)\nWe chose I0 = 1 × 105 photons per ray and σ = 5 in\nour experiments.\nWe approximated elements of the di-\nagonal weighting matrix W of the data-ﬁdelity in (1) by\ny2\nli\nyli+σ2 [38]. The images are of size 512 × 512 at a resolu-\ntion of 0.9766 mm×0.9766 mm, when reconstructed using\nthe FBP method. These reconstructed low-dose FBP im-\nages were paired with their corresponding regular-dose CT\nimages for training the SUPER model.\n3.2. Training Procedures\nIn our experiments, the total number of training image\npairs was 100, of which 50 were chosen from patient L067\nand 50 from patient L096.\nThe image set was used to\ntrain the networks of FBPConvNet, FBPConvNet+EP, and\nSUPER-ULTRA. Fig. 2 shows some example (regular-dose\nor reference) images from the training and testing datasets.\nDifferent body parts are included in the datasets. In our ex-\nperiments, we used the default network architecture in the\nFBPConvNet public implementation. For PWLS-ULTRA,\nwe pre-learned a union of ﬁve sparsifying transforms (cor-\nresponding to ﬁve classes) from twelve (regular-dose) slices\nthat include three slices each from four patients (L109,\nL143, L192, L506).\nFigure 2: Example CT images in the training (top row) and\ntesting (bottom row) datasets. The display window is [800\n1200] HU.\nWe used a GTX Titan GPU graphic processor for train-\ning and testing. The union of transforms was learned ef-\nﬁciently using similar parameters as in [14]. For SUPER-\nULTRA and FBPConvNet+EP, the training time was about\n46 hours and 13 hours, respectively, for 15 super layers.\nSince the iterative reconstruction modules are relatively\ncomputationally expensive, we chose 15 super layers to bal-\nance reconstruction quality and computational time. The\ntraining hyper-parameters for the CNN part of FBPConvNet\nwere set as follows for the various models: the learning rate\ndecreased logarithmically from 0.001 to 0.0001; the batch-\nsize was 1; and the momentum was 0.99. We initialized the\nﬁlters in the various networks during training with i.i.d. ran-\ndom Gaussian entries with zero mean and variance 0.005,\nand initialized the bias with zero vectors.\nFor training FBPConvNet+EP, we ran 10 epochs of FBP-\nConvNet training (using ADAM) in each super layer to en-\nsure that the learned networks capture layer-wise features.\nFor the constituent PWLS-EP modules, we ran 4 iterations\nof the relaxed OS-LALM algorithm with 4 subsets, and set\nδ = 20 HU and the regularization parameter β = 215. For\ntraining SUPER-ULTRA, we ran 10 epochs of FBPCon-\nvNet training in each super layer along with 4 outer iter-\nations of PWLS-ULTRA with 5 inner iterations of the im-\nage update step that used the relaxed OS-LALM algorithm\nwith 4 subsets. We set β = 5 × 103 and γ = 20 for the\nPWLS-ULTRA module.\nWe compared our learned models with the iterative\nschemes PWLS-EP and PWLS-ULTRA. Since FBPCon-\nvNet+EP and SUPER-ULTRA already include the learned\nFBPConvNet modules, we used smaller regularization pa-\nrameters for them compared to the usual or stand-alone\nPWLS-EP and PWLS-ULTRA, which worked well in our\nexperiments. For the stand-alone PWLS-EP iterative recon-\nstruction algorithm that solves a convex problem, β and δ\nwere set as 216 and 20, respectively, and we ran 100 iter-\nations of the OS-LALM algorithm to ensure convergence.\nFor the stand-alone PWLS-ULTRA, wherein the optimiza-\ntion problem is nonconvex, we set β and γ as 104 and\n25, respectively, and ran 1000 iterations of the alternat-\ning algorithm to ensure convergence. The above param-\neters provided optimal image quality in our experiments.\nBoth PWLS-EP and PWLS-ULTRA were initialized with\nthe simple FBP reconstructions.\n3.3. Evaluation Metrics\nTo quantitatively evaluate the performances of the\nvarious reconstruction models,\nwe chose three clas-\nsic metrics, namely RMSE, peak signal to noise ra-\ntio (PSNR), and structural similarity index measure\n(SSIM) [39]. RMSE in Hounsﬁeld units (HU) is deﬁned as\nRMSE=\nqPNp\ni=1 (ˆxi −x∗\ni )2 /Np, where x∗is the reference\nregular-dose CT image, ˆx is the reconstruction, and Np is\nthe number of pixels. We computed PSNR in decibels (dB).\n3.4. Results and Comparisons\n3.4.1\nVisual Results\nWe applied the various methods to six testing slices (three\nslices from L067 and three slices from L096) indexed as\nTest #1 to Test #6. Figs. 3 and 4 show the reconstructions\nof Test #4 (from patient L096) and Test #2 (from patient\nL067), respectively, using different methods. Clearly, the\ntraditional FBP reconstruction shows severe artifacts, while\nthe results obtained by the other methods have much better\nimage quality. The stand-alone optimal PWLS-EP recon-\nstruction still contains noise such as in the central soft-tissue\narea (Fig. 3). The (unsupervised) learning-based PWLS-\nULTRA removes noise, but the edges of soft-tissues are\nmore blurry in the results. The supervised learning-based\nFBPConvNet achieves better trade-off between resolution\nand noise compared to PWLS-EP and PWLS-ULTRA, but\nmany small structures were missed or distorted. With the\nsame training data as FBPConvNet, the learned FBPCon-\nvNet+EP and SUPER-ULTRA models provide much better\nreconstructions. However, FBPConvNet+EP suffers from\nsome streak artifacts in the central area (Fig. 3) as well as\nnoise generally, and SUPER-ULTRA mitigates these arti-\nfacts. In both Figs. 3 and 4, SUPER-ULTRA that com-\nbines supervised learned networks and transform learning-\nbased iterative reconstructions achieved the best overall vi-\nsual quality. 1\n3.4.2\nQuantitative Results\nFig. 5 shows the RMSE and SSIM evolution for the stand-\nalone PWLS-EP and PWLS-ULTRA along with those for\nFBPConvNet+EP and SUPER-ULTRA. The latter two in-\nvolve 15 super layers with 4 (outer) iterations in the iterative\nmodule per layer, and thus, the RMSE and SSIM evolution\nis plotted over these individual iterations. For PWLS-EP\nand PWLS-ULTRA, we show the evolution of the metrics\nover 60 iterations. FBPConvNet+EP and SUPER-ULTRA\nclearly achieve much lower RMSE values and higher SSIM\nvalues over layers than PWLS-EP and PWLS-ULTRA. The\nplots also show faster convergence for the SUPER models.\nTable. 1 shows the RMSE, PSNR, and SSIM values\nwith various methods for the testing slices. The proposed\nSUPER-ULTRA typically achieves signiﬁcant improve-\nments in RMSE, SSIM, and PSNR over the other meth-\nods for all testing slices.\nImportantly, FBPConvNet+EP\nand SUPER-ULTRA perform much better than PWLS-EP\nand PWLS-ULTRA, respectively, and also provide more\npromising results than FBPConvNet, demonstrating that the\ncombination of the supervised module and iterative mod-\nules in the SUPER model works well and outperforms the\nindividual modules.\n3.4.3\nVisual Quality over SUPER layers\nFig. 6 shows the output of SUPER-ULTRA after different\nnumbers of SUPER layers. The initial FBP image and the\nﬁnal output (after 15 layers) were shown in Fig. 4. The\nﬁrst several layers of SUPER-ULTRA mainly remove se-\nvere noise and artifacts, while the later layers recover some\nstructural details.\n1Additional comparisons between reconstructions for the other testing\nslices are included in the supplement.\nPSNR:13.3\nPSNR:20.3\nPSNR:23.4\nPSNR:27.1\nPSNR: 29.0\nPSNR: 31.7\nFigure 3: Reconstructed testing image (Test #4, from patient L096) obtained by FBP, FBPConvNet, PWLS-EP, PWLS-\nULTRA, FBPConvNet + EP, SUPER-ULTRA, and regular-dose FBP. The display window is [800 1200] HU.\nTable 1: PSNR, RMSE, and SSIM of reconstructed test images for different methods.\nFBP\nFBPConvNet\nPWLS-\nEP\nPWLS-\nULTRA\nFBPConvNet+EP\nSUPER-ULTRA\nL067\nTest #1\nPSNR\n11.0\n29.8\n23.5\n29.3\n30.5\n30.9\nRMSE\n245.5\n26.2\n54.1\n28.3\n24.5\n23.3\nSSIM\n0.29\n0.74\n0.73\n0.74\n0.77\n0.77\nTest #2\nPSNR\n13.6\n30.3\n23.0\n29.6\n31.6\n31.6\nRMSE\n170.3\n22.4\n52.2\n24.5\n19.5\n19.3\nSSIM\n0.40\n0.79\n0.78\n0.79\n0.81\n0.81\nTest #3\nPSNR\n9.5\n19.9\n24.4\n28.6\n32.2\n32.1\nRMSE\n299.5\n81.7\n47.9\n29.6\n19.6\n19.8\nSSIM\n0.24\n0.56\n0.69\n0.69\n0.72\n0.72\nL096\nTest #4\nPSNR\n13.3\n20.3\n23.4\n27.1\n29.0\n31.7\nRMSE\n172.6\n70.7\n48.7\n31.9\n25.5\n18.9\nSSIM\n0.37\n0.67\n0.77\n0.79\n0.80\n0.81\nTest #5\nPSNR\n9.3\n29.7\n23.3\n25.7\n30.6\n30.8\nRMSE\n304.4\n25.8\n53.8\n40.6\n23.3\n22.7\nSSIM\n0.20\n0.71\n0.70\n0.70\n0.74\n0.75\nTest #6\nPSNR\n9.7\n27.6\n23.6\n28.1\n30.7\n32.3\nRMSE\n274.2\n32.3\n48.6\n29.0\n21.5\n17.8\nSSIM\n0.23\n0.66\n0.73\n0.74\n0.75\n0.76\n3.4.4\nBehavior of the ULTRA model in the SUPER ar-\nchitecture\nNext, to better illustrate the image-adaptive learned cluster-\ning in the SUPER-ULTRA model, Fig. 13 shows pixel-level\nclustering results from the last super layer for test slice #1.\nSince the ULTRA modules cluster image patches into spe-\nciﬁc classes, we cluster each pixel here using a majority\nvote among the patches overlapping the pixel. Class 1 con-\ntains most of the more uniform soft-tissues; Classes 2, 3,\nand 5 contain many oriented edges (e.g., at 45-degree and\n135-degree orientation); and class 4 contains most of the\nvertical edges and some horizontal edges as well as most of\nthe bones. The latter classes help provide sharper SUPER-\nPSNR:13.6\nPSNR: 30.3\nPSNR: 23.0\nPSNR: 31.6\nPSNR: 31.6\nPSNR: 29.6\nFigure 4:\nComparison of Test #2 for FBP, FBPCon-\nvNet, PWLS-EP, PWLS-ULTRA, FBPConvNet + EP, and\nSUPER-ULTRA (clockwise top left). The display window\nis [800 1200] HU.\n0\n10\n20\n30\n40\n50\n60\nNumber Iteration\n20\n40\n60\n80\n100\n120\n140\n160\n180\nRMSE (HU)\nPWLS-EP\nFBPConvNet+EP\n0\n10\n20\n30\n40\n50\n60\nNumber Iteration\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\nRMSE (HU)\nPWLS-ULTRA\nSUPER-ULTRA\n0\n10\n20\n30\n40\n50\n60\nNumber Iteration\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nSSIM\nPWLS-EP\nFBPConvNet+EP\n0\n10\n20\n30\n40\n50\n60\nNumber Iteration\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nSSIM\nPWLS-ULTRA\nSUPER-ULTRA\nFigure 5: RMSE (ﬁrst row) and SSIM (second row) val-\nues for Test #4 over the iterations of PWLS-EP, FBPCon-\nvNet+EP, PWLS-ULTRA, and SUPER-ULTRA.\nULTRA reconstructions. The pre-learned transforms corre-\nsponding to each class are also shown in Fig. 13, and con-\ntain various directional and edge-like features.\n4. Conclusions\nThis paper presented a new framework that combined su-\npervised learned networks and unsupervised iterative algo-\nrithms for low-dose CT reconstruction. The proposed SU-\nPER framework effectively combines various kinds of pri-\nors and learning methods. In particular, we studied SUPER-\nULTRA that combines (supervised) deep learning (FBP-\nConvNet) and the recent iterative (unsupervised) PWLS-\nULTRA, as well as FBPConvNet+EP (or SUPER-EP). Both\nmethods showed better performance and faster convergence\ncompared to their individual modules. FBPConvNet+EP\nsubstantially improved the performance of PWLS-EP, while\nSuper Layer 1\nSuper Layer 3\nSuper Layer 7\nSuper Layer 13\nFigure 6: Outputs of 1st, 3rd, 7th, and 13th SUPER-ULTRA\nlayers with display window [800 1200] HU. The reconstruc-\ntion is visually converging.\nSUPER-ULTRA typically performed the best by effectively\nleveraging deep learning and transform learning. While SU-\nPER model learning can exploit a variety of architectures\nand algorithms for the supervised and iterative modules, a\nmore detailed study of various such architectures is left for\nfuture work. We also plan to explore layer-dependent pa-\nrameter selection for the iterative modules to further im-\nprove performance in future work.\nReferences\n[1] L. C. Feldkamp, L. A .and Davis and J. W. Kress. Practical\ncone-beam algorithm. J. Opt. Soc. Amer. A, Opt. Image Sci.,\n1(6):612–619, 1984.\n[2] K. Imai, M. Ikeda, Y. Enchi, and T. Niimi.\nStatistical\ncharacteristics of streak artifacts on CT images: Relation-\nship between streak artifacts and ma s values. Med. Phys.,\n36(2):492–499, 2009.\n[3] H. Zhang, J. Ma, J. Wang, Y. Liu, H. Lu, and Z. Liang. Sta-\ntistical image reconstruction for low-dose CT using nonlocal\nmeans-based regularization. Computerized Medical Imaging\nand Graphics, 38(6):423–435, 2014.\n[4] J. Nuyts, B. De Man, J. A. Fessler, W. Zbijewski, and F. J.\nBeekman. Modelling the physics in the iterative reconstruc-\ntion for transmission computed tomography.\nPhys. Med.\nBiol., 58(12):R63, 2013.\n[5] Jeffrey A Fessler.\nPenalized weighted least-squares im-\nage reconstruction for positron emission tomography. IEEE\ntransactions on medical imaging, 13(2):290–300, 1994.\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\nFigure 7: Pixel-level clustering results in Test #2. The top row shows the transforms, with the transform rows shown as 8 × 8\npatches. The bottom row shows the clustering results of SUPER-ULTRA with display window [800 1200] HU.\n[6] J-B. Thibault, K. Sauer, C. Bouman, and J. Hsieh. A three-\ndimensional statistical approach to improved image quality\nfor multi-slice helical CT.\nMed. Phys., 34(11):4526–44,\nNovember 2007.\n[7] M. Beister, D. Kolditz, and W. A Kalender. Iterative recon-\nstruction methods in X-ray CT. Physica Medica: European\nJournal of Medical Physics, 28(2):94–108, 2012.\n[8] Q. Xu, H. Yu, X. Mou, L. Zhang, J. Hsieh, and G. Wang.\nLow-dose X-ray CT reconstruction via dictionary learning.\nIEEE Trans. Med. Imag., 31(9):1682–97, September 2012.\n[9] S. Ravishankar and Y. Bresler.\nMR image reconstruction\nfrom highly undersampled k-space data by dictionary learn-\ning. IEEE Trans. Med. Imag., 30(5):1028–1041, 2010.\n[10] J. Mairal, M. Elad, and G. Sapiro. Sparse representation for\ncolor image restoration. IEEE Trans. Im. Proc., 17(1):53–69,\n2007.\n[11] M. Elad and M. Aharon. Image denoising via sparse and\nredundant representations over learned dictionaries. IEEE\nTrans. Im. Proc., 15(12):3736–3745, 2006.\n[12] S. Ravishankar and Y. Bresler. Learning sparsifying trans-\nforms. IEEE Trans. Signal Process., 61(5):1072–1086, 2013.\n[13] S. Ravishankar and Y. Bresler. Learning doubly sparse trans-\nforms for images. IEEE Trans. Im. Proc., 22(12):4598–4612,\n2013.\n[14] X. Zheng, S. Ravishankar, Y. Long, and J. A. Fessler. PWLS-\nULTRA: An efﬁcient clustering and learning-based approach\nfor low-dose 3D CT image reconstruction. IEEE Trans. Med.\nImag., 37(6):1498–1510, 2018.\n[15] S. Ye, Y. Ravishankar, S.and Long, and J. A. Fessler. SPUL-\nTRA: Low-dose CT image reconstruction with joint statis-\ntical and learned image models. IEEE Trans. Med. Imag.,\n2019. DOI: 10.1109/TMI.2019.2934933.\n[16] G. Yang, S. Yu, H. Dong, G. Slabaugh, P. L. Dragotti, X. Ye,\nF. Liu, S. Arridge, J. Keegan, Y. Guo, and D. Firmin. DA-\nGAN: deep de-aliasing generative adversarial networks for\nfast compressed sensing MRI reconstruction. IEEE Trans.\nMed. Imag., 37(6):1310–1321, 2017.\n[17] S. Yu, H. Dong, G. Yang, G. Slabaugh, P. L. Dragotti, X. Ye,\nF. Liu, S. Arridge, J. Keegan, D. Firmin, and Y. Guo. Deep\nde-aliasing for fast compressive sensing MRI. arXiv preprint\narXiv:1705.07137, 2017.\n[18] J. Schlemper, G. Yang, P. Ferreira, A. Scott, L. McGill,\nZ. Khalique, M. Gorodezky, M. Roehl, J. Keegan, D. Pen-\nnell, D. Firmin, and D. Rueckert. Stochastic deep compres-\nsive sensing for the reconstruction of diffusion tensor cardiac\nMRI. In Med. Image Comput. Comput.-Assist. Interv., pages\n295–303. Springer, 2018.\n[19] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser. Deep\nconvolutional neural network for inverse problems in imag-\ning. IEEE Trans. Im. Proc., 26(9):4509–22, 2017.\n[20] E. Kang, J. Min, and J. C. Ye. A deep convolutional neural\nnetwork using directional wavelets for low-dose X-ray CT\nreconstruction. Med. Phys., 44(10):e360–e375, 2017.\n[21] E. Kang, W. Chang, J. Yoo, and J. C. Ye. Deep convolutional\nframelet denoising for low-dose CT via wavelet residual net-\nwork. IEEE Trans. Med. Imag., 37(6):1358–1369, 2018.\n[22] T. W¨urﬂ, F. C. Ghesu, V. Christlein, and A. Maier. Deep\nlearning computed tomography.\nIn Med. Image Comput.\nComput. Assist. Interv., pages 432–440. Springer, 2016.\n[23] B. Zhu, J. Z. Liu, S. F. Cauley, B. R. Rosen, and M. S. Rosen.\nImage reconstruction by domain-transform manifold learn-\ning. Nature, 555(7697):487, 2018.\n[24] K. Gregor and Y. LeCun. Learning fast approximations of\nsparse coding. In Proceedings of the 27th International Con-\nference on International Conference on Machine Learning,\npages 399–406. Omnipress, 2010.\n[25] A. Beck and M. Teboulle.\nA fast iterative shrinkage-\nthresholding algorithm for linear inverse problems. SIAM\nJ. Imaging Sci., 2(1):183–202, 2009.\n[26] Y. Yang, J. Sun, H. Li, and Z. Xu. Deep ADMM-Net for\ncompressive sensing MRI. In Proceedings of the 30th Inter-\nnational Conference on Neural Information Processing Sys-\ntems, pages 10–18. Curran Associates Inc., 2016.\n[27] J. Adler and O. ¨Oktem. Learned primal-dual reconstruction.\nIEEE Trans. Med. Imag., 37(6):1322–1332, 2018.\n[28] S. Ravishankar, I. Y. Chun, and J. A. Fessler. Physics-driven\ndeep training of dictionary-based algorithms for MR image\nreconstruction. In 2017 51st Asilomar Conference on Sig-\nnals, Systems, and Computers, pages 1859–1863, 2017.\n[29] S. Ravishankar, A. Lahiri, C. Blocker, and J. A. Fessler.\nDeep dictionary-transform learning for image reconstruc-\ntion.\nIn 2018 IEEE 15th International Symposium on\nBiomedical Imaging (ISBI 2018), pages 1208–1212, 2018.\n[30] Y. Chun and J. A. Fessler.\nDeep BCD-Net using identi-\ncal encoding-decoding CNN structures for iterative image\nrecovery.\nIn 2018 IEEE 13th Image, Video, and Multidi-\nmensional Signal Processing Workshop (IVMSP), pages 1–5,\n2018.\n[31] J. He, Y. Yang, Y. Wang, D. Zeng, Z. Bian, H. Zhang, J. Sun,\nZ. Xu, and J. Ma. Optimizing a parameterized plug-and-play\nadmm for iterative low-dose CT reconstruction. IEEE Trans.\nMed. Imag., 38(2):371–382, 2018.\n[32] S. Ravishankar and Y. Bresler.\nData-driven learning of a\nunion of sparsifying transforms model for blind compressed\nsensing. IEEE Trans. Comput. Imag., 2(3):294–309, 2016.\n[33] J. H. Cho and J. A. Fessler. Regularization designs for uni-\nform spatial resolution and noise properties in statistical im-\nage reconstruction for 3D X-ray CT.\nIEEE Trans. Med.\nImag., 34(2):678–89, February 2015.\n[34] O. Ronneberger, P. Fischer, and T. Brox. U-net: convolu-\ntional networks for biomedical image segmentation. In Med-\nical Image Computing and Computer-Assisted Intervention,\npages 234–41, 2015.\n[35] H. Nien and J. A. Fessler. Relaxed linearized algorithms for\nfaster X-ray CT image reconstruction.\nIEEE Trans. Med.\nImag., 35(4):1090–8, April 2016.\n[36] D. Kim and J. A. Fessler. Generalizing the optimized gradi-\nent method for smooth convex minimization. SIAM J. Op-\ntim., 28(2):1920–1950, 2018.\n[37] C. McCollough. TU-FG-207A-04: Overview of the low dose\nCT grand challenge. Med. Phys., 43(2):3759–60, 2016.\n[38] L. Fu, T. C. Lee, S. M. Kim, A. M. Alessio, P. E. Kinahan,\nZ. Q. Chang, K. Sauer, M. K. Kalra, and B. De Man. Com-\nparison between pre-log and post-log statistical models in\nultra-low-dose CT reconstruction. IEEE Trans. Med. Imag.,\n36(3):707–720, 2017.\n[39] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.\nImage quality assessment: from error visibility to structural\nsimilarity. IEEE Trans. Im. Proc., 13(4):600–12, April 2004.\nSUPER Learning: A Supervised-Unsupervised\nFramework for Low-Dose CT Image\nReconstruction – Supplementary Material\n5. Additional Experimental Results\nHere, we show the regular-dose FBP images for the other\nfour testing slices (Test #1, #3, #5, and #6) in Fig. 8, and\ntheir reconstructions in Figs. 9, 10, 11, and 12.\nFigure 8: Regular-dose FBP images of four testing slices:\nTest #1 (top left), #3 (top right), #5 (bottom left), and #6\n(bottom right). The display window is [800 1200] HU.\nSUPER-ULTRA and FBPConvNet+EP obviously re-\nduce artifacts and noise compared to FBP, PWLS-EP, and\nFBPConvNet. Furthermore, SUPER-ULTRA also signiﬁ-\ncantly improves the sharpness in the soft-tissue compared\nto PWLS-ULTRA. Lastly, compared to FBPConvNet+EP,\nSUPER-ULTRA reduces noise while producing sharp re-\nconstructions.\nNext, to better illustrate the learned clustering in the\nSUPER-ULTRA model, Fig. 13 shows examples of pixel-\nlevel clustering results from the last super layer for test\nslices #1 and #3.\nPSNR: 11.0\nPSNR: 29.8\nPSNR: 23.5\nPSNR: 29.3\nPSNR: 30.5\nPSNR: 30.9\nFigure 9:\nReconstructed testing image (Test #1, from\npatient L067) obtained by FBP (top left), FBPConvNet\n(top right), PWLS-EP (middle left), PWLS-ULTRA (mid-\ndle right), FBPConvNet + EP (bottom left), and SUPER-\nULTRA (bottom right).\nThe display window is [800\n1200] HU.\nPSNR: 9.5\nPSNR: 19.9\nPSNR: 24.4\nPSNR: 28.6\nPSNR: 32.2\nPSNR: 32.1\nFigure 10: Reconstructed testing image (Test #3, from\npatient L067) obtained by FBP (top left), FBPConvNet\n(top right), PWLS-EP (middle left), PWLS-ULTRA (mid-\ndle right), FBPConvNet + EP (bottom left), and SUPER-\nULTRA (bottom right).\nThe display window is [800\n1200] HU.\nPSNR: 9.3\nPSNR: 29.7\nPSNR: 23.3\nPSNR: 25.7\nPSNR: 30.6\nPSNR: 30.8\nFigure 11: Reconstructed testing image (Test #5, from\npatient L096) obtained by FBP (top left), FBPConvNet\n(top right), PWLS-EP (middle left), PWLS-ULTRA (mid-\ndle right), FBPConvNet + EP (bottom left), and SUPER-\nULTRA (bottom right).\nThe display window is [800\n1200] HU.\nPSNR: 9.7\nPSNR: 27.6\nPSNR: 23.6\nPSNR: 28.1\nPSNR: 30.7\nPSNR: 32.3\nFigure 12: Reconstructed testing image (Test #6, from patient L096) obtained by FBP (top left), FBPConvNet (top middle),\nPWLS-EP (top right), PWLS-ULTRA (bottom left), FBPConvNet + EP (bottom middle), and SUPER-ULTRA (bottom\nright). The display window is [800 1200] HU.\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\nFigure 13: Top and bottom rows show the Pixel-level clustering results of SUPER-ULTRA for Test #1 and Test #3, respec-\ntively. The display window is [800 1200] HU.\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "eess.IV",
    "eess.SP",
    "stat.ML"
  ],
  "published": "2019-10-26",
  "updated": "2019-10-26"
}