{
  "id": "http://arxiv.org/abs/1910.12156v1",
  "title": "Convergent Policy Optimization for Safe Reinforcement Learning",
  "authors": [
    "Ming Yu",
    "Zhuoran Yang",
    "Mladen Kolar",
    "Zhaoran Wang"
  ],
  "abstract": "We study the safe reinforcement learning problem with nonlinear function\napproximation, where policy optimization is formulated as a constrained\noptimization problem with both the objective and the constraint being nonconvex\nfunctions. For such a problem, we construct a sequence of surrogate convex\nconstrained optimization problems by replacing the nonconvex functions locally\nwith convex quadratic functions obtained from policy gradient estimators. We\nprove that the solutions to these surrogate problems converge to a stationary\npoint of the original nonconvex problem. Furthermore, to extend our theoretical\nresults, we apply our algorithm to examples of optimal control and multi-agent\nreinforcement learning with safety constraints.",
  "text": "Convergent Policy Optimization for Safe\nReinforcement Learning\nMing Yu ∗\nZhuoran Yang †\nMladen Kolar ‡\nZhaoran Wang §\nAbstract\nWe study the safe reinforcement learning problem with nonlinear function approx-\nimation, where policy optimization is formulated as a constrained optimization\nproblem with both the objective and the constraint being nonconvex functions.\nFor such a problem, we construct a sequence of surrogate convex constrained\noptimization problems by replacing the nonconvex functions locally with convex\nquadratic functions obtained from policy gradient estimators. We prove that the\nsolutions to these surrogate problems converge to a stationary point of the original\nnonconvex problem. Furthermore, to extend our theoretical results, we apply our\nalgorithm to examples of optimal control and multi-agent reinforcement learning\nwith safety constraints.\n1\nIntroduction\nReinforcement learning [58] has achieved tremendous success in video games [41, 44, 56, 36, 66]\nand board games, such as chess and Go [53, 55, 54], in part due to powerful simulators [8, 62]. In\ncontrast, due to physical limitations, real-world applications of reinforcement learning methods often\nneed to take into consideration the safety of the agent [5, 26]. For instance, in expensive robotic\nand autonomous driving platforms, it is pivotal to avoid damages and collisions [25, 9]. In medical\napplications, we need to consider the switching cost [7].\nA popular model of safe reinforcement learning is the constrained Markov decision process (CMDP),\nwhich generalizes the Markov decision process by allowing for inclusion of constraints that model\nthe concept of safety [3]. In a CMDP, the cost is associated with each state and action experienced\nby the agent, and safety is ensured only if the expected cumulative cost is below a certain threshold.\nIntuitively, if the agent takes an unsafe action at some state, it will receive a huge cost that punishes\nrisky attempts. Moreover, by considering the cumulative cost, the notion of safety is deﬁned for the\nwhole trajectory enabling us to examine the long-term safety of the agent, instead of focusing on\nindividual state-action pairs. For a CMDP, the goal is to take sequential decisions to achieve the\nexpected cumulative reward under the safety constraint.\nSolving a CMDP can be written as a linear program [3], with the number of variables being the\nsame as the size of the state and action spaces. Therefore, such an approach is only feasible for the\ntabular setting, where we can enumerate all the state-action pairs. For large-scale reinforcement\nlearning problems, where function approximation is applied, both the objective and constraint of the\nCMDP are nonconvex functions of the policy parameter. One common method for solving CMDP\nis to formulate an unconstrained saddle-point optimization problem via Lagrangian multipliers and\nsolve it using policy optimization algorithms [18, 60]. Such an approach suffers the following two\ndrawbacks:\n∗The University of Chicago Booth School of Business, Chicago, IL. Email: ming93@uchicago.edu.\n†Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ.\n‡The University of Chicago Booth School of Business, Chicago, IL.\n§Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:1910.12156v1  [cs.LG]  26 Oct 2019\nFirst, for each ﬁxed Lagrangian multiplier, the inner minimization problem itself can be viewed\nas solving a new reinforcement learning problem. From the computational point of view, solving\nthe saddle-point optimization problem requires solving a sequence of MDPs with different reward\nfunctions. For a large scale problem, even solving a single MDP requires huge computational\nresources, making such an approach computationally infeasible.\nSecond, from a theoretical perspective, the performance of the saddle-point approach hinges on\nsolving the inner problem optimally. Existing theory only provides convergence to a stationary point\nwhere the gradient with respect to the policy parameter is zero [28, 37]. Moreover, the objective, as a\nbivariate function of the Lagrangian multiplier and the policy parameter, is not convex-concave and,\ntherefore, ﬁrst-order iterative algorithms can be unstable [27].\nIn contrast, we tackle the nonconvex constrained optimization problem of the CMDP directly. We\npropose a novel policy optimization algorithm, inspired by [38]. Speciﬁcally, in each iteration, we\nreplace both the objective and constraint by quadratic surrogate functions and update the policy\nparameter by solving the new constrained optimization problem. The two surrogate functions can\nbe viewed as ﬁrst-order Taylor-expansions of the expected reward and cost functions where the\ngradients are estimated using policy gradient methods [59]. Additionally, they can be viewed as\nconvex relaxations of the original nonconvex reward and cost functions. In §4 we show that, as the\nalgorithm proceeds, we obtain a sequence of convex relaxations that gradually converge to a smooth\nfunction. More importantly, the sequence of policy parameters converges almost surely to a stationary\npoint of the nonconvex constrained optimization problem.\nRelated work.\nOur work is pertinent to the line of research on CMDP [3]. For CMDPs with\nlarge state and action spaces, [19] proposed an iterative algorithm based on a novel construction of\nLyapunov functions. However, their theory only holds for the tabular setting. Using Lagrangian\nmultipliers, [46, 18, 1, 60] proposed policy gradient [59], actor-critic [33], or trust region policy\noptimization [51] methods for CMDP or constrained risk-sensitive reinforcement learning [26]. These\nalgorithms either do not have convergence guarantees or are shown to converge to saddle-points of the\nLagrangian using two-time-scale stochastic approximations [10]. However, due to the projection on\nthe Lagrangian multiplier, the saddle-point achieved by these approaches might not be the stationary\npoint of the original CMDP problem. In addition, [65] proposed a cross-entropy-based stochastic\noptimization algorithm, and proved the asymptotic behavior using ordinary differential equations.\nIn contrast, our algorithm and the theoretical analysis focus on the discrete time CMDP. Outside\nof the CMDP setting, [31, 35] studied safe reinforcement learning with demonstration data, [61]\nstudied the safe exploration problem with different safety constraints, and [4] studied multi-task safe\nreinforcement learning.\nOur contribution.\nOur contribution is three-fold. First, for the CMDP policy optimization problem\nwhere both the objective and constraint function are nonconvex, we propose to optimize a sequence\nof convex relaxation problems using convex quadratic functions. Solving these surrogate problems\nyields a sequence of policy parameters that converge almost surely to a stationary point of the original\npolicy optimization problem. Second, to reduce the variance in the gradient estimator that is used to\nconstruct the surrogate functions, we propose an online actor-critic algorithm. Finally, as concrete\napplications, our algorithms are also applied to optimal control (in §5) and parallel and multi-agent\nreinforcement learning problems with safety constraints (in supplementary material).\n2\nBackground\nA Markov decision process is denoted by (S, A, P, γ, r, µ), where S is the state space, A is the action\nspace, P is the transition probability distribution, γ ∈(0, 1) is the discount factor, r: S × A →R is\nthe reward function, and µ ∈P(S) is the distribution of the initial state s0 ∈S, where we denote\nP(X) as the set of probability distributions over X for any X. A policy is a mapping π : S →P(A)\nthat speciﬁes the action that an agent will take when it is at state s.\nPolicy gradient method.\nLet {πθ : S →P(A)} be a parametrized policy class, where θ ∈Θ\nis the parameter deﬁned on a compact set Θ. This parameterization transfers the original inﬁnite\ndimensional policy class to a ﬁnite dimensional vector space and enables gradient based methods\nto be used to maximize (1). For example, the most popular Gaussian policy can be written as\nπ(·|s, θ) = N\n\u0000µ(s, θ), σ(s, θ)\n\u0001\n, where the state dependent mean µ(s, θ) and standard deviation\n2\nσ(s, θ) can be further parameterized as µ(s, θ) = θ⊤\nµ · x(s) and σ(s, θ) = exp\n\u0000θ⊤\nσ · x(s)\n\u0001\nwith x(s)\nbeing a state feature vector. The goal of an agent is to maximize the expected cumulative reward\nR(θ) = Eπ\n\u0014X\nt≥0\nγt · r(st, at)\n\u0015\n,\n(1)\nwhere s0 ∼µ, and for all t ≥0, we have st+1 ∼P(· | st, at) and at ∼π(· | st). Given a policy π(θ),\nwe deﬁne the state- and action-value functions of πθ, respectively, as\nV θ(s) = Eπθ\n\u0014X\nt≥0\nγtr(st, at)\n\f\f\f\f s0 = s\n\u0015\n, Qθ(s, a) = Eπθ\n\u0014X\nt≥0\nγtr(st, at)\n\f\f\f\f s0 = s, a0 = a\n\u0015\n. (2)\nThe policy gradient method updates the parameter θ through gradient ascent\nθk+1 = θk + η · b∇θR(θk),\nwhere b∇θR(θk) is a stochastic estimate of the gradient ∇θR(θk) at k-th iteration. Policy gradient\nmethod, as well as its variants (e.g. policy gradient with baseline [58], neural policy gradient\n[64, 39, 16]) is widely used in reinforcement learning. The gradient ∇θR(θ) can be estimated\naccording to the policy gradient theorem [59],\n∇θR(θ) = E\nh\n∇θ log πθ(s, a) · Qθ(s, a)\ni\n.\n(3)\nActor-critic method.\nTo further reduce the variance of the policy gradient method, we could\nestimate both the policy parameter and value function simultaneously. This kind of method is called\nactor-critic algorithm [33], which is widely used in reinforcement learning. Speciﬁcally, in the value\nfunction evaluation (critic) step we estimate the action-value function Qθ(s, a) using, for example, the\ntemporal difference method TD(0) [20]. The policy parameter update (actor) step is implemented as\nbefore by the Monte-Carlo method according to the policy gradient theorem (3) with the action-value\nQθ(s, a) replaced by the estimated value in the policy evaluation step.\nConstrained MDP.\nIn this work, we consider an MDP problem with an additional constraint\non the model parameter θ. Speciﬁcally, when taking action at some state we incur some cost\nvalue. The constraint is such that the expected cumulative cost cannot exceed some pre-deﬁned\nconstant. A constrained Markov decision process (CMDP) is denoted by (S, A, P, γ, r, d, µ), where\nd: S × A →R is the cost function and the other parameters are as before. The goal of an the agent\nin CMDP is to solve the following constrained problem\nminimize\nθ∈Θ\nJ(θ) = Eπθ\n\u0014\n−\nX\nt≥0\nγt · r(st, at)\n\u0015\n,\nsubject to D(θ) = Eπθ\n\u0014X\nt≥0\nγt · d(st, at)\n\u0015\n≤D0,\n(4)\nwhere D0 is a ﬁxed constant. We consider only one constraint D(θ) ≤D0, noting that it is\nstraightforward to generalize to multiple constraints. Throughout this paper, we assume that both the\nreward and cost value functions are bounded:\n\f\fr(st, at)\n\f\f ≤rmax and\n\f\fd(st, at)\n\f\f ≤dmax. Also, the\nparameter space Θ is assumed to be compact.\n3\nAlgorithm\nIn this section, we develop an algorithm to solve the optimization problem (4). Note that both the\nobjective function and the constraint in (4) are nonconvex and involve expectation without closed-\nform expression. As a constrained problem, a straightforward approach to solve (4) is to deﬁne the\nfollowing Lagrangian function\nL(θ, λ) = J(θ) + λ ·\n\u0002\nD(θ) −D0\n\u0003\n,\nand solve the dual problem\ninf\nλ≥0 sup\nθ\nL(θ, λ).\n3\nHowever, this problem is a nonconvex minimax problem and, therefore, is hard to solve and establish\ntheoretical guarantees for solutions [2]. Another approach to solve (4) is to replace J(θ) and D(θ)\nby surrogate functions with nice properties. For example, one can iteratively construct local quadratic\napproximations that are strongly convex [52], or are an upper bound for the original function [57].\nHowever, an immediate problem of this naive approach is that, even if the original problem (4) is\nfeasible, the convex relaxation problem need not be. Also, these methods only deal with deterministic\nand/or convex constraints.\nIn this work, we propose an iterative algorithm that approximately solves (4) by constructing a\nsequence of convex relaxations, inspired by [38]. Our method is able to handle the possible infeasible\nsituation due to the convex relaxation as mentioned above, and handle stochastic and nonconvex\nconstraint. Since we do not have access to J(θ) or D(θ), we ﬁrst deﬁne the sample negative\ncumulative reward and cost functions as\nJ∗(θ) = −\nX\nt≥0\nγt · r(st, at)\nand\nD∗(θ) =\nX\nt≥0\nγt · d(st, at).\nGiven θ, J∗(θ) and D∗(θ) are the sample negative cumulative reward and cost value of a realization\n(i.e., a trajectory) following policy πθ. Note that both J∗(θ) and D∗(θ) are stochastic due to the\nrandomness in the policy, state transition distribution, etc. With some abuse of notation, we use\nJ∗(θ) and D∗(θ) to denote both a function of θ and a value obtained by the realization of a trajectory.\nClearly we have J(θ) = E\n\u0002\nJ∗(θ)\n\u0003\nand D(θ) = E\n\u0002\nD∗(θ)\n\u0003\n.\nWe start from some (possibly infeasible) θ0. Let θk denote the estimate of the policy parameter in\nthe k-th iteration. As mentioned above, we do not have access to the expected cumulative reward\nJ(θ). Instead we sample a trajectory following the current policy πθk and obtain a realization of the\nnegative cumulative reward value and the gradient of it as J∗(θk) and ∇θJ∗(θk), respectively. The\ncumulative reward value is obtained by Monte-Carlo estimation, and the gradient is also obtained by\nMonte-Carlo estimation according to the policy gradient theorem in (3). We provide more details on\nthe realization step later in this section. Similarly, we use the same procedure for the cost function\nand obtain realizations D∗(θk) and ∇θD∗(θk).\nWe approximate J(θ) and D(θ) at θk by the quadratic surrogate functions\neJ(θ, θk, τ) = J∗(θk) + ⟨∇θJ∗(θk), θ −θk⟩+ τ∥θ −θk∥2\n2,\n(5)\neD(θ, θk, τ) = D∗(θk) + ⟨∇θD∗(θk), θ −θk⟩+ τ∥θ −θk∥2\n2,\n(6)\nwhere τ > 0 is any ﬁxed constant. In each iteration, we solve the optimization problem\nθk = argmin\nθ\nJ\n(k)(θ)\nsubject to\nD\n(k)(θ) ≤D0,\n(7)\nwhere we deﬁne\nJ\n(k)(θ) = (1 −ρk) · J\n(k−1)(θ) + ρk · eJ(θ, θk, τ),\n(8)\nD\n(k)(θ) = (1 −ρk) · D\n(k−1)(θ) + ρk · eD(θ, θk, τ),\nwith the initial value J\n(0)(θ) = D\n(0)(θ) = 0. Here ρk is the weight parameter to be speciﬁed later.\nAccording to the deﬁnition (5) and (6), problem (7) is a convex quadratically constrained quadratic\nprogram (QCQP). Therefore, it can be efﬁciently solved by, for example, the interior point method.\nHowever, as mentioned before, even if the original problem (4) is feasible, the convex relaxation\nproblem (7) could be infeasible. In this case, we instead solve the following feasibility problem\nθk = argmin\nθ,α\nα\nsubject to\nD\n(k)(θ) ≤D0 + α.\n(9)\nIn particular, we relax the infeasible constraint and ﬁnd θk as the solution that gives the minimum\nrelaxation. Due to the speciﬁc form in (6), D\n(k)(θ) is decomposable into quadratic forms of each\ncomponent of θ, with no terms involving θi · θj. Therefore, the solution to problem (9) can be written\nin a closed form. Given θk from either (7) or (9), we update θk by\nθk+1 = (1 −ηk) · θk + ηk · θk,\n(10)\nwhere ηk is the learning rate to be speciﬁed later. Note that although we consider only one constraint\nin the algorithm, both the algorithm and the theoretical result in Section 4 can be directly generalized\nto multiple constraints setting. The whole procedure is summarized in Algorithm 1.\n4\nAlgorithm 1 Successive convex relaxation algorithm for constrained MDP\n1: Input: Initial value θ0, τ, {ρk}, {ηk}.\n2: for k = 1, 2, 3, . . . do\n3:\nObtain a sample J∗(θk) and D∗(θk) by Monte-Carlo sampling.\n4:\nObtain a sample ∇θJ∗(θk) and ∇θD∗(θk) by policy gradient theorem.\n5:\nif problem (7) is feasible then\n6:\nObtain θk by solving (7).\n7:\nelse\n8:\nObtain θk by solving (9).\n9:\nend if\n10:\nUpdate θk+1 by (10).\n11: end for\nObtaining realizations J∗(θk) and ∇θJ∗(θk).\nWe detail how to obtain realizations J∗(θk) and\n∇θJ∗(θk) corresponding to the lines 3 and 4 in Algorithm 1. The realizations of D∗(θk) and\n∇θD∗(θk) can be obtained similarly.\nFirst, we discuss ﬁnite horizon setting, where we can sample the full trajectory according to the\npolicy πθ. In particular, for any θk, we use the policy πθk to sample a trajectory and obtain J∗(θk)\nby Monte-Carlo method. The gradient ∇θJ(θ) can be estimated by the policy gradient theorem [59],\n∇θJ(θ) = −Eπθ\nh\n∇θ log πθ(s, a) · Qθ(s, a)\ni\n.\n(11)\nAgain we can sample a trajectory and obtain the policy gradient realization ∇θJ∗(θk) by Monte-Carlo\nmethod.\nIn inﬁnite horizon setting, we cannot sample the inﬁnite length trajectory. In this case, we utilize the\ntruncation method introduced in [48], which truncates the trajectory at some stage T and scales the\nundiscounted cumulative reward to obtain an unbiased estimation. Intuitively, if the discount factor\nγ is close to 0, then the future reward would be discounted heavily and, therefore, we can obtain\nan accurate estimate with a relatively small number of stages. On the other hand, if γ is close to 1,\nthen the future reward is more important compared to the small γ case and we have to sample a long\ntrajectory. Taking this intuition into consideration, we deﬁne T to be a geometric random variable\nwith parameter 1 −γ: Pr(T = t) = (1 −γ)γt. Then, we simulate the trajectory until stage T and\nuse the estimator Jtruncate(θ) = −(1 −γ) · PT\nt=0 r(st, at), which is an unbiased estimator of the\nexpected negative cumulative reward J(θ), as proved in proposition 5 in [43]. We can apply the same\ntruncation procedure to estimate the policy gradient ∇θJ(θ).\nVariance reduction.\nUsing the naive sampling method described above, we may suffer from high\nvariance problem. To reduce the variance, we can modify the above procedure in the following ways.\nFirst, instead of sampling only one trajectory in each iteration, a more practical and stable way is to\nsample several trajectories and take average to obtain the realizations. As another approach, we can\nsubtract a baseline function from the action-value function Qθ(s, a) in the policy gradient estimation\nstep (11) to reduce the variance without changing the expectation. A popular choice of the baseline\nfunction is the state-value function V θ(s) as deﬁned in (2). In this way, we can replace Qθ(s, a) in\n(11) by the advantage function Aθ(s, a) deﬁned as\nAθ(s, a) = Qθ(s, a) −V θ(s).\nThis modiﬁcation corresponds to the standard REINFORCE with Baseline algorithm [58] and can\nsigniﬁcantly reduce the variance of policy gradient.\nActor-critic method.\nFinally, we can use an actor-critic update to improve the performance further.\nIn this case, since we need unbiased estimators for both the gradient and the reward value in (5) and\n(6) in online fashion, we modify our original problem (4) to average reward setting as\nminimize\nθ∈Θ\nJ(θ) = lim\nT →∞Eπθ\n\u0014\n−1\nT\nT\nX\nt=0\nr(st, at)\n\u0015\n,\nsubject to D(θ) = lim\nT →∞Eπθ\n\u0014 1\nT\nT\nX\nt=0\nd(st, at)\n\u0015\n≤D0.\n5\nAlgorithm 2 Actor-Critic update for constrained MDP\n1: for k = 1, 2, 3, . . . do\n2:\nTake action a, observe reward r, cost d, and new state s′.\n3:\nCritic step:\n4:\nw ←w + βw · δJ∇wV J\nw (s), J ←J + βw ·\n\u0000r −J\n\u0001\n.\n5:\nv ←v + βv · δD∇vV J\nv (s), D ←D + βv ·\n\u0000d −D\n\u0001\n.\n6:\nCalculate TD error:\n7:\nδJ = r −J + V J\nw (s′) −V J\nw (s).\n8:\nδD = d −D + V D\nv (s′) −V D\nv (s).\n9:\nActor step:\n10:\nSolve θk by (7) or (9) with\nJ∗(θk), ∇θJ∗(θk) in (5) replaced by J and δJ · ∇θ log πθ(s, a);\nD∗(θk), ∇θD∗(θk) in (6) replaced by D and δD · ∇θ log πθ(s, a).\n11:\ns ←s′.\n12: end for\nLet V J\nθ (s) and V D\nθ (s) denote the value and cost functions corresponding to (2). We use possibly\nnonlinear approximation with parameter w for the value function: V J\nw (s) and v for the cost function:\nV D\nv (s). In the critic step, we update w and v by TD(0) with step size βw and βv; in the actor step, we\nsolve our proposed convex relaxation problem to update θ. The actor-critic procedure is summarized\nin Algorithm 2. Here J and D are estimators of J(θk) and D(θk). Both of J and D, and the TD\nerror δJ, δD can be initialized as 0.\nThe usage of the actor-critic method helps reduce variance by using a value function instead of\nMonte-Carlo sampling. Speciﬁcally, in Algorithm 1 we need to obtain a sample trajectory and\ncalculate J∗(θ) and ∇θJ∗(θ) by Monte-Carlo sampling. This step has a high variance since we need\nto sample a potentially long trajectory and sum up a lot of random rewards. In contrast, in Algorithm\n2, this step is replaced by a value function V J\nw (s), which reduces the variance.\n4\nTheoretical Result\nIn this section, we show almost sure convergence of the iterates obtained by our algorithm to a\nstationary point. We start by stating some mild assumptions on the original problem (4) and the\nchoice of some parameters in Algorithm 1.\nAssumption 1 The choice of {ηk} and {ρk} satisfy limk→∞\nP\nk ηk = ∞, limk→∞\nP\nk ρk = ∞\nand limk→∞\nP\nk η2\nk + ρ2\nk < ∞. Furthermore, we have limk→∞ηk/ρk = 0 and ηk is decreasing.\nAssumption 2 For any realization, J∗(θ) and D∗(θ) are continuously differentiable as functions of\nθ. Moreover, J∗(θ), D∗(θ), and their derivatives are uniformly Lipschitz continuous.\nAssumption 1 allows us to specify the learning rates. A practical choice would be ηk = k−c1 and\nρk = k−c2 with 0.5 < c2 < c1 < 1. This assumption is standard for gradient-based algorithms.\nAssumption 2 is also standard and is known to hold for a number of models. It ensures that the\nreward and cost functions are sufﬁciently regular. In fact, it can be relaxed such that each realization\nis Lipschitz (not uniformly), and the event that we keep generating realizations with monotonically\nincreasing Lipschitz constant is an event with probability 0. See condition iv) in [67] and the\ndiscussion thereafter. Also, see [45] for sufﬁcient conditions such that both the expected cumulative\nreward function and the gradient of it are Lipschitz.\nThe following Assumption 3 is useful only when we initialize with an infeasible point. We ﬁrst state\nit here and we will discuss this assumption after the statement of the main theorem.\nAssumption 3 Suppose (θS, αS) is a stationary point of the optimization problem\nminimize\nθ,α\nα\nsubject to\nD(θ) ≤D0 + α.\n(12)\nWe have that θS is a feasible point of the original problem (4), i.e. D(θS) ≤D0.\n6\nWe are now ready to state the main theorem.\nTheorem 4 Suppose the Assumptions 1 and 2 are satisﬁed with small enough initial step size η0.\nSuppose also that, either θ0 is a feasible point, or Assumption 3 is satisﬁed. If there is a subsequence\n{θkj} of {θk} that converges to some eθ, then there exist uniformly continuous functions bJ(θ) and\nbD(θ) satisfying\nlim\nj→∞J\n(kj)(θ) = bJ(θ)\nand\nlim\nj→∞D\n(kj)(θ) = bD(θ).\nFurthermore, suppose there exists θ such that bD(θ) < D0 (i.e. the Slater’s condition holds), then eθ is\na stationary point of the original problem (4) almost surely.\nThe proof of Theorem 4 is provided in the supplementary material.\nNote that Assumption 3 is not necessary if we start from a feasible point, or we reach a feasible point\nin the iterates, which could be viewed as an initializer. Assumption 3 makes sure that the iterates\nin Algorithm 1 keep making progress without getting stuck at any infeasible stationary point. A\nsimilar condition is assumed in [38] for an infeasible initializer. If it turns out that θ0 is infeasible\nand Assumption 3 is violated, then the convergent point may be an infeasible stationary point of (12).\nIn practice, if we can ﬁnd a feasible point of the original problem, then we proceed with that point.\nAlternatively, we could generate multiple initializers and obtain iterates for all of them. As long as\nthere is a feasible point in one of the iterates, we can view this feasible point as the initializer and\nTheorem 4 follows without Assumption 3. In our later experiments, for every single replicate, we\ncould reach a feasible point, and therefore Assumption 3 is not necessary.\nOur algorithm does not guarantee safe exploration during the training phase. Ensuring safety during\nlearning is a more challenging problem. Sometimes even ﬁnding a feasible point is not straightforward,\notherwise Assumption 3 is not necessary.\nOur proposed algorithm is inspired by [38]. Compared to [38] which deals with an optimization\nproblem, solving the safe reinforcement learning problem is more challenging. We need to verify\nthat the Lipschitz condition is satisﬁed, and also the policy gradient has to be estimated (instead of\ndirectly evaluated as in a standard optimization problem). The usage of the Actor-Critic algorithm\nreduces the variance of the sampling, which is unique to Reinforcement learning.\n5\nApplication to Constrained Linear-Quadratic Regulator\nWe apply our algorithm to the linear-quadratic regulator (LQR), which is one of the most fundamental\nproblems in control theory. In the LQR setting, the state dynamic equation is linear, the cost function\nis quadratic, and the optimal control theory tells us that the optimal control for LQR is a linear\nfunction of the state [23, 6]. LQR can be viewed as an MDP problem and it has attracted a lot of\nattention in the reinforcement learning literature [12, 13, 21, 47].\nWe consider the inﬁnite-horizon, discrete-time LQR problem. Denote xt as the state variable and ut\nas the control variable. The state transition and the control sequence are given by\nxt+1 = Axt + But + vt,\nut = −Fxt + wt,\n(13)\nwhere vt and wt represent possible Gaussian white noise, and the initial state is given by x0. The goal\nis to ﬁnd the control parameter matrix F such that the expected total cost is minimized. The usual\ncost function of LQR corresponds to the negative reward in our setting and we impose an additional\nquadratic constraint on the system. The overall optimization problem is given by\nminimize J(F) = E\n\u0014X\nt≥0\nx⊤\nt Q1xt + u⊤\nt R1ut\n\u0015\n,\nsubject to D(F) = E\n\u0014X\nt≥0\nx⊤\nt Q2xt + u⊤\nt R2ut\n\u0015\n≤D0,\nwhere Q1, Q2, R1, and R2 are positive deﬁnite matrices. Note that even thought the matrices are\npositive deﬁnite, both the objective function J and the constraint D are nonconvex with respect to\n7\nthe parameter F. Furthermore, with the additional constraint, the optimal control sequence may no\nlonger be linear in the state xt. Nevertheless, in this work, we still consider linear control given by\n(13) and the goal is to ﬁnd the best linear control for this constrained LQR problem. We assume that\nthe choice of A, B are such that the optimal cost is ﬁnite.\nRandom initial state.\nWe ﬁrst consider the setting where the initial state x0 ∼D follows a random\ndistribution D, while both the state transition and the control sequence (13) are deterministic (i.e.\nvt = wt = 0). In this random initial state setting, [24] showed that without the constraint, the policy\ngradient method converges efﬁciently to the global optima in polynomial time. In the constrained\ncase, we can explicitly write down the objective and constraint function, since the only randomness\ncomes from the initial state. Therefore, we have the state dynamic xt+1 = (A −BF)xt and the\nobjective function has the following expression ([24], Lemma 1)\nJ(F) = Ex0∼D\n\u0002\nx⊤\n0 PF x0\n\u0003\n,\n(14)\nwhere PF is the solution to the following equation\nPF = Q1 + F ⊤R1F + (A −BF)⊤PF (A −BF).\n(15)\nThe gradient is given by\n∇F J(F) = 2\n\u0010\u0000R1 + B⊤PF B\n\u0001\nF −B⊤PF A\n\u0011\n·\n\u0014\nEx0∼D\n∞\nX\nt=0\nxtx⊤\nt\n\u0015\n.\n(16)\nLet SF = P∞\nt=0 xtx⊤\nt and observe that\nSF = x0x⊤\n0 + (A −BF)SF (A −BF)⊤.\n(17)\nWe start from some F0 and apply our Algorithm 1 to solve the constrained LQR problem. In iteration\nk, with the current estimator denoted by Fk, we ﬁrst obtain an estimator of PFk by starting from\nQ1 and iteratively applying the recursion PFk ←Q1 + F ⊤\nk R1Fk + (A −BFk)⊤PFk(A −BFk)\nuntil convergence. Next, we sample an x∗\n0 from the distribution D and follow a similar recursion\ngiven by (17) to obtain an estimate of SFk. Plugging the sample x∗\n0 and the estimates of PFk and\nSFk into (14) and (16), we obtain the sample reward value J∗(Fk) and ∇F J∗(Fk), respectively.\nWith these two values, we follow (5) and (8) and obtain J\n(k)(F). We apply the same procedure to\nthe cost function D(F) with Q1, R1 replaced by Q2, R2 to obtain D\n(k)(F). Finally we solve the\noptimization problem (7) (or (9) if (7) is infeasible) and obtain Fk+1 by (10).\nRandom state transition and control.\nWe then consider the setting where both vt and wt are\nindependent standard Gaussian white noise. In this case, the state dynamic can be written as\nxt+1 = (A −BF)xt + ϵt where ϵt ∼N(0, I + BB⊤). Let PF be deﬁned as in (15) and SF be the\nsolution to the following Lyapunov equation\nSF = I + BB⊤+ (A −BF)SF (A −BF)⊤.\nThe objective function has the following expression ([68], Proposition 3.1)\nJ(F) = Ex∼N(0,SF )\nh\nx⊤(Q1 + F ⊤R1F)x\ni\n+ tr(R1),\n(18)\nand the gradient is given by\n∇F J(F) = 2\n\u0010\u0000R1 + B⊤PF B\n\u0001\nF −B⊤PF A\n\u0011\n· Ex∼N(0,SF )\nh\nxx⊤i\n.\n(19)\nAlthough in this setting it is straightforward to calculate the expectation in a closed form, we keep the\ncurrent expectation form to be in line with our algorithm. Moreover, when the error distribution is\nmore complicated or unknown, we can no longer calculate the closed form expression and have to\nsample in each iteration. With the formulas given by (18) and (19), we again apply our Algorithm 1.\nWe sample x ∼N(0, SF ) in each iteration and solve the optimization problem (7) or (9). The whole\nprocedure is similar to the random initial state case described above.\nOther applications.\nOur algorithm can also be applied to constrained parallel MDP and constrained\nmulti-agent MDP problem. Due to the space limit, we relegate them to supplementary material.\n8\n0\n500\n1000\n1500\niteration\n30.5\n31\n31.5\n32\n32.5\n33\nconstraint value\n(a) Constraint value D(θk) in each iteration.\n0\n500\n1000\n1500\niteration\n43.5\n44\n44.5\n45\n45.5\n46\n46.5\nobjective value\n(b) Objective value J(θk) in each iteration.\nFigure 1: An experiment on constrained LQR problem. The iterate starts from an infeasible point and\nthen becomes feasible and eventually converges.\nmin value\n# iterations\napprox. min value\napprox. # iterations\nOur method\n30.689 ± 0.114\n2001 ± 1172\n30.694 ± 0.114\n604.3 ± 722.4\nLagrangian\n30.693 ± 0.113\n7492 ± 1780\n30.699 ± 0.113\n5464 ± 2116\nTable 1: Comparison of our method with Lagrangian method\n6\nExperiment\nWe verify the effectiveness of the proposed algorithm through experiments. We focus on the LQR\nsetting with a random initial state as discussed in Section 5. In this experiment we set x ∈R15 and\nu ∈R8. The initial state distribution is uniform on the unit cube: x0 ∼D = Uniform\n\u0000[−1, 1]15\u0001\n.\nEach element of A and B is sampled independently from the standard normal distribution and scaled\nsuch that the eigenvalues of A are within the range (−1, 1). We initialize F0 as an all-zero matrix,\nand the choice of the constraint function and the value D0 are such that (1) the constrained problem is\nfeasible; (2) the solution of the unconstrained problem does not satisfy the constraint, i.e., the problem\nis not trivial; (3) the initial value F0 is not feasible. The learning rates are set as ηk = 2\n3k−3/4 and\nρk = 2\n3k−2/3. The conservative choice of step size is to avoid the situation where an eigenvalue of\nA −BF runs out of the range (−1, 1), and so the system is stable. 5\nFigure 1(a) and 1(b) show the constraint and objective value in each iteration, respectively. The red\nhorizontal line in Figure 1(a) is for D0, while the horizontal line in Figure 1(b) is for the unconstrained\nminimum objective value. We can see from Figure 1(a) that we start from an infeasible point, and the\nproblem becomes feasible after about 100 iterations. The objective value is in general decreasing\nafter becoming feasible, but never lower than the unconstrained minimum, as shown in Figure 1(b).\nComparison with the Lagrangian method.\nWe compare our proposed method with the usual\nLagrangian method. For the Lagrangian method, we follow the algorithm proposed in [18] for safe\nreinforcement learning, which iteratively applies gradient descent on the parameter F and gradient\nascent on the Lagrangian multiplier λ for the Lagrangian function until convergence.\nTable 1 reports the comparison results with mean and standard deviation based on 50 replicates. In\nthe second and third columns, we compare the minimum objective value and the number of iterations\nto achieve it. We also consider an approximate version, where we are satisﬁed with the result if the\nobjective value exceeds less than 0.02% of the minimum value. The fourth and ﬁfth columns show\nthe comparison results for this approximate version. We can see that both methods achieve similar\nminimum objective values, but ours requires less number of policy updates, for both minimum and\napproximate minimum version.\n5The code is available at https://github.com/ming93/Safe_reinforcement_learning\n9\nReferences\n[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization.\nIn International Conference on Machine Learning, pages 22–31, 2017.\n[2] Leonard Adolphs. Non convex-concave saddle point optimization. Master’s thesis, ETH Zurich,\n2018.\n[3] Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.\n[4] Haitham Bou Ammar, Rasul Tutunov, and Eric Eaton. Safe policy search for lifelong reinforce-\nment learning with sublinear regret. In International Conference on Machine Learning, pages\n2361–2369, 2015.\n[5] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e.\nConcrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.\n[6] Brian DO Anderson and John B Moore. Optimal control: linear quadratic methods. Courier\nCorporation, 2007.\n[7] Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efﬁcient q-learning with low\nswitching cost. arXiv preprint arXiv:1905.12849, 2019.\n[8] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\n[9] Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based\nreinforcement learning with stability guarantees. In Advances in Neural Information Processing\nSystems, pages 908–918, 2017.\n[10] Vivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters,\n29(5):291–294, 1997.\n[11] Craig Boutilier. Planning, learning and coordination in multiagent decision processes. In\nProceedings of the 6th conference on Theoretical aspects of rationality and knowledge, pages\n195–210. Morgan Kaufmann Publishers Inc., 1996.\n[12] Steven J Bradtke. Reinforcement learning applied to linear quadratic regulation. In Advances in\nneural information processing systems, pages 295–302, 1993.\n[13] Steven J Bradtke, B Erik Ydstie, and Andrew G Barto. Adaptive linear quadratic control\nusing policy iteration. In Proceedings of the American control conference, volume 3, pages\n3475–3475. Citeseer, 1994.\n[14] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.\n[15] Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multia-\ngent reinforcement learning. IEEE Transactions on Systems, Man, And Cybernetics-Part C:\nApplications and Reviews, 38 (2), 2008, 2008.\n[16] Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning\nconverges to global optima. arXiv preprint arXiv:1905.10027, 2019.\n[17] Tianyi Chen, Kaiqing Zhang, Georgios B Giannakis, and Tamer Bas¸ar. Communication-efﬁcient\ndistributed reinforcement learning. arXiv preprint arXiv:1812.03239, 2018.\n[18] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained\nreinforcement learning with percentile risk criteria. Journal of Machine Learning Research,\n18(167):1–167, 2017.\n[19] Yinlam Chow, Oﬁr Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A\nlyapunov-based approach to safe reinforcement learning. arXiv preprint arXiv:1805.07708,\n2018.\n[20] Christoph Dann, Gerhard Neumann, and Jan Peters. Policy evaluation with temporal differences:\nA survey and comparison. The Journal of Machine Learning Research, 15(1):809–883, 2014.\n[21] Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample\ncomplexity of the linear quadratic regulator. arXiv preprint arXiv:1710.01688, 2017.\n[22] Nelson Dunford and Jacob T Schwartz. Linear operators part I: general theory, volume 7.\nInterscience publishers New York, 1958.\n10\n[23] Lawrence C Evans. An introduction to mathematical optimal control theory. Lecture Notes,\nUniversity of California, Department of Mathematics, Berkeley, 2005.\n[24] Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy\ngradient methods for the linear quadratic regulator. In International Conference on Machine\nLearning, pages 1466–1475, 2018.\n[25] Jaime F Fisac, Anayo K Akametalu, Melanie N Zeilinger, Shahab Kaynama, Jeremy Gillula,\nand Claire J Tomlin. A general safety framework for learning-based control in uncertain robotic\nsystems. IEEE Transactions on Automatic Control, 2018.\n[26] Javier Garcıa and Fernando Fern´andez. A comprehensive survey on safe reinforcement learning.\nJournal of Machine Learning Research, 16(1):1437–1480, 2015.\n[27] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural\ninformation processing systems, pages 2672–2680, 2014.\n[28] Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. A survey of actor-\ncritic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on\nSystems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6):1291–1307, 2012.\n[29] Jingyu He, Saar Yalov, and P Richard Hahn. XBART: Accelerated Bayesian additive regression\ntrees. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages\n1130–1138, 2019.\n[30] Jingyu He, Saar Yalov, Jared Murray, and P Richard Hahn. Stochastic tree ensembles for\nregularized supervised learning. Technical report, 2019.\n[31] Jessie Huang, Fa Wu, Doina Precup, and Yang Cai. Learning safe policies with expert guidance.\narXiv preprint arXiv:1805.08313, 2018.\n[32] John L Kelley. General topology. Courier Dover Publications, 2017.\n[33] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information\nprocessing systems, pages 1008–1014, 2000.\n[34] R Matthew Kretchmar. Parallel reinforcement learning. In The 6th World Conference on\nSystemics, Cybernetics, and Informatics. Citeseer, 2002.\n[35] Jonathan Lacotte, Yinlam Chow, Mohammad Ghavamzadeh, and Marco Pavone. Risk-sensitive\ngenerative adversarial imitation learning. arXiv preprint arXiv:1808.04468, 2018.\n[36] Dennis Lee, Haoran Tang, Jeffrey O Zhang, Huazhe Xu, Trevor Darrell, and Pieter Abbeel.\nModular architecture for starcraft ii with deep reinforcement learning. In Fourteenth Artiﬁcial\nIntelligence and Interactive Digital Entertainment Conference, 2018.\n[37] Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017.\n[38] An Liu, Vincent Lau, and Borna Kananian. Stochastic successive convex approximation for\nnon-convex constrained stochastic optimization. arXiv preprint arXiv:1801.08266, 2018.\n[39] Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy\noptimization attains globally optimal policy. arXiv preprint arXiv:1906.10306, 2019.\n[40] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lilli-\ncrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning. In International conference on machine learning, pages 1928–1937,\n2016.\n[41] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\n[42] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro\nDe Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, et al.\nMassively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296,\n2015.\n[43] Santiago Paternain. Stochastic Control Foundations of Autonomous Behavior. PhD thesis,\nUniversity of Pennsylvania, 2018.\n11\n[44] Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang.\nMultiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning\nto play starcraft combat games. arXiv preprint arXiv:1703.10069, 2017.\n[45] Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in lipschitz markov\ndecision processes. Machine Learning, 100(2-3):255–283, 2015.\n[46] LA Prashanth and Mohammad Ghavamzadeh. Variance-constrained actor-critic algorithms for\ndiscounted and average reward mdps. Machine Learning, 105(3):367–417, 2016.\n[47] Benjamin Recht. A tour of reinforcement learning: The view from continuous control. Annual\nReview of Control, Robotics, and Autonomous Systems, 2018.\n[48] Chang-han Rhee and Peter W Glynn. Unbiased estimation with square root convergence for sde\nmodels. Operations Research, 63(5):1026–1043, 2015.\n[49] Andrzej Ruszczy´nski. Feasible direction methods for stochastic programming problems. Math-\nematical Programming, 19(1):220–229, 1980.\n[50] Joris Scharpff, Diederik M Roijers, Frans A Oliehoek, Matthijs TJ Spaan, and Mathijs Michiel\nde Weerdt. Solving transition-independent multi-agent mdps with sparse interactions. In AAAI,\npages 3174–3180, 2016.\n[51] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\npolicy optimization. In International Conference on Machine Learning, pages 1889–1897,\n2015.\n[52] Gesualdo Scutari, Francisco Facchinei, Peiran Song, Daniel P Palomar, and Jong-Shi Pang.\nDecomposition by partial linearization: Parallel optimization of multi-agent systems. IEEE\nTransactions on Signal Processing, 62(3):641–656, 2013.\n[53] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess-\nche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al.\nMastering the game of go with deep neural networks and tree search. nature, 529(7587):484,\n2016.\n[54] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general\nreinforcement learning algorithm that masters chess, shogi, and go through self-play. Science,\n362(6419):1140–1144, 2018.\n[55] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\nGuez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of\ngo without human knowledge. Nature, 550(7676):354, 2017.\n[56] Peng Sun, Xinghai Sun, Lei Han, Jiechao Xiong, Qing Wang, Bo Li, Yang Zheng, Ji Liu,\nYongsheng Liu, Han Liu, et al. Tstarbots: Defeating the cheating level builtin ai in starcraft ii in\nthe full game. arXiv preprint arXiv:1809.07193, 2018.\n[57] Ying Sun, Prabhu Babu, and Daniel P Palomar. Majorization-minimization algorithms in signal\nprocessing, communications, and machine learning. IEEE Transactions on Signal Processing,\n65(3):794–816, 2016.\n[58] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\n2018.\n[59] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient\nmethods for reinforcement learning with function approximation. In Advances in neural\ninformation processing systems, pages 1057–1063, 2000.\n[60] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization.\narXiv preprint arXiv:1805.11074, 2018.\n[61] Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration in ﬁnite markov\ndecision processes with gaussian processes. In Advances in Neural Information Processing\nSystems, pages 4312–4320, 2016.\n[62] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\nMichelle Yeo, Alireza Makhzani, Heinrich K¨uttler, John Agapiou, Julian Schrittwieser, et al.\nStarcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782,\n2017.\n12\n[63] Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong. Multi-agent reinforcement\nlearning via double averaging primal-dual optimization. arXiv preprint arXiv:1806.00877,\n2018.\n[64] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods:\nGlobal optimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2019.\n[65] Min Wen and Ufuk Topcu. Constrained cross-entropy method for safe reinforcement learning.\nIn Advances in Neural Information Processing Systems, pages 7461–7471, 2018.\n[66] Sijia Xu, Hongyu Kuang, Zhi Zhuang, Renjie Hu, Yang Liu, and Huyang Sun. Macro action\nselection with deep reinforcement learning in starcraft. arXiv preprint arXiv:1812.00336, 2018.\n[67] Yang Yang, Gesualdo Scutari, Daniel P Palomar, and Marius Pesavento. A parallel decomposi-\ntion method for nonconvex stochastic multi-agent optimization problems. IEEE Transactions\non Signal Processing, 64(11):2949–2964, 2016.\n[68] Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. On the global conver-\ngence of actor-critic: A case for linear quadratic regulator with ergodic cost. arXiv preprint\narXiv:1907.06246, 2019.\n[69] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Bas¸ar. Finite-sample analyses\nfor fully decentralized multi-agent reinforcement learning. arXiv preprint arXiv:1812.02783,\n2018.\n13\nA\nOther applications\nA.1\nConstrained Parallel Markov Decision Process\nWe consider the parallel MDP problem [34, 42, 17] where we have a single-agent MDP task and N\nworkers, where each worker acts as an individual agent and aims to solve the same MDP problem. In\nthe parallel MDP setting, each agent is characterized by a tuple (S, A, P, γ, ri, di, µi), where each\nagent has the same but individual state space, action space, transition probability distribution, and the\ndiscount factor. However, the reward function, cost function, and the distribution of the initial state\ns0 ∈S could be different for each agent, but satisfy E[ri(s, a)] = r(s, a), E[di(s, a)] = d(s, a), and\nE[µi(s, a)] = µ(s, a). Each agent i generates its own trajectory {si\n0, ai\n0, si\n1, ai\n1, . . . } and collects its\nown reward/cost value {ri\n0, di\n0, ri\n1, di\n1, . . . }.\nThe hope is that by solving the single-agent problem using N agents in parallel, the algorithm could\nbe more stable and converge much faster [40]. Intuitively, each agent i may have a different initial\nstate and will explore different parts of the state space due to the randomness in the state transition\ndistribution and the policy. It also helps to reduce the correlation between agents’ behaviors. As\na result, by running multiple agents in parallel, we are more likely to visit different parts of the\nenvironment and get the experience of the reward/cost function values more efﬁciently. This mimics\nthe strategy used in tree-based supervised learning algorithms [14, 29, 30].\nFollowing the settings in [17], we have N agents (i.e., N workers) and one central controller in the\nsystem. The global parameter is denoted by θ, and we consider the constrained parallel MDP problem\nwhere the goal is to solve the following optimization problem:\nminimize\nθ\nJ(θ) =\nN\nX\ni=1\nEπθ\n\u0014\n−\nX\nt≥0\nγt · ri(si\nt, ai\nt)\n\u0015\n,\nsubject to D(θ) = Eπθ\n\u0014X\nt≥0\nγt · di(si\nt, ai\nt)\n\u0015\n≤D0, i ∈N.\nDuring the estimation step, the controller broadcasts the current parameter θk to each agent and each\nagent samples its own trajectory and obtains estimators for function value/gradient of the reward/cost\nfunction. Next, each agent uploads its estimators to the central controller and the central controller\ntakes the average over these estimators, and then follow our proposed algorithm to solve for the\nQCQP problem and update the parameter to θk+1. This process continues until convergence.\nA.2\nConstrained Multi-agent Markov Decision Process\nA natural extension of the (single-agent) MDP is to consider a model with N agents termed multi-\nagent Markov decision process (MMDP). Recently this kind of problem has been attracting more\nand more attention. See [15] for a comprehensive survey. Most of the work on multi-agent MDP\nproblems consider the setting where the agents share the same global state space, but each with their\nown collection of actions and rewards [11, 63, 69]. In each stage of the system, each agent observes\nthe global state and chooses its own action individually. As a result, each agent receives its reward\nand the state evolves according to the joint transition distribution. An MMDP problem can be fully\ncollaborative where all the agents have the same goal, or fully competitive where the problem consists\nof two agents with an opposite goal, or the mix of the two.\nHere we consider a slightly different setting where each agent has its own state space. The only\nconnection between the agents is that the global reward is a function of the overall states and actions.\nFurthermore, each agent has its own constraint which depends on its own state and action only. This\nproblem is known as Transition-Independent Multi-agent MDP and is considered in [50]. Speciﬁcally,\neach agent’s task is characterized by a tuple (Si, Ai, P i, γ, di, µi) with each component deﬁned as\nusual. Note that P i : Si × Ai →P(Si) and di : Si × Ai →R are functions of each agent’s state\nand action only and do not depend on other agents. Denote S = Πi∈N Si and A = Πi∈N Ai as\nthe joint state space and action space. The global reward function is given by r: S × A →R that\ndepends on the joint state and action. Here we consider the fully collaborative setting where all\nthe agents have the same goal. Under this setting, the policy set of each agent is parameterized as\n{πi\nθi : Si →P(Ai)} and we denote θ = [θ1, . . . , θN] as the overall parameters and πθ as the overall\npolicy. In the following, we use N = {1, 2, . . . , N} to denote the N agents. Denote ai\nt as the action\n14\nchosen by agent i at stage t and at = Πi∈N ai\nt as the joint action chosen by all the agents. The goal\nof this constrained MMDP is to solve the following problem\nminimize\nθ\nJ(θ) = Eπθ\n\u0014\n−\nX\nt≥0\nγt · r(st, at)\n\u0015\n,\nsubject to Di(θi) = Eπθi\n\u0014X\nt≥0\nγt · di(si\nt, ai\nt)\n\u0015\n≤Di\n0, i ∈N.\n(20)\nInspired by the parallel implementation ([38], Section V), our algorithm applies naturally to con-\nstrained MMDP problem with some modiﬁcations. This modiﬁed procedure can also be viewed as\na distributed version of the original algorithm. The overall problem (20) can be viewed as a large\n“single-agent” problem where the constraints are decomposable into N parts. In this case, instead of\nsolving a large QCQP problem in each iteration, each agent could solve its own QCQP problem in a\ndistributed manner which is much more efﬁcient. As before, we denote the sample negative reward\nand cost function as\nJ∗(θ) = −\nX\nt≥0\nγt · r(st, at)\nand\nDi,∗(θi) =\nX\nt≥0\nγt · di(si\nt, ai\nt).\nIn each iteration with θk = [θ1\nk, ..., θN\nk ], we approximate J(θ) and D(θ) as\neJi(θi, θk, τ) = 1\nN J∗(θk) + ⟨∇θiJ∗(θk), θi −θi\nk⟩+ τ∥θi −θi\nk∥2\n2,\neDi(θi, θk, τ) = Di,∗(θi\nk) + ⟨∇θiDi,∗(θi\nk), θi −θi\nk⟩+ τ∥θi −θi\nk∥2\n2.\nNote that the constraint function is naturally decomposable into N parts. We also “manually” split the\nobjective function into N parts, so that each agent could solve its own QCQP problem in a distributed\nmanner. As before, we deﬁne\nJ\ni,(k)(θi) = (1 −ρk) · J\ni,(k−1)(θi) + ρk · eJi(θi, θk, τ),\nD\ni,(k)(θi) = (1 −ρk) · D\ni,(k−1)(θi) + ρk · eDi(θi, θk, τ).\nWith this surrogate functions, each agent then solves its own convex relaxation problem\nθ\ni\nk = argmin\nθi\nJ\ni,(k)(θi)\nsubject to\nD\ni,(k)(θi) ≤Di\n0,\n(21)\nor, alternatively, solves for the feasibility problem if (21) is infeasible\nθ\ni\nk = argmin\nθi,αi\nαi\nsubject to\nD\ni,(k)(θi) ≤Di\n0 + αi.\nThis step can be implemented in a distributed manner for each agent and is more efﬁcient than solving\nthe overall problem with the overall parameter θ. Finally, the update rule for each agent i is as usual\nθi\nt+1 = (1 −ηk) · θi\nk + ηk · θ\ni\nk.\nThis process continues until convergence.\nB\nProof of Theorem 4\nAccording to the choice of the surrogate function (5) and Assumption 2, it is straightforward to\nverify that the function J\n(k)(θ) deﬁned in (8) is uniformly strongly convex in θ for each iteration t.\nMoreover, both J\n(k)(θ) and ∇θJ\n(k)(θ) are Lipschitz continuous functions.\nFrom Lemma 1 in [49] we have\nlim\nt→∞\n\f\f\fJ\n(k)(θ) −E\n\u0002 eJ(θ, θk, τ)\n\u0003\f\f\f = 0.\nSince the function E\n\u0002 eJ(θ, θk, τ)\n\u0003\nis Lipschitz continuous in θk, we obtain that\n\f\f\fJ\n(k1)(θ) −J\n(k2)(θ)\n\f\f\f ≤L0 · ∥θk1 −θk2∥+ ϵ,\n15\nfor some constant L0 and the error term ϵ that goes to 0 as k1, k2 go to inﬁnity. This shows that the\nfunction sequence J\n(kj)(θ) is equicontinuous. Since Θ is compact and the discounted cumulative\nreward function is bounded by rmax/(1 −γ), we can apply Arzela-Ascoli theorem [22, 32] to prove\nexistence of bJ(θ) that converges uniformly. Moreover, since we apply the same operations on the\nconstraint function D(θ) as to the reward function J(θ) in Algorithm 1, the above properties also\nhold for D(θ).\nThe rest of the proof follows in a similar way as the proof of Theorem 1 in [38]. Under Assumptions 1\n- 3, the technical conditions in [38] are satisﬁed by the choice of the surrogate functions (5) and (6).\nAccording to Lemma 2 in [38], with probability one we have\nlim sup\nk→∞\nD(θk) ≤D0.\nThis shows that, although in some of the iterations the convex relaxation problem (7) is infeasible,\nand we have to solve the alternative problem (9), the iterates {θk} converge to the feasible region\nof the original problem (4) with probability one. Furthermore, with probability one, the convergent\npoint eθ is the optimal solution to the following problem\nminimize\nθ∈Θ\nbJ(θ)\nsubject to\nbD(θ) ≤D0.\n(22)\nThe KKT conditions for (22) together with the Slater condition show that the KKT conditions of\nthe original problem (4) are also satisﬁed at eθ. This shows that eθ is a stationary point of the original\nproblem almost surely.\n16\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-10-26",
  "updated": "2019-10-26"
}