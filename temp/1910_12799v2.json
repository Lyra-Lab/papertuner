{
  "id": "http://arxiv.org/abs/1910.12799v2",
  "title": "Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space",
  "authors": [
    "Taiji Suzuki",
    "Atsushi Nitanda"
  ],
  "abstract": "Deep learning has exhibited superior performance for various tasks,\nespecially for high-dimensional datasets, such as images. To understand this\nproperty, we investigate the approximation and estimation ability of deep\nlearning on anisotropic Besov spaces. The anisotropic Besov space is\ncharacterized by direction-dependent smoothness and includes several function\nclasses that have been investigated thus far. We demonstrate that the\napproximation error and estimation error of deep learning only depend on the\naverage value of the smoothness parameters in all directions. Consequently, the\ncurse of dimensionality can be avoided if the smoothness of the target function\nis highly anisotropic. Unlike existing studies, our analysis does not require a\nlow-dimensional structure of the input data. We also investigate the minimax\noptimality of deep learning and compare its performance with that of the kernel\nmethod (more generally, linear estimators). The results show that deep learning\nhas better dependence on the input dimensionality if the target function\npossesses anisotropic smoothness, and it achieves an adaptive rate for\nfunctions with spatially inhomogeneous smoothness.",
  "text": "Deep learning is adaptive to intrinsic dimensionality\nof model smoothness in anisotropic Besov space\nTaiji Suzuki\nDepartment of Mathematical Informatics, The University of Tokyo, Tokyo, Japan\nRIKEN Center for Advanced Intelligence Project, Tokyo, Japan\ntaiji@mist.i.u-tokyo.ac.jp\nAtsushi Nitanda\nKyushu Institute of Technology, Fukuoka, Japan\nRIKEN Center for Advanced Intelligence Project, Tokyo, Japan\nnitanda@ai.kyutech.ac.jp\nAbstract\nDeep learning has exhibited superior performance for various tasks, especially\nfor high-dimensional datasets, such as images.\nTo understand this property,\nwe investigate the approximation and estimation ability of deep learning on\nanisotropic Besov spaces.\nThe anisotropic Besov space is characterized by\ndirection-dependent smoothness and includes several function classes that have\nbeen investigated thus far. We demonstrate that the approximation error and es-\ntimation error of deep learning only depend on the average value of the smooth-\nness parameters in all directions. Consequently, the curse of dimensionality can\nbe avoided if the smoothness of the target function is highly anisotropic. Un-\nlike existing studies, our analysis does not require a low-dimensional structure of\nthe input data. We also investigate the minimax optimality of deep learning and\ncompare its performance with that of the kernel method (more generally, linear\nestimators). The results show that deep learning has better dependence on the in-\nput dimensionality if the target function possesses anisotropic smoothness, and it\nachieves an adaptive rate for functions with spatially inhomogeneous smoothness.\n1\nIntroduction\nBased on the recent literature pertaining to machine learning, deep learning has exhibited superior\nperformance in several tasks such as image recognition (Krizhevsky et al., 2012), natural language\nprocessing (Devlin et al., 2018), and image synthesis (Radford et al., 2015). In particular, its superi-\nority is remarkable for complicated and high-dimensional data like images. This is mainly due to its\nhigh ﬂexibility and superior feature-extraction ability for effectively extracting the intrinsic structure\nof data. Its theoretical analysis also has been extensively developed considering several aspects such\nas expressive ability, optimization, and generalization error.\nAmongst representation ability analysis of deep neural networks such as universal approximation\nability (Cybenko, 1989; Hornik, 1991; Sonoda & Murata, 2017), approximation theory of deep\nneural networks on typical function classes such as H¨older, Sobolev, and Besov spaces have been\nextensively studied. In particular, analyses of deep neural networks with the ReLU activation (Nair\n& Hinton, 2010; Glorot et al., 2011) have been recently developed. Schmidt-Hieber (2018) showed\nthat the deep learning with ReLU activations can achieve the minimax optimal estimation accuracy\nto estimate composite functions in H¨older spaces by using the approximation theory of Yarotsky\n(2017). Suzuki (2019) generalized this analysis to those on the Besov space and the mixed smooth\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:1910.12799v2  [stat.ML]  30 Sep 2021\nTable 1: Relationship between existing research and our work. β indicates the smoothness of the\ntarget function, d is the dimensionality of input x, D is the dimensionality of a low-dimensional\nstructure on which the data are distributed, and eβ is the average smoothness of an anisotropic Besov\nspace (Eq. (1)).\nFunction\nclass\nH¨older\nBesov\nmixed\nsmooth\nBesov\nH¨older on a low-\ndimensional set\nanisotropic\nBesov\nAuthor\nSchmidt-\nHieber\n(2018)\nSuzuki\n(2019)\nSuzuki (2019)\nNakada\n&\nImaizumi\n(2020);\nSchmidt-Hieber\n(2019); Chen et al. (2019)\nThis work\nEstimation\nerror\n˜O(n−\n2β\n2β+d )\n˜O(n−\n2β\n2β+d )\n˜O\n\u0010\nn−\n2β\n2β+1 ×\nlog(n)\n2(d−1)(u+β)\n1+2β\n\u0011\n˜O(n−\n2β\n2β+D )\n˜O(n\n−\n2 e\nβ\n2 e\nβ+1 )\nBesov space by utilizing the techniques developed in approximation theories (Temlyakov, 1993;\nDeVore, 1998). It was shown that deep learning can achieve an adaptive approximation error rate\nthat is faster than that of (non-adaptive) linear approximation methods (DeVore & Popov, 1988;\nDeVore et al., 1993; D˜ung, 2011b), and it outperforms any linear estimators (including kernel ridge\nregression) in terms of the minimax optimal rate.\nFrom these analyses, one can see that the approximation errors and estimation errors are strongly\ninﬂuenced by two factors, i.e., the smoothness of the target function and the dimensionality of the\ninput (see Table 1). In particular, they suffer from the curse of dimensionality, which is unavoid-\nable. However, these analyses are about the worst case errors and do not exploit speciﬁc intrinsic\nproperties of the true distributions. For example, practically encountered data usually possess low\nintrinsic dimensionality, i.e., data are distributed on a low dimensional sub-manifold of the input\nspace (Tenenbaum et al., 2000; Belkin & Niyogi, 2003). Recently, Nakada & Imaizumi (2020);\nSchmidt-Hieber (2019); Chen et al. (2019); Chen et al. (2019) have shown that deep ReLU network\nhas adaptivity to the intrinsic dimensionality of data and can avoid curse of dimensionality if the\nintrinsic dimensionality is small. However, one drawback is that they assumed exact low dimen-\nsionality of the input data. This could be a strong assumption because practically observed data\nare always noisy, and injecting noise immediately destroys the low-dimensional structure. There-\nfore, we consider another direction in this paper. In terms of curse of dimensionality, Suzuki (2019)\nshowed that deep learning can alleviate the curse of dimensionality to estimate functions in a so\ncalled mixed smooth Besov space (m-Besov). However, m-Besov space assumes strong smoothness\ntoward all directions uniformly and does not include the ordinary Besov space as a special case.\nMoreover, the convergence rate includes heavy poly-log term which is not negligible (see Table 1).\nIn practice, one of the typically expected properties of a true function on high-dimensional data is\nthat it is invariant against perturbations of an input in some speciﬁc directions (Figure 1). For ex-\nample, in image-recognition tasks, the target function must be invariant against the spatial shift of\nan input image, which is utilized by data-augmentation techniques (Simard et al., 2003; Krizhevsky\net al., 2012). In this paper, we investigate the approximation and estimation abilities of deep learn-\ning on anisotropic Besov spaces (Nikol’skii, 1975; Vybiral, 2006; Triebel, 2011) (also called dom-\ninated mixed-smooth Besov spaces). An anisotropic Besov space is a set of functions that have\n“direction-dependent” smoothness, whereas ordinary function spaces such as H¨older, Sobolev, and\nBesov spaces assume isotropic smoothness that is uniform in all directions. We consider a com-\nposition of functions included in an anisotropic Besov space, including several existing settings as\nspecial cases; it includes analyses of the H¨older space Schmidt-Hieber (2018) and Besov space\nSuzuki (2019), as well as the low-dimensional sub-manifold setting (Nakada & Imaizumi, 2020;\nSchmidt-Hieber, 2019; Chen et al., 2019; Chen et al., 2019)1. By considering such a space, we\ncan show that deep learning can alleviate curse of dimensionality if the smoothness in each direc-\ntion is highly anisotropic. Interestingly, any linear estimator (including kernel ridge regression) has\nworse dependence on the dimensionality than deep learning. Our contributions can be summarized\nas follows:\n• We consider a situation in which the target function is included in a class of anisotropic Besov\nspaces and show that deep learning can avoid the curse of dimensionality even if the input data\n1We would like to remark that the analysis of Nakada & Imaizumi (2020) does not require smoothness of\nthe embedded manifold that is not covered in this paper.\n2\ndo not lie on a low-dimensional manifold. Moreover, deep learning can achieve the optimal\nadaptive approximation error rate and minimax optimal estimation error rate.\n• We compare deep learning with general linear estimators (including kernel methods) and show\nthat deep learning has better dependence on the input dimensionality than linear estimators.\n2\nProblem setting and the model\nIn this section, we describe the problem setting considered in this work. We consider the following\nnonparametric regression model:\nyi = f o(xi) + ξi\n(i = 1, . . . , n),\nwhere xi is generated from a probability distribution PX on [0, 1]d, ξi ∼N(0, σ2), and the\ndata Dn = (xi, yi)n\ni=1 are independently identically distributed.\nf o is the true function that\nwe want to estimate. We are interested in the mean squared estimation error of an estimator bf:\nEDn[∥bf −f o∥2\nL2(PX)], where EDn[·] indicates the expectation with respect to the training data Dn.\nWe consider a least-squares estimator in the deep neural network model as bf (see Eq. (5)) and\ndiscuss its optimality. More speciﬁcally, we investigate how the “intrinsic dimensionality” of data\naffects the estimation accuracy of deep learning. For this purpose, we consider an anisotropic Besov\nspace as a model of the target function.\n2.1\nAnisotropic Besov space\nIn this section, we introduce the anisotropic Besov which was investigated as the model of the true\nfunction in this paper. Throughout this paper, we set the domain of the input to Ω= [0, 1]d. For a\nfunction f : Ω→R, let ∥f∥p := ∥f∥Lp(Ω) := (\nR\nΩ|f|pdx)1/p for 0 < p < ∞. For p = ∞, we\ndeﬁne ∥f∥∞:= ∥f∥L∞(Ω) := supx∈Ω|f(x)|. For β ∈Rd\n++, let |β| = Pd\nj=1 |βj|2.\nFor a function f : Rd →R, we deﬁne the rth difference of f in the direction h ∈Rd as\n∆r\nh(f)(x) := ∆r−1\nh\n(f)(x + h) −∆r−1\nh\n(f)(x), ∆0\nh(f)(x) := f(x),\nfor x ∈Ωwith x + rh ∈Ω, otherwise, let ∆r\nh(f)(x) = 0.\nDeﬁnition 1. For a function f ∈Lp(Ω) where p ∈(0, ∞], the r-th modulus of smoothness of f is\ndeﬁned by wr,p(f, t) = suph∈Rd:|hi|≤ti ∥∆r\nh(f)∥p, for t = (t1, . . . , td), ti > 0.\nWith this modulus of smoothness, we deﬁne the anisotropic Besov space Bβ\np,q(Ω) for β =\n(β1, . . . , βd)⊤∈Rd\n++ as follows.\nDeﬁnition 2 (Anisotropic Besov space (Bβ\np,q(Ω))). For 0 < p, q ≤∞, β = (β1, . . . , βd)⊤∈Rd\n++,\nr := maxi⌊βi⌋+ 1, let the seminorm | · |Bα\np,q be\n|f|Bβ\np,q :=\n\n\n\n\n\n\u0012 ∞\nP\nk=0\n[2kwr,p(f, (2−k/β1, . . . , 2−k/βd))]q\n\u00131/q\n(q < ∞),\nsupk≥0 2kwr,p(f, (2−k/β1, . . . , 2−k/βd))\n(q = ∞).\nThe norm of the anisotropic Besov space Bβ\np,q(Ω) is deﬁned by ∥f∥Bβ\np,q := ∥f∥p + |f|Bβ\np,q, and\nBβ\np,q(Ω) = {f ∈Lp(Ω) | ∥f∥Bβ\np,q < ∞}.\nRoughly speaking β represents the smoothness in each direction. If βi is large, then a function in\nBβ\np,q is smooth to the ith coordinate direction, otherwise, it is non-smooth to that direction. p is also\nan important quantity that controls the spatial inhomogeneity of the smoothness. If β1 = β2 = · · · =\nβd, then the deﬁnition is equivalent to the usual Besov space (DeVore & Popov, 1988; DeVore et al.,\n1993). Suzuki (2019) analyzed curse of dimensionality of deep learning through a so-called mixed\nsmooth Besov (m-Besov) space which imposes a stronger condition toward all directions uniformly.\n2We let N := {1, 2, 3, . . . }, Z+ := {0, 1, 2, 3, . . . }, Zd\n+ := {(z1, . . . , zd) | zi ∈Z+}, R+ := {x ≥0 |\nx ∈R}, and R++ := {x > 0 | x ∈R}. We let [N] := {1, . . . , N} for N ∈N.\n3\nParticularly, it imposes stronger smoothness toward non-coordinate axis directions. Moreover, m-\nBesov space does not include the vanilla Besov space as a special case and thus cannot capture the\nsituation that we consider in this paper.\nThroughout this paper, for given β = (β1, . . . , βd)⊤∈Rd\n++, we write β := mini βi (smallest\nsmoothness) and β := maxi βi (largest smoothness). The approximation error of a function in\nanisotropic Besov spaces is characterized by the harmonic mean of (βj)d\nj=1, which corresponds to\nthe average smoothness, and thus we deﬁne\neβ :=\n\u0010Pd\nj=1 1/βj\n\u0011−1\n.\n(1)\nThe Besov space is closely related to other function spaces such as H¨older space. Let ∂αf(x) =\n∂|α|f\n∂α1x1...∂αdxd (x).\nDeﬁnition 3 (H¨older space (Cβ(Ω))). For a smoothness paraemter β ∈R++ with β ̸∈N, con-\nsider an m times differentiable function f : Rd →R where m = ⌊β⌋(the largest integer\nless than β), and let the norm of the H¨older space Cβ(Ω) be ∥f∥Cβ := max|α|≤m\n\r\r∂αf∥∞+\nmax|α|=m supx,y∈Ω\n|∂αf(x)−∂αf(y)|\n∥x−y∥β−m\n. Then, (β-)H¨older space Cβ(Ω) is deﬁned as Cβ(Ω) = {f |\n∥f∥Cβ < ∞}.\nLet C0(Ω) be the set of continuous functions equipped with L∞-norm: C0(Ω) := {f : Ω→R |\nf is continuous and ∥f∥∞< ∞}. These function spaces are closely related to each other.\nProposition 1 (Triebel (2011)). There exist the following relations between the spaces:\n1. For β = (β0, . . . , β0)⊤∈Rd with β0 ̸∈N, it holds that Cβ0(Ω) = Bβ\n∞,∞(Ω).\n2. For 0 < p1, p2, q ≤∞, p1 ≤p2 and β ∈Rd\n++ with eβ > (1/p1 −1/p2)+3, it holds that4\nBβ\np1,q(Ω) ,→Bγβ\np2,q(Ω) for γ = 1 −(1/p1 −1/p2)+/eβ.\n3. For 0 < p, q1, q2 ≤∞, q1 < q2, and β ∈Rd\n++, it holds that Bβ\np,q1 ,→Bβ\np,q2. In particular,\nwith properties 1 and 2, if eβ > 1/p, it holds that Bβ\np,q(Ω) ,→Cγβ(Ω) where γ = 1 −1/(eβp).\n4. For 0 < p, q ≤∞and β ∈Rd\n++, if eβ > 1/p, then Bβ\np,q(Ω) ,→C0(Ω).\nThis result is basically proven by Triebel (2011). For completeness, we provide its derivation in\nAppendix D. If the average smoothness eβ is sufﬁciently large (eβ > 1/p), then the functions in\nBβ\np,q are continuous; however, if it is small (eβ < 1/p), then they are no longer continuous. Small\np indicates spatially inhomogeneous smoothness; thus, spikes and jumps appear (see Donoho &\nJohnstone (1998) for this perspective, from the viewpoint of wavelet analysis).\n2.2\nModel of the true function\nAs a model of the true function f o, we consider two types of models: Afﬁen composition model and\ndeep composition model. For a Banach space H, we let U(H) be the unit ball of H.\n(a) Afﬁne composition model: The ﬁrst model we introduced is a very naive model which is just a\ncomposition of an afﬁne transformation and a function in the anisotropic Besov space:\nHaﬀ:= {h(Ax + b) | h ∈U(Bβ\np,q([0, 1]\n˜d)), A ∈R\n˜d×d, b ∈Rb s.t. Ax + b ∈[0, 1]\n˜d (∀x ∈Ω)},\nwhere we assume ˜d ≤d. Here, we assumed that the afﬁne transformation has an appropriate scaling\nsuch that Ax + b is included in the domain of h for all x ∈Ω. This is a quite naive model but\nprovides an instructive example to understand how the estimation error of deep learning behaves\nunder the anisotropic setting.\n(b) Deep composition model: The deep composition model generalizes the afﬁne composition\nmodel to a composition of nonlinear functions. Let m1 = d, mL+1 = 1, mℓbe the dimension of the\n3Here, we let (x)+ := max{x, 0}.\n4The symbol ,→means continuous embedding.\n4\nSmooth \ndirection\nNon-smooth \ndirection\nFigure 1: Near low dimensional\ndata distribution with anisotropic\nsmoothness of the target func-\ntion. The target function has less\nsmoothness (s1, s2) toward the ﬁrst\ntwo coordinates on the manifold\nwhile it is almost constant toward\nthe third coordinate (large s3).\nℓth layer, and let β(ℓ) ∈Rmℓ\n++ be the smoothness parameter in the ℓth layer. The deep composition\nmodel is deﬁned as\nHdeep := {hH ◦· · · ◦h1(x) | hℓ: [0, 1]mℓ→[0, 1]mℓ+1, hℓ,k ∈U(Bβ(ℓ)\np,q ([0, 1]mℓ)) (∀k ∈[mℓ+1])}.\nHere, the interval [0, 1] can be replaced by another compact interval, such as [aℓ, bℓ], but this dif-\nference can be absorbed by changing a scaling factor. The assumption ∥hℓ,k∥Bβ(ℓ)\np,q\n≤1 can also be\nrelaxed, but we do not pursue that direction due to presentation simplicity. This model includes the\nafﬁne composition model as a special case. However, it requires a stronger assumption to properly\nevaluate the estimation error on this model.\nExamples\nThe model we have introduced includes some instructive examples as listed below:\n(a) Linear projection Schmidt-Hieber (2018) analyzed estimation of the following model by deep\nlearning: f o(x) = g(w⊤x) where g ∈Cβ([0, 1]) and w ∈Rd. In this example, the function f o\nvaries along only one direction, w. Apparently, this is an example of the afﬁne composition model.\n(b) Distribution on low dimensional smooth manifold\nAssume that the input x is distributed\non a low-dimensional smooth manifold embedded in Ω, and the smoothness of the true function\nf o is anisotropic along a coordinate direction on the manifold. We suppose that the low dimen-\nsional manifold is ˜d-dimensional and ˜d ≪d. In this situation, the true function can be written as\nf o(x) = h(φ(x)) where φ : Rd →R ˜d is a map that returns the coordinate of x on the manifold and\nh is an element in an anisotropic Besov space on R ˜d. This situation appears if data is distributed\non a low-dimensional sub-manifold of Ωand the target function is invariant against noise injection\nto some direction on the manifold at each input point x (Figure 1 illustrates this situation). One\ntypical example of this situation is a function invariant with data augmentation (Simard et al., 2003;\nKrizhevsky et al., 2012). Even if the noise injection destroys low dimensionality of the data distribu-\ntion (i.e., ˜d = d), an anisotropic smoothness of the target function eases the curse of dimensionality\nas analyzed below, which is quite different from existing works (Yang & Dunson, 2016; Bickel &\nLi, 2007; Nakada & Imaizumi, 2020; Schmidt-Hieber, 2019; Chen et al., 2019; Chen et al., 2019).\nRelated work\nHere, we introduce some more related work and discuss their relation to our analy-\nsis. The statistical analysis on an anisotropic Besov space can be back to Ibragimov & Khas’minskii\n(1984) who considered density estimation, where the density is assumed to be included in an\nanisotropic Sobolev space with p ≥2, and derived the minimax optimal rate n−reβ/(2eβ+1) with\nrespect to Lr-norm. Nyssbaum (1983, 1987) analyzed a nonparametric regression problem on an\nanisotropic Besov space. Following these results, several studied have been conducted in the liter-\nature pertaining to nonparametric statistics, such as nonlinear kernel estimator Kerkyacharian et al.\n(2001), adaptive conﬁdence band construction Hoffman & Lepski (2002), optimal aggregation Gaif-\nfas & Lecue (2011), Gaussian process estimator Bhattacharya et al. (2011, 2014), and kernel ridge\nregression Hang & Steinwart (2018). Basically, these studies investigated estimation problems in\nwhich the target function is in anisotropic Besov spaces, but the composition models considered in\nthis paper have not been analyzed. Hoffman & Lepski (2002); Bhattacharya et al. (2011) consid-\nered a dimension reduction model; that is, the target function is dependent on only a few variables\nof x, but they did not deal with more general models, such as the afﬁne/deep composition models.\nThe nonparametric regression problems where the input data are distributed on a low-dimensional\nsmooth manifold has been studied as a “manifold regression” Yang & Dunson (2016); Bickel & Li\n5\n(2007); Yang & Tokdar (2015). Such a model can be considered as a speciﬁc example of the deep\ncomposition model. In this sense, our analysis is a signiﬁcant extension of these analyses.\n3\nApproximation error analysis\nHere, we consider approximating the true function f o via a deep neural network and derive the\napproximation error.\nAs the activation function, we consider the ReLU activation denoted by\nη(x) = max{x, 0} (x ∈R). For a vector x, η(x) is operated in an element-wise manner. The\nmodel of neural networks with height L, width W, sparsity constraint S, and norm constraint B as\nΦ(L, W, S, B) := {(W(L)η(·) + b(L)) ◦· · · ◦(W(1)x + b(1)) | W(L) ∈R1×W , b(L) ∈R, W(1) ∈\nRW ×d, b(1) ∈RW , W(ℓ) ∈RW ×W , b(ℓ) ∈RW (1 < ℓ< L), PL\nℓ=1(∥W(ℓ)∥0 + ∥b(ℓ)∥0) ≤\nS, maxℓ∥W(ℓ)∥∞∨∥b(ℓ)∥∞≤B}, where ∥· ∥0 is the ℓ0-norm of the matrix (the number of non-\nzero elements of the matrix), and ∥· ∥∞is the ℓ∞-norm of the matrix (maximum of the absolute\nvalues of the elements). The sparsity constraint and norm bounds are required to obtain the near-\noptimal rate of the estimation error. To evaluate the accuracy of the deep neural network model in\napproximating target functions, we deﬁne the worst-case approximation error as\nRr(F, H) := supf ∗∈H inff∈F ∥f ∗−f∥Lr(Ω),\nwhere F is the set of functions used for approximation, and H is the set of target functions.\nProposition 2 (Approximation ability for anisotropic Besov space). Suppose that 0 < p, q, r ≤∞\nand β ∈Rd\n++ satisfy the following condition: eβ > (1/p −1/r)+. Assume that m ∈N satisﬁes\n0 < β < min(m, m −1 + 1/p). Let δ = (1/p −1/r)+, ν = (eβ −δ)/(2δ) and W0(d) :=\n6dm(m + 2) + 2d. Then, for N ∈N, we can bound the approximation error as\nRr(Φ(L1, W1, S1, B1), U(Bβ\np,q(Ω))) ≲N −eβ,\nby setting\nL1(d) := 3 + 2⌈log2\n\u0010\n3d∨m\nϵc(d,m)\n\u0011\n+ 5⌉⌈log2(d ∨m)⌉, W1(d) := NW0,\n(2)\nS1(d) := [(L −1)W 2\n0 + 1]N, B1(d) := O(N d(1+ν−1)(1/p−eβ)+),\n(3)\nfor ϵ = N −eβ log(N)−1 and a constant c(d,m) depending only on d and m.\nThe proof of this proposition is provided in Appendix B. The rate N −eβ is the optimal adaptive\napproximation error rate that can be achieved by a model with N parameters (the difference be-\ntween adaptive and non-adaptive methods is explained in the discussion below). Note that this is\nan approximation error in an oracle setting and no sample complexity appears here. We notice that\nwe can avoid the curse of dimensionality if the average smoothness eβ is small. This means that\nif the target function is non-smooth in only a few directions and smooth in other directions, we\ncan avoid the curse of dimensionality. In contrast, if we consider an isotropic Besov space where\nβ1 = · · · = βd(= β), then eβ = β/d, which directly depends on the dimensionality d, and we need\nan exponentially large number of parameters in this situation to achieve ϵ-accuracy. Therefore, the\nanisotropic smoothness has a signiﬁcant impact on the approximation error rate. The assumption\neβ > (1/p −1/r)+ ensures the Lr-integrability of the target function, and the inequality (without\nequality) admits a near-optimal wavelet approximation of the target function in terms of Lr-norm.\nUsing this evaluation as a basic tool, we can obtain the approximation error for the deep composition\nmodels. We can also obtain the approximation error for the afﬁne composition models, but it is\nalmost identical to Proposition 2. Therefore, we defer it to Appendix A.\nTheorem 1 (Deep composition model). Assume that eβ(ℓ) > 1/p for all ℓ= 1, . . . , H. Then, the\nestimation error on the deep composition model is bounded as\nR∞(Φ(L, W, S, B), Hdeep) ≲max\nℓ∈[H] N −eβ∗(ℓ),\n(4)\nwhere eβ∗(ℓ) = eβ(ℓ) QH\nk=ℓ+1[(β(k)−1/p)∧1], and L = PH\nℓ=1(L1(mℓ)+1), W = maxℓ(W1(mℓ)∨\nmℓ+1), S = PH\nℓ=1(S1(mℓ) + 3mℓ+1), B = maxℓB1(mℓ).\n6\nThe proof can be found in Appendix B.1.\nSince the model is more general than the vanilla\nanisotropic Besov space, we require a stronger assumption eβ(ℓ) > 1/p on eβ(ℓ) than the condi-\ntion in Proposition 2. This is because we need to bound the H¨older smoothness of the remaining\nlayers to bound the inﬂuence of the approximation error in the internal layers to the entire function.\nH¨older smoothness is ensured according to the embedding property under this condition (Proposi-\ntion 1). This H¨older smoothness assumption affects the approximation error rate. The convergence\nrate eβ∗(ℓ) in Eq. (4) is different from that in Eq. (8). This is because the approximation error\nin the internal layers are propagated through the remaining layers with H¨older smoothness and its\namplitude is controlled by the H¨older smoothness.\nApproximation error by non-adaptive method\nThe approximation error obtained in the\nprevious section is called an adaptive error rate in the literature regarding approximation theory\n(DeVore, 1998). If we ﬁx N bases beforehand and approximate the target function by a linear\ncombination of the N bases (which is called the non-adaptive method), then we cannot achieve the\nadaptive error rate obtained in the previous section. Roughly speaking, the approximation error of\nnon-adaptive methods is lower bounded by N −(eβ−( 1\np −\n1\nmin{2,r} )+) (Myronyuk, 2015, 2016, 2017),\nwhich is slower than the approximation error rate of deep neural networks especially for small p.\n4\nEstimation error analysis\nIn this section, we analyze the accuracy of deep learning in estimating a function in compositions of\nanisotropic Besov spaces. We consider a least-squares estimator in the deep neural network model:\nbf = argmin ¯\nf:f∈Φ(L,W,S,B)\nPn\ni=1(yi −¯f(xi))2\n(5)\nwhere ¯f is the clipping of f deﬁned by ¯f = min{max{f, −F}, F} for a constant F > 0 which is\nrealized by ReLU units. The network parameters (L, W, S, B) should be speciﬁed appropriately as\nindicated in Theorems 2 and 3. In practice, these parameters can be speciﬁed by cross validation.\nIndeed, we can theoretically show that cross validation can provide the appropriate choice of these\nparameters in compensation to an additional log(n)-factor in the estimation error bound. This esti-\nmator can be seen as a sparsely regularized estimator because there are constraints on S. In terms\nof optimization, this requires a combinatorial optimization, but we do not pursue the computational\naspect. The estimation error that we derive in this section can involve the optimization error, but\nfor simplicity, we only demonstrate the estimation error of the ideal situation where there is no\noptimization error.\nAfﬁne composition model\nThe following theorem provides an upper bound of the estimation error\nfor the afﬁne composition model.\nTheorem 2. Assume the same condition as in Theorem 6; in particular, suppose 0 < p, q ≤∞\nand eβ > (1/p −1/2)+ for eβ ∈R ˜d\n++. Moreover, we assume that the distribution PX has a density\npX such that ∥pX∥∞≤R for a constant R > 0. If f o ∈Haﬀ∩L∞(Ω), and ∥f o∥∞≤F for\nF ≥1; then, letting (W, L, S, B) = (L1( ˜d), W1( ˜d), S1( ˜d), ( ˜dC + 1)B1( ˜d)) as in Theorem 6 with\nN ≍n\n1\n2 e\nβ+1 , we obtain\nEDn[∥f o −bf∥2\nL2(PX)] ≲n\n−\n2 e\nβ\n2 e\nβ+1 log(n)3,\nwhere EDn[·] indicates the expectation with respect to the training data Dn.\nThe proof is provided in Appendix C. We will show that the convergence rate n−2eβ/(2eβ+1) is min-\nimax optimal in Section 5 (see also Kerkyacharian & Picard (1992); Donoho et al. (1996); Donoho\n& Johnstone (1998); Gin´e & Nickl (2015) for ordinary Besov spaces). The L∞-norm constraint\n∥f o∥∞≤F is used to derive a uniform bound on the discrepancy between the population and the\nempirical L2-norm. Without this condition, the convergence rate could be slower.\nDeep composition model\nFor the deep composition model, we obtain the following convergence\nrate. This is an extension of Theorem 2 but requires a stronger assumption on the smoothness.\nTheorem 3. Suppose that 0 < p, q ≤∞and eβ(ℓ) > 1/p for all ℓ∈[H]. If f o ∈Hdeep ∩L∞(Ω),\nand ∥f∥∞≤F for F ≥1, then we obtain\nEDn[∥f o −bf∥2\nL2(PX)] ≲maxℓ∈[H] n−2eβ∗(ℓ)/(2eβ∗(ℓ)+1) log(n)3,\n7\nwhere eβ∗(ℓ) is deﬁned in Theorem 1, and (L, W, S, B) is as given in Theorem 1 with N ≍\nmaxℓ∈[L] n\n1\n2 e\nβ∗(ℓ)+1 .\nThe proof is provided in Appendix C. We will show that this is also minimax optimal in Theorem 4.\nBecause of the H¨older continuity, the convergence rate becomes slower than the afﬁne composition\nmodel (that is, eβ∗(ℓ) ≤eβ(ℓ)). However, this slower rate is unavoidable in terms of the minimax\noptimal rate. Schmidt-Hieber (2018) analyzed the same situation for the H¨older class which corre-\nsponds to β(ℓ)\n1\n= · · · = β(ℓ)\nd\n(∀ℓ) and p = q = ∞. Our analysis far extends their analysis to the\nsetting of anisotropic Besov spaces in which the parameters β(ℓ), p, q have much more freedom.\nFrom these two bounds (Theorems 2 and 3), we can see that as the smoothness eβ becomes large,\nthe convergence rates faster. If the target function is included in the isotropic Besov space with\nsmoothness β1 = · · · = βd(= β), then the estimation error becomes\n(Isotropic Besov)\nn−2β/(2β+d).\nIn the exponent, the dimensionality d appears, which causes the curse of dimensionality. In contrast,\nif the target function is in the anisotropic Besov space, and the smoothness in each direction is\nsufﬁciently imbalanced such that eβ does not depend on d, our obtained rate\n(Anisotropic Besov)\nn−2eβ/(2eβ+1)\navoids the curse of dimensionality. For high-dimensional settings, there would be several redundant\ndirections in which the true function does not change. Deep learning is adaptive to this redundancy\nand achieves a better estimation error. However, in Section 6, we prove that linear estimators are\naffected by the dimensionality more strongly than deep learning. This indicates the superiority of\ndeep learning.\n5\nMinimax optimal rate\nHere, we show that the estimation error rate, that we have presented, of deep learning achieves\nthe minimax optimal rate. Roughly speaking the minimax optimal risk on a model F◦of the true\nfunction is the smallest worst case error over all estimators:\nR∗(F◦) := inf b\nf supf o∈F◦EDn[∥bf −f o∥2\nL2(PX)],\nwhere bf runs over all estimators. The convergence rate of the minimax optimal risk is referred to as\nminimax optimal rate. We obtain the following minimax optimal rate for anisotropic Besov spaces.\nTheorem 4. (a) Afﬁne composition model: For 0 < p, q ≤∞and β ∈Rd\n++, assume that\neβ > max {1/p −1/2, 1/p −1, 0} . Then, the minimax optimal risk of the afﬁne composition model\nis lower bounded as R∗(Haﬀ) ≳n\n−\n2 e\nβ\n2 e\nβ+1 . (b) Deep composition model: For 0 < p, q ≤∞and\nβ(ℓ) ∈Rd\n++ (ℓ= 1, . . . , H), assume that eβ(ℓ) > 1/p. Let ϵ > 0 be arbitrarily small for q < ∞, and\nlet ϵ = 0 for q = 0. Let eβ∗(ℓ) = eβ(ℓ) QH\nk=ℓ+1[(β(k)−1/p+ϵ)∧1], and eβ∗∗:= minℓeβ∗(ℓ). Then, the\nminimax optimal risk of the deep composition model is lower bounded as R∗(Hdeep) ≳n\n−\n2 e\nβ∗∗\n2 e\nβ∗∗+1 .\nThe proof is provided in Appendix E (see also Ibragimov & Khas’minskii (1984); Nyssbaum\n(1987)). From this theorem, we can see that the estimation error of deep learning shown in The-\norems 2 and 3 indeed achieve the minimax optimal rate up to a poly-log(n) factor.\n6\nSuboptimality of linear estimators\nIn this section, we give the minimax optimal rate in the class of linear estimators. The linear\nestimator is a class of estimators that can be written as\nbf(x) = Pn\ni=1 yiϕi(x; Xn),\nwhere Xn = (x1, . . . , xn) and ϕi(x; Xn) (i = 1, . . . , n) are (measurable) functions that only\ndepend on x and Xn. This is linearly dependent on Y n = (y1, . . . , yn). We notice that the kernel\n8\nridge regression is included in this class because it can be written as bf(x) = kx,Xn(kXn,Xn +\nλI)−1Y n, which linearly depends on Y n. This class includes other important estimators, such as the\nNadaraya–Watson estimator, the k-nearest neighbor estimator, and the sieve estimator. We compare\ndeep learning with the linear estimators in terms of minimax risk. For this purpose, we deﬁne the\nminimax risk of the class of linear estimators:\nR(lin)\n∗\n(F◦) :=\ninf\nb\nf: linear\nsup\nf o∈F◦EDn[∥f o −bf∥2\nL2(PX)],\nwhere bf runs over all linear estimators. We can see that linear estimators suffer from the sub-optimal\nrate because of the following two points: (i) they do not have adaptivity, and (ii) they signiﬁcantly\nsuffer from the curse of dimensionality.\nTheorem 5. (i) Suppose that the input distribution PX is uniform distribution on Ω= [0, 1]d and\neβ > 1/p, 1 ≤p, q ≤∞. Then, the minimax optimal rate of the linear estimators is lower bounded\nas\nR(lin)\n∗\n(U(Bβ\np,q)) ≳n\n−\n2 e\nβ−v\n2 e\nβ+1−v ,\n(6)\nwhere v = 2(1/p −1/2)+. (ii) In addition to the above conditions, we assume that ˜d ≤d, β =\nβ1 = · · · = β ˜d and 0 < p ≤2. Let ad = 1 + κ (with arbitrary small κ > 0) when ˜d < d/2 and\nad = 0 when ˜d ≥d/2. Then, the minimax rate of the linear estimators on the afﬁne composition\nmodel is lower bounded by\nR(lin)\n∗\n(Haﬀ) ≳n\n−\n2(β−˜\nd/p+d/2+ad)\n2(β−˜\nd/p+d/2+ad)+d .\n(7)\nThe proof is provided in Appendix F. (i) The lower bound (7) reveals the suboptimality of linear\nestimators in terms of input dimensionality. Actually, if we consider a particular case where ˜d = 1,\np = 1 and d ≫˜d, then the obtained minimax rate of linear estimators and the estimation error rate\nof deep learning can be summarized as\nlinear : n\n−\n2β+d\n2β+2d ,\ndeep : n\n−\n2β\n2β+1 ,\nby Theorem 2 when β > 1 (which can be checked by noticing ˜d = β/eβ = 1 in this situation). We\ncan see that the dependence on the dimensionality of linear estimators is signiﬁcantly worse than that\nof deep leaning. This indicates poor adaptivity of linear estimators to the intrinsic dimensionality\nof data. Actually, as d becomes large, the rate for the linear estimator approaches to 1/√n but\nthat for the deep learning is not affected by d and still faster than 1/√n. To show the theorem, we\nused the “convex-hull argument” developed by Hayakawa & Suzuki (2019); Donoho & Johnstone\n(1998). We combined this technique with the so-called Irie-Miyake’s integral representation (Irie\n& Miyake, 1988; Hornik et al., 1990). Note that this difference appears because there is an afﬁne\ntransformation in the ﬁrst layer of the afﬁne composition model. Deep learning is ﬂexible against\nsuch a coordinate transform so that it can ﬁnd directions to which the target function is smooth. In\ncontrast, kernel methods do not have such adaptivity because there is no feature extraction layer.\n(ii) The lower bound (6) states that when p < 2 (that is, v > 0), the minimax rate of the linear\nestimators is outperformed by that of deep learning (Theorem 2). This is due to the “adaptivity” of\ndeep leaning. When p is small, the smoothness of the target function is less homogeneous, and it\nrequires an adaptive approximation scheme to achieve the best estimation error. Linear estimators\ndo not have adaptivity and thus fail to achieve the minimax optimal rate. Our bound (6) extends\nthe result by Zhang et al. (2002) to a multivariate anisotropic Besov space while Zhang et al. (2002)\ninvestigated the univariate space (d = 1).\n7\nConclusion\nWe investigated the approximation error and estimation error of deep learning in the anisotropic\nBesov spaces. It was proved that the convergence rate is determined by the average of the anisotropic\nsmoothness, which results in milder dependence on the input dimensionality. If the smoothness is\nhighly anisotropic, deep learning can avoid overﬁtting. We also compared the error rate of deep\nlearning with that of linear estimators and showed that deep learning has better dependence on\n9\nthe input dimensionality. Moreover, it was shown that deep learning can achieve the adaptive rate\nand outperform non-adaptive approximation methods and linear estimators if the homogeneity p of\nsmoothness is small. These analyses strongly support the practical success of deep learning from a\ntheoretical perspective.\nLimitations of this work\nOur work does not cover the optimization aspect of deep learning. It\nis assumed that the regularized least squares (5) can be executed. It would be nice to combine\nour study with recent developments of non-convex optimization techniques (Vempala & Wibisono,\n2019; Suzuki & Akiyama, 2021).\nPotential negative societal impact\nSince this is purely theoretical result, it is not expected that\nthere is a direct negative societal impact. However, revealing detailed properties of the deep learning\ncould promote an opportunity to pervert deep learning.\nAcknowledgment\nTS was partially supported by JSPS KAKENHI (18H03201), Japan Digital Design and JST CREST.\nAN was partially supported by JSPS Kakenhi (19K20337) and JST-PRESTO.\nReferences\nM. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.\nNeural computation, 15(6):1373–1396, 2003.\nA. Bhattacharya, D. Pati, and D. B. Dunson. Adaptive dimension reduction with a gaussian process\nprior. arXiv preprint arXiv:1111.1044, 1445, 2011.\nA. Bhattacharya, D. Pati, and D. Dunson. Anisotropic function estimation using multi-bandwidth\ngaussian processes. Annals of statistics, 42(1):352, 2014.\nP. J. Bickel and B. Li. Local polynomial regression on unknown manifolds. In Complex datasets\nand inverse problems, pp. 177–186. Institute of Mathematical Statistics, 2007.\nM. Chen, H. Jiang, W. Liao, and T. Zhao. Nonparametric Regression on Low-Dimensional Mani-\nfolds using Deep ReLU Networks. arXiv e-prints, art. arXiv:1908.01842, Aug 2019.\nM. Chen, H. Jiang, W. Liao, and T. Zhao. Efﬁcient approximation of deep relu networks for functions\non low dimensional manifolds. In Advances in Neural Information Processing Systems, pp. 8172–\n8182, 2019.\nG. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,\nSignals, and Systems (MCSS), 2(4):303–314, 1989.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. arXiv e-prints, art. arXiv:1810.04805, Oct 2018.\nR. A. DeVore. Nonlinear approximation. Acta Numerica, 7:51–150, 1998.\nR. A. DeVore and V. A. Popov.\nInterpolation of Besov spaces.\nTransactions of the American\nMathematical Society, 305(1):397–414, 1988.\nR. A. DeVore, G. Kyriazis, D. Leviatan, and V. M. Tikhomirov.\nWavelet compression and\nnonlinearn-widths. Advances in Computational Mathematics, 1(2):197–214, 1993.\nD. L. Donoho and I. M. Johnstone. Minimax estimation via wavelet shrinkage. The Annals of\nStatistics, 26(3):879–921, 1998.\nD. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Density estimation by wavelet\nthresholding. The Annals of Statistics, 24(2):508–539, 1996.\nD. D˜ung. B-spline quasi-interpolant representations and sampling recovery of functions with mixed\nsmoothness. Journal of Complexity, 27(6):541–567, 2011a.\n10\nD. D˜ung. Optimal adaptive sampling recovery. Advances in Computational Mathematics, 34(1):\n1–41, 2011b.\nS. Gaiffas and G. Lecue. Hyper-sparse optimal aggregation. Journal of Machine Learning Research,\n12(Jun):1813–1833, 2011.\nE. Gin´e and R. Nickl. Mathematical Foundations of Inﬁnite-Dimensional Statistical Models. Cam-\nbridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2015.\nX. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectiﬁer neural networks. In Proceedings of the\n14th International Conference on Artiﬁcial Intelligence and Statistics, volume 15 of Proceedings\nof Machine Learning Research, pp. 315–323, 2011.\nH. Hang and I. Steinwart.\nOptimal learning with anisotropic gaussian svms.\narXiv preprint\narXiv:1810.02321, 2018.\nS. Hayakawa and T. Suzuki. On the minimax optimality and superiority of deep neural network\nlearning over sparse parameter spaces. arXiv preprint arXiv:1905.09195, 2019.\nM. Hoffman and O. Lepski. Random rates in anisotropic regression (with a discussion and a rejoin-\nder by the authors). The Annals of Statistics, 30(2):325–396, 04 2002.\nK. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2):\n251–257, 1991.\nK. Hornik, M. Stinchcombe, and H. White. Universal approximation of an unknown mapping and\nits derivatives using multilayer feedforward networks. Neural Networks, 3(5):551–560, 1990.\nI. Ibragimov and R. Khas’minskii. More on the estimation of distribution densities. Journal of Soviet\nMathematics, 25(3):1155–1165, 1984.\nB. Irie and S. Miyake. Capabilities of three-layered perceptrons. In IEEE 1988 International Con-\nference on Neural Networks, pp. 641–648, 1988.\nG. Kerkyacharian and D. Picard. Density estimation in Besov spaces. Statistics & Probability\nLetters, 13:15–24, 1992.\nG. Kerkyacharian, O. Lepski, and D. Picard. Nonlinear estimation in anisotropic multi-index de-\nnoising. Probability Theory and Related Fields, 121(2):137–170, Oct 2001.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.\nC. Leisner. Nonlinear wavelet approximation in anisotropic besov spaces. Indiana University math-\nematics journal, pp. 437–455, 2003.\nS. Mallat. A Wavelet Tour of Signal Processing. Academic Press, 1999.\nV. Myronyuk. Trigonometric approximations and kolmogorov widths of anisotropic besov classes\nof periodic functions of several variables. Ukrainian Mathematical Journal, 66(8), 2015.\nV. V. Myronyuk. Kolmogorov widths of the anisotropic besov classes of periodic functions of many\nvariables. Ukrainian Mathematical Journal, 68(5):718–727, Oct 2016.\nV. V. Myronyuk. Widths of the anisotropic besov classes of periodic functions of several variables.\nUkrainian Mathematical Journal, 68(8):1238–1251, Jan 2017. ISSN 1573-9376. doi: 10.1007/\ns11253-017-1290-1. URL https://doi.org/10.1007/s11253-017-1290-1.\nV. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Proceed-\nings of the 27th International Conference on Machine Learning, pp. 807–814, 2010.\nR. Nakada and M. Imaizumi. Adaptive approximation and generalization of deep neural network\nwith intrinsic dimensionality. Journal of Machine Learning Research, 21(174):1–38, 2020. URL\nhttp://jmlr.org/papers/v21/20-002.html.\n11\nS. M. Nikol’skii. Approximation of functions of several variables and imbedding theorems, volume\n205. Springer-Verlag Berlin Heidelberg, 1975.\nM. Nyssbaum. Optimal ﬁltration of a function of many variables in white gaussian noise. Problems\nof Information Transmission, 19:23–29, 1983.\nM. Nyssbaum. Nonparametric estimation of a regression function that is smooth in a domain in Rk.\nTheory of Probability & Its Applications, 31(1):108–115, 1987.\nA. Radford, L. Metz, and S. Chintala. Unsupervised Representation Learning with Deep Convolu-\ntional Generative Adversarial Networks. arXiv e-prints, art. arXiv:1511.06434, Nov 2015.\nG. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over\nkernel classes via convex programming. The Journal of Machine Learning Research, 13(1):389–\n427, 2012.\nJ. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation\nfunction. ArXiv preprint arXiv:1708.06633(v3), 2018.\nJ. Schmidt-Hieber. Deep ReLU network approximation of functions on a manifold. arXiv preprint\narXiv:1908.00695, 2019.\nP. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for convolutional neural networks applied\nto visual document analysis. In Proceedings of the Seventh International Conference on Document\nAnalysis and Recognition-Volume 2, pp. 958. IEEE Computer Society, 2003.\nS. Sonoda and N. Murata. Neural network with unbounded activation functions is universal approx-\nimator. Applied and Computational Harmonic Analysis, 43(2):233–268, 2017.\nT. Suzuki. Fast generalization error bound of deep learning from a kernel perspective. In Proceedings\nof the Twenty-First International Conference on Artiﬁcial Intelligence and Statistics, volume 84\nof Proceedings of Machine Learning Research, pp. 1397–1406. PMLR, 2018.\nT. Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces:\noptimal rate and curse of dimensionality. In International Conference on Learning Representa-\ntions (ICLR2019), 2019. URL https://openreview.net/forum?id=H1ebTsActm.\nT. Suzuki and S. Akiyama. Beneﬁt of deep learning with non-convex noisy gradient descent: Prov-\nable excess risk bound and superiority to kernel methods. In International Conference on Learn-\ning Representations, 2021. URL https://openreview.net/forum?id=2m0g1wEafh.\nV. Temlyakov. Approximation of Periodic Functions. Nova Science Publishers, 1993.\nJ. B. Tenenbaum, V. De Silva, and J. C. Langford. A global geometric framework for nonlinear\ndimensionality reduction. science, 290(5500):2319–2323, 2000.\nH. Triebel. Entropy numbers in function spaces with mixed integrability. Revista matem´atica com-\nplutense, 24(1):169–188, 2011.\nA. B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics. Springer\nNew York, 2008.\nA. W. van der Vaart and J. A. Wellner. Weak Convergence and Empirical Processes: With Applica-\ntions to Statistics. Springer, New York, 1996.\nS. Vempala and A. Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry\nsufﬁces. In Advances in Neural Information Processing Systems, pp. 8094–8106, 2019.\nJ. Vybiral. Function spaces with dominating mixed smoothness. Dissertationes Math. (Rozprawy\nMat.), 436:3–73, 2006.\nY. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. The\nAnnals of Statistics, 27(5):1564–1599, 1999.\n12\nY. Yang and D. B. Dunson. Bayesian manifold regression. The Annals of Statistics, 44(2):876–905,\n2016.\nY. Yang and S. T. Tokdar. Minimax-optimal nonparametric regression in high dimensions. The\nAnnals of Statistics, 43(2):652–674, 2015.\nD. Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:\n103–114, 2017.\nS. Zhang, M.-Y. Wong, and Z. Zheng. Wavelet threshold estimation of a regression function with\nrandom design. Journal of Multivariate Analysis, 80(2):256–284, 2002.\n13\n——Appendix——\nA\nApproximation error of Afﬁne composition model\nTheorem 6 (Afﬁne composition model). Assume that the distribution of ˜x = Ax + b ∈R ˜d has a\nbounded density function on [0, 1] ˜d when x obeys the uniform distribution on Ω, and each element\nin A and b is bounded by a constant C. Assume that 0 < p, q, r ≤∞and β ∈R ˜d\n++ satisfy\neβ > (1/p −1/r)+. Then, it holds that\nRr(Φ(L1( ˜d), W1( ˜d), S1( ˜d), ( ˜dC + 1)B1( ˜d)), Haﬀ) ≲N −eβ,\n(8)\nwhere L1(·), W1(·), S1(·), B1(·) are deﬁned in Eq. (3).\nThe assumption eβ > (1/p −1/r)+ ensures the Lr-integrability of the target function, and the\ninequality (without equality) admits a near-optimal wavelet approximation of the target function in\nterms of Lr-norm. From this theorem, the approximation error is almost identical to that for Bβ\np,q(Ω)\n(Proposition 2).\nB\nProofs of approximation error bounds\nTo show the approximation accuracy, a key step is to show that the ReLU neural network can ap-\nproximate the cardinal B-spline with high accuracy. Let N(x) = 1 (x ∈[0, 1]), 0 (otherwise), then\nthe cardinal B-spline of order m is deﬁned by taking m + 1-times convolution of N:\nNm(x) = (N ∗N ∗· · · ∗N\n|\n{z\n}\nm + 1 times\n)(x),\nwhere f ∗g(x) :=\nR\nf(x −t)g(t)dt. It is known that Nm is a piece-wise polynomial of order m.\nFor k ∈Zd\n+ and j = (j1, . . . , jd) ∈Zd\n+, let\nM d\nk,j(x) =\nd\nY\ni=1\nNm(2⌊kβ′\ni⌋xi −ji),\nwhere β ∈Rd\n++ is a given smoothness parameter (we omit the dependency on β from the notation\nwhich would be obvious from the context). Here, k controls the spatial “resolution” and j speciﬁes\nthe location on which the basis is put. Basically, we approximate a function f in an anisotropic\nBesov space via super-position of M m\nk,j(x), which is closely related to wavelet analysis (Mallat,\n1999). The following is a key lemma that was proven by Suzuki (2019).\nLemma 1 (Approximation of cardinal B-spline basis by the ReLU activation). There exists a con-\nstant c(d,m) depending only on d and m such that, for all ϵ > 0, there exists a neural network ˇ\nM ∈\nΦ(L0, W0, S0, B0) with L0 := 3+2\nl\nlog2\n\u0010\n3d∨m\nϵc(d,m)\n\u0011\n+ 5\nm\n⌈log2(d ∨m)⌉, W0 := 6dm(m+2)+2d,\nS0 := L0W 2\n0 and B0 := 2(m + 1)m that satisﬁes\n∥M d\n0,0 −ˇ\nM∥L∞(Rd) ≤ϵ,\nand ˇ\nM(x) = 0 for all x ̸∈[0, m + 1]d.\nLet\n∥k∥β/β :=\nd\nX\nj=1\n⌊kβ/βj⌋\nfor a k ∈Z. For order m ∈N of the cardinal B-spline bases, let\nJi(k) = {−m, −m + 1, . . . , 2⌊kβ′\ni⌋−1, 2⌊kβ′\ni⌋}\nand\nJ(k) := J1(k) × J2(k) × · · · × Jd(k).\n14\nand the quasi-norm of the coefﬁcient (αk,j)k,j for k ∈Z+ and j ∈J(k) be\n∥(αk,j)k,j∥bβ\np,q. =\n\n\n\n∞\nX\nk=0\n\n2k[β−(Pd\ni=1⌊kβ′\ni⌋/k)/p]\u0010 X\nj∈J(k)\n|αk,j|p\u00111/p\n\n\nq\n\n\n1/q\n.\nFor p = ∞or q = ∞, the deﬁnition should be appropriately modiﬁed as usual.\nLemma 2. Assume the condition eβ > (1/p−1/r)+ in Proposition 2 and 0 < β < min(m, m−1+\n1/p) where m ∈N is the order of the cardinal B-spline bases. Then, f ∈Bβ\np,q admits the following\ndecomposition:\nf =\n∞\nX\nk=0\nX\nj∈J(k)\nαk,jM d\nk,j(x)\n(9)\nwith convergence in the sense of Lp, and the coefﬁcient (αk,j) yields the following norm equivalence\n∥f∥Bβ\np,q ≃∥(αk,j)k,j∥bβ\np,q.\n(10)\nFor an integer K ∈N, let N = ⌈2∥K∥β/β⌉, then for any f ∈Bβ\np,q(Ω), there exists fN that satisﬁes\n∥f −fN∥Lr(Ω) ≲N −eβ∥f∥Bβ\np,q,\nand has the following form:\nfN(x) =\nK\nX\nk=0\nX\nj∈J(k)\nαk,jM d\nk,j(x) +\nK∗\nX\nk=K+1\nnk\nX\ni=1\nαk,jiM d\nk,ji(x),\n(11)\nwhere K∗= ⌈K(1 + 1/ν)⌉, nk = ⌈2∥K∥β/β−ϵ(∥k∥β/β−∥K∥β/β)⌉(k = K + 1, . . . , K∗) for δ =\n(1/p −1/r)+ and ν = (eβ −δ)/(2δ), and (ji)nk\ni=1 ⊂J(k).\nProof of Lemma 2. Leisner (2003) showed that there exists a bounded linear operator Pk that can\nbe expressed as\nPk(f)(x) =\nX\nj∈J(k)\nak,jM d\nk,j(x)\n(12)\nwhere αk,j is constructed in a certain way, and for every f ∈Lp([0, 1]d) with 0 < p ≤∞, it holds\n∥f −Pk(f)∥Lp ≤Cwr,p(f, (2−kβ′\n1, . . . , 2−kβ′\nd)),\n(See Theorem 3.2.4 of Leisner (2003) and DeVore & Popov (1988)). Let\npk(f) := Pk(f) −Pk−1(f), P−1(f) = 0.\nThen, Leisner (2003) showed that when 0 < p, q ≤∞and 0 < β < min(m, m −1 + 1/p), f\nbelongs to Bβ\np,q if and only if f can be decomposed into\nf =\n∞\nX\nk=0\npk(f),\nwith the convergence condition\n∥(pk(f))∞\nk=0∥bβ\nq (Lp) :=\n\" ∞\nX\nk=0\n(2βk∥pk∥Lp)q\n#1/q\n< ∞.\nIn particular, it is shown that\n∥f∥Bsp,q ≃∥(pk(f))∞\nk=0∥bsp(Lp).\n(13)\n15\nHere, each pk can be expressed as pk(x) = P\nj∈J(k) αk,jM d\nk,j(x) for a coefﬁcient (αk,j)k,j which\ncould be different from (ak,j)k,j appearing in Eq. (12), and thus f ∈Bβ\np,q can be decomposed into\nf =\n∞\nX\nk=0\nX\nj∈J(k)\nαk,jM d\nk,j(x)\nwith\nconvergence\nin\nthe\nsense\nof\nLp.\nMoreover,\nit\nis\nshown\nthat\n∥pk∥Lp\n≃\n(2−kd P\nj∈J(k) |αk,j|p)1/p and thus\n∥f∥Bβ\np,q ≃∥(αk,j)k,j∥bβ\np,q.\nThis yields the ﬁrst assertion.\nNext, we move to the second assertion. If p ≥r, the assertion can be shown in the same manner\nas Theorem 3.1 of D˜ung (2011a). More precisely, we can show the assertion in a similar line to the\nfollowing proof for p < r by setting K = K∗. Thus, we show the assertion only for p < r. In this\nregime, we need to use an adaptive approximation method. In the following, we assume p < r. For\na given K, by appropriately choosing K∗later, we set\nRK(f)(x) =\nX\n0≤k≤K\npk +\nX\nk∈Z+:K<k≤K∗\nGk(pk),\nwhere Gk(pk) is given as\nGk(pk) =\nX\n1≤i≤nk\nαk,jiM d\nk,ji(x)\nwhere (αk,ji)|J(k)|\ni=1\nis the sorted coefﬁcients in decreasing order of absolute value: |αk,j1| ≥\n|αk,j2| ≥· · · ≥|αk,j|J(k)||. Then, it holds that\n∥pk −Gk(pk)∥r ≤∥pk∥p2δ∥k∥β/βn−δ\nk ,\nwhere δ := (1/p −1/r) (see the proof of Theorem 3.1 of D˜ung (2011b) and Lemma 5.3 of D˜ung\n(2011a)). Moreover, we also have\n∥pk∥r ≤∥pk∥p2δ∥k∥β/β\nfor k ∈Z+ with k > K∗.\nHere, we deﬁne N as\nN = ⌈2∥K∥β/β⌉.\nLet ν = (eβ −δ)/(2δ),\nK∗= ⌈K(1 + 1/ν)⌉,\nand\nnk =\nl\n2∥K∥β/β−ϵ(∥k∥β/β−∥K∥β/β)m\nfor k ∈Z+ with K + 1 ≤k ≤K∗.\nThen, by Lemma 5.3 of D˜ung (2011a), we have\n∥f −RK(f)∥r\nLr ≲\nX\nK<k≤K∗\n∥pk −Gk(pk)∥r\nLr +\nX\nK∗<k\n∥pk∥r\nLr\n≲\nX\nK<k≤K∗\n[∥pk∥p2δ∥k∥β/βn−δ\nk ]r +\nX\nK∗<k\n[2δ∥k∥β/β∥pk∥Lp]r.\n(14)\n(a) Suppose that q ≤r and r < ∞. Then,\n∥f −RK(f)∥q\nLr = ∥f −RK(f)∥\nr q\nr\nLr\n≲\n\n\n\nX\nK<∥k∥1≤K∗\n[2δ∥k∥β/βn−δ\nk ∥pk∥Lp]r +\nX\nK∗<k\n[2δ∥k∥β/β∥pk∥Lp]r\n\n\n\nq\nr\n(∵Eq. (14))\n16\n≲\nX\nK<k≤K∗\n[2δ∥k∥β/βn−δ\nk ∥pk∥Lp]q +\nX\nK∗<k\n[2δ∥k∥β/β∥pk∥Lp]q\n≤N −δq2−(eβ−δ)∥K∥β/βq\nX\nK<k≤K∗\n[2−(eβ−δ−δϵ)(∥k∥β/β−∥K∥β/β)\n|\n{z\n}\n≤1\n2\neβ∥k∥β/β∥pk∥Lp]q\n+ 2−q(eβ−δ)∥K∗∥β/β X\nK∗<k\n[2\neβ∥k∥β/β∥pk∥Lp]q\n(i)\n≲(N −δ2−(eβ−δ)∥K∥β/β + 2−(eβ−δ)K∗)q∥f∥q\nMBsp,q\n(∵Eq. (13))\n(ii)\n≲(N −eβ)q∥f∥q\nMBα\np,q\nwhere we used 2\neβ∥k∥β/β ≃2βk in (i), and N ≃2∥K∥β/β and ν = (eβ −δ)/(2δ) in (ii).\n(b) Suppose that q > r and r < ∞. Then, letting γ = q/r (> 1) and γ′ = 1/(1 −1/γ) = q/(q −r)\n(note that 1\nγ + 1\nγ′ = 1), we have\n∥f −RK(f)∥r\nLr ≲\nX\nK<k≤K∗\n[2δ∥k∥β/βn−δ\nk ∥pk∥Lp]r +\nX\nK∗<k\n[2δ∥k∥β/β∥pk∥Lp]r\n(∵Eq. (14))\n≤2−eβ∥K∥β/βr\nX\nK<k≤K∗\n[2−(eβ−δ−δν)(∥k∥β/β−∥K∥β/β)2\neβ∥k∥β/β∥pk∥Lp]r\n+\nX\nK∗<k\n[2\neβ∥k∥β/β∥pk∥Lp]r(2−(eβ−δ)∥k∥β/β)r\n≤(2−eβ∥K∥β/β + 2−(eβ−δ)∥K∗∥β/β)rn\nX\nK<k≤K∗\n[2−(eβ−δ−δν)(∥k∥β/β−∥K∥β/β)2\neβ∥k∥β/β∥pk∥Lp]r\n+\nX\nK∗<k\n[2\neβ∥k∥β/β∥pk∥Lp]r2−(eβ−δ)(∥k∥β/β−∥K∗∥β/β)ro\n≤(2−eβ∥K∥β/βr + 2−(eβ−δ)∥K∗∥β/β)r\n\n\n\nX\nK<k≤K∗\n[2\neβ∥k∥β/β∥pk∥Lp]rγ +\nX\nK∗<k\n[2\neβ∥k∥β/β∥pk∥Lp]rγ\n\n\n\n1/γ\n×\n\n\n\nX\nK<k≤K∗\n[2−(eβ−δ−δν)(∥k∥β/β−∥K∥β/β)]rγ′ +\nX\nK∗<k\n[2−(s−δ)(∥k∥β/β−K∗)]rγ′\n\n\n\n1/γ′\n≲(2−eβ∥K∥β/β + 2−(eβ−δ)∥K∗∥β/β)r∥f∥r\nBβ\np,q\n(∵Eq. (13) and 2\neβ∥k∥β/β ≃2βk )\n≲(N −eβ)r∥f∥r\nBβ\np,q.\n(c) Suppose that r = ∞. Then, similarly to the analysis in (b), we can evaluate\n∥f −RK(f)∥Lr\n≲2−eβ∥K∥β/β\nX\nK<k≤K∗\n[2−(eβ−δ−δϵ)(∥k∥β/β−∥K∥β/β)2\neβ∥k∥β/β∥pk∥Lp]\n+\nX\nK∗<k\n[2\neβ∥k∥β/β∥pk∥Lp](2−(eβ−δ)∥k∥β/β)\n≲(2−eβ∥K∥β/β + 2−(eβ−δ)∥K∗∥β/β)∥f∥Bβ\np,q\n≲N −eβ∥f∥Bβ\np,q.\nThis concludes the proof.\nProof of Proposition 2. We adopt the proof line employed by Suzuki (2019). Basically, we com-\nbine Lemma 1 and Lemma 2. We substitute the approximated cardinal B-spline basis ˇ\nM into the\n17\ndecomposition of fN (11). Let the set of indexes (k, j) ∈Z × Z that consists fN given in Eq. (11)\nbe EN, i.e., fN = P\n(k,j)∈EN αk,jM d\nk,j. Accordingly, we set ˇf := P\n(k,j)∈EN αk,j ˇ\nM d\nk,j. Note that\nfor each x, the number of (k, j) ∈EN that satisfy Mk,j(x) ̸= 0 is bounded by (m + 1)d(1 + K∗),\nand max(k,j)∈EN |αk,j| ≲2\nK∗β\ne\nβ (eβ−1/p)+ by the norm equivalence Eq. (10). For each x ∈Rd, it\nholds that\n|fN(x) −ˇf(x)| ≤\nX\n(k,j)∈EN\n|αk,j||M d\nk,j(x) −ˇ\nM d\nk,j(x)|\n≤ϵ\nX\n(k,j)∈EN\n|αk,j|1{M d\nk,j(x) ̸= 0}\n≲ϵ(m + 1)d(1 + K∗)2K∗(β/eβ)(eβ−1/p)+∥f∥Bsp,q\n≲ϵ log(N)N (1+ν−1)(eβ−1/p)+∥f∥Bsp,q,\nwhere we used the deﬁnition of K∗in the last inequality. This evaluation yields that, for each\nf ∈U(Bβ\np,q(Ω)), it holds that\n∥f −ˇf∥Lr ≲∥f −fN∥Lr + ∥fN −ˇf∥Lr ≲log(N)N (1+ν−1)(1/p−eβ)+∥f∥Bsp,qϵ + N −eβ.\nBy taking ϵ to satisfy log(N)N (1+ν−1)(1/p−eβ)+ϵ ≤N −eβ, we obtain the approximation error bound.\nAs we have seen above max(k,j)∈EN |αk,j| ≲2\nK∗β\ne\nβ (eβ−1/p)+ ≤N (1+ν−1)(1/p−eβ)+. The max of\nthe absolute values of parameters used in ˇ\nM d\nk,j can be bounded by 2K∗(see Suzuki (2019)) which\nis bounded by N d(1+ν−1)(1/p−eβ)+. Then, we obtain the assertion.\nB.1\nProof of Theorem 6 and Theorem 1\nProof of Theorem 6. This proof is almost obvious from Proposition 2. We know that, from Proposi-\ntion 2, for g ∈U(Bβ\np,q([0, 1] ˜d)), there exists ˇf ∈Φ(L1( ˜d), W1( ˜d), S1( ˜d), B1( ˜d)) such that\n∥ˇf −g∥r ≲N −eβ.\nBecause the density of the distribution of Ax + b is bounded above when x obeys the uniform\ndistribution on Ω, this also yields\n∥ˇf ◦(A · +b) −g ◦(A · +b)∥r ≲N −eβ.\n(note that the Lebesgue measure on Ω= [0, 1]d corresponds to the uniform distribution on Ω). If ˇf\ncan be written as\nˇf(x) = (W(L1)η(·) + b(L)) ◦· · · ◦(W(1)x + b(1)),\nthen we have\nˇf◦(A·+b) = (W(L1)η(·)+b(L))◦· · ·◦(W(1)A·+b(1)+W(1)b) ∈Φ(L1( ˜d), W1( ˜d), S1( ˜d), ( ˜dC+1)B1( ˜d)).\nProof of Theorem 1.\nHdeep := {hH ◦· · · ◦h1(x) | hℓ: [0, 1]mℓ→[0, 1]mℓ+1, hℓ,k ∈U(Bβ(ℓ)\np,q ([0, 1]mℓ)) (∀k ∈[mℓ+1])}.\nSince\neβ(ℓ)\n>\n1/p,\nwe\ncan\nshow\nthat\nfor\neach\nhℓ,k,\nthere\nexists\nˇfℓ,k\n∈\nΦ(L1(mℓ), W1(mℓ), S1(mℓ), B1(mℓ)) such that\n∥ˇfℓ,k −hℓ,k∥∞≲N −eβ.\nMoreover, from the proof of Proposition 2, we can share all parameters other than the last layer\namong ˇfℓ,k (k = 1, . . . , mℓ+1). If necessary, we may modify ˇfℓ,k so that 0 ≤ˇfℓ,k(x) ≤1 (∀x ∈\n18\n[0, 1]mℓ) by adding one additional clipping layer which can be realized by ReLU (actually, the\nclipping operator can be constructed by a linear combination of 2 nodes with ReLU activation as\nf(x) = max{x, 0} −max{x −1, 0} = min{max{x, 0}, 1} for x ∈R). The approximation error\nof the whole layer can be evaluated as\n∥hH ◦· · · ◦h1 −ˇfH ◦· · · ◦ˇf1∥∞\n≤\nH\nX\nℓ=1\n∥hH ◦· · · ◦hℓ+1 ◦hℓ◦ˇfℓ−1 ◦· · · ◦ˇf1 −hH ◦· · · ◦hℓ+1 ◦ˇfℓ◦ˇfℓ−1 ◦· · · ◦ˇf1∥∞\n≤\nH\nX\nℓ=1\n∥hH ◦· · · ◦hℓ+1 ◦hℓ−hH ◦· · · ◦hℓ+1 ◦ˇfℓ∥∞.\nProposition 1 tells that hℓ′,k ∈C(β(ℓ′)−1/p)∧1; thus, hℓ′,k is γℓ′-H¨older continuous where γℓ′ :=\n(β(ℓ′) −1/p) ∧1. Their composition hH ◦hH−1 ◦· · · ◦hℓ+1 is Γℓ-H¨older continuous where\nBℓ= QH\nℓ′=ℓ+1 γℓ′. Therefore, we have\n∥hH ◦· · · ◦hℓ+1 ◦hℓ−hH ◦· · · ◦hℓ+1 ◦ˇfℓ∥∞≲∥hℓ−ˇfℓ∥Bℓ\n∞,\nwhere ∥· ∥∞for a vector-valued function g : Rd′ →Rd′′ is deﬁned as supx ∥g(x)∥. Summing up\nthis evaluation for ℓ= 1, . . . , H concludes that\n∥hH ◦· · · ◦h1 −ˇfH ◦· · · ◦ˇf1∥∞≲\nH\nX\nℓ=1\nN −Bℓeβ(ℓ) ≲max\nℓ∈H N −eβ∗(ℓ).\nConsequently, the whole network can be realized as an element of Φ(L, W, S, B) where\nL =\nH\nX\nℓ=1\n(L1(mℓ) + 1), W = max\nℓ(W1(mℓ) ∨mℓ+1),\nS =\nH\nX\nℓ=1\n(S1(mℓ) + 3mℓ+1), B = max\nℓ\nB1(mℓ).\nC\nProofs of estimation error bound (Theorem 2 and Theorem 3 )\nProof of Theorem 2. We follow the proof strategy from Schmidt-Hieber (2018); Suzuki (2019)\nwhich uses Proposition 4. It sufﬁces to the covering number of ˆF = { ¯f | f ∈Ψ(L, W, S, B)}\nfor (L, W, S, B) given in Theorem 6 where ¯f is the clipped version of a function f. Note that the\ncovering number of ˆF is not larger than that of Ψ(L, W, S, B). Hence, it is sufﬁcient to evaluate\nthat of Ψ(L, W, S, B). From Lemma 6, the covering number of this class is upper bounded by\nlog N(δ, ˆF, ∥· ∥∞) ≲N log(N)[log(N)2 + log(δ−1)].\nFrom Proposition 2, there exists ˇf ∈Φ(L, W, S, B) such that\n∥f o −RK(f o)∥2 ≲N −eβ.\nMoreover, we notice that ∥f −f o∥2\nL2(PX) ≤R∥f −f o∥2\n2. for any f : [0, 1]d →R because the\ndensity pX of PX is bounded by R. Therefore, by applying Proposition 4 with δ = 1/n, we have\nEDn[∥bf −f o∥2\nL2(PX)] ≲N −2eβ + N log(N)(log(N)2 + log(n))\nn\n+ 1\nn.\nHere, we can minimize the right hand side by setting N ≍n\n1\n2 e\nβ+1 up to log(n)3-order, and then we\nobtain the estimation error of the least squares estimator as\nn\n−\n2 e\nβ\n2 e\nβ+1 log(n)3.\nThis yields the assertion.\nProof of Theorem 3. The proof is almost identical to the proof of Theorem 2, except that we use\nTheorem 1 as an approximation error bound.\n19\nD\nEmbedding theorem\nLemma 3. For 0 < p(1), p(2) ≤∞, let β(1), β(2) ∈Rd\n++ such that they satisfy\n˜β(1) −˜β(2) ≥\n1\np(1) −\n1\np(2) ,\n(15)\nβ(2) = γβ(1),\np(1) < p(2),\nfor 0 < γ < 1. Then, it holds that\nBβ(1)\np(1),q ,→Bβ(2)\np(2),q.\nProof. We show the assertion only for the situation where p(1) ̸= ∞, p(2) ̸= ∞, and q ̸= ∞. The\nproof for the setting in which p(1) = ∞, p(2) = ∞, or q = ∞is satisﬁed is almost identical. Recall\nthe following norm equivalence shown in Lemma 2:\n∥f∥Bsp,q ≃∥(αk,j)k,j∥bβ\np,q =\n\n\n\n∞\nX\nk=0\n\n2k[β−(Pd\ni=1⌊kβ′\ni⌋/k)/p]\u0010 X\nj∈J(k)\n|αk,j|p\u00111/p\n\n\nq\n\n\n1/q\n,\nwhen p, q < ∞. Since p(1)\np(2) < 1, it holds that\n\u0010 X\nj∈J(k)\n|αk,j|p(1)\u00111/p(1)\n=\n\u0010 X\nj∈J(k)\n|αk,j|\np(2) p(1)\np(2) \u00111/p(1)\n≥\n\u0010 X\nj∈J(k)\n|αk,j|p(2)\u0011 p(1)\np(2)\n1\np(1) =\n\u0010 X\nj∈J(k)\n|αk,j|p(2)\u0011\n1\np(2) .\nMoreover, we have\n2k[β(1)−(Pd\ni=1⌊kβ′(1)\ni\n⌋/k)/p(1)]\n≃2kβ(1)−Pd\ni=1 β′(1)\ni\n/p(1) = 2\nk\nβ(1)\ne\nβ(1)\n\u0012\neβ(1)−\n1\np(1)\n\u0013\n(a)\n= 2\nk\nβ(2)\ne\nβ(2)\n\u0012\neβ(1)−\n1\np(1) +\n1\np(2) −\n1\np(2)\n\u0013\n(b)\n≥2\nk\nβ(2)\ne\nβ(2)\n\u0012\neβ(2)−\n1\np(2)\n\u0013\n= 2kβ(2)−Pd\ni=1 β′(2)\ni\n/p(2) ≃2k[β(2)−(Pd\ni=1⌊kβ′(2)\ni\n⌋/k)/p(2)],\nwhere we used the condition β(2) = γβ(1) in (a), and we used the condition from Eq. (15) in (b).\nThese relations yield the following evaluation:\n∥f∥Bβ(1)\np(1),q\n≃∥(αk,j)k,j∥bβ(1)\np(1),q\n=\n\n\n\n∞\nX\nk=0\n\n2k[β(1)−(Pd\ni=1⌊kβ′(1)\ni\n⌋/k)/p(1)]\u0010 X\nj∈J(k)\n|αk,j|p(1)\u00111/p(1)\n\n\nq\n\n\n1/q\n≳\n\n\n\n∞\nX\nk=0\n\n2k[β(2)−(Pd\ni=1⌊kβ′(2)\ni\n⌋/k)/p(2)]\u0010 X\nj∈J(k)\n|αk,j|p(2)\u00111/p(2)\n\n\nq\n\n\n1/q\n≃∥f∥Bβ(2)\np(2),q\n.\nThis yields the assertion.\nBy combining Lemma 3 with the relation Bγβ\n∞,∞,→Cγβ (Triebel, 2011), we immediately obtain the\nfollowing corollary.\nCorollary 1. Suppose that eβ > p, then for γ =\neβ−p\neβ , it holds that\nBβ\np,q ,→Bγβ\n∞,q ,→Bγβ\n∞,∞,→Cγβ.\n20\nE\nMinimax optimality\nIn this section, we demonstrate the proof of Theorem 4. Before this, we prepare the basic notions.\nThe ϵ-covering number N(ϵ, C, ˆd) of a metric space C equipped with a metric ˆd that is the minimal\nnumber of balls with radius ϵ measured by the metric ˆd required to cover the set C (van der Vaart\n& Wellner, 1996). Similarly, the δ-packing number M(δ, C, ˆd) is deﬁned as the largest number of\nelements {f1, . . . , fM} ⊆C such that ˆd(fi, fj) ≥δ for all i ̸= j.\nRaskutti et al. (2012) showed the following inequality in their proof of Theorem 2(b) by utilizing\nthe result by Yang & Barron (1999).\nLemma 4. Let F◦be the model of the true function. For a given δn > 0 and εn > 0, let Q be the\nδn-packing number M(δn, F◦, L2(PX)) of F◦and N be the εn covering number of that. Suppose\nthat they satisfy the following condition:\nn\n2σ2 ε2\nn ≤log(N),\n8 log(N) ≤log(Q), 4 log(2) ≤log(Q).\n(16)\nThen, the minimax learning rate is lower bounded as\ninf\nb\nf\nsup\nf ∗∈F◦EDn[∥bf −f ∗∥2\nL2(PX)] ≥δ2\nn\n4 .\nThis concludes the assertion.\nNow, we are ready to show Theorem 4.\nProof of Theorem 4. Proposition 10 of Triebel (2011) showed that the ϵ-covering number of the unit\nball of anisotropic Besov spaces Bβ\np,q(Ω) can be evaluated as\nlog N(ϵ, U(Bβ\np,q(Ω)), ∥· ∥r) ≃ϵ−1/eβ,\nfor 0 < p, q ≤∞, 1 ≤r < ∞, and β ∈Rd\n++ that satisfy\neβ > max\n\u001a1\np −1\nr , 1\np −1, 0\n\u001b\n.\nAfﬁne composition model:\nApparently, U(Bβ\np,q(Ω)) is included in Haﬀ. Hence, noting that PX is the uniform distribution and\n∥· ∥2 = ∥· ∥L2(PX), the covering number of Haﬀcan be lower bounded by\nlog N(Haﬀ, ∥· ∥L2(PX)) ≳ϵ−1/eβ.\nFrom this evaluation, Lemma 4 yields that there exists C1 > 0 independent of n such that\ninf\nb\nf\nsup\nf ∗∈Haff\nEDn[∥bf −f ∗∥2\nL2(PX)] ≥C1n\n−\n2 e\nβ\n2 e\nβ+1 .\nTo see this, we may just set ϵn ≃δn ≃n\n−\n2 e\nβ\n2 e\nβ+1 in Eq. (16) of Lemma 4.\nDeep composition model:\nNext, we show the minimax rate for the deep composition model. Basically, we follow the same\nstrategy developed by Schmidt-Hieber (2018), but we need to modify some technical details be-\ncause we are dealing with anisotropic Besov spaces while Schmidt-Hieber (2018) analyzed isotropic\nH¨older space. Let ℓ∗:= minℓ∈[H] eβ∗(ℓ), and s(ℓ) := (β(ℓ) −1/p + ϵ) ∧1 where ϵ > 0 can be ar-\nbitrary small for q < ∞and ϵ = 0 for q = ∞. Without loss of generality, we may assume that\nβ(ℓ)\n1\n≤β(ℓ)\n2\n≤· · · ≤β(ℓ)\nd\nfor ℓ∈[H]. Let us consider a sub-model H′\ndeep of Hdeep deﬁned as\nH′\ndeep :={gH ◦· · · ◦g1 |\n21\ngℓ(x) = x (ℓ= 1, . . . , ℓ∗−1),\ngℓ∗(x) = (gℓ∗,1(x), 0, . . . , 0)⊤where gℓ∗,1 ∈U(Bβ\np,q(Ω)),\ngℓ(x) = (xs(ℓ)\n1\n, 0, . . . , 0)⊤(ℓ= ℓ∗+ 1, . . . , H)}.\nFor ℓ= ℓ∗+ 1, . . . , H, through a cumbersome calculation, we can verify that xs(ℓ)\n1\n∈B\nβ(ℓ)\np,q ([0, 1])\nfor x ∈[0, 1], which ensures gℓ,j(x) ∈Bβ(ℓ)\np,q ([0, 1]d) for j = 1, . . . , d. To lower bound the\ncovering number, we concretely construct a subset the cardinality of which can be easily esti-\nmated. For that purpose, we use the expansion f = P∞\nk=0\nP\nj∈J(k) αk,jM d\nk,j(x) and the norm\nequivalence ∥f∥Bβ\np,q ≃∥(αk,j)k,j∥bβ\np,q given in Lemma 2. For a while, we let β := β(ℓ∗) and\nB := QH\nq=ℓ∗+1 s(ℓ). We deﬁne k ∈N so that k satisﬁes 2\nk\nβ\ne\nβ ≃n\n1\n1+2B e\nβ . For this choice of k, take a\nsubset ˆJ(k) ⊂J(k) such that | ˆJ(k)| ≃|J(k)| and for each j, j′ ∈ˆJ(k) with j ̸= j′, the supports of\nM d\nk,j and M d\nk,j′ are disjoint. Using this index set ˆJ(k), we consider a set of functions that is given\nby\nˆHℓ∗:=\n\n\nf =\nX\nj∈ˆ\nJ(k)\nαk,jM d\nk,j(x) | αk,j ∈{0, 2−kβ}\n\n\n.\nWe can check that | ˆHℓ∗| = | ˆJ(k)| ≃2k Pd\nj=1 β′\nj = 2kβ/eβ and ∥f∥Bβ\np,q ≲1 for all f ∈ˆHℓ∗from the\nnorm equivalence (10). For any gw = P\nj∈ˆ\nJ(k) wj2−kβM d\nk,j(x) ∈ˆHℓ∗(w ∈{0, 1}| ˆ\nJk|), we can\nsee that\nfw(x) = gH ◦· · · ◦gℓ∗+1 ◦gw ◦gℓ∗−1 ◦· · · ◦g1(x)\n=\nX\nj∈ˆ\nJ(k)\nw2−BkβM dB\nk,j (x).\nIf w ̸= w′, then we can see that\n∥fw −fw′∥2\nL2(PX) ≳Ham(w, w′)2−2BKβ2−kβ/eβ\n≳Ham(w, w′)2−kβ(2B eβ+1)/eβ,\nwhere Ham is the Hamming distance because ∥M d\nk,j∥2\nL2(PX) ≃2−kβ/eβ.\nThen, by the Varshamov–Gilbert bound (see Lemma 2.9 of Tsybakov (2008), for example), there\nexists a subset Wk ⊂{0, 1}| ˆ\nJ(k)| such that |Wk| ≥2| ˆ\nJ(k)|/8 and Ham(w, w′) ≥| ˆJ(k)|/8 for all\nw, w′ ∈Wk with w ̸= w′. This yields\n∥fw −fw′∥2\nL2(PX) ≳2kβ/eβ2−kβ(2B eβ+1)/eβ = 2−2Bkβ ≃n\n−\n2B e\nβ\n2B e\nβ+1 ,\nwhere the deﬁnition of k is used. This implies that there exists a subset H′′\ndeep ⊂H′\ndeep(⊂Hdeep)\nsuch that\nlog(N(ϵn, H′′\ndeep, ∥· ∥L2(PX))) ≳n\n1\n1+2B e\nβ\nfor ϵn ≳n\n−\nB e\nβ\n2B e\nβ+1 . Then, by Lemma 4, we obtain that the minimax optima rate on Hdeep is lower\nbounded as\ninf\nb\nf\nsup\nf ∗∈F◦EDn[∥bf −f ∗∥2\nL2(PX)] ≳n\n−\nB e\nβ\n2B e\nβ+1 .\nF\nMinimax optimal rate of linear estimators\nDeﬁne the convex hull of a function class F◦as\nconv(F◦) :=\n\n\nf(x) =\nM\nX\nj=1\nλjfj(x) | M = 1, 2, . . . , fj ∈F◦, λj ≥0,\nM\nX\nj=1\nλj = 1\n\n\n.\n22\nLet conv(·) is the closure of conv(·) with respect to L2(PX)-norm.\nProposition 3 (Hayakawa & Suzuki (2019)). The minimax optimal rate of linear estimators on a\ntarget function class F◦is the same as that on the convex hull of F◦:\ninf\nb\nf: linear\nsup\nf o∈F◦EDn[∥f o −bf∥2\nL2(PX)] =\ninf\nb\nf: linear\nsup\nf o∈conv(F◦)\nEDn[∥f o −bf∥2\nL2(PX)].\nSee Hayakawa & Suzuki (2019) for the proof of this proposition.\nProof of Theorem 5. We basically follow the strategy developed by Zhang et al. (2002). Let µ be\nthe uniform measure on Ω. They essentially showed the following statement in their Theorem 1.\nSuppose that the space Ωhas even partition A such that |A| = 2K for an integer K ∈N, each A has\nequivalent measure µ(A) = 2−K for all A ∈A, and A is indeed a partition of Ω, i.e., ∪A∈A = Ω,\nA ∩A′ = ∅for A, A′ ∈Ωand A ̸= A′. Then, if K is chosen as n−γ1 ≤2−K ≤n−γ2 for constants\nγ1, γ2 > 0 that are independent of n, then there exists an event E such that, for a constant C′ > 0,\n|{xi | xi ∈A (i ∈{1, . . . , n})}| ≤C′n/2K (∀A ∈A),\nP(E) ≥1 + o(1).\nWe call this property of A “Condition A.”\nHere, we consider a set F◦of functions on Ωfor which there exists ∆> 0 that satisﬁes the following\nconditions:\n1. There exists F > 0 such that, for any A ∈A, there exists g ∈F◦that satisﬁes g(x) ≥\n1\n2∆F for all x ∈A,\n2. There exists K′ and C′′ > 0 such that 1\nn\nPn\ni=1 g(xi)2 ≤C′′∆22−K′ for any g ∈F◦on\nthe event E.\nWe call this condition of the function class F◦“Condition B.”\nLet the minimax optimal rate of linear estimators on the function class F◦be\nR∗=\ninf\nb\nf:linear\nsup\nf o∈F◦EDn[∥bf −f o∥2\nL2(PX)].\nThen, under Conditions A and B, there exists a constant F1 such that at least one of the following\ninequalities holds:\nF 2\n4F1C′′\n2K′\nn\n≤R∗,\n(17a)\nF 3\n32 ∆22−K ≤R∗,\n(17b)\nfor sufﬁciently large n.\n(i) Proof of Eq. (6).\nFor given k ∈N (which will be ﬁxed later), let ∆= 2−k[β−(Pd\ni=1⌊kβ′\ni⌋/k)/p]. Then, from the\nwavelet expansion of anisotropic Besov space (9),\nfw =\nX\nj∈J(k)\n∆wjM d\nk,j(x) ∈CU(Bβ\np,q(Ω)),\nwhere C > 0 is a constant and w = (wj)j∈J(k) is a one-hot vector, i.e., wj = 1 for some j ∈J(k)\nand wj′ = 0 for all j′ ∈J(k) with j′ ̸= j. This expansion ensures that, for K = Pd\ni=1⌊kβ′\ni⌋, there\nexists a partition A of Ωthat satisﬁes Condition A, and for any A ∈A, there exists w such that\nfw(x) ≳∆for all x ∈A and\n1\nn\nn\nX\ni=1\nfw(xi)2 ≤1\nn∆2|{i | xi ∈A (i = 1, . . . , n)}| ≲∆22−K,\n23\non the event E, which ensures that F◦= {fw | w is a one-hot vector} satisﬁes Condition B. Hence,\nby choosing k ∈N so that 2K ≃n\n1\n2( e\nβ+ 1\n2 −1\np )+1 (recall that K = Pd\ni=1⌊kβ′\ni⌋by deﬁnition), and\nsetting K = K′, then Eq. (17) gives\nR∗≳n\n−\n2 e\nβ−v\n2 e\nβ−v+1 ,\nfor v = 2(1/p −1/2). This yields the assertion because F◦⊂CU(Bβ\np,q(Ω)) for a constant C.\n(ii) Proof of Eq. (7).\nLet β∗:= β = β1 = · · · = β ˜d = β. For m such that β∗< min{m, m −1 + 1/p}, let φ ˜d(x) =\nQ ˜d\nj=1 Nm(xi −(m + 1)/2) (x ∈R ˜d).\n(ii-a) Setting of ˜d ≥d/2:\nLet V ˜d,d := {U ∈R ˜d×d | UU ⊤= I ˜d} be the Stiefel manifold and let πV ˜\nd,d be the invariant measure\non the Stiefel manifold (i.e., the uniform distribution). Then, let ¯φ ˜d : Rd →R be\n¯φ ˜d(x) =\nZ\nφ ˜d(Ux)dπV ˜\nd,d(U) (x ∈Rd).\nWe can see that ¯φ ˜d is spherically symmetric and there exists F, C > 0 such that\n¯φ ˜d(x) ≥F (∀x ∈Rd s.t. ∥x∥≤1),\nand\n¯φ ˜d(x) ≤\n(\nC∥x∥−˜d\n(∥x∥≥1),\n1\n(∥x∥≤1).\nThe last inequality can be checked by the fact that for a sufﬁciently large R > 0, the measure of\nthe set µR({x | ∥x∥= R, φ ˜d(x) > 0}) ≲1 × Rd−˜d−1/Rd−1 = R−˜d (here, µR is the uniform\nprobability measure on the sphere Sd−1(R) = {x ∈Rd | ∥x∥= R}) and ∥φ ˜d∥∞≤1.\nBy the construction of φ ˜d and the wavelet expansion of anisotropic Besov space (9) with the norm\nequivalence (10), we have that there exists a constant c > 0 such that, for any k ∈N and ¯b =\n\u0002 1\n2 −2−k \u0000 m+1\n2\n−\n\u0004 m+1\n2\n\u0005\u0001\u0003\n(1, . . . , 1)⊤∈R ˜d, it holds that\nc∆φ ˜d\n\u00002k(· −¯b)\n\u0001\n∈U(Bβ∗\np,q([0, 1]\n˜d)),\nwhere ∆= 2−k(β∗−˜d/p). Here, let 0 < ¯c < 1 be a constant such that ¯cU(x −b′) + ¯b ∈[0, 1] ˜d for\nany x, b′ ∈[0, 1]d and any U ∈V ˜d,d. Then, we have that, for any b′ ∈[0, 1]d,\nc∆φ ˜d(2k¯cU(· −b′)) = c∆φ ˜d(2k(· −¯b)) ◦(¯cU(· −b′) + ¯b) ∈Haﬀ,\nfor any U ∈V ˜d,d. By the convex hull argument (Proposition 3), this yields that\nRlin\n∗(Haﬀ) = Rlin\n∗(conv(Haﬀ)) ≥Rlin\n∗({c∆¯φ ˜d(2k¯c(· −b′)) | b′ ∈Ω}).\nHence, it sufﬁces to lower bound the far right-hand side of this inequality. We consider a partition\nA of Ω, where A ∈A has the form A = [2−kj1, 2−k(j1 + 1)] × · · · × [2−kjd, 2−k(jd + 1)]\nfor 0 ≤ji ≤2k −1 (i = 1, . . . , d). Let ˆJ(k) = {(j1, . . . , jd) | 0 ≤ji ≤2k−1} and Aj =\n[2−kj1, 2−k(j1 +1)]×· · ·×[2−kjd, 2−k(jd +1)] ∈A for j ∈ˆJ(k). Let ϕAj = c¯φ ˜d(2k¯c(·−bAj)),\nwhere bAj = (2−k(j1 + 1/2), . . . , 2−k(jd + 1/2))⊤for j ∈ˆJ(k). We can see that |A| = 2dk.\nHence, A satisﬁes Condition A with K = dk if 2k is in polynomial order with respect to n.\nMoreover, there exists F\n> 0 such that ϕA(x) ≥F for all x ∈A.\nNext, we evaluate\n1\nn\nPn\ni=1 ϕA(xi)2. On the event E, there exists C′ such that |{i ∈[n] | xi ∈A′}| ≤C′n/2K =\nC′nµ(A′) for all A′ ∈A. Here, let\n¯ϕA(x) :=\n(\ncC∥2k¯c(x −bA)∥−˜d\n(∥2k¯c(x −bA)∥≥1),\nc\n(otherwise),\n24\nthen ¯ϕA(x) ≥ϕA(x). Thus, we can upper bound 1\nn\nPn\ni=1 ϕA(xi)2 as\n1\nn\nn\nX\ni=1\nϕA(xi)2 ≤1\nn\nn\nX\ni=1\n¯ϕA(xi)2 = 1\nn\nX\nA′∈A\nX\nxi∈A′\n¯ϕA(xi)2 ≤1\nn\nX\nA′∈A\nC′ n\n2K max\nx∈A′ ¯ϕA(x)2\n= C′ X\nA′∈A\nµ(A′) max\nx∈A′ ¯ϕA(x)2 = C′ X\nA′∈A\nµ(A′) min\nx∈A′ ¯ϕA(x)2 maxx∈A′ ¯ϕA(x)2\nminx∈A′ ¯ϕA(x)2\n≤C′ X\nA′∈A\nµ(A′) min\nx∈A′ ¯ϕA(x)2 maxx∈A′ ¯ϕA(x)2\nminx∈A′ ¯ϕA(x)2\n≤C′ X\nA′∈A\nµ(A′) min\nx∈A′ ¯ϕA(x)2\nmax\nx:∥2k¯c(x−bA)∥≥1\n∥2k¯c(x −bA)∥−2 ˜d\n(∥2k¯c(x −bA)∥+ ¯c∥1∥)−2 ˜d\n≤C′ X\nA′∈A\nµ(A′) min\nx∈A′ ¯ϕA(x)2(1 + ¯c\n√\nd)2 ˜d\n≤C′(1 + ¯c\n√\nd)2 ˜d\nZ\nΩ\n¯ϕA(x)2dx.\nThe quantity\nR\nΩ¯ϕA(x)2dx on the right-hand side can be evaluated as\nZ\nΩ\n¯ϕA(x)2dx ≤\nZ\nx:∥x−bA∥≤2\n√\nd\n¯ϕA(x)2dx\n≤\nZ\nx:∥x−bA∥≤¯c−12−k ¯ϕA(x)2dx +\nZ\nx:¯c−12−k<∥x−bA∥≤2\n√\nd\n¯ϕA(x)2dx\n≲2−kd + C¯c−2 ˜d2−2k ˜d\nZ\n¯c−12−k≤r≤2\n√\nd\nr−2 ˜drd−1dr\n≲2−kd + 2−2k ˜d max{2k(2 ˜d−d), 1}\n≲max{2−kd, 2−2k ˜d}.\nTherefore, we have that, for a constant C′′, on the event E, we have that\n1\nn\nn\nX\ni=1\nϕA(xi)2 ≤C′′(2−kd ∨2−2k ˜d).\nLet F◦= {∆ϕA | A ∈A}, then F◦satisﬁes Condition B. When ˜d ≥d/2, by choosing k so that\n¯c2k ≃n\n1\n2(β∗+d−˜\nd/p) and K = K′ = dk, then Eq. (17) yields\nRlin\n∗(F◦) ≳n\n−\n2(β∗−˜\nd/p+d/2)\n2(β∗−˜\nd/p+d/2)+d .\nThis concludes the proof.\n(ii-b) Setting of ˜d < d/2:\nLet A be the partition of Ωas deﬁned in the proof for ˜d ≥d/2, i.e., |A| = 2dk and each A ∈A\ncan be written as A = [2−kj1, 2−k(j1 + 1)] × · · · × [2−kjd, 2−k(jd + 1)] for 0 ≤ji ≤2k −1\n(i = 1, . . . , d). Pick up A ∈A and let j ∈ˆJ(k) be the index such that A = Aj. For a while, we ﬁx A\nand let ¯b = bAj = (2−k(j1+1/2), . . . , 2−k(jd+1/2))⊤accordingly. For θ = (w, b) ∈Rd−˜d+1×R,\nlet Aθ : Rd →R ˜d be\nAθ(x) := 2k[x1 −¯b1, . . . , x ˜d−1 −¯b ˜d−1, w⊤(x ˜d:d −¯b ˜d:d) + b],\nand consider\nφθ(x) := φ ˜d(Aθ(x)).\nWe take its convex hull with respect to θ. We note that\nφθ(x) =\n\n\n˜d−1\nY\nj=1\nNm(2k(xj −¯bj) −(m + 1)/2)\n\nNm\n\u00002k[w⊤(x ˜d:d −¯b ˜d:d) + b] −(m + 1)/2\n\u0001\n.\n25\nTo analyze its convex hull,\nit sufﬁces to consider the convex hull of the last term\nNm\n\u00002k[w⊤(x ˜d:d −¯b ˜d:d) + b] −(m + 1)/2\n\u0001\n. Hence, we set ψ(·) := Nm(· −(m + 1)/2) and\nconsider a set of functions\n˜F(ψ)\nC,τ := {x ∈Rd−˜d+1 7→aψ(τ(w⊤x+b))) | |a| ≤2C, ∥w∥≤1, |b| ≤2 (a, b ∈R, w ∈Rd−˜d+1)}\nfor C > 0, τ > 0. We also deﬁne the Fourier transform of ψ as ˆψ(ω) := (2π)−1 R\ne−iωxψ(x)dx\n(ω ∈R). Then, by Lemma 5, we have that, for h = 2−k and τ = h−1−κ,\ninf\nˇg∈conv( ˜\nF(ψ)\nC,τ )\nsup\nx∈[0,1]d\n\f\f\f\fˇg(x) −exp\n\u0012\n−∥x −c∥2\n2h2\n\u0013\f\f\f\f\n≤\n4\n|2π ˆψ(1)|\nh\nCd−˜d+1R2(d−˜d−1) exp(−R2/2) + exp(−R)\ni\n,\nwhere C =\nτ\nπ| ˆ\nψ(1)| = Θ(h−1−κ) and R = h−κ(2\n√\nd + 1). This indicates that, for a ﬁxed A ∈A,\nthe convex hull of the set {aφθ | θ = (w, b) ∈Rd−˜d+1 × R, ∥w∥≤1, |b| ≤2, |a| ≤∆} where\n∆= 2−k(β∗−˜d/p) contains ϕA which satisﬁes\n\r\r\r\r\r\r\nϕA −∆(2C)−1\n\n\n˜d−1\nY\nj=1\nNm(2k(xj −¯bj) −(m + 1)/2)\n\nexp\n\u0012\n−∥x ˜d:d −¯b ˜d:d∥2\n2h2\n\u0013\r\r\r\r\r\r\n∞\n= O\n\u0010\n∆2−k(1+κ)(h−κ(2(d−˜d−1)) exp(−h−2κ/2) + exp(−h−κ))\n\u0011\n.\nWe can see that on the event E, it holds that\n1\nn\nn\nX\ni=1\nϕ2\nA(xi) ≲µ(A)(∆2−k(1+κ))2 ≲2−kd2−2k(β∗−˜d/p+1)2−2kκ = 2−2k(β∗−˜d/p+1+d/2)−2kκ,\nfor all A ∈A. Let F◦= {ϕA | A ∈A}, then F◦satisﬁes Condition B. Note that, by the deﬁnition\nof ˜F(ψ)\nC,τ is holds that ϕA ∈conv(Haﬀ) for all A ∈A. Thus\nRlin\n∗(Haﬀ) = Rlin\n∗(conv(Haﬀ)) ≥Rlin\n∗(F◦).\nTherefore, by choosing k such that 2k ≃n\n1\n2(β∗−˜\nd/p+1+d/2)+d+2κ , and setting K = K′ = dk, then\nEq. (17) gives\nRlin\n∗(F◦) ≳n\n−\n2(β∗−˜\nd/p+1+κ+d/2)\n2(β∗−˜\nd/p+1+κ+d/2)+d .\nLemma 5 (Suzuki & Akiyama (2021)). Let h > 0 and R := hτ/(2\n√\nd+1). Then, for C =\nτ\nπ| ˆ\nψ(1)|,\nthe Gaussian RBF kernel can be approximated by\ninf\nˇg∈conv( ˜\nF(ψ)\nC,τ )\nsup\nx∈[0,1]d\n\f\f\f\fˇg(x) −exp\n\u0012\n−∥x −c∥2\n2h2\n\u0013\f\f\f\f\n≤\n4\n|2π ˆψ(1)|\nh\nCdR2(d−2) exp(−R2/2) + exp(−R)\ni\nfor any c ∈[0, 1]d, where Cd is a constant depending only on d. In particular, the right hand side is\nO(exp(−nκ)) if R = nκ.\nG\nAuxiliary lemmas\nThe following proposition which were shown in Schmidt-Hieber (2018); Hayakawa & Suzuki\n(2019); Suzuki (2018) is convenient to show the estimation error rate.\n26\nProposition 4 (Schmidt-Hieber (2018); Hayakawa & Suzuki (2019)). Let F be a set of functions.\nLet bf be the least-squares estimator in F:\nbf = argmin\nf∈F\nn\nX\ni=1\n(yi −f(xi))2.\nAssume that ∥f o∥∞≤F and all f ∈F satisﬁes ∥f∥∞≤F for some F ≥1. If δ > 0 satisﬁes\nN(δ, F, ∥· ∥∞) ≥3, then it holds that\nEDn[∥bf −f o∥2\nL2(PX)] ≤C\n\u0014\ninf\nf∈F ∥f −f o∥2\nL2(PX) + (F 2 + σ2)log N(δ, F, ∥· ∥∞)\nn\n+ δ(F + σ)\n\u0015\n,\nwhere C is a universal constant.\nThe following lemma provides the covering number of the deep neural network model.\nLemma 6 (Covering number evaluation). The covering number of Φ(L, W, S, B) can be bounded\nby\nlog N(δ, Φ(L, W, S, B), ∥· ∥∞) ≤S log(δ−1L(B ∨1)L−1(W + 1)2L)\n≤2SL log((B ∨1)(W + 1)) + S log(δ−1L).\nProof of Lemma 6. Given a network f ∈Φ(L, W, S, B) expressed as\nf(x) = (W(L)η(·) + b(L)) ◦· · · ◦(W(1)x + b(1)),\nlet\nAk(f)(x) = η ◦(W(k−1)η(·) + b(k−1)) ◦· · · ◦(W(1)x + b(1)),\nand\nBk(f)(x) = (W(L)η(·) + b(L)) ◦· · · ◦(W(k)η(x) + b(k)),\nfor k = 2, . . . , L. Corresponding to the last and ﬁrst layers, we deﬁne BL+1(f)(x) = x and\nA1(f)(x) = x respectively. Then, it is easy to see that f(x) = Bk+1(f)◦(W(k)·+b(k))◦Ak(f)(x).\nNow, suppose that a pair of different two networks f, g ∈Φ(L, W, S, B) given by\nf(x) = (W(L)η(·)+b(L))◦· · ·◦(W(1)x+b(1)), g(x) = (W(L)′η(·)+b(L)′)◦· · ·◦(W(1)′x+b(1)′),\nhas parameters with distance δ: ∥W(ℓ) −W(ℓ)′∥∞≤δ and ∥b(ℓ) −b(ℓ)′∥∞≤δ. Now, not that\n∥Ak(f)∥∞≤maxj ∥W(k−1)\nj,:\n∥1∥Ak−1(f)∥∞+ ∥b(k−1)∥∞≤WB∥Ak−1(f)∥∞+ B ≤(B ∨\n1)(W + 1)∥Ak−1(f)∥∞≤(B ∨1)k−1(W + 1)k−1, and similarly, the Lipshitz continuity of Bk(f)\nwith respect to ∥· ∥∞-norm is bounded as (BW)L−k+1. Then, it holds that\n|f(x) −g(x)|\n=\n\f\f\f\f\f\nL\nX\nk=1\nBk+1(g) ◦(W(k) · +b(k)) ◦Ak(f)(x) −Bk+1(g) ◦(W(k)′ · +b(k)′) ◦Ak(f)(x)\n\f\f\f\f\f\n≤\nL\nX\nk=1\n(BW)L−k∥(W(k) · +b(k)) ◦Ak(f)(x) −(W(k)′ · +b(k)′) ◦Ak(f)(x)∥∞\n≤\nL\nX\nk=1\n(BW)L−kδ[W(B ∨1)k−1(W + 1)k−1 + 1]\n≤\nL\nX\nk=1\n(BW)L−kδ(B ∨1)k−1(W + 1)k ≤δL(B ∨1)L−1(W + 1)L.\nThus, for a ﬁxed sparsity pattern (the locations of non-zero parameters), the covering number is\nbounded by\n\u0000δ/[L(B ∨1)L−1(W + 1)L]\n\u0001−S. There are the number of conﬁgurations of the spar-\nsity pattern is bounded by\n\u0000(W +1)L\nS\n\u0001\n≤(W + 1)LS. Thus, the covering number of the whole space\nΦ is bounded as\n(W + 1)LS \b\nδ/[L(B ∨1)L−1(W + 1)L]\n\t−S = [δ−1L(B ∨1)L−1(W + 1)2L]S,\nwhich yields the assertion.\n27\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2019-10-28",
  "updated": "2021-09-30"
}