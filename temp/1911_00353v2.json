{
  "id": "http://arxiv.org/abs/1911.00353v2",
  "title": "Does deep learning always outperform simple linear regression in optical imaging?",
  "authors": [
    "Shuming Jiao",
    "Yang Gao",
    "Jun Feng",
    "Ting Lei",
    "Xiaocong Yuan"
  ],
  "abstract": "Deep learning has been extensively applied in many optical imaging\napplications in recent years. Despite the success, the limitations and\ndrawbacks of deep learning in optical imaging have been seldom investigated. In\nthis work, we show that conventional linear-regression-based methods can\noutperform the previously proposed deep learning approaches for two black-box\noptical imaging problems in some extent. Deep learning demonstrates its\nweakness especially when the number of training samples is small. The\nadvantages and disadvantages of linear-regression-based methods and deep\nlearning are analyzed and compared. Since many optical systems are essentially\nlinear, a deep learning network containing many nonlinearity functions\nsometimes may not be the most suitable option.",
  "text": "Does deep learning always outperform \nsimple linear regression in optical imaging? \nSHUMING JIAO,1 YANG GAO,1 JUN FENG,1 TING LEI,1,2 AND XIAOCONG \nYUAN1,* \n1Nanophotonics Research Center, Shenzhen University, Shenzhen, Guangdong, China \n2leiting@szu.edu.cn   \n*xcyuan@szu.edu.cn \nAbstract: Deep learning has been extensively applied in many optical imaging problems \nin recent years. Despite the success, the limitations and drawbacks of deep learning in \noptical imaging have been seldom investigated. In this work, we show that conventional \nlinear-regression-based methods can outperform the previously proposed deep learning \napproaches for two black-box optical imaging problems in some extent. Deep learning \ndemonstrates its weakness especially when the number of training samples is small. The \nadvantages and disadvantages of linear-regression-based methods and deep learning are \nanalyzed and compared. Since many optical systems are essentially linear, a deep learning \nnetwork containing many nonlinearity functions sometimes may not be the most suitable \noption.  \n© 2020 Optical Society of America under the terms of the OSA Open Access Publishing Agreement \n1. Introduction \nIn recent years, deep learning receives much attention in many research fields including \noptical design [1,2] and optical imaging [3]. In previous works, deep learning has been \nextensively applied for many optical imaging problems including phase retrieval [4-7], \nmicroscopic image enhancement [8-9], scattering imaging [10-11], holography [12-18], \nsingle-pixel imaging [19,20], super-resolution [21-24], Fourier ptychography [25-27], \noptical interferometry [28,29], wavefront sensing [30,31], and optical fiber \ncommunications [32].  \nDespite the success, deep learning has its own limitations and drawbacks, like any other \napproach [33]. For example, a huge number of training samples is usually required to train \na deep neural network, which may not be always available in practical applications. The \noptimization of connection weights in the network with many training samples requires a \nconsiderable amount of computational cost. The design of network structure and the tuning \nof network parameters are often implemented empirically and intuitively with weak \nexplainability. A deep neural network trained and tested for one category of samples may \nfail to work when it is generalized to other different testing samples. In fact, it is likely that \ndeep learning may perform worse than other machine-learning (or non-machine-learning) \nmethods in certain application scenarios. In previous works, the deficiencies of deep \nlearning in solving optical imaging problems, compared with other methods, were seldom \ninvestigated.   \nIn recent works [34,35], deep learning has been employed to address the problems of \nattacking a random-phase-encoded optical cryptosystem [34] and blind reconstruction for \nsingle-pixel imaging [35]. A random-phase-encoded optical cryptosystem is a coherent \nimaging system with multiple diffractive optical elements such as lens and random phase \nmasks. The input plaintext light field is sequentially modulated by each phase mask in the \nforward propagation and it is finally transformed to a ciphertext light field as the system \noutput. The objective of attacking a optical cryptosystem is to recover the input image from \nthe given output light field if the encoding of all the phase masks is unknown. In single-\npixel imaging [35, 36], the object image is sequentially illuminated by different structured \nlight intensity patterns and the total light intensity of the entire object scene is recorded by \na single-pixel detector for each pattern. Finally, the object image can be computationally \nreconstructed when both the illumination patterns and single-pixel intensity sequence are \nknown. However, in a blind reconstruction [35], the objective is to recover the object image \nfrom the intensity sequence when the encoding of all the illumination patterns is unknown.  \nThe two systems [34,35] stated above are both linear and can be regarded as a black box \nwhen the encoding of elements (phase masks or illumination patterns) is unknown. The \nrandom-phase-encoded optical cryptosystem is usually coherent while the single-pixel \nimaging is usually incoherent. In the previous work [37], it is shown that a multiple-phase-\nmask diffractive system and a single-pixel imaging system are similar from several aspects \nsuch as performing optical pattern recognition. In the previous works [34,35], each system \nis modeled by a different deep learning network optimized with many pairs of input and \noutput training samples. Then the input image can be predicted by the network from an \narbitrary given output. Since there is a linear relationship between the input and output of \nthe two optical imaging systems mathematically, we point out that simple linear-regression-\nbased methods can produce the same results as deep learning. A linear regression scheme \ncan recover the object image more efficiently than deep learning for these two problems in \nsome extent. The advantages and disadvantages of linear-regression-based methods and \ndeep learning are analyzed and compared.  \nThis paper is structured as follows. The linear regression model is described in Section \n2. The two black-box optical imaging problems, i.e. attacking a random-phase-encoding-\nbased optical cryptosystem and blind reconstruction in single-pixel imaging, are described \nin Section 3 and Section 4. The results and discussions about the comparison between \nlinear-regression-based methods and deep learning are given in Section 5. A final \nconclusion is made in Section 6. \n2. Linear regression model \nFor a linear optical imaging system, both the input X and output Y can be denoted by a \ncolumn vector \n1\n2\nX = [x  x  \n x ]\nM\nL\n and \n1\n2\nY = [y  y  \n y ]\nN\nL\n. It is assumed that the input \nX has totally M pixels and the output Y has totally N pixels. The relationship between Y \nand X can be modeled as a matrix multiplication Y=WX , given by Eq. (1). The weighting \nmatrix W consisting of N\nM\n\n elements can be employed to model a black-box optical \nsystem. \n                      \n1\n1\n11\n1\n2\n2\n1\nM\nN\nNM\nN\nM\ny\nx\nw\nw\ny\nx\nw\nw\ny\nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n= \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL\nM\nO\nM\nM\nM\nL\n                  (1) \nWhen a large set of training samples (many pairs of X and Y) are available, the elements \nin the matrix W can be estimated by optimization if they are not given. The elements in W \ncan be iteratively optimized with a gradient descent algorithm. Initially, all the elements in \nW are set to be random values. Then each element in W can be updated in the following \nway \n[38] \nbased \non \nthe \ngradient \ndescent \nfor \neach \ntraining \nsample: \n'\n'\n(\n)\n (1\n,1\n)\nnm\nnm\nn\nn\nm\nw\nw\nr y\ny x\nm\nM\nn\nN\n=\n+\n−\n\n\n\n\n, where \n'\nny  denotes the actual output \ngenerated from the input of one training sample by multiplying the current W, \nny  denotes \nthe target output of one training sample and r denotes the pre-defined learning rate. If the \ninput values are complex-amplitude instead of real intensities, the algorithm needs to be \nslightly modified as: \n'\n'\n(\n)\n(\n) (1\n,1\n)\nnm\nnm\nn\nn\nm\nw\nw\nr y\ny conj x\nm\nM\nn\nN\n=\n+\n−\n\n\n\n\n, where \n( )\nconj\n denotes the conjugate of a complex value. After many iterations, the adaptively \noptimized W matrix multiplied with a given input will yield an output close to the target \none. A linear regression model can be considered as one-layer fully connected neural \nnetwork without nonlinear activation functions, shown in Fig. 1. In a true fully connected \nneural network shown in Fig. 1(b), both linear connections and nonlinear activation \nfunctions are densely interconnected in the form of multiple cascaded layers between the \nnetwork input and output. Modern deep learning networks such as the ones proposed in the \nprevious works [34,35] shown in Fig. 3 and Fig. 5 usually have even more complicated \nstructures than a fully connected neural network. Compared with a deep learning network, \na linear regression model has very low complexity.  \n \nFig. 1. (a) Linear regression model; (b) A fully connected neural network. \n3. Problem 1: attacking a random-phase-encoding-based optical \ncryptosystem \nAs proposed in many previous works [34,39,40], an optical image encryption system can \nbe constructed with an optical setup consisting of cascaded lens and random phase masks. \nTypical examples include Double Random Phase Encryption (DRPE) and Triple Random \nPhase Encryption (TRPE) [34]. In this work, the one with a more complicated structure, i.e. \na TRPE system, is considered and its optical setup is shown in Fig. 2. In a TRPE system, \nthe pixel intensities of the input light field represent the plaintext image O. Then the input \nlight field is optically Fourier transformed and inverse Fourier transformed with a double-\nlens 4f setup. The light field in the output plane becomes the ciphertext C. The plaintext \nimage can be decrypted from the ciphertext with the same setup by backward light field \npropagation. Three random phase masks R1, R2 and R3 are placed in the input plane, the \nFourier plane and the output plane respectively. The pixel values of all the phase masks are \nencoded as random phases between [0 2π]. The three phase masks serve as the encryption \nand decryption key. The mathematical model of TRPE encryption and decryption is given \nby Eqs. (2) and (3).   \n              \n(\n)\n(\n)\n(\n)\n(\n)\n1\n2\n3\nexp\nexp\nexp\nC\nIFT FT O\ni R\ni R\ni R\n\n\n=\n\n\n\n\n\n\n\n\n             (2) \n             \n(\n)\n(\n)\n\n\n(\n)\n3\n2\n1\nexp\nexp\nexp\nO\nIFT FT C\ni R\ni R\ni R\n=\n\n−\n\n−\n\n−\n\n\n\n\n           (3) \nwhere FT and IFT denotes Fourier transform and inverse Fourier transform, O denotes the \ninput plaintext image and C denotes the encrypted light field (ciphertext). \n \nFig. 2. Optical setup of a triple random phase encryption (TRPE) system. \nIdeally, the plaintext image O cannot be recovered from the ciphertext C if the key is \nnot known and the information security is protected in this way. However, the encryption \nsystem can be cracked by a known-plaintext attack (KPA) if the attacker collects an \nadequate number of plaintext-ciphertext pairs. In KPA, the objective is to recover the \nplaintext O from the corresponding ciphertext C without knowing R1, R2 and R3. The \nentire system is linear and the ciphertext can be regarded as the input vector X and the \nplaintext can be regarded as the output Y in the linear regression model described in Section \n2. The two-dimensional matrices C and O can be rearranged as one-dimensional vectors X \nand Y. Consequently, a KPA to a TRPE system can be implemented with complex-\namplitude linear regression (CLR), in addition to deep learning. In the previous work [34], \nthe deep learning network structure shown in Fig. 3, referred to as DecNet, was employed \nfor the KPA. In this work, CLR is compared with DecNet for the same KPA attack to a \nTRPE system.  \n \nFig. 3. Deep learning network for attacking a TRPE system proposed in the previous work \n[34] (DecNet). \n4. Problem 2: blind reconstruction in single-pixel imaging \nIn single-pixel imaging (SPI), the light intensity is recorded by a sensor containing only one \nsingle pixel, instead of a pixelated sensor array. A typical optical setup for a SPI system is \nshown in Fig. 4.  \n \nFig. 4. Optical setup of a single-pixel imaging system. \nThe two-dimensional object image \n( , )\nO x y  is sequentially illuminated by N varying \ntwo-dimensional structured light patterns \n( , )(1\n)\nnP x y\nn\nN\n\n\n and a single-pixel intensity \nsequence \n(1\n)\nnI\nn\nN\n\n\n will be recorded. Mathematically, each element in \nnI  is the \ninner product between \n( , )\nO x y  and each pattern in \n( , )\nnP x y . The object image \n( , )\nO x y  \ncan be computationally reconstructed when both the illumination pattern sequence \n( , )\nnP x y  and the recorded intensity sequence \nnI  are known. It is assumed that the total \nnumber of pixels in \n( , )\nO x y  and \n( , )\nnP x y  is M. The sampling ratio S can be defined as \nN/M. In single-pixel imaging, various kinds of algorithms can be employed to reconstruct \n( , )\nO x y  from \n( , )\nnP x y  and \nnI  [41]. However, all the illumination patterns \n( , )\nnP x y  \nare required to be known in these reconstruction algorithms. It is usually easier to \nreconstruct a high-quality object image when the sampling ratio S is higher. A blind \nreconstruction in SPI by deep learning was attempted in the previous work [35], where the \nobject image \n( , )\nO x y  is recovered from only \nnI  when the patterns \n( , )\nnP x y  are not \ngiven. The blind reconstruction in SPI is favorable for some applications such as scattering \nimaging [35, 42]. It is assumed that multiple pairs of different object images and single-\npixel intensities are given for the fixed illumination patterns, which can be used as training \nsamples in deep learning.  \nThe blind reconstruction in SPI essentially contains two steps: (a) Recovery of the \nunknown illumination patterns \n( , )\nnP x y  from the training samples. This is similar to the \nKPA in random-phase-encoding-based optical encryption described in Section 3; (b) Object \nimage reconstruction in SPI from a given \nnI  and the estimated \n( , )\nnP x y  obtained in Step \n(a). Step (a) is most critical and it is the key in the blind reconstruction. Once the patterns \ncan be approximately estimated and recovered in Step (a), Step (b) is simply conventional \nimage reconstruction in SPI and there are many different methods proposed in the past [41]. \nThe image reconstruction quality in Step (b) mainly depends on the accuracy of the \nestimated illumination patterns by linear regression in Step (a). The deep learning approach \nin the previous work [35] is end-to-end and both two steps are realized within the network \nshown in Fig. 5.  \n \nFig. 5. Deep learning network for blind image reconstruction in SPI proposed in the \nprevious work [35] (Wang’s Net). \nIn SPI, \n( , )\nO x y  and \nnI  have a linear mathematical relationship. The M pixels in \n( , )\nO x y  can be rearranged as the one-dimensional input vector X in Equation (1) and \nnI  \nis equivalent to the output vector Y in Eq. (1). All the N illumination patterns \n( , )\nnP x y  \nwill jointly constitute the weighting matrix W in Eq. (1) and each pattern corresponds to \none row in W. Consequently, the unknown illumination patterns can be recovered from the \ntraining samples by linear regression for Step (a). Then a compressive sensing scheme with \ntotal variation minimization [41,43,44] can be employed for image reconstruction in Step \n(b). No training samples are required for compressive sensing reconstruction since it is not \na machine learning process. Our proposed scheme is referred to as “Linear Regression + \nCompressive Sensing (LRCS)”. The LRCS scheme is compared with the deep learning \nnetwork proposed in the previous work [35], referee to as Wang’s Net. It shall be noted that \nno linear regression is performed to recover the illumination patterns in the previous work \n[35], even though compressive sensing is adopted for image reconstruction by assuming the \nillumination patterns are already known.  \n  \n5. Results and discussion \n5.1 Attacking a random-phase-encoding-based optical cryptosystem \nA complex-amplitude linear regression (CLR) is performed to crack a TRPE optical \ncryptosystem. For comparison, a DecNet [34] is constructed and the corresponding cracking \nresults are obtained as well. The size of plaintext image, ciphertext and random phase masks \nis 32 × 32 pixels. Plaintext images are randomly selected from the number-digit images \nin the MNIST dataset [45], the fashion product images in the Fashion-MNIST dataset [46] \nand natural object images in the CIFAR-100 dataset [47]. The color images in the CIFAR-\n100 dataset are converted to grayscale images. The output ciphertext light fields \ncorresponding to the plaintext images are generated from a simulated TRPE system. In the \ntraining, plaintext images are used as the target output and complex-amplitude ciphertexts \nare used as the input for both CLR and DecNet. Various number of training samples are \nattempted: 50, 100, 200, 500, 2000 and 5000. In addition, 200 samples randomly selected \nfrom each dataset different from the training samples are employed to test the attacking \ncapability of the CLR and DecNet after training. The peak-signal-to-noise-ratio (PSNR) \nbetween the original plaintext image and the recovered result from the ciphertext by these \ntwo methods is employed to evaluate their performance. \nIn CLR, the learning rate is 0.01 for the MNIST dataset and 0.001 for the Fashion-\nMNIST dataset and the CIFAR-100 dataset. The number of iterations is set to be 300. In \nDecNet, the learning rate is 0.0001 and the number of epochs is 20 for all the datasets. The \nresults of our complex-amplitude linear regression are compared with the ones using deep \nlearning [34] in Table 1, Table 2, Table 3, Fig. 6 and Fig. 7. The training time of CLR is \n825 seconds for 100 training samples and 4155 seconds for 500 training samples in a Matlab \nR2018a environment with Intel(R) Core(TM) i5-8400U CPU (2.80 GHz) and 8GB RAM. \nThe training time of DecNet is 54 seconds for 100 training samples, 203 seconds for 500 \ntraining samples, 804 seconds for 2000 training samples and 2016 seconds for 5000 training \nsamples under the Keras framework in a Python 3.5 environment. The training time of \nDecNet is generally shorter than CLR. The inference time of a trained model for predicting \na plaintext from a ciphertext is both within 0.1 second in CLR and DecNet.    \nFor the MNIST dataset and Fashion-MNIST dataset, it can be observed that the \nperformance of both methods will be improved as the number of training samples increases. \nHowever, CLR performs much better than DecNet when the number of training samples is \nsmall (e.g. from 50 to 500). The DecNet can only yield satisfactory output results when the \nnumber of training samples is at least 2000 or 5000. The results from CLR with 200 training \nsamples is close to the results from DecNet with 5000 samples. Evidently, CLR has \nsignificant advantages compared with DecNet in attacking a TRPE system when the \nnumber of training samples is inadequate. It can be observed from Fig. 6 that the recovered \nMNIST images by DecNet are contaminated with stripe noise and the recovered Fashion-\nMNIST images by DecNet are heavily blurred when the number of training samples is small. \nTheoretically, it is possible for a deep learning network like DecNet to accurately model a \nlinear system. However, the network may be overfitted at a local optimal point in the \ntraining when the number of training samples is small. Since the global optimal solution is \nnot reached, the network may yield unfavorable prediction results for the testing images. \nFor the CIFAR-100 dataset, the performance of CLR is close to that for the Fashion-\nMNIST dataset and the original plaintext images can be recovered with acceptable visual \nquality. But DecNet completely fails to recover the plaintext images even when the number \nof training samples is adequate (up to 5000). The CIFAR-100 dataset contains complicated \nnatural images from 100 different categories of objects. The MNIST dataset (or Fashion-\nMNIST dataset) only contains ten different kinds of simple number digit images (or fashion \nproduct images) and all the images have black background. It is easier for DecNet to extract \ncommon features from the MNIST or Fashion-MNIST images and perform plaintext \nrecovery from a ciphertext based on these features. But it is difficult to extract common \nfeatures from the diversified CIFAR-100 images for DecNet. CLR does not extract high-\nlevel features from the images so its performance will not vary a lot for different datasets.        \nIn the previous work [34], the DecNet can still work when only ciphertext intensities \nare available as the network input, instead of both ciphertext intensities and phases. \nHowever, CLR will not work in this situation since the input-output relationship is no longer \nlinear. This is one major limitation of CLR compared with DecNet. \nTable 1. Recovered plaintext image quality with CLR and DecNet (MNIST dataset) \nNumber of training samples K \nCLR \nDecNet \n50 \n17.4439 dB \n11.5822 dB \n100 \n19.4437 dB \n11.6669 dB \n200 \n22.4502 dB \n12.0497 dB \n500 \n27.2989 dB \n13.0242 dB \n2000 \n---- \n13.6974 dB \n5000 \n---- \n21.3453 dB \nTable 2. Recovered plaintext image quality with CLR and DecNet (Fashion-MNIST dataset) \nNumber of training samples K \nCLR \nDecNet \n50 \n15.8603 dB \n11.1401 dB \n100 \n17.7190 dB \n12.1886 dB \n200 \n19.8130 dB \n15.3303 dB \n500 \n22.9465 dB \n17.4687 dB \n2000 \n---- \n19.3498 dB \n5000 \n---- \n20.6077 dB \nTable 3. Recovered plaintext image quality with CLR and DecNet (CIFAR-100 dataset) \nNumber of training samples K \nCLR \nDecNet \n50 \n15.8243 dB \n7.4102 dB \n100 \n17.3781 dB \n8.3004 dB \n200 \n18.9631 dB \n10.1093 dB \n500 \n21.5758 dB \n11.8484 dB \n2000 \n---- \n12.7018 dB \n5000 \n---- \n10.7213 dB \n \n \nFig. 6. Comparison of recovered plaintext image results for a TRPE system with CLR and \nDecNet (MNIST dataset and Fashion-MNIST dataset). \n \nFig. 7. Comparison of recovered plaintext image results for a TRPE system with CLR and \nDecNet (CIFAR-100 dataset). \nThe similarities and differences between the proposed CLR scheme and the previously \nproposed DecNet for attacking a TRPE system are summarized in Table 4.  \nTable 4. Similarities and differences between CLR and DecNet for attacking a TRPE system \n \nComplex-amplitude Linear Regression (CLR) \nDecNet \nSimilarity \nPredict the plaintext image from the ciphertext light field for a TRPE system (or other similar \noptical cryptosystems) after the model is trained with a certain number of training samples  \nDifference \n(1) Work with a small number of training \nsamples \n(2)Simple explicit model with few parameters  \n(3) Not work for intensity-only ciphertext \n(4) Work for both simple images with black \nbackground and complicated natural images \n(1) Only work with a large number of training \nsamples \n(2) Complicated black-box model with many \nparameters for tuning \n(3) Work for intensity-only ciphertext \n(4) Work for simple images with black \nbackground, not work for complicated images \n5.2 Blind reconstruction in single-pixel imaging \nIn the simulation, the size of object image and each illumination pattern is 32 × 32 pixels. \nThe pixel intensity values in each illumination pattern are randomly distributed between 0 \nand 1. Four different numbers of illuminations, N=51, N=205, N=410 and N=1024 \ncorresponding to four different sampling ratios S=0.05, S=0.2, S=0.4 and S=1, are \nattempted. Various number of training images and 200 testing images are randomly selected \nfrom the MNIST dataset and the CIFAR-100 dataset. The single-pixel intensity values can \nbe obtained based on the SPI model described in Section 4. Both our proposed “linear \nregression + compressive sensing” (LRCS) scheme and Wang’s Net proposed in the \nprevious work [35] are implemented to recover the original object image. The optimization \nsolver for the compressive-sensed image reconstruction in LRCS is based on the work [43] \nfor the MNIST dataset and based on the work [41] for the CIFAR-100 dataset. In the linear \nregression step of LRCS, the learning rate is set to be 0.01 and the number of iterations is \n300 for all the cases. In the training of Wang’s Net, the learning rate is set to be 0.00001 \nand the number of epochs is 20 for all the cases. The average PSNR of the blindly \nreconstructed images from the simulated single-pixel intensity values for the 200 testing \nsamples is presented in Table 5, Table 6, Table 7 and Table 8. Some examples of the \nreconstructed images are shown in Fig. 8 and Fig. 9.  \nFor the results of the MNIST dataset in Table 5, Table 6 and Fig. 8, it can be observed \nthat the performance of LRCS will be enhanced as the sampling ratio increases and the \nnumber of training samples increases. The performance of deep learning will be \nsignificantly enhanced as the number of training samples increases but it will not be \nnecessarily improved as the sampling ratio increases. At a very low sampling ratio S=0.05, \nthe reconstructed images by LRCS are very heavily degraded but most reconstructed \nimages by deep learning still have acceptable visual quality when the number of training \nsamples are adequate. Since deep learning can extract some high-level common features \nfrom the training images, the test object image can still be well recovered from these \nfeatures when the sampling ratio is very low. On the other hand, the feature extraction and \nreconstruction may cause more unpredicted errors in the recovered images when the \ndimension of input data is higher. So the quality of recovered images will not always be \nworse at a lower sampling ratio and better at a higher sampling ratio for the deep learning \napproach.  \nTable 5. Blindly reconstructed image quality for SPI with LRCS (MNIST dataset) \nNumber of training samples K \nS=0.05 \nS=0.2 \nS=0.4 \nS=1 \n50 \n14.2419 dB \n16.8031 dB \n17.5811 dB \n18.3476 dB \n100 \n15.1527 dB \n18.7134 dB \n20.0936 dB \n21.3368 dB \n200 \n15.5376 dB \n20.1990 dB \n22.1549 dB \n24.2062 dB \n500 \n15.3003 dB \n21.0111 dB \n23.6042 dB \n26.4277 dB \nTable 6. Blindly reconstructed image quality for SPI with Wang’s Net (MNIST dataset) \nNumber of training samples K \nS=0.05 \nS=0.2 \nS=0.4 \nS=1 \n50 \n11.9738 dB \n11.6227 dB \n12.2526 dB \n11.5919 dB \n100 \n12.8692 dB \n12.6286 dB \n12.8774 dB \n12.6637 dB \n200 \n13.0954 dB \n13.2059 dB \n13.2644 dB \n13.1486 dB \n500 \n13.2826 dB \n13.3295 dB \n13.3089 dB \n13.2944 dB \n2000 \n15.5567 dB \n14.7390 dB \n17.1544 dB \n15.2462 dB \n5000 \n17.3145 dB \n19.3743 dB \n20.1935 dB \n19.7935 dB \n \nFig. 8. Comparison of reconstructed image results for a SPI system with LRCS and deep \nlearning in the simulation (MNIST dataset). \nFig. 8 shows that some reconstructed images by LRCS have quality degradation but the \nshapes of the digits match with the original groundtruth MNIST images. On the other hand, \nsome reconstructed images by Wang’s Net can be noise-free but the digits have distorted \nshapes, which will cause a lower PSNR. From the results, it can be observed that the \nrecovered image quality of LRCS with 200 training samples is comparable with the ones \nusing deep learning with 5000 samples, except when the sampling ratio is very low. \nFrom the results of the CIFAR-100 dataset in Table 7, Table 8 and Fig. 9, it can be \nobserved that the performance of LRCS is not as good as the one for the MNIST dataset \nbut the object image can still be reconstructed with high fidelity under proper conditions. \nOn the other hand, Wang’s Net fails to work for the complicated natural object images in \nthe CIFAR-100 dataset, regardless of sampling ratios and number of training samples. \nSimilar to the DecNet, it is also hard for Wang’s Net to extract high-level common features \nfrom the CIFAR-100 images to reconstruct high-quality results. LRCS performs \nsignificantly better than Wang’s Net for the CIFAR-100 dataset. Even though DecNet and \nWang’s Net both demonstrate poor performances for complicated natural object images, it \nis possible that other different deep learning models may work for these images in the two \nblack-box imaging tasks.      \nTable 7. Blindly reconstructed image quality for SPI with LRCS (CIFAR-100 dataset) \nNumber of training samples K \nS=0.05 \nS=0.2 \nS=0.4 \nS=1 \n200 \n14.7771 dB \n16.4979 dB \n17.1860 dB \n17.3206 dB \n500 \n15.0334 dB \n17.6346 dB \n18.8980 dB \n19.2882 dB \n1000 \n14.9699 dB \n18.0890 dB \n20.1176 dB \n21.4797 dB \nTable 8. Blindly reconstructed image quality for SPI with Wang’s Net (CIFAR-100 dataset) \nNumber of training samples K \nS=0.05 \nS=0.2 \nS=0.4 \nS=1 \n200 \n10.2119 dB \n10.8575 dB \n10.6441 dB \n10.6401 dB \n500 \n10.9371 dB \n11.1875 dB \n11.0173 dB \n11.0095 dB \n1000 \n10.8714 dB \n11.1994 dB \n10.9367 dB \n10.9383 dB \n5000 \n12.6881 dB \n13.3186 dB \n12.2194 dB \n11.0790 dB \n \n \nFig. 9. Comparison of reconstructed image results for a SPI system with LRCS and deep \nlearning in the simulation (CIFAR-100 dataset). \nTable 9. Similarities and differences between LRCS and Wang’s Net for blind reconstruction in SPI \n \nLinear Regression + Compressive Sensing \n(LRCS) \nDeep learning (Wang’s Net) \nSimilarity \nBlindly reconstruct the object image from the single-pixel intensity sequence at different \nsampling ratios for a SPI system after the model is trained with a certain number of training \nsamples, when the illumination patterns are not known \nDifference \n(1) Work with a small number of training \nsamples \n(2)Simple explicit model with few parameters \n(3)Two-step, not end-to-end \n(4)Poor performance at a very low sampling \nratio  \n(5)Work for both simple images with black \nbackground and complicated natural images \n(1) Only work with a large number of training \nsamples \n(2) Complicated model with many parameters \nfor tuning \n(3)End-to-end reconstruction \n(4) Possibly reconstruct high-quality results at \na very low sampling ratio  \n(5)Work for simple images with black \nbackground, not work for complicated images \nIt takes about 725 seconds for 200 training samples and 1882 seconds for 500 training \nsamples to train the linear regression part in the LRCS scheme (S=1), with the same \nhardware and software configuration in Section 5.1. When the sampling ratio S is lower, \nthe training time will be reduced by being multiplied with S. In contrast, it takes about 790 \nseconds for 200 training samples, 1940 seconds for 500 training samples, 8060 seconds for \n2000 training samples and 20591 seconds for 5000 training samples to train Wang’s Net. \nThe training time of Wang’s Net will not be evidently reduced if the sampling ratio becomes \nlower. LRCS is generally more computationally efficient than Wang’s Net in the training \nstep. Wang’s Net is less efficient than DecNet in the training because the latter one mainly \nconsists of convolutional layers while the former one consists of several fully connected \nlayers. The inference time of a trained Wang’s Net to reconstruct a testing image is within \n0.1 second. The reconstruction time of LRCS for a testing image will be around 0.3 to 0.5 \nsecond, which is relatively longer since the optimization step in compressive sensing \nrequires certain computational cost. The similarities and differences between LRCS and \ndeep learning from different perspectives are summarized in Table 9. \nIn this work, LRCS and Wang’s Net are evaluated based on the experimentally recorded \ndata as well. The SPI experiments are conducted using the optical setup shown in Fig. 10. \nEach object image is printed on a paper card and illuminated by the patterns projected by a \nJmGO G3 projector. The single-pixel intensity values are recorded by a Thorlabs FDS1010 \nphotodiode detector and a NI USB-6216 data acquisition card. Totally ten different object \nimages are tested in the experiment. \n \nFig. 10. Optical setup of our SPI experiment. \nThe reconstruction results with LRCS and Wang’s Net from the experimentally \nrecorded data are shown in Fig. 11. It is reported in the previous work [35] that deep \nlearning is significantly more robust to the noise in the experimental data. Since the optical \nsetup in this work is different from the one in the previous work [35], the type of noise and \nits strength can be different in the experiment. For example, no laser illumination is \nemployed in this work and the speckle noise contamination will not occur. In our \nobservation, the performances of both LRCS and Wang’s Net are slightly degraded due to \nthe extra experimental noise that do not appear in the simulated training data. But it is still \nevident that Wang’s Net performs better than LRCS at a low sampling ratio and LRCS \nperform better than Wang’s Net when the number of training samples is small. \n \nFig. 11. Comparison of reconstructed image results with LRCS and Wang’s Net for a SPI \nsystem based on the recorded data in real optical experiments. \n6. Conclusion \nIn this work, we point out that linear-regression-based methods can be used to solve two \nblack-box optical imaging problems that were previously addressed by deep learning \napproaches. For attacking a TRPE optical cryptosystem, a complex-amplitude linear \nregression (CLR) scheme is proposed. For the blind image reconstruction in a SPI system, \na “linear regression + compressive sensing (LRCS)” scheme is proposed. In these two \nproblems, linear-regression-based methods show some advantages than deep learning such \nas being applicable to a small number of training samples and complicated natural object \nimages. Simulation and experimental results indicate that deep learning does not always \noutperform linear regression in this type of black-box optical imaging problems and each \napproach has its own advantages and disadvantages. Compared with linear regression, \nnonlinear deep learning models have advantages of recovering the original images when \nthe given information is incomplete (e.g. only ciphertext intensity is known for TRPE or \nthe sampling ratio is very low in SPI). The similarities and differences between linear-\nregression-based methods and deep learning are analyzed and summarized.     \nFunding \nNational Natural Science Foundation of China (61805145, 11774240); Leading Talents \nProgram of Guangdong Province (00201505); Natural Science Foundation of Guangdong \nProvince (2016A030312010). \nAcknowledgement \nWe would like to thank Dr. Fei Wang, Mr. Han Hai and Prof. Wenqi He’s assistance in this \nwork. \nDisclosures  \nThe authors declare no conflicts of interest. \nReference \n1. \nK. Yao, R. Unni, and Y. Zheng, “Intelligent nanophotonics: merging photonics and artificial intelligence at \nthe nanoscale,” Nanophotonics 8(3), 339-366 (2019). \n2. \nS. D. Campbell, D. Sell, R. P. Jenkins, E. B. Whiting, J. A. Fan, and D. H. Werner, “Review of numerical \noptimization techniques for meta-device design,” Opt. Mater. Express 9(4), 1842-1863 (2019). \n3. \nG. Barbastathis, A. Ozcan, and G. Situ, “On the use of deep learning for computational imaging,” Optica \n6(8), 921-943 (2019). \n4. \nA. Sinha, J. Lee, S. Li, and G. Barbastathis, “Lensless computational imaging through deep learning,” \nOptica 4(9), 1117-1125 (2017). \n5. \nA. Lucas, M. Iliadis, R. Molina, and A. K. Katsaggelos, “Using deep neural networks for inverse problems \nin imaging: beyond analytical methods,” IEEE Sig. Process. Mag. 35(1), 20-36 (2018). \n6. \nÇ. Işıl, Figen S. Oktem, and Aykut Koç, \"Deep iterative reconstruction for phase retrieval,\" Appl. Opt. \n58(20), 5422-5431 (2019) \n7. \nG. Zhang, T. Guan, Z. Shen, X. Wang, T. Hu, D. Wang, Y. H. and N. Xie, “Fast phase retrieval in off-axis \ndigital holographic microscopy through deep learning,” Opt. Express 26(15), 19388-19405 (2018). \n8. \nY. Rivenson, Z. Göröcs, H. Günaydin, Y. Zhang, H. Wang, and A. Ozcan, “Deep learning microscopy,” \nOptica 4(11), 1437-1443 (2017). \n9. \nB. Manifold, E. Thomas, A. T. Francis, A. H. Hill, and D. Fu, “Denoising of stimulated Raman scattering \nmicroscopy images via deep learning,” Biomed. Opt. Express 10(8), 3860-3874 (2019). \n10. M. Lyu, H. Wang, G. Li, S. Zheng, and G. Situ, “Learning-based lensless imaging through optically thick \nscattering media,” Adv. Photon. 1(3), 036002 (2019). \n11. Y. Li, Y. Xue, and L. Tian, “Deep speckle correlation: a deep learning approach toward scalable imaging \nthrough scattering media,” Optica 5, 1181–1190 (2018). \n12. Y. Rivenson, Y. Zhang, H. Günaydın, D. Teng, and A. Ozcan, “Phase recovery and holographic image \nreconstruction using deep learning in neural networks,” Light Sci. Appl. 7(2), 17141 (2018). \n13. \nH. Wang, M. Lyu, and G. Situ, “eHoloNet: a learning-based end-to-end approach for in-line digital \nholographic reconstruction,” Opt. Express 26(18), 22603–22614 (2018). \n14. \nZ. Ren, Z. Xu, and E. Y. Lam, “End-to-end deep learning framework for digital holographic \nreconstruction,” Adv. Photon. 1(1), 016004 (2019). \n15. Z. Ren, Z. Xu, and E. Y. Lam, “Learning-based nonparametric autofocusing for digital holography,” \nOptica 5(4), 337-344 (2018). \n16. \nT. Pitkäaho, A. Manninen, and T. J. Naughton, “Focus prediction in digital holographic microscopy using \ndeep convolutional neural networks,” Appl. Opt. 58(5), A202-A208 (2019). \n17. S. Jiao, Z. Jin, C. Chang, C. Zhou, W. Zou, and X. Li, “Compression of phase-only holograms with JPEG \nstandard and deep learning,” Appl. Sci. 8(8), 1258 (2018). \n18. T. Shimobaba, D. Blinder, M. Makowski, P. Schelkens, Y. Yamamoto, I. Hoshi, T. Nishitsuji, Y. Endo, T. \nKakue, and T. Ito, “Dynamic-range compression scheme for digital hologram using a deep neural \nnetwork,” Opt. Lett. 44(12), 3038-3041 (2019). \n19. T. Shimobaba, Y. Endo, T. Nishitsuji, T. Takahashi, Y. Nagahama, S. Hasegawa, M. Sano, R. Hirayama, \nT. Kakue, A. Shiraki, and T. Ito, “Computational ghost imaging using deep learning,” Opt. Commun. 413, \n147-151 (2018). \n20. C. F. Higham, R. Murray-Smith, M. J. Padgett, and M. P. Edgar, “Deep learning for real-time single-pixel \nvideo” Sci. Rep. 8(1), 2369 (2018). \n21. Z. Ren, H. K. H. So, and E. Y. Lam, “Fringe Pattern Improvement and Super-Resolution Using Deep \nLearning in Digital Holography,” IEEE Trans. Ind. Inform., 1-1 (2019). \n22. Z. Niu, J. Shi, L. Sun, Y. Zhu, J. Fan, and G. Zeng, “Photon-limited face image super-resolution based on \ndeep learning,” Opt. Express 26(18), 22773-22782 (2018). \n23. Z. Luo, A. Yurt, R. Stahl, A. Lambrechts, V. Reumers, D. Braeken, and L. Lagae, “Pixel super-resolution \nfor lens-free holographic microscopy using deep learning neural networks,” Opt. Express 27(10), 13581-\n13595 (2019). \n24. Ç. Işil, M. Yorulmaz, B. Solmaz, A. B. Turhan, C. Yurdakul, S. Ünlü, E. Ozbay, and A. Koç, \"Resolution \nenhancement of wide-field interferometric microscopy by coupled deep autoencoders,\" Appl. Opt. 57(10), \n2545-2552 (2018). \n25. T. Nguyen, Y. Xue, Y. Li, L. Tian, and G. Nehmetallah, “Deep learning approach for Fourier \nptychography microscopy,” Opt. Express 26(20), 26470–26484 (2018). \n26. S. Jiang, K. Guo, J. Liao, and G. Zheng, “Solving Fourier ptychographic imaging problems via neural \nnetwork modeling and TensorFlow Biomed,” Opt. Express 9(7), 3306–3319 (2018). \n27. Y. F. Cheng, M. Strachan, Z. Weiss, M. Deb, D. Carone, and V. Ganapati, “Illumination pattern design \nwith deep learning for single-shot Fourier ptychographic microscopy,” Opt. Express 27(2), 644-656 \n(2019). \n28. S. Feng, Q. Chen, G. Gu, T. Tao, L. Zhang, Y. Hu, W. Yin and C. Zuo, “Fringe pattern analysis using deep \nlearning,” Adv. Photon. 1(2), 025001 (2019). \n29. K. Wang, Y. Li, Q. Kemao, J. Di, and J. Zhao, “One-step robust deep learning phase unwrapping,” Opt. \nExpress 27(10), 15100-15115 (2019). \n30. Q. Xin, G. Ju, C. Zhang, and S. Xu, “Object-independent image-based wavefront sensing approach using \nphase diversity images and deep learning,” Opt. Express 27(18), 26102-26119 (2019). \n31. Y. Nishizaki, M. Valdivia, R. Horisaki, K. Kitaguchi, M. Saito, J. Tanida, and E. Vera, “Deep learning \nwavefront sensing,” Opt. Express 27(1), 240-251 (2019). \n32. B. Karanov, M. Chagnon, F. Thouin, T. A. Eriksson, H. Bülow, D. Lavery, P. Bayvel, and L. Schmalen, \n“End-to-end deep learning of optical fiber communications,” J. Lightwave Technol. 36(20), 4843-4855 \n(2018). \n33. M. Hutson, “AI researchers allege that machine learning is alchemy,” Science 360(6388), 861 (2018). \n34. H. Hai, S. Pan, M. Liao, D. Lu, W. He, and X. Peng, “Cryptanalysis of random-phase-encoding-based \noptical cryptosystem via deep learning,” Opt. Express 27(15), 21204-21213 (2019). \n35. F. Wang, H. Wang, H. Wang, G. Li, and G. Situ, “Learning from simulation: An end-to-end deep-learning \napproach for computational ghost imaging,” Opt. Express 27(18), 25560-25572 (2019). \n36. M. P. Edgar, G. M. Gibson, and M. J. Padgett, “Principles and prospects for single-pixel imaging,” Nat. \nPhotonics 13(1), 13–20(2019). \n37. S. Jiao, J. Feng, Y. Gao, T. Lei, Z. Xie, and X. Yuan, “Optical machine learning with incoherent light and \na single-pixel detector,” Opt. Lett. 44(21), 5186-5189 (2019). \n38. G. A. Seber, and A. J. Lee, \"Linear regression analysis,\" John Wiley & Sons 329(2012). \n39. S. Liu, C. Guo, and J. T. Sheridan, “A review of optical image encryption techniques,” Opt. Laser Technol. \n57, 327-342 (2014). \n40. S. Jiao, C. Zhou, Y. Shi, W. Zou, and X. Li, “Review on optical image hiding and watermarking \ntechniques,” Optics. Laser Technol. 109, 370-380 (2019). \n41. L. Bian, J. Suo, Q. Dai, and F. Chen, “Experimental comparison of single-pixel imaging algorithms,” J. \nOpt. Soc. Am. A 35(1), 78-87 (2018). \n42. E. Tajahuerce, V. Durán, P. Clemente, E. Irles, F. Soldevila, P. Andrés, and J. Lancis, “Image transmission \nthrough dynamic scattering media by single-pixel photodetection,” Opt. Express 22(14), 16945-16955 \n(2014). \n43. C. Li, W. Yin, and Y. Zhang, “User’s guide for TVAL3: TV minimization by augmented lagrangian and \nalternating direction algorithms,” CAAM report 20(46-47), 4 (2009) \n44. M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T. Sun, K. F. Kelly, and R. G. Baraniuk, “ Single-\npixel imaging via compressive sampling,” IEEE Sig. Process. Mag. 25(2), 83-91 (2008). \n45. Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document \nrecognition,” Proc. IEEE 86(11), 2278-2324 (1998). \n46. H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine \nlearning algorithms,” arXiv preprint arXiv:1708.07747 (2017). \n47. A. Krizhevsky, and G. Hinton, \"Learning multiple layers of features from tiny images\", Technical report, \nUniversity of Toronto 1(7), pp. 7 (2009). \n",
  "categories": [
    "cs.CV",
    "eess.IV"
  ],
  "published": "2019-10-31",
  "updated": "2020-01-17"
}