{
  "id": "http://arxiv.org/abs/1911.01098v1",
  "title": "Emergence of Numeric Concepts in Multi-Agent Autonomous Communication",
  "authors": [
    "Shangmin Guo"
  ],
  "abstract": "With the rapid development of deep learning, most of current state-of-the-art\ntechniques in natural langauge processing are based on deep learning models\ntrained with argescaled static textual corpora. However, we human beings learn\nand understand in a different way. Thus, grounded language learning argues that\nmodels need to learn and understand language by the experience and perceptions\nobtained by interacting with enviroments, like how humans do. With the help of\ndeep reinforcement learning techniques, there are already lots of works\nfocusing on facilitating the emergence of communication protocols that have\ncompositionalities like natural languages among computational agents\npopulation. Unlike these works, we, on the other hand, focus on the numeric\nconcepts which correspond to abstractions in cognition and function words in\nnatural language. Based on a specifically designed language game, we verify\nthat computational agents are capable of transmitting numeric concepts during\nautonomous communication, and the emergent communication protocols can reflect\nthe underlying structure of meaning space. Although their encodeing method is\nnot compositional like natural languages from a perspective of human beings,\nthe emergent languages can be generalised to unseen inputs and, more\nimportantly, are easier for models to learn. Besides, iterated learning can\nhelp further improving the compositionality of the emergent languages, under\nthe measurement of topological similarity. Furthermore, we experiment another\nrepresentation method, i.e. directly encode numerals into concatenations of\none-hot vectors, and find that the emergent languages would become\ncompositional like human natural languages. Thus, we argue that there are 2\nimportant factors for the emergence of compositional languages.",
  "text": "Emergence of Numeric Concepts\nin Multi-Agent Autonomous\nCommunication\nShangmin Guo\nMaster of Science\nData Science\nSchool of Informatics\nUniversity of Edinburgh\n2019\narXiv:1911.01098v1  [cs.CL]  4 Nov 2019\nAbstract\nNatural language understanding is a long-standing topic in artiﬁcial intelligence. With\nthe rapid development of deep learning, most of current state-of-the-art techniques\nin natural langauge processing are based on deep learning models trained with large-\nscaled static textual corpora. However, we human beings learn and understand in a\ndifferent way. Thus, grounded language learning argues that models need to learn and\nunderstand language by the experience and perceptions obtained by interacting with\nenviroments, like how humans do.\nWith the help of deep reinforcement learning techniques, there are already lots\nof works focusing on facilitating the emergence of communication protocols that have\ncompositionalities like natural languages among computational agents population. Un-\nlike these works, we, on the other hand, focus on the numeric concepts which corre-\nspond to abstractions in cognition and function words in natural language.\nBased on a speciﬁcally designed language game, we verify that computational\nagents are capable of transmitting numeric concepts during autonomous communi-\ncation, and the emergent communication protocols can reﬂect the underlying structure\nof meaning space. Although their encodeing method is not compositional like natural\nlanguages from a perspective of human beings, the emergent languages can be gener-\nalised to unseen inputs and, more importantly, are easier for models to learn. Besides,\niterated learning can help further improving the compositionality of the emergent lan-\nguages, under the measurement of topological similarity. Furthermore, we experiment\nanother representation method, i.e. directly encode numerals into concatenations of\none-hot vectors, and ﬁnd that the emergent languages would become compositional\nlike human natural languages. Thus, we argue that there are 2 important factors for\nthe emergence of compositional languages: i) input feature representations are inher-\nently disentangled; ii) effective methods to amplify compositional inductive bias, e.g.\niterated learning.\ni\nAcknowledgements\nThroughout the writing of this dissertation, I have received a great deal of support and\nassistance from many people.\nFirst of all, I would like to express my deepest appreciation to my supervisors, Dr.\nIvan Titov and Prof. Kenny Smith, for their support for the proposal of this project and\nalso their patient guidance, encouragement and advices. I am extremely lucky to have\nsupervisors who cared so much about this work and proposed so much insights about\nthe research topic.\nI would also like to extend my deepest gratitude to my tutors, Mr. Serhii Havrylov\nand Dr. Stella Frank, whose expertise were invaluable in the formulating of the re-\nsearch topic and methodologies in particular. Without their insights and sagacities,\ncompleting this work would be much more difﬁcult.\nIn particular, I greatly appreciate the support received through the collaborative\nwork undertaken with Joshua Ren for the iterated learning part in this work as well as\nhis insights for discussions about explaining the phenomena in experiments.\nI am deeply indebted to all my friends in Edinburgh who opened their homes to\nme during my time at University of Edinburgh and who were always so helpful in\nnumerous ways. Special thanks to Yan Yang, Yiyun Jin, Ruochun Jin, Muyang Liu,\nWenbin Hu, Yuanhao Li and Jie Zhou.\nI am extremely grateful to my beloved girlfriend, Siting Lu. Thank you for sup-\nporting me for everything and especially I cannot thank you enough for being with me\nthroughout this experience. Thank you for all your companionship and care to me.\nI cannot begin to express my heartfelt thank you to my Mum and Dad for always\nbelieving in me and encouraging me to follow my dreams as well as for helping in\nwhatever way they could during this challenging period.\nii\nTable of Contents\n1\nIntroduction\n1\n2\nBackground\n3\n2.1\nComputer Simulation Methods in Evolutionary Linguistics . . . . . .\n3\n2.2\nMulti-agent Games in Grounded Language Learning\n. . . . . . . . .\n4\n3\nGame, Models and Metrics\n6\n3.1\nGame Description . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3.1.1\nGame Procedure\n. . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.1.2\nFunctions of Numerals in the Game . . . . . . . . . . . . . .\n8\n3.1.3\nA Variant: Set-Select Game\n. . . . . . . . . . . . . . . . . .\n8\n3.2\nProposed Models . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.2.1\nSpeaker . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.2.2\nListener . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.2.3\nLoss/Reward and Learning . . . . . . . . . . . . . . . . . . .\n11\n3.2.4\nNeural Iterated Learning . . . . . . . . . . . . . . . . . . . .\n12\n3.2.5\nBaseline Models\n. . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.3\nCompositionality and Metrics . . . . . . . . . . . . . . . . . . . . . .\n13\n4\nExperiment Results and Discussion\n16\n4.1\nEmergence of Language without Iterated Learning\n. . . . . . . . . .\n16\n4.2\nStructure of Emergent Language . . . . . . . . . . . . . . . . . . . .\n17\n4.2.1\nEmergent Languages in Various Games . . . . . . . . . . . .\n17\n4.2.2\nTopological Similarities\n. . . . . . . . . . . . . . . . . . . .\n19\n4.2.3\nSigniﬁcance Test of Same Numeric Concepts . . . . . . . . .\n20\n4.2.4\nGeneralisation of Emergent Language . . . . . . . . . . . . .\n22\n4.2.5\nSection Conclusion . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.3\nLearning Speed & Iterated Learning . . . . . . . . . . . . . . . . . .\n24\niii\n4.3.1\nFor Listener . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.3.2\nFor Speaker . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n4.3.3\nImprovement by Iterated Learning . . . . . . . . . . . . . . .\n27\n4.4\nEffects of Different Representations . . . . . . . . . . . . . . . . . .\n27\n4.5\nFurther Discussion\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5\nConclusions\n33\n5.1\nExpress Numeric Concepts . . . . . . . . . . . . . . . . . . . . . . .\n33\n5.2\nRole of Iterated Learning . . . . . . . . . . . . . . . . . . . . . . . .\n33\n5.3\nFuture Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nBibliography\n35\niv\nChapter 1\nIntroduction\nNatural language processing (NLP) is an important and long-standing topic in artiﬁcial\nintelligence (AI), in which a core question is natural language understanding (NLU).\nWith the rapid development of deep learning (DL), most current state-of-the-art meth-\nods in NLP, e.g. [Socher et al., 2013, Mikolov et al., 2013, Kim, 2014], are based on\nDL models trained on massive static textual corpora. From an information processing\nperspective, the information ﬂow of NLP-based human-computer interaction systems\nis illustrated in Figure 1.1 given as follow. As the diagram shows, the input of NLP\nsystems are various kinds of textual materials generated by human beings to describe\ntheir experiences and perceptions. Under such a perspective, symbols in natural lan-\nguages are actually feature representations of the original experiences and perceptions,\nwhereas most current NLP systems directly take these symbols as original features.\nMassive Static \nTextual Copora\nNLP\nSystem\nTasks\nFigure 1.1: An overview of information ﬂow in current NLP systems.\nConsidering the missing original experiences and perceptions, grounded language\nlearning (GLL) argues that models need a grounded environment to learn and under-\nstand language[Matuszek, 2018]. However, natural languages of the time have been\ndeveloped for at least tens of thousands of years[Berwick and Chomsky, 2016] and\nalready became very sophisticated. Thus, to verify that computational agents can truly\n1\nChapter 1. Introduction\n2\nground symbols in natural languages to corresponding experiences and perceptions,\nas well as be able to complete the speciﬁed tasks, it is necessary to help them to\ndiscover and develop various kinds of characteristics of natural language during au-\ntonomous communication of agents. There are already lots of works, e.g. [Batali,\n1998, Christiansen and Kirby, 2003, Smith et al., 2003a, Hill et al., 2017, Havrylov\nand Titov, 2017, Yu et al., 2018, Kottur et al., 2017], aiming to facilitate the emergence\nof “natural language” in multi-agent autonomous communication systems. However,\none signiﬁcant limitation of previous works is that, only referential objects/attributes in\nenvironments, e.g. shapes and colours, were considered and to which discrete symbols\nwere grounded.\nThis project, on the other hand, aims to explore and analyse the grounding of ab-\nstractions which are non-referential1 in the original experiences and perceptions of\nhuman beings. However, as it is too huge a topic to tackle, our project is limited to nu-\nmeric concepts which correspond to cardinal numerals in natural languages for the fol-\nlowing reasons: i) numeral systems are relatively simple and self-contained[Hurford,\n1999]; ii) concepts related to cardinal numerals are more straightforward to model\nwith numeric representations; iii) functions of emergent cardinal numerals can be for-\nmalised and veriﬁed more reliably in simulation.\nIn this work, our main contributions are given as follows:\n1. We propose a language game in which we can verify whether computational\nagents can communicate numeric concepts with each other, and successfully\ntrain agents to “invent” communication protocols that can autonomously trans-\nmit numeric concepts.\n2. We further analyse and discuss the structure of the emergent communication\nprotocols, and improve the compositionality by transforming iterated leaning\nproposed by [Smith et al., 2003b] to train the DL models.\n3. We compare learning speeds of various kinds of languages as well as different\nrepresentations, and propose an alternative hypothesis for explaining the emer-\ngence of words with different types and functions.\nAll our codes are released at https://github.com/Shawn-Guo-CN/EmergentNumerals.\n1By non-referential, we mean that “there is no concrete entity in the world (real or virtual) can be\nreferred as”.\nChapter 2\nBackground\nThere are 2 almost disjointly developed research topics that motivates this project,\ni.e. computer simulation methods in evolutionary linguistics and multi-agent games\nin grounded language learning. Thus, in the following 2 sections, we will introduce\nworks which are highly related to our project from these 2 different areas.\n2.1\nComputer Simulation Methods in Evolutionary Lin-\nguistics\nOne important issue in the ﬁeld of evolutionary linguistics is to use quantitative meth-\nods to overcome the time limit on unpreserved pre-historic linguistic behaviours [Lieber-\nman, 2006, Evans and Levinson, 2009]. Since it was ﬁrst introduced by [Hurford,\n1989], computer simulation method has attracted a rapidly growing attention, e.g.\n[Hurford et al., 1998, Knight et al., 2000, Briscoe, 2002, Christiansen and Kirby,\n2003, Bickerton and Szathm´ary, 2009, Cangelosi and Parisi, 2012]. As introduced\nin Chapter 1, one of our objectives is to facilitate the discovery and development of\nvarious kinds of natural language phenomena of computational agents, which shares\na same objective and motivation of computer simulation methods in evolutionary lin-\nguistics.\nTo imply and verify a linguistic theory, there are 2 necessary component: i) envi-\nronments, in which agents can execute actions and communicate with each other; ii)\npre-deﬁned elementary linguistic knowledge that can be manipulated and altered by\nagents. Further, we could categorise the environments into the following 3 different\ntypes according to their simulation objectives:\n3\nChapter 2. Background\n4\n• Iterated learning introduced by [Kirby, 1999] which aims at simulating cultural\ntransitions from generation to generation.\n• Language games introduced by [Wittgenstein, 1953] which takes the emergent\ncommunication protocols in cooperation between individuals as a prototype of\nlanguage.\n• Genetic evolution introduced by [Briscoe, 1998] which aims at simulating evo-\nlution of languages as a kind of natural selection procedure[Darwin, 1859].\nWith environments and pre-deﬁned elementary linguistic knowledge, computa-\ntional agents can then learn bi-directional meaningutterance mapping functions[Gong\nand Shuai, 2013]. With different kinds of resulting linguistic phenomena, this simula-\ntion procedure can be broadly categorised into 2 classes:\n• lexical models, e.g. [Steels, 2005, Baronchelli et al., 2006, Puglisi et al., 2008],\nwhose main concern is whether a common lexicon can form during the commu-\nnication in agent community;\n• syntactic and grammatical models, e.g. [Kirby, 1999, Vogt, 2005], in which\nagents mainly aim to map meanings (represented in various ways) to utterances\n(either structured or unstructured ).\nHowever, no matter how these mapping functions are learnt, e.g. by neural net-\nwork models [Munroe and Cangelosi, 2002] or by mathematical equations [Minett\nand Wang, 2008, Ke et al., 2008], the most basic elements of linguistics, e.g. meanings\nto communicate about and a signalling channel to employ, are all pre-deﬁned.\nIn contract, although we also follow the framework of language games and train\nagents in an iterated learning fashion, the basic linguistic elements in our project are\nnot provided from the outset any more and computational agents can specify meanings\nof symbols/utterances by themselves.\n2.2\nMulti-agent Games in Grounded Language Learn-\ning\nUnlike how we human beings learn and understand language, the current DL-based\nNLP techniques learn semantics from only large-scaled static textual materials. Thus,\nChapter 2. Background\n5\ngrounded language learning argues that computational agents also need to learn and\nunderstand languages by interacting with environments and grounding language into\ntheir experience and perceptions. With the recent rapid development of deep rein-\nforcement learning (DRL), it has been shown that computational agents can master a\nvariety of complex cognitive activities, e.g. [Mnih et al., 2015, Silver et al., 2017].\nThus, several papers in grounded language learning apply DRL techniques to allow\nagents to learn or invent natural languages1, such as [Hermann et al., 2017, Mordatch\nand Abbeel, 2018, Havrylov and Titov, 2017, Hill et al., 2017].\nTo verify language abilities of computational agents, previous works in grounded\nlanguage learning usually follow the framework of language games, of which are\nmainly variants of referential games introduced by [Lewis, 1969], e.g. [Hermann et al.,\n2017, Havrylov and Titov, 2017]. Also, some works are more motivated by the com-\npetition instead of cooperation such as [Cao et al., 2018].\nFrom another perspective, based on the number of participated agents, we can\nbroadly categorise language games in GLL into the following 2 types:\n• Single-agent games usually need to be done by one agent and a human partici-\npator, in which the main concern is to explore how computational agents could\nlearn the compositionality of semantics.\n• Multi-agent games are usually completed by an agent population, in which the\nmain concern is to explore how various kinds of natural language phenomena\nemerge and evolve during communicating among agents.\nHowever, like we mentioned in Chapter 1, whichever kind of language game they\nfollow in previous works of grounded language learning, the objects/attributes the sym-\nbols grounded to are all referential. We, on the other hand, aim to explore the feasi-\nbility of grounding symbols to non-referential objects (speciﬁcally, numeric concepts)\nduring the game.\n1Strictly speaking, “invent natural language” should be called as “invent communication protocols\nsharing compositionality like natural languages”. However, as our project is to facilitate composition-\nality in multi-agent communication protocols, we thus call these emergent communication protocols a\nkind of “language” invented by agents\nChapter 3\nGame, Models and Metrics\nIn this chapter, we ﬁrst describe the proposed language game and the deﬁnition of\nnumerals in our game. We then introduce the architecture of models we trained and\nalso the transformed iterated learning framework for training models.\n3.1\nGame Description\nUnlike traditional simulation methods in evolutionary linguistics introduced in Section\n2.1, there are 3 necessary components in our architecture and they are given as follows:\n• Environment: To imply our linguistic assumption as well as make the size of\nenvironment limited and thus analysable, all perceptions in the established en-\nvironment are sequences of objects represented by one-hot vectors. For ease of\ndemonstration, we denote these objects as o ∈O where O = {A,B,C,...} is the\nuniversal set of all kinds of objects in the following sections.\n• Agents: There are 2 kinds of agents in our project: i) speakers S that can observe\nobjects in the environment and emit messages mi; ii) listeners L that can receive\nthe messages and generate a sequence of objects.\n• Dynamics: In this project, the dynamics mean not only the manually designed\nreward function for agents but also all elements related to training them, e.g. iter-\nated learning and blank vocabulary. The details will be introduced in Subsection\n3.2.3 and Subsection 3.2.4.\nIt worth mentioning that one premise of our project is that we do not have any\nassumption about the architecture of computational agents, and we focus more on the\nrepresentations from environments as well as how agents are trained.\n6\nChapter 3. Game, Models and Metrics\n7\n3.1.1\nGame Procedure\nThe ﬁrst proposed game is to let listeners reconstruct sets of objects based on the\nmessages transmitted by speakers, thus we call it “Set-Reconstruct” game. The overall\nview of the proposed Set-Reconstruct game is illustrated in Figure 3.1 given as follow.\nA\nB\nB\nC\nC\nD\nD\nD\nSpeaker\nListener\nMessage\n\"ABBCCDDD\"\nFigure 3.1: Diagram of Game Playing Procedure.\nAccording to the steps of playing games at iteration i, the components of our games\nare illustrated as follows:\n1. Perceptions: the perception from environments is a set of objects, i.e. si =\n{oi1,oi2,... oin} ∈S where n is the number of elements and S is meaning space.\n2. Speaker observation and message generation: after observing and encoding the\nperception, speaker S would generate a message mi = {ti1,ti2,...,ti|M|} ∈M\nwhere |M| is the maximum length of messages, tk ∈V (k ∈1,...,|V|) are se-\nlected from a randomly initialised vocabulary such that the symbols in the ini-\ntially meaningless vocabulary whose size is |V|, and M is message space;\n3. Listener receiving message and perception reconstruction: after receiving and\nencoding the message mi, the listener would generate a sequence ˆsi = { ˆoi1, ˆoi2,... ˆoin}\nwhose symbols are identical to those in the original perception si;\n4. Reward and parameter update: by comparing si and ˆsi, we take the cross-entropy\nbetween them as the reward for both listener and speaker and update parameters\nof both speaker and listener with respect to it.1\nOne thing that needs to be highlighted is that the perceptions si are sets and thus\norder of objects would not make any difference. Further, we argue that the only im-\nportant feature that need to be transmitted is actually the numbers of different objects\nwhich corresponds to the function of numerals in natural language.\n1Different ways of updating parameters are introduced in Section 3.2.\nChapter 3. Game, Models and Metrics\n8\n3.1.2\nFunctions of Numerals in the Game\nBroadly speaking, numerals are words that can describe the numerical quantities and\nusually act as determiners to specify the quantities of nouns, e.g. ”two dogs” and\n”three people”. Also, under most scenarios, numerals correspond to non-referential\nconcepts[Da Costa and Bond, 2016]. Considering the objective of listeners L in our\nlanguage game, we deﬁne a numeral as a symbol tn at position i indicating a function\nthat reconstructs some object oi exactly n times:\ntn : oi →{\nn elements\nz\n}|\n{\noi,...,oi }\n(3.1)\nNote that, the meaning of a symbol is not only decided by itself but also its position\nin message, as L would encode meanings of symbols according to their appearance in\nmessages.\nFrom the side of speakers S, a numeral is deﬁned as a symbol tn at position i that\nrepresents the numbers of speciﬁc object oi, as we cannot tell whether agents realise\nthe meanings of symbols are not related to their positions in the messages without\nspeciﬁcally designed model architecture. Thus, we expect speaker S would ﬁrst learn\nto count the number of different objects and then encode them into a sequence of\ndiscrete symbols. As [Siegelmann and Sontag, 1992] shows that Recurrent Neural\nNetworks (RNNs) are Turing-complete and Long-short Term Memory (LSTM) model\nproposed by [Hochreiter and Schmidhuber, 1997] is a super set of RNN, it is safe to\nclaim that LSTM is also Turing-complete and thus capable of counting numbers of\nobjects.\n3.1.3\nA Variant: Set-Select Game\nA\nB\nB\nC\nC\nD\nD\nD\nSpeaker\nListener\nMessage\nA\nB\nB\nC\nC\nD\nD\nD\n…\nA\nB\nB\nC\nC\nD\nC\nB\nFigure 3.2: Diagram of Referential Game Playing Procedure.\nWe illustrate the Set-Select game, a referential variant of Set-Reconstruct game, in\nChapter 3. Game, Models and Metrics\n9\nFigure 3.2 given above. The only difference is that listeners need to select the correct\nset of objects among several distractors 2 instead of reconstructing it.\n3.2\nProposed Models\nℎ\nℎ\n1\nℎ\n2\nℎ\n3\n~1\n~1\n~2\n~2\n~3\nℎ\n4\n~3\n~4\nℎ\n1\nℎ\n2\nℎ\n3\nℎ\n4\nℎ\n0\nℎ\n0\nSpeaker\nℎ\n1\nℎ\n2\nℎ\n3\n\n̂ ~1\n̂ ~2\n̂ ~1\n̂ ~2\n̂ ~3\nℎ\n8\n̂ ~7\n̂ ~8\nℎ\n9\n̂ ~8\n{\n⋯\nListener\nListener Decoder For Set-Forward Game\nListener Selector For Set-Select Game\n⋯\n⋯\nA, C, B, B,\nD, C, D, D\nA, C, B, B,\nD, C, D, D\nA, C, B, B,\nC, C, B, D\nFigure 3.3: Overall Diagram of Model Architectures for Playing Games.\nWe illustrate the overall architecture of our models in Figure 3.3 given above.\nA speaker S consists of 2 components: i) a set encoder that takes a set of objects as\ninput and outputs its vector representation hs\ns; ii) a standard LSTM sequence decoder\nthat can generate a message si1,si2,si3,... based on hs\ns.\nAs for a listeners L, it would ﬁrst encode messages with a LSTM sequence encoder\nand get the feature vector hl\nm. Then, in the Set-Reconstruct game, L would take hl\nm\nas the initial hidden state and predict a sequence of objects with a LSTM sequence\ndecoder, which is shown by the right upper part of Figure 3.3. As for in Set-Select\ngame, L would compare hl\nm with several sets which are encoded by set encoders of L,\nand select the one shown to S based on the dot product between hl\nm and feature vectors\nof all candidate sets.\nFurther details are shown in the following subsections.\n2A distractor is a set that contains different numbers of objects as the correct one.\nChapter 3. Game, Models and Metrics\n10\n3.2.1\nSpeaker\nThe architecture of our speaking agents is very similar to the Seq-to-Seq model pro-\nposed by [Sutskever et al., 2014] except that replace the encoder for input sequences\nwith a set encoder whose details are introduced in the following subsubsection. As\nSeq-to-Seq model is quite popular nowadays, we skip details about how to generate\nsequences which correspond to the messages in our games, and focus on how to en-\ncode sets of objects.\n3.2.1.1\nSet Encoder\nOur set encoder shares an almost same architecture of inputting sets proposed by\n[Vinyals et al., 2015]. However, as there is an addition in softmax function and it\nwould introduce counting bias into the feature representation of sets, we replace equa-\ntion (5) in [Vinyals et al., 2015] with the following operation in order to avoid exposing\ncounting system to models:\nai,t = σ(ei,t)\n(3.2)\nwhere σ is sigmoid function.\nThus, assume the input for speaker S is a set si = {oi1,oi2,... oin}. The ﬁrst step is\nto read the set si as a sequence and project all objects to dense vectors by an embedding\nlayer. Based on the sequence {ws\ni1,ws\ni2,... ws\nin} (where ws\nik is the embedding vector of\noik for speaker where k ∈{1,...,n}), the calculation of hs\ns can be given as follows:\nes\ni,t = f(qs\nt−1,ws\ni)\nas\ni,t = σ(es\ni,t)\nrs\nt = ∑\ni\nas\ni,tws\ni\nqs\nt = LSTM(rt,qs\nt−1,cs\nt−1)\n(3.3)\nwhere t ∈{1,...,T} is the number of attention times, f is an afﬁne layer, qs\nt and cs\nt are\nhidden and cell states in LSTM respectively.\nBesides, in our implementation, T is set to be the same as the number of all types\nof objects, as we want to help models to represent number of each kind of objects as\nfeatures in the vector representation of input set.\nChapter 3. Game, Models and Metrics\n11\n3.2.1.2\nMessage Generator\nTo generate the message mi, we follow [Havrylov and Titov, 2017] and adopt a LSTM-\nbased sequence decoder with 2 different kinds of sampling mechanisms: i) direct sam-\npling that directly sample from the corresponding categorical dist6ribution speciﬁed by\nso ftmax(Whs\nk +b) ∀k ∈1,2,...,|M|; ii) Gumbel-softmax estimator proposed by [Jang\net al., 2016] with straight-through trick introduced by [Bengio et al., 2013b]. Beside,\nthe learning mechanisms also vary for these 2 different sampling methods, which is\nfurther discussed in Subsection 3.2.3.\nNote that the length of each message mi is ﬁxed to |M| and symbols ti1,...,ti|M| are\nall one-hot vectors that represent different discrete symbols. The effect of number of\nall discrete message symbols |V| and length of messages |M| on the emergent language\nis further discussed in Chapter 4.\n3.2.2\nListener\nThe architectures of listening agents are speciﬁcally designed for handling different\nkinds of tasks/games and thus vary from Set-Reconstruct game to Set-Select game.\nListener in Set-Reconstruct Game: The listener in Set-Reconstruct game has exactly\nthe same architecture as Seq-to-Seq model proposed by [Sutskever et al., 2014]. And,\nwhen combined with speaker model, the overall model is called “Set2Seq2Seq”.\nListener in Set-Select Game: The listener in Set-Select game would also ﬁrst encode\nmessages with a LSTM like it is in standard Seq-to-Seq model. However, as it needs to\nselect among several candidates, it also needs to encode all these sets with Set Encoder\nintroduced in Subsection 3.2.1.1. Then, the listener would make predictions based on\nthe dot-products between embedding of message hr\nm and embeddings of each set of\nobjects. Similarly, when combined with speaker model, the overall model is called as\n“Set2Seq2Choice”.\n3.2.3\nLoss/Reward and Learning\nIn Set-Reconstruct game, as the predictions of listeners are a sequence of objects\nˆsi = { ˆoi1,..., ˆoin}, we use cross-entropy between the original set and the predicted\nsequence as the objective function that needs to be minimised. Formally,\nLθS,θL(oi1,...,oin) = Emi∼pθS(·|si)\n\"\n−\nn\n∑\nk=1\noik log(p( ˆoik|mi, ˆo−ik))\n#\n(3.4)\nChapter 3. Game, Models and Metrics\n12\nwhere ˆo−ik represent all predicted objects preceding ˆoik.\nIn Set-Select game, we still use the cross entropy between the correct candidate and\nas the loss to minimise, i.e.\nLθS,θL(si) = Emi∼pθS(·|si)\n\"\n−\nC\n∑\nk=1\nsilog(p(ck))\n#\n(3.5)\nwhere ck is the predicted logit score for candidate k among C candidates.\nIn the case that we use Gumbel-softmax to approximate sampling messages from\nspeaker S, parameters θS and θL are learnt by back-propagation. In the case that we\nuse direct sampling, θL is still learnt by back-propagation, where as θS is learnt by\nREINFORCE estimator [Williams, 1992] with cross-entropy scores as rewards.\n3.2.4\nNeural Iterated Learning\nThe evolutionary linguistic community has already studied the origins and metrics of\nlanguage compositionality since [Kirby and Hurford, 2002] which points out a cultural\nevolutionary account of the origins of compositionality and proposes iterated learning\nto model this procedure. Thus, to facilitate the emergence of compositionality among\nthe autonomous communication between agents, we also trained our agents in a iter-\nated learning fashion. In the original iterated learning, an agent can both speak and\nlisten. However, in this project, agent can be either a speaker or a listener, not both at\nthe same time. Thus, we slightly transform the iterated learning framework and call\nthe following one “neural iterated learning” (NIL).\nFollowing the overall architecture of iterated learning, we also train agents gen-\neration by generation. In the beginning of each generation t, we would randomly\nre-instantiate a new speaker St and a new listener Lt and then execute the following 3\nphases:\n1. Speaker Learning phase: During this phase, we would train St with the set-\nmessage pairs generated by St−1, and the number of epochs is set to be ﬁxed.\nNote that there is no such phase in the initial generation, as there is no set-\nmessage pair for training St.\n2. Game Playing phase: During this phase, we would let St and Lt cooperate to\ncomplete the game and update θS\nt and θL\nt with loss/reward illustrated in previous\nsection, and use early-stopping to avoid overﬁtting.\nChapter 3. Game, Models and Metrics\n13\n3. Knowledge Generation phase: During this phase, we would feed all si in train-\ning set into St and get corresponding messages mi. Then, we would keep the\nsampled “language” for St+1 to learn.\n3.2.5\nBaseline Models\nTo get the upper bounds of our multi-agent communication systems, we remove the\ncommunication between speaker and listener to be the baseline models.\nIn Set-Reconstruct game, our baseline is Set-to-Seq model which ﬁrst encodes the\ninput set si with the set encoder introduced in subsection 3.2.1.1 and then directly\ngenerate the predicted sequence ˆsi following the sequence generation in standard seq-\nto-seq model.\nAs for in Set-Select game, our baseline is Set-to-Choose model, in which speaker\ndirectly transmit representation vector hs\ns of set si to listener. And, listener compare hs\ns\nwith all candidate sets to make a selection.\n3.3\nCompositionality and Metrics\nWith the recent rapid development of grounded language learning, measuring the com-\npositionality of emergent communication protocol attracts more and more attention\nnowadays, e.g. [Andreas, 2019], [Lowe et al., 2019].\nFirst of all, to better deﬁne compositionality, we argue that if a language is said to\nbe perfect compositional, then it should satisfy the following 2 properties:\n• Mutual Exclusivity: Symbols describing different values of a same property\nshould be mutually exclusive to each other. For example, “green” and “red” are\nboth used to describe colour of an object and they should not appear at the same\ntime as an object can not be green and red at the same time.\n• Orthogonality: Appearance of symbols for describing a property should be in-\ndependent from the appearance of symbols used to describe another property.\nFor example, the appearance of symbols used for describing colours of ob-\njects should be independent from the appearance of symbols used for describing\nshapes of objects.\nAs the setting of our game is simple and the space size is limited, we follow\n[Brighton and Kirby, 2006] and take the topological similarity between meaning space\nChapter 3. Game, Models and Metrics\n14\n(space of all sets of objects) and message space as the metric of compositionality.\nBrieﬂy speaking, as much of language is neighbourhood related, i.e. nearby mean-\nings tend to be mapped to nearby messages, the compositionality of language can\nbe measured as the correlation degree between distances of meanings and distances\nof corresponding messages. For example, the meaning of set {A,A,A,B,B} is closer\nto {A,A,B,B} than {A,A,A,A,B,B,B}. In natural language (which is perfectly com-\npositional), messages for {A,A,A,B,B},{A,A,B,B},{A,A,A,A,B,B,B} are “3A2B”,\n“2A2B” and “4A3B”3 respectively. However, in a non-compositional language, the\nmessages may be “5B5A”, “1C2E” and “3A4C”, which are randomly sampled map-\npings between meaning space and message space.\nIn order to calculate the topological compositionality, we need deﬁne the distance\nmetric for meaning space and message space respectively. Thus, for an input set si, we\ncould ﬁrst count the number of each kind of object and then concatenate the Arabic\nnumerals as the meaning sequence. Take a set si = {A,A,A,B,B} for example, the\ncorresponding meaning sequence would be “32” as there are 3 A and 2 B in si.4 As for\nthe message space, we have several different settings which are further illustrated in\nsubsection 4.2.2, and edit distance as in [Brighton and Kirby, 2006] is also included.\nMeanwhile, as we could perfectly encode the meaning of a set into natural lan-\nguage, we could take the speaker as a machine translation model that translates a\nmeaning represented in natural language into emergent language invented by com-\nputational agents themselves. Inspired by this point of view, we could also use BLEU\nscore proposed by [Papineni et al., 2002] as a metric of semantic similarities between\nmessages. For the sets that share more similar meanings, we expect their correspond-\ning messages to share more uni-grams or bi-grams or so on. Following the above\nexample, in a perfectly compositional language, as {A,A,A,B,B} locates very close to\n{A,A,B,B}, their messages (“3A2B” and “2A2B”) share 3 uni-grams (“A”, “B” and\n“2”) and 2 bi-grams (“A2” and “2B”) in common. However, in a non-compositional\nlanguage, e.g. in which the messages for {A,A,A,B,B} and {A,A,B,B} are “5B5A”\nand “1C2E” respectively, the messages share no uni-gram and bi-gram in common.\nIn our case, the BLEU score between mi and mj is calculated as follow:\nBLEU(mi,m j) = 1−\nN\n∑\nn=1\nωn ·\nNumber of common n-grams\nNumber of total different n-grams\n(3.6)\n3In Chapter 4, we would illustrate messages with lower case alphabets. To make them easier to\nunderstand, we use natural language here.\n4Again, the appearing order of objects would not effect the meaning sequence of a set.\nChapter 3. Game, Models and Metrics\n15\nwhere n is the size of n-grams and ωn is the weight for similarity based on n-grams. In\nthe following discussions, we would denote BLEU score based on n-grams as BLEU-n,\ne.g. BLEU score based on uni-grams would be represented as BLEU-1.\nChapter 4\nExperiment Results and Discussion\n4.1\nEmergence of Language without Iterated Learning\nFirst of all, we have to verify that the agents can successfully address the problems by\ncommunicating with discrete symbols. After tried several different settings, to avoid\nthat the success of agents depends on ﬁne-tuning hyperparameters, we ﬁnd that it is\nbetter to make the size of message space much larger than the size of meaning space.\nThus, we set the size of message space |V||M| to be 100 times of the meaning space\n|No||O| and show the performance of both “Set2Seq2Seq” and “Set2Seq2Choice” in\nTable 4.1. In the table, |M| is the length of messages, |V| is the size of vocabulary1 for\nmessage, |O| is the number of all kinds of objects and |No| is the maximum number of\na single kind of object.\nAdditionally, as the training procedure is time-consuming, all the shown perfor-\nmance are based on a single run, and thus the effects from hyperparameters and ran-\ndomness cannot be completely ﬁltered out. However, as we did not intentionally ﬁne-\ntune the hyperparameters and we focus on the emergent communication protocols, we\nbelieve that the variabilities of performance is limited and would not affect our follow-\ning discussions.\nBeside the “REINFORCE” and “Gumbel” sampling methods introduced in sub-\nsection 3.2.1.2, we also tried the self-critic sequence training proposed by [Rennie\net al., 2017] as a baseline for REINFORCE algorithm, which is denoted by “SCST”.\nBrieﬂy speaking, SCST utilises the output of its own test-time inference algorithm to\nnormalize the rewards received instead of estimating a separate baseline.\n1Note that the meaning of “vocabulary” is not like it is in traditional NLP, but refers to the set of\ninitially meaningless symbols that can be used for communication.\n16\nChapter 4. Experiment Results and Discussion\n17\nModel\nSampling Method\nPerformance\nGame Setting\nSet2Seq2Seq\nGumbel\n99.89%\n|M| = 8, |V| = 10,\n|O| = 6, |No| = 10\nREINFORCE\n89.89%\nSCST\n98.67%\nSet2Seq2Choice\nGumbel\n100%\n|M| = 6, |V| = 10,\n|O| = 4, |No| = 10\nREINFORCE\n76.45%\nSCST\n83.26%\nTable 4.1: Performance of Models and Corresponding Game Settings.\nBased on the performance shown in Table 4.1, it is clear that Gumbel is the most\nstable sampling method on all different settings. Thus, unless speciﬁcally stated, the\nfollowing experiments and discussions are all based on training agents with Gumbel-\nsoftmax as message sampling method.\n4.2\nStructure of Emergent Language\n4.2.1\nEmergent Languages in Various Games\nAfter verifying that computational agents are able to complete games through com-\nmunication, we are curious about the messages produced during their communica-\ntion. However, unlike what was shown by the previous works in grounded language\nlearning, e.g. [Hill et al., 2017] and [Mordatch and Abbeel, 2018], the emergent lan-\nguage during both Set-Reconstruct and Set-Select games are not “perfectly” composi-\ntional, which will be illustrated later. From our perspective, one alternative explanation\nfor this phenomenon is that |M| > |O| in our game settings, which makes proportion\nof holistic languages 2 much larger than the proportion of compositional languages\n[Brighton, 2002], and thus it becomes very hard to ﬁnd compositional languages.\nTo have give an intuitive demonstration of the emergent language, we list all mes-\nsages transmitted in a Set-Reconstruct game where |O| = 2,|No| = 5,|M| = 4,|V| = 10\nin Table 4.2 given as follow. In the table, the ﬁrst row and ﬁrst column are the basic el-\nements of meanings and each cell is the corresponding message for that meaning. Take\ncell “1A2B” for example, the original input set is si = {A,B,B} and the corresponding\n2A holistic language is a language that needs to be learned as a whole and should not be analysed\nor compartmentalized. In this work, holistic languages are generated by randomly sampling mappings\nbetween meaning space and message space.\nChapter 4. Experiment Results and Discussion\n18\nmessage mi is “ttvz”. Note that the alphabets in the message do not correspond to any\nsymbol in natural language.\n0A\n1A\n2A\n3A\n4A\n5A\n0B\ntxtt\ntxzt\nxtzz\nxzzz\nxxvx\n1B\nttxt\nttxz\ntzzz\nztzv\nzzvz\nvzxv\n2B\ntttx\nttvz\ntzhz\ntvzv\nzvhv\nvvvz\n3B\ntttv\nttvw\nthzv\ntvwv\nhvzv\nwvzv\n4B\nttht\nthtw\nthwz\nhhvz\nhwvv\nwwvv\n5B\ntthh\nthhh\nthww\nhhwh\nhwww\nwwww\nTable 4.2: An emergent language in a Set-Reconstruct game.\nBased on the 2 properties of compositional languages illustrated in Section 3.3, we\ncould see that the emergent language shown in Table 4.2 satisﬁes neither of mutual ex-\nclusivity nor orthogonality. To be speciﬁc, there is no common substrings of messages\nin every column/row, and some substrings, e.g. “tt” in column “0A”, may be used in\nmultiple columns/rows. Thus, the emergent language is not a perfectly compositional\none as we expect. Thus, as we can see from Table 4.2, there is no clear compositional\nstructure in it.\nHowever, as Set-Reconstruct game is a generation task, one possible hypothesis is\nthat the agents may transmit more than numeric concepts in order that listeners could\ngenerate the original input. Thus, to verify whether this is the case, we illustrate an\nemergent language in a Set-Select game whose settings are exactly the same as the\nSet-Reconstruct game illustrated above, i.e. |O| = 2,|No| = 5,|M| = 4,|V| = 10. The\nmeanings and corresponding messages are shown in Table 4.3 given as follow.\n0A\n1A\n2A\n3A\n4A\n5A\n0B\nxxxv\nxxvy\nxvyy\nvyxy\nvyyy\n1B\nxxxx\nxxzx\nxwxv\nxvvv\nvvxx\nvvyy\n2B\nxxyx\nxqxx\nxzxz\nxwwv\nvwvx\nvvvv\n3B\nxyxy\nxqyx\nxqqx\nzxzz\nwwwx\nwvwv\n4B\nyxyx\nyxqy\nqxqy\nqzxq\nzzxz\nzwwz\n5B\nyyxy\nyyqy\nqyyy\nqqyy\nzqqq\nzzzz\nTable 4.3: An emergent language in a Set-Select game.\nChapter 4. Experiment Results and Discussion\n19\nBased on the message contents in Table 4.3, we could ﬁnd that the referential game\ndoes not necessarily make the emergent language perfectly compositional.\nAccording to [Kottur et al., 2017], another alternative probability is that the mes-\nsage space is much larger in the previous game settings and thus it is over-complete\nfor agents to encode the sets of objects in a compositional fashion. Thus, we re-train\nthat agents with |O| = 2,|No| = 5,|M| = 2,|V| = 10 3 (where the size of meaning and\nmessage space are 35 and 100 respectively), and the emergent language is shown in\nTable 4.4. As we can see, the smaller message space does not necessarily facilitate the\nemergence of compositional language in Set-Select game.\n0A\n1A\n2A\n3A\n4A\n5A\n0B\nzv\nvz\nvv\nvv\nxv\n1B\nzy\nzu\nzw\nwz\nwv\nxw\n2B\nyz\nuu\nzz\nzt\nww\nwx\n3B\nyz\nuy\nuq\nqz\ntz\ntw\n4B\nyx\nyy\nur\nqq\nqt\ntt\n5B\nxy\nyr\nry\nrx\nxq\nxt\nTable 4.4: Another emergent language in a Set-Select game.\n4.2.2\nTopological Similarities\nAs introduced in subsection 3.3, we measure the topological similarity between mean-\ning space and message space as a measure of compositionality. We list compositional-\nity scores under different kinds of metrics in Table 4.5 given as follow.\nHam+Edit\nHam+BLEU\nEuclid+Edit\nEuclid+BLEU\nCompositional\n1.00\n0.61\n0.38\n0.24\nSet-Reconstruct\n0.32\n0.27\n0.60\n0.65\nSet-Select\n0.13\n0.16\n0.45\n0.52\nHolistic\n-0.04\n-0.04\n0.01\n0.00\nTable 4.5: Topological similarity scores of different languages.\n3Here, the message space is still larger than meaning space, as we again do not want to make the\nsuccess of agents depend on ﬁne-tuning hyperparameters.\nChapter 4. Experiment Results and Discussion\n20\nHam+Edit: We ﬁrst follow the distance metrics in [Brighton and Kirby, 2006]: i) use\nhamming distances between meaning sequences as the similarity metric for meaning\nspace; ii) use edit distances between corresponding messages as the similarity metric\nfor message space.\nHam+BLEU4: In this setting, we use: i) hamming for meaning space too; ii) BLEU\nscore illustrated in Section 3.3 as the the similarity metric for message space.\nEuclid+Edit: In this setting, we use: i) Euclidean distance as the metric for meaning\nspace, e.g. Euclidean distance between “4A2B” and “1A3B” is\np\n(4−1)2 +(2−3)2 =\n√\n10; ii) edit distance for message space.\nEuclid+BLEU: In this setting, we use: i) Euclidean distance for meaning space; ii)\nBLEU score illustrated in Section 3.3 for message space.\nTo get the upper bound and lower bound of compositionality, we speciﬁcally de-\nsigned: i) a perfectly compositional language, in which the message is exactly the same\nas meaning sequence, e.g. “4A2B” is represented as “wsyr” (A →s,B →r,4 →w,2 →\ny); ii) a holistic language, in which messages are randomly generated.\nThen, from the above results, we could see that although the emergent languages\nin Set-Reconstruct and Set-Select games gain low topological similarity scores under\nHamming distance for meaning space, they obtain much higher similarity scores under\nEuclidean distances for meaning space. As for the compositional language, they obtain\nvery low scores under Euclidean distances, which is caused by that the numerals in nat-\nural language encode numeric concepts as different symbols and thus the edit/BLEU\ndistance between messages are all the same for different meanings. For example, al-\nthough meaning “2A1B” is closer to “1A1B” than meaning “4A1B”, the edit distance\nbetween “yszr” (message for “2A1B”) and “zszr” (message for “1A1B”) is exactly the\nsame as the edit distance between “wszr” (message for “4A1B”) and “zszr”.\n4.2.3\nSigniﬁcance Test of Same Numeric Concepts\nAlthough the compositionality of emergent language is not like our natural language,\nwe could also see that it may reﬂect the underlying structure of meaning space. Thus,\nto further verify this point of view, we further verify whether messages for mean-\ning pairs that share same numeric concepts are more similar. To do this, we estab-\nlished 2 different datasets: i) meaning pairs sharing exactly same numeric concepts,\ne.g. “4A3B” and “3A4B”, and corresponding BLEU similarity scores for their mes-\n4Without special declaration, we use “BLEU” to represent weighted average between BLEU-1 and\nBLEU-2, i.e. 0.5×BLEU-1+0.5×BLEU-2.\nChapter 4. Experiment Results and Discussion\n21\nsages; ii) pairs of meaning sequences that share no numeric concept, e.g. “4A3B” and\n“5A1B”, and corresponding BLEU similarity for their messages. Thus, we have 1,190\n(2×(35×34÷2)) meaning pairs in total, of which half share same numeric concepts\nand the other half do not. We then calculate the BLEU-1/2/3 scores of the messages of\nthese pairs and test whether these BLEU scores are correlated to sharing same numeric\nconcepts.\nThen, we establish the following hypotheses for signiﬁcance test:\n• Null hypothesis: The BLEU scores between messages are independent from\nwhether meaning pairs share same numeric concepts.\n• Alternative hypothesis: The BLEU scores between messages are not indepen-\ndent from whether meaning pairs share same numeric concepts.\nTo test whether there is a correlation between BLEU scores and sharing same nu-\nmeric concepts, we calculate the Spearman correlation coefﬁcient and the correspond-\ning p-values. The results got on different types of languages based on different n-grams\nare given in Table 4.6 as follow. 5\nLanguage Type\nBLEU-1\nBLEU-2\nBLEU-3\nρ\np-value\nρ\np-value\nρ\np-value\nCompositional\n0.96\n7.84×10−39\n0.28\n2.01×10−2\n0.28\n2.01×10−2\nEmergent-R\n0.27\n2.42×10−2\n0.26\n2.74×10−2\n0.33\n5.07×10−3\nEmergent-S\n0.29\n1.49×10−2\n0.38\n1.16×10−3\n0.38\n1.06×10−3\nHolistic\n−0.08\n4.93×10−1\n0.05\n6.77×10−1\n0.22\n6.10×10−2\nTable 4.6: ρ and p-values of different types of languages. ρ represents the Spearman\ncorrelation coefﬁcient between meanings and messages, and the “Emergent-R” and\n“Emergent-S” are emergent languages in Set-Reconstruct game and Set-Select game\nrespectively.\nThe p-values under BLEU-1/2/3 for compositional language as well as emergent\nlanguages in both Set-Reconstruct and Set-Select games are all smaller than 0.05.\nThus, it is safe to reject null hypothesis and accept the alternative hypothesis. That is,\nThe BLEU scores between messages are highly correlated with whether their meaning\n5It is clear that an effective language would not contain identical messages for different meanings.\nAs the maximum length of messages is 4 and there is no identical messages in all languages, we thus\nskip the BLEU-4 score (which would 0 for all languages) here.\nChapter 4. Experiment Results and Discussion\n22\npairs share same numeric concepts. To be more precise, as we can tell from Table 4.6,\nthe messages of meaning pairs sharing same numeric concepts have more uni-grams,\nbi-grams and tri-grams in common.\n4.2.4\nGeneralisation of Emergent Language\nA further question about the structure of emergent languages is to verify whether the\nemergent language can be generalised to unseen sets. If the emergent languages can be\ngeneralised, we then could say that these languages do capture the structure of meaning\nspaces.\nTherefore, we train many randomly initialised listeners with several kinds of lan-\nguages: i) compositional language; ii) an emergent language invented by other agents;\niii) holistic language. The game settings are |M| = 8, |V| = 10, |O| = 4, |No| = 10.\nWe expose listeners with only the training set (which contains 80% sets of objects\nrandomly sampled from the whole meaning space), but test their performance on eval-\nuation set (which contains unseen 20% sets of objects left in the meaning space). Then,\nthe generalisation ability of languages can be measured as the performance on evalu-\nation set.Learning and performance curves of different languages on Set-Reconstruct\nand Set-Select games are given in Figure 4.1 and Figure 4.2 respectively.\nNote that we run all experiments with 10 different random seeds to avoid the effect\nbrought by different initialisations. The mean of 10 different runs are given as lines\nin Figure 4.1 and Figure 4.2, and the shadow areas are corresponding standard devia-\ntions. Meanings of y-axes are given as the title for each sub-plot, and the numbers in\nparentheses are length of messages in each language. Peculiarly, “emergent - recon-\nstruct” and “emergent - select” represent emergent languages from Set-Reconstruct\nand Set-Select games respectively. This is also the case for all the following ﬁgures.\nIt is quite surprising that, although we cannot see any signiﬁcant pattern in the\nemergent language, it actually can be generalised to unseen sets of objects by listeners,\nas illustrated by the performance of listeners on evaluation dataset. Meanwhile, listen-\ners trained with emergent languages converge faster on evaluation performance as well\nas training loss, although length of emergent messages (|M| = 8) are longer than that\nof compositional language (|M| = 4).\nFrom the evaluation accuracy in Figure 4.2, we could see that the emergent lan-\nguage in Set-Reconstruct game can be well generalised to unseen samples by listeners\nin Set-Select game. On the other hand, listeners in Set-Reconstruct game, however,\nChapter 4. Experiment Results and Discussion\n23\n0\n50\n100\n150\n200\n250\n300\nNumber of Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEvaluation Sequence Accuracy\ncompositional (len 4)\nemergent - reconstruct (len 8)\nemergent - select (len 8)\nholistic (len 4)\nseq2seq\n0\n50\n100\n150\n200\n250\n300\nNumber of Epochs\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nEvaluation Token Accuracy\ncompositional (len 4)\nemergent - reconstruct (len 8)\nemergent - select (len 8)\nholistic (len 4)\nseq2seq\n0\n50\n100\n150\n200\n250\n300\nNumber of Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Sequence Accuracy\ncompositional (len 4)\nemergent - reconstruct (len 8)\nemergent - select (len 8)\nholistic (len 4)\nseq2seq\n0\n50\n100\n150\n200\n250\n300\nNumber of Epochs\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTraining Token Accuracy\ncompositional (len 4)\nemergent - reconstruct (len 8)\nemergent - select (len 8)\nholistic (len 4)\nseq2seq\n0\n50\n100\n150\n200\n250\n300\nNumber of Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Loss\ncompositional (len 4)\nemergent - reconstruct (len 8)\nemergent - select (len 8)\nholistic (len 4)\nseq2seq\nFigure 4.1: Learning and performance curves of different languages for listeners in\nSet-Reconstruct game.\n0\n2\n4\n6\n8\n10\nNumber of Epochs\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nEvaluation Accuracy\ncompositional (len 4)\nemergent - reconstruct (len 8)\nemergent - select (len 8)\nholistic (len 4)\n0\n2\n4\n6\n8\n10\nNumber of Epochs\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nTraining Loss\ncompositional (len 4)\nemergent - reconstruct (len 8)\nemergent - select (len 8)\nholistic (len 4)\n0\n2\n4\n6\n8\n10\nNumber of Epochs\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTraining Accuracy\ncompositional (len 4)\nemergent - reconstruct (len 8)\nemergent - select (len 8)\nholistic (len 4)\nFigure 4.2: Learning and performance curves of different languages for listeners in\nSet-Select game.\ncannot generalise emergent language from Set-Select game, which is illustrated by the\nevaluation accuracies in Figure 4.1. This phenomenon demonstrates that the infor-\nmation encoded by speakers in Set-Reconstruct games are richer than the information\nencoded by speakers in Set-Select games. As the only difference between candidates\nin Set-Select game is the numbers of different types of objects, if we assume that the\ndifferent numbers of objects are encoded by speakers in Set-Select game, then we can\ninfer that speakers in Set-Reconstruct game would encode more than only numeric\nconcepts. Another possible explanation is that speakers in Set-Reconstruct encode\nChapter 4. Experiment Results and Discussion\n24\nonly numeric concepts, then speakers in Set-Select game would encode less than that.\nHowever, as agents could always get almost perfect performance on both training set\nand evaluation set, it is reasonable to believe that the later assumption is less possible.\n4.2.5\nSection Conclusion\nTo sum up from all above, although we cannot ﬁnd observable patterns in emergent\nlanguages under various game settings, the emergent languages are actually easier for\nagents to learn and also can be generalised to unseen sets of objects. Thus, based\non the previous topological similarity metrics and signiﬁcance test, we claim that the\nemergent languages do capture the underlying structure of meaning space.\n4.3\nLearning Speed & Iterated Learning\nFrom the previous sections, we could see that the emergent languages can reﬂect the\nunderlying structure of meaning spaces, although they may not be as compositional as\nour natural languages. Thus, we are further curious about the motivation of the emer-\ngent language. Or, to say, the reasons why computational agents prefer to communicate\nin such a “non-natural”6 way.\n4.3.1\nFor Listener\nFirst we attempted to verify whether the emergent languages are the most easy ones\nfor listeners to understand. To verify this, we ﬁrst trained agents with game setting\n|V| = 10, |O| = 2, |No| = 5 and 2 different |M| (2 or 4), to got 2 different emergent\nlanguages. Then, we test the learning speeds of all kinds of languages with randomly\ninitialised new listeners in both Set-Reconstruct game and Set-Select game. Again, we\nrun experiments with 10 different random seeds, and draw the mean and correspond-\ning standard deviations of performance results from Set-Reconstruct and Set-Select in\nFigure 4.3 and Figure 4.4 respectively.\nWe also trained a vanilla Seq-to-Seq model to get an upper bound of learning\nspeeds and performance, as we assume that the original meaning space is the opti-\nmal message space for both speakers and listeners. However, as time is limited, we\nhave not done the same experiment on Set-Select game.\n6From a human perspective, it is not like how we communicate numeric concepts through natural\nlanguage.\nChapter 4. Experiment Results and Discussion\n25\n0\n50\n100\n150\n200\n250\n300\n350\n400\nNumber of Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\nTraining Loss\ncompositional (len 4)\nholistic (len 4)\nemergent (len 4)\nseq2seq\ncompositional (len 2)\nholistic (len 2)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nNumber of Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Sequence Accuracy\ncompositional (len 4)\nholistic (len 4)\nemergent (len 4)\nseq2seq\ncompositional (len 2)\nholistic (len 2)\n0\n50\n100\n150\n200\n250\n300\n350\n400\nNumber of Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Token Accuracy\ncompositional (len 4)\nholistic (len 4)\nemergent (len 4)\nseq2seq\ncompositional (len 2)\nholistic (len 2)\nFigure 4.3: Learning and performance curves of different languages for listeners in Set-\nReconstruct game. Numbers in the parentheses are length of messages in a language.\n0\n20\n40\n60\n80\n100\n120\n140\nNumber of Epochs\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nTraining Loss\ncompositional (len 4)\nholistic (len 4)\nemergent (len 4)\nemergent (len 2)\ncompositional (len 2)\nholistic (len 2)\n0\n20\n40\n60\n80\n100\n120\n140\nNumber of Epochs\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Accuracy\ncompositional (len 4)\nholistic (len 4)\nemergent (len 4)\nemergent (len 2)\ncompositional (len 2)\nholistic (len 2)\nFigure 4.4: Learning and performance curves of different languages for listeners in\nSet-Select game. Numbers in the parentheses are length of messages in a language.\nFrom the above ﬁgures, we could easily see that emergent languages are learnt\nfaster than compositional and holistic language in Set-Reconstruct game, while they\nhave very similar performance to the compositional language with length 2 in Set-\nSelect game but still have lower loss.\n4.3.2\nFor Speaker\nWe then test the learning speed of different languages on speaker, i.e. we randomly\ninitialise several new speakers and let it learn to produce messages of input sets under\ndifferent languages. Note that the architecture of speakers are identical in Set2Seq2Seq\nand Set2Seq2Choice model, and all the curves are drawn in one ﬁgure, i.e. Figure 4.5\nChapter 4. Experiment Results and Discussion\n26\ngiven as follow. The game settings are the same as they are during testing listener\nleaning speed. One thing we need to mention is that we also test an emergent lan-\nguage in Set-Select game with |M| = 2,|V| = 10,|O| = 2,|No| = 5 which is denoted as\n“emergent (len 2)”.\n0\n50\n100\n150\n200\n250\n300\n350\n400\nNumber of Epochs\n0.0\n0.5\n1.0\n1.5\n2.0\nTraining Loss\ncompositional (len 4)\nholistic (len 4)\nemergent - reconstruct (len 4)\nemergent - select (len 2)\ncompositional (len 2)\nholistic (len 2)\nset2seq\n0\n50\n100\n150\n200\n250\n300\n350\n400\nNumber of Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Sequence Accuracy\ncompositional (len 4)\nholistic (len 4)\nemergent - reconstruct (len 4)\nemergent - select (len 2)\ncompositional (len 2)\nholistic (len 2)\nset2seq\n0\n50\n100\n150\n200\n250\n300\n350\n400\nNumber of Epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Token Accuracy\ncompositional (len 4)\nholistic (len 4)\nemergent - reconstruct (len 4)\nemergent - select (len 2)\ncompositional (len 2)\nholistic (len 2)\nset2seq\nFigure 4.5: Learning curves of different languages for speakers in both games. Num-\nbers in the parentheses are length of messages in a language.\nMeanwhile, by comparing learning emergent languages with length 2 and 4, we\ncould easily see that larger message spaces are always easier to learn than the smaller\nones for emergent languages, which is counterintuitive. Considering that facts that the\noriginal meaning space is always the easiest for both speakers and listeners to learn,\nwhich is demonstrated by Figure 4.3, Figure 4.4 and Figure 4.5, our explanation about\nthis phenomenon is that the larger message spaces are easier to be shaped like the\noriginal set space for agents. To be speciﬁc, as the maximum size of sets is |No|×|O|\nand the vacancies in the sets can all be represented by some special symbols (e.g. blank\nspace in English), a larger |M| would make it easier for agents to create a message space\nthat is highly similar to the original set space and becomes easier to learn for agents.\nFrom the above ﬁgures, it is quite clear that compositional languages always con-\nverge faster than the same sized emergent languages, which is contradictory to the\nsituation on listener side. However, it is still the case that the emergent languages have\nlower losses than the same-sized compositional languages. Our hypothesis is that com-\npositional language is a smoother function for speaker to learn and thus it is easier to\nbe optimised. As time is limited, this phenomenon is not further discussed in this work\nbut will be explored in the future works.\nChapter 4. Experiment Results and Discussion\n27\n4.3.3\nImprovement by Iterated Learning\nIterated learning framework [Kirby and Hurford, 2002] has been proposed to explain\nthe emergence of language structures more than one decade. Thus, we are curious\nabout whether it could improve the compositionality of the emergent languages in our\nsystem. However, there are several obstacles for directly applying iterated learning\ninto our neural agents:\n1. we cannot feed prior probability that favours high compositional languages to\nneural networks;\n2. the pre-training procedure in learning phase of original iterated learning need to\nbe re-designed, as speakers and listeners in our game are not inverse functions\nto each other.\nThus, we adapt iterated learning into our project, which is illustrated in Subsection\n3.2.4, and train agent population with respect to normal training mechanism and iter-\nated learning. The results are shown in Figure 4.6. It needs to be pointed out that the\ndistance metric for meaning space is Euclidean distance for topological similarity, and\nmetric for message space is edit distance. Reminds that the topological similarity of\ncompositional language under this metric is 0.38.\nBy comparing the curves of iterated learning and normal training, we can see a\nsigniﬁcant improvement of topological similarity in iterated learning, about 0.1. How-\never, although the messages emerged in iterate learning becomes more correlated with\nEuclidean distances between meanings, the numeric concepts in them are still not rep-\nresented like numerals in natural languages.\n4.4\nEffects of Different Representations\nCompared our results in Section 4.1 to 4.3 with previous works in grounded language\nlearning, we argue that the different characteristics of emergent languages in our works\nare due to the feature representations of meanings.\nTo be speciﬁc, in our games, listeners need to generate object sequences or se-\nlect the correct object sequence according to features representing each kind of ob-\njects. For example, the feature representation of set {A,B,A} would be a sequence\n{[10],[01],[10]} (assume that |O| = 2,|No| = 8), and the corresponding message would\nbe {2,1} = {[001000000],[010000000]} (assume that |M| = |O| = 2,|V| = |No| = 8).\nChapter 4. Experiment Results and Discussion\n28\n0\n25\n50\n75\n100\n125\n150\n175\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nTopological Similarity\niterated learning\nnormal training\n0\n2000\n4000\n6000\n8000 10000 12000 14000 16000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining Sequence Accuracy\niterated learning\nnormal training\nNumber of Epochs\nNumber of Epochs\nFigure 4.6: Topological similarity curves of iterated learning and normal training in Set-\nReconstruct game. Note that one generation contains many epochs and we only test\nthe topological similarity every 10 epochs or at the end of game playing phase, so the\ntwo plots have different scales on the x-axis. As we use early-stopping during game\nplaying phase, every generation may contain different numbers of epochs.\nThus, to understand the message, the listener needs to correctly count the numbers of\neach kind of objects in the set and ground symbols to the counting results. During this\nprocedure, there are 2 gaps between meanings (or perceptions) and messages: i) from\nmeaning to numeric concepts; ii) from numeric concepts to messages.\nTo verify which step imports bias towards emergent language, we slightly change\nthe representation of sets in Set-Select game, i.e. we directly encode the numbers of\neach kind of objects as one-hot vectors and concatenate them to be the representation\nof the whole set. Take set {A,B,A} as example, its representation would be vector\n[001000000;010000000], whereas its message is still the sequence\n{[001000000],[010000000]}. Then, it is straightforward that mapping from messages\nto meanings is a linear transformation and thus it should be easy for neural networks\nto ﬁt.\nFirst of all, we test the learning speed of manually designed languages with differ-\nent topological similarity scores on both speaker and listener side, and the results are\nshown in Figure 4.7. Note that the metric for meaning distance is Hamming distance\nand thus languages with higher ρ-values would “look” more like our natural language.\nA higher ρ means that the language is more compositional from perspective of human\nbeings, e.g. ρ = 1 means that the language is perfectly compositional from our view.\nAs we can see in Figure 4.7, language with higher ρ-values are much more easier\nChapter 4. Experiment Results and Discussion\n29\n0\n200\n400\n600\n800\n1000\nNumber of epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSmoothed prediction accuracy\nRho = 1.0\nRho = 0.85\nRho = 0.62\nRho = 0.21\n(a) Listener learning speed\n0\n500\n1000\n1500\n2000\n2500\n3000\nNumber of epochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSmoothed training accuracy\nRho = 1.0\nRho = 0.725\nRho = 0.45\nRho = 0.2\np\n(b) Speaker learning speed\nFigure 4.7: Learning speed of languages with different compositionalities with linear\nfeature representations.\nto learn for both speaker and listener, under the current scenario.\nThen, we are curious about probability distributions of languages generation by\ngeneration. Recall that a language ℓis a mapping function from meaning space S to\nmessage space M , i.e. L ∈S ×M . Then, we can deﬁne the probability of a language\np(L) as the product of probabilities of the message corresponding a given set in it, i.e.\np(ℓ) = ∏i p(mi|si),∀si ∈S where mi is the corresponding message in language ℓfor set\nsi. The probability of a message given an input set is then the product of probabilities\nof each symbol, i.e. p(mi) = ∏k p(tik|hs\ns,ti−k) where hs\ns is the hidden representation of\nset si and ti−k is the symbols that appear before tik.\nWe track the probabilities of languages with different ρ-values during the iterated\nleaning procedure. To be speciﬁc, we manually designed many languages with differ-\nent ρ and calculate their probabilities at the end of each generation. The results are\nshown in Figure 4.8.\nFrom the above ﬁgure, it is straightforward to see that high compositional lan-\nguages gradually dominate among all kinds of languages generation by generation.\nTo have an intuitive feeling about the ﬁnal emergent language with iterated learning\nand current feature representations, we illustrate it in Table 4.7.\nThen, it is straightforward to decipher the emergent language shown in Table 4.7.\nBasically, the symbols appeared in the ﬁrst digit represent the numbers of “A” and\nthe symbols appeared in second digit represent the numbers of “B”. Of course, the\nlanguage is still not perfect compositional, as there are some repetitive messages for\ndifferent meanings, such as “3A5B” (sv) and “3A6B” (sv). Besides, it worth mention-\ning that the same symbol still represents different meanings if it appears at different\nChapter 4. Experiment Results and Discussion\n30\n0\n10\n20\n30\n40\n50\n60\n70\n80\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nProbability\nRho: 0.0~0.2\nRho: 0.2~0.4\nRho: 0.4~0.6\nRho: 0.6~0.8\nRho: 0.8~1.0\nNumber of generations\nFigure 4.8: Changes of probabilities of languages with different ρ-values during iterated\nlearning.\n0A\n1A\n2A\n3A\n4A\n5A\n6A\n7A\n8A\n0B\nyq\nuq\nxq\nxq\nzq\nvq\ntq\nqq\n1B\nwy\nyy\nuy\nsy\nxy\nzy\nvy\nty\nqy\n2B\nws\nys\nus\nss\nxs\nzs\nvs\nts\nqs\n3B\nwt\nyt\nut\nst\nxt\nzt\nvt\ntt\nqt\n4B\nwu\nyu\nuu\nsu\nxu\nzu\nvu\ntu\nqu\n5B\nwv\nyv\nuv\nsv\nxv\nzv\nvv\ntv\nqv\n6B\nwz\nyz\nuz\nsv\nxz\nzz\nvz\ntz\nqz\n7B\nww\nyw\nuw\nsw\nxw\nzw\nvw\ntw\nqw\n8B\nxx\nyx\nux\nsx\nxx\nzx\nvx\ntx\nqx\nTable 4.7: Final emergent language in linear feature representation and iterated learn-\ning.\npositions.\nOverall, we could say the the obstacle for the emergence of compositional lan-\nguages in our Set-Reconstruct and Set-Select games is that symbols in messages do\nnot directly correspond to numeric features in the original meaning spaces. As long\nas the features we want the emergent language to represent is established, the agents\ncould invent almost perfect compositional language by iterated learning.\nChapter 4. Experiment Results and Discussion\n31\n4.5\nFurther Discussion\nComparing the experimental results in this chapter with previous work in grounded\nlanguage learning, e.g. [Kottur et al., 2017, Hermann et al., 2017, Havrylov and Titov,\n2017, Mordatch and Abbeel, 2018], we propose an alternative hypothesis to explain the\nemergence of compositional language (some previous works call it natural language)\nduring the autonomous communication among agents population.\nFirst of all, we argue that the feature vectors of input experience and perceptions\nshould be inherently disentangled, i.e. the feature vectors of these inputs should satisfy\nmutual exclusivity and orthogonality deﬁned in Subsection 4.2.1, in order to facilitate\nthe emergence of compositional language. Then, it could be an optimal method to use\na single symbol as a feature representation of a disentangled element in feature vec-\ntors. By comparing the emergent languages in Section 4.1 to 4.3 with that in Section\n4.4, it is straightforward to see that linear transformed feature representations would\nbe much more optimal for the emergence of highly compositional languages. How-\never, as lots of previous use images as the perceptions for speakers, there is still a\ngap between our 2 representing methods. Without further experiment, we are not sure\nabout whether the emergence of compositional languages of those works are caused\nby that convolutional neural networks (CNN) can spontaneously encode images into\ndisentangled representations. Previously, it has been widely believed that the success\nof unsupervised learning for CNNs depends on that models can automatically establish\ndisentangled representations [Bengio et al., 2013a]. However, recently, this common\nassumption become questioned and challenged by researchers [Locatello et al., 2018].\nThus, our hypothesis is that the emergence of compositionality is actually highly re-\nlated to the disentangled representation of models.\nSecondly, we argue that iterated learning is an effective method to amplify induc-\ntive bias into multi-agent autonomous communication systems, and thus improve the\ncompositionalities of emergent languages. Considering the discoveries in [Locatello\net al., 2018], we claim that the compositional languages are highly correlated with the\nappearance of disentangled representations. Further, inductive bias towards compo-\nsitionalities of different kinds of symbols (which correspond to words in natural lan-\nguages) should be introduced to different spaces. For example, inductive bias towards\nthe compositionality of symbols corresponding to objects/attributes that physically ex-\nists in real/virtual world can be introduced by iterated learning, as the feature values\nof these objects/attributes are mutually exclusive and independent (or to say, they are\nChapter 4. Experiment Results and Discussion\n32\ninherently disentangled). On the other hand, compositionality of function words, such\nas numerals in our project, requires the agents to ﬁrst encode the input features in\nsome speciﬁc ways and obtain disentangled representations. Thus, without specially\ndesigned training mechanism or data samples that could introduce such pressure, it\nis natural for agents to invent effective but non-natural “languages” during their au-\ntonomous communication.\nChapter 5\nConclusions\n5.1\nExpress Numeric Concepts\nWith all experimental results shown in Chapter 4, we could conclude that the mod-\nels illustrated in Chapter 3 can successfully transmit numeric concepts in whichever\nSet-Reconstruct or Set-Select game proposed in Chapter 3. Although the emergent\nlanguages are not compositional from the perspective of humans, they do capture the\nunderlying structure of meaning space and reﬂect it into messages consist of sequences\nof discrete symbols, which is measured by the Euclidean distances between meaning\npairs. Furthermore, the messages expressing same numeric concepts have higher simi-\nlarities to each other, which is measured the BLEU score deﬁned in Section 3.3. More\nimportantly, the emergent languages can be successfully generalised to unseen mean-\nings and they are not only effective but also efﬁcient, as models can ﬁt to them faster\nthan other languages.\nTherefore, we claim that the agents capture the numeric concepts during cooperat-\ning to complete the games , and successfully transmit these numeric concepts with a\nnon-natural language.\n5.2\nRole of Iterated Learning\nBy transforming iterated learning to train our DL-based agents, it successfully im-\nproves the compositionality of emergent languages, which is measured by Euclidean\ndistances in meaning space and BLEU score in message space, in our original set\nrepresentations of objects. Then, by taking vectors that directly encode quantities of\ndifferent kinds of objects as the input for speakers, the emergent languages become\n33\nChapter 5. Conclusions\n34\nalmost perfectly compositional under iterated learning.\nTherefore, we claim that iterated learning is an effective method to improve the\ncompositionality of emergent languages, w/o inherently disentangled feature represen-\ntation of inputs. Even thought the emergent compositionality may not correspond to\nwhat it is in human natural languages.\n5.3\nFuture Works\nWith the current exploration, there are still several open questions in our work and thus\nseveral interesting and meaningful future works:\n1. Generalisation and meta-learning: [Smith et al., 2013] claims that language\nstructure is an evolutionary trade-off between simplicity and expressivity. We\nassume that generalisation is another form of this trade-off. Further, emergence\nof numerals is a good candidate for discovering the role of generalising pressure\nin language evolution, as numerals can be used for whatever kind of objects.\nMore importantly, such pressure can be formalised by meta-learning.\n2. Feature representations: As discussed in Section 4.4, different kinds of repre-\nsentations have a strong effect on the compositional form of emergent languages.\nArgued by [Locatello et al., 2018], representation learnt without supervision are\nnot disentangled. We further assume that inherently disentangled elements are\nnot only important in the input feature space but also in the parameter feature\nspace. Or, to say, some words in our natural languages directly correspond to\nelements in input feature representations, while others may correspond the fea-\ntures of speciﬁc functions, e.g. agents need to learn counting (a function) in our\ngames.\nBibliography\n[Andreas, 2019] Andreas, J. (2019).\nMeasuring compositionality in representation\nlearning. arXiv preprint arXiv:1902.07181.\n[Baronchelli et al., 2006] Baronchelli, A., Felici, M., Loreto, V., Caglioti, E., and\nSteels, L. (2006). Sharp transition towards shared vocabularies in multi-agent sys-\ntems. Journal of Statistical Mechanics: Theory and Experiment, 2006(06):P06014.\n[Batali, 1998] Batali, J. (1998). Computational simulations of the emergence of gram-\nmar. Approach to the Evolution of Language, pages 405–426.\n[Bengio et al., 2013a] Bengio, Y., Courville, A., and Vincent, P. (2013a). Representa-\ntion learning: A review and new perspectives. IEEE transactions on pattern analysis\nand machine intelligence, 35(8):1798–1828.\n[Bengio et al., 2013b] Bengio, Y., L´eonard, N., and Courville, A. (2013b). Estimating\nor propagating gradients through stochastic neurons for conditional computation.\narXiv preprint arXiv:1308.3432.\n[Berwick and Chomsky, 2016] Berwick, R. C. and Chomsky, N. (2016). Why only us:\nLanguage and evolution. MIT press.\n[Bickerton and Szathm´ary, 2009] Bickerton, D. and Szathm´ary, E. (2009). Biological\nfoundations and origin of syntax, volume 3. Mit Press.\n[Brighton, 2002] Brighton, H. (2002). Compositional syntax from cultural transmis-\nsion. Artiﬁcial life, 8(1):25–54.\n[Brighton and Kirby, 2006] Brighton, H. and Kirby, S. (2006). Understanding linguis-\ntic evolution by visualizing the emergence of topographic mappings. Artiﬁcial life,\n12(2):229–242.\n35\nBibliography\n36\n[Briscoe, 1998] Briscoe, E. J. (1998). Language as a complex adaptive system: co-\nevolution of language and of the language acquisition device. In Proceedings of\neighth computational linguistics in the Netherlands Conference, pages 3–40. Cite-\nseer.\n[Briscoe, 2002] Briscoe, T. (2002). Linguistic Evolution through Language Acquisi-\ntion: Formal and Computational Models. Cambridge University Press.\n[Cangelosi and Parisi, 2012] Cangelosi, A. and Parisi, D. (2012). Simulating the evo-\nlution of language. Springer Science & Business Media.\n[Cao et al., 2018] Cao, K., Lazaridou, A., Lanctot, M., Leibo, J. Z., Tuyls, K., and\nClark, S. (2018). Emergent communication through negotiation. arXiv preprint\narXiv:1804.03980.\n[Christiansen and Kirby, 2003] Christiansen, M. H. and Kirby, S. (2003). Language\nevolution. OUP Oxford.\n[Da Costa and Bond, 2016] Da Costa, L. M. and Bond, F. (2016). Wow! what a use-\nful extension! introducing non-referential concepts to wordnet. In Proceedings of\nthe Tenth International Conference on Language Resources and Evaluation (LREC\n2016), pages 4323–4328.\n[Darwin, 1859] Darwin, C. (1859). On the origin of species by means of natural se-\nlection (j. murray, london).\n[Evans and Levinson, 2009] Evans, N. and Levinson, S. C. (2009). The myth of lan-\nguage universals: Language diversity and its importance for cognitive science. Be-\nhavioral and brain sciences, 32(5):429–448.\n[Gong and Shuai, 2013] Gong, T. and Shuai, L. (2013). Computer simulation as a\nscientiﬁc approach in evolutionary linguistics. Language Sciences, 40:12–23.\n[Havrylov and Titov, 2017] Havrylov, S. and Titov, I. (2017). Emergence of language\nwith multi-agent games: Learning to communicate with sequences of symbols. In\nAdvances in neural information processing systems, pages 2149–2159.\n[Hermann et al., 2017] Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R.,\nSoyer, H., Szepesvari, D., Czarnecki, W. M., Jaderberg, M., Teplyashin, D., et al.\n(2017).\nGrounded language learning in a simulated 3d world.\narXiv preprint\narXiv:1706.06551.\nBibliography\n37\n[Hill et al., 2017] Hill, F., Hermann, K. M., Blunsom, P., and Clark, S. (2017). Under-\nstanding grounded language learning agents. arXiv preprint arXiv:1710.09867.\n[Hochreiter and Schmidhuber, 1997] Hochreiter, S. and Schmidhuber, J. (1997). Long\nshort-term memory. Neural computation, 9(8):1735–1780.\n[Hurford, 1989] Hurford, J. R. (1989). Biological evolution of the saussurean sign as\na component of the language acquisition device. Lingua, 77(2):187–222.\n[Hurford, 1999] Hurford, J. R. (1999). Artiﬁcially growing a numeral system. http:\n//www.lel.ed.ac.uk/˜jim/grownum.html. Accessed March 12, 2019.\n[Hurford et al., 1998] Hurford, J. R., Studdert-Kennedy, M., and Knight, C. (1998).\nApproaches to the evolution of language: Social and cognitive bases. Cambridge\nUniversity Press.\n[Jang et al., 2016] Jang, E., Gu, S., and Poole, B. (2016). Categorical reparameteriza-\ntion with gumbel-softmax. arXiv preprint arXiv:1611.01144.\n[Ke et al., 2008] Ke, J., Gong, T., and Wang, W. S. (2008). Language change and\nsocial networks. Communications in Computational Physics, 3(4):935–949.\n[Kim, 2014] Kim, Y. (2014). Convolutional neural networks for sentence classiﬁca-\ntion. Proceedings of the 2014 Conference on Empirical Methods in Natural Lan-\nguage Processing.\n[Kirby, 1999] Kirby, S. (1999). Function, selection, and innateness: The emergence\nof language universals. OUP Oxford.\n[Kirby and Hurford, 2002] Kirby, S. and Hurford, J. R. (2002). The emergence of\nlinguistic structure: An overview of the iterated learning model. In Simulating the\nevolution of language, pages 121–147. Springer.\n[Knight et al., 2000] Knight, C., Studdert-Kennedy, M., and Hurford, J. (2000). The\nevolutionary emergence of language: social function and the origins of linguistic\nform. Cambridge University Press.\n[Kottur et al., 2017] Kottur, S., Moura, J. M., Lee, S., and Batra, D. (2017). Nat-\nural language does not emerge’naturally’in multi-agent dialog.\narXiv preprint\narXiv:1706.08502.\nBibliography\n38\n[Lewis, 1969] Lewis, D. (1969). Convention: A philosophical study. John Wiley &\nSons.\n[Lieberman, 2006] Lieberman, P. (2006).\nToward an evolutionary biology of lan-\nguage. Harvard University Press.\n[Locatello et al., 2018] Locatello, F., Bauer, S., Lucic, M., Gelly, S., Sch¨olkopf, B.,\nand Bachem, O. (2018). Challenging common assumptions in the unsupervised\nlearning of disentangled representations. arXiv preprint arXiv:1811.12359.\n[Lowe et al., 2019] Lowe, R., Foerster, J., Boureau, Y.-L., Pineau, J., and Dauphin, Y.\n(2019). On the pitfalls of measuring emergent communication. In Proceedings of\nthe 18th International Conference on Autonomous Agents and MultiAgent Systems,\npages 693–701. International Foundation for Autonomous Agents and Multiagent\nSystems.\n[Matuszek, 2018] Matuszek, C. (2018). Grounded language learning: Where robotics\nand nlp meet. In IJCAI, pages 5687–5691.\n[Mikolov et al., 2013] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean,\nJ. (2013). Distributed representations of words and phrases and their composition-\nality. In Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger,\nK. Q., editors, Advances in Neural Information Processing Systems 26, pages 3111–\n3119. Curran Associates, Inc.\n[Minett and Wang, 2008] Minett, J. W. and Wang, W. S. (2008).\nModelling en-\ndangered languages: The effects of bilingualism and social structure.\nLingua,\n118(1):19–45.\n[Mnih et al., 2015] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J.,\nBellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G.,\net al. (2015). Human-level control through deep reinforcement learning. Nature,\n518(7540):529.\n[Mordatch and Abbeel, 2018] Mordatch, I. and Abbeel, P. (2018).\nEmergence of\ngrounded compositional language in multi-agent populations.\nIn Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence.\nBibliography\n39\n[Munroe and Cangelosi, 2002] Munroe, S. and Cangelosi, A. (2002). Learning and\nthe evolution of language: The role of cultural variation and learning costs in the\nbaldwin effect. Artiﬁcial life, 8(4):311–339.\n[Papineni et al., 2002] Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).\nBleu: a method for automatic evaluation of machine translation. In Proceedings of\nthe 40th annual meeting on association for computational linguistics, pages 311–\n318. Association for Computational Linguistics.\n[Puglisi et al., 2008] Puglisi, A., Baronchelli, A., and Loreto, V. (2008). Cultural route\nto the emergence of linguistic categories. Proceedings of the National Academy of\nSciences, 105(23):7936–7940.\n[Rennie et al., 2017] Rennie, S. J., Marcheret, E., Mroueh, Y., Ross, J., and Goel, V.\n(2017). Self-critical sequence training for image captioning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 7008–7024.\n[Siegelmann and Sontag, 1992] Siegelmann, H. T. and Sontag, E. D. (1992). On the\ncomputational power of neural nets. In Proceedings of the Fifth Annual Workshop\non Computational Learning Theory, COLT ’92, pages 440–449, New York, NY,\nUSA. ACM.\n[Silver et al., 2017] Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang,\nA., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the\ngame of go without human knowledge. Nature, 550(7676):354.\n[Smith et al., 2003a] Smith, K., Brighton, H., and Kirby, S. (2003a). Complex systems\nin language evolution: the cultural emergence of compositional structure. Advances\nin Complex Systems, 6(04):537–558.\n[Smith et al., 2003b] Smith, K., Kirby, S., and Brighton, H. (2003b). Iterated learning:\nA framework for the emergence of language. Artiﬁcial life, 9(4):371–386.\n[Smith et al., 2013] Smith, K., Tamariz, M., and Kirby, S. (2013). Linguistic structure\nis an evolutionary trade-off between simplicity and expressivity. In Proceedings of\nthe Annual Meeting of the Cognitive Science Society, volume 35.\n[Socher et al., 2013] Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D.,\nNg, A., and Potts, C. (2013). Recursive deep models for semantic compositionality\nBibliography\n40\nover a sentiment treebank. In Proceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing, pages 1631–1642, Seattle, Washington,\nUSA. Association for Computational Linguistics.\n[Steels, 2005] Steels, L. (2005). The emergence and evolution of linguistic structure:\nfrom lexical to grammatical communication systems. Connection science, 17(3-\n4):213–230.\n[Sutskever et al., 2014] Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to\nsequence learning with neural networks. In Advances in neural information pro-\ncessing systems, pages 3104–3112.\n[Vinyals et al., 2015] Vinyals, O., Bengio, S., and Kudlur, M. (2015). Order matters:\nSequence to sequence for sets. arXiv preprint arXiv:1511.06391.\n[Vogt, 2005] Vogt, P. (2005). On the acquisition and evolution of compositional lan-\nguages: Sparse input and the productive creativity of children. Adaptive Behavior,\n13(4):325–346.\n[Williams, 1992] Williams, R. J. (1992). Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine learning, 8(3-4):229–\n256.\n[Wittgenstein, 1953] Wittgenstein, L. (1953). Philosophical investigations. John Wi-\nley & Sons.\n[Yu et al., 2018] Yu, H., Zhang, H., and Xu, W. (2018). Interactive grounded language\nacquisition and generalization in a 2d world. arXiv preprint arXiv:1802.01433.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-11-04",
  "updated": "2019-11-04"
}