{
  "id": "http://arxiv.org/abs/1911.01248v1",
  "title": "A Holistic Natural Language Generation Framework for the Semantic Web",
  "authors": [
    "Axel-Cyrille Ngonga Ngomo",
    "Diego Moussallem",
    "Lorenz Bühmann"
  ],
  "abstract": "With the ever-growing generation of data for the Semantic Web comes an\nincreasing demand for this data to be made available to non-semantic Web\nexperts. One way of achieving this goal is to translate the languages of the\nSemantic Web into natural language. We present LD2NL, a framework for\nverbalizing the three key languages of the Semantic Web, i.e., RDF, OWL, and\nSPARQL. Our framework is based on a bottom-up approach to verbalization. We\nevaluated LD2NL in an open survey with 86 persons. Our results suggest that our\nframework can generate verbalizations that are close to natural languages and\nthat can be easily understood by non-experts. Therewith, it enables non-domain\nexperts to interpret Semantic Web data with more than 91\\% of the accuracy of\ndomain experts.",
  "text": "A Holistic Natural Language Generation Framework for the Semantic\nWeb\nAxel-Cyrille Ngonga Ngomo1 Diego Moussallem1 Lorenz B¨uhmann2\n1Data Science Group, University of Paderborn, Germany\n2AKSW Research Group, University of Leipzig, Germany\n{first.lastname}@upb.de\nlastname@informatik.uni-leipzig.de\nAbstract\nWith the ever-growing generation of data\nfor the Semantic Web comes an increas-\ning demand for this data to be made avail-\nable to non-semantic Web experts. One\nway of achieving this goal is to translate\nthe languages of the Semantic Web into\nnatural language. We present LD2NL, a\nframework for verbalizing the three key\nlanguages of the Semantic Web, i.e., RDF,\nOWL, and SPARQL. Our framework is\nbased on a bottom-up approach to verbal-\nization. We evaluated LD2NL in an open\nsurvey with 86 persons. Our results suggest\nthat our framework can generate verbaliza-\ntions that are close to natural languages\nand that can be easily understood by non-\nexperts. Therewith, it enables non-domain\nexperts to interpret Semantic Web data with\nmore than 91% of the accuracy of domain\nexperts.\n1\nIntroduction\nNatural Language Generation (NLG) is the process\nof automatically generating coherent Natural Lan-\nguage (NL) text from non-linguistic data (Reiter\nand Dale, 2000a). Recently, the ﬁeld has seen\nan increased interest in the development of NLG\nsystems focusing on verbalizing resources from\nSemantic Web (SW) data (Gardent et al., 2017).\nThe SW aims to make information available on the\nWeb easier to process for machines and humans.\nHowever, the languages underlying this vision,\ni.e., Resource Description Framework (RDF),\nSPARQL Query Language (SPARQL) and Web\nOntology Language (OWL), are rather difﬁcult\nto understand for non-expert users. For example,\nwhile the meaning of the OWL class expres-\nsion\nClass:\nProfessor SubClassOf:\nworksAt SOME University is obvious to\nevery SW expert, this expression (“Every professor\nworks at a university”) is rather difﬁcult to fathom\nfor lay persons.\nPrevious works such as SPARQL2NL (Ngonga\nNgomo et al., 2013) and SPARTIQULATION (Ell\net al., 2012) have already shown the usefulness\nof the verbalization of SPARQL 1 and RDF in ar-\neas such as question answering (Lehmann et al.,\n2012) and the explanation of the output of systems\nbased onSW technologies (Ngonga Ngomo et al.,\n2013). However, other SW languages are rarely\ninvestigated, such as OWL.\nIn this paper, we present an open-source holis-\ntic NLG framework for the SW, named LD2NL,\nwhich facilitates the verbalization of the three\nkey languages of the SW, i.e., RDF, OWL, and\nSPARQL into NL. Our framework is based on a\nbottom-up paradigm for verbalizing SW data. Ad-\nditionally, LD2NL builds upon SPARQL2NL as it\nis open-source and the paradigm it follows can be\nreused and ported to RDF and OWL. Thus, LD2NL\nis capable of generating either a single sentence or a\nsummary of a given resource, rule, or query. To val-\nidate our framework, we evaluated LD2NL using\nexperts 66 in Natural Language Processing (NLP)\nand SW as well as 20 non-experts who were lay\nusers or non-users of SW. The results suggest that\nLD2NL generates texts which can be easily under-\nstood by humans. The version of LD2NL used in\nthis paper, all experimental results will be publicly\navailable.\n2\nRelated Work\nAccording to Gatt and Krahmer (2017), there has\nbeen a plenty of works which investigated the gen-\neration of NL texts from Semantic Web Technolo-\ngies (SWT) as an input data (Cimiano et al., 2013;\n1SPARQL is the query language for RDF data.\narXiv:1911.01248v1  [cs.CL]  4 Nov 2019\nDuma and Klein, 2013; Ell and Harth, 2014; Biran\nand McKeown, 2015). However, the subject of re-\nsearch has only recently gained signiﬁcant momen-\ntum due to the great number of published works in\nthe WebNLG (Colin et al., 2016) challenge along\nwith deep learning techniques (Sleimi and Gardent,\n2016; Mrabet et al., 2016). RDF has also been\nshowing promising beneﬁts to the generation of\nbenchmarks for evaluating NLG systems (Gardent\net al., 2017; Perez-Beltrachini et al., 2016).\nDespite the plethora of recent works written on\nhandling RDF data, only a few have exploited the\ngeneration of NL from OWL and SPARQL. For\ninstance, Androutsopoulos et al. (2013) generates\nsentences in English and Greek from OWL ontolo-\ngies. Also, SPARQL2NL (Ngonga Ngomo et al.,\n2013) uses rules to verbalize atomic constructs and\ncombine their verbalization into sentences. There-\nfore, our goal with LD2NL is to provide a complete\nframework to verbalize SW concepts rather than\nbecome the state of the art on the respective tasks.\n3\nBackground\n3.1\nOWL\nOWL2 (OWL Working Group, 2009) is the de-facto\nstandard for machine processable and interoperable\nontologies on the SW. In its second version, OWL\nis equivalent to the description logic SROIQ(D).\nSuch expressiveness has a higher computational\ncost but allows the development of interesting ap-\nplications such as automated reasoning (B¨uhmann\net al., 2016). OWL 2 ontologies consist of the\nfollowing three different syntactic categories:\nEntities, such as classes, properties, and individ-\nuals, are identiﬁed by IRIs. They form the primi-\ntive terms and constitute the basic elements of an\nontology. Classes denote sets of individuals and\nproperties link two individuals or an individual and\na data value along a property. For example, a class\n:Animal can be used to represent the set of all\nanimals. Similarly, the object property :childOf\ncan be used to represent the parent-child relation-\nship and the data property :birthDate assigns\na particular birth date to an individual. Finally,\nthe individual :Alice can be used to represent a\nparticular person called ”Alice”.\nExpressions represent complex notions in the\ndomain being described. For example, a class ex-\npression describes a set of individuals in terms\n2www.w3.org/TR/owl2-overview/\nof the restrictions on the individuals’ characteris-\ntics. OWL offers existential (SOME) or universal\n(ONLY) qualiﬁers and a variety of typical logical\nconstructs, such as negation (NOT), other Boolean\noperators (OR, AND), and more constructs such as\ncardinality restriction (MIN, MAX, EXACTLY)\nand value restriction (VALUE), to create class ex-\npressions. Such constructs can be combined in\narbitrarily complex class expressions CE according\nto the following grammar\nCE = A | C AND D | C OR D | NOT C | R\nSOME C | R ONLY C | R MIN n | R MAX\nn | R EXACTLY n | R VALUE a | {a1\n,...,am}\nwhere A is an atomic class, C and D are class ex-\npressions, R is an object property, a as well as a1\nto am with m ≥1 are individuals, and n ≥0 is an\ninteger.\nAxioms are statements that are asserted to be\ntrue in the domain being described.\nUsually,\none distinguish between (1) terminological and\n(2) assertional axioms.\n(1) terminological ax-\nioms are used to describe the structure of the\ndomain, i.e., the relationships between classes\nresp.\nclass expressions.\nFor example, using\na subclass axiom (SubClassOf:), one can state\nthat the class :Koala is a subclass of the class\n:Animal.\nClasses can be subclasses of other\nclasses, thus creating a taxonomy.\nIn addi-\ntion, axioms can arrange properties in hierarchies\n(SubPropertyOf:) and can assign various char-\nacteristics (Characteristics:) such as transitivity\nor reﬂexivity to them.\n(2) Assertional axioms\nformulate facts about individuals, especially the\nclasses they belong to and their mutual relation-\nships. OWL can be expressed in various syntaxes\nwith the most common computer readable syntax\nbeing RDF/XMLA more human-readable format\nis the Manchester OWL Syntax (MOS) (Horridge\net al., 2006). For example, the class expression\nthat models people who work at a university that is\nlocated in Spain could be as follows in MOS:\nPerson AND worksAt SOME (University AND\nlocatedIn VALUE Spain)\nLikewise, expressing that every professor works at\na university would read as\nClass: Professor\nSubClassOf: worksAt SOME University\n3.2\nRDF\nRDF (RDF Working Group, 2014) uses a graph-\nbased data model for representing knowledge.\nStatements in RDF are expressed as so-called\ntriples of the form (subject, predicate,\nobject). RDF subjects and predicates are In-\nternationalized Resource Identiﬁerss (IRIs) and ob-\njects are either IRIs or literals.3 RDF literals always\nhave a datatype that deﬁnes its possible values. A\npredicate denotes a property and can also be seen as\na binary relation taking subject and object as argu-\nments. For example, the following triple expresses\nthat Albert Einstein was born in Ulm:\n:Albert_Einstein :birthPlace :Ulm .\n3.3\nSPARQL\nCommonly, the selection of subsets of RDF is\nperformed using the SPARQL query language.4\nSPARQL can be used to express queries across di-\nverse data sources. Query forms contain variables\nthat appear in a solution result. They can be used\nto select all or a subset of the variables bound in\na pattern match. They exist in four different in-\nstantiations, i.e., SELECT, CONSTRUCT, ASK and\nDESCRIBE. The SELECT query form is the most\ncommonly used and is used to return rows of vari-\nable bindings. Therefore, we use this type of query\nin our explanation. CONSTRUCT allows to cre-\nate a new RDF graph or modify the existing one\nthrough substituting variables in a graph templates\nfor each solution. ASK returns a Boolean value\nindicating whether the graph contains a match or\nnot. Finally, DESCRIBE is used to return all triples\nabout the resources matching the query. For exam-\nple, 1 represents the following query “Return all\nscientists who were born in Ulm”.\nSELECT ?person\nWHERE {\n?person a dbo:Scientist;\ndbo:birthPlace dbr:Ulm.\n}\nListing 1: All scientists who were born in Ulm\n4\nLD2NL Framework\nThe goal of LD2NL is to provide an integrated sys-\ntem which generates a complete and correct NL\n3For simplicity, we omit RDF blank nodes in subject or\nobject position.\n4http://www.w3.org/TR/sparql11-query\nrepresentation for the most common used SW mod-\neling languages RDF and OWL, and SPARQL. In\nterms of the standard model of NL generation pro-\nposed by Reiter & Dale (Reiter and Dale, 2000b),\nour steps mainly play the role of the micro-planner,\nwith focus on aggregation, lexicalization, referring\nexpressions and linguistic realization. In the fol-\nlowing, we present our approach to formalizing NL\nsentences for each of the supported languages.\n4.1\nFrom RDF to NL\n4.1.1\nLexicalization\nThe lexicalization of RDF triples must be able to\ndeal with resources, classes, properties and literals.\nClasses and resources The lexicalization of\nclasses and resources is carried out as follows:\nGiven a URI u we ask for the English label of\nu using a SPARQL query.5 If such a label does not\nexist, we use either the fragment of u (the string\nafter #) if it exists, else the string after the last oc-\ncurrence of /. Finally this NL representation is\nrealized as a noun phrase, and in the case of classes\nis also pluralized. As an example, :Person is\nrealized as people (its label).\nProperties The lexicalization of properties re-\nlies on the insight that most property labels are\neither nouns or verbs. While the mapping of a\nparticular property p can be unambiguous, some\nproperty labels are not as easy to categorize. For\nexamples, the label crosses can either be the\nplural form of the noun cross or the third person\nsingular present form of the verb to cross. To\nautomatically determine which realization to use,\nwe relied on the insight that the ﬁrst and last word\nof a property label are often the key to determining\nthe type of the property: properties whose label\nbegins with a verb (resp. noun or gerund) are most\nto be realized as verbs (resp. nouns). We devised\na set of rules to capture this behavior, which we\nomit due to space restrictions. In some cases (such\nas crosses) none of the rules applied. In these\ncases, we compare the probability of P(p|noun)\nand P(p|verb) by measuring\nP(p|X) =\nP\nt∈synset(p|X)\nlog2(f(t))\nP\nt′∈synset(p)\nlog2(f(t′)) ,\n(1)\nwhere synset(p) is the set of all synsets of p,\nsynset(p|X) is the set of all synsets of p that are of\n5Note that it could be any property which returns a NL\nrepresentation of the given URI, see (Ell et al., 2011).\nthe syntactic class X ∈{noun, verb} and f(t) is\nthe frequency of use of p in the sense of the synset\nt according to WordNet. For\nP(p|verb)\nP(p|noun) ≥θ,\n(2)\nwe choose to realize p as a noun; else we realized it\nas a verb. For θ = 1, for example, dbo:crosses\nis realized as a verb.\nLiterals Literals in an RDF graph usually con-\nsist of a lexical form LF and a datatype IRI DT,\nrepresented as \"LF\"ˆˆ<DT>. Optionally, if the\ndatatype is rdf:langString, a non-empty lan-\nguage tag is speciﬁed and the literal is denoted\nas language-tagged string6.\nThe realization of\nlanguage-tagged strings is done by using simply\nthe lexical form, while omitting the language tag.\nFor example, \"Albert Einstein\"@en is re-\nalized as Albert Einstein. For other types\nof literals, we further differentiate between built-in\nand user-deﬁned datatypes. For the former, we also\nuse the lexical form, e.g. \"123\"ˆˆxsd:int ⇒\n123, while the latter are processed by using the\nliteral value with its representation of the datatype\nIRI, e.g., \"123\"ˆˆdt:squareKilometre as\n123 square kilometres.\n4.1.2\nRealizing single triples\nThe realization ρ of a triple (s p o) depends\nmostly on the verbalization of the predicate p. If p\ncan be realized as a noun phrase, then a possessive\nclause can be used to express the semantics of (s\np o), more formally\n1. ρ(s p o) ⇒\nposs(ρ(p),ρ(s)) ∧subj(BE,ρ(p))\n∧dobj(BE,ρ(o))\nFor example,\nif ρ(p) is a relational noun\nlike\nbirth place\ne.g.\nin\nthe\ntriple\n(:Albert Einstein :birthPlace\n:Ulm),\nthen the verbalization is\nAlbert\nEinstein’s birth place is Ulm. Note\nthat BE stands for the verb “to be”.\nIn case\np’s realization is a verb, then the triple can be\nverbalized as follows:\n2. ρ(s p o) ⇒\nsubj(ρ(p),ρ(s)) ∧dobj(ρ(p),ρ(o))\nFor\nexample,\nin\n(:Albert Einstein\n:influenced :Nathan Rosen) ρ(p) is the\nverb influenced, thus, the verbalization is\nAlbert Einstein inﬂuenced Nathan Rosen.\n6In RDF 1.0 literals have been divided into ’plain’ literals\nwith no type and optional language tags, and typed literals.\n4.2\nRealization - RDF Triples to NL\nThe same procedure of generating a single triple\ncan be applied for the generation of each triple in a\nset of triples. However, the NL output would con-\ntain redundant information and consequently sound\nvery artiﬁcial. Thus, the goal is to transform the\ngenerated description to sound more natural. To\nthis end, we focus on two types of transformation\nrules (cf. (Dalianis and Hovy, 1996)): ordering and\nclustering and grouping. In the following, we de-\nscribe the transformation rules we employ in more\ndetail. Note that clustering and ordering (4.2.1) is\napplied before grouping (4.2.2).\n4.2.1\nClustering and ordering rules\nWe process the input trees in descending order with\nrespect to the frequency of the variables they con-\ntain, starting with the projection variables and only\nafter that turning to other variables. As an example,\nconsider the following triples about two of the most\nknown people in the world:\n:William_Shakespeare rdf:type :Writer .\n:Albert_Einstein :birthPlace :Ulm .\n:Albert_Einstein :deathPlace :Princeton\n:Albert_Einstein rdf:type :Scientist .\n:William_Shakespeare :deathDate\n\"1616-04-23\"ˆˆxsd:date .\nThe ﬁve triples are verbalized as given in 3a–\n3e. Clustering and ordering ﬁrst take all sentences\ncontaining the subject :Albert Einstein, i.e.\n3b –3d, which are ordered such that copula-\ntive sentences (such as Albert Einstein is a scien-\ntist) come before other sentences, and then takes\nall sentences containing the remaining subject\n:William Shakespeare in 3a and 3e result-\ning in a sequence of sentences as in 4.\n3.\n(a) William Shakespeare is a writer.\n(b) Albert Einstein’s birth place is Ulm.\n(c) Albert Einstein’s death place is Princeton.\n(d) Albert Einstein is a scientist.\n(e) William Shakespeare’s death date is 23 April\n1616.\n4. Albert Einstein is a scientist.\nAlbert Einstein’s\nbirth place is Ulm. Albert Einstein’s death place\nis Princeton. William Shakespeare’s is a writer.\nWilliam Shakespeare’s death date is 23 April 1616.\n4.2.2\nGrouping\nDalianis and Hovy (1996) describe grouping as\na process “collecting clauses with common ele-\nments and then collapsing the common elements”.\nThe common elements are usually subject noun\nphrases and verb phrases (verbs together with ob-\nject noun phrases), leading to subject grouping and\nobject grouping. To maximize the grouping ef-\nfects, we collapse common preﬁxes and sufﬁxes\nof sentences, irrespective of whether they are full\nsubject noun phrases or complete verb phrases. In\nthe following we use X1, X2,... XN as variables\nfor the root nodes of the input sentences and Y as\nvariable for the root node of the output sentence.\nFurthermore, we abbreviate a subject subj(Xi, si)\nas si, an object dobj(Xi, oi) as oi, and a verb\nroot(ROOTi, vi) as vi.\nSubject grouping collapses the predicates (i.e.\nverb and object) of two sentences if their subjects\nare the same, as speciﬁed in 5 (abbreviations as\nabove).\n5. ρ(s1) = ρ(s2) ∧cc(v1, coord)\n⇒\nroot(Y, coord(v1, v2)) ∧subj(v1, s1) ∧\ndobj(v1, o1) ∧subj(v2, s1) ∧dobj(v1, o2)\nAn example are the sentences given in 6, which\nshare the subject\nAlbert Einstein and thus can be\ncollaped into a single sentence.\n6. Albert Einstein is a scientist and Albert Einstein is\nknown for general relativity.\n⇒\nAlbert Einstein is a scientist and known for\ngeneral relativity.\nObject grouping collapses the subjects of two\nsentences if the realizations of the verbs and ob-\njects of the sentences are the same, where the\ncoord ∈{and, or} is the coordination combining\nthe input sentences X1 and X2, and coord ∈\n{conj, disj} is the corresponding coordination\ncombining the subjects.\n7. ρ(o1) = ρ(o2) ∧ρ(v1) = ρ(v2) ∧cc(v1, coord)\n⇒\nroot(Y, PLURAL(v1))\n∧\nsubj(v1, coord(s1, s2)) ∧dobj(v1, o1)\nFor example, the sentences in 8 share their verb\nand object, thus they can be collapsed into a single\nsentence. Note that to this end the singular aux-\niliary was needs to be transformed into its plural\nform were.\n8. Benjamin Franklin was born in Boston. Leonard\nNimoy was born in Boston. ⇒Benjamin Franklin\nand Leonard Nimoy were born in Boston.\n4.3\nFrom OWL to NL\nOWL 2 ontologies consist of Entities, Expressions\nand Axioms as introduced in subsection 3.1. While\nboth expressions and axioms can be mapped to\nRDF7, i.e. into a set of RDF triples, using this map-\nping and applying the triple-based verbalization on\n7http://bit.ly/2Mc0vIw\nit would lead to a non-human understandable text\nin many cases. For example, the intersection of two\nclasses :A and :B can be represented in RDF by\nthe six triples\n_:x rdf:type owl:Class .\n_:x owl:intersectionOf _:y1 .\n_:y1 rdf:first :A .\n_:y1 rdf:rest _:y2 .\n_:y2 rdf:first :B .\n_:y2 rdf:rest rdf:nil .\nThe verbalization of these triples would re-\nsult\nin\nSomething that is a class\nand the intersection of something\nwhose first is A and whose rest\nis something whose first is B and\nwhose rest ist nil., which is obviously\nfar away from how a human would express it\nin NL.\nTherefore, generating NL from OWL\nrequires a different procedure based on its syntactic\ncategories, OWL expressions and OWL axioms.\nWe show the general rules for each of them in the\nfollowing.\n4.3.1\nOWL Class Expressions\nIn theory, class expressions can be arbitrarily com-\nplex, but as it turned out in some previous analy-\nsis (Power and Third, 2010), in practice they sel-\ndom arise and can be seen as some corner cases.\nFor example, an ontology could contain the follow-\ning class expression about people and their birth\nplace:\nPerson AND birthPlace SOME (City AND\nlocatedIn VALUE France)\nClass expressions do have a tree-like structure and\ncan simply be parsed into a tree by means of the\nbinary OWL class expressions constructors con-\ntained in it. For our example, this would result in\nthe following tree:\nAND\nSOME\nAND\nVALUE\nFrance\nlocatedIn\nCity\nbirthPlace\nPerson\nSuch a tree can be traversed in post-order,\ni.e. sub-trees are processed before their parent\nnodes recursively.\nFor the sake of simplicity,\nwe only process sub-trees that represent proper\nclass expression in our example, i.e.\nwe omit\nbirthPlace,\nlocatedIn,\nand\nFrance.\nMoreover and again for simplicity, we’ll explain\nthe transformation process by starting from\nthe right-hand side of the tree.\nThus, in our\nexample we begin with the class expression\nCity which is transformed to everything\nthat is a city and locatedIn VALUE\nFrance resulting in everything that is\nlocated in France by application of a rule.\nBoth class expressions are used in the conjunction\nCity AND locatedIn VALUE France.\nThus, the next step would be to merge both\nphrases. An easy way is to use the coordinating\nconjunction and, i.e.\neverything that\nis a city and everything that is\nlocated in France.\nAlthough the output\nof this transformation is correct, it still contains\nunnecessarily redundant information. Therefore,\nwe apply the aggregation procedure described\nin Section 4.2.2, i.e. we get everything that\nis a city and located in France.\nYet, the aggregation can still be improved: if there\nis any atomic class in the conjunction, we know\nthat this is more speciﬁc than the placeholder\neverything. Thus, we can replace it by the\nplural form of the class, ﬁnally resulting in\ncities that are located in France.\nThe same procedure is applied for its parent class\nexpression being the existential restriction\nbirthPlace SOME (City AND locatedIn\nVALUE France)\nThis will be transformed to everything\nwhose birth place is a city that\nis located in France. Note, that we used\nthe singular form here, assuming that the property\nbirthPlace is supposed to be functional\nin the ontology.\nIn the last step, we process\nthe class expression Person, which gives us\neverything that is a person.\nAgain,\ndue to the conjunction we merge this result with\nwith the previous one, such that in the end we\nget\npeople whose birth place is a\ncity that is located in France.\n4.3.2\nOWL Axioms\nAs we described in Section 4.3, OWL axioms can\nroughly be categorized into terminological and as-\nsertional axioms. Therefore, we have different pro-\ncedures for processing each category:\nAssertional Axioms (ABox Axioms) - Most\nassertional axioms assert individuals to atomic\nclasses or relate individuals to another individual\nresp. literal value. For example, axioms about the\ntype as well as birth place and birth date of Albert\nEinstein can be expressed by\nIndividual: Albert_Einstein\nTypes: Person\nFacts: birthPlace Ulm, birthDate \"\n1879-03-14\"ˆˆxsd:date\nThose axioms can simply be rewritten as triples,\nthus, we can use the same procedure as we do\nfor triples (Section 4.1.2).\nConverting them\ninto NL gives us Albert Einstein is a\nperson whose birth place is Ulm\nand whose birth date is 14 March\n1879.\nOWL also allows for assigning an\nindividual to a complex class expression.\nIn\nthat case we’ll use our conversion of OWL class\nexpressions as described in Section 4.3.1.\nTerminological Axioms (TBox Axioms) - Ac-\ncording to Power and Third (2010), most of the ter-\nminological axioms used in ontologies are subclass\naxioms. By deﬁnition, subclass and superclass can\nbe arbitrarily complex class expressions CE1 and\nCE2, i.e. CE1 SubClassOf CE2, but in praxis it is\nquite often only used with atomic classes as sub-\nclass or even more simple with the superclass also\nbeeing an atomic class. Nevertheless, we support\nany kind of subclass axiom and all other logical\nOWL axioms in LD2NL. For simplicity, we outline\nhere how we verbalize subclass axioms in LD2NL.\nThe semantics of a subclass axiom denotes that\nevery individual of the subclass also belongs to the\nsuperclass. Thus, the verbalization seems to be\nrelatively straightforward, i.e. we verbalize both\nclass expressions and follow the template : every\nρ(CE1) is a ρ(CE2). Obviously, this works pretty well\nfor subclass axioms with atomic classes only. For\nexample, the axiom\nClass: Scientist\nSubClassOf: Person\nis verbalized as every scientist is a\nperson.\n4.4\nFrom SPARQL to NL\nA SPARQL SELECT query can be regarded as con-\nsisting of three parts: (1) a body section B, which\ndescribes all data that has to be retrieved, (2) an\noptional section O, which describes the data items\nthat can be retrieved by the query if they exist,\nand (3) a modiﬁer section M, which describes all\nsolution sequences, modiﬁers and aggregates that\nare to be applied to the result of the previous two\nsections of the query. Let Var be the set of all\nvariables that can be used in a SPARQL query. In\naddition, let R be the set of all resources, P the set\nof all properties and L the set of all literals con-\ntained in the target knowledge base of the SPARQL\nqueries at hand. We call x ∈Var ∪R ∪P ∪L\nan atom. The basic components of the body of\na SPARQL query are triple patterns (s, p, o) ∈\n(Var ∪R) × (Var ∪P) × (Var ∪R ∪L). Let\nW be the set of all words in the dictionary of our\ntarget language. We deﬁne the realization func-\ntion ρ : Var ∪R ∪P ∪L →W ∗as the function\nwhich maps each atom to a word or sequence of\nwords from the dictionary. The extension of ρ to all\nSPARQL constructs maps all atoms x to their real-\nization ρ(x) and deﬁnes how these atomic realiza-\ntions are to be combined. We denote the extension\nof ρ by the same label ρ for the sake of simplicity.\nWe adopt a rule-based approach to achieve this goal,\nwhere the rules extending ρ to all valid SPARQL\nconstructs are expressed in a conjunctive manner.\nThis means that for premises P1, . . . , Pn and con-\nsequences K1, . . . , Km we write P1 ∧. . . ∧Pn ⇒\nK1∧. . .∧Km. The premises and consequences are\nexplicated by using an extension of the Stanford\ndependencies8.\nFor example, a possessive dependency between\ntwo phrase elements e1 and e2 is represented as\nposs(e1, e2).\nFor the sake of simplicity, we\nslightly deviate from the Stanford vocabulary by\nnot treating the copula to be as an auxiliary, but\ndenoting it as BE. Moreover, we extend the vo-\ncabulary by the constructs conj and disj which\ndenote the conjunction resp. disjunction of two\nphrase elements. In addition, we sometimes reduce\nthe construct subj(y,x) ∧dobj(y,z) to the\ntriple (x,y,z) ∈W 3.\n5\nExperiments\nWe evaluated our approach in three different exper-\niments based on human ratings. We divided the\nvolunteers into two groups—domain experts and\nnon-experts. The group of domain experts com-\nprised 66 persons while there were 20 non-experts\nforming the second group. In the ﬁrst experiment,\nan OWL axiom and its verbalization were shown to\nthe experts who were asked to rate the verbalization\n8For a complete description of the vocabulary, see https:\n//stanford.io/2EzMjmo.\nregarding the two following measures according\nto Gardent et al. (2017): (1) Adequacy: Does the\ntext contain only and all the information from the\ndata? (2) Fluency: Does the text sound ﬂuent and\nnatural?. For both measures the volunteers were\nasked to rate on a scale from 1 (Very Bad) to 5\n(Very Good). The experiment was carried out us-\ning 41 axioms of the Koala ontology.9 Because\nof the complexity of OWL axioms, only domain\nexperts were asked to perform this experiment.\nIn the second experiment, a set of triples describ-\ning a single resource and their verbalization were\nshown to the volunteers. The experts were asked\nto rate the verbalization regarding adequacy, ﬂu-\nency and completeness, i.e., whether all triples have\nbeen covered. The non-experts were only asked to\nrate the ﬂuency. The experiment was carried out\nusing 6 DBpedia resources.In the third experiment,\nthe verbalization of an OWL class and 5 resources\nwere shown to the human raters. For non-experts,\nthe resources have been verbalized as well, while\nfor domain experts the resources were presented\nas triples. The task of the raters was to identify\nthe resource that ﬁts the class description and, thus,\nis an instance of the class. We used 4 different\nOWL axioms and measured the amount of correct\nidentiﬁed class instances.\nResults In our ﬁrst series of experiments, the ver-\nbalization of OWL axioms, we achieved an average\nadequacy of 4.4 while the ﬂuency reached 4.38. In\naddition, more than 77% of the verbalizations were\nassigned the maximal adequacy (i.e., were assigned\na score of 5, see Fig. 1). The maximal score for ﬂu-\nency was achieved in more than 69% of the cases\n(see Fig. 1). This clearly indicates that the ver-\nbalization of axioms generated by LD2NL can be\neasily understood by domain experts and contains\nall the information necessary to access the input\nOWL class expression.\nExperiments on the verbalization of summaries\nfor RDF resources revealed that verbalizing re-\nsource summaries is a more difﬁcult task. While\nthe adequacy of the verbalization was assigned an\naverage score of 3.92 by experts (see Fig. 2), the\nﬂuency was assigned a average score of 3.47 by\nexperts and 3.0 by non-experts (see Fig. 2). What\nthese results suggest is that (1) our framework\ngenerates sentences that are close to that which\na domain expert would also generate (adequacy).\nHowever (2) while the sentence is grammatically\n9https://bit.ly/2K8BWts\n0\n100\n200\n300\n400\n500\nNumber of ratings\n1\n2\n3\n4\n5\nAdequacy\nExperts\n0\n100\n200\n300\n400\n500\nNumber of ratings\n1\n2\n3\n4\n5\nFluency\nExperts\nFigure 1: Experiment I: adequacy (left) and ﬂuency (right) ratings\n0\n20\n40\n60\n80\n100\nNumber of ratings\n1\n2\n3\n4\n5\nAdequacy\nExperts\n0\n10\n20\n30\n40\n50\n60\n70\nNumber of ratings\n1\n2\n3\n4\n5\nFluency\nExperts\nNon-experts\n0\n20\n40\n60\n80\n100\n120\n140\nNumber of ratings\n1\n2\n3\n4\n5\nCompleteness\nExperts\nFigure 2: Experiment II: adequacy (left), ﬂuency (middle) and completeness (left) results\nsufﬁcient for the experts, it is regarded by non-\ndomain experts (which were mostly linguists, i.e.,\nthe worst-case scenario for such an evaluation) as\nbeing grammatically passably good but still worthy\nof improvement. The completeness rating achieves\na score of 4.31 on average (see Fig. 2). This was\nto be expected as we introduced a rule to shorten\nthe description of resources that contain more than\n5 triples which share a common subject and predi-\ncate. Finally, we measured how well the users and\nexperts were able to understand the meaning of the\ntext generated by our approach. As expected, the\ndomain experts outperform the non-expert users by\nbeing able to ﬁnd the answers to 87.2% of the ques-\ntions. The score achieved by non-domain experts,\ni.e., 80%, still suggest that our framework is able to\nbridge the gap pertaining to understand RDF and\nOWL for non-experts from 0% to 80%, which is\nmore than 91.8% of the performance of experts.\nDiscussion Our evaluation results suggest that\nthe verbalization of these languages is a non-trivial\ntask that can be approached by using a bottom-up\napproach. As expected, the verbalization of short\nexpressions leads to sentences which read as if\nthey have been generated by a human. However,\ndue to the complexity of the semantics that can\nbe expressed by the languages at hand, long ex-\npressions can sound mildly artiﬁcial. Our results\nhowever also suggest that although the text gener-\nated can sound artiﬁcial, it is still clear enough to\nenable non-expert users to achieve results that are\ncomparable to those achieved by experts. Hence,\nour ﬁrst conclusion is that our framework clearly\nserves its purpose. Still, potential improvements\ncan be derived from the results achieved during the\nexperiments. In particular, we will consider the\nused of attention-based encoder-decoder networks\nto improve the ﬂuency of complex sentences.\n6\nConclusion and Future Work\nIn this paper, we presented LD2NL, a framework\nfor verbalizing SW languages, especially on RDF\nand OWL while including the SPARQL verbal-\nization provided by SPARQL2NL. Our evaluation\nwith 86 persons revealed that our framework gener-\nates NL that can be understood by lay users. While\nthe OWL verbalization was close to NL, the RDF\nwas less natural but still sufﬁcient to convey the\nmeaning expressed by the corresponding set of\ntriples. In future work, we aim to extend LD2NL\nto verbalize the languages SWRL (Horrocks et al.,\n2004) and SHACL (Knublauch and Kontokostas,\n2017).\nAcknowledgments\nThis work was supported by the German Fed-\neral Ministry of Transport and Digital Infrastruc-\nture (BMVI) through the projects LIMBO (no.\n19F2029I) and OPAL (no. 19F2028A). This work\nwas supported by the German Federal Ministry\nof Economics and Technology (BMWI) in the\nprojects RAKI (no. 01MD19012D) as well as by\nthe BMBF project SOLIDE (no. 13N14456).\nReferences\nIon Androutsopoulos, Gerasimos Lampouras, and Dim-\nitrios Galanis. 2013.\nGenerating natural language\ndescriptions from OWL ontologies: The natural owl\nsystem. J. Artif. Int. Res. 48(1):671–715.\nOr Biran and Kathleen McKeown. 2015.\nDiscourse\nplanning with an n-gram model of relations.\nIn\nEMNLP. pages 1973–1977.\nLorenz B¨uhmann, Jens Lehmann, and Patrick West-\nphal. 2016.\nDl-learnera framework for inductive\nlearning on the semantic web. Journal of Web Se-\nmantics 39:15–24.\nPhilipp Cimiano, Janna L¨uker, David Nagel, and\nChristina Unger. 2013.\nExploiting ontology lex-\nica for generating natural language texts from rdf\ndata.\nIn Proceedings of the 14th European Work-\nshop on Natural Language Generation. ACL, Soﬁa,\nBulgaria, pages 10–19.\nEmilie Colin, Claire Gardent, Yassine Mrabet, Shashi\nNarayan, and Laura Perez-Beltrachini. 2016. The\nwebnlg challenge: Generating text from dbpedia\ndata.\nIn Proceedings of the 9th INLG conference.\npages 163–167.\nH. Dalianis and E.H. Hovy. 1996.\nAggregation in\nnatural language generation.\nIn G. Adorni and\nM. Zock, editors, Trends in natural language genera-\ntion: an artiﬁcial intelligence perspective, Springer,\nvolume 1036 of Lecture Notes in Artiﬁcial Intelli-\ngence, pages 88–105.\nDaniel Duma and Ewan Klein. 2013. Generating nat-\nural language from linked data: Unsupervised tem-\nplate extraction. In IWCS. pages 83–94.\nBasil Ell and Andreas Harth. 2014.\nA language-\nindependent method for the extraction of rdf verbal-\nization templates. In INLG. pages 26–34.\nBasil Ell, Denny Vr, and Elena Simperl. 2012. SPAR-\nTIQULATION Verbalizing SPARQL queries. In In\nProceedings of ILD Workshop, ESWC.\nBasil Ell, Denny Vrandecic, and Elena Paslaru Bon-\ntas Simperl. 2011. Labels in the web of data. In\nProceedings of ISWC. Springer, volume 7031, pages\n162–176.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. Creating train-\ning corpora for nlg micro-planning. In Proceedings\nof ACL.\nAlbert Gatt and Emiel Krahmer. 2017. Survey of the\nstate of the art in natural language generation: Core\ntasks, applications and evaluation.\narXiv preprint\narXiv:1703.09902 .\nMatthew Horridge, Nick Drummond, John Goodwin,\nAlan L Rector, Robert Stevens, and Hai Wang. 2006.\nThe Manchester OWL syntax. In OWLed. volume\n216.\nIan Horrocks, Peter F. Patel-Schneider, Harold Boley,\nSaid Tabet, Benjamin Grosofand, and Mike Dean.\n2004. SWRL: A semantic web rule language com-\nbining OWL and RuleML. W3C Member Submis-\nsion.\nHolger Knublauch and Dimitris Kontokostas. 2017.\nShapes constraint language (shacl). W3C Candidate\nRecommendation 11(8).\nJens Lehmann, Tim Furche, Giovanni Grasso, Axel-\nCyrille Ngonga Ngomo, Christian Schallhart, An-\ndrew Jon Sellers, Christina Unger, Lorenz B¨uhmann,\nDaniel Gerber, Konrad H¨offner, David Liu, and\nS¨oren Auer. 2012. DEQA: Deep Web Extraction for\nQuestion Answering. In The Semantic Web - ISWC\n2012 - 11th International Semantic Web Conference,\nBoston, MA, USA, November 11-15, 2012, Proceed-\nings, Part II. pages 131–147.\nYassine Mrabet, Pavlos Vougiouklis, Halil Kilicoglu,\nClaire Gardent, Dina Demner-Fushman, Jonathon\nHare, and Elena Simperl. 2016. Aligning texts and\nknowledge bases with semantic sentence simpliﬁca-\ntion. WebNLG 2016 .\nAxel-Cyrille\nNgonga\nNgomo,\nLorenz\nB¨uhmann,\nChristina Unger, Jens Lehmann, and Daniel Gerber.\n2013. Sorry, I Don’t Speak SPARQL: Translating\nSPARQL Queries into Natural Language.\nIn Pro-\nceedings of the 22Nd International Conference on\nWorld Wide Web. ACM, New York, NY, USA, pages\n977–988.\nAxel-Cyrille\nNgonga\nNgomo,\nLorenz\nB¨uhmann,\nChristina Unger, Jens Lehmann, and Daniel Gerber.\n2013. Sorry, i don’t speak sparql: translating sparql\nqueries into natural language. In Proceedings of the\n22nd international conference on World Wide Web.\nACM, pages 977–988.\nAxel-Cyrille\nNgonga\nNgomo,\nLorenz\nB¨uhmann,\nChristina Unger, Jens Lehmann, and Daniel Gerber.\n2013. SPARQL2NL: Verbalizing SPARQL queries.\nIn 22nd International World Wide Web Conference,\nRio de Janeiro, Brazil, May 13-17. pages 329–332.\nW3C OWL Working Group. 2009. OWL 2 Web Ontol-\nogy Language: Document Overview. W3C Recom-\nmendation.\nLaura Perez-Beltrachini, Rania Sayed, and Claire Gar-\ndent. 2016. Building rdf content for data-to-text gen-\neration. In COLING. pages 1493–1502.\nRichard Power and Allan Third. 2010.\nExpressing\nOWL Axioms by English Sentences: Dubious in\nTheory, Feasible in Practice. In Proceedings of the\n23rd International Conference on Computational\nLinguistics: Posters. ACL, Stroudsburg, PA, USA,\npages 1006–1013.\nW3C RDF Working Group. 2014. RDF 1.1 Concepts\nand Abstract Syntax. W3C Recommendation.\nEhud Reiter and Robert Dale. 2000a. Building natural\nlanguage generation systems. Cambridge university\npress.\nEhud Reiter and Robert Dale. 2000b. Building natu-\nral language generation systems. Cambridge Uni-\nversity Press, New York, NY, USA.\nAmin Sleimi and Claire Gardent. 2016.\nGenerat-\ning paraphrases from dbpedia using deep learning.\nWebNLG 2016 page 54.\n",
  "categories": [
    "cs.CL",
    "cs.DB"
  ],
  "published": "2019-11-04",
  "updated": "2019-11-04"
}