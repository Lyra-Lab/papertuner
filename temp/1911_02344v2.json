{
  "id": "http://arxiv.org/abs/1911.02344v2",
  "title": "Statistical physics of unsupervised learning with prior knowledge in neural networks",
  "authors": [
    "Tianqi Hou",
    "Haiping Huang"
  ],
  "abstract": "Integrating sensory inputs with prior beliefs from past experiences in\nunsupervised learning is a common and fundamental characteristic of brain or\nartificial neural computation. However, a quantitative role of prior knowledge\nin unsupervised learning remains unclear, prohibiting a scientific\nunderstanding of unsupervised learning. Here, we propose a statistical physics\nmodel of unsupervised learning with prior knowledge, revealing that the sensory\ninputs drive a series of continuous phase transitions related to spontaneous\nintrinsic-symmetry breaking. The intrinsic symmetry includes both reverse\nsymmetry and permutation symmetry, commonly observed in most artificial neural\nnetworks. Compared to the prior-free scenario, the prior reduces more strongly\nthe minimal data size triggering the reverse symmetry breaking transition, and\nmoreover, the prior merges, rather than separates, permutation symmetry\nbreaking phases. We claim that the prior can be learned from data samples,\nwhich in physics corresponds to a two-parameter Nishimori constraint. This work\nthus reveals mechanisms about the influence of the prior on unsupervised\nlearning.",
  "text": "Statistical physics of unsupervised learning with prior knowledge in neural networks\nTianqi Hou\nDepartment of Physics, the Hong Kong University of Science and Technology,\nClear Water Bay, Hong Kong, People’s Republic of China\nHaiping Huang∗\nPMI Lab, School of Physics, Sun Yat-sen University, Guangzhou 510275, People’s Republic of China\n(Dated: May 28, 2020)\nIntegrating sensory inputs with prior beliefs from past experiences in unsupervised learning is\na common and fundamental characteristic of brain or artiﬁcial neural computation. However, a\nquantitative role of prior knowledge in unsupervised learning remains unclear, prohibiting a scientiﬁc\nunderstanding of unsupervised learning. Here, we propose a statistical physics model of unsupervised\nlearning with prior knowledge, revealing that the sensory inputs drive a series of continuous phase\ntransitions related to spontaneous intrinsic-symmetry breaking. The intrinsic symmetry includes\nboth reverse symmetry and permutation symmetry, commonly observed in most artiﬁcial neural\nnetworks. Compared to the prior-free scenario, the prior reduces more strongly the minimal data\nsize triggering the reverse symmetry breaking transition, and moreover, the prior merges, rather\nthan separates, permutation symmetry breaking phases. We claim that the prior can be learned\nfrom data samples, which in physics corresponds to a two-parameter Nishimori constraint. This\nwork thus reveals mechanisms about the inﬂuence of the prior on unsupervised learning.\nThe sensory cortex in the brain extracts statistical reg-\nularities in the environment in an unsupervised way. This\nkind of learning is called unsupervised learning [1], rely-\ning only on raw sensory inputs, thereby thought of as a\nfundamental function of the sensory cortex [2].\nWhen\nsensory information is uncertain, a natural way to model\nthe outside world is integrating sensory inputs with inter-\nnal prior beliefs, in accordance with Bayesian inference.\nBayesian theory formalizes how the likelihood function of\nsensory inputs and the prior statistics can be coherently\ncombined, taking a trade-oﬀbetween input feature relia-\nbility, as encoded in the likelihood function, and the prior\ndistribution [3]. The Bayesian brain hypothesis [4] was\nwidely used to model sensorimotor behavior [5, 6], per-\nceptual decision making [7], object perception in visual\ncortex [8, 9], and even cognition [10]. Bayesian inference\nis also a principled computational framework in deep-\nlearning-based machine learning [11–13]. When the prior\nbeliefs are taken into account, the unsupervised learning\nmeets Bayesian inference. Thus incorporating prior be-\nliefs from past experiences into unsupervised learning is\na common and fundamental characteristic of the com-\nputation in the brain or artiﬁcial neural networks. How-\never, current studies mostly focused on neural implemen-\ntations of the Bayesian inference [4, 14], or focused on\ndesigning scalable Bayesian learning algorithms for deep\nnetworks [12, 13], making a scientiﬁc understanding of\nunsupervised learning with prior knowledge lag far be-\nhind its neural implementations or engineering applica-\ntions.\nBy asking a minimal data size to trigger learning in a\ntwo-layer neural network, namely a restricted Boltzmann\nmachine (RBM) [15], a recent study claimed that sensory\ninputs (or data streams) are able to drive a series of phase\ntransitions related to broken inherent-symmetries of the\nmodel [16]. However, this model does not assume any\nprior knowledge during learning, therefore the impact of\npriors on the learning remains unexplained.\nWhether\na learning is data-driven or prior-driven depends highly\non feature reliability of sensory inputs.\nWhen sensory\ninputs become highly unreliable, prior knowledge domi-\nnates the learning. Otherwise, the data likelihood takes\nover. A quantitative role of prior knowledge is thus a\nkey to unlock the underpinning of unsupervised learning\nwith priors.\nHere, we propose a mean-ﬁeld model of unsuper-\nvised learning with prior knowledge, to provide an an-\nalytical argument supporting surprising computational\nroles of priors.\nFirst, the prior knowledge reduces\nstrongly the minimal data (observations) size at which\nthe concept-formation starts, as quantitatively predicted\nby our model. Second, phase transitions observed in the\nRBM model of unsupervised learning without priors are\nsigniﬁcantly reshaped, showing that the prior shifts an\nintermediate phase observed in the model without pri-\nors.\nLastly, our theory reveals that the variability in\ndata samples, as encoded by a temperature-like hyper-\nparameter, as well as the intrinsic correlation between\nsynapses connecting layers of the RBM, can be learned\ndirectly from the data. In physics, this corresponds to\na two-parameter Nishimori constraint, generalizing the\noriginal concept of a single-parameter Nishimori line [17].\nTherefore, our model provides deep insights about roles\nof prior knowledge in unsupervised learning.\nFrom a neural network perspective, the interplay be-\ntween the prior and the likelihood function of data\ncan be captured by synaptic weights.\nThese synaptic\nweights are modeled by feedforward connections in a\nRBM [15, 16, 18]. More precisely, the RBM is a two-\nlayer neural network where there do not exist intra-layer\narXiv:1911.02344v2  [cond-mat.dis-nn]  27 May 2020\n2\nconnections.\nThe ﬁrst layer is called the visible layer,\nreceiving sensory inputs (e.g., images), while the second\nlayer is called the hidden layer, where each neuron’s in-\nput weights are called the receptive ﬁeld (RF) of that\nhidden neuron. In an unsupervised learning task, these\nsynaptic weights are adjusted to encode latent features in\nthe data. We assume that both the neural and synaptic\nstates take a binary value (±1). The RBM is a universal\napproximator of any discrete distributions [19]. For sim-\nplicity, we consider only two hidden neurons. Therefore,\nthe joint activity distribution reads as follows [20, 21],\nP(σ, h1, h2|ξ1, ξ2) =\n1\nZ(ξ1, ξ2)e\nβ\n√\nN\nP\nj hj\nP\ni∈∂j ξj\ni σi, (1)\nwhere ∂j denotes neighbors of the node j, σ indicates\nan N-dimensional sensory input, h1 and h2 are the hid-\nden neural activity, an inverse-temperature β character-\nizes the noise level of the input, the network-size scaling\nfactor ensures that the free energy of the model is an ex-\ntensive quantity, ﬁnally ξ1 and ξ2 are the receptive ﬁelds\nof the two hidden neurons, respectively. Z(ξ1, ξ2) is the\npartition function in physics. Another salient feature in\nthis model is that the hidden activity can be marginalized\nout, which helps the following analysis.\nFor an unsupervised learning task, one can only have\naccess to raw data, deﬁned by D = {σa}M\na=1. We assume\na weak dependence among data samples [20]. The task is\nto infer the synaptic weights encoding latent features in\nthe data. This is naturally expressed as computing the\nposterior probability, and thus the Bayes’ rule applies as\nfollows,\nP(ξ1, ξ2|D) =\nQ\na P(σa|ξ1, ξ2) QN\ni=1 P0(ξ1\ni , ξ2\ni )\nP\nξ1,ξ2\nQ\na P(σa|ξ1, ξ2) QN\ni=1 P0(ξ1\ni , ξ2\ni )\n= 1\nΩ\nY\na\n1\ncosh (β2Q) cosh\n\u0012 β\n√\nN\nξ1 · σa\n\u0013\n× cosh\n\u0012 β\n√\nN\nξ2 · σa\n\u0013 N\nY\ni=1\nP0(ξ1\ni , ξ2\ni ),\n(2)\nwhere Q =\n1\nN\nP\ni ξ1\ni ξ2\ni\nstemming from Z(ξ1, ξ2) ≃\n2Neβ2 cosh(β2Q) [16], Ωis the partition function for\nthe learning process, and P0(ξ1\ni , ξ2\ni ) = 1+q\n4 δ(ξ1\ni −ξ2\ni ) +\n1−q\n4 δ(ξ1\ni + ξ2\ni ) specifying the prior knowledge we have\nabout the distribution of synaptic weights. Note that q\ndetermines the correlation level of the two RFs. The prior\nP0(ξ1\ni , ξ2\ni ) can be recast into another form of P0(ξ1\ni , ξ2\ni ) =\neJ0ξ1\ni ξ2\ni\n4 cosh(J0) where J0 = tanh−1 q. The unsupervised learn-\ning can thus be investigated within a teacher-student\nscenario. First, we prepare a teacher-type RBM whose\nsynaptic weights are generated from the prior distribu-\ntion with a prescribed q. Then the data D is collected\nfrom the equilibrium state of the model (Eq. (1)) through\nGibbs sampling [20]. Finally, a student-type RBM tries\nto infer the teacher’s RFs only based on the noisy data\nthe teacher generates with another prescribed β. There-\nfore, the unsupervised learning with prior knowledge (in-\ncluding both q and β) can be studied within the Bayes-\noptimal framework (Eq. (2)) [22]. Note that there ex-\nist two types of inherent symmetries in the model, i.e.,\nthe model (Eq. (2)) is invariant under the operation of\nξ →−ξ for two hidden nodes (reverse symmetry) or the\npermutation of ξ1 and ξ2 (permutation symmetry (PS)).\nThe teacher-student scenario is amenable for a theoret-\nical analysis, since the data distribution can be analyti-\ncally calculated. This is diﬀerent from numerical exper-\niments on a real dataset whose exact distribution is not\naccessible. Moreover, we are interested in the limits—\nM →∞and N →∞but the ratio or data density is\nkept constant as α = M\nN .\nThe emergent behavior of the model is captured by the\nfree energy function deﬁned by −βNf = ⟨ln Ω⟩, where\n⟨•⟩denotes the disorder average over both the distribu-\ntion of planted true RFs and the corresponding data dis-\ntribution P(D|ξ1, ξ2). However, a direct computation of\nthe disorder average is impossible. Fortunately, by intro-\nducing n replicas of the original system, we can estimate\nthe free energy density by applying a mathematical iden-\ntity −βf = limn→0,N→∞\nln⟨Ωn⟩\nnN\nwhere we only need to\nevaluate an integer power of Ω. Technical calculations are\ndeferred to Supplemental Material [23]. Here we quote\nonly the ﬁnal result, and give an intuitive explanation.\nFor simplicity, we assume that order parameters of the\nmodel (deﬁned below) are invariant under permutation\nof replica indexes. This is called the replica symmetric\n(RS) Ansatz in spin glass theory [24].\nThe order parameters (T1, T2, q1, q2, τ1, τ2, R, r) and\ntheir associated conjugated counterparts are stationary\npoints of the free energy function, obtained through a\nsaddle-point analysis in the thermodynamic limit [23].\nThe order parameters are calculated as follows,\nT1 = [ξ1,true⟨ξ1⟩], T2 = [ξ2,true⟨ξ2⟩],\n(3a)\nq1 = [⟨ξ1⟩2], q2 = [⟨ξ2⟩2],\n(3b)\nτ1 = [ξ1,true⟨ξ2⟩], τ2 = [ξ2,true⟨ξ1⟩],\n(3c)\nR = [⟨ξ1ξ2⟩], r = [⟨ξ1⟩⟨ξ2⟩],\n(3d)\nwhere\n⟨•⟩\ndenotes\nan\naverage\nunder\nthe\nBoltz-\nmann\nmeasure\nof\nan\neﬀective\ntwo-spin\ninteraction\nHamiltonian,\narising\nfrom\nthe\nentropic\ncomputa-\ntion of the replica method [23], i.e., Peﬀ(ξ1, ξ2)\n∝\n1\n4 cosh J0 eb1ξ1+b2ξ2+(b3+J0)ξ1ξ2\nwhere\nb1\n(or\nb2)\nand\nb3\nare\nrandom\neﬀective\nﬁelds\nand\ncouplings,\nre-\nspectively,\nrelated\nto\nconjugated\norder\nparameters\n( ˆT1, ˆT2, ˆq1, ˆq2, ˆτ1, ˆτ2, ˆR, ˆr), and [•] indicates an average\nover the standard Gaussian random variables and the\ntrue prior P0(ξ1,true, ξ2,true).\nThese order parameters\ncapture the emergent behavior of our model. T1 and T2\n3\ncharacterize the overlap between prediction and ground\ntruth. q1 and q2 characterize the self-overlap (Edwards-\nAnderson order parameter in physics [24]).\nτ1 and τ2\ncharacterize the permutation-type overlap. R and r char-\nacterize the student’s guess on the correlation level of the\nplanted RFs.\nInterestingly, when α is small, trivial (null values) or-\nder parameters except R are a stable solution of Eq. (3),\nthereby specifying a random guess (RG) phase. As ex-\npected, R reﬂects the prior information, thus being equal\nto q irrespective of α. In this phase,\n\nξ1\u000b\n=\n\nξ2\u000b\n= 0, the\nweight thus takes ±1 with equal probabilities, implying\nthat the data does not provide any useful information to\nbias the weight’s direction. The underlying rationale is\nthat the posterior (Eq. (2)) is invariant under the reverse\noperation ξ →−ξ, and this symmetry is unbroken in the\nRG phase.\nSurprisingly, as more data is supplied, the RG phase\nwould become unstable at a critical data density. By a\nlinear stability analysis [23], this threshold can be ana-\nlytically derived as\nαc =\nΛ(β, q)\n(1 + |q|)(1 + | tanh(β2q)|),\n(4)\nwhere Λ(β, q) =\nβ−4\n1+q tanh(β2q)+|q+tanh(β2q)| denoting the\nlearning threshold for the prior-free scenario [16]. In the\ncorrelation-free case (q = 0), the known threshold αc =\nβ−4 is recovered [18, 25].\nCompared to the prior-free\nscenario, the prior knowledge contributes an additional\nfactor leading to a further reduction of the threshold (∼\n60% of the prior-free one for q = 0.3 and β = 1). Most\ninterestingly, in the weak-correlation limit, where q ∼\nβ−2 with a proportional constant q0 in the presence of\nless noisy data (large β), αcβ4 =\n1\n(1+| tanh q0|)2 , which\nimplies that the learning threshold can be lowered down\nto only 32% of the correlation-free case for q0 = 1. This\ndemonstrates that the weak correlation between synapses\nplays a key role of reducing the necessary data size to\ntrigger concept-formation, a surprising prediction of the\nmodel.\nWhen α > αc, the RG phase is replaced by the\nsymmetry-broken phase, where\n\nξ1\u000b\n=\n\nξ2\u000b\n̸= 0 and\nthe intrinsic reverse symmetry is spontaneously broken.\nWe thus call the second phase a spontaneous symme-\ntry breaking (SSB) phase. The SSB leads to a non-zero\nsolution of q1 = q2 = T1 = T2 = τ1 = τ2 = r. A rea-\nsonable interpretation is that, the student infers only the\ncommon part of the two planted RFs. Thus the PS still\nholds for the student’s hidden neurons. Moreover, ξ1,true\nand ξ2,true have the PS property as well, explaining the\nsolution we obtained. The SSB phase is thus permuta-\ntion symmetric, which is stable until a turnover of the\norder parameter r is reached (Fig. 1 (a)).\nAt the turnover, the PS is spontaneously broken,\nthereby leading to a permutation symmetry breaking\nOrder parameters\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n \nα\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n r, MP\nq1=q2, MP\nq1=q2, with prior\nq1=q2, prior-free\n r, with prior\n r, prior-free\nRG\nSSB\nPSB\nq=0.3\n(a)\n \nCritical data density\n0\n2\n4\n6\n8\n10\n \nq\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nαc\nPSB, β=0.8\nαc\nSSB, β=0.8\nαc\nPSB, β=1.0\nαc\nSSB, β=1.0\nαc\nPSB, β=1.2\nαc\nSSB, β=1.2\n(b)\nFIG. 1:\n(Color online) Phase diagram of unsupervised learn-\ning with priors. (a) Order parameters versus data densities\nwith (β, q)=(1.0, 0.3). Lines are replica results compared with\nsymbols obtained from the message passing (MP) procedure\n(instances of N = 200). Previous results of the prior-free un-\nsupervised learning [16] are also plotted for comparison. The\narrows indicate the role of priors in shifting the phase tran-\nsition points.\n(b) Critical data densities for SSB and PSB\nare obtained from replica analysis and plotted for increasing\nvalues of β.\n(PSB) phase. The third phase is characterized by two\nﬁxed points: (1) q1 = q2 = T1 = T2, and τ1 = τ2 = r;\n(2) q1 = q2 = τ1 = τ2, and T1 = T2 = r.\nThese\ntwo ﬁxed points share the same free energy, representing\ntwo possible choices of ground truth— (ξ1,true, ξ2,true)\nor (ξ2,true, ξ1,true).\nIn fact, the PSB phase has two\nsubtypes—a PSBs phase where the permutation sym-\nmetry between ξ1 and ξ2 is broken on the student’s\nside, i.e.,\n\nξ1\u000b\ncan point conversely to\n\nξ2\u000b\nyet with\nthe same magnitude, thereby q1 = q2 ̸= r, and a PSBt\nphase where the PSB occurs on the teacher’s side, i.e.,\nξ1,true and ξ2,true can not be freely permuted, thereby\nT1,2 ̸= τ2,1 [16]. Interestingly, the self-overlap deviates\nfrom r at the turnover, thereby merging PSBs phase and\nPSBt phase into a single PSB phase, rather than sep-\narating these two subtypes as in the prior-free scenario\n(Fig. 1 (a)). With the help of prior knowledge, the stu-\n4\ndent is able to distinguish two planted RFs (PSBt) at the\nsame time when starting to infer diﬀerent components of\nthe true RFs (PSBs). Furthermore, the prior does not\nchange the PSBt transition point of the prior-free case, as\nknowing q does not help to accelerate the recognition of\ntwo choices of ground truth. However, the knowledge of q\ndoes elevate the overlap values before the turnover, lead-\ning to a larger value of r in the post-turnover regime com-\npared to the prior-free case (see a proof in [23]). After the\nturnover, the overlap equal to min(T1, τ1) or min(T2, τ2)\nhas the same value with r, since (ξ1,true, ξ2,true) follows\nthe same posterior as (ξ1, ξ2).\nAs expected, r ﬁnally\ntends to q at a ﬁnite but large value of α (Fig. 1 (a)).\nWe conclude that with the prior knowledge, the data\nstream drives the SSB and PSB phase transitions of con-\ntinuous type.\nThresholds of the transitions are sum-\nmarized in Fig. 1 (b).\nThis conclusion is veriﬁed by\nnumerical simulations on single instances of the model\nby applying a message-passing-based learning algorithm\n(Fig. 1 (a)).\nBrieﬂy, a cavity probability of (ξ1\ni , ξ2\ni )\nwithout considering the a-th data sample Pi→a(ξ1\ni , ξ2\ni )\nis deﬁned.\nThen the cavity magnetization m1,2\ni→a =\nP\nξ1\ni ,ξ2\ni ξ1,2\ni\nPi→a(ξ1\ni , ξ2\ni ) and the cavity correlation qi→a =\nP\nξ1\ni ,ξ2\ni ξ1\ni ξ2\ni Pi→a(ξ1\ni , ξ2\ni ) can be written into a closed-form\niterative equation, using the approximation that the cav-\nity probabilities surrounding a data sample are factor-\nized [23]. For evaluating the order parameters, the full\n(not cavity) magnetization m1,2\ni\nand correlation qi can\nbe computed by considering contributions from all data\nsamples.\nIn our model, the learning thresholds related to the\nSSB and PSB phase transitions are only determined by\ntwo parameters β and q.\nWe then ask whether these\ntwo parameters can be learned from the raw data (not\nnecessary to explore all data samples). In principle, this\ncan be achieved by maximizing the following posterior of\nthe hyper-parameters,\nP(β, q|D) =\nX\nξ1,ξ2\nP(β, q, ξ1, ξ2|D) ∝e−αNβ2Ω,\n(5)\nwhere the Bayes’ rule is applied [23]. The maximum of\nthis posterior has the following property,\nβ = −ϵ(β)\n2α ,\n(6a)\nq = 1\nN\nN\nX\ni=1\nqi(β, q),\n(6b)\nwhere the energy density ϵ(β) independent of q and the\nfull correlation {qi} can be estimated from the mes-\nsage passing algorithm [23]. Eq. (6) constructs a two-\nparameter Nishimori constraint [23], implying that the\nenergy density of the model with prior knowledge is an-\nalytic (ϵ = −2αβ).\nGiven only the data samples, al-\nthough we do not know the hyper-parameters (β,q) un-\nderlying the data, iteratively imposing the Nishimori\n0\n2\n4\n6\n8\n10\n12\n14\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0\n100\n200\n300\n400\n500\n β, full\n q, full\n β, part\n q, part\nM=500\n \nRelative Error\n0\n0.5\n1\n1.5\n2\nα\n0\n2\n4\n6\n8\n10\nq\n \nβ or q\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.1\n \nlearning step\n0\n100\n200\n300\n400\n500\n β, M=600\n q, M=600\n β, M=800\n q, M=800\nβ\nβtrue=1.0\nqtrue=0.3\nFIG. 2:\n(Color online) Inference of the hyper-parameters\n(β, q) from raw data. The performance is measured by the\nrelative error deﬁned as |Lest/Ltrue −1|, where L denotes the\nnegative log-likelihood (Eq. (5)) [23]. The error bar charac-\nterizes the ﬂuctuation among twenty independent runs. The\ninset shows learning trajectories on random instances of the\nmodel (N = 100). The sub-inset shows results on the MNIST\ndataset including full dataset (containing all ten types of dig-\nits) or part of the dataset (containing only four types of dig-\nits).\nconstraint helps to reach a consistent value of (β, q) to\nexplain the data.\nIn statistics, this iterative scheme\nis called the expectation-maximization algorithm [26],\nwhere the message update to compute (ϵ, {qi}) is called\nan expectation-step, while the hyper-parameter update is\ncalled a maximization-step. The hyper-parameter space,\nespecially when the amount of data samples is not suf-\nﬁcient, is not guaranteed to be convex, instead being\nhighly non-convex in general, as veriﬁed by a high rel-\native inference error with a large ﬂuctuation in a data-\ndeﬁcient regime (Fig. 2).\nWe ﬁrst apply this frame-\nwork to synthetic datasets, where the hyper-parameters\ncan be accurately predicted provided that the supplied\ndataset is large enough (Fig. 2). Using the current neu-\nral network model with N input neurons and two hidden\nneurons, we then apply this algorithm to a handwritten\ndigit dataset [27], with N = 784. We obtain a value of\n(β, q) = (12.697, 0.524) (Fig. 2). This predicted β value is\nmuch smaller than that reported in a one-bit RBM [18].\nSummary.—Learning statistical regularities in sensory\ninputs is naturally characterized by integrating sensory\ninformation with prior beliefs on the latent features. Our\ntheory clariﬁes the role of prior knowledge, when unsu-\npervised learning meets Bayesian inference. Incorporat-\ning the prior merges PSBs and PSBt phases, while the\nPSBt phase lags behind the PSBs phase in the prior-free\nmodel. Thus the prior is able to reshape the concept-\nformation process in unsupervised learning. This is one\n5\nkey prediction of our theory.\nAs expected, the prior\nknowledge reduces much more signiﬁcantly the mini-\nmal data size triggering SSB than the prior-free case.\nMoreover, a weak correlation between synapses reduces\nstrongly the minimal data size as well, in consistence with\nthe well-known non-redundant weight hypothesis [28].\nThis contributes to the second prediction of the model.\nThe PSB phase was also recently observed in a RBM with\nmore than two hidden neurons [29]. The predictions may\nbe generalized to a hierarchical complex system, and even\ntestable in a visual hierarchy where top-down contextual\npriors are combined with bottom-up observations when\nimplementing probabilistic inference [3, 9].\nTherefore, using the concept of SSB and PSB in\nphysics, our theoretical study provides deep insights\nabout roles of prior knowledge in unsupervised Bayesian\nlearning, which is linked to the Bayesian brain hy-\npothesis (e.g., when sensory inputs become highly un-\nreliable, prior knowledge dominates the learning) and\nthe Bayesian inference in neural networks (both prior-\ninduced SSB and PSB are related to the weight symmetry\ncommonly observed in most artiﬁcial neural networks).\nWe would like to thank three referees for their insight-\nful comments.\nWe also thank Xiao-Hui Deng and K.\nY. Michael Wong for useful discussions. This research\nwas supported by the start-up budget 74130-18831109\nof the 100-talent- program of Sun Yat-sen University\n(H.H.), and the NSFC (Grant No.\n11805284) (H.H.),\nand grants from the Research Grants Council of Hong\nKong (16322616 and 16396817) (T.H.).\nSupplemental Material\nMessage Passing Algorithms for unsupervised learning with prior information\nIn our current setting, we assume that the statistical inference of synaptic weights from the raw data has the\ncorrect prior information P0(ξ1, ξ2) = QN\ni=1 P0(ξ1\ni , ξ2\ni ), where P0(ξ1\ni , ξ2\ni ) = 1+q\n4 δ(ξ1\ni −ξ2\ni ) + 1−q\n4 δ(ξ1\ni + ξ2\ni ), where q\nis the correlation level between the two receptive ﬁelds. According to the Bayes’ rule, the posterior probability of\nsynaptic weights is given by\nP(ξ1, ξ2|{σa}M\na=1) =\nQ\na P(σa|ξ1, ξ2) QN\ni=1 P0(ξ1\ni , ξ2\ni )\nP\nξ1,ξ2\nQ\na P(σa|ξ1, ξ2) QN\ni=1 P0(ξ1\ni , ξ2\ni )\n= 1\nΩ\nY\na\n1\ncosh (β2Q) cosh\n\u0012 β\n√\nN\nξ1 · σa\n\u0013\ncosh\n\u0012 β\n√\nN\nξ2 · σa\n\u0013 N\nY\ni=1\nP0(ξ1\ni , ξ2\ni ),\n(S1)\nwhere Q ≡1\nN\nPN\ni=1 ξ1\ni ξ2\ni , representing the overlap of the two RFs, and Ωis the so-called partition function in statistical\nphysics.\nUsing the Bethe approximation [16, 18, 30], we can easily write down the belief propagation equations as follows,\nPi→a(ξ1\ni , ξ2\ni ) =\n1\nZi→a\nP0(ξ1\ni , ξ2\ni )\nY\nb∈∂i\\a\nµb→i(ξ1\ni , ξ2\ni ),\n(S2a)\nµb→i(ξ1\ni , ξ2\ni ) =\nX\n{ξ1,ξ2}\\{ξ1\ni ,ξ2\ni }\n1\ncosh\n\u0010\nβ2Qc + β2\nN ξ1\ni ξ2\ni\n\u0011 cosh\n\u0012\nβXb +\nβ\n√\nN\nξ1\ni σb\ni\n\u0013\ncosh\n\u0012\nβYb +\nβ\n√\nN\nξ2\ni σb\ni\n\u0013\n×\nY\nj∈∂b\\i\nPj→b(ξ1\nj , ξ2\nj ),\n(S2b)\nwhere we deﬁne auxiliary variables Xb =\n1\n√\nN\nP\nj̸=i ξ1\nj σb\nj, Yb =\n1\n√\nN\nP\nj̸=i ξ2\nj σb\nj, and Qc =\n1\nN\nP\nj̸=i ξ1\nj ξ2\nj . j ∈∂b\\i\nindicates neighbors of the data node b excluding the synaptic weight i. In our model, all synaptic weights are used\nto explain each data sample. The belief propagation is commonly deﬁned in a factor graph representation, where\nthe synaptic-weight-pair acts as the variable node, while the data sample acts as the factor node (or constraint\nto be satisﬁed) [31].\nThe learning can then be interpreted as the process of synaptic weight inference based on\nthe data constraints. The cavity probability Pi→a(ξ1\ni , ξ2\ni ) is deﬁned as the probability of the pair (ξ1\ni , ξ2\ni ) without\nconsidering the contribution of the data node a. Zi→a is thus a normalization constant for the cavity probability\nPi→a(ξ1\ni , ξ2\ni ). The cavity probability can then be parameterized by the cavity magnetization m1,2\ni→a and correlation\nqi→a as Pi→a(ξ1\ni , ξ2\ni ) = 1+m1\ni→aξ1\ni +m2\ni→aξ2\ni +qi→aξ1\ni ξ2\ni\n4\n. µb→i(ξ1\ni , ξ2\ni ) represents the contribution of one data node given the\nvalue of (ξ1\ni , ξ2\ni ). Due to the central limit theorem, Xb and Yb can be considered as two correlated Gaussian random\n6\nvariables. We thus deﬁne G1\nb→i =\n1\n√\nN\nP\nj̸=i σb\njm1\nj→b, G2\nb→i =\n1\n√\nN\nP\nj̸=i σb\njm1\nj→b, Γ1\nb→i =\n1\nN\nP\nj̸=i(1 −(m1\nj→b)2), and\nΓ2\nb→i =\n1\nN\nP\nj̸=i(1 −(m2\nj→b)2) as the means and variances of the two variables, respectively. The covariance is given\nby Ξb→i = 1\nN\nP\nj̸=i(qj→b −m1\nj→bm2\nj→b). Moreover, Qc is approximated by its cavity mean Qb→i = 1\nN\nP\nj̸=i qj→b. As\na result, the intractable summation in Eq. (S2b) can be replaced by a jointly-correlated Gaussian integral,\nµb→i(ξ1\ni , ξ2\ni ) =\n1\ncosh\n\u0010\nβ2Qb→i + β2\nN ξ1\ni ξ2\ni\n\u0011\nZZ\nDxDy cosh\n\u0012\nβ\nq\nΓ1\nb→ix + βG1\nb→i +\nβ\n√\nN\nξ1\ni σb\ni\n\u0013\n× cosh\n\u0012\nβ\nq\nΓ2\nb→i(ψx +\np\n1 −ψ2y) + βG2\nb→i +\nβ\n√\nN\nξ2\ni σb\ni\n\u0013\n,\n(S3)\nwhere the standard Gaussian measure Dx = e−x2/2dx\n√\n2π\n, and ψ =\nΞb→i\n√\nΓ1\nb→iΓ2\nb→i .\nWe further deﬁne the cavity bias ub→i(ξ1\ni , ξ2\ni ) = ln µb→i(ξ1\ni , ξ2\ni ) as\nub→i(ξ1\ni , ξ2\ni ) = β2\n2 (Γ1\nb→i + Γ2\nb→i + 2Ξb→i) −ln\n\u0012\n2 cosh\n\u0010\nβ2Qb→i + β2ξ1\ni ξ2\ni\nN\n\u0011\u0013\n+ ln cosh\n\u0012\nβG1\nb→i + βG2\nb→i +\nβ\n√\nN\nσb\ni (ξ1\ni + ξ2\ni )\n\u0013\n+ ln\n\n1 + e−2β2Ξb→i cosh\n\u0010\nβG1\nb→i −βG2\nb→i +\nβ\n√\nN σb\ni (ξ1\ni −ξ2\ni )\n\u0011\ncosh\n\u0010\nβG1\nb→i + βG2\nb→i +\nβ\n√\nN σb\ni (ξ1\ni + ξ2\ni )\n\u0011\n\n.\n(S4)\nUsing Eq. (S2a), the cavity magnetizations m1\ni→a, m2\ni→a, and the cavity correlation qi→a can be computed as follows,\nm1\ni→a =\nP\nξ1\ni ,ξ2\ni ξ1\ni e\nP\nb∈∂i\\a ub→i(ξ1\ni ,ξ2\ni )P0(ξ1\ni , ξ2\ni )\nP\nξ1\ni ,ξ2\ni e\nP\nb∈∂i\\a ub→i(ξ1\ni ,ξ2\ni )P0(ξ1\ni , ξ2\ni )\n,\nm2\ni→a =\nP\nξ1\ni ,ξ2\ni ξ2\ni e\nP\nb∈∂i\\a ub→i(ξ1\ni ,ξ2\ni )P0(ξ1\ni , ξ2\ni )\nP\nξ1\ni ,ξ2\ni e\nP\nb∈∂i\\a ub→i(ξ1\ni ,ξ2\ni )P0(ξ1\ni , ξ2\ni )\n,\nqi→a =\nP\nξ1\ni ,ξ2\ni ξ1\ni ξ2\ni e\nP\nb∈∂i\\a ub→i(ξ1\ni ,ξ2\ni )P0(ξ1\ni , ξ2\ni )\nP\nξ1\ni ,ξ2\ni e\nP\nb∈∂i\\a ub→i(ξ1\ni ,ξ2\ni )P0(ξ1\ni , ξ2\ni )\n.\n(S5)\nStarting from random initialization values of cavity magnetizations and correlations, the above belief propagation\niterates until convergence.\nTo carry out the inference of synaptic weights (so-called learning), one only need to\ncompute the full magnetizations by replacing b ∈∂i\\a in Eq. (S5) by b ∈∂i. The free energy can also be estimated\nunder the Bethe approximation, given by −βfBethe = 1\nN\nP\ni ∆fi −N−1\nN\nP\na ∆fa where the single synaptic-weight-pair\ncontribution ∆fi and the single data sample contribution ∆fa are given as follows,\n∆fi = ln\nX\nξ1\ni ,ξ2\ni\nP0(ξ1\ni , ξ2\ni )\nY\nb∈∂i\nµb→i(ξ1\ni , ξ2\ni ),\n(S6a)\n∆fa = β2Γ2\na(1 −˜ψ2)\n2\n−ln\n\u00002 cosh(β2Qa)\n\u0001\n+ β2\n2\n\u0010p\nΓ1a +\np\nΓ2a ˜ψ\n\u00112\n+ ln cosh\n\u0000βG1\na + βG2\na\n\u0001\n+ ln\n\"\n1 + e−2β2Ξa cosh\n\u0000βG1\na −βG2\na\n\u0001\ncosh (βG1a + βG2a)\n#\n,\n(S6b)\nwhere G1\na =\n1\n√\nN\nP\nj∈∂a σa\nj m1\nj→a, G2\na =\n1\n√\nN\nP\nj∈∂a σa\nj m2\nj→a, Γ1\na =\n1\nN\nP\nj∈∂a(1 −(m1\nj→a)2), Γ2\na =\n1\nN\nP\nj∈∂a(1 −\n(m2\nj→a)2), Qa = 1\nN\nP\ni∈∂a qi→a, Ξa = 1\nN\nP\nj∈∂a\n\u0000qj→a −m1\nj→am2\nj→a\n\u0001\n, and ˜ψ =\nΞa\n√\nΓ1\naΓ2\na .\nReplica analysis of the model\nFor a replica analysis, we need to evaluate a disorder average of an integer power of the partition function ⟨Ωn⟩,\nwhere ⟨•⟩is the disorder average over the true RF distribution P0(ξ1,true, ξ2,true) that is factorized over components\n7\nand the corresponding data distribution P({σa}M\na=1|ξ1,true, ξ2,true) as\n⟨Ωn⟩=\nX\n{ξ1,true,ξ2,true,{σa}}\nN\nY\ni=1\nh\nP0(ξ1,true\ni\n, ξ2,true\ni\n)\ni M\nY\na=1\ncosh\n\u0012\nβ\n√\nN ξ1,true · σa\n\u0013\ncosh\n\u0012\nβ\n√\nN ξ2,true · σa\n\u0013\n2Neβ2 cosh (β2q)\n×\nX\n{ξ1,γ,ξ2,γ}\nY\na,γ\ncosh\n\u0012\nβ\n√\nN ξ1,γ · σa\n\u0013\ncosh\n\u0012\nβ\n√\nN ξ2,γ · σa\n\u0013\ncosh (β2Rγ)\nY\ni,γ\nP0(ξ1,γ\ni\n, ξ2,γ\ni\n),\n(S7)\nwhere q =\n1\nN ξ1,true · ξ2,true, and γ is the replica index. The typical free energy can then be obtained as −βf =\nlimn→0,N→∞\nln ⟨Ωn⟩\nnN\n. To compute explicitly ⟨Ωn⟩, we need to specify the order parameters as follows:\nT γ\n1 = 1\nN ξ1,true · ξ1,γ,\nT γ\n2 = 1\nN ξ2,true · ξ2,γ,\nτ γ\n1 = 1\nN ξ1,true · ξ2,γ,\nτ γ\n2 = 1\nN ξ2,true · ξ1,γ,\n(S8a)\nqγ,γ′\n1\n= 1\nN ξ1,γ · ξ1,γ′,\nqγ,γ′\n2\n= 1\nN ξ2,γ · ξ2,γ′,\nRγ = 1\nN ξ1,γ · ξ2,γ,\nrγ,γ′ = 1\nN ξ1,γ · ξ2,γ′.\n(S8b)\nInserting these deﬁnitions in the form of the delta functions as well as their corresponding integral representations,\none can decompose the computation of ⟨Ωn⟩into entropic and energetic parts. However, to further simplify the\ncomputation, we make a simple Ansatz, i.e., the order parameters are invariant under the permutation of replica\nindexes. This is the so-called RS Ansatz. The RS Ansatz reads,\nRγ = R,\ni ˆRγ = ˆR,\nT γ\n1 = T1,\ni ˆT γ\n1 = ˆT1,\nT γ\n2 = T2,\n(S9a)\ni ˆT γ\n2 = ˆT2,\nτ γ\n1 = τ1,\niˆτ γ\n1 = ˆτ1,\nτ γ\n2 = τ2,\niˆτ γ\n2 = ˆτ2,\n(S9b)\nfor any γ, and\nqγ,γ′\n1\n= q1,\ni ˆq1\nγ,γ′ = ˆq1,\nqγ,γ′\n2\n= q2,\n(S10a)\ni ˆq2\nγ,γ′ = ˆq2,\nrγ,γ′ = r,\niˆrγ,γ′ = ˆr,\n(S10b)\nfor any γ and γ′. Note that ( ˆT1, ˆT2, ˆq1, ˆq2, ˆτ1, ˆτ2, ˆR, ˆr) are conjugated order parameters introduced when using the\nintegral representation of the delta function.\nThen we can reorganize ⟨Ωn⟩as\n⟨Ωn⟩=\nZ\ndOd ˆOeNA(O, ˆ\nO,α,β,q,n),\n(S11)\nwhere O and ˆO denote, respectively, all non-conjugated and conjugated order parameters. In the large N limit, the\nintegral is dominated by an equilibrium action:\nA = −nR ˆR −nT1 ˆT1 −nT2 ˆT2 −nτ1 ˆτ1 −nτ2 ˆτ2 −n(n −1)\n2\nq1 ˆq1\n−n(n −1)\n2\nq2 ˆq2 −n(n −1)\n2\nrˆr + GS + αGE,\n(S12)\nwhere GS is the entropic term, and GE is the energetic term.\nWe ﬁrst compute the entropic term as follows,\nGS = ln\n\n\nX\n{ξ1,γ,ξ2,γ}\nexp\n \nˆR\nn\nX\nγ=1\nξ1,γξ2,γ + ˆT1\nn\nX\nγ=1\nξ1,γξ1,true + ˆT2\nn\nX\nγ=1\nξ2,γξ2,true + ˆτ1\nn\nX\nγ=1\nξ1,trueξ2,γ\n!\n× exp\n\nˆτ2\nn\nX\nγ=1\nξ1,γξ2,true +\nX\nγ<γ′\n\u0010\nˆq1ξ1,γξ1,γ′ + ˆq2ξ2,γξ2,γ′ + ˆrξ1,γξ2,γ′\u0011\n+\nn\nX\nγ=1\nln P0(ξ1,γ, ξ2,γ)\n\n\n\n\nξ1,true,ξ2,true\n.\n(S13)\n8\nAfter a bit lengthy algebraic manipulation with the techniques developed in our previous work [16], we arrive at the\nﬁnal result of GS as\nGS = ln\n\n\nZ\nDz\n\nX\nξ1,ξ2\neb1ξ1+b2ξ2+b3ξ1ξ2+ln P0(ξ1,ξ2)\n\n\nn\n\nξ1,true,ξ2,true\n−n\n2 ˆq1 −n\n2 ˆq2,\n(S14)\nwhere we deﬁne Dz = Dz1Dz2Dz3 with three independent standard Gaussian random variables (z1, z2 and z3), [•]\ndenotes a disorder average with respect to the true prior. From this expression, an eﬀective two-spin interaction\nHamiltonian can be extracted, determining the eﬀective partition function Zeﬀin the main text. The eﬀective ﬁelds\nand coupling are given as follows,\nb1 =\nr\nˆq1 −ˆr\n2z1 +\nr\nˆr\n2z3 + ˆT1ξ1,true + ˆτ2ξ2,true,\n(S15a)\nb2 =\nr\nˆq2 −ˆr\n2z2 +\nr\nˆr\n2z3 + ˆT2ξ2,true + ˆτ1ξ1,true,\n(S15b)\nb3 = ˆR −ˆr\n2.\n(S15c)\nTherefore, Zeﬀ= 1+q\n2 eb3 cosh(b1 + b2) + 1−q\n2 e−b3 cosh(b1 −b2).\nNext, we compute the energetic term GE given by\nGE = ln\n*\ncosh (βX0) cosh (βY 0)\neβ2 cosh (β2q)\nn\nY\nγ=1\ncosh (βXγ) cosh (βY γ)\ncosh (β2Rγ)\n+\n,\n(S16)\nwhere ⟨•⟩deﬁnes the disorder average. X0 =\n1\n√\nN\nPN\ni=1 ξ1,true\ni\nσi, Y 0 =\n1\n√\nN\nPN\ni=1 ξ2,true\ni\nσi, and Xγ =\n1\n√\nN\nPN\ni=1 ξ1,γ\ni\nσi,\nY γ =\n1\n√\nN\nPN\ni=1 ξ2,γ\ni\nσi, where σ represents a typical data sample.\nThese four quantities are correlated random\nGaussian variables, due to the central limit theorem. To satisfy their covariance structure determined by the order\nparameters, the random variables X0, Y 0, Xγ and Y γ are parameterized by six standard Gaussian variables of zero\nmean and unit variance (t0, x0, u, u′, yγ, ωγ) as follows,\nX0 = t0,\n(S17a)\nY 0 = qt0 +\np\n1 −q2x0,\n(S17b)\nXγ = T1t0 + τ2 −T1q\np\n1 −q2 x0 + Bu +\np\n1 −q1ωγ,\n(S17c)\nY γ = τ1t0 + T2 −τ1q\np\n1 −q2 x0 + r −A\nB\nu +\nR −r\n√1 −q1\nωγ + Ku′\n+\ns\n1 −q2 −(R −r)2\n1 −q1\nyγ,\n(S17d)\nwhere A = T1τ1 + (τ2−T1q)(T2−τ1q)\n1−q2\n, B =\nq\nq1 −(T1)2 −(τ2−T1q)2\n1−q2\n, and K =\nq\nq2 −(τ1)2 −(T2−τ1q)2\n1−q2\n−( r−A\nB )2. There-\nfore, the GE term can be calculated by a standard Gaussian integration given by\nGE = ln\n\"Z\nDt0Dx0DuDu′ cosh (βt0) cosh β(qt0 +\np\n1 −q2x0)\neβ2 cosh (β2q)\n×\n Z\nDωDy\n1\ncosh (β2R) cosh β(T1t0 + τ2 −T1q\np\n1 −q2 x0 + Bu +\np\n1 −q1ω)\n× cosh β(τ1t0 + T2 −τ1q\np\n1 −q2 x0 + r −A\nB\nu + + R −r\n√1 −q1\nω + Ku′ + Cy)\n!n#\n,\n(S18)\nwhere C ≡\nq\n1 −q2 −(R−r)2\n1−q1 .\n9\nBy introducing the auxiliary variables as follows,\nZE = eβ2(R−r) cosh (βΛ+) + e−β2(R−r) cosh (βΛ−),\n(S19a)\nΛ+ = (T1 + τ1)t0 + (T2 + τ2) −q(T1 + τ1)\np\n1 −q2\nx0 +\n\u0010\nB + r −A\nB\n\u0011\nu + Ku′,\n(S19b)\nΛ−= (T1 −τ1)t0 + (τ2 −T2) −q(T1 −τ1)\np\n1 −q2\nx0 +\n\u0010\nB −r −A\nB\n\u0011\nu −Ku′,\n(S19c)\nwe ﬁnally arrive at the free energy Fβ = −βfRS as\nFβ = −R ˆR −T1 ˆT1 −T2 ˆT2 −τ1 ˆτ1 −τ2 ˆτ2 + ˆq1\n2 (q1 −1) + ˆq2\n2 (q2 −1)\n+ rˆr\n2 +\nZ\nDz [ln Zeﬀ]ξ1,true,ξ2,true −α ln\n\u00002 cosh(β2R)\n\u0001\n+ αβ2\n\u0012\n1 −q1 + q2\n2\n\u0013\n+\nαe−β2\ncosh (β2q)\nZ\nDt cosh βt0 cosh β(qt0 +\np\n1 −q2x0) ln ZE,\n(S20)\nwhere Dt = Dt0Dx0DuDu′. The saddle-point analysis in Eq. (S11) requires that the order parameters should be\nthe stationary point of the free energy. All these conjugated and non-conjugated order parameters are subject to\nsaddle-point equations derived from setting the corresponding derivatives of the free energy with respect to the order\nparameters zero. Here we skip the technical details to derive the saddle-point equations. We refer the interested\nreaders to our previous work [16].\nThe saddle-point equations for non-conjugated order parameters are given by\nT1 = [ξ1,true⟨ξ1⟩],\n(S21a)\nT2 = [ξ2,true⟨ξ2⟩],\n(S21b)\nq1 = [⟨ξ1⟩2],\n(S21c)\nq2 = [⟨ξ2⟩2],\n(S21d)\nτ1 = [ξ1,true⟨ξ2⟩],\n(S21e)\nτ2 = [ξ2,true⟨ξ1⟩],\n(S21f)\nR = [⟨ξ1ξ2⟩],\n(S21g)\nr = [⟨ξ1⟩⟨ξ2⟩],\n(S21h)\nwhere [•] indicates an average over the standard Gaussian random variables z and the true prior P0(ξ1,true, ξ2,true),\nand ⟨•⟩is an average under the eﬀective Boltzmann distribution P(ξ1, ξ2) =\n1\nZeff eb1ξ1+b2ξ2+b3ξ1ξ2+ln P0(ξ1,ξ2).\nIt is straightforward to show that\n⟨ξ1⟩=\n∂\n∂b1\nln Zeﬀ\n= tanh b1(1 + q tanh b3) + tanh b2(q + tanh b3)\n1 + q tanh b3 + tanh b1 tanh b2(q + tanh b3) .\n(S22)\nThe expression of\n\nξ2\u000b\nis obtained by exchange of b1 and b2 in Eq. (S22). The correlation term is computed similarly,\n⟨ξ1ξ2⟩=\n∂\n∂b3\nln Zeﬀ\n= q + tanh b3 + tanh b1 tanh b2(1 + q tanh b3)\n1 + tanh b1 tanh b2(q + tanh b3) + q tanh b3\n.\n(S23)\n10\nThe saddle-point equations for conjugated order parameters are given by\nˆT1 = αβ2⟨⟨G+\ns ⟩⟩,\n(S24a)\nˆT2 = αβ2⟨⟨⟨G−\ns ⟩⟩⟩,\n(S24b)\nˆq1 = αβ2 \n(G+\ns )2\u000b\n,\n(S24c)\nˆq2 = αβ2 \n(G−\ns )2\u000b\n,\n(S24d)\nˆτ1 = αβ2⟨⟨G−\ns ⟩⟩,\n(S24e)\nˆτ2 = αβ2⟨⟨⟨G+\ns ⟩⟩⟩,\n(S24f)\nˆR = αβ2 \nG−\nc\n\u000b\n−αβ2 tanh(β2R),\n(S24g)\nˆr = 2αβ2 \nG+\ns G−\ns\n\u000b\n,\n(S24h)\nwhere we deﬁne three diﬀerent measures as\n⟨•⟩≡\ne−β2\ncosh(β2q)\nZ\nDt cosh(βt0) cosh(βqt0 + β\np\n1 −q2x0)•,\n(S25a)\n⟨⟨•⟩⟩≡\ne−β2\ncosh(β2q)\nZ\nDt sinh(βt0) cosh(βqt0 + β\np\n1 −q2x0)•,\n(S25b)\n⟨⟨⟨•⟩⟩⟩≡\ne−β2\ncosh(β2q)\nZ\nDt cosh(βt0) sinh(βqt0 + β\np\n1 −q2x0)•,\n(S25c)\nand three auxiliary quantities as\nG−\nc = eβ2(R−r) cosh βΛ+ −e−β2(R−r) cosh βΛ−\neβ2(R−r) cosh βΛ+ + e−β2(R−r) cosh βΛ−\n,\n(S26a)\nG+\ns = eβ2(R−r) sinh βΛ+ + e−β2(R−r) sinh βΛ−\neβ2(R−r) cosh βΛ+ + e−β2(R−r) cosh βΛ−\n,\n(S26b)\nG−\ns = eβ2(R−r) sinh βΛ+ −e−β2(R−r) sinh βΛ−\neβ2(R−r) cosh βΛ+ + e−β2(R−r) cosh βΛ−\n.\n(S26c)\nA small-q expansion of the order parameter r\nTo understand why the order parameter r with prior is higher than that without prior, we carry out a small-q\nexpansion of the order parameter. In the small-q limit, we can reasonably assume that the conjugated parameters\nare close to those estimated under the prior-free case, thereby avoiding a complex analysis of the original iterative\nequations. We ﬁrst denote some short-hand notations, as t1 ≡tanh b1, t2 ≡tanh b2, t3 ≡tanh b3, a ≡t1 + t2t3,\nb ≡t2 + t1t3, and c ≡1 + t1t2t3. It then follows that\n\nξ1\u000b\np\n\nξ2\u000b\np = (t1(1 + qt3) + t2(q + t3))(t2(1 + qt3) + t1(q + t3))\n(1 + qt3 + t1t2(q + t3))2\n(S27a)\n=\n\u0010\nξ1\u000b\npf + q\n\nξ2\u000b\npf\n\u0011 \u0010\nξ2\u000b\npf + q\n\nξ1\u000b\npf\n\u0011\n\u0010\n1 + q ⟨ξ1ξ2⟩pf\n\u00112\n(S27b)\n=\n\nξ1\u000b\npf\n\nξ2\u000b\npf + q\n\u0010\nξ1\u000b2\npf +\n\nξ2\u000b2\npf\n\u0011\n−2q\n\u0010\nξ1ξ2\u000b\npf\n\nξ1\u000b\npf\n\nξ2\u000b\npf\n\u0011\n+ O(q2).\n(S27c)\nwhere we have used the result of the prior-free case, i.e.,\n\nξ1\u000b\npf =\na\nc ,\n\nξ2\u000b\npf =\nb\nc, and\n\nξ1ξ2\u000b\npf =\nt3+t1t2\nc\n. The\nsubscripts p and pf indicate prior and prior-free, respectively. After taking the disorder average, i.e., [•], we obtain\nthe following relationship:\nrp = rpf + q\n\u0000[⟨ξ1⟩2\npf] + [⟨ξ2⟩2\npf] −2[⟨ξ1⟩pf⟨ξ2⟩pf⟨ξ1ξ2⟩pf]\n\u0001\n.\n(S28)\nFor positive q, the last term is veriﬁed to be non-negative and decreasing as α increases. For negative q, the result also\nholds. Therefore the last term contributes to the larger value of the order parameter rp (more precisely, in absolute\nvalue) compared with the prior-free case.\n11\nDerivation of the critical point αc for spontaneous symmetry breaking\nWhen α approaches the SSB threshold from below, all order parameters get close to zero, except for R which is\nalways equal to q due to the prior information. It is easy to show that ˆR is also zero below the SSB threshold.\nTherefore, b1, b2 and b3 are all small quantities. Then we can expand our order parameters to leading order. Note\nthat ⟨ξ1⟩≃b1 + qb2, and ⟨ξ2⟩≃b2 + qb1. It then follows that\nT1 = [ξ1,true⟨ξ1⟩] ≃ˆT1 + q ˆτ2 + q ˆτ1 + q2 ˆT2,\n(S29a)\nT2 = [ξ2,true⟨ξ2⟩] ≃ˆT2 + q ˆτ2 + q ˆτ1 + q2 ˆT1,\n(S29b)\nτ1 = [ξ1,true⟨ξ2⟩] ≃ˆτ1 + q ˆT1 + q ˆT2 + q2 ˆτ2,\n(S29c)\nτ2 = [ξ2,true⟨ξ1⟩] ≃ˆτ2 + q ˆT1 + q ˆT2 + q2 ˆτ1.\n(S29d)\nBecause R = q, by deﬁning W(q) =\neβ2q\n2 cosh(β2q), one arrives at the approximation G±\ns ≃βW(q)(Λ+ ∓Λ−) ± βΛ−.\nTo proceed, it is worth noticing that\n⟨⟨Λ+⟩⟩= β[T1 + τ1 + τ2 tanh (β2q) + T2 tanh (β2q)],\n(S30a)\n⟨⟨Λ−⟩⟩= β[T1 −τ1 + τ2 tanh (β2q) −T2 tanh (β2q)],\n(S30b)\n⟨⟨⟨Λ+⟩⟩⟩= β[T2 + τ2 + τ1 tanh (β2q) + T1 tanh (β2q)],\n(S30c)\n⟨⟨⟨Λ−⟩⟩⟩= β[τ2 −T2 + T1 tanh (β2q) −τ1 tanh (β2q)].\n(S30d)\n(S30e)\nBased on the above approximations, it is easy to derive the following approximate values of the relevant conjugated\nquantities\nˆT1 ≃αβ4[T1 + Υτ1 + τ2 tanh (β2q) + ΥT2 tanh (β2q)],\n(S31a)\nˆT2 ≃αβ4[T2 + Υτ2 + τ1 tanh (β2q) + ΥT1 tanh (β2q)],\n(S31b)\nˆτ1 ≃αβ4[τ1 + ΥT1 + T2 tanh (β2q) + Υτ2 tanh (β2q)],\n(S31c)\nˆτ2 ≃αβ4[τ2 + ΥT2 + T1 tanh (β2q) + Υτ1 tanh (β2q)],\n(S31d)\nwhere Υ ≡2W(q) −1.\nThe above approximations of (T1, T2, τ1, τ2) and ( ˆT1, ˆT2, ˆτ1, ˆτ2) can be easily recast into a compact matrix form as\nfollows,\n\n\n\n\nT1\nT2\nτ1\nτ2\n\n\n\n=\n\n\n\n\n1\nq2\nq\nq\nq2\n1\nq\nq\nq\nq\n1\nq2\nq\nq\nq2\n1\n\n\n\n\n\n\n\n\nˆT1\nˆT2\nˆτ1\nˆτ2\n\n\n\n,\n(S32)\nand\n\n\n\n\nˆT1\nˆT2\nˆτ1\nˆτ2\n\n\n\n= αβ4\n\n\n\n\n1\nΥ tanh (β2q)\nΥ\ntanh (β2q)\nΥ tanh (β2q)\n1\ntanh (β2q)\nΥ\nΥ\ntanh (β2q)\n1\nΥ tanh (β2q)\ntanh (β2q)\nΥ\nΥ tanh (β2q)\n1\n\n\n\n\n\n\n\n\nT1\nT2\nτ1\nτ2\n\n\n\n.\n(S33)\nA linear stability analysis implies that the stability matrix M can be organized in this case as a block matrix of the\nform M =\n\u0012 A B\nB A\n\u0013\n, where the matrices A and B are derived from Eq. (S32) and Eq. (S33), and given respectively\nby\nA = αβ4\n\u0012\n(1 + q tanh (β2q))(1 + qΥ)\n(tanh (β2q) + q)(q + Υ)\n(tanh (β2q) + q)(Υ + q)\n(1 + q tanh (β2q))(1 + qΥ)\n\u0013\n,\n(S34a)\nB = αβ4\n\u0012 (Υ + q)(1 + q tanh (β2q)) (Υq + 1)(q + tanh (β2q))\n(Υq + 1)(q + tanh (β2q)) (Υ + q)(1 + q tanh (β2q))\n\u0013\n.\n(S34b)\n12\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n1\n4\n1\n0\n1.5\n0.2\n0.4\nq\n2\n0.6\n0.8\n1\n2.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nprior-free\nwith prior\nFIG. S1:\n(Color online) Comparison of SSB critical data densities in models with/without prior knowledge.\nAccording to the determinant identity for a block matrix, |M −λI| = |A + B −λI| |A −B −λI|, the eigenvalues of\nthe stability matrix can be determined by the following two equations,\n\f\f\f\f\nαβ4(1 + q)(1 + q tanh (β2q))(1 + Υ) −λ\nαβ4(1 + q)(Υ + 1)(q + tanh(β2q))\nαβ4(1 + q)(Υ + 1)(q + tanh (β2q))\nαβ4(1 + q)(1 + q tanh (β2q))(1 + Υ) −λ\n\f\f\f\f = 0,\n(S35)\nand\n\f\f\f\f\nαβ4(1 −q)(1 + q tanh (β2q))(1 −Υ) −λ\nαβ4(1 −q)(Υ −1)(q + tanh (β2q))\nαβ4(1 −q)(Υ −1)(q + tanh (β2q))\nαβ4(1 −q)(1 + q tanh (β2q))(1 −Υ) −λ\n\f\f\f\f = 0.\n(S36)\nUsing the mathematical identity max(1 −q, 1 + q) = 1 + |q|, and max(1 −Υ, 1 + Υ) = 1 + |Υ|, we conclude that the\nmaximal value of all eigenvalues is given by λmax = αβ4(1 + |q|)(1 + |Υ|)(1 + q tanh (β2q) + |q + tanh (β2q)|). The\ncritical data density for the SSB phase is thus given by\nαc =\nβ−4\n(1 + |q|)(1 + |Υ|)(1 + q tanh (β2q) + |q + tanh (β2q)|).\n(S37)\nThis SSB critical data density is compared with that of the prior-free case (i.e., Λ(β, q) in the main text, see also our\nprevious work [16]) in Fig. S1. We clearly see that the prior knowledge about q signiﬁcantly reshapes the critical data\ndensity surface for the SSB phase, which provides deep insights about roles of prior information beyond our previous\nwork [16].\nPrediction of noise level β and correlation level q from raw data\nWe ﬁrst write the posterior probability of the hyper-parameters β and q as\nP(β, q|D) =\nX\nξ1,ξ2\nP(β, q, ξ1, ξ2|D) =\nX\nξ1,ξ2\nP(D|β, q, ξ1, ξ2)P0(ξ1, ξ2|q)\nR R\ndβdq P\nξ1,ξ2 P(D|β, q, ξ1, ξ2)P0(ξ1, ξ2|q),\n(S38)\nwhere we have used the Bayes’ rule, and we assume that P0(ξ1, ξ2, β, q) = P0(ξ1, ξ2|q) ˜P0(β, q) where ˜P0(β, q) is a\nconstant or we have no prior knowledge about the true values of the hyper-parameters. Therefore, we have\nP(β, q|D) ∝\nX\nξ1,ξ2\nM\nY\na=1\nP(σa|β, q, ξ1, ξ2)\nN\nY\ni=1\nP0(ξ1\ni , ξ2\ni |q).\n(S39)\n13\nNote that the data distribution can be expressed as\nP(σa|β, q, ξ1, ξ2) =\ncosh\n\u0012\nβ\n√\nN ξ1 · σa\n\u0013\ncosh\n\u0012\nβ\n√\nN ξ2 · σa\n\u0013\n2Neβ2 cosh (β2Q)\n.\n(S40)\nThe posterior probability of the hyper-parameters can be ﬁnally simpliﬁed as P(β, q|D) ∝e−β2MΩ, where Ωis exactly\nthe partition function of the posterior P(ξ1, ξ2|D). This partition function can be written explicitly as follows,\nΩ(β, q) =\nX\nξ1,ξ2\nM\nY\na=1\ncosh\n\u0012\nβ\n√\nN ξ1 · σa\n\u0013\ncosh\n\u0012\nβ\n√\nN ξ2 · σa\n\u0013\ncosh (β2Q)\nN\nY\ni=1\nP0(ξ1\ni , ξ2\ni |q).\n(S41)\nSearching for consistent hyper-parameters (β, q) compatible with the supplied dataset is equivalent to maximizing\nthe posterior P(β, q|D). Following this principle, we ﬁrst derive the temperature equation as\n∂ln P(β, q|D)\n∂β\n= −2Mβ + ∂\n∂β ln Ω(β, q).\n(S42)\nNote that in statistical physics, the energy function is given by Nϵ = −∂ln Ω\n∂β , where ϵ(β, q) denotes the energy density\n(per neuron). We thus conclude that β should obey the following temperature equation,\nβ = −ϵ(β, q)\n2α\n.\n(S43)\nNote that when the true prior is taken into account, the energy density of the model is analytic with the result\nϵ = −2αβ independent of q.\nGiven the dataset and an initial guess of β, the aforementioned message passing scheme can be used to estimate\nthe energy density of the system as Nϵ = −P\ni ∆ϵi +(N −1) P\na ∆ϵa based on the Bethe approximation. The energy\ncontribution of one synapse-pair reads\n∆ϵi =\nP\nξ1\ni ,ξ2\ni\nP\nb∈∂i\n∂ub→i(ξ1\ni ,ξ2\ni )\n∂β\ne\nP\nb∈∂i ub→i(ξ1\ni ,ξ2\ni )+ln P0(ξ1\ni ,ξ2\ni )\nP\nξ1\ni ,ξ2\ni e\nP\nb∈∂i ub→i(ξ1\ni ,ξ2\ni )+ln P0(ξ1\ni ,ξ2\ni )\n,\n(S44)\nwhere ∂ub→i(ξ1\ni ,ξ2\ni )\n∂β\nreads as follows,\nβ ∂ub→i(ξ1\ni , ξ2\ni )\n∂β\n= β2[Γ1\nb→i + Γ1\nb→i + 2Ξb→i] −2β2\n\u0012\nQb→i + ξ1\ni ξ2\ni\nN\n\u0013\ntanh\n\u0012\nβ2Qb→i + β2\nN ξ1\ni ξ2\ni\n\u0013\n+ Yb→i tanh Yb→i +\n∆b→i\n1 + ∆b→i\n\u0000−4β2Ξb→i + Xb→i tanh Xb→i −Yb→i tanh Yb→i\n\u0001\n,\n(S45)\nwhere Xb→i ≡βG1\nb→i −βG2\nb→i +\nβ\n√\nN σb\ni (ξ1\ni −ξ2\ni ), Yb→i ≡βG1\nb→i + βG2\nb→i +\nβ\n√\nN σb\ni (ξ1\ni + ξ2\ni ), and ∆b→i ≡\ne−2β2Ξb→i cosh Xb→i\ncosh Yb→i . The energy contribution of one data sample is given by\nβ∆ϵa = β2(Γ1\na + Γ2\na + 2Ξa) −2β2Qa tanh (β2Qa) + Ya tanh Ya\n+\n∆a\n1 + ∆a\n\u0000−4β2Ξa + Xa tanh Xa −Ya tanh Ya\n\u0001\n,\n(S46)\nwhere Xa ≡βG1\na −βG2\na, Ya ≡βG1\na + βG2\na, and ∆a = e−2β2Ξa cosh Xa\ncosh Ya .\nNext, we derive the correlation equation. Note that P0(ξ1\ni , ξ2\ni ) =\neJ0ξ1\ni ξ2\ni\n4 cosh J0 , where J0 = tanh−1 q. We then have\n∂P(β, q|D)\n∂q\n= e−Mβ2 ∂Ω\n∂q = 0,\n(S47)\n14\nwhich requires that ∂Ω\n∂q = 0. It then follows that\n∂Ω\n∂q = Ω\nX\nξ1,ξ2\nP(ξ1, ξ2|D)\nX\ni\n(ξ1\ni ξ2\ni −tanh J0)∂J0\n∂q\n= Ω\n X\ni\n\nξ1\ni ξ2\ni\n\u000b\nP (ξ1,ξ2|D) −N tanh J0\n!\n∂J0\n∂q = 0.\n(S48)\nTo satisfy Eq. (S48), we must enforce the following correlation equation,\nq = 1\nN\nX\ni\nqi,\n(S49)\nwhere qi can be computed in a single instance by iterating the message passing scheme. More precisely,\nqi =\nP\nξ1\ni ,ξ2\ni ξ1\ni ξ2\ni e\nP\nb∈∂i ub→i(ξ1\ni ,ξ2\ni )P0(ξ1\ni , ξ2\ni )\nP\nξ1\ni ,ξ2\ni e\nP\nb∈∂i ub→i(ξ1\ni ,ξ2\ni )P0(ξ1\ni , ξ2\ni )\n.\n(S50)\nIn addition, the negative log-likelihood of the hyper-parameter posterior per neuron can be estimated as\nL\nN =\nC −ln Ω\nN + αβ2, where C is an irrelevant constant, and the second term can be approximated by βfBethe.\nWe ﬁnally remark that, in the original Nishimori model [32], a parameter p is used to characterize the coupling\nbias in a multi-spin interaction model (e.g., two-body interactions), while in our case q is used to characterize the\ncorrelation level among synapses (couplings). Therefore, p and q are physically diﬀerent. In addition, the Nishimori\nline is speciﬁed by T = T(p), where T is the temperature of the model, and T(p) is a temperature-like parameter\nexpressed as a function of p [32]. However, in our setting, the energy is analytic, i.e., ϵ = −2αβ, only when the\nstudent uses the same true q and the same temperature as the true one (not a q-transformed temperature). During\nthe expectation-maximization estimation, the estimated q (or β) is a complex function of β and q at a previous step\n(see Eqs. (S43), (S49) and (S50)), therefore β and q have a highly non-trivial relationship, unlike simply T = T(p) in\nthe original Nishimori model.\n∗Electronic address: huanghp7@mail.sysu.edu.cn\n[1] H. Barlow, Neural Computation 1, 295 (1989).\n[2] D. Marr, Proceedings of the Royal Society of London B: Biological Sciences 176, 161 (1970).\n[3] D. Kersten, P. Mamassian, and A. Yuille, Annual Review of Psychology 55, 271 (2004).\n[4] A. N. Sanborn and N. Chater, Trends in Cognitive Sciences 20, 883 (2016).\n[5] K. P. K¨ording and D. M. Wolpert, Nature 427, 244 (2004).\n[6] T. R. Darlington, J. M. Beck, and S. G. Lisberger, Nature Neuroscience 21, 1442 (2018).\n[7] J. M. Beck, W. J. Ma, R. Kiani, T. Hanks, A. K. Churchland, J. Roitman, M. N. Shadlen, P. E. Latham, and A. Pouget,\nNeuron 60, 1142 (2008).\n[8] A. Yuille and D. Kersten, Trends in Cognitive Sciences 10, 301 (2006).\n[9] T. S. Lee and D. Mumford, Journal of the Optical Society of America A 20, 1434 (2003).\n[10] T. Griﬃths and J. Tenenbaum, J. Exp. Psychol. Gen. 140, 725 (2011).\n[11] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, in Proceedings of the 32Nd International Conference on\nInternational Conference on Machine Learning - Volume 37 (JMLR.org, 2015), ICML’15, pp. 1613–1622.\n[12] J. M. Hern´andez-Lobato and R. P. Adams, in Proceedings of the 32Nd International Conference on International Conference\non Machine Learning - Volume 37 (JMLR.org, 2015), ICML’15, pp. 1861–1869.\n[13] A. Atanov, A. Ashukha, K. Struminsky, D. Vetrov, and M. Welling, arXiv:1810.06943 (2018).\n[14] S. Hansem, N. Devika, M. Nicolas, and J. Mehrdad, Neuron 103, 934 (2019).\n[15] G. E. Hinton and R. R. Salakhutdinov, Science 313, 504 (2006).\n[16] T. Hou, K. Y. M. Wong, and H. Huang, Journal of Physics A: Mathematical and Theoretical 52, 414001 (2019).\n[17] H. Nishimori, Statistical Physics of Spin Glasses and Information Processing: An Introduction (Oxford University Press,\nOxford, 2001).\n[18] H. Huang, Journal of Statistical Mechanics: Theory and Experiment 2017, 053302 (2017).\n[19] N. Le Roux and Y. Bengio, Neural Comput. 20, 1631 (2008).\n[20] G. Hinton, Neural Computation 14, 1771 (2002).\n[21] H. Huang and T. Toyoizumi, Phys. Rev. E 91, 050101 (2015).\n15\n[22] Y. Iba, Journal of Physics A: Mathematical and General 32, 3875 (1999).\n[23] See supplemental material at http://... for technical details of the theory and simulation methods.\n[24] M. M´ezard, G. Parisi, and M. A. Virasoro, Spin Glass Theory and Beyond (World Scientiﬁc, Singapore, 1987).\n[25] H. Huang and T. Toyoizumi, Phys. Rev. E 94, 062310 (2016).\n[26] A. P. Dempster, N. M. Laird, and D. B. Rubin, Journal of the royal statistical society. Series B 39, 1 (1977).\n[27] Y. LeCun, The MNIST database of handwritten digits, retrieved from http://yann.lecun.com/exdb/mnist.\n[28] H. Barlow, in Sensory Communication, edited by W. Rosenblith (Cambridge, Massachusetts: MIT Press, 1961), pp.\n217–234.\n[29] H. Huang, arXiv:1911.07662 (2019).\n[30] M. M´ezard and G. Parisi, Eur. Phys. J. B 20, 217 (2001).\n[31] M. M´ezard and A. Montanari, Information, Physics, and Computation (Oxford University Press, Oxford, 2009).\n[32] H. Nishimori, J. Phys. C 13, 4071 (1980).\n",
  "categories": [
    "cond-mat.dis-nn",
    "cond-mat.stat-mech",
    "cs.LG"
  ],
  "published": "2019-11-06",
  "updated": "2020-05-27"
}