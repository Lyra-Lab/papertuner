{
  "id": "http://arxiv.org/abs/1911.02723v2",
  "title": "Option Compatible Reward Inverse Reinforcement Learning",
  "authors": [
    "Rakhoon Hwang",
    "Hanjin Lee",
    "Hyung Ju Hwang"
  ],
  "abstract": "Reinforcement learning in complex environments is a challenging problem. In\nparticular, the success of reinforcement learning algorithms depends on a\nwell-designed reward function. Inverse reinforcement learning (IRL) solves the\nproblem of recovering reward functions from expert demonstrations. In this\npaper, we solve a hierarchical inverse reinforcement learning problem within\nthe options framework, which allows us to utilize intrinsic motivation of the\nexpert demonstrations. A gradient method for parametrized options is used to\ndeduce a defining equation for the Q-feature space, which leads to a reward\nfeature space. Using a second-order optimality condition for option parameters,\nan optimal reward function is selected. Experimental results in both discrete\nand continuous domains confirm that our recovered rewards provide a solution to\nthe IRL problem using temporal abstraction, which in turn are effective in\naccelerating transfer learning tasks. We also show that our method is robust to\nnoises contained in expert demonstrations.",
  "text": "arXiv:1911.02723v2  [cs.LG]  19 Jan 2021\nOption Compatible Reward Inverse Reinforcement\nLearning\nRakhoon Hwanga, Hanjin Leeb, and Hyung Ju Hwang∗a\naDepartment of Mathematics, Pohang University of Science and\nTechnology (POSTECH), Pohang, Republic of Korea\nbGlobal leadership school, Handong global university, Pohang,\nRepublic of Korea\nAbstract\nReinforcement learning in complex environments is a challenging\nproblem.\nIn particular, the success of reinforcement learning algo-\nrithms depends on a well-designed reward function. Inverse reinforce-\nment learning (IRL) solves the problem of recovering reward functions\nfrom expert demonstrations.\nIn this paper, we solve a hierarchical\ninverse reinforcement learning problem within the options framework,\nwhich allows us to utilize intrinsic motivation of the expert demonstra-\ntions. A gradient method for parametrized options is used to deduce\na deﬁning equation for the Q-feature space, which leads to a reward\nfeature space. Using a second-order optimality condition for option pa-\nrameters, an optimal reward function is selected. Experimental results\nin both discrete and continuous domains conﬁrm that our recovered\nrewards provide a solution to the IRL problem using temporal ab-\nstraction, which in turn are eﬀective in accelerating transfer learning\ntasks. We also show that our method is robust to noises contained in\nexpert demonstrations.1\n∗Correponding address: Department of Mathematics, Pohang University of Science and\nTechnology (POSTECH), 37673, 77, Cheongam-ro, Nam-gu, Pohang, Gyeongsangbuk-do,\nRepublic of Korea\nE-mail address: hjhwang@postech.ac.kr\n1This paper is under consideration at Pattern Recognition Letters.\n1\n1\nIntroduction\nReinforcement learning (RL) method seeks an optimal policy for a given\nreward function in a Markov decision process (MDP). There are several cir-\ncumstances in which an agent can learn only from an expert demonstration,\nbecause it is diﬃcult to prescribe a proper reward function for a given task.\nInverse reinforcement learning (IRL) aims to ﬁnd a reward function that\ncan explain the expert’s behavior. When the IRL method is applied to a\ncomplex environment, the size of each trajectory of the required demonstra-\ntion by the expert can be huge. There are also certain complex tasks that\nmust be segmented into a sequence of sub-tasks (e.g., robotics of ubiquitous\ngeneral-purpose automation ([10] [12]), robotic surgical procedure training\n([6], [13]), hierarchical human behavior modeling [20], and autonomous driv-\ning [15]).\nFor such complex tasks, a problem designer can decompose it\nhierarchically. Then an expert can easily demonstrate it at diﬀerent levels\nof implementation.\nAnother challenge with the IRL method is the design of feature spaces\nthat capture the structure of the reward functions. Linear models for reward\nfunctions have been used in existing IRL algorithms. However, nonlinear\nmodels have recently been introduced [14], [5], [16]. Exploring more general\nfeature spaces for reward functions becomes necessary when expert intuition\nis insuﬃcient for designing good features, including linear models.\nThis\nproblem raises concerns, such as in the robotics ﬁeld [19].\nRegarding the ﬁrst aspect of our problem, several works considered the\ndecomposition of underlying reward functions for given expert demonstra-\ntions in RL and IRL problems ([8], [3], [11]). For hierarchical IRL problems,\nmost of works focus on how to perform segmentation on demonstrations\nof complex tasks and ﬁnd suitable reward functions. For the IRL problem\nin the options framework, option discovery should be ﬁrst carried out as\na segmentation process. Since our work focuses on hierarchical extensions\nof policy gradient based IRL algorithms, we assign options for each given\ndomain instead of applying certain option discovery algorithms.\nTo simultaneously solve the reward construction problem while capturing\nthe hierarchical structure, we propose a new method that applies the option\nframework presented by [22] to the compatible reward inverse reinforce-\nment learning (CR-IRL) [16], a recent work on generating a feature space\n2\nof rewards. Our method is called Option Compatible Reward Inverse Rein-\nforcement Learning (OCR-IRL). Previous works on the selection of proper\nreward functions for the IRL problem require design features that consider\nthe environment of the problem. However, the CR-IRL algorithm directly\nprovides a space of features from which compatible reward functions can be\nconstructed.\nThe main contribution of our work comprises the following items.\n• New method of assigning reward functions for a hierarchical IRL prob-\nlem is introduced. While handling the termination of each option, in-\ntroducing parameters to termination and intra-option policy functions\nin the policy gradient framework allows us to choose better reward\nfunctions while reﬂecting the hierarchical structure of the task.\n• The recovered reward functions can be used to transfer knowledge\nacross related tasks. Previous works such as [2] have shown that the\noptions framework provides beneﬁts for transfer learning. Our method\nmakes the knowledge transfer easier by converting the information\ncontained in the options into a numerical reward value.\n• It also shows better robustness to noise included in expert demon-\nstrations than other algorithms without using a hierarchical learning\nframework. The noise robustness of our algorithm is enabled by gen-\neral representation of reward functions compared to previous linear\nIRL algorithms.\nThere are diﬀerences in several aspects between our work and some of\nrecent works [8], [17] and [11] on segmentation of reward functions in IRL\nproblems. Although both OptionGAN [8] and our work use policy gradient\nmethods as a common grounding component, the former work adopts the\ngenerative adversarial approach to solve the IRL problem while we construct\nan explicit equation which deﬁnes reward features. [17] uses Bayesian non-\nparametric mixture models to simultaneously partition the demonstration\nand learn associated reward functions. It has an advantage in the case with\ndomains in which subgoals of each subtask are deﬁnite. For such domains,\na successful segmentation simply deﬁnes task-wise reward functions. How-\never, our work allows for indeﬁniteness of subgoals for which an assignment\nof rewards is not simple.\n[11] focuses on segmentation using transitions\n3\ndeﬁned as changes in local linearity about a kernel function. It assumes\npre-designed features for reward functions. On the other hand, our method\ndoes not assume any pre-knowledge on feature spaces.\n2\nPreliminaries\n2.1\nMarkov decision process\nThe Markov decision process comprises the state space, S, the action space,\nA, the transition function, P : S × A →(S →[0, 1]), and the reward\nfunction, R : S × A →R. A policy is a probability distribution, π : S ×\nA →[0, 1], over actions conditioned on the states. The value of a policy is\ndeﬁned as Vπ(s) = Eπ[P∞\nt=0 γtRt+1|S0 = s], and the action-value function is\nQπ(s, a) = Eπ[P∞\nt=0 γtRt+1|S0 = s, A0 = a], where γ ∈[0, 1] is the discount\nfactor.\n2.2\nPolicy Gradients\nPolicy gradient methods [21] aim to optimize a parametrized policy, πθ, via\nstochastic gradient ascent. In a discounted setting, the optimization of the\nexpected γ-discounted return with respect to an initial state s0, ρ(θ, s0) =\nEπθ[P∞\nt=0 γtRt+1|S0 = s0], is considered. It can be written as\nρ(θ, s0) =\nX\ns\nµπθ(s|s0)\nX\na\nπθ(a|s)R(s, a)\nwhere µπθ(s|s0) = P∞\nt=0 γtP(St = s|S0 = s0, πθ). The policy gradient theo-\nrem ([21]) states:\n∇θρ(θ, s0) =\nX\ns,a\nµπθ(s, a|s0)∇θ log πθ(a|s)Qπθ(s, a),\nwhere µπθ(s, a|s0) = µπθ(s|s0)πθ(a|s).\n2.3\nCompatible Reward Inverse Reinforcement Learning\nCompatible reward inverse reinforcement learning[16] is an algorithm that\ngenerates a set of base functions spanning the subspace of reward functions\nthat cause the policy gradient to vanish.\nAs input, a parametric policy\nspace, ΠΘ = {πθ : θ ∈Θ ⊂Rk}, and a set of trajectories from the expert\n4\npolicy, πE, are taken. It ﬁrst builds the features, {φi}, of the action-value\nfunction, which cause the policy gradient to vanish. These features can be\ntransformed into reward features, {ψi}, via the Bellman equation (model-\nbased case) or reward-shaping [18](model-free).\nThen, a reward function\nthat maximizes the expected return is chosen by enforcing a second-order\noptimality condition based on the policy Hessian [9], [7].\n2.4\nThe options framework\nWe use the options framework[22] which is a probability formulation for tem-\nporally extended actions. A Markovian option, ω ∈Ω, is a triple (Iω, πω, βω),\nwhere Iω is an initiation set, πω is an intra-option policy, and βω : S →[0, 1]\nis a termination function. Following [2], we consider the call-and-return op-\ntion execution model in which the agent selects option ω according to the\npolicy-over-options πΩ(ω|s) and follows the intra-option policy πω(a|s) until\ntermination with probability βω(s). Let πω,θ denote the intra-option policy\nof option ω parametrized by θ and βω,ϑ, the termination function of the\nsame option parametrized by ϑ.\n[2] proposed a method of option discovery based on gradient descent\napplied to the expected discounted return, deﬁned by ρ(Ω, θ, ϑ, s0, ω0) =\nEΩ,θ,ϑ\n\u0002P∞\nt=0 γtRt+1|s0, ω0\n\u0003\n. The objective function used here depends on\npolicy-over-options and the parameters for intra-option policies and termi-\nnation functions.\nIts gradient with respect to these parameters is taken\nthrough the following equations: the option-value function can be written\nas\nQΩ(s, ω) =\nX\na\nπω,θ(a|s)QU(s, ω, a)\nwhere\nQU(s, ω, a) = R(s, a) + γ\nX\ns′\nP(s′|s, a)U(ω, s′)\nis the action-value function for the state-option pair,\nU(ω, s′) = (1 −βω,ϑ(s′))QΩ(s′, ω) + βω,ϑ(s′)VΩ(s′)\nis the option-value function upon arrival, and VΩis the value function over\noptions.\n5\n3\nGeneration of Q-features compatible with the\noptimal policy\nThe ﬁrst step to obtain a reward function as a solution for a given IRL\nproblem is to generate Q-features (base functions of the action-value func-\ntion space compatible with an expert policy) using the gradient of expected\ndiscounted returns. We assume that the parametrized expert intra-option\npolicies, πω,θ, are diﬀerentiable with respect to θ. By the intra-option policy\ngradient theorem [2], the gradient of the expected discounted return with\nrespect to θ vanishes as in the following equation:\n∇θρ =\nX\ns,ω\nµΩ(s, ω|s0, ω0)\nX\na\n∇θπω,θ(a|s)QU(s, ω, a) = 0\n(1)\nwhere µΩ(s, ω|s0, ω0) = P∞\nt=0 γtP(St = s, ωt = ω|s0, ω0) is the occupancy\nmeasure of state-option pairs.\nThe ﬁrst-order optimality condition, ∇θρ = 0, gives a deﬁning equation\nfor Q-features compatible with the optimal policy. It is convenient to deﬁne\na subspace of such compatible Q-features in the Hilbert space of functions\non Ω× S × A. We deﬁne the inner product:\n< f, g >:=\nX\nω,s,a\nf(ω, s, a)µΩ(s, ω|s0, ω0)πω,θ(a|s)g(ω, s, a).\nConsider the subspace, Gπ = {∇θ log πω,θα : α ∈Rk}, of the Hilbert space\nof functions on Ω× S × A with the inner product deﬁned above. Then, the\nspace of Q-features can be represented by the orthogonal complement, G⊥\nπ\nof Gπ.\nParametrization of terminations is expected to allow us to have more\nﬁnely tuned option-wise reward functions in IRL problems. We can impose\nan additional optimality condition on the expected discounted return with\nrespect to parameters of the termination function. Let\nˆρ(Ω, θ, ϑ, s1, ω0) = EΩ,θ,ϑ[\n∞\nX\nt=0\nγtRt+1|ω0, s1]\nbe the expected discounted return with initial condition (s1, ω0). By the\ntermination gradient theorem [2], one has\n∇ϑˆρ = −\nX\ns′,ω\nµΩ(s′, ω|s1, ω0)∇ϑβω,ϑ(s′)AΩ(s′, ω)\n(2)\n6\nwhere AΩis the advantage function over options AΩ(s′, ω) = QΩ(s′, ω) −\nVΩ(s′).\nThe vanishing equation (2) gives a constraint on the space of the Q-\nfeature, ˆG⊥\nπ . For simplicity, set µ1,Ω(s′, ω) = µΩ(s′, ω|s1, ω0). The constraint\nequation for ˆG⊥\nπ is given by\nX\nω,s′\n∇ϑβω,ϑ(s′)µ1,Ω(s′, ω)(QΩ(s′, ω)\n−\nX\nω′\nπΩ(ω′|s′)QΩ(s′, ω′)) = 0\n(3)\nwhere\nQΩ(s, ω) =\nX\na\nπω,θ(a|s)QU(s, ω, a).\nThus, we can combine two linear equations (1), (3) for QU to deﬁne the\nspace of Q-features.\n4\nReward function from Q-functions\nIf two reward functions can produces the same optimal policy, then they\nsatisfy the following([18]):\nR′(s, a) = R(s, a) + γ\nX\ns′\nP(s′|s, a)χ(s′) −χ(s)\nfor some state-dependent potential function χ. This is called reward shaping.\nIf we take χ = V , then\nR′(s, a) = Q(s, a) −V (s) = Q(s, a) −\nX\na′\nπ(a′|s)Q(s, a′)\nBecause the Q-value function depends on the option in the options frame-\nwork, the potential function, χ, also depends on the option. We thus need to\nconsider reward-shaping with regards to the intra-option policy, πω. Then,\nthe reward functions also need to be deﬁned in the intra-option sense. This\nviewpoint is essential to our work and is similar to the approach taken in [8],\nin which Rω, the reward option, was introduced corresponding to the intra-\noption policy, πω. Reward functions, Rω, R′\nω, sharing the same intra-option\npolicy, πω, satisfy\nR′\nω(s, a) = Rω(s, a) + γ\nX\ns′\nP(s′|s, a)χ(s′, ω) −χ(s, ω).\n7\nIf we take χ(s, ω) = U(ω, s), then\nR′\nω(s, a) = Rω(s, a) + γ\nX\ns′\nP(s′|s, a)U(ω, s′) −U(ω, s)\n= QU(s, ω, a) −[(1 −β(s))QΩ(ω, s) + β(s)VΩ(s)]\n= QU(s, ω, a) −\nX\na′\nπω(a|s)QU(s, ω, a) + β(s)AΩ(s, ω)\nThis provides us with a way to generate reward functions from Q-features\nin the options framework.\n5\nReward selection via the second-order optimal-\nity condition\nAmong the linear combinations of reward features constructed in the pre-\nvious section, selecting a linear combination that maximizes ρ(θ) and ˆρ(ϑ)\nis required. For the purpose of optimization, we use the second-order opti-\nmality condition based on the Hessian of ρ(θ) and ˆρ(ϑ).\nConsider a trajectory, τ = ((s0, ω0, a0, b0), · · · , (sT−1, ωT−1, aT−1, bT−1)),\nwith termination indicator bt and terminal state sT. The termination indi-\ncator, bt, is 1 if a previous option terminates at step t, otherwise 0. The\nprobability density of trajectory τ is given by\nPθ,ϑ(τ) = p0(s0)δb0=1πΩ(ω0|s0)\nT−1\nY\nt=1\nP(bt, ωt|ωt−1, st)\nT−1\nY\nt=0\nπωt(at|st)p(st+1|st, at),\nwhere\nP(bt = 1, ωt|ωt−1, st) = βωt−1(st)πΩ(ωt|st)\nP(bt = 0, ωt|ωt−1, st) = (1 −βωt−1(st))δωt=ωt−1.\nWe denote the space of all possible trajectories by T and the γ-discounted\ntrajectory reward by R(τ) = PT(τ)−1\nt=0\nγtR(sτ,t, aτ,t). Then, the objective\nfunction can be rewritten as\nρ(Ω, θ, ϑ, s0, ω0) = E[\n∞\nX\nt=0\nγtRt+1|s0, ω0] =\nZ\nT\nPθ,ϑ(τ)R(τ)dτ.\n8\nIts gradient and Hessian with respect to θ can be expressed as\n∇θρ =\nZ\nT\nPθ,ϑ(τ)∇θ log Pθ,ϑ(τ)R(τ)dτ\nand\nHθρ =\nZ\nT\nPθ,ϑ(τ)(∇θ log Pθ,ϑ(τ)∇θ log Pθ,ϑ(τ)T\n+ Hθ log Pθ,ϑ(τ))R(τ)dτ.\nThe second objective function can be written as\nˆρ = E[\n∞\nX\nt=0\nγtRt+1|ω0, s1] =\nZ\nˆT\nˆPθ,ϑ(ˆτ)R(ˆτ)dˆτ,\nwhere ˆτ is a trajectory beginning with (ω0, s1) with the probability distri-\nbution\nˆPθ,ϑ(ˆτ) = p0(ω0, s1)\nT−1\nY\nt=1\nP(bt, ωt|ωt−1, st)\nT−1\nY\nt=1\nπωt(at|st)p(st+1|st, at).\nThen, its Hessian can be written as\nHϑˆρ =\nZ\nˆT\nˆPθ,ϑ(ˆτ)(∇ϑ log ˆPθ,ϑ(ˆτ)∇ϑ log ˆPθ,ϑ(ˆτ)T\n+ Hϑ log ˆPθ,ϑ(ˆτ))R(ˆτ)dˆτ.\nLet {ψω,i} be the reward features constructed from the previous section.\nRewrite each Hessian as Hθ(ρ) = P\ni wiHθρi, where ρi is the expected return\nwith respect to Pθ,ϑ for the reward function, ψi, and as Hϑ(ˆρ) = P\ni wiHϑˆρi,\nwhere ˆρi is the expected return with respect to ˆPθ,ϑ for the reward function,\nψi. Set trθ,i = tr(Hθ(ρi)) and trϑ,i = tr(Hϑ(ˆρi)) for i = 1, · · · , p.\nTo choose the reward features that achieve local maxima of the objective\nfunctions, we only need to observe whether each Hessian matrix is negative\ndeﬁnite. This can be done by imposing the constraint that the maximum\neigenvalue of the Hessian is negative. In practice, however, the strict nega-\ntive deﬁniteness is rarely met. Analysis for this result is presented in [16].\n9\nAs alternative, we determine the reward weight, w, for the reward function,\nRω = Pp\ni=1 wiψω,i, which yields a negative semi-deﬁnite Hessian with a min-\nimal trace. Also, to relieve a computational burden, we exploit a heuristic\nmethod suggested by [16]: we only choose reward features having negative\ndeﬁnite Hessians, compute the trace of each Hessian, and collect them in\nthe vectors trθ = (trθ,i) and trϑ = (trϑ,i). We determine w by solving\nmin\nw wT trθ,\nand\nmin\nw wT trϑ\ns. t.\n∥w∥2\n2 = 1.\nTypically, multi-objective optimization problems have no single solutions\nthat optimize all objective functions simultaneously. One well-known ap-\nproach to tackling this problem is a linear scalarization. Thus, we consider\nthe following single-objective problem:\nmin\nw λθwT trθ + λϑwT trϑ\ns. t.\n∥w∥2\n2 = 1\nwith positive weights λθ and λϑ. A closed-form solution is computed as w =\n−(λθwT trθ + λϑwT trϑ)/∥λθwT trθ + λϑwT trϑ∥. With a diﬀerent choice of\nscalarization weights, λθ and λϑ, diﬀerent reward functions can be produced.\nIt is natural to set λθ = 1/∥trθ∥and λϑ = 1/∥trϑ∥because the gap between\nthe magnitudes of two trace vectors can be large in practice. Here, we can\nguarantee the obtained solution is Pareto optimal.\n6\nAlgorithm\nWe summarize our algorithm of solving the IRL problem in the options\nframework as follows:\nOur algorithm consists of three phases. In the ﬁrst phase, we obtain basis\nfor Q-features space by solving linear equations. Linear equations consist\nof two parts. The ﬁrst part is deﬁned by the gradient of logarithmic policy\nand the second part is deﬁned by the gradient of option termination. The\nmatrices ΠΩand Π are introduced to carry out computation for the second\npart. The matrix ΠΩis the row repetition of policy over option, πΩ, on\nvisited option and state pair. The matrix Π is a block diagonal where each\nentry is intra-option policy over visited state and action pair for each option.\nIn the second phase, we obtain basis for reward-features using reward\nshaping for option. In the last phase, we select the deﬁnite reward by ap-\nplying Hessian test to two objective functions.\n10\nAlgorithm 1 Option Compatible Reward IRL\nInput: (1)\nExpert’s\ndemo-trajectories\nD\n=\n∪N\ni=1{(ωτi,1, sτi,0, aτi,0), (ωτi,1, sτi,1, aτi,1),\n· · · , (ωτi,T (τi), sτi,T (τi), aτi,T (τi)}, (2) Option Ωθ,ϑ = {ω}, for which expert’s policies, πω,θ,\nand termination functions, βω,ϑ, are parametrized, and a policy πΩover options.\nOutput: Reward function Rω(s, a).\nPhase 1\n1: Estimate µ = µΩ(s, ω|s0, ω0)πω,θ(a|s) for the visited state-option-action triples from the tra-\njectories.\n2: Get the matrices DΩ= diag (µ) and ∇θ log πω(a|s).\n3: Find the basis for the null space of ∇θ log πT\nω DΩ(e.g. using singular value decomposition).\n4: Estimate µ1 = µΩ(s, ω|s1, ω0) for the visited option-state pairs.\n5: Get the matrices diag(µ1), ∇ϑβ, ΠΩ, and Π.\n6: Find the basis for the null space of ∇ϑβT diag(µ1)(I −ΠΩ)Π.\n7: Find the intersection, Φ, of two null spaces.\nPhase 2\n8: Get the set of advantage functions using A = (I −ΠΩ)ΠΦ.\n9: Get the set of reward functions by applying reward shaping Ψ = (I −Π)Φ + βA.\n10: Estimate the Hessian, ˆ\nHθρi and ˆ\nHϑ ˆρi, for each reward feature, ψi, i = 1, . . . p\n11: Discard the reward feature having an indeﬁnite Hessian; switch sign for those having positive\nsemi-deﬁnite Hessian; and compute trθ,i = tr( ˆ\nHθ(ρi)) and trϑ,i = tr( ˆ\nHϑ(ˆρi)) for i = 1, · · · , p\n12: Reward function Rω = Ψwω, wω = −(1/\n√\n2)(trθ/∥trθ∥+ trϑ/∥trϑ∥)\nOur algorithm can be naturally extended to continuous states and action\nspaces. In the continuous domains we use a k-nearest neighbors method to\nextend recovered reward functions to non-visited state-action pairs. Addi-\ntional penalization terms can be included. Details about implementation\nare presented in section 7.2.\n7\nExperiment\n7.1\nTransfer Learning\nWe test our method in a navigation task in the four-rooms domain suggested\nin [22]. Our goal is to verify that our method can transfer knowledge between\ndiﬀerent environments but with similar tasks.\nFirst, a reward function is recovered by applying our method to the\nset of options which is learned in an original environment. The recovered\nreward function will be used to train in modiﬁed environments.\nTo be\nspeciﬁc, an initial goal state is located in the lower right corner, whereas\nthe goal moves to a random location in the lower right room in the modiﬁed\nenvironments. Left two gridworlds in ﬁgure 1 describe each environment in\n11\nOrignal Task\nTra sfer Task\n0\n100\n200\n300\n400\n500\nEpisodes\n−100\n−80\n−60\n−40\n−20\nRetur \nOrigi al Task\nExpert\nReward\nME-IRL\nLPAL\nOCR-IRL\n0\n100\n200\n300\n400\n500\nEpisodes\n−100\n−80\n−60\n−40\n−20\nReturn\nTransfer Task\nReward\nOCR-IRL (α = 0.2)\nOCR-IRL (α = 0.5)\nOCR-IRL (α = 0.8)\nFigure 1: Left two ﬁgures are the grid worlds in our setting, in which red\ncolor indicates goal locations. Right two ﬁgures are the plotting of average\nreturn of Fourrooms domain as a function of the number of episodes used\nin training for original task and transfer task, respectively.\nour setting in which red cells represent possible goal locations to be reached.\nThe initial states are randomly chosen in the upper left room in the both\ncases. The possible actions are movements in four directions, which can be\nfailed with probability 1/3, in which case the agent takes random actions.\nThe default reward is −1 for each step, and 0 when reaching to the goal\ncell. We evaluate our method based on options discovered by [2]. To be\nspeciﬁc, the policy over options and intra-option policies are parametrized\nas Boltzmann policies, and the terminations as sigmoid functions.\nFor comparison, we give weights to the option-wise reward function,\nRω(s, a), based on the policy over options:\nR(s, a) =\nX\nω∈Ω\nπΩ(ω|s)Rω(s, a).\nIt is easy to compare against other IRL algorithms by combining the re-\nwards assigned to each option while the modiﬁed reward R(s, a) maintains\nthe nature of each task. We ﬁrst evaluate OCR-IRL against maximum en-\ntropy IRL (ME-IRL) [23] and linear programming apprenticeship learning\n(LPAL) [1] in the original task. In this case a tabular representation for state\nis used for a reward feature in ME-IRL and LPAL. Figure 1 show the results\nof training a Boltzmann policy using SARSA, coped with the default reward\nfunction and the recovered reward functions by each algorithms. Each re-\nsult is averaged over 20 repetitions, using 50 expert demonstrations which\nare generated by the option discovered. We see that the reward obtained\nby OCR-IRL converges faster to the optimal policy than does the default\n12\n0\n5\n10\n15\n20\nIteration\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nReturn\nε = 0.0\n0\n5\n10\n15\n20\nIteration\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nReturn\nε = 0.1\n0\n5\n10\n15\n20\nIteration\n−0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nReturn\nε = 0.2\nExpert\nReward\nBC\nOCR-IRL\nFigure 2: Average return of Car-on-the-Hill domain as a function of the\nnumber of FQI iterations.\n−0.75 −0.50 −0.25 0.00\n0.25\n0.50\n0.75\n1.00\nPosition\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nVelocity\nε = 0.0\n−0.75 −0.50 −0.25 0.00\n0.25\n0.50\n0.75\n1.00\nPosition\n−1.5\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nVelocity\nε = 0.1\n−1.00 −0.75 −0.50 −0.25 0.00\n0.25\n0.50\n0.75\n1.00\nPosition\n−2\n−1\n0\n1\n2\nVelocity\nε = 0.2\nExpert\nBC\nOCR-IRL\nFigure 3: Trajectories of the expert’s policy, the BC policy, and the policy\ncomputed via FQI with the reward recovered by OCR-IRL.\nreward function and ME-IRL. Despite that the input demonstrations are\nnear-optimal, the reward recovered by our method guarantees learning the\noptimal policy, as shown in ﬁgure 1.\nOn the other hand, the rightmost plot in ﬁgure 1 shows that our reward\nfunction can be used to accelerate learning in the transfer tasks. In order\nto incorporate our reward function to the default reward, we simply use a\nweighted sum of two rewards with diﬀerent weights:\nR(s, a) = (1 −α)Rdefault(s, a) + αROCR−IRL(s, a).\nThe larger the value of α, the more information, including the hierarchical\nstructure of options, learned in the original domain can be delivered. We\nobserve that the case for α = 0.8 outperforms the other cases. The reward\nrecovered by ME-IRL has no eﬀect on transfer.\n13\n7.2\nCar on the Hill\nWe test OCR-IRL in the continuous Car-on-the-Hill domain [4].\nA car\ntraveling on a hill is required to reach the top of the hill. Here, the shape\nof the hill is given by the function, Hill(p):\nHill(p) =\n(\np2 + p\nif p < 0\np\n√\n1+5p2\nif p ≥0.\nThe state space is continuous with dimension two: position p and velocity\nv of the car with p ∈[−1, 1] and v ∈[−3, 3]. The action a ∈[−4, 4] acts on\nthe car’s acceleration. The reward function, R(p, v, a), is deﬁned as:\nR(pt, vt, at) =\n\n\n\n\n\n−1\nif pt+1 < −1 or |vt+1| > 3\n1\nif pt+1 > 1 and |vt+1| ≤3\n0\notherwise.\nThe discount factor, γ, is 0.95, and the initial state is p0 = −0.5 with v0 = 0.\nBecause the car engine is not strong enough, simply accelerating up the\nslope cannot make it to the desired goal. The entire task can be divided\ninto two subtasks: reaching enough speed at the bottom of the valley to\nleverage potential energy (subgoal 1), and driving to the top (subgoal 2).\nTo evaluate our algorithm, we introduce hand-crafted options:\n\n\n\n\n\nIω :\nthe state space S\nπω :\nthe policy for subgoal ω\nβω :\n1 if the agent achieves the subgoal; 0 otherwise\nfor ω ∈{1, 2}.\nIntra-option policy πω is deﬁned by approximating the\ndeterministic intra-option policies, πω,F QI, via the ﬁtted-Q iteration (FQI)\n[4] with the two corresponding small MDPs. We consider noisy intra-option\npolicies in which a random action is selected with probability ǫ:\nπω(a|s) = (1 −ǫ)πω,F QI(a|s) + ǫπrandom(a|s)\nfor each option, ω. Each intra-option policy is parametrized as Gaussian\npolicy πθ,ω(a|s) ∼N(yθ,ω(s), σ2), where σ2 is ﬁxed to be 0.01, and yθ,ω(s)\nis obtained using radial basis functions:\nyθ,ω(s) =\nN\nX\nk=1\nθω,ke−δ∥s−sk∥2,\n14\nwith uniform grids, sk, in the state space. The parameter, θω, is estimated\nusing 20 expert trajectories for each option. Termination probability, βϑ,ω,\nis parametrized as a sigmoid function.\nFor comparison, the task-wise reward function, Rω(s, a), is merged into\none reward, R(s, a), by omitting the option term. This modiﬁcation is pos-\nsible, because the policy-over-options is deterministic in our setting. The\nmerged reward function, R(s, a), can be compared with other reward func-\ntions using a non-hierarchical RL algorithm.\nWe extend the recovered reward function to non-visited state-action pairs\nusing a kernel k-nearest neighbors (KNN) regression with a Gaussian kernel:\nˆR(s, a) =\nP\n(s′,a′)∈KNN((s,a),k,D) K((s, a), (s′, a′))R(s′, a′)\nP\n(s′,a′)∈KNN((s,a),k,D) K((s, a), (s′, a′))\nwhere KNN((s, a), k, D) is the set of the k nearest state-action pairs in the\ndemonstrations, D, and K is a Gaussian kernel over S × A:\nK((s, a), (s′, a′)) = exp\n\u0012\n−1\n2σ2\nS\n∥s −s′∥2 −\n1\n2σ2\nA\n∥a −a′∥2\n\u0013\n.\nThese reward extension is based on [16].\nThe recovered rewards are obtained from expert demonstrations with\ndiﬀerent levels of noise, ǫ. We repeated the evaluation over 10 runs. As\nshown in Figure 2, FQI with the reward function outperforms the original\nreward in terms of convergence speed, regardless of noise level. When ǫ = 0,\nOCR-IRL converges to the optimal policy in only one iteration.\nAs the\nnoise level ǫ increases, BC yields worse performance, whereas OCR-IRL is\nstill robust to noise.\nFigure 3 displays the trajectories of the expert’s policy, the BC policy,\nand the policy computed via FQI with the reward recovered by OCR-IRL.\nWhen ǫ = 0, trajectories are almost overlapping. When ǫ increases, BC\nrequires more steps to reach to the terminal state, and some cannot ﬁnish\nthe task properly. On the other hand, we see that our reward function can\nrecover the optimal policy, even if expert demonstrations are not close to\noptimal.\n15\n8\nConclusion\nWe developed a model-free IRL algorithm for hierarchical tasks modeled in\nthe options framework. Our algorithm, OCR-IRL, extracts reward features\nusing ﬁrst-order optimality conditions based on the gradient for intra-option\npolicies and termination functions. Then, it constructs option-wise reward\nfunctions from the extracted reward spaces using a second-order optimality\ncondition. The recovered reward functions explain the expert’s behavior and\nthe underlying hierarchical structure.\nMost IRL algorithms require hand-crafted reward features, which are\ncrucial to the quality of recovered reward functions. Our algorithm directly\nbuilds the approximate space of the reward function from expert demon-\nstrations. Additionally, unlike other IRL methods, our algorithm does not\nrequire solving a forward problem as an inner step.\nSome heuristic methods were used to solve the multi-objective optimiza-\ntion problem in the reward selection step. We used linear scalarization to\nchange the problem to a single-objective optimization problem, empirically\nﬁnding that this approach resulted in good performances. Generally, de-\npending on the type of option used, one of parameters of intra-option policies\nor termination functions could be more sensitive than the other. Therefore,\nthe choice of weights can make a diﬀerence in the ﬁnal performance.\nOur algorithm was validated in several classical benchmark domains, but\nto apply it to real-world problems, we need to experiment with more complex\nenvironments. More sophisticated options should be used to better explain\nthe complex nature of a hierarchical task, making experiment extensions\neasier.\nAcknowledgments\nThis work was supported in part by the National Research Foundation of\nKorea (NRF) grant funded by the Korea Government (MSIT) under Grant\n2017R1E1A1A03070105, and in part by the Institute for the Information\nand Communications Technology Promotion (IITP) grant funded by the\nKorea Government (MSIP) [Artiﬁcial Intelligence Graduate School Program\n(POSTECH) under Grant 2019-0-01906 and the Information Technology\nResearch Center (ITRC) Support Program under Grant IITP-2018-0-01441].\n16\nReferences\n[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse\nreinforcement learning. In Proceedings of the twenty-ﬁrst international\nconference on Machine learning, page 1. ACM, 2004.\n[2] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic ar-\nchitecture. In Thirty-First AAAI Conference on Artiﬁcial Intelligence,\n2017.\n[3] Jaedeug Choi and Kee-Eung Kim. Nonparametric bayesian inverse rein-\nforcement learning for multiple reward functions. In Advances in Neural\nInformation Processing Systems, pages 305–313, 2012.\n[4] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch\nmode reinforcement learning. Journal of Machine Learning Research,\n6(Apr):503–556, 2005.\n[5] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning:\nDeep inverse optimal control via policy optimization. In International\nConference on Machine Learning, pages 49–58, 2016.\n[6] Roy Fox, Sanjay Krishnan, Ion Stoica, and Ken Goldberg. Multi-level\ndiscovery of deep options. arXiv preprint arXiv:1703.08294, 2017.\n[7] Thomas Furmston and David Barber. A unifying perspective of para-\nmetric policy search methods for markov decision processes.\nIn Ad-\nvances in neural information processing systems, pages 2717–2725,\n2012.\n[8] Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon, David Meger,\nJoelle Pineau, and Doina Precup. Optiongan: Learning joint reward-\npolicy options using generative adversarial inverse reinforcement learn-\ning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[9] Sham M Kakade. A natural policy gradient. In Advances in neural\ninformation processing systems, pages 1531–1538, 2002.\n[10] George Konidaris, Scott Kuindersma, Roderic Grupen, and Andrew\nBarto. Robot learning from demonstration by constructing skill trees.\nThe International Journal of Robotics Research, 31(3):360–375, 2012.\n17\n[11] Sanjay Krishnan, Animesh Garg, Richard Liaw, Lauren Miller, Flo-\nrian T Pokorny, and Ken Goldberg.\nHirl: Hierarchical inverse rein-\nforcement learning for long-horizon tasks with delayed rewards. arXiv\npreprint arXiv:1604.06508, 2016.\n[12] Sanjay Krishnan, Animesh Garg, Richard Liaw, Brijen Thananjeyan,\nLauren Miller, Florian T Pokorny, and Ken Goldberg. Swirl: A sequen-\ntial windowed inverse reinforcement learning algorithm for robot tasks\nwith delayed rewards. The International Journal of Robotics Research,\n38(2-3):126–145, 2019.\n[13] Sanjay Krishnan, Animesh Garg, Sachin Patil, Colin Lea, Gregory\nHager, Pieter Abbeel, and Ken Goldberg. Transition state clustering:\nUnsupervised surgical trajectory segmentation for robot learning. In\nRobotics Research, pages 91–110. Springer, 2018.\n[14] Sergey Levine, Zoran Popovic, and Vladlen Koltun. Nonlinear inverse\nreinforcement learning with gaussian processes. In Advances in Neural\nInformation Processing Systems, pages 19–27, 2011.\n[15] Richard Liaw, Sanjay Krishnan, Animesh Garg, Daniel Crankshaw,\nJoseph E Gonzalez, and Ken Goldberg. Composing meta-policies for au-\ntonomous driving using hierarchical deep reinforcement learning. arXiv\npreprint arXiv:1711.01503, 2017.\n[16] Alberto Maria Metelli, Matteo Pirotta, and Marcello Restelli. Com-\npatible reward inverse reinforcement learning. In Advances in Neural\nInformation Processing Systems, pages 2050–2059, 2017.\n[17] Bernard Michini, Thomas Walsh, Ali-akbar Agha-mohammadi, and\nJonathan P How. Bayesian nonparametric reward learning from demon-\nstration. IEEE transactions on Robotics, 31:369–386, 2015.\n[18] Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance un-\nder reward transformations: Theory and application to reward shaping.\nIn ICML, volume 99, pages 278–287, 1999.\n[19] Pierre Sermanet, Kelvin Xu, and Sergey Levine. Unsupervised percep-\ntual rewards for imitation learning. arXiv preprint arXiv:1612.06699,\n2016.\n18\n[20] Alec Solway, Carlos Diuk, Natalia C´ordova, Debbie Yee, Andrew G\nBarto, Yael Niv, and Matthew M Botvinick. Optimal behavioral hier-\narchy. PLoS computational biology, 10(8):e1003779, 2014.\n[21] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay\nMansour. Policy gradient methods for reinforcement learning with func-\ntion approximation. In Advances in neural information processing sys-\ntems, pages 1057–1063, 2000.\n[22] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps\nand semi-mdps: A framework for temporal abstraction in reinforcement\nlearning. Artiﬁcial intelligence, 112(1-2):181–211, 1999.\n[23] Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey.\nMaximum entropy inverse reinforcement learning.\nIn Twenty-Third\nAAAI Conference on Artiﬁcial Intelligence, volume 8, pages 1433–1438,\n2008.\n19\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-11-07",
  "updated": "2021-01-19"
}