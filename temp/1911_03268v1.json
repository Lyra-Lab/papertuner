{
  "id": "http://arxiv.org/abs/1911.03268v1",
  "title": "Inducing brain-relevant bias in natural language processing models",
  "authors": [
    "Dan Schwartz",
    "Mariya Toneva",
    "Leila Wehbe"
  ],
  "abstract": "Progress in natural language processing (NLP) models that estimate\nrepresentations of word sequences has recently been leveraged to improve the\nunderstanding of language processing in the brain. However, these models have\nnot been specifically designed to capture the way the brain represents language\nmeaning. We hypothesize that fine-tuning these models to predict recordings of\nbrain activity of people reading text will lead to representations that encode\nmore brain-activity-relevant language information. We demonstrate that a\nversion of BERT, a recently introduced and powerful language model, can improve\nthe prediction of brain activity after fine-tuning. We show that the\nrelationship between language and brain activity learned by BERT during this\nfine-tuning transfers across multiple participants. We also show that, for some\nparticipants, the fine-tuned representations learned from both\nmagnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI)\nare better for predicting fMRI than the representations learned from fMRI\nalone, indicating that the learned representations capture\nbrain-activity-relevant information that is not simply an artifact of the\nmodality. While changes to language representations help the model predict\nbrain activity, they also do not harm the model's ability to perform downstream\nNLP tasks. Our findings are notable for research on language understanding in\nthe brain.",
  "text": "Inducing brain-relevant bias\nin natural language processing models\nDan Schwartz\nCarnegie Mellon University\ndrschwar@cs.cmu.edu\nMariya Toneva\nCarnegie Mellon University\nmariya@cmu.edu\nLeila Wehbe\nCarnegie Mellon University\nlwehbe@cmu.edu\nAbstract\nProgress in natural language processing (NLP) models that estimate representations\nof word sequences has recently been leveraged to improve the understanding of\nlanguage processing in the brain. However, these models have not been speciﬁcally\ndesigned to capture the way the brain represents language meaning. We hypothe-\nsize that ﬁne-tuning these models to predict recordings of brain activity of people\nreading text will lead to representations that encode more brain-activity-relevant\nlanguage information. We demonstrate that a version of BERT, a recently intro-\nduced and powerful language model, can improve the prediction of brain activity\nafter ﬁne-tuning. We show that the relationship between language and brain activity\nlearned by BERT during this ﬁne-tuning transfers across multiple participants. We\nalso show that, for some participants, the ﬁne-tuned representations learned from\nboth magnetoencephalography (MEG) and functional magnetic resonance imaging\n(fMRI) are better for predicting fMRI than the representations learned from fMRI\nalone, indicating that the learned representations capture brain-activity-relevant\ninformation that is not simply an artifact of the modality. While changes to lan-\nguage representations help the model predict brain activity, they also do not harm\nthe model’s ability to perform downstream NLP tasks. Our ﬁndings are notable for\nresearch on language understanding in the brain.\n1\nIntroduction\nThe recent successes of self-supervised natural language processing (NLP) models have inspired\nresearchers who study how people process and understand language to look to these NLP models for\nrich representations of language meaning. In these works, researchers present language stimuli to\nparticipants (e.g. reading a chapter of a book word-by-word or listening to a story) while recording\ntheir brain activity with neuroimaging devices (fMRI, MEG, or EEG), and model the recorded brain\nactivity using representations extracted from NLP models for the corresponding text. While this\napproach has opened exciting avenues in understanding the processing of longer word sequences and\ncontext, having NLP models that are speciﬁcally designed to capture the way the brain represents\nlanguage meaning may lead to even more insight. We posit that we can introduce a brain-relevant\nlanguage bias in an NLP model by explicitly training the NLP model to predict language-induced\nbrain recordings.\nIn this study we propose that a pretrained language model — BERT by Devlin et al. (2018) — which\nis then ﬁne-tuned to predict brain activity will modify its language representations to better encode\nthe information that is relevant for the prediction of brain activity. We further propose ﬁne-tuning\nCode available at https://github.com/danrsc/bert_brain_neurips_2019\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:1911.03268v1  [q-bio.NC]  29 Oct 2019\nFigure 1: General approach\nfor ﬁne-tuning BERT using\nfMRI and/or MEG data. A lin-\near layer maps the output to-\nken embeddings from the base\narchitecture to brain activity\nrecordings. Only MEG record-\nings that correspond to content\nwords in the input sequence\nare considered.\nWe include\nthe word length and context-\nindependent log-probability of\neach word when predicting\nMEG. fMRI data are predicted\nfrom the pooled embedding of\nthe sequence, i.e. the [CLS]\ntoken embedding. For more\ndetails of the procedure, see\nsection 3.2.\nsimultaneously from multiple experiment participants and multiple brain activity recording modalities\nto bias towards representations that generalize across people and recording types. We suggest that\nthis ﬁne-tuning can leverage advances in the NLP community while also considering data from brain\nactivity recordings, and thus can lead to advances in our understanding of language processing in the\nbrain.\n2\nRelated Work\nThe relationship between language-related brain activity and computational models of natural lan-\nguage (NLP models) has long been a topic of interest to researchers. Multiple researchers have used\nvector-space representations of words, sentences, and stories taken from off-the-shelf NLP models\nand investigated how these vectors correspond to fMRI or MEG recordings of brain activity (Mitchell\net al., 2008; Murphy et al., 2012; Wehbe et al., 2014b,a; Huth et al., 2016; Jain and Huth, 2018;\nPereira et al., 2018). However, few examples of researchers using brain activity to modify language\nrepresentations exist. Fyshe et al. (2014) builds a non-negative sparse embedding for individual\nwords by constraining the embedding to also predict brain activity well, and Schwartz and Mitchell\n(2019) very recently have published an approach similar to ours for predicting EEG data, but most\napproaches combining NLP models and brain activity do not modify language embeddings to predict\nbrain data. In Schwartz and Mitchell (2019), the authors predict multiple EEG signals on a dataset\nusing a deep network, but they do not investigate whether the model can transfer its representations\nto new experiment participants or other modalities of brain activity recordings.\nBecause fMRI and MEG/EEG have complementary strengths (high spatial resolution vs. high\ntemporal resolution) there exists a lot of interest in devising learning algorithms that combine both\ntypes of data. One way that fMRI and MEG/EEG have been used together is by using fMRI for better\nsource localization of the MEG/EEG signal (He et al., 2018) (source localization refers to inferring\nthe sources in the brain of the MEG/EEG recorded on the head). Palatucci (2011) uses CCA to map\nbetween MEG and fMRI recordings for the same word. Mapping the MEG data to the common\nspace allows the authors to better decode the word identity than with MEG alone. Cichy et al. (2016)\npropose a way of combining fMRI and MEG data of the same stimuli by computing stimuli similarity\nmatrices for different fMRI regions and MEG time points and ﬁnding corresponding regions and time\npoints. Fu et al. (2017) proposes a way to estimate a latent space that is high-dimensional both in\ntime and space from simulated fMRI and MEG activity. However, effectively combining fMRI and\nMEG/EEG remains an open research problem.\n2\n3\nMethods\n3.1\nMEG and fMRI data\nIn this analysis, we use magnetoencephalography (MEG) and functional magnetic resonance imaging\n(fMRI) data recorded from people as they read a chapter from Harry Potter and the Sorcerer’s Stone\nRowling (1999). The MEG and fMRI experiments were shared respectively by the authors of Wehbe\net al. (2014a) at our request and Wehbe et al. (2014b) online1. In both experiments the chapter was\npresented one word at a time, with each word appearing on a screen for 0.5 seconds. The chapter\nincluded 5176 words.\nMEG was recorded from nine experiment participants using an Elekta Neuromag device (data for\none participant had too many artifacts and was excluded, leaving 8 participants). This machine has\n306 sensors distributed into 102 locations on the surface of the participant’s head. The sampling\nfrequency was 1kHz. The Signal Space Separation method (SSS) (Taulu et al., 2004) was used to\nreduce noise, and it was followed by its temporal extension (tSSS) (Taulu and Simola, 2006). The\nsignal in every sensor was downsampled into 25ms non-overlapping time bins, meaning that each\nword in our data is associated with a 306 sensor × 20 time points image.\nThe fMRI data of nine experiment participants were comprised of 3 × 3 × 3mm voxels. Data were\nslice-time and motion corrected using SPM8 (Kay et al., 2008). The data were then detrended in time\nand spatially smoothed with a 3mm full-width-half-max kernel. The brain surface of each subject\nwas reconstructed using Freesurfer (Fischl, 2012), and a thick grey matter mask was obtained to\nselect the voxels with neuronal tissue. For each subject, 50000-60000 voxels were kept after this\nmasking. We use Pycortex (Gao et al., 2015) to handle and plot the fMRI data.\n3.2\nModel architecture\nIn our experiments, we build on the BERT architecture (Devlin et al., 2018), a specialization of a\ntransformer network (Vaswani et al., 2017). Each block of layers in the network applies a transfor-\nmation to its input embeddings by ﬁrst applying self-attention (combining together the embeddings\nwhich are most similar to each other in several latent aspects). These combined embeddings are then\nfurther transformed to produce new features for the next block of layers. We use the PyTorch version\nof the BERT code provided by Hugging Face2 with the pretrained weights provided by Devlin et al.\n(2018). This model includes 12 blocks of layers, and has been trained on the BooksCorpus (Zhu et al.,\n2015) as well as Wikipedia to predict masked words in text and to classify whether two sequences\nof words are consecutive in text or not. Two special tokens are attached to each input sequence in\nthe BERT architecture. The [SEP] token is used to signal the end of a sequence, and the [CLS]\ntoken is trained to be a sequence-level representation of the input using the consecutive-sequence\nclassiﬁcation task. Fine-tuned versions of this pretrained BERT model have achieved state of the art\nperformance in several downstream NLP tasks, including the GLUE benchmark tasks (Wang et al.,\n2018). The recommended procedure for ﬁne-tuning BERT is to add a simple linear layer that maps\nthe output embeddings from the base architecture to a prediction task of interest. With this linear\nlayer included, the model is ﬁne-tuned end-to-end, i.e. all of the parameters of the model change\nduring ﬁne-tuning. For the most part, we follow this recommended procedure in our experiments.\nOne slight modiﬁcation we make is that in addition to using the output layer of the base model, we\nalso concatenate to this output layer the word length and context-independent log-probability of\neach word (see Figure 1). Both of these word properties are known to modulate behavioral data\nand brain activity (Rayner, 1998; Van Petten and Kutas, 1990). When a single word is broken into\nmultiple word-pieces by the BERT tokenizer, we attach this information to the ﬁrst token and use\ndummy values (0 for word length and -20 for the log probability) for the other tokens. We use these\nsame dummy values for the special [CLS] and [SEP] tokens. Because the time-resoluton of fMRI\nimages is too low to resolve single words, we use the pooled output of BERT to predict fMRI data.\nIn the pretrained model, the pooled representation of a sequence is a transformed version of the\nembedding of the [CLS] token, which is passed through a hidden layer and then a tanh function.\nWe ﬁnd empirically that using the [CLS] output embedding directly worked better than using this\ntransformation, so we use the [CLS] output embedding as our pooled embedding.\n1http://www.cs.cmu.edu/~fmri/plosone/\n2https://github.com/huggingface/pytorch-pretrained-BERT/\n3\n3.3\nProcedure\nInput to the model.\nWe are interested in modifying the pretrained BERT model to better capture\nbrain-relevant language information. We approach this by training the model to predict both fMRI\ndata and MEG data, each recorded (at different times from different participants) while experiment\nparticipants read a chapter of the same novel. fMRI records the blood-oxygenation-level dependent\n(BOLD) response, i.e. the relative amount of oxygenated blood in a given area of the brain, which is\na function of how active the neurons are in that area of the brain. However, the BOLD response peaks\n5 to 8 seconds after the activation of neurons in a region (Nishimoto et al., 2011; Wehbe et al., 2014b;\nHuth et al., 2016). Because of this delay, we want a model which predicts brain activity to have access\nto the words that precede the timepoint at which the fMRI image is captured. Therefore, we use the\n20 words (which cover the 10 seconds of time) leading up to each fMRI image as input to our model,\nirrespective of sentence boundaries. In contrast to the fMRI recordings, MEG recordings have much\nhigher time resolution. For each word, we have 20 timepoints from 306 sensors. In our experiments\nwhere MEG data are used, the model makes a prediction for all of these 6120 = 306 × 20 values for\neach word. However, we only train and evaluate the model on content words. We deﬁne a content\nword as any word which is an adjective, adverb, auxiliary verb, noun, pronoun, proper noun, or verb\n(including to-be verbs). If the BERT tokenizer breaks a word into multiple tokens, we attach the\nMEG data to the ﬁrst token for that word. We align the MEG data with all content words in the fMRI\nexamples (i.e. the content words of the 20 words which precede each fMRI image).\nCross-validation.\nThe fMRI data were recorded in four separate runs in the scanner for each\nparticipant. The MEG data were also recorded in four separate runs using the same division of the\nchapter as fMRI. We cross-validate over the fMRI runs. For each fMRI run, we train the model using\nthe examples from the other three runs and use the fourth run to evaluate the model.\nPreprocessing.\nTo preprocess the fMRI data, we exclude the ﬁrst 20 and ﬁnal 15 fMRI images\nfrom each run to avoid warm-up and boundary effects. Words associated with these excluded images\nare also not used for MEG predictions. We linearly detrend the fMRI data within run, and standardize\nthe data within run such that the variance of each voxel is 1 and the mean value of each voxel is 0\nover the examples in the run. The MEG data is also detrended and standardized within fMRI run (i.e.\nwithin cross-validation fold) such that each time-sensor component has mean 0 and variance 1 over\nall of the content words in the run.\n3.4\nModels and experiments\nIn this study, we are interested in demonstrating that by ﬁne-tuning a language model to predict\nbrain activity, we can bias the model to encode brain-relevant language information. We also wish to\nshow that the information the model encodes generalizes across multiple experiment participants,\nand multiple modalities of brain activity recording. For the current work, we compare the models we\ntrain to each other only in terms of how well they predict the fMRI data of the nine fMRI experiment\nparticipants, but in some cases we use MEG data to bias the model in our experiments. In all of\nour models, we use a base learning rate of 5 × 10−5. The learning rate increases linearly from 0 to\n5 × 10−5 during the ﬁrst 10% of the training epochs and then decreases linearly back to 0 during the\nremaining epochs. We use mean squared error as our loss function in all models. We vary the number\nof epochs we use for training our models, based primarily on observations of when the models seem\nto begin to converge or overﬁt, but we match all of the hyperparameters between two models we are\ncomparing. We also seed random initializations and allocate the same model parameters across our\nvariations so that the initializations are consistent between each pair of models we compare.\nVanilla model.\nAs a baseline, for each experiment participant, we add a linear layer to the pretrained\nBERT model and train this linear layer to map from the [CLS] token embedding to the fMRI data of\nthat participant. The pretrained model parameters are frozen during this training, so the embeddings\ndo not change. We refer to this model as the vanilla model. This model is trained for either 10, 20, or\n30 epochs depending on which model we are comparing this to.\nParticipant-transfer model.\nTo investigate whether the relationship between text and brain activity\nlearned by a ﬁne-tuned model transfers across experiment participants, we ﬁrst ﬁne-tune the model\non the participant who had the most predictable brain activity. During this ﬁne-tuning, we train only\n4\nthe linear layer for 2 epochs, followed by 18 epochs of training the entire model. Then, for each other\nexperiment participant, we ﬁx the model parameters, and train a linear layer on top of the model\ntuned towards the ﬁrst participant. These linear-only models are trained for 10 epochs, and compared\nto the vanilla 10 epoch model.\nFine-tuned model.\nTo investigate whether a model ﬁne-tuned to predict each participant’s data\nlearns something beyond the linear mapping in the vanilla model, we ﬁne-tune a model for each\nparticipant. We train only the linear layer of these models for 10 epochs, followed by 20 epochs of\ntraining the entire model.\nMEG-transfer model.\nWe use this model to investigate whether the relationship between text and\nbrain activity learned by a model ﬁne-tuned on MEG data transfers to fMRI data. We ﬁrst ﬁne-tune\nthis model by training it to predict all eight MEG experiment participants’ data (jointly). The MEG\ntraining is done by training only the linear output layer for 10 epochs, followed by 20 epochs of\ntraining the full model. We then take the MEG ﬁne-tuned model and train it to predict each fMRI\nexperiment participant’s data. This training also uses 10 epochs of only training the linear output\nlayer followed by 20 epochs of full ﬁne-tuning.\nFully joint model.\nFinally, we train a model to simultaneously predict all of the MEG experiment\nparticipants’ data and the fMRI experiment participants’ data. We train only the linear output layer of\nthis model for 10 epochs, followed by 50 epochs of training the full model.\nEvaluating model performance for brain prediction using the 20 vs. 20 test.\nWe evaluate the\nquality of brain predictions made by a particular model by using the brain prediction in a classiﬁcation\ntask on held-out data, in a four-fold cross-validation setting. The classiﬁcation task is to predict which\nof two sets of words was being read by the participant (Mitchell et al., 2008; Wehbe et al., 2014b,a).\nWe begin by randomly sampling 20 examples from one of the fMRI runs. For each voxel, we take\nthe true voxel values for these 20 examples and concatenate them together – this will be the target\nfor that voxel. Next, we randomly sample a different set of 20 examples from the same fMRI run.\nWe take the true voxel values for these 20 examples and concatenate them together – this will be our\ndistractor. Next we compute the Euclidean distance between the voxel values predicted by a model\non the target examples and the true voxel values on the target, and we compute the Euclidean distance\nbetween these same predicted voxel values and the true voxel values on the distractor examples. If the\ndistance from the prediction to the target is less than the distance from the prediction to the distractor,\nthen the sample has been accurately classiﬁed. We repeat this sampling procedure 1000 times to get\nan accuracy value for each voxel in the data. We observe that evaluating model performance using\nproportion of variance explained leads to qualitatively similar results (see Figure A4), but we ﬁnd the\nclassiﬁcation metric more intuitive and use it throughout the remainder of the paper.\n4\nResults\nFine-tuned models predict fMRI data better than vanilla BERT.\nThe ﬁrst issue we were inter-\nested in resolving is whether ﬁne-tuning a language model is any better for predicting brain activity\nthan using regression from the pretrained BERT model. To show that it is, we train the ﬁne-tuned\nmodel and compare it to the vanilla model by computing the accuracies of each model on the 20 vs.\n20 classiﬁcation task described in section 3.4. Figure 2 shows the difference in accuracy between the\ntwo models, with the difference computed at a varying number of voxels, starting with those that are\npredicted well by one of the two models and adding in voxels that are less and less well predicted\nby either. Figure 3 shows where on the brain the predictions differ between the two models, giving\nstrong evidence that areas in the brain associated with language processing are predicted better by the\nﬁne-tuned models (Fedorenko and Thompson-Schill, 2014).\nRelationships between text and brain activity generalize across experiment participants.\nThe\nnext issue we are interested in understanding is whether a model that is ﬁne-tuned on one participant\ncan ﬁt a second participant’s brain activity if the model parameters are frozen (so we only do a linear\nregression from the output embeddings of the ﬁne-tuned model to the brain activity of the second\nparticipant). We call this the participant-transfer model. We ﬁne-tune BERT on the experiment\nparticipant with the most predictable brain activity, and then compare that model to vanilla BERT.\n5\n(a) Fine-tuned vs. vanilla\n(b) MEG-transfer vs. fine-tuned\n(c) Participant-transfer vs. vanilla\n(d) Joint vs. vanilla\nFigure 2: Comparison of accuracies of various models. In each quadrant of the ﬁgure above, we\ncompare two models. Voxels are sorted on the x-axis in descending order of the maximum of the\ntwo models’ accuracies in the 20 vs 20 test (described in section 3.4). The colored lines (one per\nparticipant) show differences between the two models’ mean accuracies, where the mean is taken\nover all voxels to the left of each x-coordinate. In (a)-(c) Shaded regions show the standard deviation\nover 100 model initializations – that computation was not tractable in our framework for (d). The\nblack line is the mean over all participants. In (a), (c), and (d), it is clear that the ﬁne-tuned models\nare more accurate in predicting voxel activity than the vanilla model for a large number of voxels. In\n(b), the MEG-transfer model seems to have roughly the same accuracy as a model ﬁne-tuned only on\nfMRI data, but in ﬁgure 3 we see that in language regions the MEG-transfer model appears to be\nmore accurate.\nVoxels are predicted more accurately by the participant-transfer model than by the vanilla model (see\nFigure 2, lower left), indicating that we do get a transfer learning beneﬁt.\nUsing MEG data can improve fMRI predictions.\nIn a third comparison, we investigate whether\na model can beneﬁt from both MEG and fMRI data. We begin with the vanilla BERT model, ﬁne-tune\nit to predict MEG data (we jointly train on eight MEG experiment participants), and then ﬁne-tune the\nresulting model on fMRI data (separate models for each fMRI experiment participant). We see mixed\nresults from this experiment. For some participants, there is a marginal improvement in prediction\naccuracy when MEG data is included compared to when it is not, while for others training ﬁrst on\nMEG data is worse or makes no difference (see Figure 2, upper right). Figure 3 shows however, that\nfor many of the participants, we see improvements in language areas despite the mean difference in\naccuracy being small.\nA single model can be used to predict fMRI activity across multiple experiment participants.\nWe compare the performance of a model trained jointly on all fMRI experiment participants and all\nMEG experiment participants to vanilla BERT (see Figure 2, lower right). We don’t ﬁnd that this\nmodel yet outperforms models trained individually for each participant, but it nonetheless outperforms\n6\nModel 2\nModel 1\nAnatomical language regions\nFigure 3: Comparison of accuracies on the 20 vs. 20 classiﬁcation task (described in section 3.4) at a\nvoxel level for all 9 participants we analyzed. Each column shows the inﬂated lateral view of the left\nhemisphere for one experiment participant. Moving from the top to third row, models 1 and 2 are\nrespectively, the vanilla model and the ﬁne-tuned model, the vanilla model and the participant-transfer\nmodel, and the ﬁne-tuned model and MEG-transfer model. The leftmost column is the participant on\nwhom the participant-transfer model is trained. Columns with a grey background indicate participants\nwho are common between the fMRI and MEG experiments. Only voxels which were signiﬁcantly\ndifferent between the two models according to a related-sample t-test and corrected for false discovery\nrate at a .01 level using the Benjamini–Hochberg procedure (Benjamini and Hochberg, 1995) are\nshown. The color-map is set independently for each of the participants and comparisons shown such\nthat the reddest value is at the 95th percentile of the absolute value of the signiﬁcant differences\nand the bluest value is at the negative of this reddest value. We observe that both the ﬁne-tuned\nand participant-transfer models outperform the vanilla model, especially in most regions that are\nconsidered to be part of the language network. As a reference, we show an approximation of the\nlanguage network for each participant in the fourth row. These were approximated using an updated\nversion of the Fedorenko et al. (2010) language functional parcels3, corresponding to areas of high\noverlap of the brain activations of 220 subjects for a “sentences>non-word\" contrast. The parcels\nwere transformed using Pycortex (Gao et al., 2015) to each participant’s native space. The set of\nlanguage parcels therefore serve as a strong prior for the location of the language system in each\nparticipant. Though the differences are much smaller in the third row than in the ﬁrst two, we also see\nbetter performance in language regions when MEG data is included in the training procedure. Even\nin participants where performance is worse overall (e.g. the ﬁfth and sixth columns of the third row),\nvoxels where performance improves appear to be systematically distributed according to language\nprocessing function. Right hemisphere and medial views are available in the appendix.\nvanilla BERT. This demonstrates the feasibility of fully joint training and we think that with the right\nhyperparameters, this model can perform as well as or better than individually trained models.\nNLP tasks are not harmed by ﬁne-tuning.\nWe run two of our models (the MEG transfer model,\nand the fully joint model) on the GLUE benchmark (Wang et al., 2018), and compare the results to\nstandard BERT (Devlin et al., 2018) (see Table 1). These models were chosen because we thought\nthey had the best chance of giving us interesting GLUE results, and they were the only two models\nwe ran GLUE on. Apart from the semantic textual similarity (STS-B) task, all of the other tasks\nare very slightly improved on the development sets after the model has been ﬁne-tuned on brain\nactivity data. The STS-B task results are very slightly worse than the results for standard BERT. The\nﬁne-tuning may or may not be helping the model to perform these NLP tasks, but it clearly does not\nharm performance in these tasks.\nFine-tuning reduces [CLS] token attention to [SEP] token\nWe evaluate how the attention in\nthe model changes after ﬁne-tuning on the brain recordings by contrasting the model attention in\nthe ﬁne-tuned and vanilla models described in section 3.4. We focus on the attention from the\n[CLS] token to other tokens in the sequence because we use the [CLS] token as the pooled output\n3https://evlab.mit.edu/funcloc/download-parcels\n7\nMetric\nVanilla\nMEG\nJoint\nCoLA\n57.29\n57.63\n57.97\nSST-2\n93.00\n93.23\n91.62\nMRPC (Acc.)\n83.82\n83.97\n84.04\nMRPC (F1)\n88.85\n88.93\n88.91\nSTS-B (Pears.)\n89.70\n89.32\n88.60\nSTS-B (Spear.)\n89.37\n88.87\n88.23\nQQP (Acc.)\n90.72\n91.06\n90.87\nQQP (F1)\n87.41\n87.91\n87.69\nMNLI-m\n83.95\n84.26\n84.08\nMNLI-mm\n84.39\n84.65\n85.15\nQNLI\n89.04\n91.73\n91.49\nRTE\n61.01\n65.42\n62.02\nWNLI\n53.52\n53.80\n51.97\nTable 1:\nGLUE benchmark results for the\nGLUE development sets.\nWe compare the re-\nsults of two of our models to the results pub-\nlished by https://github.com/huggingface/\npytorch-pretrained-BERT/ for the pretrained\nBERT model. The model labeled ‘MEG’ is the\nMEG transfer model described in section 3.4. The\nmodel labeled ‘Joint’ is the fully joint model also\ndescribed in section 3.4. For all but one task, at\nleast one of our two models is marginally better\nthan the pretrained model. These results suggest\nthat ﬁne-tuning does not diminish – and possibly\neven enhances – the model’s ability to perform\nNLP tasks.\n2\n4\n6\n8\n10\n12\nlayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\navrg attention over heads\nvanilla CLS->CLS\nvanilla CLS->SEP\nfine-tuned CLS->CLS\nfine-tuned CLS->SEP\nFigure 4: Comparison of attention from the\n[CLS] token to the [CLS] and [SEP] tokens be-\ntween vanilla BERT and the ﬁne-tuned BERT\n(mean and standard error over example presenta-\ntions, attention heads, and different initialization\nruns). The attention from the [CLS] token notice-\nably shifts away from the [SEP] token in layers\n8 and 9.\nrepresentation of the input sequence. We observe that the [CLS] token from the ﬁne-tuned model\nputs less attention on the [SEP] token in layers 8 and 9, when compared to the [CLS] token from the\nvanilla model (see Figure 4). Clark et al. (2019) suggest that attention to the [SEP] token in BERT is\nused as a no-op, when the function of the head is not currently applicable. Our observations that the\nﬁne-tuning reduces [CLS] attention to the [SEP] token can be interpreted in these terms. However,\nfurther analysis is needed to understand whether this reduction in attention is speciﬁcally due to the\ntask of predicting fMRI recordings or generally arises during ﬁne-tuning on any task.\nFine-tuning may change motion-related representations\nIn an effort to understand how the\nrepresentations in BERT change when it is ﬁne-tuned to predict brain activity, we examine the\nprevalence of various features in the examples where prediction accuracy changes the most after\nﬁne-tuning compared to the prevalence of those features in other examples. We score how much the\nprediction accuracy of each example changes after ﬁne-tuning by looking at the percent change in\nEuclidean distance between the prediction and the target for our best participant on a set of voxels\nthat we manually select which are likely to be language-related based on spatial location. We average\nthese percent changes over all runs of the model, which gives us 25 samples per example. We take\nall examples where the absolute value of this average percent change is at least 10% as our set of\nchanged examples, giving us 146 changed examples and leaving 1022 unchanged examples. We\nthen compute the probability that each feature of interest appears on a word in a changed example\nand compare this to the probability that the feature appears on a word in an unchanged example,\nusing bootstrap resampling on the examples with 100 bootstrap-samples to estimate a standard\nerror on these probabilities. The features we evaluate come from judgments done by Wehbe et al.\n(2014b) and are available online4. The sample sizes are relatively small in this analysis and should be\nviewed as preliminary, however, we see that examples containing verbs describing movement and\nimperative language are more prevalent in examples where accuracies change during ﬁne-tuning. See\nthe appendix for further discussion and plots of the analysis.\n4http://www.cs.cmu.edu/~fmri/plosone/\n8\n5\nDiscussion\nThis study aimed to show that it is possible to learn generalizable relationships between text and brain\nactivity by ﬁne-tuning a language model to predict brain activity. We believe that our results provide\nseveral lines of evidence that this hypothesis holds.\nFirst, because a model which is ﬁne-tuned to predict brain activity tends to have higher accuracy than\na model which just computes a regression between standard contextualized-word embeddings and\nbrain activity, the ﬁne-tuning must be changing something about how the model encodes language to\nimprove this prediction accuracy.\nSecond, because the embeddings produced by a model ﬁne-tuned on one experiment participant better\nﬁt a second participant’s brain activity than the embeddings from the vanilla model (as evidenced by\nour participant-transfer experiment), the changes the model makes to how it encodes language during\nﬁne-tuning at least partially generalize to new participants.\nThird, for some participants, when a model is ﬁne-tuned on MEG data, the resulting changes to the\nlanguage-encoding that the model uses beneﬁt subsequent training on fMRI data compared to starting\nwith a vanilla language model. This suggests that the changes to the language representations induced\nby the MEG data are not entirely imaging modality-speciﬁc, and that indeed the model is learning the\nrelationship between language and brain activity as opposed to the relationship between language\nand a brain activity recording modality.\nModels which have been ﬁne-tuned to predict brain activity are no worse at NLP tasks than the\nvanilla BERT model, which suggests that the changes made to how language is represented improve\na model’s ability to predict brain activity without doing harm to how well the representations work\nfor language processing itself. We suggest that this is evidence that the model is learning to encode\nbrain-activity-relevant language information, i.e. that this biases the model to learn representations\nwhich are better correlated to the representations used by people. It is non-trivial to understand exactly\nhow the representations the model uses are modiﬁed, but we investigate this by examining how the\nmodel’s attention mechanism changes, and by looking at which language features are more likely\nto appear on examples that are better predicted after ﬁne-tuning. We believe that a more thorough\ninvestigation into how model representations change when biased by brain activity is a very exciting\ndirection for future work.\nFinally, we show that a model which is jointly trained to predict MEG data from multiple experiment\nparticipants and fMRI data from multiple experiment participants can more accurately predict fMRI\ndata for those participants than a linear regression from a vanilla language model. This demonstrates\nthat a single model can make predictions for all experiment participants – further evidence that the\nchanges to the language representations learned by the ﬁne-tuned model are generalizable. There are\noptimization issues that remain unsolved in jointly training a model, but we believe that ultimately\nit will be a better model for predicting brain activity than models trained on a single experiment\nparticipant or trained in sequence on multiple participants.\n6\nConclusion\nFine-tuning language models to predict brain activity is a new paradigm in learning about human\nlanguage processing. The technique is very adaptable. Because it relies on encoding information from\ntargets of a prediction task into the model parameters, the same model can be applied to prediction\ntasks with different sizes and with varying temporal and spatial resolution. Additionally it provides\nan elegant way to leverage massive data sets in the study of human language processing. To be sure,\nmore research needs to be done on how best to optimize these models to take advantage of multiple\nsources of information about language processing in the brain and on improving training methods for\nthe low signal-to-noise-ratio setting of brain activity recordings. Nonetheless, this study demonstrates\nthe feasibility of biasing language models to learn relationships between text and brain activity. We\nbelieve that this presents an exciting opportunity for researchers who are interested in understanding\nmore about human language processing, and that the methodology opens new and interesting avenues\nof exploration.\n9\nAcknowledgments\nThis work is supported in part by National Institutes of Health grant no. U01NS098969 and in part\nby the National Science Foundation Graduate Research Fellowship under Grant No. DGE1745016.\n7\nAppendix\n7.1\nAdditional views of voxel-level comparisons\nIn section 4, Figure 3 shows a summary of spatial distributions of changes in fMRI prediction\naccuracy after ﬁne-tuning by showing lateral views of the left hemisphere of all nine experiment\nparticipants across three different models. In Figures A1, A2, and A3 we break out the three different\nmodels into separate ﬁgures and include the right hemisphere and medial views for each participant.\n7.2\nModel comparison using proportion of variance explained\nAlthough we believe that the 20 vs. 20 accuracy described in section 3.4 (Mitchell et al., 2008; Wehbe\net al., 2014b,a) gives a more intuitive comparison of models than the proportion of variance explained,\nboth metrics have value. In some ways the proportion of variance explained is more sensitive to\nchanges since the accuracy quantizes the results. Figure A4 shows the same results as Figure 2 from\nsection 4, but in terms of proportion of variance explained rather than 20 vs. 20 accuracy. The results\nare qualitatively similar, but we can even more clearly see the effects of overﬁtting in the models\nas the proportion of variance explained becomes negative when we include all voxels in the mean\ndifference.\n7.3\nPrevalence of story features in the most changed examples\nIn an effort to understand how the representations in BERT change when it is ﬁne-tuned to predict\nbrain activity, we examine the prevalence of various features in the examples where prediction\naccuracy changes the most after ﬁne-tuning compared to the prevalence of those features in other\nexamples. We score how much the prediction accuracy of each example changes after ﬁne-tuning\nby looking at the percent change in Euclidean distance between the prediction and the target for our\nbest participant on a set of voxels that we manually select which are likely to be language-related\nbased on spatial location (see Figure A5). We average these percent changes over all runs of the\nmodel, which gives us 25 samples per example. We take all examples where the absolute value of\nthis average percent change is at least 10% as our set of changed examples, giving us 146 changed\nexamples and leaving 1022 unchanged examples. We then compute the probability that each feature\nof interest appears on a word in a changed example and compare this to the probability that the\nfeature appears on a word in an unchanged example, using bootstrap resampling on the examples\nwith 100 bootstrap-samples to estimate a standard error on these probabilities. The features we\nevaluate come from judgments done by Wehbe et al. (2014b) and are available online5. We examine\nall available features, but here we present only motion labels (Figure A6), emotion labels (Figure\nA7), and part-of-speech labels (Figure A8), as other features were either too sparse to evaluate or did\nnot show any change in distribution. The sample sizes are relatively small in this analysis and should\nbe viewed as preliminary, however, we see that examples containing verbs describing movement and\nimperative language are more prevalent in examples where accuracies change during ﬁne-tuning.\nWe believe the method of ﬁne-tuning a model and evaluating feature distributions among the most\nchanged examples is an exciting direction for future work.\n5http://www.cs.cmu.edu/~fmri/plosone/\n10\nFine-tuned\nVanilla\nFigure A1: Comparison of accuracies on the 20 vs. 20 classiﬁcation task between the ﬁne-tuned\nand vanilla models (described in section 3.4) at a voxel level for all 9 participants we analyzed.\nMoving from left to right across the page, the columns show inﬂated lateral views of the right and\nleft hemisphere followed by inﬂated medial views of the left and right hemisphere respectively. Each\nrow shows one participant. Only voxels which were signiﬁcantly different between the two models\naccording to a related-sample t-test and corrected for false discovery rate at a .01 level using the\nBenjamini–Hochberg procedure (Benjamini and Hochberg, 1995) are shown. The color-map is set\nindependently for each of the participants such that the reddest value is at the 95th percentile of the\nabsolute value of the signiﬁcant differences and the bluest value is at the negative of this reddest value.\nThe ﬁne-tuned model tends to have higher prediction accuracy than the vanilla model in language\nareas.\n11\nParticipant-\ntransfer\nVanilla\nFigure A2: Comparison of accuracies on the 20 vs. 20 classiﬁcation task between the participant-\ntransfer and vanilla models (described in section 3.4) at a voxel level for all 9 participants we analyzed.\nMoving from left to right across the page, the columns show inﬂated lateral views of the right and\nleft hemisphere followed by inﬂated medial views of the left and right hemisphere respectively. Each\nrow shows one participant. Only voxels which were signiﬁcantly different between the two models\naccording to a related-sample t-test and corrected for false discovery rate at a .01 level using the\nBenjamini–Hochberg procedure (Benjamini and Hochberg, 1995) are shown. The color-map is set\nindependently for each of the participants such that the reddest value is at the 95th percentile of the\nabsolute value of the signiﬁcant differences and the bluest value is at the negative of this reddest value.\nLike the ﬁne-tuned model, the participant-transfer model tends to have higher prediction accuracy\nthan the vanilla model in language areas.\n12\nMEG-\ntransfer\nfMRI \nonly\nFigure A3: Comparison of accuracies on the 20 vs. 20 classiﬁcation task between the MEG-transfer\nand fMRI only models (fMRI only is referred to elsewhere as the ﬁne-tuned model, both models are\ndescribed in section 3.4) at a voxel level for all 9 participants we analyzed. Moving from left to right\nacross the page, the columns show inﬂated lateral views of the right and left hemisphere followed by\ninﬂated medial views of the left and right hemisphere respectively. Each row shows one participant.\nRows with a grey background indicate participants whose data were used in both the MEG training\nand the fMRI training (i.e., these participants were scanned in separate sessions in both modalities).\nOnly voxels which were signiﬁcantly different between the two models according to a related-sample\nt-test and corrected for false discovery rate at a .01 level using the Benjamini–Hochberg procedure\n(Benjamini and Hochberg, 1995) are shown. The color-map is set independently for each of the\nparticipants such that the reddest value is at the 95th percentile of the absolute value of the signiﬁcant\ndifferences and the bluest value is at the negative of this reddest value. For most participants,\nprediction of language areas improves with the MEG-transfer model. However, the results are mixed\nand for some participants, prediction in (some) language areas is arguably worse. Nonetheless, the\nresults suggest that the changes to the model learned during training to predict MEG are helpful for\npredicting fMRI data in language areas.\n13\n(a) Fine-tuned vs. vanilla\n(b) MEG-transfer vs. fine-tuned\n(c) Participant-transfer vs. vanilla\n(d) Joint vs. vanilla\nFigure A4: Comparison of accuracies of various models. In each quadrant of the ﬁgure above, we\ncompare two models. Voxels are sorted on the x-axis in descending order of the maximum of the two\nmodels’ proportion of variance explained. The colored lines (one per participant) show differences\nbetween the two models’ mean proportion of variance explained, where the mean is taken over all\nvoxels to the left of each x-coordinate. In (a)-(c) Shaded regions show the standard deviation over\n100 model initializations – that computation was not tractable in our framework for (d). The black\nline is the mean over all participants. In (a), (c), and (d), it is clear that the ﬁne-tuned models are more\naccurate in the prediction of voxels than the vanilla model for a large number of the voxels. In (b),\nthe results are more mixed. The MEG-transfer model seems to have roughly the same proportion of\nvariance explained as a model ﬁne-tuned only on fMRI data, but in Figure A3 we see that in language\nregions, for most participants, the MEG-transfer model appears to be more accurate.\n14\nFigure A5: Voxels used to compute changes in accuracy between the ﬁne-tuned and vanilla models\nfor the feature distribution analysis described in Section 7.3. From left to right in the ﬁgure, are\ninﬂated lateral views of the right and left hemispheres followed by inﬂated medial views of the left\nand right hemispheres. Voxel selections are done manually based on location in the brain with the\ngoal of restricting the accuracy computation to areas that are more likely to be involved in language\nprocessing.\nFigure A6: Prevalence of the motion-related labels on words in examples that are most and least\nchanged in terms of prediction accuracy in language areas. The set of examples that is most and least\nchanged is computed as described in Section 7.3. The most dramatic change in feature distribution\namong all features we examine, motion or otherwise, is the ‘move’ label.\nFigure A7: Prevalence of the emotion-related labels on words in examples that are most and least\nchanged in terms of prediction accuracy in language areas. The set of examples that is most and least\nchanged is computed as described in Section 7.3. There is some indication that representations for\nimperative language is changed during ﬁne-tuning, as indicated by the change in prevalence of the\n‘commanding’ label, but note that the prevalence is low for both the most changed and least changed\nexamples, so this could easily be a sampling effect.\n15\nFigure A8: Prevalence of the part-of-speech labels on words in examples that are most and least\nchanged in terms of prediction accuracy in language areas. The set of examples that is most and least\nchanged is computed as described in Section 7.3. Verbs and determiners seem to be candidates for\nfurther study.\n16\nReferences\nBenjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful\napproach to multiple testing. Journal of the Royal statistical society: series B (Methodological),\n57(1), 289–300.\nCichy, R. M., Pantazis, D., and Oliva, A. (2016). Similarity-based fusion of meg and fmri reveals\nspatio-temporal dynamics in human cortex during visual object recognition. Cerebral Cortex,\n26(8), 3563–3579.\nClark, K., Khandelwal, U., Levy, O., and Manning, C. D. (2019). What does bert look at? an analysis\nof bert’s attention. arXiv preprint arXiv:1906.04341.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805.\nFedorenko, E. and Thompson-Schill, S. L. (2014). Reworking the language network. Trends in\ncognitive sciences, 18(3), 120–126.\nFedorenko, E., Hsieh, P.-J., Nieto-Castañón, A., Whitﬁeld-Gabrieli, S., and Kanwisher, N. (2010).\nNew method for fmri investigations of language: deﬁning rois functionally in individual subjects.\nJournal of neurophysiology, 104(2), 1177–1194.\nFischl, B. (2012). Freesurfer. Neuroimage, 62(2), 774–781.\nFu, X., Huang, K., Stretcu, O., Song, H. A., Papalexakis, E., Talukdar, P., Mitchell, T., Sidiropoulo, N.,\nFaloutsos, C., and Poczos, B. (2017). Brainzoom: High resolution reconstruction from multi-modal\nbrain signals. In Proceedings of the 2017 SIAM International Conference on Data Mining, pages\n216–227. SIAM.\nFyshe, A., Talukdar, P. P., Murphy, B., and Mitchell, T. M. (2014). Interpretable semantic vectors\nfrom a joint model of brain-and text-based meaning. In Proceedings of the 52nd Annual Meeting\nof the Association for Computational Linguistics, volume 1, pages 489–499.\nGao, J. S., Huth, A. G., Lescroart, M. D., and Gallant, J. L. (2015). Pycortex: an interactive surface\nvisualizer for fmri. Frontiers in neuroinformatics, 9, 23.\nHe, B., Sohrabpour, A., Brown, E., and Liu, Z. (2018). Electrophysiological source imaging: a\nnoninvasive window to brain dynamics. Annual review of biomedical engineering, 20, 171–196.\nHuth, A. G., de Heer, W. A., Grifﬁths, T. L., Theunissen, F. E., and Gallant, J. L. (2016). Natural\nspeech reveals the semantic maps that tile human cerebral cortex. Nature, 532(7600), 453–458.\nJain, S. and Huth, A. (2018). Incorporating context into language encoding models for fmri. bioRxiv,\npage 327601.\nKay, K. N., Naselaris, T., Prenger, R. J., and Gallant, J. L. (2008). Identifying natural images from\nhuman brain activity. Nature, 452(7185), 352.\nMitchell, T. M., Shinkareva, S. V., Carlson, A., Chang, K.-M., Malave, V. L., Mason, R. A., and Just,\nM. A. (2008). Predicting human brain activity associated with the meanings of nouns. science,\n320(5880), 1191–1195.\nMurphy, B., Talukdar, P., and Mitchell, T. (2012). Selecting corpus-semantic models for neurolin-\nguistic decoding. In Proceedings of the First Joint Conference on Lexical and Computational\nSemantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Pro-\nceedings of the Sixth International Workshop on Semantic Evaluation, pages 114–123. Association\nfor Computational Linguistics.\nNishimoto, S., Vu, A., Naselaris, T., Benjamini, Y., Yu, B., and Gallant, J. (2011). Reconstructing\nvisual experiences from brain activity evoked by natural movies. Current Biology.\nPalatucci, M. M. (2011). Thought recognition: predicting and decoding brain activity using the\nzero-shot learning model.\n17\nPereira, F., Lou, B., Pritchett, B., Ritter, S., Gershman, S. J., Kanwisher, N., Botvinick, M., and\nFedorenko, E. (2018). Toward a universal decoder of linguistic meaning from brain activation.\nNature communications, 9(1), 963.\nRayner, K. (1998). Eye movements in reading and information processing: 20 years of research.\nPsychological bulletin, 124(3), 372.\nRowling, J. K. (1999). Harry Potter and the Sorcerer’s Stone, volume 1. Scholastic, New York, 1\nedition.\nSchwartz, D. and Mitchell, T. (2019). Understanding language-elicited eeg data by predicting it\nfrom a ﬁne-tuned language model. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 43–57.\nTaulu, S. and Simola, J. (2006). Spatiotemporal signal space separation method for rejecting nearby\ninterference in meg measurements. Physics in Medicine & Biology, 51(7), 1759.\nTaulu, S., Kajola, M., and Simola, J. (2004). Suppression of interference and artifacts by the signal\nspace separation method. Brain topography, 16(4), 269–275.\nVan Petten, C. and Kutas, M. (1990).\nInteractions between sentence context and word\nfrequencyinevent-related brainpotentials. Memory & cognition, 18(4), 380–393.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\nPolosukhin, I. (2017). Attention is all you need. In Advances in neural information processing\nsystems, pages 5998–6008.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018).\nGlue: A\nmulti-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461.\nWehbe, L., Vaswani, A., Knight, K., and Mitchell, T. M. (2014a). Aligning context-based statistical\nmodels of language with brain activity during reading. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP), pages 233–243, Doha, Qatar.\nAssociation for Computational Linguistics.\nWehbe, L., Murphy, B., Talukdar, P., Fyshe, A., Ramdas, A., and Mitchell, T. M. (2014b). Simultane-\nously uncovering the patterns of brain regions involved in different story reading subprocesses.\nPLOS ONE, 9(11): e112575.\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S. (2015).\nAligning books and movies: Towards story-like visual explanations by watching movies and\nreading books. In Proceedings of the IEEE international conference on computer vision, pages\n19–27.\n18\n",
  "categories": [
    "q-bio.NC",
    "cs.CL",
    "cs.LG"
  ],
  "published": "2019-10-29",
  "updated": "2019-10-29"
}