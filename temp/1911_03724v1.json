{
  "id": "http://arxiv.org/abs/1911.03724v1",
  "title": "Error Analysis for Vietnamese Dependency Parsing",
  "authors": [
    "Kiet Van Nguyen",
    "Ngan Luu-Thuy Nguyen"
  ],
  "abstract": "Dependency parsing is needed in different applications of natural language\nprocessing. In this paper, we present a thorough error analysis for dependency\nparsing for the Vietnamese language, using two state-of-the-art parsers:\nMSTParser and MaltParser. The error analysis results provide us insights in\norder to improve the performance of dependency parsing for the Vietnamese\nlanguage.",
  "text": "Error Analysis for Vietnamese Dependency Parsing\nKiet Van Nguyen\nDepartment of Information Science and Engineering\nUniversity of Information Technology\nVietnam National University – Ho Chi Minh City, Vietnam\nEmail: kietnv@uit.edu.vn\nNgan Luu-Thuy Nguyen\nFaculty of Computer Science\nUniversity of Information Technology\nVietnam National University – Ho Chi Minh City, Vietnam\nEmail: ngannlt@uit.edu.vn\nAbstract—Dependency parsing is needed in different applica-\ntions of natural language processing. In this paper, we present\na thorough error analysis for dependency parsing for the Viet-\nnamese language, using two state-of-the-art parsers: MSTParser\nand MaltParser. The error analysis results provide us insights in\norder to improve the performance of dependency parsing for the\nVietnamese language.\nKeywords—Dependency parsing, error analysis, graph-based\nmodel, transition-based model.\nI.\nINTRODUCTION\nDependency parsing is one of the fundamental problems in\nnatural language processing. In dependency parsing, there are\ntwo main approaches: grammar-based and data-driven. Recent\nworks on dependency parsing mainly focus on data-driven\nparsing models.\nThe problem of dependency parsing is described as follows:\nInput: A sentence S consists of n words: S = w0, w1, w2,\n..., wn, where w0 = ROOT.\nOutput: A connected, acyclic, and single-head dependency\ngraph G (V, A), in which:\n•\nV = {0, 1, ..., n} is the vertex set;\n•\nA is the arc set, i.e, (i, j, lk) ∈A represents a\ndependency (arc) from wi to wj with label lk ∈L;\n•\nL = {l1, l2, ..., lL} is a set of permissible arc labels.\nDependency tree structures can be divided into two types:\nprojective and non-projective. A non-projective dependency\ntree contains crossing arcs, while projective dependency trees\ndo not.\nFigure 1 shows an example of parsing results for a Viet-\nnamese sentence Hai kịch bản mới mô tảcuộc sống hiện đại\n(English translation: Two new scripts describe the mordern\nlife.). We can see multiple instances of labeled dependency\nrelations such as the one from the verb mô tả(describe) to\nFigure 1.\nDependency graph for a Vietnamese sentence.\nkịch bản (scripts) with the SUB label indicating that kịch bản\n(scripts) is the head of the syntactic subject of the verb.\nDependency parsing has gained a wide interest in the re-\nsearch community of natural language processing during the\npast decade. Dependency parsing has been successfully em-\nployed for many applications such as information retrieval, text\nsummary, machine translation, and question answering. Large\nand prestigious conferences in the field, including ACL, EACL,\nand COLING, have constantly provided tutorials on dependency\nparsing [3], [11], [15], [16], [17]. In particular, the 2006 and\n2007 CoNLL Shared Tasks [1], [5] led to a boom in study on\ndata-driven dependency parsing on many languages: from 13\nlanguages and 19 systems (CoNLL, 2006) to 19 languages and\n23 systems (CoNLL, 2007).\nState-of-the-art methods on dependency parsing for the\nVietnamese language achieved only less than 80% [2], [8]. To\nunderstand why parsing performance is low and know how to\nimprove it, we carried out a thorough error analysis based on\ntwo state-of-the-art data-driven dependency parsing systems,\nMSTParser and MaltParser.\nII.\nRELATED WORKS\nA. State-of-the-art Dependency Parsing\nA typical data-driven model for dependency parsing is\nshown in Figure 2 which is borrowed from [11]. It includes three\nimportant components: learning algorithm, parsing model, and\nparsing algorithm. Depending on the parsing algorithm, data-\ndriven dependency parsing models can be divided two types:\ngraph-based and transition-based. The combination of graph-\nbased models and transition-based models forms hybrid models.\nIn this work, we focus on the first two approaches: graph-based\nand transition-based.\nGraph-based dependency parsing systems [13] parameterize\nmodels over dependency subgraphs and learn these parameters\nto score correct graphs above incorrect ones for every sentence\nin a training set. The parsing systems try to search the highest-\nscoring dependency graph among the set of all possible depen-\ndency graphs, which can be considered as a global inference\nprocess. MSTParser is a graph-based dependency parser devel-\noped by the group of McDonald et al. since 2006. This system\nis freely available for research purpose1.\nAs described in paper [13], transition-based dependency\nparsing systems parameterize models over transitions from a\nstate to other in an abstract state machine. Parameters in these\nmodels are learned using classification techniques to predict the\narXiv:1911.03724v1  [cs.CL]  9 Nov 2019\nFigure 2.\nA data-driven dependency parsing model [11].\nmost probable transition out of a set of possible transitions given\na state history. MaltParser is a transition-based dependency\nparser which was developed by Nirve et al. This parser is\nfreely available for research purpose2. A comparison of the\ncharacteristics of MSTParser and MaltParser is briefly shown\nin Table I.\nTable I.\nCOMPARING BETWEEN TWO DATA-DRIVEN PARSING SYSTEMS\nCharacteristic\nMST3\nMalt\nInference\nExhaustive\nGreedy\nTraining\nGlobal structure learning\nLocal decision learning\nFeatures\nLocal features\nRich decision history\nFundamental trade-off\nGolbal learning and inference\nRich feature space\nTwo commonly-used evaluation metrics for dependency\nparsing are the unlabeled attachment score (UAS) and the\nlabeled attachment score (LAS) [5]. UAS is the percentage of\ntokens that are correctly assigned to the heads. LAS is the\npercentage of tokens for which a system has predicted the\ncorrect head and dependency type.\nTable II shows the results of the two top performing systems\nin the CoNLL-X shared task, developed by two groups McDon-\nald et al. (2006) and Nirve et al.(2006). This table is borrowed\nfrom [16] for the sake of comparison with the parsing results of\nthe Vietnamese language. For all 13 languages, MSTParser and\nMaltParser achieved in average 80.83% and 80.75% in accuracy\nrespectively, as shown in Table II, which are not significantly\ndiffer (∆= 0.08%). The works [13], [14] characterize the\ndifference in errors made by a global, exhaustive, graph-based\nparsing system (MSTParser) and a local, greedy, transition-\nbased parsing system (MaltParser).\nB. Vietnamese Dependency Treebank\nThe Vietnamese Treebank [10] was developed as part of\nthe national project – Vietnamese Language and Speech Pro-\ncessing. This treebank contains about 10.200 phrase-structure\ntrees (about 220.000 tokens). Vietnamese Dependency Tree-\nbank (VnDT) contains dependency structures transferred from\nVietnamese Treebank following Dat Nguyen’s approach [2].\nThe VnDT treebank contains 33 dependency types. Table III\nshows the distributions of all the dependency types. The pro-\nportion of non-projective structures in VnDT is 4.49%. The\n1http://www.seas.upenn.edu/ strctlrn/MSTParser/MSTParser.html\n2http://www.maltparser.org/\n3In all tables, MSTParser and MaltParser are referred to as MST and Malt,\nrespectively, for short.\nTable II.\nLABELED PARSING ACCURACY FOR TOP SCORING SYSTEMS\nAT CONLL-X\nLanguage\nMST\nMalt\nLanguage\nMST\nMalt\nArabic\n66.91\n66.71\nJapanese\n90.71\n91.65\nBulgarian\n87.57\n87.41\nPortuguese\n86.82\n87.60\nChinese\n85.90\n86.92\nSlovene\n73.44\n70.30\nCzech\n80.18\n78.42\nSpanish\n82.25\n81.29\nDanish\n84.79\n84.77\nSwedish\n82.55\n84.58\nDutch\n79.19\n78.59\nTurkish\n63.19\n65.68\nGerman\n87.34\n85.82\nOverall\n80.83\n80.75\npercentage of sentences with length less than 30 tokens is\n80%. The percentage of sentences with length over 20 tokens\naccounts for 45.61%. It can be seen that the average length of the\nsentences in VnDT is 21.45 tokens, which is longer than most\nof other languages. In 13 languages in Table II, there are only\nthree languages (Arabic, Portuguese, and Spanish) with average\nsentence length over 20 tokens [13].\nTable III.\nPERCENTAGE OF DEPENDENCY TYPES IN VNDT TREEBANK.\nDependency Type\nDescription\nPercentage\nNMOD\nNoun modiﬁer\n19.01\nVMOD\nVerb modiﬁer\n14.81\nPUNCT\nPunctuation\n13.94\n*.OB\nAny type ending by OB including DOB\n(Direct Object), IOB (Indirect Object),\nand POB (Object of a Preposition)\n11.89\nSUB\nSubject\n6.80\nDET\nDeterminer\n6.18\nADV\nAdverb modiﬁer\n5.92\nROOT\nRoot\n4.66\nDEP\nUnclassiﬁed\n3.13\nAMOD\nAdjective modiﬁer\n2.35\nCOORD\nCoordination\n1.88\nCONJ\nConjunction\n1.86\nX.*\nAny dependency type starting with X\n0.28\nPMOD\nPrepositional modiﬁer\n0.24\nO.F.Tags\nO.F.Tags refers to other grammatical\nfunction\ntags\nas\ndependency\ntypes.\nThere are LOC (Location), TMP (Tem-\nporal), PRP (Purpose), MNR (Manner),\nPRD (Predicate), etc.\n7.05\nC. Dependency Parsing for the Vietnamese Language\nThere are only a few studies on dependency parsing for\nthe Vietnamese language. The works [2], [8] built a depen-\ndency treebank for the Vietnamese language by converting the\nVietnamese Treebank from phrase structures to dependency\nstructures. The highest accuracies of MSTParser and MaltParser\ntrained on this treebank are 71.66% (LAS) and 70.49% (LAS)\nrespectively [2]. We can see that the parsing results reported\nare still much lower than the accuracies of most of languages in\nTable II. This is a challenge for Vietnamese dependency parsing\nto achieve higher performance in future. However, the authors\ndid not report a detailed error analysis for further studies.\nIII.\nERROR-ANALYSIS METHOD\nWe characterize parsing errors by the linguistic and struc-\ntural properties of the dependency graph. The results of our\nanalysis experiments are reported in accuracy, precision, and\nrecall following the labeled scoring scheme. The process of\nanalyzing parsing errors has 2 steps:\n•\nStep 1: MSTParser and MaltParser are trained and\nevaluated using the n-fold cross validation scheme.\n•\nStep 2: Error analysis is performed based on three\ntypes of factors: length factors (sentence length, depen-\ndency length), graph factors (distance to root, number\nof modifier siblings, and non-projective arc degree),\nand linguistic factors (dependent part-of-speech and\ndependency type), in a way similar to [13], [14].\nThe followings describe the factors in detail:\n•\nLength of a sentence: The number of words in sen-\ntence. The sentence in Figure 1 has a length of 7.\n•\nLength of a dependency: The distance in arc between\nthe head and the dependent. The length of a depen-\ndency relation from word wi to word wj is equal to\n|i – j|. In Figure 1, the dependency arc from mô tả\n(describe) to kịch bản (scripts) has a dependency length\nof 2.\n•\nDistance to root: For a given arc, this is the number of\narcs in the reverse path from the dependent of the arc\nto the artificial root. For instance, the dependency arc\nfrom Root to mô tả(describe) in Figure 1 has a distance\nto root of 2.\n•\nSiblings: Two dependency arcs (i, j, l) and (i’, j’, l’) are\nconsidered as siblings if they represent modifiers of the\nsame head, i.e, i = i’. In Figure 1, the arcs from the\nword kịch bản (scripts) to the words hai (two) and mới\n(new) are considered as siblings under this definition.\n•\nNon-projective arc degree: The degree of a depen-\ndency arc from word w to word u is defined as the\nnumber of words occurring between w and u that are\nnot descendants of w and modify a word that does not\noccur between w and u [4]. In the example in Figure\n3, the arc from xấu hổ(ashamed) to Tùng (Tung) has a\ndegree of 1.\n•\nPart-of-speech: Part-of-speech of the dependent. We\nfocus on analyzing the major parts-of-speech such as\nnoun, verb, adjective, adjunct, preposition, and con-\njunction.\n•\nDependency type: Label of dependency arcs. We focus\non popular dependency types such as root, subject,\nobject (including direct object and indirect object),\nnoun modifier, verb modifier, adjective modifier, co-\nordination, and conjunction.\nFigure 3.\nA non-projective sentence.\n4MaltOptimizer chooses stacklazy parsing algorithm and SVM learning.\nIV.\nPARSING RESULTS\nWe evaluated the parsers using 5-fold cross validation\nscheme with the average fold size of 2400 sentences (about\n43.750 words). We used the built-in feature set of the two\nparsers that includes word features, part of speech features, and\ndependency type features. We did not add any special feature\nfor Vietnamese parsing. For MaltParser, we used the MaltOp-\ntimizer4 [9] in order to choose suitable feature model and\nparameters and the best parsing algorithm for non-projective\nstructures. For MSTParser, we employed the non-projective\nparsing algorithm. In these experiments, we used gold standard\npart-of-speech tags.\nIn general, the accuracy of MSTParser surpassed that of\nMaltParser. For UAS score, the MSTParser obtained an accu-\nracy of 76.58% which is 0.5% higher than that of MaltParser\n(76.08%). For LAS score, MSTParser performed better with\n70.10% accuracy, while MaltParser achieved a score of 69.88%.\nIt should be noted that we included punctuation dependencies in\ncalculation of the scores.\nV.\nERROR ANALYSIS RESULTS\nThe analysis results are measured on the test and parsed\n(predicted) data. We compare our results with those reported\nin the paper [14] in several aspects. Our error anlysis results are\npresented in detail below.\nA. Length Factors\nTable IV shows the accuracy of the two parsers relative to\nsentence length. Difference between the two parsing accuracies\n(∆) is less than 1.0. In general, both of the parsers tend to\nhave higher accuracies for shorter sentences. Similar to other\nlanguages, MaltParser tends to perform better on short sen-\ntences (sentence length of 11–20 accounts for 35.39%). This is\nbecause the greedy inference algorithm employed by MaltParser\nmakes fewer parsing decisions. As a result, the likelihood of\nerror propagation is reduced when parsing short sentences. The\nrich feature representation also increases the performance of\nMaltParser.\nTable IV.\nACCURACY RELATIVE TO SENTENCE LENGTH.\nSentence Length\nPercentage\nMST\nMalt\n∆\n1–10\n19.00\n79.73\n79.47\n0.26\n11–20\n35.39\n73.23\n73.35\n-0.12\n21–30\n25.61\n70.02\n69.93\n0.09\n31–40\n12.17\n67.43\n67.33\n0.10\n41-50\n4.94\n65.77\n65.94\n-0.17\n>50\n2.89\n64.74\n63.89\n0.85\nIn Vietnamese, short dependencies are often noun modi-\nfiers, prepositional objects, adjective modifiers and direct ob-\njects. Long sentences contain increasingly complex syntactic\nstructures, resulting in long dependencies which often contain\nprepositions, conjunctions, or multiple clauses (see the column\nof average dependency lengths in Table X). These characteris-\ntics affect the accuracy of the parser.\nTable V measures the precision and recall of each parser\nrelative to dependency length in predicted and gold standard\ndependency graphs. Precision measures the percentage of pre-\ndicted arcs of length d that were correct. Recall presents the\npercentage of gold standard arcs of length d that were predicted\ncorrectly. We can see that both parsers tend to have higher\nprecision for shorter dependency lengths. MSTParser is more\nprecise than MatlParser for shorter dependency length (length\nof 1 and 2 accounts for 67.97%), especially dependency length\nof 1 with ∆= 1.15. MaltParser is far more precise for longer\ndependency arcs (dependency length >2) except dependency\nlength of 12. For the recall measure, MSTParser is better than\nMaltParser for longer dependency lengths. These observations\nare contrary to those of the analysis of the other languages.\nTheoretically, MSTParser should not perform better or\nworse for arcs of any length [14]. However, it can be seen that\nthe number of dependency arcs with a length larger than 2 is\nsmaller than the number with length less than or equal to 2. The\nrich feature space employed by the MaltParser can efficiently\nreduced the phenomenon of error propagation, which could be\nthe reason why the overall accuracies of the two parsers are\nnearly identical.\nTable V.\nDEPENDENCY ARC PRECISION/RECALL RELATIVE TO\nPREDICTED/GOLD DEPENDENCY LENGTH.\nLength\nPercentage\nPrecision\nRecall\nMST\nMalt\n∆\nMST\nMalt\n∆\n1\n50.18\n85.13\n83.98\n1.15\n81.91\n82.07\n-0.16\n2\n17.79\n67.01\n66.75\n0.26\n65.38\n66.56\n-1.18\n3\n8.14\n56.20\n56.66\n-0.46\n56.78\n58.81\n-2.03\n4\n4.75\n49.53\n50.05\n-0.52\n51.73\n52.71\n-0.98\n5\n3.37\n47.05\n48.17\n-1.12\n49.76\n50.14\n-0.38\n6\n2.46\n42.69\n44.86\n-2.17\n48.16\n47.83\n0.33\n7\n1.86\n43.97\n45.45\n-1.48\n47.61\n46.23\n1.38\n8\n1.53\n42.94\n43.52\n-0.58\n46.82\n45.52\n1.30\n9\n1.25\n40.34\n43.06\n-2.72\n46.17\n45.07\n1.10\n10\n1.05\n38.46\n42.13\n-3.67\n42.57\n43.84\n-1.27\n11\n0.90\n42.69\n43.24\n-0.55\n45.45\n45.04\n0.41\n12\n0.77\n44.09\n42.96\n1.13\n47.66\n44.53\n3.13\n13\n0.69\n45.35\n48.20\n-2.85\n50.91\n49.76\n1.15\n14\n0.57\n42.62\n45.62\n-3.00\n47.29\n46.94\n0.35\n15\n0.50\n45.59\n47.86\n-2.27\n52.25\n47.49\n4.76\n≥16\n4.19\n47.72\n53.34\n-5.62\n59.75\n54.66\n5.09\nB. Graph Factors\nTable VI shows the precisions and recalls of dependency\narcs relative to distance to root. Precision is the percentage of\ndependency arcs in the predicted graphs at a distance of d that\nis correct. Recall is the percentage of dependency arcs in the\ngold standard graphs at a distance of d that is predicted. As\na result, both parsers have low precision and recall, and tend\nto be more precise for distances of 2 and 3. The figures in\nthis table shows that for arcs close to the root, MSTParser is\nbetter than MaltParser, and the reverse trend is observed for\narcs further away from the root. For MaltParser, dependency\narcs further away from the root are usually constructed early by\nthe parsing algorithm. Words that are not assigned as modifiers\nare automatically linked to the root. Therefore, MaltParser has a\nlow precision for root modifiers, and these results are consistent\nwith their previous analysis results.\nThe second graph property we examine is the sibling of arcs.\nWe want to quantify the local neighborhood of an arc within\na dependency graph. Table VII measures the precisions and\nrecalls of the parsers relative to the number of predicted and\ngold-standard siblings of dependency arcs. In general, both of\nparsers have low precisions and recalls, and tend to be more\nprecise for those arcs with fewer siblings. MSTParser performs\nbetter for dependency arcs that have no siblings (dependency\nrelations containing dependent is leaf), whereas MaltParser\nTable VI.\nDEPENDENCY ARC PRECISION/RECALL RELATIVE TO\nPREDICTED/GOLD DISTANCE TO ROOT.\nDistance\nPercentage\nPrecision\nRecall\nMST\nMalt\n∆\nMST\nMalt\n∆\n1\n15.05\n59.93\n46.62\n13.31\n60.43\n61.55\n-1.12\n2\n37.22\n61.92\n64.05\n-2.13\n61.29\n65.58\n-4.29\n3\n34.31\n61.41\n65.46\n-4.05\n59.05\n61.01\n-1.96\n4\n9.48\n44.56\n56.66\n-12.10\n45.54\n48.86\n-3.32\n5\n2.81\n31.98\n48.25\n-16.27\n36.98\n38.76\n-1.78\n6\n0.88\n20.27\n49.50\n-29.23\n27.19\n37.01\n-9.82\n≥7\n0.24\n11.97\n24.55\n-12.58\n31.36\n29.42\n1.94\ntends to be more accurate for those arcs with more siblings.\nWhen a sentence has more siblings, it generates more long-\ndistance dependencies. These results are consistent with the\nprevious analysis results.\nTable VIII shows the precisions and recalls relative to dif-\nferent arc degrees in predicted and gold standard non-projective\ndependency graphs. MSTParser recognizes more degrees than\nMaltParser. In general, both of the parsers yield low precisions\nand recalls. These results can be explained by two reasons. First,\nthe training and test data sets contain too few non-projective\ndependency structures (from 3% to 5%). Second, a high degree\nof non-projectivity corresponds to longer dependencies which\nare more challenging for the parsers to predict correctly.\nTable VII.\nDEPENDENCY ARC PRECISIONS/RECALLS RELATIVE TO\nPREDICTED/GOLD SIBLINGS. NMS = NUMBER OF MODIFIER SIBLINGS;\nPER = PERCENTAGE.\nNMS\nPER\nPrecision\nRecall\nMST\nMalt\n∆\nMST\nMalt\n∆\n0\n35.77\n80.83\n78.66\n2.17\n77.04\n77.37\n-0.33\n1\n35.77\n44.92\n49.19\n-4.27\n47.04\n50.28\n-3.24\n2\n15.04\n28.50\n36.57\n-8.07\n33.10\n38.57\n-5.47\n3\n5.65\n25.52\n32.50\n-6.98\n28.92\n33.63\n-4.71\n4\n3.09\n22.46\n28.80\n-6.34\n25.29\n29.02\n-3.73\n5\n1.73\n25.16\n31.97\n-6.81\n24.70\n31.46\n-6.76\n6\n1.16\n24.26\n28.64\n-4.38\n20.37\n26.86\n-6.49\n7\n0.76\n22.01\n25.01\n-3.00\n16.33\n23.80\n-7.47\n8\n0.45\n20.80\n22.20\n-1.40\n10.97\n19.86\n-8.89\n9\n0.27\n21.42\n17.53\n3.89\n8.95\n13.49\n-4.54\n≥10\n0.36\n9.04\n9.14\n-0.10\n2.96\n11.18\n-8.22\nTable VIII.\nDEPENDENCY ARC PRECISION/RECALL RELATIVE TO\nPREDICTED/GOLD DEGREE OF NON-PROJECTIVITY.\nDegree\nMST\nMalt\nPrecision\nRecall\nPrecision\nRecall\n0\n5.21\n51.33\n8.99\n2.50\n1\n0.06\n0.91\n0.00\n0.00\n2\n0.25\n0.95\nN/A\nN/A\n3\n0.67\n5.00\nN/A\nN/A\nC. Linguistic Factors\nTable IX shows the accuracies of the two parsers for differ-\nent parts of speech. We measure labeled dependency accuracy\nrelative to the part-of-speech of the modifier word in depen-\ndency relations. As a result, we can see that both of the parsers\nachieve very high performances for adjuncts, quantities, and\ndeterminers (≥91.00%). For dependent parts-of-speech corre-\nsponding to lower accuracies, we see that MaltParser performs\nslightly better for verbs, conjunctions and punctuations (comma,\nperiod, quotation mark), while MSTParser performs better on\nthe other categories including nouns, prepositions, adjectives,\npronouns, and particles. The difference between the accuracies\nof two parsers (∆) varies from 0.1% to 4.5%.\nTable IX.\nDEPENDENCY ARC PRECISION/RECALL RELATIVE TO\nPREDICTED/GOLD DEGREE OF NON-PROJECTIVITY.\nDependent POS\nMST\nMalt\n∆\nNoun\n73.87\n72.31\n1.56\nVerb\n63.33\n64.13\n-0.80\nAdjunct\n92.85\n92.55\n0.30\nPreposition\n52.91\n51.03\n1.88\nAdjective\n69.21\n68.19\n1.02\nPronoun\n79.61\n79.21\n0.40\nConjunction\n47.25\n51.28\n-4.03\nDeterminer\n98.76\n98.88\n-0.12\nQuantity\n91.76\n91.12\n0.64\nParticle\n72.06\n70.17\n1.89\nPunctuation\n62.93\n65.65\n-2.72\nTable XII shows the dependent parts-of-speech and possible\ndependency types for each POS. We can see the correlation be-\ntween the accuracies of dependent part-of-speech and its depen-\ndency types. For instance, MaltParser achieves high accuracies\nfor verbs and conjunctions related to the types which appear\nwith high frequencies: VMOD, NMOD, DEP, and COORD.\nOn the other hand, MSTParser is better on categories such as\nnouns, pronouns, prepositions, and adjectives. Because these\ncategories are related to high-frequency dependency relations\n(SUB, DET, POB, DOB, LOC, and DET).\nTable X displays precisions and recalls for different depen-\ndency types. In addtion, this table also gives us some informa-\ntion such as the percentage of dependency types and the average\nof dependency lengths. MSTParser tends to be more accurate\nfor shorter dependency-lengths with the relations such as SUB,\nDET, DOB, ADV, PMOD, AMOD, LOC, and CONJ, while\nMaltParser has higher precision for longer dependency-lengths\nwith the relations such as PUNCT, DEP, COORD, and PRD.\nThese results is consistent with the previous results in Table\nV. However, we can also observe that some cases reverse the\ngeneral trend, i.e, NMOD, VMOD, and ROOT. In particular,\ndependency relations achieving high accuracies are DET, POB\nand ADV. Because they have short dependency lengths (LDA\n< 1.5) and fixed-word heads. And dependency relation ADV\nprimarily links verb to adverb and appears frequently in training\nset.\nTable X.\nPRECISION/RECALL FOR DIFFERENT DEPENDENCY TYPES.\nDT = DEPENDENCY TYPE; PER = PERCENTAGE; DLA = DEPENDENCY\nLENGTH AVERAGE.\nDT\nPER\nDLA\nPrecision\nRecall\nMST\nMalt\nMST\nMalt\nNMOD\n19.49\n1.83\n78.22\n79.04\n77.07\n75.09\nVMOD\n14.77\n2.58\n59.48\n60.70\n58.17\n58.22\nPUNCT\n14.17\n8.39\n61.32\n64.00\n61.32\n64.34\nSUB\n6.76\n3.57\n66.47\n65.70\n68.66\n67.29\nDET\n6.28\n1.21\n94.54\n93.94\n93.49\n93.77\nDOB\n5.89\n1.63\n76.19\n68.76\n64.38\n64.00\nADV\n5.83\n1.45\n92.80\n92.72\n93.69\n93.61\nPOB\n5.56\n1.27\n96.75\n95.87\n93.81\n93.22\nROOT\n4.69\n5.62\n82.69\n79.92\n82.69\n79.84\nDEP\n3.13\n7.29\n33.47\n41.51\n51.18\n47.83\nAMOD\n2.25\n1.50\n73.51\n72.01\n71.71\n69.12\nLOC\n2.28\n2.59\n52.21\n45.93\n49.54\n50.18\nTMP\n2.14\n5.40\n42.29\n38.99\n46.46\n50.62\nCOORD\n1.88\n5.64\n48.52\n50.94\n43.68\n50.74\nCONJ\n1.87\n2.43\n75.44\n70.32\n67.01\n69.85\nPRP\n1.28\n3.98\n32.26\n32.47\n45.61\n42.28\nMNR\n0.39\n3.84\n25.96\n21.44\n40.71\n40.04\nPRD\n0.32\n5.83\n9.25\n1.00\n16.41\n15.78\nPMOD\n0.24\n4.81\n42.92\n41.03\n46.71\n40.81\nIOB\n0.20\n2.80\n25.69\n27.45\n36.69\n37.95\nWe consider precision and recall for dependents of the root\nnode (mostly verbal predicate, noun predicate, and adjective\npredicate), and for subjects and objects (direct objects and indi-\nrect objects). MSTParser has considerably better precision (and\nbetter recall) for the root, subject, and direct object relations, but\nMaltParser is better for indirect objects. The accuracy of verbal\npredicate (ROOT-Verb) is more precise for MSTParser, shown\nin Table XI. As results of root distance, we seen that MSTParser\nis precise for root modifiers.\nTable XI.\nACCURACY OF ROOT RELATION.\nRoot relation\nPercentage\nAccuracy\nMST\nMalt\nRoot - Verb\n88.80\n86.46\n82.98\nRoot - Noun\n5.80\n56.82\n68.77\nRoot - Adjective\n4.70\n50.92\n45.71\nIn term of COORD and CONJ, MSTParser is better for\nCONJ, but MaltParser has better precision for COORD. We\ncan see that COORD often links a noun to a conjunction with\nlong dependency length (5.64). Linking a conjunction to a\nnoun forms a relation as CONJ with short dependency length\n(2.43). Accuracies of relations related to conjunctions are also\nconsistent with previous results.\nAll experiments in this section show that there is a trade-\noff between global learning and inference in the MSTParser\nand rich feature presentation in the MaltParser. Although the\naccuracy of MaltParser is affected by error propagation, the rich\nfeature presentation employed by the model helps to overcome\nthe problem in many cases. As a result, it performs rather well\nfor long dependencies, whereas MSTParser is more precise for\nshort dependencies. This is the main difference between the\nVietnamese language and other languages (from average of 13\nlanguages in paper [14]) .\nTable XII.\nDEPENDENT POS AND A LIST OF ITS DEPENDENCY TYPES.\nDependent POS\nDependency Type\nAdjunct\nadv (76.45%), amod (12.88%), nmod (7.90%), dep (2.50%),\nothers(0.27%).\nDeterminer\ndet (99.59%), others(0.41%).\nQuantity\ndet (95,32%), nmod (1.45%), others(3.23%).\nVerb\nvmod (50.62%), root (21%), nmod (9.53%), conj (7.19%),\ndep (5.80%), others(5.86%).\nConjunction\ncoord (50.48%), dep (32.39%), nmod (8.34%), vmod\n(5.73%), amod (2.96%), others(0.37%).\nNoun\nnmod (33.17%), dob (16.97%), sub (16.69%), pob (15.64%),\nothers(17.53%).\nPronoun\nsub (31.61%), det (21.92%), nmod (17.69%), pob (11.56%),\ndob (5.89%), vmod (4.17%), tmp (2.59%), others(4.57%).\nPreposition\nloc\n(23.49%),\nnmod\n(22.30%),\nvmod\n(15.34%),\nprp\n(10.85%), pmod (3.58%), mnr (3.21%), dir (3.08%), iob\n(3.00%), amod (2.28%), others(12.87%).\nAdjective\nnmod (49.77%), vmod (24.35%), amod (5.75%), dep\n(4.69%), root (4.52%), prd (3.27%), others(7.65%).\nParticle\nnmod (36.34%), vmod (29.39%), amod (13.98%), dep\n(13.69%), det (6.52%), others(0.08%).\nPunctuation\npunct (100%).\nD. Comparison with Other Languages\nEach language has its own characteristics which lead to\ndifferences in the accuracy relative to the part-of-speech tags as\nwell as the dependency types. Table XIII and Table XIV show\nthat the difference between the parsing accuracies of the Viet-\nnamese language and the average accuracies of 13 languages\n(listed in Table II) relative to the dependency type is significant.\nThe average accuracies of 13 languages in these tables are taken\nfrom paper [13].\nThe parsing results for adverbs for Vietnamese are higher in\ncomparison with the other languages. However for other types\nof parts-of-speech, the accuracies of the Vietnamese parsing\nare considerably lower, especially verbs, adjectives, and con-\njunctions. Roots, subjects and objects in Vietnamese are far\nless precise than in others. One reason for such low accuracy\ncould be that the VnDT treebank contains many long sentences\nwith complex structures (see the statistics in Section II.B).\nInconsistencies in the Vietnamese Treebank may also affect\nas presented in paper [12]. Other reasons may come from the\ndifference between the characteristics of Vietnamese and others,\nwhich requires further effort of the research community.\nTable XIII.\nACCURACY RELATIVE TO DEPENDENT PART-OF-SPECH OF\nVIETNAMESE (VIL) VERSUS AVERAGE OF 13 LANGUAGES (OTHERS).\nDependent POS\nMST\nMalt\nViL\nOthers\n∆\nViL\nOthers\n∆\nVerb\n63.3\n82.6\n-19.3\n63.6\n81.9\n-18.3\nNoun\n73.9\n80.0\n-6.1\n72.3\n80.7\n-8.4\nPronoun\n79.6\n88.4\n-8.8\n79.5\n89.2\n-9.7\nAdjective\n69.2\n89.1\n-19.9\n68.1\n87.9\n-19.8\nAdverb5\n92.9\n78.3\n14.6\n92.6\n77.4\n15.2\nConjunction\n47.3\n73.1\n-25.8\n48.8\n69.8\n-21.0\nTable XIV.\nPRECISION RELATIVE TO DEPENDENCY TYPE OF\nVIETNAMESE (VIL) VERSUS AVERAGE OF 13 LANGUAGES (OTHERS).\nDependency Type\nMST\nMalt\nViL\nOthers\n∆\nViL\nOthers\n∆\nRoot\n82.7\n89.9\n-7.2\n79.0\n84.7\n-5.7\nSubject\n66.5\n79.9\n-13.4\n66.5\n80.3\n-13.8\nObject\n50.9\n76.5\n-25.6\n49.7\n77.2\n-27.5\nVI.\nCONCLUSION AND FUTURE DIRECTIONS\nIn this paper, we have presented a thorough study of dis-\ntinctive error distributions produced by MSTParser and Malt-\nParser for the Vietnamese language. In particular, we provide a\ncomparison with the error analysis of dependency parsing for\notherss. This would be helpful for researchers to create better\nparsing models.\nBased on the analysis results, we suggest some possible di-\nrections for the improvement of data-driven dependency parsing\nfor the Vietnamese language in future:\n1)\nWe can study a way to represent feature models suit-\nable for Vietnamese language. For instance, we can\nfocus on improving the subject (SUB) dependency that\nlinks the main verb to its modifiers. This work leads to\nthe improvement of POS features relative to subjects\nsuch as nouns and verbs.\n2)\nVietnamese dependency parsing can be improved\nby integrating the strength of both graph-based and\ntransition-based models in a similar way as proposed\nby Nirve and McDonald [6].\n3)\nWe can also build ensemble systems as proposed by\nSagae and Lavie (2006) [7]. The error analysis for\na range of linguistic and graph-based factors in this\npaper can help to build the weighing schemes for\nensemble systems.\n5In Vietnamese, an adjunct is often an adverb which used to modify a verb.\nREFERENCES\n[1]\nBuchholz, Sabine, and Erwin Marsi. ”CoNLL-X shared task on multi-\nlingual language processing”. In Proceedings of the 10th Conference on\nComputational Natural Language Learning (CoNLL), pp. 149-164, New\nYork, 2006.\n[2]\nDat Nguyen, Dai Nguyen, Son Pham, Phuong-Thai Nguyen, and Minh\nLe Nguyen. ”From treebank conversion to automatic dependency parsing\nfor Vietnamese”, In Proceedings of 19th International Conference on\nApplication of Natural Language to Information Systems, NLDB’14,\npp.196-207, France, 2014.\n[3]\nJoakim Nirve and Sandra Kubler. ”Tutorial: Dependency Parsing”. In\nProceedings of COLING/ACL 2006, Sydney, Australia, 2006.\n[4]\nJoakim Nivre. ”Constraints on non-projective dependency parsing”, In\nProceedings of the 11th Conference of the European Chapter of the\nAssociation for Computational Linguistics, pp. 73-80, Italy, 2006.\n[5]\nJoakim Nivre, Johan Hall, Sandra Kubler, Ryan McDonald, Jens Nilsson,\nSebastian Riedel, and Deniz Yure. ”The CoNLL 2007 Shared Task on\nDependency Parsing”. In Proceedings of the CoNLL Shared Task Session\nof EMNLP-CoNLL 2007, pp. 915–932, Prague, 2007.\n[6]\nJoakim Nivre and Ryan McDonald. ”Integrating Graph-Based and\nTransition-Based Dependency Parsers”. In Proceedings of the 46th An-\nnual Meeting of the Association for Computational Linguistics: Human\nLanguage Technologies (ACL-08: HLT), pp. 950–958, Ohio, 2008.\n[7]\nKenji Sagae and Alon Lavie. ”Parser Combination by Reparsing”. In\nProceedings of the Human Language Technology Conference of the North\nAmerican Chapter of the ACL, pp. 129–132, New York, 2006.\n[8]\nLuong Nguyen Thi, Linh Ha My, Hung Nguyen Viet, Huyen Nguyen Thi\nMinh, and Phuong Le Hong. ”Building a treebank for Vietnamese de-\npendency parsing”. In Proceedings of the 10th IEEE RIVF International\nConference on Computing and Communication Technologies, Research,\nInnovation, and Vision for the Future, pp.147-151, Vietnam, 2013.\n[9]\nMiguel Ballesteros and Joakim Nivre. ”MaltOptimizer: An Optimization\nTool for MaltParser”. In Proceedings of the System Demonstration\nSession of the Thirteenth Conference of the European Chapter of the\nAssociation for Computational Linguistics (EACL), France, 2012.\n[10]\nNguyen Phuong-Thai, Vu Xuan-Luong, Nguyen Thi-Minh-Huyen,\nNguyen Van-Hiep and Le Hong-Phuong. ”Building a Large Syntactically-\nAnnotated Corpus of Vietnamese”. In Proceedings of the Third Linguistic\nAnnotation Workshop, pp. 182–185, Singapore, 2009.\n[11]\nQin Iris Wang and Yue Zhang. ”Tutorial: Recent Advances in De-\npendency Parsing”. In Proceedings of the 11th Annual Conference of\nthe North American Chapter of the Association for Computational\nLinguistics (NAACL), Los Angeles, 2010.\n[12]\nQuy Nguyen, Ngan Nguyen, and Yusuke Miyao. ”Utilizing State-of-\nthe-art Parsers to Diagnose Problems in Treebank Annotation for a Less\nResourced Language”. Proceedings of the 7th Linguistic Annotation\nWorkshop and Interoperability with Discourse, pp. 19–27, Bulgaria,\n2013.\n[13]\nRyan McDonald and Joakim Nirve. ”Analyzing and Integrating Depen-\ndency Parsers”. Computational Linguistics - MIT Press Journal, vol. 37,\nno. 1, pp. 197-230, 2011.\n[14]\nRyan McDonald and Joakim Nirve. ”Characterizing the Errors of Data-\nDriven Dependency Parsing Models”. In Proceedings of the 2007 Joint\nConference on Empirical Methods in Natural Language Processing and\nComputational Natural Language Learning, pp. 122–131, Prague, 2007.\n[15]\nRyan McDonald and Joakim Nivre. ”Tutorial: Recent Advances in\nDependency Parsing”. In Proceedings of the 14th Conference of the\nEuropean Chapter of the Association for Computational Linguistics\n(EACL), Sweden, 2014.\n[16]\nWenliang Chen, Zhenghua Li, and Min Zhang. ”Tutorial: Dependency\nParsing: Past, Present, and Future”. In Proceedings of the 25th Inter-\nnational Conference on Computational Linguistics (COLING), Dublin,\n2014.\n[17]\nZhenghua Li, Wenliang Chen, and Min Zhang. ”Tutorial: Dependency\nParsing: Past, Present, and Future”. In Proceedings of the 6th Inter-\nnational Joint Conference on Natural Language Processing (IJCNLP),\nJapan, 2013.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2019-11-09",
  "updated": "2019-11-09"
}