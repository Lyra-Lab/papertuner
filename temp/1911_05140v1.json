{
  "id": "http://arxiv.org/abs/1911.05140v1",
  "title": "Unsupervised Medical Image Segmentation with Adversarial Networks: From Edge Diagrams to Segmentation Maps",
  "authors": [
    "Umaseh Sivanesan",
    "Luis H. Braga",
    "Ranil R. Sonnadara",
    "Kiret Dhindsa"
  ],
  "abstract": "We develop and approach to unsupervised semantic medical image segmentation\nthat extends previous work with generative adversarial networks. We use\nexisting edge detection methods to construct simple edge diagrams, train a\ngenerative model to convert them into synthetic medical images, and construct a\ndataset of synthetic images with known segmentations using variations on\nextracted edge diagrams. This synthetic dataset is then used to train a\nsupervised image segmentation model. We test our approach on a clinical dataset\nof kidney ultrasound images and the benchmark ISIC 2018 skin lesion dataset. We\nshow that our unsupervised approach is more accurate than previous unsupervised\nmethods, and performs reasonably compared to supervised image segmentation\nmodels. All code and trained models are available at\nhttps://github.com/kiretd/Unsupervised-MIseg.",
  "text": "Unsupervised Medical Image Segmentation with Adversarial\nNetworks: From Edge Diagrams to Segmentation Maps\nUmaseh Sivanesan\numaseh.sivanesan@medportal.ca\nLuis H. Braga\nbraga@mcmaster.ca\nRanil R. Sonnadara\nVector Institute, Toronto\nranil@mcmaster.ca\nKiret Dhindsa*\nVector Institute, Toronto\ndhindsj@mcmaster.ca\n*Corresponding Author\nDepartment of Surgery, McMaster University\nNovember 14, 2019\nAbstract\nWe develop and approach to unsupervised semantic medical image segmentation that extends previous work with\ngenerative adversarial networks. We use existing edge detection methods to construct simple edge diagrams, train\na generative model to convert them into synthetic medical images, and construct a dataset of synthetic images with\nknown segmentations using variations on extracted edge diagrams. This synthetic dataset is then used to train a\nsupervised image segmentation model. We test our approach on a clinical dataset of kidney ultrasound images and\nthe benchmark ISIC 2018 skin lesion dataset. We show that our unsupervised approach is more accurate than previous\nunsupervised methods, and performs reasonably compared to supervised image segmentation models. All code and\ntrained models are available at https://github.com/kiretd/Unsupervised-MIseg.\n1\nIntroduction\nIn vivo medical imaging is one of the primary technologies available for clinical evaluation, diagnosis, and treatment\nplanning. The physical challenge of imaging internal tissues is reﬂected in the low resolution, low signal-to-noise ratio,\nand high degree of occlusion seen with many common medical imaging technologies. Using medical images to make\naccurate and meaningful clinical decisions requires substantial training and experience combined with a large body\nof medical knowledge. As a result, current medical practice places a signiﬁcant burden on highly trained clinicians\nspecialized in interpreting medical images [46, 55].\nA fundamental step in medical image analysis is to identify a region of interest, i.e.,, segmentation. This typically\nmeans identifying a bounding region that separates an organ or abnormality from other tissue in the image. For human\nreaders, segmentation allows the extraction of clinically important metrics, such as volume, and for the planning of\nradiation therapy or surgical removal. In computer-aided-diagnosis (CAD), organ and tissue segmentation allows\ncomputer vision models to focus their feature extraction or feature learning computation on the clinically relevant\ntissue, allowing for more computationally efﬁcient models that are better able to avoid extraneous information in the\ndata [20]. Manually performing these segmentations is time-consuming, expensive, and subjective, leading to major\nresearch effort in developing algorithms that can efﬁciently perform accurate and reliable semantic segmentation in\nmedical images (i.e.,, segmentation by associating pixels or regions of the image with a classiﬁcation label).\nWe present an approach to organ and tissue segmentation based on the use of a Generative Adversarial Networks\n(GANs) to generate a labelled synthetic training set in the absence of ground truth labels for real medical images.\nWe circumvent the need for labelled real images by generating medical images from simplistic and arbitrary edge\ndiagrams. We then use the synthetic training set to train supervised segmentation models, which are then applied to\nreal images. We evaluate our approach using two datasets: a dataset of ultrasound images for which the task is to\n1\narXiv:1911.05140v1  [eess.IV]  12 Nov 2019\nsegment the kidney, and the ISIC 2018 Skin Lesion Analysis competition dataset, for which the task is to segment skin\nlesions in dermoscopic images.\nThe main contributions of this work are as follows:\n• We demonstrate a novel form of data augmentation by using GANs to generate labelled training data from\nedge diagrams for applications in which we can exploit a common geometry that is inherent to the semantic\nsegmentation task itself.\n• We show that GANs can generate reasonable synthetic medical images with corresponding organ segmentation\nmaps from just edge diagrams.\n• By generating data using edge diagrams, we show that We can obtain accurate and reliable organ segmentation\nin a fully unsupervised way, with the option of semi-supervised training if labelled data are available.\n2\nRelated Work\nTraditional approaches to algorithmic image segmentation were largely unsupervised, i.e.,, they did not rely on ground\ntruth (clinician-supplied) segmentations to train a model. A variety of such methods were developed in previous\ndecades (for example, methods based on edge detection [7], region growing [2], contour modelling [28], and texture\nanalysis [39]); however, these typically relied on built-in constraints about object appearance or differences in contrast\nor intensity between regions of interest and background pixels. Such constraints do not always work well for medi-\ncal images, particularly for imaging modalities that produce lower quality images (e.g.,, ultrasound imaging), or for\nregions of the body where multiple organ and tissue types are imaged together.\nTo overcome the shortcomings of these earlier approaches, modern image segmentation techniques often rely\non supervised learning with deep neural networks and large amounts of labelled training data. These models are\ncapable of performing semantic segmentation, and thus can categorize regions of images based on meaningful labels\nprovided by a clinician. The most common approach of the last few years has been based on convolutional neural\nnetworks (CNNs), which have been widely demonstrated to be successful for many kinds of computer vision tasks\n[59, 64, 17, 37].\nKey developments in semantic segmentation have been based on variations of CNNs. The Fully Convolutional\nNetwork (FCN) omitted the fully connected layers used in standard CNNs, which are used to obtain a pixel-wise\ngrouping label, and instead used deconvolution layers to obtain segmentation probability maps for images [35]. A\nsimilar idea based on encoder-decoder networks was developed by deconvolving VGG16 [50], a CNN pretrained on\nthe ImageNet dataset [13] that is sometimes used as a starting point for speciﬁc medical imaging problems (e.g.,\n[36, 29]). In order to take greater advantage of the spatial correlations between pixels that should be grouped together,\nCNNs have also been combined with Conditional Random Fields (CRFs) [33].\nDifferent forms of these CNN approaches have dominated the ﬁeld of medical imaging segmentation as well\n[40, 66, 32, 16, 6, 11]. In particular, a speciﬁc instance of FCNs, U-net [47], has performed well for a variety of\nmedical imaging segmentation tasks (e.g.,, [9]). Due to its success, it has since been extended in many ways: for 3D\nimages [10], with an attention mechanism [41], with a pretrained VGG11 encoder [25], and so on.\nTwo major limitations reduce the utility of the CNN approaches described above: 1) they are trained explicitly\nto minimize pixel-wise segmentation error and therefore typically require signiﬁcant post-processing of their outputs\nin order to obtain solutions that are spatially contiguous, and 2) they require ground truth segmentations for training,\nwhich can be very difﬁcult and expensive to obtain on the scale that is required for effective deep learning. While some\nrecent methods have been able to address the ﬁrst limitation by training for scalable spatial coherence using patch\nlearning with multi-scale loss functions [21, 27, 44], they do not address the need for manually segmented training\nimages. Here we propose the use of a GAN to create synthetic training data that can be used to train supervised image\nsegmentation models when no labelled training data are available, thus allowing for unsupervised medical image\nsegmentation.\nGANs have been formulated as image-to-image translation architectures that take paired images as input [26],\nand thus have been successfully applied to semantic segmentation by training them on pairs of images with their\ncorresponding ground truth segmentations. This has been done in a fully supervised manner [38, 62, 52, 63, 40] and\nin a semi-supervised or weakly supervised manner [23]. Most interestingly, researchers have taken advantage of the\nfact that GANs, by their nature, can be used to generate synthetic data as a form of data augmentation [49]. Using this\napproach, GANs can be trained with a relatively low number image-segmentation pairs to generate additional training\ndata for a DualGAN semantic segmentation model [53, 19], or a fully supervised model like U-net [49]. However,\n2\nthese approaches, like the previous CNN-based models, are limited by the fact that ground truth segmentations are\nrequired to train the GANs in the ﬁrst place.\nDifferent approaches have been taken to overcome the need for segmentation labels during training. W-net pairs\ntwo U-nets to form a deep auto-encoder that can be used in combination with a CRF algorithm for scene decomposition\n[61]. In contrast, co-segmentation approaches exploit feature similarity for multiple instances of same-class objects\nin an image, which is suitable for certain kinds of segmentation tasks with distinct ROIs [24]. Recent recomposition\napproaches based on generative modelling (e.g., SEIGAN [42]) segment foreground objects by moving them to similar\nbackground images. Perhaps most similar to ours is a very recent approach, ReDO [8], that performs scene decom-\nposition following region-wise composition using a GAN based on the assumption that different objects composing a\nscene would be statisticaly independent with respect to certain properties, such as colour and texture.\nAll of the above approaches assume that the target ROI for segmentation is easily distinguishable from the rest\nof the image along some feature dimensions, such as brightness or colour, and therefore try to deﬁne or learn the\nproperties that distinguish regions of the image. In many medical imaging applications, this is extremely difﬁcult\nto do, as there may not be a set of learnable properties that support the task. In the case of organ segmentation, as\ndemonstrated with the kidney ultrasound dataset presented here, a clinical expert would typically rely heavily on prior\nanatomical knowledge and experience, which provides an expectation of the contours of the kidney in the absence of a\nclear boundary. For this reason, non-expert humans are likely to fail at this particular task (see Figure 1). We overcome\nthis challenge using a generative process to learn an expectation of the shape of the ROI in the data generation phase,\nas described below.\n3\nMethods\n3.1\nOverview of our Approach\nHere we propose a way of extending this previous work to generate synthetic training data using GANs in a fully\nunsupervised way for applications in which there is an expected segmentation geometry that can serve as a prior. It is\nbased on the assumption that there exists a simple template structure that can be exploited to generate simple diagrams\nwith known segmentations, what we call edge diagrams, from which a GAN can generate sufﬁciently realistic (and\nsimilarly challenging) training images. As long as reasonable edge diagrams can be extracted from the original images\nto train the GAN, and new edge diagrams can be constructed using variations on the template structure as the ground\ntruth segmentations, then synthetic training data can be generated with known segmentations.\nOur approach follows a simple recipe. First we generate simple edge diagrams from real unlabelled training images\nusing available computer vision techniques. We use the corresponding image-diagram pairs to train a GAN to produce\nsynthetic medical images from the edge diagrams. We then use a simple algorithm to generate variations of these\nedge diagrams with known ROIs, and use the trained GAN to synthesize new images from these new edge diagrams.\nFinally, we use these new purely synthetic image-segmentation pairs to train a supervised image segmentation model\nthat can be used to identify ROIs in real medical images. The entire approach is illustrated in Figure 2.\n3.2\nDataset 1: Renal Ultrasound Images\nWe use a dataset of renal ultrasound images developed for prenatal hydronephrosis, a congenital kidney disorder\nmarked by excessive and potentially dangerous ﬂuid retention in the kidneys [14]. The dataset consists of 2492 2D\nsagittal kidney ultrasound images from 773 patients across multiple hospital visits. This is a difﬁcult dataset for image\nsegmentation due to poor image quality, unclear contours of the kidneys, and the large variation introduced by different\ndegrees of the kidney disorder called hydronephrosis (see Figure 1). In addition, a major challenge of this dataset is\nthat the two most salient boundaries, the outer ultrasound cone inherent to ultrasound imaging with a probe, and the\ndark inner region of the kidney, which is caused by ﬂuid retention in hydronephrosis, are both misleading with respect\nto segmenting the kidney.\n3.3\nDataset 2: Skin Lesion Segmentation\nWe use the ISIC 2018 Challenge dataset to evaluate our model with respect to Task 1 of the challenge: Lesion Boundary\nSegmentation [12, 58]. By showing that our approach is also successful on this benchmark dataset, we show that the\nmethod is not limited to only one domain and imaging modality.\n3\n(a) Grade 1\n(b) Grade 2\n(c) Grade 3\n(d) Grade 4\nFigure 1: Examples from the kidney ultrasound dataset with different hydronephrosis severity grades, from 1 (low\nseverity) to 4 (severe hydronephrosis).\n3.4\nImage Preprocessing\nWe follow a similar methodology used for preprocessing renal ultrasound imaging for deep learning described in [14].\nWe crop the images to remove white borders, despeckle them to remove speckle noise caused by interference with the\nultrasound probe during imaging [56], and re-scale to 256×256 pixels for consistency. We remove text annotations\nmade by clinicians using the pre-trained Efﬁcient and Accurate Scene Text Detector (EAST) [67]. We then normalize\nthe pixel intensity of each image to be from 0 to 1 after trimming the pixel intensity from the 2nd percentile to the 98th\npercentile of the original pixel intensity across the image. In addition, we enhance the contrast of each image using\nContrast Limited Adaptive Histogram Equalization with a clip limit of 0.03 [45]. Finally, we normalize the images by\nthe mean and standard deviation of the training set during cross-validation. The results of preprocessing can be seen\nin the example given in Figure 2.\nWe perform no preprocessing for the ISIC skin lesion images other than to resize them to 265 × 256 pixels.\n3.5\nCreating Edge Diagrams for Training\n3.5.1\nUltrasound Images\nTo obtain edge diagrams from real medical images, we start with a rough edge map given by a pre-trained edge detector\n[34] that uses richer convolutional features (RCF) with the VGG16 architecture [50], which we then ﬁne-tune using\nnon-maximum suppression with Structured Forests for [15] edge thinning (as recommended by the authors of RCF).\nIn order to simplify the edge map and remove non-zero pixels that do not belong to the ROI, we downscale the image\nto 32×32 pixels, remove any regions with an area smaller than 3 pixels, and skeletonize the image [65].\nSince the edge diagrams are simplistic, synthetic edge diagrams can be generated in a variety of ways (e.g.,, they\ncan be drawn by hand if desired). For the results presented here, we train a Variational Autoencoder (VAE) [31] to learn\na latent space representing edge diagrams obtained from real images. While this model can generate synthetic edge\ndiagrams, it does not directly provide a known ground truth segmentation. We therefore use Otsu’s method [43, 48]\nfor edge detection to extract just the outer proﬁle of the edge diagram, which corresponds to the ultrasound cone (the\nouter proﬁle of ultrasound images produced by the ultrasound probe). We then generate a ground truth segmentation\ninside the cone of the synthetic edge diagram to ensure that we know every pixel belonging to the desired segmentation\nmask.\nTo generate the ground truth segmentation representing the kidney ROI, we compute a random ellipse with a\nrandom origin, rotation, and major and minor axes within the bounds of the 32×32 pixel edge diagram. We draw\nrandomly selected arcs from the ellipse so as to leave gaps in the kidney outline, simulating occlusion of the kidney\nboundary. We then also draw an arc inside the ellipse roughly parallel to the major axis to represent the renal pelvis.\nFinally, we add some noise in the form of random pixels inside the ellipse. Both the extracted and synthetic edge\n4\ndiagrams are rescaled up to 256×256 pixels for training the GAN.\nNote that in many medical imaging applications, the entire process involving the VAE may be skipped and only the\nground truth segmentation is needed (as we do with the ISIC 2018 dataset). We speciﬁcally include the cone and the\nsegmentation in the synthetic edge diagrams for ultrasound images to ensure the GAN generates synthetic ultrasound\nimages with cone proﬁles, thus preventing the later segmentation model from learning to only segment the outer cone.\n3.5.2\nOther Medical Images\nThe same process was used to generate synthetic edge diagrams for the skin lesion images, with two notable excep-\ntions: no cone was created, and random lines, arcs, and smaller ellipses were added to some synthetic edge diagrams\nto mimic the presence of rulers, pen marks, hairs, and other objects that sometimes appeared in the real images.\nIn principle, any method that produces edge diagrams with known segmentations and enough variation can be\nused. The methods described here are included for reproducibility rather than methodological necessity.\n3.6\nGenerative Adversarial Networks\nThe conventional GAN [18] uses the loss function\n(1)\nmin\nθG max\nθD L(θG, θD) = Ex∼PX [log(D(x))] + Ez∼PZ [log(1 −D(G(z)))] ,\nwhere θG and θD are the parameters for generator G and discriminator D, x ∈X is a real image from our set\nof real ultrasound images X with unknown distribution PX, and z is a random vector noise vector drawn from some\ndeﬁned probability distribution PZ (in this case, a Gaussian distribution). Training the GAN involves setting the\ngenerator an discriminator in competition with one another: the generator is trained to minimize the objective function\nby generating images that are indistinguishable from the real training images, and the discriminator is trained to\nmaximize the objective function by learning to distinguish the images synthesized by the generator from real training\nimages. For this work we use the pix2pixHD architecture [60].\n3.6.1\npix2pixHD Architecture\nThis architecture uses two subnetworks to create a coarse-to-ﬁne generator that can upscale image quality during\nimage-to-image translation, and three multiscale discriminators to address the need to discriminate between high\nresolution synthetic images and real images while keeping the network size and memory requirements relatively low.\nTraining the entire network comes with a loss function extended from 1 for multiple discriminators by summing over\nthe discriminators to obtain\n(2)\nmin\nθG\n  \nmax\nθD1,θD3,θD3\n3\nX\nk=1\nL(θG, θDk)\n!\n+ λ\n3\nX\nk=1\nLF M(θG, θDk)\n!\n,\nwhere λ is a parameter used to balance the inﬂuence of each term of the loss function. Here, LF M is the layer-wise\nfeature matching loss that is incorporated to account for the fact that the generator must now model data distributions\nat multiple scales:\nLF M = E(Z,x)\nT\nX\ni=1\n1\nNi\nh\n∥D(i)\nk (z, x) −D(i)\nk (z, G(z)∥1\ni\n,\n(3)\nwhere T is the number of layers and Ni is the number of units in layer i. In this work, we are not upscaling the\nresolution of images, but we ﬁnd pix2pixHD to also be valuable for translating from a simple image (our edge\ndiagrams) to more complex images (medical images).\n3.7\nTraining\n3.7.1\nGAN\nFor our ultrasound images, a trained surgical urologist provided segmentations for 491 images (approximately evenly\nsplit by class; range: 96-100). We reserve those images for evaluation (i.e.,, they are not used to train any model). We\n5\nPreprocessing\nOriginal\nPreprocessed\nRCF\nEdge map\nEdge diagram\nSynthetic Diagram Construction\nVAE Cone    +      Gen Mask   =   Synth. Diagram \nVAE\nGAN (pix2pixHD)\nSegmentation Model (U-net)\nRCF image credit:\nhttp://mmcheng.net/rcfedge/\nPre-trained RCF-VGG16\npix2pixHD image credit:\nWang et al., 2018\nTraining\nGeneration in Test Phase\nU-net image credit:\nRonneberger et al., 2015\nSegmentation Masks\nSynthetic Image Generation\nImage Segmentation\nTrain on Synthetic Images\nTest on Real Images\nSynth. Images\nEdge Diagram Construction\nFigure 2: The proposed unsupervised image segmentation pipeline.\nadditionally remove any training images taken from the same patients that are also represented in the evaluation set to\navoid overﬁtting due to subject-speciﬁc characteristics. In total, we use 918 images to train the GAN with 20% used\nfor validation. From these, we create a synthetic training set of 2000 image-segmentation pairs.\nFor the ISIC 2018 dataset, 2075 images are used for training and 519 are used for evaluation. Using these data, we\ngenerate 3000 synthetic image-segmentation pairs for training and 750 for validation. For both datasets, we generated\nas many images as required until segmentation accuracy on the validation set no longer improved.\nWe train our implementation of pix2pixHD using the same settings given in [60] and choose the parameters\ncorresponding to the epoch that minimizes the Fr´echet Inception Distance (FID) with respect to the validation data\n[22]. This is the 90th epoch for the ultrasound dataset, and the 100th epoch for the skin lesion dataset.\n3.7.2\nVAE\nThe VAE we use to generate ultrasound cones for synthetic edge diagrams is shown in Figure 3. We train the VAE\nover 40 epochs and a batch size of 128 using the adaptive moment estimator (Adam) [30] and the Kullback-Leibler\ndivergence loss.\n3.7.3\nU-Net\nWe use the U-net architecture deﬁned in [47] to train a segmentation model for the ultrasound dataset. However, we\nuse the sum of the pixel-wise binary cross-entropy and the dice coefﬁcient as our loss function. We use Adam for\noptimization with a batch size of 1. Finally, we perform data augmentation with horizontal ﬂips (50% probability) and\nhorizontal and vertical translations of up to 26 pixels (10%).\n3.7.4\nMask-RCNN\nWe use the Mask-RCNN implementation provided here [1] adjusted for the ISIC 2018 dataset. We use anchor sizes\nof 2i, i ∈{3, 4, 6, 7, 8} and 32 training ROIs per image. Other hyperameters were kept as default values. We perform\ndata augmentation with both horizontal and vertical ﬂips (50% probability), rotation of 90◦or 270◦, and a Gaussian\nblur of up to 5 standard deviations.\n6\nD\nE\nFigure 3: VAE model architecture for generating ultrasound cones.\n7\n(a) Successful mask extraction\n(b) Unsuccessful mask extraction\nFigure 4: A successful and unsuccessful example of mask extraction from clinician-provided kidney segmentations.\nFrom left to right, panel 1 shows the original image with the kidney outlined by the clinician, panel 2 shows the\ndifference between panel 1 and the original image from our database without the outline, panel 3 shows the difference\nimage thresholded by pixel value, panel 4 shows the convex hull of the thresholded image in panel 3, and panel 5\nshows the mask obtained by ﬁlling in the convex hull in panel 4.\n3.8\nEvaluation\nWe evaluate our model using three standard metrics: the F1 score, mean intersection over union (mIoU), and pixel-wise\nclassiﬁcation accuracy (pACC).\n3.8.1\nComparison with W-net\nWe train W-net with the soft normalized cut term in the loss function [61]. In addition, we perform the recommended\npost-processing of the W-net generated segmentation maps using a fully-connected CRF for edge recovery, and hier-\narchical image segmentation for contour grouping [4].\n3.8.2\nMask Extraction from Clinician-Provided Kidney Segmentations\nThe clinician-provided segmentations were drawn as imprecise outlines on the ultrasound images (see Figure 4), and\ntherefore could not be used to generate masks in a simple and direct way. We therefore use OpenCV [5] to convert\nthese segmentations to masks.\nFor each clinician-provided segmentation, we ﬁrst compute its difference with the original unsegmented ultrasound\nimage. Since some background noise iss retained in most images, we use an adaptive threshold to convert the difference\nimage to a binary image B(x, y) using the following formula:\nB(x, y) =\n(\n0,\nif S(x, y) > T(x, y)\n1,\notherwise\n(4)\nwhere T(x, y) is the mean in the 25 × 40 pixel neighbourhood around each pixel (x, y) computed from the difference\nimage S(x, y).\nWe then use a border-following algorithm [54] with Teh-Chin chain approximation [57] to identify contours from\nthe binary images. Contours with an area of less than 25 pixels are removed as noise. We compute and ﬁll the\nconvex hull of the remaining contours using the Sklansky algorithm [51]. We use these as the ground truth masks for\nevaluating segmentation performance.\nFollowing this procedure, the masked images are visually inspected compared to the clinician-provided segmenta-\ntions, and those masks which deviate signiﬁcantly from the clinician’s segmentations (e.g., because additional anno-\n8\nReal\nSynthetic\nFigure 5: Real (ﬁrst four columns) and generated (last four columns) kidney ultrasound images.\ntations are added to the segmented images, as seen in Figure 4) are omitted from further analysis. In total, 53 images\nare removed (438 are used for evaluation), and the class distribution remains relatively even (range: 83-93 per class).\n4\nResults\n4.1\nSynthetic Image Generation\nTo illustrate the similarity between real and synthetic images, we show a random sample of real and generated kidney\nultrasound images in Figure 5, and a random sample of real and generated dermoscopic images in Figure 6. Since\nour goal is to generate images that are similar enough for generalizeable training of a segmentation model, our ap-\nproach does not produce state-of-the-art synthetic image generation. Instead, it produces images that have similar\nsegmentation properties.\n4.2\nKidney Segmentation Performance\nIn Figure 7 we show the kidney segmentation masks learned through our fully unsupervised approach (with U-net\nas the segmentation model), compared with a purely supervised U-net and a purely unsupervised W-net. In Table 1\nwe show the corresponding segmentation performance metrics. For the semi-supervised extensions of our approach,\nwe train a U-net using real and synthetic ultrasound images in a standard training protocol (U-net), and we also train\n9\nReal\nSynthetic\nFigure 6: Real (ﬁrst four columns) and generated (last four columns) dermoscopic images.\n10\nFigure 7: Kidney segmentation masks comparing our unsupervised method (blue) to a supervised U-Net (red) and the\nclinician-provided ground truth (green). Top row: images with high agreement between models. Middle row: images\nwith moderate agreement between models. Bottom row: images with poor agreement between models (speciﬁc cases\nwhere the unsupervised approach fails).\na U-net using just the synthetic data followed by supervised ﬁne-tuning with 45 of the real images with clinician\nsegmentations, which were then removed from the evaluation set (U-net+).\n4.3\nSkin Lesion Segmentation Performance\nPerformance metrics on the ISIC 2018 dataset using our unsupervisd approach are shown in Table 2 along with\nresults obtained by the competition winner and current top submission. Here we use the metrics given by the online\nsubmission system, which includes a thresholded mIoU (th-mIoU). This metric sets all per-image IoU scores that are\nless than 0.65 to 0 before computing the mean IoU. Semi-supervised results are not available because the ISIC 2018\ntest submission page has been removed whie preparing this manuscript, and the test set is not currently available.\nExamples of the output masks on validation images are shown in Figure 8.\n5\nDiscussion\nWe present an unsupervised approach to semantic medical image segmentation that takes advantage of recent advances\nin image synthesis and generative modelling by making assumptions about the common geometry inherent to an object\nof interest. This method performs better than some previous unsupervised methods that ﬁt the problem deﬁnition (e.g.,\nW-net), or for which results are available (e.g., the CNN-based approach in [3]). For example, W-net performs poorly\non the kidney segmentation task because it only identiﬁes the ultrasound cone itself, rather than the kidney. We\nalso show that our approach performs nearly as well as supervised methods for most images. Importantly, we show\n11\nTable 1: Performance metrics for ultrasound kidney segmentation.\nModel\nF1\nSpeciﬁcity\nSensitivity\nmIoU\npACC\nUnsup.\nOurs (U-net)\n0.81 (0.09)\n0.92 (0.05)\n0.87 (0.14)\n0.69 (0.12)\n0.90 (0.05)\nW-net\n0.46 (0.10)\n0.20 (0.05)\n0.98 (0.02)\n0.41 (0.07)\n0.41 (0.07)\nSemi-Sup.\nOurs (U-net)\n0.87 (0.11)\n0.97 (0.04)\n0.86 (0.13)\n0.78 (0.13)\n0.93 (0.05)\nOurs (U-net+)\n0.88 (0.08)\n0.97 (0.03)\n0.88 (0.09)\n0.80 (0.11)\n0.94 (0.04)\nSup.\nU-net\n0.91 (0.09)\n0.97 (0.04)\n0.90 (0.10)\n0.84 (0.10)\n0.95 (0.03)\nTable 2: Performance metrics for ISIC 2018 skin lesion boundary segmentation.\nModel\nF1\nSpeciﬁcity\nSensitivity\nmIoU\nth-mIoU\npACC\nUnsup.\nOurs (Mask-RCNN)\n0.830\n0.947\n0.835\n0.753\n0.683\n0.904\nAli et al. 2019 [3]\n0.543\nn/a\nn/a\n0.440\nn/a\nn/a\nSup.\nMask-RCNN\n0.882\n0.950\n0.922\n0.811\n0.763\n0.936\nWinner\n0.898\n0.963\n0.906\n0.838\n0.802\n0.942\nCurrent Top\n0.915\n0.941\n0.956\n0.852\n0.836\n0.954\nFigure 8: Skin lesion segmentation masks comparing our unsupervised method (blue) to a supervised Mask-RCNN\n(red). Top row: images with high agreement between models. Middle row: images with moderate agreement between\nmodels. Bottom row: images with poor agreement between models (speciﬁc cases where the unsupervised approach\nfails). Ground truth segmentation not available for ISIC 2018 test images.\n12\nthat with just a few training examples for supervised ﬁne-tuning (here, only 10% of the data used for the supervised\nmodels), we approach the segmentation performance of purely supervised models.\nOur method tends towards identifying larger ROIs that contain the desired ROI, which results in high speciﬁcity\n(0.92 and 0.947 for the kidney dataset and ISIC 2018 respectively) and only moderate sensitivity (0.87 and 0.835).\nFor both datasets, the model fails for a small subset of the images. In the case of the kidney dataset, we ﬁnd no clear\npattern to explain the failed images. However, in the case of ISIC 2018 images, the unsupervised model does poorly\nwith images that contain a lens or ﬁlm placed on top of the skin lesion (in these cases, the model incorrectly segments\nthe lens instead of the skin lesion underneath).\nInterestingly, even though we construct edge diagrams based on smooth and convex shapes for image synthesis,\nthe resulting segmentation models are able to ﬁt non-smooth and non-convex boundaries. It is possible that alternative\nmethods for generating edge diagrams with greater complexity may lead to a more ﬂexible model that can adapt to\nmore complex geometries. We are currently exploring the utility of this method in segmenting organs with more\ncomplex geometries, segmenting multiple objects per image, and performing 3D segmentation. We are also currently\nexploring adaptations of our approach that make it more end-to-end, e.g., by using multiple GANs.\nReferences\n[1] Waleed Abdulla. Mask r-cnn for object detection and instance segmentation on keras and tensorﬂow. https://github.\ncom/matterport/Mask_RCNN, 2017. 6\n[2] Rolf Adams and Leanne Bischof. Seeded region growing. IEEE Transactions on pattern analysis and machine intelligence,\n16(6):641–647, 1994. 2\n[3] Abder-Rahman Ali, Jingpeng Li, and Thomas Trappenberg. Supervised versus unsupervised deep learning based methods for\nskin lesion segmentation in dermoscopy images. In Canadian Conference on Artiﬁcial Intelligence, pages 373–379. Springer,\n2019. 11, 12\n[4] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmenta-\ntion. IEEE Trans. Pattern Anal. Mach. Intell., 33(5):898–916, May 2011. 8\n[5] G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000. 8\n[6] Tom Brosch, Lisa YW Tang, Youngjin Yoo, David KB Li, Anthony Traboulsee, and Roger Tam. Deep 3D convolutional\nencoder networks with shortcuts for multiscale feature integration applied to multiple sclerosis lesion segmentation. IEEE\ntransactions on medical imaging, 35(5):1229–1239, 2016. 2\n[7] John Canny. A computational approach to edge detection. In Readings in computer vision, pages 184–203. Elsevier, 1987. 2\n[8] Micka¨el Chen, Thierry Arti`eres, and Ludovic Denoyer. Unsupervised object segmentation by redrawing. arXiv preprint\narXiv:1905.13539, 2019. 3\n[9] Patrick Ferdinand Christ, Mohamed Ezzeldin A Elshaer, Florian Ettlinger, Sunil Tatavarty, Marc Bickel, Patrick Bilic, Markus\nRempﬂer, Marco Armbruster, Felix Hofmann, Melvin DAnastasi, et al. Automatic liver and lesion segmentation in CT using\ncascaded fully convolutional neural networks and 3D conditional random ﬁelds. In International Conference on Medical\nImage Computing and Computer-Assisted Intervention, pages 415–423. Springer, 2016. 2\n[10] ¨Ozg¨un C¸ ic¸ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3D U-Net: learning dense\nvolumetric segmentation from sparse annotation. In International conference on medical image computing and computer-\nassisted intervention, pages 424–432. Springer, 2016. 2\n[11] Dan C Cires¸an, Alessandro Giusti, Luca M Gambardella, and J¨urgen Schmidhuber. Mitosis detection in breast cancer his-\ntology images with deep neural networks. In International Conference on Medical Image Computing and Computer-assisted\nIntervention, pages 411–418. Springer, 2013. 2\n[12] Noel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi\nKalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanoma detection 2018: A challenge\nhosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368, 2019. 3\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 2\n[14] Kiret Dhindsa, Lauren C Smail, Melissa McGrath, Luis H Braga, Suzanna Becker, and Ranil R Sonnadara. Grading prenatal\nhydronephrosis from ultrasound imaging using deep convolutional neural networks. In 2018 15th Conference on Computer\nand Robot Vision (CRV), pages 80–87. IEEE, 2018. 3, 4\n[15] Piotr Doll´ar and C Lawrence Zitnick. Fast edge detection using structured forests. IEEE transactions on pattern analysis and\nmachine intelligence, 37(8):1558–1570, 2014. 4\n[16] Qi Dou, Hao Chen, Lequan Yu, Lei Zhao, Jing Qin, Defeng Wang, Vincent CT Mok, Lin Shi, and Pheng-Ann Heng. Au-\ntomatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks. IEEE transactions on\nmedical imaging, 35(5):1182–1195, 2016. 2\n13\n[17] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea, Victor Villena-Martinez, and Jose Garcia-Rodriguez. A review\non deep learning techniques applied to semantic segmentation. arXiv preprint arXiv:1704.06857, 2017. 2\n[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua\nBengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680, 2014. 5\n[19] John T Guibas, Tejpal S Virdi, and Peter S Li. Synthetic medical images from dual generative adversarial networks. arXiv\npreprint arXiv:1709.01872, 2018. 2\n[20] Yanming Guo, Yu Liu, Theodoros Georgiou, and Michael S Lew. A review of semantic segmentation using deep neural\nnetworks. International journal of multimedia information retrieval, 7(2):87–93, 2018. 1\n[21] Mohammad Havaei, Axel Davy, David Warde-Farley, Antoine Biard, Aaron Courville, Yoshua Bengio, Chris Pal, Pierre-Marc\nJodoin, and Hugo Larochelle. Brain tumor segmentation with deep neural networks. Medical image analysis, 35:18–31, 2017.\n2\n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two\ntime-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pages\n6626–6637, 2017. 6\n[23] Seunghoon Hong, Hyeonwoo Noh, and Bohyung Han. Decoupled deep neural network for semi-supervised semantic seg-\nmentation. In Advances in neural information processing systems, pages 1495–1503, 2015. 2\n[24] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Deepco3: Deep instance co-segmentation by co-peak search and co-\nsaliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8846–8855,\n2019. 3\n[25] Vladimir Iglovikov and Alexey Shvets. Ternausnet: U-net with vgg11 encoder pre-trained on imagenet for image segmenta-\ntion. arXiv preprint arXiv:1801.05746, 2018. 2\n[26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.\nImage-to-image translation with conditional adversarial\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125–1134, 2017. 2\n[27] Konstantinos Kamnitsas, Christian Ledig, Virginia FJ Newcombe, Joanna P Simpson, Andrew D Kane, David K Menon,\nDaniel Rueckert, and Ben Glocker. Efﬁcient multi-scale 3d cnn with fully connected crf for accurate brain lesion segmentation.\nMedical image analysis, 36:61–78, 2017. 2\n[28] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active contour models. International journal of computer\nvision, 1(4):321–331, 1988. 2\n[29] Brady Kieffer, Morteza Babaie, Shivam Kalra, and Hamid R Tizhoosh. Convolutional neural networks for histopathology\nimage classiﬁcation: Training vs. using pre-trained networks. In 2017 Seventh International Conference on Image Processing\nTheory, Tools and Applications (IPTA), pages 1–6. IEEE, 2017. 2\n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6\n[31] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4\n[32] Jens Kleesiek, Gregor Urban, Alexander Hubert, Daniel Schwarz, Klaus Maier-Hein, Martin Bendszus, and Armin Biller.\nDeep MRI brain extraction: a 3D convolutional neural network for skull stripping. NeuroImage, 129:460–469, 2016. 2\n[33] Guosheng Lin, Chunhua Shen, Anton Van Den Hengel, and Ian Reid. Efﬁcient piecewise training of deep structured models\nfor semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n3194–3203, 2016. 2\n[34] Yun Liu, Ming-Ming Cheng, Xiaowei Hu, Kai Wang, and Xiang Bai. Richer convolutional features for edge detection. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 3000–3009, 2017. 4\n[35] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 3431–3440, 2015. 2\n[36] Adria Romero Lopez, Xavier Giro-i Nieto, Jack Burdick, and Oge Marques. Skin lesion classiﬁcation from dermoscopic\nimages using deep learning techniques. In 2017 13th IASTED International Conference on Biomedical Engineering (BioMed),\npages 49–54. IEEE, 2017. 2\n[37] Le Lu, Yefeng Zheng, Gustavo Carneiro, and Lin Yang. Deep learning and convolutional neural networks for medical image\ncomputing. Advances in Computer Vision and Pattern Recognition; Springer: New York, NY, USA, 2017. 2\n[38] Pauline Luc, Camille Couprie, Soumith Chintala, and Jakob Verbeek. Semantic segmentation using adversarial networks.\narXiv preprint arXiv:1611.08408, 2016. 2\n[39] BS Manjunath and Rama Chellappa. Unsupervised texture segmentation using markov random ﬁeld models. IEEE Transac-\ntions on Pattern Analysis & Machine Intelligence, 5:478–482, 1991. 2\n[40] Pim Moeskops, Jelmer M Wolterink, Bas HM van der Velden, Kenneth GA Gilhuijs, Tim Leiner, Max A Viergever, and Ivana\nIˇsgum. Deep learning for multi-task medical image segmentation in multiple modalities. In International Conference on\nMedical Image Computing and Computer-Assisted Intervention, pages 478–486. Springer, 2016. 2\n[41] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven Mc-\nDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Attention u-net: learning where to look for the pancreas. arXiv preprint\narXiv:1804.03999, 2018. 2\n14\n[42] Pavel Ostyakov, Roman Suvorov, Elizaveta Logacheva, Oleg Khomenko, and Sergey I Nikolenko. Seigan: Towards com-\npositional image generation by simultaneously learning to segment, enhance, and inpaint. arXiv preprint arXiv:1811.07630,\n2019. 3\n[43] Nobuyuki Otsu. A threshold selection method from gray-level histograms. IEEE transactions on systems, man, and cyber-\nnetics, 9(1):62–66, 1979. 4\n[44] S´ergio Pereira, Adriano Pinto, Victor Alves, and Carlos A Silva.\nBrain tumor segmentation using convolutional neural\nnetworks in mri images. IEEE transactions on medical imaging, 35(5):1240–1251, 2016. 2\n[45] Stephen M Pizer, E Philip Amburn, John D Austin, Robert Cromartie, Ari Geselowitz, Trey Greer, Bart ter Haar Romeny,\nJohn B Zimmerman, and Karel Zuiderveld. Adaptive histogram equalization and its variations. Computer vision, graphics,\nand image processing, 39(3):355–368, 1987. 4\n[46] M Ravi and Ravindra S Hegadi. Pathological medical image segmentation: A quick review based on parametric techniques.\nMedical Imaging: Artiﬁcial Intelligence, Image Recognition, and Machine Learning Techniques, page 207, 2019. 1\n[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In\nInternational Conference on Medical image computing and computer-assisted intervention, pages 234–241. Springer, 2015.\n2, 6\n[48] Mehmet Sezgin and B¨ulent Sankur. Survey over image thresholding techniques and quantitative performance evaluation.\nJournal of Electronic imaging, 13(1):146–166, 2004. 4\n[49] Hoo-Chang Shin, Neil A Tenenholtz, Jameson K Rogers, Christopher G Schwarz, Matthew L Senjem, Jeffrey L Gunter,\nKatherine P Andriole, and Mark Michalski. Medical image synthesis for data augmentation and anonymization using genera-\ntive adversarial networks. In International Workshop on Simulation and Synthesis in Medical Imaging, pages 1–11. Springer,\n2018. 2\n[50] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014. 2, 4\n[51] Jack Sklansky. Finding the convex hull of a simple polygon. Pattern Recognition Letters, 1(2):79–83, 1982. 8\n[52] Jaemin Son, Sang Jun Park, and Kyu-Hwan Jung. Retinal vessel segmentation in fundoscopic images with generative adver-\nsarial networks. arXiv preprint arXiv:1706.09318, 2017. 2\n[53] Nasim Souly, Concetto Spampinato, and Mubarak Shah. Semi supervised semantic segmentation using generative adversarial\nnetwork. In Proceedings of the IEEE International Conference on Computer Vision, pages 5688–5696, 2017. 2\n[54] Satoshi Suzuki et al.\nTopological structural analysis of digitized binary images by border following.\nComputer vision,\ngraphics, and image processing, 30(1):32–46, 1985. 8\n[55] Nima Tajbakhsh, Laura Jeyaseelan, Qian Li, Jeffrey Chiang, Zhihao Wu, and Xiaowei Ding. Embracing imperfect datasets:\nA review of deep learning solutions for medical image segmentation. arXiv preprint arXiv:1908.10454, 2019. 1\n[56] Peter C Tay, Christopher D Garson, Scott T Acton, and John A Hossack. Ultrasound despeckling for contrast enhancement.\nIEEE Transactions on Image Processing, 19(7):1847–1860, 2010. 4\n[57] C-H Teh and Roland T. Chin. On the detection of dominant points on digital curves. IEEE Transactions on pattern analysis\nand machine intelligence, 11(8):859–872, 1989. 8\n[58] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection of multi-source dermatoscopic\nimages of common pigmented skin lesions. Scientiﬁc data, 5:180161, 2018. 3\n[59] Athanasios Voulodimos, Nikolaos Doulamis, Anastasios Doulamis, and Eftychios Protopapadakis. Deep learning for com-\nputer vision: A brief review. Computational intelligence and neuroscience, 2018, 2018. 2\n[60] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis\nand semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 8798–8807, 2018. 5, 6\n[61] Xide Xia and Brian Kulis. W-net: A deep model for fully unsupervised image segmentation. arXiv preprint arXiv:1711.08506,\n2017. 3, 8\n[62] Yuan Xue, Tao Xu, Han Zhang, L Rodney Long, and Xiaolei Huang. SegAN: Adversarial network with multi-scale L1 loss\nfor medical image segmentation. Neuroinformatics, 16(3-4):383–392, 2018. 2\n[63] Dong Yang, Daguang Xu, S Kevin Zhou, Bogdan Georgescu, Mingqing Chen, Sasa Grbic, Dimitris Metaxas, and Dorin\nComaniciu. Automatic liver segmentation using an adversarial image-to-image network. In International Conference on\nMedical Image Computing and Computer-Assisted Intervention, pages 507–515. Springer, 2017. 2\n[64] Hyeon-Joong Yoo. Deep convolution neural networks in computer vision. IEEE Transactions on Smart Processing & Com-\nputing, 4(1):35–43, 2015. 2\n[65] TY Zhang and Ching Y Suen. A fast parallel algorithm for thinning digital patterns. Communications of the ACM, 27(3):236–\n239, 1984. 4\n[66] Wenlu Zhang, Rongjian Li, Houtao Deng, Li Wang, Weili Lin, Shuiwang Ji, and Dinggang Shen. Deep convolutional neural\nnetworks for multi-modality isointense infant brain image segmentation. NeuroImage, 108:214–224, 2015. 2\n15\n[67] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, and Jiajun Liang. East: an efﬁcient and accurate\nscene text detector. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 5551–5560,\n2017. 4\n16\n",
  "categories": [
    "eess.IV",
    "cs.AI",
    "cs.CV",
    "cs.LG",
    "I.4.6; I.2.10; J.3"
  ],
  "published": "2019-11-12",
  "updated": "2019-11-12"
}