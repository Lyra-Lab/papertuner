{
  "id": "http://arxiv.org/abs/1911.05722v3",
  "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
  "authors": [
    "Kaiming He",
    "Haoqi Fan",
    "Yuxin Wu",
    "Saining Xie",
    "Ross Girshick"
  ],
  "abstract": "We present Momentum Contrast (MoCo) for unsupervised visual representation\nlearning. From a perspective on contrastive learning as dictionary look-up, we\nbuild a dynamic dictionary with a queue and a moving-averaged encoder. This\nenables building a large and consistent dictionary on-the-fly that facilitates\ncontrastive unsupervised learning. MoCo provides competitive results under the\ncommon linear protocol on ImageNet classification. More importantly, the\nrepresentations learned by MoCo transfer well to downstream tasks. MoCo can\noutperform its supervised pre-training counterpart in 7 detection/segmentation\ntasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large\nmargins. This suggests that the gap between unsupervised and supervised\nrepresentation learning has been largely closed in many vision tasks.",
  "text": "Momentum Contrast for Unsupervised Visual Representation Learning\nKaiming He\nHaoqi Fan\nYuxin Wu\nSaining Xie\nRoss Girshick\nFacebook AI Research (FAIR)\nAbstract\nWe present Momentum Contrast (MoCo) for unsuper-\nvised visual representation learning. From a perspective on\ncontrastive learning [29] as dictionary look-up, we build\na dynamic dictionary with a queue and a moving-averaged\nencoder. This enables building a large and consistent dic-\ntionary on-the-ﬂy that facilitates contrastive unsupervised\nlearning.\nMoCo provides competitive results under the\ncommon linear protocol on ImageNet classiﬁcation. More\nimportantly, the representations learned by MoCo transfer\nwell to downstream tasks. MoCo can outperform its super-\nvised pre-training counterpart in 7 detection/segmentation\ntasks on PASCAL VOC, COCO, and other datasets, some-\ntimes surpassing it by large margins. This suggests that\nthe gap between unsupervised and supervised representa-\ntion learning has been largely closed in many vision tasks.\nCode: https://github.com/facebookresearch/moco\n1. Introduction\nUnsupervised representation learning is highly success-\nful in natural language processing, e.g., as shown by GPT\n[50, 51] and BERT [12]. But supervised pre-training is still\ndominant in computer vision, where unsupervised meth-\nods generally lag behind. The reason may stem from dif-\nferences in their respective signal spaces. Language tasks\nhave discrete signal spaces (words, sub-word units, etc.)\nfor building tokenized dictionaries, on which unsupervised\nlearning can be based. Computer vision, in contrast, further\nconcerns dictionary building [54, 9, 5], as the raw signal is\nin a continuous, high-dimensional space and is not struc-\ntured for human communication (e.g., unlike words).\nSeveral recent studies [61, 46, 36, 66, 35, 56, 2] present\npromising results on unsupervised visual representation\nlearning using approaches related to the contrastive loss\n[29]. Though driven by various motivations, these methods\ncan be thought of as building dynamic dictionaries. The\n“keys” (tokens) in the dictionary are sampled from data\n(e.g., images or patches) and are represented by an encoder\nnetwork. Unsupervised learning trains encoders to perform\ndictionary look-up: an encoded “query” should be similar\nto its matching key and dissimilar to others. Learning is\nformulated as minimizing a contrastive loss [29].\nencoder\nmomentum\nencoder\nq\ncontrastive loss\nsimilarity\nqueue\nk0 k1 k2 ...\nxquery\nxkey\n0\nxkey\n1\nxkey\n2\n...\nFigure 1. Momentum Contrast (MoCo) trains a visual represen-\ntation encoder by matching an encoded query q to a dictionary\nof encoded keys using a contrastive loss.\nThe dictionary keys\n{k0, k1, k2, ...} are deﬁned on-the-ﬂy by a set of data samples.\nThe dictionary is built as a queue, with the current mini-batch en-\nqueued and the oldest mini-batch dequeued, decoupling it from\nthe mini-batch size. The keys are encoded by a slowly progressing\nencoder, driven by a momentum update with the query encoder.\nThis method enables a large and consistent dictionary for learning\nvisual representations.\nFrom this perspective, we hypothesize that it is desirable\nto build dictionaries that are: (i) large and (ii) consistent\nas they evolve during training. Intuitively, a larger dictio-\nnary may better sample the underlying continuous, high-\ndimensional visual space, while the keys in the dictionary\nshould be represented by the same or similar encoder so that\ntheir comparisons to the query are consistent. However, ex-\nisting methods that use contrastive losses can be limited in\none of these two aspects (discussed later in context).\nWe present Momentum Contrast (MoCo) as a way of\nbuilding large and consistent dictionaries for unsupervised\nlearning with a contrastive loss (Figure 1). We maintain the\ndictionary as a queue of data samples: the encoded repre-\nsentations of the current mini-batch are enqueued, and the\noldest are dequeued. The queue decouples the dictionary\nsize from the mini-batch size, allowing it to be large. More-\nover, as the dictionary keys come from the preceding sev-\neral mini-batches, a slowly progressing key encoder, imple-\nmented as a momentum-based moving average of the query\nencoder, is proposed to maintain consistency.\n1\narXiv:1911.05722v3  [cs.CV]  23 Mar 2020\nMoCo is a mechanism for building dynamic dictionar-\nies for contrastive learning, and can be used with various\npretext tasks. In this paper, we follow a simple instance\ndiscrimination task [61, 63, 2]: a query matches a key if\nthey are encoded views (e.g., different crops) of the same\nimage. Using this pretext task, MoCo shows competitive\nresults under the common protocol of linear classiﬁcation\nin the ImageNet dataset [11].\nA main purpose of unsupervised learning is to pre-train\nrepresentations (i.e., features) that can be transferred to\ndownstream tasks by ﬁne-tuning. We show that in 7 down-\nstream tasks related to detection or segmentation, MoCo\nunsupervised pre-training can surpass its ImageNet super-\nvised counterpart, in some cases by nontrivial margins. In\nthese experiments, we explore MoCo pre-trained on Ima-\ngeNet or on a one-billion Instagram image set, demonstrat-\ning that MoCo can work well in a more real-world, billion-\nimage scale, and relatively uncurated scenario. These re-\nsults show that MoCo largely closes the gap between un-\nsupervised and supervised representation learning in many\ncomputer vision tasks, and can serve as an alternative to Im-\nageNet supervised pre-training in several applications.\n2. Related Work\nUnsupervised/self-supervised1 learning methods gener-\nally involve two aspects: pretext tasks and loss functions.\nThe term “pretext” implies that the task being solved is not\nof genuine interest, but is solved only for the true purpose\nof learning a good data representation. Loss functions can\noften be investigated independently of pretext tasks. MoCo\nfocuses on the loss function aspect. Next we discuss related\nstudies with respect to these two aspects.\nLoss functions. A common way of deﬁning a loss function\nis to measure the difference between a model’s prediction\nand a ﬁxed target, such as reconstructing the input pixels\n(e.g., auto-encoders) by L1 or L2 losses, or classifying the\ninput into pre-deﬁned categories (e.g., eight positions [13],\ncolor bins [64]) by cross-entropy or margin-based losses.\nOther alternatives, as described next, are also possible.\nContrastive losses [29] measure the similarities of sam-\nple pairs in a representation space. Instead of matching an\ninput to a ﬁxed target, in contrastive loss formulations the\ntarget can vary on-the-ﬂy during training and can be deﬁned\nin terms of the data representation computed by a network\n[29]. Contrastive learning is at the core of several recent\nworks on unsupervised learning [61, 46, 36, 66, 35, 56, 2],\nwhich we elaborate on later in context (Sec. 3.1).\nAdversarial losses [24] measure the difference between\nprobability distributions. It is a widely successful technique\n1Self-supervised learning is a form of unsupervised learning. Their dis-\ntinction is informal in the existing literature. In this paper, we use the more\nclassical term of “unsupervised learning”, in the sense of “not supervised\nby human-annotated labels”.\nfor unsupervised data generation. Adversarial methods for\nrepresentation learning are explored in [15, 16]. There are\nrelations (see [24]) between generative adversarial networks\nand noise-contrastive estimation (NCE) [28].\nPretext tasks. A wide range of pretext tasks have been pro-\nposed. Examples include recovering the input under some\ncorruption, e.g., denoising auto-encoders [58], context auto-\nencoders [48], or cross-channel auto-encoders (coloriza-\ntion) [64, 65]. Some pretext tasks form pseudo-labels by,\ne.g., transformations of a single (“exemplar”) image [17],\npatch orderings [13, 45], tracking [59] or segmenting ob-\njects [47] in videos, or clustering features [3, 4].\nContrastive learning vs. pretext tasks. Various pretext\ntasks can be based on some form of contrastive loss func-\ntions. The instance discrimination method [61] is related\nto the exemplar-based task [17] and NCE [28]. The pretext\ntask in contrastive predictive coding (CPC) [46] is a form\nof context auto-encoding [48], and in contrastive multiview\ncoding (CMC) [56] it is related to colorization [64].\n3. Method\n3.1. Contrastive Learning as Dictionary Look-up\nContrastive learning [29], and its recent developments,\ncan be thought of as training an encoder for a dictionary\nlook-up task, as described next.\nConsider an encoded query q and a set of encoded sam-\nples {k0, k1, k2, ...} that are the keys of a dictionary. As-\nsume that there is a single key (denoted as k+) in the dic-\ntionary that q matches. A contrastive loss [29] is a function\nwhose value is low when q is similar to its positive key k+\nand dissimilar to all other keys (considered negative keys\nfor q). With similarity measured by dot product, a form of\na contrastive loss function, called InfoNCE [46], is consid-\nered in this paper:\nLq = −log\nexp(q·k+/τ)\nPK\ni=0 exp(q·ki/τ)\n(1)\nwhere τ is a temperature hyper-parameter per [61]. The sum\nis over one positive and K negative samples. Intuitively,\nthis loss is the log loss of a (K+1)-way softmax-based clas-\nsiﬁer that tries to classify q as k+. Contrastive loss functions\ncan also be based on other forms [29, 59, 61, 36], such as\nmargin-based losses and variants of NCE losses.\nThe contrastive loss serves as an unsupervised objective\nfunction for training the encoder networks that represent the\nqueries and keys [29]. In general, the query representation\nis q = fq(xq) where fq is an encoder network and xq is a\nquery sample (likewise, k = fk(xk)). Their instantiations\ndepend on the speciﬁc pretext task. The input xq and xk can\nbe images [29, 61, 63], patches [46], or context consisting a\nset of patches [46]. The networks fq and fk can be identical\n[29, 59, 63], partially shared [46, 36, 2], or different [56].\nq\nk\ncontrastive loss\ngradient\ngradient\n(a) end-to-end\nxq\nxk\nq\nk\ncontrastive loss\ngradient\n(c) MoCo\nxq\nxk\nencoder\nmomentum\nencoder\nq\nk\ncontrastive loss\nsampling\nmemory\nbank\ngradient\n(b) memory bank\nxq\nencoder\nencoder q\nencoder k\nq·k\nq·k\nq·k\nFigure 2. Conceptual comparison of three contrastive loss mechanisms (empirical comparisons are in Figure 3 and Table 3). Here we\nillustrate one pair of query and key. The three mechanisms differ in how the keys are maintained and how the key encoder is updated.\n(a): The encoders for computing the query and key representations are updated end-to-end by back-propagation (the two encoders can\nbe different). (b): The key representations are sampled from a memory bank [61]. (c): MoCo encodes the new keys on-the-ﬂy by a\nmomentum-updated encoder, and maintains a queue (not illustrated in this ﬁgure) of keys.\n3.2. Momentum Contrast\nFrom the above perspective, contrastive learning is a way\nof building a discrete dictionary on high-dimensional con-\ntinuous inputs such as images. The dictionary is dynamic in\nthe sense that the keys are randomly sampled, and that the\nkey encoder evolves during training. Our hypothesis is that\ngood features can be learned by a large dictionary that cov-\ners a rich set of negative samples, while the encoder for the\ndictionary keys is kept as consistent as possible despite its\nevolution. Based on this motivation, we present Momentum\nContrast as described next.\nDictionary as a queue. At the core of our approach is\nmaintaining the dictionary as a queue of data samples. This\nallows us to reuse the encoded keys from the immediate pre-\nceding mini-batches. The introduction of a queue decouples\nthe dictionary size from the mini-batch size. Our dictionary\nsize can be much larger than a typical mini-batch size, and\ncan be ﬂexibly and independently set as a hyper-parameter.\nThe samples in the dictionary are progressively replaced.\nThe current mini-batch is enqueued to the dictionary, and\nthe oldest mini-batch in the queue is removed. The dictio-\nnary always represents a sampled subset of all data, while\nthe extra computation of maintaining this dictionary is man-\nageable. Moreover, removing the oldest mini-batch can be\nbeneﬁcial, because its encoded keys are the most outdated\nand thus the least consistent with the newest ones.\nMomentum update. Using a queue can make the dictio-\nnary large, but it also makes it intractable to update the key\nencoder by back-propagation (the gradient should propa-\ngate to all samples in the queue). A na¨ıve solution is to\ncopy the key encoder fk from the query encoder fq, ignor-\ning this gradient. But this solution yields poor results in\nexperiments (Sec. 4.1). We hypothesize that such failure\nis caused by the rapidly changing encoder that reduces the\nkey representations’ consistency. We propose a momentum\nupdate to address this issue.\nFormally, denoting the parameters of fk as θk and those\nof fq as θq, we update θk by:\nθk ←mθk + (1 −m)θq.\n(2)\nHere m ∈[0, 1) is a momentum coefﬁcient. Only the pa-\nrameters θq are updated by back-propagation. The momen-\ntum update in Eqn.(2) makes θk evolve more smoothly than\nθq. As a result, though the keys in the queue are encoded\nby different encoders (in different mini-batches), the dif-\nference among these encoders can be made small. In ex-\nperiments, a relatively large momentum (e.g., m = 0.999,\nour default) works much better than a smaller value (e.g.,\nm = 0.9), suggesting that a slowly evolving key encoder is\na core to making use of a queue.\nRelations to previous mechanisms. MoCo is a general\nmechanism for using contrastive losses. We compare it with\ntwo existing general mechanisms in Figure 2. They exhibit\ndifferent properties on the dictionary size and consistency.\nThe end-to-end update by back-propagation is a natural\nmechanism (e.g., [29, 46, 36, 63, 2, 35], Figure 2a). It uses\nsamples in the current mini-batch as the dictionary, so the\nkeys are consistently encoded (by the same set of encoder\nparameters). But the dictionary size is coupled with the\nmini-batch size, limited by the GPU memory size. It is also\nchallenged by large mini-batch optimization [25]. Some re-\ncent methods [46, 36, 2] are based on pretext tasks driven by\nlocal positions, where the dictionary size can be made larger\nby multiple positions. But these pretext tasks may require\nspecial network designs such as patchifying the input [46]\nor customizing the receptive ﬁeld size [2], which may com-\nplicate the transfer of these networks to downstream tasks.\nAnother mechanism is the memory bank approach pro-\nposed by [61] (Figure 2b). A memory bank consists of the\nrepresentations of all samples in the dataset. The dictionary\nfor each mini-batch is randomly sampled from the memory\nbank with no back-propagation, so it can support a large\ndictionary size. However, the representation of a sample in\nAlgorithm 1 Pseudocode of MoCo in a PyTorch-like style.\n# f_q, f_k: encoder networks for query and key\n# queue: dictionary as a queue of K keys (CxK)\n# m: momentum\n# t: temperature\nf_k.params = f_q.params # initialize\nfor x in loader: # load a minibatch x with N samples\nx_q = aug(x) # a randomly augmented version\nx_k = aug(x) # another randomly augmented version\nq = f_q.forward(x_q) # queries: NxC\nk = f_k.forward(x_k) # keys: NxC\nk = k.detach() # no gradient to keys\n# positive logits: Nx1\nl_pos = bmm(q.view(N,1,C), k.view(N,C,1))\n# negative logits: NxK\nl_neg = mm(q.view(N,C), queue.view(C,K))\n# logits: Nx(1+K)\nlogits = cat([l_pos, l_neg], dim=1)\n# contrastive loss, Eqn.(1)\nlabels = zeros(N) # positives are the 0-th\nloss = CrossEntropyLoss(logits/t, labels)\n# SGD update: query network\nloss.backward()\nupdate(f_q.params)\n# momentum update: key network\nf_k.params = m*f_k.params+(1-m)*f_q.params\n# update dictionary\nenqueue(queue, k) # enqueue the current minibatch\ndequeue(queue) # dequeue the earliest minibatch\nbmm: batch matrix multiplication; mm: matrix multiplication; cat: concatenation.\nthe memory bank was updated when it was last seen, so the\nsampled keys are essentially about the encoders at multiple\ndifferent steps all over the past epoch and thus are less con-\nsistent. A momentum update is adopted on the memory\nbank in [61]. Its momentum update is on the representa-\ntions of the same sample, not the encoder. This momentum\nupdate is irrelevant to our method, because MoCo does not\nkeep track of every sample. Moreover, our method is more\nmemory-efﬁcient and can be trained on billion-scale data,\nwhich can be intractable for a memory bank.\nSec. 4 empirically compares these three mechanisms.\n3.3. Pretext Task\nContrastive learning can drive a variety of pretext tasks.\nAs the focus of this paper is not on designing a new pretext\ntask, we use a simple one mainly following the instance\ndiscrimination task in [61], to which some recent works [63,\n2] are related.\nFollowing [61], we consider a query and a key as a pos-\nitive pair if they originate from the same image, and other-\nwise as a negative sample pair. Following [63, 2], we take\ntwo random “views” of the same image under random data\naugmentation to form a positive pair. The queries and keys\nare respectively encoded by their encoders, fq and fk. The\nencoder can be any convolutional neural network [39].\nAlgorithm 1 provides the pseudo-code of MoCo for this\npretext task.\nFor the current mini-batch, we encode the\nqueries and their corresponding keys, which form the posi-\ntive sample pairs. The negative samples are from the queue.\nTechnical details. We adopt a ResNet [33] as the encoder,\nwhose last fully-connected layer (after global average pool-\ning) has a ﬁxed-dimensional output (128-D [61]). This out-\nput vector is normalized by its L2-norm [61]. This is the\nrepresentation of the query or key. The temperature τ in\nEqn.(1) is set as 0.07 [61]. The data augmentation setting\nfollows [61]: a 224×224-pixel crop is taken from a ran-\ndomly resized image, and then undergoes random color jit-\ntering, random horizontal ﬂip, and random grayscale con-\nversion, all available in PyTorch’s torchvision package.\nShufﬂing BN. Our encoders fq and fk both have Batch\nNormalization (BN) [37] as in the standard ResNet [33]. In\nexperiments, we found that using BN prevents the model\nfrom learning good representations, as similarly reported\nin [35] (which avoids using BN). The model appears to\n“cheat” the pretext task and easily ﬁnds a low-loss solu-\ntion. This is possibly because the intra-batch communica-\ntion among samples (caused by BN) leaks information.\nWe resolve this problem by shufﬂing BN. We train with\nmultiple GPUs and perform BN on the samples indepen-\ndently for each GPU (as done in common practice). For the\nkey encoder fk, we shufﬂe the sample order in the current\nmini-batch before distributing it among GPUs (and shufﬂe\nback after encoding); the sample order of the mini-batch\nfor the query encoder fq is not altered. This ensures the\nbatch statistics used to compute a query and its positive key\ncome from two different subsets. This effectively tackles\nthe cheating issue and allows training to beneﬁt from BN.\nWe use shufﬂed BN in both our method and its end-to-\nend ablation counterpart (Figure 2a). It is irrelevant to the\nmemory bank counterpart (Figure 2b), which does not suf-\nfer from this issue because the positive keys are from differ-\nent mini-batches in the past.\n4. Experiments\nWe study unsupervised training performed in:\nImageNet-1M (IN-1M): This is the ImageNet [11] train-\ning set that has ∼1.28 million images in 1000 classes (often\ncalled ImageNet-1K; we count the image number instead,\nas classes are not exploited by unsupervised learning). This\ndataset is well-balanced in its class distribution, and its im-\nages generally contain iconic view of objects.\nInstagram-1B (IG-1B): Following [44], this is a dataset\nof ∼1 billion (940M) public images from Instagram. The\nimages are from ∼1500 hashtags [44] that are related to the\nImageNet categories. This dataset is relatively uncurated\ncomparing to IN-1M, and has a long-tailed, unbalanced\ndistribution of real-world data. This dataset contains both\niconic objects and scene-level images.\nTraining. We use SGD as our optimizer. The SGD weight\ndecay is 0.0001 and the SGD momentum is 0.9. For IN-1M,\nwe use a mini-batch size of 256 (N in Algorithm 1) in 8\nGPUs, and an initial learning rate of 0.03. We train for 200\nepochs with the learning rate multiplied by 0.1 at 120 and\n160 epochs [61], taking ∼53 hours training ResNet-50. For\nIG-1B, we use a mini-batch size of 1024 in 64 GPUs, and\na learning rate of 0.12 which is exponentially decayed by\n0.9× after every 62.5k iterations (64M images). We train\nfor 1.25M iterations (∼1.4 epochs of IG-1B), taking ∼6 days\nfor ResNet-50.\n4.1. Linear Classiﬁcation Protocol\nWe ﬁrst verify our method by linear classiﬁcation on\nfrozen features, following a common protocol. In this sub-\nsection we perform unsupervised pre-training on IN-1M.\nThen we freeze the features and train a supervised linear\nclassiﬁer (a fully-connected layer followed by softmax). We\ntrain this classiﬁer on the global average pooling features of\na ResNet, for 100 epochs. We report 1-crop, top-1 classiﬁ-\ncation accuracy on the ImageNet validation set.\nFor this classiﬁer, we perform a grid search and ﬁnd the\noptimal initial learning rate is 30 and weight decay is 0\n(similarly reported in [56]). These hyper-parameters per-\nform consistently well for all ablation entries presented in\nthis subsection. These hyper-parameter values imply that\nthe feature distributions (e.g., magnitudes) can be substan-\ntially different from those of ImageNet supervised training,\nan issue we will revisit in Sec. 4.2.\nAblation: contrastive loss mechanisms. We compare the\nthree mechanisms that are illustrated in Figure 2. To focus\non the effect of contrastive loss mechanisms, we implement\nall of them in the same pretext task as described in Sec. 3.3.\nWe also use the same form of InfoNCE as the contrastive\nloss function, Eqn.(1). As such, the comparison is solely on\nthe three mechanisms.\nThe results are in Figure 3. Overall, all three mecha-\nnisms beneﬁt from a larger K. A similar trend has been\nobserved in [61, 56] under the memory bank mechanism,\nwhile here we show that this trend is more general and can\nbe seen in all mechanisms. These results support our moti-\nvation of building a large dictionary.\nThe end-to-end mechanism performs similarly to MoCo\nwhen K is small. However, the dictionary size is limited\nby the mini-batch size due to the end-to-end requirement.\nHere the largest mini-batch a high-end machine (8 Volta\n32GB GPUs) can afford is 1024. More essentially, large\nmini-batch training is an open problem [25]: we found it\nnecessary to use the linear learning rate scaling rule [25]\nhere, without which the accuracy drops (by ∼2% with a\n1024 mini-batch). But optimizing with a larger mini-batch\nis harder [25], and it is questionable whether the trend can\nbe extrapolated into a larger K even if memory is sufﬁcient.\n256\n512\n1024\n4096\n16384\n65536\nK (log-scale)\n50\n52\n54\n56\n58\n60\naccuracy (%)\n50.0\n52.0\n54.1\n56.5\n57.8\n58.0\n54.7\n56.4\n57.5\n59.0\n60.4\n60.6\n54.9\n56.3\n57.3\nend-to-end\nmemory bank\nMoCo\nFigure 3. Comparison of three contrastive loss mechanisms un-\nder the ImageNet linear classiﬁcation protocol. We adopt the same\npretext task (Sec. 3.3) and only vary the contrastive loss mecha-\nnism (Figure 2). The number of negatives is K in memory bank\nand MoCo, and is K−1 in end-to-end (offset by one because the\npositive key is in the same mini-batch). The network is ResNet-50.\nThe memory bank [61] mechanism can support a larger\ndictionary size. But it is 2.6% worse than MoCo. This is\ninline with our hypothesis: the keys in the memory bank\nare from very different encoders all over the past epoch and\nthey are not consistent. Note the memory bank result of\n58.0% reﬂects our improved implementation of [61].2\nAblation: momentum. The table below shows ResNet-50\naccuracy with different MoCo momentum values (m in\nEqn.(2)) used in pre-training (K = 4096 here) :\nmomentum m\n0\n0.9\n0.99\n0.999\n0.9999\naccuracy (%)\nfail\n55.2\n57.8\n59.0\n58.9\nIt performs reasonably well when m is in 0.99 ∼0.9999,\nshowing that a slowly progressing (i.e., relatively large mo-\nmentum) key encoder is beneﬁcial. When m is too small\n(e.g., 0.9), the accuracy drops considerably; at the extreme\nof no momentum (m is 0), the training loss oscillates and\nfails to converge. These results support our motivation of\nbuilding a consistent dictionary.\nComparison with previous results. Previous unsuper-\nvised learning methods can differ substantially in model\nsizes. For a fair and comprehensive comparison, we report\naccuracy vs. #parameters3 trade-offs. Besides ResNet-50\n(R50) [33], we also report its variants that are 2× and 4×\nwider (more channels), following [38].4 We set K = 65536\nand m = 0.999. Table 1 is the comparison.\nMoCo with R50 performs competitively and achieves\n60.6% accuracy, better than all competitors of similar\nmodel sizes (∼24M). MoCo beneﬁts from larger models and\nachieves 68.6% accuracy with R50w4×.\nNotably, we achieve competitive results using a standard\nResNet-50 and require no speciﬁc architecture designs, e.g.,\n2Here 58.0% is with InfoNCE and K=65536. We reproduce 54.3%\nwhen using NCE and K=4096 (the same as [61]), close to 54.0% in [61].\n3Parameters are of the feature extractor: e.g., we do not count the pa-\nrameters of convx if convx is not included in linear classiﬁcation.\n4Our w2× and w4× models correspond to the “×8” and “×16” cases\nin [38], because the standard-sized ResNet is referred to as “×4” in [38].\n0\n200\n400\n600\n#parameters (M)\n40\n50\n60\n70\naccuracy (%)\nExemplar\nRelativePosition\nJigsaw\nRotation\nColorization\nDeepCluster\nInstDisc\nCPCv1\nCPCv2\nBigBiGAN-R50\nBigBiGAN-Rv50w4x\nAMDIM-small\nAMDIM-large\nCMC-R50\nCMC-R50w2x\nLocalAgg\nR50\nRX50\nR50w2x\nR50w4x\nprevious\nMoCo\nmethod\narchitecture\n#params (M)\naccuracy (%)\nExemplar [17]\nR50w3×\n211\n46.0 [38]\nRelativePosition [13]\nR50w2×\n94\n51.4 [38]\nJigsaw [45]\nR50w2×\n94\n44.6 [38]\nRotation [19]\nRv50w4×\n86\n55.4 [38]\nColorization [64]\nR101∗\n28\n39.6 [14]\nDeepCluster [3]\nVGG [53]\n15\n48.4 [4]\nBigBiGAN [16]\nR50\n24\n56.6\nRv50w4×\n86\n61.3\nmethods based on contrastive learning follow:\nInstDisc [61]\nR50\n24\n54.0\nLocalAgg [66]\nR50\n24\n58.8\nCPC v1 [46]\nR101∗\n28\n48.7\nCPC v2 [35]\nR170∗\nwider\n303\n65.9\nCMC [56]\nR50L+ab\n47\n64.1†\nR50w2×L+ab\n188\n68.4†\nAMDIM [2]\nAMDIMsmall\n194\n63.5†\nAMDIMlarge\n626\n68.1†\nMoCo\nR50\n24\n60.6\nRX50\n46\n63.9\nR50w2×\n94\n65.4\nR50w4×\n375\n68.6\nTable 1. Comparison under the linear classiﬁcation protocol\non ImageNet. The ﬁgure visualizes the table. All are reported as\nunsupervised pre-training on the ImageNet-1M training set, fol-\nlowed by supervised linear classiﬁcation trained on frozen fea-\ntures, evaluated on the validation set. The parameter counts are\nthose of the feature extractors. We compare with improved re-\nimplementations if available (referenced after the numbers).\nNotations: R101∗/R170∗is ResNet-101/170 with the last residual stage\nremoved [14, 46, 35], and R170 is made wider [35]; Rv50 is a reversible\nnet [23], RX50 is ResNeXt-50-32×8d [62].\n†: Pre-training uses FastAutoAugment [40] that is supervised by ImageNet labels.\npatchiﬁed inputs [46, 35], carefully tailored receptive ﬁelds\n[2], or combining two networks [56]. By using an architec-\nture that is not customized for the pretext task, it is easier to\ntransfer features to a variety of visual tasks and make com-\nparisons, studied in the next subsection.\nThis paper’s focus is on a mechanism for general con-\ntrastive learning; we do not explore orthogonal factors (such\nas speciﬁc pretext tasks) that may further improve accuracy.\nAs an example, “MoCo v2” [8], an extension of a prelim-\ninary version of this manuscript, achieves 71.1% accuracy\nwith R50 (up from 60.6%), given small changes on the data\naugmentation and output projection head [7]. We believe\nthat this additional result shows the generality and robust-\nness of the MoCo framework.\npre-train\nAP50\nAP\nAP75\nrandom init.\n64.4\n37.9\n38.6\nsuper. IN-1M\n81.4\n54.0\n59.1\nMoCo IN-1M\n81.1 (−0.3)\n54.6 (+0.6)\n59.9 (+0.8)\nMoCo IG-1B\n81.6 (+0.2)\n55.5 (+1.5)\n61.2 (+2.1)\n(a) Faster R-CNN, R50-dilated-C5\npre-train\nAP50\nAP\nAP75\nrandom init.\n60.2\n33.8\n33.1\nsuper. IN-1M\n81.3\n53.5\n58.8\nMoCo IN-1M\n81.5 (+0.2)\n55.9 (+2.4)\n62.6 (+3.8)\nMoCo IG-1B\n82.2 (+0.9)\n57.2 (+3.7)\n63.7 (+4.9)\n(b) Faster R-CNN, R50-C4\nTable 2. Object detection ﬁne-tuned on PASCAL VOC\ntrainval07+12. Evaluation is on test2007: AP50 (default\nVOC metric), AP (COCO-style), and AP75, averaged over 5 trials.\nAll are ﬁne-tuned for 24k iterations (∼23 epochs). In the brackets\nare the gaps to the ImageNet supervised pre-training counterpart.\nIn green are the gaps of at least +0.5 point.\nR50-dilated-C5\nR50-C4\npre-train\nAP50\nAP\nAP75\nAP50\nAP\nAP75\nend-to-end\n79.2\n52.0\n56.6\n80.4\n54.6\n60.3\nmemory bank\n79.8\n52.9\n57.9\n80.6\n54.9\n60.6\nMoCo\n81.1\n54.6\n59.9\n81.5\n55.9\n62.6\nTable 3. Comparison of three contrastive loss mechanisms on\nPASCAL VOC object detection, ﬁne-tuned on trainval07+12\nand evaluated on test2007 (averages over 5 trials). All models\nare implemented by us (Figure 3), pre-trained on IN-1M, and ﬁne-\ntuned using the same settings as in Table 2.\n4.2. Transferring Features\nA main goal of unsupervised learning is to learn features\nthat are transferrable. ImageNet supervised pre-training is\nmost inﬂuential when serving as the initialization for ﬁne-\ntuning in downstream tasks (e.g., [21, 20, 43, 52]). Next\nwe compare MoCo with ImageNet supervised pre-training,\ntransferred to various tasks including PASCAL VOC [18],\nCOCO [42], etc. As prerequisites, we discuss two important\nissues involved [31]: normalization and schedules.\nNormalization. As noted in Sec. 4.1, features produced by\nunsupervised pre-training can have different distributions\ncompared with ImageNet supervised pre-training.\nBut a\nsystem for a downstream task often has hyper-parameters\n(e.g., learning rates) selected for supervised pre-training. To\nrelieve this problem, we adopt feature normalization during\nﬁne-tuning: we ﬁne-tune with BN that is trained (and syn-\nchronized across GPUs [49]), instead of freezing it by an\nafﬁne layer [33]. We also use BN in the newly initialized\nlayers (e.g., FPN [41]), which helps calibrate magnitudes.\nWe perform normalization when ﬁne-tuning supervised\nand unsupervised pre-training models. MoCo uses the same\nhyper-parameters as the ImageNet supervised counterpart.\nSchedules. If the ﬁne-tuning schedule is long enough,\ntraining detectors from random initialization can be strong\nbaselines, and can match the ImageNet supervised counter-\npart on COCO [31]. Our goal is to investigate transferabil-\nAP50\nAP\nAP75\npre-train\nRelPos, by [14] Multi-task [14]\nJigsaw, by [26]\nLocalAgg [66]\nMoCo\nMoCo\nMulti-task [14]\nMoCo\nsuper. IN-1M\n74.2\n74.2\n70.5\n74.6\n74.4\n42.4\n44.3\n42.7\nunsup. IN-1M\n66.8 (−7.4)\n70.5 (−3.7)\n61.4 (−9.1)\n69.1 (−5.5)\n74.9 (+0.5)\n46.6 (+4.2)\n43.9 (−0.4)\n50.1 (+7.4)\nunsup. IN-14M\n-\n-\n69.2 (−1.3)\n-\n75.2 (+0.8)\n46.9 (+4.5)\n-\n50.2 (+7.5)\nunsup. YFCC-100M\n-\n-\n66.6 (−3.9)\n-\n74.7 (+0.3)\n45.9 (+3.5)\n-\n49.0 (+6.3)\nunsup. IG-1B\n-\n-\n-\n-\n75.6 (+1.2)\n47.6 (+5.2)\n-\n51.7 (+9.0)\nTable 4. Comparison with previous methods on object detection ﬁne-tuned on PASCAL VOC trainval2007. Evaluation is on\ntest2007. The ImageNet supervised counterparts are from the respective papers, and are reported as having the same structure as the\nrespective unsupervised pre-training counterparts. All entries are based on the C4 backbone. The models in [14] are R101 v2 [34], and\nothers are R50. The RelPos (relative position) [13] result is the best single-task case in the Multi-task paper [14]. The Jigsaw [45] result is\nfrom the ResNet-based implementation in [26]. Our results are with 9k-iteration ﬁne-tuning, averaged over 5 trials. In the brackets are the\ngaps to the ImageNet supervised pre-training counterpart. In green are the gaps of at least +0.5 point.\nity of features, so our experiments are on controlled sched-\nules, e.g., the 1× (∼12 epochs) or 2× schedules [22] for\nCOCO, in contrast to 6×∼9× in [31]. On smaller datasets\nlike VOC, training longer may not catch up [31].\nNonetheless, in our ﬁne-tuning, MoCo uses the same\nschedule as the ImageNet supervised counterpart, and ran-\ndom initialization results are provided as references.\nPut together, our ﬁne-tuning uses the same setting as the\nsupervised pre-training counterpart. This may place MoCo\nat a disadvantage. Even so, MoCo is competitive. Doing so\nalso makes it feasible to present comparisons on multiple\ndatasets/tasks, without extra hyper-parameter search.\n4.2.1\nPASCAL VOC Object Detection\nSetup. The detector is Faster R-CNN [52] with a backbone\nof R50-dilated-C5 or R50-C4 [32] (details in appendix),\nwith BN tuned, implemented in [60]. We ﬁne-tune all lay-\ners end-to-end. The image scale is [480, 800] pixels during\ntraining and 800 at inference. The same setup is used for all\nentries, including the supervised pre-training baseline. We\nevaluate the default VOC metric of AP50 (i.e., IoU threshold\nis 50%) and the more stringent metrics of COCO-style AP\nand AP75. Evaluation is on the VOC test2007 set.\nAblation: backbones. Table 2 shows the results ﬁne-tuned\non trainval07+12 (∼16.5k images). For R50-dilated-\nC5 (Table 2a), MoCo pre-trained on IN-1M is comparable\nto the supervised pre-training counterpart, and MoCo pre-\ntrained on IG-1B surpasses it.\nFor R50-C4 (Table 2b),\nMoCo with IN-1M or IG-1B is better than the supervised\ncounterpart: up to +0.9 AP50, +3.7 AP, and +4.9 AP75.\nInterestingly, the transferring accuracy depends on the\ndetector structure. For the C4 backbone, by default used\nin existing ResNet-based results [14, 61, 26, 66], the ad-\nvantage of unsupervised pre-training is larger. The relation\nbetween pre-training vs. detector structures has been veiled\nin the past, and should be a factor under consideration.\nAblation: contrastive loss mechanisms. We point out that\nthese results are partially because we establish solid detec-\ntion baselines for contrastive learning. To pin-point the gain\nthat is solely contributed by using the MoCo mechanism\nin contrastive learning, we ﬁne-tune the models pre-trained\nwith the end-to-end or memory bank mechanism, both im-\nplemented by us (i.e., the best ones in Figure 3), using the\nsame ﬁne-tuning setting as MoCo.\nThese competitors perform decently (Table 3). Their AP\nand AP75 with the C4 backbone are also higher than the\nImageNet supervised counterpart’s, c.f. Table 2b, but other\nmetrics are lower. They are worse than MoCo in all metrics.\nThis shows the beneﬁts of MoCo. In addition, how to train\nthese competitors in larger-scale data is an open question,\nand they may not beneﬁt from IG-1B.\nComparison with previous results. Following the com-\npetitors, we ﬁne-tune on trainval2007 (∼5k images)\nusing the C4 backbone. The comparison is in Table 4.\nFor the AP50 metric, no previous method can catch\nup with its respective supervised pre-training counterpart.\nMoCo pre-trained on any of IN-1M, IN-14M (full Ima-\ngeNet), YFCC-100M [55], and IG-1B can outperform the\nsupervised baseline. Large gains are seen in the more strin-\ngent metrics: up to +5.2 AP and +9.0 AP75. These gains are\nlarger than the gains seen in trainval07+12 (Table 2b).\n4.2.2\nCOCO Object Detection and Segmentation\nSetup. The model is Mask R-CNN [32] with the FPN [41]\nor C4 backbone, with BN tuned, implemented in [60]. The\nimage scale is in [640, 800] pixels during training and is 800\nat inference. We ﬁne-tune all layers end-to-end. We ﬁne-\ntune on the train2017 set (∼118k images) and evaluate\non val2017. The schedule is the default 1× or 2× in [22].\nResults. Table 5 shows the results on COCO with the FPN\n(Table 5a, b) and C4 (Table 5c, d) backbones. With the\n1× schedule, all models (including the ImageNet super-\nvised counterparts) are heavily under-trained, as indicated\nby the ∼2 points gaps to the 2× schedule cases. With the\n2× schedule, MoCo is better than its ImageNet supervised\ncounterpart in all metrics in both backbones.\n4.2.3\nMore Downstream Tasks\nTable 6 shows more downstream tasks (implementation de-\ntails in appendix). Overall, MoCo performs competitively\npre-train\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\nrandom init.\n31.0\n49.5\n33.2\n28.5\n46.8\n30.4\nsuper. IN-1M\n38.9\n59.6\n42.7\n35.4\n56.5\n38.1\nMoCo IN-1M\n38.5 (−0.4) 58.9 (−0.7) 42.0 (−0.7) 35.1 (−0.3) 55.9 (−0.6) 37.7 (−0.4)\nMoCo IG-1B\n38.9 (+0.0) 59.4 (−0.2) 42.3 (−0.4) 35.4 (+0.0) 56.5 (+0.0) 37.9 (−0.2)\n(a) Mask R-CNN, R50-FPN, 1× schedule\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\n36.7\n56.7\n40.0\n33.7\n53.8\n35.9\n40.6\n61.3\n44.4\n36.8\n58.1\n39.5\n40.8 (+0.2) 61.6 (+0.3) 44.7 (+0.3) 36.9 (+0.1) 58.4 (+0.3) 39.7 (+0.2)\n41.1 (+0.5) 61.8 (+0.5) 45.1 (+0.7) 37.4 (+0.6) 59.1 (+1.0) 40.2 (+0.7)\n(b) Mask R-CNN, R50-FPN, 2× schedule\npre-train\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\nrandom init.\n26.4\n44.0\n27.8\n29.3\n46.9\n30.8\nsuper. IN-1M\n38.2\n58.2\n41.2\n33.3\n54.7\n35.2\nMoCo IN-1M\n38.5 (+0.3) 58.3 (+0.1) 41.6 (+0.4) 33.6 (+0.3) 54.8 (+0.1) 35.6 (+0.4)\nMoCo IG-1B\n39.1 (+0.9) 58.7 (+0.5) 42.2 (+1.0) 34.1 (+0.8) 55.4 (+0.7) 36.4 (+1.2)\n(c) Mask R-CNN, R50-C4, 1× schedule\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\n35.6\n54.6\n38.2\n31.4\n51.5\n33.5\n40.0\n59.9\n43.1\n34.7\n56.5\n36.9\n40.7 (+0.7) 60.5 (+0.6) 44.1 (+1.0) 35.4 (+0.7) 57.3 (+0.8) 37.6 (+0.7)\n41.1 (+1.1) 60.7 (+0.8) 44.8 (+1.7) 35.6 (+0.9) 57.4 (+0.9) 38.1 (+1.2)\n(d) Mask R-CNN, R50-C4, 2× schedule\nTable 5. Object detection and instance segmentation ﬁne-tuned on COCO: bounding-box AP (APbb) and mask AP (APmk) evaluated\non val2017. In the brackets are the gaps to the ImageNet supervised pre-training counterpart. In green are the gaps of at least +0.5 point.\nCOCO keypoint detection\npre-train\nAPkp\nAPkp\n50\nAPkp\n75\nrandom init.\n65.9\n86.5\n71.7\nsuper. IN-1M\n65.8\n86.9\n71.9\nMoCo IN-1M\n66.8 (+1.0)\n87.4 (+0.5)\n72.5 (+0.6)\nMoCo IG-1B\n66.9 (+1.1)\n87.8 (+0.9)\n73.0 (+1.1)\nCOCO dense pose estimation\npre-train\nAPdp\nAPdp\n50\nAPdp\n75\nrandom init.\n39.4\n78.5\n35.1\nsuper. IN-1M\n48.3\n85.6\n50.6\nMoCo IN-1M\n50.1 (+1.8)\n86.8 (+1.2)\n53.9 (+3.3)\nMoCo IG-1B\n50.6 (+2.3)\n87.0 (+1.4)\n54.3 (+3.7)\nLVIS v0.5 instance segmentation\npre-train\nAPmk\nAPmk\n50\nAPmk\n75\nrandom init.\n22.5\n34.8\n23.8\nsuper. IN-1M†\n24.4\n37.8\n25.8\nMoCo IN-1M\n24.1 (−0.3)\n37.4 (−0.4)\n25.5 (−0.3)\nMoCo IG-1B\n24.9 (+0.5)\n38.2 (+0.4)\n26.4 (+0.6)\nCityscapes instance seg.\nSemantic seg. (mIoU)\npre-train\nAPmk\nAPmk\n50\nCityscapes\nVOC\nrandom init.\n25.4\n51.1\n65.3\n39.5\nsuper. IN-1M\n32.9\n59.6\n74.6\n74.4\nMoCo IN-1M\n32.3 (−0.6)\n59.3 (−0.3)\n75.3 (+0.7) 72.5 (−1.9)\nMoCo IG-1B\n32.9 (+0.0)\n60.3 (+0.7)\n75.5 (+0.9) 73.6 (−0.8)\nTable 6. MoCo vs. ImageNet supervised pre-training, ﬁne-\ntuned on various tasks. For each task, the same architecture and\nschedule are used for all entries (see appendix). In the brackets are\nthe gaps to the ImageNet supervised pre-training counterpart. In\ngreen are the gaps of at least +0.5 point.\n†: this entry is with BN frozen, which improves results; see main text.\nwith ImageNet supervised pre-training:\nCOCO keypoint detection: supervised pre-training has\nno clear advantage over random initialization, whereas\nMoCo outperforms in all metrics.\nCOCO dense pose estimation [1]: MoCo substantially\noutperforms supervised pre-training, e.g., by 3.7 points in\nAPdp\n75, in this highly localization-sensitive task.\nLVIS v0.5 instance segmentation [27]:\nthis task has\n∼1000 long-tailed distributed categories.\nSpeciﬁcally in\nLVIS for the ImageNet supervised baseline, we ﬁnd ﬁne-\ntuning with frozen BN (24.4 APmk) is better than tunable\nBN (details in appendix). So we compare MoCo with the\nbetter supervised pre-training variant in this task. MoCo\nwith IG-1B surpasses it in all metrics.\nCityscapes instance segmentation [10]: MoCo with IG-1B\nis on par with its supervised pre-training counterpart in\nAPmk, and is higher in APmk\n50 .\nSemantic segmentation: On Cityscapes [10], MoCo out-\nperforms its supervised pre-training counterpart by up to 0.9\npoint. But on VOC semantic segmentation, MoCo is worse\nby at least 0.8 point, a negative case we have observed.\nSummary. In sum, MoCo can outperform its ImageNet\nsupervised pre-training counterpart in 7 detection or seg-\nmentation tasks.5 Besides, MoCo is on par on Cityscapes\ninstance segmentation, and lags behind on VOC semantic\nsegmentation; we show another comparable case on iNatu-\nralist [57] in appendix. Overall, MoCo has largely closed\nthe gap between unsupervised and supervised representa-\ntion learning in multiple vision tasks.\nRemarkably, in all these tasks, MoCo pre-trained on\nIG-1B is consistently better than MoCo pre-trained on\nIN-1M. This shows that MoCo can perform well on this\nlarge-scale, relatively uncurated dataset. This represents a\nscenario towards real-world unsupervised learning.\n5. Discussion and Conclusion\nOur method has shown positive results of unsupervised\nlearning in a variety of computer vision tasks and datasets.\nA few open questions are worth discussing. MoCo’s im-\nprovement from IN-1M to IG-1B is consistently noticeable\nbut relatively small, suggesting that the larger-scale data\nmay not be fully exploited. We hope an advanced pretext\ntask will improve this. Beyond the simple instance discrim-\nination task [61], it is possible to adopt MoCo for pretext\ntasks like masked auto-encoding, e.g., in language [12] and\nin vision [46]. We hope MoCo will be useful with other\npretext tasks that involve contrastive learning.\n5Namely, object detection on VOC/COCO, instance segmentation on\nCOCO/LVIS, keypoint detection on COCO, dense pose on COCO, and\nsemantic segmentation on Cityscapes.\nA. Appendix\nA.1. Implementation: Object detection backbones\nThe R50-dilated-C5 and R50-C4 backbones are similar\nto those available in Detectron2 [60]: (i) R50-dilated-\nC5: the backbone includes the ResNet conv5 stage with a\ndilation of 2 and stride 1, followed by a 3×3 convolution\n(with BN) that reduces dimension to 512. The box predic-\ntion head consists of two hidden fully-connected layers. (ii)\nR50-C4: the backbone ends with the conv4 stage, and the\nbox prediction head consists of the conv5 stage (including\nglobal pooling) followed by a BN layer.\nA.2. Implementation: COCO keypoint detection\nWe use Mask R-CNN (keypoint version) with R50-FPN,\nimplemented in [60], ﬁne-tuned on COCO train2017\nand evaluated on val2017. The schedule is 2×.\nA.3. Implementation: COCO dense pose estimation\nWe use DensePose R-CNN [1] with R50-FPN, imple-\nmented in [60], ﬁne-tuned on COCO train2017 and\nevaluated on val2017. The schedule is “s1×”.\nA.4. Implementation: LVIS instance segmentation\nWe use Mask R-CNN with R50-FPN, ﬁne-tuned in LVIS\n[27] train v0.5 and evaluated in val v0.5. We follow\nthe baseline in [27] (arXiv v3 Appendix B).\nLVIS is a new dataset and model designs on it are to be\nexplored. The following table includes the relevant abla-\ntions (all are averages of 5 trials):\n1× schedule\n2× schedule\npre-train\nBN\nAPmk APmk\n50\nAPmk\n75\nAPmk APmk\n50\nAPmk\n75\nsuper. IN-1M\nfrozen\n24.1\n37.3\n25.4\n24.4\n37.8\n25.8\nsuper. IN-1M\ntuned\n23.5\n36.6\n24.8\n23.2\n36.0\n24.4\nMoCo IN-1M\ntuned\n23.2\n36.0\n24.7\n24.1\n37.4\n25.5\nMoCo IG-1B\ntuned\n24.3\n37.4\n25.9\n24.9\n38.2\n26.4\nA supervised pre-training baseline, end-to-end tuned but\nwith BN frozen, has 24.4 APmk.\nBut tuning BN in this\nbaseline leads to worse results and overﬁtting (this is unlike\non COCO/VOC where tuning BN gives better or compara-\nble accuracy). MoCo has 24.1 APmk with IN-1M and 24.9\nAPmk with IG-1B, both outperforming the supervised pre-\ntraining counterpart under the same tunable BN setting. Un-\nder the best individual settings, MoCo can still outperform\nthe supervised pre-training case (24.9 vs. 24.4, as reported\nin Table 6 in Sec 4.2).\nA.5. Implementation: Semantic segmentation\nWe use an FCN-based [43] structure. The backbone con-\nsists of the convolutional layers in R50, and the 3×3 con-\nvolutions in conv5 blocks have dilation 2 and stride 1. This\nis followed by two extra 3×3 convolutions of 256 channels,\nwith BN and ReLU, and then a 1×1 convolution for per-\npixel classiﬁcation. The total stride is 16 (FCN-16s [43]).\nWe set dilation = 6 in the two extra 3×3 convolutions, fol-\nlowing the large ﬁeld-of-view design in [6].\nTraining is with random scaling (by a ratio in [0.5, 2.0]),\ncropping, and horizontal ﬂipping. The crop size is 513 on\nVOC and 769 on Cityscapes [6]. Inference is performed\non the original image size. We train with mini-batch size\n16 and weight decay 0.0001.\nLearning rate is 0.003 on\nVOC and is 0.01 on Cityscapes (multiplied by 0.1 at 70-\nth and 90-th percentile of training). For VOC, we train on\nthe train aug2012 set (augmented by [30], 10582 im-\nages) for 30k iterations, and evaluate on val2012. For\nCityscapes, we train on the train fine set (2975 images)\nfor 90k iterations, and evaluate on the val set. Results are\nreported as averages over 5 trials.\nA.6. iNaturalist ﬁne-grained classiﬁcation\nIn addition to the detection/segmentation experiments\nin the main paper, we study ﬁne-grained classiﬁcation on\nthe iNaturalist 2018 dataset [57].\nWe ﬁne-tune the pre-\ntrained models end-to-end on the train set (∼437k im-\nages, 8142 classes) and evaluate on the val set. Training\nfollows the typical ResNet implementation in PyTorch with\n100 epochs. Fine-tuning has a learning rate of 0.025 (vs.\n0.1 from scratch) decreased by 10 at the 70-th and 90-th\npercentile of training. The following is the R50 result:\npre-train\nrand init.\nsuper.IN-1M\nMoCoIN-1M\nMoCoIG-1B\naccuracy (%)\n61.8\n66.1\n65.6\n65.8\nMoCo is ∼4% better than training from random initializa-\ntion, and is closely comparable with its ImageNet super-\nvised counterpart. This again shows that MoCo unsuper-\nvised pre-training is competitive.\nA.7. Fine-tuning in ImageNet\nLinear classiﬁcation on frozen features (Sec. 4.1) is a\ncommon protocol of evaluating unsupervised pre-training\nmethods. However, in practice, it is more common to ﬁne-\ntune the features end-to-end in a downstream task.\nFor\ncompleteness, the following table reports end-to-end ﬁne-\ntuning results for the 1000-class ImageNet classiﬁcation,\ncompared with training from scratch (ﬁne-tuning uses an\ninitial learning rate of 0.03, vs. 0.1 from scratch):\npre-train\nrandom init.\nMoCoIG-1B\naccuracy (%)\n76.5\n77.3\nAs here ImageNet is the downstream task, the case of MoCo\npre-trained on IN-1M does not represent a real scenario\n(for reference, we report that its accuracy is 77.0% after\nﬁne-tuning). But unsupervised pre-training in the separate,\nunlabeled dataset of IG-1B represents a typical scenario: in\nthis case, MoCo improves by 0.8%.\npre-train\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\nrandom init.\n36.7\n56.7\n40.0\n33.7\n53.8\n35.9\nsuper. IN-1M\n40.6\n61.3\n44.4\n36.8\n58.1\n39.5\nMoCo IN-1M\n40.8 (+0.2) 61.6 (+0.3) 44.7 (+0.3) 36.9 (+0.1) 58.4 (+0.3) 39.7 (+0.2)\nMoCo IG-1B\n41.1 (+0.5) 61.8 (+0.5) 45.1 (+0.7) 37.4 (+0.6) 59.1 (+1.0) 40.2 (+0.7)\n(a) Mask R-CNN, R50-FPN, 2× schedule\nAPbb\nAPbb\n50\nAPbb\n75\nAPmk\nAPmk\n50\nAPmk\n75\n41.4\n61.9\n45.1\n37.6\n59.1\n40.3\n41.9\n62.5\n45.6\n38.0\n59.6\n40.8\n42.3 (+0.4) 62.7 (+0.2) 46.2 (+0.6) 38.3 (+0.3) 60.1 (+0.5) 41.2 (+0.4)\n42.8 (+0.9) 63.2 (+0.7) 47.0 (+1.4) 38.7 (+0.7) 60.5 (+0.9) 41.3 (+0.5)\n(b) Mask R-CNN, R50-FPN, 6× schedule\nTable A.1. Object detection and instance segmentation ﬁne-tuned on COCO: 2× vs. 6× schedule. In the brackets are the gaps to the\nImageNet supervised pre-training counterpart. In green are the gaps of at least +0.5 point.\nA.8. COCO longer ﬁne-tuning\nIn Table 5 we reported results of the 1× (∼12 epochs)\nand 2× schedules on COCO. These schedules were inher-\nited from the original Mask R-CNN paper [32], which could\nbe suboptimal given later advance in the ﬁeld. In Table A.1,\nwe supplement the results of a 6× schedule (∼72 epochs)\n[31] and compare with those of the 2× schedule.\nWe observe: (i) ﬁne-tuning with ImageNet-supervised\npre-training still has improvements (41.9 APbb); (ii) train-\ning from scratch largely catches up (41.4 APbb); (iii) the\nMoCo counterparts improve further (e.g., to 42.8 APbb) and\nhave larger gaps (e.g., +0.9 APbb with 6×, vs. +0.5 APbb\nwith 2×). Table A.1 and Table 5 suggest that the MoCo\npre-trained features can have larger advantages than the\nImageNet-supervised features when ﬁne-tuning longer.\nA.9. Ablation on Shufﬂing BN\nFigure A.1 provides the training curves of MoCo with\nor without shufﬂing BN: removing shufﬂing BN shows ob-\nvious overﬁtting to the pretext task: training accuracy of\nthe pretext task (dash curve) quickly increases to >99.9%,\nand the kNN-based validation classiﬁcation accuracy (solid\ncurve) drops soon. This is observed for both the MoCo and\nend-to-end variants; the memory bank variant implicitly has\ndifferent statistics for q and k, so avoids this issue.\nThese experiments suggest that without shufﬂing BN,\nthe sub-batch statistics can serve as a “signature” to tell\nwhich sub-batch the positive key is in. Shufﬂing BN can\nremove this signature and avoid such cheating.\n0\n20\n40\n60\n80\nepochs\n0\n50\n100\naccuracy (%)\nMoCo w/  ShuffleBN\nMoCo w/o ShuffleBN\nFigure A.1. Ablation of Shufﬂing BN. Dash: training curve of\nthe pretext task, plotted as the accuracy of (K+1)-way dictionary\nlookup. Solid: validation curve of a kNN-based monitor [61] (not\na linear classiﬁer) on ImageNet classiﬁcation accuracy. This plot\nshows the ﬁrst 80 epochs of training: training longer without shuf-\nﬂing BN overﬁts more.\nReferences\n[1] Rıza Alp G¨uler, Natalia Neverova, and Iasonas Kokkinos.\nDensePose: Dense human pose estimation in the wild. In\nCVPR, 2018.\n[2] Philip Bachman, R Devon Hjelm, and William Buchwalter.\nLearning representations by maximizing mutual information\nacross views. arXiv:1906.00910, 2019.\n[3] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning\nof visual features. In ECCV, 2018.\n[4] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Ar-\nmand Joulin. Unsupervised pre-training of image features\non non-curated data. In ICCV, 2019.\n[5] Ken Chatﬁeld, Victor Lempitsky, Andrea Vedaldi, and An-\ndrew Zisserman. The devil is in the details: an evaluation of\nrecent feature encoding methods. In BMVC, 2011.\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. DeepLab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected CRFs. TPAMI, 2017.\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. arXiv:2002.05709, 2020.\n[8] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nImproved baselines with momentum contrastive learning.\narXiv:2003.04297, 2020.\n[9] Adam Coates and Andrew Ng. The importance of encoding\nversus training with sparse coding and vector quantization.\nIn ICML, 2011.\n[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld,\nMarkus Enzweiler,\nRodrigo Benenson,\nUwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe Cityscapes\ndataset for semantic urban scene understanding. In CVPR,\n2016.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, 2009.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2019.\n[13] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nICCV, 2015.\n[14] Carl Doersch and Andrew Zisserman.\nMulti-task self-\nsupervised visual learning. In ICCV, 2017.\n[15] Jeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Ad-\nversarial feature learning. In ICLR, 2017.\n[16] Jeff Donahue and Karen Simonyan. Large scale adversarial\nrepresentation learning. arXiv:1907.02544, 2019.\n[17] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried-\nmiller, and Thomas Brox.\nDiscriminative unsupervised\nfeature learning with convolutional neural networks.\nIn\nNeurIPS, 2014.\n[18] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The Pascal Visual Ob-\nject Classes (VOC) Challenge. IJCV, 2010.\n[19] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-\nsupervised representation learning by predicting image rota-\ntions. In ICLR, 2018.\n[20] Ross Girshick. Fast R-CNN. In ICCV, 2015.\n[21] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In CVPR, 2014.\n[22] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr\nDoll´ar, and Kaiming He. Detectron, 2018.\n[23] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B\nGrosse. The reversible residual network: Backpropagation\nwithout storing activations. In NeurIPS, 2017.\n[24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In NeurIPS,\n2014.\n[25] Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He. Accurate, large minibatch\nSGD: Training ImageNet in 1 hour. arXiv:1706.02677, 2017.\n[26] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan\nMisra. Scaling and benchmarking self-supervised visual rep-\nresentation learning. In ICCV, 2019.\n[27] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLVIS: A\ndataset for large vocabulary instance segmentation. In CVPR,\n2019.\n[28] Michael Gutmann and Aapo Hyv¨arinen. Noise-contrastive\nestimation: A new estimation principle for unnormalized sta-\ntistical models. In AISTATS, 2010.\n[29] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension-\nality reduction by learning an invariant mapping. In CVPR,\n2006.\n[30] Bharath Hariharan, Pablo Arbel´aez, Lubomir Bourdev,\nSubhransu Maji, and Jitendra Malik. Semantic contours from\ninverse detectors. In ICCV, 2011.\n[31] Kaiming He, Ross Girshick, and Piotr Doll´ar. Rethinking\nImageNet pre-training. In ICCV, 2019.\n[32] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017.\n[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016.\n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks.\nIn ECCV,\n2016.\n[35] Olivier J H´enaff, Ali Razavi, Carl Doersch, SM Eslami, and\nAaron van den Oord. Data-efﬁcient image recognition with\ncontrastive predictive coding. arXiv:1905.09272, 2019. Up-\ndated version accessed at https://openreview.net/\npdf?id=rJerHlrYwH.\n[36] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,\nKaran Grewal, Adam Trischler, and Yoshua Bengio. Learn-\ning deep representations by mutual information estimation\nand maximization. In ICLR, 2019.\n[37] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In ICML, 2015.\n[38] Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Re-\nvisiting self-supervised visual representation learning.\nIn\nCVPR, 2019.\n[39] Yann LeCun, Bernhard Boser, John S Denker, Donnie\nHenderson, Richard E Howard, Wayne Hubbard, and\nLawrence D Jackel. Backpropagation applied to handwrit-\nten zip code recognition. Neural computation, 1989.\n[40] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and\nSungwoong Kim. Fast AutoAugment. arXiv:1905.00397,\n2019.\n[41] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyramid\nnetworks for object detection. In CVPR, 2017.\n[42] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014.\n[43] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation.\nIn\nCVPR, 2015.\n[44] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,\nKaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,\nand Laurens van der Maaten. Exploring the limits of weakly\nsupervised pretraining. In ECCV, 2018.\n[45] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In ECCV,\n2016.\n[46] Aaron van den Oord, Yazhe Li, and Oriol Vinyals.\nRep-\nresentation learning with contrastive predictive coding.\narXiv:1807.03748, 2018.\n[47] Deepak Pathak, Ross Girshick, Piotr Doll´ar, Trevor Darrell,\nand Bharath Hariharan. Learning features by watching ob-\njects move. In CVPR, 2017.\n[48] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros.\nContext encoders: Feature\nlearning by inpainting. In CVPR, 2016.\n[49] Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu\nZhang, Kai Jia, Gang Yu, and Jian Sun. MegDet: A large\nmini-batch object detector. In CVPR, 2018.\n[50] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018.\n[51] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. 2019.\n[52] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with re-\ngion proposal networks. In NeurIPS, 2015.\n[53] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In ICLR,\n2015.\n[54] Josef Sivic and Andrew Zisserman. Video Google: a text\nretrieval approach to object matching in videos. In ICCV,\n2003.\n[55] Bart Thomee, David A Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and\nLi-Jia Li. YFCC100M: The new data in multimedia research.\nCommunications of the ACM, 2016.\n[56] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nCon-\ntrastive multiview coding. arXiv:1906.05849, 2019. Updated\nversion accessed at https://openreview.net/pdf?\nid=BkgStySKPB.\n[57] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,\nChen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and\nSerge Belongie. The iNaturalist species classiﬁcation and\ndetection dataset. In CVPR, 2018.\n[58] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. Extracting and composing robust\nfeatures with denoising autoencoders. In ICML, 2008.\n[59] Xiaolong Wang and Abhinav Gupta. Unsupervised learning\nof visual representations using videos. In ICCV, 2015.\n[60] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019.\n[61] Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Un-\nsupervised feature learning via non-parametric instance dis-\ncrimination. In CVPR, 2018. Updated version accessed at:\nhttps://arxiv.org/abs/1805.01978v1.\n[62] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, 2017.\n[63] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Un-\nsupervised embedding learning via invariant and spreading\ninstance feature. In CVPR, 2019.\n[64] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful\nimage colorization. In ECCV, 2016.\n[65] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain\nautoencoders: Unsupervised learning by cross-channel pre-\ndiction. In CVPR, 2017.\n[66] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local\naggregation for unsupervised learning of visual embeddings.\nIn ICCV, 2019. Additional results accessed from supplemen-\ntary materials.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-11-13",
  "updated": "2020-03-23"
}