{
  "id": "http://arxiv.org/abs/1911.07072v1",
  "title": "Unsupervised Deep Metric Learning via Auxiliary Rotation Loss",
  "authors": [
    "Xuefei Cao",
    "Bor-Chun Chen",
    "Ser-Nam Lim"
  ],
  "abstract": "Deep metric learning is an important area due to its applicability to many\ndomains such as image retrieval and person re-identification. The main drawback\nof such models is the necessity for labeled data. In this work, we propose to\ngenerate pseudo-labels for deep metric learning directly from clustering\nassignment and we introduce unsupervised deep metric learning (UDML)\nregularized by a self-supervision (SS) task. In particular, we propose to\nregularize the training process by predicting image rotations. Our method\n(UDML-SS) jointly learns discriminative embeddings, unsupervised clustering\nassignments of the embeddings, as well as a self-supervised pretext task.\nUDML-SS iteratively cluster embeddings using traditional clustering algorithm\n(e.g., k-means), and sampling training pairs based on the cluster assignment\nfor metric learning, while optimizing self-supervised pretext task in a\nmulti-task fashion. The role of self-supervision is to stabilize the training\nprocess and encourages the model to learn meaningful feature representations\nthat are not distorted due to unreliable clustering assignments. The proposed\nmethod performs well on standard benchmarks for metric learning, where it\noutperforms current state-of-the-art approaches by a large margin and it also\nshows competitive performance with various metric learning loss functions.",
  "text": "Unsupervised Deep Metric Learning via Auxiliary Rotation Loss\nXuefei Cao\nBrown University\nxuefei_cao@brown.edu\nBor-Chun Chen\nUniversity of Maryland\nsirius@umd.edu\nSer-Nam Lim\nFacebook AI\nsernam@gmail.com\nAbstract\nDeep metric learning is an important area due to its appli-\ncability to many domains such as image retrieval and person\nre-identiﬁcation. The main drawback of such models is the\nnecessity for labeled data. In this work, we propose to gen-\nerate pseudo-labels for deep metric learning directly from\nclustering assignment and we introduce unsupervised deep\nmetric learning (UDML) regularized by a self-supervision\n(SS) task. In particular, we propose to regularize the training\nprocess by predicting image rotations. Our method (UDML-\nSS) jointly learns discriminative embeddings, unsupervised\nclustering assignments of the embeddings, as well as a self-\nsupervised pretext task. UDML-SS iteratively cluster embed-\ndings using traditional clustering algorithm (e.g., k-means),\nand sampling training pairs based on the cluster assignment\nfor metric learning, while optimizing self-supervised pretext\ntask in a multi-task fashion. The role of self-supervision is\nto stabilize the training process and encourages the model\nto learn meaningful feature representations that are not dis-\ntorted due to unreliable clustering assignments. The pro-\nposed method performs well on standard benchmarks for\nmetric learning, where it outperforms current state-of-the-art\napproaches by a large margin and it also shows competitive\nperformance with various metric learning loss functions.\n1. Introduction\nMetric learning methods aim to learn effective embed-\nding space where similar instances are mapped to nearby\npoints, while for samples coming from different classes, the\nembedding vectors are pushed apart. These methods explore\ndifferent loss functions and mining methods to measure the\nsimilarities between data points accurately and robustly. Un-\nlike traditional classiﬁcation tasks which focus on category-\nspeciﬁc concepts, metric learning aims to learn the general\nconcept of distance metrics [34]. With the recent success\nof deep neural networks in computer vision, deep metric\nlearning methods have shown impressive results. Deep met-\nric learning methods have applications in different domains,\nsuch as person re-identiﬁcation [22, 54], image retrieval\n[48, 19, 16], near-duplicate detection [56], zero-shot learn-\ning [5, 4, 52] and visual tracking [24, 31]. However, to\nobtain better performance, the training process often requires\nlarge-scale labeled data. Most of the ﬁne-grained datasets\nare especially expensive to annotate since annotators are re-\nquired to be domain experts [14]. Thus unsupervised deep\nmetric learning is becoming of great interest to the vision\ncommunity.\nA major goal of unsupervised representation learning\nis to learn similarities between images or weak category\ninformation without labeled instances [51]. Recent work\n[3, 2, 6] treats the classiﬁcation problem as a pretext task\nand explores the idea of updating the weights of models by\npredicting the cluster assignments. Iscen et al. [26] introduce\na fully unsupervised way to mine hard training samples. Ye\net al. [51] propose to sample positive pairs by using data\naugmentation and treat different instances as negative pairs.\nThe idea of using cluster assignment as pseudo-labels\nhas been studied in deep learning domain [3, 2, 6, 7] such\nas DeepCluster [6]. However, these studies mainly focus\non classiﬁcation tasks with pseudo-labels. In UDML-SS,\nwe propose to use pseudo-labels directly to generate sam-\nples for metric learning loss. By sampling the positive and\nnegative pairs based on the cluster assignments generated\nwith k-means, we are able to update weights of the unsu-\npervised metric learning model. The pseudo-labels are then\nre-computed given the new embedding vectors and this pro-\ncess iterates until the model converges.\nA challenge with clustering is that it tends to contain many\nunreliable assignments, which causes instability during train-\ning and difﬁculty of converges. Motivated by the recent\ndevelopment of self-supervised learning, we mitigate this\nproblem by adding an auxiliary, self-supervised loss to the\nmetric learning loss. This leads to more stable and accurate\ntraining because the dependency of the learned representa-\ntions on the quality of the clustering assignment is reduced.\nIn particular, we apply the state-of-the-art self-supervision\nmethod based on image rotation [15].\nOur contributions\nIn this work, we present an unsu-\npervised metric learning framework (UDML-SS). We pro-\npose a metric learning loss that is based on cluster assign-\n1\narXiv:1911.07072v1  [cs.CV]  16 Nov 2019\nUDML-SS\nCNN Feature \nExtractor (θ1)\nPseudo labels\nMetric Learning \nNetwork (θ3)\nMetric Learning \nLoss (LMS)\nRotation Prediction \nLoss (Lrot)\nTraining Images\nImage Rotation\nClustering \n(K-Means)\nRotation Prediction \nNetwork (θ2)\nRotation labels\nFigure 1. Unsupervised metric learning with rotation-based self-supervision. The red arrows indicate the network ﬂow for the rotation\nprediction task. For the rotation loss, all images are rotated by 0, 90, 180, and 270 degrees. and images are classiﬁed by the fully connected\nlayer according to their rotation degree. The black arrows indicate the network ﬂow for the metric learning loss. The pseudo-labels are\ninitiated by the cluster assignment of features extracted from pre-trained convnet. After the ﬁrst iteration, the features will be extracted\ndirectly from the fully connected layer of metric learning loss. We learn the parameters of neural networks, and the cluster assignments of\nthe resulting embedding vectors iteratively.\nments directly as well as combines the metric learning with\nself-supervised representation learning. It alternates between\nclustering the learned embedding vectors and updating the\nweights of the convnet by minimizing a loss function, which\nis a combination of metric learning loss and a self-supervised\nloss. For simplicity, we focus our study on k-means for the\nformer. For the latter, the image rotation prediction task [15]\nis chosen as our self-supervision task. UDML-SS is concep-\ntually simple and compatible with any metric learning loss\nfunctions, which we will show in Section 4. Our method\nis evaluated extensively on several benchmarks for metric\nlearning, where it outperforms current state-of-the-art unsu-\npervised metric learning approaches by a large margin, e.g.,\nimproving [51] by +8.5% Recall@1 on CUB200 [44], by\n+3.8% Recall@1 on Cars196 [29] and by +14.6% Recall@1\non Stanford Online Product (Product) [34]. Figure 1 shows\na conceptual pipeline of the proposed approach.\n2. Related Work\nMetric Learning\nWith the progress made in deep learn-\ning, many approaches have been proposed for supervised\ndeep metric learning. A lot of research effort has been de-\nvoted to designing new loss functions. Classical pair-based\nloss functions including contrastive loss [17, 23] and triplet\nloss [38, 8] are widely used in most existing metric learning\nmethods. Contrastive loss encourages samples from a posi-\ntive pair to be closer, and maximizes the distance between\na negative pair in the embedding space. Triplet loss deﬁnes\neach triplet by choosing a positive sample and a negative\nsample given the same anchor point. It aims to learn an\nembedding where the similarity of the negative sample plus\na given margin is lower than that of the positive one to the an-\nchor. Extended from triplet loss, quadruplets are also applied\nin recent work, such as histogram loss [43]. Other meth-\nods, such as lifted-structure [34], n-pair loss [40], angular\nloss [45], adapted triplet loss [53], multi-similarity [46]\nfocus on fully utilizing pairwise relations of all points in a\nbatch. Hard sample mining has also been widely adopted to\nproduce more robust models. Here, instead of sampling all\nnegative instances for an anchor point, the most challenging\nnegative instances are mined. To this end, Schroff et al. [38]\npropose semi-hard mining. They sample a negative example\nwithin the batch, such that it is close to the anchor point but\nfurther away from positives. Wu et al. [49] improve it by\nuniformly sampling negative instances weighted by their dis-\ntance. Ge et al. [13] introduce a new violate margin, which is\ncomputed dynamically over the constructed hierarchical tree.\nDuan et al. [11] introduce a deep adversarial metric learning\nframework to generate synthetic hard negatives from the ob-\nserved negative samples. To fully exploit information buried\nin all samples, Zheng et al. [57] performs linear interpolation\non embeddings to adaptively manipulate their hard levels so\nthat the metric is always challenged with proper difﬁculty.\nAll these metric learning methods are supervised with class\nlabels.\nSelf-supervised\nRepresentation\nLearning\nSelf-\nsupervised representation learning has been widely used in\ndifferent domains [9, 32, 27]. Self-supervised represen-\ntation learning utilizes only unlabeled data to formulate a\npretext learning task for which a target objective can be\nacquired without supervision. [9, 33] predict the relative\nposition of image patches to learn semantically relevant\ncontent. Larsson et al. [30] use colorization as a proxy\ntask. Giaris et al. [15] propose to rotate the image and\n2\npredict the rotation angle, which is a simple but yet effective\nmethod to achieve useful representations for downstream\nimage classiﬁcation and segmentation tasks.\nFeng et\nal. [12] introduce a split representation that contains both\nrotation related and unrelated part.\n[35, 20, 1] propose\nto train feature extractors by maximizing an estimate of\nthe mutual information (MI) between different views of\nthe data. Although these methods show state of the art\nperformance on the classiﬁcation task, it is unclear whether\nMI maximization is a good objective for learning good\nrepresentations in an unsupervised fashion [42]. Recently,\nHendrycks et al. [21] show self-supervised representation\nlearning can improve the robustness of the classiﬁcation\nmodel to label corruption.\nDeep Clustering\nClustering is a popular unsupervised\nlearning method. Caron et al. [6] proposes a scalable cluster-\ning approach for the unsupervised representation learning of\nvisual features. It iterates between clustering with k-means\nthe features generated by the deep nets and using a discrimi-\nnative loss to update the parameters by predicting the cluster\nassignments as pseudo-labels. In [6, 7], deep clustering idea\nis explored for general unsupervised feature learning, where\nthe main goal is to pre-train model without labels.\nUnsupervised Metric learning\nMost of the metric\nlearning methods are supervised with class labels. There\nhave been relatively fewer efforts devoted to unsupervised\nmetric learning. [3, 2] split the training set into different\ngroups based on complicated clustering scheme and utilize\ninduced classiﬁcation problem as a pretext task. Iscen et\nal. [26] introduce an unsupervised framework for hard train-\ning example mining which exploits the manifold distance to\nextract hard examples. Ye et al. [51] instead aim at learn-\ning data augmentation invariant features and explore the\ninstance-wise supervision. This method is related to another\nunsupervised learning method [50].\n3. Proposed Method\n3.1. Problem Formulation\nLet X denote the data space where we sample a set of un-\nlabeled data points X = [x1, x2, ..., xn]. Let fθ1 : X →W\nbe a mapping from the data space to a feature space, where\nwe have wi = fθ1(xi). f is usually represented by a convolu-\ntional neural network (CNN), e.g. the pre-trained Inception-\nV1 [41] on ImageNet. Mapping fθ1 learns a non-linear\ntransformation of the image into a deep feature space W.\nThe objective of metric learning is to learn a metric in the\nfeature space so that it can measure the visual similarity\ncorrectly based on different datasets. To learn the mapping\nfrom feature space to the embedding space, another function\ngθ2 : W →Z is appended to project feature vectors to em-\nbedding vectors. The embedding vector gθ2(wi) is usually\nnormalized to have a unit length for training stability [38].\nFinally, two mappings fθ1 and gθ2 are jointly learned (where\nthe feature extraction backbone is usually ﬁne-tuned) in such\na way that gθ2 ◦fθ1 maps images within same categories\n(positive pairs) close to one another and images in different\ncategories (negative pairs) far apart in the embedding space.\nThe similarity between two data points in the embedding\nspace is thus deﬁned as\nS(xi, xj) = ⟨gθ2(fθ1(xi)), gθ2(fθ1(xj))⟩.\n(1)\nSupervised metric learning approaches would use labeled\ndata points to construct a training set (T ) of positive and\nnegative pairs of items. And the network parameters are\nlearned by minimizing a speciﬁc loss function:\nθ1, θ2 = argmin\nθ1,θ2\nL(T , θ1, θ2).\n(2)\nOur goal is to learn an embedding space without manually\ndeﬁned labels.\n3.2. Pseudo-labels by clustering\nIn DeepCluster [6], the authors utilize a signal provided\nby the convolutional structure of the random convnet, as a\nprior to the input signal. To bootstrap this signal, they need\nto use a large amount of training samples, e.g. ImageNet\n[37] which contains 1.3M images uniformly distributed into\n1000 classes. The goal of their work is to pre-train the model\nwithout labels. However, in our work, we focus on the metric\nlearning task and study the signal provided by pre-trained\nmodels, which allows us to learn an embedding space even\nwith a few thousand samples. Using the pre-trained network\nis a common practice in deep metric learning [26, 51]. The\npre-trained convolutional neural network on ImageNet [37]\nclassiﬁcation task can usually provide decent signal for im-\nage retrievals [34]. Oh et al. [34] shows representations\nprovided by Inception-V1 [41] achieves reasonable perfor-\nmance on standard benchmarks of metric learning. The idea\nof this work is to exploit such pre-trained signal to bootstrap\nthe metric learning process in an unsupervised manner. For\nsimplicity, we use k-means to cluster the feature vectors\nprovided by the convnet and use the subsequent cluster as-\nsignments yi as pseudo-labels to initialize our proposed loss\nfunctions. After the initialization step, cluster reassignment\nis conducted on the embedding vectors instead of feature\nvectors.\n3.3. Multi-similarity Loss\nIn general, our method is compatible with any metric\nlearning loss function. Here we choose multi-similarity loss\n[46] because it shows the state of the art performance on\nsupervised metric learning. However, we also show some\nexperimental results using other popular metric learning loss\nfunctions in Section 4.\n3\nThere are two steps in the multi-similarity method [46].\nThe ﬁrst step is to mine hard sample pairs based on the\ncosine similarities between the corresponding embedding\nvectors. The goal of sampling hard examples is to speed\nup the training process and extract informative pairs. Let\n(x1, y1), (x2, y2), ..., (xn, yn) be the given instances, with yi\nas the pseudo-labels obtained from clustering assignments.\nLet xi be the anchor. Deﬁne (xi, xj) as a negative pair\nchosen as\nS(xi, xj) > min\nyh=yi S(xi, xh) −ϵ.\n(3)\nSimilarly, (xi, xk) is a positive pair chosen as\nS(xi, xk) < max\nyh̸=yi S(xi, xh) + ϵ.\n(4)\nS represents the similarity between two examples. ϵ > 0\ncontrols the margin. Denote the set of chosen positive and\nnegative pairs as Pi and Ni respectively. With the chosen\ntraining pairs, we can minimize the multi-similarity loss.\nThe multi-similarity (MS) is deﬁned as\nLMS =\nn\nX\ni=1\n( 1\nα log(1 +\nX\nl∈Pi\ne−α(Sil−λ))+\n1\nβ log(1 +\nX\nl∈Ni\neβ(Sil−λ)))\n(5)\nWe use the same α, β, λ and ϵ as in the original multi-\nsimilarity framework throughout our experiments.\n3.4. Predicting Image Rotations\nTo overcome the challenge of unreliable clustering assign-\nment, we aim to learn useful representations, independently\nof the quality of the clustering assignments. To this end, we\nexploit recent advancements in self-supervised approaches\nfor representation learning. For example, the network can\nbe trained on a pretext task including colorizing grayscale\nimages [55], image inpainting [36], image jigsaw puzzle\n[33], predicting image rotations [15]. We propose to add\na self-supervised task to our metric learning loss. In par-\nticular, in this work, we focus on the pretext task proposed\nin [15] where a model is trained to predict image rotations\n(0, 90, 180 and 270 degrees). They have shown state of\nthe art performance on standard evaluation benchmarks in\nself-supervised learning.\nGiven a set of n images x1, ..., xn, let xik for k =\n1, 2, 3, 4 be the rotated version (0, 90, 180, 270 degrees)\nof xi and zik = k. We learn the parameters θ1 of the fea-\nture extraction backbone, fθ1, jointly with parameters θ3 of\na mapping, hθ3, from features extractors to the predicted\nlabels of rotation classiﬁcation. Formally, we have\nLrot = 1\nn\n4\nX\nk=1\nn\nX\ni=1\nL(hθ3(fθ1(xik)), zik)\n(6)\nwhere L is the cross-entropy loss.\n3.5. Proposed Loss Function\nCombining (5) and (6) gives us the following loss func-\ntions:\nLUDML−SS = LMS(θ1, θ2, x, y) + ηLrot(θ1, θ3, x, z)\n(7)\nwhere x is the training example, y is pseudo-label given\nby clustering assignment and z is the label for the pretext\nrotation task. η > 0 is a tuning parameter that balances\nthe contributions of the metric learning loss and the rotation\nprediction loss. θ1 represents the parameter of feature ex-\ntraction backbone, while θ2 and θ3 are the parameters of the\nmetric learning layers and pretext task layers respectively.\nIn other words, we use a single feature extraction network\nwith two heads: one for metric learning and another for the\npretext task. Note that the input images for metric learn-\ning are not rotated because we believe that we should keep\nthe self-supervised pretext task separated to avoid “contam-\nination” from any unreliability in the cluster assignments.\nEmpirically we also ﬁnd that this choice achieves better per-\nformance. Given the loss function 7, UDML-SS iteratively\nlearns and clusters the embedding vectors.\n4. Experimental Results\n4.1. Datasets\nWe conduct experiments on three standard datasets for\nmetric learning: CUB-200-2011 [44], Cars-196 [29] and\nStanford Online Products [34]. We follow the same train-\ning/testing data split as [34].\n• The CUB-200-2011 (CUB) dataset has 11,788 images\nin total. We use the ﬁrst 100 categories (5,864 images)\nfor training and the remaining 100 categories for test-\ning.\n• The Cars-196 (Cars) dataset contains 16,185 images of\n196 classes of cars. The ﬁrst 98 model categories are\nused for training, and the rest for testing.\n• The Stanford Online Products dataset consists of\n120,053 images of 22,634 online products from\neBay.com. We use the ﬁrst 11,318 products (59,551\nimages) for training and the remaining 11,316 products\n(60,502 images) for testing.\n4.2. Experimental Settings\nOur method was implemented in PyTorch. We utilize\nInception-V1 [41] pre-trained on ImageNet [37] as the\nbackbone network, and ﬁne-tuned it for our task. We also\nshow experimental results on some other network architec-\ntures in Section 4.8. We add two separate fully connected\n4\nQuery\nRetrieval\nFigure 2. Retrieved images of sample queries on Cars196 dataset.\nThe positive (negative) retrieved results are framed in green (red).\nThe last two rows show the failure cases. Best viewed on a monitor\nzoomed in.\nlayers (512-dim) on the top of the network following the\nglobal pooling layer. The ﬁrst one is for the embedding layer\nand the second one is for rotation classiﬁcation. All the input\nimages were cropped to 227 × 227. During the training\nphase, we use random cropping with random horizontal mir-\nroring for data augmentation. In the testing phase, a single\ncenter-cropped image is the input for ﬁne-grained retrieval\nas in [46]. We use Adam [28] optimizer for all experiments\nFor each mini-batch, we follow the sampling strategy\nused in [46] and we randomly choose a certain number of\nclasses, and then sample M = 5 examples from each class for\nall datasets in our experiments. To implement the rotation\nloss, we rotate 16 images in the batch in all four considered\ndirections (0, 90, 180, and 270 degrees) and get a batch size\nof 64 (16 unique images) [15]. For CUB and Cars datasets,\nwe use 100 clusters (i.e. k = 100) while we show evaluations\nof Stanford Online Products with k = 1000, 10000, as it\ncontains a much larger number of samples. As shown in [51],\nthe pre-trained Inception-V1 performs better on the CUB\nand Products dataset than on the Cars dataset. Based on this\nfact, we set η = 0.5 for Cars so that the contribution of self-\nsupervision is increased to reduce the dependency of learned\nrepresentations on the quality of the cluster assignments,\nwhile for CUB and Products, we set η = 0.1. For more\ndetails on the performance of different η, see the appendix.\nFor all other hyperparameters, we use the values provided in\n[46]. The similarity is measured with cosine similarity.\nWe evaluate our method on image retrieval task by using\nthe standard performance metric: Recall@K. Given a query\nimage from the testing set, Recall@K is the probability that\nany correct matching occurs in the top-k retrieved images.\nWe also provide Normalized Mutual Information (NMI) to\nMethod\nR@1\nR@2\nR@4\nR@8\nNMI\nSupervised\nLifted [34]\n46.9\n59.8\n71.2\n81.5\n56.4\nAngular [45]\n53.6\n65.0\n75.3\n83.7\n61.0\nTriplet [47]\n35.9\n47.7\n59.1\n70.0\n49.8\nTriplet hard [38]\n40.6\n52.3\n64.2\n75.0\n53.4\nMulti-Sim [46]\n65.7\n77.0\n86.3\n91.2\n-\nUnsupervised\nExemplar [10]\n38.2\n50.3\n62.8\n75.0\n45.0\nNCE [50]\n39.2\n51.4\n63.7\n75.8\n45.1\nDeepCluster [6]\n42.9\n54.1\n65.6\n76.2\n53.0\nRot-Only [15]\n42.5\n55.8\n68.6\n79.4\n49.1\nMOM [26]\n45.3\n57.8\n68.6\n78.4\n55.0\nInstance [51]\n46.2\n59.0\n70.1\n80.2\n55.4\nUDML-SS\n54.7\n66.9\n77.4\n86.1\n61.4\nTable 1. Experimental results (%) on the CUB-200-2011 dataset in\ncomparison with other methods.\nmeasure the clustering performance of the testing dataset.\nNMI is deﬁned by the ratio of the mutual information of\nclusters and ground truth labels to the arithmetic mean of\ntheir entropy [39].\n4.3. Comparison with State-of-the-Art\nTables 1, 2, and 3 show quantitative results on the CUB-\n200-2011, Cars196, and Stanford Online Products datasets,\nrespectively. MOM [26] and Instance [51] are two most\nrecent state of art methods designed for unsupervised met-\nric learning and they also utilize Inception-V1 [41] as their\nbackbone. The performance of other state-of-the-art unsuper-\nvised methods (Exemplar [10], NCE [50], Deep Cluster [6]\nand Prediction Image Rotation [15]) on these three datasets\n[51] are also listed in these tables. For a fair comparison,\nwe list evaluations using the same number of clusters (k) for\nUDML-SS and DeepCluster [6] methods. We also provide\nthe performance of several popular supervised metric learn-\ning methods (Triplet [47], Triplet Hard [38], Lifted Struc-\nture [34], Angular [45], Multi-Similarity [46]) to show the\nrelative performance of unsupervised metric learning. Note\nthat only the multi-similarity work uses Inception-V2 [25].\nAs shown in Table 1, UDML-SS outperforms all compet-\ning methods with a large margin on CUB-200-2011 dataset.\nFor example, we have achieved an 11.8% and 8.5% increase\nof Recall@1 compared to DeepCluster [6] and the instance\nmethod [51], respectively. Some qualitative results on CUB\ndataset are shown in the appendix. In Table 2 and 3, we\nshow the results on Cars196 and Product datasets. We ob-\nserve UDML-SS achieves very competitive performance\nand outperforms all competing methods with a clear margin\non these two datasets. In particular, UDML-SS achieves a\n14.6% boost of Recall@1 for Product dataset (k = 10000)\nand a 3.8% boost of Recall@1 on Cars196 dataset. When\n5\nQuery\nRetrieval (k=1000)\nRetrieval (k=10000)\nFigure 3. Retrieved images of sample queries on Stanford Online Products dataset for k = 1000, 10000. The positive (negative) retrieved\nresults are framed in green (red).\nk = 1000 for Product dataset, which is much smaller than\nthe number of classes (11318), our method still outperforms\nthe competing method [51].\nIt is also noteworthy that the performance of UDML-SS\non CUB and Product datasets is much closer than expected to\nsome supervised methods. However, there is still a large gap\nbetween supervised and unsupervised method on Car. Figure\n2 shows some example queries and nearest neighbors on Car\nwith both successful and failure examples using UDML-SS.\nSome failure examples showed in Figure 2 look very similar\nto the query except logos, which is the common feature that\npeople use to discriminate different cars. In general, it is\nhard for the unsupervised method to detect these kinds of\nﬁne-grained differences and we leave it for future work. In\nFigure 3, we show a comparison of retrievals using different\nnumber of clusters on Product.\n4.4. Ablation Study\nWe also conduct ablation study of the proposed unsu-\npervised framework. Figure 4 shows the performance of\nUDML-SS with and without self-supervision loss. For all\nthree datasets, metric learning loss equipped with k-means\nalready achieves a decent performance. With additional rota-\ntion loss, the performance is further improved with a clear\nmargin. For the Car dataset, the performance difference is\nup to 7% of Recall@1 between our method with and without\nrotation loss, which shows the importance of the rotation loss\nto UDML-SS. It is surprising that UDML-SS without the\nrotation loss still achieves better results than [51] on CUB\nand Product datasets. We speculate it is because the pre-\ntrained Inception-V1 [41] itself provides a stronger signal to\nMethod\nR@1\nR@2\nR@4\nR@8\nNMI\nSupervised\nLifted [34]\n59.9\n70.4\n79.6\n87.0\n57.8\nAngular [45]\n71.3\n80.7\n87.0\n91.8\n62.4\nTriplet [47]\n45.1\n57.4\n69.7\n79.2\n52.9\nTriplet_hard [38]\n53.2\n65.4\n74.3\n83.6\n55.7\nMulti-Sim [46]\n84.1\n90.4\n94.0\n96.5\n-\nUnsupervised\nExemplar [10]\n36.5\n48.1\n59.2\n71.0\n35.4\nNCE [50]\n37.5\n48.7\n59.8\n71.5\n35.6\nDeepCluster [6]\n32.6\n43.8\n57.0\n69.5\n38.5\nRot-Only [15]\n33.3\n44.6\n56.4\n68.5\n32.7\nMOM [26]\n35.5\n48.2\n60.6\n72.4\n38.6\nInstance [51]\n41.3\n52.3\n63.6\n74.9\n35.8\nUDML-SS\n45.1\n56.1\n66.5\n75.7\n34.4\nTable 2. Experimental results (%) on the Cars196 dataset in com-\nparison with other methods.\nCUB and Product datasets than to the Car dataset. For more\ndetails about the performance of pre-trained Inception-V1\nnetwork, see [34].\n4.5. On Different Metric Learning Losses\nIn this section, we study the performance of UDML-SS\nusing different metric learning loss functions. We conduct all\nthe following experiments on the CUB dataset. It is interest-\ning to see that in Figure 4 performances for Contrastive Loss,\nBinomial Loss and Lifted Structure Loss are very similar and\nall of them outperform the competing methods. However,\nTriplet Loss [38] does not work well, possibly because the\n6\nMethod\nR@1\nR@10\nR@100\nNMI\nSupervised\nLifted [34]\n62.6\n80.9\n91.2\n87.2\nAngular [45]\n67.9\n83.2\n92.2\n87.8\nTriplet [47]\n53.9\n72.1\n85.7\n86.3\nTriplet hard [38]\n57.8\n75.3\n88.1\n86.7\nMulti-Sim [46]\n78.2\n90.5\n96.0\n-\nUnsupervised\nExemplar [10]\n45.0\n60.3\n75.2\n85.0\nNCE [50]\n46.6\n62.3\n76.8\n85.8\nDeepCluster [6]\n46.1\n61.1\n76.0\n85.3\nRot-Only [15]\n40.1\n54.6\n70.1\n82.7\nMOM [26]\n43.3\n57.2\n73.2\n84.4\nInstance [51]\n48.9\n64.0\n78.0\n86.0\nUDML-SS (k=1000)\n54.4\n70.0\n82.9\n86.5\nUDML-SS (k=10000)\n63.5\n78.0\n88.6\n88.4\nTable 3. Experimental results (%) on the Product dataset in com-\nparison with other methods.\nMethod\nR@1\nR@2\nR@4\nR@8\nNMI\nCUB\nInstance [51]\n46.2\n59.0\n70.1\n80.2\n55.4\nOurs (η = 0)\n51.7\n63.7\n74.6\n84.2\n59.0\nOurs\n54.7\n66.9\n77.4\n86.1\n61.4\nCars\nInstance [51]\n41.3\n52.3\n63.6\n74.9\n35.8\nOurs (η = 0)\n38.1\n48.2\n58.7\n69.1\n32.4\nOurs\n45.1\n56.1\n66.5\n75.7\n34.4\nMethod\nR@1\nR@100\nR@1000\nNMI\nProduct\nInstance [51]\n48.9\n64.0\n78.0\n86.0\nOurs (η = 0, k = 1000)\n53.2\n68.6\n82.0\n86.3\nOurs (k = 1000)\n54.4\n70.0\n82.9\n86.5\nOurs (η = 0, k = 10000)\n63.4\n77.4\n87.6\n88.4\nOurs (k = 10000)\n63.5\n78.0\n88.6\n88.4\nTable 4. Ablation results (%) on the CUB-200-2011, Cars196 and\nStanford Online Products datasets in comparison. The ﬁrst row for\neach dataset is from the state of art unsupervised metric learning\nmethod [51] while the second and the third row is the performance\nof UDML-SS without rotation loss, i.e. η = 0 and with rotation\nloss.\nsuccess of triplet loss depends more on the label correctness.\n4.6. On the Embedding Size\nFollowing [38], we study the performance of our pro-\nposed loss with different embedding sizes of {64, 128, 256,\n512, 1024}. As shown in Figure 5, the performance is in-\ncreased consistently with the embedding dimension except\nat 1024 on the CUB dataset. Our method with embedding\ndimension at 512 and 1024 performs similarly. We observe a\nrecall@1\nrecall@2\nrecall@4\nrecall@8\n45\n50\n55\n60\n65\n70\n75\n80\n85\nRecall@K (%)\nContrastive\nBinomial\nLiftedStructure\nTriplet\nInstance\nFigure 4. This Figure shows Recall@K of our method on CUB-\n200-2011 with different metric learning losses. For comparison,\nwe also add Instance [51] to the ﬁgure.\nsimilar pattern in the evaluations of Car and Product datasets.\nThese results are provided in the appendix.\nrecall@1\nrecall@2\nrecall@4\nrecall@8\n45\n50\n55\n60\n65\n70\n75\n80\n85\nRecall@K (%)\nDIM=64\nDIM=128\nDIM=256\nDIM=512\nDIM=1024\nInstance\nFigure 5. This Figure shows Recall@K of our method on CUB-\n200-2011 with different embedding vector size. The Instance [51]\nmethod is listed for comparison.\n4.7. Choosing the Number of Clusters\nIn this section, we measure the impact of the number of\nclusters (k) used in k-means on the performance of different\ndatasets. In Table 5, we show the results using 50, 100,\n250, 500, 1000 clusters. From the table for CUB dataset,\nwe can see from a wide range of number of clusters (e.g.\n50, 100, 250, 500), our method can achieve a better result\nthan the competing method [51]. However, when the cluster\nsize gets too large (e.g. k=1000), our method’s performance\ndrops below that of the competing method. UDML-SS using\ndifferent clusters on Car dataset shows similar performance\n7\nk\nR@1\nR@2\nR@4\nR@8\nNMI\nInstance [51]\n46.2\n59.0\n70.1\n80.2\n55.4\n50\n50.7\n63.0\n74.4\n84.1\n57.0\n100\n54.7\n66.9\n77.4\n86.1\n61.4\n250\n52.1\n64.5\n75.4\n83.9\n56.6\n500\n49.4\n61.4\n73.1\n83.3\n56.5\n1000\n46.2\n58.1\n70.4\n81.5\n52.3\nTable 5. This table shows evaluations of our method on CUB-200-\n2011 for various choices of k.\nMethod\nR@1\nR@2\nR@4\nR@8\nNMI\nInception-V1 [41]\n54.7\n66.9\n77.4\n86.1\n61.4\nInception-V2 [25]\n63.7\n75.0\n83.8\n90.1\n67.1\nResNet34 [18]\n59.0\n70.6\n80.4\n88.1\n63.4\nResNet50 [18]\n60.1\n71.6\n81.8\n88.5\n64.5\nResNet101 [18]\n61.8\n73.0\n82.6\n89.6\n65.5\nTable 6. Experimental results (%) on the CUB-200-2011 dataset\nwith different backbones.\ntrend as CUB dataset (see the appendix for details). For\nProduct dataset, we show the performance of UDML-SS\ncompared with DeepCluster [6] using the different number\nof clusters in Figure 6.\n2000\n4000\n6000\n8000\n10000\nK = number of clusters\n0\n10\n20\n30\n40\n50\n60\n70\nRecall@1 (%)\nDeepCluster\nUDML-SS\nFigure 6. This ﬁgure shows Recall@1 of our method compared\nwith DeepCluster on Product for various choices of k.\n4.8. On Different ConvNet Backbones\nHere we show the performance of UDML-SS using dif-\nferent feature extractions backbones. The experiment is\nconducted on CUB dataset. From Table 6, we observe that\nInception-V2 [25] and ResNet [18] can improve the perfor-\nmance by a large margin, while the performance difference\nbetween different ResNet is relatively small. It is notewor-\nthy that the performance of UDML-SS with Inception-V2\nis very close to that of supervised multi-similarity method\nMethods\nR@1\nR@10\nR@100\nNMI\nRandom\n18.4\n29.4\n46.0\n79.8\nExemplar [10]\n31.5\n46.7\n64.2\n82.9\nNCE [50]\n34.4\n49.0\n65.2\n84.1\nMOM [26]\n16.3\n27.6\n44.5\n80.6\nInstance [51]\n39.7\n54.9\n71.0\n84.7\nUDML-SS\n59.2\n73.9\n85.1\n87.6\nTable 7. Experimental results (%) on the Product dataset with\nrandom-initialized network.\n[46], which also uses Inception-V2.\n4.9. Learning from Scratch\nSo far, we showed that UDML-SS can perform very well\nwith a chosen pre-trained network, e.g. Inception-V1 [41].\nNow following [51], we evaluate the performance of UDML-\nSS using ResNet18 [18] without pre-training. Table 7 shows\nthe performance of UDML-SS on Product dataset. The\ncompeting methods’ performance are originally from [51].\nIt is impressive that our method with random-initialized\nnetwork can still outperform all other methods with a very\nlarge margin.\n5. Conclusion\nIn this work, we have presented a new unsupervised met-\nric learning framework (UDML-SS), which for the ﬁrst time,\ncombines clustering, self-supervised learning, and metric\nlearning. In particular, we iteratively cluster embedding\nvectors using k-means and update embedding vectors by\noptimizing a multi-task loss function, which considers both\nsimilarity learning task and image rotation prediction task.\nWe have demonstrated the effectiveness of the proposed\nframework on three popular benchmark datasets in metric\nlearning. UDML-SS obtains a new state-of-the-art perfor-\nmance on these datasets. In addition, we explore the perfor-\nmance of UDML-SS with various popular metric learning\nloss functions. We empirically show that UDML-SS obtains\nstate of the art performance even with a randomly initialized\nnetwork.\nReferences\n[1] Philip Bachman, R Devon Hjelm, and William Buchwalter.\nLearning representations by maximizing mutual information\nacross views. arXiv preprint arXiv:1906.00910, 2019.\n[2] Miguel A Bautista, Artsiom Sanakoyeu, and Bjorn Ommer.\nDeep unsupervised similarity learning using partially ordered\nsets. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7130–7139, 2017.\n[3] Miguel\nA\nBautista,\nArtsiom\nSanakoyeu,\nEkaterina\nTikhoncheva, and Bjorn Ommer. Cliquecnn: Deep unsuper-\nvised exemplar learning. In Advances in Neural Information\nProcessing Systems, pages 3846–3854, 2016.\n8\n[4] Maxime Bucher, Stéphane Herbin, and Frédéric Jurie. Hard\nnegative mining for metric learning based zero-shot classiﬁ-\ncation. In European Conference on Computer Vision, pages\n524–531. Springer, 2016.\n[5] Maxime Bucher, Stéphane Herbin, and Frédéric Jurie. Im-\nproving semantic embedding consistency by metric learning\nfor zero-shot classifﬁcation. In European Conference on\nComputer Vision, pages 730–746. Springer, 2016.\n[6] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning of\nvisual features. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 132–149, 2018.\n[7] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Ar-\nmand Joulin. Unsupervised pre-training of image features on\nnon-curated data. 2019.\n[8] De Cheng, Yihong Gong, Sanping Zhou, Jinjun Wang, and\nNanning Zheng. Person re-identiﬁcation by multi-channel\nparts-based cnn with improved triplet loss function. In Pro-\nceedings of the iEEE conference on computer vision and\npattern recognition, pages 1335–1344, 2016.\n[9] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\nvised visual representation learning by context prediction. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, pages 1422–1430, 2015.\n[10] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springen-\nberg, Martin Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with exemplar convolutional\nneural networks. IEEE transactions on pattern analysis and\nmachine intelligence, 38(9):1734–1747, 2015.\n[11] Yueqi Duan, Wenzhao Zheng, Xudong Lin, Jiwen Lu, and\nJie Zhou. Deep adversarial metric learning. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2780–2789, 2018.\n[12] Zeyu Feng, Chang Xu, and Dacheng Tao. Self-supervised\nrepresentation learning by rotation feature decoupling. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 10364–10374, 2019.\n[13] Weifeng Ge. Deep metric learning with hierarchical triplet\nloss. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), pages 269–285, 2018.\n[14] Timnit Gebru, Judy Hoffman, and Li Fei-Fei. Fine-grained\nrecognition in the wild: A multi-task domain adaptation ap-\nproach. In Proceedings of the IEEE International Conference\non Computer Vision, pages 1349–1358, 2017.\n[15] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsu-\npervised representation learning by predicting image rotations.\narXiv preprint arXiv:1803.07728, 2018.\n[16] Alexander Grabner, Peter M Roth, and Vincent Lepetit. 3d\npose estimation and 3d model retrieval for objects in the wild.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3022–3031, 2018.\n[17] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension-\nality reduction by learning an invariant mapping. In 2006\nIEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’06), volume 2, pages 1735–1742.\nIEEE, 2006.\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[19] Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang\nBai. Triplet-center loss for multi-view 3d object retrieval. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 1945–1954, 2018.\n[20] Olivier J Hénaff, Ali Razavi, Carl Doersch, SM Eslami,\nand Aaron van den Oord.\nData-efﬁcient image recogni-\ntion with contrastive predictive coding.\narXiv preprint\narXiv:1905.09272, 2019.\n[21] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and\nDawn Song.\nUsing self-supervised learning can im-\nprove model robustness and uncertainty.\narXiv preprint\narXiv:1906.12340, 2019.\n[22] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In\ndefense of the triplet loss for person re-identiﬁcation. arXiv\npreprint arXiv:1703.07737, 2017.\n[23] Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Discriminative\ndeep metric learning for face veriﬁcation in the wild. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1875–1882, 2014.\n[24] Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Deep metric learn-\ning for visual tracking. IEEE Transactions on Circuits and\nSystems for Video Technology, 26(11):2056–2068, 2015.\n[25] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[26] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondˇrej\nChum. Mining on manifolds: Metric learning without labels.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 7642–7651, 2018.\n[27] Eric Jang, Coline Devin, Vincent Vanhoucke, and Sergey\nLevine. Grasp2vec: Learning object representations from\nself-supervised grasping. arXiv preprint arXiv:1811.06964,\n2018.\n[28] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014.\n[29] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d\nobject representations for ﬁne-grained categorization. In Pro-\nceedings of the IEEE International Conference on Computer\nVision Workshops, pages 554–561, 2013.\n[30] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich.\nLearning representations for automatic colorization. In Eu-\nropean Conference on Computer Vision, pages 577–593.\nSpringer, 2016.\n[31] Laura Leal-Taixé, Cristian Canton-Ferrer, and Konrad\nSchindler. Learning by tracking: Siamese cnn for robust\ntarget association. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition Workshops, pages\n33–40, 2016.\n[32] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-\nHsuan Yang. Unsupervised representation learning by sorting\nsequences. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 667–676, 2017.\n[33] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In European\nConference on Computer Vision, pages 69–84. Springer, 2016.\n[34] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\nSavarese.\nDeep metric learning via lifted structured fea-\n9\nture embedding. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 4004–4012,\n2016.\n[35] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018.\n[36] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros. Context encoders: Feature learn-\ning by inpainting. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2536–2544,\n2016.\n[37] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211–252, 2015.\n[38] Florian Schroff, Dmitry Kalenichenko, and James Philbin.\nFacenet: A uniﬁed embedding for face recognition and clus-\ntering. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 815–823, 2015.\n[39] Hinrich Schütze, Christopher D Manning, and Prabhakar\nRaghavan. Introduction to information retrieval. In Proceed-\nings of the international communication of association for\ncomputing machinery conference, page 260, 2008.\n[40] Kihyuk Sohn. Improved deep metric learning with multi-\nclass n-pair loss objective. In Advances in Neural Information\nProcessing Systems, pages 1857–1865, 2016.\n[41] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1–9, 2015.\n[42] Michael Tschannen, Josip Djolonga, Paul K Rubenstein,\nSylvain Gelly, and Mario Lucic. On mutual information\nmaximization for representation learning.\narXiv preprint\narXiv:1907.13625, 2019.\n[43] Evgeniya Ustinova and Victor Lempitsky. Learning deep\nembeddings with histogram loss. In Advances in Neural\nInformation Processing Systems, pages 4170–4178, 2016.\n[44] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona,\nand Serge Belongie. The caltech-ucsd birds-200-2011 dataset.\n2011.\n[45] Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and Yuanqing\nLin. Deep metric learning with angular loss. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 2593–2601, 2017.\n[46] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and\nMatthew R Scott. Multi-similarity loss with general pair\nweighting for deep metric learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 5022–5030, 2019.\n[47] Kilian Q Weinberger and Lawrence K Saul. Distance met-\nric learning for large margin nearest neighbor classiﬁcation.\nJournal of Machine Learning Research, 10(Feb):207–244,\n2009.\n[48] Paul Wohlhart and Vincent Lepetit. Learning descriptors for\nobject recognition and 3d pose estimation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3109–3118, 2015.\n[49] Chao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp\nKrahenbuhl. Sampling matters in deep embedding learning.\nIn Proceedings of the IEEE International Conference on Com-\nputer Vision, pages 2840–2848, 2017.\n[50] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 3733–3742,\n2018.\n[51] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang.\nUnsupervised embedding learning via invariant and spreading\ninstance feature. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 6210–6219,\n2019.\n[52] Sasi Kiran Yelamarthi, Shiva Krishna Reddy, Ashish Mishra,\nand Anurag Mittal. A zero-shot framework for sketch based\nimage retrieval. In European Conference on Computer Vision,\npages 316–333. Springer, 2018.\n[53] Baosheng Yu, Tongliang Liu, Mingming Gong, Changxing\nDing, and Dacheng Tao. Correcting the triplet selection bias\nfor triplet loss. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 71–87, 2018.\n[54] Rui Yu, Zhiyong Dou, Song Bai, Zhaoxiang Zhang, Yongchao\nXu, and Xiang Bai. Hard-aware point-to-set deep metric for\nperson re-identiﬁcation. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 188–204,\n2018.\n[55] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful\nimage colorization. In European conference on computer\nvision, pages 649–666. Springer, 2016.\n[56] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfel-\nlow. Improving the robustness of deep neural networks via\nstability training. In Proceedings of the ieee conference on\ncomputer vision and pattern recognition, pages 4480–4488,\n2016.\n[57] Wenzhao Zheng, Zhaodong Chen, Jiwen Lu, and Jie Zhou.\nHardness-aware deep metric learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 72–81, 2019.\n6. Appendix\nIn this appendix, we show more detailed experimental\nresults to support our paper. We add more details of ex-\nperiments on three standard datasets for metric learning:\nCUB-200-2011 [44], Cars-196 [29] and Stanford Online\nProducts [34].\n6.1. On the Embedding Size\nIn Table 8 and 9, we show the experimental results of\nUDML-SS on Car and Product datasets with different em-\nbedding sizes.\n6.2. Choosing the Number of Clusters\nIn Figure 8 and 9, we show how k, i.e. the number of\nclusters affect our evaluations.\n10\nQuery\nRetrieval\nFigure 7. Retrieved images of sample queries on CUB-200-2011\ndataset. The positive (negative) retrieved results are framed in green\n(red). The last two rows show the failure cases. Best viewed on a\nmonitor zoomed in.\nEmbedding Size\nR@1\nR@2\nR@4\nR@8\n64\n36.0\n45.6\n56.7\n67.4\n128\n41.2\n50.8\n61.6\n72.0\n256\n43.2\n53.4\n63.7\n73.8\n512\n45.1\n56.1\n66.5\n75.7\n1024\n46.1\n56.2\n66.4\n75.6\nTable 8. Recall@K (%) of UDML-SS on Car dataset with different\nembedding vector size.\nEmbedding Size\nR@1\nR@2\nR@100\n64\n62.5\n77.2\n88.2\n128\n62.9\n77.7\n88.4\n256\n63.3\n77.9\n88.6\n512\n63.5\n78.0\n88.6\n1024\n63.5\n77.9\n88.5\nTable 9. Recall@K (%) of UDML-SS on Product dataset with\ndifferent embedding vector size.\n6.3. How η affects evaluation results\nThis section, we show the evaluation results on CUB,\nCar and Product datasets of UDML-SS using different η in\nFigure 10, 11 and 12.\nrecall@1\nrecall@2\nrecall@4\nrecall@8\ncar\n40\n45\n50\n55\n60\n65\n70\n75\nRecall@K (%)\nk=50\nk=100\nk=250\nk=500\nk=1000\nFigure 8. Performance under different k i.e. number of clusters of\nUDML-SS on Car dataset\nrecall@1\nrecall@10\nrecall@100\nproduct\n55\n60\n65\n70\n75\n80\n85\n90\nRecall@K (%)\nk=1000\nk=2000\nk=4000\nk=7000\nk=9000\nk=10000\nFigure 9. Performance under different k i.e. number of clusters of\nUDML-SS on Product dataset\nrecall@1\nrecall@2\nrecall@4\nrecall@8\ncub\n50\n55\n60\n65\n70\n75\n80\n85\nRecall@K (%)\n0.01\n0.05\n0.1\n0.5\nFigure 10. Performance under different η of UDML-SS on CUB\ndataset\n11\nrecall@1\nrecall@2\nrecall@4\nrecall@8\ncar\n40\n45\n50\n55\n60\n65\n70\n75\nRecall@K (%)\n0.01\n0.05\n0.1\n0.5\nFigure 11. Performance under different η of UDML-SS on Car\ndataset\nrecall@1\nrecall@10\nrecall@100\nproduct\n60\n65\n70\n75\n80\n85\n90\nRecall@K (%)\n0.01\n0.05\n0.1\n0.5\nFigure 12. Performance under different η of UDML-SS on Product\ndataset\n12\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-11-16",
  "updated": "2019-11-16"
}