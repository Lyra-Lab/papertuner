{
  "id": "http://arxiv.org/abs/1911.07247v1",
  "title": "Hebbian Synaptic Modifications in Spiking Neurons that Learn",
  "authors": [
    "Peter L. Bartlett",
    "Jonathan Baxter"
  ],
  "abstract": "In this paper, we derive a new model of synaptic plasticity, based on recent\nalgorithms for reinforcement learning (in which an agent attempts to learn\nappropriate actions to maximize its long-term average reward). We show that\nthese direct reinforcement learning algorithms also give locally optimal\nperformance for the problem of reinforcement learning with multiple agents,\nwithout any explicit communication between agents. By considering a network of\nspiking neurons as a collection of agents attempting to maximize the long-term\naverage of a reward signal, we derive a synaptic update rule that is\nqualitatively similar to Hebb's postulate. This rule requires only simple\ncomputations, such as addition and leaky integration, and involves only\nquantities that are available in the vicinity of the synapse. Furthermore, it\nleads to synaptic connection strengths that give locally optimal values of the\nlong term average reward. The reinforcement learning paradigm is sufficiently\nbroad to encompass many learning problems that are solved by the brain. We\nillustrate, with simulations, that the approach is effective for simple pattern\nclassification and motor learning tasks.",
  "text": "arXiv:1911.07247v1  [cs.LG]  17 Nov 2019\nHebbian Synaptic Modiﬁcations in Spiking\nNeurons that Learn\nPeter L. Bartlett and Jonathan Baxter\nResearch School of Information Sciences and Engineering\nAustralian National University\nPeter.Bartlett@anu.edu.au, Jonathan.Baxter@anu.edu.au\nNovember 27, 1999\nAbstract\nIn this paper, we derive a new model of synaptic plasticity, based on re-\ncent algorithms for reinforcement learning (in which an agent attempts to\nlearn appropriate actions to maximize its long-term average reward). We\nshow that these direct reinforcement learning algorithms also give locally\noptimal performance for the problem of reinforcement learning with mul-\ntiple agents, without any explicit communication between agents. By con-\nsidering a network of spiking neurons as a collection of agents attempting\nto maximize the long-term average of a reward signal, we derive a synap-\ntic update rule that is qualitatively similar to Hebb’s postulate. This rule\nrequires only simple computations, such as addition and leaky integration,\nand involves only quantities that are available in the vicinity of the synapse.\nFurthermore, it leads to synaptic connection strengths that give locally op-\ntimal values of the long term average reward. The reinforcement learning\nparadigm is sufﬁciently broad to encompass many learning problems that\nare solved by the brain. We illustrate, with simulations, that the approach is\neffective for simple pattern classiﬁcation and motor learning tasks.\n1\nWhat is a good synaptic update rule?\nIt is widely accepted that the functions performed by neural circuits are modi-\nﬁed by adjustments to the strength of the synaptic connections between neurons.\n1\nIn the 1940s, Donald Hebb speculated that such adjustments are associated with\nsimultaneous (or nearly simultaneous) ﬁring of the presynaptic and postsynaptic\nneurons [14]:\nWhen an axon of cell A ... persistently takes part in ﬁring [cell B],\nsome growth process or metabolic change takes place [to increase]\nA’s efﬁcacy as one of the cells ﬁring B.\nAlthough this postulate is rather vague, it provides the important suggestion that\nthe computations performed by neural circuits could be modiﬁed by a simple cel-\nlular mechanism. Many candidates for Hebbian synaptic update rules have been\nsuggested, and there is considerable experimental evidence of such mechanisms\n(see, for instance, [7, 23, 16, 17, 19, 21]).\nHebbian modiﬁcations to synaptic strengths seem intuitively reasonable as a\nmechanism for modifying the function of a neural circuit. However, it is not clear\nthat these synaptic updates actually improve the performance of a neural circuit\nin any useful sense. Indeed, simulation studies of speciﬁc Hebbian update rules\nhave illustrated some serious shortcomings (see, for example, [20]).\nIn contrast with the “plausibility of cellular mechanisms” approach, most ar-\ntiﬁcial neural network research has emphasized performance in practical applica-\ntions. Synaptic update rules for artiﬁcial neural networks have been devised that\nminimize a suitable cost function. Update rules such as the backpropagation al-\ngorithm [22] (see [12] for a more detailed treatment) perform gradient descent in\nparameter space: they modify the connection strengths in a direction that max-\nimally decreases the cost function, and hence leads to a local minimum of that\nfunction. Through appropriate choice of the cost function, these parameter op-\ntimization algorithms have allowed artiﬁcial neural networks to be applied (with\nconsiderable success) to a variety of pattern recognition and predictive modelling\nproblems.\nUnfortunately, there is little evidence that the (rather complicated) computa-\ntions required for the synaptic update rule in parameter optimization procedures\nlike the backpropagation algorithm can be performed in biological neural circuits.\nIn particular, these algorithms require gradient signals to be propagated backwards\nthrough the network.\nThis paper presents a synaptic update rule that provably optimizes the per-\nformance of a neural network, but requires only simple computations involving\nsignals that are readily available in biological neurons. This synaptic update rule\nis consistent with Hebb’s postulate.\n2\nRelated update rules have been proposed in the past. For instance, the updates\nused in the adaptive search elements (ASEs) described in [4, 2, 1, 3] are of a sim-\nilar form (see also [25]). However, it is not known in what sense these update\nrules optimize performance. The update rule we present here is based on similar\nfoundations to the REINFORCE class of algorithms introduced by Williams [27].\nHowever, when applied to spiking neurons such as those described here, REIN-\nFORCE leads to parameter updates in the steepest ascent direction in two limited\nsituations: when the reward depends only on the current input to the neuron and\nthe neuron outputs do not affect the statistical properties of the inputs, and when\nthe reward depends only on the sequence of inputs since the arrival of the last\nreward value. Furthermore, in both cases the parameter updates must be carefully\nsynchronized with the timing of the reward values, which is especially problem-\natic for networks with more than one layer of neurons.\nIn Section 2, we describe reinforcement learning problems, in which an agent\naims to maximize the long-term average of a reward signal. Reinforcement learn-\ning is a useful abstraction that encompasses many diverse learning problems, such\nas supervised learning for pattern classiﬁcation or predictive modelling, time se-\nries prediction, adaptive control, and game playing. We review the direct rein-\nforcement learning algorithm we proposed in [5] and show in Section 3 that, in\nthe case of multiple independent agents cooperating to optimize performance, the\nalgorithm conveniently decomposes in such a way that the agents are able to learn\nindependently with no need for explicit communication.\nIn Section 4, we consider a network of model neurons as a collection of agents\ncooperating to solve a reinforcement learning problem, and show that the direct\nreinforcement learning algorithm leads to a simple synaptic update rule, and that\nthe decomposition property implies that only local information is needed for the\nupdates. Section 5 discusses possible mechanisms for the synaptic update rule in\nbiological neural networks.\nThe parsimony of requiring only one simple mechanism to optimize param-\neters for many diverse learning problems is appealing (cf [26]). In Section 6,\nwe present results of simulation experiments, illustrating the performance of this\nupdate rule for pattern recognition and adaptive control problems.\n2\nReinforcement learning\n‘Reinforcement learning’ refers to a general class of learning problems in which\nan agent attempts to improve its performance at some task. For instance, we\n3\nmight want a robot to sweep the ﬂoor of an ofﬁce; to guide the robot, we provide\nfeedback in the form of occasional rewards, perhaps depending on how much dust\nremains on the ﬂoor. This section explains how we can formally deﬁne this class\nof problems and shows that it includes as special cases many conventional learning\nproblems. It also reviews a general-purpose learning method for reinforcement\nlearning problems.\nWe can model the interactions between an agent and its environment mathe-\nmatically as a partially observable Markov decision process (POMDP). Figure 1\nillustrates the features of a POMDP. At each (discrete) time step t, the agent and\nthe environment are in a particular state xt in a state space S. For our clean-\ning robot, xt might include the agent’s location and orientation, together with the\nlocation of dust and obstacles in the ofﬁce. The state at time t determines an\nobservation vector yt (from some set Y) that is seen by the agent. For instance\nin the cleaning example, yt might consist of visual information available at the\nagent’s current location. Since observations are typically noisy, the relationship\nbetween the state and the corresponding observation is modelled as a probability\ndistribution ν(xt) over observation vectors. Notice that the probability distribution\ndepends on the state.\nWhen the agent sees an observation vector yt, it decides on an action ut from\nsome set U of available actions. In the ofﬁce cleaning example, the available\nactions might consist of directions in which to move or operations of the robot’s\nbroom.\nA mapping from observations to actions is referred to as a policy. We allow\nthe agent to choose actions using a randomized policy. That is, the observation\nvector yt determines a probability distribution µ(yt) over actions, and the action is\nchosen randomly according to this distribution. We are concerned with random-\nized policies that depend on a vector θ ∈Rk of k parameters (and we write the\nprobability distribution over actions as µ(yt, θ)).\nThe agent’s actions determine the evolution of states, possibly in a probabilis-\ntic way. To model this, each action determines the probabilities of transitions\nfrom the current state to possible subsequent states. For a ﬁnite state space S,\nwe can write these probabilities as a transition probability matrix, P(ut). Here,\nthe i, j-the entry of P(ut) (pij(ut)) is the probability of making a transition from\nstate i to state j given that the agent took action ut in state i. In the ofﬁce, the\nactions chosen by the agent determine its location and orientation and the location\nof dust and obstacles at the next time instant, perhaps with some random element\nto model the probability that the agent slips or bumps into an obstacle.\nFinally, in every state, the agent receives a reward signal rt, which is a real\n4\nObservation\nyt\nReward\nrt\nState transition,\nP (ut).\nState,\nxt\nObservation process,\nν(xt).\nReward process.\n✛\n✛\n✛\nEnvironment\nParameters,\nθ.\nPolicy,\nµ(θ, yt).\n✻\nAgent\n.\n✲\n✲\n✛\nAction, ut\nFigure 1: Partially observable Markov decision process (POMDP).\nnumber. For the cleaning agent, the reward might be zero most of the time, but\ntake a positive value when the agent removes some dust.\nThe aim of the agent is to choose a policy (that is, the parameters that deter-\nmine the policy) so as to maximize the long-term average of the reward,\nη = lim\nT→∞E\n\"\n1\nT\nT\nX\nt=1\nrt\n#\n.\n(1)\n(Here, E is the expectation operator.) This problem is made more difﬁcult by the\nlimited information that is available to the agent. We assume that at each time step\nthe agent sees only the observations yt and the reward rt (and is aware of its policy\nand the actions ut that it chooses to take). It has no knowledge of the underlying\nstate space, how the actions affect the evolution of states, how the reward signals\ndepend on the states, or how the observations depend on the states.\n2.1\nOther learning tasks viewed as reinforcement learning\nClearly, the reinforcement learning problem described above provides a good\nmodel of adaptive control problems, such as the acquisition of motor skills. How-\n5\never, the class of reinforcement learning problems is broad, and includes a number\nof other learning problems that are solved by the brain. For instance, the super-\nvised learning problems of pattern recognition and predictive modelling require\nlabels (such as an appropriate classiﬁcation) to be associated with patterns. These\nproblems can be viewed as reinforcement learning problems with reward signals\nthat depend on the accuracy of each predicted label. Time series prediction, the\nproblem of predicting the next item in a sequence, can be viewed in the same way,\nwith a reward signal that corresponds to the accuracy of the prediction. More\ngeneral ﬁltering problems can also be viewed in this way. It follows that a single\nmechanism for reinforcement learning would sufﬁce for the solution of a consid-\nerable variety of learning problems.\n2.2\nDirect reinforcement learning\nA general approach to reinforcement learning problems was presented recently\nin [5, 6]. Those papers considered agents that use parameterized policies, and\nintroduced general-purpose reinforcement learning algorithms that adjust the pa-\nrameters in the direction that maximally increases the average reward. Such algo-\nrithms converge to policies that are locally optimal, in the sense that any further\nadjustment to the parameters in any direction cannot improve the policy’s perfor-\nmance. This section reviews the algorithms introduced in [5, 6]. The next two\nsections show how these algorithms can be applied to networks of spiking neu-\nrons.\nThe direct reinforcement learning approach presented in [5], building on ideas\ndue to a number of authors [27, 9, 10, 15, 18], adjusts the parameters θ of a\nrandomized policy that, on being presented with the observation vector yt, chooses\nactions according to a probability distribution µ(yt, θ). The approach involves the\ncomputation of a vector zt of k real numbers (one component for each component\nof the parameter vector θ) that is updated according to\nzt+1 = βzt + ∇µut(yt, θ)\nµut(yt, θ) ,\n(2)\nwhere β is a real number between 0 and 1, µut(yt, θ) is the probability of the\naction ut under the current policy, and ∇denotes the gradient with respect to the\nparameters θ (so ∇µut(yt, θ) is a vector of k partial derivatives). The vector zt is\nused to update the parameters, and can be thought of as an average of the ‘good’\ndirections in parameter space in which to adjust the parameters if a large value\n6\nof reward occurs at time t. The ﬁrst term in the right-hand-side of (2) ensures\nthat zt remembers past values of the second term. The numerator in the second\nterm is in the direction in parameter space which leads to the maximal increase\nof the probability of the action ut taken at time t. This direction is divided by the\nprobability of ut to ensure more “popular” actions don’t end up dominating the\noverall update direction for the parameters. Updates to the parameters correspond\nto weighted sums of these normalized directions, where the weighting depends on\nfuture values of the reward signal.\nTheorems 3 and 6 in [5] show that if θ remains constant, the long-term average\nof the product rtzt is a good approximation to the gradient of the average reward\nwith respect to the parameters, provided β is sufﬁciently close to 1. It is clear from\nEquation (2) that as β gets closer to 1, zt depends on measurements further back\nin time. Theorem 4 in [5] shows that, for a good approximation to the gradient of\nthe average reward, it sufﬁces if 1/(1 −β), the time constant in the update of zt,\nis large compared with a certain time constant—the mixing time—of the POMDP.\n(It is useful, although not quite correct, to think of the mixing time as the time\nfrom the occurrence of an action until the effects of that action have died away.)\nThis gives a simple way to compute an appropriate direction to update the pa-\nrameters θ. An on-line algorithm (OLPOMDP) was presented in [6] that updates\nthe parameters θ according to\nθt = θt−1 + γrtzt,\n(3)\nwhere the small positive real number γ is the size of the steps taken in parameter\nspace. If these steps are sufﬁciently small, so that the parameters change slowly,\nthis update rule modiﬁes the parameters in the direction that maximally increases\nthe long-term average of the reward.\n3\nDirect reinforcement learning with independent\nagents\nSuppose that, instead of a single agent, there are n independent agents, all co-\noperating to maximize the average reward (see Figure 2). Suppose that each of\nthese agents sees a distinct observation vector, and has a distinct parameterized\nrandomized policy that depends on its own set of parameters. This multi-agent\nreinforcement learning problem can also be modelled as a POMDP by consid-\nering this collection of agents as a single agent, with an observation vector that\n7\nReward\nrt\nState transition,\nP (ut).\nObservation\nprocesses.\nReward process.\nState,\nxt\n✛\n✛\n✛\nEnvironment\n✻\nPolicy\nParameters\n✻\nPolicy\nParameters\n.\n✛\n✲\n✲\n✲\n✲\n✲\n✲\nAction, un\nt\nAction, u1\nt\nObservation, y1\nt\nObservation, yn\nt\n.\n.\n.\nAgent 1\nAgent n\nFigure 2: POMDP controlled by n independent agents.\nconsists of the n observation vectors of each independent agent, and similarly for\nthe parameter vector and action vector. For example, if the n agents are coop-\nerating to clean the ﬂoor in an ofﬁce, the state vector would include the location\nand orientation of the n agents, the observation vector for agent i might consist\nof the visual information available at that agent’s current location, and the actions\nchosen by all n agents determine the state vector at the next time instant. The\nfollowing decomposition theorem follows from a simple calculation.\nTheorem 1. For a POMDP controlled by multiple independent agents, the direct\nreinforcement learning update equations (2) and (3) for the combined agent are\nequivalent to those that would be used by each agent if it ignored the existence of\nthe other agents.\nThat is, if we let yi\nt denote the observation vector for agent i, ui\nt denote the\naction it takes, and θi denote its parameter vector, then the update equation (3) is\n8\nequivalent to the system of n update equations,\nθi\nt = θi\nt−1 + γrtzi\nt,\n(4)\nwhere the vectors z1\nt , . . . , zn\nt ∈Rk are updated according to\nzi\nt+1 = βzi\nt +\n∇µui\nt (yi\nt, θi)\nµui\nt (yi\nt, θi) .\n(5)\nHere, ∇denotes the gradient with respect to the agent’s parameters θi.\nEffectively, each agent treats the other agents as a part of the environment,\nand can update its own behaviour while remaining oblivious to the existence of\nthe other agents. The only communication that occurs between these cooperating\nagents is via the globally distributed reward, and via whatever inﬂuence agents’\nactions have on other agents’ observations. Nonetheless, in the space of param-\neters of all n agents, the updates (4) adjust the complete parameter vector (the\nconcatenation of the vectors θi) in the direction that maximally increases the aver-\nage reward. We shall see in the next section that this convenient property leads to\na synaptic update rule for spiking neurons that involves only local quantities, plus\na global reward signal.\n4\nDirect reinforcement learning in neural networks\nThis section shows how we can model a neural network as a collection of agents\nsolving a reinforcement learning problem, and apply the direct reinforcement\nlearning algorithm to optimize the parameters of the network. The networks we\nconsider contain simple models of spiking neurons (see Figure 3). We consider\ndiscrete time, and suppose that each neuron in the network can choose one of two\nactions at time step t: to ﬁre, or not to ﬁre. We represent these actions with the\nnotation ut = 1 and ut = 0, respectively1. We use a simple probabilistic model\nfor the behaviour of the neuron. Deﬁne the potential vt in the neuron at time t as\nvt =\nX\nj\nwjuj\nt−1,\n(6)\n1The actions can be represented by any two distinct real values, such as ut ∈{±1}. An\nessentially identical derivation gives a similar update rule.\n9\nConnection\nstrength,\nwj\nPresynaptic\nactivity,\nuj ∈{0, 1}\nPotential,\nv = P\nj wjuj\nActivity,\nu ∈{0, 1},\nPr(u = 1) = σ(v)\nSynapse\nFigure 3: Model of a neuron.\nwhere wj is the connection strength of the jth synapse and uj\nt−1 is the activity at\nthe previous time step of the presynaptic neuron at the jth synapse. The potential\nv represents the voltage at the cell body (the postsynaptic potentials having been\ncombined in the dendritic tree). The probability of activity in the neuron is a\nfunction of the potential v. A squashing function σ maps from the real-valued\npotential to a number between 0 and 1, and the activity ut obeys the following\nprobabilistic rule.\nPr (neuron ﬁres at time t) = Pr (ut = 1) = σ (vt) .\n(7)\nWe assume that the squashing function satisﬁes σ(α) = 1/(1 + e−α).\nWe are interested in computation in networks of these spiking neurons, so we\nneed to specify the network inputs, on which the computation is performed, and\nthe network outputs, where the results of the computation appear. To this end,\nsome neurons in the network are distinguished as input neurons, which means\ntheir activity ut is provided as an external input to the network. Other neurons are\ndistinguished as output neurons, which means their activity represents the result\nof a computation performed by the network.\nA real-valued global reward signal rt is broadcast to every neuron in the net-\nwork at time t. We view each (non-input) neuron as an independent agent in a\nreinforcement learning problem. The agent’s (neuron’s) policy is simply how it\nchooses to ﬁre given the activities on its presynaptic inputs. The synaptic strengths\n(wj) are the adjustable parameters of this policy. Theorem 1 shows how to update\nthe synaptic strengths in the direction that maximally increases the long-term av-\nerage of the reward. In this case, we have\n∂\n∂wj µut\nµut\n=\n\n\n\n\n\nσ′(vt)uj\nt−1\nσ(vt)\nif ut = 1,\n−σ′(vt)uj\nt−1\n1−σ(vt)\notherwise\n10\n=\n(ut −σ (vt)) uj\nt−1,\nwhere the second equality follows from the property of the squashing function,\nσ′(α) = σ(α) (1 −σ(α)) .\nThis results in an update rule for the j-th synaptic strength of\nwj,t+1 = wj,t + γrt+1zj,t+1,\n(8)\nwhere the real numbers zj,t are updated according to\nzj,t+1 = βzj,t + (ut −σ (vt)) uj\nt−1.\n(9)\nThese equations describe the updates for the parameters in a single neuron. The\npseudocode in Algorithm 1 gives a complete description of the steps involved\nin computing neuron activities and synaptic modiﬁcations for a network of such\nneurons.\nAlgorithm 1 Model of neural network activity and synaptic modiﬁcation.\n1: Given:\nCoefﬁcient β ∈[0, 1),\nStep size γ,\nInitial synaptic connection strengths of the i-th neuron wi\nj,0.\n2: for time t = 0, 1, . . . do\n3:\nSet activities uj\nt of input neurons.\n4:\nfor non-input neurons i do\n5:\nCalculate potential vi\nt+1 = P\nj wi\nj,tuj\nt.\n6:\nGenerate activity ui\nt+1 ∈{0, 1} using Pr\n\u0000ui\nt+1 = 1\n\u0001\n= σ\n\u0000vi\nt+1\n\u0001\n.\n7:\nend for\n8:\nObserve reward rt+1 (which depends on network outputs).\n9:\nfor non-input neurons i do\n10:\nSet zi\nj,t+1 = βzi\nj,t+1 + (ui\nt −σ (vi\nt)) uj\nt−1.\n11:\nSet wi\nj,t+1 = wi\nj,t + γrt+1zi\nj,t+1.\n12:\nend for\n13: end for\nSuitable values for the quantities β and γ required by Algorithm 1 depend on\nthe mixing time of the controlled POMDP. The coefﬁcient β sets the decay rate of\n11\nthe variable zt. For the algorithm to accurately approximate the gradient direction,\nthe corresponding time constant, 1/(1 −β), should be large compared with the\nmixing time of the environment. The step size γ affects the rate of change of\nthe parameters. When the parameters are constant, the long term average of rtzt\napproximates the gradient. Thus, the step size γ should be sufﬁciently small so\nthat the parameters are approximately constant over a time scale that allows an\naccurate estimate. Again, this depends on the mixing time. Loosely speaking,\nboth 1/(1 −β) and 1/γ should be signiﬁcantly larger than the mixing time.\n5\nBiological Considerations\nIn modifying the strength of a synaptic connection, the update rule described by\nEquations (8) and (9) involves two components (see Figure 4). There is a Hebbian\ncomponent (utuj\nt−1) that helps to increase the synaptic connection strength when\nﬁring of the postsynaptic neuron follows ﬁring of the presynaptic neuron. When\nthe ﬁring of the presynaptic neuron is not followed by postsynaptic ﬁring, this\ncomponent is 0, while the second component (−σ (vt) uj\nt−1) helps to decrease the\nsynaptic connection strength.\nThe update rule has several attractive properties.\nLocality The modiﬁcation of a particular synapse wj involves the postsynaptic\npotential v, the postsynaptic activity u, and the presynaptic activity uj at the\nprevious time step.\nCertainly the postsynaptic potential is available at the synapse. Action po-\ntentials in neurons are transmitted back up the dendritic tree [24], so that\n(after some delay) the postsynaptic activity is also available at the synapse.\nSince the inﬂuence of presynaptic activity on the postsynaptic potential is\nmediated by receptors at the synapse, evidence of presynaptic activity is also\navailable at the synapse. While Equation (9) requires information about the\nhistory of presynaptic activity, there is some evidence for mechanisms that\nallow recent receptor activation to be remembered [19, 21]. Hence, all of\nthe quantities required for the computation of the variable zj are likely to be\navailable in the postsynaptic region.\nSimplicity The computation of zj in (9) involves only additions and subtractions\nmodulated by the presynaptic and postsynaptic activities, and combined in\na simple ﬁrst order ﬁlter. This ﬁlter is a leaky integrator which models, for\n12\n(a)\n(b)\n(d)\n(e)\n(f)\n(g)\n(j)\n(k)\n(h)\n(l)\nr\nr\n(i)\n(c)\nr\nFigure 4: An illustration of synaptic updates. The presynaptic neuron is on the\nleft, the postsynaptic on the right. The level inside the square in the postsynaptic\nneuron represents the quantity zj. (The dashed line indicates the zero value.) The\nsymbol r represents the presence of a positive value of the reward signal, which\nis assumed to take only two values here. (The postsynaptic location of zj and r is\nfor convenience in the depiction, and has no other signiﬁcance.) The size of the\nsynapse represents the connection strength. Time proceeds from top to bottom.\n(a)–(d): A sequence through time illustrating changes in the synapse when no ac-\ntion potentials occur. In this case, zj steadily decays ((a)–(b)) towards zero, and\nwhen a reward signal arrives (c), the strength of the synaptic connection is not sig-\nniﬁcantly adjusted. (e)–(h): Presynaptic action potential (e), but no postsynaptic\naction potential (f) leads to a larger decrease in zj (g), and subsequent decrease in\nconnection strength on arrival of the reward signal (h). (i)–(l): Presynaptic action\npotential (i), followed by postsynaptic action potential (j) leads to an increase in\nzj (k) and subsequent increase in connection strength (l).\n13\ninstance, such common features as the concentration of ions in some re-\ngion of a cell or the potential across a membrane. Similarly, the connection\nstrength updates described by Equation (8) involve simply the addition of a\nterm that is modulated by the reward signal.\nOptimality The results from [5], together with Theorem 1, show that this simple\nupdate rule modiﬁes the network parameters in the direction that maximally\nincreases the average reward, so it leads to parameter values that locally\noptimize the performance of the network.\nThere are some experimental results that are consistent with the involvement\nof the correlation component (the term (ut −σ(vt))uj\nt−1) in the parameter up-\ndates. For instance, a large body of literature on long-term potentiation (beginning\nwith [7]) describes the enhancement of synaptic efﬁcacy following association of\npresynaptic and postsynaptic activities. More recently, the importance of the rel-\native timing of the EPSPs and APs has been demonstrated [19, 21]. In particular,\nthe postsynaptic ﬁring must occur after the EPSP for enhancement to occur. The\nbackpropagation of the action potential up the dendritic tree appears to be crucial\nfor this [17].\nThere is also experimental evidence that presynaptic activity without the gen-\neration of an action potential in the postsynaptic cell can lead to a decrease in\nthe connection strength [23]. The recent ﬁnding [19, 21] that an EPSP occurring\nshortly after an AP can lead to depression is also consistent with this aspect of\nHebbian learning. However, in the experiments reported in [19, 21], the presence\nof the AP appeared to be important. It is not clear if the signiﬁcance of the relative\ntimings of the EPSPs and APs is related to learning or to maintaining stability in\nbidirectionally coupled cells.\nFinally, some experiments have demonstrated a decrease in synaptic efﬁcacy\nwhen the synapses were not involved in the production of an action potential [16].\nThe update rule also requires a reward signal that is broadcast to all neurons\nin the network. In all of the experiments mentioned above, the synaptic modi-\nﬁcations were observed without any evidence of the presence of a plausible re-\nward signal. However, there is limited evidence for such a signal in brains. It\ncould be delivered in the form of particular neurotransmitters, such as serotonin\nor nor-adrenaline, to all neurons in a circuit. Both of these neurotransmitters are\ndelivered to the cortex by small cell assemblies (the raphe nucleus and the locus\ncoeruleus, respectively) that innervate large regions of the cortex. The fact that\nthese assemblies contain few cell bodies suggests that they carry only limited in-\nformation. It may be that the reward signal is transmitted ﬁrst electrically from\n14\none of these cell assemblies, and then by diffusion of the neurotransmitter to all of\nthe plastic synaptic connections in a neural circuit. This would save the expense\nof a synapse delivering the reward signal to every plastic connection, but could be\nsigniﬁcantly slower. This need not be a disadvantage; for the purposes of param-\neter optimization, the required rate of delivery of the reward signal depends on\nthe time constants of the task, and can be substantially slower than cell signalling\ntimes. There is evidence that the local application of serotonin immediately after\nlimited synaptic activity can lead to long term facilitation [11].\n6\nSimulation Results\nIn this section, we describe the results of simulations of Algorithm 1 for a pattern\nclassiﬁcation problem and an adaptive control problem. In all simulation experi-\nments, we used a symmetric representation, u ∈{−1, 1}. The difference between\nthis representation and the assymmetric u ∈{0, 1} is a simple transformation of\nthe parameters, but this can be signiﬁcant for gradient descent procedures.\n6.1\nSonar signal classiﬁcation\nAlgorithm 1 was applied to the problem of sonar return classiﬁcation studied by\nGorman and Sejnowski [13]. (The data set is available from the U. C. Irvine\nrepository [8].) Each pattern consists of 60 real numbers in the range [0, 1], rep-\nresenting the energy in various frequency bands of a sonar signal reﬂected from\none of two types of underwater objects, rocks and metal cylinders. The data set\ncontains 208 patterns, 97 labeled as rocks and 111 as cylinders. We investigated\nthe performance of a two-layer network of spiking neurons on this task. The ﬁrst\nlayer of 8 neurons received the vector of 60 real numbers as inputs, and a single\noutput neuron received the outputs of these neurons. This neuron’s output at each\ntime step was viewed as the prediction of the label corresponding to the pattern\npresented at that time step. The reward signal was 0 or 1, for an incorrect or cor-\nrect prediction, respectively. The parameters of the algorithm were β = 0.5 and\nγ = 10−4. Weights were initially set to random values uniformly chosen in the\ninterval (−0.1, 0.1). Since it takes two time steps for the inﬂuence of the hidden\nunit parameters to affect the reward signal, it is essential for the value of β for\nthe synapses in a hidden layer neuron to be positive. It can be shown that for a\nconstant pattern vector, the optimal choice of β for these synapses is 0.5.\n15\n0\n10\n20\n30\n40\n50\n60\n0\n50\n100\n150\n200\n250\n300\nerror (%)\ntraining epochs\nTraining error\nTest error\nGorman and Sejnowski: training error\nGorman and Sejnowski: test error\nFigure 5: Learning curves for the sonar classiﬁcation problem.\nEach time the input pattern changed, the delay through the network meant that\nthe prediction corresponding to the new pattern was delayed by one time step.\nBecause of this, in the experiments each pattern was presented for many time\nsteps before it was changed.\nFigure 5 shows the mean and standard deviation of training and test errors over\n100 runs of the algorithm plotted against the number of training epochs. Each run\ninvolved an independent random split of the data into a test set (10%) and a train-\ning set (90%). For each training epoch, patterns in the training set were presented\nto the network for 1000 time steps each. The errors were calculated as the pro-\nportion of misclassiﬁcations during one pass through the data, with each pattern\npresented for 1000 time steps. Clearly, the algorithm reliably leads to parameter\nsettings that give training error around 10%, without passing any gradient infor-\nmation through the network.\nGorman and Sejnowski [13] investigated the performance of sigmoidal neural\nnetworks on this data. Although the networks they used were quite different (since\nthey involved deterministic units with real-valued outputs), the training error and\ntest error they reported for a network with 6 hidden units is also illustrated in\nFigure 5.\n16\nFigure 6: The inverted pendulum.\n6.2\nControlling an inverted pendulum\nWe also considered a problem of learning to balance an inverted pendulum. Fig-\nure 6 shows the arrangement: a puck moves in a square region. On the top of\nthe puck is a weightless rod with a weight at its tip. The puck has no internal\ndynamics.\nWe investigated the performance of Algorithm 1 on this problem. We used\na network with four hidden units, each receiving real numbers representing the\nposition and velocity of the puck and the angle and angular velocity of the pendu-\nlum. These units were connected to two more units, whose outputs were used to\ncontrol the sign of two 10N thrusts applied to the puck in the two axis directions.\nThe reward signal was 0 when the pendulum was upright, and −1 when it hit the\nground. Once the pendulum hit the ground, the puck was randomly located near\nthe centre of the square with velocity zero, and the pendulum was reset to vertical\nwith zero angular velocity.\nIn the simulation, the square was 5×5 metres, the dynamics were simulated in\ndiscrete time, with time steps of 0.02s, the puck bounced elastically off the walls,\ngravity was 9.8ms−2, the puck radius was 50mm, the puck height was 0, the puck\nmass was 1kg, air resistance was neglected, the pendulum length was 500mm, the\npendulum mass was 100g, the coefﬁcient of friction of the puck on the ground\nwas 5 × 10−4, and friction at the pendulum joint was set to zero.\nThe algorithm parameters were γ = 10−6 and β = 0.995.\n17\n0\n20\n40\n60\n80\n100\n120\n0\n5e+06\n1e+07\n1.5e+07\n2e+07\nseconds\niterations\naverage time between failure\nFigure 7: A typical learning curve for the inverted pendulum problem.\nFigure 7 shows a typical learning curve: the average time before the pendu-\nlum falls (in a simulation of 100000 iterations = 2000 seconds), as a function\nof total simulated time. Initial weights were chosen uniformly from the interval\n(−0.05, 0.05).\n7\nFurther work\nThe most interesting questions raised by these results are concerned with possible\nbiological mechanisms for update rules of this type. Some aspects of the update\nrule are supported by experimental results. Others, such as the reward signal, have\nnot been investigated experimentally. One obvious direction for this work is the\ndevelopment of update rules for more realistic models of neurons. First, the model\nassumes discrete time. Second, it ignores some features that biological neurons\nare known to possess. For instance, the location of synapses in the dendritic tree\nallow timing relationships between action potentials in different presynaptic cells\nto affect the resulting postsynaptic potential. Other features of dendritic process-\ning, such as nonlinearities, are also ignored by the model presented here. It is\nnot clear which of these features are important for the computational properties of\nneural circuits.\n18\n8\nConclusions\nThe synaptic update rule presented in this paper requires only simple computa-\ntions involving only local quantities plus a global reward signal. Furthermore, it\nadjusts the synaptic connection strengths to locally optimize the average reward\nreceived by the network. The reinforcement learning paradigm encompasses a\nconsiderable variety of learning problems. Simulations have shown the effective-\nness of the algorithm for a simple pattern classiﬁcation problem and an adaptive\ncontrol problem.\nReferences\n[1] A. G. Barto, C. W. Anderson, and R. S. Sutton.\nSynthesis of nonlinear\ncontrol surfaces by a layered associative search network. Biological Cyber-\nnetics, 43:175–185, 1982.\n[2] A. G. Barto and R. S. Sutton. Landmark learning: An illustration of associa-\ntive search. Biological Cybernetics, 42:1–8, 1981.\n[3] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements\nthat can solve difﬁcult learning control problems. IEEE Transactions on\nSystems, Man, and Cybernetics, SMC-13:834–846, 1983.\n[4] A. G. Barto, R. S. Sutton, and P. S. Brouwer.\nAssociative search net-\nwork: A reinforcement learning associative memory. Biological Cybernet-\nics, 40:201–211, 1981.\n[5] J. Baxter and P. L. Bartlett. On some algorithms for inﬁnite-horizon policy-\ngradient estimation. Journal of Artiﬁcial Intelligence Research, 14, March\n2001.\n[6] J. Baxter, P. L. Bartlett, and L. Weaver.\nGradient-ascent algorithms and\nexperiments with inﬁnite-horizon, policy-gradient estimation.\nJournal of\nArtiﬁcial Intelligence Research, 14, March 2001.\n[7] T. V. Bliss and T. Lomo. Long-lasting potentiation of synaptic transmission\nin the dentate area of the anaesthetized rabbit following stimulation of the\nperforant path. Journal of Physiology (London), 232:331–356, 1973.\n19\n[8] E. K. C. Blake and C. Merz. UCI repository of machine learning databases,\n1998. http://www.ics.uci.edu/∼mlearn/MLRepository.html.\n[9] X.-R. Cao and H.-F. Chen. Perturbation Realization, Potentials, and Sen-\nsitivity Analysis of Markov Processes.\nIEEE Transactions on Automatic\nControl, 42:1382–1393, 1997.\n[10] X.-R. Cao and Y.-W. Wan. Algorithms for Sensitivity Analysis of Markov\nChains Through Potentials and Perturbation Realization. IEEE Transactions\non Control Systems Technology, 6:482–492, 1998.\n[11] G. A. Clark and E. R. Kandel. Induction of long-term facilitation in Aplysia\nsensory neurons by local application of serotonin to remote synapses.\nProc. Natl. Acad. Sci. USA, 90:11411–11415, 1993.\n[12] T. L. Fine. Feedforward Neural Network Methodology. Springer, New York,\n1999.\n[13] R. P. Gorman and T. J. Sejnowski. Analysis of hidden units in a layered\nnetwork trained to classify sonar targets. Neural Networks, 1:75–89, 1988.\n[14] D. O. Hebb. The Organization of Behavior. Wiley, New York, 1949.\n[15] H. Kimura, K. Miyazaki, and S. Kobayashi.\nReinforcement learning in\nPOMDPs with function approximation.\nIn D. H. Fisher, editor, Pro-\nceedings of the Fourteenth International Conference on Machine Learning\n(ICML’97), pages 152–160, 1997.\n[16] Y. Lo and M. ming Poo. Activity-dependent synaptic competition in vitro:\nHeterosynaptic suppression of developing synapses.\nScience, 254:1019–\n1022, 1991.\n[17] J. C. Magee and D. Johnston. A synaptically controlled, associative signal\nfor Hebbian plasticity in hippocampal neurons. Science, 275:209–213, 1997.\n[18] P. Marbach and J. N. Tsitsiklis. Simulation-Based Optimization of Markov\nReward Processes. Technical report, MIT, 1998.\n[19] H. Markram, J. L¨ubke, M. Frotscher, and B. Sakmann.\nRegulation of\nsynaptic efﬁcacy by coincidence of postsynaptic APs and EPSPs. Science,\n275:213–215, 1997.\n20\n[20] J. F. Medina and M. D. Mauk.\nSimulations of cerebellar motor learn-\ning: Computational analysis of plasticity at the mossy ﬁber to deep nucleus\nsynapse. The Journal of Neuroscience, 19(16):7140–7151, 1999.\n[21] G. qiang Bi and M. ming Poo. Synaptic modiﬁcations in cultured hippocam-\npal neurons: dependence on spike timing, synaptic strength, and postsynap-\ntic cell type. The Journal of Neuroscience, 18(24):10464–10472, 1998.\n[22] D. Rumelhart, G. Hinton, and R. Williams. Learning representations by\nback-propagating errors. Nature, 323:533–536, 1986.\n[23] P. K. Stanton and T. J. Sejnowski. Associative long-term depression in the\nhippocampus induced by Hebbian covariance. Nature, 339:215–218, 1989.\n[24] G. J. Stuart and B. Sakmann. Active propagation of somatic action potentials\ninto neocortical pyramidal cell dendrites. Nature, 367:69–72, 1994.\n[25] G. Tesauro.\nSimple neural models of classical conditioning.\nBiological\nCybernetics, 55:187–200, 1986.\n[26] L. G. Valiant. Circuits of the mind. Oxford University Press, 1994.\n[27] R. J. Williams. Simple Statistical Gradient-Following Algorithms for Con-\nnectionist Reinforcement Learning. Machine Learning, 8:229–256, 1992.\n21\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2019-11-17",
  "updated": "2019-11-17"
}