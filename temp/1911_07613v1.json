{
  "id": "http://arxiv.org/abs/1911.07613v1",
  "title": "A Subword Level Language Model for Bangla Language",
  "authors": [
    "Aisha Khatun",
    "Anisur Rahman",
    "Hemayet Ahmed Chowdhury",
    "Md. Saiful Islam",
    "Ayesha Tasnim"
  ],
  "abstract": "Language models are at the core of natural language processing. The ability\nto represent natural language gives rise to its applications in numerous NLP\ntasks including text classification, summarization, and translation. Research\nin this area is very limited in Bangla due to the scarcity of resources, except\nfor some count-based models and very recent neural language models being\nproposed, which are all based on words and limited in practical tasks due to\ntheir high perplexity. This paper attempts to approach this issue of perplexity\nand proposes a subword level neural language model with the AWD-LSTM\narchitecture and various other techniques suitable for training in Bangla\nlanguage. The model is trained on a corpus of Bangla newspaper articles of an\nappreciable size consisting of more than 28.5 million word tokens. The\nperformance comparison with various other models depicts the significant\nreduction in perplexity the proposed model provides, reaching as low as 39.84,\nin just 20 epochs.",
  "text": "A Subword Level Language Model for Bangla\nLanguage\nAisha Khatun1[0000−0001−6559−4493], Anisur Rahman1[0000−0002−4616−4559],\nHemayet Ahmed Chowdhury1[0000−0002−5582−669X], Md. Saiful\nIslam1[0000−0001−9236−380X], and Ayesha Tasnim1,2[0000−0002−7143−3255]\n1 Department of Computer Science and Engineering, Shahjalal University of Science\nand Technology\nKumargaon, Sylhet 3114, Bangladesh\n{aysha.kamal7,emailforanis,hemayetchoudhury,saif.acm}@gmail.com\n2 tasnim-cse@sust.edu\nAbstract. Language models are at the core of natural language process-\ning. The ability to represent natural language gives rise to its applications\nin numerous NLP tasks including text classiﬁcation, summarization, and\ntranslation. Research in this area is very limited in Bangla due to the\nscarcity of resources, except for some count-based models and very re-\ncent neural language models being proposed, which are all based on words\nand limited in practical tasks due to their high perplexity. This paper at-\ntempts to approach this issue of perplexity and proposes a subword level\nneural language model with the AWD-LSTM architecture and various\nother techniques suitable for training in Bangla language. The model\nis trained on a corpus of Bangla newspaper articles of an appreciable\nsize consisting of more than 28.5 million word tokens. The performance\ncomparison with various other models depicts the signiﬁcant reduction\nin perplexity the proposed model provides, reaching as low as 39.84, in\njust 20 epochs.\nKeywords: Language Model · Bangla · AWD-LSTM · Continuous-Space\nLanguage Model · Neural Language Model(NLM) · Deep Learning · Sub-\nword.\n1\nIntroduction\nThere are generally two types of language models, continuous-space [18] and\ncount-based [19] language models. Count-based models calculate the n-gram\nprobabilities to make n-th order Markov assumptions by counting the frequency\nof n-gram occurrences, followed by various smoothing techniques. Most of the\nwork in Bangla [8] is based on this approach. But n-gram models are extremely\nsparse. Besides, this method of using n-grams to represent models is linguistically\nuninformed, because it has to rely on exact matches of patterns of tokens. Fur-\nthermore, the language model is limited to a speciﬁc window, whereas humans\ncan exploit a much larger context much easily.\narXiv:1911.07613v1  [cs.CL]  15 Nov 2019\n2\nA. Khatun et al.\nEven though language models are at the heart of NLP, the newest tech-\nniques of language modeling have not yet been adapted for Bangla. Speciﬁcally\nin the reign of the continuous-space language model, very little has been done for\nBangla language. Feed-forward neural language models [18] and recurrent neural\nlanguage models (RNNs) have been implemented to solve the sparsity problems\nof n-gram models. Here words are represented as vectors of continuous values\ncalled word embeddings and these vector representations are fed into the network\nfor training. These vectors maintain the property that semantically close words\nare are closer in the vector space they are in. Neural Language models have also\nbeen used in various other levels of embedding including sentence, corpus, and\nsubword.\nIn this paper, we use a variant of the recurrent neural language model,\nAverage-Stochastic-Gradient-Descent Weight-Dropped LSTM [13], on subword\ntokenized Bangla text, which was tokenized using Googles Sentencepiece tok-\nenizer. We used the unigram subword tokenization [11] to tokenize the corpus.\nThis process of tokenization helps provide structural information about the lan-\nguage. Especially for a language with inﬂection [4], such as Bangla. We also\npresent a structure that includes various techniques to optimize the training of\nthe language model, producing signiﬁcantly low perplexities on data sets.\n2\nRelated Works\n2.1\nOn Bangla Language\nBangla is the seventh most spoken language by the total number of speakers in\nthe world, yet lacks standard resources. Works on modern language modeling\nare next to existent. Most published works are those of count-based models [8],\nwhich have been far outperformed by modern neural networks. Some consider-\nable work was done in [7]. Besides this, a comparative analysis was performed for\nword embeddings [1]. But the need for more improved language models remains.\nNotably, no language model was trained keeping in mind the level of inﬂection\nthat occurs in Bangla language, thus tokenizing in subword levels may oﬀer\ngreatly towards downstream tasks such as text classiﬁcation, summarization,\nand translation.\n2.2\nOn Language Models\nLanguage modeling can be deﬁned as the conditional probability of observing\na sequence of words and predicting the next word in the sequence. Tradition-\nally this was accomplished using word-count based methods, which are vastly\nreplaced by neural language models at present. A description of count-based and\nneural language models is presented below.\nCount-based language models N-gram language models are the basic type\nof count-based methods based on Markov assumption. This model states that\nSubword Language Model\n3\nthe probability of occurrence of a word depends on the last n-1 words that occur\nbefore it. A sentence or a paragraph is predicted word by word in this manner.\nThe Markov chain rule states that :\np(wn | w1, w2, . . . , wn−1) ≈p(wn | wn−m, . . . , wn−2, wn−1)\n(1)\nHere, ‘m‘ is the number of context words. The number of previous words/history\nis the order of the model. The basic idea is to predict probability of wn from\nits preceding context. If the context contains only wn−1, it is bi-gram, and the\nresult is obtained by dividing the frequency of wn−1, wn by frequency of wn−1.\nUni-gram on the other hand considers only the frequency of wn. Following a\nsimilar pattern, a tri-gram model would be as follows:\np(w3 | w1, w2) =\ncount(w1, w2, w2)\nP\nwcount(w1, w2, w)\n(2)\nAlthough a simple concept, n-gram models have several drawbacks. First,\nout-of-sample combinations are given zero probability. This is called the spar-\nsity problem and tends to occur very frequently. An attempt to combat this is to\nuse back-oﬀ[10] and smoothing techniques [3]. Second is the curse of dimension-\nality. Due to the enormous number of possible word combinations, it becomes\nincreasingly unfeasible to train on a larger number of context words.\nBesides this, the semantic and syntactic similarity of sequences is not rec-\nognized by such models, making it unable to ‘learn‘ language. True Markov\nassumption is also not modeled due to the ﬁxed length of context words taken\ninto consideration.\nContinuous-space language models Continuous-space language model, also\ncalled neural language model(NLM), is the solution to sparsity and high dimen-\nsionality problems mentioned previously. These models can represent distributed\nvalues and thus learn syntactic and semantic features of the language, which may\nbe otherwise impossible to extract.\nRecent works exploit NLM to achieve state-of-the-art performance using re-\ncurrent neural networks or its variations such as LSTM, along with other opti-\nmization and regularization techniques. Neural language models can be of various\narchitectural backgrounds as we see below:\n– Feed-Forward Language Models:\nFeed-Forward models learn the probability of the next word for the previous\nn-1 words with the help of a few feed-forward layers. An architecture of this form\ncontains a mapping from each word of the vocabulary V, to a continuous vector\nspace ∈Rm, where m is the feature dimension. Therefore the mapping C is a\nmatrix of size | V | ×m, each row representing a feature vector of a word. Then\na composite function is formed with mappings and a function for calculating\nconditional probability of wt with respect to (wt−n+1, . . . , wt−1), which learns\nthe features and parameters. It can generalize better than the n-gram models\n4\nA. Khatun et al.\nand solves the sparsity problems, however, long training and testing times pose\na considerable threat to the scalability of such methods.\nTwo models were proposed to combat this issue. One [17] works on clustering\nsimilar words for calculation reduction by building a binary hierarchical tree on\nthe vocabulary words using expert knowledge, while another [16] uses data-\ndriven methods to do the same. 11.1% perplexity reduction was achieved from\nthe best HLBL model [16] comparing with the Kneser-Ney smoothed 5-gram\nLM.\n– Recurrent Neural Language Models:\nIn recurrent neural network based model(RNNLM) [14], there is no limitation\nto the context size, so the information can keep circulating within the network for\nas long as needed. These models provide better generalization by the recurrently\nconnected neurons, also called short term memory.\nAn improvement over RNNLM proposed in [15] implements factorization on\nthe output layer using classes, thus reducing the computational complexity of\nRNNLM. This version is faster to train and test, and also performs better than\nthe original RNNLM.\n2.3\nOn Neural Network Architectures\nRecently numerous models have been proposed with more advanced structures\nand strategies to improve on language modeling. Although most provide good\nperformance, AWD-LSTM [13], with its specialized optimization and regulariza-\ntion techniques has been outperforming others in most cases. AWD-LSTM stands\nfor ASGD(Average Stochastic Gradient Descent) Weight-Dropped LSTM. It\nuses DropConnect [21] as a means of regularization, a variant of Average-SGD\n(NT-ASGD) and some other techniques in a very eﬀective way.\nLSTM follows a method to utilize short and long term memory of context\ntexts, which can be represented using these generic equations:\nit = σ(W ixt + U iht−1)\n(3)\nft = σ(W fxt + U fht−1)\n(4)\not = σ(W oxt + U oht−1)\n(5)\nect = tanh(W cxt + U cht−1)\n(6)\nct = it ⊙ect + ft ⊙+ect−1\n(7)\nht = ot ⊙tanh(ct)\n(8)\nHere, W i, W f, W o, W c, U i, U f, U o, U c represents weight matrices, xt is the\nt-th time step vector input, ht is the present hidden state, ct is the memory cell\nstate, and ⊙is element-wise multiplication.\nThe DropConnect comes in the scene to eliminate the conventional over-ﬁtting\ncaused by RNN/LSTM layers. It is applied on the hidden-to-hidden activations,\nSubword Language Model\n5\nFig. 1. DropConnect Network [21]\n(U i, U f, U o, U c), randomly. This prevents over-ﬁtting, still maintaining RNNs\nlong-term dependency retention. Figure 1 provides an illustration of the Drop-\nConnect network.\nUnlike SGD, ASGD takes into account previous iterations in updates and\nreturns an average value. AWD-LSTM uses a variant of ASGD called NT-\nASGD(non-monotonically triggered ASGD), where ASGD is activated if the\nvalidation metric does not improve for a ﬁxed number of cycles.\nSeveral other methods for eﬀective learning includes variation in backpropagation-\nthrough-time length, variational dropout [5], and embedding Dropout [5]. These\ntechniques improve the overall structure of the network, making it easier to train\na language model eﬀectively, with signiﬁcantly small model size, and therefore\nis the ﬁrst choice of architecture.\n2.4\nOn Training Neural Networks\nVarious methods and techniques can be employed while training a model to\nmake it faster, more eﬀective and perform better. When these techniques meet\nthe right architecture, state-of-the-art performances can be achieved. Some of\nsuch methods used in this paper are described below.\nLearning rates are an important and yet tedious hyper-parameter when train-\ning a model. Finding the right learning rate is extremely crucial, but has been\nsolved over the years only through time expensive trial and error methods. An\nimpressive approach by Leslie Smith [20] addresses this problem by performing\na single trial run over the data. Training starts with a small learning rate and\nexponential increase occurs per batch. The loss is recorded for every point. The\noptimum learning rate is the highest one with a descending error.\nAnother training problem is local minima. Neural networks may often move\ntowards the local minima through gradient descent, instead of global minima,\nand get stuck. According to [12], if the learning rate is abruptly increased, the\ngradient descent will be able to jump out of local minima and start looking for\n6\nA. Khatun et al.\nthe global minima again. Experiments from [12] prove the eﬀectiveness of this\napproach.\n3\nCorpus\nA large corpus on Bangla newspaper articles was created using a custom web\ncrawler containing 12 diﬀerent topics. This corpus is relatively large compared\nto the ones used for language modeling in the past for Bangla [8]. The total\nnumber of word tokens in this dataset is 28533646 (28.5+ million), with unique\nword count of 836509. The number of unique words is around 3% of the entire\nvocabulary of the dataset. A summarized statistics of the dataset is given in\nTable 1. The Dataset is imbalanced, which is apparent from Figure 2, for the\ntrain set. 20% of the dataset was separated as a held-out dataset for testing\npurposes.\nTable 1. Dataset Statistics\ncategory\nsamples total word unique word\nopinion\n8098\n4185472\n243968\ninternational\n5155\n1089780\n86852\neconomics\n3449\n909648\n58932\nart\n2665\n1312571\n154869\nscience\n2906\n697899\n76755\npolitics\n20050\n6167418\n196541\ncrime\n8655\n2016342\n128308\neducation\n12212\n3963695\n225348\nsports\n11903\n3087029\n174677\naccident\n6328\n1086791\n77171\nenvironment\n4313\n1347509\n103783\nentertainment 10121\n2669492\n204902\naverage\n7988\n2377803\n144342\ntotal\n95855\n28533646\n836509\n4\nMethodology\n4.1\nProposed Architecture\nThis paper employs an architecture called AWD-LSTM [13] for language mod-\neling. It is a variant of Recurrent Neural Network with LSTM gates [6] along\nwith special regularization techniques and optimization which allows for con-\ntext generalization and language understanding. The LSTM layers allow for the\nmemory of the context of a sentence but are prone to over-ﬁtting. AWD-LSTM\nemploys dropconnect, among other techniques making it the choice of interest\nfor language modeling for a language such as Bangla.\nSubword Language Model\n7\nFig. 2. Distribution of classes in the dataset\nThis architecture consists of an embedding layer of size 400, followed by\n3 regular LSTM layers with no attention. Additionally, it has some short-cut\nconnections and numerous drop-out hyper-parameters. Each layer has 1150 ac-\ntivation neurons. At last a softmax layer provides probabilities for the next word.\nAdam optimizer is used along with ﬂattened categorical cross-entropy loss func-\ntion. The architecture is illustrated in ﬁgure 3.\n4.2\nSubword Tokenization\nIn traditional training, words are the tokens of the model with a ﬁxed vocabulary,\nwhere each token is represented by an integer. However, while mapping words\nto integers we lose information about word structure, therefore the language\nmodel requires more time and data to learn about languages with inﬂections\nsuch as Bangla. To help the model learn at a deeper level, one possibility is\nthe use of characters as tokens. But this approach produces much larger models\nand increases the computational costs many-fold, yet not retaining as much\ninformation as required in the LSTM layers [2]. So we opt for a method in\nthe middle, and perform subword tokenization of the text using SentencePiece\ntokenizer. This tokenizer performs unsupervised tokenization directly from raw\nsentences irrespective of the language. The unigram segmentation algorithm [11]\nwas employed to create the subword vocabulary.\n4.3\nTraining the language model\nThe SentencePiece tokenizer was set to a vocabulary size of 30,000 [4], which in-\ncluded <s>,</s>,<unk>tokens as the start token, end token, and the unknown\n8\nA. Khatun et al.\nFig. 3. Language Model Architecture\ntoken. All tokens appearing less than 3 times were discarded and replaced with\nthe <unk>token. The language model was trained with a batch size of 32, back-\npropagation-through-time window was set to 70. A dropout multiplier of 0.5 was\nused for all the LSTM layers for regularization. Weight decay of 0.1 was used.\nThe layers were unfrozen, and weights were randomly initialized. Using the\ncyclical learning rate ﬁnder technique [20], a learning rate of 1e-3 was selected\nand the model was trained for 20 epochs. Stochastic gradient descent with\nrestarts (SGDR) mentioned in Section 2 was applied to each epoch. Given a\nphrase, the language model at this point was capable of completing entire para-\ngraphs in Bangla language as illustrated in ﬁgure 4.\n5\nExperiments\nWe attempt to run some experiments for the sake of comparison, recreating\nfrom previously proposed models and evaluate their perplexities. Perplexity is\na common measure of language model performance. It calculates how well a\nmodel predicts the next possible token, given a context of any length. In this\ncase, perplexity is a way to determine the uncertainty in text generation through\nprobability assignment. Low perplexity is a sign that the model is performing\nwell, whereas high perplexity means the language model was not able to cap-\nture the language well. In simple words, perplexity can be calculated as the\nexponentiation of the loss. The models used for analysis are:\nSubword Language Model\n9\nFig. 4. Example Predictions from the Language Model\n5.1\nBi-gram language model\nWe compare our model with the bi-gram language model following [9], which\nhas been frequently used for language modeling in Bangla with variations. Let\nqt = freq(wt−1, wt−2) be the representation of the frequency of occurrence of\na speciﬁc window of words (wt−1, wt−2). Therefore the conditional probability\ntakes the following form:\nP(wt|wt−1) = α0(qt)p0 + α1(qt)p1(wt) + α2(qt)p2(wt|wt−1)\n(9)\nWith conditional weights αi(qt) ≥0, P\niαi(qt) = 1. Prediction is done using\nvarious window size, including the following ways: p0 = 1/|V |, p1 is a uni-gram\nand p2(i|j) is the bi-gram. Rest of the experimental measures are kept similar\nto [8].\n5.2\nLSTM and CNN language models\nIn order to compare the proposed architecture with its modiﬁcations against\nsimpler plain models, we conduct experiments with a few other continuous space\nneural models. We build a simple LSTM model with two layers that are trained\nfor 10 epochs. Each layer has 200 hidden units and all hidden states are initialized\nby 0. We train with a batch size of 32, and an initial learning rate of 1 is used.\nBesides this, we also try a character level CNN model, similar to the classiﬁcation\nmodel of [22]. The character set is of size 85 containing all Bangla letters, digits\namong others. The model consists of 2 convolutional layers followed by a fully\nconnected layer. The length of the input sequence is set as 1000 characters, other\nparameters are kept consistent with [22].\n5.3\nAWD-LSTM word-level language model\nFor a much closer comparison, a strand of research by Chowdhury et al, word-\nlevel language model using AWD-LSTM architecture was also compared. This\n10\nA. Khatun et al.\nmodel was fed with text by only separating the words as tokens. Vocabulary size\nof 60,000 was used and all words with frequency less than 3 were eliminated and\nreplaced with <unk>. The model was trained for 4 epochs with a learning rate of\n1e-2, and then 3 more epochs with a learning rate of 1e-3 after which the model\nstarted to overﬁt, so training was stopped. This model yielded a perplexity of\n51.2 at the end of training on the held-out test dataset.\n6\nResults and Discussion\nThe held-out test set is evaluated and the perplexities are recorded in the Table\n2.\nTable 2. Perplexity Comparison of the Language Models\nModel\nPerplexity on Test Set\nAWD-LSTM subword(proposed)\n39.84\nAWD-LSTM word\n51.2\nSimple LSTM\n227\nCharacter CNN\n125\nBi-gram\n860.1\nFrom the table, it is clear that the subword level model signiﬁcantly out-\nperformed all other models. The simple LSTM model gained a perplexity of 227\nwhile using AWD-LSTM architecture lowers it up to 39.84. Character level CNN\nimproves the perplexity from simple LSTM model but it is still outperformed\nby the proposed model signiﬁcantly. The bi-gram model did not work well at\nall, which shows the need for using a neural language model. Furthermore, the\ninﬂection of Bangla language was leveraged by SentencePiece tokenization pro-\nviding a subword text structure, which when input to the model improves the\nmodel performance from 51.2 to 39.84 perplexity.\n7\nConclusion\nIn this paper, we discuss the ways to eﬀectively train a weight-dropped LSTM\nlanguage model in Bangla using subword tokenization and other strategies. We\nshowed that the proposed subword level model outperforms the word-level mod-\nels with a perplexity of 39.84, showing the eﬀectiveness of such tokenization. This\nis a signiﬁcantly low perplexity achieved in Bangla language so far. The reasons\nfor such results are we believe the architectural makeup of AWD-LSTM using\nDropConnect mask on the hidden-to-hidden weight matrices as a technique of\nregularization, which is a common problem in language modeling. Besides this,\nthe structure of Bangla language makes it diﬃcult to perform modeling tasks,\nwhich we attempted to solve using subword tokenization. By using subwords\nSubword Language Model\n11\ninstead of words, not only does it bring together the same words with various\nextensions, but it also signiﬁcantly reduces the vocabulary size required to train\nthe model to a similar level of accuracy. Language models are being used widely\nfor transfer learning in NLP and a well-trained model that understands the lan-\nguage in question will be eﬀectively helpful in downstream tasks such as text\nsummarization, classiﬁcation, and translation.\n8\nAcknowledgement\nThis paper is a grateful recipient of the facilities and research environment of\nthe Department of Computer Science and Engineering, Shahjalal University of\nScience and Technology (SUST) and SUST NLP Research Group. All authors\nof this paper deserve equal credit providing major contributions to the research.\nReferences\n1. Ahmed Chowdhury, H., Imon, M., Saiful Islam, M.: A comparative analysis of\nword embedding representations in authorship attribution of bengali literature.\n21st International Conference of Computer and Information Technology (ICCIT)\n(2018)\n2. Bojanowski, P., Joulin, A., Mikolov, T.: Alternative structures for character-level\nrnns. arXiv preprint arXiv:1511.06303 (2015)\n3. Chen, S.F., Goodman, J.: An empirical study of smoothing techniques for language\nmodeling. Computer Speech & Language (1999)\n4. Czapla, P., Gugger, S., Howard, J., Kardas, M.: Universal language model ﬁne-\ntuning for polish hate speech detection. Proceedings ofthePolEval2019Workshop\np. 149 (2019)\n5. Gal, Y., Ghahramani, Z.: A theoretically grounded application of dropout in recur-\nrent neural networks. Advances in neural information processing systems (2016)\n6. Gers, F.A., Schmidhuber, J., Cummins, F.A.: Learning to forget: Continual pre-\ndiction with lstm. Neural Computation (2000)\n7. Haque, M., Habib, M., Rahman, M.: Automated word prediction in bangla lan-\nguage using stochastic language models. International Journal in Foundations of\nComputer Science & Technology (2015)\n8. Ismail, S., Rahman, M.S.: Bangla word clustering based on n-gram language model.\n2014 International Conference on Electrical Engineering and Information Commu-\nnication Technology (2014)\n9. Jelinek, F., Mercer, R.L.: Interpolated estimation of Markov source parameters\nfrom sparse data. Proceedings, Workshop on Pattern Recognition in Practice\n(1980)\n10. Kneser, R., Ney, H.: Improved backing-oﬀfor m-gram language modeling. 1995\nInternational Conference on Acoustics, Speech, and Signal Processing (1995)\n11. Kudo, T.: Subword regularization: Improving neural network translation models\nwith multiple subword candidates (2018)\n12. Loshchilov, I., Hutter, F.: SGDR: stochastic gradient descent with restarts. CoRR\n(2016)\n12\nA. Khatun et al.\n13. Merity, S., Keskar, N.S., Socher, R.: Regularizing and optimizing LSTM language\nmodels. CoRR (2017)\n14. Mikolov, T., Karaﬁ´at, M., Burget, L., ˇCernock`y, J., Khudanpur, S.: Recurrent neu-\nral network based language model. Eleventh annual conference of the international\nspeech communication association (2010)\n15. Mikolov, T., Kombrink, S., Burget, L., ˇCernock`y, J., Khudanpur, S.: Extensions\nof recurrent neural network language model. 2011 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) (2011)\n16. Mnih, A., Hinton, G.E.: A scalable hierarchical distributed language model. Ad-\nvances in neural information processing systems (2009)\n17. Morin, F., Bengio, Y.: Hierarchical probabilistic neural network language model.\nAistats (2005)\n18. Schwenk, H., Dchelotte, D., Gauvain, J.L.: Continuous space language models for\nstatistical machine translation. Proceedings of the COLING/ACL on Main Con-\nference Poster Sessions (2006)\n19. Siivola, V., Pellom, B.L.: Growing an n-gram language model. INTERSPEECH\n(2005)\n20. Smith, L.N.: No more pesky learning rate guessing games. CoRR (2015)\n21. Wan, L., Zeiler, M., Zhang, S., Le Cun, Y., Fergus, R.: Regularization of neural\nnetworks using dropconnect. International conference on machine learning (2013)\n22. Zhang, X., Zhao, J., LeCun, Y.: Character-level convolutional networks for text\nclassiﬁcation (2015)\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2019-11-15",
  "updated": "2019-11-15"
}