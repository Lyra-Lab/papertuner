{
  "id": "http://arxiv.org/abs/1911.08142v2",
  "title": "GraphTER: Unsupervised Learning of Graph Transformation Equivariant Representations via Auto-Encoding Node-wise Transformations",
  "authors": [
    "Xiang Gao",
    "Wei Hu",
    "Guo-Jun Qi"
  ],
  "abstract": "Recent advances in Graph Convolutional Neural Networks (GCNNs) have shown\ntheir efficiency for non-Euclidean data on graphs, which often require a large\namount of labeled data with high cost. It it thus critical to learn graph\nfeature representations in an unsupervised manner in practice. To this end, we\npropose a novel unsupervised learning of Graph Transformation Equivariant\nRepresentations (GraphTER), aiming to capture intrinsic patterns of graph\nstructure under both global and local transformations. Specifically, we allow\nto sample different groups of nodes from a graph and then transform them\nnode-wise isotropically or anisotropically. Then, we self-train a\nrepresentation encoder to capture the graph structures by reconstructing these\nnode-wise transformations from the feature representations of the original and\ntransformed graphs. In experiments, we apply the learned GraphTER to graphs of\n3D point cloud data, and results on point cloud segmentation/classification\nshow that GraphTER significantly outperforms state-of-the-art unsupervised\napproaches and pushes greatly closer towards the upper bound set by the fully\nsupervised counterparts. The code is available at:\nhttps://github.com/gyshgx868/graph-ter.",
  "text": "GraphTER: Unsupervised Learning of Graph Transformation Equivariant\nRepresentations via Auto-Encoding Node-wise Transformations\nXiang Gao1, Wei Hu1,∗, and Guo-Jun Qi2\n1Wangxuan Institute of Computer Technology, Peking University, Beijing\n2Futurewei Technologies\n{gyshgx868, forhuwei}@pku.edu.cn, guojunq@gmail.com\nAbstract\nRecent advances in Graph Convolutional Neural Net-\nworks (GCNNs) have shown their efﬁciency for non-\nEuclidean data on graphs, which often require a large\namount of labeled data with high cost.\nIt it thus criti-\ncal to learn graph feature representations in an unsuper-\nvised manner in practice. To this end, we propose a novel\nunsupervised learning of Graph Transformation Equivari-\nant Representations (GraphTER), aiming to capture intrin-\nsic patterns of graph structure under both global and lo-\ncal transformations. Speciﬁcally, we allow to sample dif-\nferent groups of nodes from a graph and then transform\nthem node-wise isotropically or anisotropically. Then, we\nself-train a representation encoder to capture the graph\nstructures by reconstructing these node-wise transforma-\ntions from the feature representations of the original and\ntransformed graphs. In experiments, we apply the learned\nGraphTER to graphs of 3D point cloud data, and results on\npoint cloud segmentation/classiﬁcation show that Graph-\nTER signiﬁcantly outperforms state-of-the-art unsupervised\napproaches and pushes greatly closer towards the upper\nbound set by the fully supervised counterparts. The code\nis available at: https://github.com/gyshgx868/graph-ter.\n1. Introduction\nGraphs are a natural representation of irregular data,\nsuch as 3D geometric points, social networks, citation net-\nworks and brain networks. Recent advances in Graph Con-\nvolutional Neural Networks (GCNNs) have shown their ef-\nﬁciency in learning representations of such data [4, 54, 53,\n46], which generalize the celebrated CNN models. Exist-\ning GCNNs are mostly trained in a supervised or semi-\nsupervised fashion, requiring a large amount of labeled data\nto learn graph representations. This prevents the wide appli-\n∗Corresponding author:\nWei Hu (forhuwei@pku.edu.cn).\nThis\nwork was supported by National Natural Science Foundation of China\n[61972009] and Beijing Natural Science Foundation [4194080].\nE\nD\nܧ܆ǡ ۯ\nܧܜ܆ǡ ෩ۯ\n܆ǡ ۯ\nܜ܆ǡ ෩ۯ\nNode-wise\nTransformation ܜ\nƸܜ\nEstimated\nTransformation\nܜଵ\nܜଶ\nܜଷ\nFigure 1. An illustration of the proposed GraphTER model for\nunsupervised feature learning. The encoder learns representa-\ntions of the original graph data X associated with adjacency ma-\ntrix A and its transformed counterpart t(X) associated with ˜A\nrespectively. By decoding node-wise transformations t from both\nrepresentations, the auto-encoder is able to learn intrinsically mor-\nphable structures of graphs.\ncability of GCNNs due to the high labeling cost especially\nfor large-scale graphs in many real scenarios. Hence, it is\ndemanded to train graph feature representations in an unsu-\npervised fashion, which can adapt to downstream learning\ntasks on graphs.\nAuto-Encoders (AEs) and Generative Adversarial Net-\nworks (GANs) are two most representative methods for un-\nsupervised learning. Auto-encoders aim to train an encoder\nto learn feature representations by reconstructing input data\nvia a decoder [42, 36, 16, 18]. Most of auto-encoders stick\nto the idea of reconstructing input data at the output end\n(e.g., images [16], graphs [19], 3D point clouds [48]), and\nthus can be classiﬁed into the Auto-Encoding Data (AED)\n[51]. In contrast, GANs [14, 10, 11, 50, 26, 7, 3] extract\nfeature representations in an unsupervised fashion by gen-\nerating data from input noises via a pair of generator and\ndiscriminator, where the noises are viewed as the data rep-\nresentations, and the generator is trained to generate data\nfrom the “noise” feature representations.\nBased on AEs and GANs, many approaches have sought\narXiv:1911.08142v2  [cs.CV]  19 Mar 2020\nto learn transformation equivariant representations (TER)\nto further improve the quality of unsupervised representa-\ntion learning. It assumes that representations equivarying\nto transformations are able to encode the intrinsic struc-\ntures of data such that the transformations can be recon-\nstructed from the representations before and after transfor-\nmations [34]. Learning transformation equivariant repre-\nsentations has been advocated in Hinton’s seminal work on\nlearning transformation capsules [16]. Following this, a va-\nriety of approaches have been proposed to learn transforma-\ntion equivariant representations [20, 41, 38, 40, 23, 12, 9, 8].\nAmong them are the group equivariant convolutions [6] and\ngroup equivariant capsule networks [24] that generalize the\nCNNs and capsule nets to equivary to various transforma-\ntions. However, these models are restricted to discrete trans-\nformations, and they should be trained in a supervised fash-\nion. This limits their ability to learn unsupervised represen-\ntations equivariant to a generic composition of continuous\ntransformations [34].\nTo generalize to generic transformations, Zhang et al.\n[51] propose to learn unsupervised feature representa-\ntions via Auto-Encoding Transformations (AET) rather than\nAED. By randomly transforming images, they seek to train\nauto-encoders by directly reconstructing these transforma-\ntions from the learned representations of both the original\nand transformed images. A variational AET [35] is also in-\ntroduced from an information-theoretic perspective by max-\nimizing the lower bound of mutual information between\ntransformations and representations. Moreover, it has also\nbeen theoretically proved [34, 35] that the unsupervised rep-\nresentations by the AET are equivariant to the applied trans-\nformations. Unfortunately, these works focus on transfor-\nmation equivariant representation learning of images that\nare Euclidean data, which cannot be extended to graphs due\nto the irregular data structures.\nIn this paper, we take a ﬁrst step towards this goal –\nwe formalize Graph Transformation Equivariant Represen-\ntation (GraphTER) learning by auto-encoding node-wise\ntransformations in an unsupervised manner. The proposed\nmethod is novel in twofold aspects.\nOn one hand, we\ndeﬁne graph signal transformations and present a graph-\nbased auto-encoder architecture, which encodes the repre-\nsentations of the original and transformed graphs so that\nthe graph transformations can be reconstructed from both\nrepresentations. On the other hand, in contrast to the AET\nwhere global spatial transformations are applied to the en-\ntire input image, we perform node-wise transformations on\ngraphs, where each node can have its own transformation.\nRepresentations of individual nodes are thus learned by de-\ncoding node-wise transformations to reveal the graph struc-\ntures around it. These representations will not only cap-\nture the local graph structures under node-wise transforma-\ntions, but also reveal global information about the graph\nas we randomly sample nodes into different groups over\ntraining iterations. Different groups of nodes model dif-\nferent parts of graphs, allowing the learned representations\nto capture various scales of graph structures under isotropic\nand/or anisotropic node-wise transformations. This results\nin transformation equivariant representations to character-\nize the intrinsically morphable structures of graphs.\nSpeciﬁcally, given an input graph signal and its associ-\nated graph, we sample a subset of nodes from the graph\n(globally or locally) and perform transformations on indi-\nvidual nodes, either isotropically or anisotropically. Then\nwe design a full graph-convolution auto-encoder architec-\nture, where the encoder learns the representations of in-\ndividual nodes in the original and transformed graphs re-\nspectively, and the decoder predicts the applied node-wise\ntransformations from both representations. Experimental\nresults demonstrate that the learned GraphTER signiﬁcantly\noutperforms the state-of-the-art unsupervised models, and\nachieves comparable results to the fully supervised ap-\nproaches in the 3D point cloud classiﬁcation and segmen-\ntation tasks.\nOur main contributions are summarized as follows.\n• We are the ﬁrst to propose Graph Transformation\nEquivariant Representation (GraphTER) learning to\nextract adequate graph signal feature representations\nin an unsupervised fashion.\n• We deﬁne generic graph transformations and formal-\nize the GraphTER to learn feature representations of\ngraphs by decoding node-wise transformations end-to-\nend in a full graph-convolution auto-encoder architec-\nture.\n• Experiments demonstrate the GraphTER model out-\nperforms the state-of-the-art methods in unsupervised\ngraph feature learning as well as greatly pushes closer\nto the upper bounded performance by the fully super-\nvised counterparts.\nThe remainder of this paper is organized as follows.\nWe ﬁrst review related works in Sec. 2. Then we deﬁne\ngraph transformations in Sec. 3 and formalize the Graph-\nTER model in Sec. 4. Finally, experimental results and con-\nclusions are presented in Sec. 5 and Sec. 6, respectively.\n2. Related Works\nTransformation Equivariant Representations. Many\napproaches have been proposed to learn equivariant rep-\nresentations, including transforming auto-encoders [16],\nequivariant Boltzmann machines [20, 41], equivariant de-\nscriptors [38], and equivariant ﬁltering [40]. Lenc et al.\n[23] prove that the AlexNet [22] trained on ImageNet learns\nrepresentations that are equivariant to ﬂip, scaling and rota-\ntion transformations. Gens et al. [12] propose an approx-\nimately equivariant convolutional architecture, which uti-\nlizes sparse and high-dimensional feature maps to deal with\ngroups of transformations. Dieleman et al. [9] show that ro-\ntation symmetry can be exploited in convolutional networks\nfor effectively learning an equivariant representation. This\nwork is later extended in [8] to evaluate on other computer\nvision tasks that have cyclic symmetry. Cohen et al. [6]\npropose group equivariant convolutions that have been de-\nveloped to equivary to more types of transformations. The\nidea of group equivariance has also been introduced to the\ncapsule nets [24] by ensuring the equivariance of output\npose vectors to a group of transformations. Zhang et al.\n[51] propose to learn unsupervised feature representations\nvia Auto-Encoding Transformations (AET) by estimating\ntransformations from the learned feature representations of\nboth the original and transformed images.\nThis work is\nlater extended in [35] by introducing a variational transfor-\nmation decoder, where the AET model is trained from an\ninformation-theoretic perspective by maximizing the lower\nbound of mutual information.\nAuto-Encoders and GANs. Auto-encoders (AEs) have\nbeen widely adopted to learn unsupervised representations\n[17], which employ an encoder to extract feature represen-\ntations and a decoder to reconstruct the input data from the\nrepresentations. The idea is based on good feature represen-\ntations should contain sufﬁcient information to reconstruct\nthe input data. A large number of approaches have been\nproposed following this paradigm of Auto-Encoding Data\n(AED), including variational AEs (VAEs) [18], denoising\nAEs [42], contrastive AEs [36], transforming AEs [16], etc.\nBased on the above approaches, graph AEs have been pro-\nposed to learn latent representations for graphs. These ap-\nproaches basically learn graph embeddings for plain graphs\n[5, 43] and attributed graphs [19, 31], which are still trained\nin the AED fashion. In addition to AEs, Generative Adver-\nsarial Networks (GANs) [14] become popular for learning\nunsupervised representations of data, which tend to gener-\nate data from noises sampled from a random distribution.\nThe basic idea of these models is to treat the sampled noise\nas the feature of the output data, and an encoder can be\ntrained to obtain the “noise” feature representations for in-\nput data, while the generator is treated as the decoder to gen-\nerate data from the “noise” feature representations [10, 11].\nRecently, several approaches have been proposed to build\ngraph GANs. For instance, [50] and [26] propose to gener-\nate nodes and edges alternately, while [7] and [3] propose\nto integrate GCNNs with LSTMs and GANs respectively to\ngenerate graphs.\n3. Graph Transformations\n3.1. Preliminaries\nConsider an undirected graph G = {V, E} composed of\na node set V of cardinality |V| = N, and an edge set E\nconnecting nodes. Graph signal refers to data/features as-\nsociated with the nodes of G, denoted by X ∈RN×C with\nith row representing the C-dimensional graph signal at the\nith node of V.\nTo characterize the similarities (and thus the graph struc-\nture) among node signals, an adjacency matrix A is deﬁned\non G, which is a real-valued symmetric N × N matrix with\nai,j as the weight assigned to the edge (i, j) connecting\nnodes i and j. Formally, the adjacency matrix is constructed\nfrom graph signals as follows,\nA = f(X),\n(1)\nwhere f(·) is a linear or non-linear function applied to each\npair of nodes to get the pair-wise similarity. For example,\na widely adopted function is to nonlinearly construct a k-\nnearest-neighbor (kNN) graph from node features [44, 52].\n3.2. Graph Signal Transformation\nUnlike Euclidean data like images, graph signals are ir-\nregularly sampled, whose transformations are thus nontriv-\nial to deﬁne. To this end, we deﬁne a graph transformation\non the signals X as node-wise ﬁltering on X.\nFormally, suppose we sample a graph transformation t\nfrom a transformation distribution Tg, i.e., t ∼Tg. Apply-\ning the transformation to graph signals X that are sampled\nfrom data distribution Xg, i.e., X ∼Xg, leads to the ﬁltered\ngraph signal\n˜X = t(X).\n(2)\nThe ﬁlter t is applied to each node individually, which\ncan be either node-invariant or node-variant. In other words,\nthe transformation of each node signal associated with t can\nbe different from each other. For example, for a translation\nt, a distinctive translation can be applied to each node. We\nwill call the graph transformation isotropic (anisotropic) if\nit is node-invariant (variant).\nConsequently, the adjacency matrix of the transformed\ngraph signal ˜X equivaries according to (1):\n˜A = f( ˜X) = f(t(X)),\n(3)\nwhich transforms the graph structures, as edge weights are\nalso ﬁltered by t(·).\nUnder this deﬁnition, there exist a wide spectrum of\ngraph signal transformations.\nExamples include afﬁne\ntransformations (translation, rotation and shearing) on the\nlocation of nodes (e.g., 3D coordinates in point clouds), and\ngraph ﬁlters such as low-pass ﬁltering on graph signals by\nthe adjacency matrix [37].\n3.3. Node-wise Graph Signal Transformation\nAs aforementioned, in this paper, we focus on node-wise\ngraph signal transformation, i.e., each node has its own\ntransformation, either isotropically or anisotropically. We\nseek to learn graph representations through the node-wise\ntransformations by revealing how different parts of graph\nstructures would change globally and locally.\nSpeciﬁcally, here are two distinct advantages.\n(a) Original model\n(b) Global+Isotropic\n(c) Global+Anisotropic\n(d) Local+Isotropic\n(e) Local+Anisotropic\nFigure 2. Demonstration of different sampling (Global or Local) and node-wise translation (Isotropic or Anisotropic) methods on\n3D point clouds. Red and blue points represent transformed and non-transformed points, respectively. Note that we adopt the wing as a\nsampled local point set for clear visualization.\n(a) Before transformation.\n(b) After transformation.\nFigure 3. An example of kNN graphs before and after node-\nwise transformations. We ﬁrst construct a kNN (k = 5) graph for\nthe yellow node (other connections are omitted). Then we perform\nnode-wise transformations on some blue nodes, which alters the\ngraph structure around the yellow node.\n• The node-wise transformations allow us to use node\nsampling to study different parts of graphs under vari-\nous transformations.\n• By decoding the node-wise transformations, we will be\nable to learn the representations of individual nodes.\nMoreover, these node-wise representations will not\nonly capture the local graph structures under these\ntransformations, but also contain global information\nabout the graph when these nodes are sampled into dif-\nferent groups over iterations during training.\nNext, we discuss the formulation of learning graph\ntransformation equivariant representations by decoding the\nnode-wise transformations via a graph-convolutional en-\ncoder and decoder.\n4. GraphTER: The Proposed Approach\n4.1. The Formulation\nGiven a pair of graph signal and adjacency matrix\n(X, A), and a pair of transformed graph signal and adja-\ncency matrix ( ˜X, ˜A) by a node-wise graph transformation\nt, a function E(·) is transformation equivariant if it satisﬁes\nE( ˜X, ˜A) = E (t(X), f (t(X))) = ρ(t) [E(X, A)] , (4)\nwhere ρ(t) is a homomorphism of transformation t in the\nrepresentation space.\nOur goal is to learn a function E(·), which extracts\nequivariant representations of graph signals X. For this pur-\npose, we employ an encoder-decoder network: we learn a\ngraph encoder E : (X, A) 7→E(X, A), which encodes the\nfeature representations of individual nodes from the graph.\nTo ensure the transformation equivariance of representa-\ntions, we train a decoder D :\n\u0010\nE(X, A), E( ˜X, ˜A)\n\u0011\n7→ˆt\nto estimate the node-wise transformation ˆt from the rep-\nresentations of the original and transformed graph signals.\nHence, we cast the learning problem of transformation\nequivariant representations as the joint training of the rep-\nresentation encoder E and the transformation decoder D. It\nhas been proved that the learned representations in this way\nsatisfy the generalized transformation equivariance without\nrelying on a linear representation of graph structures [35].\nFurther, we sample a subset of nodes S following a sam-\npling distribution Sg from the original graph signal X, lo-\ncally or globally in order to reveal graph structures at vari-\nous scales. Node-wise transformations are then performed\non the subset S isotropically or anisotropically, as demon-\nstrated in Fig. 2. In order to predict the node-wise transfor-\nmation t, we choose a loss function ℓS(t,ˆt) that quantiﬁes\nthe distance between t and its estimate ˆt in terms of their\nparameters. Then the entire network is trained end-to-end\nby minimizing the loss\nmin\nE,D\nE\nS∼Sg\nE\nt∼Tg\nX∼Xg\nℓS(t,ˆt),\n(5)\nwhere the expectation E is taken over the sampled graph\nsignals and transformations, and the loss is taken over the\n(locally or globally) sampled subset S of nodes in each iter-\nation of training.\nIn (5), the node-wise transformation ˆt is estimated from\nthe decoder\nˆt = D\n\u0010\nE(X, A), E( ˜X, ˜A)\n\u0011\n.\n(6)\nThus, we update the parameters in encoder E and decoder\nD iteratively by backward propagation of the loss.\n4.2. The Algorithm\nGiven graph signals X = {x1, x2, ..., xN}⊤over N\nnodes, in each iteration of training, we randomly sample a\nsubset of nodes S from the graph, either globally or locally.\nGlobal sampling refers to random sampling over the entire\nnodes globally, while local sampling is limited to a local set\nof nodes in the graph. Node sampling not only enables us\nto characterize global and local graph structures at various\nscales, but also reduces the number of node-wise transfor-\nmation parameters to estimate for computational efﬁciency.\nUnsupervised feature learning stage\nSupervised evaluation stage\nEdgeConv Layer\nFeature-wise Concatenation\nFC Layer\nLinear \nClassifier\nClassification\nScore\nEconv1 Econv2\n܆ǡ ۯ\nfrozen weights\nƸܜ\nEncoder\nDecoder\n܆ǡ ۯ\nܜ܆ǡ ෩ۯ\nEconv1 Econv2 Econv3\nshared weights\nEconv1 Econv2 Econv3\nFigure 4. The architecture of the proposed GraphTER. In the\nunsupervised feature learning stage, the representation encoder\nand transformation decoder are jointly trained by minimizing (5).\nIn the supervised evaluation stage, the ﬁrst several blocks of the\nencoder are ﬁxed with frozen weights and a linear classiﬁer is\ntrained with labeled samples.\nThen we draw a node-wise transformation ti corre-\nsponding to each sample xi of nodes in S, either isotrop-\nically or anisotropically. Accordingly, the graph ˜A asso-\nciated with the transformed graph also transforms equivari-\nantly from the original A under t. Speciﬁcally, as illustrated\nin Fig. 3, we construct a kNN graph to make use of the con-\nnectivity between the nodes, whose matrix representation in\nA changes after applying the sampled node-wise transfor-\nmations.\nTo learn the applied node-wise transformations, we de-\nsign a full graph-convolutional auto-encoder network as il-\nlustrated in Fig. 4. Among various paradigms of GCNNs,\nwe choose EdgeConv [44] as a basic building block of the\nauto-encoder network, which efﬁciently learns node-wise\nrepresentations by aggregating features along all the edges\nemanating from each connected node. Below we will ex-\nplain the representation encoder and the transformation de-\ncoder in detail.\n4.2.1\nRepresentation Encoder\nThe representation encoder E takes the signals of an orig-\ninal graph X and the transformed counterparts ˜X as input,\nalong with their corresponding graphs. E encodes node-\nwise features of X and ˜X through a Siamese encoder net-\nwork with shared weights, where EdgeConv layers are used\nas basic feature extraction blocks.\nAs shown in Fig. 3,\ngiven a non-transformed central node xi and its transformed\nneighbors tj(xj), the input layer of encoded feature of xi\nis\nEin( ˜X, ˜A)i = max\nj∈N(i) ˜ai,j\n= max\nj∈N(i) ReLU(θ(tj(xj) −xi) + φxi),\n(7)\nwhere ˜ai,j denotes the edge feature, i.e., edge weight in ˜A.\nθ and φ are two weighting parameters, and j ∈N(i) de-\nnotes node j is in the k-nearest neighborhood of node i.\nThen, multiple layers of regular edge convolutions [44] are\nstacked to form the ﬁnal encoder.\nEdge convolution in (7) over each node essentially ag-\ngregates features from neighboring nodes via edge weights\n˜ai,j. Since the edge information of the underlying graph\ntransforms with the transformations of individual nodes as\ndemonstrated in Fig. 3, edge convolution is able to extract\nhigher-level features from the original and transformed edge\ninformation. Also, as features of each node are learned via\npropagation from transformed and non-transformed nodes\nisotropically or anisotropically by both local or global sam-\npling, the learned representation is able to capture intrinsic\ngraph structures at multiple scales.\n4.2.2\nTransformation Decoder\nNode-wise features of the original and transformed graphs\nare then concatenated at each node, which are then fed into\nthe transformation decoder. The decoder consists of several\nEdgeConv blocks to aggregate the representations of both\nthe original and transformed graphs to predict the node-\nwise transformations t. Based on the loss in (5), t is de-\ncoded by minimizing the mean squared error (MSE) be-\ntween the ground truth and estimated transformation param-\neters at each sampled node. Fig. 4 illustrates the architecture\nof learning the proposed GraphTER in such an auto-encoder\nstructure.\n5. Experiments\nIn this section, we evaluate the GraphTER model by ap-\nplying it to graphs of 3D point cloud data on two repre-\nsentative downstream tasks: point cloud classiﬁcation and\nsegmentation. We compare the proposed method with state-\nof-the-art supervised and unsupervised approaches.\n5.1. Datasets and Experimental Setup\nModelNet40 [47]. This dataset contains 12, 311 meshed\nCAD models from 40 categories, where 9, 843 models are\nused for training and 2, 468 models are for testing. For each\nmodel, 1, 024 points are sampled from the original mesh.\nWe train the unsupervised auto-encoder and the classiﬁer\nunder the training set, and evaluate the classiﬁer under the\ntesting set.\nShapeNet part [49]. This dataset contains 16, 881 3D\npoint clouds from 16 object categories, annotated with 50\nparts. Each 3D point cloud contains 2, 048 points, most of\nTable 1. Classiﬁcation accuracy (%) on ModelNet40 dataset.\nMethod\nYear\nUnsupervised\nAccuracy\n3D ShapeNets [47]\n2015\nNo\n84.7\nVoxNet [30]\n2015\nNo\n85.9\nPointNet [32]\n2017\nNo\n89.2\nPointNet++ [33]\n2017\nNo\n90.7\nKD-Net [21]\n2017\nNo\n90.6\nPointCNN [25]\n2018\nNo\n92.2\nPCNN [2]\n2018\nNo\n92.3\nDGCNN [44]\n2019\nNo\n92.9\nRS-CNN [28]\n2019\nNo\n93.6\nT-L Network [13]\n2016\nYes\n74.4\nVConv-DAE [39]\n2016\nYes\n75.5\n3D-GAN [45]\n2016\nYes\n83.3\nLGAN [1]\n2018\nYes\n85.7\nFoldingNet [48]\n2018\nYes\n88.4\nMAP-VAE [15]\n2019\nYes\n90.2\nL2G-AE [27]\n2019\nYes\n90.6\nGraphTER\nYes\n92.0\nwhich are labeled with fewer than six parts. We employ\n12, 137 models for training the auto-encoder and the classi-\nﬁer, and 2, 874 models for testing.\nWe treat points in each point cloud as nodes in a graph,\nand the (x, y, z) coordinates of points as graph signals. A\nkNN graph is then constructed on the graph signals to guide\ngraph convolution.\nNext, we introduce our node-wise graph signal transfor-\nmation. In experiments, we sample a portion of nodes with\na sampling rate r from the entire graph to perform node-\nwise transformations, including 1) Global sampling: ran-\ndomly sample r% of points from all the points in a 3D point\ncloud; 2) Local sampling: randomly choose a point and\nsearch its k nearest neighbors in terms of Euclidean dis-\ntance, forming a local set of r% of points.\nThen, we apply three types of node-wise transformations\nto the coordinates of point clouds, including 1) Transla-\ntion: randomly translate each of three coordinates of a point\nby three parameters in the range [−0.2, 0.2]; 2) Rotation:\nrandomly rotate each point with three rotation parameters\nall in the range [−5◦, 5◦]; 3) Shearing: randomly shear the\nx-, y-, z-coordinates of each point with the six parameters\nof a shearing matrix in the range [−0.2, 0.2]. We consider\ntwo strategies to transform the sampled nodes: Isotrop-\nically or Anisotropically, which applies transformations\nwith node-invariant or node-variant parameters.\n5.2. Point Cloud Classiﬁcation\nFirst, we evaluate the GraphTER model on the Model-\nNet40 [47] dataset for point cloud classiﬁcation.\n5.2.1\nImplementation Details\nIn this task, the auto-encoder network is trained via the\nSGD optimizer with a batch size of 32. The momentum\nand weight decay rate are set to 0.9 and 10−4, respectively.\nThe initial learning rate is 0.1, and then decayed using a co-\nsine annealing schedule [29] for 512 training epochs. We\nadopt the cross entropy loss to train the classiﬁer.\nWe deploy eight EdgeConv layers as the encoder, and the\nnumber k of nearest neighbors is set to 20 for all EdgeConv\nlayers. Similar to [44], we use shortcut connections for the\nﬁrst ﬁve layers to extract multi-scale features, where we\nconcatenate features from these layers to acquire a 1, 024-\ndimensional node-wise feature vector. After the encoder,\nwe employ three consecutive EdgeConv layers as the de-\ncoder – the output feature representations of the Siamese en-\ncoder ﬁrst go through a channel-wise concatenation, which\nare then fed into the decoder to estimate node-wise trans-\nformations. The batch normalization layer and LeakyReLU\nactivation function with a negative slope of 0.2 is employed\nafter each convolutional layer.\nDuring the training procedure of the classiﬁer, the ﬁrst\nﬁve EdgeConv layers in the encoder are used to represent\ninput cloud data by node-wise concatenating their output\nfeatures with the weights frozen. After the ﬁve EdgeConv\nlayers, we apply three fully-connected layers node-wise to\nthe aggregated features. Then, global max pooling and av-\nerage pooling are deployed to acquire the global features,\nafter which three fully-connected layers are used to map the\nglobal features to the classiﬁcation scores. Dropout with a\nrate of 0.5 is adopted in the last two fully-connected layers.\n5.2.2\nExperimental Results\nTab. 1 shows the results for 3D point cloud classiﬁca-\ntion, where the proposed model applies isotropic node-\nwise shearing transformation with a global sampling rate\nof r = 25%. We compare with two classes of methods:\nunsupervised approaches and supervised approaches. The\nGraphTER model achieves 92.0% of classiﬁcation accu-\nracy on the ModelNet40 dataset, which outperforms the\nstate-of-the-art unsupervised methods. In particular, most\nof the compared unsupervised models combine the ideas\nof both GAN and AED, and map 3D point clouds to un-\nsupervised representations by auto-encoding data, such as\nFoldingNet [48], MAP-VAE [15] and L2G-AE [27]. Re-\nsults show that the GraphTER model achieves signiﬁcant\nimprovement over these methods, showing the superiority\nof the proposed node-wise AET over both the GAN and\nAED paradigms.\nMoreover, the unsupervised GraphTER model also\nachieves comparable performance with the state-of-the-art\nfully supervised results. This signiﬁcantly closes the gap\nbetween unsupervised approaches and the fully supervised\ncounterparts in literature.\n5.2.3\nAblation Studies\nFurther, we conduct ablation studies under various exper-\nimental settings of sampling and transformation strategies\nTable 2. Unsupervised classiﬁcation accuracy (%) on ModelNet40\ndataset with different sampling and transformation strategies.\nGlobal Sampling\nLocal Sampling\nMean\nIso.\nAniso.\nIso.\nAniso.\nTranslation\n90.15\n90.15\n89.91\n89.55\n89.94\nRotation\n91.29\n90.24\n90.48\n89.87\n90.47\nShearing\n92.02\n90.32\n91.65\n89.99\n90.99\nMean\n91.15\n90.24\n90.68\n89.80\n90.70\n90.24\nTable 3. Unsupervised classiﬁcation accuracy (%) on ModelNet40\ndataset applying translation at different node sampling rates.\nSampling\nRate\nGlobal Sampling\nLocal Sampling\nMean\nIso.\nAniso.\nIso.\nAniso.\n25%\n90.15\n90.15\n89.91\n89.55\n89.94\n50%\n90.03\n89.63\n89.95\n89.47\n89.77\n75%\n91.00\n89.67\n91.41\n89.75\n90.46\n100%\n89.67\n89.99\n89.67\n89.99\n89.83\non the ModelNet40 dataset.\nFirst, we analyze the effectiveness of different node-wise\ntransformations under global or local sampling.\nTab. 2\npresents the classiﬁcation accuracy with three types of\nnode-wise transformation methods. We see that the shear-\ning transformation achieves the best performance, improv-\ning by 1.05% on average over translation, and 0.52% over\nrotation. This shows that the proposed GraphTER model is\nable to learn better feature representations under more com-\nplex transformations.\nMoreover, we see that the proposed model achieves an\naccuracy of 90.70% on average under global sampling,\nwhich outperforms local sampling by 0.46%. This is be-\ncause global sampling better captures the global structure of\ngraphs, which is crucial in such a graph-level task of classi-\nfying 3D point clouds. Meanwhile, under the two sampling\nstrategies, the classiﬁcation accuracy from isotropic trans-\nformations is higher than that from the anisotropic one. The\nreason lies in the intrinsic difﬁculty of training the transfor-\nmation decoder with increased complexity of more param-\neters when applying anisotropic transformations.\nMoreover, we evaluate the effectiveness of different sam-\npling rates r under the translations as reported in Tab. 3.\nThe classiﬁcation accuracies under various sampling rates\nare almost the same, and the result under r = 25% is com-\nparable to that under r = 100%. This shows that the perfor-\nmance of the proposed model is insensitive to the variation\nof sampling rates, i.e., applying node-wise transformations\nto a small number of nodes in the graph is sufﬁcient to learn\nintrinsic graph structures.\n5.3. Point Cloud Segmentation\nWe also apply the GraphTER model to 3D point cloud\npart segmentation on ShapeNet part dataset [49].\n5.3.1\nImplementation Details\nWe also use SGD optimizer to train the auto-encoding trans-\nformation network. The hyper-parameters are the same as\nin 3D point cloud classiﬁcation except that we train for 256\nepochs. We adopt the negative log likelihood loss to train\nthe node-wise classiﬁer for segmenting each point in the\nclouds.\nThe auto-encoding architecture is similar to that of the\nclassiﬁcation task, where we employ ﬁve EdgeConv layers\nas the encoder. However, the ﬁrst two EdgeConv blocks\nconsist of two MLP layers with the number of neurons {64,\n64} in each layer. We use shortcut connections to concate-\nnate features from the ﬁrst four layers to a 512-dimensional\nnode-wise feature vector.\nAs for the node-wise classiﬁer, we deploy the same ar-\nchitecture as in [44]. The output features from the encoder\nare concatenated node-wise with globally max-pooled fea-\ntures, followed by four fully-connected layers to classify\neach node. During the training procedure, the weights of the\nﬁrst four EdgeConv blocks in the encoder are kept frozen.\n5.3.2\nExperimental Results\nWe adopt the Intersection-over-Union (IoU) metric to eval-\nuate the performance. We follow the same evaluation pro-\ntocol as in the PointNet [32]: the IoU of a shape is com-\nputed by averaging the IoUs of different parts occurring in\nthat shape, and the IoU of a category is obtained by aver-\naging the IoUs of all the shapes belonging to that category.\nThe mean IoU (mIoU) is ﬁnally calculated by averaging the\nIoUs of all the test shapes.\nWe also compare the proposed model with unsuper-\nvised approaches and supervised approaches in this task,\nas listed in Tab. 4. We achieve a mIoU of 81.9%, which\nsigniﬁcantly outperforms the state-of-the-art unsupervised\nmethod MAP-VAE [15] by 13.9%.\nMoreover, the unsupervised GraphTER model also\nachieves the comparable performance to the state-of-the-art\nfully supervised approaches, greatly pushing closer towards\nthe upper bound set by the fully supervised counterparts.\n5.3.3\nVisualization Results\nFig. 5 visualizes the results of the proposed unsupervised\nmodel and two state-of-the-art fully supervised methods:\nDGCNN [44] and RS-CNN [28]. The proposed model pro-\nduces better segmentation on the “table” model in the ﬁrst\nrow, and achieves comparable results on the other mod-\nels. Further, we qualitatively compare the proposed method\nwith the state-of-the-art unsupervised method MAP-VAE\n[15], as illustrate in Fig. 6. The proposed model leads to\nmore accurate segmentation results than MAP-VAE, e.g.,\nthe engines of planes and the legs of chairs.\nTable 4. Part segmentation results on ShapeNet part dataset. Metric is mIoU(%) on points.\nUnsup.\nMean\nAero\nBag\nCap\nCar\nChair\nEar\nPhone\nGuitar Knife\nLamp\nLaptop Motor\nMug\nPistol\nRocket Skate\nBoard\nTable\nSamples\n2690\n76\n55\n898\n3758\n69\n787\n392\n1547\n451\n202\n184\n283\n66\n152\n5271\nPointNet [32]\nNo\n83.7\n83.4\n78.7\n82.5\n74.9\n89.6\n73.0\n91.5\n85.9\n80.8\n95.3\n65.2\n93.0\n81.2\n57.9\n72.8\n80.6\nPointNet++ [33]\nNo\n85.1\n82.4\n79.0\n87.7\n77.3\n90.8\n71.8\n91.0\n85.9\n83.7\n95.3\n71.6\n94.1\n81.3\n58.7\n76.4\n82.6\nKD-Net [21]\nNo\n82.3\n80.1\n74.6\n74.3\n70.3\n88.6\n73.5\n90.2\n87.2\n81.0\n94.9\n57.4\n86.7\n78.1\n51.8\n69.9\n80.3\nPCNN [2]\nNo\n85.1\n82.4\n80.1\n85.5\n79.5\n90.8\n73.2\n91.3\n86.0\n85.0\n95.7\n73.2\n94.8\n83.3\n51.0\n75.0\n81.8\nPointCNN [25]\nNo\n86.1\n84.1\n86.5\n86.0\n80.8\n90.6\n79.7\n92.3\n88.4\n85.3\n96.1\n77.2\n95.3\n84.2\n64.2\n80.0\n83.0\nDGCNN [44]\nNo\n85.2\n84.0\n83.4\n86.7\n77.8\n90.6\n74.7\n91.2\n87.5\n82.8\n95.7\n66.3\n94.9\n81.1\n63.5\n74.5\n82.6\nRS-CNN [28]\nNo\n86.2\n83.5\n84.8\n88.8\n79.6\n91.2\n81.1\n91.6\n88.4\n86.0\n96.0\n73.7\n94.1\n83.4\n60.5\n77.7\n83.6\nLGAN [1]\nYes\n57.0\n54.1\n48.7\n62.6\n43.2\n68.4\n58.3\n74.3\n68.4\n53.4\n82.6\n18.6\n75.1\n54.7\n37.2\n46.7\n66.4\nMAP-VAE [15]\nYes\n68.0\n62.7\n67.1\n73.0\n58.5\n77.1\n67.3\n84.8\n77.1\n60.9\n90.8\n35.8\n87.7\n64.2\n45.0\n60.4\n74.8\nGraphTER\nYes\n81.9\n81.7\n68.1\n83.7\n74.6\n88.1\n68.9\n90.6\n86.6\n80.0\n95.6\n56.3\n90.0\n80.8\n55.2\n70.7\n79.1\n(a) Ground-truth\n(b) DGCNN\n(c) RS-CNN\n(d) GraphTER\nFigure 5. Visual comparison of point cloud part segmentation\nwith supervised methods.\nOur unsupervised GraphTER learn-\ning achieves comparable results with the state-of-the art fully su-\npervised approaches.\n(a) MAP-VAE\n(b) GraphTER\nFigure 6. Visual comparison of point cloud part segmentation\nwith the state-of-the-art unsupervised method MAP-VAE. We\nachieve more accurate segmentation even in tiny parts and transi-\ntion regions.\n5.3.4\nAblation Studies\nSimilar to the classiﬁcation task, we analyze the effective-\nness of different node-wise transformations under global or\nlocal sampling, as presented in Tab. 5. The proposed model\nachieves the best performance under the shearing transfor-\nmation, improving by 1.23% on average over translation,\nTable 5. Unsupervised segmentation results on ShapeNet part\ndataset with different transformation strategies. Metric is mIoU\n(%) on points.\nGlobal Sampling\nLocal Sampling\nMean\nIso.\nAniso.\nIso.\nAniso.\nTranslation\n79.83\n79.88\n80.05\n79.85\n79.90\nRotation\n80.20\n80.29\n80.87\n80.02\n80.35\nShearing\n81.88\n80.28\n81.89\n80.48\n81.13\nMean\n80.64\n80.15\n80.94\n80.12\n80.39\n80.53\nand 0.78% over rotation, which demonstrates the beneﬁts\nof GraphTER learning under complex transformations.\nFurther, the proposed model achieves a mIoU of 80.53%\non average under local sampling, which outperforms global\nsampling by 0.14%. This is because local sampling of nodes\ncaptures the local structure of graphs better, which is crucial\nin node-level 3D point cloud segmentation task.\n6. Conclusion\nIn this paper, we propose a novel paradigm of learn-\ning graph transformation equivariant representation (Graph-\nTER) via auto-encoding node-wise transformations in an\nunsupervised fashion.\nWe allow it to sample different\ngroups of nodes from a graph globally or locally and\nthen perform node-wise transformations isotropically or\nanisotropically, which enables it to characterize morphable\nstructures of graphs at various scales. By decoding these\nnode-wise transformations, GraphTER enforces the en-\ncoder to learn intrinsic representations that contain sufﬁ-\ncient information about structures under applied transfor-\nmations. We apply the GraphTER model to classiﬁcation\nand segmentation of graphs of 3D point cloud data, and\nexperimental results demonstrate the superiority of Graph-\nTER over the state-of-the-art unsupervised approaches, sig-\nniﬁcantly closing the gap with the fully supervised counter-\nparts. We will apply the general GraphTER model to more\napplications as future works, such as node classiﬁcation of\ncitation networks.\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas. Learning representations and generative\nmodels for 3d point clouds. In Proceedings of the 35th In-\nternational Conference on Machine Learning (ICML), pages\n40–49, 2018.\n[2] Matan Atzmon, Haggai Maron, and Yaron Lipman. Point\nconvolutional neural networks by extension operators. ACM\nTransactions on Graphics (TOG), 37(4):1–12, July 2018.\n[3] Aleksandar Bojchevski, Oleksandr Shchur, Daniel Z¨ugner,\nand Stephan G¨unnemann. Netgan: Generating graphs via\nrandom walks. In Proceedings of the 35th International Con-\nference on Machine Learning (ICML), pages 609–618, 2018.\n[4] Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur\nSzlam, and Pierre Vandergheynst. Geometric deep learning:\ngoing beyond euclidean data. IEEE Signal Processing Mag-\nazine, 34(4):18–42, 2017.\n[5] Shaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural net-\nworks for learning graph representations. In Thirtieth AAAI\nConference on Artiﬁcial Intelligence (AAAI), pages 1145–\n1152, 2016.\n[6] Taco Cohen and Max Welling.\nGroup equivariant con-\nvolutional networks.\nIn Proceedings of the 33rd Inter-\nnational Conference on Machine Learning (ICML), pages\n2990–2999, 2016.\n[7] Nicola De Cao and Thomas Kipf. Molgan: An implicit gen-\nerative model for small molecular graphs.\narXiv preprint\narXiv:1805.11973, 2018.\n[8] Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu.\nExploiting cyclic symmetry in convolutional neural net-\nworks. In Proceedings of the 33rd International Conference\non Machine Learning (ICML), pages 1889–1898, 2016.\n[9] Sander Dieleman, Kyle W Willett, and Joni Dambre.\nRotation-invariant convolutional neural networks for galaxy\nmorphology prediction. Monthly Notices of the Royal Astro-\nnomical Society, 450(2):1441–1459, 2015.\n[10] Jeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Ad-\nversarial feature learning. In International Conference on\nLearning Representations (ICLR), 2017.\n[11] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier\nMastropietro, Alex Lamb, Martin Arjovsky, and Aaron\nCourville. Adversarially learned inference. In International\nConference on Learning Representations (ICLR), 2017.\n[12] Robert Gens and Pedro M Domingos. Deep symmetry net-\nworks. In Advances in Neural Information Processing Sys-\ntems (NIPS), pages 2537–2545, 2014.\n[13] Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Ab-\nhinav Gupta. Learning a predictable and generative vector\nrepresentation for objects. In European Conference on Com-\nputer Vision (ECCV), pages 484–499. Springer, 2016.\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems (NIPS), pages 2672–\n2680, 2014.\n[15] Zhizhong Han, Xiyang Wang, Yu-Shen Liu, and Matthias\nZwicker. Multi-angle point cloud-vae: Unsupervised feature\nlearning for 3d point clouds from multiple angles by joint\nself-reconstruction and half-to-half prediction. In The IEEE\nInternational Conference on Computer Vision (ICCV), Octo-\nber 2019.\n[16] Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang.\nTransforming auto-encoders. In International Conference on\nArtiﬁcial Neural Networks (ICANN), pages 44–51. Springer,\n2011.\n[17] Geoffrey E Hinton and Richard S Zemel.\nAutoencoders,\nminimum description length and helmholtz free energy. In\nAdvances in Neural Information Processing Systems (NIPS),\npages 3–10, 1994.\n[18] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. In International Conference on Learning Rep-\nresentations (ICLR), 2014.\n[19] Thomas N Kipf and Max Welling. Variational graph auto-\nencoders. In Proceedings of the NIPS Workshop on Bayesian\nDeep Learning, 2016.\n[20] Jyri J Kivinen and Christopher KI Williams. Transforma-\ntion equivariant boltzmann machines. In International Con-\nference on Artiﬁcial Neural Networks (ICANN), pages 1–9.\nSpringer, 2011.\n[21] Roman Klokov and Victor Lempitsky. Escape from cells:\nDeep kd-networks for the recognition of 3d point cloud mod-\nels. In Proceedings of the IEEE International Conference on\nComputer Vision (ICCV), pages 863–872, 2017.\n[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In Advances in Neural Information Processing Sys-\ntems (NIPS), pages 1097–1105, 2012.\n[23] Karel Lenc and Andrea Vedaldi. Understanding image repre-\nsentations by measuring their equivariance and equivalence.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 991–999, 2015.\n[24] Jan Eric Lenssen, Matthias Fey, and Pascal Libuschewski.\nGroup equivariant capsule networks. In Advances in Neural\nInformation Processing Systems (NIPS), pages 8844–8853,\n2018.\n[25] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di,\nand Baoquan Chen. Pointcnn: Convolution on x-transformed\npoints. In Advances in Neural Information Processing Sys-\ntems (NIPS), pages 820–830, 2018.\n[26] Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and\nPeter Battaglia. Learning deep generative models of graphs.\narXiv preprint arXiv:1803.03324, 2018.\n[27] Xinhai Liu, Zhizhong Han, Xin Wen, Yu-Shen Liu, and\nMatthias Zwicker. L2g auto-encoder: Understanding point\nclouds by local-to-global reconstruction with hierarchical\nself-attention.\nIn Proceedings of the 27th ACM Interna-\ntional Conference on Multimedia (ACM MM), pages 989–\n997. ACM, 2019.\n[28] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong\nPan. Relation-shape convolutional neural network for point\ncloud analysis. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n8895–8904, 2019.\n[29] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. In International Conference on\nLearning Representations (ICLR), 2017.\n[30] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-\nvolutional neural network for real-time object recognition.\nIn 2015 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 922–928. IEEE, 2015.\n[31] Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao,\nand Chengqi Zhang.\nAdversarially regularized graph au-\ntoencoder for graph embedding. In International Joint Con-\nference on Artiﬁcial Intelligence (IJCAI), pages 2609–2615,\n2018.\n[32] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classiﬁcation\nand segmentation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n652–660, 2017.\n[33] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In Advances in Neural Informa-\ntion Processing Systems (NIPS), pages 5099–5108, 2017.\n[34] Guo-Jun Qi. Learning generalized transformation equivari-\nant representations via autoencoding transformations. arXiv\npreprint arXiv:1906.08628, 2019.\n[35] Guo-Jun Qi, Liheng Zhang, Chang Wen Chen, and Qi Tian.\nAvt: Unsupervised learning of transformation equivariant\nrepresentations by autoencoding variational transformations.\nProceedings of the IEEE International Conference on Com-\nputer Vision (ICCV), 2019.\n[36] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glo-\nrot, and Yoshua Bengio.\nContractive auto-encoders: Ex-\nplicit invariance during feature extraction. In Proceedings\nof the 28th International Conference on Machine Learning\n(ICML), pages 833–840. Omnipress, 2011.\n[37] Aliaksei Sandryhaila and Jos´e MF Moura. Discrete signal\nprocessing on graphs. IEEE transactions on Signal Process-\ning, 61(7):1644–1656, 2013.\n[38] Uwe Schmidt and Stefan Roth. Learning rotation-aware fea-\ntures: From invariant priors to equivariant descriptors. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 2050–2057. IEEE,\n2012.\n[39] Abhishek Sharma, Oliver Grau, and Mario Fritz.\nVconv-\ndae: Deep volumetric shape learning without object labels.\nIn European Conference on Computer Vision (ECCV), pages\n236–250. Springer, 2016.\n[40] Henrik Skibbe. Spherical Tensor Algebra for Biomedical Im-\nage Analysis. PhD thesis, Verlag nicht ermittelbar, 2013.\n[41] Kihyuk Sohn and Honglak Lee.\nLearning invariant rep-\nresentations with local transformations.\nIn Proceedings\nof the 29th International Conference on Machine Learning\n(ICML), pages 1339–1346, 2012.\n[42] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol.\nExtracting and composing ro-\nbust features with denoising autoencoders. In Proceedings\nof the 25th International Conference on Machine Learning\n(ICML), pages 1096–1103. ACM, 2008.\n[43] Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep\nnetwork embedding. In Proceedings of the ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data\nMining (KDD), pages 1225–1234. ACM, 2016.\n[44] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon.\nDynamic\ngraph cnn for learning on point clouds. ACM Transactions\non Graphics (TOG), 38(5):146, 2019.\n[45] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of ob-\nject shapes via 3d generative-adversarial modeling. In Ad-\nvances in Neural Information Processing Systems (NIPS),\npages 82–90, 2016.\n[46] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long,\nChengqi Zhang, and Philip S Yu. A comprehensive survey\non graph neural networks. arXiv preprint arXiv:1901.00596,\n2019.\n[47] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets: A deep representation for volumetric shapes. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 1912–1920, 2015.\n[48] Yaoqing Yang, Chen Feng, Yiru Shen, and Dong Tian. Fold-\ningnet: Point cloud auto-encoder via deep grid deformation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 206–215, 2018.\n[49] Li Yi, Vladimir G Kim, Duygu Ceylan, I Shen, Mengyan\nYan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer,\nLeonidas Guibas, et al. A scalable active framework for re-\ngion annotation in 3d shape collections. ACM Transactions\non Graphics (TOG), 35(6):210, 2016.\n[50] Jiaxuan You, Rex Ying, Xiang Ren, William L Hamilton,\nand Jure Leskovec. GraphRNN: A deep generative model for\ngraphs. In Proceedings of the 35th International Conference\non Machine Learning (ICML), pages 5694–5703, 2018.\n[51] Liheng Zhang, Guo-Jun Qi, Liqiang Wang, and Jiebo Luo.\nAet vs. aed: Unsupervised representation learning by auto-\nencoding transformations rather than data. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2547–2555, 2019.\n[52] Yingxue Zhang and Michael Rabbat. A graph-cnn for 3d\npoint cloud classiﬁcation. In IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pages\n6279–6283. IEEE, 2018.\n[53] Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on\ngraphs: A survey. arXiv preprint arXiv:1812.04202, 2018.\n[54] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang,\nZhiyuan Liu, and Maosong Sun.\nGraph neural networks:\nA review of methods and applications.\narXiv preprint\narXiv:1812.08434, 2018.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2019-11-19",
  "updated": "2020-03-19"
}