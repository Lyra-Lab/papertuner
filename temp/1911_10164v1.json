{
  "id": "http://arxiv.org/abs/1911.10164v1",
  "title": "Efficient Exploration through Intrinsic Motivation Learning for Unsupervised Subgoal Discovery in Model-Free Hierarchical Reinforcement Learning",
  "authors": [
    "Jacob Rafati",
    "David C. Noelle"
  ],
  "abstract": "Efficient exploration for automatic subgoal discovery is a challenging\nproblem in Hierarchical Reinforcement Learning (HRL). In this paper, we show\nthat intrinsic motivation learning increases the efficiency of exploration,\nleading to successful subgoal discovery. We introduce a model-free subgoal\ndiscovery method based on unsupervised learning over a limited memory of\nagent's experiences during intrinsic motivation. Additionally, we offer a\nunified approach to learning representations in model-free HRL.",
  "text": "Efﬁcient Exploration through Intrinsic Motivation Learning for Unsupervised\nSubgoal Discovery in Model-Free Hierarchical Reinforcement Learning\nJacob Rafati 1 David C. Noelle 1\nAbstract\nEfﬁcient exploration for automatic subgoal dis-\ncovery is a challenging problem in Hierarchical\nReinforcement Learning (HRL). In this paper, we\nshow that intrinsic motivation learning increases\nthe efﬁciency of exploration, leading to success-\nful subgoal discovery. We introduce a model-free\nsubgoal discovery method based on unsupervised\nlearning over a limited memory of agent’s expe-\nriences during intrinsic motivation. Additionally,\nwe offer a uniﬁed approach to learning represen-\ntations in model-free HRL.\n1. Introduction\nModel-free Reinforcement Learning (RL) algorithms, at-\ntempts to ﬁnd an optimal policy through learning the values\nof agent’s actions at any state by computing the expected\nfuture rewards without having access to a model of the en-\nvironment (Sutton, 1988). To learn an efﬁcient policy, the\nagent should balance its exploration, i.e. visiting already\nobserved rewarding states, with its exploration, i.e. search-\ning for better rewarding states. Exploitation is an strategy to\nmaximize the expected reward on the one step, but explo-\nration may lead to a greater return, i.e. total rewards, in the\nlong run (Sutton & Barto, 1998).\nOne of the challenges that arise in RL in real-world prob-\nlems is that the state space can be very large. This has\nclassically been called the curse of dimensionality. Non-\nlinear function approximators coupled with reinforcement\nlearning have made it possible to learn abstractions over\nhigh dimensional state spaces (Rafati & Noelle, 2015; 2017;\n2019e; Mnih et al., 2015).\nCommon approaches to exploration, such as the ϵ-greedy\nmethod (Sutton & Barto, 1998), are not sufﬁciently efﬁ-\ncient in exploring the state space to succeed on large-scale\ncomplex problems with sparse delayed rewards feedback\n1Electrical Engineering and Computer Science, University of\nCalifornia, Merced, CA, USA. Correspondence to: Jacob Rafati\n<jrafatiheravi@ucmerced.edu>.\nJacob Rafati’s Website: http://rafati.net.\n(Bellemare et al., 2016). Exploration methods based on nov-\nelty detection (Meyer & Wilson, 1991; Achiam & Sastry,\n2017) and curiosity-driven learning (Burda et al., 2019) have\nbeen particularly successful in sparse tasks but these meth-\nods typically require a generative or predictive model of the\nstate transition probabilities , which can be difﬁcult to train\nwhen the states space are very high-dimensional (Fu et al.,\n2017; Bellemare et al., 2016). Learning representations of\nthe value function is challenging for these tasks, since the\nagent receives an undiagnostic constant value, such as r = 0\nfor most experiences.\nHierarchical Reinforcement Learning (HRL) methods at-\ntempt to address the issues of RL in sparse tasks (Barto &\nMahadevan, 2003; Hengst, 2010; Dayan & Hinton, 1992;\nDietterich, 2000) by learning to operate over different levels\nof temporal abstraction (Sutton et al., 1999; Parr & Russell,\n1997; Krishnamurthy et al., 2016). One approach to tempo-\nral abstraction involves identifying a set of states that make\nfor useful subgoals. This introduces a major open problem\nin HRL: that of subgoal discovery. The efﬁcient exploration\nof the environment states has a direct effect on successful\nsubgoal discovery. Some approaches to subgoal discovery\nmaintain the value function in a large look-up table (Sutton\net al., 1999; Goel & Huber, 2003; S¸ims¸ek et al., 2005; Mc-\nGovern & Barto, 2001), and most of these methods require\nbuilding the state transition graph, providing a model of\nthe environment and the agent’s possible interactions with\nit (Machado et al., 2017; S¸ims¸ek et al., 2005; Goel & Huber,\n2003). Subgoal discovery problem is speciﬁcally challeng-\ning for the model-free HRL framework, since the agent does\nnot have access to a model of the environment.\nOnce useful subgoals are discovered, an HRL agent should\nbe able to learn the skills to attain those subgoals through\nthe use of intrinsic motivation — artiﬁcially rewarding the\nagent for attaining selected subgoals (Singh et al., 2010;\nVigorito & Barto, 2010). In such systems, knowledge of the\ncurrent subgoal is needed to estimate future intrinsic reward,\nresulting in value functions that consider subgoals along\nwith states (Vezhnevets et al., 2017). Such a parameterized\nuniversal value function, q(s, g, a; w), integrates the value\nfunctions for multiple skills into a single function, taking\nthe current subgoal, g, as an argument. Intrinsic motivation\narXiv:1911.10164v1  [cs.LG]  18 Nov 2019\nEfﬁcient Exploration through Intrinsic Motivation Learning for Unsupervised Subgoal Discovery in Model-Free HRL\nintend to provide a way for exploration while learning useful\nskills (Barto & Mahadevan, 2003). Bellemare et al. (2016)\nhas shown a connection between theoretical foundations of\nintrinsic motivation and the count-based exploration meth-\nods. However, the role of intrinsic motivation learning in\nefﬁcient exploration of sparse tasks, its effect on learning\nrepresentations in model-free HRL and its connection to the\nsubgoal discovery problem are open research problems.\nIt is important to note that model-free HRL, which does\nnot require a model of the environment, still often requires\nthe learning of useful internal representations of states. Re-\ncently, Kulkarni et al. (2016) has offered a model-free HRL\napproach, called Meta-controller and Controller framework,\nin order to integrate temporal abstraction and intrinsic moti-\nvation learning and successfully solved the ﬁrst room of the\nMontezuma’s Revenge. But, their method relied on a prior\nknowledge of the environment, including manual selection\nof interesting objects as subgoals for intrinsic motivation\nlearning.\nIn our previous work, Rafati (2019); Rafati & Noelle\n(2019c;a), we have addressed major open problems in the\nintegration of internal representation learning, temporal ab-\nstraction, automatic subgoal discovery, and intrinsic motiva-\ntion learning, all within the model-free HRL framework We\npropose and implement efﬁcient and general methods for\nsubgoal discovery using unsupervised learning and anomaly\n(outlier) detection (Rafati & Noelle, 2019d;b). The pro-\nposed method do not require information beyond that which\nis typically collected by the agent during model-free re-\ninforcement learning, such as a small memory of recent\nexperiences (agent trajectories). In our proposed approach\nfor learning representations in model-free HRL, we were\nfundamentally constrained in three ways, by design. First,\nwe remained faithful to a model-free reinforcement learning\nframework, eschewing any approach that requires the learn-\ning or use of an environment model. Second, we were de-\nvoted to integrating subgoal discovery with intrinsic motiva-\ntion learning, and temporal abstraction. Lastly, we focused\non subgoal discovery algorithms that are likely to scale to\nlarge reinforcement learning tasks. The result was a uniﬁed\nmodel-free HRL algorithm that incorporates the learning of\nuseful internal representations of states, automatic subgoal\ndiscovery, intrinsic motivation learning of skills, and the\nlearning of subgoal selection by a “meta-controller”. We\ndemonstrated the effectiveness of this algorithm by applying\nit to a variant of the rooms task, as well as the initial screen\nof the ATARI 2600 game called Montezuma’s Revenge.\nIn this paper, we investigate the role of intrinsic motiva-\ntion learning on efﬁcient exploration of environments with\nsparse delayed rewards feedback, and its connection to the\nsubgoal discovery problem in our model-free HRL frame-\nwork. We introduce an efﬁcient and general method for\nsubgoal discovery using unsupervised learning methods,\nsuch as K-means clustering and anomaly detection, over a\nsmall memory of agent’s experiences (trajectories) during\nintrinsic motivation learning. Finally, we conjecture that\nintrinsic motivation learning can increase appropriate state\nspace coverage, and it produces a policy for efﬁcient ex-\nploration that leads to a successful subgoal discovery. We\ndemonstrate the effectiveness of our method on the rooms\ntask (Figure 2(a)), as well as the initial screen of the Mon-\ntezuma’s Revenge (Figure 3(a)).\n2. A Model-Free HRL Framework\n2.1. Meta-controller/Controller Framework\nInspired by Kulkarni et al. (2016) we start by using two\nlevels of hierarchy for temporal abstraction learning (Figure\n1). The more abstract level of this hierarchy is managed\nby a meta-controller which guides the action selection pro-\ncesses of the lower level controller. This approach leads\nto integration of temporal abstraction and intrinsic motiva-\ntion learning in deep model-free HRL framework. Separate\nvalue functions are learned for the meta-controller and the\ncontroller.\nAt time step t, the meta-controller receives a state observa-\ntion, s = st, from the environment. It then selects a subgoal,\ng = gt, from a set of subgoals, G from an ϵ-greedy policy de-\nrived from the meta-controller’s value function, Q(s, g; W).\nWith the current subgoal selected, the controller uses its\npolicy to select an action, a ∈A, based on the current\nstate, s, and the current subgoal, g. We used ϵ-greedy policy\nderived from the controller’s value function, q(s, g, a; w),\nto choose an action a. In next time step, agent recieves a\nreward rt+1 = r, and the next state, st+1 = s′, and stores\nits direct experiences with the environment into an expe-\nrience memory, D, Actions continue to be selected by the\ncontroller while an internal critic monitors the current state,\ncomparing it to the current subgoal, and delivering an appro-\npriate intrinsic reward, ˜r, to the controller on each time step.\nEach transition experience, (s, g, a, ˜r, s′), is recorded in the\ncontroller’s experience memory set, D1, to support learning.\nWhen the subgoal is attained, or a maximum amount of\ntime has passed, the meta-controller observes the resulting\nstate, st′ = st+T +1, and selects another subgoal, g′. The\ntransition experience for the meta-controller, (s, g, G, st′)\nis recorded in the meta-controller’s experience memory set,\nD2, where G = Pt+T\nt′=t γt′−trt′ is the return between the\nselection of consecutive subgoals. For training the meta-\ncontroller value function, Q(s, g; W), we minimize its loss\nfunction based on the experience received from the environ-\nment, D2. The controller improves its subpolicy, π(a|s, g),\nby learning its value function, q(s, g, a; w), through mini-\nmization of its loss function over intrinsic experiences, D1.\nEfﬁcient Exploration through Intrinsic Motivation Learning for Unsupervised Subgoal Discovery in Model-Free HRL\n2.2. Unsupervised Subgoal Discovery\nThe performance of the meta-controller/controller frame-\nwork depends critically on selecting good candidate sub-\ngoals for the meta-controller to consider. In Kulkarni et al.\n(2016)’s approach to model-free HRL, the subgoals are\nmanually speciﬁed for each task, and hence, the subgoal\ndiscovery in a model-free HRL framework and its contribu-\ntion to learning representations of the meta-controller and\ncontroller value functions have not been addressed. In or-\nder to integrate the automatic subgoal discovery into the\nmeta-controller/controller framework in model-free HRL,\nwe should only use the information available from the intrin-\nsic motivation learning. Our strategy for subgoal discovery\ninvolves applying unsupervised learning methods to a re-\ncent experience memory, D, to identify sets of states that\nmay be good subgoal candidates. We focus speciﬁcally on\ntwo kinds of analysis that can be performed on the set of\ntransition experiences. We hypothesize that good subgoals\nmight be found by (1) attending to the states associated\nwith anomalous transition experiences and (2) clustering\nexperiences based on a similarity measure and collecting\nthe set of associated states into a potential subgoal. Thus,\nour proposed method merges anomaly (outlier) detection\nwith the K-means clustering of experiences.\nThe anomaly (outlier) detection process identiﬁes states\nassociated with experiences that differ signiﬁcantly from\nthe others. In the context of subgoal discovery, a relevant\nanomalous experience would be one that includes a sub-\nstantial positive reward in an environment in which reward\nis sparse. For example, in the rooms task, transitions that\narrive at the key or the lock are quite dissimilar to most\ntransitions, due to the large positive reward that is received\nat that point (see Figure 2 (b-d)). Similarity in the Mon-\ntezuma’s Revenge game, action that lead to or the doors\nlead to rewarding experiences. Additionally, when agent\nenters a new room, the state becomes very different than\nthe states from the previous room, allowing an anomaly\ndetection method to identify the door that leads to a new\nroom as a potentially useful subgoal.\nThe idea behind using the clustering of experiences involves\nboth “spatial” state space abstraction and dimensionality\nreduction with regard to the internal representations of states.\nThe learning process might be made faster by considering\nrepresentative states, such as cluster centroids as candidate\nsubgoals, rather than considering all the states. For example,\nin the rooms task, the centroids of K-means clusters, with\nK = 4 (Figure 2 (b)), lie close to the geometric center\nof each room, with the states within each room coming to\nbelong to the corresponding subgoal’s cluster. In this way,\nthe clustering of transition experiences can approximately\nproduce a coarser representation of state space, in this case\nreplacing the ﬁne grained “grid square location” with the\ncoarser “room location”. Also, K-means clustering found\nuseful subgoal regions, such as ladders, stages, and the rope\nin Montezuma’s Revenge game (Figure 3(c)).\n2.3. A Uniﬁed Approach to Model-Free HRL\nHere, we offer a framework to integrate learning represen-\ntations of the meta-controller and controller value func-\ntions, and also unsupervised subgoal discovery for model-\nfree HRL approach. The agent’s experience memory, D,\nwhich has been called experience replay memory in deep\nQ-learning, is the necessary element for integrating the HRL\ncomponents into a uniﬁed framework. The information ﬂow\nbetween the components of our uniﬁed model-free HRL is\ndepicted in Figure 1. We applied this method to a variant of\nrooms tasks with a key and a box (Figure 2 (a)), and also\na ﬁrst screen of the Montezuma’s Revenge ATARI game\n(Figure 3(a)). In these simulations, learning occurred in one\nuniﬁed phase. The meta-controller and the controller, and\nunsupervised subgoal discovery, were trained all together.\nThe average return for the uniﬁed HRL method, and reg-\nular RL is shown in Figure 2(f). In Montezua’s Revenge,\nthe controller was initially trained to navigate the man in\nred to random subgoals on the screen, derived from the\nCanny edge detection algorithm (see Figure 3(b)). Using\nthis strategy, the agent learned navigation skills, detected\nthe rewarding states: key and doors, and other interesting\nregions. Our model-free HRL could solved this room, while\ndeep Q-learning networks (Mnih et al., 2015) could not.\nFigure 1. (a) The information ﬂow in our uniﬁed model-free hier-\narchical reinforcement learning framework.\n2.4. Intrinsic Motivation for Efﬁcient Exploration\nIntrinsic motivation learning is the core idea behind the\nlearning of the value function for the controller. The in-\ntrinsic critic in this HRL framework can send much more\nregular feedback to the controller, since it is based on at-\ntaining subgoals, rather than ultimate goals. In our uniﬁed\nmodel-free HRL, the intrinsic motivation learning plays two\nmajor roles: (1) Learning skills to go from any observable\nstates to other region of states through learning subpolices\nEfﬁcient Exploration through Intrinsic Motivation Learning for Unsupervised Subgoal Discovery in Model-Free HRL\nby the controller. (2) Providing efﬁcient exploration to col-\nlect experiences that can be used for unsupervised subgoal\ndiscovery. The state space coverage rate, i.e. the number of\nvisited states divided by the size of states space during train-\ning rooms tasks is shown in Figure 2(e). Intrinsic motivation\nlearning coupled with unsupervised subgoal discovery and\na random meta-controller can visit 67% of the states. A\nregular Q-learning method converges to a solution that can\nﬁnds and picks the key, but it doesn’t have motivation to\nexplore other regions to ﬁnd more rewarding state, i.e. the\nbox. When intrinsic motivation learning of the controller is\nintegrated with unsupervised subgoal discovery and a meta-\ncontroller, the uniﬁed method can successfully cover 100%\nof the states. A random walk in Montezuma’s Revenge\ncan only visit two ladders and the rope (see Figure 3 (d)).\nBut intrinsic motivation learning with unsupervised subgoal\ndiscovery can lead to discovery of all meaningful regions of\nthe screen including rewarding ones (Figure 3(c)).\n3. Conclusions\nWe introduced a novel method for subgoal discovery, using\nunsupervised learning over a small memory of the most re-\ncent experiences of the agent. Intrinsic motivation learning\nprovides a policy for efﬁcient exploration of sparse tasks\nthat leads to successful unsupervised subgoal discovery. We\ninvestigated the role of the intrinsic motivation learning\non efﬁcient exploration of the observable state space for\ndiscovering useful subgoals.\nReferences\nAchiam, J. and Sastry, S.\nSurprise-based intrinsic\nmotivation for deep reinforcement learning.\nCoRR,\nabs/1703.01732, 2017. URL http://arxiv.org/\nabs/1703.01732.\nBarto, A. G. and Mahadevan, S. Recent advances in hierar-\nchical reinforcement learning. Discrete Event Dynamic\nSystems, 13(1):41–77, 2003.\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Sax-\nton, D., and Munos, R. Unifying count-based exploration\nand intrinsic motivation. In Advances in Neural Informa-\ntion Processing Systems 29, pp. 1471–1479. 2016.\nBurda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T.,\nand Efros, A. A. Large-scale study of curiosity-driven\nlearning. In ICLR, 2019.\nS¸ims¸ek, O., Wolfe, A. P., and Barto, A. G. Identifying\nuseful subgoals in reinforcement learning by local graph\npartitioning. In Proceedings of the 22nd International\nConference on Machine Learning, pp. 816–823, 2005.\nDayan, P. and Hinton, G. E. Feudal reinforcement learning.\nIn NeurIPS, 1992.\nDietterich, T. G. Hierarchical reinforcement learning with\nthe MAXQ value function decomposition. 13:227–303,\n2000.\nFu, J., Co-Reyes, J. D., and Levine, S. EX2: exploration\nwith exemplar models for deep reinforcement learning.\narXiv e-print (arXiv:1703.01260), 2017.\nGoel, S. and Huber, M. Subgoal discovery for hierarchical\nreinforcement learning using learned policies. In FLAIRS\nConference, pp. 346–350. AAAI Press, 2003.\nHengst, B. Hierarchical Reinforcement Learning, pp. 495–\n502. Springer US, Boston, MA, 2010.\nKrishnamurthy, R., Lakshminarayanan, A. S., Kumar, P.,\nand Ravindran, B. Hierarchical reinforcement learning\nusing spatio-temporal abstractions and deep neural net-\nworks. CoRR, abs/1605.05359, 2016.\nKulkarni, T. D., Narasimhan, K., Saeedi, A., and Tenen-\nbaum, J. Hierarchical deep reinforcement learning: Inte-\ngrating temporal abstraction and intrinsic motivation. In\nAdvances in Neural Information Processing Systems, pp.\n3675–3683, 2016.\nMachado, M. C., Bellemare, M. G., and Bowling, M. H.\nA laplacian framework for option discovery in reinforce-\nment learning. In Proceedings of the 34th International\nConference on Machine Learning, ICML 2017, Sydney,\nNSW, Australia, 6-11 August 2017, pp. 2295–2304, 2017.\nMcGovern, A. and Barto, A. G. Automatic discovery of sub-\ngoals in reinforcement learning using diverse density. In\nProceedings of the Eighteenth International Conference\non Machine Learning, ICML ’01, pp. 361–368, 2001.\nISBN 1-55860-778-1.\nMeyer, J. and Wilson, S. W. A Possibility for Implement-\ning Curiosity and Boredom in Model-Building Neural\nControllers. 1991.\nMnih, V., Kavukcuoglu, K., Silver, D., et al. Human-level\ncontrol through deep reinforcement learning. Nature, 518\n(7540):529–533, 2015.\nParr, R. and Russell, S. J. Reinforcement learning with\nhierarchies of machines. In NeurIPS, 1997.\nRafati, J. Learning Representations in Reinforcement Learn-\ning. PhD thesis, University of California, Merced, 2019.\nRafati, J. and Noelle, D. C. Lateral inhibition overcomes\nlimits of temporal difference learning. In 37th Annual\nCognitive Science Society Meeting, Pasadena, CA, USA,\n2015.\nEfﬁcient Exploration through Intrinsic Motivation Learning for Unsupervised Subgoal Discovery in Model-Free HRL\n(a)\n(b)\n(c)\n(d)\n0\n200\n400\n600\n800\n1000\nTraining Episodes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nState Space Coverage Rate\nIntrinsic Motivation within Uniﬁed HRL\nIntrinsic Motivation with Random Subgoals Selection\nRegular RL\nRandom Walk\n0\n20000\n40000\n60000\n80000\n100000\nTraining Episodes\n−10\n0\n10\n20\n30\n40\n50\nEpisode Return\nUniﬁed Model-Free HRL Method, K = 4\nUniﬁed Model-Free HRL Method, K = 6\nUniﬁed Model-Free HRL Method, K = 8\nRegular RL\n(e)\n(f)\nFigure 2. (a) The 4-room task with a key (r = +10) and a lock (r = +40). (b-d) The results of the unsupervised subgoal discovery\nalgorithm with anomalies marked with black Xs and centroids with colored ones. The number of K-means clusters was set to (b) K = 4,\n(c) K = 6, (d) K = 8. (e) The rate of visited number of states to the total states. (f) The average episode return.\n(a)\n(b)\n(c)\n(d)\n0\n500000\n1000000\n1500000\n2000000\n2500000\nTraining steps\n0\n20\n40\n60\n80\n100\nSuccess in reaching subgoals %\nUniﬁed Model-Free HRL Method\nDeepMind DQN Algorithm (Mnih et. al., 2015)\n0\n500000\n1000000\n1500000\n2000000\n2500000\nTraining steps\n0\n50\n100\n150\n200\n250\n300\n350\n400\nAverage return over 1000 episdes\nUniﬁed Model-Free HRL Method\nDeepMind DQN Algorithm (Mnih et. al., 2015)\n(e)\n(f)\nFigure 3. (a) The ﬁrst screen of the Montezuma’s Revenge game. (b) The results of an off-the-shelf Canny edge detection on a single\nimage of the game. (c) The results of the unsupervised subgoal discovery algorithm during intrinsic motivation learning in the ﬁrst room\nof Montezuma’s Revenge. Blue circles are the anomalous subgoals and the red ones are the centroids of clusters. (d) The results of the\nunsupervised subgoal discovery for a random walk. (e) The success of the controller in reaching subgoals. (f) The average game score.\nEfﬁcient Exploration through Intrinsic Motivation Learning for Unsupervised Subgoal Discovery in Model-Free HRL\nRafati, J. and Noelle, D. C. Sparse coding of learned state\nrepresentations in reinforcement learning. In Conference\non Cognitive Computational Neuroscience, New York\nCity, NY, USA, 2017.\nRafati, J. and Noelle, D. C. Learning representations in\nmodel-free hierarchical reinforcement learning. In 33rd\nAAAI Conference on Artiﬁcial Intelligence (AAAI-19),\nHonolulu, HI, USA., 2019a.\nRafati, J. and Noelle, D. C. Unsupervised methods for sub-\ngoal discovery during intrinsic motivation in model-free\nhierarchical reinforcement learning. In 33rd AAAI Con-\nference on Artiﬁcial Intelligence (AAAI-19). Workshop on\nKnowledge Extraction From Games. Honolulu, HI, USA.,\n2019b.\nRafati, J. and Noelle, D. C. Learning representations in\nmodel-free hierarchical reinforcement learning. arXiv\ne-print (arXiv:1810.10096), 2019c.\nRafati, J. and Noelle, D. C. Unsupervised subgoal discovery\nmethod for learning hierarchical representations. In 7th\nInternational Conference on Learning Representations,\nICLR 2019 Workshop on “Structure & Priors in Rein-\nforcement Learning”, New Orleans, LA, USA, 2019d.\nRafati, J. and Noelle, D. C.\nLearning sparse repre-\nsentations in reinforcement learning.\narXiv e-print\n(arXiv:1909.01575), 2019e.\nSingh, S., Lewis, R. L., Barto, A. G., and Sorg, J. Intrinsi-\ncally motivated reinforcement learning: An evolutionary\nperspective. IEEE Transaction on Autonomous Mental\nDevelopment, 2(2):70–82, 2010.\nSutton, R. S. Learning to predict by the methods of temporal\ndifferences. Machine Learning, 3:9–44, 1988.\nSutton, R. S. and Barto, A. G. Reinforcement Learning: An\nIntroduction. MIT Press, Cambridge, MA, 1st edition,\n1998.\nSutton, R. S., Precup, D., and Singh, S. Between MDPs\nand semi-MDPs: A framework for temporal abstraction\nin reinforcement learning. Artiﬁcial Intelligence, 112(1):\n181 – 211, 1999.\nVezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jader-\nberg, M., Silver, D., and Kavukcuoglu, K. Feudal net-\nworks for hierarchical reinforcement learning. In ICML,\n2017.\nVigorito, C. M. and Barto, A. G. Intrinsically motivated hi-\nerarchical skill learning in structured environments. IEEE\nTransactions on Autonomous Mental Development, 2(2):\n132–143, 2010.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2019-11-18",
  "updated": "2019-11-18"
}