{
  "id": "http://arxiv.org/abs/1911.11285v1",
  "title": "Biologically inspired architectures for sample-efficient deep reinforcement learning",
  "authors": [
    "Pierre H. Richemond",
    "Arinbjörn Kolbeinsson",
    "Yike Guo"
  ],
  "abstract": "Deep reinforcement learning requires a heavy price in terms of sample\nefficiency and overparameterization in the neural networks used for function\napproximation. In this work, we use tensor factorization in order to learn more\ncompact representation for reinforcement learning policies. We show empirically\nthat in the low-data regime, it is possible to learn online policies with 2 to\n10 times less total coefficients, with little to no loss of performance. We\nalso leverage progress in second order optimization, and use the theory of\nwavelet scattering to further reduce the number of learned coefficients, by\nforegoing learning the topmost convolutional layer filters altogether. We\nevaluate our results on the Atari suite against recent baseline algorithms that\nrepresent the state-of-the-art in data efficiency, and get comparable results\nwith an order of magnitude gain in weight parsimony.",
  "text": "Biologically inspired architectures for sample-efﬁcient\ndeep reinforcement learning\nPierre H. Richemond\nImperial College London\nphr17@imperial.ac.uk\nArinbjörn Kolbeinsson\nImperial College London\nak711@imperial.ac.uk\nYike Guo\nImperial College London\ny.guo@imperial.ac.uk\nAbstract\nDeep reinforcement learning requires a heavy price in terms of sample efﬁciency\nand overparameterization in the neural networks used for function approximation.\nIn this work, we use tensor factorization in order to learn more compact representa-\ntion for reinforcement learning policies. We show empirically that in the low-data\nregime, it is possible to learn online policies with 2 to 10 times less total coefﬁ-\ncients, with little to no loss of performance. We also leverage progress in second\norder optimization, and use the theory of wavelet scattering to further reduce the\nnumber of learned coefﬁcients, by foregoing learning the topmost convolutional\nlayer ﬁlters altogether. We evaluate our results on the Atari suite against recent\nbaseline algorithms that represent the state-of-the-art in data efﬁciency, and get\ncomparable results with an order of magnitude gain in weight parsimony.\n1\nIntroduction & Related Work\nThe successes of deep reinforcement learning (thereafter ’RL’) come at a heavy computational price.\nIt is well known that achieving human-level performance in domains such as Atari [1, 2, 3] requires\nhundreds of millions of frames of environment interaction. As such, the problem of sample efﬁciency\nin reinforcement learning is of critical importance. Several tracks of concurrent research are being\ninvestigated, and have reduced by orders of magnitude the number of environment interactions\nrequired for good performance beyond the previous benchmark of biologically-inspired episodic\ncontrol methods [4, 5] to a couple hours of human gameplay time [6, 7].\nHowever, while the data-efﬁciency of RL methods has seen recent drastic performance, their function\napproximators still use millions of learned weights, potentially still leaving them heavily overpa-\nrameterized. Independently motivated by biological facts like the behavioural readiness of newborn\nanimals, several authors [8, 9, 10] have recently looked at doing away with learning so many weights\nfor RL tasks. Smaller networks not only train faster, but may yet offer another avenue for gains in the\nform of better generalization [11]. Very recent work from [8] studies the effect of inductive bias of\nneural architectures in reinforcement learning ; they forego training altogether, but transfer networks\nthat only obtain ’better than chance performance on MNIST’. In similar fashion, [10] investigate\nthe effect of random projections in the restricted setting of imitation learning. Finally, [9] manage\nhuman-level performance on the Atari suite using a separate dictionary learning procedure for their\nfeatures, bypassing the usual end-to-end learning paradigm. The perspective of neural architecture\nsearch applied to RL appears difﬁcult, if not computationally inextricable.\nConcurrently, the study of biologically-inspired models of learning has exhibited two mathematical\ncharacterizations that might be critical in explaining how biological learning takes place so efﬁciently.\nFirst, the low-rank properties of learned perceptual manifolds [12, 13] are giving rise to a rich theory\nborrowing from statistical physics. Second, another well known line of work has identiﬁed Gabor\nﬁlters (and more generally wavelet ﬁlter-like structures) in the actual visual cortex of animals [14],\nand linked those to sparsity-promoting methods and dictionary learning [15, 16, 17]. But these\nDeep Reinforcement Learning Workshop, NeurIPS 2019, Vancouver, Canada.\narXiv:1911.11285v1  [cs.LG]  25 Nov 2019\nbreakthroughs have not, so far, been reﬂected as inductive priors in the shape of modiﬁcations in deep\nRL neural networks architectures, which remain fairly ﬁxed on the Atari domain.\nTherefore the following questions remain: how parsimonious do function approximators in rein-\nforcement learning need to be, in order to maintain good performance? And can we be at once\nsample-efﬁcient and weight-efﬁcient ? In this work, we turn to the mathematical theories of tensor\nfactorization [18], second-order optimization [19, 20] and wavelet scattering [21] to answer this\nquestion positively and empirically, in a model-free setting. To the best of our knowledge, this is the\nﬁrst time those ﬁelds have been combined together in this context, and that tensor factorization is\napplied to deep RL.\n2\nBackground\n2.1\nDeep Reinforcement Learning\nWe consider the standard Markov Decision Process framework as in [1]). This setting is characterised\nby a tuple ⟨S, A, T, R, γ⟩, where S is a set of states, A a set of actions, R a reward function that\nis the immediate, intrinsic desirability of a certain state, T a transition dynamics and γ ∈[0, 1] a\ndiscount factor. The purpose of the RL problem is to to ﬁnd a policy π, which represents a mapping\nfrom states to a probability distribution over actions, that is optimal, i.e., that maximizes the expected\ncumulative discounted return P∞\nk=0 γkRt+k+1 at each state st ∈S. In Q-learning, the policy is given\nimplicitly by acting greedily or ϵ-greedily with respect to learned action-value functions qπ(s, a),\nthat are learned following the Bellman equation. In deep Q-learning, qθ becomes parameterized by\nthe weights θ of a neural network and one minimizes the expected Bellman loss :\nE\n\u0010\nRt+1 + γt+1 max\na′ qθ (St+1, a′) −qθ (St, At)\n\u00112\nIn practice, this is implemented stochastically via uniform sampling of transitions in an experience\nreplay buffer, as is done in the seminal paper [2]. Several algorithmic reﬁnements to that approach\nexist. First, Double Q-learning [22] proposes to decouple learning between two networks in order to\nalleviate the Q-value overestimation problem. Second, dueling Q-networks [23] explicitly decompose\nthe learning of an action-value function qθ(s, a) as the sum of an action-independent state-value,\nmuch like what is traditionally done in policy gradient methods [1], implemented via a two-headed\nneural network architecture. Finally, prioritized RL [24] proposes to replace the uniform sampling of\ntransitions in the experience replay buffer with importance sampling, by prioritizing those transitions\nthat present the most Bellman error (those transitions that are deemed the most ’surprising’ by the\nagent). [25] uses extra weights to learn the variance of the exploration noise in a granular fashion,\nwhile [26] proposes to learn a full distribution of action-values for each action and state. Combined,\nthose methods form the basis of the Rainbow algorithm in [3].\n2.2\nTensor factorization\nHere we introduce notations and concepts from the tensor factorization literature. An intuition is\nthat the two main decompositions below, CP and Tucker decompositions, can be understood as\nmultilinear algebra analogues of SVD or eigendecomposition.\nCP decomposition. A tensor X ∈RI1×I2×···×IN , can be decomposed into a sum of R rank-1 tensors,\nknown as the Canonical-Polyadic decomposition, where R is as the rank of the decomposition. The\nobjective is to ﬁnd the vectors u(1)\nk , u(2)\nk , · · · , u(N)\nk\n, for k = [1 . . . R], as well as a vector of weights\nλ ∈RR such that:\nX =\nR\nX\nk=1\nλku(1)\nk\n◦u(2)\nk\n◦· · · ◦u(N)\nk\n|\n{z\n}\nrank-1 components\nTucker decomposition. A tensor X ∈RI1×I2×···×IN , can be decomposed into a low rank approxi-\nmation consisting of a core G ∈RR1×R2×···×RN and a set of projection factors \u0000U(0), · · · , U(N−1)\u0001\n,\nwith U(k) ∈RRk,ˆIk, k ∈(0, · · · , N −1) that, when projected along the corresponding dimension of\n2\nthe core, reconstruct the full tensor X. The tensor in its decomposed form can then be written:\nX = G ×1 U(1) ×2 U(2) × · · · ×N U(N) =\nî\nG; U(1), · · · , U(N)ó\nTensor regression layer.\nFor two tensors X\n∈\nRK1×···×Kx×I1×···×IN and Y\n∈\nRI1×···×IN×L1×···×Ly, we denote by ⟨X, Y⟩N\n∈\nRK1×···×Kx×L1×···×Ly the contraction of\nX by Y along their N last modes; their generalized inner product is\n⟨X, Y⟩N =\nI1\nX\ni1=1\nI2\nX\ni2=1\n· · ·\nIN\nX\nin=1\nX...,i1,i2,...,inYi1,i2,...,in,...\nThis enables us to deﬁne a tensor regression layer [27] that is differentiable and learnable end-to-end\nby gradient descent. Let us denote by X ∈RI1×I2×···×IN the input activation tensor for a sample\nand y ∈RIN the label vector. A tensor regression layer estimates the regression weight tensor\nW ∈RI1×I2×···×IN under a low-rank decomposition. In the case of a Tucker decomposition (as per\nour experiments) with ranks (R1, · · · , RN), we have :\ny = ⟨X, W⟩N + b\nwith W = G ×1 U(1) ×2 U(2) · · · ×N U(N)\nas G ∈RR1×···×RN , U(k) ∈RIk×Rk for each k in [1 . . . N] and U(N) ∈R1×RN .\n[28, 27, 29] learn parsimonious deep learning fully-connected layers thanks to low-rank constraints.\n2.3\nWavelet scattering\nThe wavelet scattering transform was originally introduced by [21] and [30] as a non-linear extension\nto the classical wavelet ﬁlter bank decomposition [31]. Its principle is as follows. Denoting by\nx ⊛y[n] the 2-dimensional, circular convolution of two signals x[n] and y[n], let us assume that\nwe have pre-deﬁned two wavelet ﬁlter banks available\n¶\nψ(1)\nλ1 [n]\n©\nλ1∈Λ1\n¶\nψ(2)\nλ2 [n]\n©\nλ2∈Λ2 , with λ1\nand λ2 two frequency indices. These wavelet ﬁlters correspond to high frequencies, so we also give\nourselves the data of a lowpass ﬁlter φJ[n]. Finally, and by opposition to traditional linear wavelet\ntransforms, we also assume a given nonlinearity ρ(t). Then the scattering transform is given by\ncoefﬁcients of order 0,1, and 2, respectively :\nS0x[n] = x ⊛φJ[n]\nS1x [n, λ1] = ρ\nÄ\nx ⊛ψ(1)\nλ1\nä\n⊛φJ[n]\nλ1 ∈Λ1\nS2x [n, λ1, λ2] = ρ\nÄ\nρ\nÄ\nx ⊛ψ(1)\nλ1\nä\n⊛ψ(2)\nλ2\nä\n⊛φJ[n]\nλ1 ∈Λ1, λ2 ∈Λ2 (λ1)\nThis can effectively be understood and implemented as a two-layer convolutional neural network\nwhose weights are not learned but rather frozen and given by the coefﬁcients of wavelets ψ and φ\n(with Gabor ﬁlters as a special case [31]). The difference with traditional ﬁlter banks comes from\nthe iterated modulus/nonlinear activation function applied at each stage, much like in traditional\ndeep learning convolutional neural networks. In practice, the potential of scattering transforms to\naccelerate deep learning by providing ready-made convolutional layers weights has been investigated\nin [32, 33, 34].\n2.4\nSecond order optimization with K-FAC\nWhile stochastic gradient descent is usually performed purely from gradient observations derived from\nauto-differentiation, faster, second order optimization methods ﬁrst multiply the weights’ θ gradient\nvector ∇θ by a preconditioning matrix, yielding the weight update θ ←θ −ηG−1∇θ. In the case of\nsecond order methods, the matrix G−1 is chosen to act as a tractable iterative approximation to the\ninverse Hessian or Empirical Fisher Information Matrix [19] of the neural network model in question.\nKronecker-factored approximate curvature or K-FAC [20] enforces a Kronecker decomposition of\nthe type G = A ⊗B, with A and B being smaller, architecture-dependent matrices. Unlike the\nabove methods, K-FAC has been applied as a plug-in in the RL literature and been shown to promote\nconvergence [35].\n3\n3\nMethods & Experimental Results\nWe do take as a baseline method the data-efﬁcient Rainbow of [6]. However, we change the\narchitecture of the neural network function approximators used, in accordance with the principles\ndescribed above, combining them to reﬂect inductive biases promoting fewer learnable parameters:\n• We replace the fully-connected, linear layers used in the Rainbow [3] and data-efﬁcient\nRainbow [6] by tensor regression layers [27] in order to learn low-rank policies (ranks in\nappendix).\n• We use either the K-FAC [20] second order stochastic optimizer, or ADAM [36].\n• We combine the two methods with various rank and therefore weight compression ratios\nand evaluate those on the same subset of Atari games as [6, 7].\n• When possible, we replace the ﬁrst convolutional layer in the approximating neural network\nwith a scattering layer for further gains in terms of learnable weights.\nFor all our Atari experiments, we used OpenAI Gym [37], and a combination of PyTorch [38],\nTensorLy [39] and Kymatio [40] for auto-differentiation. We evaluated our agents in the low-data\nregime of 100,000 steps, on half the games, with 3 different random seeds for reproducibility [41].\nOur speciﬁc hyperparameters are described in appendix. We report our results in tables 1 and 2.\nGame\nSimPLe\nRainbow\nDenoised\nTRL 2.5x\nTRL 5x\nTRL 10x\nalien\n405\n740\n684\n688\n454\n566\namidar\n88\n189\n154\n118\n86\n84\nassault\n369\n431\n321\n543\n521\n513\nasterix\n1090\n471\n500\n459\n554\n363\nbank_heist\n8\n51\n77\n59\n134\n42\nbattle_zone\n5184\n10125\n9378\n14466\n13466\n5744\nboxing\n9\n0.2\n1\n-2\n-2\n-5\nbreakout\n13\n2\n3\n2\n2\n4\nchopper_command\n1247\n862\n1293\n1255\n1243\n1106\ncrazy_climber\n39828\n16185\n9977\n3928\n4225\n2340\ndemon_attack\n170\n508\n450\n362\n263\n175\nfreeway\n20\n28\n28\n26\n25\n24\nfrostbite\n255\n867\n1101\n659\n912\n231\ngopher\n771\n349\n391\n278\n255\n396\nhero\n1295\n6857\n3013\n5351\n3732\n3321\njamesbond\n125\n302\n295\n215\n213\n218\nkangaroo\n323\n779\n1002\n804\n715\n400\nkrull\n4540\n2852\n2656\n2333\n2275\n2308\nkung_fu_master\n17257\n14346\n4037\n9392\n4764\n4031\nms_pacman\n763\n1204\n1053\n818\n838\n517\npong\n5\n-19\n-20\n-20\n-19\n-21\nprivate_eye\n58\n98\n100\n51\n100\n1128\nqbert\n560\n1153\n672\n697\n581\n733\nroad_runner\n5169.4\n9600\n5426\n6965\n3914\n1319\nseaquest\n371\n354\n387\n345\n350\n287\nup_n_down\n2153\n2877\n5123\n2197\n2302\n2179\nAverage (vs. Rainbow)\n100%\n118%\n96%\n90%\n71%\nTable 1: Mean episode returns as reported in SimPLe [7] and data-efﬁcient Rainbow [6], versus\nour agents, on 26 Atari games. ’Denoised’ is the NoisyNet ablation of Rainbow; ’TRL’ shows the\nperformance of the data-efﬁcient Rainbow with tensor regression layers substituted for linear ones.\nTable 1 shows proof of concept of the online learning of low-rank policies, with a loss of ﬁnal\nperformance varying in proportion to the compression in the low-rank linear layers, very much like in\nthe deep learning literature [28, 27]. The number of coefﬁcients in the original data-efﬁcient Rainbow\n4\nis of the order of magnitude of 1M and varies depending on the environment and its action-space size.\nThe corresponding tensor regression layer ranks are in appendix, and chosen to target 400k, 200k and\n100k coefﬁcients respectively. While individual game results tend to decrease monotonously with\nincreasing compression, we observe that they are noisy as per the nature of exploration in RL, and\naverage scores reported correspond with the intuition that performance seems to decrease fast after a\ncertain overparameterization threshold is crossed. To take this noisy character into account, we take\ncare to be conservative and report the average of the ﬁnal three episodes of the learned policy after\n80000, 90000 and 100000 steps, respectively. Also, so as to not muddy the discussion and provide\nfair baselines, we do report on the NoisyNet [25] ablation of Rainbow (’Denoised’ columns), as the\nNoisyLinear layer doubles up the number of coefﬁcients required and actually performs worse in our\nexperiments. Interestingly, the approximation error in tensor factorization seems to play a role akin\nto promoting exploration noise.\nWe then proceed to assess the impact of second-order optimization to our architecture by substituting\nADAM optimization for K-FAC, and introducing scattering, in table 2 (only a handful results being\navailable with scattering, due to computational limitations). In spite of our conservative reporting, the\nefﬁciency boost from using a second order scheme more than makes up for low-rank approximation\nerror with ﬁve times less coefﬁcients than [6], suggesting that learning with a full order of magnitude\nless coefﬁcients is well within reach of our techniques.\nGame\nKFAC+Denoised\nKFAC+TRL5x\nKFAC+TRL10x\nScattering\nalien\n996\n734\n643\n441\namidar\n163\n101\n98\n84\nassault\n501\n491\n496\n434\nasterix\n537\n549\n526\n502\nbank_heist\n100\n73\n57\n29\nbattle_zone\n8622\n15178\n6156\n4311\nboxing\n0\n-4\n-1\n-9\nbreakout\n3\n3\n2\n2\nchopper_command\n692\n611\n1302\n441\ncrazy_climber\n14242\n12377\n3546\n740\ndemon_attack\n582\n434\n318\n692\nfreeway\n26\n26\n24\n19\nfrostbite\n1760\n718\n1483\n654\ngopher\n363\n341\n265\n172\nhero\n4188\n6284\n4206\n4127\njamesbond\n263\n327\n217\n48\nkangaroo\n2085\n613\n588\n391\nkrull\n2855\n3441\n3392\n772\nkung_fu_master\n8481\n10738\n7357\n233\nms_pacman\n1137\n920\n867\n613\npong\n-19.3\n-19\n-19\n-20\nprivate_eye\n56\n100\n100\n0\nqbert\n731\n520\n538\n475\nroad_runner\n4516\n8493\n7224\n1278\nseaquest\n349\n317\n520\n213\nup_n_down\n2557\n2291\n2108\n993\nAverage (vs. Rainbow)\n114%\n109%\n98%\n56%\nTable 2: Mean episode returns of our low-rank agents with second-order optimization and scattering.\n4\nConclusion\nWe have demonstrated that in the low-data regime, it is possible to leverage biologically plausible\ncharacterizations of experience data (namely low-rank properties and wavelet scattering separability)\nto exhibit architectures that learn policies with many times less weights than current baselines, in an\nonline fashion. We do hope that this will lead to even further progress towards sample efﬁciency and\nspeedy exploration methods. Further work will ﬁrst focus on thorough evaluation and research of\nscattering architectures in order to achieve further gains, and second investigate additional, orthogonal\nbiologically-friendly research directions such as promoting sparsity.\n5\nReferences\n[1] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. The MIT Press, second ed., 2018.\n[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, “Playing\nAtari with Deep Reinforcement Learning,” arXiv e-prints, Dec. 2013.\n[3] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar,\nand D. Silver, “Rainbow: Combining Improvements in Deep Reinforcement Learning,” arXiv e-prints, Oct.\n2017.\n[4] C. Blundell, B. Uria, A. Pritzel, Y. Li, A. Ruderman, J. Z. Leibo, J. Rae, D. Wierstra, and D. Hassabis,\n“Model-Free Episodic Control,” arXiv e-prints, June 2016.\n[5] A. Pritzel, B. Uria, S. Srinivasan, A. Puigdomènech, O. Vinyals, D. Hassabis, D. Wierstra, and C. Blundell,\n“Neural Episodic Control,” arXiv e-prints, Mar. 2017.\n[6] H. van Hasselt, M. Hessel, and J. Aslanides, “When to use parametric models in reinforcement learning?,”\narXiv e-prints, June 2019.\n[7] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. H. Campbell, K. Czechowski, D. Erhan, C. Finn,\nP. Kozakowski, S. Levine, A. Mohiuddin, R. Sepassi, G. Tucker, and H. Michalewski, “Model-Based\nReinforcement Learning for Atari,” arXiv e-prints, Mar. 2019.\n[8] A. Gaier and D. Ha, “Weight Agnostic Neural Networks,” arXiv e-prints, June 2019.\n[9] G. Cuccu, J. Togelius, and P. Cudre-Mauroux, “Playing Atari with Six Neurons,” arXiv e-prints, June 2018.\n[10] R. Wang, C. Ciliberto, P. Amadori, and Y. Demiris, “Random Expert Distillation: Imitation Learning via\nExpert Policy Support Estimation,” arXiv e-prints, May 2019.\n[11] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understanding deep learning requires rethinking\ngeneralization,” arXiv e-prints, Nov. 2016.\n[12] S. Chung, D. D. Lee, and H. Sompolinsky, “Classiﬁcation and Geometry of General Perceptual Manifolds,”\nPhysical Review X, vol. 8, p. 031003, July 2018.\n[13] S. Chung, D. D. Lee, and H. Sompolinsky, “Linear readout of object manifolds.,” Physical review. E,\nvol. 93 6, p. 060301, 2016.\n[14] J. P. Jones and L. A. Palmer, “An evaluation of the two-dimensional gabor ﬁlter model of simple receptive\nﬁelds in cat striate cortex.,” Journal of neurophysiology, vol. 58 6, pp. 1233–58, 1987.\n[15] B. A. Olshausen and D. J. Field, “Emergence of simple-cell receptive ﬁeld properties by learning a sparse\ncode for natural images,” Nature, vol. 381, pp. 607–609, 1996.\n[16] B. A. Olshausen and D. J. Field, “Sparse coding with an overcomplete basis set: A strategy employed by\nv1?,” Vision Research, vol. 37, pp. 3311–3325, 1997.\n[17] A. Hyvärinen and P. O. Hoyer, “A two-layer sparse coding model learns simple and complex cell receptive\nﬁelds and topography from natural images,” Vision Research, vol. 41, pp. 2413–2423, 2001.\n[18] A. Cichocki, R. Zdunek, A. H. Phan, and S. Amari, Nonnegative Matrix and Tensor Factorizations -\nApplications to Exploratory Multi-way Data Analysis and Blind Source Separation. Wiley, 2009.\n[19] S. ichi Amari, “Natural gradient works efﬁciently in learning,” Neural Computation, vol. 10, pp. 251–276,\n1998.\n[20] J. Martens and R. B. Grosse, “Optimizing neural networks with kronecker-factored approximate curvature,”\nArXiv, vol. abs/1503.05671, 2015.\n[21] S. Mallat, “Group Invariant Scattering,” arXiv e-prints, Jan. 2011.\n[22] H. van Hasselt, A. Guez, and D. Silver, “Deep Reinforcement Learning with Double Q-learning,” arXiv\ne-prints, Sept. 2015.\n[23] Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, and N. de Freitas, “Dueling Network\nArchitectures for Deep Reinforcement Learning,” arXiv e-prints, Nov. 2015.\n[24] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized Experience Replay,” arXiv e-prints, Nov.\n2015.\n[25] M. Fortunato, M. Gheshlaghi Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos,\nD. Hassabis, O. Pietquin, C. Blundell, and S. Legg, “Noisy Networks for Exploration,” arXiv e-prints, June\n2017.\n[26] M. G. Bellemare, W. Dabney, and R. Munos, “A Distributional Perspective on Reinforcement Learning,”\narXiv e-prints, July 2017.\n[27] J. Kossaiﬁ, Z. C. Lipton, A. Khanna, T. Furlanello, and A. Anandkumar, “Tensor Regression Networks,”\narXiv e-prints, July 2017.\n6\n[28] J. Kossaiﬁ, A. Khanna, Z. C. Lipton, T. Furlanello, and A. Anandkumar, “Tensor Contraction Layers for\nParsimonious Deep Nets,” arXiv e-prints, June 2017.\n[29] X. Cao and G. Rabusseau, “Tensor Regression Networks with various Low-Rank Tensor Approximations,”\narXiv e-prints, Dec. 2017.\n[30] J. Bruna and S. Mallat, “Invariant Scattering Convolution Networks,” arXiv e-prints, Mar. 2012.\n[31] S. Mallat, A Wavelet Tour of Signal Processing. Academic Press, 1998.\n[32] E. Oyallon, S. Mallat, and L. Sifre, “Generic Deep Networks with Wavelet Scattering,” arXiv e-prints, Dec.\n2013.\n[33] E. Oyallon, S. Zagoruyko, G. Huang, N. Komodakis, S. Lacoste-Julien, M. Blaschko, and E. Belilovsky,\n“Scattering Networks for Hybrid Representation Learning,” arXiv e-prints, Sept. 2018.\n[34] T. Angles and S. Mallat, “Generative networks as inverse problems with Scattering transforms,” arXiv\ne-prints, May 2018.\n[35] Y. Wu, E. Mansimov, S. Liao, R. Grosse, and J. Ba, “Scalable trust-region method for deep reinforcement\nlearning using Kronecker-factored approximation,” arXiv e-prints, Aug. 2017.\n[36] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,” arXiv e-prints, Dec. 2014.\n[37] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, “OpenAI\nGym,” arXiv e-prints, June 2016.\n[38] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and\nA. Lerer, “Automatic differentiation in pytorch,” in OpenReview, 2017.\n[39] J. Kossaiﬁ, Y. Panagakis, A. Anandkumar, and M. Pantic, “TensorLy: Tensor Learning in Python,” arXiv\ne-prints, Oct. 2016.\n[40] M. Andreux, T. Angles, G. Exarchakis, R. Leonarduzzi, G. Rochette, L. Thiry, J. Zarka, S. Mallat, J. andén,\nE. Belilovsky, J. Bruna, V. Lostanlen, M. J. Hirn, E. Oyallon, S. Zhang, C. Cella, and M. Eickenberg,\n“Kymatio: Scattering Transforms in Python,” arXiv e-prints, Dec. 2018.\n[41] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger, “Deep Reinforcement Learning\nthat Matters,” arXiv e-prints, Sept. 2017.\n[42] M. Eickenberg, G. Exarchakis, M. J. Hirn, S. Mallat, and L. Thiry, “Solid harmonic wavelet scattering for\npredictions of molecule properties,” The Journal of chemical physics, vol. 148 24, p. 241732, 2018.\n7\nAppendix\nHyperparameters and Reproducibility\nOur codebase is available on request. Hyperparameters are as follows. First, our speciﬁc architecture-modiﬁed\nhyperparameters:\nSpeciﬁc architecture hyperparameters\nValue\nScattering maximum log-scale J\n3\nScattering volume width M\n1\nScattering tensor input shape\n(1,4,84,84)\nScattering tensor output shape\n(1,16,11,11)\nScattering type\nHarmonic 3D, see [40, 42]\nHidden linear layer rank constraint, 2.5x compression\n128\nFinal linear layer rank constraint, 2.5x compression\n48\nHidden linear layer rank constraint, 5x compression\n32\nFinal linear layer rank constraint, 5x compression\n48\nHidden linear layer rank constraint, 10x compression\n16\nFinal linear layer rank constraint, 10x compression\n10\nKFAC Tikhonov regularization parameter\n0.1\nKFAC Update frequency for inverses\n100\nTable 3: Our additional, architecture-speciﬁc hyperparameters.\nFurthermore, we mirror the Data-Efﬁcient Rainbow [6] baseline:\nData-efﬁcient Rainbow hyperparameters\nValue\nGrey-scaling\nTrue\nObservation down-sampling\n(84, 84)\nFrames stacked\n4\nAction repetitions\n4\nReward clipping\n[-1, 1]\nTerminal on loss of life\nTrue\nMax frames per episode\n108K\nUpdate\nDistributional Double Q\nTarget network update period∗\nevery 2000 updates\nSupport of Q-distribution\n51 bins\nDiscount factor\n0.99\nMinibatch size\n32\nOptimizer\nAdam\nOptimizer: ﬁrst moment decay\n0.9\nOptimizer: second moment decay\n0.999\nOptimizer: ϵ\n0.00015\nMax gradient norm\n10\nPriority exponent\n0.5\nPriority correction∗∗\n0.4 →1\nHardware\nNVidia 1080Ti GPU\nNoisy nets parameter\n0.1\nTraining frames\n400,000\nMin replay size for sampling\n1600\nMemory size\nunbounded\nReplay period every\n1 steps\nMulti-step return length\n20\nQ network: channels\n32, 64\nQ network: ﬁlter size\n5 x 5, 5 x 5\nQ network: stride\n5, 5\nQ network: hidden units\n256\nOptimizer: learning rate\n0.0001\nTable 4: Data-efﬁcient Rainbow agent hyperparameters, as per [6].\n8\nStandard deviations for score runs\nGame\nDenoised\nTRL 2.5x\nTRL 10x\nalien\n684 ± 7\n688 ± 123\n566± 38\namidar\n154 ± 21\n118 ± 12\n84± 15\nassault\n321 ± 224\n543 ± 94\n513± 64\nasterix\n500 ± 124\n459 ± 91\n363 ± 66\nbank_heist\n77 ± 23\n59 ± 22\n42 ± 2\nbattle_zone\n9378 ± 2042\n14466 ± 2845\n5744 ± 575\nboxing\n1 ± 2\n-2 ± 1\n-5 ± 1\nbreakout\n3 ± 1.5\n2 ± 1\n4 ± 0.3\nchopper_command\n1293 ± 445\n1255 ± 215\n1106 ± 124\ncrazy_climber\n9977 ± 3744\n3928 ± 221\n2340 ± 595\ndemon_attack\n450 ± 49\n362 ± 147\n175 ± 7\nfreeway\n28 ± 0.6\n26 ± 0\n24 ± 0.5\nfrostbite\n1101 ± 355\n659 ± 523\n231 ± 1\ngopher\n391 ± 46\n278 ± 39\n396 ± 24\nhero\n3013 ± 90\n5351 ± 1948\n3321 ± 598\njamesbond\n295 ± 57\n215 ± 42\n218 ± 22\nkangaroo\n1002 ± 587\n804 ± 289\n400 ± 278\nkrull\n2656 ± 180\n2333 ± 309\n2308 ± 268\nkung_fu_master\n4037 ± 2962\n9392 ± 6289\n4031 ± 3068\nms_pacman\n1053 ± 193\n818 ± 94\n517 ± 38\npong\n-20 ± 0.4\n-20 ± 0\n-21 ± 0.1\nprivate_eye\n100 ± 0\n51 ± 59\n1128 ± 1067\nqbert\n672 ± 144\n697 ± 78\n733 ± 291\nroad_runner\n5426 ± 2830\n6965 ± 6569\n1319± 216\nseaquest\n387 ± 24\n345 ± 40\n287 ± 87\nup_n_down\n5123 ± 3146\n2197 ± 231\n2179 ± 178\nTable 5: Standard deviations across seeds for runs presented Table 1.\n9\nGame\nKFAC+Denoised\nKFAC+TRL10x\nScattering\nalien\n996 ± 180\n643 ± 51\n441 ± 90\namidar\n163 ± 15\n98 ± 26\n84 ± 11\nassault\n501 ± 85\n496 ± 129\n434 ± 304\nasterix\n537 ± 96\n526 ± 64\n502 ± 91\nbank_heist\n100 ± 14\n57 ± 36\n29 ± 13\nbattle_zone\n8622 ± 5358\n6156 ± 1951\n4311 ± 1517\nboxing\n0 ± 2\n-1 ± 3\n-9 ± 12\nbreakout\n3 ± 1\n2 ± 2\n2 ± 0\nchopper_command\n692 ± 81\n1302 ± 328\n441 ± 80\ncrazy_climber\n14242 ± 2936\n3546 ± 1231\n740 ± 291\ndemon_attack\n582 ± 130\n318 ± 168\n692 ± 232\nfreeway\n26 ± 0\n24 ± 0\n19 ± 1\nfrostbite\n1760 ± 448\n1483 ± 466\n654 ± 709\ngopher\n363 ± 4\n265 ± 67\n172 ± 3\nhero\n4188 ± 1635\n4206 ± 1862\n4127 ± 1074\njamesbond\n263 ± 22\n217 ± 68\n48 ± 10\nkangaroo\n2085 ± 2055\n588 ± 5\n391 ± 52\nkrull\n2855 ± 156\n3392 ± 2205\n772± 560\nkung_fu_master\n8481 ± 8270\n7357 ± 9200\n233± 205\nms_pacman\n1137 ± 180\n867 ± 128\n613 ± 159\npong\n-19 ± 0.6\n-19 ± 1\n-20 ± 0\nprivate_eye\n56 ± 42\n100 ± 0\n0 ± 0\nqbert\n731 ± 256\n538 ± 114\n475 ± 161\nroad_runner\n4516 ± 2869\n7224 ± 4598\n1278 ±463\nseaquest\n349 ± 63\n520 ± 97\n213 ± 96\nup_n_down\n2557 ± 641\n2108 ± 298\n993 ± 244\nTable 6: Standard deviations across seeds for runs presented Table 2.\nFurther results and learning curves\nLearning curves As a simpler version of the experiments in the main text body, we show basic proof of\nconcept on the simple Pong Atari game. Our experimental setup consists in using our own implementation of\nprioritized double DQN as a baseline, which combines algorithmic advances from [24] and [22]. We replaced\nthe densely connected layer of the original DQN architecture with a tensor regression layer implementing Tucker\ndecomposition for different Tucker ranks, yielding different network compression factors. (These curves average\nthree different random seeds).\nQualitative behaviour. First results, both in terms of learning performance and compression factor, can be\nseen in ﬁgure 1. The two main ﬁndings of this experiment are that ﬁrst, and overall, the ﬁnal performance of\nthe agent remains unaffected by the tensor factorization, even with high compression rates nearing 10 times.\nSecond, this obviously comes at the expense of stability during training - in tough compression regimes, learning\ncurves are slightly delayed, and their plateauing phases contain occasional noisy drawdowns illustrating the\nincreased difﬁculty of learning, as seen in ﬁgure 2. The extra pathwise noise, however, can be seen as promoting\nexploration.\n10\n0\n200\n400\n600\n800\n1000\nNumber of Pong episodes, in thousands\n20\n15\n10\n5\n0\n5\n10\n15\n20\nScore\nMedium Tucker decomposition rank:\n12 (1.9x compression)\n10 (2.3x compression)\n8 (2.8x compression)\n6 (3.6x compression)\n4 (5.0x compression)\nFigure 1: Prioritized tensorized DQN on Atari Pong. Original learning curve versus several learning\ncurves for ﬁve different Tucker ranks factorizations and therefore parameter compression rates (3\ndifferent random seeds each, with a 30 episodes moving average for legibility). Best viewed in colour.\nFigure 2: Focus on a typical single run of the tensorized DQN learning. The overall shape of the\ntypical learning curve is preserved, but drawdowns in the plateauing phase do appear.\n11\n",
  "categories": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ],
  "published": "2019-11-25",
  "updated": "2019-11-25"
}