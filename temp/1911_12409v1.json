{
  "id": "http://arxiv.org/abs/1911.12409v1",
  "title": "PREDICT & CLUSTER: Unsupervised Skeleton Based Action Recognition",
  "authors": [
    "Kun Su",
    "Xiulong Liu",
    "Eli Shlizerman"
  ],
  "abstract": "We propose a novel system for unsupervised skeleton-based action recognition.\nGiven inputs of body keypoints sequences obtained during various movements, our\nsystem associates the sequences with actions. Our system is based on an\nencoder-decoder recurrent neural network, where the encoder learns a separable\nfeature representation within its hidden states formed by training the model to\nperform prediction task. We show that according to such unsupervised training\nthe decoder and the encoder self-organize their hidden states into a feature\nspace which clusters similar movements into the same cluster and distinct\nmovements into distant clusters. Current state-of-the-art methods for action\nrecognition are strongly supervised, i.e., rely on providing labels for\ntraining. Unsupervised methods have been proposed, however, they require camera\nand depth inputs (RGB+D) at each time step. In contrast, our system is fully\nunsupervised, does not require labels of actions at any stage, and can operate\nwith body keypoints input only. Furthermore, the method can perform on various\ndimensions of body keypoints (2D or 3D) and include additional cues describing\nmovements. We evaluate our system on three extensive action recognition\nbenchmarks with different number of actions and examples. Our results\noutperform prior unsupervised skeleton-based methods, unsupervised RGB+D based\nmethods on cross-view tests and while being unsupervised have similar\nperformance to supervised skeleton-based action recognition.",
  "text": "PREDICT & CLUSTER: Unsupervised Skeleton Based Action Recognition\nKun Su1, Xiulong Liu1, and Eli Shlizerman1, 2\n1Department of Electrical & Computer Engineering, University of Washington, Seattle, USA\n2Department of Applied Mathematics, University of Washington, Seattle, USA\nPredict & Cluster\n…\n…\n0\nT\n0\nT\nA\nB\nC\nB\nD\nD\nA\nActions\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\nActions & Classes\nFigure 1: PREDICT & CLUSTER: Unsupervised action recognition from body keypoints. Please see examples in the\nsupplementary video.\nAbstract\nWe propose a novel system for unsupervised skeleton-\nbased action recognition. Given inputs of body keypoints\nsequences obtained during various movements, our system\nassociates the sequences with actions. Our system is based\non an encoder-decoder recurrent neural network, where the\nencoder learns a separable feature representation within\nits hidden states formed by training the model to perform\nprediction task. We show that according to such unsuper-\nvised training the decoder and the encoder self-organize\ntheir hidden states into a feature space which clusters simi-\nlar movements into the same cluster and distinct movements\ninto distant clusters. Current state-of-the-art methods for\naction recognition are strongly supervised, i.e., rely on pro-\nviding labels for training. Unsupervised methods have been\nproposed, however, they require camera and depth inputs\n(RGB+D) at each time step.\nIn contrast, our system is\nfully unsupervised, does not require labels of actions at any\nstage, and can operate with body keypoints input only. Fur-\nthermore, the method can perform on various dimensions of\nbody keypoints (2D or 3D) and include additional cues de-\nscribing movements. We evaluate our system on three exten-\nsive action recognition benchmarks with different number\nof actions and examples. Our results outperform prior un-\nsupervised skeleton-based methods, unsupervised RGB+D\nbased methods on cross-view tests and while being unsu-\npervised have similar performance to supervised skeleton-\nbased action recognition.\n1. Introduction\nRobust action recognition, especially human action\nrecognition, is a fundamental capability in ubiquitous com-\nputer vision and artiﬁcial intelligence systems. While recent\nmethods have shown remarkable success rates in recogniz-\ning basic actions in videos, current methods rely on strong\nsupervision with a large number of training examples ac-\ncompanied with action labels. Collection and annotation\nof large scale datasets is implausible for various types of\nactions and applications. Furthermore, annotation is a chal-\nlenging problem by itself, since it is often up to the inter-\npretation of the annotator to assign a meaningful label for\na given sequence. This is particularly the case in situations\nwhere it is unclear what is the ground truth label, e.g., an-\nnotation of animal movements. Indeed, annotations chal-\nlenges are common in different contextual information on\nmovement, such as video (RGB), depth (+D) and keypoints\n1\narXiv:1911.12409v1  [cs.CV]  27 Nov 2019\ntracked over time. Compared to RGB+D data, keypoints in-\nclude much less information and can be challenging to work\nwith. However, on the other hand, focusing keypoints can\noften isolate the actions from other information and provide\nmore robust unique features for actions.\nFor human action recognition, time-series of body joints\n(skeleton) tracked over time are indeed known as effective\ndescriptors of actions. Here we focus on 3D skeleton time\nsequences and propose an unsupervised system to learn fea-\ntures and assign actions to classes according to them. We\ncall our system PREDICT & CLUSTER (P&C) since it is\nbased on training an encoder-decoder type network to both\npredict and cluster skeleton sequences such that the network\nlearns an effective hidden feature representation of actions.\nIndeed, an intuitive replacement of a classiﬁcation super-\nvised task by a non-classiﬁcation unsupervised task is to\nattempt to continue (predict) or reproduce (re-generate) the\ngiven sequence such that it leads the hidden states to capture\nkey features of the actions. In the encoder-decoder architec-\nture, the prediction task is typically implemented as follows:\ngiven an action sequence as the encoder input, the decoder\ncontinues or generates the encoder input sequence. Since\ninputs are sequences, both the decoder and the encoder are\nrecurrent neural networks (RNN) containing cells with hid-\nden variables for each time sample in a sequence. The ﬁ-\nnal hidden state of the encoder is typically being utilized\nto represent the action feature. While the encoder contains\nthe ﬁnal action feature, since the gradient during training\nﬂows back from the decoder to the encoder, it turns out that\nthe decoder training strategies signiﬁcantly determine the\neffectiveness of the representation. Speciﬁcally, there are\ntwo types of decoder training strategies proposed for such\nprediction/re-generation task [22]. The ﬁrst strategy is a\nconditional strategy, where the output of the previous time-\nstep of the decoder is used as input to the current time-step.\nWith such strategy the output of the decoder is expected to\nbe continuous. In contrast, the unconditional strategy as-\nsigns a zero input into each time-step of the decoder. Previ-\nous work showed that unconditional training of the decoder\nis expected to have better prediction performance since it ef-\nfectively weakens the decoder and thus forces the encoder\nto learn a more informative representation.\nIn our system, we extend such strategies to enhance the\nencoder representation. This results in enhanced clustering\nand organization of actions in the feature space. In par-\nticular, we propose two decoder training strategies, Fixed\nWeights and Fixed States to further penalize the decoder.\nThe implementation of these strategies guides the encoder\nto further learn the feature representation of the sequences\nthat it processes. In fact, in both strategies, the decoder\nis a ‘weak decoder’, i.e., the decoder is effectively not\nbeing optimized and it serves the role of propagating the\ngradient to the encoder to further optimize its ﬁnal state.\nCombining these two strategies together, we ﬁnd that the\nnetwork can learn a robust representation and our results\nshow that this strategy can achieve signiﬁcantly enhanced\nperformance than unsupervised approaches trained without\nthem. We demonstrate the effectiveness and the generality\nof our proposed methodology by evaluating our system on\nthree extensive skeleton-based and RGB+D action recog-\nnition datasets. Speciﬁcally, we show that our P&C unsu-\npervised system achieves high accuracy performance and\noutperforms prior methods.\n2. Related Work\nThe objective of action recognition is to assign a class la-\nbel to a sequence of frames with context information on the\naction performed, Fig. 1. Numerous approaches have been\nintroduced particularly for human movement action recog-\nnition. Such approaches use video frames (RGB), and/or\ndepth (RGB+D) and/or skeleton data, i.e. tracking of body\njoints (keypoints). Performing exclusive skeleton-based ac-\ntion recognition is especially advantageous since requires\nmuch less data, which is relatively easy to acquire and there-\nfore has the potential to be performed in real-time. Fur-\nthermore, in contrast to videos and depth, including various\ncontexts such as background, skeleton data can be used to\nunderstand the exclusive features of the actions. Indeed,\nin recent years, various supervised and unsupervised ap-\nproaches have been introduced for human skeleton-based\naction recognition. Most of skeleton-based approaches have\nbeen supervised approaches where an annotated set of ac-\ntions and labels should be provided for training them. In\nan unsupervised setup, the problem of action recognition is\nmuch more challenging. Only a few unsupervised skeleton-\nbased approaches have been proposed and several unsuper-\nvised approaches have been developed to use more infor-\nmation such as video frames and depth, i.e. unsupervised\nRGB+D. We review these prior approaches below and com-\npare our results with them.\nFor supervised skeleton-based action recognition, prior\nto deep learning methods, classical approaches were pro-\nposed to map the actions from Lie group to its Lie alge-\nbra and to perform classiﬁcation using a combination of\ndynamic time warping, Fourier temporal pyramid repre-\nsentation and linear SVM (e.g. LARP [24]). Deep learn-\ning approaches have been developed to classify skeleton\ndata as well, in particular, RNN based models that are de-\nsigned to work with sequences. For example, Du et al. [2]\nused hierarchical RNN (HBRNN-L) for action classiﬁca-\ntion and Shahroudy et al. [17] proposed part-aware LSTM\n(P-LSTM) as a baseline for the large scale skeleton action\nrecognition NTU RGB+D dataset. Since skeleton data is\nnoisy, largely due to variance in camera views, Zhang et\nal. [30] proposed a view-adaptive RNN (VA-RNN) which\nlearns a transformation from original skeleton data to a gen-\nFigure 2: PREDICT & CLUSTER system summary. A: System overview. B: Encoder-Decoder architecture.\neral pose. CNN based approaches have been also proposed\nfor supervised skeleton based recognition by constructing\na representation of body joints that can be processed by\nCNN. In particular, Du et al. [1] represented a skeleton se-\nquence as a matrix by concatenating the joint coordinates\nin each instant and arranging those vector representations\nin a chronological order and transforming the matrix into\nan image on which CNN is trained for classiﬁcation. In\naddition, Liu et al.[10] used an enhanced skeleton visual-\nization method in conjunction with CNN classiﬁcation for\nview invariant human action recognition. Recently, graph\nconvolution networks (GNN) gained popularity in skeleton-\nbased action recognition approaches. Yan et al. [29] in-\ntroduced Spatial Temporal Graph Convolutional Networks\n(ST-GCN), which were shown to be capable to learn both\nthe spatial and temporal patterns from skeleton data. A re-\ncent extension of such an approach by Shi et al. [18],[19]\nshowed that directed GNN can be used to encode the skele-\nton representation and also showed that two-stream GNN\ncan learn the graph in an adaptive manner.\nWhile recent supervised approaches show robust perfor-\nmance on action recognition, the unsupervised setup is ad-\nvantageous since it does not require labeling of sequences\nand may not need re-training when additional actions, not\nincluded in the training set, are introduced. Unsupervised\nmethods typically aim to obtain an effective feature repre-\nsentations by predicting future frames of input action se-\nquences or by re-generating the sequences. Unsupervised\napproaches were mostly proposed for videos of actions or\nvideos with additional information such as depth or op-\ntical ﬂow. Speciﬁcally, Srivastava et al. [22] proposed a\nrecurrent-based sequence to sequence (Seq2Seq) model as\nan autoencoder to learn the representation of a video. Such\nan approach is at the core of our method for body joints\ninput data. However, as we show, the approach will not\nbe able to achieve efﬁcient performance without particular\ntraining strategies that we develop to weaken the decoder\nand strengthen the encoder. Subsequently, Luo et al. [11]\ndeveloped a convolutional LSTM to use depth and optical\nﬂow information such that the network encodes depth input\nand uses the decoder to predict the optical ﬂow of future\nframes. Furthermore, Li et al. [8] proposed to employ a\ngenerative adversarial network (GAN) with a camera-view\ndiscriminator to assist the encoder in learning better repre-\nsentations.\nAs in unsupervised RGB+D approaches, skeleton-based\napproaches utilize the task of human motion prediction\nas the underlying task to learn action feature representa-\ntion. For such a task, RNN-based Seq2Seq models [12]\nwere shown to achieve improved accuracy in comparison\nto non-Seq2Seq based RNN models such as ERD [4] and\nS-RNN [6]. Recently, networks incorporating GANs have\nachieved improved performance on this task by utilizing the\npredictor network being RNN Seq2Seq and the discrimina-\ntor [5].\nUnsupervised approaches for skeleton-based action\nrecognition are scarce since obtaining effective feature rep-\nresentations from coordinate positions of body joints is\nchallenging. In particular, based on successful human mo-\ntion prediction network conﬁgurations, Zheng et al. [31]\n(LongT GAN) proposed a GAN encoder-decoder such that\nthe decoder attempts to re-generate the input sequence and\nthe discriminator is used to discriminate whether the re-\ngeneration is accurate. The feature representation used for\naction recognition is taken from the ﬁnal state of the en-\ncoder hidden representation. During training, the masked\nground truth input is provided to the decoder. The method\nwas tested on motion-capture databases, e.g., CMU Mocap,\nHDM05[14] and Berkeley MHAD[15]. Such datasets were\ncaptured by physical sensors (markers) and thus are much\ncleaner than marker-less data collected by depth cameras\nand do not test for multi-view variance which signiﬁcantly\naffects action recognition performance. Our baseline net-\nwork architecture is similar to the structure in Zheng et\nal. [31] since we use an encoder and decoder and we also\nuse the ﬁnal state of the encoder as a features representa-\nFigure 3: Pre-processing of body keypoints sequences ac-\ncording to view-invariant transformation.\ntion of the action sequences. However, as we show, it is re-\nquired to develop extended training strategies for the system\nto be applicable to larger scale multi-view and multi-subject\ndatasets. Speciﬁcally, instead of using the masked ground\ntruth as an input into the decoder, we propose methods to\nimprove learning of the encoder and to weaken the decoder.\n3. Methods\nPre-processing of body keypoints:\nBody keypoints\ndata is a sequence XV\nT of T frames captured from a particu-\nlar view, where each frame represents N = 3D coordinates\nof J joint keypoints\nXV\nT = {x1, x2, . . . , xT }, xt ∈RJ×N.\nAction sequences are captured from different views by\ndepth camera such as Microsoft Kinect and 3D human joint\npositions are extracted from a single depth image by a real-\ntime human skeleton tracking framework[20]. We align the\naction sequences by implementing a view-invariant trans-\nformation which transforms the keypoints coordinates from\noriginal coordinate system into a view-invariant coordinate\nsystem XV →X. The transformed skeleton joint coordi-\nnates are given by\nxj\nt = R−1(xj\nt −dR), ∀j ∈J, ∀t ∈T,\nwhere xj\nt ∈R3×1 are the coordinates of the j-th joint of\nthe t-th frame, R is the rotation matrix and dR is the origin\nof rotation. These are computed according to\nR =\n\"\nv1\n∥v1∥\n\f\f\f\f\nˆv2\n∥ˆv2∥\n\f\f\f\f\nv1 × ˆv2\n∥v1 × ˆv2∥\n#\n, dR = xroot\nt=0,\nwhere v1 = xspine\nt=0 −xroot\nt=0 is the vector perpendicular to the\nground, v2 = xhip left\nt=0\n−xhip right\nt=0\nis the difference vector be-\ntween left and right hips joints in the initial frame of each\nsequence and ˆv2 =\nv2−Projv1(v2)\n∥v2−Projv1(v2)∥. Projv1(v2) and v1 × ˆv2\ndenotes the vector projection of v2 onto v1 and the cross\nFigure 4: Encoder states trajectories visualized by projec-\ntion to 3 PCA space. Each color represents one type of ac-\ntion(blue: donning, red: sit down,green:carry, black: stand\nup). The cross symbol denotes the ﬁnal state. Left: before\ntraining; Right: after training.\nproduct of v1 and ˆv2, respectively. xRoot\nt=0 is the coordinate\nof the root joint in the initial frame [7] (see Fig. 3). Since\nactions can be of different lengths we down-sample each\naction sequence to be at most a ﬁxed length Tmax and pad\nwith zeros if the sequence length is smaller than that.\nSelf-organization of hidden states clustering:\nA key\nproperty that we utilize in our system is the recent obser-\nvation that propagation of input sequences through RNN\nself-organizes them into clusters within the hidden states\nof the network, i.e., clusters represent features in an embed-\nding of the hidden states[3]. Such strategy is a promising\nunsupervised method for multi-dimensional sequence clus-\ntering such as body keypoints sequences [23]. As we show,\nself-organization is inherent to any RNN architecture and\neven holds for random RNN which are initialized with ran-\ndom weights and kept ﬁxed, i.e., no training is performed.\nIndeed, when we input sequences of body keypoints of dif-\nferent actions into random RNN, the features in the hidden\nstate space turn out to be effective ﬁlters. While such strat-\negy is promising, the recognition accuracy outcome appears\nto be non-optimal (Table 1 P&C Rand). We therefore im-\nplement an encoder-decoder system, which we call PRE-\nDICT & CLUSTER (P&C), where the encoder propagates\ninput sequences and passes the last hidden state to the de-\ncoder. The decoder is used to regenerate the encoder in-\nput sequences. Furthermore, we utilize the random network\nsetup (which does not require training) to choose the op-\ntimal hyper-parameters for the network to be trained. We\ndescribe the components of P&C below.\nMotion prediction:\nAt the core of our unsupervised\nmethod is an encoder-decoder RNN (Seq2Seq). Such net-\nwork models were shown to be effective at prediction of fu-\nture evolution of multi-dimensional time-series of features\nincluding skeleton temporal data of various actions [12],[5].\nIn these applications the typical ﬂow in the network is uni-\ndirectional. The encoder processes an initial sequence of\nactivity and passes the last state to the decoder which in\nturn, based on this state generates the evolution forward.\nWe extend such network structure for our method (see sys-\ntem overview in Fig. 2).\nWe propose a bi-directional ﬂow such that the network\ncan capture better long-term dependencies in the action se-\nquences.\nSpeciﬁcally, the encoder is a multi-layered bi-\ndirectional Gated Recurrent Unit (GRU) which input is a\nwhole sequence of body keypoints corresponding to an ac-\ntion. We denote the forward and backward directions hid-\nden states of the last layer of encoder at time t as −→\nEt and\n←−\nEt respectively, and the ﬁnal state of the encoder as their\nconcatenation ET = {−→\nET , ←−\nET }.\nThe decoder is a uni-\ndirectional GRU with hidden states at time t denoted as Dt.\nThe ﬁnal state of the encoder is fed into the decoder as its\ninitial state, i.e., D0 = ET . In such a setup, the decoder\ngenerates a sequence based on ET initialization. In a typ-\nical prediction task, the generated sequence will be com-\npared with forward evolution of the same sequence (predic-\ntion loss). In our system, since our goal is to perform action\nrecognition, the decoder is required to re-generate the whole\ninput sequence (re-generation loss). Speciﬁcally, for the de-\ncoder outputs ˆX = {ˆx1, ˆx2, . . . , ˆxT } the re-generation loss\nfunction is the error between X and ˆX. In particular, we use\nmean square error (MSE) L = 1\nT\nPT\nt=1(xt −ˆxt)2 or mean\nabsolute error (MAE) L =\n1\nT\nPT\nt=1 |xt −ˆxt| as plausible\nlosses.\nHyper-parameter search: As in any deep learning sys-\ntem, hyper-parameters signiﬁcantly impact network perfor-\nmance and require tuning for optimal regime. We utilize\nthe self-organization feature of random initialized RNN to\npropagate the sequences through the network and use net-\nwork performance prior to training as an optimization for\nhyper-parameter tuning. Speciﬁcally, we evaluate the ca-\npacity of the encoder by propagating the skeleton sequence\nthrough the encoder and evaluate the performance of recog-\nnition on the ﬁnal encoder state. We observe that this ef-\nﬁcient hyper-parameter search signiﬁcantly reduces total\ntraining time when an optimal network amenable for train-\ning is being selected.\nTraining: With optimal hyper-parameter encoder being\nset, training is performed on the outputs of the decoder to\npredict (re-generate) the encoder’s input action sequence.\nTraining for prediction is typically performed according to\none of the two approaches: (i) unconditional training in\nwhich zeros are being fed into the decoder at each time step\nor (ii) conditional in which an initial input is fed into the\nﬁrst time-step of the decoder and subsequent time-steps use\nthe predicted output of the previous time step as their input\n[22]. Based on these training strategies, we propose two\ndecoder conﬁgurations (i) Fixed Weights decoder (FW) or\n(ii) Fixed States decoder (FS) to weaken the decoder, i.e. to\nforce it to perform the re-generation based upon the infor-\nmation provided by the hidden representation of the encoder\nFigure 5: Feature-level autoencoder and KNN Classiﬁer\nand thus improve the encoder’s clustering performance, see\nFig.4.\n1.Fixed Weights decoder (FW): The input into the decoder\nis unconditional in this conﬁguration. The decoder is not\nexpected to learn a useful information for prediction and it\nexclusively relies on the state passed by the encoder. The\nweights of the decoder can thereby be assigned as random\nand the decoder is used as a recurrent propagator of the se-\nquences. In training for the re-generation loss such conﬁg-\nuration is expected to force the encoder to learn the latent\nfeatures and represent them with the ﬁnal state passed to the\ndecoder. This intuitive method turns out to be computation-\nally efﬁcient since only the encoder is being trained and our\nresults indicate favorable performance in conjunction with\nKNN action classiﬁcation.\n2.Fixed States decoder (FS): The external input into the de-\ncoder is conditional in this conﬁguration ( external input\ninto each time-step is the output of the previous time-step),\nhowever the internal input, typically the hidden state from\nprevious step, is replaced by the ﬁnal state of the encoder\nET . Namely, in RNN cell\nht = σ(Wxxt + Whgt + bh), gt = ht−1 →ET ,\nyt = σ(Wyht + by),\nxt+1 = yt,\nwith xt the external input, yt the output and ht the hidden\nstate at time-step t, ht−1 terms are replaced by ET . In addi-\ntion, we also add residual connection between external input\nand output, which has been shown useful in human motion\nprediction as well [12]. The ﬁnal output and next input will\nbe ˆyt = yt + xt and ˆxt+1 = ˆyt, respectively. The con-\nﬁguration forces the network to rely on ET , instead of the\nhidden state at previous time-step and eliminates vanishing\nof the gradient since during back-propagation at each time-\nstep there is a deﬁned gradient back to the ﬁnal encoder\nstate.\nFeature level auto-encoder: After training the prediction\nnetwork we extract the ﬁnal encoder state ET as the fea-\nture vector associated with each action sequence. Since the\nFigure 6: Training curves (accuracy:blue; loss:red) for three of the datasets, left to right: NW-UCLA (FW v.s. no FW),\nUWA3D(FS v.s. no FS), NTU-RGB+D Cross-View(FS v.s. no FS).\nfeature vector is high-dimensional, we use a feature-level\nauto-encoder that learns the core low dimensional compo-\nnents of the high-dimensional feature so it can be utilized\nfor classiﬁcation (Fig. 5). Speciﬁcally, we implement the\nauto-encoder, denoted as f to be of an encoder-decoder ar-\nchitecture with parameters θ such that\nˆ\nET = fθ(ET ) ≈ET .\nThe encoder and the decoder are multi-layer FC networks\nwith non-linear tanh activation and we implement the\nfollowing loss laec =\n\f\f\fET −ˆ\nET\n\f\f\f.\nK-nearest neighbors classiﬁer: For evaluation of our\nmethod on action recognition task we use a K-nearest neigh-\nbors (KNN) classiﬁer on the middle layer of the auto-\nencoder feature vector.\nSpeciﬁcally, we apply the KNN\nclassiﬁer (with k = 1) on the features of the trained network\non all sequences in the training set to assign classes. We\nthen use cosine similarity as the distance metric to perform\nrecognition, i.e., place each tested sequence in a class. No-\ntably, KNN classiﬁer does not require to learn extra weights\nto action placement.\n4. Experimental Results and Datasets\nImplementation details: To train the network, all body\nkeypoints sequences are pre-processed according to the\nview-invariant transformation and down-sampled to have\nat most 50 frames (Fig. 3). The coordinates are also nor-\nmalized to the range of [−1, 1]. Using the hyper-parameter\nsearch, employing random RNN propagation discussed\nabove, we set the following architecture: Encoder: 3-Layer\nBi-GRU with N = 1024 units in each layer. Decoder:\n1-Layer Uni-GRU with N = 2048 units such that it is\ncompatible with the dimensions of the encoder ﬁnal state\nET .All GRUs are initialized with a random uniform distri-\nbution. Feature-level auto-encoder: 6 FC Layers with the\nfollowing dimensions: input feature vector(dim= 2048) →\nFC(1024) →FC(512) →FC(256)→FC(512)→FC(1024)\n→FC(2048). All FCs use tanh activation except the last\nlayer which is linear.\nThe middle layer of auto-encoder\noutputs a vector feature of 256 elements which is used as\nthe ﬁnal feature. We use Adam optimizer and learning rate\nstarting from 0.0001 and 0.95 decay rate at every 1000 iter-\nations. The gradients are clipped if the norm is greater than\n25 to avoid gradient explosion. It takes 0.7sec per training\niteration and 0.1sec to forward propagate with batch size of\n64 on one Nvidia Titan X GPU. Please see additional details\nof architecture choices in the supplementary material.\nDatasets: We use three different data-sets for training, eval-\nuation and comparison of our P&C system with related ap-\nproaches. The three data-sets include various number of\nclasses, types of actions, body keypoints captured from dif-\nferent views and on different subjects. In these datasets,\nthe body keypoints are captured by depth cameras and\nalso include additional data, e.g., videos (RGB) and depth\n(+D). Various types of action recognition approaches have\nbeen applied to these datasets, e.g., supervised skeleton ap-\nproaches and unsupervised RGB+D approaches. We list\nthese types of approaches and their performance on the tests\nin the datasets in Table 1.\nNotably, as far as we know,\nour work is the ﬁrst fully unsupervised skeleton based\napproach applied on these extensive action recognition\ntests.\nThe datasets that we have applied our P&C system to\nare (i) NW-UCLA, (ii) UWA3D, and (iii) NTU RGB+D.\nThe datasets include 3D body keypoints of 10, 30, 60 ac-\ntion classes respectively. We brieﬂy describe them below.\nNorth-Western UCLA (NW-UCLA) dataset [26] is cap-\ntured by Kinect v1 and contains 1494 videos of 10 actions.\nThese actions are performed by 10 subjects repeated 1 to\n6 times. There are three views of each action and for each\nsubject 20 joints are being recorded. We follow [10] and\n[26] to use the ﬁrst two views (V1,V2) for training and last\nMethod\nNW-UCLA\n(%)\nSupervised Skeleton\nHOPC[16]\n74.2\nActionlet Ens [25]\n76.0\nHBRNN-L[2]\n78.5\nVA-RNN-Aug[30]\n90.7\nAGC-LSTM[21]\n93.3\nUnsupervised RGB+D\nLuo et al.[11]\n50.7\nLi et al.[8]\n62.5\nUnsupervised Skeleton\nP&C Rand (Our)\n72.0\nLongT GAN [31]\n74.3\nP&C FS-AEC (Our)\n83.8\nP&C FW-AEC (Our)\n84.9\nMethod\nUWA3D\nV3 (%)\nV4 (%)\nSupervised Skeleton\nHOJ3D[28]\n15.3\n28.2\n2-layer P-LSTM[27]\n27.6\n24.3\nIndRNN (6 layers)[27]\n30.7\n47.2\nIndRNN (4 layers)[27]\n34.3\n54.8\nST-GCN[27]\n36.4\n26.2\nActionlet Ens[25]\n45.0\n40.4\nLARP[24]\n49.4\n42.8\nHOPC[16]\n52.7\n51.8\nVA-RNN-Aug[30]\n70.9\n73.2\nUnsupervised Skeleton\nP&C Rand (Our)\n48.5\n51.5\nLongT GAN [31]\n53.4\n59.9\nP&C FS-AEC (Our)\n59.5\n63.1\nP&C FW-AEC (Our)\n59.9\n63.1\nMethod\nNTU RGB-D 60\nC-View\nC-Subject\n(%)\n(%)\nSupervised Skeleton\nHOPC[16]\n52.8\n50.1\nHBRNN[2]\n64.0\n59.1\n2L P-LSTM[17]\n70.3\n62.9\nST-LSTM[9]\n77.7\n69.2\nVA-RNN-Aug[30]\n87.6\n79.4\nUnsupervised RGB+D\nShufﬂe & learn[13]\n40.9\n46.2\nLuo et al.[11]\n53.2\n61.4\nLi et al.[8]\n63.9\n68.1\nUnsupervised Skeleton\nLongT GAN [31]\n48.1\n39.1\nP&C Rand (Our)\n56.4\n39.6\nP&C FS-AEC (Our)\n76.3\n50.6\nP&C FW-AEC (Our)\n76.1\n50.7\nTable 1: Comparison of action recognition performance of our P&C system with state-of-the-art approaches of Supervised\nSkeleton (blue) and Unsupervised RGB+D (purple); Unsupervised Skeleton (red)) types.\nviews (V3) to test cross view action recognition. UWA3D\nMultiview Activity II (UWA3D) dataset [16] contains 30\nhuman actions performed 4 times by 10 subjects. 15 joints\nare being recorded and each action is observed from four\nviews: frontal, left and right sides, and top. The dataset\nis challenging due to many views and the resulting self-\nocclusions from considering only part of them. In addition,\nthere is a high similarity among actions, e.g., the two ac-\ntions ”drinking and phone answering have many keypoints\nbeing near identical and not moving and there are subtle dif-\nferences in the moving keypoints such as the location of the\nhand. NTU RGB+D dataset [17] is a large scale dataset for\n3D human activity analysis. This dataset consists of 56, 880\nvideo samples, captured from 40 different human subjects,\nusing Microsoft Kinect v2. NTU RGB+D(60) contains 60\naction classes. We use the 3D skeleton data for our exper-\niments such that each time sample contains 25 joints. We\ntest our P&C method on both cross-view and cross-subject\nprotocols.\n5. Evaluation and Comparison\nEvaluation: In all experiments, we use the K-nearest\nneighbors classiﬁer with k = 1 to compute the action recog-\nnition accuracy and evaluate the performance of our P&C\nmethod. We test different variants of P&C architectures\n(combinations of components described in Section 3) and\nreport a subset of these in the paper: baseline random ini-\ntialized encoder with no training (P&C-Rand), full system\nwith FS decoder and feature-level auto-encoder (P&C-FS-\nAEC) and full system with FW decoder and feature-level\nauto-encoder (P&C-FW-AEC). We report the rest of the\ncombinations and their results in the Supplementary mate-\nrial.\nFig. 6 shows the optimization of the regeneration loss\n(red) and the resulting accuracy (blue) during training for\neach dataset. We include plots of additional P&C conﬁg-\nurations in the Supplementary material. The initial accu-\nracy appears to be substantial and this is attributed to the\nhyper-parameter search being performed on random initial-\nized networks prior to training that we describe in Section\n3. Indeed, we ﬁnd that using appropriate initialization, the\nencoder, without any training, effectively directs similar ac-\ntion sequences to similar ﬁnal states. Training enhances this\nperformance further in both P&C FW and P&C FS conﬁg-\nurations. Over multiple training iterations both P&C FW\nand P&C FS achieve higher accuracy than no FW and no\nFS in all datasets. While the convergence of the loss curve\nindicates improvement on the accuracy, the value of the\nloss does not necessarily indicate a better accuracy as can\nbe observed from loss and accuracy curves of training on\nUWA3D and NTU-RGB+D (Fig. 6 middle, right).\nWe show the confusion matrices for the three consid-\nered datasets in Fig. 7. In NW-UCLA (with least classes)\nwe show the elements of the 10x10 matrix. Our method\nachieves high-accuracy (> 83%) on average and there are\nthree actions (pick up with two hands, drop trash, sit down)\nfor which it recognizes with nearly 100% accuracy. We also\nshow in Fig. 8 a t-SNE visualization of the learned features\nfor NW-UCLA test. Even in this 2D embedding it is clearly\nevident that the features for each class are well separated.\nAs more action classes are considered, the recognition be-\ncomes a more difﬁcult task and also depends on amount of\nFigure 7: Confusion matrices for testing P&C performance on the three datasets(from left to right): NW-UCLA(10 actions);\nUWA3D V4(30 actions); NTU-RGBD Cross-View(60 actions).\nFigure 8: t-SNE visualization of learned features on NW-\nUCLA dataset.\ntraining data. For example, while NTU RGB+D has more\nclasses than UWA3D, the recognition accuracy on NTU\nRGB+D is smoother and results with better performance\nsince it has 40 times more data than UWA3D. Our results\nshow that our method is compatible with varying data sizes\nand number of classes.\nComparison: We compare the performance of our P&C\nmethod with prior related supervised and unsupervised\nmethods applied to (left-to-right): NW-UCLA, UWA3D,\nNTU RGB+D datasets, see Table 1. In particular, we com-\npare action recognition accuracy with approaches based on\nsupervised skeleton data (blue), unsupervised RGB+D data\n(purple) and unsupervised skeleton data (red). For compari-\nson with unsupervised skeleton methods, we implement and\nreproduce the LongTerm GAN model (LongT GAN) as in-\ntroduced in [31] and list its performance.\nFor NW-UCLA, P&C outperforms previous unsuper-\nvised methods (both RGB+D and skeleton based).\nOur\nmethod even outperforms the ﬁrst three supervised meth-\nods listed in Table 1-left. UWA3D is considered a chal-\nlenging test for many deep learning approaches since the\nnumber of sequences is small, while it includes a large num-\nber of classes (30). Indeed, action recognition performance\nof many supervised skeleton approaches is low (< 50%).\nFor such datasets, it appears that the unsupervised approach\ncould be more favorable, i.e., even P&C Rand reaches per-\nformance of ≈50%. LongT GAN achieves slightly higher\nperformance than P&C Rand, however, not as high as P&C\nFS/FW-AEC which perform with ≈60%. Only a single su-\npervised skeleton method, VA-RNN-Aug, is able to perform\nbetter than our unsupervised approach, see Table 1-middle.\nOn the large scale NTU-RGB+D dataset, our method per-\nforms extremely well on the cross-view test.\nIt outper-\nforms prior unsupervised methods (both RGB+D and skele-\nton based) and on-par with ST-LSTM (second best super-\nvised skeleton method), see Table 1-right. On the cross-\nsubject test we obtain performance that is higher (including\nP&C Rand) than the prior unsupervised skeleton approach,\nhowever, our accuracy does not outperform unsupervised\nRGB+D approaches. We believe that the reason stems from\nskeleton based approaches not performing well in general\non cross-subject tests since additional aspects such as sub-\njects parameters, e.g., skeleton geometry and invariant nor-\nmalization from subject to subject, need to be taken into\naccount.\nIn summary, for all three datasets, we used a single archi-\ntecture and it was able to outperform the prior unsupervised\nskeleton method, LongT-GAN[31], most supervised skele-\nton methods and unsupervised RGB+D methods on cross\nview tests and some supervised skeleton and unsupervised\nRGB+D on large scale cross subject test.\n6. Conclusion\nWe presented a novel unsupervised model for human\nskeleton-based action recognition. Our system reaches en-\nhanced performance compared to prior approaches due to\nnovel training strategies which weaken the decoder and\ntraining of the encoder. As a result the network learns more\nseparable representations.\nExperimental results demon-\nstrate that our unsupervised model can effectively learn dis-\ntinctive action features on three benchmark datasets and\noutperform prior unsupervised methods.\nReferences\n[1] Yong Du, Yun Fu, and Liang Wang.\nSkeleton based ac-\ntion recognition with convolutional neural network. In 2015\n3rd IAPR Asian Conference on Pattern Recognition (ACPR),\npages 579–583. IEEE, 2015.\n[2] Yong Du, Wei Wang, and Liang Wang. Hierarchical recur-\nrent neural network for skeleton based action recognition. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1110–1118, 2015.\n[3] Matthew Farrell, Stefano Recanatesi, Guillaume Lajoie, and\nEric Shea-Brown. Recurrent neural networks learn robust\nrepresentations by dynamically balancing compression and\nexpansion. 2019.\n[4] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Ji-\ntendra Malik. Recurrent network models for human dynam-\nics. In Proceedings of the IEEE International Conference on\nComputer Vision, pages 4346–4354, 2015.\n[5] Liang-Yan Gui, Yu-Xiong Wang, Xiaodan Liang, and\nJos´e MF Moura. Adversarial geometry-aware human mo-\ntion prediction. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 786–803, 2018.\n[6] Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh\nSaxena. Structural-rnn: Deep learning on spatio-temporal\ngraphs. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 5308–5317, 2016.\n[7] Inwoong Lee, Doyoung Kim, Seoungyoon Kang, and\nSanghoon Lee. Ensemble deep learning for skeleton-based\naction recognition using temporal sliding lstm networks. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, pages 1012–1020, 2017.\n[8] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan Kankan-\nhalli. Unsupervised learning of view-invariant action repre-\nsentations. In Advances in Neural Information Processing\nSystems, pages 1254–1264, 2018.\n[9] Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang.\nSpatio-temporal lstm with trust gates for 3d human action\nrecognition. In European Conference on Computer Vision,\npages 816–833. Springer, 2016.\n[10] Mengyuan Liu, Hong Liu, and Chen Chen. Enhanced skele-\nton visualization for view invariant human action recogni-\ntion. Pattern Recognition, 68:346–362, 2017.\n[11] Zelun Luo, Boya Peng, De-An Huang, Alexandre Alahi, and\nLi Fei-Fei. Unsupervised learning of long-term motion dy-\nnamics for videos. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 2203–\n2212, 2017.\n[12] Julieta Martinez, Michael J Black, and Javier Romero. On\nhuman motion prediction using recurrent neural networks.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2891–2900, 2017.\n[13] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuf-\nﬂe and learn: unsupervised learning using temporal order\nveriﬁcation. In European Conference on Computer Vision,\npages 527–544. Springer, 2016.\n[14] M. M¨uller, T. R¨oder, M. Clausen, B. Eberhardt, B. Kr¨uger,\nand A. Weber.\nDocumentation mocap database hdm05.\nTechnical Report CG-2007-2, Universit¨at Bonn, June 2007.\n[15] Ferda Oﬂi, Rizwan Chaudhry, Gregorij Kurillo, Ren´e Vidal,\nand Ruzena Bajcsy. Berkeley mhad: A comprehensive mul-\ntimodal human action database.\nIn 2013 IEEE Workshop\non Applications of Computer Vision (WACV), pages 53–60.\nIEEE, 2013.\n[16] Hossein Rahmani, Arif Mahmood, Du Q Huynh, and Ajmal\nMian. Hopc: Histogram of oriented principal components of\n3d pointclouds for action recognition. In European confer-\nence on computer vision, pages 742–757. Springer, 2014.\n[17] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\nNtu rgb+ d: A large scale dataset for 3d human activity anal-\nysis. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 1010–1019, 2016.\n[18] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.\nSkeleton-based action recognition with directed graph neural\nnetworks. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 7912–7921,\n2019.\n[19] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Two-\nstream adaptive graph convolutional networks for skeleton-\nbased action recognition. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n12026–12035, 2019.\n[20] Jamie Shotton, Andrew Fitzgibbon, Mat Cook, Toby Sharp,\nMark Finocchio, Richard Moore, Alex Kipman, and Andrew\nBlake. Real-time human pose recognition in parts from sin-\ngle depth images. In CVPR 2011, pages 1297–1304. Ieee,\n2011.\n[21] Chenyang Si, Wentao Chen, Wei Wang, Liang Wang, and\nTieniu Tan. An attention enhanced graph convolutional lstm\nnetwork for skeleton-based action recognition. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1227–1236, 2019.\n[22] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudi-\nnov. Unsupervised learning of video representations using\nlstms.\nIn International conference on machine learning,\npages 843–852, 2015.\n[23] Kun Su and Eli Shlizerman. Clustering and recognition of\nspatiotemporal features through interpretable embedding of\nsequence to sequence recurrent neural networks.\n[24] Raviteja Vemulapalli, Felipe Arrate, and Rama Chellappa.\nHuman action recognition by representing 3d skeletons as\npoints in a lie group. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 588–595,\n2014.\n[25] Jiang Wang, Zicheng Liu, Ying Wu, and Junsong Yuan.\nLearning actionlet ensemble for 3d human action recogni-\ntion.\nIEEE transactions on pattern analysis and machine\nintelligence, 36(5):914–927, 2013.\n[26] Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-\nChun Zhu. Cross-view action modeling, learning and recog-\nnition. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2649–2656, 2014.\n[27] Lei Wang, Du Q Huynh, and Piotr Koniusz. A comparative\nreview of recent kinect-based action recognition algorithms.\narXiv preprint arXiv:1906.09955, 2019.\n[28] Lu Xia, Chia-Chih Chen, and Jake K Aggarwal. View invari-\nant human action recognition using histograms of 3d joints.\nIn 2012 IEEE Computer Society Conference on Computer\nVision and Pattern Recognition Workshops, pages 20–27.\nIEEE, 2012.\n[29] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-\nral graph convolutional networks for skeleton-based action\nrecognition. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, 2018.\n[30] Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng,\nJianru Xue, and Nanning Zheng. View adaptive neural net-\nworks for high performance skeleton-based human action\nrecognition. IEEE transactions on pattern analysis and ma-\nchine intelligence, 2019.\n[31] Nenggan Zheng, Jun Wen, Risheng Liu, Liangqu Long, Jian-\nhua Dai, and Zhefeng Gong. Unsupervised representation\nlearning with long-term dynamics for skeleton based action\nrecognition. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, 2018.\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "eess.IV"
  ],
  "published": "2019-11-27",
  "updated": "2019-11-27"
}