{
  "id": "http://arxiv.org/abs/1911.12461v1",
  "title": "Two-Stage Learning for Uplink Channel Estimation in One-Bit Massive MIMO",
  "authors": [
    "Eren Balevi",
    "Jeffrey G. Andrews"
  ],
  "abstract": "We develop a two-stage deep learning pipeline architecture to estimate the\nuplink massive MIMO channel with one-bit ADCs. This deep learning pipeline is\ncomposed of two separate generative deep learning models. The first one is a\nsupervised learning model and designed to compensate for the quantization loss.\nThe second one is an unsupervised learning model and optimized for denoising.\nOur results show that the proposed deep learning-based channel estimator can\nsignificantly outperform other state-of-the-art channel estimators for one-bit\nquantized massive MIMO systems. In particular, our design provides 5-10 dB gain\nin channel estimation error. Furthermore, it requires a reasonable amount of\npilots, on the order of 20 per coherence time interval.",
  "text": "Two-Stage Learning for Uplink Channel Estimation\nin One-Bit Massive MIMO\nEren Balevi and Jeffrey G. Andrews\nDepartment of Electrical and Computer Engineering\nThe University of Texas at Austin, TX 78712, USA\nEmail: erenbalevi@utexas.edu, jandrews@ece.utexas.edu\nAbstract‚ÄîWe develop a two-stage deep learning pipeline archi-\ntecture to estimate the uplink massive MIMO channel with one-\nbit ADCs. This deep learning pipeline is composed of two separate\ngenerative deep learning models. The Ô¨Årst one is a supervised\nlearning model and designed to compensate for the quantization\nloss. The second one is an unsupervised learning model and\noptimized for denoising. Our results show that the proposed deep\nlearning-based channel estimator can signiÔ¨Åcantly outperform\nother state-of-the-art channel estimators for one-bit quantized\nmassive MIMO systems. In particular, our design provides 5-10\ndB gain in channel estimation error. Furthermore, it requires a\nreasonable amount of pilots, on the order of 20 per coherence\ntime interval.\nI. INTRODUCTION\nMassive multiple-input multiple-output (MIMO) is a key\ntechnology to increase data throughput by allowing many users\nto concurrently use the same spectrum spatially [1]. This is\nachieved by eliminating interference among users thanks to the\nsubstantial degrees of freedom that comes from a large number\nof antennas. More speciÔ¨Åcally, this inter-user interference is\nmitigated by aligning beams properly for each user. This beam\nalignment conventionally requires good enough channel state\ninformation (CSI). Hence, acquiring CSI is of great importance\nin massive MIMO communication systems.\nChannel estimation has always been a challenge for com-\nmunication systems due to the endless goal of reducing\ncomputational complexity and the number of pilot tones. This\nis particularly true for massive MIMO, because of the high\nsignal dimension. Hence, massive MIMO channel estimation\nhas to be performed with low complexity hardware and limited\nnumber of pilots, in which the former requirement is mainly\nto satisfy power consumption budget and the latter is for\nbandwidth efÔ¨Åciency. In this paper, these problems are handled\nfor uplink multiuser massive MIMO. Although our main intent\nis for lower frequencies, it can be trivially applied for mmWave\ntransmission.\nUplink massive MIMO nominally requires a correspond-\ningly large number of analog-to-digital converters (ADCs) at\nthe base station. This causes untenable power consumption and\nhardware complexity. There are many papers in the literature\nto alleviate these problems. These papers support employing\nlow-resolution ADCs and analyze the impact of having a pair\nof one-bit quantization per each antenna, i.e., one for each real\nand imaginary component. More speciÔ¨Åcally, [2] shows the\neffects of one-bit quantization in terms of mutual information\nand symbol error rate for massive MIMO. A near maximum\nlikelihood detector for one-bit uplink massive MIMO was\ndesigned in [3]. Furthermore, [4], [5], [6] proposed some\nchannel estimation methods against the detrimental impacts\nof one-bit quantization.\nIn this paper, we propose to use deep learning methods\nfor channel estimation in uplink massive MIMO with one-bit\nquantized received signals. This is motivated by the success of\ndeep learning while coping with signiÔ¨Åcant nonlinearities [7].\nThere has been a growing interest to make use of deep learn-\ning in communication systems [8], [9], [10], [11] including\nchannel estimation [12], [13], [14]. The existing deep learning-\nbased channel estimators rely on discriminative models. Note\nthat a discriminative model is the one that simply maps the\ngiven input data or observations to their target values without\nusing any a prior knowledge from these data. On the other\nhand, a generative model makes use of the a prior information\nwhile maximizing the likelihood. This motivation has recently\ninspired some deep generative model-based channel estimators\n[15], [16], [17].\nThe main contribution of this paper is to estimate the\nuplink channel in massive MIMO under the constraint of\none-bit quantization by leveraging generative models. More\nspeciÔ¨Åcally, a deep learning pipeline architecture is proposed\nfor this problem. Our model is composed of two types of\ngenerative deep neural networks that were previously used for\nsingle antenna one-bit quantized OFDM channel estimation\n[15] and single antenna unquantized OFDM channel estima-\ntion [16]. The proposed pipeline model works surprisingly well\nsuch that it can outperform state-of-the-art one-bit quantized\nchannel estimators for uplink massive MIMO by 5-10 dB.\nPromisingly, this design only requires approximately 20 pilots\nper coherence time interval.\nThis paper is organized as follows. The system model and\nproblem statement are explained in Section II. The proposed\narchitecture is given in Section III. We provide the numerical\nresults and computational complexity analysis in Section IV.\nThe paper ends with concluding remarks in Section V.\nII. SYSTEM MODEL AND PROBLEM STATEMENT\nIn this paper, K single antenna users send orthogonal\nOFDM symbols at the same time and on the same frequency\nband to a base station that has M antennas. It is assumed that\neach OFDM symbol has Nf subcarriers. With these settings,\narXiv:1911.12461v1  [eess.SP]  27 Nov 2019\nthe received signal at the mth antenna in response to one\ntransmitted OFDM symbol becomes\ny[m] =\nK\nX\nk=1\nHk[m]FHxk + w[m],\n(1)\nwhere Hk[m] is an Nf √ó Nf circulant matrix, FH is an\nNf √ó Nf inverse discrete Fourier transform (IDFT) matrix,\nand the OFDM symbol xk is an Nf √ó1 vector. The zero-mean\nGaussian noise vector is represented by w[m]. This received\nsignal at each antenna port is quantized with a complex one-\nbit ADC. Thus, the real and imaginary part of the signal are\nquantized separately as\nr[m] = Q(y[m])\n(2)\nwhere\nQ(y[m])) =\n1\n‚àö\n2sign(‚Ñú{y[m]}) + j\n‚àö\n2sign(‚Ñë{y[m]})\n(3)\nis applied element-wise along the vector y[m]. Combining\nthe received signal r[m] over all antennas yields an Nf √ó M\nmatrix, which is\nR = [r[1] r[2] ¬∑ ¬∑ ¬∑ r[M]].\n(4)\nIn this paper, we assume that one coherence time is com-\nposed of N OFDM symbols. The channel or Hk[m] remains\nconstant during a coherence time interval and it periodically\nchanges with coherence time interval. In this setup, the channel\ntaps between the kth user and the mth antenna of the base\nstation in the frequency domain is Œªk[m] = diag(Œõk[m]),\nwhere\nŒõk[m] = FHk[m]FH.\n(5)\nHence, the channel between the kth user and the base station\nfor all the M antennas becomes an Nf √ó M matrix\nŒõk = [diag(Œõk[1]) ¬∑ ¬∑ ¬∑ diag(Œõk[M])].\n(6)\nOur problem is to estimate Œõk from the received signal R,\nwhich is deÔ¨Åned in (4), with some number of pilots smaller\nthan N. These Œõk matrices can be found separately for each\nuser, because each user has orthogonal pilot sequences.\nIII. DEEP LEARNING PIPELINE FOR ONE-BIT MASSIVE\nMIMO CHANNEL ESTIMATION\nOne-bit quantization leads to signiÔ¨Åcant information loss at\nthe receiver front-end. This considerably complicates channel\nestimation. We propose a pipeline deep learning architecture\nto reliably estimate the one-bit massive MIMO channel. Our\narchitecture is composed of two separate deep learning models,\neach of which is specialized for different purposes. To be more\nprecise, the Ô¨Årst deep learning model, which is a generative\nsupervised learning (SL) model, is designed for recovering\nthe information loss due to the quantization. The second one,\nwhich is a generative unsupervised learning (USL) model,\naims to denoise the received signal for channel estimation.\nThe overall two-stage system model is depicted in Fig. 1, in\n1-bit \nADC\n1-bit \nADC\nReal\nImag\n1-bit \nADC\n1-bit \nADC\nReal\nImag\nSL-M\nUSL \n(DIP)\nChannel \nEstimates\nBase Station\nUE-1\nUE-K\nAntenna-1\nAntenna-M\nSL-1\nFig. 1.\nOne-bit quantized massive MIMO system in which K users send\ntheir pilots to the base station with M antennas. The received signal is Ô¨Årst\nprocessed with M supervised learning (SL) blocks, and then processed with\none special unsupervised learning model, which is the deep image prior (DIP).\nwhich the tasks from SL-1 to SL-M involve the Ô¨Årst stage and\nthe USL task is the second stage.\nThe main reason behind the usage of generative models\nboth for supervised and unsupervised learning is associated\nwith the fact that generative models take into account the a\npriori knowledge as opposed to discriminative models. It is\nworth emphasizing that these priors are quite important for\nour problem, because one-bit channel estimation yields an ill-\nposed problem. This means that there is no hope to solve it\nwithout using priors. In particular, the quality of these priors\ndetermine the channel estimation error.\nA. Stage-1: Supervised Learning\nThe received signal at each antenna port, which is given\nin (2), is Ô¨Årst processed with a separate supervised deep\nneural network. This means that M deep neural networks are\nmaintained in parallel. It is assumed that all these M neural\nnetworks have the same architecture, which is a standard deep\nneural network that has two hidden layers as illustrated in Fig.\n2. It is worth emphasizing that the parameters of these M deep\nneural network are not the same, because each one of them\nhas its own labels and is trained individually according to their\nreceived signals and labels.\nError\n ùëãùëò\nHidden \nLayer -2\n. . .\n. . .\n. . .\n. . .\nHidden \nLayer -1\nInput \nLayer\nOutput \nLayer\nLabeled \ndata\nSL (Supervised Learning)\nFig. 2. The supervised deep learning architecture that is employed and trained\nseparately for each antenna.\nIn supervised learning, the most critical thing is to deÔ¨Åne the\nlabels intelligently to push the parameters of a neural neural\ntowards the desired goal. Our recent work proposed generating\na labeled data set on the Ô¨Çy for single antenna OFDM receivers\nusing Bussgang‚Äôs theorem [15]. Accordingly, the diagonals of\nTABLE I\nTHE SUPERVISED DNN ARCHITECTURE FOR CHANNEL ESTIMATION WITH\nONE-BIT ADCS\nLayer\nType\nSize\nActivation\nWeights\nInput Layer\nPilot Symbols\n2Nf\n-\n‚àí\nHidden Layer-1\nFully Connected\n4Nf\nReLU\nŒ¶1\nHidden Layer-2\nFully Connected\n4Nf\nReLU\nŒ¶2\nOutput\nFully Connected\n2Nf\nLinear\nŒ¶3\nthe matrix Fr[m]xkH are used as the labels. The theoretical\nground for the selection of these labels is given in [15]. This\nlabeling policy is also used in this one-bit massive MIMO\nchannel estimation while training M supervised deep neural\nnetworks.\nThe layers, their types, sizes, activation functions and\nweights of the supervised deep neural network at each RF\nbranch are summarized in Table I. State-of-the-art software\nlibraries that implement neural networks do not support com-\nplex operations. Thus the real and imaginary part of the\ncomplex vectors are concatenated at each antenna to obtain a\n2Nf √ó1 real vector. Without loss of generality, the dimension\nof the hidden layers is taken to be twice that of the input\nand output layer, giving 32N 2\nf trainable parameters, which\nincreases quadratically with the number of subcarriers. Notice\nthat a single hidden layer can give the same performance with\ntwo hidden layers if it has a sufÔ¨Åcient number of neurons\ndue to the universal function approximation theorem of neural\nnetworks [7]. However, this brings additional computational\ncomplexity, and hence having two hidden layers with reason-\nable number of neurons seems a good compromise. RectiÔ¨Åed\nlinear unit (ReLU) is used in the hidden layers as an activation\nfunction for fast convergence, and a linear activation function\nis utilized at the output layer.\nWe propose to use the aforementioned model as a regression\ntask. Accordingly, the input layer takes the pilots xk,p and\nproduces the corresponding output zp[m] for p = 1, ¬∑ ¬∑ ¬∑ , Nt\nwhere Nt is the total number of pilots transmitted over the\nchannel for one coherence interval, in which Nt < N. Notice\nthat the pilots for the users are sent orthogonally, and hence\ncan be treated individually. This zp[m] for m = 1, 2, ¬∑ ¬∑ ¬∑ , M\ncan be written in terms of the trainable weights or network\nparameters (in matrix notation) and activation functions as\nzp[m] = œÉ3(Œ¶3œÉ2(Œ¶2œÉ1(Œ¶1xk,p)))\n(7)\nwhere Œ¶i and œÉi are the network parameters and the activation\nfunction, respectively. The parameters are optimized according\nto the following cost function\nJm =\nmin\nŒ¶1,Œ¶2,Œ¶3\n\f\f\f\fzp[m] ‚àídiag(Fr[m]xk,p\nH)\n\f\f\f\f2\n(8)\nwhich are solved with gradient descent via the backpropaga-\ntion algorithm.\nThe M deep neural networks are trained separately to\nminimize the MSE between the outputs and the labeled data\nas given in (8). After training, for each supervised neural\nnetwork we generate as many output samples as needed in\nresponse to random inputs within the same channel coherence\ninterval, and take their average. The generated output samples\nfor the random inputs do not cost anything other than some\nextra processing, because these inputs are not coming from the\nchannel; rather they are generated randomly in the receiver. To\nbe more precise, each trained deep neural network generates\nsome output samples zi[m] in response to the random inputs\nxk,i. In what follows, the output of each deep neural network\nis obtained as\nÀÜŒªk[m] = 1\nNg\nNg‚àí1\nX\ni=0\nzi[m]\n(9)\nwhere Ng is the total number of arbitrarily generated out-\nput samples. There is no constraint to limit Ng except the\nprocessing complexity, i.e., the zi[m] does not consume any\nbandwidth. Each time the channel changes, the model must be\nretrained with Nt pilots, and Ng randomly generated samples.\nNote that there are many different types of generative model\napplications other than the generative adversarial networks\n(GANs) [18], and hence this is one type of generative models.\nB. Stage-2: Unsupervised Learning\nThe main beneÔ¨Åt of using a second deep learning model is\nto enhance the quality of channel estimates. In this regard, an\nunsupervised deep learning model is utilized. More precisely,\nthe output of each supervised deep learning model that esti-\nmates the channel taps in the frequency domain given in (9)\nis replicated N times in the time domain. This means that\nan Nf √ó N complex frequency-time grid is obtained for each\nuser. Then, this frequency-time grid is stacked for all antennas\nfor m = {1, ¬∑ ¬∑ ¬∑ , M} in the spatial domain. This yields a 3-\ndimensional Nf√óN√óM signal for each user, which is denoted\nas ŒõT.\nThe unsupervised deep learning model processes this 3-\ndimensional signal by Ô¨Åtting the parameters of its deep neural\nnetwork. The overall unsupervised model that depicts the\ninput, output and hidden layers is given in Fig. 3. Here, the\ndeep neural network targets to denoise the signal by Ô¨Åtting the\nsignal more and the noise less. This is possible, because the\nsignal has a structure, whereas the noise is unstructured, i.e.,\nthe noise is independent and identically distributed (i.i.d). In\nparticular, the hidden layers are crafted so as to exploit the\nstructure (or correlations) of the signal. The working principle\nof the model in Fig. 3 is to generate ŒõT by passing a randomly\nchosen input tensor Z0 through hidden layers. Here, Z0 is as\nan input Ô¨Ålled with uniform noise and once it is randomly\ninitialized, it is kept Ô¨Åxed. The weights of the hidden layers are\nalso randomly initialized, but their weights are continuously\nupdated via gradient descent.\nThe key component in the aforementioned generative unsu-\npervised model is the hidden layers. This structure of a hidden\nlayer is portrayed in Fig. 4. Each hidden layer is composed\nof four major components. These are: (i) a 1 √ó 1 convolution,\n(ii) an upsampler, (iii) a ReLU activation function, and (iv)\na batch normalization. A 1 √ó 1 convolution means that each\nreplicated \nsamples\nInput\nHidden Layers\nOutput\nnumber of \nantennas\nnumber of \nsubcarriers\nused \nsamples\nredundant \nsamples\nUSL (Unsupervised Learning) \nFig. 3. The second unsupervised deep learning model whose output gives the\nchannel estimates.\nelement in the time-frequency grid is processed with the same\nparameters through the spatial domain, which changes the\ndimension. There are N (i)\ns\ndifferent kernels, which are shared\nfor each slot in the time-frequency axes. Hence, the spatial\ndimension becomes N (i)\ns . This can be equivalently considered\nas each vector in the time-frequency slot being multiplied with\nthe same (shared) N (i)\ns\n√ó N (i‚àí1)\ns\nmatrix. In what follows,\nupsampling is performed to exploit the couplings among\nneighboring elements in the time and frequency grid. More\nprecisely, the time-frequency signal is upsampled with a factor\nof 2 via a bilinear transformation. Next, the ReLU activation\nfunction is used to make the model more expressive for\nnonlinearities. The last component of a hidden layer performs\nbatch normalization for a batch size of 1 to avoid vanishing\ngradients. All the hidden layers have the same structure except\nfor the last hidden layer, which does not have an upsampler.\nThe mathematical representation of the aforementioned\narchitecture is given next. Accordingly, the tensor ŒõT is\nparameterized for the l + 1 layer as\nÀÜŒõT = fŒ∏l(fŒ∏l‚àí1(¬∑ ¬∑ ¬∑ fŒ∏0(Z0)))\n(10)\nwhere the input Z0 has a dimension of N (0)\nf\n√ó N (0) √ó N (0)\ns\nin the frequency, time and spatial domain, respectively. These\ndimensions are determined according to the number of hidden\nlayers and the output dimension, in which N (l)\ns\n= M, N (l)\nf\n=\nNf, N (l) = N. The layers from 0 to l ‚àí1 are counted as a\nhidden layer, and for i = 0, 1, ¬∑ ¬∑ ¬∑ , l ‚àí2\nfŒ∏i = BatchNorm(ReLU(Upsampler(Œ∏i ‚äõZi))\n(11)\nwhere Zi is the input of the ith hidden layer, Œ∏i are the param-\neters, and ‚äõrepresents the so-called ‚Äúconvolution‚Äù operator,\nwhich actually refers to cross-correlation in signal processing.\nMore precisely, a 1 √ó 1 convolution is utilized as a cross-\ncorrelator, which means that the spatial vector for each element\nof the time-frequency grid is multiplied with the same shared\nparameter matrix to obtain the new spatial vector for the next\nhidden layer. The last hidden layer is\nfŒ∏l‚àí1 = BatchNorm(ReLU(Œ∏l‚àí1 ‚äõZl‚àí1),\n(12)\nand the output layer is\nfŒ∏l = Œ∏l ‚äõZl.\n(13)\nAll the parameters can be represented as\nŒò = (Œ∏0, Œ∏1, ¬∑ ¬∑ ¬∑ , Œ∏l),\n(14)\nwhich are optimized according to the square of l2-norm\nŒò‚àó= arg min\nŒò ||ŒõT ‚àíÀÜŒõT||2\n2.\n(15)\nThe output of the DNN for the optimized parameters is\nŒõ‚àó\nT = fŒò‚àó(Z0),\n(16)\nwhere the channel estimate\nÀÜŒõk\ncorresponds to the 2-\ndimensional Nf √ó M signal at the Ô¨Årst time index in (16).\nNotice that the spatial dimension of Z0 is a hyper-parameter\nand its other dimensions are determined by the output signal\nand the number of layers, since at each layer the time and\nfrequency dimensions are doubled. Furthermore, it is worth\nemphasizing that in the architecture spatial correlations in\nthe signal are captured by the 1 √ó 1 convolution so as\nto decrease the number of parameters, and frequency and\ntemporal correlations are exploited by upsampling.\nIV. NUMERICAL RESULTS AND COMPLEXITY ANALYSIS\nThe performance of the proposed channel estimator is\nevaluated for the following scenario. We assume that there\nare 4 single antenna users and a single base station that has\n16 antennas. Indeed, the number of users are not important as\nlong as K ‚â™16. Users transmit 20 orthogonal OFDM pilot\nsymbols, each of which has 64 subcarriers, over a realistic\n‚Äúextended a pedestrian‚Äù channel model employed in LTE. It\nis also assumed that pilots are sent at the beginning of each\ncoherence time interval. With these settings, the normalized\nmean square error (NMSE) of the proposed channel estimator\nis compared with the other methods that were proposed\nrecently in Fig. 5. As can be seen, our deep learning-based\nchannel estimator provides 5-10 dB gain.\nOne of the main concerns in deep learning models is\nthe overall computational complexity. For our model, the\ncomputational complexity of the Ô¨Årst stage is composed of\ntraining the M neural networks and generating random sam-\nples from the trained neural networks. The former leads to\nthe complexity of MO(W 2) where W = 32N 2\nf is the total\nnumber of adaptive parameters in the neural network, which\nstems from the backpropagation algorithm. The latter phase\nhas relatively less complexity, in particular its complexity\ncomes from matrix-vector multiplication. Hence, the Ô¨Årst\nmodel for channel estimation has a complexity of MO(W 2).\nThe computational complexity of the second stage is much\nless than the Ô¨Årst stage, because there is a convolutional deep\nneural network architecture that yields a small number of\nparameters. In particular, the number of parameters in the\nunsupervised learning model is on the order of hundreds or\nthousands [17]. Hence, the computational complexity of the\nproposed pipeline is dominated by the supervised learning\ntasks.\nInput of the \nhidden layer\n1x1 convolution\nUpsampling\nReLU\nBatch \nNormalization\nùëÅ ($) = 2ùëÅ ($())\nùëÅùëì($) = 2ùëÅùëì($())\nùëÅùëì($())\nùëÅ ($())\nùëÅùë†($())\nùëÅùë†($)\nùëÅùëì($())\nùëÅ ($())\nùëÅùëì($)\nùëÅ ($)\nùëÅùë†($)\nùëÅùë†($)\n   ùëñùë°‚Ñé \nFig. 4. The structure of the ith hidden layer, whose input dimension is N(i‚àí1)\nf\n√ó N(i‚àí1) √ó N(i‚àí1)\ns\nand output dimension is N(i)\nf\n√ó N(i) √ó N(i)\ns . Note\nthat N(i)\nf\n= 2N(i‚àí1)\nf\nand N(i) = 2N(i‚àí1). The spatial dimensions N(i‚àí1)\ns\nand N(i)\ns\nare the hyperparameters that are used by the 1 √ó 1 convolution\noperations.\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nSNR[dB]\n16\n14\n12\n10\n8\n6\n4\n2\nNMSE[dB]\nOur estimator\nThe estimator in [5]\nThe estimator in [4]\nThe estimator in [2]\nFig. 5. The performance comparison of the proposed estimator with state-of-\nthe-art one-bit massive MIMO channel estimators.\nV. CONCLUSIONS\nIn this paper we proposed a novel deep learning-based\nchannel estimator for one-bit massive MIMO in uplink com-\nmunication. Due to the signiÔ¨Åcant information loss in one-\nbit quantization, generative models are leveraged as deep\nlearning architectures so as to exploit some prior information\nin estimating the channel. Our results show that considerable\nprogress can be made with deep generative models for chal-\nlenging channel estimation problems. As a future work, it can\nbe interesting to consider different types of deep generative\nmodels for downlink massive MIMO channel estimation. This\nis much more challenging than uplink, because many more\npilots are required in the downlink channel estimation.\nREFERENCES\n[1] T. L. Marzetta, ‚ÄúNoncooperative cellular wireless with unlimited numbers\nof base station antennas,‚Äù IEEE Trans. Wireless Commun., vol. 9, no. 11,\npp. 3590 - 3600, November 2010.\n[2] C. Risi, D. Persson, and E. G. Larsson, ‚ÄúMassive MIMO with 1-bit ADC‚Äù,\n[Online]. Available: https://arxiv.org/abs/1404.7736, April 2014.\n[3] J. Choi, J. Mo, and R. W. Heath, Jr., ‚ÄúNear maximum-likelihood detector\nand channel estimator for uplink multiuser massive MIMO systems with\none-bit ADCs‚Äù, IEEE Trans. Commun., vol. 64, no. 5, pp. 2005-2018,\nMay 2016.\n[4] C. Mollen, J. Choi, E. G. Larsson, and R. W. Heath Jr., ‚ÄúUplink\nperformance of wideband massive MIMO with one-bit ADCs‚Äù, IEEE\nTransactions on Wireless Communications, vol. 16, no. 1, pp. 87-100,\nJanuary 2017.\n[5] Y. Li, C. Tao, G. Seco-Granados, A. Mezghani, A. L. Swindlehurst, and\nL. Liu, ‚ÄúChannel estimation and performance analysis of one-bit massive\nMIMO systems‚Äù, IEEE Trans. Signal Process., vol. 65, no. 15, pp. 4075-\n4089, August 2017.\n[6] C.-K. Wen, C.-J. Wang, S. Jin, K.-K. Wong, and P. Ting, ‚ÄúBayes-optimal\njoint channel-and-data estimation for massive MIMO with low-precision\nADCs,‚Äù IEEE Transactions on Signal Processing, vol. 64, no. 10, pp.\n2541-2556, May 2016.\n[7] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,\n2016.\n[8] T. O‚ÄôShea and J. Hoydis, ‚ÄúAn introduction to deep learning for the\nphysical layer‚Äù, IEEE Trans. on Cogn. Commun. Netw., vol. 3, no. 4,\npp. 563-575, December 2017.\n[9] S. Dorner, S. Cammerer, J. Hoydis, and S. ten Brink, ‚ÄúDeep learning\nbased communication over the air‚Äù, IEEE J. Sel. Topics Signal Process.,\nvol. 12, no. 1, pp. 132-143, February 2018.\n[10] C.-K. Wen, W. Shih, and S. Jin, ‚ÄúDeep learning for massive MIMO CSI\nfeedback,‚Äù IEEE Wireless Commun. Lett, vol. 7, no. 5, pp. 1-4, October\n2018.\n[11] E. Balevi and J. G. Andrews, ‚ÄúOnline antenna tuning in het-\nerogeneous\ncellular\nnetworks\nwith\ndeep\nreinforcement\nlearning,‚Äù\nIEEE Transactions on Cognitive Communications and Networking,\ndoi:10.1109/TCCN.2019.2933420, 2019.\n[12] H. He, C. K. Wen, J. Shi, and G. Y. Li, ‚ÄúDeep learning-based channel\nestimation for beamspace mmWave massive MIMO systems,‚Äù IEEE\nWireless Commun. Lett, vol. 7, no. 5, pp. 852-855, October 2018.\n[13] Y. Yang, F. Gao, X. Ma, and S. Zhang, ‚ÄúDeep learning-based channel\nestimation for doubly selective fading channels,‚Äù IEEE Access, vol. 7, pp.\n36579-36589, 2019.\n[14] M. Soltani, V. Pourahmadi, A. Mirzaei, and H. Sheikhzadeh, ‚ÄúDeep\nlearning-based channel estimation,‚Äù ArXiv preprint arXiv:1810.05893,\nFebruary 2018.\n[15] E. Balevi and J. G. Andrews, ‚ÄúOne-bit OFDM receivers via deep\nlearning,‚Äù IEEE Trans. on Communications, vol. 67, no. 6, pp. 4326-\n4336, June 2019.\n[16] E. Balevi and J. G. Andrews, ‚ÄúDeep learning-based channel estimation\nfor high-dimensional signals,‚Äù arXiv preprint arXiv:1904.09346, 2019.\n[17] E. Balevi, A. Doshi, and J. G. Andrews, ‚ÄúMassive mimo channel\nestimation with an untrained deep neural network,‚Äù arXiv preprint\narXiv:1908.00144, 2019.\n[18] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, ‚ÄúGenerative adversarial nets‚Äù,\nin Advances in Neural Information Processing Systems, pp. 2672-2680,\nDecember 2014.\n",
  "categories": [
    "eess.SP"
  ],
  "published": "2019-11-27",
  "updated": "2019-11-27"
}