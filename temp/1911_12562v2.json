{
  "id": "http://arxiv.org/abs/1911.12562v2",
  "title": "Towards Security Threats of Deep Learning Systems: A Survey",
  "authors": [
    "Yingzhe He",
    "Guozhu Meng",
    "Kai Chen",
    "Xingbo Hu",
    "Jinwen He"
  ],
  "abstract": "Deep learning has gained tremendous success and great popularity in the past\nfew years. However, deep learning systems are suffering several inherent\nweaknesses, which can threaten the security of learning models. Deep learning's\nwide use further magnifies the impact and consequences. To this end, lots of\nresearch has been conducted with the purpose of exhaustively identifying\nintrinsic weaknesses and subsequently proposing feasible mitigation. Yet few\nare clear about how these weaknesses are incurred and how effective these\nattack approaches are in assaulting deep learning. In order to unveil the\nsecurity weaknesses and aid in the development of a robust deep learning\nsystem, we undertake an investigation on attacks towards deep learning, and\nanalyze these attacks to conclude some findings in multiple views. In\nparticular, we focus on four types of attacks associated with security threats\nof deep learning: model extraction attack, model inversion attack, poisoning\nattack and adversarial attack. For each type of attack, we construct its\nessential workflow as well as adversary capabilities and attack goals. Pivot\nmetrics are devised for comparing the attack approaches, by which we perform\nquantitative and qualitative analyses. From the analysis, we have identified\nsignificant and indispensable factors in an attack vector, e.g., how to reduce\nqueries to target models, what distance should be used for measuring\nperturbation. We shed light on 18 findings covering these approaches' merits\nand demerits, success probability, deployment complexity and prospects.\nMoreover, we discuss other potential security weaknesses and possible\nmitigation which can inspire relevant research in this area.",
  "text": "arXiv:1911.12562v2  [cs.CR]  27 Oct 2020\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n1\nTowards Security Threats of Deep Learning\nSystems: A Survey\nYingzhe He1,2, Guozhu Meng1,2, Kai Chen1,2, Xingbo Hu1,2, Jinwen He1,2\n1Institute of Information Engineering, Chinese Academy of Sciences, China\n2School of Cybersecurity, University of Chinese Academy of Sciences\nAbstract—Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems\nare suffering several inherent weaknesses, which can threaten the security of learning models. Deep learning’s wide use further\nmagniﬁes the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying\nintrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and\nhow effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the\ndevelopment of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these\nattacks to conclude some ﬁndings in multiple views. In particular, we focus on four types of attacks associated with security threats of\ndeep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we\nconstruct its essential workﬂow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack\napproaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identiﬁed signiﬁcant and\nindispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring\nperturbation. We shed light on 18 ﬁndings covering these approaches’ merits and demerits, success probability, deployment complexity\nand prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research\nin this area.\nIndex Terms—deep learning, poisoning attack, adversarial attack, model extraction attack, model inversion attack\n✦\n1\nINTRODUCTION\nD\nEEP learning has gained tremendous success and is the\nmost signiﬁcant driving force for artiﬁcial intelligence\n(AI). It fuels multiple areas including image classiﬁcation,\nspeech recognition, natural language processing, and mal-\nware detection. Due to the great advances in computing\npower and the dramatic increase in data volume, deep\nlearning has exhibited superior potential in these scenarios,\ncompared to traditional techniques. Deep learning excels in\nfeature learning, deepening the understanding of one object,\nand unparalleled prediction ability. In image recognition,\nconvolutional neural networks (CNNs) can classify different\nunknown images for us, and some even perform better\nthan humans. In natural language processing, recurrent neu-\nral networks (RNNs) or long-short-term memory networks\n(LSTMs) can help us translate and summarize text infor-\nmation. Other ﬁelds including autonomous driving, speech\nrecognition, and malware detection all have widespread\napplication of deep learning. The Internet of things (IoT) and\nintelligent home systems have also arisen in recent years. As\nsuch, we are stepping into the era of intelligence.\nHowever,\ndeep\nlearning-based\nintelligent\nsystems\naround us are suffering from a number of security prob-\nlems. Machine learning models could be stolen through\nAPIs [220]. Intelligent voice systems may execute unex-\npected commands [262]. 3D-printing objects could fool real-\nworld image classiﬁers [20]. Moreover, to ensure safety, tech-\nnologies such as autonomous driving need lots of security\ntesting before it can be widely used [217] [271]. In the past\nfew years, the security of deep learning has drawn the atten-\ntion of many relevant researchers and practitioners. They are\nexploring and studying the potential attacks as well as corre-\nsponding defense techniques against deep learning systems\n(DLS). Szegedy et al. [213] pioneered exploring the stability\nof neural networks, and uncovered their fragile properties\nin front of imperceptible perturbations. Since then, adversarial\nattacks have swiftly grown into a buzzing term in both\nartiﬁcial intelligence and security. Many efforts have been\ndedicated to disclosing the vulnerabilities in varying deep\nlearning models (e.g., CNN [175] [156] [155], LSTM [67] [44]\n[177], reinforcement learning (RL) [94], generative adversar-\nial network (GAN) [114] [190]), and meanwhile testing the\nsafety and robustness for DLS [113] [146] [170] [211] [79]\n[250]. On the other hand, the wide commercial deployment\nof DLS raises interest in proprietary asset protection such\nas the training data [162] [181] [272] [10] and model pa-\nrameters [107] [122] [92] [111]. It has started a war where\nprivacy hunters exert corporate espionage to collect privacy\nfrom their rivals and the corresponding defenders conduct\nextensive measures to counteract the attacks.\nPrior works have been conducted to survey security and\nprivacy issues in machine learning and deep learning [13]\n[26] [175] [22]. They enumerate and analyze attacks as well\nas defenses that are relevant to both the training phase and\nprediction phase. However, these works mainly evaluate\nthe attacks either in limited domains (e.g., computer vision)\nor perspectives (e.g., adversarial attack). Few studies can\nprovide a systematical evaluation of these attacks in their\nentire life cycles, which include the general workﬂow, ad-\nversary model, and comprehensive comparisons between\ndifferent approaches. This knowledge can help demystify\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n2\nϬ\nϭϬ\nϮϬ\nϯϬ\nϰϬ\nϱϬ\nϲϬ\nϳϬ\nϴϬ\nϵϬ\nϭϬϬ\nфсϮϬϭϯ ϮϬϭϰ\nϮϬϭϱ\nϮϬϭϲ\nϮϬϭϳ\nϮϬϭϴ\nϮϬϭϵ\nƉŽŝƐŽŶ\nĞǆƚƌĂĐƚŝŽŶ\nŝŶǀĞƌƐŝŽŶ\nĂĚǀĞƌƐĂƌŝĂů\n\u0017\u0011\u001b\b\nϭ͘ϯй\nϱ͘Ϯй\nϳ͘ϴй\nϭϱ͘Ϯй\nϮϱ͘Ϯй\nϰϬ͘ϱй\nϮϬй\nϵй\nϮϰй\nϰϳй\nFig. 1: Publications we surveyed of four attacks and corre-\nsponding defenses in deep learning. The X-axis represents\nthe year, and the Y-axis represents the corresponding num-\nber of publications for every year.\nhow these attacks happen, what capabilities the attackers\npossess, and both salient and tiny differences in attack ef-\nfects. This motivates us to explore a variety of characteristics\nfor the attacks against deep learning. In particular, we aim to\ndissect attacks in a stepwise manner (i.e., how the attacks are\ncarried on progressively), identify the diverse capabilities\nof attackers, evaluate these attacks in terms of deliberate\nmetrics, and distill insights for future research. This study\nis deemed to beneﬁt the community threefold: 1) it presents\na ﬁne-grained description of attack vectors for defenders\nfrom which they can undertake cost-effective measures to\nenhance the security of the target model. 2) the evaluation\non these attacks can unveil some signiﬁcant properties such\nas success rate, capabilities. 3) the insights concluded from\nthe survey can inspire researchers to explore new solutions.\nOur Approach. To gain a comprehensive understanding of\nprivacy and security issues in deep learning, we conduct\nextensive investigations on the relevant literature and sys-\ntems. In total, 245 publications have been studied which are\nmainly spanning across four prevailing areas–image classiﬁ-\ncation, speech recognition, natural language processing and\nmalware detection. Overall, we summarize these attacks\ninto four classes: model extraction attack, model inversion attack,\ndata poisoning attack, and adversarial attack. In particular,\nmodel extraction and inversion attacks are targeting privacy\n(cf. Section 4,5), and data poisoning and adversarial attacks\ncan inﬂuence prediction results by either downgrading the\nformation of deep learning models or creating imperceptible\nperturbations that can deceive the model (cf. Section 6,7).\nFigure 1 shows the publications we surveyed on these\nattacks in the past years. We collect papers from author-\nitative international venues, including artiﬁcial intelligence\ncommunity, such as ICML, CVPR, NIPS, ICCV, ICLR, AAAI,\nIJCAI, ACL, and security community, such as IEEE S&P,\nCCS, USENIX Security, NDSS, TIFS, TDSC, Euro S&P, Asia\nCCS, RAID, and software engineering community, such\nas TSE, ASE, FSE, ICSE, ISSTA. We choose some key-\nwords in the search process, including “security”, “attack”,\n“defense”, “privacy”, “adversarial”, “poison”, “inversion”,\n“inference”, “membership”, “backdoor”, “extract”, “steal”,\n“protect”, “detect”, and their variants. We also pay attention\nto the topics related to machine learning security in these\nvenues. Furthermore, we also survey papers which cite or\nare cited by the foregoing papers, and include them if they\nhave high citations. The number of related publications is\nexperiencing a drastic increase in the past years. In our\nresearch, it gains 94% increase in 2017, 66% increase in 2018,\nand 61% increase in 2019. Adversarial attack is obviously\nthe most intriguing research and occupies around 47% of\nresearchers’ attention based on the papers we collected. It\nis also worth mentioning that there is an ever-increasing\ninterest in model inversion attack recently, which is largely\ncredited to the laborious processing of training data (More\ndiscussions can be found in Section 8).\nIn this study, we ﬁrst introduce the background of deep\nlearning, and summarize relevant risks and commercial\nDLS deployed in the cloud for public. For each type of\nattacks, we systematically study its capabilities, workﬂow\nand attack targets. More speciﬁcally, if one attacker is con-\nfronting a commercial deep learning system, what action it\ncan perform in order to achieve the target, how the system\nis subverted step by step in the investigated approaches,\nand what inﬂuences the attack will make to both users\nand the system owner. In addition, we develop a number\nof metrics to evaluate these approaches such as reducing\nquery strategies, precision of recovered training data, and\ndistance with perturbed images. Based on a quantitative or\nqualitative analysis, we conclude many insights covering\nthe popularity of speciﬁc attack techniques, merits and\ndemerits of these approaches, future trends and so forth.\nTakeaways. According to our investigation, we have drawn\na number of insightful ﬁndings for future research. In black-\nbox settings, attackers usually interact by querying certain\ninputs from the target DLS. How to reduce the number of\nqueries for avoiding the security detection is a signiﬁcant\nconsideration for attackers (cf. Section 4). The substitute\nmodel can be a prerequisite for attacks, because of its sim-\nilar behavior and transferability. Model extraction, model\ninversion and adversarial attacks can all beneﬁt from it (cf.\nSection 4). Data synthesis is a common practice to represent\nsimilar training data. Either generated by the distribution or\nGAN, synthesized data can provide sufﬁcient samples for\ntraining a substitute model (cf. Section 5). A more advanced\nway for poisoning purposes is to implant a backdoor in data\nand then attackers can manipulate the prediction results\nwith crafted input (cf. Section 6). Most adversarial attacks\nhave focused their main efforts on maximizing prediction\nerrors but minimizing “distance”. However, “distance” can\nbe measured in varying fashions and still need to be im-\nproved for better estimations and new applications (cf. Sec-\ntion 7). Moreover, we have discussed more security issues\nfor modern DLS in Section 8, such as ethical considerations,\nsystem security, physical attacks and interpretability. We\nhave investigated some works on deep learning defenses\nand summarized them in terms of attacks (cf. Section 8.6).\nContributions. We make the following contributions.\n• Systematic security analysis of deep learning. We sum-\nmarize 4 types of attacks. For each attack, we construct\ntheir attack vectors and pivot properties, i.e., workﬂow,\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n3\n澳\n澳\n澳\ndƌĂŝŶŝŶŐ\u0003\u0004ůŐŽƌŝƚŚŵ\ndƌĂŝŶŝŶŐ\u0003\u0018ĂƚĂ\nDŽĚĞů\u0003/ŶǀĞƌƐŝŽŶ\n\u0004ƚƚĂĐŬ\nDŽĚĞů\u0003\n\u001cǆƚƌĂĐƚŝŽŶ\u0003\u0004ƚƚĂĐŬ\nWŽŝƐŽŶŝŶŐ\u0003ĂƚƚĂĐŬ\n&ĞĂƚƵƌĞ\u0003sĞĐƚŽƌ\nDŽĚĞů\u0003dƌĂŝŶŝŶŐ\nhƐĞƌ\n\u0004ƚƚĂĐŬĞƌ\nDĂůǁĂƌĞ\u0003\nĚĞƚĞĐƚŝŽŶ\n\u0004ƵƚŽŶŽŵŽƵƐ\u0003\nsĞŚŝĐůĞƐ\n^ƉĞĞĐŚ\u0003\nZĞĐŽŐŶŝƚŝŽŶ\nEĂƚƵƌĂů\u0003>ĂŶŐƵĂŐĞ\u0003\nWƌŽĐĞƐƐŝŶŐ\n\u0004ƉƉůŝĐĂƚŝŽŶƐ\ndƌĂŝŶĞĚ\u0003DŽĚĞů\nWƌĞĚŝĐƚŝŽŶ\u0003/ŶƉƵƚ\nWƌĞĚŝĐƚŝŽŶ\u0003KƵƚƉƵƚ\n\u0004ĚǀĞƌƐĂƌŝĂů\u0003/ŶƉƵƚ\ntƌŽŶŐ\u0003KƵƚƉƵƚ\nDŽĚĞů\u0003WƌĞĚŝĐƚŝŽŶ\n\u0004ĚǀĞƌƐĂƌŝĂů\u0003\u0004ƚƚĂĐŬ\n^ƚĞĂůŝŶŐ\u0003DŽĚĞů\n^ƚĞĂůŝŶŐ\u0003dƌĂŝŶŝŶŐ\u0003\n\u0018ĂƚĂ\u0003/ŶĨŽƌŵĂƚŝŽŶ\nWŽŝƐŽŶĞĚ\u0003\u0018ĂƚĂ\nFig. 2: Deep learning systems and the encountered attacks\nadversary model (it contains attacker’s capabilities and\nlimitations), and attack goal. This could ease the under-\nstanding of how these attacks are executed and facilitate\nthe development of counter measures.\n• Quantitative and qualitative analysis. We develop a\nnumber of metrics that are pertinent to each type of\nattacks, for a better assessment of different approaches.\nThese metrics also serve as highlights in the development\nof attack approaches that facilitate more robust attacks.\n• New ﬁndings. Based on the analysis, we have concluded\n18 ﬁndings that span the four attacks, and uncover im-\nplicit properties for these attack methods. Our ﬁndings\nsummarize some results and analyze phenomena based\non existing surveyed work, and predict the possible future\ndirection of the ﬁeld based on the summary results. All\nﬁndings include quantitative or qualitative analysis. Be-\nyond these attacks, we have discussed other related secu-\nrity problems in Section 8 such as secure implementation,\ninterpretability, discrimination and defense techniques.\n2\nRELATED WORK\nThere is a line of works that survey and evaluate attacks\ntoward machine learning or deep learning.\nBarreno et al. conduct a survey of machine learning\nsecurity and present a taxonomy of attacks against machine\nlearning systems [26]. They experiment on a popular statis-\ntical spam ﬁlter to illustrate their effectiveness. Attacks are\ndissected in terms of three dimensions, including workable\nmanners, inﬂuence to input and generality. Amodei et al.\n[17] introduce ﬁve possible research problems related to\naccident risk and discuss probable approaches, with an\nexample of how a cleaning robot works. Papernot et al. [176]\nstudy the security and privacy of machine learning. They\nsummarize some attack and defense methods, and propose\na threat model for machine learning. It introduces attack\nmethods in training and inferring process, black-box and\nwhite-box model. However, they do not include much in-\nformation about defenses or the most widely used deep\nlearning models.\nBae et al. [22] review the attack and defense methods\nunder security and privacy AI concept. They inspect eva-\nsion and poisoning attacks, in black-box and white-box. In\naddition, their study focuses on privacy with no mention of\nother attack types.\nLiu et al. [138] aim to provide a literature review in\ntwo phases of machine learning, i.e., the training phase\nand the testing/inferring phase. As for the corresponding\ndefenses, they sum up with four categories. In addition, this\nsurvey focuses more on data distribution drifting caused\nby adversarial samples and sensitive information violation\nproblems in statistical machine learning algorithms.\nAkhtar et al. [13] conduct a study on adversarial attacks\nof deep learning in computer vision. They summarize 12\nattack methods for classiﬁcation, and study attacks on mod-\nels or algorithms such as autoencoders, generative models,\nRNNs and so on. They also study attacks in the real world\nand summarize defenses. However, they only research the\ncomputer vision part of adversarial attack.\nHuang et al. [96] research the safety and trustworthiness\non the deployment of DNNs. They address the trustwor-\nthiness within a certiﬁcation process and an explanation\nprocess. In certiﬁcation, they study DNN veriﬁcation and\ntesting techniques, and in explanation, they consider DNN\ninterpretability problems. Adversarial attack and defense\ntechniques go through the whole procedure. Different from\nus, their security considerations pay more attention to en-\nsure trustworthiness during the DNN deployment process.\nZhang et al. [270] summarize and analyze machine learn-\ning testing techniques. Testing can expose problems and\nimprove the trustworthiness of machine learning systems.\nTheir survey covers testing properties (such as correctness,\nrobustness, fairness), testing components (such as data,\nlearning program, framework), testing workﬂow (such as\ntest generation, test evaluation), and application scenarios\n(such as autonomous driving, machine translation). Unlike\nus, their focus on safety is from a testing perspective.\n3\nOVERVIEW\n3.1\nDeep Learning System\nDeep learning is inspired by biological nervous systems\nand is composed of thousands of neurons to transfer in-\nformation. Figure 2 demonstrates a classic deep learning\nprocess. Typically, it exhibits to the public an overall process\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n4\nTABLE 1: Notations used in this paper\nNotation\nExplanation\nD\ndataset\nx = {x1, . . . , xn}\ninputs in D\ny = {y1, . . . , yn}\npredicted labels of x\nyt = {y1\nt , . . . , yn\nt }\ntrue labels of x\n||x −y||2\nthe Euclidean distance for x and y\nF\nmodel function\nZ\noutput of second-to-last layer\nL\nloss function\nw\nweights of parameters\nb\nbias of parameters\nλ\nhyperparameters\nLp\ndistance measurement\nδ\nperturbation to input x\nincluding: 1) Model Training, where it converts a large vol-\nume of data into a trained model, and 2) Model Prediction,\nwhere the model can be used for prediction as per input\ndata. Prediction tasks are widely used in different ﬁelds. For\ninstance, image classiﬁcation, speech recognition, natural\nlanguage processing and malware detection are all pertinent\napplications for deep learning.\nTo formalize the process of deep learning systems, we\npresent some notations in Table 1. Given a learning task, the\ntraining data can be represented as (x, yt) ∈D. Let F be\nthe deep learning model and it computes the corresponding\noutcomes y based on the given input x, i.e., y = F(x). yt is\nthe true label of input x. Within the course of model training,\nthere is a loss function L to measure the prediction error\nbetween predicted result and true label, and the training\nprocess intends to gain a minimal error value via ﬁne-tuning\nparameters. There exist many loss functions to measure\nthe differences. One commonly used loss function can be\ncomputed as L = Σ1⩽i⩽n||yi\nt −yi||2. So the process of\nmodel training can be formalized as [183]:\narg min\nF\nX\n1⩽i⩽n\n||yi\nt −yi||2\n(1)\n3.2\nRisks in Deep Learning\nOne deep learning system involves several pivotal assets\nthat are conﬁdential and signiﬁcant for the owner. As per\nthe phases in Figure 2, risks stem from three types of con-\ncerned assets in deep learning systems: 1) training dataset.\n2) trained model including model structures, and model\nparameters. 3) inputs and results of predictions.\n1 Training dataset. High-quality training data is signiﬁcant\nand vital for a better performance of the deep learning\nmodel. As a deep learning system has to absorb plenty of\ndata to form a qualiﬁed model, mislabelled or inferior data\ncan hinder this formation and affect the model’s quality.\nThese kinds of data can be intentionally appended to the\nbenign data by attackers, which is referred to as poisoning\nattack (cf. Section 6). On the other hand, the collection of\ntraining data takes lots of human resources and time costs.\nIndustry giants such as Google have far more data than\nother companies. They are more inclined to share their state-\nof-the-art algorithms [106] [55], but they barely share data.\nTherefore, training data is crucial and considerably valuable\nfor a company, and its leakage means big loss of assets.\nHowever, recent research found there is an inverse ﬂow\nfrom prediction results to training data [221]. It leads that\none attacker can infer out the conﬁdential information in\ntraining data, merely relying on authorized access to the\nvictim system. It is literally noted as model inversion attack\nwhose goal is to uncover the composition of the training\ndata or its speciﬁc properties (cf. Section 5).\n2 Trained model. The trained model is an abstract repre-\nsentation of its training data. Modern deep learning systems\nhave to cope with a large volume of data in the training\nphase, which has a rigorous demand for high performance\ncomputing and mass storage. Therefore, the trained model\nis regarded as the core competitiveness for a deep learn-\ning system, endowed with commercial value and creative\nachievements. Once it is cloned, leaked or extracted, the\ninterests of model owners will be seriously damaged. More\nspeciﬁcally, attackers have started to steal model parame-\nters [220], functionality [167] or decision boundaries [174],\nwhich are collectively known as model extraction attack (cf.\nSection 4).\n3 Inputs and results of predictions. As for prediction data\nand results, curious service providers may retain user’s\nprediction data and results to extract sensitive information.\nThese data may also be attacked by miscreants who intend\nto utilize these data to make their own proﬁts. On the\nother hand, attackers may submit carefully modiﬁed input\nto fool models, which is dubbed adversarial example [213].\nAn adversarial example is crafted by inserting slight pertur-\nbations into the original normal sample which are not easy\nto perceive. This is recognized as adversarial attack or evasion\nattack (cf. Section 7).\n3.3\nCommercial Off-The-Shelf\nMachine learning as a Service (MLaaS) has gained momen-\ntum in recent years [130], and lets its clients beneﬁt from\nmachine learning without establishing their own predictive\nmodels. To ease the usage, the MLaaS suppliers make a\nnumber of APIs for clients to accomplish machine learning\ntasks, e.g., classifying an image, recognizing a slice of audio\nor identifying the intent of a passage. Certainly, these ser-\nvices are the core competence which also charge clients for\ntheir queries. Table 2 shows representative COTS as well\nas their functionalities, outputs to the clients, and usage\ncharges. Taking Amazon Image Recognition for example, it\ncan recognize the person in a proﬁle photo and tell his/her\ngender, age range, emotions. Amazon charges this service at\n1,300 USD per one million queries.\n4\nMODEL EXTRACTION ATTACK\n4.1\nIntroduction of Model Extraction Attack\nModel extraction attack attempts to duplicate a machine\nlearning model through the provided APIs, without prior\nknowledge of training data and algorithms [220]. To for-\nmalize, given a speciﬁcally selected input x, one attacker\nqueries the target model F and obtains the corresponding\nprediction results y. Then the attacker can infer or even\nextract the entire in-use model F. With regard to an arti-\nﬁcial neural network y = wx + b, model extraction attack\ncan somehow approximate the values of w and b. Model\nextraction attacks cannot only destroy the conﬁdentiality\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n5\nTABLE 2: Commercial MLaaS systems and the provided functionalities, output for clients and charges per 1M queries\nSystem\nFunctionality\nOutput\nCost/M-times\nAlibaba Image Recognition\nImage marking\nlabel, conﬁdence\n2500 CNY\nscene recognition\nlabel, conﬁdence\n1500 CNY\nporn identiﬁcation\nlabel, suggestion\n1620 CNY\nAmazon Image Recognition\nObject & Scene Recognition\nlabel, boundingbox, conﬁdence\n1300 USD\nface recognition\nAgeRange, boundingbox, emotions, eyeglasses, gender, pose, etc\n1300 USD\nGoogle Vision API\nlabel description\ndescription, score\n1500 USD\n'HHS\u0003/HDUQLQJ\u00036\\VWHPV\n\u0014\u0011\u00034XHU\\LQJ\n\u0016\u0011\u0003([WUDFWLQJ\u0003\n,QIRUPDWLRQ\n3DUDPHWHUV\n,QSXW\nSUHGLFWLRQ\n+\\SHUSDUDPHWHU\n$UFKLWHFWXUHV\n'HFLVLRQ\u0003\n%RXQGDULHV\n)XQFWLRQDOLW\\\n\u0015\u0011\u0003$SSURDFKHV\n(TXDWLRQ\u0003\n6ROYLQJ\n7UDLQLQJ\u0003\n0HWDPRGHO\n7UDLQLQJ\u0003\n6XEVWLWXWH\u0003\n0RGHO\nFig. 3: Workﬂow of model extraction attack\nof a model, and damage the interests of its owners, but\nalso construct a near-equivalent white-box model for further\nattacks such as adversarial attack [174].\nAdversary Model. This attack is mostly carried out un-\nder a black-box setting and attackers only have access to\nprediction APIs. The attacker can use an input sample to\nquery the target model, and obtain the output including\nboth predicted label and class probability vector. Their capa-\nbilities are limited in three ways: model knowledge, dataset\naccess, and query frequency. Attackers have no idea about\nmodel architectures, hyperparameters, training process of\nthe victim’s model. They cannot obtain natural data with\nthe same distribution of the target model training data. In\naddition, attackers may be blocked by API if submitting\nqueries too frequently.\nWorkﬂow. Figure 3 shows a typical workﬂow of this attack.\nFirst, attackers submit inputs to the target model and get\nprediction values. Then they use input-output pairs and\ndifferent approaches to extract the conﬁdential data. More\nspeciﬁcally, conﬁdential data includes parameters [220], hy-\nperparameters [226], architectures [164], decision bound-\naries [174] [107], and functionality [167] [54].\n4.2\nApproaches for Extracting Models\nThere are basically three types of approaches to extract\nmodels:\n• Equation Solving (ES). For a classiﬁcation model com-\nputing class probabilities as a continuous function, it can\nbe denoted as F(x) = σ(w · x + b) [220]. Hence, given\nsufﬁcient samples (x, F(x)), attackers can recover the\nparameters (e.g., w, b) by solving the equation w · x + b =\nσ−1(F(x)).\n• Training Metamodel (MM). Metamodel is a classiﬁer for\nclassiﬁcation models [164]. By querying a classiﬁcation\nmodel on the outputs y for certain inputs x, attackers train\na meta-model F m, mapping y to x, i.e., x = F m(y). The\ntrained model can further predict model attributes from\nthe query outputs y.\n• Training Substitute Model (SM). Substitute model is\na simulative model mimicking behaviors of the origi-\nnal model. With sufﬁcient querying inputs x and corre-\nsponding outputs y, attackers train the model F s where\ny = F s(x). As a result, the attributes of the substitute\nmodel can be near-equivalent to those of the original.\nStealing different information corresponds to different\nmethods. In terms of time, equation solving is earlier than\ntraining meta- and substitute models. It can restore precise\nparameters but is only suitable for small scale models.\nDue to the increase of model size, it is common to train a\nsubstitute model to simulate the original model’s decision\nboundaries or classiﬁcation functionalities. However, pre-\ncise parameters seem less important. Metamodel [164] is an\ninverse training with substitute model, as it takes the query\noutputs as input and predicts the query inputs as well as\nmodel attributes. Besides, it can be also used to explore more\ninformative inputs that help infer more internal information\nof the model.\n4.3\nDifferent Extracted Information\n4.3.1\nModel Parameters & Hyperparameters\nParameters are variables that the model can learn automati-\ncally from the data, such as weights and bias. Hyperparam-\neters are speciﬁc parameters whose values are set before\nthe training process, including dropout rate, learning rate,\nmini-batch size, parameters in objective functions to balance\nloss function and regularization terms, and so on.\nIn the\nearly work, Tram`er et al. [220] tried equation solving to\nrecover parameters in machine learning models, such as\nlogistic regression, SVM, and MLP. They built equations\nabout the model by querying APIs, and obtained parameters\nby solving equations. However, it needs plenty of queries\nand is not applicable to DNN. Wang et al. [226] tried to\nsteal hyperparameter-λ on the premise of known model\nalgorithm and training data. λ is used to balance loss\nfunctions and regularization terms. They assumed that the\ngradient of the objective function is ⃗0 and thus got many\nlinear equations through many queries. They estimated the\nhyperparameters through linear least square method.\n4.3.2\nModel Architectures\nArchitectural details include the number of layers in the\nmodel, the number of neurons in each layer, how are they\nconnected, what activation functions are used, and so on.\nRecent papers usually train classiﬁers to predict attributes.\nJoon et al. [164] trained metamodel, a supervised classiﬁer of\nclassiﬁers, to steal model attributes (architecture, operation\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n6\nTABLE 3: Evaluation on model extraction attacks as per stolen information. We sort them by the stolen “Information”,\ncorresponding to Section 4.3. “Approach” is the attack method, corresponding to Section 4.2. “Reducing Query” is the\ntechnique for reducing query number in this attack. “Recovery Rate” is the accuracy of extracted information. “SVM” is\nsupport vector machine. “DT” is decision tree. “LR” is logistic regression. “kNN” is K-nearest neighbor. “Queries” is the\nnumber of required queries for an attack.\nInformation\nPaper\nApproach\nReducing Query\nRecovery Rate (%) for Models\nQueries\nSVM\nDT\nLR\nkNN\nCNN\nDNN\nParameter\nTramer et al. [220]\nES\n-\n99\n99\n99\n-\n-\n90\n108,200\nHyperparameter\nWang et al. [226]\nES\n-\n99\n-\n99\n-\n-\n-\n200\nArchitecture\nJoon et al. [164]\nMM\nKENNEN-IO\n-\n-\n-\n-\n-\n88\n500\nDecision Boundary\nPapernot et al. [174]\nSM\nreservoir sampling [223]\n-\n-\n-\n-\n-\n84\n800\nPapernot et al. [173]\nSM\nreservoir sampling [223]\n83\n61\n89\n85\n-\n89\n800\nPRADA [107]\nSM\n-\n-\n-\n-\n-\n-\n91\n300\nFunctionality\nSilva et al. [54]\nSM\n-\n-\n-\n-\n-\n98\n-\n-\nOrekondy et al. [167]\nSM\nrandom, adaptive sampling\n-\n-\n-\n-\n98\n-\n60,000\ntime, and training data size). They submitted query inputs\nvia APIs, and took corresponding outputs as inputs of meta-\nmodel, then trained metamodel to predict model attributes\nas outputs.\n4.3.3\nModel Decision Boundaries\nDecision boundaries are the classiﬁcation boundary be-\ntween different classes. They are important for generating\nadversarial examples. In [174] [107] [173], they steal decision\nboundaries and generate transferable adversarial samples to\nattack a black box model. Papernot et al. [174] used Jacobian-\nbased Dataset Augmentation (JbDA) to produce synthetic\nsamples, which moved to the nearest boundary between\nthe current class and all other classes. This technology aims\nnot to maximize the accuracy of substitute models, but en-\nsures that samples arrive at decision boundaries with small\nqueries. Juuti et al. [107] extended JbDA to Jb-topk, where\nsamples move to the nearest k boundaries between current\nclass and any other class. They produced transferable tar-\ngeted adversarial samples rather than untargeted [174]. In\nterms of model knowledge, Papernot et al. [173] found that\nmodel architecture knowledge was unnecessary because a\nsimple model could be extracted by a more complex model,\nsuch as a DNN.\n4.3.4\nModel Functionalities\nSimilar functionalities refer to replicating the original model\nas much as possible on prediction results. The primary goal\nis to construct a predictive model that has closest input-\noutput pairs with the original. In [167] [54], they try to\nimprove the classiﬁcation accuracy of a substitute model.\nSilva et al. [54] used a problem domain dataset, non-problem\ndomain dataset, and their mixture to train a model respec-\ntively. They found the model trained with a non-problem\ndomain dataset also did well in accuracy. Besides, Orekondy\net al. [167] assumed attackers had no semantic knowledge\nover model outputs. They chose very large datasets and\nselected suitable samples one by one to query the black-box\nmodel. A reinforcement learning approach was introduced\nto improve query efﬁciency and reduce query counts.\n4.4\nAnalysis of Model Extraction Attack\nModel extraction attack is an emerging ﬁeld of attack. In\nthis study, we survey 8 related papers and classify them\nby extracted information as shown in Table 3. Based on the\nstatistics, we draw the following conclusions.\nFinding 1. Training the substitute model (SM) is the dominant\nmethod in model extraction attacks with manifold advantages.\nThe ES approach needs more than 100 thousand queries\nto attack a DNN model, while the SM method only needs\nhundreds of queries, and it can attack a more complex CNN\nnetwork. Equation solving is deemed as an efﬁcient way\nto recover parameters [220] or hyperparameters [226] in\nlinear algorithms, since it has an upper bound for sufﬁcient\nqueries. However, the ES approach is hardly applicable\nto the non-linear deep learning models. Attacking DNN\nrequires a huge amount of queries (108,200 in [220]). So\nresearchers turn to the compelling training-based approach.\nFor instance, [164] trains a classiﬁer based on a target model,\ndubbed as metamodel, to predict structure information. This\napproach cannot cope with complex model attributes such\nas decision boundary and functionality. That drives the\nprevalence of substitute models (SM) which serve as an in-\ncarnation of the target model which behaves quite similarly.\nAs such, the substitute model has approximated attributes\nand prediction results. The SM approach only needs 300\nqueries to attack DNN in [107]. For a more complex CNN,\nSM needs 60,000 queries in [167]. This shows that attacking\nmore complex models requires more queries. Besides, it can\nbe further used to steal model’s training data [107] and\ngenerating adversarial examples [173].\nFinding 2. Reducing queries, which can save monetary costs for\na pay-per-query MLaaS commercial system and also be resistant\nto attack detection, has become an intriguing research direction in\nrecent years.\nThe requirement of query reduction arises due to the\nhigh expense of queries and query amount limitation. In our\ninvestigated papers, [164] trains a metamodel–KENNEN-\nIO for optimizing the query inputs. [174], leverage reservoir\nsampling to select representative samples for querying, and\n[167] proposes two sampling strategies, i.e., random and\nadaptive to reduce queries. Moreover, active learning [126],\nnatural evolutionary strategies [99], optimization-based ap-\nproaches [50] [193] have been adopted for query reduction.\nFinding 3. Model extraction attack is evolving from a puzzle\nsolving game to a simulation game with cost-proﬁt tradeoffs.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n7\nLQSXW\n$WWDFN\u0003\n0RGHO\u0003\n0HWKRG\n7DUJHW\u00030RGHO\n\u0014\u0011\u0003'DWD\u0003\n6\\QWKHVLV\n&ODVV\u0003ODEHO\u000f\n&ODVV\u0003SURE\u0011\u000f\nPHPEHUVKLS\n\u0016\u0011\u0003$WWDFN\u00030RGHO\n&ODVV\u0003ODEHO\u000f\n&ODVV\u0003SURE\u0011\n+HXULVWLF\u0003\n0HWKRG\n\u0017\u0011\u0003'HWHUPLQDWLRQ\n7DUJHW\u0003\n0RGHO\n\u0015\u0011\u00036KDGRZ\u0003\n0RGHO\n7UDLQLQJ\n\u0014\u0011\u0003'DWD\u0003\n6\\QWKHVLV\n&ODVV\u0003ODEHO\u000f\n&ODVV\u0003SURE\u0011\u000f\nPHPEHUVKLS\n7UDLQLQJ\n7UDLQLQJ\n6LPLODU\u0003\n'DWDVHW\n0HPEHUVKLS\u0003,QIHUHQFH\nLQSXW\nLQSXW\n$SSURDFK\u0003\u0016\n$SSURDFK\u0003\u0015\n$SSURDFK\u0003\u0014\nFig. 4: Workﬂow of model inversion attack\nMLaaS magnates like Amazon and Google have a\ntremendous scale of networks running behind services. It\ncosts much to infer how many layers or neurons are in the\nneural networks. Therefore, it makes a remarkable dent in\nattackers’ interest of solving model attributes. On the other\nhand, inferring decision boundary and model functionality\nemerge as new circumvention. Treating the target model as a\nblack box, attackers observe the response by feeding it with\ncrafted inputs, and ﬁnally construct a close approximation.\nAlthough the substitute model is likely simpler and under-\nperforms in some cases, its prediction capabilities still make\nproﬁts for attackers.\n5\nMODEL INVERSION ATTACK\n5.1\nIntroduction of Model Inversion Attack\nIn a typical model training process, lots of information\nis extracted and abstracted from the training data to the\nproduct model. However, there also exists one inverse in-\nformation ﬂow which allows attackers to infer the training\ndata from the model since neural networks may remember\ntoo much information of the training data [205]. Model\ninversion attack leverages this information ﬂow and restores\ndata memberships or data properties, such as faces in face\nrecognition systems through model prediction or its conﬁ-\ndence coefﬁcient. Model inversion can also be used to form\nphysical watermarking to detect a replay attack [192].\nAdditionally, model inversion attack can be further re-\nﬁned into membership inference attack (MIA) and property\ninference attack (PIA). We distinguish them based on whether\nthe attacker obtains individual information (MIA) or statis-\ntical information (PIA). In MIA, the attacker can determine\nwhether a speciﬁc record is included or not in the training\ndata. In PIA, the attacker can speculate whether there is a\ncertain statistical property in the training dataset.\nAdversary Model. Model inversion attack can be executed\nin both black-box or white-box settings. In a white-box\nattack, the parameters and architecture of the target model\nare known by attackers. Hence, they can easily obtain a sub-\nstitute model that behaves similarly, even without querying\nthe model. In a black-box attack, attacker’s capabilities are\nlimited in model architectures, statistics and distribution\nof training data and so on. Attackers cannot obtain com-\nplete training set information. However, in either setting,\nattackers can make queries with speciﬁc inputs and get\ncorresponding outputs as well as conﬁdence values.\nWorkﬂow. Figure 4 shows a workﬂow of model inversion\nattack which is suitable for both MIA and PIA. Here we take\nMIA as an example. MIA can be accomplished in varying\nways: by querying the target model to get input-output\npairs, attackers can merely exercise Step 4 with heuristic\nmethods to determine the membership of a record [198]\n[144] [89] [137] (Approach 1); Alternatively, attackers can\ntrain an attack model for determination, which necessitates\nan attack model training process (Step 3). Attack model’s\ntraining data is obtained by query inputs and response [184]\n[19] (Approach 2); Due to the limitation of queries and\nmodel attributes, some studies introduce shadow models\n(see Section 5.2.2 in detail) to provide training data for\nthe attack model [198], [203], which necessitates shadow\nmodel training (Step 2). Moreover, data synthesis (Step 1)\nis proposed to provide more training data for a sufﬁcient\ntraining (Approach 3).\n5.2\nMembership Inference Attack\nTruex et al. [221] presented a generally systematic formula-\ntion of MIA. Given an instance x and black-box access to\nthe classiﬁcation model Ft trained on the dataset D, can\nan adversary infer whether the instance x is included in D\nwhen training Ft with a high degree of conﬁdence?\nMost of MIAs proceed in accordance with the workﬂow\nin Figure 4. More speciﬁcally, to infer whether one data\nitem or property exists in the training set, the attacker\nmay prepare the initial data and make transformations to\nthe data. Subsequently, it devises a number of principles\nfor determining the correction of its guessing. This attack\ndestroys information privacy. The privacy protection terms\nused in related articles are explained in detail in Section 8.6.\n5.2.1\nStep 1: Data Synthesis\nInitial data has to be collected as prerequisites for deter-\nmining the membership. According to our investigation,\nan approximated set of training data is desired to imply\nmembership. This set can be obtained either by:\n• Generating samples manually. This method needs some\nprior\nknowledge\nto\ngenerate\ndata.\nFor\ninstance,\nShokri [203] produced datasets similar to the target train-\ning dataset and used the same MLaaS to train several\nshadow models. These datasets were produced by model-\nbased synthesis, statistics-based synthesis, noisy real data\nand other methods. If the attacker has access to part\nof the dataset, then he can generate noisy real data by\nﬂipping a few randomly selected features on real data.\nThese data make up the noisy dataset. If the attacker has\nsome statistical information about the dataset, such as\nmarginal distributions of different features, then he can\ngenerate statistics-based synthesis using this knowledge.\nIf the attacker has no knowledge above, he can also\ngenerate model-based synthesis by searching for possible\ndata records. The records the search algorithm needs to\nﬁnd are correctly classiﬁed by the target model with high\nconﬁdence.\nIn [198], they proposed a data transferring attack with-\nout any query to the target model. They chose different\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n8\ndatasets to train the shadow model. The shadow model\nwas used to capture membership status of data points in\ndatasets.\n• Generating samples by model. This method aims to produce\ntraining records by training generated models such as\nGAN. Generated samples are similar to that from the\ntarget training dataset. Improving the similarity ratio will\nmake this method more useful.\nBoth [137] and [86] attacked generated models. Liu et\nal. [137] presented a new white-box method for single\nmembership attacks and co-membership attacks. The ba-\nsic idea was to train a generated model with the target\nmodel, which took the output of the target model as input,\nand took the similar input of the target model as output.\nAfter training, the attack model could generate data that\nis similar to the target training dataset. Considering about\nthe difﬁcult implementation of CNN in [203], Hitaj et al.\n[89] proposed a more general MIA method. They per-\nformed a white-box attack in the scenario of collaborative\ndeep learning models. They constructed a generator for\nthe target classiﬁcation model, and used it to form a GAN.\nAfter training, the GAN could generate data similar to the\ntarget training set. However, this method was limited in\nthat all samples belonging to the same classiﬁcation need\nto be visually similar, and it could not generate an actual\ntarget training pattern or distinguish them under the same\nclass. Through analyzing a black-box model before and\nafter being updated, Salem et al. [197] proposed a hybrid\ngenerative model to steal information of the updated\ndataset.\n5.2.2\nStep 2: Shadow Model Training\nAttackers have sometimes to transform the initial data for\nfurther determination. In particular, shadow model is pro-\nposed to imitate target model’s behavior by training on\na similar dataset [203]. The dataset takes records by data\nsynthesis as inputs, and their labels as outputs. Shadow\nmodel is trained on such a dataset. It can provide class prob-\nability vector and classiﬁcation result of a record. Shokri et\nal. [203] implement the ﬁrst MIA attack method for a black-\nbox model by API calls in machine learning. They produced\ndatasets similar to the target training dataset and used the\nsame MLaaS to train several shadow models. These datasets\nwere produced by model-based synthesis, statistics-based\nsynthesis, noisy real data and other methods. Shadow mod-\nels were used to provide training set (class labels, prediction\nprobabilities and whether data record belongs to shadow\ntraining set) for the attack model. Salem et al. [198] re-\nlax the constraints in [203] (need to train shadow models\non the same MLaaS, and the same distribution between\ndatasets of shadow models and target model), and use\nonly one shadow model without the knowledge of target\nmodel structure and training dataset distribution. Here,\nthe shadow model just captures the membership status of\nrecords in a different dataset.\n5.2.3\nStep 3: Attack Model Training\nThe attack model is a binary classiﬁer. Its input is the class\nprobabilities and label of the record to be judged, and output\nis yes (means the record belongs to the dataset of target\nmodel) or no. Training dataset is usually required to train\nthe attack model. The problem is that the output label of\nwhether a record belongs to the dataset of target model\ncannot be obtained. So here attackers often generate sub-\nstituted dataset by data synthesis. The input of this training\nis generated either by the shadow model (Approach 3) [203]\n[198] or the target model (Approach 2) [184] [153]. The attack\nmodel training process ﬁrst selects some records from both\ninside and outside the substituted dataset, and then obtains\nthe class probability vector through target model or shadow\nmodel. The vector and the label of record are taken as input,\nand whether this record belongs to substituted dataset is\ntaken as output.\nFor a model F and its training dataset D, training attack\nmodel needs information of label x, F(x), and whether\nx ∈D. If using a shadow model, shadow model F and its\ndataset D are known. All information is from shadow model\nand corresponding dataset. If using the target model, F is\nthe target model and D is the training dataset. However,\nattackers do not know D. So information whether x ∈D\nneed to be replaced by whether x ∈D′, where D′ is similar\nto D.\n5.2.4\nStep 4: Membership Determination\nGiven one input, this component is responsible for de-\ntermining whether the query input is a member of the\ntraining set of the target system. To accomplish the goal,\nthe contemporary approaches can be categorized into two\nclasses:\n• Attack model-based Method. In inference phase, attackers\nﬁrst put the record to be judged into the target model,\nand get its class probability vector, then put the vector and\nlabel of record into the attack model, and get the member-\nship of this record. Pyrgelis et al. [184] implemented MIA\nfor aggregating location data. The main idea was to use\npriori position information and attack through a distin-\nguishability game process with a distinguishing function.\nThey trained a classiﬁer (attack model) as distinguishing\nfunction to determine whether data is in target dataset.\nYang et al. [256] leverage the background knowledge to\nform an auxiliary set to train the attack model, without\naccess to the original training data. Nasr et al. [161] imple-\nment a white-box MIA on both centralized and federated\nlearning. They take all gradients and outputs of each layer\nas the attack features. All these features are used to train\nthe attack model.\n• Heuristic Method. This method uses prediction probability,\ninstead of an attack model, to determine the membership.\nIntuitively, the maximum value in class probabilities of\na record in the target dataset is usually greater than the\nrecord not in it. But they require some preconditions\nand auxiliary information to obtain reliable probability\nvectors or binary results, which is a limitation to apply\nto more general scenarios. How to lower attack cost and\nreduce auxiliary information can be considered in the\nfuture study. Fredrikson et al. [65] construct the probability\nof whether a certain data appears in the target training\ndataset. Then they searched for input data with maximum\nprobability, which is similar to the target training set. The\nthird attack method in Salem et al. [198] only required the\nprobability vector of outputs from the target model, and\nused statistical measurement method to compare whether\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n9\nthe maximum classiﬁcation probability exceeds a certain\nvalue.\nLong et al. [144] put forward a generalized MIA method,\nwhich was easier to attack non-overﬁtted data, different\nfrom [203]. They trained a number of reference models\nsimilar to the target model, and chose vulnerable data\naccording to the output of reference models before Soft-\nmax, then compared outputs between the target model\nand reference models to calculate the probability of data\nbelonging to the target training dataset. Reference models\nin this paper were used to mimic the target model, like\nshadow models. But they did not need an attack model.\nHayes et al. [86] proposed a method of attacking generated\nmodels. The idea was that attackers determined which\ndataset from attackers belonged to the target training set,\naccording to the probability vector output by classiﬁer.\nHigher probability was more likely from the target train-\ning set (they selected the upper n sizes). In white-box,\nthe classiﬁer was constructed by that of target model.\nIn black-box, they used obtained data by querying target\nmodel to reproduce classiﬁer with GAN.\nHagestedt et al. [81] propose an MIA tailored to DNA\nmethylation data, which may cause severe consequences.\nThis attack relies on the likelihood ratio test and prob-\nability estimation to judge membership. Sablayrolles et\nal. [196] assume attackers know the loss incurred by the\ncorrect label in black-box settings. They use a proba-\nbilistic framework including Bayesian learning and noisy\ntraining to analyze membership. They ﬁnd the optimal\ninference only depends on the loss function, not on the\nparameters. He et al. [88] extend model inversion attack\ninto collaborative inference system. They ﬁnd that one in-\ntermediate participant can recover an arbitrary input sam-\nple. They recover inference data by adopting regularized\nmaximum likelihood estimation technique under white-\nbox setting, inverse-network technique under black-box\nsetting.\n5.3\nProperty Inference Attack\nProperty inference attack (PIA) mainly deduces properties\nin the training dataset. For instance, how many people have\nlong hair or wear dresses in a generic gender classiﬁer.\nAre there enough women or minorities in the dataset of\ncommon classiﬁers. The approach is largely the same for\na membership inference attack. In this section, we only\nremark main differences between model inversion attacks.\nData Synthesis. In PIA, training datasets are classiﬁed by\nincluding or not including a speciﬁc attribute [19].\nShadow Model Training. In PIA, shadow models are\ntrained by training sets with or without a certain property. In\n[19] [66], they used several training datasets with or without\na certain property, then built corresponding shadow models\nto provide training data for a meta-classiﬁer.\nAttack Model Training. Here, attack model is usually also\na binary classiﬁer. Ateniese et al. [19] proposed a white-\nbox PIA method by training a meta-classiﬁer. It took model\nfeatures as input, and output whether the corresponding\ndataset contained a certain property. However, this ap-\nproach did not work well on DNNs. To address this, Ganju\net al. [66] mainly studied how to extract feature values\nof DNNs. The part of meta-classiﬁer was similar to [19].\nMelis et al. [153] trained a binary classiﬁer to judge dataset\nproperties in collaborative learning, which took updated\ngradient values as input. Here the model is continuously\nupdated, so attacker could analyze updated information at\neach stage to infer properties.\n5.4\nAnalysis of Model Inversion Attack\nWe have surveyed 21 model inversion attack papers, and\ndisplay 15 related papers in Table 4.\nFinding 4. There are not many papers (4/15) using shadow\nmodels to train the attack model.\nIn our surveyed papers, shadow models (4/15) are used\nin both MIA (2/15) [203] [198] and PIA (2/15) [19] [66].\nAlthough Shokri et al. [203] proposed the method of training\nshadow models to provide training data for attack model\nin a model inversion attack, few recent papers still train\nshadow models for attack. This is mainly because training\nshadow models requires much extra overhead, and the\neffect of directly training attack model is getting better.\nHowever, shadow models still have some advantages: 1)\nrequiring no additional auxiliary information [65], such as\nassuming that higher conﬁdence means higher probability\nfrom dataset. 2) providing true information as training data\nfor attack model.\nFinding 5. Data synthesis is a commonly-used solution (8/15)\nin a model inversion attack, if there is a lack of valid data, and\nattackers want to save query costs.\nData synthesis could generate data similar to the target\ndataset conveniently [203] [65] [89] [137], without querying\ntoo many times. The synthesized data could be generated\neither by the statistical distribution of known training data,\nor a generative adversarial network. These data can effec-\ntively imitate the original data. It avoids too many queries\nto the target model and thereby lowers the perception by\nsecurity mechanisms.\nFinding 6. MIA is essentially a process that expresses the logical\nrelations and data information contained in the trained model. It\nexposes many areas to the risk of information leakage.\nIn addition to centralized learning, attackers also imple-\nment model inversion attacks in federated learning [161]\n[153]. Although the majority papers of information inference\noccur in image ﬁled (13/15), some researchers also perform\ninference attack against DNA methylation data [81]. This\nmedical application could cause more serious damage to\npersonal privacy. The technology of model inversion attack\ncan also be used to recover an input sample [65] [88], and\ndetect a replay attack [192].\nFinding 7. Researchers pay more attention to individual mem-\nbership information (12/15) than statistical property information\n(4/15).\nThis is because membership inference now has a more\ngeneral adaptation scenario, and it emerges earlier. The\nleakage of individual information is more serious than that\nof statistical information. Furthermore, MIA can get more\ninformation than PIA in one-time attack (just like training\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n10\nTABLE 4: Evaluation on model inversion attack. It presents how the “Step” in “Workﬂow” proceeds for each work in\nFigure 4, and its “Goal”, either MIA or PIA. We select one experimental “Dataset” in the works and the corresponding\n“Precision” achieved as well as the target “Model”. “Precision” is the accuracy of judgement. “Knowledge” denotes the\nacquisitions of attackers to the model, and “Application” is the applicable domain of the target model. “structured data”\nrefers to any data in a ﬁxed ﬁeld within a record or ﬁle [28].\nPaper\nWorkﬂow\nGoal\nPrecision\nDataset\nModel\nKnowledge\nApplication\nStep 1\nStep 2\nStep 3\nStep 4\nTruex et al. [221]\n✓\n✓\nMIA\n61.75%\nMNIST [120]\nDT\nBlack\nimage\nPyrgelis et al. [184]\n✓\n✓\nMIA\n-\nTFL\nMLP\nBlack\nstructured data\nShokri et al. [203]\n✓\n✓\n✓\n✓\nMIA\n51.7%\nMNIST\nDNN\nBlack\nimage\nHayes et al. [86]\n✓\n✓\nMIA\n58%\nCIFAR-10 [117]\nGAN\nBlack\nimage\nLong et al. [144]\n✓\nMIA\n93.36%\nMNIST\nNN\nBlack\nimage\nMelis et al. [153]\n✓\n✓\nMIA/PIA\n-\nFaceScrub\nDNN\nWhite\nimage\nLiu et al. [137]\n✓\n✓\nMIA\n-\nMNIST\nGAN\nWhite\nimage\nSalem et al. [198]\n✓\n✓\n✓\n✓\nMIA\n75%\nMNIST\nCNN\nBlack\nimage\nAteniese et al. [19]\n✓\n✓\n✓\n✓\nPIA\n95%\n-\nSVM\nWhite\nspeech\nBuolamwini et al. [38]\n✓\nPIA\n79.6%\nIJB-A [6]\nDNN\nBlack\nimage\nGanju et al. [66]\n✓\n✓\n✓\n✓\nPIA\n85%\nMNIST\nNN\nWhite\nimage\nHitaj et al. [89]\n✓\n✓\nMIA\n-\n-\nCNN\nWhite\nimage\nYang et al. [256]\n✓\n✓\n✓\nMIA\n78.3%\nFaceScrub\nCNN\nBlack\nimage\nNasr et al. [161]\n✓\n✓\nMIA\n74.3%\nCIFAR-100\nDenseNet\nWhite\nimage\nSablayrolles et al. [196]\n✓\nMIA\n57.0%\nCIFAR-100\nResNet\nBlack\nimage\nan attack model). A trained attack model can be applied\nto many records in MIA, but only a few properties in PIA.\nIn [19], attackers want to know if their speech classiﬁer was\ntrained only with voices from people who speak Indian\nEnglish. In [66], they try to ﬁnd if some classiﬁers have\nenough women or minorities in training dataset. In [38],\nthey are interested in the global distribution of skin color.\nIn [153], they want to know the proportion between black\nand asian people.\nFinding 8. Heuristic methods (6/15) are simple, but effects are\nnot very good. More studies still adopts the attack model (9/15).\nIn heuristic methods, naively using probabilities is easy\nto implement, but barely works (0.5 precision and 0.54\nrecall) on MNIST dataset [198]. Obtaining similar datasets\nusually needs to train a generative model [86] [137] [89].\nIn attack model methods, attackers need to train an attack\nmodel [184] [19]. Shadow models [203] [198] [19] are pro-\nposed to provide datasets for the attack model, but increase\ntraining costs.\n6\nPOISONING ATTACK\nPoisoning attack seeks to downgrade deep learning sys-\ntems’ prediction accuracy by polluting training data. Since\nit happens before the training phase, the caused contamina-\ntion is usually inextricable by tuning the involved parame-\nters or adopting alternative models.\n6.1\nIntroduction of Poisoning Attack\nIn the early age of machine learning, poisoning attack had\nbeen proposed as a non-trivial threat to the mainstream\nalgorithms. It was originally proposed to decrease ma-\nchine learning model accuracy. For instance, Bayes classi-\nﬁers [163], Support Vector Machine (SVM) [31] [35] [244]\n\u0015\u0011\u0003$SSURDFKHV\n)OLSSLQJ\u0003/DEHOV\n$GG\u00033RLVRQHG\u0003\n,QVWDQFHV\n\u0014\u0011\u00033RLVRQLQJ\n0LVODEHOHG\u0003\n'DWD\n&RQIXVHG\u0003\n'DWD\n2ULJLQDO\u0003'DWDVHW\n3RLVRQHG\u00030RGHO\n3RLVRQHG\u0003'DWDVHW\n7UDLQLQJ\nFig. 5: Workﬂow of poisoning attack\n[243] [39], Hierarchical Clustering [32], Logistic Regres-\nsion [152] are all suffering degradation from data poisoning.\nAlong with the broad use of deep learning, attackers have\nmoved their attention to deep learning instead [100] [200]\n[210].\nAdversary Model. Attackers can implement this attack with\nfull knowledge (white-box) and limited knowledge (black-\nbox). Usually, black-box attackers have no knowledge of the\ntraining dataset and the trained parameters, but they can\nknow the feature set, the learning algorithm, and obtain\na substitute dataset. Knowledge mainly means the under-\nstanding of training process, including training algorithms,\nmodel architectures, and so on. Capabilities of attackers\nrefer to controlling over the training dataset. In particular,\nit discriminates how much new poisoned data attackers\ncan insert, and whether they can alter labels in the original\ndataset and so on.\nAttack Goal. There are two main purposes for poisoning\nthe data. The original and intuitive purpose is to destroy\nthe model’s availability by deviating its decision boundary.\nAs a result, the poisoned model could not well represent\nthe correct data and is prone to making wrong predictions.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n11\nThis is likely caused by mislabeled data (cf. Section 6.2.1),\nwhose labels are intentionally tampered by attackers, e.g.,\none photo with a cat in it is marked as dog. Recently, many\nresearchers utilize poisoning attack to create a backdoor in\nthe target model by inserting confused data (cf. Section 6.2.2).\nThe model may behave normally most of the time, but\narouse wrong predictions with crafted data. With the pre-\nimplanted backdoor and trigger data, one attacker can ma-\nnipulate prediction results and launch further attacks.\nWorkﬂow. Figure 5 shows a common workﬂow of poison-\ning attack. Basically, this attack is accomplished by two\nmethods: mislabel original data, and craft confused data.\nThe poisoned data then enters into the original data and\nsubverts the training process, leading to greatly degraded\nprediction capability or a backdoor implanted into the\nmodel. More speciﬁcally, mislabeled data is yielded by\nselecting certain records of interest and ﬂipping their labels.\nConfused data is crafted by embedding special features that\ncan be learnt by the model which are actually not the essence\nof target objects. These special features can serve as a trigger,\nincurring a wrong classiﬁcation.\n6.2\nPoisoning Attack Approach\n6.2.1\nManipulating Mislabeled Data\nLearning model usually experiences training under labeled\ndata in advance. Attackers may get access to a dataset, and\nchange a correct label to wrong. Mislabeled data could push\nthe decision boundary of the classiﬁer signiﬁcantly to incor-\nrect zones, thus reducing its classiﬁcation accuracy. Mu˜noz-\nGonz´alez et al. [158] undertook a poisoning attack towards\nmulti-class problem based on back-gradient optimization.\nIt calculated gradient by automatic differentiation and re-\nversed the learning process to reduce attack complexity. This\nattack is resultful for spam ﬁltering, malware detection and\nhandwirtten digit recognition.\nXiao et al. [244] adjusted a training dataset to attack SVM\nby ﬂipping labels of records. They proposed an optimized\nframework for ﬁnding the label ﬂips which maximizes clas-\nsiﬁcation errors, and thus reducing the accuracy of classiﬁer\nsuccessfully. Biggio et al. [32] used obfuscation attack to\nmaximally worsen clustering results, where they relied on\nheuristic algorithms to ﬁnd the optimal attack strategy.\nAlfeld et al. [16] added optimal special records into the\ntraining dataset to drive predictions in a certain direction.\nThey presented a framework to encode an attacker’s desires\nand constraints under linear autoregressive models. Jagiel-\nski et al. [100] could manipulate datasets and algorithms to\ninﬂuence linear regression models. They also introduced a\nfast statistical attack which only required limited knowledge\nof training process.\nLiu et al. [134] poison stochastic multi-armed bandit al-\ngorithms through convex optimization based attacks. They\ncan force the bandit algorithm to pull the target arm with a\nhigh probability through a slight operation on the reward\nin the data. Zhang et al. [267] propose a data poisoning\nstrategy against knowledge graph embedding technique.\nAttackers can effectively manipulate the plausibility of tar-\ngeted facts in the knowledge graph by adding or deleting\nfacts on the knowledge graph. Z¨ugner et al. [277] research\nthe poisoning attack on graph neural network (GNN). They\ngenerate poisoned data targeting the node’s features and\nthe graph structure. They use incremental calculation to\nsolve the potential discrete domain problem. Liu et al. [139]\npropose a data poisoning attack framework on graph-based\nsemi-supervised learning. They adopt a gradient-based al-\ngorithm and a probabilistic solver to settle two constraints\nin poisoning tasks.\nThe major research focuses on an ofﬂine environment\nwhere the classiﬁer is trained on ﬁxed inputs. However,\ntraining also happens as data arrives sequentially in a\nstream, i.e., in an online setting. Wang et al. [235] conducted\npoisoning attacks for online learning. They formalized the\nproblem into semi-online and fully-online, with three attack\nalgorithms of incremental, interval and teach-and-reinforce.\nExcept for one-party poisoning, Mahloujifar et al. [150] study\na online (k, p)-poisoning attack, which applies to multi-\nparty learning processes. The adversary controls k parties,\nand the poisoned data is still (1−p)-close to the correct data.\n6.2.2\nInjecting Confused Data\nLearning algorithms elicit representative features from a\nlarge amount of information for learning and training. How-\never, if attackers submit crafted data with special features,\nthe classiﬁer may learn fooled features. For example, mark-\ning ﬁgures with number “6” as a turn left sign and putting\nthem into the dataset, then images with a bomb may be\nidentiﬁed as a turn-left sign, even if it is in fact a STOP sign.\nXiao et al. [242] directly investigate the robustness of\npopular feature selection algorithms under poisoning at-\ntack. They reduced LASSO to almost random choices of\nfeature sets by inserting less than 5% poisoned training\nsamples. Shafahi et al. [200] ﬁnd a speciﬁc test instance to\ncontrol the behavior of classiﬁer with backdoor, without any\naccess to data collection or labeling process. They proposed\na watermarking strategy and trained a classiﬁer with multi-\nple poisoned instances. Low-opacity watermark of the target\ninstance is added to poisoned instances to allow overlap of\nsome indivisible features. Liu et al. [141] propose a trojaning\nattack. Attackers ﬁrst download a public model, then gen-\nerate a trojan trigger by inversing the neural network and\nnext retrain the model to inject malicious behaviors. Then\nthey republish the mutated neural network with a trojan\ntrigger. This attack is effective on face, speech, age, sentence\nattitude recognition. Xi et al. [240] propose graph-oriented\nGNN poisoning attack. The triggers are speciﬁc sub-graphs,\nincluding both topological structures and descriptive fea-\ntures.\n6.2.3\nAttacks in Transfer Learning.\nGu et al. [76] introduce the threat of poisoning attack in\nan outsourced training setting. The user model will also be\npoisoned if he performs transfer learning on the backdoor\nmodel (BadNet) provided by the adversary. Yao et al. [258]\npropose latent backdoors to insert a backdoor trigger into a\nteacher model in transfer learning. At the teacher side, at-\ntackers inject backdoor data related to a target class y. When\nthe student side downloads the infected teacher model, the\ntransfer learning can silently activate the latent backdoor\ninto a live backdoor, and form an infected student model.\nKurita et al. [119] ﬁnd downloading untrusted pre-trained\nweights poses a security threat. Attackers construct a weight\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n12\nTABLE 5: Evaluation on poisoning attack. The data denotes an attacker needs to contaminate how many percent of training\ndata “Poison Percent” and achieves how many “Success Rate” under speciﬁc “Dataset”. “Model” indicates the attacked\nmodel. “Timeliness” denotes whether the poison attack is in an online or ofﬂine setting. “Damage” means how many\npredictions can be impacted. Attackers may possess two different “Knowledge”, either black-box or white-box, and make\npoisoned model predict as expected, i.e., “Targeted”, or not. “structured data” is the same as Table 4. “LR” is linear\nregression. “OLR” is online logistic regression. “SLHC” is single-linkage hierarchical clustering.\nPaper\nSuccess Rate\nDataset\nPoison Percent\nModel\nTimeliness\nDamage\nKnowledge\nTargeted\nApplication\nXiao et al. [242]\n20%\n11944 ﬁles\n5%\nLASSO\nofﬂine\n-\nBlack\nNo\nmalware\nMu˜noz-Gonz´alez et al. [158]\n25%\nMNIST\n15%\nCNN\nofﬂine\n30% error\nBlack\nNo\nimage, malware\nJagielski et al. [100]\n75%\nHealth care dataset\n20%\nLASSO\nofﬂine\n75% error\nBlack\nNo\nstructured data\nAlfeld et al. [16]\n-\n-\n-\nLR\nofﬂine\n-\nWhite\nYes\n-\nShafahi et al. [200]\n60%\nCIFAR-10\n5%\nDNN\nofﬂine\n20% error\nWhite\nYes\nimage\nWang et al. [235]\n90%\nMNIST\n100%\nOLR\nonline\n-\nWhite\nBoth\nimage\nBiggio et al. [32]\n-\nMNIST\n1%\nSLHC\nofﬂine\n-\nWhite\nYes\nimage, malware\nBadNets [76]\n99%\nMNIST\n-\nCNN\nofﬂine\n-\nWhite\nBoth\nimage\nYao et al. [258]\n96%\nMNIST\n0.15%\nCNN\nofﬂine\n-\nWhite\nYes\nimage\nLiu et al. [139]\n-\nMNIST\n4%\nGNN\nofﬂine\n50% error\nWhite\nYes\nimage\npoisoning attack, and the user model will also carry a\nbackdoor after ﬁne-tuning the pre-trained injected weights.\nThis allows the attacker to manipulate model prediction.\n6.2.4\nAttacks in Federated Learning.\nSome recent articles have begun to study how to con-\nduct backdoor attacks in federated learning [23] [212]. In\nfederated learning, there may be one or several attackers\nwho can participate in model training. Their goal is to\nimplant a speciﬁc backdoor in the ﬁnal trained model. In\n[23] [212], attackers try to strictly limit the loss items to\navoid anomaly detection, and boost maliciously updated\nvalues to reserve backdoor. Bhagoji et al. [29] introduce\nthe technology of alternating minimization with distance\nconstraints to avoid the updated value statistics anomaly de-\ntection. Xie et al. [246] propose distributed backdoor attacks.\nThey decompose a trigger into several small patterns. Each\nattacker implants a small pattern into the ﬁnal model. Then\nthe complete trigger can also attack successfully in the ﬁnal\nmodel. Fang et al. [63] assume the attacker manipulates local\nmodel parameters on compromised client devices, resulting\nin a large testing error in the global model.\n6.3\nAnalysis of Poisoning Attack\nIn this Section, we investigate 20 representative poisoning\nattack papers in detail, and compare 10 of them in Table 5.\nFinding 9. Poisoning attacks have been researched in extensive\nﬁelds. In our surveyed papers, 3(/20) papers studied how to insert\nbackdoors in transfer learning settings. 4(/20) papers researched\nimplanting backdoors in federated learning settings. 2(/20) papers\nstudied online poisoning attacks.\nWith the wide application of deep learning in multiple\nﬁelds, poisoning attacks have also been studied in different\nﬁelds. Liu et al. [134] apply poisoning attacks to multi-armed\nbandit algorithms. Zhang et al. [267] attack knowledge\ngraph embedding technique. Z¨ugner et al. [277] and Xi et\nal. [240] poison graph neural networks. Liu et al. [139] attack\ngraph-based semi-supervised learning. Poisoning attacks\nfor online learning have been studied in [235] [150]. In\nonline setting, attackers feed poisonous data into the models\ngradually. This makes attackers consider more factors such\nas the order of fed data, the evasiveness of poisonous\ndata. Some attacks [76] [258] [119] inject backdoors into\npre-trained model or teacher model. When users perform\ntransfer learning through a poisonous model, the backdoors\nwill be embedded into their models accordingly. Poisoning\nattacks also exist in federated learning [23] [212] [29] [246].\nAttackers need to upload malicious updated values, bypass\nthe anomaly detection, and inject the backdoor into the ﬁnal\nmodel. These studies also mean that many current learn-\ning algorithms are not robust and vulnerable to poisoning\nattack.\nFinding 10. There are more papers using confused data to inject\nbackdoors into the model. Totally, 10(/20) papers use confused data\nto implant a backdoor. In 2019, the ratio increases up to 8/13.\nMaking mistakes imperceptible is more difﬁcult and\nharmful than making misclassiﬁcation for a model. A back-\ndoor is such an imperceptible mistake. The model performs\nwell under normal functions, while it opens the door for\nattackers when they need it. In recent years, with the devel-\nopment of technology, more research has focused on back-\ndoor poisoning attacks [258] [76] [141] [23] [212]. Backdoor\nattacks are more difﬁcult to detect, and the manipulation to\nthe model is also stronger.\nFinding 11. Pre-trained models from unknown sources still suffer\nfrom poisoning attacks.\nThe performance of learning model is largely dependent\non the quality of training data. High quality data is com-\nmonly acknowledged as being comprehensive, unbiased,\nand representative. In [258] [119] [76], researchers ﬁnd that\nthe pre-trained model can transmit its triggers to uses’\ntraining model. Even if the user has a high-quality dataset,\nas long as the pre-trained model is trained on a low-quality\ndataset or injected with a backdoor, the ﬁnal model is still at\nrisk of being poisoned.\n7\nADVERSARIAL ATTACK\nSimilar to poisoning attack, adversarial attack also makes\na model classify a malicious sample wrongly. Their differ-\nence is that poisoning attack inserts malicious samples into\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n13\n&DOFXODWLQJ\n\u0003*UDGLHQWV\n6ROYLQJ\n2SWLPL]DWLRQ\u0003\n)XQFWLRQ\n&DOFXODWLQJ\n*UDGLHQWV\n6ROYLQJ\n2SWLPL]DWLRQ\n)XQFWLRQ\n\u0016\u0011\u0003)LQGLQJ\u0003\n3HUWXUEDWLRQV\n%ODFN\u0010ER[\u00036HWWLQJ\n\u0015\u0011\u0014\u0011\u00037UDLQLQJ\u0003\n6XEVWLWXWH\u00030RGHO\n6XEVWLWXWH\u0003\n0RGHO\n7DUJHW\u00030RGHO\n:KLWH\u0010ER[\u00036HWWLQJ\n:KLWH\u0010ER[\u0003\n0RGHO\n\u0014\u0011\u00034XHU\\LQJ\u0003\n0RGHO\n\u0015\u0011\u0015\u0011\u0003(VWLPDWLQJ\u0003\n*UDGLHQWV\n$GYHUVDULDO\u0003([DPSOH\n6XE\nX VWLWXW WH\n0RGHO\n:KLWH\u0010ER[ 6HWWLQJ\n:KLWH\u0010ER[\n0RGHO\nFig. 6: Workﬂow of adversarial attack\nthe training data, directly contaminating the model, while\nadversarial attack leverages adversarial examples to exploit\nthe weaknesses of the model and gets a wrong prediction\nresult.\n7.1\nIntroduction of Adversarial Attack\nAdversarial attack adds unperceived perturbations to nor-\nmal samples during the prediction process, and then pro-\nduces adversarial examples (AEs). This is an exploratory\nattack and violates the availability of a model. It can be\nused in many ﬁelds, e.g., image classiﬁcation [213] [74] [43],\nspeech recognition [73] [262], text processing [67] [268] [199]\n[123], and malware detection [95] [177] [179] [116], partic-\nularly widespread in image classiﬁcation. They can deceive\nthe trained model but look nothing unusual to humans. That\nis to say, AEs need to both fool the classiﬁer and be imper-\nceptible to humans. For an image, the added perturbation is\nusually tuned by minimizing the distance between the orig-\ninal and adversarial examples. For a piece of speech or text,\nthe perturbation should not change the original meaning\nor context. In the ﬁeld of malware detection, AEs need to\navoid being detected by models. Adversarial attack can be\nclassiﬁed into the targeted attack and untargeted attack. The\nformer requires adversarial examples to be misclassiﬁed as\na speciﬁc label, while the latter desires a wrong prediction,\nno matter what it will be recognized as.\nAdversary Model. In adversarial attack, black-box setting\nmeans the attacker cannot directly calculate the required\ngradients (such as FGSM [74]) or solve optimization func-\ntions (such as C&W [43]) from the target model. but attack-\ners in white-box setting can do these. Black-box attackers\ncan know the model architecture and hyperparameters to\ntrain a substitute model. They can also query the target\nblack-box model and obtain outputs with predicted label\nand conﬁdence scores to estimate gradients.\nWorkﬂow. Figure 6 depicts the general workﬂow for an\nadversarial attack. In white-box setting, attackers could di-\nrectly calculate gradients [74] [15] [57] or solve optimization\nfunctions [43] [48] [87] to ﬁnd perturbations on original\nsamples (Step 3). In black-box setting, attackers obtain in-\nformation by querying the target model many times (Step\n1). Then they could train a substitute model to perform a\nwhite-box attack [173] [174] (Step 2.1), or estimate gradients\nto search for AEs [98] (Step 2.2).\nIn addition to deceiving the classiﬁcation model, AEs\nshould carry minimal perturbations that evade the aware-\nness of human. Generally, the distance between normal and\nadversarial sample can be measured by Lp Distance (or\nMinkowski Distance), e.g., L0, L1, L2 and L∞.\nLp(x, y) = (\nn\nX\ni=1\n|xi −yi|p)\n1\np\nx = {x1, x2, ...,xn}, y = {y1, y2, ..., yn}\n(2)\n7.2\nAdversarial Attack Approach\nSince the main development of adversarial attack is in the\nﬁeld of image classiﬁcation [213] [74] [43], we will introduce\nmore related work on image using CNN, and supplement\nresearch on other ﬁelds or other models at the end of this\nsection.\n7.2.1\nWhite-box attacks in the image classiﬁcation ﬁeld\nThe white-box in image ﬁeld is the main setting of early\nadversarial attack research. We introduce and compare for-\nmulas used to generate adversarial examples. First, we\ndeﬁne that F : Rn −→{1 . . .k} is the classiﬁer of a model to\nmap image value vectors to a class label. Z(·) is the output\nof second-to-last layer, usually indicates class probability.\nZ(·)t is the probability of t-th class. Loss function describes\nthe loss of input and output under classiﬁer F, and we set\nLoss(x, F(x)) = 0. δ is the perturbation. ∥δ∥p is the Lp-\nnorm of δ. x = {x1, x2, ..., xn} is the original sample, xi is\nthe pixel or element in sample where xi ∈x, 1 ⩽i ⩽n. xi\nis sample of the i-th iteration, usually x0 = x. x + δ is the\nadversarial sample. Here, x ∈[0, 1]n, x + δ ∈[0, 1]n.\nThe process of ﬁnding perturbations essentially needs to\nsolve the following optimization problems (the ﬁrst equa-\ntion is non-targeted attack, the second equation is targeted\nattack, T is targeted class label):\narg min\nδ\n∥δ∥p , s.t. F(x + δ) ̸= F(x)\narg min\nδ\n∥δ∥p , s.t. F(x + δ) = T\n(3)\nMethods of ﬁnding perturbations can be roughly di-\nvided into calculating gradients and solving optimization\nfunction. Szegedy et al. [213] ﬁrst proposed an optimization\nfunction to ﬁnd AEs and solved it with L-BFGS. FGSM [74],\nBIM [15], MI-FGSM [57] are a series of methods for ﬁnd-\ning perturbations by directly calculating gradients. Deep-\nfool [156] and NewtonFool [102] approximate the nearest\nclassiﬁcation boundary by Taylor expansion. Instead of per-\nturbing a whole image, JSMA [175] ﬁnds a few pixels to\nperturb through calculating partial derivative. C&W [43],\nEAD [48], OptMargin [87] are a series of methods to ﬁnd\nperturbations by optimizing the objective function.\nL-BFGS attack. Szegedy et al. [213] try to ﬁnd small δ that\nsatisﬁes F(x + δ) = l. So they construct a function with\nδ and Loss function, and use box-constrained L-BFGS to\nminimize this optimization problem. In Equation 4, c (> 0)\nis a hyperparameter to balance them.\nmin\nδ\nc ∥δ∥2 + Loss(x + δ, l)\n(4)\nFGSM attack. Goodfellow et al. [74] ﬁnd perturbations\nbased on the gradient of input. lx is the true label of x. The\ndirection of perturbation is determined by the computed\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n14\ngradient using back-propagation. ε is self-deﬁned, and each\npixel goes ε size in gradient direction.\nδ = ε · sign(∇xLoss(x, lx))\n(5)\nBIM attack. BIM (or I-FGSM) [15] iteratively solves δ and\nupdates new adversarial samples based on FGSM [74] in\nEquation 6. lx is the true label of x. Clip{x} is a clipping\nfunction on image per pixel.\nxi+1 = Clip{xi + ε · sign(∇xLoss(xi, lx))}\n(6)\nMI-FGSM attack. MI-FGSM [57] adds momentum based on\nI-FGSM [15]. Momentum is used to escape from poor local\nmaximum and iterations are used to stabilize optimization.\nIn Equation 7, gi represents the gradient like Equation 6,\nit has both the current step gradient and previous step\ngradient. y is the target wrong label.\nxi+1 = Clip{xi + ε ·\ngi+1\n∥gi+1∥2\n}\ngi+1 = µ · gi +\n∇xLoss(xi, y)\n∥∇xLoss(xi, y)∥1\n(7)\nJSMA attack. JSMA [175] only modiﬁes a few pixels at\nevery iteration. In each iteration, shown in Equation 8, αpq\nrepresents the impact on target classiﬁcation of pixels p, q,\nand βpq represents the impact on all other outputs. In the\nlast formula, larger value means greater possibility to fool\nthe network. They pick (p∗, q∗) pixels to perturb.\nαpq =\nX\ni∈{p,q}\n∂Z(x)t\n∂xi\nβpq = (\nX\ni∈{p,q}\nX\nj\n∂Z(x)j\n∂xi\n) −αpq\n(8)\n(p∗, q∗) = arg max\n(p,q)(−αpq · βpq) · (αpq > 0) · (βpq < 0)\nNewtonFool attack. NewtonFool [102] uses softmax output\nZ(x). In Equation 9, x0 is the original sample and l = F(x0).\nδi = xi+1 −xi is the perturbation at iteration i. They tried\nto ﬁnd small δ so that Z(x0 + δ)l ≈0. Starting with x0, they\napproximated Z(xi)l using a linear function step by step.\nZ(xi+1)l ≈Z(xi)l + ∇Z(xi)l · (xi+1 −xi)\n(9)\nC&W attack. C&W [43] tries to ﬁnd small δ in L0, L2,\nand L∞norms. They change the Loss function part in L-\nBFGS [213] to an optimization function f(·).\nmin\nδ\n∥δ∥p + c · f(x + δ)\n(10)\nf(x + δ) = max(max{Z(x + δ)i : i ̸= t} −Z(x + δ)t, −K)\nc is a hyperparameter and f(·) is an artiﬁcially deﬁned\nfunction, the above is just one case. Here, f(·) ⩽0 if and\nonly if classiﬁcation result is adversarial targeted label t. K\nguarantees x+ δ will be classiﬁed as t with high conﬁdence.\nEAD attack. EAD [48] combines L1 and L2 penalty func-\ntions based on C&W [43]. In Equation 11, f(x+δ) is the same\nas C&W and β is another hyperparameter. C&W attack\nbecomes a special EAD case when β = 0.\nmin\nδ\nc · f(x + δ) + β ∥δ∥1 + ∥δ∥2\n2\n(11)\nOptMargin attack. OptMargin [87] is an extension of\nC&W [43] attack by adding many objective functions\naround x. In Equation 12, x0 is the original example.\nx = x0 + δ is adversarial. y is the true label of x0. vi are\nmany perturbations applied to x. OptMargin guarantees not\nonly x fools network, but also its neighbors x + vi.\nmin\nδ\n∥δ∥2\n2 + c · (f1(x) + · · · + fm(x))\n(12)\nfi(x) = max(Z(x + vi)y −max{Z(x + vi)j : j ̸= y}, −K)\nUAP\nattack.\nUniversal\nadversarial\nperturbations\n(UAPs) [155] can suit almost all samples of a certain\ndataset. The purpose is to seek a universal perturbation δ\nwhich fools F(·) on almost any sample from the dataset.\nLiu et al. [135] extend UAPs to unsupervised learning.\nCo et al. [53] try to generate UAPs with procedural noise\nfunctions.\n7.2.2\nBlack-box attacks in the image classiﬁcation ﬁeld\nFinding small perturbations often requires white-box mod-\nels to calculate gradients. However, it does not work in\na black-box setting. Attackers are limited only to query\naccess to the model. Therefore, researchers propose several\nmethods to overcome constraints on query budget.\nStep 2.1. Training substitute model. As mentioned in\nSection 4, stealing decision boundaries in model extraction\nattack and training substitute model can facilitate black-box\nadversarial attacks [174] [173] [107]. Papernot et al. [174] pro-\npose a method based on an alternative training algorithm\nusing synthetic data generation in black-box settings.\nThis step needs that AEs have high transferability from\nthe substitute model to the target model [58], [248]. Gra-\ndient aligned adversarial subspace [219] estimate unknown\ndimensions of the input space. They ﬁnd that a large part\nof the subspace is shared for two different models, thus\nachieving transferability. Further, they determine sufﬁcient\nconditions for the transferability of model-agnostic pertur-\nbations. Naseer et al. [160] propose a framework to launch\nhighly transferable attacks. It can create adversarial pat-\nterns to mislead networks trained in completely different\ndomains.\nStep 2.2. Estimating gradients. This method needs many\nqueries to estimate gradients and then search for AEs.\nNarodytska et al. [159] use a technique based on local\nsearch to construct the numerical approximation of network\ngradients, and then constructed perturbations in an image.\nMoreover, Ilyas et al. [98] introduce a more rigorous and\npractical black-box threat model. They applied a natural\nevolution strategy to estimate gradients and perform black-\nbox attacks, using 2∼3 orders of magnitude less queries.\nGuo et al. [80] utilize the gradients of some reference mod-\nels to reduce queries. These reference models can span\nsome promising search subspaces. Liu et al. [142] propose a\ndecision-based attack method by constraining perturbations\nin low-frequency subspace with small queries. Cheng et\nal. [51] present a prior-guided random gradient-free method,\nwhich takes advantage of a transfer-based prior and query\ninformation simultaneously.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n15\n7.2.3\nAttacks in the speech recognition ﬁeld\nThe difﬁculties of attacking speech recognition model are\nthat, humans can identify adversarial perturbations, and\naudio AEs may be ineffective during over-the-air playback.\nYuan et al. [262] embed voice commands into songs, and\nthereby attack speech recognition systems, not being de-\ntected by humans. DeepSearch [44] could convert any given\nwaveform into any desired target phrase through adding\nsmall perturbations on speech-to-text neural networks. Qin\net al. [186] leverage the psychoacoustic principle of auditory\nmasking to generate effectively imperceptible audio AEs.\nYakura et al. [254] simulate the transformations caused by\nplayback or recording in the physical world, and incorpo-\nrates these transformations into the generation process to\nobtain robust AEs.\n7.2.4\nAttacks in the text processing ﬁeld\nConstructing adversarial examples for natural language\nprocessing (NLP) is a large challenge. The word and sen-\ntence spaces are discrete. It is difﬁcult to produce small\nperturbations along the gradient direction, and hard to\nguarantee its ﬂuency [268]. DeepWordBug [67] generate\nadversarial text sequences in black-box settings. They adopt\ndifferent score functions to better mutate words and mini-\nmize edit distance between the original and modiﬁed texts.\nTextBugger [123] also generated adversarial texts. In black-\nbox setting, its process is ﬁnding important sentences and\nwords, and bugs generation. The computational complexity\nis sub-linear to the text length. It has higher success rate and\nless perturbed words than DeepWordBug on IMDB dataset.\nHowever, the above work is achieved by similar-looking\ncharacter substitution (‘o’ and ‘0’), adding space and so on,\nwhich destroy lexical correctness. In [189] [263], they study\nword-level substitution attack to guarantee lexical correct-\nness, grammatical correctness and semantic similarity. Ren\net al. [189] propose a word replacement order determined\nby word saliency and classiﬁcation probability based on\nsynonyms replacement strategy. Zang et al. [263] present a\nword replacement method based on sememe, and a search\nalgorithm based on particle swarm optimization.\nNeural machine translation (NMT) models in NLP\nalso suffer from the vulnerability to adversarial perturba-\ntions [52]. Zou et al. [276] generate adversarial translation ex-\namples based on a new paradigm of reinforcement learning,\ninstead of limited manual analyzed error features. Experi-\nments show that the replacement of synonyms in Chinese\nwill cause obvious errors in English translation results. Sato\net al. [199] reveal that adversarial regularization technology\ncan also improve the NMT models.\n7.2.5\nAttacks in the malware detection ﬁeld\nIn the malware ﬁeld, Rigaki et al. [190] used GANs to\navoid malware detection by modifying network behavior\nto imitate trafﬁc of legitimate applications. They can adjust\ncommand and control channels to simulate Facebook chat\nnetwork trafﬁc by modifying the source code of malware.\nHu et al. [90] [91] and Rosenberg et al. [194] proposed meth-\nods to generate adversarial malware examples in black-\nbox to attack detection models. Dujaili et al. [14] proposed\nSLEIPNIR for adversarial attack on binary encoded malware\ndetection.\n7.2.6\nAttacks in the object detection ﬁeld\nZhao et al. [274] propose hiding attack and appearing attack\nto produce practical AEs. Their attacks can attack real-\nworld object detectors in both long and short distance. Wei\net al. [236] manipulate the feature maps extracted by the\nfeature network, and enhance the transferability of AEs\nwhen attacking image object detection models.\n7.2.7\nAttacks in the physical world.\nIn this setting, attackers need to consider more environ-\nmental factors. Zeng et al. [265] pay special attention to\nAEs corresponding to meaningful changes in 3D physical\nproperties, such as rotation, translation, lighting conditions,\netc. Li et al. [124] implement physical attacks by placing\na mainly-translucent sticker over the lens of a camera.\nThe perturbations are imperceptible, but can make models\nmisclassify objects taken by this camera.\n7.2.8\nAttacks in real-time stream input tasks\nIn this situation, attackers cannot observe the entire original\nsample and then add a perturbation at any point as in\nstatic input. Gong et al. [72] propose a real-time adversarial\nattack approach, in which attackers can only observe past\ndata points and add perturbations to the remaining data\npoints of the input. Li et al. [127] generate 3D adversarial\nperturbed fragments to attack real-time video classiﬁcation\nmodels. They ﬁnd AEs need to consider the uncertainty in\nthe clip boundaries input to the video classiﬁer. Ranjan et\nal. [187] ﬁnd that destroying small patches (¡1%) of the image\nsize will signiﬁcantly affect optical ﬂow estimation in self-\ndriving cars.\n7.2.9\nAttacks against graph neural networks\nGraph neural networks (GNNs) are also vulnerable to ad-\nversarial attacks [277]. However, the discrete edges and\nfeatures of the graph data also bring new challenges for\nattacks. Wu et al. [239] use integrated gradient technology to\ndeal with discrete graph connections and discrete features.\nIt can accurately determine the effect of changing selected\nfeatures or edges. For handling discrete graph data, Xu et\nal. [251] study a technology of generating topology attacks\nvia convex relaxation to apply gradient-based adversarial\nattacks to GNNs. Bojchevski et al. [33] provide adversarial\nvulnerability analysis on widely used methods based on\nrandom walks. Wang et al. [225] try to evade detection\nthrough manipulating the graph structure and formulate\nthis attack as a graph-based optimization problem.\n7.2.10\nAttacks against other models\nThere is furthermore research besides CNNs, RNNs, GNNs,\nsuch as generative model, reinforcement learning and some\nmachine learning algorithms. Mei et al. [152] identiﬁed the\noptimal training set attack for SVM, logistic regression, and\nlinear regression. They proved the optimal attack can be\ndescribed as a bilevel optimization problem, which can be\nsolved by gradient methods. Chen et al. [47] prove that tree-\nbased models are also vulnerable to AEs. Huang et al. [94]\nand Gleave et al. [71] demonstrate that adversarial attack\npolicies are also effective in reinforcement learning. Kos et\nal. [114] attempted to produce AEs using deep generative\nmodels such as variational autoencoder. Their methods in-\nclude a classiﬁer-based attack, and an attack on latent space.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n16\nTABLE 6: Evaluation on adversarial attacks. This table presents “Success Rate” of these attacks in speciﬁc “Dataset” with\nvarying target “System” and “Model”. “Distance” implies how these works measure the distance between samples. “Real-\nworld” is used to distinguish the works that are also suitable for physical adversarial attacks. “Knowledge” is valued either\nblack-box or white-box. “Iterative” illustrates whether the optimization steps are iterative. “Targeted” differs whether an\nattack is a targeted attack or not. “Application” covers the practical areas.\nPaper\nSuccess Rate\nDataset\nSystem\nDistance\nModel\nReal-world\nKnowledge\nIterative\nTargeted\nApplication\nL-BFGS [213]\n20.3%\nMNIST\nFC100-100-10\nL2\nDNN\nNo\nWhite\nYes\nYes\nimage\nFGSM [74]\n55.4%\nMNIST\na shallow RBF network\nL∞\nDNN\nNo\nWhite\nNo\nNo\nimage\nBIM [15]\n24%\nImageNet [2]\nInception v3\nL∞\nCNN\nYes\nWhite\nYes\nNo\nimage\nMI-FGSM [57]\n37.6%\nImageNet\nInception v3\nL∞\nCNN\nNo\nWhite\nYes\nBoth\nimage\nJSMA [175]\n97.05%\nMNIST\nLeNet\nL0\nCNN\nNo\nWhite\nYes\nYes\nimage\nC&W [43]\n100%\nImageNet\nInception v3\nL0, L2, L∞\nCNN\nNo\nWhite\nYes\nYes\nimage\nEAD [48]\n100%\nImageNet\nInception v3\nL1, L2, L∞\nCNN\nNo\nWhite\nYes\nYes\nimage\nOptMargin [87]\n100%\nCIFAR-10\nResNet\nL0, L2, L∞\nCNN\nNo\nWhite\nYes\nNo\nimage\nGuo et al. [77]\n95.5%\nImageNet\nResNet-50\nL2\nCNN\nNo\nBoth\nYes\nNo\nimage\nDeepfool [156]\n68.7%\nILSVRC2012\nGoogLeNet\nL2\nCNN\nNo\nWhite\nYes\nNo\nimage\nNewtonFool [102]\n81.63%\nGTSRB [4]\nCNN(3Conv+1FC)\nL2\nCNN\nNo\nWhite\nYes\nNo\nimage\nUAP [155]\n90.7%\nILSVRC2012\nVGG-16\nL2, L∞\nCNN\nNo\nWhite\nYes\nNo\nimage\nUAN [85]\n91.8%\nImageNet\nResNet-152\nL2, L∞\nCNN\nNo\nWhite\nYes\nYes\nimage\nATN [24]\n89.2%\nMNIST\nCNN(3Conv+1FC)\nL2\nCNN\nNo\nWhite\nYes\nYes\nimage\nAthalye et al. [20]\n83.4%\n3D-printed turtle\nInception-v3\nL2\nCNN\nYes\nWhite\nNo\nYes\nimage\nIlyas et al. [98]\n99.2%\nImageNet\nInception-v3\n-\nCNN\nNo\nBlack\nNo\nBoth\nimage\nNarodytska et al. [159]\n97.51%\nCIFAR-10\nVGG\nL0\nCNN\nNo\nBlack\nNo\nNo\nimage\nKos et al. [114]\n76%\nMNIST\nVAE-GAN\nL2\nGAN\nNo\nWhite\nNo\nYes\nimage\nMei et al. [152]\n-\n-\n-\nL2\nSVM\nNo\nBlack\nYes\nNo\nimage\nHuang et al. [94]\n-\n-\nA3C,TRPO,DQN\nL1, L2, L∞\nRL\nNo\nBoth\nNo\nNo\nimage\nPapernot et al. [177]\n100%\nReviews\nLSTM\nL2\nRNN\nYes\nWhite\nNo\nNo\ntext\nDeepWordBug [67]\n51.80%\nIMDB Review [7]\nLSTM\nL0\nRNN\nYes\nBlack\nYes\nYes\ntext\nDeepSpeech [44]\n100%\nMozilla Common Voice [9]\nLSTM\nL∞\nRNN\nNo\nWhite\nNo\nYes\nspeech\nGong et al. [73]\n72%\nIEMOCAP\nLSTM\nL2\nRNN\nYes\nWhite\nNo\nNo\nspeech\nCommanderSong [262]\n96%\nFisher\nASplRE Chain Model\nL1\nRNN\nYes\nWhite\nNo\nYes\nspeech\nRosenberg et al. [194]\n99.99%\n500000 ﬁles\nLSTM\nL2\nRNN\nYes\nBlack\nYes\nNo\nmalware\nMtNet [95]\n97%\n4500000 ﬁles\nDNN(4 Hidden layers)\nL2\nDNN\nYes\nBlack\nNo\nNo\nmalware\nSLEIPNIR [14]\n99.7%\n55000 PEs\nDNN\nL2, L∞\nDNN\nYes\nBlack\nNo\nNo\nmalware\nRigaki et al. [190]\n63%\n-\nGAN\nL0\nGAN\nYes\nBlack\nNo\nNo\nmalware\nPascanu et al. [179]\n69%\nDREBIN [1]\nDNN\nL1\nDNN\nYes\nBlack\nNo\nNo\nmalware\nKreuk et al. [116]\n88%\nMicrosoft Malware [8]\nCNN\nL2, L∞\nCNN\nYes\nWhite\nNo\nYes\nmalware\nHu et al. [90]\n90.05%\n180 programs\nBiLTSM\nL1\nRNN\nYes\nBlack\nYes\nNo\nmalware\nHu et al. [91]\n99.80%\n180000 programs\nMalGAN\nL1\nGAN\nYes\nBlack\nNo\nNo\nmalware\n7.3\nAnalysis of Adversarial Attack\nIn conclusion, we have surveyed 66 adversarial attack\npapers, and measured 33 related papers in Table 6, and\nidentiﬁed following observations.\nFinding 12. AEs may be inevitable in high-dimensional classiﬁers\nunder the computational limitations.\nMany classiﬁers are found to be vulnerable to adversar-\nial attacks. Besides the most commonly attacked CNNs in\nimage classiﬁcation, RNNs are also vulnerable in such text\nprocessing and malware detection ﬁelds. With the devel-\nopment of GNNs, they also suffer from adversarial attacks\n(5/66). SVM [152], reinforcement learning [94] [71], genera-\ntive models [114] are all proved to be attacked. The reason\nwhy high-dimensional classiﬁers suffer from AEs may be\nthat computational constraints and input data limitations\nmake it difﬁcult to restore the decision boundaries. AEs may\nbe an inevitable byproduct of the computational constraints\nof learning algorithms [36]. Dohmatob et al. [56] give a\ntheoretical proof that once the perturbations are slightly\nlarger than the natural noise level, any classiﬁer can be\nadversarially deceived with high probability.\nFinding 13. Adversarial samples widely exist in the samples\nspace of various ﬁelds.\nAdversarial attacks have penetrated into many ﬁelds. In\nour 66 surveyed papers, 28 papers focus on image classiﬁ-\ncation, 6 papers focus on speech recognition, 9 papers attack\ntext processing, 8 papers attack malware detection. Whether\nthe sample space is a discrete domain (text or malware) or a\ncontinuous domain (speech), whether the input is a deﬁnite\nsize (image) or an indeﬁnite size (text or speech), adversarial\nexamples are all widespread. In the entire sample space,\nadversarial samples and normal samples are likely to be in\na symbiotic relationship.\nFinding 14. Physical attacks bring the harm of adversarial\nsamples to a new level.\nAEs in the digital space may fail to fool classiﬁers in\nthe physical space because physical attacks need to consider\nmore environmental factors. Recently, in the image ﬁeld,\nreal-world attack studies become more according to our re-\nsearch (6/15 in 2019 and only 2/20 in previous years). Phys-\nical attack needs to consider photographing viewpoints,\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n17\nenvironmental lighting, camera noise and so on. This causes\nmany previous studies only worked at the digital space.\nAs the technology matures, more physical attacks are being\nstudied. Physical attacks are more harmful to us, such as\ntrafﬁc signs that truly fool object detectors [274], and voices\nthat actually fool smart speakers [262]. Physical attacks still\nneed more in-depth research, which will also lead to security\nresearch in real AI systems. Besides, physical problem does\nnot exist in text or malware ﬁeld, so we give them all “Yes”\nin “Real-world”.\nFinding 15. Untargeted adversarial attacks (57.6%) are easier to\nachieve but less severe than targeted adversarial attacks.\nUntargeted attacks aim at inducing wrong predictions,\nand thus more ﬂexible in ﬁnding perturbations which only\nneed smaller modiﬁcations. Therefore, it can achieve success\nmore easily. Targeted attacks have to make the model predict\nwhat as expected. Therefore, much more perturbations need\nto be created for accomplishing the target. However, they are\nusually more harmful and practical in reality. For example,\nattackers may disguise themselves as authenticated users\nin a face recognition system, in order to gain the access to\nprivileged resources.\nFinding 16. Almost all attacks adopt Lp-distance, including\nL0, L1, L2, L∞, while L2 distance is the most widely used.\nDistance metrics is an important factor to ﬁnd minimum\nperturbations, which mostly use Lp-distance currently. In\n“Distance” column of Table 6, 60.1% attacks use L2 distance,\n36.4% use L∞distance, 18.2% use L1 distance and 18.2%\nuse L0 distance. Considering image classiﬁcation only, 70%\nattacks use L2 distance, 45% use L∞distance, 10% use L1\ndistance and 20% use L0 distance.\nL0 distance reﬂects the number of changed elements, but\nit is unable to limit the variation of each element. It suits\nthe scenes that only care about the number of perturbation\npixels, but not variation size. L1 distance is the absolute\nvalues summation of every element in perturbations, equiv-\nalent to Manhattan distance in 2D space. It limits the sum\nof all variations, but does not limit large perturbation of\nindividual elements. L∞distance does not care about how\nmany elements have been changed, but only cares about\nthe maximum of perturbations, equivalent to Chebyshev\ndistance in 2D space. L2 distance is an Euclidean distance\nthat considers all pixel perturbation, which is a more bal-\nanced and the most widespread metric. It takes into account\nboth the largest perturbation and the number of changed\nelements.\nFinding 17. Different positions should have different weights for\nperturbations.\nIn the current measurement methods, the perturbations\nof different elements are considered to have the same\nweight. However, in face images, the same perturbations\napplied on the important part of face such as nose, eyes\nand mouth, will be easier to identify than that applied on\nthe background. Similarly, in audio analysis, perturbations\nare difﬁcult to be noticed in a chaotic scene, but are easily\nperceived in a quiet scene. According to the above analysis,\nwe can consider to adopt different weights on different\nelements when measuring distance. The important part has\na larger weight, so it can only make smaller perturbations,\nwhile the unimportant part has a smaller weight, which can\nintroduce larger perturbations.\nFinding 18. More advanced measurements for the human percep-\ntion are desired.\nThe original goal of AEs is to make the model classify\nsamples wrongly while keeping humans unaware of the\ndifferences. However, it is difﬁcult to measure humans’ per-\nception of these perturbations. Intuitively, small Lp distance\nimplies a low probability of being detected by humans.\nWhile recent work found that Lp distance is neither nec-\nessary nor sufﬁcient for perceptual similarity [201]. That is,\nperturbations with large Lp values may also look similar\nto humans, such as translations and rotations of images,\nand small Lp perturbations do not mean imperceptible. Re-\nsearch [62] also proves that neural network-based classiﬁers\nare vulnerable to rotations and translations. In a recent pa-\nper, Bhattad et al. [30] introduce unrestricted perturbations\nto generate effective and realistic AEs. Therefore, we should\nbreak the constraint of Lp distance. How to search for AEs\nsystematically without Lp limitation, and how to propose\nnew measurements that could be necessary or sufﬁcient for\nperceptual similarity, will be a trend of adversarial attack in\nthe near future.\n8\nDISCUSSION\nIn this section, we summarize 7 observations according to\nthe survey as follows.\n8.1\nRegulations on privacy protection\nAs shown in Section 4 and 5, both the enterprises and\nusers are suffering from the risk of privacy. In addition\nto removing privacy in the data, governments and related\norganizations can issue laws and regulations against privacy\nviolations in the course of data use and transmission. In\nparticular, it is recommended that: 1) introducing regulatory\nauthorities to monitor these deep learning systems and\nstrictly supervise the use of data. The involved systems are\nonly allowed to extract features and predict results within\nthe permitted range. The private information is forbidden\nfor being extracted and inferred without authorization. 2)\nestablishing and improving relevant laws and regulations\n(e.g., GDPR [3]), for supervising the process of data collec-\ntion, use, storage and deletion. 3) adding digital watermarks\ninto the data for leak source tracking [21]. The watermarks\nhelp to fast ﬁnd out the rule breakers that are liable for\nexposing privacy.\n8.2\nSecure implementation of deep learning systems\nMost of the research on deep learning security is concentrat-\ning on the leak of private data and the correctness of classi-\nﬁcation. As a software system, deep learning can be easily\nbuilt on mature frameworks such as TensorFlow, Torch or\nCaffe. The vulnerabilities residing in these frameworks can\nmake the constructed deep learning systems vulnerable to\nother types of attacks. The work [245] enumerates the secu-\nrity issues such as heap overﬂow, integer overﬂow and use after\nfree in these widespread frameworks. These vulnerabilities\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n18\ncan result in denial of service, control-ﬂow hijacking or sys-\ntem compromise. Moreover, deep learning systems often de-\npend on third-party libraries to provide auxiliary functions.\nFor instance, OpenCV is commonly used to process images,\nand Sound eXchange (SoX) is oftentimes used for audios.\nOnce the vulnerabilities are exploited, the attacker can cause\nmore severe losses to deep learning systems. Therefore, the\nsecurity auditing of deep learning implementation deserves\nmore research attention and efforts in the further work.\nOn the other hand, there are emerging a large number of\nresearch works that leverage deep learning to detect and ex-\nploit software vulnerabilities automatically [260] [252] [101]\n[209]. It is believed that these techniques are also applicable\nin deep learning systems. Even more, deep learning might\nhelp uncover the interpretation and ﬁx the classiﬁcation\nvulnerabilities in future.\n8.3\nHow far away from a complete black-box attack?\nBlack-box attacks are relatively more destructive as they\ndo not require much information about the target which\nlowers the cost of attack. Many works are claiming they\nare performing black-box attacks towards deep learning\nsystems [203] [198] [100]. But it is not clear that whether they\nare feasible on a large number of models and systems, and\nwhat is the gap between these works with the real world\nattack.\nAccording to the surveyed results, we ﬁnd that many\nblack-box attacks still assume that some information is\naccessible. For example, [220] has to know what exact model\nis running as well as its model structure before successfully\nstealing out the model parameters. [203] conducts a mem-\nbership inference attack built on the fact that the statistics\nof training data is publicly known and similar data with\nthe same distribution can be easily synthesized. However,\nthese conditions may be difﬁcult to satisfy the real world,\nand a complete black-box attack is rarely seen in the recent\nresearch.\nAnother difﬁculty of a complete black-box attack stems\nfrom the protection measures performed by deep learning\nsystems: 1) query limit. Commercial deep learning systems\nusually set a limit for service requests that prevents sub-\nstitute model training. In [107], PRADA can detect model\nextraction attacks based on characteristic distribution of\nqueries. 2) uncharted defense deployment. Besides not fully\ntangible models, a black-box attacker also cannot infer how\nthe defense is deployed and conﬁgured at the backend.\nThese defenses may block a malicious request [154] [148],\ncreate misleading results [107] and dynamically change or\nenhance their abilities [226] [220]. Due to the extreme imbal-\nance of knowledge between attackers and defenders, all of\nthe above measures can avoid black-box attacks efﬁciently\nand effectively.\n8.4\nRelationship between interpretability and security\nThe development of interpretability can help us better un-\nderstand the underlying principles of all these attacks. Since\nthe neural network was born, it has the problem of low\ninterpretability. A small change of model parameters may\naffect the prediction results drastically. People also cannot\ndirectly understand how neural network operates. Recently,\ninterpretability has become an urgent ﬁeld in deep learning.\nIn May of 2018, GDPR is announced to protect the privacy\nof personal data and it requires interpretability when using\nAI algorithms [3]. How to deeply understand the neural\nnetwork itself, and explain how the output is affected by\nthe input are all problems that need to be solved urgently.\nInterpretability mainly refers to the ability to explain the\nlogic behind every decision/judgment made by AI and how\nto trust these decisions [222]. It mainly includes rationality,\ntraceability, and understandability [109]. Rationality means\nbeing able to understand the reasoning behind each pre-\ndiction. Traceability refers to the ability to track predictive\nprocesses, which can be derived from the logic of mathe-\nmatical algorithms [110] [233]. Understandability refers to\na complete understanding of the model on which decisions\nare based.\nAt present, some work is being conducted on secu-\nrity and robustness proofs, usually against adversarial at-\ntack [233]. Deeper work requires to explain the reasons for\nprediction results, making training and prediction processes\nare no longer in black-box.\nKantchelian et al. [109] suggested that system designers\nneed to broaden the classiﬁcation goal into an explanatory\ngoal and deepen interaction with human operators to ad-\ndress the challenge of adversarial drift. Reluplex [110] can\nprove in which situations, small perturbations to inputs\ncannot cause misclassiﬁcation. The main idea is the lazy\nhandling of ReLU constraints. It temporarily ignores ReLU\nconstraints and tries to solve the linear part of problems.\nAs a development, Wang et al. [233] presented ReluVal\nto do formal security analysis of neural networks using\nsymbolic intervals. They proposed a new direction for\nformally checking security properties without Satisﬁability\nModulo Theory. They leveraged symbolic interval algorithm\nto compute rigorous bounds on DNN outputs through min-\nimizing over-estimations. AI2 [69] attempts to do abstract\ninterpretation in AI systems, and tries to prove the secu-\nrity and robustness of neural networks. They constructed\nalmost all perturbations, made them propagate automat-\nically, and captured the behavior of convolutional layers,\nmax pooling layers and fully connected layers. They also\nsolved the state space explosion problem. DeepStellar [59]\ncharacterizes RNN internal behaviors by modeling a RNN\nas an abstract state transition system. They design two trace\nsimilarity metrics to analyze RNNs quantitatively and also\ndetect AEs with very small perturbations.\nThe interpretability cannot only bring security, but also\nuncover the mystery of neural network and make us under-\nstand its working mechanism easily. However, this is also\nbeneﬁcial to attackers. They can exclude the range of input\nproved secure, thus reducing the retrieval space and ﬁnd-\ning AEs more efﬁciently. They can also construct targeted\nattacks through an in-depth understanding on models. In\nspite of this, this ﬁeld should not be stagnant. Because a\nblack-box model does not guarantee security [205]. There-\nfore, with the improvement of interpretability, deep learning\nsecurity may rise in a zigzag way.\nThe development of interpretability is also conductive to\nsolving the hysteresis of defensive methods. Since we have\nnot yet achieved a deep understanding of DNN (it is not\nclear why a record is predicted to the result, and how dif-\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n19\nferent data affect model parameters), ﬁnding vulnerabilities\nfor attack is easier than preventing in advance. So there is a\ncertain lag in deep learning security. If we can understand\nmodels thoroughly, it is believed that defense will precede\nor synchronize with attack [110] [233] [69].\n8.5\nDiscrimination in AI\nAI system may seem rational, neutral and unbiased, but\nactually, AI and algorithmic decisions can lead to unfair\nand discrimination [34]. For example, amazon’s AI hiring\ntool taught itself that male candidates were preferable [82].\nThere are also discrimination in crime prevention, online\nshops [34], bank loan [5], and so on. There are two main\nreasons causing AI discrimination [5]: 1) Imbalanced train-\ning data; 2)Training data reﬂects past discrimination.\nIn order to solve this problem and make AI system better\nbeneﬁt humans, what we need to do is: 1) balancing dataset,\nby adding/removing data about under/over represented\nsubsets. 2) modifying data or trained model where training\ndata reﬂects past discrimination [5]; 3) importing testing\ntechniques to test the fairness of models, such as sym-\nbolic execution and local interpretability [11]; 4) enacting\nnon-discrimination law, and data protection law, such as\nGDPR [3].\n8.6\nCorresponding defense methods\nThere is a line of approaches for preventing the aforemen-\ntioned attacks.\nModel extraction defense. Blurring the prediction results\nis an effective way to prevent model stealing, for instance,\nrounding parameters [226] [220], adding noise into class\nprobabilities [122] [107]. On the other hand, detecting and\nprevent abnormal queries can also resolve this attack. Ke-\nsarwani et al. [111] recorded all requests made by clients\nand calculated the explored feature space to detect attack.\nPRADA [107] detected attack based on sudden changes in\nthe distribution of samples submitted by a given customer.\nOrekondy et al. [168] proposed an active defense which per-\nturbs predictions targeted at attacking the training objective.\nModel inversion defense. To defend with model inversion\nattacks, researchers propose the following approaches:\n• Differential privacy (DP), which is a cryptographic scheme\ndesigned to maximize the accuracy of data queries while\nminimizing the opportunity to identify their records when\nquerying from a statistical database [61]. Individual fea-\ntures are removed to preserve user privacy. It is ﬁrst\nproposed in [60] and proved to be effective in privacy\npreservation in database. In model privacy preserving, DP\nstrategy can be applied to model parameters [180], predic-\ntion outputs [46] [83] [229] [269] [97], loss function [112]\n[214], and gradients [207] [27] [214] [10] [269] [273]. Yu et\nal. [261] propose concentrated DP to analyze and optimize\nprivacy loss.\n• Homomorphic encryption (HE), which is an encryption func-\ntion and enables the following two operations are value-\nequivalent [191]: exercising arithmetic operations ⊕on the\nring of plain text and encrypting the result, encrypting\noperators ﬁrst and then carry on the same arithmetic\noperations, i.e., En(x) ⊕En(y) = En(x + y). In this\nway, clients can encrypt their data and then send it to\nMLaaS. The server returns encrypted predictions without\nlearning anything about the plain data. In the meantime,\nthe clients have no idea about the model attributes [70]\n[136] [108] [105]. BAYHENN [249] uses HE to protect the\nclient data, and uses Bayesian neural network to protect\nDNN weights, realizing secure DNN inference.\n• Secure multi-party computation (SMC), stemming from\nYao’s Millionaires’ problem [257] and enabling a safe\ncalculation of contract functions without trusted third\nparties. In the context of deep learning, it extends to that\nmultiple parties collectively train a model and preserve\ntheir own data [224] [202] [181] [182] [188]. As such,\nthe training data cannot be easily inferred by attackers\nresiding at either computing servers or the client side.\nHelen [275] is a cooperative learning system that allows\nmultiple parties to train a linear model without revealing\ndata. DCOP [215] can protect privacy under the assump-\ntion of an honest majority and is not affected by collusion.\n• Training reconstitution. Cao et al. [42] put forward machine\nunlearning, which makes ML models completely forget a\npiece of training data and recover the effects to models\nand features. Ohrimenko et al. [165] proposed a data-\noblivious machine learning algorithm. Osia et al. [169]\nbroke down large, complex deep models to enable scal-\nable and privacy-preserving analytics by removing sensi-\ntive information with a feature extractor. MemGuard [104]\nadds noise to each conﬁdence score vector predicted by\nthe target classiﬁer. Song et al. [206] ﬁnd adversarial\ndefense methods even increase the risk of target model\nagainst membership inference attack.\nPoisoning defense. Poisoning attack can be mitigated\nthrough two aspects:\n• Protecting data. This method includes avoiding data tam-\npering, denial and falsiﬁcation, and detecting poisonous\ndata [234] [145] [84]. Through perturbing inputs, Gao et\nal. [68] observed the randomness of their predicted classes\nfrom a given model. The low entropy in predicted classes\nviolates the input dependency property of a benign model\nand implies the existence of a trojan input. Olufowobi et\nal. [166] described the context of creation or modiﬁcation\nof data points to enhance trustworthiness and depend-\nability of the data. Chakarov et al. [45] evaluated the effect\nof individual data points on the performance of trained\nmodel. Baracaldo et al. [25] used source information of\ntraining data points and the transformation context to\nidentify poisonous data.\n• Protecting algorithm. This method adjusts training algo-\nrithms, e.g., robust PCA [40], robust linear regression [49]\n[132], and robust logistic regression [64]. Wang et al. [227]\ndetect poisoning techniques via input ﬁlters, neuron\npruning and machine unlearning. ABS [140] analyze the\nchanges inside the neurons to detect trojan triggers, when\nintroducing different levels of stimuli to neurons. FABA\nalgorithm [241] can eliminate outliers in the uploaded\ngradient and obtain a gradient close to the true gradient in\ndistributed learning. Qiao et al. [185] explore all possible\nbackdoor triggers space formed by the pixel values and\nremove the triggers from a backdoored model.\nAdversarial defense. As adversarial attack draws the major\nattention, defensive work is more comprehensive and ample\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n20\naccordingly. The mainstream defense approaches are as\nfollows:\n• Model robustness. Model robustness means that small per-\nturbations to the input will not cause the network to\nmisclassify. Certiﬁed robustness is an effective method\nto defend against adversarial attack. In order to verify\nthe model robustness, Anderson et al. [18] combine the\ngradient-based optimization method of AEs search with\nthe abstract-based proof search. PixelDP [121] is a certi-\nﬁed defense that scales to large networks and datasets.\nIt is based on a connection between robustness against\nAEs and differential privacy. Ma et al. [147] analyze the\ninternal structures of DNN under various attacks, and\npropose a method of extracting DNN invariants to detect\nAEs at runtime. Liu et al. [133] aim to seek certiﬁed\nadversary-free regions around data points as large as pos-\nsible. Research [204] proves that adversarial vulnerability\nof networks increases as gradients, and gradients grow\nas the input image dimension. PROVEN [237] provides\nprobability certiﬁcates of the neural network robustness\nwhen the input perturbation obeys the distribution char-\nacteristics. For improving the provable error bound, Ro-\nbustra [125] utilizes the adversarial space to solve the min-\nmax game between attackers and defenders.\n• Adversarial training. This method selects AEs as part of\nthe training dataset to make trained model learn charac-\nteristics of AEs [93] [118] [103] [157]. Furthermore, En-\nsemble Adversarial Training [218] contained each turbine\ninput transferred from other pre-trained models. Wang et\nal. [228] introduce adversarial noise to the output em-\nbedding layer while training neural language models.\nYe et al. [259] propose a framework for simultaneous\nadversarial training and weights pruning, which can com-\npress the model while maintaining robustness. Wang et\nal. [232] propose bilateral adversarial training, which both\nperturbs both the image and the label. Zhang et al. [266]\ngenerate adversarial images for training by feature scat-\ntering in the latent space. Wong et al. [238] successfully\ntrained robust models using a weaker and cheaper adver-\nsary, which saves much time. Li et al. [128] proved that\nadversarial training indeed promotes robustness through\ntheoretical insights.\n• Region-based method. Understanding properties of adver-\nsarial regions and using more robust region-based classiﬁ-\ncation could also defend adversarial attack. Cao et al. [41]\ndevelop DNNs using region-based classiﬁcation instead\nof point-based. They predicted labels through randomly\nselecting several points from the hypercube centered at\nthe testing sample. In [171], the classiﬁer mapped normal\nsamples to the neighborhood of low-dimensional mani-\nfolds in the ﬁnal-layer hidden space. Local Intrinsic Di-\nmensionality [148] characterized dimensional properties\nof adversarial regions and evaluated the spatial ﬁll capa-\nbility. Background Class [151] added a large and diverse\nclass of background images into datasets.\n• Transformation. Transforming inputs can defend adversar-\nial attack to a large extent. Song et al. [208] found that AEs\nmainly lay in the low probability regions of the training\nregions. So they puriﬁed an AE by moving it back towards\nthe distribution adaptively. Guo et al. [78] explored model-\nagnostic defenses on image-classiﬁcation systems by im-\nage transformations. Xie et al. [247] used randomization at\ninference time, including random resizing and padding.\nTian et al. [216] considered that AEs are more sensitive to\ncertain image transformation operations, such as rotation\nand shifting, than normal images. Wang et al. [231] [230]\nthought AEs are more sensitive to random perturbations\nthan normal. Buckman et al. [37] used thermometer code\nand one-hot code discretization to increase the robustness\nof network to AEs. Kou et al. [115] trained a separate\nlightweight distribution classiﬁer to recognize different\nfeatures of transformed images.\n• Gradient regularization/masking. This method hides gradi-\nents or reduces the sensitivity of models. Madry et al. [149]\nrealized it by optimizing a saddle point formulation,\nwhich included solving an inner maximization solved and\nan outer minimization. Ross et al. [195] trained differen-\ntiable models that penalized the degree to inﬁnitesimal\nchanges in inputs.\n• Distillation. Papernot et al. [172] proposed Defensive Dis-\ntillation, which could successfully mitigate AEs con-\nstructed by FGSM and JSMA. Papernot et al. [178] also\nused the knowledge extracted in distillation to reduce the\nmagnitude of network gradient. Liu et al. [143] propose\nfeature distillation, a JPEG-based defensive compression\nframework to rectify AEs.\n• Data preprocessing. Liang et al. [129] introduced scalar\nquantization and smooth spatial ﬁltering to reduce the\neffect of perturbations. Zantedeschi et al. [264] used\nbounded ReLU activation function for hedging forward\npropagation of adversarial perturbation. Xu et al. [253]\nproposed feature squeezing methods, including reducing\nthe depth of color bit on each pixel and spatial smoothing.\nYang et al. [255] preprocess images by randomly removing\npixels from the image, and using matrix estimation to\nreconstruct it.\n• Defense network. Some studies use networks to automati-\ncally ﬁght against AEs. Gu et al. [75] used deep contractive\nnetwork with contractive autoencoders and denoising\nautoencoders, which can remove amounts of adversarial\nnoise. Akhtar et al. [12] proposed a perturbation rectifying\nnetwork as pre-input layers to defend against UAPs. Mag-\nNet [154] used detector networks to detect AEs which are\nfar from the boundary of manifold, and used a reformer\nto reform AEs which are close to the boundary. Liu et\nal. [131] propose a defense model which uses feature\nprioritization of the nonlinear attention module and the\nL2 feature regularization.\n8.7\nFuture direction of attack and defense\nIt is an endless war between attackers and defenders, and\nneither of them can win an absolute victory. But both\nsides can research new techniques and applications to gain\nadvantages. From the attacker’s point of view, one effective\nway is to explore new attack surfaces, ﬁnd out new attack\nscenarios, seek for new attack purposes and broaden the\nscope of attack effects. In particular, main attack surfaces on\ndeep learning systems include malformed operational in-\nput, malformed training data and malformed models [245].\nIn adversary attack, Lp-distance is not an ideal mea-\nsurement. Some images with big perturbations are still\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n21\nindistinguishable for humans. However, unlike Lp-distance,\nthere is no standard measure for large Lp perturbations.\nThis will be a hot point for adversarial learning in future.\nIn model extraction attack, stealing functionality of complex\nmodels needs massive queries. How to come up with a\nbetter method to reduce the number of queries in order of\nmagnitude will be the focus of this ﬁeld.\nThe balance of attack cost and beneﬁt is also an impor-\ntant factor. Some attacks, even can achieve fruitful targets,\nhave to perform costly computation or resources [220]. For\nexample, in [203], the attacker has to train a number of\nshadow models that simulate the target model, and then\nundertake membership inference. They need 156 queries to\nproduce a data point on average.\nAttack cost and attack beneﬁt are a trade-off pro-\ncess [152]. Generally, the cost of attack contains time, com-\nputation resources, acquired knowledge, and monetary ex-\npense. The beneﬁt from an attack include economic pay-\nback, rivals’ failure and so forth. In this study, we will not\ngive a uniform formula to quantify the cost and beneﬁt\nas the importance of each element is varying in different\nscenarios. Nevertheless, it is usually modeled as an opti-\nmization problem where the cost is minimized while the\nbeneﬁt is maximized, like a min-max game [162].\nAs for defenders, a combination of multiple defense\ntechniques is a good choice to reduce the risk of being at-\ntacked. But the combination may incur additional overhead\non the system that should be solved in design. For example,\nin [136] [108], they adopted a mixed protocol combining\nHE and MPC, which improved performance but with high\nbandwidth.\n9\nCONCLUSION\nIn this paper, we conduct a comprehensive and extensive\ninvestigation on attacks towards deep learning systems. Dif-\nferent from other surveys, we dissect an attack in a system-\natical way, where interested readers can clearly understand\nhow these attacks happen step by step. We have compared\nthe investigated works on their attack vectors and proposed\na number of metrics to compare their performance. Based\non the comparison, we then proceed to distill a number of\ninsights, disclosing advantages and disadvantages of attack\nmethods, limitations and trends. The discussion covering\nthe difﬁculties of these attacks in the physical world, secu-\nrity concerns in other aspects and potential mitigation for\nthese attacks provide a platform on which future research\ncan be based.\nREFERENCES\n[1]\nDrebin\ndataset.\nhttps:// www. sec. tu- bs. de/ ∼danarp/\ndrebin/, 2016.\n[2]\nImagenet dataset. http:// www. image- net. org, 2017.\n[3]\nGeneral data protection regulation. https:// gdpr- info. eu, May\n2018.\n[4]\nGtsrb dataset. http:// benchmark. ini. rub. de/? section=gtsrb&\nsubsection=dataset, 2019.\n[5]\nHuman bias and discrimination in ai systems.\nhttps:// ai-\nauditingframework. blogspot. com/ 2019/ 06/ human- bias-\nand- discrimination- in- ai. html, 2019.\n[6]\nIjb-a dataset. https:// www. nist. gov/ itl/ iad/ image- group/\nijb- dataset- request- form, 2019.\n[7]\nImdb review dataset.\nhttps:// www. kaggle. com/ utathya/\nimdb- review- dataset, 2019.\n[8]\nMicrosoft\nkaggle\ndataset.\nhttps:// www. kaggle. com/ c/\nmicrosoft- malware- prediction, 2019.\n[9]\nMozilla common voice. https:// voice. mozilla. org/ en, 2019.\n[10]\nM. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov,\nK. Talwar, and L. Zhang. Deep learning with differential privacy.\nIn Proceedings of the ACM Conference on Computer and Communica-\ntions Security (CCS), Vienna, Austria, 2016, pages 308–318, October\n24-28, 2016.\n[11]\nA. Aggarwal, P. Lohia, S. Nagar, K. Dey, and D. Saha.\nBlack\nbox fairness testing of machine learning models. In Proceedings\nof the ACM Joint Meeting on European Software Engineering Con-\nference and Symposium on the Foundations of Software Engineering,\nESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-30, 2019.,\npages 625–635.\n[12]\nN. Akhtar, J. Liu, and A. S. Mian.\nDefense against universal\nadversarial perturbations. CoRR, abs/1711.05929, 2017.\n[13]\nN. Akhtar and A. S. Mian. Threat of adversarial attacks on deep\nlearning in computer vision: A survey.\nIEEE Access, 6:14410–\n14430, 2018.\n[14]\nA. Al-Dujaili, A. Huang, E. Hemberg, and U. O’Reilly. Adversar-\nial deep learning for robust detection of binary encoded malware.\nIn IEEE Security and Privacy Workshops, San Francisco, CA, USA,\npages 76–82, May 24, 2018.\n[15]\nAlexey, I. J. Goodfellow, and S. Bengio. Adversarial examples in\nthe physical world. CoRR, abs/1607.02533, 2016.\n[16]\nS. Alfeld, X. Zhu, and P. Barford. Data poisoning attacks against\nautoregressive models. In Proceedings of the Thirtieth AAAI Confer-\nence on Artiﬁcial Intelligence, Phoenix, Arizona, USA., pages 1452–\n1458, February 12-17, 2016.\n[17]\nD. Amodei, C. Olah, J. Steinhardt, P. F. Christiano, J. Schul-\nman, and D. Man´e.\nConcrete problems in AI safety.\nCoRR,\nabs/1606.06565, 2016.\n[18]\nG. Anderson, S. Pailoor, I. Dillig, and S. Chaudhuri. Optimization\nand abstraction: a synergistic approach for analyzing neural\nnetwork robustness.\nIn Proceedings of the 40th ACM SIGPLAN\nConference on Programming Language Design and Implementation,\nPLDI 2019, Phoenix, AZ, USA, June 22-26, 2019, pages 731–744.\n[19]\nG. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and\nG. Felici. Hacking smart machines with smarter ones: How to\nextract meaningful data from machine learning classiﬁers. IJSN,\n10(3):137–150, 2015.\n[20]\nA. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Synthesizing\nrobust adversarial examples. In Proceedings of the 35th Interna-\ntional Conference on Machine Learning (ICML), Stockholmsm¨assan,\nStockholm, Sweden, pages 284–293, July 10-15, 2018.\n[21]\nA. Awad, J. Traub, and S. Sakr. Adaptive watermarks: A concept\ndrift-based approach for predicting event-time progress in data\nstreams.\nIn 22nd International Conference on Extending Database\nTechnology (EDBT), Lisbon, Portugal, pages 622–625, March 26-29,\n2019.\n[22]\nH. Bae, J. Jang, D. Jung, H. Jang, H. Ha, and S. Yoon. Security\nand privacy issues in deep learning. CoRR, abs/1807.11655, 2018.\n[23]\nE. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov.\nHow to backdoor federated learning. In The 23rd International\nConference on Artiﬁcial Intelligence and Statistics, AISTATS 2020,\n26-28 August 2020, Online [Palermo, Sicily, Italy], pages 2938–2948.\n[24]\nS. Baluja and I. Fischer. Learning to attack: Adversarial trans-\nformation networks.\nIn Proceedings of the Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, New Orleans, Louisiana, USA,\nFebruary 2-7, 2018.\n[25]\nN. Baracaldo, B. Chen, H. Ludwig, and J. A. Safavi. Mitigating\npoisoning attacks on machine learning models: A data prove-\nnance based approach. In Proceedings of the 10th ACM Workshop\non Artiﬁcial Intelligence and Security, AISec@CCS, Dallas, TX, USA,\npages 103–110, November 3, 2017.\n[26]\nM. Barreno, B. Nelson, A. D. Joseph, and J. D. Tygar. The security\nof machine learning. Machine Learning, 81(2):121–148, Nov 2010.\n[27]\nR. Bassily, A. D. Smith, and A. Thakurta. Private empirical risk\nminimization: Efﬁcient algorithms and tight error bounds.\nIn\n55th IEEE Annual Symposium on Foundations of Computer Science,\nFOCS, Philadelphia, PA, USA, pages 464–473, October 18-21, 2014.\n[28]\nV.\nBeal.\nWhat\nis\nstructured\ndata?\nwebopedia\ndeﬁni-\ntion.\nhttps:// www. webopedia. com/ TERM/ S/ structured\ndata. html, Aug. 2018.\n[29]\nA. N. Bhagoji, S. Chakraborty, P. Mittal, and S. B. Calo. Analyzing\nfederated learning through an adversarial lens. In Proceedings of\nthe 36th International Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA, pages 634–643.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n22\n[30]\nA. Bhattad, M. J. Chong, K. Liang, B. Li, and D. A. Forsyth.\nUnrestricted adversarial examples via semantic manipulation. In\nInternational Conference on Learning Representations, 2020.\n[31]\nB. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against\nsupport vector machines. In Proceedings of the 29th International\nConference on Machine Learning, ICML, Edinburgh, Scotland, UK,\nJune 26 - July 1, 2012.\n[32]\nB. Biggio, I. Pillai, S. R. Bul`o, D. Ariu, M. Pelillo, and F. Roli. Is\ndata clustering in adversarial settings secure? In AISec’13, Pro-\nceedings of the ACM Workshop on Artiﬁcial Intelligence and Security,\nCo-located with CCS, Berlin, Germany, pages 87–98, November 4,\n2013.\n[33]\nA. Bojchevski and S. G¨unnemann. Adversarial attacks on node\nembeddings via graph poisoning.\nIn Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA, pages 695–704.\n[34]\nF.\nZ.\nBorgesius.\nDiscrimination,\nartiﬁcial\nintelligence,\nand\nalgorithmic\ndecision-making.\nhttps:// rm. coe. int/\ndiscrimination- artiﬁcial- intelligence- and- algorithmic-\ndecision- making/ 1680925d73, 2018.\n[35]\nM. Br¨uckner and T. Scheffer. Nash equilibria of static prediction\ngames. In 23rd Annual Conference on Neural Information Process-\ning Systems, Vancouver, British Columbia, Canada, pages 171–179,\nDecember 7-10, 2009.\n[36]\nS. Bubeck, Y. T. Lee, E. Price, and I. P. Razenshteyn. Adversarial\nexamples from computational constraints. In Proceedings of the\n36th International Conference on Machine Learning, ICML 2019, 9-15\nJune 2019, Long Beach, California, USA, pages 831–840.\n[37]\nJ. Buckman, A. Roy, C. Raffel, and I. Goodfellow. Thermometer\nencoding: One hot way to resist adversarial examples. In Interna-\ntional Conference on Learning Representations, 2018.\n[38]\nJ. Buolamwini and T. Gebru. Gender shades: Intersectional accu-\nracy disparities in commercial gender classiﬁcation. In Conference\non Fairness, Accountability and Transparency, FAT, New York, NY,\nUSA, pages 77–91, February 23-24, 2018.\n[39]\nC. Burkard and B. Lagesse. Analysis of causative attacks against\nsvms learning from data streams.\nIn Proceedings of the 3rd\nACM on International Workshop on Security And Privacy Analytics,\nIWSPA@CODASPY, Scottsdale, Arizona, USA, pages 31–36, March\n24, 2017.\n[40]\nE. J. Cand`es, X. Li, Y. Ma, and J. Wright.\nRobust principal\ncomponent analysis? J. ACM, 58(3):11:1–11:37, 2011.\n[41]\nX. Cao and N. Z. Gong. Mitigating evasion attacks to deep neural\nnetworks via region-based classiﬁcation.\nIn Proceedings of the\n33rd Annual Computer Security Applications Conference, Orlando,\nFL, USA, pages 278–287, December 4-8, 2017.\n[42]\nY. Cao and J. Yang. Towards making systems forget with machine\nunlearning. In IEEE Symposium on Security and Privacy, SP, San\nJose, CA, USA, pages 463–480, May 17-21, 2015.\n[43]\nN. Carlini and D. A. Wagner. Towards evaluating the robustness\nof neural networks. In IEEE Symposium on Security and Privacy\n(SP), pages 39–57, 2017.\n[44]\nN. Carlini and D. A. Wagner.\nAudio adversarial examples:\nTargeted attacks on speech-to-text. In IEEE Security and Privacy\nWorkshops, SP Workshops, San Francisco, CA, USA, pages 1–7, May\n24, 2018.\n[45]\nA. Chakarov, A. V. Nori, S. K. Rajamani, S. Sen, and D. Vi-\njaykeerthy.\nDebugging\nmachine\nlearning\ntasks.\nCoRR,\nabs/1603.07292, 2016.\n[46]\nK. Chaudhuri and C. Monteleoni. Privacy-preserving logistic re-\ngression. In Proceedings of the Twenty-Second Annual Conference on\nNeural Information Processing Systems, Vancouver, British Columbia,\nCanada, pages 289–296, December 8-11, 2008.\n[47]\nH. Chen, H. Zhang, D. S. Boning, and C. Hsieh. Robust decision\ntrees against adversarial examples.\nIn Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA, pages 1122–1131.\n[48]\nP. Chen, Y. Sharma, H. Zhang, J. Yi, and C. Hsieh. EAD: elastic-\nnet attacks to deep neural networks via adversarial examples.\nIn Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, New Orleans, Louisiana, USA, pages 10–17, February\n2-7, 2018.\n[49]\nY. Chen, C. Caramanis, and S. Mannor. Robust high dimensional\nsparse regression and matching pursuit. CoRR, abs/1301.2725,\n2013.\n[50]\nM. Cheng, T. Le, P. Chen, H. Zhang, J. Yi, and C. Hsieh. Query-\nefﬁcient hard-label black-box attack: An optimization-based ap-\nproach. In 7th International Conference on Learning Representations,\nICLR, New Orleans, LA, USA, May 6-9, 2019.\n[51]\nS. Cheng, Y. Dong, T. Pang, H. Su, and J. Zhu. Improving black-\nbox adversarial attacks with a transfer-based prior. In Advances\nin Neural Information Processing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS 2019, 8-14\nDecember 2019, Vancouver, BC, Canada, pages 10932–10942.\n[52]\nY. Cheng, L. Jiang, and W. Macherey.\nRobust neural machine\ntranslation with doubly adversarial inputs. In Proceedings of the\n57th Conference of the Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, pages 4324–4333.\n[53]\nK. T. Co, L. Mu˜noz-Gonz´alez, S. de Maupeou, and E. C. Lupu.\nProcedural noise adversarial examples for black-box attacks on\ndeep convolutional networks.\nIn Proceedings of the 2019 ACM\nSIGSAC Conference on Computer and Communications Security, CCS\n2019, London, UK, November 11-15, 2019, pages 275–289.\n[54]\nJ. R. C. da Silva, R. F. Berriel, C. Badue, A. F. de Souza, and\nT. Oliveira-Santos.\nCopycat CNN: stealing knowledge by per-\nsuading confession with random non-labeled data. In Interna-\ntional Joint Conference on Neural Networks, IJCNN, Rio de Janeiro,\nBrazil, pages 1–8, July 8-13, 2018.\n[55]\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training\nof deep bidirectional transformers for language understanding.\nIn Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies (NAACL-HLT), pages 4171–4186, 2019.\n[56]\nE. Dohmatob. Generalized no free lunch theorem for adversarial\nrobustness.\nIn Proceedings of the 36th International Conference\non Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, pages 1646–1654.\n[57]\nY. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li.\nBoosting adversarial attacks with momentum. In IEEE Conference\non Computer Vision and Pattern Recognition, CVPR, Salt Lake City,\nUT, USA, pages 9185–9193, June 18-22, 2018.\n[58]\nY. Dong, T. Pang, H. Su, and J. Zhu. Evading defenses to trans-\nferable adversarial examples by translation-invariant attacks. In\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019, pages 4312–4321.\n[59]\nX. Du, X. Xie, Y. Li, L. Ma, Y. Liu, and J. Zhao. Deepstellar: model-\nbased quantitative analysis of stateful deep learning systems. In\nProceedings of the ACM Joint Meeting on European Software Engi-\nneering Conference and Symposium on the Foundations of Software\nEngineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-\n30, 2019., pages 477–487.\n[60]\nC. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor.\nOur data, ourselves: Privacy via distributed noise generation. In\n25th Annual International Conference on the Theory and Applications\nof Cryptographic Techniques, St. Petersburg, Russia, pages 486–503,\nMay 28-June 1, 2006.\n[61]\nC. Dwork, F. McSherry, K. Nissim, and A. D. Smith. Calibrating\nnoise to sensitivity in private data analysis. In Theory of Cryptog-\nraphy, Third Theory of Cryptography Conference, TCC, New York, NY,\nUSA, pages 265–284, March 4-7, 2006.\n[62]\nL. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry.\nExploring the landscape of spatial robustness. In Proceedings of\nthe 36th International Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA, pages 1802–1811.\n[63]\nM. Fang, X. Cao, J. Jia, and N. Gong.\nLocal model poison-\ning attacks to byzantine-robust federated learning.\nIn 29th\nUSENIX Security Symposium (USENIX Security 20), pages 1605–\n1622. USENIX Association, Aug. 2020.\n[64]\nJ. Feng, H. Xu, S. Mannor, and S. Yan. Robust logistic regression\nand classiﬁcation.\nIn Annual Conference on Neural Information\nProcessing Systems, Montreal, Quebec, Canada, pages 253–261, De-\ncember 8-13, 2014.\n[65]\nM. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks\nthat exploit conﬁdence information and basic countermeasures.\nIn Proceedings of the 22nd ACM SIGSAC Conference on Computer\nand Communications Security, Denver, CO, USA, pages 1322–1333,\nOctober 12-16, 2015.\n[66]\nK. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov.\nProperty inference attacks on fully connected neural networks\nusing permutation invariant representations.\nIn Proceedings of\nthe ACM SIGSAC Conference on Computer and Communications\nSecurity, CCS, Toronto, ON, Canada, pages 619–633, October 15-\n19, 2018.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n23\n[67]\nJ. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi. Black-box generation\nof adversarial text sequences to evade deep learning classiﬁers. In\nIEEE Security and Privacy Workshops, SP Workshops, San Francisco,\nCA, USA, pages 50–56, May 24, 2018.\n[68]\nY. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal.\nSTRIP: a defence against trojan attacks on deep neural networks.\nIn Proceedings of the 35th Annual Computer Security Applications\nConference, ACSAC 2019, San Juan, PR, USA, December 09-13, 2019,\npages 113–125.\n[69]\nT. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaud-\nhuri, and M. T. Vechev. AI2: safety and robustness certiﬁcation of\nneural networks with abstract interpretation. In IEEE Symposium\non Security and Privacy, SP, San Francisco, CA, USA, pages 3–18,\nMay 21-23, 2018.\n[70]\nR. Gilad-Bachrach, N. Dowlin, K. Laine, K. E. Lauter, M. Naehrig,\nand J. Wernsing. Cryptonets: Applying neural networks to en-\ncrypted data with high throughput and accuracy. In Proceedings of\nthe 33nd International Conference on Machine Learning, ICML 2016,\nNew York City, NY, USA, June 19-24, 2016, pages 201–210, 2016.\n[71]\nA. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell.\nAdversarial policies: Attacking deep reinforcement learning. In\nInternational Conference on Learning Representations, 2020.\n[72]\nY. Gong, B. Li, C. Poellabauer, and Y. Shi. Real-time adversarial\nattacks. In Proceedings of the Twenty-Eighth International Joint Con-\nference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August\n10-16, 2019, pages 4672–4680.\n[73]\nY. Gong and C. Poellabauer. Crafting adversarial examples for\nspeech paralinguistics applications. CoRR, abs/1711.03280, 2017.\n[74]\nI. J. Goodfellow, J. Shlens, and C. Szegedy.\nExplaining and\nharnessing adversarial examples. CoRR, abs/1412.6572, 2014.\n[75]\nS. Gu and L. Rigazio. Towards deep neural network architectures\nrobust to adversarial examples. CoRR, abs/1412.5068, 2014.\n[76]\nT. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg. Badnets: Evaluating\nbackdooring attacks on deep neural networks.\nIEEE Access,\n7:47230–47244, 2019.\n[77]\nC. Guo, J. S. Frank, and K. Q. Weinberger.\nLow frequency\nadversarial perturbation. CoRR, abs/1809.08758, 2018.\n[78]\nC. Guo, M. Rana, M. Ciss´e, and L. van der Maaten.\nCoun-\ntering adversarial images using input transformations.\nCoRR,\nabs/1711.00117, 2017.\n[79]\nJ. Guo, Y. Jiang, Y. Zhao, Q. Chen, and J. Sun. Dlfuzz: differential\nfuzzing testing of deep learning systems.\nIn Proceedings of the\n2018 ACM Joint Meeting on European Software Engineering Con-\nference and Symposium on the Foundations of Software Engineering,\nESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November\n04-09, 2018, pages 739–743.\n[80]\nY. Guo, Z. Yan, and C. Zhang.\nSubspace attack: Exploiting\npromising subspaces for query-efﬁcient black-box attacks.\nIn\nAdvances in Neural Information Processing Systems 32: Annual\nConference on Neural Information Processing Systems 2019, NeurIPS\n2019, 8-14 December 2019, Vancouver, BC, Canada, pages 3820–3829.\n[81]\nI. Hagestedt, Y. Zhang, M. Humbert, P. Berrang, H. Tang,\nX. Wang, and M. Backes. Mbeacon: Privacy-preserving beacons\nfor DNA methylation data. In 26th Annual Network and Distributed\nSystem Security Symposium, NDSS 2019, San Diego, California,\nUSA, February 24-27, 2019.\n[82]\nI. A. Hamilton. Amazon built an ai tool to hire people but had\nto shut it down because it was discriminating against women.\nhttps:// www. businessinsider. com/ amazon- built- ai- to-\nhire- people- discriminated- against- women- 2018- 10,\nOct.\n2018.\n[83]\nJ. Hamm, Y. Cao, and M. Belkin.\nLearning privately from\nmultiparty data. In Proceedings of the 33nd International Conference\non Machine Learning, ICML, New York City, NY, USA, pages 555–\n563, June 19-24, 2016.\n[84]\nR. Hasan, R. Sion, and M. Winslett.\nThe case of the fake\npicasso: Preventing history forgery with secure provenance. In\n7th USENIX Conference on File and Storage Technologies, February\n24-27, 2009, San Francisco, CA, USA. Proceedings, pages 1–14, 2009.\n[85]\nJ. Hayes and G. Danezis.\nLearning universal adversarial per-\nturbations with generative models. In IEEE Security and Privacy\nWorkshops, SP Workshops, San Francisco, CA, USA, pages 43–49,\nMay 24, 2018.\n[86]\nJ. Hayes, L. Melis, G. Danezis, and E. D. Cristofaro. LOGAN:\nevaluating privacy leakage of generative models using genera-\ntive adversarial networks. CoRR, abs/1705.07663, 2017.\n[87]\nW. He, B. Li, and D. Song.\nDecision boundary analysis of\nadversarial examples.\nIn International Conference on Learning\nRepresentations, 2018.\n[88]\nZ. He, T. Zhang, and R. B. Lee. Model inversion attacks against\ncollaborative inference. In Proceedings of the 35th Annual Computer\nSecurity Applications Conference, ACSAC 2019, San Juan, PR, USA,\nDecember 09-13, 2019.\n[89]\nB. Hitaj, G. Ateniese, and F. P´erez-Cruz.\nDeep models under\nthe GAN: information leakage from collaborative deep learning.\nIn Proceedings of the ACM SIGSAC Conference on Computer and\nCommunications Security, CCS, Dallas, TX, USA, pages 603–618,\nOctober 30-November 03, 2017.\n[90]\nW. Hu and Y. Tan. Black-box attacks against RNN based malware\ndetection algorithms. In The Workshops of the The Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, New Orleans, Louisiana,\nUSA, pages 245–251, February 2-7,.\n[91]\nW. Hu and Y. Tan. Generating adversarial malware examples for\nblack-box attacks based on GAN. CoRR, abs/1702.05983, 2017.\n[92]\nW. Hua, Z. Zhang, and G. E. Suh. Reverse engineering convolu-\ntional neural networks through side-channel information leaks.\nIn Proceedings of the 55th Annual Design Automation Conference,\nDAC, San Francisco, CA, USA, pages 4:1–4:6, June 24-29, 2018.\n[93]\nR. Huang, B. Xu, D. Schuurmans, and C. Szepesv´ari. Learning\nwith a strong adversary. CoRR, abs/1511.03034, 2015.\n[94]\nS. H. Huang, N. Papernot, I. J. Goodfellow, Y. Duan, and\nP. Abbeel. Adversarial attacks on neural network policies. CoRR,\nabs/1702.02284, 2017.\n[95]\nW. Huang and J. W. Stokes.\nMtnet: A multi-task neural net-\nwork for dynamic malware classiﬁcation.\nIn 13th International\nConference, Detection of Intrusions and Malware, and Vulnerability\nAssessment, DIMVA, San Sebasti´an, Spain, pages 399–418, July 7-8,\n2016.\n[96]\nX. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo,\nM. Wu, and X. Yi.\nA survey of safety and trustworthiness of\ndeep neural networks: Veriﬁcation, testing, adversarial attack and\ndefence, and interpretability. Comput. Sci. Rev., 37:100270, 2020.\n[97]\nN. Hynes, R. Cheng, and D. Song. Efﬁcient deep learning on\nmulti-source private data. CoRR, abs/1807.06689, 2018.\n[98]\nA. Ilyas, L. Engstrom, A. Athalye, and J. Lin.\nQuery-efﬁcient\nblack-box adversarial examples. CoRR, abs/1712.07113, 2017.\n[99]\nA. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Black-box adversar-\nial attacks with limited queries and information. In Proceedings\nof the 35th International Conference on Machine Learning, ICML,\nStockholmsm¨assan, Stockholm, Sweden, pages 2142–2151, 2018.\n[100] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and\nB. Li.\nManipulating machine learning: Poisoning attacks and\ncountermeasures for regression learning. In IEEE Symposium on\nSecurity and Privacy, SP, San Francisco, California, USA, pages 19–\n35, May 21-23, 2018.\n[101] S. Jan, A. Panichella, A. Arcuri, and L. C. Briand.\nAutomatic\ngeneration of tests to exploit XML injection vulnerabilities in web\napplications. IEEE Trans. Software Eng., 45(4):335–362, 2019.\n[102] U. Jang, X. Wu, and S. Jha.\nObjective metrics and gradient\ndescent algorithms for adversarial examples in machine learning.\nIn Proceedings of the 33rd Annual Computer Security Applications\nConference, Orlando, FL, USA, December 4-8, 2017, pages 262–277.\n[103] Y. Jang, T. Zhao, S. Hong, and H. Lee. Adversarial defense via\nlearning to generate diverse attacks.\nIn 2019 IEEE/CVF Inter-\nnational Conference on Computer Vision, ICCV 2019, Seoul, Korea\n(South), October 27 - November 2, 2019, pages 2740–2749.\n[104] J. Jia, A. Salem, M. Backes, Y. Zhang, and N. Z. Gong. Memguard:\nDefending against black-box membership inference attacks via\nadversarial examples. In Proceedings of the 2019 ACM SIGSAC\nConference on Computer and Communications Security, CCS 2019,\nLondon, UK, November 11-15, 2019, pages 259–274.\n[105] X. Jiang, M. Kim, K. E. Lauter, and Y. Song. Secure outsourced\nmatrix computation and application to neural networks.\nIn\nProceedings of the 2018 ACM SIGSAC Conference on Computer and\nCommunications Security, CCS 2018, Toronto, ON, Canada, October\n15-19, 2018, pages 1209–1222, 2018.\n[106] A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov. Bag of tricks\nfor efﬁcient text classiﬁcation. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers, pages 427–431. Association for\nComputational Linguistics, April 2017.\n[107] M. Juuti, S. Szyller, A. Dmitrenko, S. Marchal, and N. Asokan.\nPRADA: protecting against DNN model stealing attacks. CoRR,\nabs/1805.02628, 2018.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n24\n[108] C. Juvekar, V. Vaikuntanathan, and A. Chandrakasan. GAZELLE:\nA low latency framework for secure neural network inference.\nIn 27th USENIX Security Symposium, USENIX Security, Baltimore,\nMD, USA, pages 1651–1669, August 15-17, 2018.\n[109] A. Kantchelian, S. Afroz, L. Huang, A. C. Islam, B. Miller, M. C.\nTschantz, R. Greenstadt, A. D. Joseph, and J. D. Tygar.\nAp-\nproaches to adversarial drift. In Proceedings of the ACM Workshop\non Artiﬁcial Intelligence and Security, AISec, Berlin, Germany, pages\n99–110, November 4, 2013.\n[110] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochender-\nfer. Towards proving the adversarial robustness of deep neural\nnetworks.\nIn Proceedings First Workshop on Formal Veriﬁcation\nof Autonomous Vehicles, FVAV@iFM, Turin, Italy, 19th September.,\npages 19–26, 2017.\n[111] M. Kesarwani, B. Mukhoty, V. Arya, and S. Mehta.\nModel\nextraction warning in mlaas paradigm. CoRR, abs/1711.07221,\n2017.\n[112] D. Kifer, A. D. Smith, and A. Thakurta. Private convex optimiza-\ntion for empirical risk minimization with applications to high-\ndimensional regression. In The 25th Annual Conference on Learning\nTheory, COLT, Edinburgh, Scotland, pages 25.1–25.40, June 25-27,\n2012.\n[113] J. Kim, R. Feldt, and S. Yoo.\nGuiding deep learning system\ntesting using surprise adequacy. In Proceedings of the 41st Interna-\ntional Conference on Software Engineering, ICSE 2019, Montreal, QC,\nCanada, May 25-31, 2019, pages 1039–1049.\n[114] J. Kos, I. Fischer, and D. Song. Adversarial examples for genera-\ntive models. In IEEE Security and Privacy Workshops, SP Workshops,\nSan Francisco, CA, USA, May 24, 2018, pages 36–42.\n[115] C. Kou, H. K. Lee, E.-C. Chang, and T. K. Ng.\nEnhancing\ntransformation-based defenses against adversarial attacks with\na distribution classiﬁer.\nIn International Conference on Learning\nRepresentations, 2020.\n[116] F. Kreuk, A. Barak, S. Aviv-Reuven, M. Baruch, B. Pinkas, and\nJ. Keshet. Deceiving end-to-end deep learning malware detectors\nusing adversarial examples. 2018.\n[117] A. Krizhevsky, V. Nair, and G. Hinton. CIFAR dataset. https://\nwww. cs. toronto. edu/ ∼kriz/ cifar. html, 2019.\n[118] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial machine\nlearning at scale. CoRR, abs/1611.01236, 2016.\n[119] K. Kurita, P. Michel, and G. Neubig. Weight poisoning attacks\non pretrained models. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 2793–2806.\n[120] Y. LeCun, C. Cortes, and C. Burges. Mnist dataset. http:// yann.\nlecun. com/ exdb/ mnist/, 2017.\n[121] M. L´ecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana.\nCertiﬁed robustness to adversarial examples with differential\nprivacy. In 2019 IEEE Symposium on Security and Privacy, SP 2019,\nSan Francisco, CA, USA, May 19-23, 2019, pages 656–672.\n[122] T. Lee, B. Edwards, I. Molloy, and D. Su.\nDefending against\nmodel stealing attacks using deceptive perturbations.\nCoRR,\nabs/1806.00054, 2018.\n[123] J. Li, S. Ji, T. Du, B. Li, and T. Wang. Textbugger: Generating\nadversarial text against real-world applications. In 26th Annual\nNetwork and Distributed System Security Symposium, NDSS 2019,\nSan Diego, California, USA, February 24-27, 2019.\n[124] J. Li, F. R. Schmidt, and J. Z. Kolter. Adversarial camera stickers:\nA physical camera-based attack on deep learning systems.\nIn\nProceedings of the 36th International Conference on Machine Learning,\nICML 2019, 9-15 June 2019, Long Beach, California, USA, pages\n3896–3904.\n[125] L. Li, Z. Zhong, B. Li, and T. Xie. Robustra: Training provable\nrobust neural networks over reference adversarial space.\nIn\nProceedings of the Twenty-Eighth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019,\npages 4711–4717.\n[126] P. Li, J. Yi, and L. Zhang.\nQuery-efﬁcient black-box attack by\nactive learning. In IEEE International Conference on Data Mining,\nICDM , Singapore, November 17-20, 2018, pages 1200–1205.\n[127] S. Li, A. Neupane, S. Paul, C. Song, S. V. Krishnamurthy, A. K.\nRoy-Chowdhury, and A. Swami.\nStealthy adversarial pertur-\nbations against real-time video classiﬁcation systems.\nIn 26th\nAnnual Network and Distributed System Security Symposium, NDSS\n2019, San Diego, California, USA, February 24-27, 2019.\n[128] Y. Li, E. X.Fang, H. Xu, and T. Zhao. Implicit bias of gradient\ndescent based adversarial training on separable data. In Interna-\ntional Conference on Learning Representations, 2020.\n[129] B. Liang, H. Li, M. Su, X. Li, W. Shi, and X. Wang. Detecting\nadversarial image examples in deep networks with adaptive\nnoise reduction. 2017.\n[130] R.\nLight.\nAi\ntrends:\nMachine\nlearning\nas\na\nservice\n(mlaas).\nhttps:// learn. g2. com/ trends/ machine- learning-\nservice- mlaas, Jan. 2018.\n[131] C. Liu and J. J´aJ´a. Feature prioritization and regularization im-\nprove standard accuracy and adversarial robustness. In Proceed-\nings of the Twenty-Eighth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages\n2994–3000.\n[132] C. Liu, B. Li, Y. Vorobeychik, and A. Oprea. Robust linear regres-\nsion against training data poisoning. In Proceedings of the 10th\nACM Workshop on Artiﬁcial Intelligence and Security, AISec@CCS\n2017, Dallas, TX, USA, November 3, 2017, pages 91–102, 2017.\n[133] C. Liu, R. Tomioka, and V. Cevher. On certifying non-uniform\nbounds against adversarial attacks.\nIn Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA, pages 4072–4081.\n[134] F. Liu and N. B. Shroff. Data poisoning attacks on stochastic ban-\ndits. In Proceedings of the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,\npages 4042–4050.\n[135] H. Liu, R. Ji, J. Li, B. Zhang, Y. Gao, Y. Wu, and F. Huang.\nUniversal adversarial perturbation via prior driven uncertainty\napproximation.\nIn 2019 IEEE/CVF International Conference on\nComputer Vision, ICCV 2019, Seoul, Korea (South), October 27 -\nNovember 2, 2019, pages 2941–2949.\n[136] J. Liu, M. Juuti, Y. Lu, and N. Asokan. Oblivious neural network\npredictions via minionn transformations.\nIn Proceedings of the\nACM SIGSAC Conference on Computer and Communications Secu-\nrity, CCS, Dallas, TX, USA, October 30 - November 03, 2017, pages\n619–631.\n[137] K. S. Liu, B. Li, and J. Gao. Generative model: Membership attack,\ngeneralization and diversity. CoRR, abs/1805.09898, 2018.\n[138] Q. Liu, P. Li, W. Zhao, W. Cai, S. Yu, and V. C. M. Leung. A\nsurvey on security threats and defensive techniques of machine\nlearning: A data driven view. IEEE Access, 6:12103–12117, 2018.\n[139] X. Liu, S. Si, J. Zhu, Y. Li, and C. Hsieh. A uniﬁed framework for\ndata poisoning attack to graph-based semi-supervised learning.\nIn Advances in Neural Information Processing Systems 32: Annual\nConference on Neural Information Processing Systems 2019, NeurIPS\n2019, 8-14 December 2019, Vancouver, BC, Canada, pages 9777–9787.\n[140] Y. Liu, W. Lee, G. Tao, S. Ma, Y. Aafer, and X. Zhang.\nABS:\nscanning neural networks for back-doors by artiﬁcial brain stim-\nulation.\nIn Proceedings of the 2019 ACM SIGSAC Conference on\nComputer and Communications Security, CCS 2019, London, UK,\nNovember 11-15, 2019, pages 1265–1282.\n[141] Y. Liu, S. Ma, Y. Aafer, W. Lee, J. Zhai, W. Wang, and X. Zhang.\nTrojaning attack on neural networks.\nIn 25th Annual Network\nand Distributed System Security Symposium, NDSS 2018, San Diego,\nCalifornia, USA, February 18-21, 2018, 2018.\n[142] Y. Liu, S. Moosavi-Dezfooli, and P. Frossard. A geometry-inspired\ndecision-based attack. In 2019 IEEE/CVF International Conference\non Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -\nNovember 2, 2019, pages 4889–4897.\n[143] Z. Liu, Q. Liu, T. Liu, N. Xu, X. Lin, Y. Wang, and W. Wen. Feature\ndistillation: Dnn-oriented JPEG compression against adversarial\nexamples.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019,\npages 860–868.\n[144] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A.\nGunter, and K. Chen. Understanding membership inferences on\nwell-generalized learning models. CoRR, abs/1802.04889, 2018.\n[145] J. Lyle and A. P. Martin.\nTrusted computing and provenance:\nBetter together.\nIn 2nd Workshop on the Theory and Practice of\nProvenance, TaPP’10, San Jose, CA, USA, February 22, 2010, 2010.\n[146] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su,\nL. Li, Y. Liu, J. Zhao, and Y. Wang. Deepgauge: multi-granularity\ntesting criteria for deep learning systems. In Proceedings of the\n33rd ACM/IEEE International Conference on Automated Software\nEngineering, ASE 2018, Montpellier, France, September 3-7, 2018,\npages 120–131.\n[147] S. Ma, Y. Liu, G. Tao, W. Lee, and X. Zhang.\nNIC: detecting\nadversarial samples with neural network invariant checking. In\n26th Annual Network and Distributed System Security Symposium,\nNDSS 2019, San Diego, California, USA, February 24-27, 2019.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n25\n[148] X. Ma, B. Li, Y. Wang, S. M. Erfani, S. N. R. Wijewickrema, M. E.\nHoule, G. Schoenebeck, D. Song, and J. Bailey. Characterizing\nadversarial subspaces using local intrinsic dimensionality. CoRR,\nabs/1801.02613, 2018.\n[149] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.\nTowards deep learning models resistant to adversarial attacks.\nCoRR, abs/1706.06083, 2017.\n[150] S. Mahloujifar, M. Mahmoody, and A. Mohammed. Data poi-\nsoning attacks in multi-party learning. In Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA, pages 4274–4283.\n[151] M. McCoyd and D. A. Wagner. Background class defense against\nadversarial examples. In 2018 IEEE Security and Privacy Work-\nshops, SP Workshops 2018, San Francisco, CA, USA, May 24, 2018,\npages 96–102, 2018.\n[152] S. Mei and X. Zhu. Using machine teaching to identify optimal\ntraining-set attacks on machine learners.\nIn Proceedings of the\nTwenty-Ninth AAAI Conference on Artiﬁcial Intelligence, January 25-\n30, 2015, Austin, Texas, USA., pages 2871–2877, 2015.\n[153] L. Melis, C. Song, E. D. Cristofaro, and V. Shmatikov. Exploiting\nunintended feature leakage in collaborative learning.\nIn 2019\nIEEE Symposium on Security and Privacy, SP 2019, San Francisco,\nCA, USA, May 19-23, 2019, pages 691–706.\n[154] D. Meng and H. Chen. Magnet: A two-pronged defense against\nadversarial examples. In Proceedings of the ACM SIGSAC Confer-\nence on Computer and Communications Security, CCS, Dallas, TX,\nUSA, October 30 - November 03, 2017, pages 135–147.\n[155] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Uni-\nversal adversarial perturbations.\nIn 2017 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI,\nUSA, July 21-26, 2017, pages 86–94, 2017.\n[156] S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard.\nDeepfool: A\nsimple and accurate method to fool deep neural networks. In\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR,\nLas Vegas, NV, USA, June 27-30, 2016, pages 2574–2582.\n[157] C. K. Mummadi, T. Brox, and J. H. Metzen. Defending against\nuniversal perturbations with shared adversarial training. In 2019\nIEEE/CVF International Conference on Computer Vision, ICCV 2019,\nSeoul, Korea (South), October 27 - November 2, 2019, pages 4927–\n4936.\n[158] L. Mu˜noz-Gonz´alez, B. Biggio, A. Demontis, A. Paudice, V. Won-\ngrassamee, E. C. Lupu, and F. Roli.\nTowards poisoning of\ndeep learning algorithms with back-gradient optimization.\nIn\nProceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and\nSecurity, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017,\npages 27–38, 2017.\n[159] N. Narodytska and S. P. Kasiviswanathan.\nSimple black-box\nadversarial attacks on deep neural networks.\nIn IEEE Confer-\nence on Computer Vision and Pattern Recognition Workshops, CVPR\nWorkshops, Honolulu, HI, USA, July 21-26, 2017, pages 1310–1318.\n[160] M. Naseer, S. H. Khan, M. H. Khan, F. S. Khan, and F. Porikli.\nCross-domain transferability of adversarial perturbations.\nIn\nAdvances in Neural Information Processing Systems 32: Annual\nConference on Neural Information Processing Systems 2019, NeurIPS\n2019, 8-14 December 2019, Vancouver, BC, Canada, pages 12885–\n12895.\n[161] M. Nasr, R. Shokri, and A. Houmansadr. Comprehensive privacy\nanalysis of deep learning: Passive and active white-box inference\nattacks against centralized and federated learning. In 2019 IEEE\nSymposium on Security and Privacy, SP 2019, San Francisco, CA,\nUSA, May 19-23, 2019, pages 739–753.\n[162] M. Nasr, R. Shokri, and A. Houmansadr.\nMachine learning\nwith membership privacy using adversarial regularization.\nIn\nProceedings of the 2018 ACM SIGSAC Conference on Computer and\nCommunications Security (CCS), pages 634–646, 2018.\n[163] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein,\nU. Saini, C. A. Sutton, J. D. Tygar, and K. Xia. Exploiting machine\nlearning to subvert your spam ﬁlter. In First USENIX Workshop\non Large-Scale Exploits and Emergent Threats, LEET, 2008.\n[164] S. J. Oh, M. Augustin, M. Fritz, and B. Schiele. Towards reverse-\nengineering black-box neural networks. In International Confer-\nence on Learning Representations, 2018.\n[165] O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin,\nK. Vaswani, and M. Costa. Oblivious multi-party machine learn-\ning on trusted processors. In 25th USENIX Security Symposium,\nUSENIX Security 16, Austin, TX, USA, August 10-12, 2016., pages\n619–636, 2016.\n[166] H. Olufowobi, R. Engel, N. Baracaldo, L. A. D. Bathen, S. Tata,\nand H. Ludwig. Data provenance model for internet of things\n(iot) systems. In Service-Oriented Computing, ICSOC 2016 Work-\nshops, Banff, AB, Canada, October 10-13, 2016., pages 85–91.\n[167] T. Orekondy, B. Schiele, and M. Fritz.\nKnockoff nets: Stealing\nfunctionality of black-box models. June 2019.\n[168] T. Orekondy, B. Schiele, and M. Fritz.\nPrediction poisoning:\nTowards defenses against dnn model stealing attacks. In Inter-\nnational Conference on Learning Representations, 2020.\n[169] S. A. Ossia, A. S. Shamsabadi, A. Taheri, H. R. Rabiee, N. D.\nLane, and H. Haddadi. A Hybrid Deep Learning Architecture\nfor Privacy-Preserving Mobile Analytics. CoRR, abs/1703.02952,\n2017.\n[170] R. Pan. Static deep neural network analysis for robustness. In\nProceedings of the ACM Joint Meeting on European Software Engi-\nneering Conference and Symposium on the Foundations of Software\nEngineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-\n30, 2019., pages 1238–1240.\n[171] T. Pang, C. Du, and J. Zhu.\nRobust deep learning via re-\nverse cross-entropy training and thresholding test.\nCoRR,\nabs/1706.00633, 2017.\n[172] N. Papernot and P. D. McDaniel. On the effectiveness of defen-\nsive distillation. CoRR, abs/1607.05113, 2016.\n[173] N. Papernot, P. D. McDaniel, and I. J. Goodfellow. Transferability\nin machine learning: from phenomena to black-box attacks using\nadversarial samples. CoRR, abs/1605.07277, 2016.\n[174] N. Papernot, P. D. McDaniel, I. J. Goodfellow, S. Jha, Z. B. Celik,\nand A. Swami.\nPractical black-box attacks against machine\nlearning.\nIn Proceedings of the 2017 ACM on Asia Conference\non Computer and Communications Security, AsiaCCS, Abu Dhabi,\nUnited Arab Emirates, April 2-6, 2017, pages 506–519.\n[175] N. Papernot, P. D. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik,\nand A. Swami. The limitations of deep learning in adversarial\nsettings.\nIn IEEE European Symposium on Security and Privacy,\nEuroS&P 2016, Saarbr¨ucken, Germany, March 21-24, 2016, pages\n372–387, 2016.\n[176] N. Papernot, P. D. McDaniel, A. Sinha, and M. P. Wellman. Sok:\nSecurity and privacy in machine learning. In 2018 IEEE European\nSymposium on Security and Privacy, EuroS&P 2018, London, United\nKingdom, April 24-26, 2018, pages 399–414, 2018.\n[177] N. Papernot, P. D. McDaniel, A. Swami, and R. E. Harang. Craft-\ning adversarial input sequences for recurrent neural networks.\nIn 2016 IEEE Military Communications Conference, MILCOM 2016,\nBaltimore, MD, USA, November 1-3, 2016, pages 49–54, 2016.\n[178] N. Papernot, P. D. McDaniel, X. Wu, S. Jha, and A. Swami.\nDistillation as a defense to adversarial perturbations against deep\nneural networks. In IEEE Symposium on Security and Privacy, SP\n2016, San Jose, CA, USA, May 22-26, 2016, pages 582–597, 2016.\n[179] R. Pascanu, J. W. Stokes, H. Sanossian, M. Marinescu, and\nA. Thomas. Malware classiﬁcation with recurrent networks. In\n2015 IEEE International Conference on Acoustics, Speech and Signal\nProcessing, ICASSP 2015, South Brisbane, Queensland, Australia,\nApril 19-24, 2015, pages 1916–1920, 2015.\n[180] N. Phan, M. N. Vu, Y. Liu, R. Jin, D. Dou, X. Wu, and M. T.\nThai.\nHeterogeneous gaussian mechanism: Preserving differ-\nential privacy in deep learning with provable robustness.\nIn\nProceedings of the Twenty-Eighth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019,\npages 4753–4759.\n[181] L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai. Privacy-\npreserving deep learning via additively homomorphic encryp-\ntion. IEEE Trans. Information Forensics and Security, 13(5):1333–\n1345, 2018.\n[182] L. T. Phong and T. T. Phuong. Privacy-preserving deep learning\nfor any activation function. CoRR, abs/1809.03272, 2018.\n[183] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes, M.-L.\nShyu, S.-C. Chen, and S. S. Iyengar. A survey on deep learn-\ning: Algorithms, techniques, and applications. ACM Computing\nSurveys, 51(5):92:1–92:36, Sept. 2018.\n[184] A. Pyrgelis, C. Troncoso, and E. D. Cristofaro.\nKnock knock,\nwho’s there? membership inference on aggregate location data.\n2017.\n[185] X. Qiao, Y. Yang, and H. Li.\nDefending neural backdoors\nvia generative distribution modeling.\nIn Advances in Neural\nInformation Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, 8-14 December\n2019, Vancouver, BC, Canada, pages 14004–14013.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n26\n[186] Y. Qin, N. Carlini, G. W. Cottrell, I. J. Goodfellow, and C. Raffel.\nImperceptible, robust, and targeted adversarial examples for au-\ntomatic speech recognition. In Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, pages 5231–5240.\n[187] A. Ranjan, J. Janai, A. Geiger, and M. J. Black. Attacking optical\nﬂow.\nIn 2019 IEEE/CVF International Conference on Computer\nVision, ICCV 2019, Seoul, Korea (South), October 27 - November 2,\n2019, pages 2404–2413.\n[188] D. Reich, A. Todoki, R. Dowsley, M. D. Cock, and A. C. A.\nNascimento.\nPrivacy-preserving classiﬁcation of personal text\nmessages with secure multi-party computation. In Advances in\nNeural Information Processing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS 2019, 8-14\nDecember 2019, Vancouver, BC, Canada, pages 3752–3764.\n[189] S. Ren, Y. Deng, K. He, and W. Che.\nGenerating natural lan-\nguage adversarial examples through probability weighted word\nsaliency. In Proceedings of the 57th Conference of the Association for\nComputational Linguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, pages 1085–1097.\n[190] M. Rigaki and S. Garcia.\nBringing a GAN to a knife-ﬁght:\nAdapting malware communication to avoid detection. In 2018\nIEEE Security and Privacy Workshops, SP Workshops 2018, San\nFrancisco, CA, USA, May 24, 2018, pages 70–75, 2018.\n[191] R. L. Rivest, L. Adleman, M. L. Dertouzos, et al. On data banks\nand privacy homomorphisms. Foundations of secure computation,\n4(11):169–180, 1978.\n[192] R. Romagnoli, S. Weerakkody, and B. Sinopoli. A model inversion\nbased watermark for replay attack detection with output track-\ning. In 2019 American Control Conference, ACC 2019, Philadelphia,\nPA, USA, July 10-12, 2019, pages 384–390.\n[193] I. Rosenberg, A. Shabtai, Y. Elovici, and L. Rokach. Low resource\nblack-box end-to-end attack against state of the art API call based\nmalware classiﬁers. CoRR, abs/1804.08778, 2018.\n[194] I. Rosenberg, A. Shabtai, L. Rokach, and Y. Elovici. Generic black-\nbox end-to-end attack against state of the art API call based\nmalware classiﬁers.\nIn 21st International Symposium, Research\nin Attacks, Intrusions, and Defenses, RAID 2018, Heraklion, Crete,\nGreece, September 10-12, 2018, Proceedings, pages 490–510.\n[195] A. S. Ross and F. Doshi-Velez. Improving the adversarial robust-\nness and interpretability of deep neural networks by regularizing\ntheir input gradients.\nIn Proceedings of the Thirty-Second AAAI\nConference on Artiﬁcial Intelligence (AAAI), pages 1660–1669, 2018.\n[196] A. Sablayrolles, M. Douze, C. Schmid, Y. Ollivier, and H. J´egou.\nWhite-box vs black-box: Bayes optimal strategies for membership\ninference. In Proceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,\nUSA, pages 5558–5567.\n[197] A. Salem, A. Bhattacharya, M. Backes, M. Fritz, and Y. Zhang.\nUpdates-leak: Data set inference and reconstruction attacks in\nonline learning. In 29th USENIX Security Symposium (USENIX\nSecurity 20), pages 1291–1308. USENIX Association, Aug. 2020.\n[198] A. Salem, Y. Zhang, M. Humbert, M. Fritz, and M. Backes.\nMl-leaks: Model and data independent membership inference\nattacks and defenses on machine learning models.\nCoRR,\nabs/1806.01246, 2018.\n[199] M. Sato, J. Suzuki, and S. Kiyono.\nEffective adversarial regu-\nlarization for neural machine translation.\nIn Proceedings of the\n57th Conference of the Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, pages 204–210.\n[200] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Du-\nmitras, and T. Goldstein.\nPoison frogs! targeted clean-label\npoisoning attacks on neural networks.\nCoRR, abs/1804.00792,\n2018.\n[201] M. Sharif, L. Bauer, and M. K. Reiter. On the suitability of lp-\nnorms for creating and preventing adversarial examples. In 2018\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR)\nWorkshops, Salt Lake City, UT, USA, June 18-22, 2018, pages 1605–\n1613.\n[202] R. Shokri and V. Shmatikov. Privacy-preserving deep learning. In\nProceedings of the 22nd ACM SIGSAC Conference on Computer and\nCommunications Security, Denver, CO, USA, October 12-16, 2015,\npages 1310–1321, 2015.\n[203] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership\ninference attacks against machine learning models. In 2017 IEEE\nSymposium on Security and Privacy, SP 2017, San Jose, CA, USA,\nMay 22-26, 2017, pages 3–18, 2017.\n[204] C. Simon-Gabriel, Y. Ollivier, L. Bottou, B. Sch¨olkopf, and\nD. Lopez-Paz. First-order adversarial vulnerability of neural net-\nworks and input dimension. In Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, pages 5809–5817.\n[205] C. Song, T. Ristenpart, and V. Shmatikov.\nMachine learning\nmodels that remember too much. In Proceedings of ACM SIGSAC\nConference on Computer and Communications Security, CCS 2017,\nDallas, TX, USA, October 30 - November 03, 2017, pages 587–601.\n[206] L. Song, R. Shokri, and P. Mittal.\nPrivacy risks of securing\nmachine learning models against adversarial examples. In Pro-\nceedings of the 2019 ACM SIGSAC Conference on Computer and\nCommunications Security, CCS 2019, London, UK, November 11-15,\n2019, pages 241–257.\n[207] S. Song, K. Chaudhuri, and A. D. Sarwate. Stochastic gradient\ndescent with differentially private updates. In IEEE Global Con-\nference on Signal and Information Processing, GlobalSIP 2013, Austin,\nTX, USA, December 3-5, 2013, pages 245–248, 2013.\n[208] Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman. Pixelde-\nfend: Leveraging generative models to understand and defend\nagainst adversarial examples. CoRR, abs/1710.10766, 2017.\n[209] A. Stasinopoulos, C. Ntantogian, and C. Xenakis.\nCommix:\nautomating evaluation and exploitation of command injection\nvulnerabilities in web applications. Int. J. Inf. Sec., 18(1):49–72,\n2019.\n[210] J. Steinhardt, P. W. Koh, and P. S. Liang. Certiﬁed defenses for\ndata poisoning attacks. In Annual Conference on Neural Information\nProcessing Systems, 4-9 December 2017, Long Beach, CA, USA, pages\n3520–3532, 2017.\n[211] Y. Sun, M. Wu, W. Ruan, X. Huang, M. Kwiatkowska, and\nD. Kroening.\nConcolic testing for deep neural networks.\nIn\nProceedings of the 33rd ACM/IEEE International Conference on Auto-\nmated Software Engineering, ASE 2018, Montpellier, France, Septem-\nber 3-7, 2018, pages 109–119.\n[212] Z. Sun, P. Kairouz, A. T. Suresh, and H. B. McMahan. Can you\nreally backdoor federated learning? CoRR, abs/1911.07963, 2019.\n[213] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J.\nGoodfellow, and R. Fergus.\nIntriguing properties of neural\nnetworks. CoRR, abs/1312.6199, 2013.\n[214] K. Talwar, A. Thakurta, and L. Zhang.\nPrivate empirical risk\nminimization beyond the worst case: The effect of the constraint\nset geometry. CoRR, abs/1411.5417, 2014.\n[215] T. Tassa, T. Grinshpoun, and A. Yanai.\nA privacy preserving\ncollusion secure DCOP algorithm. In Proceedings of the Twenty-\nEighth International Joint Conference on Artiﬁcial Intelligence, IJCAI\n2019, Macao, China, August 10-16, 2019, pages 4774–4780.\n[216] S. Tian, G. Yang, and Y. Cai.\nDetecting adversarial examples\nthrough image transformation. In Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, New Orleans, Louisiana,\nUSA, February 2-7, 2018, pages 4139–4146, 2018.\n[217] Y. Tian, K. Pei, S. Jana, and B. Ray. Deeptest: automated testing\nof deep-neural-network-driven autonomous cars. In Proceedings\nof the 40th International Conference on Software Engineering, ICSE\n2018, Gothenburg, Sweden, May 27 - June 03, 2018, pages 303–314.\n[218] F. Tram`er, A. Kurakin, N. Papernot, D. Boneh, and P. D. Mc-\nDaniel.\nEnsemble adversarial training: Attacks and defenses.\nCoRR, abs/1705.07204, 2017.\n[219] F. Tram`er, N. Papernot, I. J. Goodfellow, D. Boneh, and P. D.\nMcDaniel. The space of transferable adversarial examples. CoRR,\nabs/1704.03453, 2017.\n[220] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart.\nStealing machine learning models via prediction apis.\nIn 25th\nUSENIX Security Symposium, USENIX Security 16, Austin, TX,\nUSA, August 10-12, 2016., pages 601–618, 2016.\n[221] S. Truex, L. Liu, M. E. Gursoy, L. Yu, and W. Wei. Towards de-\nmystifying membership inference attacks. CoRR, abs/1807.09173,\n2018.\n[222] M. Veale, R. Binns, and L. Edwards.\nAlgorithms that remem-\nber: Model inversion attacks and data protection law.\nCoRR,\nabs/1807.04644, 2018.\n[223] J. S. Vitter. Random sampling with a reservoir. ACM Trans. Math.\nSoftw., 11(1):37–57, 1985.\n[224] S. Wagh, D. Gupta, and N. Chandran. Securenn: Efﬁcient and\nprivate neural network training. IACR Cryptology ePrint Archive,\n2018:442, 2018.\n[225] B. Wang and N. Z. Gong. Attacking graph-based classiﬁcation via\nmanipulating the graph structure. In Proceedings of the 2019 ACM\nSIGSAC Conference on Computer and Communications Security, CCS\n2019, London, UK, November 11-15, 2019, pages 2023–2040.\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n27\n[226] B. Wang and N. Z. Gong. Stealing hyperparameters in machine\nlearning.\nIn IEEE Symposium on Security and Privacy (SP), San\nFrancisco, California, USA, 21-23 May 2018, pages 36–52, 2018.\n[227] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and\nB. Y. Zhao. Neural cleanse: Identifying and mitigating backdoor\nattacks in neural networks. In 2019 IEEE Symposium on Security\nand Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019,\npages 707–723.\n[228] D. Wang, C. Gong, and Q. Liu.\nImproving neural language\nmodeling via adversarial training.\nIn Proceedings of the 36th\nInternational Conference on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA, pages 6555–6565.\n[229] D. Wang, M. Ye, and J. Xu.\nDifferentially private empirical\nrisk minimization revisited: Faster and more general. In Annual\nConference on Neural Information Processing Systems, Long Beach,\nCA, USA, 4-9 December 2017, pages 2719–2728, 2017.\n[230] J. Wang, G. Dong, J. Sun, X. Wang, and P. Zhang. Adversarial\nsample detection for deep neural network through model muta-\ntion testing. In Proceedings of the 41st International Conference on\nSoftware Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31,\n2019, pages 1245–1256.\n[231] J. Wang, J. Sun, P. Zhang, and X. Wang. Detecting adversarial\nsamples for deep neural networks through mutation testing.\nCoRR, abs/1805.05010, 2018.\n[232] J. Wang and H. Zhang. Bilateral adversarial training: Towards\nfast training of more robust models against adversarial attacks. In\n2019 IEEE/CVF International Conference on Computer Vision, ICCV\n2019, Seoul, Korea (South), October 27 - November 2, 2019, pages\n6628–6637.\n[233] S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana.\nFormal\nsecurity analysis of neural networks using symbolic intervals. In\n27th USENIX Security Symposium, USENIX Security 2018, Balti-\nmore, MD, USA, August 15-17, 2018., pages 1599–1614, 2018.\n[234] X. O. Wang, K. Zeng, K. Govindan, and P. Mohapatra. Chaining\nfor securing data provenance in distributed information net-\nworks.\nIn 31st IEEE Military Communications Conference, MIL-\nCOM, Orlando, FL, USA, October 29 - November 1, 2012, pages 1–6.\n[235] Y. Wang and K. Chaudhuri. Data poisoning attacks against online\nlearning. CoRR, abs/1808.08994, 2018.\n[236] X. Wei, S. Liang, N. Chen, and X. Cao. Transferable adversarial\nattacks for image and video object detection. In Proceedings of the\nTwenty-Eighth International Joint Conference on Artiﬁcial Intelligence,\nIJCAI 2019, Macao, China, August 10-16, 2019, pages 954–960.\n[237] L. Weng, P. Chen, L. M. Nguyen, M. S. Squillante, A. Boopathy,\nI. V. Oseledets, and L. Daniel. PROVEN: verifying robustness of\nneural networks with a probabilistic approach. In Proceedings of\nthe 36th International Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA, pages 6727–6736.\n[238] E. Wong, L. Rice, and J. Z. Kolter.\nFast is better than free:\nRevisiting adversarial training.\nIn International Conference on\nLearning Representations, 2020.\n[239] H. Wu, C. Wang, Y. Tyshetskiy, A. Docherty, K. Lu, and L. Zhu.\nAdversarial examples for graph data: Deep insights into attack\nand defense.\nIn Proceedings of the Twenty-Eighth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China,\nAugust 10-16, 2019, pages 4816–4823.\n[240] Z. Xi, R. Pang, S. Ji, and T. Wang.\nGraph backdoor.\nCoRR,\nabs/2006.11890, 2020.\n[241] Q. Xia, Z. Tao, Z. Hao, and Q. Li.\nFABA: an algorithm for\nfast aggregation against byzantine attacks in distributed neural\nnetworks. In Proceedings of the Twenty-Eighth International Joint\nConference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, Au-\ngust 10-16, 2019, pages 4824–4830.\n[242] H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert, and F. Roli.\nIs feature selection secure against training data poisoning?\nIn\nProceedings of the 32nd International Conference on Machine Learning,\nICML 2015, Lille, France, 6-11 July 2015, pages 1689–1698, 2015.\n[243] H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, and F. Roli.\nSupport vector machines under adversarial label contamination.\nNeurocomputing, 160:53–62, 2015.\n[244] H. Xiao, H. Xiao, and C. Eckert. Adversarial label ﬂips attack on\nsupport vector machines. In 20th European Conference on Artiﬁcial\nIntelligence (ECAI), Montpellier, France, August 27-31, 2012, pages\n870–875, 2012.\n[245] Q. Xiao, K. Li, D. Zhang, and W. Xu.\nSecurity risks in deep\nlearning implementations. In 2018 IEEE Security and Privacy (SP)\nWorkshops, San Francisco, CA, USA, May 24, 2018, pages 123–128.\n[246] C. Xie, K. Huang, P. Chen, and B. Li. DBA: distributed backdoor\nattacks against federated learning. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020.\n[247] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. L. Yuille. Mitigating ad-\nversarial effects through randomization. CoRR, abs/1711.01991,\n2017.\n[248] C. Xie, Z. Zhang, Y. Zhou, S. Bai, J. Wang, Z. Ren, and A. L.\nYuille.\nImproving transferability of adversarial examples with\ninput diversity. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019,\npages 2730–2739.\n[249] P. Xie, B. Wu, and G. Sun.\nBAYHENN: combining bayesian\ndeep learning and homomorphic encryption for secure DNN\ninference.\nIn Proceedings of the Twenty-Eighth International Joint\nConference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, Au-\ngust 10-16, 2019, pages 4831–4837.\n[250] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y. Liu, J. Zhao,\nB. Li, J. Yin, and S. See.\nDeephunter: a coverage-guided fuzz\ntesting framework for deep neural networks. In Proceedings of the\n28th ACM SIGSOFT International Symposium on Software Testing\nand Analysis, ISSTA 2019, Beijing, China, July 15-19, 2019., pages\n146–157.\n[251] K. Xu, H. Chen, S. Liu, P. Chen, T. Weng, M. Hong, and X. Lin.\nTopology attack and defense for graph neural networks: An\noptimization perspective.\nIn Proceedings of the Twenty-Eighth\nInternational Joint Conference on Artiﬁcial Intelligence, IJCAI 2019,\nMacao, China, August 10-16, 2019, pages 3961–3967.\n[252] L. Xu, W. Jia, W. Dong, and Y. Li. Automatic exploit generation\nfor buffer overﬂow vulnerabilities.\nIn 2018 IEEE International\nConference on Software Quality, Reliability and Security (QRS) Com-\npanion, Lisbon, Portugal, July 16-20, 2018, pages 463–468.\n[253] W. Xu, D. Evans, and Y. Qi.\nFeature squeezing: Detecting\nadversarial examples in deep neural networks. In 25th Annual\nNetwork and Distributed System Security Symposium, NDSS 2018,\nSan Diego, California, USA, February 18-21, 2018, 2018.\n[254] H. Yakura and J. Sakuma. Robust audio adversarial example for\na physical attack. In Proceedings of the Twenty-Eighth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China,\nAugust 10-16, 2019, pages 5334–5341.\n[255] Y. Yang, G. Zhang, Z. Xu, and D. Katabi. Me-net: Towards effec-\ntive adversarial robustness with matrix estimation. In Proceedings\nof the 36th International Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, USA, pages 7025–7034.\n[256] Z. Yang, J. Zhang, E. Chang, and Z. Liang. Neural network inver-\nsion in adversarial setting via background knowledge alignment.\nIn Proceedings of the 2019 ACM SIGSAC Conference on Computer and\nCommunications Security, CCS 2019, London, UK, November 11-15,\n2019, pages 225–240.\n[257] A. C. Yao. Protocols for secure computations (extended abstract).\nIn 23rd Annual Symposium on Foundations of Computer Science,\nChicago, Illinois, USA, 3-5 November 1982, pages 160–164, 1982.\n[258] Y. Yao, H. Li, H. Zheng, and B. Y. Zhao. Latent backdoor attacks\non deep neural networks. In Proceedings of the 2019 ACM SIGSAC\nConference on Computer and Communications Security, CCS 2019,\nLondon, UK, November 11-15, 2019, pages 2041–2055.\n[259] S. Ye, X. Lin, K. Xu, S. Liu, H. Cheng, J. Lambrechts, H. Zhang,\nA. Zhou, K. Ma, and Y. Wang. Adversarial robustness vs. model\ncompression, or both? In 2019 IEEE/CVF International Conference\non Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -\nNovember 2, 2019, pages 111–120.\n[260] W. You, P. Zong, K. Chen, X. Wang, X. Liao, P. Bian, and\nB. Liang.\nSemfuzz: Semantics-based automatic generation of\nproof-of-concept exploits. In Proceedings of the 2017 ACM SIGSAC\nConference on Computer and Communications Security (CCS), Dallas,\nTX, USA, October 30 - November 03, 2017, pages 2139–2154.\n[261] L. Yu, L. Liu, C. Pu, M. E. Gursoy, and S. Truex. Differentially\nprivate model publishing for deep learning. In 2019 IEEE Sym-\nposium on Security and Privacy, SP 2019, San Francisco, CA, USA,\nMay 19-23, 2019, pages 332–349, 2019.\n[262] X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen, S. Zhang,\nH. Huang, X. Wang, and C. A. Gunter.\nCommandersong: A\nsystematic approach for practical adversarial voice recognition.\nIn 27th USENIX Security Symposium, USENIX Security 2018, Bal-\ntimore, MD, USA, August 15-17, 2018., pages 49–64, 2018.\n[263] Y. Zang, F. Qi, C. Yang, Z. Liu, M. Zhang, Q. Liu, and M. Sun.\nWord-level textual adversarial attacking as combinatorial opti-\nmization. In Proceedings of the 58th Annual Meeting of the Asso-\nIEEE TRANSACTIONS ON SOFTWARE ENGINEERING, OCTORBER 2020\n28\nciation for Computational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 6066–6080.\n[264] V. Zantedeschi, M. Nicolae, and A. Rawat.\nEfﬁcient defenses\nagainst adversarial attacks.\nIn Proceedings of the 10th ACM\nWorkshop on Artiﬁcial Intelligence and Security, AISec@CCS 2017,\nDallas, TX, USA, November 3, 2017, pages 39–49, 2017.\n[265] X. Zeng, C. Liu, Y. Wang, W. Qiu, L. Xie, Y. Tai, C. Tang, and\nA. L. Yuille.\nAdversarial attacks beyond the image space.\nIn\nIEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019, pages 4302–4311.\n[266] H. Zhang and J. Wang.\nDefense against adversarial attacks\nusing feature scattering-based adversarial training. In Advances\nin Neural Information Processing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS 2019, 8-14\nDecember 2019, Vancouver, BC, Canada, pages 1829–1839.\n[267] H. Zhang, T. Zheng, J. Gao, C. Miao, L. Su, Y. Li, and K. Ren.\nData poisoning attack against knowledge graph embedding. In\nProceedings of the Twenty-Eighth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019,\npages 4853–4859.\n[268] H. Zhang, H. Zhou, N. Miao, and L. Li.\nGenerating ﬂuent\nadversarial examples for natural languages. In Proceedings of the\n57th Conference of the Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, pages 5564–5569.\n[269] J. Zhang, K. Zheng, W. Mou, and L. Wang.\nEfﬁcient private\nERM for smooth objectives.\nIn Proceedings of the Twenty-Sixth\nInternational Joint Conference on Artiﬁcial Intelligence, IJCAI 2017,\nMelbourne, Australia, August 19-25, 2017, pages 3922–3928, 2017.\n[270] J. M. Zhang, M. Harman, L. Ma, and Y. Liu. Machine learning\ntesting: Survey, landscapes and horizons.\nIEEE Transactions on\nSoftware Engineering, PP(99):1–1, 2020.\n[271] M. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid.\nDeeproad: Gan-based metamorphic testing and input validation\nframework for autonomous driving systems.\nIn Proceedings of\nthe 33rd ACM/IEEE International Conference on Automated Software\nEngineering, ASE 2018, Montpellier, France, September 3-7, 2018,\npages 132–142.\n[272] T. Zhang, Z. He, and R. B. Lee.\nPrivacy-preserving machine\nlearning through data obfuscation. CoRR, abs/1807.01860, 2018.\n[273] T. Zhang and Q. Zhu. A dual perturbation approach for differen-\ntial private admm-based distributed empirical risk minimization.\nIn Proceedings of ACM Workshop on Artiﬁcial Intelligence and Secu-\nrity (AISec), Vienna, Austria, October 28, 2016, pages 129–137.\n[274] Y. Zhao, H. Zhu, R. Liang, Q. Shen, S. Zhang, and K. Chen.\nSeeing isn’t believing: Towards more robust adversarial attack\nagainst real world object detectors. In Proceedings of the 2019 ACM\nSIGSAC Conference on Computer and Communications Security, CCS\n2019, London, UK, November 11-15, 2019, pages 1989–2004.\n[275] W. Zheng, R. A. Popa, J. E. Gonzalez, and I. Stoica.\nHelen:\nMaliciously secure coopetitive learning for linear models. In 2019\nIEEE Symposium on Security and Privacy, SP 2019, San Francisco,\nCA, USA, May 19-23, 2019, pages 724–738.\n[276] W. Zou, S. Huang, J. Xie, X. Dai, and J. Chen.\nA reinforced\ngeneration of adversarial examples for neural machine transla-\ntion. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020, pages\n3486–3497.\n[277] D. Z¨ugner, A. Akbarnejad, and S. G¨unnemann.\nAdversarial\nattacks on neural networks for graph data. In Proceedings of the\nTwenty-Eighth International Joint Conference on Artiﬁcial Intelligence,\nIJCAI 2019, Macao, China, August 10-16, 2019, pages 6246–6250.\n",
  "categories": [
    "cs.CR",
    "cs.LG"
  ],
  "published": "2019-11-28",
  "updated": "2020-10-27"
}