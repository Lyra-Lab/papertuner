{
  "id": "http://arxiv.org/abs/1912.00498v1",
  "title": "Optimization for Reinforcement Learning: From Single Agent to Cooperative Agents",
  "authors": [
    "Donghwan Lee",
    "Niao He",
    "Parameswaran Kamalaruban",
    "Volkan Cevher"
  ],
  "abstract": "This article reviews recent advances in multi-agent reinforcement learning\nalgorithms for large-scale control systems and communication networks, which\nlearn to communicate and cooperate. We provide an overview of this emerging\nfield, with an emphasis on the decentralized setting under different\ncoordination protocols. We highlight the evolution of reinforcement learning\nalgorithms from single-agent to multi-agent systems, from a distributed\noptimization perspective, and conclude with future directions and challenges,\nin the hope to catalyze the growing synergy among distributed optimization,\nsignal processing, and reinforcement learning communities.",
  "text": "1\nOptimization for Reinforcement Learning:\nFrom Single Agent to Cooperative Agents\nDonghwan Lee1, Niao He1, Parameswaran Kamalaruban2, and Volkan Cevher2\n1University of Illinois at Urbana-Champaign (UIUC)\n2 ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL)\nAbstract\nThis article reviews recent advances in multi-agent reinforcement learning algorithms for large-\nscale control systems and communication networks, which learn to communicate and cooperate. We\nprovide an overview of this emerging ﬁeld, with an emphasis on the decentralized setting under different\ncoordination protocols. We highlight the evolution of reinforcement learning algorithms from single-agent\nto multi-agent systems, from a distributed optimization perspective, and conclude with future directions\nand challenges, in the hope to catalyze the growing synergy among distributed optimization, signal\nprocessing, and reinforcement learning communities.\nI. INTRODUCTION\nFueled with recent advances in deep neural networks, reinforcement learning (RL) has been in the\nlimelight for many recent breakthroughs in artiﬁcial intelligence, including defeating humans in games\n(e.g., chess, Go, StarCraft), self-driving cars, smart home automation, service robots, among many others.\nDespite these remarkable achievements, many basic tasks can still elude a single RL agent. Examples\nabound from multi-player games, multi-robots, cellular antenna tilt control, trafﬁc control systems, smart\npower grids to network management.\nOften, cooperation among multiple RL agents is much more critical: multiple agents must collaborate\nto complete a common goal, expedite learning, protect privacy, offer resiliency against failures and\nadversarial attacks, and overcome the physical limitations of a single RL agent behaving alone. These tasks\nare studied under the umbrella of cooperative multi-agent RL (MARL), where agents seek to learn optimal\npolicies to maximize a shared team reward, while interacting with an unknown stochastic environment\nand with each other. Cooperative MARL is far more challenging than the single-agent case due to: i)\nthe exponentially growing search space, ii) the non-stationary and unpredictable environment caused by\narXiv:1912.00498v1  [cs.LG]  1 Dec 2019\n2\nthe agents’ concurrent yet heterogeneous behaviors, and iii) the lack of central coordinators in many\napplications. These difﬁculties can be alleviated by appropriate coordination among agents.\nThe cooperative MARL can be further categorized into subclasses depending on the information\nstructure and types of coordination, such as how much information (e.g., state, action, reward, etc.)\nis available for each agent, what kinds of information can be shared among the agents, and what kinds\nof protocols (e.g., communication networks, etc.) are used for coordination. When only local partial\nstate observation is available for each agent, the corresponding multi-agent systems are often described\nthrough decentralized partially observable Markov decision processes (MDP), or DEC-POMDP for short,\nfor which the decision problem is known to be extremely challenging. In fact, even the planning problem of\nDEC-POMDPs (with known models) is known to be NEXT-complete [1]. Despite some recent empirical\nsuccesses [2]–[4], ﬁnding an exact solution of Dec-POMDPs using RLs with theoretical guarantees\nremains an open question.\nWhen full state information is available for each agent, we call agents joint action learners (JALs)\nif they also know the joint actions of other agents, and independent learners (ILs) if agents only know\ntheir own actions. Learning tasks for ILs are still very challenging, since each agent sees other agents\nas parts of the environment, so without observing the internal states, including other agents actions, the\nproblem essentially becomes non-Markovian [5] and a partially observable MDP (POMDP). It turns out\nthat optimal policy can be found under restricted assumptions such as deterministic MDP [6], and for\ngeneral stochastic MDPs, several attempts have demonstrated empirical successes [7]–[9]. For a more\ncomprehensive survey on independent MARLs, the reader is referred to the survey [6].\nThe form of rewards, either centralized or decentralized, also makes a huge difference in multi-agent\nsystems. If every agent receives a common reward, the situation becomes relatively easy to deal with.\nFor instance, JALs can perfectly learn exact optimal policies of the underlying decision problem even\nwithout coordination among agents [10]. The more interesting and practical scenario is when rewards\nare decentralized, i.e., each agent receives its own local reward while the global reward to be maximized\nis the sum of local rewards. This decentralization is especially important when taking into account the\nprivacy and resiliency of the system.\nClearly, learning without coordination among agents is impossible under decentralized rewards. This\narticle focuses on this important subclass of cooperative MARL with decentralized rewards, assuming\nthe full state and action information is available to each agent. In particular, we consider decentralized\ncoordination through network communications characterized by graphs, where each node in the graph\nrepresents each agent and edges connecting nodes represent communication between them.\nDistributed optimization rises to the challenge by achieving global consensus on the optimal policy\n3\nthrough only local computation and communication with neighboring agents. Recently, several important\nadvances have been made in this direction such as the distributed TD-learning [11], distributed Q-\nlearning [12], distributed actor-critic algorithm [13], and other important results [14]–[17]. These works\nlargely beneﬁt from the synergistic connection between RLs and the core idea of averaging consensus-\nbased distributed optimization [18], which leverages averaging consensus protocols for information prop-\nagation over networks and rich theory established in this ﬁeld during the last decade.\nIn this survey, we provide an overview of this emerging ﬁeld with an emphasis on optimization within\nthe decentralized setting (decentralized rewards and decentralized communication protocols). For this\npurpose, we highlight the evolution of RL algorithms from single-agent to multi-agent systems, from\na distributed optimization perspective, in the hope to catalyze the growing synergy among distributed\noptimization, signal processing, and RL communities.\nIn the sequel, we ﬁrst revisit the basics of single-agent RL in Section II and extend to multi-agent RL\nin Section III. In Section IV, we provide preliminaries of distributed optimization as well as consensus\nalgorithms. In Section V, we discuss several important consensus-based MARL algorithms with decen-\ntralized network communication protocols. Finally, in Section VI, we conclude with future directions and\nopen issues. Note that our review is not exhaustive given the magazine limits; we suggest the interested\nreader to further read [6], [19], [20].\nII. SINGLE-AGENT RL BASICS\nTo understand MARL, it is imperative that we brieﬂy review the basics of single-agent RL setting,\nwhere only a single agent interacts with an unknown stochastic environment. Such environments are\nclassically represented by a Markov decision process: M := (S, A, P, r, γ), where the state-space S :=\n{1, 2, . . . , |S|} and action-space A := {1, 2, . . . , |A|}, upon selecting an action a ∈A with the current\nstate s ∈S, the state transits to s′ ∈S according to the state transition probability P(s′|s, a), and the\ntransition incurs a random reward r(s, a). For simplicity, we consider the inﬁnite-horizon (discounted)\nMarkov decision problem (MDP), where the agent sequentially takes actions to maximize cumulative\ndiscounted rewards. The goal is to ﬁnd a deterministic optimal policy, π∗: S →A, such that\nπ∗:= arg maxπ∈Θ E\n\" ∞\nX\nk=0\nγkr(sk, π(sk))\n#\n,\n(1)\nwhere γ ∈[0, 1) is the discount factor, Θ is the set of all admissible deterministic policies, and\n(s0, a0, s1, a1, . . .) is a state-action trajectory generated by the Markov chain under policy π. Solving\nMDPs involves two key concepts associated with the expected return:\n4\n1) V π(s) := E\n\u0002P∞\nk=0 γkr(sk, π(sk))|s0 = s\n\u0003\nis called the (state) value function for a given policy π,\nwhich encodes the expected cumulative reward when starting in the state s, and then, following the\npolicy π thereafter.\n2) Qπ(s, a) := E\n\u0002P∞\nk=0 γkr(sk, π(sk))|s0 = s, a0 = a\n\u0003\nis called the state-action value function or Q-\nfunction for a given policy π, which measures the expected cumulative reward when starting from\nstate s, taking the action a, and then, following the policy π.\nTheir optima over all possible policies are deﬁned by V ∗(s) := maxπ:S→A V π(s) = maxa Q∗(s, a)\nand Q∗(s, a) := maxπ:S→A Qπ(s, a), respectively. Given the optimal value functions Q∗or V ∗, the\noptimal policy π∗can be obtained by picking an action a that is greedy with respect to V ∗or Q∗,\ni.e., π∗(s) = arg maxa Es′∼P(·|s,a)[r(s, a) + γV ∗(s′)] or π∗(s) = arg maxa Q∗(s, a), respectively. When\nthe MDP instance, M, is known, then it can be solved efﬁciently via dynamic programming (DP)\nalgorithms. Based on the Markov property, the value function V π for a given policy π, satisﬁes the\nBellman equation: V π(s) = Es′∼P(·|s,π(s)) [r(s, π(s)) + γV π(s′)]. The similar property holds for Qπ\nas well. Moreover, the optimal Q-function Q∗, satisﬁes the Bellman optimality equation, Q∗(s, a) =\nEs′∼P(·|s,a) [r(s, a) + maxa′ γQ∗(s′, a′)]. Various DP algorithms, such as the policy and value iterations,\nare obtained by turning the Bellman equations into update rules.\nA. Classical RL Algorithms\nMany classical RL algorithms can be viewed as stochastic variants of DPs. This insight will be key\nfor scaling MARL in the sequel. The temporal-difference (TD) learning is a fundamental RL algorithm\nto estimate the value function of a given policy π (called as policy evaluation method):\nVk+1(sk) = Vk(sk) + αk(r(sk, π(sk)) + γVk(sk+1) −Vk(sk)),\n(2)\nwhere sk ∼dπ, sk+1 ∼P(·|sk, π(sk)), dπ denotes the stationary state distribution under policy π, and\nαk is the learning rate (or step-size). For any ﬁxed policy π, TD update converges to V π almost surely\n(i.e., with probability 1) if the step-size satisﬁes the so-called Robbins-Monro rule, P∞\nk=0 αk = ∞,\nP∞\nk=0 α2\nk < ∞[21]. Although theoretically sound, the naive TD learning is only applicable to small-\nscale problems as it needs to store and enumerate values of all states. However, most practical problems\nwe face in the real-world have large state-space. In such cases, enumerating all values in a table is\nnumerically inefﬁcient or even intractable.\nUsing function approximations resolves this problem by encoding the value function with a param-\neterized function class, V (·) ∼= V (·; θ). The simplest example is the linear function approximation,\n5\nV (·; θ) = Φθ, where Φ = [φ(1); · · · ; φ(|S|)]⊤∈R|S|×n is a feature matrix, and φ : S →R is a pre-\nselected feature mapping. TD learning update with linear function approximation is written as follows\nθk+1 = θk + αk(r(sk, π(sk)) + γφ(sk+1)T θk −φ(sk)T θk)φ(sk).\n(3)\nThe above update is known to converge to θ∗almost surely [22], where θ∗is the solution to the projected\nBellman equation, provided that the Markov chain with transition matrix P π (state transition probability\nmatrix under policy π) is ergodic and the step-size satisﬁes the Robbins-Monro rule. Finite sample analysis\nof the TD learning algorithm is only recently established in [23]–[25]. Besides the standard TD, there\nalso exits a wide spectrum of TD variants in the literature [26]–[29]. Note that when a nonlinear function\napproximator, such as neural networks, is used, these algorithms are not guaranteed to converge.\nThe policy optimization methods aim to ﬁnd the optimal policy π∗and broadly fall under two camps,\nwith one focusing on value-based updates, and the other focusing on direct policy-based updates. There\nis also a class of algorithms that belong to both camps, called actor-critic algorithms. Q-learning is one\nof the most representative valued-based algorithms, which obeys the update rule\nQk+1(sk, ak) = Qk(sk, ak) + αk(r(sk, ak) + γ max\na∈A Qk(sk+1, a) −Qk(sk, ak)),\n(4)\nwhere sk ∼dπ, sk+1 ∼P(·|sk, πb(sk)), and πb is called the behavior policy, which refers to the policy\nused to collect observations for learning. The algorithm converges to Q∗almost surely [30] provided\nthat the step-size satisﬁes the Robbins-Monro rule, and every state is visited inﬁnitely often.\nUnlike\nvalue-based methods, direct policy search methods optimize a parameterized policy πθ from trajectories\nof the state, action, reward, (s, a, r) without any value function evaluation steps, using the following\n(stochastic) gradient steps:\nθk+1 = θk + αk ˆ∇θJ(θk), where J(θ) := E\n\" ∞\nX\nk=0\nγkrπθ(sk)\n#\n,\n(5)\nwhere ˆ∇θJ(θk) is a stochastic estimate of the gradient evaluated at θk. The gradient of the value function\nhas the simple analytical form ∇J(θ) = Es∼dπθ,a∼πθ[∇log πθ(a|s)Qπθ(s, a)], which, however, needs an\nestimate of the Q-function, Qπθ(s, a). The simple policy gradient method replaces Qπθ(s, a) with a Monte\nCarlo estimate, which is called REINFORCE [31]. However, the high variance of the stochastic gradient\nestimates due to the Monte Carlo procedure often leads to slow and sometimes unstable convergence.\nThe actor-critic methods combine the advantages of value-based and direct policy search methods [32]\nto reduce the variance. These algorithms parameterize both the policy and the value functions, and\nsimultaneously update both in training\nCritic update : wk+1 = wk + αk(r(sk, ak) + γQ(sk+1, ak+1; wk) −Q(sk, ak; wk))∇wQ(sk, ak; wk)\n6\nActor update : θk+1 = θk + βkQ(sk, ak; wk)∇θ log π(ak|sk; θk),\nwhere wk and θk are parameters of the value and policy, respectively. They often exhibit better empiri-\ncal performance than value-based or direct policy-based methods alone. Nonetheless, when (nonlinear)\nfunction approximation is used, the convergence guarantees of all these algorithms remain rather elusive.\nB. Modern Optimization-based RL Algorithms\nLeveraging the optimization perspectives of RLs, recent works (see, e.g., [26], [28], [29], [33]–\n[35]) generate new principles for solving RL problems as we transition from linear towards nonlinear\nfunction approximations as well as establish theoretical guarantees based on rich theory in mathematical\noptimization literature.\nTo build up an understanding, we ﬁrst recall the linear programming (LP) formulation of the planning\nproblem [36]\nmin\nV\nµT V\nsubject to\nRa + γPaV ≤V,\n∀a ∈A,\n(6)\nwhere µ is the initial state distribution, Ra ∈R|S| is the expected reward vector, and Pa ∈R|S|×|S| is\nthe state transition probability matrix given action a. The constraints in this LP naturally arise from the\nBellman equations. It is known that the solution to (6) is the optimal state-value function V ∗, and that\nthe solution to the dual of (6) yields the optimal policy. By exploiting the Lagrangian duality, the optimal\nvalue function and optimal policy can be found through solving the min-max problem:\nmin\nV ∈V\nmax\nλ=(λa)a∈A∈Λ L(V, λ) := µT V +\nX\na∈A\nλT\na (Ra + γPaV −V ),\n(7)\nwhere sets V and Λ are properly chosen domains that restrict on the optimal value function and policy.\nBuilding on this min-max formulation, several recent works introduce efﬁcient RL algorithms for\nﬁnding the optimal policy. For instance, the stochastic primal-dual RL (SPD-RL) in [33] solves the\nmin-max problem (7) with the stochastic primal-dual algorithm\nVk+1 = ΠV(Vk −γk ˆ∇V L(Vk, λk)),\nλk+1 = ΠΛ(λk + γk ˆ∇λL(Vk, λk)),\nwhere ˆ∇V L and ˆ∇λL are unbiased stochastic gradient estimations, which are obtained by using samples\nof (s, a, r, s′), ΠV and ΠΛ stand for the projection operators onto the sets V and Λ. Since these gradients\nare obtained based on the samples, the updates can be executed without the model knowledge. The\nSPD Q-learning in [35] extends it to the Q-learning framework with off-policy learning, where the\nsample observations are collected from some time-varying behavior policies. The dual actor-critic in [37]\ngeneralizes the setup to continuous state-action MDP and exploits nonlinear function approximations\nfor both value function and the dual policy. These primal-dual type algorithms resemble the classical\n7\nactor-critic methods by simultaneously updating the value function and policy, yet in a more efﬁcient\nand principled manner.\nApart from the LP formulation, alternative nonlinear optimization frameworks based on the ﬁxed\npoint interpretation of Bellman equations have also been explored, both for policy evaluation and policy\noptimization. To name a few, Baird’s residual gradient algorithm [38], designed for policy evaluation,\naims for minimizing the mean-squared Bellman error, i.e.,\nmin\nθ\nMSBE(θ) := Es[(Es′[r(s, π(s)) + γφT (s′)θ] −φT (s)θ)2] = min\nθ\n∥Rπ + γPπΦθ −Φθ∥2\nD,\n(8)\nwhere Rπ and Pπ are the expected reward vector and state transition probability matrix under policy π,\nrespectively, Φ is the feature matrix, D is a diagonal matrix with diagonal entries being the stationary\nstate distributions, and ∥x∥D :=\n√\nxT Dx. The gradient TD (GTD) [26] solves the projected Bellman\nequation, Φθ = Π(Rπ + αPπΦθ), by minimizing the mean-square projected Bellman error,\nmin\nθ\nMSPBE(θ) := ∥Π(Rπ + γPπΦθ) −Φθ∥2\nD ,\n(9)\nwhere Π is the projection onto the range of the feature matrix Φ. This is largely driven by the fact that\nmost temporal-difference learning algorithms converge to the minimum of MSPBE. However, directly\nminimizing these optimization objectives (8) and (9) can be challenging due to the double sampling issue\nand computational burden for the projections. Here, the double sampling issue means the requirement\nof double samples of the next stats from the current state to obtain an unbiased stochastic estimate of\ngradients of the objective mainly due to its quadratic nonlinearity. Alternatively, [28], [39] get around this\ndifﬁculty by resorting to min-max reformulations of the MSBE and MSBPE and introduce primal-dual\ntype methods for policy evaluation with ﬁnite sample analysis. Similar ideas have also been employed for\npolicy optimization based on the (softmax) Bellman optimality equation; see, e.g., [34] (called Smoothed\nBellman Error Embedding (SBEED) algorithm).\nCompared to the classical RL approaches, the optimization-based RLs exhibit several key advantages.\nFirst, in many applications such as robot control, the agents’ behaviors are required to mediate among\nmultiple different objectives. Sometimes, those objectives can be formulated as constraints, e.g., safety\nconstraints. In this respect, optimization-based approaches are more extensible than the traditional dynamic\nprogramming-based approaches when dealing with policy constraints. Second, existing optimization\ntheory provides ample opportunities in developing convergence analysis for RLs with and without function\napproximations; see, e.g., [33], [34]. More importantly, these methods are highly generalizable to the\nmulti-agent RL setup with decentralized rewards, when integrated with recent fruitful advances made in\ndistributed optimization. This last aspect is our main focus in this survey.\n8\nIII. FROM SINGLE-AGENT TO MULTI-AGENT RLS\nCooperative MARL extends the single-agent RL to N agents, V = {1, 2, . . . , N}, where the system’s\nbehavior is inﬂuenced by the whole team of simultaneously and independently acting agents in a common\nenvironment. This can be further classiﬁed into MARLs with centralized rewards and decentralized\nrewards.\nA. MARL with Centralized Rewards\nWe start with MARLs with centralized rewards, where all agents have access to a central reward. In\nthis setting, a multi-agent MDP can be characterized by the tuple, (S, {Ai}N\ni=1, P, r, γ). Each agent i\nobserves the common state s and executes action ai ∈Ai inside its own action set Ai according to its\nlocal policy πi : S →Ai. The joint action a := (a1, a2, . . . , aN) ∈A := A1 × · · · × AN causes the state\ns ∈S to transit to s′ ∈S with probability P(s′|s, a), and the agent receives the common reward r(s, a).\nThe goal for each agent is to learn a local policy πi\n∗: S →Ai, i ∈V such that (π1\n∗, π2\n∗, . . . , πN\n∗) =: π∗\nis an optimal central policy.\nSuppose each agent i ∈V receives the central reward r and knows the joint state and action pair\n(s, a) ∈S × A (i.e., agents are JALs). Cooperative MARL, in this case, is straightforward because\nall agents have full information to ﬁnd an optimal solution. As an example, a naive application of the\nQ-learning [40] to multi-agent settings is\nQi\nk+1(sk, ak) = Qi\nk(sk, ak) + αk\n\u001a\nr(sk, ak) + γ max\na∈A Qi\nk(sk+1, a) −Qi\nk(sk, ak)\n\u001b\n,\nwhere each agent keeps its local Q-function Qi : S × A →R. In particular, it is equivalent to the single-\nagent Q-learning executed by each agent in parallel, and Qi\nk →Q∗as k →∞almost surely for all\ni ∈V; thereby πi\nk(·) = arg maxa Qi\nk(·, a) →πi\n∗(·). Similarly, the policy search methods and actor-critic\nmethods can be easily generalized to MARL with JALs [41]. In such a case, coordination among agents\nis unnecessary to learn the optimal policy. However, in practice, each agent may not have access to the\nglobal rewards due to limitations of communication or privacy issues; as a result, coordination protocols\nare essential for achieving the optimal policy corresponding to the global reward.\nB. Networked MARL with Decentralized Reward\nThe main focus of this survey is on MARLs with decentralized rewards, where each agent only receives\na local reward, and the central reward function is characterized as the average of all local rewards. The\ngoal of each agent is to cooperatively ﬁnd an optimal policy corresponding to the central reward by\nsharing local learning parameters over a communication network.\n9\nMore formally, a coordinated multi-agent MDP with a communication network (i.e., networked MA-\nMDP) is given as the tuple, (S, {Ai}N\ni=1, P, {ri}N\ni=1, γ, G), where ri(s, a) is the random reward of agent\ni given action a and the current state s, and G = (V, E) is an undirected graph (possibly time-varying\nor stochastic) characterizing the communication network. Each agent i observes the common state s,\nexecutes action ai ∈Ai according to its local policy πi : S →Ai, receives the local reward ri(s, a),\nand the joint action a := (a1, a2, . . . , aN) causes the state s ∈S to transit to s′ ∈S with probability\nP(s′|s, a). The central reward is deﬁned as r = 1\nN\nPN\ni=1 ri. In the course of learning, each agent receives\nlearning parameters {θj}j∈Ni from its neighbors of the communication network. The overall model is\nillustrated as in Figure 1.\nFig. 1. Coordinated multi-agent MDP with communication network\nFor an illustrative example, we consider a wireless sensor network (WSN) [42], where data packets are\nrouted to the destination node through multi-hop communications. The WSN is represented by a graph\nwith N nodes (routers), and edges connecting nodes whenever two nodes are within the communication\nrange of each other. The route’s QoS performance (quality of service) depends on the decisions of all\nnodes. Below we formulate the WSN as a networked MA-MDP.\nExample 1 (WSN as a networked MA-MDP). The WSN is a multi-agent system, where sensor nodes are\nagents. Each agent takes action ai ∈A, which consists of forwarding a packet to one of its neighboring\nnode j ∈Ni, sending an acknowledgment message (ACK) to the predecessor, dropping the data packet,\nwhere Ni is the set of neighbors of the node i. The global state s = (s1, s2, . . . , sN) is a tuple of local\nstates si, which consists of the set of is neighboring nodes, and the set of packets encapsulated with QoS\n10\nFig. 2. Routing protocol for wireless sensor networks\nrequirement. A simple example of the reward is r(s, a) := PN\ni=1 ri(si, ai), where\nri(si, ai) :=\n\n\n\n\n\n1\nif ACK received\n0\notherwise\nThe reward measures the quality of local routing decisions in terms of meeting with QoS requirements.\nEach agent only has access to its own reward, which measures the quality of its own routing decisions\nbased on the QoS requirements, while the efﬁciency of overall tasks depends on a sum of local rewards.\nIf each node knows the global state and action (s, a), then the overall system is a networked MA-MDP.\nFinding the optimal policy for networked MA-MDPs naturally relates to one of the most fundamental\nproblems in decentralized coordination and control, called the consensus problem. In the sequel, we ﬁrst\nreview the recent advances in distributed optimization and consensus algorithms, and then march forward\nto the discussions of recent developments for cooperative MARL based on consensus algorithms.\nIV. DISTRIBUTED OPTIMIZATION AND CONSENSUS ALGORITHMS\nIn this section, we brieﬂy introduce several fundamental concepts in distributed optimization, which\nare the backbone of distributed MARL algorithms to be discussed.\nA. Consensus\nConsider a set of agents, V = {1, 2, . . . , N}, each with some initial values, xi(0) ∈Rn. The agents are\ninterconnected over an underlying communication network characterized by a graph G = (V, E), where\nE ⊂V × V is a set of undirected edges, and each agent has a local view of the network, i.e., each agent\ni ∈V is aware of its immediate neighbors, Ni, in the network, and communicates with them only.\n11\nThe goal of the consensus problem is to design a distributed algorithm that the agents can execute\nlocally to agree on a common value as they reﬁne their estimates. The algorithm must be local in the\nsense that each agent performs its own computations and communicates with its immediate neighbors\nonly. Formally speaking, the agents are said to reach a consensus if\nlim\nk→∞xi(k) = c,\n∀i ∈V,\n(10)\nfor some c ∈Rn and for every set of initial values xi(0) ∈Rn. For ease of notation, we consider the\nscalar case, n = 1, from now on.\nA popular approach to the consensus problem is the distributed averaging consensus algorithm [43]\nxi(k + 1) =\n1\n|Ni| + 1\nX\nj∈Ni∪{i}\nxj(k),\n∀k ≥0.\n(11)\nThe averaging update is executed by local agent i, as it only receives values of its neighbors, xj(k), j ∈Ni,\nand is known to ensure consensus provided that the graph is connected. Note that an undirected graph\nG is connected if there is a path connecting every pair of two distinct nodes. Using matrix notations, we\ncan compactly represent (11) as follows\nx(k + 1) = Wx(k),\n∀k ≥0,\n(12)\nwhere x(k) is a column vector with entries, xi(k), i = 1, 2, . . . , N, and W is the weight matrix associated\nwith (11) such that [W]ij :=\n1\n|Ni|+1 if j ∈Ni ∪{i} and zero otherwise. Here, [W]ij means the element\nin the i-th row and j-th column of the matrix W.\nThe matrix W is a stochastic matrix, i.e., it is nonnegative, and its row sums are one. Hence, W k\nconverges to a rank one stochastic matrix, i.e., limk→∞W k = 1nvT , where v is the unique (normalized)\nleft-eigenvector of W for eigenvalue 1 with ∥v∥1 = 1 and 1n is an n-dimensional vector with all\nentries equal to one. Since x(k) = W kx(0), ∀k ≥0, we have limk→∞x(k) = (vT x(0))1n, implying the\nconsensus.\nB. Distributed optimization with averaging consensus\nConsider a multi-agent system connected over a network, where each agent i has its own (convex) cost\nfunction, fi : Rn →R. Let F(x) := P\ni∈V fi(x) be the system objective that the agents want to minimize\ncollectively. The distributed optimization problem is to solve the following optimization problem:\nmin\nx∈Rn F(x) :=\nN\nX\ni=1\nfi(x)\nsubject to\nx ∈X,\n(13)\n12\nwhere X ⊆Rn represents additional constraints on the variable x. By introducing local copies x1, x2, . . . , xN,\nit is equivalently expressed as\nmin\nx1∈X,··· ,xN∈X F(x) :=\nN\nX\ni=1\nfi(xi)\nsubject to\nx1 = x2 = · · · = xN.\n(14)\nThe distributed averaging consensus algorithm can be generalized to solve the distributed optimization.\nAn example is the consensus-based distributed subgradient method [44], where each agent i updates its\nlocal variable xi(k) according to\nConsensus step :\nwi\nk+1 =\n1\n|Ni| + 1\nX\nj∈Ni∪{i}\nxj\nk,\nSubgradient descent step :\nxi\nk+1 = ΠX [wi\nk+1 −αk∂fi(wi\nk+1)],\nwhere ∂fi is any subgradient of fi and ΠX is the Euclidean projection onto the constraint set X.\nThe algorithm is a simple combination of the averaging consensus and the classical subgradient method.\nAs in the averaging consensus, the update is executed by local agent i, and it only receives the values of\nits neighbors, xj\nk, j ∈Ni. When all cost functions are convex, it is known that local variables, xi\nk, reach\na consensus and converge to a solution to (14), x∗∈X, under properly chosen step-sizes.\nOther distributed optimization algorithms include the EXTRA [45] (exact ﬁrst-order algorithm for\ndecentralized consensus optimization), push-sum algorithm [46] for directed graph models, gossip-based\nalgorithm [47], and etc. A comprehensive and detailed summary of the distributed optimization can be\nfound in the monograph [18].\nC. Distributed min-max optimization with averaging consensus\nTo put it one step further, distributed averaging consensus algorithm can also be generalized to solve\nthe min-max problem in a distributed fashion. The distributed min-max optimization problem deals with\nthe zero-sum game:\nmin\nx∈X max\nλ∈Λ L(x, λ) :=\nN\nX\ni=1\nLi(x, λ),\n(15)\nwhere L : Rn × Rm →R is a convex-concave function and L is separable. By introducing local copies\nx1, x2, . . . , xN, λ1, λ2, · · · , λN, the min-max problem is equivalently expressed as\nmin\nx1,...,xN∈X\nmax\nλ1,...,λN∈Λ\nN\nX\ni=1\nLi(xi, λi)\ns.t.\nx1 = x2 = · · · = xN,\nλ1 = λ2 = · · · = λN.\n(16)\nSimilar to the distributed subgradient method, the distributed primal-dual algorithm works by performing\naveraging consensus and sugradient descent for the local variable xi(k) and λi(k) of each agent:\nConsensus step :\nxi\nk+1/2 =\n1\n|Ni| + 1\nX\nj∈Ni∪{i}\nxj\nk,\nλi\nk+1/2 =\n1\n|Ni| + 1\nX\nj∈Ni∪{i}\nλj\nk,\n13\nPrimal-dual step :\nxi\nk+1 = ΠX [xi\nk+1/2 −αk∂xLi(xi\nk+1/2, λi\nk+1/2)],\nλi\nk+1 = ΠΛ[λi\nk+1/2 −βk∂λLi(xi\nk+1/2, λi\nk+1/2)]\nwhere αk and βk are step-sizes, ∂xLi and ∂λLi are any subgradients of Li(x, λ) with respect to x and λ,\nrespectively, and ΠX and ΠΛ are the Euclidean projection onto the constraint sets X and Λ, respectively.\nThe distributed primal-dual algorithm and other variants have been well studied in [48]–[50].\nV. NETWORKED MARL WITH DECENTRALIZED REWARDS\nIn this section, we focus on networked MARL with decentralized rewards, where the corresponding\nnetworked MA-MDP is described by the tuple, (S, {Ai}N\ni=1, P, {ri}N\ni=1, γ, G). The goal of each agent is\nto cooperatively ﬁnd an optimal policy corresponding to the central reward, r = (r1 + r2 + · · · + rN)/N,\nby sharing local learning parameters over a communication network characterized by graph G = (V, E).\nDecentralized rewards are common in practice when multiple agents cooperate to learn under sensing\nand physical limitations. Consider multiple robots navigating and executing multiple tasks in geometrically\nseparated regions. The robots receive different rewards based on the space they reside in. Decentralized\nrewards are also particularly useful when MARL agents cooperate to learn an optimal policy securely\ndue to privacy considerations. For instance, if we do not want to reveal full information about the policy\ndesign criterion to an RL agent to protect privacy, a plausible approach is to operate multiple RL agents,\nand provide each agent with only partial information about the reward function. In this case, no single\nagent alone can learn the optimal policy corresponding to the whole environment, without information\nexchange among other agents. Most recent algorithms to be discussed in this section, including [11]–[17],\n[51], [52], apply the distributed averaging consensus algorithm introduced in Section IV in one way or\nanother. We now discuss these algorithms in details below, with a brief summary provided in Table I.\nA. Distributed Policy Evaluation\nThe goal of distributed policy evaluation is to evaluate the central value function\nV π(s) = E\n\" ∞\nX\nk=0\nγk 1\nN\nN\nX\ni=1\nri\nπ(sk)\n\f\f\f\f\f s0 = s\n#\nin a distributed manner. The information available to each agent is (s, ri, {θj}j∈Ni), where {θj}j∈Ni\nrepresents the set of learning parameters agent i receives from its neighbors over the communication\nnetwork, and Ni is the set of all neighbors of node i over the graph G. Note that for policy evaluation\nwith state value function V , the information a or ai is not necessary, thereby it is not indicated in the\ninformation set (s, ri, {θj}j∈Ni).\n14\nTABLE I\nCOOPERATIVE MARL WITH DECENTRALIZED REWARDS AND COMMUNICATION NETWORKS (LFA: LINEAR FUNCTION\nAPPROXIMATION; NFA: NONLINEAR FUNCTION APPROXIMATION; N/A: NOT APPLICABLE\nPapers\nAvailability\nof actions\nReward\nFunction\nApprox.\nConvergence\nPolicy Evaluation\nDoan et al. [11]\nN/A\nDecentralized\nLFA\nYes\nWai et al. [16]\nLFA\nYes\nLee [17]\nLFA\nYes\nMacua et al. [51]\nN/A\nCentralized\nLFA\nYes\nStankovi´c et al. [52]\nLFA\nYes\nPolicy Optimization\nKar et al. [12]\nJAL\nDecentralized\nTabular\nYes\nZhang et al. [13]\nJAL\nLFA, NFA\nYes\nZhang et al. [14]\nJAL\nLFA, NFA\nLocal\nQu et al. [15]\nJAL\nNFA\nLocal\nThe distributed TD-learning [11] executes the following local updates of agent i:\nθi ←\n1\n|Ni| + 1\nX\nj∈Ni∪{i}\nθj\n|\n{z\n}\nMixing term\n+ γ(ri(s, π(s)) + γφ(s′)T θi −φ(s)T θi)φ(s)\n|\n{z\n}\nTD update\n,\nwhere each agent i keeps its local parameter θi. The algorithm resembles the consensus-based distributed\nsubgradient method in Section IV-B. The ﬁrst term, dubbed as the mixing term, is an average of local\ncopies of the learning parameter of neighbors, Ni, received from communication over networks, and\ncontrols local parameters to reach a consensus. The second term, referred to as the TD update, follows\nthe standard TD updates. Under suitable conditions such as the graph connectivity, each local copy, θi,\nconverges to θ∗in expectation and almost surely [11], where θ∗is the optimal solution found by the\nsingle-agent TD learning acting on the central reward.\nB. Distributed Policy Optimization\nThe goal of distributed policy optimization is to cooperatively ﬁnd an optimal central policy corre-\nsponding to the central reward, r. Note that the distributed TD-learning in the previous section only\nﬁnds the state value function under a given policy. The averaging consensus idea can also be extended\nto Q-learning and actor-critic algorithms for ﬁnding the optimal policy for networked MARL.\n15\nThe distributed Q-learning in [12] locally updates the Q-function according to\nQi(s, a) ←Qi(s, a) −η(s, a)\nX\nj∈Ni∪{i}\n(Qi(s, a) −Qj(s, a))\n|\n{z\n}\nMixing term\n+ α(s, a) (ri(s, a) + γ max\na′∈A Qi(s′, a′) −Qi(s, a))\n|\n{z\n}\nQ−learning update\n,\nwhere i is the agent index, η(s, a) and α(s, a) are learning rates (or step-sizes) depending on the number of\ninstances when (s, a) is encountered. The information available to each agent is (s, a, ri, {Qj}j∈Ni∪{i}).\nThe overall diagram of the distributed Q-learning algorithm is given in Figure 3. Each agent i keeps\nthe local Q-function, Qi, and the mixing term consists of Q-functions of neighbors received from\ncommunication networks. It has been shown that each local Qi reaches a consensus and converges\nto Q∗almost surely [12] with suitable step-size rules and under assumptions such as the connectivity of\nthe graph and an inﬁnite number of state-action visits.\nFig. 3. Diagram of distributed Q-learning algorithm in [12]. Here the joint-action ak is chosen by a behavior policy πb.\nThe distributed actor-critic algorithm in [13] generalizes the single-agent actor-critic to networked\nMA-MDP settings where the averaging consensus steps are taken for the value function parameters\nCritic update : θi\nk+1/2 = θi\nk + αk(ri(sk, ak) + γQ(sk+1, ak+1; θi\nk) −Q(sk, ak; θi\nk))∇θQ(sk, ak; θi\nk)\nActor update : wi\nk+1 = wi\nk + βkA(sk, ak; θi\nk)∇wi log πi\nwi\nk(sk, ai\nk)\n16\nMixing step : θi\nk+1 =\n1\n|Ni| + 1\nX\nj∈Ni∪{i}\nθj\nk+1/2\nwhere wi and θi are parameters of nonlinear function approximations for the local actor and local critic,\nrespectively. Here A(sk, ak; θi\nk) := Q(sk, ak; θi\nk) −P\nai∈Ai πi\nwi\nk(sk, ai)Q(sk, (a1\nk, . . . , ai, . . . , aN\nk ); θi\nk)\nis the advantage function evaluated at (sk, ak). The overall diagram of the distributed actor-critic is\ngiven in Figure 4. Each agent i keeps its local parameters {θi, wi}, and in the mixing step, it only\nreceives local parameters of the critic from neighbors. The actor and critic updates are similar to those\nof typical actor-critic algorithms with local parameters. The information available to each agent is\n(s, a, ri, wi, {θj}j∈Ni∪{i}). The results in [14] study a MARL generalization of the ﬁtted Q-learning\nwith the information structure (s, a, ri, {θj}j∈Ni∪{i}).\nCompared to the tabular distributed Q-learning\nin [12], the distributed actor-critic and ﬁtted Q-learning may not converge to an exact optimal solution\nmainly due to the use of function approximations.\nFig. 4. Diagram of distributed actor-critic algorithm in [13]. Here the joint-action ak is taken in on-policy manner.\nC. Optimization Frameworks for Networked MA-MDP\nRecall that in Section II-B, we discussed optimization frameworks of single-agent RL problem. By\nintegrating them with consensus-based distributed optimization, they can be naturally adapted to solve net-\nworked MA-MDPs. In this subsection, we introduce some recent work in this direction, such as the value\npropagation [15], primal-dual distributed incremental aggregated gradient [16], distributed GTD [17].\n17\nThe main idea of these algorithms is essentially rooted in formulating the overall MDP into a min-max\noptimization problem, minx∈X maxλ∈Λ L(x, λ), with separable function L(x, λ) = PN\ni=1 Li(x, λ), and\nsolving the distributed min-max optimization problem (16). For MARL tasks, the distributed min-max\nproblem can be solved using stochastic variants of the distributed saddle-point algorithms in Section IV-C.\nThe multi-agent policy evaluation algorithms in [16] and [17] are multi-agent variants of the GTD [26]\nbased on the consensus-based distributed saddle-point framework for solving the mean-squared projected\nBellman error in (9), which can be equivalently converted into an optimization problem with separable\nobjectives:\nmin\nθ\n1\n2\nN\nX\ni=1\n∥Π(Ri\nπ + αP πΦθ) −Φθ∥2\nD.\n(17)\nTo alleviate the double sampling issues in GTD, the approach in [16] applies the Fenchel duality with\nan additional proximal term to each objective, arriving at the reformulation:\nmin\n{θi}N\ni=1\nN\nX\ni=1\ndi(θi)\ns.t.\nθ1 = θ2 = · · · = θN,\nwhere the local objectives are expressed as max-forms\ndi(θ) := max\nwi {Ji(θ, wi) := wT\ni (ΦT D((1/N)Ri\nπ + αP πΦθ) −Φθ) −(1/2)wT\ni ΦT DΦwi + (ρ/2)∥θi∥2\n2}.\nThe resulting problem can be solved by using stochastic variants of the consensus-based distributed\nsubgradient method akin to [53]. In particular, the algorithm introduces gradient surrogates of the objective\nfunction with respect to the local primal and dual variables, and the mixing steps for consensus are applied\nto both the local parameters and local gradient surrogates. The main idea of the primal-dual algorithm\nused in [53] is brieﬂy (with some simpliﬁcations) written by\nPrimal update : θi\nk+1 =\n1\n|Ni| + 1\nX\nj∈Ni∪{i}\nθj\nk\n|\n{z\n}\nmixing term\n−αˆgi\nk\nDual update : wi\nk+1 = wi\nk + βˆhi\nk\nwhere α and β are step-sizes, ˆgi\nk and ˆhi\nk are surrogates of the gradients, ∇θiJi(θi\nk, wi\nk) and ∇wiJi(θi\nk, wi\nk),\nrespectively, from through some basic gradient tracking steps.\nThe multi-agent policy evaluation in [17] approaches in a different way to solve (17). Assuming each\nparameter θi is scalar for simplicity, the distributed optimization (17) can be converted into\nmin\n{θi}N\ni=1\n1\n2\nN\nX\ni=1\n∥Π(Ri\nπ + αP πΦθi) −Φθi∥2\nD + ¯θT LT L¯θ\ns.t.\nL¯θ = 0,\n18\nwhere ¯θ is the vector enumerating the local parameters, {θi}N\ni=1, and L = LT ∈RN is the graph\nLaplacian matrix. Note that if the underlying graph is connected, then L¯θ = 0 if and only if θ1 = θ2 =\n· · · = θN. By constructing the Lagrangian dual of the above constrained optimization, we obtain the\ncorresponding single min-max problem. Thanks to the Laplacian matrix, the corresponding stochastic\nprimal-dual algorithm is automatically decentralized. Compared to [53], it only needs to share local\nparameters with neighbors rather than the gradient surrogates.\nThe MARL in [15] combines the averaging consensus and SBEED [34] (Smoothed Bellman Error\nEmbedding), which is called distributed SBEED here. In particular, the distributed SBEED aims to solve\nthe so-called smoothed Bellman equation\nVθ(s) = 1\nN\nN\nX\ni=1\nRi\na(s) + γEs′∼P(·|s,a)[Vθ(s′)] −λ\nN\nX\ni=1\nln(πi\nwi(s, ai)),\nby minimizing the corresponding mean squared smoothed Bellman error:\nmin\nθ, {wi}N\ni=1\nEs,a\n\n\n \n1\nN\nN\nX\ni=1\nRi\na(s) + γEs′∼P(·|s,a)[Vθ(s′)] −λ\nN\nX\ni=1\nln(πi\nwi(s, ai)) −Vθ(s)\n!2\n,\nwhere λ is a positive real number capturing the smoothness level, θ and w are deep neural network\nparameters for the value and policy, respectively. Directly applying the stochastic gradient to the above\nobjective using samples leads to biases due to the nonlinearity of the objective (or double sampling\nissue). To alleviate this difﬁculty, the distributed SBEED introduces the primal-dual form as in [34],\nwhich results in a distributed saddle-point problem similar to (16) and is processed with a stochastic\nvariants of the distributed proximal primal-dual algorithm in [49].\nD. Special Case: Networked MARL with Centralized Rewards\nLastly, we remark that the algorithms in this section can be directly applied to MA-MDPs with central\nrewards. As in Section III, we consider an MDP, (S, A, P, r, γ), with an additional network communication\nmodel G, while each agent i receives the common reward r(s, a) instead of the local reward ri(s, a).\nOne may imagine reinforcement learning algorithms running in N identical and independent simulated\nenvironments. Under this assumption, a distributed policy evaluation was studied in [52]. It combines\nGTD [26] with the distributed averaging consensus algorithm as follows:\nGTD update :\n\n\n\n\n\nθi\nk+1/2 = θi\nk + αk(φ(s) −γφ(s′))(φ(s)T wi\nk)\nwi\nk+1/2 = wi\nk + αk(δi\nk −φ(s)T wi\nk)φ(s)\nMixing term :\n\n\n\n\n\nθi\nk+1 =\n1\n|Ni|+1\nP\nj∈Ni∪{i} θj\nk+1/2\nwi\nk+1 =\n1\n|Ni|+1\nP\nj∈Ni∪{i} wj\nk+1/2\n19\nwhere δi\nk = r(s, π(s)) + γφ(s′)T θi\nk −φ(s)T θi\nk is the local TD-error. Each agent has access to the\ninformation (s, a, r, {θj}j∈Ni), while the action a is not used in the updates. The ﬁrst update is equivalent\nto the GTD in [26] with a local parameter (θi, wi) and the second term is equivalent to the distributed\naveraging consensus update in (11). Since the GTD update rule is equivalent to a stochastic primal-dual\nalgorithm, the above update rule is equivalent to a distributed algorithm for solving the distributed saddle-\npoint problem in (16). Note that [52] only proves the weak convergence of the algorithm. In the same\nvein, the multi-agent policy evaluation [51] generalizes the GQ learning to distributed settings, which is\nmore general than GTD in the sense that it incorporates an importance weight of agent i that measures\nthe dissimilarity between the target and behavior policy for the off-policy learning.\nVI. FUTURE DIRECTIONS\nUntil now, we mainly focused on networked MARL and recent advances which combine tools in\nconsensus-based distributed optimization with MARL under decentralized rewards. There remain much\nmore challenging agendas to be studied. By bridging two domains in a synergistic way, these research\ntopics are expected to generate new results and enrich both ﬁelds.\na) Robustness of networked MARL: Communication networks in real world, oftentimes, suffer\nfrom communication delays, noises, link failures, or packet drops. Moreover, network topologies may\nvary as time goes by and the information exchange over the networks may not be bidirectional in\ngeneral. Extensive results on distributed optimization algorithms over time-varying, directed graphs, w/o\ncommunication delays have been actively studied in the distributed optimization community, yet mostly in\ndeterministic and convex settings. The study of networked MARLs under aforementioned communication\nlimitations is an open and challenging topic.\nb) Resilience of networked MARL: Building resilient networked MARL under adversarial attacks is\nanother important topic. A resilient consensus-based distributed optimization algorithm under adversarial\nattacks has been studied in [54], which considers scenarios where adversarial agents exist among net-\nworked agents and send arbitrary parameters to their neighboring agents to disrupt the solution search.\nIn such cases, analysis of fundamental limitations on distributed optimization algorithms and protocols\nresilient against such adversarial behaviors are available. For networked MARL, such issues remain\nlargely unexplored.\nc) Development of deep networked MARL algorithms: Another interesting direction is the appli-\ncation of consensus-based distributed optimizations to recent deep RL algorithms, such as deep Q-\nlearning [55], trust region policy optimization (TRPO) [56], proximal policy optimization (PPO) [57], deep\ndeterministic policy gradient (DDPG) [58], twin delayed DDPG (TD3) [59]. Most of these algorithms\n20\nare variants of policy search algorithm and involve optimization procedures in certain stages. Ideas of\ndistributed optimizations can potentially be applied to these deep RL algorithms as well.\nd) Theoretical understanding of networked MARL with deep neural nets: Fundamental analysis of\nnetworked MARL with nonlinear function approximation is still an open question. For the optimization-\nbased MARLs, when the value function or policy are parameterized by deep neural networks, the resulting\ndistributed min-max problems discussed eventually become nonconvex-nonconcave. Solving this class of\ndistributed optimization problems in a principled manner remains an intriguing research topic.\ne) MARL for parallel computing: Lastly, networked MARLs can be used to reduce memory and\ncomputational cost, and accelerate the training by exploiting parallel computation. Most RL algorithms\nrequire enormous experiences to ﬁnd a reasonably good policy, which may not be easily collected by a\nsingle agent. Instead, a large number of cooperative RL agents over networks can more effectively collect\nexperiences using their own sensors such as crowd sources. Moreover, these agents can learn different\nparts of learning parameters and features with lower dimensions compared to the state-space, which could\ngreatly reduce the memory and computational cost. There exist several works in this direction, such as\nthe distributed gossiping TD-learning in [60],the distributed policy search algorithm [41], etc. In this\ncase, the design of network topology and infrastructures becomes quite critical in improving the learning\nefﬁciency and balancing the tradeoff between communication and computation cost.\nREFERENCES\n[1] D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein, “The complexity of decentralized control of markov decision\nprocesses,” Mathematics of operations research, vol. 27, no. 4, pp. 819–840, 2002.\n[2] J. Z. Leibo, V. Zambaldi, M. Lanctot, J. Marecki, and T. Graepel, “Multi-agent reinforcement learning in sequential social\ndilemmas,” in Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems, 2017, pp. 464–473.\n[3] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli, and S. Whiteson, “Stabilising experience replay for\ndeep multi-agent reinforcement learning,” in Proceedings of the 34th International Conference on Machine Learning-Volume\n70, 2017, pp. 1146–1155.\n[4] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson, “Learning to communicate to solve riddles with deep distributed\nrecurrent q-networks,” arXiv preprint arXiv:1602.02672, 2016.\n[5] G. J. Laurent, L. Matignon, L. Fort-Piat et al., “The world of independent learners is not markovian,” International Journal\nof Knowledge-based and Intelligent Engineering Systems, vol. 15, no. 1, pp. 55–64, 2011.\n[6] L. Matignon, G. J. Laurent, and N. Le Fort-Piat, “Independent reinforcement learners in cooperative markov games: a\nsurvey regarding coordination problems,” The Knowledge Engineering Review, vol. 27, no. 1, pp. 1–31, 2012.\n[7] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, and R. Vicente, “Multiagent cooperation and\ncompetition with deep reinforcement learning,” PloS one, vol. 12, no. 4, p. e0172395, 2017.\n[8] M. Lauer and M. Riedmiller, “An algorithm for distributed reinforcement learning in cooperative multi-agent systems,” in\nIn Proceedings of the Seventeenth International Conference on Machine Learning, 2000.\n21\n[9] ——, “Reinforcement learning for stochastic cooperative multi-agent systems,” in Proceedings of the Third International\nJoint Conference on Autonomous Agents and Multiagent Systems-Volume 3, 2004, pp. 1516–1517.\n[10] C. Claus and C. Boutilier, “The dynamics of reinforcement learning in cooperative multiagent systems,” Proceedings of\nthe Fifteenth National Conference on Artiﬁcial Intelligence, 1998.\n[11] T. T. Doan, S. T. Maguluri, and J. Romberg, “Convergence rates of distributed TD(0) with linear function approximation\nfor multi-agent reinforcement learning,” arXiv preprint arXiv:1902.07393, 2019.\n[12] S. Kar, J. M. Moura, and H. V. Poor, “QD-learning: a collaborative distributed strategy for multi-agent reinforcement\nlearning through consensus + innovations,” IEEE Transactions on Signal Processing, vol. 61, no. 7, pp. 1848–1862, 2013.\n[13] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Bas¸ar, “Fully decentralized multi-agent reinforcement learning with networked\nagents,” arXiv preprint arXiv:1802.08757, 2018.\n[14] ——, “Finite-sample analyses for fully decentralized multi-agent reinforcement learning,” arXiv preprint arXiv:1812.02783,\n2018.\n[15] C. Qu, S. Mannor, H. Xu, Y. Qi, L. Song, and J. Xiong, “Value propagation for decentralized networked deep multi-agent\nreinforcement learning,” arXiv preprint arXiv:1901.09326, 2019.\n[16] H.-T. Wai, Z. Yang, P. Z. Wang, and M. Hong, “Multi-agent reinforcement learning via double averaging primal-dual\noptimization,” in Advances in Neural Information Processing Systems, 2018, pp. 9672–9683.\n[17] D. Lee, “Stochastic primal-dual algorithm for distributed gradient temporal difference learning,” arXiv preprint\narXiv:1805.07918, 2018.\n[18] A. Nedich et al., “Convergence rate of distributed averaging dynamics and optimization in networks,” Foundations and\nTrends R⃝in Systems and Control, vol. 2, no. 1, pp. 1–100, 2015.\n[19] L. Bu, R. Babu, B. De Schutter et al., “A comprehensive survey of multiagent reinforcement learning,” IEEE Transactions\non Systems, Man, and Cybernetics, Part C, vol. 38, no. 2, pp. 156–172, 2008.\n[20] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning for multi-agent systems: A review of\nchallenges, solutions and applications,” arXiv preprint arXiv:1812.11794, 2018.\n[21] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT Press, 1998.\n[22] J. N. Tsitsiklis and B. Van Roy, “An analysis of temporal-difference learning with function approximation,” IEEE\nTransactions on Automatic Control, vol. 42, no. 5, pp. 674–690, 1997.\n[23] J. Bhandari, D. Russo, and R. Singal, “A ﬁnite time analysis of temporal difference learning with linear function\napproximation,” arXiv preprint arXiv:1806.02450, 2018.\n[24] G. Dalal, B. Sz¨or´enyi, G. Thoppe, and S. Mannor, “Finite sample analyses for TD(0) with function approximation,” in\nThirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[25] R. Srikant and L. Ying., “Finite-time error bounds for linear stochastic approximation and TD learning,” arXiv preprint\narXiv:1902.00923, 2019.\n[26] R. S. Sutton, H. R. Maei, D. Precup, S. Bhatnagar, D. Silver, C. Szepesv´ari, and E. Wiewiora, “Fast gradient-descent\nmethods for temporal-difference learning with linear function approximation,” in International Conference on Machine\nLearning (ICML), 2009, pp. 993–1000.\n[27] R. S. Sutton, H. R. Maei, and C. Szepesv´ari, “A convergent O(n) temporal-difference algorithm for off-policy learning\nwith linear function approximation,” in Advances in neural information processing systems, 2009, pp. 1609–1616.\n[28] B. Dai, N. He, Y. Pan, B. Boots, and L. Song, “Learning from conditional distributions via dual embeddings,” in Artiﬁcial\nIntelligence and Statistics, 2017, pp. 1458–1467.\n22\n[29] D. Lee and N. He, “Target-based gradient TD-learning,” in International Conference on Machine Learning (ICML,\nsubmitted), 2019.\n[30] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-dynamic programming.\nAthena Scientiﬁc Belmont, MA, 1996.\n[31] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine\nlearning, vol. 8, no. 3-4, pp. 229–256, 1992.\n[32] V. R. Konda and J. N. Tsitsiklis, “On actor-critic algorithms,” SIAM journal on Control and Optimization, vol. 42, no. 4,\npp. 1143–1166, 2003.\n[33] Y. Chen and M. Wang, “Stochastic primal-dual methods and sample complexity of reinforcement learning,” arXiv preprint\narXiv:1612.02516, 2016.\n[34] B. Dai, A. Shaw, L. Li, L. Xiao, N. He, Z. Liu, J. Chen, and L. Song, “SBEED: convergent reinforcement learning with\nnonlinear function approximation,” in International Conference on Machine Learning, 2018, pp. 1133–1142.\n[35] D. Lee and N. He, “Stochastic primal-dual Q-learning,” arXiv preprint arXiv:1810.08298, 2018.\n[36] M. L. Puterman, Markov decision processes: Discrete stochastic dynamic programming.\nJohn Wiley & Sons, 2014.\n[37] B. Dai, A. Shaw, N. He, L. Li, and L. Song, “Boosting the actor with dual critic,” in International Conference on Learning\nRepresentations, 2018.\n[38] L. Baird, “Residual algorithms: reinforcement learning with function approximation,” in Proceedings of the 12th\nInternational Conference on Machine Learning, 1995, pp. 30–37.\n[39] S. Mahadevan, B. Liu, P. Thomas, W. Dabney, S. Giguere, N. Jacek, I. Gemp, and J. Liu, “Proximal reinforcement learning:\nA new theory of sequential decision making in primal-dual spaces,” arXiv preprint arXiv:1405.6757, 2014.\n[40] C. Claus and C. Boutilier, “The dynamics of reinforcement learning in cooperative multiagent systems,” AAAI/IAAI, vol.\n1998, pp. 746–752, 1998.\n[41] L. Peshkin, K.-E. Kim, N. Meuleau, and L. P. Kaelbling, “Learning to cooperate via policy search,” in Proceedings of the\nSixteenth conference on Uncertainty in artiﬁcial intelligence, 2000, pp. 489–496.\n[42] X. Liang, I. Balasingham, and S.-S. Byun, “A multi-agent reinforcement learning based routing protocol for wireless sensor\nnetworks,” in 2008 IEEE International Symposium on Wireless Communication Systems, 2008, pp. 552–557.\n[43] A. Jadbabaie, J. Lin, and A. Morse, “Coordination of groups of mobile autonomous agents using nearest neighbor rules,”\nIEEE Transactions on Automatic Control, vol. 48, no. 6, pp. 988–1001, 2003.\n[44] A. Nedic, A. Ozdaglar, and P. A. Parrilo, “Constrained consensus and optimization in multi-agent networks,” IEEE\nTransactions on Automatic Control, vol. 55, no. 4, pp. 922–938, 2010.\n[45] W. Shi, Q. Ling, G. Wu, and W. Yin, “EXTRA: An exact ﬁrst-order algorithm for decentralized consensus optimization,”\nSIAM Journal on Optimization, vol. 25, no. 2, pp. 944–966, 2015.\n[46] A. Nedi´c and A. Olshevsky, “Distributed optimization over time-varying directed graphs,” IEEE Transactions on Automatic\nControl, vol. 60, no. 3, pp. 601–615, 2014.\n[47] A. Nedic, “Asynchronous broadcast-based convex optimization over a network,” IEEE Transactions on Automatic Control,\nvol. 56, no. 6, pp. 1337–1351, 2010.\n[48] M. Zhu and S. Mart´ınez, “On distributed convex optimization under inequality and equality constraints,” IEEE Transactions\non Automatic Control, vol. 57, no. 1, pp. 151–164, 2011.\n[49] M. Hong, D. Hajinezhad, and M.-M. Zhao, “Prox-PDA: The proximal primal-dual algorithm for fast distributed nonconvex\noptimization and learning over networks,” in Proceedings of the 34th International Conference on Machine Learning, 2017,\npp. 1529–1538.\n[50] D. Yuan, S. Xu, and H. Zhao, “Distributed primal–dual subgradient method for multiagent optimization via consensus\n23\nalgorithms,” IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 41, no. 6, pp. 1715–1724,\n2011.\n[51] S. V. Macua, J. Chen, S. Zazo, and A. H. Sayed, “Distributed policy evaluation under multiple behavior strategies,” IEEE\nTransactions on Automatic Control, vol. 60, no. 5, pp. 1260–1274, 2015.\n[52] M. S. Stankovi´c and S. S. Stankovi´c, “Multi-agent temporal-difference learning with linear function approximation: weak\nconvergence under time-varying network topologies,” in American Control Conference (ACC), 2016, pp. 167–172.\n[53] G. Qu and N. Li, “Harnessing smoothness to accelerate distributed optimization,” IEEE Transactions on Control of Network\nSystems, vol. 5, no. 3, pp. 1245–1260, 2017.\n[54] S. Sundaram and B. Gharesifard, “Distributed optimization under adversarial nodes,” IEEE Transactions on Automatic\nControl, vol. 64, no. 3, pp. 1063–1076, 2018.\n[55] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland,\nG. Ostrovski et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533,\n2015.\n[56] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region policy optimization,” in International Conference\non Machine Learning, 2015, pp. 1889–1897.\n[57] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” arXiv preprint\narXiv:1707.06347, 2017.\n[58] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Continuous control with\ndeep reinforcement learning,” arXiv preprint arXiv:1509.02971, 2015.\n[59] S. Fujimoto, H. van Hoof, and D. Meger, “Addressing function approximation error in actor-critic methods,” arXiv preprint\narXiv:1802.09477, 2018.\n[60] A. Mathkar and V. S. Borkar, “Distributed reinforcement learning via gossip,” IEEE Transactions on Automatic Control,\nvol. 62, no. 3, pp. 1465–1470, 2017.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2019-12-01",
  "updated": "2019-12-01"
}