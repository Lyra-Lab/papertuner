{
  "id": "http://arxiv.org/abs/1912.00609v1",
  "title": "GANCoder: An Automatic Natural Language-to-Programming Language Translation Approach based on GAN",
  "authors": [
    "Yabing Zhu",
    "Yanfeng Zhang",
    "Huili Yang",
    "Fangjing Wang"
  ],
  "abstract": "We propose GANCoder, an automatic programming approach based on Generative\nAdversarial Networks (GAN), which can generate the same functional and logical\nprogramming language codes conditioned on the given natural language\nutterances. The adversarial training between generator and discriminator helps\ngenerator learn distribution of dataset and improve code generation quality.\nOur experimental results show that GANCoder can achieve comparable accuracy\nwith the state-of-the-art methods and is more stable when programming\nlanguages.",
  "text": "arXiv:1912.00609v1  [cs.CL]  2 Dec 2019\nGANCoder: An Automatic Natural\nLanguage-to-Programming Language Translation\nApproach based on GAN\nYabing Zhu, Yanfeng Zhang, Huili Yang, and Fangjing Wang\nNortheastern University, China\nAbstract. We propose GANCoder, an automatic programming approach\nbased on Generative Adversarial Networks (GAN), which can generate\nthe same functional and logical programming language codes conditioned\non the given natural language utterances. The adversarial training be-\ntween generator and discriminator helps generator learn distribution of\ndataset and improve code generation quality. Our experimental results\nshow that GANCoder can achieve comparable accuracy with the state-\nof-the-art methods and is more stable when programming languages.\nKeywords: GAN · Semantic parsing · Automatic programming · NLP.\n1\nIntroduction\nWith the development of deep learning and natural language processing (NLP),\ntranslation tasks and techniques have been signiﬁcantly enhanced. The problem\nof cross-language communication has been well solved. In this digital age, we are\nno longer a passive receiver of information but also a producer and analyst of\ndata. We need to have data management, query, and analysis skills. Especially,\nprogramming is an essential skill in the era of AI. However, it requires strong\nprofessional knowledge and practical experience to learn programming languages\nand write codes to process data eﬃciently. Although programming languages,\nsuch as SQL and Python, are relatively simple, due to education and professional\nlimitations, it is still diﬃcult for many people to learn. How to lower the access\nthreshold of learning programming languages and make coding easier is worth\nstudying.\nIn this paper, we explore how to automatically generate programming codes\nfrom natural language utterances. The inexperienced users only need to describe\nwhat they want to implement in natural language, then the programming codes\nwith the same functionality can be generated via a generator [1], so that can\nsimply complete complex tasks, such as database management, programming,\nand data processing.\nAutomatic programming is a diﬃcult task in the ﬁeld of artiﬁcial intelligence.\nIt is also a signiﬁcant symbol of strong artiﬁcial intelligence. Many researchers\nhave been studying how to convert natural language utterances into program\ncode for a long time. Before deep learning is applied, pattern matching was\n2\nY.Zhu et al.\nthe most popular method. But due to the need for a large number of artiﬁcial\ndesign templates and the diversity and fuzziness of natural language expressions,\nmatching-based methods are not ﬂexible and hard to meet the needs. With\nthe development of machine translation, some researchers try to use statistical\nmachine learning to solve the problem of automatic programming, but due to\nthe diﬀerence between the two language models, the results are not satisfactory.\nIn recent years, GANs have been proposed to deal with the problem of data\ngeneration. The game training between GAN’s discriminator and generator make\nthe generator learn data distribution better. In this paper, we propose an au-\ntomatic program generator GANCoder, a GAN-based encoder-decoder frame-\nwork which realizes the translation between natural language and programming\nlanguage. In the training phase, we adopt GAN to improve the accuracy of au-\ntomatic programming generator [2]. The main contributions of this model are\nsummarized as follows. (1) Introducing GAN into automatic programming tasks,\nthe antagonistic game between GAN’s Generator and Discriminator can make\nGenerator learn better distribution characteristics of data; (2) Using Encoder-\nDecoder framework to achieve the end-to-end conversion between two languages;\n(3) Using grammatical information of programming language when generating\nprogram codes, which provides prior knowledge and template for decoding, and\nalso solves the problem of inconsistency between natural language model and\nprogramming language model. Our results show that GANCoder can achieve\ncomparable accuracy with the state-of-the-art methods and is more stable when\nworking on diﬀerent programming languages.\n2\nRelated Work\n2.1\nSemantic Parsing and Code Generation\nSemantic parsing is the task of converting a natural language utterance to a\nlogical form: a machine-understandable representation of its meaning, such as\nﬁrst-order logical representation, lambda calculus, semantic graph, and etc. Se-\nmantic parsing can thus be understood as extracting the precise meaning of\nan utterance [3]. Applications of semantic parsing include machine translation,\nquestion answering and code generation. We focus on code generation in this\npaper.\nThe early semantic analysis systems for code generation are rule-based and\nlimited to speciﬁc areas, such as the SAVVY system [4]. It relies on pattern\nmatching to extract words and sentences from natural language utterances ac-\ncording to pre-deﬁned semantic rules. The LUNAR system [22] works based on\ngrammatical features, which converts the natural language into a grammar tree\nwith a self-deﬁned parser, and then transforms the grammar tree into an SQL\nexpression. Feature extraction by handcraft not only relies on a large amount of\nmanual work but also impacts the performance of such semantic analysis systems\nwhich is relatively fragile. Because the pre-speciﬁed rules and semantic templates\ncannot match the characteristics of natural language expression with ambiguity\nand expression diversity, the early system functionalities are relatively simple.\nGANCoder\n3\nOnly simple semantic analysis tasks are supported. Later, researchers have pro-\nposed WASP [17] and KRISP [18], combined with the grammar information of\nthe logical forms, using statistical machine learning and SVM (Support Vector\nMachine) to convert natural utterances’ grammar tree to the grammar tree of\nthe logical forms. Chris Quirk et al. propose a translation approach from natural\nlanguage utterances to the If-this-then-that program using the KRISP algorithm\n[5].\nEncoder-Decoder frameworks based on RNNs (Recurrent Neural Networks)\nhave been introduced into the code generation tasks. These frameworks have\nshown state-of-the-art performance in some ﬁelds, such as machine translation,\nsyntax parsing, and image caption generation. The use of neural networks can\nreduce the need for custom lexical, templates, and manual features, and also do\nnot need to produce intermediate representations. Li Dong et al. use the Encoder-\nDecoder model to study the code generation task and propose a general neural\nnetwork semantic parser [3]. Xi Victoria Lin et al. propose an encoder-decoder-\nbased model that converts natural language utterances into Linux shell scripts\n[6]. As shown in ﬁg.1, as an end-to-end learning framework, encoder encodes\nnatural language utterances into intermediate semantic vectors, and decoder\ndecodes intermediate vectors into logical forms. Generally speaking, encoder and\ndecoder can be any neural networks, but LSTM and RNN are mostly used.\nAlthough programming languages are sequential strings in form, they have\na hierarchical structure. A number of works utilize the hierarchical structure\nproperty of programs to generate codes .For example, the selective clause and\nthe where clause in SQL belong to diﬀerent logical levels. Based on this observa-\ntion, researchers propose tree-based LSTM and tree-based CNN [7,8]. Besides,\nEgoCoder, a hierarchical neural network based on Python program’s AST (Ab-\nstract Syntax Tree), achieves code auto-completion and code synthesis [9]. Yin\nand Neubig propose an Encoder-Decoder model that uses syntax information as\nthe prior knowledge to help decoder reduce search space [10].\n$OO\u0003SULPHV\u0003JUHDWHU\u0003\nWKDQ\u0003\u0014\u0013\u0003DUH\u0003(YHQ\nHQFRGHU\nGHFRGHU\n3ULPHV\u0011\u0003ILOWHU\u000bODPEGD\u0003\n[\u001d[!\u0014\u0013\u0003\t\t\u0003[\b\u0015\u0003  \u0014\f\nVRIW\u0010DWWHQWLRQ\n1/\u0003XWWHUDQFH\n3\\WKRQ\u0003FRGH\nFig. 1. Encoder-Decoder model for code generation\n2.2\nGenerative Adversarial Network (GAN)\nGAN [2], proposed by Ian Goodfellow in 2014, is a method of unsupervised\nlearning. GAN consists of a generator network and a discriminator network.\nThe generator produces what we want, and the discriminator judges whether the\noutput of the generator is fake or subject to the real distribution. Generator and\ndiscriminator improve themselves and adjust parameters via their adversarial\ntraining. Since GAN was proposed, it has attracted a lot of attention, and more\n4\nY.Zhu et al.\nGANs have been used in image generation, speech synthesis, etc. and achieved\nmuch success. CDGAN [19], WGAN [20], VAEGAN [21] are typical models of\nGANs. Since the object processed in NLP is discrete characters, the gradient of\ndiscriminator cannot be passed to the generator, so the application of GAN in\nNLP is not very successful. Lantao Yu et al. proposed SeqGAN [12] to optimize\nthe GAN network by using the strategy gradient in reinforcement learning to\nimprove the quality of text generation. This is also a successful attempt of GAN\nin NLP tasks .\n\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\n&RQWH[W\u0003)UHH\u0003*UDPPHUV\n6WPW\u0003Æ([SU\u000bH[SU\u0003YDOXH\f\n([SU\u0003Æ\n\u0003\u0003\u0003\u0003&DOO\u000bH[SU\u0003IXQF\u000f\u0003H[SU\r\u0003DUJV\u000f\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\nNH\\ZRUG\r\u0003NH\\ZRUGV\f\u0003\n\u0003\u0003\u0003\u0003_\u0003$WWULEXWH\u000bH[SU\u0003YDOXH\u000f\u0003LGHQWLILHU\u0003DWWU\f\n\u0003\u0003\u0003\u0003_\u00031DPH\u000bLGHQWLILHU\u0003LG\f\n\u0003\u0003\u0003\u0003_\u00036WU\u000bVWULQJ\u0003V\f\nURRW\n([SU\nH[SU>YDOXH@\n&DOO\nH[SU>IXQF@\nH[SU\u0003\r>DUJV@\n1DPH\nVWU\u000bVRUWHG\f\nH[SU\n1DPH\nVWU\u000bP\\BOLVW\f\nNH\\ZRUG\u0003\r>NH\\ZRUGV@\nNH\\ZRUG\u0003\nVWU\u000bUHYHUVH\f\nH[SU>YDOXH@\n1DPH\nVWU\u000b7UXH\f\n\u0003\u0003\u0003$EVWUDFW\u0003\u00036\\QWD[\u00037UHH\u0003RI\u00033\\WKRQ\nW\u0013\nW\u0014\nW\u0015\nW\u0016\nW\u0017\nW\u0018\nW\u0019\nW\u001a\nW\u001b\nW\u001c\nW\u0014\u0013\nW\u0014\u0016\nW\u0014\u0017\nW\u0014\u0018\nW\u0014\u0019\nW\u0014\u0014\nW\u0014\u0015\nFig. 2. Python context free grammar (left) and abstract syntax tree structure (right)\n3\nModel\n3.1\nGAN-based semantic parsing\nWe aim to design an automatic programming model which can generate a pro-\ngram code sequence Y = {y1, y2, · · · , ym} based on a natural language utterance\nX = {x1, x2, · · · , xn}. We introduce GAN into the Encoder-Decoder framework,\nand propose a new model GANCoder, as shown in ﬁg.3 . In GANCoder, the\ngenerator Gθ uses an encoder-decoder framework, which converts the natural\nlanguage utterances into program codes, where θ represents parameters. The en-\ncoder encodes the natural language utterances as intermediate semantic vectors,\nand the decoder decodes the semantic vectors into ASTs with the guidance of the\nprogramming language grammar information. At last, we parse the ASTs into\nprogram codes. The discriminator is responsible for judging whether the ASTs\ngenerated by the generator are consistent with the natural language utterance\nsemantics. We use GAN to improve the generative model Gθ, the optimization\nequation of the GAN network is as follows [13]:\nminGθmaxD∅L(θ, ∅) = EX∼px log D∅(X) + EY ∼Gθ log(1 −D∅(Y ))\n(1)\nGANCoder\n5\nGθ = p(Y |X) =\n|Y |\nY\nt=1\np(yt|Y<t, X, GrammarCF G)\n(2)\nwhere Y<t = y1, y2, · · · , yt−1 represents the sequence of the ﬁrst t −1 characters\nof the program fragment, and GrammarCF G represents the CFG (Context-Free\nGrammars) of the programming language, which provides guidance in the code\ngeneration process. X ∼px indicates that the data sample X is subject to the\ndistribution of real data, and Y ∼Gθ means that the generator generates the\ndata sample Y . Generator and discriminator play two-palyer minimax game. Dis-\ncriminator can diﬀerentiates between the tow distributions. In practice, equation\n'LVFULPLQDWRU\n1/\u0003XWWHUDQFH\n1/\u0003VHPDQWLF\n(QFRGHU\u0010'HFRGHU\n5HZDUG\n7UDLQ\n7UHH\u0003/670\n)DNH\u0003$67\n5HDO\u0003$67\n*HQHUDWRU\nǇϭ\nǇϮ\nǇϯ\nǇϰ\nǇϱ\nϭ\nϯ\nϮ\nϰ\nϱ\n6RIWPD[\nFig. 3. GAN-based automatic program generator\n1 may not provide suﬃcent gradient for generator when generating discrete data\nin NLP. Inspired by the optimization strategy of strategy gradient proposed by\nSeqGAN, combined with the characteristics of automatic programming tasks,\nwe optimize GANCoder as follows:\nJ (θ) = EY ∼Gθ log(Gθ(y1|s0)\nT\nY\nt=2\nGθ(yt|Y1:t−1))R(Y1:t)\n(3)\nR(Y1:t) = D∅(Y1:T )\n(4)\nwhere R(Y1:t) represents the generator reward function, which quantiﬁes the\nquality of the generated program fragments. In the other words, it is the prob-\nability of semantic between the natural language utterances and the generated\nprogram fragments.\n3.2\nCFG-based GAN generator\nThe main task of the GAN generator is to encode the semantics of natural\nlanguage utterances and then to decode the semantics into AST based on CFG\nof the programming language. The conversion from one language to another\n6\nY.Zhu et al.\nuses the Encoder-Decoder model to reduce the interaction between diﬀerent\nlanguages. The two ends are independently responsible for the processing of\ntheir data, simplifying the complexity of the problem. This end-to-end learning\nframework is more general, and both ends can select their own deep learning\nmodels according to the characteristics of the data. Fig.4 shows the framework\ndiagram of the Generator.\n'HFRGHU\n&\u0014\n&\u0015\n&\u0016\n1/\u0003XWWHUDQFH\n&RQWH[W\u0003)UHH\u0003*UDPPHU\n3\\WKRQ\u0003$67\n(QFRGHU\n[\u0014\n[\u0015\n[\u0016\nFig. 4. Encoder-Decoder-based generator in GANCoder\nThe encoder is responsible for encoding the semantics of the natural language\nutterances, as shown in Fig.4. We use a bidirectional LSTM to encode the text\nsequence of natural language description.←−\nht,and −→\nht respectively represent the\nhidden state of the t-th unit of the natural language description sequence from\nleft to right and from right to left,let ht = [←−\nht : −→\nht]. be the intermediate hidden\nvector of the character. The last character’s intermediate vector is the semantic\npresentation of the whole natural utterance.\nThe decoder decodes the intermediate semantic vector generated by Encoder.\nInspired by the model proposed in [10], we ﬁrst decode the intermediate seman-\ntic vector into an abstract syntax tree based on the CFG of the programming\nlanguage. According to the characteristics of CFG and AST, we deﬁne two kinds\nof actions, corresponding to the generation of non-leaf nodes and leaf nodes in\nthe abstract syntax tree in Fig.2 (right). Logically, we use LSTM to recursively\nbuild AST top-down and left-right, as shown in Fig.2 (right). We convert the\ntask into predicting the grammatic action sequence. Based on the CFG, not only\ncan the template be generated for the decoding process, but also the prediction\nrange can be constrained, so that the search space can be reduced to improve\nthe calculation eﬃciency.\nUnlike encoder, decoder uses a normal LSTM to maintain state variables to\nform an AST,\nst = fLST M([at−1 : ct : pt], st−1)\n(5)\npaction = Softmax(st)\n(6)\nwhere ”[:]” represents the concatenation operation between multiple vectors, st\nrepresents the state vector at time t in the decoder, The probability of diﬀerent\ngrammatical actions can be calculated using the Softmax(st) function. at−1\nrepresents the vector of the previous action, and ct represents the state based\nGANCoder\n7\non the input hX = {hx1, hx2, · · · , hx|X|}. We use the soft-attention mechanism\nto calculate the attention, as shown in Fig.4. Then the decoder predicts the\nprobability of each action by the state at time t. When a character is generated\nduring a predicted action, we use PointNet to copy the character from the natural\nlanguage description to AST [11]. In the process of constructing an AST, the\nBeam Search algorithm is used to aviod over-ﬁtting. As shown in Fig.2 (right),\nti represents the step of decoding. The order in which nodes are generated is\nalso clearly marked in Fig.2 (right).\n3.3\nTree-based Semantic GAN discriminator\nThe Generator can generate an AST of the program fragments, and the discrimi-\nnator quantiﬁes the similarity of the semantic relationship between the generated\nASTs and the natural language utterances. How to quantify the semantic simi-\nlarity between two diﬀerent languages is very diﬃcult. In the discriminator, the\nencoding of natural language utterances still uses the same encoder method in\nthe generator, which uses a bidirectional LSTM to encode the entire sequence\ninto intermediate semantic vectors. When encoding the semantics of a program,\nthere are two diﬀerent ways. The ﬁrst is to treat the program code sequence as\na string, and still use the same method as the generator processint it in a bidi-\nrectional LSTM. The processing is simple, but the logic and syntax information\nof the program cannot be captured. The second method is to use the structure\nof the AST generated by the generator to encode the semantics of the program.\nHowever, the semantics of the encoding program is somewhat diﬀerent from the\ngeneration of the AST. In the generator, the structure of the AST is generated\nrecursively top-down and left-right, but in the discriminator, the entire AST is\nencoded bottom-up from leaf node to root nodes of AST, and the ﬁnal vector is\nused as the semantic vector of the program fragment. In this way, the syntax and\nlogic of the program fragment can be learned in a bottom-up manner [13,14].\nLet hr be the ﬁnal encoding vector for the entire abstract syntax tree. Then\nthe hr and the semantic vector hNL of the natural language description are\nclassiﬁed into two categories:\nout = hrW dishNL + bdis\n(7)\nPsim = softmax(out) =\nesim\nP\ne∈out ei\n(8)\nwhere Psim ∈[0, 1] represents the probability that the AST is consistent with\nthe semantics of the natural language description. Since the AST is generated\nbased on the CFG of the programming language, the AST is grammatically\nstandardized. The semantics of the generated program fragments need to be\nconsistent with the natural language semantics.\n4\nExperiments\nThe experiment and evaluation are carried out in a single machine environment.\nThe speciﬁc hardware conﬁguration is: processor Intel-i7-8700, memory 32GB,\n8\nY.Zhu et al.\nNVIDIA GTX1080 graphics card, memory 8GB. The software environment is:\nUbuntu16.04, Python2.7, pytorch3, cuda9. Natural language, Python program\ncharacters, and context-free grammar characters embedding are initialized by the\nxavier uniform method [23]. The optimization function of the model is Adam.\n4.1\nDatasets\n1. Django is a web framework for Python, where each line of code is manually\nlabeled with the corresponding natural language description text. In this\npaper, there are 16,000 training data sets and 1805 veriﬁcation data sets.\n2. Atis is the ﬂight booking system dataset, where the natural language utter-\nances are the user booking inquiries, and the codes are expressed in the form\nof λ calculus. There are 4434 training data sets and 140 test data sets.\n3. Jobs is a job query dataset, where the user queries are natural language\nutterance, and the program codes are expressed in the form of Prolog. There\nare 500 training data sets and 140 test data sets.\n4.2\nExperimental results and analysis\nIf the sequence of the generated program is the same as the program sequence of\nthe training data, it means that the generated data is correct, and the correctness\nof the test set indicates the generation eﬀect of the model. As can be seen from\nTable 1, regarding the Django and Jobs datasets, the pre-trained GANCoder\nmodel improves 2.4% and 0.72% over the normal generator, respectively, and\nimproves 2.6% and 2.93% over that without pre-training. This demonstrates\nthat when training GANCoder, the pre-training has dramatically improved the\nmodel. On the ATIS training set, the normal generator is the best. In this model,\nGAN crashes, which is related to the training data and the grammar rule details\nof diﬀerent logical forms. The natural language description sequence of the Jobs\ndata set is relatively simple, so the accuracy of the model is high. The Python\nlanguage of the Django dataset has relatively good syntax information, but the\nlogic of the program is also more diﬃcult. The game training of GAN also has a\ngood eﬀect. The ATIS dataset is logically diﬃcult, but the grammar information\nis simple, which cannot provide more details when generating ASTs.\nTable 2 compares the state-of-the-art code generation models with our GAN-\nCoder model presented in this paper. Compared to the traditional Encoder-\nDecoder models, such as SEQ2SEQ and SEQ2TREE, GANCoder increases the\naccuracy by 24.6% and 30.3% on the Django dataset. These two models work\nbetter on the Jobs dataset, but the results on the other two datasets are not sat-\nisfactory. The ASN model achieves the best performance on the ATIS dataset\nbut cannot obtain results on the other two datasets. The LPN+COPY model\nand the SNM+COPY model show good results on the Django dataset, but they\nalso do not have results on the other two datasets. Although the GANCoder\nproposed in this paper is not always the best compared with the other mod-\nels, GANCoder can achieve relatively stable and satisfactory results on various\ndatasets and is a promising code generation method worth improving.\nGANCoder\n9\nTable 1. Model accuracy based of three training methods\nDataset\nGenerator normal\nGAN without pretraining\nGAN with pretraining\nDjango\n67.3\n67.1\n69.7\nJobs\n85.71\n83.5\n86.43\nATIS\n82.6\n81.5\n79.23\nTable 2. Accuracy comparison of diﬀerent models\nModels\nATIS\nDjango\nJobs\nSEQ2SEQ[3]\n84.2\n45.1\n87.1\nSEQ2TREE[3]\n84.6\n39.4\n90.0\nASN[15]\n85.3\n-\n-\nLPN+COPY[16]\n-\n62.3\n-\nSNM+COPY[10]\n-\n72.1\n-\nGANCoder(Our model)\n81.5\n69.7\n86.43\n5\nConclusion\nThis paper proposes a semantic programming-based automatic programming\nmethod GANCoder. Through the game confrontation training of GAN generator\nand discriminator, it can eﬀectively learn the distribution characteristics of data\nand improve the quality of code generation. The experimental results show that\nthe proposed GANCoder can achieve comparable accuracy with the state-of-the\nart code generation model, and the stability is better. The method proposed in\nthis paper can only realize the conversion between single-line natural language\ndescription and single-line code. Future work will study how to convert long\nnatural language description text and multi-line code.\nAcknowledgements\nThis work was partially supported by National Key R&D Program of China\n(2018YFB1003404), National Natural Science Foundation of China (61672141),\nand Fundamental Research Funds for the Central Universities (N181605017).\nReferences\n1. Aishwarya Kamath, Rajarshi Das.:A Survey on Semantic Parsing, 2018, CoRR\nabs/1812.00978\n2. I. Goodfellow, J. Pouget-Abadie, M. Mirza, et al.: Generative adversarial nets. Proc.\n28th Advances in Neural Information Processing Systems (NIPS’14), 2014, pp. 2672-\n2680\n3. Li Dong, Mirella Lapata. Language to logical form with neural attention. In Proceed-\nings of the 54th Annual Meeting of the Association for Computational Linguistics,\n2016, pages 33?43, Berlin, Germany\n10\nY.Zhu et al.\n4. William A Woods.:Progress in natural language understanding: an application to\nlunar geology. In proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics of the June 4-8,1973, national computer conference and\nexposition, ACM, 1973, pages 441-450.\n5. Chris Quirk, Raymond J. Mooney, Michel Galley.: Language to Code: Learning\nSemantic Parsers for If-This-Then-That Recipes. ACL (1) 2015: 878-888\n6. Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, et al.: NL2Bash: A Corpus\nand Semantic Parser for Natural Language Interface to the Linux Operating System.\nLREC 2018\n7. Kai Sheng Tai, Richard Socher, Christopher D. Manning.: Improved Semantic Rep-\nresentations From Tree-Structured Long Short-Term Memory Networks. ACL (1)\n2015: 1556-1566\n8. Lili Mou, Ge Li, Lu Zhang, et al.: Convolutional Neural Networks over Tree Struc-\ntures for Programming Language Processing. AAAI 2016: 1287-1293\n9. Jiawei Zhang, Limeng Cui, Fisher B.: Gouza.EgoCoder: Intelligent Program Synthe-\nsis with Hierarchical Sequential Neural Network Model,2018, CoRRabs/1805.08747\n10. Pengcheng Yin, Graham Neubig.: A Syntactic Neural Model for General-Purpose\nCode Generation. ACL (1) 2017: 440-450\n11. Oriol Vinyals, Meire Fortunato, Navdeep Jaitly.: Pointer Networks. NIPS 2015:\n2692-2700\n12. Lantao Yu, Weinan Zhang, Jun Wang, et al.: SeqGAN: Sequence Generative Ad-\nversarial Nets with Policy Gradient. AAAI 2017: 2852-2858\n13. Xinyue Liu, Xiangnan Kong, Lei Liu, et al.: TreeGAN: Syntax-Aware Sequence\nGeneration with Generative Adversarial Networks. ICDM 2018: 1140-1145\n14. Liu Chen, Guangping Zeng, Qingchuan Zhang, et al.: Tree-LSTM Guided Attention\nPooling of DCNN for Semantic Sentence Modeling. 5GWN 2017: 52-59\n15. Maxim Rabinovich, Mitchell Stern, Dan Klein.: Abstract syntax networks for code\ngeneration and semantic parsing. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, 2017, pages 1139-1149, Vancouver,\nCanada\n16. Wang Ling, Phil Blunsom, Edward Grefenstette, et al.: Latent predictor networks\nfor code generation. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics, 2016, pages 599-609, Berlin, Germany\n17. Yuk Wah Wong, Raymond J. Mooney.: Learning for Semantic Parsing with Sta-\ntistical Machine Translation. HLT-NAACL 2006\n18. Rohit J. Kate, Raymond J. Mooney.: Using String-Kernels for Learning Semantic\nParsers. ACL 2006\n19. Alec Radford, Luke Metz, Soumith Chintala.: Unsupervised Representation Learn-\ning with Deep Convolutional Generative Adversarial Networks. ICLR (Poster) 2016\n20. Martn Arjovsky, Soumith Chintala, Lon Bottou.: Wasserstein Generative Adver-\nsarial Networks. ICML 2017: 214-223\n21. Anders Boesen Lindbo Larsen, Sren Kaae Snderby, Hugo Larochelle, Ole Winther.:\nAutoencoding beyond pixels using a learned similarity metric. ICML 2016: 1558-\n1566\n22. William A Woods.: Progress in natural language understanding: an application to\nlunar geology. In Proceedings of the June 4-8, 1973, national computer conference\nand exposition, pages 441450. ACM, 1973.\n23. Xavier Pennec, Nicholas Ayache: Uniform Distribution, Distance and Expectation\nProblems for Geometric Features Processing. Journal of Mathematical Imaging and\nVision 9(1): 49-67 (1998)\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2019-12-02",
  "updated": "2019-12-02"
}