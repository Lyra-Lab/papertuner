{
  "id": "http://arxiv.org/abs/1912.01933v1",
  "title": "Deep Distributional Sequence Embeddings Based on a Wasserstein Loss",
  "authors": [
    "Ahmed Abdelwahab",
    "Niels Landwehr"
  ],
  "abstract": "Deep metric learning employs deep neural networks to embed instances into a\nmetric space such that distances between instances of the same class are small\nand distances between instances from different classes are large. In most\nexisting deep metric learning techniques, the embedding of an instance is given\nby a feature vector produced by a deep neural network and Euclidean distance or\ncosine similarity defines distances between these vectors. In this paper, we\nstudy deep distributional embeddings of sequences, where the embedding of a\nsequence is given by the distribution of learned deep features across the\nsequence. This has the advantage of capturing statistical information about the\ndistribution of patterns within the sequence in the embedding. When embeddings\nare distributions rather than vectors, measuring distances between embeddings\ninvolves comparing their respective distributions. We propose a distance metric\nbased on Wasserstein distances between the distributions and a corresponding\nloss function for metric learning, which leads to a novel end-to-end trainable\nembedding model. We empirically observe that distributional embeddings\noutperform standard vector embeddings and that training with the proposed\nWasserstein metric outperforms training with other distance functions.",
  "text": "Noname manuscript No.\n(will be inserted by the editor)\nDeep Distributional Sequence Embeddings Based on\na Wasserstein Loss\nAhmed Abdelwahab · Niels Landwehr\nReceived: date / Accepted: date\nAbstract Deep metric learning employs deep neural networks to embed in-\nstances into a metric space such that distances between instances of the same\nclass are small and distances between instances from diﬀerent classes are large.\nIn most existing deep metric learning techniques, the embedding of an instance\nis given by a feature vector produced by a deep neural network and Euclidean\ndistance or cosine similarity deﬁnes distances between these vectors. In this\npaper, we study deep distributional embeddings of sequences, where the em-\nbedding of a sequence is given by the distribution of learned deep features\nacross the sequence. This has the advantage of capturing statistical informa-\ntion about the distribution of patterns within the sequence in the embedding.\nWhen embeddings are distributions rather than vectors, measuring distances\nbetween embeddings involves comparing their respective distributions. We pro-\npose a distance metric based on Wasserstein distances between the distribu-\ntions and a corresponding loss function for metric learning, which leads to\na novel end-to-end trainable embedding model. We empirically observe that\ndistributional embeddings outperform standard vector embeddings and that\ntraining with the proposed Wasserstein metric outperforms training with other\ndistance functions.\n1 Introduction\nMetric learning is concerned with learning a representation or embedding in\nwhich distances between instances of the same class are small and distances\nA. Abdelwahab E-mail: AAbdelwahab@atb-potsdam.de\nLeibniz Institute of Agricultural Engineering and Bioeconomy e.V., Potsdam, Germany\nN. Landwehr E-mail: Landwehr@cs.uni-potsdam.de\nUniversity of Potsdam, Department of Computer Science, Potsdam, Germany\nLeibniz Institute of Agricultural Engineering and Bioeconomy e.V., Potsdam, Germany\narXiv:1912.01933v1  [cs.LG]  4 Dec 2019\n2\nAhmed Abdelwahab, Niels Landwehr\nbetween instances of diﬀerent classes are large. Deep metric learning ap-\nproaches, in which the learned embedding is given by a deep neural network,\nhave achieved state-of-the-art results in many tasks, including face veriﬁcation\nand recognition (Schroﬀet al, 2015), ﬁne-grained image classiﬁcation (Reed\net al, 2016), zero-shot classiﬁcation (Bucher et al, 2016), speech-to-text prob-\nlems (Gibiansky et al, 2017), and speaker identiﬁcation (Li et al, 2017). An\nadvantage of metric learning is that the resulting representation directly gen-\neralizes to unseen classes, so the model does not need to be retrained every\ntime a new class is introduced. This is, for example, a typical requirement in\nbiometric applications, where it should be possible to register new subjects\nwithout retraining a model. Biometric systems also have to handle imposters,\nthat is, subjects who are not registered in the database, which is not straight-\nforward in standard classiﬁcation settings.\nIn this paper, we study deep metric learning for sequence data, with a\nspeciﬁc focus on biometric problems. Building on earlier work on quantile\nlayers (Abdelwahab and Landwehr, 2019), we speciﬁcally study how the dis-\ntribution of learned deep features across a sequence can be represented in\nthe learned embedding. Quantile layers are statistical aggregation layers that\ncharacterize the distribution of patterns within a sequence by approximating\nthe quantile function of the activations of the learned ﬁlters across the se-\nquence. Characterizing this distribution has been shown to be advantageous\nfor biometric identiﬁcation based on eye movement patterns (Abdelwahab and\nLandwehr, 2019). The main contribution of this paper is to develop a deep met-\nric learning approach for distributional embeddings based on quantile layers.\nQuantile layers return an estimate of the distribution of values for each learned\nﬁlter across the sequence. Instead of a ﬁxed-length vector representation of an\ninstance, in our approach, the embedding of an instance is given by these sets of\ndistributions. When embeddings are distributions rather than simple vectors,\nmeasuring distances between the embeddings involves comparing their respec-\ntive distributions. We propose a distance metric in the embedding space that\nis based on Wasserstein distances between the respective distributions. Com-\npared to other distance functions such as Kulback-Leibler or Jensen-Shannon\ndivergence, the advantage of using Wasserstein distance is that it takes into\naccount the metric on the space in which the random variable of interest is\ndeﬁned. In our case, this means that distributions in which similar magnitudes\nof ﬁlter activations receive similar amounts of probability mass will be con-\nsidered close. We show how such embeddings can be trained end-to-end on\nlabeled training data using metric learning techniques.\nEmpirically, we study the proposed approach in biometric identiﬁcation\nproblems involving eye movement, accelerometer, and EEG data. Empirical\nresults show that the proposed distributional sequence embeddings outperform\nstandard vector embeddings and that training with the Wasserstein metric\noutperforms training with other distance functions.\nThe rest of the paper is organized as follows. Section 2 discusses related\nwork. In Section 3 we review quantile layers and develop a distributional em-\nbedding architecture based on these layers. Section 4 introduces a Wasserstein-\nDeep Distributional Sequence Embeddings Based on a Wasserstein Loss\n3\nbased distance metric for the proposed embedding model and from this derives\na novel loss function for metric learning. In Section 5 we empirically study the\nproposed method and baselines.\n2 Related work\nOur work is motivated by the goal of capturing information about the dis-\ntribution of patterns within a sequence in its embedding, where the patterns\nare deﬁned in terms of learned features of a deep neural network. It is related\nto other work in deep learning that aims to capture distributions of learned\nfeatures using statistical aggregation layers. Wang et al (2016) proposed end-\nto-end learnable histogram layers that approximate the distribution of learned\nfeatures by a histogram. Their work uses linear approximations to smoothen\nthe sharp edges in a traditional histogram function and enable gradient ﬂow.\nSedighi and Fridrich (2017) proposed a similar histogram-based aggregation\nlayer, but use Gaussian kernels as a soft, diﬀerentiable approximation to his-\ntogram bins. Abdelwahab and Landwehr (2019) introduced quantile layers to\ncapture the distribution of learned features based on an approximation of the\nquantile function, and empirically showed that this outperforms aggregation\nusing histograms. The contribution of our paper is to exploit quantile layers\nin metric learning, by deﬁning distributional embeddings based on approxi-\nmations of quantile functions and deriving loss functions for metric learning\nbased on comparing the resulting distributions.\nThere is a large body of work on deep metric learning that studies diﬀerent\nnetwork architectures and loss functions. For example, Hadsell et al (2006)\nintroduced a loss for a siamese network architecture that is based on all possible\npairs of instances in the training data, and its objective is to minimize distances\nbetween positive pairs (same class) while maximizing the distances between\nnegative pairs (diﬀerent classes). More recently, Schroﬀet al (2015) introduced\nthe triplet loss, with links positive and negative pairs by an anchor instance.\nThis idea has later been extended by Oh Song et al (2016) and Sohn (2016) by\nproviding several negative pairs linked to one positive pair to the loss function.\nThe loss function introduced by Sohn (2016) has shown superior performance\nin several studies (Sohn, 2016; Wu et al, 2017; Yuan et al, 2017). Our method\nbuilds on these established deep metric learning techniques, but extends them\nby replacing vector embeddings with distributional embeddings, which requires\ncorresponding changes in distance calculations and the loss function.\nDistributional embeddings have also been studied in natural language pro-\ncessing in the context of word embeddings. Traditional word embedding models\nsuch as word2vec represent words as vectors in a metric space such that seman-\ntically similar words are mapped to similar vectors (Mikolov et al, 2013). Vilnis\nand McCallum (2015) extend this idea by mapping each word to a Gaussian\ndistribution (with diagonal covariance), which naturally characterizes uncer-\ntainty about the embedding. Athiwaratkun and Wilson (2017) further extend\nthis model by replacing the Gaussian distribution with a mixture of Gaus-\n4\nAhmed Abdelwahab, Niels Landwehr\nsians, where the multimodal mixture can capture multiple meanings of the\nsame word. The motivation for these distributional embeddings is somewhat\ndiﬀerent from our motivation in this paper: while the distribution in our model\nresults from the inner structure of the instance being mapped (distribution of\npatterns within a sequence), the distribution in the model by Vilnis and Mc-\nCallum (2015) captures remaining uncertainty and is inferred during training.\nAnother diﬀerence in the work by Vilnis and McCallum (2015) is that their\nmodel is trained in an unsupervised fashion, while we study supervised metric\nlearning. An approach similar to that of Vilnis and McCallum (2015) has also\nbeen taken by Bojchevski and G¨unnemann (2018) in order to map nodes of an\nattributed graph onto Gaussian distributions that function as an embedding\nrepresentation. This is again an unsupervised approach, and speciﬁc to the\ntask of node embedding.\nMore generally, deep metric learning models have been recently used in\ndiﬀerent application domains featuring sequential data, including natural lan-\nguage processing (Mueller and Thyagarajan, 2016; Neculoiu et al, 2016), com-\nputer vision (McLaughlin et al, 2016; Wu et al, 2018) and speaker identiﬁca-\ntion (Li et al, 2017; Chung et al, 2018), but these approaches are based on\nvector embeddings rather than distributional embeddings.\n3 Quantile Layers and Distributional Sequence Embeddings\nThis section reviews quantile layers as introduced by Abdelwahab and Landwehr\n(2019) and discusses how they can be used to deﬁne distributional embeddings\nof variable-length sequences.\nIn this paper, we focus on variable-length sequences and deep convolutional\nneural network architectures that produce embeddings of such sequences. Typ-\nically, network architectures for such sequences would employ stacked convo-\nlution layers to extract informative features from the sequence, and in the last\nlayer use some form of global pooling to transform the remaining variable-\nlength representation into a ﬁxed-length vector representation. Global pooling\nachieves this transformation by performing a simple aggregate operation such\nas taking the maximum or average over the ﬁlter activations across the se-\nquence. This has the potential disadvantage that most information about the\ndistribution of the ﬁlter activations is lost, which might be informative for the\ntask at hand. In contrast, quantile layers aim to preserve as much informa-\ntion as possible about the distribution of ﬁlter activations along the sequence\nby approximating the quantile function of this distribution. Earlier work has\nshown that this information can be informative for sequence classiﬁcation, sub-\nstantially increasing predictive accuracy (Abdelwahab and Landwehr, 2019).\nIn this paper, we use quantile layers for deﬁning distributional embeddings\nof sequences. We assume that instances are given by variable-length sequences\nof the form s = (x1, ..., xT ) where xt ∈RD is a vector of attributes that de-\nscribes the sequence element at position t. We denote the space of all such\nsequences with D attributes by SD = S∞\nT =1 RT ×D. When a sequence is pro-\nDeep Distributional Sequence Embeddings Based on a Wasserstein Loss\n5\ncessed by a convolutional deep neural network architecture Γ, which we take\nto be the network without any ﬁnal global aggregation layers, the result is a\nvariable-length representation of the instance over K ﬁlters. We denote this\nmapping by Γ : SD →SK. Details of the deep convolutional architectures we\nemploy are given in Section 5. For s ∈SD and k ∈{1, ..., K} we will use Γk(s)\nto denote the variable-length sequence of activations of ﬁlter k produced by\nthe network for sequence s.\nAs in Abdelwahab and Landwehr (2019) we use quantile functions in order\nto characterize the distribution of ﬁlter activations across the sequence Γk(s).\nLet x ∈R be a real-valued random variable, let p(x) denote its density and\nF(x) its cumulative distribution function. The quantile function for x is deﬁned\nby\nQ(r) = inf{x ∈R : F(x) ≥r}\nwhere inf denotes the inﬁmum. If F is continuous and strictly monotonically\nincreasing, Q is simply the inverse of F.\nLet X = {x1, ..., xN} be a sample of the random variable x, that is, xn ∼\np(x) for n ∈{1, ..., N}. The empirical quantile function ˆQX : (0, 1] →R is a\nnon-parametric estimator of the quantile function Q. It is deﬁned by\nˆQX (r) = inf{x ∈R : r ≤ˆFX (x)}\n(1)\nwhere ˆFX (x) =\n1\nN\nPN\ni=1 I(xi ≤x) is the empirical cumulative distribution\nfunction and I(xi ≤x) ∈{0, 1} is an indicator. ˆQX (r) is a piecewise con-\nstant function that is essentially obtained by sorting the samples in X. More\nformally, let π be a permutation that sorts the xi, that is, xπ(i) ≤xπ(i+1)\nfor 1 ≤i ≤N −1. Then ˆQX (r) = xπ(⌈rN⌉), where ⌈x⌉denotes the smallest\ninteger larger or equal to x. The empirical quantile function ˆQX faithfully ap-\nproximates the quantile function Q in the sense that | ˆQX (r)−Q(r)| converges\nalmost surely to zero if N →∞and Q is continuous at r (Resnick, 2013).\nTo enable gradient ﬂow in end-to-end learning, we will work with a piece-\nwise linear interpolation of the piecewise constant function ˆQX (r). For i ∈\n{1, ..., N} and r ∈[ n−1\nN , n\nN ] we can deﬁne a linear approximation by\n˜QX (r) = N(xπ(n+1) −xπ(n))r + nxπ(n) + (1 −n)xπ(n+1)\n\u0012\nr ∈\n\u0014n −1\nN\n, n\nN\n\u0015\u0013\nwhere we deﬁne xπ(N+1) = xπ(N) to handle the right interval border. Combin-\ning the linear approximations over the diﬀerent n, we obtain for r ∈[0, 1] the\npiecewise linear approximation\n˜QX (r) =\nN\nX\nn=1\n˜δ(r, n)\n\u0000N(xπ(n+1) −xπ(n))r + nxπ(n) + (1 −n)xπ(n+1)\n\u0001\nwhere ˜δ(r, n) is an indicator function that is deﬁned as one if r ∈[ n−1\nN , n\nN ]\nand zero otherwise. The piecewise linear approximation ˜QX (r) of the quan-\ntile function depends on the sample size N, because there are N linear seg-\nments. To arrive at an approximation of the quantile function that is in-\ndependent of the number of samples, we deﬁne a further piecewise linear\n6\nAhmed Abdelwahab, Niels Landwehr\napproximation of ˜QX (r) using M sampling points σ(α1), ..., σ(αM), where\nσ(α) = (1 + exp(−α))−1 is the sigmoid function and αi ∈R are parameters\nwith αi ≤αi+1. Formally, we deﬁne\n¯QX (r) =\nM\nX\ni=0\n¯δ(r, i)(aX,ir + bX,i)\n(2)\nwhere\naX,i =\n˜QX (σ(αi+1)) −˜QX (σ(αi))\nσ(αi+1) −σ(αi)\n(3)\nbX,i = ˜QX (σ(αi)) −σ(αi)\n˜QX (σ(αi+1)) −˜QX (σ(αi))\nσ(αi+1) −σ(αi)\n,\n(4)\n¯δ(r, i) is an indicator function that is one if r ∈[σ(αi), σ(αi+1)] and zero\notherwise, and we have introduced α0 = −∞and αM+1 = ∞to handle\nborder cases. The function ¯QX (r) provides a piecewise linear approximation\nof the quantile function using M +1 line segments, independently of the sample\nsize N. The parameters αi are learnable model parameters in the deep neural\nnetwork architectures that we study in Section 5.\nWe are now ready to deﬁne the distributional embedding for an instance,\nwhich is obtained by passing the instance through the neural network Γ and\nfor each ﬁlter in the output of Γ approximating the quantile function of the\nﬁlter activations by the piecewise linear function ¯Q.\nDeﬁnition 1 (Distributional embedding of sequence) Let s ∈SD and\nlet Γ denote a convolutional neural network structure. The distributional em-\nbedding of sequence s is given by the vector of piecewise linear functions\nΨΓ(s) =\n\u0000 ¯QΓ1(s), ..., ¯QΓK(s)\n\u0001\n(5)\nwhere ¯QΓk(s) is deﬁned by Equation 2 using X = Γk(s). Here, we slightly\ngeneralize the notation by identifying the sequence of observations Γk(s) with\nthe corresponding set of observations.\nWe note that due to the piecewise linear approximations, gradients can\nﬂow through the entire embedding architecture, both to parameters αm and\nthe weights in the deep neural network structure Γ. This includes the sorting\noperation, where gradients can be passed through by reordering the gradient\nbackpropagated from the layer above according to the sorting indices π.\n4 A Wasserstein Loss for Distributional Embeddings\nFor training the embedding model, we will use deep metric learning approaches\nwhich train model parameters such that instances of the same class are close\nand instances of diﬀerent classes are far apart in the embedding space. In\norder to apply such approaches, a distance metric needs to be deﬁned on the\nembedding space.\nDeep Distributional Sequence Embeddings Based on a Wasserstein Loss\n7\n4.1 Distances Between Distributional Embeddings\nAs discussed in Section 3, in our setting embeddings of instances are given\nby distributions. Measuring the distance between two embeddings thus means\ncomparing their respective distributions. Diﬀerent approaches to measure dis-\ntances between probability distributions have been discussed in the literature.\nOne of the most widely used distance functions between distributions is the\nKullback-Leibler divergence. However, this measure is asymmetric and can re-\nsult in inﬁnite distances, and is therefore not a metric. A metric based on the\nKullback-Leibler divergence is the square root of the Jensen-Shannon diver-\ngence, which is symmetric, bounded between zero and\np\nlog(2), and satisﬁes\nthe triangle inequality. However, this metric does not yield useful gradients in\ncase the distributions being compared have disjoint support, which in our case\nwould occur if two sequences with non-overlapping ranges of ﬁlter values are\ncompared. To illustrate, let q1 and q2 denote densities with disjoint support\nA1 and A2, and let m(x) = q1(x)+q2(x)\n2\n. Then the Jensen-Shannon divergence\nJ of q1 and q2 is\nJ(q1, q2) = 1\n2\nZ\nA1∪A2\nq1(x)log\n\u0012q1(x)\nm(x)\n\u0013\ndx + 1\n2\nZ\nA1∪A2\nq2(x)log\n\u0012q2(x)\nm(x)\n\u0013\ndx\n= 1\n2\nZ\nA1\nq1(x)log\n\u0012\n2q1(x)\nq1(x)\n\u0013\ndx + 1\n2\nZ\nA2\nq2(x)log\n\u0012\n2q2(x)\nq2(x)\n\u0013\ndx\n= log(2)\nindependently of the distance between A1 and A2, resulting in a gradient of\nzero.\nA diﬀerent class of distance functions which are increasingly being studied\nin machine learning (Frogner et al, 2015; Gao and Kleywegt, 2016; Arjovsky\net al, 2017) are Wasserstein distances. Wasserstein distances are based on the\nidea of optimal transport plans. They do not suﬀer from the zero-gradient\nproblem exhibited by the Jensen-Shannon divergence, because they take into\naccount the metric of the underlying space. They also guarantee continuity un-\nder mild assumptions, which is not the case for the Jensen-Shannon divergence\nas illustrated by Arjovsky et al (2017). In the general case, the p-Wasserstein\ndistance (for p ∈N) between two probability measures ρ1 and ρ2 over a space\nM with metric d can be deﬁned as\nWp(ρ1, ρ2) =\n\u0012\ninf\nπ∈J (ρ1,ρ2)\nZ\nM×M\nd(x, y)pdπ(x, y)\n\u0013 1\np\n(6)\nwhere J (ρ1, ρ2) denotes the set of all joint measures on M×M with marginals\nρ1 and ρ2. For the purpose of this paper, we are interested in the case of\nreal-valued random variables. If q1(x1) and q2(x2) are two densities deﬁning\ndistributions over real-valued random variables, xi ∈R, the p-Wasserstein\ndistance between q1 and q2 is given by\nWp(q1, q2) =\n\u0012\ninf\nq∈J (q1,q2)\nZZ\n|x1 −x2|pq(x1, x2)dx1dx2\n\u0013 1\np\n(7)\n8\nAhmed Abdelwahab, Niels Landwehr\n1\n1 2 3 4 5 6 7\n1 2 3 4 5 6 7\n2 3 4 5 6 7\n0\n0 .5\n1\n0\n0 .5\n1\n0\n0 .5\n1\nq\nq\nq\n1\n2\n3\nFig. 1\nAccording to the Wasserstein metric, distributions q1 and q2 are closer than q1 and\nq3, while distances would be identical under the Jensen-Shannon measure.\nwhere J (q1, q2) deﬁnes the set of all joint distributions over x1, x2 which have\nmarginals q1 and q2. A joint distribution q ∈J (q1, q2) can be seen as a trans-\nport plan, that is, a way of moving probability mass from density q1 such that\nthe resulting density is q2, in the sense that q(x1, x2) indicates how much mass\nis moved from q1(x1) to q2(x2). The quantity\nRR\n|x1 −x2|pq(x1, x2)dx1dx2 is\nthe cost of the transport plan, which depends on the amount of probability\nmass moved, q(x1, x2), and the distance by which the mass has been moved,\n|x1 −x2|p. The inﬁmum over the set J (q1, q2) means that the distance be-\ntween the distributions is given by the optimal transport plan, which intu-\nitively characterizes the minimum changes that need to be made to q1 in order\nto transform it into q2. For p = 1 the distance is therefore also called the Earth\nMover Distance. The advantage of this measure is that it takes into account\nthe metric in the underlying space, as can be seen from Figure 1. Here, q1 is\ncloser to q2 than it is to q3 in the sense that the probability mass needs to\nbe moved less far. Thus, Wp(q1, q2) < Wp(q1, q3), while the Jensen-Shannon\ndistances between the two pairs of distributions would be identical.\nBecause Wasserstein distances are deﬁned in terms of optimal transport\nplans, computing them in general requires solving non-trivial optimization\nproblems. However, for the case of real-valued random variables xi ∈R,\nthere is a simple closed-form solution to the inﬁmum in Equation 7. Let\nx1 ∼q1, x2 ∼q2 with xi ∈R. According to Cambanis et al (1976), the\nfunction K(x1, x2) = |x1 −x2|p for p ≥1 is quasi-antitone and therefore\nthe inﬁmum of the expectation of this function over the set of all joint dis-\ntributions, infq∈J (q1,q2) E[K(x1, x2)], is given by\nR 1\n0 K(Q1(r), Q2(r))dr, where\nQi(r) = inf{t : qi(xi ≤t) ≥r} is the quantile function to the density qi. We\ncan thus rewrite Equation 7 as\nWp(q1, q2) =\n\u0012Z 1\n0\n|Q1(r) −Q2(r)|pdr\n\u0013 1\np\n.\n(8)\nWe now deﬁne the distance between two embeddings ΨΓ(s) and ΨΓ(s′)\nas the Wasserstein distance between the approximate representation of the\nquantile functions in the embedding as deﬁned by Deﬁnition 1, summed over\nthe diﬀerent ﬁlters k.\nDeep Distributional Sequence Embeddings Based on a Wasserstein Loss\n9\nDeﬁnition 2 Let s, s′ ∈SD, let Γ denote a convolutional neural network\narchitecture, and let ΨΓ(s) and ΨΓ(s′) denote the distributional embeddings\nof s, s′ as deﬁned by Deﬁnition 1. Then we deﬁne the distance between the\nembeddings as\ndp(ΨΓ(s), ΨΓ(s′)) =\nK\nX\nk=1\n\u0012Z 1\n0\n| ¯QΓk(s)(r) −¯QΓk(s′)(r)|pdr\n\u0013 1\np\n(9)\nThe next proposition gives a closed-form result for computing dp(ΨΓ(s), ΨΓ(s′)).\nProposition 1 Let s, s′ ∈SD, let Γ denote a convolutional neural network\narchitecture, let ΨΓ(s) and ΨΓ(s′) denote the distributional embeddings of s,\ns′, and let dp(ΨΓ(s), ΨΓ(s′)) denote their distance as deﬁned by Deﬁnition 2.\nThen\ndp(ΨΓ(s), ΨΓ(s′)) =\nK\nX\nk=1\n\u0012 M\nX\ni=0\n(¯ai,kσ(αi+1) + ¯bi,k)|¯bi,kσ(αi+1) + ¯bi,k|p\n¯ai,k(p + 1)\n−(¯ai,kσ(αi) + ¯bi,k)|¯ai,kσ(αi) + ¯bi,k|p\n¯ai,k(p + 1)\n\u0013 1\np\n(10)\nwith\n¯ai,k = aΓk(s),i −aΓk(s′),i\n¯bi,k = bΓk(s),i −bΓk(s′),i\nwhere aX,i and bX,i for X ∈{Γk(s), Γk(s′)} are deﬁned by Equations 3 and 4,\nσ is the sigmoid function, and as above we have introduced α0 = −∞and\nαM+1 = ∞to handle border cases.\nProof (Proposition 1) Starting from Deﬁnition 2 and plugging in ¯QΓk(s) as\ndeﬁned by Equation 2, we see that\nZ 1\n0\n| ¯QΓk(s)(r) −¯QΓk(s′)(r)|pdr\n=\nZ 1\n0\n|\nM\nX\ni=0\n¯δ(r, i)\n\u0000(aΓk(s),i −aΓk(s′),i)r + bΓk(s),i −bΓk(s′),i\n\u0001\n|pdr\n=\nM\nX\ni=0\nZ σ(αi+1)\nσ(αi)\n|¯ai,kr + ¯bi,k|pdr\n(11)\n=\nM\nX\ni=0\n(¯ai,kr + ¯bi,k)|¯ai,kr + ¯bi,k|p\n¯ai,k(p + 1)\n\f\f\f\f\f\nσ(αi+1)\nσ(αi)\n(12)\nwhere in Equation 12 we use the notation G(r)|b\na= G(b)−G(a). In Equation 11\nwe integrate over subintervals [σ(αi), σ(αi+1)] of the interval [0, 1], and can\n10\nAhmed Abdelwahab, Niels Landwehr\ntherefore remove the indicator function ¯δ(r, i). In Equation 12 we solve the\nintegral, where we exploit that according to product and chain rules\n∂\n∂r\n(¯ai,kr + ¯bi,k)|¯ai,kr + ¯bi,k|p\n¯ai,k(p + 1)\n= ¯ai,k|¯ai,kr + ¯bi,k|p+(¯ai,kr + ¯bi,k)p|¯ai,kr + ¯bi,k|p−1sign(¯ai,kr + ¯bi,k)¯ai,k\n¯ai,k(p + 1)\n= |¯ai,kr + ¯bi,k|p.\nThe claim directly follows from Equation 12.\n⊓⊔\nAn important note with respect to the distance function dp(ΨΓ(s), ΨΓ(s′))\nis that its closed-form computation given by Proposition 1 allows gradients to\nbe propagated through distance computations (as well as through embedding\ncomputations as discussed in Section 3) to the parameters of the model Γ\ndeﬁning the embedding. Moreover, all computations can be expressed using\nstandard building blocks available in common deep learning frameworks, such\nthat all gradients are available through automatic diﬀerentiation.\n4.2 Loss Function\nDeep metric learning trains models with loss functions that drive the model\ntowards minimizing distances between pairs of instances from the same class\n(positive pairs) while maximizing distances between pairs of instances from\ndiﬀerent classes (negative pairs). Existing approaches diﬀer in the way nega-\ntive and positive pairs are selected and the exact formulation of the loss. For\nexample, triplet-based losses as introduced by Schroﬀet al (2015) compare the\ndistance between an anchor instance and another instance from the same class\n(positive pair) to the distance between the anchor instance and an instance\nfrom a diﬀerent class (negative pair). However, comparing a positive pair with\nonly a single negative pair does not take into account the distance to other\nclasses and can thereby lead to suboptimal gradients; more recent approaches\ntherefore often consider several negative pairs for each positive pair (Oh Song\net al, 2016; Sohn, 2016). Inspired by these approaches, we consider several\nnegative pairs for each positive pair, leading to a loss function of the form\nL =\nX\n(s1,s2)∈P\nX\n(s3,s4)∈N\ns3∈{s1,s2}\nℓ(s1, s2, s3, s4)\nwhere P ⊂SD × SD is a set of positive pairs and N ⊂SD × SD is a set\nof negative pairs of instances, and ℓ(s1, s2, s3, s4) is a loss function that pe-\nnalizes cases in which a negative pair (s3, s4) has smaller distance than a\npositive pair (s1, s2). A straightforward linear formulation of the loss would be\nℓ(s1, s2, s3, s4) = dp(ΨΓ(s1), ΨΓ(s2))−dp(ΨΓ(s3), ΨΓ(s4)). However, only pairs\nof pairs that violate the distance criterion should contribute to the loss, lead-\ning to ℓ(s1, s2, s3, s4) = max(0, dp(ΨΓ(s1), ΨΓ(s2)) −dp(ΨΓ(s3), ΨΓ(s4))). We\nDeep Distributional Sequence Embeddings Based on a Wasserstein Loss\n11\nfurther replace this loss by a smooth upper bound using log-sum-exp, leading\nto our ﬁnal Wasserstein-based loss function\nL =\nX\n(s1,s2)∈P\nX\n(s3,s4)∈N\ns3∈{s1,s2}\nlog\n\u0010\n1 + expdp(ΨΓ(s1),ΨΓ(s2))−dp(ΨΓ(s3),ΨΓ(s4))\u0011\n.\n(13)\nEquation 13 is of similar structure as other losses used in the literature,\nincluding the angular triplet loss (Wang et al, 2017), the lifted structured\nloss (Oh Song et al, 2016), and the N-pair loss (Sohn, 2016).\nIt remains to specify how positive pairs P and negative pairs N are sam-\npled for each stochastic gradient descent step. We use the approach of Sohn\n(2016) for generating P and N, which has been shown to give state-of-the-art\nperformance in several studies (Sohn, 2016; Wu et al, 2017; Yuan et al, 2017),\nin particular outperforming triplet-based sampling (Schroﬀet al, 2015) and\nlifted structure sampling (Oh Song et al, 2016). The approach constructs a\nbatch of size 2N (where N is an adjustable parameter) by sampling from the\ntraining data N pairs of instances P = {(si, s+\ni )}N\ni=1 from N diﬀerent classes,\nsuch that each pair (si, s+\ni ) is a positive pair from a diﬀerent class. From the\nsampled batch, a set of N(N −1) negative pairs is constructed by setting\nN = {(si, s+\nj )}N\ni,j=1\nj̸=i\n. Note that Equation 13 can be computed by ﬁrst comput-\ning the embeddings of the 2N instances in the batch, and then computing the\noverall loss. Thus, although the computation is quadratic in N, the number of\nevaluations of the deep neural network model Γ is linear in the batch size.\n5 Empirical Study\nWe empirically study the proposed method in three biometric identiﬁcation\ndomains involving human eye movements, accelerometer-based observation of\nhuman gait, and EEG recordings. As an ablation study, we speciﬁcally eval-\nuate which impact the diﬀerent components of our proposed method – the\nmetric learning approach, the use of quantile layers to ﬁt the distribution of\nactivations of ﬁlters across a sequence, and the Wasserstein-based distance\nfunction – have on overall performance.\n5.1 Data Sets\nWe study biometric identiﬁcation based eye movements, the gait, or the EEG\nsignal of a subject. In all domains, the data consist of sequential observations\nof the corresponding low-level sensor signal – gaze position from an eye tracker,\naccelerometer measurements, or EEG measurements – for diﬀerent subjects.\nThe task is to identify the subject based on the observed sensor measurements.\nThe Dynamic Images and Eye Movements (DIEM) dataset (Mital et al,\n2011) contains eye movement data of 210 subjects each viewing a subset of 84\nvideo clips. The video clips are of varying length with an average of 95 seconds\n12\nAhmed Abdelwahab, Niels Landwehr\nand contain diﬀerent visual content, such as excerpts from sport matches,\ndocumentary videos, movie trailers, or recordings of street scenes. The data\ncontain the gaze position on the screen for the left and the right eye, as well\nas a measurement of the pupil dilation, at a temporal resolution of 30 Hz. The\neye movement data of a particular individual on a particular video clip is thus\ngiven by a sequence of six-dimensional vectors (horizontal and vertical gaze\ncoordinate for left and right eye plus left and right pupil dilation), that is,\nD = 6 in the notation of Section 3. The average sequence length is 2850 and\nthere are 5381 sequences overall.\nThe gait data we use come from a study by Ihlen et al (2015) who collected\nthe daily movement activity of 71 subjects for a period of 3 consecutive days.\nThe recorded data consists of time series of 3D accelerometer measurements\nrecorded at a sampling rate of 100Hz. For each point in time, the measurement\nis a D = 6 dimensional vector consisting of the acceleration and velocity in\nx, y, and z direction. In the original data set, a continuous measurement for\n3 days has been carried out for each individual. These long measurements\ncontain diﬀerent activities, but also long idle periods (for example, during\nsleep). We concentrate on subsequences showing high activity, by dividing the\nentire recording for each subject into intervals of length one minute, and then\nselecting for each subject the 30 subsequences that had the largest standard\ndeviation in the 6-dimensional observations. This resulted in 2130 sequences\noverall (30 for each of the 71 subjects), with a length of T = 6000 per sequence.\nThe EEG data we use come from a study by Zhang et al (1995) who\nconducted EEG recording sessions with 121 subjects, measuring the signal\nfrom 64 electrodes placed on the scalp at a temporal resolution of 256Hz of\nthe subjects while viewing an image stimulus. The original aim of the study\nwas to ﬁnd a correlation between EEG observations and genetic predisposition\nto alcoholism, but as subject identiﬁers are available for all recordings the data\ncan also be used in a biometric setting. Each subject completed between 40\nand 120 trials with 1 second of recorded data per trial. The resulting data\ntherefore consist of sequences of D = 64 dimensional vectors with a sequence\nlength of 256 (one trial for one subject).\n5.2 Problem Setting\nAs usual in metric learning, we study a setting in which there are distinct sets\nof subjects at training and test time. The embedding model is ﬁrst trained on\na set of training subjects. On a separate and disjoint set of test subjects, we\nthen evaluate to what degree the learned embedding assigns small distances to\npairs of test sequences from the same subject, and large distances to pairs of\nsequences from diﬀerent subjects. This reﬂects an application setting in which\nnew subjects are registered in a database without retraining the embedding\nmodel. It also naturally allows the identiﬁcation of imposters, that is, subjects\nwho have never been observed (neither during training nor in the database of\nregistered subjects) and try to gain access to the system.\nDeep Distributional Sequence Embeddings Based on a Wasserstein Loss\n13\nIn all three domains, we therefore ﬁrst split the data into training and test\ndata, such that there is no overlap in subjects between the two. For training\nthe embedding model, we use data of 105 of the 210 subjects (eye movements),\n36 of 71 subjects (gait data), or 61 of 121 subjects (EEG data). For the eye\nmovement domain, we additionally ensure that there is no overlap in visual\nstimulus (video clips) between training and test data by splitting the set of all\nvideos into training and test videos and only keeping the respective sequences\nin the training and test data. During training, each sequence constitutes an\ninstance and the subject its class, and we train either embedding models using\nmetric learning as discussed in Section 4 or, as a baseline, multiclass classiﬁ-\ncation models (see Section 5.3 for details). We also set apart the data of 20%\nof the training individuals as validation data to tune model hyperparameters.\nAt test time, we simulate a biometric application setting by ﬁrst sampling,\nfor each test subject, a random subset of the sequences available for that sub-\nject as instances that are put in an enrollment database. We then simulate\nthat we observe additional sequences from a subject which are compared to\nthe sequences of all subjects in the enrollment database. An embedding is\ngood if the distance between these additional sequences and the enrollment\nsequences of the same subject is low, compared to the distance to the enroll-\nment sequences of other subjects. More precisely, for each subject we use all\nexcept ﬁve of the sequences available for that subject as enrollment sequences.\nWe then study how well the subject can be identiﬁed based on observing n of\nthe remaining sequences, for n ∈{1, .., 5}. Given observed sequences s1, ..., sn\n(representing a subject that is unknown at test time), we compute distances\nto all subjects j as dj = 1\nn\nPn\ni=1 d(si, sij) where sij is the sequence of subject\nj in the enrollment database with minimal distance to si. Here, the deﬁnition\nof the distance function d is method-speciﬁc (see below for details).\nWe ﬁrst study a veriﬁcation scenario. This is the binary problem of deciding\nif the observed sequences s1, ..., sn match a particular subject j, by comparing\nthe computed distance dj to a threshold value. Varying the threshold trades\nof false-positive versus false-negative classiﬁcations, yielding a ROC curve and\nAUC score. Note that the veriﬁcation scenario also covers the setting in which\nin imposter is trying to get access to a system as a particular user; the false-\npositive rate is the rate at which such imposters would be accepted.\nWe then study a multiclass identiﬁcation scenario, where we use the model\nto assign the observed sequences s1, ..., sn to a subject enrolled in the database\n(the subject j∗= arg minj dj). This constitutes a multiclass classiﬁcation prob-\nlem for which (multiclass) accuracy is measured. In this experiment, we also\nvary the number of subjects under study, by randomly sampling a subset of\nsubjects which are enrolled in the database; the same subset of subjects is\nobserved at test time. The identiﬁcation problem becomes more diﬃcult as\nthe number of subjects increases.\nWe ﬁnally study the robustness of the model to imposters in the multiclass\nidentiﬁcation scenario, an experiment we denote as multiclass imposters. This\nreﬂects applications in which access to a system does not require a user name,\nbecause the system tries to automatically identify who is trying to gain access.\n14\nAhmed Abdelwahab, Niels Landwehr\nIn this experiment, half of the test subjects play the role of imposters who are\nnot registered in the enrollment database. As in the multiclass identiﬁcation\nsetting, observed sequences are matched to the enrolled subject with minimum\ndistance. This minimum distance is then compared to a threshold value; if\nthe threshold is exceeded, the match is rejected and the observed sequences\nare classiﬁed as belonging to an imposter. Varying the threshold trades oﬀ\nfalse-positives (match of imposter accepted) versus false-negatives (match of\na subject enrolled in the database rejected), yielding a ROC curve and AUC.\nCorrectly rejecting imposters is harder in this setting because it suﬃces for an\nimposter to successfully impersonate any enrolled subject. In this experiment\nwe also vary the number of subjects enrolled in the database.\nIn all three scenarios, the split of sequences into enrollment and observed\nsequences is repeated 10 times to obtain standard deviations of results. More-\nover, accuracies and AUCs will increase with increasing n, as identiﬁcation\nbecomes easier the more data of an unknown subject is available.\n5.3 Methods Under Study\nWe generally study the deep convolutional architecture proposed by Abdel-\nwahab and Landwehr (2019) for biometric identiﬁcation, which consists of\n16 stacked 1D-convolution layers with PReLU activation functions. We vary\nthe aggregation operation, loss function, and training algorithm in order to\nevaluate the impact of these components on overall performance.\nQP-WL: Our method, combining the quantile embeddings of Section 3 with\nthe Wasserstein-based loss function and metric learning algorithm of Section 4.\nIn all experiments, we set the parameter p of the distance function (see Deﬁni-\ntion 2) to one, that is, we use the Earth Mover Distance variant of the Wasser-\nstein distance. The convolutional neural network architecture Γ of Section 3 is\ngiven by 16 stacked convolution layers with parametric RELU activations as\ndeﬁned by Abdelwahab and Landwehr (2019). The number of sampling points\nfor the quantile function is M = 16. At test time, distance between instances\nis given by the distance function from Deﬁnition 2.\nQP-NPL: This method uses the same network architecture and quantile\nembedding as QP-WL. However, the resulting quantile embedding is then\nﬂattened into an K · M vector embedding, with entries ¯QΓk(s)(αm) for k ∈\n{1, ..., K} and m ∈{1, ..., M}. Then standard N-pair loss, which is based on\ncosine similarities between embedding vectors (Sohn, 2016), is used for train-\ning. At test time, the distance between instances is given by negative cosine\nsimilarity. This method utilizes quantile-based aggregation and metric learn-\ning, but does not employ our Wasserstein-based loss function.\nMP-NPL: This method uses the same basic network architecture as QP-\nNPL, but uses standard max-pooling instead of a quantile layer for global\naggregation. This results in a K-dimensional embedding vector. As for QP-\nNPL, the model is trained using metric learning with the N-pair loss. At test\nDeep Distributional Sequence Embeddings Based on a Wasserstein Loss\n15\nEye data\n1 Video\n2 Videos\n3 Videos\n4 Videos\n5 Videos\nQP-WL\n0.9466±0.0032 0.9716±0.0020 0.9799±0.0013 0.9837±0.0008 0.9860±0.0005\nQP-NPL\n0.9345±0.0033\n0.9584±0.0027\n0.9667±0.0020\n0.9705±0.0014\n0.9738±0.0010\nMP-NPL\n0.8890±0.0035\n0.9232±0.0028\n0.9334±0.0017\n0.9392±0.0014\n0.9437±0.0016\nQP-CLS\n0.9007±0.0053\n0.9318±0.0029\n0.9424±0.0025\n0.9503±0.0025\n0.9538±0.0026\nGait data 1 Minute\n2 Minutes\n3 Minutes\n4 Minutes\n5 Minutes\nQP-WL\n0.9923±0.0008 0.9963±0.0003 0.9971±0.0003 0.9974±0.0002 0.9978±0.0001\nQP-NPL\n0.9889±0.0009\n0.9932±0.0004\n0.9943±0.0003\n0.9947±0.0002\n0.9951±0.0002\nMP-NPL\n0.9459±0.0027\n0.9624±0.0027\n0.9690±0.0021\n0.9735±0.0016\n0.9757±0.0012\nQP-CLS\n0.9579±0.0040\n0.9756±0.0018\n0.9812±0.0016\n0.9856±0.0011\n0.9878±0.0008\nEEG data 1 Second\n2 Seconds\n3 Seconds\n4 Seconds\n5 Seconds\nQP-WL\n0.9968±0.0006 0.9985±0.0001 0.9988±0.0001 0.9991±0.0000 0.9992±0.0000\nQP-NPL\n0.9927±0.0005\n0.9941±0.0005\n0.9953±0.0003\n0.9955±0.0002\n0.9959±0.0001\nMP-NPL\n0.9611±0.0012\n0.9687±0.0005\n0.9713±0.0005\n0.9722±0.0005\n0.9732±0.0005\nQP-CLS\n0.9796±0.0017\n0.9868±0.0009\n0.9901±0.0010\n0.9920±0.0006\n0.9923±0.0007\nTable 1\nArea under the ROC curve with standard error for all methods and domains in\nthe veriﬁcation setting for varying number n ∈{1, 2, 3, 4, 5} of observed sequences.\ntime, distance is given by negative cosine similarity. This baseline uses metric\nlearning, but neither quantile layers nor the Wasserstein-based loss function.\nQP-CLS: This baseline uses the same network architecture and ﬂattened\nquantile embedding as QP-NPL, but feeds the ﬂattened embedding vector\ninto a dense classiﬁcation layer with softmax activation. The models is trained\nin a classiﬁcation setting using multiclass crossentropy. Distance at test time\nis given by negative cosine similarity. This model is identical to the model\npresented in Abdelwahab and Landwehr (2019), except that we remove the\nﬁnal classiﬁcation layer at test time to generate embeddings for novel subjects.\nFor all methods, training is carried out using the Adam optimizer with\nlearning rate 0.0001 for 50000 iterations, and the regularizer of the PReLU ac-\ntivation function is tuned as a hyperparameter on the validation set as in (Ab-\ndelwahab and Landwehr, 2019).\n5.4 Results\nWe present and discuss empirical results for the diﬀerent domains in turn.\n5.4.1 Eye Movements\nTable 1, upper third, shows area under the ROC curve for all methods and\nvarying number n of observed sequences in the eye movement domain. Com-\nparing QP-WL and QP-NPL, we observe that the Wasserstein-based loss in-\ntroduced in Section 4, which works on the distributional embedding given by\nthe piecewise linear approximations of the quantile functions, clearly outper-\nforms ﬂattening the distributional embedding and using N-pair loss. Com-\nparing MP-NPL with QP-NPL and QP-WL shows that using quantile layers\n16\nAhmed Abdelwahab, Niels Landwehr\n0.0\n0.2\n0.4\n0.6\n0.8\nFalse positive rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue positive rate\nEye Movements Verification\nQP-WL\nQP-NPL\nMP-NPL\nQP-CLS\n1.0\nFig. 2\nROC curves in the eye movement domain for all methods using n = 5 observed\nsequences. Shaded region in ROC curves indicates standard error.\n0.2\n0.4\n0.6\n0.8\n1\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nFraction of enrolled subjects\nClassification accuracy\nQP-WL\nEye Movements Multiclass Identification\nQP-NPL\nMP-NPL\nQP-CLS\n0.1\n0.2\n0.3\n0.4\n0.5\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nFraction of enrolled subjects\nAUC\nQP-WL\nQP-NPL\nMP-NPL\nQP-CLS\nEye Movements Multiclass Imposters\nFig. 3\nLeft: Identiﬁcation accuracy in the multiclass identiﬁcation scenario for the eye\nmovement domain and n = 5 observed test instances as a function of the fraction of subjects\nthat are enrolled. Right: area under the ROC curve for multiclass imposters as a function\nof the fraction of subjects enrolled. In the imposter scenario, 50% of subjects are imposters\nand therefore never enrolled. Error bars indicate the standard error.\nimproves accuracy compared to max-pooling even if the quantile embedding is\nﬂattened (and more so if Wasserstein-based loss is used). Classiﬁcation train-\ning (QP-CLS) reduces accuracy compared to metric learning (QP-NPL). As\nexpected, AUC increases with the number n of sequences observed at test\ntime. Figure 2 shows ROC curves in the veriﬁcation setting for n = 5.\nFigure 3 (left) shows multiclass identiﬁcation accuracy for n = 5 observed\nsequences as a function of the fraction of the 105 subjects who are enrolled. Rel-\native results for the diﬀerent methods are similar as in the veriﬁcation setting.\nAccuracy decreases slightly when more subjects are enrolled, as the multiclass\nproblem becomes more diﬃcult. Figure 3 (right) shows the robustness of the\nmodel to multiclass imposters as a function of the fraction of the 105 subjects\nwho are enrolled (up to 50%, as half of the subjects are imposters). We observe\nthat QP-WL is much more robust to imposters than the baseline methods.\nIn the eye movement domain, we also compare against the state-of-the-art\nmodel by Jager et al (2019), denoted Jager et al. (2019). Jager et al. (2019)\nuses angular gaze velocities averaged over left and right eye as input, which we\nDeep Distributional Sequence Embeddings Based on a Wasserstein Loss\n17\n0.0\n0.2\n0.4\n0.6\n0.8\nFalse positive rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue positive rate\nEye Movements Verification\nQP-WL\nJager et al.(2019)\n1.0\n0.2\n0.4\n0.6\n0.8\n1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nClassification accuracy\nQP-WL\nFraction of enrolled subjects\nJager et al. (2019)\nMulticlass Identification\n0.1\n0.2\n0.3\n0.4\n0.5\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nAUC\nQP-WL\nJager et al. (2019)\nFraction of enrolled subjects\nMulticlass Imposters\nFig. 4\nComparison between QP-WL and Jager et al. (2019) in the eye movement domain:\narea under ROC curve in veriﬁcation scenario (left), identiﬁcation accuracy in multiclass\nidentiﬁcation scenario (center), and robustness of model to multiclass imposters (right). In\nthis experiment, the data is simpliﬁed for both methods to match the requirements of Jager\net al. (2019), see text for details. Results of QP-WL therefore diﬀer from results presented\nin Figure 2 and Figure 3. Error bars indicate the standard error.\ncompute from our raw data. We replicate the setting of Jager et al (2019) by\ntraining the model using multiclass classiﬁcation and using the last layer before\nthe classiﬁcation layer as the embedding at test time. The Jager et al. (2019)\narchitecture cannot deal with variable-lenght sequences, we therefore split the\nvariable-length sequences in our data into shorter sequences of ﬁxed length,\nnamely the length of the shortest sequence (27 seconds). For a fair comparison,\nwe also simplify the data for our model in this experiment: using only the\naverage gaze point rather than left and right gaze point separately, removing\npupil dilation, and using the same ﬁxed-length sequences. Figure 4 shows ROC\ncurves for the veriﬁcation scenario (left) and identiﬁcation accuracy (center) as\nwell as AUC in the imposter scenario for our model QP-WL on the simpliﬁed\ndata and Jager et al. (2019). Comparing to Figure 2 and Figure 3 we observe\nthat accuracies are reduced for our model by using the simpliﬁed data, but the\nmodel still outperforms Jager et al. (2019) by a wide margin. We note that the\nmodel of Jager et al (2019) is focused on microsaccades, which are likely not\ndetectable in our data due to the low temporal resolution (30Hz compared to\n1000Hz in the study by Jager et al (2019)), which might explain the relatively\npoor performance of the model on our data.\n5.4.2 Gait\nTable 1, center third, shows area under the ROC curve for all methods and\nvarying number n of observed sequences in the gait domain. We observe the\nordering in terms of relative performance between the diﬀerent methods as in\nthe eye movements domain, with clear beneﬁts when using the proposed loss\nfunction based on Wasserstein distance (QP-WL versus QP-NPL), when us-\ning quantile layers instead of max-pooling aggregation (QP-WL and QP-NPL\nversus MP-NPL), and when using metric learning rather than classiﬁcation-\nbased training (QP-NPL versus QP-CLS). Figure 5 shows ROC curves for\nveriﬁcation at n = 5 in the gait domain. Figure 6 (left) shows identiﬁcation\naccuracy as a function of the fraction of subjects enrolled in the gait domain;\nin this setup the ordering of methods in terms of performance is the same\n18\nAhmed Abdelwahab, Niels Landwehr\n0.0\n0.2\n0.4\n0.6\n0.8\nFalse positive rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue positive rate\nGait Verification\nQP-WL\nQP-NPL\nMP-NPL\nQP-CLS\n1.0\nFig. 5\nROC curves in the gait domain for all methods using n = 5 observed sequences.\nShaded region in ROC curves indicates standard error.\n0.2\n0.4\n0.6\n0.8\n1\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\n0.92\n0.94\n0.96\n0.98\n1\nFraction of enrolled subjects\nClassification accuracy\nQP-WL\nQP-NPL\nMP-NPL\nQP-CLS\nGait Multiclass Identification\n0.1\n0.2\n0.3\n0.4\n0.5\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nFraction of enrolled subjects\nAUC\nQP-WL\nQP-NPL\nMP-NPL\nQP-CLS\nGait Multiclass Imposters\nFig. 6\nLeft: Identiﬁcation accuracy in the multiclass identiﬁcation scenario for the gait\ndomain and n = 5 observed test instances as a function of the fraction of subjects that\nare enrolled. Right: area under the ROC curve for multiclass imposters as a function of the\nfraction of subjects enrolled. In the imposter scenario, 50% of subjects are imposters and\ntherefore never enrolled. Error bars indicate the standard error.\nbut the diﬀerence between QP-WL and QP-NPL less pronounced. Figure 6\n(right) shows robustness to multiclass imposters, with again a clear advantage\nof QP-WL over the baselines.\n5.4.3 EEG\nTable 1, bottom third, shows area under the ROC curve for all methods and\nvarying number n of observed test sequences in the EEG domain. Relative\nperformance of methods is generally similar as in the other two domains. QP-\nWL clearly outperforms the closest baseline, reducing 1-AUC by between 56%\n(n = 1) and 80% (n = 5). Figure 7 shows ROC curves in the veriﬁcation\nsetting. Figure 8 (left) and Figure 8 (right) show identiﬁcation accuracy as a\nfunction of the fraction of subjects enrolled and robustness of the models to\nmulticlass imposters. As in the gait domain, diﬀerences are more pronounced\nin the latter setting.\nDeep Distributional Sequence Embeddings Based on a Wasserstein Loss\n19\n0.0\n0.2\n0.4\n0.6\n0.8\nFalse positive rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue positive rate\nEEG Verification\nQP-WL\nQP-NPL\nMP-NPL\nQP-CLS\n1.0\nFig. 7\nROC curves in the EEG domain for all methods using n = 5 observed sequences.\nShaded region in ROC curves indicates standard error.\n0.2\n0.4\n0.6\n0.8\n1\n0.9\n0.92\n0.94\n0.96\n0.98\n1\nFraction of enrolled subjects\nClassification accuracy\nQP-WL\nQP-NPL\nMP-NPL\nQP-CLS\nEEG Multiclass Identification\n0.1\n0.2\n0.3\n0.4\n0.5\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nFraction of enrolled subjects\nAUC\nQP-WL\nQP-NPL\nMP-NPL\nQP-CLS\nEEG Multiclass Imposters\nFig. 8\nLeft: Identiﬁcation accuracy in the multiclass identiﬁcation scenario for the EEG\ndomain and n = 5 observed test instances as a function of the fraction of subjects that\nare enrolled. Right: area under the ROC curve for multiclass imposters as a function of the\nfraction of subjects enrolled. In the imposter scenario, 50% of subjects are imposters and\ntherefore never enrolled. Error bars indicate the standard error.\n6 Conclusions\nWe developed a model for distributional embeddings of variable-length se-\nquences using deep neural networks. Building on existing work on quantile\nlayers, the model represents an instance by the distribution of the learned\ndeep features across the sequence. We developed a distance function for these\ndistributional embeddings based on the Wasserstein distance between the cor-\nresponding distributions, and from this distance function a loss function for\nperforming metric learning with the proposed model. A key point about the\nmodel is end-to-end learnability: by using piecewise linear approximations of\nthe quantile functions, and based on those providing a closed-form solution\nfor the Wasserstein distance, gradients can be traced through the embedding\nand loss calculations. In our empirical study, distributional embeddings out-\nperformed standard vector embeddings by a large margin on three data sets\nfrom diﬀerent domains.\n20\nAhmed Abdelwahab, Niels Landwehr\nReferences\nAbdelwahab A, Landwehr N (2019) Quantile layers: Statistical aggregation in\ndeep neural networks for eye movement biometrics. In: Proceedings of the\n30th European Conference on Machine Learning\nArjovsky M, Chintala S, Bottou L (2017) Wasserstein generative adversarial\nnetworks. In: International conference on machine learning, pp 214–223\nAthiwaratkun B, Wilson A (2017) Multimodal word distributions. In: Pro-\nceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp 1645–1656\nBojchevski A, G¨unnemann S (2018) Deep Gaussian embedding of graphs: Un-\nsupervised inductive learning via ranking. In: International Conference on\nLearning Representations, pp 1–13\nBucher M, Herbin S, Jurie F (2016) Improving semantic embedding consis-\ntency by metric learning for zero-shot classiﬃcation. In: European Confer-\nence on Computer Vision, Springer, pp 730–746\nCambanis S, Simons G, Stout W (1976) Inequalities for ek (x, y) when\nthe marginals are ﬁxed. Zeitschrift f¨ur Wahrscheinlichkeitstheorie und ver-\nwandte Gebiete 36(4):285–294\nChung JS, Nagrani A, Zisserman A (2018) Voxceleb2: Deep speaker recogni-\ntion. Proc Interspeech 2018 pp 1086–1090\nFrogner C, Zhang C, Mobahi H, Araya M, Poggio TA (2015) Learning with a\nWasserstein loss. In: Advances in Neural Information Processing Systems,\npp 2053–2061\nGao R, Kleywegt AJ (2016) Distributionally robust stochastic optimization\nwith Wasserstein distance. arXiv preprint arXiv:160402199\nGibiansky A, Arik S, Diamos G, Miller J, Peng K, Ping W, Raiman J, Zhou\nY (2017) Deep voice 2: Multi-speaker neural text-to-speech. In: Advances\nin neural information processing systems, pp 2962–2970\nHadsell R, Chopra S, LeCun Y (2006) Dimensionality reduction by learning an\ninvariant mapping. In: 2006 IEEE Computer Society Conference on Com-\nputer Vision and Pattern Recognition, IEEE, vol 2, pp 1735–1742\nIhlen EA, Weiss A, Helbostad JL, HausdorﬀJM (2015) The discriminant value\nof phase-dependent local dynamic stability of daily life walking in older adult\ncommunity-dwelling fallers and nonfallers. BioMed research international\nJager L, Makowski S, Prasse P, Liehr S, Seidler M, Scheﬀer T (2019) Deep\neyedentiﬁcation: Biometric identiﬁcation using micro-movements of the eye.\nIn: Proceedings of the 30th European Conference on Machine Learning\nLi C, Ma X, Jiang B, Li X, Zhang X, Liu X, Cao Y, Kannan A, Zhu Z\n(2017) Deep speaker: an end-to-end neural speaker embedding system. arXiv\npreprint arXiv:170502304\nMcLaughlin N, Martinez del Rincon J, Miller P (2016) Recurrent convolutional\nnetwork for video-based person re-identiﬁcation. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp 1325–1334\nMikolov T, Sutskever I, Chen K, Corrado GS, Dean J (2013) Distributed rep-\nresentations of words and phrases and their compositionality. In: Advances\nDeep Distributional Sequence Embeddings Based on a Wasserstein Loss\n21\nin neural information processing systems, pp 3111–3119\nMital PK, Smith TJ, Hill RL, Henderson JM (2011) Clustering of gaze dur-\ning dynamic scene viewing is predicted by motion. Cognitive Computation\n3(1):5–24\nMueller J, Thyagarajan A (2016) Siamese recurrent architectures for learning\nsentence similarity. In: Thirtieth AAAI Conference on Artiﬁcial Intelligence\nNeculoiu P, Versteegh M, Rotaru M (2016) Learning text similarity with\nsiamese recurrent networks. In: Proceedings of the 1st Workshop on Repre-\nsentation Learning for NLP, pp 148–157\nOh Song H, Xiang Y, Jegelka S, Savarese S (2016) Deep metric learning via\nlifted structured feature embedding. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pp 4004–4012\nReed S, Akata Z, Lee H, Schiele B (2016) Learning deep representations of\nﬁne-grained visual descriptions. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pp 49–58\nResnick SI (2013) Extreme values, regular variation and point processes.\nSpringer\nSchroﬀF, Kalenichenko D, Philbin J (2015) Facenet: A uniﬁed embedding for\nface recognition and clustering. In: Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp 815–823\nSedighi V, Fridrich J (2017) Histogram layer, moving convolutional neural net-\nworks towards feature-based steganalysis. Electronic Imaging 2017(7):50–55\nSohn K (2016) Improved deep metric learning with multi-class n-pair loss\nobjective. In: Advances in Neural Information Processing Systems, pp 1857–\n1865\nVilnis L, McCallum A (2015) Word representations via Gaussian embedding.\nInternational Conference on Learning Representations (ICLR)\nWang J, Zhou F, Wen S, Liu X, Lin Y (2017) Deep metric learning with angular\nloss. In: Proceedings of the IEEE International Conference on Computer\nVision, pp 2593–2601\nWang Z, Li H, Ouyang W, Wang X (2016) Learnable histogram: Statisti-\ncal context features for deep neural networks. In: European Conference on\nComputer Vision, Springer, pp 246–262\nWu CY, Manmatha R, Smola AJ, Krahenbuhl P (2017) Sampling matters in\ndeep embedding learning. In: Proceedings of the IEEE International Con-\nference on Computer Vision, pp 2840–2848\nWu L, Wang Y, Gao J, Li X (2018) Where-and-when to look: Deep siamese\nattention networks for video-based person re-identiﬁcation. IEEE Transac-\ntions on Multimedia 21(6):1412–1424\nYuan Y, Yang K, Zhang C (2017) Hard-aware deeply cascaded embedding. In:\nProceedings of the IEEE international conference on computer vision, pp\n814–823\nZhang XL, Begleiter H, Porjesz B, Wang W, Litke A (1995) Event related po-\ntentials during object recognition tasks. Brain Research Bulletin 38(6):531–\n538\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-12-04",
  "updated": "2019-12-04"
}