{
  "id": "http://arxiv.org/abs/1912.03735v1",
  "title": "Security of Deep Learning Methodologies: Challenges and Opportunities",
  "authors": [
    "Shahbaz Rezaei",
    "Xin Liu"
  ],
  "abstract": "Despite the plethora of studies about security vulnerabilities and defenses\nof deep learning models, security aspects of deep learning methodologies, such\nas transfer learning, have been rarely studied. In this article, we highlight\nthe security challenges and research opportunities of these methodologies,\nfocusing on vulnerabilities and attacks unique to them.",
  "text": "Department: Head\nEditor: Name, xxxx@email\nSecurity of Deep Learning\nMethodologies: Challenges\nand Opportunities\nShahbaz Rezaei\nUniversity of California, Davis\nXin Liu\nUniversity of California, Davis\nAbstract—Despite the plethora of studies about security vulnerabilities and defenses of deep\nlearning models, security aspects of deep learning methodologies, such as transfer learning,\nhave been rarely studied. In this article, we highlight the security challenges and research\nopportunities of these methodologies, focusing on vulnerabilities and attacks unique to them.\nKeywords: Machine Learning Security; Pri-\nvacy; Deep Learning; Machine Learning Method-\nologies.\nW ith the widespread adaptation of deep neural\nnetworks (DNN), their security challenges have\nreceived signiﬁcant attention from both academia\nand industry, especially for mission critical ap-\nplications, such as road sign detection for au-\ntonomous vehicles, face recognition in authenti-\ncation systems, and fraud detection in ﬁnancial\nsystems.\nThere are three major types of attacks on deep\nlearning models, namely adversarial attacks, data\npoisoning, and exploratory attacks. Particularly,\nadversarial attacks, which aim to carefully craft\ninputs that cause the model to misclassify, has\nbeen extensively studied and many defence mech-\nanisms have been proposed to alleviate them.\nThese attacks are of paramount importance be-\ncause they are effective, moderately simple to\nlaunch, and often transferable from one model\nto another. In literature, there are several survey\nand review papers on deep learning security and\ndefence mechanisms. In this article, we focus on\nsecurity of a much less explored area of machine\nlearning - machine learning methodologies.\nMachine learning methodologies have been\nwidely used to mitigate the restrictions and as-\nsumptions of a typical machine learning pro-\ncess. A typical DNN training process assumes\nlarge labeled dataset(s), access to high computa-\ntional resources, non-private and centralized data,\nstandard training and hyper-parameter tuning,\nand ﬁxed task distribution over time. However,\nthese assumptions are often difﬁcult to realize in\npractice. As a result, different machine learning\nmethodologies have been developed and adopted,\nsuch as transfer learning, federated learning,\nIT Professional\nPublished by the IEEE Computer Society\nc\n⃝2019 IEEE\n1\narXiv:1912.03735v1  [cs.CR]  8 Dec 2019\nDepartment Head\nmodel compression, multi-task learning, meta-\nlearning, and lifelong learning. Notwithstand-\ning the proliferation of these machine learning\nmethodologies, their security aspects have not\nbeen comprehensively analyzed, if ever studied.\nIn this article, we focus on potential attacks,\nsecurity vulnerabilities, and future directions spe-\nciﬁc to each learning methodology. Note that\nthere are many more machine learning method-\nologies in literature, including few-shot learning,\non-device learning, zero-shot learning, to name\nbut a few. However, due to the lack of space and\nthe fact that these methodologies mostly overlap\nwith the ones that we review, we limit our dis-\ncussion to the aforementioned methodologies. We\nassume that readers have rudimentary background\non deep neural networks and how they work.\nBackground\nAttack Taxonomy\nIn machine learning security, an attack has a\nthreat model that deﬁnes the goal, capabilities (or\nknowledge), and target model. The attacker’s goal\ncan be categorized in terms of security violation:\n1) violation of availability that aims to reduce\nthe conﬁdence of a model for normal inputs, 2)\nviolation of integrity that aims misclassiﬁcation\non certain inputs without affecting normal inputs,\nand 3) violation of privacy that aims to obtain\nconﬁdential information about the model, training\nor inference-time data and users, or even hyper-\nparameters used during training (hyper-parameter\nstealing attack).\nThe life-cycle of a typical machine learn-\ning model with ofﬂine training data consists of\ntraining and inference phases, which indicate\nattacker’s capabilities and knowledge. Training\nphase capabilities are data injection, where the\nattacker injects new data points to the training\ndataset, data poisoning, where the attacker modi-\nﬁes the existing data points in the training dataset,\nand logic corruption, where the attacker interferes\nwith the learning algorithm.\nIn the inference phase, the model is assumed\nto be ﬁxed and the attacker cannot change the\nmodel. However, the attacker can still craft data\ninputs that fool the model to provide incorrect\noutputs. Hence, the attacker’s capability is deﬁned\nbased on how much information she has about the\nmodel, ranging from white-box, where everything\nis possibly known including the entire model\nand training data, to black-box attacks, where\nminimum knowledge about the model, training\ndata and algorithm is known. Any attack model\nthat lays between while-box and black-box attack\nin terms of available information about the model\nis called gray-box attack.\nAttack Types\nIn machine learning security, attacks are often\ncategorized into three attack types based on the\nthreat model:\nEvasion attack (adversarial attack): The\ngoal of an evasion attack is to manipulate the\ninput data such that the model misclassiﬁes. Al-\nthough one can technically manipulate training\ndata using evasion attack methods during training\nphase (often for adversarial retraining as a de-\nfense mechanism), evasion attack is an inference\nphase attack that violates the integrity. Figure\n1(a) illustrates the adversarial attack where the\nattacker add a small perturbation, imperceptible\nto human eye, to the stop sign image to cause the\nmodel to misclassify.\nData poisoning: This is a training phase\nattack where the attacker inject or manipulate\ntraining data to either create a backdoor to use at\ninference time (without compromising the model\nperformance on normal input data) or to cor-\nrupt the training process. Hence, it can violate\navailability or integrity depending on the goal. A\ntypical example is to create a backdoor for face\nrecognition task where the attacker injects a set\nof training samples with a speciﬁc object in a\ntarget person’s training data. The aim is to force\nthe model to associate the speciﬁc object with the\ntarget class. Then, any face image with the object\nis classiﬁed as the target class even if it belongs\nto another person. For instance, in Figure 1(c),\nthe attacker inject faces of John with a special\nhat during training. Then, at the inference phase,\nany face that has the hat is classiﬁed as John by\nthe model.\nExploratory attacks: The aim of the attack is\nto violate the privacy at inference phase. It covers\nseveral types of attacks, including model extrac-\ntion, to extract model parameters, membership\ninference attack, to examine whether a data point\nis used during the training phase, model inversion,\n2\nIT Professional\nAttacker\nTraining Phase\nInference Phase\nFace \nRecognition\nSarah\nJohn\nJim\nMia\nTraining Dataset\nAttacker\nFace \nRecognition\nJohn\nJohn\nJohn\nStop sign\n+\nSpeed limit\nUser\nAttacker\nSign \nRecognition\nInference Phase\nMia\nAttacker\nFace \nRecognition\nInference Phase\nUser\nRecover Mia input\n(a)\n(b)\n(c)\nFigure 1. Typical attacks on machine learning: (a) Adversarial attack, (b) model inversion attack, and (c)\nbackdoor attack.\nto infer something about input by observing the\nmodel output. In Figure 1(b), the attacker aims\nto recover the input image of Mia by observing\nthe output and the model. Although exploratory\nattacks have been widely studied for classical\nmachine learning algorithms, there are only a\nfew work on deep learning models. For example,\nit has been recently shown that sensitive and\nout-of-distribution sentences, such as ”My social\nsecurity is —-”, can be leaked from commercial\ntext-completion neural networks [12].\nMachine Learning Methodologies\nSeveral machine learning methodologies, such\nas tranfer learning or multi-task learning, have\nbeen used to train better models, reduce train-\ning data, reduce model complexity, distribute\nresources, or use other training data or models.\nThese methodologies may overlap or have simi-\nlarities. However, each methodology is based on\ndifferent set of assumptions that may introduce\ndifferent vulnerabilities. Here, we brieﬂy explain\nthe assumptions and goals of each machine learn-\ning methodology:\nTransfer learning: Transfer learning refers\nto techniques that use the knowledge learned for\none task, called the source task, to improve the\nperformance of another task, called the target\ntask. The model trained for the source task is\ncalled the teacher model and the model trained\nfor the target task is called the student model. The\nmost common way of transfer learning for deep\nlearning models is to transfer all/some weights of\na teacher model to a student model and then train\nthe student model for the target task. The intuition\nis that the teacher model’s weights are much\ncloser to the local optimum for the target task than\nany random initialization of the student model.\nHence, the training procedure of the model for\nthe target task starts at the point where it is\nconsiderably close to the performance target of\nthe training procedure.\nMulti-task learning: The goal of multi-task\nlearning is to learn several related tasks simulta-\nneously. The most prevalent multi-task learning\napproach for deep models is parameter sharing,\nwhere the model consists of shared layers and\ntask-speciﬁc layers. An example of multi-task\nlearning is to predict the class and the coordinates\nof an object in an image.\nFederated learning: Federated learning (FL)\nis a distributed learning approach that aims to\ntrain a model on a distributed private data. The\nassumption is that data is distributed among nodes\ncapable of training a model, such as smartphone\ndevices, and there is a centralized server that co-\nordinates the training. The main approach consists\nof several training rounds where the server sends\nthe model to a set of nodes. Then, these nodes\ntrain the model with all or a portion of their local\nMay/June 2019\n3\nDepartment Head\ndataset and send the updates back to the server.\nThen, at the end of each round, the server receives\nall the updates from the nodes and aggregates\nthem to built a new model. The advantage of\nfederated learning is that contributing nodes do\nnot need to share or reveal their training data.\nModel compression: The aim of model com-\npression is to make large deep models suitable\nfor devices with limited resources (e.g. CPU,\nmemory, energy, bandwidth). The two common\ncompression approaches are pruning that reduces\nthe number of parameters of models, and quan-\ntisation that reduces the number of bits required\nto store each parameter. Although compression\nmethods try to keep the model accuracy intact on\nthe training data, the compressed model may act\ncompletely different from the original model on\nunseen or adversarial inputs.\nMeta-learning: Meta-learning is the process\nof better learning a task from past experiences\nand other tasks with the purpose of learning\nmuch faster, inspired by how humans learn. Meta-\nlearning has been proliferated in the past two\ndecades. Different categories and approaches of\nmeta-learning is out of the scope of this paper.\nAvid readers can ﬁnd several recent surveys on\nthis topic. Note that meta-learning may overlap\nmuch with other methodologies, including trans-\nfer learning, few-shot learning, and multi-task\nlearning. However, in this article, we consider\nbroader and general categories of meta-learning.\nLifelong machine learning: The ability to\ncontinually learn new tasks by building upon pre-\nviously learned knowledge and tasks over time,\nwhich is an indispensable part of humans and\nanimals life, is called lifelong learning. In lifelong\nlearning, the system should retain its ability to\nperform older tasks and learn new tasks. lifelong\nlearning has been studied for decades and the\nmost difﬁcult challenge is the catastrophic for-\ngetting, which refers to the inability of models to\nperform old tasks as accurate as new tasks over\ntime.\nAttacks on Deep Learning\nMethodologies\nTransfer Learning\nThe most common form of transfer learning\n(TL) is to transfer the ﬁrst n layers of the teacher\nmodel to the student model, add a few layers\nFeature Extractor\nClassifier\nTeacher Model\nStudent Model\nPerson A\nInternal features of Person A \nInternal features of Person B \nPerson B\nSmall \nperturbation\nInternal features of Person B +  perturbation\n+\nPerson A\nPerson A\nAttacker\nUser\nFigure 2. An adversarial attack on TL. The attacker\nthat has the teacher model can use the feature ex-\ntractor of the model to craft a small perturbation that\nchanges the internal feature of the person B to be\nsimilar to that of person A.\nat the end of the model, and retrain the student\nmodel with a new dataset. As shown in Figure\n2, the part transferred from the teacher model is\ncalled feature extractor and the new layers are\ncalled classiﬁer. The feature extractor provides\nthe high level representation of the input, such\nas the existence of certain objects in object de-\ntection, some frequencies and patterns in voice\ndetection, etc.\nThe main vulnerability of TL methods stems\nfrom the fact the teacher models are often pub-\nlicly available and known. Hence, even in the\nblack-box scenario, an attacker can easily obtain\nthe feature extractor part of the student model. It\nis shown that even if the entire student model is\nretrained with a new dataset, the feature extractor\npart is still close to the teacher’s feature extractor\n[3]. Furthermore, an attacker can manipulate the\noriginal teacher model and create a backdoor on\nit and upload it as a valid version of the teacher\nmodel.\nIn literature, three studies developed strong\nattacks on TL. All the attacks assume white-box\naccess to the teacher model, but black-box access\nto the student model. In [3], authors discuss\nthat if two input images share similar internal\nrepresentation, the classiﬁer likely classiﬁes them\nas the same class. Hence, they use the feature\nextractor from the teacher model to craft a small\nperturbation for the source input to be classiﬁed\n4\nIT Professional\nas another class. As shown in Figure 2, one can\nadd a small perturbation to person B’s image to\nfool the student model to classify it as person\nA. The optimization formulations used to craft\nadversarial inputs in all studies for machine learn-\ning strategies are similar to the formulation in\nattacks on deep models. Hence, we do not cover\nthe optimization formulation in this article.\nSimilar approach has also been used in [2]\nin a form of data poisoning attack. They craft\na set of adversarial images with similar internal\nfeatures to the internal features of an attack target\nclass. Then, they retrain the teacher model with\nthe crafted images to create a backdoor. Hence,\nany student model that uses the poisoned model\nas a teacher is likely prone to the backdoor. It\nis challenging to detect models with backdoors\nsince their behavior to the normal input is similar\nto the original model. Detecting backdoors of\nmachine learning models is still an open problem.\nIn [4], the authors introduce a target-agnostic\nattack that does not need any sample input of the\ntarget class to trigger it. They show that even if\none does not know the typical internal features\nof the target class for natural data, one can still\ntrigger the Softmax layer with high probability.\nThey argue that since the Softmax layer performs\nlinear operation on the feature extractor, numer-\nous internal features can produce the same output\non the Softmax layer as the internal features of\nthe target class have. Hence, they introduce a\nmechanism that attempts to craft images that has\na very large value on one of the internal features.\nSuch crafted images likely trigger the class that\nassigns a higher weight to that feature. This attack\nis even useful on systems that no input sample\nis available to the attacker, such as identiﬁca-\ntion/authentication systems. However, they show\nthat this target-agnostic attack can be defended\nby using a more sophisticated classiﬁer that takes\nthe distributional pattern of internal features into\naccount, not just the linear combination.\nExploratory attacks are generally harder for\ndeep learning models due to their complexity.\nThere is no study on exploratory attack specif-\nically for TL so far. However, due to the fact that\nthe feature extractor part of the model is known\nto the attacker, exploratory attacks may be even\neasier. For instance, the model extraction attack,\nthat aims to extract the entire model parameters,\nneeds to only ﬁnd the parameters of the classiﬁer\npart. So, the search space is signiﬁcantly smaller\nthan the entire model parameters.\nThe main assumption that enables such at-\ntacks is that the teacher model is known and\nthe feature extractor of the student model is not\nsigniﬁcantly different than the teacher model.\nHence, the most effective mechanism that can\npotentially prevent all these attacks is to make\na considerable change to the feature extractor\nso that it becomes different from the publicly\navailable one. Unfortunately, the typical retrain-\ning process of a student model does not change\nthe feature extractor signiﬁcantly. Hence, in [3],\nthey suggest a retraining optimization formulation\nwith a constraint that forces the distance between\nthe student feature extractor and the teacher’s one\nto be higher than a threshold. Such an approach\ncan reduce the chance of successful attack on\nTL. However, it increases the retraining time\nand requires the input samples and labels of the\noriginal teacher model.\nFederated Learning\nFederated learning (FL) allows several par-\nticipants to train a joint model using their local\nprivate data. As it is shown in Figure 3, FL con-\nsists of several rounds in which the shared model\nparameters are sent to a set of participants. Then,\neach participant updates the model parameters by\ntraining the model on a subset of local private data\nand send the parameter updates back to the server.\nFinally, the server aggregates all the updates and\nupdates the shared model. The number of papers\nabout security vulnerabilities and defences of FL\nmethodology is disproportionally large compared\nto other methodologies. In this section, we brieﬂy\nintroduce the security vulnerabilities and defense\ntypes, and encourage readers to follow the refer-\nences and related papers for more details.\nAs shown in Figure 3, a model in a FL\nmethodology has three components from which\nan attack can be launched: 1) the participants,\n2) the server, and 3) the end user. The end user\ndoes not participate in training and can only query\nthe shared model. The end user can essentially\nperform any evasion and exploratory attack that\nhe can launch on general deep models. As far as\nsecurity is concerned, FL does not open up any\nadditional vulnerability that the end user can ex-\nMay/June 2019\n5\nDepartment Head\n\u0001\u0002\u0003\u0004\u0005\u0006\u0005\u0007\u0002\b\u0004\t\n\u0001\u0002\u0003\u0004\u0002\u0003\n\u0001\u0002\u0003\u0004\u0005\u0006\u0005\u0007\u0002\b\u0004\t\u000b\n\u0001\u0002\u0003\u0004\u0005\u0006\u0005\u0007\u0002\b\u0004\t\f\n\u0001\u0001\u0001\n\r\b\u000e\t\u000f\u0010\u0011\u0003\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\u0005\u0004\u0002\u0006\u0007\b\nFigure 3. Federated learning components and training procedure.\nploit. Hence, the evasion and exploratory attacks\nand defenses on deep models apply directly in FL\non the end user side.\nThe separation of updating server from data\nintroduces more security challenges and attacks.\nA participant can launch data poisoning or back-\ndoor attack and it is much harder to defend\nagainst for two reasons: 1) It is much easier for\nan attacker to inject and provide data in FL than\nany other learning strategy by impersonating a\nparticipant, and 2) it is much harder for a server\nto detect poisoned data because it does not have\naccess to the local private data. It is shown that an\nattacker can can cause the shared model to reach\n100% accuracy on the backdoor task in only a\nsingle round [5]. Such attacks target integrity of\nthe shared model and it is difﬁcult to defend\nbecause the server only observes the parameter\nupdates from participants.\nIn addition to data poisoning and backdoor\nattack, a participant can also launch exploratory\nattacks. For instance, it has been shown that a\nmalicious participant can recover a victim partic-\nipant’s data by training a Generative Adversarial\nNetwork (GAN) that generates instances similar\nto the training data [14]. This attack is possi-\nble because the attacker can update the model\nweights such that it forces the model to be more\nover-ﬁtted to the victim’s data at each round. The\nGAN model is trained at each round and grad-\nually learns the distribution of the victim’s data.\nAlthough exploratory attacks are often known to\nbe effective on over-ﬁtted models, it has been\nshown that a non-over-ﬁtted model is still prone\nto generalized membership inference attack [15].\nModel extraction attack is trivial in FL because\nthe model is already shared with participants.\nThe third component of FL is the server. Note\nthat the most paramount goal of FL is to provide\nprivacy of the participants’ local data. Hence, an\ninteresting question arises regarding the data pri-\nvacy: Can the server recover participants’ private\ndata only using parameter updates? Due to the\nseparation of training data and updating server,\nthis attack is unique to FL. In this case, the\nserver is assumed honest-but-curious, that is, the\nserver executes the pre-designed training process\nhonestly but may aim to learn or infer private\nuser information. Several papers introduced novel\nways to partially recover local private data from\nparticipants updates. For instance, if a participant\ncomputes the parameter updates using only a sin-\ngle training sample, the server can easily recover\nthe local training data [13].\nThe defences against an attacker on end user-\nside is similar to any defense on general deep\nlearning models and are not unique in FL. What\nis unique is defence mechanisms to protect pri-\n6\nIT Professional\nvacy of participants’ data (against an attacker\non server-side or other participants) or integrity\nof the shared model (against an attacker on\nparticipant-side). All defense mechanisms pro-\nposed so far present a trade-off among privacy,\nintegrity, computational resources and communi-\ncation overhead. It is not known whether there\nis a practical defence mechanism that protects\nprivacy and integrity, and achieve high accuracy\nor converge in a reasonable time at the same time\nand much research is needed. In the remained of\nthis section, we discuss defence mechanisms for\nprivacy and integrity separately.\nDefence mechanisms for privacy: There are\nthree main approaches to preserve privacy in FL\nsetting: 1) differential privacy, 2) homomorphic\nencryption, and 3) Secure Multi-party Compu-\ntation (SMC). Differential privacy is the most\nwidely used privacy-preserving approach due to\nits simplicity and theoretical guarantees. The goal\nof differential privacy is to ensure that the out-\nput distribution of the model around a training\nsample is not too much different from the output\ndistribution of the exact training sample. Over-\nﬁtted models that lack generalizability suffer from\nthis overﬁtting issue and can be a target of the\nmembership inference attack. The most common\napproach to guarantee differential privacy is to\nadd small random noise to training data samples.\nIt is also possible to add a random noise to the\nupdate parameters obtained by participants [8].\nHowever, there is a trade-off between privacy\nand accuracy. Increasing the noise value increases\nthe privacy, but it may signiﬁcantly degrade the\naccuracy.\nThe second privacy-preserving approach is\nhomomorphic encryption. Homomorphic encryp-\ntion allows certain operations on encrypted data\nin such a way that when decrypted, the results\nmatch the results of performing the same op-\neration on the unencrypted data. Homomorphic\nencryption can be used in two different ways:\nFirst, participant can encrypt their data with\nhomomorphic encryption [11] and then perform\nthe federated learning as with unencrypted data.\nIn this scenario, even if the malicious server\nrecover a participant’s data from the parameters\nupdate, it is encrypted and the server cannot\ndecrypt it. However, homomorphic encryption\nincreases computational overhead and also needs\npolynomial approximation of non-linear functions\n(such as activation functions), which results in a\ntrade-off between privacy and accuracy. Second,\nparticipants can use homomorphic encryption to\nencrypt their updates. In this case, the server\ndoes not have the unencrypted updates to recover\nthe participant’s data during the training phase.\nHowever, studies that propose using homomor-\nphic encryption on weight updates, such as [13],\nuse the server as a storage during training. One\npractical issue with this approach is that the server\ncannot query the model to observe the progress\nof the training. Moreover, it is not suitable for\nthe online learning scenario where the model is\nneeded to be used during the training and training\nis an ongoing process. In such cases, if the\nunencrypted model is continuously revealed to the\nserver for it to use, it eventually neutralizes the\ngoal of using encrypted weight updates. In such\ncases, the homomorphic encryption acts similar\nto SMC, explained in the next paragraph.\nThe third privacy preserving approach is Se-\ncure Multi-party Computation (SMC) [7]. Simply\nput, SMC allows several participants to aggregate\ntheir update in a secure way such that the server\ncan only obtain the aggregated updates. Without\nthe exact update of a participant, the server cannot\nuse update to recover local private data. However,\nSMC cannot still protect the model from infor-\nmation leakage. If the model is inherently prone\nto data leakage, the attacker does not need indi-\nvidual weight updates. For instance, Hitaj et al.\n[14] proposed a Generative Adversarial Network\n(GAN) model that can recover training samples\nof participant by actively participating in training\nphase. Their approach only needs the shared\nmodel parameters and works even when SMC is\nused. The combination of SMC and differential\nprivacy may provide stronger privacy guarantee.\nHowever, they are not practical for large-scale\nscenarios since they incur high computation and\ncommunication costs.\nDefence\nmechanisms\nfor\nintegrity:\nTo\ndefend\nagainst\nmalicious\nparticipants,\nmost\ncommonly-used mechanisms assume that the ma-\njority of participants are honest. Then, they deﬁne\na metric based on which they can remove ma-\nlicious updates, keep the most relevant updates,\nchoose the best update, ﬁnd a robust statistic\nof the updates, etc. For instance, [9] uses the\nMay/June 2019\n7\nDepartment Head\ncoordinated-wise median of all participant up-\ndates to update the shared model weights. Krums\nﬁnds the most honest participant at each round\nand uses its update [10]. There are several similar\napproaches that are out of the scope of this article.\nHowever, the most important limitation of those\napproaches is that they implicitly assume an i.i.d.\ndistribution of data among participants which is\nunrealistic in FL [5]. The impact of non-i.i.d data\ndistribution among participants and how it may\naffect the detection of malicious updates needs\nfurther investigation. Moreover, the performance\nimpact of these approaches where certain number\nof updates are ignored needs more research. For\nexample, in reality, the number of malicious par-\nticipants maybe considerably smaller than honest\nparticipants. Hence, ignoring a large number of\nupdates, for instance when Krums is used, may\ndegrade the performance and deter fast conver-\ngence. Furthermore, certain attacks, such as the\nbackdoor attack, can easily train the local model\nwith similar data distribution as other honest\nparticipants and conceal their true purpose. As\na result, more research is needed to ensure the\nintegrity of FL.\nThe privacy and integrity aspects of FL seem\nto be irreconcilable. All current widely-used\ndefences against data poisoning requires unen-\ncrypted and unaggregated updates of each par-\nticipant, making them prone to privacy leakage\nattacks. On the other hand, all defence mecha-\nnisms to preserve privacy manipulate the updates\nsubstantially, except for homomorphic encryption\non input data, which makes integrity defences\nuseless. To the best of our knowledge, the impact\nof using homomorphic encryption of input data\non weight updates are not well studied. Hence, the\nweight updates in such cases may be consistent\nwith the assumption of defences for integrity.\nHowever, even if the combination of these two\nachieves good integrity and privacy, the compu-\ntation overhead and accuracy degradation is not\nnegligible. Therefore, more research is needed to\nprotect the privacy and integrity in FL.\nModel Compression\nModel compression aims to convert an un-\ncompressed model to a compressed model. The\nattack target can be either the compressed model\nor the uncompressed model. If the attack target is\nthe compressed (uncompressed) model and it is\navailable to the attacker, either in a form of black-\nbox or white-box, similar attacks used in deep\nlearning models can be applied and no speciﬁc\nattack is needed. Hence, the more interesting\nscenario is where an attacker has access to the\ncompressed (uncompressed) model, but she wants\nto launch the attack on the corresponding uncom-\npressed (compressed) version of the model.\nThere is only one study on the model com-\npression attack. In [1], the authors investigate\nthe transferability of adversarial crafted image\nbetween compressed and uncompressed models.\nThey study two compression techniques, namely\npruning and quantisation, and a few well-known\nadversarial attacks. Their main observation is that\nthe adversarial inputs can be transferred between\ncompressed and uncompressed model although\nthe attack effectiveness highly depends on the\nmodel, the compression method, and the attack\nalgorithm. The adversarial inputs are also trans-\nferable between different compressed version of\nan uncompressed model. Moreover, increasing the\ncompression ratio makes it marginally harder to\ntransfer the adversarial samples.\nSince compressed models are often designed\nfor devices directly accessible to customers such\nas smartphones, it is reasonable to assume that\nthe compressed version is available to an attacker.\nHence, the attacker can potentially attack the\nuncompressed model and all other compressed\nversions of the model due to the transferability\nof adversarial samples. Unfortunately, no defense\nmechanism has been proposed yet. Since the\nassumptions and the way the attack is launched\nis somehow similar to TL attacks, attempts to\nchange the compressed version of the model dur-\ning compression may reduce the transferability of\nadversarial samples.\nThe attack and defense mechanisms for the\nmodel compression needs further research. For\ninstance, it is not known whether backdoors\nare also transferable between uncompressed and\ncompressed version. Additionally, the shape of\nclassiﬁcation manifold may dramatically change\nwhen compression is applied. This change may\nnot affect the accuracy of a model on training\ndata, but it may introduce a new possible set\nof adversarial attacks. It may also be possible\nfor an attacker to meddle the training process\n8\nIT Professional\nof the uncompressed model to make it heavily\ndependable on all parameters and weights such\nthat it later prevents any compression method to\neffectively work.\nExploratory attacks have not been yet studied\non model compression. Since the compressed\nmodels are usually easy to access from personal\ndevices, it is interesting to study whether the\nuncompressed model can be extracted from the\ncompressed model. If possible, it would be a\nthreat to businesses that rely on providing com-\npressed models. Similar to TL where the assump-\ntion that feature extractor is available opens new\nattacks, the assumption that compressed model is\nobtainable may enable more attacks.\nMulti-task Learning\nMulti-task learning has been widely used to\nsolve various tasks in image classiﬁcation, natural\nlanguage processing, etc. Even when the goal\nof training a model is to perform single task,\nwe can still train the model for multiple related\nauxiliary tasks to improve the learning of the\ntarget task. However, security vulnerabilities and\ntheir potential defense mechanisms have not been\nstudied yet.\nOne possible data poisoning attack is to poi-\nson the dataset of one task and see if it can\nbe used to target other tasks. Imagine that a\nvictim wants to train a model for facial expression\ndetection. Due to the lack of data, he decides\nto deﬁne an auxilary task of face recognition\nand uses public datasets. An attacker can poison\nthe public dataset for the auxilary task such that\nit creates a set of backdoors on the model on\nwhich it is trained. How to craft images to create\nbackdoor in this scenario is not a trivial question.\nIf an attacker has access to the model, ei-\nther black-box or white-box, general adversarial\nattacks on single task models work on multi-\ntask models as well. However, the multi-task\nmodel may expose new attacks to the model. For\ninstance, lets assume steering direction prediction\nin self-driving car. A victim may deﬁne an aux-\niliary task of classifying road characteristic and\ntype. Now, since the model is trained on both\ntasks that are related, the classiﬁcation output\nof the road characteristic task probably has a\ndirect relation with the output of steering direc-\ntion prediction task. By querying the model with\ndifferent road characteristics, the attacker can ﬁnd\nthe relationship between these tasks. Although the\nattacker may not know how to change the input\nto change the output of the steering direction\ntask, he can change the input to have certain road\ncharacteristic that affects the steering prediction\ntask. In other words, an interesting question is\nthat how we can use task A to craft an adversarial\ninput targeting task B, if task A is easier to craft\nadversarial inputs for. For example, the attacker\nmay have access to a dataset for A, but not for task\nB. There are several possible attacks, including\nexploratory attacks, and potential defenses that\nare not explored yet.\nMeta-learning\nIn the most general form, we are interested\nin ﬁnding a model or recommender, called meta-\nlearner, that outputs a good model or parameters\nfor a new task. For instance, in the transfer\nlearning approach, the meta learner should ﬁnd\nthe most similar task to the target task and give\nthe suitable pre-trained model. To build such a\nmeta-learner, we often need meta-data describing\nprevious tasks, including model architecture and\nparameters, learning algorithms, and evaluation\nresults. In other words, an entire dataset for task\nA and an instance of a model that is trained on the\ndataset is just a single meta-data point for meta-\nlearner. Meta-learner can either recommends a\ngood starting point for training a model for the\nnew task or it can actively learn from models\nthat trained on the new task and iteratively rec-\nommend better models. Although meta-learning\napproaches are vastly different, we discuss them\nin a general form by considering a general setting\nusing meta-data and meta-learners.\nAn interesting question is whether meta-\nlearners are vulnerable to any attack. This is not a\ntrivial question, and we have not found any paper\nregarding meta-learning security. Evasion attack\ndoes not seem to be applicable for meta-learning\nbecause meta-learners are basically used to train\na new model on a new task in an internal training\nprocess within a company. In other words, an\nattacker does not even have an access to the\nblack-box meta-learner to send query to. Even if\nthe attacker hypothetically has an access to the\nmeta-learner, what she can do heavily depends on\nthe meta-learner structure, algorithm, and how it\nMay/June 2019\n9\nDepartment Head\nworks. Hence, it might be possible to perform an\nevasion attack on speciﬁc meta-learners with a set\nof non-restrictive assumptions, but it needs more\ninvestigation and research.\nData poisoning attack, on the other hand,\nseems feasible. Here, the attacker should inject\na set of meta-data points, not just data points.\nHowever, the process of crafting and injecting\nmeta-data point is trickier than data points. In\ngeneral, data poisoning attack can have two aims:\n1) halting or degrading learning process, or 2)\ncreating a backdoor for later use. For the ﬁrst\npurpose, the attacker can upload several datasets\nand models publicly that show good performance,\nbut they do not have suitable model architecture\nor parameters for the new task meta-learner aims\nto learn. Although it can slow down the learning\nprocess, a good meta-learning strategy should\neasily defeat this attack by exploring other model\narchitectures and parameters.\nCreating a backdoor attack on the meta-\nlearner alone might not be very interesting to an\nattacker. The reason is that the attacker probably\ncannot have access to the mete-learner, as we\ndiscussed earlier. Hence, it cannot simply use\nthe backdoor. A more sophisticated and practical\nattack is to ﬁrst create a model with a backdoor\nthat achieves a good performance on a task that\nis not far from the new task meta-learner wants to\nlearn. At the second step, the attacker can force\nthe model to choose his backdoored model, for\nexample as a pre-trained model in the context of\ntransfer learning, by launching data poisoning on\nthe meta-learner. The practicality of such attacks\nand how to defeat them needs further investiga-\ntion.\nLifelong Machine Learning\nAlthough lifelong machine learning has been\nstudied for a long time, the security aspect of it\nhas not been comprehensively studied. The reason\nis that lifelong machine learning is far from being\nsolved and it is still an active area of research.\nThere are a plethora of models and system de-\nsigns about lifelong machine learning which is\nout of the scope of this short overview. Here,\nwe only focus on potential security vulnerabilities\nthat a general lifelong learning may have. Each\nspeciﬁc lifelong learning model may suffer from\nadditional vulnerabilities, but we only consider\ngeneral case due to the lack of space.\nTwo concepts are highly associated with life-\nlong learning: 1) the assumption that the pre-\nvious knowledge is available and it is used to\nlearn new tasks and 2) the sub-goal of prevent-\ning catastrophic forgetting. The ﬁrst assumption\nenables potential data poisoning, backdoor, and\nexploratory attacks. The second goal provides\na new attack target that aims to break lifelong\nlearning by preventing the system from retaining\nprevious knowledge and tasks, which is an attack\non availability.\nThe study of how backdoor and data poison-\ning attacks can affect lifelong learning systems is\nof paramount importance. For example, if a solu-\ntion manages to tackle catastrophic forgetting, is\nit possible for an attacker to create a backdoor on\none task and use it on all other newer tasks? If\npossible, it has a disastrous security consequence\nwhere all tasks are vulnerable.\nIt is also possible to jeopardize the second\nsub-goal, i.e. solving catastrophic forgetting. One\ninteresting approach is to study whether adding\na few carefully crafted training samples, with\ncorrect labels with respect to the new task, can\nchange the structure of the model such that it\nperforms poorly on older tasks. An attacker can\npotentially formulate an optimization problem\nwhere the goal is to change the manifold of the\nold task dramatically while the label is correct\nwith respect to the new task. The attack and de-\nfense mechanisms speciﬁc to the lifelong learning\nneed more research.\nCONCLUSION\nThe adaptation of different machine learning\nmethodologies has signiﬁcantly growth recently\nand their security vulnerabilities should be stud-\nied comprehensively. In this article, we brieﬂy\nintroduce several widely-used machine learning\nmethodologies and discuss their potential secu-\nrity vulnerabilities and challenges. We note that\nthe security aspects of certain machine learning\nmethodologies are open research questions that\ncall for further investigation. We hope that this\narticle enlighten the machine learning commu-\nnity about the potential security vulnerabilities\nof these methodologies and encourage more re-\nsearch to prevent any backlash as a result of\nsecurity breakdown.\n10\nIT Professional\nREFERENCES\n1. Y. Zhao, I. Shumailov, R. Mullins, and R. Anderson, “To\ncompress or not to compress: Understanding the inter-\nactions between adversarial attacks and neural network\ncompression,” arXiv preprint arXiv:1810.00208, 2018.\n2. Y. Ji, X. Zhang, S. Ji, X. Luo, and T. Wang, “Model-reuse\nattacks on deep learning systems,” in Proceedings of\nthe 2018 ACM SIGSAC Conference on Computer and\nCommunications Security.\nACM, 2018, pp. 349–363.\n3. B. Wang, Y. Yao, B. Viswanath, H. Zheng, and B. Y. Zhao,\n“With great training comes great vulnerability: practical\nattacks against transfer learning,” in 27th {USENIX}\nSecurity Symposium ({USENIX} Security 18), 2018, pp.\n1281–1297.\n4. S. Rezaei and X. Liu, “A target-agnostic attack on deep\nmodels: Exploiting security vulnerabilities of transfer\nlearning,” arXiv preprint arXiv:1904.04334, 2019.\n5. E.\nBagdasaryan,\nA.\nVeit,\nY.\nHua,\nD.\nEstrin,\nand\nV. Shmatikov, “How to backdoor federated learning,”\narXiv preprint arXiv:1807.00459, 2018.\n6. Y. Aono, T. Hayashi, L. Wang, and S. Moriai,“ Privacy-\npreserving deep learning: Revisited and enhanced,” in In-\nternational Conference on Applications and Techniques\nin Information Security, 2017, pp. 100–110.\n7. K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone,\nH.B. McMahan, S. Patel, R. Daniel, A. Aaron, and\nK. Seth,“ Practical secure aggregation for privacy-\npreserving machine learning,” in\nProceedings of the\n2017 ACM SIGSAC Conference on Computer and Com-\nmunications Security, 2017, pp. 1175–1191.\n8. R.C. Geyer, T. Klein, and M. Nabi, “Differentially pri-\nvate federated learning: A client level perspective,” arXiv\npreprint arXiv:1712.07557, 2017.\n9. D. Yin, Y. Chen, K. Ramchandran, and P. Bartlett,\n“Byzantine-robust distributed learning: Towards optimal\nstatistical rates,” arXiv preprint arXiv:1803.01498, 2018.\n10. P. Blanchard, R. Guerraoui, J. Stainer et al., “Machine\nlearning with adversaries: Byzantine tolerant gradient\ndescent,” in Advances in Neural Information Processing\nSystems, 2017, pp. 119–129.\n11. R. Bost, R.A. Popa, S. Tu, and S. Goldwasser, “Machine\nlearning classiﬁcation over encrypted data,” in NDSS,\n2015.\n12. N. Carlini, C. Liu, U. Erlingsson, J. Kos, and D. Song,\n“The Secret Sharer: Evaluating and testing unintended\nmemorization in neural networks,” in 28th USENIX Se-\ncurity Symposium (USENIX Security 19), 2019, pp. 267–\n284.\n13. Y. Aono, T. Hayashi, L. Wang, and S. Moriai, “ Privacy-\npreserving deep learning: Revisited and enhanced,” in In-\nternational Conference on Applications and Techniques\nin Information Security, 2017, pp. 100–110.\n14. B. Hitaj, G. Ateniese, and F. Perez-Cruz, “ Deep models\nunder the GAN: information leakage from collaborative\ndeep learning,” in Proceedings of the 2017 ACM SIGSAC\nConference on Computer and Communications Security,\n2017 pp. 603–618.\n15. Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang,\nH. Tang, C.A. Gunter, and K. Chen, “Understanding\nmembership inferences on well-generalized learning\nmodels,” arXiv preprint arXiv:1802.04889, 2018.\nShahbaz Rezaei\nreceived his M.S. degree in infor-\nmation technology from the Sharif University of Tech-\nnology, Tehran, Iran, in 2013. His research interests\ninclude machine learning security, machine learning\napplication, deep reinforcement learning, and com-\nputer networks. He is currently a Ph.D. student at UC\nDavis. Contact him at srezaei@ucdavis.edu.\nXin Liu\nreceived her Ph.D. degree from Purdue\nUniversity in 2002. She is currently a professor in\nthe Computer Science Department, University of Cal-\nifornia, Davis. Her current research focuses on data-\ndriven approach in networking (i.e., using and devel-\noping machine learning and optimization techniques\nfor network control and management). She is an IEEE\nFellow. Contact her at xinliu@ucdavis.edu.\nMay/June 2019\n11\n",
  "categories": [
    "cs.CR",
    "cs.AI",
    "cs.LG"
  ],
  "published": "2019-12-08",
  "updated": "2019-12-08"
}