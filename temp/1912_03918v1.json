{
  "id": "http://arxiv.org/abs/1912.03918v1",
  "title": "Transformer Based Reinforcement Learning For Games",
  "authors": [
    "Uddeshya Upadhyay",
    "Nikunj Shah",
    "Sucheta Ravikanti",
    "Mayanka Medhe"
  ],
  "abstract": "Recent times have witnessed sharp improvements in reinforcement learning\ntasks using deep reinforcement learning techniques like Deep Q Networks, Policy\nGradients, Actor Critic methods which are based on deep learning based models\nand back-propagation of gradients to train such models. An active area of\nresearch in reinforcement learning is about training agents to play complex\nvideo games, which so far has been something accomplished only by human\nintelligence. Some state of the art performances in video game playing using\ndeep reinforcement learning are obtained by processing the sequence of frames\nfrom video games, passing them through a convolutional network to obtain\nfeatures and then using recurrent neural networks to figure out the action\nleading to optimal rewards. The recurrent neural network will learn to extract\nthe meaningful signal out of the sequence of such features. In this work, we\npropose a method utilizing a transformer network which have recently replaced\nRNNs in Natural Language Processing (NLP), and perform experiments to compare\nwith existing methods.",
  "text": "TRANSFORMER BASED REINFORCEMENT LEARNING FOR GAMES\nUddeshya Upadhyay†, Nikunj Shah⋆Sucheta Ravikantiα, Mayanka Medhe†\n†Department of Computer Science and Engineering, Indian Institute of Technology-Bombay\n⋆Department of Mechanical Engineering, Indian Institute of Technology-Bombay\nαDepartment of Electrical Engineering, Indian Institute of Technology-Bombay\nABSTRACT\nRecent times have witnessed sharp improvements in rein-\nforcement learning tasks using deep reinforcement learning\ntechniques like Deep Q Networks, Policy Gradients, Actor\nCritic methods which are based on deep learning based mod-\nels and back-propagation of gradients to train such models.\nAn active area of research in reinforcement learning is about\ntraining agents to play complex video games, which so far\nhas been something accomplished only by human intelli-\ngence. Some state of the art performances in video game\nplaying using deep reinforcement learning are obtained by\nprocessing the sequence of frames from video games, pass-\ning them through a convolutional network to obtain features\nand then using recurrent neural networks to ﬁgure out the\naction leading to optimal rewards. The recurrent neural net-\nwork will learn to extract the meaningful signal out of the\nsequence of such features. In this work, we propose a method\nutilizing transformer networks which have recently replaced\nRNNs in Natural Language Processing (NLP), and perform\nexperiments to compare with existing methods.\nIndex Terms— Deep Learning, Transformers, Q-Learning,\nLong Short Term Memory (LSTM), Natural Language Pro-\ncessing (NLP)\n1. INTRODUCTION AND RELATED WORK\nRecent advancements in reinforcement learning have wit-\nnessed the heavy use of Deep Neural Networks (DNN) to\nperform many of the reinforcement learning tasks such as\nprediction and control. These classes of approaches at the\nintersection of deep learning and reinforcement learning are\nknown as Deep reinforcement learning (DRL). DRL uses\ndeep learning and reinforcement learning principles to create\nefﬁcient algorithms that can be applied on areas like robotics,\nvideo games, ﬁnance, and healthcare [11].\nImplementing\nneural networks like deep convolutional networks with re-\ninforcement learning algorithms such as Q-learning, Actor\nCritic or Policy Search results in a powerful model (DRL)\nthat is capable to scale to previously unsolvable problems\n[12]. That is because DRL usually work with raw sensors or\nimage signals as input (for instance in Deep Q Networks -\nDQN for ATARI games [13]) and can receive the beneﬁt of\nend-to-end reinforcement learning as well as that of convolu-\ntional neural networks. In other words, neural networks act\nas function approximators, of action-value functions used in\npredicting the next best action, which is particularly useful in\nreinforcement learning when the state or action space is too\nlarge to be completely known or to be stored in memory.\nRecently, DQN with recurrent layers also called as\nDeep Recurrent Q Networks (DRQN), have started show-\ning promising results in reinforcement learning problem.\nThey have the capability to outperform the DQNs outcomes\nand generate better trained agents in certain scenarios. The\nprimary reason for this being, DQN has limited or no amount\nof distant history. In practice, DQNs are often trained with\njust a single state representation corresponding to current\ntime-steps (i.e often times they neglect the temporal aspects\nof the input), even when fed with the sequence of state rep-\nresentation, standard DQN architecture without RNNs are\nunable to capture and extract from the temporal aspect of the\nsequences. Thus DQN will be unable to master games that\nrequire the player to remember events more distant in the\npast. Put differently, any game that requires a memory of past\nstates in the trajectory will appear non-Markovian because the\nfuture game states (and rewards) depend on more than just\nDQNs current input. Instead of a Markov Decision Process\n(MDP), the game becomes a Partially-Observable Markov\nDecision Process (POMDP) [14]. Hence, LSTM along with\nDQN is used in such cases.\nSince LSTM or recurrent neural networks, in general,\nhave had a strong presence in the Natural Language Pro-\ncessing (NLP), we tried to implement a technique inspired\nfrom NLP called Transformer to perform the reinforcement\nlearning tasks. Transformer was ﬁrst introduced in [15] to\nperform sequence-to-sequence translation, however, it has\nbeen adapted successfully in various applications spanning\nlanguage, speech, etc.\nNLP uses a sequence-to-sequence architecture at the core\nin various tasks.\nIn NLP we generally need to analyze a\nsequence of words and generate another sequence of words\nbased on them, therefore language translation task is one such\nexample where sequence to sequence architecture is applica-\nble. LSTM was a popular choice to be used as an encoder\narXiv:1912.03918v1  [cs.LG]  9 Dec 2019\nand decoder for the above-speciﬁed task and related architec-\nture. LSTM based approach implicitly accounts for ’atten-\ntion’ which is a mechanism that looks at an input sequence\nand decides at each step which other parts of the sequence are\nimportant. The transformer was a novel technique introduced\nto replace this attention-mechanism performed by LSTM by\nmore effective and explicit attention-mechanism. It is also an\nencoder-decoder model but differs from LSTM by avoiding\nany usage of recurrent neural networks which is common in\nLSTM, GRU, etc. This improves training time as well as ac-\ncuracy in NLP tasks.\n2. METHODS AND EXPERIMENTS\nIn the following we describe the techniques we used to extend\nthe current framework to accommodate our transformer based\nproposal and training procedure, we also describe the various\nexperiments performed to compare methods.\nNatural Language Processing often deals with the prob-\nlem of predicting one set of sequences from another set of se-\nquences (for instance, a sequence of words from the sequence\nof acoustic features for automatic speech recognition tasks,\nor sequence of words in German from a sequence of words in\nEnglish for language translation task, etc). As described in the\nabove section, deep reinforcement learning also makes use of\nthe sequence and we take inspiration from recent updates in\nNLP to propose a new method for DRL.\nFig. 1: Frame from the Cartpole environment of OpenAI\nGym. The task is to balance the pole on the cart, by mov-\ning the cart left or right\n2.1. Environment\nIn our experiments, we set up an environment for the ”Cart-\npole” game (using OpenAI gym [16]) where the goal is to bal-\nance the pole on the cart (i.e., prevent the pole from falling) by\nmoving the cart left or right. Figure 1 shows a frame from the\ngame to visualize the environment. The set of actions A con-\nsists of {left, right} and the environment provides a reward\nfrom the set of rewards R consisting of {+1, −1}. For ever\ntime-step where the pole does not fall the environment pro-\nvides a reward of +1 and when the pole falls (i.e., the angle\nit makes from the cart crosses a certain threshold) the episode\nis completed and the agent receives a reward of −1. The typ-\nical deep reinforcement learning pipeline passes the frames\n(or sequence of frames) through a deep convolutional neural\nnetwork to extract the useful features, which are further pro-\ncessed to estimate the value function (V ) or the state-action\nvalue function (Q). However, this results in model with rela-\ntively larger number of parameters, which also requires access\nto large GPUs to train the models using learning algorithms,\nnot to mention that such algorithms take signiﬁcantly longer\nto train. In order to overcome this problem we decided to\nuse the RAM provided by the OpenAI environment describ-\ning the state of the game. In this case the state of the game\ncan be described using the:\n• position of the cart (on the X-axis, from -4.8 to 4.8)\n• velocity of the cart (from −∞to ∞)\n• angle the pole makes with the cart (from -24 to 24)\n• pole velocity at tip (from −∞to ∞)\nHowever, to make the problem more interesting and difﬁ-\ncult we set up a partially observable Markov decision process\n(POMDP), where the system dynamics are known to follow\nan MDP but the agent can not directly observe the states. In\nour experiments, the agent only observes the partial state con-\nsisting position of the cart and the angle pole makes with the\ncart. Different algorithms evaluated in this paper takes in the\nsequence of partially observed game states (a window of pre-\nvious states preceding the current state) and predicts the ac-\ntion to be taken at the current state. The intuition is that the\nsequence of partially observed states should be sufﬁcient for\nthe algorithms to learn about the missing state features and\nact optimally.\nIn this work, we evaluate three different classes of algo-\nrithms namely, 1) Deep Q-Networks (DQN), 2) Deep Re-\ncurrent Q-Networks (DRQN) and 3) Deep Transformer Q-\nNetwork (DTQN).\n2.2. DQN, DRQN, and DTQN\nDeep Q-Learning (DQN) uses a neural network to approxi-\nmate the Q-value function. The state is given as the input and\nthe Q-value of all possible actions is generated as the output.\nIn a typical Deep Q learning setup, all the past experiences are\nﬁrst stored in the memory, the next action is then determined\nby using epsilon greedy policy with respect to current Q val-\nues and the ﬁnal loss is computed using equation 1, where\nSt, at, rt is the state, action taken and reward received at time-\nstep t and St+1 is the state at the next time-step.\nL = ||rt + γ max\na∈A Q(St+1, a) −Q(St, at)||2\n2\n(1)\nThe term rt + γ maxa∈A Q(St+1, a) in equation 1 is known\nas the target and will change erratically at every time-step as\n(a) DQN\n(b) DRQN\n(c) DTQN\nFig. 2: Different representative architectures. (a) DQN, (b) DRQN, (c) DTQN.\nthe Q values will change erratically at every time-step, in or-\nder to make the learning more stable we use a second copy\nof the deep neural network called target network. The target\nnetwork has the same architecture as the function approxi-\nmator but with frozen parameters. For every C iterations (a\nhyperparameter), the parameters from the prediction network\nare copied to the target network. This leads to more stable\ntraining. Figure 2a shows a representative architecture for\nthe DQN. In our experiments, the input to DQN is the fea-\nture vector produced by concatenating the partially observed\nstates from current and time-steps with the partially observed\nstates of previous time-steps (4 time-steps in total, including\ncurrent time-step).\nFig. 3: Transformer: encoder taking input sequence and de-\ncoder taking output sequences\nDeep Recurrent Q-Learning (DRQN) uses a recurrent\nneural network to approximate the Q-value function.\nSe-\nquence of states is given as the input and the network consists\nof RNN unit (LSTM/GRU), the output of the RNN unit at the\nﬁnal time-step is used to predict the Q-value of all possible\nactions is generated. Such networks are also often trained\nusing the loss function deﬁned equation 1. Figure 2b shows\na representative architecture for DRQN. In our experiments,\nthe RNN unit (GRU) is fed the sequence of partially observed\nstates, the output from the ﬁnal time-step is used to predict\nthe Q-value function.\nThe idea behind Deep Transformer Q-learning (DTQN) is\nto use a transformer instead of RNN to extract the meaning-\nful feature out of the input sequence. However, transformers\nwere designed to work for sequence to sequence (seq2seq)\ntasks such as automatic speech recognition or language trans-\nlation. But in this work we do not have s sequence to sequence\ntask, rather we need to predict the Q value function given the\ninput sequence.\nFig. 4: Transformer: encoder taking input sequence and de-\ncoder taking output sequences\nThe transformer model consists of encoder and decoder\nmodules, as shown in ﬁgure 3.\nThe encoder module pro-\ncesses the input sequence of embeddings (tokens from input\nsequence are ﬁrst embedded and then passed through the net-\nwork) through a multi-head attention module followed by a\nfeed-forward neural network. In our experiments we only use\n(a) traces of DQN\n(b) traces of DRQN\n(c) traces of DTQN\nFig. 5: Scores vs Episodes for multiple runs of different algo-\nrithms. (a) DQN, (b) DRQN, (c) DTQN.\nthe encoder module (since our task is not Seq2Seq) to extract\nthe features from input sequence which are further used to\npredict the Q-values. The multi-head attention module in the\nencoder refers to the self-attention layer which is a mecha-\nnism relating different positions of a single sequence in order\nto compute a representation of the sequence. Figure 4 shows\nthe multi-head attention module used in the encoder, V , K,\nQ denotes the value vector, key vector, query vector. Figure\n2c shows a representative architecture for DTQN.\n3. RESULTS AND DISCUSSIONS\nWe did multiple experiments for each of the following DQN,\nDRQN and DTQN based reinforcement learning algorithms.\nEach algorithm was trained for 5000 episodes and we ran 10\ndifferent instances for each of the algorithms with random\ninitialization. Figure 5 shows the value of the scores over\nepisodes for different runs. We see that DQN and DTQN\nperformed worse compared to our DRQN model. While on\naverage the scores for the DQN and DTQN are decreasing,\nwe see that there are a few experiments where the scores im-\nprove during training as shown in the graph, but for a majority\nof the traces, the scores did not improve when using DQN or\nDTQN. However, the results show signiﬁcant improvement in\nscores when using the DRQN model.\nAs we used location and angle as the only inputs to our\nneural networks determine the next action, the scores and loss\nfunction did not show very promising trends in all three cases.\nStill, it was possible to make a distinction as to which one\nof the three: DQN, DRQN, or DTQN performed better by\nanalyzing their relative scores and loss values. DRQN gave\nthe best score results with the maximum reaching to 135 in\none of the test cases. DTQN performed the worst on average.\nWe believe that a transformer-based approach is indeed\nnot suitable for solving the reinforcement learning problem\nat least in cases where input is taken in the form of a pair of\nlocation value and angle value. Reason for the failure can be\npointed out as follows: LSTM based approach (DRQN) cap-\ntures the temporal attributes of the sequence of inputs whereas\ntransformer-based approach tries to put attention on different\ntime-steps of the sequence to obtain the representation for in-\nput sequence and does not explicitly tries to capture the tem-\nporal aspect of the sequence. As our problem is more of tem-\nporal related as the next state is the state at the next time step,\nLSTM achieves better results. We would also like to add that\nduring our course of work we found that training RL algo-\nrithm, for video games, based on DQN or neural networks, in\ngeneral, is difﬁcult to train and the performance greatly de-\npends upon the random initialization. As can be seen from\nthe traces, some of the random initialization perform too bad\nwhile a few perform well.\n4. CONCLUSIONS AND FUTURE WORK\nIn this work, we propose a new transformer based model for\nreinforcement learning, the inspiration for the same is de-\nrived from the fact that the recent advancements in NLP have\nbeen achieved by moving away from RNNs and introducing\nthe new transformer based model. Even though the standard\ntransformer consists of both encoder and decoder modules as\nit’s designed to perform Seq2Seq task, we decided to use only\nthe encoder module with the multi-head attention mechanism\nfrom the transformer to extract important features from the\nstates to learn the Q-values. We perform experiments on a\nPOMDP and compared multiple algorithms. We conclude\nthat using the state representation provided by the RAM of\nthe game (X-coordinate of the cart and the angle made by\npole with the cart), the conventional DRQN (based on RNNs\nsuch as LSTM and GRU) perform better than DQN or the\nproposed DTQN. As part of future work, we intend to com-\npare different algorithms using different state representation\nlike images instead of RAM values.\n5. REFERENCES\n[1] Rodriguez Ruben, Bontrager Philip, Togelius Julian,\nand Liu Jialin, “Deep reinforcement learning for gen-\neral video game ai,” arXiv preprint arXiv:1806.02448,\n2018.\n[2] Jaakkola Tommi, Sin Satinder P, and Jo Michael I, “Re-\ninforcement learning algorithm for partially observable\nmarkov decision problems,” In Advances in Neural In-\nformation Processing Systems 7 (NIPS), pages 345-352,\n1995.\n[3] Lample Guillaume and Chaplot Devendra S, “Playing\nfps games with deep reinforcement learning,”\narXiv\npreprint arXiv:1609.05521, 2018.\n[4] Cheng Jianpeng, Dong Li, and Lapata Mirella, “Long\nshort-term memory-networks for machine reading,”\narXiv preprint arXiv:1601.06733, 2016.\n[5] Gehring Jonas, Auli Michael, Grangier David, Yarats\nDenis,\nand Dauphin Yann N.,\n“Convolutional\nsequence to sequence learning,”\narXiv preprint\narXiv:1705.03122v2, 2017.\n[6] Parisotto Emilio, Song H. Francis, Rae Jack W., Pas-\ncanu Razvan, Gulcehre Caglar, Jayakumar Siddhant M,\nJaderberg Max, Lopez Kaufman Raphael, Clark Aidan,\nNoury Seb, Botvinick Matthew M., Heess Nicolas, and\nHadsell Raia, “Stabilizing transformers for reinforce-\nment learning,”\narXiv preprint arXiv:1910.06764v1,\n2019.\n[7] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio, “Empirical evaluation of gated re-\ncurrent neural networks on sequence modeling,” arXiv\npreprint arXiv:1412.3555, 2014.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, “Bert: Pre-training of deep bidirec-\ntional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[9] Yaser Keneshloo, Tian Shi, Naren Ramakrishnan, and\nChandan K Reddy, “Deep reinforcement learning for\nsequence-to-sequence models,” IEEE Transactions on\nNeural Networks and Learning Systems, 2019.\n[10] John N. Tsitsiklis and Benjamin Van Roy,\n“An anal-\nysis of temporal-difference learning with function ap-\nproximation,” IEEE Transactions on Automatic Control,\n1997.\n[11] Vincent Franc¸ois-Lavet, Peter Henderson, Riashat Is-\nlam, Marc G Bellemare, Joelle Pineau, et al., “An intro-\nduction to deep reinforcement learning,” Foundations\nand Trends R⃝in Machine Learning, vol. 11, no. 3-4, pp.\n219–354, 2018.\n[12] Volodymyr Mnih, Koray Kavukcuoglu, David Silver,\nAlex Graves, Ioannis Antonoglou, Daan Wierstra, and\nMartin Riedmiller, “Playing atari with deep reinforce-\nment learning,” arXiv preprint arXiv:1312.5602, 2013.\n[13] Kai Arulkumaran,\nMarc Peter Deisenroth,\nMiles\nBrundage, and Anil Anthony Bharath, “Deep reinforce-\nment learning: A brief survey,” IEEE Signal Processing\nMagazine, vol. 34, no. 6, pp. 2638, Nov 2017.\n[14] Matthew Hausknecht and Peter Stone, “Deep recurrent\nq-learning for partially observable mdps,” in 2015 AAAI\nFall Symposium Series, 2015.\n[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin, “Attention is all you need,” in Ad-\nvances in neural information processing systems, 2017,\npp. 5998–6008.\n[16] Greg Brockman, Vicki Cheung, Ludwig Pettersson,\nJonas Schneider, John Schulman, Jie Tang, and Woj-\nciech Zaremba, “Openai gym,” 2016.\n",
  "categories": [
    "cs.LG",
    "cs.NE"
  ],
  "published": "2019-12-09",
  "updated": "2019-12-09"
}