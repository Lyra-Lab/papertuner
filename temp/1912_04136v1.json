{
  "id": "http://arxiv.org/abs/1912.04136v1",
  "title": "Optimism in Reinforcement Learning with Generalized Linear Function Approximation",
  "authors": [
    "Yining Wang",
    "Ruosong Wang",
    "Simon S. Du",
    "Akshay Krishnamurthy"
  ],
  "abstract": "We design a new provably efficient algorithm for episodic reinforcement\nlearning with generalized linear function approximation. We analyze the\nalgorithm under a new expressivity assumption that we call \"optimistic\nclosure,\" which is strictly weaker than assumptions from prior analyses for the\nlinear setting. With optimistic closure, we prove that our algorithm enjoys a\nregret bound of $\\tilde{O}(\\sqrt{d^3 T})$ where $d$ is the dimensionality of\nthe state-action features and $T$ is the number of episodes. This is the first\nstatistically and computationally efficient algorithm for reinforcement\nlearning with generalized linear functions.",
  "text": "Optimism in Reinforcement Learning with Generalized Linear\nFunction Approximation\nYining Wang∗1, Ruosong Wang†2, Simon S. Du‡3, and Akshay Krishnamurthy§4\n1University of Florida, Gainsville, FL\n2Carnegie Mellon University, Pittsburgh, PA\n3Institute for Advanced Studies, Princeton, NJ\n4Microsoft Research, New York, NY\nAbstract\nWe design a new provably efﬁcient algorithm for episodic reinforcement learning with generalized\nlinear function approximation. We analyze the algorithm under a new expressivity assumption that we call\n“optimistic closure,” which is strictly weaker than assumptions from prior analyses for the linear setting.\nWith optimistic closure, we prove that our algorithm enjoys a regret bound of ˜O(\n√\nd3T) where d is the\ndimensionality of the state-action features and T is the number of episodes. This is the ﬁrst statistically\nand computationally efﬁcient algorithm for reinforcement learning with generalized linear functions.\n1\nIntroduction\nWe study episodic reinforcement learning problems with inﬁnitely large state spaces, where the agent must\nuse function approximation to generalize across states while simultaneously engaging in strategic exploration.\nSuch problems form the core of modern empirical/deep-RL, but relatively little work focuses on exploration,\nand even fewer algorithms enjoy strong sample efﬁciency guarantees.\nOn the theoretical side, classical sample efﬁciency results from the early 00s focus on “tabular” envi-\nronments with small ﬁnite state spaces (Kearns and Singh, 2002; Brafman and Tennenholtz, 2002; Strehl\net al., 2006), but as these methods scale with the number of states, they do not address problems with inﬁnite\nor large state spaces. While this classical work has inspired practically effective approaches for large state\nspaces (Bellemare et al., 2016; Osband et al., 2016; Tang et al., 2017), these methods do not enjoy sample\nefﬁciency guarantees. More recent theoretical progress has produced provably sample efﬁcient algorithms for\ncomplex environments, but many of these algorithms are relatively impractical (Krishnamurthy et al., 2016;\nJiang et al., 2017). In particular, these methods are computationally inefﬁcient or rely crucially on strong\ndynamics assumptions (Du et al., 2019b).\nIn this paper, with an eye toward practicality, we study a simple variation of Q-learning, where we\napproximate the optimal Q-function with a generalized linear model. The algorithm is appealingly simple:\ncollect a trajectory by following the greedy policy corresponding to the current model, perform a dynamic\nprogramming back-up to update the model, and repeat. The key difference over traditional Q-learning-like\nalgorithms is in the dynamic programming step. Here we ensure that the updated model is optimistic in the\nsense that it always overestimates the optimal Q-function. This optimism is essential for our guarantees.\n∗yining.wang@warrington.uﬂ.edu, †ruosongw@andrew.cmu.edu,‡ssdu@ias.edu,§akshaykr@microsoft.com\n1\narXiv:1912.04136v1  [stat.ML]  9 Dec 2019\nOptimism in the face of uncertainty is a well-understood and powerful algorithmic principle in short-\nhorizon (e.g,. bandit) problems, as well as in tabular reinforcement learning (Azar et al., 2017; Dann et al.,\n2017; Jin et al., 2018). With linear function approximation, Yang and Wang (2019) and Jin et al. (2019)\nshow that the optimism principle can also yield provably sample-efﬁcient algorithms, when the environment\ndynamics satisfy a certain linearity properties. Their assumptions are always satisﬁed in tabular problems, but\nare somewhat unnatural in settings where function approximation is required. Moreover as these assumptions\nare directly on the dynamics, it is unclear how their analysis can accommodate other forms of function\napproximation, including generalized linear models.\nIn the present paper, we replace explicit dynamics assumptions with expressivity assumptions on the\nfunction approximator, and, by analyzing a similar algorithm to Jin et al. (2019), we show that the optimism\nprinciple succeeds under these strictly weaker assumptions. More importantly, the relaxed assumption\nfacilitates moving beyond linear models, and we demonstrate this by providing the ﬁrst practical and provably\nefﬁcient RL algorithm with generalized linear function approximation.\n2\nPreliminaries\nWe consider episodic reinforcement learning in a ﬁnite-horizon markov decision process (MDP) with possibly\ninﬁnitely large state space S, ﬁnite action space A, initial distribution µ ∈∆(S), transition operator\nP : S × A →∆(S), reward function R : S × A →∆([0, 1]) and horizon H. The agent interacts with the\nMDP in episodes and, in each episode, a trajectory (s1, a1, r1, s2, a2, r2, . . . , sH, aH, rH) is generated where\ns1 ∼µ, for h > 1 we have sh ∼P(· | sh−1, ah−1), rh ∼R(sh, ah), and actions a1:H are chosen by the\nagent. For normalization, we assume that PH\nh=1 rh ∈[0, 1] almost surely.\nA (deterministic, nonstationary) policy π = (π1, · · · , πH) consists of H mappings πh : S →A, where\nπh(sh) denotes the action to be taken at time point h if at state sh ∈S The value function for a policy π is a\ncollection of functions (V π\n1 , . . . , V π\nH) where V π\nh : S →R is the expected future reward the policy collects if\nit starts in a particular state at time point h. Formally,\nV π\nh (s) ≜E\n\" H\nX\nh′=h\nrh′ | sh = s, ah:H ∼π\n#\n.\nThe value for a policy π is simply V π ≜Es1∼µ [V π\n1 (s1)], and the optimal value is V ⋆≜maxπ V π, where\nthe maximization is over all nonstationary policies. The typical goal is to ﬁnd an approximately optimal\npolicy, and in this paper, we measure performance by the regret accumulated over T episodes,\nReg(T) ≜TV ⋆−E\n\" T\nX\nt=1\nH\nX\nh=1\nrh,t\n#\n.\nHere rh,t is the reward collected by the agent at time point h in the tth episode. We seek algorithms with\nregret that is sublinear in T, which demonstrates the agent’s ability to act near-optimally.\n2.1\nQ-values and function approximation\nFor any policy π, the state-action value function, or the Q-function is a sequence of mappings Qπ =\n(Qπ\n1, . . . , Qπ\nH) where Qπ\nh : S × A →R is deﬁned as\nQπ\nh(s, a) ≜E\n\" H\nX\nh′=h\nrh′ | sh = s, ah = a, ah+1:H ∼π\n#\n.\n2\nThe optimal Q-function is Q⋆\nh ≜Qπ⋆\nh where π⋆≜argmaxπ V π is the optimal policy.\nIn the value-based function approximation setting, we use a function class G to model Q⋆. In this paper,\nwe always take G to be a class of generalized linear models (GLMs), deﬁned as follows: Let d ∈N be a\ndimensionality parameter and let Bd ≜\n\b\nx ∈Rd : ∥x∥2 ≤1\n\t\nbe the ℓ2 ball in Rd.\nDeﬁnition 1. For a known feature map φ : S × A →Bd and a known link function f : [−1, 1] 7→[−1, 1]\nthe class of generalized linear models is G ≜{(s, a) 7→f(⟨φ(s, a), θ⟩) : θ ∈Bd}.\nAs is standard in the literature (Filippi et al., 2010; Li et al., 2017), we assume the link function satisﬁes\ncertain regularity conditions.\nAssumption 1. f(·) is either monotonically increasing or decreasing. Furthermore, there exist absolute\nconstants 0 < κ < K < ∞and M < ∞such that κ ≤|f′(z)| ≤K and |f′′(z)| ≤M for all |z| ≤1.\nFor intuition, two example link functions are the identity map f(z) = z and the logistic map f(z) =\n1/(1 + e−z) with bounded z. It is easy to verify that both of these maps satisfy Assumption 1.\n2.2\nExpressivity assumptions: realizability and optimistic closure\nTo obtain sample complexity guarantees that scale polynomially with problem parameters in the function\napproximation setting, it is necessary to posit expressivity assumptions on the function class G (Krishnamurthy\net al., 2016; Du et al., 2019a). The weakest such condition is realizability, which posits that the optimal\nQ function is in G, or at least well-approximated by G. Realizability alone sufﬁces for provably efﬁcient\nalgorithms in the “contextual bandits” setting where H = 1 (Li et al., 2017; Filippi et al., 2010; Abbasi-\nYadkori et al., 2011), but it does not seem to be sufﬁcient when H > 1. Indeed in these settings it is common\nto make stronger expressivity assumptions (Chen and Jiang, 2019; Yang and Wang, 2019; Jin et al., 2019).\nFollowing these works, our main assumption is a closure property of the Bellman update operator Th.\nThis operator has type Th : (S × A →R) →(S × A →R) and is deﬁned for all s ∈S, a ∈A as\nTh(Q)(s, a) ≜E [rh + VQ(sh+1) | sh = s, ah = a] ,\nVQ(s) ≜max\na∈A Q(s, a).\nThe Bellman update operator for time point H is simply TH(Q)(s, a) ≜E [rH | sH = s, aH = a], which\nis degenerate. To state the assumption, we must ﬁrst deﬁne the enlarged function class Gup. For a d × d\nmatrix A, A ⪰0 denotes that A is positive semi-deﬁnite. For a positive semi-deﬁnite matrix A, ∥A∥op is the\nmatrix operator norm, which is just the largest eigenvalue, and ∥x∥A ≜\n√\nx⊤Ax is the matrix Mahalanobis\nseminorm. For a ﬁxed constant Γ ∈R+ that we will set to be polynomial in d and log(T), deﬁne\nGup ≜\nn\n(s, a) 7→min {1, f(⟨φ(s, a), θ⟩) + γ ∥φ(s, a)∥A} : θ ∈Bd, 0 ≤γ ≤Γ, A ⪰0, ∥A∥op ≤1\no\n,\nThe class Gup contains G in addition to all possible upper conﬁdence bounds that arise from solving least\nsquares regression problems using the class G. We now state our main expressivity assumption, which we\ncall optimistic closure.\nAssumption 2 (Optimistic closure). For any 1 ≤h < H and g ∈Gup, we have Th(g) ∈G.\nIn words, when we perform a Bellman backup on any upper conﬁdence bound function for time point\nh + 1, we obtain a generalized linear function at time h. While this property seems quite strong, we note\nthat related closure-type assumptions are common in the literature, discussed in detail in Section 2.3. More\nimportantly, we will see shortly that optimistic closure is actually strictly weaker than previous assumptions\nused in our RL setting where exploration is required. Before turning to these discussions, we mention two\nbasic properties of optimistic closure. The proofs are deferred to Appendix A.\n3\nFact 1 (Optimistic closure and realizability). Optimistic closure implies that Q⋆∈G (realizability).\nFact 2 (Optimistic closure in tabular settings). If S is ﬁnite and φ(s, a) = es,a is the standard-basis feature\nmap, then under Assumption 1 we have optimistic closure.\n2.3\nRelated work\nThe majority of the theoretical results for reinforcement learning focus on the tabular setting where the state\nspace is ﬁnite and sample complexities scaling polynomially with |S| are tolerable (Kearns and Singh, 2002;\nBrafman and Tennenholtz, 2002; Strehl et al., 2006). Indeed, by now there are a number of algorithms that\nachieve strong guarantees in these settings (Dann et al., 2017; Azar et al., 2017; Jin et al., 2018; Simchowitz\nand Jamieson, 2019). Via Fact 2, our results apply to this setting, and indeed our algorithm can be viewed as\na generalization of an existing tabular algorithm (Azar et al., 2017) to the function approximation setting.1\nTurning to the function approximation setting, several other results concern function approximation in\nsetings where exploration is not an issue, including the inﬁnite-data regime (Munos, 2003; Farahmand et al.,\n2010) and “batch RL” settings where the agent does not control the data-collection process (Munos and\nSzepesvári, 2008; Antos et al., 2008; Chen and Jiang, 2019). While the settings differ, all of these results\nrequire that the function class satisfy some form of (approximate) closure with respect to the Bellman operator.\nThese results therefore provide motivation for our optimistic closure assumption.\nA recent line of work studies function approximation in settings where the agent must explore the\nenvironment (Krishnamurthy et al., 2016; Jiang et al., 2017; Du et al., 2019b). The algorithms developed here\ncan accommodate function classes beyond generalized linear models, but they are still relatively impractical\nand the more practical ones require strong dynamics assumptions (Du et al., 2019b). In contrast, our algorithm\nis straightforward to implement and does not require any explicit dynamics assumption. As such, we view\nthese results as complementary to our own.\nLastly, we mention the recent results of Yang and Wang (2019) and Jin et al. (2019), which are most\nclosely related to our work. Both papers study MDPs with certain linear dynamics assumptions (what\nthey call the Linear MDP assumption) and use linear function approximation to obtain provably efﬁcient\nalgorithms. Our algorithm is almost identical to that of Jin et al. (2019), but, as we will see, optimistic closure\nis strictly weaker than their Linear MDP assumption (which is strictly weaker than the assumption of Yang\nand Wang (2019)). Further, and perhaps more importantly, our results enable approximation with GLMs,\nwhich are incompatible with the Linear MDP structure. Hence, the present paper can be seen as a signiﬁcant\ngeneralization of these recent results.\n3\nOn optimistic closure\nFor a more detailed comparison to the recent work results of Yang and Wang (2019) and Jin et al. (2019), we\ndeﬁne the linear MPD model studied in the latter work.\nDeﬁnition 2. An MDP is said to be a linear MDP if there exist known feature map ψ : S ×A →Rd, unknown\nsigned measures µ : S →Rd, and an unknown vector η ∈Rd such that (1) P(s′|s, a) = ⟨ψ(s, a), µ(s′)⟩\nholds for all states s, s′ and actions a, and (2) E[r | s, a] = ⟨ψ(s, a), η⟩.\nLinear MDPs are studied by Jin et al. (2019), who establish a\n√\nT-type regret bound for an optimistic\nalgorithm. This assumption already subsumes that of Yang and Wang (2019), and related assumptions also\nappear elsewhere in the literature (Bradtke and Barto, 1996; Melo and Ribeiro, 2007). In this section, we\nshow that Assumption 2 is a strictly weaker than assuming the environment is a linear MDP.\n1The description of the algorithm looks quite different from that of Azar et al. (2017), but via an equivalence between model-free\nmethods with experience replay and model-based methods (Fujimoto et al., 2018), they are indeed quite similar.\n4\nProposition 1. If an MDP is linear then Assumption 2 holds with G = {(s, a) 7→⟨w, ψ(s, a)⟩: w ∈Bd} .\nProof. The result is implicit in Jin et al. (2019), and we include the proof for completeness. For any function\ng, observe that owing to the linear MDP property\nTh(g)(s, a) = E\n\u0014\nr + max\na′\ng(s′, a′) | s, a\n\u0015\n= ⟨ψ(s, a), η⟩+\nZ\n⟨ψ(s, a), µ(s′)⟩max\na′\ng(s′, a′)ds′,\nwhich is clearly a linear function in ψ(s, a). Hence for any function g, which trivially includes the optimistic\nfunctions, we have Th(g) ∈G.\nThus the linear MDP assumption is stronger than Assumption 2. Next, we show that it is strictly stronger.\nProposition 2. There exists an MDP with H = 2, d = 2, |A| = 2 and |S| = ∞such that Assumption 2 is\nsatisﬁed, but the MDP is not a linear MDP.\nThus we have that optimistic closure is strictly weaker than the linear MDP assumption from Jin et al.\n(2019). Thus, our results strictly generalize theirs.\nProof. In this proof we ﬁx the link function f(z) = z. We ﬁrst construct the MDP. We set the action space\nA = {a1, a2}. We use ei to denote the ith standard basis element, and let x = (0.1/Γ, 0.1/Γ) be a ﬁxed\nvector where Γ appears in the construction Gup. Recall that s1 is the ﬁrst state in each trajectory. In our\nexample, for all a ∈A, φ(s1, a) is sampled uniformly at random from the set {αe1 + (1 −α)e2 : α ∈[0, 1]}.\nThe transition rule is deterministic:\nφ(s2, a1) = φ(s2, a2) = αx if φ(s1, a) = αe1 + (1 −α)e2.\nMoreover, for the reward function, R(s1, a) = 0 and R(s2, a) = 0.1α/Γ.\nWe ﬁrst show that the Linear MDP property does not hold for the constructed MDP and the given feature\nmap φ. Let s(1)\n1\nbe the state with φ(s(1)\n1 , a) = e2 and s(2)\n1\nbe the state with φ(s(2)\n1 , a) = e1. Notice that we\ndeterministically transition from s(1)\n1\nto a state s(1)\n2\nwith φ(s(1)\n2 , a) = 0, and we deterministically transition\nfrom s(2)\n1\nto a state s(2)\n2\nwith φ(s(2)\n2 , a) = x, which already ﬁxes the whole transition operator under the\nlinear MDP assumption. Thus, under the linear MDP assumption, we must therefore have a randomized\ntransition for any state s1 with φ(s1, a) = αe1 + (1 −α)e2 where α ∈(0, 1). This contradicts the fact that\nour constructed MDP has deterministic transitions everywhere, so the linear MDP cannot hold.\nWe next show that Assumption 2 holds. Consider an arbitrary optimistic Q estimate of the form\ng(z) = min{1, z⊤θ + γ\n√\nz⊤Az} ∈Gup. Notice that for x = (0.1/Γ, 0.1/Γ), we always have that x⊤θ +\nγ\n√\nx⊤Ax ≤1 for any θ ∈Bd and A with ∥A∥op ≤1. Moreover, for all s2, i.e., the second state in the\ntrajectory, we always have φ(s2, a) = αx for some α ∈[0, 1]. Hence we can ignore the ﬁrst term in the\nminimum, and, by direct calculation, we have that when φ(s, a) = αe1 + (1 −α)e2:\nT1(g)(s, a) = αx⊤θ + γ\n√\nα2x⊤Ax = α(x⊤θ + γ\n√\nx⊤Ax) = αc0.\nHence we can write T1(g) = ⟨φ(s, a), (c0, 0)⟩, which veriﬁes Assumption 2.\n4\nAlgorithm and main result\nWe now turn to our main results. We study a least-squares dynamic programming style algorithm that we call\nLSVI-UCB, with pseudocode presented in Algorithm 1. The algorithm is nearly identical to the algorithm\nproposed by Jin et al. (2019) with the same name. As such, it should be considered as a generalization.\n5\nAlgorithm 1 The LSVI-UCB algorithm with generalized linear function approximation.\n1: Initialize estimates ¯Qh,0 ≡1 for all h ≤H and ¯QH+1,t ≡0 for all 1 ≤t ≤T;\n2: Set γ = CKκ−1p\n1 + M + K + d2 ln((1 + K + Γ)TH) for a universal constant C;\n3: for t = 1, 2, · · · , T do\n4:\nCommit to policy ˆπh,t(s) ≜argmaxa∈A ¯Qh,t−1(s, a);\n5:\nUse policy ˆπ·,t to collect one trajectory {(sh,t, ah,t, rh,t)}H\nh=1;\n6:\nfor h = H, H −1, · · · , 1 do\n7:\nCompute xh,τ ≜φ(sh,τ, ah,τ) and yh,τ ≜rh,τ + maxa′∈A ¯Qh+1,t(sh+1,τ, a′) for all τ ≤t;\n8:\nCompute ridge estimate\nˆθh,t ≜argmin\n∥θ∥2≤1\nX\nτ≤t\n(yh,τ −f(⟨xh,τ, θ⟩))2;\n(1)\n9:\nCompute Λh,t ≜P\nτ≤t xh,τx⊤\nh,τ + I;\n10:\nConstruct ¯Qh,t(s, a) ≜min\nn\n1, f(φ(s, a)⊤ˆθh,t) + γ ∥φ(s, a)∥Λ−1\nh,t\no\n;\n11:\nend for\n12: end for\nThe algorithm uses dynamic programming to maintain optimistic Q function estimates { ¯Qh,t}h≤H,t≤T\nfor each time point h and each episode t. In the tth episode, we use the previously computed estimates to\ndeﬁne the greedy policy ˆπh,t(·) ≜argmaxa∈A ¯Qh,t−1(·, a), which we use to take actions for the episode.\nThen, with all of the trajectories collected so far, we perform a dynamic programming update, where the main\nper-step optimization problem is (1). Starting from time point H, we update our Q function estimates by\nsolving constrained least squares problems using our class of GLMs. At time point H, the covariates are\n{φ(sH,τ, aH,τ)}τ≤t, and the regression targets are simply the immediate rewards {rH,τ}τ≤t. For time points\nh < H, the covariates are deﬁned similarly as {φ(sh,τ, ah,τ)}τ≤t but the regression targets are deﬁned by\ninﬂating the learned Q function for time point h + 1 by an optimism bonus.\nIn detail the least squares problem for time point h + 1 yields a parameter ˆθh+1,t and we also form\nthe second moment matrix of the covariates Λh+1,t. Using these, we deﬁne the optimistic Q function\n¯Qh+1,t(s, a) ≜min\nn\n1, f(⟨φ(s, a), ˆθh+1,t⟩) + γ ∥φ(s, a)∥Λ−1\nh+1,t\no\n. In our analysis, we verify that ¯Qh+1,t is\noptimistic in the sense that it over-estimates Q⋆everywhere. Then, the regression targets for the least squares\nproblem at time point h are rh,τ + maxa′∈A ¯Qh+1,t(sh+1,τ, a′), which is a natural stochastic approximation\nto the Bellman backup of ¯Qh+1,t. Applying this update backward from time point H to 1, we obtain the\nQ-function estimates that can be used in the next episode.\nThe main conceptual difference between Algorithm 1 and the algorithm of Jin et al. (2019) is that we\nallow non-linear function approximation with GLMs, while they consider only linear models. On a more\ntechnical level, we use constrained least squares for our dynamic programming backup which we ﬁnd easier\nto analyze, while they use the ridge regularized version.\nOn the computational side, the algorithm is straightforward to implement, and, depending on the link\nfunction f, it can be easily shown to run in polynomial time. For example, when f is the identity map, (1)\nis standard least square ridge regression, and by using the Sherman-Morrison formula to amortize matrix\ninversions, we can see that the running time is O\n\u0000d2|A|HT 2\u0001\n. The dominant cost is evaluating the optimism\nbonus when computing the regression targets. In practice, we can use an epoch schedule or incremental\noptimization algorithms for updating ¯Q for an even faster algorithm. Of course, with modern machine\nlearning libraries, it is also straightforward to implement the algorithm with a non-trivial link function f,\neven though (1) may be non-convex.\n6\n4.1\nMain result\nOur main result is a regret bound for LSVI-UCB under Assumption 2.\nTheorem 1. For any episodic MDP, with Assumption 1 and Assumption 2, and for any T, the cumulative\nregret of Algorithm 1 is2\nReg(T) ≤O\n\u0010\nH\np\nT ln(TH) + HKκ−1p\n(M + K + d2 ln(KTH)) · Td ln(T/d)\n\u0011\n= ˜O\n\u0010\nH\n√\nd3T\n\u0011\n,\nwith probability 1 −1/(TH).\nThe result states that LSVI-UCB enjoys\n√\nT-regret for any episodic MDP problem and any GLM,\nprovided that the regularity conditions are satisﬁed and that optimistic closure holds. As we have mentioned,\nthese assumptions are relatively mild, encompassing the tabular setting and prior work on linear function\napproximation. Importantly, no explicit dynamics assumptions are required. Thus, Theorem 1 is one of the\nmost general results we are aware of for provably efﬁcient exploration with function approximation.\nNevertheless, to develop further intuition for our bound, it is worth comparing to prior results. First, in the\nlinear MDP setting of Jin et al. (2019), we use the identity link function so that K = κ = 1 and M = 1, and\nwe also are guaranteed to satisfy Assumption 2. In this case, our bound differs from that of Jin et al. (2019)\nonly in the dependence on H, which arises due to a difference in normalization. Our bound is essentially\nequivalent to theirs and can therefore be seen as a strict generalization.\nTo capture the tabular setting, we use the standard basis featurization as in Fact 2 and the identity link\nfunction, which gives d = |S||A|, K = κ = 1, and M = 1. Thus, we obtain the following corollary:\nCorollary 2. For MDPs with ﬁnite state and action spaces, using feature map φ(s, a) ≜es,a ∈R|S|×|A|, for\nany T, the cumulative regret of Algorithm 1 is ˜O\n\u0010\nH\np\n|S|3|A|3T\n\u0011\n, with probability 1 −1/(TH).\nNote that this bound is polynomially worse than the near-optimal ˜O(H\n√\nSAT + H2S2A log(T)) bound\nof Azar et al. (2017). However, Algorithm 1 is almost equivalent to their algorithm, and, indeed, a reﬁned\nanalysis specialized to the tabular setting can be shown to obtain a matching regret bound. Of course, our\nalgorithm and analysis address signiﬁcantly more complex settings than tabular MDPs, which we believe is\nmore important than recovering the optimal guarantee for tabular MDPs.\n5\nProof Sketch\nWe now provide a brief sketch of the proof of Theorem 1, deferring the technical details to the appendix. The\nproof has three main components: a regret decomposition for optimistic Q learning, a deviation analysis for\nleast squares with GLMs to ensure optimism, and a potential argument to obtain the ﬁnal regret bound.\nA regret decomposition.\nThe ﬁrst step of the proof is a regret decomposition that applies generically to\noptimistic algorithms.3 The lemma demonstrates concisely the value of optimism in reinforcement learning,\nand is the primary technical motivation for our interest in designing optimistic algorithms.\nWe state the lemma more generally, which requires some additional notation. Fix round t and let\n{ ¯Qh,t−1}h≤H denote the current estimated Q functions. The precondition is that ¯Qh,t−1 is optimistic and\nhas controlled overestimation. Precisely, there exists a function confh,t−1 : S × A →R+ such that\n∀s, a, h : Q⋆\nh(s, a) ≤¯Qh,t−1(s, a) ≤Th( ¯Qh+1,t−1)(s, a) + confh,t−1(s, a).\n(2)\nWe now state the lemma and an immediate corollary.\n2We use ˜O (·) to suppress factors of M, K, κ, Γ and any logarithmic dependencies on the arguments.\n3Related results appear elsewhere in the literature focusing on the tabular setting, see e.g., Simchowitz and Jamieson (2019).\n7\nLemma 1. Fix episode t and let Ft−1 be the ﬁltration of {(sh,τ, ah,τ, rh,τ)}τ<t. Assume that ¯Qh,t−1\nsatisﬁes (2) for some function confh,t−1. Then, if πt = argmaxa∈A ¯Qh,t−1(·, a) is deployed we have\nV ⋆−E\n\" H\nX\nh=1\nrh,t | Ft−1\n#\n≤ζt +\nH\nX\nh=1\nconfh,t−1(sh,t, ah,t),\nwhere E [ζt | Ft−1] = 0 and |ζt| ≤2H almost surely.\nCorollary 3. Assume that for all t, ¯Qh,t−1 satisﬁes (2) and that πt is the greedy policy with respect to ¯Qh,t−1.\nThen with probability at least 1 −δ, we have\nReg(T) ≤\nT\nX\nt=1\nH\nX\nh=1\nconfh,t−1(sh,t, ah,t) + O(H\np\nT log(1/δ)).\nThe lemma states that if ¯Qh,t−1 is optimistic and we deploy the greedy policy πt, then the per-episode\nregret is controlled by the overestimation error of ¯Qh,t−1, up to a stochastic term that enjoys favorable\nconcentration properties. Crucially, the errors are accumulated on the observed trajectory, or, stated another\nway, the confh,t−1 is evaluated on the states and actions visited during the episode. As these states and\nactions will be used to update ¯Q, we can expect that the conf function will decrease on these arguments.\nThis can yield one of two outcomes: either we will incur lower regret in the next episode, or we will explore\nthe environment by visiting new states and actions. In this sense, the lemma demonstrates how optimism\nnavigates the exploration-exploitation tradeoff in the multi-step RL setting, analogously to the bandit setting.\nNote that these results do not assume any form for ¯Qh,t−1 and do not require Assumption 2. In particular,\nthey are not specialized to GLMs. In our proof, we use the GLM representation and Assumption 2 to ensure\nthat (2) holds and to bound the conﬁdence sum in Corollary 3. We believe these technical results will be\nuseful in designing RL algorithms for general function classes, which is a natural direction for future work.\nDeviation analysis.\nThe next step of the proof is to design the conf function and ensure that (2) holds, with\nhigh probability. This is the contents of the next lemma.\nLemma 2. Under Assumption 1 and Assumption 2, with probability 1 −1/(TH), we have that\n∀t, h, s, a :\n\f\f\ff(⟨φ(s, a), ˆθh,t⟩) −Th( ¯Qh+1,t)(s, a)\n\f\f\f ≤min\nn\n2, γ ∥φ(s, a)∥Λ−1\nh,t\no\n,\nwhere γ, Λh,t are deﬁned in Algorithm 1.\nA simple induction argument then veriﬁes that (2) holds, which we summarize in the next corollary.\nCorollary 4. Under Assumption 1 and Assumption 2, with probability 1 −1/(TH), we have that (2) holds\nfor all t, h with confh,t−1(s, a) = γ ∥φ(s, a)∥Λ−1\nh,t−1.\nThe proof of the lemma requires an intricate deviation analysis to account for the dependency structure in\nthe data sequence. The intuition is that, thanks to Assumption 2 and the fact that ¯Qh+1,t ∈Gup, we know that\nthere exists a parameter ¯θh,t such that f(⟨φ(s, a), ¯θh,t⟩) = Th( ¯Qh+1,t)(s, a). It is easy to verify that ¯θh,t is\nthe Bayes optimal predictor for the square loss problem in (1), and so with a uniform convergence argument\nwe can expect that ˆθh,t is close to ¯θh,t, which is our desired conclusion.\nThere are two subtleties with this argument. First, we want to show that ¯θh,t and ˆθh,t are close in a\ndata-dependent sense, to obtain the dependence on the Λ−1\nh,t-Mahalanobis norm in the bound. This can be\ndone using vector-valued self-normalized martingale inequalities (Peña et al., 2008), as in prior work on\nlinear stochastic bandits (Abbasi-Yadkori et al., 2012; Filippi et al., 2010; Abbasi-Yadkori et al., 2011).\n8\nHowever, the process we are considering is not a martingale, since ¯Qh+1,t, which determines the\nregression targets yh,τ, depends on all data collected so far. Hence yh,τ is not measurable with respect to the\nﬁltration Fτ, which prevents us from directly applying a self-normalized martingale concentration inequality.\nTo circumvent this issue, we use a uniform convergence argument and introduce a deterministic covering of\nGup. Each element of the cover induces a different sequence of regression targets yh,τ, but as the covering is\ndeterministic, we do obtain martingale structure. Then, we show that the error term for the random ¯Qh+1,t\nthat we need to bound is close to a corresponding term for one of the covering elements, and we ﬁnish the\nproof with a uniform convergence argument over all covering elements.\nThe corollary is then obtained by a straightforward inductive argument. Assuming ¯Qh+1,t dominates Q⋆,\nit is easy to show that ¯Qh,t also dominates Q⋆, and the upper bound is immediate. Combining Corollary 4\nwith Corollary 3, all that remains is to upper bound the conﬁdence sum.\nThe potential argument.\nTo bound the conﬁdence sum, we use a relatively standard potential argument\nthat appears in a number of works on stochastic (generalized) linear bandits. We summarize the conclusion\nwith the following lemma, which follows directly from Lemma 11 of Abbasi-Yadkori et al. (2012).\nLemma 3. For any h ≤H we have that PT\nt=1 ∥φ(sh,t, ah,t)∥2\nΛ−1\nh,t−1 ≤2d ln(1 + T/d).\nWrapping up.\nTo prove Theorem 1 we ﬁrst note that via Lemma 3 and an application of the Cauchy-\nSchwarz inequality, we have that for each h ≤H\nT\nX\nt=1\nconfh,t−1(sh,t, ah,t) ≤γ\n√\nT\nv\nu\nu\nt\nT\nX\nt=1\n∥φ(sh,t, ah,t)∥2\nΛ−1\nh,t−1 ≤O\n\u0010\nγ\np\nTd ln(T/d)\n\u0011\nInvoking Corollary 4, Corollary 3, and the deﬁnition of γ yields the ˜O\n\u0010\nH\n√\nd3T\n\u0011\nregret bound.\n6\nDiscussion\nThis paper presents a provably efﬁcient reinforcement learning algorithm that approximates the Q⋆function\nwith a generalized linear model. We prove that the algorithm obtains ˜O(H\n√\nd3T) regret under mild regularity\nconditions and a new expressivity condition that we call optimistic closure. These assumptions generalize\nboth the tabular setting, which is classical, and the linear MDP setting studied in recent work. Further\nthey represent the ﬁrst statistically and computationally efﬁcient algorithms for reinforcement learning with\ngeneralized linear function approximation, without explicit dynamics assumptions.\nWe close with some open problems. First, using the fact that Corollary 3 applies beyond GLMs, can\nwe develop algorithms that can employ general function classes? While such algorithms do exist for the\ncontextual bandit setting (Foster et al., 2018), it seems quite difﬁcult to generalize this analysis to multi-step\nreinforcement learning. More importantly, while optimistic closure is weaker than some prior assumptions\n(and incomparable to others), it is still quite strong, and stronger than what is required for the batch RL setting.\nAn important direction is to investigate weaker assumptions that enable provably efﬁcient reinforcement\nlearning with function approximation. We look forward to studying these questions in future work.\nAcknowledgements\nWe thank Wen Sun for helpful conversations during the development of this paper. Simon S. Du is supported\nby National Science Foundation (Grant No. DMS-1638352) and the Infosys Membership.\n9\nA\nProofs of basic results\nProof of Fact 1. We will solve for Q⋆via dynamic programming, starting from time point H. In this case, the\nBellman update operator is degenerate, and we start by observing that TH(g) ≡Q⋆\nH for all g. Consequently\nwe have Q⋆\nH ∈G. Next, inductively we assume that we have Q⋆\nh+1 ∈G, which implies that Q⋆\nh+1 ∈Gup as\nwe may take the same parameter θ and set A ≡0. Then, by the standard Bellman ﬁxed-point characterization,\nwe know that Q⋆\nh = Th(Q⋆\nh+1), at which point Assumption 2 yields that Q⋆\nh ∈G.\nProof of Fact 2. We simply verify that G contains all mappings from (s, a) 7→[0, 1], at which point the result\nis immediate. To see why, observe that via Assumption 1 we know that f is invertible (it is monotonic with\nderivative bounded from above and below). Then, note that any function (s, a) 7→[0, 1] can be written as a\nvector v ∈[0, 1]|S|×|A|. For such a vector v, if we deﬁne θs,a ≜f−1(vs,a) we have that f(⟨es,a, θ⟩) = vs,a.\nHence G contains all functions, so we trivially have optimistic closure.\nB\nProof of Theorem 1\nTo facilitate our regret analysis we deﬁne the following important intermediate quantity:\n¯θh,t ∈Bd : f(⟨φ(s, a), ¯θh,t⟩) ≜E\n\u0014\nrh + max\na′∈A\n¯Qh+1,t(s′, a′) | s, a\n\u0015\n.\nIn words, ¯θh,t is the Bayes optimal predictor for the squared loss problem at time point h in the tth episode.\nSince by inspection ¯Qh+1,t ∈Gup, by Assumption 2 we know that ¯θh,t exists for all h and t.\nLemma 4. For any θ, θ′, x ∈Rd satisfying ∥θ∥2, ∥θ′∥2, ∥x∥2 ≤1,\nκ2 \f\f⟨x, θ′ −θ⟩\n\f\f2 ≤\n\f\ff(\n\nx, θ′\u000b\n) −f(⟨x, θ⟩)\n\f\f2 ≤K2 \r\rθ′ −θ\n\r\r2\n2 .\nProof. By the mean-value theorem, there exists ˜θ = θ + λ(θ′ −θ) for some λ ∈(0, 1) such that f(⟨x, θ′⟩) −\nf(⟨x, θ⟩) =\nD\n∇θf(⟨x, ˜θ⟩), θ′ −θ\nE\n. On the other hand, by the chain rule and Assumption 1, ∇θf(⟨x, ˜θ⟩) =\nf′(⟨x, ˜θ⟩) · x. Hence,\n|⟨∇θf(x⊤˜θ), θ′ −θ⟩|2 ≤f′(⟨x, ˜θ⟩)2 ·\n\f\f\nx, θ′ −θ\n\u000b\f\f2 ≤K2 ∥x∥2\n2\n\r\rθ′ −θ\n\r\r2\n2 ≤K2 \r\rθ′ −θ\n\r\r2\n2 ;\n|⟨∇θf(x⊤˜θ), θ′ −θ⟩|2 ≥κ2 \f\f\nx, θ′ −θ\n\u000b\f\f2 ,\nwhich are to be demonstrated.\nLemma 5. For any 0 < ε ≤1, there exists a ﬁnite subset Vε ⊂Gup with ln |Vε| ≤6d2 ln(2(1 + K + Γ)/ε),\nsuch that\nsup\ng∈Gup\nmin\nv∈Vε sup\ns,a |g(φ(s, a)) −v(φ(s, a))| ≤ε.\n(3)\nProof. Recall that for every g ∈Gup, there exists θ ∈Bd, 0 ≤γ ≤Γ and ∥A∥op ≤1 such that g(x) =\nmin{1, f(⟨x, θ⟩) + γ ∥x∥A}. Let Θε ⊆Bd, Γε ⊆[0, Γ] and Mε ⊆{M ∈S+\nd : ∥M∥op ≤1} be ﬁnite\nsubsets such that for any θ, γ, A, there exist θ′ ∈Θε, γ′ ∈Γε, A′ ∈Mε such that\nmax\nn\r\rθ −θ′\r\r\n2 ,\n\f\fγ −γ′\f\f ,\n\r\rA −A′\r\r\nop\no\n≤ε′,\n10\nwhere ε′ ∈(0, 1) will be speciﬁed later in the proof. For the function g ∈Gup corresponding to the parameters\nθ, γ, A the function g′ corresponding to parameters θ′, γ′, A′ satisﬁes\nsup\ns,a\n\f\fg(φ(s, a)) −g′(φ(s, a))\n\f\f ≤sup\nx∈Bd\n\f\fg(x) −g′(x)\n\f\f\n≤sup\nx∈Bd\n\f\ff(⟨x, θ⟩) −f(\n\nx, θ′\u000b\n) + γ ∥x∥A −γ′ ∥x∥A′\n\f\f\n≤K\n\r\rθ −θ′\r\r\n2 +\n\f\fγ −γ′\f\f + Γ |∥x∥A −∥x∥A′|\n≤K\n\r\rθ −θ′\r\r\n2 +\n\f\fγ −γ′\f\f + Γ\nq\n|x⊤(A −A′)x|\n≤Kε′ + ε′ + Γ\n√\nε′ ≤(1 + K + Γ)\n√\nϵ′.\nIn the last step we use ε′ ≤1. Therefore, if we deﬁne the class Vε ≜{(s, a) 7→min{1, f(⟨φ(s, a), θ′⟩) +\nγ′ ∥φ(s, a)∥A′ : θ′ ∈Θε, γ ∈Γε, A ∈Mε}, we know that the covering property is satisﬁed with parameter\n(1 + K + Γ)\n√\nε′. Setting ε′ = ε2/(1 + K + Γ)2 we have the desired covering property.\nFinally, we upper bound ln |Vε|. By deﬁnition, we have that ln |Vε| ≤ln |Θε| + ln |Γε| + ln |Mε|.\nFurthermore, standard covering number bounds reveals that ln |Θε| ≤d ln(2/ε′), ln |Γε| ≤ln(1/ε′) and\nln |Mε| ≤d2 ln(2/ε′). Plugging in the deﬁnition of ε′ yields the result.\nFor the next lemma, let Ft−1 ≜σ({(sh,τ, ah,τ, rh,τ)}τ<t) be the ﬁltration induced by all observed\ntrajectories up to but not including time t. Observe that ¯Q·,t−1 and our policy ˆπh,t are Ft−1 measurable.\nLemma 6. Fix any 1 ≤t ≤T and 1 ≤h ≤H. Then as long as πt is Ft−1 measurable, with probability\n1 −1/(TH)2 it holds that\n\f\f\ff(⟨φ(s, a), ˆθh,t⟩) −f(⟨φ(s, a), ¯θh,t⟩)\n\f\f\f ≤min\nn\n2, γ ∥φ(s, a)∥Λ−1\nh,t\no\n,\n∀s, a.\nfor γ ≥CKκ−1p\n1 + M + K + d2 ln((1 + K + Γ)TH) and 0 < C < ∞is a universal constant.\nNote that this is precisely Lemma 2, as ¯θh,t is deﬁned as f(⟨φ(s, a), ¯θh,t) = Th( ¯Qh+1,t)(s, a).\nProof. The upper bound of 2 is obvious, since both terms are upper bounded by 1 in absolute value. Therefore\nwe focus on the second term in the minimum. To simplify notation we omit the dependence on h in the\nsubscripts and write xτ, yτ for xh,τ and yh,τ. We also abbreviate ˆθ ≜ˆθh,t and ¯θ ≜¯θh,t.\nSince\n\r\r¯θ\n\r\r\n2 ≤1, the optimality of ˆθ for (1) implies that\nX\nτ≤t\n\u0010\nf(⟨xτ, ˆθ⟩) −yτ\n\u00112\n≤\nX\nτ≤t\n\u0000f(⟨xτ, ¯θ⟩) −yτ\n\u00012 .\nDecomposing the squares and re-organizing the terms, we have that\nX\nτ≤t\n\u0010\nf(⟨xτ, ˆθ⟩) −f(⟨xτ, ¯θ⟩)\n\u00112\n≤2\n\f\f\f\f\f\f\nX\nτ≤t\nξτ(f(⟨xτ, ˆθ⟩) −f(⟨xτ, ¯θ⟩))\n\f\f\f\f\f\f\n,\n(4)\nwhere ξτ ≜yτ −f(⟨xτ, ¯θ⟩). By the fundamental theorem of calculus, we have\nf(⟨xτ, ˆθ⟩) −f(⟨xτ, ¯θ⟩) =\nZ ⟨xτ,ˆθ⟩\n⟨xτ,¯θ⟩\nf(s)ds = ⟨xτ, ˆθ −¯θ⟩\nZ 1\n0\nf′(⟨xτ, sˆθ −(1 −s)¯θ⟩)ds\n|\n{z\n}\n≜Dτ\n.\n11\nUsing this identity on both sides of (4), we have that\nX\nτ≤t\nD2\nτ\n\u0010\n⟨xτ, ˆθ −¯θ⟩\n\u00112\n≤2\n\f\f\f\f\f\f\nX\nτ≤t\nξτDτ⟨xτ, ˆθ −¯θ⟩\n\f\f\f\f\f\f\n.\n(5)\nNote also that, by Assumption 1, Dτ satisﬁes κ2 ≤D2\nτ ≤K2 almost surely for all τ.\nThe difﬁculty in controlling (5) is that ¯θ itself is a random variable that depends on {(xτ, yτ)}τ≤t. In\nparticular, we want that E[ξτ | Dτ ⟨xτ, φ⟩, Fτ−1] = 0 for any ﬁxed φ, but this is not immediate as ¯θ depends\non xτ. To proceed, we eliminate this dependence with a uniform convergence argument. Let ε ∈(0, 1) be a\ncovering accuracy parameter to be determined later in this proof. Let Vε be the pointwise covering for Gup\nthat is implied by Lemma 5. Let gε ∈Vε be the approximation for ¯Qh+1,t that satisﬁes (3). By Assumption 2,\nthere exists some θ♯∈Bd such that\n∀s, a : f(⟨φ(s, a), θ♯⟩) = E\n\u0014\nr + max\na′∈A gε(s′, a′) | s, a\n\u0015\n.\nNow, deﬁne y♯\nτ and ξ♯\nτ as\ny♯\nτ ≜rh,τ + max\na′∈A gε(sh+1,τ, a′),\nξ♯\nτ ≜y♯\nτ −f(⟨xh,τ, θ♯⟩).\nThe right-hand side of (5) can then be upper bounded as\n2\n\f\f\f\f\f\f\nX\nτ≤t\nξτDτ⟨xτ, ˆθ −¯θ⟩\n\f\f\f\f\f\f\n≤2\n\f\f\f\f\f\f\nX\nτ≤t\nξ♯\nτDτ⟨xτ, ˆθ −¯θ⟩\n\f\f\f\f\f\f\n+ ∆,\n(6)\nwhere |∆| ≤Kt × maxτ≤t |ξ♯\nτ −ξτ| almost surely.\nUpper bounding ∆in (6).\nFix τ ≤t. By deﬁnition, we have that\n\f\f\fξ♯\nτ −ξτ\n\f\f\f ≤\n\f\f\fy♯\nτ −yτ\n\f\f\f +\n\f\f\ff(⟨xτ, ¯θ⟩) −f(⟨xτ, θ♯⟩)\n\f\f\f\n≤max\na∈A\n\f\fgε(sh+1,τ, a) −¯Qh+1,t(sh+1,τ, a)\n\f\f + K\n\r\r\r¯θ −θ♯\r\r\r\n2\n(7)\n≤ϵ + Kϵ ≤(K + 1)ϵ,\n(8)\nwhere (7) holds by Lemma 4 and (8) follows from Lemma 5. In particular, the bound on\n\r\r¯θ −θ♯\r\r\n2 can be\nveriﬁed by expanding the deﬁnitions and noting that gε is pointwise close to ¯Qh+1,t. Therefore, we have\n|∆| ≤(K + 1)2tϵ.\n(9)\nUpper bounding (6).\nNote that Dτ is a function of xτ, ˆθ, and ¯θ. For clarity, we deﬁne Dτ(θ, θ′) :=\nR 1\n0 f′(⟨xτ, sθ + (1 −s)θ′)⟩)ds. As |f′′(z)| ≤M for all |z| ≤1 and ∥xτ∥2 ≤1, we have that for every\nθ, θ′, ˜θ, ˜θ′ ∈Bd\n\f\f\fDτ(θ, θ′) −Dτ(˜θ, ˜θ′)\n\f\f\f ≤\nZ 1\n0\n\f\f\ff′(⟨xτ, sθ + (1 −s)θ′⟩) −f′(⟨xτ, s˜θ + (1 −s)˜θ′⟩)\n\f\f\f ds\n≤M(∥θ −˜θ∥2 + ∥θ′ −˜θ′∥2).\n12\nHence, for any (θ, θ′) and (˜θ, ˜θ′) pairs, we have for every τ that\n\f\f\fξ♯\nτ\nD\nxτ, Dτ(θ, θ′)(θ −θ′) −Dτ(˜θ, ˜θ′)(˜θ −˜θ′)\nE\f\f\f\n≤\n\f\fDτ(θ, θ′) −Dτ(˜θ, ˜θ′)\n\f\f × ∥θ −θ′∥2 +\n\f\fDτ(˜θ, ˜θ′)\n\f\f × (∥θ −˜θ∥2 + ∥θ′ −˜θ′∥2)\n≤M(∥θ −˜θ∥2 + ∥θ′ −˜θ′∥2) × 2 + K(∥θ −˜θ∥2 + ∥θ′ −˜θ′∥2)\n≤(2M + K)(∥θ −˜θ∥2 + ∥θ′ −˜θ′∥2).\nHere we are using that |ξτ| ≤1.\nWe are now in a position to invoke Lemma 9. Consider a ﬁxed function gε, which deﬁnes a ﬁxed θ♯.\nWe will bound\n\f\f\fP\nτ≤t ξ♯\nτ⟨xτ, Dτ(θ, θ′)(θ −θ′)⟩\n\f\f\f uniformly over all pairs (θ, θ′). With gε, θ♯ﬁxed and since\nπt is Ft−1 measurable, we have that {xτ, ξ♯\nτ}τ≤t are random variables satisfying E[ξ♯\nτ | x1:τ, ξ♯\n1:τ−1] = 0.\nFor φ = (θ, θ′) we deﬁne the function q(xτ, φ) = ⟨x, Dτ(φ)(θ −θ′)⟩, which as we have just calculated\nsatisﬁes |q(xτ, φ) −q(xτ, φ′)| ≤(2M + K) ∥φ −φ′∥2. For δ′ ∈(0, 1/2) with probability 1 −δ′ we have\n∀φ = (θ, θ′) ∈B2\nd:\n\f\f\f\f\f\f\nX\nτ≤t\nξ♯\nτ⟨xτ, Dτ(φ)(θ −θ′)⟩\n\f\f\f\f\f\f\n≤(2M + K) + 2\n\u0010\n1 +\np\nV (φ)\n\u0011 p\n2d ln(4T) + ln(1/δ′)\n≤4 max\nn\nM + K +\np\n2d ln(4T) + ln(1/δ′),\np\nV (φ)\np\n2d ln(4T) + ln(1/δ′)\no\n,\n(10)\nwhere V (φ) ≜P\nτ≤t⟨xτ, Dτ(φ)(θ −θ′)⟩2. The last inequality holds because a + b ≤2 max{a, b}.\nNext, take a union bound over all gε ∈Vε so (10) holds for any gε and any subsequently induced choice\nof ξ♯\nτ with probability at least 1 −|Vε|δ′. In particular, this union bound implies that (10) holds for the choice\nof gε that approximates ¯Qh+1,t. Therefore, combining (5), (6), (9) with (10) for this choice of gε, we have\nthat with probability at least 1 −|Vε|δ′\nX\nτ≤t\nD2\nτ⟨xτ, ˆθ −¯θ⟩2 ≤2∆+ 2\n\f\f\f\f\f\f\nX\nτ≤t\nξ♯\nτ⟨xτ, Dτ(ˆθ −¯θ)⟩\n\f\f\f\f\f\f\n≤2(K + 1)2tε + 8 max\n\u001a\nM + K +\np\n2d ln(4T) + ln(|Vε|/δ′),\nq\nV (ˆθ, ¯θ) ·\np\n2d ln(4T) + ln(|Vε|/δ′)\n\u001b\n.\nObserve that the left hand side is precisely V (ˆθ, ¯θ). Now, set ε = 1/(2(K + 1)2T) and δ′ = 1/(|Vε|T 2H2)\nand use the bound on ln |Vε| from Lemma 5 to get\np\n2d ln(4T) + ln(|Vε|/δ′) ≤\np\n2d ln(4T) + 12d2 ln(2(1 + K + Γ)/ε) + 2 ln(TH)\n≤\np\n4d ln(2TH) + 24d2 ln(2(1 + K + Γ)T) ≤\np\n28d2 ln(2(1 + K + Γ)TH)\nTherefore, we obtain\nV (ˆθ, ¯θ) ≤1 + 8 max\n\u001a\nM + K +\np\n28d2 ln(2(1 + K + Γ)TH),\nq\nV (ˆθ, ¯θ) ·\np\n28d2 ln(2(1 + K + Γ)TH)\n\u001b\n≤16 max\n\u001a\n1 + M + K +\np\n28d2 ln(2(1 + K + Γ)TH),\nq\nV (ˆθ, ¯θ) ·\np\n28d2 ln(2(1 + K + Γ)TH)\n\u001b\n.\n13\nSubsequently,\nV (ˆθ, ¯θ) =\nX\nτ≤t\nD2\nτ⟨xτ, ˆθ −¯θ⟩2\n≤16 max\nn\n1 + M + K +\np\n28d2 ln(2(1 + K + Γ)TH), 448d2 ln(2(1 + K + Γ)TH)\no\n≤C2\nV (1 + M + K + d2 ln((1 + K + Γ)TH)),\nwhere 0 < CV < ∞is a universal constant.\nNext, note that D2\nτ ≥κ2, thanks to Assumption 1. We then have\nq\n(ˆθ −¯θ)⊤Λh,t(ˆθ −¯θ) ≤κ−1\nq\nV (ˆθ, ¯θ) ≤CV κ−1p\n1 + M + K + d2 ln((1 + K + Γ)TH),\nwhere Λh,t = P\nτ<t xτ, x⊤\nτ . Finally, for any (s, a) pair, invoking Lemma 4 and the Cauchy-Schwarz\ninequality we have\n\f\f\ff(⟨φ(s, a), ˆθ⟩) −f(⟨φ(s, a), ¯θ⟩)\n\f\f\f ≤K\n\f\f\f⟨φ(s, a), ˆθ −¯θ⟩\n\f\f\f\n≤K\nq\n(ˆθ −¯θ)⊤Λh,t(ˆθ −¯θ) ×\nq\nφ(s, a)⊤Λ−1\nh,tφ(s, a)\n≤CV Kκ−1p\n1 + M + K + d2 ln((1 + K + Γ)TH) × ∥φ(s, a)∥Λ−1\nh,t\nwhich is to be demonstrated.\nCorollary 5. With probability 1 −1/(TH), ¯Qh,t(s, a) ≥Q⋆\nh(s, a) holds for all h, t, s, a.\nProof. Fix 1 ≤t ≤T. We use induction on h to prove this corollary. For h = H + 1, ¯QH+1,t(·, ·) ≥\nQ⋆\nH+1(·, ·) clearly holds because ¯QH+1,t ≡Q⋆\nH+1 ≡0. Now assume that ¯Qh+1,t ≥Q⋆\nh+1, and let us prove\nthat this is also true for time step h.\nSince ¯Qh+1,t(s′, a′) ≥Q⋆\nh+1(s′, a′) for all s′, a′, we have that f(⟨φ(s, a), ¯θh,t⟩) ≥f(⟨φ(s, a), θ⋆\nh⟩) for\nall (s, a) pairs. Then, by the deﬁnition of ¯Qh,t and Lemma 6, with probability 1 −1/(TH)2 it holds\nuniformly for all (s, a) pairs that ¯Qh,t(s, a) ≥f(⟨φ(s, a), ¯θh,t⟩). Hence, with the same probability, we have\n¯Qh,t(s, a) ≥Q⋆\nh(s, a) for all (s, a) pairs. A union bound over all t ≤T and h ≤H completes the proof.\nLemma 7 (Restatement of Lemma 1). Fix t ≤T and let Ft−1 be the ﬁltration of {(sh,τ, ah,τ, rh,τ)}τ<t.\nAssume that ¯Qh,t−1 satisﬁes\n∀s, a, h : Q⋆\nh(s, a) ≤¯Qh,t−1(s, a) ≤Th( ¯Qh+1,t−1)(s, a) + confh,t−1(s, a),\nwhere confh,t−1 is some Ft−1-measurable function. Then we have the difference between expected total\nV ⋆−E\n\" H\nX\nh=1\nrh,t | Ft−1\n#\n≤ζt +\nH\nX\nh=1\nconfh,t−1(sh,t, ah,t)\nwhere E[ζt|Ft−1] = 0 and |ζt| ≤2H almost surely.\nProof. Observe that\nV ⋆= E [Q⋆(s1, π⋆(s1))] ≤E\n\u0002 ¯Q1,t−1(s1, π⋆(s1))\n\u0003\n≤E\n\u0002 ¯Q1,t−1(s1, πt(s1))\n\u0003\n≤E [conf1,t−1(s1, πt(s1))] + E\n\u0002\nT1( ¯Q2,t−1)(s1, πt(s1))\n\u0003\n= E [conf1,t−1(s1, πt(s1))] + E [r1 | s1, a1 = πt(s1)] + Es2∼πt\n\u0002 ¯Q2,t−1(s2, πt(s2))\n\u0003\n14\nThroughout this calculation, s1 ∼µ. The ﬁrst step here is by deﬁnition, the second uses the optimism\nproperty for ¯Q1,t−1. The third uses that πt is the greedy policy with respect to ¯Q1,t−1 while the fourth uses\nthe upper bound on ¯Q1,t−1. Finally we use the deﬁnition of the Bellman operator and the fact that πt is the\ngreedy policy yet again. Comparing this upper bound with the expected reward collected by πt we observe\nthat r1 cancels, and we get\nV ⋆−E\n\" H\nX\nh=1\nrh,t | Ft−1\n#\n≤Eπt [conf1,t−1(s1, πt(s1))] + Eπt\n\"\n¯Q2,t−1(s2, πt(s2)) −\nH\nX\nh=2\nrh,t | Ft−1\n#\n.\nAt this point, notice that ¯Q2,t−1(s2, πt(s2)) is precisely what we alreacy upper bounded at time point h = 1\nand we are always considering the state-action distribution induced by πt. Hence, repeating the argument for\nall h, we obtain\nV ⋆−E\n\" H\nX\nh=1\nrh,t | Ft−1\n#\n≤\nH\nX\nh=1\nEπt [confh,t−1(sh, πt(sh))] =\nH\nX\nh=1\nconfh,t−1(sh,t, ah,t) + ζt,\nwhere\nζt ≜\nH\nX\nh=1\nEπt [confh,t−1(sh, πt(sh))] −confh,t−1(sh,t, ah,t),\nwhich is easily seen to have the required properties.\nCorollary 6. For any h ≤H, P\nt≤T ∥φ(sh,t, ah,t)∥2\nΛ−1\nh,t−1 ≤2d ln (1 + T/d).\nProof. The result follows directly from Lemma 11 of Abbasi-Yadkori et al. (2012), using the fact that Λ0 = I\nand φ(s, a) ∈Bd so that ∥φ(sh,t, ah,t)∥Λ−1\nh,t−1 ≤1 always.\nTheorem 7. The cumulative regret of Algorithm 1 is upper bounded by\neO\n\u0010\nH\n√\nT + H2√\nd3T\n\u0011\n,\nwith probability at least 1 −1/(TH).\nProof. Assume that Corollary 5 holds for all 1 ≤h ≤H and 1 ≤t ≤T. Applying Lemma 7 and the\ndeﬁnition of confh,t−1 implied by Corollary 5, the cumulative expected regret is at most\nTV ⋆−E\n\" T\nX\nt=1\nH\nX\nh=1\nrh,t\n#\n≤\nT\nX\nt=1\nζt +\nT\nX\nt=1\nH\nX\nh=1\nmin\nn\n2, γ ∥φ(sh,t, ah,t)∥Λ−1\nh,t−1\no\n≤\nT\nX\nt=1\nζt +\nH\nX\nh=1\np\nTγ2 ·\nv\nu\nu\nt\nT\nX\nt=1\n∥φ(sh,t, ah,t)∥2\nΛ−1\nh,t−1\n≤\nT\nX\nt=1\nζt +\nH\nX\nh=1\np\nTγ2 ·\np\n2d ln(1 + T/d).\nHere the last step is an application of Corollary 6. The ﬁrst term forms a martingale, and we know that\n|ζt| ≤2H. Therefore, by Azuma’s inequality, we have that with probability at least 1 −1/TH\nT\nX\nt=1\nζt ≤\np\n8TH2 ln(TH).\n15\nFinally, using the deﬁnition of γ, the ﬁnal regret is upper bounded by\nRegret(T) ≤O\n\u0010\nH\np\nT ln(TH) + HKκ−1p\n(M + K + d2 ln((K + Γ)TH)) · Td ln(1 + T/d)\n\u0011\n≤eO\n\u0010\nH\n√\nd3T\n\u0011\n.\nC\nTail inequalities\nLemma 8 (Azuma’s inequality). Suppose X0, X1, X2, · · · , XN form a martingale (i.e., E[Xk+1|X1, · · · , Xk] =\nXk) and satisfy |Xk −Xk−1| ≤ck almost surely. Then for any ϵ > 0,\nPr\n\u0002\f\fXn −X0\n\f\f ≥ϵ\n\u0003\n≤2 exp\n(\n−\nϵ2\n2 PN\nk=1 c2\nk\n)\n.\nLemma 9. Fix t, D ∈N. Let {ξτ, uτ}τ≤t be random variables such that E[ξτ|u1, ξ1, · · · , uτ−1, ξτ−1, uτ] =\n0 and |ξτ| ≤1 almost surely. Let q : (u, φ) 7→R be an arbitrary deterministic function satisfying\n|q(u, φ) −q(u, φ′)| ≤C∥φ −φ′∥2 for all u, φ and φ′, where φ, φ′ ∈RD. Then for any δ ∈(0, 1) and R > 0,\nPr\n\"\n∀φ ∈BD(R) :\n\f\f\f\f\f\nt\nX\nτ=1\nξτq(uτ, φ)\n\f\f\f\f\f ≤C + 2\n\u0012\n1 +\nq\nVq(φ)\n\u0013 p\nD ln(2tR) + ln(1/δ)\n#\n≥1 −δ,\nwhere BD(R) ≜{x ∈RD : ∥x∥2 ≤R} and Vq(φ) ≜P\nτ≤t q2(uτ, φ).\nProof. Let ϵ > 0 be a small precision parameter to be speciﬁed later. Let H ⊆BD(R) be a ﬁnite ϵ-covering\nof BD(R) such that supx∈BD(R) minz∈H ∥x −z∥2 ≤ϵ. Using standard covering number arguments, such a\ncovering exists with ln |H| ≤D ln(2R/ϵ).\nFor any φ ∈BD(R) let φ′ ≜argminz∈H ∥φ −z∥2. By deﬁnition, ∥φ −φ′∥2 ≤ϵ. This implies\n\f\fPt\nτ=1 ξτ[q(uτ, φ) −q(uτ, φ′)]\n\f\f ≤Ctϵ because |ξτ| ≤1 almost surely. Subsequently, for any ∆> 0,\nPr\n\"\n∃φ ∈BD(R) :\n\f\f\f\f\f\nt\nX\nτ=1\nξτq(uτ, φ)\n\f\f\f\f\f > Ctϵ + ∆\n#\n≤Pr\n\"\n∃φ′ ∈H :\n\f\f\f\f\f\nt\nX\nτ=1\nξτq(uτ, φ′)\n\f\f\f\f\f > ∆\n#\n≤\nX\nφ′∈H\nPr\n\"\f\f\f\f\f\nt\nX\nτ=1\nξτq(uτ, φ′)\n\f\f\f\f\f > ∆\n#\n,\nwhere the last inequality holds by the union bound.\nFor any ﬁxed φ′ ∈H, h(uτ, φ′) only depends on uτ, and therefore E[ξτ | q(uτ, φ′)] = 0 for all τ.\nInvoking Lemma 8 with Xτ ≜P\nτ ′≤τ ξτ ′q(uτ ′, φ′) and cτ ′ = |q(uτ ′, φ′)|, we have\nPr\n\"\f\f\f\f\f\nt\nX\nτ=1\nξτq(uτ, φ′)\n\f\f\f\f\f > ∆\n#\n≤2 exp\n(\n−∆2\n2 P\nτ≤t q2(uτ, φ′)\n)\n= 2 exp\n\u001a −∆2\n2Vq(φ′)\n\u001b\nEquating the right-hand side of the above inequality with δ′ and combining with the union bound application,\nwe have\nPr\n\"\n∃φ ∈Bd(R) :\n\f\f\f\f\f\nt\nX\nτ=1\nξτh(uτ, φ)\n\f\f\f\f\f > Ctϵ +\nq\n2Vq(φ′) ln(2/δ′)\n#\n≤δ′|H|.\n(11)\n16\nFurther equating δ′ = δ/|H| and using the fact that ln |H| ≤D ln(2R/ϵ), we have\nPr\n\"\n∃φ ∈Bd(R) :\n\f\f\f\f\f\nt\nX\nτ=1\nξτq(uτ, φ)\n\f\f\f\f\f > Ctϵ +\nq\n2DVq(φ′) ln(2R/ϵ) + 2Vq(φ′) ln(1/δ)\n#\n≤δ.\nFinally, as |q(uτ, φ′) −q(uτ, φ)| ≤ϵ, we have Vq(φ′) ≤2Vq(φ) + 2tϵ2 and so\nPr\n\"\n∃φ ∈BD(R) :\n\f\f\f\f\f\nt\nX\nτ=1\nξτq(uτ, φ)\n\f\f\f\f\f > Ctϵ + 2ϵ\np\nDt ln(2R/ϵδ) + 2\nq\nVq(φ)(D ln(2R/ϵ) + ln(1/δ)\n#\n≤δ.\nSetting ϵ = 1/t in the above inequality completes the proof.\nReferences\nYasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits.\nIn Advances in Neural Information Processing Systems, 2011.\nYasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-conﬁdence-set conversions and application\nto sparse stochastic bandits. In International Conference on Artiﬁcial Intelligence and Statistics, 2012.\nAndrás Antos, Csaba Szepesvári, and Rémi Munos. Learning near-optimal policies with bellman-residual\nminimization based ﬁtted policy iteration and a single sample path. Machine Learning, 2008.\nMohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement\nlearning. In International Conference on Machine Learning, 2017.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying\ncount-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems,\n2016.\nSteven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference learning.\nMachine Learning, 1996.\nRonen I. Brafman and Moshe Tennenholtz. R-MAX - A general polynomial time algorithm for near-optimal\nreinforcement learning. The Journal of Machine Learning Research, 2002.\nJinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In\nInternational Conference on Machine Learning, 2019.\nChristoph Dann, Tor Lattimore, and Emma Brunskill. Unifying PAC and regret: Uniform PAC bounds for\nepisodic reinforcement learning. In Advances in Neural Information Processing Systems, 2017.\nSimon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang. Is a good representation sufﬁcient for sample\nefﬁcient reinforcement learning? arXiv:1910.03016, 2019a.\nSimon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudík, and John Langford.\nProvably efﬁcient RL with rich observations via latent state decoding. In International Conference on\nMachine Learning, 2019b.\nAmir-massoud Farahmand, Csaba Szepesvári, and Rémi Munos. Error propagation for approximate policy\nand value iteration. In Advances in Neural Information Processing Systems, 2010.\n17\nSarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The generalized\nlinear case. In Advances in Neural Information Processing Systems, 2010.\nDylan J Foster, Alekh Agarwal, Miroslav Dudík, Haipeng Luo, and Robert E Schapire. Practical contextual\nbandits with regression oracles. In International Conference on Machine Learning, 2018.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration.\narXiv:1812.02900, 2018.\nNan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual\ndecision processes with low Bellman rank are PAC-learnable. In International Conference on Machine\nLearning, 2017.\nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efﬁcient? In\nAdvances in Neural Information Processing Systems, 2018.\nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement learning with\nlinear function approximation. arXiv:1907.05388, 2019.\nMichael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine\nlearning, 2002.\nAkshay Krishnamurthy, Alekh Agarwal, and John Langford. PAC reinforcement learning with rich observa-\ntions. In Advances in Neural Information Processing Systems, 2016.\nLihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits.\nIn International Conference on Machine Learning, 2017.\nFrancisco S Melo and M Isabel Ribeiro. Q-learning with linear function approximation. In Conference on\nLearning Theory, 2007.\nRémi Munos. Error bounds for approximate policy iteration. In International Conference on Machine\nLearning, 2003.\nRémi Munos and Csaba Szepesvári. Finite-time bounds for ﬁtted value iteration. The Journal of Machine\nLearning Research, 2008.\nIan Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped\nDQN. In Advances in neural information processing systems, 2016.\nVictor H Peña, Tze Leung Lai, and Qi-Man Shao. Self-normalized processes: Limit theory and Statistical\nApplications. Springer Science & Business Media, 2008.\nMax Simchowitz and Kevin Jamieson. Non-asymptotic gap-dependent regret bounds for tabular mdps.\narXiv:1905.03814, 2019.\nAlexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC model-free\nreinforcement learning. In International Conference on Machine Learning, 2006.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip\nDeTurck, and Pieter Abbeel. #Exploration: A study of count-based exploration for deep reinforcement\nlearning. In Advances in Neural Information Processing Systems, 2017.\nLin F Yang and Mengdi Wang. Reinforcement leaning in feature space: Matrix bandit, kernels, and regret\nbound. arXiv:1905.10389, 2019.\n18\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2019-12-09",
  "updated": "2019-12-09"
}