{
  "id": "http://arxiv.org/abs/1912.04226v1",
  "title": "Unsupervised Curricula for Visual Meta-Reinforcement Learning",
  "authors": [
    "Allan Jabri",
    "Kyle Hsu",
    "Ben Eysenbach",
    "Abhishek Gupta",
    "Sergey Levine",
    "Chelsea Finn"
  ],
  "abstract": "In principle, meta-reinforcement learning algorithms leverage experience\nacross many tasks to learn fast reinforcement learning (RL) strategies that\ntransfer to similar tasks. However, current meta-RL approaches rely on\nmanually-defined distributions of training tasks, and hand-crafting these task\ndistributions can be challenging and time-consuming. Can \"useful\" pre-training\ntasks be discovered in an unsupervised manner? We develop an unsupervised\nalgorithm for inducing an adaptive meta-training task distribution, i.e. an\nautomatic curriculum, by modeling unsupervised interaction in a visual\nenvironment. The task distribution is scaffolded by a parametric density model\nof the meta-learner's trajectory distribution. We formulate unsupervised\nmeta-RL as information maximization between a latent task variable and the\nmeta-learner's data distribution, and describe a practical instantiation which\nalternates between integration of recent experience into the task distribution\nand meta-learning of the updated tasks. Repeating this procedure leads to\niterative reorganization such that the curriculum adapts as the meta-learner's\ndata distribution shifts. In particular, we show how discriminative clustering\nfor visual representation can support trajectory-level task acquisition and\nexploration in domains with pixel observations, avoiding pitfalls of\nalternatives. In experiments on vision-based navigation and manipulation\ndomains, we show that the algorithm allows for unsupervised meta-learning that\ntransfers to downstream tasks specified by hand-crafted reward functions and\nserves as pre-training for more efficient supervised meta-learning of test task\ndistributions.",
  "text": "Unsupervised Curricula\nfor Visual Meta-Reinforcement Learning\nAllan Jabriα Kyle Hsuβ,† Benjamin Eysenbachγ\nAbhishek Guptaα Sergey Levineα Chelsea Finnδ\nAbstract\nIn principle, meta-reinforcement learning algorithms leverage experience across\nmany tasks to learn fast reinforcement learning (RL) strategies that transfer to\nsimilar tasks. However, current meta-RL approaches rely on manually-deﬁned\ndistributions of training tasks, and hand-crafting these task distributions can be\nchallenging and time-consuming. Can “useful” pre-training tasks be discovered in\nan unsupervised manner? We develop an unsupervised algorithm for inducing an\nadaptive meta-training task distribution, i.e. an automatic curriculum, by modeling\nunsupervised interaction in a visual environment. The task distribution is scaffolded\nby a parametric density model of the meta-learner’s trajectory distribution. We\nformulate unsupervised meta-RL as information maximization between a latent\ntask variable and the meta-learner’s data distribution, and describe a practical\ninstantiation which alternates between integration of recent experience into the task\ndistribution and meta-learning of the updated tasks. Repeating this procedure leads\nto iterative reorganization such that the curriculum adapts as the meta-learner’s\ndata distribution shifts. In particular, we show how discriminative clustering for\nvisual representation can support trajectory-level task acquisition and exploration\nin domains with pixel observations, avoiding pitfalls of alternatives. In experiments\non vision-based navigation and manipulation domains, we show that the algorithm\nallows for unsupervised meta-learning that transfers to downstream tasks speciﬁed\nby hand-crafted reward functions and serves as pre-training for more efﬁcient\nsupervised meta-learning of test task distributions.\n1\nIntroduction\nThe discrepancy between animals and learning machines in their capacity to gracefully adapt and\ngeneralize is a central issue in artiﬁcial intelligence research. The simple nematode C. elegans is\ncapable of adapting foraging strategies to varying scenarios [9], while many higher animals are driven\nto acquire reusable behaviors even without extrinsic task-speciﬁc rewards [64, 45]. It is unlikely that\nwe can build machines as adaptive as even the simplest of animals by exhaustively specifying shaped\nrewards or demonstrations across all possible environments and tasks. This has inspired work in\nreward-free learning [28], intrinsic motivation [55], multi-task learning [11], meta-learning [50], and\ncontinual learning [59].\nAn important aspect of generalization is the ability to share and transfer ability between related tasks.\nIn reinforcement learning (RL), a common strategy for multi-task learning is conditioning the policy\non side-information related to the task. For instance, contextual policies [49] are conditioned on a task\ndescription (e.g. a goal) that is meant to modulate the strategy enacted by the policy. Meta-learning of\nreinforcement learning (meta-RL) is yet more general as it places the burden of inferring the task on\nthe learner itself, such that task descriptions can take a wider range of forms, the most general being\nan MDP. In principle, meta-reinforcement learning (meta-RL) requires an agent to distill previous\nαUC Berkeley βUniversity of Toronto γCarnegie Mellon University δStanford University\n†Work done as a visiting student researcher at UC Berkeley.\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:1912.04226v1  [cs.AI]  9 Dec 2019\nqφ(s) =\nX\nz\nqφ(s|z)p(z)\nUpdate behavior model\n2. Meta-Train\n1. Organize\nAcquire skills and explore\nrz(s) = λ log qφ(s|z) −log qφ(s)\n+\n+\n+\nData\nTasks\nφ\nFigure 1: An illustration of CARML, our approach for unsupervised meta-RL. We choose the behavior model qφ\nto be a Gaussian mixture model in a jointly, discriminatively learned embedding space. An automatic curriculum\narises from periodically re-organizing past experience via ﬁtting qφ and meta-learning an RL algorithm for\nperformance over tasks speciﬁed using reward functions from qφ.\nexperience into fast and effective adaptation strategies for new, related tasks. However, the meta-RL\nframework by itself does not prescribe where this experience should come from; typically, meta-RL\nalgorithms rely on being provided ﬁxed, hand-speciﬁed task distributions, which can be tedious to\nspecify for simple behaviors and intractable to design for complex ones [27]. These issues beg the\nquestion of whether “useful” task distributions for meta-RL can be generated automatically.\nIn this work, we seek a procedure through which an agent in an environment with visual observations\ncan automatically acquire useful (i.e. utility maximizing) behaviors, as well as how and when to\napply them – in effect allowing for unsupervised pre-training in visual environments. Two key\naspects of this goal are: 1) learning to operationalize strategies so as to adapt to new tasks, i.e.\nmeta-learning, and 2) unsupervised learning and exploration in the absence of explicitly speciﬁed\ntasks, i.e. skill acquisition without supervised reward functions. These aspects interact insofar as the\nformer implicitly relies on a task curriculum, while the latter is most effective when compelled by\nwhat the learner can and cannot do. Prior work has offered a pipelined approach for unsupervised\nmeta-RL consisting of unsupervised skill discovery followed by meta-learning of discovered skills,\nexperimenting mainly in environments that expose low-dimensional ground truth state [25]. Yet, the\naforementioned relation between skill acquisition and meta-learning suggests that they should not be\ntreated separately.\nHere, we argue for closing the loop between skill acquisition and meta-learning in order to induce\nan adaptive task distribution. Such co-adaptation introduces a number of challenges related to the\nstability of learning and exploration. Most recent unsupervised skill acquisition approaches optimize\nfor the discriminability of induced modes of behavior (i.e. skills), typically expressing the discovery\nproblem as a cooperative game between a policy and a learned reward function [24, 16, 1]. However,\nrelying solely on discriminability becomes problematic in environments with high-dimensional\n(image-based) observation spaces as it results in an issue akin to mode-collapse in the task space. This\nproblem is further complicated in the setting we propose to study, wherein the policy data distribution\nis that of a meta-learner rather than a contextual policy. We will see that this can be ameliorated by\nspecifying a hybrid discriminative-generative model for parameterizing the task distribution.\nThe main contribution of this paper is an approach for inducing a task curriculum for unsupervised\nmeta-RL in a manner that scales to domains with pixel observations. Through the lens of information\nmaximization, we frame our unsupervised meta-RL approach as variational expectation-maximization\n(EM), in which the E-step corresponds to ﬁtting a task distribution to a meta-learner’s behavior and the\nM-step to meta-RL on the current task distribution with reinforcement for both skill acquisition and\nexploration. For the E-step, we show how deep discriminative clustering allows for trajectory-level\nrepresentations suitable for learning diverse skills from pixel observations. Through experiments in\nvision-based navigation and robotic control domains, we demonstrate that the approach i) enables\nan unsupervised meta-learner to discover and meta-learn skills that transfer to downstream tasks\nspeciﬁed by human-provided reward functions, and ii) can serve as pre-training for more efﬁcient\nsupervised meta-reinforcement learning of downstream task distributions.\n2\nPreliminaries: Meta-Reinforcement Learning\nSupervised meta-RL optimizes an RL algorithm fθ for performance on a hand-crafted distribution\nof tasks p(T ), where fθ might take the form of an recurrent neural network (RNN) implementing\na learning algorithm [13, 61], or a function implementing a gradient-based learning algorithm [18].\nTasks are Markov decision processes (MDPs) Ti = (S, A, ri, P, γ, ρ, T) consisting of state space S,\n2\nqφ\n⇡✓\nrz\nt-1\nrz\nt\nUnsupervised \nPre-training\nzn\nTransfer\nto Test Tasks\n⇡✓\nEnv\nst\nat-1\nat\nst+1\nat\nrz\nt-1\nst\nat-1\nrz\nt\nat\nst+1\nat\nEnv\nFigure 2:\nA step for the meta-learner.\n(Left) Unsupervised pre-training. The\npolicy meta-learns self-generated tasks\nbased on the behavior model qφ. (Right)\nTransfer. Faced with new tasks, the policy\ntransfers acquired meta-learning strategies\nto maximize unseen reward functions.\naction space A, reward function ri : S × A →R, probabilistic transition dynamics P(st+1|st, at),\ndiscount factor γ, initial state distribution ρ(s1), and ﬁnite horizon T. Often, and in our setting,\ntasks are assumed to share S, A. For a given T ∼p(T ), fθ learns a policy πθ(a|s, DT ) conditioned\non task-speciﬁc experience. Thus, a meta-RL algorithm optimizes fθ for expected performance of\nπθ(a|s, DT ) over p(T ), such that it can generalize to unseen test tasks also sampled from p(T ).\nFor example, RL2 [13, 61] chooses fθ to be an RNN with weights θ. For a given task T , fθ hones\nπθ(a|s, DT ) as it recurrently ingests DT = (s1, a1, r(s1, a1), d1, . . . ), the sequence of states, actions,\nand rewards produced via interaction within the MDP. Crucially, the same task is seen several times,\nand the hidden state is not reset until the next task. The loss is the negative discounted return obtained\nby πθ across episodes of the same task, and fθ can be optimized via standard policy gradient methods\nfor RL, backpropagating gradients through time and across episode boundaries.\nUnsupervised meta-RL aims to break the reliance of the meta-learner on an explicit, upfront spec-\niﬁcation of p(T ). Following Gupta et al. [25], we consider a controlled Markov process (CMP)\nC = (S, A, P, γ, ρ, T), which is an MDP without a reward function. We are interested in the problem\nof learning an RL algorithm fθ via unsupervised interaction within the CMP such that once a reward\nfunction r is speciﬁed at test-time, fθ can be readily applied to the resulting MDP to efﬁciently\nmaximize the expected discounted return.\nPrior work [25] pipelines skill acquisition and meta-learning by pairing an unsupervised RL algorithm\nDIAYN [16] and a meta-learning algorithm MAML [18]: ﬁrst, a contextual policy is used to discover\nskills in the CMP, yielding a ﬁnite set of learned reward functions distributed as p(r); then, the CMP\nis combined with a frozen p(r) to yield p(T ), which is fed to MAML to meta-learn fθ. In the next\nsection, we describe how we can generalize and improve upon this pipelined approach by jointly\nperforming skill acquisition as the meta-learner learns and explores in the environment.\n3\nCurricula for Unsupervised Meta-Reinforcement Learning\nMeta-learning is intended to prepare an agent to efﬁciently solve new tasks related to those seen\npreviously. To this end, the meta-RL agent must balance 1) exploring the environment to infer which\ntask it should solve, and 2) visiting states that maximize reward under the inferred task. The duty\nof unsupervised meta-RL is thus to present the meta-learner with tasks that allow it to practice task\ninference and execution, without the need for human-speciﬁed task distributions. Ideally, the task\ndistribution should exhibit both structure and diversity. That is, the tasks should be distinguishable\nand not excessively challenging so that a developing meta-learner can infer and execute the right skill,\nbut, for the sake of generalization, they should also encompass a diverse range of associated stimuli\nand rewards, including some beyond the current scope of the meta-learner. Our aim is to strike this\nbalance by inducing an adaptive task distribution.\nWith this motivation, we develop an algorithm for unsupervised meta-reinforcement learning in visual\nenvironments that constructs a task distribution without supervision. The task distribution is derived\nfrom a latent-variable density model of the meta-learner’s cumulative behavior, with exploration\nbased on the density model driving the evolution of the task distribution. As depicted in Figure1,\nlearning proceeds by alternating between two steps: organizing experiential data (i.e., trajectories\ngenerated by the meta-learner) by modeling it with a mixture of latent components forming the basis\nof “skills”, and meta-reinforcement learning by treating these skills as a training task distribution.\nLearning the task distribution in a data-driven manner ensures that tasks are feasible in the environ-\nment. While the induced task distribution is in no way guaranteed to align with test task distributions,\nit may yet require an implicit understanding of structure in the environment. This can indeed be\nseen from our visualizations in §5, which demonstrate that acquired tasks show useful structure,\nthough in some settings this structure is easier to meta-learn than others. In the following, we\nformalize our approach, CARML, through the lens of information maximization and describe a\nconcrete instantiation that scales to the vision-based environments considered in §5.\n3\n3.1\nAn Overview of CARML\nWe begin from the principle of information maximization (IM), which has been applied across\nunsupervised representation learning [4, 3, 41] and reinforcement learning [39, 24] for organization\nof data involving latent variables. In what follows, we organize data from our policy by maximizing\nthe mutual information (MI) between state trajectories τ := (s1, . . . , sT ) and a latent task variable z.\nThis objective provides a principled manner of trading-off structure and diversity: from I(τ; z) :=\nH(τ) −H(τ|z), we see that H(τ) promotes coverage in policy data space (i.e. diversity) while\n−H(τ|z) encourages a lack of diversity under each task (i.e. structure that eases task inference).\nWe approach maximizing I(τ; z) exhibited by the meta-learner fθ via variational EM [3], introducing\na variational distribution qφ that can intuitively be viewed as a task scaffold for the meta-learner.\nIn the E-step, we ﬁt qφ to a reservoir of trajectories produced by fθ, re-organizing the cumulative\nexperience. In turn, qφ gives rise to a task distribution p(T ): each realization of the latent variable z\ninduces a reward function rz(s), which we combine with the CMP Ci to produce an MDP Ti (Line 8).\nIn the M-step, fθ meta-learns the task distribution p(T ). Repeating these steps forms a curriculum in\nwhich the task distribution and meta-learner co-adapt: each M-step adapts the meta-learner fθ to the\nupdated task distribution, while each E-step updates the task scaffold qφ based on the data collected\nduring meta-training. Pseudocode for our method is presented in Algorithm 1.\nAlgorithm 1: CARML\n1: Require: C, an MDP without a reward function\n2: Initialize fθ, an RL algorithm parameterized by θ.\n3: Initialize D, a reservoir of state trajectories, via a randomly initialized policy.\n4: while not done do\n5:\nFit a task-scaffold qφ to D, e.g. by using Algorithm 2.\nE-step §3.2\n6:\nfor a desired mixture model-ﬁtting period do\n7:\nSample a latent task variable z ∼qφ(z).\n8:\nDeﬁne the reward function rz(s), e.g. by Eq. 8, and a task T = C ∪rz(s).\n9:\nApply fθ on task T to obtain a policy πθ(a|s, DT ) and trajectories {τi}.\n10:\nUpdate fθ via a meta-RL algorithm, e.g. RL2 [13].\nM-step §3.3\n11:\nAdd the new trajectories to the reservoir: D ←D ∪{τi}.\n12: Return: a meta-learned RL algorithm fθ tailored to C\n3.2\nE-Step: Task Acquisition\nThe purpose of the E-step is to update the task distribution by integrating changes in the meta-learner’s\ndata distribution with previous experience, thereby allowing for re-organization of the task scaffold.\nThis data is from the post-update policy, meaning that it comes from a policy πθ(a|s, DT ) conditioned\non data collected by the meta-learner for the respective task. In the following, we abuse notation by\nwriting πθ(a|s, z) – conditioning on the latent task variable z rather than the task experience DT .\nThe general strategy followed by recent approaches for skill discovery based on IM is to lower\nbound the objective by introducing a variational posterior qφ(z|s) in the form of a classiﬁer. In these\napproaches, the E-step amounts to updating the classiﬁer to discriminate between data produced by\ndifferent skills as much as possible. A potential failure mode of such an approach is an issue akin to\nmode-collapse in the task distribution, wherein the policy drops modes of behavior to favor easily\ndiscriminable trajectories, resulting in a lack of diversity in the task distribution and no incentive for\nexploration; this is especially problematic when considering high-dimensional observations. Instead,\nhere we derive a generative variant, which allows us to account for explicitly capturing modes of\nbehavior (by optimizing for likelihood), as well as a direct mechanism for exploration.\nWe introduce a variational distribution qφ, which could be e.g. a (deep) mixture model with discrete\nz or a variational autoencoder (VAE) [34] with continuous z, lower-bounding the objective:\nI(τ; z) = −\nX\nτ\nπθ(τ) log πθ(τ) +\nX\nτ,z\nπθ(τ, z) log πθ(τ|z)\n(1)\n≥−\nX\nτ\nπθ(τ) log πθ(τ) +\nX\nτ,z\nπθ(τ|z)qφ(z) log qφ(τ|z)\n(2)\nThe E-step corresponds to optimizing Eq. 2 with respect to φ, and thus amounts to ﬁtting qφ to a\nreservoir of trajectories D produced by πθ:\nmax\nφ\nEz∼qφ(z),τ∼D\n\u0002\nlog qφ(τ|z)\n\u0003\n(3)\n4\nWhat remains is to determine the form of qφ. We choose the variational distribution to be a state-level\nmixture density model qφ(s, z) = qφ(s|z)qφ(z). Despite using a state-level generative model, we can\ntreat z as a trajectory-level latent by computing the trajectory-level likelihood as the factorized product\nof state likelihoods (Algorithm 2, Line 4). This is useful for obtaining trajectory-level tasks; in the\nM-step (§3.3), we map samples from qφ(z) to reward functions to deﬁne tasks for meta-learning.\nAlgorithm 2: Task Acquisition via Discriminative Clustering\n1: Require: a set of trajectories D = {(s1, . . . , sT )}N\ni=1\n2: Initialize (φw, φm), encoder and mixture parameters.\n3: while not converged do\n4:\nCompute L(φm; τ, z) = P\nst∈τ log qφm(gφw(st)|z).\n5:\nφm ←arg maxφ′m\nPN\ni=1 L(φ′\nm; τi, z) (via MLE)\n6:\nD := {(s, y := arg maxk qφm(z = k|gφw(s))}.\n7:\nφw ←arg maxφ′w\nP\n(s,y)∈D log q(y|gφ′w(s))\n8: Return: a mixture model qφ(s, z)\nz\n…\nFigure 3: Conditional independence\nassumption for states along a trajectory.\nModeling Trajectories of Pixel Observations. While models like the variational autoencoder have\nbeen used in related settings [40], a basic issue is that optimizing for reconstruction treats all pixels\nequally. We, rather, will tolerate lossy representations as long as they capture discriminative features\nuseful for stimulus-reward association. Drawing inspiration from recent work on unsupervised feature\nlearning by clustering [6, 10], we propose to ﬁt the trajectory-level mixture model via discriminative\nclustering, striking a balance between discriminative and generative approaches.\nWe adopt the optimization scheme of DeepCluster [10], which alternates between i) clustering\nrepresentations to obtain pseudo-labels and ii) updating the representation by supervised learning\nof pseudo-labels. In particular, we derive a trajectory-level variant (Algorithm 2) by forcing the\nresponsibilities of all observations in a trajectory to be the same (see Appendix A.1 for a derivation),\nleading to state-level visual representations optimized with trajectory-level supervision.\nThe conditional independence assumption in Algorithm 2 is a simpliﬁcation insofar as it discards\nthe order of states in a trajectory. However, if the dynamics exhibit continuity and causality, the\nvisual representation might yet capture temporal structure, since, for example, attaining certain\nobservations might imply certain antecedent subtrajectories. We hypothesize that a state-level model\ncan regulate issues of over-expressive sequence encoders, which have been found to lead to skills\nwith undesirable attention to details in dynamics [1]. As we will see in §5, learning representations\nunder this assumption still allows for learning visual features that capture trajectory-level structure.\n3.3\nM-Step: Meta-Learning\nUsing the task scaffold updated via the E-step, we meta-learn fθ in the M-step so that πθ can be\nquickly adapted to tasks drawn from the task scaffold. To deﬁne the task distribution, we must specify\na form for the reward functions rz(s). To allow for state-conditioned Markovian rewards rather than\nnon-Markovian trajectory-level rewards, we lower-bound the trajectory-level MI objective:\nI(τ; z) = 1\nT\nT\nX\nt=1\nH(z) −H(z|s1, ..., sT ) ≥1\nT\nT\nX\nt=1\nH(z) −H(z|st)\n(4)\n≥Ez∼qφ(z),s∼πθ(s|z)\n\u0002\nlog qφ(s|z) −log πθ(s)\n\u0003\n(5)\nWe would like to optimize the meta-learner under the variational objective in Eq. 5, but optimizing\nthe second term, the policy’s state entropy, is in general intractable. Thus, we make the simplifying\nassumption that the ﬁtted variational marginal distribution matches that of the policy:\nmax\nθ\nEz∼qφ(z),s∼πθ(s|z)\n\u0002\nlog qφ(s|z) −log qφ(s)\n\u0003\n(6)\n= max\nθ\nI(πθ(s); qφ(z)) −DKL(πθ(s|z) ∥qφ(s|z)) + DKL(πθ(s) ∥qφ(s)))\n(7)\nOptimizing Eq. 6 amounts to maximizing the reward of rz(s) = log qφ(s|z) −log qφ(s). As shown\nin Eq. 7, this corresponds to information maximization between the policy’s state marginal and the\nlatent task variable, along with terms for matching the task-speciﬁc policy data distribution to the\n5\ncorresponding mixture mode and deviating from the mixture’s marginal density. We can trade-off\nbetween component-matching and exploration by introducing a weighting term λ ∈[0, 1] into rz(s):\nrz(s) = λ log qφ(s|z) −log qφ(s)\n(8)\n= (λ −1) log qφ(s|z) + log qφ(z|s) + C\n(9)\nwhere C is a constant with respect to the optimization of θ. From Eq. 9, we can interpret λ as trading\noff between discriminability of skills and task-speciﬁc exploration. Figure 4 shows the effect of\ntuning λ on the structure-diversity trade-off alluded to at the beginning of §3.\nFigure 4: Balancing consistency and ex-\nploration with λ in a simple 2D maze en-\nvironment. Each row shows a progression\nof tasks developed over the course of train-\ning. Each box presents the mean recon-\nstructions under a VAE qφ (Appendix C)\nof 2048 trajectories. Varying λ of Eq. 8\nacross rows, we observe that a small λ (top)\nresults in aggressive exploration; a large\nλ (bottom) yields relatively conservative\nbehavior; and a moderate λ (middle) pro-\nduces sufﬁcient exploration and a smooth\ntask distribution.\n4\nRelated Work\nUnsupervised Reinforcement Learning. Unsupervised learning in the context of RL is the problem\nof enabling an agent to learn about its environment and acquire useful behaviors without human-\nspeciﬁed reward functions. A large body of prior work has studied exploration and intrinsic motivation\nobjectives [51, 48, 43, 22, 8, 5, 35, 42]. These algorithms do not aim to acquire skills that can be\noperationalized to solve tasks, but rather try to achieve wide coverage of the state space; our objective\n(Eq. 8) reduces to pure density-based exploration with λ = 0. Hence, these algorithms still rely on\nslow RL [7] in order to adapt to new tasks posed at test-time. Some prior works consider unsupervised\npre-training for efﬁcient RL, but these works typically focus on settings in which exploration is not as\nmuch of a challenge [63, 17, 14], focus on goal-conditioned policies [44, 40], or have not been shown\nto scale to high-dimensional visual observation spaces [36, 54]. Perhaps most relevant to our work\nare unsupervised RL algorithms for learning reward functions via optimizing information-theoretic\nobjectives involving latent skill variables [24, 1, 16, 62]. In particular, with a choice of λ = 1 in\nEq. 9 we recover the information maximization objective used in prior work [1, 16], besides the fact\nthat we simulatenously perform meta-learning. The setting of training a contextual policy with a\nclassiﬁer as qφ in our proposed framework (see Appendix A.3) provides an interpretation of DIAYN\nas implicitly doing trajectory-level clustering. Warde-Farley et al. [62] also considers accumulation\nof tasks, but with a focus on goal-reaching and by maintaining a goal reservoir via heuristics that\npromote diversity.\nMeta-Learning. Our work is distinct from above works in that it formulates a meta-learning approach\nto explicitly train, without supervision, for the ability to adapt to new downstream RL tasks. Prior\nwork [31, 33, 2] has investigated this unsupervised meta-learning setting for image classiﬁcation; the\nsetting considered herein is complicated by the added challenges of RL-based policy optimization and\nexploration. Gupta et al. [25] provides an initial exploration of the unsupervised meta-RL problem,\nproposing a straightforward combination of unsupervised skill acquisition (via DIAYN) followed by\nMAML [18] with experiments restricted to environments with fully observed, lower-dimensional\nstate. Unlike these works and other meta-RL works [61, 13, 38, 46, 18, 30, 26, 47, 56, 58], we\nclose the loop to jointly perform task acquisition and meta-learning so as to achieve an automatic\ncurriculum to facilitate joint meta-learning and task-level exploration.\nAutomatic Curricula. The idea of automatic curricula has been widely explored both in supervised\nlearning and RL. In supervised learning, interest in automatic curricula is based on the hypothesis\nthat exposure to data in a speciﬁc order (i.e. a non-uniform curriculum) may allow for learning harder\ntasks more efﬁciently [15, 51, 23]. In RL, an additional challenge is exploration; hence, related work\nin RL considers the problem of curriculum generation, whereby the task distribution is designed\nto guide exploration towards solving complex tasks [20, 37, 19, 52] or unsupervised pre-training\n[57, 21]. Our work is driven by similar motivations, though we consider a curriculum in the setting\nof meta-RL and frame our approach as information maximization.\n6\n5\nExperiments\nWe experiment in visual navigation and visuomotor control domains to study the following questions:\n• What kind of tasks are discovered through our task acquisition process (the E-step)?\n• Do these tasks allow for meta-training of strategies that transfer to test tasks?\n• Does closing the loop to jointly perform task acquisition and meta-learning bring beneﬁts?\n• Does pre-training with CARML accelerate meta-learning of test task distributions?\nVideos are available at the project website https://sites.google.com/view/carml.\n5.1\nExperimental Setting\nThe following experimental details are common to the two vision-based environments we consider.\nOther experimental are explained in more detail in Appendix B.\nMeta-RL. CARML is agnostic to the meta-RL algorithm used in the M-step. We use the RL2\nalgorithm [13], which has previously been evaluated on simpler visual meta-RL domains, with a\nPPO [53] optimizer. Unless otherwise stated, we use four episodes per trial (compared to the two\nepisodes per trial used in [13]), since the settings we consider involve more challenging task inference.\nBaselines. We compare against: 1) PPO from scratch on each evaluation task, 2) pre-training with\nrandom network distillation (RND) [8] for unsupervised exploration, followed by ﬁne-tuning on\nevaluation tasks, and 3) supervised meta-learning on the test-time task distribution, as an oracle.\nVariants. We consider variants of our method to ablate the role of design decisions related to task\nacquisition and joint training: 4) pipelined (most similar to [25]) – task acquisition with a contextual\npolicy, followed by meta-RL with RL2; 5) online discriminator – task acquisition with a purely\ndiscriminative qφ (akin to online DIAYN); and 6) online pretrained-discriminator – task acquisition\nwith a discriminative qφ initialized with visual features trained via Algorithm 2.\n5.2\nVisual Navigation\nThe ﬁrst domain we consider is ﬁrst-person visual navigation in ViZDoom [32], involving a room\nﬁlled with ﬁve different objects (drawn from a set of 50). We consider a setup akin to those featured in\n[12, 65] (see Figure 3). The true state consists of continuous 2D position and continuous orientation,\nwhile observations are egocentric images with limited ﬁeld of view. Three discrete actions allow for\nturning right or left, and moving forward. We consider two ways of sampling the CMP C. Fixed: ﬁx\na set of ﬁve objects and positions for both unsupervised meta-training and testing. Random: sample\nﬁve objects and randomly place them (thereby randomizing the state space and dynamics).\nVisualizing the task distribution. Modeling pixel observations reveals trajectory-level organization\nin the underlying true state space (Figure 5). Each map portrays trajectories of a mixture component,\nwith position encoded in 2D space and orientation encoded in the jet color-space; an example of\ninterpreting the maps is shown left of the legend. The components of the mixture model reveal\nstructured groups of trajectories: some components correspond to exploration of the space (marked\nwith green border), while others are more strongly directed towards speciﬁc areas (blue border). The\nskill maps of the ﬁxed and random environments are qualitatively different: tasks in the ﬁxed room\ntend towards interactions with objects or walls, while many of the tasks in the random setting sweep\nRandom\nLegend\nFixed\nStep 1\nStep 5\nStart\nFigure 5: Skill maps for visual navigation. We visualize some of the discovered tasks by projecting trajectories\nof certain mixture components into the true state space. White dots correspond to ﬁxed objects. The legend\nindicates orientation as color; on its left is an interpretation of the depicted component. Some tasks seem to\ncorrespond to exploration of the space (green border), while others are more directed towards speciﬁc areas (blue\nborder). Comparing tasks earlier and later in the curriculum (step 1 to step 5), we ﬁnd an increase in structure.\n7\n0\n10000\n20000\n30000\n40000\n50000\n# Samples from Test Reward\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Rate\nFinetune CARML (Random)\nPPO (Scratch)\nRND Init\nCARML Step 1 (Fixed)\nCARML Step K (Fixed)\nCARML Step 1 (Random)\nCARML Step K (Random)\nHandcrafted (Oracle)\n(a) ViZDoom\n0\n5000\n10000 15000 20000 25000 30000 35000 40000\n# Samples from Test Reward\n0.2\n0.4\n0.6\n0.8\n1.0\nFinetune CARML (Random)\nPPO (Scratch)\nRND Init\nCARML Step 1\nCARML Step K\nHandcrafted (Oracle)\n(b) Sawyer\n0\n10000\n20000\n30000\n40000\n50000\n# Samples from Test Reward\n0.2\n0.4\n0.6\n0.8\n1.0\nPPO (Scratch)\nOnline Disc.\nOnline Pretrained-Disc.\nPipelined CARML\nCARML Step K\nHandcrafted (Oracle)\n(c) Variants (ViZDoom Random)\nFigure 6: CARML enables unsupervised meta-learning of skills that transfer to downstream tasks. Direct\ntransfer curves (marker and dotted line) represent a meta-learner deploying for just 200 time steps at test time.\nCompared to CARML, PPO and RND Init sample the test reward function orders of magnitude more times\nto perform similarly on a single task. Finetuning the CARML policy also allows for solving individual tasks\nwith signiﬁcantly fewer samples. The ablation experiments (c) assess both direct transfer and ﬁnetuning for\neach variant. Compared to variants, the CARML task acquisition procedure results in improved transfer due to\nmitigation of task mode-collapse and adaptation of the task distribution.\nthe space in a particular direction. We can also see the evolution of the task distribution at earlier and\nlater stages of Algorithm 1. While initial tasks (produced by a randomly initialized policy) tend to\nbe less structured, we later see reﬁnement of certain tasks as well as the emergence of others as the\nagent collects new data and acquires strategies for performing existing tasks.\nDo acquired skills transfer to test tasks? We evaluate how well the CARML task distribution\nprepares the agent for unseen tasks. For both the ﬁxed and randomized CMP experiments, each test\ntask speciﬁes a dense goal-distance reward for reaching a single object in the environment. In the\nrandomized environment setting, the target objects at test-time are held out from meta-training. The\nPPO and RND-initialized baseline polices, and the ﬁnetuned CARML meta-policy, are trained for a\nsingle target (a speciﬁc object in a ﬁxed environment), with 100 episodes per PPO policy update.\nIn Figure 6a, we compare the success rates on test tasks as a function of the number of samples\nwith supervised rewards seen from the environment. Direct transfer performance of meta-learners is\nshown as points, since in this setting the RL2 agent sees only four episodes (200 samples) at test-time,\nwithout any parameter updates. We see that direct transfer is signiﬁcant, achieving up to 71% and\n59% success rates on the ﬁxed and randomized settings, respectively. The baselines require over two\norders of magnitude more test-time samples to solve a single task at the same level.\nWhile the CARML meta-policy does not consistently solve the test tasks, this is not surprising since no\ninformation is assumed about target reward functions during unsupervised meta-learning; inevitable\ndiscrepancies between the meta-train and test task distributions will mean that meta-learned strategies\nwill be suboptimal for the test tasks. For instance, during testing, the agent sometimes ‘stalls’ before\nthe target object (once inferred), in order to exploit the inverse distance reward. Nevertheless, we also\nsee that ﬁnetuning the CARML meta-policy trained on random environments on individual tasks is\nmore sample efﬁcient than learning from scratch. This suggests that deriving reward functions from\nour mixture model yields useful tasks insofar as they facilitate learning of strategies that transfer.\nBeneﬁt of reorganization. In Figure 6a, we also compare performance across early and late outer-\nloop iterations of Algorithm 1, to study the effect of adapting the task distribution (the CARML\nE-step) by reorganizing tasks and incorporating new data. In both cases, number of outer-loop\niterations K = 5. Overall, the reﬁnement of the task distribution, which we saw in Figure 5, leads\nimproved to transfer performance. The effect of reorganization is further visualized in the Appendix F.\nVariants. From Figure 6c, we see that the purely online discriminator variant suffers in direct transfer\nperformance; this is due to the issue of mode-collapse in task distribution, wherein the task distribution\nlacks diversity. Pretraining the discriminator encoder with Algorithm 2 mitigates mode-collapse to an\nextent, improving task diversity as the features and task decision boundaries are ﬁrst ﬁt on a corpus\nof (randomly collected) trajectories. Finally, while the distribution of tasks eventually discovered\nby the pipelined variant may be diverse and structured, meta-learning the corresponding tasks from\nscratch is harder. More detailed analysis and visualization is given in Appendix E.\n5.3\nVisual Robotic Manipulation\nTo experiment in a domain with different challenges, we consider a simulated Sawyer arm interacting\nwith an object in MuJoCo [60], with end-effector continous control in the 2D plane. The observation\nis a bottom-up view of a surface supporting an object (Figure 7); the camera is stationary, but the\nview is no longer egocentric and part of the observation is proprioceptive. The test tasks involve\n8\nFigure 7: (Left) Skill maps for visuomotor control. Red encodes the true position of the object, and light blue\nthat of the end-effector. Tasks correspond to moving the object to various regions (see Appendix D for more\nskills maps and analysis). (Right) Observation and third person view from the environment, respectively.\npushing the object to a goal (drawn from the set of reachable states), where the reward function is the\nnegative distance to the goal state. A subset of the skill maps is provided below.\nDo acquired skills directly transfer to test tasks? In Figure 6b, we evaluate the meta-policy on the\ntest task distribution, comparing against baselines as previously. Despite the increased difﬁculty of\ncontrol, our approach allows for meta-learning skills that transfer to the goal distance reward task\ndistribution. We ﬁnd that transfer is weaker compared to the visual navigation (ﬁxed version): one\nreason may be that the environment is not as visually rich, resulting in a signiﬁcant gap between the\nCARML and the object-centric test task distributions.\n5.4\nCARML as Meta-Pretraining\n0\n200\n400\n600\n800\n1000\nPolicy Updates\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSuccess Rate\nRL² from Scratch\nCARML init (Ours)\nEncoder init (Ours)\n(a) ViZDoom (random)\n0\n100\n200\n300\n400\n500\n600\nPolicy Updates\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nRL² from Scratch\nCARML init (Ours)\nEncoder init (Ours)\n(b) Sawyer\nFigure 8: Finetuning the CARML meta-policy allows for\naccelerated meta-learning of the target task distribution.\nCurves reﬂect error bars across three random seeds.\nAnother compelling form of transfer is pre-\ntraining of an initialization for accelerated\nsupervised meta-RL of target task distribu-\ntions. In Figure 8, we see that the initial-\nization learned by CARML enables effective\nsupervised meta-RL with signiﬁcantly fewer\nsamples. To separate the effect of the learn-\ning the recurrent meta-policy and the visual\nrepresentation, we also compare to only ini-\ntializing the pre-trained encoder. Thus, while\ndirect transfer of the meta-policy may not directly result in optimal behavior on test tasks, accelerated\nlearning of the test task distribution suggests that the acquired meta-learning strategies may be useful\nfor learning related task distributions, effectively acting as pre-training procedure for meta-RL.\n6\nDiscussion\nWe proposed a framework for inducing unsupervised, adaptive task distributions for meta-RL that\nscales to environments with high-dimensional pixel observations. Through experiments in visual\nnavigation and manipulation domains, we showed that this procedure enables unsupervised acquisition\nof meta-learning strategies that transfer to downstream test task distributions in terms of direct\nevaluation, more sample-efﬁcient ﬁne-tuning, and more sample-efﬁcient supervised meta-learning.\nNevertheless, the following key issues are important to explore in future work.\nTask distribution mismatch. While our results show that useful structure can be meta-learned in an\nunsupervised manner, results like the stalling behavior in ViZDoom (see §5.2) suggest that direct\ntransfer of unsupervised meta-learning strategies suffers from a no-free-lunch issue: there will always\nbe a gap between unsupervised and downstream task distributions, and more so with more complex\nenvironments. Moreover, the semantics of target tasks may not necessarily align with especially\ndiscriminative visual features. This is part of the reason why transfer in the Sawyer domain is less\nsuccessful. Capturing other forms of structure useful for stimulus-reward association might involve\nincorporating domain-speciﬁc inductive biases into the task-scaffold model. Another way forward is\nthe semi-supervised setting, whereby data-driven bias is incorporated at meta-training time.\nValidation and early stopping: Since the objective optimized by the proposed method is non-\nstationary and in no way guaranteed to be correlated with objectives of test tasks, one must provide\nsome mechanism for validation of iterates.\nForm of skill-set. For the main experiments, we ﬁxed a number of discrete tasks to be learned\n(without tuning this), but one should consider how the set of skills can be grown or parameterized to\nhave higher capacity (e.g. a multi-label or continuous latent). Otherwise, the task distribution may\nbecome overloaded (complicating task inference) or limited in capacity (preventing coverage).\nAccumulation of skill. We mitigate forgetting with the simple solution of reservoir sampling. Better\nsolutions involve studying an intersection of continual learning and meta-learning.\n9\nAcknowledgments\nWe thank the BAIR community for helpful discussion, and Michael Janner and Oleh Rybkin in\nparticular for feedback on an earlier draft. AJ thanks Alexei Efros for his steadfastness and advice,\nand Sasha Sax and Ashish Kumar for discussion. KH thanks his family for their support. AJ is\nsupported by the PD Soros Fellowship. This work was supported in part by the National Science\nFoundation, IIS-1651843, IIS-1700697, and IIS-1700696, as well as Google.\nReferences\n[1] Joshua Achiam, Harrison Edwards, Dario Amodei, and Pieter Abbeel. Variational option\ndiscovery algorithms. arXiv preprint arXiv:1807.10299, 2018.\n[2] Antreas Antoniou and Amos Storkey. Assume, augment and learn: unsupervised few-shot\nmeta-learning via random labels and data augmentation. arXiv preprint arXiv:1902.09884v3,\n2019.\n[3] David Barber and Felix Agakov. The IM algorithm: a variational approach to information\nmaximization. In Neural Information Processing Systems (NeurIPS), 2004.\n[4] Anthony J. Bell and Terrence J. Sejnowski. An information-maximization approach to blind\nseparation and blind deconvolution. Neural Computation, 7(6), 1995.\n[5] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi\nMunos. Unifying count-based exploration and intrinsic motivation. In Neural Information\nProcessing Systems (NeurIPS), 2016.\n[6] Piotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. In Interna-\ntional Conference on Machine Learning (ICML), 2017.\n[7] Matthew Botvinick, Sam Ritter, Jane X Wang, Zeb Kurth-Nelson, Charles Blundell, and Demis\nHassabis. Reinforcement learning, fast and slow. Trends in Cognitive Science, 23(5), 2019.\n[8] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random\nnetwork distillation. In International Conference on Learning Representations (ICLR), 2019.\n[9] Adam J. Calhoun, Sreekanth H. Chalasani, and Tatyana O. Sharpee. Maximally informative\nforaging by Caenorhabditis elegans. eLife, 3, 2014.\n[10] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for\nunsupervised learning of visual features. In European Conference on Computer Vision (ECCV),\n2018.\n[11] Rich Caruana. Multitask learning. Machine Learning, 28(1), 1997.\n[12] Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj\nRajagopal, and Ruslan Salakhutdinov. Gated-attention architectures for task-oriented language\ngrounding. In AAAI Conference on Artiﬁcial Intelligence, 2018.\n[13] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2:\nfast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779,\n2016.\n[14] Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning\nwith temporal skip connections. In Conference on Robotic Learning (CoRL), 2017.\n[15] Jeffrey L Elman. Learning and development in neural networks: the importance of starting\nsmall. Cognition, 48(1), 1993.\n[16] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you\nneed: learning skills without a reward function. In International Conference on Learning\nRepresentations (ICLR), 2019.\n[17] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In Interna-\ntional Conference on Robotics and Automation (ICRA), 2017.\n[18] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta-\ntion of deep networks. In International Conference on Machine Learning (ICML), 2017.\n[19] Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation\nfor reinforcement learning agents. In International Conference on Machine Learning (ICML),\n2017.\n10\n[20] Carlos Florensa, David Held, Markus Wulfmeier, and Pieter Abbeel. Reverse curriculum\ngeneration for reinforcement learning. In Conference on Robotic Learning (CoRL), 2017.\n[21] Sébastien Forestier, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically motivated goal\nexploration processes with automatic curriculum learning. arXiv preprint arXiv:1708.02190,\n2017.\n[22] Justin Fu, John Co-Reyes, and Sergey Levine. EX2: exploration with exemplar models for deep\nreinforcement learning. In Neural Information Processing Systems (NeurIPS), 2017.\n[23] Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Au-\ntomated curriculum learning for neural networks. In International Conference on Machine\nLearning (ICML), 2017.\n[24] Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv\npreprint arXiv:1611.07507, 2016.\n[25] Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised\nmeta-learning for reinforcement learning. arXiv preprint arXiv:1806.04640, 2018.\n[26] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-\nreinforcement learning of structured exploration strategies. In Neural Information Processing\nSystems (NeurIPS), 2018.\n[27] Dylan Hadﬁeld-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse\nreward design. In Neural Information Processing Systems (NeurIPS), 2017.\n[28] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. Unsupervised learning. In The Elements\nof Statistical Learning. Springer, 2009.\n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385.\n[30] Rein Houthooft, Richard Y Chen, Phillip Isola, Bradly C Stadie, Filip Wolski, Jonathan Ho, and\nPieter Abbeel. Evolved policy gradients. In Neural Information Processing Systems (NeurIPS),\n2018.\n[31] Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-learning. In\nInternational Conference on Learning Representations (ICLR), 2019.\n[32] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja´skowski.\nViZDoom: a Doom-based AI research platform for visual reinforcement learning. In Conference\non Computational Intelligence and Games (CIG), 2016.\n[33] Siavash Khodadadeh, Ladislau Bölöni, and Mubarak Shah. Unsupervised meta-learning for\nfew-shot image and video classiﬁcation. arXiv preprint arXiv:1811.11819v1, 2018.\n[34] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint\narXiv:1312.6114, 2014.\n[35] Joel Lehman and Kenneth O Stanley. Abandoning objectives: evolution through the search for\nnovelty alone. Evolutionary Computation, 19(2), 2011.\n[36] Manuel Lopes, Tobias Lang, Marc Toussaint, and Pierre-Yves Oudeyer. Exploration in model-\nbased reinforcement learning by empirically estimating learning progress. In Neural Information\nProcessing Systems (NeurIPS), 2012.\n[37] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum\nlearning. Transactions on Neural Networks and Learning Systems, 2019.\n[38] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive\nmeta-learner. In International Conference on Learning Representations (ICLR), 2018.\n[39] Shakir Mohamed and Danilo J. Rezende. Variational information maximisation for intrinsically\nmotivated reinforcement learning. In Proceedings of the 28th International Conference on Neu-\nral Information Processing Systems - Volume 2, NIPS’15, pages 2125–2133, Cambridge, MA,\nUSA, 2015. MIT Press. URL http://dl.acm.org/citation.cfm?id=2969442.2969477.\n[40] Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine.\nVisual reinforcement learning with imagined goals. In Neural Information Processing Systems\n(NeurIPS), 2018.\n11\n[41] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv preprint arXiv:1807.03748, 2018.\n[42] Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep rein-\nforcement learning. In Neural Information Processing Systems (NeurIPS), 2018.\n[43] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International Conference on Machine Learning (ICML), 2017.\n[44] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu,\nEvan Shelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation.\nIn International Conference on Learning Representations (ICLR), 2018.\n[45] Jean Piaget. The Construction of Reality in the Child. Basic Books, 1954.\n[46] Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efﬁcient\noff-policy meta-reinforcement learning via probabilistic context variables. In International\nConference on Machine Learning, 2019.\n[47] Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. ProMP: proximal\nmeta-policy search. In International Conference on Learning Representations (ICLR), 2019.\n[48] Christoph Salge, Cornelius Glackin, and Daniel Polani. Empowerment – an introduction. In\nGuided Self-Organization: Inception. Springer, 2014.\n[49] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxi-\nmators. In International Conference on Machine Learning, pages 1312–1320, 2015.\n[50] Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to\nlearn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.\n[51] Jürgen Schmidhuber. Driven by compression progress: a simple principle explains essential\naspects of subjective beauty, novelty, surprise, interestingness, attention, curiosity, creativity, art,\nscience, music, jokes. In Anticipatory Behavior in Adaptive Learning Systems. Springer-Verlag,\n2009.\n[52] Jürgen Schmidhuber.\nPOWERPLAY: training an increasingly general problem solver by\ncontinually searching for the simplest still unsolvable problem. arXiv preprint arXiv:1112.5309,\n2011.\n[53] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[54] Pranav Shyam, Wojciech Ja´skowski, and Faustino Gomez. Model-based active exploration. In\nInternational Conference on Machine Learning (ICML), 2019.\n[55] Satinder Singh, Andrew G Barto, and Nuttapong Chentanez. Intrinsically motivated reinforce-\nment learning. In Neural Information Processing Systems (NeurIPS), 2005.\n[56] Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and\nIlya Sutskever. Some considerations on learning to explore via meta-reinforcement learning. In\nNeural Information Processing Systems (NeurIPS), 2018.\n[57] Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob\nFergus. Intrinsic motivation and automatic curricula via asymmetric self-play. In International\nConference on Learning Representations (ICLR), 2018.\n[58] Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn:\nmeta-critic networks for sample efﬁcient learning. arXiv preprint arXiv:1706.09529, 2017.\n[59] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media,\n1998.\n[60] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: a physics engine for model-based\ncontrol. In International Conference on Intelligent Robots and Systems (IROS), 2012.\n[61] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,\nCharles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. In\nAnnual Meeting of the Cognitive Science Society (CogSci), 2016.\n[62] David Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and\nVolodymyr Mnih. Unsupervised control through non-parametric discriminative rewards. In\nInternational Conference on Learning Representations (ICLR), 2019.\n12\n[63] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed\nto control: a locally linear latent dynamics model for control from raw images. In Neural\nInformation Processing Systems (NeurIPS), 2015.\n[64] Robert W White. Motivation reconsidered: the concept of competence. Psychological Review,\n66(5), 1959.\n[65] Annie Xie, Avi Singh, Sergey Levine, and Chelsea Finn. Few-shot goal inference for visuomotor\nlearning and planning. In Conference on Robot Learning (CoRL), 2018.\n13\nAppendix A\nDerivations\nA.1\nDerivation for Trajectory-Level Responsibilities (Section 3.2.1)\nHere we show that, assuming independence between states in a trajectory when conditioning on a\nlatent variable, computing the trajectory likelihood as a factorized product of state likelihoods for\nthe E-step in standard EM forces the component responsibilities for all states in the trajectory to\nbe identical. Begin by lower-bounding the log-likelihood of the trajectory dataset with Jensen’s\ninequality:\nX\ni\nlog p(τ) =\nX\ni\nlog p(si\n1, si\n2, ..., si\nT )\n(10)\n=\nX\ni\nlog\nX\nz\np(si\n1, si\n2, ..., si\nT |z)p(z)\n(11)\n≥\nX\ni\nX\nz\nqφ(z|s1, s2, ..., sT ) log p(si\n1, si\n2, ..., si\nT |z)p(z)\nqφ(z|s1, s2...sT )\n(12)\n=\nX\ni\nEz∼qφ(z|s1,s2,...,sT ) log p(si\n1, si\n2, ..., si\nT |z)p(z)\nqφ(z|s1, s2, ..., sT ) .\n(13)\nWe have introduced the variational distribution qφ(τ, z), where z is a categorical variable. Now, to\nmaximize Eq. 13 with respect to φ := (µ1, Σ1, π1, ..., µN, ΣN, πN), we alternate between an E-step\nand an M-step, where the E-step is computing\nqik = qφ(z = k|si\n1, si\n2, ..., si\nT )\n(14)\n=\nqφ(si\n1, si\n2, ..., si\nT |z = k)qφ(z = k)\nP\nj qφ(si\n1, si\n2, ..., si\nT |z = j)qφ(z = j)\n(15)\n=\nqφ(si\n1|z = k)qφ(si\n2|z = k) · · · qφ(si\nT |z = k)qφ(z = k)\nP\nj qφ(si\n1|z = j)qφ(si\n2|z = j) · · · qφ(si\nT |z = j)qφ(z = j).\n(16)\nWe assume that each qφ(s|z = k) is Gaussian; the M-step amounts to computing the maximum-\nlikelihood estimate of φ, under the mixture responsibilities from the E-step:\nµk =\nPN\ni=1\nqik\nT\nPT\nt=1 st\nPN\ni=1 qik\n(17)\nΣk =\nPN\ni=1\nqik\nT\nPT\nt=1(st −µk)(st −µk)⊤\nPN\ni=1 qik\n(18)\nπk = 1\nN\nN\nX\ni=1\nqik.\n(19)\nIn particular, note that the expressions are independent of t. Thus, the posterior qφ(z|s) will be, too.\nA.2\nCARML M-Step\nThe objective used to optimize the meta-RL algorithm in the CARML M-step can be interpreted as a\nsum of cross entropies, resulting in the mutual information plus two additional KL terms:\n−Es∼πθ(s|z),z∼qφ(z)\n\u0002\nlog qφ(s) −log qφ(s|z)\n\u0003\n(20)\n= −\nX\nz\nqφ(z)\nX\ns\nπθ(s|z) (log qφ(s) −log qφ(s|z))\n(21)\n= −\nX\nz\nqφ(z)\nX\ns\nπ(s|z)\n\u0012\nlog qφ(s)\nπθ(s) + log πθ(s) −log qφ(s|z)\nπθ(s|z) −log πθ(s|z)\n\u0013\n(22)\n=H(πθ(s)) + DKL(πθ(s) ∥qφ(s)) −H(πθ(s|z)) −DKL(πθ(s|z) ∥qφ(s|z))\n(23)\n=I(πθ(s); qφ(z)) + DKL(πθ(s) ∥qφ(s)) −DKL(πθ(s|z) ∥qφ(s|z)).\n(24)\n14\nThe ﬁrst KL term can be interpreted as encouraging exploration with respect to the density of the\nmixture. The second KL term is the reverse KL term for matching the modes of the mixture.\nDensity-based exploration. In practice, we may want to trade off between exploration and matching\nthe modes of the generative model:\nrz(s) = λ log qφ(s|z) −log qφ(s)\n(25)\n= (λ −1) log qφ(s|z) + log qφ(z|s) −log qφ(z)\n(26)\n= (λ −1) log qφ(s|z) + log qφ(z|s) + C\n(27)\nwhere C is constant with respect to the optimization of θ. Hence, the objective amounts to maximizing\ndiscriminability of skills where λ < 1 yields a bonus for exploring away from the mode of the\ncorresponding skill.\nA.3\nDiscriminative CARML and DIAYN\nHere, we derive a discriminative instantiation of CARML. We begin with the E-step. We leverage the\nsame conditional independence assumption as before, and re-write the trajectory-level MI as the state\nlevel MI, assuming that trajectories are all of length T:\nI(τ; z) ≥1\nT\nX\nt\nI(st; z) = I(s; z)\n(28)\nWe then decompose MI as the difference between marginal and conditional entropy of the latent,\nand choose the variational distribution to be the product of a classiﬁer qφc(z|s) and a density model\nqφd(s):\nI(s; z) = H(z) −H(z|s)\n(29)\n= −\nX\nz\np(z) log p(z) +\nX\ns,z\nπθ(s, z) log πθ(z|s)\n(30)\n≥−\nX\nz\np(z) log p(z) +\nX\ns,z\nπθ(s|z)p(z) log qφc(z|s)\n(31)\nWe ﬁx z to be a uniformly-distributed categorical variable. The CARML E-step consists of two sepa-\nrate optimizations: supervised learning of qφc(z|s) with a cross-entropy loss and density estimation\nof qφd(s):\nmax\nφc\nEz∼p(z),s∼πθ(z) [log qφc(z|s)]\nmax\nφd\nEz∼p(z),s∼πθ(z) [log qφd(s)]\n(32)\nFor the CARML M-step, we start from the form of the reward in Eq. 26 and manipulate via Bayes’:\nrz(s) = log qφc(z|s) + (λ −1) log qφc(z|s) + (λ −1) log qφd(s) −(λ −1) log p(z) −log p(z)\n= λ log qφc(z|s) + (λ −1) log qφd(s) + C\n(33)\nwhere C is constant with respect to the optimization of θ in the M-step\nmax\nθ\nEz∼qφ(z),s∼πθ(z)\n\u0002\nλ log qφc(z|s) + (λ −1) log qφd(s)\n\u0003\n(34)\nTo enable a trajectory-level latent z, we want every state in a trajectory to be classiﬁed to the same z.\nThis is achievable in a straightforward manner: when training the classiﬁer qφc(z|s) via supervised\nlearning, label each state in a trajectory with the realization of z that the policy πθ(a|s, z) was\nconditioned on when generating that trajectory.\nConnection to DIAYN. Note that with λ = 1 in Eq. 34, we directly obtain the DIAYN [16] objective\nwithout standard policy entropy regularization, and we do away with needing to maintain a density\nmodel log qφd(s), leaving just the discriminator. If πθ(a|s, z) is truly a contextual policy (rather\nthan the policy given by adapting a meta-learner), we have recovered the DIAYN algorithm. This\nallows us to interpret on DIAYN-style algorithms as implicitly doing trajectory-level clustering with\na conditional independence assumption between states in a trajectory given the latent. This arises\nfrom the weak trajectory-level supervision speciﬁed when training the discriminator: all states in a\ntrajectory are assumed to correspond to the same realization of the latent variable.\n15\nAppendix B\nAdditional Details for Main Experiments\nB.1\nCARML Hyperparameters\nWe train CARML for ﬁve iterations, with 500 PPO updates for meta-learning with RL2 in the M-step\n(i.e. update the mixture model every 500 meta-policy updates). Thus, the CARML unsupervised\nlearning process consumes on the order of 1,000,000 episodes (compared to the ~400,000 episodes\nneeded to train a meta-policy with the true task distribution, as shown in our experiments). We did\nnot heavily tune this number, though we noticed that using too few policy updates (e.g. ~100) before\nreﬁtting qφ resulted in instability insofar as the meta-learner does not adapt to learn the updated task\ndistribution. Each PPO learning update involves sampling 100 tasks with 4 episodes each, for a total\nof 400 episodes per update. We use 10 PPO epochs per update with a batch size of 100 tasks.\nDuring meta-training, tasks are drawn according to z ∼qφ(z), the mixture’s latent prior distribution.\nUnless otherwise stated, we use λ = 0.99 for all visual meta-RL experiments. For all experiments\nunless otherwise mentioned, we ﬁx the number of components in our mixture to be k = 16. We use a\nreservoir of 1000 trajectories.\nTemporally Smoothed Reward: At unsupervised meta-training time, we found it helpful to reward\nthe meta-learner with the average over a small temporal window, i.e. rW\nz (st) =\n1\nW\nPt\ni=t−W rz(si),\nchoosing W to be W = 10. This has the effect of smoothing the reward function, thereby regularizing\nacquired task inference strategies.\nRandom Seeds: The results reported in Figure 6 are averaged across policies (for each treatment)\ntrained with three different random seeds. The performance is averaged across 20 test tasks. The\nresults reported in Figure 7 are based on ﬁnetuning CARML policies trained with three different\nrandom seeds. We did not observe signiﬁcant effects of the random seed used in the ﬁnetuning\nprocedure of experiments reported for Figure 7.\nModel Selection: Models used for transfer experiments are selected by performance on a small\nheld-out validation set (ten tasks) for each task, that does not intersect with the test task.\nB.2\nMeta-RL with RL2\nWe adopt the recurrent architecture and hyperparameter settings as speciﬁed in the visual maze\nnavigation tasks of Duan et al. [13], except we:\n• Use PPO for policy optimization (clip = 0.2, value_coef = 0.1)\n• Set the entropy bonus coefﬁcient α in an environment-speciﬁc manner. We use α = 0.001\nfor MuJoCo Sawyer and α = 0.1 for ViZDoom.\n• Enlarge the input observation space to 84 × 84 × 3, adapting the encoder by half the stride\nin the ﬁrst convolutional layer.\n• Increase the size of the recurrent model (hidden state size 512) and the capacity of the output\nlayer of the RNN (MLP with one hidden layer of dimension 256).\n• Allow for four episodes per task (instead of two), since the tasks we consider involve more\nchallenging task inference.\n• Use a multi-layer perceptron with one-hidden layer to readout the output for the actor and\ncritic, given the recurrent hidden state.\nB.3\nReward Normalization\nA subtle challenge that arises in applying meta-RL across a range of tasks is difference in the\nstatistics of the reward functions encountered, which may affect task inference. Without some form\nof normalization, the statistics of the rewards of unsupervised meta-training tasks versus those of the\ndownstream tasks may be arbitrarily different, which may interfere with inferring the task. This is\nespecially problematic for RL2 (compared to e.g. MAML [18]), which relies on encoding the reward\nas a feature at each timestep. We address this issue by whitening the reward at each timestep with\nrunning mean and variance computed online, separately for each task from the unsupervised task\ndistribution during meta-training. At test-time, we share these statistics across tasks from the same\ntest task distribution.\n16\nFigure 9: Example Observation Sequences from the Sawyer (left) and Vizdoom Random (right) environments.\nB.4\nLearning Visual Representations with DeepCluster\nTo jointly learn visual representations with the mixture model, we adopt the optimization scheme of\nDeepCluster [10]. The DeepCluster model is parameterized by the weights of a convolutional neural\nnetwork encoder as well as a k-means model in embedding space. It is trained in an EM-like fashion,\nwhere the M-step additionally involves training the encoder weights via supervised learning of the\nimage-cluster mapping.\nOur contribution is that we employ a modiﬁed E-step, as presented in the main text, such that the\ncluster responsibilities are ensured to be consensual across states in a trajectory in the training data.\nAs shown in our experiments, this allows the model to learn trajectory-level visual representations.\nThe full CARML E-step with DeepCluster is presented below.\nAlgorithm 3: CARML E-Step, a Modiﬁed EM Procedure, with DeepCluster\n1: Require: a set of trajectories D = {(s1, . . . , sT )}N\ni=1\n2: Initialize φ := (φw, φm), the weights of encoder g and embedding-space mixture model\nparameters.\n3: while not converged do\n4:\nCompute L(φm; τ, z) = P\nst∈τ log qφm(gφw(st)|z).\n5:\nUpdate via MLE: φm ←arg maxφ′m\nPN\ni=1 L(φ′\nm; τi, z).\n6:\nObtain training data D := {(s, y := arg maxk qφm(z = k|gφw(s))}.\n7:\nUpdate via supervised learning: φw ←arg maxφ′w\nP\n(s,y)∈D log q(y|gφ′w(s)).\n8: Return: a mixture model qφ(s, z)\nFor updating the encoder weights, we use the default hyperparameter settings as described in [10],\nexcept 1) we modify the neural network architecture, using a smaller neural network, ResNet-10 [29]\nwith a ﬁxed number of ﬁlters (64) for every convolutional layer, and 2) we use number of components\nK = 16, which we did not tune. We tried using a more expressive Gaussian mixture model with full\ncovariances instead of k-means (when training the visual representation), but found that this resulted\nin overﬁtting. Hence, we use k-means until the last iteration of EM, wherein a Gaussian mixture\nmodel is ﬁtted under the resulting visual representation.\nB.5\nEnvironments\nB.5.1\nViZDoom Environment\nFigure 10: Top-down view of VizDoom environment, with initial agent position. White squares depict stationary\nobjects (only relevant to ﬁxed environment).\n17\nThe environment used for visual navigation is a 500x500 room built with ViZDoom [32]. We consider\nboth ﬁxed and random environments; for randomly placing objects, the only constraint enforced is\nthat objects should not be within a minimal distance of one another. There are 50 train objects and 50\ntest objects. The agent’s pose is always initialized to be at the top of the room facing forward. We\nrestrict observations from the environment to be 84 × 84 RGB images. The maximum episode length\nis set to 50 timesteps. The hand-crafted reward function corresponds to the inverse l2 distance from\nthe speciﬁed target object.\nThe environment considered is relatively simple in layout, but compared to simple mazes, can provide\na more complex observation space insofar as objects are constantly viewed from different poses and\nin various combinations, and are often occluded. The underlying ground-truth state space is the\nproduct of continuous 2D position and continuous pose spaces. There are three discrete actions that\ncorrespond to turning right, turning left, and moving forward, allowing translation and rotation in the\npose space that can vary based on position; the result is that the effective visitable set of poses is not\nstrictly limited to a subset of the pose space, despite discretized actions.\nB.5.2\nSawyer Environment\nFigure 11: Third person view of the Sawyer environment\nFor visual manipulation, we use a MuJoCo [60] environment involving a simulated Sawyer 7-DOF\nrobotic arm in front of a table, on top of which is an object. The Sawyer arm is controlled by 2D\ncontinuous control. It is almost identical to the environment used by prior work such as [40], with\nthe exception that our goal space is that of the object position. The robot pose and object are always\ninitialized to the same position at the top of the room facing forward. We restrict observations from\nthe environment to be 84 × 84 RGB images. The maximum episode length is set to 50 timesteps. The\nhand-crafted reward function corresponds to the negative l2 distance from the speciﬁed target object.\nAppendix C\nAdditional Details for Qualitative Study of λ\nC.1\nInstantiating qφ as a VAE\nThree factors motivate the use of a variational auto-encoder (VAE) as a generative model for the\n2D toy environment. First, a key inductive bias of DeepCluster, namely that randomly initialized\nconvolutional neural networks work surprisingly well, which Caron et al. [10] use to motivate its\neffectiveness in visual domains, does not apply for our 2D state space. Second, components of a\nstandard Gaussian mixture model are inappropriate for modeling trajectories involving turns. Third,\nusing a VAE allows sampling from a continuous latent, potentially affording an unbounded number\nof skills.\nWe construct the VAE model in a manner that enables expressive generative densities p(s|z) while\nallowing for computation of the policy reward quantities. We set the VAE latent to be (z, t), where\np(z, t) = p(z)p(t) = N(0, I) 1\nT . The form of p(t) follows from restricting the policy to sampling\ntrajectories of length T. We factorize the posterior as qφ(z, t|st′) = q(z|st′)δ(t −t′). Keeping with\nthe idea of having a Markovian reward, we construct the VAE’s recognition network such that it takes\nas input individual states after training. To incorporate the constraint that all states in a trajectory are\nmapped to the same posterior, we adopt a particular training scheme: we pass in entire trajectories\ns1:T , and specify the posterior parameters as µz = 1\nT\nP\nt gη(st) and σ2\nz = 1\nT\nP\nt gη(st).\nThe ELBO for this model is\nEz,t∼qφ(z,t|st′)\n\u0002\nlog qφ(st′|z, t)\n\u0003\n−DKL(qφ(z, t|st′) ∥p(z, t))\n(35)\n=Ez∼qφ(z|st′)\n\u0002\nlog qφ(st′|z, t′)\n\u0003\n−DKL(qφ(z|st′) ∥p(z)) −C\n(36)\n18\nwhere C is constant with respect to the learnable parameters. The simpliﬁcation directly follows from\nthe form of the posterior; we have essentially passed t′ through the network unchanged. Notice that\nthe computation of the ELBO for a trajectory leverages the conditional independence in our graphical\nmodel.\nC.2\nCARML Details\nSince we are not interested in meta-transfer for this experiment, we simplify the learning problem to\ntraining a contextual policy πθ(a|s, z). To reward the policy using the VAE qφ, we compute\nrz(s) = λ log qφ(s|z) −log qφ(s)\n(37)\nwhere\nlog qφ(s|z) = log\nX\nt\nqφ(s|z, t)p(t) = log 1\nT\nX\nt\nqφ(s|z, t)\n(38)\nand we approximate log qφ(s) by its ELBO (Eq. 36), substituting the above expression for the\nreconstruction term.\nAppendix D\nSawyer Task Distribution\nVisualizing the components of the acquired task distribution for the Sawyer domain reveals structure\nand diversity related to the position of the object as well as the control path taken to effect movement.\nRed encodes the true position of the object, and light blue that of the end-effector. We ﬁnd tasks\ncorresponding to moving the object to various locations in the environment, as well as tasks that\ncorrespond to moving the arm in a certain way without object interaction. The tasks provide a scaffold\nfor learning to move the object to various regions of the reachable state space.\nSince the Sawyer domain is less visually rich than the VizDoom domain, there may be less visually\ndiscriminative states that align with semantics of test task distributions. Moreover, since a large part\nof the observation is proprioceptive, the discriminative clustering representation used for density\nmodeling captures various proprioceptive features that may not involve object interaction. The\nconsequences are two-fold: 1) the gap in the CARML and the object-centric test task distributions\nmay be large, and 2) the CARML tasks may be too diverse in-so-far as tasks share less structure, and\ninferring each task involves a different control problem.\nFigure 12: Skill Maps for Visuomotor Control. Red encodes the true position of the object, and light blue that of\nthe end-effector. The tasks provide a scaffold for learning to move the object to various regions of the reachable\nstate space.\n19\nAppendix E\nMode Collapse in the Task Distribution\nHere, we present visualizations of the task distributions induced by variants of the presented method,\nto illustrate the issue of using an entirely discrimination-based task acquisition approach. Using the\nﬁxed VizDoom setting, we compare:\n(i) CARML, the proposed method\n(ii) online discriminator – task acquisition with a purely discriminative qφ (akin to an online,\npixel-observation-based adaptation of [25]);\n(iii) online pretrained-discriminator – task acquisition with a discriminative qφ as in (ii),\ninitialized with pre-trained observation encoder.\nFor all discriminative variants, we found it crucial to use a temperature ≥3 to soften the classiﬁer\nsoftmax to prevent immediate task mode-collapse.\n(i) CARML (ours)\n(ii) online discriminator [25]\n(iii) pretrained online\ndiscriminator\nWe ﬁnd the task acquisition of purely discriminative variants (ii, iii) to suffer from an effect akin\nto mode-collapse; the policy’s data distribution collapses to a smaller subset of the trajectory space\n(one or two modes), and tasks correspond to minor variations of these modes. Skill acquisition\nmethods such as DIAYN rely purely on discriminability of states/trajectories under skills, which\ncan be more easily satisﬁed in high-dimensional observation spaces and can thus lead to such mode-\ncollapse. Moreover, they do not a provide a direct mechanism for furthering exploration once skills\nare discriminable.\nOn the other hand, the proposed task acquisition approach (Algorithm 2, §3.2) ﬁts a generative model\nover jointly learned discriminative features, and is thus not only less susceptible to mode-collapse\n(w.r.t the policy data distribution), but also allows for density-based exploration (§3.3). Indeed, we\nﬁnd that (iii) seems to mitigate mode-collapse – beneﬁting from a pretrained encoder from (i) – but\ndoes not entirely prevent it. As shown in the main text (Figure 6c), in terms of meta-transfer to\nhand-crafted test tasks, the online discriminative variants (ii, iii) perform worse than CARML (i), due\nto lesser diversity in the task distribution.\n20\nAppendix F\nEvolution of Task Distribution\nHere we consider the evolution of the task distribution in the Random VizDoom environment. The\ninitial tasks (referred to as CARML It. 1) are produced by ﬁtting our deep mixture model to data\nfrom a randomly-initialized meta-policy. CARML Its. 2 and 3 correspond to the task distribution\nafter the ﬁrst and second CARML E-steps, respectively.\nWe see that the initial tasks tend to be less structured, in so far as the components appear to be noisier\nand less distinct. With each E-step we see reﬁnement of certain tasks as well as the emergence of\nothers, as the agent’s data distribution is shifted by 1) learning the learnable tasks in the current\ndata-distribution, and 2) exploration. In particular, tasks that are \"reﬁned\" tend to correspond to\nmore simple, exploitative behaviors (i.e. directly heading to an object or a region in the environment,\ntrajectories that are more straight), which may not require exploration to discover. On the other\nhand, the emergent tasks seem to reﬂect exploration strategies (i.e. sweeping the space in an efﬁcient\nmanner). We also see the beneﬁt of reorganization that comes from reﬁtting the mixture model, as\ntasks that were once separate can be combined.\nIteration 1\nIteration 3\nIteration 5\nFigure 14: Evolution of the CARML task distribution over 3 iterations of ﬁtting qφ in the random ViZDoom\nvisual navigation environment. We observe evidence of task reﬁnement and incorporation of new tasks.\n21\n",
  "categories": [
    "cs.AI",
    "cs.LG"
  ],
  "published": "2019-12-09",
  "updated": "2019-12-09"
}