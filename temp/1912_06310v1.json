{
  "id": "http://arxiv.org/abs/1912.06310v1",
  "title": "Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning",
  "authors": [
    "Shuai Lü",
    "Shuai Han",
    "Wenbo Zhou",
    "Junwei Zhang"
  ],
  "abstract": "Reinforcement learning, evolutionary algorithms and imitation learning are\nthree principal methods to deal with continuous control tasks. Reinforcement\nlearning is sample efficient, yet sensitive to hyper-parameters setting and\nneeds efficient exploration; Evolutionary algorithms are stable, but with low\nsample efficiency; Imitation learning is both sample efficient and stable,\nhowever it requires the guidance of expert data. In this paper, we propose\nRecruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning,\na scalable framework that combines advantages of the three methods mentioned\nabove. The core of this framework is a dual-actors and single critic\nreinforcement learning agent. This agent can recruit high-fitness actors from\nthe population of evolutionary algorithms, which instructs itself to learn from\nexperience replay buffer. At the same time, low-fitness actors in the\nevolutionary population can imitate behavior patterns of the reinforcement\nlearning agent and improve their adaptability. Reinforcement and imitation\nlearners in this framework can be replaced with any off-policy actor-critic\nreinforcement learner or data-driven imitation learner. We evaluate RIM on a\nseries of benchmarks for continuous control tasks in Mujoco. The experimental\nresults show that RIM outperforms prior evolutionary or reinforcement learning\nmethods. The performance of RIM's components is significantly better than\ncomponents of previous evolutionary reinforcement learning algorithm, and the\nrecruitment using soft update enables reinforcement learning agent to learn\nfaster than that using hard update.",
  "text": "Recruitment-imitation Mechanism for Evolutionary\nReinforcement Learning\nShuai L¨ua,b,∗, Shuai Hana,b, Wenbo Zhoua,b, Junwei Zhanga,b\naKey Laboratory of Symbolic Computation and Knowledge Engineering (Jilin University), Ministry of\nEducation, Changchun 130012, China\nbCollege of Computer Science and Technology, Jilin University, Changchun 130012, China\nAbstract\nReinforcement learning, evolutionary algorithms and imitation learning are three prin-\ncipal methods to deal with continuous control tasks. Reinforcement learning is sam-\nple efﬁcient, yet sensitive to hyper-parameters setting and needs efﬁcient exploration;\nEvolutionary algorithms are stable, but with low sample efﬁciency; Imitation learning\nis both sample efﬁcient and stable, however it requires the guidance of expert data.\nIn this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary\nreinforcement learning, a scalable framework that combines advantages of the three\nmethods mentioned above. The core of this framework is a dual-actors and single critic\nreinforcement learning agent. This agent can recruit high-ﬁtness actors from the popu-\nlation of evolutionary algorithms, which instructs itself to learn from experience replay\nbuffer. At the same time, low-ﬁtness actors in the evolutionary population can imitate\nbehavior patterns of the reinforcement learning agent and improve their adaptability.\nReinforcement and imitation learners in this framework can be replaced with any off-\npolicy actor-critic reinforcement learner or data-driven imitation learner. We evaluate\nRIM on a series of benchmarks for continuous control tasks in Mujoco. The experi-\nmental results show that RIM outperforms prior evolutionary or reinforcement learning\nmethods. The performance of RIM’s components is signiﬁcantly better than compo-\nnents of previous evolutionary reinforcement learning algorithm, and the recruitment\nusing soft update enables reinforcement learning agent to learn faster than that using\n∗Corresponding author\nEmail address: lus@jlu.edu.cn (Shuai L¨u)\nPreprint submitted to Journal of Information Sciences\nDecember 16, 2019\narXiv:1912.06310v1  [cs.LG]  13 Dec 2019\nhard update.\nKeywords: evolutionary reinforcement learning, reinforcement learning, evolutionary\nalgorithms, imitation learning\n1. Introduction\nAn important goal of artiﬁcial intelligence is to develop agents with excellent decision-\nmaking capabilities in complex and uncertain environments. In recent years, the rapid\ndevelopment of deep neural network enables agents based on reinforcement learning\nmethods to perform well in complex control tasks [1] [2] [3]. However, reinforcement\nlearning methods cannot always handle reward sparse problems effectively and their\nparameters are very sensitive to disturbances. Recent studies have shown that evolu-\ntionary algorithms are better at dealing with sparse reward environments [4], and can\nbe employed as an extensible alternative to reinforcement learning in various tasks.\nWhen a speciﬁc control task has expert data as a guide, imitative learning can also train\nagents efﬁciently. Currently, imitative learning-based methods have been successfully\napplied to drones [5], automated driving [6], and other ﬁelds.\nThese artiﬁcial intelligence algorithms have close relationships with principles of\nbiology [7]. Evolutionary algorithms are inspired by evolution of species. In each gen-\neration of the population, there are plenty of random mutations in different individuals.\nIndividuals with positive mutations will be selected by the environment. In imitative\nlearning, individuals with poor ﬁtness can learn behavior patterns of individuals with\ngood ﬁtness in a supervised learning way. At the same time, the diversity of genes\n(parameters) is preserved. The biological basis of reinforcement learning is that indi-\nviduals learn correct or wrong actions through trials and errors during interactions with\nthe environment. Off-policy reinforcement learning approaches allow individuals to\nlearn and update their policies from historical data [8] [1]. If reinforcement learning\nindividuals are allowed to learn from the entire historical interactions between popu-\nlation and environment, and regularly copy reinforcement learning individuals into the\npopulation to participate in the evolution, both the learning process of reinforcement\nlearning and the evolutionary process of evolutionary algorithms can be accelerated [9]\n2\nFigure 1: Framework of RIM for evolutionary reinforcement learning\n[10]. However, there are still two problems.\n• Reinforcement learning agents can only learn from experience, but not directly\naccept the guidance of elites in the current population..\n• Reinforcement learning agents must have the same structure as individuals in the\npopulation, or at least have a similar parameterized form. Otherwise, reinforce-\nment learning individuals that are copied into the population may not be able to\nparticipate in the evolutionary process.\nIn response to the above problems, this paper proposes Recruitment-imitation Mech-\nanism (RIM) for evolutionary reinforcement learning. The framework of RIM for evo-\nlutionary reinforcement learning is presented in Figure 1. The recruitment process\nallows reinforcement learning (RL) agent to recruit the best individual from popula-\ntion to participate in RL agent decision and learning process. This requires RL agent\nto have the structure of Figure 2. In Figure 2, gradient policy network and recruit-\nment policy network simultaneously accept the current state as input, and respectively\nproduce actions as output. Critic network compares the potential rewards of actions\nproduced by the two policy networks, and outputs the action with higher reward. The\ndual-policy decision-making mechanism not only enables the RL agent to produce bet-\nter experiences, but also enables the RL agent to better estimate Q value, which enables\nthe learning process of RL agent to directly accept the guidance of excellent policy in\n3\nFigure 2: Structure of dual policy RL agent\nthe population. The imitation process permits low-ﬁtness individuals in the population\nto learn behavioral patterns of RL agent. Since the structure of RL agent is inconsistent\nwith that of individuals in the population, RL agent cannot be directly injected into\nthe population and participate in the evolution. The imitation process is designed for\naddressing this problem.\nThe main contributions of this paper are as follows:\n• We propose Recruitment-imitation Mechanism (RIM) and an evolutionary rein-\nforcement learning framework that applies this mechanism. At the core of RIM,\na dual-policy RL structure is designed, which allows critic network to determines\nactions.\n• We present a series of optimization techniques for RIM, including an off-policy\nimitation learning algorithm that directly uses experience replay buffer and soft\nupdating strategies for recruiting networks.\n• We compare the performance of RIM with that of previous algorithms on Mujoco\n4\nbenchmarks. Moreover, we discuss the impact of optimization techniques on\nRIMs performance.\nThe rest of this paper is organized as follows. Section 2 brieﬂy introduces the\nbackground. Section 3 presents the proposed recruitment-imitation mechanism in de-\ntail. Section 4 shows the experimental results and comparisons. Section 5 discusses\nadvantages of RIM and outlines the future work.\n2. Related Work\nCombining reinforcement learning with imitation learning or evolutionary algo-\nrithms is not a new idea [11] [12] [13] [14]. The traditional combination methods\nmostly consider imitation learning or evolutionary algorithms as a sub-process of rein-\nforcement learning. Typical practices in such methods include: leveraging the distri-\nbution estimation method to improve the exploration noise in reinforcement learning\n[15], utilizing the evolutionary method to optimize the approximation function in Q\nlearning [16], or using the imitation learning to optimize the initial stage of the RL pro-\ncess [17]. Unlike these traditional practices, evolution and imitation learning in RIM\nare not sub-processes of reinforcement learning. In RIM, evolution and reinforcement\nlearning are two relatively independent learning processes, while imitation learning is\na way of synchronizing the behavioral policy of dual-policy agents into populations.\nEvolutionary Reinforcement Learning (ERL) [9] provides a new paradigm for the\ncombination of evolutionary algorithms and reinforcement learning. ERL’s approach is\nto reuse the interaction data between the population and the environment, and inject RL\npolicy into the population. RIM can be viewed as an extension of ERL. The expanded\nparts include:\n1) Combining evolutionary algorithms from the perspective of reinforcement learn-\ning. Namely, recruiting policy directly from the population to participate in reinforce-\nment learning processes;\n2) Constructing a dual-policy agent in the RL component. The agent can integrate\ntwo policies to generate experience, and give better estimation of Q value when policy\nlearning is insufﬁcient;\n5\n3) Leveraging off-policy imitation learning to synchronize the behavioral policy\nof dual-policy agent into the population. By the way, the RL agent does not have to\nmaintain the same structure as the individuals in the population.\nIn the latest work, there are other ways to extend ERL: Collaborative Evolution-\nary Reinforcement Learning (CERL) [18] uses different combinations of learners to\ncomplete exploration in different time-horizons of tasks; Cross-entropy Method Rein-\nforcement Learning (CEM-RL) [10] replaces the combination of standard evolution-\nary algorithm and DDPG [1] in ERL with a combination of Cross-entropy Method\n(CEM) and Twin Delayed Deep Deterministic policy gradient (TD3) [19]. Unlike the\nabove works, RIM is neither an optimization for task exploration nor a replacement of\nsub-components in ERL. Instead, it introduces a recruitment imitation mechanism that\nenables reinforcement learning, imitation learning, and evolutionary algorithms to be\nmore effectively combined. In other words, RIM, CERL and CEM-RL are independent\noptimizations in different directions of ERL.\n3. Background\nThis section introduces the background of deep reinforcement learning, evolution-\nary algorithms and imitation learning.\n3.1. Deep reinforcement learning\nReinforcement learning methods abstract the interaction between an agent and en-\nvironments into a Markov decision process. At each discrete time step t, the environ-\nment provides an observation st to the agent, and the agent takes an action at as a\nresponse to this observation. Then, the environment returns a reward rt and the next\nstate st+1 to the agent. The goal of reinforcement learning is to maximize the expec-\ntation of Rt = PK\nk=0 γkrt+k, where Rt indicates the sum of cumulative discounted\nrewards of K steps from the current moment. γ ∈(0, 1] is a discount factor.\nDeep deterministic policy gradient (DDPG) is a widely used model-free reinforce-\nment learning algorithm based on actor-critic structure [1]. In DDPG, actor and critic\nare parameterized as π(s|θπ) and Q(s, a|θQ) respectively. They are called current net-\nworks. In addition, replications of actor and critic networks π′(s|θπ′) and Q′(s, a|θQ′)\n6\nare used to provide consistent targets for the learning process. They are called target\nnetworks. During the learning process, target networks will be soft updated based on\ncurrent networks and a weighting factor τ. DDPG completes off-policy learning by\nsampling experiences from a repaly buffer B. That is, for each interaction between\nan agent and environments, the tuple (st, at, rt, st+1) is stored into the replay buffer.\nOne advantage of off-policy reinforcement learning is the sample efﬁciency of these\nalgorithms. The other advantage is that learning from historical data can decouple ex-\nploration process and learning process. DDPG’s exploration policy is constructed from\nactor policy and noise: πe(st) = π(st|θπ) + N. The actor and critic networks are\nupdated by sampling mini-batch experiences from the replay buffer. Critic network is\nupdated by minimizing the loss function as follows:\nL = 1\nN\nX\ni\n(y(i)\nt\n−Q(s(i)\nt , a(i)\nt |θQ))2\n(1)\nwhere y(i)\nt\n= r(i)\nt\n+ γQ′(s(i)\nt+1, π′(s(i)\nt+1|θπ′)|θQ′). The actor network is updated ac-\ncording to sampled policy gradient:\n▽θππ|s(i) ≈1\nN\nX\ni\n▽aQ(s(i), a|θQ)|a=π(s(i)) ▽θπ π(s(i)|θπ)\n(2)\n3.2. Evolutionary algorithms\nEvolutionary algorithms (EAs) are black-box optimization algorithms that are in-\nspired by natural evolutionary processes. The evolutionary process acts on a population\ncomposed of several parameterized candidate solutions (individuals). During each it-\neration, parameters of individuals in the population are randomly perturbed (mutated),\nenabling new individuals to be generated. After generating new individuals, the envi-\nronment evaluates their ﬁtness. Individuals with higher ﬁtness have higher probability\nto be selected, and selected individuals will participate in the next iterative process.\nWhen evolutionary algorithms are implemented to solve control tasks, several actor\nnetworks will participate in the evolutionary process. During the iterative process, ac-\ntor networks with higher ﬁtness will be retained as elites and shielded mutation step [9].\nEAs can be employed as an alternative for reinforcement learning techniques based on\n7\nMarkov decision processes [4].\n3.3. Imitation learning\nIn imitation learning, an actor imitates expert behaviors to obtain a policy that is\nclose to expert performance. Dataset aggregation (DAgger) method [20] is a widely\nused imitation learning algorithm. It is an iterative reduction to online policy training\nmethod. Suppose that there is an expert data set D = {s1, a1, ..., sn, an}. During each\niteration, the actor is trained on this dataset in a supervised manner. The loss function\nduring training can be denoted as: J(θ) =\n1\nK\nPK\nk=1 L(πil(sk), ak), where πil is a\npolicy to be trained and K is the batch size of samples. After convergence, policy\nπil is carried out to generate a state access set Dπ = {sn+1, ..., sn+m}. Then Dπ is\nlabeled by actions output from the expert policy, and the expert data is accumulated:\nD ←D ∪Dπ. Then proceed to the next iteration. The advantage of DAgger is that it\ncan employ expert policy to teach actors how to recover from errors. DAgger is a class\nof Follow-The-Leader algorithms [21].\n4. Recruitment-imitation Mechanism\nRecruitment-imitation mechanism in evolutionary reinforcement learning is pre-\nsented in this section.\n4.1. Dual policy reinforcement learning agent\nThe core of recruitment-imitation mechanism is a dual policy RL agent with dual-\nactors and single-critic. In the iterative process of the evolutionary reinforcement learn-\ning algorithm, the dual policy RL agent can recruit excellent individuals from the popu-\nlation to participate in decision-making or reinforcement learning process. In addition,\nindividuals with poor performance in the population can periodically imitate behavior\npatterns of dual policy RL agent to accelerate evolutionary speed of the population.\nDual policy RL agent in recruitment-imitation mechanism still uses the actor-critic\nstructure but has two actor networks, including a gradient policy network πpg(st|θpg)\nand a recruitment policy network πea(st|θea). When the agent makes decisions, the\n8\ncritic network will identify which policy under the current state st is potentially more\nproﬁtable. Therefore, policy of the dual policy RL agent can be indicated as:\nπrl =\n\n\n\n\n\n\n\n\n\nπpg,\nQ(st, πpg(st|θpg)|θQ) ≥Q(st, πea(st|θea)|θQ)\nπea,\notherwise\n(3)\nwhere πrl indicates the overall behavior policy of the dual policy RL agent. Similar\nto DDPG, during the learning process, the critic network is updated by minimizing the\nloss function of (1), except that y(i)\nt\nis estimated using πrl:\ny(i)\nt\n= r(i)\nt\n+ γQ′(s(i)\nt+1, πrl(s(i)\nt+1)|θQ′)\n(4)\nThe gradient policy network is still updated with the sampled policy gradient according\nto equation (2).\nThere are two reasons that the dual policy RL agent can make RIM perform better:\n• The dual policy RL agent can generate better experiences according to the be-\nhavior pattern in equation (3).\n• When gradient update is performed, πrl used in equation (4) can make more\naccurate estimation of Q′.\nThen we prove that equation (4) can give more accurate estimation of Q′.\nTheorem 1. Suppose that Q′ and Q converge at the same position for policy π∗. The\nconverged policy to maximize Q′ or Q is π∗, and the estimation value using π∗is Q∗.\nThe mean of Q′ estimation of DDPG is denoted as Eddpg( ˆQ′) and the mean of Q′\nestimation of RIM is denoted as Erim( ˆQ′). Then, Eddpg( ˆQ′) ≤Erim( ˆQ′) ≤Q∗.\nProof:\nSince π∗is the policy that maximizes Q′ (i.e., Q∗), any policy that is not\nπ∗will cause the estimate of Q′ to be less than Q∗. So, there is Erim( ˆQ′) ≤Q∗.\nErim( ˆQ′) = Q∗if and only if the policy of πrl in equation (3) is always π∗. Also\n9\nthere is Eddpg( ˆQ′) ≤Q∗. Eddpg( ˆQ′) = Q∗if and only if the policy πddpg of DDPG is\nalways π∗.\nAssume that DDPG’s behavior policy πddpg causes the estimation of Q′ to be\nshifted downward by x with the probability of p(x), then Eddpg( ˆQ′) =\nR +∞\n−∞(Q∗−\nx)p(x)dx. Since RIM uses the policy in equation (3) to estimate Q′ in equation (4).\nWhen πpg (i.e., πddpg) estimates Q′ downwards by x with the probability of p(x), πea\ncan upwards correct y (y ≥0) with the probability of q(y|x). Then we have:\nErim( ˆ\nQ′) =\nZ +∞\n−∞\nZ +∞\n−∞\n(Q∗−x + y)q(y|x)p(x)dydx\n=\nZ +∞\n−∞\nZ +∞\n−∞\n(Q∗−x)q(y|x)p(x)dydx +\nZ +∞\n−∞\nZ +∞\n−∞\nyq(y|x)p(x)dydx\n=\nZ +∞\n−∞\n(Q∗−x)(\nZ +∞\n−∞\nq(y|x)dy)p(x)dx +\nZ +∞\n−∞\nZ +∞\n−∞\nyq(y|x)p(x)dydx\n=\nZ +∞\n−∞\n(Q∗−x)p(x)dx +\nZ +∞\n−∞\nZ +∞\n−∞\nyq(y|x)p(x)dydx\n= Eddpg( ˆ\nQ′) +\nZ +∞\n−∞\nZ +∞\n−∞\nyq(y|x)p(x)dydx\nConsidering y ≥0 and q(y|x)p(x) ≥0, then\nR +∞\n−∞\nR +∞\n−∞yq(y|x)p(x)dydx ≥0,\nso it holds that Eddpg( ˆQ′) ≤Erim( ˆQ′).\n■\nIn Theorem 1, we can assume that Q and Q′ converge at the same position because\nQ and Q′ are very close using soft update setup in continuous domains [19]. Theorem\n1 states that the estimation of Q′ using πrim is more accurate than that estimated using\nπddpg, or closer to Q∗. Unlike overestimation problems [22] [23], Theorem 1 states\nthat when πddpg does not converge sufﬁciently during the updating process according\nto equation (2), if the policy πea in population gives better actions, the Q′ estimation\nof RIM will be more accurate. This is one of the reasons the RL component of RIM\ncan learn better.\n4.2. Off-policy imitation learning\nDue to the inconsistencies in structures of a dual policy RL agent (dual-actors and\nsingle-critic) and an individual (single-actor) in the population, the dual policy RL\n10\nFigure 3: Framework of off-policy imitation learning in RIM.\nagent cannot be directly injected into the population to participate in the evolution. In\nthis case, an ideal solution is to use imitation learning to unify behavior patterns of the\nRL agent and individuals in the population.\nAs a typical imitation learning algorithm, DAgger can get better performance with\nonly a few iterations when the amount of data is large. Evolutionary reinforcement\nlearning system usually has a huge experience replay buffer for storing historical data.\nThe actors in the population to be trained can directly sample states and actions in the\nexperience replay buffer and be trained according to the following loss function:\nJ0(θwt) = 1\nK\nK\nX\nk=1\nL(πwt(sk|θwt), πrl(sk))\n(5)\nwhere πwt indicates the policy of an actor to be trained. πwt is usually a policy of\nan actor with the worst performance in the population. Figure 3 shows the frame-\nwork of off-policy imitation learning and Algorithm 1 describes the detailed process\nof off-policy imitation learning in RIM. We cancel the data labeling process and ac-\ncumulation process of DAgger so that the imitation process is off-policy completely.\nThe main function of DAgger’s data labeling process and accumulation process is to\nrecover the imitation learner from errors. In the RIM environment setting, there is a\n11\nAlgorithm 1 Off-policy imitation learning in RIM\n1: Get the worst policy πwt from population\n2: for n = 1 to N do\n3:\nGet replay buffer size: size ←len(B)\n4:\nwhile size > l0 do\n5:\nSample random batch {si} from replay buffer\n6:\nGet awt\ni\naccording to πwt(si|θwt)\n7:\nGet πrl according to equation (3)\n8:\nGet expert action ai according to πrl(si)\n9:\nCalculate loss L between expert action awt\ni\nand agent action ai\n10:\nCalculate ▽θwtL\n11:\nUpdate θwt ←θwt + α · ▽θwtL\n(a) walker2d-v2\n(b) Hopper-v2\nFigure 4: Performance of different individuals in the population.\nlarge amount of erroneous data in the experience replay buffer, and most of the states\nsampled from the experience replay buffer is generated by a suboptimal or wrong pol-\nicy. As shown in Figure 4, the performance of individuals in the population varies\nwidely, and even the interaction information of the worst individual is stored into the\nreplay buffer. Therefore the training process using equation (5) is also a process to\nteach actors how to recover from an error state.\n4.3. Learning and evolution process of RIM\nThe core idea of RIM is to 1) accelerate the learning process by recruiting elite\nindividuals in the population to guide the policy gradient network learning, and 2)\naccelerate the evolutionary process by enabling individuals with poor performance in\nthe population to imitate behavioral patterns of the dual policy RL agent.\n12\nThe process of RIM is as follows:\n1) Some actor networks in the population and actor and critic networks in the dual\npolicy agent are initialized with random weights.\n2) In the evolution process, the environment evaluates the ﬁtness of actors in the\npopulation and select a part of the actors to survive with a probability according to the\nﬁtness.\n3) The actors are perturbed by mutation and crossover to generate the next genera-\ntion of actors.\n4) After generating the next generation, the dual policy RL agent recruits an actor\nwith the highest ﬁtness in the population, copies its parameters to the recruitment policy\nnetwork, and learns for a certain number of times.\nAlgorithm 2 describes the detailed learning process of dual policy RL agent. The\nworst actor in the population imitates behavioral patterns of the dual policy RL agent\nevery several generations so that the learning outcomes of the dual policy RL agent can\nbe injected into the population.\nDuring the evolutionary process, interaction information between actors and envi-\nronments will be stored into the experience replay buffer. These experiences are not\nonly used for the sampling process of gradient policy network learning in Algorithm 2,\nbut also for the sampling process of the worst actor imitating in population. In the RIM\nsetting of this paper, the structure of gradient policy network is same as that of actors\nin the population, so when the imitation learning is not performed, the gradient policy\nnetwork will be periodically copied into the population to accelerate the evolutionary\nprocess.\nThe recruitment policy network participates in the decision process of πrl. There-\nfore, it affects the value of y(i)\nt . The learning process of critic network needs to be\nprovided with a consistent y(i)\nt . Drawing on the idea of soft update in DDPG, we soft\nupdate the target gradient policy network and critic network. In order to further en-\nsure consistency in the learning process, we decide to add a target recruitment policy\nnetwork. Current recruitment policy network πea(st|θea) uses hard update to copy di-\nrectly from the population, and the target recruitment policy network uses soft update\n13\nAlgorithm 2 Learning process of RL agent in RIM\n1: Randomly initialize critic Q(s, a|θQ), RL actor πpg(s|θpg) and EA actor\nπea(s|θea)\n2: Initialize target network Q′, π′\npg, π′\nea with θQ′ ←θQ, θpg′ ←θpg, θea′ ←θea\n3: Initialize replay buffer B, environment Env\n4: for episode = 1 to M do\n5:\nRecruit champion from populations: θea ←θchamp\n6:\nReset Env and get a state s\n7:\ndone ←False\n8:\nwhile !done do\n9:\nπ(s) ←maxπ{Q(s, πpg(s)), Q(s, πea(s))}\n10:\nGet action a according to π(s) + N\n11:\nExecute a and observe (r, s′, done) ∼Env\n12:\nStore experience (s, a, r, s′) into replay buffer B\n13:\nUpdate state: s ←s′\n14:\nif len(B) > l0 then\n15:\nSample random batch {(st, at, rt, st+1)i} from replay buffer\n16:\nQ′(s(i)\nt+1, a(i)\nt+1|θQ′) ←max{Q′(s(i)\nt+1, π′\npg(s(i)\nt+1)|θQ′), Q′(s(i)\nt+1, π′\nea(s(i)\nt+1)|θQ′)}\n17:\nLQ ←1\nN\nP\ni(r(i)\nt\n+ Q′(s(i)\nt+1, a(i)\nt+1|θQ′) −Q(s(i)\nt , a(i)\nt |θQ))2\n18:\nLπ ←−1\nN\nP\ni Q(s(i)\nt , πpg(s(i)\nt |θpg)|θQ)\n19:\nUpdate θQ, θpg according to ▽θQLQ, ▽θpgLπ\n20:\nθea′ ←(1 −τ0)θea′ + τ0θea\n21:\nθQ′ ←(1 −τ1)θQ′ + τ1θQ\n22:\nθpg′ ←(1 −τ1)θpg′ + τ1θpg\nto update parameters:\nθea′ ←(1 −τ0)θea′ + τ0θea\n(6)\nwhere τ0 should be much less than 1. The recruitment process using target recruitment\nnetwork is called soft update in recruitment, and the recruitment process without target\nrecruitment network is called hard update in recruitment. We will perform comparative\nexperiments on these two recruitment models in the experimental part.\n5. Experiments\nIn this section, we show comparative experiments and a series of analytical results.\n14\n5.1. Experimental settings\nWe evaluated the performance of RIM on four continuous control tasks in Mujoco\n[24] hosted through the OpenAI gym [25], which are widely used in the evaluation of\nreinforcement learning continuous domains [26] [27] [28]. These tasks are shown in\nFigure 5.\n(a) Walker2d-v2\n(b) Hopper-v2\n(c) HalfCheetah-v2\n(d) Swimmer-v2\nFigure 5: Four continuous control tasks in Mujoco-based environments\nBaselines for comparison include Evolutionary Reinforcement Learning (ERL) al-\ngorithm [9], DDPG, and a standard evolutionary algorithm (EA). ERL is a state of the\nart algorithm in the Mujoco benchmarks; DDPG is considered as one of the best re-\ninforcement learning algorithms and is widely used in a variety of continuous control\ntasks.\nThe reinforcement learning parameters of RIM are consistent with those of ERL\nand DDPG. The crossover and mutation probability is 0 and 0.01 respectively in pop-\nulation of RIM, ERL and EA. In the comparative experiments, RIM uses soft update\n15\nto recruit policies from population. All actor networks and critic networks have two\nhidden layers, with rectiﬁed linear units (ReLU) between hidden layers. The last layer\nof actor networks is linked to a tanh unit. We use Adam [29] to update all network\nparameters. The learning rates of the gradient policy network and the critic network\nare 10−4 and 10−3, respectively. The number of individuals in the population is 10.\nImitation learning is performed every 10 generations. The loss function of the imitation\nlearning is least absolute error (L1), and the learning rate is 10−3. The sample batch\nsize of reinforcement learning and imitation learning are 128 and 32, respectively.\nThe performance of the algorithms involved in the comparison is tested using dif-\nferent random seeds and is recorded every 10,000 frames. When testing, the average of\nﬁve test results is recorded as the performance at this time. For DDPG, the actor’s ex-\nploration noise was removed during the test. For EA, ERL and RIM, the performance\nof the best individuals in their population are recorded as the ﬁnal performance. The\nscores of these algorithms are the rewards returned by environments, and the corre-\nsponding number of frames is the cumulative number of frames in which an algorithm\ninteracts with the environment as a whole. These recording methods are consistent\nwith previous comparative experiments [9].\n5.2. Comparison\nThe results in Table 1 and Figure 6 show the ﬁnal performance and learning curves\nof the four algorithms on Mujoco-based continuous control benchmarks. Each algo-\nrithm is trained for 5M frames in Walker2d-v2, 2.5M frames in Hopper-v2, and 1M\nframes in HalfCheetah-v2 and Swimmer-v2. Table 1 presents the maximum (Max) re-\nward obtained by each algorithm in the whole learning process, as well as the average\n(Mean), median (Median) and standard deviation (Std.) of 5 test results of each algo-\nrithm under different random seeds. The best statistical results have been bolded in\neach task. Figure 6 presents the average proformance and error bars, which have been\nsmoothed using a window of size 10.\nTable 1 shows that the average performance of RIM exceeds the previous methods\nin all environments, and the maximum reward obtained in the whole learning process\nand median performance exceed the previous methods in most environments. Both\n16\nTable 1: Final performance of EA, DDPG, ERL and RIM on 4 Mujoco-based continuous control benchmarks\nWalker2d-v2\nHopper-v2\nHalfcheetah-v2\nSwimmer-v2\nEA\nMax\n1339.11\n1051.19\n1755.27\n356.01\nMean\n1143.68\n1032.92\n920.60\n239.04\nMedian\n1073.38\n1027.82\n864.62\n301.93\nStd.\n11.37%\n0.08%\n65.49%\n43.21%\nDDPG\nMax\n3184.08\n3575.19\n6012.01\n55.32\nMean\n543.94\n484.29\n2601.53\n26.78\nMedian\n492.85\n590.66\n2796.43\n29.25\nStd.\n39.77%\n67.12%\n60.66%\n27.86%\nERL\nMax\n4472.21\n2392.49\n5597.36\n332.69\nMean\n1375.24\n1780.90\n5098.46\n231.38\nMedian\n1476.77\n1824.88\n5064.25\n222.56\nStd.\n23.51%\n30.23%\n5.54%\n21.37%\nRIM(ours)\nMax\n5532.66\n3439.74\n6151.81\n343.62\nMean\n2852.08\n2385.05\n5399.68\n297.43\nMedian\n3242.61\n2386.84\n5367.22\n287.49\nStd.\n26.66%\n19.66%\n6.94%\n9.78%\nRIM and ERL have higher variances on different random seeds because the compo-\nnents, such as EA and DDPG, have high variances. However, the standard deviation of\nRIM performance is much lower than ERL in Hopper-v2 and Swimmer-v2.\nThe experimental results in Figure 6 show that RIM can learn better than the pre-\nvious algorithms in four tasks. In Walker2d-v2 and Hopper-v2 environments, the su-\nperiority of RIM is more signiﬁcant, because the traditional off-policy reinforcement\nlearning method cannot explore efﬁciently in such environments. EA has a stagnation\nperiod in the evolution process. Off-policy reinforcement learning individuals in ERL\ncan help the evolutionary population to break through the stagnation period, but this\noften requires superior individuals in the population to generate a large amount of ex-\nperiences. The recruitment mechanism of RIM enables outstanding individuals of EA\nto directly guide reinforcement learning individuals, instead of guiding them by gener-\nating experience. Therefore, RIM can break through the stagnation period earlier than\nERL, which allows RIM to learn faster.\nWe reimplemented the EA, DDPG and ERL algorithms. The ERL results we pre-\nsented in Walker2d and Hopper environments are consistent with Khadka et al. [9].\n17\n(a) Walker2d-v2\n(b) Hopper-v2\n(c) HalfCheetah-v2\n(d) Swimmer-v2\nFigure 6: Learning curves of EA, DDPG, ERL and RIM on 4 Mujoco-based continuous control benchmarks\nThe results in the Halfcheetah environment are lower than Khadka et al., but close to\nthose reported by Pourchot et al. [10]. In addition, we found that in the Swimmer\nenvironment, the performance of EA, ERL and RIM is unstable. In the best case, they\ncan reach 330 or more, but in the worst case, they perform less than 250, even less than\n150 in EA.\n5.3. Component performance\nFigure 7 shows performance of the three components in Walker2d and Hopper\nenvironments: RL components of RIM, imitation learning (IL) actors in RIM, and RL\nagent in ERL. The performance of RL agent in RIM is better than RL agent in ERL,\nwhich means that the recruitment mechanism can accelerate the learning process of RL\nactors. The performance of individuals trained by imitation learning in RIM improves\n18\nas the performance of the RL components improves, which illustrates the effectiveness\nof off-policy imitation learning.\n(a) Walker2d-v2\n(b) Hopper-v2\nFigure 7: Performance of components of different algorithms\nThe externally injected individuals accepted by RIM’s population are imitation\nlearning individuals, while the externally injected individuals accepted by the ERL’s\npopulation are RL actors. In Figure 7, the performance of imitation learner in RIM\noutperforms RL agent in ERL, suggesting that externally injected individuals of the\npopulation of RIM are better. This can directly explain why the learning speed of RIM\nis better than that of ERL.\nTable 2 presents the selection rate of actors trained through imitation learning dur-\ning evolution. When the population selects excellent individuals, πwt with higher per-\nTable 2: Selection rate for πwt after training\nSelected\nDiscarded\nWalker2d-v2\n59.87%\n40.13%\nHopper-v2\n30.92%\n69.08%\nHalfCheetah-v2\n70.37%\n29.63%\nSwimmer-v2\n24.44%\n75.56%\nformance has higher probability to be selected. On the contrary, πwt with lower perfor-\nmance will have a higher probability of being discarded. The RL components performs\nbetter in Walker2d and Halfcheetah, so the selected probability of well-learned πwt is\nhigher. In Hopper environment, the RL components performs poorly before breaking\n19\n(a) HalfCheetah-v2\n(b) Hopper-v2\nFigure 8: Comparison curves between soft update recruitment and hard update recruitment\nthrough the long stagnation period. In the Swimmer environment, the RL components\ncannot play a critical role in the whole learning process, which can be seen from curve\nof DDPG in Figure 6. These reasons cause πwt cannot imitate a good policy, so the\nprobability that πwt is selected is lower. On the whole, individuals performing imita-\ntion learning can be selected in each environment, indicating that the policy learned by\nimitation learning can accelerate the evolution of the population.\n5.4. Soft update in recruitment\nWe tested the performance of RIM using hard update, and compared it with the\noriginal RIM. As shown in Figure 8, RIM using soft update recruitment performed\nslightly better than RIM using hard update recruitment. Because in the process of\nguiding the gradient policy network learning, the soft update recruitment policy net-\nwork can provide more stable y(i) when calculating equation (4), which ensures better\nconsistency for learning process. However, recruitment using hard update does not\nmake the policy gradient network unable to learn. In fact, as the population iterates,\nthe changes of parameters between generations are usually small. Therefore, parame-\nter changes of the recruitment policy network are small, which can also bring a certain\ndegree of consistency to the learning process.\n20\nTable 3: Final performance of ERL, RIM and variants of RIM on 4 Mujoco-based continuous control bench-\nmarks\nWalker2d-v2\nHopper-v2\nHalfcheetah-v2\nSwimmer-v2\nRIM\nMean\n2852.08\n2385.05\n5399.68\n297.43\nMedian\n3242.61\n2386.84\n5367.22\n287.49\nStd.\n26.66%\n19.66%\n6.94%\n9.78%\nRIM-IL\nMean\n2075.33\n1574.41\n5229.43\n269.68\nMedian\n1937.64\n1095.01\n5253.77\n267.89\nStd.\n26.43%\n56.14%\n7.5%\n9.45%\nRIM-EA\nMean\n1948.16\n1753.66\n5001.21\n258.96\nMedian\n2018.73\n1749.26\n4923.92\n261.63\nStd.\n49.10%\n29.14%\n7.67%\n20.63%\nRIM-PG\nMean\n1777.30\n1141.54\n5142.67\n222.60\nMedian\n1208.66\n1078.56\n5197.68\n209.84\nStd.\n54.97%\n15.23%\n13.96%\n32.61%\nERL\nMean\n1375.24\n1780.90\n5098.46\n231.38\nMedian\n1476.77\n1824.88\n5064.25\n222.56\nStd.\n23.51%\n30.23%\n5.54%\n21.37%\n5.5. Ablation experiments\nIn order to further prove the effectiveness of the RIM components, we performed\nablation experiments on RIM. We tested the performance of RIM without ofﬂine im-\nitation learning (RIM-IL), RIM without using the recruitment network to estimate Q′\n(RIM-EA), and RIM without using the gradient policy network to estimate Q′ (RIM-\nPG). We compare the test performance of these three RIM variants with RIM as well\nas ERL. The comparison results are presented in Table 3 and Figure 9.\nAccording to the results in Table 3 and Figure 9, RIM outperforms RIM-IL, which\nshows that imitation learning works. We believe that imitation learning can inject in-\ndividuals with dual-policy RL agent’s behavior patterns into the population, thereby\naccelerating the evolution of the population. Performance of RIM is better than that\nof RIM-EA and RIM-PG, which shows that the dual-policy learning mode can learn\nbetter policy. This stems from the fact that the dual-policy can better estimate the Q′\nvalue, as revealed in Theorem 1.\nTo further illustrate the effectiveness of the dual-policy learning model in evolu-\ntionary reinforcement learning algorithms, we compared learning curves of RL com-\n21\n(a) Walker2d-v2\n(b) Hopper-v2\n(c) HalfCheetah-v2\n(d) Swimmer-v2\nFigure 9: Learning curves of ERL, RIM and variants of RIM on 4 Mujoco-based continuous control bench-\nmarks\n22\n(a) Wakler2d-v2\n(b) Hopper-v2\nFigure 10: Learning curves of RL components in RIM, RIM-PG, and RIM-EA\nponents in RIM, RIM-EA, and RIM-PG algorithms. These curves are shown in Figure\n10. RIM’s RL policy is better than RIM-EA, which indicates that RIM estimates the Q′\nvalue better than DDPG. RIM-PG’s RL components performs poorly. This results from\nthe fact that the Q′ estimated by the recruitment network is not accurate in Walker2d-v2\nand Hopper-v2.\n6. Conclusion and Future Work\nIn this paper, we develop a dual-actors and single critic RL agent, which can be well\ncombined with evolutionary algorithms. Based on this, we propose RIM for evolution-\nary reinforcement learning. This method not only outperforms the previous evolution-\nary and off-policy reinforcement learning algorithms, but also exceeds ERL both in\noverall performance and component performance. Finally, we experimentally demon-\nstrated that the recruitment using soft update enables RL agent to learn faster than that\nusing hard update.\nFuture work can be explored from the following three aspects:\n• RIM uses a standard evolutionary algorithm to iterate populations. Other more\nefﬁcient evolutionary algorithms may provide immediate improvements to RIM’s\nperformance, such as Natural Evolution Strategy (NES) [30], NeuroEvolution of\nAugmenting Topologies (NEAT) [31].\n23\n• The dual-actors and single critic RL agent in RIM is extended based on DDPG,\nbut in fact, any off-policy actor-critic deep reinforcement learning method can\napply this structure, such as Soft Actor-Critic (SAC) [32], Twin Delayed Deep\nDeterministic policy gradient algorithm (TD3) [19]. Since these methods are\nsuperior to DDPG in most cases, applying these methods to the dual-actors and\nsingle critic RL agent may improve RIM’s performance.\n• In RIM’s experimental settings, at each time 100,000 reinforcement learning gra-\ndient updates are performed, then 30,000 imitation learning gradient updates are\nrequired. Parallel methods can be employed to reduce time for training models.\nAcknowledgement\nThis work was supported by the National Key R&D Program of China under Grant\nNo. 2017YFB1003103; the National Natural Science Foundation of China under Grant\nNos. 61300049, 61763003; and the Natural Science Research Foundation of Jilin\nProvince of China under Grant Nos. 20180101053JC, 20190201193JC.\nReferences\nReferences\n[1] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver,\nD. Wierstra, Continuous control with deep reinforcement learning, arXiv preprint\narXiv:1509.02971.\n[2] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,\nK. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, in: In-\nternational Conference on Machine Learning, 2016, pp. 1928–1937.\n[3] J. Schulman, S. Levine, P. Abbeel, M. Jordan, P. Moritz, Trust region policy\noptimization, in: International Conference on Machine Learning, 2015, pp. 1889–\n1897.\n24\n[4] T. Salimans, J. Ho, X. Chen, S. Sidor, I. Sutskever, Evolution strategies as a\nscalable alternative to reinforcement learning, arXiv preprint arXiv:1703.03864.\n[5] A. Giusti, J. Guzzi, D. C. Cires¸an, F.-L. He, J. P. Rodr´ıguez, F. Fontana,\nM. Faessler, C. Forster, J. Schmidhuber, G. Di Caro, et al., A machine learning\napproach to visual perception of forest trails for mobile robots, IEEE Robotics\nand Automation Letters 1 (2) (2016) 661–667.\n[6] F. Codevilla, M. Miiller, A. L´opez, V. Koltun, A. Dosovitskiy, End-to-end driving\nvia conditional imitation learning, in: 2018 IEEE International Conference on\nRobotics and Automation (ICRA), IEEE, 2018, pp. 1–9.\n[7] S. Zhang, O. R. Zaiane, Comparing deep reinforcement learning and evolutionary\nmethods in continuous control, arXiv preprint arXiv:1712.00006.\n[8] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,\nA. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., Human-level\ncontrol through deep reinforcement learning, Nature 518 (7540) (2015) 529–533.\n[9] S. Khadka, K. Tumer, Evolution-guided policy gradient in reinforcement learn-\ning, in: Advances in Neural Information Processing Systems, 2018, pp. 1196–\n1208.\n[10] A. Pourchot, O. Sigaud, Cem-rl: Combining evolutionary and gradient-based\nmethods for policy search, arXiv preprint arXiv:1810.01222.\n[11] S. Ross, J. A. Bagnell, Reinforcement and imitation learning via interactive no-\nregret learning, arXiv preprint arXiv:1406.5979.\n[12] E. Uchibe, Cooperative and competitive reinforcement and imitation learning\nfor a mixture of heterogeneous learning modules, Frontiers in neurorobotics 12\n(2018) 61.\n[13] D. V. Vargas, Evolutionary reinforcement learning: general models and adapta-\ntion., in: GECCO (Companion), 2018, pp. 1017–1038.\n25\n[14] M. M. Drugan, Reinforcement learning versus evolutionary computation: A sur-\nvey on hybrid algorithms, Swarm and evolutionary computation 44 (2019) 228–\n246.\n[15] H. Tan, K. Balajee, D. Lynn, Integration of evolutionary computing and rein-\nforcement learning for robotic imitation learning, in: 2014 IEEE International\nConference on Systems, Man, and Cybernetics (SMC), IEEE, 2014, pp. 407–412.\n[16] S. Whiteson, P. Stone, Evolutionary function approximation for reinforcement\nlearning, Journal of Machine Learning Research 7 (May) (2006) 877–917.\n[17] J. Kober, J. Peters, Imitation and reinforcement learning, IEEE Robotics & Au-\ntomation Magazine 17 (2) (2010) 55–62.\n[18] S. Khadka, S. Majumdar, S. Miret, E. Tumer, T. Nassar, Z. Dwiel, Y. Liu,\nK. Tumer, Collaborative evolutionary reinforcement learning, in: International\nConference on Machine Learning, 2019, pp. 3341–3350.\n[19] S. Fujimoto, H. van Hoof, D. Meger, Addressing function approximation error\nin actor-critic methods, in: International Conference on Machine Learning, 2018,\npp. 1582–1591.\n[20] S. Ross, G. J. Gordon, J. A. Bagnell, No-regret reductions for imitation learning\nand structured prediction, in: AISTATS, Citeseer, 2011.\n[21] A. Attia, S. Dayan, Global overview of imitation learning, arXiv preprint\narXiv:1801.06503.\n[22] H. V. Hasselt, Double q-learning, in: Advances in Neural Information Processing\nSystems, 2010, pp. 2613–2621.\n[23] H. van Hasselt, A. Guez, D. Silver, Deep reinforcement learning with double\nq-learning, in: Thirtieth AAAI conference on artiﬁcial intelligence, 2016.\n[24] E. Todorov, T. Erez, Y. Tassa, Mujoco: A physics engine for model-based control,\nin: 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,\nIEEE, 2012, pp. 5026–5033.\n26\n[25] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang,\nW. Zaremba, Openai gym, arXiv preprint arXiv:1606.01540.\n[26] Y. Duan, X. Chen, R. Houthooft, J. Schulman, P. Abbeel, Benchmarking deep\nreinforcement learning for continuous control, in: International Conference on\nMachine Learning, 2016, pp. 1329–1338.\n[27] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, D. Meger, Deep rein-\nforcement learning that matters, in: Thirty-Second AAAI Conference on Artiﬁ-\ncial Intelligence, 2018, pp. 3207–3214.\n[28] R. Islam, P. Henderson, M. Gomrokchi, D. Precup, Reproducibility of bench-\nmarked deep reinforcement learning tasks for continuous control, arXiv preprint\narXiv:1708.04133.\n[29] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv preprint\narXiv:1412.6980.\n[30] D. Wierstra, T. Schaul, J. Peters, J. Schmidhuber, Natural evolution strategies, in:\n2008 IEEE Congress on Evolutionary Computation, IEEE, 2008, pp. 3381–3387.\n[31] K. O. Stanley, R. Miikkulainen, Evolving neural networks through augmenting\ntopologies, Evolutionary Computation 10 (2) (2002) 99–127.\n[32] T. Haarnoja, A. Zhou, P. Abbeel, S. Levine, Soft actor-critic: Off-policy max-\nimum entropy deep reinforcement learning with a stochastic actor, in: Interna-\ntional Conference on Machine Learning, 2018, pp. 1856–1865.\n27\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NE"
  ],
  "published": "2019-12-13",
  "updated": "2019-12-13"
}