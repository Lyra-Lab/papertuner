{
  "id": "http://arxiv.org/abs/1912.06444v4",
  "title": "Deep Self-representative Concept Factorization Network for Representation Learning",
  "authors": [
    "Yan Zhang",
    "Zhao Zhang",
    "Zheng Zhang",
    "Mingbo Zhao",
    "Li Zhang",
    "Zhengjun Zha",
    "Meng Wang"
  ],
  "abstract": "In this paper, we investigate the unsupervised deep representation learning\nissue and technically propose a novel framework called Deep Self-representative\nConcept Factorization Network (DSCF-Net), for clustering deep features. To\nimprove the representation and clustering abilities, DSCF-Net explicitly\nconsiders discovering hidden deep semantic features, enhancing the robustness\nproper-ties of the deep factorization to noise and preserving the local\nman-ifold structures of deep features. Specifically, DSCF-Net seamlessly\nintegrates the robust deep concept factorization, deep self-expressive\nrepresentation and adaptive locality preserving feature learning into a unified\nframework. To discover hidden deep repre-sentations, DSCF-Net designs a\nhierarchical factorization architec-ture using multiple layers of linear\ntransformations, where the hierarchical representation is performed by\nformulating the prob-lem as optimizing the basis concepts in each layer to\nimprove the representation indirectly. DSCF-Net also improves the robustness by\nsubspace recovery for sparse error correction firstly and then performs the\ndeep factorization in the recovered visual subspace. To obtain\nlocality-preserving representations, we also present an adaptive deep\nself-representative weighting strategy by using the coefficient matrix as the\nadaptive reconstruction weights to keep the locality of representations.\nExtensive comparison results with several other related models show that\nDSCF-Net delivers state-of-the-art performance on several public databases.",
  "text": " \nDeep Self-representative Concept Factorization Network \nfor Representation Learning \nYan Zhang1     Zhao Zhang1,2,*    Zheng Zhang3     Mingbo Zhao4     Li Zhang1     Zhengjun Zha5     Meng Wang2 \n \n \n \nAbstract — In this paper, we investigate the unsupervised deep \nrepresentation learning issue and technically propose a novel frame-\nwork called Deep Self-representative Concept Factorization Network \n(DSCF-Net), for clustering deep features. To improve the representa-\ntion and clustering abilities, DSCF-Net explicitly considers discover-\ning hidden deep semantic features, enhancing the robustness proper-\nties of the deep factorization to noise and preserving the local mani-\nfold structures of deep features. Specifically, DSCF-Net seamlessly \nintegrates the robust deep concept factorization, deep self-expressive \nrepresentation and adaptive locality preserving feature learning into a \nunified framework. To discover hidden deep representations, DSCF-\nNet designs a hierarchical factorization architecture using multiple \nlayers of linear transformations, where the hierarchical representation \nis performed by formulating the problem as optimizing the basis con-\ncepts in each layer to improve the representation indirectly. DSCF-\nNet also improves the robustness by subspace recovery for sparse \nerror correction firstly and then performs the deep factorization in the \nrecovered visual subspace. To obtain locality-preserving representa-\ntions, we also present an adaptive deep self-representative weighting \nstrategy by using the coefficient matrix as the adaptive reconstruction \nweights to keep the locality of representations. Extensive comparison \nresults with several other related models show that DSCF-Net deliv-\ners state-of-the-art performance on several public databases.  \nKeywords — Unsupervised representation learning, robust deep \nfactorization; deep self-expressive representation; clustering1 \nI.  INTRODUCTION \nRepresentation learning from high-dimensional complex data is \nalways an important and fundamental problem in the fields of \npattern recognition and data mining [40-50]. To represent data, \nlots of feasible and effective approaches can be used, of which \nMatrix Factorization (MF) based models have been proven to \nbe effective for low-dimensional feature extraction and cluster-\n                                                           \n1 School of Computer Science and Technology & Provincial Key Laboratory \nfor Computer Information Processing Technology, Soochow University, Su-\nzhou, China. Emails: zhangyan0712suda@gmail.com, zhangliml@suda.edu.cn.  \n2 Key Laboratory of Knowledge Engineering with Big Data (Ministry of Edu-\ncation) & School of Computer Science and Information Engineering (School of \nArtificial Intelligence), Hefei University of Technology, Hefei, China. Emails: \ncszzhang@gmail.com, eric.mengwang@gmail.com.  \n3 Department of Computer Science, Harbin Institute of Technology (Shenzhen), \nShenzhen, China. Email: darrenzz219@gmail.com.  \n4 Department of Electronic Engineering, City University of Hong Kong, Tat \nChee Avenue, Kowloon, Hong Kong. Email: mzhao4-c@my.cityu.edu.hk.  \n5 School of Information Science and Technology, University of Science and \nTechnology of China, Hefei, China. Email: zhazj@ustc.edu.cn.  \n* indicates the corresponding author.  \ning [24-32][36-39]. Nonnegative Matrix Factorization (NMF) \n[1] and Concept Factorization (CF) [2] are two most classical \nnonnegative MF methods. Given a nonnegative data matrix X, \nboth NMF and CF aim to decompose it into the product of two \nor three nonnegative factors by minimizing the reconstruction \nerror. To be more specific, one factor contains the basis vectors \ncapturing the higher-level features of data and each sample can \nbe reconstructed by a linear combination of the bases. The oth-\ner factor corresponds to the new low-dimensional representa-\ntion. Since the nonnegative constraints are applied in NMF and \nCF, they can obtain local parts-based representations [1]. It is \nnoteworthy that those distinguishing features may be precisely \nreflected by the key parts (such as eyebrows and ears in face \nimages; directions of textures in texture images and the angular \nangles in graphs) in reality. As such, the nonnegative constraint \ncan play an essential role in feature representation.   \nFor obtaining the locality preserving feature representations, \nmany MF based methods usually adopt the graph regularization \nstrategy, such as Graph Regularized NMF (GNMF) [3], Local-\nly Consistent CF (LCCF) [4], Self-Representative Manifold \nConcept Factorization (SRMCF) [5], and some dual-graph reg-\nularized methods, e.g., Dual Regularization NMF (DNMF) [6] \nand Dual-graph regularized CF (GCF) [7]. Specifically, GNMF \nand LCCF apply the graph Laplacian to smooth the representa-\ntion and encode the geometrical information of the data space, \nwhich allows extracting the new representation with respect to \nthe intrinsic manifold structures. SRMCF constructs the affini-\nty matrix by assigning adaptive neighbors to each sample based \non the local distance of learned new representation of the origi-\nnal data with itself as a dictionary [5]. Different from GNMF, \nLCCF and SRMCF, both DNMF and GCF not only preserve \nthe geometric structures of data manifold but also the feature \nmanifold jointly using the dual-graph regularization. In addi-\ntion to the above graph regularization strategy, another way to \nretain the locality is by the local coordinate coding. Local Co-\nordinate CF (LCF) [8], Graph-Regularized LCF (GRLCF) [9] \nand Graph-Regularized CF with Local Coordinate (LGCF) [10] \nare several classical methods. The local coordinate coding can \nenable each sample to be represented of a linear combination \nwith only a few nearby basis concepts so that the locality and \nsparsity can be discovered simultaneously [8-10].  \nAlthough the above-mentioned methods have obtained en-\nhanced representation performance, they still suffer from some \ndrawbacks. (1) Most existing MF based methods aim at factor-\nizing the data in the original visual space that usually contains\nInput images\nOriginal data matrix X\nSparse error E\nRecovered clean data Xc\nAdaptive Locality \npreserving \nrepresentation V\nUL=\nXWL\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nXc\nU1\nW1\nW2\n...\nU2\nUL-1\nWL\nUL\n...\nAdaptive reconstruction \nweights S\nClustering data\nSubspace recovery process\nRobust deep Concept factorization\nAdaptive weight learning\nW1...WLV\nOptimizing basis vectors\nFinal bases\n \nFigure 1: The hierarchical architecture of our proposed DSCF-Net framework.  \nnoise and corruptions, which may directly result in the degrad-\ned performance; (2) To preserve the locality of the learned new \nrepresentation, graph regularization based algorithms, includ-\ning GNMF, DNMF, LCCF, GRLCF, GCF and LGCF, need to \nlook for the neighbors of each sample by the k-neighborhood or \nε-neighborhood. However, it still remains a tricky problem to \nestimate an optimal k or ε in practice. Moreover, they often pre-\ncompute the graph weights by a separable process before learn-\ning the new representations, but such operation cannot ensure \nthe pre-obtained weights as joint-optimal for low-dimensional \nrepresentation learning. Although recent SRMCF employs the \nself-representation of original data to construct the affinity ma-\ntrix to preserve the locality of coefficients, but it also involves \nan extra graph regularization based on the original data to en-\ncode the manifold structures of new representations, so it will \nclearly suffers from the same tricky issues as GNMF, DNMF, \nLCCF, GRLCF, GCF and LGCF;  (3) The last and most im-\nportant point is that aforementioned methods are essential sin-\ngle-layer models (i.e., 1-layer), so they can only obtain shallow \nfeatures. That is, they cannot discover deep semantic features \nfrom data, since shallow models directly map visual samples \ninto a latent subspace, while such an operation will implicitly \nneglect and lose certain important hidden information.  \nIn more recent years, researchers have also investigated the \ntopics of multilayer matrix factorization. One most commonly-\nused approach of extending the shallow model to deep model is \nto decompose the observation data matrix iteratively in a num-\nber of layers by a cascade connection of L mixing subsystems \n(L is the number of layers in the deep networks) [11-14]. Multi-\nlayer NMF (MNMF) [11], Multilayer CF (MCF) [12], Spectral \nUnmixing using Multilayer NMF (MLNMF) [13] and Graph \nRegularized Multilayer CF (GMCF) [14] are some representa-\ntive models in this category. These multilayer MF models di-\nrectly take the outputted new representation of a previous layer \nas the input of the next layer, but this strategy may be ineffec-\ntive and even unreasonable in reality, since we cannot ensure \nthat the output of previous layer is an optimal representation of \nthe original data and feeding the output of the previous layer \ndirectly to next layer may mislead and degrade the representa-\ntion learning process of the subsequent layers. Moreover, since \nthe learned representations in the first layer may be inaccurate \nand may lose important hidden information, the resulted recon-\nstruction errors may be getting larger and larger with the in-\ncreasing of layers. Besides, by simply transferring the new rep-\nresentation to the next layer, they still need to initialize the ba-\nsis vectors randomly in each layer, which is also unreasonable \nempirically. To address these issues, Weakly-supervised Deep \nMF (WDMF) [15] and Deep Semi-NMF (DSNMF) [16] have \nprovided another way to construct the deep NMF models. Spe-\ncifically, they define the deep network models to discover hid-\nden deep feature information by multiple layers of linear trans-\nformations and update the basis concepts/new representations \nin each layer. In this way, WDMF and DSNMF can obtain the \nhidden deep representation explicitly. But noting that WDMF \nand DSNMF still cannot encode the local geometry structure of \nthe new representations by self-expression in each layer explic-\nitly in an adaptive manner. They also factorize data in the orig-\ninal input space that usually has various noise and corruptions \nthat may decrease the performance. We argue that the descrip-\ntive abilities and quality of learned feature representations from \nthe first layer will be critical for the subsequent layers.  \nIn this paper, we mainly propose certain effective strategies \nto overcome the drawbacks of existing MF models and obtain \nmore powerful deep representations for unsupervised clustering. \nThe main contributions of this work are summarized as \n(1) A novel Deep Self-representative Concept Factorization \nnetwork, termed DSCF-Net, is technically proposed for deep \nfeature learning and clustering. DSCF-Net explicitly considers \nimproving the feature representation by mining deep semantic \nfeatures hidden in data, enhancing the robustness properties of \nthe learning system to noise and preserving the local manifold \nstructures of deep features in an adaptive manner. Fig.1 illus-\ntrates the flowchart of our DSCF-Net. We see that DSCF-Net \nseamlessly integrates the robust subspace recovery, robust deep \nconcept factorization, self-expressive representation learning \nand adaptive locality preservation into a united framework.  \n(2) To deliver a better higher-level deep feature representa-\ntion and well handle the semantic gap, on one hand DSCF-Net \ndesigns a hierarchical factorization architecture using the mul-\ntiple layers of linear transformations to obtain the latent repre-\nsentation by a progressive way. Such an operation can automat-\nically learn the intermediate hidden representations and update \nthe intermediate basis vectors in each layer. Because the basis \nvectors capture the higher-level features of input data and each \nsample is reconstructed using a linear combination of the bases, \nwe argue that optimizing the basis vectors to improve the rep-\nresentation indirectly may be more important than optimizing \nthe new representations in each layer. Meanwhile, DSCF-Net is \nmodeled as the formulation of learning one final representation \nmatrix and L updated sets of basis vectors. On the other hand, \nour DSCF-Net learns the latent deep representation in a recov-\nered clean subspace by leveraging the geometrical, visual and \nsemantic information jointly. Due to the fact that the used sub-\nspace recovery process can remove noise and outliers explicitly \nfrom original data, both the robustness properties and descrip-\ntive power of the learned representation in the first layer and \nsubsequent layers can be potentially enhanced.  \n(3) To obtain the locality-preserving higher-level represen-\ntations, DSCF-Net introduces the adaptive self-representative \nweighting strategy. Specifically, our DSCF-Net explicitly uses \nthe coefficient matrix as the adaptive reconstruction weights to \npreserve local information of new representation in each layer.  \n    We outline the paper as follows. Section II briefly reviews \nthe related works. Our DSCF-Net is presented in Section III. In \nSection IV, we mainly show the optimization procedures of our \nDSCF-Net. The comparison of the architectures of exiting sin-\ngle-layer and multilayer CF frameworks are discussed in Sec-\ntion V. Section VI describes the simulation settings and results. \nFinally, the paper is concluded in Section VII.   \nII. RELATED WORK \nIn this section, we review several single-layer and multilayer \nmethods relevant to our proposed DSCF-Net framework.  \nA. Single-layer CF and SRMCF \nConcept factorization [2]. We first briefly introduce the CF \nmodel. For a given data matrix\n\n\n1\n2\n,\n,...,\nD N\nN\nX\nx x\nx\n\n\n\n, where \n,\n1,2,\n,\nix i\nN\n\n is a sample vector, N is the number of samples \nand D is the original dimensionality. Denote by \nD r\nU\n\n\n and \nr N\nV\n\n\n two nonnegative matrices whose product \nD N\nUV\n\n\n \nis the approximation to X, where the rank r is a constant, by \nrepresenting each basis by using a nonnegative linear combina-\ntion of \nix , i.e.,\n1\nN\nij\ni\ni w x\n\n\n with \n0\nij\nw \n,  CF aims at solving the \nfollowing minimization problem:  \n2 , . .\n,\n0\n\n\n\nF\nO\nX\nXWV\ns t W V\n,                     (1) \nwhere \nN r\nij\nW\nw\n\n\n\n\n\n\n\n, i.e., XW approximates the bases, and V \nis the learnt new representation of X for clustering.  \nSelf-representative manifold CF (SRMCF) [5]. SRMCF \nimproves CF by integrating the adaptive neighbor structure and \nmanifold regularizers into a unified model. It is noteworthy that \nCF can be considered as an improved self-representation meth-\nod with a learning based dictionary to reveal the global struc-\nture of data, and the coefficients of CF carry plentiful semantic \nmeanings. By rewriting the CF model, one can have \n,\n\n\nX\nXR where R\nWV ,                             (2) \nwhere \n\nR\nWV is regarded as the coefficient matrix based on \nthe dictionary using the original data.  \n  To formulate the model, SRMCF involves two similarity ma-\ntrices S and A to preserve the locality of new representation V \nand coefficient matrix WV, respectively. Let LS and LA denote \nLaplacian matrices, i.e., LS=DS-S, LA=DA-A, where DS and DA \nare two diagonal matrices whose entries are column sums of \nthe similarity matrices S and A respectively. S is defined based \non the binary-weighting in the original data space as \n\n\n1\n0\ni\nk\nj\nj\nk\ni\nij\nif  x\nN\nx\n or x\nN\nx\nS\notherwise\n\n\n\n\n\n\n,  \nwhere Nk(xi) is the k-nearest neighbor set of xi. A can be ob-\ntained adaptively based on the coefficient matrix WV by solv-\ning the following problem:  \n\n\n\n\n2\n2\n1\n2\n1,\n1\nmin\n\n\n\n\n\n\n\n\nT\ni\ni\nN\nij\nij\ni\nj\nj\nA\n 0 A\nWV\nWV\nA\nA\n1\n,  \nwhere is a positive parameter and 1  is a vector of all ones. \nFinally, the objective function of SRMCF is formulated as \n\n\n\n\n2\n2\n1\n2\n3\n, ,\nmin\n. .\n,\n0,\n1,\n1\n\n\n\n\n\n\n\n\n\n\n\n\nT\nA\nS\nF\nF\nW V A\nT\ni\ni\ni\nX\nXWV\ntr WVL VW\ntr VL V\nA\ns t  W V\nA e\n 0\nA\n,    (3) \nwhere λ1, λ2, and λ3 are nonnegative tunable parameters. Note \nthat although SRMCF defines the affinity matrix A in an adap-\ntive manner, it still uses the traditional weighting method to \ndefine S, so it also faces the difficult issue of identifying k in \nreality. Note that DSCF-Net avoids this tricky issue by direct-\nly using the coefficient matrix as the adaptive reconstruction \nweight matrix for encoding the locality of V.  \nB. Weakly-supervised Deep Matrix Factorization (WDMF) \nWe briefly review the deep architecture of WDMF. Assume \nthat the hierarchical structure has L layers, WDMF factorizes \nthe observed image tagging matrix F into L+1 factor matrices, \ni.e., U, VL,…, V1, and the output of first layer is transformed \nfrom visual space, i.e., V1=W1X. Specifically, WDMF applies \na deep network to discover the hidden representations as \n1\n2\n2 1\n1\n1\nL\nL\nL\nL\nF\nUV\nV\nW V\nV\nW V\nV\nW X\n\n\n\n\n\n,                                     (4) \nwhere Wl (l=1,2,…, L) is the transformation matrix of the l-th \nlayer, U is the latent tag feature matrix in the subspace and Vl \nis the implicit representation matrix of images in the l-th layer. \nThat is, the problem of WDMF learns one factor U containing \nthe basis vectors and L representation matrices VL,…, V1. The \nunified objective function of WDMF is defined as \nX\nX\nW\nV\nX\nX\nW1\nV1\nW2\nV2\nV1\n...\n...\n...\nWL\nVL\nVL-1\nVL\nU\nWL\nVL-1\nVL-2\n...\n...\nW1\nX\nWL-1\nF\nV\nUL\nWL\nUL-2\n...\n...\nW1\nWL-1\nX-E\nX-E\nUL-1\n \n(a)                                          (b)                                                                (c)                                                                   (d) \nFigure 2: Comparison of the learning architectures of existing single-layer and multilayer CF models, where (a) Traditional 1-layer CF model; (b) Multilayer CF \nmodel (e.g., MNMF, MCF, MLNMF and GMCF); (c) WDMF model; (d) Our DSCF-Net model.  \n\n\n\n\n\n\n\n\n1\n2\n1\n1\n,\n,...,\n2\n1\n2\n1 2,1\n1\n1\nmin\n2\n2\n2\n2\n2\n2\n. .  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL\nT\nT\nL\nL\nF\nV W\nW\nL\nT\nl\nF\nl\nT\nl\nl\nF\nUV\ntr V\nGV\ntr U PU\ntr VMV\nU\nW\nW\ns t W W\nI\n,    (5) \nwhere V=WLWL-1…W1X, G=DB-B and P=(H-I)(H-I) are posi-\ntive semi-definite Laplacian matrices. Bi,j measures the seman-\ntic relevance between the i-th image and the j-th image, which \nis defined by using the cosine similarity based on the tagging \nvectors and DB a diagonal matrix over B. H is defined as: Hi,j = \ngi,j if xj∈Nk(xi) and Hi,j=0 otherwise. The local discriminative \nstructure can be well retained, which can also solve the over-\nfitting caused by noisy tagging information. ,  and are \npositive trade-off factors, \n1 and \n2\n are regularization factors.  \nIII. DEEP SELF-REPRESENTATIVE CONCEPT FACTORIZATION \nNETWORK (DSCF-NET) \nWe introduce the formulation of our DSCF-Net in this section. \nGiven data matrix\n\n\n1\n2\n,\n,\n,\nD N\nN\nX\nx x\nx\n\n\n\n, we design a deep \nhierarchical structure that has L layers. Since the basis vectors \ncapture the higher-level features of data and each sample is \nreconstructed by a linear combination of the bases, DSCF-Net \naims to find L+1 nonnegative matrices, and is formulated as \nthe problem of learning one representation matrix V and L sets \nof basis vectors, i.e., U1, U2,…, UL.  \n   Since real-world original data X usually have various noise \nand errors, DSCF-Net also considers improving the robustness \nproperties to noise by using subspace recovery. Specifically, \nDSCF-Net performs the hierarchical factorization over recov-\nered clean data X-E rather than the original data X so that the \nrepresentation ability can be enhanced, where E is the sparse \nerror by L2,1-norm regularization, i.e., \n2,1\nE\n. As a result, the \nfactorization process of our DSCF-Net is obtained as \n\n\n\n\n1\n2\n1\n2\n1\n1\nL\nL\nL\nL\nX\nE\nU V\nU\nU\nW\nU\nU W\nU\nX\nE W\n\n\n\n\n\n\n\n,                               (6) \nwhere Ul (l=1, 2,…, L) is the set of basis vectors of the l-th \nlayer in the recovered clean visual subspace and Wl (l=1, 2,…, \nL) is the intermediate matrix for updating the basis vectors. \nFinally, the product ULV that is equivalent to (X-E)W1…WLV is \nthe reconstruction of the original data. In other words, the total \nreconstruction error can be defined as \n\n\n\n0\n1\n2\n1\n...\n\n\n\n\n\nL\nL\nF\nW\nW\nX\nX\nW\nJ\nE\nE\nV\n,                (7) \nwhere \n\n0\n1\n...\n\n\nL\nL\nW\nW\nX\nE\nW  is called deep basis vectors, V is the \nnew deep representation of the input data and \n0\nW is fixed to be \nan identity matrix. Following SRMCF [5], the reconstruction \ncan be rewritten as \n\n\n\n\n\n\nX\nE\nX\nE R , where \n0\n1\n...\n\n\nL\nL\nR\nW\nW\nW V  \ncan be similarly regarded as the meaningful coefficient matrix \nbased on the clean dictionary of recovered data matrix X-E, \nand the coefficients can be used to characterize the locality \nand similarity between samples or features. Note that the coef-\nficient matrix UV of SRMCF is directly based on the diction-\nary of the input X that usually contains noise, so the resulted \ncoefficient matrix of our DSCF-Net will be potentially more \naccurate than that of SRMCF in practice. Moreover, we adopt \na different weighting strategy as SRMCF. That is, we directly \nuse the coefficient matrix \n0\n1\n...\n\n\nL\nL\nR\nW\nW\nW V  in each layer as the \nadaptive reconstruction weights to encode and preserve the \nlocality of learned new representation V, which is clearly dif-\nferent from SRMCF that involves an extra graph regulariza-\ntion on the original data to retain the locality of V. Then the \nlocality preserving constraint of DSCF-Net is defined as \n\n\n0\n2\n2\n2\n1\n...\n\n\n\n\nF\nL\nL\nF\nL\nJ\nV\nV\nV\nW\nW V\nW\nW\n.                  (8) \nBased on the reconstruction error J1 and locality preserving \nconstraint J2, the objective function of DSCF-Net is defined as \n\n\n\n\n\n\n\n\n\n1,...,\n, ,\n2\n0\n1\n1\n2\n2,1\n2\n2\n2,1\n1,2,...,\n0\n1\nmin\n...\n...\n.\n0,\n.\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL\nF\nF\nl\nL\nW\nW\nV E\nL\nL\nF\nL\nL\nL\nl\nJ\nJ\nX\nE\nX\nE\nV\nV\nE\nW\nW\nW\nW\nW\nW\nW\nE\nW\nV\nV\nt\nV\nV\ns\n,       (9) \nwhere \n,\n0\n are trade-off parameters, the L2,1-norm \n2,1 \nE\n \n2\n,\n1\n1\n\n\n\n\nN\nn\ni j\nj\ni\nE\ncan make the error term E column sparse. To \nfacilitate the optimization, we include an auxiliary variable S \nto relax the locality preserving constraint. The relaxed optimi-\nzation problem can be written as \n\n\n\n\n\n\n\n1\n2\n0\n1\n,...,\n, ,\n2\n2\n2\n2,1\n1,2 .. ,\n,\n, .\n1\n1\n. .\nmin\n...\n...\n0,\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL\nL\nF\nF\nF\nl\nL\nF\nW\nW\nV E S\nL\nL\nL\nL\nl\nX\nE\nX\nE\nV\nS\nV\nW\nW\nW\nW\nW\nW\nW\nE\nW\nV\nV\nS\ns\nV\nV\nt\n,           (10) \nwhere \n0\n\n is also a  trade-off parameter. Next, we detail the \noptimization procedures of our DSCF-Net.  \nIV. OPTIMIZATION \nFrom the objective function of DSCF-Net, we can find that the \ninvolved several variables, i.e., Wl (1\nl\nL\n\n), V, S and E, de-\npend on each other, so they cannot be solved directly. Follow-\ning the common procedures, we propose an iterative optimiza-\ntion strategy by using the Multiplicative Update Rules (MUR) \nmethod for local optimal solutions. Specifically, we solve the \nproblem by updating the variables alternately and compute one \nof the variables each time by fixing others. The detailed opti-\nmization procedures of DSCF-Net are shown as follows:  \n1) Fix others, update the factors Wl and V: \nWhen the other variables are fixed, we can update the matrices \nWl and V by solving the objective function. For the l-th layer, \nW1,…,Wl-1 are all constants, and we define\n1\n0\n1\n...\n\n\n\n\nl\nl\nW\nW\n. By \nusing Xc to denote the recovered clean data, i.e., X-E, the prob-\nlem associated with Wl and V can be defined as \n\n\n\n\n1\n2\n2\n1\n2\n,\n,\n.\nmin\n0,\n0\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\nl\nl\nl\nl\nl\nl\nF\nW\nl\nF\nF\nV\nc\nc\nT\nX\nX\nV\nS\nV\nV\ntr V QV\nW\nt\nW\ns\nW\nW\nV\n,    (11) \nwhere Q=(I-S)(I-S)T and I is an identity matrix. Let \nik\n\nand \nik\nbe the Lagrange multipliers for the constraints \n\n0\nl ik\nW\n\n \nand \n0\nik\nv\n, and \nik\n\n\n\n\n, \nik\n\n\n\n\n, the Lagrange function of \nthe above problem can be constructed as \n\n\n\n\n\n\n\n\n2\n1\n1\n,\n2\n2\nmin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nl\nl\nl\nl\nl\nc\nc\nF\nF\nT\nl\nF\nW V\nT\nT\nl\nX\nX\nV\nS\nV\nV\ntr V QV\nW\nW\nW\ntr\nW\ntr\nV\n.   (12) \n   For ease of representation, we use notation O1 to denote the \nobjective function of the above problem. Then, the variables \nWl and V can be alternately updated by fixing other variables.    \nThe derivatives of O1 w.r.t. Wm and V are computed as follows:  \n \n\n\n\n\n1\n1\n1\n1\n1\n1\n2\n2\nT\nT\nT\nT\nl\nl\nc\nl\nc\nT\nT\nT\nl\nl\nl\nO\nW\nK WVV\nK V\nSV\nWVV\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n,      (13) \n\n\n\n\n1\n1\n1\n1\n1\n1\n1\n2\n               \n2\n2\nT\nt\nT\nt\nl\nc\nl\nl\nc\nT\nt\nT\nT\nt\nl\nl\nl\nT\nO\nV\nW\nK\nWV\nW\nK\nW\nWV\nW WV\nW\nS\nVQ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n,     (14) \nwhere\n\nT\nc\nc\nc\nK\nX\nX . By using the KKT conditions \n\n\n0\nik\nl ik\nw\n\n \nand \n0\nik ik\nv\n\n\n, we can obtain the updating rules as \nAlgorithm 1: Optimization procedures of DSCF-Net \nInputs: Training data X, constant r and parameters\n,\n,\n.  \nInitialization: Initialize the variables W, and V as random matri-\nces; initialize S and E as zero matrices; initialize D to be an identi-\nty matrix; \n3\n10\n\n\n\n; t=0.  \nFor fixed number l of layers:  \nWhile not converged do \n1. Update the matrix factors Wl and V by Eqs.(15-16);  \n2. Update the auxiliary matrix S by Eq.(18);  \n3. Update the sparse error E by Eq.(21) and update the entries of  \nthe diagonal matrix D by \n\n\n2\n1/ 2\n\nii\ni\nd\ne\n;  \n4. Check for convergence: if\n2\n1\n\n\n\nt\nt\nF\nV\nV\n, stop; else t=t+1.  \nEnd while \nOutput: The learned deep new representationsV .  \n \n\n\n\n\n\n\n\n\n1\n1\n1\n1\n1\n1\nT\nT\nT\nT\nl\nc\nl\nik\nl\nl\nik\nik\nT\nT\nT\nT\nT\nT\nl\nc\nl\nl\nl\nik\nK V\nSV\nw\nw\nK\nWVV\nWVV\nWVV\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n,   \n                                                                                             (15) \n\n\n\n\n1\n1\n1\n1\n1\n1\n+\nT\nT\nT\nT\nl\nc\nl\nik\nik\nik\nT\nT\nT\nT\nT\nT\nT\nl\nc\nl\nl\nl\nik\nW\nK\nW\nS\nv\nv\nW\nK\nWV\nW\nWV\nW WV\nVQ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n. \n(16) \n2) Fix others, update the auxiliary variable S:  \nWith obtained Wl and V, we can use them to update the auxil-\niary variable S by solving the following problem:  \n\n\n\n\n\n\n\n\n\n\n2\n2\n2\nmin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nl\nF\nS\nT\nT\nl\nF\nl\nO\nS\nt\nS\nV\nV\nV\nS\nV\nS\nV\nV\nr\nVS\nV\nt\nV\nr\nS\n,    (17) \nwhere\n1\n1\n...\nl\nl\nl\nW\nW W\n\n\n. Then, the variable S can be obtained by \nsetting the derivative of \n2\nO  w.r.t S to zero:  \n\n\n\n\n\n\n\n2\n1\n0\nT\nT\nl\nT\nT\nl\nO\nS\nV VS\nV V\nS\nI\nV V\nV V\nS\nV\nV\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.             (18) \n3) Fix others, recover the sparse error E:  \nAfter calculating Wl, V and S, we can easily update the sparse \nerrors E by solving the following reduced formulation:  \n\n\n\n2\n3\n2,1\n0\n1\nmin\n...\n\n\n\n\n\n\n\nl\nl\nF\nE\nO\nW\nW W\nX\nV\nE\nE\nX\nE\n.    (19) \nBy the properties of L2,1-norm, we have \n\n\n2,1\n2\nT\nE\ntr E DE\n\n, \nwhere D is a diagonal matrix with \n\n\n2\n1/ 2\n\nii\ni\nd\ne\n being its en-\ntries, and \nie  is the i-th column vector of E . If each\n0\n\nie\n, the \nabove formulation can be approximated as \n\n\n\n\n\n2\n3\n0\n1\nmin\n...\n\n\n\n\n\n\nT\nl\nl\nF\nE\nO\nW\nW W\ntr\nE\nI\nV\nE\nX\nDE .     (20) \nLet\n0\n1\n...\n\n\n\nl\nl\nl\nW\nW W\nI\nV , then we can update E by computing \nthe derivative of the above problem O3 w.r.t. E as \n\n\n\n\n3\n1\n2\n0\n2\nT\nl\nl\nT\nT\nl\nl\nl\nl\nO\nE\nE\nX\nED\nE\nX\nD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.              (21) \nFor complete presentation of our DSCF-Net, we summarize \nthe optimization procedures of our DSCF-Net in Algorithm 1, \nwhere the diagonal matrix D is initialized as an identity matrix. \nDSCF-Net mainly optimizes the basis vectors to improve the \nrepresentation V that is the major variable. To ensure the rep-\nresentation V to converge, the stopping condition is simply set \nto\n2\n1\n\n\n\nt\nt\nF\nV\nV\n in each layer, which measures the difference \nbetween sequential representation matrices Vs and it can make \nsure that the representation result will not change drastically.  \nV. DISCUSSION AND SOME REMRKS \nIn this section, we mainly compare the architectures of exiting \nsingle-layer and multilayer CF frameworks in Fig.2. As shown \nin Fig.2(a), the single-layer CF model obtains the basis vectors \nand new representation directly from given data by 1-layer, so \nit fails to discover the deep hidden semantic and structure in-\nformation. As shown in Fig.2(b), those traditional multilayer \nCF models, e.g., MNMF, MCF, MLNMF and GMCF, directly \nuse the output of previous layer (i.e., intermediate representa-\ntion V) as the input of subsequent layer, without properly con-\nsidering the optimization of new representation and basis vec-\ntors in each layer. Besides, since one cannot ensure the inter-\nmediate representation V from the previous layer to be a good \nrepresentation for subsequent layer, which may cause the de-\ngraded results. Different from traditional single-layer and mul-\ntilayer CF models, WDMF and DSCF-Net explicitly consider \noptimizing the new representation in each layer, as can be seen \nfrom Figs.2(c) and (d). But it should be noted that our DSCF-\nNet differs from WDMF in several aspects. First, the strategies \nof optimizing the new representation in each layer are differ-\nent. Specifically, WDMF aims at fixing the basis vectors and \noptimizes the representation V directly in each layer, while our \nDSCF-Net aims at optimizing the basis vectors to improve the \nnew representation V indirectly in each layer. The major bene-\nfit of this strategy used in DSCF-Net is that the basis vectors \ncapture the higher-level features of samples, so we believe that \nthe procedure of reconstructing given sample by a linear com-\nbination of the bases will be more accurate if we can obtain a \nset of optimal basis vectors, which will also be verified by \nsimulations. Second, their learning tasks are different. WDMF \nmainly focuses on the social image understanding tasks, i.e., \ntag refinement, tag assignment and image retrieval, and the \ninitial input of WDMF is the tagging matrix F rather than im-\nage data. While DSCF-Net mainly extracts new feature repre-\nsentations from the original images and the input is image data. \nThird, the locality preserving strategies are different. From the \ndefinition of H, it is clear that WDMF still suffers from the \ntricky issue of selecting k in reality. In addition, B and H are \npre-calculated based on the image tagging matrix and samples \nrespectively, it is not guaranteed to obtain optimal results of \nsubsequent representation learning. While the locality of the \nnew representation in DSCF-Net is not pre-calculated, since it \nis jointly optimized in our model. Furthermore, the locality the \nnew representation is preserved adaptively in our model.  \nVI. EXPERIMENTAL RESULTS AND ANALYSIS \nIn this section, we mainly perform simulations to examine the \ndata representation and clustering performance of our DSCF-\nNet. The results of our DSCF-Net are mainly compared with \nseveral traditional single-layer matrix factorization techniques \n(i.e., GNMF [3], DNMF [6], LCCF [4], LCF [8]) and several \ndeep factorization models (i.e., MNMF [11], MLNMF [13], \nMCF [12] and GMCF [14]). For fair comparison, the parame-\nters of each method are carefully chosen from the candidate \nset, and the averaged results are reported.  \nIn our study, three public image databases are involved, i.e., \nETH80 object database [22], COIL100 object database [23] \nand MIT CBCL face database [21]. Following the common \nprocedures, all the face and object images are resized into \n32×32 pixels, i.e., each image is represented by a 1024-D vec-\ntor. We perform all the experiments on a PC with Intel Core i5-\n4590 CPU @ 3.30 GHz 3.30 GHz 8G.   \nA. Visual Image Analysis by Visualization \nWe firstly compare the locality representation power by visu-\nalizing the reconstruction weight matrix S of our DSCF-Net, \nthe binary weights used in DNMF and GNMF, and the Cosine \nsimilarity weights used in LCCF and GMCF. COIL100 object \ndatabase is used and we randomly choose 200 images from \nfirst five classes to construct the adjacency graphs. The num-\nber of the nearest neighbors is set to 7 [20] for other evaluated \nweighting method for fair comparison. The visualization re-\nsults are shown in Fig.3, where we show the adaptive weights \nobtained by DSCF-Net in the first and third layers. We can \neasily find that: 1) the constructed weight matrices by different \nweighting ways have approximate block-diagonal structures; 2) \nmore noisy or wrong inter-class connections are produced in \nthe Binary weights and Cosine Similarity weights than ours; 3) \nthe structures of our adaptive weights in the third layer is bet-\nter than the first layer, which means that our deep model can \npotentially improve the representation and locality.  \n \n \n(a) Binary weights                       (b) Cosine Similarity weights \n \n \n(c) DSCF-Net weights (l=1)                  (d) DSCF-Net weights (l=3) \nFigure 3. Visualization comparison of constructed weights.  \nB. Quantitative Clustering Evaluation \nWe evaluate the clustering performance of each algorithm. For \nquantitative evaluation, we employ two widely used clustering \nevaluation metrics, i.e., Accuracy (AC) and F-score [17-18]. \nTable 1. Clustering performance of each evaluated method based on three public image databases.   \nMethods \nClustering Accuracy \nETH80 object database \nCOIL100 object database \nMIT CBCL face database \nK=2 \nK=3 \nK=4 \nK=5 \nK=6 \nK=2 \nK=3 \nK=4 \nK=5 \nK=6 \nK=2 \nK=3 \nK=4 \nK=5 \nK=6 \nGNMF \n88.50 \n67.98 \n56.83 \n49.20 \n37.51 \n86.67 \n83.60 \n68.14 \n68.00 \n60.38 \n81.84 \n51.60 \n48.60 \n38.19 \n35.35 \nDNMF \n87.59 \n66.91 \n56.91 \n47.74 \n37.62 \n85.38 \n84.25 \n70.39 \n67.67 \n62.35 \n82.04 \n51.91 \n46.71 \n38.86 \n34.42 \nLCCF \n91.20 \n63.98 \n61.97 \n63.00 \n54.79 \n75.82 \n73.57 \n64.04 \n73.64 \n68.69 \n74.75 \n73.43 \n65.93 \n57.04 \n55.32 \nLCF \n91.22 \n62.95 \n50.48 \n42.50 \n32.17 \n91.14 \n76.60 \n67.58 \n69.19 \n64.53 \n68.49 \n45.52 \n44.41 \n35.52 \n31.91 \nMNMF \n73.45 \n53.15 \n47.16 \n47.15 \n39.40 \n69.44 \n60.99 \n58.28 \n58.90 \n54.44 \n71.55 \n61.17 \n62.50 \n54.51 \n56.62 \nMCF \n74.62 \n54.61 \n53.24 \n50.98 \n39.92 \n69.36 \n62.88 \n56.03 \n53.08 \n52.30 \n79.46 \n61.80 \n57.96 \n56.91 \n52.82 \nMLNMF \n77.27 \n64.89 \n63.74 \n55.15 \n41.59 \n65.72 \n60.61 \n54.84 \n50.70 \n50.49 \n68.95 \n56.43 \n58.20 \n56.80 \n57.94 \nGMCF \n79.19 \n66.21 \n57.79 \n52.95 \n51.83 \n94.62 \n83.21 \n59.90 \n71.07 \n63.55 \n72.97 \n68.27 \n70.69 \n52.04 \n52.46 \nOurs \n93.79 \n75.92 \n68.14 \n65.13 \n56.75 \n96.84 \n86.36 \n75.33 \n77.65 \n70.33 \n89.46 \n71.06 \n70.73 \n58.82 \n54.98 \nMethods \nF-score values \nETH80 object database \nCOIL100 object database \nMIT CBCL face database \nK=2 \nK=3 \nK=4 \nK=5 \nK=6 \nK=2 \nK=3 \nK=4 \nK=5 \nK=6 \nK=2 \nK=3 \nK=4 \nK=5 \nK=6 \nGNMF \n86.27 \n61.50 \n53.54 \n43.41 \n32.00 \n82.72 \n81.59 \n62.41 \n64.73 \n54.64 \n71.17 \n42.61 \n39.78 \n32.84 \n28.69 \nDNMF \n84.55 \n62.21 \n54.18 \n42.67 \n32.30 \n82.32 \n81.39 \n63.38 \n64.29 \n56.37 \n71.01 \n43.25 \n38.70 \n33.10 \n28.20 \nLCCF \n91.15 \n58.55 \n60.88 \n58.04 \n53.28 \n68.35 \n65.76 \n56.36 \n69.38 \n62.67 \n69.89 \n67.53 \n59.14 \n51.53 \n53.51 \nLCF \n91.49 \n56.48 \n57.49 \n38.89 \n27.23 \n89.11 \n76.29 \n63.98 \n66.57 \n59.81 \n58.29 \n38.18 \n37.22 \n33.61 \n27.14 \nMNMF \n65.14 \n45.83 \n43.96 \n37.77 \n32.59 \n64.52 \n54.35 \n50.36 \n51.22 \n47.52 \n66.05 \n53.02 \n52.96 \n48.25 \n50.14 \nMCF \n65.63 \n49.03 \n45.41 \n39.68 \n30.30 \n66.64 \n54.65 \n51.33 \n49.50 \n42.65 \n73.96 \n52.68 \n48.13 \n50.10 \n46.16 \nMLNMF \n66.54 \n56.04 \n55.27 \n48.98 \n32.24 \n61.24 \n52.27 \n46.08 \n43.38 \n41.76 \n61.91 \n47.44 \n48.99 \n48.40 \n49.35 \nGMCF \n75.62 \n62.52 \n60.32 \n50.81 \n46.41 \n94.75 \n81.49 \n64.39 \n69.79 \n63.20 \n72.11 \n68.41 \n66.69 \n43.99 \n55.65 \nOurs \n93.13 \n69.58 \n63.73 \n58.80 \n53.59 \n92.89 \n82.87 \n69.45 \n75.90 \n66.12 \n87.59 \n66.25 \n57.40 \n50.86 \n56.21 \n \n0\n20\n40\n60\n80\n100\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nDifferent noise levels (variance)\nClustering accuracy\n \n \nGNMF\nDNMF\nLCCF\nLCF\nMNMF\nMCF\nMLNMF\nGMCF\nOur method\n \n0\n20\n40\n60\n80\n100\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nDifferent noise levels (variance)\nClustering accuracy\n \n \nGNMF\nDNMF\nLCCF\nLCF\nMNMF\nMCF\nMLNMF\nGMCF\nOur method\n \n0\n20\n40\n60\n80\n100\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nDifferent noise levels (variance)\nClustering accuracy\n \n \nGNMF\nDNMF\nLCCF\nLCF\nMNMF\nMCF\nMLNMF\nGMCF\nOur method\n \n(a)                                                                     (b)                                                                       (c) \nFigure 4: Clustering accuracies of each method on (a) ETH80, (b) COIL100, and (c) MIT CBCL against different levels of noises.  \n \n1\n2\n3\n4\n5\n6\n7\n8\nNumber of layers\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nClustering accuracy\nMNMF\nMCF\nMLNMF\nGMCF\nOur method\n \n1\n2\n3\n4\n5\n6\n7\n8\nNumber of layers\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nClustering accuracy\nMNMF\nMCF\nMLNMF\nGMCF\nOur method\n \nFigure 5: AC values vs. Number of layers on (left) ETH80 database and (right) \nMIT CBCL database.  \nETH80, COIL100 and MIT CBCL are evaluated, which con-\ntains 3280/7200/3240 images from 80/100/10 classes respec-\ntively. For each evaluated method, we perform K-means clus-\ntering on the learnt new representation V. Specifically, follow-\ning the common procedures [19-20], for each fixed K of clus-\nters, we randomly choose K categories to form the input data \nmatrix X for representation learning and data clustering. In this \nstudy, the value of K varies from {2, 3, 4, 5, 6}. For each \nmethod, the rank r is set to K+1 as [20], and we average the \nnumerical results over 30 random selections of K categories. \nThe clustering results of AC and F-score over different values \nof K are shown in Table I. We find that: 1) the clustering re-\nsults of each method usually decrease with the increasing of K; \n2) deep matrix factorization methods (MNMF, MCF, MLNMF \nand GMCF) generally obtain the enhanced clustering perfor-\nmance than the single-layer GNMF, DNMF, LCCF and LCF \nfor K=4/5/6, and our DSCF-Net delivers higher values of AC \nand F-score than other compared methods in most cases.  \nC. Image Clustering against Corruptions \nIn this study, we prepare experiments to evaluate each method \nfor clustering corrupted image data. To corrupt the data matrix \nX, we add random Gaussian noise with the variance being 0-\n100% with interval 20% into the selected pixels of images. \nNote that the position of the corrupted pixels it is unknown to \nusers and the clustering accuracies by K-means clustering over \ncorrupted noisy images with various levels of noise are illus-\ntrated in Fig.4. The results are obtained by choosing two cate-\ngories and the AC values are averaged over 30 runs to avoid \nthe randomness. We conclude that: 1) the AC values of each \nalgorithm go down with the increasing noise levels in general; \n2) our method outperforms the other algorithms in this study, \nwhich may be attributed to the fact that our DSCF-Net incor-\nporates the error correction procedure into the representation \nlearning and the factorization procedures are performed in the \nrecovered clean data in each iteration.  \nD. Clustering with Different Numbers of Layers \nWe investigate the effects of different number of layers on the \n1e-4\n1e-2\n1\n1e2\n1e4\n1e6\n1e8\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nClustering accuracy\n \n1e-4\n1e-2\n1\n1e2\n1e4\n1e6\n1e8\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nClustering accuracy\n \n1e-4\n1e-2\n1\n1e2\n1e4\n1e6\n1e8\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nClustering accuracy\n  \n(a) ETH80                                                          (b) COIL100                                                               (c) MIT CBCL \nFigure 6: Clustering accuracies vs. varied parameters of our DSCF-Net method based on the evaluated databases.  \nclustering results of each multilayer model, i.e., MNMF, MCF, \nMLNMF, GMCF and our DSCF-Net. In this study, we aim to \nfix the data matrix, vary the number of layers from {1, 2, …, 8} \nand report the averaged clustering accuracies. ETH80 and \nMIT CBCL databases are used as the examples. For each da-\ntabase, we randomly choose four classes and average the re-\nsults over 100 random selections to avoid the randomness. The \nAC values under different number of layers are shown in Fig.5, \nfrom which we can find that: 1) our method delivers the high-\nest AC values in most cases; 2) generally, the AC value firstly \nincreases with the increase of number of layers, but it start to \ndecrease when the number of layers gets larger. It can also be \nfound that most compared methods obtain the highest accura-\ncy when the number of layers is 3, so we set the number of \nlayers to 3 for all deep models for fair comparison.  \nE. Parameter Sensitivity Analysis \nIn this study, we explore the effects of model parameters on \nthe clustering performance of DSCF-Net that has three trade-\noff parameters, i.e., , and . Since the optimal parameter \nselection is still an open issue, we adopt the widely-used grid \nsearch strategy [34-35] in our experiments to select the most \nimportant parameters. In this study, K is simply set to two, we \nrandomly choose 2 categories to train our method. The results \nare averaged based on 30 random selections of categories and \nthe central points in K-means clustering. The parameter selec-\ntion results on MIT CBCL, ETH80 and COIL100 are illustrat-\ned in Fig.6, respectively. From the results, we find that the \nbest clustering records are obtained based on similar parame-\nter combinations, which is a good phenomenon of the model \nparameter selection. Finally, α=β=γ=104 are used for MIT \nCBCL and ETH80, and α=102, β=γ=104 are used for the \nCOIL100 database in our experiments.  \nVII. CONCLUDING REMARKS \nWe proposed a novel deep self-representative concept factori-\nzation network for unsupervised representation learning and \nclustering. DSCF-Net improved the representation and cluster-\ning abilities of deep factorization in threefold. First, to mine the \nhidden deep information, it employs a hierarchical factorization \nstructure using multiple layers of linear transformations, where \nthe hierarchical representation is formulated by optimizing the \nbasis vectors in each layer to improve the representations indi-\nrectly. Second, to improve the robustness against noise, sub-\nspace recovery is integrated into the deep structures to recover \nthe underlying visual subspace in which the basis concepts and \nrepresentation are jointly optimized. Third, to obtain local rep-\nresentation, it uses an adaptive self-representative weighting \nstrategy to preserve the locality of representation and avoid the \ntricky issue of neighbor selection. We evaluated DSCF-Net for \nimage representation and clustering, and compared the results \nwith related single-layer and multilayer models. Extensive re-\nsults versified the effectiveness of DSCF-Net for representing \nand clustering images. In future, we will explore the strategy of \nupdating the basis vectors and new representation in each layer \njointly so that the representation will be more accurate.  \nACKNOWLEDGMENTS \nThis work is partially supported by National Natural Science \nFoundation of China (61672365, 61871444 and 61806035), \nthe Fundamental Research Funds for the Central Universities \nof China (JZ2019HGPA0102), and the Project Funded by the \nPriority Academic Program Development of Jiangsu Higher \nEducation Institutions. Zhao Zhang is corresponding author.  \nREFERENCES \n[1] \nD. Lee, H. Seung, “Learning the parts of objects by non-negative matrix \nfactorization,” Nature, vol.401, pp.788-791, 1999.  \n[2] \nX. Wei, Y. Gong, “Document clustering by concept factorization,” In: \nProceedings of ACM SIGIR conference on Research and Development \nin Information Retrieval, pp.202-209, 2004.  \n[3] \nD. Cai, X. F. He, J. Han, and T. Huang, “Graph regularized nonnegative \nmatrix factorization for data representation,” IEEE Trans. Pattern Anal. \nMach. Intell., vol.33, no.8, pp.1548-1560, 2011.  \n[4] \nD. Cai, X. He and J. Han, “Locally consistent concept factorization for \ndocument clustering,” IEEE Trans. Knowledge and Data Engineering, \nvol.23, no.6, pp.902-913, 2011.  \n[5] \nS. H. Ma, L. F. Zhang, W. B. Hu, Y. P. Zhang, J. Wu and X. L. Li, \n“Self-Representative Manifold Concept Factorization with Adaptive \nNeighbors for Clustering,” In: Proceedings of the International Joint \nConference on Artificial Intelligence, pp.2539-2545, 2018.  \n[6] \nF. Shang, L. Jiao and F. Wang, “Graph dual regularization non-negative \nmatrix factorization for co-clustering,” Pattern Recognition. vol.45, no.6, \npp.2237-2250, 2012.  \n[7] \nJ. Ye and Z. Jin, “Dual-graph regularized concept factorization for \nclustering,” Neurocomputing, vol.138, no.11, pp.120-130, 2014.  \n[8] \nH. Liu, Z. Yang, J. Yang, Z. Wu and X. Li, “Local coordinate concept \nfactorization for image representation,” IEEE Transactions on Neural \nNetworks and Learning Systems, vol.25, no.6, pp.1071-1082, 2014. \n[9] \nJ. Ye and Z. Jin, “Graph-Regularized Local Coordinate Concept Factori-\nzation for Image Representation,” Neural Processing Letters, vol.46, \nno.2, pp.427-449, 2017.  \n[10] H. Li, J. Zhang, and J. Liu, “Graph-regularized CF with local coordinate \nfor image representation,” Journal of Visual Communication & Image \nRepresentation, vol.49, pp.392-400, 2017.  \n[11] A. Cichocki, R. Zdunek, “Multilayer nonnegative matrix factorization,” \nElectron. Lett., vol.42, no.16, pp.947-948, Aug, 2006. \n[12] X. Li, C. X. Zhao, Z. Q. Shu and Q. Wang, “Multilayer Concept Factori-\nzation for Data Representation,” In: Proceedings of the International \nConference on Computer Science and Education, July 2015.  \n[13] R. Rajabi, and H. Ghassemian, “Spectral Unmixing of Hyperspectral \nImagery Using Multilayer NMF,” IEEE Geoscience and Remote Sensing \nLetters, vol.12, no.1, pp.38-42, 2015.  \n[14] X. Li, X. Shen, Z. Shu, Q. Ye, C. Zhao, “Graph regularized multilayer \nconcept factorization for data representation,” Neurocomputing, vol. 238, \npp. 139-151, 2017.  \n[15] Z.C. Li and J. Tang, “Weakly-supervised Deep Matrix Factorization for \nSocial Image Understanding,” IEEE Transactions on Image Processing, , \nvol.26, no.1, pp.276-288, 2017.  \n[16] G. Trigeorgis, K. Bousmalis, S. Zafeiriou, and B. W. Schuller, “A deep \nmatrix factorization method for learning attribute representations,” IEEE \nTransactions on Pattern Analysis and Machine Intelligence, vol.39, no. \n3, pp. 417-429, 2015.  \n[17] D. Cai, X. He, J. Han, “Document clustering using locality preserving \nindexing,” IEEE Transactions on Knowledge and Data Engineering, \nvol.17, no.12, pp.1624-1637, 2005.  \n[18] X.F. He, D. Cai and P. Niyogi, “Laplacian score for feature selection,” \nAdvances in neural information processing systems, 2006.  \n[19] H. Liu, G. Yang, Z. Wu, and D. Cai, “Constrained concept factorization \nfor image representation,” IEEE Transactions on Cybernetics, vol.44, \nno.7, pp.1214-1224, 2014.  \n[20] M. Sugiyama, “Dimensionality reduction of multimodal labeled data by \nlocal fisher discriminant analysis,” Journal of Machine Learning \nResearch, vol.8, pp.1027-1061, 2007.  \n[21] B.Weyrauch, J.Huang, B. Heisele and V. Blanz,“Component-based Face \nRecognition with 3D Morphable Models,” In: Proceedings of the First \nIEEE Workshop on Face Processing in Video, Washington, D. C., 2004. \n[22] B. Leibe, B. Schiele, “Analyzing appearance and contour based methods \nfor object categorization,” In: Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition, pp.409–415, 2003.  \n[23] S Nayar, S Nene, H Murase, “Columbia object image library (coil 100),” \nDepartment of Comp. Science, Columbia University, Tech. Rep. CUCS-\n006-96, 1996.  \n[24] I. Jolliffe, “Principal Component Analysis,” Springer Berlin, vol.87, \nno.100, pp.41-64, 1986.  \n[25] G. Golub, C. Reinsch, “Singular value decomposition and least squares \nsolutions,” Numerische mathematik, vol.14, no.5, pp.403-420, 1970.  \n[26] R. Gray, “Vector quantization,” IEEE Assp Magazine, vol.1, no.2, pp.4-\n29, 1984.  \n[27] Z. Zhang, Y. Zhang, S. Li, G. Liu, D. Zeng, S. Yan, M. Wang, \"Flexible \nAuto-weighted Local-coordinate Concept Factorization: A Robust \nFramework for Unsupervised Clustering,\" IEEE Trans. on Knowledge \nand Data Engineering, Sep 2019.   \n[28] Z. Zhang, Y. Zhang, G. Liu, J.i Tang, S. Yan and M. Wang, \"Joint Label \nPrediction based Semi-Supervised Adaptive Concept Factorization for \nRobust Data Representation,\" IEEE Transactions on Knowledge and \nData Engineering, Jan 2019.   \n[29] Y. Peng, R. Tang, W. Kong, J. Zhang, F.P. Nie and A.Cichocki, “Joint \nStructured Graph Learning and Clustering Based on Concept \nFactorization,” In: Proceedings of the IEEE International Conference on \nAcoustics, Speech, and Signal Processing, Brighton, UK, pp.3162-3166, \n2019.  \n[30] A. Acharya, R. Goel, A. Metallinou, I. S. Dhillon, “Online Embedding \nCompression for Text Classification Using Low Rank Matrix Factor-\nization,” In: Proceedings of AAAI Conference on Artificial Intelligence, \npp.6196-6203, 2019.  \n[31] Z. Zhang, Y. Zhang, S. Li, G. Liu, M. Wang and S. C. Yan, \"Robust \nUnsupervised Flexible Auto-weighted Local-Coordinate Concept \nFactorization for Image Clustering,\" In: Proceedings of the IEEE Inter-\nnational Conference on Acoustics, Speech, and Signal Processing, \nBrighton, UK, 2019, 2092-2096.  \n[32] T. Gao and C. Chu, “DID: Distributed Incremental Block Coordinate \nDescent for Nonnegative Matrix Factorization,” In: Proceedings of the \nAAAI Conference on Artificial Intelligence,  pp.2991-2998, 2018.  \n[33] T. Sim, S. Baker, M. Bsat, “The CMU pose, illumination, and expre- \nssion (PIE) database,” In: IEEE International Conference on Automatic \nFace and Gesture Recognition, pp.53-58, 2002.  \n[34] Z. Zhang, F. Li, M. Zhao, L. Zhang, and S. Yan, “Joint Low-Rank and \nSparse Principal Feature Coding for Enhanced Robust Representation \nand Visual Classification,” IEEE Trans. on Image Processing, vol.25, \nno.6, pp.2429-2443, 2016.  \n[35] W. Jiang, Z. Zhang, F. Li, L. Zhang, M.B. Zhao, and X. Jin, “Joint label \nconsistent dictionary learning and adaptive label prediction for \nsemisupervised machine fault classification,” IEEE Transactions on \nIndustrial Informatics, vol.12, no.1, pp.248-256, 2016.  \n[36] R. Kumar, R. Panigrahy, A. Rahimi, D. P. Woodruff, “Faster Algorithms \nfor Binary Matrix Factorization,” In: Proceedings of the International \nConference on Machine Learning, pp.3551-3559, 2019.  \n[37] S. Hess, N. Piatkowski, K. Morik, “The Trustworthy Pal: Controlling the \nFalse Discovery Rate in Boolean Matrix Factorization,” In: Proceedings \nof SIAM International Conference on Data Mining, pp.405-413, 2018.  \n[38] S. Karaev, J. Hook, P. Miettinen, “Latitude: A Model for Mixed Linear-\nTropical Matrix Factorization,” In: Proceedings of SIAM International \nConference on Data Mining, pp.360-368, 2018.  \n[39] K. Zhang, S. Zhang, J. Liu, J. Wang and J. Zhang, “Greedy Orthogonal \nPivoting Algorithm for Non-Negative Matrix Factorization,” In: Pro-\nceedings of the International Conference on Machine Learning (ICML), \npp.7493-7501, 2019.  \n[40] Z. Zhang, M. Zhao and T. Chow, \"Constrained Large Margin Local \nProjection Algorithms and Extensions for Multimodal Dimensionality \nReduction,\" Pattern Recognition, vol.46, iss.12, pp.4466-4493, 2012.   \n[41] Z. Zhang, J. Ren, S. Li, R. Hong, Z. Zha, M. Wang, \"Robust Subspace \nDiscovery by Block-diagonal Adaptive Locality-constrained Represent-\nation,\" In: Proceedings of ACM International Conference on Multimedia \n(ACM MM), Nice, France, pp.1569-1577, Oct 2019.  \n[42] Y. Zhang, Z. Zhang, S. Li, J. Qin, G.C. Liu, M. Wang and S.C. Yan, \n\"Unsupervised Nonnegative Adaptive Feature Extraction for Data \nRepresentation,\" IEEE Trans. Knowledge and Data Engineering (IEEE \nTKDE), vol.31, no.12, pp.2423-2440, Dec 2019. \n[43] Z. Zhang, W. Jiang, J. Qin, L. Zhang, F. Li, M. Zhang and S.C. Yan, \n\"Jointly Learning Structured Analysis Discriminative Dictionary and \nAnalysis Multiclass Classifier,\" IEEE Transactions on Neural Networks \nand Learning Systems, vol.29, no.8, pp.3798-3814, August 2018.  \n[44] Z. Zhang, F. Li, M. Zhao, L.Zhang and S. Yan, \"Robust Neighborhood \nPreserving Projection by Nuclear/L2,1-Norm Regularization for Image \nFeature Extraction,\" IEEE Transactions on Image Processing (IEEE \nTIP), vol.26, no.4, pp.1607-1622, April 2017.  \n[45] L. Wang, B. Wang, Z. Zhang, Q. Ye, L. Fu, G. Liu, M. Wang, \"Robust \nAuto-weighted Projective Low-Rank and Sparse Recovery for Visual \nRepresentation,\" Neural Networks (NN), vol.117, pp.201-215, 2019.  \n[46] Z. Li, Z. Zhang, J. Qin, Z. Zhang and L. Shao, \"Discriminative Fisher \nEmbedding Dictionary Learning Algorithm for Object Recognition,\" \nIEEE Transactions on Neural Networks and Learning Systems (IEEE \nTNNLS), April 2019.  \n[47] Y. Zhang, Z. Zhang, J. Qin, L. Zhang, B. Li and F. Li, \"Semi-Supervised \nLocal Multi-Manifold Isomap by Linear Embedding for Feature \nExtraction,\" Pattern Recognition (PR), vol.76, pp.662-678, 2018.   \n[48] G. Liu, Z. Zhang, Q. Liu and H. Xiong, \"Robust Subspace Clustering \nwith Compressed Data,\" IEEE Transactions on Image Processing (IEEE \nTIP), vol.28, no.10, pp.5161-5170, Oct 2019.   \n[49] Y. Ito, S. Oeda, and K. Yamanishi, “Rank Selection for Non-negative \nMatrix Factorization with Normalized Maximum Likelihood Coding,” \nIn: Proceedings of the SIAM International Conference on Data Mining, \npp.720-728, 2016.  \n[50] C. Liu, T. Zhang, J. Li, J. Yin, P. Zhao, J. Sun, S. C. H. Hoi, “Robust \nFactorization Machine: A Doubly Capped Norms Minimization,” In: \nProceedings of the SIAM International Conference on Data Mining, \npp.738-746, 2019.  \n \n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-12-13",
  "updated": "2019-12-29"
}