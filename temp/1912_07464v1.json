{
  "id": "http://arxiv.org/abs/1912.07464v1",
  "title": "Realization of spatial sparseness by deep ReLU nets with massive data",
  "authors": [
    "Charles K. Chui",
    "Shao-Bo Lin",
    "Bo Zhang",
    "Ding-Xuan Zhou"
  ],
  "abstract": "The great success of deep learning poses urgent challenges for understanding\nits working mechanism and rationality. The depth, structure, and massive size\nof the data are recognized to be three key ingredients for deep learning. Most\nof the recent theoretical studies for deep learning focus on the necessity and\nadvantages of depth and structures of neural networks. In this paper, we aim at\nrigorous verification of the importance of massive data in embodying the\nout-performance of deep learning. To approximate and learn spatially sparse and\nsmooth functions, we establish a novel sampling theorem in learning theory to\nshow the necessity of massive data. We then prove that implementing the\nclassical empirical risk minimization on some deep nets facilitates in\nrealization of the optimal learning rates derived in the sampling theorem. This\nperhaps explains why deep learning performs so well in the era of big data.",
  "text": "1\nRealization of spatial sparseness by deep ReLU nets\nwith massive data\nCharles K. Chui, Shao-Bo Lin, Bo Zhang, and Ding-Xuan Zhou\nAbstract—The great success of deep learning poses urgent\nchallenges for understanding its working mechanism and ra-\ntionality. The depth, structure, and massive size of the data\nare recognized to be three key ingredients for deep learning.\nMost of the recent theoretical studies for deep learning focus on\nthe necessity and advantages of depth and structures of neural\nnetworks. In this paper, we aim at rigorous veriﬁcation of the\nimportance of massive data in embodying the out-performance\nof deep learning. To approximate and learn spatially sparse\nand smooth functions, we establish a novel sampling theorem in\nlearning theory to show the necessity of massive data. We then\nprove that implementing the classical empirical risk minimization\non some deep nets facilitates in realization of the optimal learning\nrates derived in the sampling theorem. This perhaps explains why\ndeep learning performs so well in the era of big data.\nIndex Terms—Deep nets, Learning theory, Spatial sparseness,\nMassive data\nI. INTRODUCTION\nWith the rapid development of data mining and knowledge\ndiscovery, data of massive size are collected in various dis-\nciplines [50], including medical diagnosis, ﬁnancial market\nanalysis, computer vision, natural language processing, time\nseries forecasting, and search engines. These massive data\nbring additional opportunities to discover subtle data features\nwhich cannot be reﬂected by data of small size while creating\na crucial challenge on machine learning to develop learning\nschemes to realize beneﬁts by exploring the use of massive\ndata. Although numerous learning schemes such as distributed\nlearning [26], localized learning [32] and sub-sampling [14]\nhave been proposed to handle massive data, all these schemes\nfocused on the tractability rather than the beneﬁt of massive-\nness. Therefore, it remains open to explore the beneﬁts brought\nfrom massive data and to develop feasible learning strategies\nfor realizing these beneﬁts.\nDeep learning [18], characterized by training deep neural\nnetworks (deep nets for short) to extract data features by\nusing rich computational resources such as computational\npower of modern graphical processor units (GPUs) and custom\nprocessors, has made remarkable success in computer vision\n[23], speech recognition [24] and game theory [40], practically\nshowing its power in tackling massive data. Recent develop-\nments on deep learning theory also provide several exciting\nC. K. Chui and Bo Zhang are with Department of Mathematics, Hong\nKong Baptist University. C.K. Chui is also associated with the Department\nof Statistics, Stanford University, CA 94305, USA. Shao-Bo Lin is with\nthe Center of Intelligent Decision-making and Machine Learning, School of\nManagement, Xi’an Jiaotong University, Xi’an, China. D. X. Zhou is with\nSchool of Data Science and Department of Mathematics, City University of\nHong Kong,\nHong Kong. The corresponding author is S. B. Lin (email:\nsblin1983@gmail.com).\n  \n                  \n \n \n     \n\n\n\n\n\n\n\n\n\n(a) Limitations for small data\n  \n                  \n \n \n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Advantages for massive data\nFig. 1. The role of data size in realizing spatial sparsity\ntheoretical results to explain the efﬁciency and rationality\nof deep learning. In particular, numerous data features such\nas manifold structures of the input space [38], piecewise\nsmoothness [36], rotation-invariance [5] and sparseness in\nthe frequency domain [37] were proved to be realizable by\ndeep nets but cannot be extracted by shallow neural networks\n(shallow nets for short) with same order of free parameters.\nAll these interesting studies theoretically verify the necessity\nof depth in deep learning. The problem is, however, they do\nnot provide any explanations on why deep learning works so\nwell for big data.\nOur purpose is not only to pursue the power of depth in deep\nlearning, but also to show the important role of the data size\nin embodying advantages of deep nets. To this end, we aim at\nﬁnding data feature (or function) that is difﬁcult to be reﬂected\nby data of small size, but is easily captured by massive data.\nThe spatially sparse feature (or function) naturally comes into\nour sights. As demonstrated in Figure 1, if a function is\nsupported on the orange range, then small data content as\nshown in Figure 1 (a) cannot capture the spareness of the\nsupport. It requires at least one sample point in each sub-cube\nto reﬂect the spatial spareness as shown in Figure 1(b). Such a\nspatially sparse assumption abounds in numerous application\nregions such as computer vision [41], signal processing [11]\nand pattern recognition [19], and several special deep nets have\nbeen designed to extract spatially sparse features of data [13].\nDue to the limitation of small size data-sets in reﬂecting\nthe spatial sparseness as shown in Figure 1, this paper is\ndevoted to deriving the quantitative requirement of the data\nsize to extract the spatial sparseness. In particular, we prove\nexistence of some learning scheme that can reﬂect both the\nsmoothness and spatial sparseness, provided that the data size\nachieves a certain level. This ﬁnding coincides with the well-\nknown sampling theorem in compressed sensing [10]. We\nthen reformulate our sampling theorem in the framework of\narXiv:1912.07464v1  [cs.LG]  16 Dec 2019\n2\nlearning theory [8] by highlighting the important role of data\nsize in deriving optimal learning rates for learning smooth and\nspatially sparse functions. The established sampling theorem in\nlearning theory theoretically veriﬁes the necessity of massive\ndata in sparseness-related applications and shows that massive\ndata can extract some data features that cannot be reﬂected by\ndata of small size.\nBy applying the piecewise linear and continuous property of\nthe rectiﬁer linear unit (ReLU) function, σ(t) := max{0, t},\nwe construct a deep net with two hidden layers and ﬁnitely\nmany neurons to provide a localized approximation, which\nis beyond the capability of shallow nets [3], [35], [4]. The\nlocalized approximation of deep nets highlights their power in\ncapturing the position information of data inputs. A direct con-\nsequence is that deep nets can reﬂect the spatially sparse func-\ntions [29]. This property, together with the recently developed\napproaches in approximating smooth function by deep nets\n[44], [36], [17], give rise to the feasibility of adopting deep\nnets to extracting smoothness and spatial sparseness simulta-\nneously. We succeed in deriving almost optimal learning rates\nfor implementing empirical risk minimization (ERM) on deep\nnets and proving that up to a logarithmic factor, the derived\nlearning rates coincide with those of the sampling theorem.\nIn other words, our results theoretically verify the beneﬁts of\nmassiveness of data in learning smooth and spatially sparse\nfunctions, and that deep learning is capable of embodying\nadvantages of massive data.\nThe rest of this paper is organized as follows. In Section\nII, we show the popularity of spatially sparse functions and\npresent the sampling theorem for learning smooth and spatially\nsparse functions. In Section III, we provide the advantage\nof deep nets in embodying the beneﬁts of massive data via\nshowing the optimal learning rates for ERM on deep nets.\nIn Section IV, we establish upper bounds of the sampling\ntheorem and learning rates for ERM on deep nets. In Section\nV, we present the proofs for the lower bounds.\nII. SAMPLING THEOREM FOR REALIZING SPATIALLY\nSPARSE AND SMOOTH FEATURES\nIn this section, we discuss the beneﬁts of massive data via\npresenting a sampling theorem in the framework of learning\ntheory.\nA. Spatially sparse and smooth functions\nSpatial sparseness is a popular data feature which abounds\nin numerous applications such as handwritten digit recognition\n[7], magnetic resonance imaging (MRI) analysis [1], image\nclassiﬁcation [43] and environmental data processing [9].\nDifferent from other sparseness measurements such as the\nsparseness in the frequency domain [25], [37] and the manifold\nsparseness [4], spatial sparseness depends heavily on partitions\nof the input space. Considering handwritten digit recognition\nas an example, Figure 2 (a) shows that the handwritten digit is\nnot sparse if the partition level is 4. However, if the partition\nlevel achieves 16 as shown in Figure 2 (b), the handwritten\ndigit is sparse.\n(a) Non-sparseness for partitions\n(b) Sparseness for partitions\nFig. 2. The role of partition in reﬂecting the spatial sparsity\nBased on this observation, we present the following deﬁni-\ntion of spatially sparse functions (see, [29]). Let Id := [0, 1]d\nand N ∈N. Partition Id by N d sub-cubes {Aj}N d\nj=1 of side\nlength N −1 and with centers {ζj}Nd\nj=1. For s ∈N with s ≤N d,\nΛs :=\n\b\njℓ: jℓ∈{1, 2, . . . , N d}, 1 ≤ℓ≤s\n\t\n,\nand consider a function f deﬁned on Id, if the support of f is\ncontained in S := ∪j∈ΛsAj for a subset Λs of {1, 2, . . . , N d}\nof cardinality at most s. We then say that f is s-sparse in\nN d partitions. In what follows, we take Λs to be the smallest\nsubset that satisﬁes this condition.\nBesides the spatial sparseness, we also introduce the smooth\nproperty of f, which is a widely used a-priori assumption\n[44], [36], [28], [4], [47]. Let c0 > 0 and r = u + v with\nu ∈N0 := {0} ∪N and 0 < v ≤1. We say that a function\nf : Id →R is (r, c0)-smooth if f is u-times differentiable and\nfor any α = (α1, · · · , αd) ∈Nd\n0 with α1 + · · · + αd = u and\nx, x′ ∈Id, its partial derivative, denoted by\nf (u)\nα (x) =\n∂uf\n∂xα1\n1 . . . ∂xαd\nd\n(x),\nsatisﬁes the Lipschitz condition\n\f\f\ff (u)\nα (x) −f (u)\nα (x′)\n\f\f\f ≤c0∥x −x′∥v,\n(1)\nwhere ∥x∥denotes the Euclidean norm of x. Denote by\nLip(r,c0) the family of (r, c0)-smooth functions satisfying (1)\nand by Lip(N,s,r,c0) the set of all f ∈Lip(r,c0) which are\ns-sparse in N d partitions.\nB. Sampling theorem for realizing spatially sparse and smooth\nfeatures\nWe conduct the analysis in a standard least-square regres-\nsion framework [8], in which samples D = {(xi, yi)}m\ni=1\nare drawn independently according to an unknown Borel\nprobability measure ρ on Z = X × Y with X = Id and\nY ⊆[−M, M] for some M\n> 0. The objective is the\nregression function deﬁned by\nfρ(x) =\nZ\nY\nydρ(y|x),\nx ∈X,\nwhich minimizes the generalization error\nE(f) :=\nZ\nZ\n(f(x) −y)2dρ,\nwhere ρ(y|x) denotes the conditional distribution at x induced\nby ρ. Let ρX be the marginal distribution of ρ on X and\n3\n(L2\nρX , ∥·∥ρ) denote the Hilbert space of ρX square-integrable\nfunctions on X. Then for f ∈L2\nρX, it follows, in view of [8],\nthat\nE(f) −E(fρ) = ∥f −fρ∥2\nρ.\n(2)\nIf fρ is supported on S but ρX is supported on Id\\S,\nit is impossible to derive a satisfactory learning rate, im-\nplying the necessity of restrictions on ρX. In this section,\nwe assume ρX is the uniform distribution for the sake of\nbrevity. Our result also holds under the classical distortion\nassumption on ρX [46]. Denote by M(N, s, r, c0) the set of\nall distributions satisfying that ρX is the uniform distribution\nand fρ ∈Lip(N,s,r,c0). We enter into a competition over all\nestimators ΨD : D →fD and deﬁne\ne(N, s, r, c0) :=\nsup\nρ∈M(N,s,r,c0)\ninf\nfD∈ΨD E(∥fρ −fD∥2\nρ).\nThe following theorem is our ﬁrst main result.\nTheorem 1. Let r, c0 > 0, d, s, N, m ∈N with s ≤N d. If\nm\nlog m ≥C∗N 2r+2d\ns\n,\n(3)\nthen\nC1m−\n2r\n2r+d\n\u0010 s\nN d\n\u0011\nd\n2r+d ≤e(N, s, r, c0)\n≤\nC2\n\u0012\nm\nlog m\n\u0013−\n2r\n2r+d \u0010 s\nN d\n\u0011\nd\n2r+d ,\n(4)\nwhere C∗, C1, C2 are constants independent of m, s or N.\nThe proof of Theorem 1 will be given in Sec. V. The\nsampling theorem [39] originally focuses on deriving the\nminimal sampling rate that permits a discrete sequence of\nsamples to capture all the information from a continuous-\ntime signal of ﬁnite bandwidth in sampling processes. Recent\ndevelopments [45] imitate the sampling theorem in terms of\nderiving minimal sizes of samples to represent a signal via\nsome transformations such as wavelet, Fourier and Legendre\ntransformations. In learning theory, the sampling theorem\nstudied in this paper aims at deriving minimal sizes of samples\nthat can achieve the optimal learning rates for some speciﬁed\nlearning task. Theorem 1 shows that optimal learning rates for\nlearning spatially sparse and smooth functions are achievable\nprovided (3) holds. The size of samples, as governed in (3),\ndepends on the sparsity level s and partitions numbers N,\nand increases with respect to N, showing that more partitions\nrequire more samples. This coincides with the intuitive ob-\nservation as shown in Figure 1. Different from the classical\nresults in signal processing [45], the size of samples in (3)\ndecreases with s. This is not surprising, since the established\noptimal learning rates in (4) increase with s. In other words,\nthe size of samples in our result is to recognize the support of\nthe regression function and thus increases with N while the\nsparsity s is reﬂected by optimal learning rates in (4).\nThe almost optimal learning rate in (4) can be regarded as a\ncombination of two components m−\n2r\n2r+d for the smoothness\nand\n\u0000 s\nN d\n\u0001\nd\n2r+d for the sparseness. If s = N d, meaning that\nfρ is not spatially sparse, then the learning rate derived in\nTheorem 1 coincides with the optimal learning rate in learning\nsmooth functions ([16, Chap. 3]), up to a logarithmic factor. If\nr is extremely small, the learning rate derived in Theorem 1,\nnear to\ns\nNd due to the uniform assumption on ρX, is also the\noptimal learning rates for learning spatially sparse functions. If\nm is relatively small with respect to N, i.e. (3) does not hold,\nthen while the smoothness part m−\n2r\n2r+d can be maintained,\nthe sparseness property cannot be captured. This shows the\nbeneﬁt of massive data in learning spatially sparse functions.\nIt should be noted that there is an additional logarithmic term\nin (4). We believe that it is removable by using different tools\nfrom this paper and will consider it as a future work.\nIII. DEEP NETS IN REALIZING SPATIAL SPARENESS\nIn this section, we verify the power of depth for ReLU nets\nin localized approximation and spatially sparse approximation,\nand then show that deep nets are able to embody the beneﬁts of\nmassive data in learning spatially sparse and smooth functions.\nA. Deep ReLU nets\nOne of the main reasons for the great success of deep\nlearning is the implementation in terms of deep nets. In\ncomparision with the classical shallow nets, deep nets are\nsigniﬁcantly better in providing localized approximation [3],\nmanifold learning [38], [4], realizing rotation invariance priors\n[30], [5], embodying sparsity in the frequency domain [25],\n[37] and in the spatial domain [29], approximating piecewise\nsmooth functions [36] and capturing the hierarchical structures\n[34], [22] etc.. However, all these interesting results are not\nyet sufﬁcient to explain why deep nets perform well in the era\nof big data.\nLet σ(t) := max{t, 0} be the rectiﬁer liner unit (ReLU).\nDeep ReLU nets, i.e. deep nets with the ReLU activation\nfunction, is most popular in current research in deep learning.\nDue to the non-differentiable property of ReLU, it seems\ndifﬁcult for ReLU nets to approximate smooth functions at the\nﬁrst glance. However, it was shown in [44], [36], [48], [17]\nthat increasing the depth of ReLU nets succeeds in overcoming\nthis problem and thus provides theoretical foundations in\nunderstanding deep ReLU nets.\nDenote x = (x(1), . . . , x(d)) ∈Id. Let L ∈N and\nd0, d1, . . . , dL ∈N with d0 = d. For ⃗h = (h(1), . . . , h(dk))T ∈\nRdk, deﬁne ⃗σ(⃗h) = (σ(h(1)), . . . , σ(h(dk)))T . Deep ReLU\nnets with depth L and width dj in the j-th hidden layer can\nbe mathematically represented as\nh{d0,...,dL,σ}(x) = ⃗a · ⃗hL(x),\n(5)\nwhere\n⃗hk(x) = ⃗σ(Wk · ⃗hk−1(x) +⃗bk),\nk = 1, 2, . . . , L,\n(6)\n⃗h0(x) = x, ⃗a ∈RdL, ⃗bk ∈Rdk, and Wk = (W i,j\nk )dk,dk−1\ni=1,j=1 is a\ndk ×dk−1 matrix. Denote by H{d0,...,dL,σ} the set of all these\ndeep ReLU nets. The structures of deep nets are reﬂected by\nweight matrices Wk and threshold vectors ⃗bk, k = 1, . . . , L.\nFor example, taking the special form of Toeplitz-type weight\nmatrices leads to the deep convolutional nets [47], [48], [49],\nfull matrices correspond to deep fully connected nets [12], and\n4\ntree-type sparse matrices imply deep nets with tree structures\n[5], [6]. In this paper, we do not focus on the structure selection\nof deep nets, but rather on the existence of some deep net\nstructure for realization of the sampling theorem established\nin Theorem 1.\nB. Deep ReLU nets for localized approximation\nLocalized approximation is an important property of neural\nnetworks in that it is a crucial step-stone in approximating\npiecewise smooth functions [36] and spatially sparse functions\n[29]. The localized approximation of a neural network allows\nthe target function to be modiﬁed in any small region of the\nEuclidean space by adjusting a few neurons, rather than the\nentire network. It was originally proposed in [3, Def. 2.1] to\ndemonstrate the power of depth for deep nets with sigmoid-\ntype activation functions. The main conclusion in [3] is that\ndeep nets only with two hidden layers and 2d+1 neurons can\nprovide localized approximation, while shallow nets fail for\nd ≥2, even for the most simple Heaviside activation function.\nIn this section, we prove that deep ReLU nets with two hidden\nlayers and 4d + 1 neurons are capable of providing localized\napproximation.\nFor a, b ∈R with a < b, deﬁne a trapezoid-shaped function\nTτ,a,b with a parameter 0 < τ ≤1 as\nTτ,a,b(t) := 1\nτ\n\b\nσ(t −a + τ) −σ(t −a)\n−\nσ(t −b) + σ(t −b −τ)\n\t\n.\n(7)\nThen the deﬁnition of σ yields\nTτ,a,b(t) =\n\n\n\n\n\n\n\n1,\nif a ≤t ≤b,\n0,\nif t ≥b + τ, or t ≤a −τ,\nb+τ−t\nτ\n,\nif b < t < b + τ,\nt−a+τ\nτ\n,\nif a −τ < t < a.\n(8)\nWe may then consider\nNa,b,τ(x) := σ\n\n\nd\nX\nj=1\nTτ,a,b(x(j)) −(d −1)\n\n.\n(9)\nThe following proposition presents the localized approxima-\ntion property of Na,b,τ.\nProposition 1. Let a < b, 0 < τ ≤1 and Na,b,τ be deﬁned\nby (9). Then we have 0 ≤Na,b,τ(x) ≤1 for all x ∈Id and\nNa,b,τ(x) =\n\u001a 0,\nif x /∈[a −τ, b + τ]d,\n1,\nif x ∈[a, b]d.\n(10)\nThe proof of Proposition 1 will be postponed to Section IV.\nSimilar approximation results for deep nets with sigmoid-type\nactivation functions and 2d + 1 neurons have been established\nin [3], [38], [29]. The representation in Proposition 1 is better\nbecause the expression for x ∈[a, b]d and x /∈[a−τ, b+τ]d is\nexact. For arbitrary N ∗∈N, partition Id into (N ∗)d sub-cubes\n{Bk}(N∗)d\nk=1\nof side length 1/N ∗and with centers {ξk}(N ∗)d\nk=1 .\nWrite ˜Bk,τ := [ξk + [−1/(2N ∗) −τ, 1/(2N ∗) + τ]]d ∩Id. It\nis obvious that Bk ⊂˜Bk,τ. Deﬁne N1,N ∗,ξ,τ : Id →R for\nξ ∈Id by\nN1,N ∗,ξ,τ(x) = N−1/(2N∗),1/(2N∗),τ(x −ξ).\n(11)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n(a) τ = 0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n(b) τ = 0.05\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n(c) τ = 0.01\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n(d) τ = 0.005\nFig. 3. The localized approximation based on a cubic partition of [0, 1]2 with\nside length 1/8 for the deep net constructed in (11) with ξ = (3/16, 5/16)\nIn\nview\nof\nProposition\n1,\n(8)\nand\n(11),\nwe\nhave\n|N1,N ∗,ξk,τ(x)| ≤1 for all x ∈Id, k ∈{1, . . . , (N ∗)d} and\nN1,N ∗,ξk,τ(x) =\n\u001a\n0,\nif x /∈˜Bk,τ,\n1,\nif x ∈Bk.\n(12)\nAs shown in Figure 3, the parameter τ determines the size of\n˜Bk,τ, and thus affects the performance of localized approxi-\nmation for the constructed deep nets in (11). However, it does\nnot mean the smaller τ the better, since the norms of weights\ndecrease with respect to τ, which may result in extremely large\ncapacity of deep ReLU nets for too small τ.\nC. Deep ReLU nets for spatially sparse approximation\nThe localized approximation established in Proposition 1\ndemonstrates the power of deep ReLU nets with two hidden\nlayers to recognize some spatial information of the input.\nA direct consequence is that deep ReLU nets succeed in\ncapturing the spatially sparse property of functions and also\nmaintaining the capability of deep ReLU nets in approximating\nsmooth functions. On one hand, spatial sparseness deﬁned\nin this paper is built upon a cubic partition of Id, i.e.\nId = ∪Nd\nj=1Aj. If N ∗≥N, then Aj ⊆∪k:Aj∩Bk̸=∅can\nbe recognized by the localized approximation of N1,N ∗,ξk,τ.\nFigure 4 demonstrates that for small enough τ, summations of\nN1,N ∗,ξk,τ with different k can reﬂect the spatial sparseness\nfor N ∗= N. On the other hand, due to the localized approx-\nimation of N1,N ∗,ξk,τ(x), for any x ∈Id, there is at most 2d\nindices kj with N1,N ∗,ξkj ,τ(x) = 1 for j = 1, 2, . . . , 2d and\n|N1,N ∗,ξk,τ(x)| extremely small for k ̸= kj. Then, for large\nenough N ∗, the smoothness of f leads to small approximation\nerror for\n\f\f\ff(x) −P(N∗)d\nk=1\nf(ξk)N1,N ∗,ξk,τ(x)\n\f\f\f.\nWith the above observations, we ﬁnd that deep ReLU\nnets are capable of realizing both the smoothness and spatial\nsparseness, which is beyond the capability of shallow ReLU\nnets [3], [44]. The following proposition is the main result in\nthis subsection.\n5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n(a) τ = 0.1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n(b) τ = 0.05\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n(c) τ = 0.01\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n(d) τ = 0.005\nFig. 4. Realizing spatial sparseness by summations of the deep net constructed\nin (11) with different k\nProposition 2. Let 1 ≤p < ∞, r, c0 > 0, N, s, d ∈N with\ns ≤N d and N ∗≥max{4N, ˜C}. Then there exists a deep\nReLU net structure with ⌈25 + 4r/d + 2r2/d + 10r⌉inner\nlayers and at most C∗\n1(N ∗)d free parameters, such that for\nany f ∈Lip(N,s,r,c0) and any 0 < τ ≤\ns\n2Nd(N ∗)1+pr , there is\na deep ReLU net N3,N ∗,τ with the aforementioned structure\nand free parameters bounded by\n˜B∗:= C3 max{1/τ, (N ∗)(2d+r)γ}.\n(13)\nsuch that\n∥f −N3,N ∗,τ∥Lp(Id) ≤C4(N ∗)−r \u0010 s\nN d\n\u00111/p\n,\n(14)\nand\n∥N3,N ∗,τ∥L∞(Id) ≤C5,\n(15)\nwhere γ, ˜C, C∗\n1, C3, C4, C5 are constants depending only on\nc0, r, d and ∥f∥L∞(Id).\nThe proof of Proposition 2 will be given in Section IV. Ap-\nproximating functions in Lip(r,c0) is a classical topic in neural\nnetwork approximation. It is shown in [33] that for shallow\nnets with C∞sigmoid type activation functions and (N ∗)d\nfree parameters, an approximation rate of order (N ∗)−r can\nbe achieved. Furthermore, [31], [27] provide a lower bound.\nAlthough these nice results show the excellent approximation\ncapability of shallow nets, the weights of shallow nets in\n[33], [31] are extremely large, resulting in extremely large\ncapacity. With such extremely large weights, it follow from\nthe results in [31], [20] that there exists a deep net with\ntwo hidden layers and ﬁnitely many neurons possessing the\nuniversal approximation property. The extremely large weights\nproblem can be avoided by deepening the neural networks. In\nfact, it can be found in [44], [36], [17] that similar results hold\nfor deep ReLU nets with a few hidden layers and controllable\nweights, i.e., weights increasing polynomially fast with respect\nto the number of free parameters. Our Proposition 2 also\nimplies this ﬁnding by setting s = (N ∗)d and larger value\nof τ. It will be shown in the next subsection that controllable\nweights play a crucial role in deriving small variance as well\nas fast learning rates for implementing ERM on deep ReLU\nnets.\nThe approximation rates established in (14) not only reveal\nthe power of depth in approximating smooth functions, but\nalso exhibit the advantage of deep ReLU nets in embodying\nthe spatial sparseness by means of multiplying an additional\n\u0000 s\nN d\n\u00011/p on the optimal approximation rates (N ∗)−r for\nsmooth functions. Noting that shallow nets with the Heaviside\nactivation function [3] cannot provide localized approximation,\ncorresponding to a special case of s = 1, Proposition 2 show\nthe power of depth of deep ReLU net under the condition\nN ∗≥4N.\nD. Realizing optimal learning rates in sampling theorem by\ndeep nets\nIn this subsection, we aim at developing a learning scheme\nto take advantage of the power of deep ReLU nets in realizing\nthe spatial sparseness and smoothness. Denote by Hn,L the\ncollection of deep nets that possess the structure in Proposition\n2 with\nL = ⌈25 + 4r/d + 2r2/d + 10r⌉,\nand\nn = C∗\n1(N ∗)d.\n(16)\nDeﬁne\nHn,L,R := {hn,L ∈Hn,L : |wi,j\nk |, |bi\nk|, |ai| ≤R,\n1 ≤i ≤dk, 1 ≤j ≤dk−1, 1 ≤k ≤L},\n(17)\nwhere\nR := C3 max\n(\n2N d(N ∗)1+pr\ns\n, (N ∗)2γd+γr\n\u0012N d\ns\n\u0013γ/p)\n.\n(18)\nThen, it is easy to verify that\nN3,N ∗,τ ∈Hn,L,R\n(19)\nwith τ =\ns\n2Nd(N ∗)1+pr .\nWe consider the generalization error estimates for imple-\nmenting ERM on Hn,L,R as follows:\nfD,n,L := arg minf∈Hn,L,R\n1\nm\nm\nX\ni=1\n[f(xi) −yi]2.\n(20)\nSince |yi| ≤M, it is natural to project the ﬁnal output\nfD,n,L to the interval [−M, M] by the truncation operator\nπMfD,n,L(x) := sign(fD,n,L(x)) min{|fD,n,L(x)|, M}.\nLet p ≥2 and Jp be the identity mapping\nLp(X)\nJp\n−→L2\nρX.\nand DρX,p = ∥Jp∥. Then DρX,p is called the distortion\nof ρX (with respect to the Lebesgue measure) [46], which\nmeasures how much ρX distorts the Lebesgue measure. In\nour analysis, we assume DρX,p < ∞, which holds for the\nuniform distribution for all p ≥2 obviously. According to the\ndeﬁnition, for each f ∈L2\nρX ∩Lp(Id), we have\n∥f∥ρ ≤DρX,p∥f∥Lp(X).\n(21)\n6\nThe following theorem with proof to be given in Section V\nshows that the simple ERM strategy (20) based on deep ReLU\nnets has the capability of realizing the optimal learning rates\nestablished in Theorem 1.\nTheorem 2. Let fD,n,L be deﬁned by (20) with L, n satisfying\n(16) and R satisfying (18). Suppose that\n(N ∗)2r+d ∼m\n\u0010 s\nN d\n\u0011 2\np / log m,\n(22)\nand\nm\nlog m ≥C∗N\n2d+2rp+dp\n(2r+d)p\ns\n2\n2rp+dp\n.\n(23)\nThen\nC1m−\n2r\n2r+dd\n\u0010 s\nN d\n\u0011\nd\n2r+d\n≤\nsup\nfρ∈Lip(N,s,r,c0) E {E(πMfD,n,L∗) −E(fρ)}\n≤\nC6\n\u0012\nm\nlog m\n\u0013 −2r\n2r+d \u0010 s\nN d\n\u0011 2\np −\n2r\n2r+d ,\n(24)\nwhere C1, C6 are constants independent of m, s, or N and\na ∼b with a, b ≥0 denotes that there exist positive absolute\nconstants ˆC1, ˆC2 such that ˆC1a ≤b ≤ˆC2a.\nIf ρX is the uniform distribution, then (21) holds with p = 2\nand DρX,p = 1. Hence, if fρ ∈Lip(N,s,r,c0), we have\nE {E(πMfD,n,L∗) −E(fρ)}\n≤\nC6m−\n2r\n2r+d (log m)2r/(r+d) \u0010 s\nN d\n\u0011\nd\n2r+d ,\n(25)\nwhich coincides with the optimal learning rates in Theorem\n1 up to a logarithmic factor. Theorem 2 thus presents a\ntheoretical veriﬁcation on the success of deep learning in\nspatial sparseness related applications for massive data. In\nparticular, it presents an intuitive explanation on why deep\nlearning performs so well in handwritten digit recognition [2].\nAs shown in Figure 2, high-resolution of a ﬁgure implies large\nsize of data, which admits an extremely large partitions for the\ninput space with small sparsity s. Then, the additional term\n\u0000 s\nNd\n\u0001\nd\n2r+d in Theorem 2 yields a small generalization error.\nLearning spatially sparse and smooth functions was ﬁrst\nstudied in [29] and similar learning rate as that in Theorem 2\nhas been derived. In comparison with [29], there are three\nnovelties of our work. The ﬁrst is that we deduce lower\nbounds for learning these functions and show the optimality\nfor the derived learning rates, while [29] only focused on upper\nbounds. The second is that the range of r in our study is r > 0,\nwhile that in [29] is 0 < r ≤1. In view of the discussion\nin [44], the depth is necessary for extending the range from\n0 < r ≤1 to r > 0. Thus, more layers are required in our\nanalysis to show the advantage of deep nets. We would like to\npoint out that the activation function in the present paper is the\nwidely used ReLU function, while the activation functions in\n[29] are hybrid functions including the Heaviside function in\nthe ﬁrst layer and continuous sigmoid-type function in other\nlayers.\nIV. UPPER BOUND ESTIMATES\nThis section is devoted to the proof of Proposition 1,\nProposition 2, and the upper bounds in (24) and (4). It should\nbe noted that the upper bound in (4) is a direct corollary of\nthe upper of (24), with p = 2.\nA. Proofs of Proposition 1\nProof of Proposition 1:\nFor x ∈Id, it follows from\n(8) that 0 ≤Tτ,a,b(x(j)) ≤1 for any j ∈{1, . . . , d}. This\nimplies that Pd\nj=1 Tτ,a,b,(x(j)) ≤d and consequently 0 ≤\nNa,b,τ(x) ≤1. If x /∈[a−τ, b+τ]d, there is at least one j0 ∈\n{1, . . . , d} such that x(j0) /∈[a−τ, b+τ]. This together with (8)\nshows that Tτ,a,b(x(j0)) = 0. Therefore Pd\nj=1 Tτ,a,b(x(j)) ≤\nd−1, which implies Na,b,τ(x) = 0. If x ∈[a, b]d, then x(j) ∈\n[a, b] for every j ∈{1, . . . , d}. Hence, it follows from (8) that\nTτ,a,b(x(j)) = 1 for every j ∈{1, . . . , d}, which implies that\nPd\nj=1 Tτ,a,b(x(j)) = d and Na,b,τ(x) = 1. This completes the\nproof of Proposition 1.\nB. Proof of Proposition 2\nBefore presenting the proof of Proposition 2, we need\nseveral lemmas. The ﬁrst one can be found in [21, Lemma\n1].\nLemma 1. Let r = u + v with u ∈N0 and 0 < v ≤1. If\nf ∈Lip(r,c0), x0 ∈Rd and pu,x0,f is the Taylor polynomial\nof f with degree u at x0, i.e.,\npu,x0,f(x)\n=\nX\nk1+···+kd≤u\n1\nk1! . . . kd!\n∂k1+···+kdf(x0)\n∂k1x(1) . . . ∂kdx(d)\n(x(1) −x(1)\n0 )k1 · · · (x(d) −x(d)\n0 )kd,\n(26)\nthen\n|f(x) −pu,x0,f(x)| ≤c1∥x −x0∥r,\n∀x ∈Id,\n(27)\nwhere c1 is a constant depending only on r, c0 and d.\nFor τ > 0, deﬁne the localized Taylor polynomials by\nN2,N ∗,τ(x) :=\n(N∗)d\nX\nk=1\npu,ξk,f(x)N1,N ∗,ξk,τ(x),\n(28)\nwhere N1,N ∗,ξk,τ is given in (11). In the following lemma, we\npresent an upper bound estimate for approximating functions\nin Lip(N,s,r,c0) by N2,N ∗,τ.\nLemma 2. Let 1 ≤p < ∞and N ∗≥4N. If f ∈Lip(N,s,r,c0)\nwith N, s ∈N, r > 0 and c0 > 0, then for any 0 < τ ≤\ns\n2N d(N∗)1+pr , it follows that\n∥f −N2,N ∗,τ∥Lp(Id) ≤c2(N ∗)−r \u0010 s\nN d\n\u00111/p\n(29)\nand\n∥N2,N ∗,τ∥L∞(Id) ≤c3,\n(30)\nwhere c2, c3 are constants dependent only on d, r, c0 and\n∥f∥L∞(Id).\n7\nProof: Observe that Id = S(N∗)d\nk=1\nBk. Then for each x ∈\nId, let kx be the smallest k such that x ∈Bkx Note that kx is\nunique (the last restriction is for those points x on boundaries\nof cubes Bk). It follows from (28) that\nf(x) −N2,N ∗,τ(x)\n=\nf(x) −\n(N ∗)d\nX\nk=1\npu,ξk,f(x)N1,N ∗,ξk,τ(x)\n=\nf(x) −pu,ξkx,f(x) −\nX\nk̸=kx\npu,ξk,f(x)N1,N ∗,ξk,τ(x)\n+\npu,ξkx,f(x)[1 −N1,N ∗,ξkx,τ(x)].\nBut (12) implies that 1 −N1,N ∗,ξkx,τ(x) = 0. Thus,\n∥f −N2,N ∗,τ∥Lp(Id) ≤\n\r\rf −pu,ξkx,f\n\r\r\nLp(Id)\n+\n\r\r\r\r\r\r\nX\nk̸=kx\npu,ξk,f(x)N1,N ∗,ξk,τ(x)\n\r\r\r\r\r\r\nLp(Id)\n.\n(31)\nWe ﬁrst estimate the ﬁrst term on the right-hand side of (31).\nFor j ∈Λs, deﬁne\n˜Λj := {k ∈{1, . . . , (N ∗)d} : Bk ∩Aj ̸= ∅}.\n(32)\nSince {Aj}Nd\nj=1 and {Bk}(N∗)d\nk=1\nare cubic partitions of Id and\nN ∗≥4N, we have\n|˜Λj| ≤\n\u0012N ∗\nN + 2\n\u0013d\n≤\n\u00122N ∗\nN\n\u0013d\n,\n∀j ∈Λs.\n(33)\nIn view of (32), we obtain\nId ⊆\n\n[\nj∈Λs\n\n[\nk∈˜Λj\nBk\n\n\n\n[\n\n\n\n\n[\nk∈{1,...,(N∗)d}\\(∪j∈Λs ˜Λj)\nBk\n\n\n\n.\n(34)\nThen,\n\r\rf −pu,ξkx,f\n\r\r\nLp(Id) =\nZ\nId\n\f\ff(x) −pu,ξkx,f(x)\n\f\fp dx\n≤\n\nX\nj∈Λs\nX\nk∈˜Λj\n+\nX\nk∈{1,...,(N∗)d}\\(∪j∈Λs ˜Λj)\n\n\nZ\nBk\n\f\ff(x) −pu,ξkx,f(x)\n\f\fp dx.\n(35)\nFrom (32) again, for any k ∈{1, . . . , (N ∗)d}\\(∪j∈Λs ˜Λj),\nwe have Bk ∩S = ∅, which together with (26) and f ∈\nLip(N,s,r,c0) yields f(x) = pu,ξkx,f(x) = 0 for x ∈Bk.\nHence,\nX\nk∈{1,...,(N∗)d}\\(∪j∈Λs ˜Λj)\nZ\nBk\n\f\ff(x) −pu,ξkx,f(x)\n\f\fp dx = 0.\n(36)\nSince f ∈Lip(N,s,r,c0), it follows from Lemma 1 and (33)\nthat\nX\nj∈Λs\nX\nk∈˜Λj\nZ\nBk\n\f\ff(x) −pu,ξkx,f(x)\n\f\fp dx\n≤\ncp\n1\nX\nj∈Λs\nX\nk∈˜Λj\nZ\nBk\n∥x −ξkx∥prdx\n≤\ncp\n12ddpr/2(N ∗)−pr s\nN d .\n(37)\nInserting (37) and (36) into (35), we obtain\n\r\rf −pu,ξkx,f\n\r\r\nLp(Id) ≤c12d/pdr/2(N ∗)−r \u0010 s\nN d\n\u00111/p\n. (38)\nNow we estimate the second term of the right-hand side of\n(31). For each k′ ∈{1, . . . , (N ∗)d}, deﬁne\nΞk′ := {k ∈{1, . . . , (N ∗)d} : ˜Bk,τ ∩Bk′ ̸= ∅, k ̸= k′}.\n(39)\nSince 0 < τ ≤\n1\n2N∗, it is easy to verify that\n|Ξk′| ≤3d −1,\n∀k′ ∈{1, . . . , (N ∗)d}.\n(40)\nNoting further that\n\r\r\r\r\r\r\nX\nk̸=kx\npu,ξk,f(x)N1,N ∗,ξk,τ(x)\n\r\r\r\r\r\r\np\nLp(Id)\n≤\n(N∗)d\nX\nk′=1\nZ\nBk′\n\f\f\f\f\f\f\nX\nk̸=kx\npu,ξk,f(x)N1,N ∗,ξk,τ(x)\n\f\f\f\f\f\f\np\ndx,(41)\nwe obtain, from (39), (40), (12) and |N1,N ∗,ξk,τ(x)| ≤1, that\nZ\nBk′\n\f\f\f\f\f\f\nX\nk̸=kx\npu,ξk,f(x)N1,N ∗,ξk,τ(x)\n\f\f\f\f\f\f\np\ndx\n=\nZ\nBk′\n\f\f\f\f\f\f\nX\nk∈Ξk′\npu,ξk,f(x)N1,N ∗,ξk,τ(x)\n\f\f\f\f\f\f\np\ndx\n≤\nmax\n1≤k≤(N∗)d ∥pu,ξk,f∥p\nL∞(Id)\n×\nX\nℓ∈Ξk′\nZ\n˜\nBℓ,τ ∩Bk′\n\f\f\f\f\f\f\nX\nk∈Ξk′\nN1,N ∗,ξk,τ(x)\n\f\f\f\f\f\f\np\ndx\n≤\n3dp\nmax\n1≤k≤(N ∗)d ∥pu,ξk,f∥p\nL∞(Id)\nX\nℓ∈Ξk′\nZ\n˜\nBℓ,τ ∩Bk′\ndx.\nBut k′ /∈Ξk′ implies that for any ℓ∈Ξk′,\nZ\n˜\nBℓ,τ ∩Bk′\ndx ≤(1/N ∗+ 2τ)d −(1/N ∗)d ≤2dτ(N ∗)1−d,\n(42)\nwhere the mean value theorem is applied to yield the last\ninequality. Thus,\nZ\nBk′\n\f\f\f\f\f\f\nX\nk̸=kx\npu,ξk,f(x)N1,N ∗,ξk,τ(x)\n\f\f\f\f\f\f\np\ndx\n≤\n2d3d(p+1)\nmax\n1≤k≤(N∗)d ∥pu,ξk,f∥p\nL∞(Id)τ(N ∗)1−d.\n8\nPlugging the above estimate into (41), we may conclude from\n0 < τ ≤(N ∗)−1−pr \u0000s\n2Nd\n\u0001\nthat\n\r\r\r\r\r\r\nX\nk̸=kx\npu,ξk,f(x)N1,N ∗,ξk,τ(x)\n\r\r\r\r\r\r\nLp(Id)\n(43)\n≤\nd1/p32d\nmax\n1≤k≤(N∗)d ∥pu,ξk,f∥L∞(Id)(N ∗)−r \u0010 s\nN d\n\u0011 1\np .\nInserting (38) and (43) into (31) and noting that\nmax\n1≤k≤(N ∗)d ∥pu,ξk,f∥L∞(Id) ≤∥f∥L∞(Id) + c1dr/2,\nfrom (27), we deduce that\n∥f −N2,N ∗,τ∥Lp(Id) ≤c2(N ∗)−r \u0010 s\nN d\n\u00111/p\n,\nwith c2 := c12d/pdr/2 + d1/p32d(∥f∥L∞(Id) + c1dr/2). This\nproves (29).\nWe now turn to prove (30). First, (28) and (12) imply that\nfor x ∈Id,\nN2,N ∗,τ(x) =\n(N∗)d\nX\nk=1\npu,ξk,f(x)N1,N ∗,ξk,τ(x)\n=\nX\nk: ˜\nBk,τ ∩Bkx̸=∅\npu,ξk,f(x)N1,N ∗,ξk,τ(x).\nSince 0 < τ ≤1/(2N ∗), it follows from (40) and 0 ≤\nN1,N ∗,ξk,τ(x) ≤1 that\n|N2,N ∗,τ(x)| ≤3d(∥f∥L∞(Id) + c1dr/2) =: c3,\n∀x ∈Id.\nThis completes the proof of Lemma 2.\nThe following “product-gate” property for deep ReLU nets\ncan be found in [17].\nLemma 3. Let θ > 0 and ˜L ∈N with ˜L > (2θ)−1. For any\nℓ∈{2, 3, . . . } and ε ∈(0, 1), there exists a deep ReLU net\n˜×ℓwith 2ℓ˜L+8ℓlayers and at most c4ℓθε−θ free parameters\nbounded by ℓγε−γ, such that\n|t1t2 · · · tℓ−˜×ℓ(t1, . . . , tℓ)| ≤ε,\n∀t1, . . . , tℓ∈[−1, 1],\nwhere c4 and γ are constants depending only on θ and ˜L.\nFor β ∈N0 and B > 0, deﬁne\nPd\nβ,B :=\n\n\n\nX\n|α|≤β\ncαxα : |cα| ≤B\n\n\n,\nwhere α = (α1, . . . , αd) ∈Nd\n0, |α| = α1 + · · · + αd and\nxα = (x(1))α1 · · · (x(d))αd. The following lemma was proved\nin [17].\nLemma 4. Let β ∈N0, B > 0, θ > 0 and ˜L ∈N with\n˜L > (2θ)−1. For every P ∈Pd\nβ,B and 0 < ε < 1, there is\na deep ReLU net structure with 2β ˜L + 8β + 1 layers and\nat most βd + c4(βd+1B)θε−θ free parameters bounded by\nmax{B, (βd+1B)γε−γ} such that for any P ∈Pd\nβ,B there\nexists a deep ReLU net hP with the aforementioned structure\nthat satisﬁes\n|P(x) −hP (x)| ≤ε,\n∀x ∈Id.\nLet c5 be a constant that satisﬁes\nc42\nd\n2r+d c\n−\nd\n2r+d\n5\n+ 4d + 1 + rd\n+\nc4(ud+1(∥f∥L∞(Id) + c1dr/2))\nd\nr+d c\n−\nd\nr+d\n5\n≤\n2(4d + 1 + rd).\nFor N ∗> max\nn\nc1/(d+r)\n5\n, 1\no\n, let ˜×2 be the deep net as\nintroduced in Lemma 3 with ℓ= 2, θ =\nd\n2d+r, ε =\nc5(N ∗)−2d−r and let ˜L = ⌈2 + r/d⌉. Denote by hpu,ξk,f\nthe deep ReLU net in Lemma 4 with P = pu,ξk,f, β = u,\nB = ∥f∥L∞(Id) + c1dr/2, ε = c5(N ∗)−r−d, θ = d/(r + d),\nand ˜L = ⌈1 + r/d⌉. From Lemma 4, we have, for any\nx ∈Id, k = 1, . . . , (N ∗)d, that\n|hpu,ξk,f (x)| ≤|pu,ξk,f(x)| + 1 ≤∥f∥L∞(Id) + c1dr/2 + 1.\n(44)\nNext consider\nN3,N ∗,τ(x) := (∥f∥L∞(Id) + c1dr/2 + 1)\n(45)\n×\n(N ∗)d\nX\nk=1\n˜×2\n\u0012\nhpu,ξk,f (x)\n∥f∥L∞(Id) + c1dr/2 + 1, N1,N ∗,ξk,τ(x)\n\u0013\n.\nNoting that the parameters of the deep nets ˜×2(t1, t2) are\nindependent of t1, t2\n∈\n[−1, 1], we may conclude that\nN3,N ∗,τ is a deep net with ⌈25 + 4r/d + 2r2/d + 10r⌉\nlayers and at most C∗\n1(N ∗)d free parameters with C∗\n1 :=\n2(4d + 1 + rd) that are bounded by ˜B∗deﬁned by (13) with\nC3 := 2rd+1(∥f∥L∞(Id) + c1dr/2). With these preparations,\nwe can now prove Proposition 2 as follows.\nProof of Proposition 2: By applying the triangle inequal-\nity, we have\n∥f −N3,N ∗,τ∥Lp(Id)\n≤\n\r\r\r\r\r\r\nN2,N ∗,τ(x) −\n(N∗)d\nX\nk=1\nhpu,ξk,f (x)N1,N ∗,ξk,τ(x)\n\r\r\r\r\r\r\nLp(Id)\n+\n\r\r\r\r\r\r\n(N∗)d\nX\nk=1\nhpu,ξk,f (x)N1,N ∗,ξk,τ(x) −N3,N ∗,τ(x)\n\r\r\r\r\r\r\nLp(Id)\n+\n∥f −N2,N ∗,τ(x)∥Lp(Id)\n=:\nI1 + I2 + I3.\n(46)\nIt follows from p ≥1, N ∗≥4N and Lemma 3 with ℓ= 2,\nθ =\nd\n2d+r, ε = c5(N ∗)−2d−r and ˜L = ⌈2 + r/d⌉that\nI2\n≤\nc5(∥f∥L∞(Id) + c1dr/2 + 1)(N ∗)−r−d\n≤\nc5(∥f∥L∞(Id) + c1dr/2 + 1)(N ∗)−r \u0010 s\nN d\n\u00111/p\n.\nSimilarly, we also note 0 ≤N1,N ∗,ξk,τ(x) ≤1 and Lemma\n4 with β = u, B = ∥f∥L∞(Id) + c1dr/2, ε = c5(N ∗)−r−d,\n9\nθ = d/(r + d), and ˜L = ⌈1 + r/d⌉imply\nI1\n≤\nc5(N ∗)−r−d\n\n\nZ\nId\n\f\f\f\f\f\f\n(N∗)d\nX\nk=1\nN1,N ∗,ξk,τ(x)\n\f\f\f\f\f\f\np\ndx\n\n\n1/p\n=\nc5(N ∗)−r−d\n\n\nZ\nId\n\f\f\f\f\f\f\nX\nk̸=kx\nN1,N ∗,ξk,τ(x)\n\f\f\f\f\f\f\np\ndx\n\n\n1/p\n+\nc5(N ∗)−r−d.\nThe same approach as that in the proof of (43) yields that, for\n0 < τ ≤(N ∗)−1−pr \u0000s\n2Nd\n\u0001\n,\n\n\nZ\nId\n\f\f\f\f\f\f\nX\nk̸=kx\nN1,N ∗,ξk,τ(x)\n\f\f\f\f\f\f\np\ndx\n\n\n1/p\n≤d1/p32d(N ∗)−r \u0010 s\nN d\n\u0011 1\np .\nTherefore,\nI1 ≤c6(N ∗)−r−d ≤c6(N ∗)−r \u0010 s\nN d\n\u0011 1\np ,\nwhere c6 := c5(1 + d1/p32d). Furthermore, by Lemma 2 that\nunder 0 < τ ≤\ns\n2Nd(N∗)1+pr , we obtain\nI3 ≤c2(N ∗)−r \u0010 s\nN d\n\u00111/p\n.\nPlugging the estimates of I1, I2, I3 into (46), we then have\n∥f −N3,N ∗,τ∥Lp(Id) ≤C4(N ∗)−r \u0010 s\nN d\n\u00111/p\nwith C4 := c2 + c6 + c5. Thus, (14) holds.\nNow we turn to the proof of (15). According to (45) and\nLemma 3 with ℓ= 2, θ =\nd\n2d+r, ε = (N ∗)−2d−r and ˜L =\n⌈2 + r/d⌉, we have\n|N3,N ∗,τ(x)| ≤\n\f\f\f\f\f\f\n(N ∗)d\nX\nk=1\nhpu,ξk,f (x)N1,N ∗,ξk,τ(x)\n\f\f\f\f\f\f\n+\nc5(N ∗)−d−r(∥f∥L∞(Id) + c1dr/2 + 1).\nBut (12), together with 0 < τ\n≤1/(2N ∗), (40), 0 ≤\nN1,N ∗,ξk,τ(x) ≤1 and (44) yields\n\f\f\f\f\f\f\n(N∗)d\nX\nk=1\nhpu,ξk,f (x)N1,N ∗,ξk,τ(x)\n\f\f\f\f\f\f\n≤3d(∥f∥L∞(Id)+c1dr/2+1),\nwhich implies (15) with C5 := (c5+3d)(∥f∥L∞(Id)+c1dr/2+\n1). This completes the proof of Proposition 2.\nC. Proof of Theorem 2\nLet B be a Banach space and V be a subset of B. Denote by\nN(ε, V, B) the ε-covering number [16, Chap. 9] of V under\nthe metric of B, which is the minimal number of elements in\nan ε-net of V . The following lemma proved in [15, Theorem\n1] gives rise to a tight estimate for the covering number of\ndeep ReLU nets.\nLemma 5. Let Hn,L,R be deﬁned by (17). Then\nN\n\u0000ε, Hn,L,R, L∞(Id)\n\u0001\n≤(c7RDmax)3(L+1)2n ε−n,\n(47)\nwhere c7 is a constant depending only on d and Dmax =\nmax0≤ℓ≤L dℓ.\nTo prove Theorem 2, we also need the following lemma,\nthe proof of which can be found in [5].\nLemma 6. Let H be a collection of functions deﬁned on Id\nand deﬁne\nfD,H = arg min\nf∈H\n1\nm\nm\nX\ni=1\n(f(xi) −yi)2.\n(48)\nSuppose there exist n′, U > 0, such that\nlog N(ε, H, L∞(Id)) ≤n′ log U\nε ,\n∀ε > 0.\n(49)\nThen for any h ∈H and ε > 0,\nPr{∥πMfD,H −fρ∥2\nρ > ε + 2∥h −fρ∥2\nρ}\n≤\nexp\n\u001a\nn′ log 16UM\nε\n−\n3mε\n512M 2\n\u001b\n+\nexp\n(\n−3mε2\n16(3M + ∥h∥L∞(X))2 \u00006∥h −fρ∥2ρ + ε\n\u0001\n)\n.\nNow we are in a position to prove the upper bound of (24).\nProof of the upper bound of (24):\nFor N ∗\n≥\nmax{4N, ˜C}, Proposition 2 implies that there exists an hρ ∈\nHL,n,R with L, n satisfying (16) and R satisfying (18) such\nthat\n∥fρ −hρ∥2\nρ ≤D2\nρX,p∥fρ −hρ∥2\nLp(Id)\n≤\nC2\n4D2\nρX,p(N ∗)−2r \u0010 s\nN d\n\u00112/p\n=: Ap.\nRecalling (15), we have\n∥hρ∥L∞(Id) ≤C5.\nBut Lemma 5, together with the structure of deep nets in\nProposition 2, (16) and (18), implies Dmax ≤c8n with c8\ndepending only on r and d and\nlog N\n\u0000ε, Hn,L,R, L∞(Id)\n\u0001\n≤c9L2n log Rn\nε\n≤\nc10(N ∗)d log N ∗N d\nsε\nfor some positive constants c9, c10\ndepending only on\nd, r, C∗\n1, c7, c8, γ, p. Using the above three estimates in Lemma\n6 with n′ = c10(N ∗)d, U = N ∗N d/s, we have\nPr{∥πMfD,n,L −fρ∥2\nρ > ε + 2∥hρ −fρ∥2\nρ}\n≤\nexp\n\u001a\nc10(N ∗)d log 16MN dN ∗\nsε\n−\n3mε\n512M 2\n\u001b\n+\nexp\n\u001a\n−3mε2\n16(3M + C5 + 1)2 (6Ap + ε)\n\u001b\n.\nThus, by scaling 3ε to ε, for ε ≥Ap, we obtain\nPr{∥πMfD,n,L −fρ∥2\nρ > ε}\n≤\nexp\n\u001a\nc10(N ∗)d log 48MN ∗N d\nsε\n−\nmε\n512M 2\n\u001b\n+\nexp\n\u001a\n−mε2\n16(3M + C5 + 1)2 (18Ap + ε)\n\u001b\n.\n(50)\n10\nThus,\nE[∥πMfD,n,L −fρ∥2\nρ]\n=\nZ ∞\n0\nPr[∥πMfD,n,L −fρ∥2\nρ > ε]dε\n=\nZ ∞\n3Ap\nPr[∥πMfD,n,L −fρ∥2\nρ > ε]dε\n+\nZ 3Ap\n0\nPr[∥πMfD,n,L −fρ∥2\nρ > ε]dε\n≤\nZ ∞\n3Ap\nPr[∥πMfD,n,L −fρ∥2\nρ > ε]dε + 3Ap.\nFrom (50), we also have\nZ ∞\n3Ap\nPr[∥πMfD,n,L −fρ∥2\nρ > ε]dε\n≤\nZ ∞\n3Ap\nexp\n\u001a\nc10(N ∗)d log 48MN dN ∗\nsε\n−\nmε\n512M 2\n\u001b\ndε\n+\nZ ∞\n3Ap\nexp\n\u001a\n−mε2\n16(3M + C5 + 1)2 (18Ap + ε)\n\u001b\ndε\n=:\nJ1 + J2.\nA direct computation then yields\nJ2\n≤\nZ ∞\n3Ap\nexp\n\u001a\n−mε\n112(3M + C5 + 1)2\n\u001b\ndε\n≤\n112(3M + C5 + 1)2\nm\n.\nSet\n(N ∗)2r+d ∼c11m\n\u0010 s\nN d\n\u0011 2\np / log(c1/2r+d\n11\nm),\n(51)\nwhere c11 :=\n3C2\n4D2\nρX ,p\nc10c111024M 2 . It follows from (23) that N ∗≥\nmax{ ˜C, 4N}. Thus, for p ≥2, we have, from the deﬁnition\nof Ap, that\nlog 48MN dN ∗\nsAp\n≤log 48MN dp(N ∗)2r+1\nC2\n4D2ρX,psp\n≤\nlog 48M(N ∗)2r+1+dp\nC2\n4D2ρX,p\n≤c12 log N ∗,\nwhere c12 := (2r + 1 + dp) log\n\u0010\n48M\nC2\n4D2ρX ,p + 1\n\u0011\n. Then\nc10c12(N ∗)d log N ∗≤\n3mAp\n1024M 2 ,\nwhich implies\nJ1 ≤\nZ ∞\n3Ap\nexp\n\u001a\n−mε\n1024M 2\n\u001b\ndε ≤1024M 2\nm\n.\nThus,\nE[∥πMfD,n,L −fρ∥2\nρ] ≤c13\nm + 3Ap,\nwhere c13 := 1024M 2 + 112(3M + C5 + 1)2. Hence,\nE[∥πMfD,n,L −fρ∥2\nρ]\n≤\nC7\n\u0012\nm\nlog m\n\u0013 −2r\n2r+d \u0010 s\nN d\n\u0011 2\np −\n2r\n2r+d .\nThis provides the upper bound of (24).\nV. PROOF OF THE LOWER BOUNDS\nIn this section, we present a general lower bound estimate\nfor Theorem 1 and Theorem 2. To this end, we need the\nfollowing assumption for the qualiﬁcation of the distribution\nρ.\nAssumption 1. Assume\n(A) fρ ∈Lip(N,s,r,c0).\n(B) ρX is the uniform distribution on Id.\n(C) y = fρ(x)+ν, where ν and x are independent and ν is\ndrawn according to the standard normal distribution N(0, 1).\nLet M1(N, s, r, c0) be the set of all distributions that satisfy\nAssumption 1 and Ψm be the set of estimators fD derived from\nDm. Then\nsup\nρ∈M(N,s,r,c0)\ninf\nfD∈Ψm E[∥fD −fρ∥2\nρ]\n≥\nsup\nρ∈M1(N,s,r,c0)\ninf\nfD∈Ψm E[∥fD −fρ∥2\nρ].\n(52)\nThe following theorem is a more general lower bound than\nthat in Theorem 1.\nTheorem 3. If m satisﬁes (3), then there exists a constant ˜C\nindependent of m, s or N, such that\nsup\nρ∈M(N,s,r,c0)\ninf\nfD∈Ψm E[∥fD−fρ∥2\nρ] ≥˜Cm−\n2r\n2r+d\n\u0010 s\nN d\n\u0011\nd\n2r+d .\n(53)\nIt it easy to see that the lower bound of Theorem 1 is a direct\nconsequence of Theorem 3. Before presenting the proof, we\nintroduce a function g that satisﬁes the following assumption.\nAssumption\n2. Assume that g\n:\nRd\n→\nR satisﬁes\nsupp(g)\n=\n[−/(2\n√\nd), 1/(2\n√\nd)]d, g(x)\n=\n1 for x\n∈\n[−1/(4\n√\nd), 1/(4\n√\nd)]d and g ∈Lip(r,c02v−1), where supp(g)\ndenotes the support of g.\nLet {ϵk}(N∗)d\nk=1\nbe a set of independent Rademacher random\nvariables, i.e.,\nPr(ϵk = 1) = Pr(ϵk = −1) = 1\n2,\n∀k = 1, 2, . . . , (N ∗)d.\n(54)\nFor x ∈Id, deﬁne\ngk(x) := (N ∗)−rg(N ∗(x −ξk)),\n(55)\nwhere ξk is the center of the cube Bk. Since\n∥N ∗(x−ξk)−N ∗(x−ξk′)∥= N ∗∥ξk−ξk′∥≥1,\n∀k ̸= k′,\nat least one of N ∗(x −ξk) and N ∗(x −ξk′) lies outside\n(−1/(2\n√\nd), 1/(2\n√\nd))d. Then it follows from Assumption 2\nthat\ngk(x) = 0,\nif x /∈˙Bk,\n(56)\nwhere ˙Bk = Bk\\∂Bk and ∂A denotes the boundary of a cube\nA.\nGiven S = ∪j∈ΛsAj, consider the set FS,N∗of all func-\ntions,\nf(x) =\n( P(N ∗)d\nk=1\nϵkgk(x),\nif x ∈S,\n0,\notherwise,\n11\nwith ϵk that satisﬁes (54). It is obvious that FS,N∗is a set\nof random functions. The following lemma shows that it is\nalmost surely a subset of Lip(N,s,r,c0).\nLemma 7. If gk is deﬁned by (55) with g satisfying the\nAssumption 2, then for N ∗∈N and S = ∪j∈ΛsAj, then\nFS,N∗⊂Lip(N,s,r,c0)\nalmost surely.\nProof: From the deﬁnition of FS,N∗, it is obvious that\neach f ∈FS,N∗is s-sparse in N d partitions. So, it sufﬁces to\nprove that f ∈FS,N∗implies f ∈Lip(r,c0) almost surely. For\nx, x′ ∈Id with x ̸= x′, we divide the proof into four cases:\nx, x′ ∈S, x ∈S but x′ /∈S, x /∈S but x′ ∈S and x, x′ ̸∈S.\nCase 1: x, x′ ∈S. If x, x′ ∈Bk0 ∩S for some k0 ∈\n{1, . . . , (N ∗)d}, then (56) yields (gk)(u)\nα (x) = 0 for k ̸= k0.\nSo, for each f ∈FS,N∗, we get from |ϵk| = 1, (56), (55) and\n0 < v ≤1 that\n|f (u)\nα (x) −f (u)\nα (x′)|\n=\n\f\f\f\f\f\f\n(N∗)d\nX\nk=1\nϵk(gk)(u)\nα (x) −(gk)(u)\nα (x′)]\n\f\f\f\f\f\f\n=\n\f\f\f(gk)(u)\nα (x) −(gk)(u)\n0 (x′)\n\f\f\f\n≤\n(N ∗)−r+u \f\f\fg(u)\nα (N ∗(x −ξk0)) −g(u)\nα (N ∗(x′ −ξk0))\n\f\f\f\n≤\nc02v−1∥x −x′∥v ≤c0∥x −x′∥v.\nIf x ∈Bk1 ∩S but x′ ∈Bk2 ∩S for some k1, k2 ∈\n{1, . . . , (N ∗)d} with k1 ̸= k2, we can choose z ∈∂Bk1 and\nz′ ∈∂Bk2 such that z, z′ are on the segment between x and\nx′. Then\n∥x −z∥+ ∥x′ −z′∥≤∥x −x′∥.\n(57)\nSo, Assumption 2, (56), 0 < v ≤1, Jensen’s inequality and\n(57) show\n|f (u)\nα (x) −f (u)\nα (x′)|\n=\n\f\f\f\f\f\f\n(N∗)d\nX\nk=1\nϵk[(gk)(u)\nα (x) −(gk)(u)\nα (x′)]\n\f\f\f\f\f\f\n≤\n\f\f\f(gk1)(u)\nα (x)\n\f\f\f +\n\f\f\f(gk2)(u)\nα (x′)\n\f\f\f\n=\n\f\f\f(gk1)(u)\nα (x) −(gk1)(u)\nα (z)\n\f\f\f +\n\f\f\f(gk2)(u)\nα (x′) −(gk2)(u)\nα (z′)\n\f\f\f\n≤\n(N ∗)−r+u h\n|g(u)\nα (N ∗(x −ξk1)) −g(u)\nα (N ∗(z −ξk1))|\n+\n|g(u)\nα (N ∗(x′ −ξk2)) −g(u)\nα (N ∗(z′ −ξk2))|\ni\n≤\nc02v\n\u0014∥x −z∥v\n2\n+ ∥x′ −z′∥v\n2\n\u0015\n≤\nc02v\n\u0014∥x −z∥\n2\n+ ∥x′ −z′∥\n2\n\u0015v\n≤c0∥x −x′∥v.\nThese two assertions imply that f ∈Lip(r,c0) almost surely\nand proves Lemma 7 for the ﬁrst case.\nCase 2: Suppose x ∈S, x′ /∈S. There is some k3 ∈\n{1, . . . , (N ∗)d} such that x ∈S ∩Bk3. For each f ∈FS,N∗\nand any x′ /∈S, it follows from (55) and Assumption 2 that\nf(x′) = 0 = f(z),\n∀z ∈∂Bk3.\nSelect a z′′ ∈∂Bk3 on the segment between x and x′. Then,\n∥x −x′∥≥∥x −z′′∥. Hence, the result in the ﬁrst case above\nshows that\n|f (u)\nα (x) −f (u)\nα (x′)| = |f (u)\nα (x) −f (u)\nα (z′′)|\n≤\nc0∥x −z′′∥v ≤c0∥x −x′∥v.\nCase 3: Suppose x′ ∈S, x /∈S. The proof of this case is\nthe same as that of Case 2.\nCase 4: Suppose x, x′ /∈S. For each f ∈FS,N∗and any\nx, x′ /∈S, we have\n|f (u)\nα (x) −f (u)\nα (x′)| = 0 ≤c0∥x −x′∥v.\nCombining the above four cases, we complete the proof of\nLemma 7.\nLet HS,N∗be the set of all functions\nh(x) =\n( P(N∗)d\nk=1\nckgk(x),\nif x ∈S,\n0,\notherwise\nwith ck ∈R. It then follows from the deﬁnition of FS,N∗that\nFS,N∗⊂HS,N∗.\n(58)\nThe following lemma constructs an orthonormal basis of\nHS,N∗.\nLemma 8. Let HS,N∗be deﬁned as above with gk and g that\nsatisfy (55) and Assumption 2, respectively. Let\ng∗\nk,S(x) :=\n\u001a gk(x),\nif x ∈S,\n0,\nif x /∈S.\n(59)\nThen, the system\nn g∗\nk,S(·)\n∥g∗\nk,S∥ρ : k = 1, . . . , (N ∗)do\nis an or-\nthonormal basis of HS,N∗using the inner product of L2\nρX.\nProof: For k ̸= k′, it follows from (56) and (59) that\nZ\nId g∗\nk,S(x)g∗\nk′,S(x)dρX =\nZ\nS\ngk(x)gk′(x)dρX = 0.\n(60)\nTherefore, {g∗\nk,S(·) : k = 1, . . . , (N ∗)d} is an orthogo-\nnal set in L2\nρX. Noting further ∥g∗\nk,S∥ρ ̸= 0 for all k ∈\n{1, . . . , (N ∗)d},\nZ\nId\n \ng∗\nk,S(x)\n∥g∗\nk,S∥ρ\n!2\ndρX = 1,\n∀k = 1, 2, . . . , (N ∗)d\nand HS,N∗is an (N ∗)d-dimensional linear space, we may\nconclude that the system\nn g∗\nk,S(·)\n∥g∗\nk,S∥ρ : k = 1, . . . , (N ∗)do\nis an\northonormal basis of HS,N∗. This completes the proof of\nLemma 8.\nTo prove the lower bound, we need the following three\nlemmas. The ﬁrst one can be found in [16, Lemma 3.2].\nLemma 9. Let U be an ℓ-dimensional real vector, θ a\nzero- mean random variable with range {−1, 1}, and ν an\nℓ-dimensional random vector of standard normal variable,\nindependent of U. Denote\nψ := θU + ν.\nThen there exists an absolute constant ˜C1 > 0 such that\nmin\nf ∗:Rℓ→{−1,1} Pr{f ∗(ψ) ̸= θ} ≥˜C1e−∥U∥2\nℓ/2,\n12\nwhere ∥· ∥ℓdenotes the ℓ-dimensional Euclidean norm and\nthe minimization is over all functions f ∗: Rℓ→{−1, 1}.\nLemma 10. Under (B) in Assumption 1, if g satisﬁes Assump-\ntion 2, then for any k ∈{1, . . . , (N ∗)d}\nZ\nBk\n[g(N ∗(x −ξk))]2 dρX ≥˜C2(N ∗)−d,\n(61)\nwhere the constant ˜C2 dependent only on d.\nProof: It follows from Assumption 2 and (B) that\nZ\nBk\n[g(N ∗(x −ξk))]2 dρX\n≥\nZ\nBk\n[g(N ∗(x −ξk))]2 dx\n≥\n(N ∗)−d\nZ\n[−1/(2\n√\nd),1/(2\n√\nd)]d |g(x)|2dx\n≥\n(N ∗)−d\nZ\n[−1/(4\n√\nd),1/(4\n√\nd)]d dx\n=\n(2\n√\ndN ∗)−d,\n(62)\nwhere\nthe\nsecond\ninequality\nholds\nsince\nN ∗(x −ξk)\nis\nrestricted\nto\nsome\nsubset\nof\nRd\nthat\ncontains\n[−1/(2\n√\nd), 1/(2\n√\nd)]d for x ∈Bk. This completes the\nproof of Lemma 10 with ˜C2 = ˜C3(2\n√\nd)−d.\nIf N ∗≥4N, noting that {Aj}Nd\nj=1 and {Bk}(N∗)d\nk=1\nare cubic\npartitions of Id, we may conclude that each Aj then contains\nat least\n\u0010\nN∗\nN −2\n\u0011d\n≥\n\u0010\nN∗\n2N\n\u0011d\nBk’s. For each j ∈Λs, denote\nΛ∗\nj := {k ∈{1, . . . , (N ∗)d} : Bk ⊆Aj}.\n(63)\nThen\n|Λ∗\nj| ≥\n\u0012N ∗\n2N\n\u0013d\n.\n(64)\nWith the above preparations , we present the following lemma,\nwhich will play a crucial role in our analysis.\nLemma 11. Let Dm = {(xi, yi)}m\ni=1 be the set of samples\nwhich are independently drawn according to some distribution\nρ with the marginal distribution ρX satisfying (B) and yi =\nfρ(xi) + νi, where fρ ∈FS,N∗and νi is the standard normal\nvariable. If N ∗≥4N and N ∗=\nl\u0000 ms\nNd\n\u0001\n1\n2r+d m\n. Then for any\nj ∈Λs, k ∈Λ∗\nj, there exists a constant ˜C3 independent of m,\ns, N or N ∗such that\nmin\nh:Rm→{−1,1} Pr{h((y1, . . . , ym)) ̸= ϵk} ≥˜C3 > 0.\n(65)\nProof: Write Din = {xi}m\ni=1 and Din,S := Din ∩S. For\neach j ∈Λs and k ∈Λ∗\nj, denote further Bk,D := Bk ∩Din :=\n{xi,k}ℓ′\ni=1, where ℓ′ = 0 means Bk,D = ∅. We then divide\nthe proof into the following three steps.\nStep 1: Estimating |Din,S|. Since ρX is the uniform distri-\nbution on Id, for each xi ∈Din,\nPr{xi ∈S} =\ns\nN d .\nFor i = 1, . . . , m, deﬁne\nVi := Ixi∈S :=\n\u001a\n1,\nwith probability\ns\nN d\n0,\nwith probability 1 −\ns\nNd .\nThen\n|Din,S| =\nm\nX\ni=1\nVi =\nm\nX\ni=1\nIxi∈S.\nThis implies\nE{|Din,S|} =\nm\nX\ni=1\nE{Ixi∈S} =\nm\nX\ni=1\nPr{xi ∈S}\n=\nm\nX\ni=1\ns\nN d = ms\nN d .\nSo, it follows from Markov’s inequality that\nPr\n\u001a\n|Din,S| >\n\u00182ms\nN d\n\u0019\u001b\n≤N dE{|Din,S|}\n2ms\n= 1\n2.\nThe above estimate, together with the formula of total proba-\nbility, implies that\nmin\nh:Rm→{−1,1} Pr{h(yD) ̸= ϵk}\n(66)\n=\nmin\nh:Rm→{−1,1} Pr\n\u001a\nh(yD) ̸= ϵk\n\f\f|Din,S| >\n\u00182ms\nN d\n\u0019\u001b\nPr\n\u001a\n|Din,S| >\n\u00182ms\nN d\n\u0019\u001b\n+\nmin\nh:Rm→{−1,1} Pr\n\u001a\nh(yD) ̸= ϵk\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\u001b\nPr\n\u001a\n|Din,S| ≤\n\u00182ms\nN d\n\u0019\u001b\n≥\n1\n2\nmin\nh:Rm→{−1,1} Pr\n\u001a\nh(yD) ̸= ϵk\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\u001b\n,\nwhere h(yD) := h((y1, . . . , ym)).\nStep 2: Estimating the conditional probability. If A and B\nare random events, then\nPr{A} = E{IA} = E {E{IA|B}} = E {Pr{A|B}} , (67)\nwhere IA denotes the indicator of the event A. Hence,\nmin\nh:Rm→{−1,1} Pr\n\u001a\nh(yD) ̸= ϵk\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\u001b\n=\nE\n\b\nmin\nh:Rm→{−1,1} Pr{h(yD) ̸= ϵk\n\f\f\n|Din,S| ≤\n\u0006\n2ms/N d\u0007\n, Din}\n\t\n.\n(68)\nFor each j ∈Λs and k ∈Λ∗\nj, it follows from (63) that ℓ′ =\n|Bk,D| ≤|Din,S|. Then for each h : Rm →{−1, 1}, from the\nformula of total probability again, we obtain\nPr\n\u001a\nh(yD) ̸= ϵk\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\n, Din\n\u001b\n(69)\n=\n⌈2ms\nNd ⌉\nX\nℓ=0\nPr\n\b\nh(yD) ̸= ϵk\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\n,\nDin, ℓ′ = ℓ\n\t\nPr\n\u001a\nℓ′ = ℓ\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\n, Din\n\u001b\nand\n⌈2ms\nNd ⌉\nX\nℓ=0\nPr\n\u001a\nℓ′ = ℓ\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\n, Din\n\u001b\n= 1.\n(70)\n13\nGiven Din, ℓ′ = 0, |Din,S| ≤\n\u0006 2ms\nN d\n\u0007\n, for each k ∈\n{1, . . . , (N ∗)d}, it follows from the deﬁnition of FS,N∗and\n(56) that there exists some k′ ̸= k such that\nyi =\n(N∗)d\nX\nk=1\nϵkgk(xi)+νi = ϵk′gk′(xi)+νi,\ni = 1, 2, . . . , m,\nwhich is independent of ϵk. That is, ϵk is independent of\n(y1, . . . , ym). Thus, it follows from (54) that\nmin\nh:Rm→{−1,1} Pr\n\b\nh((y1, . . . , ym) ̸= ϵk\n\f\f\n|Din,S| ≤\n\u0006\n2ms/N d\u0007\n, Din, ℓ′ = 0\n\t\n= 1\n2.\n(71)\nGiven Din, |Din,S| ≤\n\u0006 2ms\nN d\n\u0007\nand ℓ′ = ℓwith ℓ≥1, for\neach j ∈Λs and k ∈Λ∗\nj, then we get from the deﬁnition of\nFS,N∗and (56) that there exists a k′ ̸= k, such that\nyi =\n(N∗)d\nX\nk=1\nϵkgk(xi)+νi = ϵk′gk′(xi)+νi,\nxi ∈Din\\Bk,D\nwhich is independent of ϵk. Write\nyi,k =\n(N∗)d\nX\nk=1\nϵkgk(xi,k)+νi = ϵkgk(xi,k)+νi,\ni = 1, . . . , ℓ′.\nThen, there exists an h∗: Rℓ′ →{−1, 1}, such that\nPr\n\u001a\nh(yD) ̸= ϵk\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\n, Din, ℓ′ = ℓ\n\u001b\n(72)\n=\nPr\n\u001a\nh∗(yD,ℓ′) ̸= ϵk\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\n, Din, ℓ′ = ℓ\n\u001b\n,\nwhere yD,ℓ′ := (y1,k, . . . , yℓ′,k). From (56) again, it is easy\nto see that\n(y1,k, . . . , yℓ′,k)\n(73)\n:=\nϵk (gk(x1,k), . . . , gk(xℓ′,k)) + (ν1,k, . . . , νℓ′,k),\nTherefore,\napplying\nLemma\n9\nwith\nU\n=\n(gk(x1,k), . . . , gk(xℓ′,k)) and θ = ϵk, we get from (73)\nand (72) that for each k ∈Λ∗\nj and j ∈Λs,\nmin\nh∗:Rℓ′→{−1,1} Pr\n\u001a\nh∗(yD,ℓ′) ̸= ϵk\n\f\f|Din,S| ≤\n\u00062ms\nN d\n\u0019\n,\nDin, ℓ′ = ℓ\n\t\n≥\n˜C1 exp\n\u0012\n−(gk(x1,k))2 + · · · + (gk(xℓ,k))2\n2\n\u0013\n.\n(74)\nPutting (74) and (71) into (69) and noting (72) and (70), we\nobtain that for each k ∈Λ∗\nj and j ∈Λs,\nmin\nh:Rm→{−1,1} Pr\n\b\nh(yD) ̸= ϵk\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\n,\nDin\n\t\n≥1\n2Pr\n\u001a\nℓ′ = 0\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\n, Din\n\u001b\n+\n˜C1\n⌈2ms\nNd ⌉\nX\nℓ=1\nexp\n\u0012\n−(gk(x1,k))2 + · · · + (gk(xℓ,k))2\n2\n\u0013\nPr\n\u001a\nℓ′ = ℓ\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\n, Din\n\u001b\n≥\nmin\n\u001a1\n2, ˜C1B(m, s, N, gk)\n\u001b\n.\n(75)\nwhere\nB(m, s, N, gk) := exp\n \n−\nP\nxi∈Din,S,|Din,S|≤⌈2ms\nNd ⌉(gk(xi))2\n2\n!\n.\nStep 3: Estimating the probability. Putting (75) into (68), we\nhave, from Jensen’s inequality with the convexity of exp(−·),\nthat\nmin\nh:Rm→{−1,1} Pr\n\u001a\nh(yD) ̸= ϵk\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\u001b\n≥\nE\n\u001a\nmin\n\u001a1\n2, ˜C1B(m, s, N, gk)\n\u001b\u001b\n≥\nmin\n\u001a1\n2, ˜C1C(m, s, N, gk)\n\u001b\n,\nwhere\nC(m, s, N, gk)\n:=\nexp\n\n−\nE\nnP\nxi∈Din,S,|Din,S|≤⌈2ms\nNd ⌉(gk(xi))2o\n2\n\n.\nBut (61) implies that for each j ∈Λs and k ∈Λ∗\nj,\nZ\nId g2\nk(x)dρX =\nZ\nBk\ng2\nk(x)dρX\n(76)\n=\n(N ∗)−2r\nZ\nBk\n[g(N ∗(x −ξk))]2 dρX ≥˜C2(N ∗)−2r−d,\nwhich yields\nE\n\n\n\n\n\nX\nxi∈Din,S,|Din,S|≤⌈2ms\nNd ⌉\n(gk(xi))2\n\n\n\n\n\n≥˜C2(N ∗)−2r−d 2ms\nN d .\nTherefore, for each j ∈Λs and k ∈Λ∗\nj\nmin\nh:Rm→{−1,1} Pr\n\u001a\nh(yD) ̸= ϵk\n\f\f|Din,S| ≤\n\u00182ms\nN d\n\u0019\u001b\n≥\nmin\n(\n1\n2, ˜C1 exp\n \n−\n˜C2(N ∗)−2r−d 2ms\nNd\n2\n!)\n.\nInserting the above estimate into (66), we then have\nmin\nh:Rm→{−1,1} Pr {h(yD) ̸= ϵk}\n≥\n1\n2 min\n(\n1\n2, ˜C1 exp\n \n−\n˜C2(N ∗)−2r−d 2ms\nNd\n2\n!)\n.\n14\nSince N ∗=\nl\u0000 ms\nN d\n\u00011/(2r+d)m\n, we see that, for any k ∈Λ∗\nj, j ∈\nΛs,\nmin\nh:Rm→{−1,1} Pr{h((y1, . . . , ym)) ̸= ϵk} ≥˜C3\nwith ˜C3 = 1\n2 min{1/2, ˜C1e−˜\nC2/2}. This completes the proof\nof Lemma 11.\nWe are now in a position to prove our main result.\nProof of Theorem 3. For fD ∈Ψm, deﬁne\nˆfD(x)\n:=\n(N ∗)d\nX\nk=1\nR\nId fD(x)g∗\nk,S(x)dρX\n∥g∗\nk,S∥ρ\ng∗\nk,S(x)\n=:\n(N ∗)d\nX\nk=1\nˆϵkg∗\nk,S(x),\n(77)\nwhere g∗\nk,S is deﬁned by (59). In view of Lemma 8, we observe\nthat ˆfD is the orthogonal projection of fD to HS,N∗. For N ∗≥\n4N and f ϵ\nρ ∈FS,N∗⊂HS,N∗with ϵ = (ϵ1, . . . , ϵ(N∗)d) and\nϵk the Rademacher random variable, it then follows from (58),\n(59), (56) and (63) that\n∥fD −f ε\nρ∥2\nρ ≥∥ˆfD −f ε\nρ∥2\nρ\n≥\nX\nj∈Λs\nX\nk′∈Λ∗\nj\nZ\nBk′\n[ ˆfD(x) −f ε\nρ(x)]2dρX\n=\nX\nj∈Λs\nX\nk′∈Λ∗\nj\nZ\nBk′\n\n\n(N∗)d\nX\nk=1\n(ˆϵk −ϵk)gk(x)\n\n\n2\ndρX\n=\nX\nj∈Λs\nX\nk′∈Λ∗\nj\nZ\nBk′\n[ˆϵk′ −ϵk′]2[gk′(x)]2dρX\n=\n(N ∗)−2r X\nj∈Λs\nX\nk′∈Λ∗\nj\n[ˆϵk′ −ϵk′]2\n×\nZ\nBk′\n[g(N ∗(x −ξk′))]2 dρX.\nDeﬁne ˜ϵk =\n\u001a\n1,\nˆϵk ≥0\n−1,\nˆϵk < 0.\nNoting that ˜ϵk is a decision\nof ϵk based on D, we may conclude that there exists some\nhk : Rm →{−1, 1} such that hk(y1, . . . , ym) = ˜ϵk. Since\n|ˆϵk −ϵk| ≥|˜ϵk−ϵk|\n2\n, we have from Lemma 10 that\n∥fD −f ε\nρ∥2\nρ ≥\n˜C2\n4 (N ∗)−2r−d X\nj∈Λs\nX\nk′∈Λ∗\nj\n[˜ϵk′ −ϵk′]2\n≥\n˜C2\n4 (N ∗)−2r−d X\nj∈Λs\nX\nk′∈Λ∗\nj\nI˜ϵk′̸=ϵk′.\nHence,\ninf\nfD∈Ψm E[∥fD −f ε\nρ∥2\nρ]\n≥\n˜C2\n4 (N ∗)−2r−d\ninf\n˜ϵ=(˜ϵ1,...,˜ϵ(N∗)d)\nX\nj∈Λs\nX\nk′∈Λ∗\nj\nPr{˜ϵk′ ̸= ϵk′}.\nBut Lemma 11 and (64) assure that for any set of independent\nRademacher random variables ϵ = (ϵ1, . . . , ϵ(N∗)d),\ninf\n˜ϵ\nX\nj∈Λs\nX\nk′∈Λ∗\nj\nPr{˜ϵk′ ̸= ϵk′}\n=\nX\nj∈Λs\nX\nk′∈Λ∗\nj\ninf\n˜ϵk′ Pr{˜ϵk′ ̸= ϵk′} ≥˜C3s\n\u0012N ∗\n2N\n\u0013d\n.\nTherefore, Lemma 7, together with (52), yields\nsup\nρ∈M(N,s,r)\ninf\nfD∈Ψm E[∥fD −fρ∥2\nρ]\n≥\nsup\nϵ\ninf\nfD∈Ψm E[∥fD −f ε\nρ∥2\nρ]\n≥\n˜C2\n4 (N ∗)−2r−d ˜C3s\n\u0012N ∗\n2N\n\u0013d\n=\n˜C2 ˜C3\n2d+2 (N ∗)−2r s\nN d .\nBy setting N ∗=\nl\u0000 ms\nNd\n\u00011/(2r+d)m\n, (3) implies N ∗≥4N.\nHence,,\nsup\nρ∈M(N,s,r,c0)\ninf\nfD∈Ψm E[∥fD −fρ∥2\nρ] ≥˜Cm−\n2r\n2r+d\n\u0010 s\nN d\n\u0011\nd\n2r+d ,\nwhere ˜C :=\n˜\nC2 ˜\nC3\n2d+2 . This completes the proof of Theorem 3. 2\nProof of Theorem 2:\nThe upper bound of (24) was\nestablished in Section IV. The lower bound of (24) is a direct\ncorollary of Theorem 3. This completes the proof of Theorem\n2.\nProof of Theorem 1:\nThe upper bound of (4) can be\nderived from (24) with p = 2 and the lower bound is\na consequence of Theorem 3. This completes the proof of\nTheorem 1.\nACKNOWLEDGEMENT\nThe research of CKC and BZ were partially supported\nby Hong Kong Research Council [Grant Nos. 12300917 and\n12303218] and Hong Kong Baptist University [Grant Nos.\nRC-ICRS/16-17/03 and RC-FNRA-IG/18-19/SCI/01]. The re-\nsearch of SBL was supported by the National Natural Science\nFoundation of China [Grant Nos. 61876133,11771012], and\nthe research of DXZ was partially supported by the Research\nGrant Council of Hong Kong [Project No. CityU 11306318]\nand carried out during his visit to the Erwin-Schroedinger\nInstitute in August, 2019.\nREFERENCES\n[1] Z. Akkus, A. Galimzianova, A. Hoogi, D. L. Rubin, and B. J. Erickson.\nDeep learning for brain MRI segmentation: state of the art and future\ndirections. J. Digital Imag., 30(4): 449-459, 2017.\n[2] Y. Chherawala, P. P. Roy, and M. Cheriet. Feature set evaluation for\nofﬂine handwriting recognition systems: application to the recurrent\nneural network model. IEEE Trans. Cyber., 46(12): 2825-2836, 2016.\n[3] C. K. Chui, X. Li, and H. N. Mhaskar. Neural networks for localized\napproximation. Math. Comput., 63: 607-623, 1994.\n[4] C. K. Chui, S. B. Lin, and D. X. Zhou. Construction of neural networks\nfor realization of localized deep learning. Front. Appl. Math. Statis., 4:\n14, 2018.\n[5] C. K. Chui, S. B. Lin, and D. X. Zhou. Deep neural networks for\nrotation-invariance approximation and learning. Anal. Appl., 17: 737-\n772, 2019.\n[6] C. K. Chui, S. B. Lin, and D. X. Zhou. Deep net tree structure for balance\nof capacity and approximation ability. Front. Appl. Math. Statis., 5: 46,\n2019.\n15\n[7] D. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber.\nDeep, big, simple neural nets for handwritten digit recognition. Neural\nComput., 22(12): 3207-3220, 2010.\n[8] F. Cucker and D. X. Zhou. Learning Theory: An Approximation Theory\nViewpoint. Cambridge University Press, Cambridge, 2007.\n[9] X. De Luna and M. G. Genton. Predictive spatio-temporal models for\nspatially sparse enviromental data. Statis. Sinica, 15: 547-568, 2005.\n[10] D. L. Donoho. Compressed sensing. IEEE Trans. Inf. Theory, 52: 1289-\n1306, 2006.\n[11] O. El Ayach, S. Rajagopal, S. Abu-Surra, Z. Pi, and R. W. Heath.\nSpatially sparse precoding in millimeter wave MIMO systems. IEEE\nTrans. Wireless Commun., 13: 1499-1513, 2014.\n[12] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press,\n2016.\n[13] B. Graham. Spatially-sparse convolutional neural networks. arXiv\npreprint arXiv:1409.6070.\n[14] A. Gittens and M. W. Mahoney. Revisitng the Nystr¨om method for\nimproved large scale machine learning. J. Mach. Learn. Res., 17: 1-65,\n2016.\n[15] Z. C. Guo, L. Shi, and S. B. Lin. Realizing data features by deep nets,\nIEEE Tran. Neural Netw Learn. Syst., In Press. (arXiv: 1901.00130).\n[16] L. Gy¨orfy, M. Kohler, A. Krzyzak and H. Walk. A Distribution-Free\nTheory of Nonparametric Regression. Springer, Berlin, 2002.\n[17] Z. Han, S. Yu, S. B. Lin, and D. X. Zhou. Depth-selection for deep\nReLU nets in feature extraction and generalization. IEEE Trans. Pattern\nAnal. Mach. Intel., Revised, 2019.\n[18] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for\ndeep belief netws. Neural Comput., 18: 1527-1554, 2006.\n[19] X. Hou, J. Harel, and C. Koch. Image signature: Highlighting sparse\nsalient regions. IEEE Trans. Pattern Anal. Mach. Intel., 34: 194-201,\n2012.\n[20] V. E. Ismailov. On the approximation by neural networks with bounded\nnumber of neurons in hidden layers. J. Math. Anal. Appl., 417: 963-969,\n2014.\n[21] M. Kohler. Optimal global rates of convergence for noiseless regression\nestimation problems with adaptively chosen design. J. Multivariate\nAnal., 132: 197-208, 2014.\n[22] M. Kohler and A. Krzyzak. Nonparametric regression based on hierar-\nchical interaction models. IEEE Trans. Inform. Theory, 63: 1620-1630,\n2017.\n[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation\nwith deep convolutional neural networks. NIPS, 2097-1105, 2012.\n[24] H. Lee, P. Pham, Y. Largman, and A. Y. Ng. Unsupervised feature learn-\ning for audio classiﬁcation using convolutional deep belief networks.\nNIPS, 469-477, 2010.\n[25] H. W. Lin, M. Tegmark, and D. Rolnick. Why does deep and cheap\nlearning works so well? J. Stat. Phys., 168: 1223-1247, 2017.\n[26] S. B. Lin, X. Guo, and D. X. Zhou. Distributed learning with regularized\nleast squares. J. Mach. Learn. Res., 18 (92): 1–31, 2017.\n[27] S. B. Lin. Limitations of shallow nets approximation. Neural Networks,\n94: 96-102, 2017.\n[28] S. B. Lin and D. X. Zhou. Distributed kernel-based gradient descent\nalgorithms. Constr. Approx., 47: 249-276, 2018.\n[29] S. B. Lin. Generalization and expressivity for deep nets. IEEE Trans.\nNeural Netw. Learn. Syst., 30: 1392-1406, 2019.\n[30] B. McCane and L. Szymanski. Deep radial kernel networks: approxi-\nmating radially symmetric functions with deep networks. arXiv preprint\narXiv:1703.03470, 2017.\n[31] V. Maiorov and A. Pinkus. Lower bounds for approximation by MLP\nneural networks. Neurocomputing, 25: 81-91, 1999.\n[32] M. Meister and I. Steinwart. Optimal Learning Rates for Localized\nSVMs. J. Mach. Learn. Res., 17: 1-44, 2016.\n[33] H. N. Mhaskar. Approximation properties of a multilayered feedforward\nartiﬁcial neural network. Adv. Comput. Math., 1: 61-80, 1993.\n[34] H. N. Mhaskar and T. Poggio. Deep vs. shallow networks: An approx-\nimation theory perspective, Anal. Appl., vol. 14, pp. 829-848, 2016.\n[35] I. Safran and O. Shamir. Dept-width tradeoffs in approximating natu-\nral functions with neural networks. arXiv reprint arXiv:1610.09887v2,\n2016.\n[36] P. Petersen and F. Voigtlaender. Optimal approximation of piecewise\nsmooth functions using deep ReLU neural networks. Neural Networks,\n108: 296-330, 2018.\n[37] C. Schwab and J. Zech. Deep learning in high dimension: Neural\nnetwork expression rates for generalized polynomial chaos expansions\nin UQ. Anal. Appl., 17: 19-55, 2019.\n[38] U. Shaham, A. Cloninger, and R. R. Coifman. Provable approximation\nproperties for deep neural networks. Appl. Comput. Harmon. Anal., 44:\n537-557, 2018.\n[39] C. E. Shannon. Communication in the presence of noise. Proc. Inst.\nRadio Enginer. 37: 10-21, 1949.\n[40] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van\nden Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M.\nLanctot, et al. Mastering the game of Go with deep neural networks and\ntree search. Nature, 529(7587): 484-489, 2016.\n[41] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. S. Huang, and S. Yan. Sparse\nrepresentation for computer vision and pattern recognition. Proc. IEEE,\n98: 1031-1044, 2010.\n[42] Q. Wu and D. X. Zhou. SVM soft margin classiﬁers: linear programming\nversus quadratic programming. Neural Comput., 17: 1160-1187, 2015.\n[43] J. Yang, K. Yu, Y. Gong, and T. S. Huang. Linear spatial pyramid\nmatching using sparse coding for image classiﬁcation. CVPR 1 (2):\n6-13, 2009.\n[44] D. Yarotsky. Error bounds for aproximations with deep ReLU networks.\nNeural Networks, 94: 103-114, 2017.\n[45] A. I. Zayed. Advances in Shannon’s sampling theory. Routledge, 2018.\n[46] D. X. Zhou and K. Jetter. Approximation with polynomial kernels and\nSVM classiﬁers, Adv. Comput. Math., 25: 323-344, 2006.\n[47] D. X. Zhou. Deep distributed convolutional neural networks: Universal-\nity. Anal. Appl., 16: 895-919, 2018.\n[48] D. X. Zhou. Universality of Deep Convolutional Neural Networks.\nAppl. Comput. Harmonic Anal., DOI: 10.1016/j.acha.2019.06.004\n(arXiv:1805.10769).\n[49] D. X. Zhou. Theory of convolutional neural networks: downsampling.\nNeural Networks, Minor Revision, 2019.\n[50] Z. H. Zhou, N. V. Chawla, Y. Jin, and G. J. Williams. Big data oppor-\ntunities and challenges: Discussions from data analytics perspectives.\nIEEE Comput. Intel. Mag., 9: 62-74, 2014.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-12-16",
  "updated": "2019-12-16"
}