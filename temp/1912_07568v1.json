{
  "id": "http://arxiv.org/abs/1912.07568v1",
  "title": "Simultaneous Detection of Multiple Appliances from Smart-meter Measurements via Multi-Label Consistent Deep Dictionary Learning and Deep Transform Learning",
  "authors": [
    "Vanika Singhal",
    "Jyoti Maggu",
    "Angshul Majumdar"
  ],
  "abstract": "Currently there are several well-known approaches to non-intrusive appliance\nload monitoring rule based, stochastic finite state machines, neural networks\nand sparse coding. Recently several studies have proposed a new approach based\non multi label classification. Different appliances are treated as separate\nclasses, and the task is to identify the classes given the aggregate\nsmart-meter reading. Prior studies in this area have used off the shelf\nalgorithms like MLKNN and RAKEL to address this problem. In this work, we\npropose a deep learning based technique. There are hardly any studies in deep\nlearning based multi label classification; two new deep learning techniques to\nsolve the said problem are fundamental contributions of this work. These are\ndeep dictionary learning and deep transform learning. Thorough experimental\nresults on benchmark datasets show marked improvement over existing studies.",
  "text": " \n \n1\n \nAbstract-- Currently there are several well-known approaches \nto non-intrusive appliance load monitoring – rule based, \nstochastic finite state machines, neural networks and sparse \ncoding. Recently several studies have proposed a new approach \nbased on multi-label classification. Different appliances are \ntreated as separate classes, and the task is to identify the classes \ngiven the aggregate smart-meter reading. Prior studies in this \narea have used off-the-shelf algorithms like MLKNN and \nRAKEL to address this problem. In this work, we propose a deep \nlearning based technique. There are hardly any studies in deep \nlearning based multi-label classification; two new deep learning \ntechniques to solve the said problem are fundamental \ncontributions of this work. These are deep dictionary learning \nand deep transform learning. Thorough experimental results on \nbenchmark datasets show marked improvement over existing \nstudies. \n \nIndex Terms-- deep learning, energy disaggregation, multi-\nlabel classification, non-intrusive load monitoring. \nI.  INTRODUCTION \nODAY there is a concerted effort towards sustainable \nEnergy. On one hand research is carried out on alternate \nsources of energy (Solar, Hydel, Wind etc.) to make them \ncommercially viable. On the other hand there is an effort to \nsave energy. Non-intrusive load monitoring (NILM) belongs \nto the later. Technically the goal is to disaggregate the energy \nconsumption of each appliance from the aggregate smart-\nmeter data. In the broader perspective, this information is fed \nback to the consumer, so that he/she can make an informed \nchoice about saving energy wherever possible. Since \nresidential and commercial buildings account for 40% of the \nglobal energy consumption [1]; such smart load management \nis expected to significantly save power. \nAppliance level load monitoring can be broadly categorized \nas, intrusive load monitoring and non-intrusive load \nmonitoring (NILM) [2]. The former is expensive and \ncumbersome to implement and requires installation of sensors \non each appliance; but intrusive load monitoring yields the \n                                                           \nThis work is supported by the DST IC-IMPACTS grant on Energy and \nWater Disaggregation for Non-Intrusive Load Monitoring in Buildings. \nV. Singhal, J. Maggu and A. Majumdar are with Indraprastha Institute of \nInformation Technology, Delhi 110020 India (e-mail: vanikas, jyotim, \nangshul@iiitd.ac.in).  \ngold standard. Ideally NILM should be able to disaggregate \nthe load from only the smart-meter reading. However, this is a \nvery difficult problem (especially at low sampling rates); in \npractice, a learning based paradigm is followed where the \ntraining stage is intrusive, but the testing / operation stage is \nnot. The building is instrumented during the training stage \ngather data, from which machine learning models are learnt. \nDuring operation, the sensors are removed, and the learnt \nmodels used to predict the consumption of each device.  \nThis aforesaid paradigm is not fully non-intrusive. The \ntraining stage is intrusive requiring deployment of multiple \nsensors. In recent times, a multi-label classification approach \noffers a fully non-intrusive alternative [3-6]. It does not \nrequire any instrumentation; it only requires the recording of \nthe state-of-the-appliance, i.e. whether it is ON or OFF. \nDuring the training stage, given the smart-meter reading and \nthe recorded states of the appliances, a machine learning \nmodel learns multi-label classification; here each appliance is \ntreated as a label – since several appliances can be ON at the \nsame time, it turns out to be a multi-label classification \nproblem. During the operational stage, the learnt model is used \nto predict the state of the appliances given the smart-meter \nreadings.  \nOne might argue that such techniques do not estimate the \nactual energy consumption of the device. This can be \naddressed by multiplying the state of the appliance with the \naverage power consumption of the device.  \nPrior studies have used off-the-shelf machine learning \nalgorithms for multi-label classification [7-11], e.g. Multi \nLabel K Nearest Neighbor (MLKNN), RAKEL (Random K-\nlabel sets). Our work is motivated by the success of deep \nlearning. Recent studies in almost all areas of data analysis \nshows widespread insurgence of deep learning – mainly owing \nto its superior results over traditional (shallow) machine \nlearning techniques. However, standard deep neural network \nmodels based on stacked autoencoder or deep belief networks \nare not capable of handling multi-label classification; this is \nbecause the output layer is trained by logistic regression or \nsoft-max – both of which lead to single classes. Therefore \nsuch existing deep learning tools cannot be used for the \ncurrent purpose.  \nIn this work, we propose two new deep learning techniques \nSimultaneous Detection of Multiple Appliances \nfrom Smart-meter Measurements via Multi-\nLabel Consistent Deep Dictionary Learning and \nDeep Transform Learning \nVanika Singhal, Jyoti Maggu, and Angshul Majumdar, Senior Member, IEEE \nT\n \n \n2\nfor multi-label classification. Given the shortcomings of \nexisting deep learning frameworks, we propose to build two \nnew multi-label deep learning tools based on dictionary \nlearning and transform learning. Experiments have been \ncarried out on two benchmark NILM datasets. The results \nshow significant improvement over existing techniques. \nRest of the paper is organized into several sections. The \nliterature review on NILM and background of the proposed \ntechniques is covered in the following section. Details of the \nproposed techniques is explained in the section 3. The \nexperimental results are shown in section 4. The conclusions \nare drawn in section 5. \nII.  LITERATURE REVIEW \nIn this section, we will discuss both non-intrusive load \nmonitoring methods and the signal processing / machine \nlearning background required for this work.  \nA.  Multi-label Classification for NILM \nMulti-label \nclassification \nalgorithms \nhave \nshown \nsignificant performance in identifying active appliances during \na time period. In binary label classification one sample \nbelongs to only one output class, whereas in multi-label \nproblem a sample may belong to more than one output classes. \nThe second scenario is appropriate for NILM since at a given \npoint of time it is likely that many appliances are running. \nNote that simplifying this to a single label multi-class \nclassification problem has been attempted before [12]. \nMulti-label classification algorithms can be divided into \ntwo categories: problem transformation and algorithm \nadaptation. Problem transformation methods transform a \nmulti-label problem into multi-class classification problem. \nExamples of such methods are Label power-set (LP) [7] and \nbinary relevance [13], which uses single label classifier like \nSVM as base classifiers. On the other hand, algorithm \nadaptation methods work by modifying the single label \nclassification algorithms to deal with multi-label problems. \nE.g. MLKNN is a multi-label classification methods, it is \nderived from k-nearest neighbor algorithm [9]. \nIn binary relevance, a separate binary classifier is trained \nfor each label, and the results from all the classifiers are \ncombined to produce the final output. The disadvantage of this \nmethod is that it does not consider any label dependency and \nfail to predict label combinations when some dependency \nexists. Label powerset (LP) trains a classifier for each pair of \nlabels. It takes label correlation into account but become very \ncomplex when number of classes increase. RAKEL \novercomes the disadvantage of label powerset method [7]. It \nbreaks large set of labels into smaller label sets, and each of \nthem is solved by applying LP method. MLKNN is an \nalgorithm adaptation method derived from KNN algorithm \n[9]. \nIn recent years NILM problem has been modeled as a \nmulti-label classification problem. In [3], temporal sliding \nwindow technique is employed to extract features from the \naggregated power data. Binary relevance, classifier chains and \nLP classifiers (SVM and decision tree as base classifiers) are \ntrained using extracted features. The paper [5], uses delay \nembedding to extract features from the time series data and \ncompare the results using multi-label classification algorithms. \nIn [3, 5], the performance of RAKEL and MLKNN are \ncompared on time series and Haar wavelet features extracted \nfrom the aggregated power data. \nThe primary shortcoming of all previous multi-label \nclassification techniques is that they consider all combination \nof classes as a separate class. This leads to combinatorial \ngrowth in the number of classes. This in turn leads to the \nexponential growth in model parameters. With limited training \nvolume, training so many parameters leads to over-fitting. \nThis leads to poor results in real applications.  \nB.  Rule Based Techniques \nIf one has access to high frequency data, rule based \nmethods offer a good solution. For example, in a typical \nhousehold, periodic cycles throughout the day in power \nconsumption may be related to the refrigerator. In the \nevenings, a different kind of cycle can be related to air \nconditioner (AC). In rule based techniques, instead of figuring \nout the rules manually traditional artificial intelligence \ntechniques are used to learn them.  \nOne popular approach in rule based systems are Decision \nTrees. Decision Tree uses greedy hunt’s algorithm [4]. It \nevaluates impurities on each node and then best split among \nattributes is decided by Gini index; which results in the nodes \nwith lowest value of impurity. The decision tree based \nclassification is implicitly a binary classification problem. But, \nin NILM classification, which is a multiclass problem, \ndecision tree based approach cannot be directly used [4, 14]. \nSo, one vs rest strategy is applied where one class is \nconsidered as positive class and all other classes belong to \nnegative class. In this way, multiple decision trees are built to \nsolve the multiclass problem. \nHowever, one must note that such rule based systems are \nonly successful when the sampling frequency is very high. In \npractice, the sampling frequencies are very low, the smart-\nmeter transmits the reading once every 10 or 15 minutes. At \nsuch temporal resolutions, the sharp edges required for rule \nbased systems are flattened out and the rule based methods \nperform poor. Also such rule based systems are good for \nbinary state (Fan, CFL, etc.) or multi-state (washer, dryer, AC \netc.) appliances; they cannot handle continuously varying \nloads like printers or computers.   \nC.  Stochastic Finite State Models \nEarly studies in NILM [2] modeled appliances as finite \nstate machines. Later on it was realized that stochastic finite \nstate machines are more suitable owing to noise in the data. \nSince the state of the appliances vary dynamically, Hidden \nMarkov Model (HMM) became an adequate tool for NILM \nclassification when there is only one appliance. As the name \nsuggests, HMM is based on the Markovian assumption – the \ncurrent state of the appliance is dependent on the previous \nstate. The HMM learns the state of the appliance given the \nobserved readings.  \nTypical NILM scenario consists of multiple appliances, \n \n \n3\nhence factorial hidden Markov model (FHMM) is used. In \nFHMM [15, 16], a separate and independent HMM represents \neach appliance and complex information can be captured by \ncombining outputs from all the HMMs. The product of expert \nis closely related to FHMM.  \nThe issues that hinder the performance of rule based \nsystems are also present in FHMM. They can be used for \nappliances where there is a marked difference in power \nconsumption across the states, but they fail to model \ncontinuously varying appliances. Besides, HMM based \ntechniques also rely on high frequency data that is impractical \nin most scenarios. \nD.  Neural Networks \nTraditional neural networks could not handle multi-label \nclassification problems. This is largely because of the choice \nof the supervision penalties like logistic regression or soft-\nmax. They were only applicable for single label classification \nproblems. However a recent study proposed a smart solution \nto the problem by learning one neural network for each load \n[17]. To the best of our understanding, the neural networks are \nrun on the aggregated data, where each device specific neural \nnetwork identifies if that device is turned ON or not.  \nThere is only one published study [18] that uses stacked \nautoencoders for multi-label classification. They learn a map \nfrom the deepest layer of the encoders to a multi-label target. \nThe idea has been first proposed in [19], but was used therein \nfor single label classification problems; it was generalized in \n[18]. \nE.  Dictionary Learning \nTo overcome the issues with low-frequency sampling and \ncontinuously varying appliances, a recent class of methods \nbased on dictionary learning have been proposed. \nSince dictionary learning is directly pertinent to this work, \nwe will discuss it in some detail. Kolter et al [20] introduced \ndictionary learning to solve disaggregation problems. The \nstudy assumed that there is training data collected over time, \nwhere the smart-meter logs only consumption from a single \ndevice only. This can be expressed as Xi where i is the index \nfor an appliance, the columns of Xi are the readings over a \nperiod of time. For each appliance (i) a basis (Di) is learnt, \nsuch that the data (Xi) can be regenerated from the associated \ncoefficients (Zi) \n, \n1...N\ni\ni\ni\nX\nD Z\ni\n\n\n             (1) \nThis is a typical dictionary learning problem with sparse \ncoefficients. It can be solved via the following minimization: \n2\n1\n,\nmin\ni\ni\ni\ni\ni\ni\nF\nD Z X\nD Z\nZ\n\n\n\n           (2) \nLearning the basis, constitutes the training phase. During \nactual operation, several appliances are likely to be in use \nsimultaneously. Dictionary learning based techniques make \nthe assumption that the aggregate reading by the smart-meter \nis a sum of the powers for individual appliances. Thus, if X is \nthe total power from N appliances (where the columns indicate \nsmart-meter readings over the same period of time as in \ntraining) the aggregate power is modelled as: \ni\ni\ni\ni\ni\nX\nX\nD Z\n\n\n\n\n             (3) \nGiven this model, it is possible to find out the loading \ncoefficients of each device by solving the following sparse \nrecovery problem, \n\n\n1\n2\n1\n1\n1\n,...,\n1\nmin\n|...|\n...\n...\nN\nN\nZ\nZ\nN\nN\nF\nZ\nZ\nX\nD\nD\nZ\nZ\n\n\n\n\n\n\n\n\n\n\n\n\n\n     (4) \nHere a positivity constraint on the loading coefficients is \nenforced as well. This is a convex problem since the basis are \nfixed. Once the loading coefficients are estimated, one can \neasily compute the power consumption from individual \ndevices. \nˆ\n, \n1...N\ni\ni\ni\nX\nD Z\ni\n\n\n             (5) \nThis was proposed as the initial technique in [20]. They \nproposed other formulations where discrimination was \nintroduced. However, such added penalties did not improve \nthe overall results significantly.  \nDictionary Learning based techniques in disaggregation has \nbeen gaining popularity ever since. In [21] a dynamic model is \nincorporated into the dictionaries. The most recent work in \nthis topic is deep sparse coding for energy disaggregation [22]; \nthey proposed a deep sparse coding framework by learning \nmultiple levels of dictionaries for each device.  \nF.  Transform Learning \nX\nD\nZ\n=\n \nFig. 1. Schematic Diagram for Dictionary Learning \n \nX\nZ\n=\nT\n \nFig. 2. Schematic Diagram for Transform Learning \n \nDictionary learning is a synthesis formulation. It learns a \ndictionary (D) such that it can synthesize the data (X) from the \nlearnt coefficients (Z) (see Fig. 1). Mathematically it is \nexpressed as (1), \nX\nDZ\n\n                     \nTransform Learning is the analysis equivalent of dictionary \nlearning. It learns an analysis dictionary / transform (T) such \nthat it operates on the data (X) to generate the coefficients (Z) \n(see Fig. 2). Mathematically this is represented as, \nTX\nZ\n\n                   (7) \n \n \n4\n(Synthesis) dictionary learning is very popular in signal \nprocessing and machine learning. It has hundreds of papers in \neach area. Transform learning on the other hand is relatively \nnew. There are hardly any papers on this topic outside signal \nprocessing. Therefore, we discuss it is slightly greater detail.  \nOne may be enticed to solve the transform learning \nproblem by formulating, \n2\n0\n,\nmin\n+\nF\nT Z TX\nZ\nZ\n\n\n             (8) \nUnfortunately, such a formulation would lead to degenerate \nsolutions; it is easy to verify the trivial solution T=0 and Z=0. \nIn order to ameliorate this the following formulation was \nproposed in [23] –  \n\n\n2\n2\n0\n,\nmin\n+\nlogdet\n+\nF\nF\nT Z TX\nZ\nT\nT\nZ\n\n\n\n\n    (9) \nThe factor \nlogdetT\n\nimposes a full rank on the learned \ntransform; this prevents the degenerate solution. The \nadditional penalty \n2\nF\nT\nis to balance scale; without this \nlogdetT\n\ncan keep on increasing producing degenerate \nresults in the other extreme.  \nNote that the sparsity constraint on the coefficients is not \nmandatory for machine learning problems. It is useful for \nsolving inverse problems in signal processing.  \nIn [23], [24], an alternating minimization approach was \nproposed to solve the transform learning problem (9). \n2\n0\nmin\nF\nZ\nZ\nTX\nZ\nZ\n\n\n\n\n          (10a) \n\n\n2\n2\nmin\n+\nlogdet\nF\nF\nT\nT\nTX\nZ\nT\nT\n\n\n\n\n     (10b) \nUpdating the coefficients (10a) is straightforward. It can be \nupdated via one step of Hard Thresholding [25]. This is \nexpressed as, \n\n\n(\n)\nZ\nabs TX\nTX\n\n\n\n           (11) \nHere\nindicates element-wise product.  \nFor updating the transform, one can notice that the \ngradients for different terms in (10b) are easy to compute. \nAfter ignoring the constants, the gradients are given by –  \n\n\n2\n2\nlogdet\nT\nF\nF\nT\nTX\nZ\nX\nTX\nZ\nT\nT\nT\nT \n\n\n\n\n\n\n\n\n \nIn the initial paper on transform learning [23], a non-linear \nconjugate gradient based technique was proposed to solve the \ntransform update. In the second paper [24], with some linear \nalgebraic tricks they were able to show that a closed form \nupdate exists for the transform.  \nT\nT\nXX\nI\nLL\n\n\n\n              (12a) \n1\nT\nT\nL YX\nUSV\n\n\n               (12b) \n\n\n2\n1/2\n1\n0.5\n(\n2\n)\nT\nT\nR S\nS\nI\nQ L\n\n\n\n\n\n        (12c) \nThe first step is to compute the Cholesky decomposition; \nthe decomposition exists since \nT\nXX\nI\n\n\nis symmetric \npositive definite. The next step is to compute the full SVD. \nThe final step is the update step. One must notice that \n1\nL is \neasy to compute since it is a lower triangular matrix.  \nThe proof for convergence of such an update algorithm can \nbe found in [26]. It was found that the transform learning was \nrobust to initialization. \nIII.  PROPOSED METHODOLOGY \nPosing NILM as a multi-label classification problem is \nrelatively new. As mentioned before, prior studies used \nexisting algorithms and applied it on the NILM datasets to \npublish papers. Our work is motivated by the success of deep \nlearning in almost all areas of data science and artificial \nintelligence. \nHowever standard deep neural networks are either \nunsupervised or can only handle multi-class problems; they \nare not geared for multi-label classification. Therefore, in this \nwork we propose two new deep learning approaches for multi-\nclass multi-label classification. \nThe first approach is based on the deep dictionary learning \nparadigm [27]. It has been introduced recently. It performs \nbetter than other deep learning techniques like stacked \nautoencoder and deep belief network on a variety of problems \n[28-30]. However deep dictionary learning has been an \nunsupervised learning tool so far. This will be the first work \non supervised deep dictionary learning. \nA.  Multi-Label Consistent Deep Dictionary Learning \nSince deep dictionary learning is a new approach, we will \nbriefly review it before proposing out formulation. In standard \n(shallow) dictionary learning, one level of dictionary is learnt \nto represent the training data (1). We repeat it for the sake of \nconvenience. \n1\nX\nD Z\n\n                   \nIn deep dictionary learning, multiple levels of dictionaries \nare used as basis for representing the data. In (13) we show it \nfor three levels. \n\n\n\n\n1\n2\n3\nX\nD\nD\nD Z\n\n\n\n             (13) \nHere D1, D2 and D3 are the three level dictionaries. The \nactivation function φ assures that the three levels are not \ncollapsible into one. There have been recent studies on deep \nmatrix factorization [31], [32]; these methods do not have the \nactivation \nfunction. \nDeep \ndictionary \nlearning \nis \na \ngeneralization of deep matrix factorization for arbitrary \nactivation functions.  \nUsually in deep dictionary learning, the dictionaries are \nlearnt by solving the following optimization problems –  \n\n\n\n\n1\n2\n3\n2\n1\n2\n3\n,\n,\n,\nmin\nF\nD D\nD Z X\nD\nD\nD Z\n\n\n\n         (14) \nThis is an unsupervised formulation. It does not use any \nclass information. One can imagine (14) in terms of a neural \nnetwork like interpretation, where the coefficients from the \nshallower levels feeds into the deeper levels. Z is the final \nlevel of coefficients. Note that deep dictionary learning is akin \nto a feed backward neural network. \nIn this work, we propose a supervised multi-label variant of \ndeep dictionary learning. Taking cues from label consistent \ndictionary learning [33], we propose a multi-label consistency \nterm. Basically, we learn a linear map such that the \n \n \n5\ncoefficients from the final level maps to the multi-label \ntargets. This is expressed as, \n\n\n\n\n1\n2\n3\n2\n2\n1\n2\n3\n,\n,\n, ,\nmin\nF\nF\nD D\nD Z M X\nD\nD\nD Z\nT\nMZ\n\n\n\n\n\n\n   (15) \nHere T are the targets. Each target has the same length as the \nnumber of appliance; the appliances are in an order. If an \nappliance is ON, the corresponding value is 1 or else it is 0. \nThe map M, projects the coefficients Z to the multi-label target \nlabels T.  \nSolving (15) is not trivial. In the first work on dictionary \nlearning, it has been solved greedily one layer at a time. This \nis sub-optimal; the shallower layers influence the deeper \nlayers but not the other way around. In an optimal solution all \nthe variables should be influencing each other. The \nmajorization minimization (MM) techniques used by Singhal \nand Majumdar [28] cannot be used either owing to the \nsupervision term. Besides MM converges very slowly. In this \nwork we solve it using the Augmented Lagrangian (AL) \napproach [34].  \nWe substitute\n\n\n2\n3\nZ\nD Z\n\n\n, \n\n\n1\n2\n2\nZ\nD Z\n\n\n. This leads to \nthe following AL formulation.  \n\n\n\n\n\n\n1\n2\n3\n1\n2\n2\n2\n1\n1\n,\n,\n, ,\n,\n,\n2\n2\n2\n3\n1\n2\n2\nmin\nF\nF\nD D\nD Z Z Z\nM\nF\nF\nX\nD Z\nT\nMZ\nZ\nD Z\nZ\nD Z\n\n\n\n\n\n\n\n\n\n\n\n      (16) \nFollowing the alternating direction method of multipliers we \ncan segregate (16) into the following sub-problems. In \nADMM, each of the sub-problems are for updating a single \nvariable, the rest are assumed to be constant.  \n1\n2\n1\n1\nP1:min\nF\nD\nX\nD Z\n\n \n\n\n\n\n2\n2\n2\n2\n1\n1\n2\n2\n1\n2\n2\nP2:min\nmin\nF\nF\nD\nD\nZ\nD Z\nZ\nD Z\n\n\n\n\n\n \n\n\n\n\n3\n3\n2\n2\n1\n2\n3\n2\n3\nP3:min\nmin\nF\nF\nD\nD\nZ\nD Z\nZ\nD Z\n\n\n\n\n\n \n\n\n2\n2\n2\n3\n2\n2\n1\n2\n3\nP4:min\nmin\n(\n)\nF\nF\nZ\nF\nF\nZ\nT\nMZ\nZ\nD Z\nT\nMZ\nZ\nD Z\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n1\n2\n2\n1\n1\n1\n2\n2\nP5:min\nF\nF\nZ\nX\nD Z\nZ\nD Z\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n2\n2\n2\n2\n2\n3\n1\n2\n2\n2\n2\n1\n2\n3\n1\n2\n2\nP6:min\nmin\nF\nF\nZ\nF\nF\nZ\nZ\nD Z\nZ\nD Z\nZ\nD Z\nZ\nD Z\n\n\n\n\n\n\n\n\n\n\n\n \n2\nP7:min\nF\nM\nT\nMZ\n\n \nNote that the activation function is unitary (tanh), acts \nelement-wise and hence is trivial to invert. That is the reason, \nwe can express P2, P3 and P6 in their equivalent forms. \nAll the sub-problems are linear least squares problems in \ntheir original or their equivalent forms. They all have closed \nfor solutions in the Moore-Penrose pseudo-inverse. \nThe problem is not convex, hence there is no global \nconvergence guarantee. But this is the case for most machine \nlearning models (with few exceptions like support vector \nmachine). Here, we solve the sub-problems till a local minima \nis reached. We stop the iterations when the objective function \ndoes not change substantially with further iterations.  \n \nAlgorithm for MLCDDL \nInitialize: D1, D2 and D3 randomly. From these, initialize Z by \nsolving \n\n\n\n\n2\n1\n2\n3\nmin\nF\nZ\nX\nD\nD\nD Z\n\n\n\n; the solution for the \nsame \nis \ngiven \nin \nP8-P10. \nInitialize \n\n\n1\n2\n3\n(\n)\nZ\nD\nD Z\n\n\n\nand\n\n\n2\n3\nZ\nD Z\n\n\n.  \nIterate (k) till convergence –  \n    Update:\n\n\n1\nT\nT\nM\nTZ\nZ Z\n\n\n   \n    Update:\n\n\n1\n1\n1\n1\n1\nT\nT\nD\nXZ\nZ Z\n\n\n \n    Update:\n\n\n1\n1\n2\n1\n2\n2\n2\n(\n)\nT\nT\nD\nZ Z\nZ Z\n\n\n\n\n \n    Update: \n\n\n1\n1\n3\n2\n1\n1\n1\n(\n)\nT\nT\nD\nZ\nZ\nZ Z\n\n\n\n\n \n    Update:\n\n\n\n1\n1\n3\n3\n3\n2\n(\n)\nT\nT\nT\nT\nZ\nM M\nD D\nM T\nD\nZ\n\n\n\n\n\n\n\n\n\n\n \n    Update:\n\n\n\n1\n1\n1\n1\n1\n2\n2\n(\n)\nT\nT\nZ\nD D\nI\nD X\nD Z\n\n\n\n\n\n\n \n    Update: \n\n\n\n1\n1\n2\n2\n2\n2\n1\n3\n(\n)\n(\n)\nT\nT\nZ\nD D\nI\nD\nZ\nD Z\n\n\n\n\n\n\n\n \n \nThis concludes the algorithm for training. For testing, one \nneeds to estimate the representation given the test sample x. \n\n\n2\n1\n2\n2\nmin\n(... (\n))\nN\nz\nx\nD\nD\nD z\n\n\n\n\n         (17) \nThis can be solved using substitutions as before. For first level \nit is \n\n\n1\n2\n3\n(\n)\nz\nD\nD z\n\n\n\n; for second level it is \n\n\n2\n3\nz\nD z\n\n\n. \nThe augmented Lagrangian with these proxies will be \nexpressed as, \n1\n2\n2\n1 1\n1\n2 2\n,\n,...,\n2\n2\n3\nmin\n(\n)\n(\n)\nN\nF\nF\nz z\nz\nF\nx\nD z\nz\nD z\nz\nD z\n\n\n\n\n\n\n\n\n       (18) \nHere the dictionaries learnt during the training process are \nused. \nUsing ADMM (18) is segregated into the following sub-\nproblems.  \n1\n2\n2\n1 1\n1\n2 2\nP8:min\n(\n)\nF\nF\nz\nx\nD z\nz\nD z\n\n\n\n\n \n2\n2\n2\n2\n1\n2 2\n2\n3 3\n2\n2\n1\n1\n2 2\n2\n3 3\nP9:min\n(\n)\n(\n)\nmin\n(\n)\n(\n)\nF\nF\nz\nF\nF\nz\nz\nD z\nz\nD z\nz\nD z\nz\nD z\n\n\n\n\n\n\n\n\n\n\n\n\n \n2\n2\n1\n2\n3\n2\n3\nP10:min\n(\n)\nmin\n(\n)\nF\nF\nz\nz\nz\nD z\nz\nD z\n\n\n\n\n\n \nAll the sub-problems have a closed form solution in the form \nof pseudo-inverse. \nOnce the representation is obtained, it is multiplied by the \nlearnt linear M to obtain t=Mz. Using an empirical threshold, \npositions of all elements in t above the threshold are \nconsidered as active classes for x. \nB.  Multi-Label Consistent Deep Transform Learning \nThe concept of deep dictionary learning has developed in \n \n \n6\nthe past year. Deep transform learning is even newer. There is \nonly a single paper on this topic by the authors [35] – that too \na greedy suboptimal one. This is the first work that introduces \nan optimal approach to deep transform learning, that too a \nsupervised one. \nThe main idea of deep transform learning is similar to that \nof deep dictionary learning. Instead of analyzing the data by a \nsingle level of transform (7), multiple levels of transforms are \nused to produce the final level of coefficients. It is similar to a \nfeed forward neural network. This is expressed as, \n\n\n\n\n3\n2\n1\nT\nT\nT X\nZ\n\n\n\n              (19) \nHere T1 operates on the data X to produce the first level of \ncoefficients. T2 analyzes the first level of coefficient to \nproduce the second level. Finally T3 operates the second level \nof coefficients to generate Z. In the only work on this topic, \nthis has been solved using a greedy sub-optimal approach.  \nIn this work, we extend deep transform learning to its \nsupervised version with a multi-label consistency terms. We \nsolve the problem in an optimal fashion using the variable \nsplitting Augmented Lagrangian approach followed by \nalternating direction method of multipliers.  \nThe complete formulation is as follows, \n\n\n\n\n\n\n1\n2\n3\n2\n2\n3\n2\n1\n,\n,\n, ,\n3\n2\n1\nmin\nlogdet\nF\nF\nT T T Z M\ni\ni\nF\ni\nT\nT\nT X\nZ\nT\nMZ\nT\nT\n\n\n\n\n\n\n\n\n\n\n\n     (20) \nWe substitute\n\n\n1\n1\nZ\nT X\n\n\nand\n\n\n2\n2\n1\nZ\nT Z\n\n\n. This leads to \nthe following AL. \n\n\n\n\n\n\n\n\n1\n2\n3\n1\n2\n2\n2\n3\n2\n,\n,\n, ,\n,\n3\n2\n1\n2\n2\n2\n2\n1\n1\n1\nmin\nlogdet\nF\nF\nT T T Z Z Z M\ni\ni\nF\ni\nF\nF\nT Z\nZ\nT\nMZ\nT\nT\nZ\nT Z\nZ\nT X\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       (21) \nUsing ADMM we can break it down into the following sub-\nproblems. \n\n\n\n\n1\n1\n2\n2\n1\n1\n1\n1\n2\n2\n1\n1\n1\n1\n1\nS1:min\nlogdet\nmin\nlogdet\nF\nF\nT\nF\nF\nT\nZ\nT X\nT\nT\nZ\nT X\nT\nT\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n2\n2\n2\n2\n2\n2\n1\n2\n2\n2\n2\n1\n2\n2\n1\n2\n2\nS2:min\nlogdet\nmin\nlogdet\nF\nF\nT\nF\nF\nT\nZ\nT Z\nT\nT\nZ\nT Z\nT\nT\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n3\n2\n2\n3\n2\n3\n3\nS3:min\nlogdet\nF\nF\nT\nT Z\nZ\nT\nT\n\n\n\n\n \n2\n2\n3\n2\nS4:min\nF\nF\nZ\nT Z\nZ\nT\nMZ\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n1\n1\n2\n2\n2\n2\n1\n1\n1\n2\n2\n1\n2\n2\n1\n1\n1\nS5:min\nmin\nF\nF\nZ\nF\nF\nZ\nZ\nT Z\nZ\nT X\nZ\nT Z\nZ\nT X\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n2\n2\n2\n3\n2\n2\n2\n1\nS6:min\nF\nF\nZ\nT Z\nZ\nZ\nT Z\n\n\n\n\n\n \n2\nS7:min\nF\nM\nT\nMZ\n\n \nAs in the case of synthesis deep dictionary learning, we \nhave segregated the complex problem into 7 simpler sub-\nproblems which are just least squares problems in their \noriginal or equivalent form. Expressing S1, S2 and S5 in the \nequivalent forms are trivial since the activations functions are \neasy to invert.  \nWe have used two stopping criteria for the iterations. The \nfirst one is a limit on the maximum number of iterations. The \nsecond one is local convergence of the objective function. The \nconvergence of problems such as ours via alternating direction \nmethod of multipliers have been recently proven in [36]. \n \nAlgorithm for MLCDTL \nInitialize: T1, T2 and T3 randomly. From these, initialize Z, Z1 \nand Z2 by by applying \n\n\n1\n1\nT X\nZ\n\n\n, \n\n\n2\n1\n2\nT\nT X\nZ\n\n\nand \n\n\n\n\n3\n2\n1\nT\nT\nT X\nZ\n\n\n\n.  \nIterate (k) till convergence –  \n    Update:\n\n\n1\nT\nT\nM\nTZ\nZ Z\n\n\n \n    Update: T1 by solving S1 – given in (12) \n    Update: T2 by solving S2 – given in (12) \n    Update: T3 by solving S3 – given in (12) \n    Update:\n\n\n\n1\n3\n2\nT\nT\nZ\nM M\nI\nT Z\nM T\n\n\n\n\n\n\n  \n    Update:\n\n\n\n1\n1\n1\n2\n2\n2\n2\n1\n(\n)\n(\n)\nT\nT\nT\nZ\nT T\nI\nT\nZ\nT X\n\n\n\n\n\n\n\n \n    Update:\n\n\n\n1\n2\n3\n3\n3\n2\n1\n(\n)\nT\nT\nZ\nT T\nI\nT Z\nT Z\n\n\n\n\n\n\n \n \nThis concludes the training phase. Generating the \nrepresentation from the test sample is simple for deep \ntransform learning. One simply needs to use, \n\n\n\n\n3\n2\n1\nz\nT\nT\nT x\n\n\n\n                (22) \nOnce \nthe \nrepresentation \nis \ngenerated, \nthe \ninference \n(classification) is drawn in a manner similar to deep transform \nlearning. \nC.  Computational Complexity \nFor deep dictionary learning, all the sub-problems require \nsolving a least square problem having a pseudo-inverse. The \nupper bound for complexity of computing it is O(nw) where n \nis the size of the matrix and w < 2.37; but note that this is an \nupper bound and is conjectured as w=2. In fact, if the sub-\nproblems are solved by something like conjugate gradient, the \ncomplexity is exactly O(n2). \nFor deep transform learning we need to solve two kinds of \nproblems. The updates for the transform require computing \nSVD which has a complexity of O(n3) and the updates for \ncoefficients are least square problems, whose complexity has \nalready been discussed. Therefore the overall complexity of \nthis procedure is O(n3).  \nHowever, in practice, the training complexity is hardly of \nany importance. What is of essence is the complexity during \nrun-time. The run-time complexity of deep dictionary learning \nis also O(nw) since it requires pseudo-inverses, see P8-P10. \nFurthermore it is iterative. But the computational complexity \n \n \n7\nof deep transform learning is only O(n) since it is non-iterative \nand only requires matrix products. \n \nIV.  EXPERIMENTAL RESULTS \nFor evaluating the performance of the proposed multi-label \nclassification algorithm two datasets have been used: \nReference Energy Disaggregation Dataset (REDD) [37]and \nPecan Street dataset [38]. The REDD dataset consists of both \naggregated and appliance level power data from six houses at \n1Hz. However we do not make use of the appliance level \nconsumption data; we only need its state. To emulate real \nworld conditions the samples are averaged over a time period \nof 1 minute for our experiments. Four high power consuming \ndevices used in our experiments are dishwasher, kitchen \noutlet, lighting and washer dryer \nPecan street dataset consists of one minute appliance level \nand building level electricity data from 240 houses. For \nexperiments subsets of 28 houses have been used. For this \nwork 4 most power consuming devices (site meter, air \nconditioner, electric furnace and sockets) are used for \nexperiments. To prepare training and testing data, aggregated \nand sub-metered data are averaged over a time period of 1 \nminutes Each training sample contains power consumed by a \nparticular device in one day while each testing sample \ncontains total power consumed in one day in particular house. \n80% of the houses are assigned to training set and 20% to the \ntest set. \nMetrics used in traditional single class classification \nproblems cannot be used for multi-label classification \nproblems. Prior studies in this area [3], [5] proposed using \nthree measures: macro F1, Micro F1 and energy error.   \nF1 score is widely used in single label classification \nproblems and is defined as: \n1\n2\n(\n,\n,\n)\n2\nF\nTP\nTP FP FN\nTP\nFP\nFN\n\n\n\n\n\n \nWhere TP is True positive, FP is false positive and FN is false \nnegative.   \nF1 macro and F1 micro are the measures derived from F1 \nscore. These are label based evaluation measures which \ndepend on the averaging method (macro or micro) used [3], \n[5]. F1 macro measure is computed by averaging the F1 scores \nfor each label. Whereas, F1 micro is computed after summing \ntrue positives, false positives and false negatives across all \nlabels.  \n1\n1\n1\n1\n1\n,\n,\nN\nN\nN\nmicro\ni\ni\ni\ni\ni\ni\nF\nF\nTP\nFP\nFN\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n1\n1\n1\n1(\n,\n,\n)\nN\nmacro\ni\ni\ni\ni\nF\nF TP FP FN\nN\n\n\n\n \nHere, TPi, FPi and FNi denote the number of true positives, \nfalse positive and false negative for the label i. N is the \nnumber of labels in the dataset. \nThe aforesaid measures are from the perspective of \ninformation retrieval. For us, a more useful metric would be \nthe predicted energy consumption. For that, the energy error \nhas been defined in [5]. It is defined as: \n \n1\n1\n1\n_\n_\n_\nN\nN\ni\ni\ni\ni\nN\ni\ni\nAverage\npower\nActual\npower\nerror\nActual\npower\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nWe have used two datasets and benchmarked our proposed \ntechnique with several state-of-the-art papers – prior studies in \nNILM [3, 5] used MLKNN [9] and RAKEL [7]; these \ntechniques use thresholded wavelet coefficients as input \nfeatures. We compare with MLKNN and RAKEL as \nbenchmarks.  \nComparison has also been done with AFAMAP [39]; \nhowever for this technique only average energy error is \nreported since it is not a classification method and hence F1 \nmeasures cannot be computed. \nFinally we compare with the multi-label consistent stacked \nautoencoder (MLCSAE) technique proposed in [17]. This is \nthe only known prior study on deep learning based multi-label \nclassification. We use the three layer architecture proposed in \n[17]. The number of nodes are halved in every layer following \nthe usual rule of thumb in such cases. \nFor our proposed techniques the raw data of an hour’s \nduration (therefore samples are of size 60) is used as input. In \ndeep learning there is no principled way to choose the number \nof layers or the number of nodes in each layer; it is dependent \non the collective experience of the researchers. Usually going \ndeeper helps, but with limited volume of training data, going \ndeeper also results in over-fitting; there is a trade-off. For \nmoderate sized problems, as ours, a three layer architecture \nusually yields the best results. Therefore we have used such an \narchitecture for MLCDDL (multi label consistent deep \ndictionary learning) and MLCDTL (multi label consisted deep \ntransform learning). The number of basis used are 120-80-50 \nfor MLCDDL and 120-80-40 for MLCDTL.  \nFor MLCDDL, we need to specify only a single parameter \n‘λ’. This parameter controls the relative importance of the \nfeature learning cost and the label consistency cost. Since \nthere is no reason to favor one over the other, we have kept it \nto be unity. MLCDTL requires specification of two parameters \nλ and ε. For the same reason as MLCDDL, we keep λ=1. The \nvalue of ε controls the relative importance of the data fidelity \nterm and the prior on the learnt transforms. We found that our \nmethod is robust to any value of this parameter between 0.01 \nand 1. Both the algorithms require specifying hyper-parameter \nμ; for arbitrary problems this needs to be tuned. But here the \nhyper-parameter carries a specific meaning; it controls the \nrelative importance of the different layers. Since there is no \nreason to favor one layer over another, we argue that keeping \nμ=1 is a sensible choice. For both the techniques we have used \ntanh activation function. \nThe experimental results are shown in Tables I and II. \nSince we want to showcase the change in results with layers \nwe show it for one to four layers. For the fourth layer, the \nnumber of atoms have been halved from the third layer in each \ncase. \n \n \n \n \n8\nTABLE I \nPERFORMANCE EVALUATION ON REDD DATASET \nMethod \nMacro F1-\nmeasure \nMicro F1-\nmeasure \nAverage \nenergy error \nMLCDTL (1 layer) \n0.6738 \n0.6884 \n0.0983 \nMLCDTL (2 layers) \n0.6814 \n0.6906 \n0.0539 \nMLCDTL (3 layers) \n0.6981 \n0.7001 \n0.0366 \nMLCDTL (4 layers) \n0.6901 \n0.6923 \n0.0453 \nMLCDDL (1 layer) \n0.6798 \n0.6846 \n0.0944 \nMLCDDL (2 layers) \n0.6857 \n0.6905 \n0.0592 \nMLCDDL (3 layers) \n0.7020 \n0.7046 \n0.0316 \nMLCDDL (4 layers) \n0.6951 \n0.6964 \n0.0427 \nMLKNN \n0.5931 \n0.6034 \n0.1067 \nRAKEL \n0.5334 \n0.5749 \n0.9948 \nAFAMAP \n- \n- \n0.2149 \nMLCSAE \n0.6237 \n0.6301 \n0.1145 \n \nTABLE II \nPERFORMANCE EVALUATION ON PECAN DATASET \nMethod \nMacro F1-\nmeasure \nMicro F1-\nmeasure \nAverage \nenergy error \nMLCDTL (1 layer) \n0.7039 \n0.7049 \n0.0236 \nMLCDTL (2 layers) \n0.7094 \n0.7101 \n0.0201 \nMLCDTL (3 layers) \n0.7104 \n0.7104 \n0.0115 \nMLCDTL (4 layers) \n0.7096 \n0.7098 \n0.0169 \nMLCDDL (1 layer) \n0.7065 \n0.7033 \n0.0275 \nMLCDDL (2 layers) \n0.7089 \n0.7062 \n0.0223 \nMLCDDL (3 layers) \n0.7100 \n0.7099 \n0.0178 \nMLCDDL (4 layers) \n0.7047 \n0.7026 \n0.0248 \nMLKNN \n0.6227 \n0.6263 \n0.0989 \nRAKEL \n0.6620 \n0.6663 \n0.9995 \nAFAMAP \n- \n- \n0.2371 \nMLCSAE \n0.6641 \n0.6703 \n0.0361 \n \nIn Tables 1 and 2, the aggregate results over all the houses \nare shown. We observe that our proposed deep methods \nsignificantly outperforms all other shallow and deep \ntechniques both in terms of F1-score as well as energy error. \nThe other observation is that, once we go deeper, the results \nimprove from layers 1 to 3; but when we go even deeper, the \nproblem of over-fitting arises and the results deteriorate. Of \nthe pre-existing shallow techniques, RAKEL, although \nperforms decent in terms of F1-score, is very poor in terms of \nenergy error; MLKNN yields more balanced results. The \nAFAMAP yields better results than RAKEL but is much \nworse than MLKNN. The other deep learning technique \nMLCSAE outperforms the shallow ones but is worse than \nours. \n \n \nTABLE III \nAPPLIANCE LEVEL EVALUATION ON REDD DATASET \nDevice \nMLCDTL  \nMLCDDL  \nMLKNN \nRAKEL \nAFAMAP \nMLCSAE \nError \nF1-score \nError \nF1-score \nError \nF1-score \nError \nF1-score \nError \nError \nF1-score \nDishwasher \n0.0086 \n0.5722 \n0.0179 \n0.5697 \n0.1250 \n0.4937 \n0.9964 \n0.3413 \n0.2726 \n0.0851 \n0.5124 \nKitchen \noutlet \n0.0556 \n0.5731 \n0.0492 \n0.5826 \n0.1647 \n0.5202 \n0.9952 \n0.4645 \n0.3249 \n0.1092 \n0.5253 \nLighting \n0.0841 \n0.7068 \n0.0099 \n0.6907 \n0.1105 \n0.6384 \n0.9943 \n0.5975 \n0.1953 \n0.1006 \n0.6591 \nWasher \ndryer \n0.0082 \n0.5702 \n0.0929 \n0.5648 \n0.1743 \n0.4304 \n0.9964 \n0.4302 \n0.1362 \n0.1149 \n0.5011 \n \nTABLE IV \nAPPLIANCE LEVEL EVALUATION ON PECAN DATASET \nDevice \nMLCDTL  \nMLCDDL  \nMLKNN \nRAKEL \nAFAMAP \nMLCSAE \nError \nF1-score \nError \nF1-score \nError \nF1-score \nError \nF1-score \nError \nError \nF1-score \nSite Meter \n0.0113 \n0.8404 \n0.0278 \n0.8306 \n0.0696 \n0.8096 \n0.9995 \n0.7072 \n0.1761 \n0.0432 \n0.8140 \nAir \nConditioner \n0.0125 \n0.5286 \n0.0188 \n0.5164 \n0.1381 \n0.5063 \n0.9995 \n0.5041 \n0.2263 \n0.0605 \n0.5062 \nElectric \nFurnace \n0.0059 \n0.5195 \n0.0229 \n0.5092 \n0.0899 \n0.5005 \n0.9995 \n0.4568 \n0.1104 \n0.0519 \n0.5001 \nSocket \n0.0149 \n0.5589 \n0.0333 \n0.5595 \n0.2696 \n0.5483 \n0.9995 \n0.5071 \n0.3172 \n0.0824 \n0.5413 \n \nIn Tables 3 and 4, we show the performances at the \nappliance level. We have shown results only for layer 3 since \nit yields the best results. We see that the conclusions remain \nthe same. We yield the best results in terms of all metrics. \nRAKEL yields by far the worst results. MLCSAE improves \nupon MLKNN in terms of energy error but not in terms of F1-\nscore. AFAMAP is only better than RAKEL.  \nV.  CONCLUSION \nIn this work, we propose a new approach to non-intrusive load \nmonitoring based on the multi-label classification framework. \nThere are several studies on the subject, that have used off-\nthe-shelf algorithms to the said problem. The contributions of \nthis work are far more fundamental. We propose two deep \nlearning based techniques to address the said problem. Note \nthat the deep learning techniques have been developed in a \nbottom-up manner for this study; we are not modifying any \ndeep learning algorithm to solve our problem. \nOur work is based on the dictionary learning and transform \nlearning based approaches. Unsupervised versions of deep \ndictionary learning have been proposed before; however this is \nthe first work on supervised deep dictionary learning. It is a \nmulti-label classification framework, but as a special case it \ncan solve the standard single label multi-class problem.  \nThe formulation based on transform learning is new. There \nis only a single study on greedy sub-optimal deep transform \nlearning for unsupervised problems. This is the first work that \nproposes an optimal supervised version. As in dictionary \nlearning, the single label multi-class problem is a special case \nof our proposed work. \n \n \n9\nIn this work, we posed non-intrusive load monitoring as a \nmulti-label classification problem. It is based on the \nassumption that during training only the aggregate data and \nthe logs (ON/OFF state) of the different devices are available. \nThis assumption was made in order to depict less intrusive \ntraining scenarios. However, if we assume that the complete \ndata, i.e. device levels power consumptions are available – \ninstead of classification we can pose the problem as a multi-\nvariate regression problems. Our proposed frameworks can \nnaturally handle it; instead of having 1 / 0 labels in the targets \nwe will have the corresponding power consumption levels for \neach device. During testing, the learnt model would directly \npredict the power instead of the state.  Experimental results on \nreal datasets show that our proposed method surpasses all \npopular \nand \nstate-of-the-art \ntechniques \nin \nmulti-label \nclassification. \nIn this work, we have assumed that the appliance level \npower consumption is not known, only their state is given. But \nif we consider the appliance level consumption during \ntraining, we can formulate NILM as a multi-variate regression \nproblem. In that case, instead of predicting the state (which is \na binary value), we will be predicting power consumption \n(real positive value). The methodology developed in this work \ncan automatically handle the said scenario. In the targets, \ninstead of recording the states, we need to have the power \nconsumptions; the rest would remain the same. We would like \nto try this approach in the future.  \nACKNOWLEDGEMENT \nThis work is partially supported by the Infosys Center for \nArtifical Intelligence at IIIT Delhi and by the DST IC-\nIMPACTS Indo-Canadian Grant. \nREFERENCES \n \n[1] L. Pérez-Lombard, J. Ortiz, and C. Pout, “A review on buildings energy \nconsumption information,” Energy Buildings, Vol. 40 (3), pp. 394–398, \n2008. \n[2] G. W. Hart, \"Nonintrusive appliance load monitoring,\" Proceedings of \nthe IEEE, Vol. 80 (12), pp. 1870-1891, 1992. \n[3] K. Basu, V. Debusschere, S. Bacha, U. Maulik and S. Bondyopadhyay, \n\"Nonintrusive Load Monitoring: A Temporal Multilabel Classification \nApproach,\" IEEE Transactions on Industrial Informatics, Vol. 11 (1), \npp. 262-270, 2015. \n[4] M. Nguyen, S. Alshareef, A. Gilani and W. G. Morsi, \"A novel feature \nextraction and classification algorithm based on power components \nusing single-point monitoring for NILM,\" IEEE Canadian Conference \non Electrical and Computer Engineering, pp. 37-40, 2015. \n[5] S. M. Tabatabaei, S. Dick and W. Xu, \"Toward Non-Intrusive Load \nMonitoring via Multi-Label Classification,\" IEEE Transactions on \nSmart Grid, Vol. 8 (1), pp. 26-40, 2017. \n[6] D. Li and S. Dick, \"Whole-house Non-Intrusive Appliance Load \nMonitoring \nvia \nmulti-label \nclassification,\" \nInternational \nJoint \nConference on Neural Networks, pp. 2749-2755, 2016. \n[7] G. Tsoumakas and I. Vlahavas, “Random k-labelsets: An ensemble \nmethod for multilabel classification,” European Conference on Machine \nLearning, pp. 406–417, 2007. \n[8] J. Read, B. Pfahringer, G. Holmes, and E. Frank, “Classifier chains for \nmulti-label classification,” Machine Learning, Vol. 85 (3), pp. 333–359, \n2011. \n[9] M.-L. Zhang and Z.-H. Zhou, \"A k-nearest neighbor based algorithm for \nmulti-label classification,\" IEEE International Conference on Granular \nComputing, pp. 718-721, 2005. \n[10] G. Tsoumakas and I. Katakis, “Multi-label classification: An overview,” \nInternational Joural of Data Warehouse and Mining, Vol. 3 (3), 2006. \n[11] G. Nasierding and A. Z. Kouzani, “Comparative evaluation of multi-\nlabel classification methods,” International Conference on Fuzzy \nSystems and Knowledge Discovery, pp. 679–683, 2012. \n[12] M. Gulati, S. S. Ram, A. Majumdar, and A. Singh, “Single point \nconducted emi sensor with intelligent inference for detecting it \nappliances,” IEEE Transactions on Smart Grid, 2017. DOI: \n10.1109/TSG.2016.2639295 \n[13] G. Tsoumakas, A. Dimou, E. Spyromitros, V. Mezaris, I. Kompatsiaris \nand I. Vlahavas, “Correlation-based pruning of stacked binary relevance \nmodels for multi-label learning”, Workshop on learning from multi-label \ndata, pp. 101-116, 2009. \n[14] S. Alshareef and W. G. Morsi, \"Application of wavelet-based ensemble \ntree classifier for non-intrusive load monitoring,\" IEEE Electrical Power \nand Energy Conference (EPEC), pp. 397-401, 2015. \n[15] J. Z. Kolter and T. Jaakkola, “Approximate inference in additive \nfactorial hmms with application to energy disaggregation,” Artificial \nIntelligence and Statistics, pp. 1472–1482, 2012. \n[16] S. Makonin, F. Popowich, I. V. Bajic, B. Gill and L. Bartram, \n“Exploiting HMM Sparsity to Perform Online Real-Time Nonintrusive \nLoad Monitoring”, IEEE Transactions on Smart Grid, Vol. PP (99), pp. \n1-11, 2015. \n[17] J. Kelly and W. Knottenbelt, “Neural NILM: Deep Neural Networks \nApplied to Energy Disaggregation”, ACM BuildSys, pp. 55-64. 2015 \n[18] C. K. Yeh, W. C. Wu, W. J. Ko and Y. C. F. Wang, “Learning Deep \nLatent Space for Multi-Label Classification”. AAAI, pp. 2838-2844, \n2017. \n[19] A. Majumdar, A. Gogna and R. K. Ward, “Semi-supervised Stacked \nLabel Consistent Autoencoder for Reconstruction and Analysis of \nBiomedical Signals”, IEEE Transactions on Biomedical Engineering, \nVol. 64 (9), pp. 2196 – 2205, 2017 \n[20] Z. Kolter, S. Batra and A. Y. Ng. “Energy Disaggregation via \nDiscriminative Sparse Coding”,  Neural Information Processing \nSystems, pp. 1153-1161, 2011. \n[21] E. Elhamifar and S. Sastry, “Energy Disaggregation via Learning \nPowerlets and Sparse Coding.,” AAAI, pp. 629–635, 2015. \n[22] S. Singh and A. Majumdar, “Deep Sparse Coding for Non-Intrusive \nLoad Monitoring”, IEEE Transactions on Smart Grid, vol.PP, no.99, \npp.1-1 \n[23] S. Ravishankar and Y. Bresler, “Learning sparsifying transforms,” IEEE \nTransactions on Signal Processing, Vol. 61 (5), pp. 1072–1086, 2013. \n[24] S. Ravishankar, B. Wen, and Y. Bresler, “Online sparsifying transform \nlearning—Part I: Algorithms,” IEEE Journal on Selected Topics on \nSignal Processing, Vol. 9 (4), pp. 625–636, 2015. \n[25] T. Blumensath and M. E. Davies, “Iterative thresholding for sparse \napproximations,” Journal of Fourier Analysis Applications, Vol. 14 (5), \npp. 629–654, 2008. \n[26] S. Ravishankar, and Y. Bresler, “Online sparsifying transform \nlearning—Part II: Convergence analysis,” IEEE Journal of Selected \nTopics in Signal Processing, Vol. 9 (4), pp. 637-646, 2015. \n[27] S. Tariyal, A. Majumdar, R. Singh, and M. Vatsa, “Deep dictionary \nlearning,” IEEE Access, vol. 4, pp. 10096–10109, 2016. \n[28] V. Singhal and A. Majumdar, “Majorization Minimization Technique \nfor Optimally Solving Deep Dictionary Learning,” Neural Process. \nLett., pp. 1–16, 2017. \n[29] I. Manjani, S. Tariyal, M. Vatsa, R. Singh, A. Majumdar, “Detecting \nSilicone Mask based Presentation Attack via Deep Dictionary \nLearning,” IEEE Transactions on Information Forensics and Security, \nVol. 12 (7), pp. 1713-1723, 2017. \n[30] V. Singhal, H. Agrawal, S. Tariyal and A. Majumdar, “Discriminative \nRobust \nDeep \nDictionary \nLearning \nfor \nHyperspectral \nImage \nClassification”, IEEE Transactions on Geosciences and Remote Sensing \n(accepted). \n[31] G. Trigeorgis, K. Bousmalis, S. Zafeiriou, and B. W. Schuller, “A deep \nmatrix factorization method for learning attribute representations,” IEEE \nTrans. Pattern Anal. Mach. Intell., Vol. 39 (3), pp. 417–429, 2017. \n[32] Z. Li and J. Tang, “Weakly supervised deep matrix factorization for \nsocial image understanding,” IEEE Transactions on Image Processing, \nVol. 26 (1), pp. 276–288, 2017. \n[33] Z. Jiang, Z. Lin, and L. S. Davis, “Label consistent K-SVD: Learning a \ndiscriminative dictionary for recognition,” IEEE Transactions on \nPattern Analysis and Machince Intelligence, Vol. 35 (11), pp. 2651–\n2664, 2013 \n \n \n10\n[34] R. Glowinski and P. Le Tallec, Augmented Lagrangian and operator-\nsplitting methods in nonlinear mechanics. SIAM, 1989. \n[35] J. Maggu and A. Majumdar, “Greedy Deep Transform Learning,” IEEE \nInternational Conference on Image Processing, 2017. \n[36] Y. Wang, W. Yin and J. Zeng, “Global convergence of ADMM in \nnonconvex nonsmooth optimization”, arXiv preprint arXiv:1511.06324, \n2015. \n[37] J. Z. Kolter and M. J. Johnson, “REDD: A public data set for energy \ndisaggregation research,” Workshop on Data Mining Applications in \nSustainability (SIGKDD), San Diego, CA, 2011, vol. 25, pp. 59–62. \n[38] Pecan Street Dataset. http://www.pecanstreet.org/category/dataport/. \n[39] R. Bonfigli, M. Severini, S. Squartini, M. Fagiani and F. Piazza, \n\"Improving the performance of the AFAMAP algorithm for Non-\nIntrusive Load Monitoring,\" 2016 IEEE Congress on Evolutionary \nComputation (CEC), Vancouver, BC, 2016, pp. 303-310. \n",
  "categories": [
    "eess.SP",
    "cs.LG"
  ],
  "published": "2019-12-11",
  "updated": "2019-12-11"
}