{
  "id": "http://arxiv.org/abs/1912.09261v1",
  "title": "Practical applicability of deep neural networks for overlapping speaker separation",
  "authors": [
    "Pieter Appeltans",
    "Jeroen Zegers",
    "Hugo Van hamme"
  ],
  "abstract": "This paper examines the applicability in realistic scenarios of two deep\nlearning based solutions to the overlapping speaker separation problem.\nFirstly, we present experiments that show that these methods are applicable for\na broad range of languages. Further experimentation indicates limited\nperformance loss for untrained languages, when these have common features with\nthe trained language(s). Secondly, it investigates how the methods deal with\nrealistic background noise and proposes some modifications to better cope with\nthese disturbances. The deep learning methods that will be examined are deep\nclustering and deep attractor networks.",
  "text": "arXiv:1912.09261v1  [cs.LG]  19 Dec 2019\nPractical applicability of deep neural networks for overlapping speaker\nseparation\nPieter Appeltans1, Jeroen Zegers2, Hugo Van hamme2\n1Department of Computer Science - KU Leuven - Belgium\n2Processing Speech and Images - ESAT - KU Leuven - Belgium\n{pieter,jeroen,hugo}.{appeltans,zegers,vanhamme}@kuleuven.be\nAbstract\nThis paper examines the applicability in realistic scenarios of\ntwo deep learning based solutions to the overlapping speaker\nseparation problem. Firstly, we present experiments that show\nthat these methods are applicable for a broad range of lan-\nguages. Further experimentation indicates limited performance\nloss for untrained languages, when these have common fea-\ntures with the trained language(s). Secondly, it investigates how\nthe methods deal with realistic background noise and proposes\nsome modiﬁcations to better cope with these disturbances. The\ndeep learning methods that will be examined are deep clustering\nand deep attractor networks.\nIndex Terms: Source Separation, Recurrent neural networks,\nArtiﬁcial neural networks\n1. Introduction\nThe overlapping speaker separation problem consists of separat-\ning the utterances of multiple speakers from a mixture. Many\ncues, such as the identity, position, and lip movement of the\nspeakers could be used to tackle this problem. This paper how-\never will focus on methods that only use a mono recording of\nthe mixture.\nThe current state-of-the-art methods to address the speaker\nseparation problem are based on deep neural networks. These\nmethods are able to obtain good text-independent separations\nwith no or limited prior information [2]. This is a big improve-\nment compared to previous methods like hidden Markov mod-\nels [3, 4, 5], independent component analysis [6], computational\nauditory scene analysis [7, 8] and non-negative matrix factorisa-\ntion [9, 10], which have limited separation performance or im-\npose restrictions on the speakers and vocabulary. This improved\nperformance comes at the cost of needing a lot of labelled train-\ning data (mixtures for which the desired separation is known)\nand demanding computations. The former can be tackled by\nartiﬁcially generating mixtures from two separate sources. The\nlatter becomes feasible due to the increasing parallel computa-\ntion power of graphical processing units.\nThis paper will focus on two such methods, namely deep\nclustering (DC) [2] and deep attractor networks (DAN) [11].\nBoth use (bidirectional) recurrent neural networks with long-\nshort term memory (LSTM) cells to map each bin in the log\nmagnitude spectrogram of the mixture to an embedding vec-\ntor. This mapping is learned from training data and is such that\nembedding vectors associated with bins dominated by the same\nspeaker are close. These vectors are then used to generate masks\nto ﬁlter out the individual speakers from the mixture. By using\nthese intermediate embedding vectors instead of directly out-\nputting the masks, the so called permutation problem [11] is\navoided.\nThis paper is organised as follows. In the remainder of this\nintroduction the speciﬁc details of the two examined method\nare further discussed. Section 2 presents experiments to asses\ntheir performance for six different languages.\nSubsequently,\nSection 3 will examine how well a model trained for one lan-\nguage generalises to another language and how this generali-\nsation changes when multiple languages are used for training.\nSection 4 discusses the applicability of these methods in the\npresence of background noise and proposes some modiﬁcations\nto improve their performance. Finally, Section 5 gives some\noverall conclusions.\n1.1. Deep clustering [2]\nIn DC, the network is trained by minimising the following loss\nfunction:\nN\nX\nn=1\n1\nK2n\n||VnVT\nn −YnYT\nn ||2\nF\n(1)\nwith N the number of mixtures in the training set, Kn the num-\nber of time-frequency bins in the spectrogram of mixture n, Vn\na Kn × D (with D the size of the embedding vectors) dimen-\nsional matrix with the embedding vectors, outputted by the net-\nwork, each normalised to (euclidean) norm 1, Yn a Kn × Cn\n(with Cn the number of speaker in mixture n) dimensional ma-\ntrix with y(t,f),c = 1 if speaker c dominates the time-frequency\nbin and y(t,f),c = 0 else. This cost function can be understood\nas follows.\nYnYT\nn and VnVT\nn are Kn × Kn dimensional matrices.\n{YnYT\nn }[(t, f), (t′, f ′)] is equal to one when time-frequency\nbins (t, f) and (t′, f ′) are dominated by the same speaker and\nzero in the other case and the ((t, f), (t′, f ′))th element of\nVnVT\nn is the euclidean inner product of v(t,f) and v(t′,f′).\nMinimising the cost function will thus tend to map embedding\nvectors associated with time-frequency bins dominated by the\nsame speaker near each other (v(t,f)T v(t′,f′) ≈1) and vec-\ntors associated with different speakers will tend to be orthogo-\nnal (v(t,f)T v(t′,f′) ≈0).\nAfter training, the network is used to separate new unseen\nmixtures.\nThis is done by applying its log magnitude spec-\ntrogram to the network and clustering the resulting embedding\nvectors with K-means. Each cluster represents one speaker and\nis used to create a binary mask to reconstruct the original utter-\nance of the speaker.\n1.2. Deep attractor networks [11]\nIn DANs the network is trained by minimising:\nN\nX\nn=1\n1\nKn ∗Cn\nCn\nX\nc=1\n||Smag\nn,c −Xmag\nn\n⊙Mn,c||2\nF\n(2)\nwith Xmag\nn\nthe magnitude spectrogram of mixture n, Smag\nn,c the\noriginal magnitude spectrogram of speaker c in mixture n, ⊙the\nelement wise product, and Mn,c the estimated mask for speaker\nc that is obtained as follows from the output of the network:\nMn,c[t, f] =\n1\n1 + exp(−ac · v(t, f))\n(3)\nwith ac the attraction point of speaker c, which is calculated as\nthe mean of the embedding vectors associated with the speaker:\nac =\nP\n(t,f) v(t, f)y(t,f),c\nP\n(t,f) y(t,f),c\n(4)\nBy minimising the above mentioned loss function, the network\nlearns to form an attraction point in embedding space for each\nspeaker, that attracts embedding vectors associated with time-\nfrequency bins of this source.\nTo separate an unseen mixture, its log magnitude spectro-\ngram is fed to the network and the obtained embedding vectors\nare used to create a ratio mask for each speaker using Eq. (3).\nBecause the partitioning of the bins (y(t,f),c) is not known (this\nis exactly what we are looking for), Eq. (4) cannot be used\nto calculate the attraction points. These are therefore approx-\nimated by the cluster centres found by K-means clustering of\nthe embedding vectors.\n2. Different languages\nIn [2] and [11] the separation performance of the above men-\ntioned methods is only examined for mixtures of English speak-\ners. This section presents experiments with six other languages,\nincluding a tonal language.\nIt is structured as follows: ﬁrst\nthe experiment design is explained; subsequently the separation\nscores are presented and discussed.\n2.1. Experiment set-up\nThe mixtures are generated using the global phone corpus [15]\nby overlaying utterances of two different speakers. To com-\npare with the results in [2] and [11], we used a similar set-up:\nthe signals were subsambled to 8kHz (to limit memory require-\nments and computation time); we calculate the (log magnitude)\nspectrogram using the short time Fourier transformation with a\ncosine window of 32 milliseconds and an overlap of 8 millisec-\nonds; the neural network consisted of two layers of 600 bidi-\nrectional LSTM cells, followed by a fully connected layer of\nneurons with linear activation function; a 20 dimensional em-\nbedding space was used. For each language the training set\nconsisted of 20 000 training mixtures, which each contained 2\nspeakers randomly sampled from a pool of 70 speakers, the de-\nvelopment set 3 000 mixtures sampled from 10 speakers and the\ntest set 3 000 mixtures sampled from 20 speakers. The speak-\ners in the different data sets are non-overlapping and in each set\nthere were as many male as female speakers.\nThe quality of the separations is quantiﬁed by the signal\nto distortion ratio (SDR) which measures the retrieved source\nenergy relative to the energy of interfering sources and artifacts.\n2.2. Results\nTable 1 gives the average SDR for DC and DAN for mixtures\nof two speakers in respectively Arabic, French, Mandarin, Por-\ntuguese, Spanish, and Swedish.\nThese scores are in line to\nwith the results in [2] and [11] for English. In our experiments\ndeep attractor networks outperform deep clustering for every\nlanguage and therefore seems the better choice. Both methods\nobtain their best score for Mandarin, which is the only tonal\nlanguage in our test set. This might indicate that tonality is\na useful feature for speaker separation but more research with\nother tonal languages is needed to support this thesis.\nTable 1: The average SDR (in dB) when trained and tested on\nthe same language.\nlanguage\ndeep clustering\ndeep attractor networks\nArabic\n7.50\n7.97\nFrench\n7.46\n8.20\nMandarin\n8.54\n8.86\nPortuguese\n7.24\n8.27\nSpanish\n6.72\n7.76\nSwedish\n6.93\n7.83\n3. Generalisation to an unseen language\nThis section will examine how well a network can separate mix-\ntures of an untrained language. The reasons for these experi-\nments are threefold. Firstly, it may not be reasonable to assume\nthe speaker’s language is know, e.g. when deploying a con-\nferencing service over the internet or when built into a mobile\nphone. Secondly, these results give an indication of the robust-\nness against different accents and dialects of a language. Lastly,\nthey might give some information on what cues, such as pho-\nnetic, phonotactic, lexical or grammatical, the methods exploit\nto separate speakers. In Section 3.1 the set-up of the experi-\nments is described. Next, experiments with networks trained\nwith one language are presented in Section 3.2. Section 3.3 ex-\namines whether the performance for trained and untrained lan-\nguages improves when more than one training language is used.\n3.1. Experimental set-up\nIn Section 3.2 we reuse the networks from Section 2 trained\nwith respectively French and Swedish speakers. The French\nnetwork is tested for mixtures with respectively Portuguese and\nMandarin speakers. The Swedish network is tested with mix-\nture of respectively Arabic and Spanish speakers. In Section 3.3\nnew networks are trained with {French, Turkish}, {French,\nTurkish, Japanese}, {Swedish, Turkish} and {Swedish, Turk-\nish, Japanese} datasets. For each network the training set con-\nsisted of 20 000 two-speaker mixtures sampled from a pool of\n70 speakers, equally balanced between languages and genders.\nThe development set consisted of 3 000 mixtures sampled from\n10 speakers. Mixtures consisted only of speakers of the same\nlanguage. The test sets are the same as in Section 2.\n3.2. Network trained with one language\nTable 2 gives the average separation performance of the meth-\nods for untrained languages. Also the difference with the score\nof the network trained with the considered language (Table 1) is\ngiven.\nWe noticed that for all languages there is a signiﬁcant de-\ncrease in separation quality compared to the network trained\nwith the test language itself. Portuguese, Arabic and Spanish\nhave a decrease of about 1dB, and the obtained separation are\nstill of good quality. For Mandarin on the other hand the de-\ncrease is more signiﬁcant, around 4dB. This seems to indicate\nthat the performance for untrained languages depends on the re-\nlation of the training and test language (closer related is better).\nThe fact that the methods do not break completely implies\nthat they do not create grammatical or lexical models, but at\nmost phonotactic or phonetic models. They do seem to do more\nthan tracking formants or pitch, which would make them almost\nlanguage independent.\nTable 2: The average SDR in dB for DC and DAN for an un-\nseen test language and the difference with the SDR for matched\nlanguage training.\nlanguage\ndeep clustering\ndeep attractor networks\nFrench network\nMandarin\n4.59 (-3.95)\n4.86 (-4.00)\nPortuguese\n6.33 (-1.13)\n7.22 (-0.98)\nSwedish network\nArabic\n5.98 (-1.52)\n7.01 (-0.96)\nSpanish\n6.01 (-0.71)\n7.20 (-0.55)\n3.3. Network trained with multiple languages\nTable 3 gives the separation quality for the networks trained\nwith multiple languages. The average SDR is reported for both\ntrained (t) languages and untrained (u) languages and the dif-\nference with scores of the networks trained with the language\nitself (Table 1). From the results we observe that for trained\nlanguages it is in most cases disadvantageous to replace a part\nof the training data with mixtures in other languages. For un-\ntrained languages on the other hand, it is in some cases advan-\ntageous to include multiple training languages instead of one.\nOnly for Portuguese there is a consistent decrease in perfor-\nmance compared to the results of the previous subsection.\nTable 3: The average SDR in dB for DC and DAN trained with\nmultiple languages for trained and untrained languages and the\ndifference with the SDR for matched language training.\nlanguage\ndeep clustering\ndeep attractor networks\n{French, Turkish} network\nFrench (t)\n6.92 (-0.55)\n7.75 (-0.45)\nMandarin (u)\n4.89 (-3.65)\n5.32 (-3.54)\nPortuguese (u)\n6.31 (-1.15)\n7.24 (-0.96)\n{French, Turkish, Japanese} network\nFrench (t)\n6.35 (-1.11)\n7.27 (-0.93)\nMandarin (u)\n4.57 (-3.97)\n5.34 (-3.52)\nPortuguese (u)\n5.94 (-1.53)\n6.77 (-1.44)\n{Swedish, Turkish} network\nSwedish (t)\n7.03 (0.10)\n6.97 (-0.87)\nArabic (u)\n6.45 (-1.02)\n6.71 (-1.26)\nSpanish (u)\n6.39 (-0.33)\n6.99 (-0.77)\n{Swedish, Turkish, Japanese} network\nSwedish (t)\n6.75 (-0.18)\n7.58 (-0.25)\nArabic (u)\n6.49 (-1.02)\n7.31 (-0.66)\nSpanish (u)\n6.16 (-0.56)\n7.34 (-0.42)\n4. Coping with background noise\nIn this section we examine the usability of deep clustering and\ndeep attractor networks in the presence of realistic background\nnoise and propose some modiﬁcations. This section is orga-\nnized as follows. First, the modiﬁcations to the original meth-\nods are presented. Subsequently, the set-up of the experiments\nis discussed. To conclude, the performance of the original and\nmodiﬁed methods are compared.\n4.1. Proposed modiﬁcations\n4.1.1. Modiﬁed network architecture\nFigure 1 shows the modiﬁed network architecture. Besides an\nembedding vector, it now has a (scalar) mask output for each\ntime-frequency bin. This scalar is an estimated ratio mask to\nsuppress the noise in that bin. Because noise and speech signals\nhave different roles and structures, there is no need for permu-\ntation invariance and the network can therefore directly output\na noise ﬁlter mask.\nbidirectional\nLSTM cells\nlog magnitude spectrogram\nv(0, 0) . . .\nv(T, F)\nlinear\nsigmoid\nα(0, 0) . . .\nα(T, F)\nFigure 1: Modiﬁed network architecture to better cope with\nbackground noise. It takes as input the log magnitude spec-\ntrogram of the mixture and has as output an embedding vector\nand a noise mask for each bin in the spectrogram.\n4.1.2. Deep clustering\nLoss function Eq. (1) is modiﬁed to:\nN\nX\nn=1\n1\n˜K2n\n|| ˜Vn ˜VT\nn −˜Yn ˜YT\nn ||2\nF + γ 1\nKn ||αn −α ideal\nn\n||2\nF (5)\nwith Kn as deﬁned previously, ˜Kn the number of bins not dom-\ninated by noise, ˜Yn and ˜Vn as deﬁned previously but the rows\nassociated with bins dominated by noise are set to zero, α the\nratio mask estimated by the network, and αideal the optimal ra-\ntio mask to ﬁlter the noise. The ﬁrst term is similar to Eq. (1).\nThe second term trains the network to generate ratio masks to\nﬁlter out the noise by penalizing the distance between the esti-\nmated and the optimal mask. The hyper-parameter γ weighs the\nimportance of separating the speakers and ﬁltering out noise. In\nour experiments in Section 4.3 γ is arbitrarily set to one.\nAlso the procedure to separate unseen mixtures is modi-\nﬁed. Firstly, before separating the speakers the estimated noise\nmask is used to suppress the noise. Secondly, only the embed-\nding vectors for which are the associated α is greater than 0.75\nare used in the clustering algorithm. The remaining bins are\nassigned to one of the speakers based on the distance between\ntheir embedding vector and the cluster centres of the speakers.\nBased on these clusters, a binary mask to separate the speakers\nis generated.\n4.1.3. Deep attractor networks\nFor deep attractor networks the loss function Eq. (2) is modiﬁed\nto:\nN\nX\nn=1\n1\nKn ∗Cn\nCn\nX\nc=1\n||Smag\nc,n −(Xmag\nn\n⊙Mnoise\nn\n)⊙Mn,c||2\nF (6)\nwith Mnoise[t, f] = α(t, f) the estimated noise mask. Also\nEq. (4) is modiﬁed:\nac =\nP\n(t,f) v(t, f)˜y(t,f),c\nP\n(t,f) ˜y(t,f),c\n(7)\nwith ˜y(t,f),c equal to one when speaker c dominates the bin, the\nbin has enough energy and α(t, f) bigger than 0.75 and zero\nin all other cases. Although this hard cut-off introduces dis-\ncontinuities and local optima in the cost function, an alternative\n(smoother) penalty for noisy bins did not lead to improved per-\nformance.\nTo separate new mixtures, a similar strategy as in Sec-\ntion 1.2 is applied, but with two slight modiﬁcations. Firstly,\nprior to separating the speakers, the noise was ﬁltered using\nthe estimated noise mask. Secondly, to estimate the attraction\npoints the K-means clustering is only applied to embedding vec-\ntors of time-frequency bins with enough energy and α above\n0.75.\n4.2. Experiment set-up\nThe utterances for the experiments in 4.3 were sampled from the\n‘Wall Street Journal Database’[18]. The noise signals were cho-\nsen from the ‘third CHiME speech separation and recognition\nchallenge’ data set [19], which contains recordings of realistic\nenvironment noise. As in the previous sections all signals were\nﬁrst downsampled to 8kHz. Six different two-speaker mixture\nsets were used:\n• A noise free training (20 000 mixtures) and development\nset (5 000 mixtures). The signals are normalised such\nthat the individual speakers have the same power.\n• A noisy training (100 000 mixtures) and development set\n(5 000 mixtures). The training set reuses each mixture\nof the noise free training set ﬁve times, each time with\ndifferent noise. The new development set is similar to the\nnoise free variant, only with noise added. The signals of\nthe speakers and the noise are normalised such that they\nhave the same power.\n• A noisy test set of 3 000 mixtures with different speak-\ners and utterances than in the training and development\nsets. The noise comes from different parts of the same\nrecordings as the training and development sets (for the\ntraining and development sets noise is sampled from the\nﬁrst 10 minutes of the recording, for the test set from the\nleftover part). The signals of the speakers and the noise\nare normalised such that they have equal power.\n• A second noisy test set of 3 000 mixtures. Similar to the\nprevious test set but now the signals are normalised such\nthat both speakers have equal power and the noise is 3dB\nweaker than each speaker.\n4.3. Results\nTable 4 compares the performance of the following ﬁve methods\nfor the two noisy test sets described in 4.2:\n• deep clustering trained without noise (DC no noise);\n• deep attractor networks trained without noise (DAN no\nnoise);\n• deep clustering with noise (DC with noise).\nDuring\ntraining the noise was considered as third speaker and\nthe network was trained to form three clusters: two asso-\nciated with speakers and one associated with the noise.\nDuring testing three reconstructions were created but\nonly the two that most resembled a speaker were used\nfor scoring;\n• modiﬁed deep clustering described in Section 4.1.2\n(modiﬁed DC);\n• modiﬁed deep attractor networks described in Sec-\ntion 4.1.3 (modiﬁed DAN).\nFor all methods a 20 dimensional embedding space was used.\nThe recurrent part of the networks trained without noise con-\nsisted of two layers with 800 bidirectional LSTM cells each.\nFor the networks trained with noise this consisted of four layers\nwith each 800 bidirectional LSTM cells.\nThe models trained without noise break down on noisy data.\nIncluding noise during training as a third speaker already leads\nto improved performance. The best SDRs are obtained with\nthe modiﬁed methods of Section 4.1. The SDR improvement\nw.r.t. “DC with noise” comes at a cost of a few dB in SNR,\nwhich seems less important since noise is not the main source\nof distortion.\nTable 4: The average SDR and SNR in dB for the test sets with\nrespectively the two speakers and the noise equally loud (0 dB)\nand the noise 3dB quieter than the speakers (3 dB)\n0 dB\n3 dB\nMethod\nSDR\nSNR\nSDR\nSNR\nDC no noise\n-1.75\n5.38\n1.99\n11.5\nDC with noise\n4.33\n16.1\n6.17\n19.2\nmodiﬁed DC\n5.11\n12.8\n7.43\n17.2\nDAN no noise\n-0.37\n5.83\n2.67\n10.8\nmodiﬁed DAN\n5.27\n13.5\n7.33\n17.4\n5. Conclusion\nDeep clustering and deep attractor networks are applicable to\nsource separation in a wide variety of languages, including tonal\nlanguages. Training models with (a combination of) related lan-\nguages yields only minor performance degradation compared to\ntraining on the target language. This observation supports the\nresults in [17], which showed that recurrent networks trained for\nspeech separation mainly exploit information with the time span\nof a phone and long span information is limited to speaker iden-\ntity while lexical or grammatical patterns are ignored. Further-\nmore,we extended deep clustering and deep attractor networks\nwith an estimated spectral mask to cope with noisy mixtures\nand showed signiﬁcant improvement over the baselines. A lim-\nitation of the current experiments is that they only examine how\nwell the methods perform for noise for which we have training\ndata. Future work will consider “untrained” noise types.\n6. References\n[1] E. C. Cherry, “Some experiments on the recognition of speech,\nwith one and with two ears,” The Journal of the Acoustical\nSociety of America, vol. 25, no. 5, pp. 975–979, 1953. [Online].\nAvailable: https://doi.org/10.1121/1.1907229\n[2] J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep\nclustering: Discriminative embeddings for segmentation and sep-\naration,” in 2016 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), March 2016, pp. 31–35.\n[3] J.\nR.\nHershey,\nS.\nJ.\nRennie,\nP.\nA.\nOlsen,\nand\nT. T.\nKristjansson,\n“Super-human multi-talker speech recognition:\nA\ngraphical\nmodeling\napproach,”\nComputer\nSpeech\nand\nLanguage,\nvol. 24,\nno. 1,\npp. 45 – 66,\n2010,\nspeech\nSeparation and Recognition Challenge. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0885230808000557\n[4] R. J. Weiss and D. P. W. Ellis, “Monaural speech separation using\nsource-adapted models,” in 2007 IEEE Workshop on Applications\nof Signal Processing to Audio and Acoustics, Oct 2007, pp. 114–\n117.\n[5] G. J. Mysore, P. Smaragdis, and B. Raj, “Non-negative hidden\nmarkov modeling of audio with application to source separation.”\nin LVA/ICA.\nSpringer, 2010, pp. 140–148.\n[6] Z. Koldovsky and P. Tichavsky, “Time-domain blind separation\nof audio sources on the basis of a complete ica decomposition of\nan observation space,” IEEE Transactions on Audio, Speech, and\nLanguage Processing, vol. 19, no. 2, pp. 406–416, Feb 2011.\n[7] K. Hu and D. Wang, “An unsupervised approach to cochannel\nspeech separation,” IEEE Transactions on audio, speech, and lan-\nguage processing, vol. 21, no. 1, pp. 122–131, 2013.\n[8] D. Wang and G. J. Brown, Computational auditory scene analy-\nsis: Principles, algorithms, and applications.\nWiley-IEEE press,\n2006.\n[9] J. Le Roux, F. J. Weninger, and J. R. Hershey, “Sparse nmf–half-\nbaked or well done?” Mitsubishi Electric Research Labs (MERL),\nCambridge, MA, USA, Tech. Rep., no. TR2015-023, 2015.\n[10] M. N. Schmidt, “Speech separation using non-negative features\nand sparse non-negative matrix factorization,” Elsevier, 2007.\n[11] Z. Chen, Y. Luo, and N. Mesgarani, “Deep attractor network\nfor single-microphone speaker separation,” in 2017 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), March 2017, pp. 246–250.\n[12] A. K. Jain, J. Mao, and K. M. Mohiuddin, “Artiﬁcial neural net-\nworks: a tutorial,” Computer, vol. 29, no. 3, pp. 31–44, Mar 1996.\n[13] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT\nPress, 2016.\n[14] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[15] T. Schultz, “Globalphone: a multilingual speech and text database\ndeveloped at Karlsruhe University,” in Seventh International Con-\nference on Spoken Language Processing, 2002.\n[16] E. Vincent, R. Gribonval, and C. F´evotte, “Performance measure-\nment in blind audio source separation,” IEEE transactions on au-\ndio, speech, and language processing, vol. 14, no. 4, pp. 1462–\n1469, 2006.\n[17] J. Zegers et al., “Memory time span in lstms for multi-speaker\nsource separation,” arXiv preprint arXiv:1808.08097, 2018.\n[18] J. S. Garofolo, D. Graff, D. Paul, and D. Pallett, “Wall street\njournal dataset,” https://catalog.ldc.upenn.edu/ldc93s6a, 1993.\n[Online]. Available: https://catalog.ldc.upenn.edu/ldc93s6a\n[19] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The third\nCHiME speech separation and recognition challenge: Dataset,\ntask and baselines,” in 2015 IEEE Workshop on Automatic Speech\nRecognition and Understanding (ASRU), Dec 2015, pp. 504–511.\n",
  "categories": [
    "cs.LG",
    "cs.SD",
    "eess.AS",
    "stat.ML"
  ],
  "published": "2019-12-19",
  "updated": "2019-12-19"
}