{
  "id": "http://arxiv.org/abs/1912.10804v1",
  "title": "Row-Sparse Discriminative Deep Dictionary Learning for Hyperspectral Image Classification",
  "authors": [
    "Vanika Singhal",
    "Angshul Majumdar"
  ],
  "abstract": "In recent studies in hyperspectral imaging, biometrics and energy analytics,\nthe framework of deep dictionary learning has shown promise. Deep dictionary\nlearning outperforms other traditional deep learning tools when training data\nis limited; therefore hyperspectral imaging is one such example that benefits\nfrom this framework. Most of the prior studies were based on the unsupervised\nformulation; and in all cases, the training algorithm was greedy and hence\nsub-optimal. This is the first work that shows how to learn the deep dictionary\nlearning problem in a joint fashion. Moreover, we propose a new discriminative\npenalty to the said framework. The third contribution of this work is showing\nhow to incorporate stochastic regularization techniques into the deep\ndictionary learning framework. Experimental results on hyperspectral image\nclassification shows that the proposed technique excels over all\nstate-of-the-art deep and shallow (traditional) learning based methods\npublished in recent times.",
  "text": "  \nAbstract—In \nrecent \nstudies \nin \nhyperspectral \nimaging, \nbiometrics and energy analytics, the framework of deep \ndictionary learning has shown promise. Deep dictionary learning \noutperforms other traditional deep learning tools when training \ndata is limited; therefore hyperspectral imaging is one such \nexample that benefits from this framework. Most of the prior \nstudies were based on the unsupervised formulation; and in all \ncases, the training algorithm was greedy and hence sub-optimal. \nThis is the first work that shows how to learn the deep dictionary \nlearning problem in a joint fashion. Moreover, we propose a new \ndiscriminative penalty to the said framework. The third \ncontribution of this work is showing how to incorporate \nstochastic regularization techniques into the deep dictionary \nlearning framework. Experimental results on hyperspectral \nimage classification shows that the proposed technique excels \nover all state-of-the-art deep and shallow (traditional) learning \nbased methods published in recent times.  \n \nIndex Terms—Classification, Supervised Learning, Deep \nLearning, Dictionary Learning, Hyperspectral Imaging.   \n \nI. INTRODUCTION \nN the recent past, deep learning has been gaining popularity \nin hyperspectral image classification [1-14]. All the \nstandard deep learning models – Deep Belief Network (DBN) \n[1-3], Stacked Autoencoder (SAE) [4, 5], Deep Recurrent \nNeural Network (DRNN) [6], variants of PCA-Net [7, 8] and \nConvolutional Neural Network [9-14] have been employed in \nthis context.  \nFor successful performance, deep learning requires a large \nvolume of training data. Unfortunately, this is impractical for \nhyperspectral \nimage \nclassification. \nTherefore, \nseveral \nmodifications need to be made to the classical deep learning \narchitectures to fit the said problem. In a recent work [15] a \nnovel active learning methodology has been proposed to \naddress the issue of limited training data.    \nIn a recent study [16] a new deep learning tool called deep \ndictionary learning (DDL) has been proposed. The basic idea \nthere in, is to use dictionary learning as basic building blocks \nfor a deeper architecture. Intuitively speaking, the coefficients \n/ feature from one layer of dictionary learning acts as an input \nto the subsequent layer leading to a deep network. It was \nshown in [16] that DDL can operate within the limited training \ndata regime and yet yield a performance superior to standard \n \nV. Singhal and A. Majumdar, are with Indraprastha Institute of \nInformation \nTechnology, \nNew \nDelhi, \n110020, \nIndia \n(email: \nvanikas@iiitd.ac.in and angshul@iiitd.ac.in) . \ndeep learning tools like DBN, SAE and CNN.  \nGiven the success of DDL in hyperspectral classification \n[16] amongst other areas like biometrics [17], energy analytics \n[18] and benchmark deep learning problems [19], we propose \nto build a new classifier based on the DDL framework. All \nprior studies on deep dictionary learning follow the greedy \ntraining paradigm; and hence are sub-optimal. Since each of \nthe layers are learnt separately, the shallower layers influence \nthe deeper ones, but not vice versa. For optimal training, all \nthe layers should be learnt jointly. Besides, [17-19] are all \nunsupervised learning approaches – they can only extract the \nfeatures \nbut \nrequire \nan \noff-the-shelf \nclassifier \nfor \nclassification. Only [16] is a supervised classification \ntechnique; albeit a suboptimal (greedy) one. \nThere are a few major contributions of this work. For the \nfirst time we show how the deep dictionary learning \nframework can be solved in a joint fashion. All the layers are \nupdated jointly along with the final level of coefficients. The \nsecond contribution of this work is in proposing a new \ndiscriminative penalty. The third contribution of this work is \nto show how stochastic regularization techniques can be \nincorporated into the deep dictionary learning framework. \nThis has not been attempted before.  \nThe rest of the work will be organized into several sections. \nRelevant background will be discussed in section II. The \nproposed technique will be described in section III. The \nexperimental results are detailed in section IV. Finally, the \nconclusions of this work and future direction of research will \nbe discussed in section V.  \nII. LITERATURE REVIEW \nA. Deep Learning in Hyperspectral Classification \nIn image analysis and computer vision problems CNN is the \nmost popular choice. However, CNNs are data hungry and fail \nto perform in limited training data regime. This is the reason \nwhy other deep learning techniques like DBN [1-3] continue \nto be popular. In [1], a basic DBN is used with logistic \nregression based classification. The formulation is made \nsupervised in [2] by using group-sparsity – a technique \nproposed in [20]. In [3], a diversifying pre-training technique \nis used for improving the performance of DBN based deep \nneural network.  \nThe problem of DBN is that, it can only handle limited \nvariability in the inputs; they need to be between 0 and 1. \nSome kind of batch normalization is required to bring all \ninputs within this range. However, given that the inputs (for \nRow-Sparse Discriminative Deep Dictionary \nLearning for Hyperspectral Image Classification \nVanika Singhal, and Angshul Majumdar, Senior Member, IEEE\nI\nimaging problems) are spatially correlated, such arbitrary \nchange in input values may decimate the local correlation \ninformation and reduce overall performance.  \nMuch like DBN, SAE’s have been used for hyperspectral \nimage classification. The work [4] uses a standard stacked \nautoencoder for classification using logistic regression. In [5], \nsome extra penalties are added to the basic stacked \nautoencoder formulation in order to incorporate contextual \ninformation.  \nUnlike DBNs stacked autoencoders can handle arbitrary \ninput values. But the problem with SAE’s is that given the \nsame volume of data, the number of parameters it needs to \nlearn is twice that of DBN (equal number of encoders and \ndecoders). Owing to this, SAEs are likely to suffer from over-\nfitting.  \nRecurrent Neural Network (RNN) and its variants (echo \nstate network, long short-term memory network) are \napplicable to ordinal data, for example time series data. In \nrecent times, high impact conferences like CVPR and ICCV \nare publishing papers on image analysis and computer vision \nusing RNNs (and its variants). In the same spirit, [6] used \nRNN and [7] used LSTM for analyzing hyperspectral images.  \nIt must be noted that there is no theoretical justification to \napply RNNs for image analysis. Such models are only \napplicable to ordered data (such as time series). Pixels are not \nordered. RNNs require arbitrary ordering in order to justify its \nusage. This arbitrary ordering may affect the overall \nperformance.  \nThe concept of PCA-Net [21] has been proposed recently. \nThe framework is similar to CNN. Each layer constitutes of a \nfiltering and pooling operation. But instead of convolving, the \nimage is filtered using principal component analysis. The \nstudy [8] proposes a non-linear variant of PCA-Net. In [9] \ninstead of using PCA, they use vertex component analysis into \nthe PCA-Net framework for hyperspectral image analysis.  \nAll the studies on CNN based analysis [10-14] follow the \nsame basic framework. They only differ from each other, in \nthe choice of the number of layers, size and number of \nconvolutional kernels and type of pooling used. It must be \nremembered that CNNs are data hungry, so studies in \nhyperspectral \nimage \nclassification \npropose \nslight \nmodifications to suit the goal.  \nOwing the problem of data scarcity in deep learning, an \ninteresting proposal has been laid in [15]. They propose an \nactive learning methodology to choose new training samples \nin a recursive fashion. However, active learning requires \nconstant feedback from the user; this does not seem to be very \npractical given the scenario. It is unlikely that such \nframeworks that require constants feedbacks will be pragmatic \nand popular. \nIt has been seen in [16] that the newly proposed framework \nof deep dictionary learning outperforms many traditional deep \nlearning tools in hyperspectral image analysis. In other areas \n[17-19] a similar trend has been envisaged. The DDL \nframework excels when there is limited training data is \navailable. \nB. Deep Dictionary Learning \nSince the concept of deep dictionary learning is relatively \nnew, we believe that a brief introduction on this topic will help \nreaders and make the paper self-contained. Let us reiterate the \nbasic idea of deep dictionary learning. In (shallow) dictionary \nlearning, one level of dictionary is used to represent the signal. \nIn DDL, multiple layers of dictionaries are used to represent \nthe signal.  \nFormally, in dictionary learning, the training set (X) is \ndecomposed into a dictionary (D) and a matrix of coefficients \n(Z). This is represented as, \nX\nDZ\n=\n                    (1) \nHere the training signals are stacked as columns of X; the \ncorresponding features are in Z. Usually the learning is \nexpressed as, \n2\n0\n,\nmin\nF\nD Z\nX\nDZ\nZ\nλ\n−\n+\n              (2) \nThe first term is for data fidelity; the l0-norm imposes sparsity \non the coefficients. There are well known algorithms to solve \nthe dictionary learning problem (2).  \nDictionary learning basically factors the matrix into the \ndictionary matrix and a coefficient matrix. In recent years, \nfollowing the success of deep learning, several studies have \nproposed deep matrix factorization [22, 23]; here the data is \ndecomposed into multiple layers of dictionaries (D1, D2, D3, \n…) and one final layer of coefficients (Z). Mathematically this \nis represented as follows (shown for three layers), \n1\n2\n3\nX\nD D D Z\n=\n                 (3) \nThe problem with this formulation is that, there is no \nactivation function between the layers; there is a possibility \nthat the multiple layers of dictionaries collapse into a single \nlayer.   \nThis issue is rectified in deep dictionary learning [19]; in \nthere an activation function is incorporated between each \nlayer. For a three layer DDL, the formulation is given by, \n(\n)\n(\n)\n1\n2\n3\nX\nD\nD\nD Z\nϕ\nϕ\n=\n              (4) \nUsually a non-linear activation function like sigmoid or tanh is \nused.  \nThe exact solution for DDL should have been, \n(\n)\n(\n)\n1\n2\n3\n2\n1\n2\n3\n0\n,\n,\n,\nmin\nF\nD D\nD\nZ X\nD\nD\nD Z\nZ\nϕ\nϕ\nλ\n−\n+\n       (5) \nHowever, it must be noted that none of the prior studies [16-\n19] solves (5) directly. All of them solve it greedily – one \nlayer at a time.  \nIn greedy learning, for the first layer, one substitutes \n(\n)\n(\n)\n1\n2\n3\nZ\nD\nD Z\nϕ\nϕ\n=\n. Therefore, (5) boils to the following \nproblem \n1\n1\n2\n1\n1\n,Z\nmin\nF\nD\nX\nD Z\n−\n                 (6) \nThis is a standard matrix factorization problem.  \nOnce the coefficients from the first layer is learnt, it is input \nto the second layer. Here one substitutes: \n(\n)\n2\n3\nZ\nD Z\nϕ\n=\n. This \nleads to –  \n(\n)\n1\n2\n2\nZ\nD Z\nϕ\n=\n                  (7) \nEquivalently this can be expressed as, \n(\n)\n1\n1\n2\n2\nZ\nD Z\nϕ −\n=\n                 \nSince the activation functions are unitary, inverting them is \ntrivial. It is easy to solve (7) via matrix factorization.  \n(\n)\n2\n2\n2\n1\n1\n2\n2\n,Z\nmin\nF\nD\nZ\nD Z\nϕ −\n−\n              (8) \nFor the last layer, we have, \n(\n)\n(\n)\n2\n3\n1\n2\n3\nZ\nD Z\nZ\nD Z\nϕ\nϕ −\n=\n≡\n=\n                (9) \nHere the same principle as before has been used to invert the \nactivation function and represent the last layer in the \nequivalent form.  \nSince the coefficients in the last layer are supposed to be \nsparse, one solves –  \n(\n)\n3\n2\n1\n2\n3\n0\n,\nmin\nF\nD\nZ\nZ\nD Z\nZ\nϕ\nλ\n−\n−\n+\n           (10) \nThis is the formulation used in [17-19].  \nThis concludes the training phase. In testing, the learnt \ndictionaries are used to generate the test feature. This is \nexpressed as, \n(\n)\n(\n)\n2\n1\n2\n3\n0\nmin\ntest\ntest\ntest\ntest\nF\nz\nx\nD\nD\nD z\nz\nϕ\nϕ\nλ\n−\n+\n       (11) \nUsing substitutions like before, this (11) too is solved greedily.  \nBuilding on this greedy framework, a supervised solution \nwas proposed for hyper-spectral image classification [16]. \nAfter the final layer of greedy deep dictionary learning, a label \nconsistency layer [34] was added, leading to the following \nformulation –  \n(\n)\n(\n)\n1\n2\n3\n2\n2\n1\n2\n3\n0\n,\n,\n,\nmin\nF\nF\nD D\nD\nZ X\nD\nD\nD Z\nZ\nT\nMZ\nϕ\nϕ\nλ\n−\n+\n+\n−\n  (12) \nHere T are the target labels and M the linear classifier map \nfrom the features (from the deepest layer) to the targets. This \nis a supervision term borrowed from [35]. \nHowever, note that [16] does not solve (12) jointly. Each \nlayer of deep dictionary learning (till the penultimate layer) is \nsolved piecemeal via a standard algorithm like KSVD; the \nfinal layer uses the implementation from [35].  \nIII. PROPOSED APPROACH \nThe problem with the greedy training paradigm in deep \nlearning is that it is sub-optimal. The shallower layers \ninfluence the deeper layer but not vice versa. This is the first \nwork that proposes to learn all the layers of deep dictionary \nlearning in joint fashion.  \nMost prior studies on DDL were unsupervised. It is well \nknown in machine learning literature, and specifically in deep \nlearning that discriminative learning schemes indeed improve \nclassification performance. This has been seen for both \nautoencoders [24, 25] and deep belief networks [26, 27]. This \nmotivates our work to incorporate such penalties into the deep \nlearning framework. \nOur proposed work is different to [16] in two major ways. \nFirst, our work optimally solves all the variables in a joint \nfashion. Second, it uses a more sophisticated supervision \npenalty than label consistency.  \nA. Formulation \nOur formulation stems from the basic definition of \n‘discrimination’ – samples within the same class will be close \nto each other and samples between classes will be far from \neach other.   \nThe basic structure of deep dictionary learning remains the \nsame as before; we would like to solve (5). However, since we \nare incorporating supervision / discrimination into the features \nwe propose modifications on top of the basic formulation. The \nfirst modification we propose is to impose class-sparsity, i.e. \ninstead of allowing features from the same class to be \narbitrarily sparse we impose structure. We postulate that the \nfeatures from the same class will have the same support, i.e. \nwill have the same sparsity pattern. In other words, the \npositions of the non-zero coefficients for features of the same \nclass will be the same for all samples. This has been \nintroduced into the DBN framework in [20] and in the \nautoencoder framework in [28]. This modification is \nrepresented as follows –  \n(\n)\n(\n)\n1\n2\n3\n2\n1\n2\n3\n2,0\n,\n,\n,\nmin\nc\nF\nD D\nD\nZ\nc\nX\nD\nD\nD Z\nZ\nϕ\nϕ\nλ\n−\n+ \n     (13) \nHere the l2,0-norm is defined as the number of non-zero rows \nof the matrix. This imposes row-sparsity. The sub-script ‘c’ \ndenotes the class and Z=[Z1|…|Zc|…]. Therefore the cost \nfunction imposes supervised class sparsity on each class ‘c’. \nThis cost is supervised but does not make the features \ndiscriminative. Since we are not imposing distinction between \nthe classes; there is no way we can guarantee that the features \nfrom different classes will not have the same support. To \nimpose discrimination, we need to ensure that the support \nbetween features of two classes are different. This is ensured \nby the second penalty.  \n(\n)\n(\n)\n1\n2\n3\n2\n1\n2\n3\n2,0\n,\n,\n,\n0\nmin\nc\nF\nD D\nD\nZ\nc\nk\nc\nk\nc\nX\nD\nD\nD Z\nZ\nZ\nZ\nϕ\nϕ\nλ\nµ\n≠\n−\n+\n−\n−\n\n\n    (14) \nHere \nk\nZ is the mean of the kth class repeated the same number \nof times as the number of samples in class c. This additional \nterm ensures that the support between two different classes (k \nand c) are as distinct as possible; since we are maximizing the \nl0-distance between the classes we need the negative sign.  \nNote that this is a completely different formulation from the \nlabel consistency penalty used in [16]; this new discrimination \nterm is a major contribution of this paper.  \nThis concludes the formulation for training. For testing, the \ngoal is to assign a class label for the test sample xtest. For that, \nwe need to generate the corresponding feature. This is \nachieved by solving the same problem as in the greedy case \n(11). This is repeated here for the sake of convenience.  \n(\n)\n(\n)\n2\n1\n2\n3\n0\nmin\ntest\ntest\ntest\ntest\nF\nz\nx\nD\nD\nD z\nz\nϕ\nϕ\nλ\n−\n+\n \nOnce the sparse feature is obtained, it is compared with the \ntraining samples for class assignment. In this work, the \nassignment is done in two ways.  \nIn the first approach, once the training is complete, the \nexact training features bears no importance, only the sparse \nbinary support of each class is required. An example may \nclarify the concept better.  \n \nFig. 1. Left – Sparse Features from Class 1 and its binary representation; \nRight – Features from Class 1 and its binary representation \n \nConsider a toy problem where the sparse features for classes \n1 and 2 are shown in Fig. 1. Class 1 has 3 samples and class 2 \nhas 4. Virtually, instead of storing the features after the \ntraining phase we can just store the binary representation \nvector for each class. Once the test feature is generated, we \ngenerate its corresponding binary representation. The binary \nrepresentation of the test data is compared with the binary \nrepresentation of each sample via a binary dot product / \nHamming distance. In practice, it boils down to the \ncomputation of the l0-norm between the features of the \ntraining samples and the test sample; note that the l0-norm is \ncomputed directly on the features and not on the binary \nrepresentation. This is very efficient with linear complexity. \nThe test sample is assigned to the class having the maximum \nsimilarity / minimum distance.  \nThere may be an issue with the aforesaid approach. Assume \nthat (for a problem different from the aforesaid toy problem), \nthe binary representation of class 1 is [0,0,1,1,0,0], and the \nbinary representation of class 2 is [0,0,1,0,1,0]. The binary \nrepresentation of the test sample is [0,0,1,1,1,0]. For such a \nsample, the Hamming distance from both the classes 1 and 2 \nwill be the same, and we cannot assign a unique class. In order \nto resolve this issue, in the second approach, instead of \ncomputing the l0-norm we compute the l1-norm between the \nfeatures of the training and test samples; since they account \nfor the magnitude and not only the position, such ties can be \navoided. We assign the test sample to the class having the \nminimum absolute distance.  \nB. Derivation \nFor training we need to solve (14). We following the Split \nBregman technique [29, 30]. We introduce a proxy \n(\n)\n(\n)\n1\n2\n3\nZ\nD\nD Z\nϕ\nϕ\n=\n. This leads to the following expression, \n(\n)\n(\n)\n1\n1\n2\n3\n2\n1\n1\n2,0\n0\n,\n,\n,\n,\n1\n2\n3\nmin\n \ns.t. \nc\nk\nc\nF\nD D\nD\nZ Z\nc\nk\nc\nX\nD Z\nZ\nZ\nZ\nZ\nD\nD Z\nλ\nµ\nϕ\nϕ\n≠\n−\n+\n−\n−\n=\n\n\n  (15) \nNote that Z1 basically corresponds to the features from the \nfirst level of dictionary learning.  \nIn the second step, we introduce another proxy variable \n(\n)\n2\n3\nZ\nD Z\nϕ\n=\n. This leads to, \n(\n)\n(\n)\n1\n2\n1\n2\n3\n2\n1\n1\n2,0\n0\n,\n,\n,\n,\n,\n1\n2\n2\n2\n3\nmin\n \ns.t. \n,\nc\nk\nc\nF\nD D\nD\nZ Z\nZ\nc\nk\nc\nX\nD Z\nZ\nZ\nZ\nZ\nD Z\nZ\nD Z\nλ\nµ\nϕ\nϕ\n≠\n−\n+\n−\n−\n=\n=\n\n\n (16) \nThe augmented Lagrangian is formulated from (16) by \nincorporating the Bregman relaxation variable (B1 and B2) for \neach proxy. \nWe form the augmented Lagrangian –  \n(\n)\n(\n)\n1\n2\n1\n2\n3\n2\n1\n1\n2,0\n0\n,\n,\n,\n,\n,\n2\n2\n1\n2\n1\n2\n2\n1\n2\n3\n2\nmin\n \nc\nk\nc\nF\nD D\nD\nZ Z\nZ\nc\nk\nc\nF\nF\nX\nD Z\nZ\nZ\nZ\nZ\nD Z\nB\nZ\nD Z\nB\nλ\nµ\nη\nϕ\nη\nϕ\n≠\n−\n+\n−\n−\n+\n−\n−\n+\n−\n−\n\n\n (17) \nHere B1 and B2 are the relaxation variables. \nThe final problem (17) is solved by the alternating direction \nmethod of multipliers (ADMM) [29]; the formulation (17) is \nsegregated into the following sub-problems.  \n1\n2\n1\n1\nP1:min\nF\nD\nX\nD Z\n−\n \n(\n)\n(\n)\n2\n2\n2\n2\n1\n2\n1\n1\n2\n2\n1\n1\n2\nP2:min\nmin\nD\nD\nF\nF\nZ\nD Z\nB\nZ\nB\nD Z\nϕ\nϕ −\n−\n−\n≡\n−\n−\n \n(\n)\n(\n)\n3\n3\n2\n2\n2\n1\n2\n3\n2\n2\n3\nP3:min\nmin\nF\nD\nD\nF\nZ\nD Z\nB\nZ\nB\nD Z\nϕ\nϕ −\n−\n−\n≡\n−\n−\n \n(\n)\n1\n2\n2\n1\n1\n1\n1\n1\n2\n1\nP4:min\nF\nF\nZ\nX\nD Z\nZ\nD Z\nB\nη\nϕ\n−\n+\n−\n−\n \n(\n)\n(\n)\n(\n)\n(\n)\n2\n2\n2\n2\n1\n2\n2\n1\n2\n1\n2\n3\n2\n2\n2\n1\n1\n2\n1\n1\n2\n2\n2\n3\n2\nP5:min\nmin\nF\nF\nZ\nF\nF\nZ\nZ\nD Z\nB\nZ\nD Z\nB\nZ\nB\nD Z\nZ\nD Z\nB\nη\nϕ\nη\nϕ\nη ϕ\nη\nϕ\n−\n−\n−\n+\n−\n−\n≡\n−\n−\n+\n−\n−\n \n(\n)\n(\n)\n2\n2\n2\n3\n2\n2,0\n0\n2\n2\n2\n2\n3\n2,0\n0\nP6:min\nmin\nc\nk\nc\nF\nZ\nc\nk\nc\nc\nk\nc\nZ\nF\nc\nk\nc\nZ\nD Z\nB\nZ\nZ\nZ\nZ\nB\nD Z\nZ\nZ\nZ\nη\nϕ\nλ\nµ\nη\nϕ\nλ\nµ\n≠\n≠\n−\n−\n+\n−\n−\n≡\n−\n−\n+\n−\n−\n\n\n\n\n \nFor sub-problems P2, P3, P4 and P5 the equivalent form is \ntrivial to obtain since the non-linear activation functions are \nunitary and invertible – a property used by all prior works on \nDDL. All the sub-problems except P6 are easy to solve, since \nthey are simple least squares problems with analytic solutions \nin the form of Moore Penrose Pseudoinverse.  \nSub-problem P6 is slightly more involved since it does not \nhave a direct solution. Although not exactly separable, we can \ndecouple the problem and solve for each Zc. This leads to,  \n(\n)\n2\n2\n2\n2\n3\n2,0\n0\nmin\nc\nc\nc\nc\nk\nc\nZ\nF\nZ\nB\nD Z\nZ\nZ\nZ\nη\nϕ\nλ\nµ\n−\n−\n+\n−\n−\n  (18) \nTo solve (18) we need to invoke the Split Bregman \ntechnique once more. We introduce a proxy variable \nk\nc\nP\nZ\nZ\n=\n−\n. The corresponding augmented Lagrangian after \nBregman relaxation is expressed as, \n(\n)\n(\n)\n2\n2\n2\n2\n3\n2,0\n0\n,\n2\nmin\nc\nc\nc\nc\nZ\nP\nF\nk\nc\nF\nZ\nB\nD Z\nZ\nP\nP\nZ\nZ\nC\nη\nϕ\nλ\nµ\nγ\n−\n−\n+\n−\n+\n−\n−\n−\n    (19) \nHere C is the relaxation variable. Using ADMM once again, \n(19) can be split into the following two sub-problems. \n(\n)\n(\n)\n2\n2\n2\n2\n3\n2,0\n2\nS1:min\nc\nc\nc\nc\nZ\nF\nk\nc\nF\nZ\nB\nD Z\nZ\nP\nZ\nZ\nC\nη ϕ\nλ\nγ\n−\n−\n+\n+\n−\n−\n−\n \n(\n)\n2\n0\nS2:min\nk\nc\nF\nP\nP\nZ\nZ\nC\nP\nγ\nµ\n−\n−\n−\n−\n \nSub-problem S1 is a solved problem. It can be solved by \nSimultaneous Orthogonal Matching Pursuit. Sub-problem S2 \nis not so trivial; instead of minimizing the l0-norm (as is done \nin all standard problems in optimization, signal processing and \nmachine learning) here we have to maximize it. However, the \nclosed form solution readily presents itself from the proximal \noperator for S2. It is, \n2\n \n/ 2\nk\nc\nk\nc\nZ\nZ\nC\nif\nZ\nZ\nC\nP\notherwise\nµ\nγ\nµ\nγ\n\n−\n+\n<\n−\n+\n\n= \n\n        (20) \nThis concludes the derivation of sub-problem P6. The final \nstep is to update the Bregman relaxation variables. This is \nachieved by simple gradient decent.  \n(\n)\n1\n1\n2\n2\n1\nB\nZ\nD Z\nB\nϕ\n←\n−\n−\n             (21a) \n(\n)\n2\n2\n3\n2\nB\nZ\nD Z\nB\nϕ\n←\n−\n−\n             (21b) \n(\n)\nk\nc\nC\nP\nZ\nZ\nC\n←\n−\n−\n−\n             (21c) \nThis concludes the training algorithm.  \nFor testing one needs to solve (11); repeated here for the \nsake of convenience. \n(\n)\n(\n)\n2\n1\n2\n3\n0\nmin\ntest\ntest\ntest\ntest\nF\nz\nx\nD\nD\nD z\nz\nϕ\nϕ\nλ\n−\n+\n  \nWe \nuse \nsimilar \nkind \nof \nproxies \nas \nbefore: \n(\n)\n(\n)\n1\n2\n3\ntest\ntest\nz\nD\nD z\nϕ\nϕ\n=\nfor \nthe \nfirst \nlevel \nand \n(\n)\n2\n3\ntest\ntest\nz\nD z\nϕ\n=\nfor the second. This leads to the following \naugmented Lagrangian formulation after introducing the \nBregman proxy variables b1 and b2. \n(\n)\n(\n)\n1\n2\n2\n1\n1\n0\n2\n,\n,\n2\n2\n1\n2\n2\n1\n2\n1\n2\n3\n2\n2\n2\nmin\ntest\ntest\ntest\ntest\ntest\ntest\nz\nz\nz\ntest\ntest\ntest\ntest\nx\nD z\nz\nz\nD z\nb\nz\nD z\nb\nλ\nη\nϕ\nη\nϕ\n−\n+\n+\n−\n−\n+\n−\n−\n  (22) \nFollowing ADMM, (22) can be split into the following sub-\nproblems.  \n(\n)\n(\n)\n2\n2\n2\n3\n2\n0\n2\n2\n1\n2\n2\n2\n3\n0\n2\nT1:min\ntest\ntest\ntest\ntest\nz\ntest\ntest\ntest\nz\nD z\nb\nz\nz\nb\nD z\nz\nη\nϕ\nλ\nη\nϕ\nλ\n−\n−\n−\n+\n≡\n−\n−\n+\n \n(\n)\n1\n2\n2\n1\n1\n2\n1\n1\n2\n1\n2\n2\nT2:min\ntest\ntest\ntest\ntest\ntest\nz\nx\nD z\nz\nD z\nb\nη\nϕ\n−\n+\n−\n−\n \n(\n)\n(\n)\n(\n)\n(\n)\n2\n2\n2\n2\n1\n2\n2\n1\n2\n1\n2\n3\n2\n2\n2\n2\n2\n1\n1\n2\n2\n1\n1\n2\n2\n3\n2\n2\n2\nT3:min\nmin\ntest\ntest\ntest\ntest\ntest\ntest\nz\ntest\ntest\ntest\ntest\nz\nz\nD z\nb\nz\nD z\nb\nz\nb\nD z\nz\nD z\nb\nη\nϕ\nη\nϕ\nη ϕ\nη\nϕ\n−\n−\n−\n+\n−\n−\n≡\n−\n−\n+\n−\n−\n \nSub-problem T1 (in the equivalent form) is a simple l0-\nminimization problem that can be either solved via orthogonal \nmatching pursuit or iterative hard thresholding. Sub-problem \nT2 and T3 (in equivalent form) are simple least squares \nminimization \nproblems \nwhich \ncan \nbe \nsolved \nvia \npseduoinverses. As in the training phase, we update the \nBregman relaxation variables by simple gradient descent.  \n(\n)\n1\n2\n1\n2\n1\ntest\ntest\nb\nz\nD z\nb\nϕ\n←\n−\n−\n            (23a) \n(\n)\n2\n2\n3\n2\ntest\ntest\nb\nz\nD z\nb\nϕ\n←\n−\n−\n            (23b) \nThis concludes the derivation of the testing algorithm.  \nBoth the training and the testing algorithms require solving \npseudoinverses and iterative thresholding. The complexity of \npseudoinverse is O(N3); N is the number of elements in the \nmatrix. The complexity (per iteration) of thresholding is O(N2) \n(for matrix vector products) + O(N) for thresholding. But \nabout O(N) steps are needed for convergence. Hence the total \ncomplexity \nof \nthe \nthresholding \nsub-problems \nare \napproximately O(N3). Therefore, the overall complexity of our \ntraining and testing algorithms are O(N3). \nC. Stochastic Regularization \n       \n \nFig. 2. (a) Left – DropOut Regularization. (b) Right – DropConnect1  \n \nThere are two stochastic regularization techniques that are \npopular in deep learning. They are DropOut [31] and \nDropConnect [32]. These regularization techniques have been \nused for traditional neural networks, but never for dictionary \nlearning. This is the first work that shows how such stochastic \nregularization techniques can be incorporated in deep \ndictionary learning.  \nThe main idea in DropOut is to randomly drop units (along \nwith their connections) from the network during training. This \nprevents units from co-adapting too much. Suppose we have \ntraining data X; in every iteration of DropOut some randomly \nchosen output units along with their connection weights are set \nto zero, as shown in Fig. 2. Here, out of three output neutrons, \nz2 (selected randomly) is dropped. \nDropConnect is a generalization of DropOut. Here a set of \nrandomly selected connections of network are set to zero. It \nworks similar to DropOut regularization technique, except \nthat, instead of dropping the whole unit, some connections are \ndropped. This makes output units partially active, as shown in \nFig. 2. The dotted lines here show the dropped connections. \nBoth of them are regularization techniques. Given that we \nare interested in addressing the hyperspectral image \nclassification problem, where training data is always \nparsimonious incorporating these into the deep dictionary \nlearning framework to prevent over-fitting may prove \nbeneficial.  \nWe can incorporate DropOut type regularization, by \nrandomly imputing some randomly chosen elements in the \ncoefficients of each layer (Z1, Z2 and Z) with zeroes. In each \niteration, once the coefficients are obtained by solving sub-\nproblems P4, P5 and P6, we randomly drop some of their \nvalues to zeroes before they are used for updating the \ndictionary elements via P1, P2 and P3. The proportion of the \nelements to be dropped is user-defined. However, it must be \nnoted that since we impose sparsity in the output, randomly \ndropping the coefficients from Z may not be sensible. It would \nbe beneficial to restrict the droppings to the intermediate \nlayers only.  \n \n1 http://cs.nyu.edu/~wanli/dropc/ \nSimilarly, \nwe \ncan \nincorporate \nDropConnect \ntype \nregularization into the deep dictionary learning framework. \nHere, the dictionary atoms play the role of connections. \nTherefore, to drop connections, we can impute some randomly \nchosen elements in the dictionaries to be zeroes. After \nupdating the dictionaries via P1, P2 and P3, we can impute \nsome elements (randomly chosen) in the dictionaries to be \nzeroes to emulate DropConnect. Here too, the percentage of \ndropping need to be user-defined.  \nIt must be noted that both DropConnect and DropOut type \nregularizations are only used till the pre-final iteration. In the \nfinal iteration, the obtained values of the dictionaries and the \ncoefficients are not perturbed in any fashion.  \nIV. EXPERIMENTAL RESULTS \nWe evaluate our proposed technique on two benchmark \ndatasets. \n• The Indian Pines dataset was collected by the Airborne \nVisible/Infrared Imaging Spectrometer in Northwestern \nIndiana, with a size of 145 × 145 pixels with a spatial \nresolution of 20 m per pixel and 10-nm spectral resolution \nover the range of 400–2500 nm. As is the usual protocol, \nthe work uses 200 bands, after removing 20 bands \naffected by atmosphere absorption. There are 16 classes; \nthe number of training and test samples is displayed in \nTable I.  \n• This Pavia university dataset is acquired by reflective optics \nsystem imaging spectrometer (ROSIS). The image is of \n610 × 340 pixels covering the Engineering School at the \nUniversity of Pavia, which was collected under the \nHySens project managed by the German Aerospace \nAgency (DLR). The ROSIS-03 sensor comprises 115 \nspectral channels ranging from 430 to 860 nm. In this \ndataset, 12 noisy channels have been removed and the \nremaining 103 spectral channels are investigated in this \npaper. The spatial resolution is 1.3 m per pixel. The \navailable training samples of this data set cover nine \nclasses of interests. Table II provides information about \ndifferent classes and their corresponding training and test \nsamples. \nFor both the datasets, the experimental protocol is borrowed \nfrom [6]; it is a very challenging protocol as the number of \ntraining samples is extremely limited. Some deep learning \ntechniques (such as [1]) use about 90% of the data for training \nand validation. Prior studies based on support vector \nmachines, random decision forests, sparse representation \nbased classification and dictionary learning trained with only \n10% of the data. Here, we use even less volume of training \ndata. But this protocol reflects the real life scenario \nappropriately.   \n \nTABLE I \nTRAINING AND TEST SAMPLES FOR INDIAN PINES \nClass \nTraining Samples \nTest Samples \nAlfalfa \n50 \n1384 \nCorn-notill \n50 \n784 \nCorn-min \n50 \n184 \nCorn \n50 \n447 \nGrass-pasture \n50 \n697 \nGrass-trees \n50 \n439 \nGrass-pasture mowed \n50 \n918 \nHay-windrowed \n50 \n2418 \nOats \n50 \n564 \nSoybean-notill \n50 \n162 \nSoybean-mintill \n50 \n1244 \nSoybean-clean \n50 \n330 \nWheats \n50 \n45 \nWoods \n15 \n39 \nBuildings-glass-trees \n15 \n11 \nStone-steel-towers \n15 \n5 \nTotal \n695 \n9671 \n \nTABLE II \nTRAINING AND TEST SAMPLES FOR PAVIA UNIVERSITY \nClass \nTraining Samples \nTest Samples \nAsphalt \n548 \n6631 \nMeadows \n540 \n19649 \nGravel \n392 \n2099 \nTrees \n524 \n3064 \nMetal sheets \n265 \n1345 \nBare soil \n532 \n5029 \nBitumen \n375 \n1330 \nBricks \n514 \n3682 \nShadows \n231 \n947 \nTotal \n3921 \n42776 \n \nA. Comparison with deep techniques \nIn this work we are going to compare with the latest deep \nlearning-based techniques. The DBN based technique we \ncompare against is [2]; the group-sparse formulation used \nthere-in has shown excellent results for hyperspectral image \nclassification. This technique is dubbed GBN in [2]. The \nbenchmark for deep autoencoder (DAE) is the technique \nproposed in [5]. Following [6], we compare with their \nproposed method with recurrent neural network (RNN) based \nimplementation. Of the PCA-Net variants, we compare against \n[7] that uses a non-linearity and fuses the spectral and spatial \nfeatures in the final densely connected layer for classification. \nThis technique [7] has been called NSS-Net. The final \ntechnique that we compare against is called deep fusion CNN \n(DFNN) [33]. It is a very recent work and to the best of our \nknowledge yields the best possible results. We also compare \nwith the Robust deep dictionary learning formulation (RDDL) \nproposed in [16]. For all the methods mentioned here, we \nemploy the best architecture and the corresponding parameter \nsettings used in the papers.  \nFor the proposed row sparse deep dictionary learning \n(RSDDL) formulation we use a three-layer architecture. For \nboth the datasets the number of atoms in each layer are 100-\n50-25. The value of the sparsity parameter used here is λ=0.1 \nand the diversity parameter is μ=0.5. We found that for both \nthe parametric values, our method is fairly robust. Our \nalgorithm also needs specification of the hyper-parameters. \nUsually in split Bregman techniques these need to be tuned. \nHowever, for our case, these hyper-parameters have special \nmeanings. They imply the relative weights we give to each \nlayer. Since there is no reason to favor one layer over the \nother, all of them have been fixed to unity. There is only one \nhyper-parameter we need to tune, i.e. γ in (18). We kept its \nvalue to be 0.1. Since we are using the Split Bregman \ntechnique, the algorithm is robust to whatever value of γ we \nchoose between 0.01 and 0.1. All the Bregman relaxation \nvariables have been initialized to unity. \nWe found that DropOut does not help improve the results of \nour proposed technique. Instead, even with a small percentage \nof dropping (say 5%) the results deteriorate. But DropConnect \nregularization improves our results slightly. The best results \nare obtained for 10% dropping; it improves the results by \nabout 1.2 % on average. Therefore in this work, we do not use \nDropOut; we only use DropConnect with 10% dropping.  \nNote that all the parametric values and the user defined \ndropping percentages used in this work have been tuned on a \nthird dataset, namely the Salinas dataset. We tuned the \nparameters using a greedy L-curve method [34]. Here the \nparameter λ is first tuned (using L-curve) by fixing μ to Zero. \nOnce λ is tuned, it is fixed at that value and then μ is tuned by \nthe L-curve method. These values have not been tuned on the \nIndian Pines and the Pavia University datasets.  \nThe inputs to our proposed RSDDL are spatio-spectral \nfeatures. These have been derived in the way proposed by [1, \n4], i.e. a window of size 4x4 is captured and all the bands \nwithin the window are taken. The dimensionality is reduced to \n200 by principal component analysis. This is used as input to \nour proposed method.  \nEvaluation is carried out by the standard measures of \nAverage Accuracy (AA), Overall Accuracy (OA) and Kappa \n(K) coefficient. The definitions for these terms are given \nbelow.  \n• Average Accuracy – This measure is the average value of \nthe classification accuracies of all classes. \n• Overall Accuracy – This index shows the number of \nhyperspectral pixels that are classified correctly, divided \nby the number of test samples. \n• Kappa Coefficient – This metric is a statistical measurement \nof agreement between the final classification map and the \nground-truth map. It is the percentage agreement \ncorrected by the level of agreement that could be expected \ndue to chance alone. It is generally thought to be a more \nrobust measure than a simple percent agreement \ncalculation, since it takes into account the agreement \noccurring by chance \nThe numerical results (in terms of the aforesaid indices) for \nboth the Indian Pines and Pavia University datasets are shown \nin Table III.  \n \nTABLE III \nCOMPARISON WITH STATE-OF-THE-ART DEEP LEARNING TECHNIQUES \nDataset \nMetric \nGBN [2] \nDAE [5] \nRNN [6] \nNSS-Net [7] \nDFNN [12] \nRDDL [16] \nProposed-0 \nProposed-1 \nPavia \nOA \n86.77 \n88.36 \n88.87 \n89.55 \n88.50 \n88.52 \n90.91 \n89.30 \nAA \n85.31 \n86.73 \n86.43 \n89.03 \n86.37 \n87.38 \n90.22 \n88.62 \nKappa \n0.79 \n0.80 \n0.80 \n0.79 \n0.79 \n0.80 \n0.86 \n0.84 \nIndian \nPines \nOA \n85.42 \n89.02 \n88.59 \n88.82 \n86.34 \n88.76 \n90.76 \n89.14 \nAA \n86.31 \n85.86 \n85.36 \n86.48 \n85.09 \n85.93 \n88.49 \n87.68 \nKappa \n0.74 \n0.75 \n0.73 \n0.75 \n0.74 \n0.75 \n0.78 \n0.77 \n  \n*Proposed-0: l0-norm between training and test features; Proposed-1: l1-norm between training and test samples \n \n \nFig. 3. Pavia University. Left to Right – Groundtruth, GBN, DAE, RNN, NSS-Net, DFNN, RDDL and Proposed-0. \n \n \nFig. 4. Indian Pines. Left to Right – Groundtruth, GBN, DAE, RNN, NSS-Net, DFNN, RDDL and Proposed-0. \n \n \nIn terms of numbers, we see that both versions of our \nproposed method clearly outperform others. However, the l0-\nnorm outperforms the l1-norm; even though the later is \nsupposed to be theoretically sounder. This may be owing to \ntwo reasons. First, the Hamming distances (for l0-norm) are \nnever tied in practice. Second, the l1-norm does not enforce \nexact sparsity and hence obfuscate the results.  \nThe DAE, NSS-Net, RNN and RDDL perform almost the \nsame and is worse than ours. The GBN based formulation is \nslightly worse than the aforesaid techniques. The CNN \nformulation performs at par with other existing methods. We \ntried our best to implement the CNN based technique, \nhowever with the best of our efforts we were not able to \nachieve the accuracy reported in there for the Indian Pines \ndataset. This may be owing to the randomness in seeding the \nCNN or in selecting the training samples.  \nFor visual evaluation we have shown the results in Fig.s 3 \nand 4 for the Pavia and the Indian Pines dataset respectively. \nSince, the Proposed-0 approach is the best of our two \napproaches, we show results only from it. Visual evaluation \ncorroborates the numerical results.  \nIn order to test the statistical significance of our results, we \nhave used the McNemar’s test (also carried out in [12]). The \nresults are shown in Table IV. It can be clearly seen that our \nProposed-0 method is indeed significantly better than the \nothers. \n \nTABLE IV \nSTATISTICAL SIGNIFICANCE FROM STANDARDIZED MCNEMAR’S TEST \nProposed Vs. \nPavia University \nZ / significant \nIndian Pines \nZ / significant \nGBN \n24.33 / yes \n29.12 / yes \nDAE \n20.19 / yes \n17.60 / yes \nRNN \n20.67 / yes \n21.09 / yes \nNSS-Net \n15.70 / yes \n17.61 / yes \nDFNN \n26.71 / yes \n32.64 / yes \nRDDL \n19.56 / yes \n18.97 / yes \n \nAll the experiments were conducted on an Intel Xeon E3-\n1246 CPU at 3.5 GHz with 32-GB RAM using a MATLAB \nplatform. The training and testing times for all the aforesaid \ntechniques are given in Table V.  \n \nTABLE V \nTRAINING TIME IN SECONDS \nTechnique \nPavia \n \nIndian Pines \n \nTraining \nTesting \nTraining \nTesting \nGBN \n14443 \n41 \n4591 \n10 \nDAE \n8291 \n43 \n2307 \n11 \nRNN \n4095 \n90 \n1002 \n21 \nNSS-Net \n1906 \n102 \n1523 \n28 \nDFNN \n12409 \n97 \n8002 \n23 \nRDDL \n70 \n68 \n45 \n16 \nProposed-0 \n102 \n95 \n87 \n22 \nProposed-1 \n102 \n105 \n87 \n25 \n \nNote that for training, there is no difference between our \ntwo proposed approaches, since it only pertains to testing. In \nterms of training, one can see, RDDL is the fastest; this is \nexpected because it is a greedy learning approach. Our method \ntakes slightly more time but is several orders of magnitude \nfaster than other techniques. \nFor testing, we find that GBN and DAE are the fastest; this \nis because they only require a few matrix vector products \nduring testing. RSDDL is slower compared to these, because \none needs to solve inverse problems (albeit in a closed form); \nowing to the non-linearity the inverses cannot be pre-\ncomputed. Our proposed method are relatively slow owing to \nthe necessity of solving involved optimization problems \nduring testing. We find that the l0-norm based solution is faster \ncompared to the l1-norm (the reason has been explained \nbefore). The CNN based solutions are slower than our best \nperforming overall technique (l0-norm), but the RNN based \nsolution is slightly faster than ours.   \nB. Comparison with shallow techniques \nSince deep learning techniques are data hungry and do not \nperform well in limited training data scenarios, we compared \nwith traditional (shallow) machine learning techniques as well. \nLabel consistent KSVD (LC-KSVD) [35] was proposed as a \ngeneric classification algorithm based on the popular \ndictionary learning framework; later it was adopted for \nhyperspectral image classification [36]. The results were LC-\nKSVD [36] were better than vanilla implementations of other \nmachine learning algorithms, so use it as a benchmark.  \nAnother technique that is compared against is the rotation \nbased support vector machine RO-SVM [37]; this was \nspecifically designed to function in the limited training data \nscenario.  \nThe third method compared against is based on the extreme \nlearning machine (ELM) formulation [38]. This is a state-of-\nthe-art technique known for its very short training time.  \nThe experimental protocol remains the same. For each of \nthe aforesaid methods, we have taken the best configuration \nspecified in the papers. However, we have not used fusion \napproaches used in some of the studies as a post processing \nstep to boost the accuracy.  \n \nTABLE VI \nCOMPARISON WITH STATE-OF-THE-ART SHALLOW TECHNIQUES \nDataset \nMetric \nRO-SVM \nLC-KSVD \nELM \nProposed \nPavia \nOA \n75.99 \n74.02 \n89.10 \n90.91 \nAA \n85.77 \n73.42 \n89.08 \n90.22 \nKappa \n0.73 \n0.70 \n0.77 \n0.86 \nIndian \nPines \nOA \n80.14 \n75.02 \n88.01 \n90.76 \nAA \n88.84 \n72.32 \n86.23 \n88.49 \nKappa \n0.81 \n0.70 \n0.75 \n0.78 \n \nNote that one cannot compare the results obtained here and \nthose reported in the previous papers for two reasons – 1. The \nexperimental protocols are different; and, 2. There is no post-\nprocessing step used in our experiments.  \nThe results show that of our method (with l0-norm) still \nyields the best results. Results from RO-SVM and LC-KSVD \nare much worse than ours and all the deep learning techniques \ncompared against. Results from ELM are comparable with \nother deep learning techniques, but is still worse than ours. As \nbefore, we perform McNemar’s test to show the statistical \nsignificance (improvement) of our method.  \n \nTABLE VII \nSTATISTICAL SIGNIFICANCE FROM STANDARDIZED MCNEMAR’S TEST \nProposed Vs. \nPavia University \nZ / significant \nIndian Pines \nZ / significant \nRO-SVM \n35.58 / yes \n29.36 / yes \nLC-KSVD \n32.77 / yes \n32.08 / yes \nELM \n18.13 / yes \n17.42 / yes \n \nIn the following table we show the training and testing \ntimes. ELM is the fastest; this was supposed to be given its \ndesign (closed form solution). SVM is the slowest in terms of \ntraining; its testing times are comparable with other dictionary \nlearning based methods. It is interesting to note that even \nthough LC-KSVD is a shallow method, it is slower than ours; \nthis because of the requirement of solving the dictionary \nlearning by the inefficient KSVD algorithm.  \n  \nTABLE VIII \nTRAINING AND TESTING TIMES IN SECONDS \nTechnique \nPavia \nIndian Pines \n \nTraining \nTesting \nTraining \nTesting \nRO-SVM \n1002 \n100 \n255 \n58 \nLC-KSVD \n119 \n97 \n107 \n27 \nELM \n31 \n40 \n8 \n12 \nProposed \n102 \n95 \n87 \n22 \n \nC. Effect of Stochastic Regularization \nIn this sub-section we have elaborated on the different \ndropping rates. The kappa coefficients are shown in Table IX. \nThe columns of the tables should be read independently, i.e. \nthe DropOut and the DropConnect have been used separately \nand not combined with each other. Since the Proposed-o \nmethod is better than the other approach, we show results from \nthe former only.  \n \nTABLE IX \nVARIATION OF KAPPA WITH DROPPING RATES \n%age \nof \ndropping \nPavia \nIndian Pines \nDropOut \nDropConnect \nDropOut \nDropConnect \n0 \n0.82 \n0.82 \n0.73 \n0.73 \n2.5 \n0.81 \n0.83 \n0.72 \n0.75 \n5 \n0.78 \n0.84 \n0.70 \n0.77 \n10 \n0.74 \n0.86 \n0.66 \n0.78 \n15 \n0.68 \n0.82 \n0.62 \n0.73 \n \nThe results show that with DropOut, the results deteriorate \nvery fast. This is the reason, we have not used it in our main \nexperiments. With DropConnect, the results improve till about \n10%, but with further dropping, the result deteriorates.  \nWe believe that DropOut degrades performance, both our \ndeterministic sparsity promoting regularization (l0-norm) and \nDropOut (random zero-ing out of features) negates each other. \nThe l0-norm enforces sparsity and selects very few \ncoefficients; further dropping after having a sparse output \ntends to lose information and hence deteriorates.  \nDropOut prevents coadaptation of nodes. To a certain extent \nit helps barring overfitting; but when too many are dropped, \nthe network lacks the capacity to model the problem. Hence \nthe results deteriorate (after 10%).  \nV. CONCLUSION \nIn this work we propose a new technique for hyperspectral \nimage classification based on the deep dictionary learning \nframework. We show that the proposed technique can yield \ngood results even with extremely few training samples. \nComparison has been carried out with state-of-the-art deep \nlearning based methods published in the last year. Our method \noutperforms them all by a statistically significant margin. We \nhave also carried out comparison with several traditional \n(shallow) machine learning approaches particularly tailored \nfor the said problem published in the last few years; in terms \nof accuracy we excel over these as well.  \nIn terms of technique there are several contributions of this \nwork. First, ours is the only work that can solve all the layers \nof deep dictionary learning problem in a joint fashion; instead \nof solving one layer at a time in a greedy fashion (as is done in \nprior studies), we learn all the layers jointly. Second, we \npropose a new discriminative penalty. Third, we introduce \nstochastic regularization techniques in lines with DropConnect \nand DropOut. \nAlthough used in this work for the purpose of hyperspectral \nimage classification; the method proposed here is fundamental \nand can be applied to a variety of problems.    \nACKNOWLEDGEMENT \nThis work is supported by the Infosys Center for Artificial \nIntelligence @ IIIT Delhi and by the Indo-French CEFIPRA \ngrant DST-CNRS-2016-02. \nREFERENCES \n[1] Y. Chen, X. Zhao and X. Jia, \"Spectral–Spatial Classification of \nHyperspectral Data Based on Deep Belief Network,\" in IEEE Journal of \nSelected Topics in Applied Earth Observations and Remote Sensing, \nvol. 8, no. 6, pp. 2381-2392, June 2015. \n[2] X. Zhou, S. Li, F. Tang, K. Qin, S. Hu and S. Liu, \"Deep Learning With \nGrouped Features for Spatial Spectral Classification of Hyperspectral \nImages,\" in IEEE Geoscience and Remote Sensing Letters, vol. 14, no. \n1, pp. 97-101, Jan. 2017. \n[3] P. Zhong, Z. Gong, S. Li and C. B. Schönlieb, \"Learning to Diversify \nDeep Belief Networks for Hyperspectral Image Classification,\" in IEEE \nTransactions on Geoscience and Remote Sensing, vol. 55, no. 6, pp. \n3516-3530, June 2017. \n[4] Y. Chen, Z. Lin, X. Zhao, G. Wang and Y. Gu, \"Deep Learning-Based \nClassification of Hyperspectral Data,\" in IEEE Journal of Selected \nTopics in Applied Earth Observations and Remote Sensing, vol. 7, no. 6, \npp. 2094-2107, June 2014. \n[5] X. Ma, H. Wang and J. Geng, \"Spectral–Spatial Classification of \nHyperspectral Image Based on Deep Auto-Encoder,\" in IEEE Journal of \nSelected Topics in Applied Earth Observations and Remote Sensing, \nvol. 9, no. 9, pp. 4073-4085, Sept. 2016. \n[6] L. Mou, P. Ghamisi and X. X. Zhu, \"Deep Recurrent Neural Networks \nfor Hyperspectral Image Classification,\" in IEEE Transactions on \nGeoscience and Remote Sensing, vol. 55, no. 7, pp. 3639-3655, July \n2017. \n[7] F. Zhou, R. Hang, Q. Liu and X. Yuan, “Hyperspectral image \nclassification \nusing \nspectral-spatial \nLSTMs,” \nNeurocomputing \n(accepted). \n[8] B. Pan, Z. Shi, N. Zhang and S. Xie, \"Hyperspectral Image \nClassification Based on Nonlinear Spectral–Spatial Network,\" in IEEE \nGeoscience and Remote Sensing Letters, vol. 13, no. 12, pp. 1782-1786, \nDec. 2016. \n[9] B. Pan, Z. Shi and X. Xu, \"R-VCANet: A New Deep-Learning-Based \nHyperspectral Image Classification Method,\" in IEEE Journal of \nSelected Topics in Applied Earth Observations and Remote Sensing, \nvol. 10, no. 5, pp. 1975-1986, May 2017 \n[10] A. Romero, C. Gatta and G. Camps-Valls, \"Unsupervised Deep Feature \nExtraction for Remote Sensing Image Classification,\" in IEEE \nTransactions on Geoscience and Remote Sensing, vol. 54, no. 3, pp. \n1349-1362, March 2016. \n[11] G. Cheng, Z. Li, J. Han, X. Yao and L. Guo, \"Exploring Hierarchical \nConvolutional Features for Hyperspectral Image Classification,\" in IEEE \nTransactions on Geoscience and Remote Sensing. \n[12] Y. Chen, H. Jiang, C. Li, X. Jia and P. Ghamisi, \"Deep Feature \nExtraction and Classification of Hyperspectral Images Based on \nConvolutional Neural Networks,\" in IEEE Transactions on Geoscience \nand Remote Sensing, vol. 54, no. 10, pp. 6232-6251, Oct. 2016. \n[13] W. Li, G. Wu, F. Zhang and Q. Du, \"Hyperspectral Image Classification \nUsing Deep Pixel-Pair Features,\" in IEEE Transactions on Geoscience \nand Remote Sensing, vol. 55, no. 2, pp. 844-853, Feb. 2017. \n[14] J. Yang, Y. Q. Zhao and J. C. W. Chan, \"Learning and Transferring \nDeep Joint Spectral–Spatial Features for Hyperspectral Classification,\" \nin IEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 8, \npp. 4729-4742, Aug. 2017. \n[15] P. Liu, H. Zhang and K. B. Eom, “Active Deep Learning for \nClassification of Hyperspectral Images”, in IEEE Journal of Selected \nTopics in Applied Earth Observations and Remote Sensing, vol. 10, no. \n2, pp. 712 – 724, Feb. 2017. \n[16] V. Singhal; H. K. Aggarwal; S. Tariyal; A. Majumdar, \"Discriminative \nRobust \nDeep \nDictionary \nLearning \nfor \nHyperspectral \nImage \nClassification,\" in IEEE Transactions on Geoscience and Remote \nSensing , vol.PP, no.99, pp.1-10. \n[17] I. Manjani, S. Tariyal, M. Vatsa, R. Singh and A. Majumdar, \"Detecting \nSilicone Mask-Based Presentation Attack via Deep Dictionary \nLearning,\" in IEEE Transactions on Information Forensics and Security, \nvol. 12, no. 7, pp. 1713-1723, July 2017. \n[18] S. Singh; A. Majumdar, \"Deep Sparse Coding for Non-Intrusive Load \nMonitoring,\" in IEEE Transactions on Smart Grid , vol.PP, no.99, pp.1-\n1. \n[19] S. Tariyal, A. Majumdar, R. Singh and M. Vatsa, \"Deep Dictionary \nLearning,\" in IEEE Access, vol. 4, no. , pp. 10096-10109, 2016. \n[20] A. Sankaran, G. Sharma, R. Singh, M. Vatsa and A. Majumdar, “Class \nSparsity Signature based Restricted Boltzmann Machines”, Pattern \nRecognition, Vol. 61, pp. 674-685, 2017. \n[21] T. H. Chan, K. Jia, S. Gao, J. Lu, Z. Zeng and Y. Ma, \"PCANet: A \nSimple Deep Learning Baseline for Image Classification?,\" in IEEE \nTransactions on Image Processing, vol. 24, no. 12, pp. 5017-5032, Dec. \n2015. \n[22] G. Trigeorgis, K. Bousmalis, S. Zafeiriou and B. W. Schuller, \"A Deep \nMatrix Factorization Method for Learning Attribute Representations,\" in \nIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. \n39, no. 3, pp. 417-429, March 1 2017. \n[23] Z. Li and J. Tang, \"Weakly Supervised Deep Matrix Factorization for \nSocial Image Understanding,\" in IEEE Transactions on Image \nProcessing, vol. 26, no. 1, pp. 276-288, Jan. 2017. \n[24] F. Zhuang, D. Luo, X. Jin, H. Xiong, P. Luo and Q. He, \"Representation \nLearning via Semi-Supervised Autoencoder for Multi-task Learning,\" \n2015 IEEE International Conference on Data Mining, Atlantic City, NJ, \n2015, pp. 1141-1146. \n[25] S. Gao, Y. Zhang, K. Jia, J. Lu and Y. Zhang, \"Single Sample Face \nRecognition via Learning Deep Supervised Autoencoders,\" in IEEE \nTransactions on Information Forensics and Security, vol. 10, no. 10, pp. \n2108-2118, Oct. 2015. \n[26] S. Zhou, Q. Chen and X. Wang, \"Discriminative Deep Belief Networks \nfor image classification,\" 2010 IEEE International Conference on Image \nProcessing, Hong Kong, 2010, pp. 1561-1564. \n[27] A. r. Mohamed, T. N. Sainath, G. Dahl, B. Ramabhadran, G. E. Hinton \nand M. A. Picheny, \"Deep Belief Networks using discriminative features \nfor phone recognition,\" 2011 IEEE International Conference on \nAcoustics, Speech and Signal Processing (ICASSP), Prague, 2011, pp. \n5060-5063. \n[28] A. Majumdar, R. Singh and M. Vatsa, \"Face Verification via Class \nSparsity Based Supervised Encoding,\" in IEEE Transactions on Pattern \nAnalysis and Machine Intelligence, vol. 39, no. 6, pp. 1273-1280, June 1 \n2017. \n[29] C. Wu and X. C. Tai, “Augmented Lagrangian method, dual methods, \nand split Bregman iteration for ROF, vectorial TV, and high order \nmodels,” SIAM Journal on Imaging Sciences, vol. 3, no. 3, pp. 300-339, \n2010. \n[30] T. Goldstein and S. Osher, “The split Bregman method for L1-\nregularized problems”, SIAM journal on imaging sciences, vol. 2, no. 2, \npp. 323-343, 2009. \n[31] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and R. \nSalakhutdinov, “Dropout: A simple way to prevent neural networks \nfrom overfitting,” Journal of Machine Learning Research, vol. 15, no. 1, \npp. 1929-1958, 2014. \n[32] L. Wan, M. Zeiler, S. Zhang, Y. LeCun and R. Fergus, “Regularization \nof neural networks using dropconnect”, ACM International Conference \non Machine Learning, 2013. \n[33] W. Song, S. Li, L. Fang and T. Lu, \"Hyperspectral Image Classification \nWith Deep Feature Fusion Network,\" in IEEE Transactions on \nGeoscience and Remote Sensing. \n[34] P. C. Hansen and D. P. Oleary, “The use of the L-curve in the \nregularization of discrete ill-posed problems”, SIAM Journal on \nScientific Computing, Vol. 14 (6), pp. 1487-1503, 1993. \n[35] Z. Jiang Z. Lin and L. S. Davis \"Label consistent K-SVD: Learning a \ndiscriminative dictionary for recognition\" IEEE Transactions on Pattern \nAnalysis and Machine Intelligence, Vol. 35 (11) pp. 2651 - 2664, 2013. \n[36] X. Zhang, Y. Liang, Y. Zheng, J. An and L. C. Jiao, \"Hierarchical \nDiscriminative \nFeature \nLearning \nfor \nHyperspectral \nImage \nClassification,\" IEEE Geoscience and Remote Sensing Letters, Vol. 13 \n(4), pp. 594-598, 2016. \n[37] J. Xia, J. Chanussot, P. Du and X. He, \"Rotation-Based Support Vector \nMachine Ensemble in Classification of Hyperspectral Data With Limited \nTraining Samples,\" IEEE Transactions on Geoscience and Remote \nSensing, vol. 54, no. 3, pp. 1519-1531, 2016. \n[38] M. Jiang, F. Cao and Y. Lu, \"Extreme Learning Machine With \nEnhanced Composite Feature for Spectral-Spatial Hyperspectral Image \nClassification,\" IEEE Access, vol. 6, pp. 22645-22654, 2018. \n \n",
  "categories": [
    "eess.IV",
    "cs.LG"
  ],
  "published": "2019-12-11",
  "updated": "2019-12-11"
}