{
  "id": "http://arxiv.org/abs/1912.11872v1",
  "title": "Vision and Language: from Visual Perception to Content Creation",
  "authors": [
    "Tao Mei",
    "Wei Zhang",
    "Ting Yao"
  ],
  "abstract": "Vision and language are two fundamental capabilities of human intelligence.\nHumans routinely perform tasks through the interactions between vision and\nlanguage, supporting the uniquely human capacity to talk about what they see or\nhallucinate a picture on a natural-language description. The valid question of\nhow language interacts with vision motivates us researchers to expand the\nhorizons of computer vision area. In particular, \"vision to language\" is\nprobably one of the most popular topics in the past five years, with a\nsignificant growth in both volume of publications and extensive applications,\ne.g., captioning, visual question answering, visual dialog, language\nnavigation, etc. Such tasks boost visual perception with more comprehensive\nunderstanding and diverse linguistic representations. Going beyond the\nprogresses made in \"vision to language,\" language can also contribute to vision\nunderstanding and offer new possibilities of visual content creation, i.e.,\n\"language to vision.\" The process performs as a prism through which to create\nvisual content conditioning on the language inputs. This paper reviews the\nrecent advances along these two dimensions: \"vision to language\" and \"language\nto vision.\" More concretely, the former mainly focuses on the development of\nimage/video captioning, as well as typical encoder-decoder structures and\nbenchmarks, while the latter summarizes the technologies of visual content\ncreation. The real-world deployment or services of vision and language are\nelaborated as well.",
  "text": "SIP (2015), page 1 of 7 © The Authors, 2015.\nThe online version of this article is published within an Open Access environment subject to the conditions of the Creative Commons Attribution-NonCommercial-ShareAlike\nlicense <http://creativecommons.org/licenses/by-nc-sa/3.0/>. The written permission of Cambridge University Press must be obtained for commercial re-use.\ndoi:0000000000\nVision and Language: from Visual Perception\nto Content Creation\nTAO MEI, WEI ZHANG, TING YAO\nVision and language are two fundamental capabilities of human intelligence. Humans routinely perform tasks through\nthe interactions between vision and language, supporting the uniquely human capacity to talk about what they see or\nhallucinate a picture on a natural-language description. The valid question of how language interacts with vision motivates\nus researchers to expand the horizons of computer vision area. In particular, “vision to language\" is probably one of\nthe most popular topics in the past ﬁve years, with a signiﬁcant growth in both volume of publications and extensive\napplications, e.g., captioning, visual question answering, visual dialog, language navigation, etc. Such tasks boost visual\nperception with more comprehensive understanding and diverse linguistic representations. Going beyond the progresses\nmade in “vision to language,\" language can also contribute to vision understanding and offer new possibilities of visual\ncontent creation, i.e., “language to vision.\" The process performs as a prism through which to create visual content\nconditioning on the language inputs. This paper reviews the recent advances along these two dimensions: “vision to\nlanguage\" and “language to vision.\" More concretely, the former mainly focuses on the development of image/video\ncaptioning, as well as typical encoder-decoder structures and benchmarks, while the latter summarizes the technologies\nof visual content creation. The real-world deployment or services of vision and language are elaborated as well.\nKeywords: Deep Learning, Computer Vision, Artiﬁcial Intelligence\nI. INTRODUCTION\nComputer Vision (CV) and Natural Language Processing\n(NLP) are two most fundamental disciplines under a broad\narea of Artiﬁcial Intelligence (AI). CV is regarded as a ﬁeld\nof research that explores the techniques to teach computers\nto see and understand the digital content such as images\nand videos. NLP is a branch of linguistics that enables\ncomputers to process, interpret and even generate human\nlanguage. With the rise and development of deep learning\nover the past decade, there has been a steady momentum of\ninnovation and breakthroughs that convincingly push the\nlimits and improve the state-of-the-art of both vision and\nlanguage modelling. An interesting observation is that the\nresearch in the two area starts to interact and many previ-\nous experiences have shown that by doing so can naturally\nbuild up the circle of human intelligence.\nIn general, the interactions between vision and lan-\nguage have proceeded along two dimensions: vision to\nlanguage and language to vision. The former predomi-\nnantly recognizes or describes the visual content with a\nset of individual words or a natural sentence in the form\nof tags [59], answers [2], captions [61–63] and comments\n[20]. For example, a tag usually denotes a speciﬁc object,\naction or event in visual content. An answer is a response\nto a question about the details depicted in an image or a\n1JD AI Research, Building A, North-Star Century Center, 8 Beichen West Road,\nBeijing, China\nCorresponding author: Tao Mei\nEmail: tmei@jd.com\nvideo. A caption goes beyond tags or answers by produc-\ning a natural-language utterance (usually a sentence) and a\ncomment is also a sentence which expresses an emotional\nstate on visual content. The latter of language to vision\nbasically generates visual content according to natural lan-\nguage inputs. One typical application is to create an image\nor a video from text. For instance, given a textual descrip-\ntion of “this small bird has short beak and dark stripe down\nthe top, the wings are a mix of brown, white and black,\" the\ngoal of text-to-image synthesis is to generate a bird image\nwhich meets all the details.\nThis paper reviews the recent state-of-the-art advances\nof AI technologies which boost both vision to lan-\nguage, particularly image/video captioning, and language\nto vision. The real-world deployments in the two ﬁelds are\nalso presented as the good examples of how AI transforms\nthe customer experiences and enhances user engagement\nin industrial applications. The remaining sections are orga-\nnized as follows. Section II describes the development of\nvision to language by outlining a brief road map of key\ntechnologies on image/video captioning, distilling a typical\nencoder-decoder structure, and summarizing the evalua-\ntions on a popular benchmark. The practical applications\nof vision to language are further presented. Section III\ndetails the technical advancements on language to vision\nin terms of different conditions and strategies for genera-\ntion, followed by a summary of progresses on language to\nimage, language to video, and AI-empowered applications.\nFinally, we conclude the paper in Section IV.\n1\narXiv:1912.11872v1  [cs.CV]  26 Dec 2019\n2\nTAO MEI, WEI ZHANG, TING YAO\n2019 \nBefore\n2015\n2016\n2017\n2018\nTemplate-based\n• [Yang, EMNLP11]\n• [Kulkarni, TPAMI13]\nDataset\n• [Flickr8k, JAIR13]\n• [Flickr30k, ACL14]\n• [MSCOCO, ECCV14]\nCNN + RNN\n• [Vinyals, CVPR15]\n• [Karpathy, CVPR15]\n• [Donahue, CVPR15]\nSpatial Attention\n• [Xu, ICML15]\nAttributes\n• [Wu, CVPR16]\nSemantic Attention\n• [You, CVPR16]\nNovel Object\n•\n[Hendricks, CVPR16]\nDense Captioning\n•\n[Johnson, CVPR16]\nRegion\n• [Fu, TPAMI17]\nAttributes\n• [Yao, ICCV17]\nAdaptive Attention\n• [Lu, CVPR17]\nPolicy Gradient\n• [Rennie, CVPR17]\n• [Liu, ICCV17]\nNovel Object \n• [Yao, CVPR17]\n• [Peter, EMNLP17]\nRelation\n• [Yao, ECCV18]\nRegion\n• [Peter, CVPR18]\nAttention Fusion\n• [Jiang, ECCV18]\nConvolution\n• [Aneja, CVPR18]\nNovel Object\n• [Lu, CVPR18]\nImage Domain\nHierarchy Parsing\n• [Yao, ICCV19]\nLanguage Navigation\n• [Wang, CVPR19]\nScene Graphs\n• [Yang, CVPR19]\nNovel Object\n• [Li, CVPR19]\nDataset\n• [MSVD, ACL11]\n• [TACoS, GCPR14]\nTemporal Attention\n• [Yao, ICCV15]\nSeq2Seq\n• [Venugopalan, ICCV15]\nDataset\n• [M-VAD, arxiv15]\n• [MPII-MD, CVPR15]\nCNN + Embedding\n• [Pan, CVPR16]\nST Attention\n• [Yu, CVPR16]\nDataset\n• [MSR-VTT, CVPR16]\n• [TGIF, CVPR16]\nAttributes\n• [Pan, CVPR17]\nDenseCaptioning \nEvents\n• [Krishna, ICCV17]\nDataset\n• [Visual Genome, IJCV17]\n• [ActivityNet Captions, ICCV17]\nDense Captioning\n• [Li, CVPR18]\nVideo Domain\nFourier Transform\n• [Aafaq, CVPR19]\nStreamlined\n• [Mun, CVPR19]\nConv Encoder-Decoder\n• [Chen, AAAI19]\nFig. 1.: A road map for the techniques and datasets in vision (image/video) to language in 10 years.\nII. VISION TO LANGUAGE\nThis section summarizes the development of vision to\nlanguage (particularly image/video captioning) in several\naspects, ranging from the road map of key techniques and\nbenchmarks, typical encoder-decoder architectures, to the\nevaluation results of representative methods.\nA) Road Map of Vision to Language\nIn the past 10 years, we have witnessed researchers strived\nto push the limits of vision to language systems (e.g.,\nimage/video captioning). Fig. 1 depicts the road map for\nthe techniques behind vision (image/video) to language\nand the corresponding benchmarks. Speciﬁcally, the year\nof 2015 is actually a watershed in captioning. Before that,\nthe main stream of captioning is template-based method\n[18, 56] in image domain. The basic idea is to detect\nthe objects or actions in an image and integrate these\nwords into pre-deﬁned sentence templates as subjective,\nverb and objective. At that time, most of the image cap-\ntioning datasets are ready to use, such as Flickr30K and\nMSCOCO. At the year 2015, deep learning-based image\ncaptioning models are ﬁrst presented. The common design\n[49] is to employ a Convolutional Neural Network (CNN)\nas an image encoder to produce image representations and\nexploit a decoder of Long Short-Term Memory (LSTM) to\ngenerate the sentence. The attention mechanism [54] is also\nproposed at that year which locates the most relevant spa-\ntial regions when predicting each word. After that, the area\nof image captioning is growing very fast. Researchers came\nup with a series of innovations, such as augmenting image\nfeatures with semantic attributes [61] or visual relations\n[62], predicting novel objects through leveraging unpaired\ntraining data [22, 60], and even going a step further to per-\nform language navigation [51]. Another extension direction\nof captioning in image domain is to produce multiple sen-\ntences or phrases for an image, aiming to recapitulate more\ndetails within image. In between, dense image caption-\ning [13] and image paragraph generation [50] are typical\nones, which generate a set of descriptions or paragraph that\ndescribes image in a ﬁner fashion.\nThe start point of captioning in video domain is also\nin the year of 2015. Then, researchers start to remould\nthe CNN plus RNN captioning framework towards the\nscenario of captioning in video domain. A series of tech-\nniques (e.g., temporal attention, embedding, or attributes)\nare explored to further improve video captioning. Con-\ncretely, [58] is one of early attempts that incorporate\ntemporal attention mechanism into captioning framework\nby learning to attend to the most relevant frames at each\ndecoding time step. [33] integrates LSTM with seman-\ntic embedding to preserve the semantic relevance between\nvideo content and the entire sentence. [36] further aug-\nments captioning model to emphasize the detected visual\nattributes in the generated sentence. It is also worthy men-\ntioned that in 2016, MSR-VTT video captioning dataset\n[53] is released which has been widely used and already\ndownloaded by more than 100 groups worldwide. Most\nrecently, [1] applies short Fourier transform across all the\nframe-level features along the temporal dimension to fuse\nall frame-level features into video-level representation and\nfurther enhances video captioning. Another recent attempt\nfor video captioning is to speed up the training proce-\ndure by fully employing convolutions in both encoder\nand decoder networks [6]. Nevertheless, considering that\nvideos in real life are usually long and contain multiple\nevents, the conventional video captioning methods gener-\nating only one caption for a video in general will fail to\nVISION AND LANGUAGE\n3\n…\n…\n#start\na\ndog\nfrisbee\n…\n…\nLearning visual representation by CNN\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\nLSTM\n…\n…\na\nsoftmax\ndog\nsoftmax\nleaps\nsoftmax\n#end\nsoftmax\n[1, 0, 0… 0]\n[0, 1, 0, …, 0]\n[0, 0, 1, …, 0]\n[0, 0, 0, …, 1]\nCNN Rep.\nAttributes\nAttention\nRegion\n[Fu, TPAMI17; Peter, \nCVPR18]\n[Xu, ICML15; You, CVPR16; \nLu, CVPR17 ]\n[Wu, CVPR16; Yao, ICCV17]\n[Vinyals, CVPR15; Karpathy, \nCVPR15; Donohue, CVPR15\nMao, ICLR15]\nObjects\n[Yao, CVPR17]\nRelation\nHierarchy\n[Yao, ECCV18]\n[Yao, ICCV19]\nPolicy Gradient \nOptimization\n[Rennie, CVPR17; Liu, ICCV17]\n...\nEmbed\n#start\na\nEmbed\ndog\nEmbed\nfrisbee\nEmbed\nMulti-Head\nAttention\nAdd & Norm\nMulti-Head\nAttention\nMulti-Head\nAttention\nMulti-Head\nAttention\nAdd & Norm\nAdd & Norm\nAdd & Norm\nMulti-Head\nAttention\nAdd & Norm\nMulti-Head\nAttention\nMulti-Head\nAttention\nMulti-Head\nAttention\nAdd & Norm\nAdd & Norm\nAdd & Norm\nFeed \nForward\nAdd & Norm\nFeed \nForward\nFeed \nForward\nFeed \nForward\nAdd & Norm\nAdd & Norm\nAdd & Norm\nLinear\nSoftmax\nLinear\nLinear\nLinear\nSoftmax\nSoftmax\nSoftmax\n...\nNx\nLinear\nMulti-Head\nAttention\nAdd & Norm\nFeed \nForward\nAdd & Norm\nNx\n...\nPositional \nEncoding\nRegion\n(a) CNN encoder plus LSTM decoder\n(b) Transformer-based encoder-decoder\na\ndog\nleaps\n#end\nFig. 2.: The typical architectures of (a) CNN encoder plus LSTM decoder and (b) Transformer-based encoder-decoder for\nimage captioning.\nTable 1. The reported performance (%) of image captioning on COCO testing server with 5 reference captions (c5) and 40 reference captions (c40).\nModel\nGroup\nB@4\nMETEOR\nROUGE-L\nCIDEr-D\nc5\nc40\nc5\nc40\nc5\nc40\nc5\nc40\nHIP [63]\nJD AI, ICCV’19\n39.3\n71\n28.8\n38.1\n59\n74.1\n127.9\n130.2\nGCN-LSTM [62]\nJD AI, ECCV’18\n38.7\n69.7\n28.5\n37.6\n58.5\n73.4\n125.3\n126.5\nRFNet [12]\nTencent, ECCV’18\n38\n69.2\n28.2\n37.2\n58.2\n73.1\n122.9\n125.1\nUp-Down [2]\nMSR, CVPR’18\n36.9\n68.5\n27.6\n36.7\n57.1\n72.4\n117.9\n120.5\nLSTM-A [61]\nMSRA, ICCV’17\n35.6\n65.2\n27\n35.4\n56.4\n70.5\n116\n118\nWatson Multimodal [40]\nIBM, CVPR’17\n35.2\n64.5\n27.0\n35.5\n56.3\n70.7\n114.7\n116.7\nG-RMI [24]\nGoogle, ICCV’17\n33.1\n62.4\n25.5\n33.9\n55.1\n69.4\n104.2\n107.1\nMetaMind/VT-GT [25]\nSalesforce, CVPR’17\n33.6\n63.7\n26.4\n35.9\n55\n70.5\n104.2\n105.9\nreviewnet [57]\nCMU, NIPS’16\n31.3\n59.7\n25.6\n34.7\n53.3\n68.6\n96.5\n96.9\nATT [64]\nRochester, CVPR’16\n31.6\n59.9\n25\n33.5\n53.5\n68.2\n94.3\n95.8\nGoogle [49]\nGoogle, CVPR’15\n30.9\n58.7\n25.4\n34.6\n53\n68.2\n94.3\n94.6\nrecapitulate all the events in the video. Hence the task of\ndense video captioning [17, 21] is introduced recently and\nthe ultimate goal is to generate a sentence for each event\noccurring in the video.\nB) Typical Architectures\nAccording to the road map of vision to language, the main-\nstream of modern image captioning follows the structure of\nCNN encoder plus LSTM decoder, as shown in Fig. 2 (a).\nIn particular, given an image, image features can be ﬁrstly\nextracted through multiple ways: 1) directly taking the out-\nputs of fully-connected layers as image features [49]; 2)\nincorporating high-level semantic attributes into image fea-\ntures [61]; 3) performing attention mechanism to measure\nthe contribution of each image region [54]; 4) extracting\nregion-level features [2] and further exploring relation [62]\nor image hierarchy [63] on the region-level features. The\nimage features will be further fed into LSTM decoder to\ngenerate the output sentence, one word at each time step. In\nthe training stage, the next word is generated based on the\nprevious ground-truth words while during testing the model\nuses the previously generated words to predict the next\nword. In order to bridge the mismatch between training\nand testing, reinforcement learning [24, 39, 40] is usu-\nally exploited to directly optimize LSTM decoder with the\nsentence-level reward, such as CIDEr or METEOR.\nTaking the inspiration from the recent successes of\nTransformer self-attention networks [47] in machine trans-\nlation, recent attention has been geared toward exploring\nTransformer-based structure [42] in image captioning. Fig.\n2 (b) depicts the typical architecture of Transformer-based\nencoder-decoder. Different from CNN encoder plus LSTM\ndecoder that capitalizes on LSTM to model word depen-\ndency, Transformer-based encoder-decoder model fully\nutilizes attention mechanism to capture the global depen-\ndencies among inputs. For encoder, N multi-head self-\nattention layers are stacked to model the self-attention\namong input image regions. The decoder contains a stack\nof N multi-head attention layers, each of which con-\nsists of a self-attention sub-layer and a cross-attention\nsub-layer. More speciﬁcally, the self-attention sub-layer\nis ﬁrstly adopted to capture word dependency and the\ncross-attention sub-layer is further utilized to exploit the\nco-attention across vision (image regions from encoder)\nand language (input words).\nSimilar to the mainstream in image captioning, the typ-\nical paradigm in video captioning is also essentially an\nencoder-decoder structure. A video is ﬁrst encoded into\n4\nTAO MEI, WEI ZHANG, TING YAO\n2014\n2016\n2017\n2018\nText encoding\n•\n[Reed, ICML16]\n•\npioneer work / low \nresolution\nVideo generation\n•\nTGANs-C [Pan, MM17]\n•\nlow-resolution / short duration / \ndigits motions \nAttention 2.0\n•\nAttnGAN [Xu, CVPR18], DA-GAN [Ma, \nCVPR18]\n•\nsub-region details\nConditioning\n•\ncGAN [Mirza, arXiv14]\n•\nflagship work / digits / \nlow resolution\nStacked architecture\n•\nStackGAN [Zhang, ICCV17]\n•\nStackGAN++[Zhang, arXiv17]\n•\nhigh-resolution\nSemantic map, Scene-graph\n•\n[Hong, CVPR18], [Johnson, CVPR18]\n•\nmulti-objects scene / low visual quality\n2019 +\nLayout generation\n•\n[Zheng, arXiv19], [Song, CVPR19]\n•\n[Hinz, ICLR19], [Li, CVPR19]\nRedescription\n•\n[Qiao, CVPR19]\n•\ntext -> image -> text’\nAttention 1.0\n•\n[Mansimov, ICLR16]\n•\ntext-image alignment, \nlow visual quality\nFig. 3.: The road map of “language-to-vision\" in past ﬁve years, while milestone techniques are marked along the year\naxis. Top: single object generation. Bottom: multiple-objects scene generation.\na set of frame/clip/shot features via 2D CNN [49] or 3D\nCNN [37, 45]. Next, all the frame-level, clip-level or shot-\nlevel visual features are fused into video-level representa-\ntions through pooling [33], attention [58] or LSTM-based\nencoder [48]. The video-level features are then fed into\nLSTM decoder to produce a natural sentence.\nC) Evaluation and Applications\nEvaluation. Here we summarize the reported performance\nof representative image captioning methods on the test-\ning server of popular benchmark COCO [23] in Table 1.\nIn terms of all the evaluation metrics, GCN-LSTM [62]\nand HIP [63] lead to performance boost against other cap-\ntioning systems, which veriﬁes the advantage of exploring\nrelations and hierarchal structure among image regions.\nApplications. Recently, there exist several emerging\napplications which involve the technology of vision to lan-\nguage. For example, captioning is integrated into online\nchatbot [35, 46] and an ai-created poetry [69] is pub-\nlished in China. In JD.com, we utilize captioning tech-\nniques for personalized product description generation last\nyear, which aims to produce compelling recommendation\nreasons for billions of products automatically.\nIII. LANGUAGE TO VISION\nThis section discusses from another direction of “language\nto vision\", i.e., visual content generation guided by lan-\nguage inputs. In this section, we start by reviewing the road\nmap development, as well as the technical advancements in\nthis area. Then we discuss the open issues and applications\nparticularly from the perspective of industry.\nVisual Content Generation. We brieﬂy introduce the\ndomain of visual generation, since “language to vision\" is\ndeeply rooted in the same techniques. Over the past few\nyears, we have witnessed great progresses in visual content\ngeneration. The origin of visual generation dates back to\n[8], where multiple networks are jointly trained in an adver-\nsarial manner. Subsequent works generate images in spe-\nciﬁc domains such as face [5, 15, 16], person [27, 28, 44],\nas well as generic domains [4, 26]. From the perspective of\ninputs, the generation can also be treated as conditioning\non different information, e.g., noise vector [8], seman-\ntic label [31], textual captions [38], scene-graph [14] and\nimages [11, 70]. Among all these works, visual generation\nbased on natural languages plays one of the most promis-\ning branches, since semantics are directly incorporated into\nthe pixel-wise generation process.\nA) Road Map of Language to Vision\nFig. 3 summarizes recent development of “language to\nvision\". In general, both the vision and language modalities\nare becoming more and more complicated, and the results\nare much more visually convincing, compared to when it\nwas ﬁrstly introduced in 2014.\nThe fundamental architecture is based on a conditional\ngenerative adversarial network, where the conditioning\ninput is usually the encoded natural language. After a series\nof transposed-convolutions, the language input is gradu-\nally mapped to a visual image with higher and higher\nresolution. The key challenges are in two folds: 1) how\nto interpret the language input, i.e., language representa-\ntion, and 2) how to align the visual and textual modalities,\ni.e., the semantic consistency between vision and language.\nRecent results on single object (bottom) have already been\nvisually plausible to human perception. However, state-of-\nthe-art models are still struggling in generating scenes with\nmultiple objects interacting with each other.\nB) Technical Advancements\nThe success in language to vision generation are mostly\nbased on the following technical advancements, which\nVISION AND LANGUAGE\n5\nhave become standard practices commonly accepted by the\nresearch community.\nConditioning Input. Following the standard GAN\nframework [8], Mirza and Osindero [31] derived the con-\nditional version GAN, which allows visual generation\naccording to language inputs. The conditioning informa-\ntion can be in any forms of language, such as tag, sen-\ntence, paragraph, image, scene-graph and layout. Almost\nall subsequent works in “language to vision\" are based on\nthe conditioning architecture. However at that time, only\nMNIST [19] digits are demonstrated in low resolution, and\nthe conditioning information is merely a digit-label.\nText Encoding. GAN-INT-CLS [38] is the ﬁrst work\nbased on natural-language inputs. For the ﬁrst time, it\nbridges the gap from natural language sentences to image\npixels. The key step is based on learning a text representa-\ntion based on a recurrent network to capture visual clues.\nThe rest is mostly following [31]. Additionally, a matching-\naware discriminator is proposed to keep the consistency\nbetween the generated image and textual input. Though the\nresults still look primitive, people can draw ﬂower images\nby altering textual inputs.\nStacked Architecture. Another big advancement is by\nstackGAN [65, 66], where stacked generators are intro-\nduced for high-resolution image generation. Different from\nprevious works, stackGAN can generate realistic 256x256-\npixel images by decomposing the generator into multi-\nple stages stacked sequentially. The Stage-I network only\nsketches the primitive shape and color of the object based\non text representation, yielding a low resolution image.\nThe Stage-II network further ﬁlls details, such as tex-\ntures, conditioning on the Stage-I result. A conditioning\naugmentation technique is also introduced to augment the\ntextual input and stabilize the training process. Compared\nto [38], the visual quality is much improved based on\nthis stacked architecture. Similar idea is also adopted in\nProgressively-Growing GAN [15].\nAttention Mechanism. As in other vision tasks, atten-\ntion is effective in highlighting key information. In “lan-\nguage to vision\", attention is particularly useful in aligning\nkeywords (language) and image patches (vision) during\nthe generation process. Two generations (v1.0 and v2.0)\nof attention basically follow this paradigm, but differs in\nmany details, e.g., network architecture, text encoding.\nAttention 1.0, AlignDraw [30], proposes to iteratively paint\non a canvas by looking at different words at different\nstages. However the results were not promising at that time.\nAttention 2.0, AttnGAN [55] and DA-GAN [29], basically\nfollows the similar paradigm, but improves signiﬁcantly on\nimage quality, e.g., ﬁne-grained details.\nSemantic Layout. Recent studies [3, 7, 44] have\ndemonstrated the importance of semantic layout in image\ngeneration, where layout acts as the blue-print to guide the\ngeneration process. In language to vision, semantic layout\nand scene-graph are introduced to reshape the language\ninput with more semantics. Hong et al. [10] propose to\ngenerate object bounding-boxes ﬁrst, and then reﬁne by\nestimating appearances inside each box. Johnson et al. [14]\nencode objects relationship from scene graph to construct\nthe layout for decoder generation with graph convolutions.\nZheng et al. [67] introduce spatial constraint module and\ncontextual fusion module to model the relative scale and\noffset among objects for commonsense layout generation,\nand Hinz et al. [9] further propose an object pathway for\nmulti-objects generation with complex spatial layouts.\nC) Progress and Applications\nThe development of “Language to Vision\" can be summa-\nrized as follows. On one hand, the language description\nis becoming more complex, i.e., from simple words to\nlong sentences. On the other hand, the vision part is also\nbecoming more complex, where objects-interaction and\nﬁne-grained detail are expected:\n• Language: label →sentence →paragraph →scene graph\n• Vision: single object →multiple objects\nLanguage to Image. Early studies mainly focus on sim-\nple words and single-object images, e.g., birds [52], ﬂowers\n[32] and generic objects [41]. As shown in Fig. 3 (bot-\ntom), the visual quality is much improved over the past\nfew years, and some results are plausible enough to deceive\nhuman eyes.\nThough single-object image can be well generated,\nmulti-objects scene still struggles for realistic results, as in\nFig. 3 (top). A general trend is to reduce the complexity\nby introducing semantic layout as an intermediate repre-\nsentation. Roughly, machines now can generate spatially\nreasonable images, but ﬁne-grained details are still far from\nsatisfactory at current stage.\nLanguage to Video. Compared to image, language to\nvideo is more challenging due to huge volume of infor-\nmation and extra temporal constraint. There is only a few\nworks studying this area. For example, Pan et al. [34]\nattemp to generate video out of captions based on 3D con-\nvolution operation. However, the results are quite limited\nfor practical applications.\nApplications. The application of “language to vision\"\ncan be roughly grouped into two categories: generation\nfor human eyes or for machines. In certain domains (e.g.,\nface), language to vision already starts to produce highly\nplausible results with industrial standards1. For example,\npeople can generate royalty-free facial photos on demand2\nfor games [43] or commercials, by manually specifying\ngender, hair, eyes. Another direction is generating data for\nmachine and algorithms. For example, NVIDIA [68] pro-\nposed a large-scale synthetic dataset (DG-Market) for train-\ning person Re-ID models. Also some image recognition\nand segmentation models start to beneﬁt from machine-\ngenerated training images. However, it is worth noting that\ndespite the promising results, there is still a large gap for\nmassive deployment in industrial products.\n1https://thispersondoesnotexist.com/\n2https://github.com/SummitKwan/transparent_latent_gan\n6\nTAO MEI, WEI ZHANG, TING YAO\nIV. CONCLUSION\nVision and language are two fundamental systems of\nhuman representation. Integrating the two in one intelligent\nsystem has long been an ambition in AI ﬁeld. As we have\ndiscussed in the paper, on one hand, vision to language is\ncapable of understanding visual content and automatically\nproducing a natural-language description, and on the other\nhand, language to vision is able to characterize the intrinsic\nstructure in vision data and create visual content accord-\ning to the language inputs. Such interactions, while still at\nthe early stage, motivate us to understand the mechanisms\nin connecting vision and language, reshape real-world\napplications, and re-think the end result of the integration.\nR E F E R E N C E S\n[1] Nayyer Aafaq, Naveed Akhtar, Wei Liu, Syed Zulqarnain Gilani,\nand Ajmal Mian. Spatio-temporal dynamics and semantic attribute\nenriched visual encoding for video captioning. In CVPR, 2019.\n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark\nJohnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down\nattention for image captioning and visual question answering. In\nCVPR, 2018.\n[3] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B.\nTenenbaum, William T. Freeman, and Antonio Torralba. Gan dissec-\ntion: Visualizing and understanding generative adversarial networks.\nICLR, 2019.\n[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale\nGAN training for high ﬁdelity natural image synthesis. In Interna-\ntional Conference on Learning Representations, 2019. URL https:\n//openreview.net/forum?id=B1xsqj09Fm.\n[5] Anpei Chen, Zhang Chen, Guli Zhang, Kenny Mitchell, and Jingyi\nYu. Photo-realistic facial details synthesis from single image. In\nThe IEEE International Conference on Computer Vision (ICCV),\nOctober 2019.\n[6] Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Hongyang Chao,\nand Tao Mei. Temporal deformable convolutional encoder-decoder\nnetworks for video captioning. In AAAI, 2019.\n[7] Haoye Dong, Xiaodan Liang, Ke Gong, Hanjiang Lai, Jia Zhu,\nand Jian Yin. Soft-gated warping-gan for pose-guided person image\nsynthesis. In NeurIPS, 2018.\n[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David\nWarde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.\nGenerative adversarial nets. In NIPS, 2014.\n[9] Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Generating\nmultiple objects at spatially distinct locations. ICLR, 2019.\n[10] Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak\nLee. Inferring semantic layout for hierarchical text-to-image synthe-\nsis. In CVPR, 2018.\n[11] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.\nImage-to-image translation with conditional adversarial networks.\narxiv, 2016.\n[12] Wenhao Jiang, Lin Ma, Yu-Gang Jiang, Wei Liu, and Tong Zhang.\nRecurrent fusion network for image captioning. In ECCV, 2018.\n[13] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully\nconvolutional localization networks for dense captioning. In CVPR,\n2016.\n[14] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from\nscene graphs. In CVPR, 2018.\n[15] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Pro-\ngressive growing of gans for improved quality, stability, and varia-\ntion. In ICLR, 2018.\n[16] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator\narchitecture for generative adversarial networks. In CVPR, 2019.\n[17] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Car-\nlos Niebles. Dense-captioning events in videos. In ICCV, 2017.\n[18] Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar,\nSiming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg.\nBabytalk: Understanding and generating simple image descriptions.\nIEEE Trans. on PAMI, 2013.\n[19] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.\nGradient-based learning applied to document recognition. In Pro-\nceedings of the IEEE, 1998.\n[20] Yehao Li, Ting Yao, Tao Mei, Hongyang Chao, and Yong Rui. Share-\nand-chat: Achieving human-level video commenting by search and\nmulti-view embedding. In ACM MM, 2016.\n[21] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei.\nJointly localizing and describing events for dense video captioning.\nIn CVPR, 2018.\n[22] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei.\nPointing novel objects in image captioning. In CVPR, 2019.\n[23] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bour-\ndev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common\nobjects in context. CoRR, abs/1405.0312, 2014.\n[24] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin\nMurphy. Optimization of image description metrics using policy\ngradient methods. In ICCV, 2017.\n[25] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Know-\ning when to look: Adaptive attention via a visual sentinel for image\ncaptioning. In CVPR, 2017.\n[26] Mario Luˇci´c, Michael Tschannen, Marvin Ritter, Xiaohua Zhai,\nOlivier Bachem, and Sylvain Gelly. High-ﬁdelity image genera-\ntion with fewer labels. In International Conference on Machine\nLearning, pages 4183–4192, 2019.\n[27] Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, and\nLuc Van Gool. Pose guided person image generation. In Advances\nin Neural Information Processing Systems, pages 405–415, 2017.\n[28] Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt\nSchiele, and Mario Fritz. Disentangled person image generation.\nIn IEEE Conference on Computer Vision and Pattern Recognition,\n2018.\n[29] Shuang Ma, Jianlong Fu, ChangWen Chen, and Tao Mei. Da-\ngan: Instance-level image translation by deep attention generative\nadversarial networks. In CVPR, 2018.\n[30] Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan\nSalakhutdinov. Generating images from captions with attention. In\nICLR, 2016.\n[31] Mehdi Mirza and Simon Osindero. Conditional generative adversar-\nial nets. In arXiv:1411.1784, 2014.\n[32] Maria-Elena Nilsback and Andrew Zisserman. Automated ﬂower\nclassiﬁcation over a large number of classes. In Proceedings of\nthe Indian Conference on Computer Vision, Graphics and Image\nProcessing, 2008.\nVISION AND LANGUAGE\n7\n[33] Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and Yong Rui.\nJointly modeling embedding and translation to bridge video and\nlanguage. In CVPR, 2016.\n[34] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To\ncreate what you tell: Generating videos from captions. In ACM MM,\n2017.\n[35] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei.\nSeeing bot. In SIGIR, 2017.\n[36] Yingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. Video captioning\nwith transferred semantic attributes. In CVPR, 2017.\n[37] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal\nrepresentation with pseudo-3d residual networks. In ICCV, 2017.\n[38] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran,\nBernt Schiele, and Honglak Lee. Generative adversarial text to\nimage synthesis. In ICML, 2016.\n[39] Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, and Li-Jia Li. Deep\nreinforcement learning-based image captioning with embedding\nreward. In CVPR, 2017.\n[40] Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross,\nand Vaibhava Goel. Self-critical sequence training for image cap-\ntioning. In CVPR, 2017.\n[41] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev\nSatheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya\nKhosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.\nImageNet Large Scale Visual Recognition Challenge. International\nJournal of Computer Vision (IJCV), 115(3):211–252, 2015.\n[42] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.\nConceptual captions: A cleaned, hypernymed, image alt-text dataset\nfor automatic image captioning. In ACL, 2018.\n[43] Tianyang Shi, Yi Yuan, Changjie Fan, Zhengxia Zou, Zhenwei Shi,\nand Yong Liu. Face-to-parameter translation for game character\nauto-creation. In ICCV, 2019.\n[44] Sijie Song, Wei Zhang, Jiaying Liu, and Tao Mei. Unsupervised\nperson image generation with semantic parsing transformation. In\nCVPR, 2019.\n[45] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and\nManohar Paluri. Learning spatiotemporal features with 3d convo-\nlutional networks. In ICCV, 2015.\n[46] Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cornelia Cara-\npcea, Chris Thrasher, Chris Buehler, and Chris Sienkiewicz. Rich\nimage captioning in the wild. In CVPR Workshops, 2016.\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. In NIPS, 2017.\n[48] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Ray-\nmond Mooney, Trevor Darrell, and Kate Saenko. Sequence to\nsequence-video to text. In ICCV, 2015.\n[49] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan.\nShow and tell: A neural image caption generator. In CVPR, 2015.\n[50] Jing Wang, Yingwei Pan, Ting Yao, Jinhui Tang, and Tao Mei. Con-\nvolutional auto-encoding of sentence topics for image paragraph\ngeneration. In IJCAI, 2019.\n[51] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Ding-\nhan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang.\nReinforced cross-modal matching and self-supervised imitation\nlearning for vision-language navigation. In CVPR, 2019.\n[52] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Flo-\nrian Schroff, Serge Belongie, and Pietro Perona. Caltech-UCSD\nBirds 200. Technical Report CNS-TR-2010-001, California Institute\nof Technology, 2010.\n[53] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video\ndescription dataset for bridging video and language. In CVPR, 2016.\n[54] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\nCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio.\nShow, attend and tell: Neural image caption generation with visual\nattention. In ICML, 2015.\n[55] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,\nXiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to\nimage generation with attentional generative adversarial networks.\nIn CVPR, 2018.\n[56] Yezhou Yang, Ching Lik Teo, Hal Daumé III, and Yiannis Aloi-\nmonos. Corpus-guided sentence generation of natural images. In\nEMNLP, 2011.\n[57] Zhilin Yang, Ye Yuan, Yuexin Wu, William W. Cohen, and Ruslan R.\nSalakhutdinov. Review networks for caption generation. In NIPS,\n2016.\n[58] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher\nPal, Hugo Larochelle, and Aaron Courville. Describing videos by\nexploiting temporal structure. In ICCV, 2015.\n[59] Ting Yao, Tao Mei, Chong-Wah Ngo, and Shipeng Li. Annotation\nfor free: Video tagging by mining user search behavior. In ACM MM,\n2013.\n[60] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Incorporating copy-\ning mechanism in image captioning for learning novel objects. In\nCVPR, 2017.\n[61] Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, and Tao Mei.\nBoosting image captioning with attributes. In ICCV, 2017.\n[62] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring visual\nrelationship for image captioning. In ECCV, 2018.\n[63] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Hierarchy parsing\nfor image captioning. In ICCV, 2019.\n[64] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo\nLuo. Image captioning with semantic attention. In CVPR, 2016.\n[65] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang\nWang, Xiaolei Huang, and Dimitris N. Metaxas. Stackgan++: Real-\nistic image synthesis with stacked generative adversarial networks.\nIn arXiv:1710.10916, 2017.\n[66] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang\nWang, Xiaolei Huang, and Dimitris N. Metaxas. Stackgan: Text to\nphoto-realistic image synthesis with stacked generative adversarial\nnetworks. In ICCV, 2017.\n[67] Hongdong\nZheng,\nYalong\nBai,\nWei\nZhang,\nand\nTao\nMei.\nRelationship-aware spatial perception fusion for realistic scene lay-\nout generation. arXiv:1909.00640, 2019.\n[68] Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng,\nYi Yang, and Jan Kautz. Joint discriminative and generative learning\nfor person re-identiﬁcation. In CVPR, 2019.\n[69] Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum. The design\nand implementation of xiaoice, an empathetic social chatbot. arXiv\npreprint arXiv:1812.08989, 2018.\n[70] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.\nUnpaired image-to-image translation using cycle-consistent adver-\nsarial networks. In Computer Vision (ICCV), 2017 IEEE Interna-\ntional Conference on, 2017.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2019-12-26",
  "updated": "2019-12-26"
}