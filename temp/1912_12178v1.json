{
  "id": "http://arxiv.org/abs/1912.12178v1",
  "title": "Unsupervised Few-shot Learning via Self-supervised Training",
  "authors": [
    "Zilong Ji",
    "Xiaolong Zou",
    "Tiejun Huang",
    "Si Wu"
  ],
  "abstract": "Learning from limited exemplars (few-shot learning) is a fundamental,\nunsolved problem that has been laboriously explored in the machine learning\ncommunity. However, current few-shot learners are mostly supervised and rely\nheavily on a large amount of labeled examples. Unsupervised learning is a more\nnatural procedure for cognitive mammals and has produced promising results in\nmany machine learning tasks. In the current study, we develop a method to learn\nan unsupervised few-shot learner via self-supervised training (UFLST), which\ncan effectively generalize to novel but related classes. The proposed model\nconsists of two alternate processes, progressive clustering and episodic\ntraining. The former generates pseudo-labeled training examples for\nconstructing episodic tasks; and the later trains the few-shot learner using\nthe generated episodic tasks which further optimizes the feature\nrepresentations of data. The two processes facilitate with each other, and\neventually produce a high quality few-shot learner. Using the benchmark dataset\nOmniglot and Mini-ImageNet, we show that our model outperforms other\nunsupervised few-shot learning methods. Using the benchmark dataset Market1501,\nwe further demonstrate the feasibility of our model to a real-world application\non person re-identification.",
  "text": "UNSUPERVISED\nFEW-SHOT\nLEARNING\nVIA\nSELF-\nSUPERVISED TRAINING\nZilong Ji\nState Key Laboratory of Cognitive\nNeuroscience & Learning,\nBeijing Normal University, Beijing, China\njizilong@mail.bnu.edu.cn\nXiaolong Zou, Tiejun Huang\nSchool of Electronics Engineering\nand Computer Science,\nPeking University, Beijing, China\nxiaolz, tjhuang@pku.edu.cn\nSi Wu\nSchool of Electronics Engineering\nand Computer Science,\nIDG/McGovern Institute for Brain Research,\nPeking University, Beijing, China\nsiwu@pku.edu.cn\nABSTRACT\nLearning from limited exemplars (few-shot learning) is a fundamental, unsolved\nproblem that has been laboriously explored in the machine learning community.\nHowever, current few-shot learners are mostly supervised and rely heavily on a\nlarge amount of labeled examples. Unsupervised learning is a more natural proce-\ndure for cognitive mammals and has produced promising results in many machine\nlearning tasks. In the current study, we develop a method to learn an unsupervised\nfew-shot learner via self-supervised training (UFLST), which can effectively gen-\neralize to novel but related classes. The proposed model consists of two alter-\nnate processes, progressive clustering and episodic training. The former generates\npseudo-labeled training examples for constructing episodic tasks; and the later\ntrains the few-shot learner using the generated episodic tasks which further opti-\nmizes the feature representations of data. The two processes facilitate with each\nother, and eventually produce a high quality few-shot learner. Using the bench-\nmark dataset Omniglot and Mini-ImageNet, we show that our model outperforms\nother unsupervised few-shot learning methods. Using the benchmark dataset Mar-\nket1501, we further demonstrate the feasibility of our model to a real-world appli-\ncation on person re-identiﬁcation.\n1\nINTRODUCTION\nFew-shot learning, which aims to accomplish a learning task by using very few training examples, is\nreceiving increasing attention in the machine learning community. The challenge of few-shot learn-\ning lies on that traditional techniques such as ﬁne-tuning would normally incur overﬁtting (Wang\net al., 2018). To overcome this difﬁculty, a set-to-set meta-learning(episodic learning) paradigm was\nproposed (Vinyals et al., 2016). In such a paradigm, the conventional mini-batch training is replaced\nby the episodic training, in term of that a batch of episodic tasks, each of which having the same\nsetting as the testing environment, are presented to the learning model; and in each episodic task, the\nmodel learns to predict the classes of unlabeled points (the query set) using very few labeled exam-\nples (the support set). By this, the learning model acquires the transferable knowledge (optimized\nfeature representations) across tasks, and due to the consistency between the training and testing\nenvironments, the model is able to generalize to novel but related tasks. Although this set-to-set\nfew-shot learning paradigm has made great progress, in its current supervised form, it requires a\nlarge number of labeled examples for constructing episodic tasks, which is often infeasible or too\nexpensive in practice. So, can we build up a few-shot learner in the paradigm of episodic training\nusing only unlabeled data?\n1\narXiv:1912.12178v1  [cs.CV]  20 Dec 2019\nEpisodic\n Tasks\nEpisodic Training Block\n...\ntask1\ntask2\ntask3\nPseudo Labeled set\nRaw Data\nFeatures\nClusters\nClustering\nFeature Extractor\nFew-shot Learner@T\nFew-shot Learner@T+1\n...\nClustering Block\nClustering + Episodic Training\nFigure 1: The scheme of our model UFLST, which consists of two alternate processes: clustering\nand episodic training. At each round, unlabeled data points are clustered based on extracted features,\nand pseudo labels are assigned according to cluster identities. After clustering, a set of episodic tasks\nare constructed by sampling from the pseudo-labeled data, and the few-shot learner is trained, which\nfurther optimizes feature representations. The two processes are repeated.\nIt is well-known that humans have the remarkable ability to learn a concept when given only several\nexposures to its instances, for example, young children can effortlessly learn and generalize the\nconcept of “giraffe” after seeing a few pictures of giraffes. While the speciﬁcs of the human learning\nprocess are complex (trial-based, perpetual, multi-sourced, and simultaneous for multiple tasks)\nand yet to be solved, previous works agree that its nature is progressive and unsupervised in many\ncases (Dupoux, 2018). Given a set of unlabeled items, humans are able to organize them into\ndifferent clusters by comparing one with another. The comparing or associating process follows\na coarse-to-ﬁne manner. At the beginning of learning, humans tend to group items based on fuzzy-\nrough knowledge such as color, shape or size. Subsequently, humans build up associations between\nitems using more ﬁne-grained knowledge, i.e., stripes of images, functions of items or other domain\nknowledge. Furthermore, humans can extract representative representations across categories and\napply this capability to learn new concepts (Wang et al., 2014).\nIn the present study, inspired by the unsupervised and progressive characteristics of human learning,\nwe propose an unsupervised model for few-shot learning via a self-supervised training procedure\n(UFLST). Different from previous unsupervised learning methods, our model integrates unsuper-\nvised learning and episodic training into a uniﬁed framework, which facilitates feature extraction\nand model training iteratively. Basically, we adopt the episodic training paradigm, taking advan-\ntage of its capability of extracting transferable knowledge across tasks, but we use an unsupervised\nstrategy to construct episodic tasks. Speciﬁcally, we apply progressive clustering to generate pseudo\nlabels for unlabeled data, and this is done alternatively with feature optimization via few-shot learn-\ning in an iterative manner (Fig. 1). Initially, unlabeled data points are assigned into several clusters,\nand we sample a few training examples from each cluster together with their pseudo labels (the\nidentities of clusters) to construct a set of episodic tasks having the same setting as the testing\nenvironment. We then train the few-shot learner using the constructed episodic tasks and obtain\nimproved feature representations for the data. In the next round, we use the improved features to re-\ncluster data points, generating new pseudo labels and constructing new episodic tasks, and train the\nfew-shot learner again. The above two steps are repeated till a stopping criterion is reached. After\ntraining, we expect that the few-shot learner has acquired the transferable knowledge (the optimized\nfeature representations) suitable for a novel task of the same setting as in the episodic training. Us-\ning benchmark datasets, we demonstrate that our model outperforms other unsupervised few-shot\nlearning methods and approaches to the performances of fully supervised models.\n2\nRELATED WORKS\nIn the paradigm of episodic training, few-shot learning algorithms can be divided into two main\ncategories: “learning to optimize” and “learning to compare”. The former aims to develop a learning\nalgorithm which can adapt to a new task efﬁciently using only few labeled examples or with few\nsteps of parameter updating (Finn et al., 2017; Ravi & Larochelle, 2016; Mishra et al., 2017; Rusu\n2\net al., 2018; Nichol & Schulman, 2018; Andrychowicz et al., 2016), and the latter aims to learn a\nproper embedding function, so that prediction is based on the distance (metric) of a novel example\nto the labeled instances (Vinyals et al., 2016; Snell et al., 2017; Ren et al., 2018; Sung et al., 2018;\nLiu et al., 2018). In the present study, we focus on the “learning to compare” framework, although\nthe other framework can also be integrated into our model.\nOnly very recently, people have tried to develop unsupervised few-shot learning models. Hsu et al.\n(2018) proposed a method called CACTUs, which uses progressive clustering to leverage feature\nrepresentations before carrying out episodic training. This is different from our model, in term of\nthat we carry out progressive clustering and episodic training concurrently, and the two processes\nfacilitate with each other. Khodadadeh et al. (2018) proposed a method called UMTRA, which uti-\nlizes the statistical diversity properties and domain-speciﬁc augmentations to generate training and\nvalidation data. Antoniou & Storkey (2019) proposed a similar model called AAL, which uses data\naugmentations of the unlabeled support set to generate the query data. Both methods are different\nfrom our model, in term of that we use a progressive clustering strategy to generate pseudo labels\nfor constructing episodic tasks.\nThe idea of self-supervised training is to artiﬁcially generate pseudo labels for unlabeled data, which\nis useful when supervisory signals are not available or too expensive (de Sa, 1994). Progressive\n(deep) clustering is a promising method for self-supervised training, which aims to optimize feature\nrepresentations and pseudo labels (cluster assignments) in an iterative manner. This idea was ﬁrst\napplied in NLP tasks, which tries to self-train a two-phase parser-reranker system using unlabeled\ndata (McClosky et al., 2006). Xie et al. (2016) proposed a Deep Embedded Clustering network to\njointly learn cluster centers and network parameters. Caron et al. (2018) further proposed strategies\nto solve the degenerated solution problem in progressive clustering. Fan et al. (2018) and Song et al.\n(2018) applied the progressive clustering idea to the person re-identiﬁcation task, both of which aim\nto transfer the extracted feature representations to an unseen domain. None of these studies have\nintegrated progressive clustering and episodic training in few-shot learning as we do in this work.\n3\nMETHOD\nIn this section, we describe the model UFLST in detail. Let us ﬁrst introduce some notations. Denote\nthe model at the training round t as M t, {xi} the unlabeled dataset with the number of examples N,\nand {zi}t is the corresponded feature vector with dimentionality D, which is given by fθt : x\nM t\n−−→z,\nwhere fθt representing the feature extracter and θt the training parameters of M t. {ezi}t and {exi}t\nrepresent, respectively, the selected features and unlabeled data after removing outliers from the\nclustering results, and {eyi}t the corresponding pseudo labels.\n3.1\nPROGRESSIVE CLUSTERING\n3.1.1\nK-RECIPROCAL JACCARD DISTANCE\nTo cluster unlabeled data points, we adopt the k-reciprocal Jaccard distance (KRJD) metric to mea-\nsure the distance between data points (Qin et al., 2011; Zhong et al., 2017), and it is done in the\nfeature space rather than the raw pixel space. First, we calculate the k-reciprocal nearest neighbours\nof each feature point, which are given by,\nR(z, k) = {zj |(zj ∈N(z, k)) ∩(z ∈N(zj, k))},\n(1)\nwhere N(z, k) denotes the k nearest neighbours of z. R(z, k) imposes that z and each element of\nR(z, k) are mutually k nearest neighbours of each other. Second, we compute KRJD between two\nfeature points, which is given by\nJij = 1 −|R(zi, k) ∩R(zj, k)|\n|R(zi, k) ∪R(zj, k)|.\n(2)\nCompared to the Euclidean distance, KRJD takes into account the reciprocal relationship between\ndata points, and hence is a stricter rule measuring whether two feature points matches or not. We\nﬁnd that KRJD is crucial to our model, which outperforms the Euclidean metric as demonstrated in\nFig. 2.\n3\n(A)\n(B)\nFigure 2: Comparing the performances of KRJD and the Euclidean metric. Top 10 neighbours\nof a chosen query character from Omniglot are shown. Black box: the query character. Green\nbox: the positive characters in the neighbourhood of the query character. Red box: the negative\ncharacters in the neighbourhood of query character. (A) The ranking result using Euclidean metric.\n(B) The ranking result using KRJD. The number under each image represents its true class. KRJD\noutperforms the Euclidean metric, in term of it includes more positive examples in the ranking list.\n3.1.2\nDENSITY-BASED SPATIAL CLUSTERING ALGORITHM\nFor the clustering strategy, we choose the density-based spatial clustering algorithm (DBSCAN) (Es-\nter et al., 1996), which performs better than other methods in our model. The reasons are: 1) other\nclustering methods such as k-means and hierarchical clustering are useful to ﬁnd spherical shaped or\nconvex clusters in the embedding space, while DBSCAN works well for arbitrarily shaped clusters;\n2) DBSCAN can detect outliers as noise points, which is very useful at the beginning of training\nwhen the distribution of data points in the embedding space is highly noisy; 3) DBSCAN does not\nneed to specify the number of clusters to be generated, which is appealing for unsupervised learning.\nAfter removing noisy points (outliers) as done in DBSCAN, pseudo labels (i.e., cluster identities) of\ndata points {exi}t can be expressed as,\n{eyi}t = DBSCAN(ϵ, ms, J),\n(3)\nwhere ϵ denotes the maximum distance for two points to be considered as in the same neighborhood,\nms the minimum number of points huddled together for a region to be considered as dense, and J\nthe KRJD matrix. The value of ϵ relies on the cluster density of feature points, which is set to be the\nmean distance of top P minimum distances in KRJD in this study.\n3.2\nEPISODIC TRAINING\n3.2.1\nCONSTRUCTING EPISODIC TASKS\nAfter each round of clustering, we construct a number of episodic tasks, denoted as T\n=\n{T1, T2, ..., TS} with S the number of tasks, from the pseudo-labeled data set {exi, eyi}t. For each\nepisodic task, we randomly sample NC classes and NE examples per class. Notably, the setting of\neach episodic task follows that of the test environment to be performed after training.\nWe will apply two different ways to implement few-shot learning (see below). One uses the proto-\ntype loss, which aims to learn the prototypes of each class and discriminate a novel example based\non its distances to the prototypes. In this case, we further split NE into a support set S and a query\nset Q, i.e., NE = NS + NQ. Following Snell et al. (2017), we choose a larger value of NC in train-\ning than that in testing, but keeps NS the same. The other way to implement few-shot learning is\nto use the triplet loss or the hardtriplet loss, which separates examples from different classes with a\npositive margin m. In this case, no splitting support and query data is needed. To mine hard negative\nexamples in triplets, we also use a larger NC in training than that in testing.\n3.2.2\nLOSS FUNCTIONS\nTwo types of loss functions are used in the present study, and both of them are in the framework of\n“learning to compare” and contribute to simple inductive bias of our model. One is the prototype\n4\nAlgorithm 1 Unsupervised Few-shot Learning via Self-supervised Training (UFLST)\nInput: Unlabeled dataset {xi}, initialized model parameters θ0\nOutput: Trained model parameters θT\n1: t = 0\n2: repeat\n3:\nClustering:\n4:\nExtracting features {zi}t of {xi} using the feature extractor fθt.\n5:\nComputing K-reciprocal nearest neighbours R(zi, k)t of each zi.\n6:\nCalculating Jaccard distance matrix J based on {R(zi, k)}t.\n7:\nClustering data using DBSCAN and generating pseudo labels {yi}t.\n8:\nRemoving outliers and obtaining the pseudo-labeled dataset {exi, eyi}t.\n9:\nEpisodic Training:\n10:\ns = 0\n11:\nrepeat\n12:\nConstructing a episodic task T s by randomly sampling NC classes with NE examples per\nclass from {exi, eyi}t.\n13:\nUpdating model parameters θt by training the few-shot learner on T s.\n14:\ns = s + 1\n15:\nuntil s = S\n16:\nt = t + 1\n17: until t = T\nloss, which is written as\nLproto(z, cp; θ) =\nexp(−∥z −cp∥2\n2)\nP\nk exp(−∥z −ck∥2\n2),\n(4)\nwhere ck is the prototype of class k given by ck = P\nzi∈Sk(zi)/Sk, and z a query point. In\nimplementation, we choose to minimize the negative log value of Eq. 4, i.e., Llog\nproto(z, ck; θ) =\n−log Lproto(z, ck; θ), as the log value better reﬂects the geometry of the loss function, making it\neasy to select a learning rate to minimize the loss function.\nThe other is the triplet loss (Weinberger & Saul, 2009), which has been widely used in face recogni-\ntion and image retrieval. The triplet loss Ltriplet consists of several triplets, each of which includes\na query feature z, a positive feature z+ and a negative feature z−, and is written as\nLtriplet(z, z+, z−; θ) = max(0, ∥z −z+∥2\n2 −∥z −z−∥2\n2 + m),\n(5)\nwhere m controls the margin of two classes, and the hinge term plays the role of correcting triplets,\nso that the difference between the similarities of positive and negative examples to the query point is\nlarger than a margin m. However, in the above form, positive pairs in those “already correct” triplets\nwill no longer be pulled together due to the hard cutoff. We therefore replace the hinge term by a\nsoft-margin formulation, which gives\nLtriplet−SM(z, z+, z−; θ) = log\n\u0002\n1 + exp(∥z −z+∥2\n2 −∥z −z−∥2\n2 + m)\n\u0003\n.\n(6)\nEq. 6 is similar to Eq. 5, but it decays exponentially instead of having a hard cutoff and tends to be\nnumerically more stable (Hermans et al., 2017).\nWe ﬁnd that in general our model achieves a better performance using the prototype loss than using\nthe triplet loss. However, by including hard example mining when constructing triplets, referred to\nas the hardtriplet loss hereafter, the model performance is improved signiﬁcantly and becomes better\nthat using the prototype loss. The pseudo code of our model is summarized in Algorithm 1.\n4\nEXPERIMENTS\n4.1\nDATASETS\nWe evaluate our model on three benchmark datasets, which are Omniglot (Lake et al., 2015), Mini-\nImageNet (Vinyals et al., 2016) and Market1501 (Zheng et al., 2015).\n5\nOmniglot contains 1623 different handwritten characters from 50 different alphabets. There are 20\nexamples in each class and each of them was drawn by a different human subject via Amazon’s\nMechanical Turk. We split data into two parts: 1200 characters for training and 423 for testing, but\nwe did not augment data with rotations (this is unnecessary in our model), and instead of resizing\nimages to 28 × 28, we resized them to 32 × 32.\nMini-Imagenet is derived from the ILSVRC-12 dataset. We follow the data splits proposed by Ravi\nand Larochelle Ravi & Larochelle (2016), which has a different set of 100 classes including 64 for\ntraining, 16 for validating, and 20 for testing compared to the spilt by Vinyals et al. (2016). Each\nclass contains 600 colored images of size 84 × 84.\nMarket1501 is a person re-identiﬁcation (Re-ID) dataset containing 32668 images with 1501 iden-\ntities captured from 6 cameras. The dataset is split into three parts: 12936 images with 751 identities\nforming the training set, 19732 images with 750 identities forming the gallery set, and another 3368\nimages forming the query set. All images were resized to 256×128. Except normalization, no other\npre-processing was applied.\n4.2\nIMPLEMENTATION DETAILS\nWhen training on the Omniglot and the Mini-ImageNet dataset, we chose the model architecture\nto be the same as that in Vinyals et al. (2016) , which consists of four stacked layers. Each layer\ncomprises 64-ﬁlter 3 × 3 convolution, followed by a batch normalization, a ReLU nonlinearity, and\n2 × 2 max-pooling. When training on the market1501 dataset, due to high variances of pose and\nluminance, we chose to use a highly expressive model (Xiong et al., 2018), which consists of a\nResnet50 pretrained on ImageNet as a backbone, and a batch normalization layer after the global\nmax-pooling layer to prevent overﬁtting. Our evaluation protocol on market1501 is different from\nthat in Zheng et al. (2015), where they reported Cumulative Matching Characteristic (CMC) and\nthe mean average precision (mAPs); while we consider the performance of 1-shot learning, which\nmimics the typical single query condition in a person Re-ID task.\nWhen training with the triplet loss, we set the margin between positive and negative examples to be\n0.5, and the number of training rounds T = 20. To avoid overﬁtting, the model is ﬁne-tuned for\nonly 50 epochs in each round. We used Adam with momentum to update the model parameters, and\nthe learning rate is set to 0.005 with an exponential decay after 25 epochs. The mini-batch size is\n128, which consists of 32 classes and 4 examples per class in each episodic task. When constructing\ntriplets with hard example mining, we didn’t mine hard negative examples across the whole dataset\nwhich is infeasible, rather we did only in the current episodic task. When training with the prototype\nloss, we used more classes (higher way) during training (NC = 60 in Omniglot and NC = 30 in\nMarket1501), which leads to better performances as empirically observed in Snell et al. (2017).\nOther hyper-parameters are set to be the same as training with the triplet loss.\n4.3\nPERFORMANCE OF PROGRESSIVE CLUSTERING\nWe ﬁrst check the behavior of progressive clustering via visualizing 10 hand-written characters from\nthe Futurama alphabets in the Omniglot dataset using T-SNE (Maaten & Hinton, 2008). Overall,\nwe observe that as learning progresses, the organization of data points is improved continuously,\nindicating that our model “discovers” the underlying structure of data gradually. As illustrated\nin Fig. 3A, initially, all data points are intertwined with each other and no structure exists. Over\ntraining, clusters gradually emerge, in the sense of that data points from the same class are grouped\ntogether and the margins between different classes are enlarged. We quantitatively measure the\nclustering quality by computing the Normalized Mutual Information (NMI) between real labels\n{ˆyi}t (i.e., the ground truth) and pseudo labels {eyi}t, which is given by,\nNMI\n\u0000{ˆyi}t, {eyi}t\u0001\n=\nI({ˆyi}t, {eyi}t)\np\nH({ˆyi}t)H({eyi}t)\n,\n(7)\nwhere I(·, ·) is the mutual information between {ˆyi}t and {eyi}t, and H(·) the entropy. The value\nof NMI lies in [0, 1], with 1 standing for perfect alignment between two sets. Note that NMI is\nindependent of the permutation of labeling orders. As shown in Fig. 3B, the value of NMI increases\nwith the training round and gradually reaches to a high value close to 1. Remarkably, the value of\nNMI well predicts the classiﬁcation accuracy of the learning model (comparing Fig. 3B and 3C).\n6\n(A)\n(B)\nRound1\nRound2\nRound3\nTraining Round\nTraining Round\nNMI\n(C)\nAccuracy\nFigure 3: Behaviors of progressive clustering. (A) Visualizing clustering results over training rounds\nby T-SNE. 10 characters from the Futurama alphabets in the Omniglot dataset were selected. (B)\nNMI vs. training round. (C) Classiﬁcation accuracy vs. training round.\nThese results strongly suggest that the combination of progressive clustering and episodic training in\nour model is able to discover the underlying structure of data manifold and extract the representative\nfeatures of data points necessary for the few-shot classiﬁcation task.\n4.4\nRESULTS ON OMNIGLOT\nTable 1 presents the performances of our model on the Omniglot dataset compared with other meth-\nods. We note that using the triplet loss, our model already outperforms other state-of-the-art unsu-\npervised few-shot learning methods, including CACTUs (Hsu et al., 2018), UMTRA (Khodadadeh\net al., 2018), and AAL (Antoniou & Storkey, 2019), to a large extend. Using the prototype loss, the\nperformance of our model is further improved. The best performance of our model is achieved when\nusing the hardtriplet loss. Remarkably, the best performance of our model approaches to that of two\nsupervised models, which are the upper bounds for unsupervised methods.\n5-way Acc.\n20-way Acc.\n1-shot\n5-shot\n1-shot\n5-shot\nUMTRA (Khodadadeh et al., 2018)\n77.80\n92.74\n62.20\n77.50\nCACTUs-MAML (Hsu et al., 2018)\n68.84\n87.78\n48.09\n73.36\nCACTUs-ProtNets (Hsu et al., 2018)\n68.12\n83.58\n47.75\n66.27\nAAL-MAML++ (Antoniou & Storkey, 2019)\n88.40\n97.96\n70.21\n88.32\nAAL-ProtoNets (Antoniou & Storkey, 2019)\n84.66\n89.14\n68.79\n74.28\nUFLST-Tripletloss\n88.68\n96.65\n73.21\n90.11\nUFLST-Prototypeloss\n96.51\n99.23\n90.27\n97.22\nUFLST-HardTripletloss\n97.03\n99.19\n91.28\n97.37\nMAML (Finn et al., 2017) (Supervised)\n98.7\n99.9\n95.8\n98.9\nProtoNets (Snell et al., 2017) (Supervised)\n98.8\n99.7\n96.0\n98.9\nTable 1: Performances of different unsupervised few-shot learning models on Omniglot under dif-\nferent settings.\n7\n(5,1)\n(5,5)\n(5,20) (5, 50)\nTraining fram scratch\n25.17 33.90 39.56\n41.45\nBiGAN knn-nearest neighbors\n25.56 31.10 37.31\n43.60\nBiGAN linear classiﬁer\n27.08 33.91 44.00\n50.41\nBiGAN MLP with dropout\n22.91 29.06 40.06\n48.36\nBiGAN cluster matching\n24.63 29.49 33.89\n36.13\nBiGAN CACTUs MAML\n36.24 51.28 61.33\n66.91\nBiGAN CACTUs ProtoNets\n36.62 50.16 59.56\n63.27\nDeepCluster knn-nearest neighbors\n28.90 42.25 56.44\n63.90\nDeepCluster linear classiﬁer\n29.44 39.79 56.19\n65.28\nDeepCluster MLP with dropout\n29.03 39.67 52.71\n60.95\nDeepCluster cluster matching\n22.20 23.50 24.97\n26.87\nDeepCluster CACTUs MAML\n39.90 53.97 63.84\n69.64\nDeepCluster CACTUs ProtoNets\n39.18 53.36 61.54\n63.55\nUMTRA without data Augmentation\n26.49\n-\n-\n-\nUMTRA+Shift+random ﬂip\n30.16\n-\n-\n-\nUMTRA+Shift+random ﬂip +randomly change to grayscale\n32.80\n-\n-\n-\nUMTRA+Shift+random ﬂip+random rotation+color distortions 35.09\n-\n-\n-\nUMTRA+AutoAugment\n39.93 50.73 61.11\n67.15\nAAL-MAML+++ CHV\n33.06 40.75\n-\n-\nAAL-MAML+++ CHVR\n33.21 40.34\n-\n-\nAAL-MAML+++ CHV + CUT\n33.34 39.44\n-\n-\nAAL-MAML+++ CHV + DROP\n30.86 40.41\n-\n-\nAAL-MAML+++ CHVW\n33.30 46.98\n-\n-\nAAL-MAML+++ CHVWG\n34.57 49.18\n-\n-\nAAL-MAML+++ CHVR + CUT\n33.09 40.11\n-\n-\nAAL-MAML+++ CHVR + DROP\n31.70 39.38\n-\n-\nAAL-MAML+++ CHV + DROP + CUT\n31.55 38.76\n-\n-\nAAL-MAML+++ CHVR + DROP + CUT\n31.44 39.87\n-\n-\nAAL-ProtoNets+ CHV\n37.67 40.29\n-\n-\nAAL-ProtoNets+ CHV + CUT\n36.38 40.89\n-\n-\nAAL-ProtoNets+ CHV + CUT + DROP\n33.13 36.64\n-\n-\nAAL-ProtoNets+ CHVR + CUT + DROP\n31.93 36.45\n-\n-\nAAL-ProtoNets+ CHVR + CUT\n33.92 39.87\n-\n-\nAAL-ProtoNets+ CHV + DROP\n32.12 36.12\n-\n-\nAAL-ProtoNets+ CHVR + DROP\n31.13 36.83\n-\n-\nAAL-ProtoNets+ CHVR\n34.28 39.83\n-\n-\nUFLST without data Augmentation\n33.77 45.03 53.35\n56.72\nMAML (Finn et al., 2017) (Supervised)\n46.81 62.13 71.03\n75.54\nProtoNets (Snell et al., 2017) (Supervised)\n46.56 62.29 70.05\n72.04\nTable 2: Performances of different unsupervised few-shot learning models on Mini-ImageNet under\ndifferent settings. The accuracy with std of our model is :33.77% ± 0.70%, 45.03% ± 0.73%,\n53.35% ± 0.59%, 56.72% ± 0.67% on 5-way 1-shot, 5-way 5-shot, 5-way 20-shot, 5-way 50-shot,\nrespectively.\n4.5\nRESULTS ON MINI-IMAGENET\nOverall, training a few shot learner on the Mini-ImageNet dataset under the unsupervised setting\nis quite tricky. All the three aforementioned approaches adopt domain speciﬁc knowledge and data\naugmentation tricks in their training. For example, UMTRA uses the statistical likelihood of picking\ndifferent classes for the training data of Ti in case of K = 1 and large number of classes, and\nan augmentation function T fors the validation data. CACTUs relies on an unsupervised feature\nlearning algorithm to provide a statistical likelihood of difference and sameness in the training and\nvalidation data of Ti. The choice of the right augmentation function for UMTRA and AAL, the right\nfeature embedding approach for CACTUs, and the other hyper-parameters have a strong impact on\nthe performance.\n8\nThe model architecture trained on the Mini-ImageNet dataset is exactly the same as on the Omniglot\ndataset, i.e., the 4-layer convnet described in Sec.4.2. We only report the results by training without\nany data augmentation. We achieve 33.77% and 44.03% under the 5-way 1-shot and 5-way 5-shot\nscenario respectively. Compare to the model training from scratch (25.17% under the 5-way 1-shot\nscenario), our model has a gain of 8.6%. The best 5-way 1-shot accuracy in the CACTUs model\nis 39.18%. However, comparing to the CACTUs model is unfair because they used the AlexNet or\nthe VGG16 to ﬁrst learn a very good feature embedder for downstream feature clustering process,\nwhile our model is only composed of a 4-layer convenet. Both of the best results in the UMTRA\nmodel and the ALL model are acquired by using fancy data augmentations, such as shifting, random\nﬂipping, color distortions, image-Warping and image-pixel dropout (see Khodadadeh et al. (2018);\nAntoniou & Storkey (2019) for more details) while we don’t use any data augmentation tricks here.\nIt is noteworthy that our model outperforms the UMTRA trained without any data augmentation to\na large extent (33.77% vs. 26.49%).\nCompared to the results on Omniglot and Market1501, the results on the Mini-ImageNet is not the\nstate-of-the-art. The underline reason may come from three aspects. (1) For a fair comparison to\nother unsupervised few-shot learning models, we use the 4-layer convnet. However, the in-class\nvariations of the Mini-ImageNet is very large, which is hard for such a small network to capture\nthe semantic meanings of images. (2) In unsupervised learning, it is hard to choose suitable hyper-\nparameters, such as the clustering frequency, DBSCAN-related parameters, and the learning rate. (3)\nThe ground truth for the class number of Mini-ImageNet is small 1(64 for training, 16 for validating\nand 20 for testing). But, for constructing episodic tasks, we prefer to over-segment the dataset, and\nthis over-segmentation tend to assign data belonging to the same class into different clusters, leading\nto a degenerate performance. Our model performs very well on Omniglot and Maket1501, which\nmay be attributed to that both datasets have large class numbers and the number of examples in each\nclass is small. This type of dataset is very suitable for constructing episodic tasks to learn a few-shot\nlearner. In our future work, we will explore more domain speciﬁc knowledge and data augmentation\nstrategies to improve the accuracy on the Mini-ImageNet dataset and extend our model to more\ndatasets.\n4.6\nRESULTS ON MARKET1501\nWe also applied our model to a real-world application on person Re-ID. In reality, labeled data is\nextremely lacking for person Re-ID, and unsupervised learning becomes crucial. Table 3 presents\nthe performances of our model on the benchmark datset Market1501. There is no reported unsu-\npervised few-shot learning result on this dataset in the literature.\nRahimpour & Qi (2018) report\nthe supervised results under the 100-way 1-shot scenario. To evaluate our model, we trained a su-\npervised model adapted from Xiong et al. (2018). We ﬁnd that the model performance using the\nhardtriplet loss is much better than that using the prototype loss. This is due to that large variations\nin the appearance and environment of detected pedestrians lead to that noisy samples may be chosen\nas the prototypes, which deteriorates learning; while the hardtriplet loss focuses on correcting highly\nnoisy examples that violate the margin and hence alleviates the problem. Overall, we observe that\nour model achieves encouraging performances compared to the supervised method, in particular,\nin the scenario of low-way classiﬁcation, which suggest that our model is feasible in practice for\nperson Re-ID when annotated labels are not unavailable.\n5-way\n10-way\n15-way\n20-way\n50-way\n100-way\nUFLST-Tripetloss\n72.8\n63.0\n56.2\n53.4\n42.5\n35.4\nUFLST-Prototypeloss\n88.3\n81.2\n75.8\n73.0\n62.5\n54.0\nUFLST-HardTripletloss\n91.4\n86.9\n81.6\n80.4\n70.1\n62.1\nOur supervised model\n96.8\n94.7\n92.5\n91.1\n83.7\n77.3\nARM (Rahimpour & Qi, 2018)\n-\n-\n-\n-\n-\n76.99\nTable 3: Performances of our model on Market1501 with different settings. The supervised model\nis adapted from Xiong et al. (2018). Only 1-shot learning is considered to mimic the typical single\nquery condition in person Re-ID applications.\n9\n4.7\nEFFECT OF THE SIZE OF THE UNLABELED DATASET\nWe also evaluate how our model relies on the number of unlabeled examples. Table 4 presents the\nresults on the Omniglot dataset with varying number of training examples. Overall, the model per-\nformance is improved when the number of training examples increases. Notably, by using only a\nquarter of the unlabeled data, our model already achieves performances comparable to other unsu-\npervised few-shot learning methods (comparing UFLST-300 with those in Table 1). This demon-\nstrates the feasibility of our model when the number of unlabeled examples is not large.\n5-way Acc.\n20-way Acc.\nNumber of Classes\n1-shot\n5-shot\n1-shot\n5-shot\nUFLST-200\n82.83\n92.97\n65.85\n83.73\nUFLST-300\n86.03\n95.05\n70.52\n87.60\nUFLST-400\n91.30\n97.27\n78.64\n92.50\nUFLST-500\n95.27\n98.86\n87.02\n96.05\nUFLST-1200\n97.03\n99.19\n91.28\n97.37\nTable 4: Performances of our model on Omniglot using different numbers of unlabeled training\nexamples. The hardtriplet loss is used.\n5\nDISCUSSION\nIn this study, we have proposed an unsupervised model UFLST for few-shot learning via self-\ntraining. The model consists of two processes, progressive clustering and episodic training, which\nare executed iteratively. Other unsupervised methods also consider the two processes, but they are\nperformed separately, in term of that unsupervised clustering for feature extraction is accomplished\nbefore applying episodic learning. This separation has a shortcoming, since there is no guarantee\nthat the extract features by unsupervised clustering are suitable for the followed few-shot learning.\nHere, our model carries out the two processes in an alternate manner, which allows them to facilitate\nwith each other, such that feature representation and model generalization are optimized concur-\nrently, and eventually it produces a high quality few-shot learner. To our knowledge, our work is\nthe ﬁrst one that integrates progressive clustering and episodic training for unsupervised few-shot\nlearning.\nOn the Omniglot dataset, our model outperforms other state-of-the-art unsupervised few-shot learn-\ning methods to large extend and approaches to the performances of supervised modes. On the\nMIni-ImageNey dataset, our model achieves comparable results with previous unsupervised few-\nshot learning models. On the Market1501 dataset, our model also achieves encouraging perfor-\nmances compared to a supervised method. The high effectiveness of our model makes us think\nabout why it works. Few-shot learning in essence is to extract good representations of data suitable\nfor prediction by using very few training examples. To resolve this challenge, the episodic learning\nparadigm aims to create a set of episodic few-shot learning scenarios having the same setting as the\ntesting environment, so that the model learns to extract good feature representations that are trans-\nferable to novel but related tasks. To this end, the real labels of data are helpful but not essential, and\nwe can construct pseudo-labeled examples to train the model. But crucially, as demonstrated by this\nstudy, the construction of pseudo-labeled examples must go along with the episodic training, so that\nthe extracted features of data really matches the few-shot learning task. Notably, this unsupervised\nand progressive way of learning agrees with the nature of human on few-shot learning.\nREFERENCES\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,\nBrendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient\ndescent. In Advances in neural information processing systems, pp. 3981–3989, 2016.\nAntreas Antoniou and Amos Storkey. Assume, augment and learn: Unsupervised few-shot meta-\nlearning via random labels and data augmentation. arXiv preprint arXiv:1902.09884, 2019.\n10\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-\npervised learning of visual features. In Proceedings of the European Conference on Computer\nVision (ECCV), pp. 132–149, 2018.\nVirginia R de Sa. Learning classiﬁcation with unlabeled data. In Advances in neural information\nprocessing systems, pp. 112–119, 1994.\nEmmanuel Dupoux. Cognitive science in the era of artiﬁcial intelligence: A roadmap for reverse-\nengineering the infant language-learner. Cognition, 173:43–59, 2018.\nMartin Ester, Hans-Peter Kriegel, J¨org Sander, Xiaowei Xu, et al. A density-based algorithm for\ndiscovering clusters in large spatial databases with noise. In Kdd, volume 96, pp. 226–231, 1996.\nHehe Fan, Liang Zheng, Chenggang Yan, and Yi Yang.\nUnsupervised person re-identiﬁcation:\nClustering and ﬁne-tuning. ACM Transactions on Multimedia Computing, Communications, and\nApplications (TOMM), 14(4):83, 2018.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pp. 1126–1135. JMLR. org, 2017.\nAlexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-\nidentiﬁcation. arXiv preprint arXiv:1703.07737, 2017.\nKyle Hsu, Sergey Levine, and Chelsea Finn.\nUnsupervised learning via meta-learning.\narXiv\npreprint arXiv:1810.02334, 2018.\nSiavash Khodadadeh, Ladislau B¨ol¨oni, and Mubarak Shah. Unsupervised meta-learning for few-\nshot image and video classiﬁcation. arXiv preprint arXiv:1811.11819, 2018.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning\nthrough probabilistic program induction. Science, 350(6266):1332–1338, 2015.\nYanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang.\nLearning to propagate labels: Transductive propagation network for few-shot learning. arXiv\npreprint arXiv:1805.10002, 2018.\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(Nov):2579–2605, 2008.\nDavid McClosky, Eugene Charniak, and Mark Johnson.\nEffective self-training for parsing.\nIn\nProceedings of the main conference on human language technology conference of the North\nAmerican Chapter of the Association of Computational Linguistics, pp. 152–159. Association\nfor Computational Linguistics, 2006.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-\nlearner. arXiv preprint arXiv:1707.03141, 2017.\nAlex Nichol and John Schulman.\nReptile: a scalable metalearning algorithm.\narXiv preprint\narXiv:1803.02999, 2, 2018.\nDanfeng Qin, Stephan Gammeter, Lukas Bossard, Till Quack, and Luc Van Gool. Hello neighbor:\nAccurate object retrieval with k-reciprocal nearest neighbors. In CVPR 2011, pp. 777–784. IEEE,\n2011.\nAlireza Rahimpour and Hairong Qi. Attention-based few-shot person re-identiﬁcation using meta\nlearning. CoRR, abs/1806.09613, 2018. URL http://arxiv.org/abs/1806.09613.\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.\nMengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,\nHugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classiﬁca-\ntion. arXiv preprint arXiv:1803.00676, 2018.\n11\nAndrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osin-\ndero, and Raia Hadsell.\nMeta-learning with latent embedding optimization.\narXiv preprint\narXiv:1807.05960, 2018.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems, pp. 4077–4087, 2017.\nLiangchen Song, Cheng Wang, Lefei Zhang, Bo Du, Qian Zhang, Chang Huang, and Xinggang\nWang.\nUnsupervised domain adaptive re-identiﬁcation: Theory and practice.\narXiv preprint\narXiv:1807.11334, 2018.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning.\nIn Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pp. 1199–1208, 2018.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. In Advances in neural information processing systems, pp. 3630–3638, 2016.\nRui Wang, Jun-Yun Zhang, Stanley A Klein, Dennis M Levi, and Cong Yu. Vernier perceptual\nlearning transfers to completely untrained retinal locations after double training: A piggybacking\neffect. Journal of Vision, 14(13):12–12, 2014.\nYu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from\nimaginary data.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 7278–7286, 2018.\nKilian Q Weinberger and Lawrence K Saul. Distance metric learning for large margin nearest neigh-\nbor classiﬁcation. Journal of Machine Learning Research, 10(Feb):207–244, 2009.\nJunyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.\nIn International conference on machine learning, pp. 478–487, 2016.\nFu Xiong, Yang Xiao, Zhiguo Cao, Kaicheng Gong, Zhiwen Fang, and Joey Tianyi Zhou. Towards\ngood practices on building effective cnn baseline model for person re-identiﬁcation. arXiv preprint\narXiv:1807.11042, 2018.\nLiang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person\nre-identiﬁcation: A benchmark. In Proceedings of the IEEE international conference on computer\nvision, pp. 1116–1124, 2015.\nZhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-ranking person re-identiﬁcation with\nk-reciprocal encoding. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 1318–1327, 2017.\n12\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2019-12-20",
  "updated": "2019-12-20"
}