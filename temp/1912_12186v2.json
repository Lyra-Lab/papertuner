{
  "id": "http://arxiv.org/abs/1912.12186v2",
  "title": "Unsupervised Representation Learning by Predicting Random Distances",
  "authors": [
    "Hu Wang",
    "Guansong Pang",
    "Chunhua Shen",
    "Congbo Ma"
  ],
  "abstract": "Deep neural networks have gained tremendous success in a broad range of\nmachine learning tasks due to its remarkable capability to learn semantic-rich\nfeatures from high-dimensional data. However, they often require large-scale\nlabelled data to successfully learn such features, which significantly hinders\ntheir adaption into unsupervised learning tasks, such as anomaly detection and\nclustering, and limits their applications into critical domains where obtaining\nmassive labelled data is prohibitively expensive. To enable unsupervised\nlearning on those domains, in this work we propose to learn features without\nusing any labelled data by training neural networks to predict data distances\nin a randomly projected space. Random mapping is a theoretically proven\napproach to obtain approximately preserved distances. To well predict these\nrandom distances, the representation learner is optimised to learn genuine\nclass structures that are implicitly embedded in the randomly projected space.\nEmpirical results on 19 real-world datasets show that our learned\nrepresentations substantially outperform a few state-of-the-art competing\nmethods in both anomaly detection and clustering tasks. Code is available at\nhttps://git.io/RDP",
  "text": "UNSUPERVISED REPRESENTATION LEARNING\nBY PREDICTING RANDOM DISTANCES\nHu Wang ∗\nGuansong Pang ∗\nChunhua Shen\nCongbo Ma\nAustralian Institute for Machine Learning\nThe University of Adelaide, Adelaide, Australia\nABSTRACT\nDeep neural networks have gained tremendous success in a broad range of ma-\nchine learning tasks due to its remarkable capability to learn semantic-rich features\nfrom high-dimensional data. However, they often require large-scale labelled data\nto successfully learn such features, which signiﬁcantly hinders their adaption into\nunsupervised learning tasks, such as anomaly detection and clustering, and limits\ntheir applications into critical domains where obtaining massive labelled data is\nprohibitively expensive. To enable unsupervised learning on those domains, in\nthis work we propose to learn features without using any labelled data by training\nneural networks to predict data distances in a randomly projected space. Random\nmapping is a theoretically proven approach to obtain approximately preserved\ndistances. To well predict these random distances, the representation learner is\noptimised to learn genuine class structures that are implicitly embedded in the\nrandomly projected space. Empirical results on 19 real-world datasets show that\nour learned representations substantially outperform a few state-of-the-art com-\npeting methods in both anomaly detection and clustering tasks. Code is available\nat: https://git.io/RDP\n1\nINTRODUCTION\nUnsupervised representation learning aims at automatically extracting expressive feature represen-\ntations from data without any manually labelled data. Due to the remarkable capability to learn\nsemantic-rich features, deep neural networks have been becoming one widely-used technique to em-\npower a broad range of machine learning tasks. One main issue with these deep learning techniques\nis that a massive amount of labelled data is typically required to successfully learn these expressive\nfeatures. As a result, their transformation power is largely reduced for tasks that are unsupervised\nin nature, such as anomaly detection and clustering. This is also true to critical domains, such as\nhealthcare and ﬁntech, where collecting massive labelled data is prohibitively expensive and/or is\nimpossible to scale. To bridge this gap, in this work we explore fully unsupervised representation\nlearning techniques to enable downstream unsupervised learning methods on those critical domains.\nIn recent years, many unsupervised representation learning methods (Mikolov et al., 2013a; Le &\nMikolov, 2014; Misra et al., 2016; Lee et al., 2017; Gidaris et al., 2018) have been introduced, of\nwhich most are self-supervised approaches that formulate the problem as an annotation free pretext\ntask. These methods explore easily accessible information, such as temporal or spatial neighbour-\nhood, to design a surrogate supervisory signal to empower the feature learning. These methods have\nachieved signiﬁcantly improved feature representations of text/image/video data, but they are often\ninapplicable to tabular data since it does not contain the required temporal or spatial supervisory in-\nformation. We therefore focus on unsupervised representation learning of high-dimensional tabular\ndata. Although many traditional approaches, such as random projection (Li et al., 2006), principal\ncomponent analysis (PCA) (Rahmani & Atia, 2017), manifold learning (Donoho & Grimes, 2003;\nHinton & Roweis, 2003) and autoencoder (Vincent et al., 2010), are readily available for handling\nthose data, many of them (Donoho & Grimes, 2003; Hinton & Roweis, 2003; Rahmani & Atia, 2017)\nare often too computationally costly to scale up to large or high-dimensional data. Approaches like\n∗Equal contribution.\n1\narXiv:1912.12186v2  [cs.CV]  19 Jul 2020\nrandom projection and autoencoder are very efﬁcient but they often fail to capture complex class\nstructures due to its underlying data assumption or weak supervisory signal.\nIn this paper, we introduce a Random Distance Prediction (RDP) model which trains neural networks\nto predict data distances in a randomly projected space. When the distance information captures in-\ntrinsic class structure in the data, the representation learner is optimised to learn the class structure\nto minimise the prediction error. Since distances are concentrated and become meaningless in high\ndimensional spaces (Beyer et al., 1999), we seek to obtain distances preserved in a projected space\nto be the supervisory signal. Random mapping is a highly efﬁcient yet theoretical proven approach\nto obtain such approximately preserved distances. Therefore, we leverage the distances in the ran-\ndomly projected space to learn the desired features. Intuitively, random mapping preserves rich local\nproximity information but may also keep misleading proximity when its underlying data distribu-\ntion assumption is inexact; by minimising the random distance prediction error, RDP essentially\nleverages the preserved data proximity and the power of neural networks to learn globally consis-\ntent proximity and rectify the inconsistent proximity information, resulting in a substantially better\nrepresentation space than the original space. We show this simple random distance prediction en-\nables us to achieve expressive representations with no manually labelled data. In addition, some\ntask-dependent auxiliary losses can be optionally added as a complementary supervisory source to\nthe random distance prediction, so as to learn the feature representations that are more tailored for a\nspeciﬁc downstream task. In summary, this paper makes the following three main contributions.\n• We propose a random distance prediction formulation, which is very simple yet offers\na highly effective supervisory signal for learning expressive feature representations that\noptimise the distance preserving in random projection. The learned features are sufﬁciently\ngeneric and work well in enabling different downstream learning tasks.\n• Our formulation is ﬂexible to incorporate task-dependent auxiliary losses that are comple-\nmentary to random distance prediction to further enhance the learned features, i.e., features\nthat are speciﬁcally optimised for a downstream task while at the same time preserving the\ngeneric proximity as much as possible.\n• As a result, we show that our instantiated model termed RDP enables substantially bet-\nter performance than state-of-the-art competing methods in two key unsupervised tasks,\nanomaly detection and clustering, on 19 real-world high-dimensional tabular datasets.\n2\nRANDOM DISTANCE PREDICTION MODEL\n2.1\nTHE PROPOSED FORMULATION AND THE INSTANTIATED MODEL\nWe propose to learn representations by training neural networks to predict distances in a randomly\nprojected space without manually labelled data. The key intuition is that, given some distance\ninformation that faithfully encapsulates the underlying class structure in the data, the representation\nlearner is forced to learn the class structure in order to yield distances that are as close as the given\ndistances. Our proposed framework is illustrated in Figure 1. Speciﬁcally, given data points xi, xj ∈\nRD, we ﬁrst feed them into a weight-shared Siamese-style neural network φ(x; Θ). φ : RD 7→RM\nis a representation learner with the parameters Θ to map the data onto a M-dimensional new space.\nThen we formulate the subsequent step as a distance prediction task and deﬁne a loss function as:\nLrdp(xi, xj) = l(⟨φ(xi; Θ), φ(xj; Θ)⟩, ⟨η(xi), η(xj)⟩),\n(1)\nwhere η is an existing projection method and l is a function of the difference between its two inputs.\nHere one key ingredient is how to obtain trustworthy distances via η. Also, to efﬁciently optimise the\nmodel, the distance derivation needs to be computationally efﬁcient. In this work, we use the inner\nproducts in a randomly projected space as the source of distance/similarity since it is very efﬁcient\nand there is strong theoretical support of its capacity in preserving the genuine distance information.\nThus, our instantiated model RDP speciﬁes Lrdp(xi, xj) as follows1:\nLrdp(xi, xj) = (φ(xi; Θ) · φ(xj; Θ) −η(xi) · η(xj))2 ,\n(2)\n1Since we operate on real-valued vector space, the inner product is implemented by the dot product. The\ndot product is used hereafter to simplify the notation.\n2\nLrdp\n𝜙𝒙𝒊; 𝛩\nNeural Network\n𝜙𝒙𝒋; 𝛩\nNeural Network\nShared weights\n𝑥𝑖1 𝑥𝑖2\n𝑥𝑖𝐷\n…\n𝒙𝒊\n𝑥𝑗1 𝑥𝑗2\n𝑥𝑗𝐷\n…\n𝒙𝒋\nLaux\nRandom Distance\n𝑙\n𝜙𝒙𝒊; 𝛩, 𝜙𝒙𝒋; 𝛩\n, 𝜂𝒙𝒊, 𝜂𝒙𝒋\nFigure 1: The proposed random distance prediction (RDP) framework. Speciﬁcally, a weight-shared\ntwo-branch neural network φ ﬁrst projects xi and xj onto a new space, in which we aim to min-\nimise the random distance prediction loss Lrdp, i.e., the difference between the learned distance\n⟨φ(xi; Θ), φ(xj; Θ)⟩and a predeﬁned distance ⟨η(xi), η(xj)⟩(η denotes an existing random map-\nping). Laux is an auxiliary loss that is optionally applied to one network branch to learn comple-\nmentary information w.r.t. Lrdp. The lower right ﬁgure presents a 2-D t-SNE (Hinton & Roweis,\n2003) visualisation of the features learned by RDP on a small toy dataset optdigits with 10 classes.\nwhere φ is implemented by multilayer perceptron for dealing with tabular data and η : RD 7→RK is\nan off-the-shelf random data mapping function (see Sections 3.1 and 3.2 for detail). Despite its sim-\nplicity, this loss offers a powerful supervisory signal to learn semantic-rich feature representations\nthat substantially optimise the underlying distance preserving in η (see Section 3.3 for detail).\n2.2\nFLEXIBILITY TO INCORPORATE TASK-DEPENDENT COMPLEMENTARY AUXILIARY LOSS\nMinimising Lrdp learns to preserve pairwise distances that are critical to different learning tasks.\nMoreover, our formulation is ﬂexible to incorporate a task-dependent auxiliary loss Laux, such as\nreconstruction loss (Hinton & Salakhutdinov, 2006) for clustering or novelty loss (Burda et al., 2019)\nfor anomaly detection, to complement the proximity information and enhance the feature learning.\nFor clustering, an auxiliary reconstruction loss is deﬁned as:\nLclu\naux(x) = (x −φ′(φ(x; Θ); Θ′))2 ,\n(3)\nwhere φ is an encoder and φ′ : RM 7→RD is a decoder. This loss may be optionally added into\nRDP to better capture global feature representations.\nSimilarly, in anomaly detection a novelty loss may be optionally added, which is deﬁned as:\nLad\naux(x) = (φ(x; Θ) −η(x))2 .\n(4)\nBy using a ﬁxed η, minimising Lad\naux helps learn the frequency of underlying patterns in the data\n(Burda et al., 2019), which is an important complementary supervisory source for the sake of\nanomaly detection. As a result, anomalies or novel points are expected to have substantially larger\n(φ(x; Θ⋆) −η(x))2 than normal points, so this value can be directly leveraged to detect anomalies.\nNote since Lad\naux involves a mean squared error between two vectors, the dimension of the projected\nspace resulted by φ and η is required to be equal in this case. Therefore, when this loss is added into\nRDP, the M in φ and K in η need to be the same. We do not have this constraint in other cases.\n3\nTHEORETICAL ANALYSIS OF RDP\nThis section shows the proximity information can be well approximated using inner products in two\ntypes of random projection spaces. This is a key theoretical foundation to RDP. Also, to accurately\npredict these distances, RDP is forced to learn the genuine class structure in the data.\n3\n3.1\nWHEN LINEAR PROJECTION IS USED\nRandom projection is a simple yet very effective linear feature mapping technique which has proven\nthe capability of distance preservation. Let X ⊂RN×D be a set of N data points, random projection\nuses a random matrix A ⊂RK×D to project the data onto a lower K-dimensional space by X ′ =\nAX ⊺. The Johnson-Lindenstrauss lemma (Johnson & Lindenstrauss, 1984) guarantees the data\npoints can be mapped to a randomly selected space of suitably lower dimension with the distances\nbetween the points are approximately preserved. More speciﬁcally, let ϵ ∈(0, 1\n2) and K = 20 log n\nϵ2\n.\nThere exists a linear mapping f : RD 7→RK such that for all xi, xj ∈X:\n(1 −ϵ)||xi −xj||2 ≤||f(xi) −f(xj)||2 ≤(1 + ϵ)||xi −xj||2.\n(5)\nFurthermore, assume the entries of the matrix A are sampled independently from a Gaussian distri-\nbution N(0, 1). Then, the norm of x ∈RD can be preserved as:\nPr\n\u0012\n(1 −ϵ)||x||2 ≤|| 1\n√\nK\nAx||2 ≤(1 + ϵ)||x||2\n\u0013\n≥1 −2e\n−(ϵ2−ϵ3)K\n4\n.\n(6)\nUnder such random projections, the norm preservation helps well preserve the inner products:\nPr (|ˆxi · ˆxj −f(ˆxi) · f(ˆxj)| ≥ϵ) ≤4e\n−(ϵ2−ϵ3)K\n4\n,\n(7)\nwhere ˆx is a normalised x such that ||ˆx|| ≤1.\nThe proofs of Eqns. (5), (6) and (7) can be found in (Vempala, 1998).\nEqn. (7) states that the inner products in the randomly projected space can largely preserve the inner\nproducts in the original space, particularly when the projected dimension K is large.\n3.2\nWHEN NON-LINEAR PROJECTION IS USED\nHere we show that some non-linear random mapping methods are approximate to kernel functions\nwhich are a well-established approach to obtain reliable distance/similarity information. The key to\nthis approach is the kernel function k : X ×X 7→R, which is deﬁned as k(xi, xj) = ⟨ψ(xi), ψ(xj)⟩,\nwhere ψ is a feature mapping function but needs not to be explicitly deﬁned and ⟨·, ·⟩denotes a\nsuitable inner product. A non-linear kernel function such as polynomial or radial basis function\n(RBF) kernel is typically used to project linear-inseparable data onto a linear-separable space.\nThe relation between non-linear random mapping and kernel methods is justiﬁed in (Rahimi &\nRecht, 2008), which shows that an explicit randomised mapping function g : RD 7→RK can be\ndeﬁned to project the data points onto a low-dimensional Euclidean inner product space such that\nthe inner products in the projected space approximate the kernel evaluation:\nk(xi, xj) = ⟨ψ(xi), ψ(xj)⟩≈g(xi) · g(xj).\n(8)\nLet A be the mapping matrix. Then to achieve the above approximation, A is required to be drawn\nfrom Fourier transform and shift-invariant functions such as cosine function are ﬁnally applied to\nAx to yield a real-valued output. By transforming the two data points xi and xj in this manner,\ntheir inner product g(xi) · g(xj) is an unbiased estimator of k(xi, xj).\n3.3\nLEARNING CLASS STRUCTURE BY RANDOM DISTANCE PREDICTION\nOur model using only the random distances as the supervisory signal can be formulated as:\narg min\nΘ\nX\nxi,xj∈X\n(φ(xi; Θ) · φ(xj; Θ) −yij)2 ,\n(9)\nwhere yij = η(xi) · η(xj). Let Yη ∈RN×N be the distance/similarity matrix of the N data\npoints resulted by η. Then to minimise the prediction error in Eqn. (9), φ is optimised to learn the\nunderlying class structure embedded in Y. As shown in the properties in Eqns. (7) and (8), Yη can\neffectively preserve local proximity information when η is set to be either the random projection-\nbased f function or the kernel method-based g function. However, those proven η is often built\n4\nupon some underlying data distribution assumption, e.g., Gaussian distribution in random projection\nor Gaussian RBF kernel, so the η-projected features can preserve misleading proximity when the\ndistribution assumption is inexact. In this case, Yη is equivalent to the imperfect ground truth with\npartial noise. Then optimisation with Eqn. (9) is to leverage the power of neural networks to learn\nconsistent local proximity information and rectify inconsistent proximity, resulting in a signiﬁcantly\noptimised distance preserving space. The resulting space conveys substantially richer semantics\nthan the η projected space when Yη contains sufﬁcient genuine supervision information.\n4\nEXPERIMENTS\nThis section evaluates the learned representations through two typical unsupervised tasks: anomaly\ndetection and clustering. Some preliminary results of classiﬁcation can be found in Appendix H.\n4.1\nPERFORMANCE EVALUATION IN ANOMALY DETECTION\n4.1.1\nEXPERIMENTAL SETTINGS\nOur RDP model is compared with ﬁve state-of-the-art methods, including iForest (Liu et al., 2008),\nautoencoder (AE) (Hinton & Salakhutdinov, 2006), REPEN (Pang et al., 2018), DAGMM (Zong\net al., 2018) and RND (Burda et al., 2019). iForest and AE are two of the most popular baselines.\nThe other three methods learn representations speciﬁcally for anomaly detection.\nAs shown in Table 1, the comparison is performed on 14 publicly available datasets of various\ndomains, including network intrusion, credit card fraud detection, disease detection and bank cam-\npaigning. Many of the datasets contain real anomalies, including DDoS, Donors, Backdoor, Cred-\nitcard, Lung, Probe and U2R. Following (Liu et al., 2008; Pang et al., 2018; Zong et al., 2018), the\nrare class(es) is treated as anomalies in the other datasets to create semantically real anomalies. The\nArea Under Receiver Operating Characteristic Curve (AUC-ROC) and the Area Under Precision-\nRecall Curve (AUC-PR) are used as our performance metrics. Larger AUC-ROC/AUC-PR indicates\nbetter performance. The reported performance is averaged over 10 independent runs.\nTable 1: AUC-ROC (mean±std) performance of RDP and its ﬁve competing methods on 14 datasets.\nData Characteristics\nOur Method RDP and Its Five Competing Methods\nData\nN\nD\nAnomaly (%)\niForest\nAE\nREPEN\nDAGMM\nRND\nRDP\nDDoS\n464,976\n66\n3.75%\n0.880 ± 0.018 0.901 ± 0.000 0.933 ± 0.002 0.766 ± 0.019 0.852 ± 0.011 0.942 ± 0.008\nDonors\n619,326\n10\n5.92%\n0.774 ± 0.010 0.812 ± 0.011 0.777 ± 0.075 0.763 ± 0.110 0.847 ± 0.011 0.962 ± 0.011\nBackdoor\n95,329\n196\n2.44%\n0.723 ± 0.029 0.806 ± 0.007 0.857 ± 0.001 0.813 ± 0.035 0.935 ± 0.002 0.910 ± 0.021\nAd\n3,279\n1,555\n13.99%\n0.687 ± 0.021 0.703 ± 0.000 0.853 ± 0.001 0.500 ± 0.000 0.812 ± 0.002 0.887 ± 0.003\nApascal\n12,695\n64\n1.38%\n0.514 ± 0.051 0.623 ± 0.005 0.813 ± 0.004 0.710 ± 0.020 0.685 ± 0.019 0.823 ± 0.007\nBank\n41,188\n62\n11.26%\n0.713 ± 0.021 0.666 ± 0.000 0.681 ± 0.001 0.616 ± 0.014 0.690 ± 0.006 0.758 ± 0.007\nCeleba\n202,599\n39\n2.24%\n0.693 ± 0.014 0.735 ± 0.002 0.802 ± 0.002 0.680 ± 0.067 0.682 ± 0.029 0.860 ± 0.006\nCensus\n299,285\n500\n6.20%\n0.599 ± 0.019 0.602 ± 0.000 0.542 ± 0.003 0.502 ± 0.003 0.661 ± 0.003 0.653 ± 0.004\nCreditcard 284,807\n29\n0.17%\n0.948 ± 0.005 0.948 ± 0.000 0.950 ± 0.001 0.877 ± 0.005 0.945 ± 0.001 0.957 ± 0.005\nLung\n145\n3,312\n4.13%\n0.893 ± 0.057 0.953 ± 0.004 0.949 ± 0.002 0.830 ± 0.087 0.867 ± 0.031 0.982 ± 0.006\nProbe\n64,759\n34\n6.43%\n0.995 ± 0.001 0.997 ± 0.000 0.997 ± 0.000 0.953 ± 0.008 0.975 ± 0.000 0.997 ± 0.000\nR8\n3,974\n9,467\n1.28%\n0.841 ± 0.023 0.835 ± 0.000 0.910 ± 0.000 0.760 ± 0.066 0.883 ± 0.006 0.902 ± 0.002\nSecom\n1,567\n590\n6.63%\n0.548 ± 0.019 0.526 ± 0.000 0.510 ± 0.004 0.513 ± 0.010 0.541 ± 0.006 0.570 ± 0.004\nU2R\n60,821\n34\n0.37%\n0.988 ± 0.001 0.987 ± 0.000 0.978 ± 0.000 0.945 ± 0.028 0.981 ± 0.001 0.986 ± 0.001\nOur RDP model uses the optional novelty loss for anomaly detection task by default. Similar to\nRND, given a data point x, its anomaly score in RDP is deﬁned as the mean squared error between\nthe two projections resulted by φ(x; Θ⋆) and η(x). Also, a boosting process is used to ﬁlter out\n5% likely anomalies per iteration to iteratively improve the modelling of RDP. This is because the\nmodelling is otherwise largely biased when anomalies are presented. In the ablation study in Section\n4.1.3, we will show the contribution of all these components.\n4.1.2\nCOMPARISON TO THE STATE-OF-THE-ART COMPETING METHODS\nThe AUC-ROC and AUC-PR results are respectively shown in Tables 1 and 2. RDP outperforms all\nthe ﬁve competing methods in both of AUC-ROC and AUC-PR in at least 12 out of 14 datasets. This\nimprovement is statistically signiﬁcant at the 95% conﬁdence level according to the two-tailed sign\n5\ntest (Demˇsar, 2006). Remarkably, RDP obtains more than 10% AUC-ROC/AUC-PR improvement\nover the best competing method on six datasets, including Donors, Ad, Bank, Celeba, Lung and\nU2R. RDP can be thought as a high-level synthesis of REPEN and RND, because REPEN leverages\na pairwise distance-based ranking loss to learn representations for anomaly detection while RND\nis built using Lad\naux. In nearly all the datasets, RDP well leverages both Lrdp and Lad\naux to achieve\nsigniﬁcant improvement over both REPEN and RND. In very limited cases, such as on datasets\nBackdoor and Census where RND performs very well while REPEN performs less effectively, RDP\nis slightly downgraded due to the use of Lrdp. In the opposite case, such as Probe, on which REPEN\nperforms much better than RND, the use of Lad\naux may drag down the performance of RDP a bit.\nTable 2: AUC-PR (mean±std) performance of RDP and its ﬁve competing methods on 14 datasets.\nData\niForest\nAE\nREPEN\nDAGMM\nRND\nRDP\nDDoS\n0.141 ± 0.020 0.248 ± 0.001 0.300 ± 0.012 0.038 ± 0.000 0.110 ± 0.015 0.301 ± 0.028\nDonors\n0.124 ± 0.006 0.138 ± 0.007 0.120 ± 0.032 0.070 ± 0.024 0.201 ± 0.033 0.432 ± 0.061\nBackdoor\n0.045 ± 0.007 0.065 ± 0.004 0.129 ± 0.001 0.034 ± 0.023 0.433 ± 0.015 0.305 ± 0.008\nAd\n0.363 ± 0.061 0.479 ± 0.000 0.600 ± 0.002 0.140 ± 0.000 0.473 ± 0.009 0.726 ± 0.007\nApascal\n0.015 ± 0.002 0.023 ± 0.001 0.041 ± 0.001 0.023 ± 0.009 0.021 ± 0.005 0.042 ± 0.003\nBank\n0.293 ± 0.023 0.264 ± 0.001 0.276 ± 0.001 0.150 ± 0.020 0.258 ± 0.006 0.364 ± 0.013\nCeleba\n0.060 ± 0.006 0.082 ± 0.001 0.081 ± 0.001 0.037 ± 0.017 0.068 ± 0.010 0.104 ± 0.006\nCensus\n0.071 ± 0.004 0.072 ± 0.000 0.064 ± 0.005 0.061 ± 0.001 0.081 ± 0.001 0.086 ± 0.001\nCreditcard 0.145 ± 0.031 0.382 ± 0.004 0.359 ± 0.014 0.010 ± 0.012 0.290 ± 0.012 0.363 ± 0.011\nLung\n0.379 ± 0.092 0.565 ± 0.022 0.429 ± 0.005 0.042 ± 0.003 0.381 ± 0.104 0.705 ± 0.028\nProbe\n0.923 ± 0.011 0.964 ± 0.002 0.964 ± 0.000 0.409 ± 0.153 0.609 ± 0.014 0.955 ± 0.002\nR8\n0.076 ± 0.018 0.097 ± 0.006 0.083 ± 0.000 0.019 ± 0.011 0.134 ± 0.031 0.146 ± 0.017\nSecom\n0.106 ± 0.007 0.093 ± 0.000 0.091 ± 0.001 0.066 ± 0.002 0.086 ± 0.002 0.096 ± 0.001\nU2R\n0.180 ± 0.018 0.230 ± 0.004 0.116 ± 0.007 0.025 ± 0.019 0.217 ± 0.011 0.261 ± 0.005\n4.1.3\nABLATION STUDY\nThis section examines the contribution of Lrdp, Lad\naux and the boosting process to the performance\nof RDP. The experimental results in AUC-ROC are given in Table 3, where RDP\\X means the RDP\nvariant that removes the ‘X’ module from RDP. In the last two columns, Org SS indicates that we\ndirectly use the distance information calculated in the original space as the supervisory signal, while\nSRP SS indicates that we use SRP to obtain the distances as the supervisory signal. It is clear that\nthe full RDP model is the best performer. Using the Lrdp loss only, i.e., RDP\\Lad\naux, can achieve\nperformance substantially better than, or comparably well to, the ﬁve competing methods in Table 1.\nThis is mainly because the Lrdp loss alone can effectively force our representation learner to learn\nthe underlying class structure on most datasets so as to minimise its prediction error. The use of\nLad\naux and boosting process well complement the Lrdp loss on the other datasets.\nIn terms of supervisory source, RDP and SRP SS perform substantially better than Org SS on most\ndatasets. This is because the distances in both the non-linear random projection in RDP and the\nlinear projection in SRP SS well preserve the distance information, enabling RDP to effectively\nlearn much more faithful class structure than that working on the original space.\nTable 3: AUC-ROC results of anomaly detection (see Appendix C for similar AUC-PR results).\nDecomposition\nSupervision Signal\nData\nRDP\nRDP\\Lrdp\nRDP\\Lad\naux\nRDP\\Boosting\nOrg SS\nSRP SS\nDDoS\n0.942 ± 0.008\n0.852 ± 0.011 0.931 ± 0.003\n0.866 ± 0.011\n0.924 ± 0.006 0.927 ± 0.005\nDonors\n0.962 ± 0.011\n0.847 ± 0.011 0.737 ± 0.006\n0.910 ± 0.013\n0.728 ± 0.005 0.762 ± 0.016\nBackdoor\n0.910 ± 0.021\n0.935 ± 0.002 0.872 ± 0.012\n0.943 ± 0.002\n0.875 ± 0.002 0.882 ± 0.010\nAd\n0.887 ± 0.003\n0.812 ± 0.002 0.718 ± 0.005\n0.818 ± 0.002\n0.696 ± 0.003 0.740 ± 0.008\nApascal\n0.823 ± 0.007\n0.685 ± 0.019 0.732 ± 0.007\n0.804 ± 0.021\n0.604 ± 0.032 0.760 ± 0.030\nBank\n0.758 ± 0.007\n0.690 ± 0.006 0.684 ± 0.004\n0.736 ± 0.009\n0.684 ± 0.002 0.688 ± 0.015\nCeleba\n0.860 ± 0.006\n0.682 ± 0.029 0.709 ± 0.005\n0.794 ± 0.017\n0.667 ± 0.033 0.734 ± 0.027\nCensus\n0.653 ± 0.004\n0.661 ± 0.003 0.626 ± 0.006\n0.661 ± 0.001\n0.636 ± 0.006 0.560 ± 0.006\nCreditcard\n0.957 ± 0.005\n0.945 ± 0.001 0.950 ± 0.000\n0.956 ± 0.003\n0.947 ± 0.001 0.949 ± 0.003\nLung\n0.982 ± 0.006\n0.867 ± 0.031 0.911 ± 0.006\n0.968 ± 0.018\n0.884 ± 0.018 0.928 ± 0.008\nProbe\n0.997 ± 0.000\n0.975 ± 0.000 0.998 ± 0.000\n0.978 ± 0.001\n0.995 ± 0.000 0.997 ± 0.001\nR8\n0.902 ± 0.002\n0.883 ± 0.006 0.867 ± 0.003\n0.895 ± 0.004\n0.830 ± 0.005 0.904 ± 0.005\nSecom\n0.57 ± 0.004\n0.541 ± 0.006 0.544 ± 0.011\n0.563 ± 0.008\n0.512 ± 0.007 0.530 ± 0.016\nU2R\n0.986 ± 0.001\n0.981 ± 0.001 0.987 ± 0.000\n0.988 ± 0.002\n0.987 ± 0.000 0.981 ± 0.002\n#wins/draws/losses (RDP vs.)\n13/0/1\n13/0/1\n12/0/2\n10/2/2\n6/0/8\n6\n4.2\nPERFORMANCE EVALUATION IN CLUSTERING\n4.2.1\nEXPERIMENTAL SETTINGS\nFor clustering, RDP is compared with four state-of-the-art unsupervised representation learning\nmethods in four different areas, including HLLE (Donoho & Grimes, 2003) in manifold learning,\nSparse Random Projection (SRP) (Li et al., 2006) in random projection, autoencoder (AE) (Hinton &\nSalakhutdinov, 2006) in data reconstruction-based neural network methods and Coherence Pursuit\n(COP) (Rahmani & Atia, 2017) in robust PCA. These representation learning methods are ﬁrst\nused to yield the new representations, and K-means (Hartigan & Wong, 1979) is then applied to the\nrepresentations to perform clustering. Two widely-used clustering performance metrics, Normalised\nMutual Info (NMI) score and F-score, are used. Larger NMI or F-score indicates better performance.\nThe clustering performance in the original feature space, denoted as Org, is used as a baseline. As\nshown in Table 4, ﬁve high-dimensional real-world datasets are used. Some of the datasets are\nimage/text data. Since here we focus on the performance on tabular data, they are converted into\ntabular data using simple methods, i.e., by treating each pixel as a feature unit for image data or\nusing bag-of-words representation for text data2. The reported NMI score and F-score are averaged\nover 30 times to address the randomisation issue in K-means clustering. In this section RDP adds\nthe reconstruction loss Lclu\naux by default, but RDP also works very well without the use of Lclu\naux.\n4.2.2\nCOMPARISON TO THE-STATE-OF-THE-ART COMPETING METHODS\nTable 4 shows the NMI and F-score performance of K-means clustering. Our method RDP en-\nables K-means to achieve the best performance on three datasets and ranks second in the other two\ndatasets. RDP-enabled clustering performs substantially and consistently better than that based on\nAE in terms of both NMI and F-score. This demonstrates that the random distance loss enables\nRDP to effectively capture some class structure in the data which cannot be captured by using the\nreconstruction loss. RDP also consistently outperforms the random projection method, SRP, and\nthe robust PCA method, COP. It is interesting that K-means clustering performs best in the original\nspace on Sector. This may be due to that this data contains many relevant features, resulting in no\nobvious curse of dimensionality issue. Olivetti may contain complex manifolds which require ex-\ntensive neighbourhood information to ﬁnd them, so only HLLE can achieve this goal in such cases.\nNevertheless, RDP performs much more stably than HLLE across the ﬁve datasets.\nTable 4: NMI and F-score performance of K-means on the original space and projected spaces.\nData Characteristics\nNMI Performance\nData\nN\nD\nOrg\nHLLE\nSRP\nAE\nCOP\nRDP\nR8\n7,674\n17,387\n0.524 ± 0.047 0.004 ± 0.001 0.459 ± 0.031 0.471 ± 0.043 0.025 ± 0.003 0.539 ± 0.040\n20news 18,846 130,107 0.080 ± 0.004 0.017 ± 0.000 0.075 ± 0.002 0.075 ± 0.006 0.027 ± 0.040 0.084 ± 0.005\nOlivetti\n400\n4,096\n0.778 ± 0.014 0.841 ± 0.011 0.774 ± 0.011 0.782 ± 0.010 0.333 ± 0.018 0.805 ± 0.012\nSector\n9,619\n55,197\n0.336 ± 0.008 0.122 ± 0.004 0.273 ± 0.011 0.253 ± 0.010 0.129 ± 0.014 0.305 ± 0.007\nRCV1\n20,242\n47,236\n0.154 ± 0.000 0.006 ± 0.000 0.134 ± 0.024 0.146 ± 0.010\nN/A\n0.165 ± 0.000\nData Characteristics\nF-score Performance\nData\nN\nD\nOrg\nHLLE\nSRP\nAE\nCOP\nRDP\nR8\n7,674\n17,387\n0.185 ± 0.189 0.085 ± 0.000 0.317 ± 0.045 0.312 ± 0.068 0.088 ± 0.002 0.360 ± 0.055\n20news 18,846 130,107 0.116 ± 0.006 0.007 ± 0.000 0.109 ± 0.006 0.083 ± 0.010 0.009 ± 0.004 0.119 ± 0.006\nOlivetti\n400\n4,096\n0.590 ± 0.029 0.684 ± 0.024 0.579 ± 0.022 0.602 ± 0.023 0.117 ± 0.011 0.638 ± 0.026\nSector\n9,619\n55,197\n0.208 ± 0.008 0.062 ± 0.001 0.187 ± 0.009 0.184 ± 0.010 0.041 ± 0.004 0.191 ± 0.007\nRCV1\n20,242\n47,236\n0.519 ± 0.000 0.342 ± 0.000 0.508 ± 0.003 0.514 ± 0.057\nN/A\n0.572 ± 0.003\nTable 5: F-score performance of K-means clustering (see similar NMI results in Appendix D).\nDecomposition\nSupervision Signal\nData\nRDP\nRDP\\Lrdp\nRDP\\Lclu\naux\nOrg SS\nSRP SS\nR8\n0.360 ± 0.055\n0.312 ± 0.068\n0.330 ± 0.052\n0.359 ± 0.028\n0.363 ± 0.046\n20news\n0.119 ± 0.006\n0.083 ± 0.010\n0.117 ± 0.005\n0.111 ± 0.005\n0.111 ± 0.007\nOlivetti\n0.638 ± 0.026\n0.602 ± 0.023\n0.597 ± 0.019\n0.610 ± 0.022\n0.601 ± 0.023\nSector\n0.191 ± 0.007\n0.184 ± 0.010\n0.217 ± 0.007\n0.181 ± 0.007\n0.186 ± 0.009\nRCV1\n0.572 ± 0.003\n0.514 ± 0.057\n0.526 ± 0.011\n0.523 ± 0.003\n0.532 ± 0.001\n2RDP can also build upon advanced representation learning methods for the data transformation, for which\nsome interesting preliminary results are presented in Appendix G.\n7\n4.2.3\nABLATION STUDY\nSimilar to anomaly detection, this section examines the contribution of the two loss functions Lrdp\nand Lclu\naux to the performance of RDP, as well as the impact of different supervisory sources on the\nperformance. The F-score results of this experiment are shown in Table 5, in which the notations\nhave exactly the same meaning as in Table 3. The full RDP model that uses both Lrdp and Lclu\naux\nperforms more favourably than its two variants, RDP\\Lrdp and RDP\\Lclu\naux, but it is clear that using\nLrdp only performs very comparably to the full RDP. However, using Lclu\naux only may result in large\nperformance drops in some datasets, such as R8, 20news and Olivetti. This indicates Lrdp is a more\nimportant loss function to the overall performance of the full RDP model. In terms of supervisory\nsource, distances obtained by the non-linear random projection in RDP are much more effective\nthan the two other sources on some datasets such as Olivetti and RCV1. Three different supervisory\nsources are very comparable on the other three datasets.\n5\nRELATED WORK\nSelf-supervised Learning. Self-supervised learning has been recently emerging as one of the most\npopular and effective approaches for representation learning. Many of the self-supervised methods\nlearn high-level representations by predicting some sort of ‘context’ information, such as spatial or\ntemporal neighbourhood information. For example, the popular distributed representation learning\ntechniques in NLP, such as CBOW/skip-gram (Mikolov et al., 2013a) and phrase/sentence embed-\ndings in (Mikolov et al., 2013b; Le & Mikolov, 2014; Hill et al., 2016), learn the representations\nby predicting the text pieces (e.g., words/phrases/sentences) using its surrounding pieces as the con-\ntext. In image processing, the pretext task can be the prediction of a patch of missing pixels (Pathak\net al., 2016; Zhang et al., 2017) or the relative position of two patches (Doersch et al., 2015). Also,\na number of studies (Goroshin et al., 2015; Misra et al., 2016; Lee et al., 2017; Oord et al., 2018)\nexplore temporal contexts to learn representations from video data, e.g., by learning the temporal\norder of sequential frames. Some other methods (Agrawal et al., 2015; Zhou et al., 2017; Gidaris\net al., 2018) are built upon a discriminative framework which aims at discriminating the images be-\nfore and after some transformation, e.g., ego motion in video data (Agrawal et al., 2015; Zhou et al.,\n2017) and rotation of images (Gidaris et al., 2018). There have also been popular to use generative\nadversarial networks (GANs) to learn features (Radford et al., 2015; Chen et al., 2016). The above\nmethods have demonstrated powerful capability to learn semantic representations. However, most\nof them use the supervisory signals available in image/video data only, which limits their application\ninto other types of data, such as traditional tabular data. Although our method may also work on\nimage/video data, we focus on handling high-dimensional tabular data to bridge this gap.\nOther Approaches. There have been several well-established unsupervised representation learn-\ning approaches for handling tabular data, such as random projection (Arriaga & Vempala, 1999;\nBingham & Mannila, 2001; Li et al., 2006), PCA (Wold et al., 1987; Sch¨olkopf et al., 1997; Rah-\nmani & Atia, 2017), manifold learning (Roweis & Saul, 2000; Donoho & Grimes, 2003; Hinton &\nRoweis, 2003; McInnes et al., 2018) and autoencoder (Hinton & Salakhutdinov, 2006; Vincent et al.,\n2010). One notorious issue of PCA or manifold learning approaches is their prohibitive computa-\ntional cost in dealing with large-scale high-dimensional data due to the costly neighbourhood search\nand/or eigen decomposition. Random projection is a computationally efﬁcient approach, supported\nby proven distance preservation theories such as the Johnson-Lindenstrauss lemma (Johnson & Lin-\ndenstrauss, 1984). We show that the preserved distances by random projection can be harvested to\neffectively supervise the representation learning. Autoencoder networks are another widely-used\nefﬁcient feature learning approach which learns low-dimensional representations by minimising re-\nconstruction errors. One main issue with autoencoders is that they focus on preserving global infor-\nmation only, which may result in loss of local structure information. Some representation learning\nmethods are speciﬁcally designed for anomaly detection (Pang et al., 2018; Zong et al., 2018; Burda\net al., 2019). By contrast, we aim at generic representations learning while being ﬂexible to incor-\nporate optionally task-dependent losses to learn task-speciﬁc semantic-rich representations.\n8\n6\nCONCLUSION\nWe introduce a novel Random Distance Prediction (RDP) model which learns features in a fully\nunsupervised fashion by predicting data distances in a randomly projected space. The key insight is\nthat random mapping is a theoretical proven approach to obtain approximately preserved distances,\nand to well predict these random distances, the representation learner is optimised to learn consistent\npreserved proximity information while at the same time rectifying inconsistent proximity, resulting\nin representations with optimised distance preserving. Our idea is justiﬁed by thorough experiments\nin two unsupervised tasks, anomaly detection and clustering, which show RDP-enabled anomaly\ndetectors and clustering substantially outperform their counterparts on 19 real-world datasets. We\nplan to extend RDP to other types of data to broaden its application scenarios.\nREFERENCES\nPulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In Proceedings of\nthe IEEE International Conference on Computer Vision, pp. 37–45, 2015.\nAkiko Aizawa. An information-theoretic perspective of tf–idf measures. Information Processing &\nManagement, 39(1):45–65, 2003.\nRosa I Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and\nrandom projection. In 40th Annual Symposium on Foundations of Computer Science, pp. 616–\n623. IEEE, 1999.\nKevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. When is nearest neighbor\nmeaningful? In International Conference on Database Theory, pp. 217–235. Springer, 1999.\nElla Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to\nimage and text data. In Proceedings of the seventh ACM SIGKDD international conference on\nKnowledge discovery and data mining, pp. 245–250. ACM, 2001.\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network\ndistillation. In ICLR, 2019.\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:\nInterpretable representation learning by information maximizing generative adversarial nets. In\nAdvances in neural information processing systems, pp. 2172–2180, 2016.\nJanez Demˇsar. Statistical comparisons of classiﬁers over multiple data sets. Journal of Machine\nlearning research, 7(Jan):1–30, 2006.\nCarl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by\ncontext prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n1422–1430, 2015.\nDavid L Donoho and Carrie Grimes. Hessian eigenmaps: Locally linear embedding techniques for\nhigh-dimensional data. Proceedings of the National Academy of Sciences, 100(10):5591–5596,\n2003.\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by\npredicting image rotations. In ICLR, 2018.\nRoss Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, and Yann LeCun. Unsupervised learn-\ning of spatiotemporally coherent metrics. In Proceedings of the IEEE international conference on\ncomputer vision, pp. 4086–4093, 2015.\nJohn A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Journal\nof the Royal Statistical Society. Series C (Applied Statistics), 28(1):100–108, 1979.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\nfrom unlabelled data. In 15th Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 1367–1377, 2016.\n9\nGeoffrey E Hinton and Sam T Roweis. Stochastic neighbor embedding. In Advances in neural\ninformation processing systems, pp. 857–864, 2003.\nGeoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural\nnetworks. Science, 313(5786):504–507, 2006.\nWilliam B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.\nContemporary mathematics, 26(189-206):1, 1984.\nQuoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Interna-\ntional conference on machine learning, pp. 1188–1196, 2014.\nHsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Unsupervised represen-\ntation learning by sorting sequences. In Proceedings of the IEEE International Conference on\nComputer Vision, pp. 667–676, 2017.\nPing Li, Trevor J Hastie, and Kenneth W Church. Very sparse random projections. In Proceedings\nof the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,\npp. 287–296. ACM, 2006.\nFei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 Eighth IEEE Interna-\ntional Conference on Data Mining, pp. 413–422. IEEE, 2008.\nLeland McInnes, John Healy, and James Melville. UMAP: Uniform manifold approximation and\nprojection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word represen-\ntations in vector space. arXiv preprint arXiv:1301.3781, 2013a.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in neural information pro-\ncessing systems, pp. 3111–3119, 2013b.\nIshan Misra, C Lawrence Zitnick, and Martial Hebert. Shufﬂe and learn: unsupervised learning\nusing temporal order veriﬁcation. In European Conference on Computer Vision, pp. 527–544.\nSpringer, 2016.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\nGuansong Pang, Longbing Cao, Ling Chen, and Huan Liu. Learning representations of ultrahigh-\ndimensional data for random distance-based outlier detection. In Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2041–2050.\nACM, 2018.\nDeepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context\nencoders: Feature learning by inpainting. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 2536–2544, 2016.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. In ICLR, 2015.\nAli Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in\nneural information processing systems, pp. 1177–1184, 2008.\nMostafa Rahmani and George Atia. Coherence pursuit: Fast, simple, and robust subspace recov-\nery. In Proceedings of the 34th International Conference on Machine Learning, pp. 2864–2873.\nJMLR. org, 2017.\nSam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed-\nding. Science, 290(5500):2323–2326, 2000.\nBernhard Sch¨olkopf, Alexander Smola, and Klaus-Robert M¨uller. Kernel principal component anal-\nysis. In International conference on artiﬁcial neural networks, pp. 583–588. Springer, 1997.\n10\nSantosh Vempala. Random projection: A new approach to vlsi layout. In Proceedings 39th Annual\nSymposium on Foundations of Computer Science, pp. 389–395. IEEE, 1998.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\nStacked denoising autoencoders: Learning useful representations in a deep network with a local\ndenoising criterion. Journal of machine learning research, 11:3371–3408, 2010.\nSvante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and\nintelligent laboratory systems, 2(1-3):37–52, 1987.\nRichard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning\nby cross-channel prediction. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 1058–1067, 2017.\nTinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth\nand ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pp. 1851–1858, 2017.\nBo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng\nChen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In ICLR,\n2018.\nA\nIMPLEMENTATION DETAILS\nRDP-enabled Anomaly Detection. The RDP consists of one fully connected layer with 50 hidden\nunits, followed by a leaky-ReLU layer. It is trained using Stochastic Gradient Descent (SGD) as its\noptimiser for 200 epochs, with 192 samples per batch. The learning rate is ﬁxed to 0.1. We repeated\nthe boosting process 30 times to obtain statistically stable results. In order to have fair comparisons,\nwe also adapt the competing methods AE, REPEN, DAGMM and RND into ensemble methods and\nperform the experiments using an ensemble size of 30.\nRDP-enabled Clustering. RDP uses a similar network architecture and optimisation settings as the\none used in anomaly detection, i.e., the network consists of one fully connected layer, followed by\na leaky-ReLU layer, which is optimised by SGD with 192 samples per batch and 0.1 learning rate.\nCompared to anomaly detection, more semantic information is required for clustering algorithms\nto work well, so the network consists of 1,024 hidden units and is trained for 1,000 epochs. Clus-\ntering is a signiﬁcant yet common analysis method, which aims at grouping samples close to each\nother into the same clusters and separating far away data points into different clusters. Compared\nto anomaly detection that often requires pattern frequency information, clustering has a higher re-\nquirement of the representation expressiveness. Therefore, if the representative ability of a model is\nstrong enough, it should also be able to learn representations that enable clustering to work well on\nthe projected space.\nNote that the representation dimension M in the φ function and the projection dimension K in the η\nfunction are set to be the same to alleviate parameter tuning. This means that M = K = 50 is used\nin anomaly detection and M = K = 1024 is used in clustering. We have also tried deeper network\nstructures, but they worked less effectively than the shallow networks in both anomaly detection\nand clustering. This may be because the supervisory signal is not strong enough to effectively\nlearn deeper representations. We show in Appendix E that RDP performs stably w.r.t. a range of\nrepresentation dimensions in both anomaly detection and clustering tasks.\nThe runtime of RDP at the testing stage is provided in Appendix F with that of the competing meth-\nods as baselines. For both anomaly detection and clustering tasks, RDP achieves very comparable\ntime complexity to the most efﬁcient competing methods (see Tables 10 and 11 in Appendix F for\ndetail).\nB\nDATASETS\nThe statistics and the accessible links of the datasets used in the anomaly detection and clustering\ntasks are respectively presented in Tables 6 and 7. DDoS is a dataset containing DDoS attacks\n11\nand normal network ﬂows. Donors is from KDD Cup 2014, which is used for detecting a very\nsmall number of outstanding donors projects. Backdoor contains backdoor network attacks derived\nfrom the UNSW-NB15 dataset. Creditcard is a credit card fraud detection dataset. Lung contains\ndata records of lung cancer patients and normal patients. Probe and U2R are derived from KDD\nCup 99, in which probing and user-to-root attacks are respectively used as anomalies against the\nnormal network ﬂows. The above datasets contain real anomalies. Following (Liu et al., 2008;\nPang et al., 2018; Zong et al., 2018), the other anomaly detection datasets are transformed from\nclassiﬁcation datasets by using the rare class(es) as the anomaly class, which generates semantically\nreal anomalies.\nTable 6: Datasets used in the anomaly detection task\nData\nN\nD\nAnomaly (%)\nLink\nDDoS\n464,976\n66\n3.75%\nhttp://www.csmining.org/cdmc2018/index.php\nDonors\n619,326\n10\n5.92%\nhttps://www.kaggle.com/c/kdd-cup-2014-predicting-excitement-at-donors-choose\nBackdoor\n95,329\n196\n2.44%\nhttps://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity\nAd\n3,279\n1,555\n13.99%\nhttps://archive.ics.uci.edu/ml/datasets/internet+advertisements\nApascal\n12,695\n64\n1.38%\nhttp://vision.cs.uiuc.edu/attributes/\nBank\n41,188\n62\n11.26%\nhttps://archive.ics.uci.edu/ml/datasets/Bank+Marketing\nCeleba\n202,599\n39\n2.24%\nhttp://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\nCensus\n299,285\n500\n6.20%\nhttps://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29\nCreditcard\n284,807\n29\n0.17%\nhttps://www.kaggle.com/mlg-ulb/creditcardfraud\nLung\n145\n3,312\n4.13%\nhttps://archive.ics.uci.edu/ml/datasets/Lung+Cancer\nProbe\n64,759\n34\n6.43%\nhttp://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\nR8\n3,974\n9,467\n1.28%\nhttp://csmining.org/tl ﬁles/Project Datasets/r8 r52/r8-train-all-terms.txt\nSecom\n1,567\n590\n6.63%\nhttps://archive.ics.uci.edu/ml/datasets/secom\nU2R\n60,821\n34\n0.37%\nhttp://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\nR8, 20news, Sector and RCV1 are widely used text classiﬁcation benchmark datasets. Olivetti is a\nwidely-used face recognition dataset.\nTable 7: Datasets used in the clustering task\nData\nN\nD\n#Classes\nLink\nR8\n7,674\n17,387\n8\nhttp://csmining.org/tl ﬁles/Project Datasets/r8 r52/r8-train-all-terms.txt\n20news\n18,846\n130,107\n20\nhttps://scikit-learn.org/0.19/datasets/twenty newsgroups.html\nOlivetti\n400\n4,096\n40\nhttps://scikit-learn.org/0.19/datasets/olivetti faces.html\nSector\n9,619\n55,197\n105\nhttps://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/multiclass.html#sector\nRCV1\n20,242\n47,236\n2\nhttps://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/binary.html#rcv1.binary\nC\nAUC-PR PERFORMANCE OF ABLATION STUDY IN ANOMALY DETECTION\nThe experimental results of AUC-PR performance of RDP and its variants in the anomaly detec-\ntion task are shown in Table 8. Similar to the results shown in Table 3, using the Lrdp loss only,\nour proposed RDP model can achieve substantially better performance over its counterparts. By\nremoving the Lrdp loss, the performance of RDP drops signiﬁcantly in 11 out of 14 datasets. This\ndemonstrates that the Lrdp loss is heavily harvested by our RDP model to learn high-quality rep-\nresentations from random distances. Removing Lad\naux from RDP also results in substantial loss of\nAUC-PR in many datasets. This indicates both the random distance prediction loss Lrdp and the\ntask-dependent loss Lad\naux are critical to RDP. The boosting process is also important, but is not as\ncritical as the two losses. Consistent with the observations derived from Table 3, distances calculated\nin non-linear and linear random mapping spaces are more effective supervisory sources than that in\nthe original space.\nD\nNMI PERFORMANCE OF ABLATION STUDY IN CLUSTERING\nTable 9 shows the NMI performance of RDP and its variants in the clustering task. It is clear that\nour RDP model with the Lrdp loss is able to achieve NMI performance that is comparably well to\nthe full RDP model, which is consistent to the observations in Table 5. Without using the Lrdp\n12\nTable 8: AUC-PR performance of RDP and its variants in the anomaly detection task.\nDecomposition\nSupervision Signal\nData\nRDP\nRDP\\Lrdp\nRDP\\Lad\naux\nRDP\\Boosting\nOrg SS\nSRP SS\nDDoS\n0.301 ± 0.028\n0.110 ± 0.015 0.364 ± 0.013\n0.114 ± 0.001\n0.363 ± 0.007 0.380 ± 0.030\nDonors\n0.432 ± 0.061\n0.201 ± 0.033 0.104 ± 0.007\n0.278 ± 0.040\n0.099 ± 0.004 0.113 ± 0.010\nBackdoor\n0.305 ± 0.008\n0.433 ± 0.015 0.142 ± 0.006\n0.537 ± 0.005\n0.143 ± 0.005 0.154 ± 0.028\nAd\n0.726 ± 0.007\n0.473 ± 0.009 0.491 ± 0.014\n0.488 ± 0.008\n0.419 ± 0.015 0.530 ± 0.007\nApascal\n0.042 ± 0.003\n0.021 ± 0.005 0.031 ± 0.002\n0.028 ± 0.003\n0.016 ± 0.003 0.035 ± 0.007\nBank\n0.364 ± 0.013\n0.258 ± 0.006 0.266 ± 0.018\n0.278 ± 0.007\n0.262 ± 0.016 0.265 ± 0.021\nCeleba\n0.104 ± 0.006\n0.068 ± 0.010 0.060 ± 0.004\n0.072 ± 0.008\n0.050 ± 0.009 0.065 ± 0.010\nCensus\n0.086 ± 0.001\n0.081 ± 0.001 0.075 ± 0.001\n0.087 ± 0.001\n0.077 ± 0.002 0.064 ± 0.001\nCreditcard\n0.363 ± 0.011\n0.290 ± 0.012\n0.414 ± 0.02\n0.329 ± 0.007\n0.362 ± 0.016 0.372 ± 0.024\nLung\n0.705 ± 0.028\n0.381 ± 0.104 0.437 ± 0.083\n0.542 ± 0.139\n0.361 ± 0.054 0.464 ± 0.053\nProbe\n0.955 ± 0.002\n0.609 ± 0.014 0.952 ± 0.007\n0.628 ± 0.011\n0.937 ± 0.005 0.959 ± 0.011\nR8\n0.146 ± 0.017\n0.134 ± 0.031 0.109 ± 0.006\n0.173 ± 0.028\n0.067 ± 0.016 0.134 ± 0.019\nSecom\n0.096 ± 0.001\n0.086 ± 0.002 0.096 ± 0.006\n0.090 ± 0.001\n0.088 ± 0.004 0.093 ± 0.004\nU2R\n0.261 ± 0.005\n0.217 ± 0.011 0.266 ± 0.007\n0.238 ± 0.009\n0.187 ± 0.013 0.239 ± 0.023\n#wins/draws/losses (RDP vs.)\n13/0/1\n11/0/3\n11/0/3\n12/0/2\n5/0/9\nloss, the performance of the RDP model has some large drops on nearly all the datasets. This\nreinforces the crucial importance of Lrdp to RDP, which also justiﬁes that using Lrdp alone RDP\ncan learn expressive representations. Similar to the results in Table 5, RDP is generally more reliable\nsupervisory sources than Org SS and SRP SS in this set of results.\nTable 9: NMI performance of RDP and its variants in the clustering task.\nDecomposition\nSupervision Signal\nData\nRDP\nRDP\\Lrdp\nRDP\\Lclu\naux\nOrg SS\nSRP SS\nR8\n0.539 ± 0.040\n0.471 ± 0.043\n0.505 ± 0.037\n0.567 ± 0.021\n0.589 ± 0.039\n20news\n0.084 ± 0.005\n0.075 ± 0.006\n0.081 ± 0.002\n0.075 ± 0.002\n0.074 ± 0.003\nOlivetti\n0.805 ± 0.012\n0.782 ± 0.010\n0.784 ± 0.010\n0.795 ± 0.011\n0.787 ± 0.011\nSector\n0.305 ± 0.007\n0.253 ± 0.010\n0.340 ± 0.007\n0.295 ± 0.009\n0.298 ± 0.008\nRcv1\n0.165 ± 0.000\n0.146 ± 0.010\n0.168 ± 0.000\n0.154 ± 0.002\n0.147 ± 0.000\nE\nSENSITIVITY W.R.T. THE DIMENSIONALITY OF REPRESENTATION SPACE\nThis section presents the performance of RDP using different representation dimensions in its feature\nlearning layer. The sensitivity test is performed for both anomaly detection and clustering tasks.\nE.1\nSENSITIVITY TEST IN ANOMALY DETECTION\nFigures 2 and 3 respectively show the AUC-ROC and AUC-PR performance of RDP using different\nrepresentation dimensions on all the 14 anomaly detection datasets used in this work. It is clear\nfrom both performance measures that RDP generally performs stably w.r.t. the use of different\nrepresentation dimensions on diverse datasets. This demonstrates the general stability of our RDP\nmethod on different application domains. On the other hand, the ﬂat trends also indicate that, as an\nunsupervised learning source, the random distance cannot provide sufﬁcient supervision information\nto learn richer and more complex representations in a higher-dimensional space. This also explains\nthe performance on quite a few datasets where the performance of RDP decreases when increasing\nthe representation dimension. In general, the representation dimension 50 is recommended for RDP\nto achieve effective anomaly detection on datasets from different domains.\nE.2\nSENSITIVITY TEST IN CLUSTERING\nFigure 4 presents the NMI and F-score performance of RDP-enabled K-means clustering using\ndifferent representation dimensions on all the ﬁve datasets in the clustering task. Similar to the\nsensitivity test results in the anomaly detection task, on all the ﬁve datasets, K-means clustering per-\nforms stably in the representation space resulted by RDP with different representation dimensions.\nThe clustering performance may drop a bit when the representation dimension is relatively low, e.g.,\n512. Increasing the representation to 1,280 may help RDP gain better representation power in some\n13\n25\n50\n75\n100\nRepresentation Dimension\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nAUC-ROC\nDDoS\nDonors\nBack\nAd\nApascal\nBank\nCeleba\nCensus\nCredit\nLung\nProbe\nR8\nSecom\nU2r\nFigure 2: AUC-ROC results of RDP w.r.t. different representation dimensions on 14 datasets.\n25\n50\n75\n100\nRepresentation Dimension\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAUC-PR\nDDoS\nDonors\nBack\nAd\nApascal\nBank\nCeleba\nCensus\nCredit\nLung\nProbe\nR8\nSecom\nU2r\nFigure 3: AUC-PR results of RDP w.r.t. different representation dimensions on 14 datasets.\ndatasets but is not a consistently better choice. Thus, the representation dimension 1,024 is gener-\nally recommended for clustering. Recall that the required representation dimension in clustering is\nnormally signiﬁcantly higher than that in anomaly detection, because clustering generally requires\nsigniﬁcantly more information to perform well than anomaly detection.\nF\nCOMPUTATIONAL EFFICIENCY\nThe runtime of RDP is compared with its competing methods in both anomaly detection and clus-\ntering tasks. Since training time can vary signiﬁcantly using different training strategies in deep\nlearning-based methods, it is difﬁcult to have a fair comparison of the training time. Moreover, the\nmodels can often be trained ofﬂine. Thus, we focus on comparing the runtime at the testing stage.\nAll the runtime experiments below were done on a computing server node equipped with 32 Intel\nXeon E5-2680 CPUs (2.70GHz) and 128GB Random Access Memory.\n14\n512\n768\n1024\n1280\nRepresentation Dimension\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nF-score Performance\nR8\n20news\nOlivetti\nSector\nRcv1\n512\n768\n1024\n1280\nRepresentation Dimension\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nF-score Performance\nR8\n20news\nOlivetti\nSector\nRcv1\nFigure 4: NMI and F-score performance of RDP-enabled K-means using different representation\ndimensions on all the ﬁve datasets used in clustering.\nF.1\nTESTING RUNTIME IN ANOMALY DETECTION\nThe testing runtime in seconds of RDP and its ﬁve competing anomaly detection methods on 14\nanomaly detection datasets are provided in Table 10. Since most of the methods integrate represen-\ntation learning and anomaly detection into a single framework, the runtime includes the execution\ntime of feature learning and anomaly detection for all six methods. In general, on most large datasets,\nRDP runs comparably fast to the most efﬁcient methods iForest and RND, and is faster the two re-\ncently proposed deep methods REPEN and DAGMM. Particularly, RDP runs faster than REPEN\nand DAGMM by a factor of around ﬁve on high-dimensional and large-scale datasets like Donors\nand Census. RDP is slower than the competing methods in processing small datasets. This is mainly\nbecause RDP has a base runtime of its boosting process. Therefore, the runtime of RDP seems to be\nalmost constant across the datasets. This is a very desired property for handling high-dimensional\nand large-scale datasets.\nTable 10: Testing runtime (in seconds) on 14 anomaly detection datasets.\nData Characteristics\nRDP and Its Five Competing Methods\nData\nN\nD\niForest\nAE\nREPEN DAGMM RND RDP\nDDoS\n464,976\n66\n54.06\n86.86\n172.47\n197.85\n31.86 28.93\nDonors\n619,326\n10\n28.17\n52.31\n226.14\n194.45\n44.31 36.84\nBackdoor\n95,329\n196\n26.51\n51.66\n36.43\n187.61\n12.26 29.95\nAd\n3,279\n1,555\n6.71\n14.71\n3.24\n31.54\n8.12\n30.83\nApascal\n12,695\n64\n6.53\n4.27\n6.30\n69.35\n3.62\n22.88\nBank\n41,188\n62\n9.72\n6.87\n17.25\n170.56\n9.31\n28.47\nCeleba\n202,599\n39\n20.54\n26.70\n71.60\n223.70\n18.05 33.91\nCensus\n299,285\n500\n155.77 225.29\n121.08\n236.21\n42.83 57.74\nCreditcard 284,807\n29\n22.45\n29.38\n103.18\n235.93\n20.97 30.84\nLung\n145\n3,312\n6.20\n13.11\n2.16\n39.75\n1.44\n24.29\nProbe\n64,759\n34\n9.55\n10.06\n28.14\n131.40\n9.90\n29.61\nR8\n3,974\n9,467\n59.70\n45.48\n7.81\n31.99\n8.26\n14.33\nSecom\n1,567\n590\n7.32\n5.78\n2.83\n18.22\n3.23\n22.52\nU2R\n60,821\n34\n8.95\n9.38\n26.55\n185.88\n9.90\n28.10\nF.2\nTESTING RUNTIME IN CLUSTERING\nTable 11 shows the testing runtime of RDP and its four competing methods in enabling clustering\non ﬁve datasets. Since exactly the same K-means clustering is used on the features in all the ﬁve\ncases, we exclude the runtime of the K-means clustering for more straightforward comparison. The\nresults show that RDP runs comparably fast to the very efﬁcient methods SRP and AE since they\ndo not involve complex computation at the testing stage; RDP runs about ﬁve orders of magnitude\nfaster than HLLE since HLLE takes a huge amount of time in its nearest neighbours searching. Note\nthat ‘Org’ indicates the clustering performed on the original space, so it involves no feature learning\nand does not take any time.\n15\nTable 11: Testing runtime (in seconds) on ﬁve clustering datasets.\nData Characteristics\nRDP and Its Four Competing Methods\nData\nN\nD\nOrg\nHLLE\nSRP\nAE\nRDP\nR8\n7,674\n17,387\n-\n9,658.85\n1.16\n1.08\n0.89\n20news 18,846 130,107\n-\n94,349.20 2.26 11.49\n6.85\nOlivetti\n400\n4,096\n-\n166.02\n0.73\n0.03\n0.03\nSector\n9,619\n55,197\n-\n24,477.80 1.40\n4.28\n2.87\nRCV1\n20,242\n47,236\n-\n47,584.79 2.80\n8.91\n5.04\nG\nCOMPARISON TO STATE-OF-THE-ART REPRESENTATION LEARNING\nMETHODS FOR RAW TEXT AND IMAGE DATA\nSince RDP relies on distance information as its supervisory signal, one interesting question is that,\ncan RDP still work when the presented data is raw data in a non-Euclidean space, such as raw text\nand image data? One simple and straightforward way to enable RDP to handle those raw data is,\nas what we did on the text and image data used in the evaluation of clustering, to ﬁrst convert the\nraw texts/images into feature vectors using commonly-used methods, e.g., TF-IDF (Aizawa, 2003)\nfor text data and treating each pixel as a feature unit for image data, and then perform RDP on these\nvector spaces. A further question is that, do we need RDP in handling those data since there are now\na large number of advanced representation learning methods that are speciﬁcally designed for raw\ntext/image datasets? Or, how is the performance of RDP compared to those advanced representation\nlearning methods for raw text/image datasets? This section provides some preliminary results in the\nclustering task for answering these questions.\nG.1\nON RAW TEXT DATA\nOn the raw text datasets R8 and 20news, we ﬁrst compare RDP with the advanced document rep-\nresentation method Doc2Vec3 as in (Le & Mikolov, 2014). Recall that, for RDP, we ﬁrst use the\nbag-of-words model and document frequency information (e.g., TF-IDF) to simply convert doc-\numents into high-dimensional feature vectors and then perform RDP using the feature vectors.\nDoc2Vec leverages the idea of distributed representations to directly learn representations of doc-\numents. We further derive a variant of RDP, namely Doc2Vec+RDP, which performs RDP on\nthe Doc2Vec projected representation space rather than the bag-of-words vector space. All RDP,\nDoc2Vec and Doc2Vec+RDP project data onto a 1,024-dimensional space for the subsequent learn-\ning tasks. Note that, for the method Doc2Vec+RDP, to better examine the capability of RDP in\nexploiting the Doc2Vec projected space, we ﬁrst use Doc2Vec project raw text data onto a higher-\ndimensional space (5,120 dimensions for R8 and 10,240 dimensions for 20news), and RDP further\nlearns a 1,024-dimensional space from this higher-dimensional space.\nThe comparison results are shown in Table 12. Two interesting observations can be seen. First,\nRDP can signiﬁcantly outperform Doc2Vec on R8 or performs comparably well on 20news. This\nmay be due to the fact that the local proximity information learned in RDP is critical to cluster-\ning; although the word prediction approach in Doc2Vec helps learn semantic-rich representations\nfor words/sentences/paragraphs, the pairwise document distances may be less effective than RDP\nsince Doc2Vec is not like RDP that is designed to optimise this proximity information. Second,\nDoc2Vec+RDP can achieve substantially better performance than Doc2Vec, especially on the dataset\n20news where Doc2Vec+RDP achieves a NMI score of 0.198 while that of Doc2Vec is only 0.084.\nThis may be because, as discussed in Section 3.3, RDP is equivalent to learn an optimised feature\nspace out of its input space (Doc2Vec projected feature space in this case) using imperfect super-\nvision information. When there is sufﬁcient accurate supervision information, RDP can learn a\nsubstantially better feature space than its input space. This is also consistent with the results in Ta-\nble 4, in which clustering based on the RDP projected space also performs substantially better than\nthat working in the original space ‘Org’.\n3We use the implementation of Doc2Vec in a popular text mining python package gensim available at\nhttps://radimrehurek.com/gensim/index.html\n16\nTable 12:\nNMI and F-score performance of K-means clustering using RDP, Doc2Vec, and\nDoc2Vec+RDP based feature representations of the text datasets R8 and news20.\nData Characteristics\nNMI Performance\nData\nN\nD\nDoc2Vec\nRDP\nDoc2Vec+RDP\nR8\n7,674\n17,387\n0.241 ± 0.022 0.539 ± 0.040\n0.250 ± 0.003\n20news 18,846 130,107 0.080 ± 0.003 0.084 ± 0.005\n0.198 ± 0.009\nData Characteristics\nF-score Performance\nData\nN\nD\nDoc2Vec\nRDP\nDoc2Vec+RDP\nR8\n7,674\n17,387\n0.317 ± 0.014 0.360 ± 0.055\n0.316 ± 0.007\n20news 18,846 130,107 0.115 ± 0.006 0.119 ± 0.006\n0.126 ± 0.009\nG.2\nON RAW IMAGE DATA\nOn the raw image dataset Olivetti, we compare RDP with the advanced representation learning\nmethod for raw images, RotNet (Gidaris et al., 2018). RDP uses each image pixel as a feature\nunit and performs on a 64 × 64 vector space. RotNet directly learns representations of images by\npredicting whether a given image is rotated or not. Similar to the experiments on raw text data, we\nalso evaluate the performance of RDP working on the RotNet projected space, i.e., RotNet+RDP.\nAll RDP, RotNet and RotNet+RDP ﬁrst learn a 1,024 representation space, and then K-means is\napplied to the learned space to perform clustering. In the case of RotNet+RDP, the raw image data is\nﬁrst projected onto a 2,048-dimensional space, and then RDP is applied to this higher-dimensional\nspace to learn a 1,024-dimensional representation space.\nWe use the implementation of RotNet released by its authors4. Note that the original RotNet is\napplied to large image datasets and has a deep network architecture, involving four convolutional\nblocks with three convolutional layers for each block. We found directly using the original architec-\nture is too deep for Olivetti and performs ineffectively as the data contains only 400 image samples.\nTherefore, we simplify the architecture of RotNet and derive four variants of RotNet, including\nRotNet4×2, RotNet4×1, RotNet3×1 and RotNet2×1. Here RotNeta×b represents RotNet with a con-\nvolutional blocks and b convolutional layers for each block. Note that RotNet2×1 is the simplest\nvariant we can derive that works effectively. We evaluate the original RotNet, its four variants and\nthe combination of these ﬁve RotNets and RDP.\nTable 13: NMI and F-score performance of K-means clustering using RDP, RotNet, and Rot-\nNet+RDP based feature representations of the image dataset Olivetti.\nNMI Performance F-score Performance\nOrg\n0.778 ± 0.014\n0.590 ± 0.029\nRDP\n0.805 ± 0.012\n0.638 ± 0.026\nRotNet\n0.467 ± 0.014\n0.243 ± 0.014\nRotNet+RDP\n0.472 ± 0.011\n0.242 ± 0.011\nRotNet4×2\n0.518 ± 0.010\n0.281 ± 0.014\nRotNet4×2+RDP\n0.517 ± 0.010\n0.282 ± 0.014\nRotNet4×1\n0.519 ± 0.010\n0.283 ± 0.014\nRotNet4×1+RDP\n0.536 ± 0.010\n0.298 ± 0.011\nRotNet3×1\n0.526 ± 0.014\n0.303 ± 0.018\nRotNet3×1+RDP\n0.567 ± 0.010\n0.336 ± 0.015\nRotNet2×1\n0.561 ± 0.010\n0.339 ± 0.016\nRotNet2×1+RDP\n0.587 ± 0.009\n0.374 ± 0.015\nThe evaluation results are presented in Table 13. Impressively, RDP can signiﬁcantly outperform\nRotNet and all its four variants on Olivetti. It is interesting that Org (i.e., performing K-means\nclustering on the original 64 × 64 vector space) also obtains a similar superiority over the RotNet\nfamily. This may be because Olivetti is too small to provide sufﬁcient training samples for RotNet\nand its variants to learn its underlying semantic abstractions. This conjecture can also explain the\nincreasing performance of RotNet variants with decreasing complexity of the RotNet architecture.\nSimilar to the results on the raw text data, applying RDP on the RotNet projected spaces can also\nlearn substantially more expressive representations than the representations yielded by RotNet and\n4The released code of RotNet is available at https://github.com/gidariss/FeatureLearningRotNet.\n17\nits variants, especially when the RotNet methods work well, such as the two cases: RotNet3×1 vs.\nRotNet3×1+RDP and RotNet2×1 vs. RotNet2×1+RDP.\nH\nPERFORMANCE EVALUATION IN CLASSIFICATION\nWe also performed some preliminary evaluation of the learned representations in classiﬁcation tasks\nusing a feed-forward three-layer neural network model as the classiﬁer. We used the same datasets\nas in the clustering task. Speciﬁcally, the representation learning model ﬁrst outputs the new rep-\nresentations of the input data, and then the classiﬁer performs classiﬁcation on the learned repre-\nsentations. RDP is compared with the same competing methods HLLE, SRP, AE and COP as in\nclustering. F-score is used as the performance evaluation metric here.\nThe results are shown in Table 14. Similar to the performance in clustering and anomaly detection,\nour model using only the random distance prediction loss Lrdp, i.e., RDP\\Lclu\naux, performs very\nfavourably and stably on all the ﬁve datasets. The incorporation of \\Lclu\naux into the model, i.e., RDP,\nhelps gain some extra performance improvement on datasets like 20news, but it may also slightly\ndowngrade the performance on other datasets. An extra hyperparameter may be added to control the\nimportance of these two losses.\nTable 14: F-score performance of classiﬁcation on ﬁve real-world datasets.\nData\nHLLE\nSRP\nAE\nCOP\nRDP\\Lclu\naux\nRDP\nR8\n0.246\n0.895\n0.874\n0.860\n0.900\n0.906\n20news\n0.005\n0.733\n0.709\n0.718\n0.735\n0.753\nOlivetti\n0.895\n0.899\n0.820\n0.828\n0.900\n0.896\nSector\n0.037\n0.671\n0.645\n0.689\n0.690\n0.696\nRCV1\n0.766\n0.919\n0.918\nN/A\n0.940\n0.926\n18\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2019-12-22",
  "updated": "2020-07-19"
}