{
  "id": "http://arxiv.org/abs/2001.00784v1",
  "title": "Optimizing Wireless Systems Using Unsupervised and Reinforced-Unsupervised Deep Learning",
  "authors": [
    "Dong Liu",
    "Chengjian Sun",
    "Chenyang Yang",
    "Lajos Hanzo"
  ],
  "abstract": "Resource allocation and transceivers in wireless networks are usually\ndesigned by solving optimization problems subject to specific constraints,\nwhich can be formulated as variable or functional optimization. If the\nobjective and constraint functions of a variable optimization problem can be\nderived, standard numerical algorithms can be applied for finding the optimal\nsolution, which however incur high computational cost when the dimension of the\nvariable is high. To reduce the on-line computational complexity, learning the\noptimal solution as a function of the environment's status by deep neural\nnetworks (DNNs) is an effective approach. DNNs can be trained under the\nsupervision of optimal solutions, which however, is not applicable to the\nscenarios without models or for functional optimization where the optimal\nsolutions are hard to obtain. If the objective and constraint functions are\nunavailable, reinforcement learning can be applied to find the solution of a\nfunctional optimization problem, which is however not tailored to optimization\nproblems in wireless networks. In this article, we introduce unsupervised and\nreinforced-unsupervised learning frameworks for solving both variable and\nfunctional optimization problems without the supervision of the optimal\nsolutions. When the mathematical model of the environment is completely known\nand the distribution of environment's status is known or unknown, we can invoke\nunsupervised learning algorithm. When the mathematical model of the environment\nis incomplete, we introduce reinforced-unsupervised learning algorithms that\nlearn the model by interacting with the environment. Our simulation results\nconfirm the applicability of these learning frameworks by taking a user\nassociation problem as an example.",
  "text": "1\nOptimizing Wireless Systems Using Unsupervised\nand Reinforced-Unsupervised Deep Learning\nDong Liu, Chengjian Sun, Chenyang Yang, and Lajos Hanzo\nAbstract—Resource allocation and transceivers in wireless\nnetworks are usually designed by solving optimization problems\nsubject to speciﬁc constraints, which can be formulated as vari-\nable or functional optimization. If the objective and constraint\nfunctions of a variable optimization problem can be derived,\nstandard numerical algorithms can be applied for ﬁnding the\noptimal solution, which however incur high computational cost\nwhen the dimension of the variable is high. To reduce the on-\nline computational complexity, learning the optimal solution as\na function of the environment’s status by deep neural networks\n(DNNs) is an effective approach. DNNs can be trained under the\nsupervision of optimal solutions, which however, is not applicable\nto the scenarios without models or for functional optimization\nwhere the optimal solutions are hard to obtain. If the objective\nand constraint functions are unavailable, reinforcement learning\ncan be applied to ﬁnd the solution of a functional optimization\nproblem, which is however not tailored to optimization problems\nin wireless networks. In this article, we introduce unsupervised\nand reinforced-unsupervised learning frameworks for solving\nboth variable and functional optimization problems without the\nsupervision of the optimal solutions. When the mathematical\nmodel of the environment is completely known and the distri-\nbution of environment’s status is known or unknown, we can\ninvoke unsupervised learning algorithm. When the mathematical\nmodel of the environment is incomplete, we introduce reinforced-\nunsupervised learning algorithms that learn the model by in-\nteracting with the environment. Our simulation results conﬁrm\nthe applicability of these learning frameworks by taking a user\nassociation problem as an example.\nI. INTRODUCTION\nBoth the transceivers and resource allocation of wireless\nnetworks such as power allocation, user association, caching\npolicy, etc. have been designed for decades by solving op-\ntimization problems. These problems can be formulated as\nvariable optimization or functional optimization problems,\ndepending on whether the values of the objective function\n(OF), the constraint function (CF), if any, and the “variables”\nto be optimized change on a similar timescale.\nIf they vary on a similar timescale, the problem can be\nformulated as variable optimization, where the optimization\nvariable is a scalar or a vector. Variable optimization problems\nare quite common in wireless communications [1]. A typical\nD. Liu and L. Hanzo are with the University of Southampton, Southampton\nSO17 1BJ, UK (email: d.liu, hanzo@soton.ac.uk). C. Sun and C. Yang\nare with Beihang University, Beijing 100191, China (e-mail: sunchengjian,\ncyyang@buaa.edu.cn).\nThis work was supported in part by the National Natural Science Founda-\ntion of China (NSFC) under Grant No. 61731002 and 61671036. L. Hanzo\nwould like to acknowledge the ﬁnancial support of the Engineering and\nPhysical Sciences Research Council projects EP/Noo4558/1, EP/PO34284/1,\nCOALESCE, of the Royal Society’s Global Challenges Research Fund Grant\nas well as of the European Research Council’s Advanced Fellow Grant\nQuantCom.\nexample is to optimize beamforming vectors of multiple users\nfor maximizing the instantaneous sum-rate subject to the\ninstantaneous transmit power constraint. Another example is to\noptimize a caching policy that maximizes the average spectral\nefﬁciency within the cache-update interval.\nIf they vary on quite different timescales, the problem\nis actually functional optimization. A typical example is to\noptimize the instantaneous transmit power for maximizing the\nergodic channel capacity under an average transmit power\nconstraint. In this problem, the transmit power should adapt\nto fading channels varying on the timescale of milliseconds,\nwhile the OF and CF change on the timescale of seconds.\nIn functional optimization, the optimization “variable” is a\nfunction, mapping for example from the instantaneous channel\ngain to the transmit power. Functional optimization is widely\nused in optimal control theory [2], but it is less familiar to the\nwireless community, although the classic water-ﬁlling power\nallocation is found by solving the aforementioned functional\noptimization problem. Given the increasing importance of\ncross-layer optimization, say for ultra-reliable and low-latency\ncommunications [3], functional optimization is gaining atten-\ntion in designing wireless networks.\nVariable optimization problems are often solved by con-\nventional optimization methods. For continuous variables,\nconvex optimization tools [4] such as the classic interior-point\nmethod, and non-convex optimization tools such as semi-\ndeﬁnite relaxation (SDR), have been widely used. For discrete\nvariables, various search methods such as the branch-and-\nbound and cutting-plane methods have been developed. For the\nsake of promptly adapting to time-varying environments at an\naffordable computational cost, the solutions are expected to be\nfound before the environment’s status (e.g. the instantaneous\nor average channel gains, or content popularity) changes.\nThis can be accomplished if the explicit expression of the\nsolution can be derived as a function of the environment’s\nstatus. Unfortunately, in many cases, conventional optimiza-\ntion algorithms such as the interior point or gradient descent\nmethods [4] have to be employed for solving the problems\nvia numerical iteration, which impose a high complexity for\nhigh-dimensional problems.\nFunctional optimization problems are in general hard to\nsolve. A conventional technique is the ﬁnite element method\n(FEM) [5], which converts functional optimization into high-\ndimensional variable optimization by only optimizing the\nvalues of the function at the sampling points of the envi-\nronment’s status. However, even this procedure may turn out\nto be prohibitively complex for a large number of sampling\npoints. Consequently, the functional optimization problems of\narXiv:2001.00784v1  [cs.LG]  3 Jan 2020\n2\nP1: Variable Optimization\nObjective: Given the environment’s status h, \nSubject to: Instantaneous constraints gi(x, h) ≤ 0   \nOptimization Variable: A vector x  \noptimize x to maximize the\nperformance metric J(x, h)  \nP2: Functional Optimization\nObjective: Optimize f(h) to maximize the average\n                \nSubject to: Instantaneous constraints gi(f(h), h) ≤ 0  \nOptimization “Variable”: A function f mapping h to x\nperformance metric Eh[J(f(h), h)]  \nAverage constraints Eh[cj(f(h), h)] ≤ 0\nP3: Primal-Dual Problem (Functional Optimization)\nObjective: Optimize f(h) and the multipliers to find the saddle \nSubject to: None of the multipliers are less than zero  \nOptimization “Variable”: f(h) and the Lagrangian multipliers\npoint of the Lagrangian function  \nP4: Parameterized Primal-Dual Problem (Variable Optimization)\nObjective: Optimize PNP and MNP to find the saddle point of               \nSubject to: None of the multipliers are less than zero  \nOptimization Variable: Policy network parameters (PNP)  \nthe parameterized Lagrangian function  \nMuliplier network parameters (MNP) \nEquivalent when P2 does not have average constraint\nParameterize f(h) and the multipliers by neural networks \nStrong Duality\nFig. 1. Relationships between variable optimization and functional optimization. It is noteworthy that optimization problems minimizing the OF or having\n“≥” or “=” constraints (e.g., minimal data rate constraint) can be easily transformed into problem P1 or P2.\nwireless networks are often solved by heuristic techniques [3],\nwhich inevitably leads to performance loss [6].\nThe holistic optimization problems of next-generation net-\nworking become increasingly complex, hence facing the fol-\nlowing two challenges:\n• The ﬁrst one is the computational complexity of their\non-line implementation, especially for large-scale phys-\nical layer optimization. Due to the rapidly ﬂuctuating\nnature of fading channels, the optimal solution should be\nobtained within milliseconds, hence iterative algorithms\nmay require excessive computational resources.\n• The second is the requirement of knowing the environ-\nment’s model, speciﬁcally the expressions of the OF and\nall the CFs. For conventional optimization algorithms,\nsuch as the interior point method, the gradients and\neven the Hessian matrix of the OF and the CFs with\nrespect to (w.r.t.) the optimization variables are also\nrequired. In many scenarios, however, their expressions\nare unavailable, or it is too complex to derive their\ngradients.\nOur ambitious objective is to conceive generic frameworks\nfor solving wireless optimization problems using unsupervised\ndeep neural networks (DNNs). When the environment’s model\nis completely known, or at least the mathematical model (i.e.,\nthe OF and the CFs) is known but the distribution of the\nenvironment’s status is unknown, we conceive an unsupervised\nalgorithm for learning the optimal solution as a function\nof the environment’s status. For the challenging scenarios\nwhere the mathematical model is incomplete, we enhance the\nunsupervised learning algorithm by invoking reinforcements\nfrom interactions with the environment for estimating the\nenvironment’s model.\nWe commence from recent research efforts invested in han-\ndling the aforementioned challenges. We then address model-\nbased unsupervised learning as well as model-free reinforced-\nunsupervised learning frameworks. Next, we conceive a case-\nstudy as a proof-of-concept demonstration of the unsupervised\nlearning frameworks. Finally, we conclude.\nII. STATE-OF-THE-ARTS\nTo reduce the on-line computational complexity, the idea\nof “learning to optimize” is proposed for solving variable\noptimization in [7]. Most recently, a novel framework of using\ndeep learning to ﬁnd the solution of constrained functional\noptimization is proposed in [8]. In fact, reinforcement learning\n(RL), which is recognized as a powerful tool for model-\nfree problems, is also a generic framework used for solving\nfunctional optimization.\nA. Variable Optimization Using Supervised and Unsupervised\nLearning\nAllow us to begin with the standard variable optimization\nproblem P1 of Fig. 1. Given a vector h representing the\nwireless environment’s status (e.g., channel gains of multiple\nusers), the goal is to ﬁnd a solution x (e.g., transmit powers)\nthat maximizes a performance metric J(x, h) (e.g., sum-\nrate) subject to some constraints gi(x, h) ≤0 (e.g., maximal\ntransmit power or quality of service constraint).\nTo reduce the on-line computational complexity of variable\noptimization problems, the “learning to optimize” technique\ninvokes a DNN for approximating the optimized solution as\na function of the environment’s status [7]. To train the DNN,\na training set composed of a number of realizations of the\nenvironment’s status and the corresponding optimal solution\nare ﬁrst generated by running a numerical optimization algo-\nrithm for each environment’s status. Then, the weights of the\nneurons in the DNN are optimized using stochastic gradient\ndescent (SGD) for minimizing the empirical mean square\nerror between the outputs of the DNN and the corresponding\noptimal solution. Once well trained, the DNN can readily ap-\nproximate the optimal solution for any arbitrary environment’s\nstatus, purely requiring computations of forward propagation\nthrough a few layers of neurons rather than using conventional\noptimization algorithms. In this way, the on-line computational\ncomplexity is shifted to the off-line supervised training.\nNevertheless, this approach is not applicable to learning\ngeneral functional optimization in wireless systems, where\na function of the environment’s status has to be optimized.\nThis is because the conventional algorithms conceived for\n3\nfunctional optimization (say the FEM [5]) suffer from the\ncurse of dimensionality. Even though the off-line cost is less\nof a concern, the complexity of using FEM to ﬁnd a numerical\nsolution (i.e., generate one label) increases exponentially with\nthe dimension of the environment’s status, while a DNN’s\ntraining set usually consists of tens of thousands of labels.\nDifferent from the supervised learning approach that re-\nquires the optimal solutions under different environment’s\nstatus as labels for training the DNN, unsupervised learning\napproach can train the DNN without labels. Very recently,\nan unsupervised learning approach was conceived for variable\noptimization problems [1]. The basic idea is to employ the\nOF of the problem as the “loss” function1 to train the DNN,\nand hence the labels are no longer necessary.\nHowever, neither of the supervised nor the unsupervised\napproaches deal with constraints systematically [1, 7]. In the\nsupervised learning approach, the constraints are implicitly\nreﬂected in the labels, but complex constraints are hard to\nsatisfy by a trained-DNN due to the residual errors between\nthe outputs of DNNs and the feasible solutions. In the unsuper-\nvised learning approach of [1], simple power constraints can\nbe satisﬁed by selecting an appropriate activation function for\nthe DNN’s output layer. However, again, complex constraints\ncannot be readily satisﬁed. Furthermore, neither of the two\napproaches are applicable to problems where the expressions\nof the OF or CFs are unavailable.\nB. Functional Optimization Using Unsupervised Learning\nand Reinforcement Learning\nTo ﬁnd the solution of constrained functional optimiza-\ntion problems, unsupervised learning techniques are pro-\nposed in [6, 8, 9]. A constrained optimization problem can\nbe transformed into an unconstrained problem by using the\nLagrangian approach. The function to be optimized can be\nparameterized by a DNN, and the DNN can be trained together\nwith the Lagrangian multipliers. This technique was shown\nto achieve bounded suboptimality under moderate assump-\ntions [8]. This concept can be used for solving the general\nfunctional optimization problem P2 of Fig. 1, which is subject\nto complex average and instantaneous constraints [6, 9]. In\n[6], it is shown that even the stringent quality of service\nconstraint of ultra-reliable and low-latency communications\ncan be guaranteed. The approaches in [6, 8, 9] provide more\nefﬁcient way than FEM in ﬁnding numerical solution of\nfunctional optimization.\nRL aims for ﬁnding a function that speciﬁes an action for\nthe state of a system for optimizing a criterion. By appropri-\nately interpreting the three-tuple constituted by [environment’s\nstatus, solution, performance metric] as [state, action, reward]\nin using RL parlance, various RL algorithms can be conceived\nfor learning the relationship between the optimal solution and\nthe environment’s status.\nRL is eminently suitable for solving problems formulated as\nMarkov decision processes (MDPs), e.g., distributed resource\n1We use the quotation mark on the term “loss” because the problem may\nbe formulated in the form of maximizing the OF.\noptimization [10], rather than the variable optimization prob-\nlems formulated as P1 of Fig. 1. In MDP, the current state and\naction jointly affect the distribution of the next state. Yet P1\nis actually a contextual bandit problem (if the elements of x\nare discrete and P1 does not have constraints) [11], where the\ndistribution of successor states does not depend on the current\naction. Therefore, applying RL algorithms to P1 is an overkill.\nMost RL algorithms use bootstrapping for estimating the value\nof current state or action based on the estimation of the\nnext states’ value [11], which is however unnecessary for P1\nowing to the independence of the next state from the current\naction. The convergence of RL algorithms that combines\nbootstrapping, off-policy learning and function approximation\n(termed as the deadly triad [11]), e.g., the widely adopted\ndeep Q-networks, is not theoretically guaranteed.\nSimilar to DNNs, RL algorithms have not been speciﬁcally\ndesigned for optimization problems subject to constraints,\nwhich are however integral parts of most problems in wireless\nnetworks. In the literature of wireless communications, the\nconstraints have been treated in a heuristic manner when\ninvoking RL algorithms, e.g., simply bounding the action or\nadding a term weighted by a pre-determined coefﬁcient in the\nreward to penalize the violation of constraints. Satisfying the\nconstraints of RL tasks has been investigated in the context\nof safe RL [12]. The methods proposed along this line of\nresearch are designed for the situations where the safety of\nthe agent (say a robot) is particularly important, but only the\naverage constraints are considered. Whether these methods are\napplicable for optimizing wireless systems remains unclear.\nIII. MODEL-BASED UNSUPERVISED LEARNING\nIn this section, we show how to learn the solution of\noptimization problems as a function of the environment’s\nstatus using unsupervised DNN, where the model is known\nor at least partially known. In particular, the expression of the\nOF and CF is known, while the distribution of environment’s\nstatus can be unknown. We ﬁrst introduce how to equivalently\ntransform variable optimization problems to functional opti-\nmization problems. Then, we provide a uniﬁed framework for\nboth variable optimization and functional optimization.\nA. Learning Variable Optimization Without Labels\nRecall that the goal of “learning to optimize” the variable\noptimization problem P1 of Fig. 1 is to ﬁnd a function that\nmaps the environment’s status to the corresponding optimal\nsolution. Such a goal can be naturally achieved by functional\noptimization.\nTo formulate a functional optimization problem, one needs\nto ﬁnd a functional2 that maps the function to be optimized\ninto a scalar objective to serve as the “objective function”.\nFurthermore, if we can ﬁnd a functional to make the resul-\ntant functional optimization equivalent to P1, then variable\noptimization and functional optimization can be learned in a\nuniﬁed framework. As shown in [13], such a functional can be\nfound as the performance metric averaged over the distribution\n2A functional is a mapping from functions to scalars, e.g., an integral.\n4\nStatus\nPrimal-Dual \nUpdate\n Gradients of approximated OF and CFsa\n Gradients of the OF and CFsa\nObjective Function (OF)\nConstraint Functions (CFs)\n Values of the OF and CFsa\nMultiplier Network \nSolution x\nNoise\nMultiplier \nEnvironment\nValues of approximated OF and CFs\nminimizing the L2-norm Loss\nValue Networks\nValue Networks\nPolicy Network \nPolicy Network \nModel-Based\nUpdate value network parameters by\nUpdate MNP/PNP\nh\nh\nParameterized f(h)\nFig. 2.\nModel-based and model-free unsupervised learning for deterministic policy. The dashed rectangle represents the module required for model-based\nunsupervised learning, which takes the inputs x and h, and outputs the gradients of the OF and CFs for primal-dual update. The overlapped part between the\ndashed rectangle and the environment, i.e., the OF and CFs, is the partial knowledge on the environment required for model-based unsupervised learning. In\nmodel-free unsupervised learning, the value networks play the role of approximating the model.\nof the environment’s status. Speciﬁcally, the relationship be-\ntween the optimal solution of P1 and the environment’s status\nh can be obtained equivalently by ﬁnding a function (i.e., a\npolicy) f that maps h to x (i.e., x = f(h)) for maximizing\nthe average performance metric Eh[J(f(h), h)]. This suggests\nthat we can learn a variable optimization problem by learning\nits equivalent functional optimization problem, where the\noptimization “variable” is the function f(h).\nIn problem P1 and its equivalent functional optimization,\nthe constraints are imposed for every realization of the envi-\nronment’s status (i.e., instantaneous constraints). In practice,\nthere may also exist constraints imposed on a function aver-\naged over the environment’s status, e.g., average power con-\nstraints or average data rate constraints, as shown in P2. For\nmore general wireless optimization problems, the “variables”\nto be optimized consist of both functions and variables, as\nexempliﬁed by jointly optimizing a multi-timescale policy [6].\nFor convenient exposition, we introduce the unsupervised\nlearning framework for the general functional optimization\nproblem P2, and we use f(h) and x interchangeably, since the\nframework is applicable for both functional and variable opti-\nmization. To avoid using labels by repeatedly solving P2, the\nLagrangian function3 is used as the “loss” function for training\nthe DNNs in what follows. In order to compute the gradients\nof the Lagrangian function, the expressions of the performance\nmetric J(x, h), the functions in the instantaneous constraints\ngi(x, h), and the functions in the average constraints cj(x, h)\n3If an optimization problem does not have any constraint, the Lagrangian\nfunction degenerates into the average performance metric.\nhave to be available. For brevity, we only refer to J(x, h)\n(instead of Eh[J(f(h), h)]) as the OF, and only refer to the\nfunctions gi(x, h) and cj(x, h) (instead of Eh[cj(f(h), h)])\nas the CFs.\nB. Learning Generic Functional Optimization Without Labels\nTo handle the constraints in general functional optimization\nsystematically, P2 is reconsidered in its dual domain. When\nstrong duality holds, P2 is equivalent to ﬁnding the saddle\npoint of the Langrangian function [2] as in problem P3. This\nsaddle point can be found by optimizing the policy f(h) and\nthe Lagrangian multipliers for maximizing and minimizing the\nLangrangian function, respectively. We note that in contrast to\nthe functional optimization considered in [8], P2 also takes\ngeneral instantaneous constraints into consideration whose\nassociated multipliers in P3 of Fig. 1 are actually functions\nof the environment’s status [6].\nIn general, the functional optimization P3 is hard to solve,\nbecause we have to ﬁnd the optimal value of f(h) and the\nmultipliers for every possible value of h. By resorting to\nthe universal approximation theorem, neural networks can be\nused for parameterizing the policy and the multipliers in P3,\nwhich is more efﬁcient than FEM. In particular, a policy\nnetwork and a multiplier network can be employed for pa-\nrameterizing the policy f(h) and the multipliers, respectively.\nThen, P3 degenerates into a variable optimization problem\nthat optimizes the policy network parameters (PNP) and the\nmultiplier network parameters (MNP) for maximizing and\nminimizing the Langrangian function, respectively, subject to\n5\nthe constraints that none of the multipliers are less than zero,\nas shown in P4 of Fig. 1.\nIf the gradients of the OF and CFs w.r.t. x are derivable,\nthen the policy network and multiplier network can be trained\nby the primal-dual stochastic gradient method that iteratively\nupdates the PNP and MNP along the ascent and descent\ndirections of the Langrangian function’s gradients, respec-\ntively [14]. The distribution of the environment’s status does\nnot have to be known, because the sampled-averaged gradients\ncan be used for replacing the true gradients in the sense of\nensemble average. The constraint that none of the multipliers\nare less than zero can be satisﬁed by appropriately selecting\nthe activation function of the multiplier network’s output layer,\ne.g., the ReLU.\nThe procedure of learning functional optimization is sum-\nmarized in Fig. 2. Upon obtaining a sample (or a batch of\nsamples) of the environment’s status h, the policy network and\nthe multiplier network treat h as the input, and then outputs\nthe corresponding solution x (i.e., f(h)) and multipliers,\nrespectively. By taking x and h as the input, the gradients\nof the OF and CFs are computed based on the environment’s\nmodel, which is used for training the policy network and\nmultiplier network by primal-dual update. Given sufﬁcient\nnumber of the samples for characterizing the distribution of the\nenvironment’s status and sufﬁcient iterations for the primal-\ndual updates, the policy network is capable of approximating\nthe relationship between the optimal solution and the environ-\nment’s status.\nIV. MODEL-FREE REINFORCED-UNSUPERVISED\nLEARNING\nFor many problems in wireless networks, there is a lack\nof complete knowledge concerning the mathematical model,\ni.e., the expressions of the OF and CFs are unavailable or\ntheir gradients cannot be derived analytically. In this section,\nwe introduce model-free unsupervised learning that does not\nrequire direct derivation of those gradients.\nA. Learning Deterministic Policy\nIn practice, the values of the OF and CFs at (x, h) can be\nobserved after executing x (termed as action in the following)\nat the environment’s status h. For example, for a power\nallocation problem, we can quantify the data rate attained, the\ndelay imposed, and the energy consumed after transmitting at\npower x at channel state h.\nGiven the observations of the OF’s and CFs’ value, the\nﬁnite difference method can be used for estimating their gra-\ndients [8], which is however inefﬁcient when these functions\nhave high dimensions and hence they are not applicable to\nfunctional optimization problems having inﬁnite dimensions.\nAgain, based on the universal approximation theorem, the\nOF and CFs can be parameterized by neural networks with\nﬁnite parameters, which are termed as value networks in\nFig. 2. The value networks take x and h as the input and\nthen they output the approximated values of the OF and CFs\nat h. The observed values of the OF and CFs can be used\nfor supervising the training procedure [14]. Speciﬁcally, the\nvalue networks are trained by minimizing the L2-norm loss\nbetween their outputs and the observed values of the OF and\nCFs using SGD, as shown in Fig. 2. Then, the gradients of the\napproximated OF and CFs are computed via back-propagation\nof the value networks, which are then used for the primal-dual\nupdate of the PNP and MNP.\nThe employment of the policy network and value network\nof Fig. 2 is similar to the actor-critic approach of RL. If the\nfunctional optimization problem is not subject to constraints,\nthen the gradient of the Langrangian function w.r.t. the PNP\ndegenerates into the deterministic policy gradient in RL,\nwhich is used for training the actor in the deep deterministic\npolicy gradient (DDPG) algorithm [15].\nAnalogous to the actor-critic approach, the policy network,\nmultiplier network, and value networks of Fig. 2 can be trained\nsimultaneously via interactions with the environment. As\nshown in Fig. 2, upon observing the environment’s status, the\npolicy network outputs an action x applied to the environment.\nAfter observing the values of the OF and CFs evaluated\nbased on the environment, the value network parameters are\nupdated for obtaining a better approximation of the Lagrangian\nfunction. Meanwhile, the PNP and MNP are also updated\nrelying on the gradients computed from the value networks for\nimproving the policy. Because the OF and CFs are functions\nof the action, for accurately approximating their values and\ngradients at the executed action, it is necessary to obtain\ntheir values in the neighborhood of the executed action. For\nencouraging this exploration, a gradually diminishing noise\nterm is added to the output of the policy network throughout\nthe consecutive iterations.\nSince the training procedure does not require the labels\ngenerated by ﬁnding the numerical solutions of a problem\nfor every environment’s status, this approach is still under\nthe framework of unsupervised learning. However, since the\nunknown OF and CFs are learned via interacting with the envi-\nronment, we term this framework as reinforced-unsupervised\nlearning.\nB. Learning Stochastic Policy\nSo far, we have implicitly assumed that the policy to\nbe learned is a continuous function of h, and learned its\nparameterized form as a deterministic policy in both model-\nbased and model-free unsupervised learning frameworks. In\nsome scenarios, we aim for ﬁnding a discrete policy, e.g.,\nuser association, where parameterizing a deterministic policy\nis not applicable because the output of the neural network is\na continuous mapping of its input. Although a discrete policy\ncan be obtained by discretizing a learned deterministic policy,\nthe constraints may not be satisﬁed after its discretization.\nStochastic policies are widely used in RL for trading off\nexploration against exploitation [11]. For functional optimiza-\ntion, the policy network can also parameterize a stochastic\npolicy, which is applicable to both discrete and continuous\npolicy learning [8]. To realize a stochastic policy, the policy\nnetwork is designed to take the environment’s status as its\ninput and then output the probability distribution of the action\nto be executed. Then, for each environment’s status, the action\n6\nStatus\nPrimal-Dual \nUpdate\n Values of approximated OF and CFsa \nObjective Function (OF)\nConstraint Functions (CFs)\n Values of the OF and CFsa\nMultiplier Network \nAction distribution\nMultiplier \nEnvironment\nValues of approximated OF and CFs\nminimizing the L2-norm Loss\nValue Networks\nPolicy Network \nPolicy Network \nSample\nAction x\nUpdate MNP/PNP\nUpdate value network parameters by\nh\nh\nFig. 3. Model-free unsupervised learning for stochastic policy.\nto be executed is sampled from the distribution given by the\noutput of the policy network, as shown in Fig. 3.\nIn contrast to the deterministic policy scenario, the sampled\ngradients of the OF and CFs w.r.t. the executed action are\nno longer necessary for the primal-dual update in learning\na stochastic policy [14]. To train the policy and multiplier\nnetworks, the sampled gradients of the Lagrangian function\nw.r.t. the PNP and MNP are used for the primal-dual updates,\nwhich can be readily obtained via back-propagation [14].\nNevertheless, the sampled gradient in the primal-dual update\nof the stochastic policy may exhibit large variance [11],\nbecause x also has to be averaged for obtaining the gradients\nof the Lagrangian function, which results in slow convergence.\nAnalogous to the advantage actor-critic approach, a base-\nline term that averages the Lagrangian function over the\ndistribution of the action can be subtracted from the sampled\ngradient for reducing the variance [14]. To obtain the action-\naveraged Lagrangian function, the average value of the OF\nand CFs are again approximated by value networks, which\nare trained by minimizing the L2-norm loss using SGD, as\nshown in Fig. 3.\nThe on-line and off-line computational complexity of the\nDNN-based frameworks and the conventional optimization\nmethods (e.g., convex optimization, SDR, and FEM) are\nsummarized in Fig. 4.\nV. CASE-STUDY: USER ASSOCIATION\nThe\napplicability\nof\nmodel-based\nand\nmodel-free\nreinforced-unsupervised\nlearning\nfor\nsolving\nfunctional\noptimization has been conﬁrmed by solving a power control\nproblem in [14], where both learning approaches converge to\nthe optimal policy, while satisfying both instantaneous and\naverage constraints. Additionally, the convergence speed of\nmodel-free reinforced-unsupervised learning is close to that\nof model-based unsupervised learning.\nIn this section, we illustrate the performance of model-\nbased and model-free reinforced-unsupervised learning for\nsolving variable optimization. For convenient interpretation,\nwe consider a simple user association problem optimizing\ndiscrete variables. Consider a wireless network supporting K\nusers by B BSs. Each BS can support at most N users and\neach user can only be associated with one BS. We aim for\noptimizing the user association based on the received signal-\nto-noise ratios (SNRs) of every user w.r.t. each BS (which\nrepresents the environment’s status) to maximize the sum-rate\nof all the users.\nIn the simulation, the achieved sum-rate is computed by\nShannon’s formula, while during the learning process, the\nrelationship between the sum-rate and the SNRs is assumed\nto be unknown to show the applicability of the model-\nfree unsupervised learning when no model is available. We\nconsider the scenario with two BSs, three users, and each\nBS can support at most two users. The BSs are positioned\nalong a road with 500 m inter-BS distance. The minimum\ndistance between the BSs and the road is 100 m. The users\nare uniformly distributed along the road. Each user is served\nwith 20 W transmit power and 10 MHz bandwidth.\nTo learn the relationship between the optimal user asso-\nciation and the SNRs under the user association constraint,\nwe parameterize a stochastic policy. The policy network\ntakes the SNRs as its input and then outputs the probability\ndistribution of user association solution x. The value network\nis used for approximating the sum-rate averaged over x. A\nmultiplier network is used for ensuring that the number of\n7\nModel Free\nVariable Optimization\nFunctional Optimization\nModel Based\nConventional\nSupervised\nUnsuperivsed\nReinforced-Unsuperivsed\nMethod\nComplexity\nOn-line\nN/A\nHigh\nLow\nLow\nLow\nOff-line\nHigh\nHigh\nHigh\nConventional\nSupervised\nUnsuperivsed\nReinforced-Unsuperivsed\nMethod\nComplexity\nOn-line\nLow\nLow\nLow\nOff-line\nVery High\nHigh\nHigh\nVery High\nConventional\nSupervised\nUnsuperivsed\nReinforced-Unsuperivsed\nMethod\nComplexity\nOn-line\nLow\nOff-line\nHigh\nNot Applicable\nConventional\nSupervised\nUnsuperivsed\nReinforced-Unsuperivsed\nMethod\nComplexity\nOn-line\nLow\nOff-line\nHigh\nNot Applicable\nLow\nFig. 4. Complexity comparison between the DNN-based frameworks and conventional optimization methods.\nusers associated with each BS does not exceed N. In general,\nwe can also use the multiplier network for ensuring that the\nsum of the association probabilities for different BSs as one.\nHowever, to satisfy this probability constraint, we can simply\nuse the softmax as the activation function of the policy\nnetwork’s output layer, since such a probability constraint is\ncommon for all stochastic policies. The ReLU is used as the\nactivation function of the value and the multiplier networks’\noutput layers to yield positive outputs. We use the tanh as\nthe activation function for the hidden layers of all networks for\navoiding the performance loss caused by gradient vanishing.\nEach of the neural networks consists of two fully connected\nlayers and 20 neurons in each layer. The learning rates for\nall networks are 0.01/(1 + 0.001t), where t is the number of\niterations.\nWe note that for the stochastic policy, there is no sub-\nstantial difference between the model-free and the model-\nbased unsupervised learning, since the gradients of the OF\nand CFs are not required for the primal-dual updates of the\npolicy and multiplier networks. Therefore, we only compare\nthe model-free reinforced-unsupervised learning to the optimal\nsolution obtained via exhaustive search and to the supervised\nlearning that is trained via the supervision of the optimal\nsolutions. The convergence results are provided in Fig. 5. We\ncan see that when the number of iterations is low, the model-\nfree approach experiences lower average sum-rate and higher\nconstraint violation probability than the supervised learning\napproach. This is because the model-free approach has to learn\nthe model by exploring the environment. Upon increasing the\nnumber of iterations, the model-free approach converges to\nthe optimal solution and its performance becomes superior to\nthe supervised learning both in terms of the average sum-rate\nand constraint satisfaction.\nVI. CONCLUDING REMARKS\nIn this paper, we introduced uniﬁed frameworks of model-\nbased unsupervised and of model-free reinforced-unsupervised\nDNNs for learning to optimize variable optimization and\nfunctional optimization subject to instantaneous and average\n0\n0.5\n1\n1.5\n2\n2.5\n3\nNumber of iterations\n105\n23\n23.5\n24\n24.5\n25\nAverage Sum Rate (bps/Hz)\nOptimal\nModel-Free\nSupervised\n(a) Average rate.\n0\n0.5\n1\n1.5\n2\n2.5\n3\nNumber of Iterations\n105\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nConstraint Violation Probability\nSupervised\nModel-Free\n(b) Constraint violations.\nFig. 5. Convergence of learning the user association policy, where the average\nis taken over 500 successive iterations.\nconstraints. We illustrated how to apply the frameworks to\nlearn discrete variable optimization with the aid of a user\n8\nassociation problem. Our preliminary results show that these\nframeworks are capable of learning the optimal policy, while\nsatisfying the constraints after convergence. In contrast to\nRL, these frameworks are tailored to non-MDP problems\nsubject to constraints, which cover the majority of wireless\noptimization problems. Similar to RL, these frameworks are\ncapable of adapting to the changes of the environment’s status\neven without models. Nevertheless, how to reduce the number\nof iterations required for attaining convergence and achieve\nnear-optimal performance for more complex problems deserve\nfurther investigations.\nREFERENCES\n[1] F. Liang, C. Shen, W. Yu, and F. Wu, “Power control for interference\nmanagement via ensembling deep neural networks,” in Proc. IEEE/CIC\nICCC, 2019.\n[2] J. Gregory, Constrained optimization in the calculus of variations and\noptimal control theory.\nChapman and Hall/CRC, 2018.\n[3] C. She, C. Yang, and T. Q. Quek, “Cross-layer optimization for ultra-\nreliable and low-latency radio access networks,” IEEE Trans. Wireless\nCommun., vol. 17, no. 1, pp. 127–141, Jan. 2017.\n[4] S. Boyd and L. Vandenberghe, Convex optimization.\nCambridge Univ.\nPress, 2004.\n[5] O. C. Zienkiewicz, R. L. Taylor, P. Nithiarasu, and J. Zhu, The ﬁnite\nelement method.\nMcGraw-hill London, 1977, vol. 3.\n[6] C. Sun and C. Yang, “Unsupervised deep learning for ultra-reliable and\nlow-latency communications,” in Proc. IEEE GLOBECOM, 2019.\n[7] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D. Sidiropoulos,\n“Learning to optimize: Training deep neural networks for interference\nmanagement,” IEEE Trans. Signal Process., vol. 66, no. 20, pp. 5438–\n5453, Oct. 2018.\n[8] M. Eisen, C. Zhang, L. F. Chamon, D. D. Lee, and A. Ribeiro, “Learning\noptimal resource allocations in wireless systems,” IEEE Trans. Signal\nProcess., vol. 67, no. 10, pp. 2775–2790, May 2019.\n[9] L. Hoon, L. S. Hyun, and Q. Tony, “Constrained deep learning for\nwireless resource management,” in Proc. IEEE ICC, 2019.\n[10] F. D. Calabrese, L. Wang, E. Ghadimi, G. Peters, L. Hanzo, and\nP. Soldati, “Learning radio resource management in RANs: Framework,\nopportunities, and challenges,” IEEE Commun. Mag., vol. 56, no. 9, pp.\n138–145, Sept. 2018.\n[11] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT Press Cambridge, 1998.\n[12] J. Garcıa and F. Fern´andez, “A comprehensive survey on safe reinforce-\nment learning,” Journal of Machine Learning Research (JMLR), vol. 16,\nno. 1, pp. 1437–1480, 2015.\n[13] C. Sun and C. Yang, “Learning to optimize with unsupervised learning:\nTraining deep neural networks for URLLC,” in Proc. IEEE PIMRC,\n2019.\n[14] C. Sun, D. Liu, and C. Yang, “Model-free unsupervised learning for\noptimization problems with constraints,” in Proc. IEEE APCC, 2019.\n[15] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforcement\nlearning,” in Proc. ICLR, 2016.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-01-03",
  "updated": "2020-01-03"
}