{
  "id": "http://arxiv.org/abs/2001.01284v2",
  "title": "Learning Global and Local Consistent Representations for Unsupervised Image Retrieval via Deep Graph Diffusion Networks",
  "authors": [
    "Zhiyong Dou",
    "Haotian Cui",
    "Lin Zhang",
    "Bo Wang"
  ],
  "abstract": "Diffusion has shown great success in improving accuracy of unsupervised image\nretrieval systems by utilizing high-order structures of image manifold.\nHowever, existing diffusion methods suffer from three major limitations: 1)\nthey usually rely on local structures without considering global manifold\ninformation; 2) they focus on improving pair-wise similarities within existing\nimages input output transductively while lacking flexibility to learn\nrepresentations for novel unseen instances inductively; 3) they fail to scale\nto large datasets due to prohibitive memory consumption and computational\nburden due to intrinsic high-order operations on the whole graph. In this\npaper, to address these limitations, we propose a novel method, Graph Diffusion\nNetworks (GRAD-Net), that adopts graph neural networks (GNNs), a novel variant\nof deep learning algorithms on irregular graphs. GRAD-Net learns semantic\nrepresentations by exploiting both local and global structures of image\nmanifold in an unsupervised fashion. By utilizing sparse coding techniques,\nGRAD-Net not only preserves global information on the image manifold, but also\nenables scalable training and efficient querying. Experiments on several large\nbenchmark datasets demonstrate effectiveness of our method over\nstate-of-the-art diffusion algorithms for unsupervised image retrieval.",
  "text": "1\nLearning Global and Local Consistent\nRepresentations for Unsupervised Image\nRetrieval via Deep Graph Diffusion Networks\nZhiyong Dou, Haotian Cui, Lin Zhang, and Bo Wang, Member, IEEE,\nAbstract—Diffusion has shown great success in improving the accuracy of unsupervised image retrieval systems by utilizing\nhigh-order structures of image manifold. However, existing diffusion methods suffer from three major limitations: 1) they usually rely on\nlocal structures without considering global manifold information; 2) they focus on improving pairwise similarities within existing input\nimages transductively while lacking the ﬂexibility to learn representations for unseen instances inductively; 3) they fail to scale to\nlarge datasets due to prohibitive memory consumption and computational burden due to the intrinsic high-order operations on the\nwhole graph. In this paper, to address these limitations, we propose a novel method, Graph Diffusion Networks (GRAD-Net), that\nadopts graph neural networks (GNNs), a novel variant of deep learning algorithms on irregular graphs. GRAD-Net learns semantic\nrepresentations by exploiting both local and global structures of image manifold in an unsupervised fashion. By utilizing sparse coding\ntechniques, GRAD-Net not only preserves global information on the image manifold, but also enables scalable training and efﬁcient\nquerying. Experiments on several large benchmark datasets demonstrate the effectiveness of our method over existing state-of-the-art\ndiffusion algorithms for unsupervised image retrieval.\nIndex Terms—Unsupervised Image Retrieval, Diffusion, Graph Neural Networks, Sparse Coding.\n!\n1\nINTRODUCTION\nU\nNSUPERVISED image retrieval refers to the task of\nreturning relevant instances in a database given an\nunlabeled query, which is an important basis for various\napplications such as information search and database man-\nagement [1], [2]. Traditionally, unsupervised image retrieval\nis accomplished by computing either predeﬁned pairwise\nsimilarities (e.g. Euclidean distances) or by adopting a\nrandom-walk style process (e.g. diffusion [3]). It is widely\nacknowledged that no single metric can generate reliable\nretrieval performance due to the “curse” of dimensionality\n[4]. Consequently, most of the existing works on unsuper-\nvised image retrieval have been focusing on exploiting the\ndiffusion process to learn context-sensitive afﬁnity measures\n[5]–[7].\nThe principle of capturing geometric manifold applies\nto most existing diffusion methods. First, the manifold is\ninterpreted as a weighted graph, where each instance is\nrepresented by a node, and weights on the edges connecting\n•\nZ. Dou is with the University of Toronto, Toronto, Ontario M5S 1A1,\nCanada. He is also with the School of Electronic Information and Com-\nmunications, Huazhong University of Science and Technology, Wuhan,\nHubei 430074, China.\nE-mail: zydou@hust.edu.cn\n•\nH. Cui is with the University of Toronto, Toronto, ON M5S 1A1, Canada.\nHe is also with Vector Institute, Toronto, ON M5G 1M1, Canada.\nE-mail: htcui@cs.toronto.edu\n•\nL. Zhang is with the University of Toronto, Toronto, ON M5S 1A1,\nCanada.\nE-mail: linzhang@utstat.toronto.edu\n•\nB. Wang is with Peter Munk Cardiac Centre, Toronto, ON M5G 2N2,\nCanada. He is also with Vector Institute, Toronto, ON M5G 1M1 and the\nUniversity of Toronto, Toronto, ON M5S 1A1, Canada.\nE-mail: bo.wang@uhnresearch.ca\nZhiyong Dou and Haotian Cui contribute equally. Corresponding author: Bo\nWang. E-mail: bo.wang@uhnresearch.ca\nnodes represent the similarities. The pairwise afﬁnities are\nthen updated iteratively by diffusing along the graph geom-\netry. This diffusion process, originally proposed by Zhou et\nal. [8], typically follows the concept of random walk, where a\ntransition matrix derived from the edge weights determines\nthe probabilities of transiting from one node to another.\nThe updated afﬁnity values in turn improve the retrieval\nresults. To illustrate this concept, we consider the following\ntoy example. As shown in Fig. 1, each point is a simple\nanalogy to the feature vector of an image. The data points\nconsist of four letters, each with 1,500 points. The queries\nare displayed as the star nodes. The ideal result for this\nretrieval is that the points from the same letter as the query\npoint should have the highest ranks. Euclidean distance as\na similarity metric is inadequate for this task (Fig. 1a), while\nin comparison, after diffusing the similarities on the graph,\nthe retrieval result is signiﬁcantly improved (Fig. 1b).\nThe success of deep neural networks on various applica-\ntions partly results from the powerful image features with\nrich semantic information. Models, particularly deep con-\nvolutional networks [9]–[12] pre-trained on large datasets,\nsuch as ImageNet [13] and Landmarks [9], are increasingly\nbeing used for feature extraction. Diffusion is then deployed\non the extracted features to further improve the retrieval\nresults.\nHowever, these diffusion-based methods suffer from\nthree main limitations. First, they usually rely on local\nstructures without considering global manifold information,\nthat is sparsiﬁed graphs based on neighborhood search are\nextensively used without considering the cluster structures\nas a whole. Secondly, these diffusion-based methods focus\non improving pairwise similarities within existing images\ntransductively but lack the ﬂexibility to learn representations\narXiv:2001.01284v2  [cs.CV]  11 Jun 2020\n2\nFig. 1: The retrieval results on a synthetic toy dataset using\n(a) Euclidean distance and (b) diffusion. Four queries are\nmarked as stars.\nfor unseen instances inductively. In other words, it is difﬁcult\nto generalize the diffusion models to unobserved databases\nwithout re-computing the diffusion iterations. Lastly, they\nusually fail to scale to large datasets due to prohibitive mem-\nory consumption and computational burden resulting from\nthe intrinsic high-order operations on the whole graphs.\nGraph neural networks (GNNs) have recently emerged\nas a promising line of research that adopts convolutional\noperators on irregular inputs like graphs [14]–[17]. For\ninstance, there has been an increasing interest in applying\nGNNs to citation network analysis [14], [18], collabora-\ntive ﬁltering [19] and knowledge graphs [20], all of which\ndemonstrated state-of-the-art performance. However, most\nexisting GNNs rely on supervised learning to train an\neffective model, which prevents direct adoption of GNNs\nto the unsupervised image retrieval tasks due to the lack of\nlabeled data.\nIn this paper, we propose a novel approach, Graph\nDiffusion Networks (GRAD-Net), for unsupervised image\nretrieval. GRAD-Net consists of GNNs that are trained with\ntwo loss functions that directly depict the diffusion process\nwithout any labeled information. The diffusion process is\nthus fully learnable, and the features are trained to trans-\nform on an optimal manifold subject to the deﬁned loss\nin an unsupervised fashion. Beneﬁting from the general-\nizability of GNNs, our model can be easily extended to\nunseen queries. Furthermore, to consider the global struc-\ntures, GRAD-Net constructs an additional bipartite graph\nto learn consistent global and local representations of all\nthe images. GRAD-Net alleviates the common scalability\nissue of diffusion methods by effectively sampling sub-\nnetworks in a mini-batch manner at each training iteration.\nWe demonstrate the effectiveness of GRAD-Net with exten-\nsive empirical experiments.\nOur main contributions are summarized as follows:\n1)\nUnsupervised representation learning on image\nmanifolds. GRAD-Net, among the ﬁrst-in-class\ndeep learning models for unsupervised diffusion\nprocess on image retrieval, updates the instance\nfeatures (i.e. the node attributes on the graphs) in\nan end-to-end manner rather than the conventional\nchoice of diffusion rank values or similarities on\nmanifolds. We transform the conventional diffusion\nfrom a message-passing process to an optimization\nprocess through a multi-layer neural networks with\nan efﬁcient diffusion-like operator. We show that\nthis approach enriches the features with more se-\nmantic information, and the learned features can\nbe used to perform efﬁcient retrieval with a sim-\nple nearest neighbor search and support effective\ninductive learning.\n2)\nAchieving global and local consistency. We in-\ntroduce a series of novel techniques including: (i)\nsecond order propagation that accelerates the mes-\nsage aggregation in GNNs; (ii) local and global loss\nfunctions that leverage the structure of the manifold;\n(iii) sparse coding that adopts a bipartite graph\ntechnique on top of the sparsiﬁed pairwise graphs\nto reﬂect the global cluster information. These novel\ntechniques collectively contribute to GRAD-Net to\nachieve global and local consistency.\n3)\nIntrinsic scalability and high efﬁciency. Unlike tra-\nditional diffusion methods that suffer from at least\nquadratic computational complexity, our proposed\nGRAD-Net has linear complexity and is intrinsically\nscalable to large databases through mini-batch train-\ning on the graphs. GRAD-Net has proven its high\nefﬁciency in both training and query processes on\nvery large-scale datasets.\n2\nRELATED WORK\n2.1\nDiffusion for Unsupervised Image Retrieval\nMetric learning has been a central task for image retrieval.\nThere is a myriad of works focusing on learning a general\nmetric to compute pairwise distances between images [21],\n[22]. In contrast to conventional metric learning methods,\ndiffusion-based methods have emerged as a promising alter-\nnative that learns pairwise similarities on image manifolds.\nIntroduced by Zhou et al. [8], diffusion has shown consistent\nimprovements over raw distances/similarities by exploit-\ning intrinsic manifold geometry [23]. Inspired by semi-\nsupervised learning, Graph Transduction (GT) [24] takes the\nquery point as the only labeled data, and propagates the\nlabeled point to unlabeled databases in a similar fashion to\nlabel propagation [25]. Motivated by the observation that a\ngood ranking is usually asymmetrical, Contextual Dissim-\nilarity Measure (CDM) [26] improves Bag-of-Words (BoW)\n[27] retrieval system by iteratively estimating the pairwise\ndistance in the spirit of Sinkhorns scaling algorithm [28].\nFurther, noticing that diffusion is susceptible to noise\nedges in the afﬁnity graph, Locally Constrained Diffusion\nProcess (LCDP) [29] stresses that it is crucial to constrain\nthe diffusion process “locally”. Along the same line, Tensor\nProduct Graph diffusion (TPG) [30] manages to leverage\nthe high-order information from the tensor product of the\nafﬁnity graphs. However, TPG constrains the pathways\nof message passing to local neighbors only such that the\ncomputational complexity does not increase signiﬁcantly. A\ndetailed comparison of various diffusion methods has been\nconducted in a survey [3] by enumerating 72 variants of\ndiffusion process (4 different afﬁnity initializations, 6 differ-\nent transition matrices and 3 different updating schemes).\n3\nBai et al. [31] provides empirical and theoretical results and\nsuggests that metric learning on tensor product graphs with\nhigh-order information is more robust to retrieval. Recently,\nBai et al. [6] enhances the tensor product to a regularization\nprocess on graphs and displays its potential of retrieval for\nheterogeneous instances.\n2.2\nGraph Neural Networks\nRecent years have witnessed an increasing trend in applying\ndeep learning algorithms to irregular inputs such as graphs.\nEarly works include recursive neural networks [32] that\nrepresent and process graph data. GNNs were introduced\nby Gori et al. [33] and Scarselli et al. [34] as a general-\nization of recursive neural networks that can directly deal\nwith graphs. Typical GNNs consist of an iterative random-\nwalk process in which node states are propagated based\non certain probability distribution until convergence. This\nidea was improved by Li et al. [35] that uses gated recurrent\nunits [36] in the propagation step. However, these recurrent\nmodels usually suffer from large computational burdens\nand therefore hard to train and scale to large graphs.\nInspired by the tremendous success of convolutional\nneural networks (CNNs) in computer vision, Bruna et al.\n[37] introduced the convolution operation in the Fourier\ndomain by computing the eigendecomposition of the graph\nLaplacian. It was then improved by Henaff et al. [38] that pa-\nrameterizes the spectral ﬁlters with smooth coefﬁcients such\nthat the computations are spatially localized. The milestone\nwork by Kipf et al. [14] introduced graph convolutional\nnetwork (GCN) that simpliﬁes the previous methods by\nrestricting the ﬁlters to operate in the ﬁrst-order neighbor-\nhood around each node. Since then, various modiﬁcations\nhave been proposed to improve upon GCN. For example,\nVelikovi et al. [39] adopted the attention mechanism to\naddress the high sensitivity to noise issue in the graph\nedges of GCN by computing learnable attentions among the\nnodes.\nThe applications of GNNs in computer vision are boom-\ning with notable examples such as few-shot image clas-\nsiﬁcation [40], [41], semantic segmentation [42], [43] and\nvisual question answering [44], [45]. However, most of\nthese applications require an extensive amount of labeled\ndata. Recently, Jiang et al. [46] introduces new GNN layer\noperations for feature diffusion in a semi-supervised setting,\nand Guided Similarity Separation (GSS) [47] applies a Kipf-\nstyle GCN [14] to unsupervised image retrieval but lacks the\nscalability to large datasets. Apart from the aforementioned\nworks, few has focused on unsupervised image retrieval.\nThe proposed GRAD-Net, to our best knowledge, is one\nof the ﬁrst attempts to adopt GCN-like algorithms in un-\nsupervised image retrieval. Our proposed network requires\nno label information yet extends the traditional diffusion\nmodels to learn global and local consistent representations\nand enables effective training and querying on large-scale\ndatasets.\n3\nOUR APPROACH\nIn this section, we introduce the detailed components of\nthe proposed Graph Diffusion Network (GRAD-Net). We\nﬁrst review the theoretical bases of the diffusion process on\ngraphs and GCN, and then elaborate on the structures of our\nproposed GRAD-Net. Though our proposed method can be\neasily generalized to the retrieval of various types of data,\nin this work, we will focus on image retrieval to illustrate\nthe idea of GRAD-Net.\n3.1\nProblem Setup and Notations\nFor image retrieval tasks, we denote the dataset as X =\n{x1, x2, . . . , xn} ⊂Rd, where xi is a feature vector of an\nimage. We assume the set of queries can be projected to\nthe same feature space as the instances in the dataset, and\ndenote the queries as Q = {q1, q2, . . . , qm} ⊂Rd, where qi\nrepresents the feature vector of the i-th query image and m\nis the number of query images. For simplicity, but without\nloss of generality, we consider the scenario that the instance\n(or query) is interpreted as a single vector. The combined set\nof dataset instances and queries can then be expressed as\n¯\nX = X ∪Q = {q1, ..., qm, x1, ..., xn},\n(1)\nand we use ¯\nXi denotes the i-th instance in ¯\nX .\n3.2\nDiffusion on Graphs\nA graph G = {V, E} consists of a set of nodes V and edges\nE. An afﬁnity matrix A represents the adjacent node pairs\nin the graph. Two main approaches to conduct diffusion on\ngraphs are (i) iteratively updating the pairwise similarities\n[3], [8] and (ii) solving the closed form of Eq.4 directly [5].\nHowever, both approaches are essentially based on the same\nrandom walk mechanism proposed by Zhou et al. [8].\nTo perform a random walk on a graph G, a transition ma-\ntrix is introduced to describe the probability of walking from\none node to another, which is considered to be proportional\nto the afﬁnity value. A degree matrix D is introduced to\nnormalize the afﬁnity matrix, which produces the transition\nmatrix. The degree matrix\nDi,j :=\n(Pn+m\nk=1 aik,\nif i = j\n0,\notherwise\nis a diagonal matrix with each diagonal element correspond-\ning to the row-wise sum of a predeﬁned afﬁnity matrix A.\nThen the transition matrix S is computed as:\nS = D−1/2AD−1/2.\n(2)\nBased on the transition matrix S, random walk is then\nperformed on the graph to update a state vector f t ∈Rn+m\nuntil it converges. The process iterates at the t-th step of the\nrandom walk as follows,\nf t+1 = αSf t + (1 −α)f 0, α ∈(0, 1),\n(3)\nwhere f t = [f t\nq\nT , f t\nd\nT ]T is composed of both the query state\nf t\nq ∈Rm and the dataset state f t\nd ∈Rn. The initial state f 0\nis a binary vector set to f 0 = [f 0\nq\nT , f 0\nd\nT ]T = [1T , 0T ]T . Eq.3\ncan be intuitively interpreted as follows. Given a state f t, it\ntransits based on S with a probability of α and restarts from\nthe initial state with a probability of (1 −α). This process\nwill converge to a closed-form solution [8]:\nf ∗= (1 −α)(I −αS)−1f 0\n(4)\n4\nThe ﬁnal state, f t=T , obtained after certain iterations\nor derived directly from the closed-form solution, repre-\nsents the similarities between the dataset instances and the\nqueries, which eventually determines the rank of the dataset\ninstances.\nHowever, Eq.3 does not explicitly consider the high-\norder information on the manifold. Yang et al. [30] addressed\nthis issue and introduced a modiﬁed diffusion on a tensor\nproduct graph where nodes refer to the instance pairs,\nwhich naturally takes the higher-order information into\naccount. The iteration step in the Tensor Product Graph\ndiffusion (TPG diffusion) is:\nˆAt+1 = S ˆAtST + I.\n(5)\nThe ﬁnal\nˆAt=T\n∈Rn×n after iterations stands for the\nupdated afﬁnity matrix. As argued by Donoser et al. [3],\nthe TPG diffusion achieved the most robust performance\namong all the compared methods. However, TPG diffusion\nsuffers from heavy computational burden when handling\nlarge networks due to the large-scale tensor product graph.\nOur proposed method adopts a second-order operator sim-\nilar to TPG diffusion, but overcomes the scalability issue\nby applying the second-order operator on a mutual k-NN\ngraph (see detailed description in Section 3.4).\n3.3\nGraph Convolutional Networks\nGCN [14] applies a ﬁrst-order aggregation on graphs using\nafﬁnity matrices. GCN contains several hidden layers that\ntake a feature matrix H(l) ∈Rn×dl as the input of the l-th\nlayer and update the feature matrix H(l+1) ∈Rn×dl+1 by\nusing a graph convolution operator.\nGiven an input feature matrix H(0) ∈Rn×d0 and the\ngraph afﬁnity matrix A ∈Rn×n, GCN conducts the follow-\ning layer-wise propagation,\nH(l+1) = σ((I + S)H(l)W (l)),\n(6)\nwhere l ∈{1, 2, ..., L} is the layer index and S is the same\nas in Eq.2. W l ∈Rdl×dl+1 is the trainable weight matrix\nof the l-th layer. σ(·) denotes an activation function. Eq.6\nadopts S rather than A because multiplication with A will\ncompletely change the scale of the feature vectors, which can\ncause numerical instabilities. To further alleviate this issue,\nKipf et al. [14] suggested to use the re-normalized (I + S) as\nfollows:\nI + D−1/2AD−1/2 →ˆD−1/2 ˆA ˆD−1/2,\n(7)\nwhere ˆD is the diagonal degree matrix of ˆA = A + I.\n3.4\nGraph Diffusion Networks (GRAD-Net)\nIn this section, we present our proposed GRAD-Net, il-\nlustrated in Fig. 2, that is composed of multiple graph\ndiffusion layers. These layers aggregate messages between\nnodes according to the manifold structure and output the\nupdated node features. Lastly, an afﬁnity estimation module\ntakes the output features and estimates afﬁnities. Apart from\nthe local manifold structure information, a sparse coding\nfeature vector Z is also passed to the graph diffusion layer to\nprovide global structure information. GRAD-Net is trained\nwith two novel unsupervised loss functions, which we will\ndiscuss in details in Section 3.5. The details of the proposed\nGRAD-Net are as follows:\n3.4.1\nGraph Constructions\nWe ﬁrst introduce how to construct graphs as the input to\nGRAD-Net. To encode both local and global structures from\nthe image manifold, we design two types of graphs: local\nsparsiﬁed graph and global bipartite graph.\nLocal Sparsiﬁed Graph. To construct the graphs of locality,\nwe follow the mutual k-NN method in Iscen et al. [5] that\ncomputes the k nearest neighbors of every instance. The\ndataset is interpreted as a weighted graph G = (V, E),\nconsisting of N = n + m nodes vi ∈V , and the edges\neij ∈E connect those node pairs that have a non-zero\nafﬁnity value, for i, j ∈{1, 2, ..., N}. The afﬁnity matrix\nA = [aij] ∈RN×N is deﬁned as\naij =\n(\ns( ¯\nXi, ¯\nXj),\nif ¯\nXi ∈NNk( ¯\nXj), ¯\nXj ∈NNk( ¯\nXi)\n0,\notherwise\n, (8)\nwhere NNk( ¯\nXi) denotes the set of k nearest neighbors of\nnode i, and s : Rd × Rd →R is a predeﬁned similarity\nmetric typically based on cosine similarity or Euclidean\ndistance. Eq.8 enables A to be sparse when k is small, which\nmakes diffusion on large graphs feasible. The edge weight is\ninitialized as aij, and then the diffusion processes propagate\nthe afﬁnity values through the entire graph.\nGlobal Bipartite Graph by Sparse Coding. To efﬁciently\nencode the global structure, we adopt a similar sparse\ncoding in Liu et al. [48], in which we select anchor nodes\nto represent the original data distribution and reconstruct\nthe original nodes.\nWe ﬁrst apply a simple clustering algorithm, K-means,\nto select B anchor nodes, and denote the feature matrix\nof these anchor nodes as U ∈RB×d. We then construct\na bipartite graph Gg = {Vg, Eg}, where Vg is the union\nof the original image nodes and the selected anchor nodes\n(i.e. |Vg| = N + B), and Eg denotes all the edges between\noriginal images nodes and anchor nodes (i.e. Eg ∈RN×B).\nLet Z ∈RN×B denote all the weights on the edge set Eg.\nWe reconstruct each node by learning the edge weights, zi,\nthat connect xi to its c closest neighboring anchor nodes by\nmin\nzi ∥xi −Uzi∥2\n2, s.t., ℓT zi = 1, zi ≥0, |zi| = c.\n(9)\nThe optimization of Eq.9 is a typical sparse coding prob-\nlem, which we adopt the same technique as in Lan et al. [49].\nWe then obtain the global bipartite graph with edge weights\nZ ∈RN×B, and concatenate Z as additional features to the\nimage descriptors ¯\nX .\n3.4.2\nGraph Diffusion Layers.\nThe input graph G consists of an afﬁnity matrix A ∈RN×N\nand instance features ¯\nX as in Eq.1. We denote the input\nfeature matrix H(0) ∈RN×d in the ﬁrst layer as,\nH(0) = [ ¯\nX1, ¯\nX2, ..., ¯\nXN]T\n(10)\nThe graph diffusion layer of GRAD-Net has a layer function\nfd as follows,\nH(l+1) = fd(H(l), A; W (l)),\n(11)\nwhere W (l) is the trainable parameters of the l-th layer,\nand H(l) ∈Rn×dl and H(l+1) ∈Rn×dl+1 are the input\nand output features of the l-th layer, respectively. dl denotes\n5\nImage database\nImage features\nConcat\nBipartite Graph\nFeature\nExtraction\nk-NN Graph\nGraph Diffusion \nLayer\nNode sampling\nLoss computation\nGraph Diffusion Layer\nLeaky ReLu\nFirst order\nSecond order\nFig. 2: The framework of GRAD-Net. Given the input image features, GRAD-Net ﬁrst constructs local sparsiﬁed k-NN\ngraph and bipartite graph for the image instances. GRAD-Net reﬁnes the image features by the stacked graph diffusion\nlayers that are optimized by global and local loss functions. The right panel is a detailed structure of the graph diffusion\nlayer in GRAD-Net.\nthe dimensions of the features in l-th layer. The total layer\nfunction fd consists of a ﬁrst-order operator and a second-\norder operator.\nThe ﬁrst-order operator, f1, is a function that generates\nan updating message for node u based on its ﬁrst-order\nneighbor node i. The ﬁrst-order message generated by f1\nis,\nm(l)\nu←i = f (l)\n1 (h(l)\ni , p(l)\nu←i),\n(12)\nwhere h(l)\ni\ndenotes the feature of node i in the l-th layer,\nand p(l)\nu←i controls the scale of this message. We employ a\nsimilar ﬁrst-order transition operation as in the conventional\ndiffusion (i.e. Sf t in Eq.3) in our graph diffusion layer, and\nincorporate additional trainable parameters W1. In the l-th\nlayer, f1 is deﬁned as,\nf (l)\n1\n= SH(l)W (l)\n1 ,\n(13)\nwhere W (l)\n1\n∈Rdl×dl+1. This function turns out to share a\nsimilar form as the GCN layer in Eq.6.\nThe second-order operator, f2, is a function that takes\nin the information of node j, a second-order neighbor of\nnode u (i.e. the neighbor of the neighbor of u), to update\nthe feature vector of node u. The second-order message\ngenerated by f2 is,\nm(l)\nu←←j = f (l)\n2 (h(l)\nj , p(l)\nu←←j),\n(14)\nwhere h(l)\nj\nis the feature vector of node j at the l-th layer, and\np(l)\nu←←j controls the scale of this message. The double arrow,\n←←, represents the second-order connectivity between node\nj and node u.\nThe challenge of designing p(l)\nu←←j lies in the fact that\nnode j and u are not adjacent, i.e. aju = 0. Inspired by the\nTPG diffusion of Eq.5 [30], we introduce f2 as\nf (l)\n2\n= S(SH(l) ⊙H(l))W (l)\n2 ,\n(15)\nwhere ⊙denotes the element-wise product, and W (l)\n2\n∈\nRdl×dl+1 is the trainable parameters at the l-th layer. The\nsecond-order operator, f (l)\n2 , can be interpreted as a two-step\nhop. Let node i be the shared neighbor of node j and u.\nThe product SH(l) is a one-step of message passing that\nresembles the ﬁrst-order operation in Eq.13, i.e. m(l)\ni←j. Then\nm(l)\ni←j ⊙H(l) can be considered as the weighted feature of\nnode i. Lastly, S(m(l)\ni←j ⊙H(l))W (l)\n2\npasses the message from\nnode j to u through their common shared neighbor node i.\nThis second-order propagation enlarges the representational\npower of the graph diffusion layer.\nCollectively, the graph diffusion layer performs the prop-\nagation as\nH(l+1) = fd(H(l), A; W (l)) = σ(m(l)\nu←i + m(l)\nu←←j)\n= σ((I + S)H(l)W (l)\n1\n+ S(SH(l) ⊙H(l))W (l)\n2 ),\n(16)\nwhere σ is an activation function and is set to LeakyRelu [50]\nin GRAD-Net, and W (l)\n1 , W (l)\n2\n∈Rdl×dl+1 are the trainable\nparameters. An identity matrix, I, is added to the ﬁrst order\noperation to introduce a self-loop propagation. The learned\nfeatures of GRAD-Net can be the output of the last layer,\nH(L), or the concatenated layer features, H,\nH = H(0)∥H(1)∥...∥H(L),\n(17)\nwhere ∥is the concatenation along the feature dimension.\nAfﬁnity Estimation. Pairwise similarity is calculated using\nthe concatenated layer features, H ∈RN×D (Eq.17). GRAD-\nNet adopts cosine similarity. The similarity between node i\nand node j, sij, is\nsij = cosine similarity(hi, hj) =\nhi · hj\n∥hi∥∥hj∥,\n(18)\nwhere hi ∈RD and hj ∈RD are the i-th and j-th rows of\nH, respectively.\n3.5\nLoss Functions for Global and Local Consistency\nLoss functions are crucial for training the GRAD-Net model.\nTo capture both local and global structures of the underlying\ndata manifold, we propose two loss functions to optimize\nlocal smoothness and global order, which will be illustrated in\nthis section.\nLocal smoothness refers to the assumption that if two nodes\nare topologically closer in the graph G, the similarity of their\nfeatures should be higher. To regularize local smoothness,\nwe propose a similar loss as the pairwise Bayesian Person-\nalized Ranking (BPR) loss [51],\nLlocal =\nX\n(i,j,u)∈Olocal\n−ln(sij −siu),\n(19)\n6\nFig. 3: Illustration of sextet. (i) Triplet node (u, i, j): xi and\nxj are ﬁrst-order neighbors, and xu is a non-neighbor of\nxi on the graph. (ii) Quadruplet nodes (i, j, k, l): xi and\nxj are ﬁrst-order neighbors and xl and xk are ﬁrst-order\nneighbors. (iii) Sextet (i, j, k, l, u, v): the collection of xi, xj,\nxk, xl, xu and xv. Global order. slj and ski both measure the\nsimilarities between two neighborhoods and thus should\nbe similar. Llocal takes in the triplet nodes (u, i, j). Lglobal\ntakes in the quadruplet nodes (i, j, k, l).\nwhere sij is the similarity between the features of instances\ni, j, Olocal = {(i, j, u)|j ∈Ni, u /∈Ni} denotes the triplet\ntraining samples, and Ni refers to the nodes that share direct\nedge connections with i in the graph G.\nThe local smoothness loss in Eq.19 can be interpreted\nas the difference of the similarities between a close node-\npair and a distant node-pair. The local loss function en-\nforces larger similarities between the nodes in the same\nlocal neighborhood, which is consistent with the empirical\nﬁndings that the diffusion process should be performed\nlocally on the manifold [3], [29].\nGlobal Order refers to the assumption that the similari-\nties measured by different nodes from two neighborhoods\nshould remain consistent [6], [30]. As illustrated in Fig. 3, if\nnode i and j are from one neighborhood while node k and\nl are from another neighborhood, both slj and ski should\nreﬂect the similarities between the two neighborhoods, and\nthus should be similar under the global order assumption.\nTo enforce global order, we introduce a loss function suitable\nfor neural networks training,\nLglobal =\nX\n(i,j,k,l)∈Oglobal\nln(1 + β · aijaklskisli(ski −slj)2),\n(20)\nwhere Oglobal = {(i, j, k, l)|j ∈Ni, j ∈Nk}, β ∈R+ is\na weighting coefﬁcient, aij is the afﬁnity in the manifold\nafﬁnity matrix A, and ski is the similarity deﬁned in Eq.18.\nThe total loss function is a weighted combination of the\nlocal and global loss functions:\nL = Llocal + αLglobal + λ∥Θ∥2\n2,\n(21)\nwhere Θ is the set of all the trainable parameters and ∥·∥2\n2 is\nthe l2-norm function. α ∈R+ and λ ∈R+ are the weights\nof different loss components.\nThe local loss in Eq.19 works on triplet nodes (u, i, j),\nwhile the global loss in Eq.20 works on quadruplet nodes\n(i, j, k, l). Considering this, we provide a compact solution\nto implement the training process. For every training it-\neration, a sextet (i, j, k, l, u, v) is selected to compute the\ntotal loss (Eq.21) as shown in Fig. 3, where the local loss\nLlocal is calculated twice from the two triplets (i, j, u) and\n(k, l, v), while the global loss Lglobal is calculated once from\nthe quadruplet (i, j, k, l). Therefore, the implemented loss\nfunction is\nL (i,j, k, l, u, v)\n=\n1\nNbatch\nX\nbatch\n{Llocal(i, j, u) + Llocal(k, l, v)\n+ αLglobal(i, j, k, l) + λ∥Θ∥2\n2},\n(22)\nwhere Nbatch is the number of sextets in a mini batch.\n3.6\nInductive Learning on Unseen Instances\nMost existing methods assume the queries are observed\nin the dataset, yet this setting cannot be met in many\nreal-life scenarios. When queries are only accessible in the\nproduction stage after deployment, conventional diffusion\nmethods suffer substantially from computation burden, and\nthus lack the feasibility to process queries in real-time. One\nsolution to this issue is Query Expansion (QE) [52], in which\na new query is constructed by averaging the image features\nof its top nearest neighbors in the original database.\nWe extend QE to derive a fast query expansion approach,\nthe query feature expansion (QFE), that is compatible with\nour feature learning fashion. Given a new query q, QFE ﬁrst\nﬁnds the k nearest neighbor Nq of q using original image\ndescriptors, and directly computes the features of q in the\nlearned feature space by\nhq =\nX\ni∈Nq\nsqihi,\n(23)\nwhere hi is the learned feature of node i, and sqi is the\nsimilarity between q and its neighbor i computed by the\noriginal image descriptors. hq is a feature approximation for\nq and can be used to retrieve instances via similarity search.\nWe demonstrate the effectiveness of QFE with empirical\nresults in Section 5.3.\n3.7\nTraining on Large Datasets\nMini-batch Training on Graph. We found out that only the\nnodes in the computation ﬂow of the loss function (Eq.21)\nwill contribute to the parameter updating process.\nTo alleviate the computation burden on large-scale\ndatasets, we adopt a mini-batch approach similar to\nBreadth-First Search (BFS) [53] on the sparsiﬁed mutual k-\nNN graph. We start from a batch of sextets (Nbatch×6 nodes)\nto ﬁnd N ′\nbatch relevant nodes, and then only include these\nN ′\nbatch of nodes in each training iteration. This acceleration\nprocess remarkably speeds up by over 20 times on training\nlarge-scale datasets (e.g. Oxford105k [5]).\nTruncation. As image retrieval on very large datasets is\nchallenging to diffusion-based methods, truncation has be-\ncome a common practice to alleviate this issue [5], [6].\nThough our proposed GRAD-Net is capable of training and\n7\nretrieving datasets with over 100k instances efﬁciently, we\nimplement a similar truncation method to further accelerate\nthe training process. GRAD-Net ﬁrst forms a union graph of\nthe top 500 nearest neighbors for each query, and uses the\nlearned features based on this union graph for the retrieval\ntask. We demonstrate the efﬁcacy of mini-batch training and\ntruncation on various scales of datasets in Fig. 8.\n3.8\nImplementation Details\nIn this section, we address several implementation consid-\nerations, and introduce the default conﬁgurations of GRAD-\nNet. For all the experiments in Section 4 and 5, we deploy\nthe default conﬁgurations unless otherwise speciﬁed.\nWe implemented GRAD-Net on the PyTorch[54] frame-\nwork. All the experiments were conducted on a workstation\nwith a 12-core Intel(R) Xeon(R) CPU W-2133 @ 3.60GHz and\none NVIDIA RTX5000 GPU with 16GB GPU memory.\nThe number of neighbors, k, in the mutual k-NN search\nis set to 15. We set the number of anchor points B = 100\nto compute the sparse features Z. The similarity metric s in\nEq.8 is set to 1/(1 + Euclidean Distance). The number of\ngraph diffusion layers, L = 3, with d1 = 1024, d2 = 256\nand d3 = 128. An l2-normalization followed by dropout\n[55] with a probability of 0.3 is applied to the output of each\nlayer. We set β = e5 in Eq.20, α = 1.0 and λ = e−5 in\nthe total loss function (Eq.22). The sextet (i, j, k, l, u, v) are\nsampled as follows: node i, k are ﬁrst randomly sampled\nfrom all nodes, then nodes j, l are randomly selected from\nthe ﬁrst-order neighbors of node i and k, and lastly nodes\nu, v are randomly sampled from the non-neighbor nodes of\ni, j, k, l. We use Adam [56] to optimize the parameters. The\nlearning rate of Adam is initialized as 3 × e−4 and reduced\nby 50% at 30 and 100 epochs, respectively. The model is\ntrained for 300 epochs. The training loss reaches a stable\nstate after 200 epochs, and thus we take the model at the 300-\nth epoch as the ﬁnal model for evaluation. The number of\nsextets in a mini-batch, Nbatch = 64. We also incorporate the\nefﬁcient similarity search library, faiss [57], to boost up our\nmutual k-NN search and employ inverted index to reduce\nthe memory consumption of the dataset instances. We store\nthe highly compressed faiss index instead of the original\nrepresentations in deployment.\n4\nEXPERIMENT\nTo demonstrate the performance of GRAD-Net, we con-\nducted experiments on a synthetic toy dataset and seven\npopular benchmarking datasets, including face images (Sec-\ntion 4.2) and natural images (Sections 4.3, 4.4, 4.5). Table\n1 summarizes the type and statistics of all the analyzed\ndatasets. All the implementation settings follow the descrip-\ntions in Section 3.8.\n4.1\nToy example\nWe generated 1,500 points in 2-D space that construct the\nword PAMI to demonstrate that GRAD-Net can well capture\nthe geometric manifold. We take the coordinates of these\npoints as the original features. The query points are chosen\nin each letter (marked by star) colored in green, yellow, blue,\nand purple, respectively.\nTABLE 1: Summary of datasets\nDataset\nType\nClasses\nInstances\nFeature Dims\nORL[58]\nFace\n40\n400\n10,304\nOxford5k[59]\nImage\n11\n5,062\n512 or 2,048\nOxford105k[60]\nImage\n11\n105K\n512 or 2,048\nParis6k[59]\nImage\n11\n6,392\n512 or 2,048\nParis106k[60]\nImage\n11\n106K\n512 or 2,048\nROxford[61]\nImage\n11\n4,993\n2,048\nRParis[61]\nImage\n11\n6,322\n2,048\nFig. 4(a) uses Euclidean distance as the similarity metric\nto ﬁnd the k nearest neighbors of each query point while Fig.\n4(b) uses GRAD-Net to retrieve the relevant points for each\nquery. Take the blue query point in the letter M for example.\nFig. 4(e) depicts the initial, intermediate and ﬁnal retrieval\nresults using GRAD-Net. At the initial step, some points in\nthe letter I that are closer to M by the Euclidean metric and\nwere wrongly retrieved to the blue query point. However,\nas the training progresses, GRAD-Net is capable of correctly\nretrieving points from M for the blue query point.\nFig. 4(c) shows the t-SNE plot [62] of the original fea-\ntures, while Fig. 4(d) shows the t-SNE plot of the features\nproduced by GRAD-Net. The t-SNE plots clearly verify\nthat GRAD-Net better captures the intrinsic manifold than\nEuclidean distances.\n4.2\nORL Dataset\nThe ORL dataset [58] is a face image dataset that contains\n400 images of size 112 × 92 taken from 40 distinct subjects.\nThe images vary in illuminations, facial expressions and\nfacial details. We ﬁrst vectorize and normalize the raw\nimage pixels as image descriptors.\nRetrieval accuracy is measured by the average recall rate\nat a window size K for each query, also known as the\nbullseye score. For this dataset, we set K = 15 and the\nbaseline bullseye score is 62.35%. Table 2 summarizes the\nperformance of various approaches on this dataset. Our pro-\nposed GRAD-Net achieves the state-of-the-art performance.\n4.3\nOxford5k and Paris6k Dataset\nIn this section, we examine the performance of GRAD-\nNet with real natural images on the Oxford5k and Paris6k\ndatasets [59].\nExperiments on Oxford5k. The Oxford5k dataset consists\nof 5,062 pictures of 11 different Oxford buildings collected\nfrom Flickr and 55 query images with the ground truth for\nevaluation.\nFor a fair comparison, we employed the image descrip-\ntors provided in Iscen et al. [5] to perform the experiments.\nIn particular, the image descriptors are in two types: the ﬁrst\ntype consists of 512 dimensional descriptors [11] derived\nfrom a ﬁne-tuned VGG net, while the second type consists of\n2,048 dimensional descriptors [65] derived from a ﬁne-tuned\nResNet. For any given instance, Iscen et al. [5] extracts one\nglobal feature vector and multiple regional feature vectors.\nIn this experiment, we only consider the scenario where\nan instance is represented by one global feature vector.\nTherefore, we only compare GRAD-Net to the models using\nthe global features as input here and after.\n8\nFig. 4: Synthetic toy dataset results. (a) Retrieval results based on Euclidean distance as similarity metric; (b) retrieval\nresults using GRAD-Net; (c) t-SNE visualization of the original features; (d) t-SNE visualization of the features output by\nGRAD-Net; (e) initial, intermediate and ﬁnal retrieval results of the blue query point using GRAD-Net.\nTABLE 2: Performance comparison (bullseye score) on the ORL dataset.\nMethods\nk-NN\nSD [29]\nLCDP [63]\nTPG [30]\nMR [8]\nGDP [3]\nRDP [6]\nEfﬁcient [64]\nGSS [47]\nGRAD-Net(Ours)\nBullseye score\n62.35\n71.67\n74.25\n73.90\n77.58\n77.42\n79.27\n82.00*\n83.87*\n84.14\n∗Scores are reported by using the code provided by the authors.\nTABLE 3: Performance comparison (mAP scores) on Oxford5k, Oxford105k, Paris6k, Paris106k, ROxford and RParis\nMethod\nDescriptor\nOxford5k\nOxford105k\nParis6k\nParis106k\nROxford\nRParis\nk-NN\nVGG\n79.5\n72.1\n84.5\n77.1\n-\n-\nk-NN + AQE [52]\n85.4\n79.7\n88.4\n83.5\nRegional Diffusion [5]\n85.7\n82.7\n94.1\n92.5\nEfﬁcient [64]\n88.2\n85.0\n94.7\n92.2\nGSS [47]\n87.8\nOOM\n93.7\nOOM\nGRAD-Net(Ours)\n90.1\n84.6\n94.8\n91.2\nk-NN\nResNet\n83.6\n80.8\n93.8\n89.9\n64.7\n77.2\nk-NN + AQE [52]\n89.6\n88.3\n95.3\n92.7\n67.2\n80.7\nRegional Diffusion [5]\n87.1\n87.4\n96.5\n95.4\n69.8\n88.9\nEfﬁcient [64]\n92.6\n90.8\n97.1\n94.9\n72.1\n91.3\nGSS [47]\n91.5\nOOM\n96.1\nOOM\n77.8\n92.4\nGRAD-Net(Ours)\n95.9\n94.5\n97.3\n95.4\n75.6\n90.6\n(i) OOM stands for out of memory.\n(ii) VGG style descriptors are not available for the ROxford and RParis datasets.\n(iii) Scores of Efﬁcient and GSS are reported by using the code provided by the authors.\nWe compare GRAD-Net with existing methods and\nsummarizes the results in Table 3. We use the standard\nevaluation protocol, mean Average Precision (mAP) ranging\nfrom 0% to 100%, to quantify the retrieval accuracy. For both\ntypes of image descriptors, GRAD-Net signiﬁcantly im-\nproves the baseline performances and achieves the highest\nmAP among all the methods, 90.1% for the VGG descriptors\nand 95.9% for the ResNet descriptors, exceeding the existing\nstate-of-the-art method by a large margin. Fig. 5 depicts the\nretrieval results by GRAD-Net at the initial state, epoch 1,\n20 and 60, respectively, which demonstrates that the training\nprocess improves the retrieval results of GRAD-Net.\nWe also showed that a simple nearest neighbor search\nusing the sparse vectors Z achieves impressive mAP score of\n9\n87.5% on the ResNet image descriptors. This demonstrates\nthat our proposed sparse coding method is able to capture\nglobal similarity relations and reduce the noise in original\ndense vectors. GRAD-Net further improves the mAP score\nby additional 8.4%, which indicates that GRAD-Net is capa-\nble of recovering the manifold structure.\nOriginal\nQuery 18 \nEpoch 20\nEpoch 60\nEpoch 1\nBodleian Libraries\nFig. 5: Retrieval results on the Oxford5k dataset at differ-\nent epochs. The query instance is on the top left. Each row\non the right contains the top ﬁve retrieved images after the\ncorresponding training epoch. Images in the red frames are\nthe incorrect results, while images in the blue frames are\nthe correct ones. As the training progresses, GRAD-Net is\ncapable of retrieving the correct instances for the query.\nExperiments on Paris6k. The Paris6k dataset was collected\nin a similar fashion to the Oxford5k dataset. It includes 6,392\nimages of 11 buildings in Paris and uses 55 queries with the\nground truth for evaluation. We also employed the VGG\nand ResNet features from Philbin et al. [60] and adopted the\nsame model conﬁgurations as in the Oxford5k experiment,\nexcept for k = 60 for the mutual k-NN graph. Results in\nTable 3 show that GRAD-Net outperforms the existing state-\nof-the-art on both types of image descriptors.\n4.4\nOxford105k and Paris106k Datasets\nTo demonstrate the efﬁcacy of GRAD-Net on large-scale\ndatasets, we evaluate our proposed method on the Ox-\nford105k and Paris106k datasets [60]. These datasets are\nthe extensions of the Oxford5k and Paris6k datasets with\nadditional 100k irrelevant images from Flickr as distractors.\nDue to the large scale of the datasets, most conventional\ndiffusion methods fail to process the entire dataset within an\nacceptable time. For example, the diffusion iteration for one\nquery from Oxford105k takes 13.9s with a 12-core CPU [5].\nOn the contrary, GRAD-Net, deploying mini-batch training\nand feature truncation, is capable of handling the >100k\ninstances in both datasets (see Section 5.4 for the detailed\ncomplexity analyses).\nAs shown in Table 3, GRAD-Net achieves outstand-\ning performance (mAP score 94.5%) on the Oxford105k\ndataset using the ResNet descriptors and outperforms the\nexisting state-of-the-art method by 2.7%. On the Paris106k\ndataset, GRAD-Net also achieves comparable performance\nto the state-of-the-art method (mAP score 95.4% compared\nto 95.6%). Using the VGG descriptors, GRAD-Net also\nachieves comparable performance. We observed that the\ntruncated union graphs generated from the VGG image\ndescriptors omit some of the correct instances, which may\npartially explain the under-performance on the VGG de-\nscriptors.\n4.5\nROxford and RParis Datasets\nRadenovi et al. [61] generated the ROxford and RParis\ndatasets by manually examining the instances in Oxford5k\nand Paris6k, and selected a set of 70 challenging queries\nwith ground truth for evaluation. We adopted the image\ndescriptors from Radenovi et al. [66] and used the Medium\nevaluation protocol.\n5\nDISCUSSION\n5.1\nAblation Study\nIn this section, we vary the conﬁgurations of several impor-\ntant settings in GRAD-Net to verify the effectiveness of the\ncorresponding variants. For the purpose of illustration, we\nfocus on the Oxford5k dataset, and set the base model as the\none discussed in Section 4.3 that achieves an mAP score of\n95.95%.\nFor fair comparisons, we ran six trials for each variant\nand evaluated the performance with three evaluation met-\nrics:\n1)\nEpochs mAP92+ refers to the number of epochs that\na model takes to ﬁrst reach an mAP score of 92%\nduring the training process. We use this metric to\nexamine the learning efﬁciency.\n2)\nmAP is the mean Average Precision score. We report\nthe average and standard deviation of the mAP\nscores of the six trial runs.\n3)\nDiff refers to the mAP drop compared to the base\nmodel (the ﬁrst row in Table 4).\nLoss functions. The ablation study of the loss functions\nshows that Llocal and Lglobal have substantial impact on\nthe model performance. Without the local loss, the proposed\nmodel can only reach an average mAP score of 90.16% and\nexhibits a slight increase of instability in the training pro-\ncess. The absence of the global loss, Lglobal, also increases\nthe standard deviation of the mAP score by 0.42%. We also\ntested the impact of global loss function on the Oxford105k\ndataset. The performance drops by 0.96% without global\nloss.\nSparse coding. The sparse coding vector Z is another im-\nportant component of GRAD-Net. By default, Z is a 100-\ndimensional vector appended to the original features. We\ntest on two variants related to Z: taking (i) original features\nonly and (ii) Z only as the inputs to GRAD-Net. The ablation\nstudies show that both Z and the original features are\ncrucial to the retrieval performance.\nModel Structure. We vary the layer settings of GRAD-Net,\nand the results show that GRAD-Net is robust to the layer\nsize and the number of layers selection on the Oxford5k\ndataset. One interesting ﬁnding is that when replacing the\ngraph diffusion layer (that includes a second-order opera-\ntor) with the vanilla GCN network (that only includes a ﬁrst-\norder operator), the mAP score drops by 0.96%. This ﬁnding\nempirically validates the effectiveness of the second-order\n10\nTABLE 4: Ablation study results of the variants in GRAD-Net.\nCategory\nVariants\nEpochs mAP92+\nmAP\nDiff\n-\nbase\n9\n95.95±0.26\n-\nLoss (default Eq.22)\nw/o Llocal\n-\n90.16±0.39\n5.79\nw/o Lglobal\n12\n95.47±0.42\n0.48\nw/o ∥Θ∥2\n2\n8\n95.64±0.39\n0.31\nSparse coding (default\nZ∥original features)\nw/o Z\n12\n95.56±0.24\n0.39\nw/o original features\n-\n89.51±0.53\n6.44\nModel structure\n(default 1024, 256,\n128)\nLayers-1024,256,128,128\n9\n95.28±0.35\n0.67\nLayers-1024,256\n9\n95.55±0.39\n0.40\nLayers-512,128,64\n15\n95.39±0.64\n0.56\nGCN w/o 2nd order\n8\n94.99±0.52\n0.96\nw/o concatenation\n20\n92.51 ±1.02\n3.44\nManifold structure\n(default k=15)\nk=8\n30\n93.07±0.54\n2.88\nk=20\n12\n95.21±0.48\n0.74\nk=25\n10\n94.03±0.67\n1.92\nk=32\n8\n92.68±0.62\n3.35\nk=64\n-\n90.37±0.47\n5.58\nSimilarity metric\n(default-Euclidean)\nGaussian Euclidean[30]\n30\n94.36±0.58\n1.59\nCosine\n28\n94.63±0.64\n1.32\nBatch size(default\nbatch=64)\nbatch=32\n10\n95.37±0.47\n0.58\nbatch=128\n15\n95.40±0.27\n0.55\nbatch=256\n21\n94.84±0.30\n1.11\nbatch=1024\n62\n93.69±0.37\n2.04\nThe bold entries are the ones differ the most from the base model in each category.\noperator in the diffusion layer of GRAD-Net. Moreover, we\nperform ablation studies on the GRAD-Net output features\nwith or without the concatenation operation as in Eq.17. The\nresults verify that the concatenation of features in Eq.17 is\nessential and accounts for 3.44% of the performance gain as\nwell as decreases the variation of performance.\nManifold Structure. The test results on the manifold struc-\ntures show that the value of k in the mutual k-NN search\nis crucial to the learned features. On the Oxford5k dataset,\nk is optimal around 15, yet GRAD-Net demonstrates robust\nperformance for k ranging from 8 to 25.\nSimilarity metric and batch size. The variants in the sim-\nilarity metrics show that GRAD-Net is robust to different\nsimilarity metric choices. Experiments of various batch sizes\nshow that smaller batch size is preferable for better perfor-\nmance.\n(a)\n(b)\nFig. 6: The feature learning results. (a) t-SNE visualization\nof the original image features; (b) t-SNE visualization of the\nlearned features. Colors denote the image categories. Grey\npoints are denote the images that do not belong to any query\ncategory.\n5.2\nFeature Learning\nThe most noticeable difference between GRAD-Net and\nconventional diffusion methods is that GRAD-Net performs\nfeature learning, while conventional diffusion methods only\noperates on the afﬁnity matrix.\nBased on the results from Fig. 4(c-d), we argue that\nGRAD-Net learns a better representation of semantic in-\nformation. We further validate the feature learning ability\nof GRAD-Net using the Oxford5k dataset. Fig. 6 plots the\nt-SNE visualizations of the Oxford5k images using the orig-\ninal image descriptors and the learned features of GRAD-\nNet respectively. The nodes are colored by the ground truth\nlabels. Fig. 6 shows that GRAD-Net groups the instances\nfrom the same category with similar features.\nInitial\nSearch\nQFE2\nQFE\nCorn Market\nRank\n6\n9\n8\n7\n10\nFig. 7: Query Induction Example. The ﬁrst row represents\nthe initial search result in the original feature space. The\nsecond row represents the ﬁrst QFE search result, and the\nthird row is the second QFE search result, which implies\nQFE can improve the retrieval results for unseen queries.\n11\nFig. 8: Memory and time consumption comparison. (a) CPU RAM memory consumption against the number of dataset\ninstances; (b) GPU RAM memory consumption against the number of dataset instances. Efﬁcient is omitted since the\nmethod cannot be implemented in GPU. (c) Time consumption against the number of instances.\n5.3\nInductive Leaning\nTo test the generalizability of GRAD-Net for unseen queries,\nwe manually put aside 11 queries from the 55 queries of\nthe Oxford5k dataset and left them out in the training.\nWe then performed retrieval of these 11 queries using the\nQFE method described in Section 3.6. The mAP score in\nthe Table 5 shows the effectiveness of the QFE method. We\nalso observe a slight improvement in the performance when\napplying QFE twice. As shown in Fig. 7, the QFE method\ncan project approximated features of the unseen query to the\nlearned feature space, which leads to more accurate retrieval\nperformance.\nTABLE 5: Performance comparison on unseen queries\nMethod\nModel\nmAP\nQuery Expansion\nEfﬁcient [64]\n89.6\nQFE\nGRAD-Net\n96.01\nQFE × 2\n96.05\n5.4\nComplexity Analysis\nThe ﬁrst step in the pipeline of GRAD-Net is to build a\nmutual k-NN graph, which requires the pairwise similarity\nsearch among instances. This step has a computational\ncomplexity of O(N 2), where N is the number of instances.\nHowever, since we only need to build the graph once and\nit is considerably accelerated by the faiss toolbox [57], we\nargue that the computational complexity of GRAD-Net is\nO(N) that is mainly determined by the training process of\nthe neural network.\nWe conducted a series of experiments to evaluate the\nmemory and time consumption of GRAD-Net and several\nother methods. All experiments were conducted using a\nworkstation with a 12-core Intel(R) Xeon(R) CPU W-2133\n@ 3.60GHz and one NVIDIA RTX5000 GPU with 16GB GPU\nmemory. We started with the Oxford5k dataset, gradually\nadded distractors to the original dataset, and recorded the\ntime and memory consumption of training or the diffusion\niterations. Empirical results verify our argument as follows.\nMemory consumption analysis. Fig. 8(a-b) show the mem-\nory consumption on CPU and GPU respectively. As the\nnumber of instances increases, GRAD-Net can easily scale\nup to 10k samples even without using truncation, thanks\nto the mini-batch training on graphs. After integrating the\ntruncation technique, the memory consumption is notably\nreduced by a large margin. This validates the advantage of\nGRAD-Net regarding memory consumption.\nIn comparison, the memory consumption of Efﬁcient\nDiffusion [64], a conventional diffusion method, exceeds\nGRAD-Net as the number of instances reaches 20k regard-\nless of that it employs a truncated search technique. We also\ncompare with the memory consumption of GSS [47], a neu-\nral network based method, increases rapidly as the number\nof instances gets larger, and encounters out of memory (OOM)\nerror when the number of instances exceeds 21k.\nTime consumption analysis. We analyzed the time con-\nsumption of GRAD-Net as the number of instances in-\ncreases. Fig. 8(c) plots the average time per forward com-\nputing and gradient back-propagation against the numbers\nof instances. The time consumption results validate that\nGRAD-Net has linear complexity, O(N). We also compare\nGRAD-Net with the neural network based method, GSS\n(blue line in Fig. 8(c)), and argue that GRAD-Net has faster\niteration time.\n6\nCONCLUSION\nIn this paper, we propose GRAD-Net, a novel deep learning\nbased diffusion method to address the challenge of unsuper-\nvised image retrieval. In contrast to conventional diffusion\nmethods, GRAD-Net enables effective representation learn-\ning for images while preserves both the local and global geo-\nmetric properties of the image manifold. Moreover, GRAD-\nNet is trained in an unsupervised end-to-end fashion and\ncan easily scale up to handle large-scale datasets. Extensive\nempirical results on multiple benchmarks demonstrate the\nefﬁcacy of GRAD-Net. Besides its outstanding performance,\nGRAD-Net also shows its generalizability to unseen queries\nin an inductive learning manner. However, for highly dy-\nnamic image retrieval systems where instances are con-\nstantly modiﬁed in the database, we envision that GRAD-\nNet is capable of performing the retrieval tasks with reg-\nular re-training. The trigger to re-train the network can be\ndetermined by the mean average precision over a separate\n12\nvalidation set. We will explore these directions in our future\nworks.\nREFERENCES\n[1]\nM. R. Ghorab, D. Zhou, A. O‘connor, and V. Wade,\n“Personalised information retrieval: Survey and clas-\nsiﬁcation,” User Modeling and User-Adapted Interaction,\nvol. 23, no. 4, pp. 381–443, 2013.\n[2]\nD. Van Aken, A. Pavlo, G. J. Gordon, and B. Zhang,\n“Automatic database management system tuning\nthrough large-scale machine learning,” in Proceedings\nof the 2017 ACM International Conference on Management\nof Data, ACM, 2017, pp. 1009–1024.\n[3]\nM. Donoser and H. Bischof, “Diffusion processes for\nretrieval revisited,” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 2013,\npp. 1320–1327.\n[4]\nF. Bach, “Breaking the curse of dimensionality with\nconvex neural networks,” The Journal of Machine Learn-\ning Research, vol. 18, no. 1, pp. 629–681, 2017.\n[5]\nA. Iscen, G. Tolias, Y. Avrithis, T. Furon, and O. Chum,\n“Efﬁcient diffusion on region manifolds: Recovering\nsmall objects with compact cnn representations,” in\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2017, pp. 2077–2086.\n[6]\nS. Bai, X. Bai, Q. Tian, and L. J. Latecki, “Regularized\ndiffusion process on bidirectional context for object\nretrieval,” IEEE transactions on pattern analysis and\nmachine intelligence, vol. 41, no. 5, pp. 1213–1226, 2018.\n[7]\nB. Wang, Z. Tu, and J. K. Tsotsos, “Dynamic label\npropagation for semi-supervised multi-class multi-\nlabel classiﬁcation,” in Proceedings of the IEEE interna-\ntional conference on computer vision, 2013, pp. 425–432.\n[8]\nD. Zhou, J. Weston, A. Gretton, O. Bousquet, and B.\nSchlkopf, “Ranking on data manifolds,” in Advances in\nneural information processing systems, 2004, pp. 169–176.\n[9]\nA. Babenko, A. Slesarev, A. Chigorin, and V. Lempit-\nsky, “Neural codes for image retrieval,” in European\nconference on computer vision, Springer, 2014, pp. 584–\n599.\n[10]\nA. Gordo, J. Almazn, J. Revaud, and D. Larlus, “Deep\nimage retrieval: Learning global representations for\nimage search,” in European conference on computer vi-\nsion, Springer, 2016, pp. 241–257.\n[11]\nF. Radenovi, G. Tolias, and O. Chum, “Cnn image\nretrieval learns from bow: Unsupervised ﬁne-tuning\nwith hard examples,” in European conference on com-\nputer vision, Springer, 2016, pp. 3–20.\n[12]\nA. S. Razavian, J. Sullivan, S. Carlsson, and A. Maki,\n“Visual instance retrieval with deep convolutional\nnetworks,” IEEE Transactions on Media Technology and\nApplications, vol. 4, no. 3, pp. 251–258, 2016.\n[13]\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L.\nFei-Fei, “Imagenet: A large-scale hierarchical image\ndatabase,” in IEEE conference on computer vision and\npattern recognition, Ieee, 2009, pp. 248–255.\n[14]\nT. N. Kipf and M. Welling, “Semi-supervised classiﬁ-\ncation with graph convolutional networks,” in 5th In-\nternational Conference on Learning Representations, ICLR,\n2017.\n[15]\nJ. Atwood and D. Towsley, “Diffusion-convolutional\nneural networks,” in Advances in Neural Information\nProcessing Systems, 2016, pp. 1993–2001.\n[16]\nM. Niepert, M. Ahmed, and K. Kutzkov, “Learning\nconvolutional neural networks for graphs,” in Inter-\nnational conference on machine learning, 2016, pp. 2014–\n2023.\n[17]\nS. Cao, W. Lu, and Q. Xu, “Deep neural networks\nfor learning graph representations,” in Thirtieth AAAI\nConference on Artiﬁcial Intelligence, 2016.\n[18]\nR. Levie, F. Monti, X. Bresson, and M. M. Bronstein,\n“Cayleynets: Graph convolutional neural networks\nwith complex rational spectral ﬁlters,” IEEE Transac-\ntions on Signal Processing, vol. 67, no. 1, pp. 97–109,\n2018.\n[19]\nX. Wang, X. He, M. Wang, F. Feng, and T.-S. Chua,\n“Neural graph collaborative ﬁltering,” in Proceedings\nof the 42nd international ACM SIGIR conference on Re-\nsearch and development in Information Retrieval, 2019,\npp. 165–174.\n[20]\nN. Park, A. Kan, X. L. Dong, T. Zhao, and C. Faloutsos,\n“Estimating node importance in knowledge graphs\nusing graph neural networks,” in Proceedings of the\n25th ACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, 2019, pp. 596–606.\n[21]\nD. P. Vassileios Balntas Edgar Riba and K. Miko-\nlajczyk, “Learning local feature descriptors with\ntriplets and shallow convolutional neural networks,”\nin Proceedings of the British Machine Vision Conference\n(BMVC), BMVA Press, Sep. 2016, pp. 119.1–119.11.\n[22]\nR. Yu, Z. Dou, S. Bai, Z. Zhang, Y. Xu, and X. Bai,\n“Hard-aware point-to-set deep metric for person re-\nidentiﬁcation,” in Proceedings of the European Conference\non Computer Vision (ECCV), 2018, pp. 188–204.\n[23]\nD. Zhou, O. Bousquet, T. N. Lal, J. Weston, and\nB. Schlkopf, “Learning with local and global con-\nsistency,” in Advances in neural information processing\nsystems, 2004, pp. 321–328.\n[24]\nX. Bai, X. Yang, L. J. Latecki, W. Liu, and Z. Tu,\n“Learning context-sensitive shape similarity by graph\ntransduction,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 32, no. 5, pp. 861–874,\n2009.\n[25]\nU. N. Raghavan, R. Albert, and S. Kumara, “Near\nlinear time algorithm to detect community structures\nin large-scale networks,” Physical review E, vol. 76, no.\n3, p. 036 106, 2007.\n[26]\nH. Jegou, C. Schmid, H. Harzallah, and J. Verbeek,\n“Accurate image search using the contextual dissimi-\nlarity measure,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 32, no. 1, pp. 2–11, 2008.\n[27]\nL. Fei-Fei and P. Perona, “A bayesian hierarchical\nmodel for learning natural scene categories,” in 2005\nIEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR’05), IEEE, vol. 2, 2005,\npp. 524–531.\n[28]\nR. Sinkhorn, “A relationship between arbitrary posi-\ntive matrices and doubly stochastic matrices,” The an-\nnals of mathematical statistics, vol. 35, no. 2, pp. 876–879,\n1964.\n13\n[29]\nB. Wang and Z. Tu, “Afﬁnity learning via self-\ndiffusion for image segmentation and clustering,” in\n2012 IEEE Conference on Computer Vision and Pattern\nRecognition, IEEE, 2012, pp. 2312–2319.\n[30]\nX. Yang, L. Prasad, and L. J. Latecki, “Afﬁnity learning\nwith diffusion on tensor product graph,” IEEE trans-\nactions on pattern analysis and machine intelligence, vol.\n35, no. 1, pp. 28–38, 2012.\n[31]\nS. Bai, X. Bai, Q. Tian, and L. J. Latecki, “Regularized\ndiffusion process for visual retrieval,” in Thirty-First\nAAAI Conference on Artiﬁcial Intelligence, 2017.\n[32]\nP. Frasconi, M. Gori, and A. Sperduti, “A general\nframework for adaptive processing of data struc-\ntures,” IEEE transactions on Neural Networks, vol. 9, no.\n5, pp. 768–786, 1998.\n[33]\nM. Gori, G. Monfardini, and F. Scarselli, “A new\nmodel for learning in graph domains,” in Proceedings.\n2005 IEEE International Joint Conference on Neural Net-\nworks, 2005., IEEE, vol. 2, 2005, pp. 729–734.\n[34]\nF. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and\nG. Monfardini, “The graph neural network model,”\nIEEE Transactions on Neural Networks, vol. 20, no. 1,\npp. 61–80, 2008.\n[35]\nY. Li, D. Tarlow, M. Brockschmidt, and R. S. Zemel,\n“Gated graph sequence neural networks,” in 4th Inter-\nnational Conference on Learning Representations, ICLR,\n2016.\n[36]\nK. Cho, B. van Merrienboer,. Glehre, D. Bahdanau,\nF. Bougares, H. Schwenk, and Y. Bengio, “Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation,” in Proceedings of\nthe 2014 Conference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP, ACL, 2014, pp. 1724–1734.\n[37]\nJ. Bruna, W. Zaremba, A. Szlam, and Y. LeCun,\n“Spectral networks and locally connected networks\non graphs,” in 2nd International Conference on Learning\nRepresentations, ICLR, 2014.\n[38]\nM. Henaff, J. Bruna, and Y. LeCun, “Deep convo-\nlutional networks on graph-structured data,” ArXiv\npreprint arXiv:1506.05163, 2015.\n[39]\nP. Velikovi, G. Cucurull, A. Casanova, A. Romero, P.\nLio, and Y. Bengio, “Graph attention networks,” ArXiv\npreprint arXiv:1710.10903, 2017.\n[40]\nV. Garcia and J. Bruna, “Few-shot learning with graph\nneural networks,” in 6th International Conference on\nLearning Representations, ICLR, 2018.\n[41]\nM. Guo, E. Chou, D.-A. Huang, S. Song, S. Yeung,\nand L. Fei-Fei, “Neural graph matching networks for\nfewshot 3d action recognition,” in Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2018,\npp. 653–669.\n[42]\nX. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun, “3d\ngraph neural networks for rgbd semantic segmenta-\ntion,” in Proceedings of the IEEE International Conference\non Computer Vision, 2017, pp. 5199–5208.\n[43]\nL. Yi, H. Su, X. Guo, and L. J. Guibas, “Syncspeccnn:\nSynchronized spectral cnn for 3d shape segmenta-\ntion,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2017, pp. 2282–2290.\n[44]\nX. Chen, L.-J. Li, L. Fei-Fei, and A. Gupta, “Iterative\nvisual reasoning beyond convolutions,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 7239–7248.\n[45]\nM. Narasimhan, S. Lazebnik, and A. Schwing, “Out\nof the box: Reasoning with graph convolution nets\nfor factual visual question answering,” in Advances in\nNeural Information Processing Systems, 2018, pp. 2654–\n2665.\n[46]\nB. Jiang, D. Lin, J. Tang, and B. Luo, “Data represen-\ntation and learning with graph diffusion-embedding\nnetworks,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2019,\npp. 10 414–10 423.\n[47]\nC. Liu, G. Yu, M. Volkovs, C. Chang, H. Rai, J. Ma, and\nS. K. Gorti, “Guided similarity separation for image\nretrieval,” in Advances in Neural Information Processing\nSystems, 2019, pp. 1554–1564.\n[48]\nW. Liu, J. He, and S.-F. Chang, “Large graph construc-\ntion for scalable semi-supervised learning,” in Pro-\nceedings of the 27th international conference on machine\nlearning (ICML-10), 2010, pp. 679–686.\n[49]\nX. Lan, S. Zhang, P. C. Yuen, and R. Chellappa,\n“Learning common and feature-speciﬁc patterns: A\nnovel multiple-sparse-representation-based tracker,”\nIEEE Transactions on Image Processing, vol. 27, no. 4,\npp. 2022–2037, 2017.\n[50]\nB. Xu, N. Wang, T. Chen, and M. Li, “Empirical\nevaluation of rectiﬁed activations in convolutional\nnetwork,” ArXiv preprint arXiv:1505.00853, 2015.\n[51]\nS. Rendle, C. Freudenthaler, Z. Gantner, and L.\nSchmidt-Thieme, “Bpr: Bayesian personalized rank-\ning from implicit feedback,” in Proceedings of the\ntwenty-ﬁfth conference on uncertainty in artiﬁcial intel-\nligence, AUAI Press, 2009, pp. 452–461.\n[52]\nO. Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisser-\nman, “Total recall: Automatic query expansion with a\ngenerative feature model for object retrieval,” in 2007\nIEEE 11th International Conference on Computer Vision,\nIEEE, 2007, pp. 1–8.\n[53]\nW. Hamilton, Z. Ying, and J. Leskovec, “Inductive rep-\nresentation learning on large graphs,” in Advances in\nNeural Information Processing Systems, 2017, pp. 1024–\n1034.\n[54]\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G.\nChanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et\nal., “Pytorch: An imperative style, high-performance\ndeep learning library,” in Advances in Neural Informa-\ntion Processing Systems, 2019, pp. 8024–8035.\n[55]\nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,\nand R. Salakhutdinov, “Dropout: A simple way to pre-\nvent neural networks from overﬁtting,” The journal of\nmachine learning research, vol. 15, no. 1, pp. 1929–1958,\n2014.\n[56]\nD. P. Kingma and J. Ba, “Adam: A method for stochas-\ntic optimization,” in 3rd International Conference on\nLearning Representations, ICLR, 2015.\n[57]\nJ. Johnson, M. Douze, and H. Jgou, “Billion-scale\nsimilarity search with gpus,” IEEE Transactions on Big\nData, 2019.\n[58]\nF. S. Samaria and A. C. Harter, “Parameterisation of\na stochastic model for human face identiﬁcation,” in\n14\nProceedings of 1994 IEEE Workshop on Applications of\nComputer Vision, IEEE, 1994, pp. 138–142.\n[59]\nJ. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisser-\nman, “Object retrieval with large vocabularies and fast\nspatial matching,” in 2007 IEEE Conference on Computer\nVision and Pattern Recognition, IEEE, 2007, pp. 1–8.\n[60]\n——, “Lost in quantization: Improving particular ob-\nject retrieval in large scale image databases,” in 2008\nIEEE conference on computer vision and pattern recogni-\ntion, IEEE, 2008, pp. 1–8.\n[61]\nF. Radenovi, A. Iscen, G. Tolias, Y. Avrithis, and O.\nChum, “Revisiting oxford and paris: Large-scale im-\nage retrieval benchmarking,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\n2018, pp. 5706–5715.\n[62]\nL. v. d. Maaten and G. Hinton, “Visualizing data using\nt-sne,” Journal of machine learning research, vol. 9, no.\nNov, pp. 2579–2605, 2008.\n[63]\nX. Yang, S. Koknar-Tezel, and L. J. Latecki, “Locally\nconstrained diffusion process on locally densiﬁed dis-\ntance spaces with applications to shape retrieval,” in\n2009 IEEE Conference on Computer Vision and Pattern\nRecognition, IEEE, 2009, pp. 357–364.\n[64]\nF. Yang, R. Hinami, Y. Matsui, S. Ly, and S. Satoh,\n“Efﬁcient image retrieval via decoupling diffusion\ninto online and ofﬂine processing,” in Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, vol. 33,\n2019, pp. 9087–9094.\n[65]\nA. Gordo, J. Almazan, J. Revaud, and D. Larlus,\n“End-to-end learning of deep visual representations\nfor image retrieval,” International Journal of Computer\nVision, vol. 124, no. 2, pp. 237–254, 2017.\n[66]\nF. Radenovi, G. Tolias, and O. Chum, “Fine-tuning\ncnn image retrieval with no human annotation,” IEEE\ntransactions on pattern analysis and machine intelligence,\nvol. 41, no. 7, pp. 1655–1668, 2018.\nZhiyong Dou received his B.S. in Electric and\nElectronic Engineering from Huazhong Univer-\nsity of Science and Technology, Wuhan, China\nin 2017. He is currently a Ph.D candidate at\nthe School of Electronic Information and Com-\nmunications in Huazhong University of Science\nand Technology. He is also a visiting student\nat the University of Toronto beginning in 2019.\nHis research interests include computer vision,\ncomputational biology and machine learning.\nHaotian Cui received his B.S. in Biomedical En-\ngineering from the Tsinghua University, China in\n2015. He is currently pursuing his Ph.D. degree\nin Computer Science at the University of Toronto.\nHis research interests include computer vision,\ncomputational biology and machine learning.\nLin Zhang received her HB.A. in Statistics and\nB.A. in Economics from University of Califor-\nnia, Berkeley in 2012, and received her M.A. in\nApplied Statistics from University of California,\nSanta Barbara in 2015. She is currently a Ph.D.\ncandidate at the Department of Statistical Sci-\nences at the University of Toronto.\nBo Wang is the lead AI scientist for the Peter\nMunk Cardiac Centre at the University Health\nNetwork (UHN) in Toronto. He is also an As-\nsistant Professor at the Department of Medical\nBiophysics at the University of Toronto and a\nCIFAR AI Chair of the Vector Institute. Dr. Wang\nobtained his Ph.D. from the Department of Com-\nputer Science at Stanford University in 2017 and\nhis M.Sc. in Computer Science from the Uni-\nversity of Toronto in 2012. His current research\ninterests include computer vision, computational\nbiology and machine learning. Dr. Wang is a member of IEEE.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-01-05",
  "updated": "2020-06-11"
}