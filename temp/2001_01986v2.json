{
  "id": "http://arxiv.org/abs/2001.01986v2",
  "title": "Learning Speaker Embedding with Momentum Contrast",
  "authors": [
    "Ke Ding",
    "Xuanji He",
    "Guanglu Wan"
  ],
  "abstract": "Speaker verification can be formulated as a representation learning task,\nwhere speaker-discriminative embeddings are extracted from utterances of\nvariable lengths. Momentum Contrast (MoCo) is a recently proposed unsupervised\nrepresentation learning framework, and has shown its effectiveness for learning\ngood feature representation for downstream vision tasks. In this work, we apply\nMoCo to learn speaker embedding from speech segments. We explore MoCo for both\nunsupervised learning and pretraining settings. In the unsupervised scenario,\nembedding is learned by MoCo from audio data without using any speaker specific\ninformation. On a large scale dataset with $2,500$ speakers, MoCo can achieve\nEER $4.275\\%$ trained unsupervisedly, and the EER can decrease further to\n$3.58\\%$ if extra unlabelled data are used. In the pretraining scenario,\nencoder trained by MoCo is used to initialize the downstream supervised\ntraining. With finetuning on the MoCo trained model, the equal error rate (EER)\nreduces $13.7\\%$ relative ($1.44\\%$ to $1.242\\%$) compared to a carefully tuned\nbaseline training from scratch. Comparative study confirms the effectiveness of\nMoCo learning good speaker embedding.",
  "text": "Learning Speaker Embedding with Momentum Contrast\nKe Ding, Xuanji He, Guanglu Wan\nMeituan-Dianping Group\n{dingke02, hexuanji, wanguanglu}@meituan.com\nAbstract\nSpeaker veriﬁcation can be formulated as a representation\nlearning task, where speaker-discriminative embeddings are ex-\ntracted from utterances of variable lengths. Momentum Con-\ntrast (MoCo) is a recently proposed unsupervised representation\nlearning framework, and has shown its effectiveness for learn-\ning good feature representation for downstream vision tasks. In\nthis work, we apply MoCo to learn speaker embedding from\nspeech segments.\nWe explore MoCo for both unsupervised\nlearning and pretraining settings. In the unsupervised scenario,\nembedding is learned by MoCo from audio data without using\nany speaker speciﬁc information. On a large scale dataset with\n2, 500 speakers, MoCo can achieve EER 4.275% trained unsu-\npervisedly, and the EER can decrease further to 3.58% if extra\nunlabelled data are used. In the pretraining scenario, encoder\ntrained by MoCo is used to initialize the downstream super-\nvised training. With ﬁnetuning on the MoCo trained model,\nthe equal error rate (EER) reduces 13.7% relative (1.44% to\n1.242%) compared to a carefully tuned baseline training from\nscratch. Comparative study conﬁrms the effectiveness of MoCo\nlearning good speaker embedding.\nIndex Terms: speaker veriﬁcation, representation learning, un-\nsupervised learning\n1. Introduction\nSpeaker veriﬁcation (SV) is the task of conﬁrming the claimed\nidentity of a speaker given one’s speech segments. Typically, a\nﬁxed-dimensional embedding is extracted for both enrollment\nand test speech, and compared to give out a same-speaker-or-\nnot decision.\nI-vectors [1] are widely used as speaker embeddings. In\nthe standard i-vector system, a GMM is trained with all train-\ning data served as universal background model (UBM) to col-\nlect sufﬁcient statistics (typically a super vector) from speech\nsegments. A low-rank project matrix, dubbed Total Variabil-\nity Matrix, is trained with the sufﬁcient statistics. For a given\nsegment, a super vector is extracted by UBM, and projected by\nthe learned project matrix, resulting in a ﬁxed-dimensional vec-\ntor, i.e. i-vector. The similarity between two i-vectors (usually\nthe log likelihood ratio) can be computed by probabilistic linear\ndiscriminant analysis (PLDA) [2] .\nIn recent years, various neural network based methods are\nproposed to learning more discriminative embeddings.\nX-vectors are proposed to replace i-vectors [3]. In the x-\nvector system, speaker embeddings are extracted by time-delay\nneural networks (TDNNs). The TDNN is trained as a multi-\nspeaker classiﬁer using cross entropy loss and then the activa-\ntion from some hidden layer is extracted as the embedding. A\ntemporal statistical pooling layer to applied to tackle segments\nof variable length. After training, utterances are mapped di-\nrectly to ﬁxed-dimensional speaker embeddings, just as i-vector\nsystems do. Working with a PLDA backend, the x-vectors can\noutperform i-vectors for short speech segments and are compet-\nitive on long duration test conditions [3, 4].\nThe embeddings learned by cross entropy loss is separable\nby design for the close-set classiﬁcation task, but not necessarily\ndiscriminative enough, which is key to generalize to identify\nunseen speaker segments. To tackle this issue, various training\ncriterion to enhance the discriminative power of the x-vector\nembeddings.\nContrastive learning, i.e. contrastive loss [5], triplet loss [6],\nis introduced for optimizing the distances between embeddings\ndirectly. Losses of constrastive style minimize the distance be-\ntween an anchor and a positive sample while maximizes the dis-\ntance between the anchor and a negative one, thus encouraging\nembeddings with compact within-speaker variations and sepa-\nrable between-speaker differences.\nHowever, optimizing contrastive loss can be challenging [6,\n7], and selecting training pairs or triplets suffers from combina-\ntorial data expansion and negative samples mining is necessary\nfor effective and stable training. Margin based training criterion\nis proposed to learn embeddings with compact inter-class vari-\nation and bypass the training problems of constrastive learning.\nCenter loss [8] is used to work with cross entropy loss (to\navoid embedding collapsing). Center loss penalises the distance\nbetween the embeddings and their corresponding class centers\nin the Euclidean space to achieve intra-class compactness. By\nintroducing margins between classes into conventional cross en-\ntropy loss, angular softmax (A-softmax) is reported being able\nto learn more discriminative embeddings than cross entropy loss\nand triplet loss [9]. More recently, Additive Angular Margin\n(AAM) loss is proposed for extracting highly discriminative\nfeatures for face recognition [10]. AAM is successfully applied\nto speaker veriﬁcation task [11] and achieves state of the art per-\nformance in the VoxCeleb challenge [12].\nRecently, a novel unsupervised learning framework, Mo-\nmentum Contrast (MoCo) [13] is proposed. MoCo is an ex-\ntension of instance discrimination, and can learn representa-\ntion using contrastive learning criterion. In several vision tasks,\nMoCo can outperform its supervised pretraining counterpart,\nthus largely closing the gap between unsupervised and super-\nvised representation.\nIn this work, we apply MoCo to the speaker veriﬁcation\ntask. Observe that MoCo trains an encoder directly, which is\nexactly the conventional neural network based methods (e.g. x-\nvector system) do. What’s more, MoCo encourages discrimina-\ntive representation (via contrastive learning), which is key for\nopen set veriﬁcation. We explore MoCo in the ways: 1) using\nMoCo trained encoder directly, 2) using MoCo as a pretraining\nmethod to relieve downstream training of interest.\nThe remainder of this paper is organized as follows. Sec-\ntion 2 describes our proposal for using MoCo for learning\nspeaker embedding. Speciﬁcally, a SpecAugment [14] based\ndata augmentation is introduced for parallel speech segments\ngeneration. Section 3 shows the experiments conducted on a\narXiv:2001.01986v2  [cs.CL]  6 Sep 2020\nFigure 1: Training framework of MoCo\nlarge scale speaker dataset. Results on the public available Vox-\nceleb dataset are also presented. We conclude our work in Sec-\ntion 4.\n2. Proposed Method\n2.1. Momentum Contrast Learning\nMomentum Contrast (MoCo) [13] is a general mechanism for\nunsupeverised learning representation using contrastive loss. In\nMoCo, a dynamic dictionary is built with a queue and a moving-\naveraged encoder. This enables building a large and consistent\ndictionary on-the-ﬂy that facilitates contrastive unsupervised\nlearning. The representations learned by MoCo is reported to\ntransfer well to downstream visual tasks.\nThe training framework of MoCo is depicted by Fig. 1. For\neach training sample, two corrupted versions are generated by\nsome augmentation strategy. After processing one mini-batch,\nthe encoder’s parameters are updated by some optimizer, say\nSGD. The encoded representations of the current mini-batch (k0\nin Fig. 1) are enqueued, and the oldest ones are dequeued to\nkeep the queue size consistent. Before processing the next mini-\nbatch, the momentum encoder is update with the encoder with\nas momentum coefﬁcient (typically close to 1).\nAs neural network based speaker veriﬁcation systems learn\nembedding from utterances, MoCo can be applied to the train-\ning process in a natural way. To be speciﬁc, the encoder is as\nsame as the network used for conventional xvector training (see\nSec. 3). We just initialize the momentum encoder with the en-\ncoder and initialize memory queue randomly. For other details\nand import tricks for training (i.e. constrastive loss, ShufﬂeBN),\nwe refer the reader to [13].\n2.2. Data Augmentation\nWhat’s speciﬁc to our task under study (speaker veriﬁcation)\nis how to generate parallel corrupted version of the same ut-\nterance. The method we propose is depicted by Fig 2. First,\nwe randomly selected two segments from the target utterance,\nwhich is the common practice in x-vector training. Second, we\napply SpecAugment [14] to the segments, resulting in two dif-\nferent version of the same utterance with various length and\nspectrum distortion.\nVia the proposed process, parallel cor-\nrupted segments can be generated with both temporal and spec-\ntrum variability.\nFigure 2: Generating two augmented version of segments from\none utterance\n2.3. MoCo as embedding extractor\nMoCo trains the encoder as an instance discrimination task,\ntreating the parallel corrupted version as positive sample and\nall in the memory queue as negative samples. So it’s natural to\nexpect the embeddings extracted by a well trained encoder show\ndiscrimination between different speakers. Reminiscent of the\nconventional i-vector extractor, we can use the learned encoder\nwith some backend (e.g. PLDA, cosine) for veriﬁcation task di-\nrectly. As MoCo need no speaker information, we also explore\nif extra unlabelled data can lead to more robust embedding for\nveriﬁcation.\n2.4. MoCo as Pretraining\nThough unsupervised learning may not be optimal for the task\nof interest, features pretrained by unsupervised learning might\nbe transferred to downstream tasks by ﬁne-tuning. In the pre-\nvious work, cross entropy training is used for pretraining [12].\nIn this work, we explore if models unsupervised pretrained by\nMoCo is helpful for the downstream supervised learning.\n3. Experiments\n3.1. Datasets\nWe train and evaluate our models on two datasets, Dataset A\nwith speaker labels and Dataset B without any speaker informa-\ntion.\nDataset A consists of utterances from 2, 943 speakers, each\nwith 1, 000 utterances. All speech are recorded with mobile\nphones (iOS or Android systems) with a sample rate 16 kHz.\nThe duration of utterances are distributed between 2.5s and\n6.5s, with a mode duration 3.5s. We split the dataset into train-\ning and test set. The training set consists of 2, 500 speakers. For\neach of the remaining 443 speakers, we randomly selected three\nutterances for enrollments, and 50 utterance for evaluation, re-\nsulting in about 9.8 million trials. Other remaining utterances\nof the test speakers are not used in the experiments.\nDataset B consists of mobile phone recorded speech with\nsample rate 16 kHz, without any speaker speciﬁc information.\nThere are 7.6 million utterances in total, or about 9 thousand\nhours before silences removing.\nThe information of the two datasets are summarized in\nTab. 1.\n3.2. Baseline systems\n3.2.1. I-vector system\nThe i-vector system is based on the standard kaldi recipe\n[15] (egs/voxceleb/v1). The features are 30-dimensional Mel-\nFrequency Cepstral Coefﬁcients (MFCCs), with a frame shift of\ndataset\ntraining\ntest\nA\n2, 500 speakers\n1, 000 utterances\n443 speakers\n(3 for enrollment,\n50 for evaluation)\nB\n∼7.6 million utterances\n∼9k hours\n-\nTable 1: Overview of the training and evaluation data\nLayer\nLayer Context\nInput × Output\nframe1\n[t-2, t+2]\n(5 × d) × 512\nframe2\n{t-2, t, t+2}\n(3 × 512) × 512\nframe3\n{t-3, t, t+3}\n(3 × 512) × 512\nframe4\n{t}\n512 × 512\nframe5\n{t}\n512 × 1500\nstats pooling\n[0, T)\n1500T × 3000\nembed a\n{0}\n3000 × 512\nembed b\n{0}\n512 × 512\nsoftmax\n{0}\n512 × D\nTable 2: X-vector network architecture used in this paper, where\nd, T, and D denote the dimensionality of input feature, the num-\nber of utterance frames, and the number of speakers, respec-\ntively. Embeddings are extracted at layer embed b (before non-\nlinearity).\n10ms and a window width of 25ms. Delta and acceleration are\nappended to create 90 dimension feature vectors. Mean normal-\nization is then applied over a sliding window of up to 3 seconds.\nTo ﬁlter out non-speech frames, kaldi’s energy-based voice ac-\ntivity detector (VAD) is employed. (We also tried a neural net-\nwork based VAD, but observe no improvement.) The UBM is a\n2048 component full-covariance GMM. The system uses a 400\ndimension i-vector extractor. I-vectors are centered, dimension-\nality reduced to 200 using LDA. Length normalization [16] is\napplied before PLDA scoring [2].\n3.2.2. X-vector system\nOur x-vector is based kaldi’s voxceleb recipe. The TDNN struc-\nture used is as Tab. 2. We train the x-vector network using py-\ntorch [17], over four V100 GPUs. Synchronous Stochastic Gra-\ndient Descent (SGD) with weight decay = 1e −5 and momen-\ntum = 0.9 is used. The total batch size is 1024 (256 for each\nGPU). The learning rate decays from 1e−4 to 1e−5 exponen-\ntially. A dropout layer [18] with p = 0.5 is applied after before\nembed b. A max gradient norm of 2 is applied to stabilize train-\ning.\nAfter silence removing, we ﬁlter out utterances shorter than\n250 frames and make sure each speaker has at least 30 utter-\nances, ending up with 2, 453 speaker, about 1184 hours. We\ntrain 30 times of the total training frames, with each epoch pro-\ncessing 72 million frames (i.e. 200 hours). We random select 3\nutterances from each speaker as holdout validation set.\nLike the ivector-system, a PLDA is used as backend for\nscoring. The LDA reduction dimension is set to 150.\n3.2.3. X-vector with AAM loss\nAdditive Angular Margin (AAM) loss is reported more effective\nthan the conventional cross entropy loss for both face [10] and\nspeaker veriﬁcation tasks [12].\nAAM loss is deﬁned as Eq. 1:\nL = −1\nN\nN\nX\ni=1\nlog\nes(cos(θyi +m))\nes(cos(θyi +m)) + PD\nj=1,j̸=yi es cos θj\n(1)\nwhere cos(θ) is the dot product between the embed b and the\nlast fully connected layer (softmax layer) (after L2 normaliza-\ntion), additive angular m and scale factor s are hyper parame-\nters.\nIn our experiments, layers after embed b are dropped and\nan AAM loss is applied instead of cross entropy loss. We use\ns = 32, m = 0.3 for all experiments. The learning rate decays\nfrom 1e −5 to 1e −6 exponentially. No dropout is used, as we\nfound it degrades the performance. A max gradient norm of 6\nis applied to stabilize training. All other hyperparameters stay\nthe same as the cross entropy training.\nWe try two backend, 1) cosine distance and 2) PLDA as the\naforementioned the x-vector system.\nThe EERs and minDCFs of the three baseline systems are\nlisted in Tab 3. It can be found that AAM can achieve better\nperformances than cross entropy based training, and this obser-\nvation is in line with ﬁndings by [11, 12]. Direct cosine distance\ncan achieve decent performance, and a separate PLDA backend\ncontribute no improvement (though no signiﬁcant performance\ndegradation w.r.t EER, is surpassed by cosine on minDCF crite-\nrion by large margins). Thus, we use AAM-loss trained network\nwith cosine backend for future comparison.\n3.3. MoCo as extractor\nThe network used for x-vector system (c.f. Sec. 3.2.2) is used as\nthe MoCo encoder. We use a queue size 10, 000, and β = 0.99\nτ = 0.07. For SpecAugment, time warp window 10, max time\nmask width 20, max frequency mask width 10. The learning\nrate decays exponentially from 1e −4 to 1e −5.\nAfter training, the encoder is used for xvector extraction.\nAs in the xvector-AAM case, we try both cosine and PLDA\nbackends. The results are listed in Tab. 4.\nWe compare MoCo with i-vector system, as both MoCo and\ni-vevtor’s training process is unsupervised. As can be seen, with\ncosine backend (complete unsupervised), MoCo outperforms i-\nvector. It is not surprising, as MoCo optimizes dot product (co-\nsine) as the training criterion. Interestingly, helped by a strong\nbackend (PLDA), i-vector can achieve an EER 1.661%, signif-\nicantly better than MoCo (2.655%).\nIt is not clear whether some backend other than PLDA used\nhere can improve the performance for MoCo. We leave this\ntopic for future work.\nTo explore if extra data is helpful for MoCo learning, we\ntrain MoCo use extra unlabelled data from Dataset B. We use\na larger queue size (20, 000), and other hyperpamaters stay the\nsame. The performance of MoCo encoder can improve further,\nfrom 4.275% to 2.655% for the cosine backend, and 3.58% to\n2.366% for the PLDA backend.\n3.4. MoCo as pretraining\nIn this section, we study if the encoder trained with MoCo is\nhelpful for downstream supervised learning. We initialize x-\nvector with the encoder pretrained by MoCo in Sec. 3.3. As\ncomparison, we also conduct experiment with conventional pre-\ntraining with cross entropy training (as used in [12]). As shown\nin Tab 5, model pretrained by MoCo can great help the training.\nThe EER improves from 1.44% to 1.242%, a 13.7% relative\nmethod\nivector (cosine)\nivector (PLDA)\nxvector-CE (PLDA)\nxvector-AAM (PLDA)\nxvector-AAM (cosine)\nEER (%)\n5.178\n1.661\n1.562\n1.589\n1.44\nminDCF0.01\n0.397\n0.191\n0.217\n0.24\n0.165\nminDCF0.001\n0.622\n0.409\n0.487\n0.572\n0.346\nTable 3: Performance for the baseline systems.\nmethod\ncosine\nLDA-PLDA\nivector\n5.178\n1.661\nMoCo\n4.275\n2.655\nMoCo (+ Dataset B)\n3.58\n2.366\nTable 4: EERs (%) of encoder learned by MoCo with different\nbackends.\nmethod\nEER (%)\nminDCF0.01\nminDCF0.001\nxvector-AAM\n1.44\n0.165\n0.346\nxvector-AAM\n(CE)\n1.481\n0.169\n0.357\nxvector-AAM\n(MoCo)\n1.242\n0.157\n0.334\nxvector-AAM\n(MoCo-extra)\n1.223\n0.146\n0.312\nTable 5: Results of AAM trained models using different pre-\ntraining strategy. CE, Moco, MoCo-extra stands for, respec-\ntively,cross entropy pretraining, pretraining with Dataset A, and\nwith both Dataset A and Dataset B. Cosine backend is applied\nfor all models\nimprovement. At the same time, we ﬁnd that no improvement\nis observed with cross entropy pretraining. Cross entropy pre-\ntraining does not transfer well to the downstream AAM train-\ning. In fact it is harmful for AAM training in our case. One\npossible explanation is that both MoCo and AAM encourage\nthe representation to hyper-sphere subspace (via softmax over\ndot product), while cross entropy learns a difference subspace.\nThe DET curves of the systems under study are illus-\ntrated in Fig. 3. Moco pretraining improves the both EER and\nminDCF criteria; in the meantime, it achieves better perfor-\nmance under different false accept and false reject tradeoffs.\nFigure 3: The DET curves of Ivector and Xector models trained\nwith different losses and strategies trained on Dataset A.\nmethod\nEER (%)\nminDCF0.01\nminDCF0.001\nivector\n(PLDA)\n5.467\n0.4859\n0.6213\nxvector-CE\n(PLDA)\n3.356\n0.3591\n0.5890\nxvector-AAM\n(cosine)\n2.497\n0.2634\n0.3888\nxvector-AAM\n(PLDA)\n2.752\n0.3717\n0.4971\nxvector-AAM\n(CE pretraing)\n(cosine)\n2.572\n0.2535\n0.3799\nxvector-AAM\n(MoCo pretraing)\n(cosine)\n2.402\n0.2476\n0.3506\nTable 6: Results on VoxCeleb1 trained on VoxCeleb1 dev set\nand all VoxCeleb2 data.\n3.5. Results on Voxceleb\nTo check if the observations on our in-house data generalize,\nwe conduct experiments on the public available dataset Vo-\nceleb. All data from Voxceleb2 [19] and the training part to\nVoxceleb1 [20] are used for training. After converting the au-\ndios from amm to wav format, it ends up with 7, 323 speaker\nand 1, 276, 888 utterances in total. Voxceleb1 test is used as the\ntest, which has 4, 874 utterance from 40 speakers. The ofﬁcial\ntest protocol is used (37, 720 trials in total).\nThe same network architecture and training conﬁguration\nas previous experiments are used, and no further hyper param-\neter tuning is conducted for this task.\nAccording to Tab 6,\nsimilar tendency with the previous experiments is observed.\nMoCo pretraining does help AAM training, while CE pre-\ntraining doesn’t Besides, the results conﬁrms that embeddings\nlearnned by AAM works better with cosine backend.\n4. Conclusions\nIn this paper, we explore the effectiveness of MoCo for learn-\ning speaker embedding. To apply MoCo with speech data, we\npropose a parallel distorted data generating strategy based on\nSpecAugment. The experiments show that MoCo learns good\nspeaker discriminative embedding, and can be used as an effec-\ntive pretraining method for the downstream supervised train-\ning. On a large-scale dataset, we build a strong baseline sys-\ntem with AAM, which can signiﬁcantly outperform the con-\nventional cross entropy based x-vector system. Compared to\nthe baseline system trained from scratch, MoCo pretraining can\nachieve a 13.7% relative EER improvement while the conven-\ntional cross entropy pretraining gains no improvement. Thanks\nto MoCo’s unsupervised nature, extra unlabelled data could be\nused to improve the performance further.\n5. References\n[1] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet,\n“Front-end factor analysis for speaker veriﬁcation,” IEEE Trans-\nactions on Audio, Speech, and Language Processing, vol. 19,\nno. 4, pp. 788–798, 2010.\n[2] S. Ioffe, “Probabilistic linear discriminant analysis,” in European\nConference on Computer Vision.\nSpringer, 2006, pp. 531–542.\n[3] D. Snyder, D. Garcia-Romero, D. Povey, and S. Khudanpur,\n“Deep neural network embeddings for text-independent speaker\nveriﬁcation.” in Interspeech, 2017, pp. 999–1003.\n[4] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan-\npur, “X-vectors: Robust dnn embeddings for speaker recognition,”\nin 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2018, pp. 5329–5333.\n[5] K. Chen and A. Salman, “Learning speaker-speciﬁc characteris-\ntics with a deep neural architecture,” IEEE Transactions on Neural\nNetworks, vol. 22, no. 11, pp. 1744–1756, 2011.\n[6] C. Zhang, K. Koishida, and J. H. L. Hansen, “Text-independent\nspeaker veriﬁcation based on triplet convolutional neural network\nembeddings,” IEEE/ACM Trans. Audio, Speech and Lang. Proc.,\nvol. 26, no. 9, pp. 1633–1644, Sep. 2018.\n[7] P. Ghosh and L. S. Davis, “Understanding center loss based net-\nwork for image retrieval with few training data,” in Proceedings\nof the European Conference on Computer Vision (ECCV), 2018.\n[8] N. Li, D. Tuo, D. Su, Z. Li, and D. Yu, “Deep discriminative em-\nbeddings for duration robust speaker veriﬁcation.” in Interspeech,\n2018, pp. 2262–2266.\n[9] Y. Li, F. Gao, Z. Ou, and J. Sun, “Angular softmax loss for end-to-\nend speaker veriﬁcation,” in 2018 11th International Symposium\non Chinese Spoken Language Processing (ISCSLP). IEEE, 2018,\npp. 190–194.\n[10] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive\nangular margin loss for deep face recognition,” in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2019, pp. 4690–4699.\n[11] X. Xiang, S. Wang, H. Huang, Y. Qian, and K. Yu, “Margin mat-\nters: Towards more discriminative deep neural network embed-\ndings for speaker recognition,” arXiv preprint arXiv:1906.07317,\n2019.\n[12] H. Zeinali, S. Wang, A. Silnova, P. Matˇejka, and O. Plchot,\n“But system description to voxceleb speaker recognition chal-\nlenge 2019,” arXiv preprint arXiv:1910.12592, 2019.\n[13] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum\ncontrast for unsupervised visual representation learning,” arXiv\npreprint arXiv:1911.05722, 2019.\n[14] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V. Le, “Specaugment: A simple data augmen-\ntation method for automatic speech recognition,” arXiv preprint\narXiv:1904.08779, 2019.\n[15] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al.,\n“The kaldi speech recognition toolkit,” in IEEE 2011 workshop\non automatic speech recognition and understanding, no. CONF.\nIEEE Signal Processing Society, 2011.\n[16] D. Garcia-Romero and C. Y. Espy-Wilson, “Analysis of i-vector\nlength normalization in speaker recognition systems,” in Twelfth\nAnnual Conference of the International Speech Communication\nAssociation, 2011.\n[17] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch:\nAn imperative style, high-performance deep learning library,” in\nAdvances in Neural Information Processing Systems, 2019, pp.\n8024–8035.\n[18] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: a simple way to prevent neural\nnetworks from overﬁtting,” The journal of machine learning re-\nsearch, vol. 15, no. 1, pp. 1929–1958, 2014.\n[19] J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep\nspeaker recognition,” in Interspeech, 2018.\n[20] A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: A large-\nscale speaker identiﬁcation dataset,” in Interspeech, 2017, pp.\n2616–2620.\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "eess.AS"
  ],
  "published": "2020-01-07",
  "updated": "2020-09-06"
}