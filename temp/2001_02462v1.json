{
  "id": "http://arxiv.org/abs/2001.02462v1",
  "title": "From Natural Language Instructions to Complex Processes: Issues in Chaining Trigger Action Rules",
  "authors": [
    "Nobuhiro Ito",
    "Yuya Suzuki",
    "Akiko Aizawa"
  ],
  "abstract": "Automation services for complex business processes usually require a high\nlevel of information technology literacy. There is a strong demand for a\nsmartly assisted process automation (IPA: intelligent process automation)\nservice that enables even general users to easily use advanced automation. A\nnatural language interface for such automation is expected as an elemental\ntechnology for the IPA realization. The workflow targeted by IPA is generally\ncomposed of a combination of multiple tasks. However, semantic parsing, one of\nthe natural language processing methods, for such complex workflows has not yet\nbeen fully studied. The reasons are that (1) the formal expression and grammar\nof the workflow required for semantic analysis have not been sufficiently\nexamined and (2) the dataset of the workflow formal expression with its\ncorresponding natural language description required for learning workflow\nsemantics did not exist. This paper defines a new grammar for complex workflows\nwith chaining machine-executable meaning representations for semantic parsing.\nThe representations are at a high abstraction level. Additionally, an approach\nto creating datasets is proposed based on this grammar.",
  "text": "From Natural Language Instructions to Complex Processes:\nIssues in Chaining Trigger Action Rules\nNobuhiro Ito1 and Yuya Suzuki1 and Akiko Aizawa2,1\n1The University of Tokyo\n2National Institute of Informatics\nnobuhiro.ito@is.s.u-tokyo.ac.jp\n{ysuzuki, aizawa}@nii.ac.jp\nAbstract\nAutomation services for complex business processes usually\nrequire a high level of information technology literacy. There\nis a strong demand for a smartly assisted process automa-\ntion (IPA: intelligent process automation) service that enables\neven general users to easily use advanced automation. A nat-\nural language interface for such automation is expected as an\nelemental technology for the IPA realization. The workﬂow\ntargeted by IPA is generally composed of a combination of\nmultiple tasks. However, semantic parsing, one of the natural\nlanguage processing methods, for such complex workﬂows\nhas not yet been fully studied. The reasons are that (1) the\nformal expression and grammar of the workﬂow required for\nsemantic analysis have not been sufﬁciently examined and (2)\nthe dataset of the workﬂow formal expression with its cor-\nresponding natural language description required for learn-\ning workﬂow semantics did not exist. This paper deﬁnes a\nnew grammar for complex workﬂows with chaining machine-\nexecutable meaning representations for semantic parsing. The\nrepresentations are at a high abstraction level. Additionally,\nan approach to creating datasets is proposed based on this\ngrammar.\nIntroduction\nAutomation services for business process like robotic pro-\ncess automation (RPA) have recently attracted increasing\nattention. These automation services typically require ad-\nvanced information technology literacy when creating au-\ntomation programs. Thus, it is difﬁcult for non-skilled users\nto make use of the services. A smart interface that can cre-\nate and execute an automated program speciﬁed in a natural\nlanguage (NL) description would be useful. This smart in-\nterface is one of the intelligent process automation (IPA)\nrealizations. We focus on a natural language processing\nmethod that plays an important role in this interface: seman-\ntic parsing. Semantic parsing consists of three components:\n(a) a natural language description, (b) a machine-executable\nmeaning representation (MEMR), and (c) a parser that con-\nverts (a) to (b). MEMR can be a formal expression that fol-\nlows a speciﬁc grammar. Workﬂows targeted by automation\nsystems represented by RPA are generally complex work-\nﬂows composed of a combination of multiple tasks. In stud-\nies on semantic parsing, (1) the formal expression and the\nformal grammar for expressing such workﬂows have both\nnot been sufﬁciently examined. As a result, (2) there was no\ndataset that had a pairing of a complex workﬂow with its\ncorresponding NL description.\nFormal Expression and Grammar\nWith respect to the expression and the grammar, we focus\non two problems: an expression unit, and a formal grammar\nfor complex workﬂows.\nExpression Unit\nSeveral types of expressions exist de-\npending on the abstraction level of the expression. Many\nstudies have been conducted on parsing NL description to\ncode, such as transition-based neural semantic parsing like\nTranX (Yin and Neubig 2017; Yin and Neubig 2018). The\nﬁeld of code generation typically uses expressions with low\nabstraction: code itself. Therefore, the expression becomes\nbloated when attempting to express a complex workﬂow\nthat is composed of hundreds of lines of code. Consider\nan expression with high abstraction that is much closer to\nhuman language, as exempliﬁed by calling macros, APIs,\nand modules, 1 such as “Insert Rows Into a Google Spread-\nsheet”; this is a straightforward approach to avoiding the\nbloating problem. An expression with high abstraction is\ntypically used in a trigger action program (TAP) format that\nperforms a certain action when a certain trigger occurs. A\nnotable TAP dataset is the IFTTT dataset (Ur et al. 2016;\nMi et al. 2017). However, the workﬂow targeted by the\nIFTTT dataset itself is too simple to use for complex work-\nﬂows.\nFormal Grammar\nAlthough it is necessary to express a\nseries of processes as a whole in one shot, collecting such\ndatasets is inherently difﬁcult. We focus on the form of the\nTAP chain in formulating a formal grammar. TAP can be\nconsidered a type of MEMR. The advantage of TAP is that it\nexpresses the essence of being event-driven in a straightfor-\nward manner, so it has high versatility. Moreover, there are\ndatasets and existing studies on TAP. Complex workﬂows\nare composed of process chains like “If this triggered, then\ndo this action and do this action separately, and ﬁnally do\n1This expression can be considered as a “no-code/low-code”\nexpression that has recently been refocused in RPA.\narXiv:2001.02462v1  [cs.AI]  8 Jan 2020\nthis action.” On the other hand, a single TAP only ends with\n“If this triggered, then do this action,” ((a) in Figure 1) but\nwe can assume that performing this action causes another\ntrigger ((b) in Figure 1). Thus, we can express a complex\nworkﬂow by repeating TAP chaining ((c) in Figure 1). We\ncall this a “TAP chain” and incorporate it into the grammar\nformulation2. This facilitates conversion to complex work-\nFigure 1: TAP Chain\nﬂows through relatively simple MEMRs.3 Especially, this\nenables us to apply the already-mentioned transition-based\nneural semantic parsing to a workﬂow intricately combined\nwith such MEMRs. Transition-based neural semantic pars-\ning does not generate code directly, but generates a sequence\nof grammars (in the form of context-free grammars)4. To the\nbest of our knowledge, no such study has been conducted\nthus far because of the lack of an adequate grammar for such\nworkﬂows and parsing.\nComplex Workﬂow Dataset\nIn our creation of complex workﬂow dataset, we use the\nIFTTT dataset that contains TAP programs with their cor-\nresponding NL descriptions. From the IFTTT dataset, we\nmanually extract the relationships where one TAP triggers\nanother TAP, and generate the TAP chain rules. After that,\na TAP chain rule is randomly applied to generate a complex\nworkﬂow. Since it is applied randomly, the usefulness of the\ngenerated workﬂow is manually annotated. NL description\ncorresponding to the created workﬂow is generated by fus-\ning NL description of each TAP that constitutes the work-\nﬂow. Two approaches are considered: rule-based generation,\n2It seems important to consider the concept of type or category\nof a MEMR. The type may include semantic content indicating the\nclose connectivity of each task.\n3In this paper, we use the TAP chain-based formal expression\nfor workﬂow for the reason already described, but other formal ex-\npressions for workﬂow can be also considered. We reserve this is-\nsue for future work.\n4TranX mainly targets a low abstraction level. The main focus\nof TranX for applying the parsing is to ensure that the parsing re-\nsults are legal in the context of the Python abstract grammar.\nand sentence fusion generation (one approach in abstractive\nsummarizations). The generated NL descriptions are manu-\nally annotated especially with respect to test data.\nThe main contribution of this study is the deﬁnition of\na new grammar for semantic parsing to complex work-\nﬂows. In addition, an approach to creating the dataset is pro-\nposed based on this grammar. In the following, we concisely\nexplain the related work, then propose workﬂow patterns\ngrammar (WPG) and dataset creation, then concisely refer\nto the model, and ﬁnally conclude this short paper.\nRelated Work\nSemantic Parsing\nThe process of converting NL into MEMR is known as\nsemantic parsing. A typical example of semantic parsing\nis SQL generation for database queries. A semantic parser\ntranslates the sentence “How many people live in Seat-\ntle?” to “SELECT Population FROM CityData where\nCity==Seattle”. Then the SQL query is executed to obtain\nthe correct answer, “620,778” (Gardner et al. 2018). An-\nother typical example of semantic parsing is code generation\nwhere a single function declaration or class declaration is\nviewed as an MEMR (Gardner et al. 2018). Rather than gen-\nerating code directly, transition-based neural semantic pars-\ning like TranX generates a sequence of ASDL grammars that\nare sequentially expanded and applied to generate MEMRs\n(Yin and Neubig 2017; Yin and Neubig 2018). However,\nthese parsing models mainly target a low abstraction level\nof formal expression.\nTAP/IFTTT\nA typical web service of TAP is the IFTTT (Ur et al. 2016;\nMi et al. 2017). As an example, the NL description\n“youtube upload to blogger new post” is converted into\nan MEMR with a high abstraction level TRIGGER\n“YouTube.New public video uploaded by you”\nand\nACTION “Blogger.Create a post.” A study of sim-\nple\ngrammar-based\nsemantic\nparsing\nwithout\nneural\nmodel has been conducted (Quirk, Mooney, and Galley\n2015). Additionally, a method using neural semantic\nparsing has been proposed (Beltagy and Quirk 2016;\nLiu\net\nal.\n2016;\nDong\nand\nLapata\n2016;\nDong, Quirk, and Lapata 2018). Furthermore, a dialogue\nmodel (Chaurasia and Mooney 2017) and a reinforcement\nlearning dialogue model (Yao et al. 2018) have also been\nproposed. However, a target workﬂow of IFTTT itself is too\nsimple to be able to directly automate complex real-world\nworkﬂows.\nWorkﬂow Patterns Grammar\nIn this paper, we deﬁne a new grammar for semantic parsing\nto complex workﬂows (Table 1). This enables us to apply\nthe transition-based neural semantic parsing to a workﬂow\nintricately combined with MEMRs. The speciﬁc notation\nfollows Yin and Neubig (2018), which mainly refer to the\nPython ASDL grammar: The notation “?” represents the op-\ntional type, which can have one value or a null value, and the\nnotation “*” indicates the sequential type, which can have\ntwo or more values.\nstmt\n=\nWorkﬂow(wpg pattern)\n· · ·(1)\nwpg\n=\nSequence(func? trigger, func action)\n|\nParallel Split(func? trigger, func* action)\n· · ·(2)\nfunc\n=\nCall(type channel, wpg? next)\n· · ·(3)\ntype\n=\nType A | Type B | Type C ...\n· · ·(4)\nTable 1: Workﬂow Pattern Grammar\nWorkﬂow Initialization\nThe ﬁrst line (1) in the table indicates the start point of the\nworkﬂow generation. The stmt type evokes a constructor\nwith the wpg type argument called “pattern.” This expansion\ndelivers the start point of the workﬂow.\nWorkﬂow Patterns\nIn workﬂows like the ones in ofﬁce workplaces, there are\npatterns that repeatedly occur. Russell, van van der Aalst,\nand ter Hofstede (2016) introduced ﬁve basic patterns to\ncapture the elementary aspects of the ﬂow: sequence, par-\nallel split, synchronization, exclusive choice, and simple\nmerge. In this study, we consider two patterns, sequence and\nparallel split, to maintain the tree structure and simplicity of\nthe MEMRs. These constructors are shown in line (2) of Ta-\nble 1. The ﬁrst pattern expands wpg to a sequence pattern:\nthat is, it evokes the sequence pattern constructor. The sec-\nond pattern expands wpg to a parallel split pattern: that is, it\nevokes the parallel split pattern constructor.\nWPG expression in this paper also considers the ﬂow of\nprocessed data. For example, the “Send Text to Me” function\nhas no return value and therefore no function to connect to\nthe next. Since the “Archive Text in Spread Sheet” function\nmust receive text data, it cannot follow a “Send Text to Me”\nfunction having no return. If the processed output data from\nfunctions are different, it is straightforward to handle them\nin different branches. Also, transition-based neural semantic\nparsing learns the sequence of grammar expansions corre-\nsponding to NL descriptions. In this learning, the parallel\nsplit is expected to work as a signal token to decide whether\nsubsequent ﬂow should branch, based on its previous trigger\nor action and NL descriptions.\nTAP Chaining\nAs already mentioned, a complex workﬂow is generated\nfrom a simple MEMR by considering a TAP chain, as pre-\nsented in line (2) in Table 1. The sequence pattern’s argu-\nments are the func? type argument called “trigger” and the\nfunc type argument called “action.” Because this pattern is\nsimple, such that when the “trigger” is evoked, the “action”\nis activated, we consider this pattern to have two arguments.\nThe notation “?” represents the optional type, which can\nhave one value or a null value. When two sequence patterns\nare connected (for example, Sequence(Function A, Function\nB) and Sequence(Function B, Function C) are connected se-\nquentially), the action in the ﬁrst sequence pattern is the\nsame as the trigger in the second sequence pattern. Thus,\nthe function is duplicated. We expand the second sequence\npattern as Sequence(null, Function C).\nThe parallel split pattern’s arguments are the func? type\nargument called “trigger” and the func* type argument\ncalled “action.” This pattern splits the preceding function’s\nresult into two or more functions.\nTAP Call\nfunc is the type that evokes the Call constructor, as presented\nin line (3) in Table 1. The constructor controls the next task\nand the next workﬂow to be executed after the task is com-\npleted. The concrete task is derived through expansion from\nthe type argument called “channel”. On the other hand, if the\ntask is followed by another task, the constructor should get a\nvalue at the type argument called “next”. type is expanded to\na concrete macro method class that has concrete functions\nbelonging to the class. This expansion is presented in line\n(4) in Table 1.\nWorkﬂow Represented in Abstract Syntax Tree\n(AST)\nConsider a speciﬁc example of a workﬂow represented\nin AST (WAST). For example, consider a workﬂow, as\ndepicted in Figure 25. When WPG is applied to this\nFigure 2: Workﬂow Example\nworkﬂow, the WAST is that in Figure 3. The WPG\nFigure 3: Complex Workﬂow in WAST Form\ngrammars applied sequentially are depicted in Table 2.\nThe formal expression for parsing is Sequence ( An-\n5In this paper, to make the discussion easier we use a work-\nﬂow that is more complex than TAP but still relatively simple. The\nproposed framework is applicable to versions of workﬂow that are\nmore complex than this simple example.\nt\nFrontier Field\nAction\nt1\nstmt root\nWorkﬂow(wpg pattern)\nt2\nwpg pattern\nSequence(func? trigger, func action)\nt3\nfunc? trigger\nCall(type channel, wpg? next)\nt4\ntype channel\nSelectMacr[Android]\nt5\nAndroid\nSelectMacr[Any Missed Phone]\nt6\nwpg? next\nStopExpnsn(close the frontier ﬁeld)\nt7\nfunc action\nCall(type channel, wpg? next wpg)\nt8\ntype channel\nSelectMacr[Watson API]\nt9\nAndroid\nSelectMacr[Voice to Text]\nt10\nwpg? next\nParallel Split(func? trigger, func* action)\nt11\nfunc? trigger\nStopExpnsn(close the frontier ﬁeld)\nt12\nfunc* action\nCall(type channel, wpg? next wpg)\nt13\ntype channel\nSelectMacr[SMS]\nt14\nSMS\nSelectMacr[Send Text to Me]\nt15\nwpg? next\nStopExpnsn(close the frontier ﬁeld)\nt16\nfunc* action\nCall(type channel, wpg? next wpg)\nt17\ntype channel\nSelectMacr[Google Drive]\nt18\nGoogle Drive\nSelectMacr[Archive Text Spread Sheet]\nt19\nwpg? next\nStopExpnsn(close the frontier ﬁeld)\nt20\nfunc* action\nStopExpnsn(close the frontier ﬁeld)\nTable 2: WPG Expansion Example\ndroid. Any Missed Phone, Parallel Split ( Watson API.\nVoice to Text, SMS. Send Text to Me, Google Drive.\nArchive Text in Spread Sheet)).\nDataset Creation\nWe propose an approach of creating a training dataset and\na test dataset for learning transition-based neural semantic\nparsing for a complex workﬂow with TAP chain. We basi-\ncally suppose that the dataset is to be annotated manually.\nWe use an existing TAP dataset which includes correspond-\ning NL descriptions for TAPs. This dataset is beneﬁcial be-\ncause TAPs and NL descriptions are actually created and\nused by real users, and the NL descriptions enable the anno-\ntator to reuse them to create NL descriptions for the work-\nﬂows as a whole.\nWAST Generation\nIn the IFTTT data, if a trigger function is called, then an\naction function is invoked. There can exist a case wherein\nwhen an action function in one TAP occurs, a trigger func-\ntion in another TAP is ﬁred simultaneously. We manually\nconducted such action-evoke-trigger annotations and deter-\nmined the TAP chaining rules. This chain rule assumes the\nvertical expansion of TAP in a workﬂow (Figure 4). On the\nFigure 4: TAPs for Vertical Expansion\nother hand, the horizontal expansion of TAP is simple; that\nis, it is sufﬁcient to execute multiple actions that have the\nsame trigger (Figure 5). This makes it possible to create\ncomplex workﬂows from TAP chains.\nA complex workﬂow is generated by randomly chaining\nTAPs. However, it is unclear whether this automatically gen-\nFigure 5: TAPs for Horizontal Expansion\nerated workﬂow is really beneﬁcial. Therefore, the automat-\nically generated workﬂow is annotated in whether: (A) con-\nvenient and frequently used, (B) possible to use, or (C) in-\nconvenient and not used. Each TAP that is an element of\nworkﬂow generation is limited to the TAPs actually created\nand used by real users. In other words, for each TAP it is\nassumed that the combination of trigger and action is useful\nfor some user.\nGeneration of NL instructions\nFurthermore, the graphical form of the automatically gener-\nated workﬂow like Figure 2 is shown to an annotator, who is\nasked to annotate the instruction/description that should be\ngiven when they ask a machine to perform the workﬂow6.\nConsequently, a pairing of an NL description and a work-\nﬂow represented as WAST can be generated.\nModel\nWith respect to the model, we follow Yin and Neubig\n(2018). Transition-based neural semantic parsing has an in-\nput of the NL utterance x consisting of n words {wi}n\ni=1.\nThe parser outputs at, one of the three transitions: “Apply-\nConstr[c] ” expresses the instruction to apply a WPG hav-\ning constructor c, “SelectMacr” means to generate a terminal\ntoken (function), and “StopExpnsn” means stop generating\noptional or sequential arguments.\nThe probability of generating WAST z from NL instruc-\ntions x is:\nq(z|x) = Πtq(at|a<t, x).\n(1)\nThe model is trained to maximize the log-likelihood of the\ntransition sequence. Then, the best WAST is inferred from\nNL description using beam search.\nDiscussion\nIn this paper, we assume a simple design where each\nthread progresses independently and focus only on a WAST\nform with a tree structure. In other words, ﬂows that have\nbranched once never rejoin. However, complex workﬂows\nusually include a simple merge workﬂow pattern, where\nbranched ﬂows will merge at some point in the following\nprocess. WPG needs to be extended to graph a structure.\nThis paper referred to general workﬂow patterns. With\nthe spread of RPA, data on business processes have been\n6In the annotation process, we ﬁrst automatically generate in-\nstructions/descriptions by rule-based summarization. Then, anno-\ntators review and modify the instruction/description. It is possible\nto use sentence fusion models (Lebanoff et al. 2019) to generate\nthese.\naccumulated. It is possible that there are common patterns\nacross the companies. Therefore, it is useful to extract such\ncommon workﬂow patterns from real usage data of the RPA\nproducts and reduce them to WPG.\nConclusion\nIn this study, we deﬁned a new grammar for chaining high\nabstraction level MEMRs for semantic parsing into com-\nplex workﬂows. We also proposed an approach to gener-\nate a dataset based on this grammar. Consequently, it is ex-\npected that an NL interface will be constructed for the com-\nplex workﬂow assumed by IPA. In the future, we intend to\nperform semantic parsing on the dataset created by this ap-\nproach.\nReferences\n[2016] Beltagy, I., and Quirk, C. 2016. Improved semantic\nparsers for if-then statements. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), 726–736. Berlin, Germany:\nAssociation for Computational Linguistics.\n[2017] Chaurasia, S., and Mooney, R. J. 2017. Dialog for\nlanguage to code. In Proceedings of the Eighth International\nJoint Conference on Natural Language Processing (Volume\n2: Short Papers), 175–180. Taipei, Taiwan: Asian Federa-\ntion of Natural Language Processing.\n[2016] Dong, L., and Lapata, M. 2016. Language to logi-\ncal form with neural attention. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), 33–43. Berlin, Germany:\nAssociation for Computational Linguistics.\n[2018] Dong, L.; Quirk, C.; and Lapata, M. 2018. Conﬁ-\ndence modeling for neural semantic parsing. In Proceedings\nof the 56th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), 743–753. Mel-\nbourne, Australia: Association for Computational Linguis-\ntics.\n[2018] Gardner, M.; Dasigi, P.; Iyer, S.; Suhr, A.; and Zettle-\nmoyer, L. 2018. Neural semantic parsing. In Proceedings\nof the 56th Annual Meeting of the Association for Compu-\ntational Linguistics: Tutorial Abstracts, 17–18. Melbourne,\nAustralia: Association for Computational Linguistics.\n[2019] Lebanoff, L.; Muchovej, J.; Dernoncourt, F.; Kim,\nD. S.; Kim, S.; Chang, W.; and Liu, F. 2019. Analyzing\nsentence fusion in abstractive summarization. In Proceed-\nings of the 2nd Workshop on New Frontiers in Summariza-\ntion, 104–110. Hong Kong, China: Association for Compu-\ntational Linguistics.\n[2016] Liu, C.; Chen, X.; Shin, E. C.; Chen, M.; and Song,\nD. 2016. Latent attention for if-then program synthesis. In\nLee, D. D.; Sugiyama, M.; Luxburg, U. V.; Guyon, I.; and\nGarnett, R., eds., Advances in Neural Information Process-\ning Systems 29. Curran Associates, Inc. 4574–4582.\n[2017] Mi, X.; Qian, F.; Zhang, Y.; and Wang, X.\n2017.\nAn empirical characterization of ifttt: Ecosystem, usage, and\nperformance. In Proceedings of the 2017 Internet Measure-\nment Conference, IMC ’17, 398–404. New York, NY, USA:\nACM.\n[2015] Quirk, C.; Mooney, R.; and Galley, M. 2015. Lan-\nguage to code: Learning semantic parsers for if-this-then-\nthat recipes. In Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), 878–888. Beijing, China:\nAssociation for Computational Linguistics.\n[2016] Russell, N.; van van der Aalst, W. M.; and ter Hof-\nstede, A. H. M. 2016. Workﬂow Patterns: The Deﬁnitive\nGuide. The MIT Press.\n[2016] Ur, B.; Pak Yong Ho, M.; Brawner, S.; Lee, J.; Men-\nnicken, S.; Picard, N.; Schulze, D.; and Littman, M. L.\n2016. Trigger-action programming in the wild: An analy-\nsis of 200,000 ifttt recipes. In Proceedings of the 2016 CHI\nConference on Human Factors in Computing Systems, CHI\n’16, 3227–3231. New York, NY, USA: ACM.\n[2018] Yao, Z.; Li, X.; Gao, J.; Sadler, B. M.; and Sun, H.\n2018. Interactive semantic parsing for if-then recipes via\nhierarchical reinforcement learning. CoRR abs/1808.06740.\n[2017] Yin, P., and Neubig, G.\n2017.\nA syntactic neural\nmodel for general-purpose code generation. In Proceedings\nof the 55th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), 440–450. Van-\ncouver, Canada: Association for Computational Linguistics.\n[2018] Yin, P., and Neubig, G. 2018. TRANX: A transition-\nbased neural abstract syntax parser for semantic parsing and\ncode generation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: Sys-\ntem Demonstrations, 7–12. Brussels, Belgium: Association\nfor Computational Linguistics.\n",
  "categories": [
    "cs.AI",
    "cs.CL"
  ],
  "published": "2020-01-08",
  "updated": "2020-01-08"
}