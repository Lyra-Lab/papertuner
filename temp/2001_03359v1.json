{
  "id": "http://arxiv.org/abs/2001.03359v1",
  "title": "Deep Interactive Reinforcement Learning for Path Following of Autonomous Underwater Vehicle",
  "authors": [
    "Qilei Zhang",
    "Jinying Lin",
    "Qixin Sha",
    "Bo He",
    "Guangliang Li"
  ],
  "abstract": "Autonomous underwater vehicle (AUV) plays an increasingly important role in\nocean exploration. Existing AUVs are usually not fully autonomous and generally\nlimited to pre-planning or pre-programming tasks. Reinforcement learning (RL)\nand deep reinforcement learning have been introduced into the AUV design and\nresearch to improve its autonomy. However, these methods are still difficult to\napply directly to the actual AUV system because of the sparse rewards and low\nlearning efficiency. In this paper, we proposed a deep interactive\nreinforcement learning method for path following of AUV by combining the\nadvantages of deep reinforcement learning and interactive RL. In addition,\nsince the human trainer cannot provide human rewards for AUV when it is running\nin the ocean and AUV needs to adapt to a changing environment, we further\npropose a deep reinforcement learning method that learns from both human\nrewards and environmental rewards at the same time. We test our methods in two\npath following tasks---straight line and sinusoids curve following of AUV by\nsimulating in the Gazebo platform. Our experimental results show that with our\nproposed deep interactive RL method, AUV can converge faster than a DQN learner\nfrom only environmental reward. Moreover, AUV learning with our deep RL from\nboth human and environmental rewards can also achieve a similar or even better\nperformance than that with the deep interactive RL method and can adapt to the\nactual environment by further learning from environmental rewards.",
  "text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identiﬁer\nDeep Interactive Reinforcement Learning\nfor Path Following of Autonomous\nUnderwater Vehicle\nQILEI ZHANG1, JINYING LIN1, QIXIN SHA1, BO HE1,(Member, IEEE) and GUANGLIANG\nLI1,(Member, IEEE)\n1Department of Electronic Engineering, Ocean University of China, Qingdao, China\nCorresponding author: Guangliang Li (e-mail: guangliangli@ouc.edu.cn).\nABSTRACT\nAutonomous underwater vehicle (AUV) plays an increasingly important role in ocean\nexploration. Existing AUVs are usually not fully autonomous and generally limited to pre-planning or pre-\nprogramming tasks. Reinforcement learning (RL) and deep reinforcement learning have been introduced\ninto the AUV design and research to improve its autonomy. However, these methods are still difﬁcult to\napply directly to the actual AUV system because of the sparse rewards and low learning efﬁciency. In\nthis paper, we proposed a deep interactive reinforcement learning method for path following of AUV by\ncombining the advantages of deep reinforcement learning and interactive RL. In addition, since the human\ntrainer cannot provide human rewards for AUV when it is running in the ocean and AUV needs to adapt\nto a changing environment, we further propose a deep reinforcement learning method that learns from both\nhuman rewards and environmental rewards at the same time. We test our methods in two path following\ntasks—straight line and sinusoids curve following of AUV by simulating in the Gazebo platform. Our\nexperimental results show that with our proposed deep interactive RL method, AUV can converge faster\nthan a DQN learner from only environmental reward. Moreover, AUV learning with our deep RL from both\nhuman and environmental rewards can also achieve a similar or even better performance than that with the\ndeep interactive RL method and can adapt to the actual environment by further learning from environmental\nrewards.\nINDEX TERMS Autonomous Underwater Vehicle, Interactive Reinforcement Learning, Deep Q Network,\nPath Following\nI. INTRODUCTION\nI\nN recent years, the role of autonomous underwater vehicle\n(AUV) in ocean exploration has become more and more\nimportant. Equipped with a series of chemical and biological\nsensors, AUV can conduct continuous operation without\nhuman intervention in the ocean environment. In addition,\nit can work independently adjusting to the changes of marine\nenvironment to complete the ocean observation task. Because\nof the less investment, good maneuverability and ﬂexible\ncontrol, AUV has been widely applied in many ﬁelds, such\nas scientiﬁc observation, resource investigation, oil and gas\nengineering, military applications etc.\nHowever, today’s marine applications put forward higher\nand higher requirements for the autonomy of AUV. Ex-\nisting AUVs usually do not have good autonomy and are\ngenerally limited to pre-planning or pre-programming tasks.\nThey work well in known and structured environments, but\nnot in uncertain and dynamic ones. Therefore, to realize\nthe autonomy of AUV, it is necessary for it to have strong\nabilities of environmental perception and understanding, on-\nline adjustment of control policies, and task planning. The\npath planning and following of AUV, which determines the\napplication prospect of AUV in the marine ﬁeld, can only be\nrealized with accurate control technology, in consideration of\nits energy consumption, motion characteristics, speed con-\nstraints, etc. Therefore, autonomous control that can adapt to\nthe changes of marine environment is the core technology to\nrealize the autonomy of AUV.\nPID control is the most popular traditional control methods\nand has been successfully applied to AUV [1]. However,\nthe traditional control methods cannot respond and adjust\nto unpredictable environmental changes in real time, and\ncannot meet the autonomy of AUV. On the other hand, robot\nlearning based on reinforcement learning (RL) [2] has been\nVOLUME 4, 2016\n1\narXiv:2001.03359v1  [cs.AI]  10 Jan 2020\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nintroduced into the AUV design and research to improve its\nautonomy [3]. Reinforcement learning is a method for a robot\ncontroller to learn optimal control policy through interaction\nwith the environment. The policy deﬁnes which action the\ncontroller should take when the robot is in a certain envi-\nronmental state. Under the current policy, after the controller\ntries to select and execute an action in a certain state, it\nwill receive a reward signal provided by the reward function\ndeﬁned in advance by the designer in the environment. This\nreward signal reﬂects the quality of the actions performed by\nthe controller and is used to update the control policy. The\ngoal of the controller is to learn a policy that maximizes the\ntotal cumulative reward.\nRecently, Yu et al. [4] applied the latest deep reinforcement\nlearning (DRL) developed by researchers at Google Deep-\nMind [5] to AUV path following task. Deep reinforcement\nlearning combines the advantages of deep learning (DL)\n[6] and reinforcement learning, and can realize the end-to-\nend autonomous learning and control with the raw high-\ndimensional environment perception information input to the\nbehavior action. Yu et al. claimed that AUV with DRL can\nachieve better control effect than a PID controller in simula-\ntion experiments. However, because it is difﬁcult to deﬁne an\neffective reward function and it usually provides very sparse\nreward signals, the robot needs a lot of time and samples to\nexplore and test before learning an effective control policy.\nTherefore, the traditional reinforcement learning and deep\nreinforcement learning methods are still difﬁcult to apply\ndirectly to the actual AUV system.\nIn order to speed up the robot learning, researchers propose\ninteractive reinforcement learning (IRL) [7] based on reward\nshaping [8] in traditional reinforcement learning methods.\nInteractive reinforcement learning allows designers and even\nnon-technical personnel to train robots by evaluating their\nbehavior. In this way, human experience knowledge can be\nembedded into autonomous learning of robot to speed up its\nlearning.\nTherefore, in this paper, we combine the advantages of\ndeep reinforcement learning and interactive RL by propos-\ning deep interactive reinforcement learning for AUV path\nfollowing tasks. In addition, since the human trainer cannot\nprovide human rewards for AUV and AUV needs to adapt\nto a changing environment when it is running in the ocean,\nwe propose a deep reinforcement learning method that learns\nfrom both human rewards and environmental rewards at\nthe same time. We test our methods in two tasks—straight\nline and sinusoids curve following of AUV by simulating\nin Gazebo. The experimental results show that AUV with\nour deep interactive RL method learns much faster than\nwith a DQN learner from environmental reward. Moreover,\nour deep RL learning from both human and environmental\nrewards can also achieve a similar or even better performance\nthan the deep interactive RL method and adapt to the actual\nenvironment by learning from environmental rewards.\nII. RELATED WORK\nPID controller is the most popular traditional controller for\nAUV and has been successfully applied in many control\nengineering tasks [1]. The disadvantage of PID controller\nis that its effect will be affected by the disturbance in the\ncomplex environment. However, due to the simple structure,\nthe control of many underwater robots is still designed with\nPID controller. Although the traditional control methods such\nas PID controller can basically meet the control requirements\nof AUV, they cannot adapt to unpredictable environmental\nchanges in real time and cope with the uncertainty of external\nenvironment. Therefore, they cannot realize the autonomy of\nAUV.\nReinforcement learning (RL) [2] has been successfully\napplied in many robot tasks [3], and has been introduced\ninto the autonomous control of AUV. Compared with the\ntraditional control methods of AUV, a robot with reinforce-\nment learning can achieve online parameter adjustment and\ncan well cope with environmental changes and uncertainties.\nEven in the absence of accurate system model or high cou-\npling system, root learning with RL can also obtain good\ncontrol effect. For example, Yuh proposed an adaptive control\nalgorithm based on neural network [9]. In the algorithm, a\ncost function is designed as reward function and the rein-\nforcement learning method is used to realize adaptive control.\nDifferent from the traditional adaptive control method, Yuh\nand other simulation experiments show that AUV with rein-\nforcement learning can not only deal with the environmental\nchanges and uncertainties, but also has good robustness to the\nunmodeled system dynamics. El Fakdi and Carreras applied\nthe policy gradient based reinforcement learning method to\nthe underwater robot in the submarine cable tracking task,\nand showed good tracking results [10], [11].\nIn addition, similar to the Actor-Critic algorithm [12] in\nreinforcement learning, Cui et al. [13] designed two neural\nnetwork models using reinforcement learning: one is used to\nevaluate the long-term control performance of the system;\nthe other is used to compensate the unknown dynamic of\nthe system, such as unknown nonlinear characteristics and\ninterference. The simulation results of Cui et al. show that\ntheir proposed reinforcement learning algorithm based on the\ndual neural network converges faster than the general neural\nnetwork model and PD controller, and has better stability\nand control effect. In addition, in order to solve the problem\nof high-dimensional sensory information input, Yu et al. [4]\napplied the deep reinforcement learning method [5] to AUV\npath following task, which can achieve better control effect\nthan PID control in their simulation experiments.\nHowever, in traditional reinforcement learning and deep\nreinforcement learning, the reward function is pre-deﬁned by\nthe agent designer before robot learning, which determines\nthe quality of the ﬁnal learned control policy and the learning\nspeed to a large extent. To deﬁne an effective reward function\nis not easy, which often needs many debugging and is very\ntime-consuming. More importantly, much experience and\nknowledge are difﬁcult to be embedded in an effective reward\n2\nVOLUME 4, 2016\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nfunction. An inefﬁcient reward function means sparse reward\nsignal and low learning efﬁciency. The controller needs a\nlot of learning samples and time to test and explore before\nlearning an optimal policy, which seriously limits the applica-\ntion of the reinforcement learning method to the actual AUV\ncontrol system.\nBased on traditional reinforcement learning, researchers\nproposed interactive reinforcement learning method [7],\nwhich allows people to observe and evaluate the robot’s\nbehavior, and use the evaluation as a reward signal to teach\nthe robot how to perform tasks. A robot with interactive rein-\nforcement learning does not need to deﬁne reward function in\nadvance, but allows a human trainer to provide reward signal\nby evaluating the robot’s behavior according to one’s own\nexperience and knowledge. The human reward signal reﬂects\nthe quality of robot’s behavior, and is used by the robot\nto improve its behavior. Therefore, the robot can easily use\npeople’s experience and knowledge to accelerate its learning.\nThomaz and Breazeal [14] implemented a tabular Q-\nlearning [15] agent which can learn from both human and\nenvironmental rewards by maximizing the accumulated dis-\ncounted sum of them. They also show that an agent’s perfor-\nmance can be further improved if the human teacher was al-\nlowed to provide action advice besides human rewards. Knox\nand Stone [16], [17] proposed the TAMER framework—\na typical interactive reinforcement learning method. The\nTAMER framework trains a regression model to represent\na reward function that gives a reward consistent with the\nhuman’s feedback. TAMER has been tested in many simu-\nlation domains [17], such as Cart Pole, Grid World, Tetris\netc., and even in a real robot navigation task [18]. The results\nshowed that agents with TAMER can learn faster than those\nwith traditional reinforcement learning. In order to make full\nuse of the information in human feedback, Loftin et al. [19]\ninterpreted human reward as a kind of categorical feedback\nstrategy. In addition, unlike TAMER and the work of Loftin\net al., which take human reward signal as feedback to the op-\ntimal control policy expected by the trainer, Macglashan et al.\n[20] proposed the COACH algorithm by further interpreting\nhuman reward signal as feedback to the control policy that\nthe robot is executing, and test it in the robot navigation task.\nTheir results show that COACH can make the robot more\neffective in autonomous learning.\nCarrera and Ahmadzadeh et al. [21]–[23] used a imitation\nlearning method [24] to achieve the autonomous valve ro-\ntation task for AUV. They ﬁrst asked the trainer to provide\na demonstration on how to operate the valve based on their\nknowledge, and the AUV recorded the operation and learned\nhow to turn the valve using a dynamic primitive method.\nHowever, this only maximizes the ability of AUV to perform\nthe task and cannot be applied to other tasks or adapt to\nchanges in the marine environment.\nIII. BACKGROUND\nAs a branch of machine learning, reinforcement learning is\nusually modeled as an Markov decision process (MDP) [2].\nMDP mainly consists of ﬁve elements: agent, environment,\nstate, action and reward. In reinforcement learning, an agent\ninteracts with the environment by acquiring the environment\nstate, performing actions and obtaining rewards. The basic\nframework of reinforcement learning is shown in Figure 1.\nSuppose that the environmental state at time t is st, and the\nagent performs an action at after obtaining the state st, and\nthe environmental state is transformed from st to st+1 at time\nt + 1. Then the environment generates feedback reward rt+1\nto the agent in the new state st+1. The agent will update\nthe learned policy with the reward signal and perform a new\naction at+1 in the new state. The agent will optimizes the\npolicy by continually interacting with the environment until\nan optimal policy is learned. The agent’s goal is to maximize\nthe long-term cumulative rewards.\nFIGURE 1: Illustration of learning mechanism in reinforce-\nment learning.\nIn reinforcement learning, a general policy π maps states\nto actions and has Markov property. The probability of taking\naction a in the current state is only related to the current state,\nand has nothing to do with other factors. The policy can be\nformulated as:\nπ(a|s) = p(at = a|st = s).\n(1)\nMaximizing a long-term cumulative rewards means the agent\nneeds to consider the rewards of the current time step as well\nas the rewards of the future time. Assume at current time\nstep t and long-term cumulative rewards can be formulated\nas R = rt + rt+1 + ... + rn. However, due to the uncertainty\nof cumulative rewards, discounted future cumulative rewards\nGt is generally used in actual tasks:\nGt = Rt+1 + γRt+2 + γ2Rt+3 + ... = Rt+1 + γGt+1, (2)\nwhere γ is the discount factor, ranging from 0 to 1.\nA value function with discounted cumulative rewards is\nusually used to express the degree of goodness or badness\nfor an agent in a certain state. There are mainly two kinds of\nvalue functions: state value function and action value func-\ntion. A state value function V (s) represents the expectation\nof discounted cumulative rewards for an agent in state s:\nV (s) = E[Gt|st = s],\n(3)\nwhere Gt represents the discount accumulated reward in\nthe future. The action value function Q(s, a) represents the\nVOLUME 4, 2016\n3\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nreward expectation of discounted cumulative rewards for\nan agent performing an action at in a certain state st and\nfollowing the policy π(s) thereafter:\nQ(s, a) = E[Gt|st = s, at = a].\n(4)\nThe relation of action value function Q(s, a) and state value\nfunction V (s) can be formulated as:\nV (s) =\nX\na∈A\nπ(a|s)Qπ(s, a).\n(5)\nA Bellman equation is formulated to express the relationship\nbetween the state values of state st and state st+1:\nVπ(s) = E[Rt+1 + γVπ(st+1)|st = s].\n(6)\nThe objective of reinforcement learning is to ﬁnd an\noptimal policy expressed as π∗, which can be obtained by\nmaximizing the value function V(s) or Q(s, a) under all\npolicies:\nV ∗(s) = maxπV (s),\n(7)\nQ∗(s, a) = maxπQ(s, a).\n(8)\nTherefore, once the optimal value function is obtained, the\noptimal policy can be found by greedily selecting actions\naccording to it.\nBy combining reinforcement learning with deep learning\n[6], deep reinforcement learning (DRL) [5] fully embodies\nthe perceived advantages of representation learning in deep\nlearning and decision-making in reinforcement learning. In\ndeep reinforcement learning, the problem is still deﬁned by\nreinforcement learning, and the policy and value function can\nbe represented with deep neural network and optimized based\non an objective function.\nA. DEEP Q-NETWORK\nMnih et al. proposed the ﬁrst deep reinforcement learning\nalgorithm — deep Q-network (DQN) and tested in the Atari\ngames [5]. It combines deep learning and reinforcement\nlearning to successfully learn control policies directly from\nraw high-dimensional inputs. Speciﬁcally, DQN combines\nthe neural network and Q-learning algorithm [15] to ﬁt the Q\nvalue of each action with the deep neural network. The state\nvalue function Vπ(s) and action value function Qπ(s, a) are\napproximated as ˆV (s, θ) and ˆQ(s, a, θ), where θ represents\nthe weights. For the state value function ˆV (s, θ), the input\nof the neural network is the eigenvector of state s, and the\noutput is the corresponding value function. For the action\nvalue function, the input is the feature vector of the state,\nand the output is the action value function with each action\ncorresponds to an output.\nIn DQN, a value function represented with deep neural\nnetwork is learned and optimized with state data. The DQN\nalgorithm uses two networks for learning: a prediction net-\nwork Q(s, a, θ) and target network Q\n′(s, a, θ\n′). The predic-\ntion network is used to evaluate the current state action and\nupdated at each time step. The target network Q\n′(s, a, θ\n′) is\nused to generate target value. The target network is directly\ncopied from the prediction network every certain number of\ntime steps and does not update its parameters. The objective\nof introducing the target network in DQN is to keep the target\nQ value unchanged for a period of time. In this case, the\ncorrelation between the ﬁeld prediction Q value and the target\nQ value will be correlated to some extent, and the network\ninstability during training can be reduced.\nSpeciﬁcally, a DQN agent will select the action with\nthe largest Q value based on the outputted Q value by\nthe network. The experience replay mechanism [25], [26]\nis also used in DQN. It stores the experience samples\nn\ns, a, r, s\n′, ifend\no\n(ifend ﬂag indicates whether the task is\ncompleted) obtained from the interaction between the agent\nand environment at each time step into the experience replay\npool. When training the network, a small batch of samples is\nrandomly selected from the experience replay pool and the\ncurrent target Q value y is computed as:\nyi =\n\u001a Ri,\nifend = true\nRi + γmaxa ˆ\nQ\n′(s\n′, a\n′, θ\n′),\nifend = false .\n(9)\nThis helps remove the correlation and dependence between\nsamples and makes the network more convergent. With the\nmean square error loss function\nL = 1\nN\nN\nX\ni=1\n(yi −ˆQ(s, a, θ))2,\n(10)\nall parameters θ of Q network will be updated through\ngradient back propagation [27].\nB. INTERACTIVE REINFORCEMENT LEARNING\nAgent\nEnvironment\nEvaluative \nfeedback\nht\nState\nst\nAction\nɑt\nHuman\nst+1\nht+1\nFIGURE 2: Illustration of interactive reinforcement learning\nwith human feedback.\nIn reinforcement learning and deep reinforcement learn-\ning, the reward function is pre-deﬁned by the controller de-\nsigner before agent learning. The reward function determines\nthe quality of the ﬁnal control policy and the learning speed\n4\nVOLUME 4, 2016\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nto a large extent. However, it is not easy to deﬁne an effective\nreward function since it often needs many debugging, and\nmuch experience and knowledge are difﬁcult to be embedded\nin an effective reward function. An inefﬁcient reward func-\ntion means that the controller needs a lot of learning samples\nand time to test and explore, which seriously limits the\napplication of the traditional reinforcement learning method\nto the actual robot system.\nTherefore, on the basis of traditional reinforcement learn-\ning, researchers put forward interactive reinforcement learn-\ning [7], [14], [16], [17], [19], [20], [28]–[30]. In interactive\nreinforcement learning, an agent learns in an MDP without\nreward function. It allows people to observe and evaluate\nthe agent’s behavior, and give a reward signal based on their\njudgement of agent’s action selection. The stand or fall of\nreward signal reﬂects the quality of agent’s behavior. The\nagent uses the reward signal delivered by the human trainer to\nimprove its behavior. The learning mechanism of interactive\nreinforcement learning is shown in Figure 2.\nIV. APPROACH\nIn this paper, we propose deep interactive reinforcement\nlearning to complete the underwater vehicle path following\ntask. In our proposed method, we use the DQN algorithm.\nHowever, the robot learns from human reward instead of pre-\ndeﬁned environment reward as in the original DQN. In our\nproposed method, the trainer provides rewards Rh observing\nthe running state of AUV, and evaluating the actions selected\nby AUV in the current state according to her knowledge. The\nlearning mechanism of DQN is shown in Figure 3.\nIn addition, since the human trainer cannot provide re-\nwards to train AUV all the time and AUV also needs to adapt\nto the changing ocean environment in the task, we propose to\nallow our proposed method to learn from both human reward\nand environment reward at the same time.\nWe test our methods in two path following tasks: straight\nline and sinusoids curve following. In both tasks, the input\nto the behavior value function network of DQN is the state\nof AUV. In the straight line following task, the input state\ns is represented by the current course of AUV and the\nshortest distance from AUV to the target path: S = {d\nc},\nwhere d represents the distance from the position of AUV\nto P(x,y)—the intersection point of the perpendicular line to\nthe horizontal axis at the position of AUV and the target line,\nc represents the current course angle of AUV, as shown in\nFigure 4. In the sinusoids curve following task, the slope of\nthe tangent line at the intersection point P(x,y) and desired\ncourse angle are added as additional features to represent the\nstate: S = {d\nc\nk\ncd}, where d represents the distance\nfrom the position of AUV to the intersection point P(x,y),\nc represents current course angle, k represents the slope of\nthe tangent line at the intersection point P(x,y), cd represents\ndesired course angle, as shown in Figure 5. The action space\nis the rudder value of AUV, and different actions correspond\nto different rudder angle values.\nThe environment reward function is deﬁned as the differ-\nence between the current course angle and the desired course\nangle of AUV, considering the distance from AUV to the\ntarget line/curve. The way we calculate the desired course\nangle is similar to the line of sight (LOS) algorithm [31]. For\nexample, in the straight line following task, we choose the\ncurrent target path L with a ﬁxed value from the intersection\npoint P(x, y) along the target line. Then the destination point\nS(xd, yd) is decided since the length of the current path L\nis ﬁxed. The desired course angle is computed as the angle\nbetween the desired course from the current position of AUV\nto the destination point S(xd, yd) and the horizontal axis, as\nshown in Figure 4.\nIn the sinusoids curve following task, the computation of\nthe desired course angle is a bit different from the straight line\nfollowing task. Speciﬁcally, we choose current target path L\nwith a ﬁxed length along the tangent line at the intersection\npoint P(x, y). Then the desired course angle is computed\nas the angle between the desired course from the current\nposition of AUV to the current target point S(xd, yd) and the\nhorizontal axis, as shown in Figure 5.\nIf the distance between the AUV and the following line or\ncurve is directly added to the reward function, it may lead to a\nlarge ﬂuctuation of the reward value. Therefore, we introduce\nan exponential transformation to deﬁne the reward function\nas:\nR = −0.9 · |cd −c| + 0.1 · 22−d\n10 ,\n(11)\nwhere c represents the actual current course angle of AUV,\nd represents the actual distance from AUV to the target\nline/curve, cd indicates the desired course angle. In this case,\nwhen the action selected by AUV reduces the difference\nbetween the current course and the desired course or the\ndistance between AUV and the target path is smaller, the\nreceived reward R will be larger, otherwise R will be smaller.\nDuring the learning process, the experience received feed-\nback is stored as a sample in the experience replay pool.\nA small batch of samples in the experience replay pool is\nrandomly selected to update the parameters of Q network as\nthe DQN algorithm.\nV. EXPERIMENTS\nTo verify the effectiveness of our proposed method, we\nconducted experiments with an extension to the open-source\nrobot simulator Gazebo in underwater scenarios. The Au-\ntonomous Underwater Vehicle (AUV) Simulator [32] was\nused in our experiment. However, we modiﬁed it to make\nﬁt with the actual AUV system in our lab, as shown in Figure\n6. The AUV simulator uses the robot operating system (ROS)\nto communicate with the underwater environment. It can also\nsimulate multiple underwater robots and intervention tasks\nusing robotic manipulators.\nIn our proposed deep interactive reinforcement learning\nmethod, the human trainer needs to evaluate the state and\naction of AUV. It is not easy to observe the exact action of\nVOLUME 4, 2016\n5\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 3: Learning mechanism of deep interactive reinforcement learning algorithm.\nFIGURE 4: The desired course angle in the straight line\nfollowing task, where L represents the current target path,\nand d represents the distance from AUV to the target line, S\nrepresents the current target point.\nAUV in the simulated environment. Therefore, we developed\na human-machine interaction interface for the human trainer\nto observe the attitude of AUV in real time using the Rviz\ndisplay tool, as shown in Figure 7. Rviz is a built-in graphical\ntool of ROS, which makes it very convenient for users to\ndevelop and debug ROS through the graphical interface.\nIn our experiments, the AUV simulator selects and exe-\ncutes an action a based on its current state s and learned\npolicy. The human trainer observes the action selected by\nthe AUV simulator in the current state from the visualized\nFIGURE 5: The desired course angle in the sinusoids curve\nfollowing task, where L represents the current target path,\nd represents the distance from AUV to the target curve, S\nrepresents the current target point.\ninterface and evaluates its quality according to her knowl-\nedge and experience. The evaluation will be taken as human\nreward and delivered to the AUV simulator via the developed\nhuman-machine interface. The AUV simulator will use it to\nupdate the DQN network parameters to improve its control\npolicy.\nWe trained three agents in our experiments: the DQNH\nagent, a DQN agent learning from only human provided\nreward as we proposed; the DQNHE agent, a DQN agent\nlearning from both environment reward and human pro-\n6\nVOLUME 4, 2016\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 6: The autonomous underwater vehicle simulator in\nthe Gazebo robotic platform used in our experiment.\nFIGURE 7: Human-machine interaction interface for dis-\nplaying of the real-time attitude of AUV. Note that the solid\ngreen line represents target path.\nvided reward; the DQNE agent, a baseline agent where a\nDQN agent learns from only environment reward. For both\nDQNHE and DQNE agents, the environment reward is pro-\nvided via Equation 11. For the DQNH and DQNHE agents\nto learn from human reward, the human trainer will give\na reward value +0.8 or +0.5 when she thinks the AUV’s\nmovement is good through the developed interface. When\nshe thinks the AUV’s action is bad, a reward value -0.8 or\n-0.5 will be given. Speciﬁc reward value can be selected by\nthe trainer according to her experience, as below:\nRh =\n\u001a +0.8\nor + 0.5,\ngood\naction\n−0.8\nor −0.5,\nbad\naction.\n(12)\nWhen the DQNHE agent learning, the human reward Rh is\nadded to the environment reward R as the ﬁnal reward value.\nVI. EXPERIMENTAL RESULTS AND DISCUSSION\nIn this section, we present our experimental results tested in\ntwo path following tasks: straight line following and sinu-\nsoids curve following. We compare our proposed DQN agent\nlearning from solely human reward, termed DQNH, DQN\nagent learning from both human reward and environmental\nreward, termed DQNHE, to the original DQN agent learning\nfrom solely environment reward, termed DQNE.\nA. STRAIGHT LINE FOLLOWING\nIn this experiment, we trained the DQNH, DQNHE and\nDQNE agents in the straight line following task. Figure 8\nshows the trajectories of the DQNE agent learning at different\nlearning episodes in the straight line task. From Figure 8 we\ncan see that, at Episode 1, the trajectory of the agent ﬂuctuate\ndramatically along the target line at both sides. At Episode\n10, the ﬂuctuation of the learning trajectory was reduced to\nsome extent and at Episode 15, the agent can already follow\na line but still far away from the target one. Until the 50th\nepisode, the trajectory of the DQNE agent is quite close to\nthe target line.\nFIGURE 8: Trajectories of DQNE agent learning in the\nstraight line following task. Note that X is the coordinate\nalong the horizontal axis, Y is the coordinate along the\nvertical axis.\nFigure 9 shows the trajectories of the DQNH agent learn-\ning at different learning episodes in the straight line following\ntask. From Figure 9 we can see that, at Episode 1, the\ntrajectory of the DQNH agent is similar to the one of the\nDQNE agent and ﬂuctuate along the target line at both sides.\nAt Episode 5, the ﬂuctuation of the DQNH agent learning\ntrajectory was reduced to a large extent. At Episode 10, the\ntrajectory of the DQNH agent is already quite close to the\ntarget line, achieving a similar performance to the DQNE\nagent at the 50th episode as shown in Figure 8. By comparing\nthe learning trajectories of DQNH agent in Figure 9 to those\nof DQNE agent in Figure 8, we found that a DQN agent\nlearning from solely human reward converges much faster\nthan learning from solely environment reward.\nFigure 10 shows the trajectories of the DQNHE agent\nlearning at different learning episodes in the straight line\nVOLUME 4, 2016\n7\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 9: Trajectories of the DQNH agent learning in the\nstraight line following task. Note that X is the coordinate\nalong the horizontal axis, Y is the coordinate along the\nvertical axis.\nfollowing task. From Figure 10 we can see that, at Episode 1,\nthe trajectory of the DQNHE agent is similar to the ones of\nthe DQNE agent and DQNH agent. However, at Episode 5,\nthe ﬂuctuation of the DQNHE agent learning trajectory was\ndramatically reduced and even much better than that of the\nDQNH agent. At Episode 10, the trajectory of the DQNHE\nagent is also close to the target line, achieving a similar\nperformance to the DQNH agent. By comparing the learning\ntrajectories of DQNHE agent in Figure 10 to those of DQNH\nagent in Figure 9, we found that a DQN agent learning\nfrom both human reward and environment reward can further\nimprove its convergence speed, compared to learning from\nsolely human reward.\nFIGURE 10: Trajectories of the DQNHE agent learning in\nthe straight line following task. Note that X is the coordinate\nalong the horizontal axis, Y is the coordinate along the\nvertical axis.\nWe also analyzed the tracking error of the DQNE, DQNH,\nDQNHE agents along the learning process in the straight line\nfollowing task, as shown in Figure 11. From Figure 11 we can\nsee that, when the DQN agent learning from solely human\nreward or both from human reward and environment reward,\nthe tracking error was dramatically reduced to a minimum\nin about 10 episodes, while the DQNE agent learning from\nsolely environment reward achieves a similar performance\nuntil around the 40th episode.\nFIGURE 11: Tracking error of the DQNE, DQNH, DQNHE\nagents along the learning process in the straight line follow-\ning task (averaged over data collected in two trials).\nIn addition, the cumulative environment rewards obtained\nby the DQNE and DQNHE agents were analyzed and illus-\ntrated in Figure 12. Figure 12 indicates that the cumulative\nrewards obtained by DQNHE agent quickly reach the peak\nin around 10 episodes for the ﬁrst time, while it takes about\n40 episodes for the DQNE agent to reach a similar level.\nAfter that, both agents converged. These results suggest that\nthe additional human reward helps the DQN agent converge\nfaster.\nFIGURE 12: Cumulative environment rewards obtained by\nthe DQNE and DQNHE agents in the straight line following\ntask (averaged over data collected in two trials).\n8\nVOLUME 4, 2016\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nB. SINUSOIDS CURVE FOLLOWING\nIn this experiment, we trained the DQNH, DQNHE and\nDQNE agents in the sinusoids curve following task, which\nis a much complex one than the straight line following task.\nFigure 13 shows the trajectories of the DQNE agent\nlearning at different learning episodes in the sinusoids curve\nfollowing task. From Figure 13 we can see that, at Episode\n1 and 10, the DQNE agent cannot ﬁnish the task at all. Until\nthe 60th episode, the ﬂuctuation was reduced and the agent\ncan already follow the sinusoids curve with the trajectory\nquite close to the target curve. At Episode 100, the agent can\nalmost exactly follow the sinusoids curve.\nFIGURE 13: Trajectories of DQNE agent learning in the\nsinusoids curve following task. Note that X is the coordinate\nalong the horizontal axis, Y is the coordinate along the\nvertical axis.\nFigure 14 shows the trajectories of the DQNH agent\nlearning at different learning episodes in the sinusoids curve\nfollowing task. From Figure 14 we can see that, at Episode\n1 and 10, the DQNH agent cannot ﬁnish the task either.\nAt 20th episode, the DQNH agent can already follow the\nsinusoids curve with the trajectory quite close to the target\ncurve, achieving a similar performance to the DQNE agent at\nthe 60th episode. At Episode 25, the AUV with DQNH can\nalmost exactly follow the sinusoids curve, achieving a similar\nperformance to the DQNE agent at the 100th episode.\nFigure 15 shows the trajectories of the DQNHE agent\nlearning at different learning episodes in the sinusoids curve\nfollowing task. From Figure 15 we can see that, at Episode\n1 and 10, the DQNHE agent cannot ﬁnish the task either.\nAt the 20th episode, the DQNHE agent can also follow the\nsinusoids curve with the trajectory quite close to the target\ncurve. At Episode 25, the DQNHE agent can achieve a\nsimilar performance to that of the DQNE agent at Episode\n100.\nThe tracking errors of the DQNE, DQNH, DQNHE agents\nalong the learning process in the sinusoids curve following\ntask were shown in Figure 16. From Figure 16 we can\nFIGURE 14: Trajectories of DQNH agent learning in the\nsinusoids curve following task. Note that X is the coordinate\nalong the horizontal axis, Y is the coordinate along the\nvertical axis.\nFIGURE 15: Trajectories of DQNHE agent learning in the\nsinusoids curve following task. Note that X is the coordinate\nalong the horizontal axis, Y is the coordinate along the\nvertical axis.\nsee that, when the DQN agent learning from solely human\nreward, the tracking error was dramatically reduced to the\nlowest level in about 20 episodes, while the DQNE agent\nlearning from solely environment reward achieves a similar\nperformance until around the 70th episode. When learning\nboth from human reward and environment reward, the track-\ning error is reduced the fastest.\nIn addition, the cumulative environment rewards obtained\nby the DQNE and DQNHE agents were analyzed and illus-\ntrated in Figure 17. Figure 17 indicates that the cumulative\nrewards obtained by DQNHE agent also quickly reach the\npeak in around 20 episodes for the ﬁrst time, while it takes\nabout 80 episodes for the DQNE agent to reach a similar\nlevel. After that, both agents converged.\nVOLUME 4, 2016\n9\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 16: Tracking error of the DQNE, DQNH, DQNHE\nagents along the learning process in the sinusoids curve\ntracking task (averaged over data collected in two trials)\nFIGURE 17: Cumulative environment rewards obtained of\nthe DQNE and DQNHE agents in the sinusoids curve fol-\nlowing task (averaged over data collected in two trials).\nIn summary, our results in both the straight line and\nsinusoids curve following tasks suggest that our proposed\ndeep interactive reinforcement learning method can facilitate\na DQN agent to converge much faster than learning solely\nfrom environment reward. Moreover, our proposed DQNHE\nagent learning from both human reward and environment\nreward can combine the advantages of our proposed deep\ninteractive reinforcement learning from human reward and\nlearning from environment reward. The DQNHE method\nfacilitate AUV to reach a similar performance to the one\nlearning from solely environment reward with the speed same\nto or even better than the one learning from solely human\nreward. This allows AUV to keep at the peak performance\nand adapt to the environment even when the human reward is\nnot available in the ocean.\nVII. CONCLUSION\nIn this paper, we proposed a deep interactive reinforcement\nlearning method for path following of AUV by combining\nthe advantages of deep reinforcement learning and interactive\nRL. In addition, since the human trainer cannot provide\nhuman rewards for AUV all the time and AUV needs to\nadapt to a changing environment when it is running in the\nocean, we propose a deep reinforcement learning method that\nlearns from both human rewards and environmental rewards\nat the same time. We test our methods in two tasks—straight\nline and sinusoids curve following of AUV by simulating\nin the Gazebo platform. Our experimental results show that\nwith our propose deep interactive RL method AUV can learn\nmuch faster than that a DQN learner from only environmental\nreward. Moreover, our deep RL learning from both human\nand environmental rewards can also achieve a similar or even\nbetter performance than the deep interactive RL method and\nadapt to the actual environment by learning from environ-\nmental rewards.\nIn the future, we would like to extend and test our methods\nwith actual AUV system in ocean observation task.\nREFERENCES\n[1] K. H. Ang, G. Chong, and Y. Li, “Pid control system analysis, design, and\ntechnology,” IEEE transactions on control systems technology, vol. 13,\nno. 4, pp. 559–576, 2005.\n[2] R. Sutton and A. Barto, Reinforcement learning: an introduction. MIT\nPress, 1998.\n[3] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in robotics:\nA survey,” The International Journal of Robotics Research, vol. 32, no. 11,\npp. 1238–1274, 2013.\n[4] R. Yu, Z. Shi, C. Huang, T. Li, and Q. Ma, “Deep reinforcement learning\nbased optimal trajectory tracking control of autonomous underwater ve-\nhicle,” in 2017 36th Chinese Control Conference (CCC), pp. 4958–4965,\nIEEE, 2017.\n[5] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,” Nature,\nvol. 518, no. 7540, pp. 529–533, 2015.\n[6] J. Schmidhuber, “Deep learning in neural networks: an overview,” Neural\nNetworks, vol. 61, pp. 85–117, 2015.\n[7] G. Li, R. Gomez, K. Nakamura, and B. He, “Human-centered rein-\nforcement learning: A survey,” IEEE Transactions on Human-Machine\nSystems, vol. 49, no. 4, pp. 337–349, 2019.\n[8] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward\ntransformations: theory and application to reward shaping,” in Proceedings\nof the International Conference on Machine Learning (ICML), pp. 278–\n287, 1999.\n[9] J. Yuh, “Learning control for underwater robotic vehicles,” IEEE Control\nSystems Magazine, vol. 14, no. 2, pp. 39–46, 1994.\n[10] A. El-Fakdi and M. Carreras, “Policy gradient based reinforcement learn-\ning for real autonomous underwater cable tracking,” in 2008 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems, pp. 3635–\n3640, IEEE, 2008.\n[11] M. Carreras, A. El-fakdi, and P. Ridao, “Behavior adaptation by means\nof reinforcement learning,” in Marine Robot Autonomy, pp. 287–328,\nSpringer, 2013.\n[12] I. Grondman, L. Busoniu, G. A. Lopes, and R. Babuska, “A survey of actor-\ncritic reinforcement learning: standard and natural policy gradients,” IEEE\nTransactions on Systems, Man, and Cybernetics, Part C (Applications and\nReviews), vol. 42, no. 6, pp. 1291–1307, 2012.\n[13] R. Cui, C. Yang, Y. Li, and S. Sharma, “Adaptive neural network control of\nauvs with control input nonlinearities using reinforcement learning,” IEEE\nTransactions on Systems, Man, and Cybernetics: Systems, vol. 47, no. 6,\npp. 1019–1029, 2017.\n[14] A. L. Thomaz and C. Breazeal, “Teachable robots: understanding human\nteaching behavior to build more effective robot learners,” Artiﬁcial Intelli-\ngence, vol. 172, no. 6, pp. 716–737, 2008.\n[15] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, no. 3-\n4, pp. 279–292, 1992.\n10\nVOLUME 4, 2016\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[16] W. B. Knox and P. Stone, “Interactively shaping agents via human rein-\nforcement: the TAMER framework,” in Proceedings of the 5th Interna-\ntional Conference on Knowledge Capture, pp. 9–16, ACM, 2009.\n[17] W. B. Knox, Learning from human-generated reward. PhD thesis, Univer-\nsity of Texas at Austin, 2012.\n[18] W. B. Knox, P. Stone, and C. Breazeal, “Training a robot via human\nfeedback: a case study,” in Proceedings of the International Conference\non Social Robotics, pp. 460–470, Springer, 2013.\n[19] R. Loftin, B. Peng, J. MacGlashan, M. L. Littman, M. E. Taylor, J. Huang,\nand D. L. Roberts, “Learning behaviors via human-delivered discrete\nfeedback: modeling implicit feedback strategies to speed up learning,”\nAutonomous agents and multi-agent systems, vol. 30, no. 1, pp. 30–59,\n2016.\n[20] J. MacGlashan, M. K. Ho, R. Loftin, B. Peng, D. Roberts, M. E. Taylor,\nand M. L. Littman, “Interactive learning from policy-dependent human\nfeedback,” in Proceedings of the 34th International Conference on Ma-\nchine Learning, vol. 70, pp. 2285–2294, 2017.\n[21] A. Carrera, M. Carreras, P. Kormushev, N. Palomeras, and S. Nagappa,\n“Towards valve turning with an auv using learning by demonstration,” in\n2013 MTS/IEEE OCEANS-Bergen, pp. 1–7, IEEE, 2013.\n[22] A. Carrera, N. Palomeras, N. Hurtos, P. Kormushev, and M. Carreras,\n“Learning by demonstration applied to underwater intervention,” 2014.\n[23] S. R. Ahmadzadeh, P. Kormushev, and D. G. Caldwell, “Autonomous\nrobotic valve turning: A hierarchical learning approach,” in 2013 IEEE\nInternational Conference on Robotics and Automation, pp. 4629–4634,\nIEEE, 2013.\n[24] B. D. Argall, S. Chernova, M. Veloso, and B. Browning, “A survey of robot\nlearning from demonstration,” Robotics and autonomous systems, vol. 57,\nno. 5, pp. 469–483, 2009.\n[25] L.-J. Lin, “Self-improving reactive agents based on reinforcement learn-\ning, planning and teaching,” Machine learning, vol. 8, no. 3-4, pp. 293–\n321, 1992.\n[26] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience\nreplay,” in Proceedings of International Conference on Learning Represen-\ntations, pp. 1–21, 2016.\n[27] M. Riedmiller and H. Braun, “A direct adaptive method for faster back-\npropagation learning: The rprop algorithm,” in Proceedings of the IEEE\ninternational conference on neural networks, vol. 1993, pp. 586–591, San\nFrancisco, 1993.\n[28] G. Li, S. Whiteson, W. B. Knox, and H. Hung, “Using informative\nbehavior to increase engagement while learning from human reward,”\nAutonomous Agents and Multi-Agent Systems, vol. 30, no. 5, pp. 826–\n848, 2016.\n[29] G. Li, Socially intelligent autonomous agents that learn from human\nreward. PhD thesis, University of Amsterdam, 2016.\n[30] G. Li, S. Whiteson, W. B. Knox, and H. Hung, “Social interaction for\nefﬁcient agent learning from human reward,” Autonomous Agents and\nMulti-Agent Systems, vol. 32, no. 1, pp. 1–25, 2018.\n[31] T. I. Fossen, M. Breivik, and R. Skjetne, “Line-of-sight path following of\nunderactuated marine craft,” IFAC proceedings volumes, vol. 36, no. 21,\npp. 211–216, 2003.\n[32] M. M. M. Manhães, S. A. Scherer, M. Voss, L. R. Douat, and T. Rauschen-\nbach, “UUV simulator: A gazebo-based package for underwater interven-\ntion and multi-robot simulation,” in OCEANS 2016 MTS/IEEE Monterey,\nIEEE, sep 2016.\nQILEI ZHANG received the Bachelor’s degree\nin electronic information science and technology\nfrom the School of Information Science and Engi-\nneering, Shandong Agricultural University, Taian,\nChina, in 2018. He is currently a Postgraduate\nStudent at School of Information Science and En-\ngineering, Ocean University of China, Qingdao,\nChina. His current research interests include re-\ninforcement learning, human agent/robot interac-\ntion, and robotics.\nJINYING LIN received the Bachelor’s degree\nin communication engineering from the School\nof Communication and Electronic Engineering,\nQingdao University of Technology, Qingdao,\nChina, in 2018. She is currently a Postgraduate\nStudent at School of Information Science and En-\ngineering, Ocean University of China, Qingdao,\nChina. Her current research interests include re-\ninforcement learning, human agent/robot interac-\ntion, and robotics.\nQIXIN SHA received the B.S. degree in com-\nmunication engineering from Ocean University\nof China in 2007, and the M.S. degree in com-\nmunication and information systems from Ocean\nUniversity of China in 2010. He worked at Qing-\ndao Bailing Technology Co., Ltd. and Alcatel-\nLucent Qingdao R & D Center from 2010 to 2014\nas a software engineer. He is currently working\nas an experimenter in the Department of Elec-\ntronic Engineering, Ocean University of China.\nHis research interests include the design and development of architecture,\ndecision-making and software system in underwater vehicle.\nBO HE (M’18) received the Ph.D. degree in con-\ntrol theory and control engineering from Harbin\nInstitute of Technology, Harbin, China, in 1999.\nHe was a Researcher with Nanyang Technolog-\nical University, Singapore, from 2000 to 2002. He\nis currently a Full Professor with Ocean University\nof China, Qingdao, China. His research interests\ninclude SLAM, machine learning, and robotics.\nGUANGLIANG LI (M’14) received the Bache-\nlor’s degree in automation and M.Sc. degree in\ncontrol theory and control engineering from the\nSchool of Control Science and Engineering, Shan-\ndong University, Jinan, China, in 2008 and 2011,\nrespectively, and the Ph.D. degree in computer\nscience from the University of Amsterdam, Am-\nsterdam, The Netherlands, in 2016.\nHe was a Visiting Researcher with Delft Univer-\nsity of Technology, Delft, The Netherlands, and a\nResearch Intern with the Honda Research Institute Japan, Co., Ltd., Wako,\nJapan. He is currently a Lecturer with Ocean University of China, Qing-\ndao, China. His research interests include reinforcement learning, human\nagent/robot interaction, and robotics.\nVOLUME 4, 2016\n11\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.RO"
  ],
  "published": "2020-01-10",
  "updated": "2020-01-10"
}