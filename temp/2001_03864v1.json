{
  "id": "http://arxiv.org/abs/2001.03864v1",
  "title": "Learning to drive via Apprenticeship Learning and Deep Reinforcement Learning",
  "authors": [
    "Wenhui Huang",
    "Francesco Braghin",
    "Zhuo Wang"
  ],
  "abstract": "With the implementation of reinforcement learning (RL) algorithms, current\nstate-of-art autonomous vehicle technology have the potential to get closer to\nfull automation. However, most of the applications have been limited to game\ndomains or discrete action space which are far from the real world driving.\nMoreover, it is very tough to tune the parameters of reward mechanism since the\ndriving styles vary a lot among the different users. For instance, an\naggressive driver may prefer driving with high acceleration whereas some\nconservative drivers prefer a safer driving style. Therefore, we propose an\napprenticeship learning in combination with deep reinforcement learning\napproach that allows the agent to learn the driving and stopping behaviors with\ncontinuous actions. We use gradient inverse reinforcement learning (GIRL)\nalgorithm to recover the unknown reward function and employ REINFORCE as well\nas Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal\npolicy. The performance of our method is evaluated in simulation-based scenario\nand the results demonstrate that the agent performs human like driving and even\nbetter in some aspects after training.",
  "text": "Learning to Drive via Apprenticeship Learning and\nDeep Reinforcement Learning\n1st Wenhui Huang\nIndustrial and Information Engineering\nPolitecnico Di Milano\nMilano, Italy\nwenhui.huang@mail.polimi.it\n2nd Francesco Braghin\nIndustrial and Information Engineering\nPolitecnico Di Milano\nMilano, Italy\nfrancesco.braghin@polimi.it\n3rd Zhuo Wang\nSchool of communication engineering\nXidian University\nXiAn, China\nzwang ll@stu.xidian.edu.cn\nAbstract—With the implementation of reinforcement learning\n(RL) algorithms, current state-of-art autonomous vehicle technol-\nogy have the potential to get closer to full automation. However,\nmost of the applications have been limited to game domains\nor discrete action space which are far from the real world\ndriving. Moreover, it is very tough to tune the parameters of\nreward mechanism since the driving styles vary a lot among\nthe different users. For instance, an aggressive driver may\nprefer driving with high acceleration whereas some conservative\ndrivers prefer a safer driving style. Therefore, we propose an\napprenticeship learning in combination with deep reinforcement\nlearning approach that allows the agent to learn the driving\nand stopping behaviors with continuous actions. We use gradient\ninverse reinforcement learning (GIRL) algorithm to recover the\nunknown reward function and employ REINFORCE as well as\nDeep Deterministic Policy Gradient algorithm (DDPG) to learn\nthe optimal policy. The performance of our method is evaluated\nin simulation-based scenario and the results demonstrate that\nthe agent performs human like driving and even better in some\naspects after training.\nIndex Terms—Reinforcement Learning Application, Inverse\nReinforcment Learning, Autonomous Driving\nI. INTRODUCTION\nRecent studies indicate that the interest in applying robotics\nand autonomous system to real life is growing dramatically [1],\n[2]. Especially, the pace of techinical upgrading and innovation\nfor autonomous vehicle driving is accelerating a lot [3] and this\nis mostly thanks to the capability of the machine learning(ML).\nReinforcement learning (RL), as one branch of the ML, is\nthe most widely used technique in sequential decision making\nproblem. RL can learn the optimal policy through a process\nby interacting with unknown environment. RL algorithms have\nbeen successfully applied to the autonomous driving in recent\nyears [4], [5]. However, these applications are not only limited\nto the discrete aciton problems but also suffer from ”curse of\ndimensionality” once the action extends to continuous state\nspace. In order to solve large continuous state space problem,\ndeep learning (DL) has been implemented in RL, yielding\ndeep reinforcement learning (DRL) [6]. In recent study, the\nDeep Deterministic Policy Gradient (DDPG) algorithm which\nbelongs to DRL family, has been successfully applied to target\nfollowing control [7].\nOne of the issues in RL is the reward function. Knowing\nthat autonomous vehicle driving is not a trivial problem,\nthe reward function is tough to be hand-made directly. To\novercome this problem, [8] proposed apprenticeship learning\nvia inverse reinforcement learning (IRL) approach . IRL aims\nat recovering the unknown reward function by observing\nexpert demonstration.\nBoth forward driving and stopping under the trafﬁc rules\nare frequent behaviors in real life driving. However, recent\nstudies [9], [10] are only focusing on obstacle avoidance and\nthere is no research on learning forward driving and stopping\nbehaviors by considering trafﬁc rules via reinforcement learn-\ning techniques. In this paper, we addressed above problem by\nmeans of apprenticeship learning in combination with DRL\napproach. More speciﬁcally, we implemented gradient inverse\nreinforcement learning (GIRL) algorithm [11] to recover the\nunknown reward funciton and employed DDPG algorithm in\norder to train the agent to drive by keeping trafﬁc rules and\nstop in front of the stop sign autonomously. Furthermore,\nREINFORCE algorithm is employed in RL step as well in\norder to compare the performance with DDPG algorithm.\nII. RELATED WORKS\nAt the early state, researchers tried to exploit Aritiﬁcal\nNeural Networks as the controller of the taking action. One\nof the typical paper is ALVINN [12]. The paper proposed\na 3-layers back-propagation network to complete the task of\nroad following. The network takes images from camera as the\ninputs, passing through 29 hidden layers, and produces the\ndirection of the vehicle should travel along the road as the\noutput. After certain episodes of the training, the car could\nnavigate successfully along the road .\nOne of the more advanced appliations is utilizing DL\ntechnique-convolutional neural networks (CNNs) with Dave2-\nsystem which was exactly implemented in [13]. Dave2-system\nis an end-to-end system which is inspired from ALVINN. The\nCNN network in this paper consists of 9 layers, including\na normalization layer, 5 convolutional layers and 3 fully\nconnected layers . A recent paper [14] employed CNNs to\nthe motion planning layer as well.\nAlthough utilizing the DL algorithm directly as the con-\ntroller of the behavior seems good enough to achieve the target,\nit belongs to the behavior cloning which means it only has\nknowledge about the observed data. This kind of approach\narXiv:2001.03864v1  [cs.RO]  12 Jan 2020\ncan only be acceptable under a good hypothesis as well as\ngood data including all of the possible cases.\nTo avoid falling into behavior cloning class, the vehicle\nshould explore and exploit behavior by itself in unknown\nenvironment and the approach that is able to handle this case is\nReinforcement Learning. Reinforcement Learning is learning\nwhat to do-how to map situations to actions-so as to maximize\na numeral reward [15]. In [16] a deep Q-network (DQN)\nalgorithm is utilized as a decision maker. By passing through\nthe network with 84 by 84 images, three discrete actions, faster\nand faster-left as well as faster-right, are returned by the frame.\nDifferent from previous paper, [17] employed dynamic model\nrather than kinematic with same DQN algorithm. However,\nboth of the applications are still limited to the discrete action\nspace.\nBeing aware that driving in real life could not be achieved\nwith several discrete actions, researchers turn to develope\ncontinuous control algorithms. One of the popular algorithms\nthat can handle continuous action space problem is Actor-\nCritic(AC). A paper [18] from Berkeley university evaluated\nAC algorithm on the classic cart-pole balancing problem,\nas well as 3D robots by tuning with bias and variance.\nConsidering the complexity of sampling and diverse problem\nof AC, Google Deepmind team published a new upgraded AC\nalgorithm-DDPG [6] in 2016. The paper indicates that DDPG\ncan learn competitive policies using low dimentional observa-\ntions with only a straightforward actor-critic architecture.\nIn RL, the reward function plays a signiﬁcant role since\nthe agent is aiming at getting higher reward whenever it\nachieves the goal. A classic paper [8] published by Standford\nuniversity proposed IRL algorithm to recover the unknown\nreward function based on expert’s demonstration. Apprentice-\nship learning has been successfully applied to autonomous\nvehicles such as learning to drive by maximum entropy IRL\n[19] and projection-based IRL [5]. The bottleneck of the above\nmentioned methods is the requirement of solving multiple\nforward RL problems iteratively. A new IRL algorithm stated\nin [11] is gradient inverse reinforcement leanring (GIRL). The\nidea is to ﬁnd the reward function that minimizes the gradient\nof a parameterized representation of the expert’s policy based\non assumption of reward function is in linear combination with\nreward features.\nIn this paper, we recover the reward function by means\nof GIRL algorithm and implement DDPG algorithm to learn\nthe optimal policy based on the recovered reward function.\nREINFORCE algorithm is employed in RL part as well to\ncompare the performance with DDPG algirithm. Moerover, in\norder to perform human-in-the-loop (HITL), we utilize IPG\nCarMaker software which is able to interact with driving\nsimulator. Both of the dynamical model of the agent and\nvirtual environment are built in CarMaker and no other road-\nusers are involved in order to fully focus on the driving\nand stop performance. The experimental results indicate our\napproach is able to let the agent learn to drive autonomously\nover continuous actions and the performance is even better\nthan the expert in some aspects.\nIII. PRELIMINARIES\nA. Background\nA Markov decision process (MDP) is deﬁned by a tuple,\ndenoted as M = {S, A, P, R, γ}, where S is state space; A\nis action space; P is transition probability. It stands for the\nprobability of the transition from state s to s\n′ upon taking\naction a ∈A; R : S →A is the reward (function), it\nindicates how good the action a ∈A executed from state\ns ∈S is; And γ is discount factor which is limited in the\nrange of [0,1). The policy π characterizes the agent’s action\nin MDP problem. More formally, the policy is a mapping\nfrom given states to probabilities of selecting each possible\naction:π(a | s) = P(a = At | s = St). The expected retrun\nbased on the state s following the policy π is deﬁned as Value\nfunciton, also called state value function, denoted as Vπ(s). In\nRL, we formalize it in mathematical way:\nVπ(s) = Eπ[Rt | St = s] = Eπ\n\u0002 ∞\nX\nk=0\nγkrt+k+1 | St = s\n\u0003\n(1)\nNote that in case of terminating state, the value will be 0\nalways. Similarly, the expected return taking action a at state s\nfollowing policy π is deﬁned as Q function, denoted Qπ(s, a).\nThe Q funciton can be formalized as:\nQπ(s, a) = Eπ\n\u0002 ∞\nX\nk=0\nγkrt+k+1 | St = s, At = a\n\u0003\n(2)\nFurthermore, Many approaches in reinforcement learning\nmake use of the recursive relationship known as the Bellman\nequation:\nQπ = E\n\u0002\nrt+1 + γQπ(St+1, At+1) | St = s, At = a\n\u0003\n(3)\nB. Gradient Inverse Reinforcement Learning\nThe logic behind the GIRL algorithm is to ﬁnd out the\nreward function by minimizing the gradient of a parameterized\nrepresentation of the expert’s policy. In particular, when the\nreward function can be represented by linear combination with\nthe reward features, the minimization can be solved effectively\nwith optimization method. Under the assumption of linear\ncombination, it is possible to formalize the reward in the\nfollowing way:\nrω(s, a) = ϕT (s)ω\n(4)\nwhere ϕ(s), ω ∈Rq and q is the dimenstion of the reward\nfeatures. Considering the expert has his own policy and reward\nmechanism(still unknown), the objective function could be\nformalized as :\nJ(θE, ωE) =\nZ\ns\nP(s\n′ | s, a)\nZ\nA\nπE\nθ (s, a)ϕ(s)T ωE dsda (5)\nwhere the superscript E represents expert. Since the target\nof GIRL algorithm is that recovering the reward function as\nclose as the expert’s while the expert’s policy is completely\nknown, the problem can be formalized as minimizing the ℓ2-\nnorm gradient of objective function :\nω = arg min\nω∈Rq\n∥∇θJ(θE, ω)∥2\n(6)\nC. Deep Deterministic Policy Gradient\nDDPG algorithm [6] combines the advantages of the Actor-\nCritic and DQN [20] algorithm so that the converge becomes\neasier. In other words, DDPG introduces some concepts from\nDQN, which are employing target network and estimate\nnework for both of the Actor and Critic. Moreover, the policy\nof DDPG algorithm is no longer stochastic but deterministic.\nIt means the only real action is outputed from actor network\ninstead of telling probability of different actions. The critic\nnetwork updating based on the function:\nL = 1\nN\nN\nX\ni\n\u0000Q(st, at | θQ) −yi\n\u00012,\n(7)\nwhere yi = ri+γQ\n′(st+1, at | θQ\n′\n) is the Q value estimated\nby target network and and N indicates the total number of\nminibatch size. The actor network is updated by means of\ngradient term:\n∇θµJ ≈1\nN\nN\nX\ni\n∇aQ(s, a | θQ) |s=si,a=µ(si) ∇θµµ(s | θµ) |si\n(8)\nWhere Q(s, a | θQ) is from critic estimate network. Fur-\nthermore, DDPG algorithm solves continuous action space\nproblem by means of two key techniques, namely Experience\nReplay and Asynchronous Updating.\nIV. OUR APPOACH\nIn order to implement GIRL algorithm, we performed HITL\nat the ﬁrst step. Several policy features are built afterward\nwith extracted states during the HITL and the quality of the\ndesigned policy features are checked by means of maximum\nlikelihood estimation (MLE) method. Then, we designed re-\nward features in the sense of desired targets and recovered the\nweight of the each feature through GIRL algorithm. Having\nthe recovered reward function, we were able to train the agent\nwith REINFORCE and DDPG algorithms at the ﬁnal step.\nA. Human In The Loop\nTo complete HITL, the expert interfaces with simulator and\nCarMaker through controlling pedal, braking and steering(Fig.\n1). The pedal and braking are both limited in the range of\n[0,1], denoted to ap ∈[0, 1], ab ∈[0, 1] respectively. 1\ndenotes the pedal or braking has been pushed to the maximum\nwhile 0 denotes that pedal and braking are totally released.\nConsidering that no one push both of the pedal and braking at\nthe same time in real life, these two actions could be merged\nas one, denoted as ap ∈[−1, 1], where [-1,0] means braking\nand [0,1] means acceleration. Moreover, the steering is limited\nFig. 1. Human In The Loop\nin the range of as ∈[−5\n2π, 5\n2π] since the steering wheel in the\nsimulator can rotate 2 and half circle in the maximum. Hence,\nwe can write down these actions as a vector:\na = [ap, as]T\n(9)\nNotice that if all of the data are perfect, the vehicle doesn’t\nhave perception about penalization since the reward features\nwill be always assigned as 0 (no penalization). Hence we\nprovide 30 trajectories with bad performance among 150 over\nall trajectories and consequently a total of 44145 labeled\ndataset are gathered in the end.\nB. Policy Features Building\nInpired from [15], we assume that the action is in linear\ncombination with policy features a = θT ∗φ(s), where θ are\nthe policy parameters and φ(s) are policy features. The policy\nfeatures can be states directly from the sensors or constructed\nby the states. Using the states detected from sensors directly\nas the policy features may be one kind of solution but in\norder to shape the action in a smooth way, we selected to\nbuild policy features based on the gathered states. The policy\nfeatures should be built in a sensible way so that they are able\nto tell the meaningful action w.r.t, the goals. For instance, there\nshould be some features take high values when the vehicle\nneed to accelerate or decelerate hard while some low values\nin the opposite situation. The overall logic behind designing\nthe policy features are as following:\n1) Collecting data.\n2) Building the policy features φ(s) based on the gathered\ndata.\n3) Compute the policy parameters θexpert by implementing\nMLE method\n4) Input the deterministic action a = θT\nexpert ∗φ(s) to the\nsimulator(CarMaker)\n5) If the vehicle has perception of the target, the features\nare ”good” enough.(e.g. at least the vehicle should\nperform braking when it is close to stop sign even though\n(a) Feature 1\n(b) Feature 2\n(c) Feature 3\nFig. 2. Reward Features\nthe quality of performance may be poor) Otherwise, the\nfeatures are judged as bad. In this case, go back to step\n2 and repeat.\nBy following above logic, 9 features are built at the end and\nfed to the RL algorithms as the input.\nC. Reward Building\nIn this study, the reward function is built in the same way\nas [8] proposed. We assume there exists some ”true” reward\nfunction R(s) = ωT ϕ(s), where ω ∈Rk and ||ω||1 ≤1 in\norder to bound the reward in the range of [-1,0]. Since it is\nlinear relationship, the reward features should include all of\nthe aspects w.r.t. following targets:\n1) The vehicle should stop in front of the stop sign with\nreasonable distance, not in the middle of the road, not\ncrossing over.\n2) The velocity of the vehicle should not exceed the speed\nlimit, or if it is already higher than the limit, the vehicle\nshould brake at once.\n3) The vehicle should not perform sudden acceleration or\nemergency braking.\nTherefore, three reward features have been built by follow-\ning above logics:\nϕ1(s): This feature is built in order to satisfy the demand\nof stopping in front of the stop sign. There are two indices\ncan be employed to evaluate the performance of the vehicle.\nFirst one is vehicle velocity and the other one is distance from\nthe stop sign. A behavior is judged to be poor if the vehicle\nget null velocity but far from the stop sign or the speed is not\nzero even if it has reached to the stop sign. To consider both\nof the factors, we employed multivariate Gaussian distribution\nfunction as the ﬁrst reward feature (Fig. 2(a)). The mean µ is\na vector with two components that indicates the ideal value\nof the velocity and distance from the stop sign, denoted as\nµ = [v∗\nx, d∗\nstop]T .\nϕ1(vx, dstop) = exp\n\u0012\n−\n\u0000x(s) −µ)T (x(s) −µ\n\u0001\n2σ2\n\u0013\n−1 (10)\nϕ2(s): This feature is related to speed limit which is also very\nimportant during the driving(Fig. 2(b)). The vehicle should be\nTABLE I\nHYPER-PARAMETERS\nHyper-parameters\nREINFORCE\nDDPG\nInitial Policy Parameter\nθexpert\nθrandom\nDiscount Factor\n0.995\n0.990\nInitial Learning Rate(Actor)\n0.001\n0.001\nInitial Learning Rate(Critic)\n-\n0.0003\npunished when it exceeds the allowed speed. To let the vehicle\nhave a better perception, a smooth penalization has been built\nas:\nϕ2(vx, vlim) = min(0, vlim −vx)\n(11)\nϕ3(s): Last feature is related to the comfort limit of\nthe vehicle(Fig. 2(c)). The vehicle should avoid emergency\nbraking not only for the safety but also from the comfort\npoint of view since no other road-users are interfaced with\nenvironment. Also in this case, the vehicle is penalized in\nsmooth way with linear relationship:\nϕ3(g, accx) = min(0, 0.5g −|accx|)\n(12)\nBy implementing GIRL algorithm with above features, the\nﬁnal recovered weights are:\nω = [0.5512, 0.1562, 0.2926]T\n(13)\nD. Reinforcement Learning\nTo implement RL algorithms, several hyper-parameters\nshould be deﬁned in the ﬁrst place. The hyper-parameters\nutilized in this study can be found in Table I. The signiﬁ-\ncant difference between two algorithms is the initial policy\nparameter. For REINFORCE, the initial policy parameter is\nthe one recovered from the MLE method while it is randomly\ninitialized for DDPG algorithm. In other words, the agent\ntrained by REINFORCE algorithm has the pre-knowledge\nabout the driving whereas DDPG has to learn from the\nbeginning.\nMoreover, one of the most challenging part of the RL is the\ntrade off between exploitation and exploration. If the agent\nnever explores new actions, the algorithm will comverge into\n(a) Gradient of REINFORCE\n(b) Reward of REINFORCE\n(c) Critic Loss of DDPG\n(d) Reward of DDPG\nFig. 3. Converge of REINFORCE and DDPG\npoor local minima or even could fail to converge. In this study,\nthe exploration is implemented as the Gaussian noise form\ndirectly to the action and starts to discount when the counter\nis larger than the memory size. More speciﬁcally, the Gaussian\nvariance starts from 3 and decays to 0 after around 50 episodes.\nV. EXPERIMENTS\nA. Agent\nIn this study, we propose to employ a dynamic rather than\nkinematic vehicle model in order to let the simulation be\nmore real. Therefore, a classic Volkswagen Beetle model with\nelectrical powertrain is selected from IPG CarMaker software.\nThe rigid body mass is 1498kg and the car equips with four\nsame types of tyres(RT 195 65R15). The agent is allowed\nto perform continuous actions w.r.t. pedal and braking in the\nrange of [0,1]. 0 represents release the pedal/brake totally\nwhereas 1 means maximum push of both actions. Furthermore,\nmultiple sensors like trafﬁc detection sensors, lane detection\nsensors and so on, are set on the vehicle body in order to\ngather the information from the environment.\nB. Environment\nSince this study aims at learning forward driving and\nstopping behavior by keeping several trafﬁc rules, the road is\nstraight forward without any curves. Two trafﬁc signs, speed\nlimit sign and stop sign respectively, are set on the road and\nthe road condtion is regard as normal, which means friction\ncoefﬁcient is equal to 1.0.\nC. Training Strategy\nRL is deﬁnitely different from the Behavior Cloning (BC).\nBC approach recovers the expert optimal policy by learning the\nstate-action mapping in a supervised way [21]. In other words,\nthe policy can be recovered by minimizing the performance\ndifference between the agent and expert. Though this kind\nof appoach could learn the target in a fast pace, it doesn’t\nhold generalization. More speciﬁcally, the policy learnt by BC\nmethod will perform poorly once suffers from the states never\nvisited during the training. Therefore, it needs hundreds of\ndata to be fed so that cover all of the possible cases when the\nenvironment is stochastic [22]. In contrast, given a reasonable\nreward mechanism, the policy learnt by RL is able to perform\nwell with the states never observed during the training. And it\nis the exact logic implemened in this study. We ﬁxed the initial\nAlgorithm 1 Exploration\nInput: Variance\nParameter: Discount Factor\nOutput: Discounted Variance\n1: Variance=3\n2: Discount Factor=0.999\n3: Counter=1\n4: for i ∈[0, Max epsisode] do\n5:\nfor j ∈[0, Max steps] do\n6:\na ←np.random.normal(a, var)\n7:\nCounter ←Counter + 1\n8:\nif Counter > Memory then\n9:\nV ariance ←V ariance × Discount Factor\n10:\nend if\n11:\nend for\n12: end for\nvelocity of the agent as 60km/h during the training which is\nthe critical value of the speed limit sign. After learning, we\nchecked out the performance of the agent by implementing\nrandomly intialized start velocity and different road length\nwhich are never seen before. The empirical results showed\nthat the agent learnt by our approach did achieve the targets\nwith outstanding performance.\nD. Results\nIn this section, we provide and analyse the training results\nof two different RL algorithms.\nFig. 3 shows the overall converge performance during the\ntraining. As one can see from Fig. 3(a) and 3(b), the reward\nasymptotically converge to a stable value when the gradient\nof REINFORCE algorithm close to 0. Similary, the reward of\nDDPG algorithm tends to be stable around the same value as\nREINFORCE at the end of the iterations with the reduction\nof the Critic network loss. Speciﬁcally, the agent trained by\nDDPG algorithm used ﬁrst 50 episodes to ﬁll full the memory\nand explored new actions with Gaussian noise for further 50\nepisodes. Therefore, the reward in Fig. 3(d) bounces up and\ndown from 50th episode to 100th episode. However, the agent\ndid understand how to drive after the noise dacaying to 0 (after\n100th episode) and tried to get closer to the stop sign as much\nas possible. The reduction of the reward from around 160th\n(a) Distance VS Velocity\n(b) Distance VS Acceleration\nFig. 4. Performance of REINFORCE. The shaded area, dark green and red\ncurve denotes the states visited by agent, reference trajectory and critical value\nfor the penalization.\nepisode is because the agent got to the stop sign without null\nvelocity. In other words, the agent was trying to ﬁgure out\nwhat would happen in case of crossing over the stop sign.\nQualitatively, the performance of agent is very outstanding\nafter around 190 iterations. Comparing with DDPG, the reason\nfor stable increasement of reward in REINFORCE algorithm\nis that the initial policy parameter is assigned as θexpert rather\nthan randomly initialized number. Therefore, the agent already\nhad the pre-knowledge about drving before the training. How-\never, though both of the algorithms converged around 200\niterations but actually the computational cost of REINFORCE\nis much higher than DDPG. This is because REINFORCE\nis an off-line updating algorithm which means the sampling\nefﬁciency is very poor. Therefore, each of the iteration in\nREINFORCE process contains 50 trajectories. On the contrary,\nthe single iteration in DDPG algorithm includes only one\ntrajectory thanks to the on-line updating mechanism. Thus,\ncomparing with REINFORCE, DDPG algorithm holds lower\ncomputational cost and converges much faster even though it\nwas learning from the beginning.\nAfter training, we checked the performance of the agent\nby applying different initial velocity and road length. Fig.\n4 demonstrates the overall results of the agent trained by\nREINFORCE with the start velocity in the range of [30,70].\nAs seen in the Fig. 4(a), the agent is able to maintain the\nvelocity according to the speed limit of the road especially\nwhen the initial velocity is already higher than the critical\nvalue. Moreover, it did stop in front of the stop sign without\nperforming any emergency braking(Fig. 4(b)). The overall\nperformance of this agent is very similar as the expert’s during\nthe HITL.\nFig. 5 indicates the performance of the agent trained with\nDDPG algorithm by applying same conditions as REIN-\nFORCE. A completely different driving style is presented not\nonly from the velocity but also from the acceleration ﬁgure.\nThe agent is much more sensitive than the one with REIN-\nFORCE w.r.t. the speed limit. Especially, it could maintain\nthe velocity slightly lower than the speed limit of the road\nperfectly (Fig. 5(a)). This is the performance even cannot be\nachieved by the expert during the HITL because of imperfect-\nness of human-being. Fig. 5(b) indicates although the agent is\nan ”aggressive” driver, still he was driving without exceeding\n(a) Distance VS Velocity\n(b) Distance VS Acceleration\nFig. 5. Performance of DDPG. The shaded area, dark green and red curve\ndenotes the states visited by agent, reference trajectory and critical value for\nthe penalization.\nthe acceleration limit. This is reasonable since the agent\ndoesn’t have any pre-knowledge (initial policy parameter)\nabout driving and no one did tell him how to drive beyond\nthe critical value. To summurise, the agent trained by DDPG\nalgorithm successfully achieved all of the goals with much\nlower computational cost than the REINFORCE.\nVI. CONCLUSION\nIn this paper, we presented how to let the vehicle learn the\nbasic behaviors,forward driving and stopping under the trafﬁc\nrules, via apprenticeship learning and deep reinforcement\nlearning.In particular,we employed GRIL algorithm to recover\nthe reward function and implemented DDPG algorithm to train\nthe agent. Moreover, in order to highlight the performance\nof DDPG,we employed REINFORCE algorithm in RL step\nas well.The experimental result shows that our approach\nsuccessfully trained the agent to drive and stop autonomously\nby keeping trafﬁc rules and the performance is even better\nthan the expert in the aspect of keeping speed limit.\nHowever, the learnt driving behavior in this study is limited\nin longitudinal domain. We will introduce steering action and\ninvolve other road users to enrich the scenarios in future\nworks.\nREFERENCES\n[1] Pieter Abbeel, Adam Coates, and Andrew Y Ng. Autonomous helicopter\naerobatics through apprenticeship learning. The International Journal\nof Robotics Research, 29(13):1608–1639, 2010.\n[2] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,\nGeorge Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,\nVeda Panneershelvam, Marc Lanctot, et al. Mastering the game of go\nwith deep neural networks and tree search. nature, 529(7587):484, 2016.\n[3] Gary Silberg, Richard Wallace, G Matuszak, J Plessers, C Brower, and\nDeepak Subramanian.\nSelf-driving cars: The next revolution.\nWhite\npaper, KPMG LLP & Center of Automotive Research, page 36, 2012.\n[4] Martin Riedmiller, Mike Montemerlo, and Hendrik Dahlkamp. Learning\nto drive a real car in 20 minutes. In 2007 Frontiers in the Convergence of\nBioscience and Information Technologies, pages 645–650. IEEE, 2007.\n[5] Sahand Sharifzadeh, Ioannis Chiotellis, Rudolph Triebel, and Daniel\nCremers. Learning to drive using inverse reinforcement learning and\ndeep q-networks. arXiv preprint arXiv:1612.03653, 2016.\n[6] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas\nHeess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.\nContinuous control with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\n[7] Siyi Li, Tianbo Liu, Chi Zhang, Dit-Yan Yeung, and Shaojie Shen.\nLearning unmanned aerial vehicle control for autonomous target fol-\nlowing. arXiv preprint arXiv:1709.08233, 2017.\n[8] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse\nreinforcement learning. In Proceedings of the twenty-ﬁrst international\nconference on Machine learning, page 1. ACM, 2004.\n[9] Xi Xiong, Jianqiang Wang, Fang Zhang, and Keqiang Li. Combining\ndeep reinforcement learning and safety based control for autonomous\ndriving. arXiv preprint arXiv:1612.00147, 2016.\n[10] Hongsuk Yi. Deep deterministic policy gradient for autonomous vehicle\ndriving. In Proceedings on the International Conference on Artiﬁcial\nIntelligence (ICAI), pages 191–194. The Steering Committee of The\nWorld Congress in Computer Science, Computer , 2018.\n[11] Matteo Pirotta and Marcello Restelli.\nInverse reinforcement learning\nthrough policy gradient minimization. In Thirtieth AAAI Conference on\nArtiﬁcial Intelligence, 2016.\n[12] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural\nnetwork. In Advances in neural information processing systems, pages\n305–313, 1989.\n[13] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard\nFirner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Mon-\nfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving\ncars. arXiv preprint arXiv:1604.07316, 2016.\n[14] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray.\nDeeptest:\nAutomated testing of deep-neural-network-driven autonomous cars. In\nProceedings of the 40th international conference on software engineer-\ning, pages 303–314. ACM, 2018.\n[15] Richard S Sutton and Andrew G Barto. Reinforcement learning: An\nintroduction. MIT press, 2018.\n[16] April Yu, Raphael Palefsky-Smith, and Rishi Bedi. Deep reinforcement\nlearning for simulated autonomous vehicle control.\nCourse Project\nReports: Winter, pages 1–7, 2016.\n[17] M G´omez, RV Gonz´alez, Tom´as Mart´ınez-Mar´ın, Daniel Meziat, and\nS S´anchez.\nOptimal motion planning by reinforcement learning in\nautonomous mobile vehicles. Robotica, 30(2):159–170, 2012.\n[18] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and\nPieter Abbeel. High-dimensional continuous control using generalized\nadvantage estimation. arXiv preprint arXiv:1506.02438, 2015.\n[19] Markus Kuderer, Shilpa Gulati, and Wolfram Burgard. Learning driving\nstyles for autonomous vehicles from demonstration.\nIn 2015 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages\n2641–2646. IEEE, 2015.\n[20] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,\nJoel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, An-\ndreas K Fidjeland, Georg Ostrovski, et al. Human-level control through\ndeep reinforcement learning. Nature, 518(7540):529, 2015.\n[21] Alberto Maria Metelli, Matteo Pirotta, and Marcello Restelli.\nCom-\npatible reward inverse reinforcement learning. In Advances in Neural\nInformation Processing Systems, pages 2050–2059, 2017.\n[22] Jonathan Ho and Stefano Ermon.\nGenerative adversarial imitation\nlearning. In Advances in Neural Information Processing Systems, pages\n4565–4573, 2016.\n",
  "categories": [
    "cs.RO"
  ],
  "published": "2020-01-12",
  "updated": "2020-01-12"
}