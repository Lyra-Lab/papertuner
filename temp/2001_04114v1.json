{
  "id": "http://arxiv.org/abs/2001.04114v1",
  "title": "Approximation smooth and sparse functions by deep neural networks without saturation",
  "authors": [
    "Xia Liu"
  ],
  "abstract": "Constructing neural networks for function approximation is a classical and\nlongstanding topic in approximation theory. In this paper, we aim at\nconstructing deep neural networks (deep nets for short) with three hidden\nlayers to approximate smooth and sparse functions. In particular, we prove that\nthe constructed deep nets can reach the optimal approximation rate in\napproximating both smooth and sparse functions with controllable magnitude of\nfree parameters. Since the saturation that describes the bottleneck of\napproximate is an insurmountable problem of constructive neural networks, we\nalso prove that deepening the neural network with only one more hidden layer\ncan avoid the saturation. The obtained results underlie advantages of deep nets\nand provide theoretical explanations for deep learning.",
  "text": "Approximation smooth and sparse functions by deep neural\nnetworks without saturation $\nXia Liu1∗\n1. School of Sciences, Xi’an University of Technology, Xi’an 710048, China\nAbstract\nConstructing neural networks for function approximation is a classical and longstanding\ntopic in approximation theory. In this paper, we aim at constructing deep neural net-\nworks (deep nets for short) with three hidden layers to approximate smooth and sparse\nfunctions. In particular, we prove that the constructed deep nets can reach the optimal\napproximation rate in approximating both smooth and sparse functions with controllable\nmagnitude of free parameters. Since the saturation that describes the bottleneck of ap-\nproximate is an insurmountable problem of constructive neural networks, we also prove\nthat deepening the neural network with only one more hidden layer can avoid the sat-\nuration. The obtained results underlie advantages of deep nets and provide theoretical\nexplanations for deep learning.\nKeywords:\nApproximation theory, deep learning, deep neural networks, localized\napproximation, sparse approximation.\n1. Introduction\nMachine learning [5] is a key sub-ﬁeld of artiﬁcial intelligence (AI) which abounds\nin sciences, engineering, medicine, computational ﬁnance and so on.\nNeural network\n[29, 26, 16, 21] is an eternal topic of machine learning that makes machine learning no\nlonger just like a machine to execute commands, but makes machine learning have the\nability to draw inferences about other cases from one instance. Deep learning [12, 15] is\n$The research was supported by the National Natural Science Foundation of China (Grant Nos.\n61806162, 11501496 and 11701443)\n∗Corresponding author: liuxia1232007@163.com\nPreprint submitted to Elsevier\nJanuary 14, 2020\narXiv:2001.04114v1  [cs.IT]  13 Jan 2020\na new active area of machine learning research based on deep structured learning model\nwith appropriate algorithms, and is acclaimed as a magical approach to deal with massive\ndata. Indeed, neural networks with more than one hidden layer are one of the most typical\ndeep structured models in deep learning [12]. In current literature [21, 30], it was showed\nthat deep nets outperform shallow neural networks (shallow nets for short) in the sense\nthat deep nets break through some lower bounds for shallow nets. Furthermore, some\nstudies [11, 18, 22, 27, 32, 33] have demonstrated the superiority of deep nets via showing\nthat deep nets can approximate various functions while shallow nets fail with similar\nnumber of neurons.\nConstructing neural networks to approximate continuous functions is a classical and\nprevalent topic in approximation theory. In 1996, Mhaskar [25] proved that neural net-\nworks with single hidden layer are capable of providing an optimal order of approximating\nsmooth functions. The problem is, however, that the weights and biases of the construc-\ntrs shallow nets are huge, which usually leads to extremely large capacity [13]. Besides\nthis partly positive approximation results, it was shown in [7, 22] that there is a bot-\ntleneck for shallow nets in approximating smooth functions in the sense that there is\nsome lower bound for approximation. Moreover, Chui et al. [6] showed that shallow\nnets with an ideal sigmoidal activation function cannot provide localized approximation\nin Euclidean space. Furthermore, it was proved in [9] that shallow nets cannot capture\nthe rotation-invariance property by showing the same approximate rates in approximat-\ning rotation-invariant function and general smooth function. All these results presented\nlimitations of shallow nets from the approximation theory view point.\nTo overcome these limitations of shallow nets, Chui et al. [6] demonstrated that deep\nnets with two hidden layers can provide localized approximation. Further than that, Chui\net al. [9] showed that deep nets with two hidden layers and controllable norms of weights\ncan approximate the univariate smooth functions without saturation and adding depth\ncan realize the rotation-invariance. Here, saturation [21] means that the approximation\nrate cannot be improved once the smoothness of functions achieves a certain level, which\nwas proposed as an open question by Chen [2]. The general results by Lin [24] indicated\nthat deep nets with two hidden layers and controllable weights possess both localized and\n2\nsparse approximation properties in the spatial domain. They also proved that learning\nstrategies based on deep nets can learn more functions with almost optimal learning rates\nthan those based on shallow nets. The problem in [24] is that the saturation cannot be\novercome. The above theoretical veriﬁcations demonstrate that deep nets with two hidden\nlayers can really overcome some deﬁciency of shallow nets, but that is just partially.\nRecent literature in deep nets [34, 17] proved that deep nets with ReLU activation\nfunction (denoting deep ReLU nets) are more eﬃciently in approximating smooth function\nand possess better generalization performance for numerous learning tasks than shallow\nnets. Nevertheless, the constructed deep ReLU nets are too deep, which results in several\ndiﬃculty in training, including the gradient vanishing phenomenon and disvergence issue\n[12]. Furthermore, how to select the depth is still an open problem, and there is a common\nphenomenon that deep nets with huge hidden layers will lead to inoperable [15]. Under\nthis circumstance, we hope to construct a deep net with good approximation capability,\ncontrollable parameters, non-saturation and not too deep. To this end, we construct in this\npaper a deep net with three hidden layers that possesses the following properties: localized\napproximation, optimal approximation rate, controllable parameters, non-saturation and\nspatial sparsity. Our main tool for analysis is the localized approximation [7, 24], “product\ngate” strategy [9, 31, 34] and localized Taylor polynomials [17, 31].\n2. Main results\nLet I = [0, 1], d ∈N, x ∈X := Id, C(Rd) be the space of continuous functions with\nthe norm\n∥f∥∞:= ∥f∥C(Rd) := max\nx∈Rd |f(x)|.\nFor x ∈X, the set of shallow nets can be mathematically expressed as\nFσ,n(x) =\n( n0\nX\ni=1\nciσ(wi · x + bi) : wi ∈Rd, bi, ci ∈R\n)\n(2.1)\nwhere σ : R →R is an activation function, n0 is the number of hidden neurons (nodes),\nci ∈R is the outer weights, wi := (wji)d\nj=1 ∈Rd is the inner weight, and bi is the bias\n(threshold) of the i-th hidden nodes.\n3\nLet l ∈N, d0 = d, d1, · · · , dl ∈N, σk : R →R (k = 1, 2, · · · , l) be univariate nonlinear\nfunctions. For ⃗h = (h(1), · · · , h(dk))T ∈Rdk, deﬁne ⃗σ(⃗h) = (⃗σ(h(1)), · · · ,⃗σ(h(dk)))T. Denote\nH{σj,l,˜n} as the set of deep nets with l hidden layers and ˜n free parameters that can be\nmathematically represented by\nh{σj,l,˜n}(x) = ⃗a · ⃗hl(x)\n(2.2)\nwhere\n⃗hk(x) = ⃗σk(Wk · ⃗hk−1(x) +⃗bk), k = 1, 2, · · · , l,\nh0(x) = x,⃗a ∈Rdl,⃗bk ∈Rdk, Wk := (W k\ni,j)dk×dk−1 is a dk × dk−1 matrix, and ˜n denotes\nthe number of free parameters, i.e., ˜n = Pl\nk=1(dk · dk−1 + dk) + dl. The structure of deep\nnets, depicted in Figure 1, depends mainly on the structures of the weight matrices Wk\nand the parameter vectors ⃗bk and ⃗a, k = 1, 2, · · · , l. It is easy to see that when l = 1, the\nfunction deﬁned by (2.2) is a shallow net.\n\n\n\n\n\n\n1\nW\n1x\n2x\n0\ndx\n2\nW\nj\nW\n1\n1\n1\n,b\n\n1\n1\n2\n,b\n\n1\n1\n1,\ndb\n\n2\n2\n1\n,b\n\n2\n2\n2\n,b\n\n2\n2\n2,\ndb\n\n1\n,\nl\nl b\n\n2\n,\nl\nl b\n\n,\nl\nl\nl\ndb\n\n1a\n2\na\nla\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Structure for deep neural networks\n2.1. Approximation of smooth function by Deep Nets\nIn this part, we focus on approximating smooth functions by deep nets. The smooth\nproperty is a widely used priori-assumption in approximation and learning theory [8, 10,\n4\n14, 23, 34]. Let c0 be a positive constant, r = k+v with k ∈N0 := {0} S N and 0 < v ≤1.\nA function f : X →R is said to be (r, c0)-smooth if f is k-times diﬀerentiable and for any\nαj ∈N0, j = 1, · · · , d with α1 +· · ·+αd = k, then for any x, z ∈X, the partial derivatives\n∂kf/∂xα1\n1 · · · ∂xαd\nd\nexist and satisfy\n\f\f\f\f\n∂kf\n∂xα1\n1 · · · ∂xαd\nd\n(x) −\n∂kf\n∂xα1\n1 · · · ∂xαd\nd\n(z)\n\f\f\f\f ≤c0∥x −z∥v.\n(2.3)\nThroughtout this paper, ∥x∥denotes the Euclidean norm of x. In particular, if 0 < r ≤1,\nthen (2.3) coincides the well known Lipschitz condition:\n|f(x) −f(z)| ≤c0∥x −z∥r, ∀x, z ∈X.\n(2.4)\nDenote by Lip(r, c0) be the family of (r, c0)-Lipschitz functions satisfying (2.4). In fact,\nthe Lipschitz property depicts the smooth information of f and has been adopted in huge\nliterature [30, 6, 9, 24, 20] to quantify the approximation ability of neural networks.\nAs we know, diﬀerent activation functions used in neural networks will lead to diﬀer-\nent results [30]. Among all the activation functions, the sigmoidal function and Heaviside\nfunction are two commonly used ones. Similar as [24], we use these two activation func-\ntions to construct deep nets. The main reason is that the usage of Heaviside function\ncan enhance the localized approximation performance [24] and the adoption of sigmoidal\nfunction can improve the capability to approximate algebraic polynomials [9]. Let σ0 be\nthe Heaviside function, i.e.,\nσ0(t) = 1, if t ≥0; σ0(t) = 0, if t < 0,\nand σ : R →R be a sigmoidal function, i.e.,\nlim\nt→+∞σ(t) = 1, lim\nt→−∞σ(t) = 0.\n(2.5)\nDue to (2.5), for any ε > 0, there exists a Kε = K(ε, σ) > 0 such that\n\n\n\n|σ(t) −1| < ε, if t ≥Kε,\n|σ(t)| < ε,\nif t ≤−Kε.\n(2.6)\nBefore presenting the main results, we should introduce some assumptions. Assump-\ntion 1 is the r-Lipschitz continuous condition for the target function, which is a standard\ncondition in approximation and learning theory.\n5\nAssumption 1 We assume g ∈Lip(r, c0) with r = k + v, k ∈N0, 0 < v ≤1, c0 > 0.\nAssumption 2 concerns the smoothness condition on activation function σ, which has\nalready been adopted in [19].\nAssumption 2 For r > 0 with r = k+v, k ∈N0, 0 < v ≤1, let σ be a non-decreasing\nsigmoidal function with ∥σ′∥L∞(R) ≤1, ∥σ∥L∞(R) ≤1 and there exists at least a point\nb0 ∈Rd satisﬁes σ(j)(b0) ̸= 0 for all j = 0, 1, 2, · · · , k0, and k0 ≥max{k, 2} + 1.\nThere are many functions satisfy the above restrictions such as: the Logistic function\nσ(t) =\n1\n1+e−t, the Hyperbolic tangent function σ(t) =\n1\n2(tanh(t) + 1), the Gompertz\nfunction σ(t) = e−ae−bt with a, b > 0 and the Gaussian function σ(t) = e−t2.\nOur ﬁrst main result is the following theorem, in which we construct a deep net with\nthree hidden layers to approximate smooth functions. Denote by H3,˜n := H{σ0,σ,σ,3,˜n} be\nthe set of deep net with three hidden layers and ˜n free parameters, where σ0, σ, σ are the\nactivation functions in the ﬁrst, second and third hidden layers, respectively.\nTheorem 2.1. Let 0 < ε ≤1, under Assumptions 1 and 2, there exists a deep net\nH(x) ∈H{3,˜n} such that\n|g(x) −H(x)| ≤C(˜n−r\nd + ˜nε),\n(2.7)\nwhere all parameters of this deep net are bounded by poly(˜n, 1\nε), poly(˜n, 1\nε) denotes some\npolynomial function with respect to ˜n and 1\nε, and C is a constant independent of ˜n and ε.\nThe proof of Theorem 2.1 will be postponed in Section 4, and a direct consequences\nof Theorem 2.1 is as follows.\nCorollary 2.2. Under Assumptions 1 and 2, if ε = ˜n−r+d\nd , then there holds\n|g(x) −H(x)| ≤¯C˜n−r\nd,\n(2.8)\nwhere ¯C is a constant independent of ˜n, and all the parameters of the deep net are bounded\nby poly(˜n).\nThe approximation rate of shallow nets and deep nets with two hidden layers are\nO(˜n−r\nd) [29, 9], which is the same as Corollary 2.2.\nHowever, as far as the norm of\nweights is considered, all the weights in Corollary 2.2 are controllable, and are much less\nthan those of shallow nets. Speciﬁcally, for shallow nets, the norm of weights is at least\nexponential with respect to ˜n [25], while for deep nets in Corollary 2.2, the norm of weights\nis only polynomial respect to ˜n. Such a diﬀerence is essentially according to the capacity\n6\nestimate [13], where a rigorous proof was presented that the covering number of deep\nnets with controllable norms of free parameters can be tightly bounded. Furthermore,\ncompared with similar results for deep nets with two hidden layers [24], we ﬁnd that our\nconstructed deep net avoids the saturation. To sum up, the constructed deep net with\nthree hidden layers performs better than shallow nets and deep nets with two hidden\nlayers in overcoming their shortcomings.\n2.2. Sparse Approximation for Deep Nets\nSparseness in the spatial domain is a prevalent data feature that abounds in numerous\napplications such as magnetic resonance imaging (MRI) analysis [1], handwritten digit\nrecognition [4] and so on. The spatial sparseness means that the response (or function) of\nsome actions only happens on several small regions instead of the whole input space. In\nother words, the response vanishes in most of regions of the input space. Mathematically,\nthe spatially sparse function is deﬁned as follows [24].\nLet s, N ∈N, s ≤N d, N d\nN = {1, 2, ..., N}d. Denote by {BN,}∈Nd\nN a cubic partition\nof Id with centers {ζ}∈Nd\nN and side length\n1\nN . Deﬁne\nΛs := {kℓ: kℓ∈N d\nN, 1 ≤ℓ≤s}\nand\nS :=\n[\n∈Λs\nBN,.\nFor any function f deﬁned on Id, if the support of f is S, then we say that f is s-sparse\nin N d partitions. We use Lip(N, s, r, c0) to quantify both the smoothness property and\nsparseness, i.e.,\nLip(N, s, r, c0) =\n\b\nf : f ∈Lip(r, c0) and f is s-sparse in N d partition\n\t\n.\nFor n ∈N with n ≥ˆcN for some ˆc > 0, let {An,j}j∈Ndn be another cubic partition of\nId with centers {ξj}j∈Ndn and side length 1\nn. For each ∈Nd\nN, deﬁne\n¯Λ:= {j ∈N d\nn : An,j ∩BN,̸= ∅},\nit is easy to see that the set S\n∈V\ns ¯Λis the family of An,j where f is not vanished.\n7\nWith these helps, we present a spareness assumption of f as follows.\nAssumption 3 We assume f ∈Lip(N, s, r, c0) with r = k + v, k ∈N0, 0 < v ≤1,\nc0 > 0, N, s ∈N.\nIn [9], Chui et al. only discussed the approximating performance of deep nets with two\nhidden layers in approximating smooth function. Lin [24] extended the results in [9] to\napproximate spatially sparse functions. Speciﬁcally, Lin [24] proved that deep nets with\ntwo hidden layers can approximate spatially sparse function much better than shallow\nnets. However, their results suﬀered from the saturation. In this subsection, we aim\nat conquering the above deﬁciency by constructing a deep net with three hidden layers.\nTheorem 2.3 below is the second main result of this paper, and the proof also be veriﬁed\nin Section 4.\nTheorem 2.3. Let 0 < ε ≤1, ˜n ≥˜cN d for some ˜c > 0. Under Assumptions 2 and 3,\nthere exists a deep net H(x) ∈H{3,˜n} such that\n|f(x) −H(x)| ≤c0˜n−r\nd + ˜C˜nε, ∀x ∈X.\nIf x ∈Id \\ S\n∈Λs ¯Λ, then\n|H(x)| ≤˜C˜nε\nwhere all the parameters of the deep net are bounded by poly(˜n, 1\nε), and ˜C is a constant\nindependent of ˜n and ε.\nCorollary 2.4. Let T be arbitrary positive number satisﬁes T ≥r+d\nd\nand ε = ˜n−T. Under\nthe Assumptions 2 and 3, if ˜n ≥˜cN d for some ˜c > 0, then there holds\n|f(x) −H(x)| ≤ˆC˜n−r\nd, ∀x ∈X.\n(2.9)\nIf x ∈X \\ S\n∈Λs ¯Λ, then\n|H(x)| ≤˜n−T\n(2.10)\nwhere ˆC is a constant independent of ˜n.\nTo be detailed, (2.9) shows that the approximation rate of deep nets is as fast as\nO(˜n−r\nd), and (2.10) states their performance in realizing the spatial sparseness, when T\nis large. However, too large T may lead to extremely large weights, which implies huge\ncapacity measured by the covering number of H{3,˜n} according to [13]. A preferable choice\nof T should be T = O(r+d\nd ).\n8\nPrevious studies [3, 6] indicated that shallow nets cannot provide localized approx-\nimation, which is a special case for sparse approximation with s = 1. Lemma 4.1 (in\nSection 4) shows that deep nets with two hidden layers have the localized approximation\nproperty, which is the building-block to construct deep nets possessing sparse approxima-\ntion property. To the best our knowledge, [24] is the ﬁrst work to construct deep nets to\nrealize sparse features. Compared with [24], our main novelty is to deepen the network\nto conquer the saturation.\n3. Related work\nConstructing neural networks to approximate the functions is a classic problem [29,\n25, 27, 31, 10, 14, 24] in approximation theory. Traditional method to deal with this\nproblem can be divided into three steps. Step 1, constructing a neural network to ap-\nproximate polynomials; Step 2, utilizing polynomials to approximate target functions;\nStep 3, combining the above two steps to reach the ﬁnal approximation results between\nneural networks and target functions. Tayor formula is usually be used in Step 1 to obtain\nthe approximation results, which usually leads to extremely large weights, i.e., |wi| ∼em,\nwhere m is the degree of the polynomial. However, larger weight leads to large capability\nand consequently bad generalization and instable algorithms. Typical example includes\n[25] and [28]. In order to overcome this drawback, we introduce a new function by the\nproduct of Taylor polynomial and a deep net with two hidden layers to instead of the\npolynomial in Step 1 to reduce the weights of neural networks from em to poly(m).\nFor deep nets, [34] and [31] stated that deep ReLU networks are more eﬃcient to\napproximate smooth functions than shallow nets. But their results are slightly worse\nthan Theorem 2.1 in this paper, in the sense that there is either an additional logarithmic\nterm or under the weaker norm. Recently, Han et al. [17] indicated that deep ReLU nets\ncan achieve the optimal generalization performance for numerous learning tasks, but the\ndepth of [31] is much larger than ours. Recently, Zhou [35, 36] also veriﬁed that deep\nconvolutional neural network (DCNN) is universal, i.e., DCNN can be used to approximate\nany continuous function to an arbitrary accuracy when the depth of the neural network\nis large enough.\n9\nAll the above literature [34, 31, 17, 35, 36] demonstrated that deep nets with ReLU\nactivation function and DCNN have good properties both in approximation and general-\nization. However, there are too deep to be particularly used in real tasks. Compared with\nthese results, we constructed a deep net only with three hidden layers to approximate\nsmooth and sparse functions, respectively. We proved in Theorem 2.1 and Theorem 2.3\nthat the constructed deep net with three hidden layers and with controllable weights, can\nrealize smoothness and spatial sparseness without saturation, simultaneously.\n4. Proofs\nLet Pm = Pm(Rd) be the set of multivariate algebraical polynomials on Rd of degree\nat most m, i.e.,\nPm = span{xk ≡xk1\n1 · · · xkd\nd : |k| = k1 + ... + kd ≤m}.\nConsider Ph\nm as the set of homogeneous polynomials of degree m, i.e.,\nPh\nm = span{xk = xk1\n1 · · · xkd\nd : |k| = k1 + ... + kd = m}.\n4.1. Localized Approximation for Deep Nets\nLet Nd\nn = {1, 2, · · · , n}d, n ∈N, {An,j}j∈Ndn be the cubic partition of X with centers\n{ξj}j∈Ndn and side length 1\nn. If x lies on the boundary of some An,j, then jx is the set to\nbe the smallest integer satisfying x ∈An,j, i.e.,\njx =\n\b\nj|j ∈N d\nn, x ∈An,j\n\t\n.\nThen, for K > 0, any j ∈Nd\nn, x ∈X, we construct a deep net with two hidden-layer\nN ∗\nn,j,K(x) ∈H{σ0,σ,2,2d+1} as\nN ∗\nn,j,K(x) = σ\n(\n2K\n\"\nd\nX\nl=1\nσ0\n\u0014 1\n2n + x(l) −ξ(l)\nj\n\u0015\n+\nd\nX\nl=1\nσ0\n\u0014 1\n2n −x(l) + ξ(l)\nj\n\u0015\n−2d + 1\n2\n#)\n.\n(4.1)\nLocalized approximation of neural networks [6] implies that if the target function is\nmodiﬁed only on a small subset of the Euclidean space, then only a few neurons, rather\n10\nthan the entire network, need to be retrained. Lemma 4.1 below that was proved in [24]\nstates the localized approximation property of deep nets which is totally diﬀerent from\nthe shallow nets. We refer [9] (section 3.3) for details in the localized approximation of\nneural networks.\nLemma 4.1. For any ε > 0, if N ∗\nn,j,Kε is deﬁned by (4.1) with Kε satisfying (2.6) and σ\nbeing a nondecreasing sigmoidal function, then\n(i) For any x ̸∈An,j, there holds |N ∗\nn,j,Kε(x)| < ε;\n(ii) For any x ∈An,j, there holds |1 −N ∗\nn,j,Kε(x)| ≤ε.\nIt is easy to see that if ε →0, then N ∗\nn,j,Kε is an indicator function for An,j. Moreover,\nwhen n →∞, it indicates that N ∗\nn,j,Kε can recognize the location of x in an arbitrarily\nsmall region and will vanish in some of partitions of the input space.\nIn order to overcome the deﬁciency of traditional method in neural networks approxi-\nmation. We deﬁned a new function Φg(x) by a product of Taylor polynomial and a deep\nnetwork function with two hidden layers to instead of polynomials:\nΦg(x) =\nX\nj∈Ndn\nPk,ηj,g(x)N ∗\nn,j,Kε(x), x ∈X,\n(4.2)\nwhere N ∗\nn,j,Kε and Kε are deﬁned by (4.1) and (2.6). Pk,ηj,g(x) is the Taylor polynomial\nof g with degree k around ηj, ηj ∈An,j and j ∈N d\nn.\nBased on the localized approximation results and the localized Taylor polynomial in\n(4.2), we construct a deep net with three hidden layers to approximate both smooth and\nsparse functions.\n4.2. Proof of Theorem 2.1\nThe following proposition indicates that constructing a shallow net with one neuron\ncan replace a minimal.\nProposition 4.2. Let c0 > 0, L = O(md−1), σ ∈Lip(r0, c0) is a sigmoidal function with\n(m + 1)−times bounded derivatives, and ∥σ′ ∥L∞(R)≤1, σ(j)(0) ̸= 0 for all j = 0, 1, ..., s0.\nFor arbitrary Pm ∈Pm and any ε ∈(0, 1), we have\n\f\f\f\f\fPm(x) −\nL\nX\ni=1\nC(i, m)m!\nδm\nmσ(m)(0)σ(δm(wi · x)) −P ∗\nm−1(x)\n\f\f\f\f\f < ε.\n(4.3)\n11\nwhere\nP ∗\nm−1(x) =\nm−1\nX\nj=0\nL\nX\ni=1\nD(i, j)(wi · x)j, D(i, j) = C(i, j) −C(i, m)m!σ(j)(0)\nδm\nmσ(m)(0)j!\n,\nδm = min\n(\nε\nMm\n,\nε\nPL\ni=1 |C(i, m)|Mm\n)\n, Mm = max\n−1≤ξ≤1\nσ(m+1)(ξ)\nσ(m)(0) ,\nand C(i, j) is an absolute constant.\nWe use the Taylor formula [9] to prove the above Proposition 4.2.\nLemma 4.3. Let k ≥1 and ϕ be k−times diﬀerentiable on R. Then for any u, u0 ∈R,\nthere holds\nϕ(u) = ϕ(x0) + ϕ\n′(u0)\n1!\n(u −u0) + ... + ϕk(u0)\nk!\n(u −u0)k + Rk(u)\n(4.4)\nwhere\nRk(u) =\n1\n(k −1)!\nZ u\nu0\n[ϕ(k)(t) −ϕ(k)(u0)](u −t)k−1dt.\n(4.5)\nIn addition, under the condition of Lemma 4.3, for any a ∈R, there holds\nϕ(au) = ϕ(u0) + ϕ\n′(u0)\n1!\n(au −u0) + ... + ϕk(u0)\nk!\n(au −u0)k + Rk(u)\n(4.6)\nwhere\nRk(u) =\nak\n(k −1)!\nZ u\nu0\n[ϕk(at) −ϕk(u0)](u −t)k−1dt.\n(4.7)\nLemma 4.4 which was proved in [28] plays an important role in proving Proposition\n4.2.\nLemma 4.4. Let m ∈N and L = O(md−1). For any Pm ∈Pm, there exists a set of\npoints {w1, ..., wL} ⊂Id such that\nPm(x) = span{(wi · x)j : x, wi ∈Id, 0 ≤j ≤m, 1 ≤i ≤L}.\n(4.8)\nProof of Proposition 4.2.\nFor any t ∈[0, 1], δk ∈(0, 1), k ∈[1, s0], it follows from\nLemma 4.3 that\nσ(δkt) = σ(0) + σ\n′(0)\n1! δkt + ... + σk(0)\nk!\n(δkt)k + ˜Rk(t)\n(4.9)\nwhere\n˜Rk(t) =\nδk\nk\n(k −1)!\nZ t\n0\n[σ(k)(δku) −σ(k)(0)](t −u)k−1du.\n(4.10)\n12\nDenote\nQk−1(t) =\nk−1\nX\nj=0\nσ(j)(0)\nj!\n(δkt)j.\nThen (4.9) yields\ntk =\nk!\nδk\nkσ(k)(0)σ(δkt) −\nk!\nδk\nkσ(k)(0)Qk−1(t) −\nk!\nδk\nkσ(k)(0)\n˜Rk(t),\nwhich implies\ntk =\nk!\nδk\nkσ(k)(0)σ(δkt) + qk−1(t) + rk(t)\n(4.11)\nwhere\nqk−1(t) = −\nk!\nδk\nkσ(k)(0)Qk−1(t)\nand\nrk(t) = −\nk!\nδk\nkσ(k)(0)\n˜Rk(t).\n(4.12)\nSince\n|σ(k)(δku) −σ(k)(0)| ≤max\n0<ξ<1 |σ(k+1)(ξ)||δk||u|,\nthen by (4.10) and (4.12), there holds\n|rk(t)|\n=\n\f\f\f\f−\nk!\nδk\nkσ(k)(0)\n˜Rk(t)\n\f\f\f\f\n=\nk\n|σ(k)(0)|\n\f\f\f\f\nZ t\n0\n[σ(k)(δku) −σ(k)(0)](t −u)k−1du\n\f\f\f\f\n≤\nk|σ(k+1)(ξ)|\n|σ(k)(0)|\n\f\f\f\f\nZ t\n0\nδku(t −u)k−1du\n\f\f\f\f = δkk|σ(k+1)(ξ)|\n|σ(k)(0)|\n\f\f\f\f\nZ 1\n0\nu(1 −u)k−1du\n\f\f\f\f\n≤\nkδkMk(1\nk −\n1\n1 + k) ≤δkMk.\n(4.13)\nFrom Lemma 4.4, for any Pm ∈Pm, it follows\nPm(x)\n=\nm\nX\nj=0\nL\nX\ni=1\nC(i, j)(wi · x)j\n=\nL\nX\ni=1\nC(i, m)(wi · x)m +\nL\nX\ni=1\nC(i, m −1)(wi · x)m−1\n+ · · · +\nL\nX\ni=1\nC(i, 1)(wi · x) +\nL\nX\ni=1\nC(i, 0).\n(4.14)\n13\nSince x, wi ∈Id, we have |wi · x| ≤1. Then, for an arbitrary ε > 0, there exists a\nδm ∈(0, 1) such that\nL\nX\ni=1\n|C(i, m)|Mmδm ≤ε.\n(4.15)\nDue to (4.11) and δm|wi · x| ∈[0, 1], there holds\n(wi · x)m =\nm!\nδm\nmσ(m)(0)σ(δm(wi · x)) + qm−1(wi · x) + rm(wi · x).\n(4.16)\nInserting the above (4.16) into (4.14), we obtain\nPm(x)\n=\nL\nX\ni=1\nC(i, m)\n\u0012\nm!\nδm\nmσ(m)(0)σ(δm(wi · x)) + qm−1(wi · x) + rm(wi · x)\n\u0013\n+\nL\nX\ni=1\nC(i, m −1)(wi · x)m−1 + · · · +\nL\nX\ni=1\nC(i, 1)(wi · x) +\nL\nX\ni=1\nC(i, 0)\n=\nL\nX\ni=1\nC(i, m)\nm!\nδm\nmσ(m)(0)σ(δm(wi · x)) + P ∗\nm−1(x) + Rm(x)\n(4.17)\nwhere\nRm(x) =\nL\nX\ni=1\nC(i, m)rm(wi · x)\nand\nP ∗\nm−1(x)\n=\nL\nX\ni=1\nC(i, m)qm−1(wi · x) +\nL\nX\ni=1\nC(i, m −1)(wi · x)m−1\n+ · · · +\nL\nX\ni=1\nC(i, 1)(wi · x) +\nL\nX\ni=1\nC(i, 0)\n=\nL\nX\ni=1\nD(i, m −1)(wi · x)m−1 +\nL\nX\ni=1\nD(i, m −2)(wi · x)m−2\n+ · · · +\nL\nX\ni=1\nD(i, 1)(wi · x) +\nL\nX\ni=1\nD(i, 0)\n=\nm−1\nX\nj=0\nL\nX\ni=1\nD(i, j)(wi · x)j\n(4.18)\nwith D(i, j) = C(i, j) −C(i,m)m!σ(j)(0)\nδm\nmσ(m)(0)j! . It then follows from (4.13) and (4.15) that\n|Rm(x)| ≤ε.\n(4.19)\n14\nCombining (4.17)-(4.19), we have\n\f\f\f\f\fPm(x) −\nL\nX\ni=1\nC(i, m)m!\nδm\nmσ(m)(0)σ(δm(wi · x + bi)) −P ∗\nm−1(x)\n\f\f\f\f\f < ε.\nThis completes the proof of Proposition 4.2.\nNext, we show the performance of shallow nets in approximating.\nProposition 4.5. Let σ be a non-decreasing sigmoidal function with ∥σ′∥L∞(R) ≤1,\n∥σ∥L∞(R) ≤1, σ(j)(0) ̸= 0 for all j = 0, 1, ..., m + 1. For any Pm ∈Pm and ε ∈(0, 1),\nthere exists a shallow net\nhm+1(x) =\nm\nX\nj=0\nL\nX\ni=1\na(i, j)σ(δj(wi · x))\nwith a(i, j) =\nC(i,j)j!\nδj\njσ(j)(0) and δm being a polynomial with respect to 1\nε such that\n|Pm(x) −hm+1(x)| ≤ε.\nProof. From Proposition 4.2, it holds that\n\f\f\f\f\fPm(x) −\nL\nX\ni=1\na(i, m)σ(δm(wi · x)) −P ∗\nm−1(x)\n\f\f\f\f\f <\nε\nm + 1\n(4.20)\nwhere a(i, m) =\nC(i,m)m!\nδm\nmσ(m)(0) and δm ∼poly(1\nε). Similar methods as above\n\f\f\f\f\fP ∗\nm−1(x) −\nL\nX\ni=1\na(i, m −1)σ(δm−1(wi · x)) −P ∗\nm−2(x)\n\f\f\f\f\f <\nε\nm + 1,\n(4.21)\n· · ·\n\f\f\f\f\fP ∗\n1 (x) −\nL\nX\ni=1\na(i, 1)σ(δ1(wi · x)) −P ∗\n0 (x)\n\f\f\f\f\f <\nε\nm + 1,\n(4.22)\nand\n\f\f\f\fP ∗\n0 (x) −P ∗\n0 (x)\nσ(0) σ(0 · x)\n\f\f\f\f = 0 <\nε\nm + 1\n(4.23)\nThen, it follows from (4.20)-(4.23) that\n\f\f\f\f\fPm(x) −\nm\nX\nj=0\nL\nX\ni=1\na(i, m)σ(δj(wi · x))\n\f\f\f\f\f < ε.\nThis completes the proof of Proposition 4.5.\n15\nCorollary 4.6. Let m ∈N, L = O(md−1) and σ be a non-decreasing sigmoidal function\nwith ∥σ′∥L∞(R) ≤1, ∥σ∥L∞(R) ≤1. If σ(j)(b) ̸= 0 for some b ∈R and all j = 0, 1, ..., m+1,\nthen there exists a shallow net\nhm+1(x) =\nm\nX\nj=0\nL\nX\ni=1\na(i, j)σ(δj(wi · x + b))\nsuch that\n|Pm(x) −hm+1(x)| ≤ε.\nBased on the above Proposition 4.5, we are able to yield a “product-gete” property of\ndeep nets in the following Proposition 4.7, whose proof can be found in [9].\nProposition 4.7. Let m ∈N and L = O(md−1). If σ is a non-decreasing sigmoidal\nfunction with ∥σ′∥L∞(R) ≤1, ∥σ∥L∞(R) ≤1, σ(j)(0) ̸= 0 for all j = 0, 1, ..., m + 1, then for\nany ε > 0, there exists a shallow net\nh3(x) =\n3\nX\nj=1\najσ(wj · x)\nsuch that for any u1, u2 ∈[−1, 1]\n\f\f\f\fu1u2 −\n\u0012\n2h3\n\u0012u1 + u2\n2\n\u0013\n−1\n2h3(u1) −1\n2h3(u2)\n\u0013\f\f\f\f < ε.\nCorollary 4.8. Let m ∈N, L = O(md−1) and σ be a non-decreasing sigmoidal function\nwith ∥σ′∥L∞(R) ≤1, ∥σ∥L∞(R) ≤1. If there exists a point b0 ∈R satisfying σ(j)(b0) ̸= 0\nfor all j = 1, 2, 3, then for any ε > 0, there exists a shallow net\nh3(x) =\n3\nX\nj=1\najσ(wj · x + b0)\nsuch that for any u1, u2 ∈[−1, 1]\n\f\f\f\fu1u2 −\n\u0012\n2h3\n\u0012u1 + u2\n2\n\u0013\n−1\n2h3(u1) −1\n2h3(u2)\n\u0013\f\f\f\f < ε.\nIn our proof, we also need the following Lemma 4.9, which can be found in [17].\nLemma 4.9. Let x ∈Id, r = k + v with k ∈N0 and 0 < v ≤1. If f ∈Lip(r, c0) and\nPk,x0,f(x) is the Taylor polynomial of f with degree k around x0, then\n|f(x) −Pk,x0,f(x)| ≤ec1∥x −x0∥r, x0 ∈Rd,\n(4.24)\nwhere ec1 is a constant depending only on k, c0 and d.\n16\nThe following Lemma 4.10 illustrates the approximation property of the product of\nTaylor polynomial and deep nets.\nLemma 4.10. If g ∈Lip(r, c0) with r = k + v, k ∈N0, 0 < v ≤1, c0 > 0, σ is a\nnon-decreasing sigmoidal function and Φ(x) is deﬁned by (4.2), then\n|g(x) −Φg(x)| ≤˜c1n−r + ndB0ε, x ∈X,\nwhere B0 := ∥g∥L∞(X) + ˜c1.\nProof. From Lemma 4.9, we observe\n|Pk,ηjx,g(x)| ≤∥g∥L∞(X) + ˜c1∥x −ηjx∥r ≤∥g∥L∞(X) + ˜c1 := B0\n(4.25)\nwhere B0 := ∥g∥L∞(X) + ˜c1.\nSince Id = S\nj∈Ndn An,j, for each x ∈X, there exists a jx such that x ∈An,jx. Therefore,\nit follows from Proposition 4.1 that\n|g(x) −Φg(x)|\n=\n\f\f\f\f\fg(x) −Pk,ηjx,g(x) −\nX\nj̸=jx\nPk,ηj,g(x)N ∗\nn,j,Kε(x) + Pk,ηjx,g(x)(1 −N ∗\nn,jx,Kε(x))\n\f\f\f\f\f\n≤\n|g(x) −Pk,ηjx,g(x)| +\n\f\f\f\f\f\nX\nj̸=jx\nPk,ηj,g(x)N ∗\nn,j,Kε(x)]| + |Pk,ηjx,g(x)(1 −N ∗\nn,jx,Kε(x))\n\f\f\f\f\f\n≤\n˜c1∥x −ηjx∥r + (nd −1)B0ε + B0ε\n≤\n˜c1n−r + ndB0ε\n(4.26)\nThis completes the proof of Lemma 4.10.\nProof of Theorem 2.1.\nThe proof can be divided into three steps: the ﬁrst\none is to give estimates for the product function and shallow net; then, we consider the\napproximation between Taylor polynomial and shallow net; ﬁnally, we give approximation\nerrors by combining the above two steps.\nStep 1: By the deﬁnition of N ∗\nn,j,Kε(x) in (4.1), we observe\n|N ∗\nn,j,Kε(x)| ≤1.\nFurthermore, it follows from Lemma 4.9 that\n|Pk,ηj,g(x)| ≤∥g∥L∞(X) + ˜c1∥x −ηj∥r ≤∥g∥L∞(X) + ˜c1.\n(4.27)\n17\nDenote B1 := 4(∥g∥L∞(X) + ˜c1 + 1). Hence, for an arbitrary x ∈X, we have\nN∗\nn,j,Kε(x)\nB1\n∈\n[−1\n4, 1\n4] and\nPk,ηj,g(x)\nB1\n∈[−1\n4, 1\n4]. It then follows from Corollary 4.8 with u1 =\nN∗\nn,j,Kε(x)\nB1\n,\nu2 =\nPk,ηj,g(x)\nB1\nthat there exists a shallow net\nh3(x) =\n3\nX\nj=1\najσ(wj · x + bj)\nsuch that\n\f\f\f\f\f\f\nPk,ηj,g(x)N ∗\nn,j,Kε(x) −B2\n1\n\n2h3\n\u0012N ∗\nn,j,Kε(x) + Pk,ηj,g(x)\n2B1\n\u0013\n−\nh3\n\u0010 N∗\nn,j,Kε(x)\nB1\n\u0011\n2\n−\nh3\n\u0010 Pk,ηj,g(x)\nB1\n\u0011\n2\n\n\n\f\f\f\f\f\f\n≤\nB2\n1ε.\n(4.28)\nFor the sake of convenience, denote\n∆1 = B2\n1\n\n2h3\n\u0012N ∗\nn,j,Kε(x) + Pk,ηj,g(x)\n2B1\n\u0013\n−\nh3\n\u0010 N∗\nn,j,Kε(x)\nB1\n\u0011\n2\n−\nh3\n\u0010 Pk,ηj,g(x)\nB1\n\u0011\n2\n\n.\nNoting ∥σ\n′∥L∞(R) ≤1, for any x1, x2 ∈X, there holds\n|h3(x1) −h3(x2)|\n≤\n\f\f\f\f\f\n3\nX\nj=1\najσ(wj · x1 + bj) −\n3\nX\nj=1\najσ(wj · x2 + bj)\n\f\f\f\f\f\n≤\n3\nX\nj=1\n|aj||σ(wj · x1 + bj) −σ(wj · x2 + bj)|\n≤\n3\nX\ni=1\n|aj|∥wj∥∥x1 −x2∥\n≤\n3˜c2∥x1 −x2∥,\n(4.29)\nwhere ˜c2 = maxj∈{1,2,3} |aj| maxi∈{1,··· ,d}{|wj1|, · · · , |wjd|} and wj = (wj1, · · · , wjd)T.\nStep 2: It from Corollary 4.6 with Pk(t−ηj) =\nPk,ηj,g(x)\nB1\nthat there exists a shallow net\nhk+1,L(x) =\nk+1\nX\nj=1\nL\nX\ni=0\na(i, j)σ(wj · (x −xi) + bj)\n(4.30)\nsuch that\n\f\f\f\f\nPk,ηj,g(x)\nB1\n−hk+1,L(x)\n\f\f\f\f ≤ε1.\n18\nStep 3: Deﬁne\nH(x) :=\nnd\nX\nj=1\nHj(x)\n(4.31)\nwhere\nHj(x) = B2\n1\n\n2h3\n\u0012hk+1,L(x)\n2\n+ N ∗\nn,j,Kε(x)\n2B1\n\u0013\n−h3(hk+1,L(x))\n2\n−\nh3\n\u0010 N∗\nn,j,Kε(x)\nB1\n\u0011\n2\n\n.\n(4.32)\nBy (4.2) and (4.28), we get\n|H(x) −Φg(x)|\n≤\nX\nj∈Ndn\n|Hj(x) −∆1 + ∆1 −Pk,ηj,g(t)N ∗\nn,j,Kε(x)|\n≤\nX\nj∈Ndn\n\f\f\f\f\f\f\nB2\n1\n\n2h3\n\u0012hk+1,L(x)\n2\n+ N ∗\nn,j,Kε(x)\n2B1\n\u0013\n−h3(hk+1,L(x))\n2\n−\nh3\n\u0010 N∗\nn,j,Kε(x)\nB1\n\u0011\n2\n\n\n−B2\n1\n\n2h3\n\u0012N ∗\nn,j,Kε(x) + Pk,ηj,g(x)\n2B1\n\u0013\n−\nh3\n\u0010 N∗\nn,j,Kε(x)\nB1\n\u0011\n2\n−\nh3\n\u0010 Pk,ηj,g(x)\nB1\n\u0011\n2\n\n\n\f\f\f\f\f\f\n+ ndB2\n1ε\n≤\nndB2\n1\n\u00129\n2 ˜c2ε1 + ε\n\u0013\n≤\n¯C2ndε\n(4.33)\nwhere we set ε1 = 2ε and ¯C2 is a constant depending only on B1 and ˜c2. Noting (4.33)\nand Lemma 4.10, we obtain\n|g(x) −H(x)|\n≤\n|g(x) −Φg(x)| + |Φg(x) −H(x)|\n≤\n˜c1n−r + B0ndε + ¯C2ndε\n=\nC(n−r + ndε),\nwhere C is a constant depending only on k, c0, d, B0. Due to (4.1) (4.30) (4.31) and (4.32),\nthere exists a deep net H(x) ∈H3,˜n with ˜n = 6nd((L+1)(k +1)+2d+1) free parameters\nsatisfying\n|g(x) −H(x)| ≤C(˜n−r\nd + ˜nε).\nFurthermore, it is easy to check (see [9] for detailed proof) that all the parameters in\nH(x) can be bounded by poly(˜n, 1\nε). This completes the proof of Theorem 2.1.\n19\nProof of Corollary 2.2.\nThis result can be directly deduced from Theorem 2.1\nwith ε = ˜n−r+d\nd . This completes the proof of Corollary 2.2.\n4.3. Proof of Theorem 2.3\nSince the spatial sparseness depends heavily on the localized approximation property,\nwe ﬁrst show that Φf(x) succeeds to realizing the sparseness of the target function f that\nbreaks through the bottleneck of shallow nets [6]. For diﬀerent partitions {An,j}j∈Ndn and\n{BN,j}j∈Nd\nN, we assume n ≥ˆcN for some ˆc > 0 throughout the proof.\nLemma 4.11. Let 0 < ε < 1, under Assumptions 2 and 3, if Φf(x) is deﬁned by (4.2)\nand Kε satisﬁes (2.6), then\n|f(x) −Φf(x)| ≤c0n−r + B2ndε,\n(4.34)\nwhere B2 = ∥f∥L∞(X) + ˜c1. Furthermore, if n ≥ˆcN, there holds\n|Φf(x)| ≤B2ndε, ∀x ∈Id \\\n[\nk∈Λs\n¯Λk.\n(4.35)\nProof.\nSince Id = S\nj∈Ndn An,j, for each x ∈X, there exists a jx ∈N such that\nx ∈An,jx. By Lemma 4.1, we know that for any x ∈An,j, |1 −N ∗\nn,j,Kε(x)| ≤ε and for\nany x ̸∈An,j, |N ∗\nn,j,Kε(x)| ≤ε. From (4.27), we also get\n|Pk,ηj,f(x)| ≤B2,\n(4.36)\nwhere B2 = ∥f∥L∞(X) + ˜c1. Then\n|f(x) −Φf(x)|\n=\n|f(x) −Pk,ηjx,f(x) −\nX\nj̸=jx\nPk,ηj,f(x)N ∗\nn,j,Kε(x) + Pk,ηjx,f(x)(1 −N ∗\nn,jx,Kε(x))|\n≤\n|f(x) −Pk,ηjx,f(x)| + |\nX\nj̸=jx\nPk,ηj,f(x)N ∗\nn,j,Kε(x)| + |Pk,ηjx,f(x)(1 −N ∗\nn,jx,Kε(x))|\n≤\nc0∥x −ηjx∥r + (nd −1)B2ε + B2ε\n≤\nc0n−r + B2ndε.\nSince n > ˆcN, x ∈Id \\ S\nk∈Λs ¯Λk implies An,jx\nT S = ∅. This together with f ∈\n20\nLip(N, s, r, c0) yields f(x) = Pk,ηjx,f(x) = 0. From Lemma 4.1 and (4.36), we have\n|Φf(x)|\n=\n|\nX\nj̸=jx\nPk,ηj,f(x)N ∗\nn,j,K(ε)(x)| + |Pk,ηjx,f(x)N ∗\nn,jx,K(ε)(x)|\n≤\n|Pk,ηj,f(x)|\nX\nj̸=jx\n|N ∗\nn,j,K(ε)(x)|\n≤\n(nd −1)B2ε\n≤\nB2ndε.\nThis completes the proof of Lemma 4.11.\nProof of Theorem 2.3.\nThe proof of this theorem is similar to the proof of Theorem\n2.1. Similar as Step 1 and Step 2 in the proof of Theorem 2.1, we obtain\n\f\f\f\f\f\f\nPk,ηj,f(x)N ∗\nn,j,Kε(x) −B2\n3\n\n2h3\n\u00122N ∗\nn,j,Kε(x) + Pk,ηj,f(x)\n2B3\n\u0013\n−\nh3\n\u0010 N∗\nn,j,Kε(x)\nB3\n\u0011\n2\n−\nh3\n\u0010 Pk,ηj,f(x)\nB3\n\u0011\n2\n\n\n\f\f\f\f\f\f\n≤\nB2\n3ε,\n(4.37)\nwhere B3 := 2(∥f∥L∞(X) + ˜c1 + 1). Denote\n∆2 = B2\n3\n\n2h3\n\u00122N ∗\nn,j,Kε(x) + Pk,ηj,f(x)\n2B3\n\u0013\n−\nh3\n\u0010 N∗\nn,j,Kε(x)\nB3\n\u0011\n2\n−\nh3\n\u0010 Pk,ηj,f(x)\nB3\n\u0011\n2\n\n.\nDeﬁne\nH(x) :=\nnd\nX\nj=1\nHj(x)\nwhere Hj(x) = B2\n3\n\n2h3\n\u0010 2N∗\nn,j,Kε(x)+hk+1,L(x)\n2B3\n\u0011\n−\nh3\n\u0012 N∗\nn,j,Kε (x)\nB3\n\u0013\n2\n−\nh3\n\u0012\nhk+1,L(x)\nB3\n\u0013\n2\n\n.\nProposition 4.5 implies that there exists a shallow net\nhk+1,L(x) =\nk+1\nX\nj=1\nL\nX\ni=0\na(i, j)σ(wj · (x −xi) + bj)\nsuch that\n\f\f\f\f\nPk,ηj,f(x)\nB3\n−hk+1,L(x)\n\f\f\f\f ≤ε1.\n(4.38)\n21\nSince f ∈Lip(N, s, r, c0) with (4.38), we obtain\n|H(x) −Φf(x)|\n≤\nX\nj∈Ndn\n|Hj(x) −∆2 + ∆2 −Pk,ηj,f(x)N ∗\nn,j,Kε(x)|\n≤\nX\nj∈Ndn\n\f\f\f\f\f\f\nB2\n3\n\n2h3\n\u0012hk+1,L(x)\n2\n+ N ∗\nn,j,Kε(x)\n2B3\n\u0013\n−h3(hk+1,L(x))\n2\n−\nh3\n\u0010 N∗\nn,j,Kε(x)\nB3\n\u0011\n2\n\n\n−B2\n3\n\n2h3\n\u0012N ∗\nn,j,Kε(x) + Pk,ηj,f(x)\n2B3\n\u0013\n−\nh3\n\u0010 N∗\nn,j,Kε(x)\nB3\n\u0011\n2\n−\nh3\n\u0010 Pk,ηj,f(x)\nB3\n\u0011\n2\n\n\n\f\f\f\f\f\f\n+ B2\n3ndε\n≤\n9 ˜c2B2\n3nd\n2\n\f\f\f\fhk+1,L(x) −Pk,ηj,f(x)\nB3\n\f\f\f\f + B2\n3ndε\n≤\n9 ˜c2B2\n3nd\n2\nε1 + B2\n3ndε\n=\nB2\n3ndε(9 ˜c2 + 1)\n=\n˜c3ndε\n(4.39)\nwhere ε1 = 2ε, ˜c3 is a constant depending only on B3 and ˜c2.\nDue to (4.39) and Lemma 4.11, for any x ∈X, we get\n|f(x) −H(x)|\n≤\n|f(x) −Φf(x)| + |Φf(x) −H(x)|\n≤\nc0n−r + B2ndε + ˜c3ndε\n=\nc0n−r + ˜Cndε,\nwhere ˜C is a constant depending only on ˜c3, B2.\nMoreover, if x ∈Id \\ S\n∈Λs ¯Λ, we have f(x) = Pk,ηj,f(x) = 0 and |Φf(x)| ≤B2ndε, then\nit is easy to obtain that\n|H(x)|\n≤\n|Φf(x)| + |Φf(x) −H(x)|\n≤\n|Φf(x))| + |Φf(x) −H(x)|\n≤\nB2ndε + ˜c3ndε\n=\n˜Cndε,\nwhere ˜C is a constant depending only on k, c0, d, B2. It is easy to see that there are totally\n22\n˜n := 6nd((L + 1)(k + 1) + 2d + 1) free parameters in H(x). In this case, we obtain\n|f(x) −H(x)| ≤c0˜n−r\nd + ˜C˜nε.\nFurthermore, if x ∈X \\ S\n∈Λs ¯Λand ˜n ≥˜cN d, then\n|H(x)| ≤˜C˜nε.\nIt is noticeable that all the parameters of deep nets are controllable, which is bounded by\npoly(˜n, 1\nε). This completes the proof of Theorem 2.3.\nProof of Corollary 2.4.\nThe result (2.9) can be deduced directly from Theorem\n2.3 with ε = ˜n−T for T ≥r+d\nd . This completes the proof of Corollary 2.4.\n5. References\nReferences\n[1] Z. Akkus, A. Galimzianova, A. Hoogi, D. L. Rubin and B. J. Erickson. Deep learning\nfor brain MRI segmentation: state of the art and future directions. Journal of Digital\nImaging, 30(4): 449-459, 2017.\n[2] D. B. Chen. Degree of approximation by superpsitions of a sigmoidal function. Ap-\nproximation Theory and its Applications, 9:17-28, 1993.\n[3] E. Blum and L. Li. Approximation theory and neural networks. Neural Networks.\n4(4): 511-515, 1991.\n[4] D. C. Ciresan, U. Meier, L. M. Gambardella and J. Schmidhuber. Deep, big, simple\nneural nets for handwritten digit recognition. Neural Computation, 22(12): 3207-\n3220, 2010.\n[5] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.\n[6] C. K. Chui, X. Li and H. N. Mhaskar. Neural networks for localized approximation.\nMathematics of Computation, 63(208): 607-607, 1994.\n23\n[7] C. K. Chui, X. Li and H. N. Mhaskar. Limitations of the approximation capabilities\nof neural networks with one hidden layer. Advances in Computational Mathematics,\n5(1): 233-243, 1996.\n[8] C. K. Chui, S. B. Lin and D. X. Zhou. Construction of neural networks for realization\nof localized deep learning. Frontiers in Applied Mathematics and Statistics, 4: 14,\n2018.\n[9] C. K. Chui, S. B. Lin and D. X. Zhou. Deep neural networks for rotation-invariance\napproximation and learning. Analysis and Applications, 17(05): 737-772, 2019.\n[10] F.Cucker and D. X. Zhou. Learning Theory: An Approximation Theory Viewpoint.\nCambridge University Press, Cambridge, 2007.\n[11] R. Eldan R and O. Shamir. The power of depth for feedforward neural networks.\nConference on learning theory, 907-940, 2016.\n[12] I. Goodfellow, Y. Bengio and A. Courville. Deep Learning. MIT Press, 2016.\n[13] Z. C. Guo, L. Shi and S. B. Lin. Realizing data features by deep nets. IEEE Trans-\naction on Neural Networks and Learning Systems, 2019, arXiv:1901.00130.\n[14] L. Gy¨orﬁ, M. Kohler, A. Krzy ˙zak, et al. A Distribution-Free Theory of Nonparamet-\nric Regression. Springer, Berlin, 2002.\n[15] G. E. Hinton, S. Oshindero and Y. W. Teh. A fast learning algorithm for deep belief\nnetws. Neural Computation, 18: 1527-1554, 2006.\n[16] M. Hagan, M. Beale and H. Demuth. Neural Network Design. PWS Publishing Com-\npany, Boston, 1996.\n[17] Z. Han, S. Q. Yu, S. B. Lin and D. X. Zhou. Depth selection for deep ReLU nets\nin feature extraction and generalization. IEEE Transaction on Pattern Analysis and\nMachine Intelligence, 2019. Under revision.\n24\n[18] V. Kurkov and M. Sanguineti. Can two hidden layers make a diﬀerence? Interna-\ntional Conference on Adaptive and Natural Computing Algorithms. Springer, Berlin,\nHeidelberg, 2013.\n[19] S. B. Lin, X. Liu, Y. H. Rong and Z. B. Xu. Almost optimal estimates for approxima-\ntion and learning by radial basis function networks. Machine Learning, 95:147-164,\n2014.\n[20] S. B. Lin, Y. H. Rong and Z. B. Xu. Multivariate Jackson-type inequality for a\nnew type neural network approximation. Applied Mathematical Modelling, 38(24):\n6031-6037, 2014.\n[21] S. B. Lin, J. S. Zeng and X. Q. Zhang. Constructive neural network learning. IEEE\nTransactions on Cybernetics, 49(1): 221-232, 2019.\n[22] S. B. Lin. Limitations of shallow nets approximation. Neural Networks, 94: 96-102,\n2017.\n[23] S. B. Lin and D. X. Zhou. Distributed kernel-based gradient descent algorithms.\nConstructive Approximation, 47: 249-276, 2018.\n[24] S. B. Lin. Generalization and expressivity for deep nets. IEEE Transactions on Neural\nNetworks and Learning Systems, 30(5): 1392-1406, 2019.\n[25] H. N. Mhaskar. Neural networks for optimal approximation of smooth and analytic\nfunctions. Neural Computation, 8(1): 164-177, 1996.\n[26] H. N. Mhaskar. Approximation theory and neural networks. Neural Networks, 247-\n289, 2008.\n[27] H. Mhaskar H and T. Poggio. Deep vs. shallow networks: an approximation theory\nperspective. Analysis and Applications, 14(6): 829-848, 2016.\n[28] V. E. Maiorov. On best approximation by ridge functions. Journal of Approximation\nTheory, 99: 68-94, 1999.\n25\n[29] V. E. Maiorov. Approximation by neural networks and learning theory. Journal of\nComplexity, 22(1): 102-117, 2006.\n[30] A. Pinkus. Approximation theory of the MLP model in neural networks. Acta Nu-\nmerica, 8: 143-195, 1999.\n[31] P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth func-\ntions using deep ReLU neural networks. Neural Networks, 108: 296-330, 2018.\n[32] M. Raghu, B Poole, J. Kleinberg, S. Ganguli and J. Sohl-Dickstein. On the expressive\npower of deep neural networks. Proceedings of the 34th International Conference on\nMachine Learning, 70: 2847-2854, 2017.\n[33] M. Telgarsky. Beneﬁts of depth in neural networks. 2016, arXiv: 1602.04485.\n[34] D. Yarotsky. Error bounds for approximatons with deep ReLU networks. Neural\nNetworks, 94: 103-114, 2017.\n[35] D. X. Zhou. Universality of deep convolutional neural networks. Applied and Com-\nputational Harmonic Analysis, 2019. In Press.\n[36] D. X. Zhou. Deep distributed convolutional neural networks: universality. Analysis\nApplications, 16: 895-919, 2018.\n26\n",
  "categories": [
    "cs.IT",
    "cs.LG",
    "math.IT"
  ],
  "published": "2020-01-13",
  "updated": "2020-01-13"
}