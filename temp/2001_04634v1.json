{
  "id": "http://arxiv.org/abs/2001.04634v1",
  "title": "Unsupervised Distribution Learning for Lunar Surface Anomaly Detection",
  "authors": [
    "Adam Lesnikowski",
    "Valentin T. Bickel",
    "Daniel Angerhausen"
  ],
  "abstract": "In this work we show that modern data-driven machine learning techniques can\nbe successfully applied on lunar surface remote sensing data to learn, in an\nunsupervised way, sufficiently good representations of the data distribution to\nenable lunar technosignature and anomaly detection. In particular we train an\nunsupervised distribution learning neural network model to find the Apollo 15\nlanding module in a testing dataset, with no dataset specific model or\nhyperparameter tuning. Sufficiently good unsupervised data density estimation\nhas the promise of enabling myriad useful downstream tasks, including locating\nlunar resources for future space flight and colonization, finding new impact\ncraters or lunar surface reshaping, and algorithmically deciding the importance\nof unlabeled samples to send back from power- and bandwidth-constrained\nmissions. We show in this work that such unsupervised learning can be\nsuccessfully done in the lunar remote sensing and space science contexts.",
  "text": "Unsupervised Distribution Learning for Lunar\nSurface Anomaly Detection\nAdam Lesnikowski\nNVIDIA\n2701 San Tomas Expressway\nSanta Clara, CA 95051\nalesnikowski@nvidia.com\nValentin T. Bickel\nETH Zurich & MPS Goettingen\nSonneggstrasse 5\nZurich, 8092, CH\nvalentin.bickel@erdw.ethz.ch\nDaniel Angerhausen\n(a) Center for Space and Habitability\nUniversity of Bern\nGesellschaftsstrasse 6\n3012 Bern\ndaniel.angerhausen@csh.unibe.ch\n(b) Blue Marble Space Institute of Science\n1001 4th Ave, Suite 3201\nSeattle, Washington 98154\nAbstract\nIn this work we show that modern data-driven machine learning techniques can be\nsuccessfully applied on lunar surface remote sensing data to learn, in an unsuper-\nvised way, sufﬁciently good representations of the data distribution to enable lunar\ntechnosignature and anomaly detection. In particular we train an unsupervised\ndistribution learning model to ﬁnd the landing module of the Apollo 15 landing site\nin a testing dataset, with no dataset speciﬁc model or hyperparameter tuning. Sufﬁ-\nciently good unsupervised data density estimation techniques have the potential\nto enable a dazzling number of useful downstream tasks, including locating lunar\nresources for future space ﬂight and colonization, ﬁnding new impact craters or\nlunar surface reshaping, and deciding the importance of unlabeled samples to send\nback from power- and bandwidth-constrained missions. We show in this work that\nsuch unsupervised learning can be successfully done in the lunar remote sensing\nand space sciences contexts.\n1\nIntroduction, Motivations\nThe search for so called technosignatures in our Solar System and beyond has gained new attention\nrecently [1]. Here, technosignatures are physical properties or effects that provide scientiﬁc evidence\nof past or present extraterrestrial technology [2]. According to NASA the search for technosignatures\nshould be performed in parallel to the search for biosignatures in the ﬁeld of astrobiology, as\ntechnosignatures could reveal the existence of intelligent life elsewhere in the universe [1]. One\narea in the ﬁeld of technosignature research is the search for non-terrestrial artifacts in our Solar\nSystem, particularly on the surfaces of planets and moons [e.g., 1, 3, 4]. [5] suggested to start a\nsystematic search for ground-based technosignatures on the lunar surface. Besides technosignatures,\nthere are other objects of scientiﬁc relevance on the lunar surface, such as fresh impact craters, etc.\nHowever, traditional search methods involve inspection of remote sensing data by human operators,\nwhich is a slow, biased, and inefﬁcient process. In addition, the physical shape, form, and material of\nSecond Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019), Vancouver, Canada.\narXiv:2001.04634v1  [astro-ph.IM]  14 Jan 2020\ntechnosignatures is completely unknown and signatures could easily be missed by operators. However,\nthere is a wealth of remote sensing data available, meaning that the bottleneck of technosignature\nsearch is the human component.\nIn recent years, numerous powerful machine learning methods have been developed, such as vision\nmethods [7] and explicit data distribution learners, including variational autoencoders (VAEs) [8, 9],\nﬂow based models [10], and auto-regressive models [11]. Our motivation is to utilize such machine\nlearning-driven approaches to automate technosignature search and to prove the potential of explicit\ndata distribution learners to extract scientiﬁcally relevant results from space exploration data. For\ntechnosignature search, a data and ML-driven approach would provide unique advantages: 1) large\ndata sources enable us to scan the surfaces of entire planets and moons, 2) machine learning allows\nus to remove the human in the loop and to automate the scanning process, and 3) the utilization of\nunsupervised methods allows us to scan for anomalies without a priori knowledge about the physical\nstate and form of the to-be-detected targets. For this proof of concept, the Moon is an ideal test\nbed, as there is a wealth of satellite data available and as numerous exploration missions have left\nnon-natural relics on the surface, such as the Apollo and Luna missions, that can serve as ground\ntruth for model validation and testing.\n2\nMethods\n2.1\nData\nThe detection of objects with unknown shapes and sizes on the surface of a planet or moon requires\nimagery with two main characteristics, 1) sufﬁcient spatial resolution and 2) global coverage. On the\nMoon, only one sensor has both characteristics, the Narrow Angle Camera onboard NASA’s Lunar\nReconnaissance Orbiter (LRO). Since LRO’s launch in 2009, NAC covered the entire surface of the\nMoon multiple times, returning more than 1.6 million optical images with a spatial resolution ranging\nfrom 0.5 to 1.5 m/pixel. These NAC images can be retrieved from the Planetary Data System\n(PDS) in a large variety of formats and processing levels. While there are many lunar global datasets\navailable, the NAC image stack is the only data with spatial resolutions sufﬁcient for the detection of\nsmall geomorphological and human-made objects. For this study, we used the uppermost layer of\nthe pre-calibrated pyramid tif ﬁles (ptifs) that can be retrieved from the LROC image archive. These\nimages provide the full spatial resolution (in the top layer), but are reduced in size, as they have been\nreduced to 8bit. The reduced size allows for optimized ﬁle download, handling, and processing times.\nIn a ﬁrst step we collected all available NAC imagery over the Apollo 15 and 17 landing sites with\nspatial resolutions higher than 0.8 m/pixel. We then tiled these images in patches of 64x64 pixels,\nresulting in 255,000 NAC tiles. This dataset had a stride of thirty-two pixels, so that each pixel was\nincluded, on average, in four images. All used patches cover an area of approximately sixty-four\nmillion square meters and include the Apollo 15 and 17 landing sites.\n2.2\nModel\nFor our unsupervised data distribution learner, we used a variational autoencoder (VAE). [13, 14]\nThis autoencoder has an encode (decode) module layers eight layers deep that alternated between\nconvolution (transposed convolution) and batch norm layers. By the end of the encoding layer, the\ndata undergoes a learned transformation from its native 3 channels x 64 pixels x 64 pixels image\nspace size to 128 ﬁlters each of size 16 pixels x 16 pixels. We used a default informational bottleneck\nlatent code dimension size of 26, but varied this down to 23 and up to 211 to control model capacity,\nas well as training time, inference time and memory size, between experiment runs. We used the\nreparametrization trick to allow differentiability through the stochastic sampling process of the VAE.\nThe loss bound that we sought to minimize included the calculation of the KL divergence between\nthe distribution of latent codes and a multivariate Gaussian prior with dimension that is the size of the\ninformational bottleneck latent code dimension. We also used the approach of the beta-VAE literature\n[12]to upweight the divergence term of the loss to promote disentangled learned representations. The\nanomaly score of an image relative to our trained model is the L2 reconstruction loss between the\nimage and the VAE’s reconstruction of the image. A promising anomaly score, not yet fully evaluated,\nis the norm of the µ vector in the VAE’s code of that image, which can be seen as a distance of the\nimage’s latent code from the mode of the latent code distribution, and hence a quantitative degree of\nanomaly-ness.\n2\n2.3\nMetrics, Evaluating Lunar Surface Anomalies\nFor evaluating anomaly detection on the lunar surface, we took NAC image patches with human-made\nartifacts as positive anomalous samples for our experiments, and all image samples with no known\nhuman-made artifacts negative non-anomalous samples. Examples of these human-made artifacts\nin our study were lunar lander modules left from the American Apollo 15 and 17 missions. For\nmodel evaluation metrics, we used the average precision (AP) of our model’s precision recall curves,\nprecision at total recall (PaTR), deﬁned as\nPaTR = precision at the model’s threshold where the model returns all positive samples,\nand efﬁciency over manual vetting (EoMV), deﬁned as\nEoMV =\nPaTR\nprecision of the naive random anomaly scorer.\nThe motivation for EoMV is from considering it as the fraction of images needed to be vetted by a\nhuman using a random search over the ratio of images needed to be vetted by a human using our\nmodel to present images by descending anomaly score order.\n2.4\nSoftware, Hardware\nWe used PyTorch for training, JupyterLab with Python to coordinate the experiments, and the seaborn\nstatistical visualization python package to view and plot results. We used an NVIDIA GeForce GTX\n1070 and Intel Core i7 system with 512 GB SSD for training and validation.\n3\nExperiments\nFor our ﬁrst set of experiments, we sought to verify our proposed method using a toy dataset\ncontaining 200 image patches around the Apollo 17 landing site, containing four positive lunar\ntechnosignature patches. For our second set of experiments, we tuned our model architecture and\nhyper-parameters on a validation set containing the Apollo 17 landing site. We used a dataset of 2000\nimage patches around the Apollo 17 landing site with six positive patches. More positive patches\nwere obtained over our ﬁrst experiment through a ﬁner vertical striding of the dataset creation. For\nour third set of experiments, we validated the performance of our method on a test set containing a\nnew, unseen landing site, Apollo 15, with no tuning of model architecture or hyper-parameters on this\ndataset. We used a dataset of 8000 image patches around Apollo 15 landing site, which contained\nten positive technosignature patches. The model architecture and all other model and optimizer\nhyper-parameters were learned one the train set above, and evaluated on our train set in a one-shot\nfashion, with results reported below. Extra care was taken to avoid hyperparameter leakages between\noptimizing our approach on the train set and evaluating performance on our train set, due to the\nacutely limited nature of human lunar technosignatures.\n4\nResults\n4.1\nLunar VAE Landing Site Validation\nFor our smallest dataset containing 200 patches, our method achieved an AP and PaTR of 1.00 with\nminimal tuning by returning all positive samples before all negative samples. Hence we moved to a\nmore challenging and diverse validation dataset. For our larger validation dataset of 2000 patches, in\nFigure 1 we plot the precision-recall curve with metrics for validation on our Apollo 17 train set, and\nthe distribution of anomaly scores of all samples on the horizontal axis versus only positive samples\non the horizontal axis. We achieve an AP of 0.25, PaTR of 0.333, resulting in an EoMV of 0.333\n0.003 =\n111. Note the unusual shape of the PR curve, which was due to a clustering of the positive sample\nanomaly scores into one mode.\n4.2\nLunar VAE Landing Site Test\nIn Figure 2 we plot the precision-recall curve with metrics for validation on our Apollo 17 train set,\nand the distribution of anomaly scores of all samples on the horizontal axis versus only positive\n3\nFigure 1: Validation set precision-recall curve with metrics (left) and the distribution of anomaly\nscores of all samples on the horizontal axis (middle) versus only positive samples on the horizontal\naxis (right).\nsamples on the horizontal axis. We achieve an AP of 0.49, PaTR of 0.055, and an EoMV of\n0.055\n0.0012 =\n45.8. We note the high left hand portion of the PR curve, which was due to the top three returned\nsamples being positive samples.\nFigure 2: Test set precision-recall curve with metrics (left) and the distribution of anomaly scores of\nall samples on the horizontal axis (middle) versus only positive samples on the horizontal axis (right).\n4.3\nEvaluation of Learned Features\nIn Figure 3, we plot two different input images each presented twice (top row) with their corresponding\nreconstructed images through the VAE’s forward pass (bottom row). We note the stochastic nature\nof VAE generation. In the same ﬁgure we plot a sample walk through the learned latent variable\nspace of our trained model on the Apollo 17 landing site. We consider the relative quality of\nreconstruction and the smoothness of interpolation between the two data samples at upper-left and\nlower-right hand corners as evidence that the unsupervised generative model was able to learn a\ngood representation of the underlying lunar dataset, in addition to the performance of the learned\ndistribution for technosignature detection.\nFigure 3: Sample reconstructions of two images (left) and a sample walk through the learned latent\nvariable space (right).\n5\nConclusion\nIn this work we show that modern data-driven machine learning techniques can be successfully\napplied on lunar surface data to learn, in an unsupervised way, sufﬁciently good representations of\n4\nthe distribution of lunar surface data to enable lunar technosignature detection. In particular we\nhave trained an unsupervised distribution learning model to ﬁnd the landing module of the Apollo\n15 landing site in a testing dataset, with no speciﬁc model or hyperparamter tuning . Good data\ndensity estimation has myriad applications in lunar and space sciences, including ﬁnding known\nmissions with unknown landing sites, discovering non-publicly disclosed landing sites, either by\ngovernmental or non-governmental organizations, technosignatures in other signal domains, locating\nlunar resources for future space ﬂight and colonization, locating new impact craters or lunar surface\nreshaping, e.g. when applying this to temporal stacks of images, and deciding the importance of\nunlabeled samples to send back from power- and bandwidth-constrained missions. We hope this\ncurrent work stimulates and enables future work towards these goals.\n5\nReferences\n[1] N. Technosignatures Workshop Participants, NASA and the Search for Technosignatures: A Report from the\nNASA Technosignatures Workshop, arXiv e-prints (2018) arXiv:1812.0868\n[2] J. C. Tarter, The evolution of life in the universe: are we alone?, Proceedings of the International Astronomical\nUnion 2 (14) (2006)\n[3] R. N. Bracewell, Communications from Superior Galactic Communities, 186 (1960) 670-671.\ndoi:10.1038/186670a0.\n[4] R. A. Freitas, F. Valdes, The search for extraterrestrial artifacts (SETA), Acta Astronautica 12 (12) (1985)\n1027\n[5] P. C. W. Davies, R. V. Wagner, Searching for alien artifacts on the moon, Acta Astronautica 89 (2013) 261\n[6] J. Haqq-Misra, R. K. Kopparapu, On the likelihood of non-terrestrial artifacts in the Solar System, Acta\nAstronautica 72 (2012) 15\n[7] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n[8] Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\n[9] Kingma, D. P., & Welling, M. (2019). An Introduction to Variational Autoencoders. arXiv preprint\narXiv:1906.02691.\n[10] Kingma, D. P., & Dhariwal, P. (2018). Glow: Generative ﬂow with invertible 1x1 convolutions. In Advances\nin Neural Information Processing Systems (pp. 10215-10224).\n[11] Reed, S., van den Oord, A., Kalchbrenner, N., Colmenarejo, S. G., Wang, Z., Chen, Y., ... & de Freitas, N.\n(2017, August). Parallel multiscale autoregressive density estimation. In Proceedings of the 34th International\nConference on Machine Learning-Volume 70 (pp. 2912-2921). JMLR. org.\n[12] Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., ... & Lerchner, A. (2017). beta-VAE:\nLearning Basic Visual Concepts with a Constrained Variational Framework. ICLR, 2(5), 6.\n6\n",
  "categories": [
    "astro-ph.IM",
    "astro-ph.EP",
    "cs.LG",
    "I.2.1; I.4.9; I.2.10; J.2"
  ],
  "published": "2020-01-14",
  "updated": "2020-01-14"
}