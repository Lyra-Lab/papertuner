{
  "id": "http://arxiv.org/abs/2001.06280v1",
  "title": "Review: deep learning on 3D point clouds",
  "authors": [
    "Saifullahi Aminu Bello",
    "Shangshu Yu",
    "Cheng Wang"
  ],
  "abstract": "Point cloud is point sets defined in 3D metric space. Point cloud has become\none of the most significant data format for 3D representation. Its gaining\nincreased popularity as a result of increased availability of acquisition\ndevices, such as LiDAR, as well as increased application in areas such as\nrobotics, autonomous driving, augmented and virtual reality. Deep learning is\nnow the most powerful tool for data processing in computer vision, becoming the\nmost preferred technique for tasks such as classification, segmentation, and\ndetection. While deep learning techniques are mainly applied to data with a\nstructured grid, point cloud, on the other hand, is unstructured. The\nunstructuredness of point clouds makes use of deep learning for its processing\ndirectly very challenging. Earlier approaches overcome this challenge by\npreprocessing the point cloud into a structured grid format at the cost of\nincreased computational cost or lost of depth information. Recently, however,\nmany state-of-the-arts deep learning techniques that directly operate on point\ncloud are being developed. This paper contains a survey of the recent\nstate-of-the-art deep learning techniques that mainly focused on point cloud\ndata. We first briefly discussed the major challenges faced when using deep\nlearning directly on point cloud, we also briefly discussed earlier approaches\nwhich overcome the challenges by preprocessing the point cloud into a\nstructured grid. We then give the review of the various state-of-the-art deep\nlearning approaches that directly process point cloud in its unstructured form.\nWe introduced the popular 3D point cloud benchmark datasets. And we also\nfurther discussed the application of deep learning in popular 3D vision tasks\nincluding classification, segmentation and detection.",
  "text": "Review: deep learning on 3D point clouds\nSaifullahi Aminu Bello 1,2, Shangshu Yu1, Cheng Wang1, Senior Member, IEEE\n1Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, China\n2Kano University of Science and Technology, Nigeria\nAbstract\nPoint cloud is point sets deﬁned in 3D metric space. Point cloud has become one of the most signiﬁcant data format for 3D\nrepresentation. Its gaining increased popularity as a result of increased availability of acquisition devices, such as LiDAR, as well\nas increased application in areas such as robotics, autonomous driving, augmented and virtual reality. Deep learning is now the\nmost powerful tool for data processing in computer vision, becoming the most preferred technique for tasks such as classiﬁcation,\nsegmentation, and detection. While deep learning techniques are mainly applied to data with a structured grid, point cloud, on\nthe other hand, is unstructured. The unstructuredness of point clouds makes use of deep learning for its processing directly very\nchallenging. Earlier approaches overcome this challenge by preprocessing the point cloud into a structured grid format at the cost\nof increased computational cost or lost of depth information. Recently, however, many state-of-the-arts deep learning techniques\nthat directly operate on point cloud are being developed. This paper contains a survey of the recent state-of-the-art deep learning\ntechniques that mainly focused on point cloud data. We ﬁrst brieﬂy discussed the major challenges faced when using deep learning\ndirectly on point cloud, we also brieﬂy discussed earlier approaches which overcome the challenges by preprocessing the point\ncloud into a structured grid. We then give the review of the various state-of-the-art deep learning approaches that directly process\npoint cloud in its unstructured form. We introduced the popular 3D point cloud benchmark datasets. And we also further discussed\nthe application of deep learning in popular 3D vision tasks including classiﬁcation, segmentation and detection.\nKeywords: point cloud, deep learning, datasets, classiﬁcation, segmentation, object detection\n1. Introduction\nWe live in a three-dimensional world, but since the invention of\nthe camera in 1888, visual information of the 3D world is be-\ning projected onto 2D images using cameras. 2D images, how-\never, lose depth information and relative positions between two\nor more objects in the real world, which makes it less suitable\nfor applications that require depth and positioning information\nsuch as robotics, autonomous driving, virtual reality and aug-\nmented reality among others. To capture the 3D world with\ndepth information, early convention was to use stereo vision\nwhere 2 or more calibrated digital cameras are used to extract\nthe 3D information. Point cloud is a data structure that is often\nused to represent 3D geometry, making it the immediate rep-\nresentation of the extracted 3D information from stereo vision\ncameras as well as of the depth map produced by RGB-D. Re-\ncently, 3D point cloud is booming as a result of increasing avail-\nability of sensing devices such as LiDAR and more recently,\nmobile phones with time of ﬂight (tof) depth camera, which al-\nlow easy acquisition of the 3D world in 3D point cloud.\nPoint cloud is simply a set of data points in a space. The point\ncloud of a scene is the set of 3D points sampled around the\nsurface of the objects in the scene. In its simplest form, a 3D\npoint cloud is represented by the XYZ coordinates of the points,\nhowever, additional features such as surface normal, RGB val-\nues can also be used. Point cloud is a very convenient format for\nrepresenting 3d world and it has a range of application in dif-\nferent areas such as robotics, autonomous vehicles, augmented\nand virtual reality and other industrial purposes like manufac-\nturing, building rendering e.t.c.\nIn the past few years, processing of point cloud for visual intel-\nligence has been based on handcrafted features [1, 2, 3, 4, 5, 6].\nThe review of handcrafted based feature learning techniques is\nconducted in [7]. The handcrafted features do not require large\ntraining data and were seldom used as there were not enough\npoint cloud data and deep learning was not popular.\nHow-\never, with increasing availability of acquisition devices, point\ncloud data is now readily available making use of deep learning\nfor its processing feasible. However, the application of deep\nlearning on point cloud is not easy due to the nature of the\npoint cloud. In this paper, we review the challenges of point\ncloud for deep learning; the early approaches devised to over-\ncome these challenges; and also the recent state-of-the-arts ap-\nproaches that directly operate on point cloud, focusing more\non the latter. This paper is intended to serve as guide to new\nresearchers in the ﬁeld of deep learning on point cloud as it\npresents the recent state-of-the-arts approaches of deep learn-\ning on point cloud.\nWe organized the rest of the paper into the following: section 2\ndiscussed the challenges of point cloud which makes the appli-\ncation of deep learning diﬃcult. Section 3 reviewed the meth-\narXiv:2001.06280v1  [cs.CV]  17 Jan 2020\nods that overcome the challenges by converting the point cloud\ninto a structured grid. Section 4 contains in-depth of the var-\nious deep learning methods that process point cloud directly.\nIn section 5, we presented 3D point cloud benchmark datasets.\nWe discussed the application of the various approaches in the\n3D vision tasks in section 6. We summarize and conclude the\npaper in section 7.\n2. Challenges of deep learning on point clouds\nApplying deep learning on 3D point cloud data comes with\nmany challenges. Some of these challenges include occlusion\nwhich is caused by clutterd scene or blind side; noise/outliers\nwhich are unintended points; points misalignment e.t.c. How-\never, the more pronounced challenges when it comes to appli-\ncation of deep learning on point clouds can be categorized into\nthe following:\nIrregularity: Point cloud data is also irregular, meaning, the\npoints are not evenly sampled accross the diﬀerent regions of\nan object/scene, so some regions could have dense points while\nothers sparse points. These can be seen in ﬁgure 1a.\nUnstructured: Point cloud data is not on a regular grid. Each\npoint is scanned independently and its distance to neighboring\npoints is not always ﬁxed, in contrast, pixels in images are rep-\nresented on a 2 dimension grid, and spacing between two adja-\ncent pixels is always ﬁxed.\nUnorderdness:\nPoint cloud of a scene is the set of\npoints(usually represented by XYZ) obtained around the ob-\njects in the scene and are usually stored as a list in a ﬁle. As\na set, the order in which the points are stored does not change\nthe scene represented. For illustration purpose, we show the\nunordered nature of point sets in ﬁgure 1c\nThese properties of point cloud are very challenging for deep\nlearning, especially convolutional neural networks (CNN).\nThese is because convolutional neural networks are based on\nconvolution operation which is performed on a data that is\nordered, regular and on a structured grid. Early approaches\novercome these challenges by converting the point cloud into\na structured grid format, section\n3.\nHowever, recently re-\nsearchers have been developing approaches that directly uses\nthe power of deep learning on raw point cloud, see section\n4, doing away with the need for conversion to structured\ngrid.\n3. Structured grid based learning\nDeep learning, speciﬁcally convolutional neural network is suc-\ncessful because of the convolution operation. Convolution op-\neration is used for feature learning, doing away with hand-\ncrafted features. Figure 2 shows a typical convolution oper-\nation on a 2D grid. The convolusion operation requires a struc-\ntured grid. Point cloud data on the other hand is unstructured,\nand this is a challenge for deep learning, and to overcome the\nchallenge many approaches convert the point cloud data into a\nstructured form. These approaches can be broadly divided into\ntwo categories, voxel based and multiview based. In this sec-\ntion, we review some of the state-of-the-arts methods in both\nvoxel based and multiview based categories, there advantages\nas well as there drawbacks.\nFigure 2: A typical 2D convolution operation\n3.1. Voxel based\nConvolution operation on 2d images, uses a 2d ﬁlter of size ˙x×˙y\nto convolve a 2D input represented as matrix of size ˙X × ˙Y with\n˙x <= ˙X and ˙y <= ˙Y. Voxel based methods [8, 9, 10, 11, 12]\nuses similar approach by converting the point cloud into a 3D\nvoxel structure of size X × Y × Z and convolve it with 3D ker-\nnels of size x × y × z with x, y, z <= X, Y, Z respectively. Ba-\nsically, two important operations takes place in this methods,\nthe oﬄine(preprocessing) and the online (learning). The of-\nﬂine methods converts the point cloud into a ﬁxed size voxels\nas shown in ﬁgure 3. Binary voxels [13] is often used to repre-\nsent the voxels. In [11] a normal vector is added to each voxel\nto improve discimination capability.\nFigure 3: Point cloud of an airplane is voxelized to 30 × 30 × 30 volumetric\noccupancy grid\nThe online operation, is the learning stage. In this stage, deep\nconvolutional neural network is designed usually using a num-\nber of 3D convolutional, pooling, and fully connected lay-\ners.\n[13] represented 3D shapes as a probability distribution of bi-\nnary variables on a 3D voxel grid and were the ﬁrst work that\n2\n(a) Irregular. Sparse and dense regions\n(b) Unstructured. No\ngrid,\neach point is\nindependent and dis-\ntance between neigh-\nboring points is not\nﬁxed\n(c) Unordered. As a set, point cloud is invariant to permu-\ntation\nFigure 1: Challenges\nuses 3D Deep Convolutional Neural Networks. The input to\nthe network, point cloud, CAD models or RGB-D images, is\nconverted into a 3D binay voxel grid and is processed using a\nconvolusional deep belief network [14]. [8] uses 3D CNN for\nlanding zone detection for unmanned rotorcraft. LiDAR from\nthe rotorcraft is used to obtain point cloud of the landing site,\nwhich is then voxelized into 3D volumes and 3D CNN binary\nclassiﬁer is applied to classify the landing site as safe or other-\nwise. In [9] a 3D Convolutional Neural Network is proposed\nfor object recognition, like [13], the input to the network in [9]\nis converted into a 3D binary occupancy grid before applying\n3D convolution operations to generate a feature vector which\nis passed through fully connected layers to obtain class scores.\nTwo voxel based models where proposed in [10]. First model\naddressed overﬁtting using auxiliary training tasks to predict\nobject from partial subvolumes and the second model mimic\nMultiview-CNNs by convolving the 3D shapes with anisotropic\nprobing kernel.\nVoxel based methods, although have shown good performances,\nthey however do suﬀer from high memory consumption due to\nthe sparsity of the voxels, ﬁgure 3, which results in wasted\ncomputation when convolving over the non occupied regions.\nThe memory consumption also limits the voxel resolution to\nusually between 32 cube to 64 cube. These drawbacks is also\nin addition to the artifacts introduced by the voxelization oper-\nation.\nTo overcome the challenges of voxelization, [15, 16] proposed\nadaptive representation. These representation is much complex\nthan the regular 3D voxels, however, its still limited to only 256\ncube voxels.\n3.2. multiview based\nThese methods [17, 18, 19, 10, 20, 21, 22], take advantage of\nthe already matured 2D CNNs into 3D. Because images are ac-\ntually representation of the 3D world squashed onto a 2D grid\nby a camera, methods under this category follows these tech-\nnique by converting point cloud data into a collection of 2D\nimages and apply existing 2D CNN techniques to it, see 4.\nCompared to their volumetric based counter parts, Multiview\nbased methods have better performance as the Multiview im-\nages contains richer information than 3D voxels even though\nthe latter contains depth information.\nFigure 4: multiview projection of point cloud to 2D images. Each 2D image\nrepresents the same object viewed from a diﬀerent angle\n[17] is the ﬁrst work in this direction with the aim of bypassing\nthe need for 3D descriptors for recognition and achieved state-\nof-the-arts accuracy.\n[18] proposed a stacked local convolu-\ntional autoencoder (SLCAE) for 3D object retrieval.\n[10] in-\ntroduced multi-resolution ﬁltering which captures information\nat multiple scales and in addition they used data augmentation\nto improved on [17].\nMultiview based networks have better performance than voxel\nbased methods, this is because of two reasons, 1) they used an\nalready well researched 2D techniques and 2) they can contains\nreacher information as they do not have quantization artifacts\nof voxelization.\n3.3. Higher dimensional lattices\nThere are other methods for point cloud processing using deep\nlearning that converts the point cloud into higher dimensional\nregular lattice. SplatNet [23] processes point cloud directly,\nhowever, its primary feature learning operation occurs at the\nbilateral convolutional layer(BCL). The BCL layer converts the\n3\nfeatures of unordered points into a six-dimensional(6D) permu-\ntohedral lattice, and convolve it with a kernal of similar lattice.\nSFCNN [24] uses a a fractalized regular icosahedral lattice to\nmap points onto a discretized sphere and deﬁned a multi-scale\nconvolution operation on the regular shperical lattice.\n4. Deep learning directly on raw point cloud\nDeep learning on raw point cloud is receiving lot of attention\nsince PointNet [25] was released in 2017. Many state-of-the-\narts methods have been developed since then. These techniques\nprocess point cloud directly despite the challenges of section 2.\nIn this section, we review the state-of-the-arts techniques that\nwork in this direction. We began with PointNet which is the\nbedrock for most of the techniques. Other techniques improved\non PointNet by modeling local region structure.\n4.1. PointNet\nConvolutional Neural Networks is largely successful because of\nthe convolution operation, which enables learning on local re-\ngions in a hierachical manner as the network gets deeper. Con-\nvolution however, requires structured grid which is lacking in\npoint cloud data. PointNet [25] is the ﬁrst method that applies\ndeep learning on unstructured point cloud and its the basis for\nwhich most other techniques are based on. In this subsection\nwe give a review of PointNet.\nThe architecture of PointNet is shown in ﬁgure 5. The input to\nPointNet is raw point cloud P = RN×D, where N represents the\nnumber of points in the point cloud and D the dimension, usu-\nally D = 3 representing the XYZ values of each points, however\nadditional features can be used. Because points are unordered,\nPointNet is made up with symmetric funtions. Symmetric func-\ntions are functions whose output are the same irrespective of the\ninput order. PointNet is built on 2 basic symmetric functions,\nmultilayer perceptron(MLP) with learnable parameters, and a\nmaxpooling function. The MLPs are feature transformations\nthat transform the feature dimension of the points from D = 3\nto D = 1024 dimensional space and there parameters are shared\nby all the points in each layer. To aggregate the global fea-\nture, maxpooling symmetric function is employed to produce\none global 1024-dimensional feature vector. The feature vector\nrepresent the feature descriptor of the input which can be used\nfor recognition and segmentation tasks.\nPointNet achieves state-of-the-arts performance on several\nbenchmark datasets. The design of PointNet, however, do not\nconsiders local dependency among points, thus, it does not cap-\nture local structure. The global maxpooling applied select the\nfeature vector in a ”winner–take –all” [26] principle, making it\nvery susceptible to targetted adversarial attack as demonstrated\nin [27]. After PointNet many approaches were proposed to\ncapture local structure.\n4.2. Approaches with local structure computation\nMany state-of-the-arts approaches where developed after Point-\nNet that captures local structure. These techniques capture local\nstructure hierarchically in a smilar fashion to grid convolution\nwith each heirachy encoding richer representation.\nBasically, due to the inherent nature of point cloud of un-\norderedness, local structure modeling rests on three basic op-\nerations: sampling; grouping; and a mapping function that is\nusually approximated by a multilayer perceptron (MLP) which\nmaps the features of the nearest neighbor points into a feature\nrepresentation that encodes higher level information, see ﬁg-\nure 6. We brieﬂy explained this operations before reviewing\nthe various approaches.\nSampling Sampling is employed to reduce resolution of points\naccross layers in synonymity to how convolution operation re-\nduces the resolution of feature maps via convolutional and pool-\ning layers. Giving point cloud P ∈RN×3 of N points, the sam-\npling reduces it to M points ˆP ∈RM×3, where M ≤N. The\nsubsampled M points, also referred to as representative points\nor centroids, are used to represent the local region from which\nthey were sampled. Two approaches are popular for subsam-\npling 1) random point sampling, where each of the N points\nis equally likely to be sampled and 2) farthest point sampling\n(FPS) where the M points are sampled such that each sampled\npoint is the most distant point from the rest of the M −1 points.\nOther sampling methods include uniform sampling and Gumbel\nSubset Sampling [28].\nGrouping With the representative points sampled, k-nearest\nneighbor algorithm is use to select the nearest neighbor points\nto the representatives points to group them into a local patch,\nﬁgure 7. The points in a local patch will be used to compute\nthe local feature representation of the neighborhood. In grid\nconvolution, the receptive ﬁeld, are the pixels on the feature\nmap under a kernel. The kNN is either used directly where k\nnearest points to a centroid are sampled, or a ball query is used.\nWith ball query, points are selected only when they are within\na radius distance to the centroid points.\nNon-linear mapping function Once the nearest points to each\nrepresentative points are obtained, the next step is to map them\ninto a feature vector which represents the local structure. In grid\nconvolution, the receptive ﬁeld is mapped into a feature neuron\nusing a simple matrix multiplication and summation with con-\nvolutional kernels. This is not easy in point cloud, because the\npoints are not structured, therefore most approaches approxi-\nmate the function using PointNet [25] based methods which\nis composed of symmetric functions consisting of a multilayer\nperceptrons, h(·), and a maxpooling function, g(·) as shown in\nequation 1.\nf({x1, ...xk}) ≈g(h(x1), ..., h(xk))\n(1)\n4\nFigure 5: Architecture of PointNet [25]. PointNet is composed up of multilayer perceptrons(MLPs) which are shared point-wise and 2 spatial transformer\nnetworks(STN) of 3 × 3 and 64 × 64 dimensions which learn the canonical representation of the input set. Global feature is obtained in a winner-takes-all principal\nand can be used for classiﬁcation and segmentation tasks.\nFigure 6: Basic operations for capturing local structure in point cloud. Giving\nP ∈RN×(3+c) points, each point represented by XYZ and c feature channel\n(for input points, c can be point features such as normals, rgb, etc or zero).\nM ⩽N centroids points are sampled from N, and k-NN points to each of the\ncentroid are selected to form a M groups. Each of the M group represents\na local region(receptive ﬁeld). A non-linear function, usually approximated by\nPointNet based MLP, is then applied on the local region to learn C- dimensional\nlocal region feature (C ⩾c).\n4.2.1. Approaches that do not explore local correlation\nSeveral approaches follow pointnet like MLP where correlation\nbetween points within a local region are not considered and in-\nstead, individual point features are learned via shared MLP and\nlocal region feature is aggregated using a maxpooling function\nin a winner-takes-all principle.\nPointNet++ [29] extended PointNet for local region computa-\ntion by applying pointnet hiearchically in local regions. Giv-\ning a point sets, P ∈RN∗3, farthest point sampling algorithm is\nused to select centroids, and ball query is used to select near-\nest neighbor points for each centroids. PointNet is then applied\non the local regions to generate a feature vector of the regions.\nThese process is repeated in a hierarchical form thereby reduc-\ning the points resolution as it goes deeper. In the last layer along\nthe hierarchy, the whole point’s features are passed through\na PointNet to produce one global feature vector. PointNet++\nachieves state of the art accuracy on many public datasets in-\ncluding, ModelNet40 [13] and ScanNet [30].\nVoxelNet [31] proposed a Voxel Feature Encoding(VFE). Giv-\ning a point cloud, it is ﬁrst casted into 3D voxels of resolution\nˆD × ˆH × ˆW , and points are grouped according to the voxel they\nfall into. Because of the irregularity of point cloud, T points are\nsampled in each voxel inorder to have uniform number of points\nper voxel. In a VFE layer, the centroids for each of the voxel is\ncomputed as a local mean of the T points withing the voxel, the\nT points are are then processed using a fully connected network\n(FCN) to aggregate information from all the points similar to\nPointNet. The VFE layers are stacked and a maxpooling layer\nis applied to get a global feature vector of each voxel making\nthe feature of the input point cloud to be represented by a sparse\n4D vector, C × ˆD × ˆH × ˆW. To ﬁt voxelnet into ﬁgure 6 the cen-\ntroids for each voxel are the centroids/representative points, the\nT points in each voxel are the nearest neighbor points and the\nFCN is the non linear mapping function.\nSelf organizing map, (SOM), originally proposed in [32], is\nused to create a self organizing networks for point cloud in SO-\nNet [33]. While random point sampling/farthest point sam-\npling/ uniform sampling is used to select centroids in most of\nthe methods discussed, in So-Net, SOM is constructed with a\n5\nFigure 7: Sampling and Grouping of points into local patch. The reds are the centroid points selected using sampling algorithms, and the grouping shown is a ball\nquery where points are selected based on a radius distance to the centroid.\nﬁxed number of nodes which are dispersed uniformly in a unit\nball. The SOM nodes are permutation invariant and plays the\nroles of local region centroid. For each SOM node, k-NN search\nis used to ﬁnd its nearest neighbor points which are passed\nthrough a series of fully connected layers to extract point fea-\ntures which are maxpooled to generate M nodes features. To\nobtain the global feature of the input point cloud, the M nodes\nfeatures are aggregated using maxpooling.\nPointwise convolution is proposed in [34]. In this technique,\nthere is no subsampled/representative points, because the con-\nvolution operation is done on all the input points.\nIn each\npoint, nearest neighbor points are sampled based on a size or\nradius value of a kernel centered on the point. The radius value\ncan be adjusted for diﬀerent number of neighbor points in any\nlayer. Each pointwise convolution is applied independently on\nthe input and it transforms input points from 3-dimension to 9-\ndimension. The ﬁnal feature is obtained by concatenating the\noutput of all the pointwise convolution for each point and it has\na resolution equavalent to the input. These ﬁnal feature is then\nused for segmentation using convolution layer or classiﬁcation\ntask using fully connected layers.\n4.2.2. Approaches that explore local correlation\nSeveral approaches explore the local correlations between\npoints in a local region to improve discriminative capabil-\nity. This is intuitive because points do not exist in isolation,\nrather, multiple points together are needed to form a meaning-\nful shape.\nPointCNN\n[35] improved on PointNet++ by proposing an\nX-transformation on the k-nearest neighbor points of each\ncentroids before applying a PointNet-like MLP. The cen-\ntroids/representative points are randomly sampled, and k-NN\nis used to select the neighborhood points which are passed\nthrough an X-transformation block before applying the non-\nlinear mapping function. The purpose of the X-transform is to\npermute the input into a more canonical form which in essence\nalso takes into consideration the relationship between points\nwithin a local region. In pointweb [36], ”a local web of points”\nis designed by densely connecting points within a local region\nand learns the impact of each point on the other points using\nan Adaptive Feature Adjustment (AFA) module. In [37] the\nauthors propsed a ”pointConv” operation which similarly ex-\nplore the intrinsic structure of points within a local region by\ncomputing the inverse density scale of each point using a ker-\nnel density estimation (KDE). The kernel density estimation is\ncomputed oﬄine for each point, and is fed into an MLP to esti-\nmate the density estimates.\nIn [38], the centroids are selected using uniform sampling strat-\negy, and the nearest neighbor points to the centroids are selected\nusing spherical neighborhood. The non-linear function is also\napproximated using a multi-layer perceptron(MLP), but with\nadditional discriminative capability by considering the relation\nbetween each centroids to its nearest neighbor points. The re-\nlationship between neighboring points is based on the spatial\nlayout of the points. Similaryly, GeoCNN [39] explores ge-\nometric structure within local region by weighing the features\nof neighboring points based on the distance to their respective\ncentroid point, however, the authors performs point wise convo-\nlution without reducing point resolution across layers.\n[40] argues that overlapping receptive ﬁeld caused by multi-\nscale architecture of most of PointNet based approaches could\nresult in computation redundancy because same neigboring\npoints could be included in diﬀerent scaled regions. To ad-\ndress the redundancy, the authors proposed annularly convolu-\ntion which is a ring based approach that avoids having overlaps\nbetween hierarchy of receptive ﬁelds and alsp captures relation-\nship between points in within the recpetive ﬁeld.\nPointNet-like MLP is the popular mapping function for approx-\nimating points in a local patch into a feature vector, however,\n[41] argues that MLP does not account for the geometric prior\nof point clouds and also requires suﬃcently large parameters.\nTo address these issues, the authors proposed a family ﬁlters\nthat are composed of two functions, a step function that encodes\nlocal geodesic information, followed by a third order taylor ex-\npansion. The approach learns hierarchical representations and\nachieves state-of-the-art performance in classiﬁcation and seg-\nmentation tasks.\nPoint Attention transformers (PAT) is proposed in [28]. The\nauthors proposed a new subsampling method termed ”Gumbel\nSubset Sampling (GSS)” which unlike farthest point sampling\n6\n(FPS), its permutation invariant, and its robust to outliers. The\nauthors used absolute and relative position embedding, where\neach point is represented by a set of its absolute position and\nrelative position to other points in a local patch, pointNet is then\napplied on the set. And to further capture relationship between\npoints, a modiﬁed Multi-Head Attention (MHA) mechanism is\nused. A new sampling ang grouping techniques with learnable\nparameters were proposed in [42] in a module termed dynamic\npoints agglomeration module(DPAM) which learns an agglom-\neration matrix which when multiplied with incoming poimt fea-\ntures reduces the resolution(similar to sampling) and produce\nan aggregated feature (similar to grouping and pooling).\n4.2.3. Graph based\nGraph based approaches were proposed in [43, 44, 45, 47].\nGraph based approaches represents the point cloud with graph\nstructure by treating each point as a node. Graph structure is\ngood for modelling correlation between points as explicitly rep-\nresented by the graph edges. [43] uses a kd-tree which is a spe-\ncial kind of graph. The kd-tree is built in a top-down manner\non the point cloud to create a feed-forward Kd-network with\nlearnable parameters in each layer. The computation performed\nin the Kd-network is in buttom-up fashion. The leaves repre-\nsents the input points, 2 nearest neighbor (left and right) nodes\nare used to compute their parent node using shared parameters\nof weight matrix and a bias. The Kd-network captures hierar-\nchical representations along the depth of the kd-tree, however,\nbecause of tree design, nodes at the same depth level do not\ncapture overlapping receptive ﬁelds.\n[44, 45, 47] are based on typical graph network G = {V, E}\nwhose vertices V represents the points and edges E represented\nas a V × V matrix.\nIn\n[44] edge convolution is proposed.\nThe graph is represented as a k-nearest neighbor graph over\nthe inputs. In each edge convolution layer, features of each\npoint/vertex are computed by applying a non-linear function on\nits nearest neighbor vertices as captured by the edge matrix E.\nThe non-linear function is a multilayer perceptron (MLP). Af-\nter the last edgeConv layer, global maxpooling is employed to\nobtain a global feature vector similar to [25]. One distinct dif-\nference of [44] from normal graph network is that the edges are\nupdated after each edgeConv layer based on the computed fea-\ntures from the previous layer hence the name Dynamic Graph\nCNN(DGCNN). While there is no resolution reduction as the\nnetwork goes deeper in DGCNN which leads to increased in\ncomputation cost, [45] deﬁned a spectral graph convolution in\nwhich the resolution of the points reduces as the network gets\ndeeper. In each layer, k-nearest neighbor points are sampled,\nbut instead of using mlp-like operation on the the k local points\nsets like in [29], a graph Gk = {V, E} is deﬁned on the sets, the\nvertices V of the graph are the points and the edges E ⊆V × V\nare weight based on the pair-wise distance between the xyz spa-\ntial corrdinates of the points. Graph fourier transform of the\npoints is then computed and ﬁltered using spectral ﬁltering. Af-\nter the ﬁltering, the resolution of the points is still the same,\nclustering, recursive cluster pooling technique is proposed to\naggregate the information in each graph into one vertex.\nIn [47], the authors proposed a graph network that fully ex-\nplore not only the local correlation, but also non local corre-\nlation. The correlation is explored in 3 ways, self correlation\nwhich explores channel-wise correlation of a node’s feature;\nlocal correlation that explore local dependency among nodes\nin a local region; and non-local correlation for capturing better\nglobal feature by considering long-range local features.\nTable 1 summarized the approaches showing there sampling,\ngrouping and mapping function methods.\n5. Benchmark Datasets\nA considerable amount of point cloud datasets has been pub-\nlished in recent years. Most of the existing datasets are provided\nby universities and industries. They can provide a fair compar-\nison for testing diverse approaches. These public benchmark\ndatasets consist of virtual scenes or real scenes, which focus\nparticularly on point cloud classiﬁcation, segmentation, regis-\ntration and object detection. They are notably useful in deep\nlearning since they can provide huge amounts of ground truth\nlabels for training the network. The point cloud is obtained\nby diﬀerent platforms/methods, such as Structure from Motion\n(SfM), Red Green Blue -Depth (RGB-D) cameras, and Light\nDetection And Ranging (LiDAR) systems. The availability of\nbenchmark datasets usually decrease as the size and complexity\nincreases. In this section, we introduce some popular datasets\nfor 3D research.\n5.1. 3D Model Datasets\nModelNet [13]:. This dataset was developed by the Princeton\nVision & Robotics Labs. ModelNet40 has 40 man-made ob-\nject categories (such as airplane, bookshelf and chair) for shape\nclassiﬁcation and recognition. It consists of 12,311 CAD mod-\nels, which has been split into 9,843 training and 2,468 testing\nshapes. ModelNet10 dataset is a subset of ModelNet40 that\nonly contains 10 categories of classes. It is also divided into\n3991 training and 908 testing shapes.\nShapeNet [48]:. The large-scale dataset was developed by\nStanford University et al. It provides semantic category labels\nfor per model. rigid alignments, parts and bilateral symme-\ntry planes, physical sizes, keywords, as well as other planned\nannotations. ShapeNet has indexed almost 3,000,000 models\nwhen the dataset published, and there are 220,000 models has\nbeen classiﬁed into 3,135 categories. ShapeNetCore is a sub-\nset of ShapeNet, which consists of nearly 51,300 unique 3D\nmodels. It provides 55 common object categories and annota-\ntions. ShapeNetSem is also a subset of ShapeNet, which con-\ntains 12,000 models. It is more smaller but covers more exten-\nsive categories of 270.\nAugmenting ShapeNet:.\n[49] has created detailed part labels\nfor 31963 models from ShapeNetCore dataset. It provides 16\nshape categories for part segmentation.\n[50] has provided\n1200 virtual partial models from ShapeNet dataset.\n[51] has\n7\nMethod\nSampling\nGrouping\nMapping Function\nPointNet [25]\n-\n-\nMLP\nPointNet++ [29]\nUniform subsampling\nRadius-search\nMLP\nPointCNN [35]\nUniform/Random sampling\nk-NN\nMLP\nSo-Net [33]\nSOM-Nodes\nRadius-search\nMLP\nPointwise Conv [34]\n-\nRadius-search\nMLP\nKd-Network [43]\n-\nTree based nodes\nAﬃne transformations+ReLU\nDGCNN [44]\n-\nk-NN\nMLP\nLocalSpec [45]\nFarthest point sampling\nk-NN\nSpectral convolution+cluster pooling\nSpiderCNN [41]\nUniform sampling\nk-NN\nTaylor expansion\nR-S CNN [38]\nUniform sampling\nradius-nn\nMLP\nPointConv [37]\nUniform sampling\nradius-nn\nMLP\nPAT [28]\nGumbel subset sampling\nk-NN\nMLP\nA-CNN [40]\nUniform subsampling\nk-NN\nMLP+density functions\nShellNet [46]\nRandom Sampling\nSpherical Shells\n1D convolution\nTable 1: Summary of methods showing sampling, grouping and the mapping function used\nproposed an approach for automatically generating photoreal-\nistic materials for 3D shapes. It is built on the ShapeNetCore\ndataset. [52] is a large-scale dataset with ﬁne-grained and hier-\narchical part annotations. It consists of 24 object categories and\n26,671 3D models, which provides 573,585 part instance labels.\n[53] has contributed a large-scale dataset for 3D object recogni-\ntion. There are 100 categories of the dataset, which consists of\n90,127 images with 201,888 objects (from ImageNet [54]) and\n44,147 3D shapes (from ShapeNet).\nShape2Motion [55]:. Shape2Motion was developed by Bei-\nhang University and National University of Defense Technol-\nogy.\nIt has created a new benchmark dataset for 3D shape\nmobility analysis. The benchmark consists of 45 shape cate-\ngories with 2440 models where the shapes are obtained from\nShapeNet and 3D Warehouse [56]. The proposed approach in-\nputs a single 3D shape, then predicts motion part segmentation\nresults and motion corresponding attributes jointly.\nScanObjectNN [57]:. ScanObjectNN was developed by Hong\nKong University of Science and Technology et al. It is the ﬁrst\nreal-world dataset for point cloud classiﬁcation. About 15,000\nobjects are selected from indoor datasets (SceneNN [58] and\nScanNet [30]).\nAnd the objects are split into 15 categories\nwhere there are 2902 unique object instances.\n5.2. 3D Indoor Datasets\nNYUDv2 [59]. The New York University Depth Dataset v2\n(NYUDv2) was developed by the New York University et al.\nThe dataset provided 1449 RGB-D (obtained by Kinect v1) im-\nages captured from 464 various indoor scenes. All of the im-\nages are distributed segmentation labels. This dataset is mainly\nserved for understanding how 3D cues can lead to better seg-\nmentation for indoor objects.\nSUN3D [60]. This dataset was developed by the Princeton\nUniversity. It is a RGB-D video dataset where the videos are\ncaptured from 254 diﬀerent spaces in 41 buildings. SUN3D\nprovides 415 sequences with camera pose and object la-\nbels. The point cloud is generated by structure from motion\n(SfM).\nS3DIS [61]. Stanford 3D Large-Scale Indoor Spaces (S3DIS)\nwas developed by the Stanford University et al. S3DIS was\ncollected from 3 diﬀerent buildings with 271 rooms where the\ncover area is above 6,000m2.\nIt contains over 215 million\npoints, and each point has the provision of instance-level se-\nmantic segmentation labels (13 categories).\nSceneNN [58]. Singapore University of Technology and De-\nsign et al. developed this dataset. SceneNN is an RGB-D (ob-\ntained by Kinect v2) scene dataset collected form 101 indoor\nscenes.\nIt provides 40 semantic classes for the indoor scenes, and all\nsemantic labels are same as NYUDv2 dataset.\nScanNet [30]. ScanNet is a large-scal indoor dataset developed\nby Stanford University et al. It contains 1513 scanned scenes,\nincluding nearly 2.5M RGB-D (obtained by Occipital Struc-\nture Sensor) images from 707 diﬀerent indoor environments.\nThe dataset provides ground truth labels for 3D object classi-\nﬁcation with 17 categories and semantic segmentation with 20\ncategories.\nFor object classiﬁcation, ScanNet divides all instances into\n9,677 instances for training and 2,606 instances for testing. And\nScanNet splits all scans into 1201 training scenes and 312 test-\ning scenes for semantic segmentation.\nMatterport3D [62]. Matterport3D is the largest indoor dataset\nwhich developed by Princeton University et al. The cover area\nof this dataset is 219,399mm2 from 2056 rooms, and there is\n8\n46,561mm2 of ﬂoor space.\nIt consists of 10,800 panoramic\nviews where the views are from 194,400 RGB-D images of 90\nlarge-scale buildings. The labels contain surface reconstruc-\ntions, camera poses, and semantic segmentation. This dataset\ninvestigates 5 tasks for scene understanding, which are keypoint\nmatching, view overlap prediction, surface normal estimation,\nregion-type classiﬁcation, and semantic segmentation.\n3DMatch [63]. This benchmark dataset is developed by\nPrinceton University et al.\nIt is a large collection of exist-\ning datasets, such as Analysisby-Synthesis [64], 7-Scenes [65],\nSUN3D [60], RGB-D Scenes v.2 [66] and Halber et al. [67].\n3DMatch benchmark consists of 62 scenes with 54 training\nscenes and 8 testing scenes. It leverages correspondence labels\nfrom RGB-D scene reconstruction datasets, and then provides\nground truth labels for point cloud registration.\nMultisensor Indoor Mapping and Positioning Dataset [68].\nThis indoor dataset (rooms, corridor and indoor parking lots)\nwas developed by Xiamen University et al. The data was ac-\nquired by multi-sensors, such as laser scanner, camera, WIFI,\nBluetooth, and IMU. This dataset provides dense laser scanning\npoint cloud for indoor mapping and positioning. Meanwhile,\nthey also provide colored laser scans based on multi-sensor cal-\nibration and SLAM-mapping process.\n5.3. 3D Outdoor Datasets\nKITTI [69] [70]. The KITTI dataset is one of the best known\nin the ﬁeld of autonomous driving which was developed by\nKarlsruhe Institute of Technology et al.\nIt can be used for\nthe research of stereo image, optical ﬂow estimation, 3d de-\ntection, 3d tracking, visual odometry and so on.\nThe data\nacquisition platform is equipped with two color cameras, two\ngrayscale cameras, a Velodyne HDL-64E 3D laser scanner and\na high-precision GPS/IMU system. KITTI provides raw data\nwith ﬁve categories of Road, City, Residential, Campus and\nPerson. Depth completion and prediction benchmark consists\nof more than 93 thousand depth maps. 3D object detection\nbenchmark contains 7481 training point clouds and 7518 test-\ning point clouds. Visual odometry benchmark is formed by 22\nsequences, with 11 sequences (00-10) LiDAR data for training\nand 11 sequences (11-21) LiDAR data for testing. Meanwhile,\na semantic labeling [71] for Kitti odometry dataset is published\nrecently. SemanticKITTI contains 28 classes including ground,\nstructure, vehicle, nature, human, object, and others.\nASL Dataset [72]. This group of datasets was developed by\nETH Zurich. The dataset was collected between August 2011\nto January 2012. It provides 8 point cloud sequences acquired\nby a Hokuyo UTM-30LX. Each sequences has around 35 scan-\nning point clouds and the ground truth pose is supported by\nGPS/INS systems. This dataset covers the area of structured\nand unstructured environments.\niQmulus [73]. The large-scale urban scene dataset was devel-\noped by Mines ParisTech et al in January 2013. The entire 3D\npoint cloud has been classiﬁed and segmented into 50 classes.\nThe data was collected by StereopolisII MLS, a system devel-\noped by French National Mapping Agency (IGN). They use\nRiegl LMS-Q120i sensor to acquire 300 million points.\nOxford Robotcar [74]. This dataset was developed by the Uni-\nversity of Oxford. It consists of around 100 times trajectories\n(a total of 101,046km trajectories) through central Oxford be-\ntween May 2014 to December 2015. This long-term dataset\ncaptures many challenging environment changes including sea-\nson, weather, traﬃc, and so on. And the dataset provides both\nimages, LiDAR point cloud, GPS and INS ground truth for au-\ntonomous vehicles. The LIDRA data were collected by two\nSICK LMS-151 2D LiDAR scanners and one SICK LD-MRS\n3D LIDAR scanner.\nNCLT [75]. It was developed by the University of Michigan. It\ncontains 27 times trajectories through the University of Michi-\ngans North Campus between January 2012 to April 2013. This\ndataset also provides images, LiDAR, GPS and INS ground\ntruth for long-term autonomous vehicles. The LiDRA point\ncloud was collected by a Velodyne-32 LiDAR scanner.\nSemantic3D [76]. The high quality and density dataset was de-\nveloped by ETH Zurich. It contains more than four billion of\npoints where the point cloud are acquired by static terrestrial\nlaser scanners. There are 8 semantic classes provided, which\nconsist of man-made terrain, natural terrain, high vegetation,\nlow vegetation, buildings, hard scape, scanning artefacts and\ncars. And the dataset is split into 15 training scenes and 15\ntesting scenes.\nDBNet [77]. This real-world LiDAR-Video dataset was devel-\noped by Xiamen University et al. It aims at learning driving\npolicy, since it is diﬀerent from previous outdoor datasets. DB-\nNet provides LiDAR point cloud, video record, GPS and driver\nbehaviors for driving behavior study. It contains 1,000 km driv-\ning data captured by a Velodyne laser.\nNPM3D [78]. The Nuage de Points et Modlisation 3D\n(NPM3D) dataset was developed by PSL Research University.\nIt is a benchmark for point cloud classiﬁcation and segmenta-\ntion, and all point cloud has been labeled to 50 diﬀerent classes.\nIt contains 1,431 M points data collected in Paris and Lille. The\ndata was acquired by a Mobile Laser System including a Velo-\ndyne HDL-32E LiDAR and GPS/INS systems.\nApollo [79] [80]. The Apollo was developed by Baidu Re-\nsearch et al and it is a large-scale autonomous driving dataset. It\nprovides labeled data of 3D car instance understanding, LiDAR\npoint cloud object detection and tracking, and LiDAR-based\nlocalization. For 3D car instance understanding task, there are\n5,277 images with more than 60K car instances. Each car has\nan industry-grade CAD model. 3D object detection and track-\ning benchmark dataset contains 53 minutes sequences for train-\ning and 50 minutes sequences for testing. It is acquired at the\nframe rate of 10fps/sec and labeled at the frame rate of 2fps/sec.\nThe Apollo-SouthBay dataset provides LiDAR frames data for\nlocalization. It was collected in southern San Francisco Bay\nArea. They equip a high-end autonomous driving sensor suite\n(Velodyne HDL-64E, NovAtel ProPak6, and IMU-ISA-100C)\non a standard Lincoln MKZ sedan.\nnuScenes [81]. The nuTonomy scenes (nuScenes) dataset pro-\nposes a novel metric for 3D object detection which was devel-\noped by nuTonomy (an APTIV company). The metric consists\n9\nof multi-aspects, which are classiﬁcation, velocity, size, local-\nization, orientation, and attribute estimation of the object. This\ndataset was acquired by an autonomous vehicle sensor suite (6\ncameras, 5 radars and 1 lidar) with 360 degree ﬁeld of view. It\ncontains 1000 driving scenes collected from Boston and Singa-\npore, where the two cities are both traﬃc-clogged. The objects\nin this dataset have 23 classes and 8 attributes, and they are all\nlabeled with 3D bounding boxes.\nBLVD [82]. This dataset was developed by Xian Jiaotong Uni-\nversity and it was collected in Changshu (China). It introduces a\nnew benchmark which focuses on dynamic 4D object tracking,\n5D interactive event recognition and 5D intention prediction.\nBLVD dataset consists of 654 video clips, where the videos are\n120k frames and the frame rate is 10fps/sec. All frames are\nannotated to obtain 249,129 3D annotations. There are totally\n4,902 unique objects for tracking, 6,004 fragments for interac-\ntive event recognition, and 4,900 objects for intention predic-\ntion.\n6. Application of deep learning in 3D vision tasks\nIn this section we discussed the application of the methods\ndiscussed in section 4 in 3 popular 3D vision tasks namely:\nclassiﬁcation, segmentation and object detection.\nSee ﬁg-\nure 8. We review the performance of the methods on pop-\nular benchmark datasets, Modelnet40 dataset [13] for clas-\nsiﬁcation, ShapeNet [48] and Stanford 3D Indoor Semantics\nDataset(S3DIS) [61] datasets for parts and semantic segmenta-\ntion respectively.\n6.1. Classiﬁcation\nObject classiﬁcation has been one of the primary areas for\nwhich deep learning is used. In object classiﬁcation the ob-\njective is: giving a point cloud, a network should classify it into\na certain category. Classiﬁcation is the pioneering task in deep\nlearning because early breakthrough deep learning models such\nas AlexNet [83], VGGNet [84], and ResNet [85] are classiﬁca-\ntion models. In point cloud, most early techniques for classiﬁ-\ncation using deep learning relied on a structured grid, section 3,\nhowever, we limit ourself to only approaches that process point\ncloud directly.\nThe features learned by the techniques reviewed in both section\n4 and 3 can easily be used for classiﬁcation task by passing\nthem through a fully connected network whose last layer repre-\nsents classes. Other machine learning classiﬁers such as SVM\ncan also be used as in [9, 86]. In ﬁgure 9 a timeline perfor-\nmance of point based deep learning approaches on modelnet40\nis shown.\n6.2. segmentation\nSegmentation of point cloud is the grouping of points into\nhomegenous regions. Traditionally, segmentation is done us-\ning edges [87] or surface properties such as normals, curvature\nand orientation [87, 88]. Recently, feature based deep learn-\ning approaches are used for point cloud segmentation with the\ngoal of segmenting the points into diﬀerent aspects. The aspects\ncould be diﬀerent parts of an object, referred to as part segmen-\ntation or diﬀerent class categories, also referred to as semantic\nsegmentation.\nIn parts segmentation, the input point cloud represent a certain\nobject and the goal is to assign each point to a certain parts\nas shown in ﬁgure 8b, hence the name ”part” segmentation.\nIn [25, 33, 44] the global descreptor learned is concateneated\nwith the features of the points and then passed through MLP\nto classify each point into a part category. [29, 35] propagates\nthe global descreptor into high resolution predictions using in-\nterpolation and deconvolution methods respectively. In [34]\nthe per point features learned are used to achieve segmentation\nby passing them through dense convolutional layers. Encoder-\ndecoder architecture is used in [43] for both parts and semantic\nsegmenatation. In table 8b the result of various techniques on\nShapeNet parts datasets are shown.\nIn semantic segmentation, the goal is to assign each point to a\nparticular class. For example, in ﬁgure 8d, the points belonging\nto chair are shown in red, while ceiling and ﬂoor in green and\nblue respectively, e.t.c. Popular public datasets for Semantic\nsegmentation evaluation are S3DIS [61] and ScanNet [30]. We\nshow in table 4 performances of some of the state-of-the-arts\nmethods on S3DIS and ScanNet datasets.\nInstance segmentation on point cloud recieves less attention\ncompared to part and semantic segmentation. Instance segmen-\ntation is when the grouping is based on instances where mul-\ntiple objects of the same class are uniquely identiﬁed. Some\nstate-of-the-art works on instance segmentation on point cloud\nare [89, 90, 91, 92, 93] which are built on PointNet/PointNet++\nfeature learning backbone.\nTable 8b shows performances of the methods discussed 4 on\nthe popular ShapeNet datasets.\n6.3. Object detection\nObject detection is an extension of classiﬁcation where multi-\nple objects can be recognized and each object is localized us-\ning a bounding box as shown in ﬁgure 8c. RCNN [96] were\nthe ﬁrst that proposed 2D object detection by selective search,\nwhere diﬀerent regions are selected and passed to the network\none at a time. Several variants were later proposed [97, 98, 99].\nOther state-of-the-art 2D object detection is YOLO [100] and\nits variants such as [101, 102]. In summary, 2D object detec-\ntion is based on 2 major stages, region proposals and classiﬁca-\ntion.\nLike in 2D images, detection in 3D point cloud is also emperical\non the two stages of proposal and classiﬁcation. Proposal stage\nin 3D point cloud, however, is more challenging than in 2D\ndue to the search space being 3 dimensional and the sliding\nwindow or region to be proposed is also in 3 dimension. vote3D\n[103] and vote3Deep [104] convert input point cloud into a\nstructured grid and perform extensive sliding window operation\n10\nModel\nIndoor\nOutdoor\nCAD\nModelNet\n(2015,\ncls),\nShapeNet\n(2015,\nseg),\nAugmenting\nShapeNet,\nShape2Motion\n(2019, seg, mot)\nRGB-D\nScanObjectNN (2019, cls)\nNYUDv2 (2012, seg),\nSUN3D (2013, seg),\nS3DIS (2016, seg),\nSceneNN (2016, seg),\nScanNet (2017, seg),\nMatterport3D (2017, seg),\n3DMatch (2017, reg)\nLiDAR\nterrestrial LiDAR scanning\nSemantic3D (2017, seg)\nmobile LiDAR scan-\nning\nMultisensor\nIndoor\nMap-\nping and Positioning Dataset\n(2018, loc)\nKITTI (2012, det, odo),\nSemantic\nKITTI\n(2019,\nseg),\nASL Dataset (2012, reg),\niQmulus (2014, seg),\nOxford\nRobotcar\n(2017,\naut),\nNCLT (2016, aut),\nDBNet (2018, dri),\nNPM3D (2017, seg),\nApollo (2018, det, loc),\nnuScenes (2019, det, aut),\nBLVD (2019, det)\nTable 2: categorization of benchmark datasets. (cls: classiﬁcation, seg: segmentation, loc: localization, reg: registration, aut: autonomous driving, det: object\ndetection, dri: driving behavior, mot: motion estimation, odo: odometry, )\nMethod\nScore\nPointNet [25]\n83.7%\nPointCNN [35]\n84.6%\nSo-Net [33]\n84.6%\nPointConv [37]\n85.7%\nKd-Network [43]\n82.3%\nDGCNN [44]\n85.2%\nLocalSpec [45]\n85.4%\nSpiderCNN [41]\n85.3%\nR-S CNN [38]\n86.1%\nA-CNN [40]\n86.1%\nShellNet [46]\n82.8%\nInterpCNN [94]\n84.0%\nDensePoint [95]\n84.2%\nTable 3: Part segmentation on ShapeNet part dataset. The score is the mean\nIntersection Over Union(mIOU)\nfor detection which is computationally expensive. To perform\nobject detection directly in point cloud, several techniques used\nfeature learning techniques discussed in section 4.\nIn VoxelNet\n[31], the sparse 4D feature vectore is passed\nthrough a region proposal network to generate 3D detection.\nFrustumNet [105] proposed regions in 2D and obtain the 3D\nfrustrum of the region from the point cloud and pass it through\nPointNet to predict 3D bouding box.\n[89] ﬁrst uses Point-\nNet/PointNet++ to obtain feature vector of each point, and\nbased on the hypothesis that points belonging to the same object\nare closer in feature space proposed a similarity matrix which\npredicts if a given pair of points belong to the same object. In\n[106], PointNet and PointNet++ are used to designed a gener-\native shape proposal network to generate proposals which are\nfurther processed using PointNet for classiﬁcation and segmen-\ntation. PointNet++ is used in [107] to learn point-wise fea-\ntures which are used to segment foreground points from back-\ngroud points and employs buttom-up 3D proposal to generate\n3D box proposals from the foreground points. The 3D box pro-\nposals are further reﬁned using another PointNet++-like struc-\nture. [108] used PointNet++ to learn point wise features which\n11\n(a) Object classiﬁcation\n(b) Parts segmentation\n(c) Object detection\n(d) Semantic segmentation [25]\nFigure 8: Deep learning tasks on point cloud. (a) Object classiﬁcation (b) Parts segmentation (c) Object detection (d) Semantic segmentation (Best viewed with\ncolor)\nare considered to be seeds. The seeds then independently cast a\nvote using a hough voting module based on MLP. The votes of\nthe same object are close in space hence allow for easy cluster-\ning. The clusters are further processed using a shared PointNet-\nlike module for vote aggregation and propsal. PointNet is also\nutilized in [109] with Single Short Detector (SSD) [110] for\nobject detection.\nOne of the popular object detection dataset is the Kitti dataset\n[69, 70]. The evaluation on kitti is divided into easy, moderate\nand hard depending on occlusion level, minimum height of the\nbounding box and maximum truncation. We report the perfor-\nmance of various object detection methods on Kitti dataset in\ntables 5 and 6.\n7. Summary and Conclusion\nThe increasing availability of point cloud as a result of evolv-\ning scanning devices coupled with increasing application in au-\ntonomous vehicles, robotics, AR and VR demands for fast and\neﬃcient algorithms for the point cloud processing inorder to\nachieve improved visual perception such as recognition, seg-\nmentation and detection. Due to scarse data availability, un-\npopularity of deep learning, early methods for point cloud pro-\ncessing relied on handcrafted features. However, with the rev-\nolution brought about by deep learning in 2D vision tasks, and\nevolution of acquisition devices of point cloud which leads to\navailability of point cloud data, computer vision community are\nfocusing on how to utilize the power of deep learning on point\ncloud data. Point cloud provides more accurate 3D information\n12\nFigure 9: Timeline of classiﬁcation accuracy of ModelNet40\nwhich is vital in applications that require 3D information. Due\nto the nature of point cloud, its very challenging to use deep\nlearning for its processing. Most approaches resolve to convert\nthe point cloud into a structured grid for easy processing by\ndeep neural networks. These approaches, however, leads to ei-\nther loss of depth information or introduces conversion artifacts\nand requires higher computational cost. Recently, deep learn-\ning directly on point cloud is recieving alot of attention. Learn-\ning on point cloud directly do away with convertion artifacts\nand mitigates the need for higher computational cost. Point-\nNet is the basic deep learning method that process point cloud\ndirectly. PointNet however, does not capture local structures.\nMany approaches were developed to improve on pointNet by\ncapturing local structures. Inorder to capture local structures,\nmost methods follows three basic steps; sampling to reduce the\nresolution of points and to get centroids for representing lo-\ncal neighborhood; grouping, based on K-NN to select neigh-\nboring points to each centroids; mapping function, usually ap-\nproximated by an MLP, which learn the representation of neigb-\nhoring points. Several methods resolves to approximating the\nMLP with PointNet-like network. However because PointNet\ndoes not explore inter points relationship, several approaches\nexplore inter-points relationships within a local patch before\napplying pointNet like MLPs. Taking into account the point-\nto-point relationship between points has proven to increase the\ndiscriminative capability of the networks.\nWhile deep learning on 3D point cloud has shown good per-\nformance on several tasks including classiﬁcation, parts and se-\nmantic segmentation, other areas, however, are recieving less\nattention. Instance segmentation on 3D point cloud, where in-\ndividual objects are segmented in a scene, remain largely un-\ncharted direction. Most current object detection relies on 2D\ndetection for region proposal, few works are available on de-\ntecting objects directly on point cloud. Scaling to larger scene\nalso remain largely unexploited as most of the current works re-\nlies on cutting large scenes into smaller pieces. As at the time of\nthis review, only few works [120, 121] explored deep learning\non large scale 3D scene.\nReferences\n[1] A. E. Johnson, M. Hebert, Using spin images for eﬃcient\nobject recognition in cluttered 3d scenes, IEEE Trans.\nPattern Anal. Mach. Intell. 21 (1999) 433–449. URL:\nhttps://doi.org/10.1109/34.765655. doi:10.\n1109/34.765655.\n[2] H. Chen, B. Bhanu, 3d free-form object recognition in\nrange images using local surface patches, in: 17th Inter-\nnational Conference on Pattern Recognition, ICPR 2004,\nCambridge, UK, August 23-26, 2004., IEEE Computer\nSociety, 2004, pp. 136–139. URL: https://doi.org/\n10.1109/ICPR.2004.1334487. doi:10.1109/ICPR.\n2004.1334487.\n[3] Y. Zhong, Intrinsic shape signatures: A shape descrip-\ntor for 3d object recognition,\nin: 12th IEEE Inter-\nnational Conference on Computer Vision Workshops,\nICCV Workshops 2009, Kyoto, Japan, September 27\n- October 4, 2009, IEEE Computer Society, 2009, pp.\n689–696. URL: https://doi.org/10.1109/ICCVW.\n2009.5457637. doi:10.1109/ICCVW.2009.5457637.\n[4] R. B. Rusu, N. Blodow, Z. C. Marton, M. Beetz,\nAligning point cloud views using persistent feature his-\n13\nMethod\nDatasets\nMeasure\nScore\nPointNet [25]\nS3DIS\nmIOU\n47.71%\nPointwise Conv [34]\n56.1%\nDGCNN [44]\n56.1%\nPointCNN [35]\n65.39%\nPAT [28]\n54.28%\nShellNet [46]\n66.8%\nPoint2Node [47]\n70.0%\nInterpCNN [94]\n66.7%\nPointNet [25]\nOA\n78.5%\nPointCNN [35]\n88.1%\nDGCNN [44]\n84.1%\nA-CNN [40]\n87.3%\nJSIS3D [92]\n87.4%\nPointNet++ [29]\nScanNet\nmIOU\n55.7%\nPointNet [25]\n33.9%\nPointConv [37]\n55.6%\nPointCNN [35]\n45.8%\nPointNet [25]\nOA\n73.9%\nPointCNN [35]\n85.1%\nA-CNN [40]\n85.4%\nLocalSpec [45]\n85.4%\nPointNet++ [29]\n84.5%\nShellNet [46]\n85.2%\nPoint2Node [47]\n86.3%\nTable 4: Semantic segmentation on S3DIS and ScanNet datasets. mIOU stands for mean Intersection Over Union and OA stands for Overall Accuracy\ntograms,\nin:\n2008 IEEE/RSJ International Confer-\nence on Intelligent Robots and Systems, September 22-\n26, 2008, Acropolis Convention Center, Nice, France,\nIEEE, 2008, pp. 3384–3391. URL: https://doi.org/\n10.1109/IROS.2008.4650967. doi:10.1109/IROS.\n2008.4650967.\n[5] R. B. Rusu, N. Blodow, M. Beetz,\nFast point feature\nhistograms (FPFH) for 3d registration, in: 2009 IEEE\nInternational Conference on Robotics and Automation,\nICRA 2009, Kobe, Japan, May 12-17, 2009, IEEE, 2009,\npp. 3212–3217. URL: https://doi.org/10.1109/\nROBOT.2009.5152473. doi:10.1109/ROBOT.2009.\n5152473.\n[6] F. Tombari, S. Salti, L. di Stefano, Unique shape context\nfor 3d data description, in: M. Daoudi, M. Spagnuolo,\nR. C. Veltkamp (Eds.), Proceedings of the ACM work-\nshop on 3D object retrieval, 3DOR ’10, Firenze, Italy,\nOctober 25, 2010, ACM, 2010, pp. 57–62. URL: https:\n//doi.org/10.1145/1877808.1877821. doi:10.\n1145/1877808.1877821.\n[7] A. E. Johnson, M. Hebert, Comparison of 3d interest\npoint detectors and descriptors for point cloud fusion,\nISPRS Annals of the Photogrammetry, Remote Sens-\ning and Spatial Information Sciences 2 (2014). URL:\nhttps://doi.org/10.1109/34.765655.\n[8] D. Maturana, S. Scherer, 3d convolutional neural net-\nworks for landing zone detection from lidar, in: IEEE\nInternational Conference on Robotics and Automation,\nICRA 2015, Seattle, WA, USA, 26-30 May, 2015,\nIEEE, 2015, pp. 3471–3478. URL: https://doi.org/\n10.1109/ICRA.2015.7139679. doi:10.1109/ICRA.\n2015.7139679.\n[9] D. Maturana, S. Scherer, Voxnet: A 3d convolutional\nneural network for real-time object recognition,\nin:\n2015 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, IROS 2015, Hamburg, Germany,\nSeptember 28 - October 2, 2015, IEEE, 2015, pp. 922–\n928. URL: https://doi.org/10.1109/IROS.2015.\n7353481. doi:10.1109/IROS.2015.7353481.\n[10] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, L. J.\nGuibas, Volumetric and multi-view cnns for object clas-\nsiﬁcation on 3d data,\nin: 2016 IEEE Conference on\n14\nMethod\nModality\nSpeed(HZ)\nmAP\nCar\nPedestrian\nCyclist\nModerate\nEasy\nModerate\nHard\nEasy\nModerate\nHard\nEasy\nModerate\nHard\nMV3D [111]\nLiDAR & Image\n2.8\nN/A\n86.02\n76.9\n68.49\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nCont-Fuse [112]\nLiDAR & Image\n16.7\nN/A\n88.81\n85.83\n77.33\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nRoarnet [113]\nLiDAR & Image\n10\nN/A\n88.2\n79.41\n70.02\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nAVOD-FPN [114]\nLiDAR & Image\n10\n64.11\n88.53\n83.79\n77.9\n58.75\n51.05\n47.54\n68.09\n57.48\n50.77\nF-PointNet [105]\nLiDAR & Image\n5.9\n65.39\n88.7\n84\n75.33\n58.09\n50.22\n47.2\n75.38\n61.96\n54.68\nHDNET [115]\nLiDAR & Map\n20\nN/A\n89.14\n86.57\n78.32\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nPIXOR++ [116]\nLiDAR\n35\nN/A\n89.38\n83.7\n77.97\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nVoxelNet [31]\nLiDAR\n4.4\n58.52\n89.35\n79.26\n77.39\n46.13\n40.74\n38.11\n66.7\n54.76\n50.55\nSECOND [117]\nLiDAR\n20\n60.56\n88.07\n79.37\n77.95\n55.1\n46.27\n44.76\n73.67\n56.04\n48.78\nPointRCNN [118]\nLiDAR\nN/A\nN/A\n89.28\n86.04\n79.02\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nPointPillars [119]\nLiDAR\n62\n66.19\n88.35\n86.1\n79.83\n58.66\n50.23\n47.19\n79.14\n62.25\n56\nTable 5: Performance on the KITTI Birds Eye View detection benchmark\nMethod\nModality\nSpeed(HZ)\nmAP\nCar\nPedestrian\nCyclist\nModerate\nEasy\nModerate\nHard\nEasy\nModerate\nHard\nEasy\nModerate\nHard\nMV3D [111]\nLiDAR & Image\n2.8\nN/A\n71.09\n62.35\n55.12\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nCont-Fuse [112]\nLiDAR & Image\n16.7\nN/A\n82.54\n66.22\n64.04\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nRoarnet [113]\nLiDAR & Image\n10\nN/A\n83.71\n73.04\n59.16\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nAVOD-FPN [114]\nLiDAR & Image\n10\n55.62\n81.94\n71.88\n66.38\n50.8\n42.81\n40.88\n64\n52.18\n46.64\nF-PointNet [105]\nLiDAR & Image\n5.9\n57.35\n81.2\n70.39\n62.19\n51.21\n44.89\n40.23\n71.96\n56.77\n50.39\nVoxelNet [31]\nLiDAR\n4.4\n49.05\n77.47\n65.11\n57.73\n39.48\n33.69\n31.5\n61.22\n48.36\n44.37\nSECOND [117]\nLiDAR\n20\n56.69\n83.13\n73.66\n66.2\n51.07\n42.56\n37.29\n70.51\n53.85\n46.9\nPointRCNN [118]\nLiDAR\nN/A\nN/A\n84.32\n75.42\n67.86\nN/A\nN/A\nN/A\nN/A\nN/A\nN/A\nPointPillars [119]\nLiDAR\n62\n59.2\n79.05\n74.99\n68.3\n52.08\n43.53\n41.49\n75.78\n59.07\n52.92\nTable 6: Performance on the KITTI 3D object detection benchmark\n15\nComputer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV, USA, June 27-30, 2016, IEEE Computer\nSociety, 2016, pp. 5648–5656. URL: https://doi.\norg/10.1109/CVPR.2016.609. doi:10.1109/CVPR.\n2016.609.\n[11] C. Wang, M. Cheng, F. Sohel, M. Bennamoun, J. Li,\nNormalnet: A voxel-based CNN for 3d object classiﬁca-\ntion and retrieval, Neurocomputing 323 (2019) 139–147.\nURL: https://doi.org/10.1016/j.neucom.2018.\n09.075. doi:10.1016/j.neucom.2018.09.075.\n[12] S. Ghadai, X. Y. Lee, A. Balu, S. Sarkar, A. Krishna-\nmurthy,\nMulti-resolution 3d convolutional neural net-\nworks for object recognition,\nCoRR abs/1805.12254\n(2018). URL: http://arxiv.org/abs/1805.12254.\narXiv:1805.12254.\n[13] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang,\nJ. Xiao, 3d shapenets: A deep representation for vol-\numetric shapes,\nin: IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2015, Boston,\nMA, USA, June 7-12, 2015, IEEE Computer Soci-\nety, 2015, pp. 1912–1920. URL: https://doi.org/\n10.1109/CVPR.2015.7298801. doi:10.1109/CVPR.\n2015.7298801.\n[14] G. E. Hinton, S. Osindero, Y. W. Teh,\nA fast learn-\ning algorithm for deep belief nets, Neural Computation\n18 (2006) 1527–1554. URL: https://doi.org/10.\n1162/neco.2006.18.7.1527. doi:10.1162/neco.\n2006.18.7.1527.\n[15] G. Riegler, A. O. Ulusoy, A. Geiger,\nOctnet: Learn-\ning deep 3d representations at high resolutions,\nin:\n2017 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July 21-\n26, 2017, 2017, pp. 6620–6629. URL: https://doi.\norg/10.1109/CVPR.2017.701. doi:10.1109/CVPR.\n2017.701.\n[16] M. Tatarchenko, A. Dosovitskiy, T. Brox, Octree gen-\nerating networks: Eﬃcient convolutional architectures\nfor high-resolution 3d outputs, in: IEEE International\nConference on Computer Vision, ICCV 2017, Venice,\nItaly, October 22-29, 2017, 2017, pp. 2107–2115.\nURL: https://doi.org/10.1109/ICCV.2017.230.\ndoi:10.1109/ICCV.2017.230.\n[17] H. Su, S. Maji, E. Kalogerakis, E. G. Learned-Miller,\nMulti-view convolutional neural networks for 3d shape\nrecognition, in: 2015 IEEE International Conference on\nComputer Vision, ICCV 2015, Santiago, Chile, Decem-\nber 7-13, 2015, IEEE Computer Society, 2015, pp. 945–\n953. URL: https://doi.org/10.1109/ICCV.2015.\n114. doi:10.1109/ICCV.2015.114.\n[18] B. Leng, S. Guo, X. Zhang, Z. Xiong,\n3d object\nretrieval with stacked local convolutional autoencoder,\nSignal Processing 112 (2015) 119–128. URL: https://\ndoi.org/10.1016/j.sigpro.2014.09.005. doi:10.\n1016/j.sigpro.2014.09.005.\n[19] S. Bai, X. Bai, Z. Zhou, Z. Zhang, L. J. Latecki,\nGIFT: A real-time and scalable 3d shape search en-\ngine, in: 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, Las Vegas, NV,\nUSA, June 27-30, 2016, IEEE Computer Society, 2016,\npp. 5023–5032. URL: https://doi.org/10.1109/\nCVPR.2016.543. doi:10.1109/CVPR.2016.543.\n[20] E. Kalogerakis, M. Averkiou, S. Maji, S. Chaudhuri, 3d\nshape segmentation with projective convolutional net-\nworks,\nin: 2017 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2017, Honolulu,\nHI, USA, July 21-26, 2017, 2017, pp. 6630–6639.\nURL: https://doi.org/10.1109/CVPR.2017.702.\ndoi:10.1109/CVPR.2017.702.\n[21] Z. Cao, Q. Huang, K. Ramani, 3d object classiﬁcation\nvia spherical projections, in: 2017 International Confer-\nence on 3D Vision, 3DV 2017, Qingdao, China, October\n10-12, 2017, IEEE Computer Society, 2017, pp. 566–\n574. URL: https://doi.org/10.1109/3DV.2017.\n00070. doi:10.1109/3DV.2017.00070.\n[22] L. Zhang, J. Sun, Q. Zheng,\n3d point cloud recogni-\ntion based on a multi-view convolutional neural network,\nSensors 18 (2018) 3681. URL: https://doi.org/10.\n3390/s18113681. doi:10.3390/s18113681.\n[23] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis,\nM. Yang, J. Kautz,\nSplatnet: Sparse lattice networks\nfor point cloud processing, in: 2018 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2018, Salt Lake City, UT, USA, June 18-22, 2018, IEEE\nComputer Society, 2018, pp. 2530–2539. URL: http:\n//openaccess.thecvf.com/content_cvpr_2018/\nhtml/Su_SPLATNet_Sparse_Lattice_CVPR_2018_\npaper.html. doi:10.1109/CVPR.2018.00268.\n[24] Y. Rao, J. Lu, J. Zhou,\nSpherical fractal convolu-\ntional neural networks for point cloud recognition, in:\nThe IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019.\n[25] C. R. Qi, H. Su, K. Mo, L. J. Guibas, Pointnet: Deep\nlearning on point sets for 3d classiﬁcation and segmenta-\ntion, in: 2017 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2017, Honolulu, HI, USA,\nJuly 21-26, 2017, IEEE Computer Society, 2017, pp. 77–\n85. URL: https://doi.org/10.1109/CVPR.2017.\n16. doi:10.1109/CVPR.2017.16.\n[26] M. Oster, R. J. Douglas, S. Liu,\nComputation with\nspikes in a winner-take-all network, Neural Computation\n21 (2009) 2437–2465. URL: https://doi.org/10.\n1162/neco.2009.07-08-829. doi:10.1162/neco.\n2009.07-08-829.\n16\n[27] C. Xiang, C. R. Qi, B. Li,\nGenerating 3d adversarial\npoint clouds, in: The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019.\n[28] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, Q. Tian,\nModeling point clouds with self-attention and gumbel\nsubset sampling, in: The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019.\n[29] C. R. Qi, L. Yi, H. Su, L. J. Guibas,\nPointnet++:\nDeep hierarchical feature learning on point sets in\na metric space,\nin:\nI. Guyon, U. von Luxburg,\nS. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vish-\nwanathan, R. Garnett (Eds.), Advances in Neural In-\nformation Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, 4-\n9 December 2017, Long Beach, CA, USA, 2017, pp.\n5099–5108. URL: http://papers.nips.cc/paper/\n7095- pointnet- deep- hierarchical- feature-\nlearning-on-point-sets-in-a-metric-space.\n[30] A.\nDai,\nA.\nX.\nChang,\nM.\nSavva,\nM.\nHalber,\nT. Funkhouser, M. Nießner, Scannet: Richly-annotated\n3d reconstructions of indoor scenes, in: Proc. Computer\nVision and Pattern Recognition (CVPR), IEEE, 2017.\n[31] Y. Zhou, O. Tuzel, Voxelnet: End-to-end learning for\npoint cloud based 3d object detection, in: 2018 IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2018, Salt Lake City, UT, USA, June 18-22,\n2018, IEEE Computer Society, 2018, pp. 4490–4499.\nURL: http://openaccess.thecvf.com/content_\ncvpr_2018/html/Zhou_VoxelNet_End-to-End_\nLearning_CVPR_2018_paper.html. doi:10.1109/\nCVPR.2018.00472.\n[32] T. Kohonen,\nThe self-organizing map,\nNeurocom-\nputing 21 (1998) 1–6. URL: https://doi.org/\n10.1016/S0925-2312(98)00030-7. doi:10.1016/\nS0925-2312(98)00030-7.\n[33] J. Li, B. M. Chen, G. H. Lee, So-net: Self-organizing\nnetwork for point cloud analysis, in: 2018 IEEE Con-\nference on Computer Vision and Pattern Recognition,\nCVPR 2018, Salt Lake City, UT, USA, June 18-22,\n2018, IEEE Computer Society, 2018, pp. 9397–9406.\nURL: http://openaccess.thecvf.com/content_\ncvpr_2018/html/Li_SO-Net_Self-Organizing_\nNetwork_CVPR_2018_paper.html. doi:10.1109/\nCVPR.2018.00979.\n[34] B. Hua, M. Tran, S. Yeung,\nPointwise convolutional\nneural networks, in: 2018 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2018,\nSalt Lake City, UT, USA, June 18-22, 2018, IEEE\nComputer Society, 2018, pp. 984–993. URL: http:\n//openaccess.thecvf.com/content_cvpr_2018/\nhtml / Hua _ Pointwise _ Convolutional _ Neural _\nCVPR_2018_paper.html. doi:10.1109/CVPR.2018.\n00109.\n[35] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, B. Chen,\nPointcnn: Convolution on x-transformed points,\nin:\nS. Bengio, H. M. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, R. Garnett (Eds.), Advances in Neu-\nral Information Processing Systems 31: Annual Confer-\nence on Neural Information Processing Systems 2018,\nNeurIPS 2018, 3-8 December 2018, Montr´eal, Canada.,\n2018, pp. 828–838. URL: http://papers.nips.\ncc/paper/7362-pointcnn-convolution-on-x-\ntransformed-points.\n[36] H. Zhao, L. Jiang, C.-W. Fu, J. Jia, Pointweb: Enhancing\nlocal neighborhood features for point cloud processing,\nin: The IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2019.\n[37] W. Wu, Z. Qi, L. Fuxin, Pointconv: Deep convolutional\nnetworks on 3d point clouds, in: The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR),\n2019.\n[38] Y. Liu, B. Fan, S. Xiang, C. Pan, Relation-shape con-\nvolutional neural network for point cloud analysis, in:\nThe IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019.\n[39] S. Lan, R. Yu, G. Yu, L. S. Davis, Modeling local ge-\nometric structure of 3d point clouds using geo-cnn, in:\nThe IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019.\n[40] A. Komarichev, Z. Zhong, J. Hua,\nA-cnn:\nAnnu-\nlarly convolutional neural networks on point clouds, in:\nThe IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019.\n[41] Y. Xu, T. Fan, M. Xu, L. Zeng, Y. Qiao,\nSpidercnn:\nDeep learning on point sets with parameterized convolu-\ntional ﬁlters, in: V. Ferrari, M. Hebert, C. Sminchisescu,\nY. Weiss (Eds.), Computer Vision - ECCV 2018 - 15th\nEuropean Conference, Munich, Germany, September 8-\n14, 2018, Proceedings, Part VIII, volume 11212 of Lec-\nture Notes in Computer Science, Springer, 2018, pp. 90–\n105. URL: https://doi.org/10.1007/978-3-030-\n01237-3_6. doi:10.1007/978-3-030-01237-3\\_6.\n[42] J. Liu, B. Ni, C. Li, J. Yang, Q. Tian, Dynamic points\nagglomeration for hierarchical point sets learning,\nin:\nThe IEEE International Conference on Computer Vision\n(ICCV), 2019.\n[43] R. Klokov, V. S. Lempitsky, Escape from cells: Deep kd-\nnetworks for the recognition of 3d point cloud models,\nin: IEEE International Conference on Computer Vision,\nICCV 2017, Venice, Italy, October 22-29, 2017, IEEE\nComputer Society, 2017, pp. 863–872. URL: https:\n//doi.org/10.1109/ICCV.2017.99. doi:10.1109/\nICCV.2017.99.\n[44] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bron-\nstein, J. M. Solomon,\nDynamic graph CNN for\n17\nlearning on point clouds,\nCoRR abs/1801.07829\n(2018). URL: http://arxiv.org/abs/1801.07829.\narXiv:1801.07829.\n[45] C. Wang, B. Samari, K. Siddiqi, Local spectral graph\nconvolution for point set feature learning,\nin: V. Fer-\nrari, M. Hebert, C. Sminchisescu, Y. Weiss (Eds.), Com-\nputer Vision - ECCV 2018 - 15th European Confer-\nence, Munich, Germany, September 8-14, 2018, Pro-\nceedings, Part IV, volume 11208 of Lecture Notes in\nComputer Science, Springer, 2018, pp. 56–71. URL:\nhttps://doi.org/10.1007/978-3-030-01225-\n0_4. doi:10.1007/978-3-030-01225-0\\_4.\n[46] Z. Zhang, B.-S. Hua, S.-K. Yeung, Shellnet: Eﬃcient\npoint cloud convolutional neural networks using concen-\ntric shells statistics, in: The IEEE International Confer-\nence on Computer Vision (ICCV), 2019.\n[47] W. Han, C. Wen, C. Wang, Q. Li, X. Li, forthcoming:\nPoint2node: Correlation learning of dynamic-node for\npoint cloud feature modeling, in: Conference on Artiﬁ-\ncial Intelligence, (AAAI), 2020.\n[48] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanra-\nhan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song,\nH. Su, J. Xiao, L. Yi, F. Yu, Shapenet: An information-\nrich 3d model repository,\nCoRR abs/1512.03012\n(2015). URL: http://arxiv.org/abs/1512.03012.\narXiv:1512.03012.\n[49] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su,\nC. Lu, Q. Huang, A. Sheﬀer, L. Guibas, et al., A scal-\nable active framework for region annotation in 3d shape\ncollections, ACM Transactions on Graphics (TOG) 35\n(2016) 210.\n[50] A. Dai, C. R. Qi, M. Nießner,\nShape completion us-\ning 3d-encoder-predictor cnns and shape synthesis, in:\nProc. Computer Vision and Pattern Recognition (CVPR),\nIEEE, 2017.\n[51] K. Park, K. Rematas, A. Farhadi, S. M. Seitz, Photo-\nshape: Photorealistic materials for large-scale shape col-\nlections, ACM Trans. Graph. 37 (2018).\n[52] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J.\nGuibas, H. Su, PartNet: A large-scale benchmark for\nﬁne-grained and hierarchical part-level 3D object under-\nstanding, in: The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2019.\n[53] Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su,\nR. Mottaghi, L. Guibas, S. Savarese, Objectnet3d: A\nlarge scale database for 3d object recognition, in: Euro-\npean Conference Computer Vision (ECCV), 2016.\n[54] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei,\nImagenet: A large-scale hierarchical image database, in:\n2009 IEEE conference on computer vision and pattern\nrecognition, Ieee, 2009, pp. 248–255.\n[55] X. Wang, B. Zhou, Y. Shi, X. Chen, Q. Zhao, K. Xu,\nShape2motion: Joint analysis of motion parts and at-\ntributes from 3d shapes, in: CVPR, 2019, p. to appear.\n[56] 3d warehouse, ????\nURL: https://3dwarehouse.\nsketchup.com/.\n[57] M. A. Uy, Q.-H. Pham, B.-S. Hua, D. T. Nguyen, S.-\nK. Yeung,\nRevisiting point cloud classiﬁcation:\nA\nnew benchmark dataset and classiﬁcation model on real-\nworld data, in: International Conference on Computer\nVision (ICCV), 2019.\n[58] B.-S. Hua, Q.-H. Pham, D. T. Nguyen, M.-K. Tran, L.-F.\nYu, S.-K. Yeung, Scenenn: A scene meshes dataset with\nannotations, in: International Conference on 3D Vision\n(3DV), 2016.\n[59] N. Silberman, D. Hoiem, P. Kohli, R. Fergus, Indoor\nsegmentation and support inference from rgbd images,\nin: European Conference on Computer Vision, Springer,\n2012, pp. 746–760.\n[60] J. Xiao, A. Owens, A. Torralba,\nSun3d: A database\nof big spaces reconstructed using sfm and object labels,\nin: Proceedings of the IEEE International Conference on\nComputer Vision, 2013, pp. 1625–1632.\n[61] I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. K. Brilakis,\nM. Fischer, S. Savarese, 3d semantic parsing of large-\nscale indoor spaces, in: 2016 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2016, Las\nVegas, NV, USA, June 27-30, 2016, IEEE Computer So-\nciety, 2016, pp. 1534–1543. URL: https://doi.org/\n10.1109/CVPR.2016.170. doi:10.1109/CVPR.2016.\n170.\n[62] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niess-\nner, M. Savva, S. Song, A. Zeng, Y. Zhang,\nMatter-\nport3d: Learning from rgb-d data in indoor environ-\nments,\nInternational Conference on 3D Vision (3DV)\n(2017).\n[63] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao,\nT. Funkhouser, 3dmatch: Learning local geometric de-\nscriptors from rgb-d reconstructions, in: CVPR, 2017.\n[64] J. Valentin, A. Dai, M. Nießner, P. Kohli, P. Torr, S. Izadi,\nC. Keskin, Learning to navigate the energy landscape,\narXiv preprint arXiv:1603.05772 (2016).\n[65] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi,\nA. Fitzgibbon, Scene coordinate regression forests for\ncamera relocalization in rgb-d images, in: Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 2013, pp. 2930–2937.\n[66] M. De Deuge, A. Quadros, C. Hung, B. Douillard, Un-\nsupervised feature learning for classiﬁcation of outdoor\n3d scans, in: Australasian Conference on Robitics and\nAutomation, volume 2, 2013, p. 1.\n18\n[67] M. Halber, T. A. Funkhouser, Structured global regis-\ntration of rgb-d scans in indoor environments,\nArXiv\nabs/1607.08539 (2016).\n[68] C. Wang, S. Hou, C. Wen, Z. Gong, Q. Li, X. Sun, J. Li,\nSemantic line framework-based indoor building model-\ning using backpacked laser scanning point cloud,\nIS-\nPRS journal of photogrammetry and remote sensing 143\n(2018) 150–166.\n[69] A. Geiger, P. Lenz, R. Urtasun, Are we ready for au-\ntonomous driving? the kitti vision benchmark suite, in:\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2012.\n[70] A. Geiger, P. Lenz, C. Stiller, R. Urtasun, Vision meets\nrobotics: The kitti dataset,\nInternational Journal of\nRobotics Research (IJRR) (2013).\n[71] J. Behley,\nM. Garbade,\nA. Milioto,\nJ. Quenzel,\nS. Behnke, C. Stachniss, J. Gall,\nSemanticKITTI: A\nDataset for Semantic Scene Understanding of LiDAR\nSequences,\nin: Proc. of the IEEE/CVF International\nConf. on Computer Vision (ICCV), 2019.\n[72] F. Pomerleau, M. Liu, F. Colas, R. Siegwart, Challeng-\ning data sets for point cloud registration algorithms, The\nInternational Journal of Robotics Research 31 (2012)\n1705–1711.\n[73] M. Br´edif, B. Vallet, A. Serna, B. Marcotegui, N. Papar-\noditis, Terramobilita/iqmulus urban point cloud classiﬁ-\ncation benchmark, 2014.\n[74] W.\nMaddern,\nG.\nPascoe,\nC.\nLinegar,\nP.\nNew-\nman,\n1 Year,\n1000km:\nThe Oxford RobotCar\nDataset,\nThe International Journal of Robotics Re-\nsearch (IJRR) 36 (2017) 3–15. URL: http://dx.doi.\norg/10.1177/0278364916679498. doi:10.1177/\n0278364916679498.\n[75] N. Carlevaris-Bianco, A. K. Ushani, R. M. Eustice, Uni-\nversity of michigan north campus long-term vision and\nlidar dataset, The International Journal of Robotics Re-\nsearch 35 (2016) 1023–1035.\n[76] T. Hackel, N. Savinov, L. Ladicky, J. D. Wegner,\nK. Schindler, M. Pollefeys,\nSemantic3d. net: A new\nlarge-scale point cloud classiﬁcation benchmark, arXiv\npreprint arXiv:1704.03847 (2017).\n[77] Y. Chen, J. Wang, J. Li, C. Lu, Z. Luo, H. Xue, C. Wang,\nLidar-video driving dataset: Learning driving policies\neﬀectively,\nin: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp.\n5870–5878.\n[78] X. Roynard, J.-E. Deschaud, F. Goulette,\nParis-lille-\n3d: A large and high-quality ground-truth urban point\ncloud dataset for automatic segmentation and classi-\nﬁcation,\nThe International Journal of Robotics Re-\nsearch 37 (2018) 545–557. URL: https://doi.\norg/10.1177/0278364918767506. doi:10.1177/\n0278364918767506.\n[79] X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai,\nH. Su, H. Li, R. Yang,\nApollocar3d: A large 3d car\ninstance understanding benchmark for autonomous driv-\ning, in: Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2019, pp. 5452–\n5462.\n[80] W. Lu, Y. Zhou, G. Wan, S. Hou, S. Song, L3-net: To-\nwards learning based lidar localization for autonomous\ndriving,\nin: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2019, pp.\n6389–6398.\n[81] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Li-\nong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, O. Beijbom,\nnuscenes: A multimodal dataset for autonomous driving,\narXiv preprint arXiv:1903.11027 (2019).\n[82] J. Xue, J. Fang, T. Li, B. Zhang, P. Zhang, Z. Ye, J. Dou,\nBLVD: Building a large-scale 5d semantics benchmark\nfor autonomous driving, in: Proc. International Confer-\nence on Robotics and Automation, in press, 2019.\n[83] A. Krizhevsky, I. Sutskever, G. E. Hinton,\nIma-\ngenet classiﬁcation with deep convolutional neural net-\nworks,\nin: P. L. Bartlett, F. C. N. Pereira, C. J. C.\nBurges, L. Bottou, K. Q. Weinberger (Eds.), Advances\nin Neural Information Processing Systems 25: 26th An-\nnual Conference on Neural Information Processing Sys-\ntems 2012. Proceedings of a meeting held December\n3-6, 2012, Lake Tahoe, Nevada, United States., 2012,\npp. 1106–1114. URL: http://papers.nips.cc/\npaper/4824- imagenet- classification- with-\ndeep-convolutional-neural-networks.\n[84] K. Simonyan, A. Zisserman, Very deep convolutional\nnetworks for large-scale image recognition, in: Y. Ben-\ngio, Y. LeCun (Eds.), 3rd International Conference on\nLearning Representations, ICLR 2015, San Diego, CA,\nUSA, May 7-9, 2015, Conference Track Proceedings,\n2015. URL: http://arxiv.org/abs/1409.1556.\n[85] K. He, X. Zhang, S. Ren, J. Sun,\nDeep residual\nlearning for image recognition, CoRR abs/1512.03385\n(2015). URL: http://arxiv.org/abs/1512.03385.\narXiv:1512.03385.\n[86] Y. Yang, C. Feng, Y. Shen, D. Tian,\nFoldingnet:\nPoint cloud auto-encoder via deep grid deformation, in:\n2018 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2018, Salt Lake City, UT, USA,\nJune 18-22, 2018, IEEE Computer Society, 2018, pp.\n206–215. URL: http://openaccess.thecvf.com/\ncontent _ cvpr _ 2018 / html / Yang _ FoldingNet _\nPoint _ Cloud _ CVPR _ 2018 _ paper . html. doi:10 .\n1109/CVPR.2018.00029.\n19\n[87] T. Rabbani, F. van den Heuvel, G. Vosselman, Segmen-\ntation of point clouds using smoothness constraints, in:\nH. Maas, D. Schneider (Eds.), ISPRS 2006 : Proceedings\nof the ISPRS commission V symposium Vol. 35, part 6 :\nimage engineering and vision metrology, Dresden, Ger-\nmany 25-27 September 2006, volume 35, International\nSociety for Photogrammetry and Remote Sensing (IS-\nPRS), 2006, pp. 248–253.\n[88] A. Jagannathan, E. L. Miller, Three-dimensional surface\nmesh segmentation using curvedness-based region grow-\ning approach, IEEE Trans. Pattern Anal. Mach. Intell.\n29 (2007) 2195–2204. URL: https://doi.org/10.\n1109/TPAMI.2007.1125. doi:10.1109/TPAMI.2007.\n1125.\n[89] W. Wang, R. Yu, Q. Huang, U. Neumann, SGPN: sim-\nilarity group proposal network for 3d point cloud in-\nstance segmentation,\nin: 2018 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2018,\nSalt Lake City, UT, USA, June 18-22, 2018, IEEE\nComputer Society, 2018, pp. 2569–2578. URL: http:\n//openaccess.thecvf.com/content_cvpr_2018/\nhtml/Wang_SGPN_Similarity_Group_CVPR_2018_\npaper.html. doi:10.1109/CVPR.2018.00272.\n[90] X. Wang, S. Liu, X. Shen, C. Shen, J. Jia,\nAsso-\nciatively segmenting instances and semantics in point\nclouds,\nin:\nIEEE Conference on Computer Vi-\nsion and Pattern Recognition,\nCVPR 2019,\nLong\nBeach, CA, USA, June 16-20, 2019, Computer Vi-\nsion Foundation / IEEE, 2019, pp. 4096–4105. URL:\nhttp://openaccess.thecvf.com/content_CVPR_\n2019 / html / Wang _ Associatively _ Segmenting _\nInstances_and_Semantics_in_Point_Clouds_\nCVPR_2019_paper.html.\n[91] B. Yang,\nJ. Wang,\nR. Clark,\nQ. Hu,\nS. Wang,\nA. Markham, N. Trigoni,\nLearning object bounding\nboxes for 3d instance segmentation on point clouds,\nCoRR abs/1906.01140 (2019). URL: http://arxiv.\norg/abs/1906.01140. arXiv:1906.01140.\n[92] Q. Pham, D. T. Nguyen, B. Hua, G. Roig, S. Ye-\nung,\nJSIS3D: joint semantic-instance segmentation\nof 3d point clouds with multi-task pointwise networks\nand multi-value conditional random ﬁelds,\nin: IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-\n20, 2019, Computer Vision Foundation / IEEE, 2019,\npp. 8827–8836. URL: http://openaccess.thecvf.\ncom / content _ CVPR _ 2019 / html / Pham _ JSIS3D _\nJoint _ Semantic - Instance _ Segmentation _ of _\n3D_Point_Clouds_With_Multi-Task_CVPR_2019_\npaper.html.\n[93] L. Yi, W. Zhao, H. Wang, M. Sung, L. J. Guibas, GSPN:\ngenerative shape proposal network for 3d instance seg-\nmentation in point cloud, in: CVPR, Computer Vision\nFoundation / IEEE, 2019, pp. 3947–3956.\n[94] J. Mao, X. Wang, H. Li, Interpolated convolutional net-\nworks for 3d point cloud understanding, in: The IEEE\nInternational Conference on Computer Vision (ICCV),\n2019.\n[95] Y. Liu, B. Fan, G. Meng, J. Lu, S. Xiang, C. Pan, Dense-\npoint: Learning densely contextual representation for ef-\nﬁcient point cloud processing,\nin: The IEEE Interna-\ntional Conference on Computer Vision (ICCV), 2019.\n[96] R. B. Girshick, J. Donahue, T. Darrell, J. Malik, Rich\nfeature hierarchies for accurate object detection and se-\nmantic segmentation,\nin: 2014 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2014,\nColumbus, OH, USA, June 23-28, 2014, IEEE Computer\nSociety, 2014, pp. 580–587. URL: https://doi.org/\n10.1109/CVPR.2014.81. doi:10.1109/CVPR.2014.\n81.\n[97] R. Girshick, Fast r-cnn, in: The IEEE International Con-\nference on Computer Vision (ICCV), 2015.\n[98] S. Ren, K. He, R. B. Girshick, J. Sun,\nFaster R-\nCNN: towards real-time object detection with region\nproposal networks,\nin: C. Cortes, N. D. Lawrence,\nD. D. Lee, M. Sugiyama, R. Garnett (Eds.), Advances\nin Neural Information Processing Systems 28:\nAn-\nnual Conference on Neural Information Processing Sys-\ntems 2015, December 7-12, 2015, Montreal, Quebec,\nCanada, 2015, pp. 91–99. URL: http://papers.\nnips.cc/paper/5638-faster-r-cnn-towards-\nreal - time - object - detection - with - region -\nproposal-networks.\n[99] K. He, G. Gkioxari, P. Doll´ar, R. B. Girshick,\nMask\nR-CNN,\nin: IEEE International Conference on Com-\nputer Vision, ICCV 2017, Venice, Italy, October 22-29,\n2017, IEEE Computer Society, 2017, pp. 2980–2988.\nURL: https://doi.org/10.1109/ICCV.2017.322.\ndoi:10.1109/ICCV.2017.322.\n[100] J. Redmon, S. K. Divvala, R. B. Girshick, A. Farhadi,\nYou only look once: Uniﬁed, real-time object detection,\nin: 2016 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2016, Las Vegas, NV, USA,\nJune 27-30, 2016, IEEE Computer Society, 2016, pp.\n779–788. URL: https://doi.org/10.1109/CVPR.\n2016.91. doi:10.1109/CVPR.2016.91.\n[101] J. Redmon, A. Farhadi,\nYOLO9000: better, faster,\nstronger, in: 2017 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2017, Honolulu, HI,\nUSA, July 21-26, 2017, IEEE Computer Society, 2017,\npp. 6517–6525. URL: https://doi.org/10.1109/\nCVPR.2017.690. doi:10.1109/CVPR.2017.690.\n[102] J. Redmon, A. Farhadi,\nYolov3:\nAn incremen-\ntal improvement,\nCoRR abs/1804.02767 (2018).\nURL:\nhttp : / / arxiv . org / abs / 1804 . 02767.\narXiv:1804.02767.\n20\n[103] D. Z. Wang,\nI. Posner,\nVoting for voting in\nonline point cloud object detection,\nin:\nL. E.\nKavraki, D. Hsu, J. Buchli (Eds.), Robotics:\nSci-\nence and Systems XI, Sapienza University of Rome,\nRome, Italy, July 13-17, 2015, 2015. URL: http://\nwww.roboticsproceedings.org/rss11/p35.html.\ndoi:10.15607/RSS.2015.XI.035.\n[104] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, I. Pos-\nner,\nVote3deep:\nFast object detection in 3d point\nclouds using eﬃcient convolutional neural networks, in:\n2017 IEEE International Conference on Robotics and\nAutomation, ICRA 2017, Singapore, Singapore, May\n29 - June 3, 2017, IEEE, 2017, pp. 1355–1361. URL:\nhttps://doi.org/10.1109/ICRA.2017.7989161.\ndoi:10.1109/ICRA.2017.7989161.\n[105] C. R. Qi, W. Liu, C. Wu, H. Su, L. J. Guibas,\nFrustum pointnets for 3d object detection from RGB-\nD data,\nin:\n2018 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2018, Salt\nLake City, UT, USA, June 18-22, 2018, IEEE Com-\nputer Society, 2018, pp. 918–927. URL: http://\nopenaccess.thecvf.com/content_cvpr_2018/\nhtml/Qi_Frustum_PointNets_for_CVPR_2018_\npaper.html. doi:10.1109/CVPR.2018.00102.\n[106] L. Yi, W. Zhao, H. Wang, M. Sung, L. J. Guibas, Gspn:\nGenerative shape proposal network for 3d instance seg-\nmentation in point cloud,\nin: The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR),\n2019.\n[107] S. Shi, X. Wang, H. Li, Pointrcnn: 3d object proposal\ngeneration and detection from point cloud, in: The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2019.\n[108] C. R. Qi, O. Litany, K. He, L. J. Guibas, Deep hough\nvoting for 3d object detection in point clouds,\nCoRR\nabs/1904.09664 (2019). URL: http://arxiv.org/\nabs/1904.09664. arXiv:1904.09664.\n[109] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, O. Bei-\njbom, Pointpillars: Fast encoders for object detection\nfrom point clouds, in: The IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2019.\n[110] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed,\nC. Fu, A. C. Berg,\nSSD: single shot multibox detec-\ntor, in: B. Leibe, J. Matas, N. Sebe, M. Welling (Eds.),\nComputer Vision - ECCV 2016 - 14th European Con-\nference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part I, volume 9905 of Lecture Notes\nin Computer Science, Springer, 2016, pp. 21–37. URL:\nhttps://doi.org/10.1007/978-3-319-46448-\n0_2. doi:10.1007/978-3-319-46448-0\\_2.\n[111] X. Chen, H. Ma, J. Wan, B. Li, T. Xia, Multi-view 3d\nobject detection network for autonomous driving,\nin:\n2017 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July 21-\n26, 2017, IEEE Computer Society, 2017, pp. 6526–6534.\nURL: https://doi.org/10.1109/CVPR.2017.691.\ndoi:10.1109/CVPR.2017.691.\n[112] M. Liang, B. Yang, S. Wang, R. Urtasun,\nDeep con-\ntinuous fusion for multi-sensor 3d object detection, in:\nV. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss (Eds.),\nComputer Vision - ECCV 2018 - 15th European Con-\nference, Munich, Germany, September 8-14, 2018, Pro-\nceedings, Part XVI, volume 11220 of Lecture Notes in\nComputer Science, Springer, 2018, pp. 663–678. URL:\nhttps://doi.org/10.1007/978-3-030-01270-\n0_39. doi:10.1007/978-3-030-01270-0\\_39.\n[113] K. Shin, Y. P. Kwon, M. Tomizuka, Roarnet: A robust 3d\nobject detection based on region approximation reﬁne-\nment, in: 2019 IEEE Intelligent Vehicles Symposium,\nIV 2019, Paris, France, June 9-12, 2019, IEEE, 2019, pp.\n2510–2515. URL: https://doi.org/10.1109/IVS.\n2019.8813895. doi:10.1109/IVS.2019.8813895.\n[114] J. Ku, M. Moziﬁan, J. Lee, A. Harakeh, S. L. Waslander,\nJoint 3d proposal generation and object detection from\nview aggregation, in: 2018 IEEE/RSJ International Con-\nference on Intelligent Robots and Systems, IROS 2018,\nMadrid, Spain, October 1-5, 2018, IEEE, 2018, pp. 1–\n8. URL: https://doi.org/10.1109/IROS.2018.\n8594049. doi:10.1109/IROS.2018.8594049.\n[115] B. Yang, M. Liang, R. Urtasun, HDNET: exploiting HD\nmaps for 3d object detection, in: 2nd Annual Confer-\nence on Robot Learning, CoRL 2018, Z¨urich, Switzer-\nland, 29-31 October 2018, Proceedings, volume 87 of\nProceedings of Machine Learning Research, PMLR,\n2018, pp. 146–155. URL: http://proceedings.mlr.\npress/v87/yang18b.html.\n[116] B. Yang, W. Luo, R. Urtasun, PIXOR: real-time 3d ob-\nject detection from point clouds, in: 2018 IEEE Con-\nference on Computer Vision and Pattern Recognition,\nCVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018,\nIEEE Computer Society, 2018, pp. 7652–7660. URL:\nhttp://openaccess.thecvf.com/content_cvpr_\n2018/html/Yang_PIXOR_Real- Time_3D_CVPR_\n2018_paper.html. doi:10.1109/CVPR.2018.00798.\n[117] Y. Yan, Y. Mao, B. Li, SECOND: sparsely embedded\nconvolutional detection, Sensors 18 (2018) 3337. URL:\nhttps://doi.org/10.3390/s18103337. doi:10.\n3390/s18103337.\n[118] S. Shi, X. Wang, H. Li, Pointrcnn: 3d object proposal\ngeneration and detection from point cloud,\nin: IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, Computer Vision Foundation / IEEE, 2019, pp.\n770–779. URL: http://openaccess.thecvf.com/\ncontent_CVPR_2019/html/Shi_PointRCNN_3D_\n21\nObject_Proposal_Generation_and_Detection_\nFrom_Point_Cloud_CVPR_2019_paper.html.\n[119] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang,\nO. Beijbom,\nPointpillars:\nFast encoders for object\ndetection from point clouds,\nin:\nIEEE Conference\non Computer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019, Com-\nputer Vision Foundation / IEEE, 2019, pp. 12697–\n12705. URL: http://openaccess.thecvf.com/\ncontent_CVPR_2019/html/Lang_PointPillars_\nFast_Encoders_for_Object_Detection_From_\nPoint_Clouds_CVPR_2019_paper.html.\n[120] M. Angelina Uy, G. Hee Lee, Pointnetvlad: Deep point\ncloud based retrieval for large-scale place recognition,\nin: The IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2018.\n[121] Z. Liu, S. Zhou, C. Suo, P. Yin, W. Chen, H. Wang,\nH. Li, Y.-H. Liu, Lpd-net: 3d point cloud learning for\nlarge-scale place recognition and environment analysis,\nin: The IEEE International Conference on Computer Vi-\nsion (ICCV), 2019.\n22\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-01-17",
  "updated": "2020-01-17"
}