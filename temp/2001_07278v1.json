{
  "id": "http://arxiv.org/abs/2001.07278v1",
  "title": "Mixed integer programming formulation of unsupervised learning",
  "authors": [
    "Arturo Berrones-Santos"
  ],
  "abstract": "A novel formulation and training procedure for full Boltzmann machines in\nterms of a mixed binary quadratic feasibility problem is given. As a proof of\nconcept, the theory is analytically and numerically tested on XOR patterns.",
  "text": "Mixed integer programming formulation of\nunsupervised learning\nArturo Berrones-Santos\nUniversidad Aut´onoma de Nuevo Le´on\nFacultad de Ingenier´ıa Mec´anica y El´ectrica\nPosgrado en Ingenier´ıa de Sistemas\nFacultad de Ciencias F´ısico Matem´aticas\nPosgrado en Ciencias con Orientaci´on en Matem´aticas\nAP 126, Cd. Universitaria, San Nicol´as de los Garza, NL 66450, M´exico\narturo.berronessn@uanl.edu.mx\nJanuary 22, 2020\nAbstract\nA novel formulation and training procedure for full Boltzmann ma-\nchines in terms of a mixed binary quadratic feasibility problem is given.\nAs a proof of concept, the theory is analytically and numerically tested\non XOR patterns.\nKeywords: Boltzamnn machines; Mixed integer programming; Unsu-\npervised learning.\n1\nIntroduction\nA central open question in machine learning is the eﬀective handling of unlabeled\ndata [1, 2]. The construction of balanced representative datasets for supervised\nmachine learning for the most part still requires a very close and time consuming\nhuman direction, so the development of eﬃcient learning from data algorithms\nin an unsupervised fashion is a very active area of research [1, 2]. A general\nframework to deal with unlabeled data is the Boltzmann machine paradigm, in\nwhich is attempted to learn a probability distribution for the patterns in the\ndata without any previous identiﬁcation of input and output variables. In its\nmost general setups however, the training of Blotzmann machines is compu-\ntationally intractable [2, 3, 4]. In this contribution is established a relation,\nwhich to the best of my knowledge was previously unknown, between Mixed\nInteger Programing (MIP) and the full Boltzmann machine in binary variables.\nIs hoped that this novel formulation opens the road to more eﬃcient learning\nalgorithms by taking advantage of the great variety of techniques available for\nMIP.\n1\narXiv:2001.07278v1  [cs.LG]  20 Jan 2020\n2\nFull Boltzmann machine with data as con-\nstraints\nConsider a network of units with binary state space. Each unit depends on all\nthe others by a logistic-type response function,\nxi = round\n\n\n1\n1 + exp\n\u0010\n−P\nj̸=i qj,ixj −bi\n\u0011\n\n≡fi,\n(1)\nwhere the “round” indicates the nearest integer function, the q’s are pairwise\ninteractions between units and the b’s are shift parameters. As will later be\nclear, the proposed model supports both supervised and unsupervised learning\nand leads to a full Boltzmann machine in its classical sense.\nFigure 1: Full Boltzmann machine with ﬁve units.\nSuppose a data set of D visible binary vectors, {⃗vd}, d = 1, 2, ..., D with I\ncomponents each and a collection of M hidden units um, m = 1, ..., M. The\ntotal number of units in the system is N = I + M. If the connectivity and shift\nparameters are given, each sample ﬁxes the binary vector ⃗xd = {⃗vd, ⃗ud} and\n2\ntherefore the data set imposes the following ND constraints:\n(−1)vd,i\n\n\nI\nX\nj̸=i\nqi,jvd,j + bi\n\n\n≤0,\n(2)\n(−1)ud,m\n\n\nM\nX\nj̸=m\nqm,jud,j + bm\n\n\n≤0,\nd = 1, 2, ..., D; i = 1, 2, ..., I; m = 1, 2, ..., M.\nA posterior distribution for the parameters P({qi,j, bi}|D), can be constructed\nby the maximum entropy principle, which gives the less biased distribution that\nis consistent with a set of constraints [5, 6]. This is done by the minimization\nof the Lagrangian\nL =\nZ\nP ln Pd⃗w +\nND\nX\nr=1\nλr ⟨constraint(r)⟩,\n(3)\nwhere the brackets represent average under the posterior, ⃗w is a vector that\ncontains the connectivity and shift parameters and the λ’s are positive Lagrange\nmultipliers. Due to the linearity of the system of inequalities (2), the average of\nthe constraints under P with ﬁxed unit values is simply given by the same set\nof inequalities with the coeﬃcients qj,i’s and bi’s substituted by their averages\n⟨qj,i⟩’s, ⟨bi⟩’s. The maximum entropy distribution for the parameters is therefore\ngiven by\nP({qi,j, bi}|D) = 1\nZ exp\n\n−\nX\n{d,i}\nλ{d,i}\n\n(−1)xd,i\n\n\nN\nX\nj̸=i\nqi,jxd,j + bi\n\n\n\n\n\n,\n(4)\nwhere Z is a normalization factor. So due to the linearity of the constraints, P\nis a tractable (i. e. an easy to sample) product of independent two parameter\nexponential distributions:\nP({qi,j, bi}|D) = P(⃗w|D) =\nN\nY\nn=1\nαne−αn(wn−βn),\n(5)\nwhere ⟨wn⟩=\n1\nαn +βn [7]. Therefore a necessary and suﬃcient condition for the\nexistence of the above distribution is the existence of the averages ⟨wn⟩, which\nis determined by the satisfaction of the inequalities (2).\nThe representation of the posterior by its two parameter exponential form\nEq. (5) gives a codiﬁcation of the training data in terms of a tractable distribu-\ntion for the parameters that in conjunction with Eq. (1) is in fact a distribution\nfor new unlabeled binary strings of data. For fully connected topologies, this is\nwhat is usually understood by an equilibrium distribution of a full Boltzmann\nmachine [2].\n3\nFigure 2: Three diﬀerent architectures for the XOR problem.\n2.1\nIllustrative example 1: Supervised XOR\nThe theoretical soundness of the proposed approach is now shown through the\nXOR logical table, D = {(0, 0, 0), (1, 0, 1), (0, 1, 1), (1, 1, 0)}. Let’s consider ﬁrst\na restricted architecture with only two directed arcs that connect two inputs\nwith an output unit, as represented in Figure 2A. The inequalities (2) in this\ncase read,\nb2 ≤0,\n−q0,2 −b2 ≤0,\n(6)\n−q1,2 −b2 ≤0,\nq0,2 + q1,2 + b2 ≤0.\nThere are no values for b2, q0,2 and q1,2 that satisfy all the inequalities. This is\nreﬂected in the maximum entropy distribution,\nP = 1\nZ e−λ1(b2)e−λ2(−q0,2−b2)e−λ3(−q1,2−b2)e−λ4(q0,2+q1,2+b2)\n(7)\nwhich to be a properly normalized product of two-parameter exponential dis-\ntributions must satisfy the contradictory conditions b2 < 0, q0,2 > 0, q1,2 > 0,\n|b2| < q0,2, |b2| < q1,2, |b2| > q0,2 + q1,2. A valid model is however attainable by\nthe addition of a single hidden unit. Consider the architecture represented in\nFigure 2B. This leads to a two stage constraint satisfaction problem. The ﬁrst\n4\nstage is given by the data evaluated on the visible units,\nq3,2f3(0, 0) + b2 ≤0,\n(8)\n−q0,2 −q3,2f3(1, 0) −b2 ≤0,\n−q1,2 −q3,2f3(0, 1) −b2 ≤0,\nq0,2 + q1,2 + q3,2f3(1, 1) + b2 ≤0,\nfor which solutions certainly exist. Take for instance,\n|b2| < q0,2,\n|b2| < q1,2,\n|b2 + q3,2| > q0,2 + q1,2,\n(9)\nf3(0, 0) = f3(1, 0) = f3(0, 1) = 0,\nf3(1, 1) = 1.\nThe second stage is consequently given by,\nb3 ≤0,\n(10)\nq0,3 + b3 ≤0,\nq1,3 + b3 ≤0,\n−q0,3 −q1,3 −b3 ≤0,\nfor which solutions exist under the conditions b3 < −C, |b3| > |q0,3|, |b3| > |q1,3|,\n|b3| < |q0,3 + q1,3|, where C is a positive constant. Therefore, the maximum\nentropy distribution for the parameters of the model represented in the Figure\n2B exists. Equivalently, this result shows that the classical XOR supervised\nlearning problem can be solved by the proposed MIP feasibility formulation.\n2.2\nIllustrative example 2: Unsupervised XOR\nA model capable of unsupervised learning is sketched in Figure 2C. The system\nof inequalities should be now extended to consider inputs to nodes x0 and x1,\nq3,0f3(0, 0) + b0 ≤0,\n(11)\n−q2,0 −q3,0f3(1, 0) −b0 ≤0,\nq1,0 + q2,0 + q3,0f3(0, 1) + b0 ≤0,\n−q3,0f3(1, 1) −q1,0 −b0 ≤0,\nq3,1f3(0, 0) + b1 ≤0,\nq0,1 + q2,1 + q3,1f3(1, 0) + b1 ≤0,\n−q2,1 −q3,1f3(0, 1) −b1 ≤0,\n−q0,1 −q3,1f3(1, 1) −b1 ≤0,\nwhich has indeed solutions, as discussed in the following section.\n3\nSampling from the posterior distribution\nThe equilibrium posterior distribution of patterns can be sampled by taking\nan arbitrary solution of the MIP feasibility problem and using it to deﬁne the\n5\naverages ⟨wn⟩=\n1\nαn + βn. The standard deviation of each two-parameter expo-\nnential distribution is given by σn =\n1\nαn , which can be at ﬁrst instance assigned\nto some positive value related to the constant C. If y is an uniform random\ndeviate in the interval [0, 1], then wn = −1\nαn ln(1 −y) + βn is a deviate from\nthe two-parameter exponential distribution associated to wn. In this way, the\nvector of visible units can be sampled in a computation time that is quadratic\nin the number of total (visible and invisible) units.\nAlgorithm 1 (Pseudo-code for sampling from the maximum entropy posterior.)\n1: Initialize: βn from a solution of the MIP feasibility problem and\n1\nαn ←ϵC\n(ϵ arbitrary positive real number).\n2: Asign value to size (desired number of samples).\n3: Generate yτ, (τ = 1, ..., size×N) uniform and independent random deviates\nin the [0, 1] interval.\n4: for s = 1 to size do\n5:\nwn,s = −\n1\nαn,s ln(1 −y) + βn,s, n = 1, ..., N\n6:\nGenerate ⃗xs by inserting ⃗ws in Eq. (1)\n7: end for\nThe step 6 of the algorithm above is made by starting with an inital random\nbinary vector ⃗x at each s. The self-consistent system Eq. (1) is then iterated.\nNo more than 10 iterations are needed to achieve convergence.\nThe sampling procedure Algorithm 1 is now shown through the XOR exam-\nple. Take an arbitrary solution of the MIP feasibility problem, say\nf3(0, 0) = f3(1, 0) = f3(0, 1) = 0, f3(1, 1) = 1,\n(12)\n−b0 = −b1 = −b2 = −b3 = C,\nq0,3 = q1,3 = 3\n4C,\nq2,0 = q0,2 = q1,2 = q2,1 = −q0,1 = −q1,0 = 2C,\nq3,0 = q3,1 = −q3,2 = 4C.\nDue to the rounding operator in Eq. (1), any C > 0 can work. In the following\nexperiments the value C = 100 is used with sample sizes of 1500. Some of the\nsamples drawn for each C are shown.\n(a) :\n1\nαn\n= 0.1C\n∀\nn :\n[011], [000], [110], [101], [000], [011], [000],\n[000], [110], [110], [000], [011], [000], [000], [110]...\n6\n(b) :\n1\nαn\n= 0.5C\n∀\nn :\n[011], [000], [000], [111], [110], [000], [000],\n[000], [000], [000], [111], [000], [101], [000], [011]...\n(c) :\n1\nαn\n= 2.0C\n∀\nn :\n[100], [001], [111], [011], [000], [000], [101],\n[000], [001], [000], [000], [001], [000], [100], [101]...\nThe resulting ratios of XOR patterns relative to non-XOR patterns over the\nentire 1500 samples for each case are, (a) : 1, (b) : 0.9 and (c) : 0.6\n4\nDiscussion\nIn the author’s view, this paper presents a formalism that has the potential not\nonly to give more eﬃcient learning algorithms but to improve the understanding\nof the learning from data itself. Particularly, datasets explicitly constrain the\nparameters of the learning model by a set of feasiblity mixed binary inequalities.\nFor ﬁxed binary values, the system is linear and continuous. For ﬁxed model\nparameters, it’s a linear constraint satisfaction problem in binary variables. The\nauthor together with collaborators is now working in diﬀerent ways to exploit\nthese structures in order to scale the framework to solve realistic large scale\nunsupervised learning problems. In such problems, a measure proportional to\nthe number of satisﬁed constraints might be used to gide the learning procedure\nand to assign sensible values to the\n1\nαn hyperparameter.\nacknowledgements\nThe author acknowledge partial ﬁnancial support from UANL and CONACyT.\nConﬂict of interest\nThe author declares that he have no conﬂict of interest.\nReferences\n[1] Oliver, A., Odena, A., Raﬀel, C. A., Cubuk, E. D., & Goodfellow, I. Realistic\nevaluation of deep semi-supervised learning algorithms. Advances in Neural\nInformation Processing Systems, 3235-3246, (2018).\n7\n[2] Goodfellow, I., Bengio, Y., & Courville, A., Deep learning. MIT press,\n(2016).\n[3] Fischer, A., & Igel, C., Training restricted Boltzmann machines: An intro-\nduction. Pattern Recognition, 47(1), 25-39, (2014).\n[4] Li, R. Y., Albash, T., & Lidar, D. A., Improved Boltzmann machines with\nerror corrected quantum annealing. arXiv preprint arXiv:1910.01283, (2019).\n[5] Jaynes, E. T., Information theory and statistical mechanics. Physical review,\n106(4), 620, (1957).\n[6] Shore, J., & Johnson, R., Axiomatic derivation of the principle of maximum\nentropy and the principle of minimum cross-entropy. IEEE Transactions on\ninformation theory, 26(1), 26-37, (1980).\n[7] Kececioglu, D., Reliability engineering handbook (Vol. 1). DEStech Publi-\ncations, Inc., (2002).\n8\n",
  "categories": [
    "cs.LG",
    "cond-mat.dis-nn",
    "stat.ML"
  ],
  "published": "2020-01-20",
  "updated": "2020-01-20"
}