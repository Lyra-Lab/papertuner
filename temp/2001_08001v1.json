{
  "id": "http://arxiv.org/abs/2001.08001v1",
  "title": "Safety Concerns and Mitigation Approaches Regarding the Use of Deep Learning in Safety-Critical Perception Tasks",
  "authors": [
    "Oliver Willers",
    "Sebastian Sudholt",
    "Shervin Raafatnia",
    "Stephanie Abrecht"
  ],
  "abstract": "Deep learning methods are widely regarded as indispensable when it comes to\ndesigning perception pipelines for autonomous agents such as robots, drones or\nautomated vehicles. The main reasons, however, for deep learning not being used\nfor autonomous agents at large scale already are safety concerns. Deep learning\napproaches typically exhibit a black-box behavior which makes it hard for them\nto be evaluated with respect to safety-critical aspects. While there have been\nsome work on safety in deep learning, most papers typically focus on high-level\nsafety concerns. In this work, we seek to dive into the safety concerns of deep\nlearning methods and present a concise enumeration on a deeply technical level.\nAdditionally, we present extensive discussions on possible mitigation methods\nand give an outlook regarding what mitigation methods are still missing in\norder to facilitate an argumentation for the safety of a deep learning method.",
  "text": "arXiv:2001.08001v1  [cs.LG]  22 Jan 2020\nSafety Concerns and Mitigation Approaches Regarding the Use of\nDeep Learning in Safety-Critical Perception Tasks\nOliver Willers, Sebastian Sudholt, Shervin Raafatnia, Stephanie Abrecht\nRobert Bosch GmbH, Chassis Systems Control, Automated Driving\n74232 Abstatt, Germany\nJanuary 23, 2020\nAbstract\nDeep learning methods are widely regarded as indispens-\nable when it comes to designing perception pipelines for\nautonomous agents such as robots, drones or automated\nvehicles. The main reasons, however, for deep learning\nnot being used for autonomous agents at large scale al-\nready are safety concerns. Deep learning approaches typ-\nically exhibit a black-box behavior which makes it hard\nfor them to be evaluated with respect to safety-critical as-\npects. While there have been some work on safety in deep\nlearning, most papers typically focus on high-level safety\nconcerns. In this work, we seek to dive into the safety\nconcerns of deep learning methods and present a concise\nenumeration on a deeply technical level. Additionally,\nwe present extensive discussions on possible mitigation\nmethods and give an outlook regarding what mitigation\nmethods are still missing in order to facilitate an argu-\nmentation for the safety of a deep learning method.\n1\nIntroduction\nDuring the last years new and exciting applications were\nenabled by machine learning (ML) and especially, by\ndeep learning (DL) methods. Their capability of solv-\ning problems which cannot be fully speciﬁed makes DL a\nkey enabler in many applications. Therefore, DL is also\nof fundamental importance for the fast growing ﬁeld of\nAdvanced Driver Assistance Systems (ADAS) and Auto-\nmated Driving (AD) as it is not possible to specify an open\ncontext in every detail (e.g., the data representation of a\npedestrian in all varieties cannot be speciﬁed such that it\ncould always be recognized by a rule-based algorithm).\nDifferent from humans, current DL algorithms do not\nlearn semantic or causal relationships but simply corre-\nlations in data they are presented with. For example, a\nDL algorithm used for detecting objects in camera im-\nages learns correlations between the pixels of the image\nand object representations, e.g., bounding boxes. While\nDL algorithms provide state-of-the-art performance, it is\nmore difﬁcult to understand how they arrive at their pre-\ndictions. This poses a problem when releasing systems\nthat incorporate DL methods from a safety point of view.\nWhile safety related aspects in the automotive area\nare usually handled through approaches deﬁned in the\nISO 26262 [1], the usage of DL methods introduces a\nnumber of additional safety-related aspects not covered in\nthe aforementioned norm. Most notably, DL algorithms\nmay predict incorrect results, e.g., an object detection al-\ngorithm may miss to predict an existing object. These\nkinds of limitations are not covered in the ISO 26262\nbut rather in the recently published ISO PAS 21448\nalso known as the Safety of the Intended Functionality\n(SOTIF) [2].\nAccording to this standard, SOTIF is the absence of un-\nreasonable risk due to hazards resulting from functional\ninsufﬁciencies of the intended functionality. A prerequi-\nsite for achieving SOTIF is a proper understanding of the\nsystem, its limitations as well as the conditions which may\nunveil these limitations. This is a difﬁcult task for sys-\ntems incorporating DL components because the learning\nprocess of DL algorithms is entirely different from that of\n1\na human being. Humans intuitively analyze systems and\ntheir weaknesses on a semantic level, e.g., interpreting a\ndifﬁcult scene as a composition of things like lightning\nconditions, type and position of objects, behavior of ac-\ntors, etc. However, in DL the problem space shifts from\na semantic level to the level of data representations (e.g.,\npixel values of an image). Thus, DL-speciﬁc insufﬁcien-\ncies and failure causes are not necessarily intuitive for hu-\nmans, making it difﬁcult to understand such methods and\ntheir limitations. Hence, arguing the safety of a system\nthat relies on the correctness of DL outputs requires a ded-\nicated safety consideration of such algorithms.\nIn this paper, we give a concise overview of safety con-\ncerns and their underlying problems regarding the use\nof DL algorithms focusing on Deep Neural Networks\n(DNNs)1. In particular, we will consider DNNs used in\nthe perception pipeline of an ADAS or AD system. Typ-\nical use cases for such components are DNN-based ob-\nject detection or semantic segmentation of the input data.\nThe information obtained from these DNN-components\nare then further used in an ADAS or AD system which\nmay incorporate additional information such as parallel\nsensing paths or post processing of the DNN’s output. The\ngoal of the system is to enable one or multiple functions,\ne.g., an automated emergency brake or a highway pilot.\nFurthermore, we present potential mitigation approaches\nalong with a deep technical discussion.\n2\nRelated Work\nThe question how one can use ML in safety-critical tasks\nand especially highly automated driving has attracted a\nconsiderable amount of research over the last years (e.g.,\n[3]–[11]). As discussed in the previous section, existing\nautomotive standards such as ISO 26262 do not address\nthe unique characteristics of data-driven approaches used\nin an open context.\nAs pointed out in [8], currently, there exists no agreed-\nupon way to verify and validate ML components used in\nADAS or AD systems. In particular, the foundational sta-\ntistical ML principles of empirical risk minimization and\naverage losses are not fully applicable when considering\n1Please note that while we focus on DNNs, a large amount of the\nsafety concerns discussed in this paper may also be valid for other types\nof ML-based methods.\nsafety, as discussed in [5]. However, several works exist\nwhich deﬁne requirements or safety criteria such a com-\nponent needs to fulﬁll.\nIn [3], the authors derive safety criteria for neural net-\nworks from an abstract top-level goal. Thereby, the posed\ngoals and criteria are on a purely functional level outlined\nin a Goal Structuring Notation (GSN). Following this line\nof work, Burton et al. [6] and Gauerhof et al. [11] propose\na systematic approach using GSN in order to argue the\nsafety of ML-based components. In contrast to Kurd et\nal. [3], they put the focus already more on the speciﬁc is-\nsues of ML models. In their work, they formulate require-\nments for an ML model derived from the discussed safety\nconcerns. Furthermore, they discuss potential sources of\nevidences for the constructed assurance case.\nIn a further work, Burton et al. [9] propose an approach\nfor constructing an argumentation for the safety of an ML\nmodel which they term performance evidence conﬁdence.\nThe approach is based on a design-by-contract principle\nof the safety argumentation which in turn uses safety con-\ntracts. These safety contracts provide certain guarantees\nif a deﬁned set of assumptions hold.\nAnother work that deals with this topic is given by\nAdler et al. [10]. Here, the authors extract areas of ac-\ntivity by a systematic literature search. Based on this,\nchallenges regarding the use of DNNs in safety critical\napplications are listed and methods which might help to\novercome them are mapped. However, the validity of the\nlist as well as the effectiveness of the mapped methods\nremains to be shown.\nIn this work, we seek to expand the discussion about\nsafety concerns with regard to the usage of DNNs in\nsafety-critical perception tasks.\nFurthermore, we con-\ncretize these concerns including root causes and discuss\npotentials as well as limitations of possible mitigation ap-\nproaches.\n3\nDeﬁnitions\nBefore going into the details of safety concerns, we ﬁrst\ngive deﬁnitions for the most crucial terms used in this pa-\nper.\nA Deep Neural Network (DNN) is a machine learning\nmodel which is made up of layers. The layers may be\neither connected in a feedforward or recurrent fashion.\n2\nEach layer takes some form of data as input, processes\nit, applies a so-called activation function and then outputs\nthe result. This output may in turn be used by other layers\nas input. The output of the ﬁnal layer is used as the predic-\ntion of the DNN. In most use-cases arising for DNNs in\nthe context of highly automated driving, the DNN is asked\nto predict a conditional probability p(Y = y | X = x). In\nother words, the DNN is tasked to predict the posterior\nprobability for a dependent random variable Y (e.g., class\nprobabilities) based on the independent random variable\nX (e.g., input images). For this, one needs to specify the\nexpected type of distribution of Y . This is important as\nthe DNN needs to be equipped with a so-called link func-\ntion which maps to the correct range of Y . If, for exam-\nple, one wants to perform classiﬁcation with the DNN,\nY is typically expected to follow a multinomial distribu-\ntion. In this case, the link function of choice is the well-\nknown softmax. As X and Y are unknown, the typical\napproach for obtaining a good DNN model is to record\na dataset D = {(xi, yi)}N\ni=1 with realizations of X and\nY and perform maximum-likelihood estimation of the pa-\nrameters with respect to the data. Here, xi is a data sam-\nple (e.g., camera image) and yi the corresponding annota-\ntion(s) (e.g., bounding boxes of objects to be detected). In\npractice, optimization is typically achieved by minimiz-\ning the value of the negative log-likelihood function using\n(stochastic) gradient descent. The negative log-likelihood\nfunction is commonly referred to as loss function in this\ncase.\nAccording to ISO PAS 21448, functional insufﬁcien-\ncies are insufﬁciencies inherent in the system possibly\nleading to hazards. Such an insufﬁciency can appear, e.g.,\nin form of a performance limitation leading to an incom-\nplete or wrong perception of the environment. A func-\ntional insufﬁciency can be unveiled under some condi-\ntions. A set of such conditions is referred to as a trig-\ngering event. In particular, considering a DNN module in\nthe perception pipeline of an ADAS or AD system, such\nan event can provoke an erroneous output (see Figure 1)\npossibly causing a hazardous behavior of the system.\n4\nSafety Concerns\nWe deﬁne safety concerns (SCs) (or simply concerns) as\nunderlying issues which may negatively affect the safety\nTriggering event\n+\nFunctional\nInsufﬁciencies\nSafety\nConcerns\nError\nFigure 1: The relation between safety concerns, func-\ntional insufﬁciencies, triggering event, and error. Con-\ncerns potentially lead to insufﬁciencies inherent in the\nfunction. Together with a triggering event, a functional\ninsufﬁciency provoke an erroneous output of the function.\nof a system. They are either the direct root of a functional\ninsufﬁciency or describe a black-box-like characteristic of\nthe system which in turn makes it hard to assess safety.\nSafety concerns are usually tied to subcomponents of the\nsystem. In particular, there exist speciﬁc concerns when\ndeploying a DL algorithm in an ADAS or AD vehicle.\nThe concerns which turn into functional insufﬁciencies\noriginate from the inherent design of DL methods. In gen-\neral, a supervised DL algorithm tries to extract the joint\nprobability distribution p(X, Y ) [12]. As the distribution\nis inherently unknown, the only option is to approximate\nit through a dataset D and extract the characteristics of\nthe distribution from the dataset. The algorithm produces\nincorrect results, if its approximation of the underlying\ndistribution p is not good enough at a given data point.\nThe concerns relating to black-box-characteristicsorig-\ninate from DL-speciﬁc properties. DL algorithms usu-\nally project the input data into high-dimensional spaces\nwhich cannot be entirely interpreted by a human anymore.\nWhile it is, for example, well known that classiﬁcation-\nbased DL methods partition their input space into non-\nconvex subspaces, giving semantic meaning to these sub-\nspaces is largely impossible.\nIn the following, we will describe the safety concerns\nof DL algorithms in an AD perception pipeline in detail.\nData distribution is not a good approximation of\nreal world (SC-1) The ﬁrst overarching concern is that\nthe distribution of the data used in development might\nnot be a good approximation to the one of the real\nopen world which is a priori unknown. As mentioned\nbefore, the distribution meant here is on the level of\n3\ndata representations, which are high-dimensional and\nnon-intuitive.\nTherefore, we can only approach them\nfrom (or estimate them on) a semantic level by analyzing\ninﬂuencing factors such as daylight, object appearance\nand weather conditions. This is prone to incompleteness\nsince not all aspects important for the data representation\nmay be covered this way. Besides, the data collection\ncan have other shortcomings which are independent of\nthe level at which it is represented. Examples of such\nproblems are bias (e.g.\nover- or under-representation\nof certain factors) or disregarding effects related to\ndifferent physical deployments (e.g.\nvarying sensor\nposition and angle due to different system variants or\nmanufacturing tolerances). Training and testing a DNN\nwith data which do not sufﬁciently approximate the Op-\nerational Design Domain (ODD) will very likely lead to\nan insufﬁcient performance or robustness later in the ﬁeld.\nDistributional shift over time (SC-2) A DNN is\ntrained and tested at a certain point in time, e.g., during\ndevelopment. However, our world is changing continu-\nously. This means that even if we would train a “perfect”\nalgorithm, the probability distribution of the input data\nwill change over time (e.g., new vehicles with a different\nappearance will be released). Since such a change will\noccur naturally, this concern needs to be addressed by\nappropriate measures being effective over the product’s\nlifetime.\nIncomprehensible behavior (SC-3) One of the main\ndifﬁculties in arguing the safety of DNNs is our inability\nto explain exactly how they come to a decision.\nIn\nother words, the non-linearity and complexity of DNNs\nis a double-edged sword; on the one hand, it enables\nthem to automatically extract features and relate those\nto outputs via non-linear activation functions, which, in\nturn, makes them so suitable for solving non-speciﬁable\nproblems. On the other hand, those features and their\nconnection to the outputs are rather counterintuitive and\nincomprehensible for us. Therefore, unlike in the case\nof rule-based functions, it is hardly possible to derive\na causal relation between the data representation and\npredictions of the network.\nConsequently, identifying\nweaknesses and failure causes of DNNs is difﬁcult\nand sometimes infeasible, impeding the applicability\nof common safety engineering methods (e.g., fault tree\nanalysis, common cause analysis, etc.).\nUnknown behavior in rare critical situations (SC-4)\nThis concern is directly related to the well-known long-\ntail problem in the context of AD. The long-tail problem\ndescribes the fact that there exists an enormous amount of\nscenarios that have a low occurrence probability. These\nscenarios may however be safety-critical. If one wants\nto test them, it would require a practically impossible\namount of driving hours to capture them by chance.\nRegarding this issue, two important aspects need to be\nmentioned: ﬁrst, note that according to the statistical\nlearning theory, the performance of an ML algorithm\nevaluated on a test data set can only be generalized if\ntest data, training data and the data which the function\nis facing later in the open world are independent and\nidentically distributed (i.i.d.) samples out of the same\nprobability distribution [12].\nThus, it might be prob-\nlematic to artiﬁcially insert such scenarios in the test\ndata used to estimate the generalization capability of\nDNN’s performance.\nSecond, even though one could\ndeﬁne a separate dataset in order to test the function\nwith respect to such data, it is hardly possible to identify\na rare critical situation from the perspective of a DNN\na priori.\nThis is due to the fact that DNNs do not\nlook at semantic content but rather the data itself (see\nsection 1). This makes it very difﬁcult to deﬁne appropri-\nate test cases in advance in order to deal with this concern.\nUnreliable conﬁdence information (SC-5) In prac-\ntice, DNNs will be faced with input data for which they\ncannot make an accurate prediction.\nThis may either\nstem from an insufﬁcient amount or representativeness\nof training data or an inherent uncertainty in the data\nitself (e.g., motion blur).\nIdeally, the DNN should\nreliably indicate if its prediction can be trusted or not.\nThis behavior would allow for a number of established\nsafety approaches to be used for a DNN component\nsuch as giving more weight to parallel information paths,\ninitiating an emergency maneuver or a driver handover.\nMost DNN algorithms used in practice output some form\nof posterior probability (e.g., class probabilities in case\nof a classiﬁer) and one may be tempted to use the value\nof the highest probability or the information entropy as\na measure of conﬁdence. This may, however, be highly\ncritical if the probabilities are not well calibrated. In par-\n4\nticular, it has been shown that DNNs using the standard\nmultinomial cross entropy loss in combination with the\nsoftmax as link function tend to be overconﬁdent in their\npredictions [13]. Even worse, it can be shown that if these\nDNNs use Rectiﬁed Linear Units (ReLUs) as activation\nfunctions they can produce arbitrarily high posterior\nprobabilities when dealing with data far away from the\ntraining data [14].\nWhile conﬁdence information may\nnot beneﬁt the solution of the problem itself, it serves\nas a vital enabler of a safety argument in a respective\nsafety case.\nFor example, well calibrated conﬁdence\ninformation may be used in a multi-sensor system to fuse\nconcurrent predictions from different sensors.\nBrittleness of DNNs (SC-6) As shown by many\nworks, the brittleness of DNNs is a major safety con-\ncern.\nThis includes the robustness against common\nperturbations such as noise or certain weather conditions,\ne.g. [15], and translations/rotations, e.g. [16], as well as\ntargeted perturbations known as adversarial examples,\ne.g. [17], [18]. Note that regarding adversarial examples,\nthe so-called adversarial patches are of special interest\nin the context of ADAS and AD (e.g., [19]–[21]). This\nis due to the fact that a would-be attacker can simply\nchange the operation environment of a vehicle instead of\nhaving to hack into the vehicle itself. Physical adversarial\npatch-based attacks do thus scale considerably better than\nthose based on overlaying the raw sensor data recorded\nin a vehicle with noise.\nInadequate separation of test and training data\n(SC-7) Another concern is that test data might be in-\nadequately separated from training data.\nFor training\nand testing DNNs, the data is usually divided into\ntraining, validation and test datasets.\nIn order not to\noverestimate the DNN’s performance, the test dataset\nneeds to be (sufﬁciently) uncorrelated to the other ones.\nHowever, in practice, highly correlated data is usually\nacquired because, e.g., data is recorded in sequences (i.e.\nconsecutive frames are rather similar) or data is recorded\nat the same locations several times. Another aspect is\nthat developers tend to optimize on test datasets during\ntraining because they strive for the maximum perfor-\nmance which is measured on this test data. Therefore, a\ntraining process is continued until performance goals of\na network are met on the test dataset. Good and labeled\ndata is expensive and thus, rare in practice, but using a\ntest dataset several times means also an optimization with\nrespect to the test data leading to an overestimation of a\nDNN’s performance.\nDependence on labeling quality (SC-8) In the case\nof supervised learning, labeled datasets are required for\ntraining and testing a DNN.\nNotice that the labeling,\nwhich is typically done manually, and its quality directly\naffect the resulting function and therefore, the obtained\ntest results as shown, e.g., in [22]. In particular, if the\nlabel quality is not sufﬁcient, the results obtained during\ntesting may be misleading. As a result, the function could\nhave an insufﬁcient performance later in the ﬁeld. Hence,\nthe labeling quality needs to be ensured in order to argue\nthe safety of such a learning function.\nInsufﬁcient consideration of safety in metrics (SC-9)\nUsing state-of-the-art metrics such as mean average\nprecision and false positive/negative rate, only the av-\nerage performance of DNNs is evaluated. Additionally,\nwhen assessing the performance of a DNN, typically\nall elements of a test dataset inﬂuence the performance\nmetric. There may, however, be elements which the DNN\npredicted incorrectly but would not impact the system\nitself. For example, consider the case of a DNN used\nfor pedestrian detection which serves the function of\nan automated emergency brake. If the car is driving at\n30 kph and fails to detect a pedestrian at 500 m distance,\nthis will in all likelihood not have an impact on the\nsafety of the system. However, in common metrics, such\na person will be counted in the same way as a person\nstanding directly in front of the car. This will inevitably\nlead to giving the DNN a worse safety rating than is\nactually the case.\n5\nPotential Mitigation Approaches\nReleasing an ADAS or AD system requires a comprehen-\nsive argumentation to show that all concerns related to the\nsystem’s safety are identiﬁed, understood and mitigated.\nAfter having discussed the safety concerns regarding the\nuse of DNNs within such systems in section 4, we present\nseveral promising mitigation approaches (MAs) which\n5\ncould be used in order to provide supporting arguments\nand evidences for a safety case.\nWell-justiﬁed\ndata\nacquisition\nstrategy\n(MA-1)\nThe basis for testing ML functions is an appropriate\ndataset reﬂecting the context in which the function is\nsupposed to work.\nIn particular, one needs to argue\nthat the dataset used is a suitable representation of the\ndata which the DNN will be faced within the ODD. As\npointed out before, the distribution which is relevant here\nis on the level of the data representations (e.g., pixel-level\ndistribution).\nFinding suitable random samples from\nthis distribution is - in most cases - highly non-trivial,\nmainly due to the dimensionality of the data. Thus, we\npropose to follow a two-step approach here.\nThe ﬁrst\nstep is to specify the data content, as well as the data\nacquisition and selection process, in a structured and\nthorough manner. For this, essential ODD factors such as\nweather conditions, road types, occurring objects as well\nas their variations in the ODD need to be determined,\nsee e.g., [23]. Additional factors such as tolerances in\nthe mounting positions of the sensors and predictable\nchanges over the product’s lifetime (e.g. sensor aging)\nshould be considered as well. Finally, the existence of\nspeciﬁed variations and their frequencies in the acquired\ndata need to be veriﬁed.\nThe aforementioned analysis happens on a semantic\nlevel and may not fully cover the speciﬁcs of the data at\nhand (e.g., certain biases in the pixel distribution of an\nimage). Thus, the second step is to analyze the raw data\nand ﬁnd suitable datapoints which are missing from the\nﬁrst step. This can, for example, be achieved by ﬁnding\na latent representation of the data using a variational\nautoencoder and sampling the latent space in a suitable\nmanner.\nEnabling the output of reliable conﬁdence infor-\nmation (MA-2) As explained before, the posterior\nprobability predicted by a DNN tends to be overconﬁdent\neven for inputs close to the training data [13] and may be\narbitrarily high when moving away from the training data\n[14]. In order to be able to output reliable conﬁdence\ninformation, a number of approaches have been proposed.\nIn [13] a number of heuristic approaches are evaluated,\nwhich either make use of the logit or posterior probability\noutputs in order to calibrate the output probabilities and\nin turn allow them to be used as a reliable measure of\nconﬁdence.\nBesides heuristics, other approaches have made use of\nBayesian methods in order to extract uncertainties. In [24]\nthe authors use dropout during inference which turns their\nneural network into a Bayesian model with the weights\nbeing represented by Bernoulli distributions. They show\nthat when dropout is used at inference time, one approx-\nimately marginalizes over the weights of the neural net-\nwork using Monte Carlo integration. This approach is\nhence termed Monte Carlo Dropout. Another Bayesian\napproach is presented by Blundell et al. [25].\nHere,\nthe authors model the weights of the neural network us-\ning Gaussian distributions and minimize the ELBO loss.\nThey achieve this using also Monte Carlo integration to\napproximately marginalize over the weights of the neural\nnetwork.\nBesides the actual method itself, it is still an open\nquestion how one can determine if a measure of conﬁ-\ndence is reliable or not in the context of AD. In [26] the\nuse of expected calibration error (ECE) and maximum\ncalibration error (MCE) is proposed.\nBoth metrics\noperate on the probabilities predicted from the neural\nnetwork.\nFirst, the maximum posterior probability is\nquantized into a desired number of bins for a test dataset.\nThen, the accuracy is computed for each.\nGenerally,\nthe outputs are well-calibrated if the accuracy of each\nbin is equal to the average probability in this bin. The\ndifference in these two values is called calibration error.\nWhile for ECE the calibration error is averaged over all\nbins, MCE simply returns the largest calibration error.\nHowever, a main drawback of both ECE and MCE is that\nboth metrics depend on a parameter, namely the number\nof bins. This parameter heavily inﬂuences the obtained\nresult.\nUsing gray-box methods (MA-3).\nA major imped-\niment to the safety argumentation of DNNs is their\nblack-box character SC-3.\nEven though turning the\nblack-box to a white-box will be scarcely possible in the\nforeseeable future, several methods were introduced re-\ncently to gain understanding of the root causes for DNN’s\npredictions by visualizing decisive parts of the input (e.g.,\ngradient-weighted class activation mapping [27]) or by\nforcing the DNN to provide more interpretable outputs\n(e.g., object attributes [28]). While these methods cannot\n6\nenable an analytical safety evaluation, they still can\ncontribute to a safety case, e.g., by making the analysis\nof a test result more meaningful or by supporting the\nextraction of uncertainties for DNN’s prediction (e.g., by\nanalyzing the distribution of decisive parts of an image\nwith respect to certain object classes).\nNote that the\ntrustworthiness of such methods needs to be shown which\nis indeed a non-trivial task as well.\nSpeciﬁcation of adversarial threat models and in-\ncorporation of defense methods (MA-4) Before being\nable to defend against adversarial examples, one must\nﬁrst determine a threat model, which in essence repre-\nsents an assumption on what a possible attacker is capable\nto perform as an attack.\nMost of the current work in\nadversarial examples focuses on data-level threat models,\nmeaning that an attacker is allowed to change the values\nof a given datum. For example, in computer vision-based\nproblems this is typically achieved by changing the\nvalues of pixels in an image at arbitrary locations. This\nkind of threat model typically involves some form of\nbudget that may not be exceeded, e.g., the difference in\npixel values between an original image and an adversarial\nexample may not exceed a certain amount, oftentimes\nmeasured in either l2 or l∞norm (e.g., [18], [29],\n[30]). Other data-level threat models include adversarial\npatches [31] or afﬁne transformation-based attacks [32].\nOf course, allowing data-level changes may oftentimes\nbe an unrealistic or highly improbable threat model. For\nexample, in the case of autonomous vehicles, an attacker\nwould need access to the pixel buffer in order to alter\npixel values. This form of attack does not scale well and\nis thus probably a neglectable threat model. However,\nthere exist a number of techniques, which are known as\nphysical adversarial examples, that do scale well. Here,\nthe environment in which a datum is recorded is altered\ninstead of the datum itself. Common techniques for this\ninclude sticker-based attacks which can either be applied\nto objects in the environment (e.g., [19], [21], [33]) or\nbe used to partially occlude the sensor which is used for\nrecording the data (e.g., [34]).\nThere exist many other threat models which have not\nbeen listed so far2. In general, there exists no model which\nmay be assumed by default. In the future, there might be\n2For a concise overview of common threat models see, e.g., [35].\nstandards and norms which deﬁne an appropriate model\nfor a given domain (e.g., physical based attacks for AD).\nHowever, in the meantime the choice of threat model must\nbe made on a per-case basis and argued accordingly.\nHaving chosen and argued for a speciﬁc threat model,\none has to deploy defense mechanisms which protect\nagainst falling victim to adversarial examples. The main\nproblem with most known defense mechanisms is that\nthey may have given good results initially but were\nquickly exposed after having been published. This has\nbeen the fate of distillation-based defenses [36] (exposed\nin [37]), defenses based on transforming the input such as\nJPEG compression [38] (exposed in [39]) and gradient-\nobfuscation methods [40] (exposed in [39]). As of writ-\ning this paper, there only exist two approaches for defend-\ning against adversarial examples, which are effective to at\nleast a certain degree and are somewhat accepted in the\nML community3. First, there is an empirical approach\nknown as adversarial training with PGD adversaries [29].\nThis method tries to optimize a DNN to predict the cor-\nrect class for a given sample’s strongest adversarial ex-\nample. While this approach is not able to guarantee that it\nactually ﬁnds the strongest adversary under a given threat\nmodel, it is very ﬂexible with respect to the model actually\nused. For example, the commonly used bounded pixel-\nlevel threat model can be easily replaced by other models\nsuch as rotation- or sticker-based attacks. The second ap-\nproach uses a convex outer approximation of reachable\nactivations of the ReLU units of a neural network to de-\nfend against adversarial examples [41]. This method can\ngive guaranteed lower bounds on the loss values of adver-\nsarial examples. A drawback of this analytic method is\nthat the training procedure takes considerably longer than\nstandard SGD training4.\nBeside making the network itself more robust, other\napproaches aim at detecting adversarial attacks, e.g.,\nby using a trained subnetwork [30].\nEven though the\nDNN would still be fooled by the attack in this case, the\ninformation that the DNN’s prediction is not trustworthy\nat this moment could trigger an appropriate system reac-\ntion preventing harm. However, such a detector-network\ncould be attacked as well which means that its robustness\n3Defending against adversarial examples is currently a heavily re-\nsearched topic and there may exist other effective methods.\n4There have been improvements in the training time of this method\nin order to scale to larger datasets, see [42].\n7\nneeds to be argumented too.\nTesting (MA-5) Naturally,\na key component of a\nsafety argumentation is testing usually including veriﬁ-\ncation and validation activities. While veriﬁcation rather\naddresses issues which are already known or foreseeable\n(e.g., lack of robustness against certain perturbations),\nvalidation focuses on identifying unknown issues. In the\nfollowing, we will refer to mitigation approaches that\naddress these issues as MA-5a and MA-5b respectively.\nMA-5a: Known or predictable critical cases can be as-\nsessed via targeted testing. This approach supports miti-\ngating SC-1, SC-4 and SC-6. The selection of test data is\nkey for a thorough analysis of DNNs. One of the meth-\nods for identifying targeted test cases is HAZOP (Haz-\nard&Operability, [43]). It is a standard safety procedure\nused to systematically identify malfunctions and risks of\na complex system. In [44], the authors adapt HAZOP to\ncomputer vision systems and provide a catalog containing\nan extensive set of known critical situations for computer\nvision tasks as a basis for assessing the quality and thor-\noughness of test data. Of special interest is the stability of\nDL algorithms with respect to certain effects in the input\nspace (e.g. blur, windscreen smudges or exposure related\neffects). As highlighted by Zendel et al. [45], the evalua-\ntion of robustness requires a targeted addition of difﬁcult\nsamples into a test dataset. A benchmark for robustness\nagainst known corruptions and perturbations is introduced\nin [15]. Another approach for effectively testing DNN\nalgorithms is search-based-testing [46]. This technique\naims at exploring the input space in a targeted manner en-\nabling, e.g., a sensitivity analysis with respect to certain\nODD factors or different combinations of them. Note that\nwhile some of the approaches mentioned can make use of\nreal data (recorded on public roads or test tracks) others\nrequire artiﬁcially generated data. Thus, it is important\nto mention that for obtaining reliable test results on syn-\nthetic data, the validity of this data with respect to real\ndata has to be shown. This is, in turn, a highly non-trivial\nproblem.5.\nMA-5b: The unknown and unpredictable problems as-\nsociated with deploying DNNs in a safety-critical open-\nworld context can only be identiﬁed by chance. For this\n5Even though synthetic data may look “realistic” to a human, the\ndata-level distribution may be signiﬁcantly different leading to non-\nmeaningful test results.\npurpose, ﬁeld test data need to be collected randomly in\naccordance to the guidelines mentioned in MA-1.\nSuch a testing mainly addresses SC-4, but also supports\nthe mitigation of SC-6, by providing a means for ﬁnding\npreviously unknown safety-critical situations6.\nNote\nthat the open-context nature of the operational domain,\nrenders the coverage of the entire problem space via\nbrute-force approaches practically infeasible.\nInstead,\none needs to combine ﬁeld testing with other methods,\nas pointed out in this paper, to enable the release of such\nsystems.\nDeep analysis of test results obtained in an iterative\ndevelopment process (MA-6).\nAs is known, DL is\na data-driven approach and its development should be\npursued in an iterative way.\nDiscovered weaknesses\nof the DL component are continuously mitigated by\noptimizing architectures and hyperparameters or by\nadding new data that covers previously missing aspects.\nHence, a fundamental part of this process is analyzing\nthe intermediate results, ideally leading to a continuous\nimprovement. In order to extract as much information\nas possible from these results, the analysis should be\nperformed in a structured, careful, and if possible, auto-\nmated manner (e.g., by extracting systematic weaknesses\nfrom comprehensive metadata by which the data should\nbe enriched beforehand).\nIn addition to cases where\nthe DL component makes wrong predictions, cases\nassociated with high uncertainty should be considered.\nThis is important because even though the function\nmight have been correct “at random”, it could lead to\nwrong predictions and therefore, cannot be ignored. This\napproach can contribute to the mitigation of SC-4 and\nSC-6.\nData partitioning guidelines (MA-7) In order to\naddress SC-7 and estimate a DNN’s performance cor-\nrectly, guidelines regarding partitioning the data into\ntraining, validation and test datasets are necessary.\nIn\nparticular, test data must not be correlated with training\ndata since otherwise the generalization capability of\nthe ML algorithm will be overestimated.\nThis means\nthat, e.g., consecutive frames of a video sequence may\n6It is important to note that for reasons described in SC-7, the test\nset used for the ultimate performance evaluation needs to remain unseen\nuntil ﬁnal testing.\n8\nTable 1: Overview of safety concerns and associated mitigation approaches.\nSafety concern\nMitigation approaches\nData distribution is not a good approxi-\nmation of real world (SC-1)\nWell-justiﬁed data acquisition strategy (MA-1), enabling the output of\nreliable conﬁdence information (MA-2), testing (MA-5), deep analysis\nof test results obtained in an iterative development process (MA-6),\nlabeling guidelines (MA-8)\nDistributional shift over time (SC-2)\nEnabling the output of reliable conﬁdence information (MA-2), contin-\nuous learning and updating (MA-10)\nIncomprehensible behavior (SC-3)\nUsing gray-box methods (MA-3)\nUnknown behavior in rare critical situa-\ntions (SC-4)\nWell-justiﬁed data acquisition strategy (MA-1), enabling the output of\nreliable conﬁdence information (MA-2), testing (MA-5), deep analysis\nof test results obtained in an iterative development process (MA-6),\ncontinuous learning and updating (MA-10)\nUnreliable\nconﬁdence\ninformation\n(SC-5)\nEnabling the output of reliable conﬁdence information (MA-2), using\ngray-box methods (MA-3), testing (MA-5)\nBrittleness of DNNs (SC-6)\nEnabling the output of reliable conﬁdence information (MA-2), speciﬁ-\ncation of adversarial threat models and incorporation of defense meth-\nods (MA-4), testing (MA-5), deep analysis of test results obtained in\nan iterative development process (MA-6), continuous learning and up-\ndating (MA-10)\nInadequate separation of test and training\ndata (SC-7)\nData partitioning guidelines (MA-7)\nDependence on labeling quality (SC-8)\nLabeling guidelines (MA-8)\nInsufﬁcient consideration of safety in\nmetrics (SC-9)\nEvaluating performance with respect to safety (MA-9)\nnot be assigned to different partitions.\nFurther mea-\nsures could be that test data needs to be acquired at\ndifferent days and locations as training data. Such guide-\nlines need to be well-justiﬁed and the partitioning needs\nto be subsequently reviewed with regard to the guidelines.\nLabeling\nguidelines\n(MA-8)\nThe\ndependence\nof\nsupervised learning methods on well-labeled data (see\nSC-8 in section 4) requires strict labeling guidelines\nand checks.\nThe guidelines should be deﬁned with\nrespect to the speciﬁc task (e.g. semantic segmentation\nor object detection) and should ideally contain additional\napplication-speciﬁc annotations in order to enable an\nautomated evaluation, e.g., of the relative frequencies of\nODD factors such as weather conditions, object-speciﬁc\nmetadata, etc. Guidelines compilation has to be justiﬁed\nand the adherence to them needs to be reviewed. Appro-\npriately performed, this mitigates SC-8 and supports the\nargumentation with respect to SC-1.\nEvaluating\nperformance\nwith\nrespect\nto\nsafety\n(MA-9). As pointed out above, current state-of-the-art\nperformance metrics in machine learning are calculating\naverage values not considering safety with respect to a\ncertain function (e.g., automated emergency brake) SC-9.\nRealizing that it will not be possible to reach 100%\nperformance, it is obvious that a safety argumentation is\nhardly possible based on these metrics. However, con-\nsidering an object detection component in the perception\nof an AD vehicle, it is actually not necessary to assure\n9\nthat all objects are detected but all the objects which\nare relevant with respect to system safety. Additionally,\none could further reﬁne that all relevant objects need to\nbe detected or a low conﬁdence value needs to indicate\nthat the DNN might be wrong such that the system can\nmanage the situation safely (e.g., by relying more on\nother information paths).\nAnother important aspect is\nthe analysis of errors over time.\nIf one considers, for\nexample, an object detection network, missing an object\nin one single frame might not be problematic at all\nbecause this can be compensated, e.g., by state-of-the-art\nobject tracking methods or by plausibility checks (e.g.,\na pedestrian will probably not disappear within a few\nmilliseconds). But if an object is not detected in several\nconsecutive frames, the severity of the error is much\nhigher. Therefore, tailored evaluation metrics are neces-\nsary in order to meaningfully assess DNNs from a safety\nperspective.\nContinuous\nlearning\nand\nupdating\n(MA-10)\nIn\norder to maintain the safety of a DNN-based component,\nthe open context and distributional shift over time\nproblems (issued in SC-4 and in SC-2 respectively)\nneed to be addressed in the product’s life cycle.\nIn\nparticular, the DNN could face novel inputs in which the\nparameter distribution (e.g.\npixel values in an image)\ndiffer from that of the data seen during development.\nThis can occur either because the difference oversteps\nthe generalization abilities of the network (long-tailed\nopen context) or the input includes something completely\nnew (e.g. a new type of vehicle) which has not existed\nbefore (temporal distributional shift) possibly leading to\nhazards. Therefore, it may be necessary to continually\ndevelop the algorithm further and updating it. Note that\ncontinuous learning does not necessarily mean online\nlearning of the DNN already applied in the vehicle.\nWhile this approach is generally possible, it comes with\nits own speciﬁc problems, namely continuous validation\nof the newly learned model in the car with only minimal\ncomputation power as well as weak to no supervision.\nContinuous learning as proposed here includes an ofﬂine\ndevelopment step.\nNew and useful data is recognized\nby a DNN or some other mechanism and send back to\nan ofﬂine data center where a new version of the DNN\nis trained and validated.\nFinally, the old DNN in the\nADAS or AD vehicle is replaced with the new one, either\nthrough software-over-the-air solutions or in a workshop.\nThis process ensures the in-use DNN to be up-to-date\nwhile still having the ability to make use of large scale\ncomputation power for validation.\n6\nConclusion\nIn this work, we have presented a concise list of safety\nconcerns regarding deep learning methods used in percep-\ntion pipelines of autonomous agents, especially highly au-\ntomated vehicles. We also presented an extensive discus-\nsion on possible mitigation approaches addressing those\nsafety concerns (the mapping is presented in table 1). It is\nimportant to note that the discussed approaches have very\ndifferent maturity and complexity. Furthermore, while\nall of the approaches can deﬁnitely contribute to a safety\ncase, for the time being it remains an open question when\na speciﬁc safety concern is sufﬁciently mitigated. In par-\nticular, many of the mitigation methods involve parame-\nters for which there does not exist a single correct value.\nFor example, some methods supply a key performance in-\ndicator (KPI) telling the user how well the deep learning\nalgorithm under test performed with respect to this KPI.\nHowever, the threshold for this KPI used to determine\nwhether the deep learning algorithm is safe cannot be ob-\ntained analytically in many cases. Thus, it is essential to\ncollect knowledge and consolidate this in standardization\nactivities in order to deﬁne suitable processes, practices\nand thresholds.\nAcknowledgments\nParts of the research leading to the results presented above\nare funded by the German Federal Ministry for Economic\nAffairs and Energy within the project “Safe AI - Meth-\nods and measures for safeguarding AI-based perception\nfunctions for automated driving”. We would like to thank\nthe consortium for the successful cooperation. In partic-\nular, we would like to thank Peter Schlicht and Christian\nHellert for reviewing our work and their thoughtful com-\nments.\n10\nReferences\n[1]\nInternational Standards Organisation (ISO), Road\nvehicles - functional safety (ISO 26262), 2018.\n[2]\n——, Road vehicles — safety of the intended func-\ntionality (ISO/PAS 21448), 2019.\n[3]\nZ. Kurd and T. Kelly, “Establishing Safety Crite-\nria for Artiﬁcial Neural Networks”, in Knowledge-\nBased Intelligent Information and Engineering\nSystems, G. Goos, J. Hartmanis, J. van Leeuwen, V.\nPalade, R. J. Howlett, and L. Jain, Eds., vol. 2773,\nBerlin, Heidelberg: Springer Berlin Heidelberg,\n2003, pp. 163–169.\n[4]\nD. Amodei, C. Olah, J. Steinhardt, P. F. Christiano,\nJ. Schulman, and D. Man´e, “Concrete Problems in\nAI Safety”, ArXiv, 2016.\n[5]\nK. R. Varshney, “Engineering Safety in Machine\nLearning”, Information Theory and Applications\nWorkshop, 2016.\n[6]\nS. Burton, L. Gauerhof, and C. Heinzemann,\n“Making the Case for Safety of Machine Learn-\ning in Highly Automated Driving”, in Computer\nSafety, Reliability, and Security, 2017, pp. 5–16.\n[7]\nR. Salay, R. Queiroz, and K. Czarnecki, “An anal-\nysis of iso 26262: Machine learning and safety in\nautomotive software”, in WCX World Congress Ex-\nperience, SAE International, 2018.\n[8]\nM. Gharib, P. Lollini, M. Botta, E. Amparore,\nS. Donatelli, and A. Bondavalli, “On the Safety\nof Automotive Systems Incorporating Machine\nLearning Based Components: A Position Paper”, in\nInternational Conference on Dependable Systems\nand Networks Workshops, 2018, pp. 271–274.\n[9]\nS. Burton, L. Gauerhof, B. B. Sethy, I. Habli,\nand R. Hawkins, “Conﬁdence Arguments for\nEvidence of Performance in Machine Learning\nfor Highly Automated Driving Functions”, in\nComputer Safety, Reliability, and Security, 2019,\npp. 365–377.\n[10]\nR. Adler, M. N. Akram, P. Bauer, P. Feth, P. Gerber,\nA. Jedlitschka, L. J¨ockel, M. Kl¨as, and D. Schnei-\nder, “Hardening of Artiﬁcial Neural Networks for\nUse in Safety-Critical Applications - A Mapping\nStudy”, ArXiv, 2019.\n[11]\nL. Gauerhof, P. Munk, and S. Burton, “Structuring\nValidation Targets of a Machine Learning Func-\ntion Applied to Automated Driving”, in Computer\nSafety, Reliability, and Security, B. Gallina, A.\nSkavhaug, and F. Bitsch, Eds., Springer Interna-\ntional Publishing, 2018, pp. 45–58.\n[12]\nO. Bousquet, S. Boucheron, and G. Lugosi, “In-\ntroduction to Statistical Learning Theory”, in Ad-\nvanced Lectures on Machine Learning: ML Sum-\nmer Schools 2003, 2004, pp. 169–207.\n[13]\nC. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger,\n“On Calibration of Modern Neural Networks”,\nArXiv, 2017.\n[14]\nM. Hein, M. Andriushchenko, and J. Bitterwolf,\n“Why ReLU networks yield high-conﬁdence pre-\ndictions far away from the training data and how\nto mitigate the problem”, in Computer Vision and\nPattern Recognition, 2019, pp. 41–50.\n[15]\nD. Hendrycks and T. Dietterich, “Benchmarking\nNeural Network Robustness to Common arXivup-\ntions and Perturbations”, in International Confer-\nence on Learning Representations, 2019.\n[16]\nM. A. Alcorn, Q. Li, Z. Gong, C. Wang, L. Mai,\nW.-S. Ku, and A. Nguyen, “Strike (with) a Pose:\nNeural Networks Are Easily Fooled by Strange\nPoses of Familiar Objects”, in ARXIV, 2018.\n[17]\nC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna,\nD. Erhan, I. Goodfellow, and R. Fergus, “Intrigu-\ning properties of neural networks”, in International\nConference on Learning Representations, 2014.\n[18]\nI. Goodfellow, J. Shlens, and C. Szegedy, “Explain-\ning and Harnessing Adversarial Examples”, in In-\nternational Conference on Learning Representa-\ntions, 2015.\n[19]\nK. Eykholt, I. Evtimov, E. Fernandes, B. Li, A.\nRahmati, F. Tram`er, A. Prakash, T. Kohno, and D.\nSong, “Physical Adversarial Examples for Object\nDetectors”, in ARXIV, 2018.\n11\n[20]\nNir Morgulis, Alexander Kreines, Shachar Mende-\nlowitz, and Yuval Weisglass, “Fooling a Real Car\nwith Adversarial Trafﬁc Signs”, ArXiv, 2019.\n[21]\nM. Lee and J. Z. Kolter, “On Physical Adversarial\nPatches for Object Detection”, ArXiv, 2019.\n[22]\nC. Haase-Sch¨utz, H. Hertlein, and W. Wiesbeck,\n“Estimating Labeling Quality with Deep Object\nDetectors”, in Intelligent Vehicles Symposium,\n2019, pp. 33–38.\n[23]\nKoopman, P. and Fratrik, F., “How many opera-\ntional design domains, objects, and events?”, in\nWorkshop on AI Safety, 2019.\n[24]\nY. Gal and Z. Ghahramani, “Dropout as a Bayesian\nApproximation: Representing Model Uncertainty\nin Deep Learning”, in International Conference on\nMachine Learning, M. F. Balcan and K. Q. Wein-\nberger, Eds., vol. 48, 2016, pp. 1050–1059.\n[25]\nC. Blundell, J. Cornebise, K. Kavukcuoglu, and\nD. Wierstra, “Weight Uncertainty in Neural Net-\nworks”, in International Conference on Machine\nLearning, 2015, pp. 1613–1622.\n[26]\nM.\nPakdaman\nNaeini,\nG.\nCooper,\nand\nM.\nHauskrecht, “Obtaining Well Calibrated Probabil-\nities Using Bayesian Binning”, in Conference on\nArtiﬁcial Intelligence, 2015, pp. 2901–2907.\n[27]\nR. R. Selvaraju, M. Cogswell, A. Das, R. Vedan-\ntam, D. Parikh, and D. Batra, “Grad-cam: Vi-\nsual explanations from deep networks via gradient-\nbased localization”, in International Conference on\nComputer Vision, 2017, pp. 618–626.\n[28]\nC. H. Lampert, H. Nickisch, and S. Harmeling,\n“Attribute-Based Classiﬁcation for Zero-Shot Vi-\nsual Object Categorization”, Transactions on Pat-\ntern Analysis and Machine Intelligence, vol. 36, no.\n3, pp. 453–465, 2014.\n[29]\nA. Madry, A. Makelov, L. Schmidt, D. Tsipras, and\nA. Vladu, “Towards deep learning models resistant\nto adversarial attacks”, in International Conference\non Learning Representations, 2018.\n[30]\nJ. H. Metzen, T. Genewein, V. Fischer, and B.\nBischoff, “On detecting adversarial perturbations”,\nin Proceedings of 5th International Conference on\nLearning Representations, 2017.\n[31]\nT. B. Brown, D. Man´e, A. Roy, M. Abadi, and J.\nGilmer, “Adversarial Patch”, ArXiv, 2017.\n[32]\nL. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and\nA. Madry, “Exploring the Landscape of Spatial Ro-\nbustness”, in International Conference on Machine\nLearning, 2019, pp. 1802–1811.\n[33]\nK. Eykholt, I. Evtimov, E. Fernandes, B. Li, A.\nRahmati, C. Xiao, A. Prakash, T. Kohno, and D.\nSong, “Robust Physical-World Attacks on Deep\nLearning Models”, in COMPUTER Vision and Pat-\ntern Recognition, 2018.\n[34]\nJ. Li, F. R. Schmidt, and J. Z. Kolter, “Adversar-\nial camera stickers: A physical camera-based at-\ntack on deep learning systems”, ArXiv, 2019.\n[35]\nX. Yuan, P. He, Q. Zhu, and X. Li, “Adversarial\nExamples: Attacks and Defenses for Deep Learn-\ning”, Transactions on Neural Networks and Learn-\ning Systems, vol. 30, no. 9, pp. 2805–2824, 2019.\n[36]\nN. Papernot, P. McDaniel, X. Wu, S. Jha, and A.\nSwami, “Distillation as a Defense to Adversarial\nPerturbations Against Deep Neural Networks”, in\nSymposium on Security and Privacy, 2016.\n[37]\nN. Carlini and D. A. Wagner, “Towards evaluating\nthe robustness of neural networks”, IEEE Sympo-\nsium on Security and Privacy, 2017.\n[38]\nC. Guo, M. Rana, M. Cisse, and L. van der\nMaaten, “Countering Adversarial Images using In-\nput Transformations”, in International Conference\non Learning Representations, 2018.\n[39]\nA. Athalye, N. Carlini, and D. A. Wagner, “Ob-\nfuscated Gradients Give a False Sense of Secu-\nrity: Circumventing Defenses to Adversarial Ex-\namples”, in International Conference on Machine\nLearning, 2018, pp. 274–283.\n[40]\nJ. Buckman, A. Roy, C. Raffel, and I. Goodfellow,\n“Thermometer Encoding: One Hot Way To Resist\nAdversarial Examples”, in International Confer-\nence on Learning Representations, 2018.\n[41]\nE. Wong and J. Z. Kolter, “Provable Defenses\nagainst Adversarial Examples via the Convex\nOuter Adversarial Polytope”, in International Con-\nference on Machine Learning, 2018, pp. 5283–\n5292.\n12\n[42]\nE. Wong, F. Schmidt, J. Hendrik Metzen, and\nJ. Zico Kolter, “Scaling provable adversarial de-\nfenses”, in ARXIV, 2018.\n[43]\nT. A. Kletz, HAZOP & HAZAN: Notes on the Iden-\ntiﬁcation and Assessment of Hazards, ser. Hazard\nWorkshop Modules. Institution of Chemical Engi-\nneers, 1986.\n[44]\nO. Zendel, M. Murschitz, M. Humenberger, and\nW. Herzner, “CV-HAZOP: Introducing Test Data\nValidation for Computer Vision”, in IEEE Inter-\nnational Conference on Computer Vision, 2015,\npp. 2066–2074.\n[45]\nO.\nZendel,\nK.\nHonauer,\nM.\nMurschitz,\nD.\nSteininger, and G. Fernandez Dominguez, “Wild-\ndash - creating hazard-aware benchmarks”, in Eu-\nropean Conference on Computer Vision, 2018.\n[46]\nJ. M. Zhang, M. Harman, L. Ma, and Y. Liu, “Ma-\nchine Learning Testing: Survey, Landscapes and\nHorizons”, ArXiv, 2019.\n13\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2020-01-22",
  "updated": "2020-01-22"
}