{
  "id": "http://arxiv.org/abs/2001.10362v2",
  "title": "The Final Frontier: Deep Learning in Space",
  "authors": [
    "Vivek Kothari",
    "Edgar Liberis",
    "Nicholas D. Lane"
  ],
  "abstract": "Machine learning, particularly deep learning, is being increasing utilised in\nspace applications, mirroring the groundbreaking success in many earthbound\nproblems. Deploying a space device, e.g. a satellite, is becoming more\naccessible to small actors due to the development of modular satellites and\ncommercial space launches, which fuels further growth of this area. Deep\nlearning's ability to deliver sophisticated computational intelligence makes it\nan attractive option to facilitate various tasks on space devices and reduce\noperational costs. In this work, we identify deep learning in space as one of\ndevelopment directions for mobile and embedded machine learning. We collate\nvarious applications of machine learning to space data, such as satellite\nimaging, and describe how on-device deep learning can meaningfully improve the\noperation of a spacecraft, such as by reducing communication costs or\nfacilitating navigation. We detail and contextualise compute platform of\nsatellites and draw parallels with embedded systems and current research in\ndeep learning for resource-constrained environments.",
  "text": "The Final Frontier: Deep Learning in Space\nVivek Kothari†∗, Edgar Liberis†∗, Nicholas D. Lane†⋄\n†University of Oxford\n⋄Samsung AI\nAbstract\nMachine learning, particularly deep learning, is being increas-\ning utilised in space applications, mirroring the groundbreaking\nsuccess in many earthbound problems. Deploying a space device,\ne.g. a satellite, is becoming more accessible to small actors due\nto the development of modular satellites and commercial space\nlaunches, which fuels further growth of this area. Deep learning’s\nability to deliver sophisticated computational intelligence makes it\nan attractive option to facilitate various tasks on space devices and\nreduce operational costs. In this work, we identify deep learning\nin space as one of development directions for mobile and embed-\nded machine learning. We collate various applications of machine\nlearning to space data, such as satellite imaging, and describe how\non-device deep learning can meaningfully improve the operation\nof a spacecraft, such as by reducing communication costs or facili-\ntating navigation. We detail and contextualise compute platform of\nsatellites and draw parallels with embedded systems and current\nresearch in deep learning for resource-constrained environments.\n1\nIntroduction\nMachine learning scales to and thrives in data-abundant envi-\nronments making it well suited to applications in space. Satellite\nimagery is ubiquitous in space. Produced by both imaging the Earth\nand space (telescope satellites), it allows learned models to power\na range of monitoring tasks. Machine learning can also play an\nactive role in the operation of a spacecraft, allowing for precise\nautomated control and facilitating on-board tasks, such as docking\nor navigation.\nMachine learning’s impact in space applications will continue to\ngrow as cheaper satellite platforms mature and become more acces-\nsible to small actors, thus widening the range of possible activities\nin space. Similarly, hardware innovations from terrestrial systems,\nsuch as the multi-core design and specially-designed accelerators,\nare increasing the compute power available to spacecraft.\nThis increased accessibility of space platforms and space data\nis fuelling excitement in “space activity” as a new direction for ap-\nplied machine learning researchers with potentially transformative\nresults for future applications and satellite hardware. In particu-\nlar, recent advances in machine learning (ML) and deep learning\n(DL) in constrained environments [9, 26, 40, 46] would enable run-\nning neural networks on the spacecraft itself. Doing so will enable\nmany “smart” applications to be run in space autonomously (as\nthe communication channel between the spacecraft and ground\nstation is often limited), which is likely to have a similar impact and\ndevelopment trajectory to that of terrestrial smart devices, homes,\nembedded systems and mobile phones [? ].\nIn addition to challenges faced by DL for embedded systems,\nspace imposes extra requirements: the need for radiation-hardened\nhardware, robustness, and extensive verification. We discuss those\nchallenges, demonstrating how space is a similar and yet unique\nenvironment for DL applications, which makes applying develop-\nments from embedded systems non-trivial.\nIn this work, we argue that ML/DL in space is an important direc-\ntion for mobile and embedded computing (MEC) moving forward.\nWe summarise prominent applications of ML in space, give an in-\ndication of the range of compute and sensor capabilities of space\nhardware, and present pointers for future work, thus providing\nthe reader with the necessary information to start exploring this\narea. We quantitatively illustrate how DL based solutions can offer\ndouble the power efficiency compared to the current state of the\nart. ∗\n2\nDeep Learning Meets Space\nSpacecraft (vehicles designed for operation outside the earth’s\natmosphere) and satellites (objects that orbit a natural body) have\ntwo types of systems: payload, which comprises instruments that\nfaciliate the primary purpose of the spacecrafts; and operations\nsystems, which support the payload and allow it to reach, stay, and\nwork in space. This section will analyse current applications of ML\npertaining to both systems.\n2.1\nAnalysis of Payload Data\nEarth observation satellites in either geostationary (GEO) or\nLow Earth (LEO) orbits carry sensors ranging from RGB imagers\nfor cloud cover detection to more specialised sensors for other at-\nmospheric properties: temperature, humidity, wind vectors, and\ngaseous composition. Data, often from radiometric or spectral sen-\nsors, has traditionally been processed at ground station primarily\nusing classical ML and hand-crafted specialised algorithms.\nThe similarity of terrestrial and space sensor modalities makes\nML/DL well-suited for payload data. The following areas are only\na few that have seen remarkable success from such methods. We\nwill later (Sec. 4) quantitatively demonstrate how DL methods can\nhelp save power on satellites.\nWeather & atmospheric monitoring. Cloud detection [25]\nand estimating precipitation [2], and green-house gas concentra-\ntions use classical ML.[47] use Faster-RCNNs to achieve remarkable\nresults in estimating tropical storm intensity, but the required data\nis limited and has to be aligned to microwave wavelengths.\nVegetation and ground cover classification. Hyperspectral\ndata (HSD) is used to identify land cover [3], with data from MODIS\nand LANDSAT satellites successfully used to show the diminishing\nwetlands.\nDL models naturally excel at challenges presented by the HSD.\nIn a hyperspectral image, a pixel represents a large spatial area\nwhich may have several types of vegetation. This leads to the en-\ntanglement of spectral signatures and, when compounded with\n∗A version of this paper was accepted at HotMobile 2020: The 21st International\nWorkshop on Mobile Computing Systems and Applications.\narXiv:2001.10362v2  [eess.SP]  3 Feb 2020\nthe high-dimensionality and a large intra-class variability of HSD,\nmakes ground cover classification a formidable problem. 3D-CNNs\nand ResNets [28] successfully address these challenges on ISS data\nto achieve 96.4% classification accuracy. [35] review a number of\nDL architectures and demonstrate their effectiveness in situations\nwith semi-supervised or sparse data.\nObject detection and tracking. When pointed towards earth,\nHSD has been used to detect humans during natural disasters, track\nendangered animals [22], military troops and ships and monitor\noil spills [37]. When pointed at the sky, they leverage the lack of\natmospheric interference to detect galactic phenomena. The James\nWebb Space Telescope project is one of the first to use DL in data\npost-processing to detect galaxy clusters [7].\n2.2\nGeneral Spacecraft Operation\nOperation systems include the Guidance, Navigation and Control\n(GNC), communication, power, and propulsion systems. NASA has\ndefined four levels of spacecraft autonomy [12]. The lowest level\ncorresponds to a primarily ground controlled mission, whereas\nthe highest expects an ability to independently re-evaluate goals.\nCurrently automation is provided by pervasive on-board control\nprocedures (OBCPs). Used in satellites such as the Rosetta, Venus,\nHerschel & Planck [17], OBCPs initiate a predefined series of actions\nwhen an event is detected. DL can improve not only OBCPs, through\nbetter event detection and subsequent planning, but also systems\nwhich follow.\nCommunication. Software-defined radio (SDR) is replacing\nmultiple antennae designs. Its communication protocols can depend\non many parameters, such as packet re-transmission rate and band.\nThe use of DL, such as reinforcement learning (RL) to dynamically\noptimise parameters has been proposed for space applications but\nnot yet adopted [18, 34].\nAutomated control and navigation. The GNC and propul-\nsion systems control the movement of a spacecraft. Historically\nthe majority of these manoeuvring operations were directed by\na human, which is cumbersome and only feasible for near Earth\nmissions. Onboard DL systems would bring a much needed degree\nof autonomy and robustness to GNCs, which is particularly im-\nportant for deep space missions that suffer from lag and gaps in\ncommunication.\nPositioning becomes important during docking or landing. Typi-\ncally, such systems almost exclusively rely on LIDAR, but newer\ntechniques, such as the natural feature tracking (NFT), use opti-\ncal systems. Evers [16] uses the YOLO vision model to estimate\npose and relative distance, which achieves a 98% accuracy on the\nauthor’s dataset and lays the groundwork for real-time models.\nDespite impressive accuracy, DL systems would require large banks\nof images to train on [27].\nSpacecraft are particularly sensitive during docking and landing:\nseveral tonnes need to be moved with centimetre-level precision.\nWhile traditionally the domain of control systems, preliminary\nwork applies reinforcement learning methods to 6-DOF cold gas\nthruster systems [31].\nWhile spacecraft travel space, Landers and rovers must traverse\nnon-Earth surfaces. While they do not use deep models, conven-\ntional ML/control systems like AEGIS have been used on the MER\nproject [15]. Projects like the Surrey Rover Autonomy Software\n& Hardware Testbed (SMART) [19] provide terrestrial simulation\nfacilities and are looking at modular (deep) systems. Blacker et al.\n[4] use a yet-to-be-deployed CNN based system which judges the\nnavigability of each part of the terrain then plans a safe path based\non the results. The system can be tuned to run at different latency\nand memory capacities. GNC’s similarity to terrestrial problems\nof vision and autonomous driving make it a particularly attractive\narea of development.\n3\nSpace Hardware and Software\nComputational resources in space have traditionally been highly\nspecialised, tightly-integrated monoliths. In contrast with terrestrial\nhardware systems, the harsh and remote environment of space\nrequires compute systems (incl. the processor and memory chips)\nto be simultaneously efficient, radiation-resistant, and fault-tolerant.\nIn addition systems sent into space have to be thoroughly verified.\nAs a result, space systems, especially hardware, lag considerably\nbehind modern compute.\n3.1\nSystem Platforms\nSpacecraft have highly mission-dependent designs (Tab. 1), with\npurpose-designed hardware (ASICs) or high end micro-controllers\npowering older space missions. However, such systems are high\ncost, non-resilient, and large. With time, these specialisations be-\ncame infeasible along several axes (power, cost, weight, volume),\nthus to reduce the development costs, systems are increasingly be-\ning assembled using off-the-shelf components (COTS). This applies\nto both smaller spacecraft, such as CubeSats, and large multi-million\ndollar satellites.\nSystem Software and Operating systems. Due to their speci-\nficity, historically, large satellites have had minimal software inter-\nfaces: either embedded modules, like vxWorks RTEMS, or purpose-\nmade minimal software with a tightly coupled software/hardware\ninterface [41]. These systems varied dramatically and often did\nnot support floating-point calculations, integer multiplication or\ndivision units, or interrupt or dynamic memory allocation. Contem-\nporary embedded systems still in use include the core Flight System\n(cFS) and core Executive (cFE) from Goddard Space Flight Centre,\nand COSMOS by Ball Aerospace. Projects like, such as SPINAS\n[33], are attempting to create a more uniform (equal-capability),\nand open source, platform for smaller spacecraft with more so-\nphisticated compute. Some newer spacecraft also see Linux-based\nOSes [32].\nMemory and Compute Capabilities. Modern space compute\nsystems are moving towards shared/re-configurable [20], multi-\ncore systems which would be capable of running DL models. For\nexample, the recent JUICE mission to map Jupiter’s moons uses\na common digital processing unit (DPU) and software packages,\nwhich are shared between 10 of its instruments [41]. Other systems\nmay utilise multicore processors, e.g. LEON-GR740 (32 bit, quad-\ncore, rad-hard SOC), with cores preferentially being assigned to spe-\ncific tasks (such as navigation). Recently, some workloads, such as\nlinear algebra, are being accelerated with field programmable gate\narrays (FPGAs) or low-power GPU-like accelerators [33]. Special\nlow-power accelerators can be incorporated into an FPGA-based\non-board computer or as a separate chip, e.g. the Movidius compute\nstick, which was being tested for deployment in space. Tab. 1 shows\nsome of these processing elements and their specifications.\nPower budget. The largest limiting factor for on-board compute\nis power. Power generation, storage, and dissemination is facilitated\nthrough the electrical power systems (EPS). The wattage of supplied\npower is typically adjusted for payload requirements, which ranges\n20W to 95W. In small satellites, power is generated through multi-\njunction solar cells, which have 28-38% efficiency and so need to\nbe quite large to sustain the required power output. Power is most\ncommonly stored in rechargeable Li-ion or Li-Po batteries ranging\nfrom 58-243 Wh/Kg [32].\n3.2\nRadiation Hardening\nIn space, devices are no longer protected from Sun’s radiation by\nthe Earth’s atmosphere, which can cause spurious errors or stuck\ntransistors in the device’s circuitry. Radiation damages the hardware\neither through its cumulative effects (total ionizing dose, TID) or\nthrough single event effects (SEE). Recoverable SEEs are called\nsingle event upsets (SEU) and can affect the logic state of memory.\nRadiation hardening (rad-hard) allows a compute component to\nwithstand such errors. Rad-hard components are twice as slow\nand many times as expensive as their regular counterparts [21].\nOverheads incurred by the space-grade CPUs are typically much\nlarger than those incurred by the DSP and FPGA because they\nrequired more significant decreases in operating frequencies [21].\nPhysical hardening. This involves using different materials, for\nexample, insulating substrates such as silicon on sapphire [42].Other\napproaches involve shielding the circuit and alternative doping\nmechanisms.\nCircuit based hardening. This involves adding extra circuitry/logic\nto correct for the effects of SEEs. These include: watchdog pro-\ntection, overcurrent circuits, power control and error correcting\ncircuits (e.g. CRC and forward error correction in communication\nboards). Error correction is implemented both at the hardware and\nsoftware levels, such as in the main memory where hardware level\nECC and software EDAC are used in synergy.\n3.3\nSuitability for DL Workloads\nAccording to Dennehy [11], an ML-assisted optical navigation\nsystem would have: 2–5 Gbps sensor I/O, 1–10 GOPs CPU, 1GB/s\nmemory bandwidth, 250 Mbps cross link bandwidth to Earth. As\nthe previous sections showed we are almost in a position to put\nthat into a small satellites.\nSpace hardware is becoming increasingly closer to the multicore\nterrestrial mobile edge computing (MEC). While rad-hard compo-\nnents do decrease performance and raise costs, cheaper off-the-shelf\ncomponents are finding their way to cubesats. Progress in hardware\nand research in deep learning algorithms for constrained environ-\nments have begun to meet at a point which allows deep models to\nbe deployed to space.\nSeveral DL systems are being initially tested on Earth. Schartel\n[39] train a SqueezeNet model with the intent of transferring it to a\nspace embedded system; Buonaiuto et al. [5] consider Nvidia-TX1\nhardware with the CUDA Deep Neural Network (cuDNN) library\nand TensorRT software. FPGAs, like the Xilinx Artix-7 and the\nXilinx Zynq-7020, have been used for neuromorphic chips and\nimage analysis.\nFigure 1: Area imaged using SAR (left) and optical sensors\n(right). Reproduced from Wang and Patel [45].\nThe first space systems are specifically geared towards DL work-\nloads are making their way to production. CloudScout [14], a cloud\nidentification algorithm, uses a specialised Visual Processing Unit\nto identify cloud patterns.\n4\nCase Study: On-device Satellite Imagery\nObtaining satellite imagery is one of the most widespread uses\nof spaceborne hardware. However sending and receiving large vol-\numes of data is power consuming. In situations, such as with rovers,\nthe high-latency and low bandwidth communication channels make\nthis prohibitive. As seen in section 3 we are approaching have the\nhardware, software, and algorithmic capability required to use DL\nmethods on board the highly constrained environment of a space-\ncraft. In this section, we describe how they can both select relevant\nimaging data and compress it. We then quantitatively show that\nDL can save at least half the power.\n4.1\nEfficient Satellite Imaging\nMost imagining sensors capture several bands (commonly be-\ntween IR to UV) spread across the electromagnetic spectrum. Mod-\nern sensors can capture very high resolution (VHR) images at up to\n31cm of ground per pixel (in panchromatic mode) [38]. Even higher\nresolution images can be obtained using synthetic-aperture radar\n(SAR), which uses the motion of radio antenna over the surface\nto map the surface in three dimensions at a resolution of just a\nfew centimetres per pixel [30]. A number of both raw [43] and\npreprocessed [29] hyper spectral datasets are readily available.\nCaptured data needs to be transmitted to the ground station\nfor aggregation and analysis, which can be expensive. A satellite\ncan reduce the amount of data transmitted by employing deep\nlearning: on-board pre-processing can discard parts of the image of\nno interest, e.g. occluded by clouds. Global annual cloud coverage is\nestimated to be at 66%, so excluding cloud images would drastically\nreduce the amount of data transmitted [24]. For satellites deployed\nfor a particular purpose, such as boat or whale detection, neural\nnetworks can also be used to facilitate the primary task of a satellite\nand only transmit regions of interest.\nTransmission costs can be further reduced by employing a neu-\nral network to compress image data. While the following models\noffer spectacular gains, training models specifically for satellite\ndata would yield considerably better results. Near-lossless compres-\nsion [36] achieved a 20:1 compression ration(CR) with hyperspectral\nType\nCompute platform\nSpecifications\nPower budget\nmicrocontroller\nTI MSP430F2618\n12MHz, 8KB SRAM, 116KB FLASH, X-band (8kbps)\n35W\nprocessor SOC\nBAE RAD 750\n200MHz, 2GB flash 256MB DRAM\n5W\naccelerator\nIntel Movidius NCS\nVPU, 4GB RAM\n1W\nmicrocontroller\nVA41630 (Cortex M4)\n100 MHz, 64KB SRAM, 256KB FLASH\n–\nFGPA\nXilinx Virtex-5QV\n81920 LUT6, 596 RAMB16, 320 DSPs, 65 nm SRAM\n5âĂŞ10 W\nTable 1: Different types of space hardware and their configurations. The 1st entry was used on a Mars Cube One (CubeSat) mission, while\n2nd is charted to be used on the Mars 2020 interplanetary rover. The power envelope of these devices is magnitudes lower, compared to the\nWorldView 3 imaging satellite (3100 W).\ndata. In lossy compression, the Feb 2019 CCSDS standard for on-\nboard lossy compression of hyperspectral images, uses predictive\ncoding on-board and a residual hyperspectral CNN back on Earth\nto de-quantize the results [44] and achieves 0.1 bits per pixel com-\npression ratio, far surpassing classical compression standards, e.g\nJPEG.\n4.2\nPotential Efficiency Gains\nHere we present a typical system and quantify the bare mini-\nmum power saving an out-of-the-box DL systems would be able\nto provide us. Note that this is an approximate calculation: a finely\ntuned and end-to-end designed system could yield considerably\nbetter power gains.\nModel. For example, MobileNet-V2 family of models were shown\nto run successfully on microcontroller-sized hardware (with 8-bit\nquantization) [9] and power some image segmentation models (e.g.\nMobileNet-V2-backed DeepLabV3+ model [8]), which can be used\nfor cloud detection.\nCommunication. Assuming the use of an S-band transmitter\noperating at 13W (for 33dBm output power) with a 4.3Mbps data\nrate [23], transferring a 512x512 patch of data at 12-bits per pixel,\nwould take approximately 0.73s and consume around 9.5J of power\n(ignoring communication protocol overheads).\nCompute. If, instead, we perform 3s worth of inference on a\nLEON3 processor† [10] to achieve at least 20:1 compression ratio\n(incl. cloud removal), we spend 4.5J on computation and 0.2J on\ntransmission.\nPower saving. The above shows a nearly 2x power saving. Con-\nsidering different model architectures would result in other compu-\ntation vs transmission power usage trade-offs.\nPerforming neural network compression before data transmis-\nsion can considerably improve transmission latency and power\nusage, allowing longer mission lengths and the use of less costly\ntransmission hardware. Space devices present an interesting con-\nstraint space for ultra-compact computer vision models.\n5\nChallenges and Opportunities Ahead\nWe have barely scratched the surface of the what is possible\nwith deep learning in space. We outline challenges and applications\nwhich hold the greatest potential.\n5.1\nNew Applications in Space\nTerrestrial DL vision models are built for optical (narrow band)\ndata whereas most image data from space is hyperspectral. Having\nHSD is especially important because several optical artefacts (e.g.\nmetal ground cover, x-ray star bursts) are only often present within\n†Est. under 300MAdds for a MobileNet-V2 on a 512x512 input, running on a 100MHz\nprocessor.\na subset of the spectrum and require models capable of searching\nthrough deeper data cubes.\nNot only have individual modalities not been completely ex-\nploited but due to the lack of compute, multi-modal DL systems\nhave yet to make their way to space. Multimodal approaches would\nubiquitously improve spacecraft and payload operation e.g. fusing\nmagnetometer, horizon and sun sensor data for GNC operation,\nand fusing SAR and HSD for terrain characterisation.\nThere are also numerous applications which are just waiting to\nsee DL methods adapted - Ranging from DL/RL robotic construction\nin zero gravity environments to DL for crew health monitoring.\n5.2\nImproved Compute Paradigms for Space\nThe characterisation of DL models on a spacecraft must encom-\npass more than just accuracy. The ability of the model to perform\ndepends not only its construction but also on an environment con-\nstrained in terms of memory, power, compute, and reliability. Thus\nthe definition of efficiency must be expanded to include hardware\nand context aware characterisation.\nThe uptake of off-the-shelf components, would allows us to\nleverage recent developments in compute efficient (quantized) [40],\nmemory sensitive (compressed) [26], and energy aware [46] ter-\nrestrial DL models. While made significantly easier, the adaption\nprocess would still need to accommodate formidable hurdles en-\ndemic to space hardware, such as the higher error rates and the\nincreased memory latency with rad-hard components.\nNot only must the efficiency of DL models on a compute unit\nbe measured along multiple axes, but it must also be characterised\nin the context of the overall spacecraft’s operation. This becomes\nincreasingly important as compute components in newer spacecraft\nare shared between various subsystems. Real time systems, e.g.\nnavigation, may be sensitive to interrupts and IO bottlenecks.\nAs powerful hardware becomes common in space, it may be\npossible to leverage more than a single satellite for computation.\nSuch networks would offer not only more computational power\nbut also greater fault tolerance.\n5.3\nRedefining Robustness and Reliability\nDeveloping a comprehensive Validation and Verification (V&V)\nframework for embedded and robotics systems in critical areas such\nas healthcare and civil infrastructure is an area of current research.\nHowever the remoteness and expense of space make it radically\nmore risk averse and often with higher costs than MEC systems.\nThe current validation standards for space-based systems and\nsoftware listed in ECSS-E-ST-10-02C Rev.1 and ECSS-E-ST-40C [1]\nare inadequate for automated DL systems on spacecraft. Core to\nV&V is the qualification process which has 4 components: analysis,\ntesting, inspection, and demonstration. Each of these components\ndiffers significantly when applied to ML systems and especially to\nDL systems. DL systems are less deterministic, less amenable to\nisolation and component testing, are data driven, and suffer from a\nlack of a testing oracle. Adding to the challenge, DL systems in space\nneed to be measured along multiple axes: correctness, robustness,\nefficiency, interpretability. The rigour and span of V&V in space set\nit apart from methods used for MEC systems.\nA few preliminary approaches would combine formal verification\n(analysis) with simulation (testing) and interpretability mechanisms\n(inspection). Analysis methods utilise sample perturbation or mixed\ninteger linear programming [13] to characterise individual com-\nponents. [48] surveys the adaptation of more formal verification\nmethods. These methods are only able to characterise a finite set\nof cases. Inspection of DL using interpretability models such as\nLIME or contextuality models [6] allows for humans-in-the-loop\nsystems both in V&V and mission control. Finally DL sub-systems\nare tested through simulations before testing the entire component\nin the field.\n6\nConclusion\nAs space devices become more affordable to launch and their\nhardware becomes more powerful to run non-trivial workloads,\ndeep learning in space will continue to grow as a topic within\nmobile and embedded machine learning. In this work, we presented\nhow machine learning can be applied to space data, drew a parallel\nbetween space and terrestrial embedded hardware and showed\nhow on-device deep learning can improve the operation of a space\ndevice.\nAcknowledgments\nThis work was supported by the EPRSC through Grants DTP\n(EP/R513295/1) and MOA (EP/S001530/), and Samsung AI. We would\nalso like to thank Dr. Aakanksha Chowdhery and other anonymous\nreviewers for their input throughout the submission process for\nACM HotMobile 2020.\nReferences\n[1] 2014. ECSS Active Standards. https://ecss.nl/active-standards/\n[2] Mamoudou B Ba and Arnold Gruber. 2001. GOES multispectral rainfall algorithm\n(GMSRA). Journal of Applied Meteorology 40, 8 (2001), 1500–1514.\n[3] Corey Baker, Rick L Lawrence, Clifford Montagne, and Duncan Patten. 2007.\nChange detection of wetland ecosystems using Landsat imagery and change\nvector analysis. Wetlands 27, 3 (2007), 610.\n[4] P Blacker, CP Bridges, and S Hadfield. 2019. Rapid Prototyping of Deep Learning\nModels on Radiation Hardened CPUs. In 2019 NASA/ESA Conference on Adaptive\nHardware and Systems (AHS). IEEE, 25–32.\n[5] Nick Buonaiuto, Mark Louie, Jim Aarestad, Rohit Mital, Dennis Mateik, Robert\nSivilli, Apoorva Bhopale, Craig Kief, and Brian Zufelt. 2017. Satellite identification\nimaging for small satellites using NVIDIA. (2017).\n[6] Diogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso. 2019. Machine\nLearning Interpretability: A Survey on Methods and Metrics. Electronics 8, 8\n(2019), 832.\n[7] Matthew C Chan and John P Stott. 2019. Deep-CEE I: Fishing for Galaxy Clusters\nwith Deep Neural Nets. arXiv preprint arXiv:1906.08784 (2019).\n[8] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig\nAdam. 2018. Encoder-decoder with atrous separable convolution for semantic\nimage segmentation. In Proceedings of the European conference on computer vision\n(ECCV). 801–818.\n[9] Aakanksha Chowdhery, Pete Warden, Jonathon Shlens, Andrew Howard, and\nRocky Rhodes. 2019. Visual Wake Words Dataset. (2019). arXiv:1906.05721\nhttp://arxiv.org/abs/1906.05721\n[10] COBHAM. 2018. GR712RC Dual-Core LEON3-FT SPARC V8 Processor Data\nSheet. https://www.gaisler.com/doc/gr712rc-datasheet.pdf (Accessed Oct 2019).\n[11] Cornelius Dennehy. [n.d.]. A NASA GN&C Viewpoint on On-Board Processing\nChallenges to Support Optical Navigation and Other GN&C Critical Functions.\nhttps://indico.esa.int/event/225/contributions/4249/\n[12] Jacco Drabbe and Jacco Drabbe. 2008. ECSS-E-ST-70-11C âĂŞ Space segment op-\nerability. https://ecss.nl/standard/ecss-e-st-70-11c-space-segment-operability/\n[13] Souradeep Dutta, Taisa Kushner, Susmit Jha, Sriram Sankaranarayanan, Natarajan\nShankar, and Ashish Tiwari. [n.d.]. Sherlock: A Tool for Verification of Deep\nNeural Networks. ([n. d.]).\n[14] Marco Esposito. 2019. CloudScout: In Orbit Demonstration of Machine Learning\napplied on hyperspectral and multispectral thermal imaging. https://indico.esa.\nint/event/225/timetable/#20190225.detailed\n[15] Tara A Estlin, Benjamin J Bornstein, Daniel M Gaines, Robert C Anderson, David R\nThompson, Michael Burl, Rebecca Castaño, and Michele Judd. 2012. Aegis au-\ntomated science targeting for the mer opportunity rover. ACM Transactions on\nIntelligent Systems and Technology (TIST) 3, 3 (2012), 50.\n[16] Nick Evers. 2019. Deep learning in Space. https://towardsdatascience.com/deep-\nlearning-in-space-964566f09dcd (Accessed Oct 2019).\n[17] Massimo Ferraguto, Tim Wittrock, Mark Barrenscheen, Matti Paakko, and Ville\nSipinen. 2008. The on-board control procedures subsystem for the Herschel and\nPlanck satellites. In 2008 32nd Annual IEEE International Computer Software and\nApplications Conference. IEEE, 1366–1371.\n[18] Paulo Victor R Ferreira, Randy Paffenroth, Alexander M Wyglinski, Timothy M\nHackett, Sven G Bilen, Richard C Reinhart, and Dale J Mortensen. 2019. Re-\ninforcement Learning for Satellite Communications: From LEO to Deep Space\nOperations. IEEE Communications Magazine 57, 5 (2019), 70–75.\n[19] Yang Gao, Renato Samperio, Karin Shala, and Y Cheng. 2012. Modular design\nfor planetary rover autonomous navigation software using ROS. Acta Futura 5\n(2012), 9–16.\n[20] Alan D George and Christopher M Wilson. 2018. Onboard processing with\nhybrid and reconfigurable computing on small satellites. Proc. IEEE 106, 3 (2018),\n458–470.\n[21] Evan W Gretok, Evan T Kain, and Alan D George. 2019. Comparative Bench-\nmarking Analysis of Next-Generation Space Processors. In 2019 IEEE Aerospace\nConference. IEEE, 1–16.\n[22] Emilio Guirado, Siham Tabik, Marga L Rivas, Domingo Alcaraz-Segura, and\nFrancisco Herrera. 2019. Whale counting in satellite and aerial images with deep\nlearning. Scientific reports 9, 1 (2019), 1–12.\n[23] ISIS Space. 2018. ISIS High Data Rate S-Band Transmitter. https://www.isispace.\nnl/product/isis-txs-s-band-transmitter/ (Accessed Oct 2019).\n[24] Jacob Høxbroe Jeppesen, Rune Hylsberg Jacobsen, Fadil Inceoglu, and\nThomas Skjødeberg Toftegaard. 2019. A cloud detection algorithm for satel-\nlite imagery based on deep learning. Remote Sensing of Environment 229 (2019),\n247–259.\n[25] HG Lewis, S Cote, and ARL Tatnall. 1997. Determination of spatial and temporal\ncharacteristics as an aid to neural network cloud classification. International\nJournal of Remote Sensing 18, 4 (1997), 899–915.\n[26] Edgar Liberis and Nicholas D Lane. 2019.\nNeural networks on microcon-\ntrollers: saving memory at inference via operator reordering. arXiv preprint\narXiv:1910.05110 (2019).\n[27] David A Lorenz, Ryan Olds, Alexander May, Courtney Mario, Mark E Perry, Eric E\nPalmer, and Michael Daly. 2017. Lessons learned from OSIRIS-Rex autonomous\nnavigation using natural feature tracking. In 2017 IEEE Aerospace Conference.\nIEEE, 1–12.\n[28] Jacob Manning, David Langerman, Barath Ramesh, Evan Gretok, Christopher\nWilson, Alan George, James MacKinnon, and Gary Crum. 2018. Machine-learning\nspace applications on smallsat platforms with tensorflow. In Proceedings of the\n32nd Annual AIAA/USU Conference on Small Satellites, Logan, UT, USA. 4–9.\n[29] S. Mohajerani and P. Saeedi. 2019. Cloud-Net: An End-To-End Cloud Detec-\ntion Algorithm for Landsat 8 Imagery. In IGARSS 2019 - 2019 IEEE International\nGeoscience and Remote Sensing Symposium. 1029–1032. https://doi.org/10.1109/\nIGARSS.2019.8898776\n[30] Alberto Moreira. 2019. Synthetic Aperture Radar (SAR): Principles and Applica-\ntions. https://earth.esa.int/documents/10174/642943/6-LTC2013-SAR-Moreira.\npdf (Accessed Oct 2019).\n[31] Angadh Nanjangud, Peter Blacker, Saptarshi Bandyopadhyay, and Yang Gao.\n2018. Robotics and AI-enabled on-orbit operations with future generation of\nsmall satellites. Proc. IEEE 106, 3 (2018), 429–439.\n[32] NASA. 2018. State of the Art Small Spacecraft Technology. https://www.nasa.\ngov/sites/default/files/atoms/files/soa2018_final_doc.pdf (Accessed Oct 2019).\n[33] Olivier\nNotebaert.\n[n.d.].\nOn-Board\nPayload\nData\nProcessing\nre-\nquirements and ...\nhttps://indico.esa.int/event/225/contributions/\n4298/attachments/3359/4397/OBDP2019-S01-05-Airbus_Notebaert_On-\nBoard_Payload_Data_Processing_Requirements_and_Technology_Trends.pdf\n[34] Flor G Ortíz-Gómez, Ramón Martínez Rodríguez-Osorio, Miguel A Salas-Natera,\nSalvador Landeros-Ayala, Daniele Tarchi, and Alessandro Vanelli-Coralli. [n.d.].\nON THE USE OF NEURAL NETWORKS FOR FLEXIBLE PAYLOAD MANAGE-\nMENT IN VHTS SYSTEMS. ([n. d.]).\n[35] ME Paoletti, JM Haut, J Plaza, and A Plaza. 2019. Deep learning classifiers for\nhyperspectral imaging: A review. ISPRS Journal of Photogrammetry and Remote\nSensing 158 (2019), 279–317.\n[36] Shen-En Qian, Martin Bergeron, Ian Cunningham, Luc Gagnon, and Allan\nHollinger. 2006. Near lossless data compression onboard a hyperspectral satellite.\nIEEE Trans. Aerospace Electron. Systems 42, 3 (2006), 851–866.\n[37] Foudan Salem, Menas Kafatos, Tarek El-Ghazawi, Richard Gomez, and Ruixin\nYang. 2001. Hyperspectral image analysis for oil spill detection. In Summaries of\nNASA/JPL Airborne Earth Science Workshop, Pasadena, CA. 5–9.\n[38] Satellite Imaging Corporation. 2019. WorldView-4 Satellite Sensor. https://www.\nsatimagingcorp.com/satellite-sensors/geoeye-2/ (Accessed Oct 2019).\n[39] Andreas Schartel. 2017. Increasing Spacecraft Autonomy through Embedded\nNeural Networks for Semantic Image Analysis.\n[40] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. 2017. Efficient\nprocessing of deep neural networks: A tutorial and survey. Proc. IEEE 105, 12\n(2017), 2295–2329.\n[41] Felice Torelli. 2019. Common DPU and Basic SW for JUICE instruments. https:\n//indico.esa.int/event/225/contributions/3688/\n[42] Rakesh Trivedi and Usha S Mehta. 2016. A survey of radiation hardening by design\n(rhbd) techniques for electronic systems for space application. International\nJournal of Electronics and Communication Engineering Technology (IJECET) 7, 1\n(2016), 75.\n[43] Usgs. [n.d.]. Home. https://earthexplorer.usgs.gov/\n[44] Diego Valsesia and Enrico Magli. 2019. Image dequantization for hyperspectral\nlossy compression with convolutional neural networks. https://indico.esa.int/\nevent/225/contributions/245487/ (Accessed Oct 2019).\n[45] Puyang Wang and Vishal M Patel. 2018. Generating high quality visible images\nfrom SAR images using CNNs. In 2018 IEEE Radar Conference (RadarConf18).\nIEEE, 0570–0575.\n[46] Yanzhi Wang, Caiwen Ding, Zhe Li, Geng Yuan, Siyu Liao, Xiaolong Ma, Bo\nYuan, Xuehai Qian, Jian Tang, Qinru Qiu, et al. 2018. Towards ultra-high perfor-\nmance and energy efficiency of deep learning systems: an algorithm-hardware\nco-optimization framework. In Thirty-Second AAAI Conference on Artificial Intel-\nligence.\n[47] Anthony Wimmers, Christopher Velden, and Joshua H Cossuth. 2019. Using deep\nlearning to estimate tropical cyclone intensity from satellite passive microwave\nimagery. Monthly Weather Review 147, 6 (2019).\n[48] Weiming Xiang, Patrick Musau, Ayana A Wild, Diego Manzanas Lopez, Nathaniel\nHamilton, Xiaodong Yang, Joel Rosenfeld, and Taylor T Johnson. 2018. Verifica-\ntion for machine learning, autonomy, and neural networks survey. arXiv preprint\narXiv:1810.01989 (2018).\n",
  "categories": [
    "eess.SP",
    "cs.AI",
    "cs.SY",
    "eess.SY"
  ],
  "published": "2020-01-27",
  "updated": "2020-02-03"
}