{
  "id": "http://arxiv.org/abs/2002.01633v3",
  "title": "Structural Deep Clustering Network",
  "authors": [
    "Deyu Bo",
    "Xiao Wang",
    "Chuan Shi",
    "Meiqi Zhu",
    "Emiao Lu",
    "Peng Cui"
  ],
  "abstract": "Clustering is a fundamental task in data analysis. Recently, deep clustering,\nwhich derives inspiration primarily from deep learning approaches, achieves\nstate-of-the-art performance and has attracted considerable attention. Current\ndeep clustering methods usually boost the clustering results by means of the\npowerful representation ability of deep learning, e.g., autoencoder, suggesting\nthat learning an effective representation for clustering is a crucial\nrequirement. The strength of deep clustering methods is to extract the useful\nrepresentations from the data itself, rather than the structure of data, which\nreceives scarce attention in representation learning. Motivated by the great\nsuccess of Graph Convolutional Network (GCN) in encoding the graph structure,\nwe propose a Structural Deep Clustering Network (SDCN) to integrate the\nstructural information into deep clustering. Specifically, we design a delivery\noperator to transfer the representations learned by autoencoder to the\ncorresponding GCN layer, and a dual self-supervised mechanism to unify these\ntwo different deep neural architectures and guide the update of the whole\nmodel. In this way, the multiple structures of data, from low-order to\nhigh-order, are naturally combined with the multiple representations learned by\nautoencoder. Furthermore, we theoretically analyze the delivery operator, i.e.,\nwith the delivery operator, GCN improves the autoencoder-specific\nrepresentation as a high-order graph regularization constraint and autoencoder\nhelps alleviate the over-smoothing problem in GCN. Through comprehensive\nexperiments, we demonstrate that our propose model can consistently perform\nbetter over the state-of-the-art techniques.",
  "text": "Structural Deep Clustering Network\nDeyu Boâˆ—\nBeijing University of Posts and\nTelecommunications\nBeijing, China\nbodeyu@bupt.edu.cn\nXiao Wangâˆ—\nBeijing University of Posts and\nTelecommunications\nBeijing, China\nxiaowang@bupt.edu.cn\nChuan Shiâ€ \nBeijing University of Posts and\nTelecommunications\nBeijing, China\nshichuan@bupt.edu.cn\nMeiqi Zhu\nBeijing University of Posts and\nTelecommunications\nBeijing, China\nzhumeiqi@bupt.edu.cn\nEmiao Lu\nTencent\nShenzhen, China\nemiao.lu@gmail.com\nPeng Cui\nTsinghua University\nBeijing, China\ncuip@tsinghua.edu.cn\nABSTRACT\nClustering is a fundamental task in data analysis. Recently, deep\nclustering, which derives inspiration primarily from deep learning\napproaches, achieves state-of-the-art performance and has attracted\nconsiderable attention. Current deep clustering methods usually\nboost the clustering results by means of the powerful representation\nability of deep learning, e.g., autoencoder, suggesting that learning\nan effective representation for clustering is a crucial requirement.\nThe strength of deep clustering methods is to extract the useful rep-\nresentations from the data itself, rather than the structure of data,\nwhich receives scarce attention in representation learning. Moti-\nvated by the great success of Graph Convolutional Network (GCN)\nin encoding the graph structure, we propose a Structural Deep\nClustering Network (SDCN) to integrate the structural information\ninto deep clustering. Specifically, we design a delivery operator\nto transfer the representations learned by autoencoder to the cor-\nresponding GCN layer, and a dual self-supervised mechanism to\nunify these two different deep neural architectures and guide the\nupdate of the whole model. In this way, the multiple structures of\ndata, from low-order to high-order, are naturally combined with the\nmultiple representations learned by autoencoder. Furthermore, we\ntheoretically analyze the delivery operator, i.e., with the delivery\noperator, GCN improves the autoencoder-specific representation as\na high-order graph regularization constraint and autoencoder helps\nalleviate the over-smoothing problem in GCN. Through compre-\nhensive experiments, we demonstrate that our propose model can\nconsistently perform better over the state-of-the-art techniques.\nKEYWORDS\ndeep clustering, graph convolutional network, neural network, self-\nsupervised learning\nâˆ—Both authors contributed equally to this research.\nâ€ Corresponding author\nThis paper is published under the Creative Commons Attribution 4.0 International\n(CC-BY 4.0) license. Authors reserve their rights to disseminate the work on their\npersonal and corporate Web sites with the appropriate attribution.\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nÂ© 2020 IW3C2 (International World Wide Web Conference Committee), published\nunder Creative Commons CC-BY 4.0 License.\nACM ISBN 978-1-4503-7023-3/20/04.\nhttps://doi.org/10.1145/3366423.3380214\nACM Reference Format:\nDeyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui. 2020.\nStructural Deep Clustering Network. In Proceedings of The Web Conference\n2020 (WWW â€™20), April 20â€“24, 2020, Taipei, Taiwan. ACM, New York, NY,\nUSA, 11 pages. https://doi.org/10.1145/3366423.3380214\n1\nINTRODUCTION\nClustering, one of the most fundamental data analysis tasks, is to\ngroup similar samples into the same category [5, 21]. Over the past\ndecades, a large family of clustering algorithms has been devel-\noped and successfully applied to various real-world applications,\nsuch as image clustering [28] and text clustering [1]. Recently, the\nbreakthroughs in deep learning have led to a paradigm shift in\nartificial intelligence and machine learning, achieving great success\non many important tasks, including clustering. Therefore, the deep\nclustering has caught significant attention [6]. The basic idea of\ndeep clustering is to integrate the objective of clustering into the\npowerful representation ability of deep learning. Hence learning\nan effective data representation is a crucial prerequisite for deep\nclustering. For example, [27] uses the representation learned by\nautoencoder in K-means; [4, 26] leverage a clustering loss to help\nautoencoder learn the data representation with high cluster cohe-\nsion [22], and [9] uses a variational autoencoder to learn better\ndata representation for clustering. To date, deep clustering meth-\nods have achieved state-of-the-art performance and become the de\nfacto clustering methods.\nDespite the success of deep clustering, they usually focus on the\ncharacteristic of data itself, and thus seldom take the structure of\ndata into account when learning the representation. Notably, the\nimportance of considering the relationship among data samples\nhas been well recognized by previous literatures and results in data\nrepresentation field. Such structure reveals the latent similarity\namong samples, and therefore provides a valuable guide on learning\nthe representation. One typical method is the spectral clustering\n[21], which treats the samples as the nodes in weighted graph and\nuses graph structure of data for clustering. Recently, the emerging\nGraph Convolutional Networks (GCN) [11] also encode both of\nthe graph structure and node attributes for node representation.\nIn summary, the structural information plays a crucial role in data\narXiv:2002.01633v3  [cs.LG]  12 Feb 2020\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nDeyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui\nrepresentation learning. However, it has seldom been applied for\ndeep clustering.\nIn reality, integrating structural information into deep cluster-\ning usually needs to address the following two problems. (1) What\nstructural information should be considered in deep clustering? It is\nwell known that the structural information indicates the underlying\nsimilarity among data samples. However, the structure of data is\nusually very complex, i.e., there is not only the direct relationship\nbetween samples (also known as first-order structure), but also the\nhigh-order structure. The high-order structure imposes the sim-\nilarity constraint from more than one-hop relationship between\nsamples. Taking the second-order structure as an example, it im-\nplies that for two samples with no direct relationship, if they have\nmany common neighbor samples, they should still have similar\nrepresentations. When the structure of data is sparse, which always\nholds in practice, the high-order structure is of particular impor-\ntance. Therefore, only utilizing the low-order structure in deep\nclustering is far from sufficient, and how to effectively consider\nhigher-order structure is the first problem; (2) What is the relation\nbetween the structural information and deep clustering? The basic\ncomponent of deep clustering is the Deep Neural Network (DNN),\ne.g. autoencoder. The network architecture of autoencoder is very\ncomplex, consisting of multiple layers. Each layer captures different\nlatent information. And there are also various types of structural\ninformation between data. Therefore, what is the relation between\ndifferent structures and different layers in autoencoder? One can\nuse the structure to regularize the representation learned by the\nautoencoder in some way, however, on the other hand, one can\nalso directly learn the representation from the structure itself. How\nto elegantly combine the structure of data with the autoencoder\nstructure is another problem.\nIn order to capture the structural information, we first construct\na K-Nearest Neighbor (KNN) graph, which is able to reveal the\nunderlying structure of the data[? ? ]. To capture the low-order and\nhigh-order structural information from the KNN graph, we propose\na GCN module, consisting of multiple graph convolutional layers,\nto learn the GCN-specific representation.\nIn order to introduce structural information into deep clustering,\nwe introduce an autoencoder module to learn the autoencoder-\nspecific representation from the raw data, and propose a delivery\noperator to combine it with the GCN-specific representation. We\ntheoretically prove that the delivery operator is able to assist the\nintegration between autoencoder and GCN better. In particular,\nwe prove that GCN provides an approximate second-order graph\nregularization for the representation learned by autoencoder, and\nthe representation learned by autoencoder can alleviate the over-\nsmoothing issue in GCN.\nFinally, because both of the autoencoder and GCN modules will\noutput the representations, we propose a dual self-supervised mod-\nule to uniformly guide these two modules. Through the dual self-\nsupervised module, the whole model can be trained in an end-to-end\nmanner for clustering task.\nIn summary, we highlight the main contributions as follows:\nâ€¢ We propose a novel Structural Deep Clustering Network\n(SDCN) for deep clustering. The proposed SDCN effectively\ncombines the strengths of both autoencoder and GCN with\na novel delivery operator and a dual self-supervised module.\nTo the best of our knowledge, this is the first time to apply\nstructural information into deep clustering explicitly.\nâ€¢ We give a theoretical analysis of our proposed SDCN and\nprove that GCN provides an approximate second-order graph\nregularization for the DNN representations and the data rep-\nresentation learned in SDCN is equivalent to the sum of the\nrepresentations with different-order structural information.\nBased on our theoretical analysis, the over-smoothing issue\nof GCN module in SDCN will be effectively alleviated.\nâ€¢ Extensive experiments on six real-world datasets demon-\nstrate the superiority of SDCN in comparison with the state-\nof-the-art techniques. Specifically, SDCN achieves signifi-\ncant improvements (17% on NMI, 28% on ARI) over the best\nbaseline method.\n2\nRELATED WORK\nIn this section, we introduce the most related work: deep clustering\nand graph clustering with GCN.\nDeep clustering methods aim to combine the deep representation\nlearning with the clustering objective. For example, [27] proposes\ndeep clustering network, using the loss function of K-means to help\nautoencoder learn a \"K-means-friendly\" data representation. Deep\nembedding clustering [26] designs a KL-divergence loss to make\nthe representation learned by autoencoder surround the cluster\ncenters closer, thus improving the cluster cohesion. Improved deep\nembedding clustering [4] adds a reconstruction loss to the objective\nof DEC as a constraint to help autoencoder learn a better data\nrepresentation. Variational deep embedding [9] is able to model\nthe data generation process and clusters jointly by using a deep\nvariational autoencoder, so as to achieve better clustering results.\n[8] proposes deep subspace clustering networks, which uses a novel\nself-expressive layer between the encoder and the decoder. It is able\nto mimic the \"self-expressiveness\" property in subspace clustering,\nthus obtaining a more expressive representation. DeepCluster [3]\ntreats the clustering results as pseudo labels so that it can be applied\nin training deep neural network with large datasets. However, all\nof these methods only focus on learning the representation of data\nfrom the samples themselves. Another important information in\nlearning representation, the structure of data, is largely ignored by\nthese methods.\nTo cope with the structural information underlying the data,\nsome GCN-based clustering methods have been widely applied.\nFor instance, [10] proposes graph autoencoder and graph variation\nautoencoder, which uses GCN as an encoder to integrate graph\nstructure into node features to learn the nodes embedding. Deep at-\ntentional embedded graph clustering [25] uses an attention network\nto capture the importance of the neighboring nodes and employs\nthe KL-divergence loss from DEC to supervise the training process\nof graph clustering. All GCN-based clustering methods mentioned\nabove rely on reconstructing the adjacency matrix to update the\nmodel, and those methods can only learn data representations from\nthe graph structure, which ignores the characteristic of the data\nitself. However, the performance of this type of methods might be\nlimited to the overlapping between community structure.\nStructural Deep Clustering Network\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nGCN Module\nDNN Module\nğ’  \nğ‘¿ \nğ‘¿  \nğ‘³ğ’“ğ’†ğ’”= ğŸ\nğŸğ‘µ ||ğ‘¿âˆ’ğ‘¿ ||ğ‘­\nğŸ\nğ‘µ\nğ’Š=ğŸ\n \nğ‘¯(ğŸ)\nğ‘¯(ğŸ) \nP\nP\nQ\nP\nQ\nğ‘³ğ’ˆğ’„ğ’= ğ‘²ğ‘³(ğ’||ğ‘·) \nDual Self-Supervised Module\nğ‘³ğ’„ğ’ğ’–= ğ‘²ğ‘³(ğ‘¸||ğ‘·) \nğ‘¯(ğ‘³) \nğ‘¯(ğ‘³) \nğ’(ğ‘³) \n...\n...\nGraph\nğ’(ğŸ) \nğ’(ğŸ) \nğ’(ğŸ‘) ...\nFigure 1: The framework of our proposed SDCN. X, Ë†X are the input data and the reconstructed data, respectively. H(â„“) and\nZ(â„“) are the representations in the â„“-th layer in the DNN and GCN module, respectively. Different colors represent different\nrepresentations H(â„“), learned the by DNN module. The blue solid line represents that target distribution P is calculated by the\ndistribution Q and the two red dotted lines represent the dual self-supervised mechanism. The target distribution P to guide\nthe update of the DNN module and the GCN module at the same time.\n3\nTHE PROPOSED MODEL\nIn this section, we introduce our proposed structural deep clustering\nnetwork, where the overall framework is shown in Figure 1. We first\nconstruct a KNN graph based on the raw data. Then we input the\nraw data and KNN graph into autoencoder and GCN, respectively.\nWe connect each layer of autoencoder with the corresponding\nlayer of GCN, so that we can integrate the autoencoder-specific\nrepresentation into structure-aware representation by a delivery\noperator. Meanwhile, we propose a dual self-supervised mechanism\nto supervise the training progress of autoencoder and GCN. We\nwill describe our proposed model in detail in the following.\n3.1\nKNN Graph\nAssume that we have the raw data X âˆˆRN Ã—d, where each row\nxi represents the i-th sample, and N is the number of samples\nand d is the dimension. For each sample, we first find its top-K\nsimilar neighbors and set edges to connect it with its neighbors.\nThere are many ways to calculate the similarity matrix S âˆˆRN Ã—N\nof the samples. Here we list two popular approaches we used in\nconstructing the KNN graph:\n1) Heat Kernel. The similarity between samples i and j is cal-\nculated by:\nSij = eâˆ’âˆ¥xi âˆ’xj âˆ¥2\nt\n,\n(1)\nwhere t is the time parameter in heat conduction equation.\nFor continuous data, e.g., images.\n2) Dot-product.The similarity between samples i and j is cal-\nculated by:\nSij = xT\nj xi.\n(2)\nFor discrete data, e.g., bag-of-words, we use the dot-product\nsimilarity so that the similarity is related to the number of\nidentical words only.\nAfter calculating the similarity matrix S, we select the top-K similar-\nity points of each sample as its neighbors to construct an undirected\nK-nearest neighbor graph. In this way, we can get the adjacency\nmatrix A from the non-graph data.\n3.2\nDNN Module\nAs we mentioned before, learning an effective data representation is\nof great importance to deep clustering. There are several alternative\nunsupervised methods for different types of data to learn represen-\ntations. For example, denoising autoencoder [24], convolutional\nautoencoder [19], LSTM encoder-decoder [18] and adversarial au-\ntoencoder [17]. They are variations of the basic autoencoder [7].\nIn this paper, for the sake of generality, we employ the basic au-\ntoencoder to learn the representations of the raw data in order to\naccommodate for different kinds of data characteristics. We assume\nthat there are L layers in the autoencoder and â„“represents the layer\nnumber. Specifically, the representation learned by the â„“-th layer\nin encoder part, H(â„“), can be obtained as follows:\nH(â„“) = Ï•\n\u0010\nW(â„“)\ne H(â„“âˆ’1) + b(â„“)\ne\n\u0011\n,\n(3)\nwhere Ï• is the activation function of the fully connected layers\nsuch as Relu [20] or Sigmoid function, W(â„“)\ne\nand b(â„“)\ne\nare the weight\nmatrix and bias of the â„“-th layer in the encoder, respectively. Besides,\nwe denote H(0) as the raw data X.\nThe encoder part is followed by the decoder part, which is to\nreconstruct the input data through several fully connected layers\nby the equation\nH(â„“) = Ï•\n\u0010\nW(â„“)\nd H(â„“âˆ’1) + b(â„“)\nd\n\u0011\n,\n(4)\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nDeyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui\nwhere W(â„“)\nd\nand b(â„“)\nd\nare the weight matrix and bias of the â„“-th layer\nin the decoder, respectively.\nThe output of the decoder part is the reconstruction of the raw\ndata Ë†X = H(L), which results in the following objective function:\nLres =\n1\n2N\nN\nÃ•\ni=1\nâˆ¥xi âˆ’Ë†xi âˆ¥2\n2 =\n1\n2N ||X âˆ’Ë†X||2\nF .\n(5)\n3.3\nGCN Module\nAutoencoder is able to learn the useful representations from the\ndata itself, e.g. H(1), H(2), Â· Â· Â· , H(L), while ignoring the relationship\nbetween samples. In the section, we will introduce how to use the\nGCN module to propagate these representations generated by the\nDNN module. Once all the representations learned by DNN module\nare integrated into GCN, then the GCN-learnable representation\nwill be able to accommodate for two different kinds of informa-\ntion, i.e., data itself and relationship between data. In particular,\nwith the weight matrix W, the representation learned by the â„“-th\nlayer of GCN, Z(â„“), can be obtained by the following convolutional\noperation:\nZ(â„“) = Ï•(eDâˆ’1\n2 eAeDâˆ’1\n2 Z(â„“âˆ’1)W(â„“âˆ’1)),\n(6)\nwhere eA = A + I and eDii = Ã\nj eAij. I is the identity diagonal matrix\nof the adjacent matrix A for the self-loop in each node. As can be\nseen from Eq. 6, the representation Z(â„“âˆ’1) will propagate through\nthe normalized adjacency matrix eDâˆ’1\n2 eAeDâˆ’1\n2 to obtain the new rep-\nresentation Z(â„“). Considering that the representation learned by\nautoencoder H(â„“âˆ’1) is able to reconstruct the data itself and contain\ndifferent valuable information, we combine the two representations\nZ(â„“âˆ’1) and H(â„“âˆ’1) together to get a more complete and powerful\nrepresentation as follows:\neZ(â„“âˆ’1) = (1 âˆ’Ïµ)Z(â„“âˆ’1) + ÏµH(â„“âˆ’1),\n(7)\nwhere Ïµ is a balance coefficient, and we uniformly set it to 0.5 here.\nIn this way, we connect the autoencoder and GCN layer by layer.\nThen we use eZ(â„“âˆ’1) as the input of the l-th layer in GCN to\ngenerate the representation Z(â„“):\nZ(â„“) = Ï•\n\u0010\neDâˆ’1\n2 eAeDâˆ’1\n2 eZ(â„“âˆ’1)W(â„“âˆ’1)\u0011\n.\n(8)\nAs we can see in Eq. 8, the autoencoder-specific representation\nH(â„“âˆ’1) will be propagated through the normailized adjacency ma-\ntrix eDâˆ’1\n2 eAeDâˆ’1\n2 . Because the representations learned by each DNN\nlayer are different, to preserve information as much as possible, we\ntransfer the representations learned from each DNN layer into a\ncorresponding GCN layer for information propagation, as in Figure\n1. The delivery operator works L times in the whole model. We will\ntheoretically analyze the advantages of this delivery operator in\nSection 3.5.\nNote that, the input of the first layer GCN is the raw data X:\nZ(1) = Ï•(eDâˆ’1\n2 eAeDâˆ’1\n2 XW(1)).\n(9)\nThe last layer of the GCN module is a multiple classification\nlayer with a softmax function:\nZ = sof tmax\n\u0010\neDâˆ’1\n2 eAeDâˆ’1\n2 Z(L)W(L)\u0011\n.\n(10)\nThe result zij âˆˆZ indicates the probability sample i belongs to\ncluster center j, and we can treat Z as a probability distribution.\n3.4\nDual Self-Supervised Module\nNow, we have connected the autoencoder with GCN in the neural\nnetwork architecture. However, they are not designed for the deep\nclustering. Basically, autoencoder is mainly used for data represen-\ntation learning, which is an unsupervised learning scenario, while\nthe traditional GCN is in the semi-supervised learning scenario.\nBoth of them cannot be directly applied to the clustering problem.\nHere, we propose a dual self-supervised module, which unifies\nthe autoencoder and GCN modules in a uniform framework and\neffectively trains the two modules end-to-end for clustering.\nIn particular, for the i-th sample and j-th cluster, we use the\nStudentâ€™s t-distribution [16] as a kernel to measure the similarity\nbetween the data representation hi and the cluster center vector\nÂµj as follows:\nqij =\n(1 +\n\r\rhi âˆ’Âµj\n\r\r2 /v)âˆ’v+1\n2\nÃ\njâ€²(1 +\n\r\rhi âˆ’Âµjâ€²\r\r2 /v)âˆ’v+1\n2\n,\n(11)\nwhere hi is the i-th row of H(L), Âµj is initialized by K-means on\nrepresentations learned by pre-train autoencoder and v are the\ndegrees of freedom of the StudentÃ¢Ä‚Å¹s t-distribution. qij can be\nconsidered as the probability of assigning sample i to cluster j, i.e.,\na soft assignment. We treat Q = [qij] as the distribution of the\nassignments of all samples and let Î±=1 for all experiments.\nAfter obtaining the clustering result distribution Q, we aim to\noptimize the data representation by learning from the high confi-\ndence assignments. Specifically, we want to make data representa-\ntion closer to cluster centers, thus improving the cluster cohesion.\nHence, we calculate a target distribution P as follows:\npij =\nq2\nij/fj\nÃ\njâ€² q2\nijâ€²/fjâ€² ,\n(12)\nwhere fj = Ã\ni qij are soft cluster frequencies. In the target distribu-\ntion P, each assignment in Q is squared and normalized so that the\nassignments will have higher confidence, leading to the following\nobjective function:\nLclu = KL(P||Q) =\nÃ•\ni\nÃ•\nj\npijloĞ´pij\nqij\n.\n(13)\nBy minimizing the KL divergence loss between Q and P distribu-\ntions, the target distribution P can help the DNN module learn a\nbetter representation for clustering task, i.e., making the data rep-\nresentation surround the cluster centers closer. This is regarded as\na self-supervised mechanism1, because the target distribution P is\ncalculated by the distribution Q, and the P distribution supervises\nthe updating of the distribution Q in turn.\nAs for training the GCN module, one possible way is to treat the\nclustering assignments as the truth labels [3]. However, this strategy\nwill bring noise and trivial solutions, and lead to the collapse of\nthe whole model. As mentioned before, the GCN module will also\n1Although some previous work tend to call this mechanism self-training, we prefer to\nuse the term \"self-supervised\" to be consistent with the GCN training method.\nStructural Deep Clustering Network\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nprovide a clustering assignment distribution Z. Therefore, we can\nuse distribution P to supervise distribution Z as follows:\nLĞ´cn = KL(P||Z) =\nÃ•\ni\nÃ•\nj\npijloĞ´pij\nzij\n.\n(14)\nThere are two advantages of the objective function: (1) compared\nwith the traditional multi-classification loss function, KL divergence\nupdates the entire model in a more \"gentle\" way to prevent the data\nrepresentations from severe disturbances; (2) both GCN and DNN\nmodules are unified in the same optimization target, making their\nresults tend to be consistent in the training process. Because the\ngoal of the DNN module and GCN module is to approximate the\ntarget distribution P, which has a strong connection between the\ntwo modules, we call it a dual self-supervised mechanism.\nAlgorithm 1: Training process of SDCN\nInput: Input data: X, Graph: G, Number of clusters: K,\nMaximum iterations: MaxIter;\nOutput: Clustering results R;\n1 Initialize W(â„“)\ne , b(â„“)\ne , W(â„“)\nd , b(â„“)\nd\nwith pre-train autoencoder;\n2 Initialize Âµ with K-means on the representations learned by\npre-train autoencoder;\n3 Initialize W(â„“) randomly;\n4 for iter âˆˆ0, 1, Â· Â· Â· , MaxIter do\n5\nGenerate DNN representations H(1), H(2), Â· Â· Â· , H(L);\n6\nUse H(L) to compute distribution Q via Eq. 11;\n7\nCalculate target distribution P via Eq. 12;\n8\nfor â„“âˆˆ1, Â· Â· Â· , L do\n9\nUse the delivery operator with Ïµ=0.5\neZ(â„“) = 1\n2Z(â„“) + 1\n2H(â„“);\n10\nGenerate the next GCN layer representation\nZ(â„“+1) = Ï•\n\u0010\neDâˆ’1\n2 eAeDâˆ’1\n2 eZ(â„“)W(â„“)\nĞ´\n\u0011\n;\n11\nend\n12\nCalculate the distribution Z via Eq. 10;\n13\nFeed H(L) to the decoder to construct the raw data X;\n14\nCalculate Lres, Lclu, LĞ´cn, respectively;\n15\nCalculate the loss function via Eq. 15;\n16\nBack propagation and update parameters in SDCN;\n17 end\n18 Calculate the clustering results based on distribution Z;\n19 return R;\nThrough this mechanism, SDCN can directly concentrate two\ndifferent objectives, i.e. clustering objective and classification ob-\njective, in one loss function. And thus, the overall loss function of\nthe our proposed SDCN is:\nL = Lres + Î±Lclu + Î²LĞ´cn,\n(15)\nwhere Î± > 0 is a hyper-parameter that balances the clustering\noptimization and local structure preservation of raw data and Î² > 0\nis a coefficient that controls the disturbance of GCN module to the\nembedding space.\nIn practice, after training until the maximum epochs, SDCN will\nget a stable result. Then we can set labels to samples. We choose\nthe soft assignments in distribution Z as the final clustering results.\nBecause the representations learned by GCN contains two different\nkinds of information. The label assigned to sample i is:\nri = arg max\nj\nzij,\n(16)\nwhere zij is calculated in Eq. 10.\nThe algorithm of the whole model is shown in Algorithm 1.\n3.5\nTheory Analysis\nIn this section, we will analyze how SDCN introduces structural\ninformation into the autoencoder. Before that, we give the definition\nof graph regularization and second-order graph regularization.\nDefinition 1. Graph regularization [2]. Given a weighted graph\nG, the objective of graph regularization is to minimize the following\nequation:\nÃ•\nij\n1\n2\n\r\rhi âˆ’hj\n\r\r2\n2 wij,\n(17)\nwhere wij means the weight of the edge between node i and node j,\nand hi is the representation of node i.\nBased on Definition 1, we can find that the graph regularization\nindicates that if there is a larger weight between nodes i and j, their\nrepresentations should be more similar.\nDefinition 2. Second-order similarity. We assume that A is the\nadjacency matrix of graph G and ai is the i-th column of A. The\nsecond-order similarity between node i and node j is\nsij =\naT\ni aj\nâˆ¥ai âˆ¥\n\r\raj\n\r\r =\naT\ni aj\nâˆšdi\np\ndj\n=\nC\nâˆšdi\np\ndj\n,\n(18)\nwhere C is the number of common neighbors between node i and node\nj and di is the degree of node i.\nDefinition 3. Second-order graph regularization. The objective\nof second-order graph regularization is to minimize the equation\nÃ•\ni,j\n1\n2\n\r\rhi âˆ’hj\n\r\r2\n2 sij,\n(19)\nwhere sij is the second-order similarity.\nCompared with Definition 1, Definition 3 imposes a high-order\nconstraint, i.e., if two nodes have many common neighbors, their\nrepresentations should also be more similar.\nTheorem 1. GCN provides an approximate second-order graph\nregularization for the DNN representations.\nProof. Here we focus on the â„“-th layer of SDCN. hi is the i-\nth row of H(â„“), representing the data representation of sample\ni learned by autoencoder and Ë†hi = Ï•\n\u0012Ã\nj âˆˆNi\nhj\nâˆšdiâˆš\ndj W\n\u0013\nis the\nrepresentation hi passing through the GCN layer. Here we assume\nthat Ï•(x) = x and W = I, and Ë†hi can be seen as the average\nof neighbor representations. Hence we can divide Ë†hi into three\nparts: the node representations hi\ndi , the sum of common neighbor\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nDeyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui\nrepresentations S = Ã\np âˆˆNiâˆ©Nj\nhp\nâˆš\ndp and the sum of non-common\nneighbor representations Di = Ã\nqâˆˆNiâˆ’Niâˆ©Nj\nhq\nâˆš\ndq , where Ni is\nthe neighbors of node i. The distance between the representations\nË†hi and Ë†hj is:\n\r\r\rË†hi âˆ’Ë†hj\n\r\r\r2\n=\n\r\r\r\r\r\n\u0012 hi\ndi\nâˆ’hj\ndj\n\u0013\n+\n \nS\nâˆšdi\nâˆ’S\np\ndj\n+\n!\n+\n \nDi\nâˆšdi\nâˆ’Dj\np\ndj\n!\r\r\r\r\r2\nâ‰¤\n\r\r\r\r\nhi\ndi\nâˆ’hj\ndj\n\r\r\r\r\n2\n+\n\f\f\f\f\f\nâˆšdi âˆ’\np\ndj\nâˆšdi\np\ndj\n\f\f\f\f\f âˆ¥Sâˆ¥2 +\n\r\r\r\r\r\nDi\nâˆšdi\nâˆ’Dj\np\ndj\n\r\r\r\r\r2\nâ‰¤\n\r\r\r\r\nhi\ndi\nâˆ’hj\ndj\n\r\r\r\r\n2\n+\n\f\f\f\f\f\nâˆšdi âˆ’\np\ndj\nâˆšdi\np\ndj\n\f\f\f\f\f âˆ¥Sâˆ¥2 +\n \r\r\r\r\nDi\nâˆšdi\n\r\r\r\r\n2\n+\n\r\r\r\r\r\nDj\np\ndj\n\r\r\r\r\r2\n!\n.\n(20)\nWe can find that the first term of Eq. 20 is independent of the second-\norder similarity. Hence the upper bound of the distance between\ntwo node representations is only related to the second and third\nterms. For the second item of Eq. 20, if di â‰ªdj, wij â‰¤\nqdi\ndj , which\nis very small and not consistent with the precondition. If di â‰ˆdj,\nthe effect of the second item is paltry and can be ignored. For the\nthird item of Eq. 20, if two nodes have many common neighbors,\ntheir non-common neighbors will be very few, and the values of\n\r\r\r Di\nâˆšdi\n\r\r\r\n2 and\n\r\r\r\r\nDj\nâˆš\ndj\n\r\r\r\r2\nare positively correlated with non-common\nneighbors. If the second-order similarity sij is large, the upper\nbound of\n\r\r\rË†hi âˆ’Ë†hj\n\r\r\r2 will drop. In an extreme case, i.e. wij = 1,\n\r\r\rË†hi âˆ’Ë†hj\n\r\r\r2 = 1\nd\n\r\rhi âˆ’hj\n\r\r2 .\nâ–¡\nThis shows that after the DNN representations pass through the\nGCN layer, if the nodes with large second-order similarity, GCN\nwill force the representations of nodes to be close to each other,\nwhich is same to the idea of second-order graph regularization.\nTheorem 2. The representation Z(â„“) learned by SDCN is equiva-\nlent to the sum of the representations with different order structural\ninformation.\nProof. For the simplicity of the proof, let us assume that Ï• (x) =\nx, b(â„“)\ne\n= 0 and W(â„“)\nĞ´\n= I, âˆ€â„“âˆˆ[1, 2, Â· Â· Â· , L]. We can rewrite Eq. 8\nas\nZ(â„“+1) = bAeZ(â„“)\nZ(â„“+1) = (1 âˆ’Ïµ)bAZ(â„“) + ÏµbAH(â„“),\n(21)\nwhere bA = eDâˆ’1\n2 eAeDâˆ’1\n2 . After L-th propagation step, the result is\nZ(L) = (1 âˆ’Ïµ)L bALX + Ïµ\nL\nÃ•\nâ„“=1\n(1 âˆ’Ïµ)â„“âˆ’1 bAâ„“H(â„“).\n(22)\nNote that bALX is the output of standard GCN, which may suffer\nfrom the over-smoothing problem. Moreover, if L â†’âˆ, the left\nterm tends to 0 and the right term dominants the data representa-\ntion. We can clearly see that the right term is the sum of different\nrepresentations, i.e. H(â„“), with different order structural informa-\ntion.\nâ–¡\nTable 1: The statistics of the datasets.\nDataset\nType\nSamples\nClasses\nDimension\nUSPS\nImage\n9298\n10\n256\nHHAR\nRecord\n10299\n6\n561\nReuters\nText\n10000\n4\n2000\nACM\nGraph\n3025\n3\n1870\nDBLP\nGraph\n4058\n4\n334\nCiteseer\nGraph\n3327\n6\n3703\nThe advantages of the delivery operator in Eq. 7 are two-folds:\none is that the data representation Z(â„“) learn by SDCN contains\ndifferent structural information. Another is that it can alleviate the\nover-smoothing phenomenon in GCN. Because multilayer GCNs\nfocus on higher-order information, the GCN module in SDCN is\nthe sum of the representations with different order structural infor-\nmation. Similar to [12], our method also uses the fusion of different\norder information to alleviate the over-smoothing phenomenon\nin GCN. However, different from [12] treating different order ad-\njacency matrices with the same representations, our SDCN gives\ndifferent representations to different order adjacency matrices. This\nmakes our model incorporate more information.\n3.6\nComplexity Analysis\nIn this work, we denote d as the dimension of the input data and\nthe dimension of each layer of the autoencoder is d1,d2, Â· Â· Â· ,dL.\nThe size of weight matrix in the first layer of the encoder is W(1)\ne\nâˆˆ\nRdÃ—d1. N is the number of the input data. The time complexity of\nthe autoencoder is O(Nd2d2\n1...d2\nL). As for the GCN module, because\nthe operation of GCN can be efficiently implemented using sparse\nmatrix, the time complexity is linear with the number of edges |E|.\nThe time complexity is O(|E|dd1...dL). In addition, we suppose that\nthere are K classes in the clustering task, so the time complexity of\nthe Eq. 11 is O(NK + N log N) corresponding to [26]. The overall\ntime complexity of our model is O(Nd2d2\n1...d2\nL +|E|dd1...dL +NK +\nN log N), which is linearly related to the number of samples and\nedges.\n4\nEXPERIMENTS\n4.1\nDatasets\nOur proposed SDCN is evaluated on six datasets. The statistics of\nthese datasets are shown in Table 1 and the detailed descriptions\nare the followings:\nâ€¢ USPS[13]: The USPS dataset contains 9298 gray-scale hand-\nwritten digit images with size of 16x16 pixels. The features\nare the gray value of pixel points in images and all features\nare normalized to [0, 2].\nâ€¢ HHAR[23]: The Heterogeneity Human Activity Recogni-\ntion (HHAR) dataset contains 10299 sensor records from\nsmart phones and smart watches. All samples are partitioned\ninto 6 categories of human activities, including biking, sit-\nting, standing, walking, stair up and stair down.\nâ€¢ Reuters[14]: It is a text dataset containing around 810000\nEnglish news stories labeled with a category tree. We use\nStructural Deep Clustering Network\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nTable 2: Clustering results on six datasets (meanÂ±std). The bold numbers represent the best results and the numbers with\nasterisk are the best results of the baselines.\nDataset\nMetric\nK-means\nAE\nDEC\nIDEC\nGAE\nVGAE\nDAEGC\nSDCNQ\nSDCN\nUSPS\nACC\n66.82Â±0.04\n71.04Â±0.03\n73.31Â±0.17\n76.22Â±0.12âˆ—\n63.10Â±0.33\n56.19Â±0.72\n73.55Â±0.40\n77.09Â±0.21\n78.08Â±0.19\nNMI\n62.63Â±0.05\n67.53Â±0.03\n70.58Â±0.25\n75.56Â±0.06âˆ—\n60.69Â±0.58\n51.08Â±0.37\n71.12Â±0.24\n77.71Â±0.21\n79.51Â±0.27\nARI\n54.55Â±0.06\n58.83Â±0.05\n63.70Â±0.27\n67.86Â±0.12âˆ—\n50.30Â±0.55\n40.96Â±0.59\n63.33Â±0.34\n70.18Â±0.22\n71.84Â±0.24\nF1\n64.78Â±0.03\n69.74Â±0.03\n71.82Â±0.21\n74.63Â±0.10âˆ—\n61.84Â±0.43\n53.63Â±1.05\n72.45Â±0.49\n75.88Â±0.17\n76.98Â±0.18\nHHAR\nACC\n59.98Â±0.02\n68.69Â±0.31\n69.39Â±0.25\n71.05Â±0.36\n62.33Â±1.01\n71.30Â±0.36\n76.51Â±2.19âˆ—\n83.46Â±0.23\n84.26Â±0.17\nNMI\n58.86Â±0.01\n71.42Â±0.97\n72.91Â±0.39\n74.19Â±0.39âˆ—\n55.06Â±1.39\n62.95Â±0.36\n69.10Â±2.28\n78.82Â±0.28\n79.90Â±0.09\nARI\n46.09Â±0.02\n60.36Â±0.88\n61.25Â±0.51\n62.83Â±0.45âˆ—\n42.63Â±1.63\n51.47Â±0.73\n60.38Â±2.15\n71.75Â±0.23\n72.84Â±0.09\nF1\n58.33Â±0.03\n66.36Â±0.34\n67.29Â±0.29\n68.63Â±0.33\n62.64Â±0.97\n71.55Â±0.29\n76.89Â±2.18âˆ—\n81.45Â±0.14\n82.58Â±0.08\nReuters\nACC\n54.04Â±0.01\n74.90Â±0.21\n73.58Â±0.13\n75.43Â±0.14âˆ—\n54.40Â±0.27\n60.85Â±0.23\n65.50Â±0.13\n79.30Â±0.11\n77.15Â±0.21\nNMI\n41.54Â±0.51\n49.69Â±0.29\n47.50Â±0.34\n50.28Â±0.17âˆ—\n25.92Â±0.41\n25.51Â±0.22\n30.55Â±0.29\n56.89Â±0.27\n50.82Â±0.21\nARI\n27.95Â±0.38\n49.55Â±0.37\n48.44Â±0.14\n51.26Â±0.21âˆ—\n19.61Â±0.22\n26.18Â±0.36\n31.12Â±0.18\n59.58Â±0.32\n55.36Â±0.37\nF1\n41.28Â±2.43\n60.96Â±0.22\n64.25Â±0.22âˆ—\n63.21Â±0.12\n43.53Â±0.42\n57.14Â±0.17\n61.82Â±0.13\n66.15Â±0.15\n65.48Â±0.08\nACM\nACC\n67.31Â±0.71\n81.83Â±0.08\n84.33Â±0.76\n85.12Â±0.52\n84.52Â±1.44\n84.13Â±0.22\n86.94Â±2.83âˆ—\n86.95Â±0.08\n90.45Â±0.18\nNMI\n32.44Â±0.46\n49.30Â±0.16\n54.54Â±1.51\n56.61Â±1.16âˆ—\n55.38Â±1.92\n53.20Â±0.52\n56.18Â±4.15\n58.90Â±0.17\n68.31Â±0.25\nARI\n30.60Â±0.69\n54.64Â±0.16\n60.64Â±1.87\n62.16Â±1.50âˆ—\n59.46Â±3.10\n57.72Â±0.67\n59.35Â±3.89\n65.25Â±0.19\n73.91Â±0.40\nF1\n67.57Â±0.74\n82.01Â±0.08\n84.51Â±0.74\n85.11Â±0.48\n84.65Â±1.33\n84.17Â±0.23\n87.07Â±2.79âˆ—\n86.84Â±0.09\n90.42Â±0.19\nDBLP\nACC\n38.65Â±0.65\n51.43Â±0.35\n58.16Â±0.56\n60.31Â±0.62\n61.21Â±1.22\n58.59Â±0.06\n62.05Â±0.48âˆ—\n65.74Â±1.34\n68.05Â±1.81\nNMI\n11.45Â±0.38\n25.40Â±0.16\n29.51Â±0.28\n31.17Â±0.50\n30.80Â±0.91\n26.92Â±0.06\n32.49Â±0.45âˆ—\n35.11Â±1.05\n39.50Â±1.34\nARI\n6.97Â±0.39\n12.21Â±0.43\n23.92Â±0.39\n25.37Â±0.60âˆ—\n22.02Â±1.40\n17.92Â±0.07\n21.03Â±0.52\n34.00Â±1.76\n39.15Â±2.01\nF1\n31.92Â±0.27\n52.53Â±0.36\n59.38Â±0.51\n61.33Â±0.56\n61.41Â±2.23\n58.69Â±0.07\n61.75Â±0.67âˆ—\n65.78Â±1.22\n67.71Â±1.51\nCiteseer\nACC\n39.32Â±3.17\n57.08Â±0.13\n55.89Â±0.20\n60.49Â±1.42\n61.35Â±0.80\n60.97Â±0.36\n64.54Â±1.39âˆ—\n61.67Â±1.05\n65.96Â±0.31\nNMI\n16.94Â±3.22\n27.64Â±0.08\n28.34Â±0.30\n27.17Â±2.40\n34.63Â±0.65\n32.69Â±0.27\n36.41Â±0.86âˆ—\n34.39Â±1.22\n38.71Â±0.32\nARI\n13.43Â±3.02\n29.31Â±0.14\n28.12Â±0.36\n25.70Â±2.65\n33.55Â±1.18\n33.13Â±0.53\n37.78Â±1.24âˆ—\n35.50Â±1.49\n40.17Â±0.43\nF1\n36.08Â±3.53\n53.80Â±0.11\n52.62Â±0.17\n61.62Â±1.39\n57.36Â±0.82\n57.70Â±0.49\n62.20Â±1.32âˆ—\n57.82Â±0.98\n63.62Â±0.24\n4 root categories: corporate/industrial, government/social,\nmarkets and economics as labels and sample a random subset\nof 10000 examples for clustering.\nâ€¢ ACM2[? ]: This is a paper network from the ACM dataset.\nThere is an edge between two papers if they are written\nby same author. Paper features are the bag-of-words of the\nkeywords. We select papers published in KDD, SIGMOD,\nSIGCOMM, MobiCOMM and divide the papers into three\nclasses (database, wireless communication, data mining) by\ntheir research area.\nâ€¢ DBLP3[? ]: This is an author network from the DBLP dataset.\nThere is an edge between two authors if they are the co-\nauthor relationship. The authors are divided into four areas:\ndatabase, data mining, machine learning and information\nretrieval. We label each authorâ€™s research area according\nto the conferences they submitted. Author features are the\nelements of a bag-of-words represented of keywords.\nâ€¢ Citeseer4: It is a citation network which contains sparse\nbag-of-words feature vectors for each document and a list\nof citation links between documents. The labels contain six\narea: agents, artificial intelligence, database, information\nretrieval, machine language, and HCI.\n2http://dl.acm.org/\n3https://dblp.uni-trier.de\n4http://citeseerx.ist.psu.edu/index\n4.2\nBaselines\nWe compare our proposed method SDCN with three types of meth-\nods, including clustering methods on raw data, DNN-based cluster-\ning methods and GCN-based graph clustering methods.\nâ€¢ K-means [5]: A classical clustering method based on the\nraw data.\nâ€¢ AE [7]: It is a two-stage deep clustering algorithm which\nperforms K-means on the representations learned by autoen-\ncoder.\nâ€¢ DEC [26]: It is a deep clustering method which designs a\nclustering objective to guide the learning of the data repre-\nsentations.\nâ€¢ IDEC [4]: This method adds a reconstruction loss to DEC,\nso as to learn better representation.\nâ€¢ GAE & VGAE [10]: It is an unsupervised graph embedding\nmethod using GCN to learn data representations.\nâ€¢ DAEGC [25]: It uses an attention network to learn the node\nrepresentations and employs a clustering loss to supervise\nthe process of graph clustering.\nâ€¢ SDCNQ: The variant of SDCN with distribution Q.\nâ€¢ SDCN: The proposed method.\nMetrics. We employ four popular metrics: Accuracy (ACC), Nor-\nmalized Mutual Information (NMI), Average Rand Index (ARI) and\nmacro F1-score (F1). For each metric, a larger value implies a better\nclustering result.\nParameter Setting. We use the pre-trained autoencoder for all\nDNN-based clustering methods (AE+K-means, DEC, IDEC) and\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nDeyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui\nSDCN. We train the autoencoder end-to-end using all data points\nwith 30 epochs and the learning rate is 10âˆ’3. In order to be con-\nsistent with previous methods [4, 26], we set the dimension of the\nautoencoder to d-500-500-2000-10, where d is the dimension of\nthe input data. The dimension of the layers in GCN module is the\nsame to the autoencoder. As for the GCN-based methods, we set\nthe dimension of GAE and VAGE to d-256-16 and train them with\n30 epochs for all datasets. For DAEGC, we use the setting of [25].\nIn hyperparameter search, we try {1, 3, 5} for the update interval\nin DEC and IDEC, {1, 0.1, 0.01, 0.001} for the hyperparameter Î³\nin IDEC and report the best results. For our SDCN, we uniformly\nset Î± = 0.1 and Î² = 0.01 for all the datasets because our method\nis not sensitive to hyperparameters. For the non-graph data, we\ntrain the SDCN with 200 epochs, and for graph data, we train it\nwith 50 epochs. Because the graph structure with prior knowledge,\ni.e. citation network, contains more information than KNN graph,\nwhich can accelerate convergence speed. The batch size is set to\n256 and learning rate is set to 10âˆ’3 for USPS, HHAR, ACM, DBLP\nand 10âˆ’4 for Reuters, Citeseer. For all methods using K-means algo-\nrithm to generate clustering assignments, we initialize 20 times and\nselect the best solution. We run all methods 10 times and report the\naverage results to prevent extreme cases.\n4.3\nAnalysis of Clustering Results\nTable 2 shows the clustering results on six datasets. Note that in\nUSPS, HHAR and Reuters, we use the KNN graph as the input of\nthe GCN module, while for ACM, DBLP and Citeseer, we use the\noriginal graph. We have the following observations:\nâ€¢ For each metric, our methods SDCN and SDCNQ achieve\nthe best results in all the six datasets. In particular, compared\nwith the best results of the baselines, our approach achieves\na significant improvement of 6% on ACC, 17% on NMI and\n28% on ARI averagely. The reason is that SDCN successfully\nintegrates the structural information into deep clustering\nand the dual self-supervised module guides the update of\nautoencoder and GCN, making them enhance each other.\nâ€¢ SDCN generally achieves better cluster results than SDCNQ.\nThe reason is that SDCN uses the representations containing\nthe structural information learned by GCN, while SDCNQ\nmainly uses the representations learned by the autoencoder.\nHowever, in Reuters, the result of SDCNQ is much better\nthan SDCN. Because in the KNN graph of Reuters, many\ndifferent classes of nodes are connected together, which\ncontains much wrong structural information. Therefore, an\nimportant prerequisite for the application of GCN is to con-\nstruct a KNN graph with less noise.\nâ€¢ Clustering results of autoencoder based methods (AE, DEC,\nIDEC) are generally better than those of GCN-based methods\n(GAE, VAGE, DAEGC) on the data with KNN graph, while\nGCN-based methods usually perform better on the data with\ngraph structure. The reason is that GCN-based methods only\nuse structural information to learn the data representation.\nWhen the structural information in the graph is not clear\nenough, e.g. KNN graph, the performance of the GCN-based\nmethods will decline. Besides, SDCN integrates structural in-\nformation into deep clustering, so its clustering performance\nis better than these two methods.\nâ€¢ Comparing the results of AE with DEC and the results of\nGAE with DAEGC, we can find that the clustering loss func-\ntion, defined in Eq. 13, plays an important role in improving\nthe deep clustering performance. Because IDEC and DAEGC\ncan be seen as the combination of the clustering loss with AE\nand GAE, respectively. It improves the cluster cohesion by\nmaking the data representation closer to the cluster centers,\nthus improving the clustering results.\n4.4\nAnalysis of Variants\nWe compare our model with two variants to verify the ability of\nGCN in learning structural information and the effectiveness of the\ndelivery operator. Specifically, we define the following variants:\n60\n70\n80\n90\nUSPS\nHHAR\nReuters\nSDCN\nSDCN-w/o\nSDCN-MLP\n(a) Datasets with KNN graph\n60\n70\n80\n90\nACM\nDBLP\nCiteseer\nSDCN\nSDCN-w/o\nSDCN-MLP\n(b) Datasets with original graph\nFigure 2: Clustering accuracy with different variants\nâ€¢ SDCN-w/o: This variant is SDCN without delivery operator,\nwhich is used to validate the effectiveness of our proposed\ndelivery operator.\nâ€¢ SDCN-MLP: This variant is SDCN replacing the GCN mod-\nule with the same number of layers of multilayer perceptron\n(MLP), which is used to validate the advantages of GCN in\nlearning structural information.\nFrom Figure 2, we have the following observations:\nâ€¢ In Figure 2(a), we can find that the clustering accuracy of\nSDCN-MLP is better than SDCN-w/o in Reuters and achieves\nsimilar results in USPS and HHAR. This shows that in the\nKNN graph, without delivery operator, the ability of GCN\nin learning structural information is severely limited. The\nreason is that multilayer GCN will produce a serious over-\nsmoothing problem, leading to the decrease of the clustering\nresults. On the other hand, SDCN is better than SDCN-MLP.\nThis proves that the delivery operator can help GCN alleviate\nthe over-smoothing problem and learn better data represen-\ntation.\nâ€¢ In Figure 2(b), we can find that the clustering accuracy of\nSDCN-w/o is better than SDCN-MLP in all three datasets\ncontaining original graph. This shows that GCN has the pow-\nerful ability in learning data representation with structural\ninformation. Besides, SDCN performs better than SDCN-w/o\nin the three datasets. This proves that there still exists over-\nsmoothing problem in the SDCN-w/o, but the good graph\nstructure still makes SDCN-w/o achieve not bad clustering\nresults.\nStructural Deep Clustering Network\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nTable 3: Effect of different propagation layers (L)\nACC\nNMI\nARI\nF1\nACM\nSDCN-4\n90.45\n68.31\n73.91\n90.42\nSDCN-3\n89.06\n64.86\n70.51\n89.03\nSDCN-2\n89.12\n66.48\n70.94\n89.04\nSDCN-1\n77.69\n51.59\n50.13\n74.62\nDBLP\nSDCN-4\n68.05\n39.51\n39.15\n67.71\nSDCN-3\n65.11\n36.81\n36.03\n64.98\nSDCN-2\n66.72\n37.19\n37.58\n65.37\nSDCN-1\n64.19\n30.69\n33.62\n60.44\nCiteseer\nSDCN-4\n65.96\n38.71\n40.17\n61.62\nSDCN-3\n59.18\n32.11\n32.16\n55.92\nSDCN-2\n60.96\n33.69\n34.49\n57.31\nSDCN-1\n58.58\n32.91\n32.31\n52.38\nâ€¢ Comparing the results in Figure 2(a) and Figure 2(b), we can\nfind no matter on which types of datasets, SDCN achieves\nthe best performance, compared with SDCN-w/o and SDCN-\nMLP. This proves that both the delivery operator and GCN\nplay an important role in improving clustering quality.\n4.5\nAnalysis of Different Propagation Layers\nTo investigate whether SDCN benefits from multilayer GCN, we\nvary the depth of the GCN module while keeping the DNN module\nunchanged. In particular, we search the number of layers in the\nrange of {1, 2, 3, 4}. There are a total of four layers in the encoder\npart of the DNN module in SDCN, generating the representation\nH(1), H(2), H(3), H(4), respectively. SDCN-L represents that there is\na total of L layers in the GCN module. For example, SDCN-2 means\nthat H(3), H(4) will be transferred to the corresponding GCN lay-\ners for propagating. We choose the datasets with original graph to\nverify the effect of the number of the propagation layers on the clus-\ntering effect because they have the nature structural information.\nFrom Table 3, we have the following observations:\nâ€¢ Increasing the depth of SDCN substantially enhances the\nclustering performance. It is clear that SDCN-2, SDCN-3 and\nSDCN-4 achieve consistent improvement over SDCN-1 in\nall the across. Besides, SDCN-4 performs better than other\nmethods in all three datasets. Because the representations\nlearned by each layer in the autoencoder are different, to\npreserve information as much as possible, we need to put\nall the representations learned from autoencoder into corre-\nsponding GCN layers.\nâ€¢ There is an interesting phenomenon that the performance\nof SDCN-3 is not as good as SDCN-2 in all the datasets. The\nreason is that SDCN-3 uses the representation H(2), which is\na middle layer of the encoder. The representation generated\nby this layer is in the transitional stage from raw data to se-\nmantic representation, which inevitably loses some underly-\ning information and lacks of semantic information. Another\nreason is that GCN with two layers does not cause serious\nover-smoothing problems, proved in [15]. For SDCN-3, due\nto the number of layers is not enough, the over-smoothing\nterm in Eq. 22 is not small enough so that it is still plagued\nby the over-smoothing problems.\n0.60\n0.65\n0.70\n0.75\n0.80\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nÎµ\n(a) USPS\n0.65\n0.70\n0.75\n0.80\n0.85\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nÎµ\n(b) HHAR\n0.60\n0.65\n0.70\n0.75\n0.80\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nÎµ\n(c) Reuters\n0.75\n0.80\n0.85\n0.90\n0.95\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nÎµ\n(d) ACM\n0.55\n0.60\n0.65\n0.70\n0.75\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nÎµ\n(e) DBLP\n0.55\n0.60\n0.65\n0.70\n0.75\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nÎµ\n(f) Citeseer\nFigure 3: Clustering accuracy with different Ïµ\n4.6\nAnalysis of balance coefficient Ïµ\nIn previous experiments, in order to reduce hyperparameter search,\nwe uniformly set the balance coefficient Ïµ to 0.5. In this experiment,\nwe will explore how SDCN is affected by different Ïµ on different\ndatasets. In detail, we set Ïµ = {0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0}. Note\nthat Ïµ = 0.0 means the representations in GCN module do not\ncontain the representation from autoencoder and Ïµ = 1.0 represents\nthat GCN only use the representation H(L) learned by DNN. From\nFigure 4, we can find:\nâ€¢ Clustering accuracy with parameter Ïµ = 0.5 in four datasets\n(Reuters, ACM, DBLP, Citeseer) achieve the best perfor-\nmance, which shows that the representations of GCN module\nand DNN module are equally important and the improve-\nment of SDCN depends on the mutual enhancement of the\ntwo modules.\nâ€¢ Clustering accuracy with parameter Ïµ = 0.0 in all datasets\nperforms the worst. Clearly, when Ïµ = 0.0, the GCN module\nis equivalent to the standard multilayer GCN, which will\nproduce very serious over-smoothing problem [15], leading\nto the decline of the clustering quality. Compared with the\naccuracy when Ïµ = 0.1, we can find that even injecting a\nsmall amount of representations learned by autoencoder into\nGCN can help alleviate the over-smoothing problem.\nâ€¢ Another interesting observation is that SDCN with parame-\nter Ïµ = 1.0 still gets a higher clustering accuracy. The reason\nis that although SDCN with parameter Ïµ = 1.0 only use the\nrepresentation H(L), it contains the most important informa-\ntion of the raw data. After passing through one GCN layer,\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nDeyu Bo, Xiao Wang, Chuan Shi, Meiqi Zhu, Emiao Lu, and Peng Cui\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n50\n100\n150\n200\nAccuracy\nIteration\nSDCN-Q\nSDCN-Z\nSDCN-P\n(a) USPS\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n50\n100\n150\n200\nAccuracy\nIteration\nSDCN-Q\nSDCN-Z\nSDCN-P\n(b) HHAR\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n10\n20\n30\n40\n50\nAccuracy\nIteration\nSDCN-Q\nSDCN-Z\nSDCN-P\n(c) ACM\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n10\n20\n30\n40\n50\nAccuracy\nIteration\nSDCN-Q\nSDCN-Z\nSDCN-P\n(d) DBLP\nFigure 4: Training process on different datasets\n30\n40\n50\n60\n70\n80\n1\n3\n5\n10\nK\nGAE\nVGAE\nDAEGC\nSDCN\n(a) Accuracy on USPS\n30\n40\n50\n60\n70\n80\n1\n3\n5\n10\nK\nGAE\nVGAE\nDAEGC\nSDCN\n(b) Accuracy on HHAR\n30\n40\n50\n60\n70\n80\n1\n3\n5\n10\nK\nGAE\nVGAE\nDAEGC\nSDCN\n(c) Accuracy on Reuters\n30\n40\n50\n60\n70\n80\n1\n3\n5\n10\nK\nGAE\nVGAE\nDAEGC\nSDCN\n(d) NMI on USPS\n30\n40\n50\n60\n70\n80\n1\n3\n5\n10\nK\nGAE\nVGAE\nDAEGC\nSDCN\n(e) NMI on HHAR\n0\n10\n20\n30\n40\n50\n1\n3\n5\n10\nK\nGAE\nVGAE\nDAEGC\nSDCN\n(f) NMI on Reuters\nFigure 5: Clustering results with different K\nit can still achieve some structural information to improve\nclustering performance. However, due to the limitation of\nthe number of layers, the results are not the best.\n4.7\nK-sensitivity Analysis\nSince the number of the nearest neighbors K is an important param-\neter in the construction of the KNN graph, we design a K-sensitivity\nexperiment on the datasets with KNN graph. This experiment is\nmainly to prove that our model is K-insensitive. Hence we compare\nSDCN with the clustering methods focusing on the graph data (GAE,\nVGAE, DAEGC). From Figure 5, we can find that with K={1, 3, 5, 10},\nour proposed SDCN is much better than GAE, VGAE and DAEGC,\nwhich proves that our method can learn useful structural infor-\nmation even in the graphs containing noise. Another finding is\nthat these four methods can achieve good performance when K =\n3 or K = 5, but in the case of K = 1 and K = 10, the performance\nwill drop significantly. The reason is that when K = 1, the KNN\ngraph contains less structural information and when K = 10, the\ncommunities in KNN graph are over-lapping. In summary, SDCN\ncan achieve stable results compared with other baseline methods\non the KNN graphs with different number of nearest neighbors.\n4.8\nAnalysis of Training Process\nIn this section, we analyze the training progress in different datasets.\nSpecifically, we want to explore how the cluster accuracy of the\nthree sample assignments distributions in SDCN varies with the\nnumber of iterations. In Figure 4, the red line SDCN-P, the blue\nline SDCN-Q and the orange line SDCN-Z represent the accuracy\nof the target distribution P, distribution Q and distribution Z, re-\nspectively. In most cases, the accuracy of SDCN-P is higher than\nthat of SDCN-Q, which shows that the target distribution P is able\nto guide the update of the whole model. At the beginning, the re-\nsults of the accuracy of three distributions all decrease in different\nranges. Because the information learned by autoencoder and GCN\nis different, it may rise a conflict between the results of the two\nmodules, making the clustering results decline. Then the accuracy\nof SDCN-Q and SDCN-Z quickly increase to a high level, because\nthe target distribution SDCN-P eases the conflict between the two\nmodules, making their results tend to be consistent. In addition,\nwe can see that with the increase of training epochs, the cluster-\ning results of SDCN tend to be stable and there is no significant\nfluctuation, indicating the good robustness of our proposed model.\n5\nCONCLUSION\nIn this paper, we make the first attempt to integrate the structural in-\nformation into deep clustering. We propose a novel structural deep\nclustering network, consisting of DNN module, GCN module, and\ndual self-supervised module. Our model is able to effectively com-\nbine the autoencoder-spectific representation with GCN-spectific\nrepresentation by a delivery operator. Theoretical analysis is pro-\nvided to demonstrate the strength of the delivery operator. We show\nthat our proposed model consistently outperforms the state-of-the-\nart deep clustering methods in various open datasets.\nACKNOWLEDGMENTS\nThis work is supported by the National Key Research and Develop-\nment Program of China (2018YFB1402600) and the National Natural\nScience Foundation of China (No. 61772082, 61702296, 61806020,\n61972442, U1936104). It is also supported by 2018 Tencent Market-\ning Solution Rhino-Bird Focused Research Program.\nStructural Deep Clustering Network\nWWW â€™20, April 20â€“24, 2020, Taipei, Taiwan\nREFERENCES\n[1] Charu C Aggarwal and ChengXiang Zhai. 2012. A survey of text clustering\nalgorithms. In Mining text data. Springer, 77â€“128.\n[2] Mikhail Belkin and Partha Niyogi. 2003. Laplacian eigenmaps for dimensionality\nreduction and data representation. Neural computation 15, 6 (2003), 1373â€“1396.\n[3] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. 2018.\nDeep clustering for unsupervised learning of visual features. In ECCV. 132â€“149.\n[4] Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. 2017. Improved deep\nembedded clustering with local structure preservation.. In IJCAI. 1753â€“1759.\n[5] John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means\nclustering algorithm. Journal of the Royal Statistical Society. Series C (Applied\nStatistics) 28, 1 (1979), 100â€“108.\n[6] William Grant Hatcher and Wei Yu. 2018. A survey of deep learning: platforms,\napplications and emerging research trends. IEEE Access 6 (2018), 24411â€“24432.\n[7] Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensional-\nity of data with neural networks. Science 313, 5786 (2006), 504â€“507.\n[8] Pan Ji, Tong Zhang, Hongdong Li, Mathieu Salzmann, and Ian Reid. 2017. Deep\nsubspace clustering networks. In NIPS. 24â€“33.\n[9] Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou.\n2017. Variational deep embedding: An unsupervised and generative approach to\nclustering. IJCAI (2017).\n[10] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. arXiv\npreprint arXiv:1611.07308 (2016).\n[11] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph\nconvolutional networks. ICLR (2017).\n[12] Johannes Klicpera, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. 2018. Pre-\ndict then propagate: Graph neural networks meet personalized pagerank. (2018).\n[13] Yann Le Cun, Ofer Matan, Bernhard Boser, John S Denker, Don Henderson,\nRichard E Howard, Wayne Hubbard, LD Jacket, and Henry S Baird. 1990. Hand-\nwritten zip code recognition with multilayer networks. In ICPR, Vol. 2. IEEE,\n35â€“40.\n[14] David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. 2004. Rcv1: A new\nbenchmark collection for text categorization research. Journal of machine learning\nresearch 5, Apr (2004), 361â€“397.\n[15] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph\nconvolutional networks for semi-supervised learning. In AAAI.\n[16] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJournal of machine learning research 9, Nov (2008), 2579â€“2605.\n[17] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan\nFrey. 2015. Adversarial autoencoders. arXiv preprint arXiv:1511.05644 (2015).\n[18] Pankaj Malhotra, Anusha Ramakrishnan, Gaurangi Anand, Lovekesh Vig, Puneet\nAgarwal, and Gautam Shroff. 2016. LSTM-based encoder-decoder for multi-sensor\nanomaly detection. arXiv preprint arXiv:1607.00148 (2016).\n[19] Jonathan Masci, Ueli Meier, Dan CireÅŸan, and JÃ¼rgen Schmidhuber. 2011. Stacked\nconvolutional auto-encoders for hierarchical feature extraction. In ICANN.\nSpringer, 52â€“59.\n[20] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted\nboltzmann machines. In ICML. 807â€“814.\n[21] Andrew Y Ng, Michael I Jordan, and Yair Weiss. 2002. On spectral clustering:\nAnalysis and an algorithm. In NIPS. 849â€“856.\n[22] Peter J Rousseeuw. 1987. Silhouettes: a graphical aid to the interpretation and\nvalidation of cluster analysis. Journal of computational and applied mathematics\n20 (1987), 53â€“65.\n[23] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\nMikkel Baun KjÃ¦rgaard, Anind Dey, Tobias Sonne, and Mads MÃ¸ller Jensen.\n2015. Smart devices are different: Assessing and mitigatingmobile sensing het-\nerogeneities for activity recognition. In SenSys. ACM, 127â€“140.\n[24] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.\n2008. Extracting and composing robust features with denoising autoencoders. In\nICML. ACM, 1096â€“1103.\n[25] Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi Zhang.\n2019. Attributed Graph Clustering: A Deep Attentional Embedding Approach.\nIJCAI (2019).\n[26] Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016. Unsupervised deep embedding\nfor clustering analysis. In ICML. 478â€“487.\n[27] Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. 2017. Towards\nk-means-friendly spaces: Simultaneous deep learning and clustering. In ICML.\n3861â€“3870.\n[28] Yi Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting Zhuang. 2010. Im-\nage clustering using local discriminant models and global integration. IEEE\nTransactions on Image Processing 19, 10 (2010), 2761â€“2773.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-02-05",
  "updated": "2020-02-12"
}