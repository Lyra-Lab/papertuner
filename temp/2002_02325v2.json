{
  "id": "http://arxiv.org/abs/2002.02325v2",
  "title": "Social diversity and social preferences in mixed-motive reinforcement learning",
  "authors": [
    "Kevin R. McKee",
    "Ian Gemp",
    "Brian McWilliams",
    "Edgar A. Duéñez-Guzmán",
    "Edward Hughes",
    "Joel Z. Leibo"
  ],
  "abstract": "Recent research on reinforcement learning in pure-conflict and pure-common\ninterest games has emphasized the importance of population heterogeneity. In\ncontrast, studies of reinforcement learning in mixed-motive games have\nprimarily leveraged homogeneous approaches. Given the defining characteristic\nof mixed-motive games--the imperfect correlation of incentives between group\nmembers--we study the effect of population heterogeneity on mixed-motive\nreinforcement learning. We draw on interdependence theory from social\npsychology and imbue reinforcement learning agents with Social Value\nOrientation (SVO), a flexible formalization of preferences over group outcome\ndistributions. We subsequently explore the effects of diversity in SVO on\npopulations of reinforcement learning agents in two mixed-motive Markov games.\nWe demonstrate that heterogeneity in SVO generates meaningful and complex\nbehavioral variation among agents similar to that suggested by interdependence\ntheory. Empirical results in these mixed-motive dilemmas suggest agents trained\nin heterogeneous populations develop particularly generalized, high-performing\npolicies relative to those trained in homogeneous populations.",
  "text": "Social Diversity and Social Preferences\nin Mixed-Motive Reinforcement Learning\nKevin R. McKee, Ian Gemp, Brian McWilliams,\nEdgar A. Duéñez-Guzmán, Edward Hughes, & Joel Z. Leibo\nDeepMind\nLondon\n{kevinrmckee,imgemp,bmcw,duenez,edwardhughes,jzl}@google.com\nABSTRACT\nRecent research on reinforcement learning in pure-conflict and\npure-common interest games has emphasized the importance of\npopulation heterogeneity. In contrast, studies of reinforcement\nlearning in mixed-motive games have primarily leveraged homo-\ngeneous approaches. Given the defining characteristic of mixed-\nmotive games—the imperfect correlation of incentives between\ngroup members—we study the effect of population heterogeneity on\nmixed-motive reinforcement learning. We draw on interdependence\ntheory from social psychology and imbue reinforcement learning\nagents with Social Value Orientation (SVO), a flexible formaliza-\ntion of preferences over group outcome distributions. We subse-\nquently explore the effects of diversity in SVO on populations of\nreinforcement learning agents in two mixed-motive Markov games.\nWe demonstrate that heterogeneity in SVO generates meaningful\nand complex behavioral variation among agents similar to that\nsuggested by interdependence theory. Empirical results in these\nmixed-motive dilemmas suggest agents trained in heterogeneous\npopulations develop particularly generalized, high-performing poli-\ncies relative to those trained in homogeneous populations.\n1\nINTRODUCTION\nIn multi-agent reinforcement learning, the actions of one agent\ncan influence the experience and outcomes for other agents—that\nis, agents are interdependent. Interdependent interactions can be\nsorted into two categories based on the alignment of incentives for\nthe agents involved [47]:\n(1) Pure-motive interactions, in which the group’s incentives are\neither entirely aligned (pure-common interest) or entirely\nopposed (pure-conflict),\n(2) and mixed-motive interactions, in which the group’s incen-\ntives are sometimes aligned and sometimes in conflict.1\nExamples of the former include games such as Hanabi [6] and Go\n[49]. The latter category is typified by games like the Prisoner’s\nDilemma [46, 53] and the tragedy of the commons [20, 31]. This\n1When Schelling originally introduced the pure- and mixed-motive framework, he\nexplained, “Mixed-motive refers not, of course, to an individual’s lack of clarity about\nhis own preferences but rather to the ambivalence of his relation to the other player—\nthe mixture of mutual dependence and conflict, of partnership and competition” [47].\nProc. of the 19th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2020), B. An, N. Yorke-Smith, A. El Fallah Seghrouchni, G. Sukthankar (eds.),\nMay 2020, Auckland, New Zealand\n© 2020 International Foundation for Autonomous Agents and Multiagent Systems\n(www.ifaamas.org). All rights reserved.\ncategorical distinction is especially relevant for spatially and tem-\nporally extended Markov games. In these games, interdependence\nemerges both as a direct impact of one agent’s actions on another’s\noutcomes and as an indirect effect of each agent on the state of the\nsubstrate environment in which others co-exist.\nHomogeneous\nPopulation\nHeterogeneous\nPopulation\nSamples\nSamples\nHomogeneous\nHeterogeneous\nPure-common interest\nMixed-motive\nPure-conﬂict\nOvercooked [11]\nSwitch [52]\nCoins [32]\nGo [49]\nStarCraft II [55]\n*This paper\na)\nb)\n푖\n푖\n푖\n푖\n푖\n푖\n푖\n푖\nFigure 1: Homogeneity and heterogeneity in population-\nbased multi-agent reinforcement learning. (a) Population\nhomogeneity and heterogeneity result in different training\nexperiences for a given agent i. In the homogeneous case,\nagent policies are either identical or very similar (e.g., due\nto identical training distributions or shared motivations).\nIn the heterogeneous setting, a given agent i encounters a\nrange of group compositions over time. The variability in\npolicies can stem from agents training under different distri-\nbutions or with different motivations. (b) Representative ex-\namples of previous multi-agent reinforcement learning re-\nsearch. We study the mixed-motive, heterogeneous setting.\narXiv:2002.02325v2  [cs.MA]  12 Feb 2020\nAAMAS’20, May 2020, Auckland, New Zealand\nK. McKee et al.\nIn pure-conflict reinforcement learning, self-play solutions for\nMarkov games [5, 22, 49] have gradually given way to population-\nbased approaches [26, 55] (Figure 1a). A central impetus for this\nshift has been an interest in ensuring agent performance is robust\nto opponent heterogeneity (i.e., variation in the set of potential\nopponent policies). Similarly, recent work on pure-common inter-\nest reinforcement learning in Markov games has highlighted the\nimportance of robustness to diverse partner policies [1, 11]. In both\nof these contexts, it is desirable to train agents capable of adapting\nand best responding to a wide range of potential policy sets.\nIn mixed-motive Markov games, the effects of partner hetero-\ngeneity have not received much attention. Most mixed-motive rein-\nforcement learning research has produced policies through self-play\n[32, 43] or co-training policies in fixed groups [24, 31]. Such meth-\nods foster homogeneity in the set of policies each agent encounters.\nWe aim to introduce policy heterogeneity into mixed-motive\nreinforcement learning (Figure 1b). In recent years, a growing body\nof work has explored the effects of furnishing agents in mixed-\nmotive games with various motivations such as inequity aversion\n[24, 56], social imitation [15], and social influence [28]. Thus, a\nnatural starting point for the study of heterogeneity is to explore the\neffects of diversity in intrinsic motivation [51]. Here we endow our\nagents with Social Value Orientation (SVO), an intrinsic motivation\nto prefer certain group reward distributions between self and others.\nPsychology and economics research has repeatedly demonstrated\nthat human groups sustain high levels of cooperation across differ-\nent games through heterogeneous distributive preferences [7, 13,\n16, 45, 50]. A particularly compelling account from interdependence\nc\nc\nd\nd\n-1, -1\n-4, 1\n1, -4\n-2, -2\nc\nc\nd\nd\n-1 - 1\n-4 + 1\n1 - 4\n-2 - 2\nc\nc\nd\nd\nc\nc\nd\nd\n-1 + 0\n-4 - 5\n1 + 5\n-2 + 0\n-1\n-9\n6\n-2\nc\nc\nd\nd\n-2\n-3\n-3\n-4\nGiven matrix\nEﬀective matrix\nOutcome\ntransformation\nFigure 2: Interdependence theory. The top pathway depicts\na transformation process for a row player who has altru-\nistic preferences. In this case, the outcome transformation\ndirectly transfers the column player’s payoff into the ef-\nfective matrix. The bottom pathway shows a transforma-\ntion process for a row player with competitive preferences,\nwho finds it rewarding to maximize the distance between\ntheir payoff and the column player’s payoff. These two out-\ncome transformations suggest different dominant strategies\n(highlighted in blue).\ntheory holds that humans deviate from game theoretic predictions\nin economic games because each player acts not on the “given ma-\ntrix” of a game—which reflects the extrinsic payoffs set forth by the\ngame rules—but on an “effective matrix”, which represents the set\nof outcomes as subjectively evaluated by that player [29]. Players\nreceive the given matrix from their environment and subsequently\napply an “outcome transformation” reflecting their individual pref-\nerences for various outcome distributions. The combination of the\ngiven matrix and the outcome transformation form the effective\nmatrix (Figure 2). Though an individual’s social preferences may de-\ncrease their given payoff in a single game, groups with diverse sets\nof preferences are capable of resisting suboptimal Nash equilibria.\nIn multi-agent reinforcement learning, reward sharing is com-\nmonly used to resolve mixed-motive dilemmas [24, 44, 52, 56]. To\ndate, agent hyperparameters controlling reward mixture have typi-\ncally been shared. This approach implies a homogeneous popula-\ntion of policies, echoing the representative agent assumption from\neconomics [21, 30]. The continued reliance on shared reward mix-\ntures is especially striking considering that the ability to capture\nheterogeneity is a key strength of agent-based models over other\nmodeling approaches [19].\nHomogeneous populations often fall prey to a peculiar variant\nof the lazy agent problem [52], wherein one or more agents begin\nignoring the individual learning task at hand and instead optimize\nfor the shared reward [24]. These shared-reward agents shoulder\nthe burden of prosocial work, in a manner invariant to radical\nshifts in group composition across episodes. This “specialization”\nrepresents a failure of training to generate generalized policies.\nTo investigate the effects of heterogeneity in mixed-motive rein-\nforcement learning, we introduce a novel, generalized mechanism\nfor reward sharing. We derive this reward-sharing mechanism, So-\ncial Value Orientation (SVO), from studies of human cooperative be-\nhavior in social psychology. We show that across several games, het-\nerogeneous distributions of these social preferences within groups\ngenerate more generalized individual policies than do homoge-\nneous distributions. We subsequently explore how heterogeneity in\nSVO sustains positive group outcomes. In doing so, we demonstrate\nthat this formalization of social preferences leads agents to discover\nspecific prosocial behaviors relevant to each environment.\n2\nAGENTS\n2.1\nMulti-agent reinforcement learning and\nMarkov games\nIn this work, we consider n-player partially observable Markov\ngames. A partially observable Markov game M is defined on a\nfinite set of states S. The game is endowed with an observation\nfunction O : S × {1, . . . ,n} →Rd; a set of available actions for\neach player, A1, . . . , An; and a stochastic transition function T :\nS×A1×· · ·×An →∆(S), which maps from the joint actions taken\nby the n players to the set of discrete probability distributions over\nstates. From each state, players take a joint action ®a = (a1, . . . ,an) ∈\nA1, . . . , An.\nEach agent i independently experiences the environment and\nlearns a behavior policy π(ai |oi) based on its own observation\noi = O(s,i) and (scalar) extrinsic reward ri(s, ®a). Each agent learns\nSocial Diversity and Social Preferences in Mixed-Motive Reinforcement Learning\nAAMAS’20, May 2020, Auckland, New Zealand\na policy which maximizes a long term γ-discounted payoff defined\nas:\nV®πi (s0) = E\n\" ∞\nÕ\nt=0\nγ tUi(st, ®ot, ®at )|®at ∼®πt,st+1 ∼T(st, ®at )\n#\n(1)\nwhere Ui(st, ®ot, ®at ) is a utility function and ®o = (o1, . . . ,on) for\nsimplicity. In standard reinforcement learning, the utility function\nmaps directly to the extrinsic reward provided by the environment.\n2.2\nSocial Value Orientation\nHere we introduce Social Value Orientation (SVO), an intrinsic\nmotivation to prefer certain group reward distributions between\nself and others.\nWe introduce the concept of a reward angle as a scalar represen-\ntation of the observed distribution of reward between player i and\nall other players in the group. The size of the angle formed by these\ntwo scalars represents the relative distribution of reward between\nself and others (Figure 3). Given a group of size n, its corresponding\nreward vector is ®R = (r1, . . . ,rn). The reward angle for player i is:\nθ(®R) = atan\n\u0012 ¯r−i\nri\n\u0013\n(2)\nwhere ¯r−i is a statistic summarizing the rewards of all other group\nmembers. We choose the arithmetic mean ¯r−i =\n1\nn−1\nÍ\nj,i rj [38].\nNote that reward angles are invariant to the scalar magnitude of ®R.\nThe Social Value Orientation, θSVO, for player i is player i’s\ntarget distribution of reward among group members. We use the\ndifference between the observed reward angle and the targeted SVO\nto calculate an intrinsic reward. Combining this intrinsic reward\nwith the extrinsic reward signal from the environment, we can\ndefine the following utility function Ui to be maximized in Eq. (1):\nUi(s,oi,ai) = ri −w · |θSVO −θ(®R)|\n(3)\nwhere w is a weight term controlling the effect of Social Value\nOrientation on Ui.\nIn constructing this utility function, we follow a standard ap-\nproach in mixed-motive reinforcement learning research [24, 28, 56]\nwhich provides agents with an overall reward signal combining\nextrinsic and intrinsic reward [51]. This approach parallels interde-\npendence theory, wherein the effective matrix is formed from the\ncombination of the given matrix and the outcome transformation,\nand ultimately serves as the basis of actors’ decisions [29].\nFor the exploratory work we detail in the following sections,\nwe restrict our experiments to SVO in the non-negative quadrant\n(all θ ∈[0°, 90°]). The preference profiles in the non-negative quad-\nrant provide the closest match to parameterizations in previous\nmulti-agent research on reward sharing. Nonetheless, we note that\ninteresting preference profiles exist throughout the entire ring [37].\n2.3\nAlgorithm\nWe deploy advantage actor-critic (A2C) as the learning algorithm\nfor our agents [35]. A2C maintains both value (critic) and policy\n(actor) estimates using a deep neural network. The policy is up-\ndated according to the REINFORCE policy-gradient method, using a\nvalue estimate as a baseline to reduce variance. Our neural network\nAltruistic\nProsocial\nIndividualistic\nMartyrial\nCompetitive\nr푖\nr–\n 푖\nθ\n_\nFigure 3: Reward angles and the ring formulation of Social\nValue Orientation (SVO). Reward angles are a scalar repre-\nsentation of the tradeoff between an agent’s own reward and\nthe reward of other agents in the environment. The reward\nangle an agent prefers is its SVO.\ncomprises a convolutional layer, a feedforward module, an LSTM\nwith contrastive predictive coding [41], and linear readouts for pol-\nicy and value. We apply temporal smoothing to observed rewards\nwithin the model’s intrinsic motivation function, as described by\n[24].\nWe use a distributed, asynchronous framework for training [56].\nWe train populations of N = 30 agents with policies {πi }. For each\npopulation, we sample n = 5 players at a time to populate each of\n100 arenas running in parallel (see also Figure 1a, in which arenas\nare represented as “samples” from the agent population). Each arena\nis an instantiation of a single episode of the environment. Within\neach arena, the sampled agents play an episode of the environment,\nafter which a new group is sampled. Episode trajectories last 1000\nsteps and are written to queues for learning. Weights are updated\nfrom queues using V-Trace [17].\n3\nMIXED-MOTIVE GAMES\n3.1\nIntertemporal social dilemmas\nFor our experiments, we consider two temporally and spatially\nextended mixed-motive games played with group size n = 5: Har-\nvestPatch and Cleanup. These two environments are intertemporal\nsocial dilemmas, a particular class of mixed-motive Markov games\n(c.f., [33]).\nIntertemporal social dilemmas are group situations which present\na tension between short-term individual incentives and the long-\nterm collective interest [24]. Each individual has the option of\nbehaving prosocially (cooperation) or selfishly (defection). Though\nunanimous cooperation generates welfare-maximizing outcomes\nin the long term, on short timescales the personal benefits of acting\nselfishly strictly dominate those of prosocial behavior. Thus, though\nall members of the group prefer the rewards of mutual coopera-\ntion, the intertemporal incentive structure pushes groups toward\nAAMAS’20, May 2020, Auckland, New Zealand\nK. McKee et al.\nApple\nAgent\nApple patch\nCleaning beam\nPollution\nb)\na)\nFigure 4: Screenshots of gameplay from (a) HarvestPatch and (b) Cleanup.\nwelfare-suppressing equilibria. Previous work has evaluated the\ngame theoretic properties of intertemporal social dilemmas [24].\n3.2\nHarvestPatch\nHarvestPatch is a variant of the common-pool resource appro-\npriation game Harvest [24] (Figure 4a). Players are rewarded for\ncollecting apples (reward +1) within a 24 × 26 gridworld environ-\nment. Apples regrow after being harvested at a rate dependent on\nthe number of unharvested apples within a regrowth radius of 3. If\nthere are no apples within its radius, an apple cannot regrow. At\nthe beginning of each episode, apples are probabilistically spawned\nin a hex-like pattern of patches, such that each apple is within the\nregrowth radius of all other apples in its patch and outside of the\nregrowth radius of apples in all other patches. This creates localized\nstock and flow properties [18] for each apple patch. Each patch is\nirreversibly depleted when all of its apples have been harvested—\nregardless of how many apples remain in other patches. Players\nare also able to use a beam to punish other players (reward −50), at\na small cost to themselves (reward −1). This enables the possible\nuse of punishment to discourage free-riding [23, 40].\nA group can achieve indefinite sustainable harvesting by ab-\nstaining from eating “endangered apples” (apples which are the last\nunharvested apple remaining in their patch). However, the reward\nfor sustainable harvesting only manifests after a period of regrowth\nif all players abstain. In contrast, an individual is immediately and\nunilaterally guaranteed the reward for eating an endangered apple\nif it acts greedily. This creates a dilemma juxtaposing the short-term\nindividual temptation to maximize reward through unsustainable\nbehavior and the long-term group interest of generating higher\nreward by acting sustainably.\nIn HarvestPatch, episodes last 1000 steps. Each agent’s observ-\nability is limited to a 15 × 15 RGB window, centered on its current\nlocation. The action space consists of movement, rotation, and use\nof the punishment beam (8 actions total).\n3.3\nCleanup\nCleanup [24] is a public goods game (Figure 4b). Players are again\nrewarded for collecting apples (reward +1) within a 25 × 18 grid-\nworld environment. In Cleanup, apples grow in an orchard at a\nrate inversely related to the cleanliness of a nearby river. The river\naccumulates pollution with a constant probability over time. Be-\nyond a certain threshold of pollution, the apple growth rate in the\norchard drops to zero. Players have an additional action allowing\nthem to clean a small amount of pollution from the river. How-\never, the cleaning action only works on pollution within a small\ndistance in front of the agents, requiring them to physically leave\nthe apple orchard to clean the river. Thus, players maintain the\npublic good of orchard regrowth through effortful contributions.\nAs in HarvestPatch, players are able to use a beam to punish other\nplayers (reward −50), at a small cost to themselves (reward −1).\nA group can achieve continuous apple growth in the orchard\nby keeping the pollution levels of the river consistently low over\ntime. However, on short timescales, each player would prefer to\ncollect apples in the orchard while other players provide the public\ngood in the river. This creates a tension between the short-term\nindividual incentive to maximize reward by staying in the orchard\nand the long-term group interest of maintaining the public good\nthrough sustained contributions over time.\nEpisodes last 1000 steps. Agent observability is again limited to\na 15 × 15 RGB window, centered on the agent’s current location. In\nCleanup, agents have an additional action for cleaning (9 actions\ntotal).\n4\nRESULTS\n4.1\nSocial diversity and agent generality\nWe began by training 12 homogeneous populations per task, with\nN = 30: four consisting of individualistic agents (all θ = 0°), four\nof prosocial agents (all θ = 45°), and four of altruistic agents (all\nSocial Diversity and Social Preferences in Mixed-Motive Reinforcement Learning\nAAMAS’20, May 2020, Auckland, New Zealand\na)\nb)\nFigure 5: Episode rewards for homogeneous selfish popula-\ntions playing (a) HarvestPatch and (b) Cleanup. Each indi-\nvidual line shows a single agent’s return over training.\nθ = 90°) agents. These resemble previous approaches using self-\nishness [42], inequity aversion [24, 56], and strong reward sharing\n[44, 52], respectively. The population training curves for homoge-\nneous selfish populations closely resembled group training curves\nfrom previous studies [24, 42] (see sample population training\ncurves in Figure 5). In particular, performance in both environ-\nments generated negative returns at the beginning of training due\nto high-frequency use of the punishment beam. Agents quickly\nimproved performance by learning not to punish one another, but\nfailed to learn cooperative policies. Ultimately, selfish agents were\nunable to consistently avoid the tragedy of the commons in Har-\nvestPatch or provide public goods in Cleanup.\nOptimal hyperparameter values may vary between HarvestPatch\nand Cleanup. Thus, we selected the weight values w for the two\ntasks by conducting an initial sweep overw with homogeneous pop-\nulations of N = 20 altruistic agents (all θ = 90°). In HarvestPatch,\na weight w = 0.2 produced the highest collective returns across\nseveral runs (Figure 6a). In Cleanup, a weight w = 0.1 produced\nthe highest collective returns across several runs (Figure 6b).\nAs expected, in HarvestPatch, the highest collective returns\namong the homogeneous populations were achieved by the al-\ntruistic populations (Table 1, Homogeneous row). The prosocial\nand individualistic populations performed substantially worse. In\nCleanup, the highest collective returns similarly emerged among\nthe altruistic populations. The populations of prosocial and individ-\nualistic agents, in contrast, achieved near-zero collective returns.\na)\nb)\nFigure 6: Equilibrium collective return for homogeneous\npopulations of altruistic agents in (a) HarvestPatch and (b)\nCleanup. Closed dots reflect populations in which all agents\nreceive positive returns at equilibrium. Open dots indicate\npopulations in which some agents receive zero or negative\nreward.\nMean SVO\nHarvestPatch\nCleanup\nHomogeneous\n0°\n587.6 (101.7)\n-9.9 (11.7)\n45°\n665.9 (52.4)\n1.1 (2.1)\n90°\n1552.7 (248.2)\n563.8 (235.2)\nHeterogeneous\n15°\n553.4 (574.6)\n-0.1 (5.7)\n30°\n658.7 (107.1)\n2.0 (2.4)\n45°\n764.1 (236.3)\n6.3 (7.1)\n60°\n860.9 (121.5)\n318.5 (335.0)\n75°\n1167.9 (232.6)\n1938.5 (560.6)\nTable 1: Mean collective returns achieved at equilibrium by\nhomogeneous and heterogeneous populations. Standard de-\nviations are reported in parentheses.\nWe next trained 80 heterogeneous populations per task. To gen-\nerate each heterogeneous population, we sampled N = 30 SVO\nvalues from a normal distribution with a specified mean and disper-\nsion. Since we treated SVO as a bounded variable for these initial\nexperiments, we selected five equally spaced values from 15° to 75°\nto act as population means and and four equally spaced values from\n5.6° ( π\n32 radians) to 11.3° ( π\n16 radians) to act as population standard\ndeviations. For each mean-standard deviation pair, we generated\nfour populations using different random seeds. We used the same\nweights w as for the homogeneous populations.\nAmong the heterogeneous populations, we observed the highest\nequilibrium collective returns among the 75° population (Table 1,\nHeterogeneous row). In HarvestPatch, the performance of homo-\ngeneous altruistic populations outstripped the performance of the\n75° populations. In Cleanup, the reverse pattern emerged: the high-\nest collective returns among all populations are achieved by the\nheterogeneous 75° populations.\nWe unexpectedly find that homogeneous populations of altru-\nistic agents produced lower equality scores than most other ho-\nmogeneous and heterogeneous populations (Table 2). Homoge-\nneous, altruistic populations earned relatively high collective re-\nturns in both tasks. However, in each case the produced rewards\nwere concentrated in a small proportion of the population. Agents\nMean SVO\nHarvestPatch\nCleanup\nHomogeneous\n0°\n0.90 (0.09)\n0.16 (0.09)\n45°\n0.97 (0.01)\n0.54 (0.07)\n90°\n0.29 (0.03)\n0.41 (0.08)\nHeterogeneous\n15°\n0.90 (0.13)\n0.34 (0.09)\n30°\n0.94 (0.06)\n0.40 (0.09)\n45°\n0.95 (0.03)\n0.38 (0.10)\n60°\n0.91 (0.02)\n0.64 (0.21)\n75°\n0.76 (0.04)\n0.87 (0.08)\nTable 2: Mean equality scores achieved at equilibrium by ho-\nmogeneous and heterogeneous populations. Equality is cal-\nculated as the inverse Gini coefficient, with 0 representing\nall reward being received by a single agent and 1 represent-\ning all agents receiving an identical positive reward. Stan-\ndard deviations are reported in parentheses.\nAAMAS’20, May 2020, Auckland, New Zealand\nK. McKee et al.\na)\nb)\nFigure 7: Equilibrium performance of homogeneous and\nheterogeneous agent populations in HarvestPatch. (a) Het-\nerogeneous populations enjoyed significantly higher me-\ndian return at equilibrium. (b) Among heterogeneous pop-\nulations, the highest median returns emerged among the\npopulations whose SVO distributions have both a high mean\nand a high standard deviation.\nin these homogeneous populations appear to adopt a lazy-agent\napproach [52] to resolve the conflict between the group’s shared\npreferences for selfless reward distributions. To break the symmetry\nof this dilemma, most agents in the population selflessly support\ncollective action, thereby optimizing for their social preferences. A\nsmaller number of agents then specialize in accepting the generated\nreward—shouldering the “burden” of being selfish, in contravention\nof their intrinsic preferences. This result highlights a drawback of\nusing collective return as a performance metric. Though collective\nreturn is the traditional social outcome metric used in multi-agent\nreinforcement learning, it can mask high levels of inequality.\nWe therefore revisited population performance by measuring\nmedian return, which incorporates signal concerning the efficiency\nand the equality of a group’s outcome distribution [8]. Median\nreturn can help estimate the generality of learned policies within\nhomogeneous and heterogeneous populations. We compare median\nreturn for the two population types by measuring the median return\nfor each population after it reaches equilibrium. We conduct a\nWelch’s t-test and report the resulting t-statistic, degrees of freedom,\nand p-value. We subsequently provide effect estimates (β) and p-\nvalues from linear models regressing median return on the mean\nand standard deviation of population SVO.\nFigures 7 and 8 show the generality of policies trained in Harvest-\nPatch and Cleanup, respectively. In HarvestPatch, heterogeneous\npopulations enjoyed significantly higher median return (µ = 27.8)\nthan homogeneous populations (µ = 15.8) at equilibrium, t(13.7) =\n4.21,p < 0.001 (Figure 7a). Among heterogeneous populations, a\nclear pattern could be observed: the higher the population mean\nSVO, the higher the median return received (Figure 7b). Specifically\nfor populations with high mean SVO, median return appeared to\nincrease slightly when the SVO distribution was more dispersed.\nWhen tested with a linear model regressing median return on the\nmean and standard deviation of SVO, these trends primarily mani-\nfested as an interaction effect between mean population SVO and\nthe standard deviation of population SVO, β = 0.025, p = 0.030. In\nCleanup, heterogeneous populations received significantly higher\na)\nb)\nFigure 8: Equilibrium performance of homogeneous and\nheterogeneous agent populations in Cleanup. (a) Heteroge-\nneous populations received higher median return at equi-\nlibrium. (b) Among heterogeneous populations, the highest\nmedian returns emerged among the populations with a high\nmean SVO.\nmedian return (µ = 14.4) than homogeneous populations (µ = 4.2)\nat equilibrium, t(35.9) = 2.43,p = 0.020 (Figure 8a). Among hetero-\ngeneous populations, the highest median returns were observed in\ntandem with high mean SVO (Figure 8b). However, in this case the\neffect of the interaction between mean SVO and standard deviation\nof SVO was non-significant, p = 0.30.\nIn summary, our comparison of homogeneous and heteroge-\nneous populations shows that populations of altruists performed\neffectively in traditional terms (collective return). However, these\npopulations produced highly specialized agents, resulting in unde-\nsirably low equality metrics. Populations with diverse SVO distribu-\ntions were able to circumvent this symmetry-breaking problem and\nachieve high levels of median return in HarvestPatch and Cleanup.\n4.2\nSocial preferences and prosocial behavior\nHow exactly does the distribution of SVO help diverse popula-\ntions resolve these social dilemmas? We next evaluated the behav-\nioral effects of SVO by examining a single, heterogeneous popula-\ntion within each task. We randomly selected two populations that\na)\nb)\nFigure 9: Correlation between target and observed reward\nangles among SVO agents in (a) HarvestPatch and (b)\nCleanup. The higher an agent’s SVO, the higher the reward\nangles it tended to observe.\nSocial Diversity and Social Preferences in Mixed-Motive Reinforcement Learning\nAAMAS’20, May 2020, Auckland, New Zealand\nFigure 10: SVO and prosocial behavior in HarvestPatch.\nAgents with higher SVOs were significantly more likely to\nabstain from depleting local resource stocks. Here the yel-\nlow agent faces the choice of consuming an endangered ap-\nple for immediate reward or abstaining and traveling to a\ndifferent patch.\nachieved high equilibrium performance during training, parame-\nterized with mean SVOs of 75° and standard deviations of 7.5°. We\ngathered data from 100 episodes of play for both of these evalu-\nation experiments, sampling 5 agents randomly for each episode.\nAll regressions reported in this section are mixed error-component\nmodels, incorporating a random effect to account for the repeated\nsampling of individual agents. The accompanying figures depict\naverage values per agent, with superimposed regression lines rep-\nresenting the fixed effect estimate (β) of SVO.\nIn our evaluation experiments, we observed a positive relation-\nship between an agent’s target reward angle and the group reward\nangles it tended to observe in HarvestPatch, β = 1.51, p < 0.001\n(Figure 9a). The effect of SVO on observed reward angle was simi-\nlarly significant in Cleanup, β = 2.08, p < 0.001 (Figure 9b). This\nreflects the association of higher agent SVO with the realization of\nmore-prosocial distributions. In both environments, the estimated\neffect lies below the 45° line, indicating that agents acted somewhat\nmore selfishly than their SVO would suggest.\nIn HarvestPatch, an agent’s prosociality can be estimated by\nmeasuring its abstention from consuming endangered apples. We\ncalculated abstention as an episode-level metric incorporating the\nnumber of endangered apples an agent consumed and a normaliza-\ntion factor encoding at what points in the episode the endangered\napples were consumed. An abstention score of 1 indicates that\nan agent did not eat a single endangered apple (or that it ate one\nor more endangered apples on the final step of the episode). An\nabstention score of 0, though not technically achievable, would\nindicate that an agent consumed one endangered apple from every\napple patch in the environment on the first step of the episode. We\nobserve a significant and positive relationship between an agent’s\nSVO and its abstention, β = 0.0065, p = 0.006 (Figure 10).\nThe structure of the HarvestPatch environment creates localized\nstock and flow components. Hosting too many agents in a single\npatch threatens to quickly deplete the local resource pool. Thus,\none rule groups can use to maintain sustainability is for group\nmembers to harvest in separate patches, rather than harvesting to-\ngether and sequentially destroying the environment’s apple patches.\nFigure 11: SVO and prosocial conventions in HarvestPatch.\nThe higher an agent’s SVO, the more distance it tended to\nmaintain from other agents in its environment. Here the\nteal agent is maintaining a particularly high interagent dis-\ntance, allowing it to sustainably harvest from a single patch.\nWe find that SVO correlated with distance maintained from other\ngroup members, β = 0.005, p = 0.016 (Figure 11). Consequently,\ngroups with higher mean SVO established stronger conventions\nof interagent distance. This simple behavioral convention helped\nhigher-SVO groups guard against environmental collapse.\nIn Cleanup, an agent’s prosociality can be estimated by measur-\ning the amount of pollution it cleans from the river. There was a\nsignificant and positive relationship between an agent’s SVO and\nthe amount of pollution it cleaned, β = 1.68, p = 0.001 (Figure 12).\nAgents with higher SVOs acted more prosocially by making greater\ncontributions to the public good.\nFinally, do SVO agents develop any sort of prosocial conventions\nin Cleanup to help maintain high levels of river cleanliness? In\nCleanup, we examined one potential coordinating convention that\nwe term behavioral preparedness: an inclination to transition from\nharvesting to cleaning even before the orchard is fully depleted.\nIn Cleanup, groups that follow the short-term, individual-level\nincentive structure will respond primarily to the depletion of the\nFigure 12: SVO and prosocial behavior in Cleanup. Agents\nwith higher SVOs cleaned a significantly greater amount of\npollution per episode than did peers with low SVO. Here the\npink agent is actively cleaning two cells of pollution from\nthe river. The yellow agent is using its cleaning action out-\nside of the river, which does not affect its contribution score.\nAAMAS’20, May 2020, Auckland, New Zealand\nK. McKee et al.\nFigure 13: SVO and prosocial conventions in HarvestPatch.\nAgents with higher SVOs were significantly more likely to\nenter the river while there were unharvested apples within\nview. Here the magenta agent is transitioning to clean the\nriver, even though it can observe multiple unharvested ap-\nples in the orchard.\norchard, rather than acting preventatively to ensure the public\ngood is sustained over time. Groups that adopt welfare-maximizing\nstrategies, on the other hand, will not wait for the orchard to be\nfully harvested to clean the river. We find a positive relationship\nbetween the average number of apples observable to agents at\nthe times of their transitions to cleaning in each episode, β =\n0.028, p < 0.001. The size and significance of this effect are not\nmeaningfully affected by controlling for the number of times each\nagent transitioned to the river in a given episode, β = 0.028, p <\n0.001 (Figure 13). In aggregate, this behavioral pattern helped high-\nSVO groups maintain higher levels of orchard regrowth over time.\n5\nDISCUSSION\nRecent research on pure-conflict and pure-cooperation reinforce-\nment learning has highlighted the importance of developing robust-\nness to diversity in opponent and partner policies [11, 26, 55]. We\nextend this argument to the mixed-motive setting, focusing in par-\nticular on the effects of heterogeneity in social preferences. Drawing\nfrom interdependence theory, we endow agents with Social Value\nOrientation (SVO), a flexible formulation for reward sharing among\ngroup members.\nIn the mixed-motive games considered here, homogeneous pop-\nulations of pure altruists achieved high collective returns. How-\never, these populations tended to produce hyper-specialized agents\nwhich reaped reward primarily from either intrinsic or extrinsic\nmotivation, rather than both—a method of breaking the symmetry\nof the shared motivation structure. Thus, when equality-sensitive\nmetrics are considered, populations with diverse distributions of\nSVO values were able to outperform homogeneous populations.\nThis pattern echoes the historic observation from interdepen-\ndence theory that, if both players in a two-player matrix game adopt\na “maximize other’s outcome” transformation process, the resulting\neffective matrices often produce deficient group outcomes:\n“It must be noted first that, in a number of matrices with a\nmutual interest in prosocial transformations, if one person acts\naccording to such a transformation, the other is better off by\nacting according to his own given outcomes than by adopting a\nsimilar transformation.” [29]\nThis quote highlights a striking parallel between our findings and\nthe predictions of interdependence theory. We believe this is in-\ndicative of a broader overlap in perspective and interests between\nmulti-agent reinforcement learning and the social-behavioral sci-\nences. Here we capitalize on this overlap, drawing inspiration from\nsocial psychology to formalize a general mechanism for reward\nsharing. Moving forward, SVO agents can be leveraged as a model-\ning tool for social psychology research [36].\nIn this vein, group formation is a topic important to both fields. It\nis well established among pyschologists that an individual’s behav-\nior is strongly guided by their ingroup—the group with which they\npsychologically identify [14]. However, the processes by which\nindividuals form group identities are still being studied and inves-\ntigated [10, 54]. What sort of mechanisms transform and redefine\nself-interest to incorporate the interests of a broader group? This\nline of inquiry has potential linkages to the study of team and\ncoalition formation in multi-agent research [48].\nOur findings show that in multi-agent environments, heteroge-\nneous distributions of SVO can generate high levels of population\nperformance. A natural question follows from these results: how\ncan we identify optimal SVO distributions for a given environment?\nEvolutionary approaches to reinforcement learning [27] could be\napplied to study the variation in optimal distributions of SVO across\nindividual environments. We note that our results mirror findings\nfrom evolutionary biology that across-individual genetic diversity\ncan produce group-wide benefits [39]. We suspect that SVO agents\ncan be leveraged in simulo to study open questions concerning the\nemergence and adaptiveness of human altruism [9, 34].\nThe development of human-compatible agents still faces major\nchallenges [2, 3, 25]. In pure-common interest reinforcement learn-\ning, robustness to partner heterogeneity is seen as an important step\ntoward human compatibility [11]. The same holds true for mixed-\nmotive contexts. Within “hybrid systems” containing humans and\nartificial agents [12], agents should be able to predict and respond\nto a range of potential partner behaviors. Social preferences are,\nof course, an important determinant of human behavior [4, 29].\nEndowing agents with SVO is a promising path forward for train-\ning diverse agent populations, expanding the capacity of agents\nto adapt to human behavior, and fostering positive human-agent\ninterdependence.\nREFERENCES\n[1] Christopher Amato, Girish Chowdhary, Alborz Geramifard, N. Kemal Üre, and\nMykel J. Kochenderfer. Decentralized control of partially observable Markov\ndecision processes. In 52nd IEEE Conference on Decision and Control, pages\n2398–2405. IEEE, 2013.\n[2] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza.\nPower to the people: The role of humans in interactive machine learning. AI\nMagazine, 35(4):105–120, 2014.\n[3] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira\nNushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al.\nGuidelines for human-AI interaction. In Proceedings of the 2019 CHI Conference\non Human Factors in Computing Systems, pages 1–13, 2019.\n[4] Daniel Balliet, Craig Parks, and Jeff Joireman. Social Value Orientation and\ncooperation in social dilemmas: A meta-analysis. Group Processes & Intergroup\nRelations, 12(4):533–547, 2009.\n[5] Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mor-\ndatch.\nEmergent complexity via multi-agent competition.\narXiv preprint\narXiv:1710.03748, 2017.\nSocial Diversity and Social Preferences in Mixed-Motive Reinforcement Learning\nAAMAS’20, May 2020, Auckland, New Zealand\n[6] Nolan Bard, Jakob N. Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H. Fran-\ncis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes,\net al. The Hanabi challenge: A new frontier for AI research. arXiv preprint\narXiv:1902.00506, 2019.\n[7] C. Daniel Batson. A history of prosocial research. In A. W. Kruglanski and\nW. Stroebe, editors, Handbook of the History of Social Psychology, pages 243–264.\nPsychology Press, 2012.\n[8] Tony A. Blakely and Ichiro Kawachi. What is the difference between controlling\nfor mean versus median income in analyses of income inequality? Journal of\nEpidemiology & Community Health, 55(5):352–353, 2001.\n[9] Samuel Bowles. Group competition, reproductive leveling, and the evolution of\nhuman altruism. Science, 314(5805):1569–1572, 2006.\n[10] Marilynn B. Brewer.\nIn-group bias in the minimal intergroup situation: A\ncognitive-motivational analysis. Psychological Bulletin, 86(2):307, 1979.\n[11] Micah Carroll, Rohin Shah, Mark K. Ho, Thomas L. Griffiths, Sanjit A. Seshia,\nPieter Abbeel, and Anca Dragan. On the utility of learning about humans for\nhuman-AI coordination. arXiv preprint arXiv:1910.05789, 2019.\n[12] Nicholas Christakis. Blueprint: The Evolutionary Origins of a Good Society. Ha-\nchette, 2019.\n[13] David J. Cooper and John H. Kagel. Other-regarding preferences: A selective\nsurvey of experimental results. In The Handbook of Experimental Economics,\nvolume 2, pages 217–289. Princeton University Press, 2016.\n[14] Carsten K. W. de Dreu. Social value orientation moderates ingroup love but not\noutgroup hate in competitive intergroup conflict. Group Processes & Intergroup\nRelations, 13(6):701–713, 2010.\n[15] Tom Eccles, Edward Hughes, János Kramár, Steven Wheelwright, and Joel Z.\nLeibo. The Imitation Game: Learned reciprocity in markov games. In Proceed-\nings of the 18th International Conference on Autonomous Agents and Multi-Agent\nSystems, pages 1934–1936. International Foundation for Autonomous Agents and\nMulti-Agent Systems, 2019.\n[16] Catherine C. Eckel and Philip J. Grossman. Altruism in anonymous dictator\ngames. Games and Economic Behavior, 16(2):181–191, 1996.\n[17] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih,\nTom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala:\nScalable distributed deep-RL with importance weighted actor-learner architec-\ntures. arXiv preprint arXiv:1802.01561, 2018.\n[18] Roy Gardner, Elinor Ostrom, and James M. Walker. The nature of common-pool\nresource problems. Rationality and Society, 2(3):335–358, 1990.\n[19] Andrew G. Haldane and Arthur E. Turrell. Drawing on different disciplines:\nMacroeconomic agent-based models. Journal of Evolutionary Economics, 29(1):39–\n66, 2019.\n[20] Garrett Hardin. The tragedy of the commons. Science, 162(3859):1243–1248, 1968.\n[21] James E. Hartley. Retrospectives: The origins of the representative agent. Journal\nof Economic Perspectives, 10(2):169–177, 1996.\n[22] Johannes Heinrich and David Silver. Deep reinforcement learning from self-play\nin imperfect-information games. arXiv preprint arXiv:1603.01121, 2016.\n[23] Joseph Henrich. Cooperation, punishment, and the evolution of human institu-\ntions. Science, 312(5770):60–61, 2006.\n[24] Edward Hughes, Joel Z. Leibo, Matthew Phillips, Karl Tuyls, Edgar Dueñez-\nGuzman, Antonio García Castañeda, Iain Dunning, Tina Zhu, Kevin R. McKee,\nRaphael Koster, et al. Inequity aversion improves cooperation in intertemporal\nsocial dilemmas. In Advances in Neural Information Processing Systems, pages\n3326–3336, 2018.\n[25] Fatimah Ishowo-Oloko, Jean-François Bonnefon, Zakariyah Soroye, Jacob Cran-\ndall, Iyad Rahwan, and Talal Rahwan. Behavioural evidence for a transparency–\nefficiency tradeoff in human–machine cooperation. Nature Machine Intelligence,\n1(11):517–521, 2019.\n[26] Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever,\nAntonio Garcia Castañeda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos,\nAvraham Ruderman, et al. Human-level performance in 3d multiplayer games\nwith population-based reinforcement learning. Science, 364(6443):859–865, 2019.\n[27] Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M. Czarnecki,\nJeff Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Si-\nmonyan, et al. Population based training of neural networks. arXiv preprint\narXiv:1711.09846, 2017.\n[28] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro\nOrtega, D. J. Strouse, Joel Z. Leibo, and Nando De Freitas. Social influence as\nintrinsic motivation for multi-agent deep reinforcement learning. In International\nConference on Machine Learning, pages 3040–3049, 2019.\n[29] Harold H. Kelley and John W. Thibaut. Interpersonal Relations: A Theory of\nInterdependence. John Wiley & Sons, 1978.\n[30] Alan P. Kirman. Whom or what does the representative individual represent?\nJournal of Economic Perspectives, 6(2):117–136, 1992.\n[31] Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Grae-\npel. Multi-agent reinforcement learning in sequential social dilemmas. In Proceed-\nings of the 16th Conference on Autonomous Agents and Multi-Agent Systems, pages\n464–473. International Foundation for Autonomous Agents and Multi-Agent\nSystems, 2017.\n[32] Adam Lerer and Alexander Peysakhovich. Maintaining cooperation in com-\nplex social dilemmas using deep reinforcement learning.\narXiv preprint\narXiv:1707.01068, 2017.\n[33] Michael L. Littman. Markov games as a framework for multi-agent reinforcement\nlearning. In Machine Learning Proceedings 1994, pages 157–163. Elsevier, 1994.\n[34] Joshua Mitteldorf and David Sloan Wilson. Population viscosity and the evolution\nof altruism. Journal of Theoretical Biology, 204(4):481–496, 2000.\n[35] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timo-\nthy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous\nmethods for deep reinforcement learning. In International Conference on Machine\nLearning, pages 1928–1937, 2016.\n[36] Margaret Morrison and S. Mary Morgan. Models as mediating instruments. In\nModels as Mediators, chapter 2. Cambridge University Press, 1999.\n[37] Ryan O. Murphy and Kurt A. Ackermann. Social Value Orientation: Theoretical\nand measurement issues in the study of social preferences. Personality and Social\nPsychology Review, 18(1):13–41, 2014.\n[38] Richard E. Nisbett and Ziva Kunda. Perception of social distributions. Journal of\nPersonality and Social Psychology, 48(2):297, 1985.\n[39] P. Nonacs and K. M. Kapheim. Social heterosis and the maintenance of genetic\ndiversity. Journal of Evolutionary Biology, 20(6):2253–2265, 2007.\n[40] Rick O’Gorman, Joseph Henrich, and Mark Van Vugt. Constraining free riding\nin public goods games: Designated solitary punishers can sustain human cooper-\nation. Proceedings of the Royal Society B: Biological Sciences, 276(1655):323–329,\n2008.\n[41] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n[42] Julien Perolat, Joel Z. Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, and\nThore Graepel. A multi-agent reinforcement learning model of common-pool\nresource appropriation. In Advances in Neural Information Processing Systems,\npages 3643–3652, 2017.\n[43] Alexander Peysakhovich and Adam Lerer. Consequentialist conditional co-\noperation in social dilemmas with imperfect information.\narXiv preprint\narXiv:1710.06975, 2017.\n[44] Alexander Peysakhovich and Adam Lerer. Prosocial learning agents solve gener-\nalized stag hunts better than selfish ones. In Proceedings of the 17th International\nConference on Autonomous Agents and Multi-Agent Systems, pages 2043–2044.\nInternational Foundation for Autonomous Agents and Multi-Agent Systems,\n2018.\n[45] J. Philippe Rushton, Roland D. Chrisjohn, and G. Cynthia Fekken. The altru-\nistic personality and the self-report altruism scale. Personality and Individual\nDifferences, 2(4):293–302, 1981.\n[46] Tuomas W. Sandholm and Robert H. Crites. Multiagent reinforcement learning\nin the iterated prisoner’s dilemma. Biosystems, 37(1-2):147–166, 1996.\n[47] Thomas C. Schelling. The Strategy of Conflict. Harvard University Press, 1960.\n[48] Prakash P. Shenoy. On coalition formation: A game-theoretical approach. Inter-\nnational Journal of Game Theory, 8(3):133–164, 1979.\n[49] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew\nLai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel,\net al. A general reinforcement learning algorithm that masters chess, shogi, and\nGo through self-play. Science, 362(6419):1140–1144, 2018.\n[50] Herbert A. Simon. Altruism and economics. The American Economic Review,\n83(2):156–161, 1993.\n[51] Satinder Singh, Andrew G. Barto, and Nuttapong Chentanez. Intrinsically moti-\nvated reinforcement learning. In Proceedings of the 17th International Conference\non Neural Information Processing Systems, pages 1281–1288. MIT Press, 2004.\n[52] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vini-\ncius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl\nTuyls, et al. Value-decomposition networks for cooperative multi-agent learning\nbased on team reward. In Proceedings of the 17th International Conference on\nAutonomous Agents and Multi-Agent Systems, pages 2085–2087. International\nFoundation for Autonomous Agents and Multi-Agent Systems, 2018.\n[53] Albert W. Tucker. A two-person dilemma. Unpublished, 1950.\n[54] John C. Turner. Social categorization and the self-concept: A social cognitive\ntheory of group behavior. In Tom Postmes and Nyla R. Branscombe, editors,\nKey Readings in Social Psychology: Rediscovering Social Identity, pages 243–272.\nPsychology Press, 2010.\n[55] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew\nDudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko\nGeorgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement\nlearning. Nature, pages 1–5, 2019.\n[56] Jane X. Wang, Edward Hughes, Chrisantha Fernando, Wojciech M. Czarnecki,\nEdgar A. Duéñez-Guzmán, and Joel Z. Leibo. Evolving intrinsic motivations\nfor altruistic behavior. In Proceedings of the 18th International Conference on\nAutonomous Agents and Multi-Agent Systems, pages 683–692. International Foun-\ndation for Autonomous Agents and Multi-Agent Systems, 2019.\n",
  "categories": [
    "cs.MA",
    "cs.AI"
  ],
  "published": "2020-02-06",
  "updated": "2020-02-12"
}