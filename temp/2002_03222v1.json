{
  "id": "http://arxiv.org/abs/2002.03222v1",
  "title": "SUOD: Toward Scalable Unsupervised Outlier Detection",
  "authors": [
    "Yue Zhao",
    "Xueying Ding",
    "Jianing Yang",
    "Haoping Bai"
  ],
  "abstract": "Outlier detection is a key field of machine learning for identifying abnormal\ndata objects. Due to the high expense of acquiring ground truth, unsupervised\nmodels are often chosen in practice. To compensate for the unstable nature of\nunsupervised algorithms, practitioners from high-stakes fields like finance,\nhealth, and security, prefer to build a large number of models for further\ncombination and analysis. However, this poses scalability challenges in\nhigh-dimensional large datasets. In this study, we propose a three-module\nacceleration framework called SUOD to expedite the training and prediction with\na large number of unsupervised detection models. SUOD's Random Projection\nmodule can generate lower subspaces for high-dimensional datasets while\nreserving their distance relationship. Balanced Parallel Scheduling module can\nforecast the training and prediction cost of models with high confidence---so\nthe task scheduler could assign nearly equal amount of taskload among workers\nfor efficient parallelization. SUOD also comes with a Pseudo-supervised\nApproximation module, which can approximate fitted unsupervised models by lower\ntime complexity supervised regressors for fast prediction on unseen data. It\nmay be considered as an unsupervised model knowledge distillation process.\nNotably, all three modules are independent with great flexibility to \"mix and\nmatch\"; a combination of modules can be chosen based on use cases. Extensive\nexperiments on more than 30 benchmark datasets have shown the efficacy of SUOD,\nand a comprehensive future development plan is also presented.",
  "text": "SUOD: Toward Scalable Unsupervised Outlier Detection\nYue Zhao,1 Xueying Ding,1 Jianing Yang,2 Haoping Bai2\n1H. John Heinz III College, Carnegie Mellon University, Pittsburgh, PA 15213, USA\n2Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA\nzhaoy@cmu.edu, xding2@andrew.cmu.edu, jianing3@cs.cmu.edu, haopingb@andrew.cmu.edu\nAbstract\nOutlier detection is a key ﬁeld of machine learning for iden-\ntifying abnormal data objects. Due to the high expense of\nacquiring ground truth, unsupervised models are often cho-\nsen in practice. To compensate for the unstable nature of un-\nsupervised algorithms, practitioners from high-stakes ﬁelds\nlike ﬁnance, health, and security, prefer to build a large num-\nber of models for further combination and analysis. However,\nthis poses scalability challenges in high-dimensional large\ndatasets. In this study, we propose a three-module accelera-\ntion framework called SUOD to expedite the training and pre-\ndiction with a large number of unsupervised detection mod-\nels. SUOD’s Random Projection module can generate lower\nsubspaces for high-dimensional datasets while reserving their\ndistance relationship. Balanced Parallel Scheduling module\ncan forecast the training and prediction cost of models with\nhigh conﬁdence—so the task scheduler could assign nearly\nequal amount of taskload among workers for efﬁcient paral-\nlelization. SUOD also comes with a Pseudo-supervised Ap-\nproximation module, which can approximate ﬁtted unsuper-\nvised models by lower time complexity supervised regressors\nfor fast prediction on unseen data. It may be considered as\nan unsupervised model knowledge distillation process. No-\ntably, all three modules are independent with great ﬂexibility\nto “mix and match”; a combination of modules can be chosen\nbased on use cases. Extensive experiments on more than 30\nbenchmark datasets have shown the efﬁcacy of SUOD, and a\ncomprehensive future development plan is also presented.\nIntroduction\nAnomaly detection aims to identify the samples that are de-\nviant from the general data distribution (2019). Most of the\nexisting outlier detection algorithms are unsupervised due\nto the high cost of acquiring ground truth (2019). It is noted\nthat outlier detection can be viewed as a binary classiﬁca-\ntion problem under extreme imbalance (i.e., the number of\noutliers is way smaller than the number of normal samples).\nAs a result, using a single unsupervised model is risky by\nnature; it is sensible to use a large group of unsupervised\nmodels with variations, e.g., different algorithms, varying\nparameters, distinct views of the datasets, etc., and more re-\nliable results may be achieved. Outlier ensemble methods\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nthat select and combine base detectors are designed for this\npurpose (2013; 2014; 2017). The simplest way is to take the\naverage or maximum values across all the detectors as the\nﬁnal result (2017). More complex combination can also be\ndone in both unsupervised (2019) and semi-supervised man-\nners (2018).\nHowever, both training and predicting with a large num-\nber of unsupervised detectors are computationally expen-\nsive. This problem is more severe on high-dimensional\nlarge samples, especially for proximity-based algorithms\nthat assume outliers behave very differently in speciﬁc re-\ngions (2016). Most algorithms in this category, including k\nnearest neighbors (kNN) (2000), local outlier factor (LOF)\n(2000), and local outlier probabilities (LoOP) (2009), op-\nerate in Euclidean space and suffer from the curse of di-\nmensionality. They can be prohibitively slow or even fail\nto work completely. The effort has been made to project\nhigh-dimensional data to lower subspaces (2001), like sim-\nple Principal Component Analysis (PCA) (2003) and more\ncomplex subspace method HiCS (2012). Engineering cures\nhave been explored as well—the process can be expedited by\nparallelization on multiple workers (e.g., CPU cores) (2005;\n2014). Recently, knowledge distillation emerges as a popu-\nlar way of compressing large neural network models (2015),\nwhile its usage in outlier detection is still limited.\nThe aforementioned treatments face various challenges.\nFirst, deterministic projection methods, e.g., PCA, are fast\nbut not ideal for building a large number of diversiﬁed out-\nlier detectors—it results in the same subspace that cannot\ninduce diversity for outlier ensembles. Complex projection\nand subspace methods may bring performance improvement\nfor outlier mining, but the generalization capacity is often\nreduced with strong assumptions. They are not suited for\ngeneral-purpose outlier detector acceleration. Existing par-\nallelization learning frameworks can be inefﬁcient if train-\ning and prediction task assignments are not balanced among\nworkers. In fact, a group of heterogeneous models can have\nsigniﬁcantly different computational cost. As a simple ex-\nample, let us split 100 heterogeneous models into 4 groups\n(cores) for training. If group #2 takes signiﬁcantly longer\ntime than the others to ﬁnish, it will behave like the bot-\ntleneck of the system. More formally, the imbalanced task\narXiv:2002.03222v1  [cs.LG]  8 Feb 2020\nscheduling ampliﬁes the impact of slower worker(s) in a sys-\ntem; the system efﬁciency is curbed by the slowest group.\nAs we have shown in the later section, the existing parallel\ntask scheduling algorithms in popular machine learning li-\nbraries like scikit-learn (2011) may be inefﬁcient under this\nsetting. In addition to these limitations, unsupervised mod-\nels such as LOF can be slow (high time complexity), or\neven inappropriate, to predict on unseen data samples by na-\nture. Another downside of unsupervised and non-parametric\nmodels is their limited interpretability. In summary, training\nand predicting with a large number of heterogeneous unsu-\npervised models is computationally expensive, inefﬁcient in\nparallelization, and limited in interpretability.\nTo tap the gap, we propose a three-module accelera-\ntion framework called SUOD that leverages random projec-\ntion, pseudo-supervised approximation, and balanced par-\nallel scheduling for scalability. For high-dimensional data,\nSUOD generates a random low-dimensional subspace for\neach unsupervised model by Johnson-Lindenstrauss projec-\ntion, on which the model is then trained. To improve the\ntraining and prediction efﬁciency in a distributed system, we\npropose a balanced parallel scheduling heuristic. The key\nidea is to forecast the running time of each model so that\nthe workload could be evenly distributed among workers.\nIf prediction is needed for unseen data, lower cost super-\nvised regressors are initialized to approximate complex un-\nsupervised models—the supervised models are trained on\nthe original feature space using the outlier scores generated\nby unsupervised models as the “pseudo ground truth”. The\nrationale behind is efﬁcient supervised models are faster for\nprediction and usually more interpretable; it can be viewed\nas a way of distilling knowledge from unsupervised models\n(2015). Notably, all these three modules are designed to be\nfully independent for model acceleration from complemen-\ntary perspectives. They have great ﬂexibility to be mixed for\ndifferent needs.\nIn this work, we make the following contributions:\n1. Examine the effect of various deterministic and ran-\ndom projection methods on varying size and dimension\ndatasets, and identify the applicable cases of using them\nfor faster execution and diversity induction.\n2. Identify an imbalanced scheduling issue in existing dis-\ntributed learning systems for heterogeneous detectors and\nﬁx it by a new scheduling schema.\n3. Conduct extensive experiments to analyze the results of\nusing pseudo-supervised regression models for approxi-\nmating unsupervised outlier detectors. To our best knowl-\nedge, this is the ﬁrst research attempt in outlier detection\nsetting.\n4. Demonstrate the effectiveness of the three modules in-\ndependently, and the extensibility of combining them to-\ngether as a scalable training and prediction framework.\n5. To foster reproducibility, all code, ﬁgure, and datasets\nused in this study are openly shared1. A scalable imple-\nmentation will also be included in PyOD (2019) soon.\n1https://github.com/yzhao062/SUOD\nRelated Works\nOutlier Detection and Outlier Ensembles\nAnomaly detection has numerous important applications in\nvarious ﬁelds, such as rare disease detection (2018), fraud-\nulent online review analysis (2013), and network intrusion\ndetection (2003). Despite, detecting outliers is a challeng-\ning classiﬁcation task due to multiple reasons (2019). First,\nanomalies only consist of a small portion of the entire data–\nextreme data imbalance incurs difﬁculty. Second, the lim-\nited amount of data and available labels impede learning\ndata representation accurately. Third, the deﬁnition of out-\nliers can be ambiguous; outliers may be heterogeneous that\nshould be treated as a multi-class problem.\nMost of the existing detection algorithms are unsuper-\nvised as ground truth is absent; acquiring labels can be pro-\nhibitively expensive in practice. As a result, there are a few\nestablished unsupervised anomaly detection algorithms like\nIsolation Forest (2008), Local Outlier Factor (LOF) (2000),\nand Angle-based Outlier Detection (ABOD) (2009), with\ndifferent assumptions of the underlying data. Regarding un-\nsupervised deep models like autoencoders and generative\nadversarial networks (2019), the amount of accessible data\nlimits their effectiveness on learning representations. No al-\ngorithm could always outperform as the assumptions may be\nincorrect, and it is hard to asses without ground truth.\nTherefore, relying on a single unsupervised model has\nan inherently high risk, and outlier ensembles that lever-\nage a group of detectors become increasingly popular (2013;\n2014). There are a group of unsupervised outlier ensem-\nble frameworks proposed in the last several years from\nsimple average, maximization, weighted average, second-\nphase combination methods (2017) to more complex se-\nlective models like SELECT (2016) and LSCP (2019). Al-\nthough unsupervised outlier ensembles methods can be ef-\nfective in certain cases, they could not incorporate the exist-\ning ground truth information regardless of its richness. As a\nresult, a group of semi-supervised detection frameworks that\nleverage existing labels and enhance the data representation\nby unsupervised models are proposed. The representative\nones include BORE (2014) and XGBOD (2018). They use\nunsupervised outlier detection scores as additional features\nto enhance the original feature space, which can be consid-\nered as unsupervised feature engineering or representation\nlearning (extraction). It is noted that for both unsupervised\nand supervised outlier ensembles, a large group diversiﬁed\nunsupervised base detectors are needed—SUOD is therefore\ndesigned to facilitate this process.\nKnowledge Distillation and Model Approximation\nKnowledge distillation refers to the notion of compressing\na or an ensemble of large, often cumbersome model(s) into\na small and more interpretable model. This is often done by\ntraining an ensemble of large models (can be seen as “Teach-\ners”) on a large dataset, followed by using a small and simple\nmodel (“Student”) to learn the output of the ensemble. There\nare two main motivations behind knowledge distillation: (i)\nto reduce the deployment-time computational cost by re-\nplacing the large models with a small model and (ii) to in-\ncrease interpretability as simple models are often more easy\nto be understood by human. This strategy has seen success\nin tasks including computer vision (2014), automatic speech\nrecognition (2015), and neural machine translation (2016).\nIt is noted that the proposed SUOD framework shares a simi-\nlar concept as knowledge distillation for computational cost\noptimization but comes with a few fundamental differences\nas described in Algorithm Design section.\nAlgorithm Design\nThe proposed SUOD contains three independent modules. As\nshown in Algorithm 1, the modules may be enabled if spe-\nciﬁc conditions are met. For high-dimensional data, SUOD\nrandomly project the original input onto lower-dimensional\nspaces (Module I). For expediting the training and predic-\ntion with a large number of models, a balanced parallel\nscheduling mechanism is proposed (Module II). If predic-\ntion on new samples is needed, an efﬁcient supervised re-\ngressor may be initialized to approximate the output of each\ncostly unsupervised detector for prediction (Module III).\nModule I: Random Projection\nA widely used algorithm to alleviate the curse of dimension-\nality on high-dimensional data is the Johnson-Lindenstraus\n(JL) projection (1984), although its use in outlier mining\nis still unexplored. JL projection is a simple compression\nscheme without heavy distortion on the Euclidean distances\nof the data. Its built-in randomness is also useful for induc-\ning diversity for outlier ensembles. Despite, projection may\nbe less useful or even detrimental for methods like Isolation\nForests and HBOS that rely on subspace splitting.\nThis linear transformation is deﬁned as: given a set of data\nX = {x1, x2, ...xn}, each xi ∈Rd, let W be a k × d matrix\nwith each entry drawing independently from a N(0, 1) dis-\ntribution or a Rademacher distribution. Then the JL projec-\ntion is a function f : Rd →Rk such that f(xi) =\n1\n√\nkWxi.\nJL projection randomly projects high-dimensional data to\nlower dimension subspaces, but preserve the distance rela-\ntionship between points. In fact, if we ﬁx some v ∈Rd,\nand let W be the k × d matrix such that each entry is from\nN(0, 1). For every ϵ ∈(0, 3), we have:\nP\n\u0014\n(1 −ϵ)∥v∥2 ≤∥1\n√\nk\nWv∥\n2\n≤(1 + ϵ)∥v∥2\n\u0015\n≤2e−ϵ2 k\n6\n(1)\nFurthermore, ﬁx v to be the differences between vectors.\nThen, the above bound also shows that for a ﬁnite set of N\nvectors X = {x1, x2, ...xn} ⊆Rd, the pairwise Euclidean\ndistance is preserved within a factor of (1 ± ϵ), if we reduce\nthe vectors to k = O( log(N)\nϵ2\n) dimensions.\nFour JL projection methods are therefore introduced for\ntheir great property in compression and diversity induction:\n(i) basic: the transformation matrix is generated by stan-\ndard Gaussian; (ii) discrete: the transformation matrix is\npicked randomly from Rademacher distribution (uniform in\n{−1, 1}); (iii) circulant: the transformation matrix is ob-\ntained by rotating the subsequent rows from the ﬁrst row\nwhich is generated from standard Gaussian and (iv) toeplitz:\nthe ﬁrst row and column of the transformation matrix are\ngenerated from standard Gaussian, and each diagonal uses a\nconstant value from the ﬁrst row and column.\nLet X ∈Rn×d denote a dataset with n points and d fea-\ntures. In this work, we only invoke the projection if the data\ndimension exceeds 20 (dimension threshold θ = 20) and re-\nduce the original dimension by half (projection dimension\nk =\n1\n2d). SUOD check whether d is higher the projection\nthreshold θ. If so, a JL transformation matrix W ∈Rk×d\nis initialized by one of the JL projection methods and X is\nprojected onto the k dimension subspace by W.\nModule II: Balanced Parallel Scheduling\nBalanced Parallel Scheduling (BPS) aims for assigning\ntraining and prediction tasks more evenly based on the\nmodel costs, across all available workers. For instance, one\nmay train 25 detectors with varying parameters from each of\nthe four algorithm groups {kNN, LOF, ABOD, OCSVM},\nresulting in 100 models in total. The existing parallel frame-\nworks, e.g., the voting machine in scikit-learn (2011), will\nsimply split the the models into 4 subgroups by order and\nschedule the ﬁrst 25 models (all kNNs) on Core 1 (worker 1),\nthe next 25 models on Core 2, etc. This does not account for\nthe fact that within a group of heterogeneous detectors, the\ncomputational cost varies. Scheduling the task with equal\nnumber of models can result in highly imbalanced tasks.\nIn the worst case scenario, one worker may be signiﬁcantly\nslower than the rest, and the entire process halts. Obviously,\nthis problem applies to both training and prediction stage.\nThe proposed BPS heuristic focuses on designing a more\nbalanced schedule among workers. Ideally, all workers can\nﬁnish the scheduled tasks within the similar duration of time\nand return the result. As shown in Fig. 1, we build a model\ncost predictor Ccost to forecast the model running time (sum\nof 10 trails) given the input data size, input data dimension,\nand the algorithm embedding (one-hot). Given the time and\nresource limitation, we built a training set with 11 algorithms\non 47 benchmark datasets (see Experiment section for de-\ntails), and a random forest regressor (2001) is trained on the\ndataset with 10-fold cross validation. Although the ﬁtted re-\ngressor does not have a high R2 score, the Spearman’s Rank\ncorrelation (1904) consistently shows high value (rs > 0.9)\nwith low p-value (p < 0.0001). This implies that even the\ncost predictor Ccost could not predict the running time pre-\ncisely, it can predict the rank of the running time with high\naccuracy. As a result, we propose a scheduling heuristic by\nenforcing nearly equal sum of the rank on running time.\nGiven there are m models to be trained, cost predictor Ccost\nis ﬁrst invoked to forecast the time cost for a given model\nDi in D as Ccost(Di). After the prediction is done, the pre-\ndicted time is converted to a rank in [1, m]. If there are t\ncores (workers), each worker will be assigned a group of\nmodels to achieve the objective of minimizing the workload\nimbalance among workers (Eq. 2). So each group has a rank\nsum at around m2+m\n2t\n. One additional advantage of using\nrank is transferability: the running time will vary on differ-\nent machines but the relative rank should preserve.\nFigure 1: Flowchart of Balanced Parallel Scheduling\nmin\nW\nt\nX\ni=1\n\f\f\f\f\f\f\nX\nDj∈Wi\nCcost(Di) −\nm\nX\nl=1\nCcost(Dl)\n\f\f\f\f\f\f\n(2)\nModule III: Pseudo-Supervised Approximation\nOnce the unsupervised models have been ﬁtted on the re-\nduced feature space generated in Module I or the original\nspace (if no random projection is involved). SUOD can ap-\nproximate and replace each of costly unsupervised model\nby a faster supervised regression model for predicting on\nunseen samples. In other words, not all unsupervised models\nshould be replaced but only the expensive ones. There is no\nefﬁciency incentive to approximate fast algorithms like Iso-\nlation Forest. However, most of the proximity-based algo-\nrithms like kNN and LOF have high time complexity for pre-\ndiction (upper bounded by Θ(nd)), which can be effectively\nreplaced by fast supervised models like random forest (up-\nper bounded by Θ(dp) where p denotes the number of base\ntrees). This “pseudo-supervised” model uses the output of\nunsupervised models as “the pseudo ground truth”—the goal\nis to approximate and ﬁnd a better mapping from the original\ninput to “the output of an unsupervised model”. Ensemble-\nbased tree models are recommended for their outstand-\ning scalability, robustness to overﬁtting, and interpretabil-\nity (e.g., feature importance). Notably, this process can also\nbe viewed as using supervised regressors to distill knowl-\nedge from unsupervised models. However, our approxima-\ntion is different from the established knowledge distillation\nmainly in three aspects. First, our approximation works in a\nfully unsupervised manner unlike the classic distillation un-\nder supervised settings. Second, our “teacher” and “student”\nmodels have totally different architectures with little to no\nsimilarity. For instance, we use random forest (an ensemble\ntree model) to approximate LOF (a proximity-based density\nestimation model). Third, our approximation leads to a clear\ninterpretability improvement, whereas the student models in\nneural networks lack that. See Appendix ??.\nAs shown in Algorithm 1, for each trained unsupervised\nmodel Di, a supervised regressor Ri might be built with the\noriginal input X and the pseudo ground truth (the output of\nDi). The prediction on unseen data will be then made by R.\nThis approximation shows multiple beneﬁts:\n1. Compared with non-parametric unsupervised models,\nparametric supervised models may have lower space com-\nplexity and faster prediction speed.\n2. Supervised models generally show better interpretability\ncompared with unsupervised counterparts. For instance,\nrandom forest used in this work can generate feature im-\nportance automatically to facilitate understanding.\n3. Not all unsupervised models are appropriate for making\nprediction. Taking LOF as an example, the ﬁtted model\nis not supposed to predict on unseen data. A supervised\napproximator may be used for prediction in this case.\nAlgorithm 1 Scalable Unsupervised Outlier Detection\nInputs: a group of m unsupervised outlier detectors D; d\ndimension training data Xtrain and test data Xtest (op-\ntional); projection threshold θ; projection dimension k;\npre-trained cost predictor Ccost; # of workers t\nOutputs: ﬁtted unsupervised models D; ﬁtted pseudo-\nsupervised regressors R (optional); prediction results on\ntest data ˆytest (optional)\n1: for Each detector Di in D do\n2:\nif d > θ then\n// enable random projection\n3:\nGenerate random subspace ψi by JL projection\non Xtrain (see Module I)\n4:\nelse d < θ\n// disable random projection\n5:\nUse the original feature space ψi := Xtrain\n6:\nend if\n7: end for\n8: if Parallel learning == True then\n9:\nTrain each Di on the corresponding ψi by Balanced\nParallel Scheduling on t workers (Eq. (2) and Fig. 1)\n10: end if\n11: Return D\n12: if Xtest is presented and Approximation == True then\n13:\nAcquire the pseudo ground truth targetψi as the\noutput of Di on ψi: targetψi := Di(ψi)\n14:\nfor Each detector Di in D do\n15:\nInitialize a supervised regressor Ri\n16:\nFit Ri by {X, targetψi} in pseudo-supervised\napproximation described in Module III\n17:\nˆyi\ntest = Ri.predict(Xtest)\n18:\nend for\n19:\nReturn ˆytest and ﬁtted regressors R\n20: end if\nNumerical Experiments and Discussion\nIn this preliminary study, three independent experiments are\nconducted to understand: (i) how will random projection\nmethods affect the performance of outlier detection algo-\nrithms; (ii) whether the proposed parallel scheduling algo-\nrithm brings performance improvement over the existing ap-\nproaches and (iii) will pseudo-supervised models lead to de-\ngraded prediction performance compared with the original\nunsupervised models? Because all three modules are inde-\npendent and can be combined seamlessly, it is assumed that\nthe (partly) combined framework should also work if indi-\nvidual components manage to work.\nDatasets, Evaluation Metrics, and Implementation\nMore than 30 outlier detection benchmark datasets are used\nin this study12); the detail is available on online supplemen-\ntary due to the space limit. The data size n varies from 219\n(Glass) to 567,479 (HTTP) samples and the dimension d\nranges from 3 to 400. For both random projection and par-\nallel scheduling experiments, full datasets are used for train-\ning. For the pseudo-supervised approximation experiments,\n60% of the data is used for training and the remaining 40%\nis set aside for validation. For all experiments, performance\nis evaluated by taking the average of 10 independent trials\nusing area under the receiver operating characteristic (ROC)\nand precision at rank n (P@N). Both metrics are widely used\nin outlier research (2008; 2014; 2016; 2017; 2018; 2019;\n2019).\nAll the unsupervised models are from Python Outlier De-\ntection Toolbox (PyOD), a popular library for outlier mining\n(2019). Supervised regressors and utility functions are from\nstandard libraries (scikit-learn and numpy). For a fair\ncomparison, none of the models involve parameter tuning\nprocess—the default values are used. As all three modules\ninvolve time proﬁling, the same machine (Intel i7-9700 @\n3.00 GHZ; 32 GB RAM) is used for a fair comparison.\nComparison among Projection Methods\nTo evaluate the effect of projection methods on outlier detec-\ntor performance, we choose three expensive detection algo-\nrithms (ABOD, LOF, and kNN) to measure their execution\ntime, ROC, and P@N with different projection methods. All\nthree methods directly or indirectly measure sample similar-\nity in the Euclidean space , e.g., pairwise sample distances,\nwhich is susceptible to the curse of dimensionality and pro-\njection may be particularly helpful.\nTable 1 shows the comparison among four JL projection\nvariations with original (no projection is used), PCA, and\nRS (randomly select k features from the original d features,\nused in Feature Bagging (2005) and LSCP). First, all projec-\ntion methods show superiority regarding time cost. Second,\nusing RS method comes with high instability, and shows\nperformance decrease on all three datasets. This observa-\ntion agrees with the ﬁnding by Zhao et al. (2019), in which\nthey used an ensemble projection to overcome the instabil-\nity. Third, PCA is slightly faster than JL projection methods,\nalthough the detector performance by PCA projection is not\nas good as the ones with JL projections as shown in sub-\ntable (b)-(i). Moreover, PCA as a deterministic method, may\nnot be ideal for inducing diversity in outlier ensembles, as it\nalways result in the same sets of subspaces. Fourth, among\nall four JL projection methods, circulant and toeplitz out-\nperform in most cases. Since toeplitz is slightly faster than\ncirculant, it is a reasonable choice for reducing dimensional-\n1ODDS Library: http://odds.cs.stonybrook.edu\n2DAMI Datasets: http://www.dbs.iﬁ.lmu.de/research/outlier-\nevaluation/DAMI\nity and inducing diversity for the models that are susceptible\nto the curse of dimensionality.\nThe Effect of Balanced Parallel Scheduling\nTo verify the effectiveness of the proposed BPS algorithm,\nwe run the following experiments by varying: (i) the size\n(n) and the dimension (d) of the datasets, (ii) the number of\nestimators to train (m) and (iii) the number of CPU cores\n(t). The time elapsed is measured in seconds. Due to the\nspace limit, we only show the comparison between the sim-\nple scheduling and BPS on Cardio (n = 1831, d = 21),\nPageBlocks (n = 5393, d = 10), and Pendigits (n =\n6870, d = 16), by setting t ∈{2, 4, 6} and m ∈{100, 500}.\nMore results can be found on the online supplementary,\nand the conclusion holds for all tested datasets. Table 2\nshows that the proposed BPS has a clear edge over the sim-\nple scheduling mechanism (denoted as Simple in the tables)\nthat equally splits the tasks by order. It yields a signiﬁcant\ntime reduction (denoted as % RED in the tables), and gets\nmore signiﬁcant if more estimators and cores are involved.\nFor instance, if 500 estimators and 6 cores are used, the\ntime reduction over the simple scheduling is more than 40%.\nThis agrees with our assumption that the imbalanced task\nscheduling will lead to more inefﬁcient consequences with\nthe increasing number of estimators and workers, and will\ntherefore beneﬁt more from the proposed BPS heuristic.\nThe Analysis of Pseudo-Supervised Approximation\nTo better understand the behavior of the pseudo-supervised\napproximation, we ﬁrst generate 200 synthetic points with\nNormal distribution for outliers (40 points) and Uniform dis-\ntribution for normal samples (160 points). In Fig. 2, we plot\nthe decision surfaces of unsupervised models and their su-\npervised approximators (random forest regressor). It is clear\nthat the decision surfaces are different and some regular-\nization effect appears (lower errors on Feature Bagging and\nkNN). One of the assumptions is that the approximation pro-\ncess improves the generalization ability of the model by “ig-\nnoring” the overﬁtted points. This fails to work with ABOD\nbecause it has a extremely coarse decision surfaces to ap-\nproximate (see Fig. 2).\nTable 3 and 4 compare prediction performance between\nthe original unsupervised models and pseudo-supervised ap-\nproximators on 8 datasets with 6 algorithms. These algo-\nrithms are known to be more computationally expensive than\nthe supervised random forest regressors. The approximators\nwith performance degradation are highlighted in bold and\nitalicized in the tables. The prediction time comparison is\nomitted due to space limit, but the gain is clear (see on-\nline supplementary). Therefore, the focus is put on predic-\ntion ROC and P@N to see whether the approximators could\npredict unseen data as good as the original unsupervised\nmodels. The acceptable threshold of performance degrada-\ntion between an approximator and its original unsupervised\nmodels is set as [0, 0.01] and any negative difference larger\nthan 0.01 will be regarded as degradation. The tables reveal\nthat not all the algorithms can be approximated well by su-\npervised regressors: ABOD has a performance decrease re-\ngarding ROC on multiple datasets. ABOD is a linear mod-\nTable 1: Comparison of various projection methods on different outlier detectors and datasets\n(a) ABOD on MNIST\nMethod\nTime\nROC\nPRN\noriginal\n12.89\n0.80\n0.39\nPCA\n8.93\n0.81\n0.37\nRS\n8.27\n0.74\n0.32\nbasic\n8.94\n0.80\n0.38\ndiscrete\n8.86\n0.80\n0.39\ncirculant\n9.33\n0.80\n0.38\ntoeplitz\n8.96\n0.80\n0.38\n(b) ABOD on Satellite\nMethod\nTime\nROC\nPRN\noriginal\n4.03\n0.59\n0.41\nPCA\n3.01\n0.62\n0.44\nRS\n3.53\n0.63\n0.44\nbasic\n3.10\n0.64\n0.45\ndiscrete\n3.12\n0.65\n0.46\ncirculant\n3.14\n0.66\n0.48\ntoeplitz\n3.14\n0.66\n0.47\n(c) ABOD on Satimage-2\nMethod\nTime\nROC\nPRN\noriginal\n3.68\n0.85\n0.28\nPCA\n2.70\n0.88\n0.30\nRS\n3.20\n0.89\n0.28\nbasic\n2.78\n0.91\n0.29\ndiscrete\n2.79\n0.91\n0.31\ncirculant\n2.85\n0.91\n0.29\ntoeplitz\n2.83\n0.92\n0.30\n(d) LOF on MNIST\nMethod\nTime\nROC\nPRN\noriginal\n7.64\n0.68\n0.29\nPCA\n4.92\n0.67\n0.27\nRS\n3.65\n0.63\n0.23\nbasic\n4.87\n0.70\n0.31\ndiscrete\n5.21\n0.70\n0.32\ncirculant\n5.06\n0.69\n0.31\ntoeplitz\n4.97\n0.71\n0.31\n(e) LOF on Satellite\nMethod\nTime\nROC\nPRN\noriginal\n0.82\n0.55\n0.38\nPCA\n0.23\n0.54\n0.36\nRS\n0.39\n0.54\n0.37\nbasic\n0.31\n0.54\n0.37\ndiscrete\n0.32\n0.54\n0.37\ncirculant\n0.39\n0.55\n0.37\ntoeplitz\n0.37\n0.54\n0.37\n(f) LOF on Satimage-2\nMethod\nTime\nROC\nPRN\noriginal\n0.79\n0.54\n0.07\nPCA\n0.20\n0.52\n0.04\nRS\n0.37\n0.53\n0.08\nbasic\n0.29\n0.52\n0.08\ndiscrete\n0.30\n0.53\n0.07\ncirculant\n0.43\n0.59\n0.11\ntoeplitz\n0.32\n0.54\n0.09\n(g) kNN on MNIST\nMethod\nTime\nROC\nPRN\noriginal\n7.13\n0.84\n0.42\nPCA\n3.92\n0.84\n0.40\nRS\n3.33\n0.77\n0.34\nbasic\n4.17\n0.84\n0.42\ndiscrete\n4.11\n0.84\n0.41\ncirculant\n4.13\n0.84\n0.41\ntoeplitz\n4.11\n0.84\n0.42\n(h) kNN on Satellite\nMethod\nTime\nROC\nPRN\noriginal\n0.71\n0.67\n0.49\nPCA\n0.18\n0.67\n0.50\nRS\n0.31\n0.68\n0.49\nbasic\n0.24\n0.68\n0.49\ndiscrete\n0.25\n0.69\n0.50\ncirculant\n0.33\n0.70\n0.50\ntoeplitz\n0.30\n0.70\n0.51\n(i) kNN on Satimage-2\nMethod\nTime\nROC\nPRN\noriginal\n0.68\n0.94\n0.39\nPCA\n0.15\n0.94\n0.39\nRS\n0.29\n0.94\n0.38\nbasic\n0.23\n0.94\n0.38\ndiscrete\n0.20\n0.95\n0.37\ncirculant\n0.36\n0.96\n0.37\ntoeplitz\n0.25\n0.96\n0.39\nels that look for a lower-dimensional subspace to embed the\nnormal samples (2016), so the approximation may not work\nif it has extremely complex decision surfaces as mentioned\nbefore. In contrast, proximity-based models that aim to iden-\ntify speciﬁc Euclidean regions in which outliers are differ-\nent, beneﬁt from the approximation. Both table shows, kNN,\nLoF, and AkNN (average kNN) experience an performance\ngain. Speciﬁcally, all three algorithms yield around 100%\nROC increase on HTTP. Other algorithms, such as Feature\nBagging and CBLOF, the ROC and PRC performances stay\nwithin the acceptable range. In other words, it is useful to\nperform pseudo-supervised approximation for these estima-\ntors as the time efﬁciency is improved at little to no loss in\naccuracy. Through the visualization and quantitative com-\nparisons, we believe that the proposed pseudo-supervised\napproximation is meaningful for prediction acceleration.\nConclusion and Future Directions\nIn this work, a three-module framework called SUOD is\nproposed to accelerate the training and prediction with a\nlarge number of unsupervised anomaly detectors. The three\nmodules in SUOD focus on different perspectives of scala-\nbility enhancement: (i) Random Projection module gener-\nTable 2: Comparison between simple scheduling and BPS\nn\nd\nm\nt\nSimple\nBPS\n% RED\n1831\n21\n100\n2\n26.33\n19.85\n24.61\n1831\n21\n100\n4\n17.93\n13.69\n23.65\n1831\n21\n100\n6\n19.16\n15.23\n20.51\n1831\n21\n500\n2\n100.51\n72.16\n28.21\n1831\n21\n500\n4\n80.38\n39.46\n50.91\n1831\n21\n500\n6\n55.3\n32.78\n40.72\n5393\n10\n100\n2\n51.11\n35.17\n31.19\n5393\n10\n100\n4\n42.49\n16.23\n61.80\n5393\n10\n100\n6\n38.45\n16.97\n55.86\n5393\n10\n500\n2\n197.84\n137.46\n30.52\n5393\n10\n500\n4\n167.36\n76.14\n54.51\n5393\n10\n500\n6\n127.08\n66.29\n47.84\n6870\n16\n100\n2\n80.89\n67.23\n16.89\n6870\n16\n100\n4\n68.31\n35.29\n48.34\n6870\n16\n100\n6\n49.19\n24.18\n50.84\n6870\n16\n500\n2\n336.54\n286.14\n14.98\n6870\n16\n500\n4\n345.69\n164.02\n52.55\n6870\n16\n500\n6\n211.34\n113.13\n46.47\nFigure 2: Comparison among unsupervised models and their pseudo-supervised counterparts\nates lower-dimensional subspaces to alleviate the curse of\ndimensionality using toeplitz Johnson-Lindenstraus projec-\ntion; (ii) Balanced Parallel Scheduling module ensures that\nnearly equal amount of workloads are assigned to multiple\nworkers in parallel training and prediction and (iii) Pseudo-\nsupervised Approximation module could accelerate costly\nunsupervised models’ prediction speed by replacing them\nby scalable supervised regressors, which also brings the ex-\ntra beneﬁt regarding interpretability and storage cost. The\nextensive experiments on more than 30 benchmark datasets\nempirically show the great potential of SUOD, and many in-\ntriguing results are observed. To improve the model repro-\nducibility and accessibility, all code, ﬁgures, implementation\nfor demo and production, will be released1.\nMany investigations are underway. First, we would like to\ndemonstrate SUOD’s effectiveness as an end-to-end frame-\nwork by combining three modules, and provide an easy to\nuse toolkit in Python. Three additional experiments may\nbe conducted to show that SUOD is useful for: (i) simple\ncombination like majority vote and maximization (2020);\n(ii) more complex unsupervised model combination like\nLSCP (2019) and (iii) supervised outlier combination algo-\nrithms such as XGBOD (2018). Second, although we pro-\nvide a pre-built model cost predictor as part of the frame-\nwork, a re-trainable cost predictor is expected so practition-\ners can make accurate prediction on their machines. Third,\nwe would further emphasize the interpretability provided\nby the pseudo-supervised approximation, which can be be-\nyond simple feature importance provided in tree regressors.\n1https://github.com/yzhao062/SUOD\nFourth, we see there is room to investigate why and how\nthe pseudo-supervised approximation could work in a more\nstrict and theoretical way. Speciﬁcally, we want to know how\nto choose supervised regressors and under what conditions\nthe approximation could work. This study, as the ﬁrst step,\nempirically shows that proximity-based models beneﬁt from\nthe approximation, whereas linear models may not. Lastly,\nwe want to verify that SUOD can work with real-world use\ncases. We are in contact with a pharmaceutical consultancy\nﬁrm to access their rare disease detection datasets (which\ncan be viewed as an outlier detection task). Microsoft Mal-\nware Dataset2 is also chosen to explore SUOD’s applicability.\nReferences\n[2001] Achlioptas, D. 2001. Database-friendly random pro-\njections. In PODS, 274–281. ACM.\n[2017] Aggarwal, C. C., and Sathe, S. 2017. Outlier ensem-\nbles: An introduction. Springer, 1st edition.\n[2013] Aggarwal, C. C. 2013. Outlier ensembles: position\npaper. ACM SIGKDD Explorations Newsletter 14(2):49–58.\n[2016] Aggarwal, C. C. 2016. Outlier Analysis. Springer.\n[2013] Akoglu, L.; Chandy, R.; and Faloutsos, C.\n2013.\nOpinion fraud detection in online reviews by network ef-\nfects. In ICWSM.\n[2001] Breiman, L. 2001. Random forests. Machine learn-\ning.\n2https://www.kaggle.com/c/microsoft-malware-prediction\nTable 3: Test ROC scores of unsupervised models and their pseudo-supervised approximators (avg of 10 independent trials)\nDataset\nAnnthyroid\nBreastw\nCardio\nHTTP\nMNIST\nPendigits\nPima\nSatellite\nModel\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nABOD\n0.83\n0.71\n0.92\n0.93\n0.63\n0.53\n0.15\n0.13\n0.81\n0.79\n0.67\n0.82\n0.66\n0.70\n0.59\n0.68\nCBLOF\n0.67\n0.68\n0.96\n0.98\n0.73\n0.76\n1.00\n1.00\n0.85\n0.89\n0.93\n0.93\n0.63\n0.68\n0.72\n0.77\nFB\n0.81\n0.45\n0.34\n0.10\n0.61\n0.70\n0.34\n0.97\n0.72\n0.83\n0.39\n0.51\n0.59\n0.63\n0.53\n0.64\nKNN\n0.80\n0.79\n0.97\n0.97\n0.73\n0.75\n0.19\n0.85\n0.85\n0.86\n0.74\n0.87\n0.69\n0.71\n0.68\n0.75\nAKNN\n0.81\n0.82\n0.97\n0.97\n0.67\n0.72\n0.19\n0.88\n0.84\n0.85\n0.72\n0.87\n0.69\n0.71\n0.66\n0.74\nLOF\n0.74\n0.85\n0.44\n0.45\n0.60\n0.68\n0.35\n0.75\n0.72\n0.76\n0.38\n0.47\n0.59\n0.65\n0.53\n0.66\nTable 4: Test P@N scores of unsupervised models and their pseudo-supervised approximators (avg of 10 independent trials)\nDataset\nAnnthyroid\nBreastw\nCardio\nHTTP\nMNIST\nPendigits\nPima\nSatellite\nModel\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nOrig\nAppr\nABOD\n0.31\n0.08\n0.80\n0.83\n0.27\n0.20\n0.00\n0.00\n0.40\n0.27\n0.05\n0.05\n0.48\n0.52\n0.41\n0.46\nCBLOF\n0.25\n0.24\n0.86\n0.90\n0.31\n0.34\n0.02\n0.01\n0.42\n0.48\n0.35\n0.36\n0.43\n0.48\n0.54\n0.57\nFB\n0.24\n0.02\n0.03\n0.07\n0.23\n0.26\n0.02\n0.04\n0.34\n0.36\n0.03\n0.07\n0.37\n0.44\n0.37\n0.42\nKNN\n0.30\n0.32\n0.89\n0.89\n0.37\n0.46\n0.03\n0.03\n0.42\n0.45\n0.08\n0.06\n0.47\n0.47\n0.49\n0.53\nAKNN\n0.30\n0.33\n0.88\n0.89\n0.34\n0.40\n0.03\n0.03\n0.41\n0.45\n0.05\n0.13\n0.48\n0.49\n0.47\n0.52\nLOF\n0.27\n0.36\n0.19\n0.35\n0.23\n0.23\n0.01\n0.03\n0.33\n0.32\n0.03\n0.08\n0.40\n0.44\n0.37\n0.42\n[2000] Breunig, M. M.; Kriegel, H.-P.; Ng, R. T.; and Sander,\nJ. ¨o. r. 2000. LOF: Identifying Density-Based Local Out-\nliers. SIGMOD 1–12.\n[2015] Hinton, G.; Vinyals, O.; and Dean, J.\n2015.\nDis-\ntilling the knowledge in a neural network. arXiv preprint\narXiv:1503.02531.\n[1984] Johnson, W. B., and Lindenstrauss, J. 1984. Exten-\nsions of lipschitz mappings into a hilbert space. Contempo-\nrary mathematics 26(189-206):1.\n[2012] Keller, F.; Muller, E.; and Bohm, K. 2012. Hics: High\ncontrast subspaces for density-based outlier ranking.\n[2016] Kim, Y., and Rush, A. M.\n2016.\nSequence-level\nknowledge distillation. EMNLP.\n[2009] Kriegel, H.-P.; Kr ¨o ger, P.; Schubert, E.; and Zimek,\nA. 2009. LoOP: local outlier probabilities. CIKM.\n[2005] Lazarevic, A., and Kumar, V. 2005. Feature bagging\nfor outlier detection. In KDD, 157–166. ACM.\n[2003] Lazarevic, A.; Ertoz, L.; Kumar, V.; Ozgur, A.; and\nSrivastava, J. 2003. A comparative study of anomaly detec-\ntion schemes in network intrusion detection. In SDM.\n[2018] Li, W.; Wang, Y.; Cai, Y.; Arnold, C.; Zhao, E.; and\nYuan, Y. 2018. Semi-supervised rare disease detection using\ngenerative adversarial network. In NeurIPS Workshop.\n[2019] Liu, Y.; Li, Z.; Zhou, C.; Jiang, Y.; Sun, J.; Wang, M.;\nand He, X. 2019. Generative adversarial active learning for\nunsupervised outlier detection. TKDE.\n[2008] Liu, F. T.; Ting, K. M.; and Zhou, Z.-H. 2008. Isola-\ntion forest. In ICDM, 413–422. IEEE.\n[2005] Lozano, E., and Acuﬁa, E. 2005. Parallel algorithms\nfor distance-based and density-based outliers. In ICDM.\n[2014] Micenkov´a, B.; McWilliams, B.; and Assent, I. 2014.\nLearning outlier ensembles: The best of both worlds–\nsupervised and unsupervised. In SIGKDD Workshop.\n[2014] Oku, J.; Tamura, K.; and Kitakami, H. 2014. Parallel\nprocessing for distance-based outlier detection on a multi-\ncore cpu. In IWCIA.\n[2011] Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel,\nV.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer, P.;\nWeiss, R.; Dubourg, V.; et al. 2011. Scikit-learn: Machine\nlearning in python. JMLR 12(Oct):2825–2830.\n[2000] Ramaswamy, S.; Rastogi, R.; Shim, K.; Hill, M.;\nShim, K.; and Ramaswamy, Sridhar, Rajeev rastogi, K. S.\n2000. Efﬁcient algorithms for mining outliers from large\ndata sets. ACM SIGMOD Record 29(2):427–438.\n[2016] Rayana, S., and Akoglu, L.\n2016.\nLess is more:\nBuilding selective anomaly ensembles. TKDD 10(4):42.\n[2014] Romero, A.; Ballas, N.; Kahou, S. E.; Chassang, A.;\nGatta, C.; and Bengio, Y. 2014. Fitnets: Hints for thin deep\nnets. arXiv preprint arXiv:1412.6550.\n[2003] Shyu, M.-L.; Chen, S.-C.; Sarinnapakorn, K.; and\nChang, L. 2003. A novel anomaly detection scheme based\non principal component classiﬁer. Technical report.\n[1904] Spearman, C. 1904. The proof and measurement of\nassociation between two things. AJP.\n[2018] Zhao, Y., and Hryniewicki, M. K. 2018. Xgbod: Im-\nproving supervised outlier detection with unsupervised rep-\nresentation learning. In IJCNN. IEEE.\n[2019] Zhao, Y.; Nasrullah, Z.; Hryniewicki, M. K.; and Li,\nZ. 2019. LSCP: locally selective combination in parallel\noutlier ensembles. In SDM, 585–593.\n[2020] Zhao, Y.; Wang, X.; Cheng, C.; and Ding, X. 2020.\nCombining machine learning models and scores using\ncombo library. In AAAI.\n[2019] Zhao, Y.; Nasrullah, Z.; and Li, Z. 2019. PyOD: A\npython toolbox for scalable outlier detection. JMLR.\n[2014] Zimek, A.; Campello, R. J.; and Sander, J. 2014. En-\nsembles for unsupervised outlier detection: challenges and\nresearch questions a position paper. ACM SIGKDD Explo-\nrations Newsletter 15(1):11–22.\n",
  "categories": [
    "cs.LG",
    "cs.IR",
    "stat.ML"
  ],
  "published": "2020-02-08",
  "updated": "2020-02-08"
}