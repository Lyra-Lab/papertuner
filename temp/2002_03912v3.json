{
  "id": "http://arxiv.org/abs/2002.03912v3",
  "title": "A Probabilistic Formulation of Unsupervised Text Style Transfer",
  "authors": [
    "Junxian He",
    "Xinyi Wang",
    "Graham Neubig",
    "Taylor Berg-Kirkpatrick"
  ],
  "abstract": "We present a deep generative model for unsupervised text style transfer that\nunifies previously proposed non-generative techniques. Our probabilistic\napproach models non-parallel data from two domains as a partially observed\nparallel corpus. By hypothesizing a parallel latent sequence that generates\neach observed sequence, our model learns to transform sequences from one domain\nto another in a completely unsupervised fashion. In contrast with traditional\ngenerative sequence models (e.g. the HMM), our model makes few assumptions\nabout the data it generates: it uses a recurrent language model as a prior and\nan encoder-decoder as a transduction distribution. While computation of\nmarginal data likelihood is intractable in this model class, we show that\namortized variational inference admits a practical surrogate. Further, by\ndrawing connections between our variational objective and other recent\nunsupervised style transfer and machine translation techniques, we show how our\nprobabilistic view can unify some known non-generative objectives such as\nbacktranslation and adversarial loss. Finally, we demonstrate the effectiveness\nof our method on a wide range of unsupervised style transfer tasks, including\nsentiment transfer, formality transfer, word decipherment, author imitation,\nand related language translation. Across all style transfer tasks, our approach\nyields substantial gains over state-of-the-art non-generative baselines,\nincluding the state-of-the-art unsupervised machine translation techniques that\nour approach generalizes. Further, we conduct experiments on a standard\nunsupervised machine translation task and find that our unified approach\nmatches the current state-of-the-art.",
  "text": "Published as a conference paper at ICLR 2020\nA PROBABILISTIC FORMULATION OF\nUNSUPERVISED TEXT STYLE TRANSFER\nJunxian He∗, Xinyi Wang∗, Graham Neubig\nCarnegie Mellon University\n{junxianh,xinyiw1,gneubig}@cs.cmu.edu\nTaylor Berg-Kirkpatrick\nUniversity of California San Diego\ntberg@eng.ucsd.edu\nABSTRACT\nWe present a deep generative model for unsupervised text style transfer that uniﬁes\npreviously proposed non-generative techniques. Our probabilistic approach models\nnon-parallel data from two domains as a partially observed parallel corpus. By\nhypothesizing a parallel latent sequence that generates each observed sequence, our\nmodel learns to transform sequences from one domain to another in a completely\nunsupervised fashion. In contrast with traditional generative sequence models (e.g.\nthe HMM), our model makes few assumptions about the data it generates: it uses\na recurrent language model as a prior and an encoder-decoder as a transduction\ndistribution. While computation of marginal data likelihood is intractable in this\nmodel class, we show that amortized variational inference admits a practical surro-\ngate. Further, by drawing connections between our variational objective and other\nrecent unsupervised style transfer and machine translation techniques, we show\nhow our probabilistic view can unify some known non-generative objectives such\nas backtranslation and adversarial loss. Finally, we demonstrate the effectiveness\nof our method on a wide range of unsupervised style transfer tasks, including senti-\nment transfer, formality transfer, word decipherment, author imitation, and related\nlanguage translation. Across all style transfer tasks, our approach yields substantial\ngains over state-of-the-art non-generative baselines, including the state-of-the-art\nunsupervised machine translation techniques that our approach generalizes. Further,\nwe conduct experiments on a standard unsupervised machine translation task and\nﬁnd that our uniﬁed approach matches the current state-of-the-art.1\n1\nINTRODUCTION\nText sequence transduction systems convert a given text sequence from one domain to another.\nThese techniques can be applied to a wide range of natural language processing applications such\nas machine translation (Bahdanau et al., 2015), summarization (Rush et al., 2015), and dialogue\nresponse generation (Zhao et al., 2017). In many cases, however, parallel corpora for the task at hand\nare scarce. Therefore, unsupervised sequence transduction methods that require only non-parallel\ndata are appealing and have been receiving growing attention (Bannard & Callison-Burch, 2005;\nRavi & Knight, 2011; Mizukami et al., 2015; Shen et al., 2017; Lample et al., 2018; 2019). This\ntrend is most pronounced in the space of text style transfer tasks where parallel data is particularly\nchallenging to obtain (Hu et al., 2017; Shen et al., 2017; Yang et al., 2018). Style transfer has\nhistorically referred to sequence transduction problems that modify superﬁcial properties of text –\ni.e. style rather than content.2 We focus on a standard suite of style transfer tasks, including formality\ntransfer (Rao & Tetreault, 2018), author imitation (Xu et al., 2012), word decipherment (Shen et al.,\n2017), sentiment transfer (Shen et al., 2017), and related language translation (Pourdamghani &\nKnight, 2017). General unsupervised translation has not typically been considered style transfer, but\nfor the purpose of comparison we also conduct evaluation on this task (Lample et al., 2017).\n∗Equal Contribution.\n1Code and data are available at https://github.com/cindyxinyiwang/deep-latent-sequence-model.\n2Notably, some tasks we evaluate on do change content to some degree, such as sentiment transfer, but for\nconciseness we use the term “style transfer” nonetheless.\n1\narXiv:2002.03912v3  [cs.CL]  29 Apr 2020\nPublished as a conference paper at ICLR 2020\nRecent work on unsupervised text style transfer mostly employs non-generative or non-probabilistic\nmodeling approaches. For example, Shen et al. (2017) and Yang et al. (2018) design adversarial\ndiscriminators to shape their unsupervised objective – an approach that can be effective, but often\nintroduces training instability. Other work focuses on directly designing unsupervised training\nobjectives by incorporating intuitive loss terms (e.g. backtranslation loss), and demonstrates state-of-\nthe-art performance on unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2019)\nand style transfer (Lample et al., 2019). However, the space of possible unsupervised objectives is\nextremely large and the underlying modeling assumptions deﬁned by each objective can only be\nreasoned about indirectly. As a result, the process of designing such systems is often heuristic.\nIn contrast, probabilistic models (e.g. the noisy channel model (Shannon, 1948)) deﬁne assumptions\nabout data more explicitly and allow us to reason about these assumptions during system design.\nFurther, the corresponding objectives are determined naturally by principles of probabilistic inference,\nreducing the need for empirical search directly in the space of possible objectives. That said, classical\nprobabilistic models for unsupervised sequence transduction (e.g. the HMM or semi-HMM) typically\nenforce overly strong independence assumptions about data to make exact inference tractable (Knight\net al., 2006; Ravi & Knight, 2011; Pourdamghani & Knight, 2017). This has restricted their develop-\nment and caused their performance to lag behind unsupervised neural objectives on complex tasks.\nLuckily, in recent years, powerful variational approximation techniques have made it more practical\nto train probabilistic models without strong independence assumptions (Miao & Blunsom, 2016; Yin\net al., 2018). Inspired by this, we take a new approach to unsupervised style transfer.\nWe directly deﬁne a generative probabilistic model that treats a non-parallel corpus in two domains\nas a partially observed parallel corpus. Our model makes few independence assumptions and its true\nposterior is intractable. However, we show that by using amortized variational inference (Kingma &\nWelling, 2013), a principled probabilistic technique, a natural unsupervised objective falls out of our\nmodeling approach that has many connections with past work, yet is different from all past work in\nspeciﬁc ways. In experiments across a suite of unsupervised text style transfer tasks, we ﬁnd that the\nnatural objective of our model actually outperforms all manually deﬁned unsupervised objectives\nfrom past work, supporting the notion that probabilistic principles can be a useful guide even in deep\nneural systems. Further, in the case of unsupervised machine translation, our model matches the\ncurrent state-of-the-art non-generative approach.\n2\nUNSUPERVISED TEXT STYLE TRANSFER\nWe ﬁrst overview text style transfer, which aims to transfer a text (typically a single sentence or a short\nparagraph – for simplicity we refer to simply “sentences” below) from one domain to another while\npreserving underlying content. For example, formality transfer (Rao & Tetreault, 2018) is the task of\ntransforming the tone of text from informal to formal without changing its content. Other examples\ninclude sentiment transfer (Shen et al., 2017), word decipherment (Knight et al., 2006), and author\nimitation (Xu et al., 2012). If parallel examples were available from each domain (i.e. the training data\nis a bitext consisting of pairs of sentences from each domain), supervised techniques could be used to\nperform style transfer (e.g. attentional Seq2Seq (Bahdanau et al., 2015) and Transformer (Vaswani\net al., 2017)). However, for most style transfer problems, only non-parallel corpora (one corpus from\neach domain) can be easily collected. Thus, work on style transfer typically focuses on the more\ndifﬁcult unsupervised setting where systems must learn from non-parallel data alone.\nThe model we propose treats an observed non-parallel text corpus as a partially observed parallel\ncorpus. Thus, we introduce notation for both observed text inputs and those that we will treat\nas latent variables. Speciﬁcally, we let X = {x(1), x(2), · · · , x(m)} represent observed data from\ndomain D1, while we let Y = {y(m+1), y(m+2), · · · , y(n)} represent observed data from domain\nD2. Corresponding indices represent parallel sentences. Thus, none of the observed sentences\nshare indices. In our model, we introduce latent sentences to complete the parallel corpus. Speciﬁ-\ncally, ¯X = {¯x(m+1), ¯x(m+2), · · · , ¯x(n)} represents the set of latent parallel sentences in D1, while\n¯Y = {¯y(1), ¯y(2), · · · , ¯y(m)} represents the set of latent parallel sentences in D2. Then the goal of\nunsupervised text transduction is to infer these latent variables conditioned the observed non-parallel\ncorpora; that is, to learn p(¯y|x) and p(¯x|y).\n2\nPublished as a conference paper at ICLR 2020\nFigure 1: Proposed graphical model for style transfer via bitext completion. Shaded circles denote\nthe observed variables and unshaded circles denote the latents. The generator is parameterized as an\nencoder-decoder architecture and the prior on the latent variable is a pretrained language model.\n3\nTHE DEEP LATENT SEQUENCE MODEL\nFirst we present our generative model of bitext, which we refer to as a deep latent sequence model.\nWe then describe unsupervised learning and inference techniques for this model class.\n3.1\nMODEL STRUCTURE\nDirectly modeling p(¯y|x) and p(¯x|y) in the unsupervised setting is difﬁcult because we never directly\nobserve parallel data. Instead, we propose a generative model of the complete data that deﬁnes a\njoint likelihood, p(X, ¯X, Y, ¯Y ). In order to perform text transduction, the unobserved halves can be\ntreated as latent variables: they will be marginalized out during learning and inferred via posterior\ninference at test time.\nOur model assumes that each observed sentence is generated from an unobserved parallel sentence\nin the opposite domain, as depicted in Figure 1. Speciﬁcally, each sentence x(i) in domain D1 is\ngenerated as follows: First, a latent sentence ¯y(i) in domain D2 is sampled from a prior, pD2(¯y(i)).\nThen, x(i) is sampled conditioned on ¯y(i) from a transduction model, p(x(i)|¯y(i)). Similarly, each\nobserved sentence y(j) in domain D2 is generated conditioned on a latent sentence, ¯x(j), in domain\nD1 via the opposite transduction model, p(y(j)|¯x(j)), and prior, pD1(¯x(j)). We let θx|¯y and θy|¯x\nrepresent the parameters of the two transduction distributions respectively. We assume the prior\ndistributions are pretrained on the observed data in their respective domains and therefore omit their\nparameters for simplicity of notation. Together, this gives the following joint likelihood:\np(X, ¯\nX, Y, ¯Y ; θx|¯y, θy|¯x) =\n m\nY\ni=1\np\n\u0000x(i)|¯y(i); θx|¯y\n\u0001\npD2\n\u0000¯y(i)\u0001\n!  \nn\nY\nj=m+1\np\n\u0000y(j)|¯x(j); θy|¯x\n\u0001\npD1\n\u0000¯x(j)\u0001\n!\n(1)\nThe log marginal likelihood of the data, which we will approximate during training, is:\nlog p(X, Y ; θx|¯y, θy|¯x) = log\nX\n¯\nX\nX\n¯Y p(X, ¯X, Y, ¯Y ; θx|¯y, θy|¯x)\n(2)\nNote that if the two transduction models share no parameters, the training problems for each observed\ndomain are independent. Critically, we introduce parameter sharing through our variational inference\nprocedure, which we describe in more detail in Section 3.2.\nArchitecture:\nSince we would like to be able to model a variety of transfer tasks, we choose a\nparameterization for our transduction distributions that makes no independence assumptions. Speciﬁ-\ncally, we employ an encoder-decoder architecture based on the standard attentional Seq2Seq model\nwhich has been shown to be successful across various tasks (Bahdanau et al., 2015; Rush et al., 2015).\nSimilarly, our prior distributions for each domain are parameterized as recurrent language models\nwhich, again, make no independence assumptions. In contrast, traditional unsupervised generative\nsequence models typically make strong independence assumptions to enable exact inference (e.g. the\nHMM makes a Markov assumption on the latent sequence and emissions are one-to-one). Our model\nis more ﬂexible, but exact inference via dynamic programming will be intractable. We address this\nproblem in the next section.\n3\nPublished as a conference paper at ICLR 2020\nFigure 2: Depiction of amortized variational approximation. Distributions q(¯y|x) and q(¯x|y) represent\ninference networks that approximate the model’s true posterior. Critically, parameters are shared\nbetween the generative model and inference networks to tie the learning problems for both domains.\n3.2\nLEARNING\nIdeally, learning should directly optimize the log data likelihood, which is the marginal of our model\nshown in Eq. 2. However, due to our model’s neural parameterization which does not factorize,\ncomputing the data likelihood cannot be accomplished using dynamic programming as can be done\nwith simpler models like the HMM. To overcome the intractability of computing the true data\nlikelihood, we adopt amortized variational inference (Kingma & Welling, 2013) in order to derive a\nsurrogate objective for learning, the evidence lower bound (ELBO) on log marginal likelihood3 :\nlog p(X, Y ; θx|¯y, θy|¯x)\n≥LELBO(X, Y ; θx|¯y, θy|¯x, φ¯x|y, φ¯y|x)\n=\nX\ni\nh\nEq(¯y|x(i);φ¯y|x)[log p(x(i)|¯y; θx|¯y)] −DKL\n\u0000q(¯y|x(i); φ¯y|x)||pD2(¯y)\n\u0001i\n+\nX\nj\nh\nEq(¯x|y(j);φ¯x|y)[log p(y(j)|¯x; θy|¯x)]\n|\n{z\n}\nReconstruction likelihood\n−DKL\n\u0000q(¯x|y(j); φ¯x|y)||pD1(¯x)\n\u0001i\n|\n{z\n}\nKL regularizer\n(3)\nThe surrogate objective introduces q(¯y|x(i); φ¯y|x) and q(¯x|y(j); φ¯x|y), which represent two separate\ninference network distributions that approximate the model’s true posteriors, p(¯y|x(i); θx|¯y) and\np(¯x|y(j); θy|¯x), respectively. Learning operates by jointly optimizing the lower bound over both\nvariational and model parameters. Once trained, the variational posterior distributions can be used\ndirectly for style transfer. The KL terms in Eq. 3, that appear naturally in the ELBO objective, can be\nintuitively viewed as regularizers that use the language model priors to bias the induced sentences\ntowards the desired domains. Amortized variational techniques have been most commonly applied to\ncontinuous latent variables, as in the case of the variational autoencoder (VAE) (Kingma & Welling,\n2013). Here, we use this approach for inference over discrete sequences, which has been shown to be\neffective in related work on a semi-supervised task (Miao & Blunsom, 2016).\nInference Network and Parameter Sharing:\nNote that the approximate posterior on one domain\naims to learn the reverse style transfer distribution, which is exactly the goal of the generative\ndistribution in the opposite domain. For example, the inference network q(¯y|x(i); φ¯y|x) and the\ngenerative distribution p(y|¯x(i); θy|¯x) both aim to transform D1 to D2. Therefore, we use the same\narchitecture for each inference network as used in the transduction models, and tie their parameters:\nφ¯x|y = θx|¯y, φ¯y|x = θy|¯x. This means we learn only two encoder-decoders overall – which are\nparameterized by θx|¯y and θy|¯x respectively – to represent two directions of transfer. In addition to\nreducing the number of learnable parameters, this parameter tying couples the learning problems for\nboth domains and allows us to jointly learn from the full data. Moreover, inspired by recent work that\n3Note that in practice, we add a weight λ (the same to both domains) to the KL term in ELBO since the\nregularization strength from the pretrained language model varies depending on the datasets, training data size,\nor language model structures. Such reweighting has proven necessary in previous work that is trained with\nELBO (Bowman et al., 2016; Miao & Blunsom, 2016; Yin et al., 2018).\n4\nPublished as a conference paper at ICLR 2020\nbuilds a universal Seq2Seq model to translate between different language pairs (Johnson et al., 2017),\nwe introduce further parameter tying between the two directions of transduction: the same encoder is\nemployed for both x and y, and a domain embedding c is provided to the same decoder to specify\nthe transfer direction, as shown in Figure 2. Ablation analysis in Section 5.3 suggests that parameter\nsharing is important to achieve good performance.\nApproximating Gradients of ELBO:\nThe reconstruction and KL terms in Eq. 3 still involve\nintractable expectations due to the marginalization over the latent sequence, thus we need to approxi-\nmate their gradients. Gumbel-softmax (Jang et al., 2017) and REINFORCE (Williams, 1987) are\noften used as stochastic gradient estimators in the discrete case. Since the latent text variables have an\nextremely large domain, we ﬁnd that REINFORCE-based gradient estimates result in high variance.\nThus, we use the Gumbel-softmax straight-through estimator to backpropagate gradients from the\nKL terms.4 However, we ﬁnd that approximating gradients of the reconstruction loss is much more\nchallenging – both the Gumbel-softmax estimator and REINFORCE are unable to outperform a\nsimple stop-gradient method that does not back-propagate the gradient of the latent sequence to the\ninference network. This conﬁrms a similar observation in previous work on unsupervised machine\ntranslation (Lample et al., 2018). Therefore, we use greedy decoding without recording gradients to\napproximate the reconstruction term.5 Note that the inference networks still receive gradients from\nthe prior through the KL term, and their parameters are shared with the decoders which do receive\ngradients from reconstruction. We consider this to be the best empirical compromise at the moment.\nInitialization.\nGood initialization is often necessary for successful optimization of unsupervised\nlearning objectives. In preliminary experiments, we ﬁnd that the encoder-decoder structure has\ndifﬁculty generating realistic sentences during the initial stages of training, which usually results in a\ndisastrous local optimum. This is mainly because the encoder-decoder is initialized randomly and\nthere is no direct training signal to specify the desired latent sequence in the unsupervised setting.\nTherefore, we apply a self-reconstruction loss Lrec at the initial epochs of training. We denote the\noutput the encoder as e(·) and the decoder distribution as pdec, then\nLrec = −α ·\nX\ni[pdec(e(x(i)), cx)] −α ·\nX\nj[pdec(e(y(j)), cy)],\n(4)\ncx and cy are the domain embeddings for x and y respectively, α decays from 1.0 to 0.0 linearly in\nthe ﬁrst k epochs. k is a tunable parameter and usually less than 3 in all our experiments.\n4\nCONNECTION TO RELATED WORK\nOur probabilistic formulation can be connected with recent advances in unsupervised text transduction\nmethods. For example, back translation loss (Sennrich et al., 2016) plays an important role in recent\nunsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018; Artetxe et al., 2019) and\nunsupervised style transfer systems (Lample et al., 2019). In order to incorporate back translation\nloss the source language x is translated to the target language y to form a pseudo-parallel corpus,\nthen a translation model from y to x can be learned on this pseudo bitext just as in supervised setting.\nWhile back translation was often explained as a data augmentation technique, in our probabilistic\nformulation it appears naturally with the ELBO objective as the reconstruction loss term.\nSome previous work has incorporated a pretrained language models into neural semi-supervised or\nunsupervised objectives. He et al. (2016) uses the log likelihood of a pretrained language model as\nthe reward to update a supervised machine translation system with policy gradient. Artetxe et al.\n(2019) utilize a similar idea for unsupervised machine translation. Yang et al. (2018) employed a\nsimilar approach, but interpret the LM as an adversary, training the generator to fool the LM. We\nshow how our ELBO objective is connected with these more heuristic LM regularizers by expanding\nthe KL loss term (assume x is observed):\nDKL(q(¯y|x)||pD2(¯y)) = −Hq −Eq[log pD2(¯y)],\n(5)\nNote that the loss used in previous work does not include the negative entropy term, −Hq. Our\nobjective results in this additional “regularizer”, the negative entropy of the transduction distribution,\n−Hq. Intuitively, −Hq helps avoid a peaked transduction distribution, preventing the transduction\n4We use one sample to approximate the expectations.\n5We compare greedy and sampling decoding in Section 5.3.\n5\nPublished as a conference paper at ICLR 2020\nfrom constantly generating similar sentences to satisfy the language model. In experiments we will\nshow that this additional regularization is important and helps bypass bad local optima and improve\nperformance. These important differences with past work suggest that a probabilistic view of the\nunsupervised sequence transduction may provide helpful guidance in determining effective training\nobjectives.\n5\nEXPERIMENTS\nWe test our model on ﬁve style transfer tasks: sentiment transfer, word substitution decipherment,\nformality transfer, author imitation, and related language translation. For completeness, we also\nevaluate on the task of general unsupervised machine translation using standard benchmarks.\nWe compare with the unsupervised machine translation model (UNMT) which recently demonstrated\nstate-of-the-art performance on transfer tasks such as sentiment and gender transfer (Lample et al.,\n2019).6 To validate the effect of the negative entropy term in the KL loss term Eq. 5, we remove\nit and train the model with a back-translation loss plus a language model negative log likelihood\nloss (which we denote as BT+NLL) as an ablation baseline. For each task, we also include strong\nbaseline numbers from related work if available. For our method we select the model with the best\nvalidation ELBO, and for UNMT or BT+NLL we select the model with the best back-translation loss.\nComplete model conﬁgurations and hyperparameters can be found in Appendix A.1.\n5.1\nDATASETS AND EXPERIMENT SETUP\nWord Substitution Decipherment. Word decipherment aims to uncover the plain text behind a\ncorpus that was enciphered via word substitution where word in the vocabulary is mapped to a\nunique type in a cipher dictionary (Dou & Knight, 2012; Shen et al., 2017; Yang et al., 2018). In our\nformulation, the model is presented with a non-parallel corpus of English plaintext and the ciphertext.\nWe use the data in (Yang et al., 2018) which provides 200K sentences from each domain. While\nprevious work (Shen et al., 2017; Yang et al., 2018) controls the difﬁculty of this task by varying the\npercentage of words that are ciphered, we directly evaluate on the most difﬁcult version of this task\n– 100% of the words are enciphered (i.e. no vocabulary sharing in the two domains). We select the\nmodel with the best unsupervised reconstruction loss, and evaluate with BLEU score on the test set\nwhich contains 100K parallel sentences. Results are shown in Table 2.\nSentiment Transfer. Sentiment transfer is a task of paraphrasing a sentence with a different sentiment\nwhile preserving the original content. Evaluation of sentiment transfer is difﬁcult and is still an open\nresearch problem (Mir et al., 2019). Evaluation focuses on three aspects: attribute control, content\npreservation, and ﬂuency. A successful system needs to perform well with respect to all three aspects.\nWe follow prior work by using three automatic metrics (Yang et al., 2018; Lample et al., 2019):\nclassiﬁcation accuracy, self-BLEU (BLEU of the output with the original sentence as the reference),\nand the perplexity (PPL) of each system’s output under an external language model. We pretrain a\nconvolutional classiﬁer (Kim, 2014) to assess classiﬁcation accuracy, and use an LSTM language\nmodel pretrained on each domain to compute the PPL of system outputs.\nWe use the Yelp reviews dataset collected by Shen et al. (2017) which contains 250K negative\nsentences and 380K positive sentences. We also use a small test set that has 1000 human-annotated\nparallel sentences introduced in Li et al. (2018). We denote the positive sentiment as domain D1 and\nthe negative sentiment as domain D2. We use Self-BLEU and BLEU to represent the BLEU score of\nthe output against the original sentence and the reference respectively. Results are shown in Table 1.\nFormality Transfer. Next, we consider a harder task of modifying the formality of a sequence. We\nuse the GYAFC dataset (Rao & Tetreault, 2018), which contains formal and informal sentences from\ntwo different domains. In this paper, we use the Entertainment and Music domain, which has about\n52K training sentences, 5K development sentences, and 2.5K test sentences. This dataset actually\ncontains parallel data between formal and informal sentences, which we use only for evaluation. We\nfollow the evaluation of sentiment transfer task and test models on three axes. Since the test set is\n6The model they used is slightly different from the original model of Lample et al. (2018) in certain details\n– e.g. the addition of a pooling layer after attention. We re-implement their model in our codebase for fair\ncomparison and verify that our re-implementation achieves performance competitive with the original paper.\n6\nPublished as a conference paper at ICLR 2020\nTable 1: Results on the sentiment transfer, author imitation, and formality transfer. We list the PPL of\npretrained LMs on the test sets of both domains. We only report Self-BLEU on the sentiment task to\ncompare with existing work.\nTask\nModel\nAcc.\nBLEU\nSelf-BLEU\nPPLD1\nPPLD2\nSentiment\nTest Set\n-\n-\n-\n31.97\n21.87\nShen et al. (2017)\n79.50\n6.80\n12.40\n50.40\n52.70\nHu et al. (2017)\n87.70\n-\n65.60\n115.60\n239.80\nYang et al. (2018)\n83.30\n13.40\n38.60\n30.30\n42.10\nUNMT\n87.17\n16.99\n44.88\n26.53\n35.72\nBT+NLL\n88.36\n12.36\n31.48\n8.75\n12.82\nOurs\n87.90\n18.67\n48.38\n27.75\n35.61\nAuthor Imitation\nTest Set\n-\n-\n-\n132.95\n85.25\nUNMT\n80.23\n7.13\n-\n40.11\n39.38\nBT+NLL\n76.98\n10.80\n-\n61.70\n65.51\nOurs\n81.43\n10.81\n-\n49.62\n44.86\nFormality\nTest Set\n-\n-\n-\n71.30\n135.50\nUNMT\n78.06\n16.11\n-\n26.70\n10.38\nBT+NLL\n82.43\n8.57\n-\n6.57\n8.21\nOurs\n80.46\n18.54\n-\n22.65\n17.23\na parallel corpus, we only compute reference BLEU and ignore self-BLEU. We use D1 to denote\nformal text, and D2 to denote informal text. Results are shown in Table 1.\nAuthor Imitation. Author imitation is the task of paraphrasing a sentence to match another author’s\nstyle. The dataset we use is a collection of Shakespeare’s plays translated line by line into modern\nEnglish. It was collected by Xu et al. (2012)7 and used in prior work on supervised style trans-\nfer (Jhamtani et al., 2017). This is a parallel corpus and thus we follow the setting in the formality\ntransfer task. We use D1 to denote modern English, and D2 to denote Shakespeare-style English.\nResults are shown in Table 1.\nRelated Language Translation. Next, we test our method on a challenging related language\ntranslation task (Pourdamghani & Knight, 2017; Yang et al., 2018). This task is a natural test bed\nfor unsupervised sequence transduction since the goal is to preserve the meaning of the source\nsentence while rewriting it into the target language. For our experiments, we choose Bosnian (bs) and\nSerbian (sr) as the related language pairs. We follow Yang et al. (2018) to report BLEU-1 score on\nthis task since BLEU-4 score is close to zero. Results are shown in Table 2.\nUnsupervised MT. In order to draw connections with a related work on general unsupervised\nmachine translation, we also evaluate on the WMT’16 German English translation task. This task\nis substantially more difﬁcult than the style transfer tasks considered so far. We compare with the\nstate-of-the-art UNMT system using the existing implementation from the XLM codebase,8 and\nimplement our approach in the same framework with XLM initialization for fair comparison. We\ntrain both systems on 5M non-parallel sentences from each language. Results are shown in Table 2.\nTable 2: BLEU for decipherment, related language\ntranslation (Sr-Bs), and general unsupervised transla-\ntion (En-De).\nModel\nDecipher\nSr-Bs\nBs-Sr\nEn-De\nDe-En\nShen et al. (2017)\n50.8\n-\n-\n-\n-\nYang et al. (2018)\n49.3\n31.0\n33.4\n-\n-\nUNMT\n76.4\n31.4\n33.4\n26.5\n32.2\nBT+NLL\n78.0\n29.6\n31.4\n-\n-\nOurs\n78.4\n36.2\n38.3\n26.9\n32.0\nIn Tables 1 we also list the PPL of the\ntest set under the external LM for both the\nsource and target domain. PPL of system\noutputs should be compared to PPL of the\ntest set itself because extremely low PPL\noften indicates that the generated sentences\nare short or trivial.\n5.2\nRESULTS\nTables 1 and 2 demonstrate some general\ntrends. First, UNMT is able to outperform\nother prior methods in unsupervised text style transfer, such as (Yang et al., 2018; Hu et al., 2017;\nShen et al., 2017). The performance improvements of UNMT indicate that ﬂexible and powerful\n7https://github.com/tokestermw/tensorflow-shakespeare\n8https://github.com/facebookresearch/XLM\n7\nPublished as a conference paper at ICLR 2020\narchitectures are crucial (prior methods generally do not have an attention mechanism). Second, our\nmodel achieves comparable classiﬁcation accuracy to UNMT but outperforms it in all style transfer\ntasks in terms of the reference-BLEU, which is the most important metric since it directly measures\nthe quality of the ﬁnal generations against gold parallel data. This indicates that our method is both\neffective and consistent across many different tasks. Finally, the BT+NLL baseline is sometimes quite\ncompetitive, which indicates that the addition of a language model alone can be beneﬁcial. However,\nour method consistently outperforms the simple BT+NLL method, which indicates the effectiveness\nof the additional entropy regularizer in Eq. 5 that is the byproduct of our probabilistic formulation.\nNext, we examine the PPL of the system outputs under pretrained domain LMs, which should be\nevaluated in comparison with the PPL of the test set itself. For both the sentiment transfer and the\nformality transfer tasks in Table 1, BT+NLL achieves extremely low PPL, lower than the PPL of the\ntest corpus in the target domain. After a close examination of the output, we ﬁnd that it contains\nmany repeated and overly simple outputs. For example, the system generates many examples of “I\nlove this place” when transferring negative to positive sentiment (see Appendix A.3 for examples). It\nis not surprising that such a trivial output has low perplexity, high accuracy, and low BLEU score.\nOn the other hand, our system obtains reasonably competitive PPL, and our approach achieves the\nhighest accuracy and higher BLEU score than the UNMT baseline.\n5.3\nFURTHER ABLATIONS AND ANALYSIS\nImprovement Percentage\n-5%\n11.25%\n27.5%\n43.75%\n60%\nNaive Bayes Classiﬁcation \nAccuracy\n0.4\n0.7\n1\nSentiment\nDecipher\nEn-De\nAuthor\nSr-Bs\nFormality\nFigure\n3:\nImprovement\nover\nUNMT vs. classiﬁcation accuracy.\nParameter Sharing. We also conducted an experiment on the\nword substitution decipherment task, where we remove parame-\nter sharing (as explained in Section 3.2) between two directions\nof transduction distributions, and optimize two encoder-decoder\ninstead. We found that the model only obtained an extremely\nlow BLEU score and failed to generate any meaningful outputs.\nPerformance vs. Domain Divergence. Figure 3 plots the rel-\native improvement of our method over UNMT with respect\nto accuracy of a naive Bayes’ classiﬁer trained to predict the\ndomain of test sentences. Tasks with high classiﬁcation accu-\nracy likely have more divergent domains. We can see that for\ndecipherment and en-de translation, where the domains have\ndifferent vocabularies and thus are easily distinguished, our method yields a smaller gain over UNMT.\nThis likely indicates that the (discrimination) regularization effect of the LM priors is less important\nor necessary when the two domains are very different.\nWhy does the proposed model outperform UNMT? Finally, we examine in detail the output of our\nmodel and UNMT for the author imitation task. We pick this task because the reference outputs for\nthe test set are provided, aiding analysis. Examples shown in Table 3 demonstrate that UNMT tends\nto make overly large changes to the source so that the original meaning is lost, while our method is\nbetter at preserving the content of the source sentence. Next, we quantitatively examine the outputs\nfrom UNMT and our method by comparing the F1 measure of words bucketed by their syntactic tags.\nWe use the open-sourced compare-mt tool (Neubig et al., 2019), and the results are shown in Figure\n4. Our system has outperforms UNMT in all word categories. In particular, our system is much better\nat generating nouns, which likely leads to better content preservation.\nCC\nDT\nIN\nJJ\nNN\nNNP\nNNS\nPRP\nRB\nTO\nVB\nVBP\nVBZ\nother\nlabels\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nfmeas\nUNMT\nOurs\nFigure 4: Word F1 score by POS tag.\nTable 3: Examples for author imitation task\nMethods\nShakespeare to Modern\nSource\nNot to his father’s .\nReference\nNot to his father’s house .\nUNMT\nNot to his brother .\nOurs\nNot to his father’s house .\nSource\nSend thy man away .\nReference\nSend your man away .\nUNMT\nSend an excellent word .\nOurs\nSend your man away .\nSource\nWhy should you fall into so deep an O ?\nReference\nWhy should you fall into so deep a moan ?\nUNMT\nWhy should you carry so nicely , but have your legs ?\nOurs\nWhy should you fall into so deep a sin ?\n8\nPublished as a conference paper at ICLR 2020\nTable 4: Comparison of gradient approximation on the sentiment transfer task.\nMethod\ntrain ELBO↑\ntest ELBO↑\nAcc.\nBLEUr\nBLEUs\nPPLD1\nPPLD2\nSample-based\n-3.51\n-3.79\n87.90\n13.34\n33.19\n24.55\n25.67\nGreedy\n-2.05\n-2.07\n87.90\n18.67\n48.38\n27.75\n35.61\nTable 5: Comparison of gradient propagation method on the sentiment transfer task.\nMethod\ntrain ELBO↑\ntest ELBO↑\nAcc.\nBLEUr\nBLEUs\nPPLD1\nPPLD2\nGumbel Softmax\n-2.96\n-2.98\n81.30\n16.17\n40.47\n22.70\n23.88\nREINFORCE\n-6.07\n-6.48\n95.10\n4.08\n9.74\n6.31\n4.08\nStop Gradient\n-2.05\n-2.07\n87.90\n18.67\n48.38\n27.75\n35.61\nGreedy vs. Sample-based Gradient Approximation. In our experiments, we use greedy decoding\nfrom the inference network to approximate the expectation required by ELBO, which is a biased\nestimator. The main purpose of this approach is to reduce the variance of the gradient estimator during\ntraining, especially in the early stages when the variance of sample-based approaches is quite high. As\nan ablation experiment on the sentiment transfer task we compare greedy and sample-based gradient\napproximations in terms of both train and test ELBO, as well as task performance corresponding\nto best test ELBO. After the model is fully trained, we ﬁnd that the sample-based approximation\nhas low variance. With a single sample, the standard deviation of the EBLO is less than 0.3 across\n10 different test repetitions. All ﬁnal reported ELBO values are all computed with this approach,\nregardless of whether the greedy approximation was used during training. The reported ELBO values\nare the evidence lower bound per word. Results are shown in Table 4, where the sampling-based\ntraining underperforms on both ELBO and task evaluations.\n5.4\nCOMPARISON OF GRADIENT PROPAGATION METHODS\nAs noted above, to stabilize the training process, we stop gradients from propagating to the inference\nnetwork from the reconstruction loss. Does this approach indeed better optimize the actual proba-\nbilistic objective (i.e. ELBO) or only indirectly lead to improved task evaluations? In this section we\nuse sentiment transfer as an example task to compare different methods for propagating gradients and\nevaluate both ELBO and task evaluations.\nSpeciﬁcally, we compare three different methods:\n• Stop Gradient: The gradients from reconstruction loss are not propagated to the inference\nnetwork. This is the method we use in all previous experiments.\n• Gumbel Softmax (Jang et al., 2017): Gradients from the reconstruction loss are propagated\nto the inference network with the straight-through Gumbel estimator.\n• REINFORCE (Williams, 1987): Gradients from reconstruction loss are propagated to the\ninference network with ELBO as a reward function. This method has been used in previous\nwork for semi-supervised sequence generation (Miao & Blunsom, 2016; Yin et al., 2018),\nbut often suffers from instability issues.\nWe report the train and test ELBO along with task evaluations in Table 5, and plot the learning\ncurves on validation set in Figure 5.9 While being much simpler, we show that the stop-gradient\ntrick produces superior ELBO over Gumbel Softmax and REINFORCE. This result suggests that\nstopping gradient helps better optimize the likelihood objective under our probabilistic formulation in\ncomparison with other optimization techniques that propagate gradients, which is counter-intuitive. A\nlikely explanation is that as a gradient estimator, while clearly biased, stop-gradient has substantially\nreduced variance. In comparison with other techniques that offer reduced bias but extremely high\nvariance when applied to our model class (which involves discrete sequences as latent variables),\nstop-gradient actually leads to better optimization of our objective because it achieves better balance\nof bias and variance overall.\n9We remove REINFORCE from this ﬁgure since it is very difﬁcult to stabilize training and obtain reasonable\nresults (e.g. the ELBO value is much worse than others in Table 5)\n9\nPublished as a conference paper at ICLR 2020\n0\n10000\n20000\n30000\n40000\n50000\nstep\n5.0\n4.5\n4.0\n3.5\n3.0\n2.5\n2.0\nELBO\ngumbel softmax\nstop gradient\nFigure 5: ELBO on the validation set v.s. the number training steps.\n6\nCONCLUSION\nWe propose a probabilistic generative forumalation that unites past work on unsupervised text style\ntransfer. We show that this probabilistic formulation provides a different way to reason about\nunsupervised objectives in this domain. Our model leads to substantial improvements on ﬁve text\nstyle transfer tasks, yielding bigger gains when the styles considered are more difﬁcult to distinguish.\nACKNOWLEDGEMENT\nThe work of Junxian He and Xinyi Wang is supported by the DARPA GAILA project (award\nHR00111990063) and the Tang Family Foundation respectively. The authors would like to thank\nZichao Yang for helpful feedback about the project.\nREFERENCES\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine\ntranslation. In Proceedings of ICLR, 2018.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. An effective approach to unsupervised machine\ntranslation. arXiv preprint arXiv:1902.01313, 2019.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of ICLR, 2015.\nColin Bannard and Chris Callison-Burch. Paraphrasing with bilingual parallel corpora. In Proceedings\nof ACL, 2005.\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio.\nGenerating sentences from a continuous space. In Proceedings of ConNLL, 2016.\nQing Dou and Kevin Knight. Dependency-based decipherment for resource-limited machine transla-\ntion. Proceedings of EMNLP, 2012.\nDi He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. Dual learning\nfor machine translation. In Proceedings of NeurIPS, 2016.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. Toward controlled\ngeneration of text. In Proceedings of ICML, 2017.\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In\nProceedings of ICLR, 2017.\nHarsh Jhamtani, Varun Gangal, Edward Hovy, and Eric Nyberg. Shakespearizing modern language\nusing copy-enriched sequence-to-sequence models. Proceedings of EMNLP, 2017.\n10\nPublished as a conference paper at ICLR 2020\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil\nThorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, et al. Google’s multilingual neural\nmachine translation system: Enabling zero-shot translation. Transactions of the Association for\nComputational Linguistics, 2017.\nYoon Kim. Convolutional neural networks for sentence classiﬁcation. In Proceedings of EMNLP,\n2014.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nKevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. Unsupervised analysis for decipherment\nproblems. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pp. 499–\n506, 2006.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised\nmachine translation using monolingual corpora only. arXiv preprint arXiv:1711.00043, 2017.\nGuillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Phrase-\nbased & neural unsupervised machine translation. arXiv preprint arXiv:1804.07755, 2018.\nGuillaume Lample, Sandeep Subramanian, Eric Smith, Ludovic Denoyer, Marc’Aurelio Ranzato,\nand Y-Lan Boureau. Multiple-attribute text rewriting. In Proceedings of ICLR, 2019.\nJuncen Li, Robin Jia, He He, and Percy Liang. Delete, retrieve, generate: A simple approach to\nsentiment and style transfer. arXiv preprint arXiv:1804.06437, 2018.\nYishu Miao and Phil Blunsom. Language as a latent variable: Discrete generative models for sentence\ncompression. In Proceedings of EMNLP, 2016.\nRonen Mir, Bjarke Felbo, Nick Obradovich, and Iyad Rahwan. Evaluating style transfer for text. In\nProceedings of NAACL, 2019.\nMasahiro Mizukami, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. Linguis-\ntic individuality transformation for spoken language. In Natural Language Dialog Systems and\nIntelligent Assistants. 2015.\nGraham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel, Danish Pruthi, and Xinyi Wang. compare-mt: A\ntool for holistic comparison of language generation systems. In Meeting of the North American\nChapter of the Association for Computational Linguistics (NAACL) Demo Track, Minneapolis,\nUSA, June 2019. URL http://arxiv.org/abs/1903.07926.\nNima Pourdamghani and Kevin Knight. Deciphering related languages. Proceedings of EMNLP,\n2017.\nSudha Rao and Joel Tetreault. Dear sir or madam, may i introduce the gyafc dataset: Corpus,\nbenchmarks and metrics for formality style transfer. arXiv preprint arXiv:1803.06535, 2018.\nSujith Ravi and Kevin Knight. Deciphering foreign language. In Proceedings of ACL, 2011.\nAlexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive\nsentence summarization. In Proceedings of EMNLP, 2015.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models\nwith monolingual data. In Proceedings of ACL, 2016.\nClaude Elwood Shannon. A mathematical theory of communication. Bell system technical journal,\n27(3):379–423, 1948.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. Style transfer from non-parallel text\nby cross-alignment. In Proceeings of NIPS, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of NeurIPS, 2017.\n11\nPublished as a conference paper at ICLR 2020\nR Williams. A class of gradient-estimation algorithms for reinforcement learning in neural networks.\nIn Proceedings of the International Conference on Neural Networks, pp. II–601, 1987.\nWei Xu, Alan Ritter, William B. Dolan, Ralph Grishman, and Cherry Colin. Paraphrasing for style.\nCOLING, 2012.\nZichao Yang, Zhiting Hu, Chris Dyer, Eric P Xing, and Taylor Berg-Kirkpatrick. Unsupervised text\nstyle transfer using language models as discriminators. In Proceedings of NeurIPS, 2018.\nPengcheng Yin, Chunting Zhou, Junxian He, and Graham Neubig. Structvae: Tree-structured latent\nvariable models for semi-supervised semantic parsing. In Proceedings of ACL, 2018.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural\ndialog models using conditional variational autoencoders. In Proceedings of ACL, 2017.\n12\nPublished as a conference paper at ICLR 2020\nA\nAPPENDIX\nA.1\nMODEL CONFIGURATIONS.\nWe adopt the following attentional encoder-decoder architecture for UNMT, BT+NLL, and our\nmethod across all the experiments:\n• We use word embeddings of size 128.\n• We use 1 layer LSTM with hidden size of 512 as both the encoder and decoder.\n• We apply dropout to the readout states before softmax with a rate of 0.3.\n• Following Lample et al. (2019), we add a max pooling operation over the encoder hidden\nstates before feeding it to the decoder. Intuitively the pooling window size would control\nhow much information is preserved during transduction. A window size of 1 is equivalent to\nstandard attention mechanism, and a large window size corresponds to no attention. See\nAppendix A.2 for how to select the window size.\n• There is a noise function for UNMT baseline in its denoising autoencoder loss (Lample\net al., 2017; 2019), which is critical for its success. We use the default noise function\nand noise hyperparameters in Lample et al. (2017) when running the UNMT model. For\nBT+NLL and our method we found that adding the extra noise into the self-reconstruction\nloss (Eq. 4) is only helpful when the two domains are relatively divergent (decipherment\nand related language translation tasks) where the language models play a less important\nrole. Therefore, we add the default noise from UNMT to Eq. 4 for decipherment and related\nlanguage translation tasks only, and do not use any noise for sentiment, author imitation,\nand formality tasks.\nA.2\nHYPERPARAMETER TUNING.\nWe vary pooling windows size as {1, 5}, the decaying patience hyperparameter k for self-\nreconstruction loss (Eq. 4) as {1, 2, 3}. For the baseliens UNMT and BT+NLL, we also try the\noption of not annealing the self-reconstruction loss at all as in the unsupervised machine translation\ntask (Lample et al., 2018). We vary the weight λ for the NLL term (BT+NLL) or the KL term (our\nmethod) as {0.001, 0.01, 0.03, 0.05, 0.1}.\nA.3\nSENTIMENT TRANSFER EXAMPLE OUTPUTS\nWe list some examples of the sentiment transfer task in Table 6. Notably, the BT+NLL method tends\nto produce extremely short and simple sentences.\nA.4\nREPETITIVE EXAMPLES OF BT+NLL\nIn Section 5 we mentioned that the baseline BT+NLL has a low perplexity for some tasks because it\ntends to generate overly simple and repetitive sentences. From Table 1 we see that two representative\ntasks are sentiment transfer and formatliy transfer. In Appendix A.3 we have demonstrated some\nexamples for sentiment transfer, next we show some repetitive samples of BT+NLL in Table 7.\n13\nPublished as a conference paper at ICLR 2020\nTable 6: Random Sentiment Transfer Examples\nMethods\nnegative to positive\nOriginal\nthe cake portion was extremely light and a bit dry .\nUNMT\nthe cake portion was extremely light and a bit spicy .\nBT+NLL\nthe cake portion was extremely light and a bit dry .\nOurs\nthe cake portion was extremely light and a bit fresh .\nOriginal\nthe “ chicken ” strip were paper thin oddly ﬂavored strips .\nUNMT\nthe “ chicken ” were extra crispy noodles were fresh and incredible .\nBT+NLL\nthe service was great .\nOurs\nthe “ chicken ” strip were paper sweet & juicy ﬂavored .\nOriginal\nif i could give them a zero star review i would !\nUNMT\nif i could give them a zero star review i would !\nBT+NLL\ni love this place .\nOurs\ni love the restaurant and give a great review i would !\npositive to negative\nOriginal\ngreat food , staff is unbelievably nice .\nUNMT\nno , food is n’t particularly friendly .\nBT+NLL\ni will not be back .\nOurs\nno apologies , staff is unbelievably poor .\nOriginal\nmy wife and i love coming here !\nUNMT\nmy wife and i do n’t come here !\nBT+NLL\ni will not be back .\nOurs\nmy wife and i walked out the last time .\nOriginal\nmy wife and i love coming here !\nUNMT\nmy wife and i do n’t come here !\nBT+NLL\ni will not be back .\nOurs\nmy wife and i walked out the last time .\nOriginal\nthe premier hookah lounge of las vegas !\nUNMT\nthe worst museum of las vegas !\nBT+NLL\nthe worst frame shop of las vegas !\nOurs\nthe hallways scam lounge of las vegas !\nTable 7: Repetitive examples of BT+NLL baseline on Formality transfer.\nOriginal\nTransferred\nformal to informal\nI like Rhythm and Blue music .\nI like her and I don’t know .\nThere’s nothing he needs to change .\nI don’t know , but I don’t know .\nI enjoy watching my companion attempt to role @-@ play with them .\nI don’t know , but I don’t know .\nI am watching it right now .\nI don’t know , but I don’t know .\nThat is the key point , that you fell asleep .\nI don’t know , but I don’t know .\ninformal to formal\nits a great source just download it .\nI do not know , but I do not know .\nHappy Days , it was the coolest !\nI do not know , but I do not know .\nI used to play ﬂute but once I started sax , I got hooked .\nI do not know , but I do not know .\nThe word you are looking for is ............. strengths\nThe word you are looking for is : )\nPlus you can tell she really cared about her crew .\nPlus you can tell she really cared about her crew .\n14\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-02-10",
  "updated": "2020-04-29"
}