{
  "id": "http://arxiv.org/abs/2002.04806v1",
  "title": "The Unreasonable Effectiveness of Deep Learning in Artificial Intelligence",
  "authors": [
    "Terrence J. Sejnowski"
  ],
  "abstract": "Deep learning networks have been trained to recognize speech, caption\nphotographs and translate text between languages at high levels of performance.\nAlthough applications of deep learning networks to real world problems have\nbecome ubiquitous, our understanding of why they are so effective is lacking.\nThese empirical results should not be possible according to sample complexity\nin statistics and non-convex optimization theory. However, paradoxes in the\ntraining and effectiveness of deep learning networks are being investigated and\ninsights are being found in the geometry of high-dimensional spaces. A\nmathematical theory of deep learning would illuminate how they function, allow\nus to assess the strengths and weaknesses of different network architectures\nand lead to major improvements. Deep learning has provided natural ways for\nhumans to communicate with digital devices and is foundational for building\nartificial general intelligence. Deep learning was inspired by the architecture\nof the cerebral cortex and insights into autonomy and general intelligence may\nbe found in other brain regions that are essential for planning and survival,\nbut major breakthroughs will be needed to achieve these goals.",
  "text": "1 \n \nSejnowski, T. J. The unreasonable effectiveness of deep learning in artificial intelligence,  \nProceedings of the National Academy of Sciences U.S.A.  (2020) \nhttps://www.pnas.org/content/early/2020/01/23/1907373117 \n \nThe Unreasonable Effectiveness of Deep Learning in Artificial \nIntelligence \nTerrence J. Sejnowskia,b,* \na Computational Neurobiology Laboratory, Salk Institute, La Jolla, CA, 92037 USA \nb Division of  Biological Sciences, University of California San Diego, La Jolla, California \n92093 USA \nAbstract:   \nDeep learning networks have been trained to recognize speech, caption photographs and translate \ntext between languages at high levels of performance.  Although applications of deep learning \nnetworks to real world problems have become ubiquitous, our understanding of why they are so \neffective is lacking.  These empirical results should not be possible according to sample \ncomplexity in statistics and non-convex optimization theory.  However, paradoxes in the training \nand effectiveness of deep learning networks are being investigated and insights are being found \nin the geometry of high-dimensional spaces.   A mathematical theory of deep learning would \nilluminate how they function, allow us to assess the strengths and weaknesses of different \nnetwork architectures and lead to major improvements.  Deep learning has provided natural ways \nfor humans to communicate with digital devices and is foundational for building artificial \ngeneral intelligence.   Deep learning was inspired by the architecture of the cerebral cortex and \ninsights into autonomy and general intelligence may be found in other brain regions that are \nessential for planning and survival, but major breakthroughs will be needed to achieve these \ngoals.   \n \n* Correspondence: terry@salk.edu \n \n2 \n \nIn 1884, Edwin Abbott wrote “Flatland: A Romance of Many Dimensions” (1) (Fig. 1). This \nbook was written as a satire on Victorian society, but it has endured because of its exploration of \nhow dimensionality can change our intuitions about space.  Flatland was a two-dimensional \nworld inhabited by geometrical creatures.  The mathematics of two dimensions was fully \nunderstood by these creatures, with circles being more perfect than triangles.  In it a gentleman \nsquare has a dream about a sphere and wakes up to the possibility that his universe might be \nmuch larger than he or anyone in flatland could imagine.   He was not able to convince anyone \nthat this was possible and in the end he was imprisoned.  \nWe can easily imagine adding another spatial dimension when going from a one- to a two-\ndimensional world and from a two- to a three-dimensional world.   Lines can intersect \nthemselves in two dimensions and sheets can fold back onto themselves in three dimensions.  \nBut imagining how a three-dimensional object can fold back on itself in a four-dimensional \nspace is a stretch that was achieved by Charles Howard Hinton in the 19th century (2).  What are \nthe properties of spaces having even higher dimensions?  What is it like to live in a space with \none hundred dimensions?  Or a million dimensions?  Or a space like our brain that has a million \nbillion dimensions (the number of synapses between neurons)? \nThe first Neural Information Processing Systems (NeurIPS) Conference and Workshop took \nplace at the Denver Tech Center in 1987 (Fig. 2).  The 600 attendees were from a wide range of \ndisciplines, including physics, neuroscience, psychology, statistics, electrical engineering, \ncomputer science, computer vision, speech recognition and robotics.  But they all had something \nin common:  they all worked on intractably difficult problems that were not easily solved with \ntraditional methods and they tended to be outliers in their home disciplines.  In retrospect, 33 \nyears later, these misfits were pushing the frontiers of their fields into high-dimensional spaces \npopulated by big data sets, the world we are living in today.  As the president of the Foundation \nthat organizes the annual NeurIPS conferences, I oversaw the remarkable evolution of a \ncommunity that created modern machine learning.  This conference has grown steadily and in \n2019 attracted over 14,000 participants. Many intractable problems eventually became tractable, \nand today machine learning serves as a foundation for contemporary artificial intelligence (AI). \n3 \n \nThe early goals of machine learning were more modest than those of artificial intelligence.  \nRather than aim directly at general intelligence, machine learning started by attacking practical \nproblems in perception, language, motor control, prediction and inference using learning from \ndata as the primary tool.  In contrast, early attempts in AI were characterized by low-dimensional \nalgorithms that were handcrafted.  However, this approach only worked for well controlled \nenvironments.  For example, in Blocks World, all objects were rectangular solids, identically \npainted and in an environment with fixed lighting.  These algorithms did not scale up to vision in \nthe real world, where objects have complex shapes, a wide range of reflectances and lighting \nconditions are uncontrolled.  The real world is high-dimensional and there may not be any low-\ndimensional model that can be fit to it (3).  Similar problems were encountered with early \nmodels of natural languages based on symbols and syntax, which ignored the complexities of \nsemantics (4).  Practical natural language applications became possible once the complexity of \ndeep learning language models approached the complexity of the real world.  Models of natural \nlanguage with millions of parameters and trained with millions of labeled examples are now used \nroutinely.  Even larger deep learning language networks are in production today, providing \nservices to millions of users online, less than a decade since they were introduced. \n \nOrigins of Deep Learning.  I have written a book, The Deep Learning Revolution: Artificial \nIntelligence Meets Human Intelligence (5), which tells the story of how deep learning came \nabout.  Deep learning was inspired by the massively parallel architecture found in brains and its \norigins can be traced to  Frank Rosenblatt’s perceptron (6) in the 1950s that was based on a \nsimplified model of a single neuron introduced by McCulloch and Pitts (7).  The perceptron \nperformed pattern recognition and learned to classify labeled examples (Fig. 3).  Rosenblatt \nproved a theorem that if there was a set of parameters that could classify new inputs correctly, \nand there were enough examples, his learning algorithm was guaranteed to find it. The learning \nalgorithm used labeled data to make small changes to parameters, which were the weights on the \ninputs to a binary threshold unit, implementing gradient descent.  This simple paradigm is at the \ncore of much larger and more sophisticated neural network architectures today, but the jump \nfrom perceptrons to deep learning was not a smooth one.  There are lessons to be learned from \nhow this happened. \n4 \n \nThe perceptron learning algorithm required computing with real numbers, which digital \ncomputers performed inefficiently in the 1950s.  Rosenblatt received a grant for the equivalent \ntoday of one million dollars from the Office of Naval Research to build a large analog computer \nthat could perform the weight updates in parallel using banks of motor-driven potentiometers \nrepresenting variable weights (Fig. 3).   The great expectations in the press (Fig. 3) were dashed \nby Marvin Minsky and Seymour Papert, who showed in their book Perceptrons (8) that a \nperceptron can only represent categories that are linearly separable in weight space. Although, at \nthe end of their book, Minsky and Papert considered the prospect of generalizing single- to \nmultiple-layer perceptrons, one layer feeding into the next, they doubted there would ever be a \nway to train these more powerful multilayer perceptrons. Unfortunately, many took this doubt to \nbe definitive, and the field was abandoned until a new generation of neural network researchers \ntook a fresh look at the problem in the 1980s.  \nThe computational power available for research in the 1960s was puny compared to what we \nhave today; this favored programming rather than learning, and early progress with writing \nprograms to solve toy problems looked encouraging.  By the 1970s, learning had fallen out of \nfavor, but by the 1980s digital computers had increased in speed making it possible to simulate \nmodestly-sized neural networks.  During the ensuing neural network revival in the 1980s, \nGeoffrey Hinton and I introduced a learning algorithm for Boltzmann machines proving that \ncontrary to general belief, it was possible to train multilayer networks (9).  The Boltzmann \nmachine learning algorithm is local and only depends on correlations between the inputs and \noutputs of single neurons, a form of Hebbian plasticity that is found in the cortex (10). \nIntriguingly, the correlations computed during training must be normalized by correlations that \noccur without inputs, which we called the sleep state, to prevent self-referential learning.  It is \nalso possible to learn the joint probability distributions of inputs without labels in an \nunsupervised learning mode.  However, another learning algorithm introduced at around the \nsame time based on the backpropagation of errors was much more efficient, though at the \nexpense of locality (11).  Both of these learning algorithm use stochastic gradient descent, an \noptimization technique that incrementally changes the parameter values to minimize a loss \nfunction.  Typically this is done after averaging the gradients for a small batch of training \nexamples. \n5 \n \nLost in Parameter Space.  The network models in the 1980s rarely had more than one layer of \nhidden units between the inputs and outputs, but they were already highly over-parameterized by \nthe standards of statistical learning.  Empirical studies uncovered a number of paradoxes that \ncould not be explained at the time.  Even though the networks were tiny by today’s standards, \nthey had orders of magnitude more parameters than traditional statistical models.   According to \nbounds from theorems in statistics, generalization should not be possible with the relatively small \ntraining sets that were available. But even simple methods for regularization, such as weight \ndecay, led to models with surprisingly good generalization.   \nEven more surprising, stochastic gradient descent of non-convex loss functions were rarely \ntrapped in local minima. There were long plateaus on the way down when the error hardly \nchanged, followed by sharp drops.  Something about these network models and the geometry of \ntheir high-dimensional parameter spaces allowed them to navigate efficiently to solutions and \nachieve good generalization, contrary to the failures predicted by conventional intuition.   \nNetwork models are high-dimensional dynamical systems that learn how to map input spaces \ninto output spaces. These functions have special mathematical properties that we are just \nbeginning to understand.  Local minima during learning are rare because in the high-dimensional \nparameter space, most critical points are saddle points (12).  Another reason why good solutions \ncan be found so easily by stochastic gradient descent is that, unlike low-dimensional models \nwhere a unique solution is sought, different networks with good performance converge from \nrandom starting points in parameter space.  Because of over-parameterization (13), the \ndegeneracy of solutions changes the nature of the problem from finding a needle in a haystack to \na haystack of needles.   \nMany questions are left unanswered.  Why is it possible to generalize from so few examples and \nso many parameters?  Why is stochastic gradient descent so effective at finding useful functions \ncompared to other optimization methods?  How large is the set of all good solutions to a \nproblem?  Are good solutions related to each other in some way?  What are the relationships \nbetween architectural features and inductive bias that can improve generalization?  The answers \nto these questions will help us design better network architectures and more efficient learning \nalgorithms.   \n6 \n \nWhat no one knew back in the 1980s was how well neural network learning algorithms would \nscale with the number of units and weights in the network.  Unlike many AI algorithms that scale \ncombinatorically, as deep learning networks expanded in size, training scaled linearly with the \nnumber of parameters and performance continued to improve as more layers were added (14).  \nFurthermore, the massively parallel architectures of deep learning networks can be efficiently \nimplemented by multicore chips.  The complexity of learning and inference with fully parallel \nhardware is O(1).  This is a rare conjunction of favorable computational properties. \nWhen a new class of functions is introduced, it takes generations to fully explore them. For \nexample, when Joseph Fourier introduced Fourier series in 1807, he could not prove \nconvergence and their status as functions was questioned.  This did not stop engineers from using \nFourier series to solve the heat equation and apply them to other practical problems. The study of \nthis class of functions eventually led to deep insights into functional analysis, a jewel in the \ncrown of mathematics.   \nThe Nature of Deep Learning.  The third wave of exploration into neural network architectures, \nunfolding today, has greatly expanded beyond its academic origins, following the first two waves \nspurred by perceptrons in the 1950s and multilayer neural networks in the 1980s.  The press has \nrebranded deep learning as AI.  What deep learning has done for AI is to ground it in the real \nworld. The real world is analog, noisy, uncertain and high dimensional, which never jived with \nthe black and white world of symbols and rules in traditional AI.  Deep learning provides an \ninterface between these two worlds.  For example, natural language processing has traditionally \nbeen cast as a problem in symbol processing. However, end-to-end learning of language \ntranslation in recurrent neural networks extracts both syntactic and semantic information from \nsentences.  Natural language applications often start not with symbols, but with word \nembeddings in deep learning networks trained to predict the next word in a sentence (15), which \nare semantically deep and represent relationships between words as well as associations.  Once \nregarded as “just statistics,” deep recurrent networks are high-dimensional dynamical systems \nthrough which information flows much as electrical activity flows through brains.   \nOne of the early tensions in artificial intelligence research in the 1960s was its relationship to \nhuman intelligence.  The engineering goal of artificial intelligence was to reproduce the \n7 \n \nfunctional capabilities of human intelligence by writing programs based on intuition.  I once \nasked Allen Newell, a computer scientist from Carnegie Mellon University and one of the \npioneers of artificial intelligence who attend the seminal Dartmouth summer conference in 1956, \nwhy AI pioneers had ignored brains, the substrate of human intelligence.  The performance of \nbrains was the only existence proof that any of the hard problems in AI could be solved.  He told \nme that he personally had been open to insights from brain research, but there simply hadn’t \nbeen enough known about brains at the time to be of much help.  \nOver time, the attitude in AI had changed from “not enough is known” to “brains are not \nrelevant.”  This view was commonly justified by an analogy with aviation:  “If you want to build \na flying machine, you would be wasting your time studying birds that flap their wings or the \nproperties of their feathers.”  Quite to the contrary, the Wright Brothers were keen observers of \ngliding birds, which are highly efficient flyers (16).  What they learned from birds was ideas for \ndesigning practical airfoils and basic principles of aerodynamics.  Modern jets have even \nsprouted winglets at the tips of wings, which saves 5% on fuel and look suspiciously like \nwingtips on eagles (Fig. 4).  Much more is now known about how brains process sensory \ninformation, accumulate evidence, make decisions and plan future actions.  Deep learning was \nsimilarly inspired by nature.  There is a burgeoning new field in computer science, called \nalgorithmic biology, which seeks to describe the wide range of problem-solving strategies used \nby biological systems (17).  The lesson here is we can learn from nature general principles and \nspecific solutions to complex problems, honed by evolution and passed down the chain of life to \nhumans.   \nThere is a stark contrast between the complexity of real neurons and the simplicity of the model \nneurons in neural network models.  Neurons are themselves complex dynamical systems with a \nwide range of internal time scales.    Much of the complexity of real neurons is inherited from \ncell biology – the need for each cell to generate its own energy and maintain homeostasis under a \nwide range of challenging conditions.  But other features of neurons are likely to be important \nfor their computational function, some of which have not yet been exploited in model networks.  \nThese features include a diversity of cell types, optimized for specific functions; short-term \nsynaptic plasticity, which can be either facilitating or depressing on a time scales of seconds; a \ncascade of biochemical reactions underlying plasticity inside synapses controlled by the history \n8 \n \nof inputs that extends from seconds to hours; sleep states during which a brain goes offline to \nrestructure itself; and communication networks that control traffic between brain areas (18).  \nSynergies between brains and AI may now be possible that could benefit both biology and \nengineering. \nThe neocortex appeared in mammals 200 million years ago.  It is a folded sheet of neurons on \nthe outer surface of the brain, called the gray matter, which in humans is about 30 cm in diameter \nand 5 mm thick when flattened. There are about 30 billion cortical neurons forming 6 layers that \nare highly interconnected with each other in a local stereotyped pattern.  The cortex greatly \nexpanded in size relative the central core of the brain during evolution, especially in humans \nwhere it constitutes 80% of the brain volume.  This expansion suggests that the cortical \narchitecture is scalable-- more is better--unlike most brain areas, which have not expanded \nrelative to body size.  Interestingly, there are many fewer long-range connections than local \nconnections, which form the white matter of the cortex, but its volume scales as the 5/4 power of \nthe gray matter volume and becomes larger than the volume of the gray matter in large brains \n(19).  Scaling laws for brain structures can provide insights into important computational \nprinciples (20).  Cortical architecture including cell types and their connectivity is similar \nthroughout the cortex, with specialized regions for different cognitive systems.  For example, the \nvisual cortex has evolved specialized circuits for vision, which have been exploited in \nconvolutional neural networks (CNN), the most successful deep learning architecture.  Having \nevolved a general purpose learning architecture, the neocortex greatly enhances the performance \nof many special purpose subcortical structures.  \nBrains have eleven orders of magnitude of spatially structured computing components (Fig. 5).  \nAt the level of synapses, each cubic millimeter of the cerebral cortex, about the size of a rice \ngrain, contains a billion synapses.  The largest deep learning networks today are reaching a \nbillion weights.  The cortex has the equivalent power of hundreds of thousands of deep learning \nnetworks, each specialized for solving specific problems.  How are all these expert networks \norganized?  The levels of investigation above the network level organize the flow of information \nbetween different cortical areas, a system-level communications problem. There is much to be \nlearned about how to organize thousands of specialized networks by studying how the global \nflow of information in the cortex is managed.  Long-range connections within the cortex are \n9 \n \nsparse because they are expensive, both because of the energy demand needed to send \ninformation over a long distance and also because they occupy a large volume of space.  A \nswitching network routes information between sensory and motor areas that can be rapidly \nreconfigured to meet ongoing cognitive demands (18).   \nAnother major challenge for building the next generation of AI systems will be memory \nmanagement for highly heterogeneous systems of deep learning specialist networks. There is \nneed to flexibly update these networks without degrading already learned memories; this is the \nproblem of maintaining stable, lifelong learning (21).  There are ways to minimize memory loss \nand interference between subsystems.  One way is to be selective about where to store a new \nexperiences. This occurs during sleep, when the cortex enters globally coherent patterns of \nelectrical activity.  Brief oscillatory events, known as sleep spindles, recur thousands of times \nduring the night and are associated with the consolidation of memories.  Spindles are triggered \nby the replay of recent episodes experienced during the day and are parsimoniously integrated \ninto long-term cortical semantic memory (22, 23). \nThe Future of Deep Learning.  Although the focus today on deep learning was inspired by the \ncerebral cortex, a much wider range of architectures is needed to control movements and vital \nfunctions.   Subcortical parts of mammalian brains essential for survival can be found in all \nvertebrates, including the basal ganglia that is responsible for reinforcement learning and the \ncerebellum, which provides the brain with forward models of motor commands.   Humans are \nhypersocial, with extensive cortical and subcortical neural circuits to support complex social \ninteractions (24). These brain areas will provide inspiration to those who aim to build \nautonomous AI systems.   \nFor example, the dopamine neurons in the brainstem compute reward prediction error, which is a \nkey computation in the temporal difference learning algorithm in reinforcement learning and, in \nconjunction with deep learning, powered AlphaGo to beat Ke Jie, the world champion Go player \nin 2017 (25).  Recordings from dopamine neurons in the midbrain, which project diffusely \nthroughout the cortex and basal ganglia, modulate synaptic plasticity and provide motivation for \nobtaining long-term rewards (26).  Subsequent confirmation of the role of dopamine neurons in \nhumans has led to a new field, neuroeconomics, whose goal is better understand how humans \n10 \n \nmake economic decisions (27). Several other neuromodulatory systems also control global brain \nstates to guide behavior, representing negative rewards, surprise, confidence and temporal \ndiscounting (28). \nMotor systems are another area of AI where biologically-inspired solutions may be helpful.  \nCompare the fluid flow of animal movements to the rigid motions of most robots.  The key \ndifference is the exceptional flexibility exhibited in the control of high-dimensional musculature \nin all animals.  Coordinated behavior in high-dimensional motor planning spaces is an active \narea of investigation in deep learning networks (29).   There is also a need for a theory of \ndistributed control to explain how the multiple layers of control in the spinal cord, brainstem and \nforebrain are coordinated.  Both brains and control systems have to deal with time delays in \nfeedback loops, which can become unstable.  The forward model of the body in the cerebellum \nprovides a way to predict the sensory outcome of a motor command, and the sensory prediction \nerrors are used to optimize open loop control.  For example, the vestibulo-ocular reflex (VOR) \nstabilizes image on the retina despite head movements by rapidly using head acceleration signals \nin an open loop; the gain of the VOR is adapted by slip signals from the retina, which the \ncerebellum uses to reduce the slip (30). Brains have additional constraints due to the limited \nbandwidth of sensory and motor nerves, but these can be overcome in layered control systems \nwith components having a diversity of speed-accuracy tradeoffs (31).  A similar diversity is also \npresent in engineered systems, allowing fast and accurate control despite having imperfect \ncomponents (32).  \nTowards artificial general intelligence.  Is there a path from the current state-of-the-art in deep \nlearning to artificial general intelligence?  From the perspective of evolution, most animals can \nsolve problems needed to survive in their niches, but general abstract reasoning emerged more \nrecently in the human lineage.  However, we are not very good at it and need long training to \nachieve the ability to reason logically.  This is because we are using brain systems to simulate \nlogical steps that have not been optimized for logic.  Students in grade school work for years to \nmaster simple arithmetic, effectively emulating a digital computer with a one second clock.   \nNonetheless, reasoning in humans is proof of principle that it should be possible to evolve large-\nscale systems of deep learning networks for rational planning and decision making.  However, a \nhybrid solution might also be possible, similar to neural Turing machines developed by \n11 \n \nDeepMind for learning how to copy, sort, and navigate (33).  According to Orgel’s Second Rule, \nnature is cleverer than we are, but improvements may still be possible. \nRecent successes with supervised learning in deep networks have led to a proliferation of \napplications where large data sets are available.  Language translation was greatly improved by \ntraining on large corpora of translated texts.  However, there are many applications for which \nlarge sets of labeled data are not available.  Humans commonly make subconscious predictions \nabout outcomes in the physical world, and are surprised by the unexpected.  Self-supervised \nlearning, in which the goal of learning is to predict the future output from other data streams is a \npromising direction (34).  Imitation learning is also a powerful way learn important behaviors \nand gain knowledge about the world (35).   Humans have many ways to learn and require a long \nperiod of development to achieve adult levels of performance.  \nBrains intelligently and spontaneously generate ideas and solutions to problems. When a subject \nis asked to lie quietly at rest in a brain scanner, activity switches from sensorimotor areas to a \ndefault mode network of areas that support inner thoughts, including unconscious activity.  \nGenerative neural network models can learn without supervision, with the goal of learning joint \nprobability distributions from raw sensory data, which is abundant.  The Boltzmann machine is \nan example of generative model (9).  After a Boltzmann machine has been trained to classify \ninputs, clamping an output unit on generates a sequence of examples from that category on the \ninput layer (36).  Generative adversarial networks (GANs) can also generate new samples from a \nprobability distribution learned by self-supervised learning (37).  Brains also generate vivid \nvisual images during dream sleep that are often bizarre. \nLooking ahead.  We are at the beginning of a new era that could be called the age of \ninformation.  Data are gushing from sensors, the sources for pipelines that turn data into \ninformation, information into knowledge, knowledge into understanding, and, if we are \nfortunate, knowledge into wisdom.  We have taken our first steps toward dealing with complex \nhigh-dimensional problems in the real world; like a baby’s, they are more stumble than stride, \nbut what is important is that we are heading in the right direction.  Deep learning networks are \nbridges between digital computers and the real world; this allows us to communicate with \ncomputers on our own terms.  We already talk to smart speakers, which will become much \n12 \n \nsmarter.  Keyboards will become obsolete, taking their place in museums alongside typewriters.  \nThis makes the benefits of deep learning available to everyone.  \nIn his essay on “The unreasonable effectiveness of mathematics in the natural sciences,” Eugene \nWigner marveled that the mathematical structure of a physical theory often reveals deep insights \ninto that theory that lead to empirical predictions (38).  Also remarkable is that there are so few \nparameters in the equations, called physical constants. The title of this article mirrors Wigner’s.  \nBut unlike the laws of physics, there is an abundance of parameters in deep learning networks \nand they are variable.  We are just beginning to explore representation and optimization in very \nhigh-dimensional spaces.  Perhaps someday an analysis of the structure of deep learning \nnetworks will lead to theoretical predictions and reveal deep insights into the nature of \nintelligence.  We can benefit from the blessings of dimensionality. \nHaving found one class of functions to describe the complexity of signals in the world, perhaps \nthere are others.  Perhaps there is a universe of massively-parallel algorithms in high-\ndimensional spaces that we have not yet explored, which go beyond intuitions from the three-\ndimensional world we inhabit and the one-dimensional sequences of instructions in digital \ncomputers.  Like the gentleman square in flatland (Fig. 1) and the explorer in the Flammarion \nengraving (Fig. 6), we have glimpsed a new world stretching far beyond old horizons.  \n \n \n \n13 \n \nFigure Captions \n \nFigure 1. Cover of the 1884 edition of Flatland: A Romance in Many Dimensions by Edwin \nA. Abbott (1).  Inhabitants were two-dimensional shapes, with their rank in society determined \nby the number of sides. \n \nFigure 2.  The Neural Information Processing Systems conference brought together \nresearchers from many fields of science and engineering. The first Conference was held at the \nDenver Tech Center in 1987 and has been held annually since then.  The first few meetings were \nsponsored by the IEEE Information Theory Society.   \n \nFigure 3.  Early perceptrons were large-scale analog systems (4).  (A) An analog perceptron \ncomputer receiving a visual input.  The racks contained potentiometers driven by motors whose \nresistance was controlled by the perceptron learning algorithm (B) Article in the New York \nTimes, July 8, 1958, from a UPI wire report. The perceptron machine was expected to cost \n$100,000 on completion in 1959, or around $1 million in today’s dollars; the IBM 704 computer \nthat cost $2 million in 1958, or $20 million in today’s dollars, could perform 12,000 multiplies \nper second, which was blazingly fast at the time. The much less expensive Samsung Galaxy S6 \nphone, which can perform 34 billion operations per second, is more than a million times faster. \nPhoto courtesy of George Nagy.  \n \nFigure 4.  Nature has optimized birds for energy efficiency.  (A)  The curved feathers at the \nwingtips of an eagle boosts energy efficiency during gliding.  (B)  Winglets on a commercial jets \nsave fuel by reducing drag from vortices.  \n \n \n14 \n \nFigure 5.  Levels of investigation of brains.  Energy efficiency is achieved by signaling with \nsmall numbers of molecules at synapses.  Interconnects between neurons in the brain are three \ndimensional.  Connectivity is high locally, but relatively sparse between distant cortical areas.  \nThe organizing principle in the cortex is based on multiple maps of sensory and motor surfaces \nin a hierarchy.  The cortex coordinates with many subcortical areas to form the central nervous \nsystem (CNS) that generates behavior (Adapted from The Computational Brain, Churchland, P. \nand Sejnowski, T., MIT Press, 1992). \n \nFigure 6.  Engraving from Camille Flammarion's 1888 book L'atmosphère: météorologie \npopulaire (\"The Atmosphere: Popular Meteorology,\") Paris: Hachette. p. 163.  The caption that \naccompanies the engraving in Flammarion's book reads: “A missionary of the Middle Ages tells \nthat he had found the point where the sky and the Earth touch …” \n \n \n15 \n \n  \nReferences \n \n1. Abbott, E. A. (1884) Flatland: A Romance in Many Dimensions. Seeley & Co.: London  \n2.  Charles Howard Hinton, https://en.wikipedia.org/wiki/Charles_Howard_Hinton \n3.  Breiman, L., (2001) Statistical Modeling: The Two Cultures, Statistical Science, 16, No. 3, \n199–231. \n4.  Chomsky, N. (1986) Knowledge of Language: Its Nature, Origins, and Use (Convergence). \nPraeger: Westport, CT. \n5.  Sejnowski, T. J., The Deep Learning Revolution, Cambridge, MA:  MIT Press (2018). \n6.  Rosenblatt, F. (1961) “Principles of Neurodynamics: Perceptrons and the Theory of Brain \nMechanics”, Cornell Aeronautical Lab Inc Buffalo, NY, vol. VG-1196-G, pp. 621.    \n7.  McCulloch, W. S., and Pitts, W. H. (1943) “A Logical Calculus of the Ideas Immanent in \nNervous Activity,” Bulletin of Mathematical Biophysics 5: 115–133. \n8.  Minsky, M., and Papert, S. (1969) Perceptrons (Cambridge, MA: MIT Press) \n9.  Ackley, D. H., Hinton, G. E., and  Sejnowski, T. J., (1985) A learning algorithm for \nBoltzmann Machines, Cognitive Science  9: 147-169. \n10.  Sejnowski, T. J. (1999) The Book of Hebb, Neuron, 24, 773-776. \n11.  Rumelhart, D. E., Hinton, G. E. and Williams, R. J. (1986) Learning representations by \nback-propagating errors Nature 323: 533–536  \n12.  Pascanu, R., Dauphin, Y. N., Ganguli, S., Bengio, Y. (2014) On the saddle point problem for \nnon-convex optimization, arXiv:1405.4604    \n13.  Bartlett, P. L., Long, P. M. Lugosi, G. Tsigler, A. (2019) Benign Overfitting in Linear \nRegression, arXiv:1906.11300  \n14.  Poggio, T., Banburski, A., Liao, Q. (2109) Theoretical Issues in Deep Networks: \nApproximation, Optimization and Generalization, arXiv:1908.09375 \n15.  Mikolov, T. , Sutskever, I., Chen, K., Corrado, G., and Dean, J. (2013) “Distributed \nRepresentations of Words and Phrases and Their Compositionality,” Advance in Neural \nInformation Processing Systems. \n16.  McCullough, D. (2015) The Wright Brothers (New York, NY: Simon & Schuster.) \n16 \n \n17.  NavlakhaS.  and Bar-Joseph, Z. (2011) “Algorithms in Nature: The Convergence of Systems \nBiology and Computational Thinking,” Molecular Systems Biology 7: 546. \n18.  Laughlin, S. B., and Sejnowski, T. J., (2003) Communication in neuronal networks, Science \n301, 1870-1874. \n19.  Zhang, K., and Sejnowski, T. J., (2000) A universal scaling law between gray matter and \nwhite matter of cerebral cortex, Proceedings of the National Academy of Sciences U.S.A. \n97(10), 5621-5626. \n20.  Srinivasan S, Stevens C. (2019) Scaling Principles of Distributed Circuits.  Current Biology \n29: 2533-2540. \n21.  Gary Anthes, G. (2019) Lifelong Learning in Artificial Neural Networks, Communications \nof the ACM, 62: 13-15. \n22.  Muller, L., Piantoni, S., Koller, D., Cash, S. S., Halgren, E., Sejnowski, T. J. (2016). \nRotating waves during human sleep spindles organize global patterns of activity during the \nnight, eLife, e17267.    \n23.  Todorova, R., Zugaro, M. (2019) Isolated cortical computations during delta waves support \nmemory consolidation, Science 366: 377-381 \n24.  Churchland, P. S., Conscience: The Origins of Moral Intuition (2019) W. W. Norton: New \nYork  \n25.  AlphaGo versus Ke Jie, Wikipedia, https://en.wikipedia.org/wiki/AlphaGo_versus_Ke_Jie  \nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, \nL., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., Hassabis, D. (2018). A general \nreinforcement learning algorithm that masters chess, shogi, and go through self-play. Science. \n362: 1140–1144 \n26.  Montague, P. R., Dayan, P. and Sejnowski, T. J., (1996) A framework for mesencephalic \ndopamine systems based on predictive Hebbian learning, Journal of Neuroscience 16, 1936-\n1947. \n27.  Glimcher; P. W., Camerer, C., Poldrack, R. A., Fehr, E., (2008) Neuroeconomics: Decision \nMaking and the Brain, Academic Press: New York,. \n28.  Marder, E. (2012). Neuromodulation of Neuronal Circuits: Back to the Future. Neuron. 7: \n1–11. \n29.  Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., \nPlappert, M., Powell, G., Ribas, R., Schneider, J., Tezak, N., Tworek, J., Welinder, P., Weng, \nL., Yuan, Q., Zaremba, W., Zhang, L.  (2019) Solving Rubik's Cube with a Robot Hand, \narXiv:1910.07113 \n17 \n \n30.  du Lac, S., Raymond, J.L., Sejnowski, T.J., Lisberger, S.G. (1995) Learning and memory in \nthe vestibulo-ocular reflex., Annual Reviews Neuroscience 18: 409-41. \n31.  Nakahira, Y., Liu, Q., Sejnowski, T. J., Doyle, J. C. (2019) Fitts' Law for speed-accuracy \ntrade-off describes a diversity-enabled sweet spot in sensorimotor control. arXiv:1906.00905 \n32.  Nakahira, Y., Liu, Q., Sejnowski, T. J., Doyle, J. C. (2019) Diversity-enabled sweet spots in \nlayered architectures and speed-accuracy trade-offs in sensorimotor control  arXiv:1909.08601 \n33.  Graves, A. Wayne, G., Danihelka, I. (2015) Neural Turing Machines, arXiv:1410.540  \n34.  Rouditchenko, A., Zhao, H., Gan, C., McDermott, J., Torralba, A. (2109) Self-Supervised \nAudio-Visual Co-Segmentation, arXiv:1904.09013  \n35.  Schaal, S., (1999) Is imitation learning the route to humanoid robots? Trends in Cognitive \nSciences 3 (6), 233-242 \n36.  Hinton, G. E., Osindero, S. and Teh, Y. (2006) A fast learning algorithm for deep belief nets. \nNeural Computation 18: 1527-1554 \n37.  Goodfellow, I. J. , Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \nCourville, A., Bengio, Y. “Generative Adversarial Nets,” Advances in Neural Information \nProcessing Systems , 2014. https://arxiv.org/pdf/1406.2661.pdf \n38.  Wigner, E. P. (1960). \"The unreasonable effectiveness of mathematics in the natural \nsciences. Richard Courant lecture in mathematical sciences delivered at New York University, \nMay 11, 1959\". Communications on Pure and Applied Mathematics. 13: 1–14.  \n \n \n \n \n \n \n18 \n \n \n \nFigure 1. Cover of the 1884 edition of Flatland: A Romance in Many Dimensions by Edwin \nA. Abbott (1).  Inhabitants were two-dimensional shapes, with their rank in society determined \nby the number of sides. \n \n19 \n \n \n \nFigure 2.  The Neural Information Processing Systems conference brought together \nresearchers from many fields of science and engineering. The first Conference was held at the \nDenver Tech Center in 1987 and has been held annually since then.  The first few meetings were \nsponsored by the IEEE Information Theory Society.   \n \n20 \n \n \n \n. \n \n \nFigure 3.  Early perceptrons were large-scale analog systems (4).  \n(Above) An analog perceptron computer receiving a visual input.  \nThe racks contained potentiometers driven by motors whose \nresistance was controlled by the perceptron learning algorithm. (from \nRef 6) . (Right) Article in the New York Times, July 8, 1958, from a \nUPI wire report. The perceptron machine was expected to cost \n$100,000 on completion in 1959, or around $1 million in today’s \ndollars; the IBM 704 computer that cost $2 million in 1958, or $20 \nmillion in today’s dollars, could perform 12,000 multiplies per \nsecond, which was blazingly fast at the time. The much less \nexpensive Samsung Galaxy S6 phone, which can perform 34 billion \noperations per second, is more than a million times faste. \n \n21 \n \n \n \nFigure 4.  Nature has optimized birds for energy efficiency.  (Top)  The curved feathers at the \nwingtips of an eagle boosts energy efficiency during gliding.  (Bottom)  Winglets on a \ncommercial jets save fuel by reducing drag from vortices.  \n \n \n22 \n \n \n  \nFigure 5.  Levels of investigation of brains.  Energy efficiency is achieved by signaling with \nsmall numbers of molecules at synapses.  Interconnects between neurons in the brain are three \ndimensional.  Connectivity is high locally, but relatively sparse between distant cortical areas.  \nThe organizing principle in the cortex is based on multiple maps of sensory and motor surfaces \nin a hierarchy.  The cortex coordinates with many subcortical areas to form the central nervous \nsystem (CNS) that generates behavior (Adapted from The Computational Brain, Churchland, P. \nand Sejnowski, T., MIT Press, 1992). \n \n23 \n \n \n \n \nFigure 6.  Engraving from Camille Flammarion's 1888 book L'atmosphère: météorologie \npopulaire (\"The Atmosphere: Popular Meteorology,\") Paris: Hachette. p. 163.  The caption that \naccompanies the engraving in Flammarion's book reads: “A missionary of the Middle Ages tells \nthat he had found the point where the sky and the Earth touch …”  Creative Commons License:  \nhttps://commons.wikimedia.org/wiki/File:Flammarion_Colored.jpg \n \n  \n \n",
  "categories": [
    "q-bio.NC",
    "cs.AI",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2020-02-12",
  "updated": "2020-02-12"
}