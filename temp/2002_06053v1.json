{
  "id": "http://arxiv.org/abs/2002.06053v1",
  "title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery",
  "authors": [
    "Hakime Öztürk",
    "Arzucan Özgür",
    "Philippe Schwaller",
    "Teodoro Laino",
    "Elif Ozkirimli"
  ],
  "abstract": "Text-based representations of chemicals and proteins can be thought of as\nunstructured languages codified by humans to describe domain-specific\nknowledge. Advances in natural language processing (NLP) methodologies in the\nprocessing of spoken languages accelerated the application of NLP to elucidate\nhidden knowledge in textual representations of these biochemical entities and\nthen use it to construct models to predict molecular properties or to design\nnovel molecules. This review outlines the impact made by these advances on drug\ndiscovery and aims to further the dialogue between medicinal chemists and\ncomputer scientists.",
  "text": "Exploring Chemical Space using Natural Language\nProcessing Methodologies for Drug Discovery\nHakime ¨Ozt¨urka, Arzucan ¨Ozg¨ura, Philippe Schwallerb, Teodoro Lainob,∗, Elif\nOzkirimlic,d,∗\naDepartment of Computer Engineering, Bogazici University, Istanbul, Turkey\nbIBM Research, Zurich, Switzerland\ncDepartment of Chemical Engineering, Bogazici University, Istanbul, Turkey\ndDepartment of Biochemistry, University of Zurich, Winterthurerstrasse 190, CH-8057\nZurich, Switzerland\nAbstract\nText based representations of chemicals and proteins can be thought of as un-\nstructured languages codiﬁed by humans to describe domain speciﬁc knowledge.\nAdvances in natural language processing (NLP) methodologies in the process-\ning of spoken languages accelerated the application of NLP to elucidate hidden\nknowledge in textual representations of these biochemical entities and then use it\nto construct models to predict molecular properties or to design novel molecules.\nThis review outlines the impact made by these advances on drug discovery and\naims to further the dialogue between medicinal chemists and computer scien-\ntists.\nTeaser. The application of natural language processing methodologies to an-\nalyze text based representations of molecular structures opens new doors in\ndeciphering the information rich domain of biochemistry toward the discovery\nand design of novel drugs.\nKeywords:\nNatural Language Processing, Machine Translation, Molecule\nGeneration, Drug Discovery, Cheminformatics, Bioinformatics, Biochemical\nLanguages, SMILES\n∗Corresponding author\nEmail addresses: teo@zurich.ibm.com (Teodoro Laino ), elif.ozkirimli@boun.edu.tr\n(Elif Ozkirimli ), +41 76 349 7471 (Elif Ozkirimli )\nPreprint submitted to Elsevier\nFebruary 17, 2020\narXiv:2002.06053v1  [q-bio.BM]  10 Feb 2020\n1. Introduction\nThe design and discovery of novel drugs for protein targets is powered by\nan understanding of the underlying principles of protein-compound interaction.\nBiochemical methods that measure aﬃnity and biophysical methods that de-\nscribe the interaction in atomistic level detail have provided valuable informa-\ntion toward a mechanistic explanation for bimolecular recognition [1]. However,\nmore often than not, compounds with drug potential are discovered serendipi-\ntously or by phenotypic drug discovery [2] since this highly speciﬁc interaction\nis still diﬃcult to predict [3]. Protein structure based computational strategies\nsuch as docking [4], ultra-large library docking for discovering new chemotypes\n[5], and molecular dynamics simulations [4] or ligand based strategies such as\nquantitative structure-activity relationship (QSAR) [6, 7], and molecular sim-\nilarity [8] have been powerful at narrowing down the list of compounds to be\ntested experimentally. With the increase in available data, machine learning and\ndeep learning architectures are also starting to play a signiﬁcant role in chem-\ninformatics and drug discovery [9]. These approaches often require extensive\ncomputational resources or they are limited by the availability of 3D informa-\ntion. On the other hand, text based representations of biochemical entities are\nmore readily available as evidenced by the 19,588 biomolecular complexes (3D\nstructures) in PDB-Bind [10] (accessed on Nov 13, 2019) compared with 561,356\n(manually annotated and reviewed) protein sequences in Uniprot [11] (accessed\non Nov 13, 2019) or 97 million compounds in Pubchem [12] (accessed on Nov\n13, 2019). The advances in natural language processing (NLP) methodologies\nmake processing of text based representations of biomolecules an area of intense\nresearch interest.\nThe discipline of natural language processing (NLP) comprises a variety of\nmethods that explore a large amount of textual data in order to bring unstruc-\ntured, latent (or hidden) knowledge to the fore [13].\nAdvances in this ﬁeld\nare beneﬁcial for tasks that use language (textual data) to build insight. The\n2\nlanguages in the domains of bioinformatics and cheminformatics can be investi-\ngated under three categories: (i) natural language (mostly English) that is used\nin documents such as scientiﬁc publications, patents, and web pages, (ii) domain\nspeciﬁc language, codiﬁed by a systematic set of rules extracted from empirical\ndata and describing the human understanding of that domain (e.g. proteins,\nchemicals, etc), and (iii) structured forms such as tables, ontologies, knowledge\ngraphs or databases [14]. Processing and extracting information from textual\ndata written in natural languages is one of the major application areas of NLP\nmethodologies in the biomedical domain (also known as BioNLP). Information\nextracted with BioNLP methods is most often shared in structured databases\nor knowledge graphs [15]. We refer the reader to the comprehensive review on\nBioNLP by Krallinger et al. [16]. Here, we will be focusing on the application\nof NLP to domain speciﬁc, unstructured biochemical textual representations\ntoward exploration of chemical space in drug discovery eﬀorts.\nWe can view the textual representation of biomedical/biochemical entities\nas a domain-speciﬁc language. For instance, a genome sequence is an exten-\nsive script of four characters (A, T, G, C) constituting a genomic language. In\nproteins, the composition of 20 diﬀerent natural amino acids in varying lengths\nbuilds the protein sequences. Post-translational modiﬁcations expand this 20\nletter alphabet and confer diﬀerent properties to proteins [17]. For chemicals\nthere are several text based alternatives such as chemical formula, IUPAC Inter-\nnational Chemical Identiﬁer (InChI) [18] and Simpliﬁed Molecular Input Line\nEntry Speciﬁcation (SMILES) [19].\nToday, the era of “big data” boosts the “learning” aspect of computational\napproaches substantially, with the ever-growing amounts of information pro-\nvided by publicly available databases such as PubChem [12], ChEMBL [20],\nUniProt [11]. These databases are rich in biochemical domain knowledge that\nis in textual form, thus building an eﬃcient environment in which NLP-based\ntechniques can thrive. Furthermore, advances in computational power allow the\ndesign of more complex methodologies, which in turn drive the ﬁelds of machine\nlearning (ML) and NLP. However, biological and chemical interpretability and\n3\nexplainability remain among the major challenges of AI-based approaches. Data\nmanagement in terms of access, interoperability and reusability are also critical\nfor the development of NLP models that can be shared across disciplines.\nWith this review, we aim to provide an outline of how the ﬁeld of NLP\nhas inﬂuenced the studies in bioinformatics and cheminformatics and the im-\npact it has had over the last decade. Not only are NLP methodologies facil-\nitating processing and exploitation of biochemical text, they also promise an\n“understanding” of biochemical language to elucidate the underlying principles\nof bimolecular recognition. NLP technologies are enhancing the biological and\nchemical knowledge with the ﬁnal goal of accelerating drug discovery for improv-\ning human health. We highlight the signiﬁcance of an interdisciplinary approach\nthat integrates computer science and natural sciences.\n1.1. NLP Basics\nChowdhury [21] describes NLP on three levels: (i) the word level in which\nthe smallest meaningful unit is extracted to deﬁne the morphological structure,\n(ii) the sentence level where grammar and syntactic validity are determined, and\n(iii) the domain or context level in which the sentences have global meaning.\nSimilarly, our review is organized in three parts in which bio-chemical data is\ninvestigated at: (i) word level, (ii) sentence (text) level, and (iii) understand-\ning text and generating meaningful sequences. Table 1 summarizes important\nNLP concepts related to the processing of biochemical data. We refer to these\nconcepts and explain their applications in the following sections.\nAll NLP technology relates to speciﬁc AI architectures. In Table 2 W-we\nsummarize the main ML and deep learning (DL) architectures that will be\nmentioned throughout the review.\n2. Biochemical Language Processing\nThe language-like properties of text-based representations of chemicals were\nrecognized more than 50 years ago by Garﬁeld [22]. He proposed a “chemico-\nlinguistic” approach to representing chemical nomenclature with the aim of\n4\ninstructing the computer to draw chemical diagrams.\nProtein sequence has\nbeen an important source of information about protein structure and function\nsince Anﬁnsen’s experiment [23]. Alignment algorithms, such as Needleman-\nWunsh [24] and Smith-Waterman [25], rely on sequence information to identify\nfunctionally or structurally critical elements of proteins (or genes).\nTo make predictions about the structure and function of compounds or pro-\nteins, the understanding of these sequences is critical for bioinformatics tasks\nwith the ﬁnal goal of accelerating drug discovery. Much like a linguist who uses\nthe tools of language to bring out hidden knowledge, biochemical sequences\ncan be processed to propose novel solutions, such as predicting interactions be-\ntween chemicals and proteins or generating new compounds based on the level\nof understanding. In this section, we will review the applications of some of\nthe NLP-concepts to biochemical data in order to solve bio/cheminformatics\nproblems.\n2.1. Textual Chemical Data\nInformation about chemicals can be found in repositories such as PubChem\n[12], which includes information on around 100 million compounds, or Drugbank\n[26], which includes information on around 10,000 drugs.\nThe main textual\nsources used in drug discovery are textual representations of chemicals and\nproteins. Table 3 lists some sources that store diﬀerent types of biochemical\ninformation.\nChemical structures can be represented in diﬀerent forms that can be one-\ndimensional (1D), 2D, and 3D. Table 4 depicts diﬀerent identiﬁers/representations\nof the drug ampicillin. While the 2D and 3D representations are also used in ML\nbased approaches [9], here we focus on the 1D form, which is the representation\ncommonly used in NLP.\nIUPAC name. The International Union of Pure and Applied Chemistry (IU-\nPAC) scheme (i.e. nomenclature) is used to name compounds following pre-\ndeﬁned rules such that the names of the compounds are unique and consistent\nwith each other (iupac.org/).\n5\nChemical Formula. The chemical formula is one of the simplest and most widely-\nknown ways of describing chemicals using letters (i.e. element symbols), num-\nbers, parentheses, and (-/+) signs. This representation gives information about\nwhich elements and how many of them are present in the compound.\nSMILES. The Simpliﬁed Molecular Input Entry Speciﬁcation (SMILES) is a\ntext-based form of describing molecular structures and reactions [19]. SMILES\nstrings can be obtained by traversing the 2D graph representation of the com-\npound and therefore SMILES provides more complex information than the\nchemical formula. Moreover, due to its textual form, SMILES takes 50% to\n70% less space than other representation methods such as an identical connec-\ntion table (daylight.com/dayhtml/doc/theory/theory.smiles.html).\nSMILES notation is similar to a language with its own set of rules. Just\nlike it is possible to express the same concept with diﬀerent words in natural\nlanguages, the SMILES notation allows molecules to be represented with more\nthan one unique SMILES. Although this may sound like a signiﬁcant ambigu-\nity, the possibility of using diﬀerent SMILES to represent the same molecule\nwas successfully adopted as a data augmentation strategy by various groups\n(Bjerrum [27], Kimber et al. [28], Schwaller et al. [29]).\nCanonical SMILES can provide a unique SMILES representation.\nHow-\never, diﬀerent databases such as PubChem and ChEMBL might use diﬀerent\ncanonicalization algorithms to generate diﬀerent unique SMILES. OpenSMILES\n(opensmiles.org/opensmiles.html) is a new platform that aims to universal-\nize the SMILES notation. In isomeric SMILES, isotopism and stereochemistry\ninformation of a molecule is encoded using a variety of symbols (“/”, “\\”, “@”,\n“@@”).\nDeepSMILES. DeepSMILES is a novel SMILES-like notation that was proposed\nto address two challenges of the SMILES syntax: (i) unbalanced parentheses and\n(ii) ring closure pairs [30]. It was initially designed to enhance machine/deep-\nlearning based approaches that utilize SMILES data as input (github.com/\nnextmovesoftware/deepsmiles). DeepSMILES was adopted in a drug-target\n6\nbinding aﬃnity prediction task in which the ﬁndings highlighted the eﬃcacy of\nDeepSMILES over SMILES in terms of identifying undetectable patterns [31].\nDeepSMILES was also utilized in a molecule generation task in which it was\ncompared to canonical and randomized SMILES text [32]. Here, the results\nsuggested that DeepSMILES might limit the learning ability of the SMILES-\nbased molecule generation models because its syntax is more grammar sensitive\nwith the ring closure alteration and the use of a single symbol for branching\n(i.e. “)”) introducing longer sequences.\nSELFIES. SELF-referencIng Embedding Strings (SELFIES) is an alternative\nsequence-based representation that is built upon “semantically constrained graphs”\n[33].\nEach symbol in a SELFIES sequence indicates a recursive Chomsky-2\ntype grammar, and can thus be used to convert the sequence representation to\na unique graph. SELFIES utilize SMILES syntax to extract words that will\ncorrespond to semantically valid graphs (github.com/aspuru-guzik-group/\nselfies). Krenn et al. [33] compared SELFIES, DeepSMILES and SMILES\nrepresentations in terms of validity in cases where random character mutations\nare introduced. The evaluations on the QM9 dataset yielded results in the favor\nof SELFIES.\nInChI. InChI is the IUPAC International Chemical Identiﬁer, which is a non-\nproprietary and open-source structural representation (inchi-trust.org) [34].\nThe InChIKey is a character-based representation that is generated by hashing\nthe InChI strings in order to shorten them. InChi representation has several\nlayers (each) separated by the “/” symbol.\nThe software that generates InChi is publicly available and InChi does not\nsuﬀer from ambiguity problems. However, its less complex structure makes the\nSMILES representation easier to use as shown in a molecular generation study\n[35] and in building meaningful chemical representations with a translation-\nbased system [36]. Interestingly, the translation model was able to translate\nfrom InChi to canonical SMILES, whereas it failed to translate from canonical\n7\nSMILES to InChi. Winter et al. [36] suggested that the complex syntax of InChi\nmade it diﬃcult for the model to generate a correct sequence.\nSMARTS. SMiles ARbitrary Target Speciﬁcation (SMARTS) is a language that\ncontains specialized symbols and logic operators that enable substructure (pat-\ntern) search on SMILES strings [37]. SMARTS can be used in any task that re-\nquires pattern matching on a SMILES string such as, querying databases or cre-\nating rule dictionaries such as RECAP [38] and BRICS [39] to extract fragments\nfrom SMILES (daylight.com/dayhtml/doc/theory/theory.smarts.html).\nSMIRKS. SMIRKS notation can be used to describe generic reactions (also\nknown as transforms) that comprise one or more changes in atoms and bonds\n(https://daylight.com/daycgi_tutorials/smirks_examples.html). These\ntransforms are based on “reactant to product” notation, and thus make use of\nSMILES and SMARTS languages. SMIRKS is utilized in tasks such as con-\nstructing an online transform database [40] and predicting metabolic trans-\nformations [41]. A recent study achieves a similar performance to rule-based\nsystems in classifying chemical reactions by learning directly from SMILES text\nwith transforms via neural networks [42].\n2.2. Identiﬁcation of Words/Tokens\nSimilar to words in natural languages, we can assume that the “words” of\nbiochemical sequences convey signiﬁcant information (e.g. folding, function etc)\nabout the entities. In this regard, each compound/protein is analogous to a sen-\ntence, and each compound/protein unit is analogous to a word. Therefore, if we\ncan decipher the grammar of biochemical languages, it would be easier to model\nbio/cheminformatics problems. However, protein and chemical words are not\nexplicitly known and diﬀerent approaches are needed to extract syntactically\nand semantically meaningful biochemical word units from these textual infor-\nmation sources (i.e. sequences). Here, we review some of the most common\ntokenization approaches used to determine the words of biochemical languages.\n8\nk-mers (n-grams). One of the simplest approaches in NLP to extract a small\nlanguage unit is to use k-mers, also known as n-grams.\nk-mers indicate k\nconsecutive overlapping characters that are extracted from the sequence with\na sliding window approach.\n“LINGO”, which is one of the earliest applica-\ntions of k-mers in cheminformatics, is the name of the overlapping 4-mers that\nare extracted from SMILES strings [43]. 4-mers of the SMILES of ampicillin,\n“CC1(C(N2C(S1)C(C2=O)NC(=O)C(C3=CC=CC=C3)N)C(=O)O)C”, can be\nlisted as { ‘CC1(’, ‘C1(C’, ‘1(C(’, ..., ‘O)O)’, ‘)O)C’ }. From a sequence of\nlength l, a total of (l −n) + 1 k-mers can be extracted. Extracting LINGOs\nfrom SMILES is a simple yet powerful idea that has been successfully used to\ncompute molecular similarities, to diﬀerentiate between bioisosteric and ran-\ndom molecular pairs [43] and in a drug-target interaction prediction task [44],\nwithout requiring 2D or 3D information. The results suggested that a SMILES-\nbased approach to compute the similarity of chemicals is not only as good as a\n2D-based similarity measurement, but also faster [44].\nk-mers were successfully utilized as protein [45] and chemical words [46] in\nprotein family classiﬁcation tasks. 3-mers to 5-mers were often considered as\nthe words of the protein sequence. Motomura et al. [47] reported that some 5-\nmers could be matched to motifs and protein words are most likely a mixture of\ndiﬀerent k-mers. For the protein function prediction task, Cao et al. [48] decided\nto choose among the 1000 most frequent words to build the protein vocabulary,\nwhereas Ranjan et al. [49] utilized each k-mer type separately and showed that\n4-mers provided the best performance. In the latter work, instead of using the\nwhole protein sequence, the words were extracted from diﬀerent length protein\nsegments, which are also long k-mers (i.e. 100-mer, 120-mer) with 30 amino-\nacid gaps. The use of segmented protein sequences yielded better results than\nusing the whole protein sequence, and important and conserved subsequences\nwere highlighted. k-mers were also used as features, along with position speciﬁc\nscore matrix features, in the protein fold prediction problem [50].\n9\nLongest Common Subsequences. The identiﬁcation of the longest common sub-\nsequence (LCS) of two sequences is critical for detecting their similarity. When\nthere are multiple sequences, LCSs can point to informative patterns. LCSs ex-\ntracted from SMILES sequences performed similarly well to 4-mers in chemical\nsimilarity calculation [44].\nMaximum Common Substructure. Cadeddu et al. [51] investigated organic chem-\nistry as a language in an interesting study that extracts maximum common\nsubstructures (MCS) from the 2D structures of pairs of compounds to build a\nvocabulary of the molecule corpus. Contrary to the common idea of functional\ngroups (e.g. methyl, ethyl etc.) being “words” of the chemical language, the\nauthors argued that MCSs (i.e. fragments) can be described as the words of\nthe chemical language [51]. A recent work investigated the distribution of these\nwords in diﬀerent molecule subsets [52]. The “words” followed Zipf’s Law, which\nindicates the relationship between the frequency of a word and its rank (based\non the frequency) [53], similar to most natural languages. Their results also\nshowed that drug “words” are shorter compared to natural product “words”.\nMinimum Description Length. Minimum Description Length (MDL) is an un-\nsupervised compression-based word segmentation technique in which words of\nan unknown language are detected by compressing the text corpus. In a protein\nclassiﬁcation task, each protein was assigned to the family in which its sequence\nis compressed the most, according to the MDL-based representation [54]. Gane-\nsan et al. [54] investigated whether the MDL-based words of the proteins show\nsimilarities to PROSITE patterns [55] and showed that less conserved residues\nwere compressed less by the algorithm. Ganesan et al. [54] also emphasized\nthat the integration of domain knowledge, such as the consideration of the hy-\ndrophilic and hydrophobic aminoacids in the words (i.e. grammar building),\nmight prove eﬀective.\nByte-Pair Encoding. Byte-Pair Encoding (BPE) generates words based on high\nfrequency subsequences starting from frequent characters [56]. A recent study\n10\nadopted a linguistic-inspired approach to predict protein-protein interactions\n(PPIs) [57]. Their model was built upon “words” (i.e. bio-words) of the protein\nlanguage, in which BPE was utilized to build the bio-word vocabulary. Wang\net al. [57] suggested that BPE-segmented words indicate a language-like behav-\nior for the protein sequences and reported improved accuracy results compared\nto using 3-mers as words.\nPattern-based words. Subsequences that are conserved throughout evolution\nare usually associated with protein structure and function. These conserved\nsequences can be detected as patterns via multiple sequence alignment (MSA)\ntechniques and Hidden Markov Models (HMM). PROSITE [55], a public database\nthat provides information on domains and motifs of proteins, uses regular ex-\npressions (i.e. RE or regex) to match these subsequences.\nProtein domains have been investigated for their potential of being the words\nof the protein language. One earlier study suggested that folded domains could\nbe considered as “phrases/clauses” rather than “words” because of the higher\nsemantic complexity between them [58]. Later, domains were described as the\nwords, and domain architectures as sentences of the language [59, 60]. Protein\ndomains were treated as the words of multi-domain proteins in order to evalu-\nate the semantic meaning behind the domains [61]. The study supported prior\nwork by Yu et al. [60] suggesting that domains displayed syntactic and seman-\ntic features, but there are only a few multi-domain proteins with more than\nsix domains limiting the use of domains as words to build sentences. Protein\ndomains and motifs have also been utilized as words in diﬀerent drug discovery\ntasks such as the prediction of drug-target interaction aﬃnity [62, 63]. These\nstudies showed that motifs and domains together contribute to the prediction\nas much as the use of the full protein sequence.\nSMARTS is a well-known regex-based querying language that is used to\nidentify patterns in a SMILES string. SMARTS has been utilized to build spe-\nciﬁc rules for small-molecule protonation [64], to design novel ligands based on\nthe fragments connected to the active site of a target [65], and to help generate\n11\nproducts in reaction prediction [66]. MolBlocks, a molecular fragmentation tool,\nalso adopted SMARTS dictionaries to partition a SMILES string into overlap-\nping fragments [37]. Furthermore, MACCS [67] and PubChem [12] Fingerprints\n(FP) are molecular descriptors that are described as binary vectors based on the\nabsence/presence of substructures that are predeﬁned with SMARTS language.\nA recent study on protein family clustering uses a ligand-centric representa-\ntion to describe proteins in which ligands were represented with SMILES-based\n(i.e. 8-mers) representation, MACCS and Extended Connectivity Fingerprint\n(ECFP6) [46]. The results indicate that three of the ligand representation ap-\nproaches provide similar performances for protein family clustering.\nTo the best of our knowledge, there is no comprehensive evaluation of the\ndiﬀerent word extraction techniques except a comparison by Wang et al. [57] of\nthe performance of BPE-based words against k-mers in a PPI prediction task.\nSuch comparison would provide important insights to the bio/cheminformatics\ncommunity.\n2.3. Text representation\nThe representation of a text (e.g. molecule or protein sequence) aims to\ncapture syntactic, semantic or relational meaning. In the widely used Vector\nSpace Model (VSM), a text is represented by a feature vector of either weighted\nor un-weighted terms [68]. The terms of this vector may correspond to words,\nphrases, k-grams, characters, or dimensions in a semantic space such as in the\ndistributed word embedding representation models. The similarity between two\ntexts represented in the vector space model is usually computed using the cosine\nsimilarity metric [69], which corresponds to the cosine of the angle between the\ntwo vectors.\nSimilarly to the one-hot encoding scheme [70], in the traditional bag-of-\nwords [71] and term frequency-inverse document frequency (TF-IDF) [72] text\nrepresentation models, each word corresponds to a diﬀerent dimension in the\nvector space. Therefore, the similarity between two words in the vector space is\nzero, even if they are synonymous or related to each other. In the distributed\n12\nrepresentation models [73] on the other hand, words are represented as dense\nvectors based on their context. Words that occur in similar contexts have similar\nvector representations. In this subsection, we review these commonly used text\nrepresentation models with their applications in cheminformatics.\nBag-of-words representation. In this representation model, a text is represented\nas a vector of bag-of-words, where the multiplicity of the words is taken into ac-\ncount, but the order of the words in the text is lost [71]. For instance, the\nSMILES of ampicillin “CC1(C(N2C(S1)C(C2=O)NC(=O)C(\nC3=CC=CC=C3)N)C(=O)O)C” can be represented as a bag-of 8-mers as fol-\nlows: {“CC1(C(N2”, “C1(C(N2C”, “1(C(N2C(”, “(C(N2C(S”,...,“N)C(=O)O”\n,“)C(=O)O)” ,“C(=O)O)C” }. We can vectorize it as S = [1, 1, 1, 1, ..., 1, 1, 1]\nin which each number refers to the frequency of the corresponding 8-mer.\nBag-of-words representation was used in molecular similarity computation,\nin which the SMILES string and the LINGOs extracted from it were treated\nas the sentence and words, respectively [43]. The unique LINGOs were consid-\nered for each pair and a Tanimoto coeﬃcient was used to measure the similarity\n[43]. Another approach called SMILES Fingerprint (SMIfp) also adopted bag-of-\nwords to create representations of molecules for a ligand-based virtual screening\ntask [74]. SMIfp considered 34 unique symbols in SMILES strings to create a\nfrequency-based vector representation, which was utilized to compute molecu-\nlar similarity. SMIfp provided comparable results to a chemical representation\ntechnique that also incorporated polar group and topological information, as\nwell as atom and bond information, in recovering active compounds amongst\ndecoys [74].\nTF-IDF. The bag-of-words model, which is based on counting the terms of\nthe sentence/document, might prioritize insigniﬁcant but frequent words. To\novercome this issue, a weighting scheme can be integrated into the vector repre-\nsentation in order to give more importance to the rare terms that might play a\nkey role in detecting similarity between two documents. One popular weighting\napproach is to use term frequency-inverse document frequency (TF-IDF) [72].\n13\nTF refers to the frequency of a term in the document, and IDF denotes the loga-\nrithm of the total number of documents over the number of documents in which\nthe term appears. IDF is therefore an indicator of uniqueness. For instance,\nthe IDF of “C3=CC=CC” is lower than that of “(C(N2C(S”, which appears in\nfewer compounds. Therefore, the existence of “(C(N2C(S” in a compound may\nbe more informative.\nTF-IDF weigthing was utilized to assign weights to LINGOs that were ex-\ntracted from SMILES in order to compute molecule similarity using cosine sim-\nilarity [44]. Molecular similarities were then used as input for drug-target in-\nteraction prediction. A similar performance between TF-IDF weighted LINGO\nand a graph-based chemical similarity measurement was obtained. Cadeddu\net al. [51] used TF-IDF weighting on chemical bonds to show that bonds with\nhigher TF-IDF scores have a higher probability of breaking.\nOne-hot representation. In one-hot representation, for a given vocabulary of a\ntext, each unique word/character is represented with a binary vector that has\na 1 in the corresponding position, while the vector positions for the remaining\nwords/characters are ﬁlled with 0s [70]. One-hot encoding is fast to build, but\nmight lead to sparse vectors with large dimensions based on the size of the vo-\ncabulary (e.g. one million unique words in the vocabulary means one million\ndimensional binary vectors ﬁlled with zeros except one). It is a popular choice,\nespecially in machine learning-based bio/cheminformatic studies to encode dif-\nferent types of information such as SMILES characters [75, 76], atom/bond\ntypes [77, 78] and molecular properties [79].\nDistributed representations. The one-hot encoding builds discrete representa-\ntions, and thus does not consider the relationships between words. For instance,\nthe cosine similarity of two diﬀerent words is 0 even if they are semantically sim-\nilar. However, if the word (i.e. 8-mer) “(C(N2C(S” frequently appears together\nwith the word “C(C2=O)N” in SMILES strings, this might suggest that they\nhave related “meanings”. Furthermore, two words might have similar semantic\n14\nmeanings even though they are syntactically apart. This is where distributed\nvector representations come into play.\nThe distributed word embeddings models gained popularity with the intro-\nduction of Word2Vec [73] and GloVe [80].\nThe main motivation behind the\nWord2Vec model is to build real-valued high-dimensional vectors for each word\nin the vocabulary based on the context in which they appear. There are two\nmain approaches in Word2Vec: (i) Skip-Gram and (ii) Continuous Bag of Words\n(CBOW). The aim of the Skip-Gram model is to predict context words given the\ncenter word, whereas in CBOW the objective is to predict the target word given\nthe context words. Figure 1 depicts the Skip-gram architecture in Word2Vec\n[73]. For the vocabulary of size V , given the target word “2C(S”, the model\nlearns to predict two context words. Both target word and context words are\nrepresented as one-hot encoded binary vectors of size V . The number of neurons\nin the hidden layer determines the size of the embedding vectors. The weight\nmatrix between the input layer and the hidden layer stores the embeddings of\nthe vocabulary words. The ith row of the embedding matrix corresponds to the\nembedding of the ith word.\nThe Word2Vec architecture has inspired a great deal of research in the\nbio/cheminformatics domains. The Word2Vec algorithm has been successfully\napplied for determining protein classes [45] and protein-protein interactions\n(PPI) [57]. Asgari and Mofrad [45] treated 3-mers as the words of the protein\nsequence and observed that 3-mers with similar biophysical and biochemical\nproperties clustered together when their embeddings were mapped onto the 2D\nspace. Wang et al. [57], on the other hand, utilized BPE-based word segmen-\ntation (i.e. bio-words) to determine the words. The authors argued that the\nimproved performance for bio-words in the PPI prediction task might be due\nto the segmentation-based model providing more distinct words than k-mers,\nwhich include repetitive segments. Another recent study treated multi-domain\nproteins as sentences in which each domain was recognized as a word [61]. The\nWord2Vec algorithm was trained on the domains (i.e. PFAM domain identiﬁers)\nof eukaryotic protein sequences to learn semantically interpretable representa-\n15\ntions of them. The domain representations were then investigated in terms of\nthe Gene Ontology (GO) annotations that they inherit. The results indicated\nthat semantically similar domains share similar GO terms.\nThe Word2Vec algorithm was also utilized for representation of chemicals.\nSMILESVec, a text-based ligand representation technique, utilized Word2Vec\nto learn embeddings for 8-mers (i.e. chemical words) that are extracted from\nSMILES strings [46]. SMILESVec was utilized in protein representation such\nthat proteins were represented as the average of the SMILESVec vectors of their\ninteracting ligands. The results indicated comparable performances for ligand-\nbased and sequence based protein representations in protein family/superfamily\nclustering. Mol2Vec [81], on the other hand, was based on the identiﬁers of the\nsubstructures (i.e. words of the chemical) that were extracted via Extended\nConnectivity Fingerprint (ECFP) [82].\nThe results showed a better perfor-\nmance with Mol2Vec than with the simple Morgan Fingerprint in a solubility\nprediction task, and a comparable performance to graph-based chemical repre-\nsentation [83]. Chakravarti [84] also employed the Word2vec model that was\ntrained on the fragments that are extracted from SMILES strings using a graph\ntraversing algorithm. The results favored the distributed fragment-based lig-\nand representation over fragment-based binary vector representation in a ring\nsystem clustering task and showed a comparable performance in the predic-\ntion of toxicity against Tetrahymena [84]. Figure 2 illustrates the pipeline of a\ntext-based molecule representation based on k-mers.\nFP2Vec is another method that utilizes embedding representation for molecules,\nhowever instead of the Word2Vec algorithm, it depends on a Convolutional\nNeural Network (CNN) to build molecule representations to be used in toxic-\nity prediction tasks [85]. CNN architectures have also been utilized for drug-\ntarget binding aﬃnity prediction [86] and drug-drug interaction prediction [76]\nto build representations for chemicals from raw SMILES strings, as well as for\nprotein fold prediction [87] to learn representations for proteins from amino-\nacid sequences. SMILES2Vec adopted diﬀerent DL architectures (GRU, LSTM,\nCNN+GRU, and CNN+LSTM) to learn molecule embeddings, which were then\n16\nused to predict toxicity, aﬃnity and solubility [88]. A CNN+GRU combination\nwas better at the prediction of chemical properties. A recent study compared\nseveral DL approaches to investigate the eﬀect of diﬀerent chemical representa-\ntions, which were learned through these architectures, on a chemical property\nprediction problem [89]. The authors also combined DL architectures that were\ntrained on SMILES strings with the MACCS ﬁngerprint, proposing a combined\nrepresentation for molecules (i.e. CheMixNet). The CheMixNet representation\noutperformed the other representations that were trained on a single data type\nsuch as SMILES2Vec (i.e. SMILES) and Chemception (i.e. 2D graph) [90].\n2.4. Text generation\nText generation is a primary NLP task, where the aim is to generate gram-\nmatically and semantically correct text, with many applications ranging from\nquestion answering to machine translation [91]. It is generally formulated as a\nlanguage modeling task, where a statistical model is trained using a large cor-\npus to predict the distribution of the next word in a given context. In machine\ntranslation, the generated text is the translation of an input text in another\nlanguage.\nMedicinal chemistry campaigns use methods such as scaﬀold hopping [92] or\nfragment-based drug design [4] to build and test novel molecules but the chemo-\ntype diversity and novelty may be limited. It is possible to explore uncharted\nchemical space with text generation models, which learn a distribution from the\navailable data (i.e. SMILES language) and generate novel molecules that share\nsimilar physicochemical properties with the existing molecules [75]. Molecule\ngeneration can then be followed by assessing physicochemical properties of the\ngenerated compound or its binding potential to a target protein [75]. For a com-\nprehensive review of molecule generation methodologies, including graph-based\nmodels, we refer the reader to the review of Elton et al. [93]. Machine transla-\ntion models have also been recently adapted to text-based molecule generation,\nwhich start with one “language” such as that of reactants and generate a novel\ntext in another “language” such as that of products [29]. Below, we present\n17\nrecent studies on text based molecule generation.\nRNN models, which learn a probability distribution from a training set of\nmolecules, are commonly used in molecule generation to propose novel molecules\nsimilar to the ones in the training data set. For instance, given the SMILES\nsequence “C(=O”, the model would predict the next character to be “)” with a\nhigher probability than “(”. The production of valid SMILES strings, however,\nis a challenge because of the complicated SMILES syntax that utilizes paren-\ntheses to indicate branches and ring numbers. The sequential nature of RNNs,\nwhich may miss long range dependencies, is a disadvantage of these models\n[75]. RNN descendants LSTM and GRU, which model long-term dependencies,\nare better suited for remembering matching rings and branch closures. Moti-\nvated by such a hypothesis, Segler et al. [75] and Ertl et al. [94] successfully\npioneered de novo molecule generation using LSTM architecture to generate\nvalid novel SMILES. Segler et al. [75] further modiﬁed their model to generate\ntarget-speciﬁc molecules by integrating a target bioactivity prediction step to\nﬁlter out inactive molecules and then retraining the LSTM network. In another\nstudy, transfer learning was adopted to ﬁne-tune an LSTM-based SMILES gen-\neration model so that structurally similar leads were generated for targets with\nfew known ligands [95]. Olivecrona et al. [96] and Popova et al. [97] used re-\ninforcement learning (RL) to bias their model toward compounds with desired\nproperties. Merk et al. [98, 99] ﬁne-tuned their LSTM model on a target-focused\nlibrary of active molecules and synthesized some novel compounds. Ar´us-Pous\net al. [100] explored how much of the GDB-13 database [101] they could redis-\ncover by using an RNN-based generative model.\nThe variational Auto-encoder (VAE) is another widely adopted text gener-\nation architecture [102]. G´omez-Bombarelli et al. [35] adopted this architecture\nfor molecule generation.\nA traditional auto-encoder encodes the input into\nthe latent space, which is then decoded to reconstruct the input. VAE diﬀers\nfrom AE by explicitly deﬁning a probability distribution on the latent space to\ngenerate new samples. G´omez-Bombarelli et al. [35] hypothesized that the vari-\national part of the system integrates noise to the encoder, so that the decoder\n18\ncan be more robust to the large diversity of molecules. However, the authors\nalso reported that the non-context free property of SMILES caused by match-\ning ring numbers and parentheses might often lead the decoder to generate\ninvalid SMILES strings. A grammar variational auto-encoder (GVAE), where\nthe grammar for SMILES is explicitly deﬁned instead of the auto-encoder learn-\ning the grammar itself, was proposed to address this issue [103]. This way, the\ngeneration is based on the pre-deﬁned grammar rules and the decoding process\ngenerates grammar production rules that should also be grammatically valid.\nAlthough syntactic validity would be ensured, the molecules may not have se-\nmantic validity (chemical validity). Dai et al. [104] built upon the VAE [35] and\nGVAE [103] architectures and introduced a syntax-directed variational autoen-\ncoder (SD-VAE) model for the molecular generation task. The syntax-direct\ngenerative mechanism in the decoder contributed to creating both syntactically\nand semantically valid SMILES sequences. Dai et al. [104] compared the la-\ntent representations of molecules generated by VAE, GVAE, and SD-VAE, and\nshowed that SD-VAE provided better discriminative features for druglikeness.\nBlaschke et al. [105] proposed an adversarial AE for the same task. Conditional\nVAEs [106, 107] were trained to generate molecules conditioned on a desired\nproperty. The challenges that SMILES syntax presents inspired the introduc-\ntion of new syntax such as DeepSMILES [30] and SELFIES [33] (details in\nSection 2.1).\nGenerative Adversarial Network (GAN) models generate novel molecules by\nusing two components: the generator network generates novel molecules, and\nthe discriminator network aims to distinguish between the generated molecules\nand real molecules [108]. In text generation models, the novel molecules are\ndrawn from a distribution, which are then ﬁne-tuned to obtain speciﬁc features,\nwhereas adversarial learning utilizes generator and discriminator networks to\nproduce novel molecules [108, 109].\nORGAN [109], a molecular generation\nmethodology, was built upon a sequence generative adversarial network (Se-\nqGAN) from NLP [110]. ORGAN integrated RL in order to generate molecules\nwith desirable properties such as solubility, druglikeness, and synthetizability\n19\nthrough using domain-speciﬁc rewards [109].\nMachine Translation. Machine translation ﬁnds use in cheminformatics in “trans-\nlation” from one language (e.g. reactants) to another (e.g. products). Machine\ntranslation is a challenging task because the syntactic and semantic dependen-\ncies of each language diﬀer from one another and this may give rise to ambi-\nguities. Neural Machine Translation (NMT) models beneﬁt from the potential\nof deep learning architectures to build a statistical model that aims to ﬁnd the\nmost probable target sequence for an input sequence by learning from a corpus\nof examples [111, 112]. The main advantage of NMT models is that they provide\nan end-to-end system that utilizes a single neural network to convert the source\nsequence into the target sequence. Sutskever et al. [111] refer to their model as\na sequence-to-sequence (seq2seq) system that addresses a major limitation of\nDNNs that can only work with ﬁxed-dimensionality information as input and\noutput. However, in the machine translation task, the length of the input se-\nquences is not ﬁxed, and the length of the output sequences is not known in\nadvance.\nThe NMT models are based on an encoder-decoder architecture that aims\nto maximize the probability of generating the target sequence (i.e. most likely\ncorrect translation) for the given source sequence. The ﬁrst encoder-decoder ar-\nchitectures in NMT performed poorly as the sequence length increased mainly\nbecause the encoder mapped the source sequence into a single ﬁxed-length vec-\ntor.\nHowever, ﬁxed-size representation may be too small to encode all the\ninformation required to translate long sequences [113]. To overcome the issue\nof the ﬁxed context vector (Figure 3a), a new method was developed, in which\nevery source token was encoded into a memory bank independently (Figure 3b).\nThe decoder could then selectively focus on parts of this memory bank during\ntranslation [113, 114]. This technique is known as “attention mechanism” [115].\nInspired by the successes in NMT, the ﬁrst application of seq2seq models\nin cheminformatics was for reaction prediction by Nam and Kim [116], who\nproposed to translate the SMILES strings of reactants and separated reagents\n20\nto the corresponding product SMILES. The authors hypothesized that the re-\naction prediction problem can be re-modelled as a translation system in which\nboth inputs and output are sequences. Their model used GRUs for the encoder-\ndecoder and a Bahdanau [113] attention layer in between. Liu et al. [117] in\ncontrast, performed the opposite task, the single-step retrosynthesis prediction,\nusing a similar encoder-decoder model. When given a product and a reaction\nclass, their model predicted the reactants that would react together to form that\nproduct. One major challenge in the retrosynthesis prediction task is the possi-\nbility of multiple correct targets, because more than one reactant combination\ncould lead to the same product. Similarly to Nam and Kim [116], Schwaller\net al. [118] also adopted a seq2seq model to translate precursors into products,\nutilizing the SMILES representation for the reaction prediction problem. Their\nmodel used a diﬀerent attention mechanism by Luong et al. [114] and LSTMs\nin the encoder and decoder. By visualizing the attention weights, an atom-wise\nmapping between the product and the reactants could be obtained and used to\nunderstand the predictions better. Schwaller et al. [118] showed that seq2seq\nmodels could compete with graph neural network-based models in the reaction\nprediction task [119].\nA translation model was also employed to learn a data-driven representation\nof molecules [36]. Winter et al. [36] translated between two textual representa-\ntions of a chemical, InChi and SMILES, to extract latent representations that\ncan integrate the semantic “meaning” of the molecule. The results indicated a\nstatistically signiﬁcant improvement with the latent representations in a ligand-\nbased virtual screening task against ﬁngerprint methods such as ECFP (i.e.\nMorgan algorithm). NMT architectures were also adopted in a protein func-\ntion prediction task for the ﬁrst time, in which “words” that were extracted\nfrom protein sequences are translated into GO identiﬁers using RNNs as en-\ncoder and decoder [48]. Although exhibiting a comparable performance to the\nstate-of-the-art protein function prediction methods, the authors argued that\nthe performance of the model could be improved by determining more mean-\ningful “words” such as biologically interpretable fragments.\n21\nTransformer is an attention-based encoder-decoder architecture that was in-\ntroduced in NMT by Vaswani et al. [120]. Although similar to previous studies\n[111, 112, 113] in terms of adopting an encoder-decoder architecture, Trans-\nformer diﬀers from the others because it only consists of attention and feed-\nforward layers in the encoder and decoder. As transformers do not contain an\nRNN, positional embeddings are needed to capture order relationships in the\nsequences. Schwaller et al. [29] were the ﬁrst to adopt the Transformer architec-\nture in cheminformatics and designed a Molecular Transformer for the chemical\nreaction prediction task. The Molecular Transformer, which was atom-mapping\nindependent, outperformed the other algorithms (e.g. based on a two-step con-\nvolutional graph neural network [121]) on commonly used benchmark data sets.\nTransformer architecture was also adopted to learn representations for chemicals\nin prediction of drug-target interactions [122] and molecular properties [123] in\nwhich the proposed systems either outperformed the state-of-the-art systems or\nobtained comparable results.\n3. Future Perspectives\nThe increase in the biochemical data available in public databases combined\nwith the advances in computational power and NLP methodologies have given\nrise to a rapid growth in the publication rate in bio/cheminformatics, especially\nthrough pre-print servers. As this interdisciplinary ﬁeld grows, novel opportu-\nnities come hand in hand with novel challenges.\n3.1. Challenges\nThe major challenges that can be observed from investigating these studies\ncan be summarized as follows: (i) the need for universalized benchmarks and\nmetrics, (ii) reproducibility of the published methodologies, (iii) bias in avail-\nable data, and (iv) biological and chemical interpretability/explainability of the\nsolutions.\n22\nBenchmarking. There are several steps in the drug discovery pipeline, from\naﬃnity prediction to the prediction of other chemical properties such as toxic-\nity, and solubility. The use of diﬀerent datasets and diﬀerent evaluation metrics\nmakes the assessment of model performance challenging. Comprehensive bench-\nmarking platforms that can assess the success of diﬀerent tools are still lacking.\nA benchmarking environment rigorously brings together the suitable data sets\nand evaluation methodologies in order to provide a fair comparison between the\navailable tools. Such environments are available for molecule generation task\nfrom MOSES [124] and GuacaMol [125]. MoleculeNet is also a similar attempt\nto build a benchmarking platform for tasks such as prediction of binding aﬃnity\nand toxicity [83].\nReproducibility. Despite the focus on sharing datasets and source codes on pop-\nular software development platforms such as GitHub (github.com) or Zenodo\n(zenodo.org), it is still a challenge to use data or code from other groups. The use\nof FAIR (Findable, Accessible, Interoperable and Reusable) (meta)data princi-\nples can guide the management of scientiﬁc data [126]. Automated workﬂows\nthat are easy to use and do not require programming knowledge encourage the\nﬂow of information from one discipline to the other. Platform-free solutions such\nas Docker (docker.com) in which an image of the source code is saved and can be\nopened without requiring further installation could accelerate the reproduction\nprocess. A recent initiative to provide a uniﬁed-framework for predictive mod-\nels in genomics can quickly be adopted by the medicinal chemistry community\n[127].\nBias in data. The available data has two signiﬁcant sources of bias, one related\nto the limited sampling of chemical space and the other related to the quality\nand reproducibility of the data. The lack of information about some regions\nof the protein/chemical landscape limits the current methodologies to the ex-\nploitation of data rather than full exploration. The data on protein-compound\ninteractions is biased toward some privileged molecules or proteins because the\nprotein targets are related to common diseases or the molecules are similar to\n23\nknown actives. Hence, not all of chemical space is sampled, and chemical space\nis expanded based on the similarity of an active compound to others, which is\nalso referred to as inductive bias [128]. Data about proteins or molecules related\nto rare diseases is limited and inactive molecules are frequently not reported.\nMoreover, some experimental measurements that are not reproducible across\ndiﬀerent labs or conditions limit their reliability [129].\nSieg et al. [130] and\nZhang and Lee [131] have recently discussed the bias factors in dataset compo-\nsition. Zhang and Lee have also addressed the sources of bias in the data and\nproposed to use Bayesian deep learning to quantify uncertainty.\nInterpretability. The black box nature of ML/DL methodologies makes assign-\ning meaning to the results diﬃcult. Explainability of an ML model is especially\ncritical in drug discovery to facilitate the use of these ﬁndings by medicinal\nchemists, who can contribute to the knowledge loop.\nexplainable-AI (XAI)\nis a current challenge that calls for increased interpretability of AI solutions\nfor a given context and includes several factors such as trust, safety, privacy,\nsecurity, fairness and conﬁdence [132]. Explainability is also critical for the do-\nmain experts to assess the reliability of new methodolodogies. Interpretability\nis usually classiﬁed into two categories: post-hoc (i.e. after) and ante-hoc (i.e.\nbefore).\nPost-hoc approaches explain the predictions of the model, whereas\nante-hoc approaches integrate explainability into the model.\nRecent studies\nhave already aimed to map the semantic meaning behind the models onto the\nbiochemical description.\nAn attentive pooling network, a two-way attention\nsystem that extends the attention mechanism by allowing input nodes to be\naware of one another, is one approach that has been employed in drug-target\ninteraction prediction [133]. Preuer et al. [77] showed that mapping activations\nof hidden neurons in feed-forward neural networks to pharmacophores, or link-\ning atom representations computed by convolutional ﬁlters to substructures in\na graph-convolution model, are possible ways of integrating explainability into\nAI-based drug discovery systems. Bradshaw et al. [134] also demonstrated a\nnovel approach that combines molecule generation and retrosynthesis predic-\n24\ntion to generate synthesizable molecules. Integration of such solutions to drug\ndiscovery problems will not only be useful for computational researchers but\nalso for the medicinal chemistry community.\n3.2. Opportunities\nThe NLP ﬁeld has seen tremendous advances in the past ﬁve years, start-\ning with the introduction of distributed word embedding algorithms such as\nWord2Vec [73] and Glove [80].\nThe concept of contextualized word embed-\ndings (i.e. ELMo) was introduced soon after [135]. Here, the embedding of\nthe word is not ﬁxed, but changes according to the context (i.e. sentence) in\nwhich it appears. These advances continued with more complicated architec-\ntures such as Transformer (i.e.\nGenerative Pre-Training or GPT) [136] and\nBERT [137], RoBERTa [138], GPT2 [139], Transformer-XL [140], and XLNet\n[141] models. Such models with a focus on context might have signiﬁcant im-\npact not only on drug discovery, but also on the protein folding problem, which\nis critical for predicting structural properties of the protein partner. Secondary\nstructure [142, 143, 144], domain boundary [145] and fold [50] prediction studies\noften use sequence information in combination with similarity to available struc-\ntures. The recent success of AlphaFold [146] in Critical Assessment of Protein\nStructure Prediction (CASP) competitions (http://predictioncenter.org/)\nshowed that the enhanced deﬁnitions of context, brought about by the advances\nin machine/deep learning systems, might be useful for capturing the global de-\npendencies in protein sequences to detect interactions between residues sepa-\nrated in sequence space but close together in 3D space [142].\nUnsupervised learning can be used on “big” textual data through using lan-\nguage models with attention [120] and using pre-trained checkpoints from lan-\nguage models [147]. Encoder-decoder architectures have also had signiﬁcant im-\npact on solving text generation and machine translation problems and were suc-\ncessfully applied to molecule generation problem. As NLP moves forward, the\nmost recent approaches such as Topic-Guided VAE [91] and knowledge graphs\nwith graph transformers [148] will easily ﬁnd application in bio/cheminformatics.\n25\nRecent NLP models are not domain-speciﬁc, and they can help with the\ngeneralization of models [139]. Current studies emphasize multi-task learning,\nwhich requires the use of DNNs that share parameters to learn more information\nfrom related but individual tasks [149, 139]. Combined with the transferability\nof contextual word representation models, multi-task learning can also provide\nsolutions to drug discovery which has many interwoven tasks, such as chemical\nproperty prediction and molecule generation.\nLanguage has an important power, not only for daily communication but also\nfor the communication of codiﬁed domain knowledge. Deciphering the mean-\ning behind text is the primary purpose of NLP, which inevitably has found its\nway to bio/cheminformatics. The complicated nature of biochemical text makes\nunderstanding the semantic construction of the hidden words all the more chal-\nlenging and interesting. The applications we discussed in this review provide\na broad perspective of how NLP is already integrated with the processing of\nbiochemical text. A common theme in all of these applications is the use of\nAI-based methodologies that drive and beneﬁt from the NLP ﬁeld. Novel ad-\nvances in NLP and ML are providing auspicious results to solving long-standing\nbio/cheminformatics problems.\nWith this review, we have summarized the impact of NLP on bio/cheminformatics\nto encourage this already interdisciplinary ﬁeld to take advantage of recent ad-\nvances. The communication between researchers from diﬀerent backgrounds and\ndomains can be enhanced through establishing a common vocabulary toward\ncommon goals. This review has been an attempt to facilitate this conversation.\nAcknowledgement\nThis work is partially supported by TUBITAK (The Scientiﬁc and Techno-\nlogical Research Council of Turkey) under grant number 119E133. HO acknowl-\nedges TUBITAK-BIDEB 2211 scholarship program and thanks G¨ok¸ce Uludo˘gan\nfor her comments on ﬁgures. EO thanks Prof. Amedeo Caﬂisch for hosting her\nat the University of Zurich during her sabbatical.\n26\nReferences\n[1] G. Schneider, Automating drug discovery, Nature Reviews Drug Discovery\n17 (2018) 97–113.\n[2] J. G. Moﬀat, F. Vincent, J. A. Lee, J. Eder, M. Prunotto, Opportunities\nand challenges in phenotypic drug discovery: an industry perspective,\nNature reviews Drug discovery 16 (2017) 531.\n[3] Y. Duarte, V. M´arquez-Miranda, M. J. Miossec, F. Gonz´alez-Nilo, Inte-\ngration of target discovery, drug discovery and drug delivery: A review on\ncomputational strategies, Wiley Interdisciplinary Reviews: Nanomedicine\nand Nanobiotechnology (2019) e1554.\n[4] P. ´Sled´z, A. Caﬂisch, Protein structure-based drug design: from docking\nto molecular dynamics, Current opinion in structural biology 48 (2018)\n93–102.\n[5] J. Lyu, S. Wang, T. E. Balius, I. Singh, A. Levit, Y. S. Moroz, M. J.\nOMeara, T. Che, E. Algaa, K. Tolmachova, et al.,\nUltra-large library\ndocking for discovering new chemotypes, Nature 566 (2019) 224.\n[6] P. Schneider, G. Schneider, De novo design at the edge of chaos: Miniper-\nspective, Journal of medicinal chemistry 59 (2016) 4077–4086.\n[7] N. Bosc, F. Atkinson, E. Felix, A. Gaulton, A. Hersey, A. R. Leach, Large\nscale comparison of qsar and conformal prediction methods and their ap-\nplications in drug discovery, Journal of cheminformatics 11 (2019) 4.\n[8] H. Eckert, J. Bajorath, Molecular similarity analysis in virtual screening:\nfoundations, limitations and novel approaches, Drug discovery today 12\n(2007) 225–233.\n[9] Y.-C. Lo, S. E. Rensi, W. Torng, R. B. Altman,\nMachine learning in\nchemoinformatics and drug discovery,\nDrug discovery today 23 (2018)\n1538–1546.\n27\n[10] R. Wang, X. Fang, Y. Lu, C.-Y. Yang, S. Wang, The pdbbind database:\nmethodologies and updates,\nJournal of medicinal chemistry 48 (2005)\n4111–4119.\n[11] R. Apweiler, A. Bairoch, C. H. Wu, W. C. Barker, B. Boeckmann, S. Ferro,\nE. Gasteiger, H. Huang, R. Lopez, M. Magrane, et al.,\nUniprot: the\nuniversal protein knowledgebase, Nucleic acids research 32 (2004) D115–\nD119.\n[12] E. E. Bolton, Y. Wang, P. A. Thiessen, S. H. Bryant, Pubchem: integrated\nplatform of small molecules and biological activities, in: Annual reports\nin computational chemistry, volume 4, Elsevier, 2008, pp. 217–241.\n[13] C. D. Manning, C. D. Manning, H. Sch¨utze, Foundations of statistical\nnatural language processing, MIT press, 1999.\n[14] D. Oliveira, R. Sahay, M. d’Aquin, Leveraging ontologies for knowledge\ngraph schemas (2019).\n[15] P. Ernst, A. Siu, G. Weikum, Knowlife: a versatile approach for construct-\ning a large knowledge graph for biomedical sciences, BMC bioinformatics\n16 (2015) 157.\n[16] M. Krallinger, O. Rabal, A. Lourenco, J. Oyarzabal, A. Valencia, Infor-\nmation retrieval and text mining technologies for chemistry,\nChemical\nreviews 117 (2017) 7673–7761.\n[17] T. M. Karve, A. K. Cheema,\nSmall changes huge impact: the role of\nprotein posttranslational modiﬁcations in cellular homeostasis and disease,\nJournal of amino acids 2011 (2011).\n[18] S. Heller, A. McNaught, S. Stein, D. Tchekhovskoi, I. Pletnev, Inchi-the\nworldwide chemical structure identiﬁer standard, Journal of cheminfor-\nmatics 5 (2013) 7.\n28\n[19] D. Weininger, Smiles, a chemical language and information system. 1.\nintroduction to methodology and encoding rules,\nJournal of chemical\ninformation and computer sciences 28 (1988) 31–36.\n[20] A. Gaulton, L. J. Bellis, A. P. Bento, J. Chambers, M. Davies, A. Hersey,\nY. Light, S. McGlinchey, D. Michalovich, B. Al-Lazikani, et al., Chembl: a\nlarge-scale bioactivity database for drug discovery, Nucleic acids research\n40 (2011) D1100–D1107.\n[21] G. G. Chowdhury, Natural language processing, Annual review of infor-\nmation science and technology 37 (2003) 51–89.\n[22] E. Garﬁeld, Chemico-linguistics: computer translation of chemical nomen-\nclature, Nature 192 (1961) 192.\n[23] C. B. Anﬁnsen, Principles that govern the folding of protein chains, Sci-\nence 181 (1973) 223–230.\n[24] S. B. Needleman, C. D. Wunsch,\nA general method applicable to the\nsearch for similarities in the amino acid sequence of two proteins, Journal\nof molecular biology 48 (1970) 443–453.\n[25] T. F. Smith, M. S. Waterman, et al., Identiﬁcation of common molecular\nsubsequences, Journal of molecular biology 147 (1981) 195–197.\n[26] D. S. Wishart, C. Knox, A. C. Guo, S. Shrivastava, M. Hassanali,\nP. Stothard, Z. Chang, J. Woolsey, Drugbank: a comprehensive resource\nfor in silico drug discovery and exploration,\nNucleic acids research 34\n(2006) D668–D672.\n[27] E. J. Bjerrum, Smiles enumeration as data augmentation for neural net-\nwork modeling of molecules, arXiv preprint arXiv:1703.07076 (2017).\n[28] T. B. Kimber, S. Engelke, I. V. Tetko, E. Bruno, G. Godin, Synergy eﬀect\nbetween convolutional neural networks and the multiplicity of smiles for\n29\nimprovement of molecular prediction,\narXiv preprint arXiv:1812.04439\n(2018).\n[29] P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter, C. Bekas,\nA. A. Lee,\nMolecular transformer: A model for uncertainty-calibrated\nchemical reaction prediction, ACS Central Science (2019).\n[30] N. O’Boyle, A. Dalke, Deepsmiles: An adaptation of smiles for use in\nmachine-learning of chemical structures (2018).\n[31] H. ¨Ozt¨urk, A. ¨Ozg¨ur, E. Ozkirimli, A chemical language based approach\nfor protein-ligand interaction prediction, arXiv preprint arXiv:1811.00761\n(2018).\n[32] J. Ar´us-Pous, S. Johansson, O. Ptykhodko, E. J. Bjerrum, C. Tyrchan,\nJ. Reymond, H. Chen, O. Engkvist, Randomized smiles strings improve\nthe quality of molecular generative models (2019).\n[33] M. Krenn, F. H¨ase, A. Nigam, P. Friederich, A. Aspuru-Guzik, Selﬁes: a\nrobust representation of semantically constrained graphs with an example\napplication in chemistry, arXiv preprint arXiv:1905.13741 (2019).\n[34] S. R. Heller, A. McNaught, I. Pletnev, S. Stein, D. Tchekhovskoi, Inchi,\nthe iupac international chemical identiﬁer, Journal of cheminformatics 7\n(2015) 23.\n[35] R. G´omez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hern´andez-Lobato,\nB. S´anchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel,\nR. P. Adams, A. Aspuru-Guzik, Automatic chemical design using a data-\ndriven continuous representation of molecules,\nACS central science 4\n(2018) 268–276.\n[36] R. Winter, F. Montanari, F. No´e, D.-A. Clevert,\nLearning continuous\nand data-driven molecular descriptors by translating equivalent chemical\nrepresentations, Chemical science 10 (2019) 1692–1701.\n30\n[37] D. Ghersi, M. Singh, molblocks: decomposing small molecule sets and\nuncovering enriched fragments, Bioinformatics 30 (2014) 2081–2083.\n[38] X. Q. Lewell, D. B. Judd, S. P. Watson, M. M. Hann, Recap retrosynthetic\ncombinatorial analysis procedure: a powerful new technique for identifying\nprivileged molecular fragments with useful applications in combinatorial\nchemistry,\nJournal of chemical information and computer sciences 38\n(1998) 511–522.\n[39] J. Degen, C. Wegscheid-Gerlach, A. Zaliani, M. Rarey,\nOn the art of\ncompiling and using’drug-like’chemical fragment spaces, ChemMedChem:\nChemistry Enabling Drug Discovery 3 (2008) 1503–1507.\n[40] S. Avramova, N. Kochev, P. Angelov, Retrotransformdb: A dataset of\ngeneric transforms for retrosynthetic analysis, Data 3 (2018) 14.\n[41] S. Arvidsson, O. Spjuth, L. Carlsson, P. Toccaceli, Prediction of metabolic\ntransformations using cross venn-abers predictors,\nin: Conformal and\nProbabilistic Prediction and Applications, 2017, pp. 118–131.\n[42] P. Schwaller, A. C. Vaucher, V. H. Nair, T. Laino, Data-driven chemical\nreaction classiﬁcation with attention-based neural networks (2019).\n[43] D. Vidal, M. Thormann, M. Pons, Lingo, an eﬃcient holographic text\nbased method to calculate biophysical properties and intermolecular simi-\nlarities, Journal of Chemical Information and Modeling 45 (2005) 386–393.\n[44] H. ¨Ozt¨urk, E. Ozkirimli, A. ¨Ozg¨ur,\nA comparative study of smiles-\nbased compound similarity functions for drug-target interaction predic-\ntion, BMC bioinformatics 17 (2016) 128.\n[45] E. Asgari, M. R. Mofrad, Continuous distributed representation of bio-\nlogical sequences for deep proteomics and genomics, PloS one 10 (2015)\ne0141287.\n31\n[46] H. ¨Ozt¨urk, E. Ozkirimli, A. ¨Ozg¨ur, A novel methodology on distributed\nrepresentations of proteins using their interacting ligands, Bioinformatics\n34 (2018) i295–i303.\n[47] K. Motomura, T. Fujita, M. Tsutsumi, S. Kikuzato, M. Nakamura, J. M.\nOtaki, Word decoding of protein amino acid sequences with availability\nanalysis: a linguistic approach, PloS one 7 (2012) e50039.\n[48] R. Cao, C. Freitas, L. Chan, M. Sun, H. Jiang, Z. Chen, Prolango: protein\nfunction prediction using neural machine translation based on a recurrent\nneural network, Molecules 22 (2017) 1732.\n[49] A. Ranjan, M. S. Fahad, D. Fernandez-Baca, A. Deepak, S. Tripathi, Deep\nrobust framework for protein function prediction using variable-length\nprotein sequences,\nIEEE/ACM transactions on computational biology\nand bioinformatics (2019).\n[50] L. Wei, M. Liao, X. Gao, Q. Zou,\nEnhanced protein fold prediction\nmethod through a novel feature extraction technique, IEEE transactions\non nanobioscience 14 (2015) 649–659.\n[51] A. Cadeddu, E. K. Wylie, J. Jurczak, M. Wampler-Doty, B. A. Grzy-\nbowski, Organic chemistry as a language and the implications of chemical\nlinguistics for structural and retrosynthetic analyses, Angewandte Chemie\nInternational Edition 53 (2014) 8108–8112.\n[52] M. Wo´zniak, A. Wo los, U. Modrzyk, R. L. G´orski, J. Winkowski, M. Ba-\njczyk, S. Szymku´c, B. A. Grzybowski, M. Eder, Linguistic measures of\nchemical diversity and the keywords of molecular collections, Scientiﬁc\nreports 8 (2018).\n[53] G. K. Zipf, Human behavior and the principle of least eﬀort. (1949).\n[54] D. Ganesan, A. V. Tendulkar, S. Chakraborti, Protein word detection\nusing text segmentation techniques, in: BioNLP 2017, 2017, pp. 238–246.\n32\n[55] N. Hulo, A. Bairoch, V. Bulliard, L. Cerutti, E. De Castro, P. S.\nLangendijk-Genevaux, M. Pagni, C. J. Sigrist, The prosite database, Nu-\ncleic acids research 34 (2006) D227–D230.\n[56] R. Sennrich, B. Haddow, A. Birch, Neural machine translation of rare\nwords with subword units, arXiv preprint arXiv:1508.07909 (2015).\n[57] Y. Wang, Z.-H. You, S. Yang, X. Li, T.-H. Jiang, X. Zhou, A high eﬃ-\ncient biological language model for predicting protein–protein interactions,\nCells 8 (2019) 122.\n[58] M. Gimona, Protein linguisticsa grammar for modular protein assembly?,\nNature Reviews Molecular Cell Biology 7 (2006) 68.\n[59] A. Scaiewicz, M. Levitt, The language of the protein universe, Current\nopinion in genetics & development 35 (2015) 50–56.\n[60] L. Yu, D. K. Tanwar, E. D. S. Penha, Y. I. Wolf, E. V. Koonin, M. K.\nBasu,\nGrammar of protein domain architectures,\nProceedings of the\nNational Academy of Sciences 116 (2019) 3636–3645.\n[61] D. Buchan, D. Jones,\nInferring protein domain semantic roles using\nword2vec, bioRxiv (2019) 617647.\n[62] P. Greenside, M. Hillenmeyer, A. Kundaje, Prediction of protein-ligand\ninteractions from paired protein sequence motifs and ligand substructures,\nin: Paciﬁc Symposium on Biocomputing, volume 23, World Scientiﬁc,\n2017.\n[63] H. ¨Ozt¨urk, E. Ozkirimli, A. ¨Ozg¨ur, Widedta: prediction of drug-target\nbinding aﬃnity, arXiv preprint arXiv:1902.04166 (2019).\n[64] P. J. Ropp, J. C. Kaminsky, S. Yablonski, J. D. Durrant, Dimorphite-dl:\nan open-source program for enumerating the ionization states of drug-like\nsmall molecules, Journal of cheminformatics 11 (2019) 14.\n33\n[65] N. Cheron, N. Jasty, E. I. Shakhnovich, Opengrowth: an automated and\nrational algorithm for ﬁnding new protein ligands, Journal of medicinal\nchemistry 59 (2015) 4171–4188.\n[66] J. N. Wei, D. Duvenaud, A. Aspuru-Guzik, Neural networks for the predic-\ntion of organic chemistry reactions, ACS central science 2 (2016) 725–732.\n[67] J. L. Durant, B. A. Leland, D. R. Henry, J. G. Nourse, Reoptimization of\nmdl keys for use in drug discovery, Journal of chemical information and\ncomputer sciences 42 (2002) 1273–1280.\n[68] G. Salton, A. Wong, C.-S. Yang,\nA vector space model for automatic\nindexing, Communications of the ACM 18 (1975) 613–620.\n[69] M. Bilenko, R. J. Mooney, Adaptive duplicate detection using learnable\nstring similarity measures, in: Proceedings of the ninth ACM SIGKDD\ninternational conference on Knowledge discovery and data mining, ACM,\n2003, pp. 39–48.\n[70] C. M. Bishop, Pattern recognition and machine learning, Springer Sci-\nence+ Business Media, 2006.\n[71] P. D. Turney, P. Pantel, From frequency to meaning: Vector space models\nof semantics, Journal of artiﬁcial intelligence research 37 (2010) 141–188.\n[72] K. S. Jones, A statistical interpretation of term speciﬁcity and its appli-\ncation in retrieval, Journal of documentation (2004).\n[73] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed\nrepresentations of words and phrases and their compositionality, in: Ad-\nvances in neural information processing systems, 2013, pp. 3111–3119.\n[74] J. Schwartz, M. Awale, J.-L. Reymond, Smifp (smiles ﬁngerprint) chem-\nical space for virtual screening and visualization of large databases of or-\nganic molecules, Journal of chemical information and modeling 53 (2013)\n1979–1989.\n34\n[75] M. H. Segler, T. Kogej, C. Tyrchan, M. P. Waller, Generating focused\nmolecule libraries for drug discovery with recurrent neural networks, ACS\ncentral science 4 (2018) 120–131.\n[76] S. Kwon, S. Yoon,\nDeepcci:\nEnd-to-end deep learning for chemical-\nchemical interaction prediction, in: Proceedings of the 8th ACM Interna-\ntional Conference on Bioinformatics, Computational Biology, and Health\nInformatics, ACM, 2017, pp. 203–212.\n[77] K. Preuer,\nG. Klambauer,\nF. Rippmann,\nS. Hochreiter,\nT. Un-\nterthiner, Interpretable deep learning in drug discovery, arXiv preprint\narXiv:1903.02788 (2019).\n[78] N. De Cao, T. Kipf,\nMolgan: An implicit generative model for small\nmolecular graphs, arXiv preprint arXiv:1805.11973 (2018).\n[79] A. Mayr, G. Klambauer, T. Unterthiner, S. Hochreiter, Deeptox: toxic-\nity prediction using deep learning, Frontiers in Environmental Science 3\n(2016) 80.\n[80] J. Pennington, R. Socher, C. Manning, Glove: Global vectors for word\nrepresentation, in: Proceedings of the 2014 conference on empirical meth-\nods in natural language processing (EMNLP), 2014, pp. 1532–1543.\n[81] S. Jaeger, S. Fulle, S. Turk,\nMol2vec: Unsupervised machine learning\napproach with chemical intuition, Journal of chemical information and\nmodeling 58 (2018) 27–35.\n[82] D. Rogers, M. Hahn, Extended-connectivity ﬁngerprints, Journal of chem-\nical information and modeling 50 (2010) 742–754.\n[83] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S.\nPappu, K. Leswing, V. Pande, Moleculenet: a benchmark for molecular\nmachine learning, Chemical science 9 (2018) 513–530.\n35\n[84] S. K. Chakravarti, Distributed representation of chemical fragments, ACS\nomega 3 (2018) 2825–2836.\n[85] W. Jeon, D. Kim, Fp2vec: a new molecular featurizer for learning molec-\nular properties, Bioinformatics (2019).\n[86] H. ¨Ozt¨urk, A. ¨Ozg¨ur, E. Ozkirimli, Deepdta: deep drug–target binding\naﬃnity prediction, Bioinformatics 34 (2018) i821–i829.\n[87] J. Hou, B. Adhikari, J. Cheng, Deepsf: deep convolutional neural network\nfor mapping protein sequences to folds, Bioinformatics 34 (2017) 1295–\n1303.\n[88] G. B. Goh, N. O. Hodas, C. Siegel, A. Vishnu,\nSmiles2vec:\nAn in-\nterpretable general-purpose deep neural network for predicting chemical\nproperties, arXiv preprint arXiv:1712.02034 (2017).\n[89] A. Paul, D. Jha, R. Al-Bahrani, W.-k. Liao, A. Choudhary, A. Agrawal,\nChemixnet: Mixed dnn architectures for predicting chemical properties\nusing multiple molecular representations, arXiv preprint arXiv:1811.08283\n(2018).\n[90] G. B. Goh, C. Siegel, A. Vishnu, N. O. Hodas, N. Baker,\nChemcep-\ntion: a deep neural network with minimal chemistry knowledge matches\nthe performance of expert-developed qsar/qspr models,\narXiv preprint\narXiv:1706.06689 (2017).\n[91] W. Wang, Z. Gan, H. Xu, R. Zhang, G. Wang, D. Shen, C. Chen, L. Carin,\nTopic-guided variational autoencoders for text generation, arXiv preprint\narXiv:1903.07137 (2019).\n[92] F. Grisoni, D. Merk, V. Consonni, J. A. Hiss, S. G. Tagliabue, R. Todes-\nchini, G. Schneider, Scaﬀold hopping from natural products to synthetic\nmimetics by holistic molecular similarity, Communications Chemistry 1\n(2018) 44.\n36\n[93] D. C. Elton, Z. Boukouvalas, M. D. Fuge, P. W. Chung, Deep learning\nfor molecular design-a review of the state of the art, Molecular Systems\nDesign & Engineering (2019).\n[94] P. Ertl, R. Lewis, E. Martin, V. Polyakov, In silico generation of novel,\ndrug-like chemical matter using the lstm neural network, arXiv preprint\narXiv:1712.07449 (2017).\n[95] A. Gupta, A. T. M¨uller, B. J. Huisman, J. A. Fuchs, P. Schneider,\nG. Schneider,\nGenerative recurrent networks for de novo drug design,\nMolecular informatics 37 (2018) 1700111.\n[96] M. Olivecrona, T. Blaschke, O. Engkvist, H. Chen, Molecular de-novo\ndesign through deep reinforcement learning, Journal of cheminformatics\n9 (2017) 48.\n[97] M. Popova, O. Isayev, A. Tropsha, Deep reinforcement learning for de\nnovo drug design, Science advances 4 (2018) eaap7885.\n[98] D. Merk, L. Friedrich, F. Grisoni, G. Schneider, De novo design of bioac-\ntive small molecules by artiﬁcial intelligence, Molecular informatics 37\n(2018) 1700153.\n[99] D. Merk, F. Grisoni, L. Friedrich, G. Schneider, Tuning artiﬁcial intelli-\ngence on the de novo design of natural-product-inspired retinoid x receptor\nmodulators, Communications Chemistry 1 (2018) 68.\n[100] J. Ar´us-Pous, T. Blaschke, S. Ulander, J.-L. Reymond, H. Chen, O. En-\ngkvist, Exploring the gdb-13 chemical space using deep generative models,\nJournal of cheminformatics 11 (2019) 20.\n[101] L. C. Blum, J.-L. Reymond,\n970 million druglike small molecules for\nvirtual screening in the chemical universe database gdb-13, Journal of the\nAmerican Chemical Society 131 (2009) 8732–8733.\n37\n[102] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, S. Ben-\ngio,\nGenerating sentences from a continuous space,\narXiv preprint\narXiv:1511.06349 (2015).\n[103] M. J. Kusner, B. Paige, J. M. Hern´andez-Lobato, Grammar variational\nautoencoder,\nin: Proceedings of the 34th International Conference on\nMachine Learning-Volume 70, JMLR. org, 2017, pp. 1945–1954.\n[104] H. Dai, Y. Tian, B. Dai, S. Skiena, L. Song, Syntax-directed variational\nautoencoder for molecule generation, in: Proceedings of the International\nConference on Learning Representations, 2018.\n[105] T. Blaschke, M. Olivecrona, O. Engkvist, J. Bajorath, H. Chen, Appli-\ncation of generative autoencoder in de novo molecular design, Molecular\ninformatics 37 (2018) 1700123.\n[106] J. Lim, S. Ryu, J. W. Kim, W. Y. Kim,\nMolecular generative model\nbased on conditional variational autoencoder for de novo molecular design,\nJournal of cheminformatics 10 (2018) 31.\n[107] S. Kang, K. Cho,\nConditional molecular design with deep generative\nmodels, Journal of chemical information and modeling 59 (2018) 43–52.\n[108] Y. Hong, U. Hwang, J. Yoo, S. Yoon, How generative adversarial networks\nand their variants work: An overview, ACM Computing Surveys (CSUR)\n52 (2019) 10.\n[109] G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias,\nA. Aspuru-Guzik,\nObjective-reinforced generative adversarial networks\n(organ) for sequence generation models, arXiv preprint arXiv:1705.10843\n(2017).\n[110] L. Yu, W. Zhang, J. Wang, Y. Yu, Seqgan: Sequence generative adver-\nsarial nets with policy gradient, in: Thirty-First AAAI Conference on\nArtiﬁcial Intelligence, 2017.\n38\n[111] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learning with\nneural networks, in: Advances in neural information processing systems,\n2014, pp. 3104–3112.\n[112] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, Y. Bengio,\nLearning phrase representations using rnn\nencoder-decoder for statistical machine translation,\narXiv preprint\narXiv:1406.1078 (2014).\n[113] D. Bahdanau, K. Cho, Y. Bengio, Neural machine translation by jointly\nlearning to align and translate, arXiv preprint arXiv:1409.0473 (2014).\n[114] M.-T. Luong, H. Pham, C. D. Manning, Eﬀective approaches to attention-\nbased neural machine translation, arXiv preprint arXiv:1508.04025 (2015).\n[115] A. Graves, Generating sequences with recurrent neural networks, arXiv\npreprint arXiv:1308.0850 (2013).\n[116] J. Nam, J. Kim, Linking the neural machine translation and the prediction\nof organic chemistry reactions, arXiv preprint arXiv:1612.09529 (2016).\n[117] B. Liu, B. Ramsundar, P. Kawthekar, J. Shi, J. Gomes, Q. Luu Nguyen,\nS. Ho, J. Sloane, P. Wender, V. Pande, Retrosynthetic reaction prediction\nusing neural sequence-to-sequence models, ACS central science 3 (2017)\n1103–1113.\n[118] P. Schwaller, T. Gaudin, D. Lanyi, C. Bekas, T. Laino, found in trans-\nlation: predicting outcomes of complex organic chemistry reactions using\nneural sequence-to-sequence models,\nChemical science 9 (2018) 6091–\n6098.\n[119] W. Jin, C. Coley, R. Barzilay, T. Jaakkola, Predicting organic reaction\noutcomes with weisfeiler-lehman network, in: Advances in Neural Infor-\nmation Processing Systems, 2017, pp. 2607–2616.\n39\n[120] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n L. Kaiser, I. Polosukhin, Attention is All you Need (2017) 5998–6008.\n[121] C. W. Coley, W. Jin, L. Rogers, T. F. Jamison, T. S. Jaakkola, W. H.\nGreen, R. Barzilay, K. F. Jensen, A graph-convolutional neural network\nmodel for the prediction of chemical reactivity, Chemical science 10 (2019)\n370–377.\n[122] B. Shin, S. Park, K. Kang, J. C. Ho,\nSelf-attention based molecule\nrepresentation for predicting drug-target interaction,\narXiv preprint\narXiv:1908.06760 (2019).\n[123] S. Wang, Y. Guo, Y. Wang, H. Sun, J. Huang, Smiles-bert: Large scale\nunsupervised pre-training for molecular property prediction,\nin: Pro-\nceedings of the 10th ACM International Conference on Bioinformatics,\nComputational Biology and Health Informatics, ACM, 2019, pp. 429–436.\n[124] D. Polykovskiy,\nA. Zhebrak,\nB. Sanchez-Lengeling,\nS. Golovanov,\nO. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov, V. Aladinskiy,\nM. Veselov, et al.,\nMolecular sets (moses): a benchmarking platform\nfor molecular generation models, arXiv preprint arXiv:1811.12823 (2018).\n[125] N. Brown, M. Fiscato, M. H. Segler, A. C. Vaucher, Guacamol: bench-\nmarking models for de novo molecular design, Journal of chemical infor-\nmation and modeling 59 (2019) 1096–1108.\n[126] M. D. Wilkinson, M. Dumontier, I. J. Aalbersberg, G. Appleton, M. Ax-\nton, A. Baak, N. Blomberg, J.-W. Boiten, L. B. da Silva Santos, P. E.\nBourne, et al., The fair guiding principles for scientiﬁc data management\nand stewardship, Scientiﬁc data 3 (2016).\n[127] ˇZ. Avsec, R. Kreuzhuber, J. Israeli, N. Xu, J. Cheng, A. Shrikumar,\nA. Banerjee, D. S. Kim, T. Beier, L. Urban, et al.,\nThe kipoi reposi-\ntory accelerates community exchange and reuse of predictive models for\ngenomics, Nature biotechnology (2019) 1.\n40\n[128] A. E. Cleves, A. N. Jain,\nEﬀects of inductive bias on computational\nevaluations of ligand-based modeling and on drug discovery, Journal of\ncomputer-aided molecular design 22 (2008) 147–159.\n[129] R. E. Pogue, D. P. Cavalcanti, S. Shanker, R. V. Andrade, L. R. Aguiar,\nJ. L. de Carvalho, F. F. Costa, Rare genetic diseases: update on diagnosis,\ntreatment and online resources, Drug discovery today 23 (2018) 187–195.\n[130] J. Sieg, F. Flachsenberg, M. Rarey, In need of bias control: Evaluating\nchemical data for machine learning in structure-based virtual screening,\nJournal of chemical information and modeling 59 (2019) 947–961.\n[131] Y. Zhang, A. A. Lee, Bayesian semi-supervised learning for uncertainty-\ncalibrated prediction of molecular properties and active learning, arXiv\npreprint arXiv:1902.00925 (2019).\n[132] A. Holzinger, C. Biemann, C. S. Pattichis, D. B. Kell, What do we need\nto build explainable ai systems for the medical domain?, arXiv preprint\narXiv:1712.09923 (2017).\n[133] K. Y. Gao, A. Fokoue, H. Luo, A. Iyengar, S. Dey, P. Zhang, Interpretable\ndrug target prediction using deep neural representation., in: IJCAI, 2018,\npp. 3371–3377.\n[134] J. Bradshaw, B. Paige, M. J. Kusner, M. H. S. Segler, J. M. Hern´andez-\nLobato,\nA model to search for synthesizable molecules,\nCoRR\nabs/1906.05221 (2019).\n[135] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nL. Zettlemoyer, Deep contextualized word representations, arXiv preprint\narXiv:1802.05365 (2018).\n[136] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever,\nImprov-\ning\nlanguage\nunderstanding\nby\ngenerative\npre-training,\nURL\nhttps://s3-us-west-2.\namazonaws.\ncom/openai-assets/research-\ncovers/languageunsupervised/language understanding paper. pdf (2018).\n41\n[137] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep\nbidirectional transformers for language understanding,\narXiv preprint\narXiv:1810.04805 (2018).\n[138] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pre-\ntraining approach, arXiv preprint arXiv:1907.11692 (2019).\n[139] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language\nmodels are unsupervised multitask learners, OpenAI Blog 1 (2019).\n[140] Z. Dai, Z. Yang, Y. Yang, W. W. Cohen, J. Carbonell, Q. V. Le,\nR. Salakhutdinov,\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context, arXiv preprint arXiv:1901.02860 (2019).\n[141] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, Q. V. Le, Xlnet:\nGeneralized autoregressive pretraining for language understanding, arXiv\npreprint arXiv:1906.08237 (2019).\n[142] J. Hanson, K. K. Paliwal, T. Litﬁn, Y. Yang, Y. Zhou, Getting to know\nyour neighbor: protein structure prediction comes of age with contextual\nmachine learning, Journal of Computational Biology (2019).\n[143] X.-J. Zhu, C.-Q. Feng, H.-Y. Lai, W. Chen, L. Hao, Predicting protein\nstructural classes for low-similarity sequences by evaluating diﬀerent fea-\ntures, Knowledge-Based Systems 163 (2019) 787–793.\n[144] S. Wang, J. Peng, J. Ma, J. Xu, Protein secondary structure prediction\nusing deep convolutional neural ﬁelds, Scientiﬁc reports 6 (2016) 18962.\n[145] Q. Shi, W. Chen, S. Huang, F. Jin, Y. Dong, Y. Wang, Z. Xue, Dnn-dom:\npredicting protein domain boundary from sequence alone by deep neural\nnetwork, Bioinformatics (2019).\n[146] R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green, C. Qin, A. Zidek,\nA. Nelson, A. Bridgland, H. Penedones, et al., De novo structure pre-\n42\ndiction with deeplearning based scoring, Annu Rev Biochem 77 (2018)\n6.\n[147] S. Rothe, S. Narayan, A. Severyn, Leveraging Pre-trained Checkpoints\nfor Sequence Generation Tasks, arXiv.org (2019).\n[148] R. Koncel-Kedziorski, D. Bekal, Y. Luan, M. Lapata, H. Hajishirzi,\nText generation from knowledge graphs with graph transformers, arXiv\npreprint arXiv:1904.02342 (2019).\n[149] S. Ruder, Neural Transfer Learning for Natural Language Processing,\nPh.D. thesis, NATIONAL UNIVERSITY OF IRELAND, GALWAY,\n2019.\n[150] X. Yang, J. Zhang, K. Yoshizoe, K. Terayama, K. Tsuda, Chemts: an\neﬃcient python library for de novo molecular generation,\nScience and\ntechnology of advanced materials 18 (2017) 972–976.\n[151] O. Prykhodko, S. Johansson, P.-C. Kotsias, E. J. Bjerrum, O. Engkvist,\nH. Chen,\nA de novo molecular generation method using latent vector\nbased generative adversarial network (2019).\n[152] Y. Bengio, et al., Learning deep architectures for ai, Foundations and\ntrends R⃝in Machine Learning 2 (2009) 1–127.\n[153] Y. LeCun, L. Bottou, Y. Bengio, P. Haﬀner, et al., Gradient-based learn-\ning applied to document recognition, Proceedings of the IEEE 86 (1998)\n2278–2324.\n[154] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.\n[155] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural compu-\ntation 9 (1997) 1735–1780.\n[156] D. P. Kingma, M. Welling,\nAuto-encoding variational bayes,\narXiv\npreprint arXiv:1312.6114 (2013).\n43\n[157] R. S. Sutton, A. G. Barto, Reinforcement learning: An introduction, MIT\npress, 2018.\n[158] S. J. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions on\nknowledge and data engineering 22 (2009) 1345–1359.\n[159] R. J. Williams, D. Zipser, A learning algorithm for continually running\nfully recurrent neural networks, Neural computation 1 (1989) 270–280.\n[160] H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weis-\nsig, I. N. Shindyalov, P. E. Bourne, The protein data bank, Nucleic acids\nresearch 28 (2000) 235–242.\n[161] A. Bateman, L. Coin, R. Durbin, R. D. Finn, V. Hollich, S. Griﬃths-Jones,\nA. Khanna, M. Marshall, S. Moxon, E. L. Sonnhammer, et al., The pfam\nprotein families database, Nucleic acids research 32 (2004) D138–D141.\n[162] T. Liu, Y. Lin, X. Wen, R. N. Jorissen, M. K. Gilson, Bindingdb: a web-\naccessible database of experimentally determined protein–ligand binding\naﬃnities, Nucleic acids research 35 (2006) D198–D201.\n[163] J. J. Irwin, B. K. Shoichet, Zinc- a free database of commercially available\ncompounds for virtual screening,\nJournal of chemical information and\nmodeling 45 (2005) 177–182.\n44\nFigure 1:\nThe illustration of the Skip-Gram architecture of the Word2Vec algo-\nrithm. For a vocabulary of size V, each word in the vocabulary is described as a\none-hot encoded vector (a binary vector in which only the corresponding word\nposition is set to 1). The Skip-Gram architecture is a simple one hidden-layer\nneural network that aims to predict context (neighbor) words of a given target\nword. The extent of the context is determined by the window size parameter.\nIn this example, the window size is equal to 1, indicating that the system will\npredict two context words (the word on the left and the word on the right of\nthe target word) based on their probability scores. The number of nodes in the\nhidden layer (N) controls the size of the embedding vector. The weight matrix\nof VxN stores the trained embedding vectors.\n45\nFigure 2:\n(Continued on the following page.)\n46\nFigure 2: The workﬂow for building a SMILES-based molecule representation.\nIn the ﬁrst box, SMILES text of ampicillin is utilized to extract words. In this\ncase, the words are overlapping 4-mers and there are total 42 unique words.\nTo represent multiple compounds, words are extracted from each compound,\nthus building a vocabulary of size V . In the second box, two popular word rep-\nresentations are illustrated: (left) one-hot encoded representation, and (right)\ndistributed representation. With the one-hot encoding, we build a binary vector\nof size V , in which the position of the corresponding word is set to 1, while the\nrest remains as 0. In the distributed representations, however, the dimension\nof the word representation (embedding) is D, which is usually smaller than V\nand 50 < D < 500. Furthermore, distributed representations are continuous\nvectors. Therefore, the cosine similarity of two distributed word vectors is equal\nto or greater than 0, whereas with one-hot encoded word vectors, their simi-\nlarity is 0 if they are not equal. Finally, the third box demonstrates the text\nlevel representation. The analogy between texts and SMILES strings allows us\nto represent chemicals as groups of “chemical words”. Term-Frequency-Inverse\nDocument Frequency (TF-IDF) weighting, which is a widely adopted weighting\nscheme in Information Retrieval domain, assigns higher weights to rare words.\nIn a corpus with a vocabulary size of V, each word is represented as the mul-\ntiplication of its frequency and IDF values. In the distributed representation\nof texts, since each word also has a D dimensional embedding vector, text rep-\nresentation is computed based on these word embedding vectors, for example\nby taking their average. Dots (.) in the third box represent “chemical words”\nin 2D space, whereas stars (*) represent the whole SMILES (i.e. compound).\nWith both techniques, the compound (i.e. text) representations can be mapped\nto 2D. We expect chemicals such as ampicillin and penicillin, which are from\nthe same antibiotic class, to be close to each other in vector space, whereas\nstreptomycin, an antibiotic from a diﬀerent class, to be distant.\n47\nx0\nx1\nx2\nx3\n…\ny0\ny1\ny2\ny3\n…\ny4\ny1\ny2\ny3\n…\n{\n}\nx0\nx1\nx2\nx3\n…\nBr c\n1\nc\n…\nBr c\n1\nc\n…\ny0\ny1\ny2\ny3\n…\n<s> c\n1\nc\n…\n<s> c\n1\nc\n…\nBr\nN\nN\nOH\nOH\nB\nHO\nN\nN\n+\n+\n+\n+\ny4\ny1\ny2\ny3\n…\nc\nc\n1\nc\n…\nc\nc\n1\nc\n…\nh0\nh =\nT\nh1\nh2\nh3\n…\nencoder\nencoder\ndecoder\ndecoder\nmemory bank\nﬁxed sized\nvector\nEncoder-decoder\nattention\nK, V\nQ\nc\na) seq-2-seq\nb) seq-2-seq with attention \nN\nN\nN\nN\nPd\nFigure 3: (Continued on the following page.)\n48\nFigure 3: Sequence-2-Sequence models take as input a sequence of tokens and\ngenerate a sequence of tokens as output. The example in this Figure is a chemical\nreaction prediction, where given a set of precursors the most likely products\nare predicted. The input tokens correspond to the tokenized SMILES of the\nprecursors and the generated tokens to the SMILES of the product.\nIn the\noriginal sequence-2-sequence models, the encoder encoded the input sequence\ninto a ﬁxed size context vector, as shown in (a). The decoder had access only to\nthis ﬁxed size vector, which limited its application for long input sequences. To\novercome this drawback, the attention mechanism was introduced, as shown in\n(b). In a sequence-2-sequence model with attention, the encoder encodes every\ntoken independently into a memory bank. The longer the input sequence is,\nthe larger is the memory bank. The decoder then queries the memory bank at\nevery decoding step and selectively attends the most relevant value vectors to\npredict the next token.\n49\nTable 1: NLP concepts and their applications in drug discovery\nConcept\nDeﬁnition\nMethodologies\nApplications\nToken/word\nA series of characters (i.e. word, number, symbol)\nthat constitutes the smallest unit of a language.\nThe identiﬁcation of tokens (i.e. tokenization) is an\nimportant pre-processing step in many NLP tasks,\ne.g. substructures of a molecule.\nk-mers\nprotein family classiﬁcation [45, 46]\nprotein function prediction [48, 49]\nprotein language analysis [47]\nmolecular similarity [43, 44]\npatterns\ndrug-target interaction prediction[62, 63]\nprotein language analysis [58, 59, 60, 61]\nmolecule fragmentation [37]\nreaction prediction [66]\nligand design [65]\nMCS\nchemical language analysis [51, 52]\nBPE\nprotein-protein interaction prediction [57]\nMDL\nprotein family classiﬁcation [54]\nSentence\nA text containing one or more tokens/words,\ne.g. textual representations of chemicals and\nproteins.\nSMILES [19]\nmolecular property prediction [89]\nbinding aﬃnity prediction [86, 88]\nreaction prediction [116, 117, 118, 29]\ndata augmentation [27]\nand more.\nDeepSMILES [30]\nbinding aﬃnity prediction [31]\nSELFIES [33]\n-\nprotein sequence\ntoxicity prediction [81]\nprotein family classiﬁcation [45, 54]\nprotein function prediction [48, 49]\nprotein language analysis [47]\nand more.\nWord/sentence\nrepresentation\nThe aim to describe a text that can reﬂect its syntactic\nand semantic features,\ne.g. vector representation of SMILES based on\nthe occurrences of each symbol.\nbag-of-words\nmolecular similarity [43, 44]\ndistributed\nrepresentations\nbinding aﬃnity prediction [86]\nchemical property prediction [81, 84]\ntoxicity prediction [88, 81, 84, 85]\ndrug-drug interaction prediction [76]\nprotein family classiﬁcation [45, 46]\nprotein-protein interaction prediction [57]\nMachine\ntranslation\nThe task of converting a sequence of meaningful\nsymbols in one language into a meaningful sequence\nin another language,\ne.g.translating SMILES to InChi in molecules.\nRNN-based\nseq2seq\nprotein function prediction [48]\nchemical representation [36]\nreaction prediction [116, 118]\nretrosynthesis [117]\nTransformer\nreaction prediction [29]\ndrug-target interaction prediction [122]\nLanguage\ngeneration\nThe aim to generate a sequence of meaningful symbols\nin the given language that are close to real.\ne.g. generating SMILES of a novel lead\nRNN-types\nmolecule generation [75, 94, 95, 96, 150]\nVAE-types\nmolecule generation [35, 103, 104]\nGAN\nmolecule generation [109, 151]\n50\nTable 2: Widely used AI methodologies in NLP-based drug discovery studies\nModel\nDescription\nDeep Neural Network (DNN) [152]\nAn artiﬁcial neural network (ANN) witha large number\nof hidden layers and neurons.\nWord2Vec [73]\nAn ANN-based word embedding architecture that\ncaptures the semantic information of the words based\non the context in which they appear.\nConvolutional Neural Network (CNN) [153]\nA type of ANN that utilizes convolutions in the layers.\nRecurrent Neural Network (RNN) [154]\nA type of ANN that has a feedback loop connected to\nprevious time samples.\nLong-short Term Memory (LSTM) [155]\nA type of RNN that captures long distance dependencies\nand comprises update, forget, and output gates.\nGated Recurrent Unit\nA type of RNN that captures long distance dependencies\nand comprises an update gate.\nAuto-encoder (AE) [154]\nA neural network based architecture that comprises an\nencoder that maps the input in a narrow space and a\ndecoder that reconstructs the compressed representation.\nVariational Auto-encoder (VAE) [156]\nA type of AE that generates outputs based on a speciﬁc\ndistribution.\nGenerative Adversarial Network (GAN) [109]\nA generative model with generator and discriminator\nnetworks.\nSequence-to-sequence (seq2seq)\nAn encoder-decoder based architecture that maps an\ninput sequence into an output sequence.\nAttention mechanism [113]\nenables the model to choose among the important parts\nof a sequence that are relevant to the output.\nTransformer [120]\nAn encoder-decoder architecture that employs\nself-attention and ANNs in encoder and decoder parts.\nNeural Machine Translation (NMT) [113]\nA seq2seq translation architecture.\nReinforcement Learning (RL) [157]\nA ML algorithm in which an agent performs a series of\ndecisions in order to maximize its rewards.\nTransfer Learning [158]\nA methodology to learn a model on a task (or on a large\ndata) and then to adjust (i.e. ﬁne-tune) the learned model\non a diﬀerent task (or on a smaller dataset) with the ﬁnal\ngoal of generalization.\nTeacher Forcing [159]\nA technique that is used in training RNNs such that the\nactual word is given to the decoder as the input instead\nof the output word that is predicted in the previous step.\n51\nTable 3: Commonly used databases in drug discovery\nSource\nAddress\nDescription\nUniProt [11]\nhttps://www.uniprot.org/\nThe Universal Protein Resource: stores protein sequence and function\ninformation.\nPDB [160]\nhttps://www.rcsb.org/\nThe Protein Data Bank: a source of structural information for around\n152,000 macro-molecular structures.\nPFam [161]\nhttps://pfam.xfam.org/\nA protein family database based on multiple sequence alignment (MSA)\nand Hidden Markov Models (HMM).\nPROSITE [55]\nhttps://prosite.expasy.org/\nA database that contains protein domains, motifs, families and functional\nsites.\nPubChem [12]\nhttps://PubChem.ncbi.nlm.nih.gov/\nAn extensive resource for around 96 million compounds and 265 million\nsubstances. PubChem also acts as a cheminformatics tool by providing an\ninterface that enables the computation of 2D/3D similarity of compounds\nand introduces a 1D chemical descriptor.\nChEMBL [20]\nhttps://www.ebi.ac.uk/chembl/\nA widely accessed database that stores manually curated information\nabout protein targets, chemical properties and bioactivities for\n1.9 million compounds.\nDrugBank [26]\nhttps://www.drugbank.ca/\nAn online resource for chemical, pharmacological and pharmaceutical\ninformation for 13K drugs and 5K proteins (e.g. drug targets/enzymes)\nthat are associated with these drugs.\nBindingDB [162]\nhttps://www.bindingdb.org/\nA database of protein and small molecule interactions that stores\nbinding aﬃnities.\nPDB-Bind [10]\nwww.pdbbind.org.cn/\nA public resource for binding aﬃnity data for protein-ligand complexes.\nZINC [163]\nhttps://zinc.docking.org/\nA database of over 230 million commercially-available compounds in\n3D form.\nAll databases were accessed on June 28, 2019.\n52\nTable 4: Diﬀerent representations of the drug ampicillin\nIdentiﬁer\nRepresentation\nIUPAC name\n(2S,5R,6R)-6-[[(2R)-2-amino-2-phenylacetyl]amino]-3,3-\ndimethyl-7-oxo-4-thia-1-azabicyclo[3.2.0]heptane-2-carboxylic acid\nChemical Formula\nC16H19N3O4S\nCanonical SMILES\nCC1(C(N2C(S1)C(C2=O)NC(=O)C(C3=CC=CC=C3)N)C(=O)O)C\nIsomeric SMILES\nCC1([C@@H](N2[C@H](S1)[C@@H](C2=O)NC(=O)[C@@H]\n(C3=CC=CC=C3)N)C(=O)O)C\nDeepSMILES\n(Canonical)\nCCCNCS5)CC4=O))NC=O)CC=CC=CC=C6))))))N)))))))C=O)O)))C\nSELFIES\n(Canonical)\n[C][C][Branch2 3][Ring1][epsilon][C][Branch2 3]\n[epsilon][=O][N][C][Branch1 3][Ring2][S][Ring1][Ring2]\n[C][Branch1 3][Branch1 1][C][Ring1][Ring2][=O][N][C]\n[Branch1 3][epsilon][=O][C][Branch1 3][Branch2 2][C][=C]\n[C][=C][C][=C][Ring1][Branch1 1][N][C][Branch1 3]\n[epsilon][=O][O][C]\nInChi\nInChI=1S/C16H19N3O4S/c1-16(2)11(15(22)23)19-13\n(21)10(14(19)24-16)18-12(20)9(17)8-6-4-3-5-7-8/h3-7\n9-11,14H,17H2,1-2H3,(H,18,20)(H,22,23)/t9-,10-,11+\n14-/m1/s1\nInChi Key\nAVKUERGKIZMTKX-NJBDSQKTSA-N\n2D\n3D\n2D and 3D ﬁgures were generated using MolView (molview.org).\n53\n",
  "categories": [
    "q-bio.BM",
    "cs.CL",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-02-10",
  "updated": "2020-02-10"
}