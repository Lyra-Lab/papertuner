{
  "id": "http://arxiv.org/abs/2002.06262v2",
  "title": "Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? -- A Neural Tangent Kernel Perspective",
  "authors": [
    "Kaixuan Huang",
    "Yuqing Wang",
    "Molei Tao",
    "Tuo Zhao"
  ],
  "abstract": "Deep residual networks (ResNets) have demonstrated better generalization\nperformance than deep feedforward networks (FFNets). However, the theory behind\nsuch a phenomenon is still largely unknown. This paper studies this fundamental\nproblem in deep learning from a so-called \"neural tangent kernel\" perspective.\nSpecifically, we first show that under proper conditions, as the width goes to\ninfinity, training deep ResNets can be viewed as learning reproducing kernel\nfunctions with some kernel function. We then compare the kernel of deep ResNets\nwith that of deep FFNets and discover that the class of functions induced by\nthe kernel of FFNets is asymptotically not learnable, as the depth goes to\ninfinity. In contrast, the class of functions induced by the kernel of ResNets\ndoes not exhibit such degeneracy. Our discovery partially justifies the\nadvantages of deep ResNets over deep FFNets in generalization abilities.\nNumerical results are provided to support our claim.",
  "text": "Why Do Deep Residual Networks Generalize Better\nthan Deep Feedforward Networks? — A Neural\nTangent Kernel Perspective\nKaixuan Huang∗\nPeking University\nhackyhuang@pku.edu.cn\nYuqing Wang∗\nGeorgia Institute of Technology\nywang3398@gatech.edu\nMolei Tao\nGeorgia Institute of Technology\nmtao@gatech.edu\nTuo Zhao\nGeorgia Institute of Technology\ntourzhao@gatech.edu\nAbstract\nDeep residual networks (ResNets) have demonstrated better generalization per-\nformance than deep feedforward networks (FFNets). However, the theory behind\nsuch a phenomenon is still largely unknown. This paper studies this fundamental\nproblem in deep learning from a so-called “neural tangent kernel” perspective.\nSpeciﬁcally, we ﬁrst show that under proper conditions, as the width goes to inﬁn-\nity, training deep ResNets can be viewed as learning reproducing kernel functions\nwith some kernel function. We then compare the kernel of deep ResNets with that\nof deep FFNets and discover that the class of functions induced by the kernel of\nFFNets is asymptotically not learnable, as the depth goes to inﬁnity. In contrast,\nthe class of functions induced by the kernel of ResNets does not exhibit such\ndegeneracy. Our discovery partially justiﬁes the advantages of deep ResNets over\ndeep FFNets in generalization abilities. Numerical results are provided to support\nour claim.\n1\nIntroduction\nDeep Neural Networks (DNNs) have made signiﬁcant progress in a variety of real-world applications,\nsuch as computer vision [1, 2, 3], speech recognition, natural language processing [4, 5, 6], recom-\nmendation systems, etc. Among various network architectures, Residual Networks (ResNets, [7]) are\nundoubtedly a breakthrough. Residual Networks are equipped with residual connections, which skip\nlayers in the forward step. Similar ideas based on gating mechanisms are also adopted in Highway\nNetworks [8], and further inspire many follow-up works such as Densely Connected Networks [9].\nCompared with conventional Feedforward Networks (FFNets), residual networks demonstrate surpris-\ning generalization abilities. Existing literature rarely considers deep feedforward networks with more\nthan 30 layers. This is because many experimental results have suggested that very deep feedforward\nnetworks yield worse generalization performance than their shallow counterparts [7]. In contrast, we\ncan train residual networks with hundreds of layers, and achieve better generalization performance\nthan that of feedforward networks. For example, ResNet-152 [7], achieving a 19.38% top-1 error\non the ImageNet data set, consists of 152 layers; ResNet-1001 [10], achieving a 4.92% error on the\nCIFAR-10 data set, consists of 1000 layers.\nDespite the great success and popularity of the residual networks, the reason why they generalize so\nwell is still largely unknown. There have been several lines of research attempting to demystify this\nphenomenon. One line of research focuses on empirical studies of residual networks, and provides\n∗Equal contribution.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2002.06262v2  [cs.LG]  22 Dec 2020\nintriguing observations. For example, [11] show that residual networks behave like an ensemble of\nweakly dependent networks of much smaller sizes, and meanwhile, they also show that the gradient\nvanishing issue is also signiﬁcantly mitigated due to these smaller networks. [12] further provide\na more reﬁned elaboration on the gradient vanishing issue. They demonstrate that the gradient\nmagnitude in residual networks only shows sublinear decay (with respect to the layer), which is\nmuch slower than the exponential decay of gradient magnitude in feedforward neural networks.\n[13] propose a visualization approach for analyzing the landscape of neural networks, and further\ndemonstrate that residual networks have smoother optimization landscape due to the skip-layer\nconnections.\nAnother line of research focuses on theoretical investigations of residual networks under simpliﬁed\nnetwork architectures. A commonly adopted structure, which is a reformulation of FFNets, is\nxℓ= φ(xℓ−1 + αWℓxℓ−1),\n(1)\nwhere ℓis the number of layers and the skip-connection only bypasses the weight matrix Wℓat each\nlayer [14, 15, 16, 17, 18]. Speciﬁcally, [16] study the optimization landscape with linear activation;\n[17] study using Stochastic Gradient Descent (SGD) to train a two-layer ResNet. [18] study using\nGradient Descent (GD) to train a two-layer non-overlapping residual network. [14, 15] both take the\nperturbation analysis approach to show convergence of such ResNets. A more realistic structure is\nxℓ= xℓ−1 + φ(αWℓxℓ−1),\n(2)\nwhere the skip-connection bypasses the activation function [19, 20]. [20] only consider separable\nsetting and take the perturbation analysis to show the convergence and generalization property of such\nResNet. These results, however, are only loosely related to the generalization abilities of residual\nnetworks, and often considered to be overoptimistic, due to the oversimpliﬁed assumptions.\nSome more recent works provide a new theoretical framework for analyzing overparameterized\nneural networks [21, 22, 23, 24, 14, 25, 26, 27]. They focus on connecting two- or three-layer over-\nparameterized (sufﬁciently wide) neural networks to reproducing kernel Hilbert spaces. Speciﬁcally,\nthey show that under proper conditions, the weight matrices of a well trained overparameterized\nneural network (achieving any given small training error) are actually very close to their initialization.\nAccordingly, the training process can be described as searching within some class of reproducing\nkernel functions, where the associated kernel is called the “neural tangent kernel” (NTK, [21]) and\nonly depends on the initialization of the weights. Accordingly, the generalization properties of the\noverparameterized neural network are equivalent to those of the associated NTK function class. Based\non such a framework, [19] derived the NTK of the ResNet (2) when only the last layer is trained, and\nproved the convergence of such ResNet. However, they did not provide an explicit formula for the\nNTK when all layers are trained, which is required for characterizing the generalization property of\nResNets.\nTo better understand the generalization abilities of deep feedforward and residual networks, we\npropose to investigate the NTKs associated with these networks when all but the last layers are\ntrained, and consider the case when both widths and depths go to inﬁnity2. For the structure of\nResNets, we adopt (2) only with a slight modiﬁcation, since it captures the essence of the skip-\nconnection; see Section 2\nxℓ= xℓ−1 + α\nr\n1\nmVℓσ0\n\u0010r\n2\nmWℓxℓ−1\n\u0011\n.\n(3)\nSpeciﬁcally, we prove that similar to what has been shown for feedforward networks [21], as the\nwidth of deep residual networks increases to inﬁnity, training residual networks can also be viewed as\nlearning reproducing kernel functions with some NTK. However, such an NTK associated with the\nresidual networks exhibits a very different behavior from that of feedforward networks.\nTo demonstrate such a difference, we further consider the regime, where the depths of both feedfor-\nward and residual networks are allowed to increase to inﬁnity. Accordingly, both NTKs associated\nwith deep feedforward and residual networks converge to their limiting forms sublinearly (in terms of\nthe depth). For notational simplicity, we refer to the limiting form of the NTKs as the limiting NTK.\nBesides asymptotic analysis, we also provide nonasymptotic bounds, which demonstrate equivalence\nbetween limiting NTKs and neural networks with sufﬁcient depth and width.\nWhen comparing their limiting NTKs, we ﬁnd that the class of functions induced by the limiting\nNTKs associated with deep feedforward networks is essentially not learnable. Such a class of\n2More precisely, our analysis considers the regime, where the widths go to inﬁnity ﬁrst, and then the depths\ngo to inﬁnity. See more details in Section 4.\n2\nfunctions is sufﬁcient to overﬁt training data. Given any ﬁnite sample size, however, the learned\nfunction cannot generalize. In contrast, the class of functions induced by the limiting NTKs associated\nwith deep residual networks does not exhibit such degeneracy. Our discovery partially justiﬁes the\nadvantages of deep residual networks over deep feedforward networks in terms of generalization\nabilities. Numerical results are provided to support our claim.\nOur work is closely related to [28]. They also investigate the so-called “Gaussian Process” kernel\ninduced by feedforward networks under the regime where the depth is allowed to increase to\ninﬁnity. However, their studied neural networks are essentially some speciﬁc implementations of the\nreproducing kernels using random features, since the training process only updates the last layer of\nthe neural networks, and keeps other layers unchanged. In contrast, we assume the training process\nupdates all layers except for the last layer.\nNotations: We use σ0(z) = max(0, z) to denote the ReLU activation function in neural networks.\nWe use σ(z) to denote the normalized ReLU function σ(z) =\n√\n2 max(0, z). The derivative 3 of\nReLU function (step function) is σ′\n0(z) = I{z≥0}. Then σ′(z) =\n√\n2I{z≥0} is the normalized step\nfunction. We use D to denote the input dimension and SD−1 to denote the unit sphere in RD. We\nuse m to denote the network width (the number of neurons at each layer) and L to denote the depth.\nLet M2\n+ be the set of all 2 × 2 positive semi-deﬁnite matrices. We use F to denote the set of all\nsymmetric and positive semi-deﬁnite functions from RD × RD to R. We use ∥· ∥max to denote the\nentry-wise ℓ∞norm for matrices and use ∥· ∥to denote the ℓ2 norm for vectors and the spectral norm\nfor matrices. We use diag(·) to denote the diagonal matrix. We use In to denote the n × n identity\nmatrix. We use x and ˜x to denote a pair of inputs. We use xℓand ˜xℓto denote the output of the ℓ-th\nlayer of a network for the input x and ˜x, respectively. We use f and ˜f to denote the ﬁnal output of the\nnetwork for x and ˜x, respectively. We use ∇θf = ∇θfθ(x) to denote the derivative of parametrized\nmodel fθ w.r.t. θ at the input x, and ∇θ ˜f to denote the counterpart at the input ˜x.\n2\nBackground\nFor self-containedness, we ﬁrst brieﬂy review feedforward networks, residual networks and dual\nkernels associated with neural networks.\nFeedforward Networks. We deﬁne an L-layer feedforward network (FFNet) f(x) with ReLU\nactivation in a recursive manner,\nx0 = x; xℓ=\nr\n2\nmσ0(Wℓxℓ−1), ℓ= 1, · · · , L; f(x) = v⊤xL,\n(4)\nwhere W1 ∈Rm×D and W2, · · · , WL ∈Rm×m are weight matrices, and v ∈Rm is the output\nweight vector. For simplicity, we only consider feedforward networks with scalar outputs.\nResidual Networks. We deﬁne an L-layer residual network (ResNet) f(x) in a recursive manner,\nx0 =\nr\n1\nmAx; xℓ= xℓ−1 + α\nr\n1\nmVℓσ0\n\u0010r\n2\nmWℓxℓ−1\n\u0011\n, ℓ= 1, · · · , L; f(x) = v⊤xL,\n(5)\nwhere Wℓ, Vℓ∈Rm×m for ℓ= 1, · · · , L, A ∈Rm×D, v ∈Rm, and α = L−γ is the scaling factor\nof the bottleneck layers. The scaling factor α is necessary for controlling the norm of xl.\nThe network architecture in (5) is similar to the “pre-activation\" shortcuts in [10], except that each\nbottleneck layer only contains one activation - between Wℓand Vℓ. We remove the activation of the\ninput due to some technical issues (See more details in Section 3).\nDual and Normalized Kernels. The dual kernel technique was ﬁrst proposed in [29] and motivated\nseveral follow-up works such as [28, 30]. Here we adopt the description in [28]. We use K to denote\na kernel function on the input space RD, i.e., K : RD × RD →R. We denote\nΣ(x, ˜x) =\n\u0012\nK(x, x)\nK(x, ˜x)\nK(˜x, x)\nK(˜x, ˜x)\n\u0013\nand Nρ =\n\u0012\n1\nρ\nρ\n1\n\u0013\n,\nwhere K ∈F, ρ ∈R. Given an activation function φ : R →R, its dual activation function\nˆφ : [−1, 1] →[−1, 1] is deﬁned to be ˆφ(ρ) = E(X, ˜\nX)∼N(0,Nρ)φ(X)φ( ˜X).\nWe then deﬁne the dual kernel as follows.\nDeﬁnition 1. We say that Γφ(K) : RD × RD →R is the dual kernel of K with respect to the\nactivation φ, if we have Γφ(K)(x, ˜x) = E(X, ˜\nX)∼N(0,Σ(x,˜x))φ(X)φ( ˜X).\n3Although the ReLU function σ0 is not differentiable at 0, we call σ′\n0 derivative for notational convenience.\n3\nNote that Γφ(K) is also positive semi-deﬁnite. We also deﬁne the normalized kernel.\nDeﬁnition 2. We say that a kernel K ∈F is normalized, if K(x, x) = 1 for all x ∈RD. For a\ngeneral kernel K ∈F, we deﬁne its normalized kernel by K where K(x, ˜x) =\nK(x,˜x)\n√\nK(x,x)K(˜x,˜x).\nFor normalized ReLU function σ(z) =\n√\n2 max(0, z), [28] show ˆσ(ρ) =\n√\n1−ρ2+(π−cos−1(ρ))ρ\nπ\n.\nSince σ(z) is positive homogeneous, we have Γσ(K)(x, ˜x) =\np\nK(x, x)K(˜x, ˜x) ˆσ(K(x, ˜x)). For\nderivative of normalized ReLU function σ′(z) =\n√\n2I{z≥0}, [28] show that bσ′(ρ) = π−cos−1(ρ)\nπ\n.\nSince σ′(z) is zeroth-order positive homogeneous, we have Γσ′(K)(x, ˜x) = bσ′(K(x, ˜x)). For more\ntechnical details of the dual kernel, we refer the readers to [28].\n3\nNeural Tangent Kernels of Deep Networks\nThere are two approaches to connecting neural networks to kernels: one is Gaussian Process Kernel\n(GP Kernel); the other is Neural Tangent Kernel (NTK). GP Kernel corresponds to the regime where\nthe ﬁrst L layers are ﬁxed after random initialization, and only the last layer is trained. Therefore,\nthe ﬁrst L layers are essentially random feature mapping [31]. This is inconsistent with the practice,\nas the ﬁrst L layers should also be trained. In contrast, NTK corresponds to the regime where the\nﬁrst L layers are also trained. For both GP Kernel and NTK, we consider the case when the width of\nthe neural network goes to inﬁnity. Due to space limit, we only provide some proof sketches for our\ntheory, and all technical details are deferred to the appendix.\n3.1\nFeedforward Networks\nWe consider the Feedforward Network (FFNet) deﬁned in (4), where W1 ∈Rm×D, W2, · · · , WL ∈\nRm×m and v ∈Rm are all initialized as i.i.d. N(0, 1) variables.4 Given such random initialization,\nthe outputs converge to a Gaussian process, as the width goes to inﬁnity [32, 21]. Accordingly, the\nGP kernel is deﬁned as follows.\nProposition 1 ([28, 21]). The GP kernel of the L-layer FFNet deﬁned in (4) is\nK0(x, ˜x) = x⊤˜x; Kℓ(x, ˜x) = Γσ(Kℓ−1)(x, ˜x), ℓ= 1, · · · , L.\n(6)\nTheorem 1 ([28]). For the FFNet deﬁned in (4), there exists an absolute constant C, given the width\nm ≥Cϵ−2L2 log(8L/δ), with probability at least 1 −δ over the randomness of the initialization, for\ninput x, ˜x on the unit sphere, the inner product of the outputs of the ℓ-th layer can be approximated\nby Kℓ(x, ˜x), i.e.,\n|⟨xℓ, ˜xℓ⟩−Kℓ(x, ˜x)| ≤ϵ, for all ℓ= 1, · · · , L.\nThe next proposition shows the NTK of this FFNet. Unlike the GP kernel, the NTK corresponds to\nthe case when θ = (W1, · · · , WL) are trained.\nProposition 2 ([21]). The NTK of the FFNet can be derived in terms of the GP kernels as\nΩL(x, ˜x) =\nL\nX\nℓ=1\nh\nKℓ−1(x, ˜x)\nL\nY\ni=ℓ\nΓσ′(Ki−1)(x, ˜x)\ni\n.\n(7)\nBesides the asymptotic result, [22] further provide a nonasymptotic bound as follows.\nTheorem 2 ([22]). For the FFNet deﬁned in (4), when the width m ≥CL6ϵ−4 log(L/δ), where C\nis a constant, with probability at least 1 −δ over the initialization, for input x, ˜x on the unit sphere,\nthe Neural Tangent Kernel can be approximated by ΩL(x, ˜x), i.e.,\n\f\f⟨∇θf, ∇θ ˜f⟩−ΩL(x, ˜x)\n\f\f ≤Lϵ.\n[22] then showed that a sufﬁciently wide FFNet trained by gradient ﬂow is close to the kernel\nregression predictor via its NTK.\nRemark 1. For self-containedness, we directly adopt the results from existing literature in this\nsubsection. For more technical details on gradient ﬂow and kernel ridge regression, we refer the\nreaders to [28, 21, 22].\n3.2\nResidual Networks\nWe\nconsider\nthe\nResidual\nNetwork\n(ResNet)\nin\n(5),\nwhere\nall\nparameters\n(A, v, W1, · · · , WL, V1, · · · , VL) are independently initialized from the standard Gaussian\n4In general, the weight matrices do not need to be square matrices, nor do they need to be of the same size.\n4\ndistribution. For simplicity, we only train θ = (W1, · · · , WL, V1, · · · , VL), but not A or v, and the\nNTK of the ResNet is computed accordingly. Note that our theory can be naturally generalized to the\nsetting where all parameters including A and v are trained, but the analysis will be more involved.\nOur next proposition derives the GP kernel of the ResNet.\nProposition 3. The GP kernel of the ResNet is\nK0(x, ˜x) = x⊤˜x; Kℓ(x, ˜x) = Kℓ−1(x, ˜x) + α2Γσ(Kℓ−1)(x, ˜x),\nwhere ℓ= 1, · · · , L, and α = L−γ for 0.5 ≤γ ≤1.\nProposition 3 demonstrates that each layer of the ResNet recursively “contributes” to the kernel in\nan incremental manner, which is quite different from that of the FFNet (shown in Proposition 1).\nProposition 3 essentially provides a rigorous justiﬁcation for the intuition discussed by [33]. Besides\nthe above asymptotic result, we also derive a nonasymptotic bound as follows.\nTheorem 3. For the ResNet deﬁned in (5), given two inputs on the unit sphere x, ˜x ∈SD−1, ϵ < 0.5,\nand\nm ≥Cϵ−2L2−2γ log(36(L + 1)/δ),\nwhere C is a constant and 0.5 ≤γ ≤1, with probability at least 1 −δ over the randomness of the\ninitialization, for all layers ℓ= 0, · · · , L and (x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, ˜x)}, we have\n|⟨x(1)\nℓ, x(2)\nℓ⟩−Kℓ(x(1), x(2))| ≤ϵ,\nwhere Kℓis recursively deﬁned in Proposition 3.\nTheorem 3 implies that sufﬁciently wide residual networks are mimicking the GP kernel under proper\nconditions. The proof can be found in Appendix A. Next we present the NTK of the ResNet deﬁned\nin (5) in the following proposition.\nProposition 4. The NTK of the ResNet is ΩL(x, ˜x) = α2 PL\nℓ=1\n\u0002\nBℓ+1(x, ˜x)Γσ(Kℓ−1)(x, ˜x) +\nKℓ−1(x, ˜x)Bℓ+1(x, ˜x)Γσ′(Kℓ−1)(x, ˜x)\n\u0003\n, where Kℓ’s are deﬁned in Proposition 3; BL+1(x, ˜x) = 1,\nand for ℓ= 1, · · · , L, Bℓ’s are deﬁned as\nBℓ+1(x, ˜x) = Bℓ+2(x, ˜x) + α2Bℓ+2(x, ˜x)Γσ′(Kℓ)(x, ˜x).\nProposition 4 implies that similar to what has been proved for the FFNet, the ResNet trained\nby gradient ﬂow is also equivalent to the kernel regression predictor with some NTK. Note that\nProposition 4 is an asymptotic result. We defer the proof, as it can be straightforwardly derived from\nthe nonasymptotic bound as follows.\nTheorem 4. For the ResNet deﬁned in (5), given two inputs on the unit sphere x, ˜x ∈SD−1, ϵ < 0.5,\nand\nm ≥Cϵ−4L2−2γ\u0000log(320(L2 + 1)/δ) + 1\n\u0001\n,\nwhere C is a constant, with probability at least 1 −δ over the randomness of the initialization, we\nhave\n\f\f\n∇θf, ∇θ ˜f\n\u000b\n−ΩL(x, ˜x)\n\f\f ≤2Lα2ϵ,\nwhere α = L−γ with γ ∈[0.5, 1], ΩL(x, ˜x) is deﬁned in Proposition 4.\nProof Sketch of Proposition 4 and Theorem 4. For simplicity, we use φW\n:\nRm\n→\nRm to\ndenote φW (z)\n=\nq\n2\nmσ0(Wz). Then its derivative w.r.t.\nz is as follows, φ′\nW (z)\n=\nq\n2\nmD(Wz)W, where D(Wz) is an operator deﬁned as D(Wz)\n≡\ndiag(σ′\n0(Wz))\n=\ndiag([I{W1,·z≥0}, · · · , I{Wm,·z≥0}]⊤).\nFor simplicity, we denote Dℓ= D(Wℓxℓ−1), where ℓ= 1, 2, · · · , L. Note that Dℓis essentially the\nactivation pattern of the ℓ-th bottleneck layer on the input x. We denote eDℓfor ˜x in a similar fashion.\nThen we have\n∂xℓ\n∂xℓ−1 = Im +α\nq\n1\nmVℓ\nq\n2\nmDℓWℓ. For ℓ= 1, · · · , L, we denote bℓ+1 = ∇xℓf. Then\nwe have bℓ+1 =\n\u0000v⊤\n∂xL\n∂xL−1\n∂xL−1\n∂xL−2 · · · ∂xℓ+1\n∂xℓ\n\u0001⊤.\nCombining all above derivations,\nwe have ∇Vℓf\n=\nα\n√mbℓ+1 · (φWℓ(xℓ−1))⊤, and\n∇Wℓf =\nα\n√m\nq\n2\nmDℓV ⊤\nℓbℓ+1 · x⊤\nℓ−1. Then we can derive the kernel PL\nℓ=1⟨∇Wℓf, ∇Wℓ˜f⟩+\nPL\nℓ=1⟨∇Vℓf, ∇Vℓ˜f⟩, where⟨∇Vℓf, ∇Vℓ˜f⟩=α2 1\nm⟨bℓ+1,˜bℓ+1⟩\n|\n{z\n}\nTℓ,1\n⟨φWℓ(xℓ−1), φWℓ(˜xℓ−1)⟩\n|\n{z\n}\nTℓ,2\n,\n5\n⟨∇Wℓf, ∇Wℓ˜f⟩=α2⟨xℓ−1, ˜xℓ−1⟩\n|\n{z\n}\nTℓ,3\n2\nm2˜b⊤\nℓ+1VℓeDℓDℓV ⊤\nℓbℓ+1\n|\n{z\n}\nTℓ,4\n. Note that the concentration of Tℓ,3 can\nbe shown by Theorem 3. We then show the concentration of Tℓ,1, Tℓ,2 and Tℓ,4, respectively.\nFor simplicity, we deﬁne two matrices for each layer,\nbΣℓ(x, ˜x) =\n\u0014\n⟨xℓ, xℓ⟩\n⟨xℓ, ˜xℓ⟩\n⟨˜xℓ, xℓ⟩\n⟨˜xℓ, ˜xℓ⟩\n\u0015\n, Σℓ(x, ˜x) =\n\u0014\nKℓ(x, x)\nKℓ(x, ˜x)\nKℓ(˜x, x)\nKℓ(˜x, ˜x)\n\u0015\n.\nWe deﬁne ψσ : M2\n+ →R as ψσ(Σ) = E(X, ˜\nX)∼N(0,Σ)σ(X)σ( ˜X) and ψσ′ : M2\n+ →R as ψσ′(Σ) =\nE(X, ˜\nX)∼N(0,Σ)σ′(X)σ′( ˜X). Note Γσ(Kℓ−1) = ψσ(Σℓ−1) and Γσ′(Kℓ−1) = ψσ′(Σℓ−1).\nThe following lemmas are technical results and very involved. Please see Appendix B for details.\nLemma 1. Suppose that for ℓ= 1, · · · , L,\n∥bΣℓ−1(x, ˜x) −Σℓ−1(x, ˜x)∥max ≤cϵ2, m ≥C1ϵ−2L2−2γ\u0000log(80L2/δ) + 1\n\u0001\n,\n(8)\nwith probability at least 1 −3δ, we have |Tℓ,1 −Bℓ+1(x, ˜x)| ≤c1ϵ, for ℓ= 1, · · · , L, where C1, c1,\nand c are constants.\nLemma 2. Suppose (8) holds for ℓ= 1, · · · , L. With probability at least 1 −δ, we have |Tℓ,2 −\nΓσ(Kℓ−1)(x, ˜x)| ≤c2ϵ, for ℓ= 1, · · · , L, where C2 and c2 are constants.\nLemma 3. Suppose that (8) holds for ℓ= 1, · · · , L. With probability at least 1 −3δ, we have\n|Tℓ,4 −Bℓ+1(x, ˜x)Γσ′(Kℓ−1)(x, ˜x)| ≤c3ϵ, for ℓ= 1, · · · , L, where c3 is a constant.\nWe remark: (1) Lemma 1 is proved by reverse induction; (2) Lemma 2 exploits the concentration\nproperties of Wℓand local Lipschitz properties of ψσ; (3) We prove Lemma 3 and Lemma 1\nsimultaneously with the Hölder continuity of ψσ′. Combining all results above, we complete\nTheorem 4. Moreover, taking m →∞, we have Proposition 4.\n4\nDeep Feedforward v.s. Residual Networks\nTo compare the NTKs associated with deep FFNets and ResNets, we consider proper normalization,\nwhich avoids the kernel function blowing up or vanishing as the depth L goes to inﬁnity.\n4.1\nThe Limiting NTK of the Feedforward Networks\nRecall that the NTK of the L-layer FFNet deﬁned in (4) is ΩL(x, ˜x) = PL\nℓ=1\n\u0002\nKℓ−1(x, ˜x) ·\nQL\ni=ℓΓσ′(Ki−1)(x, ˜x)\n\u0003\n. One can check that ΩL(x, x) = L for all x ∈SD−1.\nTo avoid\nΩL(x, x) →∞, as L →∞. We consider a normalized version as\nΩL(x, ˜x) = 1\nLΩL(x, ˜x).\nWe characterize the impact of the depth L on the NTK in the following theorem.\nTheorem 5. For the NTK of the FFNet, as L →∞, given x, ˜x ∈SD−1 and |1 −x⊤˜x| ≥δ > 0,\nwhere δ is a constant and does not scale with L, we have\n\f\f\fΩL(x, ˜x) −1/4\n\f\f\f = O\n\u0010polylog(L)\nL\n\u0011\n,\nWhen x = ˜x, we have ΩL(x, ˜x) = 1, ∀L.\nProof Sketch of Theorem 5 . The main challenge comes from the sophisticated recursion of the kernel.\nTo handle the recursion, we employ the following bound.\nLemma 4. When L is large enough, we have\ncos\n\nπ\n\n1 −\n\u0012\nn\nn + 1\n\u00133+ log(L)2\nL\n\n\n\n≤Kn(x, ˜x) ≤cos\n\nπ\n\n1 −\n\u0012\nn + log(L)p\nn + log(L)p + 1\n\u00133−log(L)2\nL\n\n\n\n,\nwhere p is a positive constant depending on δ.\n6\nBy Lemma 4, we can further bound QL\ni=ℓΓσ′(Ki−1(x, ˜x)) by\n\u0010ℓ−1\nL\n\u00113+ log(L)2\nL\n≤\nL\nY\ni=ℓ\nΓσ′(Ki−1(x, ˜x)) ≤\n\u0010ℓ+ log(L)p −1\nL + log(L)p\n\u00113−log(L)2\nL\n(9)\nHence we can measure the rate of convergence. The detailed proof is the following.\nAs can be seen from Theorem 5, the NTK of the FFNet converges to a limiting form, i.e.,\nΩ∞(x, ˜x) = lim\nL→∞ΩL(x, ˜x) =\n\u001a\n1/4,\nx ̸= ˜x\n1,\nx = ˜x .\nFor simplicity, we refer to Ω∞as the limiting NTK of the FFNets.\nThe limiting NTK of the FFNets is actually a non-informative kernel. For example, we consider\na kernel regression problem with n independent observations {(xi, yi)}n\ni=1, where xi ∈RD is\nthe feature vector, and yi ∈R is the response. Without loss of generality, we assume that the\ntraining samples have been properly processed such that xi ̸= xj for i ̸= j, and Pn\ni=1 yi = 0. By\nthe Representer theorem [34], we know that the kernel regression function can be represented by\nf(·) = Pn\ni=1 βiΩ∞(xi, ·). We then minimize the regularized empirical risk as follows.\nˆβ = min\nβ ∥y −eΩβ∥2 + λβ⊤eΩβ,\n(10)\nwhere β = (β1, ...βn)⊤∈Rn, y = (y1, ..., yn)⊤∈Rn, eΩ∈Rn×n with eΩij = Ω∞(xi, xj), and λ\nis the regularization parameter and usually very small for large n. One can check that (10) admits\na closed form solution ˆβ = (eΩ+ λIn)−1y. Note that we have ˜Ω+ λIn = 1/4Jn + (λ + 3/4)In,\nwhich is the sum of a diagonal matrix and a rank-one matrix and Jn is n × n all-ones matrix. By\nSherman – Morrison formula\n(A + uv⊤)−1 = A−1 −A−1uv⊤A−1\n1 + v⊤A−1u , we have ˆβ =\n1\nλ + 3/4\n\u0010\nIn −\n1\nn + 4λ + 3Jn\n\u0011\ny.\nThen we further have f(xj) = Pn\ni=1 ˆβiΩ∞(xi, xj) =\n3\n4λ+3yj.\nAs can be seen, for sufﬁciently large n and sufﬁciently small λ, we have f(xj) ≈yj, which means\nthat we can ﬁt the training data well. However, for an unseen data point x∗, where x∗̸= x1, ..., xn,\nthe regression function f always gives an output 0, i.e.,\nf(x∗) =\nn\nX\ni=1\nˆβiΩ∞(xi, x∗) = 1\n4\nn\nX\ni=1\nˆβi = 0.\nThis indicates that the function class induced by the limiting NTK of the FFNets Ω∞is not learnable.\n4.2\nThe Limiting NTK of the Residual Networks\nRecall that the inﬁnite-width NTK of the L-layer ResNet is\nΩL(x, ˜x) = α2\nL\nX\nℓ=1\nh\nBℓ+1(x, ˜x)Γσ(Kℓ−1)(x, ˜x) + Kℓ−1(x, ˜x)Bℓ+1(x, ˜x)Γσ′(Kℓ−1)(x, ˜x)\ni\n,\nwhere BL+1(x, ˜x) = 1 and for ℓ= 1, .., L −1, Bℓ+1(x, ˜x) = QL−1\ni=ℓ(1 + α2Γσ′(Ki)(x, ˜x)). One\ncan check that for x ∈SD−1, ΩL(x, x) = 2Lα2(1 + α2)L−1.\nDifferent from the NTK of the FFNet, ΩL(x, x) →0 as L →∞. Therefore, we also consider the\nnormalized NTK for the ResNet to prevent the kernel from vanishing. Speciﬁcally, the normalized\nNTK of the ResNet on SD−1 × SD−1, ΩL(x, ˜x), is deﬁned as follows,\n1/(2L)\n(1 + α2)L−1\nL\nX\nℓ=1\nh\nBℓ+1(x, ˜x)Γσ(Kℓ−1)(x, ˜x) + Kℓ−1(x, ˜x)Bℓ+1(x, ˜x)Γσ′(Kℓ−1)(x, ˜x)\ni\n. (11)\nWe then analyze the limiting NTK of the ResNets. Recall that α = L−γ. Our next theorem only\nconsiders γ = 1, i.e., α = 1/L.\nTheorem 6. For the NTK of the ResNet, as L →∞, given α =\n1\nL and x, ˜x ∈SD−1 such that\n|1 −x⊤˜x| ≥δ > 0, where δ is a constant and does not scale with L, we have\n\f\fΩL(x, ˜x) −Ω1(x, ˜x)\n\f\f = O (1/L) ,\nwhere Ω1(x, ˜x) = 1\n2\n\u0010\nˆσ(x⊤˜x) + x⊤˜x · ˆσ′(x⊤˜x)\n\u0011\n.\n7\nProof Sketch of Theorem 6. The main technical challenge here is also handling the recursion. Specif-\nically, we denote Kℓ,L to be the ℓ-th layer of the GP kernel when the depth is L, which is orig-\ninally denoted by Kℓ(x, ˜x). Let S0 = K0(x, ˜x) and Sℓ,L =\nKℓ,L\n(1+α2)ℓ=\nKℓ,L\n(1+1/L2)ℓ. We have\nΓσ(Kℓ,L) = (1 + α2)ℓˆσ(Sℓ,L) and Γσ′(Kℓ,L) = bσ′(Sℓ,L). We rewrite the recursion of Kℓ,L as\nSℓ,L = Sℓ−1,L+α2ˆσ(Sℓ−1,L)\n(1+α2)\n≥Sℓ−1,L, which eases the technical difﬁculty. However, the proof is\nstill highly involved, and more details can be found in Appendix E.\nNote that we do not consider γ = 0.5 for technical concerns, as ΩL(x, ˜x) in (11) becomes very\ncomplicated to compute, as L →∞. Also we ﬁnd that considering γ = 1 is sufﬁcient to provide us\nnew theoretical insights on ResNets (See more details in Section 5).\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nInner product of the inputs\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized NTK\nL=1\nL=2\nL=5\nL=10\nL=50\nL=200\nL=2000\n(a) FFNets\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nInner product of the inputs\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized NTK\nL=1\nL=2\nL=5\nL=10\nL=50\nL=200\nL=2000\n(b) ResNets with γ = 1\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nInner product of the inputs\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized NTK\nL=1\nL=2\nL=5\nL=10\nL=50\nL=200\nL=2000\n(c) ResNets with γ = 0.5\nFigure 1: Normalized Neural Tangent Kernels Associated with Different Deep Networks.\nDifferent from FFNets, the class of functions induced by the NTKs of the ResNets does not signiﬁ-\ncantly change, as the depth L increases. Surprisingly, we actually have Ω∞= Ω1 for α = 1/L, i.e.,\ninﬁnitely deep and 1-layer ResNets induce the same NTK. To further visualize such a difference, we\nplot the NTKs of the ResNets in Fig. 1(b) and 1(c) for α = 1/L and α = 1/\n√\nL, respectively. As\ncan be seen, the increase of the depth yields very small changes to the NTKs. This partially explains\nwhy increasing the depth of the ResNet does not signiﬁcantly deteriorate the generalization.\nMoreover, as long as x ̸= ex, i.e., ⟨x, ex⟩̸= 1, the limiting NTK of the FFNets always yields 1/4\nregardless how different x is from ex. In contrast, the residual networks do not suffer from this\ndrawback. The limiting NTK of the ResNets can greatly distinguish the difference between x and\nex, e.g., ⟨x, ex⟩= −0.5, 0, and 0.5 yield different values. Therefore, for an unseen data point, the\ncorresponding regression model does not always output 0, which is in sharp contrast to that of the\nlimiting NTK of the FFNets.\n5\nExperiments\nWe demonstrate the generalization properties of the kernel regression based on the NTKs of the\nFFNets and the ResNets with varying depths. Our experiments follow similar settings to [22, 23].\nWe adopt two widely used data sets – MNIST [35] and CIFAR10 [36], which are popular in existing\nliterature. Note that both MNIST and CIFAR10 contains 10 classes of images. For simplicity,\nwe select 2 classes out of 10 (digits “0” and “8” for MNIST, categories “airplane” and “ship” for\nCIFAR10), respectively, which results in two binary classiﬁcation problems, denoted by MNIST2\nand CIFAR2.\nSimilar to [22, 23], we use the kernel regression model for classiﬁcation. Speciﬁcally, given the\ntraining data (x1, y1), · · · , (xn, yn), where xi ∈RD and yi ∈{−1, +1} for i = 1, ..., n, we compute\nthe kernel matrix ˜K = [ ˜Kij]n\ni,j=1 using the NTKs associated with the FFNets and the ResNets, where\n˜Kij = ΩL(xi, xj). Then we compute the kernel regression function f(x) = Pn\ni=1 αiΩL(x, xi),\nwhere [α1, ..., αn]⊤= ( ˜K + λI)−1y, y = [y1, ..., yn]⊤and λ = 0.1/n is a very small constant. We\npredict the label of x to be sign(f(x)).\nOur experiments adopt the NTKs associated with three network architectures: (1) FFNets, (2) ResNets\n(γ = 0.5) and (3) ResNets (γ = 1). We set n = 200 and n = 2000. For each data set, we randomly\nselect n training data points (n/2 for each class) and 2000 testing data points (1000 for each class).\nWhen training the kernel regression models, we normalize all training data points to have zero mean\nand unit norm. We repeat the procedure for 20 simulations. We ﬁnd that the training errors of all\nsimulations (L varies from 1 to 2000) are 0.0, which means that all NTK-based models are sufﬁcient\nto overﬁt the training data, regardless n = 200 or n = 2000. The test accuracies of the kernel\nregression models with different kernels and depths are shown in Figure 2.\n8\nAs can be seen, the test accuracies of the kernel regression models of ResNets (both γ = 0.5 and\nγ = 1) are not sensitive to the depth. In contrast, the test accuracies of the kernel regression models\nof the FFNets signiﬁcantly decrease, as the depth L increases. Especially when the sample size is\nsmall (n = 200), the kernel regression models behave like random guess for both MNIST2 and\nCIFAR2 when L ≥1000. This is consistent with our analysis.\n0\n200\n400\n600\n800\n1000\nDepth\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTest Accuracy (%)\nFFNet NTK\nResNet NTK ( =0.5)\nResNet NTK ( =1)\n(a) MNIST2 (n = 200)\n0\n200\n400\n600\n800\n1000\nDepth\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTest Accuracy (%)\nFFNet NTK\nResNet NTK ( =0.5)\nResNet NTK ( =1)\n(b) MNIST2 (n = 2000)\n0\n200\n400\n600\n800\n1000\nDepth\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTest Accuracy (%)\nFFNet NTK\nResNet NTK ( =0.5)\nResNet NTK ( =1)\n(c) CIFAR2 (n = 200)\n0\n200\n400\n600\n800\n1000\nDepth\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTest Accuracy (%)\nFFNet NTK\nResNet NTK ( =0.5)\nResNet NTK ( =1)\n(d) CIFAR2 (n = 2000)\nFigure 2: Test accuracies of the kernel regression models evaluated on MNIST2 and CIFAR2.\nNext we provide numerical veriﬁcations for our theorems. For Theorem 4, we randomly initialize the\nResNet with width=500, scaling factor γ = 1 and depth L = 5, 10, 100, 300, and then calculate the\ninner product of the Jacobians of the ResNet for two different inputs as in the deﬁnition of NTK. We\nrepeat the procedure for 500 times and plot the mean value (black cross) and the 1/4, 3/4 quantiles\n(\"I\"-shape line) of the sampled random NTKs and the theoretical NTK value in Fig. 3(a), which\nshows the two results match very well. For Theorem 5 and Theorem 6, Fig. 3(b) and Fig. 3(c) show\nthat limL→∞|ΩL(x, ˜x) −1/4| · L/log(L) ≈constant and limL→∞|ΩL(x, ˜x) −Ω1(x, ˜x)| · L ≈\nconstant with x⊤˜x = K0 chosen at 9 points.\n5\n10\n100\n300\nnumber of layers\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nNormalized NTK\nempirical, K0=0.3\ntheoretical K0=0.3\nempirical, K0=0.9\ntheoretical K0=0.9\n(a) Theorem 4\nnumber of layers L\nK0=-0.8\nK0=-0.6\nK0=-0.4\nK0=-0.2\nK0=0\nK0=0.2\nK0=0.4\nK0=0.6\nK0=0.8\n(b) Theorem 5\nnumber of layers L\nK0=-0.8\nK0=-0.6\nK0=-0.4\nK0=-0.2\nK0=0\nK0=0.2\nK0=0.4\nK0=0.6\nK0=0.8\n(c) Theorem 6\nFigure 3: Veriﬁcation of main theorems. (a) Theorem 4, m = 500 and scaling γ = 1; (b) Theorem 5,\ny-axis is |ΩL(x, ˜x) −1/4| · L/log(L); (c) Theorem 6, y-axis is |ΩL(x, ˜x) −Ω1(x, ˜x)| · L\n6\nDiscussion\n(a) CIFAR2 (n=200)\n(b) CIFAR2 (n=2000)\nFigure 4: Test accuracies of\nthe kernel regression models\nevaluated on CIFAR2.\nWe discuss the NTK of the ResNet in more details. We remark unless\nspeciﬁed, the NTK mentioned below indicates the normalized NTK.\nOur theory shows the function class induced by the NTK of the deep\nResNet asymptotically converges to that by the NTK of the 1-layer\nResNet, as the depth increases. This indicates that the complexity of\nsuch a function class is not signiﬁcantly different from that by the NTK\nof the 1-layer ResNet, for large enough L. Thus, the generalization\ngap does not signiﬁcantly increase, as L increases.\nOn the other hand, our experiments suggest that, as illustrated in Figure\n4, the NTK of the ResNet with γ = 1 actually achieves the best testing\naccuracy for CIFAR2 when L = 2. The accuracy slightly decreases\nas L increases, and becomes stable when L ≥9. For the NTK of the\nResNet with γ = 0.5, the accuracy achieves the best when L ≈15,\nand becomes stable for L ≥15. Such evidence suggests that the\nfunction class induced by the NTKs of the ResNets with large L and\nlarge γ are possibly not as ﬂexible as those by the NTKs of the deep\nResNets with small L and small γ.\nExisting literature connects overparameterized neural networks to\nNTKs only under some very speciﬁc regime. Practical neural networks, however, are trained under\nmore complicated regimes. Therefore, there still exists a signiﬁcant theoretical gap between NTKs and\npractical neural networks. For example, Theorem 6 shows that the NTK of the inﬁnitely deep ResNet\nis identical to that of the 1-layer ResNet, while practical ResNets often show better generalization\nperformance, as the depth increases. Also, we do not consider batch norm in our networks but refer\nto [37] if necessary. We will leave these challenges for future investigation.\n9\nBroader Impact\nThis paper makes a signiﬁcant contribution to extending the frontier of deep learning theory, and\nincreases the intellectual rigor. To the best of our knowledge, our results are the ﬁrst one for analyzing\nthe effect of depth on the generalization of neural tangent kernels (NTKs). Moreover, our results are\nalso the ﬁrst one establishing the non-asymptotic bounds for NTKs of ResNets when all but the last\nlayers are trained, which enables us to successfully analyze the generalization properties of ResNets\nthrough the perspective of NTK. This is in sharp contrast to the existing impractical theoretical results\nfor NTKs of ResNets, which either only apply to an over-simpliﬁed structure of ResNets or only deal\nwith the case when the last layer is trained.\nAcknowledgement\nMolei Tao was partially supported by NSF DMS-1847802 and ECCS-1936776 and Yuqing Wang\nwas partially supported by NSF DMS-1847802.\nReferences\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in Neural Information Processing Systems, pages\n1097–1105, 2012.\n[2] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural\nInformation Processing Systems, pages 2672–2680, 2014.\n[3] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\nsegmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nJune 2015.\n[4] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep\nrecurrent neural networks. In 2013 IEEE international conference on acoustics, speech and\nsignal processing, pages 6645–6649. IEEE, 2013.\n[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[6] Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. Recent trends in\ndeep learning based natural language processing. ieee Computational intelligenCe magazine,\n13(3):55–75, 2018.\n[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[8] Rupesh K Srivastava, Klaus Greff, and Jürgen Schmidhuber. Training very deep networks. In\nAdvances in Neural Information Processing Systems, pages 2377–2385, 2015.\n[9] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\nconvolutional networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4700–4708, 2017.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. In European conference on computer vision, pages 630–645. Springer, 2016.\n[11] Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles\nof relatively shallow networks. In Advances in Neural Information Processing Systems, pages\n550–558, 2016.\n[12] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian\nMcWilliams. The shattered gradients problem: If resnets are the answer, then what is the\nquestion? arXiv preprint arXiv:1702.08591, 2017.\n10\n[13] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the\nloss landscape of neural nets. In Advances in Neural Information Processing Systems, pages\n6389–6399, 2018.\n[14] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via\nover-parameterization. arXiv preprint arXiv:1811.03962, 2018.\n[15] Huishuai Zhang, Da Yu, Mingyang Yi, Wei Chen, and Tie-Yan Liu. Convergence theory of\nlearning over-parameterized resnet: A full characterization, 2019.\n[16] Moritz Hardt and Tengyu Ma.\nIdentity matters in deep learning.\narXiv preprint\narXiv:1611.04231, 2016.\n[17] Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu\nactivation. In Advances in neural information processing systems, pages 597–607, 2017.\n[18] Tianyi Liu, Minshuo Chen, Mo Zhou, Simon S Du, Enlu Zhou, and Tuo Zhao. Towards\nunderstanding the importance of shortcut connections in residual networks. In Advances in\nNeural Information Processing Systems, pages 7890–7900, 2019.\n[19] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds\nglobal minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.\n[20] Spencer Frei, Yuan Cao, and Quanquan Gu. Algorithm-dependent generalization bounds for\noverparameterized deep residual networks. In Advances in Neural Information Processing\nSystems, pages 14769–14779, 2019.\n[21] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. In Advances in Neural Information Processing Systems,\npages 8571–8580, 2018.\n[22] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.\nOn exact computation with an inﬁnitely wide neural net. arXiv preprint arXiv:1904.11955,\n2019.\n[23] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis\nof optimization and generalization for overparameterized two-layer neural networks. arXiv\npreprint arXiv:1901.08584, 2019.\n[24] Zeyuan Allen-Zhu and Yuanzhi Li. Can sgd learn recurrent neural networks with provable\ngeneralization? arXiv preprint arXiv:1902.01028, 2019.\n[25] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overpa-\nrameterized neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918,\n2018.\n[26] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic\ngradient descent on structured data. In Advances in Neural Information Processing Systems,\npages 8157–8166, 2018.\n[27] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes\nover-parameterized deep relu networks. arxiv e-prints, art. arXiv preprint arXiv:1811.08888,\n2018.\n[28] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:\nThe power of initialization and a dual view on expressivity. In Advances In Neural Information\nProcessing Systems, pages 2253–2261, 2016.\n[29] Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Y. Bengio,\nD. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural\nInformation Processing Systems 22, pages 342–350. Curran Associates, Inc., 2009.\n[30] Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel\nnetworks. In Advances in neural information processing systems, pages 2627–2635, 2014.\n11\n[31] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances\nin neural information processing systems, pages 1177–1184, 2008.\n[32] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington,\nand Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint\narXiv:1711.00165, 2017.\n[33] Adrià Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional\nnetworks as shallow gaussian processes. arXiv preprint arXiv:1808.05587, 2018.\n[34] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning,\nvolume 1. Springer series in statistics New York, 2001.\n[35] Yann LeCun. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/,\n1998.\n[36] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. URl:\nhttps://www. cs. toronto. edu/kriz/cifar. html (vi sited on Mar. 1, 2016), 2009.\n[37] Arthur Jacot, Franck Gabriel, Franccois Gaston Ged, and Clément Hongler. Order and chaos:\nNtk views on dnn normalization, checkerboard and boundary artifacts. 2019.\n[38] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv\npreprint arXiv:1011.3027, 2010.\n[39] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.\nMIT press, 2018.\n[40] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart.\nConcentration inequalities: A\nnonasymptotic theory of independence. Oxford University Press, 2013.\n12\nA\nProof of GP Kernels of ResNets\nA.1\nNotation and Main Idea\nFor a ﬁxed pair of inputs x and ˜x, we introduce two matrices for each layer\nˆΣℓ(x, ˜x) =\n\u0014\n⟨xℓ, xℓ⟩\n⟨xℓ, ˜xℓ⟩\n⟨˜xℓ, xℓ⟩\n⟨˜xℓ, ˜xℓ⟩\n\u0015\n,\nand\nΣℓ(x, ˜x) =\n\u0014\nKℓ(x, x)\nKℓ(x, ˜x)\nKℓ(˜x, x)\nKℓ(˜x, ˜x)\n\u0015\n.\nˆΣℓ(x, ˜x) is the empirical Gram matrix of the outputs of the ℓ-th layer, while Σℓ(x, ˜x) is the inﬁnite-\nwidth version. Theorem 3 says that with high probability, for each layer ℓ, the difference of these two\nmatrices measured by the entry-wise L∞norm (denoted by ∥· ∥max) is small.\nThe idea is to bound how much the ℓ-th layer magniﬁes the input error to the output. Speciﬁcally, if\nthe outputs of (ℓ−1)-th layer satisfy\n\r\r\rˆΣℓ−1(x, ˜x) −Σℓ−1(x, ˜x)\n\r\r\r\nmax ≤τ,\nwe hope to prove that with high probability over the randomness of Wℓand Vℓ, we have\n\r\r\rˆΣℓ(x, ˜x) −Σℓ(x, ˜x)\n\r\r\r\nmax ≤\n\u0012\n1 + O\n\u0012 1\nL\n\u0013\u0013\nτ.\nThen the theorem is proved by ﬁrst showing that w.h.p.\n\r\r\rˆΣ0(x, ˜x) −Σ0(x, ˜x)\n\r\r\r\nmax ≤(1 +\nO(1/L))−Lϵ and then applying the result above for each layer.\nA.2\nLemmas\nWe introduce the following lemmas. The ﬁrst lemma shows the boundedness of Kℓ(x, ˜x).\nLemma 5. For the ResNet deﬁned in Eqn. (5), Kℓ(x, x) = (1 + α2)ℓfor all x ∈SD−1, ℓ=\n0, 1, · · · , L. Also Kℓ(x, x) is bounded uniformly when 0.5 ≤γ ≤1.\nRecall that φWℓ(z) =\nq\n2\nmσ0(Wℓz). Since Wℓis Gaussian, we know that φWℓ(xℓ−1) and φWℓ(˜xℓ−1)\nare both sub-Gaussian random vectors over the randomness of Wℓ. Then their inner product enjoys\nsub-exponential property.\nLemma 6 (Sub-exponential concentration). With probability at least 1 −δ′ over the randomness of\nWℓ∼N(0, I), when m ≥c′ log(6/δ′), the following hold simultaneously\n\f\f\f⟨φWℓ(xℓ−1), φWℓ(˜xℓ−1)⟩−ψσ(ˆΣℓ−1(x, ˜x))\n\f\f\f ≤\nr\nc′ log(6/δ′)\nm\n∥xℓ−1∥∥˜xℓ−1∥,\n(12)\n\f\f\f∥φWℓ(xℓ−1)∥2 −∥xℓ−1∥2\f\f\f ≤\nr\nc′ log(6/δ′)\nm\n∥xℓ−1∥2,\n(13)\n\f\f\f∥φWℓ(˜xℓ−1)∥2 −∥˜xℓ−1∥2\f\f\f ≤\nr\nc′ log(6/δ′)\nm\n∥˜xℓ−1∥2.\n(14)\nLemma 7 (Locally Lipschitzness, based on [28]). ψσ is (1 + 1\nπ( r\nµ)2)-Lipschitz w.r.t. max norm in\nMµ,r =\n\u001a\u0014\na\nb\nb\nc\n\u0015\n|a, c ∈[µ −r, µ + r]; ac −b2 > 0\n\u001b\nfor all µ > 0, 0 < r ≤µ/2. That means, if\n(i). ∥ˆΣℓ−1(x, ˜x) −Σℓ−1(x, ˜x)∥max ≤τ and (ii). Kℓ−1(x, x) = Kℓ−1(˜x, ˜x) = µ, for τ ≤µ/2, we\nhave\n\f\f\fψσ(ˆΣℓ−1(x, ˜x)) −ψσ(Σℓ−1(x, ˜x))\n\f\f\f ≤\n\u0010\n1 + 1\nπ\n\u0010τ\nµ\n\u00112\u0011\nτ.\nA.3\nProof of Theorem 3\nProof. In this proof, we also show the following hold with the same probability.\n1. For ℓ= 0, 1, · · · , L, ∥xℓ∥and ∥˜xℓ∥are bounded by an absolute constant C1 (C1 = 4).\n13\n2. For ℓ= 1, · · · , L, ∥φWℓ(xℓ−1)∥and ∥φWℓ(˜xℓ−1)∥are bounded by an absolute constant C2\n(C2 = 8).\n3.\n\f\f\f⟨φWℓ(x(1)\nℓ−1), φWℓ(x(2)\nℓ−1)⟩−Γσ(Kℓ−1)(x(1), x(2))\n\f\f\f ≤2ϵ for all ℓ= 1, · · · , L and\n(x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, ˜x)}.\nWe focus on the ℓ-th layer. Let τ =\n\r\r\rˆΣℓ−1(x, ˜x) −Σℓ−1(x, ˜x)\n\r\r\r\nmax. Recall that Γσ(Kℓ−1)(x, ˜x) =\nψσ(Σℓ−1(x, ˜x)) = E(X, ˜\nX)∼N(0,Σℓ−1(x,˜x))σ(X)σ( ˜X). Then\nKℓ(x, ˜x) = Kℓ−1(x, ˜x) + α2ψσ(Σℓ−1(x, ˜x)).\nSince xℓ= xℓ−1 +\nα\n√mVℓφWℓ(xℓ−1), we have\n⟨xℓ, ˜xℓ⟩= ⟨xℓ−1, ˜xℓ−1⟩+ α2\nm ⟨VℓφWℓ(xℓ−1), VℓφWℓ(˜xℓ−1)⟩\n+ α 1\n√m\n\u0000⟨VℓφWℓ(xℓ−1), ˜xℓ−1⟩+ ⟨VℓφWℓ(˜xℓ−1), xℓ−1⟩\n\u0001\n= ⟨xℓ−1, ˜xℓ−1⟩+ α2P + α(Q + R),\nwhere\nP ≡1\nm⟨VℓφWℓ(xℓ−1), VℓφWℓ(˜xℓ−1)⟩,\nQ ≡\n1\n√m\n\u0000⟨VℓφWℓ(xℓ−1), ˜xℓ−1⟩\n\u0001\n,\nR ≡\n1\n√m\n\u0000⟨VℓφWℓ(˜xℓ−1), xℓ−1⟩\n\u0001\n.\nUnder the randomness of Vℓ, P is sub-exponential, and Q and R are Gaussian random variables.\nTherefore, for a given δ0, if m ≥c0 log(2/δ0), with probability at least 1 −δ0 over the randomness\nof Vℓ, we have\n\f\f\fP −⟨φWℓ(xℓ−1), φWℓ(˜xℓ−1)⟩\n\f\f\f ≤∥φWℓ(xℓ−1)∥∥φWℓ(˜xℓ−1)∥\nr\nc0 log(2/δ0)\nm\n;\n(15)\nfor a given ˜δ, with probability at least 1 −2˜δ over the randomness of Vℓ, we have\n|Q| ≤∥φWℓ(xℓ−1)∥∥˜xℓ−1∥\ns\n˜c log(2/˜δ)\nm\n,\n(16)\nand\n|R| ≤∥φWℓ(˜xℓ−1)∥∥xℓ−1∥\ns\n˜c log(2/˜δ)\nm\n,\n(17)\nwhere c0, ˜c > 0 are absolute constants.\nUsing the above result and Lemma 6 and setting δ0 = ˜δ =\nδ\n18(L+1), δ′ =\nδ\n6(L+1), when m ≥\nC log(36(L + 1)/δ), we have (15), (16), (17), (12), (13), and (14) hold with probability at least\n1 −\nδ\n3(L+1).\nRecall that τ =\n\r\r\rˆΣℓ−1(x, ˜x) −Σℓ−1(x, ˜x)\n\r\r\r\nmax. Conditioned on τ < 0.5, we have\n∥xℓ−1∥2 ≤Kℓ−1(x, x) + τ ≤(1 + α2)L + τ ≤e + τ.\nSimilarly we can show ∥˜xℓ−1∥2 is bounded by e + τ. By (13) and (14) we have ∥φWℓ(xℓ−1)∥2 ≤\n2∥xℓ−1∥2 and ∥φWℓ(˜xℓ−1)∥2 ≤2∥˜xℓ−1∥2, which are both bounded.\n14\nThen\n\f\f\f⟨xℓ, ˜xℓ⟩−\n\u0000α2ψσ(Σℓ−1(x, ˜x)) + Kℓ−1(x, ˜x)\n\u0001 \f\f\f\n≤τ + α2\u0000P −ψσ(Σℓ−1(x, ˜x))\n\u0001\n+ α(|Q| + |R|)\n≤τ + α2\f\f\fP −⟨φWℓ(xℓ−1), φWℓ(˜xℓ−1)⟩\n\f\f\f + α\ns\n˜c log(2/˜δ)\nm\n\u0000∥φWℓ(˜xℓ−1)∥∥xℓ−1∥+ ∥φWℓ(xℓ−1)∥∥˜xℓ−1∥\n\u0001\n+ α2\f\f\fψσ(ˆΣℓ−1(x, ˜x)) −ψσ(Σℓ−1(x, ˜x))\n\f\f\f + α2\f\f\f⟨φWℓ(xℓ−1), φWℓ(˜xℓ−1)⟩−ψσ(ˆΣℓ−1(x, ˜x))\n\f\f\f\n≤τ + (α2 + α)\nr\nC3 log(36(L + 1)/δ)\nm\n+ α2τ\n\u0012\n1 + 1\nπ\n\u0012\nτ\nKℓ−1(x, x)\n\u00132\u0013\n≤τ + (α2 + α)\nr\nC3 log(36(L + 1)/δ)\nm\n+ α2τ\n\u0012\n1 + 1\n4π\n\u0013\n.\nWhen α =\n1\nLγ , γ ∈[0.5, 1], we have α2 ≤1/L. Then when\nm ≥C3L2(1−γ) log(36(L + 1)/δ)\nτ 2\n,\nwe have\n\f\f\f⟨xℓ, ˜xℓ⟩−Kℓ(x, ˜x)\n\f\f\f ≤τ + 4\nLτ.\nAs a byproduct, we have\n\f\f\f⟨φWℓ(xℓ−1), φWℓ(˜xℓ−1)⟩−ψσ(Σℓ−1(x, ˜x))\n\f\f\f\n≤\nr\nC4 log(36(L + 1)/δ)\nm\n+\n\u0010\n1 + 1\nπ\n\u0010τ\nµ\n\u00112\u0011\nτ ≤2τ.\nRepeat the above for (xℓ−1, xℓ−1) and (˜xℓ−1, ˜xℓ−1), we have with probability at least 1 −δ/(L + 1)\nover the randomness of Vℓand Wℓ,\n\r\r\rˆΣℓ−1(x, ˜x) −Σℓ−1(x, ˜x)\n\r\r\r\nmax ≤τ ⇒\n\r\r\rˆΣℓ(x, ˜x) −Σℓ(x, ˜x)\n\r\r\r\nmax ≤(1 + 4/L)τ.\n(18)\nFinally, when m ≥C5 log(6(L+1)/δ)\n(ϵ/e4)2\n, with probability at least 1 −δ/(L + 1) over the randomness of\nA, we have\n\r\r\rˆΣ0(x, ˜x) −Σ0(x, ˜x)\n\r\r\r\nmax ≤ϵ/e4.\nThen the result follows by successively using (18).\nA.4\nproof of lemma 7\nProof. [28] showed that\n\r\r\r\r∇ψσ\n\u0014\na\nb\nb\nc\n\u0015\r\r\r\r\n1\n= 1\n2\na + c\n√ac\n\f\f\f\fˆσ\n\u0012\nb\n√ac\n\u0013\n−\nb\n√ac ˆσ′\n\u0012\nb\n√ac\n\u0013\f\f\f\f + ˆσ′\n\u0012\nb\n√ac\n\u0013\n.\nWhen a, c ∈[µ −r, µ + r], we have\n1\n2\na + c\n√ac = 1\n2\n\u0012ra\nc +\nr c\na\n\u0013\n≤1\n2\n\u0012rµ + r\nµ −r +\nrµ −r\nµ + r\n\u0013\n=\n\u0012\n1 −\n\u0012 r\nµ\n\u00132\u0013−1/2\n≤1 +\n\u0012 r\nµ\n\u00132\n.\nThe last inequality holds when r < µ\n2 .\nDeﬁne ρ =\nb\n√ac, we have ρ ∈[−1, 1]. Then\n∥∇φσ∥1 ≤\n\u0012\n1 +\n\u0012 r\nµ\n\u00132\u0013\f\f\fˆσ (ρ) −ρˆσ′ (ρ)\n\f\f\f + ˆσ′ (ρ)\n=\n\u0012\n1 +\n\u0012 r\nµ\n\u00132\u0013 \f\f\f\f\f\np\n1 −ρ2\nπ\n\f\f\f\f\f + 1 −cos−1 ρ\nπ\n≤\np\n1 −ρ2\nπ\n+ 1 −cos−1 ρ\nπ\n+ 1\nπ\n\u0012 r\nµ\n\u00132\n≤1 + 1\nπ\n\u0012 r\nµ\n\u00132\n.\n15\nB\nProof of Theorem 4\nB.1\nNotation and Main Idea\nWe already know that when the network width m is large enough, ⟨xℓ−1, ˜xℓ−1⟩≈Kℓ−1(x, ˜x), and\n⟨φWℓ(xℓ−1), φWℓ(˜xℓ−1)⟩≈Γσ(Kℓ−1)(x, ˜x).\nNext we need to show the concentration of the inner product of\nbℓ\n√m and\n˜bℓ\n√m. We deﬁne two matrices\nfor each layer\nˆΘℓ(x, ˜x) = 1\nm\n\u0014\n⟨bℓ, bℓ⟩\n⟨bℓ,˜bℓ⟩\n⟨˜bℓ, bℓ⟩\n⟨˜bℓ,˜bℓ⟩\n\u0015\n,\nand\nΘℓ(x, ˜x) =\n\u0014\nBℓ(x, x)\nBℓ(x, ˜x)\nBℓ(˜x, x)\nBℓ(˜x, ˜x)\n\u0015\n.\nRecall that\nbℓ= α\nr\n1\nm\nr\n2\nmW ⊤\nℓDℓV ⊤\nℓbℓ+1 + bℓ+1.\nWe aim to show that when ∥ˆΘℓ+1(x, ˜x) −Θℓ+1(x, ˜x)∥max ≤τ, with high probability over the\nrandomness of Wℓand Vℓ, we have ∥ˆΘℓ(x, ˜x) −Θℓ(x, ˜x)∥max ≤(1 + O(1/L))τ. Notice that bℓ+1\nand ˜bℓ+1 contain the information of Wℓand Vℓ; they are not independent. Nevertheless we can\ndecompose the randomness of Wℓand Vℓto show the concentration. This technique is also used in\n[22].\nB.2\nLemmas\nIn this part we introduce some useful lemmas. The ﬁrst one shows the property of the step activation\nfunction.\nLemma 8 (Property of σ′). [22]\n(1). Sub-Gaussian concentration. With probability at least 1 −δ over the randomness of Wℓ, we have\n\f\f\f 2\nm Tr(DℓeDℓ) −ψσ′(ˆΣℓ−1(x, ˜x))\n\f\f\f ≤\nr\nc log(2/δ)\nm\n.\n(2). Holder continuity. Fix µ > 0, 0 < r ≤µ. For all A, B ∈Mµ,r =\n\u001a \u0014\na\nb\nb\nc\n\u0015 \f\f\f\fa, c ∈\n[µ −r, µ + r]; ac −b2 > 0\n\u001b\n, if ∥A −B∥max ≤(µ −r)ϵ2, then\n|ψσ′(A) −ψσ′(B)| ≤ϵ.\nThe following lemma shows that regardless the fact that bℓ+1 and ˜bℓ+1 depend on Vℓ, we can treat Vℓ\nas a Gaussian matrix independent of bℓ+1 and ˜bℓ+1 when the network width is large enough.\nLemma 9. Assume the following inequality hold simultaneously for all ℓ= 1, 2, · · · , L\n\r\r\r 1\n√mWℓ\n\r\r\r ≤C,\n\r\r\r 1\n√mVℓ\n\r\r\r ≤C.\nFix an ℓ. Further assume that\n∥ˆΘℓ+1(x, ˜x) −Θℓ+1(x, ˜x)∥max ≤1.\nWhen m ≥max{ C\nϵ2 (1 + log 6\nδ ), C\nϵ2 log 8L\nδ′ , cL2−2γ log 8L\nδ′ }, the following holds for all (x(1), x(2)) ∈\n{(x, x), (x, ˜x), (˜x, ˜x)} with probability at least 1 −δ −δ′\n\f\f\f\f\n2\nm\nb(1)\nℓ+1\n√m\n⊤\nVℓD(1)\nℓD(2)\nℓV ⊤\nℓ\nb(2)\nℓ+1\n√m −⟨b(1)\nℓ+1\n√m , b(2)\nℓ+1\n√m ⟩2\nm Tr(D(1)\nℓD(2)\nℓ)\n\f\f\f\f ≤ϵ.\nThe following lemma shows the same thing for Wℓas Vℓin Lemma 9.\nLemma 10. Assume the conditions and the results of Lemma 9 hold.\n(1).\nWhen m ≥max{ C\nϵ2 (1 + log 6\nδ ), C\nϵ2 log 8L\nδ′ , cL2−2γ log 8L\nδ′ },the following holds for all\n(x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, ˜x)} with probability at least 1 −δ −δ′\n\f\f\f\f\n1\nm\n2\nm⟨W ⊤\nℓD(1)\nℓV ⊤\nℓ\nb(1)\nℓ+1\n√m , W ⊤\nℓD(2)\nℓV ⊤\nℓ\nb(2)\nℓ+1\n√m ⟩−2\nm⟨D(1)\nℓV ⊤\nℓ\nb(1)\nℓ+1\n√m , D(2)\nℓV ⊤\nℓ\nb(2)\nℓ+1\n√m ⟩\n\f\f\f\f ≤ϵ.\n16\n(2).\nWhen\nm\n≥\nmax{ C\n˜ϵ2 log 16L\n˜δ , cL2−2γ log 16L\n˜δ },\nfor\nall\n(x(1), x(2))\n∈\n{(x, x), (x, ˜x), (˜x, x), (˜x, ˜x)}, the following holds with probability at least 1 −˜δ\n\f\f\f\f\n1\nm\nr\n1\nm\nr\n2\nm⟨W ⊤\nℓD(1)\nℓV ⊤\nℓb(1)\nℓ+1, b(2)\nℓ+1⟩\n\f\f\f\f ≤˜ϵ.\nB.3\nProof of Theorem 4\nProof. In this proof we are going to prove that when m satisﬁes the assumption, with probability at\nleast 1 −δ0, the following hold for ℓ= 1, · · · , L.\n\f\f\f\f\n1\nα2 ⟨∇Vℓf, ∇Vℓ˜f⟩−Bℓ+1(x, ˜x)Γσ(Kℓ−1)(x, ˜x)\n\f\f\f\f ≤ϵ0,\n\f\f\f\f\n1\nα2 ⟨∇Wℓf, ∇Wℓ˜f⟩−Kℓ−1(x, ˜x)Bℓ+1(x, ˜x)Γσ′(Kℓ−1)(x, ˜x)\n\f\f\f\f ≤ϵ0.\nWe break the proof into several steps. Each step is based on the result of the previous steps. Note that\nthe absolute constants c and C may vary throughout the proof.\nStep 1. Norm Control of the Gaussian Matrices\nWith probability at least 1 −δ1, when m > c log 4L\nδ1 , one can show that the following hold simultane-\nously for all ℓ= 1, 2, · · · , L [38]\r\r\r\r\n1\n√mWℓ\n\r\r\r\r ≤C,\n\r\r\r\r\n1\n√mVℓ\n\r\r\r\r ≤C.\nStep 2. Concentration of the GP kernels\nBy Theorem 3, with probability at least 1 −δ2, when\nm ≥C\nϵ4\n2\nL2−2γ log 36(L + 1)\nδ2\n,\nwe have\n1. For ℓ= 0, · · · , L,\n\r\r\rΣℓ(x, ˜x) −ˆΣℓ(x, ˜x)\n\r\r\r\nmax ≤cϵ2\n2;\n2. For ℓ= 0, 1, · · · , L, ∥xℓ∥and ∥˜xℓ∥are bounded by an absolute constant C1 (C1 = 4);\n3. For ℓ= 1, · · · , L, ∥φWℓ(xℓ−1)∥and ∥φWℓ(˜xℓ−1)∥are bounded by an absolute constant C2\n(C2 = 8);\n4.\n\f\f\f⟨φWℓ(x(1)\nℓ−1), φWℓ(x(2)\nℓ−1)⟩−Γσ(Kℓ−1)(x(1), x(2))\n\f\f\f ≤2cϵ2\n2 for all ℓ= 1, · · · , L and\n(x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, ˜x)}.\nStep 3. Concentration of σ′\nBy Lemma 8, when m ≥C\nϵ2\n2 log 6L\nδ3 , with probability at least 1 −δ3, for all ℓ= 1, 2, · · · , L and\n(x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, ˜x)}, we have\n\f\f\f 2\nm Tr(D(1)\nℓD(2)\nℓ) −Γσ′(Kℓ−1)(x(1), x(2))\n\f\f\f ≤\nr\nc log(6L/δ3)\nm\n+\nr\n2\n\r\r\rˆΣℓ−1(x, ˜x) −Σℓ−1(x, ˜x)\n\r\r\r\nmax ≤ϵ2.\nStep 4. Concentration of Bℓ\nRecall that\nbℓ+1 =\n\u0012\nv⊤∂xL\n∂xL−1\n∂xL−1\n∂xL−2\n· · · ∂xℓ+1\n∂xℓ\n\u0013⊤\n.\nWe have\nbL+1 = v,\nand for ℓ= 1, 2, · · · , L −1,\nbℓ+1 = ∂xℓ+1\n∂xℓ\n⊤\nbℓ+2 = α\nr\n1\nm\nr\n2\nmW ⊤\nℓ+1Dℓ+1V ⊤\nℓ+1bℓ+2 + bℓ+2.\n17\nFollowing the same idea in Thm 3, we prove by induction.\nFirst of all, for bL+1, we have\nΘL+1(x, ˜x) =\n\u0014\n1\n1\n1\n1\n\u0015\n, ˆΘL+1(x, ˜x) =\n∥v∥2\nm\n\u0014\n1\n1\n1\n1\n\u0015\n. Then by Bernstein inequality [39], with\nprobability at least 1 −δ4\nL , when m ≥C\nϵ2\n4 log 2L\nδ4 , we have\n\f\f\f\f\n∥v∥2\nm\n−1\n\f\f\f\f ≤ϵ4.\nFix ℓ∈{2, 3, · · · , L}. Assume that\n\r\r\rˆΘℓ+1(x, ˜x) −Θℓ+1(x, ˜x)\n\r\r\r\nmax ≤τ ≤1,\nwe hope to prove with high probability,\n\r\r\rˆΘℓ(x, ˜x) −Θℓ(x, ˜x)\n\r\r\r\nmax ≤(1 + O(1/L))τ.\nFirst write\n1\nm⟨b(1)\nℓ, b(2)\nℓ⟩= 1\nm⟨b(1)\nℓ+1, b(2)\nℓ+1⟩+ α2P + α(Q + R),\nwhere\nP = 1\nm\n2\nm⟨W ⊤\nℓD(1)\nℓV ⊤\nℓ\nb(1)\nℓ+1\n√m , W ⊤\nℓD(2)\nℓV ⊤\nℓ\nb(2)\nℓ+1\n√m ⟩,\nQ = 1\nm\nr\n1\nm\nr\n2\nm⟨W ⊤\nℓD(1)\nℓV ⊤\nℓb(1)\nℓ+1, b(2)\nℓ+1⟩,\nR = 1\nm\nr\n1\nm\nr\n2\nm⟨W ⊤\nℓD(2)\nℓV ⊤\nℓb(2)\nℓ+1, b(1)\nℓ+1⟩.\nThen\n\f\f\f 1\nm⟨b(1)\nℓ, b(2)\nℓ⟩−(Bℓ+1(x(1), x(2)) + α2Bℓ+1(x(1), x(2))Γσ′(Kℓ−1)(x(1), x(2))\n\f\f\f\n≤\n\f\f\f 1\nm⟨b(1)\nℓ+1, b(2)\nℓ+1⟩−Bℓ+1(x(1), x(2))\n\f\f\f + α2\f\f\fP −Bℓ+1(x(1), x(2))Γσ′(Kℓ−1)(x(1), x(2))\n\f\f\f + α|Q| + α|R|\n≤τ + α2\f\f\fP −2\nm⟨D(1)\nℓV ⊤\nℓ\nb(1)\nℓ+1\n√m , D(2)\nℓV ⊤\nℓ\nb(2)\nℓ+1\n√m ⟩\n\f\f\f\n+ α2\f\f\f 2\nm⟨D(1)\nℓV ⊤\nℓ\nb(1)\nℓ+1\n√m , D(2)\nℓV ⊤\nℓ\nb(2)\nℓ+1\n√m ⟩−⟨b(1)\nℓ+1\n√m , b(2)\nℓ+1\n√m ⟩2\nm Tr(D(1)\nℓD(2)\nℓ)\n\f\f\f\n+ α2\f\f\f⟨b(1)\nℓ+1\n√m , b(2)\nℓ+1\n√m ⟩−Bℓ+1(x(1), x(2))\n\f\f\f\n\f\f\f 2\nm Tr(D(1)\nℓD(2)\nℓ)\n\f\f\f\n+ α2\f\f\fBℓ+1(x(1), x(2))\n\f\f\f\n\f\f\f 2\nm Tr(D(1)\nℓD(2)\nℓ) −Γσ′(Kℓ−1)(x(1), x(2))\n\f\f\f\n+ α|Q| + α|R|.\nIn Lemma 9 and Lemma 10, set ˜ϵ = cLγ−1τ, ϵ = cτ, δ = ˜δ = δ′ = δ4/5L. When m ≥\nmax{ C\nτ 2 (1 + log 30L\nδ4 ), C\nτ 2 log 40L2\nδ4 , C\nτ 2 L2−2γ log 80L2\nδ4 , cL2−2γ log 80L2\nδ4 }, with probability at least\n1−δ4\nL , the results of Lemma 9 and Lemma 10 hold. Then for all (x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, ˜x)},\n\f\f\f 1\nm⟨b(1)\nℓ, b(2)\nℓ⟩−Bℓ(x(1), x(2))\n\f\f\f ≤τ + α2cτ + α2cτ + α22τ + α2eϵ2 + 2αcLa−1τ\n≤τ(1 + O(1/L)).\n(Set ϵ2 ≤cτ.)\nBy taking union bound, with probability at least 1 −δ4, we have for all ℓ= 1, 2, · · · , L,\n∥ˆΘℓ+1(x, ˜x) −Θℓ+1(x, ˜x)∥max ≤(1 + O(1/L))Lϵ4 ≤Cϵ4.\nMeanwhile, we have for all (x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, ˜x)} and ℓ= 1, · · · , L,\n\f\f\f\f\n2\nm⟨D(1)\nℓV ⊤\nℓ\nb(1)\nℓ+1\n√m , D(2)\nℓV ⊤\nℓ\nb(2)\nℓ+1\n√m ⟩−Bℓ+1(x(1), x(2))Γσ′(Kℓ−1)(x(1), x(2))\n\f\f\f\f ≤(2+c)τ+eϵ2 ≤Cϵ4.\nStep 5. Summary\n18\nUsing previous results, for all ℓ, we have\n\f\f\f 1\nα2 ⟨∇Vℓf, ∇Vℓ˜f⟩−Bℓ+1Γσ(Kℓ−1)\n\f\f\f\n≤\n\f\f\f 1\nm⟨bℓ+1,˜bℓ+1⟩−Bℓ+1\n\f\f\f · |⟨φWℓ(xℓ−1), φWℓ(˜xℓ−1)⟩| + |Bℓ+1| · |⟨φWℓ(xℓ−1), φWℓ(˜xℓ−1)⟩−Γσ(Kℓ−1)|\n≤Cϵ4 + Cϵ2\n2,\nand\f\f\f 1\nα2 ⟨∇Wℓf, ∇Wℓ˜f⟩−Kℓ−1Bℓ+1Γσ′(Kℓ−1)\n\f\f\f\n≤\n\f\f\f 1\nm⟨xℓ−1, ˜xℓ−1⟩−Kℓ−1\n\f\f\f ·\n\f\f\f 2\nm\n˜b⊤\nℓ+1VℓeDℓDℓV ⊤\nℓbℓ+1\n\f\f\f + |Kℓ−1| ·\n\f\f\f 2\nm\n˜b⊤\nℓ+1VℓeDℓDℓV ⊤\nℓbℓ+1 −Bℓ+1Γσ′(Kℓ−1)\n\f\f\f\n≤Cϵ2\n2 + Cϵ4.\nTo sum up, by choosing ϵ4 = cϵ0, ϵ2 = cϵ4, and δ1 = δ2 = δ3 = δ4 = δ0/4, then with probability at\nleast 1 −δ0, when\nm ≥C\nϵ4\n0\nL2−2γ\n\u0012\nlog 320(L2 + 1)\nδ0\n+ 1\n\u0013\n≥max\n\u001a\nc log 16L\nδ0\n, C\nϵ4\n0\nL2−2γ log 144(L + 1)\nδ0\n, C\nϵ2\n0\nlog 24L\nδ0\n,\nC\nϵ2\n0\nlog 8L\nδ0\n, C\nϵ2\n0\n(1 + log 120L\nδ0\n), C\nϵ2\n0\nlog 160L2\nδ0\n, C\nϵ2\n0\nL2−2γ log 320L2\nδ0\n, cL2−2γ log 320L2\nδ4\n0\n\u001b\n,\nthe desired results hold.\nC\nProofs of the Lemmas\nC.1\nSupporting lemmas\nLemma 11. Deﬁne G = [φWℓ(xℓ−1), φWℓ(˜xℓ−1)], and Π⊥\nG as the orthogonal projection onto the\northogonal complement of the column space of G. when m ≥1 + log 6\nδ, the following holds with\nprobability at least 1 −δ for all (x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, ˜x)},\n\f\f\f\f\n2\nm\nb(1)\nℓ+1\n√m\n⊤\nVℓΠ⊥\nGD(1)\nℓD(2)\nℓΠ⊥\nGV ⊤\nℓ\nb(2)\nℓ+1\n√m −⟨b(1)\nℓ+1\n√m , b(1)\nℓ+1\n√m ⟩2\nm Tr(D(1)\nℓD(2)\nℓ)\n\f\f\f\f ≤(4 + 4\n√\n2)M\ns\n1 + log 6\nδ\nm\n,\nwhere\nM = max\n(\n∥bℓ+1∥2\nm\n, ∥˜bℓ+1∥2\nm\n)\n.\nproof of Lemma 11. We\nprove\nthe\nlemma\non\nany\nrealization\nof\n(A, W1, V1, · · · , Wℓ−1, Vℓ−1, Wℓ, Wℓ+1, Vℓ+1, · · · , WL, VL, v), VℓφWℓ(xℓ−1) and VℓφWℓ(˜xℓ−1),\nand consider the remaining randomness of Vℓ. In this case, Dℓ, eDℓ, bℓ+1 and ˜bℓ+1 are ﬁxed.\nOne can show that conditioned on the realization of VℓG (whose “degree of freedom” is 2m), VℓΠ⊥\nG\nis identically distributed as eVℓΠ⊥\nG, where eVℓis an i.i.d. copy of Vℓ. The remaining m2 −2m “degree\nof freedom” is enough for a good concentration. For the proof of this result, we refer the readers to\nLemma E.3 in [22].\nDenote T = Π⊥\nGD(1)\nℓD(2)\nℓΠ⊥\nG,\nS =\n\n\neV ⊤\nℓ\nb(1)\nℓ+1\n√m\neV ⊤\nℓ\nb(2)\nℓ+1\n√m\n\n.\nWe know that S is a 2m-dimensional Gaussian random vector, and\nS ∼N\n\n0,\n\n⟨\nb(1)\nℓ+1\n√m ,\nb(1)\nℓ+1\n√m ⟩Im\n⟨\nb(1)\nℓ+1\n√m ,\nb(2)\nℓ+1\n√m ⟩Im\n⟨\nb(2)\nℓ+1\n√m ,\nb(1)\nℓ+1\n√m ⟩Im\n⟨\nb(2)\nℓ+1\n√m ,\nb(2)\nℓ+1\n√m ⟩Im\n\n\n\n.\n19\nThen there exists a matrix P ∈R2m×2m, such that\nPP ⊤=\n\n⟨\nb(1)\nℓ+1\n√m ,\nb(1)\nℓ+1\n√m ⟩Im\n⟨\nb(1)\nℓ+1\n√m ,\nb(2)\nℓ+1\n√m ⟩Im\n⟨\nb(2)\nℓ+1\n√m ,\nb(1)\nℓ+1\n√m ⟩Im\n⟨\nb(2)\nℓ+1\n√m ,\nb(2)\nℓ+1\n√m ⟩Im\n\n,\nand S\nd= Pξ, ξ ∼N(0, I2m).\nThus\nb(1)\nℓ+1\n√m\n⊤\neVℓΠ⊥\nGD(1)\nℓD(2)\nℓΠ⊥\nG eV ⊤\nℓ\nb(2)\nℓ+1\n√m\nd= ξ⊤P ⊤\n\u0014\nIm\n0\n\u0015⊤\nT\n\u0014\n0\nIm\n\u0015\nPξ = 1\n2ξ⊤P ⊤\n\u0014\n0\nT\nT\n0\n\u0015\nPξ.\nWe have\n\r\r\r\r\n1\n2P ⊤\n\u0014\n0\nT\nT\n0\n\u0015\nP\n\r\r\r\r ≤1\n2\n\r\rP ⊤\r\r · ∥P∥·\n\r\r\r\r\n\u0014\n0\nT\nT\n0\n\u0015\r\r\r\r\n= 1\n2\n\r\rPP ⊤\r\r · ∥T∥\n≤1\n2\n\r\r\r\r\r\r\n\n⟨\nb(1)\nℓ+1\n√m ,\nb(1)\nℓ+1\n√m ⟩Im\n⟨\nb(1)\nℓ+1\n√m ,\nb(2)\nℓ+1\n√m ⟩Im\n⟨\nb(2)\nℓ+1\n√m ,\nb(1)\nℓ+1\n√m ⟩Im\n⟨\nb(2)\nℓ+1\n√m ,\nb(2)\nℓ+1\n√m ⟩Im\n\n\n\r\r\r\r\r\r\n\r\rΠ⊥\nG\n\r\r\n\r\r\rD(1)\nℓ\n\r\r\r\n\r\r\rD(2)\nℓ\n\r\r\r\n\r\rΠ⊥\nG\n\r\r\n≤\n⟨\nb(1)\nℓ+1\n√m ,\nb(1)\nℓ+1\n√m ⟩+ ⟨\nb(2)\nℓ+1\n√m ,\nb(2)\nℓ+1\n√m ⟩\n2\n≤M.\nAnd\n\r\r\r\r\n1\n2P ⊤\n\u0014\n0\nT\nT\n0\n\u0015\nP\n\r\r\r\r\nF\n≤\n√\n2mM.\nThen by the Hanson-Wright Inequality for Gaussian chaos [40], we have with probability at least\n1 −δ/3,\n2\nm\n\f\f\f\f\f\f\nb(1)\nℓ+1\n√m\n⊤\neVℓΠ⊥\nGD(1)\nℓD(2)\nℓΠ⊥\nG eV ⊤\nℓ\nb(2)\nℓ+1\n√m −EeVℓ\n\nb(1)\nℓ+1\n√m\n⊤\neVℓΠ⊥\nGD(1)\nℓD(2)\nℓΠ⊥\nG eV ⊤\nℓ\nb(2)\nℓ+1\n√m\n\n\n\f\f\f\f\f\f\n≤4\nm\n \n√\n2mM\nr\nlog 6\nδ + M log 6\nδ\n!\n,\nFurthermore, we have\nEeVℓ\n\nb(1)\nℓ+1\n√m\n⊤\neVℓΠ⊥\nGD(1)\nℓD(2)\nℓΠ⊥\nG eV ⊤\nℓ\nb(2)\nℓ+1\n√m\n\n= ⟨b(1)\nℓ+1\n√m , b(1)\nℓ+1\n√m ⟩Tr(Π⊥\nGD(1)\nℓD(2)\nℓ).\nThus\n\f\f\f\f\n2\nmEeVℓ\n\nb(1)\nℓ+1\n√m\n⊤\neVℓΠ⊥\nGD(1)\nℓD(2)\nℓΠ⊥\nG eV ⊤\nℓ\nb(2)\nℓ+1\n√m\n\n−⟨b(1)\nℓ+1\n√m , b(1)\nℓ+1\n√m ⟩2\nm Tr(D(1)\nℓD(2)\nℓ)\n\f\f\f\f\n= 2\nm\n\f\f\f\f⟨b(1)\nℓ+1\n√m , b(1)\nℓ+1\n√m ⟩Tr(ΠGD(1)\nℓD(2)\nℓ)\n\f\f\f\f\n≤2\nmM Tr(ΠGD(1)\nℓD(2)\nℓΠG)\n≤4\nmM.\nBy taking union bound, we have with probability at least 1 −δ, for all (x(1), x(2))\n∈\n{(x, x), (x, ˜x), (˜x, ˜x)},\n\f\f\f\f\n2\nm\nb(1)\nℓ+1\n√m\n⊤\nVℓΠ⊥\nGD(1)\nℓD(2)\nℓΠ⊥\nGV ⊤\nℓ\nb(2)\nℓ+1\n√m −⟨b(1)\nℓ+1\n√m , b(1)\nℓ+1\n√m ⟩2\nm Tr(D(1)\nℓD(2)\nℓ)\n\f\f\f\f\n≤4\nm\n \n√\n2mM\nr\nlog 6\nδ + M log 6\nδ\n!\n+ 4\nmM\n≤(4 + 4\n√\n2)M\ns\n1 + log 6\nδ\nm\n,\n20\nwhere the last inequality holds when m ≥1 + log 6\nδ .\nLemma 12 (Norm controls of bℓ+1). Assume the following inequalities hold simultaneously for all\nℓ= 1, 2, · · · , L\n\r\r\r 1\n√mWℓ\n\r\r\r ≤C,\n\r\r\r 1\n√mVℓ\n\r\r\r ≤C.\nThen for any ﬁxed input x, 1 ≤ℓ≤L and u ∈Rm, when\nm ≥cL2−2γ log 2L\nδ′ ,\nwith probability at least 1 −δ′ over the randomness of Wℓ+1, Vℓ+1, · · · , WL, VL, v, we have\n|⟨u, bℓ+1⟩| ≤C′∥u∥\nr\nlog 2L\nδ′ .\nproof of Lemma 12. Denote uℓ= u, and\nui+1 = α\nr\n1\nm\nr\n2\nmVi+1Di+1Wi+1ui + ui,\ni = ℓ, ℓ+ 1, · · · , L −1.\nOne can show that ⟨u, bℓ+1⟩= ⟨v, uL⟩. Next we show that ∥ui+1∥= (1 + O( 1\nL))∥ui∥with high\nprobability. First write\n∥ui+1∥2 = ∥ui∥2 + α2\n\r\r\r\r\nr\n1\nm\nr\n2\nmVi+1Di+1Wi+1ui\n\r\r\r\r\n2\n+ 2α\n*\nui,\nr\n1\nm\nr\n2\nmVi+1Di+1Wi+1ui\n+\n.\nBy the assumption we have\r\r\r\r\nr\n2\nmDi+1Wi+1ui\n\r\r\r\r ≤\n√\n2C∥ui∥,\n\r\r\r\r\nr\n1\nm\nr\n2\nmVi+1Di+1Wi+1ui\n\r\r\r\r ≤\n√\n2C2∥ui∥.\nWith probability at least 1 −δ′/L over the randomness of Vi+1, we have\n\r\r\r\r\r⟨ui,\nr\n1\nm\nr\n2\nmVi+1Di+1Wi+1ui⟩\n\r\r\r\r\r ≤∥ui∥·\n\r\r\r\r\nr\n2\nmDi+1Wi+1ui\n\r\r\r\r\ns\nc log 2L\nδ′\nm\n.\nThen when\nm ≥cL2−2γ log 2L\nδ′ ,\nwe have\n∥ui+1∥2 = ∥ui∥2 + α2\n\r\r\r\r\nr\n1\nm\nr\n2\nmVi+1Di+1Wi+1ui\n\r\r\r\r\n2\n+ 2α⟨ui,\nr\n1\nm\nr\n2\nmVi+1Di+1Wi+1ui⟩\n≤(1 + 2C4/L)∥ui∥2 + 2α\n√\n2C∥ui∥2\ns\nc log 2L\nδ′\nm\n≤(1 + 2C4/L + 2\n√\n2C/L)∥ui∥2 = (1 + O(1/L))∥ui∥2.\nThen with probability at least 1 −δ′(L −1)/L we have ∥uL∥≤C∥u∥. Finally the result holds from\nthe standard concentration bound for Gaussian random variables [39].\nC.2\nProofs of Lemma 9\nproof of Lemma 9. By the assumption, we have\n1\nm∥bℓ+1∥2 ≤Bℓ+1(x, x) + 1 ≤4.\nSimilarly,\n1\nm∥˜bℓ+1∥2 ≤4. Then by Lemma 11, when m ≥\nC\nϵ2 (1 + log 6\nδ ), we have for all\n(x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, ˜x)},\n\f\f\f\f\n2\nm\nb(1)\nℓ+1\n√m\n⊤\nVℓΠ⊥\nGD(1)\nℓD(2)\nℓΠ⊥\nGV ⊤\nℓ\nb(2)\nℓ+1\n√m −⟨b(1)\nℓ+1\n√m , b(1)\nℓ+1\n√m ⟩2\nm Tr(D(1)\nℓD(2)\nℓ)\n\f\f\f\f ≤cϵ.\nSpeciﬁcally, we have\n\r\r\r\r\nr\n2\nm\nbℓ+1\n√m\n⊤\nVℓΠ⊥\nGDℓ\n\r\r\r\r ≤\nr\ncϵ + 2\nm Tr(Dℓ) 1\nm∥bℓ+1∥2 ≤O(1),\n21\nand similarly\n\r\r\r\r\nr\n2\nm\n˜bℓ+1\n√m\n⊤\nVℓΠ⊥\nG eDℓ\n\r\r\r\r ≤O(1).\nNext we bound\n\r\r\r\r\nbℓ+1\n√m\n⊤\nVℓΠG\n\r\r\r\r.\nNotice that ΠG is a orthogonal projection onto the column space of G, which is at most 2-dimension.\nOne can write ΠG = u1u⊤\n1 + u2u⊤\n2 , where ∥ui∥= 1 or 0. By Lemma 12, ﬁxing u1, u2 and Vℓ, w.p\ngreater than 1 −δ′ over the randomness of Wℓ+1, Vℓ+1, · · · , WL, VL, v, we have\n\f\f\f\fb⊤\nℓ+1\n1\n√mVℓui\n\f\f\f\f ≤C′′\nr\nlog 8L\nδ′ ,\nand\n\f\f\f\f˜b⊤\nℓ+1\n1\n√mVℓui\n\f\f\f\f ≤C′′\nr\nlog 8L\nδ′ ,\nfor both i = 1, 2 when\nm ≥cL2−2γ log 8L\nδ′ .\nTherefore\n\r\r\r\r\nbℓ+1\n√m\n⊤\nVℓΠG\n\r\r\r\r,\n\r\r\r\r\n˜bℓ+1\n√m\n⊤\nVℓΠG\n\r\r\r\r ≤O\n\u0012r\nlog 8L\nδ′\n\u0013\n.\nFinally, using Im = ΠG + Π⊥\nG, we have\n\f\f\f\f\n2\nm\nb(1)\nℓ+1\n√m\n⊤\nVℓD(1)\nℓD(2)\nℓV ⊤\nℓ\nb(2)\nℓ+1\n√m −⟨b(1)\nℓ+1\n√m , b(2)\nℓ+1\n√m ⟩2\nm Tr(D(1)\nℓD(2)\nℓ)\n\f\f\f\f\n≤\n\f\f\f\f\n2\nm\nb(1)\nℓ+1\n√m\n⊤\nVℓΠ⊥\nGD(1)\nℓD(2)\nℓΠ⊥\nGV ⊤\nℓ\nb(2)\nℓ+1\n√m −⟨b(1)\nℓ+1\n√m , b(1)\nℓ+1\n√m ⟩2\nm Tr(D(1)\nℓD(2)\nℓ)\n\f\f\f\f\n+\nr\n2\nm\n\f\f\f\f\nb(1)\nℓ+1\n√m\n⊤\nVℓΠGD(1)\nℓD(2)\nℓΠ⊥\nGV ⊤\nℓ\nb(2)\nℓ+1\n√m\nr\n2\nm\n\f\f\f\f\n+\nr\n2\nm\n\f\f\f\f\nr\n2\nm\nb(1)\nℓ+1\n√m\n⊤\nVℓΠ⊥\nGD(1)\nℓD(2)\nℓΠGV ⊤\nℓ\nb(2)\nℓ+1\n√m\n\f\f\f\f\n+ 2\nm\n\f\f\f\f\nb(1)\nℓ+1\n√m\n⊤\nVℓΠGD(1)\nℓD(2)\nℓΠGV ⊤\nℓ\nb(2)\nℓ+1\n√m\n\f\f\f\f\n≤cϵ +\nr\n2\nmO\n\u0012r\nlog 8L\nδ′\n\u0013\n+ 2\nmO\n\u0012\nlog 8L\nδ′\n\u0013\n≤ϵ.\nThe last inequality holds when m ≥C\nϵ2 log 8L\nδ′ .\nC.3\nProof of Lemma 10\nproof of Lemma 10. The ﬁrst part of the proof is essentially the same as Lemma 9. Deﬁne\ndℓ+1 = Dℓ\n1\n√mV ⊤\nℓ\nbℓ+1\n√m ,\n˜dℓ+1 = eDℓ\n1\n√mV ⊤\nℓ\n˜bℓ+1\n√m .\nWe know that dℓ+1 and ˜dℓ+1 depend on Wℓonly through Wℓxℓ−1 and Wℓ˜xℓ−1.\nLet H =\n[xℓ−1, ˜xℓ−1]. Then\n\f\f\f 2\nm⟨W ⊤\nℓd(1)\nℓ+1, W ⊤\nℓd(2)\nℓ+1⟩−2⟨d(1)\nℓ+1, d(2)\nℓ+1⟩\n\f\f\f\n≤\n\f\f\f 2\nm⟨Π⊥\nHW ⊤\nℓd(1)\nℓ+1, Π⊥\nHW ⊤\nℓd(2)\nℓ+1⟩−2⟨d(1)\nℓ+1, d(2)\nℓ+1⟩\n\f\f\f +\n\f\f\f 2\nm⟨ΠHW ⊤\nℓd(1)\nℓ+1, Π⊥\nHW ⊤\nℓd(2)\nℓ+1⟩\n\f\f\f\n+\n\f\f\f 2\nm⟨Π⊥\nHW ⊤\nℓd(1)\nℓ+1, ΠHW ⊤\nℓd(2)\nℓ+1⟩\n\f\f\f +\n\f\f\f 2\nm⟨ΠHW ⊤\nℓd(1)\nℓ+1, ΠHW ⊤\nℓd(2)\nℓ+1⟩\n\f\f\f.\nSince ∥dℓ+1∥, ∥˜dℓ+1∥= O(1), similar to Lemma 11, when m ≥1 + log 6\nδ, w.p at least 1 −δ we\nhave\n\f\f\f 2\nm⟨Π⊥\nHW ⊤\nℓd(1)\nℓ+1, Π⊥\nHW ⊤\nℓd(2)\nℓ+1⟩−2⟨d(1)\nℓ+1, d(2)\nℓ+1⟩\n\f\f\f ≤O\n\u0012s\n1 + log 6\nδ\nm\n\u0013\n,\n22\nand\n\r\r\r\nr\n2\nmΠ⊥\nHW ⊤\nℓd(i)\nℓ+1\n\r\r\r = O(1),\ni = 1, 2,\nUsing the same argument as in the proof of Lemma 9, we decompose ΠH into two vectors w1 and\nw2, whose randomness comes from W1, V1, · · · , Wℓ−1, Vℓ−1. By writing\nw⊤\ni W ⊤\nℓd(i)\nℓ+1 = ⟨b(i)\nℓ+1,\n1\n√mVℓD(i)\nℓ\n1\n√mWℓwi⟩,\nwe can also apply Lemma 12. Then we conclude that w.p. greater than 1 −δ′ over the randomness of\nv, we have\n∥ΠHW ⊤\nℓdℓ+1∥, ∥ΠHW ⊤\nℓ˜dℓ+1∥= O\n\u0012r\nlog 8L\nδ′\n\u0013\n,\nwhen\nm ≥cL2−2γ log 8L\nδ′ .\nThen exactly the same result of Lemma 9 holds.\nFor the second part, notice that\n1\nm\nr\n1\nm\nr\n2\nm⟨W ⊤\nℓD(1)\nℓV ⊤\nℓb(1)\nℓ+1, b(2)\nℓ+1⟩=\nr\n2\nm⟨W ⊤\nℓD(1)\nℓ\nr\n1\nmV ⊤\nℓ\nb(1)\nℓ+1\n√m , b(2)\nℓ+1\n√m ⟩\n=\nr\n2\nm⟨W ⊤\nℓd(1)\nℓ+1, b(2)\nℓ+1\n√m ⟩\n=\nr\n2\nm⟨Π⊥\nHW ⊤\nℓd(1)\nℓ+1, b(2)\nℓ+1\n√m ⟩+\nr\n2\nm⟨ΠH\n1\n√mW ⊤\nℓd(1)\nℓ+1, b(2)\nℓ+1⟩.\nConditioned on xℓ−1, ˜xℓ−1, Wℓxℓ−1, and Wℓ˜xℓ−1, Wℓis independent of bℓ+1,˜bℓ+1, dℓ+1, and ˜dℓ+1.\nFurthermore, we have Π⊥\nHW ⊤\nℓ=d Π⊥\nHc\nW ⊤\nℓ, where c\nWℓis an i.i.d. copy of Wℓ. Then for the ﬁrst term,\nwith probability at least 1 −˜δ/2, we have for all (x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, x), (˜x, ˜x)},\n\f\f\f\f\nr\n2\nm⟨Π⊥\nHW ⊤\nℓd(1)\nℓ+1, b(2)\nℓ+1\n√m ⟩\n\f\f\f\f ≤\n\r\r\r\r\rΠ⊥\nH\nb(2)\nℓ+1\n√m\n\r\r\r\r\r ∥d(1)\nℓ+1∥\ns\n2c log 16\n˜δ\nm\n≤O\n\u0012s\nlog 16\n˜δ\nm\n\u0013\n.\nFor the second term, write ΠH = w1w⊤\n1 + w2w⊤\n2 , where ∥wi∥= 1 or 0. Then by Lemma 12,\nwith probability at least 1 −˜δ/2, for all (x(1), x(2)) ∈{(x, x), (x, ˜x), (˜x, x), (˜x, ˜x)}, when m ≥\ncL2−2γ log 16L\n˜δ , we have\n\f\f\f\f\nr\n2\nm⟨wiw⊤\ni\n1\n√mW ⊤\nℓd(1)\nℓ+1, b(2)\nℓ+1⟩\n\f\f\f\f =\n\f\f\f\f\nr\n2\nmw⊤\ni\n1\n√mW ⊤\nℓd(1)\nℓ+1⟨wi, b(2)\nℓ+1⟩\n\f\f\f\f\n≤\nr\n2\nm∥wi∥\n\r\r\r 1\n√mW ⊤\nℓ\n\r\r\r∥d(1)\nℓ+1∥\n\f\f\f⟨wi, b(2)\nℓ+1⟩\n\f\f\f\n≤O\n\n\ns\nlog 16L\n˜δ\nm\n\n.\nD\nProof of Theorem 5\nProof. For x, ˜x ∈SD−1, we have Kℓ(x, x) = Kℓ(˜x, ˜x) = 1 for all ℓ. Hence we only need to study\nwhen x ̸= ˜x. Note we have\nKℓ(x, ˜x) = Γσ(Kℓ−1)(x, ˜x) = ˆσ(Kℓ−1(x, ˜x)), and Γσ′(Kℓ)(x, ˜x) = bσ′(Kℓ(x, ˜x)).\nFor simplicity, we use Kℓto denote Kℓ(x, ˜x), where x ̸= ˜x and x, ˜x ∈SD−1.\nRecall that\nˆσ(ρ) =\np\n1 −ρ2 +\n\u0000π −cos−1(ρ)\n\u0001\nρ\nπ\n, and bσ′(ρ) = π −cos−1(ρ)\nπ\n.\nHence we have ˆσ(1) = 1, Kℓ−1 ≤ˆσ(Kℓ−1) = Kℓ, (ˆσ)′(ρ) = bσ′(ρ) ∈[0, 1], and ( bσ′)\n′(ρ) ≥0.\nThen ˆσ is a convex function.\n23\nSince {Kℓ} is an increasing sequence and |Kℓ| ≤1, we have Kℓconverges as ℓ→∞. Taking the\nlimit of both sides of ˆσ(Kℓ−1) = Kℓ, we have Kℓ→1 as ℓ→∞.\nFor Kℓ, we also have\nKℓ= ˆσ(Kℓ−1) =\nq\n1 −K2\nℓ−1 + (π −cos−1(Kℓ−1))Kℓ−1\nπ\n= Kℓ−1 +\nq\n1 −K2\nℓ−1 −cos−1(Kℓ−1)Kℓ−1\nπ\n.\nLet eℓ= 1 −Kℓ, we can easily check that\neℓ−1 −e3/2\nℓ−1\nπ\n≤eℓ≤eℓ−1 −2\n√\n2e3/2\nℓ−1\n3π\n.\n(19)\nHence as eℓ→0, we have\neℓ\neℓ−1 →1, which implies {Kℓ} converges sublinearly.\nAssume eℓ= C\nℓp + O(ℓ−(p+1)). By taking the assumption into (19) and comparing the highest order\nof both sides, we have p = 2.\nThus ∃C, s.t. |1 −Kℓ| ≤C\nℓ2 , i.e. the convergence rate of Kℓis O\n\u0000 1\nℓ2\n\u0001\n.\nLemma 13. For each K0 < 1, there exists p > 0 and n0 = n0(δ) > 0, such that Kn ≤1 −\n9π2\n2(n+n0)2+ log(L)p\nL\n, ∀n = 0, . . . , L, when L is large.\nProof. First, solve K0 ≤1 −\n9π2\n2n2+ log(L)p\nL\n. Then we can choose n0 ≥\nq\n9π2\n2δ ≥\nq\n9π2\n2(1−K0), which is\nindependent of L and n. For the rest of the proof, without loss of generality, we just use n instead of\nn+n0. Also for small δ( when δ is not small enough we can pick a small δ0 < δ and let n0 ≥\nq\n9π2\n2δ0 ),\nwe have\n9π2\n2(n+n0)2+ log(L)p\nL\n≤δ(or δ0) which is also small.\nLet Kn = 1 −ϵ. Then, when ϵ is small, we have\nKn+1 −Kn = ˆσ(Kn) −Kn = O(ϵ3/2).\nAlso, we have \n1 −\n9π2\n2(n + 1)2+ log(L)p\nL\n!\n−\n\u0012\n1 −\n9π2\n2n2+ log(L)p\nL\n\u0013\n= O\n\u0012\n1\nn3+ log(L)p\nL\n\u0013\n≥O\n \u0012\n1\nn2+ log(L)p\nL\n\u00133/2!\n= O\n\u0012\n1\nn3+ 3 log(L)p\n2L\n\u0013\n.\nOverall, we want an upper bound for Kn and from the above we only know that Kn is of order\n1 −O(n−2) but this order may hide some terms of logarithmic order. Hence we use the order\n1 −O(n−(2+ϵ)) to provide an upper bound of Kn. Here log(L)p\nL\nis constructed for the convenience\nof the rest of the proof.\nLet N0 = N0(L) be the solution of\ncos\n\nπ\n\n1 −\n\u0012n + 1\nn + 2\n\u00133−log(L)2\nL\n\n\n\n= ˆσ\n\ncos\n\nπ\n\n1 −\n\u0012\nn\nn + 1\n\u00133−log(L)2\nL\n\n\n\n\n\n,\nwhere for N0 < n < NL with some NL, we have\ncos\n\nπ\n\n1 −\n\u0012n + 1\nn + 2\n\u00133−log(L)2\nL\n\n\n\n≥ˆσ\n\ncos\n\nπ\n\n1 −\n\u0012\nn\nn + 1\n\u00133−log(L)2\nL\n\n\n\n\n\n.\nOne can check by series expansion that N0 = N0(L) ≤5\nL\nlog(L)2 .\nNext we would like to ﬁnd n such that\nKn = cos\n\n\nπ\n\n\n1 −\n \n5\nL\nlog(L)2\n5\nL\nlog(L)2 + 1\n!3−log(L)2\nL\n\n\n\n\n\n.\n24\nBy series expansion, we know\ncos\n\n\nπ\n\n\n1 −\n \n5\nL\nlog(L)2\n5\nL\nlog(L)2 + 1\n!3−log(L)2\nL\n\n\n\n\n\n≥1 −\n9π2\n2\n\u0010\n5L\nlog(L)2\n\u00112 .\nThen it sufﬁces to solve\n1 −\n9π2\n2(\n5L\nlog(L)2 )2 ≥1 −\n9π2\n2n2+ log(L)p\nL\n≥Kn, i.e., n2+ log(L)p\nL\n≤\n\u0012\n5L\nlog(L)2\n\u00132\n.\n(20)\nLemma 14. When q > p −1, we have n ≲\n5L\nlog(L)2 −log(L)q satisﬁes (20).\nProof. If the condition above holds, we have\nn2+ log(L)p\nL\n≤\n\u0012\n5L\nlog(L)2 −log(L)q\n\u00132+ log(L)p\nL\n,\nwhich is\nn1+ log(L)p\n2L\n≤\n\u0012\n5L\nlog(L)2 −log(L)q\n\u0013 \u0012\n5L\nlog(L)2 −log(L)q\n\u0013 log(L)p\n2L\n≤\n\u0012\n5L\nlog(L)2 −log(L)q\n\u0013  \n1 +\nlog(L)p log(\n5L\nlog(L)2 )\n2L\n!\n=\n5L\nlog(L)2 −log(L)q + 5\n2 log(L)p−2 log\n\u0012\n5L\nlog(L)2\n\u0013\n−1\n2L log(L)p+q log\n\u0012\n5L\nlog(L)2\n\u0013\n,\nwhere\n\u0010\n5L\nlog(L)2 −log(L)q\u0011 log(L)p\n2L\n→1 as L →∞.\nThus we have q > p −1.\nJust pick q = p. Then we have n1+ log(L)p\n2L\n≲\n5L\nlog(L)2 and n ≲\n5L\nlog(L)2 −log(L)p.\nLemma 15. When L is large enough, we have\ncos\n\nπ\n\n1 −\n\u0012\nn\nn + 1\n\u00133+ log(L)2\nL\n\n\n\n≤Kn ≤cos\n\nπ\n\n1 −\n\u0012\nn + log(L)p\nn + log(L)p + 1\n\u00133−log(L)2\nL\n\n\n\n.\nProof. Let F(n) = cos\n \nπ\n \n1 −\n\u0010\nn+log(L)p\nn+log(L)p+1\n\u00113−log(L)p\nL\n!!\n.\nFor the right hand side, when n ≳\n5L\nlog(L)2 −log(L)p, we have, by series expansion, F(n + 1) ≥\nˆσ (F(n)). Also, when n ∼aL, where 0 < a ≤1, we have\nF(n + 1) −ˆσ(F(n)) = O\n \n3\n\u00002π2a log10(L) + π2 log8(L)\n\u0001\n2L4 \u0000a log2(L) + 5\n\u00014\n!\n> 0.\nThen for\n5L\nlog(L)2 −log(L)p ≲n ≲L, we have F(n + 1) ≥ˆσ (F(n)) and thus Kn ≤F(n).\nWhen n ≲\n5L\nlog(L)2 −log(L)p, we have F(n + 1) ≤ˆσ (F(n)). Hence Kn ≤F(n).\nFor the left hand side,\ncos\n\nπ\n\n1 −\n\u0012n + 1\nn + 2\n\u00133+ log(L)2\nL\n\n\n\n−ˆσ\n\ncos\n\nπ\n\n1 −\n\u0012\nn\nn + 1\n\u00133+ log(L)2\nL\n\n\n\n\n\n\n∼−27π2\n2n4 −3π2 log(L)2\nn3L\n, ∀n = 1, ..., L.\nHence we have the left hand side.\n25\nFrom Lemma 15, by series expansion, we have\n|1 −Kn| ≤\n\u0010\n3π + π log(L)2\nL\n\u00112\n2n2\n∼9π2\n2n2 ,\nwhen L is large.\nMoreover, we can get\n\u0012\nn\nn + 1\n\u00133+ log(L)2\nL\n≤Γσ′(Kn) ≤\n\u0012\nn + log(L)p\nn + log(L)p + 1\n\u00133−log(L)2\nL\n.\nThen\n\u0012ℓ−1\nL\n\u00133+ log(L)2\nL\n≤\nL\nY\ni=ℓ\nΓσ′(Ki−1) ≤\n\u0012ℓ+ log(L)p −1\nL + log(L)p\n\u00133−log(L)2\nL\n.\nLet N = log(L)p. For the right hand side, if we sum over ℓ, we have\n1\nL\nL\nX\nℓ=1\n\u0012ℓ+ N −1\nL + N\n\u00133−log(L)2\nL\n≤1\nL\nZ L+1\n1\n\u0012x + N −1\nL + N\n\u00133−log(L)2\nL\ndx\n=\n\u0012\n(L + N)4−log(L)2\nL\n−(N)4−log(L)2\nL\n\u0013\nL(L + N)3−log(L)2\nL\n\u0010\n4 −log(L)2\nL\n\u0011\n.\nTaking the limit of both sides, we have\nlim\nL→∞\n1\nL\nL\nX\nℓ=1\n\u0012ℓ+ N −1\nL + N\n\u00133−log(L)2\nL\n≤1\n4.\nSimilarly, by\n1\nL\nL\nX\ni=1\n\u0012ℓ−1\nL\n\u00133+ log(L)2\nL\n≥1\nL\nZ L\n1\n\u0012x −1\nL\n\u00133+ log(L)2\nL\ndx =\n(L −1)4+ log(L)2\nL\n\u0010\n4 + log(L)2\nL\n\u0011\nL4+ log(L)2\nL\n,\nwe have\nlim\nL→∞\n1\nL\nL\nX\ni=1\n\u0012ℓ−1\nL\n\u00133+ log(L)2\nL\n≥1\n4.\nHence,\nlim\nL→∞\n1\nL\nL\nX\nℓ=1\n\u0012ℓ+ N −1\nL + N\n\u00133−log(L)2\nL\n= lim\nL→∞\n1\nL\nL\nX\nℓ=1\n\u0012ℓ−1\nL\n\u00133+ log(L)2\nL\n= lim\nL→∞\n1\nL\nL\nX\nℓ=1\nL\nY\ni=ℓ\nΓσ′(Ki−1) = 1\n4.\nRecall from previous discussion, Kℓ= 1 −O( 1\nℓ2 ). Therefore,\nlim\nL→∞\n1\nL\nL\nX\nℓ=1\nKℓ−1\nL\nY\ni=ℓ\nΓσ′(Ki−1) = 1\n4.\nAlso, when L is large, we have\n\u0012\n(L + N)4−log(L)2\nL\n−(N)4−log(L)2\nL\n\u0013\nL(L + N)3−log(L)2\nL\n\u0010\n4 −log(L)2\nL\n\u0011\n> 1\n4 >\n(L −1)4+ log(L)2\nL\n\u0010\n4 + log(L)2\nL\n\u0011\nL4+ log(L)2\nL\n.\nHence we can estimate the convergence rate of the normalized kernel\n\f\f\f\f\n1\nL\nL\nX\nℓ=1\nKℓ−1\nL\nY\ni=ℓ\nΓσ′(Ki−1) −1\n4\n\f\f\f\f =\n\f\f\f\f\n1\nL\nL\nX\nℓ=1\n \nKℓ−1\n L\nY\ni=ℓ\nΓσ′(Ki−1) −1\n4\n!\n+ 1\n4(Kℓ−1 −1)\n! \f\f\f\f\n26\n≤\n\f\f\f\f\n1\nL\nL\nX\nℓ=1\nL\nY\ni=ℓ\nΓσ′(Ki−1) −1\n4\n\f\f\f\f + 1\n4\n\f\f\f\f\n1\nL\nL\nX\nℓ=1\n(Kℓ−1 −1)\n\f\f\f\f\n≤\n\f\f\f\f\f\f\f\f\n\u0012\n(L + N)4−log(L)2\nL\n−(N)4−log(L)2\nL\n\u0013\nL(L + N)3−log(L)2\nL\n\u0010\n4 −log(L)2\nL\n\u0011\n−\n(L −1)4+ log(L)2\nL\n\u0010\n4 + log(L)2\nL\n\u0011\nL4+ log(L)2\nL\n\f\f\f\f\f\f\f\f\n+ 1\n4\n\f\f\f\f\n1\nL\nL\nX\ni=1\n(Kℓ−1 −1)\n\f\f\f\f\n≲4 log(L)p + log(L)2\n16L\n= O\n\u0012poly log(L))\nL\n\u0013\nE\nProof of Theorem 6\nProof. We denote Kℓ,L to be the ℓ-th layer of K when the depth is L, which is originally denoted by\nKℓ.\nLet Sℓ,L =\nKℓ,L\n(1+α2)ℓ=\nKℓ,L\n(1+1/L2)ℓand S0 = K0, then Γσ(Kℓ,L) = (1 + α2)ℓˆσ(Sℓ,L) and\nΓσ′(Kℓ,L) = bσ′(Sℓ,L). Hence we can rewrite the recursion to be\nSℓ,L = Sℓ−1,L + α2ˆσ(Sℓ−1,L)\n(1 + α2)\n≥Sℓ−1,L.\n(21)\nMoreover, since Sℓ,L−Sℓ−1,L =\nα2\n1+α2 (ˆσ(Sℓ−1,L)−Sℓ−1,L) and (ˆσ(Sℓ−1,L)−Sℓ−1,L) is decreasing,\nwe can have\nSℓ,L ≤S0 + (ˆσ(S0) −S0)ℓ\nL2\n.\nDenote Pℓ+1,L = Bℓ+1,L(1 + α2)−(L−ℓ) = QL−1\ni=ℓ\n1+α2 b\nσ′(Si,L)\n1+α2\n. Since\n1 −1 + α2 bσ′(Si,L)\n1 + α2\n= α2(1 −bσ′(Si,L))\n1 + α2\n= 1 −bσ′(Si,L)\nL2 + 1\n,\nwe have\n1 −Pℓ+1,L = 1 −\nL−1\nY\ni=ℓ\n\u0012\n1 −1 −bσ′(Si,L)\nL2 + 1\n\u0013\n≤\nL−1\nX\ni=ℓ\n1 −bσ′(Si,L)\nL2 + 1\n= L −ℓ−PL−1\ni=ℓbσ′(Si,L)\nL2 + 1\n,\nwhere ℓ= 1, . . . , L −1. For PL+1,L, we have 1 −PL+1,L = 0.\nThen we can rewrite the normalized kernel to be\nΩL = 1\n2L\nL\nX\nℓ=1\nPℓ+1,L(ˆσ(Sℓ−1,L) + Sℓ−1,L bσ′(Sℓ−1,L)).\nHence we have the bound for each layer\n\f\f\fPℓ+1,L(ˆσ(Sℓ−1,L) + Sℓ−1,L bσ′(Sℓ−1,L)) −(ˆσ(S0) + S0 bσ′(S0))\n\f\f\f\n≤\n\f\f\fPℓ+1,L\n\f\f\f ·\n\f\f\f(ˆσ(Sℓ−1,L) + Sℓ−1,L bσ′(Sℓ−1,L)) −(ˆσ(S0) + S0 bσ′(S0))\n\f\f\f +\n\f\f\fˆσ(S0) + S0 bσ′(S0)\n\f\f\f ·\n\f\f\f1 −Pℓ+1,L\n\f\f\f\n≤\n\f\f\f bσ′(Sℓ−1,L)(Sℓ−1,L −S0)\n\f\f\f +\n\f\f\f bσ′(Sℓ−1,L)Sℓ−1,L −bσ′(S0)S0\n\f\f\f +\n\f\f\fˆσ(S0) + S0 bσ′(S0)\n\f\f\f ·\n\f\f\f1 −Pℓ+1,L\n\f\f\f\n= 2\n\f\f\f bσ′(Sℓ−1,L)(Sℓ−1,L −S0)\n\f\f\f +\n\f\f\fS0( bσ′(Sℓ−1,L) −bσ′(S0))\n\f\f\f +\n\f\f\fˆσ(S0) + S0 bσ′(S0)\n\f\f\f ·\n\f\f\f1 −Pℓ+1,L\n\f\f\f\n≤2 bσ′(Sℓ−1,L)(ˆσ(S0) −S0)ℓ\nL2\n+ |S0|(ˆσ(S0) −S0)(ℓ−1)\nπL2q\n1 −S2\nℓ−1,L\n+\n\f\f\fˆσ(S0) + S0 bσ′(S0)\n\f\f\fL −ℓ−PL−1\ni=ℓbσ′(Si,L)\nL2 + 1\n≤2 bσ′(Sℓ−1,L)(ˆσ(S0) −S0)ℓ\nL2\n+ |S0|(ˆσ(S0) −S0)(ℓ−1)\nπL2q\n1 −S2\nℓ−1,L\n+\n\f\f\fˆσ(S0) + S0 bσ′(S0)\n\f\f\fL −ℓ−(L −ℓ) bσ′(S0)\nL2 + 1\n.\n27\nTherefore we have the bound for the normalized kernel\n\f\f\f\fΩL −1\n2(ˆσ(S0) + S0 bσ′(S0))\n\f\f\f\f\n=\n\f\f\f\f\n1\n2L\nL\nX\nℓ=1\n\u0010\nPℓ+1,L(ˆσ(Sℓ−1,L) + Sℓ−1,L bσ′(Sℓ−1,L))\n\u0011\n−1\n2(ˆσ(S0) + S0 bσ′(S0))\n\f\f\f\f\n≤1\n2L\nL\nX\nℓ=1\n\n2 bσ′(Sℓ−1,L)(ˆσ(S0) −S0)ℓ\nL2\n+ |S0|(ˆσ(S0) −S0)(ℓ−1)\nπL2q\n1 −S2\nℓ−1,L\n\n\n+ 1\n2L\nL−1\nX\nℓ=1\n \f\f\fˆσ(S0) + S0 bσ′(S0)\n\f\f\fL −ℓ−(L −ℓ) bσ′(S0)\nL2 + 1\n!\n≤1\n2L\n \nL + 1\nL\n(ˆσ(S0) −S0) + |S0|(ˆσ(S0) −S0)L(L −1)\n2πL2C\n+\n\f\f\fˆσ(S0) + S0 bσ′(S0)\n\f\f\f\nL(L−1)\n2\n(1 −bσ′(S0))\nL2 + 1\n!\n∼\n\u0012(ˆσ(S0) −S0)\n2\n\u0012\n1 + |S0|\n2πC\n\u0013\n+ 1\n2\n\f\f\fˆσ(S0) + S0 bσ′(S0)\n\f\f\f(1 −bσ′(S0))\n\u0013 1\nL\nwhere C = C(δ) =\np\n1 −(1 −δ)2 and S0 = K0.\n28\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-02-14",
  "updated": "2020-12-22"
}