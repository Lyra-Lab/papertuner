{
  "id": "http://arxiv.org/abs/2002.06979v3",
  "title": "Convergence of End-to-End Training in Deep Unsupervised Contrastive Learning",
  "authors": [
    "Zixin Wen"
  ],
  "abstract": "Unsupervised contrastive learning has gained increasing attention in the\nlatest research and has proven to be a powerful method for learning\nrepresentations from unlabeled data. However, little theoretical analysis was\nknown for this framework. In this paper, we study the optimization of deep\nunsupervised contrastive learning. We prove that, by applying end-to-end\ntraining that simultaneously updates two deep over-parameterized neural\nnetworks, one can find an approximate stationary solution for the non-convex\ncontrastive loss. This result is inherently different from the existing\nover-parameterized analysis in the supervised setting because, in contrast to\nlearning a specific target function, unsupervised contrastive learning tries to\nencode the unlabeled data distribution into the neural networks, which\ngenerally has no optimal solution. Our analysis provides theoretical insights\ninto the practical success of these unsupervised pretraining methods.",
  "text": "arXiv:2002.06979v3  [cs.LG]  30 May 2021\nConvergence of End-to-End Training in Deep Unsupervised\nContrasitive Learning\nZixin Wen∗\nJune 1, 2021\nAbstract\nUnsupervised contrastive learning has gained increasing attention in the latest research\nand has proven to be a powerful method for learning representations from unlabeled data.\nHowever, little theoretical analysis was known for this framework. In this paper, we study the\noptimization of deep unsupervised contrastive learning. We prove that, by applying end-to-\nend training that simultaneously updates two deep over-parameterized neural networks, one\ncan ﬁnd an approximate stationary solution for the non-convex contrastive loss. This result\nis inherently diﬀerent from the existing over-parameterized analysis in the supervised setting\nbecause, in contrast to learning a speciﬁc target function, unsupervised contrastive learning\ntries to encode the unlabeled data distribution into the neural networks, which generally has\nno optimal solution. Our analysis provides theoretical insights into the practical success of\nthese unsupervised pretraining methods.\n1\nIntroduction\nUnsupervised representation learning has achieved enormous success in practical applications,\nespecially in natural language processing, such as the famous word2vec (Mikolov et al., 2013)\nand the groundbreaking advent of BERT (Devlin et al., 2019) and its variants as unsupervised\npretrained language models. Among the unsupervised learning approaches, contrastive learning\nhas gained increasing attention in the deep learning community. More surprisingly, as shown by\nHe et al. (2019), unsupervised contrastively pretrained models can outperform their supervised\ncounterparts in many downstream vision tasks, suggesting that the area of computer vision,\nwhich was previously dominated by supervised pretraining, can also beneﬁt from unsupervised\npretraining. Beyond these conventional approaches, unsupervised contrastive learning has also\nbeen employed in a variety of novel applications such as layer-wise representation learning\n(L¨owe et al., 2019) and representation learning of the actual world (Kipf et al., 2019). These\nstudies together reﬂect the popularity and capability of the unsupervised contrastive methods.\nIn this paper, we view the unsupervised contrastive learning as a pretraining method, where\nthe goal is to obtain pretrained representations that can be transferred to downstream tasks via\nﬁne-tuning. The beneﬁt of doing unsupervised rather than supervised learning is its capability\nof leveraging the unlabeled data, which are more accessible and inexpensive relative to the\nlabeled data. Developing and understanding unsupervised pretraining methods are necessary\ndue to these limitations.\n∗Dept. of Statistics, University of International Business and Economics, Beijing; davidzxwen@icloud.com.\n1\nHowever, besides the plentiful achievements in the practical side of deep unsupervised learn-\ning (and speciﬁcally, contrastive learning), recent theoretical studies focus mainly on super-\nvised methods and their learning dynamics. Since the work of Jacot et al. (2018); Li and Liang\n(2018), the over-parameterization theory of deep learning has grown and brought about several\nbreakthrough results on the convergence of deep neural networks trained by gradient descent\nor stochastic gradient descent, as shown in Du et al. (2018); Allen-Zhu et al. (2018); Zou et al.\n(2018); Oymak and Soltanolkotabi (2019); Zou and Gu (2019); Ji and Telgarsky (2019). These\nanalyses have contributed a lot to our understanding of the supervised deep learning. Neverthe-\nless, the success of deep learning cannot be ascribed to supervised learning alone. It is unclear\nwhether we can obtain similar results under the unsupervised setting, where there are no labels\nto ﬁt or target functions to learn. This paper intends to ﬁll this void by analyzing the optimiza-\ntion of unsupervised contrastive learning using deep neural networks in the over-parameterized\nregime.\nIn unsupervised contrastive learning, the networks learn through comparing examples by\ntheir feature representations. The main idea, as described in He et al. (2019), can be thought of\nas training encoders for a dictionary look-up task. Consider a query q and a set of keys kj, where\na query matches a key if they encode information of the same image (in vision) or they encode\ncontextual messages coherent in a sentence (in NLP). At random initialization, the model is\nlikely to match a query to a wrong key and incurs a large loss, and therefore needs to be trained\nto match the query to the right key. To formulate this idea mathematically, We consider the\nfollowing loss function:\nL = −E\n\"\nlog\nexp(q⊤k0)\nPk\nj=0 exp(q⊤kj)\n#\n(1.1)\nwhere q = f q(x) is the query representation of x, k0 = f k(x+) is the key representation of\nthe positive example x+, and kj = f k(xj) are the key representations of the negative examples\n{xj}k\nj=1. The encoders f q and f k are trained to capture the correlation between these examples\nand project them into a new feature space.\nIntuitively, minimizing the loss function (1.1) is similar to classify q as k0, which is a convex\nprogram. But in contrastive learning, both encoders f q and f k are updated at each iteration,\nwhich makes the contrastive loss (1.1) jointly non-convex for the outputs of two networks. This\nsimultaneous updating scheme signiﬁcantly complicates the analysis of its training dynamics,\nand motivates us to ask the following question: What solution can we obtain via unsupervised\ncontrastive pretraining? We answered this question in our paper and summarize our contribution\nas follows:\n• We show that, if the query and key encoders are suﬃciently over-parameterized (the\nnumber of hidden nodes m is large enough), by applying end-to-end training that simul-\ntaneously updates the query and key encoders, one can ﬁnd an approximate stationary\nsolution for the non-convex contrastive loss in polynomial time.\n2\nRelated Work\nThe result of this paper involves both the aspect of unsupervised contrastive learning and the\nguarantees for the optimization of deep learning. We discuss both sides below.\n2\nUnsupervised Contrastive Learning\nThe ﬁrst paper on contrastive learning is Smith and Eisner (2005), which contains almost all the\nimportant ideas for contrastive learning. Hinton and Salakhutdinov (2006) used the term con-\ntrastive loss for the ﬁrst time, while their loss function is actually distance-based, similar to many\nother unsupervised methods.\nGutmann and Hyv¨arinen (2010) and Gutmann and Hyv¨arinen\n(2012) proposed the noise contrastive estimation (NCE) which is widely-used today.\nIn natural language processing, many well-known unsupervised/self-supervised1 models can\nbe thought of as certain forms of contrastive learning.\nMikolov et al. (2013) proposed the\nrevolutionary word2vec for contextual word embedding, which can be thought of as unsuper-\nvised contrastive learning using only one-layer query/key networks, and also they introduced\nthe widely-used negative sampling (see also Goldberg and Levy, 2014). Some following work\nLevy and Goldberg (2014); Li et al. (2015); Sharan and Valiant (2017); Frandsen and Ge (2019)\nfurther characterized and developed word2vec via matrix/tensor decomposition. In the subse-\nquent years many contextual embedding/language modelling methods have been proposed, say\nELMo (Peters et al., 2018), ULM-FiT (Howard and Ruder, 2018), BERT (Devlin et al., 2019)\nand its variants (Yang et al., 2019; Lan et al., 2019). The pretraining stage of these language\nmodels often involves inner products like f(x)⊤θ to match the context to the right words, which\ncan be viewed as contrastive learning with deep query encoder and shallow key encoder.\nBesides language modeling, Wu et al. (2018) applied the NCE objective to perform un-\nsupervised pretraining based on imageNet level data. Oord et al. (2018) heuristically proved\nthat contrastive learning maximizes the lower bound of the mutual information between the\nquery and keys’ representation. Further work such as Hjelm et al. (2019); Zhuang et al. (2019);\nH´enaﬀet al. (2019); Tian et al. (2019) extended the applications of contrastive learning in com-\nputer vision. Very recently, the work of He et al. (2019) and Misra and van der Maaten (2019)\nshowed that models pretrained via unsupervised contrastive learning can outperform supervised\npretrained counterparts in many downstream vision tasks. Chen et al. (2020) showed that con-\ntrastive pretraining can achieve over 76% top-1 accuracy in imageNet classiﬁcation by runing\nlinear regression over frozen features.\nOn the theoretical side, Ma and Collins (2018) analyzed the statistical properties of the NCE\nobjective and its eﬀectiveness in natural language processing. Arora et al. (2019b) theoretically\nstudied the generalization performance of unsupervised contrastive learning under the latent\nclass framework proposed in their paper, which, as far as we know, is the ﬁrst theoretical analysis\nof unsupervised pretraining. But their focus is on learning theory instead of optimization.\nOptimization of Deep Learning\nPrevious to the emergence of over-parameterized analysis, much work has been done on the opti-\nmization of shallow neural networks, say Tian (2017); Zhong et al. (2017); Brutzkus and Globerson\n(2017); Li and Yuan (2017); Du et al. (2017). But most of the results in these papers are under\nstringent assumptions such as Gaussian distribution of input data or requiring special initial-\nization methods (such as orthogonal initialization).\nRecently there have been several breakthroughs in the optimization of deep neural net-\nworks in the over-parameterized regime. Jacot et al. (2018) showed that as the width of the\n1We view self-supervised learning as a form of unsupervised learning, following He et al. (2019), as there is\nno formal diﬀerence in the existing literature. We use the term ”unsupervised learning” as long as the learning\nprocedure is ”not supervised by human-annotated labels”.\n3\nfully-connected network goes to inﬁnity, the network converges to a feature map in the repro-\nducing kernel Hilbert space induced by the Neural Tangent Kernel (NTK). Li and Liang (2018)\nindependently proved the convergence of stochastic gradient descent for over-parameterized\ntwo-layer networks.\nFollowing these two papers, Du et al. (2018); Allen-Zhu et al. (2018);\nZou and Gu (2019) proved the convergence of (stochastic) gradient descent to a global min-\nimum for deep neural networks (fully-connected, CNN and ResNet) if they are suﬃciently over-\nparameterized. Follow-up work (Wu et al., 2019; Oymak and Soltanolkotabi, 2019; Zou and Gu,\n2019; Ji and Telgarsky, 2019; Chen et al., 2019) further improved the convergence rates and\nover-parameterization conditions under diﬀerent assumptions and settings. However, none of\nthe existing papers have ever touch the setting of unsupervised deep learning, which is the focus\nof the current paper.\n3\nPreliminaries\n3.1\nNotations\nWe denote [n] = {1, . . . , n}, and S = {xi}n\ni=1 to be our training set, S\\i = S \\ {xi} as the\ntraining set without the data point xi. We use N(0, Im) to denote the multivariate standard\nGaussian distribution with m-dimensions. For a vector v = (v1, . . . , vm)⊤∈Rm, we denote\n∥v∥2 = (Pm\ni=1 v2\ni )1/2 to be its ℓ2 norm.\nFor a matrix A = (ai,j)m×n we denote ∥A∥0 to\nbe the number of non-zero entries of A, ∥A∥2 to be its spectral norm.\nFor two matrices\nA = (aij)m×n, B = (bij)m×n, we denote ⟨A, B⟩= ⟨A, B⟩F = tr(A⊤B) = (P\ni,j aijbij)1/2\nto be its trace inner product and ∥A∥F =\np\n⟨A, A⟩to be the Frobenius norm of A.\nFor\nneural network parameters W = (W0, . . . , WL) and W′ = (W0, . . . , WL) ∈W, where W :=\nRm×b × R(m×m)·(L−1) × Rd×m, we let ⟨W, W′⟩:= PL\nl=0⟨Wl, W′\nl⟩and ∥W∥F =\np\n⟨W, W⟩. We\nuse O(·), Ω(·) and Θ(·) to denote the standard big-O, big-Omega and big-Theta notations, only\nhiding positive constants.\n3.2\nProblem Setup\nThe method of contrastive learning involves two neural networks, and we deﬁne their architec-\ntures in the deﬁnition below.\nDeﬁnition 3.1 (Network Architecture). In contrastive learning, we need two neural networks,\nthe query encoder f q\nW and the key encoder f k\nθ , and without loss of generality we let them to be\n(L + 1)-layer fully connected networks with the same architecture. Our deﬁnitions of f q\nW and\nf k\nθ are:\nf q\nW(x) = WLσ(· · · σ(W1σ(W0x))),\nf k\nθ (x) = θLσ(· · · σ(θ1σ(θ0x)))\nwhere σ(·) is the ReLU activation. WL, θL ∈Rd×m, θl, Wl ∈Rm×m for every 1 ≤l ≤L −1,\nW0, θ0 ∈Rm×b, where b is the input dimension, d is the output dimension. We use the compact\nnotation W = (Wl)L\nl=0 and θ = (θl)L\nl=0 to denote the parameters of the two networks.\nRemark. In practice, the architectures of query and key encoders are possibly diﬀerent. We\nadopt the setting where they are of the same architecture, which is not essential and can be\nmodiﬁed to the more general setting. However, such a modiﬁcation may slightly complicate the\nﬁnal result and we decide not to carry it out.\n4\nWe present our initialization scheme of the network parameters below, which is knwon as\nHe initialization He et al. (2015), and has been adopted in the theoretical work Li and Liang\n(2018); Allen-Zhu et al. (2019); Zou et al. (2018); Zou and Gu (2019).\nDeﬁnition 3.2 (Initialization). The initializations of our parameters W, θ are deﬁned as follows,\n• (W0)i,j, (θ0)i,j\ni.i.d.\n∼N\n\u00000, 2\nm\n\u0001\nfor (i, j) ∈[m] × [b];\n• (Wl)i,j, (θl)i,j\ni.i.d.\n∼N\n\u00000, 2\nm\n\u0001\nfor (i, j) ∈[m] × [m] and every l ∈[L −1];\n• (WL)i,j, (θL)i,j\ni.i.d.\n∼N\n\u00000, 1\nd\n\u0001\nfor (i, j) ∈[d] × [m].\nWe present our deﬁnition of the contrastive loss function below, which lies in the core of this\npaper.\nDeﬁnition 3.3 (Contrastive Loss). Fixed k as the number of negative samples. For a speciﬁc\nsample xi ∈S, we select xi,1, . . . , xi,k ∈S\\i to be its negative samples.\nUsing our query\nencoder f q\nW and key encoder f k\nθ , we represent these data points as query qi = f q\nW(xi) and keys\nki,0 = f k\nθ (xi), ki,j = f k\nθ (xi,j). The contrastive loss of xi to negative samples {xi,j}k\nj=1 is deﬁned\nas\nℓ(f q\nW, f k\nθ , xi, {xi,j}k\nj=1) = −log\n\"\nexp(q⊤\ni ki,0)\nPk\nj=0 exp(q⊤\ni ki,j)\n#\n(3.1)\nwhich intuitively can be viewed as (k + 1)-way classiﬁcation loss that tries to classify qi as ki,0.\nWe minimize the following total loss\nLS(W, θ) = 1\nn\nn\nX\ni=1\nENeg(i)\u0002\nℓ(f q\nW, f k\nθ , xi, {xi,j}k\nj=1)\n\u0003\n(3.2)\nwhere ENeg(i) is deﬁned as the expectation over the uniform sampling of all negative samples\n{xi,j}k\nj=1 ⊂S\\i.\nRemark. This form of contrastive loss is designed for the pretext task instance-level discrim-\nination (Wu et al., 2018), which treats each image as a distinct class of its own. The resulting\nnegative sampling procedure can be described as one-against-all negative sampling. Similar con-\ntrastive loss functions are also used in practical work He et al. (2019) and Chen et al. (2020).\nWe present the algorithm of end-to-end contrastive learning via gradient descent below. This\nalgorithm is described in Figure 2 of He et al. (2019) as an alternative approach for MoCo, and\nis implemented in section 4.1 in He et al. (2019), where they showed that it is almost as equally\ncompetitive as MoCo. The analysis of better algorithms such as MoCo requires dealing with\nmore practical issues that are hard to analyze mathematically.\nAlgorithm 1 End-to-end training via gradient descent\ninput: Training data S = {xi}n\ni=1, step sizes η, γ, total number of iterations T.\ninitialization: Initialize W(0), θ(0) randomly, following Deﬁnition 3.2.\nfor t = 0, . . . , T −1 do\nW(t+1) = W(t) −η∇WLS(W(t), θ(t))\nθ(t+1) = θ(t) −γ∇θLS(W(t), θ(t))\nend for\noutput:{W(t), θ(t)}T\nt=0\n5\nRemark. In practical papers such as He et al. (2019); Tian et al. (2019), they usually optimize\nthe networks by performing stochastic gradient descent with respect to a minibatch of data and\na random set of negative examples. In practice the adoption of this doubly stochastic algorithm\nis due to the limitations of computation resources.\nIn our analysis we instead evaluate the\ncontrastive loss against all possible negative examples and perform gradient descent with repect\nto this non-random loss, which makes the algorithm non-random. The analysis of stochastic\nalgorithm would signiﬁcantly complicate the analysis. And we remark that the state-of-the-art\nanalysis for stochastic gradient descent with respect to cross-entropy (logistic) loss for neu-\nral networks (see Ji and Telgarsky, 2019; Chen et al., 2019) usually assume that there exist a\n”stochastic oracle”, which is not applicable to our setting.\n3.3\nAssumptions\nThe ﬁrst assumption we made is that all the data points lie in the 1-sphere with respect to the\n∥· ∥2 norm.\nAssumption 1 (Normalization). Every training data point xi ∈S satisﬁes ∥xi∥2 = 1.\nThis assumption is common in deep learning theory literature. As existing papers Du et al.\n(2018); Allen-Zhu et al. (2019); Cao and Gu (2019) have pointed out, restricting the inputs xi\nto the 1-sphere is not essential, and can be relaxed to requiring c1 ≤∥xi∥≤c2 for some absolute\nconstants c2 > c1 > 0.\nOur second assumption is the non-degeneracy of data points, which ﬁrst appeared in the\npapers Li and Liang (2018), and has been adopted and further modiﬁed by Allen-Zhu et al.\n(2018); Zou et al. (2018); Oymak and Soltanolkotabi (2019); Zou and Gu (2019); Chen et al.\n(2019).\nAssumption 2 (Non-degeneracy). There exist a universal constant δ > 0 such that, for any\ni, j ∈[n] with i ̸= j,\n∥xi −xj∥2 ≥δ\nRemark. In Du et al. (2018), they have shown that the above data non-degeneracy assumption\ncan implies λmin(K(L)) > 0, where K(L) is the Gram matrix, which is also known as the Neural\nTangent Kernel (Jacot et al., 2018) (see their papers for details).\nWe also remark that the assumptions on the data in this paper are no more than existing\npapers studying the supervised setting. It is interesting whether the result in this paper would\nstill hold if we the second assumption is signiﬁcantly weakened by only requiring separation\nbetween groups of data points as in Chen et al. (2019).\n4\nMain Theory\nBefore presenting our convergence theorem, we give a necessary deﬁnition.\nDeﬁnition 4.1 (loss-vectors). Denote qi = f q\nW(xi) and ki = f k\nθ (xi), we deﬁne\ng\nloss = ( g\nlossi)n\ni=1 =\n\u0000∂LS/∂qi\n\u0001\ni∈[n],\nd\nloss = ( d\nlossi)n\ni=1 =\n\u0000∂LS/∂ki\n\u0001\ni∈[n]\nAnd we further deﬁne loss = ( g\nloss, d\nloss) as our surrogate objective.\n6\nNow We present our main theorem for end-to-end contrastive learning.\nTheorem 4.2. For any ε ∈(0, 1), δ ≤O(1/L), suppose Assumption 1 and Assumption 2 holds,\nwith over-parameterization condition\nm ≥Ω\n\u0000n15L12(log m)5δ−5ε−2\u0001\nd ≥Ω(log2 m)\nand if we perform Algorithm 1, with step sizes\nη, γ = Θ(dε2δ2/(n7L2km))\nthen with probability at least 1 −O(m−1) over the initialization, we have\n1\nT\nT−1\nX\nt=0\n∥loss(t)∥2 ≤ε\nfor T = Θ\n\u0012n10L2k\nδ3\n· 1\nε4\n\u0013\nwhere the loss(t)-vectors are deﬁned in Deﬁnition 4.1, with f q and f k parameterized by W(t)\nand θ(t) respectively.\nAs mentioned in Allen-Zhu et al. (2018), the result of ﬁnding weight matrices W that satisﬁes\n∥∇fLS∥≤ε cannot be derived from the classical theory of ﬁnding approximate saddle points\nfor non-convex objectives. And since in our end-to-end training we update two neural networks\nsimultaneously, the interaction between these two networks during the optimization process\nmakes it even harder for the optimization analysis.\nNote that in contrast to existing work on the convergence of supervised training, we require\nthe output dimension d to be suﬃciently large (of magnitude Θ(log2 m)). This requirement\nis necessary for both query encoder and key encoder to project suﬃcient information onto the\noutput space and contrast between each queries qi = f q\nW(xi) and keys kj = f k\nθ (xj). Without\nthis requirement, it would be diﬃcult for the outputs to represent the high-dimensional infor-\nmation learned by the over-parameterized hidden layers. And also this requirement of d is not\nimpractical because it is only for pretraining. One can always add a new fully-connected layer\non top in the ﬁne-tuning stage.\nOur proof of Theorem 4.2 relies on two technical lemmas, and we shall elaborate them below.\n4.1\nMain Technical Lemmas\nWe present two lemmas below that are the key components of our ﬁnal convergence proof. The\nﬁrst lemma concerns the gradient bounds for updating both W and θ. The proof of Lemma 4.3\nis in Appendix C.\nLemma 4.3 (Gradient Bounds). Suppose ω, τ ≤O(δ3/2/(n3L6 log3/2 m)) then with probability\nat least 1 −e−Ω(mω3/2L) −e−Ω(mτ 3/2L) over the randomness of initialization, if W ∈B(W(0), ω)\nand θ ∈B(θ(0), τ), the following holds.\n• For ∥∇WLS(W, θ)∥F , we have\n\r\r∇WLS(W, θ)\n\r\r2\nF ≥Ω\n\u0012 mδ\nn3d\n\u0013\nn\nX\ni=1\n∥g\nlossi∥2\n2\n\r\r∇WLS(W, θ)\n\r\r2\nF ≤O\n\u0012Lm\nnd\n\u0013\nn\nX\ni=1\n∥g\nlossi∥2\n2\n7\n• For ∥∇θLS(W, θ)∥F , we have\n\r\r∇θLS(W, θ)\n\r\r2\nF ≥Ω\n\u0012 mδ\nn3d\n\u0013\nn\nX\ni=1\n∥d\nlossi∥2\n2\n\r\r∇θLS(W, θ)\n\r\r2\nF ≤O\n\u0012Lm\nnd\n\u0013\nn\nX\ni=1\n∥d\nlossi∥2\n2\nwhere the g\nloss and d\nloss-vectors are deﬁned in Deﬁnition 4.1.\nThe second lemma veriﬁes the semi-smoothness properties for updating both the query en-\ncoder f q\nW and the key encoder f k\nθ simultaneously. The semi-smoothness condition instead of\nLipschitz smoothness is due to the non-smooth property of ReLU activations, as illustrated in\nAllen-Zhu et al. (2018). Our derivations of the semi-smoothness lemma is diﬀerent in many as-\npects to the original one in Allen-Zhu et al. (2018), since not only do we need to simultaneously\nupdate two neural networks, we also need to compute the exact form of the gradient of loss func-\ntion to the outputs of these two neural networks g\nloss = (∂LS/∂qi)n\ni=1 and d\nloss = (∂LS/∂ki)n\ni=1\nwhich is complicated after taking expectations with respect to negative sampling.\nLemma 4.4 (The Semi-smoothness Properties). For any perturbations ∥W′∥2 ≤ω and ∥θ′∥2 ≤\nτ, where\nω, τ ∈[Ω(\np\nd/m), O(1/(L9/2(log m)3/2))]\nand W ∈B(W(0), ω), θ ∈B(θ(0), τ) such that\nW + W′ ∈B(W(0), ω),\nθ + θ′ ∈B(θ(0), τ)\nwe have, with probability at least 1−exp(−Ω(mω3/2L))−exp(−Ω(mτ 2/3L)) over the randomness\nof initialization, the following inequality holds,\nLS(W + W′, θ + θ′) ≤LS(W, θ) + ⟨∇(W,θ)LS(W, θ), (W′, θ′)⟩\n+ O\n\u0012ω1/3L2√m log m\n√\nnd\n\u0013\n∥g\nloss∥2 · ∥W′∥F\n+ O\n\u0012τ 1/3L2√m log m\n√\nnd\n\u0013\n∥d\nloss∥2 · ∥θ′∥F\n+ O\n\u0012kL2m2\nd2\n\u0013\u0010\nτ 2∥W′∥2\nF + ω2∥θ′∥2\nF\n\u0011\n(4.1)\n5\nProof Techniques\n5.1\nKey Facts\nSince the contrastive loss function deﬁned in Deﬁnition 3.3 is inherently diﬀerent in form to the\nloss functions used in supervised learning, and also since we have taken expectation with respect\nto the negative sampling, we need to derive some basic facts of how the gradient is calculated\nfor both the query and key encoders. The exact calculations are done in Appendix C.1.\nFor notational convenience in the expositions below, we denote\nqi = f q\nW(xi),\nzj = f k\nθ (xj) −f k\nθ (xi),\nzi,j = f k\nθ (xi,j) −f k\nθ (xi)\n(5.1)\nThe form of g\nloss-vectors are directly to compute from the our deﬁnition of contrastive loss.\n8\nFact 1 ( g\nloss-vector). For each i ∈[n], the g\nlossi-vector is the following vector obtained from\ncalculating the gradient of LS(W, θ) with respect to the query encoder qi = f q\nW(xi):\ng\nlossi = ∂LS/∂qi = ENeg(i)\n\u0014 k\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pj\ns=1 exp(q⊤\ni zi,j)\n· zi,j\n\u0015\nwhere the expectation ENeg(i) is taken with respect to the uniform sampling of {xi,j}k\nj=1 ⊂S\\i.\nThe exact form of d\nloss-vectors are more subtle, and we present it below.\nFact 2 ( d\nloss-vector). For each pair (i, j) ∈[n] × [n] such that i ̸= j, we denote the d\nloss(xi, xj)-\nvector to be the following vector :\nd\nloss(xi, xj) :=\n1\n\u0000n−1\nk\n\u0001\nX\nxj∈{xi,s}s∈[k]⊂S\\i\nexp(q⊤\ni zj)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n· qi\nwhere qi, zj, zi,s is deﬁned in (5.1). The summation is over all set of negative samples {xi,s}k\ns=1 ⊂\nS\\i that contains the sample xj. Now the d\nlossi-vector can be calculated as\nd\nlossi = ∂LS/∂ki =\nX\nj̸=i\n(loss(xj, xi) −loss(xi, xj))\n5.2\nProof Overview of Technical Lemmas\nWe outline the proof of Lemma 4.3 and Lemma 4.4 here. Firstly we deﬁne the following nota-\ntions:\nhq\ni,0 = σ(W0xi)\nhq\ni,l = σ(Wlhq\ni,l−1)\nhk\ni,0 = σ(θ0xi)\nhk\ni,l = σ(θlhq\ni,l−1)\nand also diagonal matrices Dq\ni,l and Dk\ni,l as\n(Dq\ni,l)a,a := 1{σ(Wlhq\ni,l−1)a ≥0}\n(Dk\ni,l)a,a := 1{σ(θlhk\ni,l−1)a ≥0}\nGradient bounds:\nFor notational convenience, we deﬁne the back-propagation matrices\nBackq\ni,l and Backk\ni,l as\nBackq\ni,l = WLDi,L−1 · · · Di,lWl,\nBackk\ni,l = θLDi,L−1 · · · Di,lθl\nFrom the derivation of Fact 1 and Fact 2 we can transform the gradient ∇WlLS(W, θ) and\n∇θlLS(W, θ) into more operable forms\n∇WlLS(W, θ) = 1\nn\nn\nX\ni=1\nDq\ni,l\n\u0010\n(Backq\ni,l+1)⊤g\nlossi\n\u0011\nhq,⊤\ni,l−1\n∇θlLS(W, θ) = 1\nn\nn\nX\ni=1\nDk\ni,l\n\u0010\n(Backk\ni,l+1)⊤d\nlossi\n\u0011\nhk,⊤\ni,l−1\nFrom the initialization, the norm of the product (Backq\ni,l+1)⊤g\nlossi is of magnitude ∼\np\nm/d∥g\nlossi∥2\n(and similarly for (Backk\ni,l+1)⊤d\nlossi). The lower bounds can be derived from the randomness\ndecomposition arguement in Allen-Zhu et al. (2018) and an improved version in Zou and Gu\n(2019). The upper bounds follows from the naive bounds ∥(Backq\ni,l+1)⊤g\nlossi∥2 ≤O(\np\nm/d)∥g\nlossi∥2\nwith high probability.\n9\nSemi-smoothness:\nTo derive the semi-smoothness for updating two neural networks, we start\nfrom the function ℓ(f q\nW, f k\nθ , xi, {xi,j}k\nj=1) deﬁned in Deﬁnition 3.3. We transform it to\nℓ(f q\nW, f k\nθ , xi, {xi,j}k\nj=1) = log\n\u0014\n1 +\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n\u0015\nwhere qi, zi,j is deﬁned in (5.1). Clearly this function is convex with respect to q⊤\ni zi,j, and from\nsimple calculation we showed that this function is 1-Lipschitz smooth. Thus we obtain a second\norder bound with respect to q⊤\ni zi,j\nℓ(f q\nf\nW, f k\neθ , xi, {xi,j}k\nj=1) ≤ℓ(f q\nW, f k\nθ , xi, {xi,j}k\nj=1)\n+\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n·\n\u0000˜q⊤\ni ˜zi,j −q⊤\ni zi,j\n\u0001\n|\n{z\n}\n①\n+ 1\n2\nk\nX\nj=1\n(˜q⊤\ni ˜zi,j −q⊤\ni zi,j)2\n|\n{z\n}\n②\nwhere ˜qi, ˜zi,j are the qi, zi,j paramterized by f\nW = W + W′ and eθ = θ + θ′, which are not far\nfrom the initialization. We decompose ˜q⊤\ni ˜zi,j −q⊤\ni zi,j into three terms\n(˜qi −qi)⊤zi,j\n|\n{z\n}\n③\n+ q⊤\ni (˜zi,j −zi,j)\n|\n{z\n}\n④\n+ (˜qi −qi)⊤(˜zi,j −zi,j)\n|\n{z\n}\n⑤\nand tackle them separately. For the terms ①& ③and , after taking expectation with respect\nto negative sampling, we obtain\n1\nn\nn\nX\ni=1\n⟨g\nlossi, ˜qi −qi⟩\nand similarly, for the term ①& ④, we can take expectation with respect to negative sampling\nand rearrange to get\n1\nn\nn\nX\ni=1\n⟨d\nlossi, ˜ki −ki⟩\nwhere ˜ki = f k\neθ (xi) and ki = f k\nθ (xi). The terms ①& ⑤and ②& (③+ ④+ ⑤) can be bounded\nas\nO(kL2m2/d2) · (τ 2∥f\nW −W∥2\n2 + ω2∥eθ −θ∥2\n2)\nvia ﬁne analysis of the perturbations to the neural network outputs f q\nf\nW(x)−f q\nW(x) and f k\neθ (x)−\nf k\nθ (x). In the rest of the proof, we apply techniques from NTK analysis to deal with the ﬁrst\norder perturbations f q\nf\nW(x) −f q\nW(x) −∇Wf q\nW(x)(W′), which eventually leads to:\n1\nn\nn\nX\ni=1\n⟨g\nlossi, ˜qi −qi⟩−⟨∇WLS(W, θ), W′⟩≤∥g\nloss∥2 · O\n\u0012ω1/3L2√m log m\n√\nnd\n\u0013\n∥W′∥F\nAnd similarly for the perturbations to f k\nθ (·). Combining these calculations completes the proof.\n10\n5.3\nProof Sketch of Theorem 4.2\nEquipped with Lemma 4.3 and Lemma 4.4, we can sketch a proof of the convergence theorem\nof end-to-end training via gradient descent in unsupervised contrastive learning.\nProof sketch of Theorem 4.2. Firstly we set the trajectory parameters as\nω, τ = O\n\u0000n3.5√\nd/(δε√m)\n\u0001\nand we assume that the parameters W(t), θ(t) in the training process always satisfy\nW(t) ∈B(W(0), ω), θ(t) ∈B(θ(0), τ)\nand we will justify this condition in trajectory analysis. Employing Algorithm 1, we denote the\ngradient update at t-th iteration as\n∇W,t = ∇WLS(W(t), θ(t)), ∇θ,t = ∇θLS(W(t), θ(t))\nNow from Lemma 4.4 and our choice of ω, τ, η, γ, we can drop the second order terms in (4.1)\nand obtain\nLS(W(t+1), θ(t+1)) ≤LS(W(t), θ(t)) −Ω(η)∥∇W,t∥2\nF −Ω(γ)∥∇θ,t∥2\nF\n+ O\n\u0012ω1/3L2√m log m\n√\nnd\n\u0013\n∥g\nloss∥2 · η∥∇W,t∥F\n+ O\n\u0012τ 1/3L2√m log m\n√\nnd\n\u0013\n∥d\nloss∥2 · γ∥∇θ,t∥F\nFrom the gradient lower bound in Lemma 4.3 and our trajectory parameters ω, τ, we can reduce\nthe above inequality to\nLS(W(t+1), θ(t+1)) ≤LS(W(t), θ(t)) −Ω\n\u0012ηδm\nn3d\n\u0013\n∥g\nloss\n(t)∥2\n2 −Ω\n\u0012γδm\nn3d\n\u0013\n∥d\nloss\n(t)∥2\n2\n≤−Ω\n\u0012 δm\nn3d min(η, γ)\n\u0013\n∥loss(t)∥2\n2\nwhere the last inequality is from our deﬁnition of loss-vector in Deﬁnition 4.1. Now, by summing\nover t = 0, . . . , T −1 and taking square root, and also by our choice of step sizes η, γ, we can\ncalculate\n1\nT\nT−1\nX\nt=0\n∥loss(t)∥2 ≤O\n\u0012s\nn3d\nT min(η, γ)δm\n\u0013\n·\nq\nLS(W(0), θ(0)) −LS(W(T), θ(T))\n≤\n1\n√\nT\n· O\n\u0012n5L\n√\nk\nδ3/2ε\n\u0013\nSo for T = Θ(n10L2k/(δ3ε4)) iterations, we obtain\n1\nT\nT−1\nX\nt=0\n∥loss(t)∥2 ≤ε\nand in order for W(t) and θ(t) to stay in B(W(0), ω) and B(θ(0), τ) respectively, the over-\nparametrization needed would be m ≥Ω(n12L12(log m)5/(δ5ε2)).\nThe details of the above\ncalculations and trajectory analysis is presented in Appendix A.\n11\n6\nConclusion and Future Work\nIn this paper, we show that in unsupervised contrastive learning, end-to-end training via gradi-\nent descent can ﬁnd an approximate stationary solution for the non-convex contrastive loss in\npolynomial time. Our proof is based on a careful analysis of the contrastive loss function and\nthe gradient updates for two interactive deep neural networks, which allows us to analyze its\noptimization behavior.\nWe discuss some directions for future research.\n• In Arora et al. (2019b) they established generalization bound for pretrained represen-\ntations, but the representation is assumed to be frozen after pretraining (which means\ntraining only the top layer only). From our analysis of optimization, it would be possible\nto obtain a generalization bound that involves ﬁne-tuning (which jointly trains all the\nlayers).\n• It would be of interest to know why minimizing the contrastive loss can lead to good\nfeature representations. In the supervised setting, Arora et al. (2019a) and Cao and Gu\n(2019) proved that the generalization performance of over-parameterized neural networks\nare closely related to their NTK. But since in contrastive learning we need two neural\nnetworks, their analysis cannot be trivially generalize to this setting.\nReferences\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via\nover-parameterization. arXiv preprint arXiv:1811.03962, 2018.\nZeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.\nLearning and generalization in overpa-\nrameterized neural networks, going beyond two layers. In Advances in neural information\nprocessing systems, pages 6155–6166, 2019.\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis\nof optimization and generalization for overparameterized two-layer neural networks. arXiv\npreprint arXiv:1901.08584, 2019a.\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saun-\nshi. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint\narXiv:1902.09229, 2019b.\nAlon Brutzkus and Amir Globerson.\nGlobally optimal gradient descent for a convnet with\ngaussian inputs. In Proceedings of the 34th International Conference on Machine Learning-\nVolume 70, pages 605–614. JMLR. org, 2017.\nYuan Cao and Quanquan Gu.\nGeneralization error bounds of gradient descent for learning\noverparameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey E. Hinton. A simple framework\nfor contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.\nZixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is\nsuﬃcient to learn deep relu networks? arXiv preprint arXiv:1911.12360, 2019.\n12\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT,\npages 4171–4186, 2019.\nSimon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional ﬁlter easy to learn?\narXiv preprint arXiv:1709.06129, 2017.\nSimon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent ﬁnds\nglobal minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.\nAbraham Frandsen and Rong Ge. Understanding composition of word embeddings via ten-\nsor decomposition. In International Conference on Learning Representations, 2019.\nURL\nhttps://openreview.net/forum?id=H1eqjiCctX.\nYoav Goldberg and Omer Levy. word2vec explained: deriving mikolov et al.’s negative-sampling\nword-embedding method. arXiv preprint arXiv:1402.3722, 2014.\nMichael Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation: A new estimation prin-\nciple for unnormalized statistical models. In Proceedings of the Thirteenth International Con-\nference on Artiﬁcial Intelligence and Statistics, pages 297–304, 2010.\nMichael U Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation of unnormalized sta-\ntistical models, with applications to natural image statistics. Journal of Machine Learning\nResearch, 13(Feb):307–361, 2012.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Sur-\npassing human-level performance on imagenet classiﬁcation.\nIn Proceedings of the IEEE\ninternational conference on computer vision, pages 1026–1034, 2015.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\nunsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.\nOlivier J H´enaﬀ, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-eﬃcient\nimage recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.\nGeoﬀrey E Hinton and Ruslan R Salakhutdinov.\nReducing the dimensionality of data with\nneural networks. science, 313(5786):504–507, 2006.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. Learning deep representations by mutual information estima-\ntion and maximization. In International Conference on Learning Representations, 2019. URL\nhttps://openreview.net/forum?id=Bklr3j0cKX.\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁca-\ntion. arXiv preprint arXiv:1801.06146, 2018.\nArthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks.\nIn Advances in neural information processing systems,\npages 8571–8580, 2018.\nZiwei Ji and Matus Telgarsky. Polylogarithmic width suﬃces for gradient descent to achieve\narbitrarily small test error with shallow relu networks. arXiv preprint arXiv:1909.12292, 2019.\n13\nThomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world\nmodels. In International Conference on Learning Representations, 2019.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut.\nAlbert: A lite bert for self-supervised learning of language representations.\nIn\nInternational Conference on Learning Representations, 2019.\nOmer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In\nAdvances in neural information processing systems, pages 2177–2185, 2014.\nYitan Li, Linli Xu, Fei Tian, Liang Jiang, Xiaowei Zhong, and Enhong Chen. Word embedding\nrevisited: A new representation learning and explicit matrix factorization perspective. In\nTwenty-Fourth International Joint Conference on Artiﬁcial Intelligence, 2015.\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gra-\ndient descent on structured data. In Advances in Neural Information Processing Systems,\npages 8157–8166, 2018.\nYuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu acti-\nvation. In Advances in neural information processing systems, pages 597–607, 2017.\nSindy L¨owe, Peter O’Connor, and Bastiaan Veeling. Putting an end to end-to-end: Gradient-\nisolated learning of representations. In Advances in Neural Information Processing Systems,\npages 3033–3045, 2019.\nZhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for condi-\ntional models: Consistency and statistical eﬃciency. arXiv preprint arXiv:1809.01812, 2018.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word rep-\nresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\nIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant repre-\nsentations. arXiv preprint arXiv:1912.01991, 2019.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv preprint arXiv:1807.03748, 2018.\nSamet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global con-\nvergence guarantees for training shallow neural networks. arXiv preprint arXiv:1902.04674,\n2019.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken-\nton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint\narXiv:1802.05365, 2018.\nVatsal Sharan and Gregory Valiant. Orthogonalized als: A theoretically principled tensor de-\ncomposition algorithm for practical use. In Proceedings of the 34th International Conference\non Machine Learning-Volume 70, pages 3095–3104. JMLR. org, 2017.\nNoah A. Smith and Jason Eisner. Contrastive estimation: Training log-linear models on un-\nlabeled data. In Proceedings of the 43rd Annual Meeting on Association for Computational\nLinguistics, ACL ’05, page 354–362, USA, 2005. Association for Computational Linguistics.\ndoi: 10.3115/1219840.1219884. URL https://doi.org/10.3115/1219840.1219884.\n14\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint\narXiv:1906.05849, 2019.\nYuandong Tian.\nAn analytical formula of population gradient for two-layered relu network\nand its applications in convergence and critical point analysis. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70, pages 3404–3413. JMLR. org,\n2017.\nMartin J Wainwright.\nHigh-dimensional statistics: A non-asymptotic viewpoint, volume 48.\nCambridge University Press, 2019.\nXiaoxia Wu, Simon S Du, and Rachel Ward. Global convergence of adaptive gradient methods\nfor an over-parameterized neural network. arXiv preprint arXiv:1902.07111, 2019.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via\nnon-parametric instance discrimination. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 3733–3742, 2018.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances\nin neural information processing systems, pages 5754–5764, 2019.\nKai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guaran-\ntees for one-hidden-layer neural networks. In Proceedings of the 34th International Conference\non Machine Learning-Volume 70, pages 4140–4149. JMLR. org, 2017.\nChengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learn-\ning of visual embeddings. In Proceedings of the IEEE International Conference on Computer\nVision, pages 6002–6012, 2019.\nDifan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural\nnetworks. In Advances in Neural Information Processing Systems, pages 2053–2062, 2019.\nDifan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes\nover-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.\n15\nAppendix\nA\nProof of the Main Theorem\nFirst we restate the necessary deﬁnitions.\nDeﬁnition A.1 (loss-vectors). For each i ∈[n], we denote the gradients of our loss function\nto the outputs of both neural networks as\ng\nloss = ( g\nlossi)n\ni=1 =\n\u0000∂LS/∂qi)\n\u0001\ni∈[n],\nd\nloss = ( d\nlossi)n\ni=1 =\n\u0000∂LS/∂ki\n\u0001\ni∈[n]\nAnd we further deﬁne loss = ( g\nloss, d\nloss) as our objective.\nWe restate our main result of convergence here.\nTheorem A.2 (Convergence of Gradient Descent). For any ε ∈(0, 1), δ ∈(0, O(L−1)). Let\nm ≥Ω\n\u0000n12L12(log m)5δ−5ε−2\u0001\n,\nη, γ = Θ(dε2δ2/(n7L2km)),\nd ≥Ω(log2 m)\n(A.1)\nSuppose we do gradient descent at each iteration t = 0, 1, . . . , T −1. Then, with probability at\nleast 1 −O(m−1) over the random initialization, we have\n1\nT\nT−1\nX\nt=0\n∥loss(t)∥2 ≤ε\nfor T = Θ\n\u0012n10L2k\nδ3\n· 1\nε4\n\u0013\niterations\nwhere the loss(t)-vectors are deﬁned in Deﬁnition A.1, with f q and f k parameterized by W(t)\nand θ(t) respectively.\nWe also restate the lemmas appeared in Section 5.\nA.1\nMain Technical Lemmas\nLemma A.3 (Gradient Bounds). Let ω, τ ≤O(δ3/2/(n3L6(log m)3/2)), with probability at\nleast 1 −e−Ω(mω3/2L) −e−Ω(mτ 3/2L) over the randomness of initialization, it satisﬁes for every\nW ∈B(W(0), ω) and θ ∈B(θ(0), τ), the following holds.\n• For ∥∇WLS(W, θ)∥F , we have\nΩ\n\u0012 mδ\nn3d\n\u0013\nn\nX\ni=1\n∥g\nlossi∥2\n2 ≤\n\r\r\r∇WLS(W, θ)\n\r\r\r\n2\nF ≤O\n\u0012Lm\nnd\n\u0013\nn\nX\ni=1\n∥g\nlossi∥2\n2\n• For ∥∇θLS(W, θ)∥F , we have\nΩ\n\u0012 mδ\nn3d\n\u0013\nn\nX\ni=1\n∥d\nlossi∥2\n2 ≤\n\r\r\r∇θLS(W, θ)\n\r\r\r\n2\nF ≤O\n\u0012Lm\nnd\n\u0013\nn\nX\ni=1\n∥d\nlossi∥2\n2\n16\nLemma A.4 (The Semi-smoothness Properties). For any ∥W′∥2 ≤ω and ∥θ′∥2 ≤τ, where\nω, τ ∈[Ω(\np\nd/m), O(1/(L9/2(log m)3/2))]\nThen we have, with probability at least 1 −e−Ω(mω3/2L) −e−Ω(mτ 2/3L) over the randomness of\ninitialization, the following inequality holds,\nLS(W + W′, θ + θ′) ≤LS(W, θ) + ⟨∇WLS(W, θ), W′⟩+ ⟨∇θLS(W, θ), θ′⟩\n+ O\n\u0012ω1/3L2√m log m\n√\nnd\n\u0013\n∥g\nloss∥2 · ∥W′∥F\n+ O\n\u0012τ 1/3L2√m log m\n√\nnd\n\u0013\n∥d\nloss∥2 · ∥θ′∥F\n+ O\n\u0012kL2m2\nd2\n\u0013\u0010\nτ 2∥W′∥2\nF + ω2∥θ′∥2\nF\n\u0011\nA.2\nProof of Theorem A.2\nProof of Theorem A.2. We restate our parameter choice here for the convenience of readers:\nm ≥Ω\n\u0000n12L12(log m)5δ−5ε−2\u0001\n,\nη, γ = Θ(dε2δ2/(n7kL2m)),\nd ≥Ω(log2 m)\n(A.2)\nAnd we set the trajectory parameter\nω, τ = O\n\u0012n7/2√\nd\nδε√m\n\u0013\nwhich satisfy all the requirements in all the lemmas we have employed. In the proof below, we\nﬁrst\nWe denote g\nloss\n(t), d\nloss\n(t) as the g\nloss and d\nloss-vectors where the query encoder f q\nW and the\nkey encoder f k\nθ are parameterized by W(t) and θ(t) respectively. To perform gradient descent,\nwe let the gradient update be\nW(t+1) = W(t) −η∇WLS(W(t), θ(t)),\nθ(t+1) = θ(t) −γ∇θLS(W(t), θ(t))\nAnd for technical convenience we denote\n∇W,t = ∇WLS(W(t), θ(t))\nand\n∇θ,t = ∇θLS(W(t), θ(t))\nNow from Lemma A.4, we can calculate\nLS(W(t+1), θ(t+1)) ≤LS(W(t), θ(t)) −η∥∇W,t∥2\nF −γ∥∇θ,t∥2\nF\n+ ∥g\nloss\n(t)∥2 · O\n\u0012ω1/3L2√m log m\n√\nnd\n\u0013\n· ∥∇W,t∥F\n+ ∥d\nloss\n(t)∥2 · O\n\u0012τ 1/3L2√m log m\n√\nnd\n\u0013\n· ∥∇θ,t∥F\n+ O\n\u0012kL2m2\nd2\n\u0013\n·\n\u0010\nη2τ 2∥∇W,t∥2\n2 + γ2ω2∥∇θ,t∥2\nF\n\u0011\n17\nNow from our step size choice η, γ = Θ(dε2δ2/(n7L2km)) and our trajectory parameter choice\nω, τ = O(n7/2√\nd/(δε√m)), we can obtain\nLS(W(t+1), θ(t+1)) ≤LS(W(t), θ(t)) −3η\n4 ∥∇W,t∥2\nF −3γ\n4 ∥∇θ,t∥2\nF\n+ ∥g\nloss\n(t)∥2 · O\n\u0012ω1/3L2√m log m\n√\nnd\n\u0013\n· ∥∇W,t∥F\n+ ∥d\nloss\n(t)∥2 · O\n\u0012τ 1/3L2√m log m\n√\nnd\n\u0013\n· ∥∇θ,t∥F\n≤−3η\n4 ∥∇W,t∥F\n\u0012\n∥∇W,t∥F −O\n\u0012ω1/3L2√m log m\n√\nnd\n\u0013\n· ∥g\nloss\n(t)∥2\n\u0013\n−3γ\n4 ∥∇θ,t∥F\n\u0012\n∥∇θ,t∥F −O\n\u0012τ 1/3L2√m log m\n√\nnd\n\u0013\n· ∥d\nloss\n(t)∥2\n\u0013\nNow from Lemma A.3 and our notation ∇W,t = ∇WLS(W(t), θ(t)), ∇θ,t = ∇θLS(W(t), θ(t)),\nwe have\n∥∇W,t∥2\nF ≥Ω\n\u0012 mδ\nn3d\n\u0013\n∥g\nloss\n(t)∥2\n2,\n∥∇θ,t∥2\nF ≥Ω\n\u0012 mδ\nn3d\n\u0013\n∥d\nloss\n(t)∥2\n2\nWe can choose ω, τ = O(δ3/2/(n3L6(log m)3/2)) to ensure that\nLS(W(t+1), θ(t+1)) −LS(W(t), θ(t)) ≤−Ω\n\u0012ηδm\nn3d\n\u0013\n∥g\nloss\n(t)∥2\n2 −Ω\n\u0012γδm\nn3d\n\u0013\n∥d\nloss\n(t)∥2\n2\n≤−Ω\n\u0012min(η, γ)δm\nn3d\n\u0013\n∥loss(t)∥2\n2\nBy averaging over t = 0, . . . , T −1, we arrive at\n1\nT\nT−1\nX\nt=0\n∥loss(t)∥2 ≤O\n s\nn3d\nT min(η, γ)δm\n!q\nLS(W(0), θ(0))\n①\n≤O\n s\nn4d\nT min(η, γ)δm\n!\n(A.3)\nwhere ①is due to the fact that, by Johnson-Lindenstrauss Lemma, with probability at least\n1 −O(n)e−Ω(d), we have\n∥f q\nW(0)(xi)∥2, ∥f k\nθ(0)(xi)∥2 ≤O(1)\nfor all i ∈[n] =⇒LS(W(0), θ(0)) ≤O(log k) ≤O(n)\nThus for T min(η, γ) = Θ(n3d/(mδε2)), we have\n1\nT\nT−1\nX\nt=0\n∥loss(t)∥2 ≤ε\n=⇒\n1\nT\nT−1\nX\nt=0\n∥g\nloss\n(t)∥2 ≤ε,\n1\nT\nT−1\nX\nt=0\n∥d\nloss\n(t)∥2 ≤ε\nNote that from our choice of step sizes, η and γ are of the same order, which implies\nTη, Tγ = Θ(n3d/(mδε2))\nTherefore the trajectory of W satisﬁes\n∥W(t) −W(0)∥F ≤\nT−1\nX\nt=0\nη∥∇W,t∥F ≤\nT−1\nX\nt=0\nη\np\nLm/d · ∥g\nloss\n(t)∥2\n≤\np\nTη · O\n\u0012r\nLm\nd\n· n4d\nmδ\n\u0013\n≤O\n\u0012n3.5√\nd\nδε√m\n\u0013\n= ω\n(A.4)\n18\nAnd similarly, the trajectory of θ satisﬁes\n∥θ(t) −θ(0)∥F ≤\nT−1\nX\nt=0\nγ∥∇θ,t∥F ≤\nT−1\nX\nt=0\nγ\np\nLm/d · ∥d\nloss\n(t)∥2\n≤\np\nTγ · O\n\u0012r\nLm\nd\n· n4d\nmδ\n\u0013\n≤O\n\u0012n3.5√\nd\nδε√m\n\u0013\n= τ\nAnd our ﬁnal running time is\nT = Θ\n\u0012\nn3d\nmin(η, γ)mδε2\n\u0013\n= Θ\n\u0012n10L2k\nδ3\n· 1\nε4\n\u0013\nB\nAuxiliary Lemmas\nThe lemmas in this section are adapted from Allen-Zhu et al. (2018); Zou and Gu (2019) and\nmodiﬁed to ﬁt our setting. Note that all the lemmas are written with respect to the parameters\nW of the query encoders f q\nW. They can be applied to the parameters θ of the key encoders f k\nθ\nas well.\nFirstly we deﬁne the following notations: Let Di,l be diagonal matrices deﬁned as follows\n(Di,l)r,r = 1{([Wl]rhi,l) ≥0} r ∈[m]\nfor i ∈[n], 0 ≤l ≤L,\nwhere [Wl]r is the r-th row of Wl. Now we can represent the outputs of hidden layers recursively\nas\nhi,0 = Di,0W0xi,\nhi,l = Di,lWlhi,l−1 for 1 ≤l ≤L −1,\nfW(xi) = WLhi,L−1\nFor clarity we further deﬁne the product of matrices Ai as\nb\nY\nl=a\nAl = AbAb−1 · · · Aa\nfor matrices Aa, . . . , Ab\nSpeciﬁcally, we deﬁne the following notations for parameter W at its random initialization W(0)\n(see Deﬁnition 3.2). Set notations: for every i ∈[n] and 1 ≤l ≤L, we deﬁne the matrices D(0)\ni,l\nand vectors h(0)\ni,l as\nD(0)\ni,l := diag\n\u0010\n1(⟨[W(0)\nl\n]r, hi,l−1⟩≥0)\n\u0011m\nk=1,\nh(0)\ni,0 := D(0)\ni,0 W(0)\n0 xi,\nh(0)\ni,l = D(0)\ni,l W(0)\nl\nh(0)\ni,l−1\nNow equipped with these notations, we can present the following technical lemmas\nLemma B.1 (Lemma 7.1 in Allen-Zhu et al. (2018)). If ε ∈(0, 1), with probability at least\n1 −e−Ω(ε2m/L) over the randomness of W(0), we have ∥hi,l∥∈[1 −ε, 1 + ε] for all i ∈[n] and\nl ∈[L].\nLemma B.2 (Lemma 7.3 in Allen-Zhu et al. (2018)). Suppose m ≥Ω(nL log(nL)).\nWith\nprobability at least 1 −e−Ω(m/L) over the randomness of initialization of W(0), for all i ∈[n]\nand 1 ≤a ≤b ≤L −1\n19\n(a) ∥W(0)\nb\n\u0000Qb−1\nl=a D(0)\ni,l W(0)\nl\n\u0001\n∥2 ≤O(\n√\nL).\n(b) ∥W(0)\nb\n\u0000Qb−1\nl=a D(0)\ni,l W(0)\nl\n\u0001\nv∥2 ≤2∥v∥2 for all v ∈Rm with ∥v∥0 ≤O(\nm\nL log m).\n(c) ∥u⊤W(0)\nb\n\u0000Qb−1\nl=a D(0)\ni,l W(0)\nl\n\u0001\n∥2 ≤O(1)∥u∥2 for all u ∈Rm with ∥u∥0 ≤O(\nm\nL log m).\n(d) For any integer 1 ≤s ≤O(\nm\nL log m), with probability at least 1 −e−Ω(s log m) over the\nrandomness of initialization, we have |u⊤W(0)\nb\n\u0000Qb−1\nl=a D(0)\ni,l W(0)\nl\n\u0001\nv| ≤O(\nq\ns log m\nm\n)∥u∥2∥v∥2\nfor all vectors u, v ∈Rm with ∥u∥0, ∥v0∥≤s.\nLemma B.3 (backward propagation). Suppose m ≥Ω(nL log(nL)), Ω(\nd\nlog m) ≤s ≤O(\nm\nL log m)\nand d ≤O(\nm\nL log m), then for all indices i ∈[n], 1 ≤a ≤L −1,\n(a) with probability at least 1 −eΩ(s log m), for all v ∈Rd such that ∥v∥0 ≤s, we have\n\f\f\fu⊤W(0)\nL\n\u0010 L\nY\nl=a\nD(0)\ni,l W(0)\nl\n\u0011\nv\n\f\f\f ≤O\n\u0010r\ns log m\nm\n\u0011\n∥u∥2∥v∥2\n(b) with probability at least 1 −e−Ω(m/L), for all vectors u ∈Rd, we have\n\r\r\ru⊤W(0)\nL\n\u0010 L\nY\nl=a\nD(0)\ni,l W(0)\nl\n\u0011\r\r\r\n2 ≤O(\np\nm/d) · ∥u∥2\nLemma B.4 (Lemma 8.2(b), 8.2(c) in Allen-Zhu et al. (2018)). Suppose ω ≤O(L−9/2(log m)3),\nwith probability at least 1 −e−Ω(mω2/3L), for every W such that ∥W −W(0)∥2 ≤ω:\n(b) Let the diagonal matrices Di,l, D(0)\ni,l and D′\ni,l be deﬁned as\n(Di,l)k,k = 1{(Wlhi,l−1)k ≥0},\n(D(0)\ni,l )k,k = 1{(W(0)\nl\nh(0)\ni,l−1)k ≥0},\nD′\ni,l = Di,l −D(0)\ni,l\nwe have ∥D′\ni,l∥0 ≤O(mω2/3L) and ∥D′\ni,lWlhi,l−1∥2 ≤O(ωL3/2).\n(c) ∥hi,l −h(0)\ni,l ∥2 ≤O(ωL5/2√log m).\nWe present a modiﬁed lemma on the perturbation analysis of intermediate layers with respect\nto small changes of parameters. Note that in our paper, the last hidden layer is the L −1-th\nlayer.\nLemma B.5 (Modiﬁcation of Lemma 8.6 in Allen-Zhu et al. (2018)). For any interger s such\nthat 1 ≤s ≤O(\nm\nL3 log m), with probability at least 1 −e−Ω(s log m) over the randomness of\ninitialization,\n• for every i ∈[n] and 0 ≤a ≤b ≤L\n• for every diagonal matrices D′′\ni,0, . . . , D′′\ni,L−1 ∈[−3, 3]m×m with at most s non-zero entries.\n• for every perturbation matrices W′ = (W′\n0, W′\n1, . . . , W′\nL−1) ∈(Rm×b, R(m×m)L) with\n∥W∥2 ≤ω ∈[0, 1].\n20\nWe have\n(a) ∥W(0)\nb (D(0)\ni,b−1 + D′′\ni,b−1) · · · (D(0)\ni,a + D′′\ni,a)W(0)\na ∥2 ≤O(\n√\nL).\n(b) ∥(W(0)\nb\n+W′\nb)(D(0)\ni,b−1 +D′′\ni,b−1) · · · (D(0)\ni,a +D′′\ni,a)(W(0)\na +W′\na)∥2 ≤O(\n√\nL) if ω ≤O(L−3/2).\nProof. The only diﬀerence of this lemma and Lemma 8.6 in Allen-Zhu et al. (2018) is that we\nhave taken into account the ﬁrst layer W0 ∈Rm×b. Actually we can go through the same proce-\ndure as in Lemma 7.3 in Allen-Zhu et al. (2018) to give a bound ∥W(0)\nb D(0)\ni,b−1 · · · D(0)\ni,aW(0)\n0 ∥2 ≤\nO(\n√\nL) with probability at least 1 −e−Ω(m/L). Then with the same techniques in the proof of\nLemma 8.6 in Allen-Zhu et al. (2018), we obtain the same result.\nEquipped with this lemma, we are now ready to give our version of backward perturbation\nlemma, which takes into account both the ﬁrst layer and the last layer.\nLemma B.6 (Modiﬁcation of Lemma 8.7 in Allen-Zhu et al. (2018)). Suppose d ≤O(\nm\nL log m)),\n• for any integer s such that Ω(\nd\nlog m) ≤s ≤O(\nm\nL3 log m),\n• for all i ∈[n] and 1 ≤a ≤L,\n• for every diagonal matrices D′′\ni,0, . . . , D′′\ni,L−1 ∈[−3, 3]m×m with at most s non-zero entries,\n• for every perturbation matrices W′ = (W′\ni,0, . . . , W′\ni,L) with ∥W′∥2 ≤ω = O(\n1\nL3/2 ),\nit satisﬁes, with probability at least 1 −e−Ω(s log m) over the randomness of initialization,\n\r\r\r\r(W(0)\nL + W′\nL)\n\u0012\nL\nY\nl=a+1\n(D(0)\ni,l + D′′\ni,l)(W(0)\nl\n+ W′\nl)\n\u0013\n(D(0)\ni,a + D′′\ni,a) −W(0)\nL\n\u0012\nL\nY\nl=a+1\nD(0)\ni,l W(0)\nl\n\u0013\nD(0)\ni,a\n\r\r\r\r\n2\n≤O(\np\nL3s log m/d + ω\np\nL3m/d)\nNote that if s = O(mω2/3L), this perturbation bound becomes O(ω1/3L2p\nm log m/d).\nProof. For notational simplicity we ignore subscripts for i in the proof. Now we compute\n\r\r\r\r(W(0)\nL + W′\nL)\n\u0012\nL\nY\nl=a+1\n(D(0)\nl\n+ D′′\nl )(W(0)\nl\n+ W′\nl)\n\u0013\n(D(0)\na\n+ D′′\na) −W(0)\nL\n\u0012\nL\nY\nl=a+1\nD(0)\nl\nW(0)\nl\n\u0013\nD(0)\na\n\r\r\r\r\n2\n≤\nL−1\nX\nl=a\n\r\r\r\rW(0)\nL\n\u0012\nL\nY\nb=l+1\nD(0)\nb W(0)\nb\n\u0013\r\r\r\r\n2\n|\n{z\n}\n①\n∥D′′\nl ∥2\n\r\r\r\r\n\u0012 lY\nc=a\n(W(0)\nc\n+ W′\nc)(D(0)\nc\n+ D′′\nc)\n\u0013\r\r\r\r\n2\n|\n{z\n}\n②\n+\nL\nX\nl=a\n\r\r\r\rW(0)\nL\nL−1\nY\nb=l+1\n(D(0)\nb W(0)\nb )D(0)\nl\n\r\r\r\r\n2\n|\n{z\n}\n③\n∥W′\nl∥2∥D(0)\nl\n+ D′′\nl ∥2\n\r\r\r\r\n\u0012l−1\nY\nc=a\n(W(0)\nc\n+ W′\nc)(D(0)\nc\n+ D′′\nc)\n\u0013\r\r\r\r\n2\n|\n{z\n}\n④\n≤L · O\n\u0012r\ns log m\nd\n·\n√\nL\n\u0013\n+ L · O\n\u0012p\nm/d · ω ·\n√\nL\n\u0013\n= O(\np\nL3s log m/d + ω\np\nL3m/d)\nwhere ①is from Lemma B.3(a) and the fact that D′′\nl\nQl\nc=a(W(0)\nc\n+W′\nc)(D(0)\nc\n+D′′\nc) is a s-sparse\nmatrix; ②is from Lemma B.5(b); ③is from Lemma B.3(b); ④is again from Lemma B.5(b).\n21\nTo conclude this section, we modify the Claim 11.2 in Allen-Zhu et al. (2018) to ﬁt our\nsetting.\nLemma B.7. Let W ∈B(W(0), ω) and W′ = (W′\n0, W′\n1, . . . , W′\nL) be such that ∥W′∥2 ≤ω,\nwhere ω ≤O(\n1\nL6n3(log m)3/2 ). Denote\nhi,0 = σ(W0xi),\nhi,l = σ(Wlhi,l−1),\nh′\ni,0 = σ((W0 + W′\n0)xi),\nh′\ni,l = σ((Wl + W′\nl)h′\ni,l−1)\nThen their exist diagonal matrices D′′\ni,l ∈Rm×m with entries in [−1, 1] such that, for any i ∈[n]\nand 0 ≤l ≤L −1,\nh′\ni,l −hi,l =\nl\nX\na=1\n\u0012\nlY\nb=a+1\n(Di,l + D′′\ni,l)Wl\n\u0013\n(Di,a + D′′\ni,a)W′\nah′\ni,a−1\nFurther more, with probability at least 1 −e−Ω(mω2/3L), we have\n• ∥h′\ni,l −hi,l∥2 ≤O(L3/2)∥W′∥2,\n• ∥fW+W′(xi) −fW(xi)∥2 ≤O(L\np\nm/d)∥W′∥2,\n• ∥D′′\ni,l∥0 ≤O(mω2/3L).\nBefore we came to the proof of Lemma B.7, we present the following auxiliary lemma.\nLemma B.8 (Proposition 11.3 in Allen-Zhu et al. (2018)). Given vectors a, b ∈Rm and diagonal\nmatrices D where Dk,k = 1ak≥0. Then, there exist a diagonal matrix D′′ ∈Rm with\n• |Dk,k −D′′\nk,k| ≤1 and |D′′\nk,k| ≤1 for k ∈[m],\n• D′′\nk,k ̸= 0 only when 1ak≥0 ̸= 1bk≥0,\n• σ(a) −σ(b) = (D + D′′)(a −b).\nProof of Lemma B.7. The proof is almost the same with the proof of Claim 11.2 in Allen-Zhu et al.\n(2018), and we do not repeat most of its content here. The only diﬀerence in our claim is that we\nconsider the training of the ﬁrst and the last layer. We prove the part of here. Ignore subscripts\nof i for simplicity, we calculate\n∥(WL + W′\nL)h′\nL−1 −WLhL−1∥2\n= ∥W′\nLh′\nL−1 + WL(h′\nL−1 −hL−1)∥2\n= ∥W′\nLh′\nL−1 + W′\nL(σ((WL−1 + W′\nL−1)h′\nL−2) −σ(WlhL−2))∥2\n①= ∥W′\nLh′\nL−1 + WL(DL−1 + D′′\nL−1)((WL−1 + W′\nL−1)h′\nL−2 −WL−1hL−2)∥2\n≤∥W′\nLh′\nL−1∥2 + ∥WL(DL−1 + D′′\nL−1)W′\nL−1h′\nL−2∥2\n+ ∥WL(DL−1 + D′′\nL−1)WL−1(h′\nL−2 −hL−2)∥2\n= ∥W′\nLh′\nL−1∥2\n|\n{z\n}\n≤∥W′\nL∥2∥h′\nl−1∥2\n+\n\r\r\r\r\nL−1\nX\nl=0\nWL\n\u0012 L−1\nY\na=l+1\n(Da + D′′\na)Wa\n\u0013\n(Dl + D′′\nl )\n\r\r\r\r\n2\n|\n{z\n}\n≤O(L√\nm/d) by Lemma B.3 and Lemma B.6\n·∥W′\nlh′\nl−1∥2\n≤O(L\np\nm/d)∥W′∥2\nwhere in ①we have used Lemma B.8. And in the last inequality we have used Lemma B.4(c)\nto give ∥h′\nl∥2 ≤∥h(0)\nl\n∥2 + ∥h′\nl −h(0)\nl\n∥2 ≤O(1) for all 0 ≤l ≤L −1.\n22\nCombine Lemma B.3 Lemma B.7 together, we have a corollary.\nCorollary B.9 (output-boundedness). Let W ∈B(W(0), ω), where ω meets all the requirement\nin previous Lemmas and d ≤O(m/(L log m)), with probability at least 1−O(n)e−Ω(d), we have\n∥fW(0)(xi)∥2 ≤O(1) and ∥fW(xi)∥2 ≤O(1 + ωL\np\nm/d) for all i ∈[n].\nProof. Firstly, from Lemma B.1 we know that, with probability at least 1 −O(nL)e−Ω(m/L) we\nhave\n∥σ(W(0)\nL−1σ(· · · σ(W0xi)))∥2 ≤O(1)\nConditioning on this event, since (W(0)\nL )i,j ∼N(0, 1\nd), (i, j) ∈[d] × [m], we have, over the\nrandomness of W(0)\nL ,\nf q\nW(0)(xi) = WLσ(· · · σ(W0xi)) ∼N(0, κ2Id)\nwhere κ2 ≤O(1\nd). Therefore, with probability at least 1−O(n)e−Ω(d) over the initialization, we\nhave\n∥f q\nW(0)(xi)∥2 ≤O(1)\nand then apply Lemma B.7 to bound the perturbation of W′ = W −W(0), where we have\nassumed ∥W′∥2 ≤ω.\nFinally we present the δ-separateness lemma in Allen-Zhu et al. (2018).\nLemma B.10. [] Suppose δ ≤O(1/L), for every i ̸= j and every layer l ∈[L], we have with\nprobability at least 1 −O(n2)e−Ω(mδ4) over the initialization,\n∥h(0)\ni,l −h(0)\nj,l ∥2 ≥δ/2\nProof. We prove the lemma via induction. Suppose at layer l −1 we have δl−1-separateness,\nthat is\n∥h(0)\ni,l−1 −h(0)\nj,l−1∥2 ≥δl−1\nfor some δl−1 ≥δ/2. We try to prove that it still holdes for layer l. Denote wl,r to be the r-th\nrow of Wl at l-th layer, where wl,r ∈R1×m are row vectors, following the distribution N(0, 2\nmI).\nThen over the randomness of Wl and ﬁx hi,l−1, hj,l−1, we have that wl,rhi,l−1, wl,rhj,l−1 are\ntwo mean zero Gaussian variables (though they are not independent). Therefore σ(wl,khi,l−1)−\nσ(wl,rhj,l−1) may have four diﬀerent output. Now we ignore the subscript of layer l −1 for\nsimplicity and write\nσ(wrhi) −σ(wrhj) =\n\n\n\n\n\n\n\n\n\n\n\nwr(hi −hj),\n♦if both wrhi, wrhj ≥0\n0,\n♣if both wrhi, wrhj ≤0\nwrhi,\n♥if wrhi ≥0, wrhj ≤0\nwrhj,\n♠if wrhi ≤0, wrhj ≥0\nIn the case ♦, we have\nE[(σ(wrhi) −σ(wrhj))2|♦] ≥2δ2\nm\nfrom our inductive assumption. In the case ♣, we have\nE[(σ(wrhi) −σ(wrhj))2|♣] = 0\n23\nIn the case ♥and ♠, we have from Lemma B.1, ∥hi∥2, ∥hj∥2 ∈[1/2, 2] with high probability.\nTherefore we can calculate\nE[(σ(wrhi) −σ(wrhj))2|♥∨♠] ≥2δ2\nm\nNotice that the probability of the event ♣is no more than 1/2 (for ﬁxed (i, j)-pair). So we\nobtain\nE[(σ(wrhi) −σ(wrhj))2] ≥δ2\nm,\nE[(σ(wrhi) −σ(wrhj))2] ≤2\nNow pick up the subscripts for layer l, via Chernoﬀbound, we have, with probability at least\n1 −e−Ω(mδ4),\n∥hi,l −hj,l∥2\n2 ≥\nm\nX\nr=1\n(1 −O(δ))E[(σ(wrhi) −σ(wrhj))2] ≥δ2(1 −O(δ))\nthen we can take a union bound over all (i, j)-pair, and proceed induction step over all layer\n0 ≤l ≤L to conclude the proof.\nC\nProof of Gradient Bounds\nC.1\nKey Calculations\n• For a matrix W or θ, we denote [W]r or [θ]r their r-th row, [W]r or [θ]r their r-th column.\n• For the query encoder f q\nW, we deﬁne Backq\ni,l := WLDi,L−1 · · · Di,lWl, Backq\ni,L = WL\n• For the key encoder f k\nθ , we deﬁne Backk\ni,l := θLDi,L−1 · · · Di,lθl, Backk\ni,L = θL.\n• For gradient ∇WLS(W, θ) with respect to W, we have\n∇[Wl]rLS(W, θ) := 1\nn\nn\nX\ni=1\n\u0010\n(Backq\ni,l+1)⊤\nr g\nlossi\n\u0011\n· σ′(⟨[Wl]r, hi,l−1⟩) · hi,l−1\nwhere g\nlossi is deﬁned as\ng\nlossi := ENeg(i)\n\" k\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,j)\nzi,j\n#\nand qi := f q\nW(xi), zi,j = f k\nθ (xi,j) −f k\nθ (xi).\n• For gradient ∇θLS(W, θ) with respect to θ, we carefully compute\n∇[θl]rLS(W, θ) = 1\nn\nn\nX\ni=1\nENeg\n\u0014\n∇[θl]rℓ(W, θ, xi, {xi,j}k\nj=1)\n\u0015\n= 1\nn\nn\nX\ni=1\n1\n\u0000n−1\nk\n\u0001\nX\n{xi,j}k\nj=1⊂S\\i\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n∇[θl]r\n\u0010\nq⊤\ni (f k\nθ (xi,j) −f k\nθ (xi))\n\u0011\n24\nTo handle this complex summation, we introduce the notation d\nloss(xi, xj) as the loss\nvector (corresponding to θ) which only contains f q\nW(xi) and f k\nθ (xj) in the nominator of\nthe coeﬃcients:\nd\nloss(xi, xj) :=\n1\n\u0000n−1\nk\n\u0001\nX\nxj∈{xi,s}s∈[k]⊂S\\i\nexp(q⊤\ni zj)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n· qi\n(C.1)\nwhere qi = f q\nW(xi), zj = f k\nθ (xj) −f k\nθ (xi), zi,s = f k\nθ (xi,s) −f k\nθ (xi). Then we can rearrange\nterms in ∇[θl]rLS(W, θ) to get\n∇[θl]rLS(W, θ)\n= 1\nn\nn\nX\ni=1\nX\nj̸=i\n\u0012\n(Backk\ni,l)⊤d\nloss(xi, xj)\n\u0013\u0010\nσ′(⟨[θl]r, hj,l−1⟩)hj,l−1 −σ′(⟨[θl]r, hi,l−1⟩)hi,l−1\n\u0011\n= 1\nn\nn\nX\ni=1\n\u0010\n(Backk\ni,l)⊤\u0010X\nj̸=i\n( d\nloss(xj, xi) −d\nloss(xi, xj))\n\u0011\u0011\n· σ′(⟨[θl]r, hi,l−1⟩) · hi,l−1\n= 1\nn\nn\nX\ni=1\n\u0010\n(Backk\ni,l)⊤d\nlossi\n\u0011\n· σ′(⟨[θl]r, hi,l−1⟩) · hi,l−1\n(C.2)\nwhere d\nlossi := P\nj̸=i( d\nloss(xj, xi) −d\nloss(xi, xj)). This form (C.2) of ∇[θl]rLS(W, θ) will\nfacilitate our calculations in the proofs in Subsection C.3.\nC.2\nLemma of Gradient Lower Bound\nWe present our lemma of gradient lower bound at initialization here, where the only diﬀerence\nof our lemma and the Lemma B.2 in Zou and Gu (2019) is that we have a probability bound\n1 −e−Ω(mδ2/n2) instead of 1 −e−Ω(mδ/nd).\nLemma C.1. Assume m ≥Ω(n3dδ−2), Let WL and WL−1 be at random initialization, then\nwith probability at least 1 −e−Ω(mδ2/n2) for any vectors vi, i ∈[n], it holds that\nm\nX\nr=1\n\r\r\r\r\n1\nn\nn\nX\ni=1\n⟨[WL]r, vi⟩σ′(⟨[WL−1]r, hi,L−2⟩)hi,L−2\n\r\r\r\r\n2\n≥Ω\n\u0012 mδ\nn3d\n\u0013\nn\nX\ni=1\n∥vi∥2\n2\nBefore we state the technical lemmas for the proof of Lemma C.1, we introduce the notations\nin Zou and Gu (2019). Let h1, . . . , hn ∈Rm such that 1/2 ≤∥hi∥2 ≤2. Let ¯hi := hi/∥hi∥2 and\nassume ∥¯hi −¯hj∥2 ≥δ/2 (from Lemma B.10 we know this holds with high probability). Now\nwe construct orthonormal matrices Qi = [¯hi, Q′\ni] ∈Rm×m. For a standard gaussian random\nvector w ∼N(0, Im), we decompose w = Qiui = ui,1¯hi + Q′\niu′\ni, where ui,1 is the ﬁrst entry of\nui and u′\ni = (ui,2, . . . , ui,m) ∈Rm−1. Let ξ = √πδ/(16n), deﬁne the following event over the\nrandomness of w:\nWi = {|ui,1| ≤ξ, |⟨Q′\niu′\ni, ¯hj⟩| ≥2ξ for all ¯hj where j ̸= i}\nThen we have\nLemma C.2 (Lemma C.1 in Zou and Gu (2019)). For each Wi and Wj, we have\nP(w ∈Wi) ≥\nδ\nn16\n√\n2e\nand\nWi ∩Wj = ∅\n25\nNow we present two lemmas for technical purposes.\nLemma C.3 (Lemma C.2 in Zou and Gu (2019)). For any numbers a1, . . . , an, let\nh(w) :=\nn\nX\ni=1\naiσ′(⟨w, hi⟩)hi\nwhere w ∼N(0, Im). It holds that\nP\n\u0010\n∥h(w)∥2 ≥|ai|\n4\n\f\f\fw ∈Wi\n\u0011\n≥1/2\nProof of Lemma C.1. Fix v1, . . . , vn. For r ∈[m], deﬁne the function hr as\nhr(WL, WL−1) :=\nn\nX\ni=1\n⟨[WL]r, vi⟩· σ′(⟨wr,L−1, hi⟩) · hi\nwhere [WL]r is the r-th column of WL, and wr,L−1 =\np\nm/2[WL−1]r is the r-th row of\np\nm/2WL−1. Obviously we have σ′(⟨wr,L−1, hi⟩) = σ′(⟨[WL−1]r, hi⟩), and from our initial-\nization scheme we also have wr,L−1 ∼N(0, Im).\nNow we deﬁne events {Ai}i∈[n] over the\nrandomness of [WL]r and [WL−1]r at initialization:\nAi = Ai,1 ∩Ai,2 ∩Ai,3\nwhere\n• Ai,1 := {wr,L−1 ∈Wi},\n• Ai,2 := {∥hr(WL, WL−1)∥2 ≥|⟨[WL]r, vi⟩|/4},\n• Ai,3 := {⟨[WL]r, vi⟩| ≥∥vi∥2/\n√\nd}.\nNow by Lemma C.2 and Lemma C.3, and the independence of WL and WL−1, we have\nP(r ∈Ai) = P(Ai,2|Ai,1) · P(Ai,1) · P(Ai,3) ≥\nδ\n256\n√\n2en\nand\nAi ∩Aj = ∅if i ̸= j\nand also Pn\ni=1 1r∈Ai ≤1. Therefore we can directly calculate\nm\nX\nr=1\n∥hr(WL, WL−1)∥2\n2 ≥\nm\nX\nr=1\n∥hr(WL, WL−1)∥2\n2\nm\nX\nr=1\n1r∈Ai ≥\nm\nX\nr=1\nn\nX\ni=1\n∥vi∥2\n2\n32d 1r∈Ai\nNow deﬁne a random variable Zr := Pn\ni=1 1r∈Ai∥vi∥2\n2/(32d), and from the deﬁnition of Ai we\nknow that (Zr)r∈[m] are independent (since wr,L−1, [WL]r are independent for diﬀenrent r).\nThen for all r ∈[m], we have\nE[Zr] ≥Ω\n\u0012δ Pn\ni=1 ∥vi∥2\n2\ndn\n\u0013\n,\n(E[Zr])2 ≥Ω\n\u0012δ2 Pn\ni=1 ∥vi∥4\n2\nd2n2\n\u0013\n,\nE[Z2\nr ] ≤O\n\u0012Pn\ni=1 ∥vi∥4\n2\nd2\n\u0013\n26\nFrom one-sided Bernstein inequality for nonnegative random variables (see equation (2.23) in\nWainwright (2019)), we have\nP\n\u0012 m\nX\nr=1\n(Zr −E[Zr]) ≤m\n2\n\u0012 1\nm\nm\nX\nr=1\nE[Zr]\n\u0013\u0013\n≤exp\n\u001a\n−Ω\n\u0012m(Pm\nr=1 E[Zr]/m)2\nPm\nr=1 E[Z2r ]/m\n\u0013\u001b\n≤exp\n\u001a\n−Ω\n\u0012m2 minr∈[m](E[Zr])2\nPm\nr=1 E[Z2r ]\n\u0013\u001b\n≤exp(−Ω(mδ2/n2))\nwhich means, with probability at least 1 −e−Ω(mδ2/n2),\n1\nn2\nm\nX\nr=1\n∥hr(WL, WL−1)∥2\n2 ≥\n1\n2n2\nm\nX\nr=1\nE[Zr] ≥Ω\n\u0012 mδ\nn3d\nn\nX\ni=1\n∥vi∥2\n2\n\u0013\nTherefore we have proved the case of ﬁxed vectors (vi)i∈[n]. Applying ε-net argument, we know\nthat for m ≥Ω(n3dδ−2), the probability bound 1 −e−Ω(mδ2/n2) still holds. This concludes the\nproof.\nC.3\nGradient Bounds at Initialization\nWe ﬁrst derive the gradient bounds for updating both W and θ at their random initializations,\nthe result is summarized in the following lemma.\nLemma C.4 (Gradient Bounds at Initialization). With probability at least 1 −2e−Ω(mδ2/n2),\nthe following holds\n• For ∥∇WLS(W(0), θ(0))∥F , we have\nΩ\n\u0012 mδ\nn3d\n\u0013\nn\nX\ni=1\n∥g\nlossi∥2\n2 ≤∥∇WLS(W(0), θ(0))∥2\nF ≤O\n\u0012Lm\nnd\n\u0013\nn\nX\ni=1\n∥g\nlossi∥2\n2\n• For ∥∇θLS(W(0), θ(0)∥F , we have\nΩ\n\u0012 mδ\nn3d\n\u0013\nn\nX\ni=1\n∥d\nlossi∥2\n2 ≤∥∇θLS(W(0), θ(0))∥2\nF ≤O\n\u0012Lm\nnd\n\u0013\nn\nX\ni=1\n∥d\nlossi∥2\n2\nProof. In the proof below, we drop all the superscripts appeared in W(0) and θ(0) for simplicity.\n1. Gradient Upper Bound for updating the query encoder f q\nW(xi): For each i ∈[n]\nand l ∈[L], we calculate\n\r\r\r∇WlLS(W, θ)\n\r\r\r\nF =\n\r\r\r\r\n1\nn\nn\nX\ni=1\nDi,l · g\nloss\n⊤\ni (Backq\ni,l) · h⊤\ni,l−1\n\r\r\r\r\nF\n≤1\nn\nn\nX\ni=1\n∥Di,l∥2 ·\n\r\r g\nloss\n⊤\ni (Backq\ni,l)\n\r\r\n2 · ∥hi,l−1∥2\n①\n≤O(\np\nm/d) · 1\nn\nn\nX\ni=1\n∥g\nlossi∥2 ≤O(\np\nm/nd) ·\n\u0012 n\nX\ni=1\n∥g\nlossi∥2\n2\n\u00131/2\n27\nwhere the inequality ①has employed Lemma B.1 and Lemma B.3 with probability at least\n1 −exp(−Ω(m/L)). Taking squares and summing over l ∈[L] give the desired result.\n2. Gradient Lower Bound for updating the query encoder f q\nW(xi): Applying Lemma\nC.1, we have, with probability at least 1 −exp(−Ω(mδ2/n2)), the following lower bound holds:\n∥∇WLS(W, θ)∥2\nF ≥∥∇WL−1LS(W, θ)∥2\nF\n=\nm\nX\nr=1\n\r\r\r\r\n1\nn\nn\nX\ni=1\n⟨[WL]r, g\nlossi⟩σ′(⟨[WL−1]r, hi,L−2⟩)hi,L−2\n\r\r\r\r\n2\n2\n≥Ω\n\u0012 mδ\nn3d\n\u0013\nn\nX\ni=1\n∥g\nlossi∥2\n2\n3. Gradient Upper Bound for updating the key encoder f k\nθ (xi): From previous calcu-\nlations (C.2), we have\n∥∇θlLS(W, θ)∥F =\n\r\r\r\r\n1\nn\nn\nX\ni=1\nDk\ni,l\n\u0010\n(Backk\ni,l+1)⊤d\nlossi\n\u0011\nh⊤\ni,l−1\n\r\r\r\r\nF\nTherefore\n∥∇θlLS(W, θ)∥F ≤1\nn\nn\nX\ni=1\n∥Di,l∥2 · ∥d\nloss\n⊤\ni Backk\ni,l+1∥2 · ∥hi,l−1∥2\n①\n≤O(\np\nm/d) · 1\nn\nn\nX\ni=1\n∥d\nlossi∥2 ≤O(\np\nm/nd)\n\u0012 n\nX\ni=1\n∥d\nlossi∥2\n2\n\u00131/2\nwhere in ①we have used Lemma B.1, Lemma B.3 again, with probability at least 1−exp(−Ω(m/L)).\nSumming over l ∈L gives the desired result.\n4. Gradient Lower Bound for updating the key encoder f k\nθ (xi): From (C.2), we can\nrewrite the Frobenius norm of the gradient ∇θLLS(W, θL) to the following form:\n∥∇θL−1LS(W, θ)∥2\nF =\nm\nX\nr=1\n\r\r\r\r\n1\nn\nn\nX\ni=1\n⟨[θL]r, d\nlossi⟩· σ′(⟨[θL−1]r, hi,L−2⟩) · hi,L−2\n\r\r\r\r\n2\n2\nApplying Lemma C.1, we have, with probability at least 1 −exp(−Ω(mδ2/n2)), the following\nlower bound holds:\n∥∇θLS(W, θ)∥2\nF ≥∥∇θL−1LS(W, θ)∥2\nF ≥Ω\n\u0012 mδ\nn3d\n\u0013\nn\nX\ni=1\n∥d\nlossi∥2\n2\nThus all the claims are proven.\nC.4\nGradient Bounds After Pertubations\nSince we require the trajectory of the updated parameters W(t) and θ(t) to stay within certain\nneighborhoods B(W(0), ω) and B(θ(0), τ) of the random initilization, we need to prove that the\ngradient bounds remain valid in the neighborhood, which concludes of proof of Lemma A.3\n28\nProof of Lemma A.3. Denote D(0)\ni,l := diag\n\u0010\n1{[W(0)]⊤\nr h(0)\ni,l−1 ≥0}m\nr=1\n\u0011\nand h(0)\ni,l = D(0)\ni,l W(0)\nl\nh(0)\ni,l−1\nto be the activated relus and the hidden-states of l-th layer for input xi at initialization, with\nDi,l, hi,l their perturbed counterparts. Also,for simplicity we deﬁne\nBackq,(0)\ni,l\n= W(0)\nL D(0)\ni,L−1W(0)\nL−1 · · · D(0)\ni,l W(0)\nl\nand vi = g\nlossi. The case of WL is trivial, for l ≤L −1, we can calculate\n∇WlLS(W(0), θ) −∇WlLS(W, θ)\n= 1\nn\nn\nX\ni=1\n\u0012\nv⊤\ni (Backq,(0)\ni,l+1D(0)\ni,l ) · (h(0)\ni,l−1)⊤−v⊤\ni (Backq\ni,l+1Di,l) · (hi,l−1)⊤\n\u0013\nFrom Lemma B.6, we have\n\r\rv⊤\ni Backq,(0)\ni,l+1D(0)\ni,l −v⊤\ni Backq\ni,l+1Di,l\n\r\r\n2 ≤O(ω1/3L2p\nm log m/d) · ∥vi∥2\nFrom Lemma B.3, we have\n\r\rv⊤\ni Backq,(0)\ni,l+1D(0)\ni,l\n\r\r\n2 ≤O(\np\nm/d) · ∥vi∥2\nBy Lemma B.1 and Lemma B.7, we have, for all i ∈[n]\n∥h(0)\ni,l−1∥2 ≤O(1) and ∥hi,l−1 −h(0)\ni,l−1∥2 ≤O(ωL3/2) =⇒∥hi,l−1∥2 ≤O(1)\nPutting together we arrive at\n\r\r\r∇WlLS(W(0), θ) −∇WlLS(W, θ)\n\r\r\r\n2\nF ≤1\nn\nn\nX\ni=1\n\r\r\r\rv⊤\ni (Backq,(0)\ni,l+1D(0)\ni,l ) · (h(0)\ni,l−1 −hi,l−1)⊤\n\r\r\r\r\n2\nF\n+ 1\nn\nn\nX\ni=1\n\r\r\r\rv⊤\ni\n\u0010\nBackq,(0)\ni,l+1D(0)\ni,l −Backq\ni,l+1Di,l\n\u0011\n· h⊤\ni,l−1\n\r\r\r\r\n2\nF\n≤O\n\u0012ω2/3L4m log m\nnd\n\u0013\n·\nn\nX\ni=1\n∥vi∥2\n2\n①\n≤O\n\u0012 mδ\nn3d\n\u0013\nn\nX\ni=1\n∥vi∥2\n2\nwhere ①is from our choice of ω. By summing over l ∈[L], we arrive at the desired results.\nNote that any change of θ only aﬀect the vector (vi)i∈[n], thus our analysis is still valid. The\ncase of ∥∇θLS(W, θ)∥F can be similarly proved.\nD\nThe Semi-smoothness Property\nIn this section, we prove Lemma A.4. Firstly, we present the following two lemmas and their\nproofs.\n29\nD.1\nTechnical Lemmas\nLemma D.1. Let y = (yi)i∈[k]. The function g(y) = log(1+Pk\ni=1 exp(yi)) is 1-Lipschitz smooth\nwith respect to (yi)i∈[k] and satisﬁes\ng(y + y′) ≤g(y) + ∇yg(y)⊤y′ + 1\n2∥y′∥2\n2\nProof. Trivially this function (cross-entropy loss) is convex with respect to y = (yi)k\ni=1, which\nmeans the Hessian ∇2g(y) is positive-semideﬁnite. And we can calculate\n(∇2g(y))i,i =\nexp(yi)\n(1 + Pk\ns=1 exp(ys))2\n\u0010\n1 +\nX\nj̸=i\nexp(yj)\n\u0011\nSumming over i ∈[k], we have Pk\ni=1(∇2g(y))i,i ≤1. And since g(y) is convex, the eigenvalues\n(λi)i∈[k] of ∇2g(y) satisﬁes λi ≥0 and ∥∇2g(y)∥2 ≤Pk\ni=1 λi = Pk\ni=1(∇2g(y))i,i ≤1. Note that\nthe bound for ∥∇2g(y)∥2 is valid for all y = (yi) ∈Rk, which proves the claim by doing simple\nTaylor expansion.\nLemma D.2. For W, f\nW ∈B(W(0), ω) and θ, eθ ∈B(θ(0), τ), where\nω, τ ∈[Ω(\np\nd/m), O(1/(L9/2(log m)3/2))]\nwe have, with probability at least 1 −e−Ω(mω3/2L) −e−Ω(mτ 3/2L) over the initialization,\nLS(f\nW, eθ) −LS(W, θ) ≤1\nn\nn\nX\ni=1\n⟨g\nlossi, ˜qi −qi⟩+ ⟨d\nlossi, ˜ki −ki⟩\n+ O\n\u0012kL2m2\nd2\n\u0013\u0010\nτ 2∥f\nW −W∥2\n2 + ω2∥eθ −θ∥2\n2\n\u0011\nProof. Recall from Deﬁnition 3.3 that our loss function is of the form:\nLS(W, θ) = 1\nn\nn\nX\ni=1\nENeg(i)[ℓ(f q\nW, f k\nθ , xi, {xi,j}k\nj=1)]\n= 1\nn\nn\nX\ni=1\n1\n\u0000n−1\nk\n\u0001\nX\n{xi,j}k\nj=1⊂S\\i\nlog\n\u0012\n1 +\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n\u0013\n= 1\nn\nn\nX\ni=1\nX\nj̸=i\nX\nxj∈{xi,j}k\nj=1⊂S\\i\n1\n\u0000n−1\nk\n\u0001 log\n\u0012\n1 +\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n\u0013\nwhere qi := f q\nW(xi), zi,j := f k\nθ (xi,j)−f k\nθ (xi) and zj = f k\nθ (xj)−f k\nθ (xi). Now for a set of diﬀerent\nparameters f\nW ∈B(W(0), ω) and eθ ∈B(θ(0), τ), we deﬁne new queries and keys as\n˜qi := f q\nf\nW(xi),\n˜zi,j := f k\neθ (xi,j) −f k\neθ (xi)\n30\nApplying Lemma D.1, we have\nℓ(f q\nf\nW, f k\neθ , xi, {xi,j}k\nj=1) = log\n\u0012\n1 +\nk\nX\nj=1\nexp(˜q⊤\ni ˜zi,j)\n\u0013\n≤log\n\u0012\n1 +\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n\u0013\n+\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n·\n\u0010\n˜q⊤\ni ˜zi,j −q⊤\ni zi,j\n\u0011\n+ 1\n2\nk\nX\nj=1\n(˜q⊤\ni ˜zi,j −q⊤\ni zi,j)2\n= ℓ(f q\nW, f k\nθ , xi, {xi,j}k\nj=1) + Φ1 + Φ2\nWe now decompose ˜q⊤\ni ˜zi,j −q⊤\ni zi,j:\n˜q⊤\ni ˜zi,j −q⊤\ni zi,j = (˜qi −qi)⊤zi,j + q⊤\ni (˜zi,j −zi,j) + (˜qi −qi)⊤(˜zi,j −zi,j)\n(D.1)\nTherefore Φ1 can be calculated as\nΦ1 =\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n·\n\u0010\n˜q⊤\ni ˜zi,j −q⊤\ni zi,j\n\u0011\n=\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n· z⊤\ni,j(˜qi −qi)\n+\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n· q⊤\ni (˜zi,j −zi,j)\n+\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n· (˜qi −qi)⊤(˜zi,j −zi,j)\n= Ψ1(xi, {xi,j}k\nj=1) + Ψ2(xi, {xi,j}k\nj=1) + Ψ3(xi, {xi,j}k\nj=1)\nFor Ψ1(xi, {xi,j}k\nj=1), its expectation with respect to negative sampling {xi,j}k\nj=1 ⊂S\\i is\nENeg(i)[Ψ1(xi, {xi,j}k\nj=1)] = ENeg(i)\n\u0014\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n· zi,j\n\u0015⊤\n(˜qi −qi)\n①= ⟨g\nlossi, (˜qi −qi)⟩\nwhere ①is from Deﬁnition A.1, which implies\n1\nn\nn\nX\ni=1\nENeg(i)[Ψ1(xi, {xi,j}k\nj=1)] = 1\nn\nn\nX\ni=1\n⟨g\nlossi, ˜qi −qi⟩\n(D.2)\n31\nNow for Ψ2(xi, {xi,j}k\nj=1), we calculate\n1\nn\nn\nX\ni=1\nENeg(i)[Ψ2(xi, {xi,j}k\nj=1)]\n= 1\nn\nn\nX\ni=1\n1\n\u0000n−1\nk\n\u0001\nX\n{xi,j}k\nj=1⊂S\\i\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n· q⊤\ni (˜zi,j −zi,j)\n= 1\nn\nn\nX\ni=1\n1\n\u0000n−1\nk\n\u0001\nX\n{xi,j}k\nj=1⊂S\\i\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n· q⊤\ni (˜zi,j −zi,j)\n(D.3)\nNow set notations\nki,j = f k\nθ (xi,j),\n˜ki,j = f k\neθ (xi,j),\nki = f k\nθ (xi),\n˜ki = f k\neθ (xi)\nthen we can rearrange (D.3) to\n1\nn\nn\nX\ni=1\nENeg(i)[Ψ2(xi, {xi,j}k\nj=1)]\n= 1\nn\nn\nX\ni=1\nX\nj̸=i\n1\n\u0000n−1\nk\n\u0001\nX\nxj∈{xi,j}k\nj=1⊂S\\i\nexp(q⊤\ni (kj −ki))\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n· q⊤\ni\n\u0010\n(˜kj −kj) −(˜ki −ki)\n\u0011\n①= 1\nn\nn\nX\ni=1\nX\nj̸=i\nd\nloss(xi, xj)⊤\u0010\n(˜kj −kj) −(˜ki −ki)\n\u0011\n= 1\nn\nn\nX\ni=1\nX\nj̸=i\n( d\nloss(xj, xi) −d\nloss(xi, xj))⊤(˜ki −ki)\n②= 1\nn\nn\nX\ni=1\n⟨d\nlossi, ˜ki −ki⟩\nwhere ①and ②are both from Deﬁnition A.1. For Ψ3(xi, {xi,j}k\nj=1), we can use Cauchy-Schwarz\ninequality to get\nΨ3(xi, {xi,j}k\nj=1) ≤\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n· ∥˜qi −qi∥2 · ∥˜zi,j −zi,j∥2\n①\n≤O\n\u0012L2m\nd\n\u0013\n· ∥f\nW −W∥2 · ∥eθ −θ∥2\n≤O\n\u0012L2m\nd\n\u0013\u0010\n∥f\nW −W∥2\n2 + ∥eθ −θ∥2\n2\n\u0011\nwhere in ①we have employed Lemma B.7, which requires ∥f\nW −W∥2 ≤ω and ∥eθ −θ∥2 ≤τ.\nThis implies\n1\nn\nn\nX\ni=1\nENeg(i)[Ψ3(xi, {xi,j}k\nj=1)] ≤O\n\u0012L2m\nd\n\u0013\u0010\n∥f\nW −W∥2\n2 + ∥eθ −θ∥2\n2\n\u0011\n32\nNow we come to deal with Φ2. From the decomposition (D.1) we have\nΦ2 = 1\n2\nk\nX\nj=1\n(˜q⊤\ni ˜zi,j −q⊤\ni zi,j)2\n≤3\n2\nk\nX\nj=1\n\u0010\n∥˜qi −qi∥2\n2 · ∥zi,j∥2\n2 + ∥qi∥2\n2 · ∥˜zi,j −zi,j∥2\n2 + ∥˜qi −qi∥2\n2 · ∥˜zi,j −zi,j∥2\n2\n\u0011\n①\n≤O\n\u0012kL2m2\nd2\n\u0013\n·\n\u0010\nτ 2∥f\nW −W∥2\n2 + ω2∥eθ −θ∥2\n2 + τ∥f\nW −W∥2 · ω∥eθ −θ∥2\n2\n\u0011\n②\n≤O\n\u0012kL2m2\nd2\n\u0013\n·\n\u0010\nτ 2∥f\nW −W∥2\n2 + ω2∥eθ −θ∥2\n2\n\u0011\nwhere ①have employed Lemma B.9 to obtain ∥qi∥2\n2, ∥zi,j∥2\n2 ≤O(1) (smaller than ωL\np\nm/d) at\ninitialization and Lemma B.7 to obtain\n∥˜qi −qi∥2\n2 ≤O\n\u0012L2m\nd\n\u0013\n· ∥f\nW −W∥2\n2,\n∥˜zi,j −zi,j∥2\n2 ≤O\n\u0012L2m\nd\n\u0013\n· ∥eθ −θ∥2\n2\nand ②is due to Cauchy-Schwarz inequality. Thus we can prove the claim by taking expectations\nwith respect to negative sampling of {xi,j}k\nj=1 ⊂S\\i and sum over i ∈[n].\nD.2\nProof of Lemma A.4\nProof of Lemma A.4. Similar to the proofs of previous lemmas, we set notations as follows. For\nparameters W′, θ′ with ∥W′∥2 ≤ω, ∥θ′∥2 ≤τ, we denote\n˜qi = f q\nW+W′(xi),\nqi = f q\nW(xi),\n˜ki = f k\nθ+θ′(xi),\nki = f k\nθ (xi).\nApplying Lemma D.2, we can calculate\nLS(W + W′, θ + θ′) −LS(W, θ) −⟨∇WLS(W, θ), W′⟩−⟨∇θLS(W, θ), θ′⟩\n≤−⟨∇WLS(W, θ), W′⟩−⟨∇θLS(W, θ), θ′⟩+ 1\nn\nn\nX\ni=1\n⟨g\nlossi, ˜qi −qi⟩+ ⟨d\nlossi, ˜ki −ki⟩\n+ O\n\u0012kL2m2\nd2\n\u0013\u0010\nτ 2∥W′∥2\n2 + ω2∥θ′∥2\n2\n\u0011\n= F1 + F2 + F3\nwhere\nF1 = −⟨∇WLS(W, θ), W′⟩+ 1\nn\nn\nX\ni=1\n⟨g\nlossi, ˜qi −qi⟩\nF2 = −⟨∇θLS(W, θ), θ′⟩+ 1\nn\nn\nX\ni=1\n⟨d\nlossi, ˜ki −ki⟩\nF3 = O\n\u0012kL2m2\nd2\n\u0013\u0010\nτ 2∥W′∥2\n2 + ω2∥θ′∥2\n2\n\u0011\nThe goal here is to obtain bounds for F1 and F2, so we divide our proof into two steps:\n33\nStep 1. The case of F1:\nFor F1 we have F1 = 1\nn\nPn\ni=1 F i\n1, where F i\n1 can be calculated as\nF i\n1 = g\nloss\n⊤\ni\n \nf q\nW+W′(xi) −f q\nW(xi) −\nL\nX\nl=0\nWL\n\u0012 L−1\nY\na=l+1\nDi,aWa\n\u0013\nDi,lW′\nlhi,l−1\n!\n= g\nloss\n⊤\ni\n \n(WL + W′\nL)h′\ni,L−1 −WLhi,L−1 −\nL\nX\nl=0\nWL\n\u0012 L−1\nY\na=l+1\nDi,aWa\n\u0013\nDi,lW′\nlhi,l−1\n!\nNow recall our notations:\nh′\ni,−1 = xi,\nh′\ni,l = σ((Wl + W′\nl)h′\ni,l−1)\nfor 0 ≤l ≤L −1\nBy applying Lemma B.7, for all i ∈[n] and k ∈[m], there exist diagonal matrices D′′\ni,l such that\n|(Di,l + D′′\ni,l)k,k| ≤1, and\n(WL + W′\nL)h′\ni,L−1 −WLhi,L−1\n= W′\nLh′\ni,L−1 +\nL−1\nX\nl=0\nWL\n\u0012 L−1\nY\na=l+1\n(Di,a + D′′\ni,a)Wa\n\u0013\n(Di,l + D′′\ni,l)W′\nlh′\ni,l−1\nSo we can further calculte\nF i\n1 = g\nloss\n⊤\ni\n\u0014\nW′\nLh′\ni,L−1 +\nL−1\nX\nl=0\nWL\n\u0012 L−1\nY\na=l+1\n(Di,a + D′′\ni,a)Wa\n\u0013\n(Di,l + D′′\ni,l)W′\nlh′\ni,l−1\n−W′\nLhi,L−1 +\nL−1\nX\nl=0\nWL\n\u0012 L−1\nY\na=l+1\nDi,aWa\n\u0013\nDi,lW′\nlhi,l−1\n\u0015\n= g\nloss\n⊤\ni\nL−1\nX\nl=0\n\u0014\nWL\nL−1\nY\na=l+1\n\u0000(Di,a + D′′\ni,a)Wa\n\u0001\n(Di,l + D′′\ni,l) −WL\nL−1\nY\na=l+1\n\u0000Di,aWa\n\u0001\nDi,l\n\u0015\nW′\nlh′\ni,l−1\n+ g\nloss\n⊤\ni\n\u0014\nW′\nL +\nL−1\nX\nl=0\nWL\n\u0012\nL\nY\na=l+1\nDi,aWa\n\u0013\nDi,lW′\nl\n\u0015\n(h′\ni,l−1 −hi,l−1)\n=\nL−1\nX\nl=0\nQl\n1 +\nL\nX\nl=0\nQl\n2\n(D.4)\nTherefore, we can bound the two terms Ql\n1 and Ql\n2 separately. For Ql\n1, we apply Lemma B.6 with\ns = O(mω2/3L) (where the choice of s is from Lemma B.4(b)), the Cauchy-Schwarz theorem,\nand the boundedness of h′\ni,l−1 with respect to perturbations to get:\nQl\n1 = g\nloss\n⊤\ni\n\u0014\nWL\n\u0012\nL\nY\na=l+1\n(Di,a + D′′\ni,a)Wa\n\u0013\n(Di,l + D′′\ni,l) −WL\n\u0012\nL\nY\na=l+1\nDi,aWa\n\u0013\nDi,l\n\u0015\nW′\nlh′\ni,l−1\n≤∥g\nlossi∥\n\r\r\r\r\rWL\nL\nY\na=l+1\n\u0000(Di,a + D′′\ni,a)Wa\n\u0001\n(Di,l + D′′\ni,l) −WL\nL\nY\na=l+1\n(Di,aWa)Di,l\n\r\r\r\r\r∥W′\nlh′\ni,l−1∥\n≤∥g\nlossi∥2 · O\n\u0012ω1/3L2√m log m\n√\nd\n\u0013\n· ∥W′\nl∥F\n(D.5)\n34\nand for the second term Ql\n2, when l = L, we have\nQl\n2 ≤∥g\nlossi∥2 · ∥W′\nL∥2 · ∥h′\ni,L−1 −hi,L−1∥2 ≤∥g\nlossi∥2 · O\n\u0012ω1/3L2√log m\n√\nd\n\u0013\n· ∥W′∥F\nfor l ≤L −1, we calculate\nQl\n2 = g\nloss\n⊤\ni\n\u0014\nWL\n\u0012\nL\nY\na=l+1\nDi,aWa\n\u0013\nDi,lW′\nl\n\u0015\n(h′\ni,l−1 −hi,l−1)\n= g\nloss\n⊤\ni\n\u0014\nWL\n\u0012 L−1\nY\na=l+1\nDi,aWa\n\u0013\nDi,l −B\n\u0012\nL\nY\na=l+1\nD0\ni,aW(0)\na\n\u0013\nD(0)\ni,l\n\u0015\nW′\nl(h′\ni,l−1 −hi,l−1)\n+ g\nloss\n⊤\ni WL\n\u0012 L−1\nY\na=l+1\nD(0)\ni,aW(0)\na\n\u0013\nD(0)\ni,l W′\nl(h′\ni,l−1 −hi,l−1)\n≤∥g\nlossi∥2 · O\n\u0012rm\nd + ω1/3L2√m log m\n√\nd\n\u0013\n· ∥W′\nl∥F · ∥h′\ni,l−1 −hi,l−1∥2\nAgain from Lemma B.7 that ∥h′\ni,l −hi,l∥≤O(L3/2)∥W∥2 and our choice of ω we have\nQl\n2 ≤∥g\nlossi∥2 · O\n\u0012ω1/3L2√m log m\n√\nd\n\u0013\n· ∥W′\nl∥F\n(D.6)\nCombining (D.5) and (D.6), we have\nF1 = 1\nn\nn\nX\ni=1\nF i\n1 ≤1\nn\nn\nX\ni=1\nL\nX\nl=0\n∥g\nlossi∥2 · O\n\u0012ω1/3L2√m log m\n√\nd\n\u0013\n· ∥W′\nl∥F\n≤∥g\nloss∥2 · O\n\u0012ω1/3L2√m log m\n√\nnd\n\u0013\n· ∥W′∥F\n(D.7)\nwhich proves the case of F1.\nStep 2. The case of F2:\nwe rearrange the ﬁrst order term ⟨∇θLS(W, θ), θ′⟩to a more operable form: ﬁrst we calculate,\nas in Section C.1,\n⟨∇θLS(W, θ), θ′⟩\n=\n\u001c 1\nn\nn\nX\ni=1\nENeg\n\u0014\n∇θℓ(W, θ, xi, {xi,j}k\nj=1)\n\u0015\n, θ′\n\u001d\n= 1\nn\nn\nX\ni=1\n1\n\u0000n−1\nk\n\u0001\nX\n{xi,j}k\nj=1⊂S\\i\nk\nX\nj=1\nexp(q⊤\ni zi,j)\n1 + Pk\ns=1 exp(q⊤\ni zi,s)\n\n∇θ q⊤\ni (f k\nθ (xi,j) −f k\nθ (xi)), θ′\u000b\n= 1\nn\nn\nX\ni=1\nX\nj̸=i\nd\nloss(xi, xj)⊤\u0010\n∇θf k\nθ (xj)(θ′) −∇θf k\nθ (xi)(θ′)\n\u0011\n= 1\nn\nn\nX\ni=1\nX\nj̸=i\n\u0000 d\nloss(xj, xi) −d\nloss(xi, xj)\n\u0001⊤∇θf k\nθ (xi)(θ′)\n= 1\nn\nn\nX\ni=1\nd\nloss\n⊤\ni ∇θf k\nθ (xi)(θ′)\n(D.8)\n35\nwhere we have denote\n∇θf k\nθ (xi)(θ′) :=\nL\nX\nl=0\nθLDi,L−1θL−1 · · · θl+1Di,l θ′\nl hi,l−1\nNow we let F2 = 1\nn\nPn\ni=1 F i\n2, where\nF i\n2 = d\nloss\n⊤\ni\n\u0014\n(θL + θ′\nL)h′\ni,L −θLhi,L −\nL\nX\nl=0\nθL\n\u0012 L−1\nY\na=l+1\nDi,Lθa\n\u0013\nDi,l θ′\nl hi,l−1\n\u0015\n(D.9)\nby Lemma B.7, we have\n(θL + θ′\nL)h′\ni,L−1 −θLhi,L−1 = θ′\nLh′\ni,L−1 +\nL−1\nX\nl=0\n\u0012 L−1\nY\na=l+1\n(Di,a + D′′\ni,a)θa\n\u0013\n(Di,l + D′′\ni,l) θ′\nl h′\ni,l−1\nSubstitute this into equation (D.9), we can further calculate\nF i\n2 = d\nloss\n⊤\ni\n\u0014\nθ′\nLh′\ni,L−1 +\nL−1\nX\nl=1\nθL\n\u0012 L−1\nY\na=l+1\n(Di,a + D′′\ni,a)θa\n\u0013\n(Di,l + D′′\ni,l)θ′\nl h′\ni,l−1\n\u0015\n−d\nloss\n⊤\ni\nL\nX\nl=0\nθL\n\u0012 L−1\nY\na=l+1\nDi,aθa\n\u0013\nDi,l θ′\nl hi,l−1\n=\nL−1\nX\nl=0\nd\nloss\n⊤\ni\n\"\nθL\n\u0012 L−1\nY\na=l+1\n(Di,a + D′′\ni,a)θa\n\u0013\n(Di,l + D′′\ni,l) θ′\nl −θL\n\u0012 L−1\nY\na=l+1\nDi,aθa\n\u0013\nDi,l θ′\nl\n#\nh′\ni,l−1\n+\nL\nX\nl=0\nd\nloss\n⊤\ni θL\n\u0012\nL\nY\na=l+1\nDi,aθa\n\u0013\nDi,l θ′\nl\n\u0000h′\ni,l−1 −hi,l−1\n\u0001\n= F i\n2,1 + F i\n2,2\nNow we apply Cauchy-Schwarz inequality to F i\n2,1 and get\nF i\n2,1\n①\n≤\nL−1\nX\nl=0\n∥d\nlossi∥2\n\r\r\r\rθL\n\u0012\nL\nY\na=l+1\n(Di,a + D′′\ni,a)θa\n\u0013\n(Di,l + D′′\ni,l) −θL\n\u0012\nL\nY\na=l+1\nDi,aθa\n\u0013\nDi,l\n\r\r\r\r\n2\n∥θ′\nl∥F\n②\n≤∥d\nlossi∥· O\n\u0012ω1/3L2√m log m\n√\nd\n\u0013\n· ∥θ′∥F\nwhere in ①we have used Lemma B.1 and Lemma B.9 to obtain the boundedness of ∥h′\ni,l−1∥2,\nand in ②we have used Lemma B.6. On the other hand, we have\nF i\n2,2 ≤\nL−1\nX\nl=0\n∥d\nlossi∥2 · O\n\u0012rm\nd + τ 1/3L2√m log m\n√\nd\n\u0013\n· ∥θ′\nl∥F · ∥h′\ni,l−1 −hi,l−1∥2\n≤∥d\nlossi∥2 · O\n\u0012τ 1/3L2√m log m\n√\nd\n\u0013\n· ∥θ′∥F\nvia applying Lemma B.1, Lemma B.6 and Lemma B.7, and also by our choice of τ. This implies\nF2 = 1\nn\nn\nX\ni=1\nF i\n2 = 1\nn\nn\nX\ni=1\n(F i\n2,1 + F i\n2,2) ≤∥d\nloss∥2 · O\n\u0012τ 1/3L2√m log m\n√\nnd\n\u0013\n· ∥θ′∥F\nAnd thus we conclude the proof.\n36\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-02-17",
  "updated": "2021-05-30"
}