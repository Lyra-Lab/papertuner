{
  "id": "http://arxiv.org/abs/2002.07306v2",
  "title": "From English To Foreign Languages: Transferring Pre-trained Language Models",
  "authors": [
    "Ke Tran"
  ],
  "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream\nnatural language processing (NLP) tasks. The availability of multilingual\npre-trained models enables zero-shot transfer of NLP tasks from high resource\nlanguages to low resource ones. However, recent research in improving\npre-trained models focuses heavily on English. While it is possible to train\nthe latest neural architectures for other languages from scratch, it is\nundesirable due to the required amount of compute. In this work, we tackle the\nproblem of transferring an existing pre-trained model from English to other\nlanguages under a limited computational budget. With a single GPU, our approach\ncan obtain a foreign BERT base model within a day and a foreign BERT large\nwithin two days. Furthermore, evaluating our models on six languages, we\ndemonstrate that our models are better than multilingual BERT on two zero-shot\ntasks: natural language inference and dependency parsing.",
  "text": "From English to Foreign Languages:\nTransferring Pretrained Language Models\nKe Tran\nAmazon Alexa AI\ntrnke@amazon.com\nAbstract\nPretrained models have demonstrated their ef-\nfectiveness in many downstream natural lan-\nguage processing (NLP) tasks. The availabil-\nity of multilingual pretrained models enables\nzero-shot transfer of NLP tasks from high re-\nsource languages to low resource ones. How-\never, recent research in improving pretrained\nmodels focuses heavily on English. While it\nis possible to train the latest neural architec-\ntures for other languages from scratch, it is un-\ndesirable due to the required amount of com-\npute.\nIn this work, we tackle the problem\nof transferring an existing pretrained model\nfrom English to other languages under a lim-\nited computational budget. With a single GPU,\nour approach can obtain a foreign BERTBASE\nmodel within a day (20 hours) and a foreign\nBERTLARGE within two days (46 hours). Fur-\nthermore, evaluating our models on six lan-\nguages, we demonstrate that our models are\nbetter than multilingual BERT on two zero-\nshot tasks: natural language inference and de-\npendency parsing.\nOur code is available at\nhttps://github.com/anonymized.\n1\nIntroduction\nPretrained models (Devlin et al., 2019; Peters et al.,\n2018) have received much of attention recently\nthanks to their impressive results in many down-\nstream NLP tasks. Additionally, multilingual pre-\ntrained models enable many NLP applications for\nother languages via zero-short cross-lingual trans-\nfer. Zero-shot cross-lingual transfer has shown\npromising results for rapidly building applications\nfor low resource languages. Wu and Dredze (2019)\nshow the potential of multilingual BERT (Devlin\net al., 2019) in zero-shot transfer for a large number\nof languages from different language families on\nﬁve NLP tasks, namely, natural language inference,\ndocument classiﬁcation, named entity recognition,\npart-of-speech tagging, and dependency parsing.\nAlthough multilingual models are an important\npiece for building up language technology in many\nlanguages, recent research on improving pretrained\nmodels puts much emphasis on English (Radford\net al., 2019; Dai et al., 2019; Yang et al., 2019).\nThe current state of affairs makes it difﬁcult to\ntranslate developments in pre-training from English\nto non-English languages. To our best knowledge,\nthere are only three publicly available multilingual\npretrained models to date:\n1. The multilingual BERT (mBERT) model (De-\nvlin et al., 2019) that supports 104 languages;\n2. Cross-lingual language model (XLM-R)1\n(Lample and Conneau, 2019; Conneau et al.,\n2019) that supports 100 languages;\n3. Language Agnostic SEntence Representa-\ntions model (LASER)2 (Artetxe and Schwenk,\n2019) that supports 93 languages.\nAmong the three models, LASER is based on neu-\nral machine translation approach and strictly re-\nquires parallel data to train.\nA common practice to train a large-scale mul-\ntilingual model is to do so from scratch. But do\nmultilingual models always need to be trained from\nscratch? Can we transfer linguistic knowledge\nlearned by English pretrained models to other lan-\nguages? In this work, we develop a technique to\nrapidly transfer an existing pretrained model from\nEnglish to other languages. As the ﬁrst step, we\nfocus on building a bilingual language model (LM)\nof English and a target language. Starting from\na pretrained English LM, we learn the target lan-\nguage speciﬁc parameters (i.e., word embeddings),\nwhile keeping the encoder layers of the pretrained\n1https://github.com/facebookresearch/\nXLM\n2https://github.com/facebookresearch/\nLASER\narXiv:2002.07306v2  [cs.CL]  29 Apr 2020\nEnglish LM ﬁxed. We then ﬁne-tune both English\nand target model to obtain the bilingual LM. We ap-\nply our approach to autoencoding language models\nwith masked language model objective and show\nthe advantage of the proposed approach in zero-\nshot transfer. Our main contributions in this work\nare:\n• We propose a fast adaptation method for ob-\ntaining a bilingual BERTBASE of English and\na target language within a day using one Tesla\nV100 16GB GPU (§3).\n• We evaluate our bilingual LMs for six lan-\nguages on two zero-shot cross-lingual trans-\nfer tasks, namely natural language inference\n(XNL) (Conneau et al., 2018) and universal\ndependency parsing. We show that our models\noffer competitive performance or even better\nthat mBERT (§4).\n• We illustrate that our bilingual LMs can serve\nas an excellent feature extractor in supervised\ndependency parsing task (§5).\nConcurrent to our work, Artetxe et al. (2019)\ntransfer pretrained English model to other lan-\nguages by ﬁne-tuning only randomly initialized\ntarget word embeddings while keeping the Trans-\nformer encoder ﬁxed. Their approach is simpler\nthan ours but requires more compute (64 TPUv3\nchips) to achieve good results.\n2\nBilingual Pretrained LMs\nWe ﬁrst provide some background of pretrained\nlanguage models.\nLet Ee be English word-\nembeddings and enc(θ) be the Transformer\n(Vaswani et al., 2017) encoder with parameters\nθ. Let ewi denote the embedding of word wi (i.e.,\newi = Ee[w1]). We omit positional embeddings\nand bias for clarity. A pretrained LM typically\nperforms the following computations:\n(i) transform a sequence of input tokens to con-\ntextualized representations [cw1, . . . , cwn] =\nenc(ew1, . . . , ewn; θ)\n(ii) predict an output word yi at ith position\np(yi | cwi) ∝exp(c⊤\nwieyi)\nAutoencoding LM (Devlin et al., 2019) corrupts\nsome input tokens wi by replacing them with a spe-\ncial token [MASK]. It then predicts the original\ntokens yi = wi from the corrupted tokens. Au-\ntoregressive LM (Radford et al., 2019) predicts the\nnext token (yi = wi+1) given all the previous to-\nkens. The recently proposed XLNet model (Yang\net al., 2019) is an autoregressive LM that factor-\nizes output with all possible permutations, which\nshows empirical performance improvement over\nGPT-2 due to the ability to capture bidirectional\ncontext. Here we assume that the encoder performs\nnecessary masking with respect to each training\nobjective.\nGiven an English pretrained LM, we wish to\nlearn a bilingual LM for English and a given tar-\nget language ℓunder a limited computational re-\nsource budget. To quickly build a bilingual LM,\nwe directly adapt the English pre-traind model to\nthe target model. Our approach consists of two\nsteps. First, we initialize target language word-\nembeddings Eℓin the English embedding space\nsuch that embeddings of a target word and its En-\nglish equivalents are close together (§2.1). Next,\nwe construct a bilingual LM of Ee, Eℓ, and enc(θ)\nand ﬁne-tune all the parameters (§2.2). Figure 1\nillustrates the two steps in our approach.\n[ CLS]\npi pe\nn' est\nCeci\npas\n!\nyou\ndoubl e\nI\n[ CLS]\nFi ne- t uni ng bot h f or ei gn and Engl i sh\nune\nShar ed Tr ansf or mer  Layer s\n[ CLS]\nCeci\nn' est\npas\n[ MASK]\npi pe\nShar ed Tr ansf or mer  Layer s\n[ CLS]\nI\ndoubl e\n[ MASK]\nyou\n!\ndar e\n2\nI ni t i al i zi ng f or ei gn embeddi ngs\nEngl i sh embeddi ngs\nFor ei gn embeddi ngs\n1\nspar se wor d \nt r ansl at i on mat r i x\nFigure 1: Illustration of our two-step approach. In the\nﬁrst step, foreign embeddings are initialized in English\nspace (§2.1). In the second step, we joinly ﬁne-tune\nboth English and foreign models (§2.2).\n2.1\nInitializing Target Embeddings\nOur approach to learn the initial foreign word em-\nbeddings Eℓ∈R|Vℓ|×d is based on the idea of\nmapping the trained English word embeddings\nEe ∈R|Ve|×d to Eℓsuch that if a foreign word and\nan English word are similar in meaning then their\nembeddings are similar. We represent each foreign\nword embedding Eℓ[i] ∈Rd as a linear combina-\ntion of English word embeddings Ee[j] ∈Rd\nEℓ[i] =\n|Ve|\nX\nj=1\nαijEe[j] = αiEe\n(1)\nwhere αi\n∈\nR|Ve| is a sparse vector and\nP|Ve|\nj\nαij = 1.\nIn this step of initializing foreign embeddings,\nhaving a good estimation of α could speed of the\nconvergence when tuning the foreign model and\nenable zero-shot transfer (§5). In the following, we\ndiscuss how to estimate αi ∀i ∈{1, 2, . . . , |Vℓ|}\nunder two scenarios: (i) we have parallel data of\nEnglish-foreign, and (ii) we only rely on English\nand foreign monolingual data.\nLearning from Parallel Corpus\nGiven an\nEnglish-foreign parallel corpus, we can estimate\nword translation probability p(ej | ℓi) for any\n(English-foreign) pair (ej, ℓi) using popular word-\nalignment (Brown et al., 1993) toolkits such as\nfast-align (Dyer et al., 2013). We then assign\nαij = p(ej | ℓi)\n(2)\nSince αi is estimated from word alignment, it is a\nsparse vector.\nLearning from Monolingual Corpus\nFor low\nresource languages, parallel data may not be avail-\nable. In this case, we rely only on monolingual\ndata (e.g., Wikipedias). We estimate word trans-\nlation probabilities from word embeddings of the\ntwo languages. Word vectors of these languages\ncan be learned using fastText (Bojanowski et al.,\n2017) and then are aligned into a shared space\nwith English (Lample et al., 2018b; Joulin et al.,\n2018). Unlike learning contextualized representa-\ntions, learning word vectors is fast and computa-\ntionally cheap. Given the aligned vectors ¯Eℓof\nforeign and ¯Ee of English, we calculate the word\ntranslation matrix A ∈R|Vℓ|×|Ve| as\nA = sparsemax( ¯Eℓ¯E⊤\ne )\n(3)\nHere, we use sparsemax (Martins and Astudillo,\n2016) instead of softmax. sparsemax is a sparse\nversion of softmax and it puts zero probabilities\non most of the words in the English vocabulary\nexcept few English words that are similar to a given\nforeign word. This property is desirable in our\napproach since it leads to a better initialization of\nthe foreign word embeddings.\n2.2\nFine-tuning Bilingual LM\nWe create a bilingual LM by plugging foreign lan-\nguage speciﬁc parameters to the pretrained English\nLM (Figure 1). The new model has two separate\nword embedding layers (and output layers), one for\nEnglish and one for foreign language. The encoder\nlayer in between is shared. We then ﬁne-tune this\nmodel using English and foreign monolingual data.\nHere, we keep tuning the model on English to en-\nsure that it does not forget what it has learned in\nEnglish and that we can use the resulting model for\nzero-shot transfer (§3). In this step, the encoder is\nupdated so that in can learn syntactic aspects (i.e.,\nword order, morphological agreement) of the target\nlanguages.\n3\nZero-shot Experiments\nIn the scope of this work, we focus on transfer-\nring autoencoding LMs trained with masked lan-\nguage model objective. We choose BERT and\nRoBERTa (Liu et al., 2019) as the source models\nfor building our bilingual language models, named\nRAMEN3 for the ease of discussion. We imple-\nment our models on top of HuggingFace’s Trans-\nformer (Wolf et al., 2019).4 For each pretrained\nmodel, we experiment with 12 layers (BERTBASE\nand RoBERTaBASE) and 24 layers (BERTLARGE and\nRoBERTaLARGE) variants. Using BERTBASE allows\nus to compare the results with mBERT model. Us-\ning BERTLARGE and RoBERTa allows us to inves-\ntigate whether the performance of the target LM\ncorrelates with the performance of the source pre-\ntrained model. RoBERTa is a recently published\nmodel that is similar to BERT architecturally but\nwith an improved training procedure. By training\nfor longer time, with bigger batches, on more data,\nand on longer sequences, RoBERTa matched or ex-\nceed previously published models including XLNet.\nWe include RoBERTa in our experiments to vali-\ndate the motivation of our work: with similar archi-\ntecture, does a stronger pretrained English model\nresult in a stronger bilingual LM? We evaluate our\nmodels on two cross-lingual zero-shot tasks: (1)\nCross-lingual Natural Language Inference (XNLI)\nand (2) dependency parsing.\n3The author likes ramen.\n4https://github.com/huggingface/\ntransformers\n3.1\nData\nWe evaluate our approach for six target languages:\nFrench (fr), Russian (ru), Arabic (ar), Chinese\n(zh), Hindi (hi), and Vietnamese (vi). These lan-\nguages belong to four different language families.\nFrench, Russian, and Hindi are Indo-European lan-\nguages, similar to English. Arabic, Chinese, and\nVietnamese belong to Afro-Asiatic, Sino-Tibetan,\nand Austro-Asiatic family respectively. The choice\nof the six languages also reﬂects different training\nconditions depending on the amount of monolin-\ngual data. French and Russian, and Arabic can\nbe regarded as high resource languages whereas\nHindi has far less data and can be considered as\nlow resource.\nFor experiments that use parallel data to initial-\nize foreign speciﬁc parameters, we use the same\ndatasets in the work of Lample and Conneau (2019).\nSpeciﬁcally, we use United Nations Parallel Cor-\npus (Ziemski et al., 2016) for en-ru, en-ar,\nen-zh, and en-fr. We collect en-hi paral-\nlel data from IIT Bombay corpus (Kunchukuttan\net al., 2018) and en-vi data from OpenSubtitles\n2018.5 For experiments that use only monolin-\ngual data to initialize foreign parameters, instead\nof training word-vectors from the scratch, we use\nthe pretrained word vectors6 from fastText (Bo-\njanowski et al., 2017) to estimate word transla-\ntion probabilities (Eq. 3). We align these vectors\ninto a common space using orthogonal Procrustes\n(Artetxe et al., 2016; Lample et al., 2018b; Joulin\net al., 2018). We only use identical words be-\ntween the two languages as the supervised signal.\nWe use WikiExtractor7 to extract extract raw sen-\ntences from Wikipedias as monolingual data for\nﬁne-tuning target embeddings and bilingual LMs\n(§2.2). We do not lowercase or remove accents in\nour data preprocessing pipeline.\nWe tokenize English using the provided tok-\nenizer from pretrained models.\nFor target lan-\nguages, we use fastBPE8 to learn 30,000 BPE codes\nand 50,000 codes when transferring from BERT\nand RoBERTa respectively. We truncate the BPE\nvocabulary of foreign languages to match the size\nof the English vocabulary in the source models.\nPrecisely, the size of foreign vocabulary is set to\n5http://opus.nlpl.eu\n6https://fasttext.cc/docs/en/\ncrawl-vectors.html\n7https://github.com/attardi/\nwikiextractor\n8https://github.com/glample/fastBPE\n32,000 when transferring from BERT and 50,000\nwhen transferring from RoBERTa.\nWe use XNLI dataset (Conneau et al., 2018)\nfor classiﬁcation task and Universal Dependen-\ncies v2.4 (Nivre et al., 2019) for dependency pars-\ning task.\nSince a language might have more\nthan one treebank in Universal Dependencies,\nwe use the following treebanks: en ewt (En-\nglish), fr gsd (French), ru syntagrus (Rus-\nsian), zh gsd (Chinese), vi vtb (Vietnamese),\nhi hdtb (Hindi), and ar padt (Arabic).\nRemark on BPE\n(Lample et al., 2018a) show\nthat sharing subwords between languages improves\nalignments between embedding spaces. Wu and\nDredze (2019) observe a strong correlation be-\ntween the percentage of overlapping subwords and\nmBERT’s performances for cross-lingual zero-shot\ntransfer. On the other hand, K et al., (2020) report\nin their control experiments that subword overlap-\nping has minimal contribution cross-lingual ability\nof BERT. Artetxe et al., (2019) conﬁrms in their\nstudy that a shared vocabulary is not necessary for\nmultilingual models. In our current approach, sub-\nwords between source and target are not shared. A\nsubword that is in both English and foreign vocab-\nulary has two different embeddings.\n3.2\nEstimating translation probabilities\nSince pretrained models operate on subword level,\nwe need to estimate subword translation probabili-\nties. Therefore, we subsample 2M sentence pairs\nfrom each parallel corpus and tokenize the data\ninto subwords before running fast-align (Dyer et al.,\n2013).\nEstimating subword translation probabilities\nfrom aligned word vectors requires an additional\nprocessing step since the provided vectors from\nfastText are not at subword level.9 We use the fol-\nlowing approximation to obtain subword vectors:\nthe vector es of subword s is the weighted average\nof all the aligned word vectors ewi that have s as\nan subword\nes =\nX\nwj: s∈wj\np(wj)\nns\newj\n(4)\nwhere p(wj) is the unigram probability of word\nwj estimated from monolingual data and ns =\nP\nwj: s∈wj p(wj).\n9In our preliminary experiments, we learned the aligned\nsubword vectors but it results in poor performances.\nfr\nvi\nzh\nru\nar\nhi\navg\n(Conneau et al., 2018)\n✓\n67.7\n66.4\n65.8\n65.4\n64.8\n64.1\n65.7\n(Artetxe and Schwenk, 2019)\n+\n71.9\n72.0\n71.4\n71.5\n71.4\n65.5\n70.6\n(Lample and Conneau, 2019) (MLM)\n✓\n76.5\n72.1\n71.9\n73.1\n68.5\n65.7\n71.3\n(Lample and Conneau, 2019) (MLM+TLM)\n+\n78.7\n76.1\n76.5\n75.3\n73.1\n69.6\n74.9\nmBERT (Wu and Dredze, 2019)\n✓\n73.8\n69.5\n69.3\n69.0\n64.9\n60.0\n67.8\nRAMENBASE\n+ BERT\n✓\n75.2\n71.8\n70.7\n71.1\n69.3\n62.8\n70.1\n+\n75.1\n72.5\n71.9\n70.8\n69.7\n63.5\n70.6\n+ RoBERTa\n✓\n79.9\n75.9\n73.7\n73.6\n71.9\n65.6\n73.4\n+\n80.3\n75.6\n76.2\n75.8\n73.1\n68.1\n74.9\nRAMENLARGE\n+ BERT\n✓\n78.1\n74.8\n74.5\n73.7\n70.8\n64.5\n72.7\n+\n78.0\n75.1\n71.3\n74.0\n71.8\n66.1\n72.7\n+ RoBERTa\n✓\n81.3\n76.2\n76.3\n75.6\n73.5\n64.5\n74.6\n+\n81.0\n76.2\n76.8\n75.0\n72.9\n68.2\n75.0\nTable 1: Zero-shot classiﬁcation results on XNLI. + indicates parallel data is used. RAMEN only uses parallel data\nfor initialization. The best results are marked in bold.\nWe take the top 50,000 words in each aligned\nword-vectors to compute subword vectors.\nIn both cases, not all the words in the foreign vo-\ncabulary can be initialized from the English word-\nembeddings. Those words are initialized randomly\nfrom a Gaussian N(0, 1/d2).\n3.3\nHyper-parameters\nIn all the experiments, we tune RAMENBASE for\n120,000 updates and RAMENLARGE for 300,000 up-\ndates. The sequence length is set to 256. For tuning\nbilingual LMs, we use a mini-batch size of 112 for\nRAMENBASE and 24 for RAMENLARGE where half\nof the batch are English sequences and the other\nhalf are foreign sequences. This strategy of balanc-\ning mini-batch has been used in multilingual neural\nmachine translation (Firat et al., 2016; Lee et al.,\n2017).\nWe optimize RAMEN using Adam optimizer. We\nlinearly increase the learning rate from 10−7 to\n10−4 in the ﬁrst 4000 updates and then follow an\ninverse square root decay. When ﬁne-tuning RA-\nMEN on XNLI and UD, we use a mini-batch size\nof 32, Adam’s learning rate of 10−5. The number\nof epochs are set to 4 and 50 for XNLI and UD\ntasks respectively.\nAll experiments are carried out on a single Tesla\nV100 16GB GPU. Each RAMENBASE model is\ntrained within a day and each RAMENLARGE is\ntrained within two days.10\n1022 and 46 GPU hours, to be precise. Learning alignment\nwith fast-align takes less than 2 hours and we do not account\nfor training time of fastText vectors.\n4\nResults\nIn this section, we present the results of out models\nfor two zero-shot cross lingual transfer tasks: XNLI\nand universal dependency parsing.\n4.1\nCross-lingual Natural Language\nInference\nTable 1 shows the XNLI test accuracy. For ref-\nerence, we also include the scores from the pre-\nvious work, notably the state-of-the-art system\nXLM (Lample and Conneau, 2019). Before dis-\ncussing the results, we spell out that the fairest\ncomparison in this experiment is the comparison\nbetween mBERT and RAMENBASE+BERT trained\nwith monolingual only. We ﬁrst discuss the trans-\nfer results from BERT. Initialized from fastText\nvectors, RAMENBASE slightly outperforms mBERT\nby 2.6 points on average and widen the gap of\n4.4 points on Arabic.\nRAMENBASE gains extra\n0.3 points on average when initialized from par-\nallel data.\nWith triple number of parameters,\nRAMENLARGE offers an additional boost in term\nof accuracy and initialization with parallel data\nconsistently improves the performance. It has been\nshown that BERTLARGE signiﬁcantly outperforms\nBERTBASE on eleven English NLP tasks (Devlin\net al., 2019), the strength of BERTLARGE also shows\nup when adapted to foreign languages.\nTransferring from RoBERTa leads to better\nzero-shot accuracies. With the same initializing\ncondition, RAMENBASE+RoBERTa outperforms\nRAMENBASE+BERT on average by 3.1 and 4.3\npoints when initializing from monolingual and par-\nallel data respectively. This result show that with\nsimilar number of parameters, our approach bene-\nﬁts from a better English pretrained model. When\ntransferring from RoBERTaLARGE, we obtain state-\nof-the-art results for ﬁve languages. It is worth\nmentioning that these models are still underﬁt and\nthey have potential to push the XNLI performance\nfurther.\nCurrently, RAMEN only uses parallel data to\ninitialize foreign embeddings. RAMEN can also\nexploit parallel data through translation objective\nproposed in XLM. We believe that by utilizing par-\nallel data during the ﬁne-tuning of RAMEN would\nbring additional beneﬁts for zero-shot tasks. We\nleave this exploration to future work. In summary,\nstarting from BERTBASE, our approach obtains com-\npetitive bilingual LMs with mBERT for zero-shot\nXNLI. Our approach shows the accuracy gains\nwhen adapting from a better pretrained model.\n4.2\nUniversal Dependency Parsing\nWe build on top of RAMEN a graph-based depen-\ndency parser (Dozat and Manning, 2016). For the\npurpose of evaluating the contextual representa-\ntions learned by our model, we do not use part-\nof-speech tags. Contextualized representations are\ndirectly fed into Deep-Biafﬁne layers to predict arc\nand label scores. Table 2 presents the Labeled At-\ntachment Scores (LAS) for zero-shot dependency\nparsing.\nfr\nvi\nzh\nru\nar\nhi\navg\nmBERT\n✓\n71.6\n35.7\n26.6\n65.2\n36.4\n30.4\n44.3\nRAMENBASE\n+ BERT\n✓\n78.0\n38.3\n30.1\n67.2\n40.6\n38.6\n48.8\n+\n77.0\n36.7\n30.8\n66.8\n40.9\n40.9\n48.9\n+ RoBERTa\n✓\n79.1\n38.9\n31.5\n67.7\n42.2\n41.2\n50.1\n+\n78.5\n39.4\n31.3\n65.1\n41.4\n44.2\n50.0\nRAMENLARGE\n+ BERT\n✓\n78.2\n39.5\n30.5\n65.6\n45.3\n44.0\n50.5\n+\n78.9\n38.4\n30.5\n66.3\n43.4\n44.1\n50.3\n+ RoBERTa\n✓\n79.7\n40.0\n32.2\n65.7\n44.1\n44.6\n51.1\n+\n79.1\n39.2\n30.5\n65.4\n43.8\n46.8\n50.8\nTable 2: LAS scores for zero-shot dependency parsing.\n+ indicates parallel data is used for initialization. Punc-\ntuation are removed during the evaluation. The best re-\nsults are marked in bold.\nWe ﬁrst look at the fairest comparison be-\ntween mBERT and monolingually initialized\nRAMENBASE+BERT. The latter outperforms the for-\nmer on all the languages with the average of 4.5\nLAS point. We observe the largest gain of +8.2\nfor Hindi and +6.2 LAS for French. Arabic enjoys\n+4.2 LAS from our approach. With similar archi-\ntecture (12 or 24 layers) and initialization (using\nmonolingual or parallel data), RAMEN+RoBERTa\nperforms better than RAMEN+BERT for most of\nthe languages. Arabic and Hindi beneﬁt the most\nfrom bigger models. For the other four languages,\nRAMENLARGE renders a modest improvement over\nRAMENBASE.\n5\nAnalysis\nThroughout this section, we carry out the main anal-\nysis for the RAMENBASE+BERT model that only\nuse monolingual data.\n5.1\nHow does linguistic knowledge transfer\nhappen through each training stages?\nWe\nevaluate\nthe\nperformance\nof\nRAMEN+BERTBASE\n(initialized\nfrom\nmono-\nlingual data) at each 20K training steps.\nThe\nresults are presented in Figure 2.\n20\n40\n60\n80\n100\n120\nNumber of updates (x 1000)\nfr\nvi\nzh\nru\nar\nhi\nLanguage\n74.5\n75.5\n74.7\n73.4\n74.7\n75.2\n70.4\n71.1\n72.4\n71.3\n71.7\n71.8\n69.5\n70.3\n71.3\n70.7\n69.6\n70.5\n68.2\n70.3\n68.5\n69.6\n68.0\n71.1\n67.2\n67.2\n67.5\n68.2\n69.3\n68.5\n60.6\n61.8\n61.7\n62.2\n62.2\n62.8\nZero-shot XNLI\n20\n40\n60\n80\n100\n120\nNumber of updates (x 1000)\nfr\nvi\nzh\nru\nar\nhi\nLanguage\n77.5\n78.0\n77.9\n78.0\n78.2\n78.0\n36.6\n37.2\n37.1\n37.5\n37.6\n38.3\n29.1\n30.3\n30.4\n31.1\n30.4\n30.1\n62.0\n66.0\n66.1\n66.1\n67.0\n67.2\n38.2\n38.5\n39.6\n39.2\n39.0\n40.6\n36.2\n36.2\n38.3\n38.8\n38.8\n38.6\nZero-shot Parsing\nFigure 2: Accuracy and LAS evaluated at each check-\npoints.\nWe observe the general trend that futher ﬁne-\ntuning RAMEN improves the performances of\ntwo zero-shot tasks.\nJust after 40,000 ﬁne-\ntuning updates—corresponding to seven training\nhours—RAMENBASE+BERT surpasses mBERT on\nboth XNLI (Table 1) and universal dependency\nparsing (Table 2). Interstingly, we ﬁnd that the\nEnglish encoder can quickly adapt to the foreign\nsyntax that has different word order (ar and hi)\nwith a little training data.\nLanguage similarities seem to have more impact\non transferring syntax than semantics. French en-\njoys 78.0 LAS for being closely related to English,\nwhereas Arabic and Hindi, SOV languages, mod-\nestly reach 38.2 and 36.2 points using the SVO\nencoder. Although Chinese has SVO order, it is\noften seen as head-ﬁnal while English is strong\nhead-initial. Perhaps, this explains the poor perfor-\nmance for Chinese.\n5.2\nImpact of initialization\nInitializing foreign embeddings is the backbone\nof our approach. A good initialization leads to\nbetter zero-shot transfer results and enables fast\nadaptation. To verify the importance of a good\ninitialization, we train a RAMENBASE+BERT with\nforeign word-embeddings that are initialized ran-\ndomly from N(0, 1/d2). For a fair comparison, we\nuse the same hyper-parameters in §3.3. Table 3\nshows the results of XNLI and UD parsing of ran-\ndom initialization. In comparison to the initial-\nization using aligned fastText vectors, random ini-\ntialization decreases the zero-shot performance of\nRAMENBASE by 10.3% for XNLI and 11.6 points\nfor UD parsing on average. We also see that zero-\nshot parsing of SOV languages (Arabic and Hindi)\nsuffers random initialization.\nfr\nvi\nzh\nru\nar\nhi\navg\nXNLI\nrnd\n66.9\n67.1\n65.7\n59.1\n50.8\n48.5\n59.7\nscr\n71.1\n58.0\n62.8\n63.8\n62.1\n47.5\n60.8\nvec\n75.2\n71.8\n70.5\n71.1\n68.5\n62.8\n70.0\nUD\nrnd\n71.0\n33.2\n25.5\n60.3\n19.7\n13.4\n37.2\nscr\n63.8\n17.0\n13.6\n61.0\n15.5\n11.2\n30.3\nvec\n78.0\n38.3\n30.1\n67.2\n40.6\n38.6\n48.8\nTable 3: Comparison between random initialization\n(rnd) of language speciﬁc parameters and initializa-\ntion using aligned fastText vectors (vec) and bilingual\nBERT trained from scratch (scr) for 400 hours.\nTo highlight the efﬁciency of our transferring\napproach, we compare RAMENBASE+BERT with\na bilingual BERT (bBERT) trained from scratch\n(scr). For a fair comparision, we use the hyper-\nparameters described in section §3.3. All bBERT\nmodels are trained for 2,300,000 updates (400 GPU\nhours), which is more than sixteen times longer\nthan RAMEN. We emphasize two important ob-\nservations from Table 3. First, the randomly ini-\ntialized foreign embeddings RAMEN performs on\npar with bBERT on XNLI task and signiﬁcantly\nbetter than bBERT on UD parsing. This suggests\nthat a large amount of linguistic knowledge can\nbe transfered through reusing the pretrained BERT\nencoder. Secondly, with a careful initialization of\nforeign embeddings, our RAMENBASE+BERT out-\nshines bBERT with even just 20,000 updates (3.5\nGPU hours) as shown in Figure 2.\n5.3\nAre contextual representations from\nRAMEN also good for supervised parsing?\nAll the RAMEN models are built from English and\ntuned on English for zero-shot cross-lingual tasks.\nIt is reasonable to expect RAMENs do well in those\ntasks as we have shown in our experiments. But are\nthey also a good feature extractor for supervised\ntasks? We offer a partial answer to this question by\nevaluating our model for supervised dependency\nparsing on UD datasets.\nfr\nvi\nzh\nru\nar\nhi\navg\nmBERT\n92.1\n62.2\n85.1\n93.1\n83.6\n91.3\n84.6\nRAMENBASE\n+ BERT\n92.2\n63.3\n85.0\n93.3\n83.9\n92.2\n85.0\n+ RoBERTa\n92.8\n65.3\n86.0\n93.7\n84.9\n92.5\n85.9\nRAMENLARGE\n+ BERT\n92.8\n64.7\n86.2\n93.9\n84.9\n92.0\n85.7\n+ RoBERTa\n93.0\n66.4\n87.3\n94.0\n85.3\n92.8\n86.5\nTable 4: Evaluation in supervised UD parsing. The\nscores are LAS.\nWe used train/dev/test splits provided in UD to\ntrain and evaluate our RAMEN-based parser. Ta-\nble 4 summarizes the results (LAS) of our super-\nvised parser. For a fair comparison, we choose\nmBERT as the baseline and all the RAMEN mod-\nels are initialized from aligned fastText vectors.\nWith the same architecture of 12 Transformer lay-\ners, RAMENBASE+BERT performs competitive to\nmBERT and outshines mBERT by +1.1 points for\nVietnamese. The best LAS results are obtained\nby RAMENLARGE+RoBERTa with 24 Transformer\nlayers. Overall, our results indicate the potential of\nusing contextual representations from RAMEN for\nsupervised tasks.\n6\nConclusions\nIn this work, we have presented a simple and ef-\nfective approach for rapidly building a bilingual\nLM under a limited computational budget. Us-\ning BERT as the starting point, we demonstrate\nthat our approach performs better than mBERT on\ntwo cross-lingual zero-shot sentence classiﬁcation\nand dependency parsing. We ﬁnd that the perfor-\nmance of our bilingual LM, RAMEN, correlates\nwith the performance of the original pretrained En-\nglish models. We also ﬁnd that RAMEN is also\na powerful feature extractor in supervised depen-\ndency parsing. Finally, we hope that our work\nsparks of interest in developing fast and effective\nmethods for transferring pretrained English models\nto other languages.\nReferences\nJoakim Nivre et al. 2019. Universal dependencies 2.4.\nLINDAT/CLARIN digital library at the Institute of\nFormal and Applied Linguistics ( ´UFAL), Faculty of\nMathematics and Physics, Charles University.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.\nLearning principled bilingual mappings of word em-\nbeddings while preserving monolingual invariance.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2289–2294, Austin, Texas. Association for Compu-\ntational Linguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2019.\nOn the Cross-lingual Transferability of\nMonolingual Representations. arXiv e-prints, page\narXiv:1910.11856.\nMikel Artetxe and Holger Schwenk. 2019.\nMas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond.\nTransac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, and Robert L. Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2):263–\n311.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018.\nXNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc V. Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTimothy Dozat and Christopher D. Manning. 2016.\nDeep biafﬁne attention for neural dependency pars-\ning. In ICLR.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013.\nA simple, fast, and effective reparameter-\nization of IBM model 2.\nIn Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 644–648, At-\nlanta, Georgia. Association for Computational Lin-\nguistics.\nOrhan Firat, Kyunghyun Cho, and Yoshua Bengio.\n2016. Multi-way, multilingual neural machine trans-\nlation with a shared attention mechanism. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n866–875, San Diego, California. Association for\nComputational Linguistics.\nArmand Joulin, Piotr Bojanowski, Tomas Mikolov,\nHerv´e J´egou, and Edouard Grave. 2018.\nLoss in\ntranslation: Learning bilingual word mapping with\na retrieval criterion.\nIn Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2979–2984, Brussels, Bel-\ngium. Association for Computational Linguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual {bert}: An empirical study. In International\nConference on Learning Representations.\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhat-\ntacharyya. 2018.\nThe IIT Bombay English-Hindi\nparallel corpus. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation (LREC-2018), Miyazaki, Japan. Euro-\npean Languages Resources Association (ELRA).\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. NeurIPS.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018a.\nUnsupervised\nmachine translation using monolingual corpora only.\nIn International Conference on Learning Represen-\ntations.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv´e J´egou. 2018b.\nWord translation without parallel data. In Interna-\ntional Conference on Learning Representations.\nJason Lee, Kyunghyun Cho, and Thomas Hofmann.\n2017.\nFully character-level neural machine trans-\nlation without explicit segmentation. Transactions\nof the Association for Computational Linguistics,\n5:365–378.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nAndr´e F. T. Martins and Ram´on F. Astudillo. 2016.\nFrom softmax to sparsemax: A sparse model of at-\ntention and multi-label classiﬁcation.\nIn Proceed-\nings of the 33rd International Conference on Inter-\nnational Conference on Machine Learning - Volume\n48, ICML’16, pages 1614–1623. JMLR.org.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations.\nIn Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V. Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019.\nHuggingface’s trans-\nformers: State-of-the-art natural language process-\ning.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. NeurIPS.\nMichał Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016.\nThe united nations parallel cor-\npus v1.0. In Proceedings of the Tenth International\nConference on Language Resources and Evaluation\n(LREC 2016), Paris, France. European Language Re-\nsources Association (ELRA).\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-02-18",
  "updated": "2020-04-29"
}