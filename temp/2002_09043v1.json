{
  "id": "http://arxiv.org/abs/2002.09043v1",
  "title": "oIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions",
  "authors": [
    "David Venuto",
    "Jhelum Chakravorty",
    "Leonard Boussioux",
    "Junhao Wang",
    "Gavin McCracken",
    "Doina Precup"
  ],
  "abstract": "Explicit engineering of reward functions for given environments has been a\nmajor hindrance to reinforcement learning methods. While Inverse Reinforcement\nLearning (IRL) is a solution to recover reward functions from demonstrations\nonly, these learned rewards are generally heavily \\textit{entangled} with the\ndynamics of the environment and therefore not portable or \\emph{robust} to\nchanging environments. Modern adversarial methods have yielded some success in\nreducing reward entanglement in the IRL setting. In this work, we leverage one\nsuch method, Adversarial Inverse Reinforcement Learning (AIRL), to propose an\nalgorithm that learns hierarchical disentangled rewards with a policy over\noptions. We show that this method has the ability to learn \\emph{generalizable}\npolicies and reward functions in complex transfer learning tasks, while\nyielding results in continuous control benchmarks that are comparable to those\nof the state-of-the-art methods.",
  "text": "oIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally\nExtended Actions\nDavid Venuto 1 2 Jhelum Chakravorty 1 2 Leonard Boussioux 3 2 Junhao Wang 1 2 Gavin McCracken 1 2\nDoina Precup 1 2 4\nAbstract\nExplicit engineering of reward functions for given\nenvironments has been a major hindrance to rein-\nforcement learning methods. While Inverse Rein-\nforcement Learning (IRL) is a solution to recover\nreward functions from demonstrations only, these\nlearned rewards are generally heavily entangled\nwith the dynamics of the environment and there-\nfore not portable or robust to changing environ-\nments. Modern adversarial methods have yielded\nsome success in reducing reward entanglement\nin the IRL setting. In this work, we leverage one\nsuch method, Adversarial Inverse Reinforcement\nLearning (AIRL), to propose an algorithm that\nlearns hierarchical disentangled rewards with a\npolicy over options. We show that this method has\nthe ability to learn generalizable policies and re-\nward functions in complex transfer learning tasks,\nwhile yielding results in continuous control bench-\nmarks that are comparable to those of the state-of-\nthe-art methods.\n1. Introduction\nReinforcement learning (RL) has been able to learn policies\nin complex environments but it usually requires designing\nsuitable reward functions for successful learning. This can\nbe difﬁcult and may lead to learning sub-optimal policies\nwith unsafe behavior (Amodei et al., 2016) in the case of\npoor engineering. Inverse Reinforcement Learning (IRL)\n(Ng & Russell, 2000; Abbeel & Ng, 2004) can facilitate\nsuch reward engineering through learning an expert’s reward\nfunction from expert demonstrations.\nIRL, however, comes with many difﬁculties and the problem\n*Equal contribution\n1Department of Computer Science,\nMcGill University, Montreal, Canada 2Mila, Montreal, Canada\n3Department of Operations Research, MIT, Cambridge, USA\n4DeepMind, Montreal, Canada.\nCorrespondence to:\nDavid\nVenuto <david.venuto@mail.mcgill.com>, Doina Precup <dpre-\ncup@cs.mcgill.ca>.\nPreprint Under Review\nis not well-deﬁned because, for a given set of demonstra-\ntions, the number of optimal policies and corresponding\nrewards can be very large, especially for high dimensional\ncomplex tasks. Also, many IRL algorithms learn reward\nfunctions that are heavily shaped by environmental dynam-\nics. Policies learned on such reward functions may not\nremain optimal with slight changes in the environment. Ad-\nversarial Inverse Reinforcement Learning (AIRL) (Fu et al.,\n2018) generates more generalizable policies with disentan-\ngled reward functions that are invariant to the environmental\ndynamics. The reward and value function are learned si-\nmultaneously to compute the reward function in a state-only\nmanner. This is an instance of a transfer learning problem\nwith changing dynamics, where the agent learns an optimal\npolicy in one environment and then transfers it to an envi-\nronment with different environmental dynamics. A practical\nexample of this transfer learning problem would be teaching\na robot to walk with some mechanical structure, and then\ngeneralize this knowledge to perform the task with different\nsized structural components.\nOther methods have been developed to learn demonstra-\ntions in environments with differing dynamics and then\nexploit this knowledge while performing tasks in a novel\nenvironment. As complex tasks in different environments\noften come from several reward functions, methods such\nas Maximum Entropy IRL and GAN-Guided Cost Learn-\ning (GAN-GCL) tend to overgeneralize (Finn et al., 2016b).\nOne way to help solve the problem of over-ﬁtting is to\nbreak down a policy into small option (temporally extended\naction)-policies that solve various aspects of an overall task.\nThis method has been shown to create a policy that is more\ngeneralizable (Sutton et al., 1999; Taylor & Stone, 2009).\nMethods such as Option-Critic have implemented modern\nRL architectures with a policy over options and have shown\nimprovements for generalization of policies (Bacon et al.,\n2017). OptionGAN (Henderson et al., 2018) also proposed\nan IRL framework for a policy over options and is shown to\nhave some improvement in one-shot transfer learning tasks,\nbut does not return disentangled rewards.\nIn this paper, we introduce Option-Inverse Reinforcement\nLearning (oIRL), to investigate transfer learning with op-\ntions. Following AIRL, we propose an algorithm that com-\narXiv:2002.09043v1  [cs.LG]  20 Feb 2020\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nputes disentangled rewards to learn joint reward-policy op-\ntions, with each option-policy having rewards that are dis-\nentangled from environmental dynamics. These policies\nare shown to be heavily portable in transfer learning tasks\nwith differences in environments. We evaluate this method\nin a variety of continuous control tasks in the Open AI\nGym environment using the MuJoCo simulator (Brockman\net al., 2016; Todorov et al.) and GridWorlds with Mini-\nGrid (Chevalier-Boisvert et al., 2018). Our method shows\nimprovements in terms of reward on a variety of transfer\nlearning tasks while still performing better than benchmarks\nfor standard continuous control tasks.\n2. Preliminaries\nMarkov Decision Processes (MDP) are deﬁned by a tuple\n⟨S, A, P, R, γ⟩where S is a set of states, A is the set of\nactions available to the agent, P is the transition kernel giv-\ning a probability over next states given the current state and\naction, R : S × A →[0, Rmax] is a reward function and\nγ ∈[0, 1] is a discount factor. st and at are respectively\nthe state and action of the expert at time instant t. We de-\nﬁne a policy π as the probability distribution over actions\nconditioned on the current state; π : S × A →[0, 1]. A pol-\nicy is modeled by a Gaussian distribution πθ ∼N(µ, σ2)\nwhere θ is the policy parameters. The value of a policy is\ndeﬁned as Vπ(s) = Eπ[P∞\nt=0 γtrt+1|s], where E denotes\nthe expectation. An agent follows a policy π and receives\nreward from the environment. A state-action value func-\ntion is Qπ(s, a) = Eπ[P∞\nt=0 γtrt+1|s, a]. The advantage is\nAπ(s, a) = Qπ(s, a)−Vπ(s). r(s, a) represents a one-step\nreward.\nOptions (ω ∈Ω) are deﬁned as a triplet (Iω, πω, βω), where\nπω is a policy over options, Iω ∈S is the initiation set of\nstates and βω : S →[0, 1] is the termination function. The\npolicy over options is deﬁned by πΩ. An option has a reward\nrω and an option policy πω.\nThe policy over options is parameterized by ζ, the intra-\noption policies by α for each option, the reward approx-\nimator by θ, and the option termination probabilities by\nδ.\nIn the one-step case, selecting an option using the policy-\nover-options can be viewed as a mixture of completely\nspecialized experts. This overall policy can be deﬁned as\nπΘ(a|s) = P\nω∈ΩπΩ(ω|s)πω(a|s).\nDisentangled Rewards are formally deﬁned as a reward\nfunction r∗\nθ(s, a, s′) that is disentangled with respect to\n(w.r.t.) a ground-truth reward and a set of environmental\ndynamics T such that under all possible dynamics T ∈T ,\nthe optimal policy computed w.r.t. the reward function is\nthe same.\n3. Related Work\nGenerative Adversarial Networks (GANs) learn the gen-\nerator distribution pg and discriminator DθD(x). They use\na prior distribution over input noise variables p(z). Given\nthese input noise variables the mapping Gθg(z) is learned,\nwhich maps these input noise variables to the data set space.\nG is a neural network. Another neural network, DθD(x),\nlearns to estimate the probability that x came from the data\nset and not the generator pg.\nIn our two-player adversarial training procedure, D is\ntrained to maximize the probability of assigning the cor-\nrect labels to the dataset and the generated samples. G is\ntrained to minimize the objective log(1 −DθD(GθG(z))),\nwhich causes it to generate samples that are more likely to\nfool the discriminator.\nPolicy Gradient methods optimize a parameterized policy\nπθ using a gradient ascent. Given a discounting term, the ob-\njective to be optimized is p(θ, s0) = E[P∞\nt=0 γtrθ(st)|s0].\nProximal policy optimization (PPO)\n(Schulman et al.,\n2017) is a policy gradient method that uses policy gradi-\nent theorem, which states ∂p(θ,s0)\n∂θ\n= P\ns\nP∞\nt=0 γtP(st =\ns|s0) P\na\n∂π(a|s)\n∂θ\nQπθ(s, a). PPO has been adapted for the\noption-critic architecture (PPOC) (Klissarov et al., 2017).\nInverse Reinforcement Learning (IRL) is a form of imi-\ntation learning 1, where the expert’s reward is estimated\nfrom demonstrations and then forward RL is applied to that\nestimated reward to ﬁnd the optimal policy. Generative\nAdversarial Imitation Learning (GAIL) directly extracts op-\ntimal policies from expert’s demonstrations (Ho & Ermon,\n2016). IRL infers a reward function from expert demonstra-\ntions, which is then used to optimize a generator policy.\nIn IRL, an agent observes a set of state-action trajec-\ntories from an expert demonstrator D.\nWe let TD =\n{τ E\n1 , τ E\n2 , . . . , τ E\nn } be the state-action trajectories of the ex-\npert, τ E\ni ∼τD where τ E\ni = {s0, a0, s1, a1 . . . , sk, ak}. We\nwish to ﬁnd the reward function r(s, a) given the set of\ndemonstrations TD. It is assumed that the demonstrations\nare drawn from the optimal policy π∗(a|s). The Maximum\nLikelihood Estimation (MLE) objective in the IRL problems\nis therefore:\nmax\nθ\nJ(θ) = max\nθ\nEτ∼τ E[log(pθ(τ))],\n(1)\nwith pθ(τ) ∝p(s0) QT\nt=0 p(st+1|st, at) exp (γtrθ(st, at)).\nAdversarial Inverse Reinforcement Learning (AIRL) is\nbased on GAN-Guided Cost Learning (Finn et al., 2016a),\nwhich casts the MLE objective as a Generative Adversarial\nNetwork (GAN) (Goodfellow et al., 2014) optimization\n1Here the agent learns the expert’s policy by observing expert\ndemonstrations (Ng & Russell, 2000).\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nproblem over trajectories. In AIRL (Fu et al., 2018), the\ndiscriminator probability Dθ is evaluated using the state-\naction pairs from the generator (agent), as given by\nDθ(s, a) =\nexp(fθ(s, a))\nexp(fθ(s, a)) + π(a|s).\n(2)\nThe agent tries to maximize R(s, a) = log(1 −Dθ(s, a)) −\nlog(Dθ(s, a)) where fθ(s, a) is a learned function and π is\npre-computed. This formula is similar to GAIL but with\na recoverable reward function since GAIL outputs 0.5 for\nthe reward of all states and actions at optimality. The dis-\ncriminator function is then formulated as fθ,Φ(s, a, s′) =\ngθ(s, a) + γhΦ(s′) −hΦ(s) given shaping function hΦ and\nreward approximator gθ. Under deterministic dynamics, it\nis shown in AIRL that there is a state-only reward approxi-\nmator ( f ∗(s, a, s′) = r∗(s)+γV ∗(s′)−V ∗(s) = A∗(s, a)\nwhere the reward is invariant to transition dynamics and is\ndisentangled.\nHierarchical Inverse Reinforcement Learning learns\npolicies with high level temporally extended actions us-\ning IRL. OptionGAN (Henderson et al., 2018) provides\nan adversarial IRL objective function for the discriminator\nwith a policy over options. It is formulated such that Lreg\ndeﬁnes the regularization terms on the mixture of experts so\nthat they converge to options. The discriminator objective\nin OptionGAN takes state-only input and is formulated as:\nLΩ= Eω[πΩ,ζ(ω|s)(Lα,ω)] + Lreg,\nwhere\nLα,ω = Eτ N [log(rθ,ω(s))] + Eτ E[log(1 −rθ,ω(s))]. (3)\nIn Directed-Info GAIL (Sharma et al., 2019) implements\nGAIL in a policy over options framework.\nWork such as (Krishnan et al., 2016) solves this hierarchi-\ncal problem of segmenting expert demonstration transitions\nby analyzing the changes in local linearity w.r.t a kernel\nfunction. It has been suggested that decomposing the re-\nward function is not enough (Henderson et al., 2018). Other\nworks have learned the latent dimension along with the pol-\nicy for this task (Hausman et al., 2017; Wang et al., 2017). In\nthis formulation, the latent structure is encoded in an unsu-\npervised manner so that the desired latent variable does not\nneed to be provided. This work parallels many hierarchical\nIRL methods but with recoverable robust rewards.\n4. MLE Objective for IRL Over Options\nLet (s0, a0, . . . sT , aT ) ∈τ E\ni\nbe an expert trajectory of\nstate-action pairs. Denote by (s0, a0, ω0 . . . sT , aT , ωT ) ∈\nτπΘ,t a novice trajectory generated by policy over options\nπΘ,t of the generator at iteration t.\nGiven a trajectory of state-action pairs, we ﬁrst deﬁne an\noption transition probability given a state and an option.\nSimilar transition probabilities given state, action or option\ninformation are deﬁned in (Appendix A).\nP(st+1, ωt+1 | st, ωt)\n=\nX\na∈A\nπω,α(a|st)P(st+1|st, a)((1 −βωt,δ(st+1))1ωt=ωt+1\n+ βωt,δ(st+1)πΩ,ζ(ωt+1|st+1)).\n(4)\nWe can similarly deﬁne a discounted return recursively. Con-\nsider the policy over options based on the probabilities of\nterminating or continuing the option policies given a reward\napproximator ˆrθ(s, a) for the state-action reward.\nRθ,δ(s, ω, a) := E\nh\nˆrθ,ω(s, a) + γ\nX\ns′∈S\nP(s\n′|s, a)\n\u0010\nβω,δ(s\n′)RΩ\nζ,θ,α,δ(s\n′) +\n\u00001 −βω,δ(s\n′)\n\u0001\nRθ,α,δ(s\n′, ω)\n\u0011i\n.\n(5)\nω0 is selected according to πζ,Ω(ω|s). The expressions for\nall relevant discounted returns appearing in the analysis are\ngiven in Appendix B. A suitable parameterization of the\ndiscounted return R can be found by maximizing the causal\nentropy Eτ∼D[log(pθ(τ))] w.r.t parameter θ. We then have\nfor a trajectory τ with T time-steps:\npθ(τ)\n(6)\n≈p(s0, ω0)\nT −1\nY\nt=0\nP(st+1, ωt+1|st, ωt, at)eRθ,δ(st,ωt,at).\n4.1. MLE Derivative\nSimilar to (Fu et al., 2018) and (Finn et al., 2016a), we\ndeﬁne the MLE objective for the generator pθ as\nJ(θ) = Eτ∼τ E[\nT\nX\nt=0\nRΩ\nζ,θ,δ(st, at)]\n−Epθ[\nT\nX\nt=0\nX\nω∈Ω\nπζ,Ω(ω|st)Rθ,δ(st, ω, at)].\n(7)\nNote that we may or may not know the option trajectories\nin our expert demonstrations, instead they are estimated\naccording to the policy over options. The gradient of (7)\nw.r.t θ (See Appendix B for detailed derivations) is given\nby:\n∂\n∂θJ(θ) = Eτ∼τ E\nh ∂\n∂θ log(pθ(τ))\ni\n≈Eτ∼τ E\nh\nT\nX\nt=0\n∂\n∂θRΩ\nζ,θ,δ(st, at)\ni\n−Epθ\nh\nT\nX\nt=0\n∂\n∂θRΩ\nζ,θ,δ(st, at)\ni\n.\nWe deﬁne pθ,t(st, at) =\nR\nst′ ̸=t,at′ ̸=t pθ(τ)dst′dat′ as the\nstate action marginal at time t. This allows us to examine\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nthe trajectory from step t as deﬁned similarly in (Fu et al.,\n2018). Consequently, we have\n∂\n∂θJ(θ) =\nT\nX\nt=0\nEτ∼τ E\n\u0014 ∂\n∂θRΩ\nζ,θ,δ(st, at)\n\u0015\n(xyz)\n−Epθ,t\n\u0014 ∂\n∂θRΩ\nζ,θ,δ(st, at)\n\u0015\n.\n(8)\nSince pθ is difﬁcult to draw samples from, we estimate it\nusing importance sampling distribution over the generator\ndensity. Then, we compute an importance sampling estimate\nof a mixture policy µt,w(τ) for each option w as follows.\nWe sample a mixture policy µω(a|s) deﬁned as 1\n2πω(a|s) +\n1\n2 ˆpω(a|s) and ˆpω(a|s) is a density estimate trained on the\ndemonstrations. We wish to minimize DKL(πw(τ)|pω(τ))\nto reduce the importance sampling distribution variance,\nwhere DKL is the Kullback–Leibler divergence metric (Kull-\nback & Leibler, 1951) between two probability distributions.\nApplying the aforementioned density estimates in (8), we\ncan express the gradient of the MLE objective J follows:\n∂\n∂θJ(θ) =\nT\nX\nt=0\nEτ∼τ E[ ∂\n∂θRΩ\nζ,θ,δ(st, at)]−\nEµt\n\u0014X\nω∈Ω\nπζ,Ω(ω|st)pθ,t,ω(st, at)\nµt,w(st, at)\n∂\n∂θRθ,δ(st, ω, at)\n\u0015\n,\n(9)\nwhere\n∂\n∂θRΩ\nζ,θ,α,δ(s) = E\n\" X\nω∈Ω\nπΩ,ζ(ω|s)\nh X\na∈A\nπω,α(a|s)\n\u0010 ∂\n∂θ ˆrθ(s, a) + γ\nX\ns′∈S\nP(s\n′|s, a)\n\u0010\nβw,δ(s\n′) ∂\n∂θRΩ\nζ,θ,α,δ(s\n′)\n+ (1 −βω,δ(s\n′)) ∂\n∂θRθ,α,δ(s\n′, ω)\n\u0011\u0011i#\n.\n(10)\n5. Discriminator Objective\nIn this section we formulate the discriminator, parameter-\nized by θ, as the odds ratio between the policy and the\nexponentiated reward distribution for option ω. We have a\ndiscriminator Dθ,ω for each option ω and a sample generator\noption policy πw, deﬁned as follows:\nDθ,ω(s, a) =\nexp(fθ,ω(s, a))\nexp(fθ,ω(s, a)) + πw(a|s).\n(11)\n5.1. Recursive Loss Formulation\nThe discriminator Dθ,ω is trained by minimizing the cross-\nentropy loss between expert demonstrations and generated\nexamples assuming we have the same number of options\nin the generated and expert trajectories. We deﬁne the loss\nfunction Lθ as follows:\nlθ(s, a, ω)\n(12)\n= −ED[log(Dθ,ω(s, a))] −EπΘ,t[log(1 −Dθ,ω(s, a))].\nThe parameterized total loss for the entire trajectory,\nLθ,α,δ(s, a, ω), can be expressed recursively as follows by\ntaking expectations over the next options and states:\nLθ,δ(s, a, ω)\n= lθ(s, a, ω) + γ\nX\ns′∈S\nP(s\n′|s, a)\n\u0010\nβw,δ(s\n′)LΩ\nζ,θ,α,δ(s\n′)\n+\n\u00001 −βw,δ(s\n′)\n\u0001\nLθ,α,δ(s\n′, w)\n\u0011\n(13)\nLθ,α,δ(s, w) := Ea∈A[Lθ,δ(s, w, a)]\n(14)\nLΩ\nζ,θ,δ(s, a) := Ew∈Ω[Lθ,δ(s, w, a)]\n(15)\nLΩ\nζ,θ,α,δ(s) := Eω∈Ω[Lθ,α,δ(s, ω)].\n(16)\nThe agent wishes to minimize Lθ,α,δ to ﬁnd its optimal\npolicy.\n5.2. Optimization Criteria\nFor a given option ω,\ndeﬁne the reward function\nˆRθ,δ(s, ω, a), which is to be maximised. We then write\na negative discriminator loss (−LD) to turn our loss mini-\nmization problem into a maximization problem, as follows:\n−LD = ˆRθ,δ(s, ω, a) =\nlog(Dθ,ω(s, a)) −log(1 −Dθ,ω(s, a)).\n(17)\nWe set a mixture of experts and novice as ¯µ observations\nin our gradient. We then wish to take the derivative of the\ninverse discriminator loss as,\n∂\n∂θ(−LD) =\nT\nX\nt=0\nEτ∼τ E\n\"X\nω∈Ω\nπΩ,ζ(ω|st) ∂\n∂θ\n\u0010\n−Lθ,δ(st, ω, at)\n\u0011#\n−\nE¯µt\nh X\nω∈Ω\nπΩ,ζ(ω|st)\n \nexp(−Lθ,δ(st, ω, at))\n1\n2 exp(−Lθ,δ(st, ω, at)) + 1\n2πω(at|st)\n!\n∂\n∂θ\n\u0010\n−Lθ,δ(st, ω, at)\n\u0011i\n.\n(18)\nWe can multiply the top and bottom of the fraction in\nthe mixture expectation by the state marginal πω(st) =\nR\na∈A πω(st, at). This allows us to write ˆpθ,t,ω(st, at) =\nexp(Lθ,δ(st, ω, at))πω,t(st). Using this, we can derive an\nimportance sampling distribution in our loss,\n∂\n∂θ(−LD) =\nT\nX\nt=0\nEτ∼τ E\n\"X\nω∈Ω\nπΩ,ζ(ω|st) ∂\n∂θ\n\u0010\n−Lθ,δ(st, ω, at)\n\u0011#\n−E¯µt\n\"X\nω∈Ω\nπΩ,ζ(ω|st)\n\u0012 ˆpθ,t,ω(st, at)\nˆµt,ω(st, at)\n\u0013 ∂\n∂θ\n\u0010\n−Lθ,δ(st, ω, at)\n\u0011#\n.\n(19)\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nThe gradient of this parametrized reward function corre-\nsponds to the inverse of the discriminator’s objective:\n∂\n∂θ\nˆRθ,δ(s, ω, a) ≈∂\n∂θ\n\u0010\n−Lθ,δ(s, ω, a)\n\u0011\n= E\nh ∂\n∂θrθ,ω(s, a) + γ\nX\ns′∈S\nP(s\n′|s, a)\n\u0010\nβω,δ(s\n′)\n∂\n∂θ\n\u0000−LΩ\nζ,θ,α,δ(s\n′)\n\u0001\n+\n\u00001 −βω,δ(s\n′)\n\u0001 ∂\n∂θ\n\u0000−Lθ,δ(s\n′, ω)\n\u0001\u0011i\n.\n(20)\nSee Appendix C for the detailed derivations of the terms\nappearing in (20). Substituting (20) into (19) one can see\nthat (9) (derivative of MLE objective) and (10) are of the\nsame form as of (19) (derivative of the discriminator objec-\ntive and (20)).\n6. Learning Disentangled State-only Rewards\nwith Options\nIn this section, we provide our main algorithm for learning\nrobust rewards with options. Similar to AIRL, we imple-\nment our algorithm with a discriminator update that consid-\ners the rollouts of a policy over options. We perform this\nupdate with (s, a, s′) triplets and a discriminator function in\nthe form of fθ,ω(s, a, s′) as given in (21). This allows us to\nformulate the discriminator with state-only rewards in terms\nof option-value function estimates. We can then compute an\noption-advantage estimate. Since the reward function only\nrequires state, we learn a reward function and corresponding\npolicy that is disentangled from the environmental transition\ndynamics.\nfω,θ(s, a, s′) = ˆrω,θ(s) + γ ˆVΩ(s′) −ˆVΩ(s) = ˆA(s, a, ω)\n(21)\nWhere\nQ(s, ω)\n=\nP\na∈A πω,α(a|s)[rω,θ(s, a)\n+\nγ P\ns′∈S P(s′|s, a)((1\n−\nβδ,ω(s′))Q(s′, ω)\n+\nβδ,ω(s′)VΩ(s′))] and VΩ(s) = P\nω∈ΩπΩ,ζ(ω|s)Q(s, ω).\nOur discriminator model must learn a parameterization of\nthe reward function and the value function for each option,\ngiven the total loss function in (37). These parameterized\nmodels are learned with a multi-layer perceptron. For each\noption, the termination functions βω,δ and option-policies\nπω,α are learned using PPOC.\n6.1. The main algorithm: Option-Adversarial Inverse\nReinforcement Learning (oIRL)\nOur main algorithm, oIRL, is given by Algorithm 1. Here,\nwe iteratively train a discriminator from expert and novice\nsampled trajectories using the derived discriminator objec-\ntive. This allows us to obtain reward function estimates for\neach option. We then use any policy optimization method\nfor a policy over options given these estimated rewards.\nWe can also have discriminator input of state-only format as\ndescribed in (21). It is important to note that in our recursive\nloss, we recursively simulate a trajectory to compute the loss\na ﬁnite number of times (and return if the state is terminal).\nWe show the adversarial architecture of this algorithm in\nAppendix D.\n7. Convergence Analysis\nIn this section we explain the gist of the analysis of con-\nvergence of oIRL. The detailed proofs can be found in Ap-\npendix E and F.\nWe ﬁrst show that the actual reward function is recovered\n(up to a constant) by the reward estimators. We show that for\neach option’s reward estimator gθ,ω(s), we have g∗\nω(s) =\nr∗(s) + cω, where cω is a ﬁnite constant. Using the fact\nthat gθ,ω(s) →g∗\nω(s) = r∗(s) + cω, and by using Cauchy-\nSchwarz inequality of sup-norm, we prove that the update\nof the TD-error is a contraction, i.e.,\nmax\ns′′,ω′′ |QπΩ,t(s, ω) −Q∗(s, ω)| ≤ϵ + max\nω∈Ωcω.\n(22)\nIn order to prove asymptotic convergence to the optimal\noption-value Q∗, we show using the contraction argument\nthat gθ,ω(s) + γQ(s′, ω) converges to Q∗by establishing\nthe following inequality:\n|E[gθ,ω(s)] + γE[Q(s′, ω)|s] −Q∗(s′, ω)|\n≤(max\nω∈Ωcω)(ϵ + max\nω∈Ωcω)γ.\n(23)\n8. Experiments\noIRL learns disentangled reward functions for each option\npolicy, which facilitates policy generalizability and is instru-\nmental in transfer learning.\nTransfer learning can be described as using information\nlearned by solving one problem and then applying it to a\ndifferent but related problem. In the RL sense, it means\ntaking a policy trained on one environment and then using\nthe policy to solve a similar task in a different previously\nunseen environment.\nWe run experiments in different environments to address the\nfollowing questions:\n• Does learning a policy over options with the AIRL\nframework improve policy generalization and reward\nrobustness in transfer learning tasks where the environ-\nmental dynamics are manipulated?\n• Can the policy over options framework match or ex-\nceed benchmarks for imitation learning on complex\ncontinuous control tasks?\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nAlgorithm 1 IRL Over Options with Robust Rewards (oIRL)\nRequire: Expert Trajectories: {τ E\n1 , . . . , τ E\nn } ∈TD, Initial Parameters: (θ0, ζ0, δ0, α0), γ\n1: Initialize policies πω,α0, πΩ,ζ0 and discriminators Dθ0,ω, and βω,δ0∀ω ∈Ω\n2: for step t = 0, 1, 2, . . . , T do\n3:\nCollect trajectories τi = (s0, a0, ω0, . . . ) from πω,αt, πΩ,ζt, βω,δt\n4:\nTrain discriminator Dθt,ω\n5:\nfor step k = 0, 1, 2, . . . do\n6:\nSample (sk, ak, s′k, ωk) ∼τi,t\n7:\nif s′ not terminal state then\n8:\nSample ω\n′\nk ∼πΩ,ζt(ω|s\n′\nk), a\n′\nk,1 ∼πω′\nk,αt(a|s\n′\nk), a\n′\nk,2 ∼πωk,αt(a|s\n′\nk)\n9:\nObserve s\n′′\nk,1, s\n′′\nk,2 from environment\n10:\nLk(sk, ak, s\n′\nk, ωk) = −ED[log(Dθt,ωk(sk, ak, s\n′\nk))] −EπΘ,t[log(1 −Dθt,ωk(sk, ak, s\n′\nk))]\n11:\nOptimize model parameters w.r.t.: −LD = Lk + γ(βδt,ωk(s′)L(s\n′\nk, a\n′\nk,1, s\n′′\nk,1, ω\n′\nk)\n12:\n+(1 −βδt,ωk(s\n′\nk))L(s\n′\nk, a\n′\nk,2, s\n′′\nk,2, ωk))\n13:\nend if\n14:\nend for\n15:\nObtain reward rθt,ω(s, a, s′) ←log(Dθt,ω(s, a, s′)) −log(1 −Dθt,ω(s, a, s′))\n16:\nUpdate πω,αt, βω,δt∀ω ∈Ωand πΩ,ζt with any policy optimization method (e.g. PPOC)\n17: end for\nTo answer these questions, we compare our model against\nAIRL (the current state of the art for transfer learning) in a\ntransfer task by learning in an ant environment and modify-\ning the physical structure of the ant and compare our method\non various benchmark IRL continuous control tasks. We\nwish to see if learning disentangled rewards for sub-tasks\nthrough the options framework is more portable.\nWe train a policy using each of the baseline methods and\nour method on these expert demonstrations for 500 time\nsteps on the gait environments and 500 time steps on the\nhierarchical ones. Then we take the trained policy (the\nparameterized distribution) and use this policy on the trans-\nfer environments and observe the reward obtained. Such a\nmethod of transferring the policy is called a direct policy\ntransfer.\n8.1. Gait Transfer Learning Environments\nFor the transfer learning tasks we use Transfer Environ-\nments for MuJoCo (Chu & Arnold, 2018), a set of gym\nenvironments for studying potential improvements in trans-\nfer learning tasks. The task involves an Ant as an agent\nwhich optimizes a gait to crawl sideways across the land-\nscape. The expert demonstrations are obtained from the\noptimal policy in the basic Ant environment. We disable\nthe agent ant in two ways for two transfer learning tasks.\nIn BigAnt tasks, the length of all legs is doubled, no extra\njoints are added though. The Amputated Ant task modiﬁes\nthe agent by shortening a single leg to disable it. These\ntransfer tasks require the learning of a true disentangled re-\nward of walking sideways instead of directly imitating and\nlearning the reward speciﬁc to the gait movements. These\nTable 1. The mean reward obtained (higher is better) over 100 runs\nfor the Gait transfer learning tasks. We also show the results of\nPPO optimizing the ground truth reward.\nBIG ANT\nAMPUTATED ANT\nAIRL (PRIMITIVE)\n-11.6\n134.3\n2 OPTIONS OIRL\n4.7\n122.6\n4 OPTIONS OIRL\n-1.7\n167.1\nGROUND TRUTH\n142.9\n335.4\nmanipulations are shown in Figure 1.\n(a) Ant environment (b) Big Ant environ-\nment\n(c) Amputated Ant en-\nvironment\nFigure 1. MuJoCo Ant Gait transfer learning task environments.\nWhen the ant is disabled, it must position itself correctly to crawl\nforward. This requires a different initial policy than the original\nenvironment where the ant must only crawl sideways.\nTable 1 shows the results in terms of reward achieved for the\nant gait transfer tasks. As we can see, in both experiments\nour algorithm performs better than AIRL. Remark that the\nground truth is obtained with PPO after 2 million iterations\n(therefore much less sample efﬁcient than IRL).\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\n8.2. Maze Transfer Learning Tasks\nWe also create transfer learning environments in a 2D Maze\nenvironment with lava blockades. The goal of the agent is to\ngo through the opening in a row of lava cells and reach a goal\non the other end. For the transfer learning task, we train the\nagent on an environment where the \"crossing\" path requires\nthe agent to go through the middle for (LavaCrossing-M)\nand then the policy is directly transferred and used on a\nGridWorld of the same size where the crossing is on the\nright end of the room (LavaCrossing-R). An additional task\nwould be changing a blockade in a Maze (FlowerMaze-\n(R,T)). The two environments are shown in Figure 2. We\ncan think of two sub-tasks in this environment, going to the\nlava crossing and then going to the goal.\nIn all of these environments, the rewards are sparse. The\nagent receives a non-zero reward only after completing the\nmission, and the magnitude of the reward is 1−0.9·n/nmax,\nwhere n is the length of the successful episode and nmax is\nthe maximum number of steps that we allowed for complet-\ning the episode, different for each mission.\n(a) LavaCrossing-M\nMiniGrid Env\n(b) LavaCrossing-R\nMiniGrid Env\n(c) FlowerMaze-R\nMiniGrid Env\n(d) FlowerMaze-T\nMiniGrid Env\nFigure 2. The MiniGrid transfer learning task set 1. Here the policy\nis trained on (a or c) using our method and the baseline methods\nand then transferred to be used on environment (b or d). The green\ncell is the goal.\nWe show the mean reward after 10 runs using the direct\npolicy transfers on the environments in Table 2. The 4 op-\ntion oIRL achieved the highest reward on the LavaCrossing\ntasks. The FlowerMaze task was quite difﬁcult with most\nalgorithms obtaining very low reward. Options still result\nin a large improvement.\nTable 2. The mean reward obtained (higher is better) over 10 runs\nfor the Maze transfer learning tasks. We also show the results of\nPPO optimizing the ground truth reward.\nLAVACROSSING\nFLOWERMAZE\nAIRL (PRIMITIVE)\n0.64\n0.11\n2 OPTIONS OIRL\n0.67\n0.20\n4 OPTIONS OIRL\n0.81\n0.23\nGROUND TRUTH\n1.00\n1.00\n8.3. Hierarchical Transfer Learning Tasks\nIn addition, we adopt more complex hierarchical environ-\nments that require both locomotion and object interaction.\nIn the ﬁrst environment, the ant must interact with a large\nmovable block. This is called the Ant-Push environment\n(Duan et al., 2016). To reach the goal, the ant must complete\ntwo successive processes: ﬁrst, it must move to the left of\nthe block and then push the block right, which clears the\npath towards the target location. There is a maximum of\n500 timesteps. These can be thought of as hierarchical tasks\nwith pushing to the left, pushing to the right and going to\nthe goal as sub-goals.\nWe also utilize an Ant-Maze environment (Florensa et al.,\n2017) where we have a simple maze with a goal at the end.\nThe agent receives a reward of +1 if it reaches the goal and\n0 elsewhere. The ant must learn to make two turns in the\nmaze, the ﬁrst is down the hallway for one step and then a\nturn towards the goal. Again, we see hierarchical behavior\nin this task: we can think of sub-goals consisting of learning\nto exit the ﬁrst hall of the maze, then making the turn and\nﬁnally going down the ﬁnal hall towards the goal. The two\ncomplex environments are shown in Figure 3.\n(a) Ant-Maze envi-\nronment\n(b) Ant-Push environment\nFigure 3. MuJoCo Ant Complex Gait transfer learning task envi-\nronments. We perform these transfer learning tasks with the Big\nAnt and the Amputated Ant.\nTable 3 shows that oIRL performs better than AIRL in all of\nthe complex hierarchical transfer tasks. In some tasks such\nas the Maze environment, AIRL fails to have any or very\nfew successful runs while our method achieves reasonably\nhigh reward. In the BigAnt push task, AIRL achieves only\nvery minimal reward where oIRL succeeds to perform the\ntask in some cases.\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nTable 3. The mean reward obtained (higher is better) over 100 runs for the MuJoCo Ant Complex Gait transfer learning tasks. We also\nshow the results of PPO optimizing the ground truth reward.\nBIG ANT MAZE\nAMPUTATED ANT MAZE\nBIG ANT PUSH\nAMPUTATED ANT PUSH\nAIRL (PRIMITIVE)\n0.28\n0.14\n0.02\n0.17\n2 OPTIONS OIRL\n0.62\n0.29\n0.46\n0.34\n4 OPTIONS OIRL\n0.55\n0.31\n0.55\n0.41\nGROUND TRUTH\n0.96\n0.98\n0.90\n0.86\n(a) Ant\n(b) Half Cheetah\n(c) Walker\nFigure 4. MuJoCo Continuous control locomotion tasks showing the mean reward (higher is better) achieved over 500 iterations of the\nbenchmark algorithms for 10 random seeds. The shaded area represents the standard deviation.\n8.4. MuJoCo Continuous Control Benchmarks\nWe also test our algorithm on a number of robotic contin-\nuous control benchmark tasks. These tasks do not involve\ntransfer.\nWe show the plots of the average reward for each iteration\nduring training in Figure 4. Achieving a higher reward in\nfewer iterations is better for these experiments. We exam-\nine the Ant, the Half Cheetah, the and Walker MuJoCo\ngait/locomotion tasks. We run these experiments with 10\nrandom seeds. The results are quite similar between the\nbenchmarks. Using a policy over options shows reasonable\nimprovements in each task.\n9. Discussion\nThis work presents Option-Inverse Reinforcement Learning\n(oIRL), the ﬁrst hierarchical IRL algorithm with disentan-\ngled rewards. We validate oIRL on a wide variety of tasks,\nincluding transfer learning tasks, locomotion tasks, com-\nplex hierarchical transfer RL environments and GridWorld\ntransfer navigation tasks and compare our results with the\nstate-of-the-art algorithm. Combining options with a disen-\ntangled IRL framework results in highly portable policies.\nOur empirical studies show clear and signiﬁcant improve-\nments for transfer learning. The algorithm is also shown to\nperform well in continuous control benchmark tasks.\nFor future work, we wish to test other sampling methods\n(e.g., Markov-chain Monte Carlo) to estimate the implicit\ndiscriminator-generator pair’s distribution in our GAN, such\nas Metropolis-Hastings GAN (Turner et al., 2019). We also\nwish to investigate methods to reduce the computational\ncomplexity for the step of computing the recursive loss\nfunction, which requires simulating some short trajectories,\nlowering the variance. Analyzing our algorithm using phys-\nical robotic tests for tasks that require multiple sub-tasks\nwould be an interesting future course of research.\nReferences\nAbbeel, P. and Ng, A. Y. Apprenticeship learning via inverse\nreinforcement learning. in ICML, 2004.\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schul-\nman, J., and Mané, D. Concrete problems in ai safety.\narXiv preprint arXiv:1606.06565, 2016.\nBacon, P.-L., Harb, J., and Precup, D. The option-critic\narchitecture. in AAAI, 2017.\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J.,\nSchulman, J., Tang, J., and Zaremba, W. Openai gym,\n2016.\nChevalier-Boisvert, M., Willems, L., and Pal, S. Minimalis-\ntic gridworld environment for openai gym. https://\ngithub.com/maximecb/gym-minigrid, 2018.\nChu, E. and Arnold, S.\nTransfer environments for mu-\njoco. GitHub, 2018. URL https://github.com/\nseba-1511/shapechanger.\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nDuan, Y., Chen, X., Houthooft, R., Schulman, J., and\nAbbeel, P. Benchmarking deep reinforcement learning\nfor continuous control, 2016.\nFinn, C., Christiano, P., Abbeel, P., and Levine, S. A con-\nnection between generative adversarial networks, inverse\nreinforcement learning, and energy-based models. in\nNeurIPS, 2016a.\nFinn, C., Levine, S., and Abbeel, P. Guided cost learning:\nDeep inverse optimal control via policy optimization. in\nICML, 2016b.\nFlorensa, C., Held, D., Wulfmeier, M., Zhang, M., and\nAbbeel, P. Reverse curriculum generation for reinforce-\nment learning. in CoRL, 2017.\nFu, J., Luo, K., and Levine, S. Learning robust rewards\nwith adversarial inverse reinforcement learning. in ICLR,\n2018.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.\nGenerative adversarial nets. In NeurIPS, 2014.\nHausman, K., Chebotar, Y., Schaal, S., Sukhatme, G., and\nLim, J. J. Multi-modal imitation learning from unstruc-\ntured demonstrations using generative adversarial nets. in\nNeurIPS, 2017.\nHenderson, P., Chang, W.-D., Bacon, P.-L., Meger, D.,\nPineau, J., and Precup, D. Optiongan: Learning joint\nreward-policy options using generative adversarial in-\nverse reinforcement learning. in AAAI, 2018.\nHo, J. and Ermon, S. Generative adversarial imitation learn-\ning. in NeurIPS, 2016.\nKlissarov, M., Bacon, P., Harb, J., and Precup, D. Learnings\noptions end-to-end for continuous action tasks. CoRR,\nabs/1712.00004, 2017.\nKrishnan, S., Garg, A., Liaw, R., Miller, L., Pokorny, F. T.,\nand Goldberg, K. Y.\nHirl: Hierarchical inverse rein-\nforcement learning for long-horizon tasks with delayed\nrewards. ArXiv, abs/1604.06508, 2016.\nKullback, S. and Leibler, R. A. On information and suf-\nﬁciency. The Annals of Mathematical Statistics, 22(1):\n79–86, 1951. ISSN 00034851.\nNg, A. Y. and Russell, S. Algorithms for inverse reinforce-\nment learning. in ICML, 2000.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nSharma, M., Sharma, A., Rhinehart, N., and Kitani, K. M.\nDirected-info GAIL: Learning hierarchical policies from\nunsegmented demonstrations using directed information.\nin ICLR, 2019.\nSutton, R., Precup, D., and Singh, S. Between MDPs and\nsemi-MDPs: A framework for temporal abstraction in\nreinforcement learning. Artiﬁcial Intelligence, 1999.\nTaylor, M. E. and Stone, P. Transfer learning for reinforce-\nment learning domains: A survey. J. Mach. Learn. Res.,\n2009.\nTodorov, E., Erez, T., and Tassa, Y. Mujoco: A physics\nengine for model-based control. In 2012 IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Systems.\nTurner, R., Hung, J., Frank, E., Saatchi, Y., and Yosinski,\nJ. Metropolis-hastings generative adversarial networks.\nICML, 2019.\nWang, Z., Merel, J., Reed, S., Wayne, G., de Freitas, N.,\nand Heess, N. Robust imitation of diverse behaviors. in\nNeurIPS, 2017.\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nA. Option Transition Probabilities\nIt is useful to redeﬁne transition probabilities in terms of options. Since at each step we have a additional consideration, we\ncan continue following the policy of the current option we are in or terminate the option with some probability, sample a\nnew option and follow that option’s policy from a stochastic policy dependent on states. We have\nP(st+1, ωt+1|st, ωt) =\nX\na∈A\nπω,α(a|st)P(st+1|st, a)((1 −βωt,δ(st+1))1ωt=ωt+1+\nβωt,δ(st+1)πΩ,ζ(ωt+1|st+1))\n(24)\nP(st+1, ωt+1|st) =\nX\nω∈Ω\nπζ,Ω(ω|st)\nX\na∈A\nπω,α(a|st)P(st+1|st, a)((1 −βωt,δ(st+1))1ωt=ωt+1+\nβωt,δ(st+1)πΩ,ζ(ωt+1|st+1))\n(25)\nP(st+1, wt+1|st, ωt, at) = P(st+1|st, at)((1 −βωt,δ(st+1))1ωt=ωt+1+\nβωt,δ(st+1)πΩ,ζ(ωt+1|st+1))\n(26)\nB. MLE Objective for IRL Over Options\nWe can deﬁne a discounted return recursively for a policy over options in a similar manner to the transition probabilities.\nConsider the policy over options based on the probabilities of terminating or continuing the option policies given a reward\napproximator ˆrθ(s, a) for the state-action reward.\nRΩ\nζ,θ,α,δ(s) = Eω∈Ω[Rθ,α,δ(s, ω)]\nRθ,α,δ(s, ω) = Ea∈A[Rθ,δ(s, ω, a)]\nRΩ\nζ,θ,δ(s, a) = Eω∈Ω[Rθ,δ(s, ω, a)]\nRθ,δ(s, ω, a) = E\n\u0014\nˆrω,θ(s, a)+\nγ\nX\ns′∈S\nP(s\n′|s, a, ω)(βω,δ(s\n′)RΩ\nζ,θ,α,δ(s\n′) + (1 −βω,δ(s\n′))Rθ,α,δ(s\n′, ω))\n\u0015\n,\n(27)\nThese formulations of the reward function account for option transition probabilities, including the probability of terminating\nthe current option and therefore selecting a new one according to the policy over options.\nWith ω0 selected according to πζ,Ω(ω|s), we can deﬁne a parameterization of the discounted return R in the style of a\nmaximum causal entropy RL problem with objective maxθ Eτ∼D[log(pθ(τ))], where\npθ(τ) ∼p(s0, ω0)\nT −1\nY\nt=0\nP(st+1, ωt+1|st, ωt, at)eRθ,δ(st,ωt,at).\n(28)\nMLE Derivative\nWe can write out our MLE objective for our generator. We may or may not know the option trajectories in our expert\ndemonstrations, but they are estimated below according to the policy over options. This is deﬁned similarly in (Fu et al.,\n2018) and (Finn et al., 2016a) as J(θ) = Eτ∼τ E[PT\nt=0 RΩ\nζ,θ,δ(st, at)] −Epθ[PT\nt=0\nP\nω∈Ωπζ,Ω(ω|st)Rθ,δ(st, ω, at)]. The\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nfull derivation is shown as (with generator pθ):\nJ(θ) = Eτ∼τ E [log(pθ(τ))]\n= Eτ∼τ E\n\u0002\nRθ,δ(st, ω, at)\n\u0003\n−log(Zθ)\n= Eτ∼τ E\n\" T\nX\nt=0\nX\nω∈Ω\nπζ,Ω(ω|st)Rθ(st, ω, at)\n#\n−log(Zθ)\n≈Eτ∼τ E\n\" T\nX\nt=0\nX\nω∈Ω\nπζ,Ω(ω|st)Rθ,δ(st, ω, at)\n#\n−Epθ\n\" T\nX\nt=0\nX\nω∈Ω\nπζ,Ω(ω|st)Rθ,δ(st, ω, at)\n#\n= Eτ∼τ E\n\" T\nX\nt=0\nRΩ\nζ,θ,δ(st, at)\n#\n−Epθ\n\" T\nX\nt=0\nRΩ\nζ,θ,δ(st, at)\n#\n(29)\nWe go from Line 4 to 5 seeing RΩ\nζ,θ,δ(st, at) = P\nω∈Ωπζ,Ω(ω|st)Rθ,δ(st, ω, at).\nNow, we take the gradient of the MLE objective w.r.t θ yields,\n∂\n∂θJ(θ) = Eτ∼τ E\n\u0014 ∂\n∂θ log(pθ(τ))\n\u0015\n∂\n∂θJ(θ) = Eτ∼τ E\n\u0014 T\nX\nt=0\nX\nω∈Ω\nπζ,Ω(ω|st) ∂\n∂θRθ,δ(st, ω, at)\n\u0015\n−∂\n∂θ log(Zθ)\n≈Eτ∼τ E\n\u0014 T\nX\nt=0\n∂\n∂θRΩ\nζ,θ,δ(st, at)\n\u0015\n−Epθ\n\u0014 T\nX\nt=0\n∂\n∂θRΩ\nζ,θ,δ(st, at)\n\u0015\n(30)\nRemark we deﬁne pθ,t(st, at) =\nR\nst′ ̸=t,at′ ̸=t pθ(τ)dst′dat′ as the state action marginal at time t.\n∂\n∂θJ(θ) =\nT\nX\nt=0\nEτ∼τ E\n\u0014 ∂\n∂θRΩ\nζ,θ,δ(st, at)\n\u0015\n−Epθ,t\n\u0014 ∂\n∂θRΩ\nζ,θ,δ(st, at)\n\u0015\n(31)\nWe perform importance sampling over the hard to estimate generator density. We make an importance sampling distribution\nµt,w(τ) for option w.\nWe sample a mixture policy µω(a|s) deﬁned as 1\n2πω(a|s) + 1\n2 ˆpω(a|s) and ˆpω(a|s) is a rough density estimate trained on\nthe demonstrations. We wish to minimize the DKL(πw(τ)|pω(τ)). KL refers to the Kullback–Leibler divergence metric\nbetween two probability distributions. Our new gradient is:\n∂\n∂θJ(θ) =\nT\nX\nt=0\nEτ∼τ E[ ∂\n∂θRΩ\nζ,θ,δ(st, at)] −Eµt\n\u0014X\nω∈Ω\nπζ,Ω(ω|st)pθ,t,ω(st, at)\nµt,w(st, at)\n∂\n∂θRθ,δ(st, ω, at)\n\u0015\n.\n(32)\nTaking the derivative of the discounted option return results in\n∂\n∂θRΩ\nζ,θ,α,δ(s) = E\n\u0014X\nω∈Ω\nπΩ,ζ(ω|s)[\nX\na∈A\n[πw,α(a|s)\n\u0012 ∂\n∂θ ˆrω,θ(s, a)\n+γ\nX\ns′∈S\nP(s\n′|s, a)(βω,δ(s\n′) ∂\n∂θRΩ\nζ,θ,α,δ(s\n′) + (1 −βω,δ(s\n′)) ∂\n∂θRθ,α,δ(s\n′, ω))\n\u0013\n]\n\u0015\n.\n(33)\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\n∂\n∂θRΩ\nζ,θ,δ(s, a) = E\n\u0014X\nω∈Ω\nπΩ,ζ(ω|s)\n\u0012 ∂\n∂θ ˆrω,θ(s, a)\n+γ\nX\ns′∈S\nP(s\n′|s, a)(βω,δ(s\n′) ∂\n∂θRΩ\nζ,θ,α,δ(s\n′) + (1 −βω,δ(s\n′)) ∂\n∂θRθ,α,δ(s\n′, ω))\n\u0013\u0015\n(34)\nC. Discriminator Objective\nWe formulate the discriminator as the odds ratio between the policy and the exponentiated reward distribution for option ω\nas in AIRL parameterized by θ. We have a discriminator for each option ω and generator option policy πw,\nDθ,ω(s, a) =\nexp(fθ,ω(s, a))\nexp(fθ,ω(s, a)) + πw(a|s).\n(35)\nC.1. Recursive Loss Formulation\nWe minimize the cross-entropy loss between expert demonstrations and generated examples assuming we have the same\nnumber of options in the generated and expert trajectories. We deﬁne the loss function Lθ as follows:\nLθ(s, a, ω) = −ED[log(Dθ,ω(s, a))] −EπΘ,t[log(1 −Dθ,ω(s, a))].\n(36)\nThe total loss for the entire trajectory can be expressed recursively as follows by taking expectations over the next options or\nstates:\nLθ,δ(s, a, ω) = lθ(s, a, ω) + γ\nX\ns′∈S\nP(s\n′|s, a)(βw,δ(s\n′)LΩ\nζ,θ,α,δ(s\n′) + (1 −βw,δ(s\n′))Lθ,α,δ(s\n′, w))\nLθ,α,δ(s, w) = Ea∈A[[Lθ,δ(s, w, a)]\nLΩ\nζ,θ,δ(s, a) = Ew∈Ω[Lθ,δ(s, w, a)]\nLΩ\nζ,θ,α,δ(s) = Eω∈Ω[Lθ,α,δ(s, ω)]\n(37)\nThe agent wishes to minimize Lθ,δ to ﬁnd its optimal policy.\nWe can let cost function fθ,w(s, a) = Lθ,δ(s, ω, a) as shown in AIRL and we have:\nDθ,ω =\nexp(Lθ,δ(s, ω, a))\nexp(Lθ,δ(s, ω, a)) + πω(a|s)\n(38)\nC.2. Optimization Criteria\nFor a given option ω, we can write the reward function ˆRθ,δ(s, ω, a) to be maximised, as follows. Note that θ parameterizes\nthe state-action reward function estimate for option ω. −LD is the negative discriminator loss. We therefore turn our\nminimization problem into a maximization problem. We deﬁne our objective similar to the GAN objective from AIRL:\n−LD = ˆRθ,δ(s, ω, a) = log (Dθ,ω(s, a)) −log (1 −Dθ,ω(s, a))\n(39)\nNow we can write out our reward function in terms of the optimal discriminator\nˆRθ,δ(s, ω, a) = log\n\u0012\nexp (−Lθ,δ(s, ω, a))\nexp (−Lθ,δ(s, ω, a)) + πω(a|s)\n\u0013\n−log\n\u0012\nπω(a|s)\nexp (−Lθ,δ(s, ω, a)) + πω(a|s)\n\u0013\n= −Lθ,δ(s, ω, a) −log(πω(a|s))\n(40)\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nThe derivative of this reward function can now be computed as follows:\n∂\n∂θ\nˆRθ,δ(s, ω, a) ≈∂\n∂θ −Lθ,δ(s, ω, a)\n= E\n\n∂\n∂θrω,θ(s, a) + γ\nX\ns′∈S\nP(s\n′|s, a)\n\u0012\nβω,δ(s\n′) ∂\n∂θ −LΩ\nζ,θ,α,δ(s\n′) + (1 −βω,δ(s\n′)) ∂\n∂θ −Lθ,α,δ(s\n′, ω)\n\u0013\n\n= E\n\u0014 ∂\n∂θrω,θ(s, a)\n\u0015\n+ E\n\u0014\nγ\nX\ns′∈S\nP(s\n′|s, a)\n\u0012\nβω,δ(s\n′) ∂\n∂θ −LΩ\nζ,θ,α,δ(s\n′)+\n(1 −βω,δ(s\n′)) ∂\n∂θ −Lθ,α,δ(s\n′, ω)\n\u0013\u0015\n(41)\nWriting out our discriminator objective yields:\n−LD =\nT\nX\nt=0\nEτ∼τ E\n\u0012h X\nω∈Ω\nπΩ,ζ(ω|st) log(Dθ,ω(st, at))\ni\n+\nEπt\nh X\nω∈Ω\nπΩ,ζ(ω|st) log(1 −Dθ,ω(st, at))\ni\u0013\n=\nT\nX\nt=0\nEτ∼τ E\n\"X\nω∈Ω\nπΩ,ζ(ω|st) log\n\u0012\nexp(−Lθ,δ(st, ω, at))\nexp(−Lθ,δ(st, ω, at)) + πω(at|st)\n\u0013#\n+ Eπt\n\"X\nω∈Ω\nπΩ,ζ(ω|st) log\n\u0012\nπω(at|st)\nexp(−Lθ,δ(st, ω, at)) + πω(at|st)\n\u0013#\n=\nT\nX\nt=0\nEτ∼τ E\n\"X\nω∈Ω\nπΩ,ζ(ω|st) −Lθ,δ(st, ω, at)\n#\n−\nEτ∼τ E\n\"X\nω∈Ω\nπΩ,ζ(w|st) log(exp(−Lθ,δ(st, w, at)) + πω(a|st))\n#\n+ Eπt\n\"X\nω∈Ω\nπΩ,ζ(ω|st) log(πω(at|st))\n#\n−\nEπt\n\"X\nω∈Ω\nπΩ,ζ(ω|st) log(exp(−Lθ,δ(st, ω, at)) + πω(at|st))\n#\n(42)\nWe set a mixture of experts and novice as ¯µ observations.\n=\nT\nX\nt=0\nEτ∼τ E\n\"X\nω∈Ω\nπΩ,ζ(ω|st) −Lθ,δ(st, ω, at)\n#\n+ Eπt\n\"X\nω∈Ω\nπΩ,ζ(ω|st) log(πω(at|st))\n#\n−2 E¯µt\n\"X\nω∈Ω\nπΩ,ζ(ω|st) log (exp(−Lθ,δ(st, ω, at)) + πω(at|st))\n#\n(43)\nWe can take the derivative w.r.t θ (state-action reward function estimate parameter):\n∂\n∂θ(−LD) =\nT\nX\nt=0\nEτ∼τ E\n\"X\nω∈Ω\nπΩ,ζ(ω|st) ∂\n∂θ −Lθ,δ(st, ω, at)\n#\n−E¯µt\n\"X\nω∈Ω\nπΩ,ζ(ω|st)\n\u0012\nexp(−Lθ,δ(st, ω, at))\n1\n2 exp(−Lθ,δ(st, ω, at)) + 1\n2πω(at|st)\n\u0013 ∂\n∂θ −Lθ,δ(st, ω, at)\n#\n(44)\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nWe can multiply the top and bottom of the fraction in the mixture expectation by the state marginal πω(st) =\nR\na∈A πω(st, at).\nThis allows us to write ˆpθ,t,ω(st, at) = exp(Lθ,δ(st, ω, at))πω,t(st). Now we have an importance sampling.\n∂\n∂θ(−LD) =\nT\nX\nt=0\nEτ∼τ E\n\"X\nω∈Ω\nπΩ,ζ(ω|st) ∂\n∂θ −Lθ,δ(st, ω, at)\n#\n−E¯µt\n\"X\nω∈Ω\nπΩ,ζ(ω|st)\n\u0012 ˆpθ,t,ω(st, at)\nˆµt,ω(st, at)\n\u0013 ∂\n∂θ −Lθ,δ(st, ω, at)\n#\n(45)\nIt is now easy to see we have the same form as our MLE objective loss function, our loss (the function we approximate with\nthe GAN) is the discounted reward for a state action pair with the expectation over options. We change the loss functions to\nreward functions to show this, as they are deﬁned equivalently.\n∂\n∂θ(−LD) =\nT\nX\nt=0\nEτ∼τ E\n\u0014 ∂\n∂θRζ,θ,δ(st, at)\n\u0015\n−E¯µt\n\"X\nω∈Ω\nπΩ,ζ(ω|s)\n\u0012 ˆpθ,t,ω(st, at)\nˆµt,ω(st, at)\n\u0013 ∂\n∂θRθ,δ(st, ω, at)\n#\n(46)\nIn addition, we can decompose the reward into a state-action reward and a future discounted sum of rewards considering the\npolicy over options as follows:\n∂\n∂θ(−LD) =\nT\nX\nt=0\nEτ∼τ E\n\"X\nω∈Ω\nπΩ,ζ(ω|st) ∂\n∂θrω,θ(st, at)\n#\n|\n{z\n}\nState-Action Reward\n+ Eτ∼τ E\n\u0014X\nω∈Ω\nπΩ,ζ(ω|st)γ\nX\nst+1∈S\nP(st+1|st, at)(βω,δ(st+1) ∂\n∂θRΩ\nζ,θ,α,δ(st+1)+\n(1 −βw,δ(st+1)) ∂\n∂θRθ,α,δ(st+1, ω))\n\u0015\n−E¯µt\n\"X\nω∈Ω\nπΩ,ζ(ω|st)\n\u0012 ˆpθ,t,ω(st, at)\nˆµt,ω(st, at)\n\u0013 ∂\n∂θRθ,δ(st, ω, at)\n#\n(47)\ne are given a mixture of experts and generated policies as ¯µt and perform importance sampling with respect to this\ndistribution.\nD. GAN Architecture\nThe architecture for our GAN-IRL framework is described in Figure 5.\nE. Proof of Recoverable Rewards\nA substantial amount of this proof is derived from (Fu et al., 2018).\nLemma 1: fθ,ω(s, a) recovers the advantage.\nProof: It is known that when πω = πE\nω , we have achieved the global min of the discriminator objective. The discriminator\nmust then output 0.5 for all state action pairs. This results in exp(fθ,ω(s, a)) = πE\nω (a|s). Equivalently we have f ∗\nω(s, a) =\nlog πE\nω (a|s) = A∗(s, a, ω).\nDeﬁnition 1: Decomposability condition. We ﬁrst deﬁne 2 states s1, s2 as 1-step linked under dynamics T(s′|s, a) if there\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nFigure 5. Architecture of GAN-IRL framework.\nexists a state s that can reach s1 and s2 with non-zero probability in one timestep. The transitivity property holds for the\nlinked relationship. We can say that if s1 and s2 and linked, s2 and s3 are linked then s1 and s3 must also be linked.\nThe Decomposability condition for transition dynamics T holds if all states in the MDP are linked with all other states.\nLemma 2: For an MDP, where the decomposability condition holds for all dynamics.\nFor arbitrary functions\na(s), b(s), c(s), d(s), if for all s and s′\na(s) + b(s′) = c(s) + d(s′)\n(48)\nand for all s\na(s) = c(s) + consts\n(49)\nb(s) = d(s) + consts,\n(50)\nwhere consts is a constant dependent with respect to state s.\nProof: If we rearrange Equation 48, we can obtain the quality a(s) −c(s) = b(s′) −d(s′).\nNow we deﬁne f(s) = a(s) −c(s). Given our equality, we have f(s) = a(s) −c(s) = b(s′) −d(s′). This holds for some\nfunction dependent on s.\nTo represent this, b(s′) −d(s′) must be equal to a constant (with the constant’s value dependent on the state s) for all\none-step successor states s′ from s.\nNow, under decomposability, all one step successor states (s′) from s must be equal through the transitivity property so\nb(s′) −d(s′) must be a constant with respect to state s. Therefore, we can write a(s) = c(s) + const+s for an arbitrary\nstate s and functions b and d.\nSubstituting this into the Equation 48, we can obtain b(s) = d(s) + consts. This completes our proof.\nInductive proof for any successor state\nLet us consider for any MDP and any arbitrary functions a(·), b(·), c(·) and d(·),\na(s) + b(S(k)) = c(s) + d(S(k)),\n(51)\nwhere S(k) is the k-th successor state reached in k time-steps from the current state. Let us denote by T π,(k)(s, S(k)) the\nprobability of transitioning from state s to S(k) in k steps using policy π. Then, we can express T π,(k)(s, S(k)) recursively\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nas follows:\nT π,(k)(s, S(k)) =\nX\ns′∈S\nT π,(k−1)(s, s′)T π(s′, S(k)),\n(52)\nwhere T π(s′, S(k)) is the one-step transition probability from state s′ to state S(k) (by deﬁnition of the Bellman operator).\nDenote by P(S(k)) the probability of landing in state S(k) in k steps from any current state. We can write P(S(k)) using (52)\nas follows:\nP(S(k)) :=\nX\ns∈S\nT π,(k)(s, S(k))µ(s),\n(53)\nwhere µ is the state-distribution.\nThe unbiased estimator ˆs(k) of an unknown successor state S(k) is given by:\nˆs(k) := E(S(k)) =\nX\ns(k)∈S\ns(k)P(S(k)),\n(54)\nwhere P(S(k)) is given in (53).\nNow, replacing S(k) in (51) with its unbiased estimator ˆs(k) as given by (54), we have\na(s) −c(s) = b(ˆs(k)) −d(ˆs(k))\n(a)\n= f(k),\n(55)\nfor some function f, where (a) holds since ˆs(k) depends only on k. Thus, we get a(s) = c(s)+const. and b(s) = d(s)+const.\nwhere the constant is with respect to the state s.\nTheorem 1: Suppose we have, for a MDP where the decomposability condition holds,\nfθ,ω(s, a, s′) = gω(s, a) + γhΦ(s′) −hΦ(s)\n(56)\nwhere hΦ is a shaping term. If we obtain the optimal f ∗\nθ,ω(s, a, s′), with a reward approximator g∗\nω(s, a). Under deterministic\ndynamics the following holds\ng∗\nω(s, a) + γh∗\nΦ(s′) −h∗\nΦ(s) = r∗\nω(s) + γV ∗\nΩ(s′) −V ∗\nΩ(s)\n(57)\nand\ng∗\nω(s) = r∗\nω(s) + cω.\n(58)\nProof: We know f ∗\nω(s, a, s′) = A∗(s, a, ω) = Q∗(s, a, ω) −V ∗\nΩ(s) = r∗\nω(s) + γV ∗\nΩ(s′) −V ∗\nΩ(s). We can substitute the\ndeﬁnition of f ∗\nω(s, a, s′) to obtain our Theorem.\nWhere Q(s, ω)\n=\nP\na∈A πω,α(a|s)[rω,θ(s, a) + γ P\ns′∈S P(s′|s, a) ((1 −βδ,ω(s′))Q(s′, ω) + βδ,ω(s′)VΩ(s′))] and\nVΩ(s) = P\nω∈ΩπΩ,ζ(ω|s)Q(s, ω)\nQ(s, a, ω) = πω,α(a|s)[rω,θ(s, a) + γ P\ns′∈S P(s′|s, a) ((1 −βδ,ω(s′))Q(s′, ω) + βδ,ω(s′)VΩ(s′))]\nwhich holds for all s and s′. Now we apply Lemma 2. We say that a(s) = g∗\nω(s) −h∗\nΦ(s), b(s′) = γh∗\nΦ(s′), c(s) = r(s) −\nV ∗\nΩ(s) and d(s′) = γV ∗\nΩ(s′) and rearrange according to Lemma 2. We therefore have our results that g∗\nω(s) = rω(s) + cω.\nWhere cω is a constant.\nF. Proof of Convergence\nDeﬁnition 2: Reward Approximator Error. From Theorem 1, we can see that our reward approximator g∗\nω(s) =\nrω(s) + cω. We deﬁne a reward approximator error over all options as δr = P\nω∈ΩπΩ(ω)|g∗\nω(s) −r∗(s)|. This error is\nbounded by\nδr =\nX\nω∈Ω\nπΩ(ω)|g∗\nω(s) −r∗(s)| ≤max\nω∈Ωcω\n(59)\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nBy deﬁnition of g∗\nω(s).\nLemma 3: The Bellman operator for options in the IRL problem is a contraction.\nProof: We prove this by Cauchy-Schwarz and the deﬁnition of the sup-norm. We must deﬁne this inequality in terms of\nthe IRL problem where we have a reward estimator ˆgθω(s) under our learned parameter θ and an optimal reward estimator\nr∗(s).\n||QπΩ,t(s, ω) −Q∗(s, ω)||∞\n= ||ˆgθ(s) + γ\nX\ns′∈S\nP(s′|s, a)((1 −β(s′)QπΩ,t(s′, ω) + β(s′) max\nω∈ΩQπΩ,t(s′, ω))−\nr∗(s) + γ\nX\ns′∈S\nP(s′|s, a)((1 −β(s′)Q∗(s′, ω) + β(s′) max\nω∈ΩQ∗(s′, ω))||∞\n= ||ˆgθ(s) −r∗(s) +\nX\ns′∈S\nP(s′|s, a)[(1 −β(s′))(QπΩ,t(s′, ω) −Q∗(s′, ω))]+\n[β(s′)(max\nω∈ΩQπΩ,t(s′, ω) −max\nω∈ΩQ∗(s′, ω))]||∞\n= ||\nX\ns′∈S\nP(s′|s, a)[(1 −β(s′))(QπΩ,t(s′, ω) −Q∗(s′, ω))]+\n[β(s′)(max\nω∈ΩQπΩ,t(s′, ω) −max\nω∈ΩQ∗(s′, ω))]||∞+ max\nω∈Ωcω\n≤\nX\ns′∈S\nP(s′|s, a) max\ns′′,ω′′ ||QπΩ,t(s′′, ω′′) −Q∗(s′′, ω′′)||∞+ max\nω∈Ωcω\n≤γ max\ns′′,ω′′ ||QπΩ,t(s′′, ω′′) −Q∗(s′′, ω′′)||∞+ max\nω∈Ωcω\n(60)\nThis is given by Lemma 3 and (Sutton et al., 1999) [Theorem 3].\nGiving our results maxs′′,ω′′ |QπΩ,t(s, ω) −Q∗(s, ω)| ≤ϵ + maxω∈Ωcω. For ϵ ∈R>0\nTheorem 2: gθ(s) + γQ(s′, ω) converges to Q∗.\nProof: We know gθ(s) →g∗\nθ(s) = r∗(s) + const. Given this we can show by Cauchy-Schwarz:\n|E[gθ(s)] + γE[Q(s′, ω)|s] −Q∗(s′, ω)|\n= |E[gθ(s)] + γ\nX\ns′∈S\nP(s′|s, a)((1 −βω(s′))Q(s′ω) + βω(s′)VΩ(s′))\n−r∗(s) −\nX\ns′∈S\nP(s′|s, a)((1 −βω(s′)Q∗(s′, ω)) + βω(s′) max\nω∈ΩQ∗(s′, ω)|\n= |E[gθ(s)] −r∗(s) + γ\nX\ns′∈S\nP(s′|s, a)[βω(s′)[max\nω∈ΩQ(s′ω) −max\nω∈ΩQ∗(s′, ω)]\n+ (1 −βω(s′))[Q(s′ω) −Q∗(s′, ω)]]|\n(a)\n≤(max\nω∈Ωcω)|γ\nX\ns′∈S\nP(s′|s, a)[max\ns′′,ω′′ ||Q(s′′, ω′′) −Q∗(s′′, ω′′)|]|\n(b)\n≤(max\nω∈Ωcω)(ϵ + max\nω∈Ωcω)γ\nX\ns′∈S\nP(s′|s, a)\n≤(max\nω∈Ωcω)(ϵ + max\nω∈Ωcω)γ,\n(61)\nwhere (a) follows from Lemma 3 and (b) holds since P\ns′∈S P(s′|s, a) ≤1.\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nG. Parameters for Experiments\nG.1. MuJoCo Tasks\nFor these experiments, we use PPO to obtain an optimal policy given our ground truth rewards for 2 million iterations\nand 20 million on the complex tasks. This is used to obtain the expert demonstrations. We sample 50 expert trajectories.\nPPOC is used for the policy optimization step for the policy over options. We tune the deliberation cost hyper-parameter\nvia cross-validation. The optimal deliberation cost found was 0.1 for PPOC. We also use state-only rewards for the policy\ntransfer tasks. The hyperparameters for our policy optimization are given in Table 4.\nOur discriminator is a neural network with the optimal architecture of 2 linear layers of 50 hidden states, each with ReLU\nactivation followed by a single node linear layer for output. We also tried a variety of hidden states including 100 and 25\nand tanh activation during our hyperparameter optimization step using cross-validation.\nThe policy network has 2 layers of 64 hidden states. A batch size of 64 or 32 is used for 1 and any number of options greater\nthan 1 respectively. No mini-batches are used in the discriminator since the recursive loss must be computed. There are 2048\ntimesteps per batch. Generalized Advantage Estimation is used to compute advantage estimates. We list additional network\nparameters in the next section. The output of the policy network gives the Gaussian mean and the standard deviation. This is\nthe same procedure as in (Schulman et al., 2017).\nTable 4. Policy Optimization parameters for MuJoCo\nParameter\nValue\nDiscr. Adam optimizer learning rate\n1 · 10−3\nAdam ϵ\n1 · 10−5\nPPOC Adam optimizer learning rate\n3 · 10−4\nGAE λ\n0.95\nEntropy coefﬁcient\n10−2\nvalue loss coefﬁcient\n0.5\ndiscount\n0.99\nbatch size for PPO\n64 or 32\nPPO epochs\n10\nentropy coefﬁcient\n10−2\nclip parameter\n0.2\nG.2. MuJoCo Continuous Control Tasks\nIn this section, we describe the structure of the objects that gait in the continuous control benchmarks and the reward\nfunctions. For the transfer learning tasks, we use the same reward function described here for the Ant.\nWalker: The walker is a planar biped. There are 7 rigid links comprised of legs, a torso. This includes 6 actuated joints.\nThis task is particularly prone to falling. The state space is of 21 dimensions. The observations in the states include joint\nangles, joint velocities, the center of mass’s coordinates. The reward function is r(s, a) = vx −0.005||a||2\n2. The termination\ncondition occurs when zbody < 0.8, zbody > 2.0 or ||θy|| > 1.0.\nHalf-Cheetah: The half-cheetah is a planar biped also like the Walker. There are 9 rigid links comprised of 9 actuated\njoints, a leg and a torso. The state space is of 20 dimensions. The observations include joint angles, the center of mass’s\ncoordinates, and joint velocities. The reward function is r(s, a) = vx −0.005||a||2\n2. There is no termination condition.\nAnt: The ant has four legs with 13 rigid links in its structure. The legs have 8 actuated joints. The state space is of 125\ndimensions. This includes joint angles, joint velocities, coordinates of the center of mass, the rotation matrix for the body,\nand a vector of contact forces. The function is r(s, a) = vx −0.005||a||2\n2 −Ccontact + 0.05, where Ccontact is a penalty for\ncontacts to the ground. This is 5 × 10−4||Fcontact||2\n2. Fcontact is the contact force. It’s values are clipped to be between 0 and\n1. The termination condition occurs zbody < 0.2 or zbody > 1.0.\noIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions\nFigure 6. Architecture of the actor-critic policies on MiniGrid. Conv is Convolutional Layer and ﬁlter sized is described below. FC is a\nfully connected layer.\nG.3. MiniGrid Tasks\nFor experiments, we used the PPOC algorithm with parallelized data collection and GAE. 0.1 is the optimal deliberation\ncost. Each environment is run with 10 random network initialization. As before, in Table 5, we show some of the policy\noptimization parameters for MiniGrid Tasks. We rely on an actor-critic network architecture for these tasks. Since the\nstate space is relatively large and spatial features are relevant, we use 3 convolutional layers in the network. The network\narchitecture is detailed in Figure 6. n and m are deﬁned by the grid dimensions.\nThe discriminator network is again an neural network with the optimal architecture of 3 linear layers of 150 hidden states,\neach with ReLU activation followed by a single node linear layer for output.\nTable 5. Policy optimization parameters for benchmark tasks in MiniGrid\nParameter\nValue\nAdam optimizer learning rate\n7 · 10−4\nAdam ϵ\n10−5\nentropy coefﬁcient\n10−2\nvalue loss coefﬁcient\n0.5\ndiscount\n0.99\nmaximum norm of gradient in PPO\n0.5\nnumber of PPO epochs\n4\nbatch size for PPO\n256\nentropy coefﬁcient\n10−2\nclip parameter\n0.2\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-02-20",
  "updated": "2020-02-20"
}