{
  "id": "http://arxiv.org/abs/2002.11816v1",
  "title": "Streaming Active Deep Forest for Evolving Data Stream Classification",
  "authors": [
    "Anh Vu Luong",
    "Tien Thanh Nguyen",
    "Alan Wee-Chung Liew"
  ],
  "abstract": "In recent years, Deep Neural Networks (DNNs) have gained progressive momentum\nin many areas of machine learning. The layer-by-layer process of DNNs has\ninspired the development of many deep models, including deep ensembles. The\nmost notable deep ensemble-based model is Deep Forest, which can achieve highly\ncompetitive performance while having much fewer hyper-parameters comparing to\nDNNs. In spite of its huge success in the batch learning setting, no effort has\nbeen made to adapt Deep Forest to the context of evolving data streams. In this\nwork, we introduce the Streaming Deep Forest (SDF) algorithm, a\nhigh-performance deep ensemble method specially adapted to stream\nclassification. We also present the Augmented Variable Uncertainty (AVU) active\nlearning strategy to reduce the labeling cost in the streaming context. We\ncompare the proposed methods to state-of-the-art streaming algorithms in a wide\nrange of datasets. The results show that by following the AVU active learning\nstrategy, SDF with only 70\\% of labeling budget significantly outperforms other\nmethods trained with all instances.",
  "text": "Streaming Active Deep Forest for Evolving Data Stream Classiﬁcation\nAnh Vu Luong1 , Tien Thanh Nguyen2 and Alan Wee-Chung Liew1\n1School of Information and Communication Technology, Grifﬁth University, Australia\n2School of Computing Science and Digital Media, Robert Gordon University, Aberdeen, Scotland, UK\nvu.luong@grifﬁthuni.edu.au\nAbstract\nIn recent years, Deep Neural Networks (DNNs)\nhave gained progressive momentum in many areas\nof machine learning. The layer-by-layer process of\nDNNs has inspired the development of many deep\nmodels, including deep ensembles. The most no-\ntable deep ensemble-based model is Deep Forest,\nwhich can achieve highly competitive performance\nwhile having much fewer hyper-parameters com-\nparing to DNNs. In spite of its huge success in the\nbatch learning setting, no effort has been made to\nadapt Deep Forest to the context of evolving data\nstreams. In this work, we introduce the Streaming\nDeep Forest (SDF) algorithm, a high-performance\ndeep ensemble method specially adapted to stream\nclassiﬁcation.\nWe also present the Augmented\nVariable Uncertainty (AVU) active learning strat-\negy to reduce the labeling cost in the streaming con-\ntext. We compare the proposed methods to state-\nof-the-art streaming algorithms in a wide range of\ndatasets. The results show that by following the\nAVU active learning strategy, SDF with only 70%\nof labeling budget signiﬁcantly outperforms other\nmethods trained with all instances.\n1\nIntroduction\nRecent years have witnessed a remarkable success of Deep\nNeural Networks (DNNs) [LeCun et al., 2015] in various do-\nmains, including images, videos, audios, and text processing\ntasks. Though DNNs are extremely powerful, they have some\nlimitations: (1) they require a huge amount of labeled data to\nachieve high performance; (2) training them is hard and slow\nwith an enormous number of parameters; (3) their effective-\nness highly depends on careful hyper-parameters tuning for\ndifferent tasks. These problems are even more severe when\napplying DNNs to the online setting, where the model cannot\nreaccess historical data, leading to its slow convergence.\nThe success of DNNs is commonly attributed to its rep-\nresentation learning capability, which mainly relies on layer-\nby-layer processing of the feature information. This recog-\nnition inspired the emergence of deep ensemble methods,\nmost notably the gcForest model [Zhou and Feng, 2017],\nwhich can solve the above-mentioned problems of DNNs\nwhile keeping the representation learning ability and pro-\nducing high prediction accuracy. In particular, gcForest has\nmuch fewer hyper-parameters in comparison to DNNs and\ncan achieve better results across various domains when using\nthe same setting.\nData stream mining has become increasingly important in\nrecent years owing to the massive amount of real-time data\ngenerated by sensor networks, IoT devices, and system logs.\nBuilding a strong predictive model for data streams is, there-\nfore, a crucial task for many applications.\nUnlike in tra-\nditional batch classiﬁcation where we can store the entire\ndataset in memory and process them with unlimited time,\nhere we consider the evolving data stream setting where the\nfollowing learning paradigms and resource constraints need\nto be satisﬁed: (1) the model is ready to classify any sequen-\ntially arriving instances at any time; (2) we expect an inﬁnite\nsequence of data processed under limited time and memory;\n(3) the data distribution may change over time (the appear-\nance of concept drift [Webb et al., 2016]); (4) the model can\nonly observe each instance once before discarding it.\nIn learning from evolving data streams, the labeling pro-\ncess may incur high costs and may require a great deal of hu-\nman effort. Active learning studies how to wisely query the\nmost informative instances instead of asking for all labels. An\neffective active learning strategy can save us a huge number\nof label requests while keeping the performance of the learner\nas high as possible. It also helps accelerate the learning pro-\ncess since the learner will be trained on fewer instances.\nIn this work, we introduce a novel active classiﬁcation\nmethod for evolving data streams. First, we present Stream-\ning Deep Forest (SDF), which is an adaptation of the gcForest\nmodel for the stream setting. SDF retains the representation\nlearning ability of gcForest by reusing its cascade structure.\nTo update the model on the ﬂy, we replace the classic Random\nForest [Breiman, 2001] at each layer by Adaptive Random\nForest (ARF) [Gomes et al., 2017], a high-performance for-\nest model for the stream setting. Concerning the problem of\nconcept drift, SDF incorporates an active drift detection strat-\negy. More details of SDF is described in Section 3. Second,\nwe enhance the Variable Uncertainty (VU) strategy [ˇZliobait˙e\net al., 2011] to obtain a novel active learning method, namely\nAugmented Variable Uncertainty (AVU). We provide a theo-\nretical proof that the VU strategy does not take the full advan-\ntage of the given budget, and then we propose AVU strategy\narXiv:2002.11816v1  [cs.LG]  26 Feb 2020\nto tackle this issue. Our contributions in this work are sum-\nmarized as follows:\n1) Streaming Deep Forest (SDF): We introduce a deep en-\nsemble method, namely SDF, that achieves high prediction\naccuracy by exploiting the layer-by-layer processing of raw\nfeatures. To the best of our knowledge, SDF is the ﬁrst deep\nensemble model being used under the data stream setting.\n2) Augmented Variable Uncertainty (AVU) active learning\nstrategy: We theoretically show a problem of the Variable\nUncertainty (VU) strategy that it does not make full use of\nthe given budget, and propose the AVU strategy to ﬁx that\nissue.\n3) Empirical analysis: We compare the proposed methods\nwith a number of state-of-the-art algorithms for streaming\ncontext concerning a wide range of datasets. The experiment\nresults show that by following the AVU strategy, SDF signif-\nicantly outperforms all the benchmark algorithms even when\nit uses only 70% of the labeling budget.\nIn the next sections, we will discuss the background and\nrelated work (Section 2), followed by the proposed methods\n(Section 3) and experiments (Section 4). Finally, Section 5\nconcludes this work and presents directions for future works.\n2\nBackground and Related Work\n2.1\nEnsemble Methods and Deep Ensemble\nMethods for batch learning\nA multitude of ensemble systems are widely used in the tra-\nditional batch learning setting, including Bagging [Breiman,\n1996], Boosting [Freund and Schapire, 1997], Random Sub-\nspace [Barandiaran, 1998], and Random Forest [Breiman,\n2001]. These methods are different in how they generate di-\nversity in the ensemble. Bagging, for example, trains base\nlearners on different bootstrap replicates obtained by using\nsampling with replacement of the training set. Meanwhile,\nRandom Subspace pays attention to the feature space by train-\ning each base learner on a randomly selected subset of fea-\ntures. Random Forest extends Bagging by using Decision\nTrees as its base learners and choosing a random subset of\nfeatures to be used for splits in each base tree.\nRecently, the ﬁrst ensemble-based deep model has been in-\ntroduced, namely gcForest [Zhou and Feng, 2017]. It was\nconstructed using multiple layers, each of which contains two\nCompletely-Random Tree Forests and two Random Forests\n[Breiman, 2001]. In detail, each forest in a layer outputs a\nclass vector obtained by averaging the class distribution vec-\ntors of all the base decision trees. Then a concatenation of\nthe original feature vector and four class vectors returned by\nfour random forests is used as the input data for the next layer.\nThe gcForest model achieves superior performance on a wide\nrange of domains in comparison to DNNs and other ensem-\nble algorithms. More importantly, gcForest has much fewer\nhyper-parameters than DNNs and performs robustly on vari-\nous datasets by using the same parameter setting.\n2.2\nState-of-the-art methods for evolving data\nstreams\nThere are a massive number of methods for data stream clas-\nsiﬁcation. Here we only consider state-of-the-art algorithms\naccording to their prediction performance and ﬂexibility.\nAlmost all the strongest models for evolving data streams\nare ensemble-based methods, because they can handle con-\ncept drifts effectively by selectively removing or adding base\nlearners when changes happen. Online Bagging [Oza and\nRussell, 2001] is an adapted replicate of the classical Bagging\nalgorithm, in which the Poisson(1) distribution is employed\nto simulate the behavior of bootstrap technique in an online\nmanner. Leveraging Bagging [Bifet et al., 2010] enhances\nOnline Bagging by adding more randomization to the input\nand output of the base learners and employing the ADap-\ntive WINdow (ADWIN) drift detection algorithm [Bifet and\nGavalda, 2007] to selectively reset the base models when-\never concept drift occurs. Chen et al. proposed an online\nversion of Smooth Boost [Servedio, 2003], namely Online\nSmooth Boost (OSBoost) [Chen et al., 2012], which aims\nto generate only smooth distributions that do not assign too\nmuch weight to a single instance. It is theoretically guaran-\nteed that OSBoost can achieve arbitrarily small error rate as\nlong as the number of weak learners and instances are suf-\nﬁciently large. Adaptive Random Forest (ARF) [Gomes et\nal., 2017] aims to adapt the classical Random Forest to the\ndata stream setting by employing the online bootstrap resam-\npling, similar to Leveraging Bagging. To deal with concept\ndrift, ARF uses two change detectors per base tree to detect\nwarnings and drifts. In particular, when a warning is trig-\ngered, a background tree is created and updated without af-\nfecting the ensemble predictions. If the warning escalates to\na drift after a period of time, the background tree replaces the\ncorresponding base tree in the ensemble. Recently, Gomes\net al. introduced Streaming Random Patches (SRP) [Gomes\net al., 2019], which resembles the classic Random Patches\n[Louppe and Geurts, 2012] by combining the Random Sub-\nspace method and Online Bagging [Oza and Russell, 2001].\nSRP exploits the global subspace randomization (as in Ran-\ndom Subspace), while ARF takes advantage of local subspace\nrandomization (as in Random Forest). In SRP, the drift detec-\ntion and recovery strategy follows the procedure used in ARF.\nAn Online Deep Learning (ODL) framework has been pro-\nposed recently [Sahoo et al., 2017], which employs Hedge\nBackpropagation to overcome the slow convergence issue of\nDNNs in the online setting. However, ODL has no explicit\nmechanism to deal with changes in the data distribution, re-\nsulting in poor performance when concept drift occurs.\n2.3\nActive Learning with evolving data streams\nIn active learning setting for streaming data, the decision to\nrequest the true label for a data point must be made immedi-\nately when that instance arrives. Only a few active learning\nstrategies have been proposed for evolving data streams. In\n[ˇZliobait˙e et al., 2011], the ﬁrst theoretically supported ac-\ntive learning framework for instance-incremental streaming\ndata was introduced. The authors also proposed two novel\nactive learning strategies, namely Variable Uncertainty (VU)\nand Variable Randomized Uncertainty (VRU), that can han-\ndle concept drift explicitly. The VU strategy employs a vari-\nable threshold, which adjusts itself based on arriving data\npoints to align with the given budget.\nMeanwhile, in the\nVRU strategy, the labeling threshold is multiplied by a ran-\nARF Adaptive Random Forest\nUpdate Phase\nPrediction Phase\nLayer 1\nARF\nARF\nARF\nARF\nLayer 2\nARF\nARF\nARF\nARF\nLayer L\nARF\nARF\nARF\nARF\nAve.\nMax\ny\nx\nŷ\nFigure 1: Streaming Deep Forest\ndom variable that follows the normal distribution N(0, 1).\nThis strategy labels the data points that are close to the deci-\nsion boundary more often, but occasionally requests labels for\nsome distant instances. Cesa-Bianchi et al. developed an on-\nline active learning method, namely Selective Sampling (SS)\n[Cesa-Bianchi et al., 2006], using a variable labeling thresh-\nold b/(b+|p|), where b is a parameter, and p is the prediction\nof the perceptron. This method could be adapted to changes,\nalthough the authors did not explicitly handle concept drift.\nXu et al. employed a Paired Ensemble Framework to per-\nform active learning from evolving data streams [Xu et al.,\n2016]. In detail, an ensemble of two base learners is used to\npredict new instances and detect changes over time. Mean-\nwhile, two active learning strategies (Random strategy and\nVariable Uncertainty strategy) work alternatively to look for\nthe most informative instances.\n3\nProposed Methods\n3.1\nProblem setting\nLet a data stream X = {x1, x2, ..., x∞} be an inﬁnite se-\nquence of data points where xk is a d-dimensional vector of\nfeatures. Correspondingly, let Y = {y1, y2, ..., y∞}, yk ∈\n{l1, l2, . . . , lM} be the sequence of class labels, such that an\nentry yk in Y is the true label of xk in X. Most of the existing\nworks on data stream classiﬁcation assume that the true label\nyk is available before the next data point xk+1 arrives.\nFurthermore, we assume evolving data streams, in which\nconcept drifts may occur over time. The appearance of con-\ncept drifts inﬂuences the decision boundary and damages the\ncurrent learned model. Here, an i.i.d. assumption is made for\neach concept, i.e., each concept is treated as a separate i.i.d.\nstream. As a result, we have to deal with a series of i.i.d.\nstreams.\n3.2\nStreaming Deep Forest\nIt is widely acknowledged that the success of deep neural\nnetworks is attributed to its representation learning ability,\nwhich mostly relies on layer-by-layer processing of the fea-\ntures. Similarly, gcForest [Zhou and Feng, 2017] generates a\ndeep forest ensemble based on cascade structure to perform\nrepresentation learning. Speciﬁcally, each layer of gcForest\ntakes the output of its previous layer as the feature informa-\ntion and transmits its processing result to the next layer.\nUnder the data stream setting, we employ the cascade\nstructure of gcForest to retain the representation learning ca-\npability. However, we change the constituents and propose a\nnew training scheme to make the model able to learn incre-\nmentally from data streams. Figure 1 illustrates the proposed\nmethod, which we refer to as Streaming Deep Forest (SDF).\nEach layer is represented by an ensemble of Adaptive Ran-\ndom Forests (ARF), i.e., an ensemble of ensembles. Due to\nthe fact that ARF is an online classiﬁer, we can update all lay-\ners on the ﬂy and use them to make predictions at any time.\nIn addition, to promote diversity, a crucial factor in ensem-\nble learning [Zhou, 2012], we construct each layer using four\nARFs with different hyper-parameters.\nConsider an arbitrary data point, each ARF will output an\nestimate of the posterior distribution, which is the weighted\naverage across all trees’ class distribution. In more detail,\nARF employs the Hoeffding Tree [Domingos and Hulten,\n2000] algorithm with Na¨ıve Bayes classiﬁer at the leaves as\nthe base learner, which we call Hoeffding Na¨ıve Bayes Tree\n(HNBT). Note that in ARF, each base tree limits its splits to\nm(m < d) randomly selected features.\nThe posterior distribution given by each ARF forms a class\nvector. We then concatenate all these class vectors and the\noriginal feature vector to input to the next layer. Lets take\na problem that aims to classify 5D feature vectors into four\nclasses as an example. In this case, each of four ARFs out-\nputs a four-dimensional class vector; thus, the next layer will\nreceive 21(= 4 × 4 + 5) features.\nWhen the true label yk of the data point xk is revealed, we\nupdate each layer by using yk and the input vector (the con-\ncatenation of the previous layer’s outputs and xk). In each\nlayer, the update process can be easily parallelized since the\nfour ARFs are independently executed, and so are the base\ntrees of each ARF. In this work, we use the CPU multi-\nprocessor architecture to parallelize each layer of SDF.\nRegarding the issue of concept drift, SDF follows the active\nchange detection and recovery strategy used in ARF, which is\ndescribed in Sub-section 2.2\n3.3\nAugmented Variable Uncertainty Strategy\nWe study active learning for instance-incremental streaming\ndata, where concept drift is expected to occur. The true la-\nbel can be requested immediately or never, as the data points\nare discarded from memory after being used. The goal is to\nmaximize the prediction accuracy over time while keeping\nthe labeling cost ﬁxed within an allocated budget.\nGiven a data stream X = {x1, x2, ..., x∞}, we assume\nthat the labeling cost is the same for any data point. A budget\nB is imposed to request the true labels, i.e., the maximum\nfraction of the incoming data points that we can obtain the\ntrue labels. If B = 1, for example, all arriving data points are\nlabeled, whereas if B = 0.6, we can request the true labels of\nup to 60% of the arriving data points.\nIn this work, we improve the Variable Uncertainty (VU)\nstrategy [ˇZliobait˙e et al., 2011]. Here, the certainty is mea-\nsured by using the posterior probability estimates, i.e., the\nhigher the maximum of the posterior probabilities is, the more\ncertain the prediction is. This strategy tries to label the least\ncertain data points within a time period by using a variable\nAlgorithm 1 Active Learning with the AVU strategy\nInput: xk - incoming instance, B - budget, s - adjusting step\nOutput: label ∈{true, false} speciﬁes whether to query the\ntrue label yk\n1: Initialize labeling cost c = 0, labeling threshold θ = 1\n2: if (c/k < B) then {budget is not exceeded}\n3:\nˆp = maxy P(y|xk), y ∈{l1, l2, ..., lM}\n4:\nif ˆp < θ then {certainty below the threshold}\n5:\nc = c + 1; θ = θ(1 −s)\n6:\nreturn true\n7:\nelse {certainty is good}\n8:\nθ = θ(1 + s)\n9:\nGenerate a uniform random variable ρ ∈[0, 1]\n10:\nreturn ρ < 2 × (B −0.5)\n11:\nend if\n12: else {budget is exceeded}\n13:\nreturn false\n14: end if\ncertainty threshold, which adapts itself according to the ar-\nriving instances. Speciﬁcally, VU queries labels for the in-\nstances with their certainty scores below the variable thresh-\nold. In stable data concept (no change happens), the classiﬁer\nbecomes more conﬁdent about its predictions; thus, the cer-\ntainty threshold will grow to cover some high-certainty data\npoints. By contrast, if a concept drift occurs and lots of la-\nbeling requests suddenly appear, then the certainty threshold\nis contracted to be able to query labels for the most uncertain\ndata points ﬁrst.\nA problem with the VU strategy is that it does not take full\nadvantage of the given budget B. In Proposition 1, we show\nthat by following this strategy, we only spend a maximum\nbudget of 0.5 in expectation. As a consequence, when the\nbudget B > 0.5, it will miss out a fraction of about (B −0.5)\nof incoming instances that we can ask for their labels. To\naddress this issue, we proposed the Augmented Variable Un-\ncertainty (AVU) strategy in Algo 1. The difference between\nAVU and VU is that when “the certainty is good”, VU always\nrefuses to query labels, whereas AVU requests labels with a\nprobability P = 2 × (B −0.5). This allows AVU to take the\nfull advantage of the labeling budget B.\nRequesting labels when “certainty is good” is beneﬁcial in\nevolving data streams, as changes can happen everywhere in\nthe instance space. Thus, if we refuse to query labels for cer-\ntain data points, some regions will never be observed, and we\nnever know that concept drifts are occurring in those regions\nand, therefore, never adapt.\n4\nExperiments\nWe compared the parallel implementation of SDF against\nstate-of-the-art algorithms for evolving data streams, both\nconcerning prediction accuracy and CPU run time. We used\nthe test-then-train strategy, where each instance is ﬁrst used\nfor testing and then for training, to evaluate the accuracy of\neach classiﬁcation method. The benchmark algorithms used\nin the comparison were the Online Deep Learning (ODL)\nframework, Leverage Bagging (LB), Online Smooth Boost-\nTable 1: Datasets used in the experiments\nDataset\n# Instances\n# Classes\n# Features\nType\nDrifts\nAirlines\n539,383\n2\n7\nReal\n-\nCovtype\n581,012\n7\n54\nReal\n-\nAdult\n48,842\n2\n14\nReal\n-\nElectricity\n45,312\n2\n8\nReal\n-\nKDDCup99\n4,898,431\n23\n41\nReal\n-\nMnist a\n70,000\n10\n784\nReal\nA\nNomao\n34,465\n2\n118\nReal\n-\nVehicle\n98,528\n2\n100\nReal\n-\n20 newsgroups\n399,940\n2\n1000\nReal\n-\nAGR a\n1,000,000\n2\n9\nSynthetic\nA\nAGR g\n1,000,000\n2\n9\nSynthetic\nG\nBNG tic-tac-toe\n39,366\n2\n9\nSynthetic\nN\nBNG vote\n131,072\n2\n16\nSynthetic\nN\nBNG segment\n1,000,000\n7\n19\nSynthetic\nN\nHYPER\n1,000,000\n2\n10\nSynthetic\nF\nRBF f\n1,000,000\n5\n10\nSynthetic\nF\nRBF m\n1,000,000\n5\n10\nSynthetic\nM\nRTG\n1,000,000\n2\n10\nSynthetic\nN\nSEA a\n1,000,000\n2\n3\nSynthetic\nA\nSEA g\n1,000,000\n2\n3\nSynthetic\nG\n(A) Abrupt, (G) Gradual, (M) Incremental (moderate), (F) Incremental (fast), and (N) No drift.\ning (OSB), Adaptive Random Forest (ARF), and Stream-\ning Random Patches (SRP). These are recently proposed\nmethods that consistently outperform other classiﬁers, as\nshown by experiments in the literature [Gomes et al., 2017;\nGomes et al., 2019; Sahoo et al., 2017].\nTo evaluate the proposed active learning method AVU,\nwe compared it to three techniques: Variable Uncertainty\n(VU), Variable Randomized Uncertainty (VRU), and Selec-\ntive Sampling (SS). The ideas of these methods are brieﬂy\ndiscussed in Sub-section 2.3, and the implementations of\nthem are available in the MOA library1.\nRegarding the hyper-parameters, we used Hoeffding Tree\n(HT) as the base classiﬁer for all ensemble-based methods.\nThe number of layers of SDF was set to 3 when comparing\nto other benchmarks. Each layer contained 4 ARFs, each of\nwhich comprised of 50 base trees. We, therefore, used 200\nHTs as the base learners for other ensemble algorithms. We\nemployed ADWIN to be the drift detector for all ensemble\nmethods that rely on active drift detection (i.e., SDF, ARF,\nSRP, LB). We also incorporated ADWIN to ODL to help it\ndeal with concept drift. The conﬁdence bound δ of ADWIN\nwas set to δ = 10−4 for warning detection and δ = 10−5 for\ndrift detection in SDF, ARF, and SRP. In LB and ODL, δ was\nset to its default value δ = 0.002. In the AVU, VU, and VRU\nactive learning strategies, the adjusting step s was set to s =\n0.01 as used in [ˇZliobait˙e et al., 2011]. When comparing to\nother benchmark algorithms, we added a variant of SDF that\nfollows AVU active learning strategy with budget B = 0.7,\nwhich we refer to as SDF(B=0.7). Other hyper-parameters\nthat are not mentioned here were set to their default values,\nas shown in the original papers, and they can also be found in\nthe MOA library.\nWe conducted experiments on 20 datasets, including 11\nsynthetic data streams and 9 real-world datasets.\nThese\ndatasets have been extensively used in the data stream liter-\nature, containing concept drifts (gradual, abrupt, and incre-\nmental) and stationary streams. More details of these datasets\n1https://moa.cms.waikato.ac.nz\nTable 2: Test-then-train accuracy(%)\nARF\nSRP\nLB\nOSB\nODL\nSDF\nSDF\n(B=0.7)\nAirlines\n66.4646\n68.4972\n63.7109\n65.0028\n61.3000\n68.4934\n68.5680\nCovtype\n92.5220\n94.8010\n93.5106\n87.2538\n89.5800\n95.7142\n95.6591\nAdult\n83.9503\n84.6485\n84.2554\n83.4200\n76.0700\n84.6812\n84.7426\nElectricity\n89.0228\n89.4333\n88.4225\n88.3077\n73.8900\n91.3180\n91.0465\nKDDCup99\n99.9716\n99.9768\n99.9503\n99.8541\n99.9600\n99.9737\n99.9719\nMnist a\n91.5614\n84.4300\n60.9343\n28.7500\n88.8300\n93.1129\n93.5343\nNomao\n97.0985\n97.2813\n95.8741\n93.6980\n96.2000\n97.5337\n97.5018\nVehicle\n85.0652\n84.6856\n84.8957\n78.1057\n85.4000\n86.8271\n86.7388\n20 newsgroups\n99.6534\n99.7010\n99.4957\n98.7401\n99.5600\n99.7082\n99.7132\nAGR a\n90.7030\n93.0238\n89.8923\n93.0665\n60.8900\n94.7449\n94.8459\nAGR g\n87.0745\n89.4430\n86.7219\n90.4978\n60.0800\n91.5984\n91.7029\nBNG tic-tac-toe\n78.4103\n77.1884\n77.8565\n75.3340\n70.8400\n78.9615\n78.9920\nBNG vote\n96.9841\n96.8628\n96.9643\n96.6232\n96.6100\n97.1886\n97.1649\nBNG segment\n87.3781\n86.9372\n87.1968\n85.9996\n86.3600\n87.5945\n87.6018\nHYPER\n85.2711\n84.9455\n87.3113\n89.1766\n91.8000\n88.4881\n88.8437\nRBF f\n73.7903\n75.2759\n63.4204\n43.4631\n61.7800\n78.8835\n77.9373\nRBF m\n86.2290\n85.1323\n84.8242\n66.8997\n82.9800\n87.8207\n87.9004\nRTG\n94.0765\n90.7654\n97.8457\n94.6737\n82.6200\n98.0519\n97.9177\nSEA a\n89.6332\n88.2105\n86.9402\n88.9516\n86.6100\n89.7021\n89.7040\nSEA g\n88.9488\n87.4435\n88.4956\n88.4597\n85.6900\n89.0484\n89.0599\nAvg Rank Real\n4.11\n3.22\n5.33\n6.56\n5.44\n1.67\n1.67\nAvg Rank Synt.\n3.91\n4.91\n4.73\n4.91\n6.18\n1.91\n1.45\nAvg Rank\n4.00\n4.15\n5.00\n5.65\n5.85\n1.80\n1.55\nare shown in Table 1.\n4.1\nAugmented Variable Uncertainty strategy\nFirst, we designed an experiment to conﬁrm Proposition 1\nby examining the fraction of labeling requests of all active\nlearning methods on the Electricity dataset when the labeling\nbudget B > 0.5. Figure 3 shows the results for B = 0.7\nand B = 0.9. Clearly, in both cases, the labeling amount\nthat VU and VRU request quickly converges to 0.5, which is\nconsistent with our proposition. Note that we only considered\nVU in the proof, but it can be easily extended for VRU.\nFigure 4 shows the comparisons of AVU against other ac-\ntive learning strategies on the Electricity and Airlines datasets\ngiven different values of the budget. When B ≤0.5, the pro-\nposed method yields the same result as the VU method, and\nthe accuracy goes up when more budget is given. By contrast,\nin cases of B = 0.7 and B = 0.9, the performance of VU no\nlonger increases, while the performance of AVU keeps ris-\ning. This observation demonstrates that it is beneﬁcial to take\nfull advantage of the given budget. In comparison to SS and\nVRU, AVU completely outperforms them in almost all cases.\nThe only exception is the Electricity dataset with B = 0.1,\nwhere SS yields higher accuracy than AVU.\n4.2\nStreaming Deep Forest vs. Others\nTable 2 shows the accuracy of SDF, SDF(B=0.7) and other\nalgorithms on 20 datasets. Since some methods may per-\nform better on synthetic data while not so well in gen-\neral, we present both the average ranking for the real-world\ndatasets (Avg Rank Real) and the average ranking for the\nsynthetic datasets (Avg Rank Synt.) alongside the general\naverage ranking for all datasets (Avg Rank).\nThe result\nshows that SDF variants consistently rank ﬁrst on almost all\ndatasets (18/20) except for the KDDcup99 dataset and HY-\nPER dataset, where they still yield reasonable performance.\nAn interesting observation here is that SDF(B=0.7) achieves\nbetter average ranking than SDF though it queries only 70%\nof the true labels for training. On real datasets, they both\nobtain the best average ranking (1.67), whereas SDF(B=0.7)\nperforms slightly better than SDF on synthetic datasets.\nTo assess the statistical signiﬁcance of the comparisons, we\napply the Friedman test and the Nemenyi post-hoc test with\nthe signiﬁcance level α = 0.05 to evaluate multiple meth-\nods on multiple datasets [Demˇsar, 2006]. The Friedman test\nrejected the hypothesis that “all methods perform equally”.\nFigure 5 illustrates the results of the post-hoc tests regarding\nthe accuracy and the run time. In terms of accuracy, Figure 5a\nshows that the proposed methods (SDF and SDF(B=0.7)) sig-\nniﬁcantly outperform all the benchmark algorithms, while no\nsigniﬁcant difference has been found among ARF, SRP, LB,\nOSB, and ODL. Meanwhile, the run time of SDF is high in\ncomparison to other methods, as shown in Figure 5b, which is\nattributable to its multi-layer structure. Fortunately, by using\nactive learning, SDF(B=0.7) performs much faster than SDF\nand obtains comparable run time to LB and SRP.\n4.3\nEffect of hyper-parameters\nThe only hyper-parameter of SDF apart from those of ARF\nbase learner is the number of layers. Figure 2 shows the ac-\ncuracy of SDF when we vary this hyper-parameter from 1 to\n5. It is clear that SDF with only one layer performs much\nworse than that with more layers. In addition, there is an up-\nward trend in almost all datasets, meaning that adding more\nlayers tends to give better (or at least equal) accuracy. There-\nfore, we recommend using SDF with at least two layers when\nhaving low computing power; otherwise, use as many layers\nas the hardware can handle.\nIn the batch learning setting where the training time is un-\nlimited, the classiﬁcation model can be very deep with many\nlayers. However, in the stream setting, the model is expected\nto process instances at least as fast as new instances are avail-\nable. Thus, we only use up to ﬁve layers to align with our\ncomputation resources, but the proposed method can be di-\nrectly extended to many more layers when more computing\npower is available.\n5\nConclusions\nIn this work, we have adapted the gcForest model to the con-\ntext of evolving data streams and proposed the Streaming\nDeep Forest. In particular, we exploited the cascade struc-\nture of gcForest to retain its representation learning ability,\nchanged the base forest model at each layer to ARF, and em-\nployed an online training scheme to update SDF on the ﬂy.\nWe compared SDF to various state-of-the-art streaming clas-\nsiﬁcation methods over 20 datasets from both real-world ap-\nplications and synthetic data generators. We also proposed an\nactive online learning framework for evolving data streams,\nnamely Augmented Variable Uncertainty. Our experiments\nshowed that by following the AVU active learning strategy,\nSDF with only 70% of the true labels signiﬁcantly beats other\nbenchmark methods trained with all the true labels.\nIn future work, we will study how to make SDF a deeper\nmodel while keeping the run time reasonable by considering\nsparse structures. When having a very deep model, an online\nweighted scheme for the layers can be employed to reduce\nthe effort to tune the number of layers.\nAirlines\nCovtype\nAdult\nElectricity\nKDDCup99\nMnist_a\nNomao\nVehicle\n20_newsgroups\nAGR_a\nAGR_g\nBNG_tic-tac-toe\nBNG_vote\nBNG_segment\nHYPER\nRBF_f\nRBF_m\nRTG\nSEA_a\nSEA_g\n60\n65\n70\n75\n80\n85\n90\n95\n100\n1 layer\n2 layers\n3 layers\n4 layers\n5 layers\nFigure 2: Test-then-train accuracy of SDF using different numbers of layers\n1000\n10000\n20000\n30000\n40000 45312\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675\n0.700\nBudget B = 0.7\nLabeling requests by SS\nLabeling requests by VRU\nLabeling requests by VU\nLabeling requests by AVU\n1000\n10000\n20000\n30000\n40000 45312\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nBudget B = 0.9\nLabeling requests by SS\nLabeling requests by VRU\nLabeling requests by VU\nLabeling requests by AVU\n(a) Budget B = 0.7\n(b) Budget B = 0.9\nFigure 3: Electricity - Labeling costs over time\n0.1\n0.3\n0.5\n0.7\n0.9\nbudget used\n66.5\n67.0\n67.5\n68.0\n68.5\naccuracy\nVRU\nSS\nVU\nAVU\n(a) Airlines\n0.1\n0.3\n0.5\n0.7\n0.9\nbudget used\n84\n86\n88\n90\naccuracy\nVRU\nSS\nVU\nAVU\n(b) Electricity\nFigure 4: Accuracies given a budget. a Airlines. b Electricity\n1\n2\n3\n4\n5\n6\nCD\nSDF(B=0.7)\nSDF\nARF\nSRP\nLB\nOSB\nODL\n(a)\n2\n3\n4\n5\n6\nCD\nOSB\nARF\nODL\nLB\nSDF(B=0.7)\nSRP\nSDF\n(b)\nFigure 5: Nemenyi test (α = 0.05). a Accuracy. b Run time\nProposition 1. Given a data stream X = {x1, x2, ..., x∞},\na classiﬁer L, and a small positive number s (e.g. s = 0.01).\nLet uk be the certainty score of L on xk which lies in the\nrange [a, b](0 ≤a < b), and θ be a variable certainty thresh-\nold. Consider the following strategy:\n• Initialize the certainty threshold θ1 = b\n• For all k = 1, 2, ...\n– If uk < θk then θk+1 = θk(1 −s)\n– If uk ≥θk then θk+1 = θk(1 + s)\nLet ¯θk be the expectation of the threshold at the k-th instance.\nIf we follow the above strategy, then the probability P(uk <\n¯θk) converges to 0.5 when k approaches inﬁnity.\nProof. Assume the certainty score u is uniformly distributed\nfrom a to b: u ∼uniform(a, b), which means P(uk <\n¯θk) =\n¯θk−a\nb−a , and P(uk ≥¯θk) = b−¯θk\nb−a . Hence, the expecta-\ntion ¯θk+1 is:\n¯θk+1 = P(uk < ¯θk)× ¯θk(1−s)+P(uk ≥¯θk)× ¯θk(1+s)\n=\n¯θk −a\nb −a × ¯θk(1 −s) + b −¯θk\nb −a × ¯θk(1 + s)\n(1)\nTo prove P(uk\n<\n¯θk) −−−−→\nk→∞\n0.5, we will prove that\n¯θk −−−−→\nk→∞\na+b\n2 .\nFirst, we use induction to show that ¯θk ≥a+b\n2\nfor all k ≥1.\nAssume that ¯θk ≥a+b\n2 . From (1), the inequality ¯θk+1 ≥a+b\n2\nis equivalent to:\n[2¯θk −(a + b)][2s¯θk −(b −a)] ≤0\n(2)\nwhich is satisﬁed due to the small value of s and the induc-\ntion assumption. Hence, the inequality ¯θk+1 ≥a+b\n2\nis also\nsatisﬁed. By induction, we have:\n¯θk ≥a + b\n2\nfor all k ≥1\n(3)\nSecond, we show that {¯θk} is a decreasing sequence, or\n¯θk+1 ≤¯θk for all k = 1, 2, .... Substituting (1) to this in-\nequality, we have:\ns¯θk[−2¯θk + (a + b)] ≤0\n(4)\nwhich holds due to (3). Consequently, the inequality ¯θk+1 ≤\n¯θk holds. Combining with (3), we have {¯θk} is a decreasing\nand bounded below sequence. Therefore, it is converging.\nNow, let its limit be l = limk→∞¯θk. When k approaches ∞,\nwe have:\nl = ¯θk+1 = (l −a)l(1 −s)\nb −a\n+ (b −l)l(1 + s)\nb −a\nwhich is equivalent to l = a+b\n2 , or limk→∞¯θk = a+b\n2 . There-\nfore, P(uk < ¯θk) −−−−→\nk→∞0.5.\nReferences\n[Barandiaran, 1998] I˜nigo Barandiaran.\nThe random sub-\nspace method for constructing decision forests.\nIEEE\nTrans. Pattern Anal. Mach. Intell, 20(8):1–22, 1998.\n[Bifet and Gavalda, 2007] Albert Bifet and Ricard Gavalda.\nLearning from time-changing data with adaptive window-\ning. In Proceedings of the 2007 SIAM international con-\nference on data mining, pages 443–448. SIAM, 2007.\n[Bifet et al., 2010] Albert Bifet, Geoff Holmes, and Bern-\nhard Pfahringer.\nLeveraging bagging for evolving data\nstreams. In Joint European conference on machine learn-\ning and knowledge discovery in databases, pages 135–\n150. Springer, 2010.\n[Breiman, 1996] Leo Breiman. Bagging predictors. Machine\nlearning, 24(2):123–140, 1996.\n[Breiman, 2001] Leo Breiman. Random forests. Machine\nlearning, 45(1):5–32, 2001.\n[Cesa-Bianchi et al., 2006] Nicolo Cesa-Bianchi,\nClaudio\nGentile, and Luca Zaniboni. Worst-case analysis of selec-\ntive sampling for linear classiﬁcation. Journal of Machine\nLearning Research, 7(Jul):1205–1230, 2006.\n[Chen et al., 2012] Shang-Tse Chen, Hsuan-Tien Lin, and\nChi-Jen Lu. An online boosting algorithm with theoret-\nical justiﬁcations.\nIn Proceedings of the 29th Interna-\ntional Coference on International Conference on Machine\nLearning (ICML), pages 1873–1880. Omnipress, 2012.\n[Demˇsar, 2006] Janez Demˇsar.\nStatistical comparisons of\nclassiﬁers over multiple data sets.\nJournal of Machine\nlearning research, 7(Jan):1–30, 2006.\n[Domingos and Hulten, 2000] Pedro Domingos and Geoff\nHulten.\nMining high-speed data streams.\nIn Kdd, vol-\nume 2, page 4, 2000.\n[Freund and Schapire, 1997] Yoav Freund and Robert E\nSchapire. A decision-theoretic generalization of on-line\nlearning and an application to boosting. Journal of com-\nputer and system sciences, 55(1):119–139, 1997.\n[Gomes et al., 2017] Heitor M Gomes, Albert Bifet, Jesse\nRead, Jean Paul Barddal, Fabr´ıcio Enembreck, Bernhard\nPfharinger, Geoff Holmes, and Talel Abdessalem. Adap-\ntive random forests for evolving data stream classiﬁcation.\nMachine Learning, 106(9-10):1469–1495, 2017.\n[Gomes et al., 2019] Heitor M Gomes, Jesse Read, and Al-\nbert Bifet. Streaming random patches for evolving data\nstream classiﬁcation. In IEEE International Conference\non Data Mining. IEEE, 2019.\n[LeCun et al., 2015] Yann LeCun, Yoshua Bengio, and Ge-\noffrey Hinton.\nDeep learning.\nnature, 521(7553):436,\n2015.\n[Louppe and Geurts, 2012] Gilles Louppe and Pierre Geurts.\nEnsembles on random patches. In Joint European Confer-\nence on Machine Learning and Knowledge Discovery in\nDatabases, pages 346–361. Springer, 2012.\n[Oza and Russell, 2001] Nikunj C Oza and Stuart Russell.\nExperimental comparisons of online and batch versions of\nbagging and boosting. In Proceedings of the seventh ACM\nSIGKDD international conference on Knowledge discov-\nery and data mining, pages 359–364. ACM, 2001.\n[Sahoo et al., 2017] Doyen Sahoo, Quang Pham, Jing Lu,\nand Steven CH Hoi.\nOnline deep learning:\nLearn-\ning deep neural networks on the ﬂy.\narXiv preprint\narXiv:1711.03705, 2017.\n[Servedio, 2003] Rocco A Servedio. Smooth boosting and\nlearning with malicious noise. Journal of Machine Learn-\ning Research, 4(Sep):633–648, 2003.\n[Webb et al., 2016] Geoffrey I Webb, Roy Hyde, Hong Cao,\nHai Long Nguyen, and Francois Petitjean. Characteriz-\ning concept drift. Data Mining and Knowledge Discovery,\n30(4):964–994, 2016.\n[Xu et al., 2016] Wenhua Xu, Fengfei Zhao, and Zheng-\ncai Lu.\nActive learning over evolving data streams us-\ning paired ensemble framework.\nIn 2016 Eighth Inter-\nnational Conference on Advanced Computational Intelli-\ngence (ICACI), pages 180–185. IEEE, 2016.\n[Zhou and Feng, 2017] Zhi-Hua Zhou and Ji Feng. Deep for-\nest: Towards an alternative to deep neural networks. In\nProceedings of the Twenty-Sixth International Joint Con-\nference on Artiﬁcial Intelligence, IJCAI-17, pages 3553–\n3559, 2017.\n[Zhou, 2012] Zhi-Hua Zhou.\nEnsemble methods: founda-\ntions and algorithms. Chapman and Hall/CRC, 2012.\n[ˇZliobait˙e et al., 2011] Indr˙e ˇZliobait˙e, Albert Bifet, Bern-\nhard Pfahringer, and Geoff Holmes.\nActive learning\nwith evolving streaming data. In Joint European Confer-\nence on Machine Learning and Knowledge Discovery in\nDatabases, pages 597–612. Springer, 2011.\n",
  "categories": [
    "cs.LG",
    "stat.ML",
    "I.2.m"
  ],
  "published": "2020-02-26",
  "updated": "2020-02-26"
}