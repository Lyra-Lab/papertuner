{
  "id": "http://arxiv.org/abs/2002.12177v1",
  "title": "Evolving Losses for Unsupervised Video Representation Learning",
  "authors": [
    "AJ Piergiovanni",
    "Anelia Angelova",
    "Michael S. Ryoo"
  ],
  "abstract": "We present a new method to learn video representations from large-scale\nunlabeled video data. Ideally, this representation will be generic and\ntransferable, directly usable for new tasks such as action recognition and zero\nor few-shot learning. We formulate unsupervised representation learning as a\nmulti-modal, multi-task learning problem, where the representations are shared\nacross different modalities via distillation. Further, we introduce the concept\nof loss function evolution by using an evolutionary search algorithm to\nautomatically find optimal combination of loss functions capturing many\n(self-supervised) tasks and modalities. Thirdly, we propose an unsupervised\nrepresentation evaluation metric using distribution matching to a large\nunlabeled dataset as a prior constraint, based on Zipf's law. This unsupervised\nconstraint, which is not guided by any labeling, produces similar results to\nweakly-supervised, task-specific ones. The proposed unsupervised representation\nlearning results in a single RGB network and outperforms previous methods.\nNotably, it is also more effective than several label-based methods (e.g.,\nImageNet), with the exception of large, fully labeled video datasets.",
  "text": "Evolving Losses for Unsupervised Video Representation Learning\nAJ Piergiovanni, Anelia Angelova, Michael S. Ryoo\nResearch at Google\n{ajpiergi,anelia,mryoo}@google.com\nAbstract\nWe present a new method to learn video representations\nfrom large-scale unlabeled video data. Ideally, this rep-\nresentation will be generic and transferable, directly us-\nable for new tasks such as action recognition and zero or\nfew-shot learning. We formulate unsupervised representa-\ntion learning as a multi-modal, multi-task learning prob-\nlem, where the representations are shared across different\nmodalities via distillation. Further, we introduce the con-\ncept of loss function evolution by using an evolutionary\nsearch algorithm to automatically ﬁnd optimal combina-\ntion of loss functions capturing many (self-supervised) tasks\nand modalities. Thirdly, we propose an unsupervised rep-\nresentation evaluation metric using distribution matching\nto a large unlabeled dataset as a prior constraint, based\non Zipf’s law. This unsupervised constraint, which is not\nguided by any labeling, produces similar results to weakly-\nsupervised, task-speciﬁc ones. The proposed unsupervised\nrepresentation learning results in a single RGB network and\noutperforms previous methods. Notably, it is also more ef-\nfective than several label-based methods (e.g., ImageNet),\nwith the exception of large, fully labeled video datasets.\n1. Introduction\nVideo representation learning is an important problem\nwhich beneﬁts high-level perception tasks including action\nrecognition and video object detection [39, 6, 40]. It has\nmany key applications, such as web-video retrieval, robot\nperception, and smart homes and cities. However, learn-\ning visual representations generally requires a large num-\nber of labeled training examples. This is even more so for\nvideos, as videos are higher-dimensional input than images\nand video CNNs have more learnable parameters than 2D\nones. Simultaneously, videos are more expensive to collect\nand annotate than images, as they require additional, and\noften ambiguous, temporal annotations [34]. Additionally,\nin rare event detection, very few examples may be available\nto begin with. Thus, obtaining a good video representation\nwithout relying on domain speciﬁc, annotated video sam-\nRepresentation space \nfrom unlabeled data\nFuture Prediction\nReconstruction\nFrame Order Detection\n=\n=\nMultiple \nself-supervised tasks\n...\nIn order\nOut of order\n...\nAction/event \nclusters\nVideo\nCNN\nFigure 1:\nOverview of our unsupervised representation\nlearning framework. The objective is to obtain a good rep-\nresentation (blue outlined box) from a set of self-supervised\ntasks. We use an evolutionary algorithm to automatically\nﬁnd the optimal combination of tasks and power law distri-\nbution matching to ‘supervise’ the clustering and guide the\nevolution. No labeling or supervision is needed.\nples, has signiﬁcant impact on real-world scenarios where\nlarge-scale video data curation and labeling is prohibitive.\nIn this paper, we present a new, principled method for\nunsupervised learning of video representations from unla-\nbeled video data. It is based on the observation that the\noptimized combination of multiple self-supervised tasks1,\nwhich are additionally encouraged by multi-modal distil-\nlation, is often sufﬁcient to learn good feature representa-\ntions. Importantly, we demonstrate that such combination\ncould be found without per-class or per-video labeling by\ninstead matching the representation statistics to a general\npower distribution of video classes, e.g., Zipf’s law [49].\nOur approach is to train the network so that its in-\ntermediate representations reﬂect not just information di-\nrectly obtained from its own input modality (e.g., RGB im-\nage) but also information from different modalities (e.g.,\n1In this work we use unsupervised and self-supervised interchangeably.\n1\narXiv:2002.12177v1  [cs.CV]  26 Feb 2020\n...\n...\nTask 1\nTask 2\nTask N\n...\nTask 1\nTask 2\nTask N\n...\nTask 1\nTask 2\nTask N\nOptical Flow\nRGB\nAudio\nDistillation\nLosses\nEaudio\nxR\nxF\nxA\nλ1,1L1,1\nλ1,2L1,2\nλ1,NL1,N\nλ2,1L2,1\nλ2,2L2,2\nλ2,NL2,N\nλM,1LM,1\nλM,2LM,2\nλM,NLM,N\n...\n...\nλd\nFigure 2: The multi-task, multi-modal, unsupervised representation learning framework. Each modality is trained to opti-\nmize a set of tasks. Distillation regularization loss terms ‘infuse’ each modality’s information into the main RGB network\n(drawn center). We evolve the loss function to automatically ﬁnd the weights for each task and distillation location, via an\nunsupervised objective. The goal is to obtain representation from a single RGB network that transfers to recognition tasks.\ngrayscale, optical ﬂow, and audio). The idea is that synchro-\nnized multi-modal data sources should beneﬁt representa-\ntion learning of each other as they correspond to the same\ncontent. This is done by the introduction of ‘distillation’\n[16] losses between multiple streams of networks. The dis-\ntillation losses, as well as the self-supervised tasks, do not\nrely on human annotation or supervision. As a result, our\napproach is formulated as a multi-modal, multi-task unsu-\npervised learning, where the tasks include single-modality\ntasks like frame ordering as well as multi-modality tasks\nlike video-audio alignment.\nHowever, combining multiple different self-supervised\ntask losses and distillation losses for unlabeled representa-\ntion learning is a challenging problem, as certain tasks and\nmodalities are more relevant to the ﬁnal task than others\nand different loss functions have different scales. Thus, we\nnewly introduce the concept of using an evolutionary algo-\nrithm to obtain a better multi-modal, multi-task loss func-\ntion that appropriately combines all the losses to train the\nnetwork. AutoML has successfully been applied to archi-\ntecture search [25] and data augmentation [8]. Here we ex-\ntend this concept to unsupervised learning by automatically\nﬁnding the weighting of self-supervised tasks for video rep-\nresentation learning. The ‘ﬁtness’ of this evolution could\nnaturally be measured with task speciﬁc labels (e.g., ac-\ncuracy). However, we instead propose a purely unsuper-\nvised alternative based on power law distribution matching\nbetween the datasets, by using KL divergence constraints.\nThese constraints do not require any labeled data, enabling\nfully unsupervised and unlabeled learning.\nOur goal is to ﬁnd video feature representations, based\non a single RGB network, that can seamlessly improve su-\npervised or unsupervised tasks without additional annota-\ntions. The main contributions are:\n• Formulation of unsupervised learning as multi-modal,\nmulti-task learning, including distillation tasks to transfer\nfeatures across modalities into a single-stream network.\nOnce learned, it allows for faster representation compu-\ntation while still capturing multi-modal features.\n• Evolutionary search for a loss function that automatically\ncombines self-supervised and distillation tasks that are\nbeneﬁcial for unsupervised representation learning.\n• Introduction of an unsupervised representation evalua-\ntion metric based on power law distribution matching,\nwhich requires no labels and performs similarly to the\nlabel-guided one.\nThis work makes the surprising ﬁnding that large\namounts of unlabeled data, combined with self-supervised\ntasks and the power law distribution matching, produces\nvery powerful feature representations which are only ri-\nvaled by large datasets with very extensive data labeling:\nOur feature representations (obtained with zero labels) out-\nperform ImageNet pre-training, and pre-training on small\nand medium-size labeled video datasets; it is only outper-\nformed by Kinetics pre-training with full annotations based\non human labeling of more than 200,000 videos. Further,\nthe proposed representations outperform Kinetics training,\nwhen ﬁne-tuned with Kinetics labels. We refer to the model\nas ‘ELo’, as it is based on evolving unsupervised losses.\n2. Related Work\nUnsupervised Video Representation Learning: Ob-\ntaining labeled video data is expensive, and unlabeled video\ndata is plentiful, and there have been many methods pro-\nposed for self-supervised learning of video representations.\nSome tasks take advantage of the temporal structure in\nvideos, such as predicting if frames appear in order, re-\nverse order, shufﬂed, color-consistency across frames, etc.\n[12, 26, 30, 24, 31, 46, 19, 21, 44, 45, 41]. Other work has\nexplored using the spatial structure present in images, such\nas predicting relative spatial location of image patches [28]\nor tracking patches over time [43], showing promising re-\nsults. Reconstruction or prediction of future frames[37], or\ntime-contrastive learning[18, 33] to obtain representations\nhas also been successful. Learning representations taking\nadvantage of audio and video features has been explored by\npredicting if an audio clip is from a video [2] or if audio and\nvideo are temporally aligned [29, 7, 22, 3].\nMulti-task self-supervised learning has also shown\npromising results [10, 32, 47], where tasks are assumed\nto have equal weights and are not multi-modal.\nGener-\nating weak labels using k-means clustering on CNN fea-\ntures [4, 5] or using clustering with meta-learning [17] has\nalso been explored. In this paper, we propose a generalized\napproach to unsupervised representation learning, allowing\nfor multi-modal inputs and automatic discovery of the tasks\nthat beneﬁt recognition performance.\nActivity Recognition: Activity recognition is a active\narea of vision research, with a variety of methods pro-\nposed [42, 39, 35, 11]. With the introduction of large ac-\ntivity recognition datasets (e.g., Kinetics and Moments in\nTime [20, 27]), much more accurate deep video CNNs are\npossible [6]. We here show that they can be further im-\nproved by unsupervised representation learning.\n3. Method\nWe formulate unsupervised video representation learn-\ning as a combination of multi-task, multi-modal learning.\nThe objective is not only to take advantage of multiple self-\nsupervised tasks for the learning of a (good) representation\nspace, but also to do so across multiple modalities. The idea\nis that models from synchronized multi-modal data, sharing\nthe same semantic content, will beneﬁt the representation\nlearning of each other. We encourage that via introducing\n‘distillation’ losses. At the same time, each modality may\nhave multiple self-supervised tasks with their corresponding\nlosses. Fig. 2 illustrates the multi-task, multi-modal formu-\nlation with multiple losses and distillation, Section 3.1 has\nthe details.\nTo facilitate multi-task, multi-modal learning, impor-\ntantly, we introduce the new concept of automatically evolv-\ning the main loss function. Certain tasks and modalities are\nmore relevant to the ﬁnal task so the representation needs\nto focus on those more than the others. The idea is to com-\nputationally search for how different multi-task and distil-\nlation losses should be combined, instead of constructing\na loss function by trial-and-error. We discuss this more in\nSection 3.2.\nOne key technical question is how one can guide the evo-\nlution without a pre-deﬁned task or a ﬁtness function. We\npropose an unsupervised method to evaluate each loss func-\ntion, based on matching of the power law distribution of\nactivity classes (Section 3.2.2).\n3.1. Unsupervised multi-modal learning\nWe construct a CNN for each modality. Each network is\ntrained using several tasks not requiring labeled video data,\nand the information from each modality is combined using\ndistillation [16] (Fig. 2). More speciﬁcally, we take advan-\ntage of multiple self-supervised tasks, such as frame recon-\nstruction, future frame prediction, and frame temporal or-\ndering (Section 3.3 discusses them in detail). Each of these\ntasks will yield an unsupervised loss for training. Learning\nwith multiple self-supervised tasks makes our representa-\ntions more generic, as they need to generalize to many tasks\nand are more transferable to unseen tasks.\nFor each modality, m and its input Im, we build an em-\nbedding network, Em, which generates an embedded rep-\nresentation of the input: xm = Em(Im). xm is the fea-\nture representation for modality m. Our embedding net-\nworks are (2+1)D ResNet-50 models, which take advantage\nof both 2D spatial convolutions and 1D temporal convolu-\ntions to represent videos; they provide state-of-the-art per-\nformance on video understanding tasks. As mentioned, for\neach modality, we consider several learning tasks, for ex-\nample, frame reconstruction. Each of the tasks per modality\nhas its own loss function. Lm,t is the loss from task t and\nmodality m and {t1, t2..., tNm} is the set of tasks for the\nmodality.\nFurther, to better take advantage of the multi-modal rep-\nresentations, we use distillation to ‘infuse’ the other modal-\nities into the RGB network at different locations. Our ﬁnal\nobjective is to train a single RGB network that provides a\nstrong representation for video understanding. Our formu-\nlation allows the RGB network to learn representations from\nvarious tasks and modalities.\nWe combine the multi-task losses for unsupervised train-\ning, and per each modality, by a weighted sum, and we\nfurther combine it with a number of distillation losses Ld\nwhich fuse or synchronize the multiple modalities:\nL =\nX\nm\nX\nt\nλm,tLm,t +\nX\nd\nλdLd\n(1)\nwhere λm,t and λd are the weights of the losses.\nThe\nweighted sum L is the loss we use to train the entire model.\nEvolution Iterations\nFigure 3: Evolution of the weights deciding our ﬁnal loss function. Each square represents a λm,t and how it changes\nover the evolutionary search.\nThe weight symbols are as follows: the ﬁrst letter is representation modality (R=RGB,\nA=Audio, F=Flow, G=Grey), The tasks are S=Shufﬂe, C=colorize, A=Audio align, P=Future prediction, B=backward detec-\ntion, D=Distill, E=Embed. The numbers indicate the layer the distillation loss is applied.\n3.1.1\nDistillation\nDistillation was introduced to train smaller networks by\nmatching representation of deeper ones [16], or for gen-\nerally transferring knowledge from pre-trained networks.\nHere, we use distillation to ‘infuse’ representations of dif-\nferent modalities into the main, RGB network. Note that we\ndistill representations jointly while training. The distilla-\ntion losses learn features by transferring information across\nmodalities. More speciﬁcally, our formulation allows for\nthe distillation of audio, optical ﬂow and temporal informa-\ntion into a single, RGB-based convolutional neural network.\nThe distillation loss is the L2 difference between the ac-\ntivations of a layer in the main network Mi and a layer in\nanother network Li. Such constraint encourages the activa-\ntions of the main network to match the activations of the\nother network, infusing other features into the main net-\nwork.\nLd(Li, Mi) = ||Li −Mi||2\n(2)\nDistillation has previously been used for combining net-\nworks such as ensembles [16] or learning to predict optical\nﬂow features from RGB [38], here we are extending its us-\nage for multi-modal representation learning from unlabeled\nvideo data. While in principle distillation can happen across\nall modalities, we do distillation only into the RGB stream,\nso as to obtain a ﬁnal single-tower efﬁcient representation\nfor learning subsequent tasks. Using the learned weights for\nthe RGB network, we can then extract representations for a\nset of videos.\n3.2. Evolving an unsupervised loss function\nOur representation learning is governed by the weight\ncoefﬁcients of the loss in Equation 1, and they need to be ap-\npropriately determined. The weighting supposedly reﬂects\nthe importance or relevance of each task and modality to\nthe main task; for example the optical ﬂow modality may\nbe important for tracking, whereas audio may give more\ninformation for temporal segmentation of videos in certain\nsettings.\n3.2.1\nUnsupervised loss construction\nInstead of manually constructing a loss function, we evolve\nthe loss function by taking advantage of well-established\nevolutionary algorithms, e.g., [13]. More speciﬁcally, our\nsearch space consists of all the weights of the loss function,\nboth task weights and distillation weights. Each λm,t or\nλd is constrained to be in [0, 1]. Our evolutionary algorithm\nmaintains a pool (i.e., population) of individuals where each\nindividual is a set of weight values that compose the ﬁnal\nloss function.\n3.2.2\nUnsupervised Zipf distribution matching\nThe evolutionary algorithm requires evaluation of the loss\nfunction (i.e., ﬁtness measure) at each round to optimize the\nloss weight coefﬁcients. We propose a new, unsupervised\nmethod for this. In order to measure the ﬁtness of each in-\ndividual (i.e., the set of weights to combine the tasks and\nmodalities to form the ﬁnal loss), we apply a k-means clus-\ntering on the representation learned with the correspond-\ning loss function, and analyze the cluster distributions. We\nﬁrst train the network using a smaller subset (100k) of unla-\nbeled, random YouTube videos for 10,000 iterations (using\nthe corresponding loss function). We then use a subset of\nrandom YouTube videos and similarly extract representa-\ntions xRGB = ERGB(I). Given these representations, we\ncan then cluster them into k clusters.\nk-means clustering can be viewed as a Gaussian Mix-\nture Model with ﬁxed variances and we calculate probabili-\nties of each feature vector belonging to a cluster, which re-\nduces to computing the distance. Speciﬁcally, for the cluster\ncentroids {c1, c2, . . . ck} where ci ∈RD (a D-dimensional\nvector), we can compute the probability of a feature vector\nx ∈RD belonging to a cluster ci as:\np(x|ci) =\n1\n√\n2σ2π\nexp\n\u0012\n−(x −ci)2\n2σ2\n\u0013\n(3)\nSince we are (naively) assuming all clusters have the same\nvariance (for simplicity, let 2σ2 = 1) and an equal prior\nover all clusters, we can use Bayes rules to rewrite as:\np(ci|x) =\np(ci)p(x|ci)\nPk\nj p(cj)p(x|cj)\n=\nexp −(x−ci)2\n2σ2\nPk\nj=1 exp −(x−cj)2\n2σ2\n=\nexp −(x −ci)2\nPk\nj=1 exp −(x −cj)2\n(4)\nwhich we note is the standard softmax function applied to\nthe squared distances from a feature x to a cluster center ci.\nAs observed in many large activity recognition datasets,\nlike AVA [14] and Kinetics [20], the activity classes of\nvideos follow a Zipf distribution. We can use this as a prior\nconstraint on the distribution of the videos in these clusters.\nSpeciﬁcally, given the above probability of each video be-\nlonging to each cluster, and the Zipf distribution, we com-\npute the prior probability of each class as q(ci) =\n1/is\nHk,s\nwhere H is the kth harmonic number and s is some real\nconstant. We then let p(ci) = 1\nN\nP\nx∈V p(ci|x), the average\nover all videos in the set. Using these two probability func-\ntions representing the classes/clusters, we can minimize the\nKL divergence:\nKL(p||q) =\nk\nX\ni=1\np(ci) log\n\u0012p(ci)\nq(ci)\n\u0013\n(5)\nBy using this as a ﬁtness metric, it poses a prior con-\nstraint over the distribution of (learned) video representa-\ntions in clusters to follow the Zipf distribution. Note that\nthis method requires no labeled data and is fully unsuper-\nvised.\nWe refer to this entirely unsupervised method as\n‘ELo’.\nWeakly-supervised baseline:\nAs an upper-bound and an\nalternative to our approach, we use a handful of class la-\nbels to evaluate the clustering (which is referred to as ELo-\nweak).\nThis is done for the sake of comparison and is\nalso a good alternative to align the ﬁnal loss to a down-\nstream video classiﬁcation task. A subset of HMDB is used,\nand k-means clustering is applied to the output representa-\ntion of the RGB stream. The clusters are used for nearest-\nneighbors classiﬁcation and the accuracy is the ﬁtness of\nthe individual. Due to randomness in k-means clustering,\nin both settings, we run this processes 20 times and average\nﬁtness across all trials.\n3.2.3\nLoss evolution\nAs is typical with evolution approaches, the evolution of\nthe loss is driven by mutations. Since our search space con-\nsists of continuous values in [0, 1], we compare two differ-\nent evolutionary strategies: tournament selection [13] and\nCMA-ES [15].\nFor the tournament selection search, we\nmutate an individual loss function by randomly picking one\nAligned \nAudio\nRGB\n1\n0\nMisaligned\nAudio\nFigure 4: Example of the multi-modal alignment tasks. The\nnetworks take input of temporally aligned RGB and Audio\n(or other modalities) and a sample from one modality that\nis temporally different. The network is trained to predict if\na pair is temporally aligned or not.\nweight and assigning new value in uniformly sampled from\n[0, 1]. For CMA-ES, at each iteration, all the components\nare changed based on the ﬁtness of all the individuals in\nthe evolution pool. For tournament selection, we evolve the\nloss function for 2000 rounds, generating and evaluating\n2000 different loss functions and we use 250 rounds with\nCMA-ES, ﬁnding much faster convergence. Fig. 3 shows\nan example how our weights evolve over the rounds and\nTable 4 compares the performance of different search meth-\nods. Since everything is differentiable, we could learn these\nweights also with gradient descent, however, we leave this\nfor future exploration as taking the derivative of the entire\nnetwork w.r.t. to task weights is non-trivial.\n3.3. Self-supervised tasks\nMany tasks have been designed for unsupervised learn-\ning. We describe brieﬂy the tasks we employ for our rep-\nresentation learning. Importantly, we allow many possible\ntasks and let the evolved loss function automatically dis-\ncover which tasks are important and the optimal relative\nweightings. We also use tasks like DeepCluster [5] and lo-\ncal aggregation [48].\nReconstruction and prediction tasks: Given the repre-\nsentation for a modality, xm, we use a decoder CNN to gen-\nerate the output. As the reconstruction is only used as a su-\npervision signal, we do not need to generate extremely high\nquality reconstructions. Thus, we use a small, cheap de-\ncoder with only 6 convolutional layers, saving both memory\nand training time. Once the unsupervised learning is com-\nplete, we discard the decoders. Further, following previous\nwork [38], our decoders have no temporal convolution, forc-\ning the representations to contain all needed temporal infor-\nmation, which is desirable for video understanding. Each\nmodality (e.g., RGB, optical ﬂow, and audio) will be recon-\nstructed. We also use several cross-modality transfer tasks:\nRGB to Flow, Flow to RGB, etc.\nAnother way to learn video temporal structure is to train\na decoder to predict the next N frames given T frames. Fu-\nture prediction of frames has previously been used for rep-\nresentation learning [37] and we perform this task for each\nmodality. For these tasks, we minimize the L2 distance be-\ntween the ground truth (I) and predicted output (ˆI):\nLR(ˆI, I) = ||ˆI −I||2\n(6)\nTemporal ordering: We use two tasks to learn repre-\nsentations that take advantage of temporal structure: binary\nclassiﬁcation of ordered frames and shufﬂed frames [26]\nand binary classiﬁcation of forward and backward videos\n[31]. We use a single, fully-connected layer to make a bi-\nnary classiﬁcation of the representation: p = Wxm (xm\nis the representation for a modality). These are trained to\nminimize binary cross-entropy:\nLB(p, y) = −(y log(p) + (1 −y) log(1 −p))\n(7)\nwhere p is the output of the binary classiﬁer and y is the\nground truth.\nMulti-modal contrastive loss: As videos contain multi-\nple modalities, we want to take advantage of these different\nrepresentations to learn an a generic representation by using\na multi-modal embedding space. Given the representations\nfor each modality, xm, we minimize a contrastive loss be-\ntween various embedding spaces:\nLc(x1, x2, xn) = ||x1 −x2||2 + max(0, α −||x1 −xn||2)\n(8)\nwhere xm1 and xm2 are representations from the same\nvideo but different modalities and xn is a representation\nfrom a different video. This task encourages the represen-\ntations from the same video, but different modalities, to\nbe close in the representation space, while representations\nfrom different videos are further apart.\nMulti-modal alignment: We can further take advantage\nof both the temporal information and the multi-modal data\nby performing a multi-modal alignment task, illustrated in\nFig. 4. The networks take input of temporally aligned sam-\nples from two modalities, and a sample from one modalitiy\nfrom a temporally different region. The model is trained to\nmake a binary prediction if the two samples are temporally\naligned.\n4. Experiments\n4.1. Datasets\nUnsupervised data source.\nWe use two million random,\nunlabeled YouTube video clips sampled randomly from the\nYoutube-8M dataset [1] (limiting the size for computational\nreasons). Previous works on self-supervised learning used\nvideos from datasets (e.g., Kinetics or AudioSet in [22])\nwhile ignoring the labels, leading a bias in the dataset, as\nMethod\nk-means\n1-layer\nﬁne-tune\nSupervised using additional labeled data\nScratch (No Pretraining)\n15.7\n17.8\n35.2\nImageNet Pretrained\n32.5\n37.8\n49.8\nKinetics Pretrained\n68.8\n71.5\n74.3\nUnsupervised using unlabeled videos\nFrame Shufﬂe [26]\n22.3\n24.3\n28.4\nReverse Detection [31]\n21.3\n24.3\n27.5\nAudio/RGB Align [29, 22]\n32.4\n36.8\n40.2\nRGB to Flow\n31.5\n36.4\n39.9\nPredicting 4 future frames\n31.8\n35.8\n39.2\nJoint Embedding\n29.4\n32.5\n38.4\nOurs, weakly-sup clustering, using unlabeled videos\nEvolved Loss - ELo-weak\n45.7\n64.3\n67.8\nOurs, unsupervised, using unlabeled videos\nRandom Loss (unsup.)\n26.4\n26.9\n31.2\nEvolved Loss - ELo (unsup.)\n43.4\n64.5\n67.4\nTable 1: Evaluation of various self-supervised methods on\nHMDB51 [23]. We compare to a randomly initialized, Ima-\ngeNet pretrained and Kinetics pretrained networks. We also\ncompare to various single-task baselines, an average of 10\nrandomly sampled loss functions, and the evolved loss func-\ntion using both ﬁtness metrics. All tasks were trained on our\nrandom, unlabeled YouTube videos.\nthose videos are trimmed to intervals with speciﬁc activi-\nties. Using a random sample from Youtube is less prone to\nbias as the videos are user generated, the labels are automat-\nically tagged (no human veriﬁcation), and potentially offers\na very large set for training (up to 8M). We have veriﬁed\nthere is no overlap between those datasets and the ones the\nmodels are evaluated on (e.g., HMDB).\nEvaluation datasets and protocols. We used the fol-\nlowing widely used datasets for evaluation: HMDB [23],\nUCF101 [36], Kinetics [20]. We also used Imagenet [9]\nand Kinetics for reporting results with pre-training, as is\ncustomary in previous work. We use the standard protocols\nwhen evaluating video classiﬁcation results on the labeled\ndatasets, adopted by prior work as well. Please see the sup.\nmaterial for dataset details.\nImplementation details. We use a (2+1)D ResNet-50\nas our backbone network. Given a loss function, we train\nthe network for 100 epochs on 2 million unlabeled videos.\nThe learning rate is set to 0.1 (during both the evolution and\nthe ﬁnal training). We use cosine learning rate decay with a\nwarmup period of 2 epochs.\nDuring search, we used smaller networks, similar to a\nResNet-18 for faster learning. For search, the ﬁtness of each\nmodel can be found in 4 hours using 8 GPUs. The ﬁnal\nmodel uses 64 GPUs for 3 days to train (equivalent time to\ntraining I3D/(2+1)D ResNet-50 on Kinetics).\nMethod\nHMDB\nUCF101\nSupervised\n(2+1)D ResNet-50 Scratch\n35.2\n63.1\n(2+1)D ResNet-50 ImageNet\n49.8\n84.5\n(2+1)D ResNet-50 Kinetics\n74.3\n95.1\nUnsupervised\nShufﬂe [26]\n18.1\n50.2\nO3N [12]\n32.5\n60.3\nOPN [24]\n37.5\n37.5\nPatch [43]\n-\n41.5\nMultisensory [29]\n-\n82.1\nAVTS [22]\n61.6\n89.0\nWeakly guided, HMDB\nEvolved Loss (ours)\n67.8\n94.1\nUnsupervised\nEvolved Loss (ours, no distiliation)\n53.7\n84.2\nEvolved Loss - ELo (ours)\n67.4\n93.8\nTable 2: Comparison to the state-of-art on HMDB51 and\nUCF101. Note that previous approaches train on activity\nrecognition datasets (e.g., Kinetics) are much more aligned\nto the ﬁnal task, whereas we use random video clips. Even\nusing more difﬁcult data, we outperform the previous meth-\nods. (The top portion shows results for (2+1)D ResNet-50\nwith supervised pretraining as in Table 1).\n0\n56k\n112k\n168k\n225k\nNumber of Labeled Training Samples\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy\nRandomly Initialized\nOurs (unsupervised)\nAVTS\nShuffle\nSupervised Baseline\nFigure 5: How much labeled, supervised data is needed\nonce the unsupervised representation is learned. We achieve\ncomparable performance with roughly half the data and out-\nperform the supervised baselines using the entire dataset.\n4.2. Comparison to previous methods\nWe evaluate our method in comparison to prior unsuper-\nvised and supervised representation learning. Speciﬁcally,\nwe evaluate the representations in 3 settings: (1) k-means\nclustering of the representations (2) ﬁxing the weights of\nthe network and training a single, fully-connected layer for\nclassiﬁcation and (3) ﬁne-tuning the entire network. These\n100k\n500k\n1m\n1.5m\n2m\nNumber of Unlabeled Samples\n0\n10\n20\n30\n40\n50\n60\n70\nHMDB Accuracy\nOurs\nRandom Initialized Baseline\nImageNet-Pretrained Baseline\nKinetics-Pretrained Baseline\n100k\n500k\n1m\n1.5m\n2m\nNumber of Unlabeled Samples\n0\n10\n20\n30\n40\n50\n60\n70\nHMDB Accuracy\nOurs\nRandom Initialized Baseline\nImageNet-Pretrained Baseline\nKinetics-Pretrained Baseline\nFigure 6: Comparisons of different amounts of unsuper-\nvised data. Left: Total number of training iterations ﬁxed\n(i.e., less epochs as data is added). Right: Total number of\nepochs ﬁxed (i.e., more iterations as more data is added).\nWe observe that adding more data without increasing train-\ning time improves performance, while training longer on\nmore data is better. On HMDB.\nthree evaluations are done by directly evaluating the repre-\nsentation as well as ﬁnetuning the entire network.\nWe ﬁnd that while all approaches outperform the ran-\ndomly initialized networks, only our evolved loss function\noutperforms ImageNet pretraining and performs compara-\nbly to the pretrained network with labeled Kinetics data (Ta-\nble 1). Furthermore, we outperform all prior unsupervised\nmethods. Our approach performs similarly to the weakly\nsupervised version of our evolution method, despite being\nunsupervised. We also compare to a loss function randomly\nsampled from our search space, which performs poorly.\nWe ﬁnd that some tasks are not beneﬁcial to representation\nlearning, thus the evolution is quite important as this allows\nautomatically ﬁnding the best tasks and weightings.\nIn Table 2, we compare our approach to previous re-\nported methods. We ﬁnd that even though our approach is\nusing more difﬁcult unlabeled data, we still outperform the\nexiting methods by a signiﬁcant margin.\nWe also ﬁnd that distillation is extremely important.\nWithout it, the RGB network can only take advantage of\nthe other modalities through a limited number of tasks (e.g.,\nRGB to Flow, audio/rgb alignment tasks). To learn a single\nRGB network, many important and high performing self-\nsupervised tasks, such as ﬂow, shufﬂing, can only inﬂuence\nthe weights through distillation.\n4.3. Improving supervised learning\nOnce we have learned a representation space using large\namounts of unlabeled data, we want to determine how much\nlabeled data is needed to achieve competitive performance.\nIn Fig.\n5 and Table 3, we compare various approaches\ntrained using our unlabeled videos then ﬁne-tuned on Ki-\nnetics using different amounts of labeled data. The Kinet-\nNumber of Labeled Samples\nMethod\n400\n2k\n4k\n8k\n20k\n40k\n80k\n120k\n160k\n225k\n(all samples)\nRandom Init\n0.93\n2.1\n2.8\n4.4\n6.2\n12.5\n26.4\n52.5\n64.3\n71.2\nFrame Shufﬂe\n1.5\n5.3\n12.4\n18.4\n28.4\n32.5\n38.2\n57.4\n66.8\n70.9\nAudio Align\n2.5\n9.8\n17.2\n28.1\n36.0\n46.0\n54.1\n64.3\n69.5\n71.5\nELo (unsupervised)\n3.6\n15.8\n24.8\n47.0\n58.3\n67.5\n69.2\n70.2\n72.2\n74.4\nTable 3: Using different amounts of labeled samples on the currently available (March 2019) Kinetics-400 dataset using a\n(2+1)D ResNet-50. We achieve similar performance with only ∼50% of the data. Using the entire dataset, we outperform\nthe randomly initialized network.\n0\n200\n0\n1\nFrame\nShuffle\n0\n200\nFlow\nShuffle\n0\n200\nRGB/Audio\nAlign\n0\n200\nFlow/Audio\nAlign\n0\n200\nRGB to\nFlow\n0\n200\nFuture\nRGB\n0\n200\nFuture\nAudio\nValue of λt\nEvolution Round\nFigure 7: The values of the loss function for the various\ntasks throughout evolution. Higher weight values indicate\nthe task is more important. The learned loss functions auto-\nmatically ﬁnds the tasks that most beneﬁt recognition.\nFigure 8: Heatmap visualization of the learned loss func-\ntion. Higher values indicate the components importance.\nSee Fig. 3 for description.\nics dataset has 225k labeled samples and we ﬁnd that using\nonly 25k (10%) yields reasonable performance (58.1% ac-\ncuracy), only 11% lower than our baseline, fully-supervised\nmodel using all samples.\nWe are able to match performance using only 120k sam-\nples, about half the dataset. Using the entire dataset, we\noutperform the baseline network, due to better initilizations\nand the distillation of modalities into the RGB stream.\n4.4. Beneﬁt of additional unlabeled data\nWe explore the effect of using different amounts of unla-\nbeled data. Given a loss function, we train a network using\nN unlabeled samples. As adding more data while keeping\nthe number of epochs ﬁxed increases the number of itera-\ntions, we compare the training both keeping the iterations\nﬁxed and the number of epochs ﬁxed.\nThe results on HMDB are shown in Fig. 6. When ﬁxing\nthe number of iterations to 100k, the performance increases\nas we add more data, even though the number of epochs\n(e.g., number of times each sample is seen) decreases. This\nsuggests that during unsupervised training, the use of more,\ndiverse data is beneﬁcial, even when samples are seen fewer\ntimes. When ﬁxing the number of epochs to 100, we ﬁnd\nMethod\nNum iter.\nAcc\nRandom Search\n2000\n52.4\nGrid Search\n2000\n57.3\nTournament Selection\n2000\n61.4\nCMA-ES\n250\n67.4\nTable 4: Comparison of best loss found with different evo-\nlutionary strategies evaluated on HMDB.\n0\n2\n4\n6\n8\n10\n12\n14\nHMDB Clustering Accuracy\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nZipf Matching Score\n0\n20\n40\n60\n80\n100\nHMDB Clustering Rank\n0\n20\n40\n60\n80\n100\nZipf Matching Rank\nFigure 9: Comparison of the two ﬁtness measures for 100\ndifferent loss functions. The plots show HMDB clustering\nand KL-divergence to Zipf distribution on random videos.\nThe measures are quite correlated. Left: Fitness value (cor-\nrelation r = 0.93). Right: Ranking of each loss function\n(Spearmans’s rho ρ = 0.91).\nthat adding more data further improves performance, sug-\ngesting that more training plus more data is best.\n4.5. Additional Analysis\nExamining the weights of the evolved loss function, λm,t\nand λd, allows us to check which tasks are more important\nfor the target task. Fig. 7 illustrates the weights for sev-\neral tasks (λm,t) over the 250 evolution rounds. We observe\ntasks such as RGB frame shufﬂe get very low weights, sug-\ngesting they are not very useful for the action recognition\ntask. Tasks such as audio alignment are quite important.\nThe ﬁnal fully-evolved loss is shown in Fig. 8.\nTable 4 compares different search methods.\nAs seen\nCMA-ES converges the most quickly and to the best ﬁtness.\nIn Fig. 9, we compare the two different ﬁtness measures,\nﬁnding strong correlation. This suggests that Zipf matching\nis suitable for unsupervised representation evaluation.\n5. Conclusion\nWe proposed a uniﬁed framework for multi-task, multi-\nmodal unsupervised video representation learning and\nfound it beneﬁts recognition tasks. We further introduced\nthe concept of loss function evolution to automatically ﬁnd\nthe weights of the self-supervised tasks and modalities,\nwith unsupervised ﬁtness measure. We ﬁnd powerful un-\nsupervised video representations that outperform prior self-\nsupervised tasks and can match or improve the performance\nof networks trained on supervised data.\nReferences\n[1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul\nNatsev, George Toderici, Balakrishnan Varadarajan, and\nSudheendra Vijayanarasimhan.\nYoutube-8m:\nA large-\nscale video classiﬁcation benchmark.\narXiv preprint\narXiv:1609.08675, 2016. 6\n[2] Relja Arandjelovic and Andrew Zisserman. Look, listen and\nlearn. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), pages 609–617, 2017. 3\n[3] Relja Arandjelovic and Andrew Zisserman.\nObjects that\nsound. In Proceedings of European Conference on Computer\nVision (ECCV), 2018. 3\n[4] Miguel\nA\nBautista,\nArtsiom\nSanakoyeu,\nEkaterina\nTikhoncheva, and Bjorn Ommer.\nCliquecnn: Deep un-\nsupervised exemplar learning.\nIn Advances in Neural\nInformation Processing Systems (NIPS), pages 3846–3854,\n2016. 3\n[5] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning\nof visual features. In Proceedings of European Conference\non Computer Vision (ECCV), pages 132–149, 2018. 3, 5\n[6] Joao Carreira and Andrew Zisserman.\nQuo vadis, action\nrecognition? a new model and the kinetics dataset. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2017. 1, 3, 11\n[7] J. S. Chung and A. Zisserman. Out of time: automated lip\nsync in the wild. In Workshop on Multi-view Lip-reading,\nACCV, 2016. 3\n[8] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\npolicies from data. arXiv preprint arXiv:1805.09501, 2018.\n2\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR09, 2009. 6\n[10] Carl Doersch and Andrew Zisserman.\nMulti-task self-\nsupervised visual learning. In Proceedings of the IEEE In-\nternational Conference on Computer Vision (ICCV), 2017.\n3\n[11] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. arXiv\npreprint arXiv:1812.03982, 2018. 3\n[12] Basura Fernando, Hakan Bilen, Efstratios Gavves, and\nStephen Gould. Self-supervised video representation learn-\ning with odd-one-out networks. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2017. 3, 7\n[13] David E. Goldberg and Kalyanmoy Deb.\nA comparative\nanalysis of selection schemes used in genetic algorithms. In\nFoundations of Genetic Algorithms, pages 69–93. Morgan\nKaufmann, 1991. 4, 5\n[14] Chunhui Gu, Chen Sun, Sudheendra Vijayanarasimhan, Car-\noline Pantofaru, David A. Ross, George Toderici, Yeqing\nLi, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid,\nand Jitendra Malik.\nAVA: A video dataset of spatio-\ntemporally localized atomic visual actions. arXiv preprint\narXiv:1705.08421, 2017. 5\n[15] Nikolaus Hansen, Sibylle D M¨uller, and Petros Koumout-\nsakos. Reducing the time complexity of the derandomized\nevolution strategy with covariance matrix adaptation (cma-\nes). Evolutionary computation, 2003. 5\n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015. 2, 3, 4\n[17] Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised\nlearning via meta-learning. In International Conference on\nLearning Representations, 2019. 3\n[18] Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature\nextraction by time-contrastive learning and nonlinear ica. In\nD. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R.\nGarnett, editors, Advances in Neural Information Processing\nSystems 29. 2016. 3\n[19] Dinesh Jayaraman and Kristen Grauman. Slow and steady\nfeature analysis: higher order temporal coherence in video.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2016. 3\n[20] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017. 3, 5, 6\n[21] Dahun Kim, Donghyeon Cho, and In So Kweon.\nSelf-\nsupervised video representation learning with space-time cu-\nbic puzzles. In AAAI, 2018. 3\n[22] Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative\nlearning of audio and video models from self-supervised syn-\nchronization. In Advances in Neural Information Processing\nSystems (NIPS), pages 7774–7785, 2018. 3, 6, 7, 13\n[23] Hildegard Kuehne, Hueihan Jhuang, Est´ıbaliz Garrote,\nTomaso Poggio, and Thomas Serre. Hmdb: a large video\ndatabase for human motion recognition.\nIn Proceedings\nof the IEEE International Conference on Computer Vision\n(ICCV). IEEE, 2011. 6, 11\n[24] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-\nHsuan Yang. Unsupervised representation learning by sort-\ning sequences.\nIn Proceedings of the IEEE International\nConference on Computer Vision (ICCV), pages 667–676,\n2017. 3, 7\n[25] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon\nShlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan\nHuang, and Kevin Murphy. Progressive neural architecture\nsearch. In Proceedings of European Conference on Com-\nputer Vision (ECCV), 2018. 2\n[26] Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuf-\nﬂe and learn: unsupervised learning using temporal order\nveriﬁcation.\nIn Proceedings of European Conference on\nComputer Vision (ECCV), 2016. 3, 6, 7\n[27] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ra-\nmakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown,\nQuanfu Fan, Dan Gutfruend, Carl Vondrick, et al. Moments\nin time dataset: one million videos for event understanding.\narXiv preprint arXiv:1801.03150, 2018. 3\n[28] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In Proceed-\nings of European Conference on Computer Vision (ECCV),\npages 69–84, 2016. 3\n[29] Andrew Owens and Alexei A Efros.\nAudio-visual scene\nanalysis with self-supervised multisensory features.\nIn\nProceedings of European Conference on Computer Vision\n(ECCV), 2018. 3, 6, 7\n[30] Dilip Krishnan Phillip Isola, Daniel Zoran and Edward H\nAdelson.\nLearning visual groups from co-occurrences in\nspace and time. 2015. 3\n[31] Lyndsey C Pickup, Zheng Pan, Donglai Wei, YiChang Shih,\nChangshui Zhang, Andrew Zisserman, Bernhard Scholkopf,\nand William T Freeman. Seeing the arrow of time. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2014. 3, 6\n[32] Zhongzheng Ren and Yong Jae Lee.\nCross-domain self-\nsupervised multi-task feature learning using synthetic im-\nagery. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2018. 3\n[33] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine\nHsu, Eric Jang, Stefan Schaal, and Sergey Levine. Time-\ncontrastive networks: Self-supervised learning from pixels.\n2017. 3\n[34] Gunnar A Sigurdsson, Olga Russakovsky, and Abhinav\nGupta. What actions are needed for understanding human\nactions in videos? arXiv preprint arXiv:1708.02696, 2017.\n1\n[35] Karen Simonyan and Andrew Zisserman. Two-stream con-\nvolutional networks for action recognition in videos. In Ad-\nvances in Neural Information Processing Systems (NIPS),\npages 568–576, 2014. 3\n[36] K. Soomro, A. Roshan Zamir, and M. Shah. UCF101: A\ndataset of 101 human actions classes from videos in the wild.\nIn CRCV-TR-12-01, 2012. 6, 11\n[37] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudi-\nnov. Unsupervised learning of video representations using\nlstms.\nIn International Conference on Machine Learning\n(ICML), pages 843–852, 2015. 3, 6\n[38] Jonathan C Stroud, David A Ross, Chen Sun, Jia Deng, and\nRahul Sukthankar. D3d: Distilled 3d networks for video ac-\ntion recognition. arXiv preprint arXiv:1812.08249, 2018. 4,\n5\n[39] Du Tran, Lubomir D Bourdev, Rob Fergus, Lorenzo Torre-\nsani, and Manohar Paluri. C3d: generic features for video\nanalysis. CoRR, abs/1412.0767, 2(7):8, 2014. 1, 3\n[40] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann\nLeCun, and Manohar Paluri. A closer look at spatiotemporal\nconvolutions for action recognition. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 6450–6459, 2018. 1\n[41] Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio\nGuadarrama, and Kevin Murphy. Tracking emerges by col-\norizing videos. In Proceedings of European Conference on\nComputer Vision (ECCV), 2018. 3\n[42] Heng Wang, Alexander Kl¨aser, Cordelia Schmid, and\nCheng-Lin Liu.\nAction recognition by dense trajectories.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 3169–3176. IEEE,\n2011. 3\n[43] Xiaolong Wang and Abhinav Gupta. Unsupervised learning\nof visual representations using videos. In Proceedings of the\nIEEE International Conference on Computer Vision (ICCV),\npages 2794–2802, 2015. 3, 7\n[44] X. Wang, K. He, and A. Gupta. Transitive invariance for\nselfsupervised visual representation learning. In Proceedings\nof the IEEE International Conference on Computer Vision\n(ICCV), 2017. 3\n[45] Xiaolong Wang, Allan Jabri, and Alexei A. Efros. Learning\ncorrespondence from the cycle-consistency of time. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019. 3\n[46] Donglai\nWei,\nJoseph\nLim,\nAndrew\nZisserman,\nand\nWilliam T. Freeman. Learning and using the arrow of time.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2018. 3\n[47] Amir Zamir, Alexander Sax, William Shen, Leonidas\nGuibas, Jitendra Malik, and Silvio Savarese. Taskonomy:\nDisentangling task transfer learning. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2018. 3\n[48] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Lo-\ncal aggregation for unsupervised learning of visual embed-\ndings. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), pages 6002–6012, 2019. 5\n[49] George K. Zipf. Human behavior and the principle of least\neffort. In Addison-Wesley, Cambridge, Massachusetts, 1949.\n1\nA. Datasets\nWe compare our approach, referred to as ELo in the pa-\nper, on 3 standard video recognition datasets. Kinetics [6]\nis a large-scale video dataset with over 200k labeled video\nclips for 400 different activity classes. Each clip is 10 sec-\nonds long, leading to over over 500 hours of annotated video\ndata. HMDB [23] is a smaller dataset with around 3000\ntraining and 1500 validation video clips for 51 different ac-\ntivities. On average, each video is 3 seconds long. UCF-\n101 [36] is similar to HMDB with 101 different actions,\nand about 13,000 videos split into training and test sets.\nUsing both large-scale data and smaller datasets shows\nthat the representation obtained from unsupervised learning\nis general and works well even with limited labeled data.\nB. Visualization of loss evolution\nFig. 10 shows the t-SNE embedding for our unsuper-\nvised approach compared to random weights, ImageNet\ntrained CNNs, and Kinetics trained CNNs. ELo generates\nmore clear video clusters than random weights and Ima-\ngeNet weights, and is more comparable to the model trained\nwith supervised Kinetics videos and labels.\nC. Supplemental Results\nFig 11 visualizes the ﬁlters our approach learned com-\npared to other approaches: it shows ﬁlters from random\ninitialization, supervised learning with large data, previ-\nous self-supervised learning, and our unsupervised learning\nmethod ELo. ELo ﬁlters are quite similar to those learned\nwith labeled data, but do show some differences.\n(a)\n(b)\n(c)\n(d)\nFigure 10: t-SNE embeddings of HMDB test videos from networks trained on various data. Each color represents a different\nactivity. (a) Randomly initialized network (b) ImageNet trained network (c) Kinetics trained network (d) Our evolved loss.\n(a)\n(b)\n(c)\n(d)\nFigure 11: Visualization of (a) random ﬁlters (b) ﬁlters learned with standard supervised learning and (c) AVTS [22] self-\nsupervised learned ﬁlters (d) ﬁlters learned with our evolved multi-modal, multi-task loss function.\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2020-02-26",
  "updated": "2020-02-26"
}