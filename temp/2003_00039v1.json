{
  "id": "http://arxiv.org/abs/2003.00039v1",
  "title": "Unsupervised Machine Learning of Quenched Gauge Symmetries: A Proof-of-Concept Demonstration",
  "authors": [
    "Daniel Lozano-Gómez",
    "Darren Pereira",
    "Michel J. P. Gingras"
  ],
  "abstract": "In condensed matter physics, one of the goals of machine learning is the\nclassification of phases of matter. The consideration of a system's symmetries\ncan significantly assist the machine in this goal. We demonstrate the ability\nof an unsupervised machine learning protocol, the Principal Component Analysis\nmethod, to detect hidden quenched gauge symmetries introduced via the so-called\nMattis gauge transformation. Our work reveals that unsupervised machine\nlearning can identify hidden properties of a model and may therefore provide\nnew insights into the models themselves.",
  "text": "Unsupervised Machine Learning of Quenched Gauge Symmetries:\nA Proof-of-Concept Demonstration\nDaniel Lozano-G´omez,∗Darren Pereira,∗and Michel J.P. Gingras\nDepartment of Physics and Astronomy, University of Waterloo, Ontario, N2L 3G1, Canada\nIn condensed matter physics, one of the goals of machine learning is the classiﬁcation of phases of\nmatter. The consideration of a system’s symmetries can signiﬁcantly assist the machine in this goal.\nWe demonstrate the ability of an unsupervised machine learning protocol, the Principal Component\nAnalysis method, to detect hidden quenched gauge symmetries introduced via the so-called Mattis\ngauge transformation. Our work reveals that unsupervised machine learning can identify hidden\nproperties of a model and may therefore provide new insights into the models themselves.\nIntroduction – Machine learning (ML) has in recent\nyears proven to be a powerful pattern recognition tool\nwith applications in various branches of science. These\ntechniques have shown their ability to extract, identify,\nand even propose descriptive patterns found in the input\ndata. Particularly in condensed matter physics, the ap-\nplication of ML techniques began with the use of the\nPrincipal Component Analysis (PCA) method [1] and\nneural networks [2] to identify the ferromagnetic and\nparamagnetic phases of the Ising model on a square lat-\ntice. Since then, this ﬁeld has exploded with a variety of\nML applications [3–5]. These techniques and applications\ncan be broadly grouped into two categories: supervised\nML (SML), in which the input data is labelled to train the\nmachine [6–21]; and unsupervised ML (UML), in which\nthe input data is unlabelled and the machine proposes its\nown classiﬁcation scheme [19–32]. As a major task of the\ncondensed matter physicist, the classiﬁcation of phases\nin various models has remained central among these ap-\nplications. Evidence is accumulating that the machine\nlearning of phases can be guided by physical insights into\nthe model or system, such as symmetries. This has been\nmost clearly demonstrated by exploiting properties such\nas locality and translational symmetry via convolutional\nneural networks [2], or by taking advantage of symmetry-\nbreaking to extract order parameters for hidden orders\n[7].\nIn light of the beneﬁts that these physically-inspired\nshortcuts provide, one may ask a question of foremost\nimportance for the usage of ML in physics: is it possible\nfor ML to provide theoretical insight into the hidden or\nunknown properties of a model itself ? A ﬁtting testing\nground for such a question is physical models possess-\ning gauge symmetries, as these models can be simpliﬁed\nby a suitable mathematical transformation. Our ques-\ntion then becomes a matter of determining if ML can de-\ntect the gauge symmetry of these models without prior\nknowledge. Doing so would prove that ML is capable of\nlearning fundamental mathematical details of the studied\nmodel and not just thermodynamic quantities. This abil-\nity oﬀers clear beneﬁts for various branches of physics,\nincluding the aforementioned exploitation of symmetries\nfor phase classiﬁcation.\nFurthermore, the controlled\nmathematical nature of these gauge-symmetric models\nwould also suggest their use as a probe of how ML meth-\nods work and what they are truly learning.\nTo explore this question, we therefore require (i) a\nmodel that seems complex but can be simpliﬁed by some\ngauge transformation, and (ii) a UML method whose\nself-determined classiﬁcation scheme can be exposed. In\nlight of (i), we study the Mattis Ising Spin Glass (MISG)\n[33, 34] and the Mattis XY Gauge Glass (MXYGG) mod-\nels [34]. At ﬁrst glance, the MISG and MXYGG models\nlook prohibitively complex: the Hamiltonians for these\nmodels possess almost arbitrary bond interactions, which\nmake an analytical approach seem intractable. A visual\nsnapshot of their ground state conﬁgurations displays\nno recognizable pattern but instead appears completely\ndisordered.\nHowever, the MISG and MXYGG models\ncan be transformed into the regular ferromagnetic Ising\nand XY models, respectively, under a (Mattis) gauge\ntransformation [34]. Regarding (ii), an important con-\nsideration is the trade-oﬀbetween interpretability and\nscalability. We therefore use PCA [35], which is highly\ninterpretable and simple to apply, as opposed to neural-\nnetwork-based methods, which may be more powerful but\nare not as open to interpretation.\nThe outline of the paper is as follows.\nWe ﬁrst\ndescribe the MISG and MXYGG models, as well as the\nMattis gauge transformation, in the Models section. We\nthen give a brief introduction to PCA in the Methods\nsection.\nIn the Results section, we demonstrate that\nPCA is able to identify the gauge variables that quantify\nthe Mattis gauge transformation.\nPCA additionally\nﬁnds that the bond-disordered MISG and MXYGG\nmodels are simply disguised versions of the regular Ising\nand XY models, classifying the phases in the former\ngauge-transformed models in exactly the same manner\nas it would with the regular models. Our work suggests\nthat interpretable ML methods can therefore be used\nto reveal hidden features in the models themselves,\ngiving a positive answer to our above question.\nWe\nconclude by discussing the implications of our ﬁndings\nfor investigations into other models and for other ML\napplications.\narXiv:2003.00039v1  [cond-mat.dis-nn]  28 Feb 2020\n2\nModels – The MISG model [33, 34] on a square lattice\nis deﬁned by the Hamiltonian\nH = −\nX\n⟨i,j⟩\nJijσz\ni σz\nj ,\n(1)\nwhere the spin variables are σz\ni = ±1.\nThe couplings\n{Jij} are free to take the values ±J randomly, with the\nimposed constraint that the product P of the couplings\naround a square plaquette is positive:\nP ≡\nY\n⟨i,j⟩∈□\nJij >0.\n(2)\nThis constraint enforces a non-frustrated ground state in\nthe system and allows a so-called Mattis gauge transfor-\nmation to be applied [33, 34]. This gauge transforma-\ntion reexpresses the interaction couplings as Jij = ϵiϵjJ,\nwhere {ϵi} are random site (gauge) variables that take\nvalues of ±1. Through this transformation, the Hamilto-\nnian (1) becomes\nH = −J\nX\n⟨i,j⟩\nϵiσz\ni ϵjσz\nj = −J\nX\n⟨i,j⟩\nτ z\ni τ z\nj ,\n(3)\nwhere τ z\ni ≡ϵiσz\ni = ±1 are new Ising variables.\nIt is now clear that this system possesses a well-\ndeﬁned order parameter given by the Ising model\n“τ−magnetization”,\nM ≡⟨\nX\ni\nτ z\ni ⟩= ⟨\nX\ni\nϵiσz\ni ⟩,\n(4)\nillustrating that the MISG model is nothing but an Ising\nmodel in disguise. Further information about this map-\nping is given in the Appendix.\nSimilarly, the MXYGG model is described by an XY\nmodel with random phase factors {Aij} [36–38],\nH = −J\nX\n⟨i,j⟩\ncos(∆φij −Aij),\n(5)\nwhere ∆φij = φi −φj is the diﬀerence between the on-\nsite angular variables φi ∈[0, 2π). This is equivalent to\nan XY model with random Heisenberg exchange Jij ≡\nJ cos(Aij) and Dzyaloshinsky-Moriya interactions Dij ≡\nJ sin(Aij).\nThis Hamiltonian is unfrustrated as long as the phase\nfactors around a plaquette add to a multiple of 2π, i.e.\nPXY = (P\n⟨i,j⟩∈□Aij) mod 2π = 0 [34]. A Mattis gauge\ntransformation can then be applied by deﬁning random\nsite (gauge) variables {bi} such that Aij = bi −bj, with\nbi ∈[0, 2π). The Hamiltonian then becomes\nH = −J\nX\n⟨i,j⟩\ncos(∆θij),\n(6)\nwhere θi ≡φi + bi are new XY variables. The MXYGG\nmodel can thus be mapped onto a ferromagnetic XY\nmodel under this gauge transformation. This model pos-\nsesses a magnetization vector M = ⟨P\ni(cos θi, sin θi)⟩,\nor\nMx = ⟨\nX\ni\n(cos φi cos bi −sin φi sin bi)⟩,\nMy = ⟨\nX\ni\n(sin φi cos bi + cos φi sin bi)⟩.\n(7)\nMethods – PCA is a dimensional reduction technique\nthat identiﬁes which linear combinations of the input\ndata best characterize the full dataset. The input data for\nthis method is deﬁned as n sets of conﬁgurations {xi(Tj)}\nof an N−site system, where xi is some variable (e.g. σz\ni )\nassociated with the ith site and sampled at a tempera-\nture Tj (j = 1, . . . , n). The full dataset can then be\nformatted as a data matrix Xdata,\nXdata ≡\n\n\n\n\n\n{xi(T1)}\n{xi(T2)}\n...\n{xi(Tn)}\n\n\n\n\n.\n(8)\nAfter each row is centered by subtracting its mean value,\nthe covariance matrix deﬁned as XT\ndataXdata is diagonal-\nized. The normalized eigenvalues and eigenvectors ob-\ntained are the so-called explained variance ratios {λk}\nand principal components {⃗u(k)}, respectively. Note that\nthe eigenvectors can be rescaled by any convenient factor,\nsuch as the system size. The projection ℓ(k)(Tj) of the jth\nconﬁguration {xi(Tj)} onto the kth principal component\n⃗u(k) takes the form\nℓ(k)(Tj) ≡\nX\ni\nu(k)\ni\nxi(Tj).\n(9)\nThese linear combinations {ℓ(k)(Tj)} are the new quanti-\nties used by PCA to characterize the full dataset, where\ntheir relative importance is given by the values of their\nexplained variance ratios. By construction, the ith value\nof any principal component is the coeﬃcient multiplying\nthe variable xi for any projection ℓ(k)(Tj); therefore, the\ncomponents of the eigenvector ⃗u(k) directly contain site-\ndependent information.\nPlotting diﬀerent projections\nagainst each other visually reveals how PCA “clusters”\nthe input data and along which projections the data is\nmost or least correlated. These clusters are composed of\npoints which represent conﬁgurations with similar values\nof the projections.\nPCA is applied to the MISG model by using spin\nconﬁgurations {σz\ni (Tj)} as the input data {xi(Tj)},\nwith Tj ∈[J, 4J] sampled from a single spin ﬂip Monte\nCarlo (MC) algorithm of a system of N = 2500 spins\n(L = 50).\nThe {ϵi} gauge variables at each site are\nrandomly chosen as either ±1 with equal probabilities.\nFor the MXYGG model, a MC simulation is performed\nin a system of 900 spins (L = 30) for temperatures\n3\nTj\n∈\n[0.2J, 1.8J] to sample the continuous angular\nvariables {φi}. PCA is then applied to three diﬀerent\ndatasets: {cos(φi)(Tj)} (the “X dataset”), {sin(φi)(Tj)}\n(the “Y dataset”), or {{cos(φi)(Tj)}, {sin(φi)(Tj)}} (the\nfull dataset).\nThe gauge variables {bi} are randomly\ndrawn from a discrete distribution { 2πn\n5\n| n = 1, . . . , 5}\n[39].\nIn order to sample uncorrelated data, 3 × 104\nthermalization sweeps and 5 × 104 measurement sweeps\nare used at every temperature for both models; 50\ndiﬀerent temperatures are selected.\nSampling is done\nevery 50 (100) measurement sweeps for the MISG\n(MXYGG) model, producing n = 5×104 (n = 2.5×104)\nconﬁgurations. In both the MISG and MXYGG cases,\nPCA has no information about the gauge variables {ϵi}\nor {bi}, nor the Ising variables {τ z\ni } or XY variables\n{θi}. The objective is to determine if PCA can identify\nthese gauge variables and the underlying Ising or XY\nmodels regardless, to which we now turn.\nResults for the MISG Model – PCA is applied to\nthe conﬁgurations sampled through MC simulations for\nthe MISG model.\nPlotting ℓ(1) versus ℓ(2) reveals a\ncentral high-temperature cluster and two adjacent low-\ntemperature clusters as illustrated in Fig. 8 of the Ap-\npendix, which is precisely how PCA clusters the input\ndata of the regular ferromagnetic Ising model [1]. The\nsimilarity in this clustering suggests that PCA is charac-\nterizing the input data according to the Ising magnetiza-\ntion order parameter as it did in the regular case, thereby\ndetecting the underlying Ising model.\nTo verify this quantitatively, the projection ℓ(1) of the\ninput data onto the ﬁrst (and most important) principal\ncomponent is compared with the τ-magnetization cal-\nculated within MC simulations using Eq. (4), as shown\nin Fig. 1. The resemblance of the projection in Fig. 1a\nto the Ising magnetization in Fig. 1b demonstrates that\nPCA is learning this order parameter.\nThis is further\nconﬁrmed when this projection is plotted against the\nIsing magnetization in Fig. 1c, revealing a linear rela-\ntionship with a slope of 1. Since ℓ(1) is equivalent to the\nτ-magnetization even when PCA was only provided with\n{σz\ni }, the ﬁrst principal component contains information\nabout the gauge variables {ϵi} that are hidden from PCA.\nIn other words, the components of ⃗u(1) are identiﬁed as\nthe values of the gauge variables in the lattice. Therefore,\nby comparing the projection ℓ(1) with Eq. (4), the learned\ngauge variables can be extracted. Moreover, since PCA\nprovides site-dependent information, the learned bond in-\nteractions {Jij} and square plaquette values {P} can be\ncalculated and compared with the original values. His-\ntograms of the learned and known gauge variables {ϵi},\nbond interactions {Jij} and square plaquette values {P}\nvalues are illustrated in Fig 2. As can be seen in Fig. 2a\nand Fig. 2b, the learned values of the gauge variables and\nthe bond interactions are described by a bimodal distri-\nbution centered around ±1.\nThese distributions agree\n(a)\n(b)\n(c)\nFIG. 1. Comparison of (a) ℓ(1)(T), the projection of the spin\nconﬁguration data {σz\ni } onto the ﬁrst principal component,\nwith (b) the τ−magnetization M of the MISG model from\nMC simulations. (c) Plot of ℓ(1)(T) against M.\n−1\n0\n1\nεi\n0\n200\n400\n600\n800\n1000\n1200\nPCA\nMC\n−1\n0\n1\nJij\n0\n500\n1000\n1500\n2000\n2500\nPCA\nMC\n0.5\n1.0\n1.5\nP\n100\n101\n102\n103\nPCA\nMC\n(a)\n(b)\n(c)\nFIG. 2. Histograms of the (a) gauge variables {ϵi}, (b) bond\ninteractions {Jij}, and (c) square plaquette values {P} for\nthe MISG model, comparing real values from MC (known)\nand values from PCA (learned).\nwith the ones produced with the known gauge variables\nused in the MC simulation. Moreover, a remarkable re-\nsult comes from the distribution for the square plaquette\nvalues {P}, shown in Fig. 2c: this distribution is cen-\ntered near the value P = 1 which deﬁnes the plaquette\nconstraint used in the MC simulation. PCA’s ability to\nlearn the values of the gauge variables is additionally pro-\nvided by MC simulations: when the learned gauge vari-\nables {ϵi} are used within MC simulations, the resulting\nenergy per spin and speciﬁc heat curves are equivalent to\nthe original curves which used the known gauge variables,\nas detailed in Fig. 7 of the Appendix.\nAfter extracting the gauge variables, we can apply this\nlearned gauge transformation to other quantities to con-\nﬁrm that the MISG model is transformed into the regular\nIsing model. For example, the second principal compo-\nnent of this model has been computed and plotted on\nthe associated lattice sites [24]; by multiplying the sec-\nond principal component of the MISG model by the ﬁrst,\n4\n(a)\n(b)\n(c)\nFIG. 3. Values of the (a) ﬁrst and (b) second principal com-\nponents of the MISG model, plotted on the lattice sites that\nthey are associated with. (c) Product of the values of the ﬁrst\nand second principal components on each site. This plot has\nbeen rescaled by the lattice dimension L.\nas shown in Fig. 3, the known regular Ising result is recon-\nstructed. This operation is therefore equivalent to apply-\ning the gauge transformation to go from the MISG model\nto the regular Ising model. Altogether, this comparison\nof the learned and known gauge variables and thermody-\nnamic quantities demonstrates PCA’s ability to identify\nthe correct values of these quenched gauge variables.\nResults for the MXYGG Model – We now turn to\nthe more complex case of the MXYGG model.\nAs\nin [22, 24], we ﬁrst perform PCA on the full dataset\n{{cos(φi)}, {sin(φi)}} generated from MC simulations.\nBy projecting this data onto the ﬁrst two principal com-\nponents, which are equally most important, the result-\ning clusters have the same U(1) symmetry as the ones\nreported for the regular XY model (see Fig. 2 of [22]\nand discussion therein). This similarity in the clusters\nsuggests that PCA is characterizing the full dataset of\nthe MXYGG and XY models in the same fashion, i.e.\naccording to the magnetization vector [22]. However, if\nPCA is performed only on the X dataset or the Y dataset\nof the MXYGG model, the resulting clusters still reveal\na U(1) symmetry, as shown in Fig. 4; this is in contrast\nto the results for the regular XY model (see Fig. 9 of the\nAppendix). This diﬀerence indicates that PCA identiﬁes\nsome feature that diﬀerentiates the MXYGG model from\nthe regular XY model. This suggests that PCA has de-\ntected the Mattis gauge transformation, which must also\nbe present in the full dataset.\nNow that the presence of the gauge transformation\nhas been identiﬁed, we return to the principal compo-\nnents calculated from the full dataset. As in the regu-\nlar XY model [22], the ﬁrst two principal components,\n⃗u(1) and ⃗u(2), have the largest explained variance ratios.\nThese two eigenvectors describe the non-zero magnetiza-\ntion components observed in the ﬁnite system [22]. The\nprojections of the data onto the ﬁrst and second principal\n20\n10\n0\n10\n20\n(1)\n20\n10\n0\n10\n20\n(2)\n{cos(\ni)}  Data\n0.5\n1.0\n1.5\n0\n5\n10\n15\nk\n0.00\n0.05\n0.10\n0.15\nx\nk\n20\n10\n0\n10\n20\n(1)\n20\n10\n0\n10\n20\n(2)\n{sin(\ni)}  Data\n0.5\n1.0\n1.5\n0\n5\n10\n15\nk\n0.00\n0.05\n0.10\n0.15\ny\nk\n(a)\n(b)\n(c)\n(d)\nFIG. 4. Principal component projections ℓ(1) versus ℓ(2) ((a)\nand (b)) and explained variance ratios ((c) and (d)) for the\nMXYGG model, for PCA applied to {cos (φi)} or {sin (φi)}\nonly.\ncomponents take the form\nℓ(1) ≡\nX\ni\n\u0010\nu(1c)\ni\ncos (φi) + u(1s)\ni\nsin (φi)\n\u0011\nℓ(2) ≡\nX\ni\n\u0010\nu(2c)\ni\ncos (φi) + u(2s)\ni\nsin (φi)\n\u0011\n,\n(10)\nwhere we have deﬁned ⃗u(k) ≡({u(kc)\ni\n}, {u(ks)\ni\n}) to match\nthe separation of cosines and sines in the full dataset\n{{cos(φi)}, {sin(φi)}}.\nSince we know that PCA is\ncharacterizing the data according to the magnetization\nvector, we identify the projections ℓ(1) and ℓ(2) with the\ncomponents of the magnetization in Eq. (7). Through\nthis identiﬁcation the values of the gauge variables {bi}\nare extracted from the principal components, as detailed\nin the Appendix.\nThe distribution of the extracted\ngauge variables {bi} is shown in Fig. 5, revealing ﬁve\nequally-spaced peaks as expected for the ﬁve equally-\nspaced choices of gauge variables. PCA is therefore able\nto calculate the transformation that maps the MXYGG\nmodel onto the regular XY model.\nConclusion – We have applied PCA to two spin mod-\nels with random interactions, the MISG and MXYGG\nmodels on a square lattice.\nPCA was able to deter-\nmine that each spin model can be related to a simpler\nmodel, namely the regular Ising and XY models. This\nwas accomplished by (1) recognizing the similarities be-\ntween the projections of the input data onto the prin-\ncipal components of the regular and gauge-transformed\nmodels, (2) identifying that PCA characterizes the data\nusing the same thermodynamic quantity (i.e. the magne-\ntization), and (3) verifying that the gauge variables cal-\nculated by PCA were consistent with the ones selected\nwithin MC simulations. These results should easily gen-\neralize to other gauge-symmetric spin models with ran-\ndom interactions, such as spin glass models with O(3)\ngauge symmetry [34, 40].\n5\n0\n4\n2\n3\n4\n5\n4\n3\n2\n7\n4\n2\n{bi}\n0\n20\n40\n60\nFIG. 5. Histogram of extracted gauge variables {bi} for the\nMXYGG model, revealing ﬁve equally-spaced peaks corre-\nsponding to the ﬁve discrete choices of bi in the MC simu-\nlation.\nNote that the gauge variables are known up to an\noverall rotation given by the global U(1) symmetry of the\nHamiltonian (5).\nOur work suggests that UML is capable of more than\njust classifying data; interpretable UML methods could\npossibly learn hidden features of an underlying model,\nsuch as symmetries and gauge transformations.\nFor\nthe physicist, this means UML could reveal previously\nunknown insights into a simulated model. It is of interest\nto investigate how other UML methods beyond PCA\nfare in this regard (e.g. autoencoders, which share some\nsimilarities with PCA, are capable of nonlinear ﬁtting\nand therefore possess greater descriptive power [25]).\nSuch methods may not be as interpretable as PCA;\nhence, using them to discover the hidden properties of\na model might be a more complicated task. However,\neven in such cases, our work indicates that UML could\nat least “see through” nontrivial characteristics such as\ngauge symmetries.\nThis suggests that UML methods\ncould alternatively be used to eﬃciently label data\nfor subsequently applied SML methods, which may\nexplain how PCA and a neural network together learned\nthe SU(2) gauge theory order parameter [21].\nThe\ngeneralization of this idea to other and more powerful\nUML methods may therefore expedite the learning\nprocess of a neural network,\nwhich makes this an\navenue worth pursuing in its own right and especially\nfor classifying phases. Lastly, gauge-symmetric models\nrepresent a class of models with known mathematical\nsimpliﬁcations. Applying UML methods to these models\nmay therefore provide a deeper understanding of how\nthese methods work and what exactly they learn.\nWe thank W. Jin, C. X. Cerkauskas, K. Chung, A. Gol-\nubeva, R. G. Melko, and S. J. Wetzel for helpful discus-\nsions. This work was supported by the Canada Research\nChair program (M.J.P.G., Tier 1) and by the NSERC of\nCanada CGS-M program (D.P.).\nAPPENDIX\nAdditional Analysis of the MISG Model\nDeﬁnition of Plaquettes\nA plaquette in the lattice is deﬁned as the smallest re-\ngion contained within a closed loop of neighbouring sites.\nOn the square lattice, the resulting plaquettes are com-\nposed of four sites.\nFor the Mattis transformation we\nintroduce gauge variables ϵi for every site to deﬁne the\ncoupling constant Jij = ϵiϵjJ on every nearest neighbour\nbond. This procedure is sketched in Fig. 6.\n(a)\n(b)\n(c)\n(d)\nFIG. 6. Plaquette in the MISG model. (a) Example of gauge\nvariables {ϵi} for the four sites. (b) Resulting signs of the\nbond interactions {Jij} for the four bonds. (c) Example of\nground state spin conﬁguration of σz\ni variables for these ran-\ndom bond interactions. Note that the coupling in the Hamil-\ntonian is −Jij. (d) Resulting ground state conﬁguration of τ z\ni\nas a product of the spin conﬁguration illustrated in (c) with\nthe gauge variables in (a).\nMC Simulation with the Learned Gauge Variables\nAfter applying PCA to the MISG model, we study the\nfaithfulness of the learned gauge variables. We performed\na MC simulation on the MISG model as before, but in-\nstead used the learned gauge variables in place of the\nknown gauge variables. The thermodynamic quantities\nobtained with this simulation are then compared with the\nthermodynamic quantities which used the known gauge\nvariables, as shown in Fig. 7. As can be seen, the en-\nergy and speciﬁc curves obtained for both simulations\nare identical, supporting PCA’s ability to learn the gauge\nvariables of the MISG model.\nPCA Results for the Regular Ising and XY Models\nPCA Clusters for the Regular Ising and XY Models\nFor completeness and comparison, MC simulations are\nrun for the regular Ising model (L = 20) and the regular\nXY model (L = 30) on a square lattice; the exact same\nparameters for the MC simulation temperature sweeps\nas detailed in the main text for the MISG and MXYGG\n6\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n−2.0\n−1.5\n−1.0\n−0.5\nE\nMC PCA\nMC Original\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nT\n0.0\n0.5\n1.0\n1.5\nC\nMC PCA\nMC Original\nFIG. 7.\nComparison of thermodynamic quantities (energy\nper spin E and speciﬁc heat C) calculated within MC sim-\nulations, using the original known gauge variables and the\nlearned gauge variables from PCA.\nmodels are used here. PCA is applied to the spin con-\nﬁgurations from both sets of data, which are formatted\nin the same manner as the input data of the MISG and\nMXYGG models. The clusters identiﬁed by PCA for the\nregular Ising model are shown in Fig. 8. For the regu-\nlar XY model, PCA is applied to either the X dataset\n({cos(φi)}) or the Y dataset ({sin(φi)}).\nThe projec-\ntions onto the ﬁrst two principal components of the X\ndataset alone or the Y dataset alone are shown in Fig. 9.\nFirstly, Fig. 9 should be compared with Fig. 4 of the\nmain text. Although the clusters that PCA identiﬁes for\nthe regular XY and the MXYGG models look the same\nwhen provided with the full dataset, there is a clear dif-\nference when PCA is provided with only the X or Y\ndataset. This diﬀerence is indicative of an identiﬁed fea-\nture which is not present in the regular XY model. Sec-\nondly, Fig. 9 should be compared with Fig. 8. Previous\nwork on the regular Ising model [1] has shown that the\ncentral high-temperature cluster and the two adjacent\nlow-temperature clusters correspond to the paramagnetic\nand ferromagnetic phases, respectively, which PCA de-\ntermines by summing the spin conﬁgurations. Fig. 9 can\nbe similarly interpreted in light of this. PCA character-\nizes the input data of the regular XY model by directly\nsumming the spin conﬁgurations along two orthogonal di-\nrections. This explains why the PCA clusters for the X\nand Y datasets of the regular XY model look like those\nof the regular Ising model. However, since the spin vari-\nables in the regular XY model are continuous and not\ndiscrete ±1 values as in the regular Ising model, the low-\ntemperature projection forms one continuous line rather\nthan two separate clusters. Note that the two orthogonal\ndirections along which the magnetization is determined\nby PCA are not necessarily the chosen x and y directions\nof the MC simulation, owing to the global U(1) rotational\nsymmetry of the regular XY model. The determination\nof this global rotation is the focus of the next section.\n(a)\n(b)\nFIG. 8. (a) Principal component projection ℓ(1) versus ℓ(2)\nand (b) ﬁrst 20 explained variance ratios for the regular Ising\nmodel on an L = 20 square lattice.\n20\n0\n20\n(1)\n10\n5\n0\n5\n10\n(2)\n{cos(\ni)}  Data\n0.5\n1.0\n1.5\n0\n5\n10\n15\nk\n0.0\n0.1\n0.2\n0.3\nx\nk\n20\n0\n20\n(1)\n10\n5\n0\n5\n10\n15\n(2)\n{sin(\ni)}  Data\n0.5\n1.0\n1.5\n0\n5\n10\n15\nk\n0.0\n0.1\n0.2\n0.3\ny\nk\n(a)\n(b)\n(c)\n(d)\nFIG. 9. Principal component projections ℓ(1) versus ℓ(2) ((a)\nand (b)) and ﬁrst 20 explained variance ratios ((c) and (d))\nfor the regular XY model, for PCA applied to {cos (φi)} or\n{sin (φi)} only. The conﬁgurations were sampled from an L =\n30 square lattice.\nProof of Global Rotation for the Regular XY Model\nThe regular XY\nmodel can be considered as the\nMXYGG model with bi = 0 for all lattice sites i.\nIn\nthis case, the magnetization vector for the regular XY\nmodel is\nMx =\nX\ni\ncos (φi)\nMy =\nX\ni\nsin (φi),\nin contrast to Eq. (7).\nComparing with the principal\ncomponent projections in Eq. (10), this would imply that\nthe principal component eigenvectors should have com-\nponents {u1c\ni } = {u2s\ni } = 1 and {u1s\ni } = {u2c\ni } = 0 if\n7\n0\n500\n1000\n1500\ni\n1.0\n0.5\n0.0\n0.5\n1.0\nu(1)\ni\n0\n500\n1000\n1500\ni\n1.0\n0.5\n0.0\n0.5\n1.0\nu(2)\ni\n(a)\n(b)\nFIG. 10. Components of the (a) ﬁrst and (b) second princi-\npal component eigenvectors, ⃗u(1) and ⃗u(2), for the regular XY\nmodel before applying a global rotation. The two branches\nof each graph correspond to coeﬃcients for {cos (φi)} or\n{sin (φi)} data.\n0\n4\n2\n3\n4\n5\n4\n3\n2\n7\n4\n2\n0\n5\n10\n15\nFIG. 11. Histogram of extracted global rotation angle α for\nthe regular XY model.\nPCA was learning the magnetization along the x and y\ndirections from MC simulations. This is clearly not the\ncase; see Fig. 10. However, accounting for the global U(1)\nrotation symmetry of the XY model, the magnetization\ntakes the general form\nMx =\nX\ni\ncos(φi + α) =\nX\ni\n(cos φi cos α −sin φi sin α) ,\nMy =\nX\ni\nsin(φi + α) =\nX\ni\n(sin φi cos α + cos φi sin α) ,\nfor a global rotation angle α. Comparing this expression\nwith Eq. (10), the components of the principal compo-\nnent eigenvectors in Fig. 10 therefore indicate the global\nrotation α along which PCA learns the magnetization.\nBy considering this global rotation, the components of\nthese principal component eigenvectors can be used to\nanalytically determine the value of α; the histogram for\nthis extraction is shown in Fig. 11.\nWhen this global\nrotation is accounted for, the principal eigenvectors do\ntake values of only 1s or 0s, as shown in Fig. 12. This\nproves that PCA is learning the magnetization of the\nXY model along two orthogonal directions; this same\nanalysis can be applied to the principal components of\nthe MXYGG model to extract the local rotations pro-\nduced by the gauge variables {bi}, giving the histogram\nin Fig. 5.\n0\n500\n1000\n1500\ni\n1.0\n0.5\n0.0\n0.5\n1.0\nu(1)\ni\n0\n500\n1000\n1500\ni\n1.0\n0.5\n0.0\n0.5\n1.0\nu(2)\ni\n(a)\n(b)\nFIG. 12. Components of the (a) ﬁrst and (b) second principal\ncomponent eigenvectors, ⃗u(1) and ⃗u(2), for the regular XY\nmodel after applying a global rotation.\nThe two branches\nof each graph correspond to coeﬃcients for {cos (φi)} or\n{sin (φi)} data.\n∗These authors contributed equal work.\n[1] L. Wang, Phys. Rev. B 94, 195105 (2016).\n[2] J. Carrasquilla and R. G. Melko, Nat. Phys 13, 431\n(2017).\n[3] P. Mehta, M. Bukov, C.-H. Wang, A. r. G. R. Day,\nC. Richardson, C. K. Fisher,\nand D. J. Schwab, Phys.\nRep. 810, 1 (2019).\n[4] V. Dunjko and H. J. Briegel, Reports on Progress in\nPhysics 81, 074001 (2018).\n[5] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld,\nN. Tishby, L. Vogt-Maranto,\nand L. Zdeborov´a, Rev.\nMod. Phys. 91, 045002 (2019).\n[6] K.-W. Zhao, W.-H. Kao, K.-H. Wu, and Y.-J. Kao, Phys.\nRev. E 99, 062106 (2019).\n[7] J. Greitemann, K. Liu, and L. Pollet, Phys. Rev. B 99,\n060404 (2019).\n[8] M. J. S. Beach, A. Golubeva,\nand R. G. Melko, Phys.\nRev. B 97, 045207 (2018).\n[9] J. Greitemann, K. Liu, L. D. C. Jaubert, H. Yan,\nN. Shannon,\nand L. Pollet, Phys. Rev. B 100, 174408\n(2019).\n[10] P. Ponte and R. G. Melko, Phys. Rev. B 96, 205146\n(2017).\n[11] K. Liu, J. Greitemann, and L. Pollet, Phys. Rev. B 99,\n104410 (2019).\n[12] H. Th´eveniaut and F. Alet, Phys. Rev. B 100, 224202\n(2019).\n[13] A. Canabarro, S. Brito, and R. Chaves, Phys. Rev. Lett.\n122, 200401 (2019).\n[14] X. Liang, W.-Y. Liu, P.-Z. Lin, G.-C. Guo, Y.-S. Zhang,\nand L. He, Phys. Rev. B 98, 104426 (2018).\n[15] M. August and X. Ni, Phys. Rev. A 95, 012335 (2017).\n[16] A. Decelle, V. Martin-Mayor, and B. Seoane, Phys. Rev.\nE 100, 050102 (2019).\n[17] H. Xu, J. Li, L. Liu, Y. Wang, H. Yuan, and X. Wang,\nnpj Quantum Inf. 5, 82 (2019).\n[18] A. Bohrdt, C. S. Chiu, G. Ji, M. Xu, D. Greif, M. Greiner,\nE. Demler, F. Grusdt, and M. Knap, Nat. Phys 15, 921\n(2019).\n[19] C. Casert, T. Vieijra, J. Nys, and J. Ryckebusch, Phys.\nRev. E 99, 023304 (2019).\n8\n[20] A. Canabarro, F. F. Fanchini, A. L. Malvezzi, R. Pereira,\nand R. Chaves, Phys. Rev. B 100, 045129 (2019).\n[21] S. J. Wetzel and M. Scherzer, Phys. Rev. B 96, 184410\n(2017).\n[22] C. Wang and H. Zhai, Phys. Rev. B 96, 144432 (2017).\n[23] C. Wang and H. Zhai, Front. Phys. 13, 130507 (2018).\n[24] W. Hu, R. R. P. Singh, and R. T. Scalettar, Phys. Rev.\nE 95, 062122 (2017).\n[25] S. J. Wetzel, Phys. Rev. E 96, 022140 (2017).\n[26] W. Zhang, J. Liu,\nand T.-C. Wei, Phys. Rev. E 99,\n032142 (2019).\n[27] Y. Iwasaki, R. Sawada, V. Stanev, M. Ishida, A. Kiri-\nhara, Y. Omori, H. Someya, I. Takeuchi, E. Saitoh, and\nS. Yorozu, npj Comput. Mater. 5, 103 (2019).\n[28] P. Ponte and R. G. Melko, Phys. Rev. B 96, 205146\n(2017).\n[29] Y. Wu and H. Zhai, arXiv:1904.05067.\n[30] T.\nHou,\nK.\nY.\nM.\nWong,\nand\nH.\nHuang,\narXiv:1904.13052.\n[31] R. B. Jadrich, B. A. Lindquist, and T. M. Truskett, J.\nChem. Phys. 149, 194109 (2018).\n[32] S. N. Shah, J. Phys. Commun. 3, 075006 (2019).\n[33] D. Mattis, Physics Letters A 56, 421 (1976).\n[34] K. H. Fischer and J. A. Hertz, Spin Glasses (Cambridge\nUniversity Press, 1991).\n[35] K. Pearson, Philos. Mag. 2, 559 (1901).\n[36] M. J. P. Gingras, Phys. Rev. B 44, 7139 (1991).\n[37] M. J. P. Gingras, Phys. Rev. B 43, 13747 (1991).\n[38] M. J. P. Gingras, Phys. Rev. B 45, 7547 (1992).\n[39] It is easier to determine PCA’s success with determining\nthe gauge variables if they are taken from a discrete dis-\ntribution, which can be clearly identiﬁed in a histogram\nsuch as Fig. 5, as opposed to a continuous distribution.\n[40] H. Nishimori, Statistical Physics of Spin Glasses and In-\nformation Processing: An Introduction (Oxford Univer-\nsity Press, 2001).\n",
  "categories": [
    "cond-mat.dis-nn",
    "physics.comp-ph"
  ],
  "published": "2020-02-28",
  "updated": "2020-02-28"
}