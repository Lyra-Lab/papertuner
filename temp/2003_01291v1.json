{
  "id": "http://arxiv.org/abs/2003.01291v1",
  "title": "Overall error analysis for the training of deep neural networks via stochastic gradient descent with random initialisation",
  "authors": [
    "Arnulf Jentzen",
    "Timo Welti"
  ],
  "abstract": "In spite of the accomplishments of deep learning based algorithms in numerous\napplications and very broad corresponding research interest, at the moment\nthere is still no rigorous understanding of the reasons why such algorithms\nproduce useful results in certain situations. A thorough mathematical analysis\nof deep learning based algorithms seems to be crucial in order to improve our\nunderstanding and to make their implementation more effective and efficient. In\nthis article we provide a mathematically rigorous full error analysis of deep\nlearning based empirical risk minimisation with quadratic loss function in the\nprobabilistically strong sense, where the underlying deep neural networks are\ntrained using stochastic gradient descent with random initialisation. The\nconvergence speed we obtain is presumably far from optimal and suffers under\nthe curse of dimensionality. To the best of our knowledge, we establish,\nhowever, the first full error analysis in the scientific literature for a deep\nlearning based algorithm in the probabilistically strong sense and, moreover,\nthe first full error analysis in the scientific literature for a deep learning\nbased algorithm where stochastic gradient descent with random initialisation is\nthe employed optimisation method.",
  "text": "Overall error analysis for the training of\ndeep neural networks via stochastic\ngradient descent with random initialisation\nArnulf Jentzen1 and Timo Welti2\n1 Faculty of Mathematics and Computer Science, University of M¨unster,\nM¨unster, Germany; e-mail: ajentzen a⃝uni-muenster.de\n2 SAM, Department of Mathematics, ETH Z¨urich,\nZ¨urich, Switzerland; e-mail: twelti a⃝twelti.org\nMarch 4, 2020\nAbstract\nIn spite of the accomplishments of deep learning based algorithms in numerous\napplications and very broad corresponding research interest, at the moment there\nis still no rigorous understanding of the reasons why such algorithms produce useful\nresults in certain situations. A thorough mathematical analysis of deep learning\nbased algorithms seems to be crucial in order to improve our understanding and\nto make their implementation more eﬀective and eﬃcient. In this article we pro-\nvide a mathematically rigorous full error analysis of deep learning based empirical\nrisk minimisation with quadratic loss function in the probabilistically strong sense,\nwhere the underlying deep neural networks are trained using stochastic gradient\ndescent with random initialisation. The convergence speed we obtain is presumably\nfar from optimal and suﬀers under the curse of dimensionality. To the best of our\nknowledge, we establish, however, the ﬁrst full error analysis in the scientiﬁc liter-\nature for a deep learning based algorithm in the probabilistically strong sense and,\nmoreover, the ﬁrst full error analysis in the scientiﬁc literature for a deep learning\nbased algorithm where stochastic gradient descent with random initialisation is the\nemployed optimisation method.\nKeywords: deep learning, deep neural networks, empirical risk minimisation,\nfull error analysis, approximation, generalisation, optimisation, strong\nconvergence, stochastic gradient descent, random initialisation\n1\narXiv:2003.01291v1  [math.ST]  3 Mar 2020\nContents\n1\nIntroduction\n3\n2\nBasics on deep neural networks (DNNs)\n6\n2.1\nVectorised description of DNNs\n. . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.2\nActivation functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3\nRectiﬁed DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3\nAnalysis of the approximation error\n8\n3.1\nA covering number estimate . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.2\nConvergence rates for the approximation error . . . . . . . . . . . . . . . .\n10\n4\nAnalysis of the generalisation error\n12\n4.1\nMonte Carlo estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4.2\nUniform strong error estimates for random ﬁelds . . . . . . . . . . . . . . .\n14\n4.3\nStrong convergence rates for the generalisation error . . . . . . . . . . . . .\n18\n5\nAnalysis of the optimisation error\n23\n5.1\nProperties of the gamma and the beta function\n. . . . . . . . . . . . . . .\n24\n5.2\nStrong convergence rates for the optimisation error\n. . . . . . . . . . . . .\n27\n6\nAnalysis of the overall error\n31\n6.1\nOverall error decomposition . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n6.2\nFull strong error analysis for the training of DNNs . . . . . . . . . . . . . .\n33\n6.3\nFull strong error analysis with optimisation via stochastic gradient descent\n40\n2\n1\nIntroduction\nDeep learning based algorithms have been applied extremely successfully to overcome fun-\ndamental challenges in many diﬀerent areas, such as image recognition, natural language\nprocessing, game intelligence, autonomous driving, and computational advertising, just\nto name a few. In line with this, researchers from a wide range of diﬀerent ﬁelds, includ-\ning, for example, computer science, mathematics, chemistry, medicine, and ﬁnance, are\ninvesting signiﬁcant eﬀorts into studying such algorithms and employing them to tackle\nchallenges arising in their ﬁelds. In spite of this broad research interest and the accom-\nplishments of deep learning based algorithms in numerous applications, at the moment\nthere is still no rigorous understanding of the reasons why such algorithms produce useful\nresults in certain situations. Consequently, there is no rigorous way to predict, before ac-\ntually implementing a deep learning based algorithm, in which situations it might perform\nreliably and in which situations it might fail. This necessitates in many cases a trial-and-\nerror approach in order to move forward, which can cost a lot of time and resources. A\nthorough mathematical analysis of deep learning based algorithms (in scenarios where it\nis possible to formulate such an analysis) seems to be crucial in order to make progress on\nthese issues. Moreover, such an analysis may lead to new insights that enable the design\nof more eﬀective and eﬃcient algorithms.\nThe aim of this article is to provide a mathematically rigorous full error analysis of deep\nlearning based empirical risk minimisation with quadratic loss function in the probabilis-\ntically strong sense, where the underlying deep neural networks (DNNs) are trained using\nstochastic gradient descent (SGD) with random initialisation (cf. Theorem 1.1 below).\nFor a brief illustration of deep learning based empirical risk minimisation with quadratic\nloss function, consider natural numbers d, d ∈N, a probability space (Ω, F, P), random\nvariables X : Ω→[0, 1]d and Y : Ω→[0, 1], and a measurable function E : [0, 1]d →[0, 1]\nsatisfying P-a.s. that E(X) = E[Y |X]. The goal is to ﬁnd a DNN with appropriate ar-\nchitecture and appropriate parameter vector θ ∈Rd (collecting its weights and biases)\nsuch that its realisation Nθ : Rd →R approximates the target function E well in the\nsense that the error E[|Nθ(X)−E(X)|p] =\nR\n[0,1]d|Nθ(x)−E(x)|p PX(dx) ∈[0, ∞) for some\np ∈[1, ∞) is as small as possible. In other words, given X we want Nθ(X) to predict Y as\nreliably as possible. Due to the well-known bias–variance decomposition (cf., e.g., Beck,\nJentzen, & Kuckuck [10, Lemma 4.1]), for the case p = 2 minimising the error function\nRd ∋θ 7→E[|Nθ(X) −E(X)|2] ∈[0, ∞) is equivalent to minimising the risk function\nRd ∋θ 7→E[|Nθ(X) −Y |2] ∈[0, ∞) (corresponding to a quadratic loss function). Since\nin practice the joint distribution of X and Y is typically not known, the risk function is\nreplaced by an empirical risk function based on i.i.d. training samples of (X, Y ). This\nempirical risk is then approximatively minimised using an optimisation method such as\nSGD. As is often the case for deep learning based algorithms, the overall error arising\nfrom this procedure consists of the following three diﬀerent parts (cf. [10, Lemma 4.3]\nand Proposition 6.1 below): (i) the approximation error (cf., e.g., [5, 6, 14, 21, 24, 37,\n39, 54–58, 66, 75] and the references in the introductory paragraph in Section 3), which\narises from approximating the target function E by the considered class of DNNs, (ii) the\ngeneralisation error (cf., e.g., [7, 10, 13, 23, 31–33, 52, 67, 87, 92]), which arises from\nreplacing the true risk by the empirical risk, and (iii) the optimisation error (cf., e.g., [2,\n4, 8, 10, 12, 18, 25, 26, 28, 29, 38, 60, 62, 63, 65, 88, 97, 98]), which arises from computing\nonly an approximate minimiser using the selected optimisation method.\nIn this work we derive strong convergence rates for the approximation error, the gen-\neralisation error, and the optimisation error separately and combine these ﬁndings to\n3\nestablish strong convergence results for the overall error (cf. Subsections 6.2 and 6.3),\nas illustrated in Theorem 1.1 below. The convergence speed we obtain (cf. (4) in The-\norem 1.1) is presumably far from optimal, suﬀers under the curse of dimensionality (cf.,\ne.g., Bellman [11] and Novak & Wo´zniakowski [73, Chapter 1; 74, Chapter 9]), and is,\nas a consequence, very slow. To the best of our knowledge, Theorem 1.1 is, however,\nthe ﬁrst full error result in the scientiﬁc literature for a deep learning based algorithm\nin the probabilistically strong sense and, moreover, the ﬁrst full error result in the scien-\ntiﬁc literature for a deep learning based algorithm where SGD with random initialisation\nis the employed optimisation method. We now present Theorem 1.1, the statement of\nwhich is entirely self-contained, before we add further explanations and intuitions for the\nmathematical objects that are introduced.\nTheorem 1.1. Let d, d, L, J, M, K, N ∈N, γ, L ∈R, c ∈[max{2, L}, ∞), l = (l0, . . . , lL)\n∈NL+1, N ⊆{0, . . . , N}, assume 0 ∈N, l0 = d, lL = 1, and d ≥PL\ni=1 li(li−1 + 1), for\nevery m, n ∈N, s ∈N0, θ = (θ1, . . . , θd) ∈Rd with d ≥s + mn + m let Aθ,s\nm,n : Rn →Rm\nsatisfy for all x = (x1, . . . , xn) ∈Rn that\nAθ,s\nm,n(x) =\n\n\n\n\n\nθs+1\nθs+2\n· · ·\nθs+n\nθs+n+1\nθs+n+2\n· · ·\nθs+2n\n...\n...\n...\n...\nθs+(m−1)n+1\nθs+(m−1)n+2\n· · ·\nθs+mn\n\n\n\n\n\n\n\n\n\n\nx1\nx2\n...\nxn\n\n\n\n\n+\n\n\n\n\n\nθs+mn+1\nθs+mn+2\n...\nθs+mn+m\n\n\n\n\n,\n(1)\nlet ai : Rli →Rli, i ∈{1, . . . , L}, satisfy for all i ∈N ∩[0, L), x = (x1, . . . , xli) ∈\nRli that ai(x) = (max{x1, 0}, . . . , max{xli, 0}), assume for all x ∈R that aL(x) =\nmax{min{x, 1}, 0}, for every θ ∈Rd let Nθ : Rd →R satisfy Nθ = aL ◦Aθ,PL−1\ni=1 li(li−1+1)\nlL,lL−1\n◦\naL−1 ◦Aθ,PL−2\ni=1 li(li−1+1)\nlL−1,lL−2\n◦. . . ◦a1 ◦Aθ,0\nl1,l0, let (Ω, F, P) be a probability space, let Xk,n\nj\n: Ω→\n[0, 1]d, k, n, j ∈N0, and Y k,n\nj\n: Ω→[0, 1], k, n, j ∈N0, be functions, assume that\n(X0,0\nj , Y 0,0\nj\n), j ∈N, are i.i.d. random variables, let E : [0, 1]d →[0, 1] satisfy P-a.s.\nthat E(X0,0\n1 ) = E[Y 0,0\n1\n|X0,0\n1 ], assume for all x, y ∈[0, 1]d that |E(x) −E(y)| ≤L∥x −\ny∥1, let Θk,n : Ω→Rd, k, n ∈N0, and k: Ω→(N0)2 be random variables, assume\n\u0000S∞\nk=1 Θk,0(Ω)\n\u0001\n⊆[−c, c]d, assume that Θk,0, k ∈N, are i.i.d., assume that Θ1,0 is\ncontinuous uniformly distributed on [−c, c]d, let Rk,n\nJ : Rd × Ω→[0, ∞), k, n, J ∈N0,\nand Gk,n : Rd × Ω→Rd, k, n ∈N, satisfy for all k, n ∈N, ω ∈Ω, θ ∈{ϑ ∈\nRd : (Rk,n\nJ (·, ω): Rd →[0, ∞) is diﬀerentiable at ϑ)} that Gk,n(θ, ω) = (∇θRk,n\nJ )(θ, ω),\nassume for all k, n ∈N that Θk,n = Θk,n−1 −γGk,n(Θk,n−1), and assume for all k, n ∈N0,\nJ ∈N, θ ∈Rd, ω ∈Ωthat\nRk,n\nJ (θ, ω) = 1\nJ\n\u0014 JP\nj=1\n|Nθ(Xk,n\nj\n(ω)) −Y k,n\nj\n(ω)|2\n\u0015\nand\n(2)\nk(ω) ∈arg min(l,m)∈{1,...,K}×N, ∥Θl,m(ω)∥∞≤c R0,0\nM (Θl,m(ω), ω).\n(3)\nThen\nE\nhZ\n[0,1]d|NΘk(x) −E(x)| PX0,0\n1 (dx)\ni\n≤\ndc3\n[min{L, l1, . . . , lL−1}]\n1/d + c3L(∥l∥∞+ 1) ln(eM)\nM\n1/4\n+ L(∥l∥∞+ 1)LcL+1\nK[(2L)−1(∥l∥∞+1)−2] .\n(4)\nRecall that we denote for every p ∈[1, ∞] by ∥·∥p :\n\u0000S∞\nn=1 Rn\u0001\n→[0, ∞) the p-norm of\nvectors in S∞\nn=1 Rn (cf. Deﬁnition 3.1). In addition, note that the function Ω× [0, 1]d ∋\n4\n(ω, x) 7→|NΘk(ω)(ω)(x) −E(x)| ∈[0, ∞) is measurable (cf. Lemma 6.2) and that the\nexpression on the left hand side of (4) above is thus well-deﬁned. Theorem 1.1 follows\ndirectly from Corollary 6.9 in Subsection 6.3, which, in turn, is a consequence of the main\nresult of this article, Theorem 6.5 in Subsection 6.2.\nIn the following we provide additional explanations and intuitions for Theorem 1.1.\nFor every θ ∈Rd the functions Nθ : Rd →R are realisations of fully connected feedforward\nartiﬁcial neural networks with L+1 layers consisting of an input layer of dimension l0 = d,\nof L −1 hidden layers of dimensions l1, . . . , lL−1, respectively, and of an output layer of\ndimension lL = 1 (cf. Deﬁnition 2.8). The weights and biases stored in the DNN param-\neter vector θ ∈Rd determine the corresponding L aﬃne linear transformations (cf. (1)\nabove). As activation functions we employ the multidimensional versions a1, . . . , aL−1\n(cf. Deﬁnition 2.3) of the rectiﬁer function R ∋x 7→max{x, 0} ∈R (cf. Deﬁnition 2.4)\njust in front of each of the hidden layers and the clipping function aL (cf. Deﬁnition 2.6)\njust in front of the output layer. Furthermore, observe that we assume the target func-\ntion E : [0, 1]d →[0, 1], the values of which we intend to approximately predict with the\ntrained DNN, to be Lipschitz continuous with Lipschitz constant L. Moreover, for every\nk, n ∈N0, J ∈N the function Rk,n\nJ : Rd × Ω→[0, ∞) is the empirical risk based on the J\ntraining samples (Xk,n\nj\n, Y k,n\nj\n), j ∈{1, . . . , J} (cf. (2) above). Derived from the empirical\nrisk, for every k, n ∈N the function Gk,n : Rd × Ω→Rd is a (generalised) gradient of\nthe empirical risk Rk,n\nJ\nwith respect to its ﬁrst argument, that is, with respect to the\nDNN parameter vector θ ∈Rd. These gradients are required in order to formulate the\ntraining dynamics of the (random) DNN parameter vectors Θk,n ∈Rd, k ∈N, n ∈N0,\ngiven by the SGD optimisation method with learning rate γ. Note that the subscript\nn ∈N0 of these SGD iterates (i.e., DNN parameter vectors) is the current training step\nnumber, whereas the subscript k ∈N counts the number of times the SGD iteration\nhas been started from scratch so far. Such a new start entails the corresponding initial\nDNN parameter vector Θk,0 ∈Rd to be drawn continuous uniformly from the hypercube\n[−c, c]d, in accordance with Xavier initialisation (cf. Glorot & Bengio [41]). The (ran-\ndom) double index k ∈N × N0 represents the ﬁnal choice made for the DNN parameter\nvector Θk ∈Rd (cf. (4) above), concluding the training procedure, and is selected as\nfollows. During training the empirical risk R0,0\nM has been calculated for the subset of the\nSGD iterates indexed by N ⊆{0, . . . , N} provided that they have not left the hypercube\n[−c, c]d (cf. (3) above). After the SGD iteration has been started and ﬁnished K times\n(with maximally N training steps in each case) the ﬁnal choice for the DNN parameter\nvector Θk ∈Rd is made among those SGD iterates for which the calculated empirical risk\nis minimal (cf. (3) above). Observe that we use mini-batches of size J consisting, during\nSGD iteration number k ∈{1, . . . , K} for training step number n ∈{1, . . . , N}, of the\ntraining samples (Xk,n\nj\n, Y k,n\nj\n), j ∈{1, . . . , J}, and that we reserve the M training samples\n(X0,0\nj , Y 0,0\nj\n), j ∈{1, . . . , M}, for checking the value of the empirical risk R0,0\nM . Regarding\nthe conclusion of Theorem 1.1, note that the left hand side of (4) is the expectation of the\noverall L1-error, that is, the expected L1-distance between the trained DNN NΘk and the\ntarget function E. It is bounded from above by the right hand side of (4), which consists\nof following three summands: (i) the ﬁrst summand corresponds to the approximation\nerror and converges to zero as the number of hidden layers L −1 as well as the hidden\nlayer dimensions l1, . . . , lL−1 increase to inﬁnity, (ii) the second summand corresponds to\nthe generalisation error and converges to zero as number of training samples M used\nfor calculating the empirical risk increases to inﬁnity, and (iii) the third summand corre-\nsponds to the optimisation error and converges to zero as total number of times K the\nSGD iteration has been started from scratch increases to inﬁnity. We would like to point\n5\nout that the the second summand (corresponding to the generalisation error) does not\nsuﬀer under the curse of dimensionality with respect to any of the variables involved.\nThe main result of this article, Theorem 6.5 in Subsection 6.2, covers, in comparison\nwith Theorem 1.1, the more general cases where Lp-norms of the overall L2-error instead\nof the expectation of the overall L1-error are considered (cf. (167) in Theorem 6.5), where\nthe training samples are not restricted to unit hypercubes, and where a general stochastic\napproximation algorithm (cf., e.g., Robbins & Monro [83]) with random initialisation is\nused for optimisation. Our convergence proof for the optimisation error relies, in fact, on\nthe convergence of the Minimum Monte Carlo method (cf. Proposition 5.6 in Section 5)\nand thus only exploits random initialisation but not the speciﬁc dynamics of the employed\noptimisation method (cf. (155) in the proof of Proposition 6.3). In this regard, note that\nTheorem 1.1 above also includes the application of deterministic gradient descent instead\nof SGD for optimisation since we do not assume the samples used for gradient iterations\nto be i.i.d. Parts of our derivation of Theorem 1.1 and Theorem 6.5, respectively, are\ninspired by Beck, Jentzen, & Kuckuck [10], Berner, Grohs, & Jentzen [13], and Cucker &\nSmale [23].\nThis article is structured in the following way. Section 2 recalls some basic deﬁnitions\nrelated to DNNs and thereby introduces the corresponding notation we use in the sub-\nsequent parts of this article. In Section 3 we examine the approximation error and, in\nparticular, establish a convergence result for the approximation of Lipschitz continuous\nfunctions by DNNs. The following section, Section 4, contains our strong convergence\nanalysis of the generalisation error. In Section 5, in turn, we address the optimisation\nerror and derive in connection with this strong convergence rates for the Minimum Monte\nCarlo method. Finally, we combine in Section 6 a decomposition of the overall error (cf.\nSubsection 6.1) with our results for the diﬀerent error sources from Sections 3, 4, and 5 to\nprove strong convergence results for the overall error. The employed optimisation method\nis initially allowed to be a general stochastic approximation algorithm with random ini-\ntialisation (cf. Subsection 6.2) and is afterwards specialised to the setting of SGD with\nrandom initialisation (cf. Subsection 6.3).\n2\nBasics on deep neural networks (DNNs)\nIn this section we present the mathematical description of DNNs which we use throughout\nthe remainder of this article. It is a vectorised description in the sense that all the weights\nand biases associated to the DNN under consideration are collected in a single parameter\nvector θ ∈Rd with d ∈N suﬃciently large (cf. Deﬁnitions 2.2 and 2.8). The content\nof this section is taken from Beck, Jentzen, & Kuckuck [10, Section 2.1] and is based\non well-known material from the scientiﬁc literature, see, e.g., Beck et al. [8], Beck, E, &\nJentzen [9], Berner, Grohs, & Jentzen [13], E, Han, & Jentzen [30], Goodfellow, Bengio, &\nCourville [43], and Grohs et al. [46]. In particular, Deﬁnition 2.1 is [10, Deﬁnition 2.1] (cf.,\ne.g., (25) in [9]), Deﬁnition 2.2 is [10, Deﬁnition 2.2] (cf., e.g., (26) in [9]), Deﬁnition 2.3\nis [10, Deﬁnition 2.3] (cf., e.g., [46, Deﬁnition 2.2]), and Deﬁnitions 2.4, 2.5, 2.6, 2.7,\nand 2.8 are [10, Deﬁnitions 2.4, 2.5, 2.6, 2.7, and 2.8] (cf., e.g., [13, Setting 2.5] and [43,\nSection 6.3]).\n2.1\nVectorised description of DNNs\nDeﬁnition 2.1 (Aﬃne function). Let d, m, n ∈N, s ∈N0, θ = (θ1, θ2, . . . , θd) ∈Rd\nsatisfy d ≥s + mn + m. Then we denote by Aθ,s\nm,n : Rn →Rm the function which satisﬁes\n6\nfor all x = (x1, x2, . . . , xn) ∈Rn that\nAθ,s\nm,n(x) =\n\n\n\n\n\n\n\nθs+1\nθs+2\n· · ·\nθs+n\nθs+n+1\nθs+n+2\n· · ·\nθs+2n\nθs+2n+1\nθs+2n+2\n· · ·\nθs+3n\n...\n...\n...\n...\nθs+(m−1)n+1\nθs+(m−1)n+2\n· · ·\nθs+mn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\n...\nxn\n\n\n\n\n\n\n\n+\n\n\n\n\n\n\n\nθs+mn+1\nθs+mn+2\nθs+mn+3\n...\nθs+mn+m\n\n\n\n\n\n\n\n(5)\n=\n\u0012\u0014 nP\ni=1\nθs+ixi\n\u0015\n+ θs+mn+1,\n\u0014 nP\ni=1\nθs+n+ixi\n\u0015\n+ θs+mn+2, . . . ,\n\u0014 nP\ni=1\nθs+(m−1)n+ixi\n\u0015\n+ θs+mn+m\n\u0013\n.\nDeﬁnition 2.2 (Fully connected feedforward artiﬁcial neural network). Let d, L, l0, l1, . . . ,\nlL ∈N, s ∈N0, θ ∈Rd satisfy d ≥s + PL\ni=1 li(li−1 + 1) and let ai : Rli →Rli,\ni ∈{1, 2, . . . , L}, be functions. Then we denote by N θ,s,l0\na1,a2,...,aL : Rl0 →RlL the function\nwhich satisﬁes for all x ∈Rl0 that\n\u0000N θ,s,l0\na1,a2,...,aL\n\u0001\n(x) =\n\u0000aL ◦A\nθ,s+PL−1\ni=1 li(li−1+1)\nlL,lL−1\n◦aL−1 ◦A\nθ,s+PL−2\ni=1 li(li−1+1)\nlL−1,lL−2\n◦. . .\n. . . ◦a2 ◦Aθ,s+l1(l0+1)\nl2,l1\n◦a1 ◦Aθ,s\nl1,l0\n\u0001\n(x)\n(6)\n(cf. Deﬁnition 2.1).\n2.2\nActivation functions\nDeﬁnition 2.3 (Multidimensional version). Let d ∈N and let a: R →R be a func-\ntion.\nThen we denote by Ma,d : Rd →Rd the function which satisﬁes for all x =\n(x1, x2, . . . , xd) ∈Rd that\nMa,d(x) = (a(x1), a(x2), . . . , a(xd)).\n(7)\nDeﬁnition 2.4 (Rectiﬁer function). We denote by r: R →R the function which satisﬁes\nfor all x ∈R that\nr(x) = max{x, 0}.\n(8)\nDeﬁnition 2.5 (Multidimensional rectiﬁer function). Let d ∈N. Then we denote by\nRd : Rd →Rd the function given by\nRd = Mr,d\n(9)\n(cf. Deﬁnitions 2.3 and 2.4).\nDeﬁnition 2.6 (Clipping function). Let u ∈[−∞, ∞), v ∈(u, ∞]. Then we denote by\ncu,v : R →R the function which satisﬁes for all x ∈R that\ncu,v(x) = max{u, min{x, v}}.\n(10)\nDeﬁnition 2.7 (Multidimensional clipping function). Let d ∈N, u ∈[−∞, ∞), v ∈\n(u, ∞]. Then we denote by Cu,v,d : Rd →Rd the function given by\nCu,v,d = Mcu,v,d\n(11)\n(cf. Deﬁnitions 2.3 and 2.6).\n7\n2.3\nRectiﬁed DNNs\nDeﬁnition 2.8 (Rectiﬁed clipped DNN). Let d, L ∈N, u ∈[−∞, ∞), v ∈(u, ∞],\nl = (l0, l1, . . . , lL) ∈NL+1, θ ∈Rd satisfy d ≥PL\ni=1 li(li−1 + 1). Then we denote by\nN θ,l\nu,v : Rl0 →RlL the function which satisﬁes for all x ∈Rl0 that\nN θ,l\nu,v (x) =\n(\u0000N θ,0,l0\nCu,v,lL\n\u0001\n(x)\n: L = 1\n\u0000N θ,0,l0\nRl1,Rl2,...,RlL−1,Cu,v,lL\n\u0001\n(x)\n: L > 1\n(12)\n(cf. Deﬁnitions 2.2, 2.5, and 2.7).\n3\nAnalysis of the approximation error\nThis section is devoted to establishing a convergence result for the approximation of Lips-\nchitz continuous functions by DNNs (cf. Proposition 3.5). More precisely, Proposition 3.5\nestablishes that a Lipschitz continuous function deﬁned on a d-dimensional hypercube\nfor d ∈N can be approximated by DNNs with convergence rate 1/d with respect to a\nparameter A ∈(0, ∞) that bounds the architecture size (that is, depth and width) of the\napproximating DNN from below. Key ingredients of the proof of Proposition 3.5 are Beck,\nJentzen, & Kuckuck [10, Corollary 3.8] as well as the elementary covering number estimate\nin Lemma 3.3. In order to improve the accessibility of Lemma 3.3, we recall the deﬁnition\nof covering numbers associated to a metric space in Deﬁnition 3.2, which is [10, Deﬁni-\ntion 3.11]. Lemma 3.3 provides upper bounds for the covering numbers of hypercubes\nequipped with the metric induced by the p-norm (cf. Deﬁnition 3.1) for p ∈[1, ∞] and is\na generalisation of Berner, Grohs, & Jentzen [13, Lemma 2.7] (cf. Cucker & Smale [23,\nProposition 5] and [10, Proposition 3.12]). Furthermore, we present in Lemma 3.4 an ele-\nmentary upper bound for the error arising when Lipschitz continuous functions deﬁned on\na hypercube are approximated by certain DNNs. Additional DNN approximation results\ncan be found, e.g., in [3, 5, 6, 14–17, 19–21, 24, 27, 34–37, 39, 44–51, 53–59, 61, 64, 66,\n68–72, 75–80, 82, 84–86, 89–91, 93, 95, 96] and the references therein.\n3.1\nA covering number estimate\nDeﬁnition 3.1 (p-norm). We denote by ∥·∥p :\n\u0000S∞\nd=1 Rd\u0001\n→[0, ∞), p ∈[1, ∞], the func-\ntions which satisfy for all p ∈[1, ∞), d ∈N, θ = (θ1, θ2, . . . , θd) ∈Rd that\n∥θ∥p =\n\u0012 dP\ni=1\n|θi|p\n\u00131/p\nand\n∥θ∥∞=\nmax\ni∈{1,2,...,d}|θi|.\n(13)\nDeﬁnition 3.2 (Covering number). Let (E, δ) be a metric space and let r ∈[0, ∞]. Then\nwe denote by C(E,δ),r ∈N0∪{∞} (we denote by CE,r ∈N0∪{∞}) the extended real number\ngiven by\nC(E,δ),r = inf\n\u0012\u001a\nn ∈N0 :\n\u0014\n∃A ⊆E :\n\u0012(|A| ≤n) ∧(∀x ∈E :\n∃a ∈A: δ(a, x) ≤r)\n\u0013\u0015\u001b\n∪{∞}\n\u0013\n.\n(14)\nLemma 3.3. Let d ∈N, a ∈R, b ∈(a, ∞), r ∈(0, ∞), for every p ∈[1, ∞] let\nδp : ([a, b]d)×([a, b]d) →[0, ∞) satisfy for all x, y ∈[a, b]d that δp(x, y) = ∥x−y∥p, and let\n⌈·⌉: [0, ∞) →N0 satisfy for all x ∈[0, ∞) that ⌈x⌉= min([x, ∞)∩N0) (cf. Deﬁnition 3.1).\nThen\n8\n(i) it holds for all p ∈[1, ∞) that\nC([a,b]d,δp),r ≤\n\u0010l\nd1/p(b−a)\n2r\nm\u0011d\n≤\n(\n1\n: r ≥d(b−a)/2\n\u0000 d(b−a)\nr\n\u0001d\n: r < d(b−a)/2\n(15)\nand\n(ii) it holds that\nC([a,b]d,δ∞),r ≤\n\u0000\u0006 b−a\n2r\n\u0007\u0001d ≤\n(\n1\n: r ≥(b−a)/2\n\u0000 b−a\nr\n\u0001d\n: r < (b−a)/2\n(16)\n(cf. Deﬁnition 3.2).\nProof of Lemma 3.3. Throughout this proof let (Np)p∈[1,∞] ⊆N satisfy for all p ∈[1, ∞)\nthat\nNp =\nl\nd1/p(b−a)\n2r\nm\nand\nN∞=\n\u0006 b−a\n2r\n\u0007\n,\n(17)\nfor every N ∈N, i ∈{1, 2, . . . , N} let gN,i ∈[a, b] be given by gN,i = a + (i−1/2)(b−a)/N, and\nfor every p ∈[1, ∞] let Ap ⊆[a, b]d be given by Ap = {gNp,1, gNp,2, . . . , gNp,Np}d. Observe\nthat it holds for all N ∈N, i ∈{1, 2, . . . , N}, x ∈[a + (i−1)(b−a)/N, gN,i] that\n|x −gN,i| = a + (i−1/2)(b−a)\nN\n−x ≤a + (i−1/2)(b−a)\nN\n−\n\u0000a + (i−1)(b−a)\nN\n\u0001\n= b−a\n2N .\n(18)\nIn addition, note that it holds for all N ∈N, i ∈{1, 2, . . . , N}, x ∈[gN,i, a + i(b−a)/N] that\n|x −gN,i| = x −\n\u0000a + (i−1/2)(b−a)\nN\n\u0001\n≤a + i(b−a)\nN\n−\n\u0000a + (i−1/2)(b−a)\nN\n\u0001\n= b−a\n2N .\n(19)\nCombining (18) and (19) implies for all N ∈N, i ∈{1, 2, . . . , N}, x ∈[a + (i−1)(b−a)/N, a +\ni(b−a)/N] that |x −gN,i| ≤(b−a)/(2N). This proves that for every N ∈N, x ∈[a, b] there\nexists y ∈{gN,1, gN,2, . . . , gN,N} such that\n|x −y| ≤b−a\n2N .\n(20)\nThis, in turn, establishes that for every p ∈[1, ∞), x = (x1, x2, . . . , xd) ∈[a, b]d there\nexists y = (y1, y2, . . . , yd) ∈Ap such that\nδp(x, y) = ∥x −y∥p =\n\u0012 dP\ni=1\n|xi −yi|p\n\u00131/p\n≤\n\u0012 dP\ni=1\n(b−a)p\n(2Np)p\n\u00131/p\n= d1/p(b−a)\n2Np\n≤d1/p(b−a)2r\n2d1/p(b−a) = r. (21)\nFurthermore, again (20) shows that for every x = (x1, x2, . . . , xd) ∈[a, b]d there exists\ny = (y1, y2, . . . , yd) ∈A∞such that\nδ∞(x, y) = ∥x −y∥∞=\nmax\ni∈{1,2,...,d}|xi −yi| ≤\nb−a\n2N∞≤(b−a)2r\n2(b−a) = r.\n(22)\nNote that (21), (17), and the fact that ∀x ∈[0, ∞): ⌈x⌉≤1(0,1](x) + 2x1(1,∞)(x) =\n1(0,r](rx) + 2x1(r,∞)(rx) yield for all p ∈[1, ∞) that\nC([a,b]d,δp),r ≤|Ap| = (Np)d =\n\u0010l\nd1/p(b−a)\n2r\nm\u0011d\n≤\n\u0000\u0006 d(b−a)\n2r\n\u0007\u0001d\n≤\n\u00001(0,r]\n\u0000 d(b−a)\n2\n\u0001\n+ 2d(b−a)\n2r\n1(r,∞)\n\u0000 d(b−a)\n2\n\u0001\u0001d\n= 1(0,r]\n\u0000 d(b−a)\n2\n\u0001\n+\n\u0000 d(b−a)\nr\n\u0001d1(r,∞)\n\u0000 d(b−a)\n2\n\u0001\n.\n(23)\nThis proves (i). In addition, (22), (17), and again the fact that ∀x ∈[0, ∞): ⌈x⌉≤\n1(0,r](rx) + 2x1(r,∞)(rx) demonstrate that\nC([a,b]d,δ∞),r ≤|A∞| = (N∞)d =\n\u0000\u0006 b−a\n2r\n\u0007\u0001d ≤1(0,r]\n\u0000 b−a\n2\n\u0001\n+\n\u0000 b−a\nr\n\u0001d1(r,∞)\n\u0000 b−a\n2\n\u0001\n.\n(24)\nThis implies (ii) and thus completes the proof of Lemma 3.3.\n9\n3.2\nConvergence rates for the approximation error\nLemma 3.4. Let d, d, L ∈N, L, a ∈R, b ∈(a, ∞), u ∈[−∞, ∞), v ∈(u, ∞],\nl = (l0, l1, . . . , lL) ∈NL+1, assume l0 = d, lL = 1, and d ≥PL\ni=1 li(li−1 + 1), and\nlet f : [a, b]d →([u, v] ∩R) satisfy for all x, y ∈[a, b]d that |f(x) −f(y)| ≤L∥x −y∥1\n(cf. Deﬁnition 3.1). Then there exists ϑ ∈Rd such that ∥ϑ∥∞≤supx∈[a,b]d|f(x)| and\nsupx∈[a,b]d|N ϑ,l\nu,v (x) −f(x)| ≤dL(b −a)\n2\n(25)\n(cf. Deﬁnition 2.8).\nProof of Lemma 3.4. Throughout this proof let d ∈N be given by d = PL\ni=1 li(li−1 + 1),\nlet m = (m1, m2, . . . , md) ∈[a, b]d satisfy for all i ∈{1, 2, . . . , d} that mi = (a+b)/2, and let\nϑ = (ϑ1, ϑ2, . . . , ϑd) ∈Rd satisfy for all i ∈{1, 2, . . . , d}\\{d} that ϑi = 0 and ϑd = f(m).\nObserve that the assumption that lL = 1 and the fact that ∀i ∈{1, 2, . . . , d −1}: ϑi = 0\nshow for all x = (x1, x2, . . . , xlL−1) ∈RlL−1 that\nA\nϑ,PL−1\ni=1 li(li−1+1)\n1,lL−1\n(x) =\n\u0014lL−1\nP\ni=1\nϑ[\nPL−1\ni=1 li(li−1+1)]+ixi\n\u0015\n+ ϑ[\nPL−1\ni=1 li(li−1+1)]+lL−1+1\n=\n\u0014lL−1\nP\ni=1\nϑ[\nPL\ni=1 li(li−1+1)]−(lL−1−i+1)xi\n\u0015\n+ ϑPL\ni=1 li(li−1+1)\n=\n\u0014lL−1\nP\ni=1\nϑd−(lL−1−i+1)xi\n\u0015\n+ ϑd = ϑd = f(m)\n(26)\n(cf. Deﬁnition 2.1).\nCombining this with the fact that f(m) ∈[u, v] ensures for all\nx ∈RlL−1 that\n\u0000Cu,v,lL ◦A\nϑ,PL−1\ni=1 li(li−1+1)\nlL,lL−1\n\u0001\n(x) =\n\u0000Cu,v,1 ◦A\nϑ,PL−1\ni=1 li(li−1+1)\n1,lL−1\n\u0001\n(x) = cu,v(f(m))\n= max{u, min{f(m), v}} = max{u, f(m)} = f(m)\n(27)\n(cf. Deﬁnitions 2.6 and 2.7). This proves for all x ∈Rd that\nN ϑ,l\nu,v (x) = f(m).\n(28)\nIn addition, note that it holds for all x ∈[a, m1], x ∈[m1, b] that |m1 −x| = m1 −x =\n(a+b)/2 −x ≤(a+b)/2 −a = (b−a)/2 and |m1 −x| = x −m1 = x −(a+b)/2 ≤b −(a+b)/2 = (b−a)/2.\nThe assumption that ∀x, y ∈[a, b]d : |f(x)−f(y)| ≤L∥x−y∥1 and (28) hence demonstrate\nfor all x = (x1, x2, . . . , xd) ∈[a, b]d that\n|N ϑ,l\nu,v (x) −f(x)| = |f(m) −f(x)| ≤L∥m −x∥1 = L\ndP\ni=1\n|mi −xi|\n= L\ndP\ni=1\n|m1 −xi| ≤\ndP\ni=1\nL(b −a)\n2\n= dL(b −a)\n2\n.\n(29)\nThis and the fact that ∥ϑ∥∞= maxi∈{1,2,...,d}|ϑi| = |f(m)| ≤supx∈[a,b]d|f(x)| complete\nthe proof of Lemma 3.4.\nProposition 3.5. Let d, d, L ∈N, A ∈(0, ∞), L, a ∈R, b ∈(a, ∞), u ∈[−∞, ∞),\nv ∈(u, ∞], l = (l0, l1, . . . , lL) ∈NL+1, assume L ≥A1(6d,∞)(A)/(2d) + 1, l0 = d, l1 ≥\nA1(6d,∞)(A), lL = 1, and d ≥PL\ni=1 li(li−1 + 1), assume for all i ∈{2, 3, . . .} ∩[0, L)\n10\nthat li ≥1(6d,∞)(A) max{A/d −2i + 3, 2}, and let f : [a, b]d →([u, v] ∩R) satisfy for all\nx, y ∈[a, b]d that |f(x)−f(y)| ≤L∥x−y∥1 (cf. Deﬁnition 3.1). Then there exists ϑ ∈Rd\nsuch that ∥ϑ∥∞≤max{1, L, |a|, |b|, 2[supx∈[a,b]d|f(x)|]} and\nsupx∈[a,b]d|N ϑ,l\nu,v (x) −f(x)| ≤3dL(b −a)\nA\n1/d\n(30)\n(cf. Deﬁnition 2.8).\nProof of Proposition 3.5. Throughout this proof assume w.l.o.g. that A > 6d (cf. Lem-\nma 3.4), let N ∈N be given by\nN = max\nn\nn ∈N: n ≤\n\u0000 A\n2d\n\u00011/do\n,\n(31)\nlet r ∈(0, ∞) be given by r = d(b−a)/(2N), let δ: ([a, b]d) × ([a, b]d) →[0, ∞) satisfy for all\nx, y ∈[a, b]d that δ(x, y) = ∥x −y∥1, let D ⊆[a, b]d satisfy |D| = max{2, C([a,b]d,δ),r} and\nsupx∈[a,b]d infy∈D δ(x, y) ≤r\n(32)\n(cf. Deﬁnition 3.2), and let ⌈·⌉: [0, ∞) →N0 satisfy for all x ∈[0, ∞) that ⌈x⌉=\nmin([x, ∞) ∩N0). Note that it holds for all d ∈N that\n2d ≤2 · 2d−1 = 2d.\n(33)\nThis implies that 3d = 6d/2d ≤A/(2d). Equation (31) hence demonstrates that\n2 ≤2\n3\n\u0000 A\n2d\n\u00011/d =\n\u0000 A\n2d\n\u00011/d −1\n3\n\u0000 A\n2d\n\u00011/d ≤\n\u0000 A\n2d\n\u00011/d −1 < N.\n(34)\nThis and (i) in Lemma 3.3 (with δ1 ←δ, p ←1 in the notation of (i) in Lemma 3.3)\nestablish that\n|D| = max{2, C([a,b]d,δ),r} ≤max\nn\n2,\n\u0010l\nd(b−a)\n2r\nm\u0011do\n= max{2, (⌈N⌉)d} = Nd.\n(35)\nCombining this with (31) proves that\n4 ≤2d|D| ≤2dNd ≤2dA\n2d = A.\n(36)\nThe fact that L ≥A1(6d,∞)(A)/(2d) + 1 = A/(2d) + 1 hence yields that |D| ≤A/(2d) ≤L −1.\nThis, (36), and the facts that l1 ≥A1(6d,∞)(A) = A and ∀i ∈{2, 3, . . .} ∩[0, L) =\n{2, 3, . . . , L −1}: li ≥1(6d,∞)(A) max{A/d −2i + 3, 2} = max{A/d −2i + 3, 2} imply for all\ni ∈{2, 3, . . . , |D|} that\nL ≥|D| + 1,\nl1 ≥A ≥2d|D|,\nand\nli ≥A/d −2i + 3 ≥2|D| −2i + 3.\n(37)\nIn addition, the fact that ∀i ∈{2, 3, . . .} ∩[0, L): li ≥max{A/d −2i + 3, 2} ensures for all\ni ∈N ∩(|D|, L) that\nli ≥2.\n(38)\nFurthermore, observe that it holds for all x = (x1, x2, . . . , xd), y = (y1, y2, . . . , yd) ∈[a, b]d\nthat\n|f(x) −f(y)| ≤L∥x −y∥1 = L\n\u0014 dP\ni=1\n|xi −yi|\n\u0015\n.\n(39)\n11\nThis, the assumptions that l0 = d, lL = 1, and d ≥PL\ni=1 li(li−1 +1), (37)–(38), and Beck,\nJentzen, & Kuckuck [10, Corollary 3.8] (with d ←d, d ←d, L ←L, L ←L, u ←u,\nv ←v, D ←[a, b]d, f ←f, M ←D, l ←l in the notation of [10, Corollary 3.8]) show\nthat there exists ϑ ∈Rd such that ∥ϑ∥∞≤max{1, L, supx∈D∥x∥∞, 2[supx∈D|f(x)|]} and\nsup\nx∈[a,b]d |N ϑ,l\nu,v (x) −f(x)| ≤2L\n\u0014\nsup\nx=(x1,x2,...,xd)∈[a,b]d\n\u0012\ninf\ny=(y1,y2,...,yd)∈D\ndP\ni=1\n|xi −yi|\n\u0013\u0015\n= 2L\n\u0014\nsup\nx∈[a,b]d inf\ny∈D∥x −y∥1\n\u0015\n= 2L\n\u0014\nsup\nx∈[a,b]d inf\ny∈D δ(x, y)\n\u0015\n.\n(40)\nNote that this demonstrates that\n∥ϑ∥∞≤max{1, L, |a|, |b|, 2[supx∈[a,b]d|f(x)|]}.\n(41)\nMoreover, (40) and (32)–(34) prove that\nsupx∈[a,b]d|N ϑ,l\nu,v (x) −f(x)| ≤2L\n\u0002\nsupx∈[a,b]d infy∈D δ(x, y)\n\u0003\n≤2Lr\n= dL(b −a)\nN\n≤dL(b −a)\n2\n3\n\u0000 A\n2d\n\u00011/d\n= (2d)\n1/d3dL(b −a)\n2A\n1/d\n≤3dL(b −a)\nA\n1/d\n.\n(42)\nCombining this with (41) completes the proof of Proposition 3.5.\n4\nAnalysis of the generalisation error\nIn this section we consider the worst-case generalisation error arising in deep learning\nbased empirical risk minimisation with quadratic loss function for DNNs with a ﬁxed\narchitecture and weights and biases bounded in size by a ﬁxed constant (cf. Corollary 4.15\nin Subsection 4.3). We prove that this worst-case generalisation error converges in the\nprobabilistically strong sense with rate 1/2 (up to a logarithmic factor) with respect to\nthe number of samples used for calculating the empirical risk and that the constant in\nthe corresponding upper bound for the worst-case generalisation error scales favourably\n(i.e., only very moderately) in terms of depth and width of the DNNs employed, cf. (ii)\nin Corollary 4.15.\nCorollary 4.15 is a consequence of the main result of this section,\nProposition 4.14 in Subsection 4.3, which provides a similar conclusion in a more general\nsetting. The proofs of Proposition 4.14 and Corollary 4.15, respectively, rely on the tools\ndeveloped in the two preceding subsections, Subsections 4.1 and 4.2.\nOn the one hand, Subsection 4.1 provides an essentially well-known estimate for the Lp-\nerror of Monte Carlo-type approximations, cf. Corollary 4.5. Corollary 4.5 is a consequence\nof the well-known result stated here as Proposition 4.4, which, in turn, follows directly\nfrom, e.g., Cox et al. [22, Corollary 5.11] (with M ←M, q ←2, (E, ∥·∥E) ←(Rd, ∥·∥2|Rd),\n(Ω, F, P) ←(Ω, F, P), (ξj)j∈{1,2,...,M} ←(Xj)j∈{1,2,...,M}, p ←p in the notation of [22,\nCorollary 5.11] and Proposition 4.4, respectively). In the proof of Corollary 4.5 we also\napply Lemma 4.3, which is Grohs et al. [45, Lemma 2.2]. In order to make the statements\nof Lemma 4.3 and Proposition 4.4 more accessible for the reader, we recall in Deﬁnition 4.1\n(cf., e.g., [22, Deﬁnition 5.1]) the notion of a Rademacher family and in Deﬁnition 4.2 (cf.,\ne.g., [22, Deﬁnition 5.4] or Gonon et al. [42, Deﬁnition 2.1]) the notion of the p-Kahane–\nKhintchine constant.\nOn the other hand, we derive in Subsection 4.2 uniform Lp-estimates for Lipschitz\ncontinuous random ﬁelds with a separable metric space as index set (cf. Lemmas 4.10\n12\nand 4.11 and Corollary 4.12). These estimates are uniform in the sense that the supremum\nover the index set is inside the expectation belonging to the Lp-norm, which is necessary\nsince we intend to prove error bounds for the worst-case generalisation error, as illustrated\nabove. One of the elementary but crucial arguments in our derivation of such uniform Lp-\nestimates is given in Lemma 4.9 (cf. Lemma 4.8). Roughly speaking, Lemma 4.9 illustrates\nhow the Lp-norm of a supremum can be bounded from above by the supremum of certain\nLp-norms, where the Lp-norms are integrating over a general measure space and where\nthe suprema are taken over a general (bounded) separable metric space. Furthermore, the\nelementary and well-known Lemmas 4.6 and 4.7, respectively, follow immediately from\nBeck, Jentzen, & Kuckuck [10, (ii) in Lemma 3.13 and (ii) in Lemma 3.14] and ensure\nthat the mathematical statements of Lemmas 4.8, 4.9, and 4.10 do indeed make sense.\nThe results in Subsections 4.2 and 4.3 are in parts inspired by [10, Subsection 3.2] and\nwe refer, e.g., to [7, 13, 23, 31–33, 52, 67, 87, 92] and the references therein for further\nresults on the generalisation error.\n4.1\nMonte Carlo estimates\nDeﬁnition 4.1 (Rademacher family). Let (Ω, F, P) be a probability space and let J be\na set. Then we say that (rj)j∈J is a P-Rademacher family if and only if it holds that\nrj : Ω→{−1, 1}, j ∈J, are independent random variables with ∀j ∈J : P(rj = 1) =\nP(rj = −1).\nDeﬁnition 4.2 (p-Kahane–Khintchine constant). Let p ∈(0, ∞). Then we denote by\nKp ∈(0, ∞] the extended real number given by\nKp = sup\n\n\n\n\n\n\n\n\n\n\n\n\n\nc ∈[0, ∞):\n\n\n∃R-Banach space (E, ∥·∥E):\n∃probability space (Ω, F, P):\n∃P-Rademacher family (rj)j∈N :\n∃k ∈N: ∃x1, x2, . . . , xk ∈E \\ {0}:\n\u0010\nE\nh\r\rPk\nj=1 rjxj\n\r\rp\nE\ni\u00111/p\n= c\n\u0010\nE\nh\r\rPk\nj=1 rjxj\n\r\r2\nE\ni\u00111/2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(43)\n(cf. Deﬁnition 4.1).\nLemma 4.3. It holds for all p ∈[2, ∞) that Kp ≤√p −1 < ∞(cf. Deﬁnition 4.2).\nProposition 4.4. Let d, M ∈N, p ∈[2, ∞), let (Ω, F, P) be a probability space,\nlet Xj : Ω→Rd, j ∈{1, 2, . . . , M}, be independent random variables, and assume\nmaxj∈{1,2,...,M} E[∥Xj∥2] < ∞(cf. Deﬁnition 3.1). Then\n\u0012\nE\n\u0014\r\r\r\r\n\u0014 M\nP\nj=1\nXj\n\u0015\n−E\n\u0014 M\nP\nj=1\nXj\n\u0015\r\r\r\r\np\n2\n\u0015\u00131/p\n≤2Kp\n\u0014 M\nP\nj=1\n\u0000E\n\u0002\n∥Xj −E[Xj]∥p\n2\n\u0003\u00012/p\n\u00151/2\n(44)\n(cf. Deﬁnition 4.2 and Lemma 4.3).\nCorollary 4.5. Let d, M\n∈N, p ∈[2, ∞), let (Ω, F, P) be a probability space,\nlet Xj : Ω→Rd, j ∈{1, 2, . . . , M}, be independent random variables, and assume\nmaxj∈{1,2,...,M} E[∥Xj∥2] < ∞(cf. Deﬁnition 3.1). Then\n\u0012\nE\n\u0014\r\r\r\r\n1\nM\n\u0014 M\nP\nj=1\nXj\n\u0015\n−E\n\u0014 1\nM\nM\nP\nj=1\nXj\n\u0015\r\r\r\r\np\n2\n\u0015\u00131/p\n≤2√p −1\n√\nM\n\u0014\nmax\nj∈{1,2,...,M}\n\u0000E\n\u0002\n∥Xj −E[Xj]∥p\n2\n\u0003\u00011/p\n\u0015\n.\n(45)\n13\nProof of Corollary 4.5. Observe that Proposition 4.4 and Lemma 4.3 imply that\n\u0012\nE\n\u0014\r\r\r\r\n1\nM\n\u0014 M\nP\nj=1\nXj\n\u0015\n−E\n\u0014 1\nM\nM\nP\nj=1\nXj\n\u0015\r\r\r\r\np\n2\n\u0015\u00131/p\n= 1\nM\n\u0012\nE\n\u0014\r\r\r\r\n\u0014 M\nP\nj=1\nXj\n\u0015\n−E\n\u0014 M\nP\nj=1\nXj\n\u0015\r\r\r\r\np\n2\n\u0015\u00131/p\n≤2Kp\nM\n\u0014 M\nP\nj=1\n\u0000E\n\u0002\n∥Xj −E[Xj]∥p\n2\n\u0003\u00012/p\n\u00151/2\n≤2Kp\nM\n\u0014\nM\n\u0012\nmax\nj∈{1,2,...,M}\n\u0000E\n\u0002\n∥Xj −E[Xj]∥p\n2\n\u0003\u00012/p\n\u0013\u00151/2\n= 2Kp\n√\nM\n\u0014\nmax\nj∈{1,2,...,M}\n\u0000E\n\u0002\n∥Xj −E[Xj]∥p\n2\n\u0003\u00011/p\n\u0015\n≤2√p −1\n√\nM\n\u0014\nmax\nj∈{1,2,...,M}\n\u0000E\n\u0002\n∥Xj −E[Xj]∥p\n2\n\u0003\u00011/p\n\u0015\n(46)\n(cf. Deﬁnition 4.2). The proof of Corollary 4.5 is thus complete.\n4.2\nUniform strong error estimates for random ﬁelds\nLemma 4.6. Let (E, E ) be a separable topological space, assume E ̸= ∅, let (Ω, F) be a\nmeasurable space, let fx : Ω→R, x ∈E, be F/B(R)-measurable functions, and assume\nfor all ω ∈Ωthat E ∋x 7→fx(ω) ∈R is a continuous function. Then it holds that the\nfunction\nΩ∋ω 7→supx∈E fx(ω) ∈R ∪{∞}\n(47)\nis F/B(R ∪{∞})-measurable.\nLemma 4.7. Let (E, δ) be a separable metric space, assume E ̸= ∅, let L ∈R, let\n(Ω, F, P) be a probability space, let Zx : Ω→R, x ∈E, be random variables, and assume\nfor all x, y ∈E that E[|Zx|] < ∞and |Zx−Zy| ≤Lδ(x, y). Then it holds that the function\nΩ∋ω 7→supx∈E|Zx(ω) −E[Zx]| ∈[0, ∞]\n(48)\nis F/B([0, ∞])-measurable.\nLemma 4.8. Let (E, δ) be a separable metric space, let N ∈N, p, L, r1, r2, . . . , rN ∈\n[0, ∞), z1, z2, . . . , zN ∈E satisfy E ⊆SN\ni=1{x ∈E : δ(x, zi) ≤ri}, let (Ω, F, µ) be a\nmeasure space, let Zx : Ω→R, x ∈E, be F/B(R)-measurable functions, and assume for\nall ω ∈Ω, x, y ∈E that |Zx(ω) −Zy(ω)| ≤Lδ(x, y). Then\nZ\nΩ\nsup\nx∈E\n|Zx(ω)|p µ(dω) ≤\nNP\ni=1\nZ\nΩ\n(Lri + |Zzi(ω)|)p µ(dω)\n(49)\n(cf. Lemma 4.6).\nProof of Lemma 4.8. Throughout this proof let B1, B2, . . . , BN ⊆E satisfy for all i ∈\n{1, 2, . . . , N} that Bi = {x ∈E : δ(x, zi) ≤ri}. Note that the fact that E = SN\ni=1 Bi\nshows for all ω ∈Ωthat\nsupx∈E|Zx(ω)| = supx∈(\nSN\ni=1 Bi)|Zx(ω)| = maxi∈{1,2,...,N} supx∈Bi|Zx(ω)|.\n(50)\n14\nThis establishes that\nZ\nΩ\nsup\nx∈E\n|Zx(ω)|p µ(dω) =\nZ\nΩ\nmax\ni∈{1,2,...,N} sup\nx∈Bi\n|Zx(ω)|p µ(dω)\n≤\nZ\nΩ\nNP\ni=1\nsup\nx∈Bi\n|Zx(ω)|p µ(dω) =\nNP\ni=1\nZ\nΩ\nsup\nx∈Bi\n|Zx(ω)|p µ(dω).\n(51)\nFurthermore, the assumption that ∀ω ∈Ω, x, y ∈E : |Zx(ω) −Zy(ω)| ≤Lδ(x, y) implies\nfor all ω ∈Ω, i ∈{1, 2, . . . , N}, x ∈Bi that\n|Zx(ω)| = |Zx(ω) −Zzi(ω) + Zzi(ω)| ≤|Zx(ω) −Zzi(ω)| + |Zzi(ω)|\n≤Lδ(x, zi) + |Zzi(ω)| ≤Lri + |Zzi(ω)|.\n(52)\nCombining this with (51) proves that\nZ\nΩ\nsup\nx∈E\n|Zx(ω)|p µ(dω) ≤\nNP\ni=1\nZ\nΩ\n(Lri + |Zzi(ω)|)p µ(dω).\n(53)\nThe proof of Lemma 4.8 is thus complete.\nLemma 4.9. Let p, L, r ∈(0, ∞), let (E, δ) be a separable metric space, let (Ω, F, µ)\nbe a measure space, assume E ̸= ∅and µ(Ω) ̸= 0, let Zx : Ω→R, x ∈E, be F/B(R)-\nmeasurable functions, and assume for all ω ∈Ω, x, y ∈E that |Zx(ω)−Zy(ω)| ≤Lδ(x, y).\nThen\nZ\nΩ\nsup\nx∈E\n|Zx(ω)|p µ(dω) ≤C(E,δ),r\n\u0014\nsup\nx∈E\nZ\nΩ\n(Lr + |Zx(ω)|)p µ(dω)\n\u0015\n(54)\n(cf. Deﬁnition 3.2 and Lemma 4.6).\nProof of Lemma 4.9. Throughout this proof assume w.l.o.g. that C(E,δ),r < ∞, let N ∈N\nbe given by N = C(E,δ),r, and let z1, z2, . . . , zN ∈E satisfy E ⊆SN\ni=1{x ∈E : δ(x, zi) ≤r}.\nNote that Lemma 4.8 (with r1 ←r, r2 ←r, . . . , rN ←r in the notation of Lemma 4.8)\nestablishes that\nZ\nΩ\nsup\nx∈E\n|Zx(ω)|p µ(dω) ≤\nNP\ni=1\nZ\nΩ\n(Lr + |Zzi(ω)|)p µ(dω)\n≤\nNP\ni=1\n\u0014\nsup\nx∈E\nZ\nΩ\n(Lr + |Zx(ω)|)p µ(dω)\n\u0015\n= N\n\u0014\nsup\nx∈E\nZ\nΩ\n(Lr + |Zx(ω)|)p µ(dω)\n\u0015\n.\n(55)\nThe proof of Lemma 4.9 is thus complete.\nLemma 4.10. Let p ∈[1, ∞), L, r ∈(0, ∞), let (E, δ) be a separable metric space, assume\nE ̸= ∅, let (Ω, F, P) be a probability space, let Zx : Ω→R, x ∈E, be random variables,\nand assume for all x, y ∈E that E[|Zx|] < ∞and |Zx −Zy| ≤Lδ(x, y). Then\n\u0000E\n\u0002\nsupx∈E|Zx −E[Zx]|p\u0003\u00011/p ≤(C(E,δ),r)\n1/ph\n2Lr + supx∈E\n\u0000E\n\u0002\n|Zx −E[Zx]|p\u0003\u00011/pi\n(56)\n(cf. Deﬁnition 3.2 and Lemma 4.7).\nProof of Lemma 4.10. Throughout this proof let Yx : Ω→R, x ∈E, satisfy for all x ∈E,\nω ∈Ωthat Yx(ω) = Zx(ω) −E[Zx]. Note that it holds for all ω ∈Ω, x, y ∈E that\n|Yx(ω) −Yy(ω)| = |(Zx(ω) −E[Zx]) −(Zy(ω) −E[Zy])|\n≤|Zx(ω) −Zy(ω)| + |E[Zx] −E[Zy]| ≤Lδ(x, y) + E[|Zx −Zy|]\n≤2Lδ(x, y).\n(57)\n15\nCombining this with Lemma 4.9 (with L ←2L, (Ω, F, µ) ←(Ω, F, P), (Zx)x∈E ←(Yx)x∈E\nin the notation of Lemma 4.9) implies that\n\u0000E\n\u0002\nsupx∈E|Zx −E[Zx]|p\u0003\u00011/p =\n\u0000E\n\u0002\nsupx∈E|Yx|p\u0003\u00011/p\n≤(C(E,δ),r)\n1/ph\nsupx∈E\n\u0000E\n\u0002\n(2Lr + |Yx|)p\u0003\u00011/pi\n≤(C(E,δ),r)\n1/ph\n2Lr + supx∈E\n\u0000E\n\u0002\n|Yx|p\u0003\u00011/pi\n= (C(E,δ),r)\n1/ph\n2Lr + supx∈E\n\u0000E\n\u0002\n|Zx −E[Zx]|p\u0003\u00011/pi\n.\n(58)\nThe proof of Lemma 4.10 is thus complete.\nLemma 4.11. Let M ∈N, p ∈[2, ∞), L, r ∈(0, ∞), let (E, δ) be a separable metric\nspace, assume E ̸= ∅, let (Ω, F, P) be a probability space, for every x ∈E let Yx,j : Ω→\nR, j ∈{1, 2, . . . , M}, be independent random variables, assume for all x, y ∈E, j ∈\n{1, 2, . . . , M} that E[|Yx,j|] < ∞and |Yx,j −Yy,j| ≤Lδ(x, y), and let Zx : Ω→R, x ∈E,\nsatisfy for all x ∈E that\nZx = 1\nM\n\u0014 M\nP\nj=1\nYx,j\n\u0015\n.\n(59)\nThen\n(i) it holds for all x ∈E that E[|Zx|] < ∞,\n(ii) it holds that the function Ω∋ω 7→supx∈E|Zx(ω) −E[Zx]| ∈[0, ∞] is F/B([0, ∞])-\nmeasurable, and\n(iii) it holds that\n\u0000E\n\u0002\nsupx∈E|Zx −E[Zx]|p\u0003\u00011/p\n≤2(C(E,δ),r)\n1/ph\nLr +\n√p−1\n√\nM\n\u0010\nsupx∈E maxj∈{1,2,...,M}\n\u0000E\n\u0002\n|Yx,j −E[Yx,j]|p\u0003\u00011/p\u0011i\n(60)\n(cf. Deﬁnition 3.2).\nProof of Lemma 4.11. Note that the assumption that ∀x ∈E, j ∈{1, 2, . . . , M}:\nE[|Yx,j|] < ∞implies for all x ∈E that\nE[|Zx|] = E\n\u0014 1\nM\n\f\f\f\f\nM\nP\nj=1\nYx,j\n\f\f\f\f\n\u0015\n≤1\nM\n\u0014 M\nP\nj=1\nE[|Yx,j|]\n\u0015\n≤\nmax\nj∈{1,2,...,M} E[|Yx,j|] < ∞.\n(61)\nThis proves (i). Next observe that the assumption that ∀x, y ∈E, j ∈{1, 2, . . . , M}:\n|Yx,j −Yy,j| ≤Lδ(x, y) demonstrates for all x, y ∈E that\n|Zx −Zy| = 1\nM\n\f\f\f\f\n\u0014 M\nP\nj=1\nYx,j\n\u0015\n−\n\u0014 M\nP\nj=1\nYy,j\n\u0015\f\f\f\f ≤1\nM\n\u0014 M\nP\nj=1\n|Yx,j −Yy,j|\n\u0015\n≤Lδ(x, y).\n(62)\nCombining this with (i) and Lemma 4.7 establishes (ii). It thus remains to show (iii). For\nthis note that (i), (62), and Lemma 4.10 yield that\n\u0000E\n\u0002\nsupx∈E|Zx −E[Zx]|p\u0003\u00011/p ≤(C(E,δ),r)\n1/ph\n2Lr + supx∈E\n\u0000E\n\u0002\n|Zx −E[Zx]|p\u0003\u00011/pi\n.\n(63)\n16\nMoreover, (61) and Corollary 4.5 (with d ←1, (Xj)j∈{1,2,...,M} ←(Yx,j)j∈{1,2,...,M} for\nx ∈E in the notation of Corollary 4.5) prove for all x ∈E that\n\u0000E\n\u0002\n|Zx −E[Zx]|p\u0003\u00011/p =\n\u0012\nE\n\u0014\f\f\f\f\n1\nM\n\u0014 M\nP\nj=1\nYx,j\n\u0015\n−E\n\u0014 1\nM\nM\nP\nj=1\nYx,j\n\u0015\f\f\f\f\np\u0015\u00131/p\n≤2√p −1\n√\nM\n\u0014\nmax\nj∈{1,2,...,M}\n\u0000E\n\u0002\n|Yx,j −E[Yx,j]|p\u0003\u00011/p\n\u0015\n.\n(64)\nThis and (63) imply that\n\u0000E\n\u0002\nsupx∈E|Zx −E[Zx]|p\u0003\u00011/p\n≤(C(E,δ),r)\n1/ph\n2Lr + 2√p−1\n√\nM\n\u0010\nsupx∈E maxj∈{1,2,...,M}\n\u0000E\n\u0002\n|Yx,j −E[Yx,j]|p\u0003\u00011/p\u0011i\n= 2(C(E,δ),r)\n1/ph\nLr +\n√p−1\n√\nM\n\u0010\nsupx∈E maxj∈{1,2,...,M}\n\u0000E\n\u0002\n|Yx,j −E[Yx,j]|p\u0003\u00011/p\u0011i\n.\n(65)\nThe proof of Lemma 4.11 is thus complete.\nCorollary 4.12. Let M ∈N, p ∈[2, ∞), L, C ∈(0, ∞), let (E, δ) be a separable metric\nspace, assume E ̸= ∅, let (Ω, F, P) be a probability space, for every x ∈E let Yx,j : Ω→\nR, j ∈{1, 2, . . . , M}, be independent random variables, assume for all x, y ∈E, j ∈\n{1, 2, . . . , M} that E[|Yx,j|] < ∞and |Yx,j −Yy,j| ≤Lδ(x, y), and let Zx : Ω→R, x ∈E,\nsatisfy for all x ∈E that\nZx = 1\nM\n\u0014 M\nP\nj=1\nYx,j\n\u0015\n.\n(66)\nThen\n(i) it holds for all x ∈E that E[|Zx|] < ∞,\n(ii) it holds that the function Ω∋ω 7→supx∈E|Zx(ω) −E[Zx]| ∈[0, ∞] is F/B([0, ∞])-\nmeasurable, and\n(iii) it holds that\n\u0000E\n\u0002\nsupx∈E|Zx −E[Zx]|p\u0003\u00011/p\n≤2√p−1\n√\nM\n\u0010\nC(E,δ), C√p−1\nL\n√\nM\n\u00111/ph\nC + supx∈E maxj∈{1,2,...,M}\n\u0000E\n\u0002\n|Yx,j −E[Yx,j]|p\u0003\u00011/pi (67)\n(cf. Deﬁnition 3.2).\nProof of Corollary 4.12. Note that Lemma 4.11 shows (i) and (ii).\nIn addition, Lem-\nma 4.11 (with r ←C√p−1/(L\n√\nM) in the notation of Lemma 4.11) ensures that\n\u0000E\n\u0002\nsupx∈E|Zx −E[Zx]|p\u0003\u00011/p\n≤2\n\u0010\nC(E,δ), C√p−1\nL\n√\nM\n\u00111/ph\nL C√p−1\nL\n√\nM +\n√p−1\n√\nM\n\u0010\nsupx∈E maxj∈{1,2,...,M}\n\u0000E\n\u0002\n|Yx,j −E[Yx,j]|p\u0003\u00011/p\u0011i\n= 2√p−1\n√\nM\n\u0010\nC(E,δ), C√p−1\nL\n√\nM\n\u00111/ph\nC + supx∈E maxj∈{1,2,...,M}\n\u0000E\n\u0002\n|Yx,j −E[Yx,j]|p\u0003\u00011/pi\n.\n(68)\nThis establishes (iii) and thus completes the proof of Corollary 4.12.\n17\n4.3\nStrong convergence rates for the generalisation error\nLemma 4.13. Let M ∈N, p ∈[2, ∞), L, C, b ∈(0, ∞), let (E, δ) be a separable\nmetric space, assume E ̸= ∅, let (Ω, F, P) be a probability space, let Xx,j : Ω→R,\nj ∈{1, 2, . . . , M}, x ∈E, and Yj : Ω→R, j ∈{1, 2, . . . , M}, be functions, assume\nfor every x ∈E that (Xx,j, Yj), j ∈{1, 2, . . . , M}, are i.i.d. random variables, assume\nfor all x, y ∈E, j ∈{1, 2, . . . , M} that |Xx,j −Yj| ≤b and |Xx,j −Xy,j| ≤Lδ(x, y), let\nR: E →[0, ∞) satisfy for all x ∈E that R(x) = E[|Xx,1 −Y1|2], and let R: E × Ω→\n[0, ∞) satisfy for all x ∈E, ω ∈Ωthat\nR(x, ω) = 1\nM\n\u0014 M\nP\nj=1\n|Xx,j(ω) −Yj(ω)|2\n\u0015\n.\n(69)\nThen\n(i) it holds that the function Ω∋ω 7→supx∈E|R(x, ω)−R(x)| ∈[0, ∞] is F/B([0, ∞])-\nmeasurable and\n(ii) it holds that\n\u0000E\n\u0002\nsupx∈E|R(x) −R(x)|p\u0003\u00011/p ≤\n\u0010\nC(E,δ), Cb√p−1\n2L\n√\nM\n\u00111/p\u00142(C + 1)b2√p −1\n√\nM\n\u0015\n(70)\n(cf. Deﬁnition 3.2).\nProof of Lemma 4.13. Throughout this proof let Yx,j : Ω→R, j ∈{1, 2, . . . , M}, x ∈E,\nsatisfy for all x ∈E, j ∈{1, 2, . . . , M} that Yx,j = |Xx,j −Yj|2. Note that the assumption\nthat for every x ∈E it holds that (Xx,j, Yj), j ∈{1, 2, . . . , M}, are i.i.d. random variables\nensures for all x ∈E that\nE[R(x)] = 1\nM\n\u0014 M\nP\nj=1\nE\n\u0002\n|Xx,j −Yj|2\u0003\u0015\n= M E\n\u0002\n|Xx,1 −Y1|2\u0003\nM\n= R(x).\n(71)\nFurthermore, the assumption that ∀x ∈E, j ∈{1, 2, . . . , M}: |Xx,j −Yj| ≤b shows for\nall x ∈E, j ∈{1, 2, . . . , M} that\nE[|Yx,j|] = E[|Xx,j −Yj|2] ≤b2 < ∞,\n(72)\nYx,j −E[Yx,j] = |Xx,j −Yj|2 −E\n\u0002\n|Xx,j −Yj|2\u0003\n≤|Xx,j −Yj|2 ≤b2,\n(73)\nand\nE[Yx,j] −Yx,j = E\n\u0002\n|Xx,j −Yj|2\u0003\n−|Xx,j −Yj|2 ≤E\n\u0002\n|Xx,j −Yj|2\u0003\n≤b2.\n(74)\nCombining (72)–(74) implies for all x ∈E, j ∈{1, 2, . . . , M} that\n\u0000E\n\u0002\n|Yx,j −E[Yx,j]|p\u0003\u00011/p ≤\n\u0000E\n\u0002\nb2p\u0003\u00011/p = b2.\n(75)\nMoreover, note that the assumptions that ∀x, y ∈E, j ∈{1, 2, . . . , M}: [|Xx,j −Yj| ≤\nb and |Xx,j −Xy,j| ≤Lδ(x, y)] and the fact that ∀x1, x2, y ∈R: (x1 −y)2 −(x2 −y)2 =\n(x1 −x2)((x1 −y) + (x2 −y)) establish for all x, y ∈E, j ∈{1, 2, . . . , M} that\n|Yx,j −Yy,j| = |(Xx,j −Yj)2 −(Xy,j −Yj)2|\n≤|Xx,j −Xy,j|(|Xx,j −Yj| + |Xy,j −Yj|)\n≤2b|Xx,j −Xy,j| ≤2bLδ(x, y).\n(76)\n18\nCombining this, (71), (72), and the fact that for every x ∈E it holds that Yx,j, j ∈\n{1, 2, . . . , M}, are independent random variables with Corollary 4.12 (with L ←2bL,\nC ←Cb2, (Yx,j)x∈E, j∈{1,2,...,M} ←(Yx,j)x∈E, j∈{1,2,...,M}, (Zx)x∈E ←(Ω∋ω 7→R(x, ω) ∈\nR)x∈E in the notation of Corollary 4.12) and (75) proves (i) and\n\u0000E\n\u0002\nsupx∈E|R(x) −R(x)|p\u0003\u00011/p =\n\u0000E\n\u0002\nsupx∈E|R(x) −E[R(x)]|p\u0003\u00011/p\n≤2√p−1\n√\nM\n\u0010\nC(E,δ), Cb2√p−1\n2bL\n√\nM\n\u00111/ph\nCb2 + supx∈E maxj∈{1,2,...,M}\n\u0000E\n\u0002\n|Yx,j −E[Yx,j]|p\u0003\u00011/pi\n≤2√p−1\n√\nM\n\u0010\nC(E,δ), Cb√p−1\n2L\n√\nM\n\u00111/p\n[Cb2 + b2] =\n\u0010\nC(E,δ), Cb√p−1\n2L\n√\nM\n\u00111/p\u00142(C + 1)b2√p −1\n√\nM\n\u0015\n.\n(77)\nThis shows (ii) and thus completes the proof of Lemma 4.13.\nProposition 4.14. Let d, d, M ∈N, L, b ∈(0, ∞), α ∈R, β ∈(α, ∞), D ⊆Rd,\nlet (Ω, F, P) be a probability space, let Xj : Ω→D, j ∈{1, 2, . . . , M}, and Yj : Ω→R,\nj ∈{1, 2, . . . , M}, be functions, assume that (Xj, Yj), j ∈{1, 2, . . . , M}, are i.i.d. random\nvariables, let f = (fθ)θ∈[α,β]d : [α, β]d →C(D, R) be a function, assume for all θ, ϑ ∈\n[α, β]d, j ∈{1, 2, . . . , M}, x ∈D that |fθ(Xj) −Yj| ≤b and |fθ(x) −fϑ(x)| ≤L∥θ −ϑ∥∞,\nlet R: [α, β]d →[0, ∞) satisfy for all θ ∈[α, β]d that R(θ) = E[|fθ(X1) −Y1|2], and let\nR: [α, β]d × Ω→[0, ∞) satisfy for all θ ∈[α, β]d, ω ∈Ωthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|fθ(Xj(ω)) −Yj(ω)|2\n\u0015\n(78)\n(cf. Deﬁnition 3.1). Then\n(i) it holds that the function Ω∋ω 7→supθ∈[α,β]d|R(θ, ω) −R(θ)| ∈[0, ∞] is F/\nB([0, ∞])-measurable and\n(ii) it holds for all p ∈(0, ∞) that\n\u0000E\n\u0002\nsupθ∈[α,β]d|R(θ) −R(θ)|p\u0003\u00011/p\n≤\ninf\nC,ε∈(0,∞)\n\"\n2(C + 1)b2 max{1, [2\n√\nML(β −α)(Cb)−1]ε}\np\nmax{1, p, d/ε}\n√\nM\n#\n≤\ninf\nC∈(0,∞)\n\"\n2(C + 1)b2p\ne max{1, p, d ln(4ML2(β −α)2(Cb)−2)}\n√\nM\n#\n.\n(79)\nProof of Proposition 4.14. Throughout this proof let p ∈(0, ∞), let (κC)C∈(0,∞) ⊆(0, ∞)\nsatisfy for all C ∈(0, ∞) that 2\n√\nML(β−α)/(Cb), let Xθ,j : Ω→R, j ∈{1, 2, . . . , M},\nθ ∈[α, β]d, satisfy for all θ ∈[α, β]d, j ∈{1, 2, . . . , M} that Xθ,j = fθ(Xj), and let\nδ: ([α, β]d)×([α, β]d) →[0, ∞) satisfy for all θ, ϑ ∈[α, β]d that δ(θ, ϑ) = ∥θ −ϑ∥∞. First\nof all, note that the assumption that ∀θ ∈[α, β]d, j ∈{1, 2, . . . , M}: |fθ(Xj) −Yj| ≤b\nimplies for all θ ∈[α, β]d, j ∈{1, 2, . . . , M} that\n|Xθ,j −Yj| = |fθ(Xj) −Yj| ≤b.\n(80)\nIn addition, the assumption that ∀θ, ϑ ∈[α, β]d, x ∈D: |fθ(x) −fϑ(x)| ≤L∥θ −ϑ∥∞\nensures for all θ, ϑ ∈[α, β]d, j ∈{1, 2, . . . , M} that\n|Xθ,j −Xϑ,j| = |fθ(Xj) −fϑ(Xj)| ≤supx∈D|fθ(x) −fϑ(x)| ≤L∥θ −ϑ∥∞= Lδ(θ, ϑ). (81)\n19\nCombining this, (80), and the fact that for every θ ∈[α, β]d it holds that (Xθ,j, Yj),\nj ∈{1, 2, . . . , M}, are i.i.d. random variables with Lemma 4.13 (with p ←q, C ←C,\n(E, δ) ←([α, β]d, δ), (Xx,j)x∈E, j∈{1,2,...,M} ←(Xθ,j)θ∈[α,β]d, j∈{1,2,...,M} for q ∈[2, ∞), C ∈\n(0, ∞) in the notation of Lemma 4.13) demonstrates for all C ∈(0, ∞), q ∈[2, ∞) that\nthe function Ω∋ω 7→supθ∈[α,β]d|R(θ, ω) −R(θ)| ∈[0, ∞] is F/B([0, ∞])-measurable and\n\u0000E\n\u0002\nsupθ∈[α,β]d|R(θ) −R(θ)|q\u0003\u00011/q ≤\n\u0010\nC([α,β]d,δ), Cb√q−1\n2L\n√\nM\n\u00111/q\u00142(C + 1)b2√q −1\n√\nM\n\u0015\n(82)\n(cf. Deﬁnition 3.2). This ﬁnishes the proof of (i). Next observe that (ii) in Lemma 3.3\n(with d ←d, a ←α, b ←β, r ←r for r ∈(0, ∞) in the notation of Lemma 3.3) shows\nfor all r ∈(0, ∞) that\nC([α,β]d,δ),r ≤1[0,r]\n\u0000 β−α\n2\n\u0001\n+\n\u0000 β−α\nr\n\u0001d1(r,∞)\n\u0000 β−α\n2\n\u0001\n≤max\nn\n1,\n\u0000 β−α\nr\n\u0001do\u00001[0,r]\n\u0000 β−α\n2\n\u0001\n+ 1(r,∞)\n\u0000 β−α\n2\n\u0001\u0001\n= max\nn\n1,\n\u0000 β−α\nr\n\u0001do\n.\n(83)\nThis yields for all C ∈(0, ∞), q ∈[2, ∞) that\n\u0010\nC([α,β]d,δ), Cb√q−1\n2L\n√\nM\n\u00111/q\n≤max\n\u001a\n1,\n\u0010\n2(β−α)L\n√\nM\nCb√q−1\n\u0011d\nq \u001b\n≤max\n\u001a\n1,\n\u0010\n2(β−α)L\n√\nM\nCb\n\u0011d\nq \u001b\n= max\nn\n1, (κC)\nd\nq\no\n.\n(84)\nJensen’s inequality and (82) hence prove for all C, ε ∈(0, ∞) that\n\u0000E\n\u0002\nsupθ∈[α,β]d|R(θ) −R(θ)|p\u0003\u00011/p\n≤\n\u0000E\n\u0002\nsupθ∈[α,β]d|R(θ) −R(θ)|max{2,p,d/ε}\u0003\u0001\n1\nmax{2,p,d/ε}\n≤max\nn\n1, (κC)\nd\nmax{2,p,d/ε}\no2(C + 1)b2p\nmax{2, p, d/ε} −1\n√\nM\n= max\n\b\n1, (κC)min{d/2,d/p,ε}\t2(C + 1)b2p\nmax{1, p −1, d/ε −1}\n√\nM\n≤2(C + 1)b2 max{1, (κC)ε}\np\nmax{1, p, d/ε}\n√\nM\n.\n(85)\nNext note that the fact that ∀a ∈(1, ∞): a\n1/(2 ln(a)) = e\nln(a)/(2 ln(a)) = e\n1/2 = √e ≥1 ensures\nfor all C ∈(0, ∞) with κC > 1 that\ninf\nε∈(0,∞)\n\"\n2(C + 1)b2 max{1, (κC)ε}\np\nmax{1, p, d/ε}\n√\nM\n#\n≤2(C + 1)b2 max{1, (κC)\n1/(2 ln(κC ))}\np\nmax{1, p, 2d ln(κC)}\n√\nM\n= 2(C + 1)b2p\ne max{1, p, d ln([κC]2)}\n√\nM\n.\n(86)\n20\nIn addition, observe that it holds for all C ∈(0, ∞) with κC ≤1 that\ninf\nε∈(0,∞)\n\"\n2(C + 1)b2 max{1, (κC)ε}\np\nmax{1, p, d/ε}\n√\nM\n#\n≤\ninf\nε∈(0,∞)\n\"\n2(C + 1)b2p\nmax{1, p, d/ε}\n√\nM\n#\n≤2(C + 1)b2p\nmax{1, p}\n√\nM\n≤2(C + 1)b2p\ne max{1, p, d ln([κC]2)}\n√\nM\n.\n(87)\nCombining (85) with (86) and (87) demonstrates that\n\u0000E\n\u0002\nsupθ∈[α,β]d|R(θ) −R(θ)|p\u0003\u00011/p\n≤\ninf\nC,ε∈(0,∞)\n\"\n2(C + 1)b2 max{1, (κC)ε}\np\nmax{1, p, d/ε}\n√\nM\n#\n=\ninf\nC,ε∈(0,∞)\n\"\n2(C + 1)b2 max{1, [2\n√\nML(β −α)(Cb)−1]ε}\np\nmax{1, p, d/ε}\n√\nM\n#\n≤\ninf\nC∈(0,∞)\n\"\n2(C + 1)b2p\ne max{1, p, d ln([κC]2)}\n√\nM\n#\n=\ninf\nC∈(0,∞)\n\"\n2(C + 1)b2p\ne max{1, p, d ln(4ML2(β −α)2(Cb)−2)}\n√\nM\n#\n.\n(88)\nThis establishes (ii) and thus completes the proof of Proposition 4.14.\nCorollary 4.15. Let d, d, L, M ∈N, B, b ∈[1, ∞), u ∈R, v ∈[u + 1, ∞), l =\n(l0, l1, . . . , lL) ∈NL+1, D ⊆[−b, b]d, assume l0 = d, lL = 1, and d ≥PL\ni=1 li(li−1 + 1), let\n(Ω, F, P) be a probability space, let Xj : Ω→D, j ∈{1, 2, . . . , M}, and Yj : Ω→[u, v],\nj ∈{1, 2, . . . , M}, be functions, assume that (Xj, Yj), j ∈{1, 2, . . . , M}, are i.i.d. ran-\ndom variables, let R: [−B, B]d →[0, ∞) satisfy for all θ ∈[−B, B]d that R(θ) =\nE[|N θ,l\nu,v (X1) −Y1|2], and let R: [−B, B]d × Ω→[0, ∞) satisfy for all θ ∈[−B, B]d,\nω ∈Ωthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\n(89)\n(cf. Deﬁnition 2.8). Then\n(i) it holds that the function Ω∋ω 7→supθ∈[−B,B]d|R(θ, ω) −R(θ)| ∈[0, ∞] is F/\nB([0, ∞])-measurable and\n(ii) it holds for all p ∈(0, ∞) that\n\u0000E\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|p\u0003\u00011/p\n≤9(v −u)2L(∥l∥∞+ 1)\np\nmax{p, ln(4(Mb)\n1/L(∥l∥∞+ 1)B)}\n√\nM\n≤9(v −u)2L(∥l∥∞+ 1)2 max{p, ln(3MBb)}\n√\nM\n(90)\n(cf. Deﬁnition 3.1).\n21\nProof of Corollary 4.15. Throughout this proof let d ∈N be given by d = PL\ni=1 li(li−1+1),\nlet L ∈(0, ∞) be given by L = bL(∥l∥∞+ 1)LBL−1, let f = (fθ)θ∈[−B,B]d : [−B, B]d →\nC(D, R) satisfy for all θ ∈[−B, B]d, x ∈D that fθ(x) = N θ,l\nu,v (x), let R : [−B, B]d →\n[0, ∞) satisfy for all θ ∈[−B, B]d that R(θ) = E[|fθ(X1) −Y1|2] = E[|N θ,l\nu,v (X1) −Y1|2],\nand let R: [−B, B]d × Ω→[0, ∞) satisfy for all θ ∈[−B, B]d, ω ∈Ωthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|fθ(Xj(ω)) −Yj(ω)|2\n\u0015\n= 1\nM\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\n.\n(91)\nNote that the fact that ∀θ ∈Rd, x ∈Rd : N θ,l\nu,v (x) ∈[u, v] and the assumption that\n∀j ∈{1, 2, . . . , M}: Yj(Ω) ⊆[u, v] imply for all θ ∈[−B, B]d, j ∈{1, 2, . . . , M} that\n|fθ(Xj) −Yj| = |N θ,l\nu,v (Xj) −Yj| ≤supy1,y2∈[u,v]|y1 −y2| = v −u.\n(92)\nMoreover, the assumptions that D ⊆[−b, b]d, l0 = d, and lL = 1, Beck, Jentzen, &\nKuckuck [10, Corollary 2.37] (with a ←−b, b ←b, u ←u, v ←v, d ←d, L ←L, l ←l\nin the notation of [10, Corollary 2.37]), and the assumptions that b ≥1 and B ≥1 ensure\nfor all θ, ϑ ∈[−B, B]d, x ∈D that\n|fθ(x) −fϑ(x)| ≤supy∈[−b,b]d|N θ,l\nu,v (y) −N ϑ,l\nu,v (y)|\n≤L max{1, b}(∥l∥∞+ 1)L(max{1, ∥θ∥∞, ∥ϑ∥∞})L−1∥θ −ϑ∥∞\n≤bL(∥l∥∞+ 1)LBL−1∥θ −ϑ∥∞= L∥θ −ϑ∥∞.\n(93)\nFurthermore, the facts that d ≥d and ∀θ = (θ1, θ2, . . . , θd) ∈Rd : N θ,l\nu,v = N (θ1,θ2,...,θd),l\nu,v\nprove for all ω ∈Ωthat\nsupθ∈[−B,B]d|R(θ, ω) −R(θ)| = supθ∈[−B,B]d|R(θ, ω) −R(θ)|.\n(94)\nNext observe that (92), (93), Proposition 4.14 (with d ←d, b ←v −u, α ←−B,\nβ ←B, R ←R, R ←R in the notation of Proposition 4.14), and the facts that\nv −u ≥(u + 1) −u = 1 and d ≤L∥l∥∞(∥l∥∞+ 1) ≤L(∥l∥∞+ 1)2 demonstrate for\nall p ∈(0, ∞) that the function Ω∋ω 7→supθ∈[−B,B]d|R(θ, ω) −R(θ)| ∈[0, ∞] is\nF/B([0, ∞])-measurable and\n\u0000E\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|p\u0003\u00011/p\n≤\ninf\nC∈(0,∞)\n\"\n2(C + 1)(v −u)2p\ne max{1, p, d ln(4ML2(2B)2(C[v −u])−2)}\n√\nM\n#\n≤\ninf\nC∈(0,∞)\n\"\n2(C + 1)(v −u)2p\ne max{1, p, L(∥l∥∞+ 1)2 ln(24ML2B2C−2)}\n√\nM\n#\n.\n(95)\nThis and (94) establish (i). In addition, combining (94)–(95) with the fact that 26L2 ≤\n26 · 22(L−1) = 24+2L ≤24L+2L = 26L and the facts that 3 ≥e, B ≥1, L ≥1, M ≥1, and\n22\nb ≥1 shows for all p ∈(0, ∞) that\n\u0000E\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|p\u0003\u00011/p =\n\u0000E\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|p\u0003\u00011/p\n≤2(1/2 + 1)(v −u)2p\ne max{1, p, L(∥l∥∞+ 1)2 ln(24ML2B222)}\n√\nM\n= 3(v −u)2p\ne max{p, L(∥l∥∞+ 1)2 ln(26Mb2L2(∥l∥∞+ 1)2LB2L)}\n√\nM\n≤3(v −u)2p\ne max{p, 3L2(∥l∥∞+ 1)2 ln([26LMb2(∥l∥∞+ 1)2LB2L]\n1/(3L))}\n√\nM\n≤3(v −u)2p\n3 max{p, 3L2(∥l∥∞+ 1)2 ln(22(Mb2)\n1/(3L)(∥l∥∞+ 1)B)}\n√\nM\n≤9(v −u)2L(∥l∥∞+ 1)\np\nmax{p, ln(4(Mb)\n1/L(∥l∥∞+ 1)B)}\n√\nM\n.\n(96)\nFurthermore, note that the fact that ∀n ∈N: n ≤2n−1 and the fact that ∥l∥∞≥1 imply\nthat\n4(∥l∥∞+ 1) ≤22 · 2(∥l∥∞+1)−1 = 23 · 2(∥l∥∞+1)−2 ≤32 · 3(∥l∥∞+1)−2 = 3(∥l∥∞+1).\n(97)\nThis demonstrates for all p ∈(0, ∞) that\n9(v −u)2L(∥l∥∞+ 1)\np\nmax{p, ln(4(Mb)\n1/L(∥l∥∞+ 1)B)}\n√\nM\n≤9(v −u)2L(∥l∥∞+ 1)\np\nmax{p, (∥l∥∞+ 1) ln([3(∥l∥∞+1)(Mb)\n1/LB]\n1/(∥l∥∞+1))}\n√\nM\n≤9(v −u)2L(∥l∥∞+ 1)2 max{p, ln(3MBb)}\n√\nM\n.\n(98)\nCombining this with (96) shows (ii). The proof of Corollary 4.15 is thus complete.\n5\nAnalysis of the optimisation error\nThe main result of this section, Proposition 5.6, establishes that the optimisation error of\nthe Minimum Monte Carlo method applied to a Lipschitz continuous random ﬁeld with\na d-dimensional hypercube as index set, where d ∈N, converges in the probabilistically\nstrong sense with rate 1/d with respect to the number of samples used, provided that\nthe sample indices are continuous uniformly drawn from the index hypercube (cf. (ii)\nin Proposition 5.6). We refer to Beck, Jentzen, & Kuckuck [10, Lemmas 3.22–3.23] for\nanalogous results for convergence in probability instead of strong convergence and to Beck\net al. [8, Lemma 3.5] for a related result. Corollary 5.8 below specialises Proposition 5.6\nto the case where the empirical risk from deep learning based empirical risk minimisation\nwith quadratic loss function indexed by a hypercube of DNN parameter vectors plays the\nrole of the random ﬁeld under consideration. In the proof of Corollary 5.8 we make use of\nthe elementary and well-known fact that this choice for the random ﬁeld is indeed Lipschitz\ncontinuous, which is the assertion of Lemma 5.7. Further results on the optimisation error\nin the context of stochastic approximation can be found, e.g., in [2, 4, 12, 18, 25, 26, 28,\n29, 38, 60, 62, 63, 65, 88, 97, 98] and the references therein.\n23\nThe proof of the main result of this section, Proposition 5.6, crucially relies (cf.\nLemma 5.5) on the complementary distribution function formula (cf., e.g., Elbr¨achter\net al. [35, Lemma 2.2]) and the elementary estimate for the beta function given in Corol-\nlary 5.4. In order to prove Corollary 5.4, we ﬁrst collect a few basic facts about the\ngamma and the beta function in the elementary and well-known Lemma 5.1 and derive\nfrom these in Proposition 5.3 further elementary and essentially well-known properties\nof the gamma function. In particular, the inequalities in (100) in Proposition 5.3 below\nare slightly reformulated versions of the well-known inequalities called Wendel’s double\ninequality (cf. Wendel [94]) or Gautschi’s double inequality (cf. Gautschi [40]); cf., e.g.,\nQi [81, Subsection 2.1 and Subsection 2.4].\n5.1\nProperties of the gamma and the beta function\nLemma 5.1. Let Γ: (0, ∞) →(0, ∞) satisfy for all x ∈(0, ∞) that Γ(x) =\nR ∞\n0 tx−1e−t dt\nand let B: (0, ∞)2 →(0, ∞) satisfy for all x, y ∈(0, ∞) that B(x, y) =\nR 1\n0 tx−1(1−t)y−1 dt.\nThen\n(i) it holds for all x ∈(0, ∞) that Γ(x + 1) = x Γ(x),\n(ii) it holds that Γ(1) = Γ(2) = 1, and\n(iii) it holds for all x, y ∈(0, ∞) that B(x, y) = Γ(x)Γ(y)\nΓ(x+y) .\nLemma 5.2. It holds for all α, x ∈[0, 1] that (1 −x)α ≤1 −αx.\nProof of Lemma 5.2. Note that the fact that for every y ∈[0, ∞) it holds that the function\n[0, ∞) ∋z 7→yz ∈[0, ∞) is a convex function implies for all α, x ∈[0, 1] that\n(1 −x)α = (1 −x)α·1+(1−α)·0\n≤α(1 −x)1 + (1 −α)(1 −x)0\n= α −αx + 1 −α = 1 −αx.\n(99)\nThe proof of Lemma 5.2 is thus complete.\nProposition 5.3. Let Γ: (0, ∞) →(0, ∞) satisfy for all x ∈(0, ∞) that Γ(x) =\nR ∞\n0 tx−1e−t dt and let z·{: (0, ∞) →N0 satisfy for all x ∈(0, ∞) that zx{ = max([0, x) ∩\nN0). Then\n(i) it holds that Γ: (0, ∞) →(0, ∞) is a convex function,\n(ii) it holds for all x ∈(0, ∞) that Γ(x + 1) = x Γ(x) ≤xzx{ ≤max{1, xx},\n(iii) it holds for all x ∈(0, ∞), α ∈[0, 1] that\n(max{x + α −1, 0})α ≤\nx\n(x + α)1−α ≤Γ(x + α)\nΓ(x)\n≤xα,\n(100)\nand\n(iv) it holds for all x ∈(0, ∞), α ∈[0, ∞) that\n(max{x + min{α −1, 0}, 0})α ≤Γ(x + α)\nΓ(x)\n≤(x + max{α −1, 0})α.\n(101)\n24\nProof of Proposition 5.3. First, observe that the fact that for every t ∈(0, ∞) it holds\nthat the function R ∋x 7→tx ∈(0, ∞) is a convex function implies for all x, y ∈(0, ∞),\nα ∈[0, 1] that\nΓ(αx + (1 −α)y) =\nZ ∞\n0\ntαx+(1−α)y−1e−t dt =\nZ ∞\n0\ntαx+(1−α)yt−1e−t dt\n≤\nZ ∞\n0\n(αtx + (1 −α)ty)t−1e−t dt\n= α\nZ ∞\n0\ntx−1e−t dt + (1 −α)\nZ ∞\n0\nty−1e−t dt\n= α Γ(x) + (1 −α)Γ(y).\n(102)\nThis shows (i).\nSecond, note that (ii) in Lemma 5.1 and (i) establish for all α ∈[0, 1] that\nΓ(α + 1) = Γ(α · 2 + (1 −α) · 1) ≤α Γ(2) + (1 −α)Γ(1) = α + (1 −α) = 1.\n(103)\nThis yields for all x ∈(0, 1] that\nΓ(x + 1) ≤1 = xzx{ = max{1, xx}.\n(104)\nInduction, (i) in Lemma 5.1, and the fact that ∀x ∈(0, ∞): x −zx{ ∈(0, 1] hence ensure\nfor all x ∈[1, ∞) that\nΓ(x + 1) =\n\u0014 zx{\nQ\ni=1\n(x −i + 1)\n\u0015\nΓ(x −zx{ + 1) ≤xzx{Γ(x −zx{ + 1) ≤xzx{ ≤xx = max{1, xx}.\n(105)\nCombining this with again (i) in Lemma 5.1 and (104) establishes (ii).\nThird, note that H¨older’s inequality and (i) in Lemma 5.1 prove for all x ∈(0, ∞),\nα ∈[0, 1] that\nΓ(x + α) =\nZ ∞\n0\ntx+α−1e−t dt =\nZ ∞\n0\ntαxe−αtt(1−α)x−(1−α)e−(1−α)t dt\n=\nZ ∞\n0\n[txe−t]α[tx−1e−t]1−α dt\n≤\n\u0012Z ∞\n0\ntxe−t dt\n\u0013α\u0012Z ∞\n0\ntx−1e−t dt\n\u00131−α\n= [Γ(x + 1)]α[Γ(x)]1−α = xα[Γ(x)]α[Γ(x)]1−α\n= xαΓ(x).\n(106)\nThis and again (i) in Lemma 5.1 demonstrate for all x ∈(0, ∞), α ∈[0, 1] that\nx Γ(x) = Γ(x + 1) = Γ(x + α + (1 −α)) ≤(x + α)1−αΓ(x + α).\n(107)\nCombining (106) and (107) yields for all x ∈(0, ∞), α ∈[0, 1] that\nx\n(x + α)1−α ≤Γ(x + α)\nΓ(x)\n≤xα.\n(108)\nFurthermore, observe that (i) in Lemma 5.1 and (108) imply for all x ∈(0, ∞), α ∈[0, 1]\nthat\nΓ(x + α)\nΓ(x + 1) = Γ(x + α)\nx Γ(x)\n≤xα−1.\n(109)\n25\nThis shows for all α ∈[0, 1], x ∈(α, ∞) that\nΓ(x)\nΓ(x + (1 −α)) = Γ((x −α) + α)\nΓ((x −α) + 1) ≤(x −α)α−1 =\n1\n(x −α)1−α.\n(110)\nThis, in turn, ensures for all α ∈[0, 1], x ∈(1 −α, ∞) that\n(x + α −1)α = (x −(1 −α))α ≤Γ(x + α)\nΓ(x)\n.\n(111)\nNext note that Lemma 5.2 proves for all x ∈(0, ∞), α ∈[0, 1] that\n(max{x + α −1, 0})α = (x + α)α\n\u0012max{x + α −1, 0}\nx + α\n\u0013α\n= (x + α)α\n\u0012\nmax\n\u001a\n1 −\n1\nx + α, 0\n\u001b\u0013α\n≤(x + α)α\n\u0012\n1 −\nα\nx + α\n\u0013\n= (x + α)α\n\u0012\nx\nx + α\n\u0013\n=\nx\n(x + α)1−α.\n(112)\nThis and (108) establish (iii).\nFourth, we show (iv). For this let ⌊·⌋: [0, ∞) →N0 satisfy for all x ∈[0, ∞) that\n⌊x⌋= max([0, x] ∩N0). Observe that induction, (i) in Lemma 5.1, the fact that ∀α ∈\n[0, ∞): α −⌊α⌋∈[0, 1), and (iii) demonstrate for all x ∈(0, ∞), α ∈[0, ∞) that\nΓ(x + α)\nΓ(x)\n=\n\u0014⌊α⌋\nQ\ni=1\n(x + α −i)\n\u0015Γ(x + α −⌊α⌋)\nΓ(x)\n≤\n\u0014⌊α⌋\nQ\ni=1\n(x + α −i)\n\u0015\nxα−⌊α⌋\n≤(x + α −1)⌊α⌋xα−⌊α⌋\n≤(x + max{α −1, 0})⌊α⌋(x + max{α −1, 0})α−⌊α⌋\n= (x + max{α −1, 0})α.\n(113)\nFurthermore, again the fact that ∀α ∈[0, ∞): α −⌊α⌋∈[0, 1), (iii), induction, and (i) in\nLemma 5.1 imply for all x ∈(0, ∞), α ∈[0, ∞) that\nΓ(x + α)\nΓ(x)\n= Γ(x + ⌊α⌋+ α −⌊α⌋)\nΓ(x)\n≥(max{x + ⌊α⌋+ α −⌊α⌋−1, 0})α−⌊α⌋\n\u0014Γ(x + ⌊α⌋)\nΓ(x)\n\u0015\n= (max{x + α −1, 0})α−⌊α⌋\n\u0014⌊α⌋\nQ\ni=1\n(x + ⌊α⌋−i)\n\u0015Γ(x)\nΓ(x)\n≥(max{x + α −1, 0})α−⌊α⌋x⌊α⌋\n= (max{x + α −1, 0})α−⌊α⌋(max{x, 0})⌊α⌋\n≥(max{x + min{α −1, 0}, 0})α−⌊α⌋(max{x + min{α −1, 0}, 0})⌊α⌋\n= (max{x + min{α −1, 0}, 0})α.\n(114)\nCombining this with (113) shows (iv). The proof of Proposition 5.3 is thus complete.\n26\nCorollary 5.4. Let B: (0, ∞)2 →(0, ∞) satisfy for all x, y ∈(0, ∞) that B(x, y) =\nR 1\n0 tx−1(1 −t)y−1 dt and let Γ: (0, ∞) →(0, ∞) satisfy for all x ∈(0, ∞) that Γ(x) =\nR ∞\n0 tx−1e−t dt. Then it holds for all x, y ∈(0, ∞) with x + y > 1 that\nΓ(x)\n(y + max{x −1, 0})x ≤B(x, y) ≤\nΓ(x)\n(y + min{x −1, 0})x ≤\nmax{1, xx}\nx(y + min{x −1, 0})x.\n(115)\nProof of Corollary 5.4. Note that (iii) in Lemma 5.1 ensures for all x, y ∈(0, ∞) that\nB(x, y) = Γ(x)Γ(y)\nΓ(y + x) .\n(116)\nIn addition, observe that it holds for all x, y ∈(0, ∞) with x+y > 1 that y+min{x−1, 0} >\n0. This and (iv) in Proposition 5.3 demonstrate for all x, y ∈(0, ∞) with x + y > 1 that\n0 < (y + min{x −1, 0})x ≤Γ(y + x)\nΓ(y)\n≤(y + max{x −1, 0})x.\n(117)\nCombining this with (116) and (ii) in Proposition 5.3 shows for all x, y ∈(0, ∞) with\nx + y > 1 that\nΓ(x)\n(y + max{x −1, 0})x ≤B(x, y) ≤\nΓ(x)\n(y + min{x −1, 0})x ≤\nmax{1, xx}\nx(y + min{x −1, 0})x.\n(118)\nThe proof of Corollary 5.4 is thus complete.\n5.2\nStrong convergence rates for the optimisation error\nLemma 5.5. Let K ∈N, p, L ∈(0, ∞), let (E, δ) be a metric space, let (Ω, F, P) be a\nprobability space, let R: E × Ω→R be a (B(E) ⊗F)/B(R)-measurable function, assume\nfor all x, y ∈E, ω ∈Ωthat |R(x, ω) −R(y, ω)| ≤Lδ(x, y), and let Xk : Ω→E,\nk ∈{1, 2, . . . , K}, be i.i.d. random variables. Then it holds for all x ∈E that\nE\n\u0002\nmink∈{1,2,...,K}|R(Xk) −R(x)|p\u0003\n≤Lp\nZ ∞\n0\n[P(δ(X1, x) > ε\n1/p)]K dε.\n(119)\nProof of Lemma 5.5. Throughout this proof let x ∈E and let Y : Ω→[0, ∞) be the\nfunction which satisﬁes for all ω ∈Ωthat Y (ω) = mink∈{1,2,...,K}[δ(Xk(ω), x)]p.\nOb-\nserve that the fact that Y is a random variable, the assumption that ∀x, y ∈E, ω ∈\nΩ: |R(x, ω) −R(y, ω)| ≤Lδ(x, y), and the complementary distribution function formula\n(see, e.g., Elbr¨achter et al. [35, Lemma 2.2]) demonstrate that\nE\n\u0002\nmink∈{1,2,...,K}|R(Xk) −R(x)|p\u0003\n≤Lp E\n\u0002\nmink∈{1,2,...,K}[δ(Xk, x)]p\u0003\n= Lp E[Y ] = Lp\nZ ∞\n0\ny PY (dy) = Lp\nZ ∞\n0\nPY ((ε, ∞)) dε\n= Lp\nZ ∞\n0\nP(Y > ε) dε = Lp\nZ ∞\n0\nP\n\u0000mink∈{1,2,...,K}[δ(Xk, x)]p > ε\n\u0001\ndε.\n(120)\nMoreover, the assumption that Θk, k ∈{1, 2, . . . , K}, are i.i.d. random variables shows\nfor all ε ∈(0, ∞) that\nP\n\u0000mink∈{1,2,...,K}[δ(Xk, x)]p > ε\n\u0001\n= P\n\u0000∀k ∈{1, 2, . . . , K}: [δ(Xk, x)]p > ε\n\u0001\n=\nKQ\nk=1\nP([δ(Xk, x)]p > ε) = [P([δ(X1, x)]p > ε)]K = [P(δ(X1, x) > ε\n1/p)]K.\n(121)\nCombining (120) with (121) proves (119). The proof of Lemma 5.5 is thus complete.\n27\nProposition 5.6. Let d, K ∈N, L, α ∈R, β ∈(α, ∞), let (Ω, F, P) be a probability\nspace, let R: [α, β]d × Ω→R be a random ﬁeld, assume for all θ, ϑ ∈[α, β]d, ω ∈\nΩthat |R(θ, ω) −R(ϑ, ω)| ≤L∥θ −ϑ∥∞, let Θk : Ω→[α, β]d, k ∈{1, 2, . . . , K}, be\ni.i.d. random variables, and assume that Θ1 is continuous uniformly distributed on [α, β]d\n(cf. Deﬁnition 3.1). Then\n(i) it holds that R is a (B([α, β]d) ⊗F)/B(R)-measurable function and\n(ii) it holds for all θ ∈[α, β]d, p ∈(0, ∞) that\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(Θk) −R(θ)|p\u0003\u00011/p\n≤L(β −α) max{1, (p/d)\n1/d}\nK\n1/d\n≤L(β −α) max{1, p}\nK\n1/d\n.\n(122)\nProof of Proposition 5.6. Throughout this proof assume w.l.o.g. that L > 0, let δ:\n([α, β]d) × ([α, β]d) →[0, ∞) satisfy for all θ, ϑ ∈[α, β]d that δ(θ, ϑ) = ∥θ −ϑ∥∞, let\nB: (0, ∞)2 →(0, ∞) satisfy for all x, y ∈(0, ∞) that B(x, y) =\nR 1\n0 tx−1(1 −t)y−1 dt, and\nlet Θ1,1, Θ1,2, . . . , Θ1,d : Ω→[α, β] satisfy Θ1 = (Θ1,1, Θ1,2, . . . , Θ1,d). First of all, note\nthat the assumption that ∀θ, ϑ ∈[α, β]d, ω ∈Ω: |R(θ, ω) −R(ϑ, ω)| ≤L∥θ −ϑ∥∞en-\nsures for all ω ∈Ωthat the function [α, β]d ∋θ 7→R(θ, ω) ∈R is continuous. Combining\nthis with the fact that ([α, β]d, δ) is a separable metric space, the fact that for every\nθ ∈[α, β]d it holds that the function Ω∋ω 7→R(θ, ω) ∈R is F/B(R)-measurable, and,\ne.g., Aliprantis & Border [1, Lemma 4.51] (see also, e.g., Beck et al. [8, Lemma 2.4])\nproves (i). Next observe that it holds for all θ ∈[α, β], ε ∈[0, ∞) that\nmin{θ + ε, β} −max{θ −ε, α} = min{θ + ε, β} + min{ε −θ, −α}\n= min\n\b\nθ + ε + min{ε −θ, −α}, β + min{ε −θ, −α}\n\t\n= min\n\b\nmin{2ε, θ −α + ε}, min{β −θ + ε, β −α}\n\t\n≥min\n\b\nmin{2ε, α −α + ε}, min{β −β + ε, β −α}\n\t\n= min{2ε, ε, ε, β −α} = min{ε, β −α}.\n(123)\nThe assumption that Θ1 is continuous uniformly distributed on [α, β]d hence shows for\nall θ = (θ1, θ2, . . . , θd) ∈[α, β]d, ε ∈[0, ∞) that\nP(∥Θ1 −θ∥∞≤ε) = P\n\u0000maxi∈{1,2,...,d}|Θ1,i −θi| ≤ε\n\u0001\n= P\n\u0000∀i ∈{1, 2, . . . , d}: −ε ≤Θ1,i −θi ≤ε\n\u0001\n= P\n\u0000∀i ∈{1, 2, . . . , d}: θi −ε ≤Θ1,i ≤θi + ε\n\u0001\n= P\n\u0000∀i ∈{1, 2, . . . , d}: max{θi −ε, α} ≤Θ1,i ≤min{θi + ε, β}\n\u0001\n= P\n\u0000Θ1 ∈\n\u0002\n×d\ni=1[max{θi −ε, α}, min{θi + ε, β}]\n\u0003\u0001\n=\n1\n(β−α)d\ndQ\ni=1\n(min{θi + ε, β} −max{θi −ε, α})\n≥\n1\n(β−α)d[min{ε, β −α}]d = min\nn\n1,\nεd\n(β−α)d\no\n.\n(124)\nTherefore, we obtain for all θ ∈[α, β]d, p ∈(0, ∞), ε ∈[0, ∞) that\nP(∥Θ1 −θ∥∞> ε\n1/p) = 1 −P(∥Θ1 −θ∥∞≤ε\n1/p)\n≤1 −min\nn\n1,\nεd/p\n(β−α)d\no\n= max\nn\n0, 1 −\nεd/p\n(β−α)d\no\n.\n(125)\n28\nThis, (i), the assumption that ∀θ, ϑ ∈[α, β]d, ω ∈Ω: |R(θ, ω) −R(ϑ, ω)| ≤L∥θ −ϑ∥∞,\nthe assumption that Θk, k ∈{1, 2, . . . , K}, are i.i.d. random variables, and Lemma 5.5\n(with (E, δ) ←([α, β]d, δ), (Xk)k∈{1,2,...,K} ←(Θk)k∈{1,2,...,K} in the notation of Lemma 5.5)\nestablish for all θ ∈[α, β]d, p ∈(0, ∞) that\nE\n\u0002\nmink∈{1,2,...,K}|R(Θk) −R(θ)|p\u0003\n≤Lp\nZ ∞\n0\n[P(∥Θ1 −θ∥∞> ε\n1/p)]K dε\n≤Lp\nZ ∞\n0\nh\nmax\nn\n0, 1 −\nεd/p\n(β−α)d\noiK\ndε = Lp\nZ (β−α)p\n0\n\u0010\n1 −\nεd/p\n(β−α)d\n\u0011K\ndε\n= p\ndLp(β −α)p\nZ 1\n0\nt\np/d−1(1 −t)K dt = p\ndLp(β −α)p\nZ 1\n0\nt\np/d−1(1 −t)K+1−1 dt\n= p\ndLp(β −α)p B(p/d, K + 1).\n(126)\nCorollary 5.4 (with x ←p/d, y ←K + 1 for p ∈(0, ∞) in the notation of (115) in\nCorollary 5.4) hence demonstrates for all θ ∈[α, β]d, p ∈(0, ∞) that\nE\n\u0002\nmink∈{1,2,...,K}|R(Θk) −R(θ)|p\u0003\n≤\np\ndLp(β −α)p max{1, (p/d)\np/d}\np\nd(K + 1 + min{p/d −1, 0})\np/d ≤Lp(β −α)p max{1, (p/d)\np/d}\nK\np/d\n.\n(127)\nThis implies for all θ ∈[α, β]d, p ∈(0, ∞) that\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(Θk) −R(θ)|p\u0003\u00011/p\n≤L(β −α) max{1, (p/d)\n1/d}\nK\n1/d\n≤L(β −α) max{1, p}\nK\n1/d\n.\n(128)\nThis shows (ii) and thus completes the proof of Proposition 5.6.\nLemma 5.7. Let d, d, L, M ∈N, B, b ∈[1, ∞), u ∈R, v ∈(u, ∞), l = (l0, l1, . . . , lL) ∈\nNL+1, D ⊆[−b, b]d, assume l0 = d, lL = 1, and d ≥PL\ni=1 li(li−1 + 1), let Ωbe a set, let\nXj : Ω→D, j ∈{1, 2, . . . , M}, and Yj : Ω→[u, v], j ∈{1, 2, . . . , M}, be functions, and\nlet R: [−B, B]d × Ω→[0, ∞) satisfy for all θ ∈[−B, B]d, ω ∈Ωthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\n(129)\n(cf. Deﬁnition 2.8). Then it holds for all θ, ϑ ∈[−B, B]d, ω ∈Ωthat\n|R(θ, ω) −R(ϑ, ω)| ≤2(v −u)bL(∥l∥∞+ 1)LBL−1∥θ −ϑ∥∞\n(130)\n(cf. Deﬁnition 3.1).\nProof of Lemma 5.7. Observe that the fact that ∀x1, x2, y ∈R: (x1 −y)2 −(x2 −y)2 =\n(x1 −x2)((x1 −y) + (x2 −y)), the fact that ∀θ ∈Rd, x ∈Rd : N θ,l\nu,v (x) ∈[u, v], and the\nassumption that ∀j ∈{1, 2, . . . , M}, ω ∈Ω: Yj(ω) ∈[u, v] prove for all θ, ϑ ∈[−B, B]d,\n29\nω ∈Ωthat\n|R(θ, ω) −R(ϑ, ω)|\n= 1\nM\n\f\f\f\f\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\n−\n\u0014 M\nP\nj=1\n|N ϑ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\f\f\f\f\n≤1\nM\n\u0014 M\nP\nj=1\n\f\f[N θ,l\nu,v (Xj(ω)) −Yj(ω)]2 −[N ϑ,l\nu,v (Xj(ω)) −Yj(ω)]2\f\f\n\u0015\n= 1\nM\n\u0014 M\nP\nj=1\n\u0000\f\fN θ,l\nu,v (Xj(ω)) −N ϑ,l\nu,v (Xj(ω))\n\f\f\n·\n\f\f[N θ,l\nu,v (Xj(ω)) −Yj(ω)] + [N ϑ,l\nu,v (Xj(ω)) −Yj(ω)]\n\f\f\u0001\u0015\n≤2\nM\n\u0014 M\nP\nj=1\n\u0000\u0002\nsupx∈D|N θ,l\nu,v (x) −N ϑ,l\nu,v (x)|\n\u0003\u0002\nsupy1,y2∈[u,v]|y1 −y2|\n\u0003\u0001\u0015\n= 2(v −u)\n\u0002\nsupx∈D|N θ,l\nu,v (x) −N ϑ,l\nu,v (x)|\n\u0003\n.\n(131)\nIn addition, combining the assumptions that D ⊆[−b, b]d, d ≥PL\ni=1 li(li−1 + 1), l0 = d,\nlL = 1, b ≥1, and B ≥1 with Beck, Jentzen, & Kuckuck [10, Corollary 2.37] (with\na ←−b, b ←b, u ←u, v ←v, d ←d, L ←L, l ←l in the notation of [10, Corollary 2.37])\nshows for all θ, ϑ ∈[−B, B]d that\nsupx∈D|N θ,l\nu,v (x) −N ϑ,l\nu,v (x)| ≤supx∈[−b,b]d|N θ,l\nu,v (x) −N ϑ,l\nu,v (x)|\n≤L max{1, b}(∥l∥∞+ 1)L(max{1, ∥θ∥∞, ∥ϑ∥∞})L−1∥θ −ϑ∥∞\n≤bL(∥l∥∞+ 1)LBL−1∥θ −ϑ∥∞.\n(132)\nThis and (131) imply for all θ, ϑ ∈[−B, B]d, ω ∈Ωthat\n|R(θ, ω) −R(ϑ, ω)| ≤2(v −u)bL(∥l∥∞+ 1)LBL−1∥θ −ϑ∥∞.\n(133)\nThe proof of Lemma 5.7 is thus complete.\nCorollary 5.8. Let d, d, d, L, M, K ∈N, B, b ∈[1, ∞), u ∈R, v ∈(u, ∞), l =\n(l0, l1, . . . , lL) ∈NL+1, D ⊆[−b, b]d, assume l0 = d, lL = 1, and d ≥d = PL\ni=1 li(li−1 +1),\nlet (Ω, F, P) be a probability space, let Θk : Ω→[−B, B]d, k ∈{1, 2, . . . , K}, be i.i.d.\nrandom variables, assume that Θ1 is continuous uniformly distributed on [−B, B]d, let\nXj : Ω→D, j ∈{1, 2, . . . , M}, and Yj : Ω→[u, v], j ∈{1, 2, . . . , M}, be random vari-\nables, and let R: [−B, B]d × Ω→[0, ∞) satisfy for all θ ∈[−B, B]d, ω ∈Ωthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\n(134)\n(cf. Deﬁnition 2.8). Then\n(i) it holds that R is a (B([−B, B]d) ⊗F)/B([0, ∞))-measurable function and\n(ii) it holds for all θ ∈[−B, B]d, p ∈(0, ∞) that\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(Θk) −R(θ)|p\u0003\u00011/p\n(135)\n≤4(v −u)bL(∥l∥∞+ 1)LBLp\nmax{1, p/d}\nK\n1/d\n≤4(v −u)bL(∥l∥∞+ 1)LBL max{1, p}\nK[L−1(∥l∥∞+1)−2]\n(cf. Deﬁnition 3.1).\n30\nProof of Corollary 5.8. Throughout this proof let L ∈R be given by L = 2(v−u)bL(∥l∥∞\n+1)LBL−1, let P : [−B, B]d →[−B, B]d satisfy for all θ = (θ1, θ2, . . . , θd) ∈[−B, B]d that\nP(θ) = (θ1, θ2, . . . , θd), and let R: [−B, B]d × Ω→R satisfy for all θ ∈[−B, B]d, ω ∈Ω\nthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\n.\n(136)\nNote that the fact that ∀θ ∈[−B, B]d : N θ,l\nu,v = N P(θ),l\nu,v\nimplies for all θ ∈[−B, B]d,\nω ∈Ωthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\n= 1\nM\n\u0014 M\nP\nj=1\n|N P(θ),l\nu,v\n(Xj(ω)) −Yj(ω)|2\n\u0015\n= R(P(θ), ω).\n(137)\nFurthermore, Lemma 5.7 (with d ←d, R ←([−B, B]d × Ω∋(θ, ω) 7→R(θ, ω) ∈[0, ∞))\nin the notation of Lemma 5.7) demonstrates for all θ, ϑ ∈[−B, B]d, ω ∈Ωthat\n|R(θ, ω) −R(ϑ, ω)| ≤2(v −u)bL(∥l∥∞+ 1)LBL−1∥θ −ϑ∥∞= L∥θ −ϑ∥∞.\n(138)\nMoreover, observe that the assumption that Xj, j ∈{1, 2, . . . , M}, and Yj, j ∈{1, 2, . . . ,\nM}, are random variables ensures that R: [−B, B]d × Ω→R is a random ﬁeld. This,\n(138), the fact that P ◦Θk : Ω→[−B, B]d, k ∈{1, 2, . . . , K}, are i.i.d. random variables,\nthe fact that P ◦Θ1 is continuous uniformly distributed on [−B, B]d, and Proposition 5.6\n(with d ←d, α ←−B, β ←B, R ←R, (Θk)k∈{1,2,...,K} ←(P ◦Θk)k∈{1,2,...,K} in\nthe notation of Proposition 5.6) prove for all θ ∈[−B, B]d, p ∈(0, ∞) that R is a\n(B([−B, B]d) ⊗F)/B(R)-measurable function and\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(P(Θk)) −R(P(θ))|p\u0003\u00011/p\n≤L(2B) max{1, (p/d)\n1/d}\nK\n1/d\n= 4(v −u)bL(∥l∥∞+ 1)LBL max{1, (p/d)\n1/d}\nK\n1/d\n.\n(139)\nThe fact that P is a B([−B, B]d)/B([−B, B]d)-measurable function and (137) hence\nshow (i).\nIn addition, (137), (139), and the fact that 2 ≤d = PL\ni=1 li(li−1 + 1) ≤\nL(∥l∥∞+ 1)2 yield for all θ ∈[−B, B]d, p ∈(0, ∞) that\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(Θk) −R(θ)|p\u0003\u00011/p\n=\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(P(Θk)) −R(P(θ))|p\u0003\u00011/p\n(140)\n≤4(v −u)bL(∥l∥∞+ 1)LBLp\nmax{1, p/d}\nK\n1/d\n≤4(v −u)bL(∥l∥∞+ 1)LBL max{1, p}\nK[L−1(∥l∥∞+1)−2]\n.\nThis establishes (ii). The proof of Corollary 5.8 is thus complete.\n6\nAnalysis of the overall error\nIn Subsection 6.2 below we present the main result of this article, Theorem 6.5, that\nprovides an estimate for the overall L2-error arising in deep learning based empirical\nrisk minimisation with quadratic loss function in the probabilistically strong sense and\nthat covers the case where the underlying DNNs are trained using a general stochastic\noptimisation algorithm with random initialisation.\n31\nIn order to prove Theorem 6.5, we require a link to combine the results from Sections 3,\n4, and 5, which is given in Subsection 6.1 below. More speciﬁcally, Proposition 6.1 in\nSubsection 6.1 shows that the overall error can be decomposed into three diﬀerent error\nsources: the approximation error (cf. Section 3), the worst-case generalisation error (cf.\nSection 4), and the optimisation error (cf. Section 5). Proposition 6.1 is a consequence\nof the well-known bias–variance decomposition (cf., e.g., Beck, Jentzen, & Kuckuck [10,\nLemma 4.1] or Berner, Grohs, & Jentzen [13, Lemma 2.2]) and is very similar to [10,\nLemma 4.3].\nThereafter, Subsection 6.2 is devoted to strong convergence results for deep learning\nbased empirical risk minimisation with quadratic loss function where a general stochastic\napproximation algorithm with random initialisation is allowed to be the employed op-\ntimisation method. Apart from the main result (cf. Theorem 6.5), Subsection 6.2 also\nincludes on the one hand Proposition 6.3, which combines the overall error decompo-\nsition (cf. Proposition 6.1) with our convergence result for the generalisation error (cf.\nCorollary 4.15 in Section 4) and our convergence result for the optimisation error (cf.\nCorollary 5.8 in Section 5), and on the other hand Corollary 6.6, which replaces the ar-\nchitecture parameter A ∈(0, ∞) in Theorem 6.5 (cf. Proposition 3.5) by the minimum of\nthe depth parameter L ∈N and the hidden layer sizes l1, l2, . . . , lL−1 ∈N of the trained\nDNN (cf. (178) below).\nFinally, in Subsection 6.3 we present three more strong convergence results for the spe-\ncial case where SGD with random initialisation is the employed optimisation method. In\nparticular, Corollary 6.7 speciﬁes Corollary 6.6 to this special case, Corollary 6.8 provides\na convergence estimate for the expectation of the L1-distance between the trained DNN\nand the target function, and Corollary 6.9 reaches an analogous conclusion in a simpliﬁed\nsetting.\n6.1\nOverall error decomposition\nProposition 6.1. Let d, d, L, M, K, N ∈N, B ∈[0, ∞), u ∈R, v ∈(u, ∞), l =\n(l0, l1, . . . , lL) ∈NL+1, N ⊆{0, 1, . . . , N}, D ⊆Rd, assume 0 ∈N, l0 = d, lL =\n1, and d ≥PL\ni=1 li(li−1 + 1), let (Ω, F, P) be a probability space, let Xj : Ω→D,\nj ∈{1, 2, . . . , M}, and Yj : Ω→[u, v], j ∈{1, 2, . . . , M}, be random variables, let\nE : D →[u, v] be a B(D)/B([u, v])-measurable function, assume that it holds P-a.s. that\nE(X1) = E[Y1|X1], let Θk,n : Ω→Rd, k, n ∈N0, satisfy\n\u0000S∞\nk=1 Θk,0(Ω)\n\u0001\n⊆[−B, B]d,\nlet R: Rd →[0, ∞) satisfy for all θ ∈Rd that R(θ) = E[|N θ,l\nu,v (X1) −Y1|2], and let\nR: Rd × Ω→[0, ∞) and k: Ω→(N0)2 satisfy for all θ ∈Rd, ω ∈Ωthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\nand\n(141)\nk(ω) ∈arg min(k,n)∈{1,2,...,K}×N, ∥Θk,n(ω)∥∞≤B R(Θk,n(ω), ω)\n(142)\n(cf. Deﬁnitions 2.8 and 3.1). Then it holds for all ϑ ∈[−B, B]d that\nZ\nD\n|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n≤\n\u0002\nsupx∈D|N ϑ,l\nu,v (x) −E(x)|2\u0003\n+ 2\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|\n\u0003\n+ min(k,n)∈{1,2,...,K}×N, ∥Θk,n∥∞≤B|R(Θk,n) −R(ϑ)|.\n(143)\nProof of Proposition 6.1. Throughout this proof let R : L2(PX1; R) →[0, ∞) satisfy for\nall f ∈L2(PX1; R) that R(f) = E[|f(X1) −Y1|2]. Observe that the assumption that\n32\n∀ω ∈Ω: Y1(ω) ∈[u, v] and the fact that ∀θ ∈Rd, x ∈Rd : N θ,l\nu,v (x) ∈[u, v] ensure for all\nθ ∈Rd that E[|Y1|2] ≤v2 < ∞and\nZ\nD\n|N θ,l\nu,v (x)|2 PX1(dx) = E\n\u0002\n|N θ,l\nu,v (X1)|2\u0003\n≤v2 < ∞.\n(144)\nThe bias–variance decomposition (cf., e.g., Beck, Jentzen, & Kuckuck [10, (iii) in Lem-\nma 4.1] with (Ω, F, P) ←(Ω, F, P), (S, S) ←(D, B(D)), X ←X1, Y ←(Ω∋ω 7→\nY1(ω) ∈R), E ←R, f ←N θ,l\nu,v |D, g ←N ϑ,l\nu,v |D for θ, ϑ ∈Rd in the notation of [10, (iii)\nin Lemma 4.1]) hence proves for all θ, ϑ ∈Rd that\nZ\nD\n|N θ,l\nu,v (x) −E(x)|2 PX1(dx)\n= E\n\u0002\n|N θ,l\nu,v (X1) −E(X1)|2\u0003\n= E\n\u0002\n|N θ,l\nu,v (X1) −E[Y1|X1]|2\u0003\n= E\n\u0002\n|N ϑ,l\nu,v (X1) −E[Y1|X1]|2\u0003\n+ R(N θ,l\nu,v |D) −R(N ϑ,l\nu,v |D)\n(145)\n= E\n\u0002\n|N ϑ,l\nu,v (X1) −E(X1)|2\u0003\n+ E\n\u0002\n|N θ,l\nu,v (X1) −Y1|2\u0003\n−E\n\u0002\n|N ϑ,l\nu,v (X1) −Y1|2\u0003\n=\nZ\nD\n|N ϑ,l\nu,v (x) −E(x)|2 PX1(dx) + R(θ) −R(ϑ).\nThis implies for all θ, ϑ ∈Rd that\nZ\nD\n|N θ,l\nu,v (x) −E(x)|2 PX1(dx)\n=\nZ\nD\n|N ϑ,l\nu,v (x) −E(x)|2 PX1(dx) −[R(θ) −R(θ)] + R(ϑ) −R(ϑ) + R(θ) −R(ϑ)\n≤\nZ\nD\n|N ϑ,l\nu,v (x) −E(x)|2 PX1(dx) + |R(θ) −R(θ)| + |R(ϑ) −R(ϑ)| + R(θ) −R(ϑ)\n≤\nZ\nD\n|N ϑ,l\nu,v (x) −E(x)|2 PX1(dx) + 2\n\u0002\nmaxη∈{θ,ϑ}|R(η) −R(η)|\n\u0003\n+ R(θ) −R(ϑ).\n(146)\nNext note that the fact that ∀ω ∈Ω: ∥Θk(ω)(ω)∥∞≤B ensures for all ω ∈Ωthat\nΘk(ω)(ω) ∈[−B, B]d. Combining (146) with (142) hence establishes for all ϑ ∈[−B, B]d\nthat\nZ\nD\n|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n≤\nZ\nD\n|N ϑ,l\nu,v (x) −E(x)|2 PX1(dx) + 2\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|\n\u0003\n+ R(Θk) −R(ϑ)\n=\nZ\nD\n|N ϑ,l\nu,v (x) −E(x)|2 PX1(dx) + 2\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|\n\u0003\n+ min(k,n)∈{1,2,...,K}×N, ∥Θk,n∥∞≤B[R(Θk,n) −R(ϑ)]\n≤\n\u0002\nsupx∈D|N ϑ,l\nu,v (x) −E(x)|2\u0003\n+ 2\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|\n\u0003\n+ min(k,n)∈{1,2,...,K}×N, ∥Θk,n∥∞≤B|R(Θk,n) −R(ϑ)|.\n(147)\nThe proof of Proposition 6.1 is thus complete.\n6.2\nFull strong error analysis for the training of DNNs\nLemma 6.2. Let d, d, L ∈N, p ∈[0, ∞), u ∈[−∞, ∞), v ∈(u, ∞], l = (l0, l1, . . . , lL) ∈\nNL+1, D ⊆Rd, assume l0 = d, lL = 1, and d ≥PL\ni=1 li(li−1 + 1), let E : D →R be a\nB(D)/B(R)-measurable function, let (Ω, F, P) be a probability space, and let X : Ω→D,\nk: Ω→(N0)2, and Θk,n : Ω→Rd, k, n ∈N0, be random variables. Then\n33\n(i) it holds that the function Rd×Rd ∋(θ, x) 7→N θ,l\nu,v (x) ∈R is (B(Rd)⊗B(Rd))/B(R)-\nmeasurable,\n(ii) it holds that the function Ω∋ω 7→Θk(ω)(ω) ∈Rd is F/B(Rd)-measurable, and\n(iii) it holds that the function\nΩ∋ω 7→\nZ\nD\n|N Θk(ω)(ω),l\nu,v\n(x) −E(x)|p PX(dx) ∈[0, ∞]\n(148)\nis F/B([0, ∞])-measurable\n(cf. Deﬁnition 2.8).\nProof of Lemma 6.2. First, observe that Beck, Jentzen, & Kuckuck [10, Corollary 2.37]\n(with a ←−∥x∥∞, b ←∥x∥∞, u ←u, v ←v, d ←d, L ←L, l ←l for x ∈Rd in the\nnotation of [10, Corollary 2.37]) demonstrates for all x ∈Rd, θ, ϑ ∈Rd that\n|N θ,l\nu,v (x) −N ϑ,l\nu,v (x)| ≤supy∈[−∥x∥∞,∥x∥∞]l0|N θ,l\nu,v (y) −N ϑ,l\nu,v (y)|\n≤L max{1, ∥x∥∞}(∥l∥∞+ 1)L(max{1, ∥θ∥∞, ∥ϑ∥∞})L−1∥θ −ϑ∥∞\n(149)\n(cf. Deﬁnition 3.1). This implies for all x ∈Rd that the function\nRd ∋θ 7→N θ,l\nu,v (x) ∈R\n(150)\nis continuous. In addition, the fact that ∀θ ∈Rd : N θ,l\nu,v ∈C(Rd, R) ensures for all θ ∈Rd\nthat the function Rd ∋x 7→N θ,l\nu,v (x) ∈R is B(Rd)/B(R)-measurable. This, (150), the fact\nthat (Rd, ∥·∥∞|Rd) is a separable normed R-vector space, and, e.g., Aliprantis & Border [1,\nLemma 4.51] (see also, e.g., Beck et al. [8, Lemma 2.4]) show (i).\nSecond, we prove (ii). For this let Ξ: Ω→Rd satisfy for all ω ∈Ωthat Ξ(ω) =\nΘk(ω)(ω). Observe that the assumption that Θk,n : Ω→Rd, k, n ∈N0, and k: Ω→(N0)2\nare random variables establishes for all U ∈B(Rd) that\nΞ−1(U) = {ω ∈Ω: Ξ(ω) ∈U} = {ω ∈Ω: Θk(ω)(ω) ∈U}\n=\n\b\nω ∈Ω:\n\u0002\n∃k, n ∈N0 : ([Θk,n(ω) ∈U] ∧[k(ω) = (k, n)])\n\u0003\t\n=\n∞S\nk=0\n∞S\nn=0\n\u0000{ω ∈Ω: Θk,n(ω) ∈U} ∩{ω ∈Ω: k(ω) = (k, n)}\n\u0001\n=\n∞S\nk=0\n∞S\nn=0\n\u0000[(Θk,n)−1(U)] ∩[k−1({(k, n)})]\n\u0001\n∈F.\n(151)\nThis implies (ii).\nThird, note that (i)–(ii) yield that the function Ω×Rd ∋(ω, x) 7→N Θk(ω)(ω),l\nu,v\n(x) ∈R is\n(F ⊗B(Rd))/B(R)-measurable. This and the assumption that E : D →R is B(D)/B(R)-\nmeasurable demonstrate that the function Ω× D ∋(ω, x) 7→|N Θk(ω)(ω),l\nu,v\n(x) −E(x)|p ∈\n[0, ∞) is (F ⊗B(D))/B([0, ∞))-measurable. Tonelli’s theorem hence establishes (iii). The\nproof of Lemma 6.2 is thus complete.\nProposition 6.3. Let d, d, L, M, K, N ∈N, b, c ∈[1, ∞), B ∈[c, ∞), u ∈R, v ∈(u, ∞),\nl = (l0, l1, . . . , lL) ∈NL+1, N ⊆{0, 1, . . . , N}, D ⊆[−b, b]d, assume 0 ∈N, l0 = d,\nlL = 1, and d ≥PL\ni=1 li(li−1 + 1), let (Ω, F, P) be a probability space, let Xj : Ω→D,\nj ∈N, and Yj : Ω→[u, v], j ∈N, be functions, assume that (Xj, Yj), j ∈{1, 2, . . . , M},\nare i.i.d. random variables, let E : D →[u, v] be a B(D)/B([u, v])-measurable function,\n34\nassume that it holds P-a.s. that E(X1) = E[Y1|X1], let Θk,n : Ω→Rd, k, n ∈N0, and\nk: Ω→(N0)2 be random variables, assume\n\u0000S∞\nk=1 Θk,0(Ω)\n\u0001\n⊆[−B, B]d, assume that\nΘk,0, k ∈{1, 2, . . . , K}, are i.i.d., assume that Θ1,0 is continuous uniformly distributed on\n[−c, c]d, and let R: Rd × Ω→[0, ∞) satisfy for all θ ∈Rd, ω ∈Ωthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\nand\n(152)\nk(ω) ∈arg min(k,n)∈{1,2,...,K}×N, ∥Θk,n(ω)∥∞≤B R(Θk,n(ω), ω)\n(153)\n(cf. Deﬁnitions 2.8 and 3.1). Then it holds for all p ∈(0, ∞) that\n\u0010\nE\nh\u0010Z\nD|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n\u0011p i\u00111/p\n≤\n\u0002\ninfθ∈[−c,c]d supx∈D|N θ,l\nu,v (x) −E(x)|2\u0003\n+ 4(v −u)bL(∥l∥∞+ 1)LcL max{1, p}\nK[L−1(∥l∥∞+1)−2]\n+ 18 max{1, (v −u)2}L(∥l∥∞+ 1)2 max{p, ln(3MBb)}\n√\nM\n≤\n\u0002\ninfθ∈[−c,c]d supx∈D|N θ,l\nu,v (x) −E(x)|2\u0003\n+ 20 max{1, (v −u)2}bL(∥l∥∞+ 1)L+1BL max{p, ln(3M)}\nmin{\n√\nM, K[L−1(∥l∥∞+1)−2]}\n(154)\n(cf. (iii) in Lemma 6.2).\nProof of Proposition 6.3. Throughout this proof let R: Rd →[0, ∞) satisfy for all θ ∈\nRd that R(θ) = E[|N θ,l\nu,v (X1) −Y1|2].\nFirst of all, observe that the assumption that\n\u0000S∞\nk=1 Θk,0(Ω)\n\u0001\n⊆[−B, B]d, the assumption that 0 ∈N, and Proposition 6.1 show for all\nϑ ∈[−B, B]d that\nZ\nD\n|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n≤\n\u0002\nsupx∈D|N ϑ,l\nu,v (x) −E(x)|2\u0003\n+ 2\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|\n\u0003\n+ min(k,n)∈{1,2,...,K}×N, ∥Θk,n∥∞≤B|R(Θk,n) −R(ϑ)|\n≤\n\u0002\nsupx∈D|N ϑ,l\nu,v (x) −E(x)|2\u0003\n+ 2\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|\n\u0003\n+ mink∈{1,2,...,K}, ∥Θk,0∥∞≤B|R(Θk,0) −R(ϑ)|\n=\n\u0002\nsupx∈D|N ϑ,l\nu,v (x) −E(x)|2\u0003\n+ 2\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|\n\u0003\n+ mink∈{1,2,...,K}|R(Θk,0) −R(ϑ)|.\n(155)\nMinkowski’s inequality hence establishes for all p ∈[1, ∞), ϑ ∈[−c, c]d ⊆[−B, B]d that\n\u0010\nE\nh\u0010Z\nD|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n\u0011p i\u00111/p\n≤\n\u0000E\n\u0002\nsupx∈D|N ϑ,l\nu,v (x) −E(x)|2p\u0003\u00011/p + 2\n\u0000E\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|p\u0003\u00011/p\n+\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(Θk,0) −R(ϑ)|p\u0003\u00011/p\n≤\n\u0002\nsupx∈D|N ϑ,l\nu,v (x) −E(x)|2\u0003\n+ 2\n\u0000E\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|p\u0003\u00011/p\n+ supθ∈[−c,c]d\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(Θk,0) −R(θ)|p\u0003\u00011/p\n(156)\n(cf. (i) in Corollary 4.15 and (i) in Corollary 5.8). Next note that Corollary 4.15 (with\nv ←max{u + 1, v}, R ←R|[−B,B]d, R ←R|[−B,B]d×Ωin the notation of Corollary 4.15)\n35\nproves for all p ∈(0, ∞) that\n\u0000E\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|p\u0003\u00011/p\n≤9(max{u + 1, v} −u)2L(∥l∥∞+ 1)2 max{p, ln(3MBb)}\n√\nM\n= 9 max{1, (v −u)2}L(∥l∥∞+ 1)2 max{p, ln(3MBb)}\n√\nM\n.\n(157)\nIn addition, observe that Corollary 5.8 (with d ←PL\ni=1 li(li−1 +1), B ←c, (Θk)k∈{1,2,...,K}\n←(Ω∋ω 7→1{Θk,0∈[−c,c]d}(ω)Θk,0(ω) ∈[−c, c]d)k∈{1,2,...,K}, R ←R|[−c,c]d×Ωin the nota-\ntion of Corollary 5.8) implies for all p ∈(0, ∞) that\nsupθ∈[−c,c]d\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(Θk,0) −R(θ)|p\u0003\u00011/p\n= supθ∈[−c,c]d\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(1{Θk,0∈[−c,c]d}Θk,0) −R(θ)|p\u0003\u00011/p\n≤4(v −u)bL(∥l∥∞+ 1)LcL max{1, p}\nK[L−1(∥l∥∞+1)−2]\n.\n(158)\nCombining this, (156), (157), and the fact that ln(3MBb) ≥1 with Jensen’s inequality\ndemonstrates for all p ∈(0, ∞) that\n\u0010\nE\nh\u0010Z\nD|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n\u0011p i\u00111/p\n≤\n\u0012\nE\n\u0014\u0010Z\nD|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n\u0011max{1,p}\u0015\u0013\n1\nmax{1,p}\n≤\n\u0002\ninfθ∈[−c,c]d supx∈D|N θ,l\nu,v (x) −E(x)|2\u0003\n+ supθ∈[−c,c]d\n\u0000E\n\u0002\nmink∈{1,2,...,K}|R(Θk,0) −R(θ)|max{1,p}\u0003\u0001\n1\nmax{1,p}\n+ 2\n\u0000E\n\u0002\nsupθ∈[−B,B]d|R(θ) −R(θ)|max{1,p}\u0003\u0001\n1\nmax{1,p}\n≤\n\u0002\ninfθ∈[−c,c]d supx∈D|N θ,l\nu,v (x) −E(x)|2\u0003\n+ 4(v −u)bL(∥l∥∞+ 1)LcL max{1, p}\nK[L−1(∥l∥∞+1)−2]\n+ 18 max{1, (v −u)2}L(∥l∥∞+ 1)2 max{p, ln(3MBb)}\n√\nM\n.\n(159)\nMoreover, note that the fact that ∀x ∈[0, ∞): x+1 ≤ex ≤3x and the facts that Bb ≥1\nand M ≥1 ensure that\nln(3MBb) ≤ln(3M3Bb−1) = ln(3BbM) = Bb ln([3BbM]\n1/(Bb)) ≤Bb ln(3M).\n(160)\nThe facts that ∥l∥∞+ 1 ≥2, B ≥c ≥1, ln(3M) ≥1, b ≥1, and L ≥1 hence show for\nall p ∈(0, ∞) that\n4(v −u)bL(∥l∥∞+ 1)LcL max{1, p}\nK[L−1(∥l∥∞+1)−2]\n+ 18 max{1, (v −u)2}L(∥l∥∞+ 1)2 max{p, ln(3MBb)}\n√\nM\n≤2(∥l∥∞+ 1) max{1, (v −u)2}bL(∥l∥∞+ 1)LBL max{p, ln(3M)}\nK[L−1(∥l∥∞+1)−2]\n+ 18 max{1, (v −u)2}bL(∥l∥∞+ 1)2B max{p, ln(3M)}\n√\nM\n≤20 max{1, (v −u)2}bL(∥l∥∞+ 1)L+1BL max{p, ln(3M)}\nmin{\n√\nM, K[L−1(∥l∥∞+1)−2]}\n.\n(161)\n36\nThis and (159) complete the proof of Proposition 6.3.\nLemma 6.4. Let a, x, p ∈(0, ∞), M, c ∈[1, ∞), B ∈[c, ∞). Then\n(i) it holds that axp ≤exp\n\u0000a\n1/p px\ne\n\u0001\nand\n(ii) it holds that ln(3MBc) ≤23B\n18 ln(eM).\nProof of Lemma 6.4. First, note that the fact that ∀y ∈R: y +1 ≤ey demonstrates that\naxp = (a\n1/px)p =\n\u0002\ne\n\u0000a\n1/p x\ne −1 + 1\n\u0001\u0003p ≤\n\u0002\ne exp\n\u0000a\n1/p x\ne −1\n\u0001\u0003p = exp\n\u0000a\n1/p px\ne\n\u0001\n.\n(162)\nThis proves (i).\nSecond, observe that (i) and the fact that 2\n√\n3/e ≤23/18 ensure that\n3B2 ≤exp\n\u0000√\n32B\ne\n\u0001\n= exp\n\u0000 2\n√\n3B\ne\n\u0001\n≤exp\n\u0000 23B\n18\n\u0001\n.\n(163)\nThe facts that B ≥c ≥1 and M ≥1 hence imply that\nln(3MBc) ≤ln(3B2M) ≤ln([eM]\n23B/18) = 23B\n18 ln(eM).\n(164)\nThis establishes (ii). The proof of Lemma 6.4 is thus complete.\nTheorem 6.5. Let d, d, L, M, K, N ∈N, A ∈(0, ∞), L, a, u ∈R, b ∈(a, ∞), v ∈\n(u, ∞), c ∈[max{1, L, |a|, |b|, 2|u|, 2|v|}, ∞), B ∈[c, ∞), l = (l0, l1, . . . , lL) ∈NL+1, N ⊆\n{0, 1, . . . , N}, assume 0 ∈N, L ≥A1(6d,∞)(A)/(2d)+1, l0 = d, l1 ≥A1(6d,∞)(A), lL = 1, and\nd ≥PL\ni=1 li(li−1 + 1), assume for all i ∈{2, 3, . . .} ∩[0, L) that li ≥1(6d,∞)(A) max{A/d −\n2i + 3, 2}, let (Ω, F, P) be a probability space, let Xj : Ω→[a, b]d, j ∈N, and Yj : Ω→\n[u, v], j ∈N, be functions, assume that (Xj, Yj), j ∈{1, 2, . . . , M}, are i.i.d. random\nvariables, let E : [a, b]d →[u, v] satisfy P-a.s. that E(X1) = E[Y1|X1], assume for all\nx, y ∈[a, b]d that |E(x) −E(y)| ≤L∥x −y∥1, let Θk,n : Ω→Rd, k, n ∈N0, and k: Ω→\n(N0)2 be random variables, assume\n\u0000S∞\nk=1 Θk,0(Ω)\n\u0001\n⊆[−B, B]d, assume that Θk,0, k ∈\n{1, 2, . . . , K}, are i.i.d., assume that Θ1,0 is continuous uniformly distributed on [−c, c]d,\nand let R: Rd × Ω→[0, ∞) satisfy for all θ ∈Rd, ω ∈Ωthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\nand\n(165)\nk(ω) ∈arg min(k,n)∈{1,2,...,K}×N, ∥Θk,n(ω)∥∞≤B R(Θk,n(ω), ω)\n(166)\n(cf. Deﬁnitions 2.8 and 3.1). Then it holds for all p ∈(0, ∞) that\n\u0010\nE\nh\u0010Z\n[a,b]d|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n\u0011p i\u00111/p\n≤9d2L2(b −a)2\nA\n2/d\n+ 4(v −u)L(∥l∥∞+ 1)LcL+1 max{1, p}\nK[L−1(∥l∥∞+1)−2]\n+ 18 max{1, (v −u)2}L(∥l∥∞+ 1)2 max{p, ln(3MBc)}\n√\nM\n≤36d2c4\nA\n2/d\n+ 4L(∥l∥∞+ 1)LcL+2 max{1, p}\nK[L−1(∥l∥∞+1)−2]\n+ 23B3L(∥l∥∞+ 1)2 max{p, ln(eM)}\n√\nM\n(167)\n(cf. (iii) in Lemma 6.2).\n37\nProof of Theorem 6.5. First of all, note that the assumption that ∀x, y ∈[a, b]d : |E(x) −\nE(y)| ≤L∥x −y∥1 ensures that E : [a, b]d →[u, v] is a B([a, b]d)/B([u, v])-measurable\nfunction. The fact that max{1, |a|, |b|} ≤c and Proposition 6.3 (with b ←max{1, |a|, |b|},\nD ←[a, b]d in the notation of Proposition 6.3) hence show for all p ∈(0, ∞) that\n\u0010\nE\nh\u0010Z\n[a,b]d|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n\u0011p i\u00111/p\n≤\n\u0002\ninfθ∈[−c,c]d supx∈[a,b]d|N θ,l\nu,v (x) −E(x)|2\u0003\n+ 4(v −u) max{1, |a|, |b|}L(∥l∥∞+ 1)LcL max{1, p}\nK[L−1(∥l∥∞+1)−2]\n+ 18 max{1, (v −u)2}L(∥l∥∞+ 1)2 max{p, ln(3MB max{1, |a|, |b|})}\n√\nM\n(168)\n≤\n\u0002\ninfθ∈[−c,c]d supx∈[a,b]d|N θ,l\nu,v (x) −E(x)|2\u0003\n+ 4(v −u)L(∥l∥∞+ 1)LcL+1 max{1, p}\nK[L−1(∥l∥∞+1)−2]\n+ 18 max{1, (v −u)2}L(∥l∥∞+ 1)2 max{p, ln(3MBc)}\n√\nM\n.\nFurthermore, observe that Proposition 3.5 (with f ←E in the notation of Proposition 3.5)\nproves that there exists ϑ ∈Rd such that ∥ϑ∥∞≤max{1, L, |a|, |b|, 2[supx∈[a,b]d|E(x)|]}\nand\nsupx∈[a,b]d|N ϑ,l\nu,v (x) −E(x)| ≤3dL(b −a)\nA\n1/d\n.\n(169)\nThe fact that ∀x ∈[a, b]d : E(x) ∈[u, v] hence implies that\n∥ϑ∥∞≤max{1, L, |a|, |b|, 2|u|, 2|v|} ≤c.\n(170)\nThis and (169) demonstrate that\ninfθ∈[−c,c]d supx∈[a,b]d|N θ,l\nu,v (x) −E(x)|2\n≤supx∈[a,b]d|N ϑ,l\nu,v (x) −E(x)|2\n≤\n\u00143dL(b −a)\nA\n1/d\n\u00152\n= 9d2L2(b −a)2\nA\n2/d\n.\n(171)\nCombining this with (168) establishes for all p ∈(0, ∞) that\n\u0010\nE\nh\u0010Z\n[a,b]d|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n\u0011p i\u00111/p\n≤9d2L2(b −a)2\nA\n2/d\n+ 4(v −u)L(∥l∥∞+ 1)LcL+1 max{1, p}\nK[L−1(∥l∥∞+1)−2]\n+ 18 max{1, (v −u)2}L(∥l∥∞+ 1)2 max{p, ln(3MBc)}\n√\nM\n.\n(172)\nMoreover, note that the facts that max{1, L, |a|, |b|} ≤c and (b −a)2 ≤(|a| + |b|)2 ≤\n2(a2 + b2) yield that\n9L2(b −a)2 ≤18c2(a2 + b2) ≤18c2(c2 + c2) = 36c4.\n(173)\nIn addition, the fact that B ≥c ≥1, the fact that M ≥1, and (ii) in Lemma 6.4 ensure\nthat ln(3MBc) ≤\n23B\n18 ln(eM). This, (173), the fact that (v −u) ≤2 max{|u|, |v|} =\n38\nmax{2|u|, 2|v|} ≤c ≤B, and the fact that B ≥1 prove for all p ∈(0, ∞) that\n9d2L2(b −a)2\nA\n2/d\n+ 4(v −u)L(∥l∥∞+ 1)LcL+1 max{1, p}\nK[L−1(∥l∥∞+1)−2]\n+ 18 max{1, (v −u)2}L(∥l∥∞+ 1)2 max{p, ln(3MBc)}\n√\nM\n≤36d2c4\nA\n2/d\n+ 4L(∥l∥∞+ 1)LcL+2 max{1, p}\nK[L−1(∥l∥∞+1)−2]\n+ 23B3L(∥l∥∞+ 1)2 max{p, ln(eM)}\n√\nM\n.\n(174)\nCombining this with (172) shows (167). The proof of Theorem 6.5 is thus complete.\nCorollary 6.6. Let d, d, L, M, K, N ∈N, L, a, u ∈R, b ∈(a, ∞), v ∈(u, ∞), c ∈\n[max{1, L, |a|, |b|, 2|u|, 2|v|}, ∞), B ∈[c, ∞), l = (l0, l1, . . . , lL) ∈NL+1, N ⊆{0, 1, . . . ,\nN}, assume 0 ∈N, l0 = d, lL = 1, and d ≥PL\ni=1 li(li−1 + 1), let (Ω, F, P) be a\nprobability space, let Xj : Ω→[a, b]d, j ∈N, and Yj : Ω→[u, v], j ∈N, be functions,\nassume that (Xj, Yj), j ∈{1, 2, . . . , M}, are i.i.d. random variables, let E : [a, b]d →[u, v]\nsatisfy P-a.s. that E(X1) = E[Y1|X1], assume for all x, y ∈[a, b]d that |E(x) −E(y)| ≤\nL∥x −y∥1, let Θk,n : Ω→Rd, k, n ∈N0, and k: Ω→(N0)2 be random variables, assume\n\u0000S∞\nk=1 Θk,0(Ω)\n\u0001\n⊆[−B, B]d, assume that Θk,0, k ∈{1, 2, . . . , K}, are i.i.d., assume that\nΘ1,0 is continuous uniformly distributed on [−c, c]d, and let R: Rd × Ω→[0, ∞) satisfy\nfor all θ ∈Rd, ω ∈Ωthat\nR(θ, ω) = 1\nM\n\u0014 M\nP\nj=1\n|N θ,l\nu,v (Xj(ω)) −Yj(ω)|2\n\u0015\nand\n(175)\nk(ω) ∈arg min(k,n)∈{1,2,...,K}×N, ∥Θk,n(ω)∥∞≤B R(Θk,n(ω), ω)\n(176)\n(cf. Deﬁnitions 2.8 and 3.1). Then it holds for all p ∈(0, ∞) that\n\u0010\nE\nh\u0010Z\n[a,b]d|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n\u0011p/2 i\u00111/p\n≤\n3dL(b −a)\n[min({L} ∪{li : i ∈N ∩[0, L)})]\n1/d + 2[(v −u)L(∥l∥∞+ 1)LcL+1 max{1, p/2}]\n1/2\nK[(2L)−1(∥l∥∞+1)−2]\n+ 3 max{1, v −u}(∥l∥∞+ 1)[L max{p, 2 ln(3MBc)}]\n1/2\nM\n1/4\n(177)\n≤\n6dc2\n[min({L} ∪{li : i ∈N ∩[0, L)})]\n1/d + 2L(∥l∥∞+ 1)LcL+1 max{1, p}\nK[(2L)−1(∥l∥∞+1)−2]\n+ 5B2L(∥l∥∞+ 1) max{p, ln(eM)}\nM\n1/4\n(cf. (iii) in Lemma 6.2).\nProof of Corollary 6.6. Throughout this proof let A ∈(0, ∞) be given by\nA = min({L} ∪{li : i ∈N ∩[0, L)}).\n(178)\nNote that (178) ensures that\nL ≥A = A −1 + 1 ≥(A −1)1[2,∞)(A) + 1\n≥\n\u0000A −A\n2\n\u0001\n1[2,∞)(A) + 1 =\nA1[2,∞)(A)\n2\n+ 1 ≥A1(6d,∞)(A)\n2d\n+ 1.\n(179)\nMoreover, the assumption that lL = 1 and (178) imply that\nl1 = l11{1}(L) + l11[2,∞)(L) ≥1{1}(L) + A1[2,∞)(L) = A ≥A1(6d,∞)(A).\n(180)\n39\nMoreover, again (178) shows for all i ∈{2, 3, . . .} ∩[0, L) that\nli ≥A ≥A1[2,∞)(A) ≥1[2,∞)(A) max{A −1, 2} = 1[2,∞)(A) max{A −4 + 3, 2}\n≥1[2,∞)(A) max{A −2i + 3, 2} ≥1(6d,∞)(A) max{A/d −2i + 3, 2}.\n(181)\nCombining (179)–(181) and Theorem 6.5 (with p ←p/2 for p ∈(0, ∞) in the notation of\nTheorem 6.5) establishes for all p ∈(0, ∞) that\n\u0010\nE\nh\u0010Z\n[a,b]d|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n\u0011p/2 i\u00112/p\n≤9d2L2(b −a)2\nA\n2/d\n+ 4(v −u)L(∥l∥∞+ 1)LcL+1 max{1, p/2}\nK[L−1(∥l∥∞+1)−2]\n+ 18 max{1, (v −u)2}L(∥l∥∞+ 1)2 max{p/2, ln(3MBc)}\n√\nM\n(182)\n≤36d2c4\nA\n2/d\n+ 4L(∥l∥∞+ 1)LcL+2 max{1, p/2}\nK[L−1(∥l∥∞+1)−2]\n+ 23B3L(∥l∥∞+ 1)2 max{p/2, ln(eM)}\n√\nM\n.\nThis, (178), and the facts that L ≥1, c ≥1, B ≥1, and ln(eM) ≥1 demonstrate for all\np ∈(0, ∞) that\n\u0010\nE\nh\u0010Z\n[a,b]d|N Θk,l\nu,v\n(x) −E(x)|2 PX1(dx)\n\u0011p/2 i\u00111/p\n≤\n3dL(b −a)\n[min({L} ∪{li : i ∈N ∩[0, L)})]\n1/d + 2[(v −u)L(∥l∥∞+ 1)LcL+1 max{1, p/2}]\n1/2\nK[(2L)−1(∥l∥∞+1)−2]\n+ 3 max{1, v −u}(∥l∥∞+ 1)[L max{p, 2 ln(3MBc)}]\n1/2\nM\n1/4\n≤\n6dc2\n[min({L} ∪{li : i ∈N ∩[0, L)})]\n1/d + 2[L(∥l∥∞+ 1)LcL+2 max{1, p/2}]\n1/2\nK[(2L)−1(∥l∥∞+1)−2]\n(183)\n+ 5B3[L(∥l∥∞+ 1)2 max{p/2, ln(eM)}]\n1/2\nM\n1/4\n≤\n6dc2\n[min({L} ∪{li : i ∈N ∩[0, L)})]\n1/d + 2L(∥l∥∞+ 1)LcL+1 max{1, p}\nK[(2L)−1(∥l∥∞+1)−2]\n+ 5B2L(∥l∥∞+ 1) max{p, ln(eM)}\nM\n1/4\n.\nThe proof of Corollary 6.6 is thus complete.\n6.3\nFull strong error analysis for the training of DNNs with\noptimisation via stochastic gradient descent with random\ninitialisation\nCorollary 6.7. Let d, d, L, M, K, N ∈N, L, a, u ∈R, b ∈(a, ∞), v ∈(u, ∞), c ∈\n[max{1, L, |a|, |b|, 2|u|, 2|v|}, ∞), B ∈[c, ∞), l = (l0, l1, . . . , lL) ∈NL+1, N ⊆{0, 1, . . . ,\nN}, (Jn)n∈N ⊆N, (γn)n∈N ⊆R, assume 0 ∈N, l0 = d, lL = 1, and d ≥PL\ni=1 li(li−1 + 1),\nlet (Ω, F, P) be a probability space, let Xk,n\nj\n: Ω→[a, b]d, k, n, j ∈N0, and Y k,n\nj\n: Ω→\n[u, v], k, n, j ∈N0, be functions, assume that (X0,0\nj , Y 0,0\nj\n), j ∈{1, 2, . . . , M}, are i.i.d. ran-\ndom variables, let E : [a, b]d →[u, v] satisfy P-a.s. that E(X0,0\n1 ) = E[Y 0,0\n1\n|X0,0\n1 ], assume for\nall x, y ∈[a, b]d that |E(x)−E(y)| ≤L∥x−y∥1, let Θk,n : Ω→Rd, k, n ∈N0, and k: Ω→\n(N0)2 be random variables, assume\n\u0000S∞\nk=1 Θk,0(Ω)\n\u0001\n⊆[−B, B]d, assume that Θk,0, k ∈\n40\n{1, 2, . . . , K}, are i.i.d., assume that Θ1,0 is continuous uniformly distributed on [−c, c]d,\nlet Rk,n\nJ : Rd × Ω→[0, ∞), k, n, J ∈N0, and Gk,n : Rd × Ω→Rd, k, n ∈N, satisfy for\nall k, n ∈N, ω ∈Ω, θ ∈{ϑ ∈Rd : (Rk,n\nJn (·, ω): Rd →[0, ∞) is diﬀerentiable at ϑ)} that\nGk,n(θ, ω) = (∇θRk,n\nJn )(θ, ω), assume for all k, n ∈N that Θk,n = Θk,n−1 −γnGk,n(Θk,n−1),\nand assume for all k, n ∈N0, J ∈N, θ ∈Rd, ω ∈Ωthat\nRk,n\nJ (θ, ω) = 1\nJ\n\u0014 JP\nj=1\n|N θ,l\nu,v (Xk,n\nj\n(ω)) −Y k,n\nj\n(ω)|2\n\u0015\nand\n(184)\nk(ω) ∈arg min(l,m)∈{1,2,...,K}×N, ∥Θl,m(ω)∥∞≤B R0,0\nM (Θl,m(ω), ω)\n(185)\n(cf. Deﬁnitions 2.8 and 3.1). Then it holds for all p ∈(0, ∞) that\n\u0010\nE\nh\u0010Z\n[a,b]d|N Θk,l\nu,v\n(x) −E(x)|2 PX0,0\n1 (dx)\n\u0011p/2 i\u00111/p\n≤\n3dL(b −a)\n[min({L} ∪{li : i ∈N ∩[0, L)})]\n1/d + 2[(v −u)L(∥l∥∞+ 1)LcL+1 max{1, p/2}]\n1/2\nK[(2L)−1(∥l∥∞+1)−2]\n+ 3 max{1, v −u}(∥l∥∞+ 1)[L max{p, 2 ln(3MBc)}]\n1/2\nM\n1/4\n(186)\n≤\n6dc2\n[min({L} ∪{li : i ∈N ∩[0, L)})]\n1/d + 2L(∥l∥∞+ 1)LcL+1 max{1, p}\nK[(2L)−1(∥l∥∞+1)−2]\n+ 5B2L(∥l∥∞+ 1) max{p, ln(eM)}\nM\n1/4\n(cf. (iii) in Lemma 6.2).\nProof of Corollary 6.7. Observe that Corollary 6.6 (with (Xj)j∈N ←(X0,0\nj )j∈N, (Yj)j∈N ←\n(Y 0,0\nj\n)j∈N, R ←R0,0\nM in the notation of Corollary 6.6) shows (186). The proof of Corol-\nlary 6.7 is thus complete.\nCorollary 6.8. Let d, d, L, M, K, N ∈N, L, a, u ∈R, b ∈(a, ∞), v ∈(u, ∞), c ∈\n[max{1, L, |a|, |b|, 2|u|, 2|v|}, ∞), B ∈[c, ∞), l = (l0, l1, . . . , lL) ∈NL+1, N ⊆{0, 1, . . . ,\nN}, (Jn)n∈N ⊆N, (γn)n∈N ⊆R, assume 0 ∈N, l0 = d, lL = 1, and d ≥PL\ni=1 li(li−1 + 1),\nlet (Ω, F, P) be a probability space, let Xk,n\nj\n: Ω→[a, b]d, k, n, j ∈N0, and Y k,n\nj\n: Ω→\n[u, v], k, n, j ∈N0, be functions, assume that (X0,0\nj , Y 0,0\nj\n), j ∈{1, 2, . . . , M}, are i.i.d. ran-\ndom variables, let E : [a, b]d →[u, v] satisfy P-a.s. that E(X0,0\n1 ) = E[Y 0,0\n1\n|X0,0\n1 ], assume for\nall x, y ∈[a, b]d that |E(x)−E(y)| ≤L∥x−y∥1, let Θk,n : Ω→Rd, k, n ∈N0, and k: Ω→\n(N0)2 be random variables, assume\n\u0000S∞\nk=1 Θk,0(Ω)\n\u0001\n⊆[−B, B]d, assume that Θk,0, k ∈\n{1, 2, . . . , K}, are i.i.d., assume that Θ1,0 is continuous uniformly distributed on [−c, c]d,\nlet Rk,n\nJ : Rd × Ω→[0, ∞), k, n, J ∈N0, and Gk,n : Rd × Ω→Rd, k, n ∈N, satisfy for\nall k, n ∈N, ω ∈Ω, θ ∈{ϑ ∈Rd : (Rk,n\nJn (·, ω): Rd →[0, ∞) is diﬀerentiable at ϑ)} that\nGk,n(θ, ω) = (∇θRk,n\nJn )(θ, ω), assume for all k, n ∈N that Θk,n = Θk,n−1 −γnGk,n(Θk,n−1),\nand assume for all k, n ∈N0, J ∈N, θ ∈Rd, ω ∈Ωthat\nRk,n\nJ (θ, ω) = 1\nJ\n\u0014 JP\nj=1\n|N θ,l\nu,v (Xk,n\nj\n(ω)) −Y k,n\nj\n(ω)|2\n\u0015\nand\n(187)\nk(ω) ∈arg min(l,m)∈{1,2,...,K}×N, ∥Θl,m(ω)∥∞≤B R0,0\nM (Θl,m(ω), ω)\n(188)\n41\n(cf. Deﬁnitions 2.8 and 3.1). Then\nE\nhZ\n[a,b]d|N Θk,l\nu,v\n(x) −E(x)| PX0,0\n1 (dx)\ni\n≤2[(v −u)L(∥l∥∞+ 1)LcL+1]\n1/2\nK[(2L)−1(∥l∥∞+1)−2]\n+\n3dL(b −a)\n[min{L, l1, l2, . . . , lL−1}]\n1/d + 3 max{1, v −u}(∥l∥∞+ 1)[2L ln(3MBc)]\n1/2\nM\n1/4\n≤\n6dc2\n[min{L, l1, l2, . . . , lL−1}]\n1/d + 5B2L(∥l∥∞+ 1) ln(eM)\nM\n1/4\n+ 2L(∥l∥∞+ 1)LcL+1\nK[(2L)−1(∥l∥∞+1)−2]\n(189)\n(cf. (iii) in Lemma 6.2).\nProof of Corollary 6.8. Note that Jensen’s inequality implies that\nE\nhZ\n[a,b]d|N Θk,l\nu,v\n(x)−E(x)| PX0,0\n1 (dx)\ni\n≤E\nh\u0010Z\n[a,b]d|N Θk,l\nu,v\n(x)−E(x)|2 PX0,0\n1 (dx)\n\u00111/2 i\n. (190)\nThis and Corollary 6.7 (with p ←1 in the notation of Corollary 6.7) complete the proof\nof Corollary 6.8.\nCorollary 6.9. Let d, d, L, M, K, N ∈N, L ∈R, c ∈[max{2, L}, ∞), B ∈[c, ∞),\nl = (l0, l1, . . . , lL) ∈NL+1, N ⊆{0, 1, . . . , N}, (Jn)n∈N ⊆N, (γn)n∈N ⊆R, assume\n0 ∈N, l0 = d, lL = 1, and d ≥PL\ni=1 li(li−1 + 1), let (Ω, F, P) be a probability space, let\nXk,n\nj\n: Ω→[0, 1]d, k, n, j ∈N0, and Y k,n\nj\n: Ω→[0, 1], k, n, j ∈N0, be functions, assume\nthat (X0,0\nj , Y 0,0\nj\n), j ∈{1, 2, . . . , M}, are i.i.d. random variables, let E : [0, 1]d →[0, 1]\nsatisfy P-a.s. that E(X0,0\n1 ) = E[Y 0,0\n1\n|X0,0\n1 ], assume for all x, y ∈[0, 1]d that |E(x)−E(y)| ≤\nL∥x −y∥1, let Θk,n : Ω→Rd, k, n ∈N0, and k: Ω→(N0)2 be random variables, assume\n\u0000S∞\nk=1 Θk,0(Ω)\n\u0001\n⊆[−B, B]d, assume that Θk,0, k ∈{1, 2, . . . , K}, are i.i.d., assume that\nΘ1,0 is continuous uniformly distributed on [−c, c]d, let Rk,n\nJ : Rd × Ω→[0, ∞), k, n, J ∈\nN0, and Gk,n : Rd × Ω→Rd, k, n ∈N, satisfy for all k, n ∈N, ω ∈Ω, θ ∈{ϑ ∈\nRd : (Rk,n\nJn (·, ω): Rd →[0, ∞) is diﬀerentiable at ϑ)} that Gk,n(θ, ω) = (∇θRk,n\nJn )(θ, ω),\nassume for all k, n ∈N that Θk,n = Θk,n−1 −γnGk,n(Θk,n−1), and assume for all k, n ∈N0,\nJ ∈N, θ ∈Rd, ω ∈Ωthat\nRk,n\nJ (θ, ω) = 1\nJ\n\u0014 JP\nj=1\n|N θ,l\nu,v (Xk,n\nj\n(ω)) −Y k,n\nj\n(ω)|2\n\u0015\nand\n(191)\nk(ω) ∈arg min(l,m)∈{1,2,...,K}×N, ∥Θl,m(ω)∥∞≤B R0,0\nM (Θl,m(ω), ω)\n(192)\n(cf. Deﬁnitions 2.8 and 3.1). Then\nE\nhZ\n[0,1]d|N Θk,l\nu,v\n(x) −E(x)| PX0,0\n1 (dx)\ni\n≤\n3dL\n[min{L, l1, l2, . . . , lL−1}]\n1/d + 3(∥l∥∞+ 1)[2L ln(3MBc)]\n1/2\nM\n1/4\n+ 2[L(∥l∥∞+ 1)LcL+1]\n1/2\nK[(2L)−1(∥l∥∞+1)−2]\n≤\ndc3\n[min{L, l1, l2, . . . , lL−1}]\n1/d + B3L(∥l∥∞+ 1) ln(eM)\nM\n1/4\n+ L(∥l∥∞+ 1)LcL+1\nK[(2L)−1(∥l∥∞+1)−2]\n(193)\n(cf. (iii) in Lemma 6.2).\nProof of Corollary 6.9. Observe that Corollary 6.8 (with a ←0, u ←0, b ←1, v ←1 in\nthe notation of Corollary 6.8), the facts that B ≥c ≥max{2, L} and M ≥1, and (ii) in\n42\nLemma 6.4 show that\nE\nhZ\n[0,1]d|N Θk,l\nu,v\n(x) −E(x)| PX0,0\n1 (dx)\ni\n≤\n3dL\n[min{L, l1, l2, . . . , lL−1}]\n1/d + 3(∥l∥∞+ 1)[2L ln(3MBc)]\n1/2\nM\n1/4\n+ 2[L(∥l∥∞+ 1)LcL+1]\n1/2\nK[(2L)−1(∥l∥∞+1)−2]\n≤\ndc3\n[min{L, l1, l2, . . . , lL−1}]\n1/d + (∥l∥∞+ 1)[23BL ln(eM)]\n1/2\nM\n1/4\n+ [L(∥l∥∞+ 1)Lc2L+2]\n1/2\nK[(2L)−1(∥l∥∞+1)−2]\n≤\ndc3\n[min{L, l1, l2, . . . , lL−1}]\n1/d + B3L(∥l∥∞+ 1) ln(eM)\nM\n1/4\n+ L(∥l∥∞+ 1)LcL+1\nK[(2L)−1(∥l∥∞+1)−2] .\n(194)\nThe proof of Corollary 6.9 is thus complete.\nAcknowledgements\nThis work has been funded by the Deutsche Forschungsgemeinschaft (DFG, German Re-\nsearch Foundation) under Germany’s Excellence Strategy EXC 2044-390685587, Mathe-\nmatics M¨unster: Dynamics-Geometry-Structure, by the Swiss National Science Founda-\ntion (SNSF) under the project “Deep artiﬁcial neural network approximations for stochas-\ntic partial diﬀerential equations: Algorithms and convergence proofs” (project number\n184220), and through the ETH Research Grant ETH-47 15-2 “Mild stochastic calculus\nand numerical approximations for nonlinear stochastic evolution equations with L´evy\nnoise”.\nReferences\n[1]\nAliprantis, C. D., and Border, K. C. Inﬁnite dimensional analysis. Third. A\nhitchhiker’s guide. Springer, Berlin, 2006, xxii+703. isbn: 978-3-540-32696-0. url:\nhttps://doi.org/10.1007/3-540-29587-9.\n[2]\nArora, S., Du, S., Hu, W., Li, Z., and Wang, R.\nFine-Grained Analysis of\nOptimization and Generalization for Overparameterized Two-Layer Neural Net-\nworks. In: Proceedings of the 36th International Conference on Machine Learning.\nEd. by Chaudhuri, K., and Salakhutdinov, R. Vol. 97. Proceedings of Machine\nLearning Research. PMLR, Long Beach, California, USA, June 2019, 322–332. url:\nhttp://proceedings.mlr.press/v97/arora19a.html.\n[3]\nBach, F.\nBreaking the curse of dimensionality with convex neural networks. J.\nMach. Learn. Res. 18 (2017), 53 pages. issn: 1532-4435.\n[4]\nBach, F., and Moulines, E. Non-strongly-convex smooth stochastic approxima-\ntion with convergence rate O(1/n). In: Advances in Neural Information Processing\nSystems 26. Ed. by Burges, C. J. C., Bottou, L., Welling, M., Ghahramani,\nZ., and Weinberger, K. Q. Curran Associates, Inc., 2013, 773–781. url: http:\n//papers.nips.cc/paper/4900-non-strongly-convex-smooth-stochastic-\napproximation-with-convergence-rate-o1n.pdf.\n[5]\nBarron, A. R. Universal approximation bounds for superpositions of a sigmoidal\nfunction. IEEE Trans. Inform. Theory 39, 3 (1993), 930–945. issn: 0018-9448. url:\nhttps://doi.org/10.1109/18.256500.\n43\n[6]\nBarron, A. R.\nApproximation and estimation bounds for artiﬁcial neural net-\nworks. Machine Learning 14, 1 (1994), 115–133. url: https://doi.org/10.1007/\nBF00993164.\n[7]\nBartlett, P. L., Bousquet, O., and Mendelson, S. Local Rademacher com-\nplexities. Ann. Statist. 33, 4 (2005), 1497–1537. issn: 0090-5364. url: https://\ndoi.org/10.1214/009053605000000282.\n[8]\nBeck, C., Becker, S., Grohs, P., Jaafari, N., and Jentzen, A.\nSolving\nstochastic diﬀerential equations and Kolmogorov equations by means of deep learn-\ning. ArXiv e-prints (2018), 56 pages. arXiv: 1806.00421 [math.NA].\n[9]\nBeck, C., E, W., and Jentzen, A. Machine Learning Approximation Algorithms\nfor High-Dimensional Fully Nonlinear Partial Diﬀerential Equations and Second-\norder Backward Stochastic Diﬀerential Equations. J. Nonlinear Sci. 29, 4 (2019),\n1563–1619. issn: 0938-8974. url: https://doi.org/10.1007/s00332-018-9525-\n3.\n[10]\nBeck, C., Jentzen, A., and Kuckuck, B.\nFull error analysis for the train-\ning of deep neural networks. ArXiv e-prints (2019), 53 pages. arXiv: 1910.00121\n[math.NA].\n[11]\nBellman, R.\nDynamic programming. Princeton University Press, Princeton, N.\nJ., 1957, xxv+342.\n[12]\nBercu, B., and Fort, J.-C.\nGeneric Stochastic Gradient Methods. In: Wiley\nEncyclopedia of Operations Research and Management Science. American Cancer\nSociety, 2013, 1–8. isbn: 9780470400531. url: https://doi.org/10.1002/97804\n70400531.eorms1068.\n[13]\nBerner, J., Grohs, P., and Jentzen, A. Analysis of the generalization error:\nEmpirical risk minimization over deep artiﬁcial neural networks overcomes the curse\nof dimensionality in the numerical approximation of Black-Scholes partial diﬀeren-\ntial equations. ArXiv e-prints (2018), 30 pages. arXiv: 1809.03062 [cs.LG].\n[14]\nBlum, E. K., and Li, L. K.\nApproximation theory and feedforward networks.\nNeural Networks 4, 4 (1991), 511–515. issn: 0893-6080. url: https://doi.org/\n10.1016/0893-6080(91)90047-9.\n[15]\nB¨olcskei, H., Grohs, P., Kutyniok, G., and Petersen, P. Optimal approx-\nimation with sparsely connected deep neural networks. SIAM J. Math. Data Sci. 1,\n1 (2019), 8–45. issn: 2577-0187. url: https://doi.org/10.1137/18M118709X.\n[16]\nBurger, M., and Neubauer, A. Error bounds for approximation with neural\nnetworks. J. Approx. Theory 112, 2 (2001), 235–250. issn: 0021-9045. url: https:\n//doi.org/10.1006/jath.2001.3613.\n[17]\nCand`es, E. J.\nRidgelets: theory and applications. PhD thesis. Department of\nStatistics, Stanford University, 1998.\n[18]\nChau, N. H., Moulines, ´E., R´asonyi, M., Sabanis, S., and Zhang, Y. On\nstochastic gradient Langevin dynamics with dependent data streams: the fully non-\nconvex case. ArXiv e-prints (2019), 27 pages. arXiv: 1905.13142 [math.ST].\n[19]\nChen, T., and Chen, H. Approximation capability to functions of several vari-\nables, nonlinear functionals, and operators by radial basis function neural networks.\nIEEE Trans. Neural Netw. 6, 4 (1995), 904–910. issn: 1941-0093. url: https :\n//doi.org/10.1109/72.392252.\n44\n[20]\nCheridito, P., Jentzen, A., and Rossmannek, F. Eﬃcient approximation of\nhigh-dimensional functions with deep neural networks. ArXiv e-prints (2019), 19\npages. arXiv: 1912.04310 [math.NA].\n[21]\nChui, C. K., Li, X., and Mhaskar, H. N.\nNeural networks for localized ap-\nproximation. Math. Comp. 63, 208 (1994), 607–623. issn: 0025-5718. url: https:\n//doi.org/10.2307/2153285.\n[22]\nCox, S., Hutzenthaler, M., Jentzen, A., van Neerven, J., and Welti, T.\nConvergence in H¨older norms with applications to Monte Carlo methods in inﬁnite\ndimensions. ArXiv e-prints (2016), 50 pages. arXiv: 1605.00856 [math.NA]. To\nappear in IMA J. Num. Anal.\n[23]\nCucker, F., and Smale, S. On the mathematical foundations of learning. Bull.\nAmer. Math. Soc. (N.S.) 39, 1 (2002), 1–49. issn: 0273-0979. url: https://doi.\norg/10.1090/S0273-0979-01-00923-5.\n[24]\nCybenko, G.\nApproximation by superpositions of a sigmoidal function. Math.\nControl Signals Systems 2, 4 (1989), 303–314. issn: 0932-4194. url: https://doi.\norg/10.1007/BF02551274.\n[25]\nDereich, S., and Kassing, S.\nCentral limit theorems for stochastic gradient\ndescent with averaging for stable manifolds. ArXiv e-prints (2019), 42 pages. arXiv:\n1912.09187 [math.PR].\n[26]\nDereich, S., and M¨uller-Gronbach, T.\nGeneral multilevel adaptations for\nstochastic approximation algorithms of Robbins-Monro and Polyak-Ruppert type.\nNumer. Math. 142, 2 (2019), 279–328. issn: 0029-599X. url: https://doi.org/\n10.1007/s00211-019-01024-y.\n[27]\nDeVore, R. A., Oskolkov, K. I., and Petrushev, P. P. Approximation by\nfeed-forward neural networks. In: The heritage of P. L. Chebyshev: a Festschrift in\nhonor of the 70th birthday of T. J. Rivlin. Vol. 4. Baltzer Science Publishers BV,\nAmsterdam, 1997, 261–287.\n[28]\nDu, S. S., Zhai, X., Pocz´os, B., and Singh, A.\nGradient Descent Provably\nOptimizes Over-parameterized Neural Networks. ArXiv e-prints (2018), 19 pages.\narXiv: 1810.02054 [cs.LG].\n[29]\nDu, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient Descent Finds Global\nMinima of Deep Neural Networks. In: Proceedings of the 36th International Con-\nference on Machine Learning. Ed. by Chaudhuri, K., and Salakhutdinov, R.\nVol. 97. Proceedings of Machine Learning Research. PMLR, Long Beach, California,\nUSA, June 2019, 1675–1685. url: http://proceedings.mlr.press/v97/du19c.\nhtml.\n[30]\nE, W., Han, J., and Jentzen, A. Deep learning-based numerical methods for\nhigh-dimensional parabolic partial diﬀerential equations and backward stochastic\ndiﬀerential equations. Commun. Math. Stat. 5, 4 (2017), 349–380. issn: 2194-6701.\nurl: https://doi.org/10.1007/s40304-017-0117-6.\n[31]\nE, W., Ma, C., and Wang, Q.\nA Priori Estimates of the Population Risk for\nResidual Networks. ArXiv e-prints (2019), 19 pages. arXiv: 1903.02154 [cs.LG].\n[32]\nE, W., Ma, C., and Wu, L. A priori estimates of the population risk for two-\nlayer neural networks. Comm. Math. Sci. 17, 5 (2019), 1407–1425. url: https:\n//doi.org/10.4310/CMS.2019.v17.n5.a11.\n45\n[33]\nE, W., Ma, C., and Wu, L. A comparative analysis of optimization and gener-\nalization properties of two-layer neural network and random feature models under\ngradient descent dynamics. Sci. China Math. (2020). url: https://doi.org/10.\n1007/s11425-019-1628-5.\n[34]\nE, W., and Wang, Q. Exponential convergence of the deep neural network ap-\nproximation for analytic functions. Sci. China Math. 61 (2018), 1733–1740. url:\nhttps://doi.org/10.1007/s11425-018-9387-x.\n[35]\nElbr¨achter, D., Grohs, P., Jentzen, A., and Schwab, C. DNN Expression\nRate Analysis of High-dimensional PDEs: Application to Option Pricing. ArXiv\ne-prints (2018), 50 pages. arXiv: 1809.07669 [math.FA].\n[36]\nEldan, R., and Shamir, O. The Power of Depth for Feedforward Neural Net-\nworks. In: 29th Annual Conference on Learning Theory. Ed. by Feldman, V.,\nRakhlin, A., and Shamir, O. Vol. 49. Proceedings of Machine Learning Re-\nsearch. PMLR, Columbia University, New York, New York, USA, June 2016, 907–\n940. url: http://proceedings.mlr.press/v49/eldan16.pdf.\n[37]\nEllacott, S. W. Aspects of the numerical analysis of neural networks. In: Acta\nnumerica, 1994. Acta Numer. Cambridge University Press, Cambridge, 1994, 145–\n202. url: https://doi.org/10.1017/S0962492900002439.\n[38]\nFehrman, B., Gess, B., and Jentzen, A. Convergence rates for the stochastic\ngradient descent method for non-convex objective functions. ArXiv e-prints (2019),\n59 pages. arXiv: 1904.01517 [math.NA].\n[39]\nFunahashi, K.-I. On the approximate realization of continuous mappings by neu-\nral networks. Neural Networks 2, 3 (1989), 183–192. issn: 0893-6080. url: https:\n//doi.org/10.1016/0893-6080(89)90003-8.\n[40]\nGautschi, W. Some elementary inequalities relating to the gamma and incomplete\ngamma function. J. Math. and Phys. 38 (1959), 77–81. issn: 0097-1421. url: https:\n//doi.org/10.1002/sapm195938177.\n[41]\nGlorot, X., and Bengio, Y. Understanding the diﬃculty of training deep feed-\nforward neural networks. In: Proceedings of the Thirteenth International Conference\non Artiﬁcial Intelligence and Statistics. Ed. by Teh, Y. W., and Titterington,\nM. Vol. 9. Proceedings of Machine Learning Research. PMLR, Chia Laguna Resort,\nSardinia, Italy, May 2010, 249–256. url: http://proceedings.mlr.press/v9/\nglorot10a.html.\n[42]\nGonon, L., Grohs, P., Jentzen, A., Kofler, D., and ˇSiˇska, D. Uniform error\nestimates for artiﬁcial neural network approximations for heat equations. ArXiv e-\nprints (2019), 70 pages. arXiv: 1911.09647 [math.NA].\n[43]\nGoodfellow, I., Bengio, Y., and Courville, A.\nDeep learning. Adaptive\nComputation and Machine Learning. MIT Press, Cambridge, MA, 2016, xxii+775.\nisbn: 978-0-262-03561-3.\n[44]\nGribonval, R., Kutyniok, G., Nielsen, M., and Voigtlaender, F. Approx-\nimation spaces of deep neural networks. ArXiv e-prints (2019), 63 pages. arXiv:\n1905.01208 [math.FA].\n46\n[45]\nGrohs, P., Hornung, F., Jentzen, A., and von Wurstemberger, P.\nA\nproof that artiﬁcial neural networks overcome the curse of dimensionality in the\nnumerical approximation of Black-Scholes partial diﬀerential equations. ArXiv e-\nprints (2018), 124 pages. arXiv: 1809.02362 [math.NA]. To appear in Mem. Amer.\nMath. Soc.\n[46]\nGrohs, P., Hornung, F., Jentzen, A., and Zimmermann, P. Space-time error\nestimates for deep neural network approximations for diﬀerential equations. ArXiv\ne-prints (2019), 86 pages. arXiv: 1908.03833 [math.NA].\n[47]\nGrohs, P., Jentzen, A., and Salimova, D. Deep neural network approxima-\ntions for Monte Carlo algorithms. ArXiv e-prints (2019), 45 pages. arXiv: 1908.\n10828 [math.NA].\n[48]\nGrohs, P., Perekrestenko, D., Elbr¨achter, D., and B¨olcskei, H. Deep\nNeural Network Approximation Theory. ArXiv e-prints (2019), 60 pages. arXiv:\n1901.02220 [cs.LG].\n[49]\nG¨uhring, I., Kutyniok, G., and Petersen, P. Error bounds for approxima-\ntions with deep ReLU neural networks in W s,p norms. ArXiv e-prints (2019), 42\npages. arXiv: 1902.07896 [math.FA].\n[50]\nGuliyev, N. J., and Ismailov, V. E. Approximation capability of two hidden\nlayer feedforward neural networks with ﬁxed weights. Neurocomputing 316 (2018),\n262–269. issn: 0925-2312. url: https://doi.org/10.1016/j.neucom.2018.07.\n075.\n[51]\nGuliyev, N. J., and Ismailov, V. E. On the approximation by single hidden\nlayer feedforward neural networks with ﬁxed weights. Neural Networks 98 (2018),\n296–304. issn: 0893-6080. url: https://doi.org/10.1016/j.neunet.2017.12.\n007.\n[52]\nGy¨orfi, L., Kohler, M., Krzy˙zak, A., and Walk, H.\nA distribution-free\ntheory of nonparametric regression. Springer Series in Statistics. Springer-Verlag,\nNew York, 2002, xvi+647. isbn: 0-387-95441-4. url: https://doi.org/10.1007/\nb97848.\n[53]\nHan, J., and Long, J.\nConvergence of the Deep BSDE Method for Coupled\nFBSDEs. ArXiv e-prints (2018), 27 pages. arXiv: 1811.01165 [math.PR].\n[54]\nHartman, E. J., Keeler, J. D., and Kowalski, J. M. Layered Neural Networks\nwith Gaussian Hidden Units as Universal Approximations. Neural Comput. 2, 2\n(1990), 210–215. url: https://doi.org/10.1162/neco.1990.2.2.210.\n[55]\nHornik, K. Approximation capabilities of multilayer feedforward networks. Neural\nNetworks 4, 2 (1991), 251–257. issn: 0893-6080. url: https://doi.org/10.1016/\n0893-6080(91)90009-T.\n[56]\nHornik, K. Some new results on neural network approximation. Neural Networks\n6, 8 (1993), 1069–1072. issn: 0893-6080. url: https://doi.org/10.1016/S0893-\n6080(09)80018-X.\n[57]\nHornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks\nare universal approximators. Neural Networks 2, 5 (1989), 359–366. issn: 0893-6080.\nurl: https://doi.org/10.1016/0893-6080(89)90020-8.\n47\n[58]\nHornik, K., Stinchcombe, M., and White, H. Universal approximation of an\nunknown mapping and its derivatives using multilayer feedforward networks. Neural\nNetworks 3, 5 (1990), 551–560. issn: 0893-6080. url: https://doi.org/10.1016/\n0893-6080(90)90005-6.\n[59]\nHutzenthaler, M., Jentzen, A., Kruse, T., and Nguyen, T. A. A proof that\nrectiﬁed deep neural networks overcome the curse of dimensionality in the numer-\nical approximation of semilinear heat equations. ArXiv e-prints (2019), 29 pages.\narXiv: 1901.10854 [math.NA]. To appear in SN Partial Diﬀerential Equations and\nApplications.\n[60]\nJentzen, A., Kuckuck, B., Neufeld, A., and von Wurstemberger, P.\nStrong error analysis for stochastic gradient descent optimization algorithms. ArXiv\ne-prints (2018), 75 pages. arXiv: 1801.09324 [math.NA]. Revision requested from\nIMA J. Numer. Anal.\n[61]\nJentzen, A., Salimova, D., and Welti, T. A proof that deep artiﬁcial neural\nnetworks overcome the curse of dimensionality in the numerical approximation of\nKolmogorov partial diﬀerential equations with constant diﬀusion and nonlinear drift\ncoeﬃcients. ArXiv e-prints (2018), 48 pages. arXiv: 1809.07321 [math.NA].\n[62]\nJentzen, A., and von Wurstemberger, P. Lower error bounds for the stochas-\ntic gradient descent optimization algorithm: Sharp convergence rates for slowly and\nfast decaying learning rates. J. Complexity 57 (2020), 101438. issn: 0885-064X. url:\nhttps://doi.org/10.1016/j.jco.2019.101438.\n[63]\nKarimi, B., Miasojedow, B., Moulines, E., and Wai, H.-T. Non-asymptotic\nAnalysis of Biased Stochastic Approximation Scheme. ArXiv e-prints (2019), 32\npages. arXiv: 1902.00629 [stat.ML].\n[64]\nKutyniok, G., Petersen, P., Raslan, M., and Schneider, R. A Theoretical\nAnalysis of Deep Neural Networks and Parametric PDEs. ArXiv e-prints (2019), 43\npages. arXiv: 1904.00377 [math.NA].\n[65]\nLei, Y., Hu, T., Li, G., and Tang, K. Stochastic Gradient Descent for Nonconvex\nLearning without Bounded Gradient Assumptions. ArXiv e-prints (2019), 7 pages.\narXiv: 1902.00908 [cs.LG].\n[66]\nLeshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. Multilayer feedforward\nnetworks with a nonpolynomial activation function can approximate any function.\nNeural Networks 6, 6 (1993), 861–867. issn: 0893-6080. url: https://doi.org/\n10.1016/S0893-6080(05)80131-5.\n[67]\nMassart, P. Concentration inequalities and model selection. Vol. 1896. Lecture\nNotes in Mathematics. Lectures from the 33rd Summer School on Probability The-\nory held in Saint-Flour, July 6–23, 2003. Springer, Berlin, 2007, xiv+337. isbn:\n978-3-540-48497-4. url: https://doi.org/10.1007/978-3-540-48503-2.\n[68]\nMhaskar, H. N.\nNeural Networks for Optimal Approximation of Smooth and\nAnalytic Functions. Neural Comput. 8, 1 (1996), 164–177. url: https://doi.org/\n10.1162/neco.1996.8.1.164.\n[69]\nMhaskar, H. N., and Micchelli, C. A. Degree of approximation by neural and\ntranslation networks with a single hidden layer. Adv. in Appl. Math. 16, 2 (1995),\n151–183. issn: 0196-8858. url: https://doi.org/10.1006/aama.1995.1008.\n48\n[70]\nMhaskar, H. N., and Poggio, T. Deep vs. shallow networks: an approximation\ntheory perspective. Anal. Appl. (Singap.) 14, 6 (2016), 829–848. issn: 0219-5305.\nurl: https://doi.org/10.1142/S0219530516400042.\n[71]\nMontanelli, H., and Du, Q. New error bounds for deep ReLU networks using\nsparse grids. SIAM J. Math. Data Sci. 1, 1 (2019), 78–92. url: https://doi.org/\n10.1137/18M1189336.\n[72]\nNguyen-Thien, T., and Tran-Cong, T. Approximation of functions and their\nderivatives: A neural network implementation with applications. Appl. Math. Model.\n23, 9 (1999), 687–704. issn: 0307-904X. url: https://doi.org/10.1016/S0307-\n904X(99)00006-2.\n[73]\nNovak, E., and Wo´zniakowski, H. Tractability of multivariate problems. Vol.\n1: Linear information. Vol. 6. EMS Tracts in Mathematics. European Mathematical\nSociety (EMS), Z¨urich, 2008, xii+384. isbn: 978-3-03719-026-5. url: https://doi.\norg/10.4171/026.\n[74]\nNovak, E., and Wo´zniakowski, H. Tractability of multivariate problems. Vol-\nume II: Standard information for functionals. Vol. 12. EMS Tracts in Mathematics.\nEuropean Mathematical Society (EMS), Z¨urich, 2010, xviii+657. isbn: 978-3-03719-\n084-5. url: https://doi.org/10.4171/084.\n[75]\nPark, J., and Sandberg, I. W. Universal Approximation Using Radial-Basis-\nFunction Networks. Neural Comput. 3, 2 (1991), 246–257. url: https://doi.org/\n10.1162/neco.1991.3.2.246.\n[76]\nPerekrestenko, D., Grohs, P., Elbr¨achter, D., and B¨olcskei, H.\nThe\nuniversal approximation power of ﬁnite-width deep ReLU networks. ArXiv e-prints\n(2018), 16 pages. arXiv: 1806.01528 [cs.LG].\n[77]\nPetersen, P., Raslan, M., and Voigtlaender, F. Topological properties of\nthe set of functions generated by neural networks of ﬁxed size. ArXiv e-prints (2018),\n51 pages. arXiv: 1806.08459 [math.GN].\n[78]\nPetersen, P., and Voigtlaender, F. Equivalence of approximation by convo-\nlutional neural networks and fully-connected networks. ArXiv e-prints (2018), 10\npages. arXiv: 1809.00973 [math.FA].\n[79]\nPetersen, P., and Voigtlaender, F.\nOptimal approximation of piecewise\nsmooth functions using deep ReLU neural networks. Neural Networks 108 (2018),\n296–330. issn: 0893-6080. url: https://doi.org/10.1016/j.neunet.2018.08.\n019.\n[80]\nPinkus, A. Approximation theory of the MLP model in neural networks. In: Acta\nnumerica, 1999. Vol. 8. Acta Numer. Cambridge University Press, Cambridge, 1999,\n143–195. url: https://doi.org/10.1017/S0962492900002919.\n[81]\nQi, F. Bounds for the ratio of two gamma functions. J. Inequal. Appl. (2010), Art.\nID 493058, 84. issn: 1025-5834. url: https://doi.org/10.1155/2010/493058.\n[82]\nReisinger, C., and Zhang, Y. Rectiﬁed deep neural networks overcome the curse\nof dimensionality for nonsmooth value functions in zero-sum games of nonlinear stiﬀ\nsystems. ArXiv e-prints (2019), 34 pages. arXiv: 1903.06652 [math.NA].\n[83]\nRobbins, H., and Monro, S. A stochastic approximation method. Ann. Math.\nStatistics 22 (1951), 400–407. issn: 0003-4851. url: https://doi.org/10.1214/\naoms/1177729586.\n49\n[84]\nSchmitt, M.\nLower Bounds on the Complexity of Approximating Continuous\nFunctions by Sigmoidal Neural Networks. In: Advances in Neural Information Pro-\ncessing Systems 12. Ed. by Solla, S. A., Leen, T. K., and M¨uller, K. MIT\nPress, 2000, 328–334. url: http : / / papers . nips . cc / paper / 1692 - lower -\nbounds-on-the-complexity-of-approximating-continuous-functions-by-\nsigmoidal-neural-networks.pdf.\n[85]\nSchwab, C., and Zech, J.\nDeep learning in high dimension: neural network\nexpression rates for generalized polynomial chaos expansions in UQ. Anal. Appl.\n(Singap.) 17, 1 (2019), 19–55. issn: 0219-5305. url: https://doi.org/10.1142/\nS0219530518500203.\n[86]\nShaham, U., Cloninger, A., and Coifman, R. R.\nProvable approximation\nproperties for deep neural networks. Appl. Comput. Harmon. Anal. 44, 3 (2018),\n537–557. issn: 1063-5203. url: https://doi.org/10.1016/j.acha.2016.04.003.\n[87]\nShalev-Shwartz, S., and Ben-David, S.\nUnderstanding Machine Learning:\nFrom Theory to Algorithms. Cambridge University Press, Cambridge, 2014. url:\nhttps://doi.org/10.1017/CBO9781107298019.\n[88]\nShamir, O. Exponential Convergence Time of Gradient Descent for One-Dimen-\nsional Deep Linear Neural Networks. In: Proceedings of the Thirty-Second Confer-\nence on Learning Theory. Ed. by Beygelzimer, A., and Hsu, D. Vol. 99. Pro-\nceedings of Machine Learning Research. PMLR, Phoenix, USA, June 2019, 2691–\n2713. url: http://proceedings.mlr.press/v99/shamir19a.html.\n[89]\nShen, Z., Yang, H., and Zhang, S. Deep Network Approximation Character-\nized by Number of Neurons. ArXiv e-prints (2019), 36 pages. arXiv: 1906.05497\n[math.NA].\n[90]\nShen, Z., Yang, H., and Zhang, S. Nonlinear approximation via compositions.\nNeural Networks 119 (2019), 74–84. issn: 0893-6080. url: https://doi.org/10.\n1016/j.neunet.2019.07.011.\n[91]\nSirignano, J., and Spiliopoulos, K.\nDGM: A deep learning algorithm for\nsolving partial diﬀerential equations. J. Comput. Phys. 375 (2018), 1339–1364. issn:\n0021-9991. url: https://doi.org/10.1016/j.jcp.2018.08.029.\n[92]\nvan de Geer, S. A. Applications of empirical process theory. Vol. 6. Cambridge\nSeries in Statistical and Probabilistic Mathematics. Cambridge University Press,\nCambridge, 2000, xii+286. isbn: 0-521-65002-X.\n[93]\nVoigtlaender, F., and Petersen, P. Approximation in Lp(µ) with deep ReLU\nneural networks. ArXiv e-prints (2019), 4 pages. arXiv: 1904.04789 [math.FA].\n[94]\nWendel, J. G. Note on the gamma function. Amer. Math. Monthly 55 (1948),\n563–564. issn: 0002-9890. url: https://doi.org/10.2307/2304460.\n[95]\nYarotsky, D. Error bounds for approximations with deep ReLU networks. Neural\nNetworks 94 (2017), 103–114. issn: 0893-6080. url: https://doi.org/10.1016/\nj.neunet.2017.07.002.\n[96]\nYarotsky, D.\nUniversal approximations of invariant maps by neural networks.\nArXiv e-prints (2018), 64 pages. arXiv: 1804.10306 [cs.NE].\n50\n[97]\nZhang, G., Martens, J., and Grosse, R. B.\nFast Convergence of Natural\nGradient Descent for Over-Parameterized Neural Networks. In: Advances in Neu-\nral Information Processing Systems 32. Ed. by Wallach, H., Larochelle, H.,\nBeygelzimer, A., d’Alch´e-Buc, F., Fox, E., and Garnett, R. Curran As-\nsociates, Inc., 2019, 8082–8093. url: http://papers.nips.cc/paper/9020-\nfast-convergence-of-natural-gradient-descent-for-over-parameterized-\nneural-networks.pdf.\n[98]\nZou, D., Cao, Y., Zhou, D., and Gu, Q.\nGradient descent optimizes over-\nparameterized deep ReLU networks. Mach. Learn. (2019). url: https://doi.org/\n10.1007/s10994-019-05839-6.\n51\n",
  "categories": [
    "math.ST",
    "cs.LG",
    "cs.NA",
    "math.NA",
    "math.PR",
    "stat.ML",
    "stat.TH",
    "62M45, 68T05, 62L20, 60H30"
  ],
  "published": "2020-03-03",
  "updated": "2020-03-03"
}