{
  "id": "http://arxiv.org/abs/2003.01670v1",
  "title": "EXPLAIN-IT: Towards Explainable AI for Unsupervised Network Traffic Analysis",
  "authors": [
    "Andrea Morichetta",
    "Pedro Casas",
    "Marco Mellia"
  ],
  "abstract": "The application of unsupervised learning approaches, and in particular of\nclustering techniques, represents a powerful exploration means for the analysis\nof network measurements. Discovering underlying data characteristics, grouping\nsimilar measurements together, and identifying eventual patterns of interest\nare some of the applications which can be tackled through clustering. Being\nunsupervised, clustering does not always provide precise and clear insight into\nthe produced output, especially when the input data structure and distribution\nare complex and difficult to grasp. In this paper we introduce EXPLAIN-IT, a\nmethodology which deals with unlabeled data, creates meaningful clusters, and\nsuggests an explanation to the clustering results for the end-user. EXPLAIN-IT\nrelies on a novel explainable Artificial Intelligence (AI) approach, which\nallows to understand the reasons leading to a particular decision of a\nsupervised learning-based model, additionally extending its application to the\nunsupervised learning domain. We apply EXPLAIN-IT to the problem of YouTube\nvideo quality classification under encrypted traffic scenarios, showing\npromising results.",
  "text": "EXPLAIN-IT: Towards Explainable AI\nfor Unsupervised Network Traffic Analysis\nAndrea Morichetta\nPolitecnico di Torino\nAIT Austrian Institute of\nTechnology\nandrea.morichetta@polito.it\nPedro Casas\nAIT Austrian Institute of\nTechnology\npedro.casas@ait.ac.at\nMarco Mellia\nPolitecnico di Torino\nmarco.mellia@polito.it\nABSTRACT\nThe application of unsupervised learning approaches, and in\nparticular of clustering techniques, represents a powerful ex-\nploration means for the analysis of network measurements.\nDiscovering underlying data characteristics, grouping simi-\nlar measurements together, and identifying eventual patterns\nof interest are some of the applications which can be tack-\nled through clustering. Being unsupervised, clustering does\nnot always provide precise and clear insight into the pro-\nduced output, especially when the input data structure and\ndistribution are complex and difficult to grasp. In this pa-\nper we introduce EXPLAIN-IT, a methodology which deals\nwith unlabeled data, creates meaningful clusters, and sug-\ngests an explanation to the clustering results for the end-user.\nEXPLAIN-IT relies on a novel explainable Artificial Intelli-\ngence (AI) approach, which allows to understand the reasons\nleading to a particular decision of a supervised learning-\nbased model, additionally extending its application to the\nunsupervised learning domain. We apply EXPLAIN-IT to\nthe problem of YouTube video quality classification under\nencrypted traffic scenarios, showing promising results.\nKEYWORDS\nClustering; Explainable AI; Network Measurements; High-\nDimensional Data.\n1\nINTRODUCTION\nThe undeniable popularity of Artificial Intelligence, and in\nparticular of Machine Learning (ML) techniques, have also\nconcerned the network community in the last decade [2, 12].\nThe capability of addressing big data problems and automat-\ning processing measurements is appealing for multiple prob-\nlems, from network security to Quality of Experience (QoE)\nmonitoring and analysis [2].\nWhen it comes to ML techniques and methodologies, we\noften and more extensively refer to supervised approaches.\nSupervised learning builds a model starting from the data,\nrequiring these to be apriori categorized, i.e., labeled accord-\ning to the ground truth. Ground truth is generally missing\ndue to the structural complexity of the data, limits of hu-\nman knowledge, and significant volumes that complicate the\ncategorization process. This scenario is especially critical\nwhen it comes to network traffic, where researchers and\npractitioners have indeed to deal with small and outdated\ndatasets.\nUnsupervised techniques offer a solution to this lack of\nground-truth since their goal is to analyze the structural prop-\nerties of the data, based on some form of similarity among\ndata instances. Different approaches are possible, depending\non the overall goal (e.g., outlier detection, categorization, and\nothers), and the different levels of complexity of the analy-\nsis. In any case, there is no need for ground truth. However,\nanalyzing and interpreting the results obtained through clus-\ntering is a cumbersome and challenging task, often requiring\ntime and sophisticated, expert-based manual inspection. In\nmost cases, the complexity of the data - volume and dimen-\nsionality, and the size of the obtained results - number of\nclusters and outliers, is such that manual inspection analysis\nbecomes forbidding. Unsupervised quality metrics, such as\nthe Silhouette or Rank Index [8], provide only structural\ninsights on the obtained results, but they do not explain why\nthe clustering methodology grouped points in the same clus-\nter. Supervised quality metrics, such as cohesiveness and\npurity, require ground truth and, even when it is available,\nthis approach suffers if ground truth is partial, wrong, or\nbiased.\nWhen it comes to the interpretability of results, traditional\napproaches are based on white-box supervised techniques\nto explain the decisions provided by a particular ML model.\nWhite-box techniques use simple and easy-to-interpret mod-\nels such as linear discriminant functions or decision trees,\nwhich generally offers a straightforward interpretation of the\nmodels’ decisions. However, white box explainability limits\nthe set of applicable algorithms to those who are natively\ninterpretable, with the additional drawback of potentially\nlimiting performance.\nRecent work and efforts in the field of Explainable Arti-\nficial Intelligence (XAI) [5, 7, 10] provide both useful termi-\nnology and discussion on where and how such explainable\narXiv:2003.01670v1  [cs.AI]  3 Mar 2020\nUnsupervised XAI, Clustering, Network Measurements\nA. Morichetta, P. Casas, M. Mellia\napproaches can be useful. Current work has focused on black-\nbox explanation methods, which require no knowledge about\nthe model internals, and analyze it as a black-box through\ninput/response analysis. Methodologies such as LIME [13]\nprovide local interpretation methods to explain single model\ndecisions/predictions by linearizing a general model around\nthe specific inputs, identifying the most relevant input fea-\ntures for that prediction. This approach guarantees consid-\nerable flexibility in the model selection. Other current black-\nbox approaches include LEMNA [6], which extends LIME to\nimprove local linearization, and SHAP [9], which provides\nglobal interpretation methods based on aggregations. XAI is\nmainly linked to supervised learning scenarios and relies on\ndomain knowledge to interpret the proposed explanations.\nLooking into XAI applications for unsupervised learn-\ning, and in particular, for clustering methodologies, very\nfew solutions have been proposed. Authors of [1] propose\nthe application of interpretable algorithms as a guideline\nfor clustering execution. The authors propose a clustering\nmethodology based on decision tree principles to create a\nrule path. Corral et al. [3] develop a straightforward system\nto explain and describe the results of unsupervised learning\nfor network security, only looking at the attributes common\nto most of the points in a cluster.\nIn this paper, we focus precisely on the problem of XAI\nto interpret unsupervised learning results. We introduce\nEXPLAIN-IT, a generic framework for unsupervised and\nself explainable learning, based on clustering. EXPLAIN-IT\nis not only capable to extract cohesive clusters from the\ndata, but also to provide guidelines easing the interpretation\nof the results. In a nutshell, it uses clustering results as in-\nput to train a classification model, which is then explained\nthrough the application of black-box XAI approaches for\ninterpretation of results. To serve as an example, we apply\nEXPLAIN-IT to the unsupervised analysis of QoE in YouTube\nvideo streaming, analyzing YouTube encrypted network traf-\nfic to autonomously identify different and relevant video\nresolution groupings caused by poor network performance.\nWe note that EXPLAIN-IT is not a final system: this paper\nsets the initial steps into the overall ambitious goal so far\ndescribed.\nThe remainder of the paper is organized as follows: Section\n2 provides additional concepts on the overall XAI domain,\nfocusing on interpretability definitions and background. The\nEXPLAIN-IT framework is fully described in Section 3. Sec-\ntion 4 presents evaluation results on the capabilities of EXPLAIN-\nIT, using the YouTube video streaming QoE dataset described\nbefore. Finally, Section 5 provides concluding remarks, dis-\ncussing on EXPLAIN-IT limitations and outlying future work.\n2\nEXPLAINABLE AI BASICS\nXAI refers to methods and techniques in the application of\nAI/ML such that the results of the solution can be under-\nstood by human experts [5, 7, 10, 13]. Achieving this goal\nis possible by identifying the inputs of a model leading to\na particular output. XAI methods can be classified accord-\ning to various criteria [11], namely: (i) intrinsic or post hoc\nmethodologies; (ii) objective of the interpretation method -\n(a) feature summary statistics, (b) feature summary visualiza-\ntion, (c) model internals or (d) data points; (iii) model-specific\nor model-agnostic; (iv) local or global interpretation.\nIntrinsic interpretability means that the model is inter-\npretable at its basis - e.g., linear models. Post hoc refers to\nthe application of interpretation methods at testing time,\nafter building the learning model. Different approaches can\nbe used to interpret results, including: (ii.a) feature summary\nstatistics, e.g., feature importance, (ii.b) feature summary\nvisualization, e.g., partial dependence of features, (ii.c) model\ninternals, e.g., checking the structure of the decision tree,\n(ii.d) data points, e.g., looking for representative prototypes\nof classes. The interpretability method can be model-specific\nif it applies only to one particular model, or model-agnostic\n- usually, post hoc interpretability is model-agnostic. At last,\ninterpretability is defined global if it can explain the entire\nmodel behavior, or local if it explains an individual predic-\ntion.\nGlobal, holistic model interpretability is frequently very\nhard to achieve, often because the model is too tangled, and\nbecause the input features usually correlate on a global scale.\nThis global picture can be more easily obtained at a mod-\nular level, combining local interpretability approaches. By\nzooming on a single instance and perturbing it, it is pos-\nsible to obtain the local interpretability of the model. This\napproach allows the linearization of the problem and usu-\nally the neutralization of the dependencies between features,\noften allowing for more consistent results.\nEXPLAIN-IT relies on local, post hoc, model-agnostic in-\nterpretability [13], which provides high flexibility in terms of\ninterpretable models, the obtainable explanations, and their\nrepresentation. This overall flexibility allows a rapid switch\nbetween different problems, data types, and analysis models,\nmaking of EXPLAIN-IT a generic approach.\n3\nTHE EXPLAIN-IT FRAMEWORK\nThe goal of EXPLAIN-IT is to explain the outcome of un-\nsupervised algorithms. In general terms, it addresses the\nprocess of knowledge discovery in datasets [4], providing\na comprehensive tool tackling the variety of steps of this\nprocess. Fig. 1 depicts an overview of the system and its mul-\ntiple steps. EXPLAIN-IT consists of two consecutive analysis\nsteps, namely (i) reduction of the cardinality of the problem\nEXPLAIN-IT: XAI for Unsupervised Network Traffic Analysis\nUnsupervised XAI, Clustering, Network Measurements\nData\nExploration Space\nKnowledge\nStep 1\nReduction of Cardinality\nSummary Space\nStep 2\nAdvanced Knowledge Extraction.\nIdentify the most explainable dimensions.\nSplitting model\nIdentification of XAI \nfeatures\nKnowledge 2.0\nUnsupervised techniques\ne.g., Advanced Clustering\nFigure 1: The EXPLAIN-IT system. Data is firstly embedded into the exploration space, relying on expert knowl-\nedge when available. The summary space is the result obtained by clustering the exploration space. Next, we build\na supervised data splitting model out of the clustering results. Finally, we apply an XAI approach (LIME) to this\nsplitting model, interpreting the contents of the clusters by adding local interpretations.\nunder analysis (e.g., data summarization and compression),\nand (ii) knowledge extraction and building of explanations.\n3.1\nExploration and Summary Spaces\nIn the first step, EXPLAIN-IT relies (when available) on the\nknowledge of experts to extract the right features, struc-\nturing the data analysis and the way for interpreting the\nresults. This process may include feature selection and engi-\nneering methodologies. The definition/extraction of features\ncorresponds to embedding the data into an exploration space,\nwhich serves as a basis for the data exploration process. Nat-\nurally, when no expert domain knowledge is available, the\nembedding would take place in an entirely blind fashion,\nusing the features defined by the analyst. Once the data is\nembedded, EXPLAIN-IT uses unsupervised learning tech-\nniques to explore it, looking for relevant structures of interest.\nEXPLAIN-IT uses in particular clustering techniques, but\nany unsupervised methodology which creates a summary\nof the data can be applied. Clustering here plays the role\nof a meta-learning approach, which reduces the complexity\nof the analysis by aggregating similar instances, generating\nwhat we refer to as the summary space. Different cluster-\ning algorithms can be used, depending on the desired result,\namount of data, space dimensionality, and other constraints.\nFor example, in case the user wants to obtain a certain num-\nber of groupings or classes, solutions that require the number\nof clusters as input are preferred, e.g., K-Means. Otherwise,\nif the data structure itself should determine the exploration\nprocess, or if anomaly detection is relevant in the process,\nother techniques such as density-based ones (e.g., DBSCAN)\nare better suited.\n3.2\nModeling the Summary Space\nThe second step of EXPLAIN-IT consists of automatically\ncharacterizing the resulting clusters obtained in the sum-\nmary space. A common way to measure the effectiveness\nof clustering algorithms [8], it is to use intrinsic or derived\ncharacteristics, like complexity, steadiness, computational\ntime, as well as internal and external validity metrics, like the\nsilhouette and the rank indexes. Those measures are useful\nto evaluate the goodness of the algorithm, however, they\nprovide little insight into what clusters contain. The main\nidea is, therefore, to identify, for each of the obtained clusters,\nthe most relevant features explaining the assignment of each\ndata instance to it. While this could be in-principle done\nby per-feature analysis and using weighted distances and\nlinear discriminant functions, there is always a limitation of\nsuch an approach, based precisely on the considered notion\nof distance. Depending on which type of distance metric\none would use, the obtained explanations would be differ-\nent. For example, Euclidean distances would favor features\ndefining spherical-like rules explaining the results, whereas\nsparse spaces and heavy-tailed distributions would impact\nprobability-based or correlation-based distances. Also, global\nlinear discrimination would not perform accurately in most\npractical cases. Other empirical approaches, such as manual\ninspection, relevant features description, or extraction of the\nUnsupervised XAI, Clustering, Network Measurements\nA. Morichetta, P. Casas, M. Mellia\nmost representative data instances, suffer from scalability,\nand the limitations mentioned above.\nIn this absence of a general solution to this problem, and\ninspired by the notions behind white-box and black-box\nXAI, we rely on a purely data-driven approach and decide to\nmodel the results of the clustering step through a supervised\nlearning model, which we then explain through XAI. As we\nsaid before, to improve discrimination power, we do not rely\non natively interpretable models (e.g., decision trees or lin-\near discriminant functions), but we consider more powerful\ndata splitting models. Here, in particular, we take Support\nVector Machine (SVM) discriminant models [16], which are\nwe well-known for their performance to identify non-linear\nand complex boundaries among instances, relying on the\nuse of kernel functions and the so-called “kernel trick” to\nconstruct such boundaries.\n3.3\nXAI with LIME\nThe final step of the analysis consists of using black-box XAI\napproaches to finally interpret the structure of the summary\nspace by explaining the SVM-based modeled clusters. In\nparticular, we use LIME [13], a model-agnostic interpretation\napproach which relies on local model linearization to identify\nthe most relevant features leading to a particular decision, for\nan individual data instance. LIME relies on sampling for local\nmodel exploration and linearization. To explain the LIME\napproach, let us assume we have a complex model f (·), and a\nspecific instance x for which we want to explain the features\nleading to f (x). LIME constructs a natively interpretable\nmodel д(·), which is locally faithful to f (·) in the vicinity of\nx, the latter captured by certain similarity measure Dx(·). For\ndoing so, LIME randomly generates new instances z around\nx, which are then weighted by Dx(z) to define their local\nrelevance. Finally, д(·) is built based on inputs z and their\ncorresponding labels f (z). In particular, LIME uses linear\ndiscriminant functions to build д(·).\nNaturally, other XAI methodologies such as SHAP [9]\nare also very promising and could be part of EXPLAIN-IT.\nSHAP has solid mathematical foundations, but it has a much\nhigher complexity that makes it hard to use on exploratory\napproaches. However, it results in better explanations - or at\nleast approximations, making it more reliable in case of more\nstringent requirements. Nevertheless, LIME offers the best\ntrade-off between computational time and interpretability,\ntherefore its high popularity.\nBy combining the explanations provided by LIME for each\ndata instance belonging to each cluster, EXPLAIN-IT finally\nprovides guidelines easing the interpretation of the cluster-\ning results. The overall solution is very modular, allowing\ndifferent settings and flexibility in the various stages of the\nprocess.\nLD\nSD\nHD\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPDF\nCl. 0\nCl. 1\nCl. 2\nFigure 2: Distribution of real labels - LD, SD, and HD\nin the clusters generated by Agglomerative_Ward.\n4\nEXPLAIN-IT FOR YOUTUBE QOE\nTo serve as an application example of EXPLAIN-IT, we apply\nthe proposed system to the unsupervised analysis of QoE in\nYouTube video streaming, analyzing YouTube encrypted net-\nwork traffic to autonomously identify different and relevant\nvideo resolution groupings caused by poor network perfor-\nmance. The main challenge in the analysis of YouTube QoE\nfrom in-network measurements is that the wide adoption of\nend-to-end encryption through HTTPS blinds previous Deep\nPacket Inspection (DPI) based approaches, making of ML an\nappealing solution. We have extensively worked on this prob-\nlem [14, 15], but always considering a supervised learning\napproach. For this example, we would focus exclusively on\nthe prediction and analysis of the average video resolution\nor video quality (AVGQ) of YouTube video sessions as the\ntarget metric.\nNext, we describe the YouTube dataset analyzed in this\nAVGQ prediction task, and go across each of the steps fol-\nlowed by EXPLAIN-IT, including the clustering performance\nand its validation through standard metrics, the modeling of\nthe obtained clusters through SVM, and the final obtained\nexplanations through LIME.\n4.1\nYouTube QoE Dataset\nThe dataset we analyze consists of 10.654 YouTube video ses-\nsions, collected in [14, 15] from different sources, including\nsmartphones (HTML player and YouTube app) and desktop\n(HTML player) devices, and considering both TCP and QUIC\nprotocols. A set of m = 477 features, extracted from the\nencrypted stream of packets, describes each video session.\nThese include features at the full video session level (e.g., ses-\nsion downlink throughput) as well as at different time reso-\nlutions, with time slots aggregating packets in ∆t = [1, 5, 10]\nseconds. Features include not only traditional metrics such as\nmin/avg/max values, but also sampled values of the observed\nEXPLAIN-IT: XAI for Unsupervised Network Traffic Analysis\nUnsupervised XAI, Clustering, Network Measurements\nTable 1: Supervised and unsupervised quality metrics for clustering. The higher the values, the better the results.\nalgorithm\nadj_mutual_info\nadj_rand\ncompleteness\nfowlkes_mallows\nhomogeneity\nsilhouette\nv_measure\nAgglomerative_Ward\n0.131581\n0.103339\n0.217531\n0.532475\n0.131746\n0.379734\n0.164104\nAgglomerative_Single\n0.034626\n0.026033\n0.197189\n0.581536\n0.034815\n0.077997\n0.059182\nKMeans\n0.191741\n0.190060\n0.283335\n0.557967\n0.191894\n0.329705\n0.228817\nBirch\n0.132633\n0.109831\n0.224226\n0.535183\n0.132798\n0.325101\n0.166805\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nMean throughput downlink (ﬁrst 10 s) [Mb]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCDF\n(a) YouTube sessions in Cluster 0\n0.0\n0.5\n1.0\n1.5\n2.0\nMean throughput downlink (ﬁrst 10 s) [Mb]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCDF\n(b) YouTube sessions in Cluster 1\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nMean throughput downlink (ﬁrst 10 s) [Mb]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCDF\n(c) YouTube sessions in Cluster 2\nFigure 3: Average downlink throughput (ADT) distribution on the first 10s slot, per cluster. While there is strong\nvariance within each cluster, ADT distributions are ordered, with ADT(C1) < ADT(C0) < ADT(C2).\nempirical distributions, including percentiles as well as dis-\npersion metrics such as entropy h(·). Several QoE-related\nmetrics are directly measured at the player side and con-\nsidered as ground-truth metrics, including the number and\nduration of stalling events, number of quality switches, aver-\nage video bitrate, and AVGQ. See [14] for a full description\nof features and labels.\nUsing AVGQ as target metric, we build a discrete label\nto analyze the problem as a three classes classification task,\nincluding: Low Definition (LD) - AVGQ < 480, Standard\nDefinition (SD) - 480 ≤AVGQ ≤720, and High Definition\n(HD) - AVGQ ≥720. We use these labels as ground truth for\nvalidation purposes only, and not as part of the EXPLAIN-IT\nprocessing itself. We use the full set of 477 features as the\nembedding, exploration space.\n4.2\nClustering Approaches and Validation\nWe evaluate four different algorithms in the clustering step.\nOur goal is to find three clusters representing the apriori\nexpected AVGQ classes - LD, SD, and HD. Therefore, our se-\nlection of algorithms includes only those methodologies that\nhave the number of clusters as an input parameter. The se-\nlected algorithms include: Agglomerative_Ward - hierarchical\nclustering with Ward Links (Ward minimizes the variance of\nthe clusters during the merging phase); Agglomerative_Single\n- hierarchical clustering with Single Links (Single uses the\nminimum of the distances between all observations of the\ntwo sets); K-Means: best known clustering algorithm, with\nK=3; BIRCH - Balanced Iterative Reducing and Clustering\nusing Hierarchies [17]. We use the implementations of these\nalgorithms, as provided by Scikit-Learn.\nWe begin by evaluating the performance of these cluster-\ning algorithms using popular and well-accepted validation\nmetrics, namely adjusted mutual information, adjusted ran-\ndom score, completeness, Fowlkes-Mallows, homogeneity,\nsilhouette, and V-measure, see [8] for specific definitions on\nthese metrics. For all these metrics, the higher the value, the\nbetter the results. Tab. 1 depicts the obtained values for these\nmetrics, for the different clustering algorithms. K-Means and\nAgglomerative_Ward have, on average, the best results over\nthe considered metrics. Based on the results for the silhou-\nette score, we select Agglomerative_Ward as the clustering\nalgorithm for the next step. The silhouette score is a measure\nof how similar an instance is to the other cluster neighboring\ninstances as compared to instances in other clusters.\nWe are taking the results provided by Agglomerative_Ward,\nFig. 2 to show how the distribution of the clustered sessions\namong the three different AVGQ labels. Even if the split of\nthe video sessions is not pure, we can identify three different\ngroupings. Cluster C1 contains mostly LD and SD YouTube\nsessions; clusterC0 is more shifted towards SD and HD, while\nHD sessions predominantly characterize cluster C2. To fur-\nther verify these observations, we study the distribution of\nself-explainable, domain expert knowledge features across\nUnsupervised XAI, Clustering, Network Measurements\nA. Morichetta, P. Casas, M. Mellia\nf1-score\nprecision\nrecall\n0\n1\n2\nmacro avg\nmicro avg\nweighted avg\n0.91\n0.97\n0.87\n0.97\n0.96\n0.99\n0.89\n0.87\n0.94\n0.92\n0.93\n0.93\n0.96\n0.96\n0.96\n0.95\n0.96\n0.96\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 4: 10-fold cross SVM classification results, us-\ning Agglomerative_Ward clusters as labels.\nclusters. In particular, it is coherent to accept that higher\nresolution video sessions would correlate to sessions with\nhigher Average Downlink Throughput (ADT), and especially\nwhen considering the beginning of the video session. Fig. 3\ndepicts the distribution of ADT values over the first 10 sec-\nonds slot of the video sessions. Again, we can identify the\ntrend for the three classes: C1 shows the lowest values of\nADT, with more than 95% of the sessions having an ADT\n< 0.5 Mbps. For video sessions in C0, ADT is higher than 1\nMbps for about 60% of the sessions, whereas this amount\nincreases to about 80% of the sessions in C2.\nThis type of explanation is what one could expect by man-\nually inspecting each of the most relevant and discriminative\nfeatures. However, manually inspecting the full set of 477\nfeatures is not doable in practice, therefore the relevance of\nautomating this process through EXPLAIN-IT.\n4.3\nClusters’ Modeling with SVM\nWe want to use a model that interprets the clustering re-\nsults. We want to understand why the algorithm assigned\nan instance to a specific cluster and which are the main fea-\ntures that are distinctive for that assignment. As explained\nbefore, we do this by training an SVM model on the clus-\ntering results of Agglomerative_Ward, i.e., using the result-\ning clusters as labels for the classification algorithm. Fig. 4\ndepicts the performance achieved by SVM in this classifica-\ntion task in terms of precision, recall, and f-score - per class\nand micro/macro/weighted average, relying on 10-fold cross-\nvalidation. As observed, the SVM model is capable of almost\nentirely model the clusters obtained by Agglomerative_Ward,\nwith f-scores close or above 90% for all three clusters. The\nadvantage now is that we have a model which represents\nthe obtained cluster partitions as classification classes, and\ncan be automatically used to interpret the assignments of\ninstances to each of these clusters.\n4.4\nExplaining Results with LIME\nThe last step of EXPLAIN-IT is to use LIME to interpret the\ncluster assignment decisions as modeled by the SVM clas-\nsifier. We apply LIME with a neighborhood of 100 random\ninstances for each YouTube session to be explained. As out-\nput, LIME provides the probability of this session belonging\nto a specific class - in this case, modeling a cluster, and a\nlist of the most relevant features and associated values lead-\ning to the decision. We report an example in Tab. 2, which\nshows the top 10 features that characterize the prediction of\na YouTube session as belonging to cluster C2. As expected,\nand based on the results observed in Fig. 2 and Fig. 3, we see\nthat the explanations provided by LIME point to multiple\nfeatures, which should be higher than certain thresholds to\nbelong to C2. For example, packet length in the downlink\ndirection should be higher than 1,380 bytes, uplink accumu-\nlated number of bytes sent at the initial time slots of 1 second\nshould be higher than 10 to 20 KB, downlink accumulated\nnumber of bytes sent at time slots of 5 seconds should be\nhigher than 2.7 MB, and so on. These explanations show\nthat i) some features drive the decision better than others;\nii) these features relate to the quality of the connection and\ncontent of the video session; iii) top percentiles of the dis-\ntributions are very relevant for the assignments, suggesting\nthat peak values are more important than averages.\nBy aggregating the explanations provided for single in-\nstances, we can further gain insights about the generated\nclusters and their commonalities/differences. For example,\nTab. 3 shows the intersection among the 30 most popular\nfeatures deciding the assignment to clusters C1 and C2, con-\nsidering all the video sessions in each cluster. As we see,\nclusters C1 (mostly LD and SD, cf. Fig 2) and C2 (mostly SD\nand HD, cf. Fig 2) share six features which influence the\nassignment of video sessions to them, but with different\nrelevance, and also having different value intervals. The in-\ntersection comes mostly from the SD video sessions, spread\namong the three clusters.\n5\nCONCLUDING REMARKS\nEXPLAIN-IT is a novel approach to support unsupervised\ndata analysis with improved insights on the obtained results.\nIt may complement and, in some cases, even substitute tra-\nditional clustering evaluation methodologies. The overall\ntarget for EXPLAIN-IT is to implement a fully end-user-\ninteractive unsupervised analysis system, which the end\nuser could apply to refine the analysis and obtain better in-\nsights step by step. For example, the end-user could use the\nEXPLAIN-IT: XAI for Unsupervised Network Traffic Analysis\nUnsupervised XAI, Clustering, Network Measurements\nTable 2: EXPLAIN-IT reported features for an instance\nassigned toC2. The table reports the features and their\nimportance in the decision process.\nFeature Interval\nFeature Importance\nuplink_bytes_second_slot_1s > 10,470\n0.100\ndist_packet_length_downlink_p25 > 1,380\n0.090\ndist_slotted_uplink_bytes_p97_1s > 18,446\n0.083\nuplink_packets_first_slot_5s > 860\n0.070\n420,630 < dist_slotted_bytes_p97_1s <= 902,384\n0.067\ndist_slotted_downlink_bytes_p97_5s > 2,711,880\n0.059\ndist_slotted_downlink_bytes_h_1s > 0.66\n0.053\n335.405 < dist_slotted_uplink_packets_p99_1s <= 502\n0.044\ndist_slotted_uplink_bytes_p90_1s > 7,630\n0.044\ndist_slotted_bytes_mean_5s > 845,018\n0.037\nTable 3: Common “most popular” features explaining\nassignments to C1 and C2.\nRanking in C1\nFeature Name\nRanking in C2\n2\nuplink_throughput_second_slot_1s\n16\n4\nuplink_packets_second_slot_1s\n23\n5\nuplink_bytes_second_slot_1s\n10\n6\ndownlink_packets_second_slot_1s\n5\n7\ndownlink_throughput_second_slot_1s\n11\n9\ndownlink_bytes_second_slot_1s\n9\nresults of an EXPLAIN-IT round to identify and select the\nmost meaningful and explainable features to re-use in the\nnext step, refining the embedding on the exploration space.\nThe application of EXPLAIN-IT to the unsupervised anal-\nysis of YouTube QoE shows that it is possible to improve the\ninterpretation of clustering results by relying on XAI princi-\nples. However, being unsupervised by nature, EXPLAIN-IT\nhas still many open questions that we are trying to solve.\nFor example, the cluster modeling step through supervised\nlearning inherently introduces a bias due to the application\nof a specific model, in our case, SVM. In terms of XAI, LIME\nworks by generating random instances to extract the expla-\nnations. However, these random instances can turn out to be\nmeaningless. Especially if the data itself is complex, and with\nmany features, the explanations obtained by LIME in such a\nway can likely change. We are working on these and other\nopen issues to improve EXPLAIN-IT. For example, we are\ncurrently testing more robust XAI approaches such as SHAP\nand LEMNA to improve explainability properties, as well as\nworking on different unsupervised models - such as auto-\nencoders, as well as other supervised ones for clustering-\nresults-modeling - such as CNNs. We are also evaluating\nto use the clustering results themselves as the model to ex-\nplain, avoiding the bias introduced by the supervised-based\nmodeling step. As we said before, this has associated limita-\ntions and challenges we should tackle to provide a significant\nadvantage in terms of generalization of the framework.\nREFERENCES\n[1] J. Basak and R. Krishnapuram. 2005. Interpretable hierarchical clus-\ntering by constructing an unsupervised decision tree. IEEE Transac-\ntions on Knowledge and Data Engineering 17, 1 (Jan 2005), 121–132.\nhttps://doi.org/10.1109/TKDE.2005.11\n[2] Raouf Boutaba, Mohammad A. Salahuddin, Noura Limam, Sara Ay-\noubi, Nashid Shahriar, Felipe Estrada-Solano, and Oscar M. Caicedo.\n2018. A Comprehensive Survey on Machine Learning for Networking:\nEvolution, Applications and Research Opportunities. J. Internet Serv.\nAppl. 9, 16 (2018).\n[3] G. Corral, E. Armengol, A. Fornells, and E. Golobardes. 2009. Ex-\nplanations of unsupervised learning clustering applied to data secu-\nrity analysis. Neurocomputing 72, 13 (2009), 2754 – 2762.\nhttps:\n//doi.org/10.1016/j.neucom.2008.09.021 Hybrid Learning Machines\n(HAIS 2007) / Recent Developments in Natural Computation (ICNC\n2007).\n[4] Usama M. Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth.\n1996. From Data Mining to Knowledge Discovery in Databases. AI\nMagazine 17, 3 (1996), 37–54.\n[5] David Gunning. 2017. Explainable Artificial Intelligence (XAI). Defense\nAdvanced Research Projects Agency (DARPA) (2017).\nhttps://www.\ndarpa.mil/attachments/XAIProgramUpdate.pdf\n[6] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, and Xinyu\nXing. 2018. LEMNA: Explaining Deep Learning Based Security Appli-\ncations. In Proceedings of the 2018 ACM SIGSAC Conference on Computer\nand Communications Security (CCS ’18). ACM, New York, NY, USA,\n364–379. https://doi.org/10.1145/3243734.3243792\n[7] Christopher J. Hazard, Christopher Fusting, Michael Resnick, Michael\nAuerbach, Michael Meehan, and Valeri Korobov. 2019.\nNatively\nInterpretable Machine Learning and Artificial Intelligence: Prelim-\ninary Results and Future Directions. CoRR abs/1901.00246 (2019).\narXiv:1901.00246 http://arxiv.org/abs/1901.00246\n[8] F. Iglesias, T. Zseby, and A. Zimek. 2019. Absolute Cluster Validity.\nIEEE Transactions on Pattern Analysis and Machine Intelligence (2019),\n1–1. https://doi.org/10.1109/TPAMI.2019.2912970\n[9] Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpret-\ning Model Predictions. In Advances in Neural Information Processing\nSystems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer-\ngus, S. Vishwanathan, and R. Garnett (Eds.). Curran Associates, Inc.,\n4765–4774. http://papers.nips.cc/paper/7062-a-unified-approach-to-\ninterpreting-model-predictions.pdf\n[10] Tim Miller. 2019. Explanation in artificial intelligence: Insights from\nthe social sciences. Artificial Intelligence 267 (2019), 1 – 38.\nhttps:\n//doi.org/10.1016/j.artint.2018.07.007\n[11] Christoph Molnar. 2019. Interpretable Machine Learning.\nhttps://\nchristophm.github.io/interpretable-ml-book/.\n[12] T. T. T. Nguyen and G. Armitage. 2008. A survey of techniques for\ninternet traffic classification using machine learning.\nIEEE Com-\nmunications Surveys Tutorials 10, 4 (Fourth 2008), 56–76.\nhttps:\n//doi.org/10.1109/SURV.2008.080406\n[13] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why\nshould i trust you?: Explaining the predictions of any classifier. In\nProceedings of the 22nd ACM SIGKDD international conference on knowl-\nedge discovery and data mining. ACM, 1135–1144.\n[14] Michael Seufert, Pedro Casas, Nikolas Wehner, Li Gang, and Kuang\nLi. 2019. Features that Matter: Feature Selection for On-line Stalling\nPrediction in Encrypted Video Streaming. In International Conference\non Computer Communications (INFOCOM), 2nd International Workshop\non Network Intelligence. IEEE.\n[15] Michael Seufert, Pedro Casas, Nikolas Wehner, Li Gang, and Kuang Li.\n2019. Stream-based Machine Learning for Real-time QoE Analysis of\nUnsupervised XAI, Clustering, Network Measurements\nA. Morichetta, P. Casas, M. Mellia\nEncrypted Video Streaming Traffic. In 3rd International Workshop on\nQuality of Experience Management (QoE Management 2019). IEEE.\n[16] Vladimir N Vapnik. 1999. An overview of statistical learning theory.\nIEEE transactions on neural networks 10, 5 (1999), 988–999.\n[17] Tian Zhang, Raghu Ramakrishnan, and Miron Livny. 1996. BIRCH:\nan efficient data clustering method for very large databases. In ACM\nSigmod Record, Vol. 25. ACM, 103–114.\n",
  "categories": [
    "cs.AI",
    "cs.NI"
  ],
  "published": "2020-03-03",
  "updated": "2020-03-03"
}