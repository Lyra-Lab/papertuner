{
  "id": "http://arxiv.org/abs/2003.02218v1",
  "title": "The large learning rate phase of deep learning: the catapult mechanism",
  "authors": [
    "Aitor Lewkowycz",
    "Yasaman Bahri",
    "Ethan Dyer",
    "Jascha Sohl-Dickstein",
    "Guy Gur-Ari"
  ],
  "abstract": "The choice of initial learning rate can have a profound effect on the\nperformance of deep networks. We present a class of neural networks with\nsolvable training dynamics, and confirm their predictions empirically in\npractical deep learning settings. The networks exhibit sharply distinct\nbehaviors at small and large learning rates. The two regimes are separated by a\nphase transition. In the small learning rate phase, training can be understood\nusing the existing theory of infinitely wide neural networks. At large learning\nrates the model captures qualitatively distinct phenomena, including the\nconvergence of gradient descent dynamics to flatter minima. One key prediction\nof our model is a narrow range of large, stable learning rates. We find good\nagreement between our model's predictions and training dynamics in realistic\ndeep learning settings. Furthermore, we find that the optimal performance in\nsuch settings is often found in the large learning rate phase. We believe our\nresults shed light on characteristics of models trained at different learning\nrates. In particular, they fill a gap between existing wide neural network\ntheory, and the nonlinear, large learning rate, training dynamics relevant to\npractice.",
  "text": "The large learning rate phase of deep learning:\nthe catapult mechanism\nAitor Lewkowycz 1 Yasaman Bahri 2 Ethan Dyer 1 Jascha Sohl-Dickstein 2 Guy Gur-Ari 1\nAbstract\nThe choice of initial learning rate can have a profound effect on the performance of deep networks. We present a\nclass of neural networks with solvable training dynamics, and conﬁrm their predictions empirically in practical\ndeep learning settings. The networks exhibit sharply distinct behaviors at small and large learning rates. The two\nregimes are separated by a phase transition. In the small learning rate phase, training can be understood using the\nexisting theory of inﬁnitely wide neural networks. At large learning rates the model captures qualitatively distinct\nphenomena, including the convergence of gradient descent dynamics to ﬂatter minima. One key prediction of our\nmodel is a narrow range of large, stable learning rates. We ﬁnd good agreement between our model’s predictions\nand training dynamics in realistic deep learning settings. Furthermore, we ﬁnd that the optimal performance in\nsuch settings is often found in the large learning rate phase. We believe our results shed light on characteristics\nof models trained at different learning rates. In particular, they ﬁll a gap between existing wide neural network\ntheory, and the nonlinear, large learning rate, training dynamics relevant to practice.\n1. Introduction\nDeep learning has shown remarkable success across a variety of machine learning tasks. At the same time, our theoretical\nunderstanding of deep learning methods remains limited. In particular, the interplay between training dynamics, properties\nof the learned network, and generalization remains a largely open problem.\nIn this work we take a step toward addressing these questions. We present a dynamical mechanism that allows deep networks\ntrained using SGD to ﬁnd ﬂat minima and achieve superior performance. Our theoretical predictions agree well with\nempirical results in a variety of deep learning settings. In many cases we are able to predict the regime of learning rates\nwhere optimal performance is achieved. Figure 1 summarizes our main results. This work builds on several existing results,\nwhich we now review.\n1.1. Large learning rate SGD improves generalization\nSGD training with large initial learning rates often leads to improved performance over training with small initial learning\nrates (see Li et al. (2019); Leclerc & Madry (2020); Xie et al. (2020); Frankle et al. (2020); Jastrzebski et al. (2020) for\nrecent discussions). It has been suggested that one of the mechanisms underlying the beneﬁt of large learning rates is\nthat noise from stochastic gradient descent leads to ﬂat minima, and that ﬂat minima generalize better than sharp minima\n(Hochreiter & Schmidhuber, 1997; Keskar et al., 2016; Smith & Le, 2018; Jiang et al., 2020; Park et al., 2019) (though\nsee Dinh et al. (2017) for discussion of some caveats). According to this suggestion, training with a large learning rate (or\nwith a small batch size) can improve performance because it leads to more stochasticity during training (Mandt et al., 2017;\nSmith et al., 2017; Smith & Le, 2018; Smith et al., 2018).\nWe will develop a connection between large learning rate and ﬂatness of minima in models trained via SGD. Unlike the\nrelationship explored in most previous work though, this connection is not driven by SGD noise, but arises solely as a result\nof training with a large initial learning rate, and holds even for full batch gradient descent.\n1Google 2Google Brain. Correspondence to: Guy Gur-Ari <guyga@google.com>.\narXiv:2003.02218v1  [stat.ML]  4 Mar 2020\nThe large learning rate phase of deep learning\n-1\n0\n1\nWeight correlation\n0\n1\n2\nCurvature\ninitial\nfinal\n(a)\n10\n1\n100\nLearning rate\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nTest accuracy\nlazy / catapult boundary\ncatapult / divergent boundary\nWide Resnet 28-10\n(b)\nFigure 1. A summary of our main results. (a) A visualization of gradient descent dynamics derived in our theoretical setup. A 2D slice of\nparameter space is shown, where lighter color indicates higher loss and dots represents points visited during optimization. Initially, the\nloss grows rapidly while local curvature decreases. Once curvature is sufﬁciently low, gradient descent converges to a ﬂat minimum. We\ncall this the catapult effect. See Figures 2 and S1 for more details. (b) Conﬁrmation of our theoretical predictions in a practical deep\nlearning setting. Line shows the test accuracy of a Wide ResNet trained on CIFAR-10 as a function of learning rate, each trained for a\nﬁxed number of steps. Dashed lines show our predictions for the boundaries of the large learning rate regime (the catapult phase), where\nwe expect optimal performance to occur. Maximal performance is achieved between the dashed lines, conﬁrming our predictions. See\nSection 3 for details.\n1.2. The existing theory of inﬁnite width networks is insufﬁcient to describe large learning rates\nA recent body of work has investigated the gradient descent dynamics of deep networks in the limit of inﬁnite width (Daniely,\n2017; Jacot et al., 2018; Lee et al., 2019; Du et al., 2019; Zou et al., 2018; Allen-Zhu et al., 2019; Li & Liang, 2018; Chizat\net al., 2019; Mei et al., 2018; Rotskoff & Vanden-Eijnden, 2018; Sirignano & Spiliopoulos, 2018; Woodworth et al., 2019;\nNaveh et al.). Of particular relevance is the work by Jacot et al. (2018) showing that gradient ﬂow in the space of functions\nis governed by a dynamical quantity called the Neural Tangent Kernel (NTK) which is ﬁxed at its initial value in this limit.\nLee et al. (2019) showed this result is equivalent to training the linearization of a model around its initialization in parameter\nspace. Finally, moving away from the strict limit of inﬁnite width by working perturbatively, Dyer & Gur-Ari (2020); Huang\n& Yau (2019) introduced an approach to computing the ﬁnite-width corrections to network evolution.\nDespite this progress, it seems these results are insufﬁcient to capture the full dynamics of deep networks, as well as their\nsuperior performance, in regimes applicable to practice. Prior work has focused on comparisons between various inﬁnite-\nwidth kernels associated with deep networks and their ﬁnite-width, SGD-trained counterparts (Lee et al., 2018; Novak\net al., 2019; Arora et al., 2019). Speciﬁc ﬁndings vary depending on precise choices for architecture and hyperparameters.\nHowever, dramatic performance gaps are consistently observed between non-linear CNNs and their limiting kernels,\nimplying that the theory is not sufﬁcient to explain the performance of deep networks in this realistic setup. Furthermore,\nsome hyperparameter settings in ﬁnite-width models have no known analogue in the inﬁnite width limit, and it is these\nsettings that often lead to optimal performance.\nIn particular, ﬁnite width networks are often trained with large learning rates that would cause divergence for inﬁnite width\nlinearized models. Further, these large learning rates cause ﬁnite width networks to converge to ﬂat minima. For inﬁnite\nwidth linearized models, trained with MSE loss, all minima have the same curvature, and the notion of ﬂat minima does\nnot apply. We argue that the reduction in curvature during optimization, and support for learning rates that are infeasible\nfor inﬁnite width linearized models, may thus partially explain performance gaps observed between linear and non-linear\nmodels.\n1.3. Our contribution: three learning rate regimes\nIn this work, we identify a dynamical mechanism which enables ﬁnite-width networks to stably access large learning rates.\nWe show that this mechanism causes training to converge to ﬂatter minima and is associated with improved generalization.\nWe further show that this same mechanism can describe the behavior of inﬁnite width networks, if training time is increased\nThe large learning rate phase of deep learning\nalong with network width.\nThis new mechanism enables a characterization of gradient descent training in terms of three learning rate regimes, or\nphases: the lazy phase, the catapult phase, and the divergent phase. In Section 2 we analytically derive the behavior in\nthese three learning rate regimes for one hidden layer linear networks with large but ﬁnite width, trained with MSE loss. We\nconﬁrm experimentally in Section 3 that these phases also apply to deep nonlinear fully- connected, convolutional, and\nresidual architectures. In Section 4 we study additional predictions of the analytic solution.\nWe now summarize all three phases, using η to indicate the learning rate, and λ0 to indicate the initial curvature (deﬁned\nprecisely in Section 2.1). The phase is determined by the curvature at initialization and by the learning rate, despite the fact\nthat the curvature may change signiﬁcantly during training. Based on the experimental evidence we expect the behavior\ndescribed below to apply in typical deep learning settings, when training sufﬁciently wide networks using SGD.\nLazy phase: η < 2/λ0 .\nFor sufﬁciently small learning rate, the curvature λt at training step t remains constant during\nthe initial part of training. The model behaves (loosely) as a model linearized about its initial parameters (Lee et al., 2019);\nthis becomes exact in the inﬁnite width limit, where these dynamics are sometimes called lazy training (Jacot et al., 2018;\nLee et al., 2019; Du et al., 2019; Li & Liang, 2018; Zou et al., 2018; Allen-Zhu et al., 2019; Chizat et al., 2019; Dyer &\nGur-Ari, 2020). For a discussion of trainability and the connection to the NTK in the lazy phase see Xiao et al. (2019).\nCatapult phase: 2/λ0 < η < ηmax .\nIn this phase, the curvature at initialization is too high for training to converge to a\nnearby point, and the linear approximation quickly breaks down. Optimization begins with a period of exponential growth\nin the loss, coupled with a rapid decrease in curvature, until curvature stabilizes at a value λﬁnal < 2/η. Once the curvature\ndrops below 2/η, training converges, ultimately reaching a minimum that is ﬂatter than those found in the lazy phase. This\ninitial period lasts for a number of training steps that is of order log(n), where n is the network width, and is therefore quite\nshort for realistic networks (often lasting less than a single epoch). Optimal performance is often achieved when the initial\nlearning rate is in this range. The gradient descent dynamics in this phase are visualized in SM Figure S1 and in Figure 1.\nThe maximum learning rate is approximately given by ηmax = cact./λ0, where cact. is an architecture-dependent constant.\nEmpirically, we ﬁnd that this constant depends strongly on the non-linearity but only weakly on other aspects of the\narchitecture. For networks with ReLU non-linearity we ﬁnd empirically that cact. ≈12. For the theoretical model, we show\nthat cact. = 4.\nDivergent phase: η > ηmax .\nWhen the learning rate is above the maximum learning rate of the model, the loss diverges\nand the model does not train.\n2. Theoretical results\nWe now present our main theoretical result, an analysis of gradient descent dynamics for a neural network with large but\nﬁnite width.\nGiven a network function f : Rd →R with model parameters θ ∈Rp, and a training set {(xα, yα)}m\nα=1, the MSE loss is\nL =\n1\n2m\nm\nX\nα=1\n(f(xα) −yα)2 .\n(1)\nThe NTK Θ : Rd × Rd →R is deﬁned by\nΘ(x, x′) := 1\nm\np\nX\nµ=1\n∂f(x)\n∂θµ\n∂f(x′)\n∂θµ\n.\n(2)\nWe denote by λ the maximum eigenvalue of the kernel. In large width models, λ provides a local measure of the loss\nlandscape curvature that is similar to the top eigenvalue of the Hessian (Dyer & Gur-Ari, 2020).\nIn this section, we will consider a network with one hidden layer and linear activations, where the network function f is\ngiven by\nf(x) = n−1/2vT ux .\n(3)\nThe large learning rate phase of deep learning\nHere n is the width (number of neurons in the hidden layer), v ∈Rn and u ∈Rn×d are the model parameters (collectively\ndenoted θ), and x ∈Rd is the training input. At initialization, the weights are drawn from N(0, 1).\n2.1. Warmup: a simpliﬁed model\nBefore analyzing the dynamics of the model, we analyze a simpler setting which captures the most important aspects of the\nfull solution. Consider a dataset with 1D inputs, and with a single training sample x = 1 with label y = 0. The network\nfunction evaluated on this input is then f = n−1/2vT u, with u, v ∈Rn, and the loss is L = f 2/2. The gradient descent\nequations at training step t are\nut+1 = ut −ηn−1/2ftvt , vt+1 = vt −ηn−1/2ftut .\n(4)\nNext, consider the update equations in function space. These can be written in terms of the Neural Tangent Kernel. For this\nmodel, the kernel evaluated on the training set is a scalar which is equal to λ, its top eigenvalue, and is given by\nΘ(1, 1) = λ = n−1 \u0000∥v∥2\n2 + ∥u∥2\n2\n\u0001\n.\n(5)\nAt initialization, both f 2 and λ scale as n0 = 1 with width. The following update equations for f and λ at step t can be\nderived from (4).\nft+1 =\n\u0012\n1 −ηλt + η2f 2\nt\nn\n\u0013\nft ,\n(6)\nλt+1 = λt + ηf 2\nt\nn (ηλt −4) .\n(7)\nIt is important to note that these are the exact update equations for this model, and that no higher-order terms were neglected.\nWe now analyze these dynamical equations assuming the width n is large. Two learning rates that will be important in the\nanalysis are ηcrit = 2/λ0 and ηmax = 4/λ0. In terms of the notation introduced above, the architecture-dependent constant\nthat determines that maximum learning rate in this model is cact. = 4.\n2.1.1. LAZY PHASE\nTaking the strict inﬁnite width limit, equations (6) and (7) become\nft+1 = (1 −ηλt) ft ,\nλt+1 = λt .\n(8)\nWhen η < ηcrit, λ remains constant throughout training. This is a special case of NTK dynamics, where the kernel is\nconstant and the network evolves as a linear model (Lee et al., 2019). The function and the loss both shrink to zero because\nthe multiplicative factor obeys |1 −ηλt| < 1. This convergence happens in O(n0) = O(1) steps.\n2.1.2. CATAPULT PHASE\nWhen ηcrit < η < ηmax, the loss diverges in the inﬁnite width limit. Indeed, from (8) we see that the kernel is constant in\nthe limit, while f receives multiplicative updates where |1 −ηλt| > 1. This is the well known instability of gradient descent\ndynamics for linear models with MSE loss. However, the underlying model is not linear in its parameters, and ﬁnite width\ncontributions turn out to be important. We therefore relax the inﬁnite width limit and analyze equations (6,7) for large but\nﬁnite width, n ≫1.\nFirst, note that ηλ0 −4 < 0 by assumption, and therefore the (additive) kernel updates are negative for all t. During early\ntraining, |ft| grows (as in the inﬁnite width limit) while λt remains constant up to small O(n−1) updates. After t ∼log(n)\nsteps, |ft| grows to order n1/2. At this point, the kernel updates are no longer negligible because f 2\nt /n is of order n0. The\nkernel λt receives negative, non-negligible updates while both ft and the loss continue to grow (for now, we ignore the term\nin (6) with an explicit 1/n dependence). This continues until the kernel is sufﬁciently small that the condition ηλt ≲2 is\nmet.1 We call this curvature-reduction effect the catapult effect. Beyond this point, |1 −ηλt| < 1 holds, |ft| shrinks, and the\nloss converges to a global minimum. The n dependence of the steps until optimization converges is log (n).\nIt remains to show that the term in (6) with an explicit n−1 dependence does not affect these conclusions. Once |ft| grows\nto order n1/2, this term is no longer negligible and can cause the multiplicative factor in front of ft to become smaller than 1\n1The bound is not exact because of the term we neglected.\nThe large learning rate phase of deep learning\nin absolute value, causing |ft| to start shrinking. However, once |ft| shrinks sufﬁciently this term again becomes negligible.\nTherefore, the loss will not converge to zero unless the curvature eventually drops below 2/η. Conversely, notice that this\nterm cannot cause |ft| to diverge for learning rates below ηmax. Indeed, if this were to happen then equation (7) would drive\nλt to negative values, leading to a contradiction. This completes the analysis in this phase.\nLet us make a few comments about the catapult phase.\nIt is important for the analysis that we take a modiﬁed large width limit, in which the number of training steps grows like\nlog(n) as n becomes large. This is different than the large width limit commonly studied in the literature, in which the\nnumber of steps is kept ﬁxed as the width is taken large. When using this modiﬁed limit, the analysis above holds even in\nthe limit. Note as well that the catapult effect takes place over log(n) steps, and for practical networks will occur within the\nﬁrst 100 steps or so of training.\nIn the catapult phase, the kernel at the end of training is smaller by an order n0 amount compared with its value at\ninitialization. The kernel provides a local measure of the loss curvature. Therefore, the minima that SGD ﬁnds in the catapult\nphase are ﬂatter than those it ﬁnds in the lazy phase. Contrast this situation, in which the kernel receives non-negligible\nupdates, with the conclusions of Jacot et al. (2018) where the kernel is constant throughout training. The difference is due to\nthe large learning rate, which leads to a breakdown of the linearized approximation even at large width.\nFigure 2 illustrates the dynamics in the catapult phase. For learning rates ηcrit < η < ηmax we observe the catapult effect:\nthe loss goes up before converging to zero. The curvature exhibits the expected sharp transitions as a function of the learning\nrate: it is constant in the lazy phase, decreases in the catapult phase, and diverges for η > ηmax.\n0\n10\n20\nt · η\n0\n25\n50\n75\nTraining loss\nη=0.495\nη=0.915\nη=1.11\nη=1.2\nη=1.32\n(a)\n0\n10\n20\nt · η\n0\n1\n2\nλt\nη=0.495\nη=0.915\nη=1.11\nη=1.2\nη=1.32\n(b)\n1\n2\nη\n0\n1\n2\nλt\n2/λ0\n4/λ0\n2/η\n(c)\nFigure 2. Empirical results for the gradient descent dynamics of the warmup model with n = 103, for which ηcrit ≈1. (a) Training loss\nfor different learning rates. (b) Maximum NTK eigenvalue as a function of time. For η > 1, λt decreases rapidly to a ﬁxed value. (c)\nMaximum NTK eigenvalue at t = 25/η. The shaded area indicates learning rates for which training diverges empirically. The results are\npresented as a function of t · η (rather than t) for convenience.\n2.1.3. DIVERGENT PHASE\nCompleting the analysis of this model, when η > ηmax the loss diverges because the kernel receives positive updates,\naccelerating the rate of growth of the function. Therefore, ηmax = 4/λ0 is the maximum learning rate of the model.\n2.2. Full model\nWe now turn to analyzing the model presented at the beginning of this section, with d-dimensional inputs and m training\nsamples with general labels. The full analysis is presented in SM Section D.1; here we summarize the argument. The\nconclusions are essentially the same as those of the warmup model.\nWe introduce the notation fα := f(xα) for the function evaluated on a training sample, ˜fα := fα −yα for the error, and\nΘαβ := Θ(xα, xβ) for the kernel elements. We will treat f, ˜f evaluated on the training set as vectors in Rm, whose elements\nare fα, ˜fα. Consider the following update equation for the error, which can be derived from the update equations for the\nThe large learning rate phase of deep learning\nparameters. Note that this is the exact update equation for this model; no higher-order terms were neglected.\n˜f t+1\nα\n=\nX\nβ\n(δαβ −ηΘαβ) ˜fβ + η2\nnm(xT\nαζ)(f T ˜f) .\n(9)\nHere, ζ := P\nα ˜fαxα/m ∈Rd, and all variables are implicitly evaluated at step t unless speciﬁed otherwise.\nWe again take the modiﬁed large width limit n →∞, allowing the number of steps to scale logarithmically in the width.\nAt initialization, fα, ˜fα, and Θαβ are all of order n0. We now analyze the gradient descent dynamics as a function of the\nlearning rate.\nThe maximum eigenvalue of the kernel at step t is λt. When η < ηcrit, the norm ∥˜f t∥2 shrinks to zero in O(n0) time while\nthe kernel receives O(n−1) corrections. Therefore, in the limit the kernel remains constant until convergence. This is a\nspecial case of the NTK result (Jacot et al., 2018), and the model evolves as a linear model.\nNext, suppose that ηcrit < η < ηmax. Early during training ∥˜f∥2 grows, with the fastest growth taking place along the\ndirection of the top kernel eigenvector, emax\nt\n∈Rm. During this part of training the kernel receives O(n−1) updates, and so\nemax\nt\ndoes not change much. As a result, ˜ft becomes aligned with emax\nt\n. In addition, ft becomes close to ˜ft because ft grows\nwhile the label is constant. We therefore consider the following approximate update equations for ˜f max := P\nα ˜fαemax\nα\nand\nfor the maximum eigenvalue λ, which can be approximated by ˜f T Θ ˜f/∥˜f∥2\n2.\n˜f max\nt+1 ≈(1 −ηλt) ˜f max\nt\n+ O(n−1) ,\n(10)\nλt+1 ≈λt + η∥ζ∥2\n2\nn\n(ηλt −4) .\n(11)\nWe note in passing the similarity between these equations and (6), (7). We see that once ˜f max and ζ become of order n1/2,\nλt receives non-negligible negative corrections of order n0. This evolution continues until λt ≲2/η, after which the error\nconverges to zero. Finally, if η > ηmax, the error grows while λt receives positive updates, and the loss diverges. This\nconcludes the discussion of the theoretical model; further details can be found in Section 4 and in SM Section D.1.\n3. Experimental results\nIn this section we test the extent to which the behavior of our theoretical model describes the dynamics of deep networks in\npractical settings. The theoretical results of Section 2, describing distinct learning rate phases, are not guaranteed to hold\nbeyond the model analyzed there. We treat these results as predictions to be tested empirically, including the values ηcrit and\nηmax of the learning rates that separate the three phases.\nIn a variety of deep learning settings, we ﬁnd clear evidence of the different phases predicted by the model. The experiments\nall use MSE loss, sufﬁciently wide networks, and SGD2. Parameters such as network architecture, choice of non-linearity,\nweight parameterization, and regularization, do not signiﬁcantly affect this conclusion.\nIn terms of the learning rates that determine the location of the transitions, the only modiﬁcation needed to obtain\ngood agreement with experiment is to replace the theoretical maximum learning rate, 4/λ0, with a 1-parameter function\nηmax = cact./λ0, where cact. is an architecture-dependent constant. We ﬁnd that cact. ≈12 for all network that use\nReLU non-linearity, and it seems this parameter depends only weakly on other details of the architecture. We ﬁnd the\nlevel of agreement with the experiments surprising, given that our theoretical model involves a shallow network without\nnon-linearities.\nBuilding on the observed correlation between lower curvature and generalization performance (Keskar et al., 2016; Jiang\net al., 2020), we conjecture that optimal performance occurs in the large learning rate (catapult) phase, where the loss\nconverges to a ﬂatter minimum. For a ﬁxed amount of computational budget, we ﬁnd that this conjecture holds in all cases\nwe tried. Even when comparing different learning rates trained for a ﬁxed amount of physical time tphys = t · η, we ﬁnd that\nperformance of models trained in the catapult phase either matches or exceeds that of models trained in the lazy phase.\n2While our theoretical framework focused on (full-batch) gradient descent, we expect these the phases to happen at similar points for\nSGD as long as evolution is not noise dominated, in which case we expect all phases to be shifted towards smaller learning rates.\nThe large learning rate phase of deep learning\n3.1. Early time curvature dynamics\nOur theoretical model makes detailed predictions for the gradient descent evolution of λ, the top eigenvalue of the NTK. Here\nwe test these predictions against empirical results in a variety of deep learning models (see the Supplement for additional\nexperimental results).\nFigure 3 shows λ during the early part of training for two deep learning settings. The results are compared against the\ntheoretical predictions of a phase transition at ηcrit = 2/λ0, and a maximum learning rate of 4/λ0. Here λ0 is the top\neigenvalue of the empirical NTK at initialization.\nFor learning rates η < ηcrit, we ﬁnd that λ is independent of the learning rate and constant throughout training, as expected\nin the lazy phase. For ηcrit < η < 4/λ0 we ﬁnd that λ decreases during training to below 2/η, matching the predicted\nbehavior in the catapult phase (note that in the Wide ResNet example, λ initially increases before reaching its stable value).\nThe large learning rate behavior predicted by the model appears to persist up to the maximum learning rate, which is larger\nin these experiments than in the theoretical model. In these and other experiments involving ReLU networks, we ﬁnd that\nηmax ≈12/λ0 is a good predictor of the maximum learning rate (in the SM C.4 we discuss other nonlinearities). We\nconjecture that this is the typical maximum learning rate of networks with ReLU non-linearities.\nFigure 3 also shows the loss initially increasing before converging in the catapult phase, conﬁrming another prediction of the\nmodel. This transient behavior is very short, taking less than 10 steps to complete.\n0\n100\n200\nt · η\n0\n5\n10\nTraining loss\nη=4.5\nη=5.67\nη=9.0\nη=11.2\n(a)\n0\n100\n200\nt · η\n0.0\n0.1\n0.2\n0.3\nλt\nη=4.5\nη=5.67\nη=9.0\nη=11.2\n(b)\n20\n40\nη\n0.0\n0.1\n0.2\n0.3\nλt\n2/λ0\n4/λ0\n12/λ0\n2/η\n(c)\n0\n5\n10\n15\nt · η\n0\n20\n40\nTraining loss\nη=0.1\nη=0.2\nη=0.25\nη=0.4\n(d)\n0\n5\n10\n15\nt · η\n0\n5\n10\nλt\nη=0.1\nη=0.2\nη=0.25\nη=0.4\n(e)\n0.5\n1.0\nη\n0\n5\n10\nλt\n2/λ0\n4/λ0\n12/λ0\n2/η\n(f)\nFigure 3. Early time dynamics. (a,b,c) A 3 hidden layer fully-connected network with ReLU non-linearity trained on MNIST (ηcrit = 6.25).\n(d,e,f) Wide ResNet 28-10 trained on CIFAR-10 (ηcrit = 0.18). Both networks are trained with vanilla SGD; for more experimental\ndetails see SM Section A. (a,d) Early time dynamics of the training loss for learning rates in the linear and catapult phases. (b,e) Early\ntime dynamics of the curvature for learning rates in the linear and catapult phase. (c,f) λt measured at t · η = 250 (for FC) and t · η = 30\n(for WRN), as a function of learning rate, compared with theoretical predictions for the locations of phase transitions. Training diverges\nfor learning rates in the shaded region.\nThe large learning rate phase of deep learning\n3.2. Generalization performance\nWe now consider the performance of trained models in the different phases discussed in this work. Keskar et al. (2016)\nobserved a correlation between the ﬂatness of a minimum found by SGD and the generalization performance (see Jiang et al.\n(2020) for additional empirical conﬁrmation of this correlation). In this work, we showed that the minima SGD ﬁnds are\nﬂatter in the catapult phase, as measured by the top kernel eigenvalue. Our measure of ﬂatness differs from that of Keskar\net al. (2016), but we expect that these measures are correlated.\nWe therefore conjecture that optimal performance is often obtained for learning rates above ηcrit and below the maximum\nlearning rate.\nIn this section we test this conjecture empirically. We ﬁnd that performance in the large learning rate range always matches\nor exceeds the performance when η < ηcrit. For a ﬁxed compute budget, we ﬁnd that the best performance is always found\nin the catapult phase.\nFigure 4 shows the accuracy as a function of the learning rate for a fully-connected ReLU network trained on a subset of\nMNIST. We ﬁnd that the optimal performance is achieved above ηcrit and close to ηmax = 12/λ0, the expected maximum\nlearning rate.\n100\n101\n102\nη\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n2/λ0\n12/λ0\nTrain accuracy\nTest accuracy\nFigure 4. Final accuracy versus learning rate for a fully-connected 1 hidden layer ReLU network, trained on 512 samples of MNIST with\nfull-batch gradient descent until training accuracy reaches 1 or 700k physical steps (see SM Section A for details). We used a subset of\nsamples to accentuate the performance difference between phases. The optimal performance is obtained when the learning rate is above\nηcrit, and close to ηmax.\nNext, Figure 5 shows the performance of a convolutional network and a Wide ResNet (WRN) trained on CIFAR-10. The\nexperimental setup, which we now describe, was chosen to ensure a fair comparison of the performance across different\nlearning rates. The network is trained with different initial learning rates, followed by a decay at a ﬁxed physical time t · η to\nthe same ﬁnal learning rate. This schedule is introduced in order to ensure that all experiments have the same level of SGD\nnoise toward the end of training.\nWe present results using two different stopping conditions. In Figure 5a, 5c, all models were trained for a ﬁxed number of\ntraining steps. We ﬁnd a signiﬁcant performance gap between small and large learning rates, with the optimal learning rate\nabove ηcrit and close to ηmax. Beyond this learning rate, performance drops sharply.\nThe ﬁxed compute stopping condition, while of practical interest, biases the results in favor of large learning rates. Indeed,\nin the limit of small learning rate, training for a ﬁxed number of steps will keep the model close to initialization. To control\nfor this, in Figure 5b,5d models were trained for the same amount of physical time t · η. For the CNN of ﬁgure 5b, decaying\nthe learning rate does not have a signiﬁcant effect on performance and we observe that performance is ﬂat up to ηmax, and\nthere is no correlation between our measure of curvature and generalization performance. Figure 5d shows the analogous\nexperiment for WRN. When decaying the learning rate toward the end of training to control for SGD noise, we ﬁnd that\noptimal performance is achieved above ηcrit. In all these cases, ηmax is a good predictor of the maximal learning rate,\ndespite signiﬁcant differences in the architectures. Notice that by tuning the learning rate to the catapult phase, we are able\nto achieve performance using MSE loss, and without momentum, that is competitive with the best reported results for this\nThe large learning rate phase of deep learning\n10−4\nη\n0.50\n0.55\n0.60\n0.65\n0.70\nTest accuracy\n2/λ0\n12/λ0\nNo decay\n(a)\n10−4\nη\n0.60\n0.62\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\nTest accuracy\n2/λ0\n12/λ0\nNo decay\nDecay η →2 · 10−5\n(b)\n10−1\n100\nη\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nTest accuracy\n2/λ0\n12/λ0\nNo decay\n(c)\n10−1\n100\nη\n0.90\n0.91\n0.92\n0.93\n0.94\n0.95\n0.96\nTest accuracy\n2/λ0\n12/λ0\nNo decay\nDecay η →0.035\n(d)\nFigure 5. Test accuracy vs learning rate for (a,b) a CNN trained on CIFAR-10 using SGD with batch size 256 and L2 regularization\n(ηcrit ≈10−4) and (c,d) WRN28-10 trained on CIFAR-10 using SGD with batch size 1024, L2 regularization, and data augmentation\n(ηcrit ≈0.14); see SM A for details. (a,c) have a ﬁxed compute budget: (a) 437k steps and (b) 12k steps. (b,d) have been evolved for a\nﬁxed amount of physical time: (b) was evolved for 475/η steps (purple) and evolved for 50k more steps at learning rate 2 · 10−5 (red) and\n(d) was evolved for 3360/η steps with learning rate η (purple) and then evolved for 4800 more steps at learning rate 0.035 (red). In all\ncases, optimal performance is achieved above ηcrit and close to the expected maximum learning rate, in agreement with our predictions.\nmodel (Zagoruyko & Komodakis, 2016).\nIn SM B.1, we present additional results for WRN on CIFAR-100, with similar conclusions as those for WRN on CIFAR-10.\n4. Additional properties of the model\nSo far we have focused on the generalization performance and curvature of the large learning rate phase. Here we investigate\nadditional predictions made by our model.\n4.1. Restoration of linear dynamics\nOne striking prediction of the model is that after a period of excursion, the logit differences settle back to O(1) values, the\nNTK stops changing, and evolution is again well approximated by a linear model with constant kernel at large width.\nWe speculate that the return to linearity and constancy of the kernel may hold asymptotically in width for more general\nmodels for a range of learning rates above ηcrit. We test this by evolving the model for order log(n) steps until the catapult\neffect is over, linearizing the model, and comparing the evolution of the two models beyond this point. Figure 6 shows an\nThe large learning rate phase of deep learning\nexample of this. At ﬁxed width, the accuracy of the linear and non-linear networks match for a range of learning rates above\nthe transition up to 4/λ0. We present additional evidence for this asymptotic linearization behavior in the Supplement.\n100\n101\n102\nη\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest accuracy\n2/λ0\n4/λ0\n12/λ0\nLin at t=0\nLin at t=10\nNon-linear\nFigure 6. Evidence for linear dynamics after the catapult effect is over. Here we show the same model as in Figure 4 with the addition of\nmodels linearized at step 0 and another linearized at step 10. We observe that the model linearized after 10 steps tracks the non-linear\nperformance in the catapult phase up to η ≈4/λ0.\n4.2. Non-perturbative phase transition\nThe large width analysis of the small learning rate phase has been the subject of much work. In this phase, at inﬁnite\nwidth, the network map evolves as a linear random features model, f (0)\nt+1 = f (0)\nt\n−Θf (0)\nt\n, where f (0) is the function of the\nlinearized model. At large but ﬁnite width, corrections to this linear evolution can be systematically incorporated via a\nperturbative expansion (Taylor expansion) around inﬁnite width (Dyer & Gur-Ari, 2020; Huang & Yau, 2019).\nft = f (0)\nt\n+ 1\nnf (1)\nt\n+ · · · .\n(12)\nThe evolution equations (10) and (11) of the solvable model are an example of this. At large width and in the small learning\nrate phase, the O(n−1) terms are suppressed for all times. In contrast, the leading order dynamics of f (0)\nt\ndiverge when\nη > ηcrit, and so the true evolution cannot be described by the linear model. Indeed, the logits grow to O(n1/2) and thus\nall terms in (10) and (11) are of the same order. Similarly, the growth observed empirically in the catapult phase for more\ngeneral models cannot be described by truncating the series (12) at any order, because the terms all become comparable.\n5. Discussion\nIn this work we took a step toward understanding the role of large learning rates in deep learning. We presented a dynamical\nmechanism that allows deep networks to be trained at larger learning rates than those accessible to their linear counterparts.\nFor MSE loss, linear model training diverges when the learning rate is above the critical value ηcrit = 2/λ0, where λ0 is the\ncurvature at initialization. We showed that deep networks can train for larger learning rates by navigating to an area of the\nlandscape that has sufﬁciently low curvature. Perhaps counterintuitively, training in this regime involves an initial period\nduring which the loss increases before converging to its ﬁnal, small value. We call this the catapult effect.\n5.1. A tractable model illustrating catapult dynamics\nThese observations are made concrete in our theoretical model, where we fully analyze the gradient descent dynamics as a\nfunction of the learning rate. The analysis involves a modiﬁed large width limit, in which both the width and training time\nare taken to be large. Sweeping the learning rate from small to large, and working in the limit, we ﬁnd sharp transitions\nfrom a lazy phase where linearized model training is stable, to a catapult phase in which only the full model converges,\nand ﬁnally to a divergent phase in which training is unstable. These transitions have the hallmarks of phase transitions that\ncommonly appear in physical systems such as ferromagnets or water, as one changes parameters such as temperature. In\nThe large learning rate phase of deep learning\nparticular, these transitions are non-perturbative: a Taylor series expansion of the linearized model that takes into account\nﬁnite width corrections is not sufﬁcient to describe the behavior beyond the critical learning rate.\nWe derive the learning rates at which these transitions occur as a function of the curvature at initialization. We then treat\nthese theoretical results as predictions, to be tested beyond the regime where they are guaranteed to hold, and ﬁnd good\nquantitative agreement with empirical results across a variety of realistic deep learning settings.\nWe ﬁnd it striking that a relatively simple theoretical model can correctly predict the behavior of realistic deep learning\nmodels. In particular, we conjecture that the maximum learning rate is typically a simple function of the curvature at\ninitialization, with a single parameter cact. that seems to depend only on the non-linearity. For ReLU networks, we conjecture\nthat the maximum learning rate is approximately 12/λ0, which we conﬁrm in many cases.\n5.2. Reducing misalignment of activations and gradients\nThe catapult dynamics for the simpliﬁed model in Section 2.1 reduce curvature by shrinking the component of the ﬁrst layer\nweights u which is orthogonal to the second layer weights v, and shrinking the component of the second layer weights v\nwhich is orthogonal to the ﬁrst layer weights u. We can rewrite the simpliﬁed model in terms of a hidden layer h = ux,\nwhere f(x) = n−1/2v⊤h. The gradient with respect to this hidden layer is ∂L\n∂h = n−1/2f(x)v. These hidden layer gradients\n∂L\n∂h thus point in the same direction as v, while the hidden activations h point in the same direction as u. An alternative\ninterpretation of the catapult dynamics is then that they reduce the components of h and ∂L\n∂h which are orthogonal to\neach other. The catapult dynamics thus serve, in this simpliﬁed model, to reduce the misalignment between feedforward\nactivations h, and backpropagated gradients ∂L\n∂h . We hypothesize that this reduction of misalignment between activations\nand gradients may be a feature of large learning rates and catapult dynamics in deep, as well as shallow, networks. We\nfurther hypothesize that it may play a directly beneﬁcial role in generalization, for instance by making the model output less\nsensitive to orthogonal, out-of-distribution, perturbations of activations.\n5.3. Catapult dynamics often improve generalization\nOur results shed light on the regularizing effect of training at large learning rates. The effect presented here is independent\nof the regularizing effect of stochastic gradient noise, which has been studied extensively. Building on previous works, we\nnoted the observed correlation between ﬂatness and generalization performance. Based on these observations, we expect the\noptimal performance to often occur for learning rates larger than ηcrit, where the linearized model is unstable. Observing\nthis effect required controlling for several confounding factors that affect the comparison of performance between different\nlearning rates. Under a fair comparison, and also for a ﬁxed compute budget, we ﬁnd that this expectation holds in practice.\n5.4. Beyond inﬁnite linear models\nOne outcome of our work is to address the performance gap between ordinary neural networks, and linear models inspired\nby the theory of wide networks. Optimal performance is often obtained at large learning rates which are inaccessible to\nlinearized models. In such cases, we expect the performance gap to persist even at arbitrarily large widths. We hope our\nwork can further improve the understanding of deep learning methods.\n5.5. Other open questions\nThere are several remaining open questions. While the model predicts a maximum learning rate of 4/λ0, for models with\nReLU activations we ﬁnd that the maximum learning rate is consistently higher. This may be due to a separate dynamical\ncurvature-reduction mechanism that relies on ReLU. In addition, we do not explore the degree to which our results extend\nto softmax classiﬁcation. While we expect qualitatively similar behavior there, the non-constant Hessian of the softmax\ncross entropy makes controlled experiments more challenging. Similarly, behavior for other optimizers such as SGD with\nmomentum may differ. For example, the maximum learning rate when training a linear model is larger for gradient descent\nwith momentum than for vanilla gradient descent, and therefore the transition to the catapult phase (if it exists) will occur at\na higher learning rate. We leave these questions to future work.\nThe large learning rate phase of deep learning\nAcknowledgements\nThe authors would like to thank Kyle Aitken, Dar Gilboa, Justin Gilmer, Boris Hanin, Tengyu Ma, Andrea Montanari, and\nBehnam Neyshabur for useful discussions. We would also like to thank Jaehoon Lee for early discussions about empirical\nproperties of the lazy phase.\nReferences\nAllen-Zhu, Z., Li, Y., and Song, Z. A convergence theory for deep learning via over-parameterization. In Chaudhuri, K.\nand Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of\nProceedings of Machine Learning Research, pp. 242–252, Long Beach, California, USA, 09–15 Jun 2019. PMLR.\nArora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an inﬁnitely wide neural\nnet. In Advances in Neural Information Processing Systems, pp. 8139–8148, 2019.\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman-Milne, S. JAX: composable\ntransformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\nChizat, L., Oyallon, E., and Bach, F. On lazy training in differentiable programming. In Wallach, H., Larochelle,\nH., Beygelzimer, A., d ´Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Process-\ning Systems 32, pp. 2933–2943. Curran Associates, Inc., 2019.\nURL http://papers.nips.cc/paper/\n8559-on-lazy-training-in-\\differentiable-programming.pdf.\nDaniely, A. Sgd learns the conjugate kernel class of the network. In Advances in Neural Information Processing Systems,\npp. 2422–2430, 2017.\nDinh, L., Pascanu, R., Bengio, S., and Bengio, Y. Sharp minima can generalize for deep nets. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70, pp. 1019–1028. JMLR. org, 2017.\nDu, S. S., Lee, J. D., Li, H., Wang, L., and Zhai, X. Gradient descent ﬁnds global minima of deep neural networks. In\nProceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, pp. 1675–1685, 2019. URL http://proceedings.mlr.press/v97/du19c.html.\nDyer, E. and Gur-Ari, G. Asymptotics of wide networks from feynman diagrams. In International Conference on Learning\nRepresentations, 2020. URL https://openreview.net/forum?id=S1gFvANKDS.\nFrankle, J., Schwab, D. J., and Morcos, A. S. The early phase of neural network training. arXiv preprint arXiv:2002.10365,\n2020.\nHochreiter, S. and Schmidhuber, J. Flat minima. Neural Computation, 9(1):1–42, 1997.\nHuang, J. and Yau, H.-T. Dynamics of Deep Neural Networks and Neural Tangent Hierarchy. arXiv e-prints, art.\narXiv:1909.08156, Sep 2019.\nJacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Bengio,\nS., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems 31, pp. 8571–8580. Curran Associates, Inc., 2018.\nJastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho, K., and Geras, K. The break-even point on optimization\ntrajectories of deep neural networks. arXiv preprint arXiv:2002.09572, 2020.\nJiang, Y., Neyshabur, B., Krishnan, D., Mobahi, H., and Bengio, S. Fantastic generalization measures and where to ﬁnd\nthem. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?\nid=SJgIPJBFvH.\nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. On large-batch training for deep learning:\nGeneralization gap and sharp minima. CoRR, abs/1609.04836, 2016. URL http://arxiv.org/abs/1609.\n04836.\nLeclerc, G. and Madry, A. The two regimes of deep network training, 2020.\nThe large learning rate phase of deep learning\nLee, J., Bahri, Y., Novak, R., Schoenholz, S., Pennington, J., and Sohl-dickstein, J. Deep neural networks as gaussian\nprocesses. In International Conference on Learning Representations, 2018. URL https://openreview.net/\nforum?id=B1EA-M-0Z.\nLee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J. Wide neural networks of any\ndepth evolve as linear models under gradient descent. In Wallach, H., Larochelle, H., Beygelzimer, A., d’ Alch´e-Buc,\nF., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 8570–8581. Cur-\nran Associates, Inc., 2019. URL http://papers.nips.cc/paper/9063-wide-neural-networks-of-\\\nany-depth-evolve-as-linear-models-\\under-gradient-descent.pdf.\nLi, Y. and Liang, Y. Learning overparameterized neural networks via stochastic gradient descent on structured data. In\nAdvances in Neural Information Processing Systems, pp. 8157–8166, 2018.\nLi, Y., Wei, C., and Ma, T. Towards explaining the regularization effect of initial large learning rate in training neural\nnetworks. In Wallach, H., Larochelle, H., Beygelzimer, A., d’Alch´e Buc, F., Fox, E., and Garnett, R. (eds.), Advances in\nNeural Information Processing Systems 32, pp. 11669–11680. Curran Associates, Inc., 2019.\nMandt, S., Hoffman, M. D., and Blei, D. M. Stochastic gradient descent as approximate bayesian inference. The Journal of\nMachine Learning Research, 18(1):4873–4907, 2017.\nMay, R. M. Simple mathematical models with very complicated dynamics. Nature, 261(5560):459–467, 1976.\nMei, S., Montanari, A., and Nguyen, P.-M. A mean ﬁeld view of the landscape of two-layer neural networks. 115(33):\nE7665–E7671, 2018. doi: 10.1073/pnas.1806579115.\nNaveh, Ben-David, Sompolinsky, and Ringel. to be published.\nNovak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Abolaﬁa, D. A., Pennington, J., and Sohl-dickstein, J. Bayesian\ndeep convolutional networks with many channels are gaussian processes. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.net/forum?id=B1g30j0qF7.\nNovak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A., Sohl-Dickstein, J., and Schoenholz, S. S. Neural tangents: Fast\nand easy inﬁnite neural networks in python. In International Conference on Learning Representations, 2020. URL\nhttps://github.com/google/neural-tangents.\nPark, D. S., Sohl-Dickstein, J., Le, Q. V., and Smith, S. L. The effect of network width on stochastic gradient descent and\ngeneralization: an empirical study. CoRR, abs/1905.03776, 2019. URL http://arxiv.org/abs/1905.03776.\nRotskoff, G. and Vanden-Eijnden, E. Parameters as interacting particles: long time convergence and asymptotic error scaling\nof neural networks. In Advances in neural information processing systems, pp. 7146–7155, 2018.\nSirignano, J. and Spiliopoulos, K. Mean ﬁeld analysis of neural networks. arXiv preprint arXiv:1805.01053, 2018.\nSmith, S. L. and Le, Q. V. A bayesian perspective on generalization and stochastic gradient descent. In International\nConference on Learning Representations, 2018. URL https://openreview.net/forum?id=BJij4yg0Z.\nSmith, S. L., Kindermans, P.-J., Ying, C., and Le, Q. V. Don’t Decay the Learning Rate, Increase the Batch Size. arXiv\ne-prints, art. arXiv:1711.00489, Nov 2017.\nSmith, S. L., Duckworth, D., Rezchikov, S., Le, Q. V., and Sohl-Dickstein, J. Stochastic natural gradient descent draws\nposterior samples in function space. arXiv preprint arXiv:1806.09597, 2018.\nWoodworth, B., Gunasekar, S., Lee, J., Soudry, D., and Srebro, N. Kernel and deep regimes in overparametrized models.\narXiv preprint arXiv:1906.05827, 2019.\nXiao, L., Pennington, J., and Schoenholz, S. S. Disentangling trainability and generalization in deep learning, 2019.\nXie, Z., Sato, I., and Sugiyama, M. A diffusion theory for deep learning dynamics: Stochastic gradient descent escapes from\nsharp minima exponentially fast. arXiv preprint arXiv:2002.03495, 2020.\nThe large learning rate phase of deep learning\nZagoruyko, S. and Komodakis, N. Wide residual networks. CoRR, abs/1605.07146, 2016. URL http://arxiv.org/\nabs/1605.07146.\nZou, D., Cao, Y., Zhou, D., and Gu, Q. Stochastic gradient descent optimizes over-parameterized deep relu networks. arXiv\npreprint arXiv:1811.08888, 2018.\nThe large learning rate phase of deep learning\nSupplementary materials\nA. Experimental details\nWe are using JAX (Bradbury et al., 2018) and the Neural Tangents Library for our experiments (Novak et al., 2020).\nAll the models have been trained with Mean Squared Error normalized as L({x, y}B) =\n1\n2k|B|\nP\n(x,y)∈B,i(f i(x) −yi)2,\nwhere k is the number of classes and yi are one-targets.\nIn a similar way, we have normalized the NTK as Θij(x, x′) =\n1\nk|B|\nP\nα ∂αf i(x)∂αf j(x′) so that the eigenvalues of the\nNTK are the same as the non-zero eigenvalues of the Fisher information:\n1\nk|B|\nP\nx∈B,i ∂αf i(x)∂βf i(x).\nIn our experiments we measure the top eigenvalue of the NTK using Lanczos’ algorithm. We construct the NTK on a small\nbatch of data, typically several hundred samples, compute the top eigenvalue, and then average over batches. In this work,\nwe do not focus on precision aspects such as ﬂuctuations in the top eigenvalue across batches.\nAll experiments that compare different learning rates use the same seed for the weights at initialization and we consider only\none such initialization (unless otherwise stated) although we have not seen much variance in the phenomena described. We\nlet σw, σb denote the constant (width-independent) coefﬁcient of the standard deviation of the weight and bias initializations,\nrespectively.\nHere we describe experimental settings speciﬁc to a ﬁgure.\nFigure 3a,3b,3c. Fully connected, three hidden layers w = 2048, ReLU non-linearity trained using SGD (no momentum)\non MNIST. Batch size= 512, using NTK normalization, σw =\n√\n2, σb = 0.\nFigures 3d,3e,3f. Wide ResNet 28-18 trained on CIFAR10 with SGD (no momentum). Batch size of 128, LeCun\ninitialization with σw =\n√\n2, σb = 0, L2 = 0.\nFigures 4,6 Fully connected network with one hidden layer and ReLU non-linearity trained on 512 samples of MNIST with\nSGD (no momentum). Batch size of 512, NTK initialization with σw =\n√\n2, σb = 0.\nFigures 5a,5b. The convolutional network has the following architecture: Conv1(320) →ReLU →Conv2(320) →\nReLU →MaxPool((2,2), ’VALID’) →Conv1(320) →ReLU →Conv2(128) →MaxPool((2,2), ’VALID’) →\nFlatten() →Dense(256) →ReLU →Dense(10). Dense(n) denotes a fully-connected layer with output dimension\nn. Conv1(n), Conv2(n) denote convolutional layers with ’SAME’ or ’VALID’ padding and n ﬁlters, respectively; all\nconvolutional layers use (3, 3) ﬁlters. MaxPool((2,2), ’VALID’) performs max pooling with ’VALID’ padding and a (2,2)\nwindow size. LeCun initialization is used, with the standard deviation of the weights and biases drawn as σw =\n√\n2,\nσb = 0.05, respectively. Trained on CIFAR-10 with SGD, batch size of 256 and L2 regularization = 0.001.\nFigures 1, 5c,5d. Wide ResNet on CIFAR10 using SGD (no momentum). Training on v3-8 TPUs with a total batch size of\n1024 (and per device batch size of 128). They all use L2 regularization= 0.0005, LeCun initialization with σw = 1, σb = 0.\nThere is also data augmentation: we use ﬂip, crop and mixup. With softmax classiﬁcation, these models can get test accuracy\nof 0.965 if one uses cosine decay, so we don’t observe a big performance decay due to using MSE. Furthermore, we are\nusing JAX’s implementation of Batch Norm which doesn’t keep track of training batch statistics for test mode evaluation.\nWe have not hyperparameter tuned for learning rates nor L2 regularization parameter.\nFigures S2,S3. Wide ResNet on CIFAR100 using SGD (no momentum). Same setting as ﬁgure 5c, 5d except for the\ndifferent dataset, different L2 regularization = 0.000025 and label smoothing (we have subtracted 0.01 from the target\none-hot labels).\nFigure S7. Two hidden layer, ReLU network for one data point x = 1, y = 1.\nFigure S10. Fully connected network with two hidden layers and tanh non-linearity trained on MNIST with SGD (no\nmomentum). Batch size of 512, LeCun initialization with σw = 1, σb = 0.\nFigure S8a. Two-hidden layer fully connected network trained on MNIST with batch size 512, NTK normalization with\nσw =\n√\n2, σb = 0. Trained using both momenta γ = 0.9 and vanilla SGD for three different non-linearities: tanh, ReLU\nand identity (no non-linearity). The learning rate for each non-linearity was chosen to correspond to η =\n1\nλ0 .\nRest of SM ﬁgures. Small modiﬁcations of experiments in previous ﬁgures, speciﬁed in captions.\nThe large learning rate phase of deep learning\n20\n10\n0\n10\n20\ndim 2\nLoss\nsurface\nLazy\n= 1 /\n0\n20\n10\n0\n10\n20\ndim 1\n20\n10\n0\n10\n20\ndim 2\nCatapult\n= 3 /\n0\n20\n10\n0\n10\n20\ndim 1\nDivergent\n= 4.1 /\n0\n10\n1\n101\n103\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ncos( (u, v))\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\ninitial\nfinal\nCatapult dynamics and loss surface in terms of\nweight correlation and magnitude\n10\n1\n101\n103\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure S1. Visualization of training dynamics in all three phases. In the lazy phase, the network is approximately linear in its parameters,\nand converges exponentially to a global minimum. In the catapult phase, the loss initially grows, while the weight norm and curvature\ndecrease. Once the curvature is low enough, optimization converges. In the divergent phase, both the loss and parameter magnitudes\ndiverge. (a)-(d) Loss surface and training dynamics visualized in a 2d linear subspace. The network has a single hidden layer with width\nn = 500, linear activations, and is trained with MSE loss on a single 1D sample x = 1 with label y = 0. The parameter subspace is\ndeﬁned by u = [dim1] r + [dim2] s, v = [dim1] r −[dim2] s, where r and s are orthonormal vectors, u, v ∈Rn are the weight vectors,\nand [dim1], [dim2] are the coordinates in the subspace. If initialized in this 2d subspace, ut and vt remain in the subspace throughout\ntraining, and so training dynamics can be fully visualized with a two dimensional plot. (e) Visualization of the loss surface and training\ndynamics in terms of a nonlinear reparameterization, providing interpretable properties: x-axis correlation between weight vectors, y-axis\ncurvature λ. The trajectory shown is identical to that in (c), and in Figure 1.\nB. Experimental results: Late time performance\nB.1. CIFAR-100 performance\nWe can also repeat the performance experiments for CIFAR-100 and the same Wide ResNet 28-10 setup. In this case, using\nMSE and SGD we require to evolve the system for longer times, which requires a smaller L2 regularization. We didn’t tune\nfor it, but found that 2.5 × 10−5 works. With only one decay we can get within 3% of the Zagoruyko & Komodakis (2016)\nperformance that used softmax classiﬁcation and two learning rate decays. However, evolution for longer time is needed:\nwe found that different learning rates converge at ≈2000 physical epochs. Similar to the main text experiments, we observe\nthat if we decay after evolving for the same amount of physical epochs, larger learning rates do better. See ﬁgure S2.\nB.2. Different learning rates converge at the same physical time\nWe can also plot the test accuracy versus physical time for different learning rates to show that for vanilla SGD, the\nperformance curves of different learning rates are basically on top of each other if we plot them in physical time, which is\nwhy we ﬁnd that the fair comparison between learning rates should be at the same physical time.\nWe have picked a subset of learning rates of the previous WRN28-18 CIFAR100 experiment of SM B.1. In ﬁgure S3, we see\nhow even if the curves are slightly different they converge to roughly the same accuracy. The only curve which is slightly\ndifferent is η = 2.5 which is a rather high learning rate (close to 12\nλ0 ).\nB.3. Comparison of learning rates for different L2 regularization for WRN28-10 on CIFAR10\nEven if in the main section we have considered a model with ﬁxed L2 regularization, we can study the effect without L2 or\nwith a different value. In these two examples, we will be considering the same setup as ﬁgures 5c,5d.\nWithout L2 regularization, we see that the larger learning rate does better even in the absence of learning rate decay, although\ntraining takes a really long time. In our experience, comparing this setup with state of the art, L2 = 0 regularization makes\nThe large learning rate phase of deep learning\n10−1\n100\nη\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTest accuracy\n2/λ0\n12/λ0\nNo decay\n(a)\n10−1\n100\nη\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\nTest accuracy\n2/λ0\n12/λ0\nNo decay\nDecay η →0.01\n(b)\nFigure S2. Test accuracy vs learning rate for WRN28-10 and CIFAR100 with vanilla SGD, L2 regularization, data augmentation, label\nsmoothing and batch size 1024. The critical learning rate is ηcrit ≈0.4. (a) Evolved for 38400 steps. (b) Evolved for 96000/η steps with\nlearning rate η (blue) and then evolved for 7200 more steps at learning rate 0.01 (red).\n0\n500\n1000\n1500\n2000\nηt\n0.0\n0.2\n0.4\n0.6\n0.8\nTest accuracy\nη =0.1. Max acc 0.75\nη =0.2. Max acc 0.75\nη =0.8. Max acc 0.75\nη =1.2. Max acc 0.75\nη =1.6. Max acc 0.75\nη =2.5. Max acc 0.76\n(a)\nFigure S3. Test accuracy vs physical time for different learning rates in the WRN CIFAR100 experiment of the previous section B.1\nthe experiment take longer before convergence but does not inﬂuence performance much.\nIn the presence of L2 regularization we picked the particular value L2 = 0.0005 in order to make sure that our conclusion is\nnot dependent on the choice of L2, the only hyperparameter (other than η), we have considered a larger L2 = 0.001. We see\nthat the optimal performance in physical time is also peaked in the catapult phase, although the difference here is smaller.\nB.4. Training accuracy plots\nThe training accuracies of the previous experiments are shown in ﬁgure S6.\nThe large learning rate phase of deep learning\n0\n100\n200\n300\n400\n500\n600\nPhysical epochs\n0.86\n0.88\n0.90\n0.92\n0.94\nTest accuracy\nη = 0.2\nη = 0.02\nFigure S4. WRN28-10 on CIFAR10 without L2. Same setup as 5d but evolved for longer times.\n10−1\n100\nη\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\n2/λ0\n12/λ0\nNo decay\n(a)\n10−1\n100\nη\n0.90\n0.91\n0.92\n0.93\n0.94\n0.95\n0.96\nAccuracy\n2/λ0\n12/λ0\nNo decay\nDecay η →0.01\n(b)\nFigure S5. Test accuracies for a larger L2 CIFAR10 experiment like that of the main section. (a) WRN CIFAR-10 7200 steps as in ﬁgure\n5c. (b) WRN CIFAR10 2400 physical steps and then 4800 more steps at learning rate 0.01 as in ﬁgure 5d.\nThe large learning rate phase of deep learning\n10−1\n100\nη\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nTrain accuracy\n2/λ0\n12/λ0\nNo decay\n(a)\n10−1\n100\nη\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nTrain accuracy\n2/λ0\n12/λ0\nNo decay\nDecay η →0.035\n(b)\n10−1\n100\nη\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nTrain accuracy\n2/λ0\n12/λ0\nNo decay\n(c)\n10−1\n100\nη\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nTrain accuracy\n2/λ0\n12/λ0\nNo decay\nDecay η →0.01\n(d)\nFigure S6. Training accuracies for the performance experiments. Smaller learning rates have higher training accuracy when compared in\nphysical time. However, they still perform worse for a ﬁxed number of steps. (a) WRN CIFAR-10 12000 steps as in ﬁgure 5c. (b) WRN\nCIFAR10 3360 physical steps as in ﬁgure 5d. (c) WRN CIFAR100 38400 steps as in ﬁgure S2a.(d) WRN CIFAR100 96000 physical\nsteps as in ﬁgure S2b.\nThe large learning rate phase of deep learning\nC. Experimental results: Early time dynamics\nC.1. ReLU activations for the simple model\nIn the main text we have been using ReLU non-linearities. Compared with the simple model with no non-linearities, ReLU\nnetworks have a broader trainability regime after η =\n4\nλ0 . It looks like these networks generically well train until η = 12\nλ0 .\nThis is a generic feature of deep ReLU networks and can be already observed for the model of section 2 with a target y = 1,\ntwo hidden layers and a ReLU non-linearity: f = u.ReLU(w.ReLU(v)), as shown in ﬁgure S7). In this single sample\ncontext for η ≥12\nλ , the loss doesn’t diverge but the neurons die and end up giving the trivial f = 0 function. For deep\nnetworks with more than one hidden layer and multiple samples, as discussed in the main text, we observe that the loss\ndiverges after ∼12\nλ .\n0\n25\n50\n75\n100\nt · η\n0.0\n0.2\n0.4\n0.6\nλt\nη=0.564\nη=1.69\nη=5.36\nη=7.87\n(a)\n5\n10\n15\nη\n0.00\n0.25\n0.50\n0.75\nλt\n2/λ0\n4/λ0\n12/λ0\n2/η\n(b)\nFigure S7. Simple model ReLU non-linearity (ηcrit = 2.54). (b) is evaluated at physical time 100.\nC.2. Momenta\nThe effect of the optimizer also affects these dynamics. If we consider a similar setup with momenta, ﬁrst we expect that a\nlinear model converges in a broader range η <\n2\nλ0 (1 + γ). For smooth non-linearities, we observe that for η <\n2\nλ0 , the λt is\nconstant. However this is not true for ReLU, see ﬁgure S8a. In fact, for ReLu networks, we observe that there is a small\nlearning rate, roughly ηeﬀ,crit = ηcrit\n1−γ , below which the time dynamics of λt is similar (but non-constant). However, for\nη > ηeﬀ,crit, there are strong time dynamics, we illustrate this in ﬁgure S8b with a 3 hidden layer ReLu network.\nC.3. Effect of L2 regularization to early time dynamics\nWe don’t expect L2 regularization to affect the early time dynamics, but because of the strong rearrangement that goes on in\nthe ﬁrst steps, it could potentially have a non-trivial effect; among other things, the Hessian spectrum necessarily is decaying.\nWe can see how the dynamics that drives the rearrangement is roughly the same, even in the maximum eigenvalue at early\ntimes is decreasing slowly.\nC.4. Tanh activations\nWe observe that for Tanh activation, ηmax is closer to the simple model expectation\n4\nλ0 , see ﬁgure S10.\nC.5. WRN NTK Normalization\nAs illustrated in the text in ﬁgures 3b, 3c we also see this behaviour for NTK normalization. For completeness we include\nthe WRN model with NTK normalization. From the linearized intuition, we expect the phases to also be determined by the\nquantity ηλt, independently of the normalization. Figure S11 has the same setup as in ﬁgure 3.\nThe large learning rate phase of deep learning\n0\n50\n100\n150\n200\n250\n300\nsteps\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nλt/λ0\nIdentity\nRelu\nTanh\n(a)\n0\n50\n100\n150\n200\nt · η\n0.0\n0.1\n0.2\n0.3\nλt\nη=0.01\nη=0.1\nη=0.5\nη=2.0\nη=5.67\nη=14.0\nη=20.0\n(b)\nFigure S8. (a) Evolution of the normalized curvature λt/λ0 for d = 2 w = 2048 FC connected networks evolved with momenta (same\nnetworks with SGD with dashed line for reference) evolved for η =\n1\nλ0 . We observe that ReLU networks evolved with momenta doesn’t\nhave a constant kernel in the naive ‘lazy’ phase. (b) ηcrit = 6.96, ηcrit,eﬀ= 0.69 Same setup as the FC network of ﬁgure 3 with momenta\nγ = 0.9: fully connected, three hidden layers w = 2048, ReLU non-linearity. ηcrit is slightly different due to variations at initialization.\n0\n20\n40\nt · η\n0\n5\n10\nλt\nη=0.08\nη=0.15\nη=0.4\nη=0.8\n(a)\n0.5\n1.0\nη\n0\n5\n10\nλt\n2/λ0\n4/λ0\n12/λ0\n2/η\n(b)\nFigure S9. Same WRN as ﬁgure 3d,f with L2 regularization= 0.0005. Dynamics in physical steps of the λt and λt vs η. ηcrit = 0.18 a)\nλt, b) λt at physical time 25\nThe large learning rate phase of deep learning\n0\n1\n2\n3\nt · η\n0\n10\n20\n30\nλt\nη=0.04\nη=0.05\nη=0.07\nη=0.09\nη=0.11\n(a)\n0.05\n0.10\n0.15\nη\n0\n20\nλt\n2/λ0\n4/λ0\n2/η\n(b)\nFigure S10. Maximum NTK eigenvalue λ at early times for a 2 hidden layer fully connected network with tanh non-linearity trained on\nMNIST, with ηcrit = 0.06. (a) Early time dynamics of the curvature for learning rates in the linear and catapult phase. (b) λ measured at\nηt = 3.\n0\n2000\n4000\nt · η\n0.00\n0.02\n0.04\n0.06\nλt\nη=8.0\nη=20.0\nη=40.0\nη=55.0\nη=80.0\n(a)\n100\n200\nη\n0.00\n0.02\n0.04\n0.06\nλt\n2/λ0\n4/λ0\n12/λ0\n2/η\n(b)\nFigure S11. Same as ﬁgures 3e,3f but with NTK normalization. a,b) Wide Resnet 28-10. ηcrit = 31.47,λ vs η at physical time 4000\nThe large learning rate phase of deep learning\nD. Theoretical details\nD.1. Full model analysis\nHere we provide additional details on the theoretical analysis of the full model in Section 2.2. The gradient descent update\nequations are\nut+1\nia\n= uia −\nη\n√nmvixaα ˜fα ,\nvt+1\ni\n= vi −\nη\n√nmuiaxaα ˜fα .\n(S1)\nand\nΘαβ =\n1\nnm(|v|2xT\nαxβ + xT\nαuT uxβ)\n(S2)\nThe update equations for the error and kernel evaluated on training set inputs are\n˜f t+1\nα\n= (δαβ −ηΘαβ) ˜fβ + η2\nnm(xT\nαζ)(f T ˜f) ,\n(S3)\nΘt+1\nαβ = Θαβ −\nη\nnm\n\u0014\n(xT\nβ ζ)fα + (xT\nαζ)fβ + 2\nm(xT\nαxβ)( ˜f T f)\n\u0015\n+\nη2\nn2m\n\u0002\n|v|2(xT\nαζ)(xT\nβ ζ) + (ζT uT uζ)(xT\nαxβ)\n\u0003\n.\n(S4)\nWhere ζ := P\nα ˜fαxα/m ∈Rd. We now consider the dynamics of the kernel projected onto the ˜f direction, which is given\nby\n˜f T Θt+1 ˜f = ˜f T Θ ˜f + η\nnζT ζ\n\u0010\nη ˜f T Θ ˜f −4f T ˜f\n\u0011\n.\n(S5)\nLet us now analyze the phase structure of (S3) and (S5). For now, we neglect the last term on the right-hand side of (S3) (at\ninitialization this term is of order n−1 and is negligible at large width). Let λ0 be the maximal eigenvalue of the kernel at\ninitialization, and let emax ∈Rm be the corresponding eigenvector. Notice that ˜f projected onto the top eigenvector evolves\nas\n(emax)T ˜ft+1 = (1 −ηλ)emaxT ˜f + O(n−1) .\n(S6)\nLazy phase.\nWhen ηλ0 < 2, we see that |emaxT ˜f t| shrinks during training. The kernel updates are of order n−1, while\nconvergence happens in order n0 steps. Therefore the kernel does not change by much during training. This is a special case\nof the NTK result (Jacot et al., 2018). Effectively, the model evolves as a linear model in this phase.\nCatapult phase.\nWhen 2 < ηλ0 < 4, ∥˜f∥2 grows exponentially fast, and it grows fastest in the emax direction. Therefore,\nthe vector ˜f becomes aligned with emax after a number of steps that is of order n0. Also, f itself grows quickly while the\nlabel is constant, and so we ﬁnd that f ≈˜f ≈(emaxT ˜f)emax after a similar number of steps. When these approximations\nhold, notice that ˜f T Θ ˜f ≈λ · ∥˜f∥2\n2. From equation (S5) we can then derive an approximate equation for the evolution of the\ntop NTK eigenvalue.\nλt+1 ≈λ + η\nnζT ζ(ηλ −4) .\n(S7)\nWhile ˜f grows exponentially fast, so will ζ. When ζt becomes of order n1/2, the updates to the top eigenvalue become of\norder n0 (and negative), causing λt to decrease by a non-negligible amount. This will continue until λt < 2/η, at which\npoint ˜ft will start converging to zero. Eventually, after a number of steps of order log(n), gradient descent will converge to\na global minimum that has a lower curvature than the curvature at initialization.\nThe justiﬁcation for dropping the order n−1 term in (S6) was explained in the warmup model: While this term may affect\nthe details of the dynamics, eventually the maximum kernel eigenvalue must drop below 2/η for the component emaxT ˜f of\nthe error (and therefore for the loss) to converge to zero.\nDivergent phase.\nWhen ηλ0 > 4, both ∥˜f∥2\n2 and λ will grow, and optimization will diverge. Therefore, 4/λ0 is the\nmaximum learning rate for this model.\nThe large learning rate phase of deep learning\nE. Model dynamics close to the critical learning rate\nHere we consider the gradient descent dynamics of the model analyzed in Section 2, for learning rates η that are close to the\ncritical point ηcrit = 2/λ0. The analysis reveals that the gradient descent dynamics of the model are qualitatively different\nabove and below this point. For example, the loss decreases monotonically during training when η < ηcrit, but not when\nη > ηcrit. In this section we show that the transition from small to large learning rate becomes sharp once we take the\nmodiﬁed large width limit, in the following sense: certain functions of the learning rate become non-analytic at ηcrit in the\nlimit. This sharp transition bears close resemblance to phase transitions of the kind found in physical systems, such as the\ntransition between the liquid and gaseous phases of water. In particular, our case involves a dynamical system, where the\ndynamics are governed by the gradient descent equations. These dynamics undergo a phase transition as a function of the\nlearning rate — an external parameter. We point to the logistic map (May, 1976) as a well-known example of a dynamical\nsystem that undergoes phase transitions as a function of an external parameter.\nE.1. Non-perturbative dynamics\nA phase transition is a drastic change in a system’s behavior incurred under a small change in external parameters.\nMathematically, it is a non-analyticity in some property of the system as a function of these parameters. For example,\nconsider the property λ∗(η), the curvature of the model at the end of training as a function of the learning rate. In the\nmodiﬁed large width limit, λ∗(η) is constant for η < ηcrit, but not for η > ηcrit. Therefore, this function is not analytic at\nηcrit. Notice that this statement is true in the limit but not necessarily at ﬁnite width, where the ﬁnal curvature may be an\nanalytic function of the learning rate even at ηcrit. It is well known in physics that phase transitions only occur in a limit\nwhere the number of dynamical variables (in this case the number of model parameters) is taken to inﬁnity. One immediate\nconsequence of the non-analyticity at ηcrit is that the large learning rate phase is inaccessible from the small learning rate\nphase via a perturbative expansion. In other words, we cannot describe all properties of the model for some η > ηcrit by\ndoing a Taylor expansion around a point η0 < ηcrit and keeping a ﬁnite number of terms.\nDyer & Gur-Ari (2020); Huang & Yau (2019) developed a formalism that allows one to compute ﬁnite-width corrections to\nvarious properties of deep networks, using a perturbative expansion around the inﬁnite width limit. We have argued that the\nusual inﬁnite width approximation to the training dynamics is not valid for learning rates above ηcrit, and that a full analysis\nmust account for large ﬁnite-width effects. One may have hoped that including the perturbative ﬁnite-width corrections\ndiscussed in Dyer & Gur-Ari (2020); Huang & Yau (2019) would allow us to regain analytic control over the dynamics. The\nresults presented here suggest that this is not the case: For η > ηcrit, we expect that the perturbative expansion will not\nprovide a good approximation to the gradient descent dynamics at any ﬁnite order in inverse width.\nE.2. Critical exponents\nWhen the external parameters are close to a phase transition, one often ﬁnds that the dynamical properties of the system\nobey power law behavior. The exponents of these power laws (called critical exponents) are of interest because they are\noften found to be universal, in the sense that the same set of exponents is often found to describe the phase transitions of\ncompletely different physical systems.\nHere we consider t∗(η), the number of steps until convergence, as a function of the learning rate. We will now show\nthat t∗exhibits power-law behavior when η is close to ηcrit. For simplicity we consider the warmup model studied\nin Section 2. First, suppose that we are below the transition, setting ηλ0 = 2 −ϵ for some small ϵ > 0. From the\nupdate equation, ft+1 ≈(1 −ηλt)ft ≈−(1 −ϵ)ft we see that ft will converge to some ﬁxed small value f∗after time\nt∗≈ϵ−1 log(f −1\n∗) ∼ϵ−1. Here we assumed that λt is constant in t, which is true as long as t∗is independent of n (namely\nwe ﬁx ϵ and then take n large). Therefore, the convergence time below the transition scales as t∗∼(ηcrit −η)−1, and the\ncritical exponent is -1.\nNext, suppose that ηλ0 = 2 + ϵ with ϵ > 0. Now the update equation reads ft+1 ≈−(1 + ϵ)ft. This approximation holds\nearly during training, when the curvature updates are small. Initially, |ft| will grow until it is of order √n, at which point\nthe updates to λt become of order n0. This will happen in time ˆt ∼ϵ−1 log √n. Following this, the optimizer will converge.\nAt this point ηλt is no longer tuned to be close to the transition, and so the convergence time measured from this point on\nwill not be sensitive to ϵ. Therefore, for small ϵ the convergence time will be dominated by the early part of training, namely\nt∗≈ˆt ∼ϵ−1. The critical exponent is again -1. Figure S12 show an empirical veriﬁcation of this behavior.\nThe large learning rate phase of deep learning\n0.925\n0.950\n0.975\n1.000\n1.025\n1.050\n1.075\nlearning rate\n50\n100\n150\n200\n250\n300\n350\nsteps till convergence\nexperiment (lin)\nfit below (-0.95)\nfit above (-0.73)\n(a)\nFigure S12. The convergence time diverges when the learning rate is close to the critical value ηcrit, indicated by the solid green line. The\nmeasured exponents (shown in parentheses) are close to the predicted value of -1. Experiment involves the warmup model of Section 2\nwith width 16,000.\nF. Additional evidence for linearization in the catapult phase.\nHere we present some more detailed evidence for the re-emergence of linear dynamics in the catapult phase. Figure S13\nshow results for models trained on subsets of MNIST with learning rates η > ηcrit. In ﬁgure Figure S13a we see that for a\none-hidden-layer fully connected model trained on 512 MNIST images, the performance of the full non-linear model and\nmodel linearized after 10 steps track closely. Models evolve as linear models when the NTK is constant. In Figure S13b we\ngive evidence that as networks become wider, the change in the kernel decreases.\n0\n200\n400\n600\n800\n1000\n1200\n1400\nsteps\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nTest accuracy\nLinearized at step=0\nLinearized at step=10\nNon linear\n(a)\n29\n210\n211\n212\nwidth\n10−3\n10−2\n|Θ(t) −Θ(tlin)|\n1hl cnn (tanh), ﬁt: -0.99\n1hl fc (tanh), ﬁt: -0.82\n1hl fc (relu), ﬁt: -0.49\n2hl fc (relu), ﬁt: -0.47\n(b)\nFigure S13. Evidence for a return of linear dynamics after tlin. (a,b) Show the same model as in ﬁgure 4 with the addition of linearized\nmodels at step 0 and 10. We observe that the linearized model after 10 steps tracks the non-linear performance in the ‘catapult’ phase up\nto η ∼\n4\nλ0 (c) The change in the NTK between tlin = 50 steps and t = 1000 steps decreases as the width increases. Here we consider\n2-class MNIST with 100 samples per class.\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2020-03-04",
  "updated": "2020-03-04"
}