{
  "id": "http://arxiv.org/abs/2003.02894v2",
  "title": "Distributional Robustness and Regularization in Reinforcement Learning",
  "authors": [
    "Esther Derman",
    "Shie Mannor"
  ],
  "abstract": "Distributionally Robust Optimization (DRO) has enabled to prove the\nequivalence between robustness and regularization in classification and\nregression, thus providing an analytical reason why regularization generalizes\nwell in statistical learning. Although DRO's extension to sequential\ndecision-making overcomes $\\textit{external uncertainty}$ through the robust\nMarkov Decision Process (MDP) setting, the resulting formulation is hard to\nsolve, especially on large domains. On the other hand, existing regularization\nmethods in reinforcement learning only address $\\textit{internal uncertainty}$\ndue to stochasticity. Our study aims to facilitate robust reinforcement\nlearning by establishing a dual relation between robust MDPs and\nregularization. We introduce Wasserstein distributionally robust MDPs and prove\nthat they hold out-of-sample performance guarantees. Then, we introduce a new\nregularizer for empirical value functions and show that it lower bounds the\nWasserstein distributionally robust value function. We extend the result to\nlinear value function approximation for large state spaces. Our approach\nprovides an alternative formulation of robustness with guaranteed finite-sample\nperformance. Moreover, it suggests using regularization as a practical tool for\ndealing with $\\textit{external uncertainty}$ in reinforcement learning methods.",
  "text": "arXiv:2003.02894v2  [math.OC]  14 Jul 2020\nDistributional Robustness and Regularization in Reinforcement Learning\nEsther Derman 1 Shie Mannor 1\nAbstract\nDistributionally Robust Optimization (DRO) has\nenabled to prove the equivalence between ro-\nbustness and regularization in classiﬁcation and\nregression, thus providing an analytical reason\nwhy regularization generalizes well in statistical\nlearning. Although DRO’s extension to sequen-\ntial decision-making overcomes external uncer-\ntainty through the robust Markov Decision Pro-\ncess (MDP) setting, the resulting formulation is\nhard to solve, especially on large domains. On\nthe other hand, existing regularization methods\nin reinforcement learning (RL) only address in-\nternal uncertainty due to stochasticity. Our study\naims to facilitate robust RL by establishing a\ndual relation between robust MDPs and regular-\nization. We introduce Wasserstein distribution-\nally robust MDPs and prove that they hold out-of-\nsample performance guarantees. We also deﬁne a\nnew regularizer on empirical value functions and\nshow that it lower bounds the Wasserstein distri-\nbutionally robust value function. Then, we ex-\ntend the result to linear value function approxima-\ntion for large state spaces. Our approach provides\nan alternative formulation of robustness with\nguaranteed ﬁnite-sample performance.\nMore-\nover, it suggests using our regularizer as a prac-\ntical tool for dealing with external uncertainty in\nRL.\n1. Introduction\nMarkov Decision Processes (MDPs) originated from the\nseminal works of Bellman (1957) and Howard (1960) to\nmodel sequential decision-making problems and provide\na theoretical basis for RL methods. Real-world applica-\ntions which include healthcare and marketing, for exam-\nple, give rise to several challenging issues.\nFirstly, the\nmodel parameters are generally unknown but rather es-\n1Technion,\nIsrael.\nCorrespondence\nto:\nEsther Der-\nman\n<estherderman@campus.technion.ac.il>,\nShie\nMannor\n<shie@ee.technion.ac.il>.\ntimated through historical data.\nThis may lead the per-\nformance of a learned strategy to signiﬁcantly degrade\nwhen deployed (Mannor et al., 2007).\nSecondly, experi-\nmentation can be expensive or time-consuming which con-\nstrains policy evaluation and improvement to perform with\nlimited data (Lange et al., 2012). Lastly, when the state-\nspace is large, the value function is commonly approxi-\nmated by a parametric function, which results in additional\nuncertainty regarding the efﬁciency of a learned policy\n(Farahmand et al., 2009; Farahmand, 2011).\nThis phenomenon is reminiscent of over-ﬁtting in statis-\ntical learning that can be interpreted as the following\nsingle-stage decision-making problem (Zhang et al., 2018).\nConsider a training set of random input-output vectors\n(bxi, byi)n\ni=1 generated by a ﬁxed distribution and assume\none wants to ﬁnd a parameter θ ∈Θ that minimizes\nthe expected loss function ℓθ with respect to (w.r.t.) the\ngenerating distribution.\nIn general, the true distribution\nis unknown and hard to estimate accurately. A classical\nmethod to overcome this is to minimize the empirical risk:\nminθ∈Θ 1\nn\nPn\ni=1 ℓθ(bxi, byi), but this often yields solutions\nthat perform poorly on out-of-sample data (Friedman et al.,\n2001).\nSeveral methods ensure better generalization to new, un-\nseen data (test set) while performing well on available\ndata (training set).\nThese may be categorized into two\nmain approaches.\nThe ﬁrst one regularizes the empir-\nical risk and optimizes the resulting objective (Vapnik,\n2013). Another approach robustiﬁes the objective function\nby introducing ambiguity w.r.t. the empirical distribution\n(Kuhn et al., 2019). The resulting problem can be formu-\nlated as (DRO) : minθ∈Θ supQ∈M(bPn) E(x,y)∼Q[ℓθ(x, y)],\nwhere bPn is the empirical distribution w.r.t. the sample\nset and M(bPn) is an ambiguity set of probability dis-\ntributions consistent with the dataset.\nSuch ambiguity\nsets can be based on speciﬁed properties such as moment\nconstraints (Delage & Ye, 2010; Bertsimas et al., 2018;\nWiesemann et al., 2014), or on a given divergence from the\nempirical distribution (Hu & Hong, 2013; Ben-Tal et al.,\n2013; Erdo˘gan & Iyengar, 2006; Esfahani & Kuhn, 2017).\nThe resulting problem can be solved using Distributionally\nRobust Optimization (DRO).\nWasserstein distance-based ambiguity sets are of par-\nDistributional Robustness and Regularization in Reinforcement Learning\nticular interest in DRO theory,\nas they display in-\nteresting\nramiﬁcations\nin\nmain\nstatistical\nlearning\nproblems.\nMore\nprecisely,\nthe\nspeciﬁc\nproblem\n(DRO) is equivalent to regularization for fundamental\nlearning tasks such as classiﬁcation (Xu et al., 2009;\nShaﬁeezadeh-Abadeh et al., 2015; Blanchet et al., 2016),\nregression (Shaﬁeezadeh-Abadeh et al., 2017) and maxi-\nmum likelihood estimation (Kuhn et al., 2019). However,\nequivalence between robustness and regularizaion has only\nbeen studied on single-stage decision problems.\nRegularization techniques are widely used in RL to\nmitigate uncertainty in value function approximation\n(Farahmand et al., 2009) or to derive improved versions of\npolicy optimization methods (Shani et al., 2019). Although\nregularized policy learning helps to derive risk-sensitive\nstrategies that satisfy safety criteria (Ruszczy´nski, 2010;\nTamar et al., 2015), existing connections between regular-\nization in RL and robustness are still weak: Prior regular-\nization methods address the internal uncertainty i. e., the\ninherent stochasticity of the dynamical system, without ac-\ncounting for the external uncertainty of the MDP i. e., tran-\nsition and reward functions. Although robust MDPs pro-\nvide a convenient framework for dealing with external un-\ncertainty and enabling better generalization in sequential\ndecision-making (Iyengar, 2005; Nilim & El Ghaoui, 2005;\nXu & Mannor, 2010; Yu & Xu, 2015), solving them re-\nmains challenging even on small domains, mainly because\nit is hard to construct an uncertainty set that yields a robust\npolicy without being too conservative (Petrik & Russell,\n2019).\nOur study aims to facilitate robust RL by addressing a new\nregularization perspective on sequential decision-making\nsettings. In Sec. 2, we recall the MDP framework and de-\nscribe its robust and distributionally robust formulations. In\nSec. 2.4, we introduce Wasserstein distributionally robust\nMDPs as an analytical tool to establish a connection be-\ntween robustness and regularization. We address our main\nresult in Sec. 3: In Thm. 3.1, we devise the ﬁrst dual\nrelation between robustness to model uncertainty and regu-\nlarized value functions. An extension to linear function ap-\nproximation is addressed and formally stated in Thm. 3.2.\nFinally, we establish out-of-sample guarantees for Wasser-\nstein distributionally robust MDPs, thus demonstrating the\nfact that our regularization method enables better general-\nization to unseen data. All proofs can be found in the Ap-\npendix.\nRelated Work.\nRegularization in statistical learning pre-\ncedes its robust formulations. Indeed, a robust optimization\ninterpretation has ﬁrst been suggested by Xu et al. (2009)\nfor support vector machines, long after the regularization\nmethods of Vapnik (2013). Then, advancing research on\ndata-driven DRO has enabled to establish equivalence be-\ntween robustness and regularization in a wider range of\nstatistical learning problems (Shaﬁeezadeh-Abadeh et al.,\n2015; 2017; Blanchet et al., 2016; Kuhn et al., 2019). Dif-\nferently, in RL, RMDPs date back to 2005 with the con-\ncurrent works (Iyengar, 2005; Nilim & El Ghaoui, 2005)\nand their extension to DRMDPs (Xu & Mannor, 2010;\nYu & Xu, 2015; Yang, 2017; 2018; Chen et al., 2019),\nwhile to our knowledge, our study suggests the ﬁrst con-\nnection between regularization and robustness to parameter\nuncertainty in RL.\nMoreover, distributional RL as in (Bellemare et al., 2017)\ndiffers from our approach.\nThere, an optimal policy is\nlearned through the internal distribution of the cumula-\ntive reward while we study its worst-case expectation to\naccount for the external uncertainty of the MDP parame-\nters. Also, Bellemare et al. (2017) consider a different met-\nric, namely a minimum of Wasserstein distances over the\nstate-action space. Such a metric is problematic in our set-\nting, as it can falter the rectangularity assumption (see Sec.\n2.2-2.3).\nNotation.\nM(E) denotes the set of distributions over a\nBorel set E. For all n ∈N, we deﬁne [n] := {1, · · · , n}.\n2. From MDPs to distributionally robust\nMDPs\nThis section provides the theoretical background used\nthroughout this study: It describes the MDP setting and its\ngeneralization to robust and distributionally robust MDPs.\n2.1. Markov Decision Process\nA Markov Decision Process (MDP) is a tuple ⟨S, A, r, p⟩\nwith ﬁnite state and action spaces S and A respectively,\nsuch that r : S × A →R is a deterministic reward func-\ntion bounded by Rmax and p : S →M(S)|A| denotes the\ntransition model i. e., for all s ∈S, the elements of ps :=\n(p(·|s, a1), · · · , p(·|s, a|A|)) ∈M(S)|A| ⊂R|S|×|A| are\nlisted in such a way that transition probabilities of the\nsame action are arranged in the same block. At step t, the\nagent is in state st, chooses action at according to a policy\nπ : S →A and gets a reward r(st, at). It is then brought\nto state st+1 with probability p(st+1|st, at).\nThe agent’s goal is to maximize the following value func-\ntion over the set of policies Π for all s ∈S: vπ\np (s) =\nEπ\np\n\u0014P∞\nt=0 γtr(st, at)\n\f\f\f\f s0 = s\n\u0015\n, where γ ∈[0, 1) is a dis-\ncount factor and the expectation is conditioned on transi-\ntion model p, policy π and initial state s. It can be efﬁ-\nciently computed thanks to the Bellman operator contrac-\ntion, which admits vπ\np as a unique ﬁxed point (Puterman,\n2014).\nDistributional Robustness and Regularization in Reinforcement Learning\n2.2. Robust Markov Decision Process\nA robust MDP ⟨S, A, r, P⟩is an MDP with uncertain\ntransition model p ∈P.\nWe assume that the uncer-\ntainty set P is (s, a)-rectangular, i. e., P = N\ns∈S Ps =\nN\ns∈S,a∈A Ps,a, where for all s ∈S, Ps is a set of transi-\ntion matrices ps ∈Ps (Wiesemann et al., 2013). Accord-\ningly, for all s, s′ ∈S and a ∈A, the probability of getting\nfrom state s to state s′ after applying action a is given by\nany ps ∈Ps. Moreover, we assume that P is closed, con-\nvex and compact.\nThe robust value function under any policy π is the worst-\ncase performance: vπ\nP(s) = infp∈P vπ\np (s),\n∀s ∈S, and\na policy is robust optimal whenever it solves the max-min\nproblem maxπ∈Π minp∈P vπ\np . Thanks to the rectangular-\nity assumption, one can show that vπ\nP is the unique ﬁxed\npoint of a contracting robust Bellman operator (Iyengar,\n2005; Nilim & El Ghaoui, 2005), and a robust MDP can\nbe solved efﬁciently using robust dynamic programming.\n2.3. Distributionally Robust Markov Decision Process\nIn a Distributionally Robust Markov Decision Process\n(DRMDP) ⟨S, A, r, P, M⟩, the transition model is also\nunknown but instead, it is a random variable supported\non P and obeying a distribution µ ∈M ⊆M(P)\n(Xu & Mannor, 2010; Yu & Xu, 2015). Here, the class of\nprobability distributions M is the ambiguity set and each of\nthem is supported on the uncertainty set P. We assume that\nP and M are rectangular, so that any µ ∈M is a product\nof independent measures µs over Ps.\nGiven any policy π, the distributionally robust value func-\ntion is the worst-case expectation over the ambiguity set,\ni. e., ∀s ∈S, vπ\nM(s) = infµ∈M Ep∼µ[vπ\np (s)], and a dis-\ntributionally robust optimal policy π∗\nM satisﬁes π∗\nM ∈\narg maxπ∈Π vπ\nM.\nAlthough the DRMDP setting can be\nvery general, the ambiguity set must satisfy speciﬁc prop-\nerties for the solution to be tractable (Xu & Mannor, 2010;\nYu & Xu, 2015; Chen et al., 2019). In the following, we\nshall introduce DRMDPs with Wasserstein distance-based\nambiguity sets that yield a solvable reformulation. This\nwill enable us to connect robust MDPs with regulariza-\ntion in Sec. 3, and to establish out-of-sample guarantees\nin Sec. 4.\n2.4. Wasserstein Distributionally Robust Markov\nDecision Process\nThe Wasserstein metric arises from the theory of optimal\ntransport and measures the optimal transport cost between\ntwo probability measures (Villani, 2008). It can be viewed\nas the minimal cost required for turning a pile of sand\ninto another, with the cost function being the amount of\nsand times its distance to destination. More formally, for\nany s ∈S, let ∥·∥be a norm on the uncertainty set\nPs ⊆M(S)|A|. We deﬁne the Wasserstein distance over\ndistributions supported on Ps as follows.\nDeﬁnition 2.1. The 1-Wasserstein distance between two\nprobability measures µs, νs ∈M(Ps) is:\nd(µs, νs) :=\nmin\nγ∈Γ(µs,νs)\n\u001aZ\nPs × Ps\n∥ps −p′\ns∥γ(dps, dp′\ns)\n\u001b\n,\nwhere Γ(µs, νs) is the set of distributions over Ps × Ps\nwith marginals µs and νs.\nGiven ˆµs ∈M(Ps), we can deﬁne the Wasserstein ball of\nradius αs centered at ˆµs as:\nMαs(ˆµs) := {νs ∈M(Ps) : d(ˆµs, νs) ≤αs},\nwhich leads us to introduce Wasserstein DRMDPs. In that\nsetting, the construction of a contracting Wasserstein dis-\ntributionally robust Bellman operator enables to use stan-\ndard planning algorithms for ﬁnding an optimal policy\n(Chen et al., 2019).\nDeﬁnition 2.2. A Wasserstein DRMDP (WDRMDP) is a\ntuple ⟨S, A, r, P, Mα(ˆµ)⟩with Mα(ˆµ) = N\ns∈S Mαs(ˆµs)\nsuch that ps ∈Ps is unknown for all s ∈S. Instead, it is a\nrandom variable of distribution µs ∈Mαs(ˆµs).\n3. Regularization and Wasserstein DRMDPs\nThis section addresses the core contributions of our study.\nWe ﬁrst deﬁne the empirical value function as an MDP\ncounterpart of the empirical risk deﬁned in Sec. 1. Then,\nwe introduce a regularization method whose connection\nwith robustness is detailed in Sec. 3.2.\n3.1. Empirical Value Function and Distribution\nIn our setting, we take the empirical distribution as the cen-\nter of the Wasserstein ball ambiguity set. We estimate it\nbased on historical observations, as described below.\nTabular State-Space.\nGiven n episodes of respective\nlengths (Ti)i∈[n] we estimate the transition model through\nvisit counts as:\nbpi(s′|s, a) :=\nni(s,a,s′)\nP\ns′′∈S ni(s,a,s′′), where\ns, s′ ∈S, a ∈A, i ∈[n] and ni(s, a, s′) is the number\nof transitions (s, a, s′) occurred during episode i.\nLarge State-Space. If the state space is too large to be\nstored in a table, we use kernel averages to approximate\nthe empirical transition function (Lim & Autef, 2019). For\nall action a ∈A, deﬁne a kernel ψa : S × S →R+. Then,\nwe deﬁne the empirical transition function for all s, s′ ∈S\nas:\nbpi(s′|s, a) :=\nψa(s, s′)ni(s, a, s′)\nP\ns′′∈S ψa(s, s′′)ni(s, a, s′′),\n∀i ∈[n].\nDistributional Robustness and Regularization in Reinforcement Learning\nIn both tabular and large state-spaces, a model estimate bpi\ncan be deduced from each episode, which yields an em-\npirical value function vπ\nˆpi for any policy π ∈Π. Then,\nthe empirical distribution over transition functions is de-\nﬁned as bµn := N\n(s,a)∈S × A bµn\ns,a where for all (s, a) ∈\nS × A, bµn\ns,a :=\n1\nn\nPn\ni=1 δˆpi(·|s,a) and δˆpi(·|s,a) is a Dirac\ndistribution with full mass on ˆpi(·|s, a).\nSetting δi :=\nN\n(s,a)∈S × A δˆpi(·|s,a), the empirical distribution ˆµn can\nfurther be written as ˆµn =\n1\nn\nPn\ni=1 δi, which deﬁnes the\ncenter of our Wasserstein ball ambiguity set and enables to\nconstruct the WDRMDP ⟨S, A, r, Mα(ˆµn)⟩.\n3.2. Robustiﬁcation via Regularization\nWe now hold the tools for establishing our main result,\nwhich we prove for tabular and large state-spaces.\nTheorem\n3.1\n(Tabular\nCase).\nLet\nthe\nWDRMDP\n⟨S, A, r, Mα(ˆµn)⟩as deﬁned above and Lγ,β,Rmax :=\nβγRmax\n(1−γ)2 where β ≥0. Then, for all π ∈Π, s ∈S, we\nhave: vπ\nMα(ˆµn)(s) ≥\n1\nn\nnP\ni=1\nvπ\nˆpi(s) −κπ\ns α, where κπ\ns ≥0\ndepends on π and s while satisfying κπ\ns ≤Lγ,β,Rmax.\nFrom the above theorem we obtain that\n1\nn\nnP\ni=1\nvπ\nˆpi(s) ≥vπ\nMα(ˆµn)(s) ≥1\nn\nnP\ni=1\nvπ\nˆpi(s) −Lγ,β,Rmaxα.\nIn\nparticular,\nthe\nregularized\nvalue\nfunction\n1\nn\nPn\ni=1 vπ\nˆpi(s) −Lγ,β,Rmaxα is guaranteed to be dis-\ntributionally robust w.r.t.\nthe Wasserstein ball Mα(ˆµn)\ncentered at the empirical distribution. This is of particular\nuse for RL methods, as it enables to ensure better gen-\neralization without resorting the additional computations\nthat DRMDPs require.\nAs a matter of fact, standard\nvalue iteration can be performed to learn vπ\nˆpi(s) for all\ni ∈[n] and subtracting Lγ,β,Rmaxα to the resulting average\nensures distributional robustness w.r.t. the ambiguity set\nMα(ˆµn).\nWhen the state-space is large, one generally approximates\nthe value function using feature vectors. Speciﬁcally, de-\nﬁne as Φ(·) ∈Rm a feature vector function such that for\nall p ∈P we have vπ\np (s) ≈Φ(s)⊤wp and assume all fea-\nture vectors are linearly independent. Under standard con-\nditions, Thm. 3.1 generalizes to large scale MDPs using\nthis linear value function approximation.\nTheorem 3.2 (Linear Approximation Case). Let the\nWDRMDP ⟨S, A, r, Mα(ˆµn)⟩. Then, for all π ∈Π, s ∈\nS :\ninf\nµ∈Mα(bµn) Ep∼µ[Φ(s)⊤wp] ≥1\nn\nn\nX\ni=1\nΦ(s)⊤wbpi −ηπ\ns α,\nwhere ηπ\ns ≥0 depends on s and π.\n4. Out-of-Sample Performance Guarantees\nAssume that at each episode, a transition model p has\nbeen generated by some unknown distribution µ. A poten-\ntial defect of non-robust MDP formulations is that optimal\npolicies may perform poorly once deployed on new data.\nThis section gives out-of-sample performance guarantees\nin WDRMDPs with carefully determined Wasserstein-ball\nradii.\nFrom a statistical learning viewpoint, transition model es-\ntimates can be seen as a training set bPn := (bpi)1≤i≤n\nfollowing a distribution µn supported on Pn.\nTo avoid\ncluttered notation, we shall denote by ˆπ∗:= π∗\nMα(ˆµn)\nan optimal policy for the WDRMDP induced by the train-\ning set and ˆv∗:= vˆπ∗\nMα(ˆµn) the optimal distributionally\nrobust value function.\nThen, the out-of-sample perfor-\nmance of ˆπ∗is given by Ep∼µ[vˆπ∗\np (s)]. Deﬁning the event\nA :=\nn\nbp | Ep∼µ[vˆπ∗\np (s)] ≥vˆπ∗\nMα(ˆµn)(s),\n∀s ∈S\no\nwith\nvˆπ∗\nMα(n,ǫ)(bµn)(s) representing the certiﬁcate of the out-of-\nsample performance, Thm. 4.1 establishes that ˆπ∗satisﬁes\nµn (A) ≥1 −ǫ, where ǫ ∈(0, 1) is a conﬁdence level\n(Fournier & Guillin, 2015). This bound is the best we can\nhope for, as the true generating distribution µ is unknown.\nTheorem 4.1 (Finite-sample Guarantee). Let ǫ\n∈\n(0, 1), m := |S| × |A| and ⟨S, A, r, Mα(n,ǫ)(bµn)⟩a WDR-\nMDP. Denote by ˆv∗and ˆπ∗its optimal value and policy,\nrespectively. If the radius of the Wasserstein ball at s ∈S\nsatisﬁes\nαs(ns, ǫ) :=\n\n\n\n\n\nc0\n\u0012\n1\nnsc2\nlog\n\u0010c1\nǫ\n\u0011\u00131/(m∨2)\nif ns ≥Cǫ\nm\nc0\notherwise\nwith\nCǫ\nm\n=\n1\nc2 log\n\u0000 c1\nǫ\n\u0001\nand\nns\n=\nP\ni∈[n],a∈A,s′∈S ni(s, a, s′),\nthen µn (A)\n≥\n1 −ǫ,\nwhere c0, c1, c2 ∈R+ only depend on m ̸= 21.\nIn the above theorem, c0 corresponds to the diameter of\nthe whole space M(Ps).\nTherefore, if the sample size\nis smaller than Cǫ\nm, then the WDRMDP as deﬁned in\nThm. 4.1 becomes a robust MDP of uncertainty set Ps. Ad-\nditionally, since the radius αs(ns, ǫ) tends to 0 as ns goes\nto inﬁnity, the solution becomes less conservative as the\nsample size increases. Thm. 4.1 extends Yang (2018)[Thm.\n3] to MDPs and ensures that with high probability, the dis-\ntributionally robust optimal policy cannot yield lower value\nthan the certiﬁcate performance vˆπ∗\nMα(ˆµn)(s) deduced from\nthe training set. As a result, the regularized value function\na fortiori satisﬁes this performance guarantee.\n1A comparable conclusion can be established for m = 2, but\nwe omit it due to its limited interest.\nDistributional Robustness and Regularization in Reinforcement Learning\n5. Discussion\nOur study facilitates robust RL by establishing regularized\nvalue function as a lower-bound of a distributionally ro-\nbust value. Future work should analyze the tightness and\nthe asymptotic consistency of our approach with increas-\ning sample size. Other compelling directions include the\nextension of our results to non-linear function approxima-\ntion and deep architectures (Levine et al., 2017). It would\nalso be interesting to consider extensions of our regularized\nformulation to policy optimization, and build a connection\nwith regularized policy search.\nAcknowledgements\nThe authors would like to thank Shimrit Shtern for point-\ners to the relevant literature. Thanks also to Shirli Di Cas-\ntro Shashua, Guy Tennenholtz and Nadav Merlis for their\ncomprehensive review of an earlier draft.\nReferences\nBarbu, V. and Precupanu, T. Convexity and optimization\nin Banach spaces. Springer Science & Business Media,\n2012.\nBellemare, M. G., Dabney, W., and Munos, R. A distribu-\ntional perspective on reinforcement learning. In Proceed-\nings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 449–458. JMLR. org, 2017.\nBellman, R. A Markovian decision process. Journal of\nmathematics and mechanics, pp. 679–684, 1957.\nBen-Tal, A., den Hertog, D., De Waegenaere, A., Melen-\nberg, B., and Rennen, G. Robust solutions of optimiza-\ntion problems affected by uncertain probabilities. Man-\nagement Science, 59(2):341–357, 2013.\nBertsekas, D. P. Convex optimization theory. Athena Sci-\nentiﬁc Belmont, 2009.\nBertsekas, D. P. and Tsitsiklis, J. N. Neuro-Dynamic Pro-\ngramming. Athena Scientiﬁc, 1st edition, 1996. ISBN\n1886529108.\nBertsimas, D., Sim, M., and Zhang, M. Adaptive distribu-\ntionally robust optimization. Management Science, 65\n(2):604–618, 2018.\nBlanchet, J., Kang, Y., and Murthy, K. Robust Wasserstein\nproﬁle inference and applications to machine learning.\narXiv:1610.05627, 2016.\nChen, Z., Yu, P., and Haskell, W. B. Distributionally robust\noptimization for sequential decision-making. Optimiza-\ntion, 68(12):2397–2426, 2019.\nDelage, E. and Ye, Y.\nDistributionally robust optimiza-\ntion under moment uncertainty with application to data-\ndriven problems. Operations Research, 58(3):595–612,\n2010.\nDerman, E., Mankowitz, D., Mann, T., and Mannor, S.\nSoft-robust actor-critic policy-gradient. AUAI press for\nAssociation for Uncertainty in Artiﬁcial Intelligence, pp.\n208–218, 2018.\nDerman, E., Mankowitz, D., Mann, T., and Mannor, S. A\nBayesian approach to robust reinforcement learning. Un-\ncertainty in Artiﬁcial Intelligence, 2019.\nErdo˘gan, E. and Iyengar, G.\nAmbiguous chance con-\nstrained problems and robust optimization. Mathemat-\nical Programming, 107(1-2):37–61, 2006.\nEsfahani, P. and Kuhn, D. Data-driven distributionally ro-\nbust optimization using the Wasserstein metric: perfor-\nmance guarantees and tractable reformulations. Mathe-\nmatical Programming, 2017.\nFarahmand, A. M. Regularization in reinforcement learn-\ning. 2011.\nFarahmand, A. M., Ghavamzadeh, M., Mannor, S., and\nSzepesv´ari, C. Regularized policy iteration. In Advances\nin Neural Information Processing Systems, pp. 441–448,\n2009.\nFournier, N. and Guillin, A. On the rate of convergence in\nWasserstein distance of the empirical measure. Probabil-\nity Theory and Related Fields, 162(3-4):707–738, 2015.\nFriedman, J., Hastie, T., and Tibshirani, R. The elements of\nstatistical learning, volume 1. Springer series in statis-\ntics New York, 2001.\nHoward, R. A. Dynamic programming and Markov pro-\ncesses. 1960.\nHu, Z. and Hong, L. J. Kullback-Leibler divergence con-\nstrained distributionally robust optimization. Available\nat Optimization Online, 2013.\nIyengar, G. N. Robust dynamic programming. Mathemat-\nics of Operations Research, 30(2):257–280, 2005.\nKuhn,\nD.,\nEsfahani,\nP. M.,\nNguyen,\nV. A.,\nand\nShaﬁeezadeh-Abadeh, S. Wasserstein distributionally ro-\nbust optimization: Theory and applications in machine\nlearning. In Operations Research & Management Sci-\nence in the Age of Analytics, pp. 130–166. INFORMS,\n2019.\nLange, S., Gabel, T., and Riedmiller, M. Batch reinforce-\nment learning.\nIn Reinforcement learning, pp. 45–73.\nSpringer, 2012.\nDistributional Robustness and Regularization in Reinforcement Learning\nLevine, N., Zahavy, T., Mankowitz, D. J., Tamar, A., and\nMannor, S.\nShallow updates for deep reinforcement\nlearning. In Advances in Neural Information Processing\nSystems, pp. 3135–3145, 2017.\nLim, S. H. and Autef, A. Kernel-based reinforcement learn-\ning in robust Markov decision processes.\nIn Interna-\ntional Conference on Machine Learning, pp. 3973–3981,\n2019.\nMannor, S., Simester, D., Sun, P., and Tsitsiklis, J. N. Bias\nand variance approximation in value function estimates.\nManagement Science, 53(2):308–322, 2007.\nNilim, A. and El Ghaoui, L. Robust control of Markov de-\ncision processes with uncertain transition matrices. Op-\nerations Research, 53(5):780–798, 2005.\nPetrik, M. and Russell, R. H. Beyond conﬁdence regions:\nTight Bayesian ambiguity sets for robust MDPs. arXiv\npreprint arXiv:1902.07605, 2019.\nPuterman, M. L. Markov Decision Processes.: Discrete\nStochastic Dynamic Programming. John Wiley & Sons,\n2014.\nRoy, A., Xu, H., and Pokutta, S. Reinforcement learning\nunder model mismatch. In Advances in neural informa-\ntion processing systems, pp. 3043–3052, 2017.\nRuszczy´nski, A.\nRisk-averse dynamic programming for\nmarkov decision processes. Mathematical programming,\n125(2):235–261, 2010.\nShaﬁeezadeh-Abadeh, S., Esfahani, P., and Kuhn, D. Distri-\nbutionally robust logistic regression. Advances in Neural\nInformation Processing Systems, pp. 1576–1584, 2015.\nShaﬁeezadeh-Abadeh, S., Kuhn, D., and Esfahani, P. Reg-\nularization via mass transportation. arXiv:1710.10016,\n2017.\nShani, L., Efroni, Y., and Mannor, S. Adaptive trust region\npolicy optimization: Global convergence and faster rates\nfor regularized mdps. arXiv preprint arXiv:1909.02769,\n2019.\nStrehl, A. L. and Littman, M. L. An analysis of model-\nbased interval estimation for markov decision processes.\nJournal of Computer and System Sciences, 74(8):1309–\n1331, 2008.\nTamar, A., Mannor, S., and Xu, H. Scaling up robust MDPs\nusing function approximation. In International Confer-\nence on Machine Learning, pp. 181–189, 2014.\nTamar, A., Chow, Y., Ghavamzadeh, M., and Mannor, S.\nPolicy gradient for coherent risk measures. In Advances\nin Neural Information Processing Systems, pp. 1468–\n1476, 2015.\nVapnik, V.\nThe nature of statistical learning theory.\nSpringer science & business media, 2013.\nVillani, C. Optimal transport: old and new, volume 338.\nSpringer Science & Business Media, 2008.\nWiesemann, W., Kuhn, D., and Rustem, B. Robust Markov\ndecision processes.\nMathematics of Operations Re-\nsearch, 38(1):153–183, 2013.\nWiesemann, W., Kuhn, D., and Sim, M. Distributionally\nrobust convex optimization. Operations Research, 62(6):\n1358–1376, 2014.\nXu, H. and Mannor, S. Distributionally robust Markov deci-\nsion processes. In Advances in Neural Information Pro-\ncessing Systems, pp. 2505–2513, 2010.\nXu, H., Caramanis, C., and Mannor, S. Robustness and\nregularization of support vector machines. Journal of\nMachine Learning Research, 10(Jul):1485–1510, 2009.\nYang, I. A convex optimization approach to distribution-\nally robust Markov decision processes with Wasserstein\ndistance. IEEE Control Systems Letters, 1(1), 2017.\nYang, I.\nWasserstein distributionally robust stochas-\ntic control: A data-driven approach.\narXiv preprint\narXiv:1812.09808, 2018.\nYu, P. and Xu, H. Distributionally robust counterpart in\nmarkov decision processes. IEEE Transactions on Auto-\nmatic Control, 61(9):2538–2543, 2015.\nZhang, C., Vinyals, O., Munos, R., and Bengio, S. A study\non overﬁtting in deep reinforcement learning.\narXiv\npreprint arXiv:1804.06893, 2018.\nDistributional Robustness and Regularization in Reinforcement Learning\nA. Preliminaries on Convex Analysis\nFor completeness, this section provides the theoretical background used throughout the proofs. We ﬁrst recall some def-\ninitions and fundamental results of convex analysis. This preliminary study will play a crucial role in our regularization\nmethod.\nConsider a convex Euclidean space X equipped with a scalar product ⟨·, ·⟩. Given a norm ∥·∥over X, the dual norm\nis deﬁned through ∥·∥∗= sup∥x∥≤1⟨·, x⟩. Further denote by R = [−∞, +∞] the extended reals and U : X →R an\nextended real-valued function over X. We then deﬁne the following.\nDeﬁnition A.1. (a) Proper Function. We say that U is proper if U > −∞and there exists x ∈X such that U(x) < +∞.\n(b) Closed Function. We say that U is a closed function if its epigraph epi(U) := {(x, c) ∈X ×R|U(x) ≤c} is a closed\nsubset of X × R.\nConvex Closure. The convex closure ˘cl(U) of U is the greatest closed and convex function upper-bounded by U i. e., if Uc\nis a closed and convex function that satisﬁes Uc ≤U, then Uc ≤˘cl(U).\nIn other words, a function is proper if and only if its epigraph is nonempty and does not contain a vertical line. Moreover,\nwhen dealing with a non-convex function, we may work with its convex closure instead, in order to apply standard results\nfrom convex analysis. In particular, if the convex closure of a function is proper, then it coincides with its double conjugate,\nas we detail below.\nDeﬁnition A.2 (Conjugate Function). The Legendre-Fenchel transform (or conjugate function) of U is the mapping\nU ∗: X →R deﬁned by\nU ∗(y) := sup\nx∈X\n⟨y, x⟩−U(x),\nDenoting by X∗:= {y : X →R|y is linear} the dual space of X, we further deﬁne the conjugate function of U ∗as\nU ∗∗(x) := sup\ny∈X∗⟨y, x⟩−U ∗(y),\nwhich is also the double conjugate of U.\nRegardless of the initial function U, its conjugate U ∗is convex and closed but not necessarily proper. In fact, if U is convex,\nthen U ∗is proper if and only if U is, as stated in the fundamental theorem below (see Bertsekas (2009); Barbu & Precupanu\n(2012) for a proof).\nTheorem A.1 (Conjugacy Theorem). The following holds:\n(a) U ≥U ∗∗\n(b) If U is convex and closed, then U is proper if and only if U ∗is.\n(c) If U is closed, proper and convex, then U = U ∗∗.\n(d) The conjugates of U and ˘cl(U) are equal.\nAdditionally to this standard theorem, we will be using the following result.\nProposition A.1. If X is compact and U is a proper closed function over X, then its convex closure ˘cl(U) is also proper.\nProof. We apply the Weierstrass theorem (Barbu & Precupanu, 2012)[Thm. 2.8.]: Since U is closed on X compact, it\ntakes a minimal value on X. Therefore, there exists x0 ∈X such that infx∈X U(x) = U(x0). Moreover, we have\ninfx∈X ˘cl(U)(x) = infx∈X U(x), so infx∈X ˘cl(U)(x) = U(x0). It follows that ˘cl(U) > −∞. On the other hand, since\nU is proper, there exists x ∈X such that U(x) < +∞. By deﬁnition of the convex closure, ˘cl(U)(x) ≤U(x) so ˘cl(U) is\nproper.\nB. Regularization and Wasserstein DRMDPs\nB.1. Proof of Theorem 3.1\nWe ﬁrst establish the following lemma.\nLemma B.1. For all policy π ∈Π and state s ∈S deﬁne the mapping uπ\ns : P →R as p 7→vπ\np (s). Then, the following\nholds:\n(i) uπ\ns is proper.\n(ii) If P is closed, then uπ\ns is continuous and thus, it is a closed function.\nDistributional Robustness and Regularization in Reinforcement Learning\nProof. Claim (i). By assumption, the reward function is bounded by Rmax, so we have\n\f\fvπ\np (s)\n\f\f =\n\f\f\f\f\fEπ\np\n\" ∞\nX\nt=0\nγtr(st, at)|s0 = s\n#\f\f\f\f\f ≤\n∞\nX\nt=0\nγtRmax = Rmax\n1 −γ ,\nand uπ\ns is proper.\nClaim (ii). Denote by (pn)n≥1 a sequence that converges to p. Since P is closed, p ∈P and uπ\ns (p) is well\ndeﬁned. Moreover, uπ\ns (p) = vπ\np (s). For all n ≥1 we introduce the Bellman operator T π\npn w.r.t. transition\npn:\nT π\npnv(s) = r(s, π(s)) + γ\nX\ns′∈S\npn(s, π(s), s′)v(s′),\nand T π\np the Bellman operator w.r.t. transition p:\nT π\np v(s) = r(s, π(s)) + γ\nX\ns′∈S\np(s, π(s), s′)v(s′).\nThen, we have\nlim\nn→∞T π\npnv(s) = r(s, π(s)) + γ lim\nn→∞\nX\ns′∈S\npn(s, π(s), s′)v(s′)\n(a)\n= r(s, π(s)) + γ\nX\ns′∈S\nlim\nn→∞pn(s, π(s), s′)v(s′)\n= r(s, π(s)) + γ\nX\ns′∈S\np(s, π(s), s′)v(s′)\n= T π\np v(s),\nwhere equality (a) holds since S and A are ﬁnite sets. Remark that here, we established the continuity of\nthe Bellman operator with respect to the transition function.\nAs a result, for all ǫ > 0, there exists nǫ such that for all n ≥nǫ we have |T π\npnvπ\npn(s)−T π\np vπ\npn(s)| ≤(1−γ)ǫ.\nUsing the fact that vπ\npn (resp. vπ\np ) is the unique ﬁxed point of T π\npn (resp. T π\np ) and that T π\np is a γ-contraction,\nfor all n ≥nǫ we can write:\n|vπ\npn(s) −vπ\np (s)| = |T π\npnvπ\npn(s) −T π\np vπ\np (s)|\n≤|T π\npnvπ\npn(s) −T π\np vπ\npn(s)| + |T π\np vπ\npn(s) −T π\np vπ\np (s)|\n≤(1 −γ)ǫ + γ|vπ\npn(s) −vπ\np (s)|,\nso (1 −γ)|vπ\npn(s) −vπ\np (s)| ≤(1 −γ)ǫ. Since γ ∈(0, 1), (1 −γ) is positive and dividing both sides\nby (1 −γ) yields |vπ\npn(s) −vπ\np (s)| ≤ǫ. Based on the fact that ǫ > 0 is arbitrary, we have shown that\nvπ\npn(s) →n→∞vπ\np (s), which concludes the proof.\nWe are now ready to prove Thm.3.1, whose full statement is recalled below:\nTheorem (Tabular Case). Let the WDRMDP ⟨S, A, r, Mα(ˆµn)⟩as deﬁned above and Lγ,β,Rmax := βγRmax\n(1−γ)2 where β ≥0.\nThen, for all π ∈Π, s ∈S, we have: vπ\nMα(ˆµn)(s) ≥1\nn\nnP\ni=1\nvπ\nˆpi(s)−κπ\ns α, where κπ\ns ≥0 depends on π and s while satisfying\nκπ\ns ≤Lγ,β,Rmax.\nThe proof proceeds in two steps. First, we establish the regularized value function as a lower bound of the distributionally\nrobust value function. Then, we provide an upper bound of the regularization term, thus enabling to determine the maximal\ngap between both values.\nClaim (i). Let ⟨S, A, r, Mα(ˆµn)⟩be a Wasserstein DRMDP with a radius-α-ball ambiguity set centered at the\nempirical distribution (see its construction in Sec. 3.1). Then, for all π ∈Π, s ∈S, we have: vπ\nMα(ˆµn)(s) ≥\n1\nn\nnP\ni=1\nvπ\nˆpi(s) −κπ\ns α, where κπ\ns ≥0 depends on π and s.\nDistributional Robustness and Regularization in Reinforcement Learning\nProof. For all i ∈[n], let ˆpi\ns := N\na∈A ˆpi\ns,a and ˆµn\ns = 1\nn\nPn\ni=1 δˆpis, so that ˆµn := N\ns∈S ˆµn\ns . The Wasserstein distribution-\nally robust value function is given by\nvπ\nMα(ˆµn)(s) =\ninf\nµ∈Mα(ˆµn) Ep∼µ[vπ\np (s)].\nBy construction of the ambiguity set Mα(ˆµn) and the empirical distribution ˆµn, the constraint µ ∈Mα(ˆµn) is equivalent\nto requiring µs ∈Mαs(ˆµn\ns ),i. e., d(µs, ˆµn\ns ) ≤αs for all s ∈S. Therefore, recalling the deﬁnition of the Wasserstein\nmetric\nd(µs, ˆµn\ns ) :=\nmin\nγ∈Γ(µs,ˆµn\ns )\n\u001aZ\nPs × Ps\n∥ps −p′\ns∥γ(dps, dp′\ns)\n\u001b\n,\nwe have µs\n∈\nMαs(ˆµn\ns ) if and only if there exists µ1\ns, · · · , µn\ns\n∈\nM(Ps) such that µs\n=\n1\nn\nPn\ni=1 µi\ns and\n1\nn\nPn\ni=1 Epis∼µis\n\u0002\n∥pi\ns −bpi\ns∥\n\u0003\n≤αs.\nFor all i ∈[n], deﬁne pi := N\ns∈S pi\ns, µi := N\ns∈S µi\ns, and the average distribution ¯µ := 1\nn\nPn\ni=1 µi. Further consider the\nproduct space M(S)|S|×|A| with the product norm corresponding to ∥·∥e. g., if ∥·∥= ∥·∥2 on Ps, take the ℓ2-norm on P\ndeﬁned as ∥p∥2 :=\n\u0010P\ns∈S∥ps∥2\n2\n\u00111/2\n. Then, with the slight abuse of notation ∥p∥≡∥ps∥, the worst-case distributionally\nrobust value function may be formulated as\ninf\nµ∈Mα(ˆµn) Ep∼µ[vπ\np (s)] = min\nµ Ep∼µ[vπ\np (s)] s.t.\n\n\n\n\n\nµ = ¯µ\n1\nn\nn\nX\ni=1\nEpi∼µi [∥pi −bpi∥] ≤α,\nfor a radius α determined by radii αs-s. Thus, replacing distribution µ by its constrained law and using a duality argument,\nwe obtain\ninf\nµ∈Mα(ˆµn) Ep∼µ[vπ\np (s)] =\ninf\nµ1s,··· ,µn\ns :µi=N\ns∈S µis\nsup\nλ≥0\n \nEp∼¯µ[vπ\np (s)] −λ\n \nα −1\nn\nn\nX\ni=1\nEpi∼µi [∥pi −bpi∥]\n!!\n.\nThanks to the maxmin inequality and setting all µi\ns-s to a Dirac distribution with full mass on the worst-case transition\np ∈M(S)|S|×|A|, we can write\ninf\nµ∈Mα(ˆµn) Ep∼µ[vπ\np (s)] ≥sup\nλ≥0\ninf\nµ1\ns,··· ,µn\ns :µi=N\ns∈S µi\ns\n \nEp∼¯µ[vπ\np (s)] −λ\n \nα −1\nn\nn\nX\ni=1\nEpi∼µi [∥pi −bpi∥]\n!!\n= sup\nλ≥0\n1\nn\nn\nX\ni=1\ninf\nµi:µi=N\ns∈S µis\n\u0000Epi∼µi\n\u0002\nvπ\npi(s) + λ∥pi −bpi∥\n\u0003\u0001\n−λα\n= sup\nλ≥0\n1\nn\nn\nX\ni=1\ninf\np∈M(S)|S|×|A|\n\u0000vπ\np (s) + λ∥p −bpi∥\n\u0001\n−λα.\nNow, given uπ\ns : p 7→vπ\np (s), we deﬁne ˜uπ\ns : R|S|×|A|×|S| →R as\n˜uπ\ns (˜p) :=\ninf\np∈M(S)|S|×|A| uπ\ns (p) + Lγ,β,Rmax∥p −˜p∥.\nThis function will be used in the sequel to bound the gap between the distributionally robust value function and the regu-\nlarized one. Also, it is clear that ˜uπ\ns is a continuation function of uπ\ns , so can write\ninf\nµ∈Mα(ˆµn) Ep∼µ[vπ\np (s)] ≥sup\nλ≥0\n1\nn\nn\nX\ni=1\ninf\np∈M(S)|S|×|A|\n\u0000vπ\np (s) + λ∥p −bpi∥\n\u0001\n−λα\n≥sup\nλ≥0\n1\nn\nn\nX\ni=1\ninf\n˜p∈R|S|×|A|×|S| (˜uπ\ns (˜p) + λ∥˜p −bpi∥) −λα.\nDistributional Robustness and Regularization in Reinforcement Learning\nWe introduce auxiliary variables x1, · · · , xn in order to reformulate the bound as\ninf\nµ∈Mα(ˆµn) Ep∼µ[vπ\np (s)] ≥\nmax\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\ninf\n˜p∈R|S|×|A|×|S| (˜uπ\ns (˜p) + λ∥˜p −bpi∥) ≥xi,\n∀i ∈[n]\nλ ≥0.\nFor all i ∈[n], let the mapping fi : ˜p 7→˜uπ\ns (˜p) + λ∥˜p −bpi∥. By applying Prop. 1.3.17. of (Bertsekas, 2009), we obtain\n˘cl(fi)(˜p) = ˘cl(˜uπ\ns )(˜p) + λ∥˜p −bpi∥, where we used the fact that ˜p 7→λ∥˜p −bpi∥is convex and closed for all λ ≥0. By\nProp. 1.3.13. of (Bertsekas, 2009), we thus have\nmax\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\ninf\n˜p∈R|S|×|A|×|S| (˜uπ\ns (˜p) + λ∥˜p −bpi∥) ≥xi,\n∀i ∈[n]\nλ ≥0\n=\nmax\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\ninf\n˜p∈R|S|×|A|×|S|\n\u0010\n˘cl(˜uπ\ns )(˜p) + λ∥˜p −bpi∥\n\u0011\n≥xi,\n∀i ∈[n]\nλ ≥0.\nMoreover, using the deﬁnition of the dual norm ∥·∥∗, the inequality constraints are equivalent to the following:\nmax\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\ninf\n˜p∈R|S|×|A|×|S|\nsup\n∥yi∥∗≤λ\n\u0010\n˘cl(˜uπ\ns )(˜p) + ⟨yi, ˜p −bpi⟩\n\u0011\n≥xi,\n∀i ∈[n]\nλ ≥0,\nso the worst-case expectation can be reformulated as\ninf\nµ∈Mα(ˆµn) Ep∼µ[vπ\np (s)] ≥\nmax\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\ninf\n˜p∈R|S|×|A|×|S|\nsup\n∥yi∥∗≤λ\n\u0010\n˘cl(˜uπ\ns )(˜p) + ⟨yi, ˜p −bpi⟩\n\u0011\n≥xi,\n∀i ∈[n]\nλ ≥0.\nNow introduce the conjugate transform of ˘cl(˜uπ\ns ) w.r.t. uncertainty set P := R|S|×|A|×|S|:\n˘cl(˜uπ\ns )∗(z) :=\nsup\n˜p∈R|S|×|A|×|S|\n\u0010\n⟨z, ˜p⟩−˘cl(˜uπ\ns )(˜p)\n\u0011\n.\nBy Thm. A.1(d), ˘cl(˜uπ\ns )∗(z) = (˜uπ\ns )∗(z). Moreover, by Lemma B.1 and by construction of its continuation function, ˜uπ\ns is\nproper. Therefore, its convex closure ˘cl(˜uπ\ns ) is also proper: Indeed, if it were not, then it would be identically equal to −∞.\nMoreover, we have ˘cl(uπ\ns )(p) ≤uπ\ns (p) = (˜uπ\ns )(p) for all p ∈M(S)|S|×|A|, and by deﬁnition of ˘cl(uπ\ns ) as the greatest\nclosed and convex minorant of ˜uπ\ns , we end up with ˘cl(uπ\ns )(p) ≤˘cl(˜uπ\ns )(p). As a result, we have ˘cl(uπ\ns )(p) = −∞. This\ncannot happen thanks to Prop. A.1.\nFinally, according to Thm. A.1(c), ˘cl(˜uπ\ns ) coincides with its bi-conjugate function and\n˘cl(˜uπ\ns )(˜p) = ˘cl(˜uπ\ns )∗∗(˜p) = sup\nzi∈Zπ\ns\n\u0010\n⟨z, ˜p⟩−˘cl(˜uπ\ns )∗(z)\n\u0011\n= sup\nzi∈Zπ\ns\n(⟨z, ˜p⟩−(˜uπ\ns )∗(z)) ,\nwhere Zπ\ns := {z : ˘cl(˜uπ\ns )∗(z) < ∞} = {z : (˜uπ\ns )∗(z) < ∞} is the effective domain of (˜uπ\ns )∗. Thus, if we use the\nreformulation of the convex closure and apply the minimax theorem 2 we obtain\ninf\n˜p∈R|S|×|A|×|S|\nsup\n∥yi∥∗≤λ\n\u0010\n˘cl(˜uπ\ns )(˜p) + ⟨yi, ˜p −bpi⟩\n\u0011\n=\ninf\n˜p∈R|S|×|A|×|S| sup\nzi∈Zπ\ns\nsup\n∥yi∥∗≤λ\n⟨zi, ˜p⟩−(˜uπ\ns )∗(zi) + ⟨yi, ˜p −bpi⟩\n= sup\nzi∈Zπ\ns\nsup\n∥yi∥∗≤λ\ninf\n˜p∈R|S|×|A|×|S|⟨zi, ˜p⟩−(˜uπ\ns )∗(zi) + ⟨yi, ˜p −bpi⟩\n= sup\nzi∈Zπ\ns\nsup\n∥yi∥∗≤λ\n−(˜uπ\ns )∗(zi) −⟨yi, bpi⟩+\ninf\n˜p∈R|S|×|A|×|S|⟨˜p, zi + yi⟩\n= sup\nzi∈Zπ\ns\nsup\n∥yi∥∗≤λ\n−(˜uπ\ns )∗(zi) −⟨yi, bpi⟩−σR|S|×|A|×|S|(−zi −yi),\n2Prop. 5.5.4. of (Bertsekas, 2009)\nDistributional Robustness and Regularization in Reinforcement Learning\nwhere σP(y) := sup˜p∈P⟨˜p, y⟩denotes the support function of a general set P. We then deduce that\ninf\n˜p∈R|S|×|A|×|S|\nsup\n∥yi∥∗≤λ\n\u0010\n˘cl(˜uπ\ns )(˜p) + ⟨yi, ˜p −bpi⟩\n\u0011\n= sup\nzi∈Zπ\ns\nsup\n∥yi∥∗≤λ\n−(˜uπ\ns )∗(zi) −⟨yi, bpi⟩−σR|S|×|A|×|S|(−zi −yi)\n= sup\nzi∈Zπ\ns\nsup\n∥zi∥∗≤λ\n−(˜uπ\ns )∗(zi) + ⟨zi, bpi⟩\n=\n(˘cl(˜uπ\ns )(ˆpi) if sup{∥zi∥∗: zi ∈Zπ\ns } ≤λ\n−∞otherwise.\nTherefore, recalling the notation κπ\ns := sup{∥z∥∗: z ∈Zπ\ns }, we obtain\nsup\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\ninf\n˜p∈R|S|×|A|×|S|\nsup\n∥yi∥∗≤λ\n\u0010\n˘cl(˜uπ\ns )(˜p) + ⟨yi, ˜p −bpi⟩\n\u0011\n≥xi,\n∀i ∈[n]\nλ ≥0\n≥sup\nλ\nsup\nx1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n(˘cl(˜uπ\ns )(ˆpi) ≥xi,\n∀i ∈[n]\nλ ≥κπ\ns .\nPutting this altogether yields vπ\nMα(ˆµn)(s) ≥1\nn\nPn\ni=1 ˘cl(˜uπ\ns )(ˆpi) −κπ\ns α. Now let F : (ˆp1, · · · , ˆpn) 7→Pn\ni=1 ˜uπ\ns (ˆpi). By\nProp.1.3.17 of (Bertsekas, 2009), we have ˘cl(F)(ˆp1, · · · , ˆpn) = Pn\ni=1 ˘cl(˜uπ\ns )(ˆpi) and since f ≤C if and only if ˘cl(f) ≤C\nfor any function f and constant C, we obtain:\nvπ\nMα(ˆµn)(s) + κπ\ns α ≥1\nn\nn\nX\ni=1\n˜uπ\ns (ˆpi).\nRecalling that ˜uπ\ns (ˆpi) = vπ\nˆpi(s) enables to establish vπ\nMα(ˆµn)(s) ≥1\nn\nPn\ni=1 vπ\nˆpi(s) −κπ\ns α.\nClaim (ii).\nFor all p ∈M(S)|S|×|A|, let the norm ∥ps∥∞,1 := maxa∈A\nP\ns′∈S|p(s, a, s′) −p′(s, a, s′)| and\nLγ,β,Rmax :=\nβγRmax\n(1−γ)2 where β satisﬁes P\ns∈S∥ps∥∞,1 ≤β∥p∥. Also recall ˜uπ\ns (˜p): R|S|×|A|×|S| →R the continu-\nation function of uπ\ns deﬁned as ˜uπ\ns (˜p) := infp∈M(S)|S|×|A| uπ\ns (p) + Lγ,β,Rmax∥p −˜p∥, and Zπ := {z : (˜uπ\ns )∗(z) < ∞}.\nThen, we have κπ\ns ≤Lγ,β,Rmax.\nProof. We ﬁrst introduce the following result of Strehl & Littman (2008)[Lemma 1]:\nLemma B.2. For all p, p′ ∈M(S)|S|×|A|, π ∈Π and s ∈S, we have\n|vπ\np (s) −vπ\np′(s)| ≤\nγRmax∥ps −p′\ns∥∞,1\n(1 −γ)2\n.\nBy Lemma B.2, for all p, p′ ∈M(S)|S|×|A| we have\nuπ\ns (p′) −Lγ,β,Rmax∥p −p′∥≤uπ\ns (p) ≤uπ\ns (p′) + Lγ,β,Rmax∥p −p′∥,\nso uπ\ns is Lγ,β,Rmax-Lipschitz continuous over M(S)|S|×|A|, and by construction, so is its continuation function ˜uπ\ns over\nR|S|×|A|×|S|. Therefore, if we ﬁx a transition function p ∈M(S)|S|×|A| it holds that\n(˜uπ\ns )∗(z) :=\nsup\n˜p∈R|S|×|A|×|S| ˜uπ\ns (˜p) −⟨z, ˜p⟩\n≥\nsup\n˜p∈R|S|×|A|×|S| vπ\np (s) −Lγ,β,Rmax∥p −˜p∥−⟨z, ˜p⟩\n=\nsup\n˜p∈R|S|×|A|×|S| vπ\np (s) −\nsup\n∥x∥∗≤Lγ,β,Rmax\n⟨x, ˜p −p⟩−⟨z, ˜p⟩\n=\nsup\n˜p∈R|S|×|A|×|S|\ninf\n∥x∥∗≤Lγ,β,Rmax\nvπ\np (s) + ⟨x, p −˜p⟩−⟨z, ˜p⟩.\nDistributional Robustness and Regularization in Reinforcement Learning\nUsing the minimax theorem on the right hand side of the inequality, we obtain\n(˜uπ\ns )∗(z) ≥\ninf\n∥x∥∗≤Lγ,β,Rmax\nsup\n˜p∈R|S|×|A|×|S| vπ\np (s) + ⟨x, p⟩−⟨x + z, ˜p⟩\n= vπ\np (s) +\ninf\n∥x∥∗≤Lγ,β,Rmax\n⟨x, p⟩+\nsup\n˜p∈R|S|×|A|×|S|⟨−x −z, ˜p⟩\n=\ninf\n∥x∥∗≤Lγ,β,Rmax\nσR|S|×|A|×|S|(−x −z) + vπ\np (s) + ⟨x, p⟩\n=\n(\nvπ\np (s) −⟨z, p⟩if ∥z∥∗≤Lγ,β,Rmax\n∞otherwise.\nTherefore, the effective domain of (˜uπ\ns )∗is included in the ∥·∥∗-ball of radius Lγ,β,Rmax, which implies κπ\ns ≤Lγ,β,Rmax.\nB.2. Proof of Theorem 3.2\nTheorem (Linear Approximation Case). Let the WDRMDP ⟨S, A, r, Mα(ˆµn)⟩. Then, for any π ∈Π, s ∈S,\ninf\nµ∈Mα(bµn) Ep∼µ[Φ(s)⊤wp] ≥1\nn\nn\nX\ni=1\nΦ(s)⊤wbpi −ηπ\ns α,\nwhere ηπ\ns is a positive constant that depends on s and π.\nProof. The proof starts similarly as in Thm. 3.1[Claim (i)]. For all i ∈[n], deﬁne pi := N\ns∈S pi\ns, µi := N\ns∈S µi\ns, and\nthe average distribution ¯µ := 1\nn\nPn\ni=1 µi. Then, the worst-case distributionally robust value function can be expressed as\ninf\nµ∈Mα(bµn) Ep∼µ[Φ(s)⊤wp] = min\nµ Ep∼µ[Φ(s)⊤wp] s.t.\n\n\n\n\n\n\n\n\n\n\n\nµ =\nO\ns∈S\n \n1\nn\nn\nX\ni=1\nµi\ns\n!\n1\nn\nn\nX\ni=1\nEpi∼µi [∥pi −bpi∥] ≤α,\nfor an α determined by radii αs-s. Thus, replacing distribution µ by its constrained law and using a duality argument, we\nobtain\ninf\nµ∈Mα(bµn) Ep∼µ[Φ(s)⊤wp] =\ninf\nµ1,··· ,µn\nµi=N\ns µi\ns\nsup\nλ≥0\n \nEp∼¯µ[Φ(s)⊤wp] −λ\n \nα −1\nn\nn\nX\ni=1\nEpi∼µi [∥pi −bpi∥]\n!!\n.\nApplying the maxmin inequality and setting all µi-s to a Dirac distribution with full mass on the worst-case model yields\ninf\nµ∈Mα(bµn) Ep∼µ[Φ(s)⊤wp] ≥sup\nλ≥0\ninf\nµ1,··· ,µn\nµi=N\ns µi\ns\n \nEp∼¯µ[Φ(s)⊤wp] −λ\n \nα −1\nn\nn\nX\ni=1\nEpi∼µi [∥pi −bpi∥]\n!!\n= sup\nλ≥0\n1\nn\nn\nX\ni=1\ninf\nµi:µi=N\ns µi\ns\n\u0000Epi∼µi\n\u0002\nΦ(s)⊤wpi +λ∥pi −bpi∥\n\u0003\u0001\n−λα\n= sup\nλ≥0\n1\nn\nn\nX\ni=1\ninf\np∈M(S)|S|×|A|\n\u0000Φ(s)⊤wp +λ∥p −bpi∥\n\u0001\n−λα.\nWe introduce auxiliary variables x1, · · · , xn in order to reformulate the bound as\ninf\nµ∈Mα(bµn) Ep∼µ[Φ(s)⊤wp] ≥\nmax\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\ninf\np∈M(S)|S|×|A|\n\u0000Φ(s)⊤wp +λ∥p −bpi∥\n\u0001\n≥xi,\n∀i ∈[n]\nλ ≥0,\nDistributional Robustness and Regularization in Reinforcement Learning\nand since S is ﬁnite, M(S) is compact and the inﬁma in the ﬁrst-line constraints are minima.\nFor all i ∈[n], let the mapping fi : p 7→wπ\ns (p) + λ∥p −bpi∥. By applying Prop. 1.3.17. of (Bertsekas, 2009), we obtain\n˘cl(fi)(˜p) = ˘cl(˜uπ\ns )(˜p) + λ∥˜p −bpi∥, where we used the fact that ˜p 7→λ∥˜p −bpi∥is convex and closed for all λ ≥0. Thus,\napplying Prop.1.3.13. of (Bertsekas, 2009) on all fi-s yields\nmax\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\nmin\np∈M(S)|S|×|A|\n\u0000Φ(s)⊤wp +λ∥p −bpi∥\n\u0001\n≥xi,\n∀i ∈[n]\nλ ≥0\n=\nmax\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\nmin\np∈M(S)|S|×|A|\n\u0010\n˘cl(wπ\ns )(p) + λ∥p −bpi∥\n\u0011\n≥xi,\n∀i ∈[n]\nλ ≥0.\nMoreover, by deﬁnition of the dual norm ∥·∥∗, the right hand side of the inequality is equivalent to the following:\nmax\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\nmin\np∈M(S)|S|×|A|\nmax\n∥yi∥∗≤λ\n\u0010\n˘cl(wπ\ns )(p) + ⟨yi, p −bpi⟩\n\u0011\n≥xi,\n∀i ∈[n]\nλ ≥0,\nso the worst-case expectation can be reformulated as\ninf\nµ∈Mα(bµn) Ep∼µ[Φ(s)⊤wp] ≥\nmax\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\nmin\np∈M(S)|S|×|A|\nmax\n∥yi∥∗≤λ\n\u0010\n˘cl(wπ\ns )(p) + ⟨yi, p −bpi⟩\n\u0011\n≥xi,\n∀i ∈[n]\nλ ≥0.\nNow introduce the conjugate transform of ˘cl(wπ\ns ):\n˘cl(wπ\ns )∗(z) :=\nmax\np∈M(S)|S|×|A|\n\u0010\n⟨z, p⟩−˘cl(wπ\ns )(p)\n\u0011\n.\nBy Thm. A.1(d), ˘cl(wπ\ns )∗= (wπ\ns )∗.\nMoreover, since the feature vectors (Φ(s))s∈S are linearly independent, by\n(Bertsekas & Tsitsiklis, 1996)[Lemma 6.8.], wπ\ns is proper and closed. As a result, thanks to Lemma A.1, the convex\nclosure ˘cl(wπ\ns ) is proper. Therefore, by Thm. A.1(c), ˘cl(wπ\ns ) coincides with its bi-conjugate function and\n˘cl(wπ\ns )(p) = ˘cl(wπ\ns )∗∗(p) = max\nz∈Wπ\ns\n\u0010\n⟨z, p⟩−˘cl(wπ\ns )∗(z)\n\u0011\n= max\nz∈Wπ\ns\n(⟨z, p⟩−(wπ\ns )∗(z)) ,\nwhere Wπ\ns := {z : (wπ\ns )∗(z) < ∞} is the effective domain of (wπ\ns )∗. Thus, if we use the reformulation of the convex\nclosure and apply the minimax theorem (Bertsekas, 2009)[Prop. 5.5.4.] we obtain\nmin\np∈M(S)|S|×|A|\nmax\n∥yi∥∗≤λ\n\u0010\n˘cl(wπ\ns )(p) + ⟨yi, p −bpi⟩\n\u0011\n=\nmin\np∈M(S)|S|×|A| max\nzi∈Wπ\ns\nmax\n∥yi∥∗≤λ −(wπ\ns )∗(zi) + ⟨p, zi⟩+ ⟨yi, p −bpi⟩\n= max\nzi∈Wπ\ns\nmax\n∥yi∥∗≤λ\nmin\np∈M(S)|S|×|A| −(wπ\ns )∗(zi) + ⟨p, zi⟩+ ⟨yi, p −bpi⟩\n= max\nzi∈Wπ\ns\nmax\n∥yi∥∗≤λ −(wπ\ns )∗(zi) −⟨u, bpi⟩+\nmin\np∈M(S)|S|×|A|⟨p, zi + ui⟩\n= max\nzi∈Wπ\ns\nmax\n∥yi∥∗≤λ −(wπ\ns )∗(zi) −σM(S)|S|×|A|(−zi −ui) −⟨yi, bpi⟩,\nDistributional Robustness and Regularization in Reinforcement Learning\nwhere σM(S)|S|×|A| is the support function of M(S)|S|×|A|. We use the bound σM(S)|S|×|A| ≤σR|S|×|A|×|S| to deduce\nmin\np∈M(S)|S|×|A|\nmax\n∥yi∥∗≤λ\n\u0010\n˘cl(wπ\ns )(p) + ⟨yi, p −bpi⟩\n\u0011\n= max\nzi∈Wπ\ns\nmax\n∥yi∥∗≤λ −(wπ\ns )∗(zi) −⟨yi, bpi⟩−σM(S)|S|×|A|(−zi −ui)\n≥max\nzi∈Wπ\ns\nmax\n∥yi∥∗≤λ −(wπ\ns )∗(zi) −⟨yi, bpi⟩−σR|S|×|A|×|S|(−zi −ui)\n= max\nzi∈Wπ\ns\nmax\n∥zi∥∗≤λ −(wπ\ns )∗(zi) + ⟨zi, bpi⟩\n=\n(˘cl(wπ\ns )(ˆpi) if sup{∥zi∥∗: zi ∈Wπ\ns } ≤λ\n−∞otherwise.\nTherefore, recalling the notation ηπ\ns := sup{∥z∥∗: z ∈Wπ\ns }, we obtain\nsup\nλ,x1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n\n\n\nmin\np∈M(S)|S|×|A|\nmax\n∥yi∥∗≤λ\n\u0010\n˘cl(wπ\ns )(p) + ⟨yi, p −bpi⟩\n\u0011\n≥xi,\n∀i ∈[n]\nλ ≥0\n≥sup\nλ\nsup\nx1,··· ,xn\n1\nn\nn\nX\ni=1\nxi −λα s.t.\n(˘cl(wπ\ns )(ˆpi) ≥xi,\n∀i ∈[n]\nλ ≥ηπ\ns .\nPutting this altogether yields infµ∈Mα(bµn) Ep∼µ[Φ(s)⊤wp] ≥1\nn\nPn\ni=1 ˘cl(wπ\ns )(ˆpi) −ηπ\ns α. Now let F : (ˆp1, · · · , ˆpn) 7→\nPn\ni=1 wπ\ns (ˆpi). By Prop.1.3.17 of (Bertsekas, 2009), we have ˘cl(F)(ˆp1, · · · , ˆpn) = Pn\ni=1 ˘cl(wπ\ns )(ˆpi) and since f ≤C if\nand only if ˘cl(f) ≤C for any function f and constant C, we obtain:\ninf\nµ∈Mα(bµn) Ep∼µ[Φ(s)⊤wp] + ηπ\ns α ≥1\nn\nn\nX\ni=1\nwπ\ns (ˆpi),\nwhich ends the proof.\nC. Out-of-Sample Performance Guarantees\nC.1. Proof of Theorem 4.1\nThm. 4.1 is based on the following result, which is a direct consequence of (Fournier & Guillin, 2015)[Lemma 5;Prop. 10].\nLemma C.1. Let ǫ ∈(0, 1), m := |S| × |A| and ˆµs\nn ∈M([0, 1]m) be the empirical distribution at s ∈S. Then for all\nαs ∈(0, ∞)\nµn\ns ({ˆps : d(ˆµs\nn, µs) ≥c0βs}) ≤c1b1(ns, βs)1βs≤1,\nwhere b1(ns, βs) := exp(−c2ns · (βs)m∨2) and c0, c1, c2 are positive constants that only depend on m ̸= 2.\nThe positive constant c0 corresponds to the one that appears in (Fournier & Guillin, 2015)[Lemma 5]: it bounds the de-\ngree by which the Wasserstein distance is coarser than another one deﬁned in (Fournier & Guillin, 2015)[Notation 4] and\nbounded by 1. As a result, the Wasserstein diameter of M([0, 1]m) is bounded by c0 so in Lemma C.1, the probability\nvanishes when βs > 1.\nWe are now ready to prove Thm. 4.1. For completeness, we recall its statement below. The proof uses results from\n(Fournier & Guillin, 2015) and is based on (Yang, 2018). There, similar guarantees were established in a stochastic control\nsetting. Differently, our proof focuses on the MDP framework.\nTheorem (Finite-sample Guarantee). Let ǫ ∈(0, 1), m := |S| × |A|. Denote by ˆπ∗an optimal policy of the Wasserstein\nDRMDP ⟨S, A, r, Mα(n,ǫ)(bµn)⟩and ˆv∗its optimal value. If for all s ∈S the radius of the Wasserstein ball at s satisﬁes\nαs(ns, ǫ) :=\n\n\n\n\n\nc0 ·\n\u0012\n1\nnsc2\nlog\n\u0010c1\nǫ\n\u0011\u00131/(m∨2)\nif ns ≥Cǫ\nm\nc0\notherwise,\nDistributional Robustness and Regularization in Reinforcement Learning\nwith Cǫ\nm :=\n1\nc2 log\n\u0000 c1\nǫ\n\u0001\nand\nns :=\nX\ni∈[n],a∈A,s′∈S\nni(s, a, s′),\nthen it holds that\nµn \u0010n\nbp | Ep∼µ[vˆπ∗\np (s)] ≥vˆπ∗\nMα(ˆµn)(s),\n∀s ∈S\no\u0011\n≥1 −ǫ,\nwhere c0, c1, c2 are positive constants that only depend on m ̸= 2.\nProof. Set\nβs(ns, ǫ) :=\n\n\n\n\n\n\u0012\n1\nnsc2\nlog\n\u0010c1\nǫ\n\u0011\u00131/(m∨2)\nif ns ≥Cǫ\nm\notherwise,\nso we have c0 · βs(ns, ǫ) = αs(ns, ǫ) and Lemma C.1 ensures that the radius αs(ns, ǫ) provides the following guarantee:\nµn\ns ({ˆps : d(ˆµs\nn, µs) ≤αs(ns, ǫ)}) ≥1 −ǫ.\n(1)\nNow, we introduce the following operators\nTMα(ˆµ)v(s) := sup\nπs∈A\ninf\nµs∈Mαs (bµs) T πs\nµs v(s),\n∀s ∈S,\nwhere\nT πs\nµs v(s) : = r(s, πs) + γ\nZ\nps∈Ps\nX\ns′∈S\nv(s′)ps(s′|s, πs)dµs(ps),\nand consider ˆπ∗= (ˆπ∗\ns)s∈S an optimal policy of the Wasserstein DRMDP ⟨S, A, r, Mα(n,ǫ)(bµn)⟩3. By Equation (1), we\nhave the following one-step guarantee:\nµn\ns ({ˆps : T ˆπ∗\ns\nµs v(s) ≥TMα(ˆµn)v(s)}) ≥1 −ǫ.\nRemarking that T ˆπ∗\ns\nµs is a non-decreasing γ-contraction w.r.t. the sup-norm (see (Chen et al., 2019)[Lemma 4.2.]), we show\nby induction on k ≥1 that if µs ∈Mαs(ns,ǫ)(ˆµs\nn), then (T ˆπ∗\ns\nµs )kv(s) ≥(TMα(ˆµn))kv(s). By deﬁnition of TMα(ˆµn), we\nhave T ˆπ∗\ns\nµs v(s) ≥TMα(ˆµn)v(s). Supposing that the condition holds for an arbitrary k ≥1, we have\n(T ˆπ∗\ns\nµs )k+1v(s) = T ˆπ∗\ns\nµs ((T ˆπ∗\ns\nµs )kv)(s) ≥T ˆπ∗\ns\nµs ((TMα(ˆµn))k)v(s) ≥TMα(ˆµ)(TMα(ˆµn))kv(s),\nso the induction assumption holds for all k ≥1. Using the contracting property of T ˆπ∗\ns\nµs , we have limk→∞(T ˆπ∗\ns\nµs )kv(s) =\nEp∼µ[vˆπ∗\np (s)] and by applying (Chen et al., 2019)[Thm. 4.5.], we obtain limk→∞(TMα(ˆµ))kv(s) = vˆπ∗\nMα(ˆµn)(s). There-\nfore, setting k →∞, if µs ∈Mαs(ns,ǫ)(ˆµs\nn), then Ep∼µ[vˆπ\np (s)] ≥vˆπ∗\nMα(ˆµn)(s) and the following probabilistic guarantee\nholds for all s ∈S:\nµn\ns ({ˆps : Ep∼µ[vˆπ\np (s)] ≥vˆπ∗\nMα(ˆµn)(s)}) ≥1 −ǫ,\nwhich can be rewritten as\nµn\ns ({ˆps : Ep∼µ[vˆπ\np (s)] < vˆπ∗\nMα(ˆµn)(s)}) ≤ǫ.\nThe independence structure µn = N\ns∈S µn\ns enables to obtain\nµn({ˆp : Ep∼µ[vˆπ\np (s)] < vˆπ∗\nMα(ˆµn)(s),\n∀s ∈S}) ≤\nY\ns∈S\nǫ ≤ǫ,\nwhich concludes the proof, by taking the complementary event.\n3Although this work is restricted to the set of deterministic policies, we do not lose generality as long as P is (s, a)-rectangular\nbecause then, one can ﬁnd an optimal policy that is stationary deterministic (Wiesemann et al., 2013).\n",
  "categories": [
    "math.OC",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-03-05",
  "updated": "2020-07-14"
}