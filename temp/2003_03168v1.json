{
  "id": "http://arxiv.org/abs/2003.03168v1",
  "title": "Lane-Merging Using Policy-based Reinforcement Learning and Post-Optimization",
  "authors": [
    "Patrick Hart",
    "Leonard Rychly",
    "Alois Knol"
  ],
  "abstract": "Many current behavior generation methods struggle to handle real-world\ntraffic situations as they do not scale well with complexity. However,\nbehaviors can be learned off-line using data-driven approaches. Especially,\nreinforcement learning is promising as it implicitly learns how to behave\nutilizing collected experiences. In this work, we combine policy-based\nreinforcement learning with local optimization to foster and synthesize the\nbest of the two methodologies. The policy-based reinforcement learning\nalgorithm provides an initial solution and guiding reference for the\npost-optimization. Therefore, the optimizer only has to compute a single\nhomotopy class, e.g.\\ drive behind or in front of the other vehicle. By storing\nthe state-history during reinforcement learning, it can be used for constraint\nchecking and the optimizer can account for interactions. The post-optimization\nadditionally acts as a safety-layer and the novel method, thus, can be applied\nin safety-critical applications. We evaluate the proposed method using\nlane-change scenarios with a varying number of vehicles.",
  "text": "Lane-Merging Using Policy-based Reinforcement Learning and\nPost-Optimization\nPatrick Hart1, Leonard Rychly1 and Alois Knoll2\nc⃝2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing\nthis material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of\nthis work in other works\nAbstract— Many current behavior generation methods strug-\ngle to handle real-world trafﬁc situations as they do not scale\nwell with complexity. However, behaviors can be learned off-line\nusing data-driven approaches. Especially, reinforcement learn-\ning is promising as it implicitly learns how to behave utilizing\ncollected experiences. In this work, we combine policy-based\nreinforcement learning with local optimization to foster and\nsynthesize the best of the two methodologies. The policy-based\nreinforcement learning algorithm provides an initial solution\nand guiding reference for the post-optimization. Therefore, the\noptimizer only has to compute a single homotopy class, e.g.\ndrive behind or in front of the other vehicle. By storing the\nstate-history during reinforcement learning, it can be used\nfor constraint checking and the optimizer can account for\ninteractions. The post-optimization additionally acts as a safety-\nlayer and the novel method, thus, can be applied in safety-\ncritical applications. We evaluate the proposed method using\nlane-change scenarios with a varying number of vehicles.\nI. INTRODUCTION\nAs autonomous driving strives towards level-ﬁve auton-\nomy novel behavior generation methods have to be developed\nto tackle the remaining challenges. One of these remaining\nchallenges is to plan behaviors for complex trafﬁc scenarios\nhaving multiple trafﬁc participants.\nData-driven methods, such as policy-based reinforcement\nlearning can learn how to behave in complex situations\nutilizing collected experiences. However, since the whole\nconﬁguration space cannot be explored during training and\nmost approaches use deep artiﬁcial neural networks, the\nreinforcement learning algorithm might fail to interpolate\nand to generalize well [1]. This can lead to unwanted\nand unexpected behaviors making reinforcement learning\ninfeasible to use in safety-critical applications.\nContrary to this, conventional approaches, such as opti-\nmization and search-based techniques provide optimal and\nsafe solutions given the problem is well deﬁned. However,\nthese approaches might fail to handle complex situations in\nreal-time since they do not scale well with the increasing\ncomplexity of the environment. A local non-linear optimizer\nwould have to solve multiple homotopy classes to ﬁnd the\nglobal optimum. Figure 1 shows a scenario where a local\noptimizer would have to evaluate at least two combinatorial\noptions: merge behind or in front of the red vehicle.\nThe main contributions of this work are the following:\n1Patrick Hart\nand Leonard\nRychly are\nwith the\nfortiss GmbH,\nAn-Institut Technische Universit¨at M¨unchen, Munich, Germany. Email:\npatrick.hart@tum.de, rychly@fortiss.org\n2Alois Knoll is with the Chair of Robotics, Artiﬁcial Intelligence and\nReal-time Systems, Technische Universit¨at M¨unchen, Munich, Germany\nFig. 1: The blue vehicle wants to merge onto the highway.\nIn this scenario, two homotopy classes exist – merge in front\nof or behind the red vehicle.\n• Obtain a behavior for complex trafﬁc situations using\nreinforcement learning,\n• use this behavior to choose a homotopy class and to\ninclude interactions in the optimization and,\n• make the reinforcement learning algorithm combined\nwith the post-optimization feasible in safety-critical\napplications.\nIn this work, we combine policy-based reinforcement\nlearning with local non-linear optimization to synthesize\nthe best of the two methodologies. The learned policy of\nthe reinforcement learning algorithm is iteratively executed\nin the environment to generate an initial estimate and a\nguiding reference trajectory for the optimization. The state\nhistory of all vehicles is stored and later used for constraint\nchecking in the optimization. Since all vehicles react to their\nsurroundings according to their intrinsic behavior policy, the\ninteractions between the trafﬁc participants are implicitly\nincluded in the optimization. Additionally, due to the initial\nestimate proposed by the reinforcement learning algorithm,\nthe optimizer only has to solve a single homotopy class\n– making the optimization also feasible for very complex\ntrafﬁc situations. Moreover, the post-optimization acts as\nan additional safety-layer after the reinforcement learning\nalgorithm. Therefore, a high level of safety and comfort\nis guaranteed and the novel approach is also applicable in\nsafety-critical applications.\nWe demonstrate and evaluate our novel approach using\ninteractive lane-merging scenarios having a varying number\nof vehicles. All vehicles in the simulation framework have\na behavior policy, such as the Intelligent Driver Model [2,\n3]. The ego-vehicle is controlled by the novel behavior\ngeneration method presented in this work.\nThe work is further organized as follows: Section II\narXiv:2003.03168v1  [cs.LG]  6 Mar 2020\ngives a quick overview of state-of-the-art algorithms used\nin behavior generation with a special focus on reinforcement\nlearning. Next, a problem deﬁnition for this work is provided.\nIn Section IV, our combined reinforcement and optimization\napproach is introduced. Section V evaluates the performance\nof the novel method using a lane-merging scenario. Finally,\na conclusion, discussion, and an outlook are provided in\nSection VI.\nII. RELATED WORK\nThis section gives a brief overview of state-of-the-art\nreinforcement learning methods – with a special focus on\nbehavior generation for autonomous driving. Moreover, a\nshort overview of optimization-based techniques to generate\nbehaviors for autonomous vehicles is given.\nReinforcement learning can roughly be divided into two\ncategories: policy- and value-based methods [4]. In this work,\nwe focus on policy-based reinforcement learning since it\nachieves state-of-the-art performance in large conﬁguration\nspaces and dynamic control problems [5, 6]. In the determin-\nistic case, policy-based methods map states to actions and in\nthe stochastic case, they create distributions over actions for\neach state.\nThe Trust Region Policy Optimization (TRPO) is a policy-\nbased method that additionally restrains the next policy to\nbe close to the previous one by using the Kullback-Leibler\n(KL) divergence [7]. Based on this the Proximal Policy\nOptimization (PPO) has been developed that also restricts the\nobjective function, but by using a clipping function instead of\nusing the KL divergence [8]. This is computationally more\nefﬁcient than the TRPO and yields equally good or better\nresults in a wide range of applications [9, 10].\nHowever, the above-presented methods explore on-policy\nand are, thus, more likely to become stuck in local minima.\nRecently, policy-based methods have been suggested that\nexplore off-policy and, therefore, mitigate this problem, such\nas the Maximum a Posteriori Policy Optimization (MPO) [6]\nand the Soft Actor-Critic (SAC) [11]. To leverage the off-\npolicy exploration advantage, we use the SAC algorithm in\nthis work.\nReinforcement learning has been successfully applied to\ngenerate behaviors for autonomous vehicles. Shalev-Shwartz\net al. [12] use deep reinforcement learning to ﬁnd long\nterm driving strategies. Other than using an option-graph,\nthey showed that the long-term driving problem can be\nsolved without assuming the Markov property. However, as\nmost of the reinforcement learning methods use deep neural\nnetworks, they cannot be simply applied in safety-critical\napplications as the interpolation and generalization of these\ncannot be guaranteed [1].\nTherefore, reinforcement learning has been combined with\nformal veriﬁcation to ensure that only safe actions are exe-\ncuted by the reinforcement learning algorithm [13]. However,\ndue to the curse of dimensionality in formal veriﬁcation,\nthese methods become untractable fast with a growing action\nand conﬁguration space [14].\nAnother approach trying to make reinforcement learning\nsafe and applicable was introduced by Levine et al. [15].\nThey use differential dynamic programming in order to gen-\nerate suitable guiding samples to direct policy learning and\nto avoid poor local optima. However, with the characteristic\nof neural networks having a large number of local optima,\nunwanted behavior still can occur and generalization cannot\nbe guaranteed [16].\nContrary to data-driven reinforcement learning techniques,\noptimization-based behavior generation methods provide safe\nand optimal solutions. These are, therefore, already used\nin safety-critical applications, such as autonomous driving.\nAs shown by Bender et al. [17] all combinatorial options\n(homotopy classes) have to be evaluated in order to ﬁnd the\nglobal optimum when using local optimization. Therefore,\nthese methods generally do not scale well with the increasing\ncomplexity of the environment and become infeasible for\nreal-time applications. In this work, we combine reinforce-\nment learning to propose an initial solution and, thus, also\na homotopy class for the optimizer. Therefore, the optimizer\nonly has to solve a single homotopy and local-optimization\ntechniques become feasible in very complex trafﬁc scenarios.\nIII. PROBLEM STATEMENT\nThe proposed behavior generation method consists of two\nmain components:\n1) An\noff-line\nstochastic\npolicy-based\nreinforcement\nlearning method and,\n2) a local non-linear post-optimization.\nThe stochastic policy-based reinforcement learning provides\nan approximately optimal solution for the Markov Deci-\nsion Process (MDP). This is achieved by interacting with\nan environment, collecting experiences and improving its\nstochastic policy πφ parametrized by an artiﬁcial neural\nnetwork having the parameters φ. This policy is then iter-\natively executed to generate a state-space trajectory xRL =\n[x0, . . . , xN] and a control input sequence aRL = [a0, . . . , aN]\nfor the ego-vehicle having N time-steps. The other vehi-\ncles behave according to their deﬁned policies [π0, . . . , πM]\nfor M other vehicles. All agent’s state-space trajectories\n[xother,1, . . . , xother,M] are stored as they are later used for\nconstraint checking in the optimization.\nThe second part consists of the local non-linear post-\noptimization that uses the generated state-space trajectory\nxRL and the control input sequence aRL as a guiding\nreference and initial optimization vector, respectively. Due to\nthis, a homotopy class is implicitly chosen for the optimizer\nand only a single homotopy has to be evaluated by the local-\noptimization method to ﬁnd the globally optimal control\ninput sequence a∗= [a0, . . . , aN] that produces the state-\nspace trajectory x∗= [x0, . . . , xN]. Constraints and interac-\ntive behavior with other trafﬁc participants are integrated into\nthe optimization using the recorded state-space trajectories\n[xother,1, . . . , xother,M].\nIV. OUR METHOD\nIn this section, we introduce the used policy-based re-\ninforcement learning and how we generate behaviors for\nautonomous vehicles. Moreover, we then go into detail of\nthe post-optimization and how it is combined with the policy-\nbased reinforcement learning algorithm.\nA. Policy-based Reinforcement Learning\nThis section describes the stochastic policy-based rein-\nforcement learning and how the environment’s rewards are\ndeﬁned. In the reinforcement learning problem, we try to op-\ntimize the expected cumulative future reward for the stochas-\ntic policy denoted by πφ parametrized by the neural network\nparameters φ. The basic objective function expressing the\nexpected cumulative future reward can be mathematically\ndenoted as\nR =\nX\nt\nE(st,at)∼πφ[r(st, at)]\n(1)\nwith r(·) being the reward function, st the concatenated state-\nspace and at the action of the ego-vehicle. However, in this\nwork, we use an extended maximum entropy objective as\nintroduced by Haarnoja et al. [11].\nThe rewards of the environment are designed to avoid\ncrashes, provide a comfortable driving experience and to\nfollow a given reference with a desired speed. Derived from\nthese characteristics the reward function is deﬁned as\nRcost = Rref + Rcomfort + Rvel + Rcol\n(2)\nwith Rref being the deviation to the reference path, Rcomfort\nrating the jerk of the trajectory, Rvel being the deviation to\nthe desired velocity and Rcol the penalty for collisions. All\nrewards besides the collision-term are modeled as\nR = rmax −∆2\ndesired\n(3)\nwith ∆2\ndesired being the squared deviation to a desired value,\nsuch as the speed or reference path and rmax being the\nmaximum achievable reward. Equation 3 will assume its\nmaximum value if the deviation to the desired values is zero.\nThe penalty for a collision Rcol is selected to be a large\nnegative scalar value.\nThe input state consists of the concatenated states of all\nvehicles st = [x0, . . . , xM] with M being the number of\nvehicles, t the time-index and xi the state of the i-th vehicle.\nOn the output layer of the policy network, we utilize a normal\ndistribution πφ(st) ∼N(·) whose mean is scaled to limit the\noutput space. For scaling the mean to a deﬁned range of\nminimum and maximum control values (amin and amax), we\nuse a hyperbolic tangent scaling function that can be denoted\nas\nat = (amax + amin)\n2\n+ (amax −amin)\n2\ntanh πφ(st).\n(4)\nAfter training has been concluded, the policy πφ(st) is\ngreedily executed iteratively N-times in order to create the\nstate-space trajectory xRL = [x0, . . . , xN] and the control\ninput sequence aRL = [a0, . . . , aN−1]. During the greedy ex-\necution of the stoachastic policy πφ the maximum values of\nthe distribution are being used in each time-step maximizing\nEquation 1. All used hyper-parameters, reward values and\nother characteristics are explained in detail in Section V.\nB. Non-linear Post-Optimization\nThe non-linear optimization utilizes the behavior policy of\nthe reinforcement learning algorithm to obtain an initial opti-\nmization vector and a guiding reference. With the assumption\nthat reinforcement learning approximates an optimal solution\nfor the Markov Decision Process (MDP), the optimized\nsolution is enforced to be close to it. Thus, a homotopy class\nfor the optimization is implicitly selected and also complex\nplanning problems become feasible to be solved online\nand in real-time using local optimization. Additionally, the\noptimization checks constraints and smoothens the control\ninputs aRL of the reinforcement learning.\nThe optimization problem can be mathematically denoted\nas\nminimize J(x)\n(5)\nsubject to ˙x(t) = F(x(t), a(t))\n(6)\nwith J being the objective function and F(x(t), u(t))\nthe dynamic model of the ego-vehicle. The dynamic model\n˙x(t) = F(x(t), a(t)) is forward integrated N-times and\na state-space trajectory xOpt. is obtained having the same\nlength as xRL. Thus, a dynamically feasible state-space\ntrajectory is guaranteed. In this work, we use a simpliﬁed\nsingle track model as presented in [18]. The function J(x)\nis deﬁned as\nJ(x) = Jjerk + Jcol +\nN\nX\ni=0\n(xOpt.\ni\n−xRL\ni\n)\n(7)\nwith Jjerk being the squared jerk cost of the optimized\ntrajectory xOpt., Jcol being the collision cost and the last\nterm being the distance of the optimized trajectory to the\nreinforcement learning algorithm’s solution.\nFig. 2: State-space trajectory provided by the reinforcement\nlearning algorithm and the trajectory generatey by the post-\noptimization.\nCollisions with other vehicles, as well as with boundaries,\nare included in the collision term Jcol. The collision term\nuses the state history of all agents [xother,1, . . . , xother,M]\nthat have been recorded stepping the environment during\nreinforcement learning. Thus, the interaction with other\nvehicles is implicitly integrated into the local optimization\nmethod. Collision costs are imposed as soon as the ego-\nvehicle crosses a pre-deﬁned safety-distance threshold.\nThe optimization outputs an optimal control input se-\nquence a∗\n=\n[a0, . . . , aN] that generates the optimal\nstate-space trajectory x∗= [x0, . . . , xN]. After the post-\noptimization has ﬁnished an additional collision-check is\nperformed to guarantee a high level of safety.\nV. RESULTS AND EVALUATION\nIn this section, we ﬁrst present the used simulation\nframework and scenario chosen for training, applying and,\nevaluating the novel approach. Next, details of the used\nstochastic policy-based reinforcement learning algorithm –\nthe Soft Actor-Critic (SAC) – are presented. Finally, we\nshow and discuss the results of the novel approach that\ncombines the reinforcement learning algorithm with the post-\noptimization.\nA. Simulation and Scenario\nA semantic simulation framework capable of simulating\nmultiple agents has been developed and released in the\ncourse of this work [19]. The framework is capable of\nsimulating structured and unstructured environments and,\nthus, enables simulating a wide range of scenarios. Each\nagent in the environment is either controlled internally using\na behavior model or can be controlled externally by providing\nsystem inputs. In this work, all agents but the ego-agent are\ncontrolled internally and drive with a constant velocity or\nbehave according to the Intelligent Driver Model (IDM). The\nego-vehicle is controlled externally and uses a single-track\nmodel that requires the steering angle and acceleration as\nsystem inputs (a = [δ, a]).\nThe initial states of all agents are uniformly sampled and,\nthus, the initial position, velocity, and angle in every episode\nare varied according to the speciﬁed range. In this work,\nwe sample the speed in a range of v ∈[4m/s, 6m/s]. The\ndesired speed for all agents is set to 5m/s. Furthermore,\neach agent has a pre-deﬁned reference path that implicitly\nmotivates each agent’s goal.\nThe simulation framework is executed in an episodical\nfashion. An episode is counted as successful once the max-\nimum number of steps has been reached without the ego-\nvehicle causing a collision with any other object. Further-\nmore, an observer assigns rewards to all events that occur\nduring simulation and also calculates a concatenated state-\nspace st = [(x, y, vx, vy)0, . . . , (x, y, vx, vy)M] with M being\nthe number of vehicles. In this work, the concatenated state\nspace includes the Cartesian coordinates x, y as well as the\nvelocities vx, vy of each vehicle. The rewards are designed to\navoid collisions, generate a comfortable driving behavior and\nto be close to the original reinforcement learning solution. To\nmodel the aforementioned characteristics, the reward values\nare set to\n• rmax,vel = 10 for the deviation to the desired velocity,\n• rmax,ref = 10 for the deviation to the reference path,\n• rcol = −100 for collisions with other objects and,\n• rjerk = −0.1 for the jerk of the trajectory.\nSince rmax,vel and rmax,ref are the only positive rewards, the\nmaximum achievable reward per epidose can be calculated\nusing rtotal = PN\ni=0 rmax,vel + rmax,ref with N being the\nnumber of steps in a single episode. With the rewards pre-\nsented above, the maximum achievable reward per episode\nwith a hundred steps is rtotal = 2000. However, if there is\nan initial deviation to the reference path or to the desired\nvelocity, the maximum achievable reward is smaller.\nB. Policy-based Reinforcement Learning\nIn this work, we use the Soft Actor-Critic (SAC) policy-\nbased reinforcement learning algorithm. It uses a distribu-\ntional actor neural network having three layers (512, 512,\n128) and a deterministic critic network also having three\nlayers (256, 256, 128). The distributional network uses a\nnormal distribution on the output layer and all other layers\nuse the standard ReLU-activation function. In order to limit\nthe input space for the dynamic model to feasible ranges, we\nlimit the steering-angle δ ∈[−0.2, 0.2] and the acceleration\na ∈[−1, 1]. To evaluate and quantify the results obtained\nby the SAC algorithm, we train and evaluate the following\nscenarios:\n1) Two-lane merging having two other vehicles,\n2) two-lane merging having three other vehicles and,\n3) highway merging having ﬁve other vehicles.\nFigure 3, shows the average reward obtained during train-\ning plotted over the episode numbers. It can be seen, that\nthe reward increases for all of the above-presented scenarios\nand that the algorithm can handle all scenarios well.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nEpisodes [N]\n0\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nReward\nTwo lanes; Three vehicles\nTwo lanes; Four vehicles\nHighway merging\nFig. 3: The average rewards obtained during training by the\nSAC algorithm.\nQuantitative results of applying the SAC algorithm to the\nabove-presented scenarios are shown in Table I. As expected\nthe agent performs best, the less complex the scenario is.\nDespite this, the SAC algorithm is able to perform well\nin more complex scenarios, such as the highway merging\nscenario having many vehicles and multiple lanes.\nIn Figure 5, a spatial plot of the highway scenario is shown\nwith the ego-vehicle being controlled by the policy-based\nScenario\nSuccess-rate [%]\nAvg. Reward\nTwo-lane merging (Three vehicles)\n99.99 %\n1816.70\nTwo-lane merging (Four vehicles)\n97.31 %\n1827.36\nHighway merging\n96.05 %\n1722.91\nTABLE I: For evaluation, the scenarios were run 1, 000\ntimes.\nreinforcement learning algorithm (SAC) and the other agents\nusing the Intelligent Driver Model (IDM) as behavior policy.\nFig. 4: Resulting behavior produced by the policy-based\nreinforcement learning approach is depicted in blue. All other\nvehicles behaving according to the IDM are shown in gray.\nThe inference time executing the actor-network a single\ntime on an i7-6700HQ processor having 2.60GHz and using\nUbuntu 16.04 takes on average about 2.24ms. The next sub-\nsection will present the results of combining the policy-based\nreinforcement learning with the post-optimization method.\nC. Non-linear Post-Optimization\nThe results of the novel approach combining policy-\nbased reinforcement learning with a post-optimization are\npresented and discussed in this sub-section. We use two\nof the above-presented scenarios for the evaluation – the\ntwo-lane with four vehicles and the highway scenario. The\noptimizer we use in this work is an unconstrained least-\nsquares solver [20].\nIn Figure 5, a spatial plot of the highway scenario is\nshown. The reinforcement learning algorithm’s solution is\ndepicted in blue and the optimized solution in green. Due\nto the additional safety-distance in the optimization, the\ntrajectory is pushed further away from the other vehicle. The\ncontrol input sequence produced by the learned policy of\nthe reinforcement learning algorithm as well as the control\ninput sequence of the optimization is shown in Figure 6.\nDue to the additional jerk term in the objective function\nof the optimization, the resulting input sequences of the\noptimization are smoothened.\nQuantitative results of the novel approach are shown in\nTable II. Due to the optimizer pushing the ego-vehicle away\nFig. 5: The reinforcement learning algorithm’s solution is\ndepicted in blue and the optimizer’s solution in green. The\noptimizer pushes the trajectory away from the other vehicle\nand additionally smoothens the trajectory.\nfrom other vehicles and boundaries, the collision rate drops\nto zero for the give time-horizon. Additionally, the jerk\nvalues are reduced and, thus, the comfort of the passengers\nis increased signiﬁcantly. In case of a non-convergence of\nthe optimizer, an emergency maneuver, such as emergency-\nbraking could be performed.\n0\n20\n40\n60\n80\n0.8\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\nSAC Acceleration\nOpt. Acceleration\nSAC Steering-Angle\nOpt. Steering-Angle\nFig. 6: The control input sequences produced by the rein-\nforcement learning algorithm and the optimization.\nAs shown in the previous sub-section, the inference-time\nof the policy-network is on average about 2.24ms. Therefore,\nit would take around 112ms to produce a trajectory having\n50 time-steps. Contrary to this it takes the non-linear local\noptimizer on average about 206ms to solve a single homo-\ntopy class with N = 40 time-steps [18]. Thus, choosing a\nhomotopy class using reinforcement learning and obtaining\nan approximately optimal solution for the Markov decision\nprocess is more efﬁcient than solving multiple homotopy\nScenario\nJerk. Opt./Jerk SAC [%]\nJerk Opt.\nTwo-lane merging\n0.026\n9.7\nHighway merging\n0.034\n7.75\nTABLE II: Results of the novel approach applied to the two-\nlane and highway scenario.\nclasses. Furthermore, by using Graphics Processing Units\n(GPUs) the speed of both methods could be further improved\n– during training as well as during application.\nVI. CONCLUSION\nIn this work, we showed that a stochastic policy-based\nreinforcement learning algorithm is able to solve complex\nscenarios, such as merging lanes with multiple other vehicles\npresent. It learns implicitly and without any prior knowl-\nedge how to behave and which homotopy class to choose\nbased on its collected experiences. In order to improve\nthe solution provided by the reinforcement learning and to\nmake it applicable in safety-critical applications, we used\na non-linear post-optimization. By using the solution of\nthe reinforcement learning algorithm in the optimization, a\nhomotopy class is implicitly chosen. Thus, only a single\nobjective has to be solved using local optimization instead\nof all in order to ﬁnd the global optimum. Moreover, the\ncomfort of the reinforcement learning algorithm’s solution\nis improved due to the post-optimization. Furthermore, the\noptimizer’s convergence can be evaluated for the use in\nsafety-critical applications.\nIn future research should be conducted on the general-\nization capabilities of the reinforcement learning to novel\nscnearios and to a varying number of vehicles. Moreover,\nocclusions and uncertainties could be modeled in the envi-\nronment as well as more sophisticated behaviors of the other\nagents.\nREFERENCES\n[1]\nJustin A. Boyan and Andrew W. Moore. “General-\nization in Reinforcement Learning: Safely Approxi-\nmating the Value Function”. In: Advances in Neural\nInformation Processing Systems 7. 1994, pp. 369–376.\n[2]\nRajesh Rajamani. Vehicle dynamics and control.\nSpringer Science & Business Media, 2011.\n[3]\nMartin Treiber, Ansgar Hennecke, and Dirk Helbing.\n“Congested trafﬁc states in empirical observations and\nmicroscopic simulations”. In: Physical review E 62.2\n(2000), p. 1805.\n[4]\nKai Arulkumaran et al. “A Brief Survey of Deep\nReinforcement Learning”. In: CoRR abs/1708.05866\n(2017).\n[5]\nTimothy P. Lillicrap et al. “Continuous control with\ndeep reinforcement learning”. In: 4th International\nConference on Learning Representations. 2016.\n[6]\nAbbas Abdolmaleki et al. “Maximum a Posteriori\nPolicy Optimisation”. In: 6th International Conference\non Learning Representations. 2018.\n[7]\nJohn Schulman et al. “Trust Region Policy Optimiza-\ntion”. In: Proceedings of the 32nd International Con-\nference on Machine Learning. 2015, pp. 1889–1897.\n[8]\nJohn Schulman et al. “Proximal Policy Optimization\nAlgorithms”. In: CoRR abs/1707.06347 (2017).\n[9]\nMartin Klissarov et al. “Learnings Options End-\nto-End for Continuous Action Tasks”. In: CoRR\nabs/1712.00004 (2017).\n[10]\nNicolas\nHeess\net\nal.\n“Emergence\nof\nLocomo-\ntion Behaviours in Rich Environments”. In: CoRR\nabs/1707.02286 (2017).\n[11]\nTuomas Haarnoja et al. “Soft Actor-Critic: Off-Policy\nMaximum Entropy Deep Reinforcement Learning\nwith a Stochastic Actor”. In: Proceedings of the 35th\nInternational Conference on Machine Learning, ICML\n2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-\n15, 2018. 2018, pp. 1856–1865.\n[12]\nShai Shalev-Shwartz, Shaked Shammah, and Amnon\nShashua. “Safe, Multi-Agent, Reinforcement Learning\nfor Autonomous Driving”. In: CoRR abs/1610.03295\n(2016).\n[13]\nBranka Mirchevska et al. “High-level Decision Mak-\ning for Safe and Reasonable Autonomous Lane\nChanging using Reinforcement Learning”. In: 21st\nInternational Conference on Intelligent Transportation\nSystems. 2018, pp. 2156–2162.\n[14]\nNikolaus Correll. Distributed Autonomous Robotic\nSystems: The 14th International Symposium. Springer,\n2019, pp. 266–267.\n[15]\nSergey Levine and Vladlen Koltun. “Guided Policy\nSearch”. In: Proceedings of the 30th International\nConference on Machine Learning. 2013, pp. 1–9.\n[16]\nGrzegorz Swirszcz, Wojciech Marian Czarnecki, and\nRazvan Pascanu. “Local minima in training of neu-\nral networks”. In: arXiv preprint arXiv:1611.06310\n(2016).\n[17]\nPhilipp Bender et al. “The combinatorial aspect of\nmotion planning: Maneuver variants in structured en-\nvironments”. In: 2015 IEEE Intelligent Vehicles Sym-\nposium. 2015, pp. 1386–1392.\n[18]\nPatrick Hart and Alois Knoll. “Kinodynamic Mo-\ntion Planning Using Multi-Objective optimization”.\nIn: 2018 IEEE Intelligent Vehicles Symposium. 2018,\npp. 1185–1190.\n[19]\nPatrick Hart. Divine-RL: an autonomous driving envi-\nronment for applied reinforcement learning. https:\n//github.com/patrickhart/divine- rl.\n2019.\n[20]\nSameer Agarwal, Keir Mierle, et al. Ceres Solver.\nhttp://ceres-solver.org.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA",
    "cs.RO"
  ],
  "published": "2020-03-06",
  "updated": "2020-03-06"
}