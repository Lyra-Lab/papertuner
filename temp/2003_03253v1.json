{
  "id": "http://arxiv.org/abs/2003.03253v1",
  "title": "Introduction to deep learning",
  "authors": [
    "Lihi Shiloh-Perl",
    "Raja Giryes"
  ],
  "abstract": "Deep Learning (DL) has made a major impact on data science in the last\ndecade. This chapter introduces the basic concepts of this field. It includes\nboth the basic structures used to design deep neural networks and a brief\nsurvey of some of its popular use cases.",
  "text": "Introduction to deep learning\nLihi Shiloh-Perl and Raja Giryes\nAbstract Deep Learning (DL) has made a major impact on data science in the last\ndecade. This chapter introduces the basic concepts of this ﬁeld. It includes both the\nbasic structures used to design deep neural networks and a brief survey of some of\nits popular use cases.\n1 General overview\nNeural Networks (NN) have revolutionized the modern day-to-day life. Their sig-\nniﬁcant impact is present even in our most basic actions, such as ordering products\non-line via Amazon’s Alexa or passing the time with on-line video games against\ncomputer agents. The NN eﬀect is evident in many more occasions, for example,\nin medical imaging NNs are utilized for lesion detection and segmentation [40, 5],\nand tasks such as text-to-speech [38, 120] and text-to-image [101] have remarkable\nimprovements thanks to this technology. In addition, the advancements they have\ncaused in ﬁelds such as natural language processing (NLP) [24, 144, 77], optics\n[114, 42], image processing [110, 143] and computer vision (CV) [10, 34] are aston-\nishing, creating a leap forward in technology such as autonomous driving [13, 79],\nface recognition [109, 134, 23], anomaly detection [64], text understanding [54] and\nart [35, 53], to name a few. Its inﬂuence is powerful and is continuing to grow.\nThe NN journey began in the mid 1960’s with the publication of the Perceptron\n[105]. Its development was motivated by the formulation of the human neuron\nactivity [80] and research regarding the human visual perception [49]. However,\nquite quickly, a deceleration in the ﬁeld was experienced, which lasted for almost\nthree decades. This was mainly the result of lack of theory with respect to the\npossibility of training the (single-layer) perceptron and a series of theoretical results\nSchool\nof\nElectrical\nEngineering,\nTel\nAviv\nUniversity,\ne-mail:\n{lihishiloh@mail\n,raja@tauex}.tau.ac.il\n1\narXiv:2003.03253v1  [cs.LG]  29 Feb 2020\n2\nLihi Shiloh-Perl and Raja Giryes\nthat emphasized its limitations, where the most remarkable one is its inability to\nlearn the XOR function [82].\nThis NN ice age came to a halt in the mid 1980’s, mainly with the introduction\nof the multi-layer perceptron (MLP) and the backpropagation algorithm [107]. Fur-\nthermore, the revolutionary convolutional layer was presented [68], where one of its\nnotable achievements was successfully recognizing hand-written digits [67].\nWhile some other signiﬁcant developments have happened in the following\ndecade, such as the development of the long-short memory machine (LSTM) [46],\nthe ﬁeld experienced another deceleration. Questions were arising with no adequate\nanswers especially with respect to the non-convex nature of the used optimization ob-\njectives, overﬁtting the training data, and the challenge of vanishing gradients. These\ndiﬃculties led to a second NN winter, which lasted two decades. In the meantime,\nclassical machine learning techniques were developed and attracted much academic\nand industry attention. One of the prominent algorithms was the newly proposed\nSupport Vector Machine (SVM) [17], which deﬁned a convex optimization prob-\nlem with a clear mathematical interpretation [16]. These properties increased its\npopularity and usage in various applications.\nThe 21st century began with some advancements in neural networks in the areas\nof speech processing and Natural Language Processing (NLP). Hinton et al. [45]\nproposed a method for layer-wise initial training of neural networks, which leveraged\nsome of the challenges in training networks with several layers. However, the great\nNN tsunami truly hit the ﬁeld with the publication of AlexNet in 2012 [62]. In this\npaper, Krizhevsky et al. presented a neural network that achieved state-of-the-art\nperformance on the ImageNet [22] challenge, where the goal is to classify images\ninto 1000 categories using 1.2 Million images for training and 150000 images for\ntesting. The improvement over the runner-up, which relied on hand crafted features\nand one of the best classiﬁcation techniques of that time, was notable - more than\n10%. This caused the whole research community to understand that neural networks\nare way more powerful than what was thought and they bear a great potential for\nmany applications. This led to a myriad of research works that applied NNs for\nvarious ﬁelds showing their great advantage.\nNowadays, it is safe to say that almost every research ﬁeld has been aﬀected\nby this NN tsunami wave, experiencing signiﬁcant improvements in abilities and\nperformance. Many of the tools used today are very similar to the ones used in the\nprevious phase of NN. Indeed, some new regularization techniques such as batch-\nnormalization [50] and dropout [121] have been proposed. Yet, the key-enablers for\nthe current success is the large amounts of data available today that are essential for\nlarge NN training, and the developments in GPU computations that accelerate the\ntraining time signiﬁcantly (sometimes even leading to ×100 speed-up compared to\ntraining on a conventional CPU). The advantages of NN is remarkable especially\nat large scales. Thus, having large amounts of data and the appropriate hardware to\nprocess them, is vital for their success.\nA major example of a tool that did not exist before is the Generative Adversarial\nNetwork (GAN [39]). In 2014, Goodfellow et al. published this novel framework for\nlearning data distribution. The framework is composed of two models, a generator\nIntroduction to deep learning\n3\nand a discriminator, trained as adversaries. The generator is trained to capture the\ndata distribution, while the discriminator is trained to diﬀerentiate between generated\n(“fake”) data and real data. The goal is to let the generator synthesize data, which the\ndiscriminator fails to discriminate from the real one. The GAN architecture is used\nin more and more applications since its introduction in 2014. One such application is\nthe rendering of real scene images were GANs have proved very successful [36, 151].\nFor example, Brock et al. introduced the BigGAN [7] architecture that exhibited im-\npressive results in creating high-resolution images, shown in Fig. 1. While most GAN\ntechniques learn from a set of images, recently it has been successfully demonstrated\nthat one may even train a GAN just using one image [112]. Other GAN application\ninclude inpainting [73, 145], retargeting [115], 3D modeling [1], semi-supervised\nlearning [31], domain adaptation [47] and more.\nFig. 1: Class-conditional samples generated by a GAN, [7].\nWhile neural networks are very successful, the theoretical understanding behind\nthem is still missing. In this respect, there are research eﬀorts that try to provide a\nmathematical formulation that explains various aspects of NN. For example, they\nstudy NN properties such as their optimization [124], generalization [52] and ex-\npressive power [108, 88].\nThe rest of the chapter is organized as follows. In Section 2 the basic structure\nof a NN is described, followed by details regarding popular loss functions and\nmetric learning techniques used today (Section 3). We continue with an introduction\nto the NN training process in Section 4, including a mathematical derivation of\nbackpropagation and training considerations. Section 5 elaborates on the diﬀerent\noptimizers used during training, after which Section 6 presents a review of common\nregularization schemes. Section 7 details advanced NN architecture with state-of-\nthe-art performances and Section 8 concludes the chapter by highlighting some\ncurrent important NN challenges.\n2 Basic NN structure\nThe basic building block of a NN consists of a linear operation followed by a non-\nlinear function. Each building block consists of a set of parameters, termed weights\nand biases (sometimes the term weights includes also the biases), that are updated\nin the training process with the goal of minimizing a pre-deﬁned loss function.\n4\nLihi Shiloh-Perl and Raja Giryes\nAssume an input data x ∈Rd0, the output of the building block is of the form\nψ(Wx + b), where ψ(·) is a non-linear function, W ∈Rd1×d0 is the linear operation\nand b ∈Rd1 is the bias. See Fig. 2 for an illustration of a single building block.\nFig. 2: NN building block consists of a linear and a non-linear elements. The weights\nW and biases b are the parameters of the layer.\nFig. 3: NN layered structure: concatenation of N building blocks, e.g., model layers.\nTo form an NN model, such building blocks are concatenated one to another in a\nlayered structure that allows the input data to be gradually processed as it propagates\nthrough the network. Such a process is termed the (feed-)forward pass. Following it,\nduring training, a backpropagation process is used to update the NN parameters, as\nelaborated in Section 4.1. In inference time, only the forward pass is used.\nFig. 3 illustrates the concatenation of K building blocks, e.g., layers. The inter-\nmediate output at the end of the model (before the “task driven block”) is termed the\nnetwork embedding and it is formulated as follows:\nΦ(x, W(1), ..., W(K), b(1), ..., b(K)) = ψ(W(K)...ψ(W(2)ψ(W(1)x + b(1)) + b(2))... + b(K)). (1)\nThe ﬁnal output (prediction) of the network is estimated from the network embedding\nof the input data using an additional task driven layer. A popular example is the case\nof classiﬁcations, where this block is usually a linear operation followed by the\ncross-entropy loss function (detailed in Section 3).\nWhen approaching the analysis of data with varying length, such as sequential\ndata, a variant of the aforementioned approach is used. A very popular example for\nsuch a neural network structure is the Recurrent Neural Network (RNN [51]). In a\nvanilla RNN model, the network receives at each time step just a single input but\nwith a feedback loop calculated using the result of the same network in the previous\ntime-step (see an illustration in Fig. 4). This enables the network to \"remember\"\ninformation and support multiple inputs and producing one or more outputs.\nIntroduction to deep learning\n5\nMore complex RNN structures include performing bi-directional calculations or\nadding gating to the feedback and the input received by the network. The most known\ncomplex RNN architecture is the Long-Term-Short-Memory (LSTM) [46, 37], which\nadds gates to the RNN. These gates decide what information from the current input\nand the past will be used to calculate the output and the next feedback, as well as\nwhat information to mask (i.e., causing the network to forget). This enables an easier\ncombination of past and present information. It is commonly used for time-series\ndata in domains such as NLP and speech processing.\nFig. 4: Recurrent NN (RNN) illustration for time series data. The feedback loop\nintroduces time dependent characteristics to the NN model using an element-wise\nfunction. The weights are the same along all time steps.\nAnother common network structure is the Encoder-Decoder architecture. The\nﬁrst part of the model, the encoder, reduces the dimensions of the input to a compact\nfeature vector. This vector functions as the input to the second part of the model, the\ndecoder. The decoder increases its dimension, usually, back to the original input size.\nThis architecture essentially learns to compress (encode) the input to an eﬃciently\nsmall vector and then decode the information from its compact representation. In\nthe context of regular feedforward NN, this model is known as autoencoder [119]\nand is used for several tasks such as image denoising [102], image captioning [133],\nfeature extraction [132] and segmentation [2]. In the context of sequential data, it is\nused for tasks such as translation, where the decoder generates a translated sentence\nfrom a vector representing the input sentence [126, 14].\n2.1 Common linear layers\nA common basic NN building block is the Fully Connected (FC) layer. A net-\nwork composed of a concatenation of such layers is termed Multi-Layer Perceptron\n(MLP) [106]. The FC layer connects every neuron in one layer to every neuron in\nthe following layer, i.e. the matrix W is dense. It enables information propagation\nfrom all neurons to all the ones following them. However it may not maintain spatial\ninformation. Figure 5 illustrates a network with FC layers.\n6\nLihi Shiloh-Perl and Raja Giryes\nThe convolutional layer [66, 68] is another very common layer. We discuss here\nthe 2D case, where the extension to other dimension is straight-forward. This layer\napplies one or multiple convolution ﬁlters to its input with kernels of size W × H.\nThe output of the convolution layer is commonly termed a feature map.\nEach neuron in a feature map receives inputs from a set of neurons from the\nprevious layer, located in a small neighborhood deﬁned by the kernel size. If we\napply this relationship recursively, we can ﬁnd the part of the input that aﬀects each\nneuron at a given layer, i.e., the area of visible context that each neuron sees from\nthe input. The size of this part is called the receptive ﬁeld. It impacts the type and\nsize of visual features each convolution layer may extract, such as edges, corners\nand even patterns. Since convolution operations maintain spatial information and are\ntranslation equivariant, they are very useful, namely, in image processing and CV.\nIf the input to a convolution layer has some arbitrary third dimension, for example\n3-channels in an RGB image (C = 3) or some C > 1 channels from an output of a\nhidden layer in the model, the kernel of the matching convolution layer should be\nof size W × H × C. This corresponds to applying a diﬀerent convolution for each\ninput channel separately, and then summing the outputs to create one feature map.\nThe convolution layer may create a multi-channel feature map by applying multiple\nﬁlters to the input, i.e., using a kernel of size W × H × Cin × Cout, where Cin and Cout\nare the number of channels at the input and output of the layer respectively.\n2.2 Common non-linear functions\nThe non-linear functions deﬁned for each layer are of great interest since they\nintroduce the non-linear property to the model and can limit the propagating gradient\nfrom vanishing or exploding (see Section 4).\nNon-linear functions that are applied element-wise are known as activation func-\ntions. Common activation functions are the Rectiﬁed Linear Unit (ReLU [20]), leaky\nReLU [141], Exponential Linear Unit (ELU) [15], hyperbolic tangent (tanh) and sig-\nmoid. There is no universal rule for choosing a speciﬁc activation function, however,\nFig. 5: Fully-connected layers.\nIntroduction to deep learning\n7\nReLUs and ELUs are currently more popular for image processing and CV while\nsigmoid and tanh are more common in speech and NLP. Fig. 6 presents the response\nof the diﬀerent activation functions and Table 1 their mathematical formulation.\nFig. 6: Diﬀerent activation functions. Leaky ReLU with α = 0.1, ELU with α = 1.\nTable 1: Mathematical expressions for non-linear activation functions.\nFunction\nFormulation s(x)\nDerivative ds(x)\ndx\nFunction output range\nReLU\n(\n0,\nfor x < 0\nx,\nfor x ≥0\n(\n0,\nfor x < 0\n1,\nfor x ≥0\n[0, ∞)\nLeaky ReLU\n(\nαx,\nfor x < 0\nx,\nfor x ≥0\n(\nα,\nfor x < 0\n1,\nfor x ≥0\n(−∞, ∞)\nELU\n(\nα(ex −1),\nfor x < 0\nx,\nfor x ≥0\n(\nαex,\nfor x < 0\n1,\nfor x ≥0\n[−α, ∞)\nSigmoid\n1\n1+e−x\ne−x\n(1+e−x)2\n(0, 1)\ntanh\ntanh(x) = e2x−1\ne2x+1\n1 −tanh2(x)\n(−1, 1)\nAnother common non-linear operations in a NN model are the pooling functions.\nThey are aggregation operations that reduce dimensionality while keeping dominant\nfeatures. Assume a pooling size of q and an input vector to a hidden layer of size\nd, z = [z1, z1, ..., zd]. For every m ∈[1, d], the subset of the input vector ˜z =\n[zm, zm+1, ..., zq+m] may undergo one of the following popular pooling operations:\n1. Max pooling: g(˜z) = maxi ˜z\n2. Mean pooling: g(˜z) = 1\nq\nÍq+m\ni=m zi\n3. ℓp pooling: g(˜z) =\npqÍq+m\ni=m zp\ni\nAll pooling operations are characterized by a stride, s, that eﬀectively deﬁnes the\noutput dimensions. Applying pooling with a stride s, is equivalent to applying the\n8\nLihi Shiloh-Perl and Raja Giryes\npooling with no stride (i.e., s = 1) and then sub-sampling by a factor of s. It is\ncommon to add zero padding to z such that its length is divisible by s.\nAnother very common non-linear function is the softmax, which normalizes\nvectors into probabilities. The output of the model, the embedding, may undergo an\nadditional linear layer to transform it to a vector of size 1 × N, termed logits, where\nN is the number of classes. The logits, here denoted as v, are the input to the softmax\noperation deﬁned as follows:\nsoftmax(vi) =\nevi\nÍN\nj=1 evj ,\ni ∈[1, ..., N].\n(2)\n3 Loss functions\nDeﬁning the loss function of the model, denoted as L, is critical and usually chosen\nbased on the characteristics of the dataset and the task at hand. Though datasets\ncan vary, tasks performed by NN models can be divided into two coarse groups: (1)\nregression tasks and (2) classiﬁcation tasks.\nA regression problem aims at approximating a mapping function from input\nvariables to a continuous output variable(s). For NN tasks, the output of the network\nshould predict a continues value of interest. Common NN regression problems\ninclude image denoising [148], deblurring [84], inpainting [142] and more. In these\ntasks, it is common to use the Mean Squared Error (MSE), Structural SIMilarity\n(SSIM) or ℓ1 loss as the loss function. The MSE (ℓ2 error) imposes a larger penalty\nfor larger errors, compared to the ℓ1 error which is more robust to outliers in the data.\nThe SSIM, and its multiscale version [149], help improving the perceptual quality.\nIn the classiﬁcation task, the goal is to identify the correct class of a given\nsample from pre-deﬁned N classes. A common loss function for such tasks is the\ncross-entropy loss. It is implemented based on a normalized vector of probabilities\ncorresponding to a list of potential outcomes. This normalized vector is calculated\nby the softmax non-linear function (Eq. (2)). The cross-entropy loss is deﬁned as:\nLCE = −\nN\nÕ\ni=1\nyi log(pi),\n(3)\nwhere yi is the ground-truth probability (the label) of the input to belong to class\ni and pi is the model prediction score for this class. The label is usually binary,\ni.e., it contains 1 in a single index (corresponding to the true class). This type of\nrepresentation is known as one-hot encoding. The class is predicted in the network by\nselecting the largest probability and the log-loss is used to increase this probability.\nNotice that a network may provide multiple outputs per input data-point. For\nexample, in the problem of image semantic segmentation, the network predicts\na class for each pixel in the image. In the task of object detection, the network\noutputs a list of objects, where each is deﬁned by a bounding box (found using a\nIntroduction to deep learning\n9\nregression loss) and a class (found using a classiﬁcation loss). Section 7.1 details\nthese diﬀerent tasks. Since in some problems, the labelled data are imbalanced, one\nmay use weighted softmax (that weigh less frequent classes) or the focal loss [72].\n3.1 Metric Learning\nAn interesting property of the log-loss function used for classiﬁcation is that it\nimplicitly cluster classes in the network embedding space during training. However,\nfor a clustering task, these vanilla distance criteria often produce unsatisfactory\nperformance as diﬀerent class clusters can be positioned closely in the embedding\nspace and may cause miss-classiﬁcation for samples that do not reside in the speciﬁc\ntraining set distribution.\nTherefore, diﬀerent metric learning techniques have been developed to produce\nan embedding space that brings closer intra-class samples and increases inter-class\ndistances. This results in better accuracy and robustness of the network. It allows\nthe network to be able to distinguish between two data samples if they are from the\nsame class or not, just by comparing their embeddings, even if their classes have not\nbeen present at training time.\nMetric learning is very useful for tasks such as face recognition and identiﬁcation,\nwhere the number of subjects to be tested are not known at training time and new\nidentities that were not present during training should also be identiﬁed/recognized\n(e.g., given two images the network should decide whether these correspond to the\nsame or diﬀerent persons).\nAn example for a popular metric loss is the triplet loss [109]. It enforces a margin\nbetween instances of the same class and other classes in the embedding feature\nspace. This approach increases performance accuracy and robustness due to the\nlarge separation between class clusters in the embedding space. The triplet loss can\nbe used in various tasks, namely detection, classiﬁcation, recognition and other tasks\nof unknown number of classes.\nIn this approach, three instances are used in each training step i: an anchor xa\ni ,\nanother instance xp\ni from the same class of the anchor (positive sample), and a sample\nxn\ni from a diﬀerent class (negative class). They are required to obey the following\ninequality:\n\r\rΦ(xa\ni ) −Φ(xp\ni )\n\r\r2\n2 + α <\n\r\rΦ(xa\ni ) −Φ(xn\ni )\n\r\r2\n2 ,\n(4)\nwhere α < 0 enforces the wanted margin from other classes. Thus, the triplet loss is\ndeﬁned by:\nL =\nÕ\ni\n\r\rΦ(xa\ni ) −Φ(xp\ni )\n\r\r2\n2 −\n\r\rΦ(xa\ni ) −Φ(xn\ni )\n\r\r2\n2 + α.\n(5)\nFig. 7 presents a schematic illustration of the triplet loss inﬂuence on samples in\nthe embedding space. This illustration also exhibits a speciﬁc triplet example, where\nthe positive examples are relatively far from the anchor while negative examples are\n10\nLihi Shiloh-Perl and Raja Giryes\nrelatively near the anchor. Finding such examples that violate the triplet condition is\ndesirable during training. They may be found by on-line or oﬀ-line searches known\nas hard negative mining. A preprocessing of the instances in the embedding space is\nperformed to ﬁnd violating examples for training the network.\nFinding the \"best\" instances for training can, evidently, aid in achieving improved\nconvergence. However, searching for them is often time consuming and therefore\nalternative techniques are being explored.\nFig. 7: Triplet loss: minimizes the distance between two similar class examples (an-\nchor and positive), and maximizes the distance between two diﬀerent class examples\n(anchor and negative).\nAn intriguing metric learning approach relies on ’classiﬁcation’-type loss func-\ntions, where the network is trained given a ﬁxed number of classes. Yet, these losses\nare designed to create good embedding space that creates margin between classes,\nwhich in turn provides good prediction of similarity between two inputs. Popular\nexamples include the Cos-loss [134], Arc-loss [23] and SphereFace [76].\n4 Neural network training\nGiven a loss function, the weights of the neural network are updated to minimize\nit for a given training set. The training process of a neural network requires a large\ndatabase due to the nature of the network (structure and amount of parameters) and\nGPUs for eﬃcient training implementation.\nIn general, training methods can be divided into supervised and unsupervised\ntraining. The former consists of labeled data that are usually very expensive and\ntime consuming to obtain. Whereas the latter is the more common case and does not\nassume known ground-truth labels. However, supervised training usually achieves\nsigniﬁcantly better network performance compared to the unsupervised case. There-\nfore, a lot of resources are invested in labeling datasets for training. Thus, we focus\nhere mainly on the supervised setting.\nIn neural networks, regardless of the model task, all training phases have the same\ngoal: to minimize a pre-deﬁned error function, also denoted as the loss/cost function.\nThis is done in two stages: (a) a feed-forward pass of the input data through all the\nnetwork layers, calculating the error using the predicted outputs and their ground-\ntruth labels (if available); followed by (b) backpropogation of the errors through\nthe network to update their weights, from the last layer to the ﬁrst. This process is\nperformed continuously to ﬁnd the optimized values for the weights of the network.\nIntroduction to deep learning\n11\nThe backpropagation algorithm provides the gradients of the error with respect to\nthe network weights. These gradients are used to update the weights of the network.\nCalculating them based on the whole input data is computationally demanding and\ntherefore, the common practice is to use subsets of the training set, termed mini-\nbatches, and cycle over the entire training set multiple times. Each cycle of training\nover the whole dataset is termed an epoch and in every cycle the data samples are\nused in a random order to avoid biases. The training process ends when convergence\nin the loss function is obtained. Since most NN problems are not convex, an optimal\nsolution is not assured. We turn now to describe in more details the training process\nusing backpropagation.\n4.1 Backpropogation\nFig. 8: Simple classiﬁcation model ex-\nample, consisting of a two layered fully-\nconnected model.\nThe backpropagation process is performed\nto update all the parameters of the model,\nwith the goal of decreasing the loss func-\ntion value. The process starts with a feed-\nforward pass of input data, x, through all\nthe network layers. After which the loss\nfunction value is calculated and denoted as\nL(x, W), where W are the model parame-\nters (including the model weights and bi-\nases, for formulation convenience). Then\nthe backpropagation is initiated by com-\nputing the value of: ∂L\n∂W, followed by the\nupdate of the network weights. All the\nweights are updated recursively by calcu-\nlating the gradients of every layer, from\nthe ﬁnal one to the input layer, using the\nchain rule.\nDenote the output of layer l as z(l). Fol-\nlowing the chain rule, the gradients of a\ngiven layer l with parameters W(l) with respect to its input z(l) are:\n∂L\n∂z(l−1) = ∂L\n∂z(l) · ∂z(l)(W(l), z(l−1))\n∂z(l−1)\n,\n(6)\nand the gradients with respect to the parameters are:\n∂L\n∂W(l) = ∂L\n∂z(l) · ∂z(l)(W(l), z(l−1))\n∂W(l)\n.\n(7)\nThese two formulas of the backpropagation algorithm dictate the gradients calcula-\ntion with respect to the parameters for each layer in the network and, therefore, the\n12\nLihi Shiloh-Perl and Raja Giryes\noptimization can be performed using gradient-based optimizers (see Section 5 for\nmore details).\nTo demonstrate the use of the backpropagation technique for the calculation of the\nnetwork gradients, we turn to consider an example of a simple classiﬁcation model\nwith two-layers: a fully-connected layer with a ReLU activation function followed\nby another fully-connected layer with softmax function and log-loss. See Fig. 8 for\nthe model illustration.\nDenote by z(3) the output of the softmax layer and assume that the input x belongs\nto class k (using one-hot encoding yk = 1). The log-loss in this case is:\nL = −\nÕ\ni\nlog \u0000z(3)\ni\n\u0001yi = −log\n \nexp \u0000z(2)\nk\n\u0001\nÍ\ni exp \u0000z(2)\ni\n\u0001\n!\n= −z(2)\nk + log\n\u0010 Õ\nj\nexp z(2)\nj\n\u0011\n.\n(8)\nFor all i , k, the gradient of the error with respect to the softmax input z(2)\ni\nis\n∂L\n∂z(2)\ni\n=\nexp \u0000z(2)\ni\n\u0001\nÍ\nj exp \u0000z(2)\nj\n\u0001 ≡gi.\n(9)\nNotice that this implies that we need to decrease the value of z(2)\ni\n(the ith-logit)\nproportionally to the probability the network provides to it. While for the correct\nlabel, i = k, the derivative is:\n∂L\n∂z(2)\nk\n= −1 +\nexp \u0000z(2)\nk\n\u0001\nÍ\nj exp \u0000z(2)\nj\n\u0001 = gk −1,\n(10)\nwhich implies that the value of the logit element associated with the true label\nshould be increased proportionally to the mistake the network is currently doing in\nthe prediction.\nThe output z(2) is a product of a fully-connect layer. Therefore, it can be formulated\nas follows:\nz(2) = W(2)˜z(1),\n(11)\nwhere ˜z(1) is the output of the ReLu function. Following the backpropagation rules\nwe get that for this layer, the derivative with respect to its input is:\n∂L\n∂˜z(1) = ∂L\n∂z(2) · ∂z(2)(W(2), ˜z(1))\n∂˜z(1)\n= ∂L\n∂z(2) · W(2),\n(12)\nwhereas, the derivative with respect to its parameters is:\n∂L\n∂W(2) = ∂L\n∂z(2) · ∂z(2)(W(2), ˜z(1))\n∂W(1)\n= ∂L\n∂z(2) · ˜z(1).\n(13)\nThe ReLU operation has no weight to update, but aﬀects the gradients. The derivative\nof this stage follows:\nIntroduction to deep learning\n13\n∂L\n∂z(1) = ∂L\n∂˜z(1) · ∂˜z(1)(W(1), I)\n∂z(1)\n=\n(\n0,\nif z(1) < 0\n∂L\n∂˜z(1),\notherwise.\n(14)\nThe ﬁnal derivative with respect to the input ∂L/∂x is calculated similar to Eq. (12).\n4.2 Training considerations\nThere are several considerations that should be addressed when training a NN. The\nmost infamous is the overﬁtting, i.e., when the model too closely ﬁts to the training\ndataset but does not generalize well to the test set. When this occurs, high training\ndata precision is achieved, while the precision on the test data (not used during\ntraining) is low [129]. For this purpose, various regularization techniques have been\nproposed. We discuss some of them in Section 6.\nA second consideration is the vanishing/exploding gradients occurring during\ntraining. Vanishing gradients are a result of multiplications with values smaller than\none during their calculation in the backpropagation recursion. This can be resolved\nusing activation functions and batch normalization detailed in Section 6. On the\nother hand, the gradients might also explode due to derivatives that are signiﬁcantly\nlarger than one in the backpropogation calculation. This makes the training unstable\nand may imply the need for re-designing the model (e.g., replace a vanilla RNN with\na gated architecture such as LSTMs) or the use of gradient clipping [91].\nAnother important issue is the requirement that the training dataset must represent\nthe true distribution of the task at hand. This usually enforces very large annotated\ndatasets, which necessitate signiﬁcant funding and manpower to obtain. In this case,\nconsiderable eﬀorts must be invested to train the network using these large datasets,\ncommonly with multiple GPUs for several days [62, 58]. One may use techniques\nsuch as domain adaptation [138] or transfer learning [128] to use already existing\nnetworks or large datasets for new tasks.\n5 Training optimizers\nTraining neural networks is done by applying an optimizer to reach an optimal\nsolution for the deﬁned loss function. Its goal is to ﬁnd the parameters of the model,\ne.g., weights and biases, which achieve minimum error for the training set samples:\n(xi, yi), where yi is the label for the instance xi. For a loss function L(·), the objective\nreads as:\nÕ\ni\nL(Φ(xi, W), yi),\n(15)\nfor ease of notation, all model parameters are denoted as W. A variety of optimizers\nhave been proposed and implemented for minimizing Eq. 15. Yet, due to the size of\nthe network and training dataset, mainly ﬁrst-order methods are being considered,\n14\nLihi Shiloh-Perl and Raja Giryes\ni.e. strategies that rely only on the gradients (and not on second-order derivatives\nsuch as the Hessian).\nSeveral gradient based optimizers are commonly used for updating the parameters\nof the model. These NN parameters are updated in the opposite direction of the\nobjective function’s gradient, g{GD,T(t)}, where T(t) is a randomly chosen subgroup\nof size n′ < n training samples used in iteration t (n is the size of the training dataset).\nNamely, at iteration t the weights are calculated as\nW(t) = W(t −1) −η · g{GD,T(t)},\n(16)\nwhere η is the learning rate that determines the size of the steps taken to reach the\n(local) minimum and the gradient step, g{GD,T(t)} is computed using the samples in\nT(t) as\ng{GD,T(t)} = 1\nn′\nÕ\ni∈T(t)\n∇W L(W(t); xi; yi),\n(17)\nwhere the pair (xi, yi) is a training example and its corresponding label in the training\nset, and L is the loss function. However, needless to say that calculating the gradient\non the whole dataset is computationally demanding. To this end, Stochastic Gradient\nDescent (SGD) is more popular, since it calculates the gradient in Eq. (17) for only\none randomly chosen example from the data, i.e., n′ = 1.\nSince the update by SGD depends on a diﬀerent sample at each iteration, it has\na high variance that causes the loss value to ﬂuctuate. While this behavior may\nenable it to jump to a new and potentially better local minima, it might ultimately\ncomplicates convergence, as SGD may keep overshooting. To improve convergence\nand exploit parallel computing power, mini-batch SGD is proposed in which the\ngradient in Eq. (17) is calculated with n′ > 1 (but not all the data).\nAn acceleration in convergence may be obtained by using the history of the last\ngradient steps, in order to stabilize the optimization. One such approach uses adaptive\nmomentum instead of a ﬁxed step size. This is calculated based on exponential\nsmoothing on the gradients, i.e:\nM(t) = β · M(t −1) + (1 −β) · g{SGD,T(t)},\nW(t) = W(t −1) −ηM(t),\n(18)\nwhere M(t) approximates the 1st moment of g{SGD,T(t)}. A typical value for the\nconstant is β ∼0.9, which implies taking into account the last 10 gradient steps in\nthe momentum variable M(t) [95]. A well-known variant of Momentum proposed\nby Nestrov et al. [85] is the Nestrov Accelerated Gradient (NAG). It is similar to\nMomentum but calculates the gradient step as if the network weights have been\nalready updated with the current Momentum direction.\nAnother popular technique is the Adaptive Moment Estimation (ADAM) [61],\nwhich also computes adaptive learning rates. In addition to storing an exponentially\ndecaying average of past squared gradients, V(t), ADAM also keeps an exponentially\ndecaying average of past gradients, M(t), in the following way:\nIntroduction to deep learning\n15\nM(t) = β1M(t −1) + (1 −β1)gt,\nV(t) = β2V(t −1) + (1 −β2)g2\nt ,\n(19)\nwhere gt is the gradient of the current batch, β1 and β2 are ADAM’s hyperparameters,\nusually set to 0.9 and 0.999 respectively, and M(t) and V(t) are estimates of the ﬁrst\nmoment (the mean) and the second moment (the uncentered variance) of the gradients\nrespectively. Hence the name of the method - Adaptive Moment Estimation. As M(t)\nand V(t) are initialized as vectors of 0âĂŹs, the authors of ADAM observe that they\nare biased towards zero, especially during the initial time steps. To counteract these\nbiases, a bias-corrected ﬁrst and second moment are used: ˆM(t) = M(t)/(1 −β1(t))\nand ˆV(t) = V(t)/(1 −β2(t)). Therefore, the ADAM update rule is as follows:\nW(t + 1) = W(t) −\nη\np ˆV(t) + ϵ\nˆM(t).\n(20)\nADAM has two popular extensions: AdamW by Loshchilov et al. [78] and AMSGrad\nby Redddi et al. [97]. There are several additional common optimizers that have\nadaptive momentum, such as AdaGrad [29], AdaDelta [146] or RMSprop [21]. It\nmust be noted that since the NN optimization is non-convex, the minimal error\npoint reached by each optimizer is rarely the same. Thus, speedy convergence is not\nalways favored. In particular, it has been observed that Momentum leads to better\ngeneralization than ADAM, which usually converges faster [60]. Thus, the common\npractice is to make the development with ADAM and then make the ﬁnal training\nwith Momentum.\n(a) Original image\n(b) Flip augmentation\n(c) Crop and scale augmentation\n(d) Noise augmentation\nFig. 9: Diﬀerent image augmentations.\n16\nLihi Shiloh-Perl and Raja Giryes\n6 Training regularizations\nOne of the great advantageous of NN is their ability to generalize, i.e., correctly\npredict unseen data [52]. This must be ensured during the training process and is\naccomplished by several regularization methods, detailed here. The most common\nare weight decay [63], dropout [121], batch normalization [50] and the use of data\naugmentation [116].\nWeight decay is a basic tool to limit the growth of the weights by adding a\nregularization term to the cost function for large weights, which is the sum of\nsquares of all the weights, i.e., Í\ni |Wi|2.\nThe key idea in dropout is to randomly drop units (along with their connections)\nfrom the neural network during training and thus prevent units from co-adapting too\nmuch. The percentage of dropped units is critical since a large amount will result in\npoor learning. Common values are 20% −50% dropped units.\nBatch normalization is a mean to deal with changes in the distribution of the\nmodel’s parameters during training. The layers need to adapt to these (often noisy)\nchanges between instances during training. Batch normalization causes the features\nof each training batch to have a mean of 0 and a variance of 1 in the layer it is\nbeing applied. To normalize a value across a batch, i.e. to batch normalize the value,\nthe batch mean, µB, is subtracted and the result is divided by the batch standard\ndeviation,\nq\nσ2\nB + ϵ. Note that a small constant ϵ is added to the variance in order to\navoid dividing by zero. The batch normalizing transform of a given input, x, is:\nBN(x) = γ\n \nx −µB\nq\nσ2\nB + ϵ\n!\n+ β.\n(21)\nNotice the (learnable) scale and bias parameters γ and β, which provides the NN\nwith freedom to deviate from the zero mean and unit variance. BN is less eﬀective\nwhen used with small batch sizes since in this case the statistics calculated per each is\nless accurate. Thus, techniques such as group normalization [139] or Filter Response\nNormalization (FRN) [118] have been proposed.\nData augmentation is a very common strategy used during training to artiﬁcially\n“increase” the size of the training data and make the network robust to transformations\nthat do not change the input label. For example, in the task of classiﬁcation a shifted\ncat is still a cat; see Fig. 9 for more similar augmentation. In the task of denoising,\nﬂipped noisy input should result in a ﬂipped clean output. Thus, during training the\nnetwork is trained also with the transformed data to improve its performance.\nCommon augmentations are randomly ﬂipping, rotating, scaling, cropping, trans-\nlating, or adding noise to the data. Other more sophisticated techniques that lead to\na signiﬁcant improvement in network performance include mixup [147], cutout [26]\nand augmentations that are learned automatically [18, 71, 19].\nIntroduction to deep learning\n17\n7 Advanced NN architectures\nThe basic building blocks, which compose the NN model architecture, are used in\nfrequently innovative structures. In this section, such known architectures with state-\nof-the-art performance are presented, divided by tasks and data types: detection\nand segmentation tasks are described in Section 7.1, sequential data handling is\nelaborated in Section 7.2 and processing data on irregular grids is presented in\nSection 7.3. Clearly, there are many other use-cases and architectures, which are not\nmentioned here.\n7.1 Deep learning for detection and segmentation\nMany research works focus on detecting multiple objects in a scene, due to its\nnumerous applications. This problem can be divided into four sub-tasks as follows,\nwhere we refer here to image datasets although the same concept can be applied to\ndiﬀerent domains as well.\n1. Classiﬁcation and localization: The main object in the image is detected and then\nlocalized by a surrounding bounding box and classiﬁed from a pre-known set.\n2. Object detection: Detection of all objects in a scene that belong to a pre-known\nset and then classifying and providing a bounding box for each of them.\n3. Semantic segmentation: Partitioning the image into coherent parts by assigning\neach pixel in the image with its own classiﬁcation label (associated with the object\nthe pixel belongs to). For example, having a pixel-wise diﬀerentiation between\nanimals, sky and background (generic class for all object that no class is assigned\nto) in an image.\n4. Instance segmentation: Multiple objects segmentation and classiﬁcation from a\npre-known set (similar to object detection but for each object all its pixels are\nidentiﬁed instead of providing a bounding box for it).\nToday, state-of-the-art object detection performance is achieved with architectures\nsuch as Faster-RCNN [103, 135], You Only Look Once (YOLO) [98, 99, 100], Single\nShot Detector (SSD) [75] and Fully Convolutional One-Stage Object Detection\n(FCOS) [150]. The object detection models provide a list of detected bounding\nboxes with the class of each of them.\nSegmentation tasks are mostly implemented using fully convolutional net-\nwork. Known segmentation models include UNet [104], Mask-RCNN [44] and\nDeeplab [11]. These architecture have the same input/output spatial size since the\noutput represents the segmentation map of the input image.\nBoth object detection and segmentation tasks are analyzed via the Intersection\nover Union (IoU) metric. The IoU is deﬁned as the ratio between the intersection\narea of the object’s ground-truth pixels, Bg, with the corresponding predicted pixels,\nBp, and the union of these group of pixels. The IoU is formulated as:\n18\nLihi Shiloh-Perl and Raja Giryes\nIoU = Area{Bg ∩Bp}\nArea{Bg ∪Bp} .\n(22)\nAs this measure evaluate only the quality of the bounding box, a mean Average\nPrecision (mAP) is commonly used to evaluate the models performance. The mAP\nis deﬁned as the ratio of the correctly detected (or segmented) objects, where an\nobject is considered to be detected correctly if there is a bounding box for it with the\ncorrect class and a IoU greater than 0.5 (or another speciﬁed constant).\nAnother common evaluation metric is the F1 score, which is the harmonic average\nof the precision and the recall values. See Eq. (24) below. They are calculated using\nthe following deﬁnitions that are presented for the case of semantic segmentation:\n•\nTrue Positive (TP): the predicted class of a pixel matches it ground-truth label.\n•\nFalse Positive (FP): the predicted pixel of an object was falsely determined.\n•\nFalse Negative (FN): a ground-truth pixel of an object was not predicted.\nNow that they are deﬁned, the precision, recall and F1 are given by:\nprecision =\nTP\nTP + FP,\nrecall =\nTP\nTP + FN\n(23)\nF1 = 2 · precision · recall\nprecision + recall.\n(24)\n7.2 Deep learning on sequential data\nSequential data are composed of time-sensitive signals such as the output of diﬀerent\nsensors, audio recordings, NLP sentences or any signal that its order is of importance.\nTherefore, this data must be processed accordingly.\nInitially, sequential data was processed with Recurrent NN (RNN) [51] that has\nrecurrent (feedback) connections, where outputs of the network at a given time-step\nserve as input to the model (in addition to the input data) at the next time-step. This\nintroduces the time dependent feature of the NN. A RNN is illustrated in Fig. 4.\nHowever, it was quickly realized that during training, vanilla RNNs suﬀer from\nvanishing/exploding gradients. This phenomena, originated from the use of ﬁnite-\nprecision back-propagation process, limits the size of the sequence.\nTo this end, a corner stone block is used: the Long-Short-Term-Memory\n(LSTM [46]). Mostly used for NLP tasks, the LSTM is a RNN block with gates.\nDuring training, these gates learn which part of the sentence to forget or to mem-\norize. The gating allow some of the gradients to backpropagate unchanged, which\naids the vanishing gradient symptom. Notice that RNNs (and LSTMs) can process\na sentence in a bi-directional mode, i.e., process a sentence in two directions, from\nthe beginning to the end and vice verse. This mechanism allows a better grasp of\nthe input context by the network. Examples for popular research tasks in NLP data\ninclude question answering [96], translation [65] and text generation [41].\nIntroduction to deep learning\n19\nSentences processing. An important issue in NLP is representing words in prepa-\nration to serve as network input. The use of straight forward indices is not eﬀective\nsince there are thousands of words in a language. Therefore, it is common to process\ntext data via word embedding, which is a vector representation of each word in some\nﬁxed dimension. This method enables to encapsulate relationships between words.\nA classic methodology to calculate the word embedding is Word2Vec [81], in\nwhich these vector representations are calculated using a NN model that learn their\ncontext. More advanced options for creating eﬃcient word representations include\nBERT [25], ELMO [92], RoBERTa [77] and XLNet [144].\nAudio processing. Audio recordings are used for multiple interesting tasks, such\nas speech to text, text to speech and speech processing. In the audio case, the\ncommon input to speech systems is the Mel Frequency Cepstral Coeﬃcient (MFCC)\nor a Short Time Fourier Transform (STFT) image, as opposed to the audio raw-data.\nA milestone example for speech processing NN architecture is the wavenet [89]. This\narchitecture is an autoregressive model that synthesizes speech or audio signals. It\nis based on dilated convolutional layers that have large receptive ﬁelds, that allow\neﬃcient processing. Another prominent synthesis model for sequential data is the\nTacotron [113].\nThe attention model. As mentioned in Section 2, one may use RNN for transla-\ntion using the encoder decoder model, which encodes a source sentence into a vector,\nwhich is then decoded to a target language. Instead of relying on a compressed vector,\nwhich may lose information, the attention models learn where or what to focus on\nfrom the whole input sequence. Introduced in 2015 [3], attention models have shown\nsuperior performance over encoder-decoder architectures in tasks such as translation,\ntext to speech and image captioning. Recently, it has been suggested to replace the\nrecurrent network structure totally by the attention mechanism, which results with\nthe transformers network models [131].\n7.3 Deep learning on irregular grids\nA wide variety of data acquisition mechanisms do not represent the data on a grid as is\ncommon with images data. A prominent example is 3D imaging (e.g. using LIDAR),\nwhere the input data are represented as points in a 3D space with or without color\ninformation. Processing such data is not trivial as standard network components,\nsuch as convolutions, assume a grid of the data. Therefore, they cannot be applied\nas is and custom operations are required. We focus our discussion here on the case\nof NN for 3D data.\nToday, real-time processing of 3D scenes can be achieved with advanced NN\nmodels that are customized to these irregular grids. The diﬀerent processing tech-\nniques for these irregular grid data can be divided by the type of representation used\nfor the data:\n1. Points processing. 3D data points are processed as points in space, i.e., a list\nof the point coordinates is given as the input to the NN. A popular network for\n20\nLihi Shiloh-Perl and Raja Giryes\nthis representation is PointNet [93]. It is the ﬁrst to eﬃciently achieve satisfactory\nresults directly on the point cloud. Yet, it is limited by the number of points\nthat can be analyzed, computational time and performance. Some more recent\nmodels that improves its performance include PointNet++ [94], PointCNN [69],\nDGCNN [136]. Strategies to improve its eﬃciency have been proposed in learning\nto sample [28] and RandLA-Net [48].\n2. Multi-view 2D projections. 3D data points are projected (from various angles)\nto the 2D domain so that known 2D processing techniques can be used [70, 56].\n3. Volumetric (voxels). 3D data points are represented in a grid-based voxel repre-\nsentation. This is analogous to a 2D representation and is therefore advantageous.\nHowever, it is computationally exhaustive [140] and losses resolution.\n4. Meshes. Mesh represents the 3D domain via a graph that deﬁnes the connectivity\nbetween the diﬀerent points. Yet, this graph has a special structure such that it\ncreates the surface of the 3D shape (in the common case of triangular mesh, the\nshape surface is presented by a set of triangles connected to each other). In 2015\nMasci et al. [6] have shown it is possible to learn features using DL on meshes.\nSince then, a signiﬁcant advancement has been made in mesh processing [43, 83].\n5. Graphs. Graph representations are common for representing non-linear struc-\ntured data. Some works have proposed eﬃcient NN models for 3D data points on\na grid-based graph structure [122, 86].\n8 Summary\nThis chapter provided a general survey of the basic concepts in neural networks. As\nthis ﬁeld is expanding very fast, the space is too short to describe all the developments\nin it, even though most of them are from the past eight years. Yet, we brieﬂy mention\nhere few important problems that are currently being studied.\n1. Domain adaptation and transfer learning. As many applications necessitate\ndata that is very diﬃcult to obtain, some methods aim at training models based on\nscarce datasets. A popular methodology for dealing with insuﬃcient annotated\ndata is domain adaptation, in which a robust and high performance NN model,\ntrained on a source distribution, is used to aid the training of a similar model\n(usually with the same goal, e.g., in classiﬁcation the same classes are searched\nfor) on data from a target distribution that are either unlabelled or small in number\n[33, 90, 117]. An example is adapting a NN trained on simulation data to real-life\ndata with the same labels [130, 47]. On a similar note, transfer learning [128, 27]\ncan also be used in similar cases, where in addition to the diﬀerence in the data,\nthe input and output tasks are not the same but only similar (in domain adaptation\nthe task is the same and only the distributions are diﬀerent). One such example,\nis using a network trained on natural images to classify medical data [4].\n2. Few shot learning. A special case of learning with small datasets is few-shot\nlearning [137], where one is provided either with just semantic information of the\nIntroduction to deep learning\n21\ntarget classes (zero-shot learning), only one labelled example per class (1-shot\nlearning) or just few samples (general few-shot learning). Approaches developed\nfor these problems have shown great success in many applications, such as image\nclassiﬁcation [125, 111, 123], object detection [57] and segmentation [8].\n3. On-line learning. Various deep learning challenges occur due to new distributions\nor class types introduced to the model during a continuous operation of the system\n(post-training), and now must be learnt by the model. The model can update its\nweights to incorporate these new data using on-line learning techniques. There\nis a need for special training in this case, as systems that just learn based on the\nnew examples may suﬀer from a reduced performance on the original data. This\nphenomena is known as catastrophic forgetting [59]. Often, the model tends to\nforget the representation of part of the distribution it already learned and thus\nit develops a bias towards the new data. A speciﬁc example of on-line learning\nis incremental learning [9], where the new data is of diﬀerent classes than the\noriginal ones.\n4. AutoML. When approaching real-life problems, there is an inherent pipeline of\ntasks to be preformed before using DL tools, such as problem deﬁnition, preparing\nthe data and processing it. Commonly, these tasks are preformed by specialists and\nrequire deep system understating. To this end, the autoML paradigm attempts to\ngeneralize this process by automatically learning and tuning the model used [32].\nA particular popular task in autoML is Neural Architecture Search (NAS) [30].\nThis is of interest since the NN architecture restricts its performance. However,\nsearching for the optimal architecture for a speciﬁc task, and from a set of pre-\ndeﬁned operations, is computationally exhaustive when performed in a straight\nforward manner. Therefore, on-going research attempts to overcome this limita-\ntion. An example is the DARTS [74] strategy and its extensions [87, 12] where the\nkey contribution is ﬁnding, in a diﬀerentiable manner, the connections between\nnetwork operations that form a NN architecture. This framework decreases the\nsearch time and improves the ﬁnal accuracy.\n5. Reinforcement Learning. To date, the most eﬀective training method for deci-\nsion based actions, such as robot movement and video games, is Reinforcement\nLearning (RL) [55, 127]. In RL, the model tries to maximize some pre-deﬁned\naward score by learning which action to take, from a set of deﬁned actions in\nspeciﬁc scenarios.\nTo summarize, being able to eﬃciently train deep neural networks has revolu-\ntionized almost every aspect of the modern day-to-day life. Examples span from\nbio-medical applications through computer graphics in movies and videos to inter-\nnational scale applications of big companies, such as Google, Amazon, Microsoft,\nApple and Facebook. Evidently, this theory is drawing much attention and we be-\nlieve there is still much to unravel, including exploring and understanding the NN’s\npotential abilities and limitations.\nThe next chapters detail Convolutional Neural Networks (CNN), Recurrent Neu-\nral Networks (RNN), generative models and autoencoders. All are very important\nparadigms that are used in numerous applications.\n22\nLihi Shiloh-Perl and Raja Giryes\nReferences\n1. Achlioptas, P., Diamanti, O., Mitliagkas, I., Guibas, L.: Learning representations and gen-\nerative models for 3D point clouds. In: J. Dy, A. Krause (eds.) Proceedings of the 35th\nInternational Conference on Machine Learning, Proceedings of Machine Learning Research,\nvol. 80, pp. 40–49. PMLR, StockholmsmÃďssan, Stockholm Sweden (2018)\n2. Atlason, H.E., AskellLove, Sigurdsson, S., Gudnason, V., Ellingsen, L.M.: Unsupervised brain\nlesion segmentation from mri using a convolutional autoencoder. In: Medical Imaging 2019:\nImage Processing, vol. 10949, p. 109491H. International Society for Optics and Photonics\n(2019)\n3. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align\nand translate. In: 3rd International Conference on Learning Representations, ICLR (2015)\n4. Bar, Y., Diamant, I., Wolf, L., Greenspan, H.: Deep learning with non-medical training used\nfor chest pathology identiﬁcation. In: Medical Imaging 2015: Computer-Aided Diagnosis,\nvol. 9414, pp. 215 – 221. International Society for Optics and Photonics, SPIE (2015)\n5. Ben-Cohen, A., Diamant, I., Klang, E., Amitai, M., Greenspan, H.: Fully convolutional\nnetwork for liver segmentation and lesions detection. In: Deep learning and data labeling for\nmedical applications, pp. 77–85. Springer (2016)\n6. Boscaini, D., Masci, J., Melzi, S., Bronstein, M.M., Castellani, U., Vandergheynst, P.: Learn-\ning class-speciﬁc descriptors for deformable shapes using localized spectral convolutional\nnetworks. Comput. Graph. Forum 34, 13–23 (2015)\n7. Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high ﬁdelity natural\nimage synthesis. In: International Conference on Learning Representations (ICLR) (2019)\n8. Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taixé, L., Cremers, D., Van Gool, L.: One-shot\nvideo object segmentation. In: Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 221–230 (2017)\n9. Castro, F.M., Marín-Jiménez, M.J., Guil, N., Schmid, C., Alahari, K.: End-to-end incremental\nlearning. In: Proceedings of the European Conference on Computer Vision (ECCV), pp. 233–\n248 (2018)\n10. Chen, L.C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H.: Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In: Proceedings of the European\nconference on computer vision (ECCV), pp. 801–818 (2018)\n11. Chen, L.C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H.: Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In: ECCV (2018)\n12. Chen, X., Xie, L., Wu, J., Tian, Q.: Progressive darts: Bridging the optimization gap for nas\nin the wild. arXiv preprint arXiv:1912.10952 (2019)\n13. Chen, Z., Zhang, J., Tao, D.: Progressive lidar adaptation for road detection. IEEE/CAA\nJournal of Automatica Sinica 6(3), 693–702 (2019)\n14. Cho, K., van Merriënboer, B., Bahdanau, D., Bengio, Y.: On the properties of neural machine\ntranslation: Encoder–decoder approaches. In: Workshop on Syntax, Semantics and Structure\nin Statistical Translation, pp. 103–111. Association for Computational Linguistics (2014)\n15. Clevert, D.A., Unterthiner, T., Hochreiter, S.: Fast and accurate deep network learning by\nexponential linear units (elus). CoRR (2015)\n16. Cortes, C., Vapnik, V.: Support vector networks. Machine Learning 20, 273–297 (1995)\n17. Cristianini, N., Shawe-Taylor, J.: An Introduction to Support Vector Machines and Other\nKernel-based Learning Methods.\nCambridge University Press (2000).\nDOI 10.1017/\nCBO9780511801389\n18. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning aug-\nmentation strategies from data. In: The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2019)\n19. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data aug-\nmentation with a reduced search space. arXiv (2019)\n20. Dahl, G.E., Sainath, T.N., Hinton, G.E.: Improving deep neural networks for lvcsr using\nrectiﬁed linear units and dropout. In: ICASSP, pp. 8609–8613. IEEE (2013)\nIntroduction to deep learning\n23\n21. Dauphin, Y.N., de Vries, H., Chung, J., Bengio, Y.: Rmsprop and equilibrated adaptive\nlearning rates for non-convex optimization. CoRR (2015)\n22. Deng, J., Dong, W., Socher, R., jia Li, L., Li, K., Fei-fei, L.: Imagenet: A large-scale hierar-\nchical image database. CVPR (2009)\n23. Deng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive angular margin loss for deep face\nrecognition. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n(2019)\n24. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional\ntransformers for language understanding (2018)\n25. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional\ntransformers for language understanding. In: NAACL-HLT (2019)\n26. DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural networks with\ncutout. arXiv (2017)\n27. Donahue, J., Jia, Y., Vinyals, O., Hoﬀman, J., Zhang, N., Tzeng, E., Darrell, T.: Decaf: A\ndeep convolutional activation feature for generic visual recognition. In: E.P. Xing, T. Jebara\n(eds.) Proceedings of the 31st International Conference on Machine Learning, Proceedings\nof Machine Learning Research, vol. 32, pp. 647–655. PMLR, Bejing, China (2014)\n28. Dovrat, O., Lang, I., Avidan, S.: Learning to sample. In: The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) (2019)\n29. Duchi, J., Hazan, E., yORAM Singer: Adaptive subgradient methods for online learning and\nstochastic optimization. J. Mach. Learn. Res. 12, 2121–2159 (2011)\n30. Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: A survey. J. Mach. Learn.\nRes. 20, 55:1–55:21 (2018)\n31. van Engelen, J.E., Hoos, H.H.: A survey on semi-supervised learning.\nMachine Learn-\ning (2019).\nDOI 10.1007/s10994-019-05855-6.\nURL https://doi.org/10.1007/\ns10994-019-05855-6\n32. Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., Hutter, F.: Eﬃ-\ncient and robust automated machine learning. In: C. Cortes, N.D. Lawrence, D.D. Lee,\nM. Sugiyama, R. Garnett (eds.) Advances in Neural Information Processing Systems 28,\npp. 2962–2970. Curran Associates, Inc. (2015). URL http://papers.nips.cc/paper/\n5872-efficient-and-robust-automated-machine-learning.pdf\n33. Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation.\narXiv\npreprint arXiv:1409.7495 (2014)\n34. Gao, C., Gu, D., Zhang, F., Yu, Y.: Reconet: Real-time coherent video style transfer network.\nIn: Asian Conference on Computer Vision, pp. 637–653. Springer (2018)\n35. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks.\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2414–\n2423 (2016)\n36. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neural networks.\n2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 2414–2423\n(2016)\n37. Gers, F.A., Schmidhuber, J., Cummins, F.: Learning to forget: Continual prediction with lstm.\nICANN (1999)\n38. Gibiansky, A., Arik, S., Diamos, G., Miller, J., Peng, K., Ping, W., Raiman, J., Zhou, Y.: Deep\nvoice 2: Multi-speaker neural text-to-speech. In: Advances in neural information processing\nsystems, pp. 2962–2970 (2017)\n39. Goodfellow, I., Jean Pouget-Abadieand, M.M., Xu, B., Warde-Farley, D., Ozair, S., Courville,\nA., Bengio, Y.: Generative adversarial nets. In: Z. Ghahramani, M. Welling, C. Cortes, N.D.\nLawrence, K.Q. Weinberger (eds.) Advances in Neural Information Processing Systems 27,\npp. 2672–2680. Curran Associates, Inc. (2014)\n40. Greenspan, H., van Ginneken, B., Summers, R.M.: Guest editorial deep learning in medical\nimaging: Overview and future promise of an exciting new technique. CVPR 35, 1153 – 1159\n(2016)\n24\nLihi Shiloh-Perl and Raja Giryes\n41. Guo, J., Lu, S., Cai, H., Zhang, W., Yu, Y., Wang, J.: Long text generation via adversarial train-\ning with leaked information. In: Thirty-Second AAAI Conference on Artiﬁcial Intelligence\n(2018)\n42. Haim, H., Elmalem, S., Giryes, R., Bronstein, A.M., Marom, E.: Depth estimation from a\nsingle image using deep learned phase coded mask. IEEE Transactions on Computational\nImaging 4(3), 298–310 (2018)\n43. Hanocka, R., Hertz, A., Fish, N., Giryes, R., Fleishman, S., Cohen-Or, D.: Meshcnn: A\nnetwork with an edge. ACM Transactions on Graphics (TOG) 38(4), 90 (2019)\n44. He, K., Gkioxari, G., Dollár, P., Girshick, R.B.: Mask r-cnn. IEEE International Conference\non Computer Vision (ICCV) pp. 2980–2988 (2017)\n45. Hinton, G.E., Osindero, S., Teh, Y.W.: A fast learning algorithm for deep belief nets. Neural\nComput. 18(7), 1527–1554 (2006). DOI 10.1162/neco.2006.18.7.1527. URL http://dx.\ndoi.org/10.1162/neco.2006.18.7.1527\n46. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8), 1735–1780\n(1997). DOI 10.1162/neco.1997.9.8.1735\n47. Hoﬀman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A., Darrell, T.:\nCyCADA: Cycle-consistent adversarial domain adaptation. In: J. Dy, A. Krause (eds.) Pro-\nceedings of the 35th International Conference on Machine Learning, Proceedings of Machine\nLearning Research, vol. 80, pp. 1989–1998. PMLR, StockholmsmÃďssan, Stockholm Sweden\n(2018)\n48. Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni, N., Markham, A.: Randla-net:\nEﬃcient semantic segmentation of large-scale point clouds. arXiv preprint arXiv:1911.11236\n(2019)\n49. Hubel, D.H., Wiesel, T.N.: Receptive ﬁelds of single neurons in the cat’s striate cortex. Journal\nof Physiology 148, 574–591 (1959)\n50. Ioﬀe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In: Proceedings of the 32nd International Conference on Machine\nLearning, vol. 37, pp. 448–456 (2015)\n51. Jain, L.C., Medsker, L.R.: Recurrent Neural Networks: Design and Applications, 1st edn.\nCRC Press, Inc., Boca Raton, FL, USA (1999)\n52. Jakubovitz, D., Giryes, R., Rodrigues, M.R.D.: Generalization Error in Deep Learning, pp.\n153–193. Springer International Publishing, Cham (2019)\n53. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and super-\nresolution. In: European conference on computer vision, pp. 694–711. Springer (2016)\n54. Kadlec, R., Schmid, M., Bajgar, O., Kleindienst, J.: Text understanding with the attention sum\nreader network. In: Proceedings of the 54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pp. 908–918. Association for Computational\nLinguistics, Berlin, Germany (2016). DOI 10.18653/v1/P16-1086\n55. Kaelbling, L.P., Littman, M.L., Moore, A.W.: Reinforcement learning: A survey. Journal of\nartiﬁcial intelligence research 4, 237–285 (1996)\n56. Kalogerakis, E., Averkiou, M., Maji, S., Chaudhuri, S.: 3d shape segmentation with projective\nconvolutional networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) pp. 6630–6639 (2016)\n57. Karlinsky, L., Shtok, J., Harary, S., Schwartz, E., Aides, A., Feris, R., Giryes, R., Bronstein,\nA.M.: Repmet: Representative-based metric learning for classiﬁcation and few-shot object\ndetection. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n(2019)\n58. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of GANs for improved\nquality, stability, and variation. In: International Conference on Learning Representations\n(2018). URL https://openreview.net/forum?id=Hk99zCeAb\n59. Kemker, R., McClure, M., Abitino, A., Hayes, T.L., Kanan, C.: Measuring catastrophic\nforgetting in neural networks. In: Thirty-second AAAI conference on artiﬁcial intelligence\n(2018)\n60. Keskar, N.S., Socher, R.: Improving generalization performance by switching from adam to\nsgd. arXiv preprint arXiv:1712.07628 (2017)\nIntroduction to deep learning\n25\n61. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR (2014)\n62. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep convolutional\nneural networks. In: F. Pereira, C.J.C. Burges, L. Bottou, K.Q. Weinberger (eds.) Advances in\nNeural Information Processing Systems 25, pp. 1097–1105. Curran Associates, Inc. (2012)\n63. Krogh, A., Hertz, J.A.: A simple weight decay can improve generalization.\nIn: J.E.\nMoody, S.J. Hanson, R.P. Lippmann (eds.) Advances in Neural Information Processing Sys-\ntems 4, pp. 950–957. Morgan-Kaufmann (1992). URL http://papers.nips.cc/paper/\n563-a-simple-weight-decay-can-improve-generalization.pdf\n64. Kwon, D., Kim, H., Kim, J., Suh, S.C., Kim, I., Kim, K.J.: A survey of deep learning-\nbased network anomaly detection. Cluster Computing 22(1), 949–961 (2019). DOI 10.1007/\ns10586-017-1117-8\n65. Lample, G., Conneau, A., Denoyer, L., Ranzato, M.: Unsupervised machine translation using\nmonolingual corpora only. arXiv preprint arXiv:1711.00043 (2017)\n66. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.:\nBackpropagation applied to handwritten zip code recognition. Neural Comput. 1(4), 541–\n551 (1989). DOI 10.1162/neco.1989.1.4.541. URL http://dx.doi.org/10.1162/neco.\n1989.1.4.541\n67. LeCun, Y., Boser, B.E., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.E., Jackel,\nL.D.: Hand-written digit recognition with a back-propagation network. NIPS (1990)\n68. Lecun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to document\nrecognition. In: Proceedings of the IEEE, pp. 2278–2324 (1998)\n69. Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B.: Pointcnn: Convolution on x-transformed\npoints. In: NeurIPS (2018)\n70. Li, Y., Pirk, S., Su, H., Qi, C.R., Guibas, L.J.: Fpnn: Field probing neural net-\nworks\nfor\n3d\ndata.\nIn:\nD.D.\nLee,\nM.\nSugiyama,\nU.V.\nLuxburg,\nI.\nGuyon,\nR.\nGarnett\n(eds.)\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems\n29,\npp.\n307–315. Curran Associates, Inc. (2016).\nURL http://papers.nips.cc/paper/\n6416-fpnn-field-probing-neural-networks-for-3d-data.pdf\n71. Lim, S., Kim, I., Kim, T., Kim, C., Kim, S.: Fast autoaugment. In: Advances in Neural\nInformation Processing Systems (NeurIPS) (2019)\n72. Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P.: Focal loss for dense object detection. In:\nProceedings of the IEEE international conference on computer vision, pp. 2980–2988 (2017)\n73. Liu, G., Reda, F.A., andx Ting-Chun Shih, K.J.S., Tao, A., Catanzaro, B.: Image inpainting\nfor irregular holes using partial convolutions. In: The European Conference on Computer\nVision (ECCV) (2018)\n74. Liu, H., Simonyan, K., Yang, Y.: DARTS: Diﬀerentiable architecture search. In: International\nConference on Learning Representations (2019)\n75. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.E., Fu, C.Y., Berg, A.C.: Ssd: Single\nshot multibox detector. In: ECCV (2016)\n76. Liu, W., Wen, Y., Yu, Z., Li, M., Raj, B., Song, L.: Sphereface: Deep hypersphere embedding\nfor face recognition. 2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) pp. 6738–6746 (2017)\n77. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer,\nL., Stoyanov, V.: Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692 (2019)\n78. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2017)\n79. Ma, W.C., Wang, S., Hu, R., Xiong, Y., Urtasun, R.: Deep rigid instance scene ﬂow. In:\nCVPR (2019)\n80. McCulloch, W.S., Pitts, W.: A logical calculus of the ideas immanent in nervous activity. The\nbulletin of mathematical biophysics 5(4), 115–133 (1943). DOI 10.1007/BF02478259\n81. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed rep-\nresentations\nof\nwords\nand\nphrases\nand\ntheir\ncompositionality.\nIn:\nC.J.C.\nBurges,\nL.\nBottou,\nM.\nWelling,\nZ.\nGhahramani,\nK.Q.\nWeinberger\n(eds.)\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems\n26,\npp.\n3111–3119.\n26\nLihi Shiloh-Perl and Raja Giryes\nCurran\nAssociates,\nInc.\n(2013).\nURL\nhttp://papers.nips.cc/paper/\n5021-distributed-representations-of-words-and-phrases-and-their-compositionality.\npdf\n82. Minsky, M., Papert, S.: Perceptrons: An Introduction to Computational Geometry. MIT Press,\nCambridge, MA, USA (1969)\n83. Monti, F., Boscaini, D., Masci, J., Rodolà, E., Svoboda, J., Bronstein, M.M.: Geometric\ndeep learning on graphs and manifolds using mixture model cnns. In: IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR, pp. 5425–5434 (2017). DOI 10.1109/\nCVPR.2017.576\n84. Nah, S., Kim, T.H., Lee, K.M.: Deep multi-scale convolutional neural network for dynamic\nscene deblurring. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 3883–3891 (2017)\n85. Nesterov, Y.E.: A method for solving the convex programming problem with convergence\nrate o (1/kˆ 2). In: Dokl. akad. nauk Sssr, vol. 269, pp. 543–547 (1983)\n86. Niepert, M., Ahmed, M., Kutzkov, K.: Learning convolutional neural networks for graphs. In:\nProceedings of the 33rd International Conference on International Conference on Machine\nLearning - Volume 48, ICML’16, pp. 2014–2023. JMLR.org (2016). URL http://dl.acm.\norg/citation.cfm?id=3045390.3045603\n87. Noy, A., Nayman, N., Ridnik, T., Zamir, N., Doveh, S., Friedman, I., Giryes, R., Zelnik-Manor,\nL.: Asap: Architecture search, anneal and prune. arXiv preprint arXiv:1904.04123 (2019)\n88. Ongie, G., Willett, R., Soudry, D., Srebro, N.: A function space view of bounded norm\ninﬁnite width re{lu} nets: The multivariate case. In: International Conference on Learning\nRepresentations (ICLR) (2020)\n89. van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner,\nN., Senior, A., Kavukcuoglu, K.: Wavenet: A generative model for raw audio. In: Arxiv (2016).\nURL https://arxiv.org/abs/1609.03499\n90. Pan, S.J., Tsang, I.W., Kwok, J.T., Yang, Q.: Domain adaptation via transfer component\nanalysis. IEEE Transactions on Neural Networks 22(2), 199–210 (2010)\n91. Pascanu, R., Mikolov, T., Bengio, Y.: On the diﬃculty of training recurrent neural networks.\nIn: S. Dasgupta, D. McAllester (eds.) Proceedings of the 30th International Conference on\nMachine Learning, Proceedings of Machine Learning Research, vol. 28, pp. 1310–1318.\nPMLR, Atlanta, Georgia, USA (2013)\n92. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.: Deep\ncontextualized word representations. In: Proc. of NAACL (2018)\n93. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for 3d classiﬁca-\ntion and segmentation. 2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) pp. 77–85 (2016)\n94. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature learning on point\nsets in a metric space. arXiv preprint arXiv:1706.02413 (2017)\n95. Qian, N.: On the momentum term in gradient descent learning algorithms. Neural Networks\n12(1), 145–151 (1999)\n96. Radford, A., Sutskever, I.: Improving language understanding by generative pre-training. In:\narxiv (2018)\n97. Reddi, S.J., Kale, S., Kumar, S.: On the convergence of adam and beyond. In: International\nConference on Learning Representations (ICLR) (2018)\n98. Redmon, J., Divvala, S.K., Girshick, R.B., Farhadi, A.: You only look once: Uniﬁed, real-\ntime object detection. 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) pp. 779–788 (2015)\n99. Redmon, J., Farhadi, A.: Yolo9000: Better, faster, stronger. 2017 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) pp. 6517–6525 (2016)\n100. Redmon, J., Farhadi, A.: Yolov3: An incremental improvement.\nArXiv abs/1804.02767\n(2018)\n101. Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative adversarial text\nto image synthesis. In: Proceedings of the 33rd International Conference on International\nIntroduction to deep learning\n27\nConference on Machine Learning - Volume 48, ICMLâĂŹ16, p. 1060âĂŞ1069. JMLR.org\n(2016)\n102. Remez, T., Litany, O., Giryes, R., Bronstein, A.M.: Class-aware fully convolutional gaussian\nand poisson denoising. IEEE Transactions on Image Processing 27(11), 5707–5722 (2018)\n103. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with\nregion proposal networks. In: C. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama, R. Garnett\n(eds.) Advances in Neural Information Processing Systems 28, pp. 91–99. Curran Associates,\nInc. (2015)\n104. Ronneberger, O., P.Fischer, Brox, T.: U-net: Convolutional networks for biomedical image\nsegmentation. In: Medical Image Computing and Computer-Assisted Intervention (MICCAI),\nLNCS, vol. 9351, pp. 234–241. Springer (2015)\n105. Rosenblatt, F.: The perceptron: A probabilistic model for information storage and organization\nin the brain. Psychological Review pp. 65–386 (1958)\n106. Ruck, D.W., Rogers, S.K.: Feature Selection Using a Multilayer Perceptron. Journal of Neural\nNetwork Computing 2(July 1993), 40–48 (1990)\n107. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning Representations by Back-propagating\nErrors. Nature 323(6088), 533–536 (1986). DOI 10.1038/323533a0\n108. Safran, I., Eldan, R., Shamir, O.: Depth separations in neural networks: What is actually being\nseparated? In: Conference on Learning Theory (COLT), pp. 2664–2666. PMLR (2019)\n109. Schroﬀ, F., Kalenichenko, D., Philbin, J.: Facenet: A uniﬁed embedding for face recognition\nand clustering. CVPR pp. 815–823 (2015)\n110. Schwartz, E., Giryes, R., Bronstein, A.M.: Deepisp: Toward learning an end-to-end image\nprocessing pipeline. IEEE Transactions on Image Processing 28(2), 912–923 (2019). DOI\n10.1109/TIP.2018.2872858\n111. Schwartz, E., Karlinsky, L., Shtok, J., Harary, S., Marder, M., Kumar, A., Feris, R., Giryes,\nR., Bronstein, A.: Delta-encoder: an eﬀective sample synthesis method for few-shot object\nrecognition. In: S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, R. Gar-\nnett (eds.) Advances in Neural Information Processing Systems 31, pp. 2845–2855. Curran\nAssociates, Inc. (2018)\n112. Shaham, T.R., Dekel, T., Michaeli, T.: Singan: Learning a generative model from a single\nnatural image. In: The IEEE International Conference on Computer Vision (ICCV) (2019)\n113. Shen, J., Pang, R., Weiss, R.J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang,\nY., Skerrv-Ryan, R., Saurous, R.A., Agiomvrgiannakis, Y., Wu, Y.: Natural tts synthesis\nby conditioning wavenet on mel spectrogram predictions. In: International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp. 4779–4783 (2018)\n114. Shiloh, L., Eyal, A., Giryes, R.: Eﬃcient processing of distributed acoustic sensing data using\na deep learning approach. J. Lightwave Technol. 37(18), 4755–4762 (2019)\n115. Shocher, A., Bagon, S., Isola, P., Irani, M.: Ingan: Capturing and retargeting the \"dna\" of a\nnatural image. In: The IEEE International Conference on Computer Vision (ICCV) (2019)\n116. Shorten, C., Khoshgoftaar, T.M.: A survey on image data augmentation for deep learning.\nJournal of Big Data 6(1), 60 (2019). DOI 10.1186/s40537-019-0197-0\n117. Shu, R., Bui, H.H., Narui, H., Ermon, S.: A DIRT-T approach to unsupervised domain\nadaptation. In: 6th International Conference on Learning Representations, ICLR (2018)\n118. Singh, S., Krishnan, S.: Filter response normalization layer: Eliminating batch dependence\nin the training of deep neural networks. arXiv (2019)\n119. Sønderby, C.K., Raiko, T., Maaløe, L., Sønderby, S.K., Winther, O.: Ladder variational\nautoencoders. In: Advances in neural information processing systems, pp. 3738–3746 (2016)\n120. Sotelo, J., Mehri, S., Kumar, K., Santos, J.F., Kastner, K., Courville, A.C., Bengio, Y.:\nChar2wav: End-to-end speech synthesis. In: ICLR (2017)\n121. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: A\nsimple way to prevent neural networks from overﬁtting. J. Mach. Learn. Res. 15(1), 1929–\n1958 (2014)\n122. Such, F.P., Sah, S., Domínguez, M., Pillai, S., Zhang, C., Michael, A., Cahill, N.D., Ptucha,\nR.W.: Robust spatial ﬁltering with graph convolutional neural networks. IEEE Journal of\nSelected Topics in Signal Processing 11, 884–896 (2017)\n28\nLihi Shiloh-Perl and Raja Giryes\n123. Sun, Q., Liu, Y., Chua, T.S., Schiele, B.: Meta-transfer learning for few-shot learning. In: The\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)\n124. Sun, R.: Optimization for deep learning: theory and algorithms.\narXiv preprint\narXiv:1912.08957 (2019)\n125. Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P.H., Hospedales, T.M.: Learning to compare:\nRelation network for few-shot learning. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 1199–1208 (2018)\n126. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural networks. In:\nAdvances in Neural Information Processing Systems, pp. 3104–3112. Curran Associates, Inc.\n(2014)\n127. Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduction. MIT press (2018)\n128. Tan, C., Sun, F., Kong, T., Zhang, W., Yang, C., Liu, C.: A survey on deep transfer learning.\nIn: V. Kůrková, Y. Manolopoulos, B. Hammer, L. Iliadis, I. Maglogiannis (eds.) Artiﬁcial\nNeural Networks and Machine Learning – ICANN 2018, pp. 270–279. Springer International\nPublishing, Cham (2018)\n129. Tetko, I.V., Livingstone, D.J., Luik, A.I.: Neural network studies. 1. comparison of overﬁtting\nand overtraining. Journal of chemical information and computer sciences 35(5), 826–833\n(1995)\n130. Tzeng, E., Hoﬀman, J., Saenko, K., Darrell, T.: Adversarial discriminative domain adaptation.\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 2962–2971\n(2017)\n131. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u.,\nPolosukhin, I.: Attention is all you need. In: I. Guyon, U.V. Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, R. Garnett (eds.) Advances in Neural Information Processing\nSystems 30, pp. 5998–6008. Curran Associates, Inc. (2017). URL http://papers.nips.\ncc/paper/7181-attention-is-all-you-need.pdf\n132. Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.A.: Extracting and composing robust\nfeatures with denoising autoencoders. In: Proceedings of the 25th international conference\non Machine learning, pp. 1096–1103 (2008)\n133. Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural image caption\ngenerator. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n(2015)\n134. Wang, H., Wang, Y., Zhou, Z., Ji, X., Li, Z., Gong, D., Zhou, J., Liu, W.: Cosface: Large\nmargin cosine loss for deep face recognition. In: IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (2018)\n135. Wang, X., Shrivastava, A., Gupta, A.: A-fast-rcnn: Hard positive generation via adversary for\nobject detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 2606–2615 (2017)\n136. Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon, J.M.: Dynamic graph\ncnn for learning on point clouds. ACM Transactions on Graphics (TOG) (2019)\n137. Wang, Y., Yao, Q.: Generalizing from a few examples: A survey on few-shot learning. ArXiv\n(2019)\n138. Wilson, G., Cook, D.J.: A survey of unsupervised deep domain adaptation. In: arxiv (2018)\n139. Wu, Y., He, K.: Group normalization. In: The European Conference on Computer Vision\n(ECCV) (2018)\n140. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets: A deep\nrepresentation for volumetric shapes. 2015 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) pp. 1912–1920 (2014)\n141. Xu, B., Wang, N., Chen, T., Li, M.: Empirical evaluation of rectiﬁed activations in convolu-\ntional network. ArXiv (2015)\n142. Yang, C., Lu, X., Lu, Z., Shechtman, E., Wang, O., Li, H.: High-resolution image inpainting\nusing multi-scale neural patch synthesis. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 6721–6729 (2017)\nIntroduction to deep learning\n29\n143. Yang, W., Zhang, X., Tian, Y., Wang, W., Xue, J., Liao, Q.: Deep learning for single image\nsuper-resolution: A brief review.\nIEEE Transactions on Multimedia 21(12), 3106–3121\n(2019). DOI 10.1109/TMM.2019.2919431\n144. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., Le, Q.V.: Xlnet: Generalized\nautoregressive pretraining for language understanding (2019)\n145. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Free-form image inpainting with gated\nconvolution. In: The IEEE International Conference on Computer Vision (ICCV) (2019)\n146. Zeiler, M.D.: Adadelta: An adaptive learning rate method. ArXiv abs/1212.5701 (2012)\n147. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk\nminimization.\nIn: International Conference on Learning Representations (2018).\nURL\nhttps://openreview.net/forum?id=r1Ddp1-Rb\n148. Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual\nlearning of deep cnn for image denoising. IEEE Transactions on Image Processing 26(7),\n3142–3155 (2017)\n149. Zhao, H., Gallo, O., Frosio, I., Kautz, J.: Loss functions for image restoration with neural\nnetworks. IEEE Transactions on Computational Imaging 3, 47–57 (2017)\n150. Zhi Tian Chunhua Shen, H.C., He, T.: Fcos: Fully convolutional one-stage object detection.\nIn: Proceedings of the IEEE International Conference on Computer Vision (ICCV), ICCV\n’19. IEEE Computer Society (2019)\n151. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-\nconsistent adversarial networks. 2017 IEEE International Conference on Computer Vision\n(ICCV) pp. 2242–2251 (2017)\n",
  "categories": [
    "cs.LG"
  ],
  "published": "2020-02-29",
  "updated": "2020-02-29"
}