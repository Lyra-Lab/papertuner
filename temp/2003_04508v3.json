{
  "id": "http://arxiv.org/abs/2003.04508v3",
  "title": "Unsupervised Graph Embedding via Adaptive Graph Learning",
  "authors": [
    "Rui Zhang",
    "Yunxing Zhang",
    "Xuelong Li"
  ],
  "abstract": "Graph autoencoders (GAEs) are powerful tools in representation learning for\ngraph embedding. However, the performance of GAEs is very dependent on the\nquality of the graph structure, i.e., of the adjacency matrix. In other words,\nGAEs would perform poorly when the adjacency matrix is incomplete or be\ndisturbed. In this paper, two novel unsupervised graph embedding methods,\nunsupervised graph embedding via adaptive graph learning (BAGE) and\nunsupervised graph embedding via variational adaptive graph learning (VBAGE)\nare proposed. The proposed methods expand the application range of GAEs on\ngraph embedding, i.e, on the general datasets without graph structure.\nMeanwhile, the adaptive learning mechanism can initialize the adjacency matrix\nwithout be affected by the parameter. Besides that, the latent representations\nare embedded in the laplacian graph structure to preserve the topology\nstructure of the graph in the vector space. Moreover, the adjacency matrix can\nbe self-learned for better embedding performance when the original graph\nstructure is incomplete. With adaptive learning, the proposed method is much\nmore robust to the graph structure. Experimental studies on several datasets\nvalidate our design and demonstrate that our methods outperform baselines by a\nwide margin in node clustering, node classification, and graph visualization\ntasks.",
  "text": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n1\nUnsupervised Graph Embedding via Adaptive\nGraph Learning\nRui Zhang, Member, IEEE, Yunxing Zhang, Xuelong Li∗, Fellow, IEEE\nAbstract—Graph autoencoders (GAEs) are powerful tools in representation learning for graph embedding. However, the performance\nof GAEs is very dependent on the quality of the graph structure, i.e., of the adjacency matrix. In other words, GAEs would perform\npoorly when the adjacency matrix is incomplete or be disturbed. In this paper, two novel unsupervised graph embedding methods,\nunsupervised graph embedding via adaptive graph learning (BAGE) and unsupervised graph embedding via variational adaptive graph\nlearning (VBAGE) are proposed. The proposed methods expand the application range of GAEs on graph embedding, i.e, on the general\ndatasets without graph structure. Meanwhile, the adaptive learning mechanism can initialize the adjacency matrix without be affected by\nthe parameter. Besides that, the latent representations are embedded in the laplacian graph structure to preserve the topology structure\nof the graph in the vector space. Moreover, the adjacency matrix can be self-learned for better embedding performance when the original\ngraph structure is incomplete. With adaptive learning, the proposed method is much more robust to the graph structure. Experimental\nstudies on several datasets validate our design and demonstrate that our methods outperform baselines by a wide margin in node\nclustering, node classiﬁcation, and graph visualization tasks.\nIndex Terms—Graph Embedding, Adaptive Graph Learning, Graph Autoencoder\n!\n1\nINTRODUCTION\nG\nRaphs are powerful tools to seek the geometric struc-\nture of data and graph analysis has been attracting\nincreasing attention in recent years due to the ubiquity of\nnetworks in the real world. There are various applications\nusing graphs in machine learning and data mining ﬁelds\nsuch as node classiﬁcation [1] [2], node clustering [3] [4], link\nprediction [5], and visualization [6]. However, it is difﬁcult\nto directly apply the existing machine learning methods\nto graph data, due to the high computational complexity,\nlow parallelizability, and inapplicability of most methods to\ngraph data [7].\nIn response to these problems, many graph embedding\nmethods have been proposed in recent years. As one of the\nrepresentation learning, the purpose of graph embedding is\nto learn the low-dimensional feature vectors which should\npreserve the topology structure of the graph. In the early\n2000s, researchers developed graph embedding algorithms\nas part of dimensionality reduction techniques. The early\ngraph embedding methods such as Laplacian Eigenmaps\n[8] and Locally Linear Embedding (LLE) [9] map the nodes\nof the graph into a low-dimensional vector space. Since\n2010, research on graph embedding has shifted to obtain-\ning scalable graph embedding techniques that leverage the\nsparsity of real-world networks [10]. The methods at this\nstage such as Graph Factorization [11], LINE [12], HOPE\n[13], and SDNE [14] attempt to preserve both ﬁrst order and\nsecond proximities.\nIn recent years, graph neural networks (GNNs) [15]\nemerge as powerful node embedding methods with suc-\nXuelong Li∗is the corresponding author.\nRui Zhang, Yunxing Zhang, and Xuelong Li are with the School of Computer\nScience and Center for OPTical IMagery Analysis and Learning (OPTIMAL),\nNorthwestern Polytechnical University, Xi’an 710072, Shaanxi, P. R. China.\nE-mail: ruizhang8633@gmail.com, zhangyunxing423@outlook.com, and xue-\nlong li@nwpu.edu.cn\ncessful applications in broad areas such as social networks,\nrecommended systems, and natural language processing.\nGraph neural networks (GNNs) are powerful tools in repre-\nsentation learning for graphs and able to incorporate sparse\nand discrete dependency structures between data points.\nGraph neural networks (GNNs) could be roughly catego-\nrized into four categories: recurrent graph neural networks\n(RecGNNs), convolutional graph neural networks (Con-\nvGNNs), graph autoencoders (GAEs), and spatial-temporal\ngraph neural networks (STGNNs) [16].\nGraph autoencoders (GAEs) are the effective unsuper-\nvised learning frameworks that encode the node features\nand graph construction into the latent representations and\ndecode the graph construction. GAEs and most of their\nextensions rely on graph convolutional networks (GCN) to\nlearn vector space representations of nodes. GAEs can be\nused to learn graph (network) embeddings [17] and graph\ngenerative distributions [18]. For graph embedding, GAEs\nmainly learn the latent representations by reconstructing the\ngraph construction, e.g., the adjacency matrix. For graph\ngeneration, GAEs can learn the generative distribution of\ngraphs and are mostly designed to solve the molecular\ngraph generation problem [18]. The main distinction be-\ntween GAEs and graph embedding is that GAEs are de-\nsigned for various tasks while graph embedding covers\nvarious kinds of methods targeting the same task.\nAlthough GNNs achieve great success from in repre-\nsentation learning of graphs, recent studies show that the\nperformance of GNNs is very dependent on the quality\nof the adjacency matrix. In other words, GNNs will per-\nform poorly while the adjacency matrix is under attack or\nincomplete. The incomplete means the adjacency matrix is\npartially missing or be disturbed. To defend against adver-\nsarial attacks, Jin et al. [19] propose a framework Pro-GNN,\nwhich can jointly learn a structural graph and a robust graph\narXiv:2003.04508v3  [cs.LG]  23 Mar 2021\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n2\nneural network model from the perturbed graph guided by\nthese properties. For the incomplete, Chen et al. [20] propose\na graph learning framework that jointly learning the graph\nstructure and graph embeddings simultaneously.\nHowever, these methods are all designed for the super-\nvised graph neural networks (GNNs) not the unsupervised\ngraph autoencoder (GAEs). Besides that, they use k-nearest\nneighbor (kNN) to initialize the adjacency matrix when the\ngraph structure is unavailable. A major shortcoming of this\napproach is that the efﬁcacy of the resulting models hinges\non the choice of k [21]. In any case, the graph creation\nand parameter learning steps are independent and require\nheuristics and trial and error.\nIn this paper, two novel unsupervised graph embedding\nmethods, unsupervised graph embedding via adaptive graph\nlearning (BAGE) and unsupervised graph embedding via vari-\national adaptive graph learning (VBAGE) are developed. The\ncontributions can be summarized below:\n• The proposed method expands the application range\nof GAEs on graph embedding, i.e, on the general datasets\nwithout graph structure. The adaptive learning mechanism\nis able to initialize the adjacency matrix without be affected\nby the parameter k.\n• The latent representations are embedded in the lapla-\ncian graph structure to preserve the topology structure of\nthe graph in the vector space.\n• With adaptive learning, the adjacency matrix can be\nself-learned for better embedding performance which en-\nhances the robustness of the model.\n2\nRELATED WORK\nIn this section, we outline the background and develop-\nment of graph embedding and graph autoencoders (GAEs).\nThe goal of graph embedding is to learn the low-\ndimensional latent representations of nodes that preserve\nthe topological information of the graph. The early methods\nsuch as DeepWalk [22] uses a random walk to generate\nsequences of nodes from a network and transform graph\nconstruction information into linear sequences. Inspired by\nDeepWalk, DRNE [23] adopts a Long Short Term Memory\n(LSTM) network to aggregate a node’s neighbors. Similar\nto DRNE, NetRA [24] also uses the LSTM network with\nrandom walks rooted on each node and regularizes the\nlearned network embeddings within a prior distribution\nvia adversarial training. The embedding method SDNE [14]\nexploits the ﬁrst-order and second-order proximity jointly\nto preserve the network structure. The structure of SDNE is\nvery similar to the graph autoencoders and can be seen as\nan early approach of GAEs.\nThe relationship between GAEs and graph embedding\ncan be understood as: GAEs are a general term for a series\nof methods, and graph embedding is one of the tasks that\nGAE can perform. Earlier GAE approaches such as DNGR\n[25] and SDNE [14] mainly build the GAE frameworks for\ngraph embedding by multi-layer perceptrons. Nevertheless,\nDNGR and SDNE only consider node structure information\nbut ignore the node features information. In other words,\nthe early GAE methods directly learn the node embeddings\nfrom a graph where each node on this graph does not\ncontain feature information. What really kicked off graph\nautoencoder is [26] whose methods GAE and VGAE laid\nthe foundation for the later GAE methods. Inspired by\ngenerative adversarial networks (GANs) and VGAE [26],\nAdversarially Regularized Variational Graph Autoencoder\n(ARGE and ARVGE) [27] is proposed that endeavors to\nlearn an encoder that produces the empirical distribution.\nBy replacing the GCN encoder with a simple linear model,\nSalha et al. [28] propose a linear graph autoencoder (LGAE\nand LVGAE) which is more simple.\nOur methods (BAGE and VBAGE) also use the graph au-\ntoencoder to learn graph embedding. However, our method\ndiffers from these methods in the following points:\n1) Our methods can be applied to more general datasets, i.e.,\nthe dataset without the graph structure.\n2) The learned latent representations are embedded into the\nlaplacian graph structure to preserve the topology structure\nof the graph in the vector space.\n3) The adjacency matrix in our framework can be adaptively\nlearned, which enhances the robustness of the model.\nWe will detailly describe these aspects in the rest of the\npaper.\n3\nNOTATIONS AND PROBLEM STATEMENT\nBefore we present the problem statement, we ﬁrst in-\ntroduce some notations and basic concepts. The Frobenius\nnorm of a matrix A is deﬁned by ∥A∥2\nF = Σija2\nij. We use ⊙\nto denote the Hadamard product of matrices and tr(A) to\nindicate the trace of matrix A, i.e., tr(A) = P\ni aii. Epoch in\nthe paper means the number of iteration.\nLet\nG = {V, E, X}\nbe\na\ngraph,\nwhere\nV\n=\n{v1, v2, . . . , vn} is a set of nodes with |V| = n and E is\nthe set of connecting edges among each node. The edges\ndescribe the relations between nodes and can also be repre-\nsented by an adjacency matrix A = [aij] ∈Rn×n where aij\ndenotes the relation between nodes vi and vj. Furthermore,\nwe use X = [x1, x2, · · · , xn]T ∈Rn×m to denote the node\nfeature matrix where xi is the feature vector of node vi. The\nnumber of nodes is n and m is the dimension of the raw\ndata.\nThe aim of graph embedding in our method is to learn\nthe low-dimensional latent representation Z ∈Rn×f from\nthe node matrix V with the formal format as: f : (A, X) 7→\nZ. The learned latent representation Z in latent space should\npreserve the topological structure of the graph as well as\nnode feature information.\n4\nFRAMEWORK\nFig. 1 illustrates the workﬂow of our methods (BAGE\nand VBAGE) that consists of three modules: the graph\nconvolutional encoder, the decoder, and the laplacian graph\nstructure.\n4.1\nGraph Convolutional Encoder Model\nThe encoder model learns a layer-wise transformation by\na spectral graph convolutional function f\n\u0010\nZ(l), A|W(l)\u0011\n:\nZ(l+1) = f(Z(l), A|W(l)),\n(1)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n3\nX\nA\n(M/2)\nZ=Z\n*\nσ(\n)\nZ\nmap\ni\nL\nL\ni\nA\nAdaptive Learning\nL\nA\nT\nZ\nEncoder\nLatent Representation\nDecoder\nInitialization\n)\nˆA\nFig. 1: The architecture of the proposed framework (See Algorithm 1 for details).\nwhere Z(l) is the input for convolution and Z(l+1) is the\noutput after convolution. W(l) is the weight parameter\nmatrix that needs to be learned in the neural network. In this\npaper, Z(0) = X ∈Rn×m is the input node features matrix.\nSpeciﬁcally speaking, each layer of our graph convolutional\nnetwork can be calculated as follows:\nf\n\u0010\nZ(l), A|W(l)\u0011\n= φ\n\u0010\neD−1\n2 eA eD−1\n2 Z(l)W(l)\u0011\n.\n(2)\nHere, eA = A + I, eDii = P\nj eAij, I is the identity matrix\nof A, and φ is the activation function such as Relu(t) =\nmax(0, t).\n• The Graph Encoder in our method (BAGE) is like [26]\nand [17], which is constructed as follows:\nZ(1) = fRelu(X, A|W(0));\n(3)\nZ(2) = flinear (Z(1), A|W(1)).\n(4)\n• The Variational Graph Encoder in our method (VBAGE)\nis deﬁned by an inference model:\nq(Z | X, A) =\nn\nY\ni=1\nq (zi | X, A) ,\nq (zi | X, A) = N\n\u0000zi | µi, diag\n\u0000σ2\u0001\u0001 .\n(5)\nHere, µ = Z(2) is the matrix of mean vectors zi; similarly\nlog σ = flinear\n\u0010\nZ(1), A | W′(1)\u0011\nwhich share the weights\nW(0) with µ in the ﬁrst layer in Eq. (3).\n4.2\nDecoder Model\nThe decoder model reconstructs the graph from the\nlearned latent representations and the reconstructed adja-\ncency matrix bA can be represented as follows:\nbA = sigmoid(ZZT),\n(6)\nwhere\nZ\nis\nthe\nlatent\nrepresentation\nand\nZ\n=\nencoder (Z|X, A).\nIn terms of loss function, we did not use the cross-\nentropy loss function to deﬁne the reconstruction loss like\n[26] and [17]. We impose more penalty on the reconstruction\nerror of the non-zero elements than that of zero elements.\n• The reconstruction loss of the graph construction for\nBAGE is calculated as:\nLG1 =\nn\nX\ni=1\n∥(ai −ˆai) ⊙bi∥2\n2 ,\n= ∥(A −bA) ⊙B∥2\nF ,\n(7)\nwhere ⊙means the Hadamard product, bi = {bi,j}n\nj=1 . If\nai,j = 0, bi,j = 1, else bi,j = β > 1.\n• The reconstruction loss of the graph construction for\nVBAGE is calculated as:\nLG2 = ∥(A −bA) ⊙B∥2\nF −KL[q(Z | X, A)∥p(Z)],\n(8)\nwhere KL[q(•)∥p(•)] is the Kullback-Leibler divergence be-\ntween q(•) and p(•). We also take a Gaussian prior p(Z) =\nQ\ni p (zi) = Q\ni N (zi | 0, I) like [17] and [26].\n4.3\nLaplacian Graph Structure\nThe reconstruction loss in Eqs. (7) and (8) is only focused\non graph reconstruction while ignoring latent representa-\ntions. Aim at this, we embed the latent representations into\nthe laplacian graph structure. The loss function for this goal\nis deﬁned as follows:\nLL =\nn\nX\ni,j=1\n\u0010\n∥zi −zj∥2\n2 aij + γia2\nij\n\u0011\n= tr(ZLZT ) + γ∥A∥2\nF\ns.t.\naT\ni 1 = 1, 0 ≤ai ≤1,\n(9)\nwhere γ is a regularization parameter that can be adaptively\nsolved. The laplacian graph structure is also the basis of the\nadaptive learning of the adjacency matrix that is also the\nbiggest contribution of this paper.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n4\nThe objective function of Eq. (9) borrows the idea of\nLaplacian Eigenmaps [29], which incurs a penalty when\nsimilar latent representations are far away in the embedding\nspace. The Laplacian matrix L is deﬁned as:\nL = D −A,\n(10)\nwhere the D is the degree matrix whose diagonal element\ndii = P\nj aij.\nSimilar ideas appear in some works on graph learning\nand network embedding. The difference between our meth-\nods and them is that we directly incorporate the laplacian\ngraph structure into the latent representations, not into\nthe feature matrix, like CAN [30] or into the label matrix,\nlike SDNE [14]. Thus laplacian graph structure can make\nthe vertexes linked by an edge be mapped near in the\nembedding space.\n• In summary, the loss function of the BAGE can be\nwritten as:\nLBAGE = LG1 + λLL + νLreg.\n(11)\n• And the loss function of the VBAGE can be written as:\nLV BAGE = LG2 + λLL + νLreg,\n(12)\nwhere Lreg is the ℓ2-norm regularizer term with coefﬁcient\nν to prevent overﬁtting, which is deﬁned as follows:\nLreg = 1\n2\nX\ni\n(∥W(i)∥2\nF ).\n(13)\n5\nADAPTIVE LEARNING OF THE ADJACENCY MA-\nTRIX\nThe most important contribution of this paper is the\nadaptive learning of the adjacency matrix that tries to an-\nswer the following two questions:\n• When the original graph structure is incomplete, can\nwe learn an alternative graph structure to obtain better\nembedding effects?\n• When we apply graph convolution to the data without\nan initial graph structure, can we get the graph structure\nthrough adaptive learning rather than kNN initialization?\n5.1\nThe Solution of The Adjacency Matrix\nIn practical applications, the adjacency matrix with ad-\njustable sparsity tends to bring better results. And that is\na reason why we do not update the adjacency matrix by\nback-propagation algorithm directly, which will produce\na meaningless dense matrix. The adaptive learning of the\nadjacency matrix is based on the laplacian graph structure\nin Eq. (9) as follows:\nmin\naij\nn\nX\ni,j=1\n\u0010\n∥zi −zj∥2\n2 aij + γia2\nij\n\u0011\ns.t.\naT\ni 1 = 1, 0 ≤ai ≤1.\n(14)\nLet us denote the distance between two nodes as hij, i.e.,\nhij = ∥zi −zj∥2\n2. The j-th element of vector hi ∈Rn×1\nis j-th element hij and ai ∈Rn×1 is a vector with its j-\nth element aij. The Lagrange equation of problem (14) is\nrepresented as:\nL(ai, η, ζi) = 1\n2\n\r\r\r\rai + hi\n2γi\n\r\r\r\r\n2\n2\n−η(aT\ni 1 −1) −ζT\ni ai,\n(15)\nwhere η and ζi ≥0 are the Lagrange multipliers. Using the\nKarush-Kuhn-Tucker (KKT) conditions, we can derive the\noptimal solution of aij as:\naij =\n\u0012\n−hij\n2γi\n+ η\n\u0013\n+\n,\n(16)\nwhere (•)+ = max(•, 0). To equip the adjacency matrix with\nadjustable sparsity, we take only k nodes points closest to zi\ninto consideration and the parameter k is responsible for\nadjusting the sparsity of the adjacency matrix. Therefore, ai\nsatisﬁes aik > 0 ≥ai,k+1 as:\n(\naik > 0 ⇒−hik\n2γi + η > 0\nai,k+1 ≤0 ⇒−hi,k+1\n2γi\n+ η ≤0 .\n(17)\nAccording to Eq. (16) and the constraint aT\ni 1 = 1, we have:\nk\nX\nj=1\n\u0012\n−hij\n2γi\n+ η\n\u0013\n= 1 =⇒η = 1\nk +\n1\n2kγi\nk\nX\nj=1\nhij.\n(18)\nThe overall γ is set to the mean of γi and it can be learned\nadaptively as:\nγ = 1\nn\nn\nX\ni=1\n\nk\n2hi,k+1 −1\n2\nk\nX\nj=1\nhij\n\n.\n(19)\nWithout loss of generality, let us suppose hi1, hi2, . . . , hin\nare ordered from small to large. Then the adjacency matrix\ncan be solved as:\naij =\n \nhi,k+1 −hij\nkhi,k+1 −Pk\nj=1 hij\n!\n+\n.\n(20)\nBased on Eq. (20), the adjacency matrix A can be updated\nto AL by the latent representations. What is more, the\nadjacency matrix can also be initialized by Eq. (20) when\nthere is no initial graph structure.\n5.2\nUpdate The Adjacency Matrix\nEq. (20) provides a solution to the graph structure when\nthe adjacency matrix does not exist or is incomplete. Dur-\ning the iterative process, the learned latent representations\ncan be used to calculate the adjacency matrix for the next\niteration. However, it is harmful to discard the initial graph\nstructure totally since the optimal graph structure is poten-\ntially a small shift from the initial graph structure [20].\nWith this assumption, we combine the learned graph\nstructure with the initial graph structured as follows:\nA = αAL + (1 −α)A0,\n(21)\nwhere A0 is initial adjacency matrix and AL is the learned\nadjacency matrix in the iteration. A hyperparameter α is\nused to balance the trade-off between the learned graph\nstructure and the initial graph structure. Moreover, we set a\nthreshold τ for stopping updates, i.e., if (epoch > τ), then\nthe updating stop.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n5\n5.3\nDistribution of Parameter k\nThere is only one parameter k in Eq. (20). If we set k as\na ﬁxed hyperparameter, it will cause all samples to have\nthe same number of neighbors, i.e., the number of non-\nzero elements in each row of the adjacency matrix is the\nsame. It will bring the same disadvantage as kNN since the\nnumber of neighbors for each sample is mostly different in\nreal-world graph data.\nInstead of ﬁxing k, we make k to be a parameter that can\nbe learned adaptively. Since the k represents the number\nof neighbors for each sample, i.e., the number of non-zero\nelements in each row of the adjacency matrix, we sample k\nfrom the normal distribution. k obeys a normal distribution\nwith a mean µk and a variance of 1, i.e., k ∼N(uk, 1).\nThe value of µk is the number of neighbors for the sample\nin the former iteration. Besides that, we set a maximum and\nminimum limit on k to keep it in a reasonable range. Speciﬁc\nto Eq. (20), the number of neighbors for each sample in the\nadjacency matrix will be dynamically obtained during the\nupdating.\n6\nOPTIMIZATION\nTo optimize the aforementioned models, the goal is to\nminimize the loss of LBAGE 11 and LV BAGE 12 which are\nthe function of the neural network parameter W. The cal-\nculation of the partial derivative of LBAGE 11 and LV BAGE\n12 are estimated using the back-propagation. Furthermore,\nthe proposed framework is optimized by adaptive moment\nestimation (Adam), where T is the maximum iteration num-\nber.\nAs for the adaptive learning of the adjacency matrix, the\ntime complexity of the updating process is O((dn2)τ). To\nshow clearly the information transmission in the adaptive\nlearning process, Fig. 2 is made to show the information\nﬂow of the learned adjacency matrix A and the intermediate\nnode embedding matrix Z during the iterative procedure.\nThe presence of the blue arrow from X to A(0) depends on\nwhether there is a graph structure in the initial data. The\npseudocode of our methods is summarized in Algorithm 1.\nX\n0\nA（）\nZ（1）\nA（1）\nZ（2）\nA（2）\nτ\nA（）\nτ\nZ（）\nτ\nA（）\nτ+1\nZ（\n）\nT\nZ（）\nFig. 2: Information ﬂow of the adaptive learning process.\n7\nEXPERIMENTS\nWe report our results on three tasks: node clustering,\nnode classiﬁcation, and graph visualization. The benchmark\ndatasets used in this paper include two real-world graph\ndatasets and six general datasets. The results not only\ndemonstrate the advantage of our methods but also support\nthe effectiveness of the adaptive learning of the adjacency\nmatrix.\nAlgorithm 1 Algorithm to our methods (BAGE and VBAGE)\nInput: Node features matrix X, maximum iteration num-\nber T, parameters λ, α and τ.\nInitialization:\n1: if There is the initial graph structure in the data:\n2:\nInput G = {V, E, X}\n3: else:\n4:\nInitialize the adjacency matrix with Eq. (20)\n5: end\nOptimization:\n6: for epoch = 1,2,3, . . . , T do:\n7:\nif epoch = 1,2,3, . . . , τ:\n8:\nUpdate the adjacency matrix by Eqs. (20) and (21)\n9:\nend\n10:\nCompute the loss of BAGE by Eq. (11)\n11:\nCompute the loss of VBAGE by Eq. (12)\n12:\nBackpropagate loss and update W\n13: end\nOutput: Latent representation Z\n7.1\nDatasets\nGraph Datasets. In the node clustering task, two graph\ndatasets Cora and Citeseer are used. In order to verify\nthe effectiveness of adaptive learning in our method, we\nperform incomplete processing on the adjacency matrix on\nthese two graph datasets. Incomplete processing means that\nthe elements in the adjacency matrix are randomly set to\n0 with a certain probability (Missing Ratio). The detailed\ninformation of the two graph datasets is shown in Table 1.\nGeneral Datasets. In the node classiﬁcation task, six general\ndatasets are used. The is no graph structure in these datasets\nwhere we initialize the adjacency matrix by kNN for other\nmethods which are based on the graph convolutional neural\nnetwork. The purpose of using these datasets is to verify the\nsuperiority of the adaptive learning of the adjacency matrix\nin our methods. The detailed information of the six general\ndatasets is shown in Table 2.\n7.2\nCompetitors\nWe compare our algorithms against several state-of-the-\nart algorithms:\n• LGAE and LVGAE [28]: is the latest improvement\nto GAE and VGAE, which replace the GCN encoder by a\nsimple linear model w.r.t. the adjacency matrix of the graph.\n• ARGE and ARVGE [17]: is the adversarial graph\nembedding framework that enforces latent representations\nto match a prior distribution, which is achieved by an\nadversarial training mode.\n• GAE and VGE [26]: is the classical graph autoen-\ncoder and variational graph autoencoder. And it is the ﬁrst\nmethod to to apply graph convolution in the autoencoder.\n• SDNE [14]: exploits the ﬁrst order and second order\nproximity jointly to preserve the network structure and we\nuse it as a baseline.\n• Spectral Clustering [31]: is a famous graph based\nclustering method and we use it as a baseline.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n6\nTABLE 1: Information of the two graph datasets.\nDataset\n# Nodes\n# Links\n# Content Words\n# Features\n# Missing Ratio\nCora\n2,708\n5,429\n3,880,564\n1,433\n{0%, 5%, 10%, 15%, 20%, 25%, 50%}\nCiteseer\n3,327\n4,732\n12,274,336\n3,703\n{0%, 5%, 10%, 15%, 20%, 25%, 50%}\nTABLE 2: Information of the six general datasets.\nDatasets\nIMM\nATT\nUMIST\nCOIL\nUSPS\nPIE\n# Nodes\n240\n400\n575\n1,440\n2007\n3332\n# Features\n1,024\n1024\n1024\n1,024\n256\n4096\n• k-means is the base of many clustering methods. Here\nwe run k-means on raw node features as a baseline.\n7.3\nTask 1: Node Clustering\nLike most papers based on the graph convolutional\nneural\nnetwork,\nwe\napply\nour\nmethods\nand\nother\ncompetitors on the node clustering task to verify the\neffect of graph embedding.\nParameter Settings and Study. For the Cora and Citeseer\ndatasets, we train all autoencoder-related models for 200\niterations and optimize them with the Adam algorithm.\nThe learning rate for the BAGE is 0.0001 and VBAGE is\n0.001. When the adjacency matrix is incomplete, the scale\nof adaptive learning α is 10% and the number of adaptive\nlearning iterations (τ) is limited to 10-15. The λ is set as 0.01,\nand the elements of weight matrix β is set as 20. What’s\nmore, the parameters for other competitors are set to the\nvalues that make the best experimental results.\nAs for the impact of parameters, we study the inﬂuence\nof parameters λ and β on the experimental results. The\nresults and analysis of the parameter study are merged into\nthe next subsection.\nMetrics. To verify the effect of graph embedding, we run\nk-means on the learned latent representations to perform\nthe node clustering task. We run k-means 10 times with\ndifferent initializations and report the mean and standard\ndeviation of all methods. To validate the clustering results,\nwe employ two metrics: Accuracy (ACC) and Normalized\nMutual Information (NMI).\nExperimental Results and Analysis. The details of the\nexperimental results on the node clustering are given in\nTables 3 and 4, where the best and second results have been\nhighlighted. Since the performance of competitors SDNE,\nSpectral Clustering (SC), and k-means is not very good, the\nresults of those baselines are listed in a separate Table 5. The\nresults observations are as follows:\n1) Our methods outperform all other competitors on the\nCora dataset regardless of whether the adjacency matrix is\nincomplete.\n2) In the Cora dataset, our methods (BAGE and VBAGE)\nare nearly 4%-6% higher than the second place (ARGE\nand ARVGE) when the missing ratio is little, and 7%-9%\nhigher than the second place (ARGE and ARVGE) when the\nmissing ratio is large.\n3) In the Citeseer dataset, our methods perform 3%-6%\nhigher than the other competitors when the missing ratio is\nless than 50%.\n7.4\nTask 2: Node Classiﬁcation\nIn this task, we apply all methods on six general datasets\nto learn the latent representations which are then applied\nto the node classiﬁcation task to verify the quality of the\nembedding results.\nParameter Settings and Study. All the parameter settings\nfor our methods in this task are the same as in the node\nclustering task. As for the parameters in other competitors,\nthey are set as the values that make the best experimental\nresults.\nFor the impact of the parameters, we study the inﬂuence\nof parameters λ and β on the experimental results. The\nvalue range of λ is {0.001, 0.01, 0.1, 1, 10} and the value\nrange of β in weight matrix B is {1, 10, 20, 30, 40}. The\nevaluation index is F1-score that is commonly used in the\nclassiﬁcation task. Fig. 5 shows the results of parameters\nstudy where the results show that the parameters’ inﬂuence\non our method on the node classiﬁcation task is small.\nMetrics. To verify the effect of graph embedding, we run\nSVM on the learned latent representations to perform the\nclassiﬁcation task. The latent representations are divided\ninto a training set (70%) and a test set (30%). We ﬁrst\nperform 10-fold cross-validation to select the best SVM\nmodel, and then we apply the selected SVM model on\nthe test set. To validate the node classiﬁcation results, we\nemploy the F1-score as the metric.\nExperimental Results and Analysis. The details of the\nexperimental results on the node classiﬁcation task are\ndisplayed in Table 6 where the best and second results have\nbeen highlighted. Due to space constraints, we only select a\nfew outstanding competitors, i.e., ARGE, LGAE, GAE, and\nVGAE, to show in Table 6. The node classiﬁcation results in\nTable 6 show the superiority of our method.\n1) The performance of BAGE on the six general datasets\nhas always remained in the top two and our method (BAGE\nand VBAGE) is higher than other methods 3%-6% in the\nCOIL dataset.\n2) In UMIST and USPS, our methods (BAGE and\nVBAGE) perform better than GAE, ARGE, and LGAE and\nthe performance of our method occupies the top two.\n3) In the PIE dataset, BAGE can achieve 95% when other\nmethods can only reach 91%.\nIn summary, the experiment results in the node classiﬁ-\ncation task fully illustrate the rationality and superiority of\nthe adaptive learning of the adjacency matrix in our method.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n7\nTABLE 3: The node clustering results on the Cora dataset.\nMethods\nMetrics\n0%\n5%\n10%\n15%\n20%\n25%\n50%\nGAE\nACC(%)\n59.53±2.00\n58.27±3.68\n55.13±2.74\n57.19±2.30\n54.57±1.91\n55.58±3.60\n48.77±2.73\nNMI(%)\n40.48±1.09\n38.82±1.78\n32.64±1.12\n36.67±1.77\n35.10±1.61\n36.98±1.69\n25.23±1.60\nVGAE\nACC(%)\n58.58±4.02\n57.64±2.82\n56.48±3.84\n56.64±2.31\n56.88±2.92\n56.95±4.56\n54.75±4.31\nNMI(%)\n38.46±2.22\n37.39±1.59\n38.14±2.72\n35.42±1.35\n36.80±2.44\n35.05±3.26\n33.00±2.91\nARGE\nACC(%)\n68.64±0.60\n67.66±0.69\n65.73±1.62\n63.37±2.54\n60.69±1.20\n59.44±1.78\n55.21±0.52\nNMI(%)\n49.22±0.17\n46.30±0.91\n45.12±1.22\n44.07±1.72\n42.07±0.30\n39.52±0.83\n34.33±0.42\nARVGE\nACC(%)\n67.66±0.26\n66.60±0.80\n65.99±1.12\n62.44±2.31\n58.47±0.91\n60.70±0.08\n56.32±1.33\nNMI(%)\n48.17±0.21\n47.24±0.42\n47.20±0.67\n44.07±0.65\n41.75±0.09\n42.27±0.14\n33.92±0.71\nLGAE\nACC(%)\n58.42±0.90\n57.46±3.93\n57.82±1.64\n53.02±2.47\n51.55±2.81\n54.66±2.86\n45.12±2.35\nNMI(%)\n36.63±0.49\n39.71±1.39\n39.34±0.84\n36.62±1.44\n32.89±0.92\n35.46±0.94\n23.84±1.28\nLVGAE\nACC(%)\n57.10±2.29\n56.25±2.75\n57.82±2.93\n54.08±3.87\n55.00±2.84\n55.92±1.86\n49.68±1.63\nNMI(%)\n32.77±1.63\n30.71±1.10\n32.08±1.23\n30.26±2.25\n29.41±1.42\n29.93±0.77\n23.50±0.68\nBAGE\nACC(%)\n72.57±0.68\n71.21±0.18\n70.32±0.33\n72.74±0.12\n69.43±2.27\n66.59±0.87\n64.22±0.29\nNMI(%)\n56.91±0.49\n54.73±0.15\n52.39±0.17\n52.43±0.24\n49.96±0.79\n51.88±0.54\n48.53±0.25\nVBAGE\nACC(%)\n73.11±0.25\n71.97±0.05\n71.45±0.87\n71.23±0.19\n70.72±0.27\n68.25±2.42\n64.86±1.18\nNMI(%)\n55.66±0.14\n54.13±0.23\n53.15±0.28\n51.12±0.24\n51.02±0.32\n49.87±1.29\n44.85±0.21\nTABLE 4: The node clustering results on the Citeseer dataset.\nMethods\nMetrics\n0%\n5%\n10%\n15%\n20%\n25%\n50%\nGAE\nACC(%)\n54.54±3.45\n54.19±2.62\n50.95±2.27\n49.17±1.50\n50.96±1.76\n44.25±2.76\n43.51±2.81\nNMI(%)\n27.04±1.53\n24.40±2.12\n21.49±1.44\n20.55±1.09\n22.33±1.28\n16.25±1.62\n16.18±1.73\nVGAE\nACC(%)\n53.96±0.98\n53.73±1.43\n52.84±1.27\n50.24±0.88\n51.78±1.28\n47.93±2.50\n41.87±2.81\nNMI(%)\n23.36±0.82\n23.87±0.96\n22.66±0.89\n21.48±0.97\n22.71±0.93\n18.89±1.55\n14.29±1.35\nARGE\nACC(%)\n58.67±1.20\n57.95±0.62\n54.80±1.19\n50.49±1.68\n51.66±0.99\n51.40±1.42\n45.42±0.14\nNMI(%)\n31.33±0.85\n30.69±0.41\n27.75±1.18\n24.17±1.08\n24.81±0.55\n24.18±0.82\n20.26±0.26\nARVGE\nACC(%)\n49.85±0.60\n51.43±0.50\n49.93±0.10\n48.14±0.11\n50.04±0.20\n52.14±0.88\n42.96±1.88\nNMI(%)\n24.67±0.42\n25.10±0.48\n24.15±0.27\n22.71±0.28\n25.42±0.20\n25.58±0.43\n17.28±1.41\nLGAE\nACC(%)\n56.66±0.96\n56.93±0.88\n56.58±0.94\n55.08±1.14\n55.63±1.01\n54.74±0.42\n46.83±1.00\nNMI(%)\n27.22±0.59\n28.41±0.73\n27.00±0.86\n26.67±0.88\n26.38±0.91\n25.21±0.34\n28.48±0.54\nLVGAE\nACC(%)\n50.86±1.05\n50.83±1.27\n50.06±0.69\n49.07±1.09\n49.91±1.57\n48.82±0.82\n28.97±0.41\nNMI(%)\n22.55±0.48\n21.38±0.41\n22.03±0.55\n20.19±0.38\n20.85±0.64\n19.45±0.36\n10.31±0.06\nBAGE\nACC(%)\n64.64±0.19\n62.19±0.34\n58.83±2.23\n57.96±1.14\n59.64±0.48\n55.18±2.12\n39.91±3.57\nNMI(%)\n39.07±0.30\n37.99±0.21\n36.03±1.13\n32.13±0.61\n35.17±0.35\n26.17±0.98\n14.62±1.85\nVBAGE\nACC(%)\n63.13±0.16\n62.73±0.28\n60.91±0.19\n58.40±0.62\n58.80±0.41\n55.87±2.11\n35.75±1.54\nNMI(%)\n36.92±0.12\n37.62±0.15\n35.81±0.16\n33.79±0.26\n27.81±0.22\n26.89±0.82\n13.11±0.42\nTABLE 5: The node clustering results for three baselines.\nMethods\nMetrics\nSDNE\nSC\nk-means\nCora\nACC(%)\n41.52±3.38\n38.08±0.04\n37.22±3.91\nNMI(%)\n20.17±2.39\n15.99±0.13\n18.87±3.51\nCiteseer\nACC(%)\n30.21±0.67\n21.46±0.00\n43.80±5.83\nNMI(%)\n4.44±0.33\n1.72±0.00\n20.63±4.67\n7.5\nTask 3: Graph Visualization\nIn this task, we apply our methods and several out-\nstanding competitors to perform visualization in two graph\ndatasets. We visualize the Cora and Citeseer datasets in\na two-dimensional space by applying the t-SNE [32] algo-\nrithm on the learned embeddings.\nThe results in Figs. 3 and 4 validate that by applying\nadaptive learning of the adjacency matrix, we can obtain a\nmore meaningful layout of the graph data.\n8\nCONCLUSION\nIn this paper, we propose two novel unsupervised graph\nembedding methods, unsupervised graph embedding via adap-\ntive graph learning (BAGE) and unsupervised graph embedding\nvia variational adaptive graph learning (VBAGE). Aiming at the\nproblem that the existing GAE methods are sensitive to the\nadjacency matrix, we embed the adaptive learning to the\nframework, which enhances the robustness of the model. In\naddition, the adaptive learning mechanism expands the ap-\nplication range of GAEs on graph embedding and is able to\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n8\n(a) BAGE\n(b) VBAGE\n(c) ARGE\n(d) GAE\n(e) LGAE\nFig. 3: The data visualization comparison on Cora.\n(a) BAGE\n(b) VBAGE\n(c) ARGE\n(d) GAE\n(e) LGAE\nFig. 4: The data visualization comparison on Citeseer.\n1\n10\n20\n30\n40\n0\n20\n40\n60\n80\n100\nF1-score\nVBAGE\nBAGE\n0.001\n0.01\n0.1\n1\n10\n0\n20\n40\n60\n80\n100\nF1-score\nVBAGE\nBAGE\nFig. 5: The parameters study results on COIL dataset.\nTABLE 6: The F1-score (%) on the node classiﬁcation task.\nMethods\nBAGE\nVBAGE\nARGE\nLGAE\nGAE\nVGAE\nCOIL\n97.50\n98.15\n94.33\n93.85\n93.86\n92.84\nATT\n86.59\n87.54\n83.87\n83.50\n78.48\n76.24\nIMM\n36.71\n25.81\n32.39\n30.05\n20.43\n19.54\nUMIST\n96.46\n95.92\n92.39\n91.80\n87.86\n85.88\nUSPS\n94.19\n93.86\n92.88\n93.06\n93.37\n92.89\nPIE\n95.29\n91.00\n91.63\n91.69\n87.69\n87.54\ninitialize the adjacency matrix without be affected by the pa-\nrameter k. Furthermore, the learned latent representations\nare embedded in the laplacian graph structure to preserve\nthe topology structure of the graph in the vector space.\nExperimental studies on several datasets demonstrated that\nour methods (BAGE and VBAGE) outperform baselines by\na wide margin in node clustering, node classiﬁcation, and\ngraph visualization tasks.\nREFERENCES\n[1]\nZhengyang Wang and Shuiwang Ji,\n“Second-order pooling for\ngraph neural networks,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2020.\n[2]\nLu Bai, Lixin Cui, Yuhang Jiao, Luca Rossi, and Edwin Han-\ncock, “Learning backtrackless aligned-spatial graph convolutional\nnetworks for graph classiﬁcation,”\nIEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2020.\n[3]\nZhen Wang, Zhaoqing Li, Rong Wang, Feiping Nie, and Xuelong\nLi, “Large graph clustering with simultaneous spectral embedding\nand discretization,”\nIEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2020.\n[4]\nXuelong Li, Han Zhang, Rong Wang, and Feiping Nie, “Multi-\nview clustering: A scalable and parameter-free bipartite graph\nfusion method,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2020.\n[5]\nJunming Shao, Zhong Zhang, Zhongjing Yu, Jun Wang, Yi Zhao,\nand Qinli Yang, “Community detection and link prediction via\ncluster-driven low-rank matrix completion.,” in IJCAI, 2019, pp.\n3382–3388.\n[6]\nJunyu Gao, Tianzhu Zhang, and Changsheng Xu,\n“Learning\nto model relationships for zero-shot video classiﬁcation,”\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2020.\n[7]\nPeng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu, “A survey on\nnetwork embedding,”\nIEEE Transactions on Knowledge and Data\nEngineering, vol. 31, no. 5, pp. 833–852, 2018.\n[8]\nMikhail Belkin and Partha Niyogi,\n“Laplacian eigenmaps and\nspectral techniques for embedding and clustering,” in Advances\nin neural information processing systems, 2002, pp. 585–591.\n[9]\nSam T Roweis and Lawrence K Saul, “Nonlinear dimensionality\nreduction by locally linear embedding,” science, vol. 290, no. 5500,\npp. 2323–2326, 2000.\n[10] Palash Goyal and Emilio Ferrara, “Graph embedding techniques,\napplications, and performance: A survey,” Knowledge-Based Sys-\ntems, vol. 151, pp. 78–94, 2018.\n[11] Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy,\nVanja Josifovski, and Alexander J Smola, “Distributed large-scale\nnatural graph factorization,” in Proceedings of the 22nd international\nconference on World Wide Web, 2013, pp. 37–48.\n[12] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and\nQiaozhu Mei,\n“Line: Large-scale information network embed-\nding,” in Proceedings of the 24th international conference on world\nwide web, 2015, pp. 1067–1077.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n9\n[13] Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu\nZhu, “Asymmetric transitivity preserving graph embedding,” in\nProceedings of the 22nd ACM SIGKDD international conference on\nKnowledge discovery and data mining, 2016, pp. 1105–1114.\n[14] Daixin Wang, Peng Cui, and Wenwu Zhu,\n“Structural deep\nnetwork embedding,”\nin Proceedings of the 22nd ACM SIGKDD\ninternational conference on Knowledge discovery and data mining, 2016,\npp. 1225–1234.\n[15] Thomas N Kipf and Max Welling,\n“Semi-supervised classi-\nﬁcation with graph convolutional networks,”\narXiv preprint\narXiv:1609.02907, 2016.\n[16] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long,\nChengqi Zhang, and S Yu Philip, “A comprehensive survey on\ngraph neural networks,” IEEE Transactions on Neural Networks and\nLearning Systems, 2020.\n[17] Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and\nChengqi Zhang,\n“Adversarially regularized graph autoencoder\nfor graph embedding.,” in IJCAI, 2018, pp. 2609–2615.\n[18] Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter\nBattaglia, “Learning deep generative models of graphs,” arXiv\npreprint arXiv:1803.03324, 2018.\n[19] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and\nJiliang Tang, “Graph structure learning for robust graph neural\nnetworks,” arXiv preprint arXiv:2005.10203, 2020.\n[20] Yu Chen, Lingfei Wu, and Mohammed J Zaki,\n“Deep iterative\nand adaptive learning for graph neural networks,” arXiv preprint\narXiv:1912.07832, 2019.\n[21] Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao\nHe, “Learning discrete structures for graph neural networks,” in\nICML, 2019.\n[22] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena,\n“Deepwalk:\nOnline learning of social representations,”\nin Proceedings of the\n20th ACM SIGKDD international conference on Knowledge discovery\nand data mining. ACM, 2014, pp. 701–710.\n[23] Ke Tu, Peng Cui, Xiao Wang, Philip S Yu, and Wenwu Zhu,\n“Deep recursive network embedding with regular equivalence,”\nin Proceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining. ACM, 2018, pp. 2357–2366.\n[24] Wenchao Yu, Cheng Zheng, Wei Cheng, Charu C Aggarwal,\nDongjin Song, Bo Zong, Haifeng Chen, and Wei Wang, “Learning\ndeep network representations with adversarially regularized au-\ntoencoders,” in Proceedings of the 24th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining. ACM, 2018, pp.\n2663–2671.\n[25] Shaosheng Cao, Wei Lu, and Qiongkai Xu, “Deep neural networks\nfor learning graph representations,” in Thirtieth AAAI Conference\non Artiﬁcial Intelligence, 2016.\n[26] Thomas N Kipf and Max Welling,\n“Variational graph auto-\nencoders,” NIPS Workshop on Bayesian Deep Learning, 2016.\n[27] Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and\nChengqi Zhang,\n“Adversarially regularized graph autoencoder\nfor graph embedding,” .\n[28] Guillaume Salha, Romain Hennequin, and Michalis Vazirgiannis,\n“Keep it simple: Graph autoencoders without graph convolutional\nnetworks,” Workshop on Graph Representation Learning, 33rd\nConference on Neural Information Processing Systems (NeurIPS),\n2019.\n[29] Mikhail Belkin and Partha Niyogi,\n“Laplacian eigenmaps for\ndimensionality reduction and data representation,” Neural com-\nputation, vol. 15, no. 6, pp. 1373–1396, 2003.\n[30] Feiping Nie, Xiaoqian Wang, and Heng Huang, “Clustering and\nprojected clustering with adaptive neighbors,” in Proceedings of the\n20th ACM SIGKDD international conference on Knowledge discovery\nand data mining, 2014, pp. 977–986.\n[31] Andrew Y Ng, Michael I Jordan, and Yair Weiss, “On spectral\nclustering: Analysis and an algorithm,”\nin Advances in neural\ninformation processing systems, 2002, pp. 849–856.\n[32] Laurens Van Der Maaten,\n“Accelerating t-sne using tree-based\nalgorithms,” The Journal of Machine Learning Research, vol. 15, no.\n1, pp. 3221–3245, 2014.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-03-10",
  "updated": "2021-03-23"
}