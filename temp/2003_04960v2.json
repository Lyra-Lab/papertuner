{
  "id": "http://arxiv.org/abs/2003.04960v2",
  "title": "Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey",
  "authors": [
    "Sanmit Narvekar",
    "Bei Peng",
    "Matteo Leonetti",
    "Jivko Sinapov",
    "Matthew E. Taylor",
    "Peter Stone"
  ],
  "abstract": "Reinforcement learning (RL) is a popular paradigm for addressing sequential\ndecision tasks in which the agent has only limited environmental feedback.\nDespite many advances over the past three decades, learning in many domains\nstill requires a large amount of interaction with the environment, which can be\nprohibitively expensive in realistic scenarios. To address this problem,\ntransfer learning has been applied to reinforcement learning such that\nexperience gained in one task can be leveraged when starting to learn the next,\nharder task. More recently, several lines of research have explored how tasks,\nor data samples themselves, can be sequenced into a curriculum for the purpose\nof learning a problem that may otherwise be too difficult to learn from\nscratch. In this article, we present a framework for curriculum learning (CL)\nin reinforcement learning, and use it to survey and classify existing CL\nmethods in terms of their assumptions, capabilities, and goals. Finally, we use\nour framework to find open problems and suggest directions for future RL\ncurriculum learning research.",
  "text": "Journal of Machine Learning Research 21 (2020) 1-50\nSubmitted 3/20; Revised 7/20; Published 7/20\nCurriculum Learning for Reinforcement Learning Domains:\nA Framework and Survey\nSanmit Narvekar\nsanmit@cs.utexas.edu\nDepartment of Computer Science\nUniversity of Texas at Austin\nBei Peng\nbei.peng@cs.ox.ac.uk\nDepartment of Computer Science\nUniversity of Oxford\nMatteo Leonetti\nm.leonetti@leeds.ac.uk\nSchool of Computing\nUniversity of Leeds\nJivko Sinapov\njivko.sinapov@tufts.edu\nDepartment of Computer Science\nTufts University\nMatthew E. Taylor\nmatthew.e.taylor@ualberta.ca\nAlberta Machine Intelligence Institute\nDepartment of Computing Science\nUniversity of Alberta\nPeter Stone\npstone@cs.utexas.edu\nDepartment of Computer Science\nUniversity of Texas at Austin\nand Sony AI\nEditor: George Konidaris\nAbstract\nReinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks\nin which the agent has only limited environmental feedback. Despite many advances over\nthe past three decades, learning in many domains still requires a large amount of inter-\naction with the environment, which can be prohibitively expensive in realistic scenarios.\nTo address this problem, transfer learning has been applied to reinforcement learning such\nthat experience gained in one task can be leveraged when starting to learn the next, harder\ntask. More recently, several lines of research have explored how tasks, or data samples\nthemselves, can be sequenced into a curriculum for the purpose of learning a problem that\nmay otherwise be too diﬃcult to learn from scratch. In this article, we present a framework\nfor curriculum learning (CL) in reinforcement learning, and use it to survey and classify\nexisting CL methods in terms of their assumptions, capabilities, and goals. Finally, we\nuse our framework to ﬁnd open problems and suggest directions for future RL curriculum\nlearning research.\nKeywords:\ncurriculum learning, reinforcement learning, transfer learning\nc⃝2020 Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, and Peter Stone.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided\nat http://jmlr.org/papers/v21/20-212.html.\narXiv:2003.04960v2  [cs.LG]  17 Sep 2020\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nFigure 1: Diﬀerent subgames in the game of Quick Chess, which are used to form a cur-\nriculum for learning the full game of Chess.\n1. Introduction\nCurricula are ubiquitous throughout early human development, formal education, and life-\nlong learning all the way to adulthood. Whether learning to play a sport, or learning to\nbecome an expert in mathematics, the training process is organized and structured so as\nto present new concepts and tasks in a sequence that leverages what has previously been\nlearned. In a variety of human learning domains, the quality of the curricula has been shown\nto be crucial in achieving success. Curricula are also present in animal training, where it is\ncommonly referred to as shaping (Skinner, 1958; Peterson, 2004).\nAs a motivating example, consider the game of Quick Chess (shown in Figure 1), a game\ndesigned to introduce children to the full game of chess, by using a sequence of progressively\nmore diﬃcult “subgames.” For example, the ﬁrst subgame is played on a 5x5 board with\nonly pawns, where the player learns how pawns move, get promoted, and take other pieces.\nNext, in the second subgame, the king piece is added, which introduces a new objective:\nkeeping the king alive. In each successive subgame, new elements are introduced (such as\nnew pieces, a larger board, or diﬀerent conﬁgurations) that require learning new skills and\nbuilding upon knowledge learned in previous games. The ﬁnal game is the full game of\nchess.\nThe idea of using such curricula to train artiﬁcial agents dates back to the early 1990s,\nwhere the ﬁrst known applications were to grammar learning (Elman, 1993; Rohde and\nPlaut, 1999), robotics control problems (Sanger, 1994), and classiﬁcation problems (Bengio\net al., 2009). Results showed that the order of training examples matters and that gen-\nerally, incremental learning algorithms can beneﬁt when training examples are ordered in\nincreasing diﬃculty. The main conclusion from these and subsequent works in curriculum\nlearning is that starting small and simple and gradually increasing the diﬃculty of the task\ncan lead to faster convergence as well as increased performance on a task.\nRecently, research in reinforcement learning (RL) (Sutton and Barto, 1998) has been\nexploring how agents can leverage transfer learning (Lazaric et al., 2008; Taylor and Stone,\n2009) to re-use knowledge learned from a source task when attempting to learn a subsequent\ntarget task. As knowledge is transferred from one task to the next, the sequence of tasks\ninduces a curriculum, which has been shown to improve performance on a diﬃcult problem\nand/or reduce the time it takes to converge to an optimal policy.\n2\nCurriculum Learning for Reinforcement Learning Domains\nMany groups have been studying how such a curriculum can be generated automatically\nto train reinforcement learning agents, and many approaches to do so now exist. However,\nwhat exactly constitutes a curriculum and what precisely qualiﬁes an approach as being\nan example of curriculum learning is not clearly and consistently deﬁned in the literature.\nThere are many ways of deﬁning a curriculum: for example, the most common way is as an\nordering of tasks. At a more fundamental level, a curriculum can also be deﬁned as an or-\ndering of individual experience samples. In addition, a curriculum does not necessarily have\nto be a simple linear sequence. One task can build upon knowledge gained from multiple\nsource tasks, just as courses in human education can build oﬀof multiple prerequisites.\nMethods for curriculum generation have separately been introduced for areas such as\nrobotics, multi-agent systems, human-computer and human-robot interaction, and intrinsi-\ncally motivated learning. This body of work, however, is largely disconnected. In addition,\nmany landmark results in reinforcement learning, from TD-Gammon (Tesauro, 1995) to\nAlphaGo (Silver et al., 2016) have implicitly used curricula to guide training.\nIn some\ndomains, researchers have successfully used methodologies that align with our deﬁnition of\ncurriculum learning without explicitly describing it that way (e.g., self-play). Given the\nmany landmark results that have utilized ideas from curriculum learning, we think it is\nvery likely that future landmark results will also rely on curricula, perhaps more so than\nresearchers currently expect. Thus, having a common basis for discussion of ideas in this\narea is likely to be useful for future AI challenges.\nOverview\nThe goal of this article is to provide a systematic overview of curriculum learning (CL) in\nRL settings and to provide an over-arching framework to formalize this class of methods.\nWe aim to deﬁne classiﬁcation criteria for computational models of curriculum learning for\nRL agents, that describe the curriculum learning research landscape over a broad range of\nframeworks and settings. The questions we address in this survey include:\n• What is a curriculum, and how can it be represented for reinforcement learning tasks?\nAt the most basic level, a curriculum can be thought of as an ordering over experience\nsamples. However, it can also be represented at the task level, where a set of tasks\ncan be organized into a sequence or a directed acyclic graph that speciﬁes the order\nin which they should be learned. We address this question in detail in Section 3.1.\n• What is the curriculum learning method, and how can such methods be evaluated?\nWe formalize this class of methods in Section 3.2 as consisting of three parts, and\nextend metrics commonly used in transfer learning (introduced in Section 2) to the\ncurriculum setting to facilitate evaluation in Section 3.3.\n• How can tasks be constructed for use in a curriculum? The quality of a curriculum\nis dependent on the quality of tasks available to select from. Tasks can either be\ngenerated in advance, or dynamically and on-the-ﬂy with the curriculum. Section 4.1\nsurveys works that examine how to automatically generate good intermediate tasks.\n• How can tasks or experience samples be sequenced into a curriculum? In practice,\nmost curricula for RL agents have been manually generated for each problem. How-\n3\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\never, in recent years, automated methods for generating curricula have been proposed.\nEach makes diﬀerent assumptions about the tasks and transfer methodology used. In\nSection 4.2, we survey these diﬀerent automated approaches, as well as describe how\nhumans have approached curriculum generation for RL agents.\n• How can an agent transfer knowledge between tasks as it learns through a curricu-\nlum? Curriculum learning approaches make use of transfer learning methods when\nmoving from one task to another.\nSince the tasks in the curriculum can vary in\nstate/action space, transition function, or reward function, it’s important to transfer\nrelevant and reusable information from each task, and eﬀectively combine information\nfrom multiple tasks. Methods to do this are enumerated and discussed in Section 4.3.\nThe next section provides background in reinforcement learning and transfer learning.\nIn Section 3, we deﬁne the curriculum learning method, evaluation metrics, and the di-\nmensions along which we will classify curriculum learning approaches. Section 4, which\ncomprises the core of the survey, provides a detailed overview of the existing state of the art\nin curriculum learning in RL, with each subsection considering a diﬀerent component of the\noverall curriculum learning approach. Section 5 discusses paradigms related to curriculum\nlearning for RL, such as curriculum learning for supervised learning and for human educa-\ntion. Finally, in Section 6, we identify gaps in the existing literature, outline the limitations\nof existing CL methods and frameworks, and provide a list of open problems.\n2. Background\nIn this section, we provide background on Reinforcement Learning (RL) and Transfer Learn-\ning (TL).\n2.1 Reinforcement Learning\nReinforcement learning considers the problem of how an agent should act in its environment\nover time, so as to maximize some scalar reward signal. We can formalize the interaction\nof an agent with its environment (also called a task) as a Markov Decision Process (MDP).\nIn this article, we restrict our attention to episodic MDPs:1\nDeﬁnition 1 An episodic MDP M is a 6-tuple (S, A, p, r, ∆s0, Sf), where S is the set of\nstates, A is the set of actions, p(s′|s, a) is a transition function that gives the probability of\ntransitioning to state s′ after taking action a in state s, and r(s, a, s′) is a reward function\nthat gives the immediate reward for taking action a in state s and transitioning to state s′.\nIn addition, we shall use ∆s0 to denote the initial state distribution, and Sf to denote the\nset of terminal states.\nWe consider time in discrete time steps. At each time step t, the agent observes its state\nand chooses an action according to its policy π(a|s). The goal of the agent is to learn an\n1. In continuing tasks, a discount factor γ is often included.\nFor simplicity, and due to the fact that\ntasks typically terminate in curriculum learning settings, we present the undiscounted case. But unless\notherwise noted, our deﬁnitions and discussions can easily apply to the discounted case as well.\n4\nCurriculum Learning for Reinforcement Learning Domains\noptimal policy π∗, which maximizes the expected return Gt (the cumulative sum of rewards\nR) until the episode ends at timestep T:\nGt =\nT−t\nX\ni=1\nRt+i\nThere are three main classes of methods to learn π∗: value function approaches, policy\nsearch approaches, and actor-critic methods. In value function approaches, a value vπ(s) is\nﬁrst learned for each state s, representing the expected return achievable from s by following\npolicy π. Through policy evaluation and policy improvement, this value function is used\nto derive a policy better than π, until convergence towards an optimal policy. Using a\nvalue function in this process requires a model of the reward and transition functions of\nthe environment. If the model is not known, one option is to learn an action-value function\ninstead, qπ(s, a), which gives the expected return for taking action a in state s and following\nπ after:\nqπ(s, a) =\nX\ns′\np(s′|s, a)[r(s, a, s′) + qπ(s′, a′)] , where a′ ∼π(·|s′)\nThe action-value function can be iteratively improved towards the optimal action-value\nfunction q∗with on-policy methods such as SARSA (Sutton and Barto, 1998). The optimal\naction-value function can also be learned directly with oﬀ-policy methods such as Q-learning\n(Watkins and Dayan, 1992). An optimal policy can then be obtained by choosing action\nargmaxaq∗(s, a) in each state. If the state space is large or continuous, the action-value\nfunction can instead be estimated using a function approximator (such as a neural network),\nq(s, a; w) ≈q∗(s, a), where w are the weights of the network.\nIn contrast, policy search methods directly search for or learn a parameterized policy\nπθ(a|s), without using an intermediary value function. Typically, the parameter θ is modi-\nﬁed using search or optimization techniques to maximize some performance measure J(θ).\nFor example, in the episodic case, J(θ) could correspond to the expected value of the policy\nparameterized by θ from the starting state s0 ∼∆s0: vπθ(s0).\nA third class of methods, actor-critic methods, maintain a parameterized representation\nof both the current policy and value function. The actor is a parameterized policy that\ndictates how the agent selects actions. The critic estimates the (action-)value function for\nthe actor using a policy evaluation method such as temporal-diﬀerence learning. The actor\nthen updates the policy parameter in the direction suggested by the critic. An example of\nactor-critic methods is Deterministic Policy Gradient (Silver et al., 2014).\n2.2 Transfer Learning\nIn the standard reinforcement learning setting, an agent usually starts with a random policy,\nand directly attempts to learn an optimal policy for the target task. When the target task is\ndiﬃcult, for example due to adversarial agents, poor state representation, or sparse reward\nsignals, learning can be very slow.\nTransfer learning is one class of methods and area of research that seeks to speed up\ntraining of RL agents. The idea behind transfer learning is that instead of learning on the\ntarget task tabula rasa, the agent can ﬁrst train on one or more source task MDPs, and\n5\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\ntransfer the knowledge acquired to aid in solving the target. This knowledge can take the\nform of samples (Lazaric et al., 2008; Lazaric and Restelli, 2011), options (Soni and Singh,\n2006), policies (Fern´andez et al., 2010), models (Fachantidis et al., 2013), or value functions\n(Taylor and Stone, 2005). As an example, in value function transfer (Taylor et al., 2007),\nthe parameters of an action-value function qsource(s, a) learned in a source task are used to\ninitialize the action-value function in the target task qtarget(s, a). This biases exploration\nand action selection in the target task based on experience acquired in the source task.\nSome of these methods assume that the source and target MDPs either share state and\naction spaces, or that a task mapping (Taylor et al., 2007) is available to map states and\nactions in the target task to known states and actions in the source. Such mappings can\nbe speciﬁed by hand, or learned automatically (Taylor et al., 2008; Ammar et al., 2015).\nOther methods assume the transition or reward functions do not change between tasks.\nThe best method to use varies by domain, and depends on the relationship between source\nand target tasks. Finally, while most methods assume that knowledge is transferred from\none source task to one target task, some methods have been proposed to transfer knowledge\nfrom several source tasks directly to a single target (Svetlik et al., 2017). See Taylor and\nStone (2009) or Lazaric (2012) for a survey of transfer learning techniques.\n2.3 Evaluation Metrics for Transfer Learning\nThere are several metrics to quantify the beneﬁt of transferring from a source task to\na target task (Taylor and Stone, 2009). Typically, they compare the learning trajectory\non the target task for an agent after transfer, with an agent that learns directly on the\ntarget task from scratch (see Figure 2a). One metric is time to threshold, which computes\nhow much faster an agent can learn a policy that achieves expected return G0 ≥δ on\nthe target task if it transfers knowledge, as opposed to learning the target from scratch,\nwhere δ is some desired performance threshold. Time can be measured in terms of CPU\ntime, wall clock time, episodes, or number of actions taken. Another metric is asymptotic\nperformance, which compares the ﬁnal performance after convergence in the target task of\nlearners when using transfer versus no transfer. The jumpstart metric instead measures\nthe initial performance increase on the target task as a result of transfer. Finally, the total\nreward ratio compares the total reward accumulated by the agent during training up to a\nﬁxed stopping point, using transfer versus not using transfer.\nAn important evaluation question is whether to include time spent learning in source\ntasks into the cost of using transfer. The transfer curve in Figure 2a shows performance on\nthe target task, and starts at time 0, even though time has already been spent learning one\nor more source tasks. Thus, it does not reﬂect time spent training in source tasks before\ntransferring to the target task. This is known in transfer learning as the weak transfer\nsetting, where time spent training in source tasks is treated as a sunk cost. On the other\nhand, in the strong transfer setting, the learning curves must account for time spent in all\nsource tasks. One way to do this is to oﬀset the curves to reﬂect time spent in source tasks,\nas shown in Figure 2b. Another option is to freeze the policy while learning on source tasks,\nand plot that policy’s performance on the target task.\n6\nCurriculum Learning for Reinforcement Learning Domains\n(a)\n(b)\nFigure 2: Performance metrics for transfer learning using (a) weak transfer and (b) strong\ntransfer with oﬀset curves.\n3. The Curriculum Learning Method\nA curriculum serves to sort the experience an agent acquires over time, in order to accel-\nerate or improve learning. In the rest of this section we formalize this concept and the\nmethodology of curriculum learning, and describe how to evaluate the beneﬁts and costs of\nusing a curriculum. Finally, we provide a list of attributes which we will use to categorize\ncurriculum learning approaches in the rest of this survey.\n3.1 Curricula\nA curriculum is a general concept that encompasses both schedules for organizing past\nexperiences, and schedules for acquiring experience by training on tasks. As such, we ﬁrst\npropose a fully general deﬁnition of curriculum, and then follow it with reﬁnements that\napply to special cases common in the literature.\nWe assume a task is modeled as a Markov Decision Process, and deﬁne a curriculum as\nfollows:\nDeﬁnition 2 (Curriculum) Let T be a set of tasks, where mi = (Si, Ai, pi, ri) is a\ntask in T . Let DT be the set of all possible transition samples from tasks in T : DT =\n{(s, a, r, s′) | ∃mi ∈T s.t. s ∈Si, a ∈Ai, s′ ∼pi(·|s, a), r ←ri(s, a, s′)}.\nA curricu-\nlum C = (V, E, g, T ) is a directed acyclic graph, where V is the set of vertices, E ⊆\n{(x, y) | (x, y) ∈V × V ∧x ̸= y} is the set of directed edges, and g : V →P(DT ) is a\nfunction that associates vertices to subsets of samples in DT , where P(DT ) is the power set\nof DT . A directed edge ⟨vj, vk⟩in C indicates that samples associated with vj ∈V should\nbe trained on before samples associated with vk ∈V. All paths terminate on a single sink\nnode vt ∈V.2\nA curriculum can be created online, where edges are added dynamically based on the\nlearning progress of the agent on the samples at a given vertex. It can also be designed\n2. In theory, a curriculum could have multiple sink nodes corresponding to diﬀerent target tasks. For the\npurpose of exposition, we assume a separate curriculum is created and used for each task.\n7\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\ncompletely oﬄine, where the graph is generated before training, and edges are selected\nbased on properties of the samples associated with diﬀerent vertices.\nCreating a curriculum graph at the sample level can be computationally diﬃcult for\nlarge tasks, or large sets of tasks. Therefore, in practice, a simpliﬁed representation for a\ncurriculum is often used. There are 3 common dimensions along which this simpliﬁcation\ncan happen. The ﬁrst is the single-task curriculum, where all samples used in the curriculum\ncome from a single task:\nDeﬁnition 3 (Single-task Curriculum) A single-task curriculum is a curriculum C where\nthe cardinality of the set of tasks considered for extracting samples |T | = 1, and consists of\nonly the target task mt.\nA single-task curriculum essentially considers how best to organize and train on experi-\nence acquired from a single task. This type of curriculum is common in experience replay\nmethods (Schaul et al., 2016).\nA second common simpliﬁcation is to learn a curriculum at the task level, where each\nvertex in the graph is associated with samples from a single task.\nAt the task level, a\ncurriculum can be deﬁned as a directed acyclic graph of intermediate tasks:\nDeﬁnition 4 (Task-level Curriculum) For each task mi ∈T , let DT\ni\nbe the set of all\nsamples associated with task mi: DT\ni\n= {(s, a, r, s′) | s ∈Si, a ∈Ai, s′ ∼pi(·|s, a), r ←\nri(s, a, s′)}. A task-level curriculum is a curriculum C = (V, E, g, T ) where each vertex is\nassociated with samples from a single task in T . Thus, the mapping function g is deﬁned\nas g : V →{DT\ni | mi ∈T }.\nIn reinforcement learning, the entire set of samples from a task (or multiple tasks) is\nusually not available ahead of time. Instead, the samples experienced in a task depend on\nthe agent’s behavior policy, which can be inﬂuenced by previous tasks learned. Therefore,\nwhile generating a task-level curriculum, the main challenge is how to order tasks such that\nthe behavior policy learned is useful for acquiring good samples in future tasks. In other\nwords, selecting and training on a task m induces a mapping function g, and determines the\nset of samples DT\ni that will be available at the next vertex based on the agent’s behavior\npolicy as a result of learning m. The same task is allowed to appear at more than one\nvertex, similar to how in Deﬁnition 2 the same set of samples can be associated with more\nthan one vertex. Therefore, tasks can be revisited when the agent’s behavior policy has\nchanged. Several works have considered learning task-level curricula over a graph of tasks\n(Svetlik et al., 2017; MacAlpine and Stone, 2018). An example can be seen in Figure 3b.\nFinally, another simpliﬁcation of the curriculum is the linear sequence.\nThis is the\nsimplest and most common structure for a curriculum in existing work:\nDeﬁnition 5 (Sequence Curriculum) A sequence curriculum is a curriculum C where\nthe indegree and outdegree of each vertex v in the graph C is at most 1, and there is exactly\none source node and one sink node.\nThese simpliﬁcations can be combined to simplify a curriculum along multiple dimen-\nsions. For example, the sequence simpliﬁcation and task-level simpliﬁcation can be com-\n8\nCurriculum Learning for Reinforcement Learning Domains\nbined to produce a task-level sequence curriculum. This type of curriculum can be rep-\nresented as an ordered list of tasks [m1, m2, ...mn]. An example can be seen in Figure 3a\n(Narvekar et al., 2017).\nA ﬁnal important question when designing curricula is determining the stopping criteria:\nthat is, how to decide when to stop training on samples or tasks associated with a vertex,\nand move on to the next vertex. In practice, typically training is stopped when performance\non the task or set of samples has converged. Training to convergence is not always necessary,\nso another option is to train on each vertex for a ﬁxed number of episodes or epochs. Since\nmore than one vertex can be associated with the same samples/tasks, this experience can\nbe revisited later on in the curriculum.\n3.2 Curriculum Learning\nCurriculum learning is a methodology to optimize the order in which experience is accumu-\nlated by the agent, so as to increase performance or training speed on a set of ﬁnal tasks.\nThrough generalization, knowledge acquired quickly in simple tasks can be leveraged to\nreduce the exploration of more complex tasks. In the most general case, where the agent\ncan acquire experience from multiple intermediate tasks that diﬀer from the ﬁnal MDP,\nthere are 3 key elements to this method:\n• Task Generation. The quality of a curriculum is dependent on the quality of tasks\navailable to choose from. Task generation is the process of creating a good set of\nintermediate tasks from which to obtain experience samples. In a task-level curricu-\nlum, these tasks form the nodes of the curriculum graph. This set of intermediate\ntasks may either be pre-speciﬁed, or dynamically generated during the curriculum\nconstruction by observing the agent.\n• Sequencing. Sequencing examines how to create a partial ordering over the set of ex-\nperience samples D: that is, how to generate the edges of the curriculum graph. Most\nexisting work has used manually deﬁned curricula, where a human selects the ordering\nof samples or tasks. However, recently automated methods for curriculum sequencing\nhave begun to be explored. Each of these methods make diﬀerent assumptions about\nthe tasks and transfer methodology used. These methods will be the primary focus\nof this survey.\n• Transfer Learning. When creating a curriculum using multiple tasks, the intermedi-\nate tasks may diﬀer in state/action space, reward function, or transition function from\nthe ﬁnal task. Therefore, transfer learning is needed to extract and pass on reusable\nknowledge acquired in one task to the next. Typically, work in transfer learning has\nexamined how to transfer knowledge from one or more source tasks directly to the\ntarget task. Curriculum learning extends the transfer learning scenario to consider\ntraining sessions in which the agent must repeatedly transfer knowledge from one task\nto another, up to a set of ﬁnal tasks.\n9\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\n(a)\n(b)\nFigure 3: Examples of structures of curricula from previous work. (a) Linear sequences in\na gridworld domain (Narvekar et al., 2017) (b) Directed acyclic graphs in block\ndude (Svetlik et al., 2017).\n3.3 Evaluating Curricula\nCurricula can be evaluated using the same metrics as for transfer learning (cf. Section 2.3),\nby comparing performance on the target task after following the complete curriculum, versus\nperformance following no curriculum (i.e., learning from scratch). If there are multiple ﬁnal\ntasks, the metrics can easily be extended: for example, by comparing the average asymptotic\nperformance over a set of tasks, or the average time to reach a threshold performance level\nover a set of tasks.\nSimilarly, it is possible to distinguish between weak and strong transfer. However, in\ncurriculum learning, there is the additional expense required to build the curriculum itself,\nin addition to training on intermediate tasks in the curriculum, which can also be factored\nin when evaluating the cost of the curriculum. As in the transfer learning case, cost can be\nmeasured in terms of wall clock time, or data/sample complexity.\nMost existing applications of curricula in reinforcement learning have used curricula cre-\nated by humans. In these cases, it can be diﬃcult to assess how much time, eﬀort, and prior\nknowledge was used to design the curriculum. Automated approaches to generate a cur-\nriculum also typically require some prior knowledge or experience in potential intermediate\ntasks, in order to guide the sequencing of tasks. Due to these diﬃculties, these approaches\nhave usually treated curriculum generation as a sunk cost, focusing on evaluating the per-\nformance of the curriculum itself, and comparing it versus other curricula, including those\ndesigned by people.\nThe best set of evaluation criteria to use ultimately depends on the speciﬁc problem and\nsettings being considered. For example, how expensive is it to collect data on the ﬁnal task\ncompared to intermediate tasks? If intermediate tasks are relatively inexpensive, we can\ntreat time spent in them as sunk costs. Is it more critical to improve initial performance,\nﬁnal performance, or reaching a desired performance threshold? If designing the curriculum\n10\nCurriculum Learning for Reinforcement Learning Domains\nwill require human interaction, how will this time be factored into the cost of using a\ncurriculum? Many of these questions depend on whether we wish to evaluate the utility\nof a speciﬁc curriculum (compared to another curriculum), or whether we wish to evaluate\nthe utility of using a curriculum design approach versus training without one.\n3.4 Dimensions of Categorization\nWe categorize curriculum learning approaches along the following seven dimensions, or-\nganized by attributes (in bold) and the values (in italics) they can take. We use these\ndimensions to create a taxonomy of surveyed work in Section 4.\n1. Intermediate task generation: target / automatic / domain experts / naive users.\nIn curriculum learning, the primary challenge is how to sequence a set of tasks to\nimprove learning speed. However, ﬁnding a good curriculum depends on ﬁrst having\nuseful source tasks to select from. Most methods assume the set of possible source\ntasks is ﬁxed and given ahead of time. In the simplest case, only samples from the\ntarget task are used. When more than one intermediate task is used, typically they\nare manually designed by humans. We distinguish such tasks as designed by either\ndomain experts, who have knowledge of the agent and its learning algorithm, or naive\nusers, who do not have this information. On the other hand, some works consider\nautomatically creating tasks online using a set of rules or generative process. These\napproaches may still rely on some human input to control/tune hyper-parameters,\nsuch as the number of tasks generated, or to verify that generated tasks are actually\nsolvable.\n2. Curriculum representation: single / sequence / graph. As we discussed previously,\nthe most general form of a curriculum is a directed acyclic graph over subsets of\nsamples.\nHowever, in practice, simpliﬁed versions of this representation are often\nused. In the simplest case, a curriculum is an ordering over samples from a single\ntask. When multiple tasks can be used in a curriculum, curricula are often created at\nthe task-level. These curricula can be represented as a linear chain, or sequence. In\nthis case, there is exactly one source for each intermediate task in the curriculum. It is\nup to the transfer learning algorithm to appropriately retain and combine information\ngathered from previous tasks in the chain. More generally, they can be represented as\na full directed acyclic graph of tasks. This form supports transfer learning methods\nthat transfer from many-to-one, one-to-many, and many-to-many tasks.\n3. Transfer method: policies / value function / task model / partial policies / shap-\ning reward / other / no transfer. Curriculum learning leverages ideas from transfer\nlearning to transfer knowledge between tasks in the curriculum. As such, the trans-\nfer learning algorithm used aﬀects how the curriculum will be produced. The type\nof knowledge transferred can be low-level knowledge, such as an entire policy, an\n(action-)value function, or a full task model, which can be used to directly initialize\nthe learner in the target task. It can also be high-level knowledge, such as partial\npolicies (e.g. options) or shaping rewards. This type of information may not fully ini-\ntialize the learner in the target task, but it could be used to guide the agent’s learning\nprocess in the target task. We use partial policies as an umbrella term to represent\n11\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nclosely related ideas such as options, skills, and macro-actions. When samples from\na single task are sequenced, no transfer learning algorithm is necessary. Finally, we\nuse other to refer to other types of transfer learning methods. We categorize papers\nalong this dimension based on what is transferred between tasks in the curriculum in\neach paper’s experimental results.\n4. Curriculum sequencer: automatic / domain experts / naive users. Curriculum\nlearning is a three-part method, consisting of task generation, sequencing, and transfer\nlearning.\nWhile much of the attention of this survey is on automated sequencing\napproaches, many works consider the other parts of this method, and assume the\nsequencing is done by a human or oracle. Thus, we identify and categorize the type\nof sequencing approach used in each work similar to task generation: it can be done\nautomatically by a sequencing algorithm, or manually by humans that are either\ndomain experts or naive users.\n5. Curriculum adaptivity: static / adaptive. Another design question when creating\na curriculum is whether it should be generated in its entirety before training, or\ndynamically adapted during training. We refer to the former type as static and to\nthe latter as adaptive. Static approaches use properties of the domain and possibly\nof the learning agent, to generate a curriculum before any task is learned. Adaptive\nmethods, on the other hand, are inﬂuenced by properties that can only be measured\nduring learning, such as the learning progress by the agent on the task it is currently\nfacing. For example, learning progress can be used to guide whether subsequent tasks\nshould be easier or harder, as well as how relevant a task is for the agent at a particular\npoint in the curriculum.\n6. Evaluation metric: time to threshold / asymptotic / jumpstart / total reward. We\ndiscussed four metrics to quantify the eﬀectiveness of learned curricula in Section\n3.3.\nWhen calculating these metrics, one can choose whether to treat time spent\ngenerating the curriculum and training on the curriculum as a sunk cost, or whether\nto account for both of these for performance. Speciﬁcally, there are three ways to\nmeasure the cost of learning and training via a curriculum. 1) The cost of generating\nand using the curriculum is treated as a sunk cost, and the designer is only concerned\nwith performance on the target task after learning. This case corresponds to the weak\ntransfer setting. 2) The cost of training on intermediate tasks in the curriculum is\naccounted for, when comparing to training directly on the target task. This case is\nmost common when it is hard to evaluate the cost of generating the curriculum itself,\nfor example if it was hand-designed by a human. 3) Lastly, the most comprehensive\ncase accounts for the cost of generating the curriculum as well as training via the\ncurriculum. We will refer to the last two as strong transfer, and indicate it by bolding\nthe corresponding metric. Note that achieving asymptotic performance improvements\nimplies strong transfer.\n7. Application area: toy / sim robotics / real robotics / video games / other. Curricu-\nlum learning methods have been tested in a wide variety of domains. Toy domains\nconsist of environments such as grid worlds, cart-pole, and other low dimensional envi-\nronments. Sim robotics environments simulate robotic platforms, such as in MuJoCo.\n12\nCurriculum Learning for Reinforcement Learning Domains\nReal robotics papers test their method on physical robotic platforms. Video games\nconsist of game environments such as Starcraft or the Arcade Learning Environment\n(Atari). Finally, other is used for custom domains that do not ﬁt in these categories.\nWe list these so that readers can better understand the scalability and applicability\nof diﬀerent approaches, and use these to inform what methods would be suitable for\ntheir own problems.\n4. Curriculum Learning for Reinforcement Learning Agents\nIn this section, we systematically survey work on each of the three central elements of\ncurriculum learning: task generation (Section 4.1), sequencing (Section 4.2), and transfer\nlearning (Section 4.3). For each of these subproblems, we provide a table that categorizes\nwork surveyed according to the dimensions outlined in Section 3. The bulk of our attention\nwill be devoted to the subproblem most commonly associated with curriculum learning:\nsequencing.\n4.1 Task Generation\nTask generation is the problem of creating intermediate tasks speciﬁcally to be part of a\ncurriculum. In contrast to the life-long learning scenario, where potentially unrelated tasks\nare constantly proposed to the agent (Thrun, 1998), the aim of task generation is to create\na set of tasks such that knowledge transfer through them is beneﬁcial. Therefore, all the\ngenerated tasks should be relevant to the ﬁnal task(s) and avoid negative transfer, where\nusing a task for transfer hurts performance. The properties of the research surveyed in this\nsection are reported in Table 1.\nVery limited work has been dedicated to formally studying this subproblem in the con-\ntext of reinforcement learning. All known methods assume the domain can be parameterized\nusing some kind of representation, where diﬀerent instantiations of these parameters create\ndiﬀerent tasks. For instance, Narvekar et al. (2016) introduce a number of methods to create\nintermediate tasks for a speciﬁc ﬁnal task. The methods hinge on a deﬁnition of a domain\nas a set of MDPs identiﬁed by a task descriptor, which is a vector of parameters specifying\nthe degrees of freedom in the domain. For example, in the quick chess example (see Section\n1), these parameters could be the size of the board, number of pawns, etc. By varying\nthe degrees of freedom and applying task restrictions, the methods deﬁne diﬀerent types of\ntasks. Methods introduced include: task simpliﬁcation, which directly changes the degrees\nof freedom to reduce the task dimensions; promising initialization, which modiﬁes the set\nof initial states by adding states close to high rewards; mistake learning, which rewinds the\ndomain to a state a few steps before a mistake is detected and resumes learning from there;\nand several other methods. The set of methods determine diﬀerent kinds of possible tasks,\nwhich form a space of tasks in which appropriate intermediate tasks can be chosen.\nDa Silva and Reali Costa (2018) propose a similar partially automated task generation\nprocedure in their curriculum learning framework, based on Object-Oriented MDPs. Each\ntask is assumed to have a class environment parameterized by a number of attributes.\nA function, which must be provided by the designer, creates simpler versions of the ﬁnal\ntask by instantiating the attributes with values that make the tasks easier to solve. For\nexample, continuing the quick chess example, the attributes could be the types of pieces,\n13\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nCitation\nIntermediate\nTask\nGeneration\nCurriculum\nRepresentation\nTransfer\nMethod\nCurriculum\nSequencer\nCurriculum\nAdaptivity\nEvaluation\nMetric\nApplication\nArea\nDa Silva and Reali Costa (2018)\nautomatic\ngraph\nvalue function\nautomatic\nstatic\ntime to threshold, total reward\ntoy, video games\nNarvekar et al. (2016)\nautomatic\nsequence\nvalue function\ndomain experts\nadaptive\nasymptotic\nvideo games\nSchmidhuber (2013)\nautomatic\nsequence\npartial policies\nautomatic\nadaptive\nasymptotic\nother\nStone and Veloso (1994)\nautomatic\nsequence\nother\ndomain experts\nadaptive\ntime to threshold\nother\nTable 1: The papers discussed in Section 4.1, categorized along the dimensions presented\nin Section 3.4. Bolded values under evaluation metric indicate strong transfer.\nand the values are the number of each type of piece. The presence of diﬀerent kinds and\nnumbers of objects provide a range of tasks with diﬀerent levels of diﬃculty. However, since\nthe generation is mostly random, the designer has to make sure that the tasks are indeed\nsolvable.\nGenerating auxiliary intermediate tasks is a problem that has been studied in non-RL\ncontexts as well. For instance, Stone and Veloso (1994) consider how to semiautomatically\ncreate subproblems to aid in learning to solve diﬃcult planning problems. Rather than\nusing a static analysis of the domain’s properties, they propose to use a partially completed\nsearch trajectory of the target task to identify what makes a problem diﬃcult, and suggest\nauxiliary tasks. For example, if the task took too long and there are multiple goals in the\ntask, try changing the order of the goals. Other methods they propose include reducing the\nnumber of goals, creating tasks to solve diﬃcult subgoals, and changing domain operators\nand objects available for binding.\nLastly, Schmidhuber (2013) introduced Powerplay, a framework that focuses on inventing\nnew problems to train a more and more general problem solver in an unsupervised fashion.\nThe system searches for both a new task and a modiﬁcation of the current problem solver,\nsuch that the modiﬁed solver can solve all previous tasks, plus the new one. The search acts\non a domain-dependent encoding of the problem and the solver, and has been demonstrated\non pattern recognition and control tasks (Srivastava et al., 2013). The generator of the task\nand new solver is given a limited computational budget, so that it favors the generation\nof the simplest tasks that could not be solved before. Furthermore, a possible task is to\nsolve all previous tasks, but with a more compact representation of the solver. The resulting\niterative process makes the system increasingly more competent at diﬀerent tasks. The task\ngeneration process eﬀectively creates a curriculum, although in this context there are no\nﬁnal tasks, and the system continues to generate pairs of problems and solvers indeﬁnitely,\nwithout any speciﬁc goal.\n4.2 Sequencing\nGiven a set of tasks, or samples from them, the goal of sequencing is to order them in a\nway that facilitates learning. Many diﬀerent sequencing methods exist, each with their own\nset of assumptions. One of the fundamental assumptions of curriculum learning is that we\ncan conﬁgure the environment to create diﬀerent tasks. For the practitioner attempting\nto use curriculum learning, the amount of control one has to shape the environment af-\nfects the type of sequencing methods that could be applicable. Therefore, we categorize\nsequencing methods by the degree to which intermediate tasks may diﬀer. Speciﬁcally, they\n14\nCurriculum Learning for Reinforcement Learning Domains\nform a spectrum, ranging from methods that simply reorder experience in the ﬁnal task\nwithout modifying any property of the corresponding MDP, to ones that deﬁne entirely\nnew intermediate tasks, by progressively adjusting some or all of the properties of the ﬁnal\ntask.\nIn this subsection, we discuss the diﬀerent sequencing approaches.\nFirst, in Section\n4.2.1, we consider methods that reorder samples in the target task to derive a curriculum.\nExperience replay methods are one such example.\nIn Section 4.2.2, we examine multi-\nagent approaches to curriculum generation, where the cooperation or competition between\ntwo (typically evolving) agents induces a sequence of progressively challenging tasks, like\na curriculum.\nThen, in Section 4.2.3, we begin describing methods that explicitly use\nintermediate tasks, starting with ones that vary in limited ways from the target task. In\nparticular, these methods only change the reward function and/or the initial and terminal\nstate distributions to create a curriculum. In Section 4.2.4, we discuss methods that relax\nthis assumption, and allow intermediate tasks that can vary in any way from the target\ntask MDP. Finally, in Section 4.2.5, we discuss work that explores how humans sequence\ntasks into a curriculum.\n4.2.1 Sample Sequencing\nFirst we consider methods that reorder samples from the ﬁnal task, but do not explicitly\nchange the domain itself. These ideas are similar to curriculum learning for supervised\nlearning (Bengio et al., 2009), where training examples are presented to a learner in a speciﬁc\norder, rather than completely randomly. Bengio et al. (2009) showed that ordering these\nexamples from simple to complex can improve learning speed and generalization ability.\nAn analogous process can be used for reinforcement learning. For example, many current\nreinforcement learning methods, such as Deep Q Networks (DQN) (Mnih et al., 2015) use\na replay buﬀer to store past state-action-reward experience tuples. At each training step,\nexperience tuples are sampled from the buﬀer and used to train DQN in minibatches. The\noriginal formulation of DQN performed this sampling uniformly randomly. However, as in\nthe supervised setting, samples can be reordered or “prioritized,” according to some measure\nof usefulness or diﬃculty, to improve learning.\nThe ﬁrst to do this type of sample sequencing in the context of deep learning were Schaul\net al. (2016). They proposed Prioritized Experience Replay (PER), which prioritizes and\nreplays important transitions more. Important transitions are those with high expected\nlearning progress, which is measured by their temporal diﬀerence (TD) error. Intuitively,\nreplaying samples with larger TD errors allows the network to make stronger updates. As\ntransitions are learned, the distribution of important transitions changes, leading to an\nimplicit curriculum over the samples.\nAlternative metrics for priority/importance have been explored as well. Ren et al. (2018)\npropose to sort samples using a complexity index (CI) function, which is a combination of\na self-paced prioritized function and a coverage penalty function. The self-paced prioritized\nfunction selects samples that would be of appropriate diﬃculty, while the coverage function\npenalizes transitions that are replayed frequently. They provide one speciﬁc instantiation\nof these functions, which are used in experiments on the Arcade Learning Environment\n(Bellemare et al., 2013), and show that it performs better than PER in many cases. However,\n15\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nCitation\nIntermediate\nTask\nGeneration\nCurriculum\nRepresentation\nTransfer\nMethod\nCurriculum\nSequencer\nCurriculum\nAdaptivity\nEvaluation\nMetric\nApplication\nArea\nSample Sequencing (Section 4.2.1)\nAndrychowicz et al. (2017)\ntarget\nsingle\nno transfer\nautomatic\nadaptive\nasymptotic\nsim robotics\nFang et al. (2019)\ntarget\nsingle\nno transfer\nautomatic\nadaptive\nasymptotic\nsim robotics\nKim and Choi (2018)\ntarget\nsingle\nno transfer\nautomatic\nadaptive\nasymptotic\ntoy, other\nLee et al. (2019)\ntarget\nsingle\nno transfer\nautomatic\nadaptive\ntime to threshold\ntoy, video games\nRen et al. (2018)\ntarget\nsingle\nno transfer\nautomatic\nadaptive\nasymptotic\nvideo games\nSchaul et al. (2016)\ntarget\nsingle\nno transfer\nautomatic\nadaptive\nasymptotic\nvideo games\nCo-learning (Section 4.2.2)\nBaker et al. (2020)\nautomatic\nsequence\npolicies\nautomatic\nadaptive\nasymptotic, time to threshold\nother\nBansal et al. (2018)\nautomatic\nsequence\npolicies\nautomatic\nadaptive\nasymptotic\nsim robotics\nPinto et al. (2017)\nautomatic\nsequence\npolicies\nautomatic\nadaptive\ntime to threshold\nsim robotics\nSukhbaatar et al. (2018)\nautomatic\nsequence\npolicies\nautomatic\nadaptive\ntime to threshold, asymptotic\ntoy, video games\nVinyals et al. (2019)\nautomatic\nsequence\npolicies\nautomatic\nadaptive\nasymptotic\nvideo games\nReward and Initial/Terminal State Distribution Changes (Section 4.2.3)\nAsada et al. (1996)\ndomain experts\nsequence\nvalue function\nautomatic\nadaptive\nasymptotic\nsim/real robotics\nBaranes and Oudeyer (2013)\nautomatic\nsequence\npartial policies\nautomatic\nadaptive\nasymptotic\nsim/real robotics\nFlorensa et al. (2017)\nautomatic\nsequence\npolicies\nautomatic\nadaptive\nasymptotic\nsim robotics\nFlorensa et al. (2018)\nautomatic\nsequence\npolicies\nautomatic\nadaptive\nasymptotic\nsim robotics\nIvanovic et al. (2019)\nautomatic\nsequence\npolicies\nautomatic\nadaptive\nasymptotic\nsim robotics\nRacaniere et al. (2019)\nautomatic\nsequence\npolicies\nautomatic\nadaptive\nasymptotic\ntoy, video games\nRiedmiller et al. (2018)\ndomain experts\nsequence\npolicies\nautomatic\nadaptive\ntime to threshold\nsim/real robotics\nWu and Tian (2017)\ndomain experts\nsequence\ntask model\nautomatic\nboth\nasymptotic\nvideo games\nNo Restrictions (Section 4.2.4)\nBassich et al. (2020)\ndomain experts\nsequence\npolicies\nautomatic\nadaptive\nasymptotic, time to threshold\ntoy\nDa Silva and Reali Costa (2018)\nautomatic\ngraph\nvalue function\nautomatic\nstatic\ntime to threshold, total reward\ntoy, video games\nFoglino et al. (2019a)\ndomain experts\nsequence\nvalue function\nautomatic\nstatic\ntime to threshold, asymptotic, total reward\ntoy\nFoglino et al. (2019b)\ndomain experts\nsequence\nvalue function\nautomatic\nstatic\ntotal reward\ntoy\nFoglino et al. (2019c)\ndomain experts\nsequence\nvalue function\nautomatic\nstatic\ntotal reward\ntoy\nJain and Tulabandhula (2017)\ndomain experts\nsequence\nvalue function\nautomatic\nadaptive\ntime to threshold, total reward\ntoy\nMatiisen et al. (2017)\ndomain experts\nsequence\npolicies\nautomatic\nadaptive\nasymptotic\ntoy, video games\nNarvekar et al. (2017)\nautomatic\nsequence\nvalue function\nautomatic\nadaptive\ntime to threshold\ntoy\nNarvekar and Stone (2019)\ndomain experts\nsequence\nvalue function, shaping reward\nautomatic\nadaptive\ntime to threshold\ntoy, video games\nSvetlik et al. (2017)\ndomain experts\ngraph\nshaping reward\nautomatic\nstatic\nasymptotic, time to threshold\ntoy, video games\nHuman-in-the-loop Curriculum Generation (Section 4.2.5)\nHosu and Rebedea (2016)\ntarget\nsingle\nno transfer\nautomatic\nadaptive\nasymptotic\nvideo games\nKhan et al. (2011)\ndomain experts\nsequence\nno transfer\nnaive users\nstatic\nN/A\nother\nMacAlpine and Stone (2018)\ndomain experts\ngraph\npolicies\ndomain experts\nstatic\nasymptotic\nsim robotics\nPeng et al. (2018)\ndomain experts\nsequence\ntask model\nnaive users\nstatic\ntime to threshold\nother\nStanley et al. (2005)\ndomain experts\nsequence\npartial policies\ndomain experts\nadaptive\nasymptotic\nvideo games\nTable 2: The papers discussed in Section 4.2, categorized along the dimensions presented\nin Section 3.4. Bolded values under evaluation metric indicate strong transfer.\nthese functions must be designed individually for each domain, and designing a broadly\napplicable domain-independent priority function remains an open problem.\nKim and Choi (2018) consider another extension of prioritized experience replay, where\nthe weight/priority of a sample is jointly learned with the main network via a secondary\nneural network. The secondary network, called ScreenerNet, learns to predict weights ac-\ncording to the error of the sample by the main network. Unlike PER, this approach is\nmemoryless, which means it can directly predict the signiﬁcance of a training sample even\nif that particular example was not seen.\nThus, the approach could potentially be used\nto actively request experience tuples that would provide the most information or utility,\ncreating an online curriculum.\nInstead of using sample importance as a metric for sequencing, an alternative idea is to\nrestructure the training process based on trajectories of samples experienced. For example,\nwhen learning, typically easy to reach states are encountered ﬁrst, whereas harder to reach\nstates are encountered later on in the learning cycle. However, in practical settings with\n16\nCurriculum Learning for Reinforcement Learning Domains\nsparse rewards, these easy to reach states may not provide a reward signal.\nHindsight\nExperience Replay (HER) (Andrychowicz et al., 2017) is one method to make the most\nof these early experiences. HER is a method that learns from “undesired outcomes,” in\naddition to the desired outcome, by replaying each episode with a goal that was actually\nachieved rather than the one the agent was trying to achieve. The problem is set up as\nlearning a Universal Value Function Approximator (UVFA) (Schaul et al., 2015), which is a\nvalue function vπ(s, g) deﬁned over states s and goals g . The agent is given an initial state\ns1 and a desired goal state g. Upon executing its policy, the agent may not reach the goal\nstate g, and instead land on some other terminal state sT . While this trajectory does not\nhelp to learn to achieve g, it does help to learn to achieve sT . Thus, this trajectory is added\nto the replay buﬀer with the goal state substituted with sT , and used with an oﬀ-policy RL\nalgorithm. HER forms a curriculum by taking advantage of the implicit curriculum present\nin exploration, where early episodes are likely to terminate on easy to reach states, and\nmore diﬃcult to reach states are found later in the training process.\nOne of the issues with vanilla HER is that all goals in seen trajectories are replayed\nevenly, but some goals may be more useful at diﬀerent points of learning. Thus, Fang et al.\n(2019) later proposed Curriculum-guided HER (CHER) to adaptively select goals based on\ntwo criteria: curiosity, which leads to the selection of diverse goals, and proximity, which\nselects goals that are closer to the true goal.\nBoth of these criteria rely on a measure\nof distance or similarity between goal states. At each minibatch optimization step, the\nobjective selects a subset of goals that maximizes the weighted sum of a diversity and\nproximity score. They manually impose a curriculum that starts biased towards diverse\ngoals and gradually shifts towards proximity based goals using a weighting factor that is\nexponentially scaled over time.\nOther than PER and HER, there are other works that reorder/resample experiences in a\nnovel way to improve learning. One example is the episodic backward update (EBU) method\ndeveloped by Lee et al. (2019). In order to speed up the propagation of delayed rewards\n(e.g., a reward might only be obtained at the end of an episode), Lee et al. (2019) proposed\nto sample a whole episode from the replay buﬀer and update the values of all transitions\nwithin the sampled episode in a backward fashion. Starting from the end of the sampled\nepisode, the max Bellman operator is applied recursively to update the target Q-values\nuntil the start of the sampled episode. This process basically reorders all the transitions\nwithin each sampled episode from the last timestep of the episode to the ﬁrst, leading to an\nimplicit curriculum. Updating highly correlated states in a sequence while using function\napproximation is known to suﬀer from cumulative overestimation errors. To overcome this\nissue, a diﬀusion factor β ∈(0, 1) was introduced to update the current Q-value using a\nweighted sum of the new bootstrapped target value and the pre-existing Q-value estimate.\nTheir experimental results show that in 49 Atari games, EBU can achieve the same mean\nand median human normalized performance of DQN by using signiﬁcantly fewer samples.\nMethods that sequence experience samples have wide applicability and found broad\nsuccess in many applications, since they can be applied directly on the target task without\nneeding to create intermediate tasks that alter the environment. In the following sections,\nwe consider sequencing approaches that progressively alter how much intermediate tasks in\nthe curriculum may diﬀer.\n17\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\n4.2.2 Co-learning\nCo-learning is a multi-agent approach to curriculum learning, in which the curriculum\nemerges from the interaction of several agents (or multiple versions of the same agent)\nin the same environment. These agents may act either cooperatively or adversarially to\ndrive the acquisition of new behaviors, leading to an implicit curriculum where both sets\nof agents improve over time. Self-play is one methodology that ﬁts into this paradigm, and\nmany landmark results such as TD-Gammon (Tesauro, 1995) and more recently AlphaGo\n(Silver et al., 2016) and AlphaStar (Vinyals et al., 2019) fall into this category. Rather than\ndescribing every work that uses self-play or co-learning, we describe a few papers that focus\non how the objectives of the multiple agents can be set up to facilitate co-learning.\nSukhbaatar et al. (2018) proposed a novel method called asymmetric self-play that allows\nan agent to learn about the environment without any external reward in an unsupervised\nmanner. This method considers two agents, a teacher and a student, using the paradigm of\n“the teacher proposing a task, and the student doing it.” The two agents learn their own\npolicies simultaneously by maximizing interdependent reward functions for goal-based tasks.\nThe teacher’s task is to navigate to an environment state that the student will use either\nas 1) a goal, if the environment is resettable, or 2) as a starting state, if the environment is\nreversible. In the ﬁrst case, the student’s task is to reach the teacher’s ﬁnal state, while in\nthe second case, the student starts from the teacher’s ﬁnal state with the aim of reverting\nthe environment to its original initial state. The student’s goal is to minimize the number\nof actions it needs to complete the task. The teacher, on the other hand, tries to maximize\nthe diﬀerence between the actions taken by the student to execute the task, and the actions\nspent by the teacher to set up the task. The teacher, therefore, tries to identify a state that\nstrikes a balance between being the simplest goal (in terms of number of teacher actions) for\nitself to ﬁnd, and the most diﬃcult goal for the student to achieve. This process is iterated\nto automatically generate a curriculum of intrinsic exploration.\nAnother example of jointly training a pair of agents adversarially for policy learning\nin single-agent RL tasks is Robust Adversarial RL (RARL) by Pinto et al. (2017). Unlike\nasymmetric self-play (Sukhbaatar et al., 2018), in which the teacher deﬁnes the goal for\nthe student, RARL trains a protagonist and an adversary, where the protagonist learns\nto complete the original RL task while being robust to the disturbance forces applied by\nthe adversarial agent. RARL is targeted at robotic systems that are required to generalize\neﬀectively from simulation, and learn robust policies with respect to variations in physical\nparameters. Such variations are modeled as disturbances controlled by an adversarial agent,\nand the adversarial agent’s goal is to learn the optimal sequence of destabilizing actions via\na zero-sum game training procedure. The adversarial agent tries to identify the hardest\nconditions under which the protagonist agent may be required to act, increasing the agent’s\nrobustness. Learning takes place in turns, with the protagonist learning against a ﬁxed\nantagonist’s policy, and then the antagonist learning against a ﬁxed protagonist’s policy.\nEach agent tries to maximize its own return, and the returns are zero-sum. The set of\n“destabilizing actions” available to the antagonist is assumed to be domain knowledge, and\ngiven to the adversary ahead of time.\nFor multi-agent RL tasks, several works have shown how simple interaction between\nmultiple learning agents in an environment can result in emergent curricula. Such ideas\n18\nCurriculum Learning for Reinforcement Learning Domains\nwere explored early on in the context of evolutionary algorithms by Rosin and Belew (1997).\nThey showed that competition between 2 groups of agents, dubbed hosts and parasites,\ncould lead to an “arms race,” where each group drives the other to acquire increasingly\ncomplex skills and abilities. Similar results have been shown in the context of RL agents by\nBaker et al. (2020). They demonstrated that increasingly complex behaviors can emerge in\na physically grounded task. Speciﬁcally, they focus on a game of hide and seek, where there\nare two teams of agents. One team must hide with the help of obstacles and other items\nin the environment, while the other team needs to ﬁnd the ﬁrst team. They were able to\nshow that as one team converged on a successful strategy, the other team was pressured to\nlearn a counter-strategy. This process was repeated, inducing a curriculum of increasingly\ncompetitive agents.\nA similar idea was explored by Bansal et al. (2018). They proposed to use multi-agent\ncurriculum learning as an alternative to engineering dense shaping rewards. Their method\ninterpolates between dense “exploration” rewards, and sparse multi-agent competitive re-\nwards, with the exploration reward gradually annealed over time. In order to prevent the\nadversarial agent from getting too far ahead of the learning agent and making the task im-\npossible, the authors propose to additionally sample older versions of the opponent. Lastly,\nin order to increase robustness, the stochasticity of the tasks is increased over time.\nCurriculum learning approaches have also been proposed for cooperative multi-agent\nsystems (Wang et al., 2020; Yang et al., 2020). In these settings, there is a natural cur-\nriculum created by starting with a small number of agents, and gradually increasing them\nin subsequent tasks. The schedule with which to increase the number of agents is usually\nmanually deﬁned, and the emphasis instead is on how to perform transfer when the number\nof agents change. Therefore, we discuss these approaches in more detail in Section 4.3.\nFinally, while self-play has been successful in a wide variety of domains, including solving\ngames such as Backgammon (Tesauro, 1995) and Go (Silver et al., 2016), such an approach\nalone was not suﬃcient for producing strong agents in a complex, multi-agent, partially-\nobservable game like Starcraft. One of the primary new elements of Vinyals et al. (2019)\nwas the introduction of a Starcraft League, a group of agents that have diﬀering strategies\nlearned from a combination of imitation learning from human game data and reinforcement\nlearning. Rather than have every agent in the league maximize their own probability of\nwinning against all other agents like in standard self play, there were some agents that did\nthis, and some whose goal was to optimize against the main agent being trained. In eﬀect,\nthese agents were trained to exploit weaknesses in the main agent and help it improve.\nTraining against diﬀerent sets of agents over time from the league induced a curriculum\nthat allowed the main agents to achieve grandmaster status in the game.\n4.2.3 Reward and Initial/Terminal State Distribution Changes\nThus far, the curriculum consisted of ordering experience from the target task or modifying\nagents in the target environment. In the next two sections, we begin to examine approaches\nthat explicitly create diﬀerent MDPs for intermediate tasks, by changing some aspect of\nthe MDP. First we consider approaches that keep the state and action spaces the same, as\nwell as the environment dynamics, but allow the reward function and initial/terminal state\ndistributions to vary.\n19\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nOne of the earliest examples of this type of method was learning from easy missions.\nAsada et al. (1996) proposed this method to train a robot to shoot a ball into a goal\nbased on vision inputs. The idea was to create a series of tasks, where the agent’s initial\nstate distribution starts close to the goal state, and is progressively moved farther away in\nsubsequent tasks, inducing a curriculum of tasks. In this work, each new task starts one\n“step” farther away from the goal, where steps from the goal is measured using a domain\nspeciﬁc heuristic: a state is closer to the terminal state if the goal in the camera image\ngets larger. The heuristic implicitly requires that the state space can be categorized into\n“substates,” such as goal size or ball position, where the ordering of state transitions in a\nsubstate to a goal state is known. Thus, each substate has a dimension for making the task\nsimpler or more complex. Source tasks are manually created to vary along these dimensions\nof diﬃculty.\nRecently, Florensa et al. (2017) proposed more general methods for performing this\nreverse expansion. They proposed reverse curriculum generation, an algorithm that gener-\nates a distribution of starting states that get increasingly farther away from the goal. The\nmethod assumes at least one goal state is known, which is used as a seed for expansion.\nNearby starting states are generated by taking a random walk from existing starting states\nby selecting actions with some noise perturbation. In order to select the next round of\nstarting states to expand from, they estimate the expected return for each of these states,\nand select those that produce a return between a manually set minimum and maximum\ninterval. This interval is tuned to expand states where progress is possible, but not too\neasy. A similar approach by Ivanovic et al. (2019) considered combining the reverse ex-\npansion phase for curriculum generation with physics-based priors to accelerate learning by\ncontinuous control agents.\nAn opposite “forward” expansion approach has also been considered by Florensa et al.\n(2018). This method allows an agent to automatically discover diﬀerent goals in the state\nspace, and thereby guide exploration of the space. They do this discovery with a Gener-\native Adversarial Network (GAN) (Goodfellow et al., 2014), where the generator network\nproposes goal regions (parameterized subsets of the state space) and the discriminator eval-\nuates whether the goal region is of appropriate diﬃculty for the current ability of the agent.\nGoal regions are speciﬁed using an indicator reward function, and policies are conditioned\non the goal in addition to the state, like in a universal value function approximator (Schaul\net al., 2015). The agent trains on tasks suggested by the generator. In detail, the approach\nconsists of 3 parts: 1) First, goal regions are labelled according to whether they are of appro-\npriate diﬃculty. Appropriate goals are those that give a return between hyperparameters\nRmin and Rmax. Requiring at least Rmin ensures there is a signal for learning progress.\nRequiring less than Rmax ensures that it is not too easy. 2) They use the labeled goals\nto train a Goal GAN. 3) Goals are sampled from the GAN as well as a replay buﬀer, and\nused for training to update the policy. The goals generated by the GAN shift over time to\nreﬂect the diﬃculty of the tasks, and gradually move from states close to the starting state\nto those farther away.\nRacaniere et al. (2019) also consider an approach to automatically generate a curriculum\nof goals for the agent, but for more complex goal-conditioned tasks in dynamic environments\nwhere the possible goals vary between episodes. The idea was to train a “setter” model\nto propose a curriculum of goals for a “solver” agent to attempt to achieve. In order to\n20\nCurriculum Learning for Reinforcement Learning Domains\nhelp the setter balance its goal predictions, they proposed three objectives which lead to a\ncombination of three losses to train the setter model: goal validity (the goal should be valid\nor achievable by the current solver), goal feasibility (the goal should match the feasibility\nestimates for the solver with current skill), and goal coverage (encourage the setter to choose\nmore diverse goals to encourage exploration in the space of goals). In addition, a “judge”\nmodel was trained to predict the reward the current solver agent would achieve on a goal\n(the feasibility of a goal) proposed by the setter. Their experimental results demonstrate\nthe necessity of all three criteria for building useful curricula of goals. They also show that\ntheir approach is more stable and eﬀective than the goal GAN method (Florensa et al.,\n2018) on complex tasks.\nAn alternative to modifying the initial or terminal state distribution is to modify the\nreward function. Riedmiller et al. (2018) introduce SAC-X (Scheduled Auxiliary Control),\nan algorithm for scheduling and executing auxiliary tasks that allow the agent to eﬃciently\nexplore its environment and also make progress towards solving the ﬁnal task. Auxiliary\ntasks are deﬁned to be tasks where the state, action, and transition function are the same\nas the original MDP, but where the reward function is diﬀerent. The rewards they use\nin auxiliary tasks correspond to changes in raw or high level sensory input, similar to\nJaderberg et al. (2017). However, while Jaderberg et al. (2017) only used auxiliary tasks\nfor improving learning of the state representation, here they are used to guide exploration,\nand are sequenced.\nThe approach is a hierarchical RL method: they need to 1) learn\nintentions, which are policies for the auxiliary tasks, and 2) learn the scheduler, which\nsequences intention policies and auxiliary tasks.\nTo learn the intentions, they learn to\nmaximize the action-value function of each intention from a starting state distribution that\ncomes as a result of following each of the other intention policies.\nThis process makes\nthe policies compatible. The scheduler can be thought of as a meta-agent that performs\nsequencing, whose goal is to maximize the return on the target task MDP. The scheduler\nselects intentions, whose policy is executed on the extrinsic task, and is used to guide\nexploration.\nHeuristic-based methods have also been designed to sequence tasks that diﬀer in their\nreward functions. One such approach is SAGG-RIAC (Self-Adaptive Goal Generation -\nRobust Intelligent Adaptive Curiosity) (Baranes and Oudeyer, 2013). They deﬁne compe-\ntence as the distance between the achieved ﬁnal state and the goal state, and interest as\nthe change in competence over time for a set of goals. A region of the task space is deemed\nmore interesting than others, if the latest tasks in the region have achieved a high increase\nin competence. The approach repeatedly selects goals by ﬁrst picking a region with a prob-\nability proportional to its interest, and then choosing a goal at random within that region.\nWith a smaller probability the system also selects a goal at random over the whole task set\nor a goal close to a previously unsuccessful task. The bias towards interesting regions causes\nthe goals to be more dense in regions where the competence increases the fastest, creating\na curriculum. Because of the stochastic nature of the goal generating process, however, not\nevery task is necessarily beneﬁcial in directly increasing the agent’s ability on the target\ntask, but contributes to updating the competence and interest measures. Since the inter-\nmediate tasks are generated online as the agent learns, in this approach both sequencing\nand generation result from the same sampling process.\n21\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nFinally, Wu and Tian (2017) also consider changing the transition dynamics and the\nreward functions of the intermediate tasks. They propose a novel framework for training\nan agent in a partially observable 3D Doom environment. Doom is a First-Person Shooter\ngame, in which the player controls the agent to ﬁght against enemies. In their experiment,\nthey ﬁrst train the agent on some simple maps with several curricula. Each curriculum\nconsists of a sequence of progressively more complex environments with varying domain\nparameters (e.g., the movement speed or initial health of the agent).\nAfter learning a\ncapable initial task model, the agent is then trained on more complicated maps and more\ndiﬃcult tasks with a diﬀerent reward function. They also design an adaptive curriculum\nlearning strategy in which a probability distribution over diﬀerent levels of curriculum is\nmaintained.\nWhen the agent performs well on the current distribution, the probability\ndistribution is shifted towards more diﬃcult tasks.\n4.2.4 No restrictions\nNext, there is a class of methods that create a curriculum using intermediate tasks, but\nmake no restrictions on the MDPs of these intermediate tasks.\nWe categorize them in\nthree ways by how they address the task sequencing problem: treating sequencing 1) as\nan MDP/POMDP, 2) as a combinatorial optimization over sequences, and 3) as learning\nthe connections in a directed acyclic task graph. Because there are no limitations on the\ntypes of intermediate tasks allowed, some assumptions are usually made about the transfer\nlearning algorithm, and additional information about the intermediate tasks (such as task\ndescriptors) is typically assumed. Finally, we also discuss work on an auxiliary problem to\nsequencing: how long to spend on each task.\nMDP-based Sequencing\nThe ﬁrst formalization of the sequencing problem is as a Markov Decision Process. These\nmethods formulate curriculum generation as an interaction between 2 types of MDPs. The\nﬁrst is the standard MDP, which models a learning agent (i.e., the student) interacting with\na task. The second is a higher level meta-MDP for the curriculum agent (i.e., the teacher),\nwhose goal is to select tasks for the learning agent.\nNarvekar et al. (2017) denote the meta-MDP as a curriculum MDP (CMDP), where the\nstate space S is the set of policies the learning agent can represent. These can be represented\nparametrically using the weights of the learning agent. The action space A is the set of\ntasks the learning agent can train on next. Learning a task updates the learning agent’s\npolicy, and therefore leads to a transition in the CMDP via a transition function p. Finally,\nthe reward function r is the time in steps or episodes that it took to learn the selected task.\nUnder this model, a curriculum agent typically starts in an initial state corresponding to a\nrandom policy for the learning agent. The goal is to reach a terminal state, which is deﬁned\nas a policy that can achieve some desired performance threshold on the target task, as fast\nas possible.\nMatiisen et al. (2017) consider a similar framework, where the interaction is deﬁned as\na POMDP. The state and action spaces of the meta-POMDP are the same as in Narvekar\net al. (2017), but access to the internal parameters of the learning agent is not available.\nInstead, an observation of the current score of the agent on each intermediate task is given.\n22\nCurriculum Learning for Reinforcement Learning Domains\nThe reward is the change in the score on the task from this timestep to the previous\ntimestep when the same task was trained on. Thus, while Narvekar et al. (2017) focused on\nminimizing time to threshold performance on the target task, the design of Matiisen et al.\n(2017) aims to maximize the sum of performance in all tasks encountered.\nWhile both approaches are formalized as POMDPs, learning on these POMDPs is\ncomputationally expensive. Thus, both propose heuristics to guide the selection of tasks.\nNarvekar et al. (2017) take a sample-based approach, where a small amount of experience\nsamples gathered on the target and intermediate tasks are compared to identify relevant\nintermediate tasks. The task that causes the greatest change in policy as evaluated on the\ntarget task samples is selected. In contrast, Matiisen et al. (2017) select tasks where the\nabsolute value of the slope of the learning curve is highest. Thus it selects tasks where the\nagent is making the most progress or where the agent is forgetting the most about tasks\nit has already learned. Initially tasks are sampled randomly. As one task starts making\nprogress, it will be sampled more, until the learning curve plateaus. Then another will be\nselected, and the cycle will repeat until all the tasks have been learned.\nSubsequently, Narvekar and Stone (2019) explored whether learning was possible in a\ncurriculum MDP, thus avoiding the need for heuristics in task sequencing. They showed that\nyou can represent a CMDP state using the weights of the knowledge transfer representation.\nFor example, if the agent uses value function transfer, the CMDP state is represented using\nthe weights of the value function. By utilizing function approximation over this state space,\nthey showed it is possible to learn a policy over this MDP, termed a curriculum policy,\nwhich maps from the current status of learning progress of the agent, to the task it should\nlearn next. In addition, the approach addresses the question of how long to train on each\nintermediate task.\nWhile most works have trained on intermediate tasks until learning\nplateaus, this is not always necessary. Narvekar and Stone (2019) showed that training\non each intermediate task for a few episodes, and letting the curriculum policy reselect\ntasks that require additional time, results in faster learning. However, while learning a\ncurriculum policy is possible, doing so independently for each agent and task is still very\ncomputationally expensive.\nCombinatorial Optimization and Search\nA second way of approaching sequencing is as a combinatorial optimization problem: given\na ﬁxed set of tasks, ﬁnd the permutation that leads to the best curriculum, where best\nis determined by one of the CL metrics introduced in Section 3.3. Finding the optimal\ncurriculum is a computationally diﬃcult black-box optimization problem. Thus, typically\nfast approximate solutions are preferred.\nOne such popular class of methods are metaheuristic algorithms, which are heuristic\nmethods that are not tied to speciﬁc problem domains, and thus can be used as black boxes.\nFoglino et al. (2019a) adapt and evaluate four representative metaheuristic algorithms to\nthe task sequencing problem: beam search (Ow and Morton, 1988), tabu search (Glover and\nLaguna, 1998), genetic algorithms (Goldberg, 1989), and ant colony optimization (Dorigo\net al., 1991). The ﬁrst two are trajectory-based, which start at a guess of the solution,\nand search the neighborhood of the current guess for a better solution. The last two are\npopulation-based, which start with a set of candidate solutions, and improve them as a\n23\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\ngroup towards areas of increasing performance. They evaluate these methods for 3 diﬀerent\nobjectives: time to threshold, maximum return (asymptotic performance), and cumulative\nreturn. Results showed that the trajectory-based methods outperformed their population-\nbased counterparts on the domains tested.\nWhile metaheuristic algorithms are broadly applicable, it is also possible to create spe-\nciﬁc heuristic search methods targeted at particular problems, such as task sequencing with\na speciﬁc transfer metric objective.\nFoglino et al. (2019b) introduce one such heuristic\nsearch algorithm, designed to optimize for the cumulative return. Their approach begins\nby computing transferability between all pairs of tasks, using a simulator to estimate the\ncumulative return attained by using one task as a source for another. The tasks are then\nsorted according to their potential of being a good source or target, and iteratively chained\nin curricula of increasing length. The algorithm is anytime, and eventually exhaustively\nsearches the space of all curricula with a predeﬁned maximum length.\nJain and Tulabandhula (2017) propose 4 diﬀerent online search methods to sequence\ntasks into a curriculum. Their methods also assume a simulator is available to evaluate\nlearning on diﬀerent tasks, and use the learning trajectory of the agent on tasks seen so far\nto select new tasks. The 4 approaches are: 1) Learn each source task for a ﬁxed number of\nsteps, and add the one that gives the most reward. The intuition is that high reward tasks\nare the easiest to make progress on. 2) Calculate a transferability matrix for all pairs of\ntasks, and create a curriculum by chaining tasks backwards from the target tasks greedily\nwith respect to it. 3) Extract a feature vector for each task (as in Narvekar et al., 2016),\nand learn a regression model to predict transferability using the feature vector. 4) Extract\npair wise feature vectors between pairs of tasks, and learn a regression model to predict\ntransferability.\nFinally, instead of treating the entire problem as a black box, it has also been treated as\na gray box. Foglino et al. (2019c) propose such an approach, formulating the optimization\nproblem as the composition of a white box scheduling problem and black box parameter\noptimization. The scheduling formulation partially models the eﬀects of a given sequence,\nassigning a utility to each task, and a penalty to each pair of tasks, which captures the\neﬀect on the objective of learning two tasks one after the other. The white-box scheduling\nproblem is an integer linear program, with a single optimal solution that can be computed\neﬃciently. The quality of the solution, however, depends on the parameters of the model,\nwhich are optimized by a black-box optimization algorithm. This external optimization\nproblem searches the optimal parameters of the internal scheduling problem, so that the\noutput of the two chained optimizers is a curriculum that maximizes cumulative return.\nGraph-based Sequencing\nAnother class of approaches explicitly treats the curriculum sequencing problem as connect-\ning nodes with edges into a directed acyclic task graph. Typically, the task-level curriculum\nformulation is used, where nodes in the graph are associated with tasks. A directed edge\nfrom one node to another implies that one task is a source task for another.\nExisting work has relied on heuristics and additional domain information to determine\nhow to connect diﬀerent task nodes in the graph. For instance, Svetlik et al. (2017) assume\nthe set of tasks is known in advance, and that each task is represented by a task feature\n24\nCurriculum Learning for Reinforcement Learning Domains\ndescriptor. These features encode properties of the domain. For example, in a domain like\nMs. Pac-Man, features could be the number of ghosts or the type of maze. The approach\nconsists of three parts. First, a binary feature vector is extracted from the feature vector\nto represent non-zero elements. This binary vector is used to group subsets of tasks that\nshare similar elements. Second, tasks within each group are connected into subgraphs using\na novel heuristic called transfer potential. Transfer potential is deﬁned for discrete state\nspaces, and trades oﬀthe applicability of a source task against the cost needed to learn it.\nApplicability is deﬁned as the number of states that a value function learned in the source\ncan be applied to a target task. The cost of a source task is approximated as the size of\nits state space. Finally, once subgraphs have been created, they are linked together using\ndirected edges from subgraphs that have a set of binary features to subgraphs that have a\nsuperset of those features.\nDa Silva and Reali Costa (2018) follow a similar procedure, but formalize the idea of task\nfeature descriptors using an object-oriented approach. The idea is based on representing\nthe domain as an object-oriented MDP, where states consist of a set of objects. A task OO-\nMDP is speciﬁed by the set of speciﬁc objects in this task, and the state, action, transition,\nand reward functions of the task. With this formulation, source tasks can be generated by\nselecting a smaller set of objects from the target task to create a simpler task. To create the\ncurriculum graph, they adapt the idea of transfer potential to the object-oriented setting:\ninstead of counting the number of states that the source task value function is applicable in,\nthey compare the sets of objects between the source and target tasks. While the sequencing\nis automated, human input is still required to make sure the tasks created are solvable.\nAuxiliary Problems\nFinally, we discuss an additional approach that tackles an auxiliary problem to sequencing:\nhow long to spend on each intermediate task in the curriculum. Most existing work trains\non intermediate tasks until performance plateaus. However, as we mentioned previously,\nNarvekar and Stone (2019) showed that this is unnecessary, and that better results can be\nobtained by training for a few episodes, and reselecting or changing tasks dynamically as\nneeded.\nBassich et al. (2020) consider an alternative method for this problem based on progres-\nsion functions. Progression functions specify the pace at which the diﬃculty of the task\nshould change over time. The method relies on the existence of a task-generation function,\nwhich maps a desired complexity ct ∈[0, 1] to a task of that complexity. The most com-\nplex task, for which ct = 1, is the ﬁnal task. After every episode, the progression function\nreturns the diﬃculty of the task that the agent should face at that time.\nThe authors\ndeﬁne two types of progression functions: ﬁxed progressions, for which the learning pace is\npredeﬁned before learning takes place; and adaptive progressions, which adjust the learning\npace online based on the performance of the agent. Linear and exponential progressions are\ntwo examples of ﬁxed progression functions, and increase the diﬃculty of the task linearly\nand exponentially, respectively, over a prespeciﬁed number of time steps. The authors also\nintroduce an adaptive progression based on a friction model from physics, which increases\nct as the agent’s performance is increasing, and slows down the learning pace if performance\ndecreases. Progression functions allow the method to change the task at every episode, solv-\n25\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\ning the problem of deciding how long to spend in each task, while simultaneously creating\na continually changing curriculum.\n4.2.5 Human-in-the-Loop Curriculum Generation\nThus far, all the methods discussed in Section 4.2 create a curriculum automatically using\na sequencing algorithm, which either reorders samples from the ﬁnal task or progressively\nalters how much intermediate tasks in the curriculum may diﬀer. Bengio et al. (2009) and\nTaylor (2009) both emphasize the importance of better understanding how humans ap-\nproach designing curricula. Humans may be able to design good curricula by considering\nwhich intermediate tasks are “too easy” or “too hard,” given the learner’s current ability\nto learn, similar to how humans are taught with the zone of proximal development (Vygot-\nsky, 1978). These insights could then be leveraged when designing automated curriculum\nlearning systems. Therefore, in this section, we consider curriculum sequencing approaches\nthat are done manually by humans who are either domain experts, who have specialized\nknowledge of the problem domain, or naive users, who do not necessarily know about the\nproblem domain and/or machine learning.\nOne example of having domain experts manually generate the curriculum is the work\ndone by Stanley et al. (2005), in which they explore how to keep video games interesting\nby allowing agents to change and to improve through interaction with the player. They use\nthe NeuroEvolving Robotic Operatives (NERO) game, in which simulated robots start the\ngame with no skills and have to learn complicated behaviors in order to play the game. The\nhuman player takes the role of a trainer and designs a curriculum of training scenarios to\ntrain a team of simulated robots for military combat. The player has a natural interface\nfor setting up training exercises and specifying desired goals. An ideal curriculum would\nconsist of exercises with increasing diﬃculty so that the agent can start with learning basic\nskills and gradually building on them. In their experiments, the curriculum is designed by\nseveral NERO programmers who are familiar with the game domain. They show that the\nsimulated robots could successfully be trained to learn diﬀerent sophisticated battle tactics\nusing the curriculum designed by these domain experts. It is unclear whether the human\nplayer who is not familiar with the game can design good curriculum.\nA more recent example is by MacAlpine and Stone (2018). They use a very extensive\nmanually constructed curriculum to train agents to play simulated robot soccer. The cur-\nriculum consists of a training schedule over 19 diﬀerent learned behaviors. It encompasses\nskills such as moving to diﬀerent positions on the ﬁeld with diﬀerent speeds and rotation,\nvariable distance kicking, and accessory skills such as getting up when fallen. Optimizing\nthese skills independently can lead to problems at the intersection of these skills. For ex-\nample, optimizing for speed in a straight walk can lead to instability if the robot needs\nto turn or kick due to changing environment conditions. Thus, the authors of this work\nhand-designed a curriculum to train related skills together using an idea called overlapping\nlayered learning. This curriculum is designed using their domain knowledge of the task and\nagents.\nWhile domain experts usually generate good curricula to facilitate learning, most ex-\nisting work does not explicitly explore their curriculum design process. It is unclear what\nkind of design strategies people follow when sequencing tasks into a curriculum. Published\n26\nCurriculum Learning for Reinforcement Learning Domains\n(a)\n(b)\nFigure 4: One example of curricula designed by human users. (a) Given ﬁnal task. (b) A\ncurriculum designed by one human participant.\nresearch on Interactive Reinforcement Learning (Thomaz and Breazeal, 2006; Knox and\nStone, 2009; Suay and Chernova, 2011; Knox and Stone, 2012; Griﬃth et al., 2013; Subra-\nmanian et al., 2016; Loftin et al., 2016; MacGlashan et al., 2017) has shown that RL agents\ncan successfully speed up learning using human feedback, demonstrating the signiﬁcant role\ncan humans play in teaching an agent to learn a (near-) optimal policy. This large body\nof work mainly focuses on understanding how human teachers want to teach the agent and\nhow to incorporate these insights into the standard RL framework. Similarly, the way we\ndeﬁne curriculum design strategies still leaves a lot to be deﬁned by human teachers. As\npointed out by Bengio et al. (2009), the notion of simple and complex tasks is often based\non human intuition, and there is value in understanding how humans identify “simple”\ntasks. Along these lines, some work has been done to study whether curriculum design\nis a prominent teaching strategy that naive users choose to teach the agent and how they\napproach designing curricula.\nTo study the teaching strategies followed by naive users, Khan et al. (2011) conduct\nbehavioral studies in which human participants need to teach a robot the concept of whether\nan object can be grasped with one hand. In their experiment, participants are provided\nwith 31 cards with photos of common objects (e.g., food, furniture, and animals) for them\nto select. The experiment consists of two subtasks. In the ﬁrst subtask, participants sort\nthe objects on the table based on their subjective ratings of their graspability. In the second\nsubtask, participants pick up the cards from the table and show them to the robot while\nteaching the robot the concept of graspability, using as few cards as possible. While teaching\nthe robot the object’s graspability, participants can either use any natural language or say\neither “graspable” or “not graspable,” depending on one of the two conditions they are\nrandomly assigned. They observe that participants follow three distinct teaching strategies,\none of which is consistent with the curriculum learning principle, i.e., starting simple and\ngradually increasing the diﬃculty of the task. Furthermore, they propose a novel theoretical\nframework as a potential explanation for the teaching strategy that follows the curriculum\nlearning principle, which shows that it is the result of minimizing per-iteration expected\nerror of the learner.\nPeng et al. (2018) also explore how naive users design a curriculum of tasks for an agent,\nbut in a more complex sequential decision-making task. Speciﬁcally, a simple simulated\nhome environment is used, where the agent must learn to perform tasks in a variety of\n27\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nenvironments.\nThe tasks are speciﬁed via text commands and the agent is trained to\nperform the task via reinforcement and punishment feedback from a human trainer. It uses\nthe goal-directed Strategy-Aware Bayesian Learning (SABL) algorithm (Loftin et al., 2016)\nfor learning from human feedback. In the user study, participants are asked to design a set\nof training assignments for the agent to help it quickly learn to complete the given ﬁnal\nassignment (shown in Figure 4a). A set of source tasks are provided for human participants\nto select and sequence. One example of curricula designed by human participants is shown in\nFigure 4b. Their empirical results show that, compared to directly learning the pre-speciﬁed\nﬁnal task from scratch, non-expert humans can successfully design curricula that result in\nbetter overall agent performance on learning both the entire curriculum and the ﬁnal task.\nThey also discover that humans are more likely to select commands for intermediate tasks\nthat include concepts that are important for the ﬁnal task, and that doing so results in\ncurricula that lead to better overall agent performance. Furthermore, they demonstrate\nthat by taking advantage of this type of non-expert guidance, their curriculum-learning\nalgorithm can be adapted to learn the human-generated curricula more eﬃciently.\nThere is also some work that does not explicitly ask humans to design a curriculum, but\nuses human data to help generate the curriculum. One example is the work done by Hosu\nand Rebedea (2016), in which they propose a deep RL method that combines online agent\nexperiences with oﬄine human experiences to train the agent more eﬃciently.\nIn some\nsparse-reward Atari games such as Montezuma’s Revenge and Private Eye, the agent needs\nto execute a long sequence of speciﬁc actions to receive the ﬁrst positive reward from the\nenvironment, which makes the exploration problem much harder. Thus, the commonly used\nϵ-greedy strategy could not ﬁnd any game paths to reach a ﬁrst state with positive reward,\npreventing the neural network from learning relevant features to good states. Inspired by\ncurriculum learning and the human starts evaluation metric used for testing Atari agents,\nthey use checkpoints sampled from a human player’s game experience as starting points\nfor the learning process. The main intuition behind this approach is that at least some\nof the checkpoints will be an “easier” starting point, which is closer to some states with\npositive reward that the agent can beneﬁt from. While this method belongs to the class of\nsequencing approaches, as discussed in Section 4.2.1, that reorders samples in the ﬁnal task\nto derive a curriculum, it additionally considers more informative sample data generated\nby naive human users in order to build a more eﬃcient curriculum.\nWe ﬁnd that very limited work has been done on investigating how humans design cur-\nricula. While the work discussed in this section enriches our empirical understanding of\nhuman teaching and gives us some insights into the development of new machine-learning\nalgorithms and interfaces that can better accommodate machine- or human-created curric-\nula, we believe more work needs to be done along this line.\n4.3 Knowledge Transfer\nWhile we view sequencing, as covered in Section 4.2, to be the core concept of curriculum\nlearning, the whole premise of CL depends on an agent’s ability to transfer knowledge\namong tasks. While a full discussion of transfer learning for RL is beyond the scope of this\nsurvey, this subsection is designed to provide the reader a brief introduction to the area so\nthat they can eﬀectively leverage it as part of their own explorations in curriculum learning.\n28\nCurriculum Learning for Reinforcement Learning Domains\nCitation\nIntermediate\nTask\nGeneration\nCurriculum\nRepresentation\nTransfer\nMethod\nCurriculum\nSequencer\nCurriculum\nAdaptivity\nEvaluation\nMetric\nApplication\nArea\nClegg et al. (2017)\ndomain experts\nsequence\npolicies\ndomain experts\nstatic\nasymptotic, time to threshold\nsim robotics\nFujii et al. (1998)\ndomain experts\nsequence\npartial policies\ndomain experts\nstatic\nasymptotic\nreal robotics\nKarpathy and Van De Panne (2012)\ndomain experts/target\nsequence/single\npartial policies /no transfer\ndomain experts/automatic\nstatic/adaptive\ntime to threshold\nsim robotics\nRusu et al. (2016)\ndomain experts\nsequence\npolicies\ndomain experts\nstatic\nasymptotic\nvideo games\nShao et al. (2018)\ndomain experts\nsequence\ntask model\ndomain experts\nstatic\nasymptotic, total reward\nvideo games\nSinapov et al. (2015)\nautomatic\nsequence\nvalue function\nautomatic\nstatic\njump start\nvideo games\nTessler et al. (2017)\ndomain experts\nsequence\npartial policies\ndomain experts\nstatic\nasymptotic\nvideo games\nVezhnevets et al. (2016)\nautomatic\nsequence\npartial policies\nautomatic\nstatic\nasymptotic, total reward\nvideo games\nWang et al. (2020)\ndomain experts\nsequence\npolicies\ndomain experts\nstatic\nasymptotic\nvideo games\nYang and Asada (1996)\ndomain experts\nsequence\npartial policies\nautomatic\nadaptive\nasymptotic, time to threshold\nreal robotics\nYang et al. (2020)\ndomain experts\nsequence\npolicies\ndomain experts\nstatic\nasymptotic, time to threshold\ntoy, other\nZimmer et al. (2018)\ndomain experts\nsequence\npartial policies\ndomain experts\nstatic\nasymptotic, total reward\nsim robotics\nTable 3: The papers discussed in Section 4.3, categorized along the dimensions presented\nin Section 3.4. Bolded values under evaluation metric indicate strong transfer.\nIn curriculum learning, transfer learning methods are used to allow the agent to reuse\nknowledge learned from one intermediate task to another within the curriculum. It is worth\nnoting that when creating a curriculum using only samples from the target task (discussed\nin Section 4.2.1), there is no transfer as there is only a single task (the target task) and\ncorrespondingly no change in the environment. However, when creating a curriculum using\nmultiple intermediate tasks, which may diﬀer in state/action space, reward function, or\ntransition function from the ﬁnal task, transfer learning is needed to extract and pass on\nreusable knowledge acquired in one intermediate task to the next. The type of knowledge\ntransferred also directly aﬀects the type of learner that is applicable to the learning process.\nTransferred knowledge can be low-level, such as an entire policy, a value function, a\nfull task model, or some training instances, which can be directly used to initialize the\nlearner in the target task. The knowledge can also be high-level, such as partial policies\nor options, skills, shaping rewards, or subtask deﬁnitions. This type of information may\nnot fully initialize the learner in the target task, but it could be used to guide the agent’s\nlearning process in the target task. In this subsection, we discuss diﬀerent transfer learning\napproaches used in curricula.\nIn policy transfer, a policy learned in a source or intermediate task is used to initialize\nthe policy in the target task. When transferring policies between diﬀerent tasks, the tasks\nmay diﬀer in some aspect of the MDP, such as starting states (Florensa et al., 2017), reward\nfunctions (Florensa et al., 2018; Riedmiller et al., 2018), or transition functions (Clegg et al.,\n2017).\nFor instance, Clegg et al. (2017) demonstrate that an arm-like manipulator can\nsuccessfully learn the control policy for a simulated dressing task, by transferring policies\nbetween tasks with diﬀerent transition functions. In a dressing task, the goal is to achieve a\ndesired relative positioning of the garment and the limb. To do this, they ﬁrst train a sphere\nto move through a funnel-like geometry to reach some target location. They then directly\napply the learned policy to a diﬀerent scenario in which a manipulator with arbitrary shape\nnavigates through a simulated garment. The main trick is to train multiple spheres using\na curriculum learning strategy and then aggregate them to control the manipulator in the\ndressing task.\nIn Shao et al. (2018), a learned task model is transferred between tasks, which is used\nto initialize the policy network. Thus, it is similar to transferring policies. Their work aims\n29\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nto solve the problem of multi-agent decision making in StarCraft micromanagement, where\nthe goal is to control a group of units to destroy the enemy under certain terrain condi-\ntions. A parameter sharing multi-agent gradient-descent Sarsa(λ) (PS-MAGDS) method\nis proposed to train the units to learn an optimal policy, which is parametrized by a feed-\nforward neural network. PS-MAGDS simply extends the traditional Sarsa(λ) to multiple\nunits by sharing parameters of the policy network among units to encourage cooperative\nbehaviors. A reward function including small immediate rewards is also designed to accel-\nerate the learning process. When using transfer learning in their experiments, the agents\nare ﬁrst trained in some small scale source scenarios using PS-MAGDS. The well-trained\nmodel is then used to initialize the policy network to learn micromanagement in the target\nscenarios. To scale the combat to a large scale scenario, they combine curriculum learning\nand transfer learning where the agents are trained with a sequence of progressively more\ncomplex micromanagement tasks. The diﬃculty of the micromanagement task is controlled\nby changing the number and type of units.\nValue function transfer is another common method for transferring low-level knowledge\nbetween intermediate tasks within a curriculum. In most existing work (Sinapov et al., 2015;\nNarvekar et al., 2017; Da Silva and Reali Costa, 2018), value function transfer is achieved\nby using the parameters of a value function learned in one intermediate task to initialize the\nvalue function in the next intermediate task in the curriculum, such that the agent learns\nthe ﬁnal task with some initial policy that is better than random exploration. For example,\nSinapov et al. (2015) focus on addressing the task selection problem in curriculum learning\nusing value function transfer, under the assumption that no samples from the ﬁnal tasks are\navailable. They propose to use meta-data (i.e., a ﬁxed-length feature vector that describes\nthe task) associated with each task to identify suitable intermediate tasks. The main idea\nis to use such meta-data to learn the beneﬁts of transfer between diﬀerent ‘source-target’\ntask pairs, and have this generalize to new unseen task pairs to guide task selection.\nWhen transferring low-level policies or value functions across tasks, there are several\nchallenges that arise, particularly in the modern context of deep reinforcement learning.\nFirst is the problem of catastrophic forgetting, where knowledge from previously learned\ntasks is lost as information on a new task is incorporated.\nThis eﬀect occurs because\nthe weights of the neural network optimized for a ﬁrst task must be changed to meet\nthe objectives of a new task, often resulting in poorer performance on the original task.\nTypically, in the curriculum setting, we only care about performance in the ﬁnal tasks.\nHowever, if information from two orthogonal tasks needs to be combined (such as two\nindependent skills), this challenge needs to be addressed.\nOne approach is progressive\nneural networks (Rusu et al., 2016), which trains a new network “column” for each new\ntask, and leverages lateral connections to previously learned network columns to achieve\ntransfer. When training subsequent columns, parameters from previous columns are frozen,\nwhich prevents catastrophic forgetting. The limitation is that the number of parameters\ngrows with the number of tasks, and at inference time, the task label is needed to know\nwhich column to extract output from.\nA second problem is the case where the state and action spaces diﬀer between tasks.\nOne alternative is to transfer higher-level knowledge across tasks, such as partial policies\nor options. A partial policy is a policy that is not necessarily deﬁned for all states in the\nstate space of an MDP. We use partial policies as an umbrella term to represent closely\n30\nCurriculum Learning for Reinforcement Learning Domains\nrelated ideas such as options, skills, and macro-actions.\nYang and Asada (1996) transfer\nlearned control parameters between tasks, which are similar to partial policies. To solve\nthe impedance learning problem for high-speed robotic assembly, they allow the system to\nlearn impedance parameters associated with diﬀerent dynamic motions separately, rather\nthan to learn all the control parameters simultaneously. For instance, they ﬁrst learn only\nthe parameters associated with quasistatic motion by driving the system slowly, leaving\nother parameters unlearned. After the quasistatic parameters have been learned, they then\nslightly increase the motion speed, and use the learned values to initialize the quasistatic\nparameters when learning other parameters. Another example of transferring partial policies\nbetween tasks is the work done by Zimmer et al. (2018). Their main idea is to progressively\nincrease the dimensionality of the tackled problem by increasing the (continuous) state and\naction spaces of the MDP, while an agent is learning a policy. The agent ﬁrst learns to\nsolve the source task with reduced state and action spaces until the increase in performance\nstagnates. Then, the partial policy learned by the agent is used as an initialization to learn\nthe full policy in the target task with full state and action spaces. A developmental layer\n(like a dropout layer) is added to the network to ﬁlter dimensions of the states and actions.\nSimilarly, Fujii et al. (1998) transfer options between tasks. To train mobile robots to\nlearn collision avoidance behaviors in multi-robot systems more eﬃciently, they develop a\nmulti-layered RL mechanism. Rather than gradually increasing the level of task complexity\nbased on the learner’s performance as in Yang and Asada (1996), their learning process\nconsists of four stages like a curriculum in which each stage learns a pre-deﬁned controller.\nEach controller learns an option to solve a pre-deﬁned sub-task. For instance, the ﬁrst\ncontroller learns to move toward a speciﬁc goal. Then the output (goal-directed behavior)\nof the ﬁrst controller is used as input for the second controller, which aims to learn to avoid\nthe collision to a single robot, and so on.\nVezhnevets et al. (2016) also transfer high-level macro-actions between tasks, which are\nsimpler instances of options. In their experiment, the agent is trained with a curriculum\nwhere the goal state is ﬁrst set to be very close to the start state and is then moved further\naway during learning process. Although the task gets progressively harder, the temporally\nabstracted macro-actions remain the same. The macro-actions learned early on can also\nbe easily adapted using their proposed architecture. Speciﬁcally, a deep recurrent neural\nnetwork architecture is used to maintain a multi-step action plan. The network learns when\nto commit to the action plan to generate macro-actions and when to update the plan based\non observations.\nAnother mechanism for transfer are skills.\nTessler et al. (2017) propose a deep RL\nmethod that eﬀectively retains and transfers learned skills to solve lifelong learning in\nMineCraft. In their work, a set of N skills are trained a priori on various sub-tasks, which\nare then reused to solve the harder composite task. In their MineCraft experiment, the\nagent’s action space includes the original primitive actions as well as the set of pre-learned\nskills (e.g., navigate and pickup). A hierarchical architecture is developed to learn a policy\nthat determines when to execute primitive actions and when to reuse pre-learned skills, by\nextending the vanilla DQN architecture (Mnih et al., 2015). The skills could be sub-optimal\nwhen they are directly reused for more complex tasks, and this hierarchical architecture\nallows the agent to learn to reﬁne the policy by using primitive actions. They also show\n31\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nthe potential for reusing the pre-learned skill to solve related tasks without performing any\nadditional learning.\nRather than selectively reusing pre-learned skills, Karpathy and Van De Panne (2012)\nfocus on learning motor skills in an order of increasing diﬃculty.\nThey decompose the\nacquisition of skills into a two-level curriculum: a high-level curriculum speciﬁes the order\nin which diﬀerent motor skills should be learned, while the low-level curriculum deﬁnes the\nlearning process for a speciﬁc skill. The high-level curriculum orders the skills in a way\nsuch that each skill is relatively easy to learn, using the knowledge of the previously learned\nskills. For instance, the Acrobot ﬁrst learns the Hop (easy to learn from scratch) and Flip\n(similar to hopping very slowly) skills, and then learns the more complex Hop-Flip skill.\nThe learned skill-speciﬁc task parameters for easier skills will highly constrain the states\nthat the Acrobat could be in, making it easier to learn more complex skills. For example,\nthe Hop-Flip skills begin from a hopping gait of some speed, which can be reached by\nrepeatedly executing the previously learned Hop skill.\nIn multi-agent settings, several speciﬁc methods have been designed for curricula that\nprogressively scale the number of agents between tasks. In these settings, the state and\naction spaces often scale based on the number of agents present. One common assumption\nin many of these methods is that the state space can be factored into elements for the\nenvironment senv, the agent sn, and all other agents s−n. For example, Yang et al. (2020)\npropose CM3, which takes a two-stage approach. In the ﬁrst stage, a single agent is trained\nwithout the presence of other agents. This is done by inducing a new MDP that removes\nall dependencies on agent interactions (i.e., removing s−n) and training a network on this\nsubspace. Then in the second stage, cooperation is learned by adding the parameters for\nthe other agents into the network.\nWang et al. (2020) propose 3 diﬀerent approaches for multi-agent settings. The ﬁrst is\nbuﬀer reuse, which saves the replay buﬀers from all previous tasks, and samples experience\nfrom all of them to train in the current task. Samples from lower dimensional tasks are\npadded with zeros. The second is curriculum distillation, which adds a distillation loss based\non KL divergence between policies/q-values between tasks. The third is transferring the\nmodel using a new network architecture called Dynamic Agent-number Network (DyAN).\nIn this architecture, the state space elements related to the agent and environment go\nthrough a fully connected network, while the observations for each teammate agent are\npassed through a graph neural network (GNN) and then aggregated. These networks are\nsubsequently combined to produce q-values or policies.\n5. Related Areas and Paradigms\nCurriculum learning is an idea that has been studied in other areas of machine learning and\nhuman education, and is similar to several existing paradigms in reinforcement learning. In\nthis section, we ﬁrst relate curriculum learning to approaches in reinforcement learning that\naim to improve sample complexity, and that consider learning multiple sets of tasks (Section\n5.1). Then we describe approaches to learn curricula in supervised learning (Section 5.2) and\nfor teaching and human education (Section 5.3). We include these approaches with the idea\nthat the insights discovered in these areas could be adapted to apply to the reinforcement\nlearning setting with autonomous agents.\n32\nCurriculum Learning for Reinforcement Learning Domains\n5.1 Related Paradigms in Reinforcement Learning\nOne of the central challenges in applying reinforcement learning to real world problems\nis sample complexity. Due to issues such as a sparse reward signal or complex dynamics,\ndiﬃcult problems can take an RL agent millions of episodes to learn a good policy, with\nmany suboptimal actions taken during the course of learning. Many diﬀerent approaches\nhave been proposed to deal with this issue. To name a few, one method is imitation learning\n(Schaal, 1997), which uses demonstrations from a human as labels for supervised learning\nto bootstrap the learning process. Another example is oﬀ-policy learning (Hanna et al.,\n2017), which uses existing data from an observed behavior policy, to estimate the value of a\ndesired target policy. Model-based approaches (Sutton and Barto, 1998) ﬁrst learn a model\nof the environment, which can then be used for planning the optimal policy.\nEach of these methods come with their advantages and disadvantages. For imitation\nlearning, the assumption is that human demonstrations are available. However, these are\nnot always easy to obtain, especially when a good policy for the task is not known. In oﬀ-\npolicy learning, in order to make full use of existing data, it is assumed that the behavior\npolicy has a nonzero probability of selecting each action, and typically that every action\nto be evaluated or the target policy has been seen at least once.\nFinally, model-based\napproaches typically ﬁrst learn a model of the environment, and then use it for planning.\nHowever, any inaccuracies in the learned model can compound as the planning horizon\nincreases.\nCurriculum learning takes a diﬀerent approach, and makes a diﬀerent set of\nassumptions. The primary assumption is that the environment can be conﬁgured to create\ndiﬀerent subtasks, and that it is easier for the agent to discover on its own reusable pieces\nof knowledge in these subtasks that can be used for solving a more challenging task.\nWithin reinforcement learning, there are also several paradigms that consider learning on\na set of tasks so as to make learning more eﬃcient. Multitask learning, lifelong/continuous\nlearning, active learning, and meta-learning are four such examples.\nIn multitask learning, the goal is to learn how to solve sets of prediction or decision\nmaking tasks. Formally, given a set of tasks m1, m2, . . . mn, the goal is to co-learn all of\nthese tasks, by optimizing the performance over all n tasks simultaneously. Typically, this\noptimization is facilitated by learning over some shared basis space. For example, Caruana\n(1997) considers multitask learning for supervised learning problems, and shares layers of a\nneural network between tasks. In supervised learning, these tasks are diﬀerent classiﬁcation\nor regression problems. Similar ideas have been applied in a reinforcement learning context\nby Wilson et al. (2007). In reinforcement learning, diﬀerent tasks correspond to diﬀerent\nMDPs.\nLifelong learning and continual learning can be viewed as an online version of multitask\nlearning. Tasks are presented one at a time to the learner, and the learner must use shared\nknowledge learned from previous tasks to more eﬃciently learn the presented task. As in\nmultitask learning, typically the goal is to optimize performance over all tasks given to the\nlearner. Lifelong and continual learning have been examined in both the supervised setting\n(Ruvolo and Eaton, 2013a) and the reinforcement learning setting (Ring, 1997; Ammar\net al., 2014). The distinguishing feature of curriculum learning compared to these works is\nthat in curriculum learning, we have full control over the order in which tasks are selected.\nIndeed, we may have control over the creation of tasks as well. In addition, the goal is to\n33\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\noptimize performance for a speciﬁc target task, rather than all tasks. Thus, source tasks in\ncurriculum learning are designed solely to improve performance on the target task—we are\nnot concerned with optimizing performance in a source.\nIn active learning, the learner chooses which task or example to learn or ask about next,\nfrom a given set of tasks. Typically, active learning has been examined in a semi-supervised\nlearning setting: a small amount of labeled data exists whereas a larger amount of unlabeled\ndata is present. The labeled data is used to learn a classiﬁer to infer labels for unlabeled\ndata. Unlabeled data that the classiﬁer is not conﬁdent about is requested for a label from\na human user. For example, Ruvolo and Eaton (2013b) consider active learning in a lifelong\nlearning setting, and show how a learner can actively select tasks to improve learning speed\nfor all tasks in a set, or for a speciﬁc target task. The selection of which task to be learned\nnext is similar to the sequencing aspect of curriculum learning. However, the full method\nof curriculum learning is much broader, as it also encompasses creating the space of tasks\nto consider. Ruvolo and Eaton (2013b) and similar active learning work typically assume\nthe set of tasks to learn and select from are already given. In addition, typically active\nlearning has been examined for supervised prediction tasks, whereas we are concerned with\nreinforcement learning tasks.\nFinally, in meta-learning (Finn et al., 2017), the goal is to train an agent on a variety of\ntasks such that it can quickly adapt to a new task within a small number of gradient descent\nsteps. Typically, the agent is not given information identifying the task it is training on.\nIn contrast, in curriculum learning, the learning agent may or may not have information\nidentifying the task. However, the process that designs the curriculum by sequencing tasks\nusually does have this information.\nLike in the lifelong setting, there is no signiﬁcance\nattached to the order in which tasks are presented to the learner. In addition, the objective\nin meta-learning is to train for fast adaptability, rather than for a speciﬁc ﬁnal task as is\nthe case in curriculum learning.\n5.2 Curricula in Supervised Machine Learning\nIn addition to reinforcement learning, curriculum learning has been examined for supervised\nlearning. While it is beyond the scope of this article to extensively survey supervised CL\nmethods, we would like to highlight a few that could inspire ideas and draw parallels to the\nRL setting.\nBengio et al. (2009) ﬁrst formalized the idea of curriculum learning in the context of\nsupervised machine learning. They conducted case studies examining when and why train-\ning with a curriculum can be beneﬁcial for machine learning algorithms, and hypothesized\nthat a curriculum serves as both a continuation method and a regularizer. A continuation\nmethod is an optimization method for non-convex criteria, where a smoothed version of the\nobjective is optimized ﬁrst, with the smoothing gradually reduced over training iterations.\nTypically, “easy” examples in a curriculum correspond to a smoother objective. Using a\nsimple shape recognition and language domain, they showed that training with a curriculum\ncan improve both learning speed and performance.\nWhile many papers before Bengio et al. (2009) used the idea of a curriculum to improve\ntraining of machine learning algorithms, most work considering how to systematically learn\na curriculum came after.\nOne recent example is work by Graves et al. (2017).\nThey\n34\nCurriculum Learning for Reinforcement Learning Domains\nintroduced measures of learning progress, which indicate how well the learner is currently\nimproving from the training examples it is being given. They introduce 2 main measures\nbased on 1) rate of increase in prediction accuracy and 2) rate of increase of network\ncomplexity. These serve as the reward to a non-stationary multi-armed bandit algorithm,\nwhich learns a stochastic policy for selecting tasks. These signals of learning progress could\nin theory be applied or adapted to the reinforcement learning setting as well. Graves et al.\n(2017) also make an interesting observation, which is that using a curriculum is similar to\nchanging the step size of the learning algorithm. Speciﬁcally, in their experiments, they\nfound that a random curriculum still serves as a strong baseline, because all tasks in the\nset provide a gradient3. Easier tasks provide a stronger gradient while harder tasks provide\na gradient closer to 0. Thus, choosing easy, useful tasks allows the algorithm to take larger\nsteps and converge faster.\nMore recently, Fan et al. (2018) frame curriculum learning as “Learning to Teach,”\nwhere a teacher agent learned to train a learning agent using a curriculum. The process is\nformulated as an MDP between these two interacting agents, similar to the MDP approaches\ndiscussed in Section 4.2.4: the teacher agent selects the training data, loss function, and\nhypothesis space, while the learning agent trains given the parameters speciﬁed by the\nteacher. The state space of the MDP is represented as a combination of features of the\ndata, features of the student model, and features that represent the combination of both\ndata and learner models. The reward signal is the accuracy on a held-out development set.\nTraining a teacher agent can be computationally expensive. They amortize this cost by using\na learned teacher agent to teach a new student with the same architecture. For example,\nthey train the teacher using the ﬁrst half of MNIST, and use the learned teacher to train a\nnew student from the second half of MNIST. Another way they amortize the cost is to train\na new student with a diﬀerent architecture (e.g., changing from ResNet32 to ResNet110).\nSimilar ideas have been explored in the reinforcement learning setting. However, the test\nset distribution is diﬀerent from the training set distribution, which makes performing these\nkind of evaluations more challenging. However, showing that the cost for training a teacher\ncan be amortized is an important direction for future work.\nFinally, Jiang et al. (2015) explore the idea of self-paced curriculum learning for su-\npervised learning, which uniﬁes and takes advantage of the beneﬁts of self-paced learning\nand curriculum learning. In their terminology, curriculum learning uses prior knowledge,\nbut does not adapt to the learner. Speciﬁcally, a curriculum is characterized by a ranking\nfunction, which orders a dataset of samples by priority. This function is usually derived by\npredetermined heuristics, and cannot be adjusted by feedback from the learner. In contrast,\nself-paced learning (SPL) adjusts to the learner, but does not incorporate prior knowledge\nand leads to overﬁtting. In SPL, the curriculum design is implicitly embedded as a reg-\nularization term into the learning objective. However, during learning, the training loss\nusually dominates over the regularization, leading to overﬁtting. This paper proposes a\nframework that uniﬁes these two ideas into a concise optimization problem, and discusses\nseveral concrete implementations. The idea is to replace the regularization term in SPL\nwith a self-paced function, such that the weights lie within a predetermined curriculum\nregion. In short, the curriculum region induces a weak ordering over the samples, and the\n3. Note however that in the reinforcement learning setting, because the policy aﬀects the distribution of\nstates an agent encounters, random training can be signiﬁcantly worse.\n35\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nself-paced function determines the actual learning scheme within that ordering. The idea\nhas parallels to a task-level curriculum for RL, where the curriculum induces a weak or-\ndering over samples from all tasks, and with the learning algorithm determining the actual\nscheme within that ordering.\n5.3 Algorithmically Designed Curricula in Education\nCurriculum learning has also been widely used for building eﬀective Intelligent Tutoring\nSystems (ITS) for human education (Iglesias et al., 2003, 2009; Green et al., 2011; Brunskill\nand Russell, 2011; Doroudi et al., 2016). An ITS system involves a student interacting\nwith an intelligent tutor (a computer-based system), with the goal of helping the student\nto master all skills quickly, using as little learning content as possible. Given that students\nhave diﬀerent learning needs, styles, and capabilities, the intelligent tutor should be able\nto provide customized instructions to them. To achieve this goal, one common strategy is\ncalled curriculum sequencing, which aims to provide the learning materials in a meaningful\norder that maximizes learning of the students with diﬀerent knowledge levels. The main\nproblem this strategy must solve is to ﬁnd the most eﬀective lesson to propose next, given\nthe student’s current learning needs and capabilities.\nReinforcement learning is one of the machine learning techniques that has been used\nwith intelligent tutors to partially automate construction of the student model and to au-\ntomatically compute an optimal teaching policy (Woolf, 2007). One advantage of using RL\nmethods in tutoring is that the model can learn adaptive teaching actions based on each\nindividual student’s performance in real time, without needing to encode complex pedagog-\nical rules that the system requires to teach eﬀectively (e.g., how to sequence the learning\ncontent, when and how to provide an exercise). Another advantage is that it is a general\ndomain-independent technique that can be applied in any ITS.\nAs a concrete example, Iglesias et al. (2003, 2009) adapt Q-learning (Watkins, 1989) to\nan adaptive and intelligent educational system to allow it to automatically learn how to\nteach each student. They formulate the learning problem as an RL problem, where the state\nis deﬁned as the description of the student’s knowledge, indicating whether the student has\nlearned each knowledge item. The set of actions the intelligent tutor can execute includes\nselecting and showing a knowledge item to the student. A positive reward is given when all\nrequired content has been learned, otherwise no reward is given. The system evaluates the\nstudent’s knowledge state through tests, which shows how much the student knows about\neach knowledge item. The Q-value estimates the usefulness of executing an action when\nthe student is in a particular knowledge state. Then, the tutoring problem can be solved\nusing the traditional Q-learning algorithm.\nGreen et al. (2011) propose using a multi-layered Dynamic Bayes Net (DBN) to model\nthe teaching problem in an ITS system. The main idea is to model the dynamics of a\nstudent’s skill acquisition using a DBN, which is normally used in RL to represent transition\nfunctions for state spaces. More speciﬁcally, they formulate the problem as a factored MDP,\nwhere the state consists of one factor for each skill, corresponding to the student’s proﬁciency\non that particular skill. The actions are to either provide a hint or to pose a problem about\na particular skill to the student. From a history of teacher-student interaction, the teacher\ncan model the student’s proﬁciency state, with the goal of teaching the student to achieve\n36\nCurriculum Learning for Reinforcement Learning Domains\nthe highest possible proﬁciency value on each skill, using as few problems and hints as\npossible. Subsequently, the learned DBN model is used by a planning algorithm to search\nfor the optimal teaching policy, mapping proﬁciency states of student knowledge to the\nmost eﬀective problem or hint to pose next.\nTo allow the automated teacher to select a sequence of pedagogical actions in cases\nwhere learner’s knowledge may be unobserved, a diﬀerent problem formulation is posed\nby Raﬀerty et al. (2016). They formulate teaching as a partially observable Markov decision\nprocess (POMDP), where the learner’s knowledge state is considered as a hidden state,\ncorresponding to the learner’s current understanding of the concept being taught.\nThe\nactions the automated teacher can select is a sequence of pedagogical choices, such as\nexamples or short quizzes. The learner’s next knowledge state is dependent on her current\nknowledge state and the pedagogical action the teacher chooses. Changes in the learner’s\nknowledge state reﬂect learning. In this framework, the automated teacher makes some\nassumptions about student learning, which is referred to as the learner model: it speciﬁes\nthe space of possible knowledge states and how the knowledge state changes. Then the\nteacher can update its beliefs about the learner’s current knowledge state based on new\nobservations, given this learner model. Using this POMDP framework, they explore how\ndiﬀerent learner models aﬀect the teacher’s selection of pedagogical actions.\nWhile most approaches seek to solely maximize overall learning gains, Ramachandran\nand Scassellati (2014) propose an RL-based approach that uses a personalized social robot\nto tutor children, that maximizes learning gains and sustained engagement over the student-\nrobot interaction. The main goal of the social robot is to learn the ordering of questions\npresented to a child, based on diﬃculty level and the child’s engagement level in real time. To\nrepresent the idea that children with diﬀerent knowledge levels need a diﬀerent curriculum,\neach child is categorized into a given group based on knowledge level at the start of the\none-on-one tutoring interaction. An optimal teaching policy is then learned speciﬁc to each\ngroup. In particular, their approach consists of a training phase and an interaction phase.\nIn the training phase, participants are asked to complete a tutoring exercise. A pretest\nand post-test will be used to evaluate the participant’s relative learning gains, which will\nalso be used as the reward function to learn an optimal policy during the training phase.\nSubsequently, in the interaction phase, the child’s real-time engagement will be detected,\nserving as another reward signal for the RL algorithm to further optimize the teaching\npolicy.\nNon-RL-based algorithms have been considered as well.\nBallera et al. (2014) leverage\nthe roulette wheel selection algorithm (RWSA) to perform personalized topic sequencing in\ne-learning systems. RWSA is typically used in genetic algorithms to arrange the chromo-\nsomes based on their ﬁtness function, such that individuals with higher ﬁtness value will\nhave higher probability of being selected (Goldberg, 1989). Similarly, in an e-learning sys-\ntem, a chromosome is denoted by a lesson. Each lesson has a ﬁtness value that dynamically\nchanges based on the student’s learning performance. This ﬁtness value indicates how well\nthe topic was learned by the student, depending on three performance parameters: exam\nperformance, study performance, and review performance of the learner. A lower ﬁtness\nvalue means that the student has a poorer understanding of the topic. Thus, a reversed\nmechanism of RWSA is implemented, so as to select the lessons with lower ﬁtness values\n37\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nmore often for reinforcement. Then, this reversed RWSA algorithm is combined with linear\nranking algorithm to sort the lessons.\n6. Open Questions\nThrough our survey of the literature, we have identiﬁed several open problems that have\nnot been suﬃciently studied in past work, and could be useful avenues for future research.\n6.1 Fully Automated Task Creation\nTask creation is an important piece of the method of curriculum learning. Whether tasks are\ncreated “on-demand” or all in advance, the quality of the pool of tasks generated directly\naﬀects the quality of curricula that can be produced. In addition, the quantity of tasks\nproduced aﬀect the search space and eﬃciency of curriculum sequencing algorithms. Despite\nthis, very limited work (see Section 4.1) has been done on the problem of automatically\ngenerating tasks.\nExisting work either assumes the pool of tasks are manually crafted\nand speciﬁed beforehand, or deﬁnes a set of rules for semi-automatically creating tasks.\nHowever, these rules often have hyper-parameters that control how many tasks are created,\nand are also usually manually tuned. Reducing the amount of manual input required by\nthese methods remains an important area for future work.\n6.2 Transferring Diﬀerent Types of Knowledge\nBetween each pair of tasks in a curriculum, knowledge must be transferred from one task\nto the subsequent task.\nIn virtually all of the works surveyed, the type of knowledge\ntransferred has been ﬁxed. For example, a value function was always transferred between\ntasks by Narvekar et al. (2017) while a shaping reward was always transferred by Svetlik\net al. (2017). However, this limitation opens the question of whether diﬀerent tasks could\nbeneﬁt from extracting diﬀerent types of knowledge.\nFor instance, it may be useful to\nextract an option from one task, and a model from another. Thus, in addition to deciding\nwhich task to transfer from, we could also ask what to extract and transfer from that task.\nPast transfer learning literature has shown that many forms of transfer are possible. The\nbest type of knowledge to extract may diﬀer based on task, and techniques will need to be\ndeveloped to eﬀectively combine these diﬀerent types of knowledge.\n6.3 Reusing Curricula and Sim-to-Real Curriculum Learning\nAnother limitation of many curriculum learning approaches is that the time to generate a\ncurriculum can be greater than the time to learn the target task outright. This shortcoming\nstems from the fact that curricula are typically learned independently for each agent and\ntarget task. However, in areas such as human education, curricula are used to train multiple\nstudents in multiple subjects. Thus, one way to amortize the cost would be to learn a\ncurriculum to train multiple diﬀerent agents, or to solve multiple diﬀerent target tasks\n(Narvekar and Stone, 2020).\nAnother option for amortizing the cost is to learn curricula for a sim-to-real setting\non physical robots, where a curriculum is learned in simulation and then used to train a\nphysical robot. While the exact weights of the policy learned in simulation would not apply\n38\nCurriculum Learning for Reinforcement Learning Domains\nin the real world, the semantics of the curriculum tasks might. Therefore, the physical robot\ncould go through the same training regimen, but learn using the physics and dynamics of\nthe real world.\n6.4 Combining Task Generation and Sequencing\nThe curriculum learning method can be thought of as consisting of 3 parts: task generation,\nsequencing, and transfer learning. For the most part, previous work has tackled each of\nthese pieces independently. For example, sequencing methods typically assume the tasks\nare prespeciﬁed, or a task generation method exists. However, an interesting question is\nwhether the task generation and task sequencing phases can be done simultaneously, by\ndirectly generating the next task in the curriculum. Some very preliminary work has been\ndone in this direction in the context of video game level generation. For example, Green\net al. (2019) used an evolutionary algorithm to generate maps for a gridworld, where each\ntile had a diﬀerent element. The generator was optimized to maximize the loss of deep RL\nagent’s network, inducing a training curriculum.\nCombining task generation and sequencing has additional challenges, such as specifying\nthe space of possible maps, ensuring those maps are valid/solvable, and creating maps that\nare challenging, but not too diﬃcult to solve. In addition, training the generator can be\nvery expensive. However, it promises an end-to-end solution that could reduce the amount\nof human intervention needed to design curricula.\n6.5 Theoretical Results\nThere have been many practical applications of curricula to speed up learning in both\nsupervised and reinforcement learning. However, despite empirical evidence that curricula\nare beneﬁcial, there is a lack of theoretical results analyzing when and why they are useful,\nand how they should be created. An initial analysis in the context of supervised learning was\ndone by Weinshall et al. (2018) and Weinshall and Amir (2018). They analyzed whether\nreordering samples in linear regression and binary classiﬁcation problems could improve\nthe ability to learn new concepts. They did this analysis by formalizing the idea of an\nIdeal Diﬃculty Score (IDS), which is the loss of the example with respect to the optimal\nhypothesis, and the Local Diﬃculty Score (LDS), which is the loss of the example with\nrespect to the current hypothesis. These are 2 ways to classify the diﬃculty of a sample,\nwhich can be used as a means to sequence samples. They showed that the convergence\nof an algorithm like stochastic gradient descent monotonically decreases with the IDS,\nand monotonically increases with the LDS. An open question is whether similar grounded\nmetrics for diﬃculty of tasks can be identiﬁed in reinforcement learning, and what kind of\nconvergence guarantees we can draw from them.\n6.6 Understanding General Principles for Curriculum Design\nDetermining the diﬃculty of a training example for an agent, and ensuring that each ex-\nample presented to the agent is suitable given its current ability, is a major challenge in\ncurriculum learning. In most existing work, the curriculum is generated either automat-\nically (see Section 4.2), by ordering samples from the target tasks or iteratively selecting\n39\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nintermediate tasks with increasing diﬃculty tailored to the current ability of the learner; or\nmanually by domain experts, who will typically have specialized knowledge of the problem\ndomain. Very limited work (see Section 4.2.5) has been done to better understand how\nnon-expert humans design curricula. The way we deﬁne curriculum design strategies still\nleaves a lot to be deﬁned by human teachers.\nCan non-expert humans design eﬀective curricula for a given ﬁnal task? What kind of\ncurriculum design strategies do they tend to follow when building curricula? If we could ﬁnd\nsome general principles non-expert humans follow for designing and/or sequencing more “in-\nteresting” intermediate tasks into a curriculum, we could incorporate these insights into the\nautomatic process of generating useful source tasks for any task domain. Furthermore, can\nwe adapt curriculum learning algorithms to better take advantage of this type of non-expert\nguidance to learn more eﬃciently? We believe a better understanding of the curriculum-\ndesign strategies used by non-expert humans may help us to 1) understand the general\nprinciples that make some curriculum strategies work better than others, and 2) inspire\nthe design of new machine-learning algorithms and interfaces that better accommodate the\nnatural tendencies of human trainers.\n7. Conclusion\nThis survey formalized the concept of a curriculum, and the method of curriculum learning\nin the context of reinforcement learning. Curriculum learning is a 3-part approach consisting\nof 1) task generation, 2) sequencing, and 3) transfer learning. We systematically surveyed\nexisting work addressing each of these parts, with a particular focus on sequencing methods.\nWe broke down sequencing methods into ﬁve categories, based on the assumptions they make\nabout intermediate tasks in the curriculum. The simplest of these are sample sequencing\nmethods, which reorder samples from the ﬁnal task itself, but do not explicitly change the\ndomain. These were followed by co-learning methods, where a curriculum emerges from the\ninteraction of several agents in the same environment. Next we considered methods that\nexplicitly changed the MDP to produce intermediate tasks. Some of these assumed that\nthe environment dynamics stay the same, but that the initial/terminal state distribution\nand reward function can change. Others made no restrictions on the diﬀerences allowed\nfrom the target task MDP. Finally, we also discussed how humans approach sequencing, to\nshed light on manually designed curricula in existing work. Our survey of the literature\nconcluded with a list of open problems, which we think will serve as worthwhile directions\nfor future work. As a budding area in reinforcement learning, we hope that this survey will\nprovide a common foundation and terminology to promote discussion and advancement in\nthis ﬁeld.\nAcknowledgments\nWe would like to sincerely thank Brad Knox, Garrett Warnell, and the anonymous reviewers\nfor helpful comments and suggestions that improved the presentation of many ideas in this\narticle. Part of this work has taken place in the Learning Agents Research Group (LARG)\nat the Artiﬁcial Intelligence Laboratory, The University of Texas at Austin. LARG re-\n40\nCurriculum Learning for Reinforcement Learning Domains\nsearch is supported in part by grants from the National Science Foundation (CPS-1739964,\nIIS-1724157, NRI-1925082), the Oﬃce of Naval Research (N00014-18-2243), Future of Life\nInstitute (RFP2-000), Army Research Oﬃce (W911NF-19-2-0333), DARPA, Lockheed Mar-\ntin, General Motors, and Bosch. The views and conclusions contained in this document are\nthose of the authors alone. Peter Stone serves as the Executive Director of Sony AI America\nand receives ﬁnancial compensation for this work. The terms of this arrangement have been\nreviewed and approved by the University of Texas at Austin in accordance with its policy on\nobjectivity in research. Part of this work has taken place in the Sensible Robots Research\nGroup at the University of Leeds, which is partially supported by the Engineering and\nPhysical Sciences Research Council of the UK (EP/R031193/1, EP/S005056/1), and the\nBritish Council. Part of this work has taken place in the Control, Robotics, Identiﬁcation\nand Signal Processing (CRISP) Laboratory at Tufts University which is partially supported\nby DARPA (W911NF-19-2-0006), the Verizon Foundation, PTC Inc., and the Center for\nApplied Brain and Cognitive Sciences (CABCS). Part of this work has taken place in the\nWhiteson Research Lab at the University of Oxford, which is partially supported by the\nEuropean Research Council (ERC), under the European Union’s Horizon 2020 research\nand innovation programme (grant agreement number 637713). Part of this work has taken\nplace in the Intelligent Robot Learning (IRL) Lab at the University of Alberta, which is\nsupported in part by research grants from the Alberta Machine Intelligence Institute.\n41\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nReferences\nHaitham Bou Ammar, Eric Eaton, Paul Ruvolo, and Matthew E Taylor. Online multi-task\nlearning for policy gradient methods. In International Conference on Machine Learning\n(ICML), pages 1206–1214, 2014.\nHaitham Bou Ammar, Eric Eaton, Jos´e Marcio Luna, and Paul Ruvolo.\nAutonomous\ncross-domain knowledge transfer in lifelong policy gradient reinforcement learning. In\nInternational Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 3345–3351, 2015.\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welin-\nder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight ex-\nperience replay. In Advances in Neural Information Processing Systems (NIPS), pages\n5048–5058, 2017.\nMinoru Asada, Shoichi Noda, Sukoya Tawaratsumida, and Koh Hosoda. Purposive behavior\nacquisition for a real robot by vision-based reinforcement learning. Machine Learning, 23\n(2-3):279–303, 1996.\nBowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew,\nand Igor Mordatch. Emergent tool use from multi-agent autocurricula. In International\nConference on Learning Representations (ICLR), 2020.\nMelvin Ballera, Ismail Ateya Lukandu, and Abdalla Radwan.\nPersonalizing e-learning\ncurriculum using reversed roulette wheel selection algorithm. In International Conference\non Education Technologies and Computers (ICETC), pages 91–97. IEEE, 2014.\nTrapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emer-\ngent complexity via multi-agent competition. In International Conference on Learning\nRepresentations (ICLR), 2018.\nAdrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrin-\nsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):\n49–73, 2013.\nAndrea Bassich, Francesco Foglino, Matteo Leonetti, and Daniel Kudenko.\nCurriculum\nlearning with a progression function. https://arxiv.org/abs/2008.00511, 2020.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\nYoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learn-\ning. In International Conference on Machine Learning (ICML), pages 41–48, 2009.\nEmma Brunskill and Stuart Russell. Partially observable sequential decision making for\nproblem selection in an intelligent tutoring system. In Poster at International Conference\non Educational Data Mining (EDM). Citeseer, 2011.\nRich Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997.\n42\nCurriculum Learning for Reinforcement Learning Domains\nAlexander Clegg, Wenhao Yu, Zackory Erickson, Jie Tan, C Karen Liu, and Greg Turk.\nLearning to navigate cloth using haptics.\nIn International Conference on Intelligent\nRobots and Systems (IROS), pages 2799–2805, 2017.\nFelipe Leno Da Silva and Anna Reali Costa. Object-oriented curriculum generation for\nreinforcement learning. In International Conference on Autonomous Agents & Multiagent\nSystems (AAMAS), 2018.\nMarco Dorigo, Vittorio Maniezzo, and Alberto Colorni. The ant system: An autocatalytic\noptimizing process. Technical Report, 1991.\nShayan Doroudi, Kenneth Holstein, Vincent Aleven, and Emma Brunskill. Sequence mat-\nters but how exactly? a method for evaluating activity sequences from data. Grantee\nSubmission, 2016.\nJeﬀrey L Elman. Learning and development in neural networks: The importance of starting\nsmall. Cognition, 48(1):71–99, 1993.\nAnestis Fachantidis, Ioannis Partalas, Grigorios Tsoumakas, and Ioannis Vlahavas. Trans-\nferring task models in reinforcement learning agents. Neurocomputing, 107:23–32, 2013.\nYang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. In\nInternational Conference on Learning Representations (ICLR), 2018.\nMeng Fang, Tianyi Zhou, Yali Du, Lei Han, and Zhengyou Zhang. Curriculum-guided hind-\nsight experience replay. In Advances in Neural Information Processing Systems (NIPS),\npages 12602–12613, 2019.\nFernando Fern´andez, Javier Garc´ıa, and Manuela Veloso.\nProbabilistic policy reuse for\ninter-task transfer learning. Robotics and Autonomous Systems, 58(7):866–871, 2010.\nChelsea Finn, Pieter Abbeel, and Sergey Levine.\nModel-agnostic meta-learning for fast\nadaptation of deep networks. In International Conference on Machine Learning (ICML),\npages 1126–1135. JMLR. org, 2017.\nCarlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Re-\nverse curriculum generation for reinforcement learning. In Conference on Robot Learning\n(CoRL), 2017.\nCarlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation\nfor reinforcement learning agents.\nIn International Conference on Machine Learning\n(ICML), pages 1514–1523, 2018.\nFrancesco Foglino, Christiano Coletto Christakou, and Matteo Leonetti. An optimization\nframework for task sequencing in curriculum learning. In International Conference on\nDevelopmental Learning (ICDL-EPIROB), 2019a.\nFrancesco Foglino, Christiano Coletto Christakou, Ricardo Luna Gutierrez, and Matteo\nLeonetti.\nCurriculum learning for cumulative return maximization.\nIn International\nJoint Conference on Artiﬁcial Intelligence (IJCAI), 2019b.\n43\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nFrancesco Foglino, Matteo Leonetti, Simone Sagratella, and Ruggiero Seccia. A gray-box\napproach for curriculum learning. In World Congress on Global Optimization, 2019c.\nTeruo Fujii, Yoshikazu Arai, Hajime Asama, and Isao Endo. Multilayered reinforcement\nlearning for complicated collision avoidance problems. In International Conference on\nRobotics and Automation (ICRA), volume 3, pages 2186–2191. IEEE, 1998.\nFred Glover and Manuel Laguna. Tabu search. In Handbook of combinatorial optimization,\npages 2093–2229. Springer, 1998.\nDavid E Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning.\nAddison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1st edition, 1989.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems (NIPS), pages 2672–2680, 2014.\nAlex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu.\nAutomated curriculum learning for neural networks.\nIn International Conference on\nMachine Learning (ICML), 2017.\nDerek T Green, Thomas J Walsh, Paul R Cohen, and Yu-Han Chang. Learning a skill-\nteaching curriculum with dynamic Bayes nets. In Innovative Applications of Artiﬁcial\nIntelligence (IAAI), 2011.\nMichael Cerny Green, Benjamin Sergent, Pushyami Shandilya, and Vibhor Kumar.\nEvolutionarily-curated curriculum learning for deep reinforcement learning agents. In\nAAAI Reinforcement Learning in Games Workshop, 2019.\nShane Griﬃth, Kaushik Subramanian, Jonathan Scholz, Charles Isbell, and Andrea L\nThomaz. Policy shaping: Integrating human feedback with reinforcement learning. In\nAdvances in Neural Information Processing Systems (NIPS), pages 2625–2633, 2013.\nJosiah Hanna, Philip Thomas, Peter Stone, and Scott Niekum. Data-eﬃcient policy evalu-\nation through behavior policy search. In International Conference on Machine Learning\n(ICML), August 2017.\nIonel-Alexandru Hosu and Traian Rebedea. Playing Atari games with deep reinforcement\nlearning and human checkpoint replay. In Workshop on Evaluating General-Purpose AI\n(EGPAI), 2016.\nAna Iglesias, Paloma Mart´ınez, and Fernando Fern´andez. An experience applying reinforce-\nment learning in a web-based adaptive and intelligent educational system. Informatics\nin Education, 2:223–240, 2003.\nAna Iglesias, Paloma Mart´ınez, Ricardo Aler, and Fernando Fern´andez. Learning teach-\ning strategies in an adaptive and intelligent educational system through reinforcement\nlearning. Applied Intelligence, 31(1):89–106, 2009.\n44\nCurriculum Learning for Reinforcement Learning Domains\nBoris Ivanovic, James Harrison, Apoorva Sharma, Mo Chen, and Marco Pavone. Barc:\nBackward reachability curriculum for robotic reinforcement learning. In International\nConference on Robotics and Automation (ICRA), pages 15–21. IEEE, 2019.\nMax Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo,\nDavid Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxil-\niary tasks. In International Conference on Learning Representations (ICLR), 2017.\nVikas Jain and Theja Tulabandhula. Faster reinforcement learning using active simulators.\nIn NIPS Workshop on Teaching Machines, Robots, and Humans, 2017.\nLu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G Hauptmann. Self-\npaced curriculum learning. In Association for the Advancement of Artiﬁcial Intelligence\n(AAAI), 2015.\nAndrej Karpathy and Michiel Van De Panne. Curriculum learning for motor skills. In\nCanadian Conference on Artiﬁcial Intelligence, pages 325–330. Springer, 2012.\nFaisal Khan, Bilge Mutlu, and Xiaojin Zhu. How do humans teach: On curriculum learning\nand teaching dimension. In Advances in Neural Information Processing Systems (NIPS),\npages 1449–1457, 2011.\nTae-Hoon Kim and Jonghyun Choi. Screenernet: Learning self-paced curriculum for deep\nneural networks. arXiv preprint arXiv:1801.00904, 2018.\nW Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement:\nThe TAMER framework. In International Conference on Knowledge Capture, 2009.\nW Bradley Knox and Peter Stone. Reinforcement learning from simultaneous human and\nMDP reward. In International Conference on Autonomous Agents and Multiagent Sys-\ntems (AAMAS), pages 475–482, 2012.\nAlessandro Lazaric.\nTransfer in reinforcement learning: a framework and a survey.\nIn\nReinforcement Learning, pages 143–173. Springer, 2012.\nAlessandro Lazaric and Marcello Restelli. Transfer from multiple MDPs. In Advances in\nNeural Information Processing Systems (NIPS), 2011.\nAlessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Transfer of samples in batch\nreinforcement learning. In International Conference on Machine Learning (ICML), pages\n544–551, 2008.\nSu Young Lee, Choi Sungik, and Sae-Young Chung. Sample-eﬃcient deep reinforcement\nlearning via episodic backward update. In Advances in Neural Information Processing\nSystems (NeurIPS), pages 2110–2119, 2019.\nRobert Loftin, Bei Peng, James MacGlashan, Michael L Littman, Matthew E Taylor, Jeﬀ\nHuang, and David L Roberts. Learning behaviors via human-delivered discrete feedback:\nmodeling implicit feedback strategies to speed up learning.\nAutonomous Agents and\nMulti-Agent Systems, 30(1):30–59, 2016.\n45\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nPatrick MacAlpine and Peter Stone. Overlapping layered learning. Artiﬁcial Intelligence,\n254:21–43, 2018.\nJames MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, Guan Wang, David L Roberts,\nMatthew E Taylor, and Michael L Littman. Interactive learning from policy-dependent\nhuman feedback. In International Conferences on Machine Learning (ICML), 2017.\nTambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student cur-\nriculum learning. IEEE Transactions on Neural Networks and Learning Systems, 2017.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.\nSanmit Narvekar and Peter Stone. Learning curriculum policies for reinforcement learning.\nIn International Conference on Autonomous Agents and Multiagent Systems (AAMAS),\nMay 2019.\nSanmit Narvekar and Peter Stone. Generalizing curricula for reinforcement learning. In\nLifelong Learning Workshop at ICML, 2020.\nSanmit Narvekar, Jivko Sinapov, Matteo Leonetti, and Peter Stone. Source task creation for\ncurriculum learning. In International Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), Singapore, 2016.\nSanmit Narvekar, Jivko Sinapov, and Peter Stone. Autonomous task sequencing for cus-\ntomized curriculum design in reinforcement learning. In International Joint Conference\non Artiﬁcial Intelligence (IJCAI), volume 147, page 149, 2017.\nPeng Si Ow and Thomas E Morton. Filtered beam search in scheduling. The International\nJournal Of Production Research, 26(1):35–62, 1988.\nBei Peng, James MacGlashan, Robert Loftin, Michael L Littman, David L Roberts, and\nMatthew E Taylor. Curriculum design for machine learners in sequential decision tasks.\nIEEE Transactions on Emerging Topics in Computational Intelligence, 2(4):268–277,\n2018.\nGail B Peterson. A day of great illumination: B. F. Skinner’s discovery of shaping. Journal\nof the Experimental Analysis of Behavior, 82(3):317–328, 2004.\nLerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial\nreinforcement learning. In International Conference on Machine Learning (ICML), pages\n2817–2826, 2017.\nSebastien Racaniere, Andrew Lampinen, Adam Santoro, David Reichert, Vlad Firoiu, and\nTimothy Lillicrap. Automated curriculum generation through setter-solver interactions.\nIn International Conference on Learning Representations (ICLR), 2019.\nAnna N Raﬀerty, Emma Brunskill, Thomas L Griﬃths, and Patrick Shafto. Faster teaching\nvia pomdp planning. Cognitive Science, 40(6):1290–1332, 2016.\n46\nCurriculum Learning for Reinforcement Learning Domains\nAditi Ramachandran and Brian Scassellati. Adapting diﬃculty levels in personalized robot-\nchild tutoring interactions. In Workshop at the AAAI Conference on Artiﬁcial Intelli-\ngence, 2014.\nZhipeng Ren, Daoyi Dong, Huaxiong Li, and Chunlin Chen. Self-paced prioritized curricu-\nlum learning with coverage penalty in deep reinforcement learning. IEEE Transactions\non Neural Networks and Learning Systems, 29(6):2216–2226, 2018.\nMartin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom\nvan de Wiele, Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by\nplaying solving sparse reward tasks from scratch. In International Conference on Machine\nLearning (ICML), pages 4344–4353, 2018.\nMark B Ring. Child: A ﬁrst step towards continual learning. Machine Learning, 28(1):\n77–104, 1997.\nDouglas LT Rohde and David C Plaut. Language acquisition in the absence of explicit\nnegative evidence: How important is starting small? Cognition, 72(1):67–109, 1999.\nChristopher D Rosin and Richard K Belew.\nNew methods for competitive coevolution.\nEvolutionary computation, 5(1):1–29, 1997.\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,\nKoray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks.\narXiv preprint arXiv:1606.04671, 2016.\nPaul Ruvolo and Eric Eaton. ELLA: An eﬃcient lifelong learning algorithm. In International\nConference on Machine Learning (ICML), 2013a.\nPaul Ruvolo and Eric Eaton. Active task selection for lifelong machine learning. In Asso-\nciation for the Advancement of Artiﬁcial Intelligence (AAAI), 2013b.\nTerence D Sanger. Neural network learning control of robot manipulators using gradually\nincreasing task diﬃculty. IEEE Transactions on Robotics and Automation, 10(3):323–333,\n1994.\nStefan Schaal. Learning from demonstration. In Advances in Neural Information Processing\nSystems (NIPS), pages 1040–1046, 1997.\nTom Schaul, Daniel Horgan, Karol Gregor, and David Silver.\nUniversal value function\napproximators. In International Conference on Machine Learning (ICML), 2015.\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver.\nPrioritized experience\nreplay. In International Conference on Learning Representations (ICLR), 2016.\nJ¨urgen Schmidhuber. Powerplay: Training an increasingly general problem solver by con-\ntinually searching for the simplest still unsolvable problem. Frontiers in Psychology, 4:\n313, 2013.\n47\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nKun Shao, Yuanheng Zhu, and Dongbin Zhao. Starcraft micromanagement with reinforce-\nment learning and curriculum transfer learning. IEEE Transactions on Emerging Topics\nin Computational Intelligence, 2018.\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried-\nmiller. Deterministic policy gradient algorithms. In International Conference on Machine\nLearning (ICML), 2014.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van\nDen Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc\nLanctot, et al. Mastering the game of go with deep neural networks and tree search.\nNature, 529(7587):484, 2016.\nJivko Sinapov, Sanmit Narvekar, Matteo Leonetti, and Peter Stone. Learning inter-task\ntransferability in the absence of target task samples.\nIn International Conference on\nAutonomous Agents and Multiagent Systems (AAMAS), pages 725–733, 2015.\nBurrhus F Skinner. Reinforcement today. American Psychologist, 13(3):94, 1958.\nVishal Soni and Satinder Singh. Using homomorphisms to transfer options across continu-\nous reinforcement learning domains. In American Association for Artiﬁcial Intelligence\n(AAAI), 2006.\nRupesh Kumar Srivastava, Bas R. Steunebrink, and Jrgen Schmidhuber. First experiments\nwith powerplay. Neural Networks, 41:130 – 136, 2013. Special Issue on Autonomous\nLearning.\nKenneth O Stanley, Bobby D Bryant, and Risto Miikkulainen. Evolving neural network\nagents in the nero video game. In IEEE Symposium on Computational Intelligence and\nGames (CIG), Piscataway, NJ, 2005.\nPeter Stone and Manuela Veloso. Learning to solve complex planning problems: Finding\nuseful auxiliary problems. In AAAI Fall Symposium on Planning and Learning, pages\n137–141, 1994.\nHalit Bener Suay and Sonia Chernova. Eﬀect of human guidance and state space size on\ninteractive reinforcement learning.\nIn International Conference on Robot and Human\nInteractive Communication (RO-MAN), pages 1–6, 2011.\nKaushik Subramanian, Charles L Isbell Jr, and Andrea L Thomaz.\nExploration from\ndemonstration for interactive reinforcement learning.\nIn International Conference on\nAutonomous Agents and Multiagent Systems (AAMAS), pages 447–456, 2016.\nSainbayar Sukhbaatar, Zeming Li, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and\nRob Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. In\nInternational Conference on Learning Representations (ICLR), 2018.\nRichard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press,\n1998.\n48\nCurriculum Learning for Reinforcement Learning Domains\nMaxwell Svetlik, Matteo Leonetti, Jivko Sinapov, Rishi Shah, Nick Walker, and Peter Stone.\nAutomatic curriculum graph generation for reinforcement learning agents. In Association\nfor the Advancement of Artiﬁcial Intelligence (AAAI), pages 2590–2596, 2017.\nMatthew E Taylor.\nAssisting transfer-enabled machine learning algorithms: Leveraging\nhuman knowledge for curriculum design. In The AAAI Spring Symposium on Agents\nthat Learn from Human Teachers, 2009.\nMatthew E Taylor and Peter Stone. Behavior transfer for value-function-based reinforce-\nment learning. In Frank Dignum, Virginia Dignum, Sven Koenig, Sarit Kraus, Munin-\ndar P. Singh, and Michael Wooldridge, editors, International Joint Conference on Au-\ntonomous Agents and Multiagent Systems (AAMAS), pages 53–59, New York, NY, 2005.\nACM Press.\nMatthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains:\nA survey. Journal of Machine Learning Research, 10(1):1633–1685, 2009.\nMatthew E Taylor, Peter Stone, and Yaxin Liu. Transfer learning via inter-task mappings\nfor temporal diﬀerence learning. Journal of Machine Learning Research, 8(1):2125–2167,\n2007.\nMatthew E Taylor, Gregory Kuhlmann, and Peter Stone. Autonomous transfer for reinforce-\nment learning. In International Joint Conference on Autonomous Agents and Multiagent\nSystems (AAMAS), 2008.\nGerald Tesauro.\nTemporal diﬀerence learning and td-gammon. Communications of the\nACM, 38(3):58–68, 1995.\nChen Tessler, Shahar Givony, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. A deep\nhierarchical approach to lifelong learning in minecraft. In Association for the Advance-\nment of Artiﬁcial Intelligence (AAAI), pages 1553–1561, 2017.\nAndrea Lockerd Thomaz and Cynthia Breazeal. Reinforcement learning with human teach-\ners: Evidence of feedback and guidance with implications for learning performance. In\nAssociation for the Advancement of Artiﬁcial Intelligence (AAAI), volume 6, pages 1000–\n1005, 2006.\nSebastian Thrun.\nLifelong learning algorithms.\nIn Sebastian Thrun and Lorien Pratt,\neditors, Learning to Learn, pages 181–209. Kluwer Academic Publishers, Norwell, MA,\nUSA, 1998.\nAlexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol Vinyals, John\nAgapiou, et al. Strategic attentive writer for learning macro-actions. In Advances in\nNeural Information Processing Systems (NIPS), pages 3486–3494, 2016.\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik,\nJunyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al.\nGrandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, pages\n1–5, 2019.\n49\nNarvekar, Peng, Leonetti, Sinapov, Taylor, and Stone\nLev Semenovich Vygotsky. Mind in Society: Development of Higher Psychological Processes.\nHarvard University Press, 1978.\nWeixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng\nChen, Changjie Fan, and Yang Gao. From few to more: Large-scale dynamic multia-\ngent curriculum learning. In Association for the Advancement of Artiﬁcial Intelligence\n(AAAI), pages 7293–7300, 2020.\nChristopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292,\n1992.\nChristopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis,\nKing’s College, Cambridge, 1989.\nDaphna Weinshall and Dan Amir. Theory of curriculum learning, with convex loss functions.\narXiv preprint arXiv:1812.03472, 2018.\nDaphna Weinshall, Gad Cohen, and Dan Amir. Curriculum learning by transfer learning:\nTheory and experiments with deep networks. In International Conference on Machine\nLearning (ICML), pages 5235–5243, 2018.\nAaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement\nlearning: a hierarchical bayesian approach.\nIn International Conference on Machine\nLearning (ICML), pages 1015–1022. ACM, 2007.\nBeverly Park Woolf. Building Intelligent Interactive Tutors: Student-centered Strategies for\nRevolutionizing e-Learning. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA,\n2007.\nYuxin Wu and Yuandong Tian. Training agent for ﬁrst-person shooter game with actor-critic\ncurriculum learning. In International Conference on Learning Representations (ICLR),\n2017.\nBoo-Ho Yang and Haruhiko Asada.\nProgressive learning and its application to robot\nimpedance learning. IEEE Transactions on Neural Networks, 7(4):941–952, 1996.\nJiachen Yang, Alireza Nakhaei, David Isele, Kikuo Fujimura, and Hongyuan Zha. Cm3:\nCooperative multi-goal multi-stage multi-agent reinforcement learning. In International\nConference on Learning Representations (ICLR), 2020.\nMatthieu Zimmer, Yann Boniface, and Alain Dutech. Developmental reinforcement learning\nthrough sensorimotor space enlargement. In International Conference on Development\nand Learning and Epigenetic Robotics (ICDL-EpiRob), pages 33–38. IEEE, 2018.\n50\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2020-03-10",
  "updated": "2020-09-17"
}