{
  "id": "http://arxiv.org/abs/2003.05012v4",
  "title": "Retrospective Analysis of the 2019 MineRL Competition on Sample Efficient Reinforcement Learning",
  "authors": [
    "Stephanie Milani",
    "Nicholay Topin",
    "Brandon Houghton",
    "William H. Guss",
    "Sharada P. Mohanty",
    "Keisuke Nakata",
    "Oriol Vinyals",
    "Noboru Sean Kuno"
  ],
  "abstract": "To facilitate research in the direction of sample efficient reinforcement\nlearning, we held the MineRL Competition on Sample Efficient Reinforcement\nLearning Using Human Priors at the Thirty-third Conference on Neural\nInformation Processing Systems (NeurIPS 2019). The primary goal of this\ncompetition was to promote the development of algorithms that use human\ndemonstrations alongside reinforcement learning to reduce the number of samples\nneeded to solve complex, hierarchical, and sparse environments. We describe the\ncompetition, outlining the primary challenge, the competition design, and the\nresources that we provided to the participants. We provide an overview of the\ntop solutions, each of which use deep reinforcement learning and/or imitation\nlearning. We also discuss the impact of our organizational decisions on the\ncompetition and future directions for improvement.",
  "text": "Proceedings of Machine Learning Research 1:1–12, 2020\nNeurIPS2019 Competition & Demonstration Track\nRetrospective Analysis of the 2019 MineRL Competition on\nSample Eﬃcient Reinforcement Learning\nStephanie Milani\nsmilani@cs.cmu.edu\nMachine Learning Department, Carnegie Mellon University\nNicholay Topin\nntopin@cs.cmu.edu\nMachine Learning Department, Carnegie Mellon University\nBrandon Houghton∗\nbhoughton@openai.com\nOpenAI\nWilliam H. Guss\nwguss@cs.cmu.edu\nOpenAI and Machine Learning Department, Carnegie Mellon University\nSharada P. Mohanty\nmohanty@aicrowd.com\nAIcrowd\nKeisuke Nakata\nnakata@preferred.jp\nPreferred Networks\nOriol Vinyals\nvinyals@google.com\nDeepMind\nNoboru Sean Kuno\nnkuno@microsoft.com\nMicrosoft Research\nEditors: Hugo Jair Escalante and Raia Hadsell\nAbstract\nTo facilitate research in the direction of sample eﬃcient reinforcement learning, we held the\nMineRL Competition on Sample Eﬃcient Reinforcement Learning Using Human Priors at\nthe Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019).\nThe primary goal of this competition was to promote the development of algorithms that use\nhuman demonstrations alongside reinforcement learning to reduce the number of samples\nneeded to solve complex, hierarchical, and sparse environments. We describe the compe-\ntition, outlining the primary challenge, the competition design, and the resources that we\nprovided to the participants. We provide an overview of the top solutions, each of which\nuse deep reinforcement learning and/or imitation learning. We also discuss the impact of\nour organizational decisions on the competition and future directions for improvement.\nKeywords: reinforcement learning competition, reinforcement learning, imitation learning\n∗Work done as a research assistant at Carnegie Mellon University.\nc⃝2020 S. Milani, N. Topin, B. Houghton, W.H. Guss, S.P. Mohanty, K. Nakata, O. Vinyals & N.S. Kuno.\narXiv:2003.05012v4  [cs.LG]  18 Jun 2020\nRetrospective Analysis of the 2019 MineRL Competition\n1. Introduction\nMany of the recent, celebrated successes of artiﬁcial intelligence (AI), such as AlphaS-\ntar (Vinyals et al., 2019), AlphaZero (Silver et al., 2018), and OpenAI Five1 achieve human-\nor super-human-level performance using deep reinforcement learning (DRL). Thus far, im-\nprovements to the state of the art have used ever-increasing computational power2. In part,\nthis computational increase is due to the computation required per environment sample;\nhowever, it is primarily due to the increasing number of environment samples required for\ntraining these learning algorithms. These growing computational requirements mean that a\nshrinking portion of the AI community can reproduce these results, let alone improve upon\nthese systems.\nOne well-known way to reduce environment-sample complexity, and, by\nextension, make methods more accessible, is to leverage human demonstrations and priors\nover desired behavior (Pfeiﬀer et al., 2018; Dubey et al., 2018; Zhang et al., 2019).\nTo further the development of novel, sample eﬃcient methods that leverage human pri-\nors for sequential decision-making problems, we held the ﬁrst-ever MineRL Competition on\nSample Eﬃcient Reinforcement Learning Using Human Priors3 at the Thirty-third Confer-\nence on Neural Information Processing Systems (NeurIPS 2019) (Guss et al., 2019). Teams\ndeveloped procedures for training an agent to obtain a diamond in Minecraft. To limit com-\nputational requirements, the learning procedures were evaluated by retraining them from\nrandom weights under a strict computational and environment-sample budget. To assist\nparticipants with developing their algorithms, teams were provided with the largest-ever\ndata set of human trajectories in Minecraft (Guss* et al., 2019). Our challenge attracted\nover 1000 registrated participants with a total of 662 submissions4.\nIn this work, we describe the competition paradigm and procedure, and provide a sum-\nmary of the top nine solutions.\nWe identify common high-level approaches — such as\nleveraging human demonstrations and using hierarchical reinforcement learning — used in\nmany of the top solutions. We conclude by discussing the outcomes of our choices as or-\nganizers of the competition, including how we evaluated the submissions and speciﬁed the\nrules.\n2. Background\nMinecraft.\nMinecraft is a 3D, ﬁrst-person, open-world game revolving around gathering\nresources and creating structures and items. Because it is a sandbox environment, designers\ncan devise a variety of goals and challenges for intelligent agents. Additionally, because it\nis an embodied domain and the agent’s surroundings are varied and dynamic, it presents\nmany of the same challenges found in real-world control tasks, such as planning over long\ntime horizons and determining a good representation of the environment (Alterovitz et al.,\n2016). As a result, it is a promising and popular testbed for both single- and multi-agent\nreinforcement learning (RL) and planning algorithms (Abel et al., 2015; Aluru et al., 2015;\nAbel et al., 2016; Oh et al., 2016; Tessler et al., 2017; Shu et al., 2018; Frazier and Riedl,\n2019; Master et al., 2019; Arumugam et al., 2019; Trott et al., 2019).\n1. https://openai.com/five\n2. https://blog.openai.com/ai-and-compute\n3. http://minerl.io\n4. https://www.aicrowd.com/challenges/neurips-2019-minerl-competition\n2\nRetrospective Analysis of the 2019 MineRL Competition\n(a) Original texture\n(b) Validation texture\n(c) Test texture\nFigure 1: An example game state rendered with three diﬀerent textures.\nReinforcement Learning Competitions,\nTo our knowledge, no previous competitions\nexplicitly focused on using imitation learning (and, more generally, learning from demon-\nstrations) alongside RL. Most previous RL competitions (Kidziski et al., 2018; Nichol et al.,\n2018; Perez-Liebana et al., 2018; Juliani et al., 2019) focused on performing well on a given\ndomain, and not on developing robust algorithms that can perform well on a broad set of\ndomains. Consequently, winning submissions often required hand-engineered features and\nstemmed from using large amounts of computational resources for training and development.\n3. Competition Overview\nWe provide an overview of our competition. We describe the primary task and environment\nin Section 3.1. In Section 3.2, we describe the structure of our competition. In Section 3.3,\nwe describe the resources that we provided participants.\n3.1. Task\nCompetitors were tasked with solving the ObtainDiamond environment. Solving the envi-\nronment consists of controlling an embodied agent to obtain a diamond by navigating the\ncomplex item hierarchy of Minecraft. In solving this task, a learning algorithm has direct\naccess to a 64x64 pixel observation from the perspective of the embodied Minecraft agent,\nand a set of discrete observations consisting of each item required for obtaining a diamond\nthat the agent has in its possession. The action space is the Cartesian product of con-\ntinuous view adjustment (turning and pitching), binary movement commands (left/right,\nforward/backward), and discrete actions for placing blocks, crafting items, smelting items,\nand mining/hitting enemies.\nAn agent receives reward for completing the full task of obtaining a diamond. The full\ntask of obtaining a diamond can be decomposed into a sequence of prerequisite subtasks of\nincreasing diﬃculty. An agent also receives reward for the ﬁrst time it accomplishes each\nsubtask in the sequence. An agent receives twice the reward as received for accomplishing\nthe previous subtask (starting from a reward of 1). The exception to this rule is achieving\nthe full ObtainDiamond task by obtaining a diamond: accomplishing the ﬁnal task is worth\nfour times as much as completing the previous subtask.\n3\nRetrospective Analysis of the 2019 MineRL Competition\n3.2. Competition Design\nThe competition was split into two rounds.\nIn Round 1, teams of up to six total par-\nticipants trained their agents and submitted trained models for evaluation to determine\ntheir leaderboard ranks. In this round, we limited each team to 25 total submissions. At\nthe end of Round 1, the teams submitted their source code. Each submission received a\nscore consisting of the average reward achieved by that submission over 100 episodes. The\ntop submission for each team was then reviewed by the rules committee to verify that they\ncomplied with the rules. The 13 teams with the highest-scoring, rule-compliant submissions\nadvanced to Round 2. We invited these teams to present their work at the NeurIPS 2019\ncompetition workshop. With the support of Microsoft Research, we provided each team\nwith a $1200 USD travel grant to attend the conference.\nIn Round 2, each team could submit up to ﬁve learning procedures to be trained from\nrandom weights on a held-out test environment. We independently trained each submission\nfor six days using Azure NC6 and NC12 instances with a strict environment step budget of 8\nmillion samples. To encourage generalizability and compliance with rules, we retrained the\nsolutions submitted in the ﬁnal round using: 1) a perturbed action space, and 2) an entirely\nnew, previously-unseen texture pack5 (illustrated in Figure 1(c)). We provided teams with\na validation texture pack6 (illustrated in Figure 1(b)) to use during training so that they\ncould check whether their appeoaches could generalize to previously-unseen environments\nbefore submitting their learning procedures. The ﬁnal ranking was determined based on\neach team’s highest-scoring submission.\nGiven the unique paradigm of training learning procedures, we set strict rules to encour-\nage sample eﬃciency and generalizability, and prevent participants from loading pretrained\nmodels. Sample eﬃciency constraints were enforced by setting a strict environment sample\nlimit of 8 million frames (or roughly 110 hours of in-game interactions), as well as limiting\ncomputation to 144 hours of wall clock time. We permitted frame skipping; however, each\nframe skipped counted against the sample budget. Participants were also free to shape the\nobservations of the agent, but they could not directly encode them in the policy. For ex-\nample, switching sub-policies based on the number of logs collected was not permitted. To\navoid participants including pretrained models in their submissions, in Round 2, any ﬁles in\nthe submitted code repositories that exceeded 4MB were programmatically deleted. A more\ndetailed discussion of the competition paradigm can be found in previous work (Houghton\net al., 2019).\nIn both rounds, participants submitted their code as standalone repositories compati-\nble with AIcrowd-repo2docker7. These repositories included the code, trained models, and\nruntime speciﬁcations, which enabled us to build consistent Docker images out of the sub-\nmissions. These images were orchestrated on a custom Kubernetes cluster while respecting\nany round-speciﬁc policies. We evaluated the submissions in complete network isolation to\navoid any leak of information due to malicious code submitted by participants.\n5. The test texture pack we used can be found here.\n6. We used a modiﬁed version of this texture pack as our validation texture pack.\n7. https://github.com/AIcrowd/repo2docker\n4\nRetrospective Analysis of the 2019 MineRL Competition\nTeam Name\nRound 1 Score\nMeatMountain\n48.42\nxt\n46.75\nshadowyzy\n37.82\nCraftRL\n33.15\nCDS\n29.62\nmc rl\n27.43\nI4DS\n23.96\nUEFDRL\n21.70\nTD240\n17.12\nTeam Name\nRound 2 Score\nCDS\n61.61\nmc rl\n42.41\ni4DS\n40.80\nCraftRL\n23.81\nUEFDRL\n17.90\nTD240\n15.19\nLAIR\n14.73\nElytra\n8.25\nkarolisram\n7.87\nTable 1: Scores of best-performing submissions of top teams in Round 1 and Round 2.\n3.3. Resources\nIn addition to the ObtainDiamond task, we created a number of auxiliary tasks, which we\nbelieved would be useful for solving ObtainDiamond. These tasks were provided to partici-\npants as Gym environments (Brockman et al., 2016). Additionally, we provided the partic-\nipants with a comprehensive starter pack8, consisting of extensive documentation, starter\ncode, a description of the submission procedure, a Microsoft Azure quick-start template,\nand the Docker images that we used to validate the training procedures during retraining.\nWe gave the participants a large dataset of human demonstrations (Guss* et al., 2019),\nwhich participants could use to train their agents. Preferred Networks9 provided extensive\nbaselines10 implemented in ChainerRL (Fujita et al., 2019). Participants were able to incor-\nporate these baselines into their submissions. These baselines included behavioral cloning,\ndeep Q-learning from demonstrations (DQfD) (Hester et al., 2018), Rainbow (Hessel et al.,\n2018), generative adversarial inverse RL (GAIL) (Ho and Ermon, 2016), and proximal policy\noptimization (PPO) (Schulman et al., 2017).\nAIcrowd11 provided a uniﬁed interface12 for participants to sign-up for the competition,\nask the organizers and other participants questions, and monitor their progress. We also\ncreated a public Discord server for more informal communication with participants.\nThrough our generous sponsor, Microsoft Azure, we provided compute grants of $500USD\neach in Micrsoft Azure13 credits for 50 individuals that self identiﬁed as lacking access to\nthe necessary compute power to participate in the competition. Additionally, thanks to\nMicrosoft Azure, we provided each of the top teams from Round 1 with $1500USD of Azure\ncredits for their experiments in Round 2.\n8. https://github.com/minerllabs/competition_submission_starter_template\n9. https://preferred.jp/en\n10. https://github.com/minerllabs/baselines\n11. https://www.aicrowd.com\n12. https://discourse.aicrowd.com/c/neurips-2019-minerl-competition\n13. https://azure.microsoft.com/en-us\n5\nRetrospective Analysis of the 2019 MineRL Competition\nFigure 2: Maximum item score for each team over the 100 evaluation episodes in Round 2.\n4. Outcomes for Participants\nWe quantitatively and qualitatively analyze the submissions made by participants of our\ncompetition. In Section 4.1, we provide quantitative information about the submissions. In\nSection 4.2, we summarize the approaches used by the top nine teams in the ﬁnal round.\nIn Section 4.3, we describe the special awards awarded as part of the competition.\n4.1. Submission Performance Overview\nIn Round 1, there were over 540 submissions, and 25 teams drastically outperformed the\nprovided baselines. Table 1 shows that the top 3 teams from Round 2 on average outper-\nformed the top 3 teams from Round 1 (means: 48.27 and 44.33, respectively), despite the\nRound 2 agents being trained on a hold-out test environment. Perhaps unsurprisingly, the\nRound 1 scores had a higher mean (31.77) and lower standard deviation (10.22) than the\nRound 2 scores (25.84 and 17.37, respectively). Additionally, the Round 1 scores had a\nsmaller range (31.30) than the Round 2 scores (53.74). Figure 2 shows the maximum item\nscore over the evaluation episodes of Round 2. Although no team obtained a diamond, the\ntop team obtained the penultimate prerequisite item to obtaining a diamond.\n4.2. Summary of Top Nine Solutions\nThe top four teams used some form of hierarchical RL, taking advantage of the hierarchi-\ncality of Minecraft. All teams used some form of action reduction (e.g., removing actions\nthat were almost never performed by human experts) to manage the complexity of the en-\nvironment. Most teams leveraged the human data to improve the sample eﬃciency of their\nalgorithms.\nThe top team, CDS, used a hierarchical deep Q-network with forgetting that uses an\nadaptive ratio for sampling expert demonstrations from a separate demonstration replay\nbuﬀer14 (Skrynnik et al., 2019).\nThe second-place team, mc rl, trained their hierarchi-\n14. A video example of CDS’s trained agent can be found here.\n6\nRetrospective Analysis of the 2019 MineRL Competition\nUse hierarchical\nUse human\nUse action\nRL\ndata\nreduction\nNumber of teams\n6\n7\n9\nPercent of teams\n67%\n78%\n100%\nTable 2: Overview of general approaches taken by the top 9 teams.\ncal policies entirely from human demonstrations with no environment interactions15. The\nthird-place team, i4DS, used demonstrations to bootstrap their hierarchical RL algorithm,\neﬀectively using the human data to pretrain the models to predict recorded human actions\nfrom observations, then used RL to reﬁne the resulting agents (Scheller et al., 2020).\nThe fourth-place team, CraftRL, aggregated grounded actions into options (Sutton et al.,\n1999) and then used a meta-controller to select between options.\nThey pretrained the\nmeta-controller on demonstration data using behavioral cloning.\nThe ﬁfth-place team,\nUEFDRL, used a single deep residual neural network trained to mimic human actions from\nthe MineRL dataset (Kanervisto et al., 2020). This team used action discretization to deal\nwith the complexity of the environment. The sixth-place team, TD240, used a discriminator\nsoft actor critic, which uses a discriminator based on adversarial inverse RL as a reward\nfunction.\nThe seventh-place team, LAIR, used meta-learning shared hierarchies (MLSH) (Frans\net al., 2018), a hierarchical RL algorithm, and pretrained the master and subpolicies with\nhuman data before training MLSH with environment interactions. The eighth-place team,\nElytra submitted a Rainbow baseline with modiﬁcations, including adding a small amount\nof stochasticity to the environment and restricting the camera movement to 1 degree of\nfreedom. However, they explored the idea of training an ensemble of value functions and\nbelieve this approach to be promising. The ninth-place team, karolisram, used the PPO\nbaseline with modiﬁcations, including removing frame-skip and reducing the action space.\n4.3. Special Awards\nIn addition to evaluating teams on the performance of their learning algorithms, we awarded\nteams prizes for research contributions. Since this was the ﬁrst year in which the competition\nwas held, we awarded two prizes to extreme and opposite research paradigms: one based\npurely on imitation learning and the other one based purely on RL, with no use of human\npriors.\nWe awarded the main research prize to mc rl for the former approach and the\nrunner-up research prize to karolisram for the latter approach. We hope that these special\nawards will encourage future participants to maximally explore the space of solutions, even\nif this comes at the expense of performance.\n5. Organizational Outcomes\nWe describe important organizational outcomes of the competition. We begin by presenting\nthe impact of our chosen rule set in Section 5.1.\nThen, in Section 5.2, we discuss the\neﬀectiveness of our distribution of competition materials. Finally, we summarize the eﬀects\nof our chosen evaluation methodology in Section 5.3.\n15. A video example of mc rl’s trained agent can be found here.\n7\nRetrospective Analysis of the 2019 MineRL Competition\n5.1. Repercussions of the Rules\nUnlike many other DRL competitions, we sought to limit the use of domain knowledge\nthrough feature engineering and hard-coded policies. Though this ruleset has led to the use\nof more general methods, it also led to a lot of clarifying questions from participants. Since\nthese were asked across various communication channels, there were duplicate questions and\nno single, comprehensive answer. In future renditions of this competition, we will have a\nclear place for rule clariﬁcations and a more robust restriction on use of domain knowledge.\nA secondary eﬀect of the limitation on domain knowledge use was the diﬃculty in\nchecking Round 1 submissions for this rule violation. This rule was the only one which\nled to complications of this type; restrictions on training time and number of samples are\nquantitative restrictions and therefore easy to enforce. Since competitor’s submissions were\nnot trained on a “hold out” environment, a committee spent more work hours than expected\nmanually reviewing the code to identify manually speciﬁed policies and other violations.\n5.2. Eﬀectiveness of Competition Materials\nAs mentioned in Section 3.3, we provided participants with the tasks as Gym environ-\nments, the dataset, and baseline method implementations. The environments and dataset\nwere both complete before the competition began, but the baselines were developed after\nthe competition was announced because of delays providing the environment to our collab-\norators, Preferred Networks. However, Preferred Networks worked quickly to provide the\nbaselines in time for participants to use them in Round 1, and many participants found\nthese baselines to be crucial in developing their own algorithms. In future iterations of this\ncompetition, we will be able to provide the baselines as soon as the competition begins now\nthat the baselines are complete.\nThe data was readily available and participants were generally able to successfully use\nit. The provided Gym environments were a wrapper around the Malmo Minecraft envi-\nronment (Johnson et al., 2016). In addition to wrapping Malmo in a standardized way, we\nﬁxed bugs and added functionality which allows faster and more eﬃcient training. These\nmodiﬁcations have increased interest in working on Minecraft, and we have been contacted\nby several academic groups about further extensions to the environment.\nCompetitor submissions had to adhere to a speciﬁc Docker format, as outlined in Sec-\ntion 3.2. Use of a standard container streamlined the evaluation process during both rounds,\nand it saved time from our end. Although we provided information about how to use the\nprovided container, based on feedback from participants, we plan to extend the documenta-\ntion to include a step-by-step demonstration of how to submit an agent using the provided\nstructure.\n5.3. Impact of Evaluation Methodology\nWe chose to have two rounds to balance the need for an explicit computational budget with\na desire to not limit participation in the competition. We did not restrict the number of\nteams who could participate during the Round 1; however, we chose a pre-speciﬁed number\nof teams to move to Round 2. As a result, we could commit to retraining all submissions\nduring Round 2. This commitment was possible due to the strict limits on training time\nas well as the selection of a speciﬁc system on which all training and evaluation would\n8\nRetrospective Analysis of the 2019 MineRL Competition\nbe performed. Due to this structure, we did not use more computational resources than\nexpected for evaluation.\nHowever, these same strict limits on training time led to delays in obtaining ﬁnal results.\nWe delayed the submission deadlines for competitors; as a result, this reduced the time\nbefore we were scheduled to announce results. Since training had to happen for a ﬁxed\nnumber of days on a speciﬁc system, we could not parallelize an individual competitor’s\nsubmission or run it on faster hardware. Despite the delay, we will use a similar restriction\nin future competitions due to the reproducibility and sample eﬃciency beneﬁts.\nAs mentioned in Section 3.3, we provided each team in Round 2 with a compute grant\nof $1500USD in Azure credits to enable training and validation of their methods. Because\nof the training time restrictions, participants were not incentivized to spend the remainder\nof their compute time on extending the training time of their ﬁnal approach. We believe\nwe allocated a suﬃcient amount to each team, since some teams had enough compute time\nremaining to continue to develop their methods even after the competition was over.\nFinally, although we eﬀectively utilized a novel visual domain randomization method to\nensure generalization and rule compliance, competitors spent a great deal of eﬀort shaping\nthe action space using inductive bias. To yield fully generalizable competition results in\nfuture iterations of this competition, we plan to remove semantic labels of the environment’s\naction space and apply a random bijective-mapping to the action space during evaluation.\n6. Conclusion\nWe ran the MineRL Competition on Sample Eﬃcient Reinforcement Learning Using Human\nPriors at NeurIPS2019 in order to promote the development of algorithms that: 1) use\nhuman demonstrations alongside reinforcement learning, 2) are sample eﬃcient, and 3) are\naccessible to the general AI community. We summarized the submissions and looked at the\nperformance of competitors in diﬀerent rounds. We discussed the eﬀects of the competition\nrules, the provided competition materials, and the evaluation methodology. Based on the\nlargely positive outcome of the competition, we plan to hold the competition again. Because\nno team obtained a diamond, we plan to focus on the same task, ObtainDiamond. We also\nplan to keep the same paradigm promoting the development of generalizable and robust\nlearning procedures. When we hold the competition again, we will clarify the rules and\nprovide more demonstrations of how to participate in order to increase participation.\nAcknowledgments\nWe thank Microsoft Azure for providing the computational resources for the competition,\nNVIDIA for providing prizes, AIJ for funding the inclusion grants, and Microsoft Research\nfor funding travel grants. We also thank AIcrowd for hosting and helping run the competi-\ntion, and Preferred Networks for providing the baseline method implementations. Finally,\nwe thank everyone who provided advice or helped organize the competition.\n9\nRetrospective Analysis of the 2019 MineRL Competition\nReferences\nD. Abel, D. E. Hershkowitz, G. Barth-Maron, S. Brawner, K. OFarrell, J. MacGlashan, and\nS. Tellex. Goal-based action priors. In Proceedings of the 25th International Conference\non Automated Planning and Scheduling, 2015.\nD. Abel, A. Agarwal, F. Diaz, A. Krishnamurthy, and R. Schapire. Exploratory gradi-\nent boosting for reinforcement learning in complex domains. In The 33rd International\nConference on Machine Learning Workshop on Abstraction in Reinforcement Learning,\n2016.\nR. Alterovitz, S. Koenig, and M. Likhachev. Robot planning in the real world: research\nchallenges and opportunities. AI Magazine, 37(2):76–84, 2016.\nK. C. Aluru, S. Tellex, J. Oberlin, and J. MacGlashan. Minecraft as an experimental world\nfor AI in robotics. In 2015 AAAI Fall Symposium Series, 2015.\nD. Arumugam, J. K. Lee, S. Saskin, and M. L. Littman. Deep reinforcement learning from\npolicy-dependent human feedback. arXiv preprint arXiv: 1902.04257, 2019.\nG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and\nW. Zaremba. OpenAI gym. arXiv preprint arXiv:1606.01540, 2016.\nR. Dubey, P. Agrawal, D. Pathak, T. L. Griﬃns, and A. A. Efros. Investigating human\npriors for playing video games. In Proceedings of the 35th International Conference on\nMachine Learning, 2018.\nK. Frans, J. Ho, X. Chen, P. Abbeel, and J. Schulman. Meta learning shared hierarchies.\nProceedings of the 6th International Conference on Learning Representations, 2018.\nS. J. Frazier and M. O. Riedl. Improving deep reinforcement learning in Minecraft with\naction advice. In Proceedings of the 15th AAAI Conference on Artiﬁcial Intelligence and\nInteractive Digital Entertainment, 2019.\nY. Fujita, T. Kataoka, P. Nagarajan, and T. Ishikawa. ChainerRL: A deep reinforcement\nlearning library.\nIn The 33rd Conference on Neural Information Processing Systems\nWorkshop on Deep Reinforcement Learning, 2019.\nW. H. Guss, C. Codel*, K. Hofmann*, B. Houghton*, N. Kuno*, S. Milani*, S. Mohanty*,\nD. Perez Liebana*, R. Salakhutdinov*, N. Topin*, M. Veloso*, and P. Wang*.\nThe\nMineRL competition on sample eﬃcient reinforcement learning using human priors. In\nThe 33rd Annual Conference on Neural Information Processing Systems Competition\nTrack, 2019.\nW. H. Guss*, B. Houghton*, N. Topin, P. Wang, C. Codel, M. Veloso, and R. Salakhutdinov.\nMineRL: A large-scale dataset of Minecraft demonstrations. In Proceedings of the 28th\nInternational Joint Conference on Artiﬁcial Intelligence, 2019.\n10\nRetrospective Analysis of the 2019 MineRL Competition\nM. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan,\nB. Piot, M. G. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforce-\nment learning. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence,\n2018.\nT. Hester, M. Vecer´ık, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, A. Sendonaris, G. Dulac-\nArnold, I. Osband, J. Agapiou, J. Z. Leibo, and A. Gruslys. Learning from demonstrations\nfor real world reinforcement learning. In Proceedings of the 32nd AAAI Conference on\nArtiﬁcial Intelligence, 2018.\nJ. Ho and S. Ermon. Generative adversarial imitation learning. In Advances in Neural\nInformation Processing Systems 29. 2016.\nB. Houghton, S. Milani, N. Topin, W. H. Guss, K. Hofmann, D. Perez-Liebana, M. Veloso,\nand R. Salakhutdinov. Guaranteeing reproducibility in deep learning competitions. In\nThe 33rd Annual Conference on Neural Information Processing Systems Challenges in\nMachine Learning Workshop, 2019.\nM. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The Malmo platform for artiﬁcial\nintelligence experimentation. In Proceedings of the 25th International Joint Conference\non Artiﬁcial Intelligence, 2016.\nA. Juliani, A. Khalifa, V.-P. Berges, J. Harper, H. Henry, A. Crespi, J. Togelius, and\nD. Lange. Obstacle tower: A generalization challenge in vision, control, and planning. In\nProceedings of the 28th International Joint Conference on Artiﬁcial Intelligence, 2019.\nAnssi Kanervisto, Janne Karttunen, and Ville Hautamki.\nPlaying minecraft with be-\nhavioural cloning. arXiv preprint arXiv:2005.03374, 2020.\nL. Kidziski, S. P. Mohanty, C. Ong, J. L. Hicks, S. F. Carroll, S. Levine, M. Salath, and\nS. L. Delp. Learning to run challenge: Synthesizing physiologically accurate motion using\ndeep reinforcement learning. The NIPS ’17 Competition: Building Intelligent Systems,\n2018.\nJ. Master, E. Patterson, S. Yousﬁ, and A. Canedo. String diagrams for assembly planning.\narXiv preprint arXiv: 1909.10475, 2019.\nA. Nichol, V. Pfau, C. Hesse, O. Klimov, and J. Schulman.\nGotta learn fast: A new\nbenchmark for generalization in RL. arXiv preprint arXiv:1804.03720, 2018.\nJ. Oh, V. Chockalingam, S. P. Singh, and H. Lee. Control of memory, active perception,\nand action in Minecraft. In Proceedings of the 33rd International Conference on Machine\nLearning, 2016.\nD. Perez-Liebana, K. Hofmann, S. P. Mohanty, N. Kuno, A. Kramer, S. Devlin, R. D. Gaina,\nand D. Ionita. The multi-agent reinforcement learning in Malmo (MARLO) competition.\nIn The 32nd Annual Conference on Neural Information Processing Systems Challenges\nin Machine Learning Workshop, 2018.\n11\nRetrospective Analysis of the 2019 MineRL Competition\nM. Pfeiﬀer, S. Shukla, M. Turchetta, C. Cadena, A. Krause, R. Siegwart, and J. Nieto.\nReinforced imitation: Sample eﬃcient deep reinforcement learning for mapless navigation\nby leveraging prior demonstrations. IEEE Robotics and Automation Letters, 3(4):4423–\n4430, 2018.\nC. Scheller, Y. Schraner, and M. Vogel. Sample eﬃcient reinforcement learning through\nlearning from demonstrations in Minecraft. arXiv preprint arXiv:2003.06066, 2020.\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy opti-\nmization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nT. Shu, C. Xiong, and R. Socher. Hierarchical and interpretable skill acquisition in multi-\ntask reinforcement learning. In Proceedings of the 35th International Conference on Ma-\nchine Learning, 2018.\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,\nD. Kumaran, T. Graepel, et al. A general reinforcement learning algorithm that masters\nchess, shogi, and go through self-play. Science, 362, 2018.\nA. Skrynnik, A. Staroverov, E. Aitygulov, K. Aksenov, V. Davydov, and A. I. Panov.\nHierarchical deep q-network with forgetting from imperfect demonstrations in Minecraft.\narXiv preprint arXiv: 1912.08664, 2019.\nR. S. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for\ntemporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112, 1999.\nC. Tessler, S. Givony, T. Zahavy, D. J. Mankowitz, and S. Mannor. A deep hierarchical\napproach to lifelong learning in Minecraft. In Proceedings of the 31st AAAI Conference\non Artiﬁcial Intelligence, 2017.\nA. Trott, S. Zheng, C. Xiong, and R. Socher. Keeping your distance: Solving sparse reward\ntasks using self-balancing shaped rewards. In Advances in Neural Information Processes\n32. 2019.\nO. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H.\nChoi, R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in StarCraft II using\nmulti-agent reinforcement learning. Nature, 2019.\nR. Zhang, F. Torabi, L. Guan, D. H. Ballard, and P. Stone. Leveraging human guidance\nfor deep reinforcement learning tasks.\nIn Proceedings of the 28th International Joint\nConference on Artiﬁcial Intelligence, 2019.\n12\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2020-03-10",
  "updated": "2020-06-18"
}