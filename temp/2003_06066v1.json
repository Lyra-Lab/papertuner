{
  "id": "http://arxiv.org/abs/2003.06066v1",
  "title": "Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft",
  "authors": [
    "Christian Scheller",
    "Yanick Schraner",
    "Manfred Vogel"
  ],
  "abstract": "Sample inefficiency of deep reinforcement learning methods is a major\nobstacle for their use in real-world applications. In this work, we show how\nhuman demonstrations can improve final performance of agents on the Minecraft\nminigame ObtainDiamond with only 8M frames of environment interaction. We\npropose a training procedure where policy networks are first trained on human\ndata and later fine-tuned by reinforcement learning. Using a policy\nexploitation mechanism, experience replay and an additional loss against\ncatastrophic forgetting, our best agent was able to achieve a mean score of 48.\nOur proposed solution placed 3rd in the NeurIPS MineRL Competition for\nSample-Efficient Reinforcement Learning.",
  "text": "Proceedings of Machine Learning Research 1:1–10, 2020\nNeurIPS2019 Competition & Demonstration Track\nSample Eﬃcient Reinforcement Learning through Learning\nfrom Demonstrations in Minecraft\nChristian Scheller\nchristian.scheller@fhnw.ch\nYanick Schraner\nyanick.schraner@fhnw.ch\nManfred Vogel\nmanfred.vogel@fhnw.ch\nInstitute for Data Science, University of Applied Sciences Northwestern Switzerland\nEditors: Hugo Jair Escalante and Raia Hadsell\nAbstract\nSample ineﬃciency of deep reinforcement learning methods is a major obstacle for their\nuse in real-world applications.\nIn this work, we show how human demonstrations can\nimprove ﬁnal performance of agents on the Minecraft minigame ObtainDiamond with only\n8M frames of environment interaction.\nWe propose a training procedure where policy\nnetworks are ﬁrst trained on human data and later ﬁne-tuned by reinforcement learning.\nUsing a policy exploitation mechanism, experience replay and an additional loss against\ncatastrophic forgetting, our best agent was able to achieve a mean score of 48. Our proposed\nsolution placed 3rd in the NeurIPS MineRL Competition for Sample-Eﬃcient Reinforcement\nLearning.\nKeywords: Imitation learning, deep reinforcement learning, MineRL competition\n1. Introduction\nThe NeurIPS MineRL competition introduced by Guss et al. (2019a) is focused on the\nproblem of sample eﬃcient reinforcement learning by leveraging human demonstrations.\nThe goal of the competition is to solve the ObtainDiamond task using 8 million samples\nfrom the MineRL environment. Additionally, agents can learn from a dataset consisting of\nover 60 million state-action pairs of human demonstrations solving nine distinct complex,\nhierarchical tasks in the MineRL environment.\nImitation learning is a promising method to address such hard exploration tasks. In\nthis work, we propose a training pipeline that utilizes human demonstrations to bootstrap\nreinforcement learning. Agents are represented as neural networks that predict next actions\n(policy) and evaluate environment states (value function). In a ﬁrst stage, policy networks\nare trained in a supervised setting to predict recorded human actions given corresponding\nobservations. These policy networks are then reﬁned in a second stage by reinforcement\nlearning in the MineRL environment.\nWe show that naive reinforcement learning applied to the supervised trained policies did\nnot lead to improvement within 8M frames. In contrast, we observe collapsing performance.\nWe address this problem with four major enhancements: (1) To improve sample eﬃciency we\nmake extensive use of experience replay (Lin, 1992). (2) We prevent catastrophic forgetting\nand stabilize learning by using CLEAR (Rolnick et al., 2019). (3) We investigate a new\nmechanism named advantage clipping that allows agents to better exploit good behaviour\nc⃝2020 C. Scheller, Y. Schraner & M. Vogel.\narXiv:2003.06066v1  [cs.LG]  12 Mar 2020\nSample Efficient Reinforcement Learning in Minecraft\nlearned from demonstrations. (4) We demonstrate that our agents beneﬁt from a separate\ncritic network compared to a combined policy-value network commonly used. We discuss\nthe major components of our solution in the following sections.\n1.1. Related Work\nThe application of imitation learning itself or in combination with reinforcement learning\nto simplify the exploration problem and improve ﬁnal performance has been investigated\nin various challenging domains. For AlphaGO (Silver et al., 2016), the ﬁrst reinforcement\nlearning agent to beat the human world champion in the board game Go, Silver et al. ap-\nplied supervised learning from human demonstration to learn policy- and value-networks\nthat are later reﬁned by reinforcement learning. AlphaStar (Vinyals et al., 2019), the ﬁrst\nStarCraft II AI to reach grand master level performance, was initially trained on human\ndemonstrations and later improved in a league, competing with diﬀerent agents and con-\nstantly learning using reinforcement learning. In the same work Vinyals et al. introduced\nan upgoing policy gradient that is closely related to self-imitation learning (Oh et al., 2018)\nand similar to our advantage clipping. Both methods restrict updates to only better-than-\naverage trajectories. In their work on Deep Q-learning from Demonstrations, Hester et al.\n(2018) successfully incorporated demonstration data in the reinforcement learning loop to\nimprove performance in 11 out of 42 games of the Arcade Learning Environment. On the\nsame domain, Cruz Jr et al. (2017) showed that pre-training on demonstrations leads to a\nreduced training time of reinforcement learning algorithms. Gao et al. (2018) introduced a\nhybrid imitation and reinforcement learning algorithm that learns from imperfect demon-\nstrations to improve performance on tasks in realistic 3D simulations.\nExperience replay (Lin, 1992) is a common technique to improve sample eﬃciency and\nreduce sample correlation of deep Q-learning algorithms (Mnih et al., 2015; Schaul et al.,\n2016; Hessel et al., 2018). Wang et al. (2017) showed that experience replay can signiﬁcantly\nimprove sample eﬃciency of their actor-critic deep reinforcement learning agents. Espeholt\net al. (2018) achieved improved performance on tasks in visually complex environments by\ncombining a novel oﬀ-policy actor-critic algorithm with experience replay.\nWith CLEAR, Rolnick et al. (2019) showed that experience replay can eﬀectively counter\ncatastrophic forgetting in continual learning. In contrast, previous research on mitigating\nforgetting mainly focused on synaptic consolidation approaches (Rusu et al., 2016; Kirk-\npatrick et al., 2017; Schwarz et al., 2018).\n1.2. ObtainDiamond\nObtainDiamond is a Minecraft mini-game with the goal of collecting one piece of diamond\nwithin 15 minutes of play time. The player starts at a random location on a randomly\ngenerated map, without any items. To mine a diamond, a player has to ﬁrst craft an iron\npickaxe, which itself requires a list of prerequisite items that hierarchically depend on each\nother. The player receives a reward for each of these items, whereas subsequent items yield\nexponentially higher rewards. The game ends when the player obtains a diamond, dies or\nreaches the maximum step count of 18000 frames.\nThe challenges for reinforcement learning agents to succeed in this mini-game are man-\nifold. The rarity of diamonds (2-10 times less frequent than other ores), the dependence on\n2\nSample Efficient Reinforcement Learning in Minecraft\nprerequisite items and the sparsity of the reward signal make naive exploration methods\npractically infeasible. Agents have to solve long-horizon credit assignment problems, where\nrewards have to be transported over many time steps. Besides information about current\ninventory and equipment, agents perceive the environment through visually complex point-\nof-view observations, from which an optimal next action must be inferred. Since maps are\ngenerated randomly, agents have to generalize across a virtually inﬁnite number of maps.\nThe competition introduces further complexity to the problem. 8 million samples is sig-\nniﬁcantly fewer than what reinforcement learning algorithms traditionally need to master\nsimilar problems. For example, the current state of the art for the Atari-57 benchmark\nuses 20 billion frames (Schrittwieser et al., 2019). The limited number of frames forces\ncompetition entries to be particularly sample eﬃcient. The hardware- and time-limitation\nrestrict model complexity and algorithms computing power demands. Although ObtainDia-\nmond is a diﬃcult problem for reinforcement learning agents, it is a rather easy for humans.\nExperienced players solve it in less than 15 minutes (Guss et al., 2019b).\n2. Methods\nFormally, we consider the ObtainDiamond task as a partially observable Markov decision\nprocess. In order to deal with uncertainty about the current state, we employ long short-\nterm memories (LSTMs) (Hochreiter and Schmidhuber, 1997). This allows us to reduce the\nproblem to a standard Markov decision process (S, A, P, R), where state st ∈S at time\nstep t is given by observation ot and the current state of the LSTM ht. Hereby S is the set\nof states, A is the set of actions, P(s′ | s, a) is a state transition probability function and\nR(s, a) is the reward function. The goal is to learn a policy πθ(a | s) that maximizes the\nexpected sum of rewards Eπθ[PT\nt=1 rt], where θ ∈Rn is a parameter vector and T is the\nepisode length.\n2.1. Network architecture\nThe network architecture for policy and value function is based on the residual model\nintroduced by Espeholt et al. (2018). This architecture has been shown to be eﬀective in\nvisually complex environments such as the DeepMind DMLab-30 environment (Espeholt\net al., 2018), the obstacle tower challenge (Nichol, 2019) and the CoinRun environment\n(Cobbe et al., 2019).\nIn this model, spatial inputs are passed to a convolutional neural network with 6 residual\nblocks, each consisting of 3 convolutional layers to produce a spatial representation. Non-\nspatial inputs are concatenated with the agents previously taken action and processed by\ntwo dense layers (256 and 64 units respectively) to form a non-spatial representation. The\nspatial and non-spatial representations are then concatenated and fed into an LSTM cell\nwith a hidden size of 256. Since MineRL has a composed action space, we represent each\naction with an independent policy on top of the LSTM output.\nIn Section 3.1 we evaluate craft and smelt policies that use the inventory as additional\ninput, processed by a separate dense two-layer network (256 and 64 units respectively).\nThe idea behind this modiﬁcation is that the availability of craft and smelt actions directly\ndepends on the current inventory.\n3\nSample Efficient Reinforcement Learning in Minecraft\n2.2. Imitation learning\nAs a ﬁrst step, we train policies πθ(a|s) to predict human actions a, given state s on\nhuman demonstrations from the MineRL dataset Guss et al. (2019b). The episode length\nof the demonstrations is up to 60’000 frames. To train LSTMs eﬃciently an episode length\nreduction becomes necessary. We use the following subsampling strategy:\n• State-action pairs with no-op actions are skipped without compensation.\n• State-action pairs containing actions that we do not consider necessary for the task,\nlike sneak or sprint, are skipped without compensation.\n• Consecutive state-action pairs that contain the same action are skipped. Instead we\nadd a step multiplier that accounts for the skipped number of frames.\nThis step\nmultiplier must be learned by the agent.\n• Camera rotations are accumulated and skipped until a threshold of 30 degrees is\nreached, the rotation direction changes or a new action is issued.\nSequences are truncated when a length of 2’000 frames has been reached. We only use\ndemonstrations on the tasks ObtainDiamond, ObtainIronPickaxe and TreeChop for training.\nWe did not consider demonstrations on other tasks to be suitable for imitation learning on\nthe ObtainDiamond task.\nDuring training we uniformly sample batches of episodes from the resulting dataset D\nin the form of sequences of state-action pairs (s, a) ∈D. These batches are used to update\nthe parameters θ by stochastic gradient descent on the cross-entropy loss.\nWe do not learn a value function estimate from the demonstrations, since the demon-\nstrators policy is very diﬀerent compared to the policy we obtain from supervised learning.\n2.3. Reinforcement learning\nWe employ the Importance Weighted Actor-Learner Architecture (IMPALA) by Espeholt\net al. (2018) to improve policy πθ(a | s) obtained by supervised learning and to approximate\nthe value function with Vφ(s). The choice of this architecture is mainly motivated by the\nfollowing two properties. First, IMPALA is an oﬀ-policy actor-critic method, which enables\nthe use of experience replay. Second, as shown by Espeholt et al. (2018), asynchronous\nlearners and actors can signiﬁcantly improve the training throughput. Slow environments\nlike ObtainDiamond with a high variance of update time and slow episode restarts beneﬁt\nfrom this asynchrony.\nIn Section 3 we show that, within the limited number of frames available, IMPALA\napplied naively to pre-trained policies exhibits collapsing performance. In the following sec-\ntions, we describe our proposed enhancements to prevent performance decline and improve\nﬁnal performance.\n2.3.1. Separate networks for actor and critic\nIn actor-critic algorithms, neural networks for policy and value functions are often rep-\nresented by individual heads on top of a single neural network. This enables parameter\n4\nSample Efficient Reinforcement Learning in Minecraft\nsharing but introduces the problem of combined policy and value loss gradients. With sep-\narate networks for actor and critic however, all weights of a network are allocated to either\nthe task of policy or value function approximation. As shown in Section 3, we were able to\nachieve better results by using separate networks for the actor and the critic.\n2.3.2. Experience replay\nSimilar to Wang et al. (2017), we extensively use experience replay to increase sample eﬃ-\nciency of the reinforcement learning training. The use of experience replay further reduces\nthe correlation between samples. The hardware restrictions of the competition limit the\nparallelism to ﬁve instances of ObtainDiamond, which leads to a low diversity and high\ncorrelation of the training data.\nIn line with Espeholt et al. (2018), we employ a ring buﬀer from which samples are\ndrawn uniformly at random. In our experiments in Section 3 we evaluated diﬀerent replay\nratios, deﬁned as the proportion of the batch size that is sampled from the replay buﬀer\n(e.g. a replay ratio of 3 corresponds to 3 replay samples per online sample).\n2.3.3. Advantage clipping\nWe found that policies obtained from imitation learning yield returns with high variance.\nThis complicates the value function approximation. As a result, there is a risk of erroneous\nvalue estimates wrongly discouraging desired behaviour. The idea of advantage clipping is\nto prevent such destructive updates and to only reinforce better-than-expected trajectories.\nTo this end, we introduce a simple mechanism to the policy gradient loss where we clip\nnegative advantages to zero:\n−ρt(∇θ log πθ(at | st)) max(rt + γvt+1 −Vφ(st), 0)\nwhere vt is the V-trace target and ρt = min(¯ρ, π(at|st)\nµ(at|st)) is the truncated importance sampling\nweight with truncation level ¯ρ = 1 and behaviour policy µ.\nClipping the advantage to strictly positive values prevents the policy gradients from\nreducing probabilities of sampled actions.\nAs this mechanism suppresses learning from\nundesired experiences, we consider it primarily useful to stabilize training in high-variance\nenvironments. We believe that advantage clipping could also be scheduled over time. The\nchoice of clipping threshold, optimal scheduling and theoretical aspects are left for future\nresearch.\nAdvantage clipping is strongly related to self-imitation learning Oh et al. (2018) which\nexploits past beneﬁcial decisions and proved its usefulness in diﬃcult exploration tasks.\n2.3.4. Preventing catastrophic forgetting of rarely encountered sub-tasks\nFigure 2 shows how ﬁne-tuning through reinforcement learning means that agents solve\nearly sub-tasks more frequently but complete later sub-tasks signiﬁcantly less often. This\nreduces the overall performance of the agents, since the reward increases exponentially for\nlater sub-tasks. In this section we focus on how we can prevent agents from forgetting to\nsolve these highly rewarding tasks.\n5\nSample Efficient Reinforcement Learning in Minecraft\nFigure 1: Training performance with replay ratios of 1, 3, 7, 15 and 31. Lines and bars\nrepresent the mean score of 3 runs with diﬀerent random seeds. Bands and error\nbars correspond to 95% conﬁdence intervals.\nThis problem is an example of catastrophic forgetting in continual learning. Agents\ninitialized with supervised trained policies encounter later sub-tasks less frequently than\nearlier ones. As a result, most policy updates concern early sub-tasks and override behaviour\nobtained from demonstrations of later sub-tasks.\nWe employ CLEAR by Rolnick et al. (2019) to prevent such catastrophic forgetting and\nincrease stability of the learning. CLEAR is a simple but eﬀective method that builds upon\nthe concept of experience replay to reduce forgetting. It introduces two new components\nto the IMPALA loss function: (1) the KL divergence between current and past policy\ndistributions and (2) the ℓ2-norm of the diﬀerence between current and past value functions,\nwhere past policy distributions and value functions are sampled from an experience replay\nbuﬀer.\n3. Experiments\nWe evaluated the main parts of our solution and how our proposed modiﬁcations improve\nthe agent’s performance. In accordance with the MineRL competition rules our experiments\ncomplete in less than four days on hardware no more powerful than 6 CPU cores, 56 GiB\nRAM and a single Nvidia K80 GPU. The action space is transformed as follows: camera\nrotations are discretized to (−30◦, 0◦, +30◦). We ran three experiments with diﬀerent ran-\ndom seeds for each method and evaluated them with 100 episodes. We report mean scores,\nstandard deviations, best scores and max episode scores in Table 1.\n6\nSample Efficient Reinforcement Learning in Minecraft\n(a) Reward frequencies\n(b) Reward frequencies relative to\nsupervised trained policy\nIndex\nReward\n0\n1\n1\n2\n2\n4\n3\n4\n4\n8\n5\n16\n6\n32\n7\n64\n8\n128\n(c) Rewards\nFigure 2: Figure 2(a) and Figure 2(b) compare reward frequencies after (1) supervised\nlearning, (2) ﬁne-tuning with IMPALA, (3) ﬁne-tuning with IMPALA and\nCLEAR and (4) ﬁne-tuning with IMPALA and advantage clipping.\nRL ﬁne-\ntuning improved the performance on early sub-tasks, whereas CLEAR and Ad-\nvantage Clipping also enabled agents to obtain later rewards more often. These\nlater rewards have an exponentially higher eﬀect on the score, as shown in Ta-\nble 2(c).\n3.1. Imitation learning\nWe ran supervised experiments with the method described in Section 2.2. Each experiment\nwas trained for 125 epochs, with a learning rate of 0.001 and batch size of 16. As shown in\nTable 1, agents beneﬁt from an added inventory input for craft and smelt policies (CP).\n3.2. Reinforcement learning\nReinforcement learning experiments are trained on the maximum allowed number of frames\nwith a batch size of 64. The agents are initialized with the policies obtained from imitation\nlearning. In the following paragraphs, we analyze how our proposed enhancements lead to\nbetter results.\nExperience replay\nWe tested experience replay (ER) with replay ratios of 1, 3, 7, 15\nand 31. The results are shown in Figure 1. We observe signiﬁcantly increased performance\nwith larger replay ratios. A replay ratio of 15 performed best overall.\nSeparate networks for actor and critic\nWe evaluated the separation of actor and critic\n(SAC) into individual networks. Both networks use the same architecture as described in\nSection 2.1. For the ﬁrst 500’000 frames, we only trained the value network to let it catch\nup with the policy network. Our experiments show that separate actor-critics are less prone\nto performance collapse, but fall short of the maximum scores that the pre-trained policy\nachieves.\n7\nSample Efficient Reinforcement Learning in Minecraft\nTable 1: Ablation study of our proposed methods (CP: craft policy, ER: experience replay,\nSAC: separate actor and critic networks, AC: advantage clipping, CL: CLEAR).\nExperiment\nMean\nBest\nMax\nSupervised\n10.3 ± 4.9\n13.8\n131\n+CP\n16.2 ± 4.1\n20.2\n163\nIMPALA\n2.9 ± 2.0\n4.8\n35\n+ER\n9.6 ± 0.7\n10.5\n67\n+ER +SAC\n15.7 ± 3.2\n20.0\n99\n+ER +SAC +AC\n27.1 ± 4.6\n33.6\n163\n+ER +SAC +CL\n37.5 ± 4.3\n41.8\n163\n+ER +SAC +AC +CL\n39.9 ± 8.1\n47.9\n163\nHuman demonstrations\n1004.0\n-\n1571\nAdvantage clipping\nWith advantage clipping (AC) we observed a signiﬁcant improve-\nment of the mean score. We ﬁnd that advantage clipping encourages exploitation of good\nbehaviour of the pre-trained policy and counteracts catastrophic forgetting which is evident\nin the unchanged maximum score, as shown in Table 1.\nCLEAR\nWe applied the CLEAR method (CL) with policy-cloning and value-cloning\nweights of 0.01 and 0.005 respectively as proposed by Rolnick et al. (2019). Like advan-\ntage clipping, CLEAR similarly prevents catastrophic forgetting and stabilizes training,\nbut achieves better performance on average. The combination of both methods yields the\nbest results. To illustrate the eﬀects of advantage clipping and CLEAR on catastrophic\nforgetting, we break down the agent’s ability to achieve individual rewards in Figure 2.\n4. Conclusion\nWe introduced a training pipeline that combines imitation learning with reinforcement\nlearning to train agents on ObtainDiamond. Our results reveal that performance on highly\nrewarding later sub-tasks decreased when we applied IMPALA naively to imitation learned\npolicies. We found that experience replay was crucial to improve the agent’s performance\nwhen limited to 8M environment frames. Advantage clipping successfully stabilized the\nlearning and lead to substantially improved policies. By applying CLEAR, we were able\nto prevent catastrophic forgetting of rare but highly rewarding behaviour. We showed that\nthe combination of imitation learning, IMPALA, experience replay with large replay ratios,\nseparate networks for policy and value functions, advantage clipping and CLEAR allowed\nour agents to achieve a mean score of 40. For our best individual agent we observed a mean\nscore of 48.\nAcknowledgments\nWe would like to thank Stephanie Milani and Simon Felix for providing valuable feedback\non the previous versions of this manuscript.\n8\nSample Efficient Reinforcement Learning in Minecraft\nReferences\nKarl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, and John Schulman. Quan-\ntifying generalization in reinforcement learning. In Proceedings of the 36th International\nConference on Machine Learning, volume 97, pages 1282–1289, 2019.\nGabriel V Cruz Jr, Yunshu Du, and Matthew E Taylor. Pre-training neural networks with\nhuman demonstrations for deep reinforcement learning. arXiv preprint arXiv:1709.04083,\n2017.\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward,\nYotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. IMPALA: Scalable dis-\ntributed deep-RL with importance weighted actor-learner architectures. In Proceedings\nof the 35th International Conference on Machine Learning, volume 80, pages 1407–1416,\n2018.\nYang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. Reinforcement\nlearning from imperfect demonstrations. arXiv preprint arXiv:1802.05313, 2018.\nWilliam H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno,\nStephanie Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov,\nNicholay Topin, et al. The MineRL competition on sample eﬃcient reinforcement learning\nusing human priors. NeurIPS Competition Track, 2019a.\nWilliam H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel,\nManuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft\ndemonstrations. In Proceedings of the Twenty-Eighth International Joint Conference on\nArtiﬁcial Intelligence, IJCAI-19, pages 2442–2448, 2019b. doi: 10.24963/ijcai.2019/339.\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will\nDabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Com-\nbining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference\non Artiﬁcial Intelligence, 2018.\nTodd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan\nHorgan, John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demon-\nstrations. AAAI Publications, Thirty-Second AAAI Conference on Artiﬁcial Intelligence,\n2018.\nSepp Hochreiter and J¨urgen Schmidhuber. Long Short-Term Memory. Neural Computation,\n9(8):1735–1780, 1997. doi: 10.1162/neco.1997.9.8.1735.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,\nAndrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-\nBarwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings\nof the national academy of sciences, 114(13):3521–3526, 2017.\nLong-Ji Lin.\nSelf-improving reactive agents based on reinforcement learning, planning\nand teaching. Machine Learning, 8(3):293–321, 1992. ISSN 1573-0565. doi: 10.1007/\nBF00992699.\n9\nSample Efficient Reinforcement Learning in Minecraft\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.\nBellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529–533,\n2015. ISSN 1476-4687. doi: 10.1038/nature14236.\nAlex Nichol.\nPickled ml,\n2019.\nURL https://blog.aqnichol.com/2019/07/24/\ncompeting-in-the-obstacle-tower-challenge.\nJunhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee.\nSelf-imitation learning.\nIn\nProceedings of the 35th International Conference on Machine Learning, volume 80, pages\n3878–3887, 2018.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne.\nExperience replay for continual learning. In Advances in Neural Information Processing\nSystems 32, pages 348–358. Curran Associates, Inc., 2019.\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,\nKoray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks.\narXiv preprint arXiv:1606.04671, 2016.\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver.\nPrioritized experience\nreplay. In International Conference on Learning Representations, 2016.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre,\nSimon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al.\nMastering atari, go, chess and shogi by planning with a learned model. arXiv preprint\narXiv:1911.08265, 2019.\nJonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska,\nYee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable\nframework for continual learning.\nIn International Conference on Machine Learning,\npages 4528–4537, 2018.\nDavid Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den\nDriessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanc-\ntot, et al. Mastering the game of go with deep neural networks and tree search. Nature,\n529(7587):484–489, 2016. ISSN 1476-4687. doi: 10.1038/nature16961.\nOriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha¨el Mathieu, Andrew Dudzik,\nJunyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al.\nGrandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575\n(7782):350–354, 2019. ISSN 1476-4687. doi: 10.1038/s41586-019-1724-z.\nZiyu Wang,\nVictor Bapst,\nNicolas Heess,\nVolodymyr Mnih,\nR´emi Munos,\nKoray\nKavukcuoglu, and Nando de Freitas. Sample eﬃcient actor-critic with experience replay.\nIn 5th International Conference on Learning Representations, ICLR 2017, 2017.\n10\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-03-12",
  "updated": "2020-03-12"
}