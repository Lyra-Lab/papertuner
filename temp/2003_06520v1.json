{
  "id": "http://arxiv.org/abs/2003.06520v1",
  "title": "Symmetry Detection of Occluded Point Cloud Using Deep Learning",
  "authors": [
    "Zhelun Wu",
    "Hongyan Jiang",
    "Siyun He"
  ],
  "abstract": "Symmetry detection has been a classical problem in computer graphics, many of\nwhich using traditional geometric methods. In recent years, however, we have\nwitnessed the arising deep learning changed the landscape of computer graphics.\nIn this paper, we aim to solve the symmetry detection of the occluded point\ncloud in a deep-learning fashion. To the best of our knowledge, we are the\nfirst to utilize deep learning to tackle such a problem. In such a deep\nlearning framework, double supervisions: points on the symmetry plane and\nnormal vectors are employed to help us pinpoint the symmetry plane. We\nconducted experiments on the YCB- video dataset and demonstrate the efficacy of\nour method.",
  "text": " \nSymmetry Detection of Occluded Point Cloud Using Deep Learning \nZhelun Wua*, Hongyan Jiangb, Siyun Hec \na Shenzhen Institute of Advanced Technology, Chinese Academy of Science, China \nb China Telecom Tech Division Hunan, China \nc Yale University, New Haven, United States \nAbstract \nSymmetry detection has been a classical problem in computer graphics, many of which using traditional geometric methods. In \nrecent years, however, we have witnessed the arising deep learning changed the landscape of computer graphics. In this paper, we \naim to solve the symmetry detection of the occluded point cloud in a deep-learning fashion. To the best of our knowledge, we are \nthe first to utilize deep learning to tackle such a problem. In such a deep learning framework, double supervisions: points on the \nsymmetry plane and normal vectors are employed to help us pinpoint the symmetry plane. We conducted experiments on the YCB-\nvideo dataset and demonstrate the efficacy of our method. Our code will be available soon after on GitHub. \n \nKeywords: Deep Learning; 3D Symmetry Detection; Computer Graphics \n1. Introduction \nIn the human visual system, our brains are very skilled at finding and utilizing symmetry of objects as abstraction \nentails finding intrinsic logic of shape. Such an assumption is legitimate, given the ubiquity of symmetry in human-\nmade merchandise.  \n Additionally, symmetry offers a critical hint when it comes to object completion in occlusion. Completion of such \nis conducive to object detection and makes it more robust in cluttered scenarios. Despite all these advantages in mind, \nthe status quo is far from satisfactory. Most works on symmetry are built on top of classical methodology, including \ncurvature, ICP (Iterative Closest Point), and others. None had dealt with deep-learning oriented symmetry detection \non occluded point cloud before, save for PRS-Net, which aimed to predict symmetry plane for the mesh-based 3D \nobjects with deep-learning techniques. \nUnlike PRS-Net with exact three symmetry predictions, our algorithm can deal with an indefinite amount of \nsymmetries as we base on double supervisions and RANSAC (random sample consensus). In our work, we are trying \nto locate the symmetry plane by two elements: the points on the plane and the normal vector of the plane. These are \nthe very two elements that determine where a plane is. \nTwo stages of training are employed in order to get our model well-trained. In the first stage, the primary goal is to \nlet the model learn the whereabouts of those points. The second stage begins when we ascertain the stability of point \npredictions. In the second stage, the model learns to predict normal vectors concerning those points. After these two \nstages, we can extrapolate the location of the symmetry plane using RANSAC. \nActually, by introducing RANSAC into the algorithm, we successfully do away with the restriction of fixation on \nthe number of predictions. Of course, we are not saying our algorithm is robust enough. For one, our method relies on \nthe presence of points on symmetry planes. Further works on more robust algorithms may well become available after \nthis paper. \nThis paper mainly has three contributions: \n1. \nWe propose a novel deep-learning symmetry detection framework for 3D models. Our model is the \nfirst effective model to deal with the occluded point cloud. By using double supervisions, we can detect the \n                                                \n* Corresponding author. Tel.: +86-13618490805. \nE-mail address: zhelunwu@hotmail.com \n2 \n \nsymmetry as long as points on the symmetry plane are present in the observer‚Äôs view. \n2. \nAttributed to a two-step framework and RANSAC, our algorithm allows flexible numbers of symmetry to be \nlearned and detected under the framework. \n3. \nA new accuracy measurement for symmetry detection is designed, and we elaborate on how PR-curve can be \ndrawn in our scenario. \n2. Related Works \n2.1. Symmetry Detection \nBoth 2D and 3D geometry have a long history of symmetry detection. Generally, there are four types of symmetry \nproblems, two types on two dimensions: extrinsic and intrinsic, global and partial. \nGlobal and partial symmetry differ on the scale of symmetry; the former requires the symmetry to map the entire \nobject to itself while the latter only requires part of the object. Extrinsic and intrinsic symmetry differ on metrics of \nthe symmetry; the previous measures symmetry on natural Euclidean space and the other measures on different metric \nspaces. In this paper, we are aiming to solve extrinsic global symmetry detection, especially for reflective symmetry \ndetection. \nAs far as extrinsic global symmetry concerns, various works have made progress. Some researchers have made \nadaptions to the original problem. Zabrodsky et al. 1 introduce the concept of approximate symmetry to tackle the \nproblem. Raviv et al. 2 adapt the symmetries for non-rigid shapes and Kazhdan et al. 3 offer to solve n-fold rotational \nsymmetries. Others use mapping to solve the problem. Podolak et al. 4 use PRST (planar reflective symmetry transform) \nto detect symmetry. Mitra et al. 6 also use transformation to extract constant symmetry features and clustering to detect \nsymmetries. However, most of these works are aimed to tackle 2D detection. \nOther works can handle 3D symmetry detections. One of the closest is the work by Aleksandrs Ecins et al.  5 where \ncluttered objects are segmented by help of recursively detecting symmetries and segmenting. We will compare the \neffect of this paper with our paper in the experiments. PRS-Net 7 is the first one to deal with 3D data with deep learning \nmethods; however, unlike our paper, it is not designed to adapt to cluttered objects.   \n2.2. Point Cloud Deep Representation \n3D cases are quite different from 2D images. The geometry and depth information need to be absorbed in \nconquering symmetry detection. The complexity of 3D geometry representation probably explains the diversity when \nit comes to dealing with symmetry detections. PRS-Net 7 deals with mesh-based 3D models. Paper5 by Ecins uses \npoint clouds; however, it also requires occlusion map 9. Our paper does not require extra information other than RGB-\nD images. The framework is heavily dependent on the DenseFusion 8 framework. DenseFusion extracts point cloud \ninformation from multiple scales by leveraging both color and geometry information. \n \n \n \n3 \n3. Methods \n \nFig. 1. Symmetry Detection Framework Using Double Supervision \n \nCustomarily, when we predict reflectional symmetry, we are anticipating a point on the plane and the normal vector. \nIn this method, we conquer the reflection symmetry detection of occluded point cloud by leveraging two point-wise \npredictions. First, we learn to predict the on-plane confidence, whether a point is on the symmetry plane. Meanwhile, \npoint-wise normal vectors are predicted for on-plane points. Since the location of each on-plane point combined with \nits normal vector prediction uniquely decides the symmetry plane, we should fuse all these symmetry plane predictions \nof on-plane points into our final symmetry predictions. In this case, we employ RANSAC to fuse point-wise \npredictions.  \n3.1. Feature Extraction \nIn our framework, as shown in Fig. 1, we adopted the DenseFusion framework as our backbone in geometric and \ncolor embedding extraction. In the first stage, we use the YCB ground-truth segmentation label to segment the object \nout on the photo. The segmented object is then projected onto the 3D space. Apparently, these projections may be \noccluded, even heavily occluded. These projected points are passed into the network, and CNN (convolutional neural \nnetwork) is employed to extract color embeddings while resorting to PointNet 10 for geometric embeddings. These \npixel-wise features are then fused and concatenated together, leveraged to generate on-plane confidence and point-\nwise normal vectors. \n4 \n \n3.2. Training \nTo successfully train the model, we first let the point learn if it is on the symmetry plane. Those points that are \ncloser than œµ\" to the symmetry plane are regarded as on-plane points. Those on-plane points should be labeled as 1, \nwhile off-plane points should be labeled as 0. We adopt cross-entropy loss here: \n \nwhere ùëÉ(ùëù) is the on-plane confidence prediction for the point ùëù, ùëÇ(ùëÜ) is the on-plane point sets for symmetry plane \nùëÜ, i.e., those points within distance ùúñ\" of ùëÜ and ùëÜùë¶ùëö are all ground-truth symmetry planes for an occluded point cloud.  \nAfter some steps, some points start to have higher on-plane confidence prediction, then we are letting those points \nlearn normal vectors directions. Compared to on-plane prediction, we loosen up the distance from ùúñ\" to ùúñ-. The loss \nfor learning normal vectors is ùêø1 loss between the prediction and the ground truth, \n \nwhere ùëÅ(ùëù) is the normal vector prediction for point ùëù, ùëÅ12(S) is the ground-truth normal vector of plane ùëÜ, ùëÇ‚Ä≤(ùëÜ) \nare those points within distance ùúñ-  of ùëÜ and ùëÜùë¶ùëö are all ground-truth symmetry planes for an occluded point cloud.  \nSo the training loss, in conclusion, is the following, \nand P56 = 0.6, ùúÜ= 10 in our training. \n \n3.3. Testing \nNow that we are done with the training, now we have to fuse all symmetry plane predictions into final predictions \nas we briefed at the beginning of the section. We devise a three-step RANSAC in our inference stage: \n1. \nWe pick ten random points out of the point cloud and settle on the one point ùê∂ with the proposal sweeping \nacross the maximum number of points. \n2. \nAccording to the proposal of point ùê∂, we sift through those points with similar proposals (ùêø1 distance ùëëùëñùë†ùë°\") \nand take them out to prepare a unanimous vote. We denote the set as ùëâùëúùë°ùëí(ùê∂). \n3. \nInitialized with the proposal of point ùê∂, an iterative optimizer is employed to generate a proposal that covers \nthe maximum number of points. Besides the proposal, we also provide a confidence score calculated from \nthe percentage of points covered in ùëâùëúùë°ùëí(ùê∂). After getting the proposal and the confidence score, points with \nsimilar proposals (ùêø1 distance ùëëùëñùë†ùë°-, we usually set ùëëùëñùë†ùë°- > ùëëùëñùë†ùë°\") in the point cloud are scrubbed. \n \nThe three-step RANSAC is executed until there are insufficient number of points to feed into. Specifically, ùëëùëñùë†ùë°\" =\n0.6 and ùëëùëñùë†ùë°- = 1.3. \nThrough the procedure, we are able to generate some symmetry predictions and confidence scores, indicating how \nsure the algorithm is about the correctness of these predictions.  \n4. Experiments \nIn this section, we first conduct quantitative experiments comparing our performance on objects suffering from \ndifferent degrees of occlusion with the state-of-the-art work from Ecins5. Next up, we show some of the galleries \ndemonstrating prediction accuracies visually. All our experiments are conducted on the YCB-video dataset. \n \n \n5 \n \n4.1. Quantitative Accuracy Experiments \nWe start by introducing our way of accuracy measurement. In our experiments, we use PR(precision-recall) curve \nto demonstrate the efficacy of our algorithm. Recall that precision = TP/(TP + FP) and recall = TP/(TP + FN). \nFirst off, let us expound the definition of true positives, false positives, and false negatives. We say a prediction \n(c\", ùëõ\"\nWWWW‚Éó) is within the correctness threshold of a ground-truth symmetry plane (c-,ùëõ-\nWWWW‚Éó) if and only if two planes are \nclose enough: (c- ‚àíc\")ùëõ-\nWWWW‚Éó ‚â§d56 and have similar directions: ‚à†(ùëõ\"\nWWWW‚Éó, ùëõ-\nWWWW‚Éó) ‚â§angle56. The variable here for the PR \ncurve is the confidence threshold, separating open predictions (those above) and suppressed predictions (those under).  \nTrue positives(TP) are those predictions within the correctness threshold and are confident enough (above the \nconfidence threshold).  False positives(FP) are predictions out of the correctness threshold but still have higher \nconfidence than the confidence threshold. As for predictions within the correctness threshold but below the confidence \nthreshold, they are regarded as false negatives(FN).  If there is a TP for a ground truth symmetry plane, then other \npredictions are ignored and are not considered as either FP or FN. \nWith the above definition in mind, we are going to break down accuracy performance according to two standards: \nintersection circumference and degree of occlusion. The degree of occlusion is the percentage of the occluded surface \nas opposed to the entire surface, including self-occlusion. \n \nFig. 2. PR curve comparison between ours(top) and Ecins5(bottom) under different intersection circumferences \n \nIn Fig. 2, we show how our algorithm fares compared to Ecins5 in different intersection circumferences. The \nthresholds are set as d56 = 0.01,0.02, ‚Ä¶ , 0.05 and angle56 = 20¬∞. Intersection circumferences refer to the intersection \nbetween the symmetry plane and the object surface, i.e., the line on the surface that cut the object in half. Intuitively, \nwe find for Ecins5 , which based on existing pixel correspondence, longer intersection circumferences mean we are \nlikely able to find more correspondence, as shown in the bottom line in Fig. 4. That may explain why Ecins5 has a \nsudden boost in performance in cases when intersection circumferences are longer than 15cm. As for cases where \nintersection circumferences are shorter than 15cm, the accuracy of Ecins5 suffers considerably as opposed to our \nmethod. \n6 \n \nThe high accuracy of our algorithm could be led by the fact that in heavy occlusion, Ecins5, which heavily relies \non geometry information, is more susceptible to shape changes. Our algorithm, using deep learning, has acquired the \ncapability to distinguish symmetry planes from the clutter. \n \nFig. 3. PR curve comparison between ours(top) and Ecins5(bottom) under different degrees of occlusion \n \nFig. 3 offers us another angle to compare our algorithm and Ecins. Under whatever degrees of occlusion, our \nalgorithm surpasses Ecins in both recall rate and precision. Especially when it comes to large occlusions, our algorithm \nshows persistent accuracy and robustness in detecting symmetries from tiny unoccluded pieces. Thus, it is proper to \nsay that deep learning could serve as a great aid in detecting symmetries in occluded scenarios. \nTable 1. Precision boost when dist56 = 0.01. (Values are arg max (prec √ó recall)) \nDegree of Occ. \nPrecision(Ecins) \nPrecision(Ours) \nRecall(Ecins) \nRecall(Ours) \nP √ó R(Ecins) \nP √ó R(Ours) \nUnder 60% \n0.16 \n0.91 \n0.08 \n0.38 \n0.013 \n0.346 \n60% to 80% \n0.09 \n0.80 \n0.04 \n0.32 \n0.004 \n0.256 \nAbove 80% \n0.06 \n0.86 \n0.04 \n0.60 \n0.002 \n0.516 \n \nIn Table 1, we see how our algorithm performs under dist56 = 0.01 and angle56 = 20¬∞. It concretely demonstrates \nthat our algorithm has gained robustness towards reflective symmetry detection in occlusion in all categories. \n4.2. Prediction Visualization \nNow let us see how well our algorithm has dealt with each degree of occlusion split: under 60%, from 60% to 80% \nand above 80%. \n \n \n7 \n \nFig. 4. Visualization of our predictions under different degrees of occlusion (Better viewed in PDF and zoomed in) \n \n To get a clearer view of what happens in prediction, we need to take a look at Fig. 4. As we explained above, an \nRGB-D image is first segmented from a YCB-video frame and projected into 3D space as occluded point clouds. \nThrough our later deep-learning framework, we are able to predict normal vectors for those on-plane points. Those \nhighlighted points on the leftmost column are normal vector predictions by our algorithm. We have seen highlighted \npoints align with ground truth planes, and the color of those points, normal vectors indicating different directions, is \nfollowing unanimous predictions in each direction. \nIt is worth noted that for those symmetry planes with longer intersection circumferences like the bottom row, it is \nmore likely the points on the symmetry plane are missing out, causing lower accuracy, as we demonstrated in Figure \n2. \n5. Conclusion \nIn this paper, we proposed a deep-learning framework to settle symmetry detection in a 3D scenario, specifically \non point clouds. Objects may be cluttered, some heavily occluded, which are quite tricky for correspondence-based \nalgorithms like Ecins5. Our framework has been proven by far superior to Ecins. When we divide the YCB-video \ndataset by degrees of occlusion, we found our framework excels both in recall rate and precision rate, meaning we \ncan retrieve more symmetry planes and more accurately. \nAs far as we know, we are the first to leverage deep-learning techniques to tackle reflective symmetry on occluded \n3D objects. Although our algorithm does require that at least part of the symmetry plane not occluded, it could inspire \nfellow researchers to drive toward a path where more robust and efficient deep-learning algorithms lie. \nReferences \n. \n1. H. Zabrodsky, S. Peleg, and D. Avnir, ‚ÄúSymmetry as a continuous feature,‚Äù IEEE Transactions on Pattern Analysis and Machine \nIntelligence, vol. 17, no. 12, pp. 1154‚Äì1166, 1995.  \n8 \n \n. 2. D. Raviv, A. M. Bronstein, M. M. Bronstein, and R. Kimmel, ‚ÄúFull and partial symmetries of non-rigid shapes,‚Äù International journal of \ncomputer vision, vol. 89, no. 1, pp. 18‚Äì39, 2010.  \n. \n3. M. Kazhdan, B. Chazelle, D. Dobkin, T. Funkhouser, and S. Rusinkiewicz, ‚ÄúA reflective symmetry descriptor for 3d models,‚Äù \nAlgorithmica, vol. 38, no. 1, pp. 201‚Äì225, 2004.  \n. 4. J. Podolak, P. Shilane, A. Golovinskiy, S. Rusinkiewicz, and T. Funkhouser, ‚ÄúA planar-reflective symmetry transform for 3d shapes,‚Äù ACM \nTransactions on Graphics (TOG), vol. 25, no. 3, pp. 549‚Äì559, 2006.  \n. \n5. Ecins, A., Fermuller, C., & Aloimonos, Y. (2017). Detecting reflectional symmetries in 3d data through symmetrical fitting. In Proceedings \nof the IEEE International Conference on Computer Vision (pp. 1779-1783). \n. \n6. Mitra, Niloy J., Leonidas J. Guibas, and Mark Pauly. \"Partial and approximate symmetry detection for 3D geometry.\" ACM Transactions \non Graphics (TOG). Vol. 25. No. 3. ACM, 2006. \n. \n7. arXiv:1910.06511 [cs.GR] PRS-Net: Planar Reflective Symmetry Detection Net for 3D Models \n. \n8. Wang, C., Xu, D., Zhu, Y., Mart√≠n-Mart√≠n, R., Lu, C., Fei-Fei, L., & Savarese, S. (2019). Densefusion: 6d object pose estimation by \niterative dense fusion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3343-3352). \n. \n9. Hornung, A., Wurm, K. M., Bennewitz, M., Stachniss, C., & Burgard, W. (2013). OctoMap: An efficient probabilistic 3D mapping \nframework based on octrees. Autonomous robots, 34(3), 189-206. \n. \n10. Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. \nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 652-660). \n",
  "categories": [
    "cs.CV",
    "cs.GR"
  ],
  "published": "2020-03-14",
  "updated": "2020-03-14"
}