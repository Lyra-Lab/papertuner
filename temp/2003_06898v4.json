{
  "id": "http://arxiv.org/abs/2003.06898v4",
  "title": "Provably Efficient Exploration for Reinforcement Learning Using Unsupervised Learning",
  "authors": [
    "Fei Feng",
    "Ruosong Wang",
    "Wotao Yin",
    "Simon S. Du",
    "Lin F. Yang"
  ],
  "abstract": "Motivated by the prevailing paradigm of using unsupervised learning for\nefficient exploration in reinforcement learning (RL) problems\n[tang2017exploration,bellemare2016unifying], we investigate when this paradigm\nis provably efficient. We study episodic Markov decision processes with rich\nobservations generated from a small number of latent states. We present a\ngeneral algorithmic framework that is built upon two components: an\nunsupervised learning algorithm and a no-regret tabular RL algorithm.\nTheoretically, we prove that as long as the unsupervised learning algorithm\nenjoys a polynomial sample complexity guarantee, we can find a near-optimal\npolicy with sample complexity polynomial in the number of latent states, which\nis significantly smaller than the number of observations. Empirically, we\ninstantiate our framework on a class of hard exploration problems to\ndemonstrate the practicality of our theory.",
  "text": "Provably Efﬁcient Exploration for Reinforcement\nLearning Using Unsupervised Learning ∗\nFei Feng\nUniversity of California, Los Angeles\nfei.feng@math.ucla.edu\nRuosong Wang\nCarnegie Mellon University\nruosongw@andrew.cmu.edu\nWotao Yin\nUniversity of California, Los Angeles\nwotaoyin@math.ucla.edu\nSimon S. Du\nUniversity of Washington\nssdu@cs.washington.edu\nLin F. Yang\nUniversity of California, Los Angeles\nlinyang@ee.ucla.edu\nAbstract\nMotivated by the prevailing paradigm of using unsupervised learning for efﬁcient\nexploration in reinforcement learning (RL) problems (Tang et al., 2017; Bellemare\net al., 2016), we investigate when this paradigm is provably efﬁcient. We study\nepisodic Markov decision processes with rich observations generated from a small\nnumber of latent states. We present a general algorithmic framework that is built\nupon two components: an unsupervised learning algorithm and a no-regret tabular\nRL algorithm. Theoretically, we prove that as long as the unsupervised learning\nalgorithm enjoys a polynomial sample complexity guarantee, we can ﬁnd a near-\noptimal policy with sample complexity polynomial in the number of latent states,\nwhich is signiﬁcantly smaller than the number of observations. Empirically, we\ninstantiate our framework on a class of hard exploration problems to demonstrate\nthe practicality of our theory.\n1\nIntroduction\nReinforcement learning (RL) is the framework of learning to control an unknown system through trial\nand error. It takes as inputs the observations of the environment and outputs a policy, i.e., a mapping\nfrom observations to actions, to maximize the cumulative rewards. To learn a near-optimal policy, it\nis critical to sufﬁciently explore the environment and identify all the opportunities for high rewards.\nHowever, modern RL applications often need to deal with huge observation spaces such as those\nconsist of images or texts, which makes it challenging or impossible (if there are inﬁnitely many\nobservations) to fully explore the environment in a direct way. In some work, function approximation\nscheme is adopted such that essential quantities for policy improvement, e.g. state-action values,\ncan be generalized from limited observed data to the whole observation space. However, the use of\nfunction approximation alone does not resolve the exploration problem (Du et al., 2020a).\nTo tackle this issue, multiple empirically successful strategies are developed (Tang et al., 2017;\nBellemare et al., 2016; Pathak et al., 2017; Azizzadenesheli et al., 2018; Lipton et al., 2018; Fortunato\net al., 2018; Osband et al., 2016). Particularly, in Tang et al. (2017) and Bellemare et al. (2016), the\n∗Correspondence to: Simon S. Du <ssdu@cs.washington.edu>, Lin F. Yang <linyang@ee.ucla.edu>\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2003.06898v4  [cs.LG]  1 Dec 2020\nauthors use state abstraction technique to reduce the problem size. They construct a mapping from\nobservations to a small number of hidden states and devise exploration on top of the latent state space\nrather than the original observation space.\nTo construct such a state abstraction mapping, practitioners often use unsupervised learning. The\nprocedure has the following steps: collect a batch of observation data, apply unsupervised learning to\nbuild a mapping, use the mapping to guide exploration and collect more data, and repeat. Empirical\nstudy evidences the effectiveness of such an approach at addressing hard exploration problems (e.g.,\nthe infamous Montezuma’s Revenge). However, it has not been theoretically justiﬁed. In this paper,\nwe aim to answer this question:\nIs exploration driven by unsupervised learning in general provably efﬁcient?\nThe generality includes the choice of unsupervised learning algorithms, reinforcement learning\nalgorithms, and the condition of the problem structure.\nWe ﬁrst review some existing theoretical results on provably efﬁcient exploration. More discussion\nabout related work is deferred to appendix. For an RL problem with ﬁnitely many states, there are\nmany algorithms with a tabular implementation that learn to control efﬁciently. These algorithms can\nlearn a near-optimal policy using a number of samples polynomially depending on the size of the\nstate space. However, if we directly apply these algorithms to rich observations cases by treating each\nobservation as a state, the sample complexities are polynomial in the cardinality of the observation\nspace. Such a dependency is unavoidable without additional structural assumptions (Jaksch et al.,\n2010). If structural conditions are considered, for example, observations are generated from a\nsmall number of latent states (Krishnamurthy et al., 2016; Jiang et al., 2017; Dann et al., 2018;\nDu et al., 2019a), then the sample complexity only scales polynomially with the number of hidden\nstates. Unfortunately, the correctness of these algorithms often requires strict assumptions (e.g.,\ndeterministic transitions, reachability) that may not be satisﬁed in many real applications.\nOur Contributions\nIn this paper we study RL problems with rich observations generated from\na small number of latent states for which an unsupervised learning subroutine is used to guide\nexploration. We summarize our contributions below.\n• We propose a new algorithmic framework for the Block Markov Decision Process (BMDP)\nmodel (Du et al., 2019a). We combine an unsupervised learning oracle and a tabular RL algorithm\nin an organic way to ﬁnd a near-optimal policy for a BMDP. The unsupervised learning oracle\nis an abstraction of methods used in Tang et al. (2017); Bellemare et al. (2016) and widely used\nstatistical generative models. Notably, our framework can take almost any unsupervised learning\nalgorithms and tabular RL algorithms as subroutines.\n• Theoretically, we prove that as long as the unsupervised learning oracle and the tabular RL algorithm\neach has a polynomial sample complexity guarantee, our framework ﬁnds a near-optimal policy\nwith sample complexity polynomial in the number of latent states, which is signiﬁcantly smaller\nthan the number of possible observations (cf. Theorem 1). To our knowledge, this is the ﬁrst\nprovably efﬁcient method for RL problems with huge observation spaces that uses unsupervised\nlearning for exploration. Furthermore, our result does not require additional assumptions on\ntransition dynamics as used in Du et al. (2019a). Our result theoretically sheds light on the success\nof the empirical paradigms used in Tang et al. (2017); Bellemare et al. (2016).\n• We instantiate our framework with particular unsupervised learning algorithms and tabular RL\nalgorithms on hard exploration environments with rich observations studied in Du et al. (2019a),\nand compare with other methods tested in Du et al. (2019a). Our experiments demonstrate our\nmethod can signiﬁcantly outperform existing methods on these environments.\nMain Challenge and Our Technique\nWe assume there is an unsupervised learning oracle (see\nformal deﬁnition in Section 4) which can be applied to learn decoding functions and the accuracy\nof learning increases as more training data are fed. The unsupervised learning algorithm can only\nguarantee good performance with respect to the input distribution that generates the training data.\nUnlike standard unsupervised learning where the input distribution is ﬁxed, in our problem, the input\ndistribution depends on our policy. On the other hand, the quality of a policy depends on whether the\nunsupervised learning oracle has (approximately) decoded the latent states. This interdependency is\nthe main challenge we need to tackle in our algorithm design and analysis.\n2\nHere we brieﬂy explain our framework. Let M be the MDP with rich observations. We form an\nauxiliary MDP M′ whose state space is the latent state space of M. Our idea is to simulate the\nprocess of running a no-regret tabular RL algorithm A directly on M′. For each episode, A proposes\na policy π for M′ and expects a trajectory of running π on M′ for updating and then proceeds. To\nobtain such a trajectory, we design a policy φ for M as a composite of π and some initial decoding\nfunctions. We run φ on M to collect observation trajectories. Although the decoding functions may\nbe inaccurate initially, they can still help us collect observation samples for later reﬁnement. After\ncollecting sufﬁcient observations, we apply the unsupervised learning oracle to retrain decoding\nfunctions and then update φ as a composite of π and the newly-learned functions and repeat running\nφ on M. After a number of iterations (proportional to the size of the latent state space), with the\naccumulation of training data, decoding functions are trained to be fairly accurate on recovering\nlatent states, especially those π has large probabilities to visit. This implies that running the latest φ\non M is almost equivalent to running π on M′. Therefore, we can obtain a state-action trajectory\nwith high accuracy as the algorithm A requires. Since A is guaranteed to output a near-optimal\npolicy after a polynomial (in the size of the true state-space) number of episodes, our algorithm uses\npolynomial number of samples as well.\n2\nRelated Work\nIn this section, we review related provably efﬁcient RL algorithms. We remark that we focus on\nenvironments that require explicit exploration. With certain assumptions of the environment, e.g., the\nexistence of a good exploration policy or the distribution over the initial state is sufﬁciently diverse,\none does not need to explicitly explore (Munos, 2005; Antos et al., 2008; Geist et al., 2019; Kakade\nand Langford, 2002; Bagnell et al., 2004; Scherrer and Geist, 2014; Agarwal et al., 2019; Yang et al.,\n2019; Chen and Jiang, 2019). Without these assumptions, the problem can require an exponential\nnumber of samples, especially for policy-based methods (Du et al., 2020a).\nExploration is needed even in the most basic tabular setting. There is a substantial body of work\non provably efﬁcient tabular RL (Agrawal and Jia, 2017; Jaksch et al., 2010; Kakade et al., 2018;\nAzar et al., 2017; Kearns and Singh, 2002; Dann et al., 2017; Strehl et al., 2006; Jin et al., 2018;\nSimchowitz and Jamieson, 2019; Zanette and Brunskill, 2019). A common strategy is to use UCB\nbonus to encourage exploration in less-visited states and actions. One can also study RL in metric\nspaces (Pazis and Parr, 2013; Song and Sun, 2019; Ni et al., 2019). However, in general, this type of\nalgorithms has an exponential dependence on the state dimension.\nTo deal with huge observation spaces, one might use function approximation. Wen and Van Roy\n(2013) proposed an algorithm, optimistic constraint propagation (OCP), which enjoys polynomial\nsample complexity bounds for a family of Q-function classes, including the linear function class\nas a special case. But their algorithm can only handle deterministic systems, i.e., both transition\ndynamics and rewards are deterministic. The setting is recently generalized by Du et al. (2019b) to\nenvironments with low variance and by Du et al. (2020b) to the agnostic setting. Li et al. (2011)\nproposed a Q-learning algorithm which requires the Know-What-It-Knows oracle. But it is in general\nunknown how to implement such an oracle.\nOur work is closely related to a sequence of works which assumes the transition has certain low-rank\nstructure (Krishnamurthy et al., 2016; Jiang et al., 2017; Dann et al., 2018; Sun et al., 2019; Du\net al., 2019a; Jin et al., 2019; Yang and Wang, 2019). The most related paper is Du et al. (2019a)\nwhich also builds a state abstraction map. Their sample complexity depends on two quantities of\nthe transition probability of the hidden states: identiﬁability and reachability, which may not be\nsatisﬁed in many scenarios. Identiﬁability assumption requires that the L1 distance between the\nposterior distributions (of previous level’s hidden state, action pair) given any two different hidden\nstates is strictly larger than some constant (Assumption 3.2 in Du et al. (2019a)). This is an inherent\nnecessary assumption for the method in Du et al. (2019a) as they need to use the posterior distribution\nto distinguish hidden states. Reachability assumption requires that there exists a constant such that\nfor every hidden state, there exists a policy that reaches the hidden state with probability larger than\nthis constant (Deﬁnition 2.1 in Du et al. (2019a)). Conceptually, this assumption is not needed for\nﬁnding a near-optimal policy because if one hidden state has negligible reaching probability, one\ncan just ignore it. Nevertheless, in Du et al. (2019a), the reachability assumption is also tied with\nbuilding the abstraction map. Therefore, it may not be removable if one uses the strategy in Du et al.\n3\n(2019a). In this paper, we show that given an unsupervised learning oracle, one does not need the\nidentiﬁability and reachability assumptions for efﬁcient exploration.\n3\nPreliminaries\nNotations\nGiven a set A, we denote by |A| the cardinality of A, P(A) the set of all probability dis-\ntributions over A, and Unif(A) the uniform distribution over A. We use [h] for the set {1, 2, . . . , h}\nand f[h] for the set of functions {f1, f2, . . . , fh}. Given two functions f : X →Y and g : Y →Z,\ntheir composite is denoted as g ◦f : X →Z.\nBlock Markov Decision Process\nWe consider a Block Markov Decision Process (BMDP),\nwhich is ﬁrst formally introduced in Du et al. (2019a).\nA BMDP is described by a tuple\nM := (S, A, X, P, r, f[H+1], H). S is a ﬁnite unobservable latent state space, A is a ﬁnite ac-\ntion space, and X is a possibly inﬁnite observable context space. X can be partitioned into |S|\ndisjoint blocks {Xs}s∈S, where each block Xs corresponds to a unique state s. P is the collection\nof the state-transition probability p[H](s′|s, a) and the context-emission distribution q(x|s) for all\ns, s′ ∈S, a ∈A, x ∈X. r : [H] × S × A →[0, 1] is the reward function. f[H+1] is the set of\ndecoding functions, where fh maps every observation at level h to its true latent state. Finally, H is\nthe length of horizon. When X = S, this is the usual MDP setting.\nFor each episode, the agent starts at level 1 with the initial state s1 and takes H steps to the ﬁnal level\nH + 1. We denote by Sh and Xh the set of possible states and observations at level h ∈[H + 1],\nrespectively. At each level h ∈[H + 1], the agent has no access to the true latent state sh ∈Sh but an\nobservation xh ∼q(·|sh). An action ah is then selected following some policy φ : [H]×X →P(A).\nAs a result, the environment evolves into a new state sh+1 ∼ph(·|sh, ah) and the agent receives an\ninstant reward r(h, sh, ah). A trajectory has such a form: {s1, x1, a1, . . . , sH, xH, aH, sH+1, xH+1},\nwhere all state components are unknown.\nPolicy\nGiven a BMDP M := (S, A, X, P, r, f[H+1], H), there is a corresponding MDP M′ :=\n(S, A, P, r, H), which we refer to as the underlying MDP in the later context. A policy on M has\na form φ : [H] × X →P(A) and a policy on M′ has a form π : [H] × S →P(A). Given a\npolicy π on M′ and a set of functions ˆf[H+1] where ˆfh : Xh →Sh, ∀h ∈[H + 1], we can induce\na policy on M as π ◦ˆf[H+1] =: φ such that φ(h, xh) = π(h, ˆfh(xh)), ∀xh ∈Xh, h ∈[H]. If\nˆf[H+1] = f[H+1], then π and φ are equivalent in the sense that they induce the same probability\nmeasure over the state-action trajectory space.\nGiven\nan\nMDP,\nthe\nvalue\nof\na\npolicy\nπ\n(starting\nfrom\ns1)\nis\ndeﬁned\nas\nV π\n1\n=\nEπ hPH\nh=1 r(h, sh, ah)\n\f\f\fs1\ni\n,\nA policy that has the maximal value is an optimal pol-\nicy and the optimal value is denoted by V ∗\n1 , i.e., V ∗\n1 = maxπ V π\n1 . Given ε > 0, we say π is ε-optimal\nif V ∗\n1 −V π\n1 ≤ε. Similarly, given a BMDP, we deﬁne the value of a policy φ (starting from s1) as:\nV φ\n1 = Eφ hPH\nh=1 r(h, sh, ah)\n\f\f\fs1\ni\n, The notion of optimallity and ε-optimality are similar to MDP.\n4\nA Uniﬁed Framework for Unsupervised Reinforcement Learning\n4.1\nUnsupervised Learning Oracle and No-regret Tabular RL Algorithm\nIn this paper, we consider RL on a BMDP. The goal is to ﬁnd a near-optimal policy with sample\ncomplexity polynomial to the cardinality of the latent state space. We assume no knowledge of P,\nr, and f[H+1], but the access to an unsupervised learning oracle ULO and an (ε, δ)-correct episodic\nno-regret algorithm. We give the deﬁnitions below.\nDeﬁnition 1 (Unsupervised Learning Oracle ULO). There exists a function g(n, δ) such that\nfor any ﬁxed δ > 0, limn→∞g(n, δ) = 0. Given a distribution µ over S, and n samples from\nP\ns∈S q(·|s)µ(s), with probability at least 1 −δ, we can ﬁnd a function ˆf : X →S such that\nPs∼µ,x∼q(·|s)\n\u0000 ˆf(x) = α(s)\n\u0001\n≥1 −g(n, δ)\nfor some unknown permutation α : S →S.\n4\nAlgorithm 1 A Uniﬁed Framework for Unsupervised RL\n1: Input: BMDP M; ULO; (ε, δ)-correct episodic no-regret algorithm A ; batch size B > 0;\nε ∈(0, 1); δ ∈(0, 1); N := ⌈log(2/δ)/2⌉; L := ⌈9H2/(2ε2) log(2N/δ)⌉.\n2: for n = 1 to N do\n3:\nClear the memory of A and restart;\n4:\nfor episode k = 1 to K do\n5:\nObtain πk from A ;\n6:\nObtain a trajectory: τ k, f k\n[H+1] ←TSR(ULO, πk, B);\n7:\nUpdate the algorithm: A ←τ k;\n8:\nend for\n9:\nObtain πK+1 from A ;\n10:\nFinalize the decoding functions: τ K+1, f K+1\n[H+1] ←TSR(ULO, πK+1, B);\n11:\nConstruct a policy for M : φn ←πK+1 ◦f K+1\n[H+1].\n12: end for\n13: Run each φn (n ∈[N]) for L episodes and get the average rewards per episode ¯V φn\n1\n.\n14: Output a policy φ ∈argmaxφ∈φ[N] ¯V φ\n1 .\nIn Deﬁnition 1, suppose f is the true decoding function, i.e., Ps∼µ,x∼q(·|s)\n\u0000f(x) = s\n\u0001\n= 1. We\nrefer to the permutation α as a good permutation between f and ˆf. Given g(n, δ), we deﬁne\ng−1(ϵ, δ) := min{N | for all n > N, g(n, δ) < ϵ}. Since limn→∞g(n, δ) = 0, g−1(ϵ, δ) is\nwell-deﬁned. We assume that g−1(ϵ, δ) is a polynomial in terms of 1/ϵ, log(δ−1) and possibly\nproblem-dependent parameters.\nThis deﬁnition is motivated by Tang et al. (2017) in which authors use auto-encoder and\nSimHash (Charikar, 2002) to construct the decoding function and they use this UCB-based ap-\nproach on top of the decoding function to guide exploration. It is still an open problem to obtain a\nsample complexity analysis for auto-encoder. Let alone the composition with SimHash. Nevertheless,\nin Appendix B, we give several examples of ULO with theoretical guarantees. Furthermore, once we\nhave an analysis of auto-encoder and we can plug-in that into our framework effortlessly.\nDeﬁnition 2 ((ε, δ)-correct No-regret Algorithm). Let ε > 0 and δ > 0. A is an (ε, δ)-correct\nno-regret algorithm if for any MDP M′ := (S, A, P, r, H) with the initial state s1, A\n• runs for at most poly(|S|, |A|, H, 1/ε, log(1/δ)) episodes (the sample complexity of A );\n• proposes a policy πk at the beginning of episode k and collects a trajectory of M′ following πk;\n• outputs a policy π at the end such that with probability at least 1 −δ, π is ε-optimal.\nDeﬁnition 2 simply describes tabular RL algorithms that have polynomial sample complexity guaran-\ntees for episodic MDPs. Instances are vivid in literature (see Section 2).\n4.2\nA Uniﬁed Framework\nWith a ULO and an (ε, δ)-correct no-regret algorithm A , we propose a uniﬁed framework in\nAlgorithm 1. Note that we use ULO as a black-box oracle for abstraction and generality. For each\nepisode, we combine the policy π proposed by A for the underlying MDP together with certain\ndecoding functions ˆf[H+1]2 to generate a policy π◦ˆf[H+1] for the BMDP. Then we collect observation\nsamples using π ◦ˆf[H+1] and all previously generated policies over the BMDP. As more samples\nare collected, we reﬁne the decoding functions via ULO. Once the sample number is enough, a\ntrajectory of π ◦ˆf[H+1] is as if obtained using the true decoding functions (up to some permutations).\nTherefore, we successfully simulate the process of running π directly on the underlying MDP. We\nthen proceed to the next episode with the latest decoding functions and repeat the above procedure\n2For the convenience of analysis, we learn decoding functions for each level separately. In practice,\nobservation data can be mixed up among all levels and used to train one decoding function for all levels.\n5\nAlgorithm 2 Trajectory Sampling Routine TSR (ULO, π, B)\n1: Input: ULO; a policy π : [H] × S →A; episode index k; batch size B > 0; ϵ ∈(0, 1);\nδ1 ∈(0, 1); J := (H + 1)|S| + 1.\n2: Data:\n• a policy set Π;\n• label standard data Z := {Z1, Z2, . . . ZH+1}, Zh := {Dh,s1, Dh,s2, . . .};\n• present decoding functions f 0\n[H+1];\n3: for i = 1 to J do\n4:\nCombine policy: Π ←Π ∪{π ◦f i−1\n[H+1]};\n5:\nGenerate ((k −1)J + i) · B trajectories of training data D with Unif(Π);\n6:\nGenerate B trajectories of testing data D′′ with π ◦f i−1\n[H+1].\n7:\nTrain with ULO: ˜f i\n[H+1] ←ULO(D);\n8:\nMatch labels: f i\n[H+1] ←FixLabel( ˜f i\n[H+1], Z);\n9:\nfor h ∈[H + 1] do\n10:\nLet D′′\nh,s := {x ∈D′′\nh : f i\nh(x) = s, s ∈Sh};\n11:\nUpdate label standard set: if Dh,s ̸∈Zh and |D′′\nh,s| ≥3ϵ · B log(δ−1\n1 ), then let Zh ←\nZh ∪{D′′\nh,s}\n12:\nend for\n13: end for\n14: Run π ◦f J\n[H+1] to obtain a trajectory τ;\n15: Renew f 0\n[H+1] ←f J\n[H+1];\n16: Output: τ, f J\n[H+1].\nAlgorithm 3 FixLabel( ˜f[H+1], Z)\n1: Input: decoding functions ˜f[H+1]; a set of label standard data Z := {Z1, Z2, · · · , ZH+1},\nZh := {Dh,s1, Dh,s2, . . .}.\n2: for every Dh,s in Z do\n3:\nif s ∈Sh and |{x ∈Dh,s : ˜fh(x) = s′}| > 3/5|Dh,s| then\n4:\nSwap the output of s′ with s in ˜fh;\n5:\nend if\n6: end for\n7: Output: ˜f[H+1]\nuntil A halts. Note that this procedure is essentially what practitioners use in Tang et al. (2017);\nBellemare et al. (2016) as we have discussed in the beginning.\nWe now describe our algorithm in more detail. Suppose the algorithm A runs for K episodes. At the\nbeginning of each episode k ∈[K], A proposes a policy πk : [H] × S →A for the underlying MDP.\nThen we use the Trajectory Sampling Routine (TSR) to generate a trajectory τ k using πk and then\nfeed τ k to A . After K episodes, we obtain a policy πK+1 from A and a set of decoding functions\nf K+1\n[H+1] from TSR. We then construct a policy for the BMDP as πK+1 ◦f K+1\n[H+1]. We repeat this process\nfor N times for making sure our algorithm succeeds with high probability.\nThe detailed description of TSR is displayed in Algorithm 2. We here brieﬂy explain the idea. To\ndistinguish between episodes, with input policy πk (Line 6 Algorithm 1), we add the episode index\nk as superscripts to π and f[H+1] in TSR. We maintain a policy set in memory and initialize it as\nan empty set at the beginning of Algorithm 1. Note that, at each episode, our goal is to simulate a\ntrajectory of π running on the underlying MDP. TSR achieves this in an iterative fashion: it starts with\n6\nFigure 1: Performances for LockBernoulli. All lines are mean values of 50 tests and the shaded areas\ndepict the one standard deviations.\nthe input policy πk and the latest-learned decoding functions f k,0\n[H+1] := f k−1,J\n[H+1] ; for each iteration\ni, it ﬁrst adds the policy πk ◦f k,i−1\n[H+1] in Π and then plays Unif(Π) to collect a set of observation\ntrajectories (i.e., each trajectory is generated by ﬁrst uniformly randomly selecting a policy from\nΠ and then running it in the BMDP);3 then updates f k,i−1\n[H+1] to ˜f k,i\n[H+1] by running ULO on these\ncollected observations. Note that ULO may output labels inconsistent with previously trained\ndecoding functions. We further match labels of ˜f k,i\n[H+1] with the former ones by calling the FixLabel\nroutine (Algorithm 3). To accomplish the label matching process, we cache a set Z in memory which\nstores observation examples Dh,s for each state s and each level h. Z is initialized as an empty set and\ngradually grows. Whenever we conﬁrm a new label, we add the corresponding observation examples\nto Z (Line 11 Algorithm 2). Then for later learned decoding functions, they can use this standard set\nto correspondingly swap their labels and match with previous functions. After the matching step, we\nget f k,i\n[H+1]. Continuously running for J iterations, we stop and use πk ◦f k,J\n[H+1] to obtain a trajectory.\nWe now present our main theoretical result.\nTheorem 1. Suppose in Deﬁnition 1, g−1(ϵ, δ1) = poly(|S|, 1/ϵ, log(δ−1\n1 )) for any ϵ, δ1 ∈(0, 1)\nand A is (ε, δ2)-correct with sample complexity poly\n\u0000|S|, |A|, H, 1/ε, log\n\u0000δ−1\n2\n\u0001\u0001\nfor any ε, δ2 ∈\n(0, 1). Then Algorithm 1 outputs a policy φ such that with probability at least 1 −δ, φ is an ε-optimal\npolicy for the BMDP, using at most poly\n\u0000|S|, |A|, H, 1/ε, log(δ−1)\n\u0001\ntrajectories.\nWe defer the proof to Appendix A. Theorem 1 formally justiﬁes what we claimed in Section 1 that as\nlong as the sample complexity of ULO is polynomial and A is a no-regret tabular RL algorithm,\npolynomial number of trajectories sufﬁces to ﬁnd a near-optimal policy. To our knowledge, this is\nthe ﬁrst result that proves unsupervised learning can guide exploration in RL problems with a huge\nobservation space.\n7\nFigure 2: Performances for LockGaussian, σ = 0.1. All lines are mean values of 50 tests and the\nshaded areas depict the one standard deviations. OracleQ-lat and QLearning-lat have direct access to\nthe latent states, which are not for practical use. URL and PCID only have access to the observations.\nOracleQ-obs and QLearning-obs are omitted due to inﬁnitely many observations.\n5\nNumerical Experiments\nIn this section we conduct experiments to demonstrate the effectiveness of our framework. Our code\nis available at https://github.com/FlorenceFeng/StateDecoding.\nEnvironments\nWe conduct experiments in two environments: LockBernoulli and LockGaussian.\nThese environments are also studied in Du et al. (2019a), which are designed to be hard for exploration.\nBoth environments have the same latent state structure with H levels, 3 states per level and 4 actions.\nAt level h, from states s1,h and s2,h one action leads with probability 1 −α to s1,h+1 and with\nprobability α to s2,h+1, another has the ﬂipped behavior, and the remaining two lead to s3,h+1. All\nactions from s3,h lead to s3,h+1. Non-zero reward is only achievable if the agent can reach s1,H+1 or\ns2,H+1 and the reward follows Bernoulli(0.5). Action labels are randomly assigned at the beginning\nof each time of training. We consider three values of α: 0, 0.2, and 0.5.\nIn LockBernoulli, the observation space is {0, 1}H+3 where the ﬁrst 3 coordinates are reserved for\nthe one-hot encoding of the latent state and the last H coordinates are drawn i.i.d from Bernoulli(0.5).\nLockBernoulli meets our requirements as a BMDP. In LockGaussian, the observation space is RH+3.\nEvery observation is constructed by ﬁrst letting the ﬁrst three coordinates be the one-hot encoding of\nthe latent state, then adding i.i.d Gaussian noises N(0, σ2) to all H + 3 coordinates. We consider\nσ = 0.1 and 0.2. LockGaussian is not a BMDP. We use this environment to evaluate the robustness\nof our method to violated assumptions.\nThe environments are designed to be hard for exploration. There are in total 4H choices of actions of\none episode, but only 2H of them lead to non-zero reward in the end. So random exploration requires\nexponentially many trajectories. Also, with a larger H, the difﬁculty of learning accurate decoding\nfunctions increases and makes exploration with observations a more challenging task.\n3This resampling over all previous policies is mainly for the convenience of analysis. It can be replaced using\nprevious data but requires more reﬁned analysis.\n8\nFigure 3: Performances for LockGaussian, σ = 0.2. All lines are mean values of 50 tests and the\nshaded areas depict the one standard deviations. OracleQ-lat and QLearning-lat have direct access to\nthe latent states, which are not for practical use. URL and PCID only have access to the observations.\nOracleQ-obs and QLearning-obs are omitted due to inﬁnitely many observations.\nAlgorithms and Hyperparameters\nWe compare 4 algorithms: OracleQ (Jin et al., 2019); QLearn-\ning, the tabular Q-Learning with ϵ-greedy exploration; URL, our method; and PCID (Du et al., 2019a).\nFor OracleQ and QLearning, there are two implementations: 1. they directly see the latent states\n(OracleQ-lat and QLearning-lat); 2. only see observations (OracleQ-obs and QLearning-obs). For\nURL and PCID, only observations are available. OracleQ-lat and QLearning-lat are served as a near-\noptimal skyline and a sanity-check baseline to measure the efﬁciency of observation-only algorithms.\nOracleQ-obs and QLearning-obs are only tested in LockBernoulli since there are inﬁnitely many\nobservations in LockGaussian. For URL, we use OracleQ as the tabular RL algorithm. Details about\nhyperparameters and unsupervised learning oracles in URL can be found in Appendix C.\nResults\nThe results are presented in Figure 1, 2, and 3. x-axis is the number of training trajectories\nand y-axis is average reward. All lines are mean values of 50 tests and the shaded areas depict the one\nstandard deviations. The title for each subﬁgure records problem parameters and the unsupervised\nlearning method we apply for URL. In LockBernoulli, OracleQ-obs and QLearning-obs are far from\nbeing optimal even for small-horizon cases. URL is mostly as good as the skyline (OracleQ-lat) and\nmuch better than the baseline (QLearning-lat) especially when H = 20. URL outperforms PCID in\nmost cases. When H = 20, we observe a probability of 80% that URL returns near-optimal values\nfor α = 0.2 and 0.5. In LockGaussian, for H = 20, we observe a probability of > 75% that URL\nreturns a near-optimal policy for α = 0.2 and 0.5.\n6\nConclusion\nThe current paper gave a general framework that turns an unsupervised learning algorithm and a\nno-regret tabular RL algorithm into an algorithm for RL problems with huge observation spaces.\nWe provided theoretical analysis to show it is provably efﬁcient. We also conducted numerical\nexperiments to show the effectiveness of our framework in practice. An interesting future theoretical\ndirection is to characterize the optimal sample complexity under our assumptions.\n9\nBroader Impact\nOur research broadens our understanding on the use of unsupervised learning for RL, which in turn\ncan help researchers design new principled algorithms and incorporate safety considerations for more\ncomplicated problems.\nWe do not believe that the results in this work will cause any ethical issue, or put anyone at a\ndisadvantage in the society.\nAcknowledgments and Disclosure of Funding\nFei Feng was supported by AFOSR MURI FA9550-18-10502 and ONR N0001417121. This work\nwas done while Simon S. Du was at the Institute for Advanced Study and he was supported by NSF\ngrant DMS-1638352 and the Infosys Membership.\nReferences\nAchlioptas, D. and McSherry, F. (2005). On spectral learning of mixtures of distributions. In\nInternational Conference on Computational Learning Theory, pages 458–469. Springer.\nAgarwal, A., Kakade, S. M., Lee, J. D., and Mahajan, G. (2019). Optimality and approximation with\npolicy gradient methods in Markov decision processes. arXiv preprint arXiv:1908.00261.\nAgrawal, S. and Jia, R. (2017). Optimistic posterior sampling for reinforcement learning: Worst-\ncase regret bounds. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems, pages 1184–1194. Curran Associates Inc.\nAntos, A., Szepesvári, C., and Munos, R. (2008). Learning near-optimal policies with Bellman-\nresidual minimization based ﬁtted policy iteration and a single sample path. Machine Learning,\n71(1):89–129.\nArora, S. and Kannan, R. (2001). Learning mixtures of arbitrary Gaussians. In Proceedings of the\nthirty-third annual ACM symposium on Theory of computing, pages 247–257.\nAzar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement learning.\nIn Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages\n263–272. JMLR. org.\nAzizzadenesheli, K., Brunskill, E., and Anandkumar, A. (2018). Efﬁcient exploration through\nBayesian deep Q-networks. In 2018 Information Theory and Applications Workshop (ITA), pages\n1–9.\nBagnell, J. A., Kakade, S. M., Schneider, J. G., and Ng, A. Y. (2004). Policy search by dynamic\nprogramming. In Advances in neural information processing systems, pages 831–838.\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. (2016). Unifying\ncount-based exploration and intrinsic motivation. In Advances in Neural Information Processing\nSystems, pages 1471–1479.\nBontemps, D. and Toussile, W. (2013). Clustering and variable selection for categorical multivariate\ndata. Electronic Journal of Statistics, 7:2344–2371.\nBouguila, N. and Fan, W. (2020). Mixture models and applications. Springer.\nCharikar, M. S. (2002). Similarity estimation techniques from rounding algorithms. In Proceedings\nof the thiry-fourth annual ACM symposium on Theory of computing, pages 380–388.\nChen, J. and Jiang, N. (2019). Information-theoretic considerations in batch reinforcement learning.\nIn International Conference on Machine Learning, pages 1042–1051.\nDahl, D. B. (2006). Model-based clustering for expression data via a Dirichlet process mixture model.\nBayesian inference for gene expression and proteomics, 4:201–218.\n10\nDann, C., Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. (2018). On\noracle-efﬁcient PAC RL with rich observations. In Advances in Neural Information Processing\nSystems, pages 1422–1432.\nDann, C., Lattimore, T., and Brunskill, E. (2017). Unifying PAC and regret: Uniform PAC bounds for\nepisodic reinforcement learning. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, NIPS’17, pages 5717–5727, USA. Curran Associates Inc.\nDasgupta, S. and Schulman, L. J. (2000). A two-round variant of EM for Gaussian mixtures. In\nProceedings of the Sixteenth conference on Uncertainty in artiﬁcial intelligence, pages 152–159.\nDu, S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudik, M., and Langford, J. (2019a). Provably\nefﬁcient RL with rich observations via latent state decoding. In International Conference on\nMachine Learning, pages 1665–1674.\nDu, S. S., Kakade, S. M., Wang, R., and Yang, L. F. (2020a). Is a good representation sufﬁcient for\nsample efﬁcient reinforcement learning? In International Conference on Learning Representations.\nDu, S. S., Lee, J. D., Mahajan, G., and Wang, R. (2020b). Agnostic Q-learning with function approx-\nimation in deterministic systems: Tight bounds on approximation error and sample complexity.\narXiv preprint arXiv:2002.07125.\nDu, S. S., Luo, Y., Wang, R., and Zhang, H. (2019b). Provably efﬁcient Q-learning with function\napproximation via distribution shift error checking oracle. In Advances in Neural Information\nProcessing Systems, pages 8058–8068.\nElhamifar, E. and Vidal, R. (2013). Sparse subspace clustering: Algorithm, theory, and applications.\nIEEE transactions on pattern analysis and machine intelligence, 35(11):2765–2781.\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Hessel, M., Osband, I., Graves, A., Mnih, V., Munos,\nR., Hassabis, D., Pietquin, O., Blundell, C., and Legg, S. (2018). Noisy networks for exploration.\nIn International Conference on Learning Representations.\nGeist, M., Scherrer, B., and Pietquin, O. (2019). A theory of regularized Markov decision processes.\nIn International Conference on Machine Learning, pages 2160–2169.\nJaksch, T., Ortner, R., and Auer, P. (2010). Near-optimal regret bounds for reinforcement learning.\nJournal of Machine Learning Research, 11(Apr):1563–1600.\nJiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. (2017). Contextual deci-\nsion processes with low Bellman rank are PAC-learnable. In Proceedings of the 34th International\nConference on Machine Learning-Volume 70, pages 1704–1713. JMLR. org.\nJin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is Q-learning provably efﬁcient? In\nAdvances in Neural Information Processing Systems, pages 4863–4873.\nJin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2019). Provably efﬁcient reinforcement learning with\nlinear function approximation. arXiv preprint arXiv:1907.05388.\nJuan, A. and Vidal, E. (2002). On the use of Bernoulli mixture models for text classiﬁcation. Pattern\nRecognition, 35(12):2705–2710.\nJuan, A. and Vidal, E. (2004). Bernoulli mixture models for binary images. In Proceedings of the 17th\nInternational Conference on Pattern Recognition, 2004. ICPR 2004., volume 3, pages 367–370.\nIEEE.\nKakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning. In\nICML, volume 2, pages 267–274.\nKakade, S., Wang, M., and Yang, L. F. (2018). Variance reduction methods for sublinear reinforcement\nlearning. arXiv preprint arXiv:1802.09184.\nKearns, M. and Singh, S. (2002). Near-optimal reinforcement learning in polynomial time. Machine\nlearning, 49(2-3):209–232.\n11\nKrishnamurthy, A., Agarwal, A., and Langford, J. (2016). PAC reinforcement learning with rich\nobservations. In Advances in Neural Information Processing Systems, pages 1840–1848.\nLi, J. and Zha, H. (2006). Two-way Poisson mixture models for simultaneous document classiﬁcation\nand word clustering. Computational Statistics & Data Analysis, 50(1):163–180.\nLi, L., Littman, M. L., Walsh, T. J., and Strehl, A. L. (2011). Knows what it knows: A framework for\nself-aware learning. Machine learning, 82(3):399–443.\nLipton, Z. C., Li, X., Gao, J., Li, L., Ahmed, F., and Deng, L. (2018). BBQ-networks: Efﬁcient\nexploration in deep reinforcement learning for task-oriented dialogue systems. In AAAI.\nMcLachlan, G. J. and Basford, K. E. (1988). Mixture models: Inference and applications to clustering,\nvolume 38. M. Dekker New York.\nMcLachlan, G. J. and Peel, D. (2004). Finite mixture models. John Wiley & Sons.\nMunos, R. (2005). Error bounds for approximate value iteration. In Proceedings of the National\nConference on Artiﬁcial Intelligence, volume 20, page 1006. Menlo Park, CA; Cambridge, MA;\nLondon; AAAI Press; MIT Press; 1999.\nNajaﬁ, A., Motahari, S. A., and Rabiee, H. R. (2020). Reliable clustering of Bernoulli mixture models.\nBernoulli, 26(2):1535–1559.\nNi, C., Yang, L. F., and Wang, M. (2019). Learning to control in metric space with optimal regret. In\n2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton),\npages 726–733.\nOsband, I., Van Roy, B., and Wen, Z. (2016). Generalization and exploration via randomized value\nfunctions. In Proceedings of the 33rd International Conference on International Conference on\nMachine Learning - Volume 48, ICML’16, pages 2377–2386. JMLR.org.\nPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven exploration by\nself-supervised prediction. In International Conference on Machine Learning (ICML), volume\n2017.\nPazis, J. and Parr, R. (2013). PAC optimal exploration in continuous space Markov decision processes.\nIn Proceedings of the Twenty-Seventh AAAI Conference on Artiﬁcial Intelligence, AAAI’13, pages\n774–781. AAAI Press.\nRegev, O. and Vijayaraghavan, A. (2017). On learning mixtures of well-separated Gaussians. In 2017\nIEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 85–96. IEEE.\nScherrer, B. and Geist, M. (2014). Local policy search in a convex space and conservative policy\niteration as boosted policy search. In Joint European Conference on Machine Learning and\nKnowledge Discovery in Databases, pages 35–50. Springer.\nSimchowitz, M. and Jamieson, K. G. (2019). Non-asymptotic gap-dependent regret bounds for\ntabular MDPs. In Advances in Neural Information Processing Systems, pages 1151–1160.\nSoltanolkotabi, M., Elhamifar, E., Candes, E. J., et al. (2014). Robust subspace clustering. The\nAnnals of Statistics, 42(2):669–699.\nSong, Z. and Sun, W. (2019). Efﬁcient model-free reinforcement learning in metric spaces. arXiv\npreprint arXiv:1905.00475.\nStrehl, A. L., Li, L., Wiewiora, E., Langford, J., and Littman, M. L. (2006). PAC model-free\nreinforcement learning. In Proceedings of the 23rd international conference on Machine learning,\npages 881–888. ACM.\nSun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J. (2019). Model-based RL\nin contextual decision processes: PAC bounds and exponential improvements over model-free\napproaches. In Conference on Learning Theory, pages 2898–2933.\n12\nTang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O. X., Duan, Y., Schulman, J., DeTurck, F.,\nand Abbeel, P. (2017). # Exploration: A study of count-based exploration for deep reinforcement\nlearning. In Advances in Neural Information Processing Systems, pages 2753–2762.\nVempala, S. and Wang, G. (2004). A spectral algorithm for learning mixture models. Journal of\nComputer and System Sciences, 68(4):841–860.\nVidal, R. (2011). Subspace clustering. IEEE Signal Processing Magazine, 28(2):52–68.\nWallace, T., Sekmen, A., and Wang, X. (2015). Application of subspace clustering in DNA sequence\nanalysis. Journal of Computational Biology, 22(10):940–952.\nWang, Y.-X., Xu, H., and Leng, C. (2013). Provable subspace clustering: When LRR meets SSC. In\nAdvances in Neural Information Processing Systems, pages 64–72.\nWen, Z. and Van Roy, B. (2013). Efﬁcient exploration and value function generalization in determin-\nistic systems. In Advances in Neural Information Processing Systems, pages 3021–3029.\nYang, L. F. and Wang, M. (2019). Sample-optimal parametric Q-learning using linearly additive\nfeatures. In International Conference on Machine Learning, pages 6995–7004.\nYang, Z., Xie, Y., and Wang, Z. (2019). A theoretical analysis of deep Q-learning. arXiv preprint\narXiv:1901.00137.\nZanette, A. and Brunskill, E. (2019). Tighter problem-dependent regret bounds in reinforcement\nlearning without domain knowledge using value function bounds. In International Conference on\nMachine Learning, pages 7304–7312.\n13\nA\nProofs for the Main Result\nWe ﬁrst give a sketch of the proof. Note that if TSR always correctly simulates a trajectory of πk on\nthe underlying MDP, then by the correctness of A , the output policy of A in the end is near-optimal\nwith high probability. If in TSR, f k,J\n[H+1] decodes states correctly (up to a ﬁxed permutation, with high\nprobability) for every observation generated by playing πk ◦f k,J\n[H+1], then the obtained trajectory\n(on S) is as if obtained with πk ◦f[H+1] which is essentially equal to playing πk on the underlying\nMDP. Let us now consider πk ◦f k,i\n[H+1] for some intermediate iteration i ∈[J]. If there are many\nobservations from a previously unseen state, s, then ULO guarantees that all the decoding functions\nin future iterations will be correct with high probability of identifying observations of s. Since\nthere are at most |S| states to reach for each level following πk, after (H + 1)|S| iterations, TSR is\nguaranteed to output a set of decoding functions that are with high probability correct under policy\nπk. With this set of decoding functions, we can simulate a trajectory for A as if we know the true\nlatent states.\nFor episode k, we denote the training dataset D generated by running Unif(Π) as {Dk,i,h}H+1\nh=1\n(Line 5) and the testing dataset D′′ generated by πk ◦f k,i−1\n[H+1] as {D′′\nk,i,h}H+1\nh=1 (Line 6). The subscript\nh represents the level of the observations. Furthermore, we denote by µk,i,h(·) the distribution over\nhidden states at level h induced by πk ◦f k,i−1\n[H+1]. To formally prove the correctness of our framework,\nwe ﬁrst present the following lemma, showing that whenever some policy π with some decoding\nfunctions visits a state s with relatively high probability, all the decoding functions of later iterations\nwill correctly decode the observations from s with high probability.\nLemma 1. Suppose for some s∗∈Sh, (k, i) is the earliest pair such that\n\f\f{x ∈D′′\nk,i,h : f k,i\nh (x) =\nαh(s∗)}| ≥3ϵ · B log(δ−1\n1 ) and {x ∈D′′\nk,i,h : f k,i\nh (x) = αh(s∗)} is added into Zh as Dh,αh(s∗)\nat line 11 Algorithm 2, where αh is a good permutation between f k,i\nh\nand fh. Then for each\n(k′, i′) > (k, i) (in lexical order), with probability at least 1 −O(δ1),\nPr\nx∼q(·|s∗)\n\u0002\nf k′,i′\nh\n(x) ̸= α∗\nh(s∗)\n\u0003\n≤ϵ\nprovided 0 < ϵ log(δ−1\n1 ) ≤0.1 and B ≥B0. Here B0 is some constant to be determined later and\nα∗\nh is some ﬁxed permutation on Sh.\nProof of Lemma 1. For iterations (k′, i′) ≥(k, i), the function ˜f k′,i′\nh\nis obtained by applying ULO\non the dataset generated by\nµ′ := Unif({µk′′,i′′,h}(k′′,i′′)<(k′,i′))\nand the dataset has size\n\u0000(k′ −1) · J + i′\u0001\n· B = Θ(k′JB). Thus, with probability at least 1 −δ1,\nfor some permutation α′\nh,\nPr\ns∼µ′,x∼q(·|s)\n\u0002 ˜f k′,i′\nh\n(x) ̸= α′\nh ◦fh(x)\n\u0003\n≤g\n\u0000Θ(k′JB), δ1\n\u0001\n.\n(1)\nBy taking\nB0 : = Θ\n\u0010g−1(ϵ2/(K · J), δ1)\nK · J\n\u0011\n,\n(2)\nwe have when B ≥B0, g\n\u0000Θ(k′JB), δ1) ≤ϵ2/(K · J) for all k′ ∈[K]. Later, in Proposition 1,\nwe will show that B0 = poly(|S|, |A|, H, 1/ε). Now we consider f k,i\nh . Since the FixLabel routine\n(Algorithm 3) does not change the accuracy ratio, from Equation (1), it holds with probability at least\n1 −δ1 that\nPr\ns∼µk,i,h,x∼q(·|s)[f k,i\nh (x) ̸= αh ◦fh(x)] ≤k · J · g\n\u0000Θ(kJB), δ1\n\u0001\n≤ϵ.\nTherefore, by Chernoff bound, with probability at least 1 −O(δ1),\n\f\f{x ∈D′′\nk,i,h : fh(x) ̸= s and f k,i\nh (x) = αh(s)}\n\f\f < ϵ · B log(δ−1\n1 ).\n14\nSince\n\f\f{x ∈D′′\nk,i,h : f k,i\nh (x) = αh(s∗)}| ≥3ϵ · B log(δ−1\n1 ), we have that\n\f\f{x ∈D′′\nk,i,h : fh(x) = s∗and f k,i\nh (x) = αh(s∗)}| > 2\n3 ·\n\f\f{x ∈D′′\nk,i,h : f k,i\nh (x) = αh(s∗)}\n\f\f(3)\n≥2ϵ · B log(δ−1\n1 ).\nThus, by Chernoff bound, with probability at least 1 −O(δ1), µk,i,h(s∗) ≥ϵ · log(δ−1\n1 ). Also note\nthat f k,i\nh\nis the ﬁrst function that has conﬁrmed on s∗(i.e., no Dh,αh(s∗) exists in Zh of line 8 at\niteration (k, i)). By Line 10 and Line 11, for later iterations, in Zh, Dh,αh(s∗) = {x ∈D′′\nk,i,h :\nf k,i\nh (x) = αh(s∗)}.\nNext, for another (k′, i′) > (k, i), we let the corresponding permutation be α′\nh for ˜f k′,i′\nh\n. Since\nµ′(s′) ≥µk,i,h(s′)/(k′ · J), with probability at least 1 −δ1,\nPr\ns∼µk,i,h,x∼q(·|s)\n\u0002 ˜f k′,i′\nh\n(x) ̸= α′\nh ◦fh(x)\n\u0003\n≤k′ · J · g(Θ(k′JB), δ1).\nNotice that\nPr\ns∼µk,i,h,x∼q(·|s)\n\u0002 ˜f k′,i′\nh\n(x) ̸= α′\nh ◦fh(x)\n\u0003\n=\nX\ns′∈Sh\nµk,i,h(s′)\nPr\nx∼q(·|s′)\n\u0002 ˜f k′,i′\nh\n(x) ̸= α′\nh ◦fh(x)\n\u0003\n≥µk,i,h(s∗)\nPr\nx∼q(·|s∗)\n\u0002 ˜f k′,i′\nh\n(x) ̸= α′\nh ◦fh(x)\n\u0003\n≥ϵ · log(δ−1)\nPr\nx∼q(·|s∗)\n\u0002 ˜f k′,i′\nh\n(x) ̸= α′\nh ◦fh(x)\n\u0003\n.\nThus, with probability at least 1 −δ1,\nPr\nx∼q(·|s∗)\n\u0002 ˜f k′,i′\nh\n(x) ̸= α′\nh ◦fh(x)\n\u0003\n≤k′ · J · g(Θ(k′JB), δ1)\nϵ · log(δ−1\n1 )\n≤ϵ\nwith B ≥B0 and B0 as deﬁned in Equation (2). Let s′ := α′\nh(s∗). Conditioning on ULO being\ncorrect on ˜f k′,i′\n[H+1] and f k,i\n[H+1], by Chernoff bound and Equation (3), with probability at least 1−O(δ1),\nwe have\n\f\f{x ∈Dh,αh(s∗) : ˜f k′,i′\nh\n(x) = s′}\n\f\f ≥\n\f\f{x ∈Dh,αh(s∗) : fh(x) = s∗, ˜f k′,i′\nh\n(x) = s′}\n\f\f\n≥(1 −ϵ · log(δ−1\n1 )) · 2\n3 ·\n\f\fDh,αh(s∗)\n\f\f > 3\n5\n\f\fDh,αh(s∗)\n\f\f,\nwhere the fraction 2\n3 follows from Equation (3) and we use the fact that D′′\nk,i,h are independent from\nthe training dataset. By our label ﬁxing procedure, we ﬁnd a permutation that swaps s′ with s for ˜f k′,i′\nh\nto obtain f k′,i′\nh\n. By the above analysis, with probability at least 1 −O(δ1), Prx∼q(·|s∗)\n\u0002\nf k′,i′\nh\n(x) ̸=\nαh(s∗)\n\u0003\n≤ϵ as desired. Consequently, we let α∗\nh(s∗) = αh(s∗), which satisﬁes the requirement of\nthe lemma.\nNext, by the deﬁnition of our procedure of updating the label standard dataset (Line 11, Algorithm 2),\nwe have the following corollary.\nCorollary 1. Consider Algorithm 2. Let Zk,i,h be the label standard dataset at episode k before\niteration i for Sh. Then, with probability at least 1 −O(H|S|δ1),\nfor all k, i and Dh,s ∈Zk,i,h, |{x ∈Dh,s : α∗\nh ◦fh(x) = s, s ∈Sh}| > 2/3|Dh,s|.\nAt episode k and iteration i of the algorithm TSR, let Ek,i be the event that for all h ∈[H +1], Dh,s ∈\nZk,i,h, Prx∼q(·|s)\n\u0002\nf k,i\nh (x) ̸= α∗\nh ◦fh(x)\n\u0003\n≤ϵ. We have the following corollary as a consequence of\nLemma 1 by taking the union bound over all states.\nCorollary 2. ∀k, i :\nPr\n\u0002\nEk,i\n\u0003\n≥1 −O(H|S|δ1).\nThe next lemma shows that after (H + 1)|S| + 1 iterations of the TSR subroutine, the algorithm\noutputs a trajectory for the algorithm A as if it knows the true mapping f[H+1].\n15\nLemma 2. Suppose in an episode k, we are running algorithm TSR. Then after J = (H + 1)|S| + 1\niterations, we have, for every j ≥J, with probability at least 1 −O(H|S|δ1),\nfor all h ∈[H + 1],\nPr\ns∼µk,j+1,h,x∼q(·|s)\n\u0002\nf k,j\nh (x) ̸= α∗\nh ◦fh(x)\n\u0003\n≤ϵ′\nfor some small enough ϵ and 50H · ϵ · |S| · log(δ−1\n1 ) < ϵ′ < 1/2, provided B ≥B0 as deﬁned in\nLemma 1.\nProof of Lemma 2. For i < J, there are two cases:\n1. there exists an h ∈[H + 1] such that Prs∼µk,i+1,h,x∼q(·|s)\n\u0002\nf k,i\nh (x) ̸= αh ◦fh(x)\n\u0003\n>\nϵ′/(2H);\n2. for all h ∈[H + 1], Prs∼µk,i+1,h,x∼q(·|s)\n\u0002\nf k,i\nh (x) ̸= αh ◦fh(x)\n\u0003\n≤ϵ′/(2H),\nwhere αh is some good permutations between f k,i\nh\nand fh. If case 1 happens, then there exists a state\ns∗∈Sh such that\nPr\nx∼q(·|s∗)\n\u0002\nf k,i\nh (x) ̸= αh ◦fh(x)\n\u0003\n· µk,i+1,h(s∗) >\nϵ′\n2H|S|.\n(4)\nIf Dh,αh(s∗) ∈Zk,i,h, where Zk,i,h is deﬁned as in Corollary 1, by Lemma 1, with probability at\nleast 1 −O(δ1),\nPr\nx∼q(·|s∗)[f k,i\nh (x) ̸= α∗\nh ◦fh(x)] ≤ϵ\nand α∗\nh(s∗) = αh(s∗). Thus, µk,i+1,h(s∗) >\nϵ′\n2H|S|/ϵ > 1, a contradiction with µk,i+1,h(s∗) ≤1.\nTherefore, there is no Dh,αh(s∗) in Zk,i,h. Then, due to Prx∼q(·|s∗)\n\u0002\nf k,i\nh (x) ̸= αh ◦fh(x)\n\u0003\n≤1, by\nEquation (4), we have\nµk,i+1,h(s∗) >\nϵ′\n2H|S|.\n(5)\nSince f k,i+1\nh\nis trained on Unif({µk′,i′,h}(k′,i′)<(k,i+1)), by Deﬁnition of ULO, with probability at\nleast 1 −δ1,\nPr\ns∼µk,i+1,h,x∼q(·|s)\n\u0002\nf k,i+1\nh\n(x) ̸= α′\nh(s)\n\u0003\n≤k · J · g(Θ(kJB), δ1) ≤ϵ2,\nwith B ≥B0 (B0 is deﬁned in Equation (2)) and α′\nh is some good permutation between f k,i+1\nh\nand\nfh. Thus, by Equation (5) and the choice of ϵ and ϵ′, we have\nPr\nx∼q(·|s∗)\n\u0002\nf k,i+1\nh\n(x) ̸= α′\nh(s∗)\n\u0003\n< ϵ/25.\nThus,\nµk,i+1,h(s∗) ·\nPr\nx∼q(·|s∗)\n\u0002\nf k,i+1\nh\n(x) = α′\nh(s∗)\n\u0003\n>\nϵ′\n2H|S| · (1 −ϵ/25) > 24ϵ · log(δ−1\n1 ),\nwhere the last inequality is due to ϵ < ϵ′ < 1. By Chernoff bound, with probability at least 1−O(δ1),\n|{x ∈D′′\nk,i+1,h : f k,i+1\nh\n(x) = α′\nh(s∗)}| ≥3ϵ · B log(δ−1\n1 ).\nTherefore, if case 1 happens, one state s will be conﬁrmed in iteration i + 1 and α∗\nh(s∗) = α′\nh(s∗) is\ndeﬁned.\nTo analyze case 2, we ﬁrst deﬁne sets {Gk,i+1,h}H+1\nh=1 with Gk,i+1,h := {s ∈Sh | Dh,s ∈Zk,i+1,h},\ni.e., Gk,i+1,h contains all conﬁrmed states of level h before iteration i + 1 at episode k. If case 2\nhappens, we further divide the situation into two subcases:\na) for all h ∈[H + 1], for all s ∈Gc\nk,i+1,h, µk,i+1,h(s) ≤ϵ′/(8H|S|);\n16\nb) there exists an h ∈[H + 1] and a state s∗∈Gc\nk,i+1,h such that µk,i+1,h(s∗) ≥ϵ′/(8H|S|),\nFirst notice that for every h ∈[H +1] and j > i, since f k,j\nh\nis trained on Unif({µk′,i′,h}(k′,i′)≤(k,j)),\nby Deﬁnition of ULO and our choice of B in Equation (2), with probability at least 1 −δ1, we have\nPr\ns∼µk,i+1,h,x∼q(·|s)[f k,j\nh\n̸= α′\nh(s)] ≤ϵ2,\n(6)\n⇒\nX\ns∈Gk,i+1,h\nµk,i+1,h(s)\nPr\nx∼q(·|s)[f k,j\nh (x) ̸= α′\nh(s)] +\nX\ns/∈Gk,i+1,h\nµk,i+1,h(s)\nPr\nx∼q(·|s)[f k,j\nh (x) ̸= α′\nh(s)] ≤ϵ2,\nwhere α′\nh is some good permutation between f k,j\nh\nand fh.\nIf subcase a) happens, note that for s ∈Gk,i+1,h, due to the FixLabel routine (Algorithm 3),\nα′\nh(s) = α∗\nh(s), for f k,j\nh (j > i) we have\nX\ns∈Sh\nµk,i+1,h(s)\nPr\nx∼q(·|s)[f k,j\nh (x) ̸= α∗\nh(s)]\n=\nX\ns∈Gk,i+1,h\nµk,i+1,h(s)\nPr\nx∼q(·|s)[f k,j\nh (x) ̸= α∗\nh(s)] +\nX\ns/∈Gk,i+1,h\nµk,i+1,h(s)\nPr\nx∼q(·|s)[f k,j\nh (x) ̸= α∗\nh(s)]\n=\nX\ns∈Gk,i+1,h\nµk,i+1,h(s)\nPr\nx∼q(·|s)[f k,j\nh (x) ̸= α′\nh(s)] +\nX\ns/∈Gk,i+1,h\nµk,i+1,h(s)\nPr\nx∼q(·|s)[f k,j\nh (x) ̸= α∗\nh(s)]\n≤ϵ2 + ϵ′/(8H) < ϵ′/(4H).\nTaking a union bound over all f k,j\n[H+1], we have that for any h ∈[H + 1], with probability at least\n1 −O(Hδ1),\nPr\ns∼µk,j+1,h,x∼q(·|s)[f k,j\nh (x) = α∗\nh(s)] ≥\nPr\ns∼µk,j+1,h,x∼q(·|s)[f k,j\nh (x) = α∗\nh(s) = f k,i\nh (x)]\n≥\nPr\nfor all h′∈[h],sh′∼µk,j+1,h′,xh′∼q(·|sh′)[ for all h′ ∈[h], f k,j\nh′ (xh′) = α∗\nh′(s) = f k,i\nh′ (xh′)]\n=\nPr\nfor all h′∈[h],sh′∼µk,i+1,h′,xh′∼q(·|sh′)[ for all h′ ∈[h], f k,j\nh′ (xh′) = α∗\nh′(s) = f k,i\nh′ (xh′)]\n≥1 −(ϵ′/(2H) + ϵ′/(4H)) · H ≥1 −ϵ′.\nTherefore, if case 2 and subcase a) happens, the desired result is obtained.\nIf subcase b) happens, we consider the function f k,i+1\nh\n. By Equation (6),\nµk,i+1,h(s∗) ·\nPr\nx∼q(·|s∗)[f k,i+1\nh\n(x) ̸= α′\nh(s∗)] ≤ϵ2\n⇒\nPr\nx∼q(·|s∗)[f k,i+1\nh\n(x) ̸= α′\nh(s∗)] ≤ϵ2/(ϵ′/(8H|S|)) ≤ϵ,\nwhere α′\nh here is some good permutation between f k,i+1\nh\nand fh. Thus,\nµk,i+1,h(s∗) ·\nPr\nx∼q(·|s∗)\n\u0002\nf k,i+1\nh\n(x) = α′\nh(s∗)\n\u0003\n>\nϵ′\n8H|S| · (1 −ϵ) > 6ϵ · log(δ−1\n1 ).\nBy Chernoff bound, with probability at least 1 −O(δ1), |{x ∈D′′\nk,i+1,h : f k,i+1\nh\n(x) = α′\nh(s∗)}| ≥\n3ϵ · B log(δ−1\n1 ). Therefore, the state s∗will be conﬁrmed in iteration i + 1 and α∗\nh(s∗) = α′\nh(s∗) is\ndeﬁned.\nIn conclusion, for each iteration, there are two scenarios, either the desired result in Lemma 2 holds\nalready or a new state will be conﬁrmed for the next iteration. Since there are in total PH+1\nh=1 |Sh| ≤\n(H + 1)|S| states, after J := (H + 1)|S| + 1 iterations, by Lemma 1, with probability at least\n17\n1 −O(H|S|δ1), for every j ≥J, for all h ∈[H + 1] and all s ∈Sh, we have Prx∼q(·|s)[f k,j\nh (x) ̸=\nα∗\nh(s)] ≤ϵ. Therefore, it holds that for\nPr\ns∼µk,j+1,h,x∼q(·|s)(f k,j\nh (x) ̸= α∗\nh(s)) ≤ϵ < ϵ′.\nProposition 1. Suppose in Deﬁnition 1, g−1(ϵ, δ1) = poly(1/ϵ, log(δ−1\n1 )) for any ϵ, δ1 ∈(0, 1) and\nA is (ε, δ2)-correct with sample complexity poly\n\u0000|S|, |A|, H, 1/ε, log\n\u0000δ−1\n2\n\u0001\u0001\nfor any ε, δ2 ∈(0, 1).\nThen for each iteration of the outer loop of Algorithm 1, the policy φn is an ε/3-optimal policy for\nthe BMDP with probability at least 0.99, using at most poly (|S|, |A|, H, 1/ε) trajectories.\nProof of Proposition 1. We ﬁrst show that the trajectory obtained by running πk with the learned\ndecoding functions f k,J\n[H+1] matches, with high probability, that from running πk with α∗\n[H+1] ◦f[H+1].\nLet K = C(ε/4, δ2) be the total number of episodes played by A to learn an ε/4-optimal policy\nwith probability at least 1 −δ2. For each episode k ∈[K], let the trajectory of observations be\n{xk\nh}H+1\nh=1 . We deﬁne event\nEk := {∀h ∈[H + 1], f k,J\nh\n(xk\nh) = α∗\nh(fh(xk\nh))},\nwhere J = (H + 1)|S| + 1. Note that on Ek, the trajectory of running πk ◦α∗\n[H+1] ◦f[H+1] equals\nrunning πk ◦f k,J\n[H+1]. We also let the event F be that ULO succeeds on every iteration (satisﬁes\nLemma 2). Thus,\nPr[F] ≥1 −K · J · δ1 = 1 −poly(|S|, |A|, H, 1/ε, log(δ−1\n1 )) · δ1.\nFurthermore, each xk,h is obtained by the distribution P\ns µk,J+1,h(s)q(·|s). On F, by Lemma 2,\nwe have\nPr[f k,J\nh\n(xk\nh) = α∗\nh(fh(xk\nh))] ≤ϵ′\nby the choice of B. Therefore,\nPr[Ek|F] ≥1 −(H + 1)ϵ′.\nOverall, we have\nPr\nh\nEk, ∀k ∈[K]\n\f\f\fF\ni\n≥1 −K(H + 1)ϵ′.\nThus, with probability at least 1 −δ2 −poly(|S|, |A|, H, 1/ε, log(δ−1\n1 )) · (ϵ′ + δ1), A outputs\na policy π, that is ε/4-optimal for the underlying MDP with state sets {Sh}H+1\nh=1 permutated by\nα∗\n[H+1], which we denote as event E′. Conditioning on E′, since on a high probability event E′′ with\nPr[E′′] ≥1 −(H + 1)ϵ′, π ◦f K,J\n[H+1] and π ◦α∗\n[H+1] ◦f[H+1] have the same trajectory, the value\nachieved by π ◦f K,J\n[H+1] and π ◦α∗\n[H+1] ◦f[H+1] differ by at most (H + 1)2ϵ′. Thus, with probability\nat least 1 −δ2 −poly(|S|, |A|, H, 1/ε, log(δ−1\n1 )) · (ϵ′ + δ1), the output policy π ◦f K,J\n[H+1] is at least\nε/4 + O(H2ϵ′) accurate, i.e.,\nV ∗\n1 −V\nπ◦f K,J\n[H+1]\n1\n≤V ∗\n1 −V\nπ◦α∗\n[H+1]◦f[H+1]\n1\n+ O(H2ϵ′) ≤ε/4 + O(H2ϵ′).\nSetting ϵ′, δ1, and δ2 properly, V ∗\n1 −V\nπ◦f K,J\n[H+1]\n1\n≤ε/3 with probability at least 0.99. Since 1/δ1 =\npoly(|S|, |A|, H, 1/ε) and 1/ϵ = poly(|S|, |A|, H, 1/ε, log(δ−1\n1 )), B0 in Lemma 1 and Lemma 2 is\npoly(|S|, |A|, H, 1/ε). The desired result is obtained.\nFinally, based on Proposition 1, we establish Theorem 1.\nProof of Theorem 1. By Proposition 1 and taking N = ⌈log(2/δ)/2⌉, with probability at least\n1 −δ/2, there exists a policy in {φn}N\nn=1 that is ε/3-optimal for the BMDP. For each policy φn, we\ntake L := ⌈9H2/(2ε2) log(2N/δ)⌉episodes to evaluate its value. Then by Hoeffding’s inequality,\nwith probability at least 1 −δ/(2N), | ¯V φn\n1\n−V φn\n1\n| ≤ε/3. By taking the union bound and selecting\nthe policy φ ∈argmaxφ∈φ[N] ¯V φ\n1 , with probability at least 1 −δ, it is ε-optimal for the BMDP.\nIn total, the number of needed trajectories is N · PK\nk=1\nPJ\ni=1\n\u0000(k −1)J + i + 1\n\u0001\nB + N · L =\nO(N · K2 · J2 · B + N · L) = poly(|S|, |A|, H, 1/ε, log(δ−1)). We complete the proof.\n18\nB\nExamples of Unsupervised Learning Oracle\nIn this section, we give some examples of ULO. First notice that the generation process of ULO\nis termed as the mixture model in statistics (McLachlan and Basford, 1988; McLachlan and Peel,\n2004), which has a wide range of applications (see e.g., Bouguila and Fan (2020)). We list examples\nof mixture models and some algorithms as candidates of ULO.\nGaussian Mixture Models (GMM)\nIn GMM, q(·|s) = N(s, σ2\ns), i.e., observations are hidden\nstates plus Gaussian noise.4 When the noises are (truncated) Gaussian, under certain conditions,\ne.g. states are well-separated, we are able to identify the latent states with high accuracy. A series\nof works (Arora and Kannan, 2001; Vempala and Wang, 2004; Achlioptas and McSherry, 2005;\nDasgupta and Schulman, 2000; Regev and Vijayaraghavan, 2017) proposed algorithms that can be\nserved as ULO.\nBernoulli Mixture Models (BMM)\nBMM is considered in binary image processing (Juan and\nVidal, 2004) and texts classiﬁcation (Juan and Vidal, 2002). In BMM, every observation is a point in\n{0, 1}d. A true state determines a frequency vector. In Najaﬁet al. (2020), the authors proposed a\nreliable clustering algorithm for BMM data with polynomial sample complexity guarantee.\nSubspace Clustering\nIn some applications, each state is a set of vectors and observations lie in the\nspanned subspace. Suppose for different states, the basis vectors differ under certain metric, then\nrecovering the latent state is equivalent to subspace clustering. Subspace clustering has a variety of\napplications include face clustering, community clustering, and DNA sequence analysis (Wallace\net al., 2015; Vidal, 2011; Elhamifar and Vidal, 2013). Proper algorithms for ULO can be found in\ne.g., (Wang et al., 2013; Soltanolkotabi et al., 2014).\nIn addition to the aforementioned models, other reasonable settings are Categorical Mixture Models\n(Bontemps and Toussile, 2013), Poisson Mixture Models (Li and Zha, 2006), Dirichlet Mixture\nModels (Dahl, 2006) and so on.\nC\nMore about Experiments\nParameter Tuning\nIn the experiments, for OracleQ, we tune the learning rate and a conﬁdence\nparameter; for QLearning, we tune the learning rate and the exploration parameter ϵ; for PCID, we\nfollow the code provided in Du et al. (2019a), tune the number of clusters for k-means and the number\nof trajectories n to collect in each outer iteration, and ﬁnally select the better result between linear\nfunction and neural network implementation.\nUnsupervised Learning Algorithms\nIn our method, we use OracleQ as the tabular RL algorithm\nto operate on the decoded state space and try three unsupervised learning approaches: 1. ﬁrst conduct\nprinciple component analysis (PCA) on the observations and then use k-means (KMeans) to cluster;\n2. ﬁrst apply PCA, then use Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\nfor clustering, and ﬁnally use support vector machine to ﬁt a classiﬁer; 3. employ Gaussian Mixture\nModel (GMM) to ﬁt the observation data then generate a label predictor. We call the python library\nsklearn for all these methods. During unsupervised learning, we do not separate observations by\nlevels but add level information in decoded states. Besides the hyperparameters for OracleQ and\nthe unsupervised learning oracle, we also tune the batch size B adaptively in Algorithm 2. In our\ntests, instead of resampling over all previous policies as Line 5 Algorithm 2, we use previous data.\nSpeciﬁcally, we maintain a training dataset D in memory and for iteration i, generate B training\ntrajectories following π ◦f i−1\n[H+1] and merge them into D to train ULO. Also, we stop training\ndecoding functions once they become stable, which takes 100 training trajectories when H = 5,\n500 ∼1000 trajectories when H = 10, and 1000 ∼2500 trajectories when H = 20. Since this\nprocess stops very quickly, we also skip the label matching steps (Line 8 to Line 12 Algorithm 2) and\nthe ﬁnal decoding function leads to a near-optimal performance as shown in the results.\n4To make the model satisfy the disjoint block assumption in the deﬁnition of BMDP, we need some truncation\nof the Gaussian noise so that each observation only corresponds to a unique hidden state.\n19\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "math.OC",
    "stat.ML"
  ],
  "published": "2020-03-15",
  "updated": "2020-12-01"
}