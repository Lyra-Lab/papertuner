{
  "id": "http://arxiv.org/abs/2003.07082v2",
  "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
  "authors": [
    "Peng Qi",
    "Yuhao Zhang",
    "Yuhui Zhang",
    "Jason Bolton",
    "Christopher D. Manning"
  ],
  "abstract": "We introduce Stanza, an open-source Python natural language processing\ntoolkit supporting 66 human languages. Compared to existing widely used\ntoolkits, Stanza features a language-agnostic fully neural pipeline for text\nanalysis, including tokenization, multi-word token expansion, lemmatization,\npart-of-speech and morphological feature tagging, dependency parsing, and named\nentity recognition. We have trained Stanza on a total of 112 datasets,\nincluding the Universal Dependencies treebanks and other multilingual corpora,\nand show that the same neural architecture generalizes well and achieves\ncompetitive performance on all languages tested. Additionally, Stanza includes\na native Python interface to the widely used Java Stanford CoreNLP software,\nwhich further extends its functionality to cover other tasks such as\ncoreference resolution and relation extraction. Source code, documentation, and\npretrained models for 66 languages are available at\nhttps://stanfordnlp.github.io/stanza.",
  "text": "Sta n z a : A Python Natural Language Processing Toolkit\nfor Many Human Languages\nPeng Qi*\nYuhao Zhang*\nYuhui Zhang\nJason Bolton\nChristopher D. Manning\nStanford University\nStanford, CA 94305\n{pengqi, yuhaozhang, yuhuiz}@stanford.edu\n{jebolton, manning}@stanford.edu\nAbstract\nWe introduce Sta n z a , an open-source Python\nnatural language processing toolkit support-\ning 66 human languages.\nCompared to ex-\nisting widely used toolkits, Sta n z a features\na language-agnostic fully neural pipeline for\ntext analysis, including tokenization, multi-\nword token expansion, lemmatization, part-of-\nspeech and morphological feature tagging, de-\npendency parsing, and named entity recogni-\ntion.\nWe have trained Sta n z a on a total of\n112 datasets, including the Universal Depen-\ndencies treebanks and other multilingual cor-\npora, and show that the same neural architec-\nture generalizes well and achieves competitive\nperformance on all languages tested. Addition-\nally, Sta n z a includes a native Python interface\nto the widely used Java Stanford CoreNLP\nsoftware, which further extends its function-\nality to cover other tasks such as coreference\nresolution and relation extraction.\nSource\ncode, documentation, and pretrained models\nfor 66 languages are available at https://\nstanfordnlp.github.io/stanza/.\n1\nIntroduction\nThe growing availability of open-source natural lan-\nguage processing (NLP) toolkits has made it easier\nfor users to build tools with sophisticated linguistic\nprocessing. While existing NLP toolkits such as\nCoreNLP (Manning et al., 2014), FLAIR (Akbik\net al., 2019), spaCy1, and UDPipe (Straka, 2018)\nhave had wide usage, they also suffer from several\nlimitations. First, existing toolkits often support\nonly a few major languages. This has signiﬁcantly\nlimited the community’s ability to process multilin-\ngual text. Second, widely used tools are sometimes\nunder-optimized for accuracy either due to a focus\non efﬁciency (e.g., spaCy) or use of less power-\nful models (e.g., CoreNLP), potentially mislead-\n∗Equal contribution. Order decided by a tossed coin.\n1https://spacy.io/\nTokenization & Sentence Split\nDOCUMENT\nSENTENCE\nWORD\nPOS\nTOKEN\nHEAD\nDEPREL\n...\nPROCESSORS\nRAW TEXT\nMultilingual: 66 Languages\nFully Neural: Language-agnostic\nTOKENIZE\nMulti-word Token Expansion\nMWT\nLemmatization\nLEMMA\nPOS & Morphological Tagging\nPOS\nDependency Parsing\nDEPPARSE\nNamed Entity Recognition\nNER\nHello!\nEN\nBonjour!\nFR\n你好!\nZH\nHallo!\nDE\nﻣرﺣﺑﺎ!\nAR\n안녕하세요!\nKO\n¡Hola!\nES\nЗдравствуйте!\nRU\nLEMMA\nWORDS\nこんにちは！\nJA\nHallo!\nNL\nxin chào!\nVI\nनमस्कार!\nHI\nNative Python Objects\nFigure 1: Overview of Sta n z a ’s neural NLP pipeline.\nSta n z a takes multilingual text as input, and produces\nannotations accessible as native Python objects. Be-\nsides this neural pipeline, Sta n z a\nalso features a\nPython client interface to the Java CoreNLP software.\ning downstream applications and insights obtained\nfrom them. Third, some tools assume input text has\nbeen tokenized or annotated with other tools, lack-\ning the ability to process raw text within a uniﬁed\nframework. This has limited their wide applicabil-\nity to text from diverse sources.\nWe introduce Sta n z a 2, a Python natural language\nprocessing toolkit supporting many human lan-\nguages. As shown in Table 1, compared to existing\nwidely-used NLP toolkits, Sta n z a has the following\nadvantages:\n• From raw text to annotations. Sta n z a fea-\ntures a fully neural pipeline which takes raw\ntext as input, and produces annotations includ-\ning tokenization, multi-word token expansion,\nlemmatization, part-of-speech and morpholog-\nical feature tagging, dependency parsing, and\nnamed entity recognition.\n• Multilinguality.\nSta n z a ’s architectural de-\nsign is language-agnostic and data-driven,\nwhich allows us to release models support-\n2The toolkit was called StanfordNLP prior to v1.0.0.\narXiv:2003.07082v2  [cs.CL]  23 Apr 2020\nSystem\n# Human\nLanguages\nProgramming\nLanguage\nRaw Text\nProcessing\nFully\nNeural\nPretrained\nModels\nState-of-the-art\nPerformance\nCoreNLP\n6\nJava\n!\n!\nFLAIR\n12\nPython\n!\n!\n!\nspaCy\n10\nPython\n!\n!\nUDPipe\n61\nC++\n!\n!\n!\nSta n z a\n66\nPython\n!\n!\n!\n!\nTable 1: Feature comparisons of Sta n z a against other popular natural language processing toolkits.\ning 66 languages, by training the pipeline on\nthe Universal Dependencies (UD) treebanks\nand other multilingual corpora.\n• State-of-the-art performance. We evaluate\nSta n z a on a total of 112 datasets, and ﬁnd its\nneural pipeline adapts well to text of different\ngenres, achieving state-of-the-art or competi-\ntive performance at each step of the pipeline.\nAdditionally, Sta n z a features a Python interface\nto the widely used Java CoreNLP package, allow-\ning access to additional tools such as coreference\nresolution and relation extraction.\nSta n z a is fully open source and we make pre-\ntrained models for all supported languages and\ndatasets available for public download. We hope Sta\nn z a can facilitate multilingual NLP research and ap-\nplications, and drive future research that produces\ninsights from human languages.\n2\nSystem Design and Architecture\nAt the top level, Sta n z a consists of two individual\ncomponents: (1) a fully neural multilingual NLP\npipeline; (2) a Python client interface to the Java\nStanford CoreNLP software. In this section we\nintroduce their designs.\n2.1\nNeural Multilingual NLP Pipeline\nSta n z a ’s neural pipeline consists of models that\nrange from tokenizing raw text to performing syn-\ntactic analysis on entire sentences (see Figure 1).\nAll components are designed with processing many\nhuman languages in mind, with high-level design\nchoices capturing common phenomena in many\nlanguages and data-driven models that learn the dif-\nference between these languages from data. More-\nover, the implementation of Sta n z a components is\nhighly modular, and reuses basic model architec-\ntures when possible for compactness. We highlight\nthe important design choices here, and refer the\nreader to Qi et al. (2018) for modeling details.\n(fr) L’Association des Hôtels\n(en) The Association of Hotels\n(fr) Il y a des hôtels en bas de la rue\n(en) There are hotels down the street\nFigure 2: An example of multi-word tokens in French.\nThe des in the ﬁrst sentence corresponds to two syntac-\ntic words, de and les; the second des is a single word.\nTokenization and Sentence Splitting.\nWhen\npresented raw text, Sta n z a tokenizes it and groups\ntokens into sentences as the ﬁrst step of processing.\nUnlike most existing toolkits, Sta n z a combines tok-\nenization and sentence segmentation from raw text\ninto a single module. This is modeled as a tagging\nproblem over character sequences, where the model\npredicts whether a given character is the end of a\ntoken, end of a sentence, or end of a multi-word\ntoken (MWT, see Figure 2).3 We choose to predict\nMWTs jointly with tokenization because this task\nis context-sensitive in some languages.\nMulti-word Token Expansion.\nOnce MWTs\nare identiﬁed by the tokenizer, they are expanded\ninto the underlying syntactic words as the basis\nof downstream processing. This is achieved with\nan ensemble of a frequency lexicon and a neural\nsequence-to-sequence (seq2seq) model, to ensure\nthat frequently observed expansions in the training\nset are always robustly expanded while maintaining\nﬂexibility to model unseen words statistically.\nPOS and Morphological Feature Tagging.\nFor\neach word in a sentence, Sta n z a assigns it a part-\nof-speech (POS), and analyzes its universal mor-\nphological features (UFeats, e.g., singular/plural,\n1st/2nd/3rd person, etc.). To predict POS and UFeats,\nwe adopt a bidirectional long short-term mem-\nory network (Bi-LSTM) as the basic architecture.\nFor consistency among universal POS (UPOS),\n3Following Universal Dependencies (Nivre et al., 2020),\nwe make a distinction between tokens (contiguous spans of\ncharacters in the input text) and syntactic words. These are\ninterchangeable aside from the cases of MWTs, where one\ntoken can correspond to multiple words.\ntreebank-speciﬁc POS (XPOS), and UFeats, we\nadopt the biafﬁne scoring mechanism from Dozat\nand Manning (2017) to condition XPOS and\nUFeats prediction on that of UPOS.\nLemmatization.\nSta n z a\nalso lemmatizes each\nword in a sentence to recover its canonical form\n(e.g., did→do). Similar to the multi-word token ex-\npander, Sta n z a ’s lemmatizer is implemented as an\nensemble of a dictionary-based lemmatizer and a\nneural seq2seq lemmatizer. An additional classiﬁer\nis built on the encoder output of the seq2seq model,\nto predict shortcuts such as lowercasing and iden-\ntity copy for robustness on long input sequences\nsuch as URLs.\nDependency Parsing.\nSta n z a parses each sen-\ntence for its syntactic structure, where each word\nin the sentence is assigned a syntactic head that\nis either another word in the sentence, or in the\ncase of the root word, an artiﬁcial root symbol. We\nimplement a Bi-LSTM-based deep biafﬁne neural\ndependency parser (Dozat and Manning, 2017). We\nfurther augment this model with two linguistically\nmotivated features: one that predicts the lineariza-\ntion order of two words in a given language, and\nthe other that predicts the typical distance in linear\norder between them. We have previously shown\nthat these features signiﬁcantly improve parsing\naccuracy (Qi et al., 2018).\nNamed Entity Recognition.\nFor each input sen-\ntence, Sta n z a also recognizes named entities in it\n(e.g., person names, organizations, etc.). For NER\nwe adopt the contextualized string representation-\nbased sequence tagger from Akbik et al. (2018).\nWe ﬁrst train a forward and a backward character-\nlevel LSTM language model, and at tagging time\nwe concatenate the representations at the end of\neach word position from both language models\nwith word embeddings, and feed the result into a\nstandard one-layer Bi-LSTM sequence tagger with\na conditional random ﬁeld (CRF)-based decoder.\n2.2\nCoreNLP Client\nStanford’s Java CoreNLP software provides a com-\nprehensive set of NLP tools especially for the En-\nglish language. However, these tools are not easily\naccessible with Python, the programming language\nof choice for many NLP practitioners, due to the\nlack of ofﬁcial support. To facilitate the use of\nCoreNLP from Python, we take advantage of the\nexisting server interface in CoreNLP, and imple-\nment a robust client as its Python interface.\nWhen the CoreNLP client is instantiated, Sta n z\na will automatically start the CoreNLP server as a\nlocal process. The client then communicates with\nthe server through its RESTful APIs, after which\nannotations are transmitted in Protocol Buffers, and\nconverted back to native Python objects. Users can\nalso specify JSON or XML as annotation format.\nTo ensure robustness, while the client is being used,\nSta n z a periodically checks the health of the server,\nand restarts it if necessary.\n3\nSystem Usage\nSta n z a ’s user interface is designed to allow quick\nout-of-the-box processing of multilingual text. To\nachieve this, Sta n z a supports automated model\ndownload via Python code and pipeline customiza-\ntion with processors of choice. Annotation results\ncan be accessed as native Python objects to allow\nfor ﬂexible post-processing.\n3.1\nNeural Pipeline Interface\nSta n z a ’s neural NLP pipeline can be initialized\nwith the Pipeline class, taking language name\nas an argument. By default, all processors will be\nloaded and run over the input text; however, users\ncan also specify the processors to load and run with\na list of processor names as an argument. Users\ncan additionally specify other processor-level prop-\nerties, such as batch sizes used by processors, at\ninitialization time.\nThe following code snippet shows a minimal us-\nage of Sta n z a for downloading the Chinese model,\nannotating a sentence with customized processors,\nand printing out all annotations:\nimport stanza\n# download Chinese model\nstanza.download(’zh’)\n# initialize Chinese neural pipeline\nnlp = stanza.Pipeline(’zh’, processors=’tokenize,\npos,ner’)\n# run annotation over a sentence\ndoc = nlp(’斯坦福是一所私立研究型大学。’)\nprint(doc)\nAfter all processors are run, a Document in-\nstance will be returned, which stores all annotation\nresults. Within a Document, annotations are fur-\nther stored in Sentences, Tokens and Words\nin a top-down fashion (Figure 1). The following\ncode snippet demonstrates how to access the text\nand POS tag of each word in a document and all\nnamed entities in the document:\n# print the text and POS of all words\nfor sentence in doc.sentences:\nfor word in sentence.words:\nprint(word.text, word.pos)\n# print all entities in the document\nprint(doc.entities)\nSta n z a is designed to be run on different hard-\nware devices. By default, CUDA devices will be\nused whenever they are visible by the pipeline, or\notherwise CPUs will be used. However, users can\nforce all computation to be run on CPUs by setting\nuse_gpu=False at initialization time.\n3.2\nCoreNLP Client Interface\nThe CoreNLP client interface is designed in a way\nthat the actual communication with the backend\nCoreNLP server is transparent to the user. To an-\nnotate an input text with the CoreNLP client, a\nCoreNLPClient instance needs to be initialized,\nwith an optional list of CoreNLP annotators. After\nthe annotation is complete, results will be accessi-\nble as native Python objects.\nThis code snippet shows how to establish a\nCoreNLP client and obtain the NER and corefer-\nence annotations of an English sentence:\nfrom stanza.server import CoreNLPClient\n# start a CoreNLP client\nwith CoreNLPClient(annotators=[’tokenize’,’ssplit\n’,’pos’,’lemma’,’ner’,’parse’,’coref’]) as\nclient:\n# run annotation over input\nann = client.annotate(’Emily said that she\nliked the movie.’)\n# access all entities\nfor sent in ann.sentence:\nprint(sent.mentions)\n# access coreference annotations\nprint(ann.corefChain)\nWith the client interface, users can annotate text\nin 6 languages as supported by CoreNLP.\n3.3\nInteractive Web-based Demo\nTo help visualize documents and their annotations\ngenerated by Sta n z a , we build an interactive web\ndemo that runs the pipeline interactively. For all\nlanguages and all annotations Sta n z a provides in\nthose languages, we generate predictions from the\nmodels trained on the largest treebank/NER dataset,\nand visualize the result with the Brat rapid annota-\ntion tool.4 This demo runs in a client/server archi-\ntecture, and annotation is performed on the server\nside. We make one instance of this demo publicly\navailable at http://stanza.run/. It can also be\nrun locally with proper Python libraries installed.\n4https://brat.nlplab.org/\nFigure 3: Sta n z a annotates a German sentence, as vi-\nsualized by our interactive demo. Note am is expanded\ninto syntactic words an and dem before downstream\nanalyses are performed.\nAn example of running Sta n z a on a German sen-\ntence can be found in Figure 3.\n3.4\nTraining Pipeline Models\nFor\nall\nneural\nprocessors,\nSta n z a\nprovides\ncommand-line interfaces for users to train their\nown customized models. To do this, users need\nto prepare the training and development data in\ncompatible formats (i.e., CoNLL-U format for the\nUniversal Dependencies pipeline and BIO format\ncolumn ﬁles for the NER model). The following\ncommand trains a neural dependency parser with\nuser-speciﬁed training and development data:\n$ python -m stanza.models.parser \\\n--train_file train.conllu \\\n--eval_file dev.conllu \\\n--gold_file dev.conllu \\\n--output_file output.conllu\n4\nPerformance Evaluation\nTo establish benchmark results and compare with\nother popular toolkits, we trained and evaluated\nSta n z a on a total of 112 datasets. All pretrained\nmodels are publicly downloadable.\nDatasets.\nWe train and evaluate Sta n z a ’s tokeniz-\ner/sentence splitter, MWT expander, POS/UFeats\ntagger, lemmatizer, and dependency parser with\nthe Universal Dependencies v2.5 treebanks (Ze-\nman et al., 2019). For training we use 100 tree-\nbanks from this release that have non-copyrighted\ntraining data, and for treebanks that do not include\ndevelopment data, we randomly split out 20% of\nTreebank\nSystem\nTokens\nSents.\nWords\nUPOS\nXPOS\nUFeats\nLemmas\nUAS\nLAS\nOverall (100 treebanks)\nSta n z a\n99.09\n86.05\n98.63\n92.49\n91.80\n89.93\n92.78\n80.45\n75.68\nArabic-PADT\nSta n z a\n99.98\n80.43\n97.88\n94.89\n91.75\n91.86\n93.27\n83.27\n79.33\nUDPipe\n99.98\n82.09\n94.58\n90.36\n84.00\n84.16\n88.46\n72.67\n68.14\nChinese-GSD\nSta n z a\n92.83\n98.80\n92.83\n89.12\n88.93\n92.11\n92.83\n72.88\n69.82\nUDPipe\n90.27\n99.10\n90.27\n84.13\n84.04\n89.05\n90.26\n61.60\n57.81\nEnglish-EWT\nSta n z a\n99.01\n81.13\n99.01\n95.40\n95.12\n96.11\n97.21\n86.22\n83.59\nUDPipe\n98.90\n77.40\n98.90\n93.26\n92.75\n94.23\n95.45\n80.22\n77.03\nspaCy\n97.30\n61.19\n97.30\n86.72\n90.83\n–\n87.05\n–\n–\nFrench-GSD\nSta n z a\n99.68\n94.92\n99.48\n97.30\n–\n96.72\n97.64\n91.38\n89.05\nUDPipe\n99.68\n93.59\n98.81\n95.85\n–\n95.55\n96.61\n87.14\n84.26\nspaCy\n98.34\n77.30\n94.15\n86.82\n–\n–\n87.29\n67.46\n60.60\nSpanish-AnCora\nSta n z a\n99.98\n99.07\n99.98\n98.78\n98.67\n98.59\n99.19\n92.21\n90.01\nUDPipe\n99.97\n98.32\n99.95\n98.32\n98.13\n98.13\n98.48\n88.22\n85.10\nspaCy\n99.47\n97.59\n98.95\n94.04\n–\n–\n79.63\n86.63\n84.13\nTable 2: Neural pipeline performance comparisons on the Universal Dependencies (v2.5) test treebanks. For our\nsystem we show macro-averaged results over all 100 treebanks. We also compare our system against UDPipe and\nspaCy on treebanks of ﬁve major languages where the corresponding pretrained models are publicly available. All\nresults are F1 scores produced by the 2018 UD Shared Task ofﬁcial evaluation script.\nthe training data as development data. These tree-\nbanks represent 66 languages, mostly European\nlanguages, but spanning a diversity of language\nfamilies, including Indo-European, Afro-Asiatic,\nUralic, Turkic, Sino-Tibetan, etc. For NER, we\ntrain and evaluate Sta n z a with 12 publicly avail-\nable datasets covering 8 major languages as shown\nin Table 3 (Nothman et al., 2013; Tjong Kim Sang\nand De Meulder, 2003; Tjong Kim Sang, 2002;\nBenikova et al., 2014; Mohit et al., 2012; Taulé\net al., 2008; Weischedel et al., 2013).\nFor the\nWikiNER corpora, as canonical splits are not avail-\nable, we randomly split them into 70% training,\n15% dev and 15% test splits. For all other corpora\nwe used their canonical splits.\nTraining.\nOn the Universal Dependencies tree-\nbanks, we tuned all hyper-parameters on several\nlarge treebanks and applied them to all other tree-\nbanks. We used the word2vec embeddings released\nas part of the 2018 UD Shared Task (Zeman et al.,\n2018), or the fastText embeddings (Bojanowski\net al., 2017) whenever word2vec is not available.\nFor the character-level language models in the NER\ncomponent, we pretrained them on a mix of the\nCommon Crawl and Wikipedia dumps, and the\nnews corpora released by the WMT19 Shared Task\n(Barrault et al., 2019), except for English and Chi-\nnese, for which we pretrained on the Google One\nBillion Word (Chelba et al., 2013) and the Chi-\nnese Gigaword corpora5, respectively. We again\napplied the same hyper-parameters to models for\nall languages.\nUniversal Dependencies Results.\nFor perfor-\nmance on UD treebanks, we compared Sta n z a\n(v1.0) against UDPipe (v1.2) and spaCy (v2.2) on\ntreebanks of 5 major languages whenever a pre-\ntrained model is available. As shown in Table 2, St\na n z a achieved the best performance on most scores\nreported. Notably, we ﬁnd that Sta n z a ’s language-\nagnostic architecture is able to adapt to datasets of\ndifferent languages and genres. This is also shown\nby Sta n z a ’s high macro-averaged scores over 100\ntreebanks covering 66 languages.\nNER Results.\nFor performance of the NER com-\nponent, we compared Sta n z a (v1.0) against FLAIR\n(v0.4.5) and spaCy (v2.2). For spaCy we reported\nresults from its publicly available pretrained model\nwhenever one trained on the same dataset can be\nfound, otherwise we retrained its model on our\ndatasets with default hyper-parameters, follow-\ning the publicly available tutorial.6 For FLAIR,\nsince their downloadable models were pretrained\n5https://catalog.ldc.upenn.edu/\nLDC2011T13\n6https://spacy.io/usage/training#ner\nNote that, following this public tutorial, we did not use\npretrained word embeddings when training spaCy NER\nmodels, although using pretrained word embeddings may\npotentially improve the NER results.\nLanguage Corpus\n# Types Sta n z a FLAIR spaCy\nArabic\nAQMAR\n4\n74.3\n74.0\n–\nChinese\nOntoNotes\n18\n79.2\n–\n–\nDutch\nCoNLL02\n4\n89.2\n90.3\n73.8\nWikiNER\n4\n94.8\n94.8\n90.9\nEnglish\nCoNLL03\n4\n92.1\n92.7\n81.0\nOntoNotes\n18\n88.8\n89.0\n85.4∗\nFrench\nWikiNER\n4\n92.9\n92.5\n88.8∗\nGerman\nCoNLL03\n4\n81.9\n82.5\n63.9\nGermEval14\n4\n85.2\n85.4\n68.4\nRussian\nWikiNER\n4\n92.9\n–\n–\nSpanish\nCoNLL02\n4\n88.1\n87.3\n77.5\nAnCora\n4\n88.6\n88.4\n76.1\nTable 3: NER performance across different languages\nand corpora.\nAll scores reported are entity micro-\naveraged test F1. For each corpus we also list the num-\nber of entity types. ∗marks results from publicly avail-\nable pretrained models on the same dataset, while oth-\ners are from models retrained on our datasets.\non dataset versions different from canonical ones,\nwe retrained all models on our own dataset splits\nwith their best reported hyper-parameters. All test\nresults are shown in Table 3. We ﬁnd that on all\ndatasets Sta n z a achieved either higher or close F1\nscores when compared against FLAIR. When com-\npared to spaCy, Sta n z a ’s NER performance is much\nbetter. It is worth noting that Sta n z a ’s high per-\nformance is achieved with much smaller models\ncompared with FLAIR (up to 75% smaller), as we\nintentionally compressed the models for memory\nefﬁciency and ease of distribution.\nSpeed comparison.\nWe compare Sta n z a against\nexisting toolkits to evaluate the time it takes to an-\nnotate text (see Table 4). For GPU tests we use a\nsingle NVIDIA Titan RTX card. Unsurprisingly,\nSta n z a ’s extensive use of accurate neural models\nmakes it take signiﬁcantly longer than spaCy to\nannotate text, but it is still competitive when com-\npared against toolkits of similar accuracy, espe-\ncially with the help of GPU acceleration.\n5\nConclusion and Future Work\nWe introduced Sta n z a , a Python natural language\nprocessing toolkit supporting many human lan-\nguages.\nWe have showed that Sta n z a ’s neural\npipeline not only has wide coverage of human lan-\nguages, but also is accurate on all tasks, thanks\nto its language-agnostic, fully neural architectural\ndesign. Simultaneously, Sta n z a ’s CoreNLP client\nextends its functionality with additional NLP tools.\nTask\nSta n z a\nUDPipe\nFLAIR\nCPU\nGPU\nCPU\nCPU\nGPU\nUD\n10.3×\n3.22×\n4.30×\n–\n–\nNER\n17.7×\n1.08×\n–\n51.8×\n1.17×\nTable 4: Annotation runtime of various toolkits rela-\ntive to spaCy (CPU) on the English EWT treebank and\nOntoNotes NER test sets. For reference, on the com-\npared UD and NER tasks, spaCy is able to process 8140\nand 5912 tokens per second, respectively.\nFor future work, we consider the following areas\nof improvement in the near term:\n• Models downloadable in Sta n z a are largely\ntrained on a single dataset. To make mod-\nels robust to many different genres of text,\nwe would like to investigate the possibility of\npooling various sources of compatible data to\ntrain “default” models for each language;\n• The amount of computation and resources\navailable to us is limited. We would there-\nfore like to build an open “model zoo” for\nSta n z a , so that researchers from outside our\ngroup can also contribute their models and\nbeneﬁt from models released by others;\n• Sta n z a was designed to optimize for accuracy\nof its predictions, but this sometimes comes at\nthe cost of computational efﬁciency and lim-\nits the toolkit’s use. We would like to further\ninvestigate reducing model sizes and speed-\ning up computation in the toolkit, while still\nmaintaining the same level of accuracy.\n• We would also like to expand Sta n z a ’s func-\ntionality by adding other processors such as\nneural coreference resolution or relation ex-\ntraction for richer text analytics.\nAcknowledgments\nThe authors would like to thank the anonymous\nreviewers for their comments, Arun Chaganty for\nhis early contribution to this toolkit, Tim Dozat for\nhis design of the original architectures of the tagger\nand parser models, Matthew Honnibal and Ines\nMontani for their help with spaCy integration and\nhelpful comments on the draft, Ranting Guo for the\nlogo design, and John Bauer and the community\ncontributors for their help with maintaining and\nimproving this toolkit. This research is funded in\npart by Samsung Electronics Co., Ltd. and in part\nby the SAIL-JD Research Initiative.\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland Vollgraf. 2019.\nFLAIR: An easy-to-use framework for state-of-the-\nart NLP. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations). Asso-\nciation for Computational Linguistics.\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics. Associa-\ntion for Computational Linguistics.\nLoïc Barrault, Ondˇrej Bojar, Marta R. Costa-jussà,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias Müller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\nFindings of the 2019 conference on machine transla-\ntion (WMT19). In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1). Association for Computational\nLinguistics.\nDarina Benikova, Chris Biemann, and Marc Reznicek.\n2014.\nNoSta-D named entity annotation for Ger-\nman: Guidelines and dataset.\nIn Proceedings of\nthe Ninth International Conference on Language Re-\nsources and Evaluation (LREC’14).\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. Tech-\nnical report, Google.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biafﬁne attention for neural dependency pars-\ning. In International Conference on Learning Rep-\nresentations (ICLR).\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP natural lan-\nguage processing toolkit. In Association for Compu-\ntational Linguistics (ACL) System Demonstrations.\nBehrang Mohit, Nathan Schneider, Rishav Bhowmick,\nKemal Oﬂazer, and Noah A Smith. 2012. Recall-\noriented learning of named entities in Arabic\nWikipedia. In Proceedings of the 13th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics. Association for Computational\nLinguistics.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Hajiˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020.\nUniversal dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the Twelfth International Conference\non Language Resources and Evaluation (LREC’20).\nJoel Nothman, Nicky Ringland, Will Radford, Tara\nMurphy, and James R Curran. 2013. Learning mul-\ntilingual named entity recognition from Wikipedia.\nArtiﬁcial Intelligence, 194:151–175.\nPeng Qi, Timothy Dozat, Yuhao Zhang, and Christo-\npher D. Manning. 2018. Universal dependency pars-\ning from scratch. In Proceedings of the CoNLL 2018\nShared Task: Multilingual Parsing from Raw Text to\nUniversal Dependencies. Association for Computa-\ntional Linguistics.\nMilan Straka. 2018. UDPipe 2.0 prototype at CoNLL\n2018 UD shared task. In Proceedings of the CoNLL\n2018 Shared Task: Multilingual Parsing from Raw\nText to Universal Dependencies. Association for\nComputational Linguistics.\nMariona Taulé, M. Antònia Martí, and Marta Recasens.\n2008.\nAnCora: Multilevel annotated corpora for\nCatalan and Spanish. In Proceedings of the Sixth\nInternational Conference on Language Resources\nand Evaluation (LREC’08). European Language Re-\nsources Association (ELRA).\nErik F. Tjong Kim Sang. 2002.\nIntroduction to the\nCoNLL-2002 shared task: Language-independent\nnamed entity recognition.\nIn COLING-02: The\n6th Conference on Natural Language Learning 2002\n(CoNLL-2002).\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2013. OntoNotes release 5.0. Lin-\nguistic Data Consortium.\nDaniel Zeman, Jan Hajiˇc, Martin Popel, Martin Pot-\nthast, Milan Straka, Filip Ginter, Joakim Nivre, and\nSlav Petrov. 2018. CoNLL 2018 shared task: Mul-\ntilingual parsing from raw text to universal depen-\ndencies. In Proceedings of the CoNLL 2018 Shared\nTask: Multilingual Parsing from Raw Text to Univer-\nsal Dependencies. Association for Computational\nLinguistics.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Noëmi\nAepli, Željko Agi´c, Lars Ahrenberg, Gabriel˙e Alek-\nsandraviˇci¯ut˙e, Lene Antonsen, Katya Aplonova,\nMaria Jesus Aranzabe, Gashaw Arutie, Masayuki\nAsahara, Luma Ateyah, Mohammed Attia, Aitz-\niber Atutxa, Liesbeth Augustinus, Elena Badmaeva,\nMiguel Ballesteros, Esha Banerjee, Sebastian Bank,\nVerginica Barbu Mititelu, Victoria Basmov, Colin\nBatchelor, John Bauer, Sandra Bellato, Kepa Ben-\ngoetxea, Yevgeni Berzak, Irshad Ahmad Bhat,\nRiyaz Ahmad Bhat, Erica Biagetti, Eckhard Bick,\nAgn˙e Bielinskien˙e, Rogier Blokland, Victoria Bo-\nbicev, Loïc Boizou, Emanuel Borges Völker, Carl\nBörstell, Cristina Bosco, Gosse Bouma, Sam Bow-\nman, Adriane Boyd, Kristina Brokait˙e, Aljoscha\nBurchardt, Marie Candito, Bernard Caron, Gauthier\nCaron, Tatiana Cavalcanti, Gül¸sen Cebiro˘glu Ery-\ni˘git, Flavio Massimiliano Cecchini, Giuseppe G. A.\nCelano,\nSlavomír\nˇCéplö,\nSavas\nCetin,\nFabri-\ncio Chalub, Jinho Choi, Yongseok Cho, Jayeol\nChun, Alessandra T. Cignarella, Silvie Cinková,\nAurélie Collomb, Ça˘grı Çöltekin, Miriam Con-\nnor, Marine Courtin, Elizabeth Davidson, Marie-\nCatherine de Marneffe, Valeria de Paiva, Elvis\nde Souza, Arantza Diaz de Ilarraza, Carly Dicker-\nson, Bamba Dione, Peter Dirix, Kaja Dobrovoljc,\nTimothy Dozat, Kira Droganova, Puneet Dwivedi,\nHanne Eckhoff, Marhaba Eli, Ali Elkahky, Binyam\nEphrem, Olga Erina, Tomaž Erjavec, Aline Eti-\nenne, Wograine Evelyn, Richárd Farkas, Hector\nFernandez Alcalde, Jennifer Foster, Cláudia Fre-\nitas, Kazunori Fujita, Katarína Gajdošová, Daniel\nGalbraith, Marcos Garcia, Moa Gärdenfors, Se-\nbastian Garza, Kim Gerdes, Filip Ginter, Iakes\nGoenaga, Koldo Gojenola, Memduh Gökırmak,\nYoav Goldberg, Xavier Gómez Guinovart, Berta\nGonzález Saavedra, Bernadeta Grici¯ut˙e, Matias Gri-\noni, Normunds Gr¯uz¯ıtis, Bruno Guillaume, Céline\nGuillot-Barbance, Nizar Habash, Jan Hajiˇc, Jan Ha-\njiˇc jr., Mika Hämäläinen, Linh Hà M˜y, Na-Rae\nHan, Kim Harris, Dag Haug, Johannes Heinecke, Fe-\nlix Hennig, Barbora Hladká, Jaroslava Hlaváˇcová,\nFlorinel Hociung,\nPetter Hohle,\nJena Hwang,\nTakumi Ikeda, Radu Ion, Elena Irimia, O. lájídé\nIshola, Tomáš Jelínek, Anders Johannsen, Fredrik\nJørgensen, Markus Juutinen, Hüner Ka¸sıkara, An-\ndre Kaasen, Nadezhda Kabaeva, Sylvain Kahane,\nHiroshi Kanayama, Jenna Kanerva, Boris Katz,\nTolga Kayadelen, Jessica Kenney, Václava Ket-\ntnerová, Jesse Kirchner, Elena Klementieva, Arne\nKöhn, Kamil Kopacewicz, Natalia Kotsyba, Jolanta\nKovalevskait˙e,\nSimon Krek,\nSookyoung Kwak,\nVeronika Laippala, Lorenzo Lambertino, Lucia\nLam, Tatiana Lando, Septina Dian Larasati, Alexei\nLavrentiev, John Lee, Phương Lê H`ông, Alessandro\nLenci, Saran Lertpradit, Herman Leung, Cheuk Ying\nLi, Josie Li, Keying Li, KyungTae Lim, Maria Li-\novina, Yuan Li, Nikola Ljubeši´c, Olga Loginova,\nOlga Lyashevskaya, Teresa Lynn, Vivien Macke-\ntanz, Aibek Makazhanov, Michael Mandl, Christo-\npher Manning, Ruli Manurung, C˘at˘alina M˘ar˘an-\nduc, David Mareˇcek, Katrin Marheinecke, Héc-\ntor Martínez Alonso, André Martins, Jan Mašek,\nYuji Matsumoto, Ryan McDonald, Sarah McGuin-\nness,\nGustavo Mendonça,\nNiko Miekka,\nMar-\ngarita Misirpashayeva, Anna Missilä, C˘at˘alin Mi-\ntitelu, Maria Mitrofan, Yusuke Miyao, Simonetta\nMontemagni, Amir More, Laura Moreno Romero,\nKeiko Sophie Mori, Tomohiko Morioka, Shin-\nsuke Mori,\nShigeki Moro,\nBjartur Mortensen,\nBohdan Moskalevskyi, Kadri Muischnek, Robert\nMunro, Yugo Murawaki, Kaili Müürisep, Pinkey\nNainwani, Juan Ignacio Navarro Horñiacek, Anna\nNedoluzhko,\nGunta Nešpore-B¯erzkalne,\nLương\nNguy˜ên Thi., Huy`ên Nguy˜ên Thi. Minh, Yoshi-\nhiro Nikaido, Vitaly Nikolaev, Rattima Nitisaroj,\nHanna Nurmi, Stina Ojala, Atul Kr. Ojha, Adédayo.\nOlúòkun, Mai Omura, Petya Osenova, Robert\nÖstling, Lilja Øvrelid, Niko Partanen, Elena Pas-\ncual, Marco Passarotti, Agnieszka Patejuk, Guil-\nherme Paulino-Passos, Angelika Peljak-Łapi´nska,\nSiyao Peng, Cenel-Augusto Perez, Guy Perrier,\nDaria Petrova, Slav Petrov, Jason Phelan, Jussi\nPiitulainen, Tommi A Pirinen, Emily Pitler, Bar-\nbara Plank, Thierry Poibeau, Larisa Ponomareva,\nMartin Popel, Lauma Pretkalnin, a, Sophie Prévost,\nProkopis Prokopidis, Adam Przepiórkowski, Tiina\nPuolakainen, Sampo Pyysalo, Peng Qi, Andriela\nRääbis, Alexandre Rademaker, Loganathan Ra-\nmasamy, Taraka Rama, Carlos Ramisch, Vinit Rav-\nishankar, Livy Real, Siva Reddy, Georg Rehm, Ivan\nRiabov, Michael Rießler, Erika Rimkut˙e, Larissa Ri-\nnaldi, Laura Rituma, Luisa Rocha, Mykhailo Ro-\nmanenko, Rudolf Rosa, Davide Rovati, Valentin\nRosca, Olga Rudina, Jack Rueter, Shoval Sadde,\nBenoît Sagot, Shadi Saleh, Alessio Salomoni, Tanja\nSamardži´c, Stephanie Samson, Manuela Sanguinetti,\nDage Särg, Baiba Saul¯ıte, Yanin Sawanakunanon,\nNathan Schneider, Sebastian Schuster, Djamé Sed-\ndah, Wolfgang Seeker, Mojgan Seraji, Mo Shen,\nAtsuko Shimada, Hiroyuki Shirasu, Muh Shohibus-\nsirri, Dmitry Sichinava, Aline Silveira, Natalia Sil-\nveira, Maria Simi, Radu Simionescu, Katalin Simkó,\nMária Šimková, Kiril Simov, Aaron Smith, Isabela\nSoares-Bastos, Carolyn Spadine, Antonio Stella,\nMilan Straka, Jana Strnadová, Alane Suhr, Umut\nSulubacak, Shingo Suzuki, Zsolt Szántó, Dima\nTaji, Yuta Takahashi, Fabio Tamburini, Takaaki\nTanaka, Isabelle Tellier, Guillaume Thomas, Li-\nisi Torga, Trond Trosterud, Anna Trukhina, Reut\nTsarfaty, Francis Tyers, Sumire Uematsu, Zdeˇnka\nUrešová, Larraitz Uria, Hans Uszkoreit, Andrius\nUtka, Sowmya Vajjala, Daniel van Niekerk, Gert-\njan van Noord, Viktor Varga, Eric Villemonte de la\nClergerie, Veronika Vincze, Lars Wallin, Abigail\nWalsh, Jing Xian Wang, Jonathan North Washing-\nton, Maximilan Wendt, Seyi Williams, Mats Wirén,\nChristian Wittern, Tsegay Woldemariam, Tak-sum\nWong, Alina Wróblewska, Mary Yako, Naoki Ya-\nmazaki, Chunxiao Yan, Koichi Yasuoka, Marat M.\nYavrumyan, Zhuoran Yu, Zdenˇek Žabokrtský, Amir\nZeldes, Manying Zhang, and Hanzhi Zhu. 2019.\nUniversal Dependencies 2.5. LINDAT/CLARIAH-\nCZ digital library at the Institute of Formal and Ap-\nplied Linguistics (ÚFAL), Faculty of Mathematics\nand Physics, Charles University.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-03-16",
  "updated": "2020-04-23"
}