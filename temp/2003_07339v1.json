{
  "id": "http://arxiv.org/abs/2003.07339v1",
  "title": "Reinforcement Learning for Electricity Network Operation",
  "authors": [
    "Adrian Kelly",
    "Aidan O'Sullivan",
    "Patrick de Mars",
    "Antoine Marot"
  ],
  "abstract": "This paper presents the background material required for the Learning to Run\nPower Networks Challenge. The challenge is focused on using Reinforcement\nLearning to train an agent to manage the real-time operations of a power grid,\nbalancing power flows and making interventions to maintain stability. We\npresent an introduction to power systems targeted at the machine learning\ncommunity and an introduction to reinforcement learning targeted at the power\nsystems community. This is to enable and encourage broader participation in the\nchallenge and collaboration between these two communities.",
  "text": "Reinforcement Learning for Electricity Network\nOperation\nLearning to Run a Power Network 2020 Challenge - White Paper\nAdrian Kelly (Electric Power Research Institute - EPRI)\nAidan O’Sullivan, Patrick de Mars (University College London - UCL),\nAntoine Marot (RTE - R´eseau de Transport d’Electricit´e)\nMarch 2020\n1\nIntroduction to the Challenge\nThe goal of this challenge is to test the potential of Reinforcement Learning\n(RL) to control electrical power transmission, in the most cost-eﬀective manner,\nwhile keeping people and equipment safe from harm. Solving this challenge may\nhave very positive impacts on society, as governments move to decarbonize the\nelectricity sector and to electrify other sectors, to help reach IPCC climate\ngoals. Existing software, computational methods and optimal powerﬂow solvers\nare not adequate for real-time network operations on short temporal horizons in\na reasonable computational time. With recent changes in electricity generation\nand consumption patterns, system operation is moving to become more of a\nstochastic rather than a deterministic control problem. In order to overcome\nthese complexities, new computational methods are required.\nThe intention\nof this challenge is to explore RL as a solution method for electricity network\ncontrol.\nThere may be under-utilized, cost-eﬀective ﬂexibility in the power\nnetwork that RL techniques can identify and capitalize on, that human operators\nand traditional solution techniques are unaware of or unaccustomed to.\nAn RL agent that can act in conjunction, or in parallel with human network\noperators, will optimize grid security and reliability, allowing more renewable\nresources to be connected while minimizing the cost and maintaining supply to\ncustomers, and preventing damage to electrical equipment.\nAnother aim of the project is to broaden the audience for the problem of\nelectricity network control and to foster collaboration between experts in both\nthe power systems community and the wider RL/ML community.\n1.1\nThe Importance of Electricity\nElectricity is an underrated, and under-appreciated foundational element of\nmodern life. With the invention of electricity networks in the early 20th century,\n1\narXiv:2003.07339v1  [eess.SP]  16 Mar 2020\nelectricity can be considered one of the most important factors for the growth\nof economies and the functioning of societies around the world. It is a vital\nresource in daily living, industry, agriculture and public transportation. The\nelectriﬁcation of developing countries and rural communities has helped to lift\nbillions of people out of poverty. Blackouts of cities or countries, when they\noccur, are catastrophic events, which can cause chaos, and have a major im-\npact on modern life. Recent high proﬁle blackouts in Great Britain, Argentina,\nNew York and Australia show the importance electricity has in society and the\ndisruption it can cause when it is disconnected. Electricity network operators,\nacross the world, must ensure a constant, reliable, secure, safe, cost-eﬀective\nand sustainable supply of electricity is maintained, while preventing blackouts.\nTo contextualize, an electric kettle uses abut 1500 Watts (W) of power when\nswitched on. If switched on for 1 hour it would use 1.5 kilowatt-hours (kWh) of\nenergy. In 2018 the average U.S. home used approximately 11 Megawatt-hours\n(MWh) of energy in the year, or 11,000,000 watt-hours 1. In 2017 the total\nenergy produced by electricity in the entire world was approximately 25,721\nTWh or 25,721 trillion watt-hours 2. Controlling electricity - from generation\nsources to end-users’ kettles - is an extremely complex task. Meeting worldwide\nelectricity demand, while electrifying sectors of society and developing countries,\nwith relatively few blackouts, is why the electricity network has been described\nas the greatest human-made machine ever built. In 1974 total global electricity\nuse was approximately 5 TWh. Electricity use has increased four-fold in less\nthan 40 years. See Figure 1 for a chart showing the growth of electricity usage\nsince 1974 broken down by end-use application. In recent years the diﬃculty\nmaintaining this level of security has increased. This challenge aims to help\nnetwork operators to control the networks and to guide the decision making of\nnetwork operators in real-time operations and in operational planning.\n2\nIntroduction to Electricity and Power Net-\nworks for the Machine Learning Community\nIn this section, there is a concise introduction to electricity concepts as they\nrelate to the challenge with the aim of opening up the challenge up to the\nbroader ML/RL community.\nNote: There will be a signiﬁcant simpliﬁcation of concepts and terms for the\nbeneﬁt of brevity and to emphasize the most important aspects to non-expert\nreaders. For a more comprehensive exposition of the theory and principles of\nelectricity and power systems please refer to [4] and [12].\n1\nSource:\nUS\nEnergy\nInformation\nAdministration\nSee:\nhttps://www.eia.gov/tools/faqs/faq.php?id=97&t=3\n2Source: International Energy Agency See: https://www.iea.org/data-and-statistics\n2\nFigure\n1:\nWorld\nelectricity\nusage\ngrowth\nby\nsector\nfrom\n1974-2017.\nSource:\nIEA\n(2019),\n”Electricity\nInformation\n2019”,\nIEA,\nParis\nhttps://www.iea.org/reports/electricity-information-2019 [1].\n2.1\nThe Basic Physics of Electricity\nElectricity is a form of energy involving the excitement of electrons in metallic\nelements. This phenomenon is not unique to electricity networks, but is also\ncritical to life itself, as our bodies are dependent on electrical pulses for the func-\ntioning of our muscles, cells and nerves. In order to develop an understanding\nof electricity, it is necessary to introduce the fundamental dimension of physi-\ncal measurement electric charge. Charge is a property of matter arising from\natomic structure which is made up of protons (positively charged), electrons\n(negatively charged) and neutrons (neutral). It is measured in coulombs (C), a\ncharge equal to that of 6.25 × 1018 protons.\nCharge induces a force with opposite charges attracting and the same charges\nrepelling. This force creates the ability to produce work and the electric poten-\ntial or voltage, which is the potential energy possessed by a charge at a location\nrelative to a reference location. It is deﬁned between two points and measured\nin Volts, denoted with the symbol V. An electric current is a ﬂow of charge\nthrough a material, measured in Coulombs per second or Amperes (A) and de-\nnoted with the symbol I. (While described as a ﬂow, nothing new is actually\ncreated or moves, current ﬂow in the electricity context means the excitement\nof electrons in metallic conductors to induce a voltage and produce charge along\nthe metallic conductor).\nThe electrical power is given as the product of the voltage and the current.\nP = V I\n(1)\n3\nPower is measured in Watts, denoted by the symbol W, see section 1.1 for\nhow this is related to everyday electricity usage. In order to try to simplify\nthese electrical concepts, an analogy with a physical water system is often used,\nwhile not quiet directly analogous, the current is similar to the ﬂow of water\nin a pipe, say in litres per second.\nVoltage would be analogous to a height\ndiﬀerence, say between a water reservoir and the downhill end of the pipe, or a\npressure diﬀerence. Intuitively, voltage is a measure of ‘how badly the material\nwants to get there’ and current is a measure of ‘how much material is actually\ngoing’. Power would be analogously produced by the force of water spinning\na hypothetical turbine that may rotate a wheel. Intuitively these phenomena\nare related, increasing the voltage or current in a system increases the power\nproduced. Electrically, this relationship is captured by Ohm’s law:\nV = IR\n(2)\nA new variable is introduced here - R - which is the resistance of the mate-\nrial the current is ﬂowing through, analogous to the size of the water-pipe. A\nsmaller pipe makes it harder for large ﬂows and it is the same with current -\nhighly conductive materials allowing current to ﬂow easily and poorly conduc-\ntive materials (called insulators) preventing current from ﬂowing. Whenever\nan electric current exists in a material with resistance it will create heat. The\namount of heating is related to the power P and combining equations (1) and\n(2) gives:\nP = I2R\n(3)\nHeating can be desirable. Heating a resistive element is how an electric kettle\nor heater works. It can also be undesirable - as is the case of power lines - where\nthe heat is energy lost and causes thermal expansion of the conductor making\nthem sag, or fall close to the ground or to people or buildings.\nIn extreme\ncases, such as a fault condition, thermal heating can melt the wires. As we\nsee from Equation (3) the amount of heating is proportional to the square of\nthe current, so increasing the current has a large eﬀect on the resistive losses.\nIt is for this reason that when electricity is transported over long distances, it\nis done at high voltages. Based on Equation (2), assuming that the resistance\nof the line remains constant, to transport the same amount of power, resistive\nlosses are reduced by increasing the voltage and lowering the current. This is\nthe fundamental concept of electricity transmission.\nIn order to produce a sustained ﬂow of current, the voltage must be main-\ntained on the conductor. This is achieved by providing a pathway to recycle\ncharge to its origin and a mechanism, called an electromotive force (emf), that\ncompels the charge to return to its original potential. Such a setup constitutes\nan electric circuit. Again, to oversimplify by relating back to the water analogy\n- if there is an open pipe in the circuit water will run out. Likewise, if there is a\nbreak in an electric circuit, current will not ﬂow but voltage will still be present\non the conductor. Simple electric circuits are often described in terms of their\nconstituent components; voltage sources, conductors and resistances. Complex\n4\nFigure 2: An example of the dangers of overheating power-lines, by transporting\ntoo much current, the metallic conductor heats and sags close to the ground\ncausing a ﬂash over to ground and endangering human life.\npower networks can be described in terms of generation sources, network lines\nand loads.\nA simple electrical power network analogous to a simple electric\ncircuit is shown in Figure 3.\nFigure 3: A simple electricity network, showing the circuit nature of a power\nnetwork, the currents I ﬂowing in the lines and the interconnectedness between\ngenerators denoted g, customer loads denoted c and substation nodes denoted\ns.\nCircuit analysis is the goal of estimating the parameters in a circuit given\na combination of the voltages, currents and resistances and the fundamental\nEquations (1), (2) and (3).\nThe more complex the circuit or network, the\nmore complex the analysis will be. Within a circuit, a series of laws known as\nKirchhoﬀ’s law also help us in the analysis:\n5\n• Kirchhoﬀ’s voltage law: voltage around any closed loop sums to zero\n• Kirchhoﬀ’s current law: current entering and exiting any node sums to\nzero\nThese principles can be applied at the micro-level to simple circuits, such\nas plugging in an electric kettle where the element is the resistor or load, the\nmains outlet is the voltage source and current is proportional to the voltage and\nresistance of the circuit. Voltage is maintained throughout the circuit when it\nis plugged in and current ﬂows from the plug outlet through the wire, into the\nheating element and back to the plug outlet, completing the circuit.\nThese concepts can also be applied at the macro level, where a house or town\ncould be considered the load and a nuclear power station could be considered\nthe voltage and current source, which is interconnected to the load by power\nlines. The electricity network is one large circuit, which is constantly satisfying\nthese laws.\n2.2\nPower Generation, Transmission and Consumption\nNow that the theoretical background on the physics of electricity has been out-\nlined at a high level, the discussion can move to how power is produced and\ntransported and consumed in national power networks (also referred to as grids).\n2.2.1\nPower Generation\nOn a network, power is provided from multiple diﬀerent technologies using dif-\nferent fuels which are all referred to as generators. They can be considered as\nsources in the power network. Traditionally power was generated by large, ther-\nmal units burning fossil fuels such as coal, oil and gas. These large generators\nwere co-located with load, in cities and towns, thereby reducing the distance\nthat power needed to be transmitted. In recent years due to the shifts in policy\nto decarbonize society and in the liberalization of electricity markets, gener-\nation sources have shifted to renewable, unpredictable, weather-based sources\nsuch as wind and solar. These sources are often installed in geographically di-\nverse and less populated areas and produce power far away from load centres.\nHydro and nuclear power stations, while not new, are carbon-free and are also\nlocated relatively far away from load centres. The network needs to be planned\nand operated in diﬀerent ways, to incorporate geographically disperse, variable\ngeneration sources and ensure that power is eﬃciently transmitted to the load\ncentres at all times. See Figure 4 for pictorial examples of generation sources.\n2.2.2\nTransmission\nWhen power networks ﬁrst came into existence there was a great controversy\nover what the best approach to the transmission of electricity might be. This\nbecame known as the war of the currents and was played out with the main\nprotagonists; Thomas Edison who supported DC transmission (Direct Current)\n6\nFigure 4: Examples of generation sources, a thermal power generator station\nand at the bottom, solar panels and wind turbines.\nversus George Westinghouse and Nikola Tesla who supported AC transmis-\nsion (Alternating Current). AC power systems were technically superior and\nemerged the victor from this battle and all power networks in the world work\non AC principles. Both AC and DC allow transmission at high voltage (thus\nreducing current), but the main reason for using AC (Alternating Current) in\npower networks is that it allows the raising and lowering of voltages using power\ntransformers. Being able to increase the voltage allows us to transmit electricity\ngreater distances due to the lower resistive heating losses, discussed in Section\n2.1. However, the voltage tends to vary based on the load on the network at\nparticular times.\nNetwork operators must maintain the voltage at its nomi-\n7\nnal levels at all times on the transmission network. The transmission network\nis made up of transmission equipment, most importantly, overhead lines and\nunderground cables which interconnect with substations. The substations con-\ntain the connections to the generation sources and the transformers to step up\nand step down the voltage to distribution systems. See Figure 5 for a simple\nschematic of the transmission grid. DC is a simpler linear system and is easier\nto understand, model and simulate, given there are no time-varying aspects.\nAC is more diﬃcult as it introduces non-linearities based on sinusoidal aspects\nof voltage and current generation, three-phase transmission and mathematics in\nthe complex domain. The RL challenge will run on an AC powerﬂow, but the\nimportant aspects of AC powerﬂow can be approximated by the linear equations\nof DC powerﬂow, with not much of a loss of accuracy of the challenge objective.\nUnderstanding DC powerﬂow is a good starting point to understanding power\nnetwork control, and it is not necessary to have a deep understanding of AC\npowerﬂow to participate in the RL challenge.\nFigure 5: Graphical illustration of power networks from generation to transmis-\nsion to consumption. Source: Wikipedia.\n2.2.3\nPower Consumption\nWhen power is transmitted to substations, it is at too high a voltage for con-\nsumers such as homes and businesses to use. Transformers must be used to\nstep-down the voltage to a low enough level for connection to distribution sys-\ntems. Power is transmitted at between 100,000 - 760,000 volts. The voltage\nin homes is 220 Volts in Europe and 120 Volts in North America. From the\npower network viewpoint, power consumption is aggregated as load - calculated\nat the Megawatt level, and the power network operator’s role generally ends\nwhen power is delivered to the step-down transformer.\n8\n2.3\nNetwork Operation\n2.3.1\nHow a Network Operates\nThe power network is operated by ensuring the three primary constraints are\nmet at all times, in all areas of the network.\n• Thermal limits of transmission equipment are not breached (measured in\ncurrent with units of Amperes (A) or power with units MegaWatts (MW)).\n• Voltage maintained within a deﬁned range (measured in voltage, units of\nVolts (V)).\n• Generation and load balanced at all times (measured in power, units of\nMegawatts (MW). The balance between load and generation is approxi-\nmated by frequency measured in Hertz (Hz).\n2.3.2\nThermal Limits of Transmission Equipment\nPower will ﬂow from source to load, around the network based on the resistance\nof the lines in the network. A transmission line has an upper limit to the amount\nof power that can ﬂow through it before it will fail in service. This limit is given\nby the thermal properties of the metallic materials, usually copper or aluminium\nand also cooling and heating due to weather conditions (such as wind, irradiance\nand ambient temperature). If too much power is forced through the equipment\nfor a long period, and the thermal limits are breached, the equipment is likely\nto fail in service and be disconnected. In reality, this means overhead lines sag\ncloser to the ground, and may cause ﬂashover as shown in Fig. 2 (the cause\nof the 2003 blackout in North America) or very expensive equipment such as\ntransformers or cables will be damaged and explode. It is better to disconnect\nthe line than let it sag close to the ground. When the line is disconnected,\nthe same amount of power is still present on the network, but one link has been\nremoved. This means that the power will reroute itself to the new most desirable\npath based on the resistance, but this rerouting may result in another line or\nlines being overloaded. The challenge of network operation (and the basis of\nthe RL challenge) is to route the power around the network in the most eﬃcient\nmanner, while avoiding overloads and cascading eﬀects.\n2.3.3\nVoltage\nOne of the key concepts of transmission is to step up and down the voltages on\nthe network to optimize powerﬂow. Because of this, the transmission network\nsubstation nodes must be maintained at, or near the nominal voltage of the\ntransmission equipment. If the voltage is not maintained, it will collapse and\ncause the system to blackout. Voltage collapse has been the cause of major\nnetwork disturbances around the world.\nWhile not discussed in detail here,\nvoltage is maintained by the reactive power balance on the network. Generation\nsources and power electronic devices produce reactive power and loads (such as\n9\nmotors) absorb reactive power. Transmission equipment like lines and cables\nalso produce and consume reactive power for eﬀective transmission of active\npower. The voltage of the system can vary around nominal values, it can be\nwithin 90 % - 110 % of its nominal at any time. For the RL challenge; voltage will\nnot have to be optimized by the agent and will be controlled on the environment\nside, owing to the non-linear complexity that would have to be introduced into\nthe problem.\nVoltage control may be introduced into future versions of the\nchallenge.\n2.3.4\nGeneration Load Balance and Frequency\nOn power networks, the third constraint to consider is that generation produced\nby sources and the power consumed by loads must be equal at all times. The\nproxy measure for how balanced the system is the system frequency. The nom-\ninal frequency in Europe is 50 Hz and in North America, it is 60 Hz. If there is\ntoo little generation relative to demand, the frequency will drop below nominal.\nIf there is too much generation relative to demand the frequency will rise above\nnominal. If the frequency deviates too far beyond nominal (approximately 2\nHz) it will likely collapse the entire grid. Generation and demand should be\ncontrolled to maintain the delicate balance at all times between generation and\nload and to maintain a balanced system frequency.\nFor the most part, this\nmeans dispatching generation sources to meet the load demanded by end-users.\nFor the RL challenge, the generation load balance will not have to be optimized\nand will be managed on the environment side of the model. Balancing of genera-\ntion and load may be introduced into future versions of the challenge. However,\ndispatching generation sources is a useful action for power network control that\ncan be utilized to alleviate line overloads in the RL challenge.\n2.3.5\nNetwork Impedance\nReturning to the concepts of circuit theory and the equations deﬁned in Section\n2, the pattern of powerﬂow around power network is governed by the resistance\nof the interconnecting lines. All equipment has a resistance, as well as a thermal\nlimit as described above. Power cannot be created from a speciﬁc source and\ndirectly dispatched to a speciﬁc load. Power ﬂows relative to the resistance of\nthe lines connected between the generation sources and the loads, and it tends to\ntake the path of least resistance. The lower the resistance, the more power will\nﬂow through the element. The line resistances can be considered to be constant\nand cannot be altered by operators in real-time and so is unlike thermal limits as\nit does not vary. However, the ﬂows on the network can be altered by changing\nthe topology of the grid by control actions such as by switching lines in and out.\n2.3.6\nNetwork States and Control\nThe network can be considered to be:\n• Interconnected\n10\n• Switchable\n• Static\nInterconnected means it is meshed and not radial.\nThis interconnectedness\nallows a large amount of potential network states. Switchable means that most\nnetwork elements have two states, i.e.\nlines can be set out-of-service or in\nservice and substations can be split or coupled. Static means that for network\noperators, new interconnected elements cannot be created in real-time.\nFor\nexample in Figure 6 in the operations timeframe, a new line cannot be created\nbetween S2 to S5. But, with a limited amount of switching controls an indirect\nlink can be created in real-time.\nFigure 6: A simplistic network on the left, showing a simple network state,\nwith no direct link between S1 and S4 since the dotted line indicates the line\nis switched out. The ﬁgure on the right shows a splitting action at substation\nnode S3 which now creates an indirect link between node S1 and S4.\n2.4\nNetwork Security\nSecure operation of power networks is required under normal operating state as\nwell as in contingency states. The following requirements must be met:\n• In the normal operating state, the power ﬂows on equipment, voltage and\nfrequency are within pre-deﬁned limits in real-time.\n• In the contingency state the power ﬂows on equipment, voltage and fre-\nquency are within pre-deﬁned limits after the loss of any single element\non the network.\nThe network must be operated to be secure in the “normal state”, i.e.\nthe\nthermal limits on equipment must not be breached, the voltage must be within\n11\nrange and generation and load must be balanced. In the context of the RL chal-\nlenge, the network must be operated such that no thermal limits on any lines\nare breached. The network must also be secure for any contingency, usually the\nloss of any element on the network (a generator, load, transmission element).\nLoss of elements can be anticipated (scheduled outages of equipment) or unan-\nticipated (faults for lightning, wind, spontaneous equipment failure). Cascading\nfailures must be avoided to prevent blackouts, this is the game over state in the\nRL challenge. The removal of equipment by protection equipment is in the mil-\nlisecond time domain and is not considered in this challenge. Constraints on\nthe system, such as line thermal overloads are alleviated in real-time by network\noperators using a range of remedial actions, from least to most costly as follows:\n• Switching lines on the network in or out\n• Splitting or coupling busbars at substations together. This means a node\ncan be split into two elements or connected together as a single element\n• Redispatch generation to increase or reduce ﬂows on lines\n• Disconnecting load\nFrom a cost perspective, the disconnection of load should be avoided due to\nthe disruption to society, business and daily life.\nRedispatching generation\ncan also be expensive. The electricity is managed by a market, based on the\ncost per unit of energy supplied. If the network operators need to redispatch\nexpensive generation, this can be sub-optimal from a market perspective and\ncause increased costs to customers.\nTo provide operational ﬂexibility, substations are usually designed so that they\ncan be separated into two or more constituent parts. Coupling a substation can\nserve to reroute power in a network and is an option to alleviate line overloads.\nSwitching lines and coupling busbars at substations are the least costly option to\nalleviate thermal overloads on the network. There is considerable operational\nﬂexibility that is under-utilized on power networks that can be released by\nswitching actions and topology changes.\nThis network ﬂexibility is easy to\nimplement and the least costly option. One of the goals of the RL challenge\nis to explore the range of switching options available and to utilize topology\nchanges to control power on the network.\n2.4.1\nTemporal Constraints\nThere are also temporal elements to network operation, so network operation\ncan be considered a time-domain challenge. The lines can be overloaded for\nshort periods of time, while a solution to the issue is found. It is rare that an\ninstantaneous overload will cause an instantaneous disconnection. The load and\nrenewable generation constantly change and the system must be secure at all\n12\ntimes. The system must be operated so that load and generation are always in\nbalance. In winter the peak load in the day might be at 6 PM to align with\nlighting, oven and heating load but some generation sources are ineﬀectual for\nthis, in winter the sun may not shine at 6 PM, so solar generation is eﬀectively\nuseless at peak demand in winter. Large generation units cannot instantaneously\nconnect and are dependent on the heat state of the metallic materials. Outages\nof equipment are scheduled, such as outages for maintenance or replacement or\nunscheduled such as spontaneous disconnections from lightning or other faults.\n2.5\nThe Role of a Network Operator\nThe transmission network is controlled from a control centre, with remote ob-\nservability from this centre to all transmission network elements. The network\noperators can control most network elements such as lines and substations via\nremote control command. The control centre also has visibility of all the gen-\neration sources and all the loads. Generation is controlled by the control centre\noperator sending dispatch instructions to change the outputs. Some loads can\nbe controlled by the control centre, but, in general, the distribution system\noperators control switching of the load. For small to medium-sized countries,\nusually, there is one control centre with responsibility for network control but\nfor larger countries like the USA, Canada there are multiple control centres that\ncontrol the network at a state or regional level on a functional basis. These con-\ntrol centres coordinate their activities with their neighbouring control centres.\nThe network operator’s role is to monitor the electricity network 24 hours per\nday, 365 days per year. The operator must keep the network within its thermal\nlimits, its frequency ranges and voltage ranges for normal operation and contin-\ngency state operation as described above. For normal operation, the operator\nhas a range of actions at their disposal to manage the network within its con-\nstraints, such as switching, generator dispatch and load disconnection. For the\ncontingency state operation, the operator must act ahead of time to mitigate\ncontingencies that may occur for the unexpected loss of any single element, us-\ning the prescribed range of actions. The operator must also plan the network\noperation for the loss of any element for a scheduled outage for maintenance.\nThe network must operate securely for the entirety of the planned outage, not\njust for the moment the outage is taken. The operator must plan for and manage\nthe network within its limits at the system peak, i.e. the largest load demand\nof the day, the generation must be managed so that the generation load balance\n(measured by the frequency) is maintained at the peak of the day.\n2.6\nHow Can RL Beneﬁt Network Operators\nToday on power networks around the world, the ﬂexibility that topological\nchanges to the network oﬀers is an underexploited and low-cost option to main-\ntaining network security. The issue is that the range of options is impossible\nto simulate in real-time and in the operations planning time frame, with the\n13\nexisting operator model and simulation toolkit. Typically operators will revert\nto their mental model and past experience for solutions to network problems\nwhen they arise. This worked in the past with a static network and predictable\nsystem operations. The new network operates diﬀerently and contains unpre-\ndictable resources, which can dramatically alter the ﬂow patterns in the network.\nThe deterministic study tools will no longer be adequate and new solutions for\ncontrolling the network, such as RL, will be required.\n3\nIntroduction to Reinforcement Learning for\nPower Systems Community\nIn this section, we give a brief overview of the ﬁeld of reinforcement learning\n(RL) and how it applies to the Learning to Run Power Network challenge.\nNote: there will be a signiﬁcant simpliﬁcation of concepts and terms for the\nbeneﬁt of brevity and to emphasize the most important RL aspects for power\nsystem experts to understand. For a more comprehensive introduction to rein-\nforcement learning please refer to [10].\nMachine learning is often characterised as consisting of three diﬀerent branches:\nsupervised learning (learning from labelled data), unsupervised learning (ﬁnd-\ning patterns in unlabelled data) and reinforcement learning, where an agent\ninteracts with an environment to earn a reward. RL is analogous in many ways\nto how humans learn, through interaction with the real world through trial\nand error and feedback. Positive rewards encourage (or reinforce) good actions,\nwhile poor actions are penalised. RL provides a means of ﬁnding new strategies\nthat improve on existing performance for a wide range of tasks. RL has had\ngreat success in achieving superhuman performance in games-playing, beating\nexpert human players at some of the most complex and challenging games like\nGo [8, 9]. RL has also been used for learning the neuromuscular movement of a\nhuman-like agent in the previous NeurIPS challenge Learning To Run [5], which\ninspired this challenge.\n3.1\nReinforcement Learning Problem Formulation\nAt a high level, a RL problem consists of an agent interacting with an envi-\nronment.\nThe agent is governed by a decision-making policy: that is, some\nfunction (possibly stochastic) which determines what the agent should do in\na given state. The environment is typically described by a Markov Decision\nProcess with the following components:\n• States: snapshots of the environment; observed by the agent\n• Actions: means by which the agent can interact with its environment\nand receive a reward\n14\n• Reward function: determines (sometimes probabilistically) the reward\nresulting from action a taken in state s\n• Transition function: determines (sometimes probabilistically) the state\nthat results from taking an action a in state s\nThese components and workﬂow are summarized graphically in Figure 7. As\nthe agent interacts with the environment through actions, the environment may\nchange state, governed by the transition function. Rewards are observed that\ndepend on both the action and the state in which it was taken. The task of an\nRL problem is to learn a policy π(s) which maximises the long-run expected\nreward. In other words, the agent aims to maximise:\nX∞\nt=0γtr(st, at)\n(4)\nwhere at is sampled from π(st). Note that we have included a discount factor\nγ: while some problems have a termination state (known as episodic problems),\nothers may continue indeﬁnitely and we need to discount the rewards to prevent\nlong-run rewards from increasing unbounded. In episodic cases, we allow γ ≤1,\notherwise γ < 1.\nFigure 7: A reinforcement learning problem consists of an agent in an environ-\nment and states, actions and rewards [10].\n3.1.1\nSolution methods\nThe previous section described the general formulation of RL problems. While\na comprehensive discussion of methods is beyond the scope of this white paper,\nsome key concepts and characteristics of algorithms for solving such problems\nare introduced below. For a thorough treatment of RL methods, see [10].\nA large number of RL algorithms depend on the estimation of a value func-\ntion. More precisely, the state-value function (typically vπ(s)) gives the expected\nreturn for following a policy π from state s, while the action-value function\n(typically qπ(s, a)) gives the expected return for taking action a in-state s and\nfollowing π thereafter. Central to many RL algorithms is policy iteration, where\nthe policy is repeatedly updated to the value function, while the value function\nis updated to match the policy.\nOn the other hand, policy optimisation methods, where the policy πθ(a|s)\nis some function (e.g. a neural network) which directly maps from states to\n15\nactions, with parameters θ. The task is, therefore, to ﬁnd parameters θ giving\nthe best policy. A variety of methods can be employed to estimate the gradient\nof the reward function with respect to the parameters and optimise the policy\n[13, 11, 2].\nAmong the principal challenges in many RL problems is the long time de-\npendencies connecting actions and rewards. For instance, in zero-sum games\nsuch as chess and Go, the reward function is usually designed as follows:\nr(s, a) =\n\n\n\n\n\n1,\nif agent has won\n−1,\nif agent has lost\n0,\notherwise\nMoves early on are not rewarded until the very end of the game, leading to\nthe well-known credit assignment problem to correctly attribute good actions to\nlater rewards.\nLastly, there is a noteworthy distinction between model-based and model-\nfree methods. In model-based methods, the agent estimates the transition and\nreward functions, sometimes learning them from experience (Figure 8). Model-\nbased methods include those used successfully in the AlphaGo algorithms [8, 9],\nmaking use of tree search planning.\nFigure 8: Model-based RL methods use estimates of the transition and reward\nfunctions to improve the policy or value functions [10].\nThere is no ‘silver bullet’ RL method which is capable of achieving expert\nperformance for all problems. Domain knowledge is often an important contri-\nbution to developing an RL algorithm for a given problem: it is for this reason\nthat power systems researchers are encouraged to participate in the L2RPN\nchallenges.\n16\n3.1.2\nFormulating the Network Operation Problem\nIn the electricity network context, the network operator aims to maintain a\nsecure network state at the lowest cost. Formulation of the network operation\nproblem depends in part on the actions and state information available to the\noperator, and in this section, the formulation in general terms is described. For\na more detailed formulation that is relevant to this challenge, see [3].\nFor RL, the network operation problem is usually described in a Markov\nDecision Process. In this case, the agent is the network operator, capable of\nperforming actions on the environment, which is the power network itself. Using\nthe MDP components described previously, a general description of the power\nnetwork operation problem is given:\n• States: describes the physical network, including load, generation, line\nconnections, time etc.\n• Actions: options to reconﬁgure the network (topological changes) or re-\ndispatch generation.\n• Reward function: depending on the problem context, measuring eco-\nnomic, environmental or security costs/performance.\n• Transition function: determines the new state of the network following\nactions taken by the agent.\nThe operator aims to determine a policy that maximises the long-run reward\nin equation (4). Several aspects of the MDP may be customised by the operator\nin order to create a simpler approximation of the problem. For instance, the\naction space may be reduced by considering only those actions which alter the\nstatus of a single node, avoiding the action space increasing exponentially in the\nnumber of buses. Similarly, the reward function is designed by the operator:\nwhile it should generally reﬂect the ultimate goal of the task (in the L2RPN\nchallenges, this is the function by which participants are scored), a hand-crafted\nreward function may result in better performance in practice.\nHaving described in general terms the formulation of the network operation\nproblem for solving with RL, in the next section L2RPN challenges developed\nby this working group is outlined.\n4\nLearning to Run a Power Network Challenges\nMotivated by the pressing challenges facing network operators and the recent\nsuccesses of RL, the Learning to Run a Power Network (L2RPN) challenges were\noriginally developed by R´eseau de Transport d’´Electricit´e (RTE), the French\ntransmission network operator, to promote investigation into the network op-\neration problem in a competitive context. A larger working group comprising\nresearchers from a range of institutions has sought to build on the initial suc-\ncesses of L2RPN to create more sophisticated, diﬃcult and practically relevant\n17\nFigure 9: An illustration of the IEEE-14 simulation environment used for the\nL2RPN 2019 challenge.\nchallenges. Researchers from the machine learning and power systems commu-\nnities are encouraged to develop RL agents capable of operating a simulated\npower network securely and at low cost.\nGiven that RL methods rely on trial-and-error to improve the decision-\nmaking policy, interaction with the physical power network is not possible during\ntraining. This required the development of Grid2Op3 an open-source environ-\nment for training RL agents to operate power networks.\nThe Grid2Op is a\nﬂexible framework, allowing researchers to accurately simulate power system\ndynamics for diﬀerent networks while interacting with the environment through\ntopological and redispatching actions.\nThe ﬁrst challenge was presented at IJCNN 2019 and was conducted on the\nIEEE-14 network (Figure 9) [7]. The goal was to operate the network under\na generation and load proﬁle by making topological changes to the network to\navoid line loadings beyond their limits. The participants scored zero if opera-\ntion resulted in network failure, and otherwise scored well by minimising line\nloadings. Several teams were successful in developing robust RL agents for net-\nwork operation, with the winning team presenting a methodology based on deep\nQ-learning [6].\nFollowing the success of the 2019 challenge, further challenges will take place\nin 2020 with two notable developments: (1) expansion to the larger IEEE-118\nnetwork (with 118 substations instead of 14); (2) availability of redispatching\nactions to change the production of generators. Both of these advancements\n3See: https://github.com/rte-france/Grid2Op\n18\nare essential for reﬂecting the real task of network operators. Due to the much\nlarger state and action spaces, the 2020 L2RPN challenge is signiﬁcantly harder\nthan before, requiring novel methods to succeed. The aim of this challenge is\nto move further toward developing RL methods for power network operation in\nthe real world.\n5\nConclusion\nElectricity systems around the world are changing in many ways to meet IPCC\nclimate goals by decarbonization and by electriﬁcation of other sectors of society\nsuch as transportation and heat.\nAs a result of these radical changes, the\nchallenge of operating power networks in the future is becoming incrementally\ngreater. The tools and techniques that are currently relied upon to keep the\nnetwork in a secure state in real-time and to plan for the near future may no\nlonger be adequate.\nRL is a very useful framework to develop new solution techniques to the\nproblem of network operation in a decarbonized world. RL involves an agent\nperforming actions, based on states of the environment and transition functions\nbetween the state and the action. By gamifying the network operation prob-\nlem, RL can be deployed as a technique to develop novel solutions to network\noperation challenges.\nFollowing the recent success of the L2RPN 2019 challenge, in 2020 a new chal-\nlenge will be launched, encompassing a larger network and the addition of re-\ndispatching actions as well as topological changes. This will be an essential step\ntowards developing practical RL methods that can be used to assist decision-\nmaking by power network operators. This challenge aims to bring experts from\nthe power systems and ML/RL communities together to develop solutions to\nsolve the problems facing network operators in the decarbonised future.\n19\n6\nContributing Organizations\nThe following organizations and individuals contributed to the development of\nthe L2RPN challenge in 2020:\nRTE:\nAntoine Marot - antoine.marot@rte-france.com\nBenjamin Donnot - benjamin.donnot@rte-france.com\nPatrick Panciatici - patrick.panciatici@rte-france.com\nPauline Gambier-Morel - pauline.gambier-morel@rte-france.com\nCamilo Romero - camilo.romers@gmail.com\nLucas Saludjan - lucas.saludjian@rte-france.com\nL2RPN and INRIA:\nIsabelle Guyon guyon@chalearn.org\nMarvin Lerousseau - marvin.lerousseau@gmail.com\nTuring Institute & UCL:\nAidan O’Sullivan - aidan.osullivan@ucl.ac.uk\nPatrick de Mars - patrick.demars.14@ucl.ac.uk\nEPRI - Electric Power Research Institute:\nAdrian Kelly - akelly@epri.com\nGoogle Brain:\nGabriel Dulac-Arnold - dulacarnold@google.com\nChina State Grid - GEIRINA:\nYan Zan - yan.zan@geirina.net\nJiajun Duan - jiajun.duan@geirina.net\nDi Shi - di.shi@geirina.net\nRuisheng Diao - ruisheng.diao@geirina.net\nZhiwei Wang - zhiwei.wang@geirina.net\nIowa & Maryland Universities:\nAmar Ramapuram - amar@iastate.edu\nSoumya Indela - indela.soumya@gmail.com\nTenneT:\nJan Viebahn - Jan.Viebahn@tennet.eu\nKassel University & pandapower:\nFlorian Sch¨afer - ﬂorian.schaefer@uni-kassel.de\nJan-Hendrik Menke - jan-hendrik.menke@uni-kassel.de\nArizona State University:\n20\nKishan Guddanti - kguddant@asu.edu\nEncoord:\nCarlo Brancucci - carlo@encoord.com\nReferences\n[1] International Energy Agency. Electricity Information Overview 2019. 2019.\n[2] Pieter Tjerk De Boer, Dirk P. Kroese, Shie Mannor, and Reuven Y. Ru-\nbinstein. A tutorial on the cross-entropy method. Annals of Operations\nResearch, 2005.\n[3] Benjamin Donnot, Isabelle Guyon, Marc Schoenauer, Patrick Panciatici,\nand Antoine Marot. Introducing machine learning for power system oper-\nation support. CoRR, abs/1709.09527, 2017.\n[4] Leonard L. Grigsby. Power System Stability and Control. Boca Raton:CRC\nPress, third edition, 2012.\n[5] Lukasz Kidzinski, Sharada Prasanna Mohanty, Carmichael F. Ong, Zhewei\nHuang, Shuchang Zhou, Anton Pechenko, Adam Stelmaszczyk, Piotr\n21\nJarosik, Mikhail Pavlov, Sergey Kolesnikov, Sergey M. Plis, Zhibo Chen,\nZhizheng Zhang, Jiale Chen, Jun Shi, Zhuobin Zheng, Chun Yuan, Zhi-\nhui Lin, Henryk Michalewski, Piotr Milos, Blazej Osinski, Andrew Melnik,\nMalte Schilling, Helge J. Ritter, Sean F. Carroll, Jennifer L. Hicks, Sergey\nLevine, Marcel Salath´e, and Scott L. Delp. Learning to run challenge solu-\ntions: Adapting reinforcement learning methods for neuromusculoskeletal\nenvironments. CoRR, abs/1804.00361, 2018.\n[6] Tu Lan, Jiajun Duan, Bei Zhange, Di Shi, Zhiwei Wang, Ruisheng Diao,\nand Xiaohu Zhange.\nAi-based autonomous line ﬂow control via topol-\nogy adjustment for maximizing time-series atcs. In IEEE PES GM 2020\n(preprint), 2020.\n[7] Antoine Marot, Benjamin Donnot, Camilo Romero, Balthazar Donon, Mar-\nvin Lerousseau, Luca Veyrin-Forrer, and Isabelle Guyon. Learning to run\na power network challenge for training topology controllers. In PSCC2020\n(preprint), 2020.\n[8] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre,\nGeorge van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda\nPanneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John\nNham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine\nLeach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis.\nMas-\ntering the game of Go with deep neural networks and tree search. Nature,\n529(7587):484–489, 2016.\n[9] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou,\nAja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai,\nAdrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre,\nGeorge Van Den Driessche, Thore Graepel, and Demis Hassabis. Mastering\nthe game of Go without human knowledge. Nature, 550(7676):354–359,\n2017.\n[10] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An In-\ntroduction. A Bradford Book, second edition, 2018.\n[11] Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour.\nPolicy gradient methods for reinforcement learning with function approxi-\nmation. In Advances in Neural Information Processing Systems 12, pages\n1057–1063, 2000.\n[12] A. Von Meier. Electric Power Systems: A Conceptual Introduction. John\nWiley and Sons, 2006.\n[13] Ronald J. Williams.\nSimple statistical gradient-following algorithms for\nconnectionist reinforcement learning. Machine Learning, 8:229–256, 1992.\n22\n",
  "categories": [
    "eess.SP",
    "cs.LG",
    "stat.ML",
    "I.2"
  ],
  "published": "2020-03-16",
  "updated": "2020-03-16"
}