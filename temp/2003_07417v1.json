{
  "id": "http://arxiv.org/abs/2003.07417v1",
  "title": "Improving Performance in Reinforcement Learning by Breaking Generalization in Neural Networks",
  "authors": [
    "Sina Ghiassian",
    "Banafsheh Rafiee",
    "Yat Long Lo",
    "Adam White"
  ],
  "abstract": "Reinforcement learning systems require good representations to work well. For\ndecades practical success in reinforcement learning was limited to small\ndomains. Deep reinforcement learning systems, on the other hand, are scalable,\nnot dependent on domain specific prior knowledge and have been successfully\nused to play Atari, in 3D navigation from pixels, and to control high degree of\nfreedom robots. Unfortunately, the performance of deep reinforcement learning\nsystems is sensitive to hyper-parameter settings and architecture choices. Even\nwell tuned systems exhibit significant instability both within a trial and\nacross experiment replications. In practice, significant expertise and trial\nand error are usually required to achieve good performance. One potential\nsource of the problem is known as catastrophic interference: when later\ntraining decreases performance by overriding previous learning. Interestingly,\nthe powerful generalization that makes Neural Networks (NN) so effective in\nbatch supervised learning might explain the challenges when applying them in\nreinforcement learning tasks. In this paper, we explore how online NN training\nand interference interact in reinforcement learning. We find that simply\nre-mapping the input observations to a high-dimensional space improves learning\nspeed and parameter sensitivity. We also show this preprocessing reduces\ninterference in prediction tasks. More practically, we provide a simple\napproach to NN training that is easy to implement, and requires little\nadditional computation. We demonstrate that our approach improves performance\nin both prediction and control with an extensive batch of experiments in\nclassic control domains.",
  "text": "Improving Performance in Reinforcement Learning by\nBreaking Generalization in Neural Networks\nSina Ghiassian, Banafsheh Rafiee, Yat Long Lo, Adam White\nReinforcement Learning and Artificial Intelligence Laboratory, University of Alberta and\nAlberta Machine Intelligence Institute (AMII)\nEdmonton, AB, Canada\n{ghiassia,rafiee,loyat,amw8}@ualberta.ca\nABSTRACT\nReinforcement learning systems require good representations to\nwork well. For decades practical success in reinforcement learning\nwas limited to small domains. Deep reinforcement learning sys-\ntems, on the other hand, are scalable, not dependent on domain\nspecific prior knowledge and have been successfully used to play\nAtari, in 3D navigation from pixels, and to control high degree of\nfreedom robots. Unfortunately, the performance of deep reinforce-\nment learning systems is sensitive to hyper-parameter settings and\narchitecture choices. Even well tuned systems exhibit significant\ninstability both within a trial and across experiment replications.\nIn practice, significant expertise and trial and error are usually re-\nquired to achieve good performance. One potential source of the\nproblem is known as catastrophic interference: when later training\ndecreases performance by overriding previous learning. Interest-\ningly, the powerful generalization that makes Neural Networks\n(NN) so effective in batch supervised learning might explain the\nchallenges when applying them in reinforcement learning tasks.\nIn this paper, we explore how online NN training and interference\ninteract in reinforcement learning. We find that simply re-mapping\nthe input observations to a high-dimensional space improves learn-\ning speed and parameter sensitivity. We also show this preprocess-\ning reduces interference in prediction tasks. More practically, we\nprovide a simple approach to NN training that is easy to implement,\nand requires little additional computation. We demonstrate that\nour approach improves performance in both prediction and control\nwith an extensive batch of experiments in classic control domains.\nKEYWORDS\nReinforcement learning; Neural networks; Interference.\nACM Reference Format:\nSina Ghiassian, Banafsheh Rafiee, Yat Long Lo, Adam White. 2020. Improv-\ning Performance in Reinforcement Learning by Breaking Generalization\nin Neural Networks. In Proc. of the 19th International Conference on Au-\ntonomous Agents and Multiagent Systems (AAMAS 2020), Auckland, New\nZealand, May 9–13, 2020, IFAAMAS, 10 pages.\n1\nINTRODUCTION\nReinforcement learning (RL) systems require good representations\nto work well. For decades practical success in RL was restricted to\nsmall domains—with the occasional exception such as Tesauro’s\nProc. of the 19th International Conference on Autonomous Agents and Multiagent Systems\n(AAMAS 2020), B. An, N. Yorke-Smith, A. El Fallah Seghrouchni, G. Sukthankar (eds.), May\n9–13, 2020, Auckland, New Zealand. © 2020 International Foundation for Autonomous\nAgents and Multiagent Systems (www.ifaamas.org). All rights reserved.\nTD-Gammon (Tesauro, 1995). High-dimensional and continuous in-\nputs require function approximation where the features must either\nbe designed by a domain expert, constructed from an exhaustive\npartitioning schemes (e.g., Tile Coding), or learned from data. Ex-\npert features can work well (Sturtevant and White, 2007 and Silver,\n2009), but depending on prior knowledge in this way limits scala-\nbility. Exhaustive partition strategies can be extended beyond small\ntoy tasks (Stone and Sutton, 2001; Modayil et al., 2014; Rafiee et\nal., 2019), but ultimately do not scale either. Neural Networks (NN),\non the other hand, are both scalable and not dependent on domain\nspecific prior knowledge. Unfortunately, training NNs is typically\nslow, finicky, and not well suited for RL tasks where the training\ndata is temporally correlated, non-stationary, and presented as an\ninfinite stream of experience rather than a batch.\nThe practice of combining neural network function approxima-\ntion and reinforcement learning has significantly improved. Deep\nreinforcement learning systems have been successfully deployed on\nvisual tasks like Atari, 3D navigation, and video games (Mnih et al.,\n2015; Parisotto and Salakhutdinov, 2017; Vinyals et al., 2019). Deep\nRL systems can control high degree of freedom robots (Riedmiller\net al., 2018), and learn in robot simulation domains directly from\njoint angles and velocities (Duan et al., 2016). All these systems rely\non a combination of improved optimization algorithms (Kingma\nand Ba, 2014), Experience Replay (Lin, 1992), and other tricks such\nas Target Networks (Mnih et al., 2015).\nThere are many challenges in designing and training NN-based\nRL systems. Many systems exhibit extreme sensitivity to key hyper-\nparameters (Henderson et al., 2018)—choices of replay buffer size\n(Zhang and Sutton, 2017), optimizer hyper-parameters (Jacobsen et\nal., 2019), and other algorithm-dependent hyper-parameters have\na large impact on performance. Many systems exhibit fast initial\nlearning, followed by catastrophic collapse in performance, as the\nnetwork unlearns its previously good policy (Goodrich, 2015). In\nsome domains, simpler learning systems can match and surpass\nstate-of-the-art NN-based alternatives (Rajeswaran et al., 2017).\nPerhaps many of these frailties can be largely explained by ag-\ngressive generalization and interference that can occur in neural\nnetwork training. The concept of Catastrophic Interference is sim-\nple to explain: training on a sequence of tasks causes the network\nto override the weights trained for earlier tasks. This problem is\nparticularly acute in RL, because the agent’s decision making policy\nchanges over time causing interference to occur during single task\ntraining (Kirkpatrick et al., 2017; Liu et al., 2019). There are three\nprimary strategies for dealing with interference: (1) adapting the\nloss to account for interference (Javed and White, 2019), (2) utilis-\ning networks that are robust to interference (Liu et al., 2019), or (3)\narXiv:2003.07417v1  [cs.LG]  16 Mar 2020\nadapting the training—classically by shuffling the training data in\nsupervised learning (French, 1999), or via experience replay in RL.\nIn this paper we propose a new approach to reducing interfer-\nence and improving online learning performance of neural-network\nbased RL. Our idea is based on a simple observation. Deep RL sys-\ntems are more difficult to train in domains where there is signifi-\ncant and inappropriate generalization of the inputs. For example,\nin Mountain Car the observations are the position and velocity\nof the car. A NN will exploit the inherent generalization between\nstates that are close in Euclidean space. However, the value function\nexhibits significant discontinuities and it is difficult for the network\nto overcome the harmful generalization to learn a good policy. This\ninappropriate generalization is prevalent in many classic RL control\ndomains, and could compound the effects of interference, resulting\nin slow learning. Inappropriate generalization is less prevalent in\nvisual tasks because standard architectures utilize convolutional\nlayers which are designed to manage the input generalization.\nOur proposed solution is to simply map the observations to a\nhigher-dimensional space. This approach significantly reduces the\nharmful generalization in the inputs, has low computational over-\nhead, and is easily scaled to higher dimensions. In fact, this is an\nold idea: randomly projecting the inputs was a common prepro-\ncessing step in training perceptrons (Minsky and Papert, 2017) and\ncan be competitive with networks learned via Backprop (Sutton\nand Whitehead, 1993; Mahmood and Sutton, 2013). We explore\ninput preprocessing based on simple independent discretization,\nand Tile Coding (Sutton and Barto, 2018). We show that our in-\nput preprocessing improves the learning speed of several standard\nneural-network systems, and reduces interference. In fact, DQN\n(Mnih et al., 2015) achieves low interference and efficient learning,\npossibly because it uses experience replay and target networks\nwhich reduce interference, as our experiments suggest. Across the\nboard, our results show our input preprocessing strategy never\nreduced performance, and in many cases dramatically improved\nlearning speed and hyper-parameter sensitivity. Our results show\nthat neural-network learners can be made more user friendly and\nexhibit reliable and efficient training.\n2\nBACKGROUND\nThis paper studies both prediction—value function approximation—\nand control—maximizing reward. We employ the Markov Decision\nProcess (MDP) framework (Puterman, 2014). In this framework, an\nagent and environment interact at discrete time steps t = 0, 1, 2, . . ..\nAt each time step,t, the agent is in a state St ∈S and takes an action\nAt ∈A, where S and A are state and action spaces respectively.\nThe agent takes actions according to a policy π : A ×S →[0, 1]. In\nresponse to the action, the environment emits a reward Rt+1 ∈R\nand takes the agent to the next state St+1. The environment makes\nthis transition according to transition function P(St+1|St,At ).\nIn prediction (policy evaluation), the policy π, is fixed. The goal\nis to estimate the value function, defined as the expected return\n(Gt ∈R) if the agent starts from a state s and follows policy π until\ntermination:\nvπ (s) \u0011 Eπ[Gt | St = s] \u0011 Eπ\n\" ∞\nÕ\nk=0\nγ kRt+k+1 | St = s\n#\n,\nfor all s ∈S, where Eπ[.|.] denotes the conditional expectation of\na random variable under π and γ ∈[0, 1] is a scalar discount factor\nparameter.\nIn the control setting, the policy is not fixed. The agent seeks\nto find a policy that maximizes the expected return. In control,\nstate-action value functions replace state value functions from the\npolicy evaluation case. The state-action value is defined as:\nqπ (s,a) \u0011 Eπ[Gt | St = s,At = a]\n\u0011 Eπ\n\" ∞\nÕ\nk=0\nγ kRt+k+1 | St = s,At = a\n#\n.\nWe consider the case in which the state space is large and we\ncannot estimate one value for each state or state-action pair. Instead,\nwe seek to make use of a parametric function to approximate the\nvalue function. We denote the approximate value function for states\nand state-action pairs by ˆv(s,w) and ˆq(s,a,w) respectively, where\nˆv(s,w) ≈vπ (s) and ˆq(s,a,w) ≈qπ (s,a) where vector w ∈Rd\nincludes the learned parameters of the approximation.\nTo estimate the state values, we use temporal-difference learning,\nmore specifically, TD(0) (Sutton, 1988), to update the neural network\nparameters at each time step, where the neural network is the\nfunction approximator to TD(0). Let w be the weights and α be a\nsmall step-size. The network parameters are updated according to:\nwt+1 ←wt + α δt ∇w ˆv(St,wt ).\n∇w ˆv(St,wt ) denotes the gradient of the function ˆv(St,wt ) with\nrespect to the parametersw wherew = wt .δt is called the temporal-\ndifference error: δt \u0011 Rt+1 + γ ˆv(St+1,wt ) −ˆv(St,wt ).\nTo estimate the state-action pair values, we use TD(0)’s control\nvariant, called Sarsa(0) (Rummery and Niranjan, 1994). Sarsa(0)\nupdate rules are the same as TD(0) except that ˆv(St,wt ) is replaced\nby ˆq(St,At,wt ). Sarsa(0) typically uses an ϵ-greedy policy with\nrespect to the state-action values to select actions.\nNeural networks have been widely used to approximate value\nfunctions in reinforcement learning tasks, but there are several\nadditional ingredients that improve performance. Often, neural net-\nworks are combined with Experience Replay (ER) (Lin, 1992), Target\nNetworks (TN), and step-size adaptation methods (called optimiz-\ners). ER is a way to reuse previous data stored in a replay buffer to\nincrease sample efficiency. The idea is simple, the agent stores recent\nexperience in a buffer of experience tuples—(St,At,Rt+1,St+1)—\nand replays it repeatedly, as if the agent was re-experiencing the\ndata. The experience is typically sampled randomly from the buffer\nto break the temporal dependencies in the training data. The main\nidea in using target networks is to stabilize the update used in TD\nmethods. TD updates towards a bootstrap target: the update target\ncontains the network’s current estimate of the value function. In-\nstead of changing the network in the target on every update, the\nTarget Network is periodically set equal to the learned network. Op-\ntimizers such as Adam (Kingma and Ba, 2014) are used in place of\na global learning rate in stochastic gradient descent (e.g., α above).\nInstead we use a vector of step-sizes—one for each weight—that\nchange with time. These three ingredients were used in DQN (Mnih\net al., 2014), which is perhaps one of the most widely used and ro-\nbust deep reinforcement learning systems available today.\nContinuous\n2D Space\nTiling 1\nTiling 2\nTiling 3\n2 Sample points \nin the state space\nNeighbourhood of\ngeneralization\nFigure 1: A continuous 2D space with 1 tiling on top of it is\non the left. Three overlapping tilings on the 2D continuous\nspace are shown in the middle in blue, green and red. The\ngeneralization region for a sample point is on the right.\n3\nBREAKING GENERALIZATION IN NEURAL\nNETWORKS\nNeural networks can forget what they learned in the past due to\na phenomenon known as catastrophic interference. Interference\nhappens when a neural network is trained on new data and it\noverwrites what it has learned in the past. This phenomenon can\nbe related to neural network’s global generalization.\nTo alleviate the interference issue, we propose mapping the input\nto a higher dimensional space. Specifically, we propose discretizing\nor tile coding the input as a preprocessing step before feeding it to\nthe neural network. This preprocessing step breaks the input gener-\nalization and our hypothesis is that it helps reduce the overly global\ngeneralization of neural networks and in turn reduces interference\nand improves performance. We propose two simple approaches for\nbreaking the generalization in the input space as discussed below\nand test our hypothesis in later sections.\nThe first approach is to simply use binning to discretize each\ndimension of the input separately. In this case, each dimension of\nthe input is covered with a one dimensional grid and a one-hot\nvector is created that has a one in the bin where the input lies in\nand zero everywhere else. The same is done for all of the input di-\nmensions and then the resulting vectors are concatenated to create\na long one-dimensional vector, which is the final representation\nfed to the neural network. We will simply refer to this method as\ndiscretization-neural network or the shorthand D-NN.\nThe second approach that we use to break the generalization\nis Tile Coding (TC) (Albus 1975, 1981). We refer to this method\nwith the shorthand TC-NN. Tile coding works by covering the state\nspace with a number of overlapping grids called tilings. Each grid\ndivides the state space into small squares, called tiles. In Figure 1, a\ncontinuous 2D space is covered by 3 tilings where each tiling has 4\ntile across each dimension (overall 16 tiles). Tile coding creates a\nrepresentation for each point in space by concatenating the repre-\nsentation it has for each tiling. The representation for each tiling\nconsists of a one hot vector that has a one for the tile that the point\nfalls within and zero otherwise. For example, the representation\nfor the point in Figure 1 will have three ones in a vector of size 48\n(3 tilings × 4 × 4 tiles). See Sutton and Barto (2018) for a thorough\nexplanation of tile coding.\nBreaking the generalization in the input space increases the\nability of the neural network to respond to different parts of the\nstate space locally. With ReLU gates, the activation region of a node\nis the open half-space: {x | ⟨w,x⟩+ b > 0} where w represents\nthe weights and b is the bias associated with a node. If the NN\nreceives raw observations as input, every node will respond to an\nentire half-space in the input space and might cause undesirable\ngeneralization. However, when the generalization is broken in the\n0\n2\n0.07\n-0.07\nVel.\n0\n2\n0.07\n-0.07\n0\n1\n-1.2\n0.6\n0.6\n0.6\n0.6\n-1.2\n-1.2\n-1.2\nPos.\nPos.\nPos.\nPos.\nVel.\nNN\nTC-NN\nFigure 2: Response functions with raw inputs (top) and tile\ncoding preprocessing (bottom) for Mountain Car control.\ninput space using discretization, each small area in the input space\nis mapped to a vertex of a hypercube. These vertices are all extreme\npoints of a convex set and thus the ReLU activations will have the\nability to respond to each of these sub-areas separately.\nFigure 2 shows heat-maps for the case where NN used raw inputs\n(top) and tile coding preprocessing (bottom). The feature maps\nwere created using a neural network trained on the Mountain Car\nproblem for 500 episodes. Each heat-map represents the magnitude\nof the output of a node from the first hidden layer. Heat-maps\non the bottom row of Figure 2 show two rather global and two\nrather local node responses from the hidden layer. As shown in\nthe figure, responses from the neural net that use raw inputs are\nglobal. So far we discussed the features of the proposed methods\nand have shown that the proposed method can have more local\ngeneralizations. However, what we have shown so far is more\nqualitative than quantitative. The next section uses quantitative\nmeasures of interference for comparison.\n4\nEXPERIMENTAL SETUP\nThe experiments described in the following sections are rather\nextensive and have many components. At the highest level we\ninvestigate the impact of input preprocessing on a variety of base\nNN learning systems. Our experiments include both prediction and\ncontrol, in two classic reinforcement learning control tasks. The\nfollowing sections describe the simulation problems, and the base\nNN learning systems we used.\n4.1\nSimulation problems\nWe investigate three different problems, one prediction problem\nand two control problems. The control problems that we used are\nMountain Car and Acrobot. The prediction problem also uses the\nMountain Car testbed, but with a fixed given policy.\nMountain Car simulates an underpowered car on the bottom\nof a hill that should pass the finish line on top of the hill (Moore,\n1991). The problem has two dimensions: position and velocity. The\nposition can vary between -1.2 and 0.6, and the velocity varies\nbetween -0.07 and 0.07. There are three actions in each state, throttle\nforward, throttle backward, and no throttle. The car starts around\nthe bottom of the hill randomly in a point uniformly chosen between\n-0.4 and -0.6. The reward is -1 for each time step before the car passes\nthe finish line at the top of the hill. When the position becomes\nlarger than 0.5, the agent receives a reward of 0 and the episode\nterminates. The problem is episodic and not discounted (γ = 1).\nIn the control version of the Mountain Car problem, the agent\nseeks to find a policy that ends the episode as fast as possible. In\ncontrol, episodes were cut-off after 1000 steps. In prediction variant\nof the problem, we used a simple energy pumping policy. This policy\nchooses the action in agreement with the current velocity: left if\nvelocity is negative, right otherwise.\nThe Acrobot (Sutton, 1996) is similar to a gymnast. Its goal is\nto swing its feet above a bar it is hanging from. The problem has\nfour dimensions, two angles and two angular velocities (all real-\nvalued). There are three discrete actions: positive torque, negative\ntorque, and no torque. The reward is -1 on each time step before the\nAcrobot swings its feet above over the bar to terminate the episode.\nThe task is undiscounted with γ = 1. Episodes are cutoff after 500\nsteps. We used the Open AI Gym implementation of the Acrobot\nproblem (Brockman et al., 2016).\n4.2\nMethods\nWe are interested in studying the impact of input preprocessing on\na variety of NN learning systems. We investigate five NN learning\nsystems that represent a spectrum from naive learning systems\nthat we expect to work poorly, to well-known and widely used\narchitectures. In particular we selected, (1) simple feed-forward NN\nwith stochastic gradient descent (SGD); (2) simple feed-forward NN\nwith ER and SGD; (3) simple feed-forward NN with Adam (Kingma\nand Ba, 2014); (4) simple feed-forward NN with ER, and Adam; (5)\nsimple feed-forward NN with ER, Adam, and target networks.\nGiven our five representative learning systems, we experiment\nwih three different input preprocessing strategies. The first involves\nno preprocessing, feeding in the raw input observations. The second,\nfirst discretizes the inputs before they are passed to the network.\nThe third, first tile codes the observations.\n5\nEXPERIMENTAL RESULTS\nOur experiments primarily focus on how different input prepro-\ncessing strategies impact both the speed of learning, and sensitivity\nto the step-size parameter values. In addition we also investigate\nhow preprocessing impacts interference. To do so, we use a recently\nproposed interference measure (Liu, 2019). Our main hypothesis is\nthat input preprocessing improves performance in prediction and\ncontrol, because it reduces interference in the network updates. We\nbegin first by reporting the improvements in prediction and control\nwith discretization and tile coding.\n5.1\nOverall performance\nWe used different measures to compare the algorithms for control\nand prediction tasks. In the control tasks, we simply report the\nnumber of steps it takes for the agent to finish the episode. Note\nthat the reward at each time step for all tasks is -1 and thus the\nagent’s objective is finish the task as fast as possible.\nFor the Mountain Car prediction task, we used an easily inter-\npretable error measure, the Root Mean Squared Value Error (RVE).\nWe measure, at the end of each episode, how far the learned ap-\nproximatet value function is from the optimal value function. The\nRVE(wt ) is defined as:\nsÕ\ns ∈S\ndπ (s) [ ˆv(s,wt ) −vπ (s)]2 ≈\ns\n1\n|D|\nÕ\ns ∈D\n[ ˆv(s,wt ) −vπ (s)]2\n(1)\nwhere dπ (s) is the stationary distribution under π, ˆv is the agent’s\nestimate of the value function, and vπ is the true value function.\nSince the state space is continuous, dπ (s) is estimated by sampling\nstates when following dπ . D is a set of states that is formed by\nfollowing π to termination and restarting the episode and following\nπ again. This was done for 10,000,000 steps, and we then sampled\n500 states from the 10,000,000 states randomly. The true valuevπ (s)\nwas simply calculated for each s ∈D by following π once to the\nend of the episode.\nTo create learning curves1, we ran each method with many\ndifferent hyper-parameter combinations and plotted the one that\nminimized the Area Under the learning Curve (AUC)—total steps\nto goal in control and total RVE in prediction. We ran each method\nwith each specific parameter setting 30 times (30 independent runs).\nWe then averaged the results over runs and computed the standard\nerror.\nFigure 3, rows 1-3 compare each method using raw input–with\nits best performing hyper-parameters–with its counterpart that\nuses discretized inputs. In most cases, NNs with discretized inputs\nlearned faster than NNs using raw inputs and converged to the\nsame or a better final performance. Figure 3, rows 4-6 compares\nraw inputs with tile coding preprocessing. NNs with tile coding\npreprocessing outperforms NN with raw inputs.\nIn most cases, the difference between the performance of NN\nusing remapped inputs and raw inputs were statistically significant\n(according to the standard error). The results suggest that prepro-\ncessing the inputs and projecting them into a higher dimensional\nspace helps neural networks learn faster and more accurately.\nTo further assess the significance of the difference in pairwise\ncomparisons, we performed two sample t-tests. The pairs for the\nt-test were: 1) the method that used raw inputs and 2) the method\nthat used either tile coding or discretization preprocessing. Each\none of the 30 plots in Figure 3 includes two learning curves. Each\nplotted learning curve is the average over 30 learning curves of\nindependent runs. We first averaged the performance (the value\nerror for Mountain Car prediction, the number of steps per episode\nfor Mountain Car control and Acrobot) over episodes for each\nrun. This produced 30 numbers (one for each run) for each of the\nlearning curves in each plot in Figure 3. We then used these 60\nnumbers (30 for each group) for a two sample t-test. Appendix A\nsummarizes the p-values of the t-test in a table. The test revealed\nthat in cases that tile coding or discretization improved performance,\nthe improvement was statistically significant.\n5.2\nSensitivity to step-size\nIn our previous experiment, we reported the results using the best\nperforming step-size, which is not always feasible in practice. Ide-\nally, an algorithm will perform well with many different settings\nof the step-size parameter. In this section, we investigate the per-\nformance of each of our five learning systems with many different\nstep-size parameter values. Our objective is to understand how the\ninput preprocessing interacts with step-size sensitivity.\nWe evaluated the performance using different step-sizes (see\nTable 1 for a list of step-sizes). Figure 4 summarizes the sensitivity\nto step-size for the methods that used SGD and the initial step-\nsize for the methods that used Adam. To create sensitivity curves,\nwe ran each method with a specific step-size parameter value 30\n1All learning curves were smoothed using a sliding window of size 10.\nNN\nNN\nTC-NN\nNN\nNN\nTC-NN\nNN\nNN\nNeural network with\nexperience replay buﬀer \nand Adam optimizer\nand target network\nNeural network with\nexperience replay buﬀer \nand Adam optimizer\nVanilla neural network\nwith Adam optimizer\nNeural network with\nexperience replay buﬀer \nVanilla neural network\nNN\nNN\nTC-NN\nNN\nNN\nTC-NN\nNN\nNN\nNN\nNN\nNN\nNN\nTC-NN\nNN\nNN\nTC-NN\nNN\nNN\nNN\nNN\nD-NN\n# Steps\nper\nepisode\n# Steps\nper\nepisode\nRVE\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nNN\nD-NN\nNN\nMountain Car \nPrediction\nMountain Car\nControl\nAcrobot\nNN\nTC-NN\nNN\nTC-NN\nNN\nNN\nTC-NN\nTC-NN\nNN\nNN\nNN\nTC-NN\nTC-NN\nMountain Car \nPrediction\nNN\nTC-NN\nNN\nTC-NN\nNN\nTC-NN\nNN\nTC-NN\nTC-NN\nNN\nNN\nNN\nNN\nTC-NN\nTC-NN\nMountain Car\nControl\nNN\nTC-NN\nNN\nTC-NN\nNN\nTC-NN\nNN\nTC-NN\nTC-NN\nNN\nNN\nNN\nNN\nTC-NN\nTC-NN\nEpisode\nEpisode\nEpisode\nEpisode\nEpisode\n# Steps\nper\nepisode\n# Steps\nper\nepisode\nAcrobot\nRVE\nNN\nTC-NN\n30\n50\n30\n50\nFigure 3: Learning curves for all tasks. Top three rows compare raw inputs with discretized inputs. Three bottom rows compare\nraw inputs with tile coding preprocessing. Discretizing/tile coding the input helped neural networks learn faster and converge\nto a better final performance. D-NN is short for Discretization+NN and TC-NN is short for Tile Coding+NN.\ntimes. We then averaged each run’s performance to get a single\nnumber that represents the area under the learning curve for that\nspecific run for that specific method with the specific parameter\nsettings. We then computed the average and standard error over\nthe 30 numbers (one for each run) and plotted this number for each\nspecific step-size in Figure 4.\nEach learning system that we tested actually has several hyper-\nparameters, and these can also have a significant impact on perfor-\nmance. For example, the algorithms that use the Adam optimizer\nhave two extra parameters that specify the exponential decay rate\nfor the first and second moment estimates, which are typically\nreferred to as β1 and β2. To create a single curve for step-size sensi-\ntivity, we first searched over all the parameters to find the minimum\nAUC, we then fixed β1 and β2 to the values that achieved the mini-\nmum AUC and plotted the AUC over the step-size for those values.\nAs we see in Figure 4, discretizing/tile coding reduced sensitivity\nto step-size (compare the size of the bowl-shaped curves when\nusing tile coding or discretization to raw inputs). The standard\nerrors in Figure 4 (some of which are not visible due to being small)\nshow that the difference in the errors are statistically significant.\n5.3\nInterference\nThe results in the previous section indicate that input preprocessing\ncan improve learning speed, stability, and parameter sensitivity, but\nit is not clear how these improvements relate to interference in the\nnetwork updates. One hypothesis is that preprocessing reduces in-\nterference. To test this hypothesis, we use a recently proposed mea-\nsure of interference (Liu, 2019). The measure is only applicable to\nprediction tasks and so the results in this section are only suggestive\nof the possible relationship between performance improvements\ndue to input preprocessing and reduction in interference.\nMountain Car \nPrediction\nMountain Car\nControl\n# Steps\nper\nepisode\n# Steps\nper\nepisode\nAcrobot\nRVE\nTC-NN\nNN\nTC-NN\nNN\nNN\nNN\nTC-NN\nTC-NN\nNN\nTC-NN\nTC-NN\nNN\nTC-NN\nNN\nNN\nNN\nTC-NN\nTC-NN\nNN\nTC-NN\nTC-NN\nNN\nTC-NN\nNN\nNN\nNN\nTC-NN\nTC-NN\nNN\nTC-NN\nMountain Car \nPrediction\nRVE\nMountain Car\nControl\n# Steps\nper\nepisode\nNN\nNN\nNN\nNN\nD-NN\nD-NN\nD-NN\nD-NN\nNN\nD-NN\nNN\nNN\nNN\nNN\nNN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\n# Steps\nper\nepisode\nAcrobot\nNN\nNN\nNN\nNN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nStepsize\nStepsize\nStepsize\nStepsize\nStepsize\nNeural network with\nexperience replay buﬀer \nand Adam optimizer\nVanilla neural network\nwith Adam optimizer\nNeural network with\nexperience replay buﬀer \nVanilla neural network\nNN\nNeural network with\nexperience replay buﬀer \nand Adam optimizer\nand target network\nFigure 4: Step-size sensitivity curves over all tasks. Top three rows compare discretized and raw inputs. Three bottom rows\ncompare tile coded preprocessing and raw inputs. Breaking the generalization in the input reduced sensitivity to step-size.\nLiu’s measure is fairly straightforward and is based on the Pair-\nwise Interference (PI):\nPI(Si,Sj) =\n∇w\n\u0002 ˆv(w,Si)\n\u0003⊤∇w\n\u0002 ˆv(w,Sj)\n\u0003\n||∇w ˆv(w,Si)||2 × ||∇w ˆv(w,Sj)||2\n,\n(2)\nwhich means if the two vectors in the numerator have a zero inner\nproduct, they are orthogonal and update made to Si does not affect\nthe update from Sj. This is similar to the neural tangent kernel\nthat has been previously used for studying generalization in deep\nreinforcement learning (Achaim, Knight and Abbeel, 2019), scaled\nby the norm. The measure is also similar to the one proposed by\nFrench (1999) for batch supervised learning.\nWe measured the pairwise interference (Equation 2) for all pairs\nof samples in the dataset D where D is the same as the one used\nto compute RVE. See Equation 1. We averaged the pairwise inter-\nference over all samples and then averaged the results over 30 runs.\nWe repeated this process as learning progressed, specifically, before\nthe learning started, after initializing the neural network, and then\nafter episodes 1, 5, 10, 25, and every 25 episodes afterwards until\nthe end of learning (500 episodes in this case). The interference for\neach method was measured for the parameter setting that produced\nthe best learning curve as explained in Section 5.1.\nTo sanity check this Liu’s measure, we can compare the inter-\nference of a learning system we expect to have high interference\nwith one we expect to have low interference and see if the mea-\nsure agrees. Let’s consider the simple feed-forward NN with SGD\nshown in Figure 5 (first row, leftmost subplot). We can compare\nthis with the measured interference of a NN with ER, Adam and\ntarget networks (Figure 5, first row, rightmost subplot). We can see\nthe simple NN results in higher measured interference than the NN\nwith target networks, ER, and Adam..\nRows 1 and 4 of Figure 5, compare the interference of differ-\nent methods over episodes. The parameters for these interference\nmeasurements were the same as the ones used to produce learning\nTC-NN\nNN\nTC-NN\nInterference\nEpisode\nEpisode\n# Hidden units\n# Hidden units\nInterference\nNN\nTC-NN\nNN\nTC-NN\n# Hidden layers\n# Hidden layers\nInterference\nNN\nTC-NN\nNN\nTC-NN\n# Hidden layers\n# Hidden layers\nNN\nTC-NN\nNN\nVanilla neural network\nwith Adam optimizer\nNeural network with\nexperience replay buﬀer \nVanilla neural network\nMountain Car Prediction\nInterference\n# Hidden units\n# Hidden units\nInterference\nNN\nNN\nInterference\nNN\nNN\nD-NN\nD-NN\nD-NN\nD-NN\nD-NN\nNN\nD-NN\nNN\nD-NN\nNN\nNN\nTC-NN\nEpisode\n# Hidden units\nNN\nTC-NN\n# Hidden layers\nNN\nTC-NN\n# Hidden layers\n# Hidden units\nNN\nNN\nD-NN\nD-NN\nNN\nTC-NN\nNN\nTC-NN\nEpisode\n# Hidden units\nNN\nTC-NN\n# Hidden layers\nNN\nTC-NN\n# Hidden layers\n# Hidden units\nNN\nNN\nD-NN\nD-NN\nNN\nTC-NN\nNN\nD-NN\nNeural network with\nexperience replay buﬀer \nand Adam optimizer\nEpisode\n# Hidden units\nTC-NN\nNN\n# Hidden layers\nTC-NN\nNN\n# Hidden layers\n# Hidden units\nNN\nNN\nD-NN\nD-NN\nEpisode\nEpisode\nEpisode\nEpisode\nEpisode\nNN\nTC-NN\nNeural network with\nexperience replay buﬀer \nand Adam optimizer\nand target network\nD-NN\nNN\nTC-NN\nNN\nFigure 5: Interference over time (first and fourth row), interference over the number of hidden units (second and fifth row),\nand interference over the number of hidden layers (third and sixth row) for discretized vs. raw and tile coding vs. raw are\nshown. Breaking the generalization in the input space reduced the interference.\ncurves in Figure 3. Discretization and tile coding both reduced the\ninterference compared to when raw input is used.\nWe compared the interference of different methods with differ-\nent numbers of hidden layers in the network. We fixed the number\nof hidden units at each layer to 25. To calculate a single number\nthat represents the interference of each setting, we measured the\ninterference over time, as discussed before for each run. We then\naveraged the results over time to get a single interference measure-\nment for each run. We then computed the average and standard\ndeviation of the resulting numbers as a measure of interference for\neach setting. Rows 3 and 6 of Figure 5, show the interference over\ndifferent number of hidden layers, 1 layer to 4 layers. In almost\nall cases, the interference was reduced when using tile coding or\ndiscretization. The interference was rather insensitive to number\nof hidden layers.\nWe also measured the interference over different number of hid-\nden units. We fixed the number of hidden layers to 1, and changed\nthe number of hidden units between 5, 10, 25, 50, 75 units. The\nprocess was the same as what we used to compute interference for\nincreasing number of hidden layers, described before.\nFigure 5, rows 2 and 5, shows that the interference generally\ndecreased with increasing number of hidden units. This is possibly\nbecause larger networks have more capacity to reduce the impact\nof input generalization.\n6\nCONCLUSION AND FUTURE WORK\nScaling up reinforcement learning requires function approximation,\nbut it is still not clear how to best learn the representation online.\nModern NN systems work well in control tasks that would be nearly\nimpossible with hand-designed static features. The NN learning\nsystems used today employ a host of algorithmic tricks to achieve\nstable learning, but these systems are still slow to train and can\nexhibit significant variability in performance. Some of these limi-\ntations can potentially be explained by catastrophic interference\nin the network updates, which has been relatively under-explored\nin online reinforcement learning. In this paper, we take some first\nsteps toward understanding how online NN training and interfer-\nence relate. We find that simply re-mapping the input observations\nto a high-dimensional space that breaks the input generalization,\nimproves learning speed, and parameter sensitivity across several\ndifferent architectures on two different control tasks. We also show\nempirically that this preprocessing can reduce interference in pre-\ndiction tasks. More practically, we provide a simple approach to\nNN training that is easy to implement, requires little additional\ncomputation, and can significantly improve performance.\nOne possible next step is to study the performance of these meth-\nods in higher dimensional spaces. We used simple classic reinforce-\nment learning tasks in this paper so we could perform extensive\nhyper-parameter analysis and average over many runs. Neverthe-\nless, the discretization methods proposed here is highly scalable\n(input dimension grows only linearly in the number of bins), but\nit remains unclear how effective it will be in higher dimensional\nspaces. Another possible next step is to investigate other preprocess-\ning schemes and a greater variety of base learning systems. Here we\nexplored discretization and tile coding because they are simple and\ntheir impact on the learning system is easy to understand. Previous\nwork demonstrated that simple strategies like tile coding can be ap-\nplied to rather high-dimensional spaces on mobile robots by tiling\nthe dimensions independently (Stone and Sutton, 2001; Modayil et\nal., 2014; Rafiee et al., 2019); tile coding based preprocessing may\nwork well in larger problems. There are likely better preprocessing\nstrategies, that may even result in performance competitive with\nmore complex deep reinforcement learning systems.\nThere is more work to be done characterizing and correlating\npoor performance in NN learning with interference. We have just\nbegun to understand how sparse representations (Liu et al., 2019),\ntraining to reduce interference (Javed and White, 2019), and input\npreprocessing—our work—improves NN training. Perhaps we need\nbetter optimizers, or to dynamically grow the network over time. It\nis still unclear if interference is worse due to changes in the policy,\nor if there are additional complications for algorithms that use\nbootstrap targets like TD. Whatever the solution, we need to clearly\ndefine and measure interference before we can hope to mitigate it.\n7\nIMPLEMENTATION DETAILS\nIn this section, we describe our most important design decisions\nand parameter settings to aid reproducibility. We used Pytorch\nversion 1.3.0 for training the neural networks (Paszke et al., 2017).\nAll weights in the neural networks were initialized using Xavier\nuniform initialization (Glorot and Bengio, 2010). All NN used ReLU\nactivation in their hidden layers and were linear in the output. We\nTable 1: Methods and parameters used for different tasks.\nMethods\nAdam β1 & β2\nα\nAdditional\nNN\nSGD\n—\nMountain car\nprediction\n2−c, c ∈\n{3, 4, ..., 18}\nMountain car\ncontrol\n2−c, c ∈\n{1, 2, ..., 18}\nAcrobot\n2−c, c ∈\n{5, 6, ... , 18}\nTarget net\nupdate frequency ∈\n{10, 50, 100}.\nSet to 100 in results.\nTC-NN:\nMountain car:\n4 × 4 tiles,\n8 tilings.\nAcrobot:\n4 × 4 × 4 tiles,\n8 tilings.\nD-NN:\nMountain car:\n20 bins/dimension.\nAcrobot:\n32 bins/dimension\nSGD+ER\n—\nAdam\nβ1 ∈{0.999,\n0.99, 0.9} and\nβ2 ∈{0.9999,\n0.999, 0.99, 0.9}\nAdam+ER\nAdam+ER\n+TN\nD-\nNN\nSGD\n—\nSGD+ER\n—\nAdam\nβ1 ∈{0.999,\n0.99, 0.9} and\nβ2 ∈{0.9999,\n0.999, 0.99, 0.9}\nAdam+ER\nAdam+ER\n+TN\nTC-\nNN\nSGD\n—\nSGD+ER\n—\nAdam\nβ1 ∈{0.999,\n0.99, 0.9} and\nβ2 ∈{0.9999,\n0.999, 0.99, 0.9}\nAdam+ER\nAdam+ER\n+TN\ntested several values of β1 and β2 to achieve good performance\nwith Adam: all possible combinations of β1 ∈{0.9, 0.99, 0.999} and\nβ2 ∈{0.9, 0.99, 0.999, 0.9999}. We used a replay buffer size of 2000.\nThe target networks were updated every 100 steps (this value was\ndetermined through trial-and-error parameter tuning)2. We used a\nmini-batch size of 32 for all network updates. In control experiments,\nwe used Sarsa(0) with an ϵ-greedy behavior and ϵ = 0.1.\nThere are a few details that were problem specific. In Mountain\nCar, the value function was approximated with a NN with one\nhidden layer and 50 hidden units, and in Acrobot the hidden layer\ncontained 100 hidden units. In control, the network outputted three\nvalues, one for each action.\nInput preprocessing was fairly generic and similar across both\nproblems. The input to the networks was normalized between -1\nand 1 when no preprocessing was used. For discretization-based\npreprocessing, each input dimension was discretized independently\nand passed to the network as concatenated one-hot vectors. In\nMountain Car, the inputs were discretized into 20 bins per dimen-\nsion, and in Acrobot we used 32 bins per dimension. For tile coding\nbased preprocessing, we tiled each input dimension together pro-\nducing a binary vector as input to the NN. We used 8 tilings, with\na tile width of 1/4 for each dimension: 4 × 4 tiles for Mountain Car\nyielding 128 length binary vectors and 4×4×4×4 tiles for Acrobot\nhashed into 512 length binary vectors. We tested both joint tiling\nthe dimensions and independent tilings. The results reported in\nSection 5 used joint tilings. We used the freely available Python tile\ncoding software3.\nACKNOWLEDGMENTS\nThe authors thank Richard Sutton,Vincent Liu, and Shivam Garg\nfor insights and helpful comments. Andrew Patterson helped proof-\nreading the paper. The authors gratefully acknowledge funding\nfrom JPMorgan Chase & Co., Google DeepMind, Natural Sciences\nand Engineering Research Council of Canada (NSERC), and Alberta\nInnovates—Technology Futures (AITF).\n2We also tried updating the target network every 50 or 10 steps instead of 100, the\nresults of which were similar to the results presented in the paper.\n3http://incompleteideas.net/tiles/tiles3.html.\nREFERENCES\n[1] Achiam, J., Knight, E., and Abbeel, P. (2019). Towards Characterizing\nDivergence in Deep Q-Learning. arXiv:1903.08894.\n[2] Albus, J. S. (1975). Data storage in the Cerebellar Model Articulation\nController (CMAC). Journal of Dynamic Systems, Measurement and\nControl, 97(3):228–233.\n[3] Albus, J. S. (1981). Brains, Behavior, and Robotics. Peterborough, NH.\n[4] Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman, J.;\nTang, J.; and Zaremba, W. (2016). Openai gym. arXiv:1606.01540.\n[5] Duan, Y., Chen, X., Houthooft, R., Schulman, J., Abbeel, P. (2016, June).\nBenchmarking deep reinforcement learning for continuous control. In\nInternational Conference on Machine Learning (pp. 1329-1338).\n[6] French, R. M. 1999. Catastrophic forgetting in connectionist net- works:\nCauses, consequences and solutions. Trends in Cognitive Sciences,\n3(4):128–135.\n[7] Glorot, X., and Bengio, Y. (2010). Understanding the difficulty of training\ndeep feedforward neural networks. In Proceedings of the thirteenth\ninternational conference on artificial intelligence and statistics.\n[8] Goodfellow I. J., Mirza M, Xiao D, Courville A, Bengio Y. An empiri-\ncal investigation of catastrophic forgetting in gradient-based neural\nnetworks. arXiv:1312.6211. 2013 Dec 21.\n[9] Goodrich, B. F. (2015). Neuron clustering for mitigating catastrophic\nforgetting in supervised and reinforcement learning.\n[10] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D. and Meger,\nD., 2018, April. Deep reinforcement learning that matters. In Thirty-\nSecond AAAI Conference on Artificial Intelligence.\n[11] Jacobsen, A., Schlegel, M., Linke, C., Degris, T., White, A., and White,\nM. (2019). Meta-descent for Online, Continual Prediction. In AAAI\nConference on Artificial Intelligence.\n[12] Javed, K., and White, M. (2019). Meta-learning representations for\ncontinual learning. In Advances in Neural Information Processing Systems\n(pp. 1818-1828).\n[13] Kingma, D.P. and Ba, J., (2014). Adam: A method for stochastic opti-\nmization. arXiv:1412.6980.\n[14] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G.,\nRusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A.\nand Hassabis, D. (2017). Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the national academy of sciences, 114(13), 3521-\n3526.\n[15] Lin, L. J. (1992). Self-improving reactive agents based on reinforcement\nlearning, planning and teaching. Machine learning, 8(3-4), 293-321.\n[16] Liu. V. (2019). Sparse Representation Neural Networks for Online Re-\ninforcement Learning. M.Sc. thesis, University of Alberta, Edmonton,\nCanada.\n[17] Liu, V. Kumaraswamy, R., Le, L., and White, M. (2019). The utility\nof sparse representations for control in reinforcement learning. In\nProceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, pp.\n4384-4391).\n[18] Mahmood, A. R., Sutton, R. S. (2013). Representation Search through\nGenerate and Test. In Proceedings of the AAAI Workshop on Learning\nRich Representations from Low-Level Sensors, Bellevue, WA, USA.\n[19] McCloskey, M., and Cohen, N. J. (1989). Catastrophic interference in\nconnectionist networks: The sequential learning problem. In Psychology\nof learning and motivation (Vol. 24, pp. 109-165). Academic Press.\n[20] Minsky, M., and Papert, S. A. (2017). Perceptrons: An introduction to\ncomputational geometry. MIT press.\n[21] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare,\nM. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen,\nS., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,\nD., Legg, S., Hassabis, D. (2015). Human level control through deep\nreinforcement learning. Nature, 518(7540):529–533.\n[22] Modayil, J., White, A., and Sutton, R. S. (2014). Multi-timescale nexting\nin a reinforcement learning robot. Adaptive Behavior, 22(2), 146-160.\n[23] Moore, A. W. (1991). Variable resolution dynamic programming: Effi-\nciently learning action maps in multivariate real-valued state-spaces.\nIn Machine Learning Proceedings 1991 (pp. 333-337). Morgan Kaufmann.\n[24] Parisotto, E. and Salakhutdinov, R. (2017). Neural map: Structured\nmemory for deep reinforcement learning. arXiv:1702.08360.\n[25] Puterman, M. L. (2014). Markov Decision Processes.: Discrete Stochastic\nDynamic Programming. John Wiley and Sons.\n[26] Rafiee, B., Ghiassian, S., White, A., and Sutton, R. S. (2019, May). Predic-\ntion in Intelligence: An Empirical Comparison of Off-policy Algorithms\non Robots. In Proceedings of the 18th International Conference on Au-\ntonomous Agents and MultiAgent Systems (pp. 332-340). International\nFoundation for Autonomous Agents and Multiagent Systems.\n[27] Rajeswaran, A., Lowrey, K., Todorov, E. V., and Kakade, S. M. (2017). To-\nwards generalization and simplicity in continuous control. In Advances\nin Neural Information Processing Systems (pp. 6550-6561).\n[28] Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., Van de\nWiele, T., Mnih, V., Heess, N., Springenberg, J. T. and Springenberg, J. T.\n(2018). Learning by playing-solving sparse reward tasks from scratch.\narXiv:1802.10567.\n[29] Rummery, G. A., and Niranjan, M. (1994). On-line Q-learning using\nconnectionist systems (Vol. 37). Cambridge, England: University of Cam-\nbridge, Department of Engineering.\n[30] Silver, D. (2009). Reinforcement learning and simulation-based search\nin computer Go.\n[31] Stone, P., Kuhlmann, G., Taylor, M. E., and Liu, Y. (2005). Keepaway\nsoccer: From machine learning testbed to benchmark. In Robot Soccer\nWorld Cup (pp. 93-105). Springer, Berlin, Heidelberg.\n[32] Stone, P., and Sutton, R. S. (2001). Scaling reinforcement learning toward\nRoboCup soccer. In Icml (Vol. 1, pp. 537-544).\n[33] Sturtevant, N. R., and White, A. M. (2006, May). Feature construction\nfor reinforcement learning in hearts. In International Conference on\nComputers and Games (pp. 122-134). Springer, Berlin, Heidelberg.\n[34] Sutton, R. S. (1996). Generalization in reinforcement learning: Successful\nexamples using sparse coarse coding. In Advances in neural information\nprocessing systems (pp. 1038-1044).\n[35] Sutton, R. S. (1988). Learning to predict by the methods of temporal\ndifferences. Machine learning, 3(1), 9-44.\n[36] Sutton, R. S., and Barto, A. G. (2018). Introduction to reinforcement\nlearning. Cambridge, MA: MIT Press, 2nd edition.\n[37] Sutton, R. S., and Whitehead, S. D. (1993). Online learning with random\nrepresentations. In Proceedings of the Tenth International Conference on\nMachine Learning (pp. 314-321).\n[38] Tesauro, G. (1995). Temporal difference learning and TD-Gammon.\nCommunications of the ACM, 38(3), 58-68.\n[39] Vinyals, O., Babuschkin, I., Chung, J., Mathieu, M., Jaderberg, M., Czar-\nnecki, W. M., Dudzik, A., Huang, A., Georgiev, P., Powell, R. and Ewalds,\nT. (2019). AlphaStar: Mastering the real-time strategy game StarCraft\nII. DeepMind Blog.\n[40] Zhang, S., and Sutton, R. S. (2017). A deeper look at experience replay.\narXiv:1702.01275.\nA\nTWO SAMPLE T-TEST RESULTS\nTable 2: P-values of the AUC of the learning curves. The difference between the AUC of the learning curves is significant.\nVanilla neural network\nNeural netowrk\nwith experience\nreplay buffer\nVanilla neural\nnetwork with\nAdam optimizer\nNeural network\nwith experience\nreplay buffer and\nAdam optimizer\nNeural network\nwith experience\nreplay buffer and\nAdam optimizer\nand target network\nDiscretized input\nversus\nraw input\n(D-NN\nvs. NN)\nMountain Car\nPrediction\n2.58 × 10−50\n8.59 × 10−9\n1.19 × 10−36\n6.33 × 10−11\n9.01 × 10−19\nMountain Car\nControl\n1.61 × 10−13\n2.07 × 10−19\n6.94 × 10−12\n2.57 × 10−18\n1.76 × 10−2\nAcrobot\n7.00 × 10−16\n3.47 × 10−7\n3.42 × 10−10\n2.95 × 10−2\n1.09 × 10−5\nTile coded input\nversus\nraw input\n(TC-NN\nvs. NN)\nMountain Car\nPrediction\n9.97 × 10−56\n1.12 × 10−22\n2.29 × 10−40\n8.75 × 10−12\n8.31 × 10−28\nMountain Car\nControl\n3.35 × 10−18\n5.52 × 10−21\n3.76 × 10−14\n1.26 × 10−18\n2.25 × 10−7\nAcrobot\n1.49 × 10−17\n4.56 × 10−8\n1.83 × 10−22\n1.16 × 10−2\n1.08 × 10−10\nWe performed a two sample t-test on the area under the learning curves reported in Figure 3. Table 2 summarizes the p-values of the test. Each cell in\nthe table reports the p-value from the two sample t-test comparing the 30 AUCs (one for each run) of the corresponding learning curve in Figure 3. The\nsignificance test rejects the null hypothesis that the area under the learning curves come from normal distribution with equal means at the 5% level.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.NE"
  ],
  "published": "2020-03-16",
  "updated": "2020-03-16"
}