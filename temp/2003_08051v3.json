{
  "id": "http://arxiv.org/abs/2003.08051v3",
  "title": "Unsupervised Domain Adaptation Through Transferring both the Source-Knowledge and Target-Relatedness Simultaneously",
  "authors": [
    "Qing Tian",
    "Yanan Zhu",
    "Chuang Ma",
    "Meng Cao"
  ],
  "abstract": "Unsupervised domain adaptation (UDA) is an emerging research topic in the\nfield of machine learning and pattern recognition, which aims to help the\nlearning of unlabeled target domain by transferring knowledge from the source\ndomain.",
  "text": "Unsupervised Domain Adaptation Through Transferring both the\nSource-Knowledge and Target-Relatedness Simultaneously\nQing Tiana,b,∗, Yanan Zhua,b, Chuang Maa,b and Meng Caoc\naSchool of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China\nbEngineering Research Center of Digital Forensics, Ministry of Education, Nanjing University of Information Science and Technology, Nanjing, China\ncCollege of Computer Science and Technology, Nanjing University of Aeronautics and Astronautic, Nanjing, China\nA R T I C L E I N F O\nKeywords:\nMulti-target domains\nSource-knowledge\nTarget-relatedness\nA B S T R A C T\nUnsupervised domain adaptation (UDA) is an emerging research topic in the ﬁeld of machine learning\nand pattern recognition, which aims to help the learning of unlabeled target domain by transferring\nknowledge from the source domain.\n1. Introduction\nIn this paper, we concentrate on 1SmT and proposes a\nUDA model through transferring both the Source-Knowledge\nand Target-Relatedness, coined as UDA-SKTR for short.\nIn addition, we also present an alternating optimization\nalgorithm to solve the proposed model with convergence\nguarantee. Overall, the contributions of this paper are three-\nfold as follows.\n1. A 1SmT UDA model, coined as UDA-SKTR for\nshort, is constructed by transferring both the source-\nknowledge and the target-relatedness, which is solved\nby a specially designed alternating algorithm with\nconvergence guarantee.\n2. Diﬀerent from existing methods that transfer directly\nfrom source domain data, we perform domain knowl-\nedge transfer from the source domain projection rather\nthan the source domain data itself, which better pro-\ntecting the privacy of the source data.\n3. Extensive evaluation experiments testify the eﬀective-\nness and superiority of the proposed method.\n2. Related work\nIn this section, we brieﬂy review several methods mostly\nrelated to our work, i.e., SLMC.\n2.1. SLMC\nSoft Large-Margin Clustering (SLMC) [1] is typical\nclustering method from the viewpoint of label space along\nthe large-margin principle. It combines the advantages of\nsoft label and large-margin clustering and achieves great\nperformance in many scenes of clustering. For the objective\nfunction of SLMC, it is deﬁned as follows:\nmin\n{W,푢푘푖}\n1\n2 ‖W‖2\n퐹+ 휆\n2\n퐾\n∑\n푘=1\n푛\n∑\n푖=1\n푢2\n푘푖\n‖‖‖퐖푇x푖−l푘‖‖‖\n2\n2\ntianqing@nuist.edu.cn (Q. Tian); 20201220059@nuist.edu.cn (Y.\nZhu); mcboo@nuist.edu.cn (C. Ma); alrash@nuist.edu.cn (M. Cao)\n푠.푡.\n푢푘푖∈[0, 1],\n퐾\n∑\n푘=1\n푢푘푖= 1\n(1)\nwhere W ∈ℝ푑×퐾is the projection matrix with 푑being\nthe feature dimension and 퐾being the total number of data\nclasses, 푛means the number of instances, 푢푘푖denotes the\nclustering degree of membership for the 푖th instance to class\n푘, and 푙푘= [0, ⋯, 0, 1, 0, ⋯, 0]푇∈ℝ퐾represents one-hot\ncoding labels for the 푘th class with the 푘th element being 1\nwhile other elements being 0.\n3. Unsupervised Domain Adaptation through\ntransferring source-knowledge and\ntarget-relatedness\n3.1. Formulation\n3.1.1. Knowledge transfer from source domain to\ntarget domains\nTo perform 1SmT UDA, we obviously need to transfer\nknowledge from the source domain to the target domains.\nConsidering that the target domains are not consistent with\neach other and the domain shift from the source to these\ntargets, it is necessary to introduce a transforming matrix\n(denoted as Q) to increase their matching ﬂexibility. Along\nthis line, we can mathematically formulate the scheme above\nas\nmin\n{W푚\n푇,Q,V푚}\n푀\n∑\n푚=1\n(\n‖‖‖W푚\n푇−QW푆V푚‖‖‖\n2\n퐹+ 훼‖‖V푚‖‖2,1\n)\n푠.푡. Q푇Q = I\n(2)\nIn (2), the ﬁrst term is responsible for knowledge transfer\nfrom the source domain W푆to the target domains W푚\n푇,\ncharacterized by domain transforming matrix Q and domain\nrepresentation matrix V푚. W푆is learned from K-means\nalgorithm. The second term encourages the UDA learning\nto select the most related knowledge components from the\nsource domain to the targets. 훼is a nonnegative tradeoﬀ\nparameter to keep a balance between the two terms. In order\nto prevent degenerated solutions of the transforming matrix\nQ, we restrict it column-orthogonal by 퐐퐓퐐= 퐈.\n00\nPage 1 of 3\narXiv:2003.08051v3  [cs.LG]  24 Dec 2021\narxiv\nTable 1\nDeﬁnition of symbols involved in this paper.\nNotation\nDimension\nMeaning\n푑\nℝ\nThe feature dimension of the data samples\n퐾, 퐾푚\n푇\nℝ, ℝ\nThe classes number of the source domain and the 푚th target domain, respectively\n푁푆, 푁푚\n푇\nℝ, ℝ\nThe samples number of the source domain and the 푚th target domain, respectively\n푀\nℝ\nThe number of target domains\nW푆, W푚\n푇\nℝ푑×퐾, ℝ푑×퐾푚\n푇\nThe projection matrices of the source domain and the 푚th target domain, respectively\nU푚\nℝ퐾푚\n푇×푁푚\n푇\nThe clustering membership matrix on the 푚th target domain\nx푚\n푇,푖\nℝ푑\nThe 푖th instance from the 푚th target domain\nD\nℝ푑×푟\nThe target-relatedness dictionary\nV푚\nℝ퐾×퐾푚\n푇\nThe transfer component matrix on the 푚th target domain from the source domain\nV푚\n푇\nℝ푟×퐾푚\n푇\nThe target-relatedness component matrix on the 푚th target domain\nQ\nℝ푑×푑\nThe transfer transforming matrix\n3.1.2. Knowledge transfer between target domains\nIn the 1SmT scenario, multiple target domains are in-\nvolved, which frequently exhibit potential promising corre-\nlations among them. Such relatedness may contribute to the\ntarget models training. To exploit these shared knowledge\namong these target domains, we propose to establish a\nover-complete representation dictionary to potentially ex-\ntract such target-relatedness. Along this line, we can con-\nsequently construct the formulation for knowledge transfer\nbetween the multi-target domains as follows,\nmin\n{W푚\n푇,D,V푚\n푇}\n푀\n∑\n푚=1\n‖‖‖W푚\n푇−DV푚\n푇\n‖‖‖\n2\n퐹+ 훽‖‖‖V푚\n푇\n‖‖‖2,1\n(3)\nIn (3), the shared dictionary D among the 푀target\ndomains plays the role of bridging them and exploring their\nrelatedness to facilitate their learning. More speciﬁcally, the\nshared dictionary D is established in the targets common\nspace and it is over-complete to cover each of the target\ndomains. That is, the projection matrix W푚\n푇for the 푚th target\ndomain can be recovered by D with its reconstruction coef-\nﬁcient V푚\n푇. In this way, the potential relatedness among the\ntarget domains is integrated into their learning. The second\nterm of (3) aims to select the most knowledge components\nfrom the dictionary to corresponding target domain.\n3.1.3. Unsupervised Domain adaptation by\nconsidering both source-knowledge and\ntarget-relatedness\nThrough taking into account both the considerations\naforementioned in Section 3.1.1 and 3.1.2, we achieve incor-\nporating both the source-knowledge and target-relatedness\ninto 1SmT UDA. In consideration that we concentrate on\nsupervised source domain and unsupervised target domains,\nwithout loss of generality, we readily take the SLMC ob-\njective function (1) for target domain clustering. Eventually,\nwe can consequently build the complete objective function\nof the UDA model through transferring Source-Knowledge\nand Target-Relatedness, UDA-SKTR for short, as follows,\nmin\n{W푚\n푇,D,V푚\n푇,V푆,푢푚\n푘,푖,Q}\n푀\n∑\n푚=1\n(\n1\n2\n퐾푚\n푇\n∑\n푘=1\n푁푚\n푇\n∑\n푖=1\n(푢푚\n푘,푖)2‖l푚\n푘−(W푚\n푇)푇x푚\n푇,푖‖2\n+ 휆1\n2 ‖W푚\n푇‖2\n퐹+ 휆2\n2 ‖W푚\n푇−QW푆V푚‖2\n퐹\n+ 휆3\n2 ‖W푚\n푇−DV푚\n푇‖2\n퐹\n)\n+ 휆4\n(\n‖V푆‖2,1 +\n푀\n∑\n푚=1\n‖V푚\n푇‖2,1\n)\n푠.푡.\n퐾푚\n푇\n∑\n푘=1\n푢푚\n푘,푖= 1, 1 ≤푚≤푀\n0 ≤푢푚\n푘,푖≤1\nQ푇Q = I\n(4)\nwhere 휆1 to 휆4 are nonnegative tradeoﬀparameters. The\nﬁrst two terms are the objective w.r.t. SLMC on the target\ndomains, the third term models target-relatedness among the\n푀target domains, while the fourth term transfers knowledge\nfrom the source to the target domains.\n3.2. Time complexity analysis\nThe time complexity of UDA-SKTR is mainly con-\nsisted of the alternating optimization steps. Assume that\nour model converges after 퐿max iterations, and let 푁max\nand 퐾max denote the maximum sample number and class\nnumber of the target domains. Taking into accounts all the\ntime costs, we conclude that the total time complexity is\n(퐿푁max\n푇\n푑2 + 퐿푁max\n푇\n푑(퐾max\n푇\n)3+퐿푑3).\n00\nPage 2 of 3\narxiv\n4. Conclusion\nIn this paper, we proposed a kind of 1SmT UDA model\nthrough transferring both the Source-Knowledge and Target-\nRelatedness, i.e., UDA-SKTR. In this way, not only the\nsupervision knowledge from the source domain, but also the\npotential relatedness among the target domains are simulta-\nneously modeled for exploitation in 1SmT UDA. In addition,\nwe constructed an alternating optimization algorithm to\nsolve the variables of the proposed model with convergence\nguarantee. Finally, through extensive experiments on both\nbenchmark and real datasets, we validated the eﬀectiveness\nand superiority of the proposed method. In the future, we\nwill consider to extend the model to more challenging multi-\nsource multi-target (mSmT) scenarios and extend it to practi-\ncal application, such as disease detection[2] in medical ﬁeld\nand fault detection[3] in industry.\nReferences\n[1] Y. Wang, S. Chen, Soft large margin clustering, Information Sciences\n232 (2013) 116–129.\n[2] Y. Jin, C. Qin, J. Liu, K. Lin, H. Shi, Y. Huang, C. Liu, A novel domain\nadaptive residual network for automatic atrial ﬁbrillation detection,\nKnowledge-Based Systems 203 (2020) 106122.\n[3] J. Jiao, J. Lin, M. Zhao, K. Liang, Double-level adversarial domain\nadaptation network for intelligent fault diagnosis, Knowledge-Based\nSystems 205 (2020) 106236.\n00\nPage 3 of 3\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-03-18",
  "updated": "2021-12-24"
}