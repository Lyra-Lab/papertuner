{
  "id": "http://arxiv.org/abs/2003.08445v1",
  "title": "Placement Optimization with Deep Reinforcement Learning",
  "authors": [
    "Anna Goldie",
    "Azalia Mirhoseini"
  ],
  "abstract": "Placement Optimization is an important problem in systems and chip design,\nwhich consists of mapping the nodes of a graph onto a limited set of resources\nto optimize for an objective, subject to constraints. In this paper, we start\nby motivating reinforcement learning as a solution to the placement problem. We\nthen give an overview of what deep reinforcement learning is. We next formulate\nthe placement problem as a reinforcement learning problem and show how this\nproblem can be solved with policy gradient optimization. Finally, we describe\nlessons we have learned from training deep reinforcement learning policies\nacross a variety of placement optimization problems.",
  "text": "Placement Optimization with Deep Reinforcement Learning\nAnna Goldie and Azalia Mirhoseini\nagoldie,azalia@google.com\nGoogle Brain\nABSTRACT\nPlacement Optimization is an important problem in systems and\nchip design, which consists of mapping the nodes of a graph onto\na limited set of resources to optimize for an objective, subject to\nconstraints. In this paper, we start by motivating reinforcement\nlearning as a solution to the placement problem. We then give an\noverview of what deep reinforcement learning is. We next formu-\nlate the placement problem as a reinforcement learning problem,\nand show how this problem can be solved with policy gradient\noptimization. Finally, we describe lessons we have learned from\ntraining deep reinforcement learning policies across a variety of\nplacement optimization problems.\nKEYWORDS\nDeep Learning, Reinforcement Learning, Placement Optimization,\nDevice Placement, RL for Combinatorial Optimization\nACM Reference Format:\nAnna Goldie and Azalia Mirhoseini. 2020. Placement Optimization with\nDeep Reinforcement Learning. In Proceedings of the 2020 International Sym-\nposium on Physical Design (ISPD ’20), March 29-April 1, 2020, Taipei, Taiwan.\nACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3372780.3378174\n1\nINTRODUCTION\nAn important problem in systems and chip design is Placement\nOptimization, which refers to the problem of mapping the nodes of\na graph onto a limited set of resources to optimize for an objective,\nsubject to constraints. Common examples of this class of problem\ninclude placement of TensorFlow graphs onto hardware devices to\nminimize training or inference time, or placement of an ASIC or\nFPGA netlist onto a grid to optimize for power, performance, and\narea.\nPlacement is a very challenging problem as several factors, in-\ncluding the size and topology of the input graph, number and\nproperties of available resources, and the requirements and con-\nstraints of feasible placements all contribute to its complexity. There\nare many approaches to the placement problem. A range of algo-\nrithms including analytical approaches [3, 12, 14, 15], genetic and\nhill-climbing methods [4, 6, 13], Integer Linear Programming (ILP)\n[2, 27], and problem-speciﬁc heuristics have been proposed.\nMore recently, a new type of approach to the placement prob-\nlem based on deep Reinforcement Learning (RL) [16, 17, 28] has\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full citation\non the ﬁrst page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nISPD ’20, March 29-April 1, 2020, Taipei, Taiwan\n© 2020 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-7091-2/20/03.\nhttps://doi.org/10.1145/3372780.3378174\nemerged. RL-based methods bring new challenges, such as inter-\npretability, brittleness of training to convergence, and unsafe ex-\nploration. However, they also oﬀer new opportunities, such as the\nability to leverage distributed computing, ease of problem formu-\nlation, end-to-end optimization, and domain adaptation, meaning\nthat these methods can potentially transfer what they learn from\nprevious problems to new unseen instances.\nIn this paper, we start by motivating reinforcement learning as\na solution to the placement problem. We then give an overview\nof what deep reinforcement learning is. We then formulate the\nplacement problem as an RL problem, and show how this problem\ncan be solved with policy gradient optimization. Finally, we describe\nlessons we have learned from training deep RL policies across a\nvariety of placement optimization problems.\n2\nDEEP REINFORCEMENT LEARNING\nMost successful applications of machine learning are examples of\nsupervised learning, where a model is trained to approximate a\nparticular function, given many input-output examples (e.g. given\nmany images labeled as cat or dog, learn to predict whether a given\nimage is that of a cat or a dog). Today’s state-of-the-art super-\nvised models are typically deep learning models, meaning that the\nfunction approximation is achieved by updating the weights of a\nmulti-layered (deep) neural network via gradient descent against a\ndiﬀerentiable loss function.\nReinforcement learning, on the other hand, is a separate branch\nof machine learning in which a model, or policy in RL parlance,\nlearns to take actions in an environment (either the real world or a\nsimulation) to maximize a given reward function. One well-known\nexample of reinforcement learning is AlphaGo [23], in which a pol-\nicy learned to take actions (moves in the game of Go) to maximize\nits reward function (number of winning games). Deep reinforce-\nment learning is simply reinforcement learning in which the policy\nis a deep neural network.\nRL problems can be reformulated as Markov Decision Processes\n(MDPs). MDPs rely on the Markov assumption, meaning that the\nnext state 푠푡+1 depends only on the current state 푠푡, and is condi-\ntionally independent of the past.\n푃(푠푡+1|푠0...푠푡) = 푃(푠푡+1|푠푡)\nLike MDPs, RL problems are deﬁned by ﬁve key components:\n• states: the set of possible states of the world (e.g. the set of\nvalid board positions in Go)\n• actions: the set of actions that can be taken by the agent (e.g.\nall valid moves in a game of Go)\n• state transition probabilities: the probability of transitioning\nbetween any two given states.\nΔ휃퐽(휃,푔) =\nÕ\n푙∼휋휃\nΔ휃휋휃(푙|푔)푅푙,푔\n(3)\n=\nÕ\n푙∼휋휃\n휋휃\nΔ휃휋휃(푙|푔)\n휋휃\n푅푙,푔\n(4)\n=\nÕ\n푙∼휋휃\n휋휃Δ휃푙표푔(휋휃(푙|푔))푅푙,푔\n(5)\n= 퐸[Δ휃푙표푔(휋휃(푙|푔))푅푙,푔]\n(6)\nThe equation above is the basis of various policy gradient opti-\nmization methods, such as REINFORCE [25], PPO [22], and SAC\n[9].\n4\nINGREDIENTS FOR RL SUCCESS\nIn this section, we will share some of the lessons that we have\nlearned in training deep reinforcement learning policies to solve\nplacement problems in computer systems and chip design.\nReward Function: Designing the right reward function is one\nof the most critical decisions. Some properties of eﬀective reward\nfunctions are as follows:\n1) Reward functions should be fast to evaluate; RL training often\nrequires 10-100s of thousands of iterations of reward evaluation\nbefore reaching convergence. While the exact timing that makes a\ntractable reward function depends on the complexity of the problem,\na sub-second reward function would be eﬀective in nearly any\nscenario.\n2) Reward functions should be strongly correlated with the true\nobjective. In many real-world scenarios, we need to use simula-\ntors or proxy reward functions to approximate the true objective,\nwhich may be prohibitively expensive to calculate. If the proxy\nreward is not well-correlated with the true objective, we are solv-\ning the wrong problem and the learned placement is unlikely to\nbe useful. While designing a good simulator or approximate func-\ntion is a challenging task in its own right, it is helpful to build a\nreward function by combining various approximate metrics that\neach independently correlate with the true reward. For example,\nfor TensorFlow placement, the proxy reward could be a composite\nfunction of total memory per device, number of inter-device (and\ntherefore expensive) edges induced by the placement, imbalance of\ncomputation placed on each device. By incorporating a weighted\naverage of multiple proxy rewards, the total variance of the reward\nerror is reduced and over-ﬁtting to a particular proxy metric is\navoided.\n3) Another important factor is correctly engineering the reward\nfunction. This could be as simple as normalizing the reward or\napplying more complex functions to change the shape of the re-\nward. For example, for the device placement problem, measuring\nthe runtime of one step of the TensorFlow graph was the true re-\nward function. Due to the runtime varying widely across diﬀerent\nplacements, using the runtime directly would interfere with learn-\ning and gradient updates. We chose to instead use the square root\nof the runtime, which eﬀectively dampened the range of values.\nAction Space: Another key ingredient is designing the appro-\npriate action space. For example, the problem could be formulated\nas placing the nodes of the netlist one at a time onto the chip netlist,\nor as placing all of the nodes and then deciding which perturbation\n(e.g. swap, shift, rotate, etc.) to apply to each of the nodes in a ﬁxed\nsequence. In device placement, we chose to place all of the Tensor-\nFlow nodes onto hardware devices before evaluating the reward\nfor that placement, because otherwise measuring the reward of\na partial placement would be very diﬃcult, if not impossible. For\nASIC placement, on the other hand, one can deﬁne partial reward\nfunctions, because it is possible to measure changes in metrics, such\nas wirelength and congestion, as nodes are being placed.\nManaging Constraints: The constraints for feasible placements\nvary across placement problems. For example, a common constraint\nis the capacity of placement locations, which limits the number of\nnodes that can be placed onto that location. For example in device\nplacement, the memory footprint of the nodes placed onto a single\ndevice should not exceed the memory limit of that device. Another\nconstraint is that certain nodes cannot co-exist on the same location.\nFor example, in ASIC placement, two macro blocks cannot overlap\non the chip canvas.\nThere are many approaches to enforcing these constraints to\navoid or reduce the number of infeasible placements generated by\nthe policy. Perhaps the most straightforward way to handle the\nconstraints is to penalize the policy with a large negative reward\nwhenever it generates infeasible placements. A challenge with this\nsolution is that the policy does not gain any information about\nhow far this placement was from a feasible placement. In the most\nextreme case, if all of the initial placements generated by the policy\nare infeasible, there will be no positive signal to teach the policy how\nto explore the environment and training will fail. Thus, creating a\nreward function that penalizes the infeasible placements relative to\nhow far they are from viable placements becomes critical.\nAnother approach is to force the policy to only generate feasible\nplacements. This can be accomplished via a function that masks\nout the infeasible placements. For example, a mask can be updated\ngiven the partial placement of the graph nodes. Each time a new\nnode is placed, the density of all the locations is updated (based\non the locations of the nodes that are already placed). The action\nspace then becomes limited to those locations that have enough\nfree capacity to accept the new node. This approach has its own\nchallenges as calculating the mask, similar to calculating the reward,\nmust be done eﬃciently.\nRepresentations: Finally, the way in which state is represented\nhas signiﬁcant impact on the performance of the policy and its\nability to generalize to unseen instances of the placement problem.\nFor example, in the earlier TensorFlow device placement papers [16,\n17], we represented the computational graph as a list of adjacencies,\nindices of the node operations, and sizes of the operations. This\napproach was eﬀective when the policy was trained from scratch\nfor each new TensorFlow graph, but was unable to generalize or\ntransfer what it learned to new graphs. On the other hand, [28] used\ngraph convolutional neural networks to learn better representations\nof the computational graph structure, and were able to transfer\nknowledge across graphs.\n5\nCONCLUSION\nIn this paper we discuss placement optimization with deep rein-\nforcement learning. Deep RL is a promising approach for solving\ncombinatorial problems, and enables domain adaptation and direct\noptimization of non-diﬀerentiable objective functions. Training RL\npolicies is a very challenging task, in part due to the brittleness\nof gradient updates and the costliness of evaluating rewards. In\nthis work, we provide an overview of deep RL, formulate the place-\nment problem as a RL problem, and discuss strategies for training\nsuccessful RL agents.\nWe predict a trend towards more eﬀective RL-based domain\nadaptation techniques, in which graph neural networks will play a\nkey role in enabling both higher sample eﬃciency and more optimal\nplacements. We also foresee a future in which easy-to-use RL-based\nplacement tools will enable non-ML experts to harness and improve\nupon these powerful techniques.\nACKNOWLEDGMENTS\nWe would like to thank our amazing collaborators on deep re-\ninforcement learning for placement research, including Ebrahim\nSonghori, Joe Jiang, Shen Wang, Hieu Pham, Yanqi Zhou, Will Hang,\nAzade Nazi, Sudip Roy, Amir Yazdanbakhsh, Benoit Steiner, Rasmus\nLarsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy\nBengio, Hieu Pham, James Laudon, Quoc Le, and JeﬀDean. We\nwould also like to thank Gabriel Warshauer-Baker for reviewing\nthe paper and improving its clarity.\nREFERENCES\n[1] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2013. Spectral\nNetworks and Locally Connected Networks on Graphs. arXiv:cs.LG/1312.6203\n[2] A. Chakraborty, A. Kumar, and D. Z. Pan. 2009. RegPlace: A high quality open-\nsource placement framework for structured ASICs. In 2009 46th ACM/IEEE Design\nAutomation Conference. 442–447.\n[3] C. Cheng, A. B. Kahng, I. Kang, and L. Wang. 2019. RePlAce: Advancing Solution\nQuality and Routability Validation in Global Placement. IEEE Transactions on\nComputer-Aided Design of Integrated Circuits and Systems 38, 9 (2019), 1717–1730.\n[4] J. P. Cohoon and W. D. Paris. 1987. Genetic Placement. IEEE Transactions on\nComputer-Aided Design of Integrated Circuits and Systems 6, 6 (November 1987),\n956–964. https://doi.org/10.1109/TCAD.1987.1270337\n[5] Michaël Deﬀerrard, Xavier Bresson, and Pierre Vandergheynst. 2016.\nCon-\nvolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.\narXiv:cs.LG/1606.09375\n[6] H. Esbensen. 1992. A genetic algorithm for macro cell placement. In Proceedings\nEURO-DAC ’92: European Design Automation Conference. 52–57. https://doi.org/\n10.1109/EURDAC.1992.246265\n[7] C. Gallicchio and A. Micheli. 2010. Graph Echo State Networks. In The 2010\nInternational Joint Conference on Neural Networks (IJCNN). 1–8. https://doi.org/\n10.1109/IJCNN.2010.5596796\n[8] M. Gori, G. Monfardini, and F. Scarselli. 2005. A new model for learning in graph\ndomains. In Proceedings. 2005 IEEE International Joint Conference on Neural Net-\nworks, 2005., Vol. 2. 729–734 vol. 2. https://doi.org/10.1109/IJCNN.2005.1555942\n[9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft\nActor-Critic: Oﬀ-Policy Maximum Entropy Deep Reinforcement Learning with a\nStochastic Actor. arXiv:cs.LG/1801.01290\n[10] Mikael Henaﬀ, Joan Bruna, and Yann LeCun. 2015. Deep Convolutional Networks\non Graph-Structured Data. arXiv:cs.LG/1506.05163\n[11] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. 2019. When to\nTrust Your Model: Model-Based Policy Optimization. arXiv:cs.LG/1906.08253\n[12] Myung-Chul Kim, Jin Hu, Dong-Jin Lee, and Igor L. Markov. 2011. A SimPLR\nMethod for Routability-Driven Placement. In Proceedings of the International\nConference on Computer-Aided Design (San Jose, California) (ICCAD ’11). IEEE\nPress, 67–73.\n[13] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. 1983. Optimization by Simulated\nAnnealing. Science 220, 4598 (1983), 671–680. https://doi.org/10.1126/science.\n220.4598.671 arXiv:https://science.sciencemag.org/content/220/4598/671.full.pdf\n[14] Yibo Lin, Shounak Dhar, Wuxi Li, Haoxing Ren, Brucek Khailany, and David Z.\nPan. 2019. DREAMPlace: Deep Learning Toolkit-Enabled GPU Acceleration for\nModern VLSI Placement. In Proceedings of the 56th Annual Design Automation\nConference 2019 (DAC ’19). Association for Computing Machinery, New York, NY,\nUSA.\n[15] Jingwei Lu, Pengwen Chen, Chin-Chih Chang, Lu Sha, Dennis Jen-Hsin Huang,\nChin-Chi Teng, and Chung-Kuan Cheng. 2015. EPlace: Electrostatics-Based\nPlacement Using Fast Fourier Transform and Nesterov’s Method. 20, 2 (2015).\n[16] Azalia Mirhoseini, Anna Goldie, Hieu Pham, Benoit Steiner, Quoc V Le, and Jeﬀ\nDean. 2018. A Hierarchical Model for Device Placement. In ICLR.\n[17] Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yue-\nfeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and JeﬀDean.\n2017. Device Placement Optimization with Reinforcement Learning. In ICML.\n[18] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timo-\nthy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn-\nchronous Methods for Deep Reinforcement Learning. arXiv:cs.LG/1602.01783\n[19] OpenAI. [n.d.]. OpenAI Five. https://blog.openai.com/openai-ﬁve/.\n[20] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. 2009. The\nGraph Neural Network Model. IEEE Transactions on Neural Networks 20, 1 (Jan\n2009), 61–80. https://doi.org/10.1109/TNN.2008.2005605\n[21] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter\nAbbeel. 2015. Trust Region Policy Optimization. arXiv:cs.LG/1502.05477\n[22] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal Policy Optimization Algorithms. arXiv:cs.LG/1707.06347\n[23] Huang-A. Maddison C Silver, D. 2016. Mastering the game of Go with deep\nneural networks and tree search. Nature (2016).\n[24] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jader-\nberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard\nPowell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Aga-\npiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky,\nSasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar\nGulcehre, Ziyu Wang, Tobias Pfaﬀ, Toby Pohlen, Dani Yogatama, Julia Cohen,\nKatrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Ko-\nray Kavukcuoglu, Demis Hassabis, and David Silver. 2019. AlphaStar: Mastering\nthe Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/alphastar-\nmastering-real-time-strategy-game-starcraft-ii/.\n[25] R.J. Williams. 1992. Simple statistical gradient-following algorithms for connec-\ntionist reinforcement learning. Mach Learn (1992).\n[26] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang,\nand Philip S. Yu. 2019. A Comprehensive Survey on Graph Neural Networks.\narXiv:cs.LG/1901.00596\n[27] Jinjun Xiong, Yiu-Chung Wong, Egino Sarto, and Lei He. 2006. Constraint Driven\nI/O Planning and Placement for Chip-package Co-design. In APSDAC.\n[28] Yanqi Zhou, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter C. Ma, Qi-\numin Xu, Ming Zhong, Hanxiao Liu, Anna Goldie, Azalia Mirhoseini, and\nJames Laudon. 2019. GDP: Generalized Device Placement for Dataﬂow Graphs.\narXiv:cs.LG/1910.01578\n",
  "categories": [
    "cs.AI"
  ],
  "published": "2020-03-18",
  "updated": "2020-03-18"
}