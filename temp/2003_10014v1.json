{
  "id": "http://arxiv.org/abs/2003.10014v1",
  "title": "Reinforcement Learning in Economics and Finance",
  "authors": [
    "Arthur Charpentier",
    "Romuald Elie",
    "Carl Remlinger"
  ],
  "abstract": "Reinforcement learning algorithms describe how an agent can learn an optimal\naction policy in a sequential decision process, through repeated experience. In\na given environment, the agent policy provides him some running and terminal\nrewards. As in online learning, the agent learns sequentially. As in\nmulti-armed bandit problems, when an agent picks an action, he can not infer\nex-post the rewards induced by other action choices. In reinforcement learning,\nhis actions have consequences: they influence not only rewards, but also future\nstates of the world. The goal of reinforcement learning is to find an optimal\npolicy -- a mapping from the states of the world to the set of actions, in\norder to maximize cumulative reward, which is a long term strategy. Exploring\nmight be sub-optimal on a short-term horizon but could lead to optimal\nlong-term ones. Many problems of optimal control, popular in economics for more\nthan forty years, can be expressed in the reinforcement learning framework, and\nrecent advances in computational science, provided in particular by deep\nlearning algorithms, can be used by economists in order to solve complex\nbehavioral problems. In this article, we propose a state-of-the-art of\nreinforcement learning techniques, and present applications in economics, game\ntheory, operation research and finance.",
  "text": "Reinforcement Learning\nin Economics and Finance\nby\nArthur Charpentier\nUniversit´e du Qu´ebec `a Montr´eal (UQAM)\n201, avenue du Pr´esident-Kennedy,\nMontr´eal (Qu´ebec), Canada H2X 3Y7\narthur.charpentier@uqam.ca\nRomuald ´Elie\nLAMA, Universit´e Gustave Eiﬀel, CNRS\n5, boulevard Descartes\nCit Descartes - Champs-sur-Marne\n77454 Marne-la-Valle cedex 2, France\nromuald.elie@u-pem.fr\nCarl Remlinger\nLAMA, Universit´e Gustave Eiﬀel, CNRS\n5, boulevard Descartes\nCit Descartes - Champs-sur-Marne\n77454 Marne-la-Valle cedex 2, France\nMarch 2020\narXiv:2003.10014v1  [econ.TH]  22 Mar 2020\nAbstract\nReinforcement learning algorithms describe how an agent can learn\nan optimal action policy in a sequential decision process, through re-\npeated experience. In a given environment, the agent policy provides\nhim some running and terminal rewards. As in online learning, the\nagent learns sequentially. As in multi-armed bandit problems, when\nan agent picks an action, he can not infer ex-post the rewards induced\nby other action choices. In reinforcement learning, his actions have\nconsequences: they inﬂuence not only rewards, but also future states\nof the world. The goal of reinforcement learning is to ﬁnd an optimal\npolicy – a mapping from the states of the world to the set of actions, in\norder to maximize cumulative reward, which is a long term strategy.\nExploring might be sub-optimal on a short-term horizon but could\nlead to optimal long-term ones. Many problems of optimal control,\npopular in economics for more than forty years, can be expressed in\nthe reinforcement learning framework, and recent advances in compu-\ntational science, provided in particular by deep learning algorithms,\ncan be used by economists in order to solve complex behavioral prob-\nlems. In this article, we propose a state-of-the-art of reinforcement\nlearning techniques, and present applications in economics, game the-\nory, operation research and ﬁnance.\nJEL: C18; C41; C44; C54; C57; C61; C63; C68; C70; C90; D40; D70; D83\nKeywords: causality; control; machine learning; Markov decision process;\nmulti-armed bandits; online-learning; Q-learning; regret; reinforcement learn-\ning; rewards; sequential learning\n1\n1\nIntroduction\n1.1\nAn Historical Overview\nReinforcement learning is related to the study of how agents, animals, au-\ntonomous robots use experience to adapt their behavior in order to maximize\nsome rewards. It diﬀers from other types of learning (such as unsupervized\nor supervised) since the learning follows from feedback and experience (and\nnot from some ﬁxed training sample of data). Thorndike (1911) or Skinner\n(1938) used reinforcement learning in the context of behavioral psychology,\nethology and biology. For instance, Thorndike (1911) studied learning be-\nhavior in cats, with some popular experiences, using some ‘puzzle box’ that\ncan be opened (from the inside) via various mechanisms (with latches and\nstrings) to obtain some food that was outside the box. Edward Thorndike\nobserved that cats usually began experimenting – by pressing levers, pulling\ncords, pawing, etc. – to escape, and over time, cats will learn how particular\nactions, repeated in a given order, could lead to the outcome (here some\nfood). To be more speciﬁc, it was necessary for cats to explore alternative\nactions in order to escape the puzzle box. Over time, cats did explore less,\nand start to exploit experience, and repeat successful actions to escape faster.\nAnd the cat needed enough time to explore all techniques, since some could\npossibly lead more quickly – or with less eﬀort – to the escape. Thorndike\n(1911) proved that there was a balance between exploration and exploitation.\nThis issue could remind us of the simulated annealing in optimization, where\na classical optimization routine is pursued, and we allow to move randomly\nto another point (which would be the exploration part) and start over (the\nexploitation part). Such a procedure reinforces the chances of converging\ntowards a global optimum, instead of converging to a more local one.\nAnother issue was that a multi-action sequence was necessary to escape,\nand therefore, when the cat was able to escape at the ﬁrst time it was diﬃcult\nto assign which action actually caused the escape. An action taken at the\nbeginning (such as pulling a string) might have an impact some time later,\nafter other actions are performed. This is usually called a credit assignment\nproblem, as in Minsky (1961). Skinner (1938) reﬁned the puzzle box experi-\nment, and introduced the concept of operant conditioning (see Jenkins (1979)\nor Garcia (1981) for an overview). The idea was to modify a part, such as\na lever, such that at some points in time pressing the lever will provide a\npositive reward (such as food) or a negative one (i.e. a punishment, such as\n2\nelectric shocks). The goal of those experiments was to understand how past\nvoluntary actions modify future ones. Those experiments were performed on\nrats, and no longer cats. Tolman (1948) used similar experiments (includ-\ning also mazes) to prove that the classical approach, based on chaining of\nstimulus-responses, was maybe not the good one to model animal (and men)\nbehaviors. A pure stimulus-responses learning could not be used by rats to\nescape a maze, when experimenters start to block roads with obstacles. He\nintroduced the idea of cognitive maps of the maze that allow for more ﬂexi-\nbility. All those techniques could be related to the ones used in reinforcement\nlearning.\nReinforcement learning is about understanding how agents might learn to\nmake optimal decisions through repeated experience, as discussed in Sutton\nand Barto (1981).\nMore formally, agents (animals, humans or machines)\nstrive to maximize some long-term reward, that is the cumulated discounted\nsum of future rewards, as in classical economic models. Even if animals can\nbe seen as have a short-term horizon, they do understand that a punishment\nfollowed by a large reward can be better than two small rewards, as explained\nin Rescorla (1979), that introduced the concept of second-order conditioning.\nA technical assumption, that could be seen as relevant in many human and\nanimal behaviors, is that the dynamics satisﬁes some Markov property, and\nin this article we will focus only on Markov decision processes. Reinforcement\nlearning is about solving the credit assignment problem by matching actions,\nstates of the world and rewards.\nAs we will see in the next section, formally, at time t, the agent at state\nof the world st ∈S makes an action at ∈A, obtains a reward rt ∈R and\nthe state of the world becomes st+1 ∈S. A policy is a mapping from S to\nA, and the goal is to learn from past data (past actions, past rewards) how\nto ﬁnd an optimal policy. A popular application of reinforcement learning\nalgorithms is in games, such as playing chess or Go, as discussed in Silver et al.\n(2018), or Igami (2017) which provides economic interpretation of several\nalgorithms used on games (Deep Blue for chess or AlphaGo for Go) based\non structural estimation and machine (reinforcement) learning. More simply,\nRussell and Norvig (2009) introduced a grid world to explain heuristics about\nreinforcement learning, see Figure 1. Positions on the 4×3 grid are the states\nS, and actions A are movements allowed. The optimal policy π : S →A\nis here computed using sequential machine learning techniques that we will\ndescribe in this article.\n3\nFigure 1: Sequential decision making problem on a 4×3 grid (S states), from\nRussell and Norvig (2009). The agent starts at the state (A,1), and moves\naround the environment, trying to reach terminal state (D,3) to get a +1\nreward - and to avoid terminal state (D,2) where a -1 reward (punishment) is\ngiven. Possible actions (A) are given on the top-right ﬁgure. On the bottom,\ntwo policies are given with π : S →A on the left, and π : S →A ⊂A on\nthe right. In the later case, there can be random selection of actions in some\nstates, for instance π((A,1)) ∈{up, right}.\n1.2\nFrom Machine to Reinforcement Learning\nSupervised Machine Learning techniques is a static problem: given a dataset\nDn = {(yi, xi)}, the goal is to learn a mapping bmn between x and y. In\ndecision theory bmn typically takes values in a binary space, which could be\nto accept or reject a mortgage in credit risk models, or to invest or not in\nsome speciﬁc asset. bmn can also take values in the real line, and denote an\namount of money to save, a quantity to purchase or a price to ask. Online\nlearning is based on the assumption that (yi, xi) arrive in a sequential order,\nand the focus is on the evolution of bmn as n growth, updating the training\ndataset from Dn−1 to Dn. Reinforcement learning incorporates the idea that\n4\nat time n−1, a choice was made, that will inﬂuence (yn, xn), and the standard\ni.i.d. assumption of the dataset is no longer valid. Reinforcement learning is\nrelated to sequential decision making and control.\nConsider an online shop, where the retailer tries to maximize proﬁt by\nsequentially suggesting products to consumers. Consumers are characterized\nby some features, such as their age, or their gender, as well as information\nabout what’s in their shopping cart. The consumer and the shop will have\nsequential interactions. Each round, the consumer can either add a prod-\nuct to the shopping cart, or not buy a product and continue shopping, or\nﬁnally stop shopping and check out. Those transitions are characterized by\ntransition probabilities, function of past states and actions. Such transition\nprobability function is unknown and must be learned by the shop. Should\nthe retailer display the most proﬁtable products, exploiting information he\nobtained previously, or explore actions, that could be less proﬁtable, but\nmight provide relevant information ?\nThe induced problems are related to the fact that acting has consequences,\npossibly delayed. It is about learning to sacriﬁce small immediate rewards in\norder to gain larger long-term ones. If standard Machine Learning is about\nlearning from given data, reinforcement learning is about active experimen-\ntation. Actions can be seen as an intervention, so there are strong connec-\ntions between reinforcement learning and causality modeling. Reinforcement\nlearning allows us to infer consequences of interventions (or actions) used in\nthe past. Pearl (2019) asked the simple economic question ‘what will happen\nif we double the price’ (of an item we try to sell)? ‘Such questions cannot be\nanswered from sales data alone, because they involve a change in customers\nbehaviour, in reaction to the new pricing’. Reinforcement learning is related\nto such problem: inferring the impact of interventions. And the fact that\nintervention will impact the environment, mentioned by Pearl (2019), is pre-\ncisely what reinforcement learning is about. So this theory, central in decision\nscience will appear naturally in sequential experimentation, optimization, de-\ncision theory, game theory, auction design, etc. As we will see in the article\n(and as already mentioned in the previous section), models in sequential de-\ncision making as long history in economics, even if rarely mentioned in the\ncomputational science literature. Most of the articles published in economic\njournal mentioned that such problems were computationally diﬃcult to solve.\nNevertheless, we will try to show that recent advances are extremely promis-\ning, and it is now to possible to model more and more complex economic\nproblems.\n5\n1.3\nAgenda\nIn section 2, we will explain connections between reinforcement learning and\nvarious related topics. We will start with machine learning principles, deﬁn-\ning standard tools that will be extended later one (with the loss function, the\nrisk of an estimator and regret minimization), in section 2.1. In section 2.2,\nwe introduce dynamical problems with online learning, where we exploit past\ninformation sequentially. In section 2.3, we present brieﬂy the multi-armed\nbandit problem, where choices are made, at each period of time, and those\nhave consequences on the information we obtain. And ﬁnally, in section 2.4\nwe start formalizing reinforcement learning models, and give a general frame-\nwork. In those sections, we mainly explain the connections between various\nlearning terms used in the literature.\nThen, we present various problems tackled in the literature, in section\n3. We will start with some general mathematical properties, giving various\ninterpretations of the optimization problem, in section 3.1. Finally, we will\nconclude, in section 3.4, with a presentation of a classical related problem,\ncalled inverse reinforcement learning, where we try to use observed decisions\nin order to infer various quantities, such as the reward or the policy function.\nFinally, three sections are presenting applications of reinforcement learn-\ning. In section 4.1, we discuss applications in economic modeling, starting\nwith the classical consumption and income dynamics, which is a classical\noptimal control problem in economics. We then discuss bounded rationality\nand strong connections with reinforcement learning. Then we will see, start-\ning from Jovanovic (1982), that reinforcement learning can be used to model\nsingle ﬁrm dynamics. And ﬁnally, we present connections with adaptative\ndesign for experiments, inspired by Weber (1992) (and multi-armed bandits).\nIn section 4.2, we discuss applications of reinforcement learning in oper-\nation research, such as the traveling salesman, where the standard dilemma\nexploration/exploitation can be used to converge faster to (near) optimal\nsolutions.\nThen we discuss stochastic games and equilibrium, as well as\nmean-ﬁeld games, and auctions and real-time bidding. Finally, we will ex-\ntend the single ﬁrm approach of the previous section to the case of oligopoly\nand dynamic games.\nFinally, in section 4.3, we detail applications in ﬁnance. We start with\nrisk management, valuation and hedging of ﬁnancial derivatives problems on\nthen focus on portfolio allocation issues. At last, we present a very natural\nframework for such algorithms: market impact and market making.\n6\n2\nFrom Machine to Reinforcement Learning\nMachine learning methods generally make decision based on known proper-\nties learned from the training data, using many principles and tools from\nstatistics. However machine learning models aspire to ﬁnd generalized pre-\ndictive pattern. Most learning problems could be seen as an optimization of\na cost: minimizing a loss or maximizing a reward. But learning algorithms\nseek to optimize a criterion (loss, reward, regret) on training and unseen\nsamples.\n2.1\nMachine Learning principles\nMachine learning has so many branches (supervised vs unsupervised learning,\nonline or not,...) that it is not always easy to identify the label associated to a\ngiven real world problem. Therefore, seeing machine learning as a set of data\nand an optimization criterion is often helpful. To introduce Reinforcement\nLearning (RL), we propose here a regret approach, which ties machine learn-\ning, online aggregation, bandits and, more generally, reinforcement learning.\nIn order to introduce most of machine learning terminology and schemes,\nwe detail a class of models: supervised learning. In this class of models, one\nvariable is the variable of interest, denoted y and usually called the endo-\ngeneous variable in econometrics. To do so, consider some learning sample\nDn = {(y1, x1), ..., (yn, xn)} seen as realization of n i.i.d. random variables\n(Y, X). We wish to map the dataset Dn into a model from the (supposed)\nstatistical relations between xi and yi that are relevant to a task. Note that\nin the context of sequential data we will prefer the generic notation (yt, xt).\nThe goal, when learning, is to ﬁnd a function f ∈F from the input space\nX into the action space A: f : X 7→A. Thus, f(x|Dn) is the action at some\npoint x. An action could be a prediction (for example what temperature\nwill it be tomorrow? Is there a cat on this image?) or a decision (a chess\nmove, go move...). Note that in a standard regression problem A is the same\nas Y, but not necessary in a classiﬁcation problem: in a logistic regression,\nY = {0, 1} but actions can be probabilities A ∈[0, 1].\nThe decision function f is all the better as its actions f(x) are good when\nconfronted to the unseen corresponding output y from Y. The loss function\n(or cost) measures the relevance of these actions when f(x) is taken and y\nhas occurred: ℓ: A × Y 7→R+.\n7\nThe risk is the expectation of the loss:\nR(f) = E\n\u0002\nℓ(f(X), Y )\n\u0003\nThus formalized, the learning could be seen as an optimization problem. We\nwish to ﬁnd a function f ∗∈F which minimizes the cost:\nR(f ∗) = inf\nf∈F {R(f)}\nIf such a function f ∗exists and is unique it is called oracle or target.\nIn most applications we do not know the distribution of the data. How-\never, given a training set Dn = {(x1, y1), . . . , (xn, yn)}, we use the empirical\ndistribution of the training data and deﬁne\nbRn(f) = 1\nn\nn\nX\ni=1\nℓ(f(xi), yi).\nThus, we minimize this empirical risk while trying to avoid over-ﬁtting and\nkeeping in mind that the real objective is to minimize R(f), i.e. the average\nloss computed on any new observation. The main diﬃculty is that the target\nfunction is only deﬁned at the training points.\nFurthermore, we need to restrain the class of target functions or loss\nfunction class. Indeed, It would be impossible to reach sub-linear regret: if\nthe loss is bounded 0 ≤ℓ≤K then Rn ≤Kn, hopefully Rn ≪n\nOne way to evaluate the learning performance is to compute regret. Re-\ngret is deﬁned as the diﬀerence between the actual risk, and the optimal\noracle risk,\nR\n=\nR(f) −R(f ∗)\n=\nR(f) −inf\nf∈F {R(f)}\n=\nE\n\u0002\nℓ(f(X), Y )\n\u0003\n−E\n\u0002\nℓ(f ∗(X), Y )\n\u0003\n.\nIn supervised learning, we prefer the name of excess risk, or excess loss.\nThis notion of regret is particularly relevant in sequential learning, where\nyour action at t depends on previous ones on t −1, t −2, ... . In online (or\nsequential) learning, the regret is measured by the cumulative loss it suﬀers\nalong its run on a sequence of examples. We could see it as the excess loss\nfor not consistently predicting with the optimal model.\nRT = 1\nT\nT\nX\nt=1\nℓ(ft(xt), yt) −inf\nf∈F\n(\n1\nT\nT\nX\nt=1\nℓ(f(xt), yt)\n)\n8\nwhere the ﬁrst term is the estimation error between the target and the predic-\ntion, and the second is the approximation error. Bandits and Reinforcement\nLearning deal with maximizing a reward, instead of minimizing a loss. Thus,\nwe can re-write regret as the diﬀerence between the reward that could have\nbeen achieved and what was actually achieved according to a sequence of\nactions,\nRT = max\na\n(\n1\nT\nT\nX\nt=1\nr(a)\n)\n−1\nT\nT\nX\nt=1\nr(at)\nThus, minimizing a loss or maximizing a reward is the same optimization\nproblem as minimizing the regret, as deﬁned in Robbins (1952).\nFor instance, in the ordinary least squares regression, A = Y = R, and\nwe use the squared loss: ℓ: (a, y) 7→(a−y)2. In that case, the mean squared\nrisk is R(f) = E [(f(X) −Y )2] while the target is f ∗(X) = E [Y |X]. In\nthe case of classiﬁcation, where y is a variable in K categories, A can be a\nselection of a class, so A = Y = {1, . . . , K}. The classical loss in that case is\nthe missclassiﬁcation dummy loss ℓ(a, y) = 1a̸=y, and the associated risk is\nthe misspeciﬁcation probability, R(f) = E\n\u0002\n1f(X)̸=Y\n\u0003\n= P(f(X) ̸= Y ), while\nthe target: is f ∗(X) = argmax\n1≤k≤K\n{P(Y = k|X)}.\nTo go further, Mullainathan and Spiess (2017), Charpentier et al. (2018)\nor Athey and Imbens (2019) recently discussed connections between econo-\nmetrics and machine learning, and possible applications of machine learning\ntechniques in econometrics.\n2.2\nOnline learning\nIn classical (or batch) learning described previously, we want to build an\nestimator bf from Dn = {(xi, yi)} such as the regret E[R( bf)] −inff∈F{R(f)}\nis as small as possible. However, in the online learning framework, we get the\ndata through a sequential process and the training set is changing at each\niteration. Here, observations are not i.i.d, and not necessarily random.\nFollowing Bottou (1998), assume that data become available at a sequen-\ntial order, and the goal is to update our previous predictor with the new\nobservation. To emphasize the dynamic procedure, let t denote the number\nof available observation (instead of n, in order to emphasize the sequential as-\npect of the problem). Formally, from our sample Dt = {(y1, x1), · · · , (yt, xt)}\nwe can derive a model f(x|Dt), denoted ft. The goal in online learning is to\n9\ncompute an update ft+1 of ft using the new observation (yt+1, xt+1).\nAt step t, the learner gets xt ∈X and predicts byt ∈Y, exploiting past\ninformation Dt−1. Then, the real observation yt is revealed and generates a\nloss ℓ(byt, yt). Thus, byt is a function of (xt, (xi, yi)i=1...t−1).\nConsider the case of forecasting with expert advice: expert aggregation.\nHere, K models can be used, in a supervised context, on the same objective\nvariable y, bf1(x|Dt), . . . , bfK(x|Dt).\nQuite naturally, it is possible a linear\ncombination (or a weighted average) of those models,\nbft,ωt(x) =\nK\nX\nk=1\nωk,t bfk(x|Dt)\nA natural question is the optimal choice of the weights ωk,t.\nAssume here, as before, a sequential model. We want to predict element\nby element a sequence of observations y1, . . . , yT. At each step t, K experts\nprovide their forecasts by1,t, . . . , byK,t for the next outcome yt. The aggregation\nweights expert’s prediction byk,t according to a rule in order to build its own\nforecast byt\nbyt =\nK\nX\nk=1\nωk,tbyk,t\nThe weighting process is online: each instant t, the rule adapts the weights to\nthe past observations and the accuracy of their respective experts, measured\nby the loss function for each expert ℓ(yt, byk,t).\nHere, the oracle (or target) is the optimal expert aggregation rule. The\nprediction by∗use best possible weight combination by minimizing the loss.\nThe empirical regret of the aggregation rule f is deﬁned by:\nRT = 1\nT\nT\nX\nt=1\nℓ(by∗\nt , yt) −inf\nω∈Ω\n(\n1\nT\nT\nX\nt=1\nℓ(byt, yt)\n)\nwhere the ﬁrst term is the estimation error between the target and the pre-\ndiction, and the second is the approximation error.\nThere exist several rules for aggregation, the most popular one is probably\nthe Bernstein Online Aggregator (BOA), described in Algorithm 1, which is\noptimal with bounded iid setting for the mean squared loss.\n10\nAlgorithm 1: Bernstein Online Aggregator (BOA).\nData: learning rate γ\nResult: Sequence ω1, . . . , ωn\ninitialization: ω0 ←initial weights (e.g. 1/k);\nfor t ∈{1, 2, . . . , n} do\nLj,t ←ℓ(yt, fj(xt|Dt−1)) −ℓ(yt, bft−1,ωt−1(xt))\nπj,t ←πj,t−1 exp\n\u0002\n−γLj,t(1 + γLj,t)\n\u0003\nexp\n\u0002\n−γ\n\u0003\n;\nend\nThis technique, also called ensemble prediction, based on aggregation of\npredictive models, gives an easy way to improve forecasting by using expert\nforecasts directly. In the context of energy markets, O’Neill et al. (2010)\nshows that a model based on aggregation of simple ones can reduce residential\nenergy cost and smooths energy usage. Levina et al. (2009) considered the\ncase where a supplier predicts consumer demand by applying an aggregating\nalgorithm to a pool of online predictors.\n2.3\nBandits\nA related problem is the one where an agent have to choose, repeatedly,\namong various options but with incomplete information. Multi-armed ban-\ndits come from one-armed bandit, understand slot machines, used in casinos.\nImagine an agent playing with several one-armed bandit machines, each one\nhaving a diﬀerent (unknown) probability of reward associated with.\nThe\ngame is seen as a sequence of single arm pull action and the goal is to maxi-\nmize its cumulative reward. What could be the optimal strategy to get the\nhighest return?\nIn order to solve this problem and ﬁnd the best empirical strategy, the\nagent has to explore the environment to ﬁgure out which arm gives the best\nreward, but at the same time must choose most of the time the empirical\noptimal one. It is the exploration-exploitation trade-oﬀ: each step either\nsearching for new actions or exploiting the current best one.\nThe one-armed bandit problem was used in economics in Rothschild\n(1974), when trying to model the strategy of a single ﬁrm facing a market\nwith unknown demand. In an extension, Keller and Rady (1999) consider\n11\nthe problem of the monopolistic ﬁrm facing an unknown demand that is\nsubject to random changes over time. Note that the case of several ﬁrms ex-\nperimenting independently in the same market was addressed in McLennan\n(1984). The choice between various research projects often takes the form\nof a bandit problem. In Weitzman (1979), each arm represents a distinct\nresearch project with a random reward associated with it. The issue is to\ncharacterize the optimal sequencing over time in which the projects should\nbe undertaken. It shows that as novel projects provide an option value to the\nresearch, the optimal sequence is not necessarily the sequence of decreasing\nexpected rewards. More recently, Bergemann and Hege (1998) and Berge-\nmann and Hege (2005) model venture, or innovation, as a Poisson bandit\nmodel with variable learning intensity.\nMulti-armed bandit problems are a particular case of reinforcement learn-\ning problems. However, in the bandits case the action does not impact the\nagent state. Bandits are an subset of model in online learning; and beneﬁts\nof theoretical results under strong assumptions, most of the time to strong\nfor real-world problems.\nThe multi-armed bandit problem, originally de-\nscribed by Robbins (1952), is a statistical decision model of an agent trying\nto optimize his decisions while improving his information at the same time.\nThe multi-armed bandit problem and many variations are presented in de-\ntail in Gittins (1989) and Berry and Fristedt (1985). An alternative proof of\nthe main theorem, based on dynamic programming can be found in Whittle\n(1983). The basic idea is to ﬁnd for every arm a retirement value, and then\nto choose in every period the arm with the highest retirement value.\nIn bandits, the information that the learner gets is more restraint than\nin general online learning: the learner has only access to the cost (loss or\nreward). At each step t, the learner choose byt ∈{1, . . . , K}. Then the loss\nvector (ℓt(1), . . . , ℓt(K)) is established. Eventually, the learner has access to\nℓt(byt).\nSuch a problem is called |A|−multi-armed bandit in the literature, where\nA is the set of action. The learner has K arms, i.e K probability distributions\n(ν1, . . . , νK). Each step t, the agent pulls an arm at ∈1, . . . , K and receives\na reward rt following the probability distribution νat. Let µk be the mean\nreward of distribution νk. The value of an action at is the expected reward\nQ(at) = E[rt|at]: if action at at t is referring to picking the k-th arm of\nthe slot machine, then Q(at) = µk. The goal is to maximize the cumulative\nrewards PT\nt=1 rt. The bandit algorithm is thus a sequential sampling strategy:\nat+1 = ft(at, rt, . . . , a1, r1).\n12\nTo measure the bandit algorithm performance, we use the previous de-\nﬁned regret. Maximizing the cumulative reward becomes maximizing the\npotential regret, i.e. the loss of not choosing the optimal actions.\nWe note µ∗=\nmax\na∈{1,...,K}{µa} and the optimal policy is\na∗= argmax\na∈{1,...,K}\n\b\nµa\n\t\n= argmax\na∈{1,...,K}\n\b\nQ(a)\n\t\n.\nThe regret of a bandit algorithm is thus:\nRν(A, T) = Tµ∗−E\n\" T\nX\nt=1\nrt\n#\n= Tµ∗−E\n\" T\nX\nt=1\nQ(at)\n#\nwhere the ﬁrst term is the sum of rewards of the oracle strategy which always\nselects a∗, and the second is the cumulative reward of the agent’s strategy.\nWhat could be an optimal strategy ? To get a small regret, a strategy\nshould not select to much sub-optimality arms, i.e. µ∗−µa > 0, which re-\nquires to try all arms to estimate the values of these gaps. This leads to\nthe exploration exploitation trade-oﬀpreviously mentioned. Betting on the\ncurrent best arm at = argmax {µat} is called exploitation, while checking\nthat no other arm are better at ̸= argmax {µat} to ﬁnd a lower gap is called\nexploration. This will be called a greedy action, since it might also be inter-\nesting to explore by selecting a non-optimal action that might improve our\nestimation.\nFor essentially computational reason (mainly keeping record of all the\nrewards on the period), it is preferred to write the value function in an\nincremental expression, as described in Sutton and Barto (1998),\nQt+1 = 1\nt\nt\nX\ni=1\nri = 1\nt ((t −1)Qt + rt) = Qt + 1\nt (rt −Qt)\nThis leads to the general update rule:\nNewEstimate = OldEstimate + StepSize (Target - OldEstimate),\nwhere Target is a noisy estimate of the true target, and StepSize may depends\non t and a. This value function expression, which also identiﬁes to a gradient\ndescent, has already be observed in concerning expert aggregation and will\nbe studied again in the following.\n13\nRecently, Misra et al. (2019) consider the case where sellers must decide,\non real-time, prices for a large number of item, with incomplete demand in-\nformation. Using experiments, the seller learns about the demand curve and\nthe proﬁt-maximizing price. The multi-armed bandit algorithms provides an\nautomated pricing policy, using a scalable distribution-free algorithm.\n2.4\nReinforcement Learning: a short description\nIn the context of prediction and games (tic-tac-toe, chess, go, or video games),\nchoosing the ‘best’ move is complicated. Creating datasets used in the previ-\nous approaches (possibly using random simulation) is too costly, since ideally\nwe would like to get all possible actions (positions on the chess board or hands\nof cards). As explained in Goodfellow et al. (2016, page 105), “some machine\nlearning algorithms do not just experience a ﬁxed dataset. For example, re-\ninforcement learning algorithms interact with an environment, so there is a\nfeedback loop between the learning system and its experiences”.\n2.4.1\nThe concepts\nIn Reinforcement Learning, as in Multi-armed Bandits, data is available at\nsequential order. But the actions depends on the environment, thus an action\nat a certain state could give a diﬀerent reward re-visiting the same state.\nMore speciﬁcally, at time t\n- the learner takes an action at ∈A\n- the learner obtains a (short-term) reward rt ∈R\n- then the state of the world becomes st+1 ∈S\nThe states S refer to the diﬀerent situations the agent might be in. In\nthe maze, the location of the rat is a state of the world. The actions A\nrefer to the set of options available to the agent at some point in time,\nacross all states of the world, and therefore, actions might depend on the\nstate. If the rat is facing a wall, in a dead-end, the only possible action is\nusually to turn back, while, at some crossroad, the rat can choose various\nactions. The rewards set R refer to how rewards (and possibly punishments)\nare distributed. It can be deterministic, or probabilistic, so in many cases,\nagents will compute expected values of rewards, conditional on states and\n14\nactions. These notations were settled in Sutton and Barto (1998), where the\ngoal is to maximize rewards, while previously, Bertsekas and Tsitsiklis (1996)\nsuggested to minimze costs, with some cost-to-go functions.\nAs in Bandits, the interaction between the environment and the agent\ninvolves a trajectory (called also episode). The trajectory is characterized by\na sequence of states, actions and rewards. The initial state leads to the ﬁrst\naction which gives a reward; then the model is fed by a new state followed\nby another action and so on.\nTo determine the dynamics of the environment, and thus the interaction\nwith the agent, the model relies on transition probabilities. It will be based on\npast states, and past actions, too. Nevertheless, with the Markov assumption,\nwe will assume that transition probabilities depend only on the current state\nand action, and not the full history.\nLet T be a transition function S × A × S →[0, 1] where:\nP\n\u0002\nst+1 = s′\f\fst = s, at = a, at−1, at−2, . . .\n\u0003\n= T(s, a, s′).\nAs a consequence, when selecting an action a, the probability distribution\nover the next states is the same as the last time we tried this action in the\nsame state.\nA policy is an action, decided at some state of the world. Formally policies\nare mapping from S into A, in the sense that π(s) ∈A is an action chosen\nin state s ∈S. Note that stochastic policies can be considered, and in that\ncase, π is a S × A →[0, 1] function, such that π(a, s) is interpreted as the\nprobability to chose action a ∈A in state s ∈S. The set of policies is\ndenoted Π.\nAfter time step t, the agent receives a reward rt. The goal is to maximize\nits cumulative reward in the long run, thus to maximize the expected return.\nResuming Sutton and Barto (1998), we can deﬁned the return as the sum of\nthe reward:\nGt =\nT\nX\nk=t+1\nrk\nUnlike in bandits approaches, here the cumulative reward is computed start-\ning from t. Sometimes the agents can receive running reward, associated\nto tasks where there is no notion of ﬁnal time step, so we introduce the\ndiscounted return:\nGt =\n∞\nX\nk=0\nγkrt+1+k\n15\nwhere 0 ≤γ ≤1 is the discount factor which gives more importance to recent\nreward (and can allow Gt to exist). We can also re-write Gt in a recursive\n(or incremental way too) since Gt = rt+1 + γGt+1.\nTo quantify the performance of an action, we introduce, as in the previous\nsection, the action-function, or Q-value on S × A:\nQπ(st, at) = EP\nh\nGt\n\f\f\fst, at, π\ni\n(1)\nIn order to maximize the reward, as in bandits, the optimal strategy is\ncharacterized by the optimal policies\nπ⋆(st) = argmax\na∈A\n\b\nQ⋆(st, a)\n\t\n.\nThat function can be used to derive an optimal policy, and the optimal value\nfunction producing the best possible return (in sense of regret):\nQ⋆(st, at) = max\nπ∈Π\n\b\nQπ(st, at)\n\t\n.\nConsidering optimal strategy and regret leads to the previously mentioned\nexploration exploitation trade-oﬀ. As seen in the bandits section, the learner\ntry various actions to explore the unknown environment in order to learn\nthe transition function T and the reward R. The exploration is commonly\nimplemented by ε-greedy algorithm (described in the bandits section), as in\nMonte-Carlo methods or Q-learning.\nBergemann and Vlimki (1996) provided a nice economic application of\nthe exploration-exploitation dilemma. In this model, the true value of each\nseller’s product to the buyer is initially unknown, but additional information\ncan be gained by experimentation. When assuming that prices are given\nexogeneously, the buyer’s problem is a standard multi-armed bandit problem.\nThe paper in nevertheless original since the cost of experimentation is here\nendogenized.\n2.4.2\nAn inventory illustration\nA classical application of such framework is the control of inventory, with\nlimited size, when the demand is uncertain.\nAction at ∈A denote the\nnumber of ordered items arriving on the morning of day t. The cost is pat if\nthe individual price of items is p (but some ﬁxed costs to order items can also\n16\nbe considered). Here A = {0, 1, 2, . . . , m} where m is the maximum size of\nstorage. States st = S are the number of items available at the end of the day\n(before ordering new items for the next day). Here also, S = {0, 1, 2, . . . , m}.\nThen, the state dynamics are\nst+1 =\n\u0000min{(st + at), m} −εt\n\u0001\n+\nwhere εt is the unpredictable demand, independent and identically distributed\nvariables, taking values in S. Clearly, (st) is a Markov chain, that can be\ndescribed by its transition function T,\nT(s, a, s′) = P\n\u0002\nst+1 = s′\f\fst = s, at = a\n\u0003\n= P\n\u0002\nεt =\n\u0000min{(s + a), m} −s′\u0001\n+\n\u0003\nThe reward function R is such that, on day t, revenue made is\nrt = −pat + pεt = −pat + p\n\u0000min{(st + at), m} −st+1\n\u0001\n+ = R(st, at, st+1)\nwhere p is the price when items are sold to consumers (and p is the price\nwhen items are purchased). Note that in order to have a more interesting\n(and realistic) model, we should introduce ﬁxed costs to order items, as costs\nto store item. In that case\nrt = −pat + p\n\u0000min{(st + at), m} −st+1\n\u0001\n+ −k11at>0 −k2st,\nfor some costs k1 and k2. Thus, reinforcement learning will appear quite\nnaturally in economic problems, and as we will see in the next section, sev-\neral algorithms can be used to solve such problems, especially when some\nquantities are unknown, and can only be estimated... assuming that enough\nobservations can be collected to do so.\n3\nReinforcement Learning\nNow that most of essential notions have been deﬁned and explained, we\ncan focus on Reinforcement Learning principles, and possible extensions.\nThis section deals with the most common approaches, its links with ordinary\neconomy or ﬁnance problems and, eventually, some know diﬃculties of those\nmodels.\n17\n3.1\nMathematical context\nClassically, a Markov property is assumed on the reward and the obser-\nvations.\nA Markov decision process (MDP) is a collection (S, A, T, r, γ)\nwhere S is a state space, A is an action space, T the transition function\nS × A × S →[0, 1], R is a reward function S × A × S →R+ and γ ∈[0, 1)\nis some discount factor. A policy π ∈Π is a mapping from S to A.\nAlgorithm 2: Policy generation\nData: transition function T and policy π\nResult: Sequence (at, st)\ninitialization: s1 ←initial state;\nfor t ∈{1, 2, . . . } do\nat ←π(st) ∈A ;\nst+1 ←T(st, at, ·) = P\n\u0002\nst+1 = ·\n\f\fst, at,\n\u0003\n∈S ;\nend\nGiven a policy π, its expected reward, starting from state s ∈S, at time\nt, is\nV π(st) = EP\n X\nk∈N\nγkrt+k\n\f\f\fst, π\n!\n(2)\ncalled value of a state s under policy π, where rt = Ea[R(st, a, st+1)] when\na ∼π(st, ·) and P is such that P(St+1 = st+1|st, at) = T(st, a, st+1). Since the\ngoal in most problem is to ﬁnd a best policy – that is the policy that receives\nthe most reward – deﬁne\nV ⋆(st) = max\nπ∈Π\n\b\nV π(st)\n\t\nAs in Watkins and Dayan (1992), one can deﬁne the Q-value on S × A\nas\nQπ(st, at) = EP\n X\nk∈N\nγkrt+k\n\f\f\fst, at, π\n!\nwhich can be written, from Bellman’s equation (see Bellman (1957))\nQπ(st, at) =\nX\ns′∈S\n\u0002\nr(st, at, s′) + γQπ(s′, π(s′))\n\u0003\nT(st, at, s′)\n(3)\n18\nAlgorithm 3: Policy valuation\nData: policy π, threshold ϵ > 0, reward R(s, a, s′), ∀s, a, s′\nResult: Value of policy π, V π\ninitialization: V (s) for all s ∈S and ∆= 2ϵ;\nwhile ∆> ϵ do\n∆←0 for s ∈S do\nv ←V (s) ;\nV (s) ←\nX\na∈A\nπ(a, s)\nX\ns′∈∫\nT(s, a, s′)\n\u0002\nR(s, a, s′) + γV (s′)\n\u0003\n;\n∆←max{∆, |v −V (s)|}\nend\nend\nand as previously, let\nQ⋆(st, at) = max\nπ∈Π\n\b\nQπ(st, at)\n\t\n.\nObserve that Qπ(st, at) identiﬁes to the value function in state st when play-\ning action at at time t and then acting optimally. Hence, knowing the Q-\nfunction directly provides the derivation of an optimal policy\nπ⋆(st) = argmax\na∈A\n\b\nQ⋆(st, a)\n\t\n.\nThis optimal policy π⋆assigns to each states s the highest-valued action. In\nmost applications, solving a problem boils down to computing the optimal\npolicy π⋆.\nNote that with ﬁnite size spaces S and A, we can use a vector form for\nQπ(s, a)’s, Qπ, which is a vector of size |S||A|. In that case, Equation (3)\ncan be written\nQπ = R + γP ΠQπ\n(4)\nwhere R is such that\nR(s,a) =\nX\ns′∈S\nr(st, at, s′)T(st, at, s′)\nand P Π is the matrix of size |S||A| × |S||A| that constraints transition\nprobabilities, from (s, a) to (s′, π(s′)) (and therefore depends on policy π).\n19\nIf we use notations introduced in section 2.4, we have to estimate Q(s, a)\nfor all states s and actions a, or function V (s). Bellman equation on Qπ\nmeans that V π satisﬁes\nV π(st) =\nX\ns′∈S\n\u0002\nr(st, π(st), s′) + γV π(s′)\n\u0003\nT(st, π(st), s′).\n(5)\nAlgorithm 4: Direct policy search\nData: A threshold ϵ, reward R(s, a, s′), ∀s, a, s′\nResult: Optimal policy π⋆\ninitialization: V (s) for all s ∈S and ∆= 2ϵ;\nwhile ∆> ϵ do\n∆←0 for s ∈S do\nv ←V (s) ;\nV (s) ←max\na∈A\n(X\ns′∈S\nT(s, a, s′)\n\u0002\nR(s, a, s′) + γV (s′)\n\u0003\n)\n;\n∆←max{∆, |v −V (s)|};\nend\nend\nfor s ∈S do\nπ(s) ←argmax\na∈A\n(X\ns′∈S\nT(s, a, s′)\n\u0002\nR(s) + γV (s′)\n\u0003\n)\n;\nend\nUnfortunately, in many applications, agents have no prior knowledge of\nreward function r, or transition function T (but do know that it satisﬁes the\nMarkov property). Thus, the agent will have to explore – or perform actions\n– that will give some feedback, that can be used, or exploited.\nAs discussed previously, Q function is updated using\nQ(s, a) ←(1 −α)Q(s, a) + α\n\u0000r(s, a, s′) + γ max\na′∈A\n\b\nQ(s′, a′)\n\t\u0001\n.\nA standard procedure for exploration is the ϵ-greedy policy, mentioned\nalready in the bandit context, where the learner makes the best action with\nprobability 1 −ϵ, and consider a randomly selected action with probability\nϵ. Alternatively, consider some exploration function that will give preference\n20\nto less-visited states, using some sort of penalty\nQ(s, a) ←(1 −α)Q(s, a) + α\n\u0012\nr(s, a, s′) + γ max\na′∈A\n\u001a\nQ(s′, a′) +\nκ\nns,a\n\u001b\u0013\n.\nwhere ns,a denotes the number of times where state (s, a) has been visited,\nwhere κ will be related to some exploration rate. Finally, with the Boltzmann\nexploration strategy, probabilities are weighted with their relative Q-values,\nwith\np(a) =\neβQ(s,a)\neβQ(s,a1) + · · · + eβQ(s,an),\nfor some β > 0 parameter. With a low value for β, the selection strategy\ntends to be purely random. On the other hand, with a high value for beta,\nthe algorithm selects the action with the highest Q-value, and thus, ceases\nthe experiment.\n3.2\nSome Dynamical Programming principles\nIn Dynamic Programming, as well as in most of Reinforcement Learning\nproblem, we use value functions to choose actions and build an optimal policy.\nMany algorithms of this ﬁeld compute optimal policies in a fully know model\nin a Markov decision process environment. It is not always possible in real-\nworld problems or too computational expensive. However, Reinforcement\nLearning lies on several principles of Dynamic Programming and we present\nhere a way to obtain an optimal policy once we have found the optimal value\nfunctions which satisfy the Bellman equation: the Policy iteration.\n3.2.1\nPolicy iteration\nValue function V π satiﬁes Equation (5), or to be more speciﬁc a system of\n|S| linear equations, that can be solved when all functions – T and r – are\nknown.\nAn alternative is to use an iterative procedure, where Bellman’s\nEquation is seen as a updating rule, where V π\nk+1 is an updated version of V π\nk\nV π\nk+1(st) =\nX\ns′∈S\n\u0002\nr(st, π(st), s′) + γV π\nk (s′)\n\u0003\nT(st, π(st), s′).\n(6)\nThe value function V π is a ﬁxed point of this recursive equation.\n21\nOnce we can evaluate a policy π, Howard (1960) suggested a simple iter-\native procedure to ﬁnd the optimal policy, called policy iteration. The value\nof all action a is obtained using\nQπ(st, a) =\nX\ns′∈S\n\u0002\nr(st, a, s′) + γV π(s′)\n\u0003\nT(st, a, s′),\nso if Qπ(st, a) is larger than V π(st) for some a ∈A, choosing a instead of\nπ(st) would have a higher value. It is then possible to improve the policy\nby selecting that better action. Hence, a greedy policy π′ can be considered,\nsimply by choosing the best action,\nπ′(st) = argmax\na∈A\n{Qπ(st, a)}.\nThe algorithm suggested by Howard (1960) starts from a policy π0, and then,\nat step k, given a policy πk, compute its value V πk then improve it with πk+1,\nand iterate.\nUnfortunately, such a procedure can be very long, as discussed in Bert-\nsekas and Tsitsiklis (1996). And it assumes that all information is available,\nwhich is not the case in many applications. As we will see in the next sec-\ntions, it is then necessary to sample to learn the model – the transition rate\nand the reward function.\n3.2.2\nPolicy Iteration using least squares\nQπ(s, a) is essentially an unknown function, since it is the expected value\nof the cumulated sum of discounted future random rewards. As discussed\nin Section 2.2, a natural stategy is to use a parametric model, Qπ(s, a, β)\nthat will approximate Qπ(s, a). Linear predictors are obtained using a linear\ncombination of some basis functions,\nQπ(s, a, β) =\nk\nX\nj=1\nψj(s, a)βj = ψ(s, a)⊤βj,\nfor some simple functions ψj, such as polynomial transformations. With the\nnotation of section 2.4.1, write Qπ = Ψβ. Thus, substituting in equation\n(4), we obtain\nΨβ ≈R + γP ΠΨβ or\n\u0000Φ −γP ΠΨ\n\u0001\nβ ≈R.\n22\nAs in section 2.4.1, we have an over-constrained system of linear equations,\nand the least-square solution is\nβ⋆=\n\u0000(Ψ −γP ΠΨ)⊤(Ψ −γP ΠΨ)\n\u0001−1(Ψ −γP ΠΨ)⊤R.\nThis is also called Bellman residual minimizing approximation.\nAnd as\nproved in Nedi´c and Bertsekas (2003) and Lagoudakis and Parr (2003), for\nany policy π, the later can be written\nβ⋆=\n\u0000Ψ⊤(Ψ −γP ΠΨ)\n|\n{z\n}\n=A\n\u0001−1 Ψ⊤R\n| {z }\n=b\n.\nUnfortunately, when rewards and transition probability are not given, we\ncannot use (directly) the equations obtained above. But some approximation,\nbased on previous t observed values can be used. More precisely, at time t\nwe have a sample Dt = (si, ai, ri), and we can use algorithm 5.\nAlgorithm 5: Least square policy iteration\nData: Policy π, γ, sample Dt and basis functions ψj\nResult: Optimal π\ninitialization bA ←0 and bB ←0;\nfor i ∈{1, 2, · · · , t −1} do\nbA ←bA + ψ(si, ai)\n\u0000ψ(si, ai) −γψ(si+1, π(si+1))\n\u0001⊤;\nbb ←bb + ψ(si, ai)ri ;\nend\nbβ⋆←bA\n−1bb ;\nπ⋆(s) ←argmax\na∈A\nn\nψ(s, a)⊤bβ⋆\no\nIf states and actions are uniformely observed on those t past values, bA\nand bb converge respectively towards A and b and therefore, bβ⋆= bA\n−1bb is a\nconsistent approximation of β⋆.\n3.2.3\nModel-Based vs Model-Free Learning\nModel-based strategies are based on a fully known environment.\nWe can\nlearn about the state transition T(st, at, st+1) = P(St+1 = st+1|st, at) and the\n23\nreward function R(st) and ﬁnd the optimal solution using dynamic program-\nming. Starting from s0, the agent will chose randomly selection actions in\nA at each step. Let (si, ai, si+1) denote the simulated set of present state,\npresent action and future state. After n generations, the empirical transition\nis\nbTn(s, a, s′) =\nP\ni 1(s,a,s′)(si, ai, si+1)\nP\ni 1(s,a)(si, ai)\nand\nbRn(s, a, s′) =\nP\ni R(si, ai, si+1)\nP\ni 1(s,a,s′)(si, ai, si+1)\nBy the law of large numbers, bTn and bRn will respectively converge towards\nT and R, as n goes to inﬁnity. This is the exploration part.\nThat strategy is opposed to so-called model-free approaches.\nIn the next sections, we will describe classical model-free algorithms:\nTemporal-Diﬀerence (TD), Policy Gradient and Actor-Critic. For the ﬁrst\none, we will focus on one signiﬁcant breakthroughs in reinforcement learn-\ning, the Q-learning (introduced in Watkins (1989)), an oﬀ-policy TD control\nmodel. As TD approach, it will necessitate to interact with the environment,\nmeaning that it will be necessary to simulate the policy, and to generate\nsamples, as in the generalized policy iteration (GPI) principle, introduced\nin Sutton and Barto (1998). Recent works using neural network, like Deep\nQ-Network (DQN) show impressive results in complex environment.\n3.3\nSome Solution Methods\nHere is presented brieﬂy some common methods to solve Reinforcement\nLearning problems.\n3.3.1\nQ-learning\nQ-learning was introduced in Watkins and Dayan (1992). Bellman Equation\n(3) was\nQπ(st, at) =\nX\ns′∈S\n\u0002\nR(st, at, s′) + γQπ(s′, π(s′))\n\u0003\nT(st, at, s′),\n24\nand the optimal value was satisﬁes\nQ⋆(st, at) =\nX\ns′∈S\n\u0002\nR(st, at, s′)+γV ⋆(s′)\n\u0003\nT(st, at, s′) where V ⋆(s′) = max\na′∈A\n\b\nQ⋆(s′, a′)\n\t\n.\nThus, Q-learning is based on the following algorithm: starting from Q0(s, a),\nat step k + 1 set\nQk+1(s, a) =\nX\ns′∈S\n\u0002\nR(s, a, s′) + γ max\na′∈A\n\b\nQk(s′, a′)\n\t\u0003\nT(s, a, s′).\nThis approach is used in Hasselt (2010) where the Q-function, i.e. value-\nfunction, is approximated by a neural network.\n3.3.2\nPolicy Optimization\nIn order to avoid computing and comparing the expected return of diﬀer-\nent actions, as in Q-learning, an agent could learn directly a mapping from\nstates to actions. Here, we try to infer a parameterized policy π(a|s, θ) that\nmaximizes the outcomes reward from an action on an environment. Pol-\nicy learning converges faster than Value-based learning process and allows\ncontinuous action space of the agent as the policy is now a parameterized\nfunction depending on θ. An inﬁnite number of actions would be compu-\ntationally too expensive to optimize. This approach is based the on Policy\nGradient Theorem from Sutton and Barto (1998).\n3.3.3\nApproximate Solution Methods: Actor-Critic\nActor-Critics aim to take advantage of both Value and Policy approaches .\nBy merging them, it can beneﬁt of continuous and stochastic environments\nand faster convergence of Policy learning, and sample eﬃciency and steady\nof Value one.\nIn the Actor-Critic approach, two model interact in order\nto gives the best cumulative reward. Using simultaneously an actor, which\nupdates the policy parameter, and a critic which updates the value function\nor action-value function, this model is able to learn complex environments\nas well as complex Value-functions.\n3.4\nInverse Reinforcement Learning\nIn the econometric literature, this problem can be found in many articles\npublished in the 80’s, such as Miller (1984) in the context of job match-\n25\ning and occupational choice, Pakes and Schankerman (1984) on the rate of\nobsolescence of patents, and research gestation lags, Wolpin (1984) on the es-\ntimation of a dynamic stochastic model of fertility and child mortality, Pakes\n(1986) on optimal investment strategies or Rust (1987) on replacement of bus\nengines, where structural models are used to better understand human de-\ncision making. Hotz and Miller (1993), Aguirregabiria and Mira (2002) or\nmore recently Magnac and Thesmar (2002) or Su and Judd (2012) mentioned\nthe computational complexity of such algorithms on economic applications.\nMost of those approaches are related to the literature on dynamic discrete\nchoice model (see Aguirregabiria and Mira (2010) for a survey, or Semenova\n(2018) for connections with machine learning tools). In those models, there is\na ﬁnite set of possible actions A, as assumed also in the previous descriptions,\nand they focus on conditional choice probability, which is the probability that\nchoosing a ∈A is optimal in state s ∈S,\nccp(a|s) = P[a is optimal in state s] = P\n\u0002\n{Q(a, s) ≥Q(a′, s), ∀a′ ∈A}\n\u0003\n.\nAssuming that rewards have a Gumbel distribution, we obtain a multinomial\nlogit model, where the log-odds ratios are proportional to the value function.\nFor instance in the bus-repair problem of Rust (1987), the state s is the\nmileage of the bus, and the action a is in the set {opr, rep} (either operate,\nor replace). Per period, the utility is\nUθ(st, εt, a) = εt + uθ = εt +\n\u001a −OCθ(st)\nif a = opr\n−RC −OCθ(0)\nif a = rep\nwhere RC is some (ﬁxed) replacing cost, OCθ is the operating cost (that\nmight depend on some parameter θ), and εt is supposed to have a Gumbel\ndistribution. The respective costs are supposed to be known\nThen\nccpθ(a|s) =\nexp[vθ(s, a)]\nexp[vθ(s, opr)] + exp[vθ(s, rep)]\nwhere vθ(s, a) = uθ(s, a) + βEVθ(s, a) where ESθ(s, a) is the unique solution\nof\nEVθ(s, a) =\nZ\nlog\n\u0002\nuθ(s, opr)+uθ(s′, opr)+β(EVθ(s, opr)+EVθ(s′, rep))\n\u0003\nT(s′|s, a)\nHotz and Miller (1993) proved that the mapping between conditional choice\nprobabilities and choice speciﬁc value function is invertible. As discussed in\n26\nSu and Judd (2012), based on observed decisions made by the superintendent\nof maintenance of the bus company, structural estimation is computationally\ncomplex.\nThe main idea of inverse reinforcement learning (or learning from demon-\nstration, as deﬁned in Schaal (1996)) is to learn the reward function based on\nthe agent’s decisions, and then ﬁnd the optimal policy (the one that maxi-\nmizes this reward function) using reinforcement learning techniques. Similar\ntechniques are related to this idea.\nIn imitation learning (also called be-\nhavioral cloning in Bain and Sammut (1995)), we learn the policy using su-\npervised learning algorithms, based on the sample of observations {(si, ai)},\nthat is unfortunately not distributed independently and identically in the\nstate-action space. In apprenticeship learning, we try to ﬁnd a policy that\nperform as well as the expert policy, as introduced in Abbeel and Ng (2004).\nRothkopf and Dimitrakakis (2011) mentioned applications of reinforcement\nlearning on preference elicitation, extended in Klein et al. (2012). See Ng\net al. (2000) for a survey of various algorithms used in inverse reinforcement\nlearning, as well as Abbeel and Ng (2004).\n4\nApplications\n4.1\nApplications in Economic Modeling\nIf it is possible to ﬁnd a framework very similar to the one use in reinforcement\nlearning in old economic literature (see for instance the seminal thesis Hellwig\n(1973)), as mentioned in Arthur (1991) or Barto and Singh (1991), two survey\nof reinforcement learning techniques in computational economics, published\nthirty years ago. Recently, Hughes (2014) updated the survey on applications\nof reinforcement learning to economic problems with up-to-date algorithms.\n4.1.1\nConsumption and Income Dynamics\nConsider an inﬁnitely living agent, with utility u(ct) when consuming ct ≥0\nin period t. That agent receives random income yt at time t, and assume that\n(yt) is a Markov process with transition T(s, s′) = P[yt+1 = s′|yt = s]. Let wt\ndenote the wealth of the agent, at time t, so that wt+1 = wt +yt −ct. Assume\nthat the wealth must be non-negative, so ct ≤wt + yt. And for convenience,\nw0 = 0, as in Lettau and Uhlig (1999). At time t, given state st = (wt, yt),\n27\nwe seek c⋆\nt solution of\nv(wt, yt) =\nmax\nc∈[0,wt+yt]\n(\nu(c) + γ\nX\ny′\n\u0002\nv(wt + yt −c, y′)\n\u0003\nT(yt, y′)\n)\nThis is a standard recursive model, discussed in Ljungqvist and Sargent\n(2018) or Hansen and Sargent (2013), assuming that utility function u is\ncontinuous, concave, strictly increasing and bounded, the value function v\nis itself continuous, concave, strictly increasing and bounded in wealth wt,\nand gives a unique decision function c⋆(wt, yt). Stokey et al. (1989) extented\nthat model to derive a general dynamic decision problem where income y\nis now a state s ∈S = {s1, . . . , sn}, and consumption c is now an action\na ∈A = {a1, . . . , am}. Utility is now a function of (s, a), and it is assume\nthat the state process (st) is a Markov chain, with transition matrix T a\n(and transition function Ta). The decision problem is written as a dynamic\nproblem\nv(s) = max\na∈A\n\b\nu(s, a) + γEs′∼Ta\n\u0002\nv(s′)\n\u0003\t\nUsing contraction mapping theorems, there is a unique solution v⋆to this\nproblem, that can be characterized by some decision function π⋆: S 7→A\nthat prescribes the best action π⋆(s) in each state s.\nvπ(s) = u(s, π(s)) + γEs′∼Ta\n\u0002\nvπ(s′)\n\u0003\nThe solution can be obtained easily using some matrix formulation, vπ =\n(In −γT π)−1uπ, where vπ = (vπ(si)) ∈Rn, T π = [T π(si)(sj)] is a n × n\nmatrix, and uπ = (si, π(si)) ∈Rn. Once vπ is obtained for any policy π,\nthen v⋆is the maximum value. Stokey et al. (1989) gives several rules of\nthumb to solve that problem more eﬃciently, inspired by Holland (1986).\nIn the context of multiple agents, Kiyotaki and Wright (1989) describes\nan economy with three indivisible goods, that could be stored, but with a\ncost, and three types of agents, inﬁnitely living, favoring one of the good. In\nBasci (1999), agents do not know the equilibrium strategies and act according\nto some randomly held beliefs regarding the values of the possible actions.\nAgents have opportunities of both learning by experience, and by imitation.\nBasci (1999) observes that the presence of imitation either speeds up social\nconvergence to the theoretical Markov-Nash equilibrium or leads every agent\nof the same type to the same mode of suboptimal behavior. We will discuss\nNash equilibrium with multiple agents in the next section.\n28\n4.1.2\nBounded Rationality\nSimon (1972) discussed the limits of the rationality concept, central in most\neconomic models, introducing the notion of bounded rationality, related to\nvarious concepts that were studied afterwards, such as bounded optimality\n(as in Russell and Subramanian (1995) with possible limited thinking time,\nor memory constraints) or computational rationality (as deﬁned in Gershman\net al. (2015)) minimal rationality (such as Cherniak (1986) where minimal\nsets of conditions to have rationality are studied), ecological or environmen-\ntal rationality (with a close look at the environment, that will inﬂuence de-\ncisions, as discussed in Gigerenzer and Goldstein (1996)).\nMore recently,\nKahneman (2011) popularized this concept with the two modes of thought:\nsystem 1 is fast, instinctive and emotional while System 2 is slower, more\ndeliberative, and more logical. Simon (1972) suggests that bounded ratio-\nnality can be related to uncertainty, incomplete information, and possible\ndeviations from the original goal, emphasizing the importance of heuristics\nto solve complex problems, also called practical rationality (see Rubinstein\n(1998) of Aumann (1997) for some detailed survey). Recently, Leimar and\nMcNamara (2019) suggested that adaptive and reinforcement learning leads\nto bounded rationality, while Abel (2019) motivates reinforcement learning\nas a suitable formalism for studying boundedly rational agents, since “at a\nhigh level, Reinforcement Learning uniﬁes learning and decision making into\na single, general framework”.\nSimon (1972) introduce dthe problem of inﬁnite regress, where agents are\nspending more resources on ﬁnding the optimal simpliﬁcation of the problem\nthan solving the original problem. This simpliﬁcation problem is related to\nthe sparsity issue in standard supervised learning. Gabaix (2014) discussed\nalgorithms for ﬁnding a sparse model, either with short range memory, or\nfocusing on local thinking, as deﬁned in Gennaioli and Shleifer (2010) (where\nagents combine data received from the external world with information re-\ntrieved from memory to evaluate a hypothesis). Reinforcement learning pro-\nvides powerful tools to solve complex problems, where agents are suppose\nto have bounded rationality. And the literature (in reinforcement learning)\nhas developed sereval measures for evaluating the capacity of an agent to\neﬀectively explore its environment. The ﬁrst one is the regret of an agent,\nwhich measures how much worse the agent is relative to the optimal strategy\n(that could be related to unbounded rationality). The second one is the sam-\nple complexity (or computational complexity) which measures the number of\n29\nsamples an agent need before it can act near-optimally, with high probability.\n4.1.3\nSingle ﬁrm dynamics\nJovanovic (1982) gave the framework for most models dealing with industry\ndynamics with Bayesian learning. In a model of competition between ﬁrms\nwith multiple equilibrium, ﬁrms are engaged in an adaptive process, where\nthey learn how to play an equilibrium of the game, as in Fudenberg and\nLevine (1998). In those models, ﬁrms know the model that describes the\nenvironment, but there are uncertainties. So agents will learn over time about\nthese elements, when new information arrives. Note that this approach is\ndiﬀerent from the one in evolutionary game theory (as in Samuelson (1997))\nfor instance, where agents might not even know that they play a game.\nConsider a monopolistic ﬁrm, taking actions at ∈A – say investment\ndecisions – in order to maximize its expected discounted inter-temporal proﬁt.\nStates of the world are st ∈S, and we assume that they can be modeled via\na Markov process. If future investments are uncertain, it can be assumed\nthat the ﬁrst will use the same optimal decision rule that the one it uses at\ntime t, taking into account available information. Let rt denote the proﬁt\nobtained at time t.\nIn economic literature, rational expectations were usually considered in\nearly models, meaning that the expectation is computed under the true\ntransition probability. Nevertheless, Cyert and DeGroot (1974) or Feldman\n(1987) suggested that the ﬁrst should learn this transition probability π, and\na Bayesian framework was considered. Starting from a prior belief, transi-\ntion probabilities T are supposed to belong to some space T , and experience\nis used to update mixing probabilities on T . Sargent (1993) considered a\nweaker updating rule, simpler (related to linear approximations in Bayesian\nmodels) but not optimal, usually called adaptative learning. In that case,\nbelief at time t, Tt(s, a, s′) is a weighted sum of Tt−1(s, a, s′) and some dis-\ntance between T(s, a, s′) and (st−1, at−1, st) (through some kernel function).\nIf the weight related to the new observation is of order 1/t, recursive least\nsquares learning is obtained; if weights are constant, adaptative learning is\nhere faster than standard Bayesian learning, which is usually seen as a good\nproperty when there are shocks in the economy.\nErev and Roth (1998) explicitly introduced the idea of stock of reinforce-\nment, corresponding to the standard Q-function. and for any action-state\n30\npair (a, s), the updating rule is\nQt+1(a, s) ←Qt(a, s) + γtk\n\u0000(a, s) −(at, st)\n\u0001\nwhere some kernel k is considered. Recently, Ito and Reguant (2016) used\nreinforcement learning to describe sequential energy markets.\n4.1.4\nAdaptative design for experiments\nMost experiments are designed to inform about the impact of choosing a\npolicy, among various that can be considered. And more precisely, as dis-\ncussed in Kasy and Sautmann (2019), the question which program will have\nthe largest eﬀect is usually preferred to the question does this program have\na signiﬁcant eﬀect, in many cases, see Chattopadhyay and Duﬂo (2004) and\nmore recently Athey and Imbens (2016), and references therein. If dynamic\nexperiments are considered, there are usually several waves, and the op-\ntimal experimental design would usually learn from earlier waves, and as-\nsign more experimental agents to the better-performing treatments in future\nwaves. Thus, this policy choice problem is a ﬁnite-horizon dynamic stochas-\ntic optimization problem. Thompson (1933) introduced this idea of adaptive\ntreatment assignment, and Weber (1992) proved that this problem can be\nexpressed using multi-armed bandits, and the optimal solution to this bandit\nproblem is to choose the arm with the to the highest Gittins index, that can\nbe related to the so-called Thompson sampling strategy. Thompson sampling\nsimply assigns the next wave of agents to each treatment with frequencies\nproportional to the probability that that each treatment is the optimal one.\nAs explained in Kasy and Sautmann (2019), standard experimental de-\nsigns are geared toward point estimation and hypothesis testing. But they\nconsider the problem of treatment assignment in an experiment with several\nnon-overlapping waves, where the goal is to choose among a set of possi-\nble policies (here treatments). The optimal experimental design learns from\nearlier waves, and assigns more experimental units to the better-performing\ntreatments in later waves : assignment probabilities are an increasing con-\ncave function of the posterior probabilities that each treatment is optimal.\nThey provide theoretical results to this exploration sampling design.\n31\n4.2\nApplications in Operations Research and Game\nTheory\nProbably more interesting is the case where there are multiple strategic\nagents, interacting (see Zhang et al. (2019) for a nice survey). But before, let\nus mention the use of reinforcement learning techniques in operation research,\nand graphs.\n4.2.1\nTraveling Salesman\nA graph (E, V ) is a collection of edges E (possibly oriented, possibly weighted)\nand vertices (or nodes) V . There are many several classical optimization\nproblems on graphs. In the traveling salesman problem, we want to ﬁnd\na subgraph (E⋆, V ) (with E⋆⊂E) which forms a cycle of minimum total\nweight that visits each node V at least once. But one might also think of\nmax-ﬂow or max-cut problems, or optimal matching on bipartite graphs (see\nGalichon (2017) for more examples, with economic applications). In several\nproblems, we seek an optimal solution, which can be a subset V ⋆or E⋆, of\nvertices or edges. In the traveling salesman problem (TSP), given an order\nlist of nodes V ′ that deﬁnes a cycle (E⋆⊂E is the subset of edges {(V ′\ni , V ′\ni+1)}\nwith V ⊂V and V ′\ni , V ′\ni+1 ∈E for all i), the associated loss function is\nℓ(V ′) =\nX\ni∈|V ′|\n\u0000w(V ′\ni , V ′\ni+1)\n\u0001\n, with V ′\n|V ′|+1 = V1.\nMost TSP algorithms are sequential, which will make reinforcement learning\nperfectly appropriate here. For instance, the 2-opt algorithm (developed in\nFlood (1956) and Croes (1958)) suggests to iteratively remove two edges and\nreplace these with two diﬀerent edges that reconnect the fragments created\nby edge removal into a shorter tour (or that increases the tour least),\n(i⋆, j⋆) = argmin\ni,j=1,...,|V ′|\nn\nℓ(V ′) −ℓ(˜V (ij))\no\n, where ˜V (ij)\nk\n=\n\n\n\nV ′\nj if k = i\nV ′\ni if k = j\nV ′\nk otherwise\nOther popular techniques are for instance Christophides algorithm (devel-\noped in Christoﬁdes (1976)) or some evolutionary model inspired by ant\ncolonies (as developed in Dorigo and Gambardella (1996)). Here also, it can\nbe interesting to explore possibly non-optimal moves on a short term ba-\nsis (in the sense that locally they end-up in a longer route) Such sequential\n32\ntechniques can be formulated using the framework of reinforcement learn-\ning. The states S are subsets of edges E in the context of TSP that form\na cycle. In the 2-opt algorithm, actions A are nodes that will be permuted.\nRewards are related to changes in the loss function (and the non-discounted\nsum of rewards is considered here). The nearest neighbour algorithm (which\nis a greedy algorithm) or cheapest insertion (as deﬁned in Rosenkrantz et al.\n(1974)) can also be seen with a reinforcement learning algorithm. States\nS are subsets of edges E that form partial cycles, and the action A means\ngrowing the route with one node, by inserting it optimally. The rewards is\nrelated to the change in the tour length. That idea was developed in Gam-\nbardella and Dorigo (1995) recently, or Dai et al. (2017) for a recent survey of\nreinforcement learning techniques in the context of optimization over graphs.\nDeudon et al. (2018) provides insights on how eﬃcient machine learning\nalgorithms could be adapted to solve combinatorial optimization problems\nin conjunction with existing heuristic procedures. In Bello et al. (2016) the\nheuristic procedure is replaced by some neural networks. Despite the com-\nputational expense, an eﬃcient algorithm is obtained.\n4.2.2\nStochastic Games and Equilibrium\nConsider n players, each of them taking actions ai ∈Ai and receives a reward\nri. Let a = (a1, . . . , an) ∈A and r = (r1, . . . , rn). Note that ri is deﬁned\non S × A.\nWhen S is a singleton (and there is no uncertainty), it is a\nsimple repeated game (or matrix game). A policy πi maps S into Ai. Let\nπ = (π1, . . . , πn), and π−i the collection of all component policies. Thus,\nπ = (πi, π−i) means that player i uses policy πi while competitors follow\nπ−i.\nMaskin and Tirole (1988a) introduced the concept of Markov perfect equi-\nlibrium, which is a set of Markovian policies π = which simultaneously forms\na Nash equilibrium, as discussed in details in Horst (2005) or Escobar (2013).\nThe existence results of such equilibrium are usually performed in two step:\nﬁrst, we should prove that given any policies chosen by opponents, π−i, there\nis a unique solution V ⋆\ni (s); and then we prove that the static game has a Nash\nequilibrium for any state s. For the ﬁrst step, the set of best response for\nplayer i is Πi(π−i) such that π⋆\ni ∈Πi(π−i) if and only if for any πi and s ∈S,\nV\n(π⋆\ni ,π−i)\ni\n(s) ≥V (πi,π−i)\ni\n(s). And a Nash equilibrium is a collection of policies\nπ = (π1, . . . , πn) such that for each player i, πi ∈Πi(π−i). And therefore, no\nplayer can do better when changing policies, when other players continue to\n33\nuse their own strategies.\nLittman (1994) used Q-learning algorithms for zero-sum stochastic games,\nwith two players. More precisely,\nV1(s) = max\nπ\n(\nmin\na2∈A2\n( X\na1∈A1\nπ(s, a1)Q1(s, a)\n))\n= −V2(s).\nErev and Roth (1998) proved that in many games, a one-parameter rein-\nforcement learning model robustly outperforms the equilibrium predictions.\nPredictive power is improved by adding a forgetting property and valuing\nexperimentation, with strong connections with rationality concepts. In the\ncontext of games, Franke (2003) applies the approach of reinforcement learn-\ning to Arthur (1994)’s El Farol problem, where repeatedly a population of\nagents decides to go to a bar or stay home, and going is enjoyable if, and\nonly if, the bar is not crowded.\nThe main diﬃculty arising when several agents are learning simultane-\nously in a game is that, for each player, the strategy of all the other players\nbecomes part of the environment. Hence the environment dynamics do not\nremain stationary as the other players are learning as they play. In such\ncontext, classical single agent based reinforcement learning algorithms may\nnot converge to a targeted Nash equilibrium, and typically cycles in between\nseveral of them, see Hart and Mas-Colell (2003). As observed by Erev and\nRoth (1998) or in a more general setting by Perolat et al. (2018), stabilizing\nprocedures such as ﬁctitious play (Robinson (1951)) allows to reach Nash\nequilibria in some (but not all, Shapley (1964)) multi Agent learning setting.\nElie et al. (2020) observed that such property also extends to the asymptotic\nmean ﬁeld game setting introduced by Huang et al. (2006) and Lasry and\nLions (2006a,b), where the size of the population is inﬁnite and shares mean\nﬁeld interaction.\nMulti-Agent reinforcement learning algorithms still lack\nscalability when the number of agents becomes large, a weakness that mean\nﬁeld games asymptotic properties may hopefully allow to partially overcome.\n4.2.3\nAuctions and real-time bidding\nThe majority of online display ads are served through real-time bidding. To\nplace an ad automatically, and optimally, it is critical for advertisers to have\na learning algorithm that cleverly bids. Schwind (2007) did show that seeing\nthe bid decision process as a reinforcement learning problem, where the state\n34\nspace is represented by the auction information and the campaign’s real-time\nparameters, while an action is the bid price to set, was very promising. More\nrecently, Even Dar et al. (2009), Zhang et al. (2014), Cai et al. (2017) or\nZhao et al. (2018) use reinforcement learning algorithms to design a bidding\nstrategy.\nAs pointed out by recent articles, the scalability problem from the large\nreal-world auction volume, and campaign budget, is well handled by state\nvalue approximation using neural networks. Dtting et al. (2017) and Feng\net al. (2018) suggested to use deep reinforcement learning (with deep neu-\nral networks) for the automated design of optimal auctions.\nEven if the\noptimal mechanism is unknown, they obtain very eﬃcient algorithm, that\noutperforms more classical ones.\n4.2.4\nOligopoly and dynamic games\nAs in the monopolistic case, the proﬁt of ﬁrm i will depend on its investing\nstrategies ai,t, the capital of ﬁrm i as well as competitors. Models of oligopoly\nwith investment and ﬁrm entry and exit have been studied in Ericson and\nPakes (1995). And in that framework, multiple equilibira are commonly ob-\nserved, as proved in Doraszelski and Satterthwaite (2010). The concept of\nexperience-based equilibrium was introduced in Fershtman and Pakes (2012),\nwith possibly asymmetric information. Hence, ﬁrms use past payoﬀs to re-\ninforce the probability of choosing an action.\nIn that framework, agents\nexplicitly construct beliefs, which is no longer necessary with reinforcement\nlearning.\nWith adaptative learning, Marcet and Sargent (1989a,b) proved that\nthere was convergence to a rational expectations equilibrium. The reinforce-\nment learning model is here similar to the previous one, there are no as-\nsumption about belief of opponents’ strategies. Somehow, those algorithms\nare more related to evolutionary games. Brown (1951) suggested that ﬁrms\ncould form beliefs about competitors’ choice probabilities, using some ﬁcti-\ntious plays, also called Cournot learnning (studied more deeply in Hopkins\n(2002)). Bernheim (1984) and Pearce (1984) added assumptions on ﬁrms\nbeliefs, called rationalizability, under which we can end-up with Nash equi-\nlibria.\nMaskin and Tirole (1988a,b) considered the case where two ﬁrms compete\nin a Stackelberg competition: they alternate in moving, and then commit to\na price for two periods, before (possibly) adjusting. They did observe cycles\n35\nand tacit collusion within the two ﬁrms. Such a result was conﬁrmed by\nKimbrough and Murphy (2008) and Waltman and Kaymak (2008). The later\nstudied repeated Cournot games where all players act simultaneously. They\nstudy the use of Q-learning for modeling the learning behavior of ﬁrms in\nthat repeated Cournot oligopoly games, and they show that Q-learning ﬁrms\ngenerally learn to collude with each other, although full collusion usually does\nnot emerge. Such a behavior was also observed in Schwalbe (2019) where\nself-learning price-setting algorithms can coordinate their pricing behavior\nto achieve a collusive outcome that maximizes the joint proﬁts of the ﬁrms\nusing them.\n4.3\nApplications in Finance\nThe dynamic control or hedge of risks on ﬁnancial markets is a natural play-\nground for the use of reinforcement learning algorithms. In the literature,\ndynamic risk management problems have been extensively studied in model-\ndriven settings, using the tools from dynamic programming either in contin-\nuous or discrete time. In such framework, reinforcement learning algorithms\nnaturally opens the door to innovative model-free numerical approximation\nschemes for hedging strategies, as soon as a realistic ﬁnancial market simula-\ntor is available. Such simulator may typically incorporate market imperfec-\ntions and frictions (transaction costs, market impact, liquidity issues...). In\nthe following sections, we detail more speciﬁcally recent applications on three\ntopics of interest in such context: pricing and hedging of ﬁnancial derivatives,\noptimal asset allocation and market impact modeling.\n4.3.1\nRisk management\nThe valuation and hedging of ﬁnancial derivatives are usually tackled in the\nquantitative ﬁnance literature using model-driven decision rules in a stochas-\ntic environment. Namely, for given model dynamics of the assets on a ﬁ-\nnancial market, pricing and hedging of a derivative boils down to solving a\ndynamic optimal control problem for a well chosen arbitrage free martingale\nmeasure.\nThe practical hedging strategy then makes use of the so-called\nGreeks, the sensitivities of the risk valuation to the diﬀerent parameters of\nthe model.\nSuch analysis usually lacks eﬃcient numerical approximation methods in\nhigh dimensional settings, as well as precise tractable analytical solutions\n36\nin the presence of realistic market frictions or imperfections. In the spirit\nof Weinan et al. (2017) , Buehler et al. (2019) introduced the idea of us-\ning reinforcement learning based algorithm in such context, see also Fcamp\net al. (2019). Let consider given a realistic simulator of the ﬁnancial market\npossible trajectories. We can encompass the price and/or hedging strategy\nof the ﬁnancial derivative in a neural deep network (or any other approxi-\nmating class of function), and train/estimate the approximating function in\na dynamic way. At each iteration, we measure the empirical performance\n(i.e.\nloss) of the hedging strategy obtained on a large number of Monte\nCarlo simulations, and update its parameters dynamically using any typical\nreinforcement learning algorithm. In particular, such approach allows to en-\ncompass scalable high dimensional risk dynamics as well as realistic market\nfrictions or hedging using a large number of ﬁnancial derivatives.\nThe design of the market simulator of course requires model-driven as-\nsumptions, such as the choice of a particular class of volatility models, as\nwell as its calibration. Nevertheless, we can mention recent attempts on the\ndesign of model free ﬁnancial market simulator based on generative methods,\nsuch as the one developed e.g. in Wiese et al. (2019a,b).\n4.3.2\nPortfolio allocation\nIn a similar manner, the design of dynamic optimal investment strategy nat-\nurally falls into the scope of reinforcement learning type algorithms. Such\nobservation goes back to Moody and Saﬀell (2001) and has developed a grow-\ning interest in the recent literature Deng et al. (2016); Almahdi and Yang\n(2017): Classical Mean-variance trade-oﬀin a continuous time setting is for\nexample revisited in Wang and Zhou (2019) using such viewpoint. Being\ngiven a ﬁnancial market simulator together with choices of return and risk\nmeasurement methods written in terms of running or terminal rewards, one\ncan learn optimal investment strategies using typical reinforcement learning\nalgorithms.\nOne could argue that such algorithms for portfolio allocation may often\nbe reduced to less sophisticate online or bandit type learning algorithms Li\nand Hoi (2014).\nSuch argumentation does not remain valid in the more\nrealistic cases where the investor has a signiﬁcant impact on the ﬁnancial\nassets dynamics, as discussed in the next section.\n37\n4.3.3\nMarket microstructure\nWhen trades occur at a very high frequency or concern a large volume of\nshares, buying and selling orders have an impact on the ﬁnancial market\nevolution, that one can not neglect. It modiﬁes the shape of the order book,\ncontaining the list of waiting orders chosen by the other traders of the market.\nBeing given a realistic order book dynamics simulator (or using the ﬁnancial\nmarket as such), one can optimize using Reinforcement Learning algorithms\nthe dynamic use of market and limit orders, see Spooner et al. (2018); Gu´eant\nand Manziuk (2020); Baldacci et al. (2019). The environment is given by the\ncurrent order book shapes while the state typically represents the inventory\nof the trader, on a possibly high-dimensional ﬁnancial market.\nSuch framework is with no doubt a perfect ﬁt for reinforcement learn-\ning algorithms. Nevertheless, a ﬁner modeling perspective should take into\naccount that the order book dynamics result from the aggregation of other\ntraders actions, i.e. buy or sell orders. Hence, as observed e.g. in Ganesh\net al. (2019); Vyetrenko and Xu (2019), such setting is more precisely de-\nscribed as a multi-agent learning problem, as the one described above in\nSection 4.2.2.\nThe practical use of reinforcement based learning algorithms on ﬁnancial\nmarkets suﬀers two main drawbacks. The ﬁrst one is the diﬃculty to create\na realistic ﬁnancial market simulator, together with the necessity to create a\nrobust optimal trading strategy, in response to the diﬀerences between the\nreal market and the virtual one. The second and main one is the lack of\nstationarity of the ﬁnancial dynamics, which hereby do not allow to apply\neﬃciently on future market dynamics, the investment strategies learned on\nthe past market data points. Besides, the aggregate use of model-free ap-\nproaches combined with hardly interpretable black box output policy shall\ninevitably lead to hardly controllable ﬁnancial market dynamics.\n5\nConclusion\nDeep Reinforcement learning is nowadays the most popular technique for\n(artiﬁcial) agent to learn closely optimal strategy by experience.\nMajors\ncompanies are training self driving cars using reinforcement learning (see\nFolkers et al. (2019), or Kiran et al. (2020) for a state-of-the-art). Such tech-\nniques are extremely powerful to models behaviors of animals, consumers,\n38\ninvestors, etc. Economists have laid the groundwork for this literature, but\ncomputational diﬃculties slowed them down. Recent advances in computa-\ntional science are extremely promising, and complex economic or ﬁnancial\nproblems would beneﬁt from being reviewed in the light of these new results.\nNevertheless, algorithms perform well assuming that a lot of information\nis available. More importantly, as the exploration may represent a very large\nnumber of possibilities, the use of deep reinforcement learning algorithms\nrapidly requires very important computer power. In ﬁnance, despite the lack\nof stationary of the market, it is worth noting that these algorithms begin to\nbe quite popular.\nReferences\nP. Abbeel and A. Ng.\nApprenticeship learning via inverse reinforcement\nlearning. In Proceedings of the Twenty-First International Conference in\nMachine Learning (ICML 2004), 2004.\nD. Abel. Concepts in Bounded Rationality: Perspectives from Reinforcement\nLearning. PhD thesis, Brown University, 2019.\nV. Aguirregabiria and P. Mira. Swapping the nested ﬁxed point algorithm:\nA class of estimators for discrete markov decision models. Econometrica,\n70(4):1519–1543, 2002.\nV. Aguirregabiria and P. Mira. Dynamic discrete choice structural models:\nA survey. Journal of Econometrics, 156(1):38 – 67, 2010.\nS. Almahdi and S. Y. Yang. An adaptive portfolio trading system: A risk-\nreturn portfolio optimization using recurrent reinforcement learning with\nexpected maximum drawdown. Expert Systems with Applications, 87:267–\n279, 2017.\nW. B. Arthur. Designing economic agents that act like human agents: A\nbehavioral approach to bounded rationality. The American Economic Re-\nview, 81(2):353–359, 1991.\nW. B. Arthur. Inductive reasoning and bounded rationality. The American\nEconomic Review, 84(2):406–411, 1994.\n39\nS. Athey and G. W. Imbens. The econometrics of randomized experiments.\nArXiv e-prints, 2016.\nS. Athey and G. W. Imbens. Machine learning methods that economists\nshould know about. Annual Review of Economics, 11(1):685–725, 2019.\nR. J. Aumann. Rationality and bounded rationality. Games and Economic\nBehavior, 21(1):2 – 14, 1997.\nM. Bain and C. Sammut. A framework for behavioural cloning. In Machine\nIntelligence 15, 1995.\nB. Baldacci, I. Manziuk, T. Mastrolia, and M. Rosenbaum. Market making\nand incentives design in the presence of a dark pool: a deep reinforcement\nlearning approach. arXiv preprint arXiv:1912.01129, 2019.\nA. G. Barto and S. P. Singh. On the computational economics of reinforce-\nment learning. In D. S. Touretzky, J. L. Elman, T. J. Sejnowski, and G. E.\nHinton, editors, Connectionist Models, pages 35 – 44. Morgan Kaufmann,\n1991.\nE. Basci. Learning by imitation. Journal of Economic Dynamics and Control,\n23(9):1569 – 1585, 1999.\nR. Bellman. Dynamic Programming. Princeton University Press, Princeton,\nNJ, 1957.\nI. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio.\nNeural\ncombinatorial optimization with reinforcement learning.\narXiv preprint\narXiv:1611.09940, 2016.\nD. Bergemann and U. Hege. Venture capital ﬁnancing, moral hazard, and\nlearning. Journal of Banking & Finance, 22(6):703 – 735, 1998.\nD. Bergemann and U. Hege.\nThe ﬁnancing of innovation: Learning and\nstopping. The RAND Journal of Economics, 36(4):719–752, 2005.\nD. Bergemann and J. Vlimki. Learning and strategic pricing. Econometrica,\n64(5):1125–1149, 1996.\nB. D. Bernheim. Rationalizable strategic behavior. Econometrica, 52(4):\n1007–1028, 1984.\n40\nD. A. Berry and B. Fristedt. Bandits Problems Sequential Allocation of Ex-\nperiments. — (Monographs on statistics and applied probability). Chapman\nand Hall, 1985.\nD. P. Bertsekas and J. Tsitsiklis.\nNeuro-Dynamic Programming. Athena\nScientiﬁc, 1996.\nL. Bottou. Online algorithms and stochastic approximations. In D. Saad,\neditor, Online Learning and Neural Networks. 1998.\nG. W. Brown. Iterative solutions of games by ﬁctitious play. In T. Koopmans,\neditor, Activity Analysis of Production and Allocation, pages 374–376. John\nWiley & Sons, Inc., 1951.\nH. Buehler, L. Gonon, J. Teichmann, and B. Wood. Deep hedging. Quanti-\ntative Finance, 19(8):1271–1291, 2019.\nH. Cai, K. Ren, W. Zhang, K. Malialis, J. Wang, Y. Yu, and D. Guo. Real-\ntime bidding by reinforcement learning in display advertising. In Proceed-\nings of the Tenth ACM International Conference on Web Search and Data\nMining, WSDM 17, pages 661–670, New York, NY, USA, 2017. Association\nfor Computing Machinery.\nA. Charpentier, E. Flachaire, and A. Ly. Econometrics and machine learning.\nEconomics and Statistics, (505-506), 2018.\nR. Chattopadhyay and E. Duﬂo. Women as policy makers: Evidence from\na randomized policy experiment in india. Econometrica, 72(5):1409–1443,\n2004.\nC. Cherniak. Minimal Rationality. MIT Press, MIT Press, 1986.\nN. Christoﬁdes.\nWorst-case analysis of a new heuristic for the travelling\nsalesman problem. Technical report, Graduate School of Industrial Ad-\nministration, CMU, 1976.\nG. A. Croes. A method for solving traveling-salesman problems. Operations\nresearch, 6(6):791–812, 1958.\nR. M. Cyert and M. H. DeGroot. Rational expectations and bayesian anal-\nysis. Journal of Political Economy, 82(3):521–536, 1974.\n41\nH. Dai, E. B. Khalil, Y. Zhang, B. Dilkina, and L. Song. Learning combinato-\nrial optimization algorithms over graphs. arXiv preprint arXiv:1704.01665,\n2017.\nY. Deng, F. Bao, Y. Kong, Z. Ren, and Q. Dai. Deep direct reinforcement\nlearning for ﬁnancial signal representation and trading. IEEE transactions\non neural networks and learning systems, 28(3):653–664, 2016.\nM. Deudon, P. Cournut, A. Lacoste, Y. Adulyasak, and L.-M. Rousseau.\nLearning heuristics for the tsp by policy gradient. In W.-J. van Hoeve,\neditor, Integration of Constraint Programming, Artiﬁcial Intelligence, and\nOperations Research, pages 170–181, Cham, 2018. Springer International\nPublishing.\nU. Doraszelski and M. Satterthwaite. Computable markov-perfect industry\ndynamics. The RAND Journal of Economics, 41(2):215–243, 2010.\nM. Dorigo and L. M. Gambardella. Ant colonies for the traveling salesman\nproblem. Istituto Dalle Molle di Studi sull’Intelligenza Artiﬁciale, 3, 1996.\nP. Dtting, Z. Feng, H. Narasimhan, D. C. Parkes, and S. S. Ravindranath.\nOptimal auctions through deep learning, 2017.\nR. Elie, J. Perolat, M. Laurire, M. Geist, and O. Pietquin. On the conver-\ngence of model free learning in mean ﬁeld games. In AAAI Conference one\nArtiﬁcial Intelligence (AAAI 2020), 2020.\nI. Erev and A. E. Roth. Predicting how people play games: Reinforcement\nlearning in experimental games with unique, mixed strategy equilibria.\nAmerican Economic Review, 88(4):848–881, 1998.\nR. Ericson and A. Pakes. Markov-perfect industry dynamics: A framework\nfor empirical work. The Review of Economic Studies, 62(1):53–82, 1995.\nJ. F. Escobar. Equilibrium analysis of dynamic models of imperfect compe-\ntition. International Journal of Industrial Organization, 31(1):92 – 101,\n2013.\nE. Even Dar, V. S. Mirrokni, S. Muthukrishnan, Y. Mansour, and U. Nadav.\nBid optimization for broad match ad auctions. In Proceedings of the 18th\nInternational Conference on World Wide Web, WWW 09, pages 231–240,\nNew York, NY, USA, 2009. Association for Computing Machinery.\n42\nM. Feldman. Bayesian learning and convergence to rational expectations.\nJournal of Mathematical Economics, 16(3):297 – 313, 1987.\nZ. Feng, H. Narasimhan, and D. C. Parkes. Deep learning for revenue-optimal\nauctions with budgets. In Proceedings of the 17th International Conference\non Autonomous Agents and MultiAgent Systems, AAMAS 18, pages 354–\n362, Richland, SC, 2018. International Foundation for Autonomous Agents\nand Multiagent Systems.\nC. Fershtman and A. Pakes. Dynamic Games with Asymmetric Information:\nA Framework for Empirical Work*. The Quarterly Journal of Economics,\n127(4):1611–1661, 11 2012.\nM. M. Flood. The Travelling Salesman Problem. Operations Research, 4:\n61–75, 1956.\nA. Folkers, M. Rick, and C. Buskens. Controlling an autonomous vehicle with\ndeep reinforcement learning. 2019 IEEE Intelligent Vehicles Symposium\n(IV), Jun 2019. doi: 10.1109/ivs.2019.8814124. URL http://dx.doi.\norg/10.1109/IVS.2019.8814124.\nR. Franke. Reinforcement learning in the el farol model. Journal of Economic\nBehavior & Organization, 51(3):367 – 388, 2003.\nD. Fudenberg and D. Levine.\nThe Theory of Learning in Games.\nMas-\nsachusetts Institute of Technology (MIT) Press, 1998.\nS. Fcamp, J. Mikael, and X. Warin. Risk management with machine-learning-\nbased algorithms. arXiv preprint arXiv:1902.05287, 2019.\nX. Gabaix. A Sparsity-Based model of bounded rationality. The Quarterly\nJournal of Economics, 129(4):1661–1710, 2014.\nA. Galichon. Optimal Transport Methods in Economics. Princeton University\nPress, 2017.\nL. M. Gambardella and M. Dorigo. Ant-Q: A reinforcement learning approach\nto the traveling salesman problem. In A. Prieditis and S. Russell, editors,\nMachine Learning Proceedings 1995, pages 252–260. Morgan Kaufmann,\n1995.\n43\nS. Ganesh, N. Vadori, M. Xu, H. Zheng, P. Reddy, and M. Veloso. Reinforce-\nment learning for market making in a multi-agent dealer market. arXiv\npreprint arXiv:1911.05892, 2019.\nJ. Garcia. The nature of learning explanations. Behavioral and Brain Sci-\nences, 4(1):143144, 1981.\nN. Gennaioli and A. Shleifer. What Comes to Mind*. The Quarterly Journal\nof Economics, 125(4):1399–1433, 11 2010.\nS. J. Gershman, E. J. Horvitz, and J. B. Tenenbaum. Computational ra-\ntionality: A converging paradigm for intelligence in brains, minds, and\nmachines. Science, 349(6245):273–278, 2015.\nG. Gigerenzer and D. Goldstein. Reasoning the fast and frugal way: models\nof bounded rationality. Psychological review, 103(4):650, 1996.\nJ. Gittins. Bandit Processes and Dynamic Allocation Indices. John Wiley,\n1989.\nI. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.\nhttp://www.deeplearningbook.org.\nO. Gu´eant and I. Manziuk. Deep reinforcement learning for market making\nin corporate bonds: beating the curse of dimensionality. Applied Mathe-\nmatical Finance, pages 1–66, 2020.\nL. P. Hansen and T. J. Sargent.\nRecursive Models of Dynamic Linear\nEconomies.\nThe Gorman Lectures in Economics. Princeton University\nPress, 2013.\nS. Hart and A. Mas-Colell. Uncoupled dynamics do not lead to nash equi-\nlibrium. American Economic Review, 93(5):1830–1836, 2003.\nH. V. Hasselt.\nDouble q-learning.\nIn J. D. Laﬀerty, C. K. I. Williams,\nJ. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural\nInformation Processing Systems 23, pages 2613–2621. Curran Associates,\nInc., 2010.\nM. F. Hellwig. Sequential models in economic dynamics. PhD thesis, Mas-\nsachusetts Institute of Technology, Department of Economics, 1973.\n44\nJ. H. Holland.\nEscaping brittleness: The possibilities of general-purpose\nlearning algorithms applied to parallel rule-based systems. In R. S. Michal-\nski, J. G. Carbonell, and T. M. Mitchell, editors, Machine Learning: An\nArtiﬁcial Intelligence Approach, volume 2. Morgan Kaufmann, Los Altos,\nCA, 1986.\nE. Hopkins. Two competing models of how people learn in games. Econo-\nmetrica, 70(6):2141–2166, 2002.\nU. Horst. Stationary equilibria in discounted stochastic games with weakly\ninteracting players. Games and Economic Behavior, 51(1):83 – 108, 2005.\nV. J. Hotz and R. A. Miller. Conditional choice probabilities and the estima-\ntion of dynamic models. The Review of Economic Studies, 60(3):497–529,\n1993.\nR. A. Howard. Dynamic Programming and Markov Processes. MIT Press,\nCambridge, Massachusetts, 1960.\nM. Huang, R. P. Malham, and P. E. Caines. Large population stochastic\ndynamic games: closed-loop mckean-vlasov systems and the nash certainty\nequivalence principle. Communications in Information & Systems, 6(3):\n221–252, 2006.\nN. Hughes. Applying reinforcement learning to economic problems. Technical\nreport, Australian National University, 2014.\nM. Igami.\nArtiﬁcial intelligence as structural estimation:\nEconomic in-\nterpretations of deep blue, bonanza, and alphago.\narXiv preprint\narXiv:1710.10967, 2017.\nK. Ito and M. Reguant. Sequential markets, market power, and arbitrage.\nAmerican Economic Review, 106(7):1921–57, July 2016. doi: 10.1257/aer.\n20141529.\nH. M. Jenkins. Animal learning and behavior theory. In E. Hearst, editor,\nThe ﬁrst century of experimental psychology, pages 177–228. 1979.\nB. Jovanovic. Selection and the evolution of industry. Econometrica, 50(3):\n649–670, 1982.\n45\nD. Kahneman. Thinking, fast and slow. Macmillan, 2011.\nM. Kasy and A. Sautmann. Adaptive treatment assignment in experiments\nfor policy choice. Technical report, Harvard University, 2019.\nG. Keller and S. Rady. Optimal experimentation in a changing environment.\nThe Review of Economic Studies, 66(3):475–507, 1999.\nS. O. Kimbrough and F. H. Murphy. Learning to collude tacitly on production\nlevels by oligopolistic agents.\nComputational Economics, 33(1):47, Jul\n2008.\nB. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yoga-\nmani, and P. Prez. Deep reinforcement learning for autonomous driving:\nA survey. arXiv preprint arXiv:2002.00444, 2020.\nN. Kiyotaki and R. Wright. On money as a medium of exchange. Journal of\nPolitical Economy, 97(4):927–954, 1989.\nE. Klein, M. Geist, B. Piot, and O. Pietquin. Inverse reinforcement learn-\ning through structured classiﬁcation. In Advances in Neural Information\nProcessing Systems, pages 1007–1015, 2012.\nM. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Ma-\nchine Learning Research, 4:1107–1149, 2003.\nJ.-M. Lasry and P.-L. Lions. Jeux\nchamp moyen. i\nle cas stationnaire.\nComptes Rendus Mathematique, 343(9):619 – 625, 2006a.\nJ.-M. Lasry and P.-L. Lions. Jeux champ moyen. ii horizon ﬁni et contrle\noptimal. Comptes Rendus Mathematique, 343(10):679 – 684, 2006b.\nO. Leimar and J. McNamara. Learning leads to bounded rationality and\nthe evolution of cognitive bias in public goods games. Nature Scientiﬁc\nReports, 9:16319, 2019.\nM. Lettau and H. Uhlig.\nRules of thumb versus dynamic programming.\nAmerican Economic Review, 89(1):148–174, March 1999.\nT. Levina, Y. Levin, J. McGill, and M. Nediak. Dynamic pricing with on-\nline learning and strategic consumers: An application of the aggregating\nalgorithm. Operations Research, 57(2):327–341, 2009.\n46\nB. Li and S. C. Hoi. Online portfolio selection: A survey. ACM Computing\nSurveys (CSUR), 46(3):1–36, 2014.\nM. L. Littman. Markov games as a framework for multi-agent reinforcement\nlearning. In Machine Learning Proceedings 1994, pages 157–163. Elsevier,\n1994.\nL. Ljungqvist and T. J. Sargent. Recursive Macroeconomic Theory. MIT\nPress, 4 edition, 2018.\nT. Magnac and D. Thesmar. Identifying dynamic discrete decision processes.\nEconometrica, 70(2):801–816, 2002.\nA. Marcet and T. J. Sargent. Convergence of least-squares learning in envi-\nronments with hidden state variables and private information. Journal of\nPolitical Economy, 97(6):1306–1322, 1989a.\nA. Marcet and T. J. Sargent. Convergence of least squares learning mech-\nanisms in self-referential linear stochastic models. Journal of Economic\nTheory, 48(2):337 – 368, 1989b.\nE. Maskin and J. Tirole. A theory of dynamic oligopoly, I: Overview and\nquantity competition with large ﬁxed costs. Econometrica, 56:549–569,\n1988a.\nE. Maskin and J. Tirole. A theory of dynamic oligopoly, II: Price competition,\nkinked demand curves, and edgeworth cycles. Econometrica, 56:571–579,\n1988b.\nA. McLennan.\nPrice dispersion and incomplete learning in the long run.\nJournal of Economic Dynamics and Control, 7(3):331 – 347, 1984.\nR. A. Miller. Job matching and occupational choice. Journal of Political\nEconomy, 92(6):1086–1120, 1984.\nM. Minsky. Steps toward artiﬁcial intelligence. Transactions on Institute of\nRadio Engineers, 49:8–30, 1961.\nK. Misra, E. M. Schwartz, and J. Abernethy. Dynamic online pricing with\nincomplete information using multiarmed bandit experiments. Marketing\nScience, 38(2):226–252, 2019.\n47\nJ. Moody and M. Saﬀell. Learning to trade via direct reinforcement. IEEE\nTransactions on Neural Networks, 12(4):875–889, July 2001.\nS. Mullainathan and J. Spiess. Machine learning: An applied econometric\napproach. Journal of Economic Perspectives, 31(2):87–106, May 2017.\nA. Nedi´c and D. P. Bertsekas. Least squares policy evaluation algorithms\nwith linear function approximation. Discrete Event Dynamic Systems, 13:\n79–110, 2003.\nA. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning. In\nProceedings of the International Conference on Machine Learning (ICML),\npages 663–670, 2000.\nD. O’Neill, M. Levorato, A. Goldsmith, and U. Mitra. Residential demand\nresponse using reinforcement learning. In 2010 First IEEE International\nConference on Smart Grid Communications, pages 409–414, Oct 2010.\nA. Pakes. Patents as options: Some estimates of the value of holding european\npatent stocks. Econometrica, 54(4):755–784, 1986.\nA. Pakes and M. Schankerman. The Rate of Obsolescence of Patents, Re-\nsearch Gestation Lags, and the Private Rate of Return to Research Re-\nsources, pages 73–88. University of Chicago Press, 1984.\nD. G. Pearce. Rationalizable strategic behavior and the problem of perfec-\ntion. Econometrica, 52(4):1029–1050, 1984.\nJ. Pearl. The seven tools of causal inference, with reﬂections on machine\nlearning. Commununications of the ACM, 62(3):54–60, Feb. 2019.\nJ. Perolat, B. Piot, and O. Pietquin. Actor-critic ﬁctitious play in simulta-\nneous move multistage games. In International Conference on Artiﬁcial\nIntelligence and Statistics, pages 919–928, 2018.\nR. A. Rescorla. Aspects of the reinforcer learned in second-order Pavlovian\nconditioning. Journal of Experimental Psychology: Animal Behavior Pro-\ncesses, 5(1):79–95, 1979.\nH. Robbins. Some aspects of the sequential design of experiments. Bulletin\nof the American Mathematical Society, 58(5):527–535, 1952.\n48\nJ. Robinson. An iterative method of solving a game. Annals of mathematics,\npages 296–301, 1951.\nD. J. Rosenkrantz, R. E. Stearns, and P. M. Lewis.\nApproximate algo-\nrithms for the traveling salesperson problem. In 15th Annual Symposium\non Switching and Automata Theory (swat 1974), pages 33–42, Oct 1974.\nC. A. Rothkopf and C. Dimitrakakis. Preference elicitation and inverse re-\ninforcement learning. In Machine Learning and Knowledge Discovery in\nDatabases, pages 34–48. Springer, 2011.\nM. Rothschild. A two-armed bandit theory of market pricing. Journal of\nEconomic Theory, 9(2):185 – 202, 1974.\nA. Rubinstein. Modeling Bounded Rationality. MIT Press, 1998.\nS. J. Russell and P. Norvig. Artiﬁcial Intelligence: A Modern Approach, 3rd\nEdition. Prentice Hall, 2009.\nS. J. Russell and D. Subramanian. Provably bounded-optimal agents. J.\nArtif. Int. Res., 2(1):575609, Jan. 1995.\nJ. Rust. Optimal replacement of gmc bus engines: An empirical model of\nharold zurcher. Econometrica, 55(5):999–1033, 1987.\nL. Samuelson.\nEvolutionary games and equilibrium selection.\nMIT Press\nCambridge, Mass, 1997.\nT. Sargent.\nBounded Rationality in Macroeconomics.\nOxford University\nPress, 1993.\nS. Schaal. Learning from demonstration. In Proceedings of the 9th Inter-\nnational Conference on Neural Information Processing Systems, NIPS96,\npage 10401046, Cambridge, MA, USA, 1996. MIT Press.\nU. Schwalbe.\nAlgorithms, Machine Learning, and Collusion.\nJournal of\nCompetition Law & Economics, 14(4):568–607, 06 2019.\nM. Schwind. Dynamic Pricing and Automated Resource Allocation for Com-\nplex Information Services:\nReinforcement Learning and Combinatorial\nAuctions. Springer-Verlag, 2007.\n49\nV. Semenova. Machine learning for dynamic discrete choice. arXiv preprint\narXiv:1808.02569, 2018.\nL. Shapley. Some topics in two-person games. Advances in game theory, 52:\n1–29, 1964.\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez,\nM. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. Lillicrap, K. Simonyan,\nand D. Hassabis. A general reinforcement learning algorithm that masters\nchess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018.\nH. A. Simon. Theories of bounded rationality. Decision and organization, 1\n(1):161–176, 1972.\nB. F. Skinner. The behavior of organisms: An experimental analysis. New\nYork: Appleton-Century-Crofts, 1938.\nT. Spooner, J. Fearnley, R. Savani, and A. Koukorinis. Market making via re-\ninforcement learning. In Proceedings of the 17th International Conference\non Autonomous Agents and MultiAgent Systems, pages 434–442. Interna-\ntional Foundation for Autonomous Agents and Multiagent Systems, 2018.\nN. L. Stokey, R. E. Lucas, and E. C. Prescott. Recursive Methods in Eco-\nnomic Dynamics. Harvard University Press, 1989.\nC.-L. Su and K. L. Judd. Constrained optimization approaches to estimation\nof structural models. Econometrica, 80(5):2213–2230, 2012.\nR. S. Sutton and A. G. Barto. Toward a modern theory of adaptive networks:\nExpectation and prediction. Psychological Review, 88(2), 1981.\nR. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction.\nMIP Press, 1998.\nW. R. Thompson. On the likelihood that one unknown probability exceeds\nanother in view of the evidence of two samples. Biometrika, 25(3/4):285–\n294, 1933.\nE. L. Thorndike. Animal Intelligence. New York, NY: Macmillan, 1911.\nE. C. Tolman. Cognitive maps in rats and men. Psychological review, 55(4):\n189, 1948.\n50\nS. Vyetrenko and S. Xu.\nRisk-sensitive compact decision trees for au-\ntonomous execution in presence of simulated market response.\narXiv\npreprint arXiv:1906.02312, 2019.\nL. Waltman and U. Kaymak. q-learning agents in a cournot oligopoly model.\nJournal of Economic Dynamics and Control, 32(10):3275 – 3293, 2008.\nH. Wang and X. Y. Zhou. Continuous-time mean-variance portfolio optimiza-\ntion via reinforcement learning. arXiv preprint arXiv:1904.11392, 2019.\nC. J. Watkins. Learning from delayed reward. PhD thesis, Cambridge Uni-\nversity, 1989.\nC. J. C. H. Watkins and P. Dayan. q-learning. Machine Learning, 8(3):\n279–292, May 1992.\nR. Weber.\nOn the gittins index for multiarmed bandits.\nThe Annals of\nApplied Probability, 2(4):1024–1033, 11 1992.\nE. Weinan, J. Han, and A. Jentzen. Deep learning-based numerical meth-\nods for high-dimensional parabolic partial diﬀerential equations and back-\nward stochastic diﬀerential equations. Communications in Mathematics\nand Statistics, 5(4):349–380, 2017.\nM. L. Weitzman. Optimal search for the best alternative. Econometrica, 47\n(3):641–654, 1979.\nP. Whittle. Optimization Over Time, volume 1. John Wiley, Chichester, UK,\n1983.\nM. Wiese, L. Bai, B. Wood, and H. Buehler. Deep hedging: learning to\nsimulate equity option markets. Available at SSRN 3470756, 2019a.\nM. Wiese, R. Knobloch, R. Korn, and P. Kretschmer. Quant gans: deep\ngeneration of ﬁnancial time series. arXiv preprint arXiv:1907.06673, 2019b.\nK. I. Wolpin. An estimable dynamic stochastic model of fertility and child\nmortality. Journal of Political Economy, 92(5):852–874, 1984.\nK. Zhang, Z. Yang, and T. Baar. Multi-agent reinforcement learning: A\nselective overview of theories and algorithms, 2019.\n51\nW. Zhang, S. Yuan, and J. Wang. Optimal real-time bidding for display\nadvertising. In Proceedings of the 20th ACM SIGKDD International Con-\nference on Knowledge Discovery and Data Mining, KDD 14, pages 1077–\n1086, New York, NY, USA, 2014. Association for Computing Machinery.\nJ. Zhao, G. Qiu, Z. Guan, W. Zhao, and X. He. Deep reinforcement learning\nfor sponsored search real-time bidding. In Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Min-\ning, KDD 18, pages 1021–1030, New York, NY, USA, 2018. Association\nfor Computing Machinery.\n52\n",
  "categories": [
    "econ.TH",
    "cs.LG",
    "q-fin.CP"
  ],
  "published": "2020-03-22",
  "updated": "2020-03-22"
}