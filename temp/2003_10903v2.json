{
  "id": "http://arxiv.org/abs/2003.10903v2",
  "title": "Distributional Reinforcement Learning with Ensembles",
  "authors": [
    "Björn Lindenberg",
    "Jonas Nordqvist",
    "Karl-Olof Lindahl"
  ],
  "abstract": "It is well known that ensemble methods often provide enhanced performance in\nreinforcement learning. In this paper, we explore this concept further by using\ngroup-aided training within the distributional reinforcement learning paradigm.\nSpecifically, we propose an extension to categorical reinforcement learning,\nwhere distributional learning targets are implicitly based on the total\ninformation gathered by an ensemble. We empirically show that this may lead to\nmuch more robust initial learning, a stronger individual performance level, and\ngood efficiency on a per-sample basis.",
  "text": "Article\nDistributional Reinforcement Learning with Ensembles\nBjörn Lindenberg *\n, Jonas Nordqvist\nand Karl-Olof Lindahl\nDepartment of Mathematics, Linnæus University, 351 95 Växjö, Sweden; jonas.nordqvist@lnu.se (J.N.);\nkarl-olof.lindahl@lnu.se (K.-O.L.)\n* Correspondence: bjorn.lindenberg@lnu.se\nReceived: 8 April 2020; Accepted: 30 April 2020; Published: 7 May 2020\n\u0001\u0002\u0003\u0001\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\nAbstract: It is well known that ensemble methods often provide enhanced performance in reinforcement\nlearning.\nIn this paper, we explore this concept further by using group-aided training within the\ndistributional reinforcement learning paradigm. Specifically, we propose an extension to categorical\nreinforcement learning, where distributional learning targets are implicitly based on the total information\ngathered by an ensemble. We empirically show that this may lead to much more robust initial learning, a\nstronger individual performance level, and good efficiency on a per-sample basis.\nKeywords:\ndistributional reinforcement learning;\nmultiagent learning;\nensembles;\ncategorical\nreinforcement learning\n1. Introduction\nThe fact that ensemble methods may outperform single agent algorithms in reinforcement learning\nhas been demonstrated numerous times [1–4]. These methods can involve combining several algorithms\ninto one agent and then taking actions by a weighted aggregation scheme or rank voting. However, most\nconventional ensemble methods in reinforcement learning are often based on expected returns. Perhaps\nthe simplest example is the average joint policy derived from an ensemble of independently trained agents,\nwhere the action of the ensemble is dictated by the average of the estimated Q-values of each agent.\nAn alternate view to that of Q-values, the distributional perspective on state-action returns, was\ndiscussed in [5]. This paradigm represents a shift of focus towards estimating or using underlying\ndistributions of random return variables instead of learning expectations. This in turn paints a complex\nand more informationally dense picture, and there exists overwhelming empirical evidence that the\ndistributional perspective is helpful in deep reinforcement learning. That is, apart from the possibility of\noverall stronger performance, algorithmic beneﬁts may also involve the reduction of prediction variance,\nmore robust learning with additional regularization effects, and a larger set of auxiliary goals such as\nlearning risk-sensitive policies [5–9]. Moreover, there have recently been important theoretical works done\non understanding the observed improvements and providing theoretical results on convergence [5,9–11].\nIn this paper, we propose a group-aided training scheme for distributional reinforcement learning,\nwhere we merge the distributional perspective with an ensemble method involving agents learning in\nseparate environments. Our main contribution in this regard is the proposed Ensemble Categorical Control\nprocedure (ECCprocedure). As an initial study, we also provide empirical results where an ECCalgorithm\nis tested on a subset of Atari 2600 games [12], which are standard environments for testing these types of\nalgorithms.\nSpeciﬁcally, ECC is an extension of Categorical Distributional Reinforcement Learning (CDRL), which\nwas introduced in [5] and made explicit in [10]. Similar to CDRL, we consider distributions deﬁned on a\nﬁxed discrete support, with projections onto the support for all possible categorical distributions arising\narXiv:2003.10903v2  [cs.LG]  22 May 2020\n2 of 14\ninternally in the algorithm. For each agent in ECC, we replace the target generation of CDRL by targets\ngenerated by the ensemble mean mixture distribution of the individual target distributions.\nWe argue that ECC implies an implicit sharing of information between agents during learning, where\nthe distributional paradigm gives us more robust targets and an arguably more nuanced aggregated\npicture, which preserves multimodality. The experiments conﬁrm the validity of the approach, where\nin all cases, the extension generates strong individual agents and good efﬁciency when regarded as an\nensemble.\nThe paper is organized in the following way. In Section 2, we give a background to distributional\nreinforcement learning. In Section 3, we introduce the proposed ECCprocedure. At the end of Section 3,\nwe give a reference to another contribution of the present work: the pseudocode and source code for\nan implementation of the ECCalgorithm. In Section 4, we present and evaluate the results of our\nimplementation of the ECCalgorithm on ﬁve speciﬁc Atari 2600 environments. Finally, in Section 5,\nwe zoom out and discuss the results in a broader context, as well as suggest future work.\n2. Background\nWe considered agent-environment interactions. For each observed state, the agent selects an action,\nwhereby the environment generates a reward and a next state. Following the framework of [10], we let X\nand A denote the sets of states and actions, respectively, and let p: X × A →P(R × X ) be a transition\nkernel that maps state-action pairs to joint distributions of immediate rewards and next states. Then,\nwe can model this interaction by a Markov Decision Process (MDP) (X , A, p, γ), where γ ∈[0, 1) is a\ndiscount factor of future rewards. Moreover, an agent can sample its actions through a stationary policy\nπ : X →P(A), which maps a current state to a distribution over available actions.\nThroughout the rest of this paper, we consider MDPs where X × A is a countable state-action space.\nWe denote by D = P(R)X ×A the set of functions where η ∈D maps each state-action pair (x, a) to a\ndistribution η(x,a) ∈P(R). Similarly, we put Dn = Pn(R)X ×A, where Pn(R) is the set of probability\ndistributions with ﬁnite nth-moments. For a given η ∈D, we let Qη : X × A →R denote the function that\nmaps state-action pairs {(x, a)} to the corresponding ﬁrst moments of {η(x,a)}, i.e.,\nQη(x, a) :=\nZ\nR z η(x,a)(dz).\nTo appreciate a subsequent summary of distributional reinforcement theory fully, we may also need\nto make the following deﬁnition explicit.\nDeﬁnition 1. For a Borel measurable function g: R →R and ν ∈P(R), we let g#ν denote the push-forward\nmeasure deﬁned by:\ng#ν(A) := ν\n\u0010\ng−1(A)\n\u0011\non all Borel sets A ⊆R. In particular, given r, γ ∈R, we let ( fr,γ)#ν be the push-forward measure where\nfr,γ(x) := r + γx.\nSuppose further that we have a set P of categorical distributions supported on a ﬁxed set z =\n{ z1, z2, . . . , zK } of equally-spaced numbers. Then, the following projection operator minimizes the\ndistance between any categorical distribution ν = ∑n\ni=1 piδyi and elements in P with respect to the\nCramér metric [9,13].\n3 of 14\nDeﬁnition 2. The Cramér projection Πz maps any Dirac measure δy to a distribution in P by:\nΠz(δy) =\n\n\n\n\n\nδz1\ny ≤z1,\nzi+1−y\n∆z\nδzi + y−zi\n∆z δzi+1\nzi < y ≤zi+1,\nδzK\ny > zK.\nMoreover, the projection is deﬁned to be linear over mixture distributions such that:\nΠz\n \n∑\ni\npiδyi\n!\n= ∑piΠz\n\u0000δyi\n\u0001\n.\n2.1. Expected Reinforcement Learning\nBefore we go into the distributional perspective, let us ﬁrst give a quick reminder about some value\nfunction fundamentals, here stated in operator form.\nLet (X , A, p, γ) be an MDP. Given (x, a) ∈X × A, we deﬁne the return of a policy π as the random\nvariable:\nZπ(x, a) :=\n∞\n∑\nt=0\nγtRt\n\f\f\f\f\f X0 = x, A0 = a ,\n(1)\nwhere (Rt)∞\nt=0 is a random sequence of immediate rewards, indexed by time step t and dependent on\nrandom state-action pairs (Xt, At)∞\nt=0 under p and π.\nIn an evaluation setting of some ﬁxed policy π, let Qπ : X × A →R be the expected return function,\nwhich by deﬁnition has values:\nQπ(x, a) = E[Zπ(x, a)].\nIf we consider distributions dictated by p and π and let R(x, a) and (X′, A′) denote the random reward\nand subsequent random state-action pair given (x, a) ∈X × A, then we recall the Bellman operator T π\ndeﬁned by:\n∀(x, a) (T πg) (x, a) = Ep[R(x, a)] + γEp,π[g(X′, A′)]\n(2)\non bounded real functions g ∈B(X × A, R). Moreover, in the search for values attained by optimal\npolicies, we also recall the optimality operator T ∗where:\n∀(x, a) (T ∗g) (x, a) = Ep[R(x, a)] + γEp[max\na′\ng(X′, a′)].\n(3)\nIt is readily veriﬁed that both operators are contraction maps on the complete metric space\n(B(X × A, R), d∞). In addition, their unique ﬁxed points are given by Qπ and Q∗, respectively, where Q∗\nis the optimal function deﬁned by:\nQ∗(x, a) = max\nπ\nQπ(x, a)\nfor all (x, a) [14].\n2.2. Distributional Reinforcement Learning\nWe now proceed by presenting some of the main ideas of distributional reinforcement learning in a\ntabular setting. We will ﬁrst look at the evaluation problem, where we are trying to ﬁnd the state-action\nvalue of a ﬁxed policy π. Second, we consider the control problem, where we try to ﬁnd the optimal\nstate-action value. Third, we consider the distributional approximation procedure CDRL used by agents\nin this paper.\n4 of 14\n2.2.1. Evaluation\nWe consider a distributional variant of (2), the distributional Bellman operator given by Tπ : D →D,\n∀(x, a) (Tπη)(x,a) :=\nZ\nR\n∑\n(x′,a′)∈X ×A\n( fr,γ)#η(x′,a′) π(a′ | x′)p(dr, x′ | x, a).\n(4)\nHere, Tπ is, for all n ≥1, a γ-contraction in Dn with a unique ﬁxed point when Dn is endowed with\nthe supremum nth-Wasserstein metric ([5], Lemma 3) (see [15] for more details on Wasserstein distances).\nMoreover by Proposition 2 of [9], Tπ is expectation preserving when we have an initial coupling with\nthe T π-iteration given in (2); that is, given an initial η0 ∈D and a function g, such that g = Qη0. Then,\n(T π)n g = Q(Tπ)nη0 holds for all n ≥0.\nThus, if we let ηπ ∈D be the function of distributions of Zπ in (1), then ηπ is the unique ﬁxed point\nsatisfying the distributional Bellman equation:\nηπ = Tπηπ.\nIt follows that iterating Tπ on any starting collection η0 with bounded moments eventually solves the\nevaluation task of π to an arbitrary degree.\n2.2.2. Control\nRecall the Bellman optimality operator T ∗of (3).\nIf we deﬁne a corresponding distributional\noptimality operator T∗: D →D,\n∀(x, a) (T∗η)(x,a) :=\nZ\nR\n∑\n(x′,a′)∈X ×A\n( fr,γ)#η(x′,a∗(x′)) p(dr, x′ | x, a),\n(5)\nwhere a∗(x′) = arg maxa′∈A Qη(x′, a′), then expectation values generated by iterates under T∗will behave\nas expected. That is, if we put Qn := Q(T∗)nη0, then we have an exponentially fast uniform convergence\nQn →Q∗as n →∞. However, T∗is not a contraction in any metric over distributions and may lack ﬁxed\npoints altogether in D [5].\n2.2.3. Categorical Evaluation and Control\nIn most real applications, the updates of (4) and (5) are either computationally infeasible or impossible\nto fully compute due to p being unknown. It follows that approximations are key to deﬁning practical\ndistributional algorithms. This could involve parametrization over some selected set of distributions along\nwith projections onto these distributional subspaces. It could also involve stochastic approximations with\nsampled transitions and gradient updates with function approximation.\nA structure for algorithms making use of such approximations is Categorical Distributional\nReinforcement Learning (CDRL). In what follows is a short summary of the CDRL procedure fundamental\nto single agent implementations in this paper.\nLet z = { z1, z2, . . . , zK } be an ordered ﬁxed set of equally-spaced real numbers such that z1 < z2 <\n· · · < zK with ∆z := zi+1 −zi. Let:\nP =\n(\nK\n∑\ni=1\npiδzi : p1, . . . , pK ≥0,\nK\n∑\ni=1\npi = 1\n)\n⊂P(R)\n5 of 14\nbe the subset of categorical distributions in P(R) supported on z. We consider parameterized distributions\nby using bD = PA×X as the collection of possible inputs and outputs of an algorithm. Moreover, for each\nη ∈bD, we have:\nQη(x, a) =\nK\n∑\ni=1\npi(x, a)zi.\nas its Q-value function.\nGiven a subsequent treatment of our extension of CDRL, we ﬁrst reproduce the steps of the general\nprocedure in Algorithm 1 (see [10], Algorithm 1).\nAlgorithm 1: Categorical Distributional Reinforcement Learning (CDRL)\n1. At each iteration step t and input ηt ∈bD, sample a transition (xt, at, rt, x′\nt).\n2. Select a∗to be either sampled from π(xt) in the evaluation setting or taken as\na∗= arg maxa Qηt(x′\nt, a) in the control setting.\n3. Recall the Cramér projection Πz given in Deﬁnition 2, and put:\nbη(xt,at)\nt\n:= Πz ( frt)# η(x′\nt,a∗)\nt\n.\n4. Take the next iterated function as some update ηt+1 such that:\nKL\n\u0010\nbη(xt,at)\nt\n∥η(xt,at)\nt+1\n\u0011\n< KL\n\u0010\nbη(xt,at)\nt\n∥η(xt,at)\nt\n\u0011\n,\nwhere:\nKL(p ∥q) :=\nK\n∑\ni=1\npi log\n\u0012 pi\nqi\n\u0013\ndenotes the Kullback–Leibler divergence.\nConsider ﬁrst a ﬁnite MDP and a tabular setting. Deﬁne bη(x,a)\nt\n:= η(x,a)\nt\nwhenever (x, a) ̸= (xt, at).\nThen, by the convexity of −log(z), it is readily veriﬁed that updates of the form:\nηt+1 = (1 −αt)ηt + αtbηt\n(αt ∈(0, 1))\nsatisfy Step 4. In fact, if there exists a unique policy π∗associated with the convergence of (3), then this\nupdate yields an almost sure convergence, with respect to the supremum-Cramér metric, to a distribution\nin bD with π∗as the greedy policy (with some additional assumptions on the stepsizes αt and sufﬁcient\nsupport (see [10], Theorem 2, for details).\nIn practice, we are often forced to use function approximation of the form:\nη(x,a) = φ(x, a; θ),\nwhere φ is parameterized by some set of weights θ. Gradient updates with respect to θ can then be made\nto minimize the loss:\nKL\n\u0010\nbη(xt,at)\nt\n∥φ (xt, at; θ)\n\u0011\n,\n(6)\nwhere bη(xt,at)\nt\n= Πz ( frt)# φ(x′\nt, a∗; θﬁxed) is the computed learning target of the transition (xt, at, rt, x′\nt).\nHowever convergence with the Kullback–Leibler loss and function approximation is still an open question.\n6 of 14\nTheoretical progress has been made when considering other losses, although we may lose the stability\nbeneﬁts coming from the relative ease of minimizing (6) [9,11,16].\nAn algorithm implementing CDRL with function approximation is C51[5]. It essentially uses the same\nneural network architecture and training procedure as DQN[17]. To increase stability during training,\nthis also involves sampling transitions from an experience buffer and maintaining an older, periodically\nupdated, copy of the weights for target computation. However, instead of estimating Q-values, C51 uses\na ﬁnite support z of 51 points and learns discrete probability distributions φ(x, a; θ) over z via soft-max\ntransfer. Training is done by using the KL-divergence as the loss function over batches with computed\ntargets bη(x,a) of CDRL.\n3. Learning with Ensembles\n3.1. Ensembles\nEnsemble methods have been widely used in both supervised learning and reinforcement learning.\nIn supervised learning, this can involve bootstrap aggregating predictors for better accuracy when given\nunstable processes such as neural networks or using “expert” opinion mixtures for better estimators\n[18,19]. A simple example that demonstrates the possible beneﬁts of aggregation is the following average\npool of k regression models: Given a sample to predict, assume that the models draw prediction errors εi,\ni = 1, . . . , k from a zero-mean multivariate normal distribution with E[ε2\ni ] = σ2 and correlations ρij = ρ.\nThen, the error made by averaging their predictions is ε := (1/k) ∑k\ni=1 εi with:\nE[ε2] = (1 + ρ(k −1)) σ2\nk .\nIt follows that the mean squared error goes to σ2/k as ρ →0, whereas we get σ2 and no beneﬁt when\nthe errors are perfectly correlated.\nUnder the assumption of independently trained agents, we have a reinforcement learning variant of\nthe average pool in the following deﬁnition.\nDeﬁnition 3. Given an ensemble of k agents, let bQ(i) denote the Q-value function estimate of agent i, and let\nbQ := (1/k) ∑k\ni=1 bQ(i) denote the mean function. Then, the average joint policy π selects actions according to:\na∗= arg max\na\nbQ(x, a) = arg max\na\n1\nk\nk\n∑\ni=1\nbQ(i)(x, a).\nat every x ∈X .\nThus, π represents an aggregation strategy where we consider the information provided by each\nagent as equally important. Moreover, by the linearity of expectations and in view of (3), if we have\ninitial functions Q(i)\n0 with n-step ensemble values Qn := (1/k) ∑k\ni=1 Q(i)\nn , then full updates Q(i)\nn := T ∗Q(i)\nn−1\nof each agent will yield Qn = T ∗Qn−1 for the ensemble. Assume further that learning is done with a\nsingle algorithm in separate environments. If we take bQ(i)(x, a) as estimates of Q(i)\nn (x, a) for some step n,\nwith errors εi distributed as multivariate Gaussian noise, then we should expect bQ(x, a) to have a smaller\nexpected error variance in its estimation of Qn(x, a) similar to regression models. This implies more robust\nperformance when given an unstable training process far from convergence, but it also implies diminishing\nimprovements when the algorithm is close to converging to a unique policy.\nHowever, in real applications, and in particular with function approximation, there may be instances\nwhere the improved performance by π does not vanish due to agents converging to distinct sub-optimal\n7 of 14\npolicies. An illustration of this phenomenon can be seen in Figure 1. It shows evaluations during\nlearning in the LunarLander-v2 environment [20]. The single agents used CDRL on a 29 atom support. To\napproximate distributions, the agents used small neural networks with three encoding layers consisting\nof 16 units each. The architecture was purposely chosen to make it harder for the optimizer to converge\nto an optimal policy, possibly due to lack of capacity. At each evaluation point, the models were tested\nwith ε = 0.001. The ﬁgure also includes evaluations of average joint policies of ﬁve agents having the\nsame evaluation ε. However, we can see that the joint information provided by an ensemble of ﬁve agents\ntranscends individual capacity, indicating that some agents settle on distinct sub-optimal solutions.\nFigure 1. Low capacity CDRL implementations in the LunarLander-v2 environment. We can see that the\nenhanced performance of an average joint policy of ﬁve agents may not vanish due to agents settling on\ndistinct sub-optimal policies.\n3.2. Ensemble Categorical Control\nWe consider an ensemble of k agents, each independently trained with the same distributional\nalgorithm, where ηi, i = 1, . . . , k are their respective distributional collections. There are several ways to\naggregate distributional information provided by the ensemble with respect to forecasts and risk-sensitivity\n[21,22]. Perhaps the simplest is a distributional variant of the average joint policy, where we consider the\nmean function η of mixture distributions:\n∀(x, a) η(x,a) := 1\nk\nk\n∑\ni=1\nη(x,a)\ni\n.\n(7)\nSince η(x,a) is a linear pool, it preserves multimodality during aggregation. Hence, it maintains an\narguably more nuanced picture of estimated future rewards compared to methods that generate unimodal\naggregations around unrealizable expected values. In addition, expectations under η yield the Q-function\nused by the average joint policy in Deﬁnition 3 with all the performance beneﬁts that this entails during\nlearning.\nThe ﬁnite support of the CDRL procedure may provide another reason to aggregate by η: Under\nthe assumption that η(x,a)\ni\n, i = 1, . . . , k are drawn as random vectors from some multivariate normal\npopulation with mean µ(x, a) and covariance Σ(x, a), then η is a maximum likelihood estimate of the mean\ncategorical distribution µ(x, a) induced by the algorithm over all possible training runs [23]. It follows that\n8 of 14\nη may provide more robust estimates in reﬂecting mean t-step capabilities of the procedure in terms of\ndistributions found by sending k →∞.\nIt then stands to reason that (7) should help accelerate learning by providing better and more robust\ntargets in the control setting of CDRL. This implies implicitly sharing information gained between agents\nand following accelerated learning trajectories closer to the true expected capability of an algorithm. We\ncan summarize this as an extension of the CDRL control procedure.\nFor a ﬁxed support z, we parameterize individual distribution functions ηi,t, i = 1, . . . , k, at time step\nt by using bD = PA×X as possible inputs and outputs of the algorithm. Let ηt be the mean function of\n{ηi,t}k\ni=1 according to (7). The extension is then given by Algorithm 2.\nAlgorithm 2: Ensemble Categorical Control (ECC)\n1. At each iteration step t and for each agent input ηi,t, sample a transition (x, a, r, x′).\n2. Let a∗= arg maxa′ Qηt(x′, a′).\n3. Recall the Cramér projection Πz given in Deﬁnition 2, and put:\nbη(x,a)\ni,t\n:= Πz ( fr)# η(x′,a∗)\nt\n.\n4. For each agent, follow Step 4 of CDRL with target bη(x,a)\ni,t\n.\nWe note that if updates are done in full or on the same transitions, then the algorithm trivially reduces\nto CDRL by the linearity of ( fr)#; hence, we lose the beneﬁts of the ensemble.\nTo avoid premature convergence to correlated errors, we would ideally want the agents to have the\nfreedom to explore different trajectories during learning. In the case of function approximation, this can\ninvolve maintaining a separate experience buffer for each agent. It can also involve periodical updates of\nensemble target networks in the hope of generating sufﬁciently diverse policies until convergence. The\nlatter is in practical terms the only way to minimize overhead costs induced by inter-thread ensemble\nqueries in simulations. Too short periods here imply fast initial learning; but with correlated errors, high\noverhead costs, and instability [17]. Long periods would imply the possibility of more diverse policies, but\nwith slower learning. The pseudocode for an algorithm using function approximation with ECC can be\nfound in Algorithm A1. The source code for an implementation of ECC can be found at [24].\n4. Empirical Results on a Subset of Atari 2600 Games\nAs a ﬁrst step in understanding the properties of the extension ECC discussed in Section 3.2, we\nnow evaluate an implementation of the procedure on ﬁve Atari 2600 environments found in the Arcade\nLearning Environment [12,20,25].\nSpeciﬁcally, we looked at ensembles of k = 5 agents. To get a proper comparison of the algorithms, we\nemployed for all agents the well-tested architecture, hyperparameters, and training procedure as C51 in [5];\nexcept for a slightly smaller individual replay buffer size at 900 K. This yielded an implicit buffer size of\n4.5 M for the entire ECCensemble. In addition, we employed for each ECC agent a larger ensemble target\nnetwork. The network consisted of copied weights from all ECCnetworks and was updated periodically\nat every 10K steps with negligible overhead.\nWe trained k agents on the ﬁrst 40 M frames (roughly 185 h of Atari-time at 60 Hz). Agent models\nwere saved every 400 K frames. For each save, we evaluated the performance of the individual agents\n(ECCagent) and the ensemble with an average joint policy (ECCensemble). Moreover, we took an ensemble\nof k = 5 independently trained agents using π as our baseline (CDRL joint). For comparison, we also\n9 of 14\nevaluated each such single agent (CDRL agent). In all performance protocols, we started an episode under\nthe 30 no-op regime [17] with an exploration epsilon set to ε = 0.001. The evaluation period was 500 K\nframes with episodes truncated at 108 K frames (30 min).\nIn our particular implementation in [24], each algorithm required roughly two days of compute time\nper environment for training and evaluation combined. Single replay buffers used ~35 GB of optimized\nRAM (~47 GB raw); hence, we used ~175 GB of RAM for concurrently training the ECCensemble.\n4.1. Online Performance\nTo get a sense of the algorithmic robustness and speed of learning, we report the online performance\nof agents and ensembles [7]. Under this protocol, we recorded the average return for each evaluation point\nduring learning. We also stored the best average return score for all points of each seed.\nWe can see in Table 1 and Figure 2 that the extension ensemble was on par or outperformed the\nbaseline in online performance over all ﬁve environments. Moreover, in four out of ﬁve games, single ECC\nagents had similar performance to the joint policy of k independently trained agents, which was the\nmain training objective of the extension algorithm. We also note that in all environments, except possibly\nBreakout and KungFuMaster, ensemble agents seemed to be uncorrelated enough to generate a boost in\nperformance by their joint information, while ECC agents had a better individual performance than single\nCDRL agents in four out of ﬁve games.\nFigure 2. Online performance over the ﬁrst 40 M frames. The evaluation scores shown are moving averages\nover 4 M frames. The data are available at [24].\n10 of 14\nTable 1. Best achieved evaluation scores in online performance over the ﬁrst 40 M samples, here with 95%\nconﬁdence when there is more than one seed. The data are available at [24].\nGame\nCDRL Agent\nECCAgent\nCDRL Joint\nECCEnsemble\nAsterix\n12,998 ± 3042\n28,196 ± 903\n39,413\n38,938\nBerzerk\n795 ± 47\n958 ± 12\n890\n1034\nSpaceInvaders\n1429 ± 91\n1812 ± 87\n1850\n2395\nBreakout\n444 ± 44\n546 ± 27\n515\n665\nKungFuMaster\n27,984 ± 1767\n27,302 ± 2213\n25,826\n29,629\n4.2. Relative Ensemble Sample Performance\nAlthough ensembles will digest frames at nearly k times the rate of a single CDRL algorithm, we\nconsidered here the relative sample performance, where we looked at performance versus the total\ninformation accumulated by an algorithm. Under this protocol, we measured the relative ratio of mean\nevaluation scores as a function of the total amount of frames seen by each learning system. This would\ngive us an idea of how efﬁciently an ensemble algorithm could translate experience into performance on a\nper-sample basis compared to single CDRL. Note that if single CDRL agents all converged to correlated\nerrors, then the joint policy should eventually converge to 1/k-efﬁciency in relative sample performance.\nThus, in general, we should expect the relative performance to degrade as training progresses with\ndiminishing ensemble beneﬁts.\nTable 2 shows the measured relative performance of the two ensemble methods, averaged over the\nﬁrst 40 M samples. We note that initial learning with ensembles may generate performance much higher\nthan 1/k-efﬁciency. We also note that the extension ensemble came close to full efﬁciency in Berzerk\nand Breakout, i.e., it displayed a near k-factor increase in learning rate. However, depending on the\nenvironment, the actual speed-up may vary wildly during learning, as shown in Figure 2.\nTable 2. Rough estimates of relative sample performance, here expressed as percentages of CDRL agent\nperformance and averaged over the ﬁrst 40 M samples. The data are available at [24].\nMethod\nAsterix\nBerzerk\nBreakout\nSpaceInvaders\nKungFuMaster\nECCEnsemble\n47.7 %\n93.7 %\n93.7 %\n63.4 %\n66.9 %\nCDRL Joint\n56.3 %\n86.7 %\n86.1 %\n67.2 %\n87.0 %\n5. Discussion\nIn this paper, we proposed and studied an extension of categorical distributional reinforcement\nlearning, where we employed averaged learning targets over an ensemble. This extension implied an\nimplicit sharing of information between agents during learning, where under the distributional paradigm,\nwe should expect a richer and more robust set of predictions while preserving multimodality during\naggregation. To test these assumptions, we did an initial empirical study on a subset of Atari 2600 games,\nwhere we employed essentially the same architecture and hyperparameter set as the C51 algorithm in [5].\nIn all cases, we saw that the single agent performance objective of the extension was accomplished. We\nalso studied the effects of keeping extension ampliﬁed agents in an ensemble, where in some cases, the\nperformance beneﬁts were present and stronger than an averaged ensemble of independent agents.\nWe note that unlike massively distributed approaches such as Ape-X [26], the extension represents a\ndecentralized distributed learning system with minimal overhead. As such, it naturally comes with poor\nscalability, but with greater efficiency on a per-sample basis. An interesting idea here would be to somewhat\ncounteract the poor scalability by choosing agents with successively lower capacity as the ensemble size\n11 of 14\nincreases. We should then expect to see better performance with increasing size until a cutoff point is reached,\nhinting at the minimum capacity needed to find and represent strong solutions effectively.\nWe leave as future work the matter of convergence analysis and hyperparameter tuning, in particular\nthe update period for a target ensemble network. It is quite possible that the update frequency of C51 was\ntoo aggressive when using ensemble targets. This may lead to premature convergence to correlated agents\nupon reaching difﬁcult environmental plateaus with rarely seen transitions to more abundant rewards.\nSome interesting ideas here would be scheduled update periods or eventually switching to CDRL from\na much stronger and robust level of individual performance. However, to gauge these matters fully, we\nwould need a more comprehensive empirical study.\nAuthor Contributions: Conceptualization, B.L., J.N., and K.-O.L.; methodology, B.L. and J.N.; software, B.L.;\nvalidation, B.L.; formal analysis, B.L., J.N., and K.-O.L.; investigation, B.L.; data curation, B.L.; writing, original\ndraft preparation, B.L.; writing, review and editing, B.L., J.N., and K.-O.L.; visualization, B.L.; supervision, K.-O.L.;\nproject administration, K.-O.L. All authors read and agreed to the published version of the manuscript.\nFunding: This research received no external funding.\nAcknowledgments: The authors would like to thank the referees for comments that helped improve the presentation.\nThe authors would also like to thank Morgan Ericsson, Department of Computer Science and Media Technology,\nLinnæus University, for productive discussions and technical assistance with the LNU-DISAHigh Performance\nComputing Platform.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\n12 of 14\nAbbreviations\nThe following abbreviations are used in this manuscript:\nCDRL\nCategorical Distributional Reinforcement Learning\nMDP\nMarkov Decision Process\nECC\nEnsemble Categorical Control\nAppendix A\nAlgorithm A1: Ensemble categorical control.\nInput: Number of iteration steps N, ensemble size k, support z\nInitialize starting states x1, . . . , xk in independent environments\nInitialize agent networks ηθ1, . . . , ηθk with random parameters θ1, . . . , θk\nInitialize target network η = 1\nk ∑i ηθ−\ni with θ−\ni ←θi\nInitialize replay buffers B1, . . . , Bk with the same size S\nfor t = 1 to N do\nfor all i ∈{ 1, . . . , k } do\nSet ai to be a uniform random action with probability εt\nOtherwise, set ai ←arg maxa′ Qηθi (xi, a′)\nExecute ai, and store the transition (xi, ai, ri, x′\ni) in Bi\nSet xi ←x′\ni\nend for\nif t ≡0 mod Pupdate then\nfor all i ∈{ 1, . . . , k } do\nInitialize loss L ←0\nSample uniformly a minibatch B ⊂Bi\nfor all (x, a, r, x′) ∈B do\nSet a∗←arg maxa′ Qη(x′, a′)\nSet L ←L + KL\n\u0010\nΠz ( fr)# η(x′,a∗) ∥η(x,a)\nθi\n\u0011\nend for\nUpdate θi by a gradient descent step on L\nend for\nend if\nif t ≡0 mod Pclone then\nfor all i ∈{ 1, . . . , k } do\nUpdate target network with θ−\ni ←θi\nend for\nend if\nend for\nReferences\n1.\nSingh, S.P. The efﬁcient learning of multiple task sequences. In Advances in Neural Information Processing Systems;\nMIT Press: Cambridge, MA, USA, 1992; pp. 251–258.\n2.\nSun, R.; Peterson, T. Multi-agent reinforcement learning: weighting and partitioning.\nNeural Netw. 1999,\n12, 727–753. [CrossRef]\n13 of 14\n3.\nWiering, M.A.; Van Hasselt, H. Ensemble algorithms in reinforcement learning. IEEE Trans. Syst. Man, Cybern.\nPart B (Cybernetics) 2008, 38, 930–936. [CrossRef] [PubMed]\n4.\nFaußer, S.; Schwenker, F. Selective neural network ensembles in reinforcement learning: Taking the advantage of\nmany agents. Neurocomputing 2015, 169, 350–357. [CrossRef]\n5.\nBellemare, M.G.; Dabney, W.; Munos, R. A distributional perspective on reinforcement learning. In Proceedings\nof the 34th International Conference on Machine Learning, Sydney, Australia, 6–11 August 2017; Volume 70, pp.\n449–458.\n6.\nMorimura, T.; Sugiyama, M.; Kashima, H.; Hachiya, H.; Tanaka, T. Parametric return density estimation for\nreinforcement learning. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artiﬁcial Intelligence,\nCatalina Island, CA, USA, 8–11 July 2010; pp. 368–375.\n7.\nDabney, W.; Rowland, M.; Bellemare, M.G.; Munos, R. Distributional reinforcement learning with quantile\nregression. In Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, New Orleans, LA,\nUSA, 2–7 February 2018.\n8.\nDabney, W.; Ostrovski, G.; Silver, D.; Munos, R. Implicit Quantile Networks for Distributional Reinforcement\nLearning. Int. Conf. Mach. Learn. 2018, 80, 1096–1105.\n9.\nLyle, C.; Bellemare, M.G.; Castro, P.S. A comparative analysis of expected and distributional reinforcement\nlearning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, Honolulu, HI, USA, 27 January–1\nFebruary 2019; Volume 33, pp. 4504–4511.\n10.\nRowland, M.; Bellemare, M.; Dabney, W.; Munos, R.; Teh, Y.W. An Analysis of Categorical Distributional\nReinforcement Learning. In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics,\nCanary Islands, Spain, 9–11 April 2018; pp. 29–37.\n11.\nBellemare, M.G.; Le Roux, N.; Castro, P.S.; Moitra, S. Distributional reinforcement learning with linear function\napproximation. In Proceedings of the 22nd International Conference on Artiﬁcial Intelligence and Statistics,\nOkinawa, Japan, 16–18 April 2019; pp. 2203–2211.\n12.\nBellemare, M.G.; Naddaf, Y.; Veness, J.; Bowling, M. The Arcade Learning Environment: An Evaluation Platform\nfor General Agents. J. Artif. Intell. Res. 2013, 47, 253–279. [CrossRef]\n13.\nRizzo, M.L.; Székely, G.J. Energy distance. Wiley Interdiscip. Rev. Comput. Stat. 2016, 8, 27–38. [CrossRef]\n14.\nBertsekas, D.P.; Tsitsiklis, J.N. Neuro-Dynamic Programming; Athena Scientiﬁc: Belmont, MA, USA, 1996.\n15.\nVillani, C. Optimal Transport: Old and New; Springer Science & Business Media: Berlin, Germany, 2008; Volume\n338.\n16.\nBellemare, M.G.; Danihelka, I.; Dabney, W.; Mohamed, S.; Lakshminarayanan, B.; Hoyer, S.; Munos, R.\nThe Cramer Distance as a Solution to Biased Wasserstein Gradients. arXiv 2017, arXiv:1705.10743.\n17.\nMnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A.A.; Veness, J.; Bellemare, M.G.; Graves, A.; Riedmiller, M.;\nFidjeland, A.K.; Ostrovski, G.; et al. Human-level control through deep reinforcement learning. Nature 2015,\n518, 529–533. [CrossRef] [PubMed]\n18.\nBreiman, L. Bagging predictors. Mach. Learn. 1996, 24, 123–140. [CrossRef]\n19.\nGoodfellow, I.; Bengio, Y.; Courville, A. Deep Learning; MIT Press: Cambridge, MA, USA, 2016.\n20.\nBrockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman, J.; Tang, J.; Zaremba, W. OpenAI Gym. arXiv\n2016, arXiv:1606.01540.\n21.\nClemen, R.T.; Winkler, R.L. Combining probability distributions from experts in risk analysis. Risk Anal. 1999,\n19, 187–203. [CrossRef]\n22.\nCasarin, R.; Mantoan, G.; Ravazzolo, F. Bayesian calibration of generalized pools of predictive distributions.\nEconometrics 2016, 4, 17. [CrossRef]\n23.\nJohnson, R.A.; Wichern, D.V. Applied Multivariate Statistical Analysis; Pearson: Harlow, UK, 2014.\n24.\nLindenberg, B.; Nordqvist, J.; Lindahl, K.O. bjliaa/ecc: ecc; (Version v0.3-alpha). Zenodo: 2020. Available online:\nhttps://zenodo.org/record/3760246#.XrP1Oi4za1U (accessed on 22 April 2020).\n25.\nHill, A.; Rafﬁn, A.; Ernestus, M.; Gleave, A.; Kanervisto, A.; Traore, R.; Dhariwal, P.; Hesse, C.; Klimov, O.; Nichol,\nA.; et al. Stable Baselines. 2018. Available online: https://github.com/hill-a/stable-baselines (accessed on 22\nApril 2020).\n14 of 14\n26.\nHorgan, D.; Quan, J.; Budden, D.; Barth-Maron, G.; Hessel, M.; van Hasselt, H.; Silver, D. Distributed Prioritized\nExperience Replay. arXiv 2018, arXiv:1803.00933.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.MA",
    "stat.ML",
    "I.2.11; I.2.8"
  ],
  "published": "2020-03-24",
  "updated": "2020-05-22"
}