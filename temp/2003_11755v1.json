{
  "id": "http://arxiv.org/abs/2003.11755v1",
  "title": "A Survey of Deep Learning for Scientific Discovery",
  "authors": [
    "Maithra Raghu",
    "Eric Schmidt"
  ],
  "abstract": "Over the past few years, we have seen fundamental breakthroughs in core\nproblems in machine learning, largely driven by advances in deep neural\nnetworks. At the same time, the amount of data collected in a wide array of\nscientific domains is dramatically increasing in both size and complexity.\nTaken together, this suggests many exciting opportunities for deep learning\napplications in scientific settings. But a significant challenge to this is\nsimply knowing where to start. The sheer breadth and diversity of different\ndeep learning techniques makes it difficult to determine what scientific\nproblems might be most amenable to these methods, or which specific combination\nof methods might offer the most promising first approach. In this survey, we\nfocus on addressing this central issue, providing an overview of many widely\nused deep learning models, spanning visual, sequential and graph structured\ndata, associated tasks and different training methods, along with techniques to\nuse deep learning with less data and better interpret these complex models ---\ntwo central considerations for many scientific use cases. We also include\noverviews of the full design process, implementation tips, and links to a\nplethora of tutorials, research summaries and open-sourced deep learning\npipelines and pretrained models, developed by the community. We hope that this\nsurvey will help accelerate the use of deep learning across different\nscientific domains.",
  "text": "A Survey of Deep Learning for Scientiﬁc Discovery\nMaithra Raghu1,2∗\nEric Schmidt1,3\n1 Google\n2 Cornell University\n3 Schmidt Futures\nAbstract\nOver the past few years, we have seen fundamental breakthroughs in core problems in machine learning,\nlargely driven by advances in deep neural networks. At the same time, the amount of data collected in a\nwide array of scientiﬁc domains is dramatically increasing in both size and complexity. Taken together,\nthis suggests many exciting opportunities for deep learning applications in scientiﬁc settings. But a\nsigniﬁcant challenge to this is simply knowing where to start. The sheer breadth and diversity of diﬀerent\ndeep learning techniques makes it diﬃcult to determine what scientiﬁc problems might be most amenable\nto these methods, or which speciﬁc combination of methods might oﬀer the most promising ﬁrst approach.\nIn this survey, we focus on addressing this central issue, providing an overview of many widely used deep\nlearning models, spanning visual, sequential and graph structured data, associated tasks and diﬀerent\ntraining methods, along with techniques to use deep learning with less data and better interpret these\ncomplex models — two central considerations for many scientiﬁc use cases. We also include overviews of\nthe full design process, implementation tips, and links to a plethora of tutorials, research summaries and\nopen-sourced deep learning pipelines and pretrained models, developed by the community. We hope that\nthis survey will help accelerate the use of deep learning across diﬀerent scientiﬁc domains.\n1\nIntroduction\nThe past few years have witnessed extraordinary advances in machine learning using deep neural networks.\nDriven by the rapid increase in available data and computational resources, these neural network models and\nalgorithms have seen remarkable developments, and are a staple technique in tackling fundamental tasks\nranging from speech recognition [70, 167], to complex tasks in computer vision such as image classiﬁcation,\n(instance) segmentation, action recognition [117, 78, 240], and central problems in natural language, including\nquestion answering, machine translation and summarization [186, 172, 233, 197]. Many of these fundamental\ntasks (with appropriate reformulation) are relevant to a much broader array of domains, and in particular\nhave tremendous potential in aiding the investigation of central scientiﬁc questions.\nHowever, a signiﬁcant obstacle in beginning to use deep learning is simply knowing where to start. The\nvast research literature, coupled with the enormous number of underlying models, tasks and training methods\nmakes it very diﬃcult to identify which techniques might be most appropriate to try, or the best way to start\nimplementing them.\nThe goal of this survey is to help address this central challenge. In particular, it has the following\nattributes:\n• The survey overviews a highly diverse set of deep learning concepts, from deep neural network models\nfor varied data modalities (CNNs for visual data, graph neural networks, RNNs and Transformers for\n∗Correspondence to maithrar@gmail.com\n1\narXiv:2003.11755v1  [cs.LG]  26 Mar 2020\nsequential data) to the many diﬀerent key tasks (image segmentation, super-resolution, sequence to\nsequence mappings and many others) to the multiple ways of training deep learning systems.\n• But the explanation of these techniques is relatively high level and concise, to ensure the core ideas are\naccessible to a broad audience, and so that the entire survey can be read end to end easily.\n• From the perspective of aiding scientiﬁc applications, the survey describes in detail (i) methods to use\ndeep learning with less data (self-supervision, semi-supervised learning, and others) and (ii) techniques\nfor interpretability and representation analysis (for going beyond predictive tasks). These are two\nexciting and rapidly developing research areas, and are also of particular signiﬁcance to possible scientiﬁc\nuse cases.\n• The survey also focuses on helping quickly ramp up implementation, and in addition to overviews of the\nentire deep learning design process and a section on implementation tips (Section 9), the survey has a\nplethora of open-sourced code, research summaries and tutorial references developed by the community\nthroughout the text, including a full section (Section 3) dedicated to this.\nWho is this survey for?\nWe hope this survey will be especially helpful for those with a basic\nunderstanding of machine learning, interested in (i) getting a comprehensive but accessible overview of many\nfundamental deep learning concepts and (ii) references and guidance in helping ramp up implementation.\nBeyond the core areas of deep learning, the survey focuses on methods to develop deep learning systems\nwith less data, and techniques for interpreting these models, which we hope will be of particular use for\nthose interested in applying these techniques in scientiﬁc problems. However, these topics and many others\npresented, along with the many code/tutorial/paper references may be helpful to anyone looking to learn\nabout and implement deep learning.\n1.1\nOutline of Survey\nThe survey is structured as follows:\n• Section 2 starts with some high level considerations for using deep learning. Speciﬁcally, we ﬁrst discuss\nsome template ways in which deep learning might be applied in scientiﬁc domains, followed by a general\noverview of the entire deep learning design process, and conclude with a brief discussion of other central\nmachine learning techniques that may be better suited to some problems. The ﬁrst part may be of\nparticular interest to those considering scientiﬁc applications, while the latter two parts may be of\ngeneral interest.\n• Section 3 provides references to tutorials, open-sourced code model/algorithm implementations, and\nwebsites with research paper summaries, all developed by the deep learning community. This section\nshould be very helpful for many readers and we encourage skimming through the links provided.\n• Section 4 then overviews many of the standard tasks and models in deep learning, covering convolutional\nnetworks and their many uses, graph neural networks, sequence models (RNNs, Transformers) and the\nmany associated sequence tasks.\n• Section 5 looks at some key variants of the supervised learning training process, such as transfer learning,\ndomain adaptation and multitask learning. These are central to many successful applications of deep\nlearning.\n• Section 6 considers ways to improve the data eﬃciency for developing deep neural network models,\nwhich has been a rapidly evolving area of research, and a core consideration for many applications,\nincluding scientiﬁc domains. It covers the many variants of self-supervision and semi-supervised learning,\nas well as data augmentation and data denoising.\n2\n• Section 7 overviews advances in interpretability and representational analysis, a set of techniques focused\non gaining insights into the internals of the end-to-end system: identifying important features in the data,\nunderstanding its eﬀect on model outputs and discovering properties of model hidden representations.\nThese are very important for many scientiﬁc problems which emphasise understanding over predictive\naccuracy, and may be of broader interest for e.g. aiding model debugging and preemptively identifying\nfailure modes.\n• Section 8 provides a brief overview of more advanced deep learning methods, speciﬁcally generative\nmodelling and reinforcement learning.\n• Section 9 concludes with some key implementation tips when putting together an end-to-end deep\nlearning system, which we encourage a quick read through!\n2\nHigh Level Considerations for Deep Learning\nIn this section we ﬁrst discuss some high level considerations for deep learning techniques. We start with\noverviews of template ways in which deep learning might be applied in scientiﬁc settings, followed by a\ndiscussion of the end-to-end design process and some brief highlights of alternate machine learning methods\nwhich may be more suited to some problems.\n2.1\nTemplates for Deep Learning in Scientiﬁc Settings\nWhat are the general ways in which we might apply deep learning techniques in scientiﬁc settings? At a very\nhigh level, we can oﬀer a few templates of ways in which deep learning might be used in such problems:\n(1) Prediction Problems Arguably the most straightforward way to apply deep learning is to use it to\ntackle important prediction problems: mapping inputs to predicted outputs. This predictive use case of\ndeep learning is typically how it is also used in core problems in computing and machine learning. For\nexample, the input might be a biopsy image, and the model must output a prediction of whether the\nimaged tissue shows signs of cancer. We can also think of this predictive use case as getting the model\nto learn a target function, in our example, mapping from input visual features to the cancer/no cancer\noutput. Using deep learning in this way also encapsulates settings where the target function is very\ncomplex, with no mathematical closed form or logical set of rules that describe how to go from input to\noutput. For instance, we might use a deep learning model to (black-box) simulate a complex process\n(e.g. climate modelling), that is very challenging to explicitly model [101].\n(2) From Predictions to Understanding One fundamental diﬀerence between scientiﬁc questions and core\nmachine learning problems is the emphasis in the former on understanding the underlying mechanisms.\nOftentimes, outputting an accurate prediction alone is not enough. Instead, we want to gain interpretable\ninsights into what properties of the data or the data generative process led to the observed prediction\nor outcome. To gain these kinds of insights, we can turn to interpretability and representation analysis\nmethods in deep learning, which focus on determining how the neural network model makes a speciﬁc\nprediction. There has been signiﬁcant work on both tools to understand what features of the input are\nmost critical to the output prediction, as well as techniques to directly analyze the hidden representations\nof the neural network models, which can reveal important properties of the underlying data.\n(3) Complex Transformations of Input Data In many scientiﬁc domains, the amount of generated data,\nparticularly visual data (e.g. ﬂuorescence microscopy, spatial sequencing, specimen videos [177, 97]) has\ngrown dramatically, and there is an urgent need for eﬃcient analysis and automated processing. Deep\nlearning techniques, which are capable of many complex transformations of data, can be highly eﬀective\nfor such settings, for example, using a deep neural network based segmentation model to automatically\n3\nData\nCollection, Preprocessing, \nVisualization, Augmentation\nLearning\nValidation + Analysis\nCollect \nRaw \nData\nLabel\nPreprocessing \nand \nAugmentation \nVisualization\nVisualization \nand testing \nsimple tasks\nModel\nConvolutional Net, \nTransformer/RNN, \nGraph Neural Net\nTask\nClassification, Detection\nLearning Embeddings,\nMethod\nSupervised Learning, \nSelf-Supervision, \nSemi-Supervised, Transfer, \nMultitask Learning\nPerformance\nTest on hold out\nDistribution shift?\nAnalysis + Interpretation\nError analysis?\nAblation Study?\nUseful representations?\nBias?\nSpurious Correlations?\nIterate (3)\nIterate (2)\nIterate (4)\nIterate (5)\nTrain\nOptimizer\nLR schedule\nRegularize\nIterate (1)\nFigure 1: Schematic of a typical deep learning workﬂow. A typical development process for deep learning\napplications can be viewed as consisting of three sequential stages (i) data related steps (ii) the learning component\n(iii) validation and analysis. Each one of these stages has several substeps and techniques associated with it, also\ndepicted in the ﬁgure. In the survey we will overview most techniques in the learning component, as well as some\ntechniques in the data and validation stages. Note that while a natural sequence is to ﬁrst complete steps in the data\nstage, followed by learning and then validation, standard development will likely result in multiple diﬀerent iterations\nwhere the techniques used or choices made in one stage are revisited based oﬀof results of a later stage.\nidentify the nuclei in images of cells, or a pose estimation system to rapidly label behaviors seen in\nvideos of mice for neuroscience analysis.\n2.2\nDeep Learning Workﬂow\nWith these examples of templates for deep learning applications in science, we next look at the end to end\nworkﬂow for designing a deep learning system. Figure 1 illustrates what a typical workﬂow might look like.\nHaving selected the overarching (predictive) problem of interest, we can broadly think of having three\nstages for designing and using the deep learning system: (i) data related steps, such as collection, labelling,\npreprocessing, visualization, etc (ii) learning focused steps, such as choice of deep neural network model, the\ntask and method used to train the model (iii) validation and analysis steps, where performance evaluations\nare conducted on held out data, as well as analysis and interpretation of hidden representations and ablation\nstudies of the overall methods.\nThese three stages are naturally sequential. However, almost all of the time, the ﬁrst attempt at building\nan end-to-end deep learning system will result in some kind of failure mode. To address these, it is important\nto keep in mind the iterative nature of the design process, with results from the diﬀerent stages informing the\nredesign and rerunning of other stages.\nFigure 1 provides some examples of common iterations with the backward connecting arrows: (i) the\nIterate (1) arrow, corresponding to iterations on the data collection process, e.g. having performed some\ndata visualization, the labelling process for the raw instances might require adjusting — the ﬁrst labelling\nmechanism might be too noisy, or not capture the objective of interest (ii) the Iterate (2) arrow, corresponding\nto iterations on the learning setup, due to e.g. deciding that a diﬀerent task or method might be more\nappropriate, or decomposing the learning process into multiple steps — ﬁrst performing self-supervision\nfollowed by supervised learning (iii) the Iterate (3) arrow, changing the data related steps based oﬀof the\nresults of the learning step (iv) the Iterate (4) arrow, redesigning the learning process informed by the\n4\nvalidation results e.g. ﬁnding out the model has overﬁt on the training data at validation and hence reducing\ntraining time or using a simpler model (v) the Iterate (5) arrow, adapting the data steps based oﬀthe\nvalidation/analysis results, e.g. ﬁnding that the model is relying on spurious attributes of the data, and\nimproving data collection/curation to mitigate this.\nFocus of Survey and Nomenclature\nIn this survey, we provide a comprehensive overview of many of\nthe techniques in the learning stage, along with some techniques (e.g. data augmentation, interpretability\nand representation analysis, Section 7) in the data and validation stages.\nFor the learning stage, we look at popular models, tasks and methods. By models (also sometimes referred\nto as architecture), we mean the actual structure of the deep neural network — how many layers, of what\ntype, and how many neurons, etc. By tasks, we mean the kind of prediction problem, speciﬁcally, the type of\ninput and output. For example, in an image classiﬁcation task, the input consists of images and the output\na probability distribution over a (discrete) set of diﬀerent categories (called classes). By methods, we refer\nto the type of learning process used to train the system. For example, supervised learning is a very general\nlearning process, consisting of the neural network being given data instances with corresponding labels, with\nthe labels providing supervision.\nUnlike diﬀerent models and tasks, methods can be subsets of other methods. For example, self-supervision,\na method where the neural network is trained on data instances and labels, but the labels automatically\ncreated from the data instance, can also be considered a type of supervised learning. This can be a little\nconfusing! But it suﬃces to keep in mind the general notions of models, tasks and methods.\n2.3\nDeep Learning or Not?\nAs a ﬁnal note before diving into the diﬀerent deep learning techniques, when formulating a problem, it\nis important to consider whether deep learning provides the right set of tools to solve it. The powerful\nunderlying neural network models oﬀer many sophisticated functionalities, such learned complex image\ntransforms. However, in many settings, deep learning may not be the best technique to start with or best\nsuited to the problem. Below we very brieﬂy overview some of the most ubiquitous machine learning methods,\nparticularly in scientiﬁc contexts.\nDimensionality Reduction and Clustering\nIn scientiﬁc settings, the ultimate goal of data analysis is\noften understanding — identifying the underlying mechanisms that give rise to patterns in the data. When\nthis is the goal, dimensionality reduction, and/or clustering are simple (unsupervised) but highly eﬀective\nmethods to reveal hidden properties in the data. They are often very useful in the important ﬁrst step of\nexploring and visualizing the data (even if more complex methods are applied later.)\nDimensionality Reduction: Dimensionality reduction methods are either linear, relying on a linear\ntransformation to reduce data dimensionality, or non-linear, reducing dimensionality while approximately\npreserving the non-linear (manifold) structure of the data. Popular linear dimensionality reduction methods\ninclude PCA and non-negative matrix factorization, with some popular non-linear methods including t-\nSNE [141] and UMAP [148]. Most dimensionality reduction methods have high-quality implementations\nin packages like scikit-learn or on github, e.g. https://github.com/oreillymedia/t-SNE-tutorial or\nhttps://github.com/lmcinnes/umap.\nClustering: Often used in combination with dimensionality reduction, clustering methods provide a\npowerful, unsupervised way to identify similarities and diﬀerences across the data population. Commonly\nused clustering methods include k-means (particularly the k-means++ variant), Gaussian Mixture Models\n(GMMs), hierarchical clustering and spectral clustering. Like dimensionality reduction techniques, these\nclustering methods have robust implementations in packages like scikit-learn.\n5\nIn Section 7.2.2, we discuss how dimensionality reduction and clustering can be used on the hidden\nrepresentations of neural networks.\nLinear Regression, Logistic Regression (and variants!)\nArguably the most fundamental techniques\nfor supervised problems like classiﬁcation and regression, linear and logistic regression, and their variants (e.g.\nLasso, Ridge Regression) may be particularly useful when there is limited data, and a clear set of (possibly\npreprocessed) features (such as in tabular data.) These methods also often provide a good way to sanity check\nthe overarching problem formulation, and may be a good starting point to test out a very simple version of\nthe full problem. Due to their simplicity, linear and logistic regression are highly interpretable, and provide\nstraightforward ways to perform feature attribution.\nDecision Trees, Random Forests and Gradient Boosting\nAnother popular class of methods are deci-\nsion trees, random forests and gradient boosting. These methods can also work with regression/classiﬁcation\ntasks, and are well suited to model non-linear relations between the input features and output predic-\ntions. Random forests, which ensemble decision trees, can often be preferred to deep learning methods\nin settings where the data has a low signal-to-noise ratio.\nThese methods can typically be less inter-\npretable than linear/logistic regression, but recent work [160] has looked at developing software libraries\nhttps://github.com/interpretml/interpret to address this challenge.\nOther Methods and Resources:\nBoth the aforementioned techniques and many other popular methods\nsuch as graphical models, Gaussian processes, Bayesian optimization are overviewed in detail in excellent\ncourse notes such as University of Toronto‘s Machine Learning Course or Stanford‘s CS229, detailed articles\nat https://towardsdatascience.com/ and even interactive textbooks such as https://d2l.ai/index.html\n(called Dive into Deep Learning [267]) and https://github.com/rasbt/python-machine-learning-book-\n2nd-edition.\n3\nDeep Learning Libraries and Resources\nA remarkable aspect of advances in deep learning so far is the enormous number of resources developed and\nshared by the community. These range from tutorials, to overviews of research papers, to open sourced code.\nThroughout this survey, we will reference some of these materials in the topic speciﬁc sections, but we ﬁrst\nlist here a few general very useful frameworks and resources.\nSoftware Libraries for Deep Learning:\nArguably the two most popular code libraries for deep learning\nare PyTorch (with a high level API called Lightning) and TensorFlow (which also oﬀers Keras as a high\nlevel API.) Developing and training deep neural network models critically relies on fast, parallelized matrix\nand tensor operations (sped up through the use of Graphical Processing Units) and performing automatic\ndiﬀerentiation for computing gradients and optimization (known as autodiﬀ.) Both PyTorch and TensorFlow\noﬀer these core utilities, as well as many other functions. Other frameworks include Chainer, ONNX, MXNET\nand JAX. Choosing the best framework has been the source of signiﬁcant debate. For ramping up quickly,\nprogramming experiences closest to native Python, and being able to use many existing code repositories,\nPyTorch (or TensorFlow with the Keras API) may be two of the best choices.\nTutorials:\n(i) https://course.fast.ai/ fast.ai provides a free, coding-ﬁrst course on the most important\ndeep learning techniques as well as an intuitive and easy to use code library, https://github.com/fastai/\nfastai, for model design and development. (ii) https://towardsdatascience.com/ contains some fantastic\ntutorials on almost every deep learning topic imaginable, crowd sourced from many contributors. (iii)\n6\nMany graduate deep learning courses have excellent videos and lecture notes available online, such as\nhttp://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/ for Deep Learning and Neural Networks, or\nthe more topic speciﬁc Stanford‘s CS224N NLP with Deep Learning. A nice collection of some of these topic\nspeciﬁc lectures is provided at https://github.com/Machine-Learning-Tokyo/AI_Curriculum. There are\nalso some basic interactive deep learning courses online, such as https://github.com/leriomaggio/deep-\nlearning-keras-tensorflow.\nResearch Overviews, Code, Discussion:\n(i) https://paperswithcode.com/ This excellent site keeps\ntrack of new research papers and their corresponding opensourced code, trending directions and displays state\nof the art results (https://paperswithcode.com/sota) across many standard benchmarks. (ii) Discussion\nof deep learning research is very active on Twitter. http://www.arxiv-sanity.com/top keeps track of\nsome of the top most discussed papers and comments. (iii) https://www.reddit.com/r/MachineLearning/\nis also a good forum for research and general project discussion.\n(iv) https://www.paperdigest.org/\nconference-paper-digest/ contains snippets of all the papers in many diﬀerent top machine learning\nconferences. (v) IPAM (Institute for Pure and Applied Mathematics) has a few programs e.g. https://\nwww.ipam.ucla.edu/programs/workshops/new-deep-learning-techniques/?tab=schedule and https://\nwww.ipam.ucla.edu/programs/workshops/deep-learning-and-medical-applications/?tab=schedule with\nvideos overviewing deep learning applications in science.\nModels, Training Code and Pretrained Models:\nAs we discuss later in the survey, publicly available\nmodels, training code and pretrained models are very useful for techniques such as transfer learning. There\nare many good sources of these, here are a few that are especially comprehensive and/or accessible:\n(i) Pytorch and TensorFlow have a collection of pretrained models, found at https://github.com/\ntensorflow/models and https://pytorch.org/docs/stable/torchvision/models.html.\n(ii) https://github.com/huggingface Hugging Face (yes, that really is the name), oﬀers a huge collection\nof both pretrained neural networks and the code used to train them. Particularly impressive is their\nlibrary of Transformer models, a one-stop-shop for sequential or language applications.\n(iii) https://github.com/rasbt/deeplearning-models oﬀers many standard neural network architectures,\nincluding multilayer perceptrons, convolutional neural networks, GANs and Recurrent Neural Networks.\n(iv) https://github.com/hysts/pytorch_image_classification does a deep dive into image classiﬁca-\ntion architectures, with training code, highly popular data augmentation techniques such as cutout, and\ncareful speed and accuracy benchmarking. See their page for some object detection architectures also.\n(v) https://github.com/openai/baselines provides implementations of many popular RL algorithms.\n(vi) https://modelzoo.co/ is a little like paperswithcode, but for models, linking to implementations of\nneural network architectures for many diﬀerent standard problems.\n(vii) https://github.com/rusty1s/pytorch_geometric. Implementations and paper links for many graph\nneural network architectures.\nData Collection, Curation and Labelling Resources:\nA crucial step in applying deep learning to a\nproblem is collecting, curating and labelling data. This is a very important, time-intensive and often highly\nintricate task (e.g. labelling object boundaries in an image for segmentation.) Luckily, there are some resources\nand libraries to help with this, for example https://github.com/tzutalin/labelImg, https://github.com/\nwkentaro/labelme, https://rectlabel.com/ for images and https://github.com/doccano/doccano for\ntext/sequential data.\n7\nData Instances\nDog\nVolcano\nFiretruck\nOptimize \nparameters\nLabels\n?\nTraining\nUnseen Data\nDog\nEvaluation\nFigure 2: The Supervised Learning process for training neural networks. The ﬁgure illustrates the supervised\nlearning process for neural networks. Data instances (in this case images) and corresponding labels are collected.\nDuring the training step, the parameters of the neural network are optimized so that when input a data instance, the\nneural network outputs the corresponding label. During evaluation, the neural network is given unseen data instances\nas input, and if trained successfully, will output a meaningful label (prediction).\nVisualization, Analysis and Compute Resources:\nWhen training deep neural network models, it is\ncritical to visualize important metrics such as loss and accuracy while the model is training. Tensorboard https:\n//www.tensorflow.org/tensorboard (which works with Pytorch and TensorFlow) is a very popular framework\nfor doing this. Related is the colab eﬀort https://colab.research.google.com/notebooks/welcome.ipynb,\nwhich, aside from providing a user-friendly, interactive way for model development and analysis (very similar\nto jupyter notebooks) also provides some (free!) compute resources.\n4\nStandard Neural Network Models and Tasks\nIn this section, we overview the standard neural network models and the kinds of tasks they can be used for,\nfrom convolutional networks for image predictions and transformations to transformer models for sequential\ndata to graph neural networks for chemistry applications.\n4.1\nSupervised Learning\nBefore diving into the details of the diﬀerent deep neural network models, it is useful to brieﬂy discuss\nsupervised learning, the most standard method to train these models. In the supervised learning framework,\nwe are given data instances and an associated label for each data instance, i.e. (data instance, label) pairs.\nFor example, the data instances might comprise of chest x-ray images, and the labels (one for each chest\nx-ray image) a binary yes/no to whether it shows the symptoms of pneumonia. Training the neural network\nmodel then consists of ﬁnding values for its parameters so that when it is fed in a data instance (chest x-ray)\nas input, it correctly outputs the corresponding label (yes/no on whether the chest x-ray has pneumonia.) To\nﬁnd these parameter values, we perform iterative optimization to guide the neural network parameters to\nappropriate values, using the given labels to provide supervision. Figure 2 shows a schematic of the supervised\nlearning setup for deep learning.\n8\nSupervised learning is the most basic yet most critical method for training deep neural networks. As will\nbe seen through the subsequent sections, there can be signiﬁcant diversity in the kinds of (data, label) pairs\nused. Even in settings where clear (data, label) pairs are not possible to collect (Sections 6, 6.2), the training\nproblem is often reformulated and recast into a supervised learning framework.\n4.2\nMultilayer Perceptrons\nThe ﬁrst and most basic kind of deep neural network is the multilayer perceptron. These models consist of a\nstack of fully connected layers (matrix multiplications) interleaved with a nonlinear transform.\nDespite their simplicity, they are useful for problems where the data might consist of a set of distinct,\n(possibly categorical) features, for example, tabular data. These models have more expressive power than\nlogistic/linear regression, though those methods would be a good ﬁrst step to try. One way to apply these\nmodels might be to ﬁrst preprocess the data to compute the distinct set of features likely to be important,\nand use this as input. https://github.com/rasbt/deeplearning-models provides some implementations\nof some example multilayer perceptron architectures.\nScientiﬁc Examples\nOne recent scientiﬁc example is given by the use of simple MLPs for pharamaceutical\nformulation [256], developing variants of a drug that is stable and safe for patient use.\n4.3\nConvolutional Neural Networks\nThese are arguably the most well known family of neural networks, and are very useful in working with any\nkind of image data. They are characterized by having convolutional layers, which allow the neural network\nto reuse parameters across diﬀerent spatial locations of an image. This is a highly useful inductive bias for\nimage data, and helping with eﬃciently learning good features, some, like Gabor ﬁlters, which correspond to\ntraditional computer vision techniques. Convolutional neural networks (CNNs) have so many possible uses\nthat we overview some of the most ubiquitous tasks separately below.\n4.3.1\nImage Classiﬁcation\nThis is arguably the simplest and most well known application of convolutional neural networks. The model\nis given an input image, and wants to output a class — one of a (typically) mutually exclusive set of labels\nfor that image. The earlier example, of mapping a chest x-ray image to a binary disease label, is precisely\nimage classiﬁcation.\nConvolutional neural networks for image classiﬁcation is an extremely common application of deep learning.\nThere many diﬀerent types of CNN models for classiﬁcation: VGG — a simple stack of convolutional layers\nfollowed by a fully connected layer [214], ResNets — which are a family of convolutional networks of diﬀerent\nsizes and depths and skip connections [79], DenseNets — another family of models where unlike standard\nneural networks, every layer in a \"block\" is connected to every other layer [94]. More recent, complex models\ninclude ResNeXt [253] and recently EﬃcientNets, which have separate scaling factors for network depth,\nwidth and the spatial resolution of the input image [223]. Tutorials, implementations and pretrained versions\nof many of these models can be found in the references given in Section 3.\nScientiﬁc Examples:\nImage classiﬁcation has found many varied scientiﬁc applications, such as in analyzing\ncryoEM data [226] (with associated code https://github.com/cramerlab/boxnet). An especially large\nbody of work has looked at medical imaging uses of image classiﬁcation, speciﬁcally, using CNNs to predict\ndisease labels. Examples range from ophthalmology [72], radiology (2D x-rays and 3D CT scans) [258, 5, 185],\n9\nFigure 3: Diﬀerences between Image Classiﬁcation, Object Detection, Semantic Segmentation and\nInstance Segmentation tasks. Image source [1] The ﬁgure illustrates the diﬀerences between classiﬁcation,\nobject detection, semantic segmentation and instance segmentation. In classiﬁcation, the whole image gets a single label\n(balloons), while in object detection, each balloon is also localized with a bounding box. In semantic segmentation, all\nthe pixels corresponding to balloon are identiﬁed, while in instance segmentation, each individual balloon is identiﬁed\nseparately.\npathology [135, 55], analyzing brain scans (PET, fMRI) [202, 45]. An excellent survey of the numerous papers\nin this area is given by [228].\n4.3.2\nObject Detection\nImage classiﬁcation can be thought of as a global summary of the image. Object detection dives into some of\nthe lower level details of the image, and looks at identifying and localizing diﬀerent objects in the image. For\nexample, given an input image of an outdoor scene having a dog, a person and a tree, object detection would\nlook at both identifying the presence of the dog, person and tree and ‘circle their location’ in the image —\nspeciﬁcally, put a bounding box around each of them. The supervised learning task is thus to take an input\nimage and output the coordinates of these bounding boxes, as well as categorizing the kind of object they\ncontain.\nLike image classiﬁcation, there are many high performing and well established convolutional architectures\nfor object detection. Because of the intricacy of the output task, these models tend to be more complex\nwith a backbone component (using an image classiﬁcation model) and a region proposal component for\nbounding box proposals. But there are still many pretrained models available to download. One of the\nmost successful early models was Faster R-CNN [192], which signiﬁcantly sped up the slow bounding\nbox proposal component. Since then there have been many improved models, including YOLOv3 [191],\nand most recently EﬃcientDets [224]. Arguably the most popular recent architecture however has been\nMask R-CNN and its variants [78, 248].\nMask R-CNN performs some segmentation as well as object\ndetection (see below).\nBesides some of the resources mentioned in Section 3, a good source of code\nand models is https://github.com/rbgirshick, one of the key authors in a long line of these object\n10\ndetection models.\n(Note though that there are many other popular implementations, such as https:\n//github.com/matterport/Mask_RCNN.) This in depth article towardsdatascience object detection Faster\nR-CNN oﬀers a detailed tutorial on downloading, setting up and training an object detection model, including\nhelpful pointers to data collection and annotation (the latter using https://rectlabel.com/.) Most recently\nthe Detectron2 system https://github.com/facebookresearch/detectron2 [248] builds on Mask R-CNN\nand oﬀers many varied image task functionalities.\nScientiﬁc Examples:\nObject detection has also gained signiﬁcant attention across diﬀerent scientiﬁc\napplications. It has been used in many medical settings to localize features of interest, for example, tumor\ncells across diﬀerent imaging modalities [125, 269] or fractures in radiology [199, 227].\n4.3.3\nSemantic Segmentation and Instance Segmentation\nSegmentation dives into the lowest possible level of detail — categorizing every single image pixel. In semantic\nsegmentation, we want to categorize pixels according to the high level group they belong to. For example,\nsuppose we are given an image of a street, with a road, diﬀerent vehicles, pedestrians, etc. We would like\nto determine if a pixel is part of any pedestrian, part of any vehicle or part of the road — i.e. label the\nimage pixels as either pedestrian, vehicle or road. Instance segmentation is even more intricate, where not\nonly do we want to categorize each pixel in this way, but do so separately for each instance (and provide\ninstance speciﬁc bounding boxes like in object detection). The diﬀerences are illustrated in Figure 3 (sourced\nfrom [1].) Returning to the example of the image of the street, suppose the image has three pedestrians. In\nsemantic segmentation, all of the pixels making up these three pedestrians would fall under the same category\n– pedestrian. In instance segmentation, these pixels would be further subdivided into those belonging to\npedestrian one, pedestrian two or pedestrian three.\nBecause segmentation models must categorize every pixel, their output is not just a single class label, or a\nbounding box, but a full image. As a result, the neural network architectures for segmentation have a slightly\ndiﬀerent structure that helps them better preserve spatial information about the image. A highly popular\nand successful architecture, particularly for scientiﬁc applications, has been the U-net [196], which also has a\n3d volumetric variant [33]. Other architectures include FCNs (Fully Convolutional Networks) [136], SegNet\n[9] and the more recent Object Contextual Representations [260]. A couple of nice surveys on semantic\nsegmentation methods are given by towardsdatascience Semantic Segementation with Deep Learning and\nhttps://sergioskar.github.io/Semantic_Segmentation/.\nFor instance segmentation, Mask R-CNN [78] and its variants [248] have been extremely popular. This\ntutorial Mask R-CNN tutorial with code provides a step by step example application. The recent Detectron2\npackage [248] (https://github.com/facebookresearch/detectron2) also oﬀers this functionality.\nScientiﬁc Examples:\nOut of all of the diﬀerent types of imaging prediction problems, segmentation\nmethods have been especially useful for (bio)medical applications. Examples include segmenting brain MR\nimages [156, 236], identifying key regions of cells in diﬀerent tissues [254, 217] and even studying bone\nstructure [129].\n4.3.4\nSuper-Resolution\nSuper resolution is a technique for transforming low resolution images to high resolution images. This problem\nhas been tackled both using convolutional neural networks and supervised learning, as well as generative\nmodels.\nSuper resolution formally deﬁned is an underdetermined problem, as there may be many possible\nhigh resolution mappings for a low resolution image.\nTraditional techniques imposed constraints such\n11\nas sparsity to ﬁnd a solution.\nOne of the ﬁrst CNNs for super resolution, SRCNN [50] outlines the\ncorrespondences between sparse coding approaches and convolutional neural networks.\nMore recently,\nResidual Dense Networks [270] have been a popular approach for super-resolution on standard benchmarks\n(with code available https://github.com/yulunzhang/RDN), as well as Predictive Filter Flow [114], (code:\nhttps://github.com/aimerykong/predictive-filter-flow) which has also looked at image denoising and\ndeblurring. In some of the scientiﬁc applications below, U-nets have also been successful for super resolution.\nScientiﬁc Examples:\nSuper resolution is arguably even more useful for scientiﬁc settings than standard\nnatural image benchmarks. Two recent papers look at U-nets for super-resolution of ﬂuorescence microscopy\n[245] (code: https://csbdeep.bioimagecomputing.com/) and electron microscopy [56]. Other examples\ninclude super resolution of chest CT scans [231] and Brain MRIs [31].\n4.3.5\nImage Registration\nImage registration considers the problem of aligning two input images to each other. Particularly relevant to\nscientiﬁc applications, the two input images might be from diﬀerent imaging modalities (e.g. a 3D scan and a\n2D image), or mapping a moving image to a canonical template image (such as in MRIs.) The alignment\nenables better identiﬁcation and analysis of features of interest.\nThe potential of image registration is primarily demonstrated through diﬀerent scientiﬁc applications. At\nthe heart of the technique is a convolutional neural network, often with an encoder-decoder structure (similar\nto the U-net [196]) to guide the alignment of two images. Note that while this underlying model is trained\nthrough supervised learning, many registration methods do not require explicit labels, using similarity functions\nand smoothness constraints to provide supervision. For example, [12] develop an unsupervised method to\nperform alignment for Brain MRIs. The code for this and several followup papers [13, 39] provides a helpful\nexample for building oﬀof and applying these methods https://github.com/voxelmorph/voxelmorph.\nOther useful resources include https://github.com/ankurhanda/gvnn (with corresponding paper [75]) a\nlibrary for learning common parametric image transformations.\n4.3.6\nPose Estimation\nPose estimation, and most popularly human pose estimation, studies the problem of predicting the pose of\na human in a given image. In particular, a deep neural network model is trained to identify the location\nof the main joints, the keypoints (e.g. knees, elbows, head) of the person in the image. These predictions\nare combined with existing body models to get the full stick-ﬁgure-esque output summarizing the pose. (See\nFigure 4, sourced from [218], for an illustration.)\n(2D) Human pose estimation is a core problem in computer vision with multiple benchmark datasets, and\nhas seen numerous convolutional architectures developed to tackle it. Some of the earlier models include\na multi-stage neural network introduced by [244], and a stacked hourglass model [158] that alternatingly\ncombines high and low resolutions of the intermediate representations. More recently, HRNet [218], which\nkeeps a high resolution representation throughout the model is a top performing architecture (code at\nhttps://github.com/leoxiaobin/deep-high-resolution-net.pytorch). Also of interest might be [24]\nprovides an end-to-end system for multiperson pose detection in the corresponding code repository https:\n//github.com/CMU-Perceptual-Computing-Lab/openpose.\nScientiﬁc Examples:\nPose estimation has gained signiﬁcant interest in neuroscience settings, where videos\nof animals are recorded, and automatically predicting poses in the image can help identify important behaviors.\nAn example is given by [146, 147], with associated code http://www.mousemotorlab.org/deeplabcut.\n12\nFigure 4: Pose Estimation.\nImage source [218] The task of pose estimation, speciﬁcally multi-person 2D\n(human) pose-estimation is depicted in the ﬁgure. The neural network model predicts the positions of the main joints\n(keypoints), which are combined with a body model to get the stick-ﬁgure like approximations of pose overlaid on the\nmultiple humans in the image. Variants of these techniques have been used to study animal behaviors in scientiﬁc\nsettings.\n4.3.7\nOther Tasks with Convolutional Neural Networks\nIn the preceding sections, we have overviewed some of the most common tasks for which convolutional neural\nnetworks are used. However, there are many additional use cases of these models that we have not covered,\nincluding video prediction [57], action recognition [52] and style transfer [64]. We hope that the provided\nreferences and resources enable future investigation into some of these methods also.\n4.4\nGraph Neural Networks\nMany datasets, such as (social) network data and chemical molecules have a graph structure to them,\nconsisting of vertices connected by edges. An active area of research, graph neural networks, has looked at\ndeveloping deep learning methods to work well with this kind of data. The input graph consists of nodes v\nhaving some associated feature vector hv, and sometimes edges euv also having associated features zeuv. For\nexample, nodes v might correspond to diﬀerent atoms, and the edges euv to the diﬀerent kinds of chemical\nbonds between atoms. At a high level, most graph neural networks compute useful information from the\ndata by (i) using the feature vectors of the neighbors of each vertex v to compute information on the input\ngraph instance (ii) using this information to update the feature vector of v. This process, which respects the\nconnectivity of the graph, is often applied iteratively, with the ﬁnal output either at the vertex level (Are\nmeaningful vertex feature vectors computed?) or at the level of the full input graph (Is some global property\nof the entire graph correctly identiﬁed?)\nApplication Characteristics\nProblems where the data has an inherent graph structure, and the goal\nis to learn some function on this graph structure — either at the per vertex level or a global property of\nthe entire graph. There are also spatio-temporal graph neural networks — performing predictions on graph\n13\nstructures evolving over time.\nTechnical References\nAlthough most graph neural networks follow the high level structure of aggregating\ninformation from vertex neighbors and using this information to update feature vectors, there are many many\ndiﬀerent architectural variants, with connections to other neural network models such as convolutional nets\nand recurrent models. Recent work has also looked at spatio-temporal graph networks for problems like action\nrecognition in video [124]. A nice uniﬁcation of many of the ﬁrst popular methods, such as [53, 15, 127], is\ngiven by [67]. A more recent survey paper [250], provides an extremely comprehensive overview of the diﬀerent\nkinds of architectures, problems, benchmark datasets and open source resources. Some useful code repositories\ninclude https://github.com/rusty1s/pytorch_geometric, https://github.com/deepmind/graph_nets\nand https://github.com/dmlc/dgl, which together cover most of the popular deep learning frameworks.\nScientiﬁc Examples\nGraph neural networks have been very popular for several chemistry tasks, such as\npredicting molecular properties [53, 93, 67, 103], determining protein interfaces [60, 229] and even generating\ncandidate molecules [41, 21]. A useful library for many of these chemistry tasks is https://github.com/\ndeepchem, which also has an associated benchmark task [249].\nA detailed tutorial of diﬀerent graph\nneural networks and their use in molecule generation can be seen at https://www.youtube.com/watch?v=\nVXNjCAmb6Zw.\n4.5\nNeural Networks for Sequence Data\nA very common attribute for data is to have a sequential structure. This might be frames in a video, amino\nacid sequences for a protein or words in a sentence. Developing neural network models to work with sequence\ndata has been one of the most extensive areas of research in the past few years. A large fraction of this has\nbeen driven by progress on tasks in natural language processing, which focuses on getting computers to work\nwith the language used by people to communicate. Two popular tasks in this area, which have seen signiﬁcant\nadvances, have been machine translation — developing deep learning models to translate from one language\nto another and question answering — taking as input a (short) piece of text and answering a question about\nit. In the following sections, we ﬁrst overview some of the main NLP tasks that have driven forward sequence\nmodelling and then the neural network models designed to solve these tasks.\n4.5.1\nLanguage Modelling (Next Token Prediction)\nLanguage modelling is a training method where the deep learning model takes as input the tokens of the\nsequence up to time/position t, and then uses these to predict token t + 1. This is in fact a self-supervised\ntraining method (see Section 6), where the data provides a natural set of labels without additional labelling\nneeded. In the NLP context, the neural network is fed in a sequence of words, corresponding to a sentence or\npassage of text, and it tries to predict the next word. For example, given a sentence, \"The cat sat on the roof\",\nthe network would ﬁrst be given as input \"The\" and asked to predict \"cat\", then be fed in \"The cat\" and asked\nto predict \"sat\", and so on. (There are some additional details in implementation, but this is the high level idea.)\nBecause of the easy availability of data/labels, and the ability to use language modelling at diﬀerent levels —\nfor words and even for characters, it has been a popular benchmark in natural language, and also for capturing\nsequence dependencies in scientiﬁc applications, such as protein function prediction [77, 80], and using the\nhidden representations as part of a larger pipeline for protein structure prediction in AlphaFold [205] (with\nopensourced code https://github.com/deepmind/deepmind-research/tree/master/alphafold_casp13.)\n14\nFigure 5: Illustration of the Sequence to Sequence prediction task. Image source [267] The ﬁgure shows\nan illustration of a Sequence to Sequence task, translating an input sentence (sequence of tokens) in English to an\noutput sentence in German. Note the encoder-decoder structure of the underlying neural network, with the encoder\ntaking in the input, and the decoder generating the output, informed by the encoder representations and the previously\ngenerated output tokens. In this ﬁgure, the input tokens are fed in one by one, and the output is also generated one\nat a time, which is the paradigm when using Recurrent Neural Networks as the underlying model. With Transformer\nmodels, which are now extremely popular for sequence to sequence tasks, the sequence is input all at once, signiﬁcantly\nspeeding up use.\n4.5.2\nSequence to Sequence\nAnother very popular task for sequence data is sequence to sequence — transforming one sequence to another.\nThis is precisely the setup for machine translation, where the model gets an input sentence (sequence) in say\nEnglish, and must translate it to German, which forms the output sentence (sequence). Some of the ﬁrst\npapers framing this task and tackling it in this way are [10, 221, 234]. Sequence to sequence tasks typically\nrely on neural network models that have an encoder-decoder structure, with the encoder neural network\ntaking in the input sequence and learning to extract the important features, which is then used by the decoder\nneural network to produce the target output. Figure 5(sourced from [267]) shows an example of this. This\nparadigm has also found some scientiﬁc applications as varied as biology [23] and energy forcasting [145].\nSequence to sequence models critically rely on a technique called attention, which we overview below. For\nmore details on this task, we recommend looking at some of the tutorials and course notes highlighted in\nSection 3.\n4.5.3\nQuestion Answering\nOne other popular benchmark for sequence data has been question answering. Here, a neural network model\nis given a paragraph of text (as context) and a speciﬁc question to answer on this context as input. It must\nthen output the part of the paragraph that answers the question. Some of the standard benchmarks for this\ntask are [83, 186], with http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture10-QA.pdf\nproviding an excellent overview of the tasks and common methodologies. Question answering critically relies\non the neural network model understanding the relevance and similarity of diﬀerent sets of sequences (e.g.\nhow relevant is this part of the context to the question of interest?). This general capability (with appropriate\nreformulation) has the potential to be broadly useful, both for determining similarity and relevance on other\ndatasets, and for question answering in specialized domains [61].\n15\nFigure 6: Diagram of a Recurrent Neural Network model, speciﬁcally a LSTM (Long-Short Term\nNetwork). Image source [163] The ﬁgure illustrates an LSTM network, a type of Recurrent Neural Network. We\nsee that the input xt at each timestep also inform the internal network state in the next timestep (hence a recurrent\nneural network) through a gating mechanism. This gating mechanism is called an LSTM, and consists of sigmoid and\ntanh functions, which transform and recombine the input for an updated internal state, and also emit an output. The\nmechanics of this gating process are shown in the middle cell of the ﬁgure.\n4.5.4\nRecurrent Neural Networks\nHaving seen some of the core tasks in deep learning for sequence data, these next few sections look at some\nof the key neural network models.\nRecurrent neural networks (RNNs) were the ﬁrst kind of deep learning model successfully used on many of\nthe aforementioned tasks. Their distinguishing feature, compared to CNNs or MLPs (which are feedforward\nneural networks, mapping input straight to output), is that there are feedback connections, enabling e.g. the\noutput at each timestep to become the input for the next timestep, and the preservation and modiﬁcation\nof an internal state across timesteps. When RNNs are used for sequential data tasks, sequences are input\ntoken by token, with each token causing an update of the internal cell state of the RNN, and also making the\nRNN emit a token output. Note that this enables these models to work with variable length data — often a\ndeﬁning characteristic of sequence data. How the input is processed, cell state updated and output emitted\nare controlled by gating functions — see the technical references!\nApplication Characteristics:\nProblems where the data has a sequential nature (with diﬀerent sequences\nof varying length), and prediction problems such as determining the next sequence token, transforming one\nsequence to another, or determining sequence similarities are important tasks.\nTechnical References:\nResearch on sequence models and RNNs has evolved dramatically in just the\npast couple of years. The most successful and popular kind of RNN is a bi-LSTM with Attention, where\nLSTM (Long-Short Term Memory) [88] refers to the kind of gating function that controls updates in the\nnetwork, bi refers to bidirectional (the neural network is run forwards and backwards on the sequence)\nand Attention is a very important technique that we overview separately below. (Some example papers\n[149, 150] and code resources https://github.com/salesforce/awd-lstm-lm.) This excellent post https:\n//colah.github.io/posts/2015-08-Understanding-LSTMs/ provides a great overview of RNNs and LSTMs\nin detail. (Figure 6 shows a diagram from the post revealing the details of the gating mechanisms in LSTMs.)\nThe post also describes a small variant of LSTMs, Gated Recurrent Units (GRUs) which are also popular in\npractice [127]. While RNNs (really bi-LSTMs) have been very successful, they are often tricky to develop and\ntrain, due to their recursiveness presenting challenges with optimization (the vanishing/exploding gradients\nproblem [87, 170, 76]), with performing fast model training (due to generating targets token by token), and\nchallenges learning long term sequential dependencies. A new type of feedforward neural network architecture,\nthe Transformer (overviewed below), was proposed to alleviate the ﬁrst two of these challenges.\n16\nFigure 7: Image of a couple of layers from a Transformer network. Image source [3] The ﬁgure depicts the\ncore sequence of layers that are fundamental to Transformer neural networks, a self-attention layer (sometimes called\na self-attention head) followed by fully connected layers. Note that when working with sequence data, transformers\ntake the entire input sequence all at once, along with positional information (in this case the input sequence being\n\"Thinking Machines\".)\nScientiﬁc Examples:\nRNNs have found several scientiﬁc applications for data with sequential structure,\nsuch as in genomics and proteomics [175, 132, 111].\n4.5.5\nAttention\nA signiﬁcant problem in using RNNs and working with sequential data is the diﬃculty in capturing long range\ndependencies. Long range dependencies are when tokens in the sequence that are very far apart from each\nother must be processed together to inform the correct output. RNNs process sequences in order, token by\ntoken, which means they must remember all of the important information from the earlier tokens until much\nlater in the sequence — very challenging as the memory of these architectures is far from perfect. Attention\n[32, 11] is a very important technique that introduces shortcut connections to earlier tokens, which alleviates\nthe necessity to remember important features for the duration of the entire sequence. Instead it provides a\ndirect way to model long term dependencies — the neural network has the ability to look back and attend to\nwhat it deems relevant information (through learning) earlier in the input. A very nice overview of attention is\nprovided by https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html. A variant\nof attention, self-attention, which can be used to help predictions on a single input sequence, is the core\nbuilding block of Transformer models.\n4.5.6\nTransformers\nWhile attention helped with challenges in long range dependencies, RNNs still remained slow to train and\ntricky to design (due to optimization challenges with vanishing/exploding gradients.) These challenges were\ninherent to their recurrent, token-by-token nature, prompting the proposal of a new feedforward neural\nnetwork to work with sequential data, the Transformer [233], which critically relies on attentional mechanisms\n(the paper is in fact titled Attention is All you Need.) During training transformers take in the entire\nsequence as input all at once, but have positional embeddings that respects the sequential nature of the data.\nTransformers have been exceptionally popular, becoming the dominant approach to many natural language\ntasks and sequential tasks.\n17\nApplication Characteristics:\nProblems where the data has a sequential nature and long range depen-\ndencies that need to be modelled. Given the large number of pretrained transformer models, they can also be\nvery useful in settings where pretrained models on standard benchmarks can be quickly adapted to the target\nproblem.\nTechnical References:\nThe original transformer paper [233] provides a nice overview of the motivations\nand the neural network architecture. The model was designed with machine translation tasks in mind, and so\nconsists of an encoder neural network and a decoder neural network. With transformers being adopted for\ntasks very diﬀerent to machine translation, the encoder and decoder are often used in stand-alone fashions\nfor diﬀerent tasks — for example, the encoder alone is used for question answering, while the decoder\nis important for text generation. Two very accessible step by step tutorials on the transformer are The\nAnnotated Transformer and The Illustrated Transformer. A nice example of some of the language modelling\ncapabilities of this models is given by [180].\nSince the development of the transformer, there has been considerable research looking at improving\nthe training of these models, adjusting the self-attention mechanism and other variants. A very important\nresult using the transformer has been BERT (Pretraining of deep Bi-directional Transformers for Language\nunderstanding) [43]. This paper demonstrates that performing transfer learning (see Section 5.1) using a\ntransformer neural network can be extremely successful for many natural language tasks. (Some of the ﬁrst\npapers showing the potential of transfer learning in this area were [92, 180], and since BERT, there have\nbeen followups which extend the model capabilities [257].) From a practical perspective, the development of\ntransformers, BERT and transfer learning mean that there are many resources available online for getting\nhold of code and pretrained models. We refer to some of these in Section 3, but of particular note is https:\n//github.com/huggingface/transformers which has an excellent library for transformer models. A good\noverview of BERT and transfer learning in NLP is given in http://jalammar.github.io/illustrated-bert/.\nScientiﬁc Examples:\nThere have been several interesting examples of transformers used in scientiﬁc\nsettings, such as training on protein sequences to ﬁnd representations encoding meaningful biological\nproperties [195], protein generation via language modelling [142], bioBERT [121] for text mining in biomedical\ndata (with pretrained model and training code), embeddings of scientiﬁc text [18] (with code https:\n//github.com/allenai/scibert) and medical question answering [237].\n4.5.7\nOther Tasks with Sequence Data\nIn the previous sections, we’ve given an overview of some of the important benchmark tasks for sequential data,\nand the types of deep learning models available to tackle them. As with convolutional networks, this is not a\ncomprehensive overview, but hopefully thorough enough to help with generating ideas on possible applications\nand oﬀering pointers to other useful related areas. A few other sequential data tasks that might be of interest\nare structured prediction, where the predicted output has some kind of structure, from tree structures (in e.g.\nparsing) [28, 246] to short, executable computer program structure [271] and summarization, where passages\nof text are summarized by a neural network [130, 273]. We’ll also discuss word embeddings later in the survey.\n4.6\nSection Summary\nIn this section, we have overviewed supervised learning, some of the core neural network models and the\nkinds of important tasks they can be used for. As previously discussed, these topics span an extremely\nlarge area of research, so there are some areas, e.g. deep neural networks for set structured data [262, 113],\nmodelling diﬀerent invariances — invariances to speciﬁed Lie groups for application to molecular property\nprediction [58], spherical invariances [35, 36] not covered. But we hope the material and references presented\nhelp inspire novel contributions to these very exciting and rapidly evolving research directions.\n18\nPretrained \nWeights: \nContain \nuseful \nfeatures\nStep 1: Pretraining\nTrain on diverse, generic \ntask, e.g. ImageNet\nRandomly \nInitialize\nStep 2: Finetuning\nTrain on target task\nFinal Model\nFigure 8: The Transfer Learning process for deep neural networks. Transfer learning is a two step process\nfor training a deep neural network. Instead of intializing parameters randomly and directly training on the target\ntask, we ﬁrst perform a pretraining step, on some diverse, generic task. This results in the neural network parameters\nconverging to a set of values, known as the pretrained weights. If the pretraining task is diverse enough, these pretrained\nweights will contain useful features that can be leveraged to learn the target task more eﬃciently. Starting from the\npretrained weights, we then train the network on the target task, known as ﬁnetuning, giving us the ﬁnal model.\n5\nKey (Supervised Learning) Methods\nIn the previous section we saw diﬀerent kinds of neural network models, and the many diﬀerent types of\ntasks they could be used for. To train the models for these tasks, we typically rely on the supervised learning\nmethodology — optimize model parameters to correctly output given labels (the supervision) on a set of\ntraining data examples.\nIn more detail, the standard supervised learning method for deep neural networks consists of (i) collecting\ndata instances (e.g. images) (ii) collecting labels for the data instances (e.g. is the image a cat or a dog)\n(iii) splitting the set of collected (data instance, label) into a training set, validation set and test set (iv)\nrandomly initializing neural network parameters (iv) optimizing parameters so the network outputs the\ncorrect corresponding label given an input data instance on the training set (v) further tuning and validating\non the validation and test sets.\nIn this section we overview methods that use variants of this process, for example initializing the neural\nnetwork parameters diﬀerently or dealing with shifts between the training data and the test sets. In Section\n6, we look at variants that reduce the dependence on collecting labels.\n5.1\nTransfer Learning\nThrough the preceding sections, we’ve made references to using pretrained models. This is in fact referring to\na very important method for training deep neural networks, known as transfer learning. Transfer learning is\na two step process for training a deep neural network model, a pretraining step, followed by a ﬁnetuning step,\nwhere the model in trained on the target task. More speciﬁcally, we take a neural network with parameters\n19\nrandomly initialized, and ﬁrst train it on a standard, generic task — the pretraining step. For example, in\nimage based tasks, a common pretraining task is ImageNet [42], which is an image classiﬁcation task on a\nlarge dataset of natural images. With an appropriate pretraining task that is generic and complex enough,\nthe pretraining step allows the neural network to learn useful features, stored in its parameters, which can\nthen be reused for the second step, ﬁnetuning. In ﬁnetuning, the pretrained neural network is further trained\n(with maybe some minor modiﬁcations to its output layer) on the true target task of interest. This process is\nillustrated in Figure 8. But being able to use the features it learned during pretraining often leads to boosts\nin performance and convergence speed of the target task, as well as needing less labelled data.\nBecause of these considerable beneﬁts, transfer learning has been extraordinarily useful in many settings,\nparticularly in computer vision [95], which had many early successful applications. As overviewed in Section\n4.5.6, the recent development of models like ULMFiT [92] and especially BERT [43] has also made transfer\nlearning extremely successful in natural language and sequential data settings, with recent work making the\ntransfer learning process even more eﬃcient [90, 201]. Most importantly, the ready availability of standard\nneural network architectures pretrained on standard benchmarks through many open sourced code repositories\non GitHub (examples given in Section 3) has meant that downloading and ﬁnetuning a standard pretrained\nmodel has become the de-facto standard for most new deep learning applications.\nTypically, performing transfer learning is an excellent way to start work on a new problem of interest.\nThere is the beneﬁt of using a well-tested, standard neural network architecture, aside from the knowledge\nreuse, stability and convergence boosts oﬀered by pretrained weights. Note however that the precise eﬀects of\ntransfer learning are not yet fully understood, and an active research area [116, 184, 266, 159, 143, 181, 235]\nlooks at investigating its exact properties. For transfer learning in vision [116, 266, 112] may be of particular\ninterest for their large scale studies and pretraining recommendations.\n5.2\nDomain Adaptation\nRelated to transfer learning is the task of domain adaptation. In (unsupervised) domain adaptation, we have\ntraining data and labels in a source domain, but want to develop a deep learning model that will also work\non a target domain, where the data instances may look diﬀerent to those in the source domain, but the high\nlevel task is the same. For instance, our source domain many consist of images of handwritten digits (zero to\nnine) which we wish to classify as the correct number. But the target domain many have photographs of\nhouse numbers (from zero to nine), that we also wish to classify as the correct number. Domain adaptation\ntechniques help build a model on the source domain that can also work (reasonably) well out-of-the-box on\nthe shifted target domain.\nThe most dominant approach to domain adaptation in deep learning is to build a model that can (i)\nperform well on the source domain task, and (ii) learns features that are as invariant to the domain shift\nas possible. This is achieved through jointly optimizing for both of these goals. Returning to our example\non handwritten digits and house number photographs, (i) corresponds to the standard supervised learning\nclassiﬁcation problem of doing well on the (source) task of identifying handwritten digits correctly while (ii)\nis more subtle, and typically involves explicitly optimizing for the hidden layer representations of handwritten\ndigits and house number photographs to look the same as each other — domain invariance. Some popular ways\nto implement this include gradient reversal [62], minimizing a distance function on the hidden representations\n[137], and even adversarial training [63, 211]. More recently, [219] look at using self-supervision (see Section\n6) to jointly train on the source and target domains, enabling better adaptation.\nOther approaches to domain adaptation include translating data instances from the source to the target\ndomain, and bootstrapping/co-training approaches (see Section 6.2). Some of these methods are overviewed\nin tutorials such as Deep Domain Adaptation in Computer Vision.\n20\n5.3\nMultitask Learning\nIn many supervised learning applications, ranging from machine translation [2] to scientiﬁc settings [187, 176],\nneural networks are trained in a multitask way – predicting several diﬀerent outputs for a single input. For\nexample, in image classiﬁcation, given an input medical image, we might train the network not only to\npredict a disease of interest, but patient age, history of other related disease, etc. This often has beneﬁcial\neﬀects even if there is only one prediction of interest, as it provides the neural network with useful additional\nfeedback that can guide it in learning the most important data features. (This can be so useful that sometimes\nauxiliary prediction targets are deﬁned solely for this purpose.) Additionally, the prediction of multiple\ntargets can mean that more data is available to train the model (only a subset of the data has the target\nlabels of interest, but many more data instances have other auxiliary labels.) The most extreme version\nof this is to simultaneously train on two entirely diﬀerent datasets. For example, instead of performing a\npretraining/ﬁnetuing step, the model could be trained on both ImageNet and a medical imaging dataset at\nthe same time.\nMultitask learning is usually implemented in practice by giving the neural network multiple heads. The\nhead of a neural network refers to its output layer, and a neural network with multiple heads has one head\nfor each predictive task (e.g. one head for predicting age, one for predicting the disease of interest) but\nshares all of the other features and parameters, across these diﬀerent predictive tasks. This is where the\nbeneﬁt of multitask learning comes from — the shared features, which comprise of most of the network, get\nmany diﬀerent sources of feedback. Implementing multitask learning often also requires careful choice of the\nway to weight the training objectives for these diﬀerent tasks. A nice survey of some popular methods for\nmultitask learning is given by https://ruder.io/multi-task/index.html#fn4, and a tutorial on some of\nthe important considerations in http://hazyresearch.stanford.edu/multi-task-learning. One package\nfor implementing multitask learning is found in https://github.com/SenWu/emmental and step-by-step\nexample with code excerpts in towardsdatascience Multitask Learning: teach your AI more to make it better.\n5.4\nWeak Supervision (Distant Supervision)\nSuppose it is very diﬃcult to collect high quality labels for the target task of interest, and neither is there an\nexisting, standard, related dataset and corresponding pretrained model to perform transfer learning from.\nHow might one provide the deep learning model with enough supervision during the training process? While\nhigh quality labels might be hard to obtain, noisy labels might be relatively easy to collect. Weak supervision\nrefers to the method of training a model on a dataset with these noisy labels (typically for future ﬁnetuning),\nwhere the noisy labels are often generated in an automatic process.\nIn computer vision (image based) tasks, some examples are: taking an image level label (for classiﬁcation)\nand automatically inferring pixel level labels for segmentation [171], clustering hidden representations\ncomputed by a pretrained network as pseudo-labels [255], or taking Instagram tags as labels [143] for\npretraining. In language tasks, examples are given by [153, 89, 264], which provide noisy supervision by\nassuming all sentences mentioning two entities of interest express a particular relation (also known as distant\nsupervision). A nice overview of weak supervision and its connection to other areas is given in https:\n//hazyresearch.github.io/snorkel/blog/ws_blog_post.html, with a related post looking speciﬁcally at\nmedical and scientiﬁc applications http://hazyresearch.stanford.edu/ws4science.\n5.5\nSection Summary\nIn this section, we have overviewed some of the central supervised learning based methodologies for developing\ndeep learning models. This is just a sampling of the broad collection of existing methods, and again, we\nhope that the descriptions and references will help facilitate further exploration of other approaches. One\nmethod not covered that might be of particular interest is multimodal learning, where neural networks are\n21\nUnlabelled Data \nInstances\nLabels from Pretext \nTask (Rotation)\n270੦\n180੦\n0੦\nTraining\n180੦\nFigure 9: Training neural networks with Self-Supervision.\nThe ﬁgure illustrates one example of a self-\nsupervision setup. In self-supervision, we typically have a collection of unlabelled data instances, in this case images.\nWe deﬁne a pretext task, that will automatically generate labels for the data instances. In this case, the pretext task is\nrotation — we randomly rotate the images by some amount and label them by the degree of rotation. During training,\nthe neural network is given this rotated image and must predict the degree of rotation. Doing so also requires the\nneural network learn useful hidden representations of the image data in general, so after training with self-supervision,\nthis neural network can then be successfully and eﬃciently ﬁnetuned on a downstream task.\nsimultaneously trained on data from diﬀerent modalities, such as images and text [139, 238, 102]. Multimodal\nlearning also provides a good example of the fact that it is often diﬃcult to precisely categorize deep learning\ntechniques as only being useful for a speciﬁc task or training regime. For example, we looked at language\nmodelling for sequence tasks in this supervised learning section, but language modelling is also an example of\nself-supervision (Section 6) and generative models (Section 8.1). There are many rich combinations of the\noutlined methods in both this section and subsequent sections, which can prove very useful in the development\nof an end to end system.\n6\nDoing More with Less Data\nSupervised learning methods, and speciﬁc variants such as transfer learning and multitask learning have\nbeen highly successful in training deep neural network models. However, a signiﬁcant limitation to their use,\nand thus the use of deep learning, is the dependence on large amounts of labelled data. In many specialized\ndomains, such as medicine, collecting a large number of high quality, reliable labels can be prohibitively\nexpensive.\nLuckily, in just the past few years, we’ve seen remarkable advances in methods that reduce this dependence,\nparticularly self-supervision and semi-supervised learning. These approaches still follow the paradigm of\ntraining a neural network to map raw data instances to a speciﬁed label, but critically, these labels are not\ncollected separately, but automatically deﬁned via a pretext task. For example, we might take a dataset of\nimages, rotate some of them, and then deﬁne the label as the degree of rotation, which is the prediction\ntarget for the neural network. This enables the use of unlabelled data in training the deep neural network. In\n22\nthis section, we cover both self-supervision and semi-supervised learning as well as other methods such as\ndata augmentation and denoising, all of which enable us to do more with less data.\n6.1\nSelf-Supervised Learning\nIn self-supervision, a pretext task is deﬁned such that labels can be automatically calculated directly from\nthe raw data instances. For example, on images, we could rotate the image by some amount, label it by\nhow much it was rotated, and train a neural network to predict the degree of rotation [66] — this setup is\nillustrated in Figure 9. This pretext task is deﬁned without needing any labelling eﬀort, but can be used\nto teach the network good representations. These representations can then be used as is or maybe with a\nlittle additional data for downstream problems. Arguably the biggest success of self-supervision has been\nlanguage modelling for sequential data and speciﬁcally natural language problems, which we overviewed in\nSection 4.5.1. Below we outline some of the most popular and successful self-supervision examples for both\nimage and sequential data. (A comprehensive list of self-supervision methods can also be found on this page\nhttps://github.com/jason718/awesome-self-supervised-learning.)\n6.1.1\nSelf-Supervised Learning for Images\nA recent, popular and simple self-supervised task for images is to predict image rotations [66]. Each image\ninstance is transformed with one of four possible rotations and the deep learning model must classify\nthe rotation correctly. Despite its simplicity, multiple studies have shown its success in learning good\nrepresentations [266, 265, 112]. Another popular method examined in those studies is exemplar [51], which\nproposes a self-supervision task relying on invariance to image transformations. For example, we might take\na source image of a cat, and perform a sequence of transformations, such as rotation, adjusting contrast,\nﬂipping the image horizontally, etc. We get multiple images of the cat by choosing many such sequences, and\ntrain the neural network to recognize these all as the same image.\nOther methods look at using image patches as context to learn about the global image structure and\nimportant features. For example, [48] deﬁnes a pretext task where the relative locations of pairs of image\npatches must be determined, while [161] teaches a neural network to solve jigsaw puzzles. This latter task\nhas been shown to be eﬀective at large scales [69], with nice implementations and benchmarking provided by\nhttps://github.com/facebookresearch/fair_self_supervision_benchmark. A recent line of work has\nlooked at using mutual information inspired metrics as a way to provide supervision on the relatedness of\ndiﬀerent image patches [84, 169, 7, 154], but these may be more intricate to implement. Many of these mutual\ninformation based metrics also rely on contrastive losses [30], which, at a high level, provides supervision\nto the network by making representations of a pair of similar inputs more similar than representations of a\npair of diﬀerent inputs. Very recently, a new self-supervision method, SimCLR [29], uses this to achieve high\nperformance (one implementation at https://github.com/sthalles/SimCLR.)\nNote that some of the image registration examples given in Section 4.3.5 are also examples of self-\nsupervised learning, where some kind of domain speciﬁc similarity function can be automatically computed\nto assess the quality of the output. Such approaches may be relevant to other domains, and are useful to\nexplore. A great set of open-sourced implementations of many of self-supervision methods is provided by\nhttps://github.com/google/revisiting-self-supervised.\n6.1.2\nSelf-Supervised Learning for Sequential (Natural Language) Data\nWhile research on self-supervision techniques for images has been extremely active, the strongest successes\nof this framework have arguably been with sequential data, particularly text and natural language. The\nsequential structure immediately gives rise to eﬀective self-supervision pretext tasks. Two dominant classes of\npretext tasks operate by either (i) using neighboring tokens of the sequence as input context for predicting a\n23\ntarget token (ii) taking in all tokens up to a particular position and predicting the next token. The latter of\nthese is language modelling, which was overviewed in Section 4.5.1. The former is the principle behind word\nembeddings.\nWord embeddings have been critical to solving many natural language problems. Before the recent\nsuccesses of full ﬂedged transfer learning in language (Section 5.1) this simple self-supervised paradigm was\nwhere knowledge reuse was concentrated, and formed a highly important component of any deep learning\nsystem for natural language (sequential) data. From a scientiﬁc perspective, learning word embeddings\nfor sequential data has the potential to identify previously unknown similarities in the data instances. It\nhas already found interesting uses in aiding with the automatic analysis of scientiﬁc texts, such as drug\nname recognition systems [131], biomedical named entity recognition [73], identifying important concepts in\nmaterials science [230] and even detecting chemical-protein interactions [37].\nThe key fundamental ideas of word embeddings are captured in the word2vec framework [152, 151], the\noriginal framework relying on either a Continuous-Bag-of-Words (CBOW) neural network or a Skip-Gram\nneural network.\nActually, both of these models are less neural networks and more two simple matrix\nmultiplications, with the ﬁrst matrix acting as a projection, and giving the desired embedding. In CBOW,\nthe context — deﬁned as the neighborhood words — are input, and the model must correctly identify the\ntarget output word. In Skip-Gram, this is reversed, with the center word being input, and the context\nbeing predicted. For example, given a sentence \"There is a cat on the roof\", with the target word being\ncat, CBOW would take in the vector representations of (There, is, a, on, the, roof) and output \"cat\", while\nSkip-Gram would roughly swap the inputs and outputs. The simplicity of these methods may make them\nmore suitable for many tasks compared to language modelling. Two nice overviews of the these methods are\ngiven by Introduction to Word Embeddings and word2vec, and https://ruder.io/word-embeddings-1/.\nOther embedding methods include [173, 123].\n6.1.3\nSelf-Supervision Summary\nIn this section we have outlined many of the interesting developments in self-supervised learning, a very\nsuccessful way to make use of unlabelled data to learn meaningful representations, either for analysis or other\ndownstream tasks. Self-supervision can be eﬀectively used along with other techniques. For example, in the\nlanguage modelling application, we saw it used for transfer learning (Section 5.1), where a deep learning\nmodel is ﬁrst pretrained using the language modelling self supervision objective, and then ﬁnetuned on the\ntarget task of interest. In the following section, we will other ways of combining self-supervision with labelled\ndata.\n6.2\nSemi-Supervised Learning\nWhile collecting large labelled datasets can be prohibitively expensive, it is often possible to collect a smaller\namount of labelled data. When assembling a brand new dataset, a typical situation is having a small amount of\nlabelled data and a (sometimes signiﬁcantly) larger number of data instances with no labels. Semi-supervised\nlearning looks at precisely this setting, proposing techniques that enable eﬀective learning on labelled and\nunlabelled data. Below we overview some of the popular methods for semi-supervised learning.\n6.2.1\nSelf-Supervision with Semi-Supervised Learning\nFollowing on from the previous section, one natural way to make use of the unlabelled data is to use a\nself-supervised pretext task. To combine this with the labelled data, we can design a neural network that has\ntwo diﬀerent outputs heads (exactly as in multitask learning, see Section 5.3), with one output head being\nused for the labelled data, and the other for the self-supervised objective on the unlabelled data. Importantly,\n24\nthis means that the features learned by the neural network are shared between the labelled and unlabelled\ndata, leading to better representations. This simple approach has been shown to be very eﬀective [266, 265].\n6.2.2\nSelf-Training (Bootstrapping)\nSelf-training, sometimes also referred to as bootstrapping or pseudo-labels, is an iterative method where a\ndeep neural network is ﬁrst developed in a supervised fashion on the labelled data. This neural network is\nthen used to provide (pseudo) labels to the unlabelled data, which can then be used in conjunction with the\nlabelled data to train a new, more accurate neural network. This approach often works well and can even be\nrepeated to get further improvements. There are a couple of common details in implementation — often\nwhen adding the neural network pseudo-labelled data, we only keep the most conﬁdently pseudo-labelled\nexamples. These pseudo-labelled examples may also be used for training with a diﬀerent objective function\ncompared to the labelled data. One of the early papers proposing this method was [120], with a more recent\npaper [252] demonstrating signiﬁcant successes at large scale. Other variants, including mean teacher [225],\ntemporal ensembling [119] and the recent MixMatch [19] also primarily use the self-training approach, but\nincorporate elements of consistency (see below). There are nice open sourced implementations of these methods,\nsuch as https://github.com/CuriousAI/mean-teacher for mean teacher and https://github.com/google-\nresearch/mixmatch and https://github.com/YU1ut/MixMatch-pytorch for MixMatch.\n6.2.3\nEnforcing Consistency (Smoothness)\nAn important theme in many semi-supervised methods has been to provide supervision on the unlabelled data\nthrough enforcing consistency. If a human was given two images A and B, where B was a slightly perturbed\nversion of A (maybe blurred, maybe some pixels obscured or blacked out), they would give these images\nthe same label — consistency. We can also apply this principle to provide feedback to our neural network\non the unlabelled data, combining it with the labelled data predictions as in multitask learning (Section\n5.3) to form a semi-supervised learning algorithm. A popular method on enforcing consistency is virtual\nadversarial training [155], which enforces consistency across carefully chosen image perturbations. Another\npaper, unsupervised data augmentation [251], uses standard data augmentation techniques such as cutout\n[44] for images and back translation for text [206] to perturb images and enforces consistency across them.\n[265] uses consistency constraints along with other semi-supervised and self-supervised techniques in its full\nalgorithm.\n6.2.4\nCo-training\nAnother way to provide feedback on unlabelled data is to train two (many) neural network models, each on a\ndiﬀerent view of the raw data. For example, with text data, each model might see a diﬀerent part of the\ninput sentence. These models can then be given feedback to be maximally consistent with each other, or with\na diﬀerent model which sees all of the data, or even used for self-training, with each diﬀerent model providing\npseudo labels on the instances it is most conﬁdent on. This post https://ruder.io/semi-supervised/\ngives a nice overview of diﬀerent co-training schemes, and [34, 179, 74] are some recent papers implementing\nthis in text and images.\n6.2.5\nSemi-Supervised Learning Summary\nSemi-supervised learning is a powerful way to reduce the need for labelled data and can signiﬁcantly boost the\neﬃcacy of deep learning models. Semi-supervised learning can be applied in any situation where a meaningful\ntask can be created on the unlabelled data. In this section we have overviewed some natural ways to deﬁne\nsuch tasks, but there may be many creative alternatives depending on the domain of interest. We hope the\nreferences will provide a helpful starting point for implementation and further exploration!\n25\nFigure 10: An illustration of the Mixup data augmentation technique. Image source [40] The ﬁgure\nprovides an example of the Mixup data augmentation method — an image of a cat and an image of a dog are linearly\ncombined, with 0.4 weight on the cat and 0.6 weight on the dog, to give a new input image shown in the bottom with\na smoothed label of 0.4 weight on cat and 0.6 weight on dog. Mixup has been a very popular and successful data\naugmentation method for image tasks.\n6.3\nData Augmentation\nAs depicted in Figure 1, data augmentation is an important part of the deep learning workﬂow. Data\naugmentation refers to the process of artiﬁcially increasing the size and diversity of the training data by\napplying a variety of transformations to the raw data instances. For example, if the raw instances were to\nconsist of images, we might artiﬁcially pad out the image borders and then perform an oﬀcenter (random)\ncrop to give us the ﬁnal augmented image instance. Aside from increasing the size and diversity of the\ndata, data augmentation oﬀers the additional beneﬁt of encouraging the neural network to be robust to\ncertain kinds of common transformations of data instances. In this section, we overview some of the most\npopular data augmentation techniques for image and sequential data. These techniques will typically already\nbe part of many open sourced deep learning pipelines, or easy to invoke in any mainstream deep learning\nsoftware package. There are also some speciﬁc libraries written for augmentations, for example imgaug\nhttps://github.com/aleju/imgaug, nlpaug https://github.com/makcedward/nlpaug and albumentations\nhttps://github.com/albumentations-team/albumentations.\n6.3.1\nData Augmentation for Image Data\nSimple augmentations for image data consider transformations such as horizontal ﬂips or random crops\n(padding the image borders and taking an oﬀcenter crop.) Inspired by these simple methods are two very\nsuccessful image augmentation strategies, cutout [44], which removes a patch from the input image, and\nRICAP [222], which combines patches from four diﬀerent input image to create a new image (with new label a\ncombination of the original labels.) This somewhat surprising latter technique of combining images has in fact\nshown to be very successful in mixup [268], another data augmentation strategy where linear combinations\nof images (instead of patches) are used. (This strategy has also been combined with cutout in the recently\nproposed cutmix augmentation strategy [261], with code https://github.com/clovaai/CutMix-PyTorch.)\nOther useful augmentation strategies include TANDA [188] which learns a model to compose data\naugmentations, the related randaugment [38], choosing a random subset of diﬀerent possible augmentations,\npopulation based augmentation [85] which randomly searches over diﬀerent augmentation policies, [91]\n26\napplying color distortions to the image and the recently proposed augmix [82] (code https://github.com/\ngoogle-research/augmix.)\n6.3.2\nData Augmentation for Sequence Data\nData augmentation for sequential data typically falls into either (i) directly modifying the input sequence, or\n(ii) in the case of sequence to sequence tasks (Section 4.5.2), increasing the number of input-output sequences\nthrough noisy translation with the neural network. When directly modifying the input sequence, common\nperturbations include randomly deleting a sequence token (comparable to the masking approach used in [43]),\nswapping sets of sequence tokens, and replacing a token with its synonym. This latter strategy is usually\nguided by word embeddings [239] or contextualized word embeddings [110]. Examples of combining these\ntransformations are given by [243, 98], with code repositories such as https://github.com/makcedward/\nnlpaug providing some simple implementations.\nThe other dominant approach to data augmentation of sequences is using sequence-to-sequence models to\ngenerate new data instances, known as back-translation [206, 54]. Concretely, suppose we have a model to\ntranslate from English sequences to German sequences. We can take the output German sequence and use\nexisting tools/noisy heuristics to translate it back to English. This gives us an additional English-German\nsequence pair.\n6.4\nData (Image) Denoising\nWhen measuring and collecting high dimensional data, noise can easily be introduced to the raw instances,\nbe they images or single-cell data. As a result there has been signiﬁcant interest and development of deep\nlearning techniques to denoise the data. Many of these recent methods work even without paired noisy and\nclean data samples, and many be applicable in a broad range of settings. For instance, Noise2Noise [122] uses\na U-net neural network architecture to denoise images given multiple noisy copies. The recent Noise2Self [14]\n(with code: https://github.com/czbiohub/noise2self) frames denoising as a self-supervision problem,\nusing diﬀerent subsets of features (with assumed independent noise properties) to perform denoising, applying\nit to both images as well as other high dimensional data.\n7\nInterpretability, Model Inspection and Representation Analysis\nMany standard applications of deep learning (and machine learning more broadly) focus on prediction —\nlearning to output speciﬁc target values given an input. Scientiﬁc applications, on the other hand, are often\nfocused on understanding — identifying underlying mechanisms giving rise to observed patterns in the data.\nWhen applying deep learning in scientiﬁc settings, we can use these observed phenomena as prediction targets,\nbut the ultimate goal remains to understand what attributes give rise to these observations. For example,\nthe core scientiﬁc question might be on how certain amino acid sequences (encoding a protein) give rise to\nparticular kinds of protein function. While we might frame this as a prediction problem, training a deep\nneural network to take as input an amino acid sequence and output the predicted properties of the protein,\nwe would ideally like to understand how that amino acid sequence resulted in the observed protein function.\nTo answer these kinds of questions, we can turn to interpretability techniques. Interpretability methods\nare sometimes equated to a fully understandable, step-by-step explanation of the model’s decision process.\nSuch detailed insights can often be intractable, especially for complex deep neural network models. Instead,\nresearch in interpretability focuses on a much broader suite of techniques that can provide insights ranging\nfrom (rough) feature attributions — determining what input features matter the most, to model inspection —\ndetermining what causes certain neurons in the network to ﬁre. In fact, these two examples also provide a\nrough split in the type of interpretability method.\n27\nFigure 11: The output of SmoothGrad, a type of saliency map. Image source [215] The ﬁgure shows the\noriginal input image (left), raw gradients (middle), which are often too noisy for reliable feature attributions, and\nSmoothGrad (right), a type of saliency map that averages over perturbations to produce a more coherent feature\nattribution visualization the input. In particular, we can clearly see that the monument in the picture is important for\nthe model output.\nOne large set of methods (which we refer to as Feature Attribution and Per Example Interpretability)\nconcentrates on taking a speciﬁc input along with a trained deep neural network, and determining what\nfeatures of the input are most important. The other broad class of techniques looks at taking a trained\nmodel, and a set of inputs, to determine what diﬀerent parts of the network have learned (referred to as\nModel Inspection and Representational Analysis). This latter set of methods can be very useful in revealing\nimportant, hidden patterns in the data that the model has implicitly learned through being trained on the\npredictive task. For example, in [118], which looks at machine translation, representation analysis techniques\nare used to illustrate latent linguistic structure learned by the model. We overview both sets of methods\nbelow.\n7.1\nFeature Attribution and Per Example Interpretability\nWe start oﬀby overviewing some of the popular techniques used to provide feature attribution at a per\nexample level, answering questions such as which parts of an input image are most important for a particular\nmodel prediction. These techniques can be further subcategorized as follows:\n7.1.1\nSaliency Maps and Input Masks\nAt a high level, saliency maps take the gradient of the output prediction with respect to the input. This gives\na mask over the input, highlighting which regions have large gradients (most important for the prediction)\nand which have smaller gradients. First introduced by [213], there are many variants of saliency maps, such\nas Grad-CAM [204], SmoothGrad [215], IntGrad [220], which make the resulting feature attributions more\nrobust. These and other methods are implemented in https://github.com/PAIR-code/saliency. Note that\nwhile these methods can be extremely useful, their predictions are not perfect [105], and must be validated\nfurther.\nClosely related to these saliency methods is [166], which provides the ability to inspect the kinds of\nfeatures causing neurons across diﬀerent hidden layers to ﬁre. The full, interactive paper can be read at\nhttps://distill.pub/2018/building-blocks/ with code and tutorials available at https://github.com/\ntensorflow/lucid.\nMany other techniques look at computing some kind of input mask, several of them using deconvolutional\nlayers, ﬁrst proposed by [263] and built on by [106] and [20]. Other work looks at directly optimizing\nto ﬁnd a sparse mask that will highlight the most important input features [59] (with associated code\nhttps://github.com/jacobgil/pytorch-explain-black-box) or ﬁnding such a mask through an iterative\n28\nFigure 12: Visualization of the kinds of features hidden neurons have learned to detect. Image source\n[165] This ﬁgure, from [165], illustrates the result of optimizing inputs to show what features hidden neurons have\nlearned to recognize. In this example, the hidden neuron has learned to detect (especially) soccer balls, tennis balls,\nbaseballs, and even the legs of soccer players.\nalgorithm [25].\n7.1.2\nFeature Ablations and Perturbations\nRelated to some of masking approaches above, but with enough diﬀerences to categorize separately are\nseveral methods that isolate the crucial features of the input either by performing feature ablations or\ncomputing perturbations of the input and using these perturbations along with the original input to inform\nthe importance of diﬀerent features.\nArguably the most well known of the ablation based approaches is the notion of a Shapely value, ﬁrst\nintroduced in [207]. This estimates the importance of a particular feature x0 in the input by computing\nthe predictive power of a subset of input features containing x0 and averaging over all possible such\nsubsets. While Shapely values may be expensive to compute naively for deep learning, follow on work [140]\nhas proposed more eﬃcient (and expressive) variants, with highly popular opensourced implementation:\nhttps://github.com/slundberg/shap.\nThe shap opensourced implementation above also uniﬁes some related approaches that use perturbations\nto estimate feature values. One such approach is LIME [194], which uses multiple local perturbations\nto enable learning an interpretable local model. Another is DeepLIFT, which uses a reference input to\ncompare activation diﬀerences [210], and yet another approach, Layer-wise Relevance Propagation [6] looks\nat computing relevance scores in a layerwise manner.\nOther work performing ablations to estimate feature importance includes [275] (with code https://\ngithub.com/lmzintgraf/DeepVis-PredDiff), while [59], described in Section 7.1.1 has elements of using\ninput perturbations.\n7.2\nModel Inspection and Representational Analysis\nIn this second class of interpretability methods, the focus is on gaining insights not at a single input example\nlevel, but using a set of examples (sometimes implicitly through the trained network) to understand the\nsalient properties of the data. We overview some diﬀerent approaches below.\n7.2.1\nProbing and Activating Hidden Neurons\nA large class of interpretability methods looks at either (i) probing hidden neurons in the neural network —\nunderstanding what kinds of inputs it activates for (ii) directly optimizing the input to activate a hidden\nneuron. Both of these techniques can provide useful insights into what the neural network has chosen to pay\nattention to, which in turn corresponds to important properties of the data.\n29\nFigure 13: Clustering neural network hidden representations to reveal linguistic structures.\nImage\nsource [118] In work on analyzing multilingual translation systems [118], representational analysis techniques are used\nto compute similarity of neural network (Transformer) hidden representations across diﬀerent languages. Performing\nclustering on the result reveals grouping of diﬀerent language representations (each language a point on the plot)\naccording to language families, which aﬀect linguistic structure. Importantly, this analysis uses the neural network to\nidentify key properties of the underlying data, a mode of investigation that might be very useful in scientiﬁc domains.\nSeveral papers falls into the probing category [259, 272], with an especially thorough study given by\nNetwork Dissection [17]. Here here hidden neurons are categorized by the kinds of features they respond to.\nThe paper website http://netdissect.csail.mit.edu/ contains method details as well as links to the code\nand data.\nThe other broad category of methods take a neural network, ﬁx its parameters, and optimize the input\nto ﬁnd the kinds of features that makes some hidden neuron activate. There are several papers using this\napproach, but of particular note is Feature Visualization [165], with an interactive article and code at:\nhttps://distill.pub/2017/feature-visualization/. Followup work, Activation Atlases [26] (with page\nhttps://distill.pub/2019/activation-atlas/), does this across many diﬀerent concepts, providing a full\nmapping of the features learned by the neural network. More recently [164] has used this as a building block\nto further understand how certain computations are performed in a neural network. Also related is [104],\nwhich looks at ﬁnding linear combinations of hidden neurons that correspond to interpretable concepts.\n7.2.2\nDimensionality Reduction on Neural Network Hidden Representations\nIn many standard scientiﬁc settings, e.g. analyzing single cell data, dimensionality reduction methods such\nas PCA, t-SNE [141], UMAP [148] are very useful in revealing important factors of variation and critical\ndiﬀerences in the data subpopulations e.g. tumor cells vs healthy cells. Such methods can also be used on the\nhidden activations (over some input dataset) of a neural network. Through the process of being trained on\nsome predictive task, the neural network may implicitly learn these important data attributes in its hidden\nrepresentations, which can then be extracted through dimensionality reduction methods.\n30\n7.2.3\nRepresentational Comparisons and Similarity\nRelated to more standard approaches of dimensionality reduction and clustering, a line of work has studied\ncomparing hidden representations across diﬀerent neural network models. Early work applied matching\nalgorithms [126] with follow on approaches using canonical correlation analysis [183, 157] (with associated\ncode https://github.com/google/svcca.) This latter approach has been used to identify and understand\nmany representational properties in natural language applications [118, 16, 235] and even in modelling the\nmouse visual cortex as an artiﬁcial neural network [208]. Another recent technique uses a kernel based\napproach to perform similarity comparisons [115] (with code https://colab.sandbox.google.com/github/\ngoogle-research/google-research/blob/master/representation_similarity/Demo.ipynb.)\n7.3\nTechnical References\nThe preceding sections contain many useful pointers to techniques and associated open sourced code references.\nOne additional reference of general interest may be https://christophm.github.io/interpretable-ml-\nbook/ a fully open sourced book on interpretable machine learning. This focuses slightly more on more\ntraditional interpretability methods, but has useful overlap with some of the techniques presented above and\nmay suggest promising open directions.\n8\nAdvanced Deep Learning Methods\nThe methods and tasks overviewed in the survey so far — supervised learning, fundamental neural network\narchitectures (and their many diﬀerent tasks), diﬀerent paradigms like transfer learning as well as ways to\nreduce labelled data dependence such as self-supervision and semi-supervised learning — are an excellent set\nof ﬁrst approaches for any problem amenable to deep learning. In most such problems, these approaches will\nalso suﬃce in ﬁnding a good solution.\nOccasionally however, it might be useful to turn to more advanced methods in deep learning, speciﬁcally\ngenerative models and reinforcement learning. We term these methods advanced as they are often more\nintricate to implement, and may require speciﬁc properties of the problem to be useful, for example an\nexcellent environment model/simulator for reinforcement learning. We provide a brief overview of these\nmethods below.\n8.1\nGenerative Models\nAt a high level, generative modelling has two fundamental goals. Firstly, it seeks to model and enable\nsampling from high dimensional data distributions, such as natural images. Secondly, it looks to learn low(er)\ndimensional latent encodings of the data that capture key properties of interest.\nTo achieve the ﬁrst goal, generative models take samples of the high dimensional distribution as input, for\nexample, images of human faces, and learn some task directly on these data instances (e.g. encoding and\nthen decoding the instance or learning to generate synthetic instances indistinguishable from the given data\nsamples or generating values per-pixel using neighboring pixels as context). If generative modelling achieved\nperfect success at this ﬁrst goal, it would make it possible to continuously sample ‘free’ data instances! Such\nperfect success is extremely challenging, but the past few years has seen enormous progress in the diversity\nand ﬁdelity of samples from the data distribution.\nFor the second goal, learning latent encodings of the data with diﬀerent encoding dimensions correspond\nto meaningful factors of variation, having an explicit encoder-decoder structure in the model can be helpful in\nencouraging learning such representations. This is the default structure for certain kinds of generative models\n31\nFigure 14: Human faces generated from scratch by StyleGAN2. Image source [100] The ﬁgure shows\nmultiple human face samples from StyleGAN2 [100]. While perfectly modelling and capture full diversity of complex\ndata distributions like human faces remains challenging, the quality and ﬁdelity of samples from recent generative\nmodels is very high.\nsuch as variational autoencoders [109] but has also been adopted into other models, such as BigBiGAN [49],\na type of generative adversarial network. In the following sections we overview some of these main types of\ngenerative models.\n8.1.1\nGenerative Adversarial Networks\nArguably the most well known of all diﬀerent types of generative models, Generative Adversarial Networks,\ncommonly known as GANs, consist of two neural networks, a generator and a discriminator, which are pitted\nin a game against each other. The generator takes as input a random noise vector and tries to output samples\nthat look like the data distribution (e.g. synthesize images of peoples faces), while the discriminator tries to\ndistinguish between true samples of the data, and those synthesized by the generator. First proposed in [68],\nGANs have been an exceptionally popular research area, with the most recent variations, such as BigGAN\n[22] (code: https://github.com/ajbrock/BigGAN-PyTorch), BigBiGAN [49] and StyleGAN(2) [100] (code:\nhttps://github.com/NVlabs/stylegan2) able to generate incredibly realistic images.\nUnconditional GANs vs Conditional GANs\nThe examples given above are all unconditional GANs,\nwhere the data is generated with only a random noise vector as input.\nA popular and highly useful\nvariant are conditional GANs, where generation is conditioned on additional information, such as a label,\nor a ‘source’ image, which might be translated to a diﬀerent style. Examples include pix2pix [96] (code:\nhttps://phillipi.github.io/pix2pix/), cycleGAN [274], and applications of these to videos [27].\nGANs have found many scientiﬁc applications, from performing data augmentation in medical image\nsettings [65] to protein generation [193]. The ‘adversarial’ loss objective of GANs can make them somewhat\ntricky to train, and useful implementation advice is given in https://www.fast.ai/2019/05/03/decrappify/,\nand (for conditional GANs) is included in https://github.com/jantic/DeOldify.\n8.1.2\nVariational Autoencoders\nAnother type of generative model is given by the variational autoencoder, ﬁrst proposed by [109]. VAEs have\nan encoder decoder structure, and thus an explicit latent encoding, which can capture useful properties of\nthe data distribution. They also enable estimation of the likelihood of a sampled datapoint — the probability\nof its occurrence in the data distribution. VAEs have also been extremely popular, with many variations and\nextensions proposed [216, 99, 107, 71]. Because of the explicit latent encoding and the ability to estimate\nlikelihoods, they have also found use cases in various scientiﬁc settings, such as for modelling gene expression\nin single-cell RNA sequencing [138].\n32\n8.1.3\nAutoregressive Models\nYet another type of generative model is autoregressive models, which take in inputs sequentially and use\nthose to generate an appropriate output. For instance, such models may take in a sequence of pixel values\n(some of them generated at a previous timestep) and use these to generate a new pixel value for a speciﬁc\nspatial location. Autoregressive models such as PixelRNN [168], PixelCNN (and variants) [232, 200] and\nthe recently proposed VQ-VAE(2) [189] (code: https://github.com/rosinality/vq-vae-2-pytorch) oﬀer\nvery high generation quality.\n8.1.4\nFlow Models\nA relatively new class of generative models, ﬂow models, looks at performing generation using a sequence\nof invertible transformations, which enables the computation of exact likelihoods. First proposed in [46,\n47], performing an expressive but tractable sequence of invertible transformations is an active area of\nresearch [108, 86]. A nice introduction to normalizing ﬂows is given in this short video tutorial https:\n//www.youtube.com/watch?v=i7LjDvsLWCg&feature=youtu.be.\n8.2\nReinforcement Learning\nReinforcement learning has quite a diﬀerent framing to the techniques and methods introduced so far,\naiming to solve the sequential decision making problem. It is typically introduced with the notions of an\nenvironment and an agent. The agent can take a sequence of actions in the environment, each of which\naﬀect the environment state in some way, and also result in possible rewards (feedback) — ‘positive’ for good\nsequences of actions resulting in a ‘good’ state and ‘negative’ for bad sequences of actions leading to a ‘bad’\nstate. For example, in a game like chess, the state is the current position of all pieces in play (the game\nstate), an action the moving of a piece, with a good sequence of actions resulting in a win, a bad sequence of\nactions in a loss and the reward might be one or zero depending on having a win or loss respectively.\nWith this being the setup, the goal of reinforcement learning is to learn, through interaction with the\nenvironment, good sequences of actions (typically referred to as a policy). Unlike supervised learning, feedback\n(the reward) is typically given only after performing the entire sequence of actions. Speciﬁcally, feedback is\nsparse and time delayed. There are a variety of diﬀerent reinforcement learning use cases depending on the\nspeciﬁcs of the problem.\n8.2.1\nRL with an Environment Model/Simulator\nSome of the most striking results with RL, such as AlphaGoZero [212], critically use an environment\nmodel/simulator. In such a setting, a variety of learning algorithms [242, 203, 128] (some code: https:\n//github.com/openai/baselines) can help the agent learn a good sequence of actions, often through\nsimultaneously learning a value function — a function that determines whether a particular environment\nstate is beneﬁcial or not. Because the beneﬁt of an environment state may depend on the entire sequence of\nactions (some still in the future), RL is very important in properly assessing the value of the environment\nstate, through implicitly accounting for possible future actions. Combining value functions with traditional\nsearch algorithms has been a very powerful way to use RL, and may be broadly applicable to many domains.\nSpeciﬁcally, if developing a solution to the problem is multistep in nature, with even a noisy validation\npossible in simulation, using RL to learn a good value function and combining that with search algorithms\nmay lead to discovering new and more eﬀective parts of the search space. Approaches like these have gained\ntraction in considering RL applications to fundamental problems in both computer systems, with [144]\nproviding a survey and a new benchmark, and machine learning systems [174], in designing task-speciﬁc\nneural network models. The latter has recently also resulted in scientiﬁc use cases — designing neural\n33\nnetworks to emulate complex processes across astronomy, chemistry, physics, climate modelling and others\n[101].\n8.2.2\nRL without Simulators\nIn other settings, we don’t have access to an environment model/simulator, and may simply have records of\nsequences of actions (and the ensuing states and rewards). This is the oﬄine setting. In this case, we may still\ntry to teach an agent a good policy, using the observed sequences of actions/states/rewards in conjunction with\noﬀ-policy methods [209, 182, 134], but thorough validation and evaluation can be challenging. Evaluation in\noﬀ-policy settings often uses a statistical technique known as oﬀ-policy policy evaluation (example algorithms\ninclude [178, 133]). In robotics, reinforcement learning literature has looked at performing transfer learning\nbetween policies learned in simulation and policies learned on real data [198]. A thorough overview of deep\nreinforcement learning is given in http://rail.eecs.berkeley.edu/deeprlcourse/.\n9\nImplementation Tips\nIn this section, we highlight some useful tips for implementing these models.\nExplore Your Data\nBefore starting with steps in the learning phase (see Figure 1), make sure to perform\na thorough exploration of your data. What are the results of simple dimensionality reduction methods or\nclustering? Are the labels reliable? Is there imbalance amongst diﬀerent classes? Are diﬀerent subpopulations\nappropriately represented?\nTry Simple Methods\nWhen starting oﬀwith a completely new problem, it is useful to try the simplest\nversion possible. (It might even be worthwhile starting with no learning at all — how does the naive majority\nbaseline perform? For datasets with large imbalances, it may be quite strong!) If the dataset is very large, is\nthere some smaller subsampled/downscaled version that can be used for faster preliminary testing? What\nis the simplest model that might work well? How does a majority baseline perform? (This ties in settings\nwhere the data has class imbalance.) Does the model (as expected) overﬁt to very small subsets of the data?\nWhere possible, start with well tested models/tasks/methods\nWith the plethora of standard\nmodels (many of them pretrained), data augmentation, and optimization methods readily available (Section\n3), most new problems will be amenable to some standard set of these choices. Start with this! Debugging\nthe dataset and objective function associated with a new problem at the same time as debugging the neural\nnetwork model, task choice, optimization algorithm, etc is very challenging.\nAdditionally, many of the standard model/task/method choices are very well benchmarked, and exploring\nperformance in these settings is an excellent ﬁrst step in understanding the inherent challenges of the new\nproblem. Wherever possible, the easiest way to get starting with the learning phase is to clone an appropriate\ngithub repository that has the models and training code needed, and make the minimal edits needed to work\nwith the new dataset and objective function.\nFirst Steps in Debugging Poor Performance\nHaving put together an end-to-end system, you observe\nthat it is not performing well on the validation data. What is the reason? Before getting into more subtle\ndesign questions on hyperparameter choice (below), some ﬁrst things to look at might be (i) Is the model\noverﬁtting? If so, more regularization, data augmentation, early stopping, smaller model may help. (ii) Is\nthere a distribution shift between the training and validation data? (iii) Is the model underﬁtting? If so,\ncheck the optimization process by seeing if the model overﬁts when trained on a smaller subset of the training\n34\ndata. Test out a simpler task. Check for noise in the labels or data instances and for distribution shift. (iv)\nLook at the instances on which the model makes errors. Is there some pattern? For imbalanced datasets,\nloss function reweighting or more augmentation on the rarer classes can help. (v) How stable is the model\nperformance across multiple random reruns? (vi) What are gradient and intermediate representation norms\nthrough the training process?\nWhich hyperparameters matter most?\nA big challenge in improving deep learning performance is the\nmultitude of hyperparameters it is possible to change. In practice, some of the simplest hyperparameters\noften aﬀect performance the most, such as learning rate and learning rate schedule. Picking an optimizer\nwith subtleties such as weight decay correctly implemented can also be very important, see this excellent\narticle on a very popular optimizer, AdamW https://www.fast.ai/2018/07/02/adam-weight-decay/. It\nmight also be very useful to visualize the contributions to total loss from the main objective function vs\ndiﬀerent regularizers such as weight decay.\nOther hyperparameters that can be explored include batch size and data preprocessing, though if standard\nsetups are used for these, varying learning rate related hyperparameters is likely to be the ﬁrst most useful\naspect to explore. To test diﬀerent hyperparameter settings, it can be very useful to cross-validate: hold\nout a portion of the training data, train diﬀerent hyperparameter settings on the remaining data, pick\nwhichever hyperparameter setting does best when evaluated on the held out data, and then ﬁnally retrain\nthat hyperparameter setting on the full training dataset.\nValidate your model thoroughly!\nDeep learning models are notorious for relying on spurious correlations\nin the data to perform their predictions [8, 162, 247]. By spurious correlation, we mean features in the data\ninstances that happen to co-occur with a speciﬁc label, but will not result in a robust, generalizable model.\nFor example, suppose we have data from diﬀerent chest x-ray machines (corresponding to diﬀerent hospitals)\nthat we put together to train a deep learning model. It might be the case that one of these machines, so\nhappens to scan many sick patients. The deep learning model might then implicitly learn about the chest\nx-ray machine instead of the features of the illness. One of the best tests for ensuring the model is learning in\na generalizable way is to evaluate the model on data collected separately from the training data, which will\nintroduce some natural distribution shift and provide a more robust estimate of its accuracy. Some recent\ninteresting papers exploring these questions include [81, 190].\nRelatedly, deep neural networks will also pick up on any biases in the data, for example, learning to\npay attention to gender (a sensitive attribute) when made to predict age due to class imbalances leading to\nspurious correlations. This can pose signiﬁcant challenges for generalizable conclusions in scientiﬁc settings\nwhere data may be collected from one population, but the predictions must be accurate across all populations.\nIt is therefore important to perform postprocessing analysis on the model representations to identify the\npresence of such biases. A line of active research studies how to debias these representations [4, 241].\nImplementation References\nSome of the general design considerations when coming to implementation\n(along with factors aﬀecting larger scale deployment, not explored in this survey) are discussed in this overview\nhttps://github.com/chiphuyen/machine-learning-systems-design/blob/master/build/build1/consolidated.pdf.\nFor speciﬁc points on training and debugging deep learning systems, two excellent guides are given\nby http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf and http:\n//karpathy.github.io/2019/04/25/recipe/.\n35\n10\nConclusion\nAs the amount of data collected across many diverse scientiﬁc domains continues to increase in both sheer\namount and complexity, deep learning methods oﬀer many exciting possibilities for both fundamental\npredictive problems as well as revealing subtle properties of the underlying data generation process. In this\nsurvey, we overviewed many of the highly successful deep learning models, tasks and methodologies, with\nreferences to the remarkably comprehensive open-sourced resources developed by the community. We hope\nthat both the overviews and the references serve to accelerate applications of deep learning to many varied\nscientiﬁc problems!\n36\nAcknowledgements\nThe authors would like to thank Jon Kleinberg, Samy\nBengio, Yann LeCun, Chiyuan Zhang, Quoc Le, Arun\nChaganty, Simon Kornblith, Aniruddh Raghu, John\nPlatt, Richard Murray, Stu Feldman and Guy Gur-Ari\nfor feedback and comments on earlier versions.\nReferences\n[1] Waleed Abdulla. Splash of Color: Instance Segmen-\ntation with Mask R-CNN and TensorFlow, 2018.\nhttps://engineering.matterport.com/splash-\nof-color-instance-segmentation-with-mask-r-\ncnn-and-tensorflow-7c761e238b46.\n[2] Roee Aharoni, Melvin Johnson, and Orhan Firat.\nMassively multilingual neural machine translation.\narXiv preprint arXiv:1903.00089, 2019.\n[3] Jay\nAlammar.\nThe\nIllustrated\nTransformer,\n2018.\nhttp://jalammar.github.io/illustrated-\ntransformer/.\n[4] Mohsan Alvi, Andrew Zisserman, and Christoﬀer\nNellåker. Turning a blind eye: Explicit removal of\nbiases and variation from deep neural network em-\nbeddings. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 0–0, 2018.\n[5] Marios Anthimopoulos, Stergios Christodoulidis,\nLukas Ebner, Andreas Christe, and Stavroula\nMougiakakou.\nLung pattern classiﬁcation for in-\nterstitial lung diseases using a deep convolutional\nneural network. IEEE transactions on medical imag-\ning, 35(5):1207–1216, 2016.\n[6] Sebastian Bach, Alexander Binder, Grégoire Mon-\ntavon, Frederick Klauschen, Klaus-Robert Müller,\nand Wojciech Samek. On pixel-wise explanations for\nnon-linear classiﬁer decisions by layer-wise relevance\npropagation. PloS one, 10(7):e0130140, 2015.\n[7] Philip Bachman, R Devon Hjelm, and William Buch-\nwalter.\nLearning representations by maximizing\nmutual information across views. arXiv preprint\narXiv:1906.00910, 2019.\n[8] Marcus A Badgeley, John R Zech, Luke Oakden-\nRayner, Benjamin S Glicksberg, Manway Liu,\nWilliam Gale, Michael V McConnell, Bethany Per-\ncha, Thomas M Snyder, and Joel T Dudley. Deep\nlearning predicts hip fracture using confounding pa-\ntient and healthcare variables. npj Digital Medicine,\n2(1):31, 2019.\n[9] Vijay Badrinarayanan, Alex Kendall, and Roberto\nCipolla.\nSegnet: A deep convolutional encoder-\ndecoder architecture for image segmentation. IEEE\ntransactions on pattern analysis and machine intel-\nligence, 39(12):2481–2495, 2017.\n[10] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473, 2014.\n[11] Dzmitry\nBahdanau,\nJan\nChorowski,\nDmitriy\nSerdyuk, Philemon Brakel, and Yoshua Bengio.\nEnd-to-end attention-based large vocabulary speech\nrecognition. In 2016 IEEE international conference\non acoustics, speech and signal processing (ICASSP),\npages 4945–4949. IEEE, 2016.\n[12] Guha Balakrishnan, Amy Zhao, Mert R Sabuncu,\nJohn Guttag, and Adrian V Dalca. An unsuper-\nvised learning model for deformable medical image\nregistration. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n9252–9260, 2018.\n[13] Guha Balakrishnan, Amy Zhao, Mert R Sabuncu,\nJohn Guttag, and Adrian V Dalca. Voxelmorph: a\nlearning framework for deformable medical image\nregistration. IEEE transactions on medical imaging,\n2019.\n[14] Joshua Batson and Loic Royer. Noise2self: Blind\ndenoising by self-supervision.\narXiv preprint\narXiv:1901.11365, 2019.\n[15] Peter Battaglia, Razvan Pascanu, Matthew Lai,\nDanilo Jimenez Rezende, et al. Interaction networks\nfor learning about objects, relations and physics. In\nAdvances in neural information processing systems,\npages 4502–4510, 2016.\n[16] Anthony Bau,\nYonatan Belinkov,\nHassan Saj-\njad, Nadir Durrani, Fahim Dalvi, and James\nGlass. Identifying and controlling important neu-\nrons in neural machine translation. arXiv preprint\narXiv:1811.01157, 2018.\n[17] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Network dissection: Quanti-\nfying interpretability of deep visual representations.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6541–6549,\n2017.\n[18] Iz Beltagy, Arman Cohan, and Kyle Lo. Scibert:\nPretrained contextualized embeddings for scientiﬁc\ntext. arXiv preprint arXiv:1903.10676, 2019.\n[19] David Berthelot, Nicholas Carlini, Ian Goodfellow,\nNicolas Papernot, Avital Oliver, and Colin Raﬀel.\nMixmatch: A holistic approach to semi-supervised\nlearning. arXiv preprint arXiv:1905.02249, 2019.\n[20] Mariusz Bojarski, Anna Choromanska, Krzysztof\nChoromanski, Bernhard Firner, Larry Jackel, Urs\nMuller, and Karol Zieba. Visualbackprop: visual-\nizing cnns for autonomous driving. arXiv preprint\narXiv:1611.05418, 2, 2016.\n37\n[21] Xavier Bresson and Thomas Laurent. A two-step\ngraph convolutional decoder for molecule generation.\narXiv preprint arXiv:1906.03412, 2019.\n[22] Andrew Brock, JeﬀDonahue, and Karen Simonyan.\nLarge scale gan training for high ﬁdelity natural\nimage synthesis. arXiv preprint arXiv:1809.11096,\n2018.\n[23] Renzhi Cao, Colton Freitas, Leong Chan, Miao\nSun, Haiqing Jiang, and Zhangxin Chen. Prolango:\nprotein function prediction using neural machine\ntranslation based on a recurrent neural network.\nMolecules, 22(10):1732, 2017.\n[24] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei,\nand Yaser Sheikh. OpenPose: realtime multi-person\n2D pose estimation using Part Aﬃnity Fields. In\narXiv preprint arXiv:1812.08008, 2018.\n[25] Brandon Carter, Jonas Mueller, Siddhartha Jain,\nand David Giﬀord. What made you do this? under-\nstanding black-box decisions with suﬃcient input\nsubsets. arXiv preprint arXiv:1810.03805, 2018.\n[26] Shan Carter, Zan Armstrong, Ludwig Schubert, Ian\nJohnson, and Chris Olah. Activation atlas. Distill,\n2019. https://distill.pub/2019/activation-atlas.\n[27] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and\nAlexei A Efros.\nEverybody dance now.\nIn Pro-\nceedings of the IEEE International Conference on\nComputer Vision, pages 5933–5942, 2019.\n[28] Danqi Chen and Christopher Manning. A fast and\naccurate dependency parser using neural networks.\nIn Proceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP),\npages 740–750, 2014.\n[29] Ting Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoﬀrey Hinton. A simple framework for con-\ntrastive learning of visual representations. arXiv\npreprint arXiv:2002.05709, 2020.\n[30] Ting Chen, Yizhou Sun, Yue Shi, and Liangjie Hong.\nOn sampling strategies for neural network-based col-\nlaborative ﬁltering. In Proceedings of the 23rd ACM\nSIGKDD International Conference on Knowledge\nDiscovery and Data Mining, pages 767–776, 2017.\n[31] Yuhua Chen, Yibin Xie, Zhengwei Zhou, Feng Shi,\nAnthony G Christodoulou, and Debiao Li. Brain\nmri super resolution using 3d deep densely con-\nnected neural networks. In 2018 IEEE 15th Inter-\nnational Symposium on Biomedical Imaging (ISBI\n2018), pages 739–742. IEEE, 2018.\n[32] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy\nSerdyuk, Kyunghyun Cho, and Yoshua Bengio.\nAttention-based models for speech recognition. In\nAdvances in neural information processing systems,\npages 577–585, 2015.\n[33] Özgün\nÇiçek,\nAhmed\nAbdulkadir,\nSoeren\nS\nLienkamp, Thomas Brox, and Olaf Ronneberger.\n3d u-net: learning dense volumetric segmentation\nfrom sparse annotation. In International conference\non medical image computing and computer-assisted\nintervention, pages 424–432. Springer, 2016.\n[34] Kevin Clark, Minh-Thang Luong, Christopher D\nManning, and Quoc V Le. Semi-supervised sequence\nmodeling with cross-view training. arXiv preprint\narXiv:1809.08370, 2018.\n[35] Taco Cohen and Max Welling. Group equivariant\nconvolutional networks. In International conference\non machine learning, pages 2990–2999, 2016.\n[36] Taco S Cohen, Mario Geiger, Jonas Köhler, and\nMax Welling.\nSpherical cnns.\narXiv preprint\narXiv:1801.10130, 2018.\n[37] Peter Corbett and John Boyle. Improving the learn-\ning of chemical-protein interactions from literature\nusing transfer learning and specialized word embed-\ndings. Database, 2018, 2018.\n[38] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and\nQuoc V Le.\nRandaugment: Practical data aug-\nmentation with no separate search. arXiv preprint\narXiv:1909.13719, 2019.\n[39] Adrian Dalca, Marianne Rakic, John Guttag, and\nMert Sabuncu.\nLearning conditional deformable\ntemplates with convolutional networks. In Advances\nin neural information processing systems, pages 804–\n816, 2019.\n[40] Yann Dauphin. mixup: Beyond Empirical Risk Min-\nimization Image, 2017. https://www.dauphin.io/.\n[41] Nicola De Cao and Thomas Kipf.\nMolgan: An\nimplicit generative model for small molecular graphs.\narXiv preprint arXiv:1805.11973, 2018.\n[42] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai\nLi, and Li Fei-Fei. Imagenet: A large-scale hierar-\nchical image database. In 2009 IEEE conference\non computer vision and pattern recognition, pages\n248–255. Ieee, 2009.\n[43] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding.\narXiv preprint arXiv:1810.04805, 2018.\n[44] Terrance DeVries and Graham W Taylor. Improved\nregularization of convolutional neural networks with\ncutout. arXiv preprint arXiv:1708.04552, 2017.\n[45] Yiming Ding, Jae Ho Sohn, Michael G Kawczynski,\nHari Trivedi, Roy Harnish, Nathaniel W Jenkins,\nDmytro Lituiev, Timothy P Copeland, Mariam S\nAboian, Carina Mari Aparici, et al. A deep learning\nmodel to predict a diagnosis of alzheimer disease by\nusing 18f-fdg pet of the brain. Radiology, 290(2):456–\n464, 2018.\n38\n[46] Laurent Dinh, David Krueger, and Yoshua Bengio.\nNice: Non-linear independent components estima-\ntion. arXiv preprint arXiv:1410.8516, 2014.\n[47] Laurent Dinh, Jascha Sohl-Dickstein, and Samy\nBengio. Density estimation using real nvp. arXiv\npreprint arXiv:1605.08803, 2016.\n[48] Carl Doersch, Abhinav Gupta, and Alexei A Efros.\nUnsupervised visual representation learning by con-\ntext prediction.\nIn Proceedings of the IEEE In-\nternational Conference on Computer Vision, pages\n1422–1430, 2015.\n[49] JeﬀDonahue and Karen Simonyan.\nLarge scale\nadversarial representation learning.\nIn Advances\nin Neural Information Processing Systems, pages\n10541–10551, 2019.\n[50] Chao Dong, Chen Change Loy, Kaiming He, and\nXiaoou Tang. Image super-resolution using deep con-\nvolutional networks. IEEE transactions on pattern\nanalysis and machine intelligence, 38(2):295–307,\n2015.\n[51] Alexey Dosovitskiy, Jost Tobias Springenberg, Mar-\ntin Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with convolutional\nneural networks. In Advances in neural information\nprocessing systems, pages 766–774, 2014.\n[52] Yong Du, Wei Wang, and Liang Wang. Hierarchical\nrecurrent neural network for skeleton based action\nrecognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n1110–1118, 2015.\n[53] David K Duvenaud, Dougal Maclaurin, Jorge Ipar-\nraguirre, Rafael Bombarell, Timothy Hirzel, Alán\nAspuru-Guzik, and Ryan P Adams. Convolutional\nnetworks on graphs for learning molecular ﬁnger-\nprints. In Advances in neural information processing\nsystems, pages 2224–2232, 2015.\n[54] Sergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. Understanding back-translation at scale.\narXiv preprint arXiv:1808.09381, 2018.\n[55] Andre Esteva, Brett Kuprel, Roberto A Novoa,\nJustin Ko, Susan M Swetter, Helen M Blau, and\nSebastian Thrun. Dermatologist-level classiﬁcation\nof skin cancer with deep neural networks. Nature,\n542(7639):115, 2017.\n[56] Linjing Fang, Fred Monroe, Sammy Weiser Novak,\nLyndsey Kirk, Cara R Schiavon, B Yu Seungyoon,\nTong Zhang, Melissa Wu, Kyle Kastner, Yoshiyuki\nKubota, et al. Deep learning-based point-scanning\nsuper-resolution imaging.\nbioRxiv, page 740548,\n2019.\n[57] Chelsea Finn, Ian Goodfellow, and Sergey Levine.\nUnsupervised learning for physical interaction\nthrough video prediction. In Advances in neural\ninformation processing systems, pages 64–72, 2016.\n[58] Marc Finzi, Samuel Stanton, Pavel Izmailov, and\nAndrew Gordon Wilson.\nGeneralizing convolu-\ntional neural networks for equivariance to lie groups\non arbitrary continuous data.\narXiv preprint\narXiv:2002.12880, 2020.\n[59] Ruth C Fong and Andrea Vedaldi. Interpretable\nexplanations of black boxes by meaningful pertur-\nbation. In Proceedings of the IEEE International\nConference on Computer Vision, pages 3429–3437,\n2017.\n[60] Alex Fout, Jonathon Byrd, Basir Shariat, and Asa\nBen-Hur. Protein interface prediction using graph\nconvolutional networks. In Advances in Neural Infor-\nmation Processing Systems, pages 6530–6539, 2017.\n[61] Ferenc Galkó and Carsten Eickhoﬀ. Biomedical ques-\ntion answering via weighted neural network passage\nretrieval. In European Conference on Information\nRetrieval, pages 523–528. Springer, 2018.\n[62] Yaroslav Ganin and Victor Lempitsky. Unsuper-\nvised domain adaptation by backpropagation. arXiv\npreprint arXiv:1409.7495, 2014.\n[63] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, François Lavi-\nolette, Mario Marchand, and Victor Lempitsky.\nDomain-adversarial training of neural networks. The\nJournal of Machine Learning Research, 17(1):2096–\n2030, 2016.\n[64] Leon A Gatys, Alexander S Ecker, and Matthias\nBethge. Image style transfer using convolutional\nneural networks. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 2414–2423, 2016.\n[65] Amirata Ghorbani, Vivek Natarajan, David Coz,\nand Yuan Liu. Dermgan: Synthetic generation of\nclinical skin images with pathology. arXiv preprint\narXiv:1911.08716, 2019.\n[66] Spyros Gidaris, Praveer Singh, and Nikos Ko-\nmodakis.\nUnsupervised representation learning\nby predicting image rotations.\narXiv preprint\narXiv:1803.07728, 2018.\n[67] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley,\nOriol Vinyals, and George E Dahl. Neural message\npassing for quantum chemistry. In Proceedings of the\n34th International Conference on Machine Learning-\nVolume 70, pages 1263–1272. JMLR. org, 2017.\n[68] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversar-\nial nets. In Advances in neural information process-\ning systems, pages 2672–2680, 2014.\n[69] Priya Goyal, Dhruv Mahajan, Abhinav Gupta,\nand Ishan Misra. Scaling and benchmarking self-\nsupervised visual representation learning.\narXiv\npreprint arXiv:1905.01235, 2019.\n39\n[70] Alex Graves, Abdel-rahman Mohamed, and Geof-\nfrey Hinton. Speech recognition with deep recurrent\nneural networks. In 2013 IEEE international con-\nference on acoustics, speech and signal processing,\npages 6645–6649. IEEE, 2013.\n[71] Aditya Grover, Aaron Zweig, and Stefano Ermon.\nGraphite: Iterative generative modeling of graphs.\narXiv preprint arXiv:1803.10459, 2018.\n[72] Varun Gulshan, Lily Peng, Marc Coram, Martin C\nStumpe, Derek Wu, Arunachalam Narayanaswamy,\nSubhashini Venugopalan, Kasumi Widner, Tom\nMadams, Jorge Cuadros, et al. Development and val-\nidation of a deep learning algorithm for detection of\ndiabetic retinopathy in retinal fundus photographs.\nJama, 316(22):2402–2410, 2016.\n[73] Maryam Habibi, Leon Weber, Mariana Neves,\nDavid Luis Wiegandt, and Ulf Leser. Deep learning\nwith word embeddings improves biomedical named\nentity recognition. Bioinformatics, 33(14):i37–i48,\n2017.\n[74] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu,\nMiao Xu, Weihua Hu, Ivor Tsang, and Masashi\nSugiyama. Co-teaching: Robust training of deep\nneural networks with extremely noisy labels.\nIn\nAdvances in neural information processing systems,\npages 8527–8537, 2018.\n[75] Ankur Handa, Michael Bloesch, Viorica Pătrăucean,\nSimon Stent, John McCormac, and Andrew Davi-\nson. gvnn: Neural network library for geometric\ncomputer vision. In European Conference on Com-\nputer Vision, pages 67–82. Springer, 2016.\n[76] Boris Hanin. Which neural net architectures give rise\nto exploding and vanishing gradients? In Advances\nin Neural Information Processing Systems, pages\n582–591, 2018.\n[77] Jack Hanson, Yuedong Yang, Kuldip Paliwal, and\nYaoqi Zhou. Improving protein disorder prediction\nby deep bidirectional long short-term memory recur-\nrent neural networks. Bioinformatics, 33(5):685–692,\n2016.\n[78] Kaiming He, Georgia Gkioxari, Piotr Dollár, and\nRoss Girshick. Mask r-cnn. In Proceedings of the\nIEEE international conference on computer vision,\npages 2961–2969, 2017.\n[79] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778, 2016.\n[80] Rhys Heﬀernan, Yuedong Yang, Kuldip Paliwal,\nand Yaoqi Zhou. Capturing non-local interactions\nby long short-term memory bidirectional recurrent\nneural networks for improving prediction of pro-\ntein secondary structure, backbone angles, contact\nnumbers and solvent accessibility. Bioinformatics,\n33(18):2842–2849, 2017.\n[81] Dan Hendrycks and Thomas Dietterich.\nBench-\nmarking neural network robustness to common\ncorruptions and perturbations.\narXiv preprint\narXiv:1903.12261, 2019.\n[82] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret\nZoph, Justin Gilmer, and Balaji Lakshminarayanan.\nAugmix: A simple data processing method to im-\nprove robustness and uncertainty. arXiv preprint\narXiv:1912.02781, 2019.\n[83] Karl Moritz Hermann, Tomas Kocisky, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines\nto read and comprehend. In Advances in neural\ninformation processing systems, pages 1693–1701,\n2015.\n[84] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-\nMarchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. Learning deep repre-\nsentations by mutual information estimation and\nmaximization.\narXiv preprint arXiv:1808.06670,\n2018.\n[85] Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel,\nand Xi Chen. Population based augmentation: Ef-\nﬁcient learning of augmentation policy schedules.\narXiv preprint arXiv:1905.05393, 2019.\n[86] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan\nDuan, and Pieter Abbeel.\nFlow++: Improving\nﬂow-based generative models with variational de-\nquantization and architecture design. arXiv preprint\narXiv:1902.00275, 2019.\n[87] Sepp Hochreiter. The vanishing gradient problem\nduring learning recurrent neural nets and problem so-\nlutions. International Journal of Uncertainty, Fuzzi-\nness and Knowledge-Based Systems, 6(02):107–116,\n1998.\n[88] Sepp Hochreiter and Jürgen Schmidhuber. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780, 1997.\n[89] Raphael Hoﬀmann, Congle Zhang, Xiao Ling, Luke\nZettlemoyer, and Daniel S Weld. Knowledge-based\nweak supervision for information extraction of over-\nlapping relations. In Proceedings of the 49th Annual\nMeeting of the Association for Computational Lin-\nguistics: Human Language Technologies-Volume 1,\npages 541–550. Association for Computational Lin-\nguistics, 2011.\n[90] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzeb-\nski, Bruna Morrone, Quentin De Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\n40\nParameter-eﬃcient transfer learning for nlp. arXiv\npreprint arXiv:1902.00751, 2019.\n[91] Andrew G Howard. Some improvements on deep con-\nvolutional neural network based image classiﬁcation.\narXiv preprint arXiv:1312.5402, 2013.\n[92] Jeremy Howard and Sebastian Ruder. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146, 2018.\n[93] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka\nZitnik, Percy Liang, Vijay Pande, and Jure Leskovec.\nPre-training graph neural networks. arXiv preprint\narXiv:1905.12265, 2019.\n[94] Gao Huang, Zhuang Liu, Laurens Van Der Maaten,\nand Kilian Q Weinberger. Densely connected con-\nvolutional networks. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4700–4708, 2017.\n[95] Minyoung Huh, Pulkit Agrawal, and Alexei A Efros.\nWhat makes imagenet good for transfer learning?\narXiv preprint arXiv:1608.08614, 2016.\n[96] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and\nAlexei A Efros. Image-to-image translation with\nconditional adversarial networks. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 1125–1134, 2017.\n[97] Na Ji.\nAdaptive optical ﬂuorescence microscopy.\nNature methods, 14(4):374, 2017.\n[98] Robin Jia and Percy Liang.\nData recombina-\ntion for neural semantic parsing.\narXiv preprint\narXiv:1606.03622, 2016.\n[99] Matthew Johnson,\nDavid K Duvenaud,\nAlex\nWiltschko, Ryan P Adams, and Sandeep R Datta.\nComposing graphical models with neural networks\nfor structured representations and fast inference. In\nAdvances in neural information processing systems,\npages 2946–2954, 2016.\n[100] Tero Karras, Samuli Laine, and Timo Aila. A style-\nbased generator architecture for generative adversar-\nial networks. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n4401–4410, 2019.\n[101] MF Kasim, D Watson-Parris, L Deaconu, S Oliver,\nP Hatﬁeld, DH Froula, G Gregori, M Jarvis, S Khati-\nwala, J Korenaga, et al. Up to two billion times accel-\neration of scientiﬁc simulations with deep neural ar-\nchitecture search. arXiv preprint arXiv:2001.08055,\n2020.\n[102] Jeremy Kawahara, Sara Daneshvar, Giuseppe Argen-\nziano, and Ghassan Hamarneh. Seven-point checklist\nand skin lesion classiﬁcation using multitask multi-\nmodal neural nets. IEEE journal of biomedical and\nhealth informatics, 23(2):538–546, 2018.\n[103] Steven Kearnes, Kevin McCloskey, Marc Berndl,\nVijay Pande, and Patrick Riley. Molecular graph\nconvolutions: moving beyond ﬁngerprints. Journal\nof computer-aided molecular design, 30(8):595–608,\n2016.\n[104] Been Kim, Martin Wattenberg, Justin Gilmer, Car-\nrie Cai, James Wexler, Fernanda Viegas, and Rory\nSayres. Interpretability beyond feature attribution:\nQuantitative testing with concept activation vectors\n(tcav). arXiv preprint arXiv:1711.11279, 2017.\n[105] Pieter-Jan Kindermans, Sara Hooker, Julius Ade-\nbayo, Maximilian Alber, Kristof T Schütt, Sven\nDähne, Dumitru Erhan, and Been Kim. The (un)\nreliability of saliency methods. In Explainable AI:\nInterpreting, Explaining and Visualizing Deep Learn-\ning, pages 267–280. Springer, 2019.\n[106] Pieter-Jan Kindermans, Kristof T Schütt, Maxim-\nilian Alber, Klaus-Robert Müller, Dumitru Erhan,\nBeen Kim, and Sven Dähne. Learning how to explain\nneural networks: Patternnet and patternattribution.\narXiv preprint arXiv:1705.05598, 2017.\n[107] D Kingma, Tim Salimans, R Josefowicz, Xi Chen,\nIlya Sutskever, Max Welling, et al. Improving varia-\ntional autoencoders with inverse autoregressive ﬂow.\n2017.\n[108] Durk P Kingma and Prafulla Dhariwal. Glow: Gen-\nerative ﬂow with invertible 1x1 convolutions. In\nAdvances in Neural Information Processing Systems,\npages 10215–10224, 2018.\n[109] Durk P Kingma, Shakir Mohamed, Danilo Jimenez\nRezende, and Max Welling. Semi-supervised learn-\ning with deep generative models. In Advances in\nneural information processing systems, pages 3581–\n3589, 2014.\n[110] Sosuke Kobayashi. Contextual augmentation: Data\naugmentation by words with paradigmatic relations.\narXiv preprint arXiv:1805.06201, 2018.\n[111] Kaname Kojima, Shu Tadaka, Fumiki Katsuoka,\nGen Tamiya, Masayuki Yamamoto, and Kengo Ki-\nnoshita. A recurrent neural network based method\nfor genotype imputation on phased genotype data.\nbioRxiv, page 821504, 2019.\n[112] Alexander Kolesnikov, Xiaohua Zhai, and Lucas\nBeyer. Revisiting self-supervised visual represen-\ntation learning. arXiv preprint arXiv:1901.09005,\n2019.\n[113] Patrick T Komiske, Eric M Metodiev, and Jesse\nThaler. Energy ﬂow networks: deep sets for particle\njets. Journal of High Energy Physics, 2019(1):121,\n2019.\n[114] Shu Kong and Charless Fowlkes.\nImage recon-\nstruction with predictive ﬁlter ﬂow. arXiv preprint\narXiv:1811.11482, 2018.\n41\n[115] Simon Kornblith, Mohammad Norouzi, Honglak\nLee, and Geoﬀrey Hinton.\nSimilarity of neural\nnetwork representations revisited. arXiv preprint\narXiv:1905.00414, 2019.\n[116] Simon Kornblith, Jonathon Shlens, and Quoc V\nLe. Do better imagenet models transfer better? In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2661–2671,\n2019.\n[117] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hin-\nton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in neural information\nprocessing systems, pages 1097–1105, 2012.\n[118] Sneha Reddy Kudugunta, Ankur Bapna, Isaac\nCaswell, Naveen Arivazhagan, and Orhan Firat. In-\nvestigating multilingual nmt representations at scale.\narXiv preprint arXiv:1909.02197, 2019.\n[119] Samuli Laine and Timo Aila.\nTemporal ensem-\nbling for semi-supervised learning. arXiv preprint\narXiv:1610.02242, 2016.\n[120] Dong-Hyun Lee.\nPseudo-label: The simple and\neﬃcient semi-supervised learning method for deep\nneural networks.\nIn Workshop on Challenges in\nRepresentation Learning, ICML, volume 3, page 2,\n2013.\n[121] Jinhyuk\nLee,\nWonjin\nYoon,\nSungdong\nKim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and Jae-\nwoo Kang. Biobert: pre-trained biomedical language\nrepresentation model for biomedical text mining.\narXiv preprint arXiv:1901.08746, 2019.\n[122] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren,\nSamuli Laine, Tero Karras, Miika Aittala, and Timo\nAila. Noise2noise: Learning image restoration with-\nout clean data. arXiv preprint arXiv:1803.04189,\n2018.\n[123] Omer Levy and Yoav Goldberg. Neural word embed-\nding as implicit matrix factorization. In Advances in\nneural information processing systems, pages 2177–\n2185, 2014.\n[124] Chaolong Li, Zhen Cui, Wenming Zheng, Chunyan\nXu, and Jian Yang.\nSpatio-temporal graph con-\nvolution for skeleton based action recognition. In\nThirty-Second AAAI Conference on Artiﬁcial Intel-\nligence, 2018.\n[125] Hailiang Li, Jian Weng, Yujian Shi, Wanrong Gu,\nYijun Mao, Yonghua Wang, Weiwei Liu, and Jiajie\nZhang. An improved deep learning approach for\ndetection of thyroid papillary cancer in ultrasound\nimages. Scientiﬁc reports, 8(1):6600, 2018.\n[126] Yixuan Li, Jason Yosinski, JeﬀClune, Hod Lipson,\nand John E Hopcroft.\nConvergent learning: Do\ndiﬀerent neural networks learn the same representa-\ntions? In Iclr, 2016.\n[127] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and\nRichard Zemel. Gated graph sequence neural net-\nworks. arXiv preprint arXiv:1511.05493, 2015.\n[128] Timothy P Lillicrap, Jonathan J Hunt, Alexan-\nder Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous con-\ntrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\n[129] Fang Liu, Zhaoye Zhou, Hyungseok Jang, Alexey\nSamsonov, Gengyan Zhao, and Richard Kijowski.\nDeep convolutional neural network and 3d de-\nformable approach for tissue segmentation in mus-\nculoskeletal magnetic resonance imaging. Magnetic\nresonance in medicine, 79(4):2379–2391, 2018.\n[130] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. Generating wikipedia by summarizing long\nsequences. arXiv preprint arXiv:1801.10198, 2018.\n[131] Shengyu Liu, Buzhou Tang, Qingcai Chen, and Xiao-\nlong Wang. Eﬀects of semantic features on machine\nlearning-based drug name recognition systems: word\nembeddings vs. manually constructed dictionaries.\nInformation, 6(4):848–865, 2015.\n[132] Xueliang Liu. Deep recurrent neural network for\nprotein function prediction from sequence. arXiv\npreprint arXiv:1701.08318, 2017.\n[133] Yao Liu,\nOmer Gottesman,\nAniruddh Raghu,\nMatthieu Komorowski, Aldo A Faisal, Finale Doshi-\nVelez, and Emma Brunskill. Representation bal-\nancing mdps for oﬀ-policy policy evaluation.\nIn\nAdvances in Neural Information Processing Systems,\npages 2644–2653, 2018.\n[134] Yao Liu, Adith Swaminathan, Alekh Agarwal,\nand Emma Brunskill.\nOﬀ-policy policy gradient\nwith state distribution correction. arXiv preprint\narXiv:1904.08473, 2019.\n[135] Yun Liu, Krishna Gadepalli, Mohammad Norouzi,\nGeorge E Dahl, Timo Kohlberger, Aleksey Boyko,\nSubhashini Venugopalan, Aleksei Timofeev, Philip Q\nNelson, Greg S Corrado, et al. Detecting cancer\nmetastases on gigapixel pathology images. arXiv\npreprint arXiv:1703.02442, 2017.\n[136] Jonathan Long, Evan Shelhamer, and Trevor Darrell.\nFully convolutional networks for semantic segmen-\ntation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3431–\n3440, 2015.\n[137] Mingsheng Long, Han Zhu, Jianmin Wang, and\nMichael I Jordan.\nDeep transfer learning with\njoint adaptation networks. In Proceedings of the\n34th International Conference on Machine Learning-\nVolume 70, pages 2208–2217. JMLR. org, 2017.\n42\n[138] Romain Lopez, Jeﬀrey Regier, Michael Cole, Michael\nJordan, and Nir Yosef. A deep generative model for\ngene expression proﬁles from single-cell rna sequenc-\ning. arXiv preprint arXiv:1709.02082, 2017.\n[139] Donghuan Lu, Karteek Popuri, Gavin Weiguang\nDing, Rakesh Balachandar, and Mirza Faisal Beg.\nMultimodal and multiscale deep neural networks\nfor the early diagnosis of alzheimer’s disease using\nstructural mr and fdg-pet images. Scientiﬁc reports,\n8(1):5697, 2018.\n[140] Scott M Lundberg and Su-In Lee. A uniﬁed approach\nto interpreting model predictions. In Advances in\nNeural Information Processing Systems, pages 4765–\n4774, 2017.\n[141] Laurens van der Maaten and Geoﬀrey Hinton. Visu-\nalizing data using t-sne. Journal of machine learning\nresearch, 9(Nov):2579–2605, 2008.\n[142] Ali Madani, Bryan McCann, Nikhil Naik, Ni-\ntish Shirish Keskar, Namrata Anand, Raphael R\nEguchi, Possu Huang, and Richard Socher. Progen:\nLanguage modeling for protein generation. bioRxiv,\n2020.\n[143] Dhruv Mahajan,\nRoss Girshick,\nVignesh Ra-\nmanathan, Kaiming He, Manohar Paluri, Yixuan\nLi, Ashwin Bharambe, and Laurens van der Maaten.\nExploring the limits of weakly supervised pretrain-\ning. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 181–196, 2018.\n[144] Hongzi Mao, Parimarjan Negi, Akshay Narayan,\nHanrui Wang, Jiacheng Yang, Haonan Wang, Ryan\nMarcus, Ravichandra Addanki, Mehrdad Khani,\nSongtao He, et al.\nPark: An open platform for\nlearning augmented computer systems. 2019.\n[145] Daniel L Marino, Kasun Amarasinghe, and Milos\nManic. Building energy load forecasting using deep\nneural networks. In IECON 2016-42nd Annual Con-\nference of the IEEE Industrial Electronics Society,\npages 7046–7051. IEEE, 2016.\n[146] Alexander Mathis, Pranav Mamidanna, Kevin M\nCury, Taiga Abe, Venkatesh N Murthy, Macken-\nzie\nWeygandt\nMathis,\nand\nMatthias\nBethge.\nDeeplabcut:\nmarkerless pose estimation of user-\ndeﬁned body parts with deep learning. Nature neu-\nroscience, 21(9):1281, 2018.\n[147] Mackenzie Weygandt Mathis and Alexander Mathis.\nDeep learning tools for the measurement of animal\nbehavior in neuroscience. Current Opinion in Neu-\nrobiology, 60:1–11, 2020.\n[148] Leland McInnes, John Healy, and James Melville.\nUmap: Uniform manifold approximation and pro-\njection for dimension reduction.\narXiv preprint\narXiv:1802.03426, 2018.\n[149] Stephen Merity, Nitish Shirish Keskar, and Richard\nSocher. Regularizing and Optimizing LSTM Lan-\nguage Models.\narXiv preprint arXiv:1708.02182,\n2017.\n[150] Stephen Merity, Nitish Shirish Keskar, and Richard\nSocher. An Analysis of Neural Language Modeling\nat Multiple Scales. arXiv preprint arXiv:1803.08240,\n2018.\n[151] Tomas Mikolov, Kai Chen, Greg Corrado, and\nJeﬀrey Dean.\nEﬃcient estimation of word rep-\nresentations in vector space.\narXiv preprint\narXiv:1301.3781, 2013.\n[152] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and JeﬀDean. Distributed representations\nof words and phrases and their compositionality. In\nAdvances in neural information processing systems,\npages 3111–3119, 2013.\n[153] Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-\nrafsky. Distant supervision for relation extraction\nwithout labeled data. In Proceedings of the Joint\nConference of the 47th Annual Meeting of the ACL\nand the 4th International Joint Conference on Nat-\nural Language Processing of the AFNLP: Volume\n2-Volume 2, pages 1003–1011. Association for Com-\nputational Linguistics, 2009.\n[154] Ishan Misra and Laurens van der Maaten.\nSelf-\nsupervised learning of pretext-invariant representa-\ntions, 2019.\n[155] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama,\nand Shin Ishii.\nVirtual adversarial training:\na\nregularization method for supervised and semi-\nsupervised learning. IEEE transactions on pattern\nanalysis and machine intelligence, 41(8):1979–1993,\n2018.\n[156] Pim Moeskops, Max A Viergever, Adriënne M Men-\ndrik, Linda S de Vries, Manon JNL Benders, and\nIvana Išgum. Automatic segmentation of mr brain\nimages with a convolutional neural network. IEEE\ntransactions on medical imaging, 35(5):1252–1261,\n2016.\n[157] Ari Morcos, Maithra Raghu, and Samy Bengio. In-\nsights on representational similarity in neural net-\nworks with canonical correlation. In Advances in\nNeural Information Processing Systems, pages 5727–\n5736, 2018.\n[158] Alejandro Newell, Kaiyu Yang, and Jia Deng.\nStacked hourglass networks for human pose esti-\nmation. In European conference on computer vision,\npages 483–499. Springer, 2016.\n[159] Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon\nKornblith, Quoc V Le, and Ruoming Pang. Domain\nadaptive transfer learning with specialist models.\narXiv preprint arXiv:1811.07056, 2018.\n43\n[160] Harsha Nori, Samuel Jenkins, Paul Koch, and Rich\nCaruana.\nInterpretml: A uniﬁed framework for\nmachine learning interpretability. arXiv preprint\narXiv:1909.09223, 2019.\n[161] Mehdi Noroozi and Paolo Favaro.\nUnsupervised\nlearning of visual representations by solving jigsaw\npuzzles. In European Conference on Computer Vi-\nsion, pages 69–84. Springer, 2016.\n[162] Luke Oakden-Rayner, Jared Dunnmon, Gustavo\nCarneiro, and Christopher Ré. Hidden stratiﬁca-\ntion causes clinically meaningful failures in ma-\nchine learning for medical imaging. arXiv preprint\narXiv:1909.12475, 2019.\n[163] Chris Olah.\nUnderstanding LSTM Networks,\n2015.\nhttps://colah.github.io/posts/2015-08-\nUnderstanding-LSTMs/.\n[164] Chris Olah, Nick Cammarata, Ludwig Schubert,\nGabriel Goh, Michael Petrov, and Shan Carter.\nZoom in:\nAn introduction to circuits.\nDistill,\n5(3):e00024–001, 2020.\n[165] Chris Olah, Alexander Mordvintsev, and Ludwig\nSchubert.\nFeature visualization.\nDistill, 2017.\nhttps://distill.pub/2017/feature-visualization.\n[166] Chris\nOlah,\nArvind\nSatyanarayan,\nIan\nJohn-\nson, Shan Carter, Ludwig Schubert, Katherine\nYe,\nand Alexander Mordvintsev.\nThe build-\ning\nblocks\nof\ninterpretability.\nDistill,\n2018.\nhttps://distill.pub/2018/building-blocks.\n[167] Aaron van den Oord, Sander Dieleman, Heiga\nZen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu.\nWavenet: A generative model for\nraw audio. arXiv preprint arXiv:1609.03499, 2016.\n[168] Aaron van den Oord, Nal Kalchbrenner, and Koray\nKavukcuoglu. Pixel recurrent neural networks. arXiv\npreprint arXiv:1601.06759, 2016.\n[169] Aaron van den Oord, Yazhe Li, and Oriol Vinyals.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n[170] Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-\ngio. Understanding the exploding gradient problem.\nCoRR, abs/1211.5063, 2, 2012.\n[171] Deepak Pathak, Philipp Krahenbuhl, and Trevor\nDarrell. Constrained convolutional neural networks\nfor weakly supervised segmentation. In Proceedings\nof the IEEE international conference on computer\nvision, pages 1796–1804, 2015.\n[172] Romain Paulus, Caiming Xiong, and Richard Socher.\nA deep reinforced model for abstractive summariza-\ntion. arXiv preprint arXiv:1705.04304, 2017.\n[173] Jeﬀrey Pennington, Richard Socher, and Christo-\npher Manning. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532–1543, 2014.\n[174] Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V\nLe, and JeﬀDean.\nEﬃcient neural architecture\nsearch via parameter sharing.\narXiv preprint\narXiv:1802.03268, 2018.\n[175] Gianluca Pollastri, Darisz Przybylski, Burkhard\nRost, and Pierre Baldi. Improving the prediction\nof protein secondary structure in three and eight\nclasses using recurrent neural networks and proﬁles.\nProteins: Structure, Function, and Bioinformatics,\n47(2):228–235, 2002.\n[176] Ryan Poplin, Avinash V Varadarajan, Katy Blumer,\nYun Liu, Michael V McConnell, Greg S Corrado,\nLily Peng, and Dale R Webster. Prediction of cardio-\nvascular risk factors from retinal fundus photographs\nvia deep learning. Nature Biomedical Engineering,\n2(3):158, 2018.\n[177] Rory M Power and Jan Huisken. A guide to light-\nsheet ﬂuorescence microscopy for multiscale imaging.\nNature methods, 14(4):360, 2017.\n[178] Doina Precup. Eligibility traces for oﬀ-policy policy\nevaluation. Computer Science Department Faculty\nPublication Series, page 80, 2000.\n[179] Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang,\nand Alan Yuille.\nDeep co-training for semi-\nsupervised image recognition. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV),\npages 135–152, 2018.\n[180] Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. OpenAI Blog,\n1(8), 2019.\n[181] Colin Raﬀel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\n[182] Aniruddh Raghu, Matthieu Komorowski, Leo An-\nthony Celi, Peter Szolovits, and Marzyeh Ghassemi.\nContinuous state-space models for optimal sepsis\ntreatment-a deep reinforcement learning approach.\narXiv preprint arXiv:1705.08422, 2017.\n[183] Maithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. Svcca: Singular vector canon-\nical correlation analysis for deep learning dynamics\nand interpretability. In Advances in Neural Infor-\nmation Processing Systems, pages 6076–6085, 2017.\n[184] Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and\nSamy Bengio. Transfusion: Understanding transfer\nlearning for medical imaging. In Advances in Neural\nInformation Processing Systems, pages 3342–3352,\n2019.\n44\n[185] Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Bran-\ndon Yang, Hershel Mehta, Tony Duan, Daisy Ding,\nAarti Bagul, Curtis Langlotz, Katie Shpanskaya,\net al.\nChexnet: Radiologist-level pneumonia de-\ntection on chest x-rays with deep learning. arXiv\npreprint arXiv:1711.05225, 2017.\n[186] Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-\nrev, and Percy Liang. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250, 2016.\n[187] Bharath Ramsundar, Steven Kearnes, Patrick Riley,\nDale Webster, David Konerding, and Vijay Pande.\nMassively multitask networks for drug discovery.\narXiv preprint arXiv:1502.02072, 2015.\n[188] Alexander Ratner, Stephen H Bach, Henry Ehren-\nberg, Jason Fries, Sen Wu, and Christopher Ré.\nSnorkel: Rapid training data creation with weak\nsupervision. Proceedings of the VLDB Endowment,\n11(3):269–282, 2017.\n[189] Ali Razavi, Aaron van den Oord, and Oriol Vinyals.\nGenerating diverse high-ﬁdelity images with vq-vae-\n2. arXiv preprint arXiv:1906.00446, 2019.\n[190] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt,\nand Vaishaal Shankar. Do imagenet classiﬁers gener-\nalize to imagenet? arXiv preprint arXiv:1902.10811,\n2019.\n[191] Joseph\nRedmon\nand\nAli\nFarhadi.\nYolov3:\nAn incremental improvement.\narXiv preprint\narXiv:1804.02767, 2018.\n[192] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. Faster r-cnn: Towards real-time object detec-\ntion with region proposal networks. In Advances in\nneural information processing systems, pages 91–99,\n2015.\n[193] Donatas Repecka, Vykintas Jauniskis, Laurynas\nKarpus, Elzbieta Rembeza, Jan Zrimec, Simona\nPoviloniene, Irmantas Rokaitis, Audrius Laurynenas,\nWissam Abuajwa, Otto Savolainen, et al. Expanding\nfunctional protein sequence space using generative\nadversarial networks. bioRxiv, page 789719, 2019.\n[194] Marco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin.\nWhy should i trust you?: Explaining\nthe predictions of any classiﬁer. In Proceedings of\nthe 22nd ACM SIGKDD international conference on\nknowledge discovery and data mining, pages 1135–\n1144. ACM, 2016.\n[195] Alexander Rives, Siddharth Goyal, Joshua Meier,\nDemi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma,\nand Rob Fergus. Biological structure and function\nemerge from scaling unsupervised learning to 250\nmillion protein sequences.\nbioRxiv, page 622803,\n2019.\n[196] Olaf Ronneberger, Philipp Fischer, and Thomas\nBrox. U-net: Convolutional networks for biomedical\nimage segmentation. In International Conference\non Medical image computing and computer-assisted\nintervention, pages 234–241. Springer, 2015.\n[197] Alexander M Rush, Sumit Chopra, and Jason\nWeston.\nA neural attention model for abstrac-\ntive sentence summarization.\narXiv preprint\narXiv:1509.00685, 2015.\n[198] Andrei A Rusu, Mel Vecerik, Thomas Rothörl, Nico-\nlas Heess, Razvan Pascanu, and Raia Hadsell. Sim-\nto-real robot learning from pixels with progressive\nnets. arXiv preprint arXiv:1610.04286, 2016.\n[199] Ruhan Sa, William Owens, Raymond Wiegand,\nMark Studin, Donald Capoferri, Kenneth Barooha,\nAlexander Greaux, Robert Rattray, Adam Hutton,\nJohn Cintineo, et al. Intervertebral disc detection\nin x-ray images using faster r-cnn. In 2017 39th\nAnnual International Conference of the IEEE Engi-\nneering in Medicine and Biology Society (EMBC),\npages 564–567. IEEE, 2017.\n[200] Tim Salimans, Andrej Karpathy, Xi Chen, and\nDiederik P Kingma. Pixelcnn++: Improving the\npixelcnn with discretized logistic mixture likeli-\nhood and other modiﬁcations.\narXiv preprint\narXiv:1701.05517, 2017.\n[201] Victor Sanh, Lysandre Debut, Julien Chaumond,\nand Thomas Wolf. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108, 2019.\n[202] Saman Sarraf, Ghassem Toﬁghi, et al.\nDeepad:\nAlzheimer disease classiﬁcation via deep convolu-\ntional neural networks using mri and fmri. BioRxiv,\npage 070441, 2016.\n[203] John Schulman,\nFilip Wolski,\nPrafulla Dhari-\nwal, Alec Radford, and Oleg Klimov.\nProximal\npolicy optimization algorithms.\narXiv preprint\narXiv:1707.06347, 2017.\n[204] Ramprasaath R Selvaraju, Michael Cogswell, Ab-\nhishek Das, Ramakrishna Vedantam, Devi Parikh,\nand Dhruv Batra. Grad-cam: Visual explanations\nfrom deep networks via gradient-based localization.\nIn Proceedings of the IEEE International Conference\non Computer Vision, pages 618–626, 2017.\n[205] Andrew W Senior, Richard Evans, John Jumper,\nJames Kirkpatrick, Laurent Sifre, Tim Green,\nChongli Qin, Augustin Žídek, Alexander WR Nelson,\nAlex Bridgland, et al. Improved protein structure\nprediction using potentials from deep learning. Na-\nture, pages 1–5, 2020.\n[206] Rico Sennrich, Barry Haddow, and Alexandra Birch.\nImproving neural machine translation models with\nmonolingual data. arXiv preprint arXiv:1511.06709,\n2015.\n45\n[207] Lloyd S Shapley. A value for n-person games. Con-\ntributions to the Theory of Games, 2(28):307–317,\n1953.\n[208] Jianghong Shi, Eric Shea-Brown, and Michael Buice.\nComparison against task driven artiﬁcial neural net-\nworks reveals functional properties in mouse visual\ncortex. In Advances in Neural Information Process-\ning Systems, pages 5765–5775, 2019.\n[209] Susan M Shortreed, Eric Laber, Daniel J Lizotte,\nT Scott Stroup, Joelle Pineau, and Susan A Mur-\nphy. Informing sequential clinical decision-making\nthrough reinforcement learning: an empirical study.\nMachine learning, 84(1-2):109–136, 2011.\n[210] Avanti Shrikumar, Peyton Greenside, and Anshul\nKundaje. Learning important features through prop-\nagating activation diﬀerences. In Proceedings of the\n34th International Conference on Machine Learning-\nVolume 70, pages 3145–3153. JMLR. org, 2017.\n[211] Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano\nErmon. A dirt-t approach to unsupervised domain\nadaptation. arXiv preprint arXiv:1802.08735, 2018.\n[212] David Silver, Julian Schrittwieser, Karen Simonyan,\nIoannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian\nBolton, et al. Mastering the game of go without\nhuman knowledge. nature, 550(7676):354–359, 2017.\n[213] Karen Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. Deep inside convolutional networks: Visual-\nising image classiﬁcation models and saliency maps.\narXiv preprint arXiv:1312.6034, 2013.\n[214] Karen Simonyan and Andrew Zisserman. Very deep\nconvolutional networks for large-scale image recog-\nnition. arXiv preprint arXiv:1409.1556, 2014.\n[215] Daniel Smilkov, Nikhil Thorat, Been Kim, Fer-\nnanda Viégas, and Martin Wattenberg. Smooth-\ngrad: removing noise by adding noise. arXiv preprint\narXiv:1706.03825, 2017.\n[216] Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe,\nSøren Kaae Sønderby, and Ole Winther. Ladder\nvariational autoencoders.\nIn Advances in neural\ninformation processing systems, pages 3738–3746,\n2016.\n[217] Youyi Song, Ling Zhang, Siping Chen, Dong Ni,\nBaopu Li, Yongjing Zhou, Baiying Lei, and Tianfu\nWang. A deep learning based framework for accurate\nsegmentation of cervical cytoplasm and nuclei. In\n2014 36th Annual International Conference of the\nIEEE Engineering in Medicine and Biology Society,\npages 2903–2906. IEEE, 2014.\n[218] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.\nDeep high-resolution representation learning for hu-\nman pose estimation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition, pages 5693–5703, 2019.\n[219] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A\nEfros. Unsupervised domain adaptation through\nself-supervision. arXiv preprint arXiv:1909.11825,\n2019.\n[220] Mukund Sundararajan, Ankur Taly, and Qiqi Yan.\nAxiomatic attribution for deep networks. In Proceed-\nings of the 34th International Conference on Ma-\nchine Learning-Volume 70, pages 3319–3328. JMLR.\norg, 2017.\n[221] I Sutskever, O Vinyals, and QV Le. Sequence to\nsequence learning with neural networks. Advances\nin NIPS, 2014.\n[222] Ryo Takahashi, Takashi Matsubara, and Kuniaki\nUehara. Data augmentation using random image\ncropping and patching for deep cnns. IEEE Transac-\ntions on Circuits and Systems for Video Technology,\n2019.\n[223] Mingxing Tan and Quoc V Le. Eﬃcientnet: Rethink-\ning model scaling for convolutional neural networks.\narXiv preprint arXiv:1905.11946, 2019.\n[224] Mingxing Tan, Ruoming Pang, and Quoc V Le.\nEﬃcientdet: Scalable and eﬃcient object detection.\narXiv preprint arXiv:1911.09070, 2019.\n[225] Antti Tarvainen and Harri Valpola. Mean teachers\nare better role models: Weight-averaged consistency\ntargets improve semi-supervised deep learning re-\nsults. In Advances in neural information processing\nsystems, pages 1195–1204, 2017.\n[226] Dimitry Tegunov and Patrick Cramer. Real-time\ncryo-em data pre-processing with warp. BioRxiv,\npage 338558, 2018.\n[227] Yee Liang Thian, Yiting Li, Pooja Jagmohan, David\nSia, Vincent Ern Yao Chan, and Robby T Tan. Con-\nvolutional neural networks for automated fracture\ndetection and localization on wrist radiographs. Ra-\ndiology: Artiﬁcial Intelligence, 1(1):e180001, 2019.\n[228] Eric J Topol. High-performance medicine: the con-\nvergence of human and artiﬁcial intelligence. Nature\nmedicine, 25(1):44–56, 2019.\n[229] Raphael Townshend, Rishi Bedi, Patricia Suriana,\nand Ron Dror. End-to-end learning on 3d protein\nstructure for interface prediction. In Advances in\nNeural Information Processing Systems, pages 15616–\n15625, 2019.\n[230] Vahe Tshitoyan, John Dagdelen, Leigh Weston,\nAlexander Dunn, Ziqin Rong, Olga Kononova,\nKristin A Persson, Gerbrand Ceder, and Anubhav\nJain. Unsupervised word embeddings capture latent\nknowledge from materials science literature. Nature,\n571(7763):95–98, 2019.\n[231] Kensuke Umehara, Junko Ota, and Takayuki Ishida.\nApplication of super-resolution convolutional neural\n46\nnetwork for enhancing image resolution in chest ct.\nJournal of digital imaging, 31(4):441–450, 2018.\n[232] Aaron Van den Oord, Nal Kalchbrenner, Lasse Es-\npeholt, Oriol Vinyals, Alex Graves, et al. Condi-\ntional image generation with pixelcnn decoders. In\nAdvances in neural information processing systems,\npages 4790–4798, 2016.\n[233] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing\nsystems, pages 5998–6008, 2017.\n[234] Oriol Vinyals and Quoc Le. A neural conversational\nmodel. arXiv preprint arXiv:1506.05869, 2015.\n[235] Elena Voita, Rico Sennrich, and Ivan Titov. The\nbottom-up evolution of representations in the trans-\nformer:\nA study with machine translation and\nlanguage modeling objectives.\narXiv preprint\narXiv:1909.01380, 2019.\n[236] Christian Wachinger, Martin Reuter, and Tassilo\nKlein. Deepnat: Deep convolutional neural network\nfor segmenting neuroanatomy. NeuroImage, 170:434–\n445, 2018.\n[237] Kun Wang, Bite Yang, Guohai Xu, and Xiaofeng\nHe. Medical question retrieval based on siamese\nneural network and transfer learning method. In\nInternational Conference on Database Systems for\nAdvanced Applications, pages 49–64. Springer, 2019.\n[238] Nancy XR Wang, Ali Farhadi, Rajesh PN Rao, and\nBingni W Brunton. Ajile movement prediction: Mul-\ntimodal deep learning for natural human neural\nrecordings and video. In Thirty-Second AAAI Con-\nference on Artiﬁcial Intelligence, 2018.\n[239] William Yang Wang and Diyi Yang. That’s so an-\nnoying!!!: A lexical and frame-semantic embedding\nbased data augmentation approach to automatic cat-\negorization of annoying behaviors using# petpeeve\ntweets. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2557–2563, 2015.\n[240] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and\nKaiming He. Non-local neural networks. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 7794–7803, 2018.\n[241] Zeyu Wang, Klint Qinami, Yannis Karakozis, Kyle\nGenova, Prem Nair, Kenji Hata, and Olga Rus-\nsakovsky. Towards fairness in visual recognition: Ef-\nfective strategies for bias mitigation. arXiv preprint\narXiv:1911.11834, 2019.\n[242] Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr\nMnih, Remi Munos, Koray Kavukcuoglu, and Nando\nde Freitas. Sample eﬃcient actor-critic with experi-\nence replay. arXiv preprint arXiv:1611.01224, 2016.\n[243] Jason W Wei and Kai Zou. Eda: Easy data augmen-\ntation techniques for boosting performance on text\nclassiﬁcation tasks. arXiv preprint arXiv:1901.11196,\n2019.\n[244] Shih-En Wei, Varun Ramakrishna, Takeo Kanade,\nand Yaser Sheikh. Convolutional pose machines. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 4724–4732,\n2016.\n[245] Martin Weigert, Uwe Schmidt, Tobias Boothe, An-\ndreas Müller, Alexandr Dibrov, Akanksha Jain, Ben-\njamin Wilhelm, Deborah Schmidt, Coleman Broad-\ndus, Siân Culley, et al. Content-aware image restora-\ntion: pushing the limits of ﬂuorescence microscopy.\nNature methods, 15(12):1090, 2018.\n[246] David Weiss, Chris Alberti, Michael Collins, and\nSlav Petrov.\nStructured training for neural net-\nwork transition-based parsing.\narXiv preprint\narXiv:1506.06158, 2015.\n[247] Julia K Winkler, Christine Fink, Ferdinand Toberer,\nAlexander Enk, Teresa Deinlein, Rainer Hofmann-\nWellenhof, Luc Thomas, Aimilios Lallas, Andreas\nBlum, Wilhelm Stolz, et al. Association between\nsurgical skin markings in dermoscopic images and\ndiagnostic performance of a deep learning convo-\nlutional neural network for melanoma recognition.\nJAMA dermatology, 155(10):1135–1141, 2019.\n[248] Yuxin Wu, Alexander Kirillov, Francisco Massa,\nWan-Yen\nLo,\nand\nRoss\nGirshick.\nDetec-\ntron2.\nhttps://github.com/facebookresearch/\ndetectron2, 2019.\n[249] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg,\nJoseph Gomes, Caleb Geniesse, Aneesh S Pappu,\nKarl Leswing, and Vijay Pande. Moleculenet: a\nbenchmark for molecular machine learning. Chemi-\ncal science, 9(2):513–530, 2018.\n[250] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong\nLong, Chengqi Zhang, and Philip S Yu. A com-\nprehensive survey on graph neural networks. arXiv\npreprint arXiv:1901.00596, 2019.\n[251] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang\nLuong, and Quoc V Le. Unsupervised data augmen-\ntation. arXiv preprint arXiv:1904.12848, 2019.\n[252] Qizhe Xie, Eduard Hovy, Minh-Thang Luong, and\nQuoc V Le.\nSelf-training with noisy student\nimproves imagenet classiﬁcation.\narXiv preprint\narXiv:1911.04252, 2019.\n[253] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen\nTu, and Kaiming He. Aggregated residual transfor-\nmations for deep neural networks. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 1492–1500, 2017.\n47\n[254] Jun Xu, Xiaofei Luo, Guanhao Wang, Hannah\nGilmore, and Anant Madabhushi. A deep convolu-\ntional neural network for segmenting and classifying\nepithelial and stromal regions in histopathological\nimages. Neurocomputing, 191:214–223, 2016.\n[255] Xueting Yan, Ishan Misra, Abhinav Gupta, Deepti\nGhadiyaram, and Dhruv Mahajan. Clusterﬁt: Im-\nproving generalization of visual representations.\narXiv preprint arXiv:1912.03330, 2019.\n[256] Yilong Yang, Zhuyifan Ye, Yan Su, Qianqian Zhao,\nXiaoshan Li, and Defang Ouyang. Deep learning for\nin vitro prediction of pharmaceutical formulations.\nActa pharmaceutica sinica B, 9(1):177–185, 2019.\n[257] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language\nunderstanding. arXiv preprint arXiv:1906.08237,\n2019.\n[258] Koichiro Yasaka, Hiroyuki Akai, Osamu Abe, and\nShigeru Kiryu. Deep learning with convolutional\nneural network for diﬀerentiation of liver masses at\ndynamic contrast-enhanced ct: a preliminary study.\nRadiology, 286(3):887–896, 2017.\n[259] Jason Yosinski, JeﬀClune, Anh Nguyen, Thomas\nFuchs, and Hod Lipson. Understanding neural net-\nworks through deep visualization. arXiv preprint\narXiv:1506.06579, 2015.\n[260] Yuhui Yuan, Xilin Chen, and Jingdong Wang.\nObject-contextual representations for semantic seg-\nmentation. arXiv preprint arXiv:1909.11065, 2019.\n[261] Sangdoo Yun, Dongyoon Han, Seong Joon Oh,\nSanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong clas-\nsiﬁers with localizable features. In Proceedings of\nthe IEEE International Conference on Computer\nVision, pages 6023–6032, 2019.\n[262] Manzil Zaheer, Satwik Kottur, Siamak Ravan-\nbakhsh, Barnabas Poczos, Russ R Salakhutdinov,\nand Alexander J Smola. Deep sets. In Advances in\nneural information processing systems, pages 3391–\n3401, 2017.\n[263] Matthew D Zeiler and Rob Fergus.\nVisualizing\nand understanding convolutional networks. In Euro-\npean conference on computer vision, pages 818–833.\nSpringer, 2014.\n[264] Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.\nDistant supervision for relation extraction via piece-\nwise convolutional neural networks. In Proceedings\nof the 2015 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1753–1762, 2015.\n[265] Xiaohua\nZhai,\nAvital\nOliver,\nAlexander\nKolesnikov,\nand\nLucas\nBeyer.\nS4l:\nSelf-\nsupervised\nsemi-supervised\nlearning.\narXiv\npreprint arXiv:1905.03670, 2019.\n[266] Xiaohua\nZhai,\nJoan\nPuigcerver,\nAlexander\nKolesnikov,\nPierre Ruyssen,\nCarlos Riquelme,\nMario Lucic, Josip Djolonga, Andre Susano Pinto,\nMaxim Neumann, Alexey Dosovitskiy, et al. The\nvisual task adaptation benchmark. arXiv preprint\narXiv:1910.04867, 2019.\n[267] Aston Zhang, Zachary C Lipton, Mu Li, and Alexan-\nder J Smola. Dive into deep learning. Unpublished\ndraft. Retrieved, 3:319, 2019.\n[268] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin,\nand David Lopez-Paz. mixup: Beyond empirical\nrisk minimization. arXiv preprint arXiv:1710.09412,\n2017.\n[269] Junkang Zhang, Haigen Hu, Shengyong Chen, Yu-\njiao Huang, and Qiu Guan. Cancer cells detection\nin phase-contrast microscopy images based on faster\nr-cnn.\nIn 2016 9th International Symposium on\nComputational Intelligence and Design (ISCID), vol-\nume 1, pages 363–367. IEEE, 2016.\n[270] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong,\nand Yun Fu. Residual dense network for image super-\nresolution. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n2472–2481, 2018.\n[271] Victor Zhong, Caiming Xiong, and Richard Socher.\nSeq2sql: Generating structured queries from natu-\nral language using reinforcement learning. arXiv\npreprint arXiv:1709.00103, 2017.\n[272] Bolei Zhou,\nAditya Khosla,\nAgata Lapedriza,\nAude Oliva, and Antonio Torralba. Object detec-\ntors emerge in deep scene cnns.\narXiv preprint\narXiv:1412.6856, 2014.\n[273] Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,\nMing Zhou, and Tiejun Zhao. Neural document\nsummarization by jointly learning to score and select\nsentences. arXiv preprint arXiv:1807.02305, 2018.\n[274] Jun-Yan Zhu, Taesung Park, Phillip Isola, and\nAlexei A Efros.\nUnpaired image-to-image trans-\nlation using cycle-consistent adversarial networks.\nIn Proceedings of the IEEE international conference\non computer vision, pages 2223–2232, 2017.\n[275] Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and\nMax Welling. Visualizing deep neural network deci-\nsions: Prediction diﬀerence analysis. arXiv preprint\narXiv:1702.04595, 2017.\n48\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-03-26",
  "updated": "2020-03-26"
}