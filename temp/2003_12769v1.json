{
  "id": "http://arxiv.org/abs/2003.12769v1",
  "title": "Learning Invariant Representation for Unsupervised Image Restoration",
  "authors": [
    "Wenchao Du",
    "Hu Chen",
    "Hongyu Yang"
  ],
  "abstract": "Recently, cross domain transfer has been applied for unsupervised image\nrestoration tasks. However, directly applying existing frameworks would lead to\ndomain-shift problems in translated images due to lack of effective\nsupervision. Instead, we propose an unsupervised learning method that\nexplicitly learns invariant presentation from noisy data and reconstructs clear\nobservations. To do so, we introduce discrete disentangling representation and\nadversarial domain adaption into general domain transfer framework, aided by\nextra self-supervised modules including background and semantic consistency\nconstraints, learning robust representation under dual domain constraints, such\nas feature and image domains. Experiments on synthetic and real noise removal\ntasks show the proposed method achieves comparable performance with other\nstate-of-the-art supervised and unsupervised methods, while having faster and\nstable convergence than other domain adaption methods.",
  "text": "Learning Invariant Representation for Unsupervised Image Restoration\nWenchao Du, Hu Chen†, Hongyu Yang\nCollege of Computer Science, Sichuan University, Chengdu 610065, China\nWenchaodu.scu@gmail.com, huchen@scu.edu.cn, yanghongyu@scu.edu.cn\nAbstract\nRecently, cross domain transfer has been applied for\nunsupervised image restoration tasks.\nHowever, directly\napplying existing frameworks would lead to domain-shift\nproblems in translated images due to lack of effective su-\npervision. Instead, we propose an unsupervised learning\nmethod that explicitly learns invariant presentation from\nnoisy data and reconstructs clear observations.\nTo do\nso, we introduce discrete disentangling representation and\nadversarial domain adaption into general domain trans-\nfer framework, aided by extra self-supervised modules in-\ncluding background and semantic consistency constraints,\nlearning robust representation under dual domain con-\nstraints, such as feature and image domains. Experiments\non synthetic and real noise removal tasks show the proposed\nmethod achieves comparable performance with other state-\nof-the-art supervised and unsupervised methods, while hav-\ning faster and stable convergence than other domain adap-\ntion methods. Code has been released.\n1. Introduction\nImage restoration (IR) attempts to reconstruct clean sig-\nnals from their corrupted observations, which is known to\nbe an ill-posed inverse problem. By accommodating differ-\nent types of corruption distributions, the same mathemat-\nical model applies to problems such as image denoising,\nsuper-resolution and deblurring. Recently, deep neural net-\nworks (DNNs) and generative adversarial networks (GANs)\n[10] have shown their superior performance in various low-\nlevel vision tasks. Nonetheless, most of these methods need\npaired training data for speciﬁc tasks, which limits their\ngenerality, scalability and practicality in real-world multi-\nmedia applications.\nIn addition, strong supervision may\nsuffer from the overﬁtting training and lower generalization\nto real image corruption types.\nMore recently, the domain transfer based unsupervised\nlearning methods have attracted lots of attention due to the\ngreat progress [9, 18, 20, 21, 40] achieved in style trans-\nfer, attribute editing and image translation, e.g., CycleGAN\n(a) Input\n(b) CycleGAN\n(c) UNIT\n(d) Ours\nFigure 1: The typical results for Gaussian Noise.\nOur\nmethod has better ability on noise removal and texture\npreservation than other domain-transfer methods.\n[40], UNIT [21] and DRIT [18]. Although these methods\nhave been expanded to speciﬁc restoration tasks, they could\nnot reconstruct the high-quality images due to losing ﬁner\ndetails or inconsistency backgrounds, as shown in Fig. 1.\nDifferent from DNNs based supervised models, which aim\nat learning a powerful mapping between the noisy and clean\nimages. Directly applying existing domain-transfer meth-\nods is unsuitable for generalized image inverse problems\ndue to the following reasons:\n• Indistinct Domain Boundary. Image translation aims to\nlearn abstract shared-representations from unpaired data\nwith clear domain characteristics, such as horse-to-zebra,\nday-to-night, etc.\nOn the contrary, varying noise lev-\nels and complicated backgrounds blur domain boundaries\nbetween unpaired inputs.\n• Weak Representation.\nUnsupervised domain-adaption\nmethods extract high-level representations from unpaired\ndata by shared-weight encoder and explicit target domain\ndiscriminator. For slight noisy signals, it is easy to cause\ndomain shift problems in translated images and lead to\nlow-quality reconstruction.\n• Poor Generalization.\nImage translation learns a do-\nmain mapping from one-to-one image, which hardly cap-\ntures the generalized semantic and texture representa-\ntions. This also exacerbates the instability of GAN.\nIn order to address these problems, inspired by image\nsparse representation [24] and domain adaption [7, 8], we\n4321\narXiv:2003.12769v1  [cs.CV]  28 Mar 2020\nattempt to learn invariant representation from unpaired sam-\nples via domain adaption and reconstruct clean images in-\nstead of relying on pure unsupervised domain transfer. Dif-\nferent from general image translation methods [18, 21, 40],\nour goal is to learn robust intermediate representation free\nof noise (referred to as Invariant Representation) and re-\nconstruct clean observations. Speciﬁcally, to achieve this\ngoal, we factorize content and noise representations for cor-\nrupted images via disentangled learning; then a represen-\ntation discriminator is utilized to align features to the ex-\npected distribution of clean domain. In addition, the extra\nself-supervised modules, including background and seman-\ntic consistency constraints, are used to supervise represen-\ntation learning from image domains further.\nIn short, the main contributions of the paper could be\nsummarized as follows: 1) Propose an unsupervised repre-\nsentation learning method for image restoration based on\ndata-driven, which is easily expanded to other low-level vi-\nsion tasks, such as super-resolution and deblurring. 2) Dis-\nentangle deep representation via dual domain constraints,\ni.e., feature and image domains. Extra self-supervised mod-\nules, including semantic meaning and background consis-\ntency modules, further improve the robustness of represen-\ntations. 3) Build an unsupervised image restoration frame-\nwork based on cross domain transfer with more effective\ntraining and faster convergence speed. To our knowledge,\nthis is the ﬁrst unsupervised representation learning ap-\nproach that achieves competing results for processing syn-\nthetic and real noise removal with end-to-end training.\n2. Related Work\n2.1. Single Image Restoraion\nTraditional Methods.\nClassical methods, containing\nTotal Variation [29, 34], BM3D [5], Non-local mean [2]\nand dictionary learning [3, 12], have achieved good per-\nformance on general image restoration tasks, such as im-\nage denoising, super-resolution and deblurring. In addition,\nconsidering that image restoration is in general an ill-posed\nproblem, some methods based on regularization are also\nproved effective [11, 42].\nDeep Neural Networks. Relying on powerful computer\nsources, data-driven DNN methods have achieved better\nperformance than traditional methods in the past few years.\nVincent et al. [35] proposed stacked denoising auto-encoder\nfor image restoration. Xie et al. [36] combined sparse cod-\ning and pre-trained DNN for image denoising and inpaint-\ning. Mao et al. [26] proposed RedNet with symmetric skip\nconnections for noise removal and super-resolution. Zhang\net al. [39] introduced residual learning for Gaussian noise\nremoval. In general, DNNs-based methods could realize su-\nperior results on synthetic noise removal via effective su-\npervised training, but it is unsuitable for real-world applica-\ntions.\n2.2. Unsupervised Learning for IR\nLearning from noisy observations. One interesting di-\nrection for unsupervised IR is directly recovering clean\nsignals from noisy observations. Dmitry et al. [32] pro-\nposed deep image prior (DIP) for IR, which requires suit-\nable networks and interrupts its training process based on\nlow-level statistical prior. That is usually unpredictable for\ndifferent samples. Via zero-mean noise distribution prior,\nNoise2Noise (N2N) [19] directly learns reconstruction be-\ntween two images with independent noise sampling. That\nis unsuitable for noise removal in real-world, e.g., medical\nimage denoising. To alleviate this problem, Noise2Void [17]\npredicted a pixel from its surroundings by learning a blind-\nspot network for corrupted images. Similar to Noise2Self\n[1], this method reduces the training efﬁciency, but also de-\ncreases the denoising performance.\nImage Domain Transfer. Another direction solves im-\nage restoration by domain transfer, which aims to learn\none2one mapping from one domain to another and output\nimage to lie on the manifold of clean image.\nPrevious\nworks, e.g., CycleGAN [40], DualGAN [37] and Bicycle-\nGAN [41] have shown great capacity in image translation.\nExpanding works, containing CouplesGAN [22], UNIT [21]\nand DRIT [18] learn shared-latent representation for diverse\nimage translation. Along this way, Yuan et al. [38] pro-\nposed a nested CycleGAN to solve the unsupervised image\nsuper-resolution. Expanding DRIT, Lu et al. [23] decou-\npled image content domain and blur domain to solve image\ndeblurring, referred to as DRNet. However, these methods\naim to learn stronger domain generators, they require ob-\nvious domain boundary and complicated network structure.\n3. The Proposed Method\nOur goal is to learn abstract intermediate representations\nfrom noise inputs and reconstruct clear observations. In a\ncertain way, unsupervised IR could be viewed as a speciﬁc\ndomain transfer problem, i.e., from noise domain to clean\ndomain. Therefore, the method is injected into the general\ndomain transfer architecture, as shown in Fig. 2.\nIn supervised domain transfer, we are given samples\n(x, y) drawn from a joint distribution PX,Y(x, y), where X\nand Y are two image domains. For unsupervised domain\ntranslation, samples (x, y) are drawn from the marginal dis-\ntributions PX (x) and PY(y). In order to infer the joint dis-\ntribution from the marginal samples, a shared-latent space\nassumption is proposed that there exists a shared latent code\nz in a shared-latent space Z, so that we can recover both\nimages from this code. Given samples (x, y) from the joint\n4322\n(a)\n(b)\nFigure 2: Method Overview. (a) The latent space assumption. Proposed method aims to learn invariant representations from\ninputs and align them via adversarial domain adaption. (b) Our method is injected into general domain-transfer framework.\nExtra self-supervised modules are introduced to learn more robust representations.\ndistribution, this process is presented by\nz = EX (x) = EY(y)\n(1)\nx = GX (z), y = GY(z)\n(2)\nA key step is how to implement this shared-latent space\nassumption. To do so, an effective strategy is sharing high-\nlevel representation by shared-weight encoder, which sam-\nples the features from the uniﬁed distribution. However, it is\nunsuitable for IR that latent representation only contains se-\nmantic meanings, which leads to domain shift in recovered\nimages, e.g., blurred details and inconsistent backgrounds.\nTherefore, we attempt to learn more generalized represen-\ntations containing richer texture and semantic features from\ninputs, i.e., invariant representations. To achieve it, adver-\nsarial domain adaption based discrete representation learn-\ning and self-supervised constraint modules are introduced\ninto our method. Details are described in the subsections.\n3.1. Discrete Representation Learning\nDiscrete representation aims to compute the latent code\nz from inputs, where z contains texture and semantic in-\nformation as much as possible.\nTo do so, we use two\nauto-encoders to model {EX , GX } and {EY, GY} sepa-\nrately. Given any unpaired samples (x, y), where x ∈X\nand y ∈Y separately denote noise and clean sample from\ndifferent domains, Eq. 1 is reformulated as zX = EX (x)\nand zY = EY(y).\nFurther, IR could be represented as\nF X→Y(x) = GY(zX ). However, considering noise always\nadheres to high-frequency signals, directly reconstructing\nclean images is difﬁcult due to varying noise levels and\ntypes, which requires powerful domain generator and dis-\ncriminator. Therefore, we introduce the disentangling rep-\nresentation into our architecture.\nDisentangling Representation. For noise sample x, an\nextra noise encoder EN\nX is used to model varying noisy\nlevels and types. The self-reconstruction is formulated by\nx = GX (zX , zN\nX ), where zX = EX (x) and zN\nX = EN\nX (x).\nAssuming the latent codes zX and zY obey same distri-\nbution in shared-space that {zX , zY} ∈Z, similar to im-\nage translation, unsupervised image restoration could be di-\nvided into two stages: forward translation and back recon-\nstruction.\nForward Cross Translation. We ﬁrst extract the repre-\nsentations {zX , zY} from (x, y) and extra noise code zN\nX .\nRestoration and degradation could be represented by\n˜xX→Y = GY(zX )\n(3)\n˜yY→X = GX (zY ⊕zN\nX )\n(4)\nwhere ˜xX→Y represents the recovered clean sample, ˜yY→X\ndenotes the degraded noise sample. ⊕represents channel-\nwise concatenation operation. GX and GY are viewed as\nspeciﬁc domain generators.\nBackward Cross Reconstruction. After performing the\nﬁrst translation, reconstruction could be achieved by swap-\nping the inputs ˜xX→Y and ˜yY→X that:\nˆx = GX (EY(˜xX→Y) ⊕EN\nX (˜yY→X ))\n(5)\nˆy = GY(EX (˜yY→X ))\n(6)\nwhere ˆx and ˆy denote reconstructed inputs. To enforce this\nconstraint, we add the cross-cycle consistency loss LCC for\nX and Y domains:\nLCC\nX (GX , GY, EX , EY, EN\nX ) =\nEX\n\u0002\n∥GX (EY(˜xX→Y) ⊕EN\nX (˜yY→X )) −x∥1\n\u0003 (7)\nLCC\nY (GX , GY, EX ,EY, EN\nX ) =\nEY\n\u0002\n∥GY(EX (˜yY→X ) −y∥1\n\u0003\n(8)\n4323\nFigure 3: Background Consistency Module (BCM). BCM\nhierarchically uses L1 loss at different Gaussian-Blur lev-\nels to ensure the inputs and outputs have consistency back-\nground.\nAdversarial Domain Adaption. Another factor is how to\nembed latent representations zX and zY into shared space.\nInspired by unsupervised domain adaption, we implement\nit by adversarial learning instead of shared-weight encoder.\nOur goal is to facilitate representations from inputs obeying\nthe similar distribution while preserving richer texture and\nsemantic information of inputs. Therefore, a representation\ndiscriminator DR is utilized in our architecture. We express\nthis feature adversarial loss LR\nadv as\nLR\nadv(EX , EY, DR) =\nEX\n\u00141\n2 log DR(zX ) + 1\n2 log(1 −DR(zX ))\n\u0015\n+\nEY\n\u00141\n2 log DR(zY) + 1\n2 log(1 −DR(zY))\n\u0015\n(9)\n3.2. Self-Supervised Constraint\nDue to lack of effective supervised signals for trans-\nlated images, only relying on feature domain discriminant\nconstraints would lead to domain shift problems inevitably\nin generated images. To speed convergence while learn-\ning more robust representations, self-supervised modules\nincluding Background Consistency Module (BCM) and Se-\nmantic Consistency Module (SCM) are introduced to pro-\nvide more reasonable and reliable supervision.\nBCM aims to preserve the background consistency be-\ntween the translated images and inputs. Similar strategies\nhave been applied for self-supervised image reconstruction\ntasks [14, 28]. These methods use the gradient error to con-\nstrain reconstructed images by smoothing the input and out-\nput images with blur operators, e.g., Gaussian blur kernel\nand guided ﬁltering [13]. Different from them, a L1 loss\nis directly used for the recovered images instead of gradient\nerror loss in our module, as shown in Fig. 3, which is simple\nbut effective to retain background consistency while recov-\nering ﬁner texture in our experiments. Speciﬁcally, a multi-\nscale Gaussian-Blur operator is used to obtain multi-scale\nfeatures respectively. Therefore, a background consistency\nloss LBC could be formulated as:\nLBC =\nX\nσ=5,9,15\nλσ∥Bσ(χ) −Bσ(˜χ)∥1\n(10)\nwhere Bσ(·) denotes the Gaussian-Blur operator with blur\nkernel σ, λσ is the hyper-parameter to balance the errors\nat different Gaussian-Blur levels.\nχ and ˜χ denote orig-\ninal input and the translated output, i.e., {x, ˜xX→Y} and\n{y, ˜yY→X }. Based on experimental attempts at image de-\nnoising, we set λσ as {0.25, 0.5, 1.0} for σ = {5, 9, 15}\nrespectively.\nIn addition, inspired by perception loss [15], the fea-\nture from the deeper layers of the pre-trained model con-\ntain semantic meanings only, which are noiseless or with\nlittle noise. Therefore, different from the general feature\nloss, which aims to recover ﬁner image texture details via\nsimilarities among shallow features, we only extract deeper\nfeatures as semantic representations from the corrupted and\nrecovered images to keep consistency, referred to as seman-\ntic consistency loss LSC. It could be formulated as\nLSC = ∥φl(χ) −φl(˜χ)∥2\n2\n(11)\nwhere φ(·) denotes the features from lth layer of the pre-\ntrained model. In our experiments, we use the conv5-1 layer\nof VGG-19 [31] pre-trained network on ImageNet.\n3.3. Jointly Optimizing\nOther than proposed cross-cycle consistency loss, repre-\nsentation adversarial loss and self-supervised loss, we also\nuse other loss functions in our joint optimization.\nTarget Domain Adversarial Loss. We impose domain\nadversarial loss Ldomain\nadv\n, where DX and DY attempt to\ndiscriminate the realness of generated images from each do-\nmain. For the noise domain, we deﬁne the adversarial loss\n4324\nFigure 4: The example results for Gaussian noise on BSD-68. Zooming in for better visualization.\nMethods\nBM3D[5]\nRedNet-30[26]\nDnCNN[39]\nN2N[19]\nDIP[32]\nCycleGAN[40]\nUNIT[21]\nDRNet[23]\nOurs\nPSNR(mean ± std)\nσ = 25\n30.18±2.07\n30.19±2.07\n30.70±2.04\n30.21±2.19\n26.48±3.14\n19.08±2.27\n20.21±1.45\n21.06±2.23\n29.02±1.93\nσ = 35\n28.09±2.17\n28.27±2.30\n28.75±2.10\n28.28±2.29\n26.06±2.78\n16.77±1.63\n18.96±1.29\n19.10±1.70\n27.58±1.98\nσ = 50\n25.87±2.31\n25.22±2.84\n26.54±2.15\n25.85±2.58\n24.80±2.25\n16.68±2.35\n17.10±1.08\n16.78±1.22\n24.69±1.59\nSSIM(mean ± std)\nσ = 25\n0.921±0.03\n0.918±0.03\n0.931±0.02\n0.919±0.03\n0.820±0.09\n0.808±0.06\n0.709±0.08\n0.626±0.09\n0.917±0.02\nσ = 35\n0.883±0.04\n0.885±0.04\n0.901±0.03\n0.886±0.04\n0.817±0.07\n0.731±0.07\n0.599±0.10\n0.505±0.09\n0.887±0.03\nσ = 50\n0.830±0.06\n0.827±0.06\n0.857±0.05\n0.832±0.06\n0.786±0.07\n0.696±0.06\n0.459±0.11\n0.374±0.08\n0.787±0.04\nTable 1: Quantitative results for Gaussian noise reduction on BSD-68 dataset.\nLX\nadv as\nLX\nadv = Ex∼PX (x) [log DX (x)] +\nEy∼PY(y)\nx∼PX (x)\n\u0002\nlog(1 −DX (GX (EY(y), EN\nX (x))))\n\u0003\n(12)\nSimilarly, we deﬁne adversarial loss for clean image domain\nas\nLY\nadv =Ey∼PY(y) [log DY(y)] +\nEx∼PX (x) [log(1 −DY(GY(EX (x))))]\n(13)\nSelf-Reconstruction Loss.\nIn addition to the cross-\ncycle reconstruction, we also apply a self-reconstruction\nloss LRec to facilitate the training. This process is repre-\nsented as ˆx = GX (EX (x)⊕EN\nX (x)) and ˆy = GY(EY(y)).\nKL Loss. In order to model the noise encoder branch,\nwe add a KL divergence loss to regularize the distribution\nof the noise code zN\nX\n= EN\nX (x) to be close to the nor-\nmal distribution that p(zN\nX\n∼N(0, 1)), where DKL =\n−\nR\np(z) log( p(z)\nq(z))dz.\nThe full objective function of our method is summarized\nas follows:\nmin\nEX ,EN\nX ,EY,GX ,GY\nmax\nDX ,DY,DR =λRLR\nadv+\nλadvLdomain\nadv\n+ λCCLCC + λrecLRec+\nλbcLBC+λscLSC + λKLLKL\n(14)\nwhere the hyper-parameters λ∗control the importance of\neach term.\nRestoration: After learning, we only retain the cross\nencoder-generator network {EX , GY}, EX extracts the\ndomain-invariant representation zX from corrupted sample\nx, and GY recover the clean image ˜xX→Y from the zX that\n˜xX→Y = GY(EX (x)).\n4. Experiments\nIn this section, we ﬁrst give the implementation details of\nour method for classical image denoising. Traditional met-\nrics, such as Peak-Signal-Noise-Rate (PSNR) and Struc-\ntural Similarity (SSIM), are used for evaluation in experi-\nments. Detailed results on synthetic and real noise removal\ntasks are shown with other state-of-the-art methods. For the\nsynthetic noise removal, we start with general noise distri-\nbutions including additive white Gaussian noise (AWGN)\n4325\nFigure 5: Sample results from Kodak dataset. Best detail visualization by zooming in.\nMethod\nPSNR(mean ± std) SSIM (mean ± std)\nDIP[32]\n27.63 ± 2.66\n0.838 ± 0.07\nN2N[19]\n28.39 ± 2.04\n0.893 ± 0.03\nANSC[25]\n30.68 ± 1.81\n0.918 ± 0.02\nRedNet-30[26]\n28.34 ± 2.07\n0.893 ± 0.03\nOurs\n32.37 ± 1.55\n0.957 ± 0.01\nTable 2: Quantitative results for Poisson noise.\nand Poisson noise. Two well-known datasets BSD68 [27]\nand Kodak are used to verify the performance of our method\nin denoising and texture restoration. Furthermore, the real\nnoise images from the medical Low-Dose Computed To-\nmography (LDCT) dataset are used to evaluate the general-\nized capacity of the method. Extra ablation study is used to\nverify the effectiveness of the proposed framework.\n4.1. Implementation\nWe follow the similar network architecture as the one\nused in [21], the difference is we introduce an extra noise\nencoder branch and remove the shared-weight encoder.\nRepresentation discriminator is a full convolutional net-\nwork structure, which stacks four convolutional layers with\ntwo strides and a global average pooling layer. Proposed\nframework is implemented with Pytorch [30] and an Nvidia\nTITAN-V GPU is used in experiments. During the train-\ning, we use Adam [16] to perform optimization and mo-\nmentum is set to 0.9. The learning rate is initially set to\n0.0001 and exponential decay over the 10K iterators. In\nall experiments, we randomly crop 64×64 patches with\nbatch size of 16 for training. Hyper-parameters are set to\nλR = λdomain\nadv\n= λsc = 1, λcc = λrec = 10, λbc = 5 and\nλKL = 0.01.\n4.2. Synthetic Noise Removal\nWe train the model with the images from the Pascal2007\n[6] training set. Samples are randomly divided into two\nparts without coinciding. We add different noise-levels to\neach sample in part one, which is viewed as corrupted set,\nand another is clean set. Proposed method needs to estimate\nthe magnitude of noise while removing it (â ˘AIJblindâ ˘A˙I im-\nage denoising). Some supervised and unsupervised based\nmethods are selected to evaluate.\nAWGN Removal. We add the AWGN with zero mean\nand standard deviation randomly generated with ranges\nfrom 5 to 50 for each training example, test on BSD68\nwith σ = {25, 35, 50}. The representative unsupervised\nmethods, including DIP [32], Noise2Noise (N2N) [19], Cy-\ncleGAN [40], UNIT [21] and DRNet [23], and supervised\nmethods (e.g., RedNet-30 [26] and DnCNN [39]), are se-\nlected to compare the performance on image denoising.\nTraditional BM3D is also included for evaluation. For Cy-\ncleGAN, UNIT and DRNet, we retrain them with the same\ntraining data.\nThe visualized results from BSD68 dataset are given in\nFig. 4. Although all the methods show the ability for noise\nreduction, domain transfer based unsupervised methods, in-\ncluding CycleGAN, UNIT and DRNet, have obvious domain\nshift problems, e.g., inconsistent brightness and undesired\nartifacts, resulting in worse visual perception. N2N and DIP\nachieve higher PSNR and SSIM. However, DIP loses ﬁne\nlocal details and leads to over-smoothness in the generated\nimages.\nDepending on the zero-mean distribution prior,\n4326\nN2N achieves similar results with other supervised meth-\nods, such as RedNet-30 and DnCNN. Our approach presents\ncomparable performance on noise removal and texture pre-\nserving. Although the PSNR is slightly lower than other\nsupervised methodsâ ˘A´Z, our method achieves better visual\nconsistency with natural images. Quantitative results for\nBSD68 are given in Table. 1. The proposed method shows\nstronger ability to blind image denoising.\nPoisson Noise Removal. For corrupted samples, we ran-\ndomly generate the noise data from Scikit-image library\n[33], which generates independent Poisson noise by the\nnumber of unique values in the given samples, and test on\nKodak1 dataset. Some representative methods, including\nDIP, N2N, ANSC [25] and RedNet-30, are selected in our\nevaluations.\nComprehensive results are shown in Fig. 5 and Table.\n2. DIP tends to generate more blurred results. The tradi-\ntional ANSC method ﬁrst transforms the Poisson noise into\nGaussian (Anscombe transform), then applies the BM3D\nto remove noise, and ﬁnally inverts the transform, achiev-\ning higher PSNR and SSIM. Considering the different way\nof generating Poisson noise, the published RedNet-30 and\nN2N models donâ ˘A´Zt achieve the best results. Our method\nachieves the highest PSNR and SSIM. In addition, visual-\nized results also show that for slight noise signals, the pro-\nposed framework has better generalized capacity to remove\nnoise while restoring ﬁner details.\n4.3. Real Noise Removal\nX-ray computed tomography (CT) is widely used as im-\nportant imaging modalities in modern clinical diagnosis.\nConsidering the potential radiation risk to the patient, low-\nering the radiation dose increases the noise and artifacts\nin reconstructed images, which can compromise diagnos-\ntic information. Typically, noise in x-ray photon measure-\nments can be simply modeled as the combination of Poisson\nquantum noise and Gaussian electronic noise. However, the\nnoise in reconstructed images is more complicated and does\nnot obey any statistical distribution across the whole image.\nTherefore, classical image post-processing methods based\non noise statistic prior, e.g., N2N, are unsuitable for Low-\ndose CT (LDCT) denoising.\nA real clinical dataset authorized by Mayo Clinic for the\n2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Chal-\nlenge2 is used to evaluate LDCT image reconstruction algo-\nrithms, which contains 5936 images in 512×512 resolution\nfrom 10 different subjects. We randomly select 4000 im-\nages as training set, the remaining is as testing set. DIP,\nBM3D and RedCNN [4], which is an extended version of\nRedNet, are selected for evaluation in our experiments. The\nrepresentative results are shown in Fig. 6, BM3D introduces\n1http://r0k.us/graphics/kodak/\n2http://www.aapm.org/GrandChallenge/LowDoseCT/\nFigure 6: LDCT Reconstruction. The display window is\n[160, 240]HU. The red circle denotes ROI area.\nMethods\nPSNR\nSSIM\nLDCT\n36.3616\n0.9423\nBM3D[5]\n40.6941\n0.9755\nRedCNN[4]\n41.8799\n0.9846\nDIP[32]\n36.2047\n0.9500\nOurs\n40.5857\n0.9811\nTable 3: Quantitative results on Mayo dataset.\nwaxy artifacts into the reconstructed image. DIP fails to\ngenerate the ﬁne local structures. RedCNN tends to gen-\nerate smoother images. Our approach achieves the better\nbalance between visual quality and noise removal. Table. 3\ngives the quantitative results.\n4.4. Ablation Study\nIn this section, we perform an ablation study to analyze\nthe effects of discrete disentangling representation and self-\nsupervised modules in the proposed framework. Both quan-\ntitative and qualitative results on Gaussian noise removal\nare shown for the following three variants of the proposed\nmethod where each component is separately studied: a) Re-\nmove the noise encoder branch; b) Remove the representa-\ntion adversarial network DR, directly learn the representa-\ntions zX and zY by the target domain constraints only; c)\nRemove the background consistency constraint from self-\nsupervised modules, only retain the semantic consistency\nconstraints.\nThe representative results are shown in Fig. 7. Com-\npared with the full model, referred to as (d), directly learn-\ning invariant representations from noise images would lead\nto the generator producing over-smooth results for (a) due\nto unexpected noise contained in features, which requires\n4327\nFigure 7: The visualized results for each variants. (a) Without EN\nX . (b) Without DR. (c) Removing BGM (d) Full model.\nVariants\nPSNR(mean ± std)\nSSIM (mean ± std)\n(a)\n25.997 ± 1.50\n0.828 ± 0.07\n(b)\n29.452 ± 1.71\n0.913 ± 0.02\n(c)\n25.220 ± 1.62\n0.817 ± 0.08\n(d)\n29.022 ± 1.93\n0.917 ± 0.02\nTable 4: Quantitative results for Gaussian noise with σ =\n25 on BSD-68.\n(a)\n(b)\nFigure 8: Online training PSNR and SSIM during the 100k\niterations.\na powerful domain generator. Although (b) gives the bet-\nter PSNR and SSIM after removing the feature adver-\nsarial module, some undesired artifacts adhere to high-\nfrequency signals.\nDue to failing to provide the effec-\ntive self-supervised constraint for the recovered images,\nalthough retaining the semantic consistency module, the\nmodel (c) also produces domain shift problems in generated\nimages, e.g., inconsistency brightness and blurred details,\nresulting in worse visual perception. Quantitative results\nare shown in Table. 4.\nIn addition, considering DRNet [23] has similar archi-\ntecture with ours, which extends DRIT [18] while introduc-\ning extra feature loss to solve image deblurring, we select it\nas a representative domain transfer method to compare the\nconvergence of algorithms on denoising task. Fig. 8 gives\nthe convergence plots for AWAN removal, where we trained\ntwo models from scratch on the same training set. Although\nDRNet also uses the similar idea of disentangled representa-\ntion to solve image restoration, which is different from ours\nin essence. Varying noise-levels and types lead to unstable\nlearning during training due to lack of clear domain bound-\nary. Aiming to learn invariant representation, our method\ngives faster and more stable convergence plots.\n5. Conclusion\nIn this paper, we propose an unsupervised learning\nmethod for image restoration. Speciﬁcally, we aim to learn\ninvariant representations from noise data via disentangling\nrepresentations and adversarial domain adaption. Aided by\neffective self-supervised constraints, our method could re-\nconstruct the higher-quality images with ﬁner details and\nbetter visual perception. Experiments on synthetic and real\nimage denoising show our method achieves comparable\nperformance with other state-of-the-art methods, and has\nfaster and more stable convergence than other domain adap-\ntion methods.\nAcknowledge\nThis work is supported by the National Natural Science\nFoundation of China under grant 61871277, and in part by\nthe Science and Technology Project of Sichuan Province of\nChina under grant 2019YFH0193.\n4328\nReferences\n[1] Joshua Batson and Loïc Royer. Noise2self: Blind denoising\nby self-supervision. In ICML, 2019.\n[2] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel. A\nnon-local algorithm for image denoising. 2005 IEEE Com-\nputer Society Conference on Computer Vision and Pattern\nRecognition (CVPR’05), 2:60–65 vol. 2, 2005.\n[3] Priyam Chatterjee and Peyman Milanfar. Clustering-based\ndenoising with locally learned dictionaries. IEEE Transac-\ntions on Image Processing, 18:1438–1451, 2009.\n[4] Hu Chen, Yi Zhang, Mannudeep Kalra, Feng Lin, Yang\nChen, Peixi Liao, Ji liu Zhou, and Ge Wang. Low-dose ct\nwith a residual encoder-decoder convolutional neural net-\nwork.\nIEEE Transactions on Medical Imaging, 36:2524–\n2535, 2017.\n[5] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and\nKaren O. Egiazarian.\nImage denoising by sparse 3-d\ntransform-domain collaborative ﬁltering. IEEE Transactions\non Image Processing, 16:2080–2095, 2007.\n[6] Mark Everingham,\nLuc Van Gool,\nChristopher K. I.\nWilliams, John M. Winn, and Andrew Zisserman. The pascal\nvisual object classes (voc) challenge. International Journal\nof Computer Vision, 88:303–338, 2009.\n[7] Yaroslav Ganin and Victor S. Lempitsky. Unsupervised do-\nmain adaptation by backpropagation. ArXiv, abs/1409.7495,\n2014.\n[8] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal\nGermain, Hugo Larochelle, FranÃ˘gois Laviolette, Mario\nMarchand, and Victor S. Lempitsky.\nDomain-adversarial\ntraining of neural networks. J. Mach. Learn. Res., 17:59:1–\n59:35, 2015.\n[9] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.\nImage style transfer using convolutional neural networks.\n2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2414–2423, 2016.\n[10] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,\nand Yoshua Bengio. Generative adversarial nets. In NIPS,\n2014.\n[11] Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu\nFeng. Weighted nuclear norm minimization with application\nto image denoising. 2014 IEEE Conference on Computer\nVision and Pattern Recognition, pages 2862–2869, 2014.\n[12] Shuhang Gu, Wangmeng Zuo, Qi Xie, Deyu Meng, Xi-\nangchu Feng, and Lei Zhang. Convolutional sparse coding\nfor image super-resolution. 2015 IEEE International Confer-\nence on Computer Vision (ICCV), pages 1823–1831, 2015.\n[13] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image ﬁl-\ntering. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 35:1397–1409, 2013.\n[14] Xin Jin, Zhibo Chen, Jianxin Lin, Zhikai Chen, and Wei\nZhou.\nUnsupervised single image deraining with self-\nsupervised constraints. 2019 IEEE International Conference\non Image Processing (ICIP), pages 2761–2765, 2018.\n[15] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nECCV, 2016.\n[16] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. CoRR, abs/1412.6980, 2014.\n[17] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug.\nNoise2void - learning denoising from single noisy images.\n2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 2124–2132, 2018.\n[18] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Ma-\nneesh Kumar Singh, and Ming-Hsuan Yang. Diverse image-\nto-image translation via disentangled representations. ArXiv,\nabs/1808.00948, 2018.\n[19] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli\nLaine,\nTero Karras,\nMiika Aittala,\nand Timo Aila.\nNoise2noise: Learning image restoration without clean data.\nArXiv, abs/1803.04189, 2018.\n[20] Ming Liu, Yukang Ding, Min Xia, Xiao Liu, Errui Ding,\nWangmeng Zuo, and Shilei Wen. Stgan: A uniﬁed selec-\ntive transfer network for arbitrary image attribute editing.\n2019 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 3668–3677, 2019.\n[21] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised\nimage-to-image translation networks. In NIPS, 2017.\n[22] Ming-Yu Liu and Oncel Tuzel. Coupled generative adversar-\nial networks. In NIPS, 2016.\n[23] Boyu Lu, Jun-Cheng Chen, and Rama Chellappa. Unsuper-\nvised domain-speciﬁc deblurring via disentangled represen-\ntations.\n2019 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 10217–10226, 2019.\n[24] Julien Mairal, Michael Elad, and Guillermo Sapiro. Sparse\nrepresentation for color image restoration. IEEE Transac-\ntions on Image Processing, 17:53–69, 2008.\n[25] Markku Mäkitalo and Alessandro Foi. Optimal inversion of\nthe anscombe transformation in low-count poisson image de-\nnoising. IEEE Transactions on Image Processing, 20:99–\n109, 2011.\n[26] Xiao-Jiao Mao, Chunhua Shen, and Yu-Bin Yang. Image\nrestoration using very deep convolutional encoder-decoder\nnetworks with symmetric skip connections. In NIPS, 2016.\n[27] David R. Martin, Charless C. Fowlkes, Doron Tal, and Jiten-\ndra Malik. A database of human segmented natural images\nand its application to evaluating segmentation algorithms and\nmeasuring ecological statistics.\nProceedings Eighth IEEE\nInternational Conference on Computer Vision. ICCV 2001,\n2:416–423 vol.2, 2001.\n[28] Thekke Madam Nimisha, Sunil Kumar, and A. N. Ra-\njagopalan. Unsupervised class-speciﬁc deblurring. In ECCV,\n2018.\n[29] Stanley Osher, Martin Burger, Donald Goldfarb, Jinjun Xu,\nand Wotao Yin. An iterative regularization method for total\nvariation-based image restoration. Multiscale Modeling &\nSimulation, 4:460–489, 2005.\n[30] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary Devito, Zeming Lin, Alban\nDesmaison, Luca Antiga, and Adam Lerer. Automatic dif-\nferentiation in pytorch. 2017.\n[31] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. CoRR,\nabs/1409.1556, 2014.\n4329\n[32] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky.\nDeep image prior. 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9446–9454, 2017.\n[33] Stéfan van der Walt, Johannes L. Schönberger, Juan Nunez-\nIglesias, FranÃ˘gois Boulogne, Joshua D. Warner, Neil\nYager, Emmanuelle Gouillart, and Tony Yu. scikit-image:\nimage processing in python. In PeerJ, 2014.\n[34] Luminita A. Vese and Stanley Osher. Image denoising and\ndecomposition with total variation minimization and oscilla-\ntory functions. Journal of Mathematical Imaging and Vision,\n20:7–18, 2004.\n[35] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. Extracting and composing robust\nfeatures with denoising autoencoders. In ICML ’08, 2008.\n[36] Junyuan Xie, Linli Xu, and Enhong Chen. Image denoising\nand inpainting with deep neural networks. In NIPS, 2012.\n[37] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dual-\ngan: Unsupervised dual learning for image-to-image trans-\nlation. 2017 IEEE International Conference on Computer\nVision (ICCV), pages 2868–2876, 2017.\n[38] Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang,\nChao Dong, and Liang Lin.\nUnsupervised image super-\nresolution using cycle-in-cycle generative adversarial net-\nworks.\n2018 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition Workshops (CVPRW), pages 814–\n81409, 2018.\n[39] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and\nLei Zhang. Beyond a gaussian denoiser: Residual learning of\ndeep cnn for image denoising. IEEE Transactions on Image\nProcessing, 26:3142–3155, 2017.\n[40] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.\nEfros.\nUnpaired image-to-image translation using cycle-\nconsistent adversarial networks.\n2017 IEEE International\nConference on Computer Vision (ICCV), pages 2242–2251,\n2017.\n[41] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-\nrell, Alexei A. Efros, Oliver Wang, and Eli Shechtman.\nToward multimodal image-to-image translation.\nArXiv,\nabs/1711.11586, 2017.\n[42] Daniel Zoran and Yair Weiss. From learning models of natu-\nral image patches to whole image restoration. 2011 Inter-\nnational Conference on Computer Vision, pages 479–486,\n2011.\n4330\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-03-28",
  "updated": "2020-03-28"
}