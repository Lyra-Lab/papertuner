{
  "id": "http://arxiv.org/abs/2003.13045v2",
  "title": "Learning by Analogy: Reliable Supervision from Transformations for Unsupervised Optical Flow Estimation",
  "authors": [
    "Liang Liu",
    "Jiangning Zhang",
    "Ruifei He",
    "Yong Liu",
    "Yabiao Wang",
    "Ying Tai",
    "Donghao Luo",
    "Chengjie Wang",
    "Jilin Li",
    "Feiyue Huang"
  ],
  "abstract": "Unsupervised learning of optical flow, which leverages the supervision from\nview synthesis, has emerged as a promising alternative to supervised methods.\nHowever, the objective of unsupervised learning is likely to be unreliable in\nchallenging scenes. In this work, we present a framework to use more reliable\nsupervision from transformations. It simply twists the general unsupervised\nlearning pipeline by running another forward pass with transformed data from\naugmentation, along with using transformed predictions of original data as the\nself-supervision signal. Besides, we further introduce a lightweight network\nwith multiple frames by a highly-shared flow decoder. Our method consistently\ngets a leap of performance on several benchmarks with the best accuracy among\ndeep unsupervised methods. Also, our method achieves competitive results to\nrecent fully supervised methods while with much fewer parameters.",
  "text": "Learning by Analogy: Reliable Supervision from Transformations\nfor Unsupervised Optical Flow Estimation\nLiang Liu1*\nJiangning Zhang1\nRuifei He1\nYong Liu1†\nYabiao Wang2\nYing Tai2 Donghao Luo2\nChengjie Wang2\nJilin Li2\nFeiyue Huang2\n1 Zhejiang University\n2Youtu Lab, Tencent\n{leonliuz, 186368, rfhe}@zju.edu.cn, yongliu@iipc.zju.edu.cn\n{caseywang, yingtai, michaelluo, jasoncjwang, jerolinli, garyhuang}@tencent.com\nAbstract\nUnsupervised learning of optical ﬂow, which leverages\nthe supervision from view synthesis, has emerged as a\npromising alternative to supervised methods. However, the\nobjective of unsupervised learning is likely to be unreliable\nin challenging scenes. In this work, we present a framework\nto use more reliable supervision from transformations. It\nsimply twists the general unsupervised learning pipeline by\nrunning another forward pass with transformed data from\naugmentation, along with using transformed predictions of\noriginal data as the self-supervision signal. Besides, we fur-\nther introduce a lightweight network with multiple frames\nby a highly-shared ﬂow decoder. Our method consistently\ngets a leap of performance on several benchmarks with the\nbest accuracy among deep unsupervised methods. Also, our\nmethod achieves competitive results to recent fully super-\nvised methods while with much fewer parameters.\n1. Introduction\nOptical ﬂow, as a motion description of images, has been\nwidely used in high-level video tasks [47, 48, 52, 3, 2, 31].\nBeneﬁtting from the growth of deep learning, learning-\nbased optical ﬂow methods [39, 30] with considerable ac-\ncuracy and efﬁcient inference are gradually replacing the\nclassical variational-based approaches [36, 25, 44]. How-\never, it is tough to collect the ground truth of dense optical\nﬂow in reality, which makes most supervised methods heav-\nily dependent on the large-scale synthetic datasets [7, 26],\nand the domain difference leads to an underlying degrada-\ntion when the model is transferred to the real-world.\nIn another point of view, many works proposed to learn\noptical ﬂow in an unsupervised way [37, 27, 42, 24], in\nwhich the ground truth is not necessary. These works aim to\ntrain networks with objective from view synthesis [51, 49],\n*Work mainly done during an internship at Tencent Youtu Lab.\n†Corresponding author.\nbefore\n2018\n2019\n2020\nYear\n3\n4\n5\n6\n7\n8\n9\nAEPE on Sintel Test Clean\nUnFlow-CSS\nOccAwareFlow\nMFOccFlow\nEpiFlow\nDDFlow\nSelFlow\nOurs\nOurs-MV\nFlowNet-S\nLiteFlowNet\nPWC-Net\nIRR-PWC\nSelFlow-ft\nContinualFlow\nUnsupervised\nSupervised\nFigure 1. Timeline of average end-point error (AEPE) advances in\ndeep optical ﬂow. Marker size indicates network size, and over-\nsized markers have been adjusted. Our method outperforms all of\nthe previous unsupervised methods, also yields comparable accu-\nracy to supervised methods while with fewer parameters. † indi-\ncates the model using more than two frames.\ni.e. optimizing the difference between reference images and\nthe ﬂow warped target images. This objective is based on\nthe assumption of brightness constancy, which will be vio-\nlated for challenging scenes, e.g. with extreme brightness or\npartial occlusion. Hence, proper regularization such as oc-\nclusion handling [42, 17] or local smooth [27] is required.\nRecent studies have focused on more complicate regular-\nizations such as 3D geometry constraints [34, 41, 22] and\nglobal epipolar constraints [50]. As shown in Fig. 1, there is\nstill a large gap between these works and supervised meth-\nods. In this paper, we do not rely on the geometrical regu-\nlarizations but rethink the task itself to improve accuracy.\nInterestingly, we notice that almost all of the unsuper-\nvised works, such as [42, 24, 41], avoid using a heavy com-\nbination of augmentations, even if it has been proven effec-\ntive in supervised ﬂow works [15, 38, 14]. The reason we\nconclude is two-fold: (i) Data augmentation is essentially a\ntrade-off between diversity and validity. It can improve the\nmodel by increasing the diversity of data, while also leads\nto a shift of data distribution which decreases the accuracy.\narXiv:2003.13045v2  [cs.CV]  29 Nov 2020\nIn unsupervised learning, the beneﬁt of diversity is limited\nsince the abundant training data is easy to access. (ii) Data\naugmentation will generate challenging samples, for which\nview synthesis is more likely to be unreliable, so the objec-\ntive cannot guide networks for a correct solution.\nMore recently, there are some works based on knowledge\ndistillation that alleviate the problem of unreliable objective\nin occluded regions [23, 24]. The training of these methods\nis split into two stages. In the ﬁrst stage, a teacher model\nis trained to make predictions on original data, and ofﬂine\ncreating occluded samples with random crop or mask out.\nIn the second stage, these artiﬁcial samples from the teacher\nmodel are used to update a student model. However, these\nmethods were designed for the case of partial occluded only.\nHence we ask: Can we generalize the distillation of occlu-\nsion to other transformation cases? Moreover, the distil-\nlation method has a bottleneck due to the frozen teacher\nmodel. We thus ask: Can we jointly optimize teacher model\nand student model, or just training a single network?\nIn this work, we address the above two questions with\na novel unsupervised learning framework of optical ﬂow.\nSpeciﬁcally, for the ﬁrst question, diverse transformations\nare used to generate challenging scenes such as low-light,\noverexposed, with large displacement or partial occlusion.\nFor the second question, instead of optimizing two models\nwith distillation, we simply twist the training step in the reg-\nular learning framework by running an additional forward\nwith the input of transformed images, and the transformed\nﬂow from the ﬁrst forward pass is treated as reliable su-\npervision. Since the self-supervision from transformations\navoids the unsupervised objective to be ambiguous in chal-\nlenging scenes, our framework allows the network to learn\nby analogy with the original samples, and gradually master-\ning the ability to handle challenging samples.\nIn summary, our contributions are: (i) We propose a\nnovel way to make use of the self-supervision signal from\nabundant augmentations for unsupervised optical ﬂow by\nonly training a single network; (ii) We demonstrate the ap-\nplicability of our method for various augmentation meth-\nods. In addition to occlusion, we develop a general form for\nmore challenging transformations. (iii) Our method leads in\na leap of performance among deep unsupervised methods.\nIt also achieves a comparable performance w.r.t. previous\nsupervised methods, but with much fewer parameters and\nexcellent cross dataset generalization capability.\n2. Related Work\nSupervised Optical Flow.\nStarting from FlowNet [7],\nvarious networks for optical ﬂow with supervised learning\nhave been proposed, e.g. FlowNet2 [15], PWC-Net [38],\nIRR-PWC [14]. These methods are comparable in accuracy\nto well-designed variational methods [36, 25], and are more\neffective during inference. However, the success of super-\nvised methods heavily dependent on the large scale syn-\nthetic datasets [26, 7], which leads to an underlying degra-\ndation when transferring to real-world applications. As an\nalternative, we dig into the unsupervised method to alleviate\nthe need for ground truth of dense optical ﬂow.\nUnsupervised Optical Flow.\nYu et al. [18] ﬁrst intro-\nduced a method for learning optical ﬂow with brightness\nconstancy and motion smoothness, which is similar to\nthe energy minimization in conventional methods.\nFur-\nther researches improve accuracy through occlusion rea-\nsoning [42, 27], multi-frame extension [17, 11], epipolar\nconstraint [50], 3D geometrical constraints with monocular\ndepth [53, 49, 34] and stereo depth [41, 22]. Although these\nmethods have become complicated, there is still a large gap\nwith state-of-the-art supervised methods. Recent works im-\nprove the performance by learning the ﬂow of occluded pix-\nels in a knowledge distillation manner [23, 24], while the\ntwo-stage training in these works is trivial. Instead of study-\ning the complicated geometrical constraints, our approach\nfocuses on the basic training strategy. It generalizes the case\nof occlusion distillation to more kinds of challenging scenes\nwith a straightforward single-stage learning framework.\nLearning with Augmentation. Data augmentation is one\nof the easiest ways to improve training.\nRecently, there\nhas been something new about integrating augmentation\ninto the learning frameworks. Mounsaveng et al. [29] and\nXiao et al. [45] suggested learning data augmentation with a\nspatial transformer network [16] to generate more complex\nsamples. Xie et al. [46] proposed to use augmentation in\nthe semi-supervised tasks by consistency training. Peng et\nal. [33] introduced to optimize data augmentation with the\ntraining of task-speciﬁc networks jointly. As a new trend in\nAutoML, several efforts to automatically search for the best\npolicy of augmentations [5, 12, 21] are proposed. All these\nmethods aimed at supervised or semi-supervised learning.\nIn this work, we present a simple yet effective approach to\nintegrate abundant augmentations with unsupervised opti-\ncal ﬂow. We propose to use reliable predictions of original\nsamples as a self-supervision signal to guide the predictions\nof augmented samples.\n3. Preliminaries\nThis work aims to learn optical ﬂow from images without\nthe need for ground truth. For completeness, we ﬁrst brieﬂy\nintroduce the general framework for unsupervised optical\nﬂow methods, which is shown in the left part of Fig. 2.\nGiven a dataset of image sequences I, our goal is to train\na network f(.) to predict dense optical ﬂow U12 for two\nconsecutive RGB frames {I1, I2} ∈I,\nU12 = f(I1, I2; Θ),\n(1)\nwhere Θ is the set of learnable parameters in the network.\nDespite the lack of direct supervision from ground truth,\nthe network can be trained implicitly with view synthesis.\nSpeciﬁcally, image I2 can be warped to synthesize the view\nof I1 with the prediction of optical ﬂow U12,\nˆI1(p) = I2 (p + U12(p)) ,\n(2)\nwhere p denotes pixel coordinates in the image, and bilinear\nsampling is used for the continuous coordinates. Then, the\nobjective of view synthesis, also known as photometric loss\nLph, can be formulated as:\nLph ∼\nX\np\nρ(ˆI(Θ), I),\n(3)\nwhere ρ(.) is a pixel-wise similarity measurement, e.g. ℓ1\ndistance or structural similarities (SSIM).\nNevertheless, the photometric loss is violated when pix-\nels are occluded or moved out of view so that there are\nno corresponding pixels in I2.\nAs a common practice\nin [27, 40], we denote these pixels by a binary occlusion\nmap O12. This map is obtained by the classical forward-\nbackward checking method, where the backward ﬂow is es-\ntimated by swapping the order of input images. The photo-\nmetric loss in the occluded region will be discarded.\nFurthermore, supervision solely based on the photomet-\nric loss is ambiguous for somewhere textureless or with\nrepetitive patterns. One of the most common ways to re-\nduce ambiguity is named smooth regularization,\nLsm ∼\nX\nd∈x,y\nX\np\n∥∇dU12∥1 e−|∇dI|,\n(4)\nwhich constrains the prediction similar to the neighbors in x\nand y directions when no signiﬁcant image gradient exists.\n4. Method\nSince the general pipeline suffers from unreliable super-\nvision for challenging cases, previous unsupervised works\navoid using heavy augmentations. In this section, we intro-\nduce a novel framework to reuse existing heavy augmen-\ntations that have been proven effective in the supervised\nscenario, but with different forms. The pipeline is shown\nin Fig. 2, and we will explain in detail next.\n4.1. Augmentation as a Regularization\nFormally, we deﬁne an augmentation parameterized by\na random vector θ as T img\nθ\n: It 7→It, from which one can\nsample augmented images {I1, I2} based on original im-\nages {I1, I2} in the dataset. In the general pipeline, the net-\nwork is trained with the data sampled from the augmented\ndataset. In contrast, we train the network on original data,\nbut leverage augmented samples as a regularization.\nI1\nI2\nU12\nU21\nO21\nˆI1\nLsm\nLph\nT img\nθ\nEq. (6)\nT ﬂo\nθ\nEq. (7)\nT occ\nθ\nEq. (8)\nI1\nI2\nU∗\n12\nLaug\nU12\nO12\nFigure 2. The pipeline of our proposed method. A complete train-\ning step includes two forwards: (i) The left side shows the ﬁrst\nforward with original samples by the regular pipeline introduced\nin Section 3. Then, we perform transformations on images, pre-\ndicted ﬂow, and occlusion map respectively to construct an aug-\nmented sample. (ii) The right side shows an additional forward\nwith the input of transformed images, and the output ﬂow is su-\npervised by the ﬂow prediction of original samples.\nMore speciﬁcally, after a regular forward pass for origi-\nnal images, we additionally run another forward for trans-\nformed images to predict the optical ﬂow U\n∗\n12. Meanwhile,\nthe prediction of optical ﬂow in the ﬁrst forward is trans-\nformed consistently by T ﬂo\nθ\n: U12 7→U12.\nThe basic assumption of our method is that augmenta-\ntion brings challenging scenes in which the unsupervised\nloss will be unreliable, while the transformed predictions of\noriginal data can provide reliable self-supervision. There-\nfore, we optimize the consistency for the transformed sam-\nples instead of the objective of view synthesis. We follow\nthe generalized Charbonnier function that commonly used\nin the supervised learning of optical ﬂow as:\nLaug ∼\nX\np\n\u0010\f\f\fS\n\u0000U12(p)\n\u0001\n−U\n∗\n12(p)\n\f\f\f + ϵ\n\u0011q\n,\n(5)\nwhere S(.) stands for stop-gradient, and the same setting\nas supervised work [38] with q = 0.4 and ϵ = 0.01 gives\nless penalty to outliers. For stability, we stop the gradients\nof Laug propagating to the transformed original ﬂow U12.\nAlso, only the loss in the non-occluded region is considered.\nAfter twice forwarding, the photometric loss Eq. (3), the\nsmooth regularization Eq. (4), and the augmentation regu-\nlarization Eq. (5) are backward at once to update the model.\nOur learning framework can be integrated with almost\nall types of augmentation methods. In the following, we\nsummarize three kinds of transformations, which compose\nthe common augmentations for the optical ﬂow task. Some\nexamples are shown in Fig. 3.\nTarget Image I2\nPredicted Flow U12\nGround Truth (Unused)\nTarget Image I2\nPredicted Flow U\n∗\n12\nTransformed Flow U12\n(a)\n(b)\nFigure 3. Some examples of the main idea. The same network is\nused to predict the optical ﬂow of original images and transformed\nimages, respectively. (a) Spatial transformation and appearance\ntransformation generate a scene with large displacement and low\nbrightness. (b) Occlusion transformation introduces additional oc-\nclusions. The pseudo label U12 that transformed from the original\npredictions U12 can provide reliable supervision.\nSpatial Transformation.\nWe assume the transformation\nthat results in a change in the location of pixels is called spa-\ntial transformation, which includes random crop, ﬂip, zoom,\nafﬁne transform, or more complicated transformations such\nas thin-plate-spline or CPAB transformations [8].\nHere we show a general form for these transformations.\nLet τθ be a transformation of pixel coordinates. The trans-\nformation of image T img\nθ\n: It 7→It can be formulated as:\nIt(p) = It (τθ(p)) ,\n(6)\nwhich can be implemented by a differentiable warping pro-\ncess, same as the one used in Eq. (2).\nSince changing pixel locations will lead to a change in\noptical ﬂow, we should warp on an intermediate ﬂow ﬁeld\neU12 instead of the original ﬂow. The transformation of op-\ntical ﬂow is T ﬂo\nθ\n: U12 7→U12 can be formulated as:\n( eU12(p) = τθ (p + U12(p)) −τθ (p) ,\nU12(p) = eU\n\u0000τ −1\nθ\n(p)\n\u0001\n.\n(7)\nAdditionally, the spatial transformation brings new oc-\nclusions. As we mentioned above, we explicitly reasoning\nocclusion from the predictions of bi-directional optical ﬂow.\nSince predictions of transformed samples are noisy, we in-\nfer the transformed occlusion map from original predictions\ninstead. The transformation T occ\nθ\n: O12 7→O12 consists of\ntwo parts: the old occlusion O\nold\n12(p) in the new view and the\nnew occlusion O\nnew\n12 (p) for pixels whose correspondences\nare out of the boundary Ω. The former can be obtained by\nthe same warping process as T img\nθ\nbut with nearest-neighbor\ninterpolation, and the latter can be explicitly estimated from\nthe ﬂow U12 by checking the boundary:\nO\nnew\n12 (p) =\n\u0000p + U12(p)\n\u0001\n/∈Ω.\n(8)\nThe ﬁnal transformed occlusion O12 is a union of these\ntwo parts. Note that, the non-occluded pixels in O\nold\n12 might\nbe occluded in O\nnew\n12 . It provides an effective way to learn\nthe optical ﬂow in occluded regions. For stability, only the\nnon-occluded pixels in O\nold\n12 contribute to the loss Laug.\nBesides, since we formulate the spatial transformation as\na warping process, there might be pixels out of boundary af-\nter transformation. The common solution, such as padding\nwith zero or the value of boundary pixels, will lead to se-\nvere artifacts. Therefore, we repeat sampling the transfor-\nmations until all transformed pixels are in the region of the\noriginal view. On the other hand, this strategy increases the\ndisplacement of the pixel in general.\nOcclusion Transformation.\nThe spatial transformation\nprovides reliable supervision for the ﬂow with large dis-\nplacement or occlusion around the boundary. As a com-\nplementary, recent work [23, 24] proposed to learn optical\nﬂow in arbitrary occluded regions with knowledge distilla-\ntion. The general learning process of these methods consists\nof training a teacher model, ofﬂine creating occluded sam-\nples, and distilling to a student model. We argue that the\nway of model distillation is too trivial, and there is a perfor-\nmance bottleneck due to the frozen teacher model.\nWe integrate the occlusion hallucination into our one-\nstage training framework and named as occlusion transfor-\nmation. Speciﬁcally, there are two steps: (i) Random crop.\nActually, random crop is a kind of spatial transformation,\nbut it efﬁciently creates new occlusion in the boundary. We\ncrop the pair of images as a preprocess of occlusion trans-\nformation. (ii) Random mask out. We randomly mask out\nsome superpixels in the target images with Gaussian noise,\nwhich will introduce new occlusion for the source image.\nNote that, we adopt a strategy consistent with the spa-\ntial transformation that only the pixels not occluded in O\nold\n12\ncontribute to Laug.\nIt is different from the previous dis-\ntillation works, in which they reasoning a new occlusion\nmap from the noisy prediction of transform images. Be-\nsides, in order to avoid creating transformed samples of-\nﬂine, we adopt a fast method of superpixel segmentation\nsimilar to [35]. The occlusion transformation in our frame-\nwork simpliﬁes the way of model distillation by optimizing\na single model in one-stage with end-to-end learning.\nAppearance Transformation. More transformations only\nchange the appearance of images, such as random color jit-\nter, random brightness, random blur, random noise. As a\nrelatively simple case, appearance transformation does not\nchange the location of pixels, nor introduce new occlusion.\nStill, the transformations lead to a risk for general methods,\ne.g. the photometric loss is meaningless when the image is\noverexposed, blurred, or in extremely low light. Instead,\nour method can exploit these transformations since the pre-\nFeature Volume Construction\nFlow Estimation\nSiamese Feature\nPyramid Network\nShared Decoder\nLeanable layer\nCorrelation\nNegation\n2x\n2x upsample with 2x upscale\nConv.\nWarp\nWarp\nLevel \n2x\n2x\nConv.−1\nWarp\nWarp\nLevel  −1\n2x\n2x\nI2\nI1\nI0\nxl−1\n0\nxl−1\n1\nxl−1\n2\nxl\n0\nxl\n1\nxl\n2\ncvl−1\n10\ncvl−1\n12\ncvl\n10\ncvl\n12\nUl\n10\nUl\n12\nUl+1\n10\nUl+1\n12\nUl−1\n12\nUl−1\n10\nF l−1\n10\nF l−1\n12\nF l\n10\nF l\n12\nF l\n12\nF l\n10\nF l−1\n10\nF l−1\n12\nFigure 4. Network architecture for our lightweight multi-frame extension of PWC-Net [38]. It shares a semi-dense ﬂow decoder for all of\nthe levels across the pyramid with both forward ﬂow and backward ﬂow. For simplicity and completeness, the pipeline of two levels in the\nfeature pyramid is displayed. Different line colors represent different levels of the process.\ndiction of the original sample provides a way to learn the\noptical ﬂow in challenging transformed scenes.\n4.2. Overall Objective and Convergence Analysis\nOur framework assumes that transformed predictions are\ngenerally more accurate than the predictions of transformed\nsamples, but what if samples are in the opposite case? In\nfact, we ensure convergence with the scope of each loss,\ni.e., which pixels affect each loss.\nAs shown in Fig. 2, the overall objective for a training\nstep consists of three loss terms in twice forwarding,\nLall = Lph (U12) + λ1Lsm (U12)\n|\n{z\n}\n1st forward\n+ λ2Laug(S(U12), U\n∗\n12)\n|\n{z\n}\n2nd forward\n,\n(9)\nin which the ﬁrst two terms propagate gradients for the orig-\ninal sample, and the last term is for the transformed sample.\nThe original data and the augmented data are treated differ-\nently. By setting a minor weight of λ2, we can ensure that\nthe original data is always dominant, so the effects of bad\ncases are limited. Moreover, the scope of the photometric\nloss Lph is the non-occluded pixels in O12. Thus the aug-\nmentation consistency loss becomes dominant for the new\noccluded pixels, which leads the network to learn the op-\ntical ﬂow with occlusion effectively. Besides, the scope of\naugmentation loss Laug avoids the network to be misguided\nfrom the original occluded predictions.\n4.3. Lightweight Network Architecture\nThe learning framework we proposed can be applied to\nany ﬂow networks. However, optical ﬂow often plays a role\nas a sub-module in high-level video tasks [47, 48, 31] where\nthe model size should be concerned. Hence, we introduce a\nlightweight architecture and extend it to multiple frames.\nWe start from a well-known network for the optical ﬂow\ntask named PWC-Net [38]. The original network shares a\nfeature encoder with a siamese feature pyramid network for\nthe images. For the level l in the pyramid, the feature maps\nof target image xl\n2 are aligned by warping operation with the\nﬂow prediction Ul+1\n12\nfrom the higher level. Then the cost\nvolume cvl\n12 is constructed with correlation operation. The\ninput for ﬂow decoder F l\n12 is organized by concatenating the\nfeature maps of source image xl\n1, the upsampled ﬂow from\nthe higher level Ul+1\n12 , and the cost volume cvl\n12. Finally,\nthe speciﬁc ﬂow decoder of level l predicts the optical ﬂow\nUl\n12. By iterating over the pyramid, the network predicts\noptical ﬂow at different scales.\nOur method follows the main pipeline of the original\nPWC-Net but with some modiﬁcations. The ﬂowchart of\nour multi-frame extension is shown in Fig. 4. We notice that\nthe majority of learnable parameters of PWC-Net is in the\nﬂow decoder of each feature level, so we take several steps\nto reduce the parameters: (i) The original implementation\nadopts a fully dense connection in each decoder, while we\nreduce the connections that only connections in the nearest\ntwo layers are retained. (ii) We share the ﬂow decoder for\nall of the levels across the pyramid, with an additional con-\nvolution layer for each level to align the feature maps. (iii)\nWe extend the model to multiple frames by repeating the\nwarping and correlation to the backward features. The ﬂow\ndecoder is shared for both forward ﬂow and backward ﬂow\nin the multi-frame extension by changing the sign of optical\nﬂow and the order in feature concatenation.\n5. Experimental Results\n5.1. Implementation Details\nWe implement our end-to-end approach in PyTorch [32].\nAll models are trained by Adam optimizer [19] with β1 =\n0.9, β2 = 0.99, batch size of 4. The learning rate is 10−4\nwithout adjustment during training. The loss weights for\nregularizations are set to λ1 = 60 and λ2 = 0.01 for all\ndatasets. In addition, an optional pre-training can be used\nfor better results, which is under almost the same setting\nabove, but with λ2 = 0, i.e. a regular training step without\nthe transformed pass in forward 1.\nOnly random ﬂip and random time order switch are per-\nformed as the regular data augmentation. The heavy com-\nbination of augmentations in supervised works [15, 38, 13]\nare used as the appearance transformation and spatial trans-\nformation in our framework, including random rotate, trans-\nlate, zoom in, as well as additive Gaussian noise, Gaussian\nblur and random jitter in brightness, color, and contrast.\n5.2. Datasets\nWe ﬁrst evaluate our method on three well-established\noptical ﬂow benchmarks, MPI Sintel [1], KITTI 2012 [10],\nand KITTI 2015 [28]. Then, we conduct a cross dataset ex-\nperiment with another optical ﬂow dataset FlyingChairs [7]\nand a segmentation dataset CityScapes [4].\nWe follow a similar data setting in previous unsupervised\nworks [23, 24]. For the MPI Sintel benchmark, we extract\nall frames from the raw movie and manually group frames\nby shots for pre-training, which consists of 14,570 image\npairs. Then, the model is ﬁne-tuned on the standard train-\ning set, which provides 1,041 image pairs with two differ-\nent rendering passes (“Clean” and “Final”). For the KITTI\n2012 and KITTI 2015, we pre-train the model on the KITTI\nraw dataset [9], but discard scenes that contain images ap-\npeared in the optical ﬂow benchmarks. The pre-training set\nconsists of 28,058 image pairs. Then the model is ﬁne-tuned\non the multi-view extension data, but discards samples con-\ntaining frames related to validation, i.e. numbers 9-12. The\nﬁnal training set consists of 6,000 samples for our basic\nmodel and 3,600 samples for the multi-frame model.\n5.3. Comparison with State-of-the-art\nWe compare our method with both supervised and un-\nsupervised methods on optical ﬂow benchmarks. Standard\nmetrics for optical ﬂow are used, including average end-\npoint error (AEPE), and percentage of erroneous pixels (Fl).\nTable 1 reports the results on MPI Sintel benchmark.\nOur basic two-frame model “ARFlow” outperforms all pre-\nvious unsupervised works with the least parameters. Fur-\nthermore, our multi-frame model “ARFlow-MV” reduces\n1Code available at https://github.com/lliuz/ARFlow.\nMethod\nSintel Training\nSintel Test\n# Param.\nClean\nFinal\nClean\nFinal\nSupervised\nFlowNetS-ft [7]\n(3.66)\n(4.44)\n6.96\n7.76\n32.07 M\nLiteFlowNet-ft[13]\n(1.64)\n(2.23)\n4.86\n6.09\n5.37 M\nPWC-Net-ft[38]\n(2.02)\n(2.08)\n4.39\n5.04\n8.75 M\nIRR-PWC-ft [14]\n(1.92)\n(2.51)\n3.84\n4.58\n6.36 M\nSelFlow-ft† [24]\n(1.68)\n(1.77)\n3.74\n4.26\n4.79 M\nUnsupervised\nUnFlow-CSS [27]\n-\n(7.91)\n9.38\n10.22\n116.58 M\nOccAwareFlow [42]\n(4.03)\n(5.95)\n7.95\n9.15\n5.12 M\nMFOccFlow† [17]\n(3.89)\n(5.52)\n7.23\n8.81\n12.21 M\nEpiFlow train-ft [50]\n(3.54)\n(4.99)\n7.00\n8.51\n8.75 M\nDDFlow [23]\n(2.92)\n(3.98)\n6.18\n7.40\n4.27 M\nSelFlow† [24]\n(2.88)\n(3.87)\n6.56\n6.57\n4.79 M\nOurs (ARFlow)\n(2.79)\n(3.73)\n4.78\n5.89\n2.24 M\nOurs (ARFlow-MV†)\n(2.73)\n(3.69)\n4.49\n5.67\n2.37 M\nTable 1. MPI Sintel Flow: AEPE and the number of CNN pa-\nrameters are reported. Missing entry (-) means that the results are\nnot reported for the respective method, and † indicates the model\nusing more than two frames.\nMethod\nKITTI 2012\nKITTI 2015\ntraining\ntest\ntraining\ntest (F1)\nSupervised\nFlowNet2-ft [15]\n(1.28)\n1.8\n(2.30)\n11.48%\nLiteFlowNet-ft [13]\n(1.26)\n1.7\n(2.16)\n11.48%\nPWC-Net-ft [38]\n(1.45)\n1.7\n(2.16)\n9.60%\nSelFlow-ft† [24]\n(0.76)\n1.5\n(1.18)\n8.42%\nUnsupervised\nBridgeDepthFlow§ [20]\n2.56\n–\n7.02\n–\nCCFlow§ [34]\n–\n–\n5.66\n25.27%\nUnOS-stereo§ [41]\n1.64\n1.8\n5.58\n18.00%\nEpiFlow-train-ft§ [50]\n(2.51)\n3.4\n(5.55)\n16.95%\nDDFlow [23]\n2.35\n3.0\n5.72\n14.29%\nSelFlow† [24]\n1.69\n2.2\n4.84\n14.19%\nOurs (ARFlow)\n1.44\n1.8\n2.85\n11.80%\nOurs (ARFlow-MV†)\n1.26\n1.5\n3.46\n11.79%\nTable 2. KITTI Optical Flow 2012 and 2015: AEPE and Fl are\nreported. For unsupervised methods, only the works published in\n2019 are shown. Missing entry (-) means that the results are not re-\nported for the respective method. † indicates the model using more\nthan two frames. § indicates training with geometrical constraints.\nthe previous best AEPE from 6.18 [23] to 4.49 on the clean\npass, with 27.3% improvement, and from 6.57 [24] to 5.67\non the ﬁnal pass, with 13.7% improvement.\nAs for KITTI benchmarks, Table 2 shows a signiﬁcant\nimprovement. On the training set, we achieve AEPE=1.26\nwith 25.4% relative improvement on KITTI 2012 and\nAEPE=2.85 with 41.2% improvement on KITTI 2015 w.r.t.\nthe previous best unsupervised method [24]. On the test set,\nour method reaches the best AEPE=1.5 and F1-all=11.79%\namong unsupervised methods, respectively.\nSeveral representative supervised methods are also re-\nported as a reference. As a result, our unsupervised models\nﬁrstly reach or approach some powerful fully supervised\nmethods such as LiteFlowNet [13], PWC-Net [38], even\nwith 27.1% parameters of PWC-Net.\nSamples on MPI Sintel and KITTI are shown in Fig. 5.\nCompared with the state-of-the-art competitor [24], for the\nlow light and large displacement scenes in MPI Sintel, our\nmethod maintains better performance in general and is more\n(a) Reference Image\n(b) Our Predictions\n(c) SelFlow [24] Predictions\n(d) Our Error\n(e) SelFlow Error [24]\nFigure 5. Qualitative visualization comparing with unsupervised SelFlow [24]. The ﬁrst two rows are from the Sintel Final pass, where the\nerrors are visualized in gray. The last two rows are from KITTI 2015, in which the correct predictions are depicted in blue and the wrongs\nin red for the error visualization. More samples will be available on the website of corresponding benchmarks.\nModel Architecture\nAR\nSintel Clean\nSintel Final\n# Param.\nALL\nNOC\nOCC\nALL\nNOC\nOCC\nPWC-Net [38]\n2.48\n1.19\n21.71\n3.47\n1.98\n25.19\n8.75 M\nPWC-Net-small [38]\n2.76\n1.28\n23.92\n3.62\n2.16\n28.15\n4.05 M\n+ Reduce Dense\n2.53\n1.23\n21.36\n3.47\n2.03\n24.20\n5.32 M\n\u0013\n2.04\n0.90\n18.47\n2.97\n1.72\n21.05\n+ Share Decoder\n2.30\n1.08\n20.00\n3.19\n1.84\n22.77\n2.24 M\n\u0013\n1.95\n0.85\n17.85\n2.86\n1.66\n20.25\n+ Multipe Frames\n2.24\n1.04\n19.60\n3.18\n1.86\n22.36\n2.37 M\n\u0013\n1.89\n0.86\n16.79\n2.85\n1.66\n20.02\nTable 3. Ablation study of our learning framework with mul-\ntiple model architectures. AEPE in speciﬁc regions of the scene\nand the number of CNN parameters are reported. AR: Training\nwith augmentation as a regularization framework.\naccurate around the boundaries.\nFor KITTI results, the\nshapes in our optical ﬂow are more structured for objects\nand more accurate in texture-less regions.\n5.4. Ablation Study\nTo further analyze the capability of each component, we\nconduct four groups of ablation studies. We randomly re-\nsplit the Sintel training set into a new training set and a val-\nidation set by the scene. We evaluate AEPE in different\nregions over all pixels (ALL), non-occluded pixels (NOC),\noccluded pixels (OCC), and according to speed (s0-10, s10-\n40 and s40+ are pixels that move less than 10 pixels, be-\ntween 10 and 40, and more than 40, respectively)\nMain Ablation.\nTable 3 assesses the overall improvement\nof our augmentation as a regularization learning framework\nunder multiple model architectures. Our framework consis-\ntently improves the accuracy of optical ﬂow over 10% for all\narchitectures, whether for occluded or non-occluded pixels.\nFor the consideration of the number of model param-\neters, we start from the original PWC-Net and a variant\nnamed PWC-Net-small without dense connections in the\nﬂow decoders [38]. Although removing dense connections\ncan reduce half parameters, it leads to severe performance\nST\nAT\nOT\nALL\nNOC\nOCC\ns0-10\ns10-40\ns40+\nSintel Clean\n2.53\n1.23\n21.36\n0.61\n2.74\n24.30\n\u0013\n2.39\n1.20\n19.61\n0.61\n2.56\n23.14\n\u0013\n2.40\n1.13\n20.67\n0.61\n2.81\n21.95\n\u0013\n2.14\n1.00\n18.63\n0.62\n2.83\n17.56\n\u0013\n\u0013\n2.09\n0.95\n18.90\n0.59\n2.65\n18.03\n\u0013\n\u0013\n\u0013\n2.04\n0.90\n18.47\n0.61\n2.55\n17.05\nSintel Final\n3.47\n2.03\n24.20\n0.82\n3.77\n33.48\n\u0013\n3.23\n1.93\n21.98\n0.82\n3.48\n30.78\n\u0013\n3.36\n1.94\n23.95\n0.81\n3.70\n32.17\n\u0013\n3.04\n1.78\n21.25\n0.78\n3.55\n27.80\n\u0013\n\u0013\n3.01\n1.76\n21.40\n0.75\n3.48\n28.48\n\u0013\n\u0013\n\u0013\n2.97\n1.72\n21.05\n0.77\n3.40\n27.25\nTable 4.\nComparison of combinations of transformations.\nAEPE in speciﬁc regions are reported. ST: Spatial transformation,\nAT: Appearance transformation, OT: Occlusion transformation.\ndegradation. In contrast, our reduced dense variant main-\ntains the performance while reducing 39.2% parameters.\nSharing decoder across feature pyramid yields an improve-\nment on ﬂow with only 25.6% parameters of the original\nmodel. The multi-frame extension reaches the best perfor-\nmance with the minimal extra overhead of parameters.\nCombination of Transformations.\nFurthermore,\nwe\ndelve into the type of transformations in our framework. Ta-\nble 4 shows the performance of the model trained with\nseveral combinations of the three kinds of transformations.\nThere are some critical observations: (i) Each transforma-\ntion can improve the performance individually. (ii) Spatial\ntransformation is the most helpful to all measurements, es-\npecially for large displacement estimation. (iii) The accu-\nracy in the occluded region can be signiﬁcantly improved\nby occlusion transformation or spatial transformation. All\nthese observations are consistent with our assumption that\nthe transformation will introduce new challenging scenes,\nand our approach can provide reliable supervision.\nUsage of Augmentation.\nAs we mentioned above, almost\nall of the unsupervised learning approaches avoid using a\nheavy combination of augmentations. As a reference, we\nMethod\nSintel Clean\nSintel Final\nALL\ns0-10\ns10-40\ns40+\nALL\ns0-10\ns10-40\ns40+\nWithout Aug.\n2.53\n0.61\n2.74\n24.30\n3.47\n0.82\n3.77\n33.48\nAug. Directly\n2.71\n0.69\n3.11\n27.13\n3.80\n0.95\n4.03\n35.90\nAug. Distillation\n2.36\n0.64\n2.61\n19.90\n3.31\n0.86\n3.50\n30.18\nOurs(aug. as reg.)\n2.04\n0.61\n2.55\n17.05\n2.97\n0.77\n3.40\n27.25\nTable 5. Comparison of our learning framework with direct data\naugmentation and the data distillation framework used in [23, 24].\nMethod\nSintel Clean\nSintel Final\nALL\ns0-10\ns10-40\ns40+\nALL\ns0-10\ns10-40\ns40+\nWithout Aug.\n2.53\n0.61\n2.74\n24.30\n3.47\n0.82\n3.77\n33.48\nCPAB [8] + AT\n2.38\n0.61\n2.78\n21.60\n3.32\n0.81\n3.59\n31.09\nAutoAugment [5]\n2.30\n0.62\n2.59\n21.18\n3.29\n0.81\n3.53\n30.11\nOurs(ST + AT)\n2.09\n0.59\n2.65\n18.03\n3.01\n0.75\n3.48\n28.48\nTable 6. Comparison of different augmentation transformations\nintegrated with our framework. AT: appearance transformation,\nST: spatial transformation.\nevaluate the same transformations with different usages.\nTable 5 reports the results of (i) training without heavy aug-\nmentation, (ii) using transformation as a regular data aug-\nmentation and training directly, (iii) training with data dis-\ntillation that similar in [23, 24], (iv) training with the learn-\ning framework we proposed. The results show that directly\naugmentation makes all metrics worse. Instead of apply-\ning transformations directly, distillation alleviates the prob-\nlem of unreliable supervision. However, the frozen teacher\nmodel is still a bottleneck for the student model. Also, the\ntedious multi-stage training process of knowledge distilla-\ntion is undesired. Our framework avoids the unreliable pho-\ntometric loss for the transformed samples. It achieves the\nbest results with a single-stage optimization.\nIntegrate Complicated Augmentation.\nBy implement-\ning the corresponding transformation of optical ﬂow and\nocclusion map, our framework can be integrated with al-\nmost all types of augmentation. We assess a complicated\nspatial transformation called CPAB [8] and a recent work\nin AutoML on searching for the best augmentation policy\ncalled AutoAugment [5]. Note that random zoom in is ap-\nplied ﬁrst to avoid invalid coordinate values of transforma-\ntions. Table 6 shows that both strategies integrated with our\nframework can improve accuracy. Note that AutoAugment\nis too time consuming for our task, therefore we adopt the\nﬁnal policy searched from ImageNet [6] classiﬁcation task.\nIt is promising that our framework with AutoAugment will\nbe further improved with policy ﬁne-tuning.\n5.5. Cross Dataset Generalization\nAlthough deep optical ﬂow methods have been far ahead\nof the most popular classical variational method TV-L1 [43]\non optical ﬂow benchmarks, the latter has not gone away.\nOne possible reason is that supervised learning methods are\nprone to overﬁtting, which results in poor generalization\nwhen transferring to high-level video tasks.\nMethod\nTraining\nChairs\nSintel\nSintel\nKIITI\nKITTI\nSet\nFull\nClean\nFinal\n2012\n2015\nPWC-Net [38]\nSintel\n3.69\n(1.86)\n(2.31)\n3.68\n10.52\nOurs(ARFlow)\nSintel\n3.50\n(2.79)\n(3.73)\n3.06\n9.04\nCityScapes\n5.10\n5.22\n6.01\n2.11\n5.33\nTable 7. Generalization performance of cross datasets evaluation.\nThe numbers indicate AEPE on each dataset. For KITTI and Sin-\ntel, the results are evaluated on the training set. () indicates the\nresults of a dataset that the method has been trained on.\nHence, we report the cross dataset accuracy in Table 7, in\nwhich our unsupervised method is compared with a fully su-\npervised method PWC-Net [38]. The supervised PWC-Net\nconsistently outperforms for the dataset that the model is\ntrained on, while our unsupervised method works much bet-\nter when transferring to other datasets. In addition, we train\na model on an urban street dataset named CityScapes [4],\nin which 50,625 image pairs are used for training without\nthe ground truth. This model performs best on the KITTI\n2012 and KITTI 2015 than any other model trained on the\nsynthetic dataset. Our method makes it possible to ﬁt the\ndomain of high-level video tasks by training a model on the\nunlabeled videos from that domain.\nRemarkably, despite the lack of cross dataset results\nfrom other unsupervised methods, the accuracy of our\nmodel trained on CityScapes is even better than most of\nthe previous works trained on KITTI (c.f. Table 2), which\nshows the superiority of our method. The results demon-\nstrate a signiﬁcant improvement of our method for unsuper-\nvised optical ﬂow task with an excellent generalization.\n6. Conclusion\nWe proposed a novel framework that learns optical ﬂow\nfrom unlabeled image sequences with the self-supervision\nfrom augmentations. To avoid the objective of view syn-\nthesis being unreliable on transformed data, we twist the\nbasic learning framework by adding another forward pass\nfor transformed images, where the supervision is from\nthe transformed prediction of original images. Besides, a\nlightweight network and its multi-frame extension were pre-\nsented. Extensive experiments have shown that our methods\nsigniﬁcantly improve accuracy, with high compatibility and\ngeneralization ability. We believe that our learning frame-\nwork can be further combined with other geometrical con-\nstraints or transferred to other visual geometry tasks, such\nas depth or scene ﬂow estimation.\nAcknowledgment We thank anonymous reviewers for their\nconstructive comments, and LL would like to thank Peng-\npeng Liu for helpful suggestions. This work is partially sup-\nported by the National Natural Science Foundation of China\n(NSFC) under Grant No. 61836015 and Key R&D Program\nProject of Zhejiang Province (2019C01004).\nReferences\n[1] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and\nMichael J Black. A naturalistic open source movie for op-\ntical ﬂow evaluation. In European Conference on Computer\nVision (ECCV), 2012. 6\n[2] Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang\nHua. Coherent online video style transfer. In IEEE Interna-\ntional Conference on Computer Vision (ICCV), 2017. 1\n[3] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and Ming-\nHsuan Yang. Segﬂow: Joint learning for video object seg-\nmentation and optical ﬂow. In IEEE International Confer-\nence on Computer Vision (ICCV), 2017. 1\n[4] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld,\nMarkus Enzweiler,\nRodrigo Benenson,\nUwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene understanding.\nIn IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2016. 6, 8\n[5] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\npolicies from data. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2019. 2, 8\n[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2009. 8\n[7] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick Van\nDer Smagt, Daniel Cremers, and Thomas Brox. Flownet:\nLearning optical ﬂow with convolutional networks. In IEEE\nInternational Conference on Computer Vision (ICCV), 2015.\n1, 2, 6\n[8] Oren Freifeld, Søren Hauberg, Kayhan Batmanghelich, and\nJonn W Fisher.\nTransformations based on continuous\npiecewise-afﬁne velocity ﬁelds. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence (TPAMI), 2017. 4,\n8\n[9] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. Interna-\ntional Journal of Robotics Research (IJRR), 2013. 6\n[10] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2012. 6\n[11] Shuosen Guan, Haoxin Li, and Wei-Shi Zheng. Unsuper-\nvised learning for optical ﬂow estimation using pyramid con-\nvolution lstm. In IEEE International Conference on Multi-\nmedia and Expo (ICME), 2019. 2\n[12] Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi\nChen. Population based augmentation: Efﬁcient learning of\naugmentation policy schedules. In International Conference\non Machine Learning (ICML), 2019. 2\n[13] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Lite-\nﬂownet: A lightweight convolutional neural network for op-\ntical ﬂow estimation. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2018. 6\n[14] Junhwa Hur and Stefan Roth. Iterative residual reﬁnement\nfor joint optical ﬂow and occlusion estimation.\nIn IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2019. 1, 2, 6\n[15] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keu-\nper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0:\nEvolution of optical ﬂow estimation with deep networks. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion(CVPR), 2017. 1, 2, 6\n[16] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2015. 2\n[17] Joel Janai, Fatma Guney, Anurag Ranjan, Michael Black,\nand Andreas Geiger. Unsupervised learning of multi-frame\noptical ﬂow with occlusions. In European Conference on\nComputer Vision (ECCV), 2018. 1, 2, 6\n[18] J Yu Jason, Adam W Harley, and Konstantinos G Derpanis.\nBack to basics: Unsupervised learning of optical ﬂow via\nbrightness constancy and motion smoothness. In European\nConference on Computer Vision (ECCV), 2016. 2\n[19] Diederik P Kingma and Jimmy Ba.\nAdam: A method\nfor stochastic optimization. In International Conference on\nLearning Representations (ICLR), 2015. 6\n[20] Hsueh-Ying Lai, Yi-Hsuan Tsai, and Wei-Chen Chiu. Bridg-\ning stereo matching and optical ﬂow via spatiotemporal cor-\nrespondence. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2019. 6\n[21] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and\nSungwoong Kim. Fast autoaugment. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2019. 2\n[22] Liang Liu, Guangyao Zhai, Wenlong Ye, and Yong Liu. Un-\nsupervised learning of scene ﬂow estimation fusing with lo-\ncal rigidity. In International Joint Conference on Artiﬁcial\nIntelligence, (IJCAI), 2019. 1, 2\n[23] Pengpeng Liu, Irwin King, Michael R Lyu, and Jia Xu.\nDdﬂow: Learning optical ﬂow with unlabeled data distilla-\ntion. In AAAI Conference on Artiﬁcial Intelligence (AAAI),\n2019. 2, 4, 6, 8\n[24] Pengpeng Liu, Michael Lyu, Irwin King, and Jia Xu. Self-\nlow: Self-supervised learning of optical ﬂow.\nIn IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2019. 1, 2, 4, 6, 7, 8\n[25] Daniel Maurer and Andr´es Bruhn.\nProﬂow: Learning to\npredict optical ﬂow.\nBritish Machine Vision Conference\n(BMVC), 2018. 1, 2\n[26] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,\nDaniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A\nlarge dataset to train convolutional networks for disparity,\noptical ﬂow, and scene ﬂow estimation. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2016. 1, 2\n[27] Simon Meister, Junhwa Hur, and Stefan Roth.\nUnﬂow:\nUnsupervised learning of optical ﬂow with a bidirectional\ncensus loss. In AAAI Conference on Artiﬁcial Intelligence\n(AAAI), 2018. 1, 2, 3, 6\n[28] Moritz Menze and Andreas Geiger. Object scene ﬂow for au-\ntonomous vehicles. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2015. 6\n[29] Saypraseuth Mounsaveng, David Vazquez, Ismail Ben Ayed,\nand Marco Pedersoli. Adversarial learning of general trans-\nformations for data augmentation.\nIn IEEE International\nConference on Computer Vision (ICCV), 2019. 2\n[30] Michal Neoral, Jan ˇSochman, and Jiˇr´ı Matas. Continual oc-\nclusion and optical ﬂow estimation. In Asian Conference on\nComputer Vision (ACCV), 2018. 1\n[31] David Nilsson and Cristian Sminchisescu. Semantic video\nsegmentation by gated recurrent ﬂow propagation. In IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2018. 1, 5\n[32] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban\nDesmaison, Luca Antiga, and Adam Lerer. Automatic dif-\nferentiation in pytorch. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2017. 6\n[33] Xi Peng, Zhiqiang Tang, Fei Yang, Rogerio S Feris, and\nDimitris Metaxas. Jointly optimize data augmentation and\nnetwork training: Adversarial data augmentation in human\npose estimation. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2018. 2\n[34] Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim,\nDeqing Sun, Jonas Wulff, and Michael J Black. Compet-\nitive collaboration: Joint unsupervised learning of depth,\ncamera motion, optical ﬂow and motion segmentation. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2019. 1, 2, 6\n[35] C. Y Ren, V. A. Prisacariu, and I. D Reid. gSLICr: SLIC su-\nperpixels at over 250Hz. ArXiv pre-prints:1509.04232, 2015.\n4\n[36] Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang,\nErik Sudderth, and Jan Kautz. A fusion approach for multi-\nframe optical ﬂow estimation. In IEEE Winter Conference\non Applications of Computer Vision (WACV), 2019. 1, 2\n[37] Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang,\nand Hongyuan Zha. Unsupervised deep learning for optical\nﬂow estimation. In AAAI Conference on Artiﬁcial Intelli-\ngence (AAAI), 2017. 1\n[38] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nPwc-net: Cnns for optical ﬂow using pyramid, warping, and\ncost volume. In IEEE Conference on Computer Vision and\nPattern Recognition(CVPR), 2018. 1, 2, 3, 5, 6, 7, 8\n[39] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nModels matter, so does training: An empirical study of cnns\nfor optical ﬂow estimation. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI), 2019. 1\n[40] Narayanan Sundaram, Thomas Brox, and Kurt Keutzer.\nDense point trajectories by gpu-accelerated large displace-\nment optical ﬂow.\nIn European Conference on Computer\nVision (ECCV), 2010. 3\n[41] Yang Wang, Peng Wang, Zhenheng Yang, Chenxu Luo, Yi\nYang, and Wei Xu.\nUnos: Uniﬁed unsupervised optical-\nﬂow and stereo-depth estimation by watching videos.\nIn\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2019. 1, 2, 6\n[42] Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng\nWang, and Wei Xu. Occlusion aware unsupervised learning\nof optical ﬂow. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2018. 1, 2, 6\n[43] Andreas Wedel, Thomas Pock, Christopher Zach, Horst\nBischof, and Daniel Cremers. An improved algorithm for\ntv-l 1 optical ﬂow. In Dagstuhl Motion Workshop, 2009. 8\n[44] Jonas Wulff, Laura Sevilla-Lara, and Michael J Black. Op-\ntical ﬂow in mostly rigid scenes. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017. 1\n[45] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan\nLiu, and Dawn Song. Spatially transformed adversarial ex-\namples. In International Conference on Learning Represen-\ntations (ICLR), 2018. 2\n[46] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong,\nand Quoc V Le. Unsupervised data augmentation for consis-\ntency training. ArXiv pre-prints:1904.12848, 2019. 2\n[47] Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy.\nDeep ﬂow-guided video inpainting. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019. 1,\n5\n[48] Yanchao Yang, Antonio Loquercio, Davide Scaramuzza, and\nStefano Soatto. Unsupervised moving object detection via\ncontextual information separation. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019. 1,\n5\n[49] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learn-\ning of dense depth, optical ﬂow and camera pose. In IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2018. 1, 2\n[50] Yiran Zhong, Pan Ji, Jianyuan Wang, Yuchao Dai, and Hong-\ndong Li. Unsupervised deep epipolar ﬂow for stationary or\ndynamic scenes. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2019. 1, 2, 6\n[51] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G\nLowe. Unsupervised learning of depth and ego-motion from\nvideo. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017. 1\n[52] Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen\nWei.\nDeep feature ﬂow for video recognition.\nIn IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2017. 1\n[53] Yuliang Zou, Zelun Luo, and Jia-Bin Huang. Df-net: Un-\nsupervised joint learning of depth and ﬂow using cross-task\nconsistency. In European Conference on Computer Vision\n(ECCV), 2018. 2\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-03-29",
  "updated": "2020-11-29"
}