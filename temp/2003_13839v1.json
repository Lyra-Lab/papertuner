{
  "id": "http://arxiv.org/abs/2003.13839v1",
  "title": "Model-Reference Reinforcement Learning Control of Autonomous Surface Vehicles with Uncertainties",
  "authors": [
    "Qingrui Zhang",
    "Wei Pan",
    "Vasso Reppa"
  ],
  "abstract": "This paper presents a novel model-reference reinforcement learning control\nmethod for uncertain autonomous surface vehicles. The proposed control combines\na conventional control method with deep reinforcement learning. With the\nconventional control, we can ensure the learning-based control law provides\nclosed-loop stability for the overall system, and potentially increase the\nsample efficiency of the deep reinforcement learning. With the reinforcement\nlearning, we can directly learn a control law to compensate for modeling\nuncertainties. In the proposed control, a nominal system is employed for the\ndesign of a baseline control law using a conventional control approach. The\nnominal system also defines the desired performance for uncertain autonomous\nvehicles to follow. In comparison with traditional deep reinforcement learning\nmethods, our proposed learning-based control can provide stability guarantees\nand better sample efficiency. We demonstrate the performance of the new\nalgorithm via extensive simulation results.",
  "text": "Model-Reference Reinforcement Learning Control of Autonomous\nSurface Vehicles with Uncertainties\nQingrui Zhang1,2, Wei Pan2, and Vasso Reppa1\nAbstract— This paper presents a novel model-reference rein-\nforcement learning control method for uncertain autonomous\nsurface vehicles. The proposed control combines a conventional\ncontrol method with deep reinforcement learning. With the\nconventional control, we can ensure the learning-based con-\ntrol law provides closed-loop stability for the overall system,\nand potentially increase the sample efﬁciency of the deep\nreinforcement learning. With the reinforcement learning, we\ncan directly learn a control law to compensate for modeling\nuncertainties. In the proposed control, a nominal system is\nemployed for the design of a baseline control law using a\nconventional control approach. The nominal system also deﬁnes\nthe desired performance for uncertain autonomous vehicles\nto follow. In comparison with traditional deep reinforcement\nlearning methods, our proposed learning-based control can\nprovide stability guarantees and better sample efﬁciency. We\ndemonstrate the performance of the new algorithm via extensive\nsimulation results.\nI. INTRODUCTION\nAutonomous surface vehicles (ASVs) have been attracting\nmore and more attention, due to their advantages in many\napplications, such as environmental monitoring [1], resource\nexploration [2], shipping [3], and many more. Successful\nlaunch of ASVs in real life requires accurate tracking control\nalong a desired trajectory [4]–[6]. However, accurate tracking\ncontrol for ASVs is challenging, as ASVs are subject to un-\ncertain nonlinear hydrodynamics and unknown environmental\ndisturbances [7]. Hence, tracking control of highly uncertain\nASVs has received extensive research attention [8]–[12].\nControl algorithms for uncertain systems including ASVs\nmainly lie in four categories: 1) robust control which is the\n“worst-case” design for bounded uncertainties and disturbances\n[9]; 2) adaptive control which adapts to system uncertainties\nwith parameter estimations [4], [5]; 3) disturbance observer-\nbased control which compensates uncertainties and distur-\nbances in terms of the observation technique [11], [13];\nand 4) reinforcement learning (RL) which learns a control\nlaw from data samples [12], [14]. The ﬁrst three algorithms\nfollow a model-based control approach, while the last one\nis data driven. Model-based control can ensure closed-loop\nstability, but a system model is indispensable. Uncertainties\nand disturbances of a system should also satisfy different\nconditions for different model-based methods. In robust\ncontrol, uncertainties and disturbances are assumed to be\nbounded with known boundaries [15]. As a consequence,\n1Department of Maritime and Transport Technology, Delft University of\nTechnology, Delft, the Netherlands Qingrui.Zhang@tudelft.nl;\nV.Reppa@tudelft.nl\n2Department of Cognitive Robotics, Delft University of Technology, Delft,\nthe Netherlands Wei.Pan@tudelft.nl\nrobust control will lead to conservative high-gain control\nlaws which usually limits the control performance (i.e.,\novershoot, settling time, and stability margins) [16]. Adaptive\ncontrol can handle varying uncertainties with unknown\nboundaries, but system uncertainties are assumed to be linearly\nparameterized with known structure and unknown constant\nparameters [17], [18]. A valid adaptive control design also\nrequires a system to be persistently excited, resulting in the\nunpleasant high-frequency oscillation behaviours in control\nactions [19]. On the other hand, disturbance observer-based\ncontrol can adapt to both uncertainties and disturbances\nwith unknown structures and without assuming systems to\nbe persistently excited [13], [20]. However, we need the\nfrequency information of uncertainty and disturbance signals\nwhen choosing proper gains for the disturbance observer-\nbased control, otherwise it is highly possible to end up with\na high-gain control law [20]. In addition, the disturbance\nobserver-based control can only address matched uncertainties\nand disturbances, which act on systems through the control\nchannel [18], [21]. In general, comprehensive modeling and\nanalysis of systems are essential for all model-based methods.\nIn comparison with model-based methods, RL is capable\nof learning a control law from data samples using much\nless model information [22]. Hence, it is more promising\nin controlling systems subject to massive uncertainties and\ndisturbances as ASVs [12], [14], [23], [24], given the\nsufﬁciency and good quality of collected data. Nevertheless,\nit is challenging for model-free RL to ensure closed-loop\nstability, though some research attempts have been made\n[25]. It implies that the learned control law must be re-\ntrained, once some changes happen to the environment or the\nreference trajectory (i.e. in [14], the authors conducted two\nindependent training procedures for two different reference\ntrajectories.). Model-based RL is possible to learn a control\nlaw which ensures the closed-loop stability by introducing a\nLyapunov constraint into the objective function of the policy\nimprovement according to the latest research [26]. However,\nthe model-based RL with stability guarantees requires an\nadmissible control law — a control law which makes the\noriginal system asymptotically stable — for the initialization.\nBoth the Lyapunov candidate function and complete system\ndynamics are assumed to be Lipschitz continuous with known\nLipschitz constants for the construction of the Lyapunov\nconstraint. It is challenging to ﬁnd the Lipschitz constant\nof an uncertain system subject to unknown environmental\ndisturbances. Therefore, the introduced Lyapunov constraint\nfunction is restrictive, as it is established based on the worst-\ncase consideration [26].\narXiv:2003.13839v1  [eess.SY]  30 Mar 2020\nWith the consideration of merits and limitations of existing\nRL methods, we propose a novel learning-based control\nalgorithm for uncertain ASVs by combining a conventional\ncontrol method with deep RL in this paper. The proposed\nlearning-based control design, therefore, consists of two\ncomponents: a baseline control law stabilizing a nominal\nASV system and a deep RL control law used to compensate\nfor system uncertainties and disturbances. Such a design\nmethod has several advantages over both conventional model-\nbased methods and pure deep RL methods. First of all, in\nrelation to the “model-free” feature of deep RL, we can learn\na control law directly to compensate for uncertainties and\ndisturbances without exploiting their structures, boundaries, or\nfrequencies. In the new design, uncertainties and disturbances\nare not necessarily matched, as deep RL seeks a control\nlaw like direct adaptive control [27]. The learning process\nis performed ofﬂine using historical data and the stochastic\ngradient descent technique, so there is no need for the ASV\nsystem be persistently excited when the learned control law\nis implemented. Second, the overall learned control law can\nprovide stability guarantees, if the baseline control law is\nable to stabilize the ASV system at least locally. Without\nintroducing a restrictive Lyapunov constraint into the objective\nfunction of the policy improvement in RL as in [26], we can\navoid exploiting the Lipschitz constant of the overall system\nand potentially produce less conservative results. Lastly, the\nproposed design is potentially more sample efﬁcient than\na RL algorithm learning from scratch – that is, fewer data\nsamples are needed for the training process. In RL, a system\nlearns from mistakes so a lot of trial and error is demanded.\nFortunately, in our proposed design, the baseline control\nwhich can stabilize the overall system under no disturbances,\ncan help to exclude unnecessary mistakes, so it provides a\ngood starting point for the RL training. A similar idea is used\nin [28] for the control of quadrotors. The baseline control\nin [28] is constructed based on the full accurate model of a\nquadrotor system, but stability analysis is missing.\nThe rest of the paper is organized as follows. In Section II,\nwe present the ASV dynamics, basic concepts of reinforce-\nment learning, and problem formulation. Section IV describes\nthe proposed methodology, including deep reinforcement\nlearning design, training setup, and algorithm analysis. In\nSection VI, numerical simulation results are provided to show\nthe efﬁciency of the proposed design. Conclusion remarks\nare given in Section VII.\nII. PROBLEM FORMULATION\nThe full dynamics of autonomous surface vehicles (ASVs)\nhave six degrees of freedom (DOF), including three linear\nmotions and three rotational motions [7]. In most scenarios,\nwe are interested in controlling the horizontal dynamics of\n(ASVs) [29], [30]. We, therefore, ignore the vertical, rolling,\nand pitching motions of ASVs by default in this paper.\nLet x and y be the horizontal position coordinates of an\nASV in the inertial frame and ψ the heading angle as shown\nin Figure 1. In the body frame (c.f., Figure 1), we use u and\nv to represent the linear velocities in surge (x-axis) and sway\nInertial frame\nMoment of Inertia\nlatio\nBody frame\n(x, y)\n \nu\nv\nYI\nEast\nXI\nNorth\nXB\nYB\n⌘= [x, y,  ]T\n⌫= [u, v, r]T\nFig. 1: Coordinate systems of an autonomous surface vehicle\n(y-axis), respectively. The heading angular rate is denoted by\nr. The general 3-DOF nonlinear dynamics of an ASV can\nbe expressed as\n\u001a\n˙η\n=\nR (η) ν\nM ˙ν + (C (ν) + D (ν)) ν + G (ν)\n=\nτ\n(1)\nwhere η = [x, y, ψ]T ∈R3 is a generalized coordinate\nvector, ν = [u, v, r]T ∈R3 is the speed vector, M is\nthe inertia matrix, C (ν) denotes the matrix of Coriolis\nand centripetal terms, D (ν) is the damping matrix, τ ∈\nR3 represents the control forces and moments, G (ν) =\n[g1 (ν) , g2 (ν) , g3 (ν)]T ∈R3 denotes unmodeled dynamics\ndue to gravitational and buoyancy forces and moments [7],\nand R is a rotation matrix given by\nR =\n\n\ncos ψ\n−sin ψ\n0\nsin ψ\ncos ψ\n0\n0\n0\n1\n\n\nThe inertia matrix M = M T > 0 is\nM = [Mij] =\n\n\nM11\n0\n0\n0\nM22\nM23\n0\nM32\nM33\n\n\n(2)\nwhere M11 = m−X ˙u, M22 = m−Y ˙v, M33 = Iz −N ˙r, and\nM32 = M23 = mxg −Y ˙r. The matrix C (ν) = −CT (ν) is\nC = [Cij] =\n\n\n0\n0\nC13 (ν)\n0\n0\nC23 (ν)\n−C13 (ν)\n−C23 (ν)\n0\n\n\n(3)\nwhere C13 (ν) = −M22v −M23r, C23 (ν) = −M11u. The\ndamping matrix D (ν) is\nD (ν) = [Dij] =\n\n\nD11 (ν)\n0\n0\n0\nD22 (ν)\nD23 (ν)\n0\nD32 (ν)\nD33 (ν)\n\n\n(4)\nwhere D11 (ν) = −Xu −X|u|u|u| −Xuuuu2, D22 (ν) =\n−Yv −Y|v|v|v| −Y|r|v|r|, D23 (ν) = −Yr −Y|v|r|v| −\nY|r|r|r|, D32 (ν) = −Nv −N|v|v|v| −N|r|v|r|, D33 (ν) =\n−Nr −N|v|r|v| −N|r|r|r|, and X(·), Y(·), and N(·) are\nhydrodynamic coefﬁcients whose deﬁnitions can be found in\n[7]. Accurate numerical models of the nonlinear dynamics\n(1) are rarely available. Major uncertainty sources come from\nM, C (ν), and D (ν) due to hydrodynamics, and G (ν)\ndue to gravitational and buoyancy forces and moments. The\nobjective of this work is to design a control scheme capable\nof handling these uncertainties.\nAutonomous Surface Vehicle\nNominal System\nub\nul\nxr\nx\nxm\nRL control\nBaseline \ncontrol\nBaseline \ncontrol\nxm\nx\nx\num\n˙xm =\n\n0\nR(⌘)\n0\nAm\n\"\nxm +\n\n0\nBm\n\"\num\nFig. 2: Model-reference reinforcement learning control\nIII. MODEL-REFERENCE REINFORCEMENT LEARNING\nCONTROL\nLet x =\n\u0002\nηT , νT \u0003T and u = τ, so (1) can be rewritten as\n˙x =\n\u0014 0\nR (η)\n0\nA (ν)\n\u0015\nx +\n\u0014 0\nB\n\u0015\nu\n(5)\nwhere A (ν) = M −1 (C (ν) + D (ν)), and B = M −1.\nAssume an accurate model (5) is not available, but it is\npossible to get a nominal model expressed as\n˙xm =\n\u0014 0\nR (η)\n0\nAm\n\u0015\nxm +\n\u0014\n0\nBm\n\u0015\num\n(6)\nwhere Am and Bm are the known system matrices. Assume\nthat there exists a control law um allowing the states of the\nnominal system (6) to converge to a reference signal xr, i.e.,\n∥xm −xr∥2 →0 as t →∞.\nThe objective is to design a control law allowing the state\nof (5) to track state trajectories of the nominal model (6).\nAs shown in Figure 2, the overall control law for the ASV\nsystem (5) has the following expression.\nu = ub + ul\n(7)\nwhere ub is a baseline control designed based on (6), and ul is\na control policy from the deep reinforcement learning module\nshown in Figure 2. The baseline control ub is employed to\nensure some basic performance, (i.e., local stability), while\nul is introduced to compensate for all system uncertainties.\nThe baseline control ub in (7) can be designed based on any\nexisting model-based method based on the nominal model (6).\nHence, we ignore the design process of ub, and mainly focus\non the development of ul based on reinforcement learning.\nA. Reinforcement learning\nIn RL, system dynamics are characterized using a\nMarkov decision process denoted by a tuple MDP :=\n\nS, U, P, R, γ\n\u000b\n, where S is the state space, U speciﬁes the\naction/input space, P : S × U × S →R deﬁnes a transition\nprobability, R : S × U →R is a reward function, and\nγ ∈[0, 1] is a discount factor. A policy in RL, denoted\nby π (ul|s), is the probability of choosing an action ul ∈U\nat a state s ∈S. Note that the state vector s contains all\navailable signals affecting the reinforcement learning control\nul. In this paper, such signals include x, xm, xr, and ub,\nwhere xm performs like a target state for system (5) and ub is\na function of x and xr. Hence, we choose s = {xm, x, ub}.\nReinforcement learning uses data samples, so it is assumed\nthat we can sample input and state data from system (5) at\ndiscrete time steps. Without loss of generality, we deﬁne xt,\nub,t, and ul,t as the ASV state, the baseline control action,\nand the control action from the reinforcement learning at the\ntime step t, respectively. The state signal s at the time step t\nis, therefore, denoted by st = {xm,t, xt, ub,t}. The sample\ntime step is assumed to be ﬁxed and denoted by δt.\nFor each state st, we deﬁne a value function Vπ (st) as\nan expected accumulated return described as\nVπ =\n∞\nX\nt\nX\nul,t\nπ (ul,t|st)\nX\nst+1\nPt+1|t\n\u0000Rt + γVπ(st+1)\n\u0001\n(8)\nwhere Rt = R(st, ul,t) and Pt+1|t = P (st+1 |st, ul,t ). The\naction-value function (a.k.a., Q-function) is deﬁned to be\nQπ (st, ul,t) = Rt + γ\nX\nst+1\nPt+1|tVπ(st+1)\n(9)\nIn our design, we aim to allow system (5) to track the nominal\nsystem (6), so Rt is deﬁned as\nRt = −(xt −xm,t)T G (xt −xm,t) −uT\nl,tHul,t\n(10)\nwhere G ≥0 and H > 0 are positive deﬁnite matrices.\nThe objective of the reinforcement learning is to ﬁnd an\noptimal policy π∗to maximize the state-value function Vπ(st)\nor the action-value function Qπ (st, ul,t), ∀st ∈S, namely,\nπ∗= arg max\nπ\nQπ (st, ul,t)\n= arg max\nπ\n\nRt + γ\nX\nst+1\nPt+1|tVπ(st+1)\n\n\n(11)\nIV. DEEP REINFORCEMENT LEARNING CONTROL DESIGN\nIn this section, we will present a deep reinforcement\nlearning algorithm for the design of ul in (7), where both\nthe control law ul and the Q-function Qπ (st, ul,t) are\napproximated using deep neural networks.\nThe deep reinforcement learning control in this paper is\ndeveloped based on the soft actor-critic (SAC) algorithm\nwhich provides both sample efﬁcient learning and convergence\n[31]. In SAC, an entropy term is added to the objective\nfunction in (11) to regulate the exploration performance at\nthe training stage. The objective of (11) is thus rewritten as\nπ∗= arg max\nπ\n\u0000Rt + γEst+1 [Vπ(st+1)\n+αH (π (ul,t+1|st+1))]\n\u0001\n(12)\nwhere Est+1 [·] = P\nst+1 Pt+1|t [·] is an expectation opera-\ntor, H (π (ul,t|st)) = −P\nul,t π (ul,t|st) ln (π (ul,t|st)) =\n−Eπ [ln (π (ul,t|st))] is the entropy of the policy, and α is\na temperature parameter.\nTraining of SAC repeatedly executes policy evaluation and\npolicy improvement. In the policy evaluation, a soft Q-value\nis computed by applying a Bellman operation Qπ (st, ul,t) =\nT πQπ (st, ul,t) where\nT πQπ (st, ul,t) = Rt + γEst+1 {Eπ [Qπ (st+1, ul,t+1)\n−α ln (π (ul,t+1|st+1))]}\n(13)\nInput layer\nHidden layers\nOutput layer\nActor neural \nnetwork\nst\nπφ (ul,t|st)\nState\nState\nInput layer\nHidden layers\nOutput layer\nCritic neural \nnetwork\nInput\nst\nul,t\nQθ (st,ul,t)\nFig. 3: Approximation of Qθ and πφ using MLP\nIn the policy improvement, the policy is updated by\nπnew = arg min\nπ′ DKL\n\u0010\nπ′ (·|st)\n\r\r\rZπoldeQπold(st,·)\u0011\n(14)\nwhere πold denotes the policy from the last update, Qπold\nis the Q-value of πold. DKL denotes the Kullback-Leibler\n(KL) divergence, and Zπold is a normalization factor. Via\nmathematical manipulations, the objective for the policy\nimprovement is transformed into\nπ∗= arg min\nπ Eπ\nh\nα ln (π (ul,t|st)) −Q (st, ul,t)\ni\n(15)\nMore details on how (15) is obtained can be found in [31],\n[32]. As shown in Figure 3, both the policy π (ul,t|st) and\nvalue function Qπ (st, ul,t) will be parameterized using fully\nconnected multiple layer perceptrons (MLP) with ’ReLU’\nnonlinearities as the activation functions. The ’ReLU’ function\nis deﬁned as\nrelu (z) = max {z, 0}\nThe “ReLU” activation function outperforms other acti-\nvation functions like sigmoid functions [33]. For a vec-\ntor z\n= [z1, . . . , zn]T\n∈Rn, there exists relu (z) =\n[relu (z1) , . . . , relu (zn)]T . Hence, a MLP with ’ReLU’ as\nthe activation functions and one hidden layer is expressed as\nMLP (z) = W1\nh\nrelu\n\u0000W0\n\u0002\nzT , 1\n\u0003\u0001T , 1\niT\nwhere\n\u0002\nzT , 1\n\u0003T is a vector composed of z and 1, and W0\nand W1 with appropriate dimensions are weight matrices to\nbe trained. For the simplicity, we use W = {W0, W1} to\nrepresent the set of parameters to be trained.\nSystem\nActor neural \nnetwork\nCritic neural \nnetwork\nReward\nRun the system using latest learned control policy and collect data\nUpdate critic and actor neural networks using historical data\nReplay memory\nRandomly sample a \nbatch of data samples\n⇠\nExploration noise\ninput\nstate\nFig. 4: Ofﬂine training process of deep reinforcement learning\nIn this paper, the Q-function is parameterized using θ\nand denoted by Qθ (st, ul,t). The parameterized policy is\ndenoted by πφ (ul,t|st), where φ is the parameter set to be\ntrained. Note that both θ and φ are a set of parameters whose\ndimensions are determined by the deep neural network setup.\nFor example, if Qθ is represented by a MLP with K hidden\nlayers and L neurons for each hidden layers, the parameter\nset θ is θ = {θ0, θ1, . . . , θK} with θ0 ∈R(dims+dimu+1)×L,\nθK ∈R(L+1), and θi ∈R1×(L)×(L+1) for 1 ≤i ≤K −1,\nwhere dims denotes the dimension of the state s and dimu\nis the dimension of the input ul. The deep neural network\nfor Qθ is called critic, while the one for πφ is called actor.\nA. Training setup\nThe algorithm training process is illustrated in Figure 4.\nThe whole training process will be ofﬂine. We repeatedly\nrun the system (5) under a trajectory tracking task. At each\ntime step t + 1, we collect data samples, such as an input\nfrom the last time step ul,t, a state from the last time step\nst, a reward Rt, and a current state st+1. Those historical\ndata will be stored as a tuple (st, ul,t, Rt, st+1) at a replay\nmemory D [34]. At each policy evaluation or improvement\nstep, we randomly sample a batch of historical data, B, from\nthe replay memory D for the training of the parameters θ\nand φ. Starting the training, we apply the baseline control\npolicy ub to an ASV system to collect the initial data D0 as\nshown in Algorithm 1. The initial data set D0 is used for the\ninitial ﬁtting of Q-value functions. When the initialization is\nover, we execute both ub and the latest updated reinforcement\nlearning policy πφ (ul,t|st) to run the ASV system.\nAt the policy evaluation step, the parameters θ are trained\nto minimize the following Bellman residual.\nJQ (θ) = E(st,ul,t)∼D\n\u00141\n2 (Qθ (st, ul,t) −Ytarget)2\n\u0015\n(16)\nwhere (st, ul,t) ∼D implies that we randomly pick data\nsamples (st, ul,t) from a replay memory D, and\nYtarget = Rt + γEst+1\n\u0002\nEπ [Q¯θ (st+1, ul,t+1) −α ln (πφ)]\n\u0003\nwhere ¯θ is the target parameter which will be updated slowly.\nApplying a stochastic gradient descent technique (ADAM\nAlgorithm 1 Reinforcement learning control\n1: Initialize parameters θ1, θ2 for Qθ1 and Qθ2, respectively,\nand φ for the actor network (18).\n2: Assign values to the the target parameters ¯θ1 ←θ1,\n¯θ2 ←θ2, D ←∅, D0 ←∅,\n3: Get data set D0 by running ub on (5) with ul = 0\n4: Turn off the exploration and train initial critic parameters\nθ0\n1, θ0\n2 using D0 according to (16).\n5: Initialize the replay memory D ←D0\n6: Assign initial values to critic parameters θ1 ←θ0\n1, θ2 ←\nθ0\n2 and their targets ¯θ1 ←θ0\n1, ¯θ2 ←θ0\n2\n7: repeat\n8:\nfor each data collection step do\n9:\nChoose an action ul,t according to πφ (ul,t|st)\n10:\nRun both the nominal system (6) and the full system\n(5) & collect st+1 = {xt+1, xm,t+1, ub,t+1}\n11:\nD ←D S {st, ul,t, R (st, ul,t) , st+1}\n12:\nend for\n13:\nfor each gradient update step do\n14:\nSample a batch of data B from D\n15:\nθj ←θj −ιQ∇θJQ (θj), and j = 1, 2\n16:\nφ ←φ −ιπ∇φJπ (φ),\n17:\nα ←α −ια∇αJα (α)\n18:\n¯θj ←κθj + (1 −κ) ¯θj, and j = 1, 2\n19:\nend for\n20: until convergence (i.e. JQ (θ) < a small threshold)\n[35] in this paper) to (16) on a data batch B with a ﬁxed\nsize, we obtain\n∇θJQ (θ) =\nX ∇θQθ\n|B|\n\u0010\nQθ (st, ul,t) −Ytarget\n\u0011\nwhere |B| is the batch size.\nAt the policy improvement step, the objective function\ndeﬁned in (15) is represented using data samples from the\nreplay memory D as given in (17).\nJπ (φ) = E(st,ul,t)∼D\n\u0010\nα ln(πφ) −Qθ (st, ul,t)\n\u0011\n(17)\nParameter φ is trained to minimize (17) using a stochastic\ngradient descent technique. At the training stage, the actor\nneural network is expressed as\nul,φ = ¯ul,φ + σφ ⊙ξ\n(18)\nwhere ¯ul,φ represents the control law to be implemented in\nthe end, σφ denotes the standard deviation of the exploration\nnoise, ξ ∼N (0, I) is the exploration noise with N (0, I)\ndenoting a Gaussian distribution, and “⊙” is the Hadamard\nproduct. Note that the exploration noise ξ is only applied to\nthe training stage. Once the training is done, we only need\n¯ul,φ in the implementation. Hence, at the training stage, ul\nin Figure 2 is equal to ul,φ. Once the training is over, we\nhave ul = ¯ul,φ.\nApplying the policy gradient technique to (17), we can\ncalculate the gradient of Jπ (φ) with respect to φ in terms\nAlgorithm 2 Policy iteration technique\n1: Start from an initial control policy u0\n2: repeat\n3:\nfor Policy evaluation do\n4:\nUnder a ﬁxed policy ul, apply the Bellman backup\noperator T π to the Q value function, Q (st, ul,t) =\nT πQ (st, ul,t) (c.f., (13))\n5:\nend for\n6:\nfor Policy improvement do\n7:\nUpdate policy π according to (12)\n8:\nend for\n9: until convergence\nof the stochastic gradient method as in (19)\n∇φJπ =\nX α∇φ ln πφ + (α∇ul ln πφ −∇ulQθ) ∇φul,φ\n|B|\n(19)\nThe temperature parameters α are updated by minimizing the\nfollowing objective function.\nJα = Eπ\n\u0002\n−α ln π (ul,t|st) −α ¯H\n\u0003\n(20)\nwhere ¯H is a target entropy. Following the same setting in\n[32], we choose ¯H = −3 where “3” here represents the\naction dimension. In the ﬁnal implementation, we use two\ncritics which are parameterized by θ1 and θ2, respectively.\nThe two critics are introduced to reduce the over-estimation\nissue in the training of critic neural networks [36]. Under the\ntwo-critic mechanism, the target value Ytarget is\nYtarget = Rt + γ min\nn\nQ¯θ1 (st+1, ul,t+1) ,\nQ¯θ2 (st+1, ul,t+1)\no\n−γα ln (πφ)\n(21)\nThe entire algorithm is summarized in Algorithm 1. In\nAlgorithm 1, ιQ, ιπ, and ια are positive learning rates\n(scalars), and κ > 0 is a constant scalar.\nV. PERFORMANCE ANALYSIS\nIn this subsection, both the convergence and stability of the\nproposed learning-based control are analyzed. For the analysis,\nthe soft actor-critic RL method in Algorithm 1 is recapped\nas a policy iteration (PI) technique which is summarized in\nAlgorithm 2. We thereafter present the following two lemmas\nwithout proofs for the convergence analysis [31], [32].\nLemma 1 (Policy evaluation): Let T π be the Bellman\nbackup operator under a ﬁxed policy π and Qk+1 (s, ul) =\nT πQk (s, ul). The sequence Qk+1 (s, ul) will converge to\nthe soft Q-function Qπ of the policy π as k →∞.\nLemma 2 (Policy improvement): Let πold be an old policy\nand πnew be a new policy obtained according to (14). There\nexists Qπnew (s, ul) ≥Qπold (s, ul) ∀s ∈S and ∀u ∈U.\nIn terms of (1) and (2), we are ready to present Theorem\n1 to show the convergence of the SAC algorithm.\nTheorem 1 (Convergence): If one repeatedly applies the\npolicy evaluation and policy improvement steps to any control\npolicy π, the control policy π will converge to an optimal\npolicy π∗such that Qπ∗(s, ul) ≥Qπ (s, ul) ∀π ∈Π, ∀s ∈\nS, and ∀u ∈U, where Π denotes a policy set.\nProof: Let πi be the policy obtained from the i-th policy\nimprovement with i = 0, 1, . . ., ∞. According to Lemma\n2, one has Qπi (s, ul) ≥Qπi−1 (s, ul), so Qπi (s, ul) is\nmonotonically non-decreasing with respect to the policy\niteration step i. In addition, Qπi (s, ul) is upper bounded\naccording to the deﬁnition of the reward given in (10), so\nQπi (s, ul) will converge to an upper limit Qπ∗(s, ul) with\nQπ∗(s, ul) ≥Qπ (s, ul) ∀π ∈Π, ∀s ∈S, and ∀ul ∈U.\nTheorem 1 demonstrates that we can ﬁnd an optimal policy\nby repeating the policy evaluation and improvement processes.\nNext, we will show the closed-loop stability of the overall\ncontrol law (baseline control ub plus the learned control ul).\nThe following assumption is made for the baseline control\ndeveloped using the nominal system (6).\nAssumption 1: The baseline control law ub can ensure that\nthe overall uncertain ASV system is stable – that is, there\nexists a Lyapunov function V (st) associate with ub such\nthat V (st+1) −V (st) ≤0 ∀st ∈S.\nNote that the baseline control ub is implicitly included in the\nstate vector s, as s consists of x, xm, and ub in this paper\nas discussed in Section III. Hence, V (st) in Assumption 1\nis the Lyapunov function for the closed-loop system of (5)\nwith the baseline control ub.\nAssumption 1 is possible in real world. One could treat\nthe nominal model (6) as a linearized model of the overall\nASV system (5) around a certain equilibrium. Therefore, a\ncontrol law, which ensures asymptotic stability for (6), can\nensure at least local stability for (5) [37]. In the stability\nanalysis, we will ignore the entropy term H (π), as it will\nconverge to zero in the end and it is only introduced to regulate\nthe exploration magnitude. Now, we present Theorem 2 to\ndemonstrate the closed-loop stability of the ASV system (5)\nunder the composite control law (7).\nTheorem 2 (Stability): Suppose Assumption 1 holds. The\noverall control law ui = ub + ui\nl can always stabilize the\nASV system (5), where ui\nl represents the RL control law\nfrom i-th iteration, and i = 0, 1, 2, ... ∞.\nProof: In our proposed algorithm, we start the train-\ning/learning using the baseline control law ub. According\nto Lemma 1, we are able to obtain the corresponding Q\nvalue function for the baseline control law ub. Let the Q\nvalue function be Q0 (s, ul) where ul is a function of s.\nAccording to the deﬁnitions of the reward function in (10)\nand Q value function in (9), we can choose the Lyapunov\nfunction candidate as V0 (s) = −Q0 (s, ul). If Assumption\n1 holds, there exists V0 (st+1) −V0 (st) ≤0 ∀st ∈S.\nIn the policy improvement, the control law is updated by\nu1 = min\nπ\n\u0000−Rt + γV0 (st+1)\n\u0001\n(22)\nwhere the expectation operator is ignored as the system is\ndeterministic. For any nonlinear system st+1 = f (st) +\ng (st) ut, a necessary condition for the existence of (22) is\nu1 = −1\n2H−1g (st)T ∂V0 (st+1)\n∂st+1\n(23)\nSubstituting (23) back into (22 yields\nV0 (st+1) −V0 (st)\n=\n−(xt −xm,t)T G (xt −xm,t)\n−1\n4\n\u0012∂V0 (st+1)\n∂st+1\n\u0013T\ng (st)\n×H−1g (st)T ∂V0 (st+1)\n∂st+1\n≤0\nHence, u1 is a control law which can stabilize the same ASV\nsystem (5), if Assumption 1 holds. Applying Lemma 1 to u1,\nwe can get a new Lyapunov function V1 (st). In terms of\nV1 (st), (22) and (23), we can show that u2 also stabilizes\nthe ASV system (5). Repeating (22) and (23) for all i = 1, 2,\n. . ., we can prove that all ui can stabilize the ASV system\n(5), if Assumption 1 holds.\nVI. SIMULATION\nIn this section, the proposed learning-based control algo-\nrithm is implemented to the trajectory tracking control of a\nsupply ship model presented in [29], [30]. Model parameters\nare summarized in Table I. The unmodeled dynamics in\nthe simulations are given by g1 = 0.279uv2 + 0.342v2r,\ng2 = 0.912u2v, and g3 = 0.156ur2+0.278urv3, respectively.\nThe based-line control law ub is designed based on a nominal\nmodel with the following simpliﬁed linear dynamics in terms\nof the backstepping control method [13], [37].\nMm ˙νm = τ −Dmνm\n(24)\nwhere\nMm\n=\ndiag {M11, M22, M33}.\nDm\n=\ndiag {−Xv, −Yv, −Nr}. The reference signal is assumed\nto be produced by the following motion planner.\n˙ηr = R (ηr) νr\n˙νr = ar\n(25)\nwhere ηr = [xr, yr, ψr]T is the generalized reference position\nvector, νr = [ur, 0, rr]T is the generalized reference velocity\nvector, and ar = [ ˙ur, 0, ˙rr]T . In the simulation, the initial\nposition vector ηr (0) is chosen to be ηr (0) =\n\u0002\n0, 0, π\n4\n\u0003T ,\nand we set ur (0) = 0.4 m/s and rr (0) = 0 rad/s. The\nreference acceleration ˙ur and angular rates are chosen to be\n˙ur\n=\n\u001a 0.005 m/s2\nif t < 20 s\n0 m/s2\notherwise\n(26)\n˙rr\n=\n\u001a\nπ\n600 rad/s2\nif 25 s ≤t < 50 s\n0 rad/s2\notherwise\n(27)\nTABLE I: Model parameters\nParameters\nValues\nParameters\nValues\nm\n23.8\nY ˙r\n−0.0\nIz\n1.76\nYr\n0.1079\nxg\n0.046\nY|v|r\n−0.845\nX ˙u\n−2.0\nY|r|r\n−3.45\nXu\n−0.7225\nNv\n−0.1052\nX|u|u\n−1.3274\nN|v|v\n5.0437\nXuuu\n−1.8664\nN|r|v\n−0.13\nY ˙v\n−10.0\nN ˙r\n−1.0\nYv\n−0.8612\nNr\n−1.9\nY|v|v\n−36.2823\nN|v|r\n0.08\nY|r|v\n−0.805\nN|r|r\n−0.75\nTABLE II: Reinforcement learning conﬁgurations\nParameters\nValues\nLearning rate ιQ\n0.001\nLearning rate ιπ\n0.0001\nLearning rate ια\n0.0001\nκ\n0.01\nactor neural network\nfully connected with two hidden layers\n(128 neurons per hidden layer)\ncritic neural networks\nfully connected with two hidden layers\n(128 neurons per hidden layer)\nReplay memory capacity\n1 × 106\nSample batch size\n128\nγ\n0.998\nTraining episodes\n1001\nSteps per episode\n1000\ntime step size δt\n0.1\nFig. 5: Learning curves of two RL algorithms at training (One\nepisode is a training trial, and 1000 time steps per episode)\nAt the training stage, we uniformly randomly sample x (0)\nand y (0) from (−1.5, 1.5), ψ (0) from (0.1π, 0.4π) and u (0)\nfrom (0.2, 0.4), and we choose v (0) = 0 and r (0) = 0. The\nproposed control algorithm is compared with two benchmark\ndesigns: the baseline control u0 and the RL control without\nu0. Conﬁgurations for the training and neural networks are\nfound in Table II. The matrix G and H are chosen to be\nG = diag {0.025, 0.025, 0.0016, 0.005, 0.001, 0} and H =\ndiag\n\b\n1.25e−4, 1.25e−4, 8.3e−5\t\n, respectively.\nAt the training stage, we run the ASV system for 100 s,\nand the repeat the training processes for 1000 times (i.e., 1000\nepisodes). Figure 5 shows the learning curves of the proposed\nalgorithm (red) and the RL algorithm without baseline control\n(blue). The learning curves demonstrate that both of the two\nalgorithms will converge in terms of the long term returns.\nHowever, our proposed algorithm results in a larger return\n(red) in comparison with the RL without baseline control\n(blue). Hence, the introduction of the baseline control helps to\nincrease the sample efﬁciency signiﬁcantly, as the proposed\nalgorithm (blue) converges faster to a higher return value.\nAt the ﬁrst evaluation stage, we run the ASV system for\n200 s to demonstrate whether the control law can ensure\nstable trajectory tracking. Note that we run the ASV for 100\ns at training. The trajectory tracking performance of the three\nalgorithms (our proposed algorithm, the baseline control u0,\nand only RL control) is shown in Figures 6. As observed\nfrom Figure 6.b, the control law learned merely using deep\nRL fails to ensure stable tracking performance. It implies\nthat only deep RL cannot ensure the closed-loop stability. In\naddition, the baseline control itself fails to achieve acceptable\ntracking performance mainly due to the existence of system\nuncertainties. By combining the baseline control and deep RL,\nthe trajectory tracking performance is improved dramatically,\nand the closed-loop stability is also ensured. The position\ntracking errors are summarized in Figure 7 and 8. Figure\n9 shows the absolute distance errors used to compare the\ntracking accuracy of the three algorithms. The introduction of\nthe deep RL increases the tracking performance substantially.\nAt the second evaluation, we still run the ASV system for\n200 s, but change the reference trajectory. Note that we use\nthe same learned control laws in both the ﬁrst and the second\nevaluations. In the second evaluation, the reference angular\nacceleration is changed to\n˙rr =\n\n\n\nπ\n600 rad/s2\nif\n25 s ≤t < 50 s\n−π\n600 rad/s2\nif 125 s ≤t < 150 s\n0 rad/s2\notherwise\n(28)\nThe trajectory tracking results are illustrated in Figure 10.\nApparently, the proposed control algorithm can ensure closed-\nloop stability, while the vanilla RL fails to do so. A better\ntracking performance is obtained by the proposed control law\nin comparison with only baseline control.\nVII. CONCLUSIONS\nIn this paper, we presented a novel learning-based algorithm\nfor the control of uncertain ASV systems by combining a\nconventional control method with deep reinforcement learning.\nWith the conventional control, we ensured the overall closed-\nloop stability of the learning-based control and increase the\nsample efﬁciency of the deep RL. With the deep RL, we\nlearned to compensate for the model uncertainties, and thus\nincreased the trajectory tracking performance. In the future\nworks, we will extend the results with the consideration of\nenvironmental disturbances. The theoretical results will be\nfurther veriﬁed via experiments instead of simulations. Sample\nefﬁciency of the proposed algorithm will also be analyzed.\nREFERENCES\n[1] D. O.B.Jones, A. R.Gates, V. A.I.Huvenne, A. B.Phillips, and B. J.Bett,\n“Autonomous marine environmental monitoring: Application in de-\ncommissioned oil ﬁelds,” Science of The Total Environment, vol. 668,\nno. 10, pp. 835– 853, 2019.\n[2] J. Majohr and T. Buch, Advances in Unmanned Marine Vehicles.\nInstitution of Engineering and Technology, 2006, ch. Modelling,\nsimulation and control of an autonomous surface marine vehicle for\nsurveying applications Measuring Dolphin MESSIN.\n[3] O. Levander, “Autonomous ships on the high seas,” IEEE Spectrum,\nvol. 54, no. 2, pp. 26 – 31, 2017.\n[4] K. Do, Z. Jiang, and J. Pan, “Robust adaptive path following of\nunderactuated ships,” Autonomous Agents and Multi-Agent Systems,\nvol. 40, no. 6, pp. 929 – 944, Nov. 2004.\n[5] K. Do and J. Pan, “Global robust adaptive path following of under-\nactuated ships,” Automatica, vol. 42, no. 10, pp. 1713 – 1722, Oct.\n2006.\n[6] C. R. Sonnenburg and C. A. Woolsey, “Integrated optimal formation\ncontrol of multiple unmanned aerial vehicles,” Journal of Field Robotics,\nvol. 3, no. 30, pp. 371 – 398, May/Jun. 2013.\n(a) Model reference reinforcement learning control\n(b) Only deep reinforcement learning\n(c) Only baseline control\nFig. 6: Trajectory tracking results of the three algorithms (The ﬁrst evaluation)\nFig. 7: Position tracking errors (ex)\nFig. 8: Position tracking errors (ey)\nFig. 9: Mean absolute distance errors (\nq\ne2x + e2y)\n[7] T. I. Fossen, Handbook of Marine Craft Hydrodynamics and Motion\nControl.\nJohn Wiley & Sons, Inc., 2011.\n[8] R. A. Soltan, H. Ashraﬁuon, and K. R. Muske, “State-dependent\ntrajectory planning and tracking control of unmanned surface vessels,”\nin Proceedings of 2009 American Control Conference.\nSt. Louis, MO,\nUSA: IEEE, Jun. 2009.\n[9] R. Yu, Q. Zhu, G. Xia, and Z. Liu, “Sliding mode tracking control of\nan underactuated surface vessel,” IET Control Theory & Applications,\nvol. 6, no. 3, pp. 461 – 466, 2012.\n[10] N. Wang, J.-C. Sun, M. J. Er, and Y.-C. Liu, “A novel extreme learning\ncontrol framework of unmanned surface vehicles,” IEEE Transactions\non Cybernetics, vol. 46, no. 5, pp. 1106 – 1117, May 2016.\n[11] N. Wang, S. Lv, W. Zhang, Z. Liu, and M. J. Er, “Finite-time observer\nbased accurate tracking control of a marine vehicle with complex\nunknowns,” arXiv preprint arXiv:1711.00832, vol. 145, no. 15, pp. 406\n– 415, 2017.\n[12] J. Woo, C. Yu, and N. Kim, “Deep reinforcement learning-based\ncontroller for path following of an unmanned surface vehicle,” Ocean\nEngineering, vol. 183, no. 1, pp. 155 – 166, Dec. 2019.\n[13] Q. Zhang and H. H. Liu, “UDE-based robust command ﬁltered\nbackstepping control for close formation ﬂight,” IEEE Transactions\non Industrial Electronics, vol. 65, no. 11, pp. 8818–8827, Nov. 2018,\nearly access online, March 12, 2018.\n[14] W. Shi, S. Song, C. Wu, and C. L. P. Chen, “Multi pseudo q-learning-\nbased deterministic policy gradient for tracking control of autonomous\nunderwater vehicles,” IEEE Transactions on Neural Networks and\nLearning Systems, vol. 30, no. 12, pp. 3534 – 3546, Dec. 2019.\n[15] T. Shen and K. Tamura, “Robust h∞control of uncertain nonlinear\nsystem via state feedback,” IEEE Transactions on Automatic Control,\nvol. 40, no. 4, pp. 766 – 768, Apr. 1995.\n[16] X. Liu, H. Su, B. Yao, and J. Chu, “Adaptive robust control of a class\nof uncertain nonlinear systems with unknown sinusoidal disturbances,”\nin Proceedings of 2008 47th IEEE Conference on Decision and Control.\nCancun, Mexico, USA: IEEE, Dec. 2008.\n[17] W. M. Haddad and T. Hayakawa, “Direct adaptive control for non-\nlinear uncertain systems with exogenous disturbances,” International\nJournal of Adaptive Control and Signal Processing, vol. 16, no. 2, pp.\n151 – 172, Feb. 2002.\n[18] Q. Zhang and H. H. Liu, “Aerodynamic model-based robust adaptive\ncontrol for close formation ﬂight,” Aerospace Science and Technology,\nvol. 79, pp. 5 – 16, 2018.\n[19] P. A. Ioannou and J. Sun, Robust Adaptive Control.\nPrentice-Hall,\nInc., 1996.\n[20] B. Zhu, Q. Zhang, and H. H. Liu, “Design and experimental evaluation\nof robust motion synchronization control for multivehicle system\nwithout velocity measurements,” International Journal of Robust and\nNonlinear Control, vol. 28, no. 7, pp. 5437 – 5463, 2018.\n[21] S. Mondal and hitralekha Mahanta, “Chattering free adaptive multivari-\nable sliding mode controller for systems with matched and mismatched\nuncertainty,” ISA Transactions, vol. 52, pp. 335 – 341, 2013.\n(a) Model reference reinforcement learning control\n(b) Only deep reinforcement learning\n(c) Only baseline control\nFig. 10: Trajectory tracking results of the three algorithms (The second evaluation)\n[22] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introductions,\n2nd ed.\nThe MIT Press, 2018.\n[23] E. Meyer, H. Robinson, A. Rasheed, and O. San, “Taming an\nautonomous surface vehicle for path following and collision avoidance\nusing deep reinforcement learning,” arXiv preprint arXiv:1912.08578,\n2019.\n[24] X. Zhou, P. Wu, H. Zhang, W. Guo, and Y. Liu, “Learn to navigate:\nCooperative path planning for unmanned surface vehicles using deep\nreinforcement learning,” IEEE Access, vol. 7, pp. 165 262 – 165 278,\nNov. 2019.\n[25] M. Han, Y. Tian, L. Zhang, J. Wang, and W. Pan, “H∞model-free\nreinforcement learning with robust stability guarantee,” arXiv preprint\narXiv:1911.02875, 2019.\n[26] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, “Safe model-\nbased reinforcement learning with stability guarantees,” in Proceedings\nof the 31st International Conference on Neural Information Processing\nSystems (NIPS 2017), Long Beach, CA, USA, Dec. 2017, p. 908919.\n[27] R. Sutton, A. Barto, and R. Williams, “Reinforcement learning is direct\nadaptive optimal control,” IEEE Control Systems Magazine, vol. 12,\nno. 2, pp. 19 – 22, Apr. 1992.\n[28] J. Hwangbo, I. Sa, R. Siegwart, and M. Hutter, “Control of a quadrotor\nwith reinforcement learning,” IEEE Robotics and Automation Letters,\nvol. 2, no. 4, pp. 2096 – 2103, Oct. 2017.\n[29] R. Skjetne, T. I. Fossen, and P. V. Kokotovi´c, “Adaptive maneuvering,\nwith experiments, for a model ship in a marine control laboratory,”\nMathematics of Operations Research, vol. 41, pp. 289 – 298, 2005.\n[30] Z. Peng, D. Wang, T. Li, and Z. Wu, “Leaderless and leader-follower\ncooperative control of multiple marine surface vehicles with unknown\ndynamics,” Nonlinear Dynamics, vol. 74, pp. 95 – 106, 2013.\n[31] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” arXiv preprint arXiv:1801.01290, 2018.\n[32] T. Haarnoja, K. H. Aurick Zhou, G. Tucker, S. Ha, J. Tan, V. Kumar,\nH. Zhu, A. Gupta, P. Abbeel, and S. Levine, “Soft actor-critic algorithms\nand applications,” arXiv preprint arXiv:1812.05905, 2018.\n[33] G. E. Dahl, T. N. Sainath, and G. E. Hinton, “Improving deep\nneural networks for lvcsr using rectiﬁed linear units and dropout,”\nin Proceedings of 2013 IEEE International Conference on Acoustics,\nSpeech and Signal Processing, Vancouver, BC, Canada, May 2013.\n[34] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\nC. B. Stig Petersen, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis, “Human-level control through\ndeep reinforcement learning,” Nature, vol. 518, pp. 529–533, Feb. 2015.\n[35] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.69801, 2014.\n[36] S. Fujimoto, H. van Hoof, and D. Meger, “Addressing function approxi-\nmation error in actor-critic methods,” arXiv preprint arXiv:1802.09477,\n2018.\n[37] H. K. Khalil, Nonlinear Systems, 3rd ed.\nPrentice Hall, 2001.\n",
  "categories": [
    "eess.SY",
    "cs.AI",
    "cs.LG",
    "cs.RO",
    "cs.SY",
    "math.OC"
  ],
  "published": "2020-03-30",
  "updated": "2020-03-30"
}