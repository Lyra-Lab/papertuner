{
  "id": "http://arxiv.org/abs/2003.14162v3",
  "title": "Deep State Space Models for Nonlinear System Identification",
  "authors": [
    "Daniel Gedon",
    "Niklas Wahlström",
    "Thomas B. Schön",
    "Lennart Ljung"
  ],
  "abstract": "Deep state space models (SSMs) are an actively researched model class for\ntemporal models developed in the deep learning community which have a close\nconnection to classic SSMs. The use of deep SSMs as a black-box identification\nmodel can describe a wide range of dynamics due to the flexibility of deep\nneural networks. Additionally, the probabilistic nature of the model class\nallows the uncertainty of the system to be modelled. In this work a deep SSM\nclass and its parameter learning algorithm are explained in an effort to extend\nthe toolbox of nonlinear identification methods with a deep learning based\nmethod. Six recent deep SSMs are evaluated in a first unified implementation on\nnonlinear system identification benchmarks.",
  "text": "Deep State Space Models for Nonlinear\nSystem Identiﬁcation ⋆\nDaniel Gedon ∗Niklas Wahlstr¨om ∗Thomas B. Sch¨on ∗\nLennart Ljung ∗∗\n∗Dept. of Information Technology, Uppsala University, Sweden\nE-mail: {daniel.gedon, niklas.wahlstrom, thomas.schon}@it.uu.se\n∗∗Div. of Automatic Control, Link¨oping University, Sweden E-mail:\nlennart.ljung@liu.se\nAbstract: Deep state space models (SSMs) are an actively researched model class for temporal\nmodels developed in the deep learning community which have a close connection to classic SSMs.\nThe use of deep SSMs as a black-box identiﬁcation model can describe a wide range of dynamics\ndue to the ﬂexibility of deep neural networks. Additionally, the probabilistic nature of the model\nclass allows the uncertainty of the system to be modelled. In this work a deep SSM class and\nits parameter learning algorithm are explained in an eﬀort to extend the toolbox of nonlinear\nidentiﬁcation methods with a deep learning based method. Six recent deep SSMs are evaluated\nin a ﬁrst uniﬁed implementation on nonlinear system identiﬁcation benchmarks.\nKeywords: Nonlinear system identiﬁcation, black box modeling, deep learning\n1. INTRODUCTION\nSystem identiﬁcation is a well-established area of auto-\nmatic control, see ˚Astr¨om and Eykhoﬀ(1971); Ljung\n(1999). A wide range of identiﬁcation methods have been\ndeveloped for parametric and non-parametric models as\nwell as for grey-box and black-box models. Contrary, the\nﬁeld of machine learning and deep learning has emerged as\nthe new standard in many disciplines to model highly com-\nplex systems, see Goodfellow et al. (2016). Deep learning\ncan identify and capture patterns as a black-box model.\nIt has been shown to be useful for high dimensional and\nnonlinear problems emerging in diverse areas such as image\nanalysis, time series modelling, speech recognition and text\nclassiﬁcation. This paper provides one step to combine the\nareas of system identiﬁcation and deep learning by showing\nthe usefulness of deep SSMs applied to nonlinear system\nidentiﬁcation. It helps to bridge the gap between the ﬁelds\nand to learn from each others advances.\nNowadays, a wide range of system identiﬁcation algo-\nrithms for parametric models are available. Parametric\nmodels such as SSMs can include pre-existing knowledge\nabout the structure of the system and can result in pre-\ncise identiﬁcation. For automatic control this is a popular\nmodel class and a variety of identiﬁcation algorithms is\navailable, e.g. Sch¨on et al. (2011).\nIn deep learning recent advances in the development of\ndeep SSMs have been made, e.g. Bayer and Osendorfer\n(2015); Chung et al. (2015); Fraccaro et al. (2016). The\nclass of deep SSMs has three main advantages. (1) Com-\npared with SSMs it is more ﬂexible due to the use of Neural\n⋆This research was partially supported by the Wallenberg AI,\nAutonomous Systems and Software Program (WASP) funded by\nKnut and Alice Wallenberg Foundation and the Swedish Research\nCouncil, contracts 2016-06079 and 2019-04956.\nNetworks (NNs). (2) In many cases it can be more expres-\nsive for temporal data than feedforward NNs because of its\nrecurrent structure including hidden states. (3) Deep SSMs\ncan capture the output uncertainty. These advantages have\nbeen exploited for the generation of handwritten text by\nLiwicki and Bunke (2005) and speech by Prahallad et al.\n(2013). The examples have highly nonlinear dynamics and\nrequire accurate uncertainty quantiﬁcation to generate\nnew realistic sequences. Our main contributions are:\n• Bring the communities of system identiﬁcation and\ndeep learning closer by rigorously elaborating a deep\nlearning model class and its learning algorithm, while\napplying it to nonlinear system identiﬁcation prob-\nlems. It extends the toolbox of possible identiﬁcation\napproaches with a new class of deep learning mod-\nels. This paper complements the work by Andersson\net al. (2019), where deterministic NNs are applied to\nnonlinear system identiﬁcation.\n• In system identiﬁcation there is a clear separation\nof model structure and parameter estimation. In this\npaper the same distinction between model structure\n(Section 2) and parameter learning (Section 3) is\ntaken as a future guideline for deep learning.\n• Six deep SSMs are compared in a uniﬁed implemen-\ntation for nonlinear system identiﬁcation (Section 4).\nThe model advantages are highlighted by showing\nthat a maximum likelihood estimate is obtained and\nadditionally the uncertainty is captured which is ben-\neﬁcial in robust control or system analysis.\n2. DEEP STATE SPACE MODELS FOR\nSEQUENTIAL DATA\nSequence modeling is an active topic in deep learning\nas motivated by the temporal nature of the physical\nenvironment. A dynamic model is required to encode the\narXiv:2003.14162v3  [eess.SY]  18 Jun 2021\nsystem dynamics. The model is identiﬁed from observed\ninput-output pairs {(ut, yt)}T\nt=1 to predicted outputs ˆyt.\nAn SSM is obtained if the computations are performed via\na latent variable h that incorporates past information:\nht = fθ(ht−1, ut, yt),\n(1a)\nˆyt = gθ(ht),\n(1b)\nwhere θ are unknown parameters. If the functions fθ(·)\nand gθ(·) are described by deep mappings such as deep\nNNs, the resulting model is referred to as a deep SSM.\nAnother deep learning research direction is that of gen-\nerative models involving generative adversarial networks\n(GANs) by Goodfellow et al. (2014) and Variational Au-\ntoencoders (VAEs) by Kingma and Welling (2014), which\nare used to learn representations of the data and generate\nnew instances from the same distribution, such as realistic\nimages. Extending VAEs to sequential models such as in\nFraccaro (2018) yields the subclass of deep SSM models\nwhich are studied in this paper. The building blocks for\nthese models are Recurrent NNs (RNNs) and VAEs.\n2.1 Recurrent Neural Networks\nRNNs are useful in modeling sequences of variable length.\nModels with external inputs ut and outputs yt at each\ntime step are considered. RNNs make use of a hidden state\nht = fθ(ht−1, ut). The function parameters are learned by\nunfolding the RNN and using backpropagation through\ntime. The most notable types of RNNs for long-term\ndependencies are Long Short-Term Memory (LSTM) net-\nworks by Hochreiter and Schmidhuber (1997) and Gated\nRecurrent Units (GRUs) by Cho et al. (2014), which yield\nempirically similar results. GRUs are used in this paper\ndue to their structural simplicity.\n2.2 Variational Autoencoders\nA VAE embeds a representation of the data distribution\nof x in a low dimensional latent variable z via an inference\nnetwork (encoder). A decoder network uses z to generate\nnew data ex of approximately the same distribution as x.\nThe conceptual idea of a VAE is visualized in Fig. 1 and\ncan be viewed as a latent variable model through z.\nWithin VAEs it is generally assumed that the data x\ncan be modelled by a normal distribution. The decoder\nis chosen accordingly as pθ(x|z) = N\n\u0000x|µdec, σdec\u0001\n. The\nparameters for this distribution are given by [µdec, σdec] =\nNNdec\nθ\n(z) as a deep NN with parameters θ, input z and\noutputs µdec and σdec. The generative model is charac-\nterized by the joint distribution pθ(x, z) = pθ(x|z)pθ(z),\nwhere the multivariate normal distribution pθ(z)\n=\nN(z|µprior, σprior) is used as prior. The prior parameters\nare usually chosen as [µprior, σprior] = [0, I].\nFig. 1. Conceptual idea of the VAE.\nFor the data embedding in z, the distribution of interest\nis the posterior p(z|x) which is intractable in general. It\nis approximated by a parametric distribution qφ(z|x) =\nN (z|µenc, σenc). The distribution parameters are encoded\nby a deep NN [µenc, σenc] = NNenc\nφ (x). This network\nis optimized by variational inference of the variational\nparameters φ shared over all data points, Blei et al. (2017).\nNotably, there exists a connection between the VAE and\nlinear dimension reduction methods such as PCA. In\nRoweis (1998) it is shown that the PCA corresponds to a\nlinear Gaussian model. Speciﬁcally, the VAE can be viewed\nas a nonlinear generalization of the probabilistic PCA.\n2.3 Combining RNNs and VAEs into deep SSMs\nTo obtain a deep SSM we combine RNNs with VAEs, see\nFig. 2 for concrete examples. The RNN can be viewed as a\nspecial case of classic SSMs with Dirac delta functions as\nstate transition distribution ep(ht|ht−1) compare with (1a)\nor Fraccaro (2018). The VAE can be used to approximate\nthe output distributions of the dynamics from the RNN\noutput, see (1b). A temporal extension of the VAE is re-\nquired for the studied class of deep SSMs. The parameters\nof the VAE prior are updated sequentially with the output\nzt of the RNN as [µprior\nt\n, σprior\nt\n] = NNprior\nθ\n(zt−1, ut). The\nstate transition distribution is given by pθ(zt|zt−1, ut) =\nN(zt|µprior\nt\n, σprior\nt\n). Note that compared with the VAE\nprior, the parameters µprior\nt\n, σprior\nt\nare now not static but\ndependent on previous time steps and describe the recur-\nrence of the model. Similarly the output distribution is\ngiven as pθ(yt|zt) = N(yt|µdec\nt\n, σdec\nt\n) with [µdec\nt\n, σdec\nt\n] =\nNNdec\nθ\n(zt). The joint distribution of the deep SSM is\npθ(y1:T , z1:T |u1:T , z0) =\nT\nY\nt=1\npθ(yt|zt)pθ(zt|zt−1, ut). (2)\nSimilar to the VAE, this expression describes the gener-\native process. It can be further decomposed with a clear\nseparation between the RNN and the VAE which yields\nthe most simple form within the studied class of deep\nSSMs, the so-called VAE-RNN from Fraccaro (2018). The\nmodel consists of stacking a VAE on top of an RNN\nas shown in Fig. 2. Notice the clear separation between\nmodel parameter learning in the inference network with\nthe available data {(ut, yt)}T\nt=1 and the output prediction\nˆyt in the generative network. The joint true posterior can\nbe factorized according to the graphical model as\npθ(y1:T , z1:T , h1:T |u1:T , h0) = pθ(y1:T |z1:T )×\n×pθ(z1:T |h1:T )ep(h1:T |u1:T , h0),\n(3)\nwith prior given by pθ(zt|ht) = N\n\u0010\nzt|µprior\nt\n, σprior\nt\n\u0011\nwith\n[µprior\nt\n, σprior\nt\n] = NNprior\nθ\n(ht) only depending on the recur-\nrent state ht. The approximate posterior can be chosen to\nmimic the same factorization\nqφ(z1:T , h1:T |y1:T , u1:T , h0) = qφ(z1:T |y1:T , h1:T )×\n×ep(h1:T |u1:T , h0).\n(4)\nIn this paper we consider variations in this class of the\ndeep SSM next to the VAE-RNN, speciﬁcally:\n• Variational RNN (VRNN) by Chung et al. (2015):\nBased on VAE-RNN but the recurrence additionally\nuses the previous latent variable zt−1 for pθ(ht) =\npθ(ht|ht−1, ut, zt−1).\n• VRNN-I by Chung et al. (2015): Same as VRNN but\na static prior is used [µprior, σprior] = [0, I] in every\ntime step.\n• Stochastic RNN (STORN) by Bayer and Osendor-\nfer (2015): Based on the VRNN-I. In the inference\nnetwork STORN additionally makes use of a forward\nrunning RNN with input yt, latent variable dt and\noutput zt. Hence, zt is characterized by pθ(zt) =\nR\npθ(zt|dt)pθ(dt|dt−1, yt)ddt.\nGraphical models for these extensions are provided in Ap-\npendix A. For VRNN and VRNN-I an additional version\nusing Gaussian mixtures as output distribution (VRNN-\nGMM) is studied. More methods are available in literature,\nsee e.g. Alias Parth Goyal et al. (2017); Fraccaro et al.\n(2016).\n(a) Inference network\n(b) Generative network\nFig. 2. Graphical model of the VAE-RNN model. Round\nblocks indicate probabilistic variables and rectangular\nblocks deterministic variables. Shaded blocks indicate\nobserved variables.\n3. MODEL PARAMETER LEARNING\n3.1 Cost Function for the VAE\nThe parameter learning method of the deep SSMs is based\non the method used for VAEs. The VAE parameters θ are\nlearned by maximum likelihood L(θ) = PN\ni=1 log pθ(xi) =\nPN\ni=1 Li(θ) with N data points {xi}N\ni=1. Performing varia-\ntional inference with shared parameters for all data results\nin the following using Jensen’s inequality\nLi(θ) = log pθ(x) = log\nZ\npθ(x, z)dz\n= log Eqφ(z|x)\n\u0014pθ(x, z)\nqφ(z|x)\n\u0015\n≥Eqφ(z|x)\n\u0014\nlog pθ(x, z)\nqφ(z|x)\n\u0015\n= eLi(θ, φ).\n(5)\nThe expression eLi(θ, φ) is referred to as the evidence lower\nbound (ELBO) and can be rewritten using the Kullback-\nLeibler (KL) divergence\neLi(θ, φ) = Eqφ [log pθ(x|z)] −KL (qφ(z|x)||pθ(z)) ,\n(6)\nwhere the expectation is w.r.t. qφ(z|x). The ﬁrst term\nencourages the reconstruction of the data by the decoder.\nThe KL-divergence in the second term is a measure of\ncloseness between the two distributions and can be inter-\npreted as a regularization term. Approximate posteriors\nqφ(z|x) far away from the prior pθ(z) are penalized. The\nELBO is then given by eL(θ, φ) = PN\ni=1 eLi(θ, φ) which is\nmaximized instead of the intractable log-likelihood L(θ).\n3.2 Cost Function for Deep SSMs\nA temporal extension of the VAE parameter learning is\nrequired for the studied deep SSMs. A similar derivation\nfor the ELBO of the VAE as in (5) leads for the generic\ndeep SSM to\neL(θ, φ) = Eqφ\n\u0014\nlog pθ(y1:T , z1:T |u1:T , z0)\nqφ(z1:T |y1:T , u1:T , z0)\n\u0015\n,\n(7)\nwhere the expectation is w.r.t. the approximate distribu-\ntion qφ(z1:T |y1:T , u1:T , z0). The factorization of the true\njoint posterior distribution from (2) can be applied which\nyields an ELBO as the sum over all time steps. Note\nthat in this generic scheme qφ(·) can be factorized as\nQT\nt=1 qφ(zt|zt−1, yt:T , ut:T ), which requires a smoothing\nstep since zt depends on all inputs and outputs for all time\nsteps t = 1, . . . , T. If there exists a similar factorization\nfor the approximate posterior as in (4), then an expression\nsimilar to (6) for the VAE in can be obtained.\nIn the VAE-RNN a solution for parameter learning is\nobtained by a clear separation between the RNN and\nthe VAE. Note that here no smoothing step for the\nvariational distribution is necessary since the states z1:T\nare independent given h1:T as can be seen by d-separation\nin Fig. 2. The same factorization as in (4) can be used.\nThe ELBO for the VAE-RNN is written as\neL(θ, φ) = Eqφ\n\u0014\nlog pθ(y1:T , z1:T , h1:T |u1:T , h0)\nqφ(z1:T , h1:T |y1:T , u1:T , h0)\n\u0015\n,\n(8)\nwhere the expectation is taken w.r.t. the approximate pos-\nterior qφ(z1:T , h1:T |y1:T , u1:T , h0). Applying the posterior\nfactorizations in (3) and (4) to the ELBO in (8) and taking\nthe expectation w.r.t. qφ(zt|yt, ht) yields\neL(θ, φ) =\nT\nX\nt=1\nEqφ\n\u0014\nlog pθ(yt|zt)pθ(zt|ht)\nqφ(zt|yt, ht)\n\u0015\n=\nT\nX\nt=1\nEqφ [log pθ(yt|zt)] −\nKL (qφ(zt|yt, ht)||pθ(zt|ht)) ,\n(9)\nwhich is of the same form as the VAE ELBO in (6), but\nwith a temporal extension summing over all time steps.\n4. NUMERICAL EXPERIMENTS\nAll six models described in Section 2 are evaluated. The\nmodel hyperparameters are the dimension of the hidden\nstate zt denoted by zdim, the dimension of the RNN hidden\nstate ht denoted by hdim and the number of layers within\nthe RNN networks nlayer. For STORN the dimension of dt\nis chosen equal to that of ht. The VRNN-GMM uses ﬁve\nGaussian mixtures in the output distribution. The encoder\nand decoder are modelled as 3-layer NN and the features\nof yt, ut, zt are extracted with 2-layer NNs.\nFor parameter learning, hyperparameter and model selec-\ntion, the data is split in training and validation data. A\nseparate test data set is used for evaluating the ﬁnal per-\nformance. The ADAM optimizer with default parameters\nis used with early stopping and batch normalization, see\nKingma and Ba (2015). The initial learning rate of 10−3\nis decreased if the validation loss plateaus. Note that the\noptimization parameters are not ﬁne-tuned for any for the\nexperiments whereas the sequence length for training is\nconsidered to be a design parameter.\nThree experiments are conducted: (1) a linear Gaussian\nsystem, (2) the nonlinear Narendra-Li Benchmark from\nNarendra and Li (1996), and (3) the Wiener-Hammerstein\n(WH) process noise benchmark from Schoukens and Noel\n(2017). The ﬁrst two experiments are considered to show\nthe power of deep SSMs for uncertainty quantiﬁcation with\nknown true uncertainty, while the last experiment serves\nas a more complex real world example. The identiﬁed\nmodels are evaluated in open loop. The initial state is\nnot estimated. The generated output sequences are com-\npared with the true test data output. As performance\nmetric, the root mean squared error (RMSE) is con-\nsidered,\nq\n1\nT\nPT\nt=1(ˆyt −yt)2 with ˆyt = µdec\nt\nsuch that\na fair comparison with maximum likelihood estimation\nmethods can be made. To quantify the quality of the\nuncertainty estimate, the negative log-likelihood (NLL)\nper time step is used, 1\nT\nPT\nt=1 −log N\n\u0000yt|µdec\nt\n, σdec\nt\n\u0001\n, de-\nscribing how likely it is that the true data point falls in\nthe model output distribution. PyTorch code is available\nhttps://github.com/dgedon/DeepSSM SysID.\n4.1 Toy Problem: Linear Gaussian System\nConsider the following linear system with process noise\nvk ∼N (0, 0.5 · I) and measurement noise wk ∼N (0, 1)\nxk+1 =\n\u0014\n0.7 0.8\n0 0.1\n\u0015\nxk +\n\u0014\n−1\n0.1\n\u0015\nuk + vk,\n(10a)\nyk = [1 0] xk + wk.\n(10b)\nThe models are trained and validated with 2 000 samples\nand tested on 5 000 samples. The same number of layers in\nthe NNs is taken for all models but with diﬀerent number\nof neurons per layer. A grid search for the selection of the\nbest architecture is performed with hdim = {50, 60, 70, 80}\nand zdim = {2, 5, 10}. Here nlayer = 1 is chosen due\nto the simplicity of the experiment. For all models the\narchitecture with the lowest RMSE value is presented.\nThe deep SSMs are compared with two methods. First,\na linear model is identiﬁed using SSEST from the system\nidentiﬁcation toolbox, Ljung (2018) with the true system\norder of 2. SSEST also estimates the output variance,\nwhich is used as baseline. Second, the true system matrices\nas best possible linear model are run in open loop without\nnoise.\nThe results are listed in Table 1; the models are listed\nwith increasing complexity. For the deep SSMs the values\nare averaged over 50 identiﬁed models and for the baseline\nmethods over 500 identiﬁcations, since these methods are\ncomputationally less expensive. The results indicate that\nthe deep SSMs can reach an accuracy close to the state\nof the art methods. Note that SSEST assumes a linear\nmodel, whereas the deep SSMs ﬁt a ﬂexible, nonlinear\nmodel. The table also shows that a more complex deep\nSSM yields more accurate results. An open loop plot with\nmean and conﬁdence interval of ±3 standard deviation\nfor the identiﬁed models by STORN and SSEST (only\nmean) is given in Fig. 3 and compared to the ground truth.\nThe uncertainty is captured well, but it is conservatively\noverestimated.\nTable 1. Results for linear Gaussian toy problem.\nModel\nRMSE\nNLL\n(hdim,zdim)\nVAE-RNN\n1.56\n1.95\n(80,10)\nVRNN-Gauss-I\n1.48\n1.82\n(50,5)\nVRNN-Gauss\n1.47\n1.85\n(80,2)\nVRNN-GMM-I\n1.45\n1.80\n(70,10)\nVRNN-GMM\n1.43\n1.79\n(50,5)\nSTORN\n1.43\n1.79\n(60,5)\nSSEST\n1.41\n1.78\n-\nTrue lin. model\n1.34\n-\n-\n300\n320\n340\n360\n380\n400\n420\n440\n−5\n0\n5\ntime steps [k]\nyk\nToy Problem: Linear Gaussian System\nTest Data\nSTORN\nSSEST\nFig. 3. Toy problem: results of open loop run for test data,\nSTORN (both with µ± 3σ) and SSEST. Shaded area\ndepicts uncertainty.\n4.2 Narendra-Li Benchmark\nThe dynamics of the Narendra-Li benchmark are given\nby Narendra and Li (1996) with additional measurement\nnoise from Stenman (1999). The benchmark is designed as\na highly nonlinear but non-physical, ﬁctional system. For\nmore details, see the appendix.\nThis benchmark is evaluated for a varying number of\ntraining samples in [2 000; 60 000]. For each identiﬁcation\n5 000 validation samples and the same 5 000 test samples\nare used. A gridsearch is performed to choose architecture\nparameters, revealing, that in general it is advantageous to\nhave larger networks. Hence, for comparability all models\nare run with hdim = 60, zdim = 10 and nlayer = 1. No\nbatch normalization is applied.\nThe results are plotted in Fig. 4 and show averaged RMSE\nand NLL values over 30 identiﬁed models for varying\ntraining data sizes. Generally, more training data yields\nmore accurate estimates, both in terms of RMSE and NLL.\nAfter a speciﬁc amount of training data, the identiﬁcation\nresults stop to improve. This plateau indicates that the\nchosen model is saturated. Larger models could be more\nﬂexible to decrease the values even further. Speciﬁcally, the\nSTORN model outperforms the other models, all of which\nshow similar performance. This is due to the enhanced\nﬂexibility in STORN via the use of a second recurrent\nnetwork in the inference, allowing for the learned state\nrepresentations zt to be more accurate.\nThe lowest RMSE values of each model are in Table 2\ncompared with results from literature. The methods com-\npared against do not estimate uncertainty, therefore NLL\ncannot be provided. Table 2 also includes the required\nnumber of samples to obtain the given performance. The\ntable indicates that deep SSMs require more samples for\nlearning than classic models which is in line with general\ndeep learning experience. Despite the performance gap, we\nbelieve this research to be of interest in areas where many\ndatapoints are available and deep SSM can provide an\naccurate black-box model. One reason for the performance\ngap can be that gray-box models from literature are com-\npared with deep SSMs which are black-box models. The\nresults indicate that in particular STORN reaches RMSE\nvalues close to gray-box models.\nAn open-loop run for identiﬁed STORN model compared\nwith the true data is given in Fig. 5. Mean value and\n±3 standard deviations are shown. The ﬁgure highlights:\nFirst, the complex nonlinear dynamics are identiﬁed well.\nSecond, the uncertainty bounds are captured but are much\nmore conservative than the true bounds.\n0\n1\n2\n3\n4\n5\n6\n·104\n0.6\n0.8\n1\n1.2\n1.4\n1.6\nRMSE\nNarendra-Li Benchmark: RMSE (top), NLL (bottom)\nVAE-RNN\nVRNN-Gauss\nVRNN-GMM\nSTORN\n0\n1\n2\n3\n4\n5\n6\n·104\n1.2\n1.4\n1.6\n1.8\nTraining data points\nNLL\nFig. 4. Narendra-Li benchmark: RMSE and NLL for vary-\ning number of training data points. VRNN-Gauss-I\nand VRNN-GMM-I with dashed lines.\nTable 2. Results for the Narendra-Li benchmark.\nModel\nRMSE\nNLL\nSamples\nVAE-RNN\n0.84\n1.34\n50 000\nVRNN-Gauss-I\n0.89\n1.31\n60 000\nVRNN-Gauss\n0.85\n1.28\n30 000\nVRNN-GMM-I\n0.87\n1.29\n20 000\nVRNN-GMM\n0.87\n1.30\n50 000\nSTORN\n0.64\n1.20\n60 000\nMultivariate adaptive\n0.46\n-\n2 000\nregression splines\nAdapt. hinging hyperplanes\n0.31\n-\n2 000\nModel-on-demand\n0.46\n-\n50 000\nDirect weight optimization\n0.43\n-\n50 000\nBasis function expansion\n0.06\n-\n2 000\n300\n320\n340\n360\n380\n400\n420\n440\n−5\n0\n5\ntime steps [k]\nyk\nNarendra-Li Benchmark\nTest Data\nSTORN\nFig. 5. Narendra-Li benchmark: Time evaluation of true\nsystem and STORN with uncertainties.\n4.3 Wiener-Hammerstein Process Noise Benchmark\nThe WH benchmark with process noise by Schoukens and\nNoel (2017) provides measured input-output data from\nan electric circuit. The system can be described by a\nnonlinear WH model which has a nonlinearity between two\nlinear dynamic systems. Process noise enters before the\nnonlinearity making the benchmark particularly diﬃcult.\nThe training data consist of 8 192 samples where the\ninput is a faded multisine realization. The validation\ndata are taken from the same data set but a diﬀerent\nrealization. The test data set consists of 16 384 samples,\none multisine realization and one swept sine. Preliminary\ntests indicate that a longer training sequence length yield\nmore accurate results, hence a length of 2 048 points is\nused. This benchmark is evaluated for varying sizes of\nthe deep SSM layers. Here hdim = {30, 40, 50, 60} with\nconstant zdim = 3 and nlayer = 3.\nThe resulting RMSE values for the multisine and swept\nsine test sequence are presented in Fig. 6. The lowest\nRMSE values of the plot are in Table 3 compared to\nstate of the art methods from the literature. The values\nare presented as averages over 20 identiﬁed models. The\nplot indicates that the inﬂuence of hdim is rather limited.\nLarger values and therefore larger NNs in general tend\nto result in more accurate identiﬁcation results. Again,\nSTORN yields the best results, while also the very simple\nVAE-RNN identiﬁes this complex benchmark well. The\njagged behaviour of the plot may arise since the chosen\nidentiﬁcation data set only consists of two realizations.\nTherefore the randomness over the multiple experiments\noriginates mainly from random initialization of the weights\nin the NNs. The diﬀerence to results from literature in\nTable 3 could result because these methods are gray-box\nmodels and incorporate system knowledge.\n5. CONCLUSION AND FUTURE WORK\nThis paper provides an introduction to deep SSMs as an\nextension to SSMs using highly ﬂexible NNs and elabo-\nrates the parameter learning method based on variational\ninference. Six deep SSMs are implemented and applied to\nthree system identiﬁcation problems to benchmark their\npotential. The results indicate that the class of deep SSMs\nis competitive to classic identiﬁcation methods. Therefore,\nthe toolbox of nonlinear identiﬁcation methods is extended\nby a new model class based on deep learning. Deep SSMs\nalso estimate the uncertainty in the dynamics by its prob-\n30\n40\n50\n60\n70\n0.05\n0.06\n0.07\n0.08\nRMSE\nWiener-Hammerstein Benchmark\nVAE-RNN\nVRNN-Gauss\nVRNN-GMM\nSTORN\n30\n40\n50\n60\n70\n0.02\n0.04\n0.06\n0.08\nhdim\nRMSE\nFig. 6. WH benchmark: RMSE of for mulitsine (top) and\nswept sine (bottom) test signal for varying hdim.\nTable 3. Results in RMSE for WH benchmark.\nModel\nswept sine\nmultisine\nVAE-RNN\n0.050\n0.059\nVRNN-Gauss-I\n0.076\n0.076\nVRNN-Gauss\n0.082\n0.079\nVRNN-GMM-I\n0.066\n0.067\nVRNN-GMM\n0.076\n0.074\nSTORN\n0.034\n0.051\nNOBF\n≈0.2\n<0.3\nNFIR\n<0.05\n<0.05\nNARX\n<0.05\n≈0.05\nPNLSS\n0.022\n0.038\nBest Linear Approx.\n-\n0.035\nML\n-\n0.016\nSMC\n0.014\n0.015\nabilistic nature, which appears to be as conservative as\nestablished uncertainty quantiﬁcation methods. This con-\nservative behavior is in line with the existing literature on\nvariational inference of deep learning models.\nThis study concerns a subclass of deep SSMs based on\nvariational inference methods. Future work should study\na broader class of deep SSMs and more nonlinear system\nidentiﬁcation benchmarks should be considered. It is of\nhigh interest to use deep SSM in automatic control like e.g.\nmodel predictive control and to elaborate how to exploit\nthe latent state variables.\nREFERENCES\nAlias Parth Goyal, A.G., Sordoni, A., Cˆot´e, M.A., Ke,\nN.R., and Bengio, Y. (2017).\nZ-Forcing: Training\nStochastic Recurrent Networks. In Advances in Neural\nInformation Processing Systems 30, 6713–6723. Curran\nAssociates, Inc.\nAndersson, C., Ribeiro, A.H., Tiels, K., Wahlstr¨om, N.,\nand Sch¨on, T.B. (2019).\nDeep Convolutional Net-\nworks in System Identiﬁcation.\nIn Proceedings of the\n58th IEEE Conference on Decision and Control. Nice,\nFrance.\n˚Astr¨om, K.J. and Eykhoﬀ, P. (1971). System identiﬁca-\ntion—A survey. Automatica, 7(2), 123–162.\nBayer, J. and Osendorfer, C. (2015). Learning Stochastic\nRecurrent Networks. arXiv:1411.7610.\nBlei, D.M., Kucukelbir, A., and McAuliﬀe, J.D. (2017).\nVariational Inference: A Review for Statisticians. Jour-\nnal of the American Statistical Association, 112(518),\n859–877.\nCho, K., van Merri¨enboer, B., Bahdanau, D., and Bengio,\nY. (2014). On the Properties of Neural Machine Trans-\nlation: Encoder–Decoder Approaches. In Proceedings of\nSSST-8, Eighth Workshop on Syntax, Semantics and\nStructure in Statistical Translation, 103–111. Associa-\ntion for Computational Linguistics, Doha, Qatar.\nChung, J., Kastner, K., Dinh, L., Goel, K., Courville, A.C.,\nand Bengio, Y. (2015).\nA Recurrent Latent Variable\nModel for Sequential Data.\nIn Advances in Neural\nInformation Processing Systems 28, 2980–2988.\nFraccaro, M. (2018).\nDeep Latent Variable Models for\nSequential Data. Ph.D. thesis, DTU Compute.\nFraccaro, M., Sønderby, S.K., Paquet, U., and Winther, O.\n(2016). Sequential neural models with stochastic layers.\nIn Proceedings of the 30th International Conference\non Neural Information Processing Systems, 2207–2215.\nBarcelona, Spain.\nGoodfellow, I., Bengio, Y., and Courville, A.C. (2016).\nDeep Learning. MIT Press.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. (2014). Generative Adversarial Nets. In Advances in\nNeural Information Processing Systems 27, 2672–2680.\nHochreiter, S. and Schmidhuber, J. (1997). Long Short-\nTerm Memory. Neural Comput., 9(8), 1735–1780.\nKingma, D.P. and Ba, J. (2015). Adam: A Method for\nStochastic Optimization. In 3rd International Confer-\nence on Learning Representations, (ICLR). San Diego,\nCA, USA.\nKingma, D.P. and Welling, M. (2014).\nAuto-Encoding\nVariational Bayes. In Proceedings of the International\nConference on Learning Representations (ICLR). Banﬀ,\nCanada.\nLiwicki, M. and Bunke, H. (2005).\nIAM-OnDB - an\non-line English sentence database acquired from hand-\nwritten text on a whiteboard. In Eighth International\nConference on Document Analysis and Recognition (IC-\nDAR’05), 956–961 Vol. 2.\nLjung, L. (1999).\nSystem identiﬁcation: theory for the\nuser. PTR Prentice Hall, Upper Saddle River, NJ.\nLjung, L. (2018).\nSystem Identiﬁcation Toolbox: The\nManual. The MathWorks Inc., Natick, MA, USA, 9th\nedition 2018 edition.\nNarendra, K.S. and Li, S.M. (1996).\nNeural networks\nin control systems.\nchapter 11, 347–394. Lawrence\nErlbaum Associates, Hillsdale, NJ, USA.\nPrahallad, K., Vadapalli, A., Elluru, N.K., Mantena, G.V.,\nPulugundla, B., Bhaskararao, P., Murthy, H.A., King,\nS.J., Karaiskos, V., and Black, A.W. (2013).\nThe\nBlizzard Challenge 2013 - Indian Language Tasks.\nRoweis, S.T. (1998). EM Algorithms for PCA and SPCA.\nIn Advances in Neural Information Processing Systems\n10, 626–632.\nSch¨on, T.B., Wills, A., and Ninness, B. (2011). System\nidentiﬁcation of nonlinear state-space models. Automat-\nica, 47(1), 39–49.\nSchoukens,\nM.\nand\nNoel,\nJ.P.\n(2017).\nWiener-\nHammerstein benchmark with process noise. 20th IFAC\nWorld Congress, 50(1), 448–453.\nStenman, A. (1999). Model on Demand: Algorithms, Anal-\nysis and Applications. Number 571 in Link¨oping Studies\nin Science and Technology Dissertation. Link¨oping Uni-\nversity.\nAppendix A. GRAPHICAL MODELS FOR DEEP SSMS\nIn section 2 the focus lies on the most simple deep SSM, namely the VAE-RNN. The loss function is then derived based\non the graphical model for the VAE-RNN in Fig 1. Here the graphical models for all other studied deep SSMs are\npresented and shortly explained.\nA.1 VRNN\nThe graphical model for the VRNN is shown in Fig. A.1. Note that the VRNN-Gauss and VRNN-GMM use the same\nnetwork architecture. The diﬀerence lies in the output distribution, which is either Gaussian or a Gaussian mixture. In\nthe VRNN the recurrence additionally makes use of the previous latent variable zt−1. In the generative network also\nthe hidden state ht has a direct inﬂuence on ˆyt. Both of these features give more ﬂexibility for the network output\ndistribution. The joint true posterior of the VRNN can be factorized according to the generative network in Fig. A.1 as\npθ(y1:T , z1:T , h1:T |u1:T , h0) = pθ(y1:T |z1:T , h1:T )×\n×pθ(z1:T |h1:T )ep(h1:T |z1:T , u1:T , h0).\n(A.1)\nThe joint approximate posterior of the VRNN factorization according to the inference network as\nqφ(z1:T , h1:T |y1:T , u1:T , h0) = qφ(z1:T |y1:T , h1:T )×\n×ep(h1:T |z1:T , u1:T , h0).\n(A.2)\n(a) Inference network\n(b) Generative network\nFig. A.1. Graphical model for VRNN.\nA.2 VRNN-I\nThe VRNN-I is a simple modiﬁcation of the VRNN and its graphical model is shown in Fig. A.2. The diﬀerence to the\nVRNN is that the prior in the generative network is static and does not change temporally by the recurrent network.\nSimilarly, here the VRNN-Gauss-I and VRNN-GMM-I only diﬀer in the output distribution but not in the network\nstructure. For the VRNN-I the same true and approximate joint posterior distributions as for the VRNN above apply\nwith the diﬀerence in the true posterior that the prior is static pθ(z1:T |[0, I]).\n(a) Inference network\n(b) Generative network\nFig. A.2. Graphical model for VRNN-I. Note that the inference network is equal to the VRNN inference network.\nA.3 STORN\nThe graphical model for STORN is given in Fig. A.3. The main diﬀerence to the other models is the additional forward\nrunning recurrent network in the inference network. This recurrence is implemented as a GRU with the same hidden layer\ndimension ddim as the other recurrence with hdim. This additional recurrence helps to encode the output distribution\nmore precisely. Note also that in the generative network a static prior is used, similar to the VRNN-I. The joint true\nposterior of STORN can be factorized according to Fig. A.3 as\npθ(y1:T , z1:T , h1:T |u1:T , h0) = pθ(y1:T |h1:T )×\n×pθ(z1:T |[0, I])ep(h1:T |z1:T , u1:T , h0).\n(A.3)\nThe joint approximate posterior of STORN factorization as\nqφ(z1:T , h1:T ,d1:T |y1:T , u1:T , h0, d0) =\n= qφ(z1:T |d1:T , h1:T )ep(d1:T |y1:T , d0)×\n× ep(h1:T |y1:T , u1:T , h0).\n(A.4)\n(a) Inference network\n(b) Generative network\nFig. A.3. Graphical model for STORN.\nAppendix B. TOY PROBLEM: LINEAR GAUSSIAN SYSTEM\nFor identiﬁcation of the linear Gaussian toy problem an excitation input signal with uniform random noise in the\nrange [−2.5; 2.5] is used for the training and validation signals. The presented results are averaged over 50 Monte Carlo\nidentiﬁcations. For each of these identiﬁcations the training and validation sequences are drawn from a new realization\nwith the same statistical properties. For the test data the input is given by\nuk = sin\n\u00122kπ\n10\n\u0013\n+ sin\n\u00122kπ\n25\n\u0013\n.\n(B.1)\nThe same test data set is used for all identiﬁed systems in order to obtain comparable performance measures.\nThe numerical results from Fig. 3 show that the uncertainty quantiﬁcation is conservative compared to the true\nuncertainty bounds of the system. Here an additional ﬁgure is provided to compare state of the art uncertainty\nquantiﬁcation as calculated by SSEST with the uncertainty quantiﬁcation given by a deep SSM. In Fig. B.1 this\ncomparison is shown for the same time sequence as previously in Fig. 3. It indicates that the uncertainty quantiﬁcation of\nSTORN is comparable with the one of SSEST. Fine tuning of the hyperparameter of STORN could yield an uncertainty\nbound which tens towards the one of SSEST. In this experiment no ﬁne tuning is performed.\nAppendix C. NARENDRA-LI BENCHMARK\nThe true dynamics of the Narendra-Li Benchmark are given by Narendra and Li (1996) with the following second order\nmodel\n300\n320\n340\n360\n380\n400\n420\n440\n−5\n0\n5\ntime steps [k]\nyk\nToy LGSSM: Comparison SSEST and deep SSM\nSTORN, µ ± 3σ\nSSEST, µ ± 3σ\nFig. B.1. LGSSM toy problem: Comparison between SSEST and STORN for their uncertainty estimation.\n\"\nx(1)\nk+1\nx(2)\nk+1\n#\n=\n\n\n \nx(1)\nk\n1 + (x(1)\nk )2 + 1\n!\nsin(x(2)\nk )\nx(2)\nk\ncos(x(2)\nk ) + x(1)\nk\nexp(−(x(1)\nk )2 + (x(2)\nk )2\n8\n) + . . .\n· · · +\nu3\nk\n1 + u2\nk + 0.5 cos(x(1)\nk\n+ x(2)\nk )\n\n\n,\nyk =\nx(1)\nk\n1 + 0.5 sin(x(2)\nk )\n+\nx(2)\nk\n1 + 0.5 sin(x(1)\nk )\n+ ek.\nAdditional measurement noise is added to the original problem by Stenman (1999) of ek ∼N (0, 0.1) to make the\nproblem more challenging. The same procedure for the excitation signals as for the linear Gaussian toy problem is used.\nNamely a training and validation data set where the input is uniform random noise in the range [−2.5; 2.5] and for the\ntest data set the input sequence is deﬁned by uk = sin\n\u0000 2kπ\n10\n\u0001\n+ sin\n\u0000 2kπ\n25\n\u0001\n.\nAppendix D. WIENER-HAMMERSTEIN PROCESS NOISE BENCHMARK\nIn section 4.3 all studied deep SSMs are compared for their performance on the test data sets of the Wiener-Hammerstein\nprocess noise benchmark. Additionally, here a time evaluation is shown in Fig. D.1 for both test data sets. Note that\nonly the ﬁrst 51.2 [ms] of the total ≈20.97 [ms] are shown to have well visible plots. The ﬁgure indicates an accurate\nidentiﬁcation of the complex system dynamics, which can represent the dynamics on two diﬀerent test data sets. The\nuncertainty bounds are similarly conservative to the ones in the Narendra-Li benchmark. Tests with identiﬁcations on\navailable data sets with more samples yield tighter uncertainty bounds but are not presented here since it would not\nbe comparable with the comparison methods from literature.\nAll comparison methods in Tab. 3 use the same amount of training samples (8192), except from PNLSS which uses 9\nrealization with each consisting of 8192 samples.\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\n−0.4\n−0.2\n0\n0.2\n0.4\ntime [s]\nyk\nWiener-Hammerstein Benchmark: Multisine Test Data\nTest Data: Multisine\nSTORN, µ ± 3σ\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\n−0.4\n−0.2\n0\n0.2\n0.4\ntime [s]\nyk\nWiener-Hammerstein Benchmark: Swept Sine Test Data\nTest Data: Swept Sine\nSTORN, µ ± 3σ\nFig. D.1. Wiener Hammerstein benchmark: Time evaluation for multisine and swept sine test data set of best results\nfrom Tab. 3, i.e. STORN with hdim = 40, zdim = 3, nlayers = 3.\n",
  "categories": [
    "eess.SY",
    "cs.LG",
    "cs.SY",
    "stat.ML"
  ],
  "published": "2020-03-31",
  "updated": "2021-06-18"
}