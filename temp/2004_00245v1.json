{
  "id": "http://arxiv.org/abs/2004.00245v1",
  "title": "Depth Selection for Deep ReLU Nets in Feature Extraction and Generalization",
  "authors": [
    "Zhi Han",
    "Siquan Yu",
    "Shao-Bo Lin",
    "Ding-Xuan Zhou"
  ],
  "abstract": "Deep learning is recognized to be capable of discovering deep features for\nrepresentation learning and pattern recognition without requiring elegant\nfeature engineering techniques by taking advantage of human ingenuity and prior\nknowledge. Thus it has triggered enormous research activities in machine\nlearning and pattern recognition. One of the most important challenge of deep\nlearning is to figure out relations between a feature and the depth of deep\nneural networks (deep nets for short) to reflect the necessity of depth. Our\npurpose is to quantify this feature-depth correspondence in feature extraction\nand generalization. We present the adaptivity of features to depths and\nvice-verse via showing a depth-parameter trade-off in extracting both single\nfeature and composite features. Based on these results, we prove that\nimplementing the classical empirical risk minimization on deep nets can achieve\nthe optimal generalization performance for numerous learning tasks. Our\ntheoretical results are verified by a series of numerical experiments including\ntoy simulations and a real application of earthquake seismic intensity\nprediction.",
  "text": "1\nDepth Selection for Deep ReLU Nets in Feature\nExtraction and Generalization\nZhi Han, Siquan Yu, Shao-Bo Lin, and Ding-Xuan Zhou\nAbstract—Deep learning is recognized to be capable of discov-\nering deep features for representation learning and pattern recog-\nnition without requiring elegant feature engineering techniques\nby taking advantage of human ingenuity and prior knowledge.\nThus it has triggered enormous research activities in machine\nlearning and pattern recognition. One of the most important\nchallenge of deep learning is to ﬁgure out relations between\na feature and the depth of deep neural networks (deep nets\nfor short) to reﬂect the necessity of depth. Our purpose is to\nquantify this feature-depth correspondence in feature extraction\nand generalization. We present the adaptivity of features to\ndepths and vice-verse via showing a depth-parameter trade-off in\nextracting both single feature and composite features. Based on\nthese results, we prove that implementing the classical empirical\nrisk minimization on deep nets can achieve the optimal general-\nization performance for numerous learning tasks. Our theoretical\nresults are veriﬁed by a series of numerical experiments including\ntoy simulations and a real application of earthquake seismic\nintensity prediction.\nIndex Terms—Deep nets, feature extractions, generalization,\nlearning theory\nI. INTRODUCTION\nA\nSystemic machine learning process frequently comes\ndown to two steps: feature extraction and target-driven\nlearning. The former focuses on designing preprocessing\npipelines and data transformations that result in a tractable rep-\nresentation of data, while the latter utilizes learning algorithms\nrelated to speciﬁc targets, such as regression, classiﬁcation and\nclustering on the data representation to ﬁnish the learning task.\nStudies in the second step abound in machine learning [5]\nand numerous learning schemes such as kernel methods [15],\nneural networks [20] and boosting [21] have been proposed.\nHowever, feature extraction in the ﬁrst step is usually labor in-\ntensive, which requires elegant feature engineering techniques\nby taking advantages of human ingenuity and prior knowledge.\nTo extend the applicability of machine learning, it is crucial\nto make learning algorithms be less dependent of human\nfactors. Deep learning [23], [16], which has been successfully\nused in image classiﬁcation, natural language processing and\ngame theory, provides a promising technique in machine\nlearning. The heart of deep learning is to adopt deep neural\nZ. Han and Yu are with the State Key Laboratory of Robotics, Shenyang\nInstitute of Automation, Chinese Academy of Sciences, Shenyang, China and\nInstitutes for Robotics and Intelligent Manufacturing, Chinese Academy of\nSciences, Shenyang, China. S. Yu is also with the School of Information\nScience and Engineering, Northeastern University, Shenyang, China. S.B. Lin\nis with the Center of Intelligent Decision-Making and Machine Learning,\nSchool of Management, Xi’an Jiaotong University, Xi’an, China. D.X. Zhou\nis with the School of Data Science and Department of Mathematics, City\nUniversity of Hong Kong, Hong Kong, China.\nCorresponding author: S. B. Lin (sblin1983@gmail.com)\nnetworks (deep nets for short) with certain structures to extract\ndata features and design target-driven algorithms, simultane-\nously. As shown in Figure 1, deep learning embodies the\nutilities of feature extraction algorithms such as bag of feature\n(BOF), local binary pattern (LBP), histogram of oriented\ngradient (HOG) and classiﬁcation algorithms like support\nvector machine (SVM), random forest, via tuning parameters\nin a uniﬁed deep nets model.\nInput \nSVM \nOutput \nDeep \nlearning \nInput \nOutput \nFeature extraction \nBOF \nLBP \nHOG \nRandom Forest \nClassification \nFig. 1: Magic behind deep learning\nThe great success of deep learning in applications demon-\nstrates the feasibility of deep nets in speciﬁc learning tasks.\nHowever, whether\ndeep learning is generalizable to other\nlearning tasks relies on rigorous theoretical veriﬁcations,\nwhich is unfortunately at its infancy. In particular, it is highly\ndesired to clarify the following three important problems:\n1) which data features1 can be extracted by deep nets; 2)\nhow to set the depth of deep nets in special learning tasks;\n3) How about the generalization ability of deep learning\nalgorithms. The ﬁrst problem\nrefers to the representation\nperformance of deep nets, needing tools from information\ntheory like coding theory [39] and entropy theory [18]. The\nsecond one concerns approximation abilities of deep nets with\ndifferent depth, requiring approximation theory techniques\nsuch as\nlocal polynomial approximations [50], covering\nnumber estimates [26] and wavelets analysis [55] to quantify\npowers and limitations of deep nets. The last one focuses on\nthe generalization capability of deep learning algorithms in\nmachine learning, for which statistical learning theory as well\nas empirical processing [10] should be utilized.\nAlthough lagging heavily behind applications, recent de-\nvelopments of deep learning theory provided some exciting\ntheoretical results on these problems. For example, [45] proved\nthat deep nets succeed in extracting some geometric structures\n1 Data feature in this paper means priors for presentation learning according\nto the terminology in the nice review paper [3]. It includes both the a-priori\ninformation of target functions and structures of the input space.\narXiv:2004.00245v1  [cs.LG]  1 Apr 2020\n2\nof data, which has been adopted in [8] to design deep learning\nalgorithm for regression problems with data generated on\nmanifolds; [7] found that deep nets can extract local position\ninformation of data, which was recently employed in [31] to\nconstruct deep nets in handling sparsely located data; [39]\nproved that deep nets can extract piecewise features of data,\nwhich was utilized in [24] to develop learning algorithms to\nlearn non-smooth functions efﬁciently. All these interesting\nstudies presented theoretical veriﬁcations on the power of deep\nlearning in the sense that deep nets succeed in extracting\ndata features and deep learning signiﬁcantly improves the\ngeneralization capabilities of learning schemes in-hand.\nThe problem is, however, there are strongly exclusive cor-\nrespondences between data features and network depth for\nthese theoretical studies in the sense that each data feature\nrequires a unique network depth and vice-verse. To be detailed,\na\nhierarchal structure corresponds to a hierarchal deep net\nwith the same depth [37]; smoothness\na-priori information\n[50] is related to a deep net with accuracy-dependent depth; a\ntranslation-invariance property requires a convolutional neural\nnetwork with accuracy-dependent layers [6]; and a rotation-\ninvariance property is associated with a deep net with tree\nstructures and four layers [9]. Such exclusive correspondences\nhinder heavily the use of deep nets in feature extraction, since\ndata features such as the smoothness information, hierarchal\nstructure, transformation-invariance are practically difﬁcult to\nbe speciﬁed before the learning process. Furthermore, it is\nquestionable to determine the network depth for simultane-\nously extracting multiple data features\nlike the translation-\ninvariance and rotation-invariance, which is pretty common\nin practice. Our ﬁrst purpose is to break through the feature-\ndepth correspondences by means of proving that deep nets\nwith certain depth can extract several data features and vice-\nverse.\nWe consider extracting both single data features such\nas smoothness, rotation-invariance, sparseness and composite\ndata features combining smoothness, rotation-invariance and\nsparseness to demonstrate the adaptivity of network depth to\nfeatures and vice-verse. Intuitively, it is difﬁcult for deep ReLU\nnets to extract smoothness features due to the non-smooth\nproperty of the ReLU function σ(t) = max{t, 0}. A natural\nremedy for this, as shown in [50], is to deepen the network\nwith an accuracy-dependent depth to eliminate the negative\neffect of non-smoothness. Since under some speciﬁed capacity\nmeasurements such as the number of linear regions [38],\nBetti numbers [4], number of monomials [12] and covering\nnumbers [18], the capacity of deep nets increases exponentially\nwith respect to the depth, large depth usually means large\ncapacity costs for feature extraction. Furthermore, from an\noptimization viewpoint, large depth requires to solve a highly\nnonconvex optimization problem [16, Sec. 8.2] involving the\nill-conditioning of the Hessian, the existence of many local\nminima, saddle points, plateau and even some ﬂat regions,\nmaking it difﬁcult to design optimization algorithms for such\ndeep nets with convergence guarantees. Based on these, we\nprovide a theoretical guidance for depth selection to extract\ndata features by showing that deep nets with various depths,\nlarger than a speciﬁed value, are capable of extracting the\nsmoothness and other data features. This shows an adaptivity\nof the depth to data features in the sense that any data features\nfrom a rich family can be extracted by deep nets with various\ndepths. Conversely, we also provide theoretical guarantees on\nthe adaptivity of the data feature to depths by showing that\ndeep ReLU nets with some speciﬁc depth succeed in extracting\nthe smoothness, spareness and composite features. All these\nremove the feature-depth correspondences in feature extraction\nfor deep ReLU nets.\nFrom feature extraction to machine learning, the tug of war\nbetween bias and variance [10] indicates that the prominent\nperformance of deep nets in feature extraction is insufﬁcient\nto demonstrate its success. The good generalization ability\nis frequently built upon the balance between the accuracy\nof feature extraction and capacity costs to achieve such an\naccuracy. This exhibits a bias-variance dilemma in selecting\nthe capacity of deep nets. Different from shallow learning\nsuch as kernel methods and boosting, recent studies [22], [18]\npresented a depth-parameter dilemma in controlling the ca-\npacity of deep nets in the sense that different depth-parameter\npairs may yield the same capacity. These two dilemmas as\nwell as the optimization difﬁculty\n[16, Sec. 8.2] pose an\nurgent issue for deep learning theory on selecting the depth\nto guarantee the good generalization ability of deep learning\nalgorithms. Our second purpose is not only to pursue the\noptimal generalization error for learning schemes based on\ndeep nets, but also to demonstrate the depth selection strategy\nto realize this optimality.\nWe study the generalization ability of deep nets with dif-\nferent depths via empirical risk minimization (ERM). Based\non the established adaptivity of the depth to data features in\nfeature extraction, we establish almost optimal generalization\nerror bounds for deep nets with numerous depth-parameter\npairs. Our results show that the feature extraction step is\nnecessary when the learning task is somewhat sophisticated\nand deep nets succeed in extracting deep data features of\nthe data distribution, which illustrates the necessity of depth\nin deep learning. However, we also prove that the depth for\nrealizing the optimal learning performance of deep nets is not\nunique. In fact, with depth larger than some speciﬁed value,\nall deep nets theoretically perform similarly and can achieve\nthe optimal generalization error bounds. The only difference\nis that deeper nets involve less free parameters.\nIn a nutshell, our analysis implies three interesting ﬁndings\nin understanding the success of deep learning. The ﬁrst is the\nﬂexibility on automatically selecting the accuracy in extracting\ndata features via tuning the network parameters, which is\ndifferent from the classical two-step learning scheme presented\nin Figure 1 that usually involves extremely high capacity costs\nto fully extract data features. The second is the versatility of\ndeep nets with ﬁxed depth in the sense that they can extract\nvarious data features. The third one is that if the depth is larger\nthan a speciﬁed value, we can always get a deep net estimator\nwith almost optimal theoretical guarantee. The problem is,\nhowever, the difﬁculty in solving ERM on deep nets increases\n3\nwith respect to the depth [16, Sec. 8.2] and [1]2. Thus, it is\nnumerically difﬁcult\nand time-consuming to get a deep net\nestimator with large depth and there is practically an optimal\ndepth to realize the established optimal generalization error\nbounds, just as our experimental results exhibit.\nThe rest of the paper is organized as follows. In the next\nsection, we will introduce deep nets and show some recent\ndevelopments of deep nets in feature extraction. Section 3\nfocuses on the depth selection for deep ReLU nets in extracting\nsingle data features, while Section 4 devotes to the depth\nselection in extracting composite data features. In Section\n5, we are interested in the generalization error analysis for\nimplementing ERM on deep nets. Section 6 exhibits some\nnumerical results to verify our theoretical assertions. In the\nlast section, we draw a simple conclusion and present some\nfurther discussions.\nII. NECESSITY OF DEPTH IN FEATURE EXTRACTION\nLet d ∈N be the dimension of input space. Denote\nx = (x(1), . . . , x(d)) ∈Id := [−1, 1]d. Let L ∈N and\nd0, d1, . . . , dL ∈N with d0 = d. For ⃗h = (h(1), . . . , h(dk))T ∈\nRdk, deﬁne ⃗σ(⃗h) = (σ(h(1)), . . . , σ(h(dk)))T . Deep ReLU\nnets with depth L and width dj in the j-th hidden layer can\nbe mathematically represented as\nh{d0,...,dL,σ}(x) = ⃗a · ⃗hL(x),\n(1)\nwhere\n⃗hk(x) = ⃗σ(Wk · ⃗hk−1(x) +⃗bk),\nk = 1, 2, . . . , L,\n(2)\n⃗h0(x) = x, ⃗a ∈RdL, ⃗bk ∈Rdk, and Wk = (W i,j\nk )dk,dk−1\ni=1,j=1\nis a dk × dk−1 matrix. Denote by H{d0,...,dL,σ} the set of all\nthese deep nets. When L = 1, the function deﬁned by (1) is\nthe classical shallow net.\nThe structure of deep nets is reﬂected by structures of\nweight matrices Wk and threshold vectors ⃗bk and ⃗a, k =\n1, 2, . . . , L. Besides the deep fully connected networks [50]\nthat counts the number of free parameters in the k-th layer to\nbe dkdk−1 + dk3, we say that there are nk free parameters\nin the k-th layer, if the weight matrix Wk and thresholds\n⃗bk are generated through the following three ways. The ﬁrst\nway is that there are totally nk tunable entries in Wk and\n⃗bk, while the remainder dkdk−1 + dk −nk entries are ﬁxed.\nAn example is deep sparsely connected neural networks. The\nsecond way is that Wk and ⃗bk are exactly generated by nk\nfree parameters including weight-sharing. The third way is\nthat the weight matrix is generated jointly by both the above\nways. Like the most widely used deep convolutional neural\nnetworks, we count the number of free parameters according\nto the third way by considering both sparse connections and\nweight-sharing [54], [55], [56]. It should be mentioned that\nsuch a way to count free parameter is different from [50]\nwhich considers deep fully connected neural networks. The\n2 Here, the difﬁculty means that larger depth requires more free parameters\nunder an over-parameterization setting to guarantee the convergence of SGD\nto a local optimal of ERM of high quality and larger depth results in more\nlocal minima, saddle points and ﬂat regions.\n3 If k=L, the number is dLdL−1 + 2dL by taking the outer weights into\naccounts\ndifferent way to count free parameters is consistent with the\nstructure of deep nets, which is the main reason why we can\nimprove the approximation result of [50].\nA. Capacity measurements of deep nets\nIt is meaningless to pursue the outperformance of deep\nnets over shallow nets without considering the capacity costs,\nsince the universality of shallow nets [11], [27] demonstrates\nthat shallow nets can extract an arbitrary data feature as long\nas the network is sufﬁciently wide. We adopt the concept\nof covering number [52] which is widely used in statistical\nlearning and information theory to measure the capacity to\ncast the comparison into a uniﬁed framework.\nLet B be a Banach space and V be a subset of B. Denote\nby N(ε, V, B) the ε-covering number of V under the metric\nof B, which is the minimal number of elements in an ε-net of\nV . Intuitively, the ε-covering number measures the capacity of\nV via counting the minimal number of balls in B with radius\nε covering V . Figure 2 showes that the 0.1-covering number\nof A is 19 while that of B is 10, coinciding with the intuitive\nobservation that A is larger than B.\nA\nB\nFig. 2: Covering numbers of different sets\nThe quantity H(ε, V, B) = log2 N(ε, V, B) is called the\nε-entropy of V in B which is close to the coding length\nin information theory according to the encode-decode theory\n[13]. Thus, it is a powerful capacity measurement to show the\nexpressivity of V in B. Furthermore, the ε-covering number\ndetermines the limitation of approximation ability of V [18]\nand also the stability of learning algorithms deﬁned on V [10].\nAll these demonstrate the rationality of adopting the covering\nnumber to measure the capacity of deep nets.\nDenote by Hn,L the set of all deep nets with L hidden\nlayers, n free parameters and by\nHn,L,R := {hn,L ∈Hn,L : |wi,j\nk |, |bi\nk|, |ai| ≤R,\n1 ≤i ≤dk, 1 ≤j ≤dk−1, 1 ≤k ≤L}\n(3)\nthe set of deep nets whose weights and thresholds are uni-\nformly bounded by R, where R is some positive number that\nmay depend on n, dk, and L. The boundedness assumption\nis necessary since it can be found in [35], [18] that there\nexists some deep nets with two hidden layers and ﬁnitely many\nneurons possessing an inﬁnite covering number.\n4\nThe following lemma proved in [18] presents a tight esti-\nmate for the covering number of deep ReLU nets.\nLemma 1. Let Hn,L,R be deﬁned by (3). Then\nN\n\u0000ε, Hn,L,R, L∞(Id)\n\u0001\n≤(CRDmax)3(L+1)2n ε−n,\n(4)\nwhere Dmax := max0≤ℓ≤L dℓand C is a constant depending\nonly on d.\nIt was deduced in [19, Chap. 16] that\nlog N(ε, Hn,1,R, L1(Id)) = O\n\u0012\nn log R\nε\n\u0013\n.\n(5)\nComparing Lemma 1 with (5), we ﬁnd that, up to a logarithmic\nfactor, deep nets with controllable magnitudes of weights do\nnot essentially enlarge the capacity of shallow nets, provided\nthat they have the same number of free parameters and the\ndepth of deep nets is at most log n. Furthermore, Lemma 1\nimplies that the depth plays a similar role as the number of\nparameters in controlling the capacity of deep nets, when ε\nis not extremely small. This shows a novel depth-parameter\ndilemma in controlling the capacity and is totally different\nfrom shallow nets.\nB. Limitations of shallow nets in extracting features\nThe study of approximation capability of shallow nets is a\nclassical topic in neural networks. We refer the readers to a\nfruitful review paper [41] for details on this topic. Compared\nwith the classical linear approaches like polynomials, shallow\nnets with sigmoidal activation function possess better approx-\nimation ability [36] and are capable of conducting dimension-\nindependent error estimates under certain restrictions on the\ntarget functions [2]. More importantly, the universality [11],\n[27] showed that shallow nets can extract any data feature as\nlong as the network is sufﬁciently wide. However, with ﬁxed\nwidth, they have limitations in feature extraction, in terms of\nsaturation [33], non-localization [7], [42], non-sparse approxi-\nmation [28], [31] and bottleneck in extracting the smoothness\nfeature [35], [30]. In particular, it was shown in [35] that\nshallow nets whose capacity satisﬁes (5) cannot extract the\nsmoothness features within accuracy O(n−r/(d−1)) with high\nprobability, where r denotes the degree of smoothness.\nFor shallow nets with ReLU (shallow ReLU nets), the\nlimitation is even stricter. It was shown in [14] that there exist\nsome analytic univariate functions which cannot be expressible\nfor shallow ReLU nets. Recently, [50, Theorem 6] proved\nthat any twice-differentiable nonlinear function deﬁned on Id\ncannot be ε-approximated by ReLU networks of ﬁxed depth\nL with the number of free parameters less than cε−1/(2(L−2)),\nwhere c is a positive constant depending only on d. A direct\nconsequence is that a ReLU network with depth L = 3 and n\nfree parameters cannot extract the simple “square-feature”, i.e.,\nt2, within accuracy n−2−τ for an arbitrary τ > 0. By noting\nt2 is an inﬁnitely differentiable function, it is well known that\nthere exist linear tools to approximate t2 within accuracy of\norder n−Γ [40] for an arbitrarily large Γ < ∞. All these results\nshowed that shallow nets, especially shallow ReLU nets, are\ndifﬁcult to extract data features and thus have bottlenecks in\ncomplex learning tasks.\nC. Necessity of the depth for ReLU nets\nAdvantages of deep nets over shallow nets were ﬁrstly re-\nvealed by [7] in the sense that deep nets can provide localized\napproximation but shallow nets fail. Since then, a great number\nof data features including those for sparseness, manifold\nstructures, piecewise smoothness and rotation-invariance are\nproved to be unrealizable by shallow nets but can be easily\nextracted by deep nets. Under the capacity constraint (4) that is\nsimilar to (5) for shallow nets, the summary of advantages of\ndeep ReLU nets in feature extraction are listed in the following\nTable I.\nTABLE I: Deep nets in feature extraction (within accuracy ε,\nr-smooth function and dm-dimensional manifold)\nRef.\nFeatures\nParameters\nDepth\n[7]\nLocalized approximation\n2d + 1\n2\n[31]\nk-spatially sparse\nk(2d + 1)\n2\n[45]\nSmooth+Manifold\nε−dm/r\n4\n[39]\nPiecewise smooth\nε−d/r\nFinite\n[42]\nℓ1 radial+smooth\nε−1/r\nlog(ε−1)\n[44]\nk-sparse (frequency)\nk log(ε−1)\nlog(ε−1)\nTo extract the “square-feature”, the following lemma has\nbeen shown in [50, Proposition 2] to verify that deep ReLU\nnets can overcome the bottleneck of shallow ReLU nets.\nLemma 2. The function f(t) = t2 on the segment [0, 1] can\nbe approximated with any error ε > 0 by a ReLU network\nhaving the depth and free parameters of order O(log(1/ε)).\nDue to the non-smoothness of ReLU, it is\ndifﬁcult for a\nshallow ReLU net with ﬁxed width to extract smooth features\nwithin an arbitrary accuracy ε. However, by deepening the\nnetwork, Lemma 2 shows that deep ReLU nets succeed in\nﬁnishing such a task with only O(log(1/ε)) free parameters.\nWith Lemma 2 and the relation t1·t2 = [(t1+t2)2−t2\n1−t2\n2]/2,\ndeep ReLU nets can be used as a “product-gate” [50, Propo-\nsition 3] to extract the “product” relation between variables.\nThen, deep ReLU nets with O(log(1/ε)) hidden layers and\nfree parameters can approximate arbitrary polynomials deﬁned\non Id [44]. Therefore, even for some simple data features, deep\nReLU nets theoretically beat shallow ReLU nets, showing\nthe necessity of the depth in feature extraction. However, as\nshown in [16, Sec. 8.2] and [1], both the convergence issue\nof the stochastic gradient descent algorithm and the gradient\nvanishing phenomenon make it difﬁcult to practically derive\na deep ReLU net estimator, which hinders the usefulness and\nefﬁciency of Lemma 2. Figure 3 presents the difﬁculty for\ndeep ReLU nets in extracting a 2-dimensional “square-feature”\ndeﬁned as f(t) = t2\n1 + t2\n2. For each depth, the network of\nthe best performance is chosen and shown in the ﬁgure as\na representation for the depth, by searching various (tens of)\ncombinations of different widths and step sizes. The statistics\nof each depth are made from 100 trials. The relation between\naccuracy and depth is recorded in Figure 3 (a) and that between\nthe frequencies of valid models and depth is recorded in Figure\n3 (b). As shown, the network performs less robust when it gets\ndeeper.\n5\n1\n3\n5\n7\n9\n11\n13\n10\n−6\n10\n−5\n10\n−4\n10\n−3\nThe number of fully connected layers\nMSE (log)\n(a) Accuracy and depth\n1\n3\n5\n7\n9\n11\n13\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\nThe number of fully connected layers\nRate of valid models (%)\n(b) Valid model and depth\nFig. 3: The role of depth for approximating t2 using SGD\nIII. DEPTH SELECTION FOR EXTRACTING A SINGLE\nFEATURE\nIn this section, we introduce several data features and study\nthe role of depth in extracting these data features to break\nthrough the feature-depth correspondences.\nSince there are\nnumerous symbols involved in different data features, we\nprovide a table of notations as follows.\nTABLE II: Notations\nL: number of layers\nn: number of parameters\ndj: width in the j-th layer\nd: input dimension\nr: smoothness index\nµ sparseness index\nR: bound of parameters\nB: bound of polynomials\nd∗: group structure dimension\n: group structure degree\nA.\nData features\nIn a seminal review paper [3], Bengio et al. presented a\nfruitful review on intuitive and experimental explanations for\nthe success of deep learning in feature extraction. From a\nnumerical viewpoint, deep nets can extract numerous data\nfeatures including those for smoothness, hierarchical organi-\nzation, shared factors, manifold structures and sparsity, part\nof which were theoretically veriﬁed in the recent paper [18].\nIn particular, [18] rigorously proved that with the similar\ncapacity costs measured by the covering number, deep nets\nbeat shallow nets in extracting data features listed in Table I\nwhile performing not essentially better than shallow nets in\nextracting the smoothness feature.\nLet f ∗: Id →R be a function to model the potential\nrelation between input and output, i.e., y ≈f ∗(x) with x ∈Id\nand y ∈R the input variable and output variable respectively.\nBoth structures of x and properties of f ∗are regarded as data\nfeatures. In the following, we introduce the smoothness feature\nof f ∗.\nDeﬁnition 1. Let c0 > 0 and r = s+v with s ∈N0 := {0}∪N\nand 0 < v ≤1. We say a function f : Id →R is (r, c0)-\nsmooth if f is s-times differentiable and for every αj ∈N0,\nj = 1, . . . , d with α1 +· · ·+αd = s, its s-th partial derivative\nsatisﬁes the Lipschitz condition\n\f\f\f\f\n∂sf\n∂xα1\n1 . . . ∂xαd\nd\n(x) −\n∂sf\n∂xα1\n1 . . . ∂xαd\nd\n(x′)\n\f\f\f\f ≤c0∥x −x′∥v\n2,\n(6)\nwhere x, x′ ∈Id and ∥x∥2 denotes the Euclidean norm of\nx. Denote by Lip(r,c0) the set of all (r, c0)-smooth functions\ndeﬁned on Id.\nThe smoothness feature of f ∗illustrates that x ≈x′ implies\nf ∗(x) ≈f ∗(x′). It is a standard feature to describe f ∗and has\nbeen used in a vast literature [7], [26], [17], [39], [50], [51],\n[32]. However, it remains open whether deep ReLU nets can\nachieve the optimal performance of algebraic polynomials for\nrealizing the smoothness feature, though encouraging develop-\nments have been made in [50], [39]. Furthermore, as pointed\nout in [3], the smoothness feature of f ∗is insufﬁcient to get\naround the curse of dimensionality, which requires additional\nstructure features of x. To this end, we introduce the following\ngroup structure feature for the input.\nDeﬁnition 2. Let , d∗∈N and D1, . . . , Dd∗∈N satisfy\nd = D1 + · · · + Dd∗. We say x possesses a (D1, . . . , Dd∗)-\ngroup structure of order with respect to f ∗, if there exists\nsome polynomials Pk,, k = 1, . . . , d∗deﬁned on IDk and of\ndegree at most and a function g : Rd∗→R such that\nf ∗(x)\n=\ng\n\u0002\nP1,(x(1), . . . , x(D1)), . . . ,\nPd∗,(x(d−Dd∗+1), . . . , x(d))\n\u0003\n.\n(7)\nThe group structure depicts the relation between different\ninput variables. The case d∗= d and\nPk,(t) = t for\nk = 1, . . . , d denotes that all variables in x are independent.\nThe rotation-invariance assumption [9] is included in the case\nd∗= 1 implies that variables in x possess strong dependence.\nThe group structure assumption is more general than the man-\nifold assumption [8], rotation-invariance assumption [9] and\nsparseness assumption [44] via imposing different restrictions\non Pk,.\nTo show the outperformance of deep nets, we impose\nboth the smoothness assumption on f ∗and group structure\nassumption on the input. Such a smooth-structure assump-\ntion abounds in applications. For d∗= 1 and P1,(x) =\n(x(1))2 +· · ·+(x(d))2, the smooth-structure assumption refers\nto a radial function that plays an important role in designing\nearthquake early warning systems [43]. For d∗< d and\nP1,(x(1), . . . , x(D1)) = (x(1))2 + · · · + (x(D1))2, the feature\nassumption is related to a partially radial function that is\nimportant in predicting the magnitude of earthquake [49]. For\nd∗= d and\ng(P1,(x(1)), . . . , Pd,(x(d)) = P1,(x(1)) + · · · + Pd,(x(d)),\nthe smooth-structure assumption corresponds to the well\nknown additive model [26] with polynomial kernels in statis-\ntics. If there exists some Pk,(·) = 0, the assumption then\nimplies sparseness which is standard in computer vision [34].\nB. Depth selection for extracting the group structure\nIt was shown in [36] that for some ﬁxed activation function,\ni.e., analytic and non-polynomials, shallow nets with\n\u0010\nβ+d\nd\n\u0011\nneurons can approximate any polynomial deﬁned on Id of\ndegree β ∈N within an arbitrary accuracy. However, if\nthe polynomial is sparse, then shallow nets fail to catch\nthe sparseness information [28] in the sense that the same\nnumber of neurons is required to approximate sparse and non-\nsparse polynomials. However, [28], [44] found that deep nets\nessentially improve the performance of shallow nets by using\n6\nthe “product-gate” property of deep nets [50]. In particular,\nfor deep ReLU nets, the following lemma was proved in [44,\nProposition 3.3].\nLemma 3. For any 0 < ε < 1 and ℓ∈N, there exists a\nReLU net ˜Q with ℓinput units, O[(1 + log ℓ) log(ℓ/ε)] depth\nand O[(1 + log ℓ) log(ℓ/ε)] free parameters such that for any\nu1, . . . , uℓsatisfying |uk| ≤1, k = 1, . . . , ℓ, there holds\n\f\f\f\f\f\n˜\nY\n(u1, . . . , uℓ) −\nℓ\nY\nk=1\nuk\n\f\f\f\f\f ≤ε.\nNoting that each monomial deﬁned on Id of degree at most\nβ can be rewritten as β products of elements in [0, 1], it\nrequires a deep net with O[(1 + log β) log(β/ε)] depth and\nfree parameters to extract the monomial feature according to\nLemma 3. Based on the “product-gate-units” (PGU), we can\nconstruct a deep net such that for any µ-sparse polynomials,\nthere are only µ + O[(1 + log β) log(β/ε)] free parameters\ninvolved to extract this structure feature, which is much\nsmaller than\n\u0010\nβ+d\nd\n\u0011\nprovided µ is small and ε is not extremely\nsmall.\nAlthough the above interesting result illustrates the power\nof depth in extracting structure features, the depth of the\nconstructed deep ReLU net depends on the approximation\naccuracy, making it be practically difﬁcult to get a deep\nnet estimator, just as Figure 3 purports to show. In this\npaper, we pursue a trade-off between depth and number of\nfree parameters in extracting structure features by using an\napproach developed in a recent paper [39]. The following\n“product gate” for deep ReLU nets is our main tool, whose\nproof can be found in Appendix A.\nLemma 4. Let θ > 0 and ˜L ∈N with ˜L > (2θ)−1. For any\nℓ∈{2, 3, . . . , } and ε ∈(0, 1), there exists a deep ReLU net\n˜×ℓwith 2ℓ˜L + 8ℓlayers and at most cℓθε−θ free parameters\nbounded by ℓγε−γ such that\n|u1u2 · · · uℓ−˜×ℓ(u1, . . . , uℓ)| ≤ε,\n∀u1, . . . , uℓ∈[−1, 1],\nwhere c and γ are constants depending only on θ and ˜L.\nComparing Lemma 4 with Lemma 3, as a “product-gate”,\nwe reduce the depth of deep ReLU nets on the price of adding\nthe number of free parameters. The positive number θ per-\nforms as a balance exponent in the sense that small θ implies\nlarge depth but few free parameters, while large θ means small\ndepth but a great number of free parameters. For a ﬁxed and\nε-independent exponent θ, the depth of ReLU nets in Lemma\n4 is independent of the accuracy ε, while the number of free\nparameters increases from O[(1 + log ℓ) log(ℓ/ε)] to ℓℓθε−ℓθ.\nTherefore, Lemma 4 exhibits a trade-off between depth and\nparameters and removes the feature-depth correspondence.\nDenote by Pd\nβ the set of algebraic polynomials deﬁned\non Id with degree at most β. For B ≥1, deﬁne further\nPd\nβ,B :=\nnP\n|α|≤β cαxα : |cα| ≤B\no\nthe set of polynomials\nin Pd\nβ whose coefﬁcients are uniformly bounded by B, where\nα = (α1, . . . , αd) ∈Nd\n0, |α| = α1 + · · · + αd and xα =\n(x(1))α1 · · · (x(d))αd. Deﬁne Pd\nβ,B,µ the set of all µ-sparse\npolynomials in Pd\nβ,B. So P ∈Pd\nβ,B,µ has at most µ nonzero\ncoefﬁcients. The following theorem shows the performance of\ndeep ReLU nets in extracting the sparse polynomial feature,\nwhose proof will be given in Appendix A.\nTheorem 1. Let β, µ ∈N, B, θ > 0 and ˜L ∈N with ˜L >\n(2θ)−1. For any 0 < ε < 1, there is a deep ReLU net structure\nwith 2β ˜L+8β+1 layers and at most µ+c(µβB)θε−θ nonzero\nparameters bounded by max{B, (µβB)γε−γ}, such that for\neach P ∈Pd\nβ,B,µ there exists a hP with the aforementioned\nstructure satisfying\n|P(x) −hP (x)| ≤ε,\n∀x ∈Id,\nwhere c and γ are the constants in Lemma 4.\nA similar result has been established in [44] for deep ReLU\nnets with depth O(log β log(µBβ/ε)) and number of free\nparameters µ + O(log β log(µBβ/ε)). Our result is different\nfrom [44] by introducing an exponent θ to balance the depth\nand number of free parameters. For a ﬁxed θ, the depth of\ndeep ReLU nets studied in Theorem 1 is independent of ε.\nThus, Theorem 1 shows a novel relation between the depth\nand feature extraction for sparse polynomial features as well as\nthe group structure features, by means of breaking through the\nexclusive feature-depth correspondence in [44]. Furthermore,\nif µ is not extremely small, we can select a θ in Theorem 1\nsuch that the capacity of deep nets in Theorem 1 is smaller\nthan that in [44] according to Lemma 1. That is, Theorem\n1 provides a theoretical guidance on using smaller capacity\ncosts than [44] to get a same accuracy in extracting the sparse\nfeature.\nC. Deep nets for extracting the smoothness feature\nIn [50], Yarotsky succeeded in establishing a tight error\nestimate of approximating smooth functions by deep ReLU\nnets by utilizing the “product-gate” property in Lemma 3. [50,\nTheorem 1] showed that for any f ∈Lip(r,c0) with r ∈N,\nthere is a deep ReLU net h⋄\nf with ﬁxed structure, n free\nparameters and O(log n) layers such that\n∥f −h⋄\nf∥L∞(Id) ≤c′n−r/d log n,\n(8)\nwhere c′ is a constant depending only on c0, d, r and\np\n∈\n[1, ∞). Comparing with standard results for linear\napproximants such as algebraic polynomials [40], there is an\nadditional logarithmic term in (8). This is due to the accuracy-\ndependent depth in Lemma 3.\nThis phenomenon was ﬁrstly noticed in [39]. After deriving\nthe “product-gate” property for deep ReLU nets with accuracy-\nindependent depth, [39, Theorem 3.1] proved that there exists\na deep ReLU net h∗\nf with ﬁxed structure, n free parameters\nlayered on (2 + ⌈log r⌉)(11 + r/d) hidden layers such that\n∥f −h∗\nf∥Lp(Id) ≤c∗n−r/d,\n(9)\nwhere c∗is a constant depending only on c0, d, r and\np ∈[1, ∞). It is obvious that (9) improves (8) by removing\nthe logarithmic term. However, the analysis in [39] relies on\nthe localized approximation [7] of deep nets and thus, their\nresult holds only under the Lp(Id) norm with 1 ≤p < ∞.\nNoting that for f ∈L∞(Id), ∥f∥Lp(Id) ≤∥f∥L∞(Id), (9)\n7\ndoes not match the optimal rate of uniform approximation by\nlinear approximants. In the following theorem, we combine\nthe approaches in [39] and [50] to get a sharp error estimate\nof approximating smooth functions by deep ReLU nets under\nthe L∞(Id) metric.\nTheorem 2. Let r = s + v with s ∈N0 and 0 < v ≤1,\nc0, θ > 0 and ˜L ∈N with ˜L > (2θ)−1. For any ε ∈(0, 1),\nthere exists a deep ReLU net structure with\nL(d, r, ˜L) := 2(d + s)˜L + 8(d + s) + 3\n(10)\nlayers and at most c(d+s)θε−(r+d)θ/r +(8d+5)\n\u0000s+d\ns\n\u0001\nε−d/r\nfree\nparameters\nbounded\nby\nmax{ ˜B, 3ε−1/r, (d\n+\ns)γε−(r+d)γ/r}, such that for any f\n∈\nLip(r,c0) there\nis a hf with the aforementioned structure satisfying\n∥f −hf∥L∞(Id) ≤c1ε,\n(11)\nwhere c1 is a constant depending only on c0, d and r and\n˜B :=\nmax\nk1+···+kd≤s max\nx∈Id\n\f\f\f\f\n1\nk1! . . . kd!\n∂k1+...kdf(x)\n∂k1x(1) . . . ∂kdx(d)\n\f\f\f\f .\nThe proof of Theorem 2 will be presented in Appendix B.\nSetting ε = n−r/d, we get from Theorem 2 that there exists a\ndeep net hf with at most O(nmax{1,(r+d)θ/d}) free parameters\nand L(d, r, ˜L) layers for ˜L > (2θ)−1 such that\n∥f −hf∥L∞(Id) ≤c1n−r/d.\n(12)\nThe depth plays a crucial role in extracting the smooth\nfeatures in the sense that to derive a similar approximation\naccuracy as linear approximants, θ should be not larger than\nd/(r + d), implying ˜L > (r + d)/(2d). However, when the\ndepth L(d, r, ˜L) with ˜L reaching this critical value, deep nets\nwith various depths are capable of extracting smooth features.\nThis removes the feature-depth correspondence in extracting\nthe smooth feature\nby making use of the structure of deep\nnets, since our constructed deep nets in the proof are sparse\nand share weights, which is different from the deep nets in the\nprominent work [50]. Recalling Lemma 1, for appropriately\nselected θ, the capacity of deep nets in our construction is\nsmaller than that of [50] by removing the logarithmic term\ncaused by the accuracy-dependent layers. Inequalities like (11)\nhave been established for shallow nets with some sigmoid-\ntype activation functions [36], [29]. However, different from\nTheorem 2, the magnitudes of weights in [36] are so large\nthat the capacity restriction (5) does not hold and the result\nin [29] suffers from the well known saturation phenomenon\nin the sense that the approximation rate cannot be improved\nany further when the smoothness of the target function goes\nbeyond a speciﬁc level. It can be found in Theorem 2 that\ndeepening the networks succeeds in overcoming these prob-\nlems. Although [18, Theorem 2] declares that to extract the\nsmoothness feature, deep nets perform not essentially better\nthan shallow nets or linear approximant, our result in Theorem\n2 yields that deep ReLU nets are at least not worse than\nshallow nets.\nIV. DEPTH SELECTION IN EXACTING COMPOSITE\nFEATURES\nThe previous section demonstrated the role of depth in\nextracting a single data feature. However, as shown in [3],\nit is much more important to simultaneously extract multiple\nfeatures to feed the target-driven learning. Extracting compos-\nite features by deep nets, which is the purpose of this section,\nbrings novel challenges in designing deep nets, including the\njunction of deep nets with different utilities, the balance of\naccuracy and depth, and the depth-parameter trade-off.\nTo build up a network to exact composite features, an intu-\nitive approach is to stack deep nets by the a-priori information\nor human experiences in a tandem manner, just as Figure\n1 implies. The problem is, however, such a brutal stacking\nis practically inefﬁcient, for both the unavailability of the a-\npriori information and lacking of the prescribed accuracy for\nextracting a speciﬁc feature. More importantly, the stacking\nscheme requires much more free parameters and depths of\ndeep nets to extract composite features, adding additional\ncapacity costs according to Lemma 1\nIn this section, we provide some theoretical guidance on\nselecting depth of deep nets to extract composite features by\ntaking the depth-parameter trade-off into account. Without loss\nof generality, we are interested in extracting features exhibited\nin the following assumption.\nAssumption 1. Let r = s + v with s ∈N0 and v ∈(0, 1],\nD1, . . . , Dd∗, d∗∈N with d = D1 + · · · + Dd∗, and , µ ∈N.\nAssume that there is a function g deﬁned on Id∗satisfying\ng ∈Lip(r,c0) such that (7) holds with Pk,∈PDk\n,1/2,µ for\nk = 1, . . . , d∗.\nThere are totally three types of features in Assumption\n1, the smoothness feature of g as well as f ∗, the group\nstructure feature of x, and the sparsity feature of the structure\npolynomials Pk,, k = 1, . . . , d∗. An intuitive observation is\nthat the depth and number of free parameters of deep nets\nto simultaneously extract these three features should be larger\nthan those to extract each single feature. However, as shown in\nthe following theorem, it is not necessarily the case, provided\nthe deep nets for different utilities are appropriately combined.\nTheorem 3. Let r = s + v with s ∈N0 and v ∈(0, 1],\nd∗, , µ ∈N, c0, θ > 0 and ˜L ∈N with ˜L > (2θ)−1. For any\n0 < ε < 1/2, there exists a deep ReLU net structure with at\nmost\nL∗(d∗, r, ˜L, ) =\nsmooth+group\nz\n}|\n{\nL(d∗, r, ˜L)\n+\ngroup\nz\n}|\n{\n2˜L + 8+ 1\n(13)\nlayers and at most\nW(d∗, ε, µ, , θ) :=\nsmooth+group\nz\n}|\n{\n(8d∗+ 5)\n\u0010\ns+d∗\ns\n\u0011\nε−d∗/r\n(14)\n+\nsparse+group\nz}|{\nµd∗\n+\ndepth-parameter trade-off\nz\n}|\n{\nc(d∗+ s)θε−(r+d∗)θ/r + c(µ)θε−θ/τr\nfree parameters bounded by\nmax{ ˜Bg, 3ε−1/r, (d∗+ s)γε−(r+d∗)γ/r, (µ)γε−γ/τr} (15)\n8\nsuch that, for any f ∗satisfying Assumption 1, there is an hf ∗\npossessing the aforementioned structure satisfying\n∥f ∗−hf ∗∥L∞(Id) ≤c2ε\nwhere τr =\n\u001a 1,\nr ≥1,\nv,\nr < 1,\nand c2, ˜Bg are constants depend-\ning only on c0, r, d∗and g.\nThe proof of Theorem 3 will be given in Appendix\nC. Assumption 1 implies f ∗∈Lip(r,c0), which requires\nL(d, r, ˜L) layers to extract the smoothness feature according\nto Theorem 2. However, with the help of the group structure\nfeature, (13) exhibits a reduction of layers from L(d∗, r, ˜L)\nto L(d, r, ˜L). To extract the group structure feature itself,\nadditional 2˜L + 8+ 1 layers are required. This shows that\nthe classical tandem stacking is not necessary. In particular,\nfor some speciﬁc group structure features, taking d∗= 1 and\n= 1 for example, it is easy to select some θ > 0 such that\nL∗(d∗, r, ˜L, ) ≤L(d, r, ˜L), implying a waste of source of the\ntandem stacking.\nThe number of free parameters, as exhibited in (14), reﬂects\nthe price to pay for extracting three composite features.\nTo yield an accuracy of order ε, the group structure and\nsmoothness feature require at least ε−d∗/r free parameters.\nIt should be mentioned that this number cannot be reduced\nfurther according to [18, Theorem 2] by noting in Assumption\n1 that f ∗corresponds a smooth function deﬁned on Id∗. The\nsecond term in (14) reﬂects the difﬁculty in extracting the\ngroup structure feature. Without the sparseness assumption,\nit requires at least O\n\u0010Pd∗\nk=1 Dk\n\u0011\nfree parameters. If there is\nsome k such that Dk > ε−d∗/r, extracting the group structure\nfeature becomes the main difﬁculty in the learning process.\nThis imposes a strict restriction on to maintain the optimality.\nThe sparsity assumption reduces this risk, allowing to be very\nlarge. The rest two term in (14) illustrates a depth-parameter\ntrade-off in extracting composite features. In particular, to\nguarantee the optimal capability of feature extraction, θ must\nbe smaller than the critical value θ0 := min\nn\nd∗\nd∗+r, d∗τr\nr\no\n.\nThis implies a smallest depth in (13) by noting ˜L > 1/(2θ).\nIn a word, less parameters requires smaller θ, which results in\nlarger ˜L and consequently larger L∗(d∗, r, ˜L, ), while more\nparameters require larger θ, and consequently smaller ˜L and\ndepth.\nAs Theorem 3 shows, the depth of network is not unique to\nextract composite features, provided it is larger than a certain\nlevel. Furthermore, our results imply two important advantages\nof deep nets in feature extraction. One is that, different from\nthe classical tandem tackling, deep nets succeed in extracting\ncomposite features by embodying their interactions, and thus\nreduce the capacity costs. Such a reduction plays an important\nrole in generalization, which will be analyzed in the next sec-\ntion. The other is the versatility of deep nets in extracting both\nsingle features and composite features in the sense that each\nfeature corresponds to numerous depths, and vice versa. To end\nthis section, we present two corollaries for deep nets to extract\ncomposite features. The ﬁrst one is the smoothness and radial\nfeatures. Let d∗= 1 and P1,(x) =\n1\n√\nd[(x(1))2+· · ·+(x(d))2],\nthen f ∗is a radial function [9]. Setting θ = τr/(2 + 2r) and\n˜L = 2(r + 1)/τr, we have from Theorem 3 with = 2 and\nµ = 1 the following corollary directly.\nCorollary 1. There exists a deep ReLU net structure with\n4(d+s+2)(r +1)/τr +8(d+s)+20 layers and at most c3n\nnonzero free parameters bounded by c4nmax{1,(r+1)γ,γr/τr}\nsuch that for any radial function f ∗∈Lip(r,c0) there is a\ndeep net hf ∗with the aforementioned structure satisfying\n∥f ∗−hf ∗∥L∞(Id) ≤c5n−r,\nc3, c4, c5 are constants depending only on c0, r, d and f ∗.\nThe derived approximation rate is almost optimal ac-\ncording to [9] in the sense that the best approxima-\ntion error for all deep nets satisfying the capacity restric-\ntion (4) with n parameters is of order (n/ log n)−r. Our\nsecond corollary considers using deep nets to simultane-\nously extract the partially radial and smooth features. Let\nd′ ≤d and f ∗(x) = f(\nradial\nz\n}|\n{\nx(1), . . . , x(d′), x(d′+1), . . . , x(d)) =\ng(td′, x(d′+1), . . . , x(d)) with td′ = (d′)−1/2((x(d1))2) + · · · +\n(x(d′))2) ∈[0, 1]. Let θ =\n(d−d′+1)τr\n2(d−d′+1+r) and L = 2(d−d′+1+r)\n(d−d′+1)τr .\nThe following corollary is a direct consequence of Theorem\n3 with d∗= d −d′ + 1, = 2 and µ = 1.\nCorollary 2. There exists a deep ReLU net structure with\n4(d −d′ + 1 + r)(d −d′ + 3 + s)\n(d −d′ + 1)τr\n+ 8(d −d′ + 1 + s) + 20\nlayers\nand\nat\nmost\nc6n\nfree\nparameters\nbounded\nby\nc7nmax{1,(r+d−d′+1)γ,γr/τr}/(d−d′+1) such that for any par-\ntially radial function f ∗∈Lip(r,c0) there is a deep net hf ∗\nwith the aforementioned structure satisfying\n∥f ∗−hf ∗∥L∞(Id) ≤c8n−r/(d−d′+1),\nwhere c6, c7, c8 are constants depending only on c0, r, d and\nf ∗.\nV.\nGENERALIZATION CAPABILITY OF DEEP NETS\nThis section aims at the generalization capability of deep\nReLU nets. Our analysis is carried out in the standard learning\ntheory framework [10] for regression. In learning theory\n[10], a sample Dm = {(xi, yi)}m\ni=1 with xi ∈X = Id\nand yi ∈Y ⊆[−M, M] for some M > 0 is assumed\nto be drawn independently according to an unknown Borel\nprobability ρ on Z = X × Y. The generalization capability\nof an estimator f is measured by the generalization error,\nE(f) :=\nR\nZ(f(x) −y)2dρ, which quantiﬁes the relation\nbetween the sample size m and prediction accuracy. The\nprimary objective is to ﬁnd an estimator based on Dm of\nthe regression function fρ(x) =\nR\nY ydρ(y|x) that minimizes\nthe generalization error, where ρ(y|x) denotes the conditional\ndistribution at x induced by ρ.\nLet Hn,L,R be deﬁned by (3). We consider generalization\nerror estimates for the following empirical risk minimization\n(ERM) on deep nets:\nfD,n,L := arg\nmin\nf∈Hn,L,R\n1\nm\nm\nX\ni=1\n[f(xi) −yi]2.\n(16)\n9\ncapcity\naccuracy\nvariance\nbias\ngeneralization\n(a) Bias-variance trade-off\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\n0\n2\n4\n6\n8\n10\n12\n14\nx 10\n5\nA\n(b) Depth-width trade-off\nFig. 4: Bias-variance trade-off for ERM on deep nets\nSince |yi| ≤M, it is natural to project the ﬁnal output\nfD,n,L to the interval [−M, M] by the truncation operator\nπMfD,n,L(x) := sign(fD,n,L(x)) min{|fD,n,L(x)|, M}.\nFrom Theorems 1-3, the accuracy of feature extraction\ndecreases as the capacity of deep nets increases, resulting in\nsmall bias for the ERM. However, too large capacity makes\nERM be sensitive to noise and leads to large variance. This\nis the well known bias-variance dilemma [10, Chap.1]. The\noptimal generalization performance for ERM is obtained by\nbalancing the bias and variance, just as Figure 4 (a) purports to\nshow. For ERM on deep nets, the problem is that the capacity\ndepends on both depth and number of free parameters. As\nshown in Figure 4 (b), all (L, n) pairs in the curve “A”\nshare the same covering number bounds in Lemma 1 with\nε = 0.01. In summary, there are two dilemmas to get a good\ngeneralization for ERM on deep nets: bias-variance dilemma\nin selecting the capacity and depth-parameter dilemma in\ncontrolling the bias. The purpose of our study is not only to\npursue the optimal generalization error for ERM on deep nets,\nbut also to derive feasible candidates of (L, n) pairs to realize\nthe optimality. The main result is the following theorem.\nTheorem 4. Let 0 < δ < 1, r = s + v with s ∈N0\nand 0 < v ≤1, µ, , d, d∗∈N and fD,n,L∗be deﬁned by\n(16) with L∗= L∗(d∗, r, ˜L, ), R be the value given in (15),\nn =\nh\nC1m\nd∗\n2r+d∗i\nand elements in HL,R,n possess the same\nstructure as that in Theorem 3. If fρ satisﬁes Assumption 1,\n˜L > (2θ)−1 with\n0 < θ ≤θ0 := min\n\u001a\nd∗\nd∗+ r, d∗τr\nr\n\u001b\n,\n(17)\nand\nµ≤n\nτrd∗+θ\nd∗τrθ ,\nand\nµ ≤n,\n(18)\nthen\nE(πMfD,n,L∗) −E(fρ) ≤C2 ˜L2m−\n2r\n2r+d∗log m log 3\nδ\n(19)\nholds with conﬁdence of at least 1 −δ, where C1, C2 are\nconstants independent of δ, m, ˜L or n.\nThe proof of Theorem 4 will be given in Appendix D.\nCondition (18) presents a restriction on the group structure\nand sparsity features in the sense that either µ or should be\nrelatively small with respect to the size of data. It should be\nnoted that the established learning rate cannot be essentially\nimproved in the sense that for some special Pk,, the learning\nrate is optimal [9, Theorem 3]. We emphasize that the learning\nrate in the above theorem is much better than the optimal\nlearning rate m−\n2r\n2r+d for learning (r, c0)-smooth functions\non Id [19], [31]. As pointed out in [9], even restricting to\nlearning a radial function, to achieve a learning rate similar\nto (19) with d∗= 1, it requires at least [m\nd−1\n2r+1 ] neurons\nto guarantee the generalization error of order m−\n2r\n2r+1 . This\nshows the outperformance of deep nets over shallow nets.\nTheorem 4 also shows the almost optimal learning rates\nfor deep nets in learning data with the group structure and\nsmoothness features. The relation n =\nh\nC1m\nd∗\n2r+d∗i\nis ob-\ntained by the bias-variance trade-off principle. It should be\nmentioned that there is an additional L2 in right-hand sides\nof (19), implying advantages of small depth. However, (17)\nimplies that there is a critical depth, larger than which deep\nnets with suitable structures can achieve the almost optimal\ngeneralization error bounds. It also exhibits a depth-parameter\ntrade-off in the sense that small θ results in small number of\nparameters. A different θ thus provides different (L, n) pairs\nto realize the optimal generalization performance of ERM on\ndeep ReLU nets.\nVI. EXPERIMENTAL RESULTS\nIn this section, we present both toy simulations and\nreal data experiments to show the roles of depth for\nReLU nets in feature selection and prediction. All the\nnumerical\nexperiments\nare\ncarried\nout\nin\nthe\nPython-\n3.5.4 environment running on a workstation with a Pas-\ncal\nTitan\nX\n12-GB\nGPU\nand\n24-GB\nmemory.\nOur\nimplementation\nis\nderived\nfrom\nthe\npublicly\navailable\nTensorﬂow-1.4.0 framework by using AdamOptimizer.\nOur\ncodes are available at http://vision.sia.cn/our%20team/Hanzhi-\nhomepage/vision-ZhiHan%28English%29.html.\nA. Experimental setting\nThe settings of simulations are described as follows.\nImplementation and Evaluation: The are ﬁve purposes in\nour experimental study. The ﬁrst one is to verify the adaptivity\nof depths to the feature. The second one is to declare the\nadaptivity of features to the depth. The third one aims at\ndemonstrating the necessity of depth in feature extraction. The\nfourth one focuses on the necessity of depths in generalization.\nIn our last experiment, we show the power of deep nets\nin some real applications. In each simulation, we randomly\ngenerate m sample points {xi}m\ni=1 on X ∈Rd according to the\nuniform distribution. Each xi corresponds to an output yi with\neither yi = f(xi) (Sections 6.2, 6.3, 6.4) or yi = f(xi) + εi\n(Section 6.5) with εi some Gaussian noise. We repeat 10 times\nand record the average values of the following ﬁve quantities:\n• Mean squared error (MSE): given an estimator fD, MSE,\ndeﬁned by\n1\nm\nPm\ni=1(fD(xi) −yi)2, measures the average\nsquared difference between the estimated values and what\nis estimated. It is a standard measurement to quantify the\nprediction performance of an estimator.\n•\nMean\nabsolute\nerror\n(MAE):\nMAE,\ndeﬁned\nby\n1\nm\nPm\ni=1 |fD(xi) −yi|, quantiﬁes the ﬁtting performance of\n10\nW∈[10,20,30,40,50,60,70,80,90,\n100,200,300,400,500,600]\n5-layer\n)\n(\n2\n3 W\nN\n)\n(\n1\n1 W\nN\nW∈[500,1000,2000,3000,4000,\n5000,6000,7000,8000,9000,\n10000,12000,13000,14000,15000]\n1-layer\n)\n(\n1\n3 W\nN\n3-layer\nW∈[10,20,30,40,50,60,\n100,200,300,400,500,600,\n1000,2000,3000]\nW\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n)\n(\n3\n5 W\nN\n1 (output)\n10 (input)\n60\n40\n50\n30\nW\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n)\n(\n1\n5 W\nN\n1 (output)\n10 (input)\n60\n40\n50\n30\nW\n)\n(\n3\n3 W\nN\nW\n...\n...\n...\n...\n10 (input)\n1 (output)\n...\n...\n...\n...\n...\n...\n...\n...\n1 (output)\n10 (input)\n60\n60\nW\n1 (output)\nW\n...\n...\n...\n...\n...\n...\n...\n...\n10 (input)\n60\n60\n...\n...\n...\n...\n...\n...\n...\n...\n1 (output)\n10 (input)\n60\n60\nFig. 5: Network architectures of various depths and widths.\nfD. It is another popular measurement, which is less sensitive\nto outliers than MSE, to quantify the prediction performance.\n• Median absolute error (MdAE): MdAE, deﬁned by\nm0.5(|fD(xi) −m0.5(yi)|), is a robust measure of the variabil-\nity of an estimator, where m0.5 means a median. Thus, MdAE,\ntogether with MAE, shows the robustness of the estimator.\n• R squared score (R2S): R2S, deﬁned by R2S(y, f) =\n1 −\nPn\ni=1(yi−fD(xi))2\nPn\ni=1(yi−¯y)2\nwith ¯y =\n1\nm\nPm\ni=1 yi, is a statistical\nmeasurement that represents the proportion of the variance of\nan estimate by that of real outputs in a regression model. It\nmeasures the ﬁtness of the model.\n• Explained variance score (EVS): EVS, deﬁned by 1 −\nPm\ni=1(yi−fD(xi))2\nPm\ni=1 y2\ni\n, measures the proportion to which a math-\nematical model accounts for the variation (dispersion) of a\ngiven data set.\nAll these measurements quantify the prediction performance\nof an estimator in terms of the prediction accuracy, sensitivity\nto outliers, robustness and ﬁtness.\nStructures of deep nets: Generally speaking, there are\nfour components in describing the structure of deep nets:\ndepth, width in each layer, sparsity in conjunction, and sharing\nweights. In our experiments, network width is equivalent to the\nnumber of neurons. If the sparsity in conjunction and sharing\nweights are considered, there are too many structures even\nfor a three-layer feed-forward network. Thus, we are only\nconcerned with fully connected deep nets with different width\nand depth. In fact, we train over 200 networks of different\ndepths and widths in our simulations. We use N ℓ\nL(W) to\nrepresent a network of L layers and width W in the ℓ-th layer\n(marked as red as in Figure 5). For example, N 2\n3 (100) and\nN 2\n3 (200) are both 3-layer networks. The widths in layer-1 and\nlayer-3 are ﬁxed. The only difference is the widths in layer-2\nare 100 and 200, respectively. In Figure 5, we present some\nexamples for the structures adopted in our simulations. The\ndetails of structures will be explained in each simulation, if it\nis needed.\nB. Adaptivity of the Depth to features\nIn this subsection, we study the performance of deep nets\nin extracting the 10-dimensional “square-feature”:\nf(x) =\n10\nX\nj=1\n(x(j))2,\nwhere x = (x(1), . . . , x(10)) is i.i.d. generated according to the\nuniform distribution on [−100, 100]10. The sizes of training\ndataset and testing dataset are 3000 and 200, respectively. Our\npurpose is to show the adaptivity of structures to the square\nfeature, i.e., there are various structures to extract the square\nfeature.\n1) The necessity of depth: For comparison, we train 135\nnetworks of different depths and widths. The network archi-\ntectures are illustrated in Figure 5. In particular, we choose 3\ndifferent depths L = {1, 3, 5} and select 15 different widths,\nwhich are shown in different colors in Figure 6 and marked\nin different curves.\n11\nAs shown in Figure 6, all the curves show similar patterns,\ni.e., along with the increasement of width, the MSE decreases\nat the beginning and increases later. The difference is, for the\ndeeper networks, it generally needs smaller width to reach\nthe best performance. The average widths in the varied layers\ncorresponding to the best performance networks of 1-layer,\n3-layer and 5-layer are 4000, 60 and 52, respectively.\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\n10\n3\n10\n4\n10\n5\n10\n6\n10\n7\nVaried width (lg)\nMSE (lg)\n \n \nN1\n1(:)\nN3\n1(:)\nN3\n2(:)\nN3\n3(:)\nN5\n1(:)\nN5\n2(:)\nN5\n3(:)\nN5\n4(:)\nN5\n5(:)\nFig. 6: MSE curves of networks with various structures\nThe outperformance of 3-layer over shallow nets in Figure\n6 veriﬁes the necessity of depth and show that deep nets can\nextract the square feature better than shallow nets with much\nfewer neurons. The superiority of 3-layer over 5-layer deep\nnets demonstrates that there exists an optimal depth in extract-\ning some speciﬁc feature. Here the optimality means not only\nthe optimal accuracy, but also the solvability or convergence of\nthe adopted AdamOptimizer algorithm, since its convergence\nissue is questionable when the depth increases [16]. Thus,\nalthough Theorem 1 proved that there are numerous depth-\nparameter pairs achieving the same accuracy, the convergence\nissue suggests to set the depth as small as possible.\n2) Role of the width for 3-layer deep nets: Theorem 1\npresents a relation between the depth and the number of free\nparameters in extracting the square feature. However, it does\nnot give any guidance on the distribution of the width in each\nhidden layer. In this experiment, we ﬁx the total number of\nparameters of a 3-layer network at 8000 (slightly variation is\nallowed, and the range is [8000, 8100]). We manually change\nwidth of each layer, the numbers of parameters connecting\nInput and Layer-1 (C1), connecting Layer-1 and Layer-2 (C2),\nconnecting Layer-2 and Layer-3 (C3), and connecting Layer-3\nand Output (C4). Hence we generate a group of networks (20\nin total) with different representative parameter distributions.\nThe details of the networks are listed in Appendix E.\nWe\nrecord\nthe\ntesting\nerrors\nin\nFigure\n7,\nwhere\neach\nspot\nrepresents\none\nnetwork\nand\nthe\ncoordinates\n(p(C1), p(C2), p(C3)) are the percentage of the parameters\noccupied. As p(C1)+p(C2)+p(C3)+p(C4) = 1, the 3-layer\nnetworks of various distributions can be uniquely positioned\nby this 3-dimensional coordinate system. The size of the spot\nrepresents the MSE of the corresponding network, i.e., smaller\nspot indicates smaller MSE. To be noted, the biggest MSE that\ncan be reﬂected by the size of the spot is 10000. The networks\nwith MSE larger than 10000 are represented by yellow spots,\nwhile the red spots mean the corresponding networks do not\nconverge at all.\nFigure 7 exhibits two phenomena for deep nets in feature\nextraction. The ﬁrst one is the huge impact of the width\ndistribution. The pattern shown in this experiment is that more\nconnections between Layer-1 and Layer-2 (p(C2)) generally\nbring better results, while a large number of connections\nwith Input or Output layers (p(C1) or p(C4)) lead to bad\nperformances. This phenomenon indicates why a network is\nusually designed in a spindle shape. The other one is the\nadaptivity of the structure to the “square-feature”. It can be\nfound in Figure 7 that all green points perform similarly, which\nmeans that if the depth is suitable selected, then there is a large\nrange of the width distributions such that deep nets with such\ndistributions succeed in extracting the “square-feature”.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0\n0.2\n0.4\n0.6\n0.8\n0\n0.2\n0.4\n0.6\n0.8\n1\n2.2x10 5\n3.1x10 4\n2.0x10 5\n5.9x10 3\n8.6x10 3\n6.6x10 3\n4.4x10 3\n7.1x10 3\n4.3x10 8\n5.3x10 4\n7.7x10 8\n4.6x10 3\n7.6x10 3\n1.1x10 4\n6.1x10 3\n5.4x10 3\n4.7x10 3\n1.7x10 4\n3.6x10 4\n4.1x10 8\np(C 3)\np(C 1)\np(C 2)\nFig. 7: Networks with various width distributions.\nC. Adaptivity of features to the depth\nIn Theorems 1-3, it was proved that deep nets with ﬁxed\ndepth can extract different data features including the sparsity,\ngroup structure, and smoothness. In this simulation, we aim\nto verify this adaptivity of the feature to structures. We are\ninterested in partially radial features deﬁned by\nfk(x) =\nk\nX\nj=1\n(x(j))2 +\n10\nX\nj=k+1\nx(j),\nwhere\nx\nis\ngenerated\nby\nuniformly\nsampling\nfrom\n[−100, 100]10.\n2\n3\n4\n5\n6\n7\n8\n9\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\nThe k of fk(x)\nError rate\n \n \n1−layer\n3−layer\nFig. 8: Adaptivity of the feature to structures.\n12\nIn the experiment, besides verifying the adaptivity of deep\nnets, we also compare the performances between deep nets\nand shallow nets to show the necessity of depth in extracting\ndifferent data features. As k varies from 2 to 9, the structure\nof deep nets (3-layer net) is ﬁxed as 50 −60 −60, while the\nwidths in shallow nets is selected according to the test data\ndirectly to optimize their performance.\nFigure 8 shows the result curves (the detailed numerical\nresults can be found in Appendix F). It can be seen that a\ndeep net with ﬁxed structures performs robustly for dealing\nwith different data features, and always outperforms shallow\nnets. This demonstrates adaptivity of features to structures.\nAdditionally, we are also aware during the experiment that\ntraining shallow nets requires more iterations.\n1\n3\n5\n7\n9\n11\n10\n3\n10\n4\n10\n5\n10\n6\n10\n7\nThe number of fully connected layers\nMSE (lg)\nFig. 9: Best results from networks of different depths.\nD. The role of depth in network\nIn this subsection, we study the role of depth in extracting\nthe 10-dimensional “square-feature”. The simulation setting is\nthe same as that in Subsection 6.2. The only difference is that\nwe select more deep nets with different structures to perceive\nthe impact of depth. There are six candidates for the depth,\n1, 3, 5, 7, 9 and 11 and the width is chosen according to the\ntest data directly from much more candidates than those in\nSubsection 6.2. In particular, the number of neurons in the\nwidest layers of are 4000, 60, 30, 30, 9 and 6, respectively.\nThe MES curve of simulation results is shown in Figure 9 (the\ndetailed numerical results can be found in Appendix G).\nIt is shown that the depth plays a crucial role in improving\nthe performance of neural networks in feature extraction. We\ncan see that a deep net performs better than a shallow net, but\na larger depth does not necessarily lead to better performance.\nFor this simple case (single feature), deep nets with 3 layers\nare enough. Besides the MSE curve in Figure 9, Table IX\npresented MSE, MAE, MdAE, R2S and EVS results for the\nsame purpose. All these measurements exhibit similar patterns\nto that of MSE in Figure 9 and verify both the necessity of\ndepth in feature extraction and limitations of deep nets with\ntoo many hidden layers.\nE. Generalization capability veriﬁcation\nIn this experiment, in order to test the generalization ability\nof networks, we train networks with noisy data in a more\ncomplex relationship. The underlying relationship between the\ninput signal x = (x1, x2) and output is:\ny = sin ∥x∥2\n2/∥x∥2\n2 + ε,\nwhere ε ∼N(0, σ2) is the Gaussian noise with the variance\nof σ2. The training and test points are generated by i.i.d.\nsampling 2000 and 200 points on [−1, 1] according to the\nuniform distribution, respectively, and the noise level is set\nσ2 = 0.1.\nTABLE III: Network width candidates.\nDepth\nRange of width\nStep length\n1\n[16, 192]\n16\n2\n[4, 32]\n4\n3\n[4, 16]\n4\n4\n[4, 16]\n4\n5\n[4, 16]\n4\n6\n[4, 16]\n4\nWe compare the optimal MSE of deep nets of different\ndepths. The optimal results are obtained by tuning two impor-\ntant parameters, the descent step (learning rate during network\ntraining) and the width of each layer. During the training\nprocess, the descent step changes dynamically as follow,\nRd = R0 ∗D⌊(Sg/Sd)⌋,\nwhere ⌊·⌋is the ﬂoor function, R0 is the initial descent step\nand Rd is the decayed descent step, D is the decay rate, Sg and\nSd are global step and decay step, respectively. Global step\nrepresents the current iteration number. Decay step controls\nthe change frequency of descent step. For example, in this\nexperiment, the decay step is 1000, and D is 0.95. The\ndescent step decays to 95% every 1000 iterations. For choosing\nadequate R0, we tried values of 0.0001, 0.0005 and 0.001 on\nvarious networks of different depths. Empirically, we notice\nthat a deeper network needs a smaller descent step. Therefore,\nin the experiment, the descent step is 0.001 for 1, and 0.0005\nfor 2, 3, 4, 5 and 6-layer networks.\nThe optimal widths of networks are chosen from a group\nof candidates, which are set empirically. Table III shows\nthe details. For example, for a 3-layer network, the width\ncandidates of each layer are {4, 8, 12, 16}. As a result, there\nare 43 networks for testing. To alleviate the test burden, in\nthe experiment, we ﬁrst ﬁx widths of non-middle layers at the\nmedians of the corresponding ranges and test the middle layer\nwidth with all the candidates to elect the optimal one, then we\ntune other layers one by one by testing the candidates around\nthe optimal width of the middle layer.\nIn Figure 10, we recorded the optimal MSE and the rate\nof valid model of deep nets with different depths. Noting\nthat the function sin ∥x∥2\n2/∥x∥2\n2 is smooth and radial, which\nare difﬁcult for shallow nets to extract them simultaneously,\naccording to the theoretical results in [9]. In our simulation, we\nshow that combining the feature extraction with target-driven\nlearning in deep net is feasible. In fact, a deep net with four\nlayers can signiﬁcantly improve the performance of shallow\nnets. Table IV presents the regression result in terms of MSE,\nMAE, MdAE, R2S, and EVS, respectively and exhibits the\nsame pattern as that of MSE in Figure 10.\n13\nTABLE IV: Noisy data training by networks of various depths.\nDepth\n1-layer\n2-layer\n3-layer\n4-layer\n5-layer\n6-layer\nMAE\n0.0765\n0.0760\n0.0741\n0.0731\n0.0755\n0.0772\nMSE\n0.0923\n0.0712\n0.0705\n0.0694\n0.0718\n0.1265\nMdAE\n0.0755\n0.0745\n0.0743\n0.0708\n0.0731\n0.0770\nR2S\n−1.2481\n−1.2040\n−0.7588\n−0.8710\n−1.6787\n−3.9073 × 1011\nEVS\n0.5909\n0.5925\n0.5943\n0.5957\n0.0674\n−2.3854 × 1011\nTABLE V: The experiment results on synthetic seismic intensity dataset.\nDepth\n1-layer\n2-layer\n3-layer\n4-layer\n5-layer\n6-layer\nMAE\n0.176\n0.125\n0.054\n0.036\n0.061\n0.25\nMSE\n0.0578\n0.0312\n0.0049\n0.0037\n0.0074\n0.1684\nMdAE\n0.151\n0.091\n0.051\n0.049\n0.064\n0.16\nR2S\n0.939\n0.97\n0.995\n1.0\n0.99\n0.859\nEVS\n0.939\n0.97\n0.995\n1.0\n0.99\n0.859\n1\n2\n3\n4\n5\n6\n0.06\n0.07\n0.08\n0.09\n0.1\n0.11\n0.12\n0.13\nThe number of fully connected layers\nMSE (log)\n(a) Accuracy and depth\n1\n2\n3\n4\n5\n6\n70\n80\n90\n100\n110\nThe number of fully connected layers\nRate of valid models (%)\n(b) Valid model and depth\nFig. 10: The generalization error result of deep nets.\nF. Applications for the earthquake seismic intensity prediction\nFor verifying our theoretical assertions on real applications,\nin this subsection, we do experiments on earthquake seismic\nintensity estimations. Earthquake early warning (EEW) sys-\ntems serve as the tools for coseismic risk reduction. One of the\nchallenges in the development of EEW systems is the accuracy\nof seismic intensity estimation at the largest possible warning\ntime. Seismic intensity is the intensity or severity of ground\nshaking at a given location. The level of seismic intensity\ndepends heavily on the distance between the observation site\nand the epicenter. It can be realized that the level of seismic\nintensity is a radial function by taking the epicenter as the\norigin. In the experiments, we test on synthetic data and then\ndeal with a real world dataset.\n1) Synthetic data experiment: The Modiﬁed Mercalli inten-\nsity scale (MM or MMI), descended from Giuseppe Mercalli’s\nMercalli intensity scale of 1902, is the most used seismic\nintensity scale for measuring the intensity of shaking at a given\nlocation. It has been a common sense that seismic intensity\nis an expression of the amplitude, duration and frequency\nof ground motion. Thus, many attempts have been made to\nestimate MMI with the ground motion parameters [46]. Fourier\namplitude spectrum (FAS) is one of the best features meeting\nthe requirement based on which [47] gives an estimation of\nMMI as\nMMI = exp{1.2655 + 0.2089M\n−0.0011d −0.2451 log(d + 2.1502M)},\n(20)\nwhere d is the estimated Joyner-Boore distance (in kilometers),\nand M is the moment magnitude. Figure 11 shows a dense\nsynthetic MMI map generated by (20).\n−50\n0\n50\n−100\n−80\n−60\n−40\n−20\n0\n20\n40\n60\n80\n100\n5\n5.5\n6\n6.5\n7\n7.5\n8\n8.5\n         epicenter\nSeismic Intensity\nFig. 11: Dense synthetic MMI map generated by (20).\nFor network testing, we generate 900 samples according to\n(20) for training networks of 6 different depths. The testing\nresults are reported in Figure 12 and Table V. Similar to the\nexperiment in Subsection VI-E, networks perform well and\nthe best result is also given by a 4-layer network.\n1\n2\n3\n4\n5\n6\n10\n−3\n10\n−2\n10\n−1\n10\n0\nThe number of fully connected layers\nMSE (lg)\nFig. 12: MSE curve of network during training on synthesis\nsynthetic intensity dataset.\n2) Real data experiment: For the real data experiment, data\nare from the U.S. Earthquake Intensity Database4, which col-\nlects damage and felt reports for over 23,000 U.S. earthquakes.\nThe digital database contains information regarding epicentral\ncoordinates, magnitudes, focal depths, names and coordinates\n4 https://www.ngdc.noaa.gov/seg/hazard /earthqk.shtml\n14\nof reporting cities (or localities), reported intensities, and the\ndistance from the city (or locality) to the epicenter. Some sam-\nples of the data are shown in Figure 13. The input of networks\nin this experiment are the latitude and longitude coordinates\nof the site where the earthquake occurred (green box), and the\noutput is seismic intensity (red box). As the seismic intensity\nvalues in the database are integers in {2, . . . , 6} , we consider\nthis task as a classiﬁcation problem rather than a regression\nproblem.\nIn order to have enough data for network training, we collect\nthe most data impacted by the same epicenter from the dataset\nas the experiment data. There are total 608 samples, in which,\n500 are used for training and the rest 108 for testing. The\nparameter tuning strategy is similar to that of Section VI-E.\nTable VI shows the comparison results between deep nets\nof different depths and traditional classiﬁcation methods, i.e.,\nsupport vector machine (SVM) and random forest (RF). It\nis shown that a 5-layer deep net gives the best performance,\nwhile networks with other depths cannot compete with SVM.\nFig. 13: U.S. Earthquake Intensity Data\nTABLE VI: Comparisons with traditional methods.\nMethod\nRecognition rate\nSVM\n62.96%\nRandom forest\n58.33%\nDeep networks\n1-layer\n57.41%\n3-layer\n60.1%\n5-layer\n66.67%\n7-layer\n62.03%\nVII. CONCLUSION\nIn this paper, we studied theoretical advantages of deep nets\nvia considering the role of depth in feature extraction and\ngeneralization. The main contributions are four folds. Firstly,\nunder the same capacity costs (via covering numbers), we\nproved that deep nets are better than shallow nets in extracting\nthe group structure features. Secondly, we proved that deep\nReLU nets are one of the optimal tools in extracting the\nsmoothness feature. Thirdly, we rigorously proved the adaptiv-\nity of features to depths and vice verse, which was adopted to\nderive the optimal learning rate for implementing empirical\nrisk minimization on deep nets. Finally, we conducted ex-\ntensive numerical experiments including toy simulations and\nreal data veriﬁcations to show the outperformance of deep\nnets in feature extraction and generalization. All these results\npresented reasonable explanations for the success of deep\nlearning and provided solid guidance on using deep nets. In\nthis paper, we only consider the depth selection in regression\nproblems. It would be interesting and important to develop\nsimilar conclusions for classiﬁcation. We will consider this\ntopic and report progress in our future study later.\nACKNOWLEDGMENTS\nThe research of Z. Han and S. Yu was partially supported by\nthe National Natural Science Foundation of China [Grant Nos.\n61773367, 61821005], the Youth Innovation Promotion Asso-\nciation of the Chinese Academy of Sciences [Grant 2016183].\nThe research of S.B. Lin was supported by the National\nNatural Science Foundation of China [Grant No. 61876133],\nand the research of D.X. Zhou was partially supported by the\nResearch Grant Council of Hong Kong [Project No. CityU\n11306617].\nREFERENCES\n[1] Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep\nlearning via over-parameterization. arXiv preprint arXiv:1811.03962,\n2018.\n[2] A. Barron. Universal approximation bounds for superpositions of a\nsigmoidal function. IEEE Trans. Inform. Theory, 1993, 39(3): 930-945.\n[3] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A\nreview and new perspectives. IEEE Trans. Pattern Anal. Mach. Intel., 3,\n1798-1828, 2013.\n[4] M. Bianchini and F. Scarselli. On the complexity of neural network\nclassiﬁers: a comparison between shallow and deep architectures. IEEE.\nTrans. Neural Netw. & Learn. Sys., 25: 1553-1565, 2014.\n[5] C. M. Bishop. Pattern Recognition and Machine Learning. Springer,\n2006.\n[6] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE\nTrans. Pattern Anal. Mach. Intel., 35: 1872-1886, 2013.\n[7] C. K. Chui, X. Li, and H. N. Mhaskar. Neural networks for lozalized\napproximation. Math. Comput., 63: 607-623, 1994.\n[8] C. K. Chui, S. B. Lin, and D. X. Zhou. Construction of neural networks\nfor realization of localized deep learning. Front. Appl. Math. Statis., 4:\n14, 2018.\n[9] C. K. Chui, S. B. Lin, and D. X. Zhou, Deep neural networks for\nrotation-invariance approximation and learning. Anal. Appl., 17: 737-\n772, 2019.\n[10] F. Cucker and D. X. Zhou. Learning Theory: An Approximation Theory\nViewpoint, Cambridge University Press, Cambridge, 2007.\n[11] G. Cybenko. Approximation by superpositions of sigmoidal function.\nMath. Control Signals Syst., 2: 303-314, 1989.\n[12] O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks.\nNIPs, 666-674, 2011.\n[13] D. L. Donoho. Unconditional bases are optimal bases for data com-\npression and for statistical estimation. Appl. Comput. Harmonic. Anal.,\n1(1):100-115, 1993.\n[14] R. Eldan and O. Shamir. The power of depth for feedforward neural\nnetworks. arXiv preprint arXiv:1512.03965, 2015.\n[15] T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and\nsupport vector machines. Adv. Comput. Math., 13: 1-50, 2000.\n[16] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press,\n2016.\n[17] Z. C. Guo, D. H. Xiang, X. Guo, and D. X. Zhou. Thresholded spectral\nalgorithms for sparse approximations. Anal. Appl. 15: 433–455, 2017.\n[18] Z.\nC.\nGuo,\nL.\nShi,\nand\nS.\nB.\nLin.\nRealizing\ndata\nfeatures\nby\ndeep\nnets,\nIEEE\nTrans.\nNeural\nNetw.\nLearn.\nSyst.,\nDOI:\n10.1109/TNNLS.2019.2951788.\n[19] L. Gy¨orfy, M. Kohler, A. Krzyzak and H. Walk. A Distribution-Free\nTheory of Nonparametric Regression. Springer, Berlin, 2002.\n[20] M. Hagan, M. Beale, and H. Demuth. Neural Network Design. PWS\nPublishing Company, Boston, 1996.\n[21] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical\nLearning: Data mining, Inference and Prediction. Springer, New York,\n2001.\n15\n[22] N. Harvey, C. Liaw, A. Mehrabian. Nearly-tight VC-dimension bounds\nfor piecewise linear neural networks. Conference on Learning Theory.\n2017: 1064-1068.\n[23] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for\ndeep belief netws. Neural Comput., 18:, 1527-1554, 2006.\n[24] M. Imaizumi and K. Fukumizu. Deep Neural Networks Learn Non-\nSmooth Functions Effectively. arXiv preprint arXiv:1802.04474, 2018.\n[25] M. Kohler. Optimal global rates of convergence for noiseless regression\nestimation problems with adaptively chosen design. J. Mult. Anal., 132:\n197-208, 2014.\n[26] M. Kohler and A. Krzyzak. Nonparametric regression based on hierar-\nchical interaction models. IEEE Trans. Inform. Theory, 63: 1620-1630,\n2017.\n[27] M. Leshno, V. Y. Lin, A. Pinks, and S. Schocken. Multilayer feedforward\nnetworks with a nonpolynomial activation function can approximat any\nfunction. Neural Networks, 6: 861-867, 1993.\n[28] H. W. Lin, M. Tegmark and D. Rolnick. Why does deep and cheap\nlearning works so well?. J. Stat. Phys., 168: 1223-1247, 2017.\n[29] S. Lin, Y. Rong and Z. Xu. Multivariate Jackson-type inequality for a\nnew type neural network approximation. Appl. Math. Model., 38: 6031-\n6037, 2014.\n[30] S. B. Lin. Limitations of shallow nets approximation. Neural Networks,\n94: 96-102, 2017.\n[31] S. B. Lin. Generalization and expressivity for deep nets. IEEE Trans.\nNeural Netw. Learn. Syst., In Press.\n[32] S. B. Lin and D. X. Zhou. Distributed kernel-based gradient descent\nalgorithms. Constr. Approx., 47: 249-276, 2018.\n[33] S. Lin, J. Zeng, and X. Zhang. Constructive neural network learning.\nIEEE Trans. Cyber., 49: 221 - 232, 2019.\n[34] T. Lin and H. Zha. Riemannian manifold learning. IEEE Trans. Pattern\nAnal. Mach. Intel., 30: 796-809, 2008.\n[35] V. Maiorov and A. Pinkus. Lower bounds for approximation by MLP\nneural networks. Neurocomputing, 25: 81-91, 1999.\n[36] H. N. Mhaskar. Neural networks for optimal approximation of smooth\nand analytic functions. Neural Comput., 8: 164-177, 1996.\n[37] H. N. Mhaskar and T. Poggio. Deep vs. shallow networks: An approx-\nimation theory perspective. Anal. Appl., 14: 829-848, 2016.\n[38] G. Mont´ufar, R. pascanu, K. Cho, and Y. Bengio. On the number of\nlinear regions of deep nerual networks. Nips, 2014: 2924-2932.\n[39] P. Petersen and F. Voigtlaender. Optimal aproximation of piecewise\nsmooth functions using deep ReLU neural networks. Neural Networks,\n108: 296-330, 2018.\n[40] A. Pinkus. n-Widths in Approximation Theory. Springer-Velag, Berlin\nHeidelberg, 1985.\n[41] A. Pinkus. Approximation theory of the MLP model in neural networks.\nActa Numerica, 8: 143-195, 1999.\n[42] I. Safran and O. Shamir. Depth-width tradeoffs in approximating natu-\nral functions with neural networks. arXiv reprint arXiv:1610.09887v2,\n2016.\n[43] C. Satriano, Y. M. Wu, A. Zollo, and H. Kanamori. Earthquake early\nwarning: Concepts, methods and physical grounds. Soil Dynamics Earth.\nEngineer., 31: 106-118, 2011.\n[44] C. Schwab and J. Zech. Deep learning in high dimension: Neural\nnetwork expression rates for generalized polynomial chaos expansions\nin UQ. Anal. Appl., 17: 19-55, 2019.\n[45] U. Shaham, A. Cloninger, and R. R. Coifman. Provable approximation\nproperties for deep neural networks. Appl. Comput. Harmonic Anal.,\n44: 537-557, 2018.\n[46] V. Y. Sokolov and Y. K. Chernov. On the correlation of seismic intensity\nwith Fourier amplitude spectra. Earthquake Spectra., 14: 679-694, 1998.\n[47] V. Y. Sokolov. Seismic intensity and Fourier acceleration spectra: revised\nrelationship. Earthquake Spectra., 18: 161-187, 2002.\n[48] I. Steinwart, and A. Christmann. Support Vector Machines. Springer,\nNew York, 2008.\n[49] K. Vikraman. A deep neural network to identify foreshocks in real time.\narXiv preprint arXiv:1611.08655, 2016.\n[50] D. Yarotsky. Error bounds for aproximations with deep ReLU networks.\nNeural Networks, 94: 103-114, 2017.\n[51] Y. Ying and D. X. Zhou. Unregularized online learning algorithms with\ngeneral loss functions. Appl. Comput. Harmonic Anal., 42: 224-244,\n2017.\n[52] D. X. Zhou. Capacity of reproducing kernel spaces in learning theory.\nIEEE Trans. Inform. Theory, 49: 1743-1752, 2003.\n[53] D. X. Zhou and K. Jetter. Approximation with polynomial kernels and\nSVM classiﬁers. Adv. Comput. Math., 25: 323-344, 2006.\n[54] D. X. Zhou. Deep distributed convolutional neural networks: Universal-\nity. Anal. Appl., 16: 895-919, 2018.\n[55] D. X. Zhou. Universality of Deep Convolutional Neural Networks. Appl.\nComput. Harmonic. Anal., 48: 784-794, 2020\n[56] D. X. Zhou. Theory of deep convolutional neural networks: Downsam-\npling. Neural Netw., 124: 319-327, 2020.\n16\nAPPENDIX A: PROOFS OF THEOREM 1\nIn Appendix A, we prove Lemma 4 and Theorem 1. Our\nmain tool is the following lemma, which can be found in [39,\nLemma A.3].\nLemma 5. Let θ > 0 and ˜L ∈N with ˜L > (2θ)−1. For\nany ε ∈(0, 1), there exists a deep ReLU net ˜×2,˜L,ε with\n2˜L + 8 layers and at most cε−θ nonzero parameters which\nare bounded by ε−γ such that\n|uu′ −˜×2,˜L,ε(u, u′)| ≤ε,\n∀u, u′ ∈[−2, 2],\nwhere c, γ are positive constants depending only on ˜L and θ.\n1u\n2\nu\n \nPGU\n)2\n(\n3\nu\n \nPGU\n)2\n(\n \nPGU\n)2\n(\n4\nu\n...\n1\n\nu\nu\n \nPGU\n)2\n(\n...\n \nPGU\n)2\n(\n\n~\nFig. 14: Network structure for PGU(ℓ)\nBased on the above lemma, we can construct product gate\nunits (PGUs) for multiple factors. In particular, there are\nnumerous network structures for extending PGU with 2 factors\nto PGU with ℓ> 2 factors. We only prove that a network\nstructure shown in Figure 14 is suitable for this purpose in\nLemma 4.\nProof of Lemma 4:\nFor j = 2, . . . , ℓand ε ∈(0, 1),\ndeﬁne\n˜×j,˜L,ε/ℓ(u1, . . . , uj)\n:=\nj−1\nz\n}|\n{\n˜×2,˜L,ε/ℓ(˜×2,˜L,ε/ℓ(· · · ˜×2,˜L,ε/ℓ(u1, u2), · · · , uuj−1), uj).\nThen,\nu1u2 · · · uℓ−˜×ℓ,˜L,ε/ℓ(u1, . . . , uℓ)\n=\nu1u2 · · · uℓ−˜×2,˜L,ε/ℓ(u1, u2)u3 · · · uℓ\n+\n˜×2,˜L,ε/ℓ(u1, u2)u3 · · · uℓ−˜×3,˜L,ε/ℓ(u1, u2, u3)u4 · · · uℓ\n+\n. . .\n+\n˜×ℓ−1,˜L,ε/ℓ(u1, . . . , uℓ−1)uℓ−˜×ℓ,˜L,ε/ℓ(u1, . . . , uℓ).\nBut Lemma 5 implies\n|˜×2,˜L,ε/ℓ(u, u′)| ≤|u||u′|+ε/ℓ≤|u|+ε/ℓ,\n∀u, u′ ∈[−1, 1].\nThis together with 0 < ε < 1 and ui ∈[−1, 1] for all i =\n1, 2, . . . , ℓyields\n|˜×j,˜L,ε/ℓ(u1, . . . , uj)|\n≤\n|˜×j−1,˜L,ε/ℓ(u1, . . . , uj−1)| + ε/ℓ\n≤\n· · · ≤|˜×2,˜L,ε/ℓ(u1, u2)| + (j −2)ε/ℓ\n≤\n1 + (j −1)ε/ℓ≤2,\n∀j = 2, . . . , ℓ.\nThen, it follows from Lemma 5 again that\n|u1u2 · · · uℓ−˜×ℓ,˜L,ε/ℓ(u1, . . . , uℓ)|\n≤\n|u1u2 −˜×2,˜L,ε/ℓ(u1, u2)|\n+\n|˜×2,˜L,ε/ℓ(u1, u2)u3 −˜×3,˜L,ε/ℓ(u1, . . . , u3)| + . . .\n+\n|˜×ℓ−1,˜L,ε/ℓ(u1, . . . , uℓ−1)uℓ−˜×ℓ,˜L,ε/ℓ(u1, . . . , uℓ)|\n≤\n(ℓ−1)ε/ℓ< ε.\nSince ˜×2,˜L,ε/ℓis a deep net with 2˜L + 8 layers and at most\ncℓθε−θ nonzero parameters bounded ℓγε−γ and the parameters\nin all PGU(2) are the same, ˜×ℓ,˜L,ε/ℓis a deep net with at most\n(2˜L + 8)ℓlayers, and at most cℓθε−θ non-zero parameters\nwhich are bounded by ℓγε−γ. Setting ˜×ℓ= ˜×ℓ,˜L,ε/ℓ, we\ncomplete the proof of Lemma 4.\nWe are now in a position to prove Theorem 1.\nProof of Theorem 1: By Lemma 4, for any ε ∈(0, 1),\nθ > 0 and ˜L with ˜L > (2θ)−1 there exists a deep ReLU net\n˜×β with at most (2˜L + 8)β layers, and at most c(µBβ)θε−θ\nnon-zero parameters which are bounded by (µBβ)γε−γ such\nthat\n|u1u2 · · · uβ−˜×β(u1, . . . , uβ)| ≤\nε\nµB ,\n∀u1, . . . , uβ ∈[−1, 1].\n(21)\nThen, for any monomial\nxα =\nα1\nz\n}|\n{\nx(1) · · · x(1) · · ·\nαd\nz\n}|\n{\nx(d) · · · x(d) ·\nβ−|α|\nz }| {\n1 · · · 1\nsatisfying |α| ≤β, it follows from (21) that\n|xα −˜×β(\nα1\nz\n}|\n{\nx(1), . . . , x(1), . . . ,\nαd\nz\n}|\n{\nx(d), . . . , x(d),\nβ−|α|\nz }| {\n1, . . . , 1)| ≤\nε\nµB .\nRewrite P ∈Pd\nβ,B,µ as P = P\nα∈Λµ cαxα with |cα| ≤B,\nwhere Λµ ⊆{α : |α| ≤β} is a set of at most µ elements.\nDeﬁne\nhP (x) =\nX\nα∈Λµ\ncα ˜×β(\nα1\nz\n}|\n{\nx(1), . . . , x(1), . . . ,\nαd\nz\n}|\n{\nx(d), . . . , x(d),\nβ−|α|\nz }| {\n1, . . . , 1).\n(22)\nThen,\n|hP (x) −P(x)| ≤\nX\nα∈Λµ\n|cα| ε\nµB ≤ε.\nSince the parameters in PGU(β) are shared to be same, µ\ndifferent α ∈ΛL activate µ PGU(β), there are totally µ +\nc(µBβ)θε−θ free parameters in hP , which are distributed in\n(2˜L + 8)β + 1 and bounded by (µBβ)γε−γ. This completes\nthe proof of Theorem 1.\nAPPENDIX B: PROOF OF THEOREM 2\nWe need a construction of deep nets motivated by [50]. For\nt ∈R, deﬁne\nψ(t) = σ(t + 2) −σ(t + 1) −σ(t −1) + σ(t −2).\n(23)\nThen,\nψ(t) =\n\n\n\n1,\n|t| ≤1,\n0,\n|t| ≥2,\n2 −|t|,\n1 < |t| < 2.\n(24)\n17\nFurthermore, for N\n∈\nN and j\n=\n(j1, . . . , jd)\n∈\n{0, 1, . . . , N}d, deﬁne\nφj,N(x) =\nd\nY\nk=1\nψ\n\u0012\n3N\n\u0012\nx(k) −jk\nN\n\u0013\u0013\n.\n(25)\nDirect computation yields\nX\nj∈{0,1,...,N}d\nφj,N(x) = 1,\n∀x ∈Id\n(26)\nand\nsuppφj,N ⊆{x : |x(k) −jk/N| < 1/N, ∀k},\n(27)\nwhere suppf denotes the support of the function f. Before\npresenting the proof of Theorem 2, we introduce two technical\nlemmas. The ﬁrst one can be found in [25, Lemma 1].\nLemma 6. Let r = s + v with s ∈N0 and 0 < v ≤1. If\nf ∈Lip(r,c0), x0 ∈Rd and ps,x0,f is the Taylor polynomial\nof f with degree s at around x0, then\n|f(x) −ps,x0,f(x)| ≤˜c1∥x −x0∥r\n2,\n∀x ∈Id,\n(28)\nwhere ˜c1 is a constant depending only on s, c0 and d.\nThe second one focuses on approximating functions in\nLip(r,c0) by products of Taylor polynomials and ReLU nets.\nLemma 7. For f ∈Lip(r,c0) with r = s + v, N ∈N and\nj/N = (j1/N, . . . , jd/N), deﬁne\nf1(x) =\nX\nj∈{0,1,...,N}d\nφj,N(x)ps,j/N,f(x),\n(29)\nthen\n|f(x) −f1(x)| ≤˜c2N −r,\n∀x ∈Id,\n(30)\nwhere ˜c2 is a constant depending only on r, c0 and d.\nProof: Due to (26), (27), 0 ≤φj,N(x) ≤1 and Lemma\n6, we have\n|f(x) −f1(x)|\n=\n\f\f\f\f\f\f\nf(x) −\nX\nj∈{0,...,N}d\nφj,N(x)ps,j/N,f(x)\n\f\f\f\f\f\f\n≤\nX\nj∈{0,...,N}d\nφj,N(x)|f(x) −ps,j/N,f(x)|\n≤\nX\nj:|x(k)−jk/N|<1/N,∀k\n|f(x) −ps,j/N,f(x)|\n≤\n2d\nmax\nj:|x(k)−jk/N|<1/N,∀k |f(x) −ps,j/N,f(x)|\n≤\n2d˜c1\nmax\nj:|x(k)−jk/N|<1/N,∀k ∥x −j/N∥r\n2\n≤\n˜c12d√\ndrN −r.\nThis proves Lemma 7 with ˜c2 = ˜c12d√\ndr.\nWith these helps, we prove Theorem 2 as follows.\nProof of Theorem 2: Rewrite\nf1 =\nX\nj∈{0,1,...,N}d\nX\nα:|α|≤s\naj,αφj,N(x)xα\n(31)\nwith |aj,α| ≤˜B. Then for arbitrarily ﬁxed α with |α| ≤s and\nj ∈{0, 1, . . . , N}d, we can rewrite\nφj,N(x)xα\n=\nα1\nz\n}|\n{\nx(1) · · · x(1) · · ·\nαd\nz\n}|\n{\nx(d) · · · x(d) ·\ns−|α|\nz }| {\n1 · · · 1\nd\nY\nk=1\nψk(x),\nwhere\nψk(x) = ψ\n\u0012\n3N\n\u0012\nx(k) −jk\nN\n\u0013\u0013\n.\n(32)\nBut Lemma 4 shows that for any 0\n<\nν\n<\n1 and\nu1, . . . , us+d ∈[−1, 1], there exists a deep net ˜×d+s with\n2(d + s)˜L + 8(d + s) layers and at most c(d + s)θν−θ free\nparameters which are bounded by (d + s)γν−γ such that\n|u1u2 · · · ud+s −˜×d+s(u1, . . . , ud+s)| ≤ν.\n(33)\nDeﬁne\nhf(x)\n:=\nX\nj∈{0,1,...,N}d\nX\nα:|α|≤s\naj,α ˜×d+s\n\u0000ψ1(x), . . . , ψd(x),\nα1\nz\n}|\n{\nx(1), · · · x(1), . . . ,\nαd\nz\n}|\n{\nx(d), . . . x(d),\ns−|α|\nz }| {\n1, . . . , 1\n\u0001\n.\n(34)\nNote that hf possesses (N + 1)d \u0000s+d\ns\n\u0001\nPGU(d + s), ˜×d+s,\nwith the same parameters and also involves (N + 1)d \u0000s+d\ns\n\u0001\nΨk(·). But (23) and (32) show that there are totally 8d + 4\nfree parameters for each Ψk(·), k = 1, . . . , d. Therefore, there\nare totally 2(s + d)˜L + 8(s + d) + 3 layers and at most c(d +\ns)θν−θ + (8d + 5)(N + 1)d \u0000s+d\ns\n\u0001\nfree parameters which are\nbounded by max{ ˜B, 3N, (d + s)γν−γ} in hf. Furthermore,\n(34), (33) and (31) yield that for arbitrary x ∈Id,\n|f1(x) −hf(x)|\n≤\nX\nj∈{0,1,...,N}d\nX\nα:|α|≤s\n|aj,α|ε\n≤\n(N + 1)d \u0000s+d\ns\n\u0001 ˜Bν.\nRecalling Lemma 7, we have\n|f(x) −hf(x)| ≤˜c2N −r + (N + 1)d \u0000s+d\ns\n\u0001 ˜Bν.\nSetting N + 1 =\n\u0002\nν−1/(d+r)\u0003\n, we obtain\n|f(x) −hf(x)| ≤˜c3νr/(r+d),\nwhere ˜c3 := ˜c2 + 2d \u0000s+d\ns\n\u0001 ˜B. Denote ε = νr/(r+d). Then, for\nany ε ∈(0, 1), hf is a deep net with 2(d+ s)˜L+8(d+s)+3\nlayers and at most c(d+s)θε−(r+d)θ/r +(8d+5)\n\u0000s+d\ns\n\u0001\nε−d/r\nfree parameters which are bounded by max{ ˜B, 3ε−1/r, (d +\ns)γε−(r+d)γ/r} satisfying\n|f(x) −hf(x)| ≤˜c3ε,\nThis completes the proof of Theorem 2.\nAPPENDIX C: PROOF OF THEOREM 3\nProof of Theorem 3:\nFor any 0 < ν1 < 1/2, θ > 0,\n˜L ∈N with ˜L > (2θ)−1 and k = 1, . . . , d∗, it follows\nfrom Theorem 1 that there is deep ReLU net hPk,with at\n18\nmost 2˜L + 8+ 1 layers and at most µ + c(µ)θν−θ\n1\nnonzero\nparameters which are bounded by (µ)γν−γ\n1\nsuch that\n|Pk,(x) −hPk,(x)| ≤ν1,\n∀x ∈IDk, k = 1, . . . , d∗.\n(35)\nSince |Pk,(x)| ≤1/2 for x ∈Id, we have from (35) and\n0 < ν1 ≤1/2 that |hPk,(x)| ≤1. Deﬁne\nhd∗(x) = (hP1,(x), hP2,(x), . . . , hPd∗,(x)).\nLet g be the d∗-dimensional function in Assumption 1, i.e.,\nf ∗(x) = g(P1,(x), . . . , Pd∗,(x)).\nFor arbitrary 0\n<\nν2\n<\n1, Theorem 2 shows that\nthere is deep net hg with L(d∗, r, ˜L) layers and at most\nc(d∗+ s)θν−(r+d∗)θ/r\n2\n+ (8d∗+ 5)\n\u0000s+d∗\ns\n\u0001\nν−d∗/r\n2\nfree pa-\nrameters which are bounded by max{ ˜Bg, 3ν−1/r\n2\n, (d∗+\ns)γν−(r+d∗)γ/r\n2\n} such that\n∥g −hg∥L∞(Id∗) ≤c1ν2,\n(36)\nwhere\n˜Bg :=\nmax\nk1+···+kd∗≤max\nx∈Id\n\f\f\f\f\n1\nk1! . . . kd∗!\n∂k1+...kd∗f(x)\n∂k1x(1) . . . ∂kdx(d∗)\n\f\f\f\f\nDeﬁne\nhf ∗(x) = hg(hd∗(x)).\n(37)\nFor arbitrary x ∈Id, there holds\n|f ∗(x) −hf ∗(x)|\n=\n|g(P1,(x), . . . , Pd∗,(x)) −hg(hd∗(x))|\n≤\n|g(P1,(x), . . . , Pd∗,(x)) −g(hd∗(x))|\n+\n|g(hd∗(x)) −hg(hd∗(x))|.\nDue to the smoothness of g, we have from (35) and (6) that\n|g(P1,(x), . . . , Pd∗,(x)) −g(hd∗(x))|\n≤\n˜c4 max\n1≤k≤d∗|Pk,(x) −hPk,(x)|τr ≤˜c4ντr\n1 ,\nwhere ˜c4 > 0 is a constant depending only on c0, d∗and g.\nBut (36) yields\n|g(hd∗(x)) −hg(hd∗(x))| ≤c1ν2,\n∀x ∈Id.\nThus, we have\n|f ∗(x) −hf ∗(x)| ≤c1ν2 + ˜c4ντr\n1 .\nSet ν2 = ντr\n1 = ε. Then for any 0 < ε ≤1/2, we get\n|f ∗(x) −hf ∗(x)| ≤˜c5ε,\nwhere ˜c5 is a constant depending only on c0, d, d∗, r, s and g.\nUnder this circumstance, hf ∗is a deep net with\nL∗(d∗, r, ˜L, ) = L(d∗, r, ˜L) + 2˜L + 8+ 1\nlayers and at most\nc(d∗+s)θε−(r+d∗)θ\nr\n+(8d∗+5)\n\u0010\ns+d∗\ns\n\u0011\nε−d∗\nr +d∗µ+c(µ)θε−θ\nτr\nfree parameters which are bounded by\nmax{ ˜Bg, 3ε−1/r, (d∗+ s)γε−(r+d∗)γ/r, (µ)γε−γ/τr}.\nThis completes the proof of Theorem 3.\nAPPENDIX D: PROOF OF THEOREM 4\nIn this section only, C′\n1, C′\n2, . . . denote constants indepen-\ndent of L, n, ε, δ, µ or . To prove Theorem 4, we need the\nfollowing well-known oracle inequality in learning theory, the\nproof of which can be found in [9].\nLemma 8. Let ρX be the marginal distribution of ρ on\nX and (L2\nρX , ∥· ∥ρ) denote the Hilbert space of square-\nintegrable functions on X with respect to ρX. Set ED(f) :=\n1\nm\nPm\ni=1(f(xi) −yi)2 and deﬁne\nfD,H = arg min\nf∈H ED(f),\n(38)\nwhere H is a set of functions deﬁned on X. Suppose further\nthat there exist n′, U > 0, such that\nlog N(ε, H, L∞(Id)) ≤n′ log U\nε ,\n∀ε > 0.\n(39)\nThen for any h ∈H,\nProb{∥πMfD,H −fρ∥2\nρ > ε + 2∥h −fρ∥2\nρ}\n≤\nexp\n\u001a\nn′ log 16UM\nε\n−\n3mε\n512M 2\n\u001b\n+\nexp\n(\n−3mε2\n16(3M + ∥h∥L∞(X))2 \u00006∥h −fρ∥2ρ + ε\n\u0001\n)\n.\nNow we apply Lemma 8, Lemma 1, and Theorem 3 to prove\nTheorem 4.\nProof of Theorem 4: Due to (17) and (18), it follows from\nTheorem 3 with ε = n−r/d∗that there exists an hρ ∈HL,n,R\nwith L = L∗(d∗, r, ˜L, ), R given in (15), and at most C′\n1n\nfree parameters such that\n∥fρ −hρ∥2\nρ ≤∥fρ −hρ∥2\nL∞(Id) ≤c2\n2n−2r/d∗.\nNoting further |y| ≤M almost surely as well as ∥fρ∥L∞(Id) ≤\nM, we get\n∥hρ∥L∞(Id) ≤c2 + M =: C′\n2.\nBut Lemma 1 shows\nlog N\n\u0000ε, Hn,L,R, L∞(Id)\n\u0001\n≤C′\n3L2n log n\nε ,\nwhere we used (18), (34), (22), (37), Figure 14 and the fact\nthat the largest width of deep nets in PGU(2) in get an ε\napproximation of the product of two real number [39, Lemma\nA.3] is smaller than ˜Cpgu2ε−θ to derive\nDmax\n≤\nmax\nn\u0010\n+d\nd\n\u0011\nc ˜Cpgu2θnθr/d∗,\nC′\n1n\n\u0010\ns+d∗\nd∗\n\u0011\n(s + 4d∗+ c ˜Cpgu2sθnθr/d∗)\no\n≤\nC′\n4nC′\n4,\nand ˜Cpgu2 is an absolute constant. Plugging the above three\nestimates into Lemma 8 with n′ = C′\n5L2n, U = n, we have\nProb{∥πMfD,n,L −fρ∥2\nρ > ε + 2∥hρ −fρ∥2\nρ}\n≤\nexp\n\u001a\nC′\n5L2n log 16Mn\nε\n−\n3mε\n512M 2\n\u001b\n+\nexp\n(\n−3mε2\n16(3M + C′\n2)2 \u00006c2\n5n−2r/d∗+ ε\n\u0001\n)\n.\n19\nTABLE VII: Network architecture and corresponding MSE.\nWidths\nParameter percentage (%)\nMSE\nLayer-1\nLayer-2\nLayer-3\np(C1)\np(C2)\np(C3)\np(C4)\n150\n30\n60\n18.62\n57.34\n22.56\n1.48\n4410.946\n50\n90\n30\n6.45\n57.52\n35.27\n0.76\n4588.056\n60\n60\n60\n5.72\n39.74\n53.11\n1.43\n4690.85\n120\n20\n200\n14.85\n30.92\n49.33\n4.91\n5402.052\n220\n20\n50\n27.80\n58.11\n12.83\n1.26\n5936.061\n80\n50\n60\n10.07\n50.75\n37.94\n1.24\n6147.745\n80\n70\n20\n10.13\n71\n18.38\n0.49\n6573.265\n50\n110\n20\n6.06\n65.99\n27.47\n0.48\n7120.719\n100\n40\n70\n13.45\n50.12\n35.41\n1.02\n7551.107\n210\n20\n70\n26.11\n54.58\n17.57\n1.73\n8574.581\n210\n10\n300\n26.24\n28.87\n37.44\n7.45\n11103.629\n40\n10\n600\n5.1\n5.47\n74.75\n14.68\n16730.241\n350\n10\n60\n43.39\n47.59\n7.54\n1.48\n31249.375\n50\n20\n300\n6.34\n12.01\n74.88\n6.77\n35842.366\n300\n5\n500\n36.2\n21.65\n30.13\n12.03\n52774.265\n420\n1\n1000\n26.24\n10.45\n12.45\n50.86\n203840.239\n380\n10\n10\n44.92\n51.48\n1.35\n2.25\n217662.506\n1\n15\n500\n0.25\n0.19\n87.14\n12.42\n413626478.3\n70\n1\n2400\n26.24\n1.74\n29.86\n33.16\n434036281.1\n1\n1\n2670\n0.25\n0.02\n33.25\n66.48\n766736147.9\nTABLE VIII: Extracting partially radial feature with different groups.\nEvaluation metrics\nk=2\nk=3\nk=4\nk=5\nk=6\nk=7\nk=8\nk=9\nMAE\n1-layer\n24.616\n26.027\n30.948\n30.957\n38.471\n47.654\n48.89\n66.579\n3-layer\n15.775\n23.827\n28.888\n29.002\n35.628\n46.716\n47.914\n56.534\nMSE\n1-layer\n1357.934\n1885.427\n2345.819\n2851.989\n4216.136\n5231.759\n6304.671\n9154.373\n3-layer\n777.817\n1682.958\n2125.355\n2555.937\n3674.015\n4884.519\n6265.782\n7098.902\nMdAE\n1-layer\n14.897\n16.511\n18.476\n24.209\n25.818\n30.385\n30.42\n35.619\n3-layer\n8.488\n13.609\n17.149\n20.01\n24.123\n28.585\n29.569\n34.299\nR2S\n1-layer\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3-layer\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nEVS\n1-layer\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n3-layer\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nTABLE IX: Extracting the square-feature of different network depths.\nDepth\n1-layer\n3-layer\n5-layer\n7-layer\n9-layer\n11-layer\nMAE\n48.989\n44.006\n145.568\n176.055\n427.975\n1204.083\nMSE\n14837.713\n4999.949\n49579.938\n86620.738\n273505.566\n3053474.62\nMdAE\n15.46\n8.624\n87.641\n92.42\n404.614\n836.368\nR2S\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nEVS\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nFor\nε ≥C′\n3n−2r/d∗L2 log n,\n(40)\nwe then get\nProb{∥πMfD,n,L −fρ∥2\nρ > 3ε}\n≤\nexp\n\b\nC′\n6(nL2 log n −mε)\n\t\n+ exp{−C′\n7mε}.\nSince n =\nh\nC1m\nd∗\n2r+d∗i\n, we obtain\nProb{∥πMfD,n,L −fρ∥2\nρ > 3ε} ≤2 exp{−C′\n8mε}\n≤\n2 exp{−C′\n9m\n2r\n2r+d∗ε/ log n}.\nLet\nε = C′\n10m−\n2r\n2r+d∗L2 log n log 3\nδ\nwith C′\n10C′\n9 > 1 such that (40) holds. Then,\nProb{∥πMfD,n,L −fρ∥2\nρ > 3ε} ≤δ.\nThus, with conﬁdence of at least 1 −δ, there holds\n∥πMfD,n,L −fρ∥2\nρ ≤C′\n11L2m−\n2r\n2r+1 log m log 3\nδ .\nThis proves (19) by noting the well-known relation\nE(f) −E(fρ) = ∥f −fρ∥2\nρ\n(41)\nand completes the proof of Theorem 4.\nAPPENDIX E: DETAILED NUMERICAL RESULTS OF\nEXPERIMENT VI-B2\nThe detailed numerical results are shown in Table VII.\nAPPENDIX F: DETAILED NUMERICAL RESULTS OF\nEXPERIMENT VI-C\nThe detailed numerical results are shown in Table VIII.\nAPPENDIX G: DETAILED NUMERICAL RESULTS OF\nEXPERIMENT VI-D\nThe detailed numerical results are shown in Table IX.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-04-01",
  "updated": "2020-04-01"
}