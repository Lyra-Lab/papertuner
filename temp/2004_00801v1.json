{
  "id": "http://arxiv.org/abs/2004.00801v1",
  "title": "Exploration of Reinforcement Learning for Event Camera using Car-like Robots",
  "authors": [
    "Riku Arakawa",
    "Shintaro Shiba"
  ],
  "abstract": "We demonstrate the first reinforcement-learning application for robots\nequipped with an event camera. Because of the considerably lower latency of the\nevent camera, it is possible to achieve much faster control of robots compared\nwith the existing vision-based reinforcement-learning applications using\nstandard cameras. To handle a stream of events for reinforcement learning, we\nintroduced an image-like feature and demonstrated the feasibility of training\nan agent in a simulator for two tasks: fast collision avoidance and obstacle\ntracking. Finally, we set up a robot with an event camera in the real world and\nthen transferred the agent trained in the simulator, resulting in successful\nfast avoidance of randomly thrown objects. Incorporating event camera into\nreinforcement learning opens new possibilities for various robotics\napplications that require swift control, such as autonomous vehicles and\ndrones, through end-to-end learning approaches.",
  "text": "Exploration of Reinforcement Learning for Event Camera using\nCar-like Robots\nRiku Arakawa∗†, Shintaro Shiba ∗∗†,\nAbstract— We demonstrate the ﬁrst reinforcement-learning\napplication for robots equipped with an event camera. Because\nof the considerably lower latency of the event camera, it is\npossible to achieve much faster control of robots compared with\nthe existing vision-based reinforcement-learning applications\nusing standard cameras. To handle a stream of events for\nreinforcement learning, we introduced an image-like feature\nand demonstrated the feasibility of training an agent in a\nsimulator for two tasks: fast collision avoidance and obstacle\ntracking. Finally, we set up a robot with an event camera in\nthe real world and then transferred the agent trained in the\nsimulator, resulting in successful fast avoidance of randomly\nthrown objects. Incorporating event camera into reinforcement\nlearning opens new possibilities for various robotics applications\nthat require swift control, such as autonomous vehicles and\ndrones, through end-to-end learning approaches.\nI. INTRODUCTION\nRobot automation has been prevailing in our society to\nassist us in various industries [1], [2]. Reinforcement learning\nis a highly promising technique that enables robots to acquire\nskills without the human effort of manually designing rules\nfor possible input–output controlling patterns [3]. Further-\nmore, transfer learning [4] can release us from training robots\nin the real world, which is often very costly. Instead, we can\ntrain them in a prepared simulator as long as its environment\nis similar to the real-world setting.\nSo far, most of the current reinforcement learning systems\nassume signals from standard RGB-cameras as input. This\nis attributed to the development of neural networks and\ncomputing power that allow us to handle high-dimensional\ndata with practical speed to get rich information for robots.\nHowever, relying on image inputs restricts the robot’s control\nfrequency to at most the sampling frequency of the image\nsensor.\nIn fact, most of the standard image sensors take images\nat 30–60 Hz (frames per second). This limitation can be a\nserious problem where high-speed control is required, such\nas in autonomous vehicles or drones because if a vehicle\nmoves at 30 m/s (67 mi/h), it moves one meter in 33 ms,\nwhich is “blind time” without any signal between each frame\nat 30 Hz. In addition, the rotational movement of robots\nresults in the rapid motion of surrounding objects, sometimes\nwith motion blur, which is a critical problem in robots such\nas autonomous drones.\n∗The University of Tokyo.\narakawa-riku428@g.ecc.u-tokyo.ac.jp\n∗∗Keio University.\nsshiba@keio.jp\n† equal contribution (ordered alphabetically)\nFig. 1: The agent trained in the simulator can be transferred to a\nreal robotic car to perform fast collision avoidance. The\nleft image is an image-like feature of events, which is\nan input for deep reinforcement-learning agent. The right\nphoto is the real robotic car equipped with an event camera\n(DAVIS240) [6]. For details, please refer to the video\nhttps://youtu.be/xc2nKJ1TLTY.\nRecently, as another “eye” for robots, event cameras have\nemerged [5]. The event camera, or event-based camera, is a\nneuromorphic vision sensor. It enables us to process stream\ndata at the sub-millisecond resolution, which is much faster\nthan the processing speed of standard CMOS or CCD cam-\neras. By using event cameras, the frequency of controlling\nrobots is no longer limited to 30 Hz or 60 Hz. Moreover,\nunlike standard high-speed cameras, it is small enough, and\nits power consumption is low enough to attach to mobile\nrobots. Therefore, the present robotics community has many\nexpectations for its applications.\nIn this paper, we demonstrate a real-world reinforcement-\nlearning application with an event camera, the policy of\nwhich is obtained in a simulation environment. First, we\npresent an efﬁcient method to directly create image-like\nfeatures of simulated events for input to conventional rein-\nforcement learning algorithms in a simulator. Then, we show\nthat we can successfully train an agent in the simulator to\nlearn collision avoidance and tracking objects, both of which\nrequire fast control. Finally, we demonstrate that the trained\nagent was able to be transferred into a real robot with an\nattached event camera by converting a stream of events into\nimage-like features (Fig. 1).\nTo the best of the authors’ knowledge, this is the ﬁrst study\nto merge reinforcement learning with the event camera and\nlaunch a demonstration of the trained agent. Along with the\nempirical result, our exploratory implementation will open\na new path for the vision-based automated robots with very\nlow latency.\narXiv:2004.00801v1  [cs.LG]  2 Apr 2020\nII. LITERATURE\nA. Event Camera Overview\nStandard cameras record scenes at ﬁxed time intervals\nand output a sequence of image frames. In contrast, an\nevent camera outputs a stream of asynchronous events at\nmicrosecond resolution, indicating when individual pixels\nrecord the log intensity to change over a preset threshold\nsize. While standard cameras have blind time-intervals and\nmay also send temporally redundant data if the captured\nscene does not change between frames, event cameras output\nchanges at more precise times with less redundancy [7].\nHence, event cameras offer the potential to overcome the\nlimitations of applications with standard cameras, such as\nlow frame rate, high latency, low dynamic range, and high\npower consumption. In fact, because of these promising fea-\ntures, this emerging camera has attracted attention from the\nindustry, and there have been several commercial products,\nsuch as asynchronous time-based image sensor (ATIS) [8],\ndynamic vision sensor (DVS) [9], and dynamic and active\npixel vision sensor (DAVIS) [6].\nIn the event camera, each pixel responds to changes in its\nphotocurrent L = log I (i.e., brightness). A stream of events\net = (t, x, y, p) is triggered at each pixel (x, y) at time t\nas soon as the intensity increases or decreases from the last\nevent in the pixel. Here, p is the polarity of events, indicating\nthe sign of the brightness change. In other words, p at time\nt and at pixel (x, y) can be written as\np =\n(\n1\n(L(x, y, t) −L(x, y, tprev)) = C)\n−1\n(L(x, y, t) −L(x, y, tprev)) = −C) ,\n(1)\nwhere C and tprev stand for a preset intensity threshold\n(positive number) and the time when the last event is\ntriggered, respectively.\nB. Related Work\nTo show the advantages of the event camera, we ﬁrst\ndiscuss its applications in the recent robotics community.\nThen, to situate the current work in the context of vision-\nbased reinforcement learning with this novel sensor, we\ncover previous vision-based reinforcement-learning studies\nin robotics and explain how leveraging the event camera can\npotentially speed up the control of autonomous robots.\n1) Event Camera Applications: Researchers have demon-\nstrated applications utilizing the advantages of the event\ncamera over standard cameras, such as low latency and\nhigh dynamic range. They include basic computer-vision\nalgorithms, such as object detection and tracking [10], and\napplied ones such as gesture recognition [11] and video\nreconstruction [12]\nRecently, studies have emerged in the robotics ﬁeld uti-\nlizing the event cameras. For example, Vidal et al. demon-\nstrated simultaneous localization and mapping (SLAM) on\nquadcopters [13], and Falanga et al. also examined the\ndifference in latency that affects high-speed control on drones\nbetween standard and event cameras [14]. Then, Dimitrova\net al. showed low-latency control of quadrotors using event\ncamera, enabling attitude tracking at speeds of over 1600◦/s\n[15]. Moreover, Delmerico et al. introduced large dataset\ntaken by the event cameras on fast-moving drones [16]. For\nfurther application examples and algorithms, see [7].\nAs these examples illustrate, the event camera can po-\ntentially beneﬁt the robotics community greatly, although\nstudies combining the event cameras and robotics are few,\nand are only the beginning of its contribution. Here, we\nanticipate our proposed approach can accelerate developing\nrobots with equipped event cameras for various cases through\nreinforcement learning.\n2) Vision-Based Reinforcement Learning in Robotics:\nMany studies that apply reinforcement learning in robotics\nuse camera images for their system’s observation, as images\ngenerally provide rich information about surrounding envi-\nronments. This method is called vision-based reinforcement\nlearning. For example, Asada et al. demonstrated a robot\nthat learned to shoot a ball into a goal using a standard\nTV camera attached to the robot [17]. With similar learning\nalgorithms, they also demonstrated a robot that could to learn\nto collaborate with other robots in soccer games [18]. In their\ncases, they classically encoded the image into several sub-\nstates by analyzing the object’s position in the image.\nThe recent development of deep neural networks has\nenabled us to handle high-dimensional data and thus, to ap-\nply reinforcement learning without such manually-encoding\nprocesses, i.e., end-to-end learning. Deep Q-network (DQN)\nsolved classic Atari 2600 games [19] and realized human-\nlevel control through deep reinforcement learning [20]. The\noutput from the simulator was high-dimensional data (210×\n160 video at 60 Hz with 128-color pallet), and was resized\ninto an 84×84-dimensional input image. This work triggered\na large number of studies on deep reinforcement learning and\nthe development of various techniques to improve learning\nprocesses. As a result, the learning efﬁciency has risen [21]\nand more complex tasks have become trainable, such as\ngenerating responses for conversational agents [22].\nHowever, applying vision-based deep reinforcement learn-\ning in robotics is not a simple task. As end-to-end rein-\nforcement learning requires numerous trials for agents to\nreach the optimal policy, we are restricted by the inability to\nhave robots perform actions over and over in the real world,\nwhich is too costly and involves safety problems [23]. The\nliterature has faced many trials to ﬁll in the gap between\nthe real and simulated environments. For example, Andrei\net al. proposed an effective method to utilize simulators for\ntraining models and then transfer them into real robots [24].\nTheir approach successfully demonstrated task learning from\nraw visual input on a fully actuated robot manipulator. To\ndate, many works have leveraged simulators to render target\nenvironments, train their models inside them, and transfer\nthem into robots [25]–[27].\nAlthough many applications have been proposed, almost\nall of them assume standard cameras as their input to\nreinforcement learning. At this point, we are interested in\nwhether replacing these cameras with the event cameras will\nalso result in the success of reinforcement learning and thus\nin the faster control of robots in the real world.\nIII. PROPOSED METHOD\nIn this section, we describe how we achieved the rein-\nforcement learning applications with event-based data input\nfor robots. First, we mention the general settings of rein-\nforcement learning for the following discussion foundation.\nSecond, we formulate an image-like feature for reinforce-\nment learning to emulate event data with comparison to\nother approaches that emulate a stream of events. Finally,\nwe explain the entire process of the learning, from deﬁning\na problem to launching a robot in the real world. The\nimplementation is open-sourced 1.\nA. Reinforcement Learning\nStandard reinforcement-learning settings consider an agent\nexploring a given environment (E) to achieve the desired\ntask. Through a sequence of observations, actions, and re-\nwards, the agent interacts with the environment and learns\nthe optimal policy. Formally, the set of possible observations\nand actions are deﬁned as S and A, respectively. The agent\nreceives an observation sn ∈S and a reward rn from\nE at each step index n, and then takes the next action\nan+1 ∈A based on its policy. At a given step index n and\nsn, the accumulated reward from the state can be written as\nRn = P∞\nk=0 γkrn+k, where γ is a discount factor for later\nrewards. The goal of reinforcement learning is to determine\nthe optimal policy that maximizes Rn at each step.\nB. Image-Like Feature of Events for Reinforcement Learning\nIn this section, we explain how we formulate and emulate\ninput features for reinforcement learning in a simulator, and\nalso how we convert an actual stream of events into the\nfeatures.\nSome prior studies proposed simulators for the event\ncamera. To obtain accurate event data, Mueggler et al. ren-\ndered images in a 3D environment at a ﬁxed high sampling\nfrequency [28]. Then, Rebecq et al. proposed an adaptive\nrendering to produce a stream of reliable event timestamp\ndata [29]. However, because we leverage reinforcement\nlearning setting and assume step-by-step formulation, it is\nnot necessary to produce a stream of events. Rather, if we\nobtain at least the accumulated event data between each\nstep, say n −1 and n, we can use it as an observation sn.\nHence, estimation of the event timestamp between frames,\nas proposed in past simulation methods, is not necessary in\nthe step-by-step reinforcement learning.\nTherefore, instead of emulating a stream of events, we\ntake the difference between two successive frames with a\ncertain threshold to create an image-like feature. This method\nis more computationally efﬁcient and is thus suitable for\nreinforcement learning simulator use, which usually requires\na large number of action steps. In fact, this approach is\nsimilar to what Kaiser et al. proposed [30], although they\ndid not apply reinforcement learning and mentioned it as\nfuture work.\n1https://github.com/EventVisionLibrary/momaku\nAlgorithm 1 Converting events into an image-like feature\nRequire: a stream of event (et = (t, x, y, p)), tn, W, H\nInitialize sn ←OH,W\nExtract a subset of events E ←{eτ|tn−1 ≤τ < tn}\nEnsure E is sorted by the timestamp in ascending order\nfor et ∈E do\nt, x, y, p ←et\nsn(x, y) ←p\nFig. 2: Overview of our approach. In the learning process, an agent\nreceives image-like feature sn and reward rn, and then takes\nthe next action an+1 in the simulator. In the deployment\nprocess, the learned agent can be deployed in a real-world\nrobot to which an event camera is connected.\nFormally, if we denote the timestamp for step index n as\ntn, we assume sn is the accumulated event data from tn−1\nto tn. The pixel (x, y) of sn can be written as\nsn(x, y) =\n\n\n\n\n\n1\n(L(x, y, tn) −L(x, y, tn−1)) ≥C)\n−1\n(L(x, y, tn) −L(x, y, tn−1)) ≤−C)\n0\n(otherwise)\n.\n(2)\nTherefore, the observation sn shall be a H ×W-dimensional\nfeature where W and H are the event camera’s width and\nheight, respectively. In the implementation of the simulator,\nwe use OpenGL2 to render an environment at each time tn\nto obtain L(x, y, tn).\nAt this point, the remaining problem is how to convert an\nactual event stream et into this format sn when we transfer\nthe agent to a real robot with an event camera attached. This\nis achieved by abandoning the timestamp information from\nthe event stream and processing it as an accumulated batch\nfeature during the time interval between tn−1 and tn. The\nprocedure is given by Algorithm 1.\nC. From Simulation to Real World\nFigure 2 illustrates the entire training and deployment\nprocess. First, we need to deﬁne an environment E where an\nagent will be trained along with observation space S, action\nspace A, and reward setting. We also need to conﬁgure the\ntime interval ∆t between each step. Thus, tn+1 = tn + ∆t\nholds. In standard vision-based reinforcement learning, the\n2https://www.opengl.org/\nsampling rate of cameras provides a restriction for action\nfrequency: ∆t ≥0.033 s for sampling at 30 Hz or ∆t ≥\n0.017 s for 60 Hz. In contrast, we can lower this limit greatly\nby utilizing the event camera, which we explain in Section\nIV.\nOnce above settings are conﬁgured, we employ a rein-\nforcement algorithm to train a simulated agent. An obser-\nvation from the environment is provided by the method we\nmentioned in the previous section. As we consider sn as\nan image-like feature, conventional reinforcement-learning\nalgorithms can be applied.\nFinally, the trained model can be transferred to a real robot\nwith the event camera attached. The interval between each\naction of the robot must be congruent with ∆t. The event\nstream et is converted to sn by the interval ∆t, and the\ntrained model is applied to a sequence of input sn.\nIV. EXPERIMENT: SIMULATOR TRAINING\nA. Problem settings\nWe assumed the agent was a small car robot, simulating\nGoPiGo3 (Dexter Industries, Inc.)3 equipped with DAVIS240\nas a vision sensor [6]. Hence, the simulated event camera had\n240 × 180 pixels, which was located at the top of the agent.\nWe trained the agent for two types of simpliﬁed experiments:\ncollision avoidance and object tracking. Each of these tasks\nhas been widely considered and explored as an important\napplication for robotics. For example, Michels et al. demon-\nstrated a fast obstacle-avoidance algorithm using standard\ncamera of 20 Hz [31]. However, even though the algorithm\nitself is fast, the control frequency was limited to a maximum\nof 20 Hz in their work. In contrast, using our proposed\nprocedure, the control frequency is no longer limited to the\ncamera frame rate theoretically. In our experiments, the step\nfrequency ∆t was 0.01 (100 Hz) for both tasks.\nB. Simulator Environment\nWe used various shapes of spheres and cubes as obstacles\nin the simulator. The agent had actions of going forward,\ngoing backward, stopping, steering to right and left, some\nof which were enabled for each experiment. To emulate the\nreal camera, impulse noise was randomly added with the\nprobability of occurrence at 0.001 independently for each\npixel, convoluted into the image-like feature.\nC. Tasks\n1) Collision Avoidance: For the collision-avoidance task,\nwe assumed a car running in a ﬁeld where spheres randomly\nfell from the sky. In each episode, one random sphere fell\nin front of the agent at a random moment, resulting in a\ncollision if the agent continued running. The agent had two\nactions of {forward, stop}. Reward was deﬁned as rn =\n−d2\nn/10.0 for the Euclidean distance dn between the agent\nand the sphere falling in front of the agent. Extra rewards\nwere added by 0.2 if the agent took the “forward” action. If\nthe agent collided with the sphere, the reward was −50. The\n3https://www.dexterindustries.com/gopigo3/\nepisode was done and reset when the agent collided with the\nsphere or acted with the maximum number of steps (100).\n2) Tracking: As the tracking task, one sphere was thrown\nfrom an arbitrary point on a ﬁeld, drawing parabola curve\nfollowing the gravity. The agent’s task was to follow the\nsphere by taking an action from three actions of {forward,\nright, left}. Reward was set as rn = 10(1 −|θn|), where θn\nwas the angle between the direction of the agent and that\nof the sphere seen from the agent. Each episode was done\nand reset when the agent collided with the sphere or acted\nmaximum number of steps (100).\nD. Reinforcement Learning Algorithm\nAs one of the popular reinforcement learning algorithms,\nDouble DQN with convolutional neural network was used to\nlearn each task [32]. The agent followed ϵ-greedy exploration\nwith ϵ = 0.1. For optimization, Adam with ϵ = 0.01 was\nused [33]. Replay buffer of Double DQN was 106 with γ =\n0.95, and the target network update interval was 200 steps.\nThe neural network consisted of two convolutional layers\nand two fully connected layers, each of which followed by\nbatch normalization and ReLU layers [34]. The kernel size\nof the convolutional layers was three, their output channel\nsizes were two and four, and the number of dimensions of\nthe fully connected layers was 100. The output dimension of\nthe neural network was the number of actions for each task.\nTo train the neural network model, it took about one hour\non GPU (GeForce GTX 1080, NVIDIA, Corp.).\nV. RESULT AND DEMONSTRATION\nWe ﬁrst show the simulation results for the two exper-\niments. Next, we provide demonstration for the collision\navoidance task on the real-world robot, a GoPiGo3 car\nequipped with the event camera.\nA. Simulation result\n1) Collision avoidance: The sum of rewards, Rsum =\nP100\nk=0 rk, over each evaluation episode is shown in Fig. 3.\nThe result showed a conﬁdence interval of nine experimental\ntrials with different random seeds. The sum of rewards\nincreased along episodes, indicating that the deep neural\nnetwork model successfully learned to avoid objects (stop)\nin front of the agent.\n2) Tracking: The sum of rewards for the tracking ex-\nperiment over each evaluation episode is shown in Fig. 4.\nThe result showed conﬁdence interval with nine experimental\ntrials with different random seeds. Similar to the avoidance\nexperiment, it increased along episodes, showing that the\nneural network successfully learned how to track object and\ncontrol itself toward the object in front.\nB. Demonstration\nAfter it was trained in the simulator for the collision-\navoidance task, the agent was transferred and deployed into\nRaspberryPi 3 on GoPiGo3. The setup image of the robot and\nthe image-like features captured by DAVIS240 are shown in\nFig. 5. We used DAVIS240 as a sensor on the robot [6]. To\nFig. 3: Sum of rewards in each episode for collision-avoidance\ntask. In each episode, the agent took at most 100 actions\nat 100 Hz and received rewards for each action. The mean\nand standard error over nine trials are described.\nFig. 4: Sum of rewards in each episode for tracking task. In each\nepisode, the agent took at most 100 actions, and received\nrewards for each action. The mean and standard error over\nnine trials are described.\ncapture and handle the streaming data from the camera, we\nused the libraries of libcaer4 and EventVisionLibrary5. The\ntrained model ran on a server computer (MacBook Pro 2018,\nApple Inc.), connected through WebSocket API and returning\nthe inferred action to the robot. The execution time of the\ninference for a single step was about 9 ms on the server.\nThe demonstration is shown in the video material 6.\nAs a result, the car agent succeeded in stopping when an\nobject was thrown in front of it suddenly. It was demonstrated\nthat our approach is effective to transfer and deploy the agent\ninto real-world robots.\nVI. DISCUSSION AND CONCLUSION\nWe have presented the ﬁrst application for reinforcement\nlearning with the event camera. After training in a simu-\nlated environment, the robot was able to perform desired\nmovements in the real world, such as avoiding collisions\nand tracking an object. Our robot was controlled faster at\n100 Hz, compared to 30 or 60 Hz that are typical frequencies\nof standard cameras. We believe that our approach opens a\n4https://gitlab.com/inivation/libcaer\n5https://github.com/EventVisionLibrary/evl\n6https://youtu.be/xc2nKJ1TLTY\nFig. 5: Captures of the demonstration. GoPiGo3 equipped with\nDAVIS240 on it (lower images), and the reinforcement\nlearning agent inferred the action from the image-like\nfeatures (upper images). When an object was thrown in\nfront of the agent, the car successfully stopped. For the\ndetail, see the attached video.\nnew possibility for fast and autonomous robots utilizing event\ncameras.\nThere remain some issues to be addressed to expand our\nresult. First, as we processed image-like features with high\nfrequency and did not account for the timestamp information\nof events, future work shall integrate that information or\nprocess the stream of events asynchronously, which would\nprovide more accurate information about the surroundings,\nthus making the control more accurate. Second, we used\nWebSocket API to calculate the neural network inference\non the server in our demonstration, since the robot car we\nused did not have enough computational resources. Hence,\nfor the next step, on-board inference without any network\nconnection will be desired to implement to take the full\nadvantage of the event camera. In addition, our simulator and\ntasks were simple to demonstrate the feasibility of combin-\ning reinforcement learning with event camera for achieving\nfaster control. Still, the reinforcement-learning agent will be\ndesired to handle more complex environment in order to\ntackle the real-world problems such as autonomous vehicles\nand drones today. Therefore, it is expected to design more\nvarious simulation environments suitable for desired tasks\nand investigate the generalization ability of the agent.\nACKNOWLEDGMENTS\nThe authors would like to thank Hidenobu Matsuki for\nhis support and guidance. This work was partially supported\nby MITOU Advanced funding program by the Ministry of\nEconomics, Trade and Industry in Japan.\nREFERENCES\n[1] M. A. K. Bahrin, et al., “Industry 4.0: A review on industrial\nautomation and robotic,” Jurnal Teknologi, vol. 78, no. 6-13, pp. 137–\n143, 2016.\n[2] M. Wollschlaeger, et al., “The future of industrial communication:\nAutomation networks in the era of the internet of things and industry\n4.0,” IEEE industrial electronics magazine, vol. 11, no. 1, pp. 17–27,\n2017.\n[3] V. Gullapalli, et al., “Acquiring robot skills via reinforcement learn-\ning,” IEEE Control Systems Magazine, vol. 14, no. 1, pp. 13–24, 1994.\n[4] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE\nTransactions on knowledge and data engineering, vol. 22, no. 10,\npp. 1345–1359, 2009.\n[5] C. Posch, et al., “Retinomorphic event-based vision sensors: bioin-\nspired cameras with spiking output,” Proceedings of the IEEE, vol.\n102, no. 10, pp. 1470–1484, 2014.\n[6] C. Brandli, et al., “A 240×180 130 db 3 µs latency global shutter\nspatiotemporal vision sensor,” IEEE Journal of Solid-State Circuits,\nvol. 49, no. 10, pp. 2333–2341, 2014.\n[7] G. Gallego, et al., “Event-based vision: A survey,” 2019.\n[8] C. Posch, et al., “A qvga 143 db dynamic range frame-free pwm\nimage sensor with lossless pixel-level video compression and time-\ndomain cds,” IEEE Journal of Solid-State Circuits, vol. 46, no. 1, pp.\n259–275, 2010.\n[9] P. Lichtsteiner, et al., “A 128×128 120 db 15 µs latency asynchronous\ntemporal contrast vision sensor,” IEEE journal of solid-state circuits,\nvol. 43, no. 2, pp. 566–576, 2008.\n[10] A. Glover and C. Bartolozzi, “Event-driven ball detection and gaze\nﬁxation in clutter,” in 2016 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS).\nIEEE, 2016, pp. 2203–2208.\n[11] A. Amir, et al., “A low power, fully event-based gesture recognition\nsystem,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2017, pp. 7243–7252.\n[12] H. Rebecq, et al., “Events-to-video: Bringing modern computer vision\nto event cameras,” 2019.\n[13] A. R. Vidal, et al., “Ultimate slam? combining events, images, and\nimu for robust visual slam in hdr and high-speed scenarios,” IEEE\nRobotics and Automation Letters, vol. 3, no. 2, p. 994–1001, Apr 2018.\n[Online]. Available: http://dx.doi.org/10.1109/LRA.2018.2793357\n[14] D. Falanga, et al., “How fast is too fast? the role of perception\nlatency in high-speed sense and avoid,” IEEE Robotics and Automation\nLetters, vol. 4, no. 2, pp. 1884–1891, 2019.\n[15] R. S. Dimitrova, et al., “Towards low-latency high-bandwidth control\nof quadrotors using event cameras,” arXiv preprint arXiv:1911.04553,\n2019.\n[16] J. Delmerico, et al., “Are we ready for autonomous drone racing? the\nuzhfpv drone racing dataset,” in IEEE Int. Conf. Robot. Autom.(ICRA),\n2019.\n[17] M. Asada, et al., “Purposive behavior acquisition for a real robot by\nvision-based reinforcement learning,” Machine learning, vol. 23, no.\n2-3, pp. 279–303, 1996.\n[18] ——, “Cooperative behavior acquisition for mobile robots in dynam-\nically changing real worlds via vision-based reinforcement learning\nand development,” Artiﬁcial Intelligence, vol. 110, no. 2, pp. 275–\n292, 1999.\n[19] M. G. Bellemare, et al., “The arcade learning environment: An eval-\nuation platform for general agents,” Journal of Artiﬁcial Intelligence\nResearch, vol. 47, pp. 253–279, 2013.\n[20] V. Mnih, et al., “Human-level control through deep reinforcement\nlearning,” Nature, vol. 518, no. 7540, p. 529, 2015.\n[21] M. Hessel, et al., “Rainbow: Combining improvements in deep rein-\nforcement learning,” in Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, 2018.\n[22] J. Li, et al., “Deep reinforcement learning for dialogue generation,”\narXiv preprint arXiv:1606.01541, 2016.\n[23] S. Gu, et al., “Deep reinforcement learning for robotic manipulation\nwith asynchronous off-policy updates,” in 2017 IEEE international\nconference on robotics and automation (ICRA).\nIEEE, 2017, pp.\n3389–3396.\n[24] A. A. Rusu, et al., “Sim-to-real robot learning from pixels with\nprogressive nets,” arXiv preprint arXiv:1610.04286, 2016.\n[25] G. Kahn, et al., “Self-supervised deep reinforcement learning with\ngeneralized computation graphs for robot navigation,” in 2018 IEEE\nInternational Conference on Robotics and Automation (ICRA). IEEE,\n2018, pp. 1–8.\n[26] J. Zhang, et al., “Deep reinforcement learning with successor features\nfor navigation across similar environments,” in 2017 IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Systems (IROS). IEEE,\n2017, pp. 2371–2378.\n[27] S. James and E. Johns, “3d simulation for robot arm control with deep\nq-learning,” arXiv preprint arXiv:1609.03759, 2016.\n[28] E. Mueggler, et al., “The event-camera dataset and simulator: Event-\nbased data for pose estimation, visual odometry, and slam,” The\nInternational Journal of Robotics Research, vol. 36, no. 2, pp. 142–\n149, 2017.\n[29] H. Rebecq, et al., “ESIM: an open event camera simulator,” Conf. on\nRobotics Learning (CoRL), Oct. 2018.\n[30] J. Kaiser, et al., “Towards a framework for end-to-end control of\na simulated vehicle with spiking neural networks,” in 2016 IEEE\nInternational Conference on Simulation, Modeling, and Programming\nfor Autonomous Robots (SIMPAR).\nIEEE, 2016, pp. 127–134.\n[31] J. Michels, et al., “High speed obstacle avoidance using monocular\nvision and reinforcement learning,” in Proceedings of the 22nd inter-\nnational conference on Machine learning.\nACM, 2005, pp. 593–600.\n[32] H. van Hasselt, et al., “Deep reinforcement learning with double q-\nlearning,” 2015.\n[33] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” 2014.\n[34] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” 2015.\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO"
  ],
  "published": "2020-04-02",
  "updated": "2020-04-02"
}