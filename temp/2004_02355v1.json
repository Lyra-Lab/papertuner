{
  "id": "http://arxiv.org/abs/2004.02355v1",
  "title": "Deep Multilayer Perceptrons for Dimensional Speech Emotion Recognition",
  "authors": [
    "Bagus Tris Atmaja",
    "Masato Akagi"
  ],
  "abstract": "Modern deep learning architectures are ordinarily performed on\nhigh-performance computing facilities due to the large size of the input\nfeatures and complexity of its model. This paper proposes traditional\nmultilayer perceptrons (MLP) with deep layers and small input size to tackle\nthat computation requirement limitation. The result shows that our proposed\ndeep MLP outperformed modern deep learning architectures, i.e., LSTM and CNN,\non the same number of layers and value of parameters. The deep MLP exhibited\nthe highest performance on both speaker-dependent and speaker-independent\nscenarios on IEMOCAP and MSP-IMPROV corpus.",
  "text": "Deep Multilayer Perceptrons for Dimensional\nSpeech Emotion Recognition\nBagus Tris Atmaja\nSchool of Information Science\nJapan Advanced Institute of Science and Technology\nNomi, Japan\nbagus@jaist.ac.jp\nMasato Akagi\nSchool of Information Science\nJapan Advanced Institute of Science and Technology\nNomi, Japan\nakagi@jaist.ac.jp\nAbstract—Modern deep learning architectures are ordinarily\nperformed on high-performance computing facilities due to the\nlarge size of the input features and complexity of its model.\nThis paper proposes traditional multilayer perceptrons (MLP)\nwith deep layers and small input size to tackle that computation\nrequirement limitation. The result shows that our proposed deep\nMLP outperformed modern deep learning architectures, i.e.,\nLSTM and CNN, on the same number of layers and value of\nparameters. The deep MLP exhibited the highest performance\non both speaker-dependent and speaker-independent scenarios\non IEMOCAP and MSP-IMPROV corpus.\nIndex Terms—Affective computing, emotion recognition, mul-\ntilayer perceptrons, neural networks, speech analysis\nI. INTRODUCTION\nSpeech emotion recognition is currently gaining interest\nfrom both academia and commercial industries. Researchers\nin the affective computing ﬁeld progressively proposed new\nmethods to improve the accuracy of automatic emotion recog-\nnition. Commercial industries are trying to make this technol-\nogy available to the market since its potential applications.\nPreviously, researchers has attempted to implement speech-\nbased emotion recognition for wellbeing detection [1], call\ncenter application [2], and automotive safety [3].\nOne of the common requirements in computing speech\nemotion recognition is the availability of high-performance\ncomputing since the dataset usually is very large in size, and\nthe classifying methods are complicated. Graphical processing\nunits (GPU)-based computers are often used over CPU-based\ncomputers due to its processing speed to process the data,\nparticularly, image-like data.\nThis paper proposes the use of deep multilayer perceptrons\n(MLP) to overcome the requirement of high computing power\nrequired by modern deep learning architectures. The inputs are\nhigh-level statistical functions (HSF), which are used to reduce\nthe dimension of input features. The outputs are emotion\ndimensions, i.e., degree of valence, arousal, and dominance.\nAccording to research in psychology, dimensional emotion\nis another view in emotion theories apart from categorical\nemotion. Russel [4] argued that emotion categories could be\nderived from this dimensional emotion, particularly in valence-\narousal space. Given the beneﬁt of the ability to convert\ndimensional emotion to categorical emotion, but not vice\nversa, predicting the emotion dimension is more beneﬁcial\nthan predicting the emotion category. We added dominance,\nsince it is suggested in [5], and the availability of those labels\nin the datasets. Dimensional emotion recognition are evaluated\nwith deep MLP from speech data since the target applications\nare speech-based technology like call center and voice assistant\napplications.\nThe contribution of this paper is the evaluation of the\nclassical MLP technique with deep layers compared with\nmodern deep learning techniques, i.e., LSTM and CNN, in\nterms of concordance correlation coefﬁcient (CCC). The deep\nneural network is an extension of the neural network with\ndeeper layers, commonly ﬁve or more layers [6]. Our results\nshow that on both speaker-dependent and speaker-independent,\nin three datasets, deep MLP obtained higher performances than\nLSTM and CNN. The proposed method worked effectively on\nthe small size of feature, in which, this may be a limitation of\nour proposed deep MLP method.\nII. DATA AND FEATURE SETS\nIn this section, we describe data and feature sets used in\nthis research.\na) IEMOCAP: The interactive emotional dyadic motion\ncapture database is used in this research [7]. Although the\ndatabase consists of the measurement of speech, facial ex-\npression, head, and movements of affective dyadic sessions,\nonly speech data are processed. The database contains ap-\nproximately 12 h of data with 10039 utterances. All of those\ndata are used. The dimensional emotion labels are given in\nvalence, arousal, and dominance, in range [1-5] score and\nnormalized into [-1, 1] as in [8] when those labels are fed\ninto the neural network system. For speech data, two versions\nare available in the dataset, stereo data per dialog and mono\ndata per sentence (utterance). We used the mono version since\nit is easy to process with the labels. The sampling rate of the\ndata was 16 kHz and 16-bit PCM.\nWe arranged the IEMOCAP dataset into two scenarios,\nspeaker-dependent (SD) and speaker-independent. On speaker-\nindependent, we split the dataset with ratio 80/20 for train-\ning/test set, while in speaker-independent, the last session,\ni.e., session ﬁve, is left for the test set (leave one session\nout, LOSO). The ratio of dataset partition in the speaker-\narXiv:2004.02355v1  [eess.AS]  6 Apr 2020\nindependent scenario is similar to speaker-dependent split\nwhich is shown in Table I.\nb) MSP-IMPROV: We used the MSP-IMPROV corpus\nto generalize the impact of the evaluated methods. MSP-\nIMPROV is an acted corpus of dyadic interactions to study\nemotion perception. This dataset consists of speech and visual\nrecording of 18 hours of affective dyadic sessions. Same as\nIEMOCAP dataset, we only used the speech data, with 8438\nutterances. The same split ratio is used in speaker-dependent\nscenario while the last session six is used for test set in in\nspeaker-independent scenario, with the same labels scale and\nnormalization. While IEMOCAP labels are annotated by at\nleast two subjects, these MSP-IMPROV labels are annotated\nby at least ﬁve annotators. The speech data were mono, 44\nkHz, and 16-bit PCM.\nTable I shows the number of utterances allocated for each set\npartition for both speaker-dependent and speaker-independent,\nincluding MSP-IMPROV dataset.\nc) Mixed-corpus: In addition to the two datasets above,\nwe mixed those two datasets to create a new category\nof dataset namely mixed-corpus. In mixed-corpus, we con-\ncatenated speaker-dependent from IEMOCAP with speaker-\ndependent from MSP-IMPROV for each, training, develop-\nment and test sets. The same rules also applied for the speaker-\nindependent scenario.\nTABLE I\nNUMBER OF UTTERANCES USED IN THE DATASETS ON EACH PARTITION\nScenarios\nTraining\nDevelopment\nTest\nIEMOCAP-SD\n6431\n1608\n2000\nIEMOCAP-LOSO\n6295\n1574\n2170\nIMPROV-SD\n5256\n1314\n1868\nIMPROV-LOSO\n5452\n1364\n1622\nd) Acoustic Feature Set: We used high statistical func-\ntions of the low-level descriptor (LLD) from Geneva Minimal-\nistic Acoustic Parameter Set (GeMAPS), which is developed\nby Eyben et al. [9]. The HSF features are extracted per\nutterance depend on the given labels, while the LLDs are\nprocessed on a frame-based level with 25 ms window size and\n10 ms of hop size. The use of HSF feature reduce computation\ncomplexity since the feature size decrease from (3409 × 23)\nto (1 × 23 features), that is, for IEMOCAP dataset. To obtain\nthe HSF feature, however, LLDs must obtained ﬁrst. Then,\nHSF can be calculated as statistics of those LLDs for a ﬁxed\ntime, in this case, per utterance.\nTo add those functionals, we used a silence feature, which\nis also extracted per utterance. Silence feature, in this paper,\nis deﬁned as the portion of the silence frames compared to the\ntotal frames in an utterance. In human-human communication,\nthis portion of silence in speaking depends on the speaker’s\nemotion. For example, high arousal emotion category like\nhappy may have fewer silences (or pauses) than a sad emotion\ncategory. The ratio of silence per utterance is calculated as\nS = Ns\nNt\n,\n(1)\nTABLE II\nGEMAPS FEATURE [9] AND ITS FUNCTIONALS; ONLY FUNCTIONALS\n(HSFS) USED IN THIS DIMENSIONAL SER.\nLLDs\nloudness, alpha ratio, hammarberg index, spectral slope\n0-500 Hz, spectral slope 500-1500 Hz, spectral ﬂux, 4\nMFCCs, F0, jitter, shimmer, Harmonics-to-Noise Ratio\n(HNR), Harmonic difference H1-H2, Harmonic differ-\nence H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2\namplitude, F3, and F3 amplitude.\nHSFs\nmean (of LLDs), standard deviation (of LLDs), silence\nwhere Ns is the number of frames to be categorized as silence\n(silence frames), and Nt is the number of total frames within\nan utterance. To be categorized as silence, a frame is checked\nwhether it is less than a threshold, which is a multiplication\nof a factor with a root mean square (RMS) energy (Xrms).\nThis RMS energy is formulated as\nth = 0.3 × Xrms\n(2)\nand Xrms is deﬁned as\nXrms =\nv\nu\nu\nt 1\nn\nn\nX\ni=1\nx[i]2\n(3)\nwhere silence factor of 0.3 is obtained from experiments.\nThese equations are similar to what is proposed in [10] and\n[11]. In [10], the author of that paper used a ﬁxed threshold,\nwhile we evaluated some factors of to ﬁnd the best factor\nfor silence feature in speech emotion recognition. In [11], the\nauthor divided the total duration of disﬂuency over the total\nutterance length on n words and counted it as a disﬂuency\nfeature.\nIII. BENCHMARKED AND PROPOSED METHOD\nWe evaluated three different methods in this paper, LSTM,\nCNN and MLP. LSTM and CNN are used as baselines, while\nMLP with deep layers is the proposed method. All evaluated\nmethods used the same numbers of layers, units and value of\nparameters.\nA. Benchmarked Methods: LSTM and CNN\nLSTM and CNN are two common deep learning architec-\ntures widely used in speech emotion recognition [12]–[14]. We\nused those two architectures as the baselines due to its reported\neffectiveness on predicting valence, arousal, and dominance.\nBoth LSTM and CNN evaluated here have the same ﬁve layers\nwith the same number of units. For the size of the kernel in\nCNN architecture, we determined it in order that the number\nof trainable parameters is similar. The other parameters, like\nbatch size, feature and label standardization, loss function,\nnumber of iterations, and callback criteria, are same for both\narchitectures.\nFig. 1 shows both structures of LSTM and CNN. On the\nﬁrst layer, 256 neurons are used and decreased half for the next\nlayers since the models are supposed to learn better along with\nthose layers. Five LSTM layers used tanh as activation func-\ntion, while ﬁve CNN layers used ReLU activation function. We\nkept all output from the last LTM layer and ﬂatten it before\nsplitting into three dense layers for obtaining the prediction of\nvalence, arousal, and dominance. For the CNN architecture,\nthe same ﬂatten layer is used before entering three one-unit\ndense layers.\nBoth architectures used the same standardization for input\nfeatures and labels. The z-score normalization is used to\nstandardize feature, while minmax scaler is used to scale\nthe labels into [0, 1] range. We used CCC [15] loss as\nthe cost function with multitask (MTL) approach in which\nthe prediction of valence, arousal, and dominance are done\nsimultaneously. While CCC loss (CCCL) is used as the cost\nfunction, the following CCC is used to evaluate the perfor-\nmance of recognition.\nCCC =\n2ρσxσy\nσ2x + σ2y + (µx −µy)2\n(4)\nCCCL = 1 −CCC\n(5)\nwhere ρ is Pearson’s correlation between gold standard y and\nand predicted score x, σ is standard deviation, and µ is the\nmean score. The total loss (LT ) for three variables is then\ndeﬁned as the sum of CCCL for those three with corresponding\nweighting factors,\nLT = αCCCLV + βCCCLA + (1 −α −β)CCCLD\n(6)\nwhere α and β are weighting factors for loss function of\nvalence (CCCLV ) and arousal (CCCLA), respectively. The\nweighting factors of loss function for dominance (CCCLD)\nis obtained by subtracting 1 by the sum of those weighting\nfactors. The CCC is used to evaluate all architectures including\nthe proposed deep MLP method.\nAll architectures used a mini-batch size of 200 utterances\nby shufﬂing its orders, maximum number iteration of 180, and\n10 iterations patience of early stopping criteria (callback). An\nadam optimizer [16] is used to adjust the learning rate during\nthe training process. Both architectures run on GPU-based\nmachine using CuDNN implementation [17] within Keras\ntoolkit [18].\nB. Proposed Method: Deep MLP\nFig. 2 shows our proposed deep MLP structure. The MLP\nused here similar to the deﬁnition of connectionist learning\nproposed by Hinton [19]. As the benchmarked methods, deep\nMLP also has ﬁve layers with the same number of units as\nprevious. The only difference of the layer structure from the\nprevious is the absent of ﬂatten layer since the output of\nthe last MLP layers can be coupled directly to three one-\nunit dense layers. While the previous LSTM and CNN used\nbatch normalization layer in the beginning (input) to speed-up\ncomputation process, this deep MLP structure did not use that\nlayer since the implementation only need a minute to run on\na CPU-based machine.\nWe used the same batch size, tolerance for early stopping\ncriteria, optimizer, and maximum number of iteration as the\nLSTM (256)\nLSTM (128)\nLSTM (64)\nFlatten()\nLSTM Model\n#: 575,505\nLSTM (32)\nLSTM (16)\nCNN (256,3)\nCNN (128,12)\nCNN (64,12)\nFlatten()\nCNN Model\n#:542,181\nCNN (32,12)\nCNN (16,12)\nFig. 1. Diagram of LSTM and CNN used for benchmarking of proposed deep\nMLP; the number inside the bracket denoted the number of units (nodes); the\nsecond number on CNN denotes kernel size, # denotes number of trainable\nparameters.\nbenchmarked methods. While the benchmarked methods used\nCCC the as loss function, the proposed deep MLP method\nused a mean square error (MSE) as the cost function,\nMSE = 1\nn\nn\nX\ni=1\n(yi −xi)2.\n(7)\nThe total loss function is given as an average of MSE scores\nfrom valence, arousal, and dominance,\nMSET = 1\n3(MSEV + MSEA + MSED).\n(8)\nThere are no weighting factors used here since we do not\nﬁnd a way to implement it via scikit-learn toolkit [20], in\nwhich the proposed deep MLP is implemented. The same\nreason applied for the selection of MSE over CCC for the\nloss function. The Python implementation codes for both\nproposed and benchmarked methods are available at https:\n//github.com/bagustris/deep mlp ser.\nIV. EXPERIMENT RESULTS AND DISCUSSION\nCCC is the standard metric used in affective computing to\nmeasure the performance of dimensional emotion recognition.\nWe presented our results in that metric in two different groups;\nwithin-corpus and mixed-corpus evaluation. The results are\nshown in Table III and IV.\nTable III shows CCC scores of valence (V), arousal (A),\ndominance (D) and its average from different datasets, sce-\nnarios, and methods. The proposed deep MLP method out-\nperforms benchmarked methods by remarkable margins. On\nevery emotion dimensions and averaged score, the proposed\ndeep MLP gained the highest CCC score for both speaker-\ndependent and speaker-independent scenarios (typed in bold).\nOn IEMOCAP dataset, the score of speaker-dependent is only\nslightly higher than speaker-independent due to the nature\nof dataset structure. The utterances in IEMOCAP dataset is\nalready in order by its session when it is sorted by ﬁle names.\nMLP (128)\nMLP (32)\nOutput(1)\nOutput(1)\nOutput(1)\nMLP (16)\nV\nA\nD\nMLP (256)\nMLP (64)\nINPUT (47)\nmean, \nstd, \nsilence\nFig. 2. Diagram of proposed deep MLP with ﬁve layers; the number inside\nthe bracket denoted the number of units.\nThe change from speaker-dependent to speaker-independent\nis done by changing the number of train/test partitions. In\ncontrast, the ﬁle naming of utterances in MSP-IMPROV made\nthe arrangement of the sessions not in order when utterances\nare sorted by its ﬁle names. We did the sorting process\nto assure the pair of features and labels. The gap between\nspeaker-dependent and speaker-independent in MSP-IMPROV\nis larger than in IEMOCAP which may be caused by those\ndifferent ﬁles organization. A case where our deep MLP\nmethod gained a lower score is on dominance part of MSP-\nIMPROV speaker-dependent scenario, however, the averaged\nCCC score, in that case, is still the highest.\nTABLE III\nRESULTS OF CCC SCORES ON WITHIN-CORPUS EVALUATION\nDataset\nMethod\nV\nA\nD\nMean\nIEMOCAP\nspeaker-dependent\nLSTM\n0.222\n0.508\n0.432\n0.387\nCNN\n0.086\n0.433\n0.401\n0.307\nMLP\n0.335\n0.599\n0.473\n0.469\nspeaker-independent (LOSO)\nLSTM\n0.210\n0.474\n0.394\n0.359\nCNN\n0.113\n0.460\n0.410\n0.328\nMLP\n0.316\n0.488\n0.454\n0.453\nMSP-IMPROV\nspeaker-dependent\nLSTM\n0.392\n0.629\n0.524\n0.515\nCNN\n0.346\n0.623\n0.522\n0.497\nMLP\n0.438\n0.650\n0.519\n0.536\nspeaker-independent (LOSO)\nLSTM\n0.210\n0.483\n0.313\n0.335\nCNN\n0.216\n0.524\n0.387\n0.375\nMLP\n0.269\n0.551\n0.401\n0.407\nTable IV shows the results from the mixed-corpus dataset.\nThis corpus is concatenation of IEMOCAP with MSP-\nIMPROV as listed in Table I, for both speaker-dependent\nand speaker-independent scenarios. In this mixed-corpus, the\nproposed deep MLP method also outperformed LSTM and\nCNN in all emotion dimensions and averaged CCC scores.\nThe score on speaker-dependent in that mixed-corpus is in be-\ntween the score of speaker-dependent in IEMOCAP and MSP-\nIMPROV within-corpus. For speaker-independent, the score\nis lower than in within-corpus. This low score suggested that\nspeaker variability (in different sessions) affected the result,\neven with the z-normalization process. Instead of predicting\none different session (LOSO), the test set in the mixed-corpus\nconsists of two different sessions, each from IEMOCAP and\nMSP-IMPROV, which made regression task more difﬁcult.\nWe showed that our proposed deep MLP functioned to over-\ncome the requirement of modern neural network architectures\nsince it surpassed the results obtained by those architectures.\nUsing a small dimension of feature size, i.e., 47-dimensional\ndata, our deep MLP with ﬁve layers, excluding input and\noutput layers, achieved the highest performance. Modern deep\nlearning architectures require high computation hardware, e.g.,\nGPU card, which costs expensive. We showed that using a\nsmall deep MLP architecture, which does not require high\ncomputation load, better performance can be achieved. Our\nproposed deep MLP method gained a higher performance than\nbenchmarked methods not only on both within-corpus and\nmixed-corpus but also on both speaker-dependent and speaker-\nindependent scenarios. Although the proposed method used the\ndifferent loss function from the benchmarked methods, i.e.,\nMSE versus CCC loss, we presumed that our proposed deep\nMLP will achieve higher performance if it used the CCC loss\nsince the evaluation metric is CCC.\nTABLE IV\nRESULTS OF CCC SCORES ON MIXED-CORPUS EVALUATION\nMethod\nV\nA\nD\nMean\nspeaker-dependent\nLSTM\n0.262\n0.518\n0.424\n0.401\nCNN\n0.198\n0.494\n0.424\n0.372\nMLP\n0.395\n0.640\n0.461\n0.499\nspeaker-independent (LOSO)\nLSTM\n0.118\n0.270\n0.242\n0.210\nCNN\n0.073\n0.265\n0.249\n0.196\nMLP\n0.212\n0.402\n0.269\n0.294\nV. CONCLUSIONS\nThis paper demonstrated that the use of deep MLP with\nproper parameter choices outperformed the more modern\nneural network architectures with the same number of layers.\nFor both speaker-dependent and speaker-independent, the pro-\nposed deep MLP gained the consistent highest performance\namong evaluated methods. The proposed deep MLP also\ngained the highest score on both within-corpus and mixed-\ncorpus scenarios. Based on the results of these investigations,\nthere is no high requirements on computing power to obtain\noutstanding results on dimensional speech emotion recog-\nnition. The proper choice of feature (i.e., using small size\nfeature) and the classiﬁer can leverage the performance of\nconventional neural networks. Future research can be directed\nto investigate the performance of the proposed method on\ncross-corpus evaluations, which is not evaluated in this paper.\nREFERENCES\n[1] Y. Gao, Z. Pan, H. Wang, and G. Chen, “Alexa, My Love:\nAnalyzing Reviews of Amazon Echo,” in 2018 IEEE SmartWorld,\nUbiquitous Intell. Comput. Adv. Trust. Comput. Scalable Comput.\nCommun. Cloud Big Data Comput. Internet People Smart City\nInnov.\nIEEE, oct 2018, pp. 372–380. [Online]. Available: https:\n//ieeexplore.ieee.org/document/8560072/\n[2] V. A. Petrushin, “Emotion In Speech: Recognition And Application To\nCall Centers,” Proc. Artif. neural networks Eng., vol. 710, pp. 22–30,\n1999.\n[3] C. Nass, I. M. Jonsson, H. Harris, B. Reaves, J. Endo, S. Brave, and\nL. Takayama, “Improving automotive safety by pairing driver emotion\nand car voice emotion,” in Conf. Hum. Factors Comput. Syst. - Proc.,\n2005.\n[4] J. A. Russell, “Affective space is bipolar,” J. Pers. Soc. Psychol., 1979.\n[5] I. Bakker, T. van der Voordt, P. Vink, and J. de Boon, “Pleasure, Arousal,\nDominance: Mehrabian and Russell revisited,” Curr. Psychol., vol. 33,\nno. 3, pp. 405–421, 2014.\n[6] B. T. Atmaja, D. Ariﬁanto, and M. Akagi, “Speech recognition on\nIndonesian language by using time delay neural network,” ASJ Spring\nMeet., pp. 1291–1294, 2019.\n[7] C. Busso, M. Bulut, C.-C. C. Lee, A. Kazemzadeh, E. Mower, S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: Interactive\nemotional dyadic motion capture database,” Lang. Resour. Eval., vol. 42,\nno. 4, pp. 335–359, 2008.\n[8] S. Parthasarathy and C. Busso, “Jointly Predicting Arousal, Valence and\nDominance with Multi-Task Learning,” in Interspeech, 2017, pp. 1103–\n1107.\n[9] F. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andre, C. Busso,\nL. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan, and K. P. Truong,\n“The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice\nResearch and Affective Computing,” IEEE Trans. Affect. Comput., vol. 7,\nno. 2, pp. 190–202, apr 2016.\n[10] G. Sahu and D. R. Cheriton, “Multimodal Speech Emotion Recognition\nand Ambiguity Resolution,” Tech. Rep. [Online]. Available: http:\n//tinyurl.com/y55dlc3m\n[11] J. D. Moore, L. Tian, and C. Lai, “Word-level emotion recognition using\nhigh-level features,” in Int. Conf. Intell. Text Process. Comput. Linguist.\nSpringer, 2014, pp. 17–31.\n[12] Y. Xie, R. Liang, Z. Liang, and L. Zhao, “Attention-based dense LSTM\nfor speech emotion recognition,” IEICE Trans. Inf. Syst., vol. E102D,\nno. 7, pp. 1426–1429, 2019.\n[13] M. Schmitt, N. Cummins, and B. W. Schuller, “Continuous Emotion\nRecognition in Speech Do We Need Recurrence?” in Interspeech 2019.\nISCA: ISCA, sep 2019, pp. 2808–2812.\n[14] B. T. Atmaja and M. Akagi, “Speech Emotion Recognition Based on\nSpeech Segment Using LSTM with Attention Model,” in 2019 IEEE\nInt. Conf. Signals Syst.\nIEEE, jul 2019, pp. 40–44.\n[15] L. I.-K. Lin, “A concordance correlation coefﬁcient to evaluate repro-\nducibility,” Biometrics, vol. 45, no. 1, pp. 255–68, 1989.\n[16] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,”\n3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc., dec\n2014. [Online]. Available: http://arxiv.org/abs/1412.6980\n[17] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,\nand E. Shelhamer, “cuDNN: Efﬁcient Primitives for Deep Learning,”\nTech. Rep., oct 2014.\n[18] F. Chollet and Others, “Keras,” https://keras.io, 2015.\n[19] G. E. Hinton, “Connectionist learning procedures,” Artif. Intell., vol. 40,\nno. 1-3, pp. 185–234, 1989.\n[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-\nnay, “Scikit-learn: Machine Learning in Python,” J. Mach. Learn. Res.,\nvol. 12, pp. 2825–2830, 2011.\n",
  "categories": [
    "eess.AS"
  ],
  "published": "2020-04-06",
  "updated": "2020-04-06"
}