{
  "id": "http://arxiv.org/abs/2004.03623v1",
  "title": "PatchVAE: Learning Local Latent Codes for Recognition",
  "authors": [
    "Kamal Gupta",
    "Saurabh Singh",
    "Abhinav Shrivastava"
  ],
  "abstract": "Unsupervised representation learning holds the promise of exploiting large\namounts of unlabeled data to learn general representations. A promising\ntechnique for unsupervised learning is the framework of Variational\nAuto-encoders (VAEs). However, unsupervised representations learned by VAEs are\nsignificantly outperformed by those learned by supervised learning for\nrecognition. Our hypothesis is that to learn useful representations for\nrecognition the model needs to be encouraged to learn about repeating and\nconsistent patterns in data. Drawing inspiration from the mid-level\nrepresentation discovery work, we propose PatchVAE, that reasons about images\nat patch level. Our key contribution is a bottleneck formulation that\nencourages mid-level style representations in the VAE framework. Our\nexperiments demonstrate that representations learned by our method perform much\nbetter on the recognition tasks compared to those learned by vanilla VAEs.",
  "text": "PatchVAE: Learning Local Latent Codes for Recognition\nKamal Gupta1\nSaurabh Singh2\nAbhinav Shrivastava1\n1University of Maryland, College Park\n{kampta,abhinav}@cs.umd.edu\n2Google Research\nsaurabhsingh@google.com\nAbstract\nUnsupervised representation learning holds the promise\nof exploiting large amounts of unlabeled data to learn gen-\neral representations. A promising technique for unsuper-\nvised learning is the framework of Variational Auto-encoders\n(VAEs). However, unsupervised representations learned by\nVAEs are signiﬁcantly outperformed by those learned by su-\npervised learning for recognition. Our hypothesis is that to\nlearn useful representations for recognition the model needs\nto be encouraged to learn about repeating and consistent\npatterns in data. Drawing inspiration from the mid-level\nrepresentation discovery work, we propose PatchVAE, that\nreasons about images at patch level. Our key contribution\nis a bottleneck formulation that encourages mid-level style\nrepresentations in the VAE framework. Our experiments\ndemonstrate that representations learned by our method per-\nform much better on the recognition tasks compared to those\nlearned by vanilla VAEs.\n1. Introduction\nDue to the availability of large labeled visual datasets,\nsupervised learning has become the dominant paradigm for\nvisual recognition. That is, to learn about any new concept,\nthe modus operandi is to collect thousands of labeled exam-\nples for that concept and train a powerful classiﬁer, such as\na deep neural network. This is necessary because the current\ngeneration of models based on deep neural networks require\nlarge amounts of labeled data [33]. This is in stark contrast\nto the insights that we have from developmental psychology\non how infants develop perception and cognition without any\nexplicit supervision [31]. Moreover, the supervised learning\nparadigm is ill-suited for applications, such as health care\nand robotics, where annotated data is hard to obtain either\ndue to privacy concerns or high cost of expert human anno-\ntators. In such cases, learning from very few labeled images\nor discovering underlying natural patterns in large amounts\nof unlabeled data can have a large number of potential ap-\nplications. Discovering such patterns from unlabeled data is\nExample parts discovered by PatchVAE\nImage\nPart Occurrence Map\nPart samples\nFigure 1: PatchVAE learns to encode repetitive parts across a\ndataset, by modeling their appearance and occurrence. (top) Given\nan image, the occurrence map of a particular part learned by Patch-\nVAE is shown in the middle, capturing the head/beak of the birds.\nSamples of the same part from other images are shown on the right,\nindicating consistent appearance. (bottom) More examples of parts\ndiscovered by our PatchVAE framework.\nthe standard setup of unsupervised learning.\nOver the past few years, the ﬁeld of unsupervised learn-\ning in computer vision has followed two seemingly different\ntracks with different goals: generative modeling and self-\nsupervised learning. The goal of generative modeling is\nto learn the probability distribution from which data was\ngenerated, given some training data. Such models, learned\nusing reconstruction-based losses, can draw samples from\nthe same distribution or evaluate the likelihoods of new data,\nand are useful for learning compact representation of images.\nHowever, we argue that these representations are not as use-\nful for visual recognition. This is not surprising since the task\nof reconstructing images does not require the bottleneck rep-\nresentation to sort out meaningful data useful for recognition\nand discard the rest; on the contrary, it encourages preserving\nas much information as possible for reconstruction.\nIn comparison, the goal in self-supervised learning is to\nlearn representations that are useful for recognition. The\nstandard paradigm is to establish proxy tasks that don’t\nrequire human-supervision but can provide signals useful\n1\narXiv:2004.03623v1  [cs.CV]  7 Apr 2020\nfor recognition. Due to the mismatch in goals of unsuper-\nvised learning for visual recognition and the representations\nlearned from generative modeling, self-supervised learn-\ning is a more popular way of learning representations from\nunlabeled data. However, fundamental limitation of this\nself-supervised paradigm is that we need to deﬁne a proxy-\ntask that can mimic the desired recognition task. It is not\npossible to always establish such a task, nor are these tasks\ngeneralizable across recognition tasks.\nIn this paper, our goal is to enable the unsupervised gen-\nerative modeling approach of VAEs to learn representations\nuseful for recognition. Our key hypothesis is that for a repre-\nsentation to be useful, it should capture just the interesting\nparts of the images, as opposed to everything in the images.\nWhat constitutes an interesting image part has been de-\nﬁned and studied in earlier works that pre-date the end-to-end\ntrained deep network methods [30, 7, 14]. Taking inspira-\ntion from these works, we propose a novel representation\nthat only encodes few such parts of an image that are repet-\nitive across the dataset, i.e., the patches that occur often in\nimages. By avoiding reconstruction of the entire image our\nmethod can focus on regions that are repeating and consistent\nacross many images. In an encoder-decoder based generative\nmodel, we constrain the encoder architecture to learn such\nrepetitive parts – both in terms of representations for appear-\nance of these parts (or patches in an image) and where these\nparts occur. We formulate this using variational auto-encoder\n(β-VAEs) [19, 23], where we impose novel structure on the\nlatent representations. We use discrete latents to model part\npresence or absence and continuous latents to model their\nappearance. Figure 1 shows an example of the discrete la-\ntents or occurrence map, and example parts discovered by\nour approach, PatchVAE. We present PatchVAE in Section 3\nand demonstrate that it learns representations that are much\nbetter for recognition as compared to those learned by the\nstandard β-VAEs [19, 23].\nIn addition, we present losses that favor foreground,\nwhich is more likely to contain repetitive patterns, in Sec-\ntion 3.4, and demonstrate that they result in representations\nthat are much better at recognition. Finally, in Section 4,\nwe present results on CIFAR100 [20], MIT Indoor Scene\nRecognition [27], Places [37], and ImageNet [4] datasets.\nTo summarize, our contributions are as follows:\n1. We propose a novel patch-based bottleneck in the VAE\nframework that learns representations that can encode\nrepetitive parts across images.\n2. We demonstrate that our method, PatchVAE, learns unsu-\npervised representations that are better suited for recogni-\ntion in comparison to traditional VAEs.\n3. We show that losses that favor foreground are better for\nunsupervised representation learning for recognition.\n4. We perform extensive ablation analysis of the proposed\nPatchVAE architecture.\n2. Related Work\nDue to its potential impact, unsupervised learning (par-\nticularly for deep networks) is one of the most researched\ntopics in visual recognition over the past few years. Genera-\ntive models such as VAEs [19, 23, 18, 11], PixelRNN [34],\nPixelCNN [12, 29], and their variants have proven effec-\ntive when it comes to learning compressed representa-\ntion of images while being able to faithfully reconstruct\nthem as well as draw samples from the data distribution.\nGANs [10, 28, 38, 3] on the other hand, while don’t model\nthe probability density explicitly, can still produce high qual-\nity image samples from noise. There has been work com-\nbining VAEs and GANs to be able to simultaneously learn\nimage data distribution while being able to generate high\nquality samples from it [15, 8, 21]. Convolution sparse cod-\ning [1] is an alternative approach for reconstruction or image\nin-painting problems. Our work complements existing gen-\nerative frameworks in that we provide a structured approach\nfor VAEs that can learn beyond low-level representations.\nWe show the effectiveness of the representations learned by\nour model by using them for visual recognition tasks.\nThere has been a lot of work in interpreting or disentan-\ngling representations learned using generative models such\nas VAEs [23, 9, 16]. However, there is little evidence of\neffectiveness of disentangled representations in visual recog-\nnition. Semi-supervised learning using generative models\n[17, 32], where partial or noisy labels are available to the\nmodel during training, has shown lots of promise in appli-\ncations of generating conditioned samples from the model.\nIn our work however, we focus on incorporating inductive\nbiases in these generative models (e.g., VAEs) so they can\nlearn representations better suited for visual recognition.\nA related, but orthogonal, line of work is self-supervised\nlearning where a proxy task is designed to learn represen-\ntation useful for recognition. These proxy tasks vary from\nsimple tasks like arranging patches in an image in the correct\nspatial order [5, 6] and arranging frames from a video in\ncorrect temporal order [35, 25], to more involved tasks like\nin-painting [26] and context prediction [24, 36]. We follow\nthe best practices from this line of work for evaluating the\nlearned representations.\n3. Our Approach\nOur work builds upon VAE framework proposed by [19].\nWe brieﬂy review relevant aspects of the VAE framework\nand then present our approach.\n3.1. VAE Review\nStandard VAE framework assumes a generative model\nfor data where ﬁrst a latent z is sampled from a prior p(z)\nand then the data is generated from a conditional distribution\nG(x|z). A variational approximation Q(z|x) to the true in-\nx\n\"x\nz ∼%(0, ))\nQ\nG\n(a) VAE Architecture\n𝜙(x)\nx\nf\nQ!\nQ\"\nQ(z#|x)\nz$%% ∼Bern z$%%\n#&'$&\n.\nmultiply\nz(## ∼𝒩(0, 𝐼)\nG(x|z#)\ndeconv\n×\nreplicate\n6x\n6z\n(b) PatchVAE Architecture\nFigure 2: (a) VAE Architecture: In a standard VAE architecture, output of encoder network is used to parameterize the variational posterior\nfor z. Samples from this posterior are input to the decoder network. (b) Proposed PatchVAE Architecture: Our encoder network computes\na set of feature maps f using φ(x). This is followed by two independent single layer networks. The bottom network generates part\noccurrence parameters QO. We combine QO with output of top network to generate part appearance parameters QA. We sample zocc and\nzapp to construct ˆz as described in Section 3.2 which is input to the decoder network. We also visualize the corresponding priors for latents\nzapp and zocc in the dashed gray boxes.\ntractable posterior is introduced and the model is learned by\nminimizing the following negative variational lower bound\n(ELBO),\nLVAE(x) = −Ez∼Q(z|x) [log G(x|z)]\n+ KL [Q(z|x) ∥p(z)]\n(1)\nwhere Q(z|x) is often referred to as an encoder as it can be\nviewed as mapping data to the the latent space, while G(x|z)\nis referred to as a decoder (or generator) that can be viewed\nas mapping latents to the data space. Both Q and G are\ncommonly parameterized as neural networks. Fig. 2a shows\nthe commonly used VAE architecture. If the conditional\nG(x|z) takes a gaussian form, negative log likelihood in\nthe ﬁrst term of RHS of Eq. 1 becomes mean squared error\nbetween generator output ˆx = G(x|z) and input data x. In\nthe second term, prior p(z) is assumed to be a multi-variate\nnormal distribution with zero-mean and identity covariance\nN (0, I) and the loss simpliﬁes to\nLVAE(x) = ∥x −ˆx∥2 + KL [Q(z|x) ∥N (0, I)]\n(2)\nWhen G and Q are differentiable, entire model can be\ntrained with SGD using reparameterization trick [19]. [23]\npropose an extension for learning disentangled representa-\ntion by incorporating a weight factor β for the KL Diver-\ngence term yielding\nLβVAE(x) = ∥x −ˆx∥2 + β KL [Q(z|x) ∥N (0, I)] (3)\nVAE framework aims to learn a generative model for the\nimages where the latents z represent the corresponding low\ndimensional generating factors. The latents z can therefore\nbe treated as image representations that capture the necessary\ndetails about images. However, we postulate that represen-\ntations produced by the standard VAE framework are not\nideal for recognition as they are learned to capture all de-\ntails, rather than capturing ‘interesting’ aspects of the data\nand dropping the rest. This is not surprising since there for-\nmulation does not encourage learning semantic information.\nFor learning semantic representations, in the absence of any\nrelevant supervision (as is available in self-supervised ap-\nproaches), inductive biases have to be introduced. Therefore,\ntaking inspiration from works on unsupervised mid-level\npattern discovery [30, 7, 14], we propose a formulation that\nencourages the encoder to only encode such few parts of an\nimage that are repetitive across the dataset, i.e., the patches\nthat occur often in images.\nSince the VAE framework provides a principled way of\nlearning a mapping from image to latent space, we consider\nit ideal for our proposed extension. We chose β-VAEs for\ntheir simplicity and widespread use. In Section 3.2, we\ndescribe our approach in detail and in Section 3.4 propose\na modiﬁcation in the reconstruction error computation to\nbias the error term towards foreground high-energy regions\n(similar to the biased initial sampling of patterns in [30]).\n3.2. PatchVAE\nGiven an image x, let f = φ(x) be a deterministic map-\nping that produces a 3D representation f of size h × w × de,\nwith a total of L = h × w locations (grid-cells). We aim\nto encourage the encoder network to only encode parts of\nan image that correspond to highly repetitive patches. For\nexample, a random patch of noise is unlikely to occur fre-\nquently, whereas patterns like faces, wheels, windows, etc.\nrepeat across multiple images. In order capture this intuition,\nwe force the representation f to be useful for predicting\nfrequently occurring parts in an image, and use just these\npredicted parts to reconstruct the image. We achieve this by\ntransforming f to ˆz which encodes a set of parts at a small\nsubset of L locations on the grid cells. We refer to ˆz as\n“patch latent codes” for an image. Next we describe how\nwe re-tool the β-VAE framework to learn these local latent\ncodes. We ﬁrst describe our setup for a single part and follow\nit up with a generalization to multiple parts (Section 3.3).\nImage Encoding. Given the image representation f = φ(x),\nwe want to learn part representations at each grid location\nl (where l ∈{1, . . . , L}). A part is parameterized by its\nappearance zapp and its occurrence zl\nocc (i.e., presence or ab-\nsence of the part at grid location l). We use two networks, QA\nf\nand QO\nf , to parameterize posterior distributions QA\nf (zapp | f)\nand QO\nf (zl\nocc | f) of the part parameters zapp and zl\nocc re-\nspectively. Since the mapping f = φ(x) is deterministic,\nwe can re-write these distributions as QA\nf (zapp | φ(x)) and\nQO\nf (zl\nocc | φ(x)); or simply QA(zapp | x) and QO(zl\nocc | x).\nTherefore, given an image x the encoder networks estimate\nthe posterior QA(zapp | x) and QO(zl\nocc | x). Note that f\nis a deterministic feature map, whereas zapp and zl\nocc are\nstochastic.\nImage Decoding. We utilize a generator or decoder network\nG, that given zocc and zapp, reconstructs the image. First, we\nsample a part appearance ˆzapp (dp dimensional, continuous)\nand then sample part occurrence ˆzl\nocc (L dimensional, binary)\none for each location l from the posteriors\nˆzapp ∼QA(zapp | x)\nˆzl\nocc ∼QO \u0000zl\nocc | x\n\u0001\n,\nwhere l ∈{1, . . . , L}\n(4)\nNext, we construct a 3D representation ˆz by placing ˆzapp at\nevery location l where the part is present (i.e., ˆzl\nocc = 1). This\ncan be implemented by a broadcasted product of ˆzapp and\nˆzl\nocc . We refer to ˆz as patch latent code. Again note that f is\ndeterministic and ˆz is stochastic. Finally, a deconvolutional\nnetwork takes ˆz as input and generates an image ˆx. This\nimage generation process can be written as\nˆx ∼G\n\u0000x | z\nocc, z\nocc, . . . , zL\nocc, zapp\n\u0001\n(5)\nSince all latent variables (zl\nocc for all l and zapp) are inde-\npendent of each other, they can be stacked as\nzp =\n\u0002\nz\nocc; z\nocc; . . . ; zL\nocc; zapp\n\u0003\n.\n(6)\nThis enables us to use a simpliﬁed the notation (refer to (4)\nand (5)):\nˆzp ∼Q{A,O}(zp | x)\nˆx ∼G (x | zp)\n(7)\nNote that despite the additional structure, our model still re-\nsembles the setup of variational auto-encoders. The primary\ndifference arises from: (1) use of discrete latents for part\noccurrence, (2) patch-based bottleneck imposing additional\nstructure on latents, and (4) feature assembly for generator.\nTraining. We use the training setup of β-VAE and use the\nmaximization of variational lower bound to train the encoder\nand decoder jointly (described in Section 3.1). The posterior\nQA, which captures the appearance of a part, is assumed\nto be a Normal distribution with zero-mean and identity\ncovariance N (0, I). The posterior QO, which captures the\npresence or absence a part, is assumed to be a Bernoulli\ndistribution Bern\n\u0010\nzprior\nocc\n\u0011\nwith prior zprior\nocc . Therefore, the\nELBO for our approach can written as (refer to (3)):\nLPatchVAE(x) = −Ezp∼Q{A,O}(zp | x) [G (x | zp)]\n+ β KL\nh\nQ{A,O}(zp | x) ∥p(zp)\ni\n(8)\nwhere, the KL term can be expanded as:\nKL\nh\nQ{A,O}(zp | x) ∥p(zp)\ni\n=\nβapp\nL\nX\nl=1\nKL\n\u0000QO(zl\nocc | x) ∥Bern\n\u0000zprior\nocc\n\u0001\u0001\n+ βocc KL\n\u0000QA(zapp | x) ∥N (0, I )\n\u0001\n(9)\nImplementation details. As discussed in Section 3.1, the\nﬁrst and second terms of the RHS of (8) can be trained using\nL2 reconstruction loss and reparameterization trick [19]. In\naddition, we also need to compute KL Divergence loss for\npart occurrence. Learning discrete probability distribution is\na challenging task since there is no gradient deﬁned to back-\npropagate reconstruction loss through the stochastic layer at\ndecoder even when using the reparameterization trick. There-\nfore, we use the relaxed-bernoulli approximation [22, 2] for\ntraining part occurrence distributions zl\nocc.\nFor an H × W image, network Q(f | x) ﬁrst generates\nfeature maps of size (h × w × de), where (h, w) are spatial\ndimensions and de is the number of channels. Therefore, the\nnumber of locations L = h × w. Encoders QA\nf (zapp | f) and\nQO\nf (zl\nocc | f) are single layer neural networks to compute\nzapp and zl\nocc. zl\nocc is (h × w × 1)-dimensional multivariate\nbernoulli parameter and zapp is (1 × 1 × dp)-dimensional\nmultivariate gaussian.\ndp is length of the latent vector\nfor a single part. Input to the decoder ˆz is (h × w × dp)-\ndimensional. In all experiments, we ﬁx h = H\n8 and w = W\n8 .\nConstructing zapp.\nNotice that f is an (h × w × de)-\ndimensional\nfeature\nmap\nand\nzl\nocc\nis\n(h × w × 1)-\ndimensional binary output,\nbut zapp is (1 × 1 × dp)-\ndimensional feature vector. If\nX\nl zl\nocc > 1, the part occurs\nat multiple locations in an image. Since all these locations\ncorrespond to same part, their appearance should be the\nsame. To incorporate this, we take the weighted average of\nthe part appearance feature at each location, weighted by the\nprobability that the part is present. Since we use the proba-\nbility values for averaging the result is deterministic. This\noperation is encapsulated by the QA encoder (refer to Fig-\nure 2b). During image generation, we sample ˆzapp once and\nreplicate it at each location where ˆzl\nocc = 1. During training,\nthis forces the model to: (1) only predict ˆzl\nocc = 1 where\nsimilar looking parts occur, and (2) learn a common repre-\nsentation for the part that occurs at these locations. Note\nthat zapp can be modeled as a mixture of distributions (e.g.,\nmixture of gaussians) to capture complicated appearances.\nHowever, in this work we assume that the convolutional\nneural network based encoders are powerful enough to map\nvariable appearance of semantic concepts to similar feature\nrepresentations. Therefore, we restrict ourselves to a single\ngaussian distribution.\n3.3. PatchVAE with multiple parts\nNext we extend the framework described above to use\nmultiple parts. To use N parts, we use N × 2 encoder net-\nworks QA(i) \u0010\nz(i)\napp | x\n\u0011\nand QO(i) \u0010\nzl(i)\nocc | x\n\u0011\n, where z(i)\napp\nand zl(i)\nocc parameterize the ith part. Again, this can be im-\nplemented efﬁciently as 2 networks by concatenating the\noutputs together. The image generator samples ˆz(i)\napp and\nˆzl(i)\nocc from the outputs of these encoder networks and con-\nstructs ˆz(i). We obtain the ﬁnal patch latent code ˆz by\nconcatenating all ˆz(i) in channel dimension. Therefore, ˆz(i)\nis (h × w × dp)-dimensional and ˆz is (h × w × (N × dp))-\ndimensional stochastic feature map. For this multiple part\ncase, (6) can be written as:\nzP =\nh\nz(1)\np ; z(1)\np ; . . . ; z(N)\np\ni\nwhere z(i)\np\n=\nh\nz(i)\nocc ; z(i)\nocc ; . . . ; zL(i)\nocc ; z(i)\napp\ni\n.\n(10)\nSimilarly, (8) and (9) can be written as:\nLMultiPatchVAE(x) = −EzP [G (x | zP)]\n+ βapp\nN\nX\ni=1\nL\nX\nl=1\nKL\n\u0010\nQO(i) \u0010\nzl(i)\nocc | x\n\u0011\n∥Bern\n\u0000zprior\nocc\n\u0001\u0011\n+ βocc\nN\nX\ni=1\nKL\n\u0010\nQA(i) \u0010\nz(i)\napp | x\n\u0011\n∥N (0, I )\n\u0011\n(11)\nThe training details and assumptions of posteriors follow the\nprevious section.\n3.4. Improved Reconstruction Loss\nThe L2 reconstruction loss used for training β-VAEs (and\nother reconstruction based approaches) gives equal impor-\ntance to each region of an image. This might be reason-\nable for tasks like image compression and image de-noising.\nHowever, for the purposes of learning semantic representa-\ntions, not all regions are equally important. For example,\n“sky” and “walls” occupy large portions of an image, whereas\nconcepts like “windows,” “wheels,”, “faces” are compara-\ntively smaller, but arguably more important. To incorporate\nthis intuition, we use a simple and intuitive strategy to weigh\nthe regions in an image in proportion to the gradient energy\nin the region. More concretely, we compute laplacian of an\nimage to get the intensity of gradients per-pixel and average\nthe gradient magnitudes in 8 × 8 local patches. The weight\nmultiplier for the reconstruction loss of each 8 × 8 patch in\nthe image is proportional to the average magnitude of the\npatch. All weights are normalized to sum to one. We refer\nto this as weighted loss (Lw). Note that this is similar to the\ngradient-energy biased sampling of mid-level patches used\nin [30, 7]. Examples of weight masks are provided in the\nsupplemental material.\nIn addition, we also consider an adversarial training strat-\negy from GANs to train VAEs [21], where the discriminator\nnetwork from GAN implicitly learns to compare images and\ngives a more abstract reconstruction error for the VAE. We\nrefer to this variant by using ‘GAN’ sufﬁx in experiments. In\nSection 4.2, we demonstrate that the proposed weighted loss\n(Lw) is complementary to the discriminator loss from adver-\nsarial training, and these losses result in better recognition\ncapabilities for both β-VAE and PatchVAE.\n4. Experiments\nDatasets. We evaluate PatchVAE on CIFAR100 [20], MIT\nIndoor Scene Recognition [27], Places [37] and Imagenet [4]\ndatasets. CIFAR100 consists of 60k 32 × 32 color images\nfrom 100 classes, with 600 images per class. There are\n50000 training images and 10000 test images. Indoor dataset\ncontains 67 categories, and a total of 15620 images. Train\nand test subsets consist of 80 and 20 images per class respec-\ntively. Places dataset has 2.5 millions of images with 205\ncategories. Imagenet dataset has over a million images from\n1000 categories.\nLearning paradigm. In order to evaluate the utility of Patch-\nVAE features for recognition, we setup the learning paradigm\nas follows: we will ﬁrst train the model in an unsupervised\nmanner on all training images. After that, we discard the\ngenerator network and use only part of the encoder network\nφ(x) to train a supervised model on the classiﬁcation task of\nthe respective dataset. We study different training strategies\nfor the classiﬁcation stage as discussed later.\nTraining details. In all experiments, we use the following\narchitectures. For CIFAR100, Indoor67, and Place205, φ(x)\nhas a conv layer followed by two residual blocks [13]. For\nImageNet, φ(x) is a ResNet18 model (a conv layer followed\nby four residual blocks). For all datasets, QA and QO have\na single conv layer each. For classiﬁcation, we start from\nφ(x), and add a fully-connected layer with 512 hidden units\nand a ﬁnal fully-connected layer as classiﬁer. More details\ncan be found in the supplemental material.\nDuring the unsupervised learning phase of training, all\nmethods are trained for 90 epochs for CIFAR100 and In-\ndoor67, 2 epochs for Places205, and 30 epochs for ImageNet\ndataset. All methods use ADAM optimizer for training, with\ninitial learning rate of 1 × 10−4 and a minibatch size of 128.\nFor relaxed bernoulli in QO, we start with the temperature of\nTable 1: Classiﬁcation results on CIFAR100, Indoor67, and Places205. We initialize the classiﬁcation model with the representations φ(x)\nlearned from unsupervised learning task. The model φ(x) comprises of a conv layer followed by two residual blocks (each having 2 conv\nlayers). First column (called ‘Conv1’) corresponds to Top-1 classiﬁcation accuracy with pre-trained model with the ﬁrst conv layer frozen,\nsecond and third columns correspond to results with ﬁrst three and ﬁrst ﬁve conv layers frozen respectively. Details in Section 4.1\nCIFAR100\nIndoor67\nPlaces205\nModel\nConv1\nConv[1-3] Conv[1-5]\nConv1\nConv[1-3] Conv[1-5]\nConv1\nConv[1-3] Conv[1-5]\nβ-VAE\n44.12\n39.65\n28.57\n20.08\n17.76\n13.06\n28.29\n24.34\n8.89\nβ-VAE + Lw\n44.96\n40.30\n28.33\n21.34\n19.48\n13.96\n29.43\n24.93\n9.41\nβ-VAE-GAN\n44.69\n40.13\n29.89\n19.10\n17.84\n13.06\n28.48\n24.51\n9.72\nβ-VAE-GAN + Lw\n45.61\n41.35\n31.53\n20.45\n18.36\n14.33\n29.63\n25.26\n10.66\nPatchVAE\n43.07\n38.58\n28.72\n20.97\n19.18\n13.43\n28.63\n24.95\n11.09\nPatchVAE + Lw\n43.75\n40.37\n30.55\n23.21\n21.87\n15.45\n29.39\n26.29\n12.07\nPatchVAE-GAN\n44.45\n40.57\n31.74\n21.12\n19.63\n14.55\n28.87\n25.25\n12.21\nPatchVAE-GAN + Lw\n45.39\n41.74\n32.65\n22.46\n21.87\n16.42\n29.36\n26.30\n13.39\nBiGAN\n47.72\n41.89\n31.58\n21.64\n17.09\n9.70\n30.06\n25.11\n10.82\nImagenet Pretrained\n55.99\n54.99\n54.36\n45.90\n45.82\n40.90\n37.08\n36.46\n31.26\nTable 2: ImageNet classiﬁcation results using ResNet18. We ini-\ntialize weights from using the unsupervised task and ﬁne-tune the\nlast two residual blocks. Details in Section 4.1\nModel\nTop-1 Acc.\nTop-5 Acc.\nβ-VAE\n44.45\n69.67\nPatchVAE\n47.01\n71.71\nβ-VAE + Lw\n47.28\n71.78\nPatchVAE + Lw\n47.87\n72.49\nImagenet Supervised\n61.37\n83.79\n1.0 with an annealing rate of 3 × 10−5 (following the details\nin [2]). For training the classiﬁer, all methods use stochastic\ngradient descent (SGD) with momentum with a minibatch\nsize of 128. Initial learning rate is 1 × 10−2 and we reduce\nit by a factor of 10 every 30 epochs. All experiments are\ntrained for 90 epochs for CIFAR100 and Indoor67, 5 epochs\nfor Places205, and 30 epochs for ImageNet datasets.\nBaselines. We use the β-VAE model (Section 3.1) as our\nprimary baseline. In addition, we use weighted loss and\ndiscriminator loss resulting in the β-VAE-* family of base-\nlines. We also compare against a BiGAN model from [8].\nWe use similar backbone architectures for encoder/decoder\n(and discriminator if present) across all methods, and tried\nto keep the number of parameters in different approaches\ncomparable to the best of our ability. Exact architecture\ndetails can be found in the supplemental material.\n4.1. Downstream classiﬁcation performance\nIn Table 1, we report the top-1 classiﬁcation results on\nCIFAR100, Indoor67, and Places205 datasets for all meth-\nods with different training strategies for classiﬁcation. First,\nwe keep all the pre-trained weights in φ(x) from the un-\nCIFAR100: Images and Encoded Occurrence Map\nImageNet: Images and Encoded Occurrence Map\nFigure 3: Encoded part occurrence maps discovered on CIFAR100\nand ImageNet. Each row represents a different part.\nsupervised task frozen and only train the two newly added\nconv layers in the classiﬁcation network (reported under col-\numn ‘Conv[1-5]’). We notice that our method (with different\nlosses) generally outperforms the β-VAE counterpart by a\nhealthy margin. This shows that the representations learned\nby PatchVAE framework are better for recognition compared\nto β-VAEs. Moreover, better reconstruction losses (‘GAN’\nand Lw) generally improve both β-VAE and PatchVAE, and\nare complementary to each other.\nNext, we ﬁne-tune the last residual block along with the\ntwo conv layers (‘Conv[1-3]’ column). We observe that\nPatchVAE performs better than VAE under all settings ex-\ncept the for CIFAR100 with just L2 loss. However, when\nusing better reconstruction losses, the performance of Patch-\nVAE improves over β-VAE. Similarly, we ﬁne-tune all but\nPart 2\nSpaghetti Squash\nButternut Squash\nOrange\nLemon\nPart 55\nKiller Whale\nCoral Reef\nScuba Diver\nGrocery Store\nPart 20\nRadiator Grille\nPart 31\nPart 0\nRadiator Grille\nAmbulance\nPart 36\nPart 17\nJack-o'-lantern\nChristmas stocking\nOrange\nFigure 4: A few representative examples for several parts to qualitatively demonstrate the visual concepts captured by PatchVAE. For each\npart, we crop image patches centered on the part location where it is predicted to be present. Selected patches are sorted by part occurrence\nprobability as score. We manually select a diverse set from the top-50 occurrences from the training images. As can be seen, a single part\nmay capture diverse set of concepts that are similar in shape or texture or occur in similar context, but belong to different categories. We\nshow which categories the patches come from (note that category information was not used while training the model).\nthe ﬁrst conv layer and report the results in ‘Conv1’ column.\nAgain, we notice similar trends, where our method gener-\nally performs better than β-VAE on Indoor67 and Places205\ndataset, but β-VAE performs better CIFAR100 by a small\nmargin. When compared to BiGAN, PatchVAE represen-\ntations are better on all datasets (‘Conv[1-5]’) by a huge\nmargin. However, when ﬁne-tuning the pre-trained weights,\nBiGAN performs better on two out of four datasets. We also\nreport results using pre-trained weights in φ(x) using su-\npervised ImageNet classiﬁcation task (last column, Table 1)\nfor completeness. The results indicate that PatchVAE learns\nbetter semantic representations compared to β-VAE.\nImageNet Results. Finally, we report results on the large-\nscale ImageNet benchmark in Table 2. For these experi-\nments, we use ResNet18 [13] architecture for all methods.\nAll weights are ﬁrst learned using the unsupervised tasks.\nThen, we ﬁne-tune the last two residual blocks and train the\ntwo newly added conv layers in the classiﬁcation network\n(therefore, ﬁrst conv layer and the following two residual\nblocks are frozen). We notice that PatchVAE framework\noutperforms β-VAE under all settings, and the proposed\nweighted loss helps both approaches. Finally, the last row\nin Table 2 reports classiﬁcation results of same architecture\nrandomly initialized and trained end-to-end on ImageNet\nusing supervised training for comparison.\n4.2. Qualitative Results\nWe present qualitative results to validate our hypothesis.\nFirst, we visualize whether the structure we impose on the\nVAE bottleneck is able to capture occurrence and appearance\nof important parts of images. We visualize the PatchVAE\ntrained on images from CIFAR100 and Imagenet datasets in\nthe following ways.\nConcepts captured. First, we visualize the part occurrences\nPart\nImage\nPart\nRecon.\nSwapped \nRecon.\nImage\nSource\nTarget\nFigure 5: Swapping source and target part appearance. Column 1,\n2 show a source image with the occurrence map of one of the parts.\nWe can swap the appearance vector of this part with appearance\nvectors of a different part in target images. Column 3, 4 show three\ntarget images with occurrence maps of one of their parts. Observe\nthe change in reconstructions (column 5, 6) as we bring in the new\nappearance vector. The new reconstruction inherits properties of\nthe source at speciﬁc locations in the target.\nin Figure 3. We can see that the parts can capture round\n(fruit-like) shapes in the top row and faces in the second row\nregardless of the class of the image. Similarly for ImageNet,\noccurrence map of a speciﬁc part in images of chicken fo-\ncuses on head and neck. Note that these semantically these\nparts are more informative than just texture or color what a\nβ-VAE can capture. In Figure 4, we show parts captured by\nthe ImageNet model by cropping a part of image centered\naround the occurring part. We can see that parts are able to\ncapture multiple concepts, similar in either shape, texture, or\ncontext in which they occur.\nSwapping appearances. Using PatchVAE, we can swap ap-\npearance of a part with the appearance vector of another part\nfrom a different image. In Figure 5, keeping the occurrence\nmap same for a target image, we modify the appearance of\nTable 3: Effect of N: Increasing\nthe maximum number of patches in-\ncreases the discriminative power for\nCIFAR100 but has little or negative\neffect for Indoor67\nN CIFAR100 Indoor67\n4\n27.59\n14.40\n8\n28.74\n12.69\n16\n28.94\n14.33\n32\n27.78\n13.28\n64\n29.00\n12.76\nTable 4: Effect of dp: Increas-\ning the number of hidden units\nfor a patch has very little im-\npact on classiﬁcation perfor-\nmance\ndp CIFAR100 Indoor67\n3\n28.63\n14.25\n6\n28.97\n14.55\n9\n28.21\n14.55\nTable 5: Effect of zprior\nocc : In-\ncreasing increasing the prior\nprobability of patch occur-\nrence has adverse effect on\nclassiﬁcation performance\nzprior\nocc\nCIFAR100 Indoor67\n0.01\n28.86\n14.33\n0.05\n28.67\n14.25\n0.1\n28.31\n14.03\nTable 6: Effect of βocc: Too\nhigh or too low βocc can de-\nteriorate the performance of\nlearned representations\nβocc CIFAR100 Indoor67\n0.06\n30.11\n14.10\n0.3\n30.37\n15.67\n0.6\n28.90\n13.51\nTable 7: Reconstruction metrics on ImageNet. PatchVAE sacriﬁces\nreconstruction quality to learn discriminative parts, resulting in\nhigher recognition performance (Table 2)\nModel\nPSNR ↑\nFID ↓\nSSIM ↑\nβ-VAE\n4.857\n108.741\n0.289\nPatchVAE\n4.342\n113.692\n0.235\na randomly chosen part and observe the change in recon-\nstructed image. We notice that given the same source part,\nthe decoder tries similar things across different target images.\nHowever, the reconstructions are worse since the decoder\nhas never encountered this particular combination of part\nappearance before.\nDiscriminative vs. Generative strength. As per our design,\nPatchVAE compromises the generative capabilities to learn\nmore discriminative features. To quantify this, we use the the\nimages reconstructed from β-VAE and PatchVAE models\n(trained on ImageNet) and compute three different metrics to\nmeasure the quality of reconstructions of test images. Table 7\nshows that β-VAE is better at reconstruction.\n4.3. Ablation Studies\nWe study the impact of various hyper-parameters used\nin our experiments. For the purpose of this evaluation, we\nfollow a similar approach as in the ‘Conv[1-5]’ column of Ta-\nble 1 and all hyperparameters from the previous section. We\nuse CIFAR100 and Indoor67 datasets for ablation analysis.\nMaximum number of patches. Maximum number of parts\nN used in our framework. Depending on the dataset, higher\nvalue of N can provide wider pool of patches to pick from.\nHowever, it can also make the unsupervised learning task\nharder, since in a minibatch of images, we might not get too\nmany repeat patches. Table 3(left) shows the effect of N on\nCIFAR100 and Indoor67 datasets. We observe that while\nincreasing number of patches improves the discriminative\npower in case of CIFAR100, it has little or negative effect\nin case of Indoor67. A possible reason for this decline in\nperformance for Indoor67 can be smaller size of the dataset\n(i.e., fewer images to learn).\nNumber of hidden units for a patch appearance ˆzapp.\nNext, we study the impact of the number of channels in the\nappearance feature ˆzapp for each patch (dp). This parameter\nreﬂects the capacity of individual patch’s latent representa-\ntion. While this parameter impacts the reconstruction quality\nof images. We observed that it has little or no effect on\nthe classiﬁcation performance of the base features. Results\nare summarized in Table 4(right) for both CIFAR100 and\nIndoor67 datasets.\nPrior probability for patch occurrence zprior\nocc . In all our\nexperiments, prior probability for a patch is ﬁxed to 1/N,\ni.e., inverse of maximum number of patches. The intuition\nis to encourage each location on occurrence maps to ﬁre for\nat most one patch. Increasing this patch occurrence prior\nwill allow all patches to ﬁre at the same location. While this\nwould make the reconstruction task easier, it will become\nharder for individual patches to capture anything meaningful.\nTable 5 shows the deterioration of classiﬁcation performance\non increasing zprior\nocc .\nPatch occurrence loss weight βocc. The weight for patch\noccurrence KL Divergence has to be chosen carefully. If βocc\nis too low, more patches can ﬁre at same location and this\nharms the the learning capability of patches; and if βocc is\ntoo high, decoder will not receive any patches to reconstruct\nfrom and both reconstruction and classiﬁcation will suffer.\nTable 6 summarizes the impact of varying βocc.\n5. Conclusion\nWe presented a patch-based bottleneck in the VAE frame-\nwork that encourages learning useful representations for\nrecognition. Our method, PatchVAE, constrains the encoder\narchitecture to only learn patches that are repetitive and con-\nsistent in images as opposed to learning everything, and\ntherefore results in representations that perform much better\nfor recognition tasks compared to vanilla VAEs. We also\ndemonstrate that losses that favor high-energy foreground\nregions of an image are better for unsupervised learning of\nrepresentations for recognition.\nReferences\n[1] Lama Affara, Bernard Ghanem, and Peter Wonka. Supervised\nconvolutional sparse coding. CoRR, abs/1804.02678, 2018. 2\n[2] Eirikur Agustsson, Fabian Mentzer, Michael Tschannen,\nLukas Cavigelli, Radu Timofte, Luca Benini, and Luc V Gool.\nSoft-to-hard vector quantization for end-to-end learning com-\npressible representations. In Advances in Neural Information\nProcessing Systems, pages 1141–1151, 2017. 4, 6\n[3] Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasser-\nstein gan. arXiv preprint arXiv:1701.07875, 2017. 2\n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn Computer Vision and Pattern Recognition, 2009. IEEE\nConference on, pages 248–255. Ieee, 2009. 2, 5\n[5] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Context\nas supervisory signal: Discovering objects with predictable\ncontext. In European Conference on Computer Vision, pages\n362–377. Springer, 2014. 2\n[6] Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsuper-\nvised visual representation learning by context prediction. In\nInternational Conference on Computer Vision (ICCV), 2015.\n2\n[7] Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic,\nand Alexei A. Efros. What makes paris look like paris? ACM\nTransactions on Graphics (SIGGRAPH), 31(4):101:1–101:9,\n2012. 2, 3, 5\n[8] Jeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Ad-\nversarial feature learning. arXiv preprint arXiv:1605.09782,\n2016. 2, 6\n[9] Marco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole\nWinther. A disentangled recognition and nonlinear dynamics\nmodel for unsupervised learning. In NIPS, 2017. 2\n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances\nin neural information processing systems, pages 2672–2680,\n2014. 2\n[11] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez\nRezende, and Daan Wierstra. Draw: A recurrent neural net-\nwork for image generation. arXiv preprint arXiv:1502.04623,\n2015. 2\n[12] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali\nTaiga, Francesco Visin, David Vazquez, and Aaron Courville.\nPixelvae: A latent variable model for natural images. arXiv\npreprint arXiv:1611.05013, 2016. 2\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 5, 7\n[14] Mayank Juneja, Andrea Vedaldi, CV Jawahar, and Andrew\nZisserman. Blocks that shout: Distinctive parts for scene\nclassiﬁcation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 923–930,\n2013. 2, 3\n[15] Salman H Khan, Munawar Hayat, and Nick Barnes. Adver-\nsarial training of variational auto-encoders for high ﬁdelity\nimage generation. arXiv preprint arXiv:1804.10323, 2018. 2\n[16] Hyunjik Kim and Andriy Mnih. Disentangling by factorising.\narXiv preprint arXiv:1802.05983, 2018. 2\n[17] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende,\nand Max Welling. Semi-supervised learning with deep gener-\native models. In Advances in neural information processing\nsystems, pages 3581–3589, 2014. 2\n[18] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi\nChen, Ilya Sutskever, and Max Welling. Improved variational\ninference with inverse autoregressive ﬂow. In Advances in\nNeural Information Processing Systems, pages 4743–4751,\n2016. 2\n[19] Diederik P Kingma and Max Welling. Auto-encoding vari-\national bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3,\n4\n[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. Technical report, Citeseer,\n2009. 2, 5\n[21] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo\nLarochelle, and Ole Winther.\nAutoencoding beyond pix-\nels using a learned similarity metric.\narXiv preprint\narXiv:1512.09300, 2015. 2, 5\n[22] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The\nconcrete distribution: A continuous relaxation of discrete\nrandom variables. arXiv preprint arXiv:1611.00712, 2016. 4\n[23] Lo¨ıc Matthey, Arka Pal, Christopher Burgess, Xavier Glo-\nrot, Matthew Botvinick, Shakir Mohamed, and Alexander\nLerchner. beta-vae: Learning basic visual concepts with a\nconstrained variational framework. In ICLR 2017, 2017. 2, 3\n[24] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of\nvisual representations by solving jigsaw puzzles. In European\nConference on Computer Vision, pages 69–84. Springer, 2016.\n2\n[25] Deepak Pathak, Ross Girshick, Piotr Doll´ar, Trevor Darrell,\nand Bharath Hariharan. Learning features by watching objects\nmove. In CVPR, 2017. 2\n[26] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor\nDarrell, and Alexei A Efros. Context encoders: Feature learn-\ning by inpainting. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 2536–2544,\n2016. 2\n[27] Ariadna Quattoni and Antonio Torralba. Recognizing indoor\nscenes. In 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pages 413–420. IEEE, 2009. 2, 5\n[28] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-\nvised representation learning with deep convolutional gen-\nerative adversarial networks. CoRR, abs/1511.06434, 2015.\n2\n[29] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P.\nKingma. Pixelcnn++: A pixelcnn implementation with dis-\ncretized logistic mixture likelihood and other modiﬁcations.\nIn ICLR, 2017. 2\n[30] Saurabh Singh, Abhinav Gupta, and Alexei A. Efros. Unsu-\npervised discovery of mid-level discriminative patches. In\nEuropean Conference on Computer Vision, 2012. 2, 3, 5\n[31] Linda Smith and Michael Gasser. The development of em-\nbodied cognition: Six lessons from babies. Artiﬁcial life,\n11(1-2):13–29, 2005. 1\n[32] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning\nstructured output representation using deep conditional gener-\native models. In Advances in neural information processing\nsystems, pages 3483–3491, 2015. 2\n[33] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. 2017 IEEE International Conference on\nComputer Vision (ICCV), pages 843–852, 2017. 1\n[34] A¨aron van den Oord,\nNal Kalchbrenner,\nand Koray\nKavukcuoglu.\nPixel recurrent neural networks.\nCoRR,\nabs/1601.06759, 2016. 2\n[35] Xiaolong Wang and Abhinav Gupta. Unsupervised learning\nof visual representations using videos. In Proceedings of the\nIEEE International Conference on Computer Vision, pages\n2794–2802, 2015. 2\n[36] Xiaolong Wang, Kaiming He, and Abhinav Gupta. Transitive\ninvariance for self-supervised visual representation learning.\nIn Proc. of Intl Conf. on Computer Vision (ICCV), 2017. 2\n[37] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Places: A 10 million image database\nfor scene recognition. IEEE transactions on pattern analysis\nand machine intelligence, 40(6):1452–1464, 2017. 2, 5\n[38] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.\nUnpaired image-to-image translation using cycle-consistent\nadversarial networks. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pages 2223–2232,\n2017. 2\nA. Training Details\nThe generator network has two deconv layers with batch-\nnorm and a ﬁnal deconv layer with tanh activation. When\ntraining with ‘GAN’ loss, the additional discriminator has\nfour conv layers, two of which have batchnorm.\nB. Visualization of Weighted Loss\nFigure 6 shows an illustration of the reconstruction loss\nLw proposed in Section 3.4. Notice that in ﬁrst column,\nguitar has more weight that rest of the image. Similarly in\nsecond, fourth and sixth columns that train, painting, and\npeople are respectively weighed more heavily by Lw than\nrest of the image; thus favoring capturing the foreground\nregions.\nC. Model Architecture\nIn this section, we share the exact architectures used in\nvarious experiments. As discussed in Section 4, we evaluated\nour proposed model on CIFAR100, Indoor67, and Places205\ndatasets. We resize and center-crop the images such that\ninput image size for CIFAR100 datasets is 32×32×3 while\nfor Indoor67 and Places205 datasets input image size is 64×\n64 × 3. PatchVAE can treat images of various input sizes in\nexactly same way allowing us to keep the architecture same\nfor different datasets. In case of VAE and BiGAN however,\nwe have to go through a ﬁxed size bottleneck layer and hence\narchitectures need to be a little different for different input\nimage sizes. Wherever possible, we have tried to keep the\nnumber of parameters in different architectures comparable.\nC.1. Architecture for unsupervised learning task\nTables 8 and 9 show the architectures for encoders used in\ndifferent models. In the unsupervised learning task, encoder\ncomprises of a ﬁxed neural network backbone φ(x), that\ngiven an image of size h × w × 3 generated feature maps\nof size h\n8 × w\n8 × de. This backbone architecture is common\nto different models discussed in the paper and consists of a\nsingle conv layer followed by 2 residual blocks. We refer to\nthis φ(x) as Resnet-9 and it is described as Conv1-5 layers in\nTable 12. Rest of the encoder architecture varies depending\non the model in consideration and is described in the Tables 8\nand 9.\nTables 10 and 11 show the architectures for decoders\nused in different models. We use a pyramid like network for\ndecoder where feature map size is doubled in consecutive lay-\ners, while number of channels is halved. Final non-linearity\nused in each decoder is tanh.\nC.2. Architecture for supervised learning task\nAs discussed in Section 4, during the supervised learning\nphase, we discard rest of the encoder model and only keep\nφ(x) for classiﬁer training. So the architectures for all base-\nlines are exactly the same. Tables 12 shows the architecture\nfor classiﬁer used in our experiments.\nRandom \nSamples\nImage \nLaplacian\nWeight masks\nmask x image\nFigure 6: Masks used for weighted reconstruction loss Lw. First row contains images randomly samples from MIT Indoor datatset. Second\nand third rows have the corresponding image laplacians and ﬁnal reconstruction weight masks respectively. In the last row, we take the\nproduct of ﬁrst and third row to highlight which parts of image are getting more attention while reconstruction.\nTable 8: Encoder architecture for unsupervised learning task on CIFAR100 - All ‘convolutional’ layers are represented as (kernel size ×\nkernel size, channels, stride, pad). BN stands for batch normalization layer and ReLU for Rectiﬁed Linear Units.\nLayer\nβ-VAE\nBiGAN\nPatchVAE\nFeatures φ\nResnet-9\nResnet-9\nResnet-9\nQO\n-\n-\n(3 × 3, 16, 1, 1)\nQA\n(1 × 1, 64, 1, 0)\nBN\nReLU\nµ : (4 × 4, 96, 1, 0)\nσ2 : (4 × 4, 96, 1, 0)\n(1 × 1, 64, 1, 0)\nBN\nReLU\n(4 × 4, 96, 1, 0)\nµ : (3 × 3, 96, 1, 1)\nσ2 : (3 × 3, 96, 1, 1)\n# Parameters\n888,192\n789,792\n922,896\nTable 9: Encoder architecture for unsupervised learning task on Indoor67 and Places205 - All ‘convolutional’ layers are represented as\n(kernel size × kernel size, channels, stride, pad). BN stands for batch normalization layer and ReLU for Rectiﬁed Linear Units. Note that\nPatchVAE and β-VAE architectures are slightly different to account for sizes.\nLayer\nβ-VAE\nBiGAN\nPatchVAE\nFeatures φ\nResnet-9\nResnet-9\nResnet-9\nQO\n-\n-\n(3 × 3, 16, 1, 1)\nQA\n(1 × 1, 64, 1, 0)\nBN\nReLU\nµ : (8 × 8, 96, 1, 0)\nσ2 : (8 × 8, 96, 1, 0)\n(1 × 1, 64, 1, 0)\nBN\nReLU\n(8 × 8, 96, 1, 0)\nµ : (3 × 3, 96, 1, 1)\nσ2 : (3 × 3, 96, 1, 1)\n# Parameters\n1,478,016\n1,084,704\n922,896\nTable 10: Decoder architecture for unsupervised earning task on CIFAR100 - All ‘deconvolutional’ layers are represented as (kernel size ×\nkernel size, channels, stride, pad). BN stands for batch normalization layer and ReLU for Rectiﬁed Linear Units.\nβ-VAE\nBiGAN\nPatchVAE\nModel\n(4 × 4, 64, 1, 0)\nBN\nLeakyReLU(0.2)\n(1 × 1, 256, 1, 0)\nBN\nLeakyReLU(0.2)\n(4 × 4, 128, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 64, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 3, 2, 1)\ntanh\n(4 × 4, 64, 1, 0)\nBN\nLeakyReLU(0.2)\n(1 × 1, 256, 1, 0)\nBN\nLeakyReLU(0.2)\n(4 × 4, 128, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 64, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 3, 2, 1)\ntanh\n(1 × 1, 256, 1, 0)\nBN\nLeakyReLU(0.2)\n(4 × 4, 128, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 64, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 3, 2, 1)\ntanh\n# Parameters\n774,144\n774,144\n683,904\nTable 11: Decoder architecture for unsupervised learning task on Indoor67 and Places205 - All ‘deconvolutional’ layers are represented as\n(kernel size × kernel size, channels, stride, pad). BN stands for batch normalization layer and ReLU for Rectiﬁed Linear Units. Note that\nPatchVAE and β-VAE architectures are slightly different to account for sizes.\nβ-VAE\nBiGAN\nPatchVAE\nModel\n(8 × 8, 64, 1, 0)\nBN\nLeakyReLU(0.2)\n(1 × 1, 256, 1, 0)\nBN\nLeakyReLU(0.2)\n(4 × 4, 128, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 64, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 3, 2, 1)\ntanh\n(8 × 8, 64, 1, 0)\nBN\nLeakyReLU(0.2)\n(1 × 1, 256, 1, 0)\nBN\nLeakyReLU(0.2)\n(4 × 4, 128, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 64, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 3, 2, 1)\ntanh\n(1 × 1, 256, 1, 0)\nBN\nLeakyReLU(0.2)\n(4 × 4, 128, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 64, 2, 1)\nBN\nLeakyReLU(0.2)\n(4 × 4, 3, 2, 1)\ntanh\n# Parameters\n1,069,056\n1,069,056\n683,904\nTable 12: Architecture for supervised learning task - same for all baselines and our model. All convolutional layers are represented\nas (kernel size × kernel size, channels, stride, pad). BN stands for batch bormalization layer and ReLU for Rectiﬁed Linear Units. All\npooling operations are MaxPool and are represented by (kernel size × kernel size, stride, pad). Like Resnet-18, downsampling happens by\nconvolutional layers that have a stride of 2. In our model, downsampling happens during Conv1, Pool, and after Conv4-5.\nLayer\nCIFAR100 (32 × 32 × 3)\nIndoor67 (64 × 64 × 3)\nPlaces205 (64 × 64 × 3)\nConv1\n1 ×\n\n\n\n\n\n\n\n\n\n(7 × 7, 64, 2, 3)\nBN\nReLU\nPool(3 × 3, 2, 1)\n1 ×\n\n\n\n\n\n\n\n\n\n(7 × 7, 64, 2, 3)\nBN\nReLU\nPool(3 × 3, 2, 1)\n1 ×\n\n\n\n\n\n\n\n\n\n(7 × 7, 64, 2, 3)\nBN\nReLU\nPool(3 × 3, 2, 1)\nConv2-3\n2 ×\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(3 × 3, 64, 1, 1)\nBN\nReLU\n(3 × 3, 64, 1, 1)\nBN\n2 ×\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(3 × 3, 64, 1, 1)\nBN\nReLU\n(3 × 3, 64, 1, 1)\nBN\n2 ×\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(3 × 3, 64, 1, 1)\nBN\nReLU\n(3 × 3, 64, 1, 1)\nBN\nConv4-5\n2 ×\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(3 × 3, 128, 1, 1)\nBN\nReLU\n(3 × 3, 128, 1, 1)\nBN\n2 ×\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(3 × 3, 128, 1, 1)\nBN\nReLU\n(3 × 3, 128, 1, 1)\nBN\n2 ×\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(3 × 3, 128, 1, 1)\nBN\nReLU\n(3 × 3, 128, 1, 1)\nBN\nFC\n2048 × 512\n512 × 100\n8192 × 512\n512 × 67\n8192 × 512\n512 × 205\n# Parameters\n1,783,460\n4,912,259\n4,983,053\n",
  "categories": [
    "cs.CV",
    "cs.LG"
  ],
  "published": "2020-04-07",
  "updated": "2020-04-07"
}