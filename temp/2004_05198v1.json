{
  "id": "http://arxiv.org/abs/2004.05198v1",
  "title": "Reinforcement Learning via Gaussian Processes with Neural Network Dual Kernels",
  "authors": [
    "Imène R. Goumiri",
    "Benjamin W. Priest",
    "Michael D. Schneider"
  ],
  "abstract": "While deep neural networks (DNNs) and Gaussian Processes (GPs) are both\npopularly utilized to solve problems in reinforcement learning, both approaches\nfeature undesirable drawbacks for challenging problems. DNNs learn complex\nnonlinear embeddings, but do not naturally quantify uncertainty and are often\ndata-inefficient to train. GPs infer posterior distributions over functions,\nbut popular kernels exhibit limited expressivity on complex and\nhigh-dimensional data. Fortunately, recently discovered conjugate and neural\ntangent kernel functions encode the behavior of overparameterized neural\nnetworks in the kernel domain. We demonstrate that these kernels can be\nefficiently applied to regression and reinforcement learning problems by\nanalyzing a baseline case study. We apply GPs with neural network dual kernels\nto solve reinforcement learning tasks for the first time. We demonstrate, using\nthe well-understood mountain-car problem, that GPs empowered with dual kernels\nperform at least as well as those using the conventional radial basis function\nkernel. We conjecture that by inheriting the probabilistic rigor of GPs and the\npowerful embedding properties of DNNs, GPs using NN dual kernels will empower\nfuture reinforcement learning models on difficult domains.",
  "text": "Reinforcement Learning via Gaussian\nProcesses with Neural Network Dual Kernels\nIm`ene R. Goumiri∗, Benjamin W. Priest, Michael D. Schneider\nLawrence Livermore National Laboratory, Livermore, CA, 94551, USA.\nApril 14, 2020\nLLNL-JRNL-808440\nAbstract\nWhile deep neural networks (DNNs) and Gaussian Processes (GPs)\nare both popularly utilized to solve problems in reinforcement learn-\ning, both approaches feature undesirable drawbacks for challenging\nproblems.\nDNNs learn complex nonlinear embeddings, but do not\nnaturally quantify uncertainty and are often data-ineﬃcient to train.\nGPs infer posterior distributions over functions, but popular kernels\nexhibit limited expressivity on complex and high-dimensional data.\nFortunately, recently discovered conjugate and neural tangent kernel\nfunctions encode the behavior of overparameterized neural networks\nin the kernel domain. We demonstrate that these kernels can be eﬃ-\nciently applied to regression and reinforcement learning problems by\nanalyzing a baseline case study.\nWe apply GPs with neural network dual kernels to solve reinforce-\nment learning tasks for the ﬁrst time. We demonstrate, using the well-\nunderstood mountain-car problem, that GPs empowered with dual\nkernels perform at least as well as those using the conventional radial\nbasis function kernel.\nWe conjecture that by inheriting the proba-\nbilistic rigor of GPs and the powerful embedding properties of DNNs,\nGPs using NN dual kernels will empower future reinforcement learning\nmodels on diﬃcult domains.\n∗goumiri1@llnl.gov\n1\narXiv:2004.05198v1  [cs.LG]  10 Apr 2020\n1\nIntroduction\nThe traditional approach in optimal control posits a controller with a suite of\ncontrol signals able to aﬀect a known dynamical system. The problem is to\ndevise a policy for scheduling control signals in order to achieve some given\nobjective. As there is no uncertainty in the model, ﬁnding such a policy\nbecomes an optimization problem.\nHowever, many applications involve decision-making challenges where\ndata are limited and the generative models are complex and partially or\ncompletely unknown. As such, the reinforcement learning (RL) branch of\nmachine learning arose to develop models for an agent or agents acting on an\ninitially unknown environment. RL algorithms learn a policy to guide agent\nactions in order to achieve some high-level goal by acting on its environment\nand using the response to model its dynamics.\nAlthough RL and optimal control are related, these research ﬁelds are\ntraditionally separate. Ultimately, both are concerned with sequential deci-\nsion making to minimize an expected long-term cost. The dynamical system,\ncontroller, and control signals of optimal control roughly map onto the envi-\nronment, agent(s), and actions of RL.\nMany RL algorithms [35, 5, 23, 36, 41] address a lack of dynamics knowl-\nedge by way of a reliance upon parametric adaptive elements or control poli-\ncies whose number of parameters or features are ﬁxed and predetermined.\nThese parameters are usually then learned from data. Deep neural networks\n(DNNs) are also used extensively in RL [27, 28, 42]. DNNs are attractive as\nthey are known to have an excellent representative power [14, 16, 18]. How-\never, tuning and training the parameters is a data-ineﬃcient practice [31].\nMoreover, DNNs usually include no natural means of quantifying the un-\ncertainty in their predictions [10]. Thus trained models may overconﬁdently\npredict the unknown dynamics when the system operates outside of the ob-\nserved domain. Such overconﬁdent prediction can lead to system instability,\nthereby making any controller stability results unachievable.\nNonparametric kernel methods such as Gaussian processes (GPs) [38]\nhave also been applied to reinforcement learning tasks [30, 19, 20, 17, 33, 9,\n8]. GPs are popular in many areas of machine learning due to their ﬂexi-\nbility, interpretability, and natural uncertainty quantiﬁcation due to being\nBayesian models. However GP-related data-driven methods remain largely\nunexploited in optimal control.\nThe choice of GP kernel function encodes our prior beliefs about the\n2\ndistribution of the function of interest and is a key part of modeling. For\nexample, Kuss and Rasmussen use the famous radial basis function (RBF)\nin a GP to solve the mountain-car problem, implying that the dynamics\nare believed to be very smooth [19]. However, GPs often struggle to learn\nthe features of complex or high-dimensional data, worrying the researcher\ninterested in extrapolating this approach to such domains.\nRecent results have shown a duality between wide, random DNNs and\nGPs through the use of the conjugate kernels (CK)[32, 6, 7, 21, 26] and\nneural tangent kernels (NTK) [15, 1, 40, 2]. These kernels capture, in a sense\nthat will be made explicit in Section 2, the nonlinear feature embedding\nlearned by the corresponding DNN architecture. However, these kernels are\nat present mostly treated as an academic curiosity and have predominantly\nbeen applied to image classiﬁcation problems [1].\n2\nGaussian Processes, Neural Networks, and\nDual Kernels\nWe will brieﬂy review GPs, DNNs, and the correspondence between GPs\nand inﬁnitely wide Bayesian DNNs. We will focus on the computation of the\nvarious models, and largely omit training details.\nGPs are ﬂexible, nonparametric Bayesian models that specify a prior dis-\ntribution over a function f : X →Y that can be updated by data D ⊂X ×Y.\nCoarsely, a GP is a collection of random variables, any ﬁnite subset of which\nhas a multivariate Gaussian distribution. We say that f ∼GP(m(·), k(·, ·)),\nwhere m : X →R is a mean function and k : X × X →R a positive def-\ninite covariance function with hyperparameters θ. In practice m is usually\nassumed to be the zero function. Let f be a vector of evaluations of f at\nsome ﬁnite X = {x1, . . . , xn} ⊂X. Then f ∼GP(0, k(·, ·)) implies that\nf = [f(x1), . . . , f(xn)]⊤∼N(0, Kﬀ).\n(1)\nHere Kﬀis an n×n matrix whose (i, j)th element is k(xi, xj) = cov(f(xi), f(xj)).\nSuch covariance matrices implicitly depend on θ.\nWe will assume that we actually observe data y = f + ϵ, where ϵi ∼\nN(0, σ2) is homoscedastic noise. Denote by f∗the vector of (unknown) eval-\nuations of f at a ﬁnite X∗= {x∗\n1, . . . , x∗\nn∗} ⊂X, and for convenience deﬁne\nQﬀ= Kﬀ+ σ2In. The GP prior on f implies that the joint distribution of y\n3\nand f∗is in the same form as Equation (1):\n\u0014y\nf∗\n\u0015\n= N\n\u0012\n0,\n\u0014Qﬀ\nKf∗\nK∗f\nK∗∗\n\u0015\u0013\n.\n(2)\nHere K∗f = K⊤\nf∗is the cross-covariance matrix between X∗and X, i.e. the\n(i, j)th element of K∗f is k(x∗\ni , xj). The predictive posterior distribution on\nf∗conditioned on X, X∗and y is therefore given by\nf∗| X, X∗, y ∼N(K∗fQ−1\nﬀy, K∗∗−K∗fQ−1\nﬀKf∗).\n(3)\nThe posterior mean of Equation 3 is often given as the prediction for f on\nX∗in GP machine learning.\nThe expressiveness of a GP is heavily dependent upon the choice of kernel\nfunction k. Most common functions, for example the RBF kernel,\nkRBF(x, x′) = exp\n\u0012\n−∥x −x′∥2\n2\nℓ2\n\u0013\n,\n(4)\nexhibit limited expressiveness on complex data and impose sometimes in-\nappropriate assumptions such as stationarity.\nGPs also suﬀer from cubic\nscaling in the observation size, although a rich literature of approximations\naddresses this problem. See [24] and [13] for reviews of GP scaling methods.\nFurthermore, the recent emergence of fast and scalable GP software is be-\nginning to challenge the conventional wisdom concerning the intractability\nof GPs by exploiting hardware acceleration [11, 37].\nDNNs learn an embedding of inputs into a latent space by way of iter-\natively applying nonlinear transforms. This embedding transforms highly-\nnonlinear data relationships into a linear feature space, allowing a ﬁnal linear\nregression to produce predictions. In contrast to GPs, DNNs are highly para-\nmetric, often utilizing more parameters than observations. For this reason,\nDNNs often require large amounts of training data, and a vast literature\nhas developed around heuristic training protocols. While DNNs do not, in\ngeneral, produce posterior distributions or exhibit robust uncertainty quan-\ntiﬁcation, their popularity is due to good empirical performance on complex\nand high-dimensional data.\nA DNN with L layers and widths {nℓ}L\nℓ=0 has parameters consisting of\nweight matrices {W ℓ∈Rnℓ×nℓ−1}L\nℓ=1 and biases {bℓ∈Rℓ}L\nℓ=1. We will assume\nthe NTK parameterization and introduce hyperparameters σw and σb, whose\n4\ninterpretation we will deﬁne in Section 2.1. The output of a DNN on input\nx is hL(x), which is computed recursively as\nh1(x) = σw\n√\nn0W 1x + σbb1,\nhℓ(x) =\nσw\n√\nnℓ−1W ℓφ\n\u0000hℓ−1(x)\n\u0001\n+ σbbℓ.\n(5)\nHere φ(·) is an element-wise scalar nonlinear activation function, such as the\npopular ReLU function:\nφReLU(x) = max{0, x}.\n(6)\n2.1\nDual Kernels\nAs we have noted, GPs and DNNs have diﬀerent advantages and disadvan-\ntages. Many attempts have been made to obtain “the best of both worlds”\n- the uncertainty quantiﬁcation and interpretability of GPs along with the\ncomputational convenience and expressivity of DNNs. Such eﬀorts include\nBayesian neural networks, which apply prior distributions to the weights\nof neural networks [32], and applying GPs to feature vectors embedded by\nDNNs [25]. Interestingly, a direct correspondence between GPs and Bayesian\nDNNs of any depth arises as the hidden layers become suﬃciently wide. We\nwill brieﬂy motivate this correspondence, its history and applications.\nInitializing all of the parameters in a DNN as W ℓ\ni,j ∼N\n\u0010\n0,\nσ2\nw\nnℓ−1\n\u0011\nand\nbℓ\ni ∼N (0, σ2\nb) for i ∈\n\u0002\nnℓ\u0003\nand j ∈\n\u0002\nnℓ−1\u0003\n(i.e. Glorot initialization [21]) is\ncommon in practice. Note that this is the equivalent of initializing all of the\nparameters in Eq. (5) as i.i.d. N(0, 1). In the study of highly overparame-\nterized (wide) models over the last several decades, investigators made two\nunexpected observations.\n1. Random initialization followed by training only the ﬁnal linear layer\noften produces high-quality predictions.\n2. Training overparameterized models tends to produce weights that diﬀer\nonly slightly from initialization.\nThe correspondence between inﬁnitely wide single hidden layer neural\nnetworks with i.i.d. Gaussian weights and biases and analytic Gaussian Pro-\ncesses kernels was ﬁrst discovered as far back as the 1990s by Neal by ap-\nplication of the Central Limit Theorem [32]. Recently, others have extended\n5\nNeal’s result to inﬁnitely wide deep neural networks [21, 26] and convolu-\ntional neural networks with inﬁnitely many channels [34, 12]. Arora et al.\nimproved these results by showing that the correspondence holds for ﬁnite\nneural networks that are suﬃciently wide [1] and showed empirical evidence\nthat the kernel process behavior occurs at lower widths than theoretically\nguaranteed [2]. This is to say that the aurthors found that DNN predictions\ntend to agree with those of their dual GP counterparts, even when the DNNs\nare much narrower than the known theorems for convergence require. The\nkernel corresponding to wide DNNs is referred to in the literature as the\nconjugate kernel (CK) [7] or NNGP kernel [21].\nIn order to express the DNN transform Equation (5) in kernel language,\nwe must obtain a dual form of the nonlinearity φ that nonlinearly embeds\na kernel matrix K in another Hilbert space [6, 7]. For nonlinearity φ and\nkernel matrix K the dual form is known to be\nVφ(K)(x, x′) =\nE\nf∼N(0,K)φ(f(x))φ(f(x′)).\n(7)\nUsing this dual transform and the notation of Eq. (5) and following the\nformulation of [40], we can express the CK recursively as\nΣ1(x, x′) = σ2\nw\nn0 ⟨x, x′⟩+ σ2\nb\nΣℓ(x, x′) = σ2\nwVφ(Σℓ−1)(x, x′) + σ2\nb.\n(8)\nThe last layer kernel ΣL is the conjugate kernel for the network. This kernel\ncorresponds exactly to that of the linear model resulting from randomly\ninitializing all weights and training the last layer.\nIf the CK lends mathematical rigor to observation 1) above, the neural\ntangent kernel (NTK) does the same with observation 2). Intuitively, the\nNTK corresponds to a generalization of the CK where we train the whole\nmodel, rather than only the last layer. The NTK emerges from the obser-\nvation that inﬁnitely wide neural networks evolve as linear models under\nstochastic gradient descent [15, 22] and has also been shown to generalize to\nconvolutional and ﬁnite architectures [1]. Evidence suggests that the NTK\nmight be capable of learning more complex features than the CK [40], and\nthe NTK has recently been shown to deliver competitive predictions in an\nSVM on small data learning benchmarks [2]. We will omit the derivation,\nwhich is somewhat involved, and instead recite the form of the NTK ΘL as\n6\ngiven in [40]:\nΘ1(x, x′) = Σ1(x, x′)\nΘℓ(x, x′) = Σℓ(x, x′) + σ2\nwΘℓ−1(x, x′)Vφ′(Σℓ−1)(x, x′).\n(9)\nAt ﬁrst blush, the formulations of Eqs. (8) and (9) are unhelpful, as com-\nputing Eq. (7) is intractable. Fortunately, closed-form solutions are known\nfor several common activation functions [6, 7], enabling eﬃcient computa-\ntion. Throughout the rest of this document we will consider only networks\nutilizing φReLU, which is known to have analytic dual activations:\nVφReLU(K)(x, x′) =\n√\nK(x,x)K(x′,x′)\n2π\n(sin c + (π −c) cos c))\n(10)\nVφ′\nReLU(K)(x, x′) = 1\n2π(π −c)\n(11)\nc = arccos\n \nK(x, x′)\np\nK(x, x)K(x′, x′)\n!\n.\n(12)\n2.2\nA motivating example\nWe will illustrate the usage of the RBF kernel along with CK and NTK on\na model of a simple machine. Consider the central example given in [4] of\nmoving a weight up a slope. We will assume that we are trying to learn a\ntrue process driven by the dynamics\nζ(x | θ, a) =\nθx\n1 −x/a.\n(13)\nHere x is a control parameter modeling the amount of force exerted on the\nsystem, while θ and a are unknown. In terms of the model, the numerator\nof Eq. (13) corresponds to the ideal eﬃciency of the machine, while the\ndenominator corresponds to ineﬃciency (such as loss due to friction).\nSay that we wish to model Eq. (13) using a GP, and that we have observed\na vector of responses y at 11 locations x evenly-spaced in [0.1, 4]. Then we\nbelieve that for each i ∈[11],\nyi = f(xi) + ϵi\n(14)\nwhere f ∼GP(0, k(·, ·)) for some kernel function k and ϵi ∼N(0, σ2) is\nmeasurement noise.\n7\n0\n2\n4\n6\n8\n0\n1\n2\n3\n4\n5\n6\nWork\nRBF Prediction\ntrue dynamics (x)\nprediction mean\nobservations y\n95% confidence interval\n0\n2\n4\n6\n8\nx (Effort)\n0\n1\n2\n3\n4\n5\n6\nCK Prediction\n0\n2\n4\n6\n8\n0\n1\n2\n3\n4\n5\n6\nNTK Prediction\nFigure 1: Predictive distributions of the RBF, CK and NTK trained on\nobservations derived from the dynamics ζ(·).\nWe simulate the dynamics\nyi = ζ(xi | 0.65, 10) + ϵi\n(15)\nfor ϵi ∼N(0, 0.12) and ﬁt GP models for each of kRBF, kCK, and kNTK as\ndeﬁned above. We use Eq. (3) to learn the posterior distributions of each\nGP over a set x∗uniformly spaced in [0.2, 9] and ﬁt the hyperparameters of\neach kernel by way of a simple grid search using the loglikelihood. See [38]\nChapter 2 for a comprehensive review of GP regression.\nFigure 1 plots the means of the resulting distributions and their 95% con-\nﬁdence intervals, along with the true dynamics in blue and the observations\nin red. Note that the RBF GP returns to the prior mean 0 when extrapo-\nlating far from the observed data. This is expected of stationary kernels, as\ninputs that are far apart are assumed to have low correlation. As given in\nEqs. (8) and (9), both the CK and NTK kernels are functions of ⟨x, x′⟩, ∥x∥,\nand ∥x′∥. Thus, they are nonstationary on Rn0. It is worth noting that most\nextant GP applications of CK and NTK use image data that has been nor-\nmalized to the unit hypersphere [21, 26, 15, 1]. In this case, CK and NTK\nare functions of the angle between the unit vectors x and x′, which maps\none-to-one with ∥x −x′∥. Consequently, in the aforementioned applications\nCK and NTK are isotropic. We do not perform normalization nor do we\nembed our data in a higher dimensional hypersphere in this work, meaning\nthat in all cases the CK and NTK kernels are nonstationary.\nThe fact that the posterior means of CK and NTK trend closer to the\ntrue dynamics far from the training data does not imply that these kernels\n8\nare somehow “better” than RBF, but rather that their implicit assumptions\nabout how the data is organized happen to center relatively well on this ex-\nample. More careful accounting of model discrepancy, such as that demon-\nstrated in [4], can produce much better extrapolation. Note that in all cases,\nhowever, the conﬁdence interval grows dramatically as we move further from\nthe observed data. This behavior indicates a low conﬁdence in any projec-\ntions in these data, which provides a good example of what is desired from\nuncertainty quantiﬁcation. The majority of practical problems involve high\ndimensional transformations that are much harder to visualize. Thankfully,\nthe posterior distributions still allow the investigator to detect where pre-\ndictions are uncertain due to the presence of high variance. The rest of this\ndocument concerns itself with such an application to reinforcement learning.\n3\nThe mountain-car reinforcement learning\nproblem\n3.1\nDescription\nThe reinforcement learning problem studied in this paper is the mountain-car\nproblem: a car drives along a mountain track and the objective is to drive\nto the top of the mountain. However gravity is stronger than the engine,\nand even at full thrust the car cannot accelerate up the steep slope. The\nonly way to solve the problem is to ﬁrst accelerate backwards, away from the\ngoal, and then apply full thrust forwards, building up enough speed to carry\nover the steep slope even while slowing down the whole way. Thus, one must\ninitially move away from the goal in order to reach it in the long run. This\nis a simple example of a task whose optimal solution is unintuitive: things\nmust get worse before they can get better. The problem is fully described in\n[29, 35] and is illustrated in Figure 2.\nThe mountain-car dynamical system has two continuous state variables,\nthe position of the car x, and the velocity of the car ˙x. The state s can be\nwritten as s = (x, ˙x). The mountain surface is described by the altitude\nH(x) =\n(\nx2 + x\nif x < 0,\nx\n√\n1+5x2\nif x ≥0.\n(16)\nThe input that the driver can apply is the horizontal force F. Boundary\nconditions are imposed for each of the position, velocity and force of the car\n9\n-1\n1\n-0.5\nV (x, ˙x)\n0.6\nx\nFigure 2: Illustration of the mountain car problem. The car is initially resting\nmotionless at x = −0.5 and the goal is to bring it up and hold it in the region\naround the ﬂag.\nrespectively as follows:\n−1 ≤x ≤1\n−2 ≤˙x ≤2\n−4 ≤F ≤4.\n(17)\nThe initial state s0 = [−0.5, 0]T indicates the car is at the unmoving at\nminimum altitude. This is the also the equilibrium of the dynamics. The\ntarget reward R is a multivariate gaussian PDF with mean (x = 0.6, ˙x = 0)\nand covariance σ2I2 with σ = 0.05. R is plotted in the top of Fig. 4.\nThis choice of instantaneous reward function encodes into the model a\ndesire to be as close as possible to the ﬂag at position x = 6.0 while remaining\nas stationary as possible. The agent’s goal is to ﬁnd the optimal trajectory for\nthe car to maneuver towards and remain near the ﬂag, given the dynamics\nof the environment. The core RL problem here is to ﬁnd a policy for the\ndecision maker (driver/car): a function π that speciﬁes the action F = π(s)\nthat the decision maker will choose when in state s.\nThe standard family of algorithms to calculate this optimal policy con-\nstructs two arrays indexed by state: policy π and value V . Upon completion\n10\nof the algorithm, π(s) speciﬁes the action to be taken in state s, while V (s)\nis the real-valued discounted sum of the rewards to be earned by following\nthat solution from s.\nThis RL algorithm has two steps, (1) a value update and (2) a policy\nupdate, which are iterated across all the states until π and V converge. The\nBellman equation is commonly used to update the value V :\nV (s) =\nZ\nPπ(s)(s, s′)\n\u0002\nRπ(s)(s, s′) + γV (s′)\n\u0003\nds′.\n(18)\nHere γ is the discount factor and satisﬁes 0 ≤γ ≤1, Pπ(s) is the transition\nprobability of going from state s to state s′ when applying action π(s) and\nRπ(s) is the corresponding immediate expected reward. Given a computed\nvalue function V for a given policy π, we can compute an implicitly optimized\nupdate policy π′ as:\nπ(s) = argmaxa\n\u001aZ\ns′ P(s′ | s, a) [R(s′ | s, a) + γV (s′)] ds′\n\u001b\n(19)\nSection 3.2 explains the algorithm in detail. The main idea is that we iterate\nthe process of evaluating V for a given policy π over the continuous state\nspace using Eq. (18) and then recompute the policy using Eq. (19).\n3.2\nAlgorithmic Implementation\nOur algorithm is a generalization of the algorithm described in [19] which\nis able to accommodate the three diﬀerent kernels described in the previous\nsection while maintaining computational eﬃciency. It proceeds by ﬁrst ini-\ntializing the dynamics of the model and value function, then iterating over\nupdating the value and policy until convergence. We model the dynamics\nusing GPs. In doing so, we explicitly solve the dynamics for a small number\nof observed position/velocity states, then train GPs to interpolate the state\nevolution of unobserved states. Similarly, we use a separate GP to model\nthe value function at a small number of position/velocity states, each with a\nsmall number uniform sample forces. We iterate over this GP, applying inter-\npolation to update the learned policy which we in turn use to update the GP.\nWe use the OpenAI Gym software [3] to model the dynamics environment.\n11\nInitialization of the dynamics\nThe ﬁrst step is to train a GP to predict\nthe dynamics of the system. The dynamical equation is\nd\ndt\n\n\nx\n˙x\nF\n\n=\n\n\n˙x\nF −G · sin(arctan(H′(x)))\n0\n\n.\n(20)\nHere G is the gravitational constant and H′ is the derivative of the altitude\ngiven in Eq. (16) with respect to x. Given a state s, we integrate Eq. (20)\nforward in time over a span ∆t of 0.3 s to obtain the corresponding next state\ns′. For training we take Nd = 128 random 3D states si chosen uniformly in\nthe domain deﬁned by (17) and we compute their corresponding next states\ns′\ni. We use these s - s′ pairs as observations to train two GPs, one for x and\none for ˙x. We can then utilize Eq. (3) to interpolate the dynamics evolution\nat unobserved states. We assume the hyperparameters of both CK and NTK\nto be distributed according to an inverse-Gamma distribution.\nWe use a\nMonte Carlo Markov Chain technique to ﬁt them by minimizing the mean\nsquare error when predicting the dynamics. See [38] for a nuanced discussion\nof hyperparameter optimization. Both kernels provide comparable accuracy\nfor predicting the dynamics once trained and tuned.\nInitialization of the value function\nNext we must train a GP to pre-\ndict the value function of any given state. The procedure is iterative so we\nuse the reward R as the initial value function. As with the dynamics, we\ntake a certain number (NV = 512) of random states in the 3D domain of\n(sj, F) uniformly from the domain (17) and associate them with their corre-\nsponding initial value (≡R) to provide training samples. Note that contrary\nto [38], we train that GP using the full 3D state (x, ˙x, F) as input rather\nthan omitting F for reasons that will be apparent in the description of the\niterations below. We tune the hyperparameters of the value GPs using the\nsame MCMC procedure applied to the dynamics.\nIteration of the value and policy\nOnce all the GPs are trained and\ntuned, we can start iterating to update the value GP and the policy until the\nvalue function converges to a ﬁxed point. For each state sj in the dynamics\ntraining set, we generate a sequence of NF = 128 states sk = (xk, ˙xk, Fk)\nwhere ∀k, xk = xj, ˙xk = ˙xj, and the actions Fk are uniformly spaced and\ncover the entire F domain. Then we use the dynamics GPs to predict their\n12\nrespective next states s′\nk as the posterior means of Eq. (3). s′\nk then serves as\ninput to the value GP to predict Vk, again as the posterior mean of Eq. (3).\nWe can then compute V max\nj\n= maxk Vk and kmax = arg maxk Vk and deduce\nthe policy π(sj) = Fkmax. Finally we can update the value associated with\neach sj using Vj ←R(sj) + γ · V max\nj\n.\nOnce the value has been updated, we retrain the value GP. We repeat\nthis procedure until the value stops evolving. Once it does we output the\noptimal policy π.\n3.3\nResults\nWe show that GPs using either both the CK and NTK as their kernel func-\ntions are suitable for solving the mountain car reinforcement learning prob-\nlem. The estimated dynamics are predicted with suﬃcient accuracy to enable\nthe iterative evolution of the value function.\nFigure 3 shows the comparison between the dynamics derived from the\nphysics (the truth) and the dynamics predicted using CK Gaussian process\nmodeling. We can see that our dynamical model is very close to the reality.\nIt captures the main features and equilibria of the dynamical system.\nFigure 4 shows the initial value (top) which is the instant reward, a\nGaussian function centered around the target at (x, ˙x) = (0.6, 0) with a small\nstandard deviation of 0.05, and the ﬁnal value function for CK (middle)\nand NTK (bottom). For both kernels, the value function converges in six\niterations. The value expands diagonally from the target to regions where\nthe velocity is high enough to overcome the steep slope and ﬁnally curves\nback to reach the car’s initial position from the left, leading to the non-\ntrivial but correct policy that the car should start by going backward before\nspeeding up the slope. The value function does not increase from zero in\nthe central region of the phase space, which corresponds to the invalid policy\nof attempting to climb the slope of the mountain in the positive x-direction\nwithout suﬃcient momentum.\nFigure 5 illustrates the learned optimal trajectories of the car along the\nx-axis over time. Again for both kernels, the car ﬁrst moves backward then\nspeeds up to quickly reach the target, where it stays indeﬁnitely. The os-\ncillations of the car around the target location (x = 0.6) are the result of\ncompounding errors in the GP predictions of the dynamics and the value.\nThese oscillations can most likely be reduced by adding more training points,\nboth for the dynamics and the value, but this comes at the cost of additional\n13\nposition (x)\nvelocity ( ˙x)\nFigure 3: True (black) and predicted (red) dynamics as a function of x and\n˙x (for F = 0). Each arrow represents a state s (base of the arrow) and\npoints in the direction of its next state s′ 0.3 s in the future. The arrow\nlengths are scaled down so as not to overlap.\nThe stable equilibrium at\n(−0.5, 0) corresponds to the bottom of the valley. The target at (0.6, 0) is\nunstable requiring a sustained force F > 0 to maintain the car at the target.\nThe discontinuity in the upper right of the phase plot is due to boundary\nconditions: hitting the boundary of the domain brings the velocity to zero.\ncomputation.\nIn our training of the GP representation of the value function, we see\nthere are clear regions of interest in the value function that change with each\ntraining iteration.\nThis indicates that choosing training points uniformly\nin the entire domain and keeping the same points throughout iterations is\nsuboptimal. It would be better to sample more points where the value is\nhigher so as to achieve better resolution in this region. In other words, we\n14\nposition (x)\nvelocity ( ˙x)\nposition (x)\nvelocity ( ˙x)\nposition (x)\nvelocity ( ˙x)\nFigure 4: Initial reward (top) and ﬁnal value function for CK (bottom left)\nand NTK (bottom right) as a function of x and ˙x for F = 0.\nexpect to see improved numerical performance by converting the value func-\ntion into a probability density function for sampling training points followed\nby resampling the points after each policy iteration step to accommodate\nthe changing value function. This should allow more accurate predictions\nwithout the performance cost of adding more training points.\n4\nPerspectives and conclusion\nWe have shown that GP kernels that are dual descriptions of neural networks\nare suitable for solving a simple reinforcement learning problem. The kernels\nwe use here have been shown to perform well for GP classiﬁcation tasks [e.g.,\n21], but we believe our result is the ﬁrst application of such kernels for GP\n15\n0\n2\n4\n6\n8\n10\ntime (s)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.60\n1.00\nx-position\nCK\nNTK\nFigure 5: Evolution of the x position over time. The car goes backward\ninitially then speeds up the hill.\nregression in a non-trivial problem. We have also improved the GP model\nfor the value function from those models presented in the literature [i.e.,\n19] to increase the computational eﬃciency of the policy iteration step by\ndecreasing the number of sample points in the combination of phase space\nand possible actions. We are able to achieve this performance improvement\nbecause of the improved expressivity of the GP regression in the combined\nsample space.\nWhile this simple mountain-car RL problem turns out to be easily soluble\nwith GPs utilizing the classic RBF kernel, we have shown that neural net-\nwork dual kernels deliver similar performance. Furthermore, we expect that\nmore challenging RL problems that have beneﬁtted from neural networks for\nmodeling the dynamics and the value function may also beneﬁt in the fu-\nture from the GP dual description of those networks [e.g., 28]. In particular,\nRL problems relying on computer vision may beneﬁt from application of the\nconvolution version of the CK or NTK [1]. Additionally, the ongoing de-\nvelopment of kernels dual to arbitrary architectures opens up the possibility\nof taking advantage of recurrent neural network expressivity within the GP\nparadigm [39]. The GP dual to neural networks applied to RL thus oﬀers\npromise of incorporating recent advances in deep RL with the probabilistic\n16\nmodeling features of GPs. Such applications also elude the grasp of more\nconventional GP models in the current literature due to the expressivity lim-\nitations of known kernels, especially on high-dimensional data.\nWe also have not fully exploited the value in utilizing dual GPs. All of the\npredictions given throughout this document utilize only the posterior mean\nof Eq. (3) for prediction. In this sense, we might as well have actually used\noverparameterized DNNs for prediction. We expect that knowledge of the\nposterior variance will be greatly beneﬁcial in more advanced RL problems\nwhere dynamics and value function propagation involves more uncertainty.\nWe will incorporate applications of uncertainty quantiﬁcation into future\nwork.\nAcknowledgments\nThis work was performed under the auspices of the U.S. Department of\nEnergy by Lawrence Livermore National Laboratory under Contract DE-\nAC52-07NA27344. Funding for this work was provided by LLNL Laboratory\nDirected Research and Development grant 19-SI-004.\nReferences\n[1] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov,\nand Ruosong Wang. On exact computation with an inﬁnitely wide neural\nnet. arXiv preprint arXiv:1904.11955, 2019.\n[2] Sanjeev Arora, Simon S Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong\nWang, and Dingli Yu. Harnessing the power of inﬁnitely wide deep nets\non small-data tasks. arXiv preprint arXiv:1910.01663, 2019.\n[3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,\nJohn Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.\n[4] Jenn`y Brynjarsd´ottir and Anthony O’Hagan. Learning about physical\nparameters: The importance of model discrepancy. Inverse problems,\n30(11):114007, 2014.\n17\n[5] Lucian Busoniu, Robert Babuska, Bart De Schutter, and Damien Ernst.\nReinforcement learning and dynamic programming using function ap-\nproximators. CRC press, 2017.\n[6] Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning.\nIn Advances in neural information processing systems, pages 342–350,\n2009.\n[7] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper under-\nstanding of neural networks: The power of initialization and a dual view\non expressivity. In Advances In Neural Information Processing Systems,\npages 2253–2261, 2016.\n[8] Marc Peter Deisenroth, Dieter Fox, and Carl Edward Rasmussen. Gaus-\nsian processes for data-eﬃcient learning in robotics and control. IEEE\ntransactions on pattern analysis and machine intelligence, 37(2):408–\n423, 2013.\n[9] Marc Peter Deisenroth, Carl Edward Rasmussen, and Jan Peters. Gaus-\nsian process dynamic programming.\nNeurocomputing, 72(7-9):1508–\n1524, 2009.\n[10] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approxima-\ntion: Insights and applications. In Deep Learning Workshop, ICML,\nvolume 1, page 2, 2015.\n[11] Jacob Gardner, GeoﬀPleiss, Kilian Q Weinberger, David Bindel, and\nAndrew G Wilson. Gpytorch: Blackbox matrix-matrix gaussian process\ninference with gpu acceleration.\nIn Advances in Neural Information\nProcessing Systems, pages 7576–7586, 2018.\n[12] Adri`a Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchi-\nson. Deep convolutional networks as shallow gaussian processes. arXiv\npreprint arXiv:1808.05587, 2018.\n[13] Matthew J Heaton, Abhirup Datta, Andrew O Finley, Reinhard Fur-\nrer, Joseph Guinness, Rajarshi Guhaniyogi, Florian Gerber, Robert B\nGramacy, Dorit Hammerling, Matthias Katzfuss, et al. A case study\ncompetition among methods for analyzing large spatial data. Journal\nof Agricultural, Biological and Environmental Statistics, 24(3):398–425,\n2019.\n18\n[14] Geoﬀrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel-rahman Mo-\nhamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick\nNguyen, Brian Kingsbury, et al.\nDeep neural networks for acoustic\nmodeling in speech recognition. IEEE Signal processing magazine, 29,\n2012.\n[15] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent\nkernel: Convergence and generalization in neural networks. In Advances\nin neural information processing systems, pages 8571–8580, 2018.\n[16] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation\nmodels. In Proceedings of the 2013 Conference on Empirical Methods in\nNatural Language Processing, pages 1700–1709, 2013.\n[17] Jonathan Ko, Daniel J Kleint, Dieter Fox, and Dirk Haehnelt. Gp-ukf:\nUnscented kalman ﬁlters with gaussian process prediction and observa-\ntion models. In 2007 IEEE/RSJ International Conference on Intelligent\nRobots and Systems, pages 1901–1907. IEEE, 2007.\n[18] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet clas-\nsiﬁcation with deep convolutional neural networks. In Advances in neural\ninformation processing systems, pages 1097–1105, 2012.\n[19] Malte Kuss and Carl E Rasmussen. Gaussian processes in reinforcement\nlearning. In Advances in neural information processing systems, pages\n751–758, 2004.\n[20] Neil Lawrence. Probabilistic non-linear principal component analysis\nwith gaussian process latent variable models. Journal of machine learn-\ning research, 6(Nov):1783–1816, 2005.\n[21] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jef-\nfrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as\ngaussian processes. In International Conference on Learning Represen-\ntations, 2018.\n[22] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri,\nJascha Sohl-Dickstein, and Jeﬀrey Pennington. Wide neural networks of\nany depth evolve as linear models under gradient descent. arXiv preprint\narXiv:1902.06720, 2019.\n19\n[23] Frank L Lewis, Draguna Vrabie, and Kyriakos G Vamvoudakis. Rein-\nforcement learning and feedback control: Using natural decision meth-\nods to design optimal adaptive controllers. IEEE Control Systems Mag-\nazine, 32(6):76–105, 2012.\n[24] Haitao Liu, Yew-Soon Ong, Xiaobo Shen, and Jianfei Cai. When gaus-\nsian process meets big data: A review of scalable gps. IEEE Transactions\non Neural Networks and Learning Systems, 2020.\n[25] Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid.\nConvolutional kernel networks. In Advances in neural information pro-\ncessing systems, pages 2627–2635, 2014.\n[26] Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E\nTurner, and Zoubin Ghahramani. Gaussian process behaviour in wide\ndeep neural networks. In Internation Conference on Learning Represen-\ntation, 2018.\n[27] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioan-\nnis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari\nwith deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n[28] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,\nJoel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, An-\ndreas K Fidjeland, Georg Ostrovski, et al. Human-level control through\ndeep reinforcement learning. Nature, 518(7540):529, 2015.\n[29] Andrew W Moore.\nThe parti-game algorithm for variable resolution\nreinforcement learning in multidimensional state-spaces. In Advances in\nneural information processing systems, pages 711–718, 1994.\n[30] Roderick Murray-Smith and Daniel Sbarbaro. Nonlinear adaptive con-\ntrol using nonparametric gaussian process prior models. IFAC Proceed-\nings Volumes, 35(1):325–330, 2002.\n[31] Anusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey\nLevine. Neural network dynamics for model-based deep reinforcement\nlearning with model-free ﬁne-tuning. In 2018 IEEE International Con-\nference on Robotics and Automation (ICRA), pages 7559–7566. IEEE,\n2018.\n20\n[32] Radford M Neal. Priors for inﬁnite networks. In Bayesian Learning for\nNeural Networks, pages 29–53. Springer, 1996.\n[33] Duy Nguyen-Tuong and Jan Peters. Local gaussian process regression for\nreal-time model-based robot control. In 2008 IEEE/RSJ International\nConference on Intelligent Robots and Systems, pages 380–385. IEEE,\n2008.\n[34] Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang,\nDan Abolaﬁa, Jeﬀrey Pennington, and Jascha Sohl-dickstein. Bayesian\ndeep convolutional neural networks with many channels are gaussian\nprocesses.\nIn International Conference on Learning Representation,\n2019.\n[35] Richard S Sutton and Andrew G Barto. Reinforcement learning: An\nintroduction. MIT press, 2018.\n[36] Kyriakos G Vamvoudakis, Hamidreza Modares, Bahare Kiumarsi, and\nFrank L Lewis. Game theory-based control system algorithms with real-\ntime reinforcement learning: How to solve multiplayer games online.\nIEEE Control Systems Magazine, 37(1):33–52, 2017.\n[37] Ke Wang, GeoﬀPleiss, Jacob Gardner, Stephen Tyree, Kilian Q Wein-\nberger, and Andrew Gordon Wilson.\nExact gaussian processes on a\nmillion data points. In Advances in Neural Information Processing Sys-\ntems, pages 14622–14632, 2019.\n[38] Christopher KI Williams and Carl Edward Rasmussen. Gaussian pro-\ncesses for machine learning, volume 2.\nMIT Press Cambridge, MA,\n2006.\n[39] Greg Yang.\nTensor programs i: Wide feedforward or recurrent neu-\nral networks of any architecture are gaussian processes. arXiv preprint\narXiv:1910.12478, 2019.\n[40] Greg Yang and Hadi Salman. A ﬁne-grained spectral perspective on\nneural networks. arXiv preprint arXiv:1907.10599, 2019.\n[41] Yuanheng Zhu and Dongbin Zhao. Comprehensive comparison of online\nadp algorithms for continuous-time optimal control. Artiﬁcial Intelli-\ngence Review, 49(4):531–547, 2018.\n21\n[42] Barret Zoph and Quoc V. Le. Neural architecture search with reinforce-\nment learning. CoRR, abs/1611.01578, 2016.\n22\n",
  "categories": [
    "cs.LG",
    "cs.SY",
    "eess.SY",
    "stat.ML"
  ],
  "published": "2020-04-10",
  "updated": "2020-04-10"
}