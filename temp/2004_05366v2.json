{
  "id": "http://arxiv.org/abs/2004.05366v2",
  "title": "In-Machine-Learning Database: Reimagining Deep Learning with Old-School SQL",
  "authors": [
    "Len Du"
  ],
  "abstract": "In-database machine learning has been very popular, almost being a cliche.\nHowever, can we do it the other way around? In this work, we say \"yes\" by\napplying plain old SQL to deep learning, in a sense implementing deep learning\nalgorithms with SQL. Most deep learning frameworks, as well as generic machine\nlearning ones, share a de facto standard of multidimensional array operations,\nunderneath fancier infrastructure such as automatic differentiation. As SQL\ntables can be regarded as generalisations of (multi-dimensional) arrays, we\nhave found a way to express common deep learning operations in SQL, encouraging\na different way of thinking and thus potentially novel models. In particular,\none of the latest trend in deep learning was the introduction of sparsity in\nthe name of graph convolutional networks, whereas we take sparsity almost for\ngranted in the database world. As both databases and machine learning involve\ntransformation of datasets, we hope this work can inspire further works\nutilizing the large body of existing wisdom, algorithms and technologies in the\ndatabase field to advance the state of the art in machine learning, rather than\nmerely integerating machine learning into databases.",
  "text": "In-Machine-Learning Database:\nReimagining Deep Learning with Old-School SQL\nLen Du\nAustralian National University\nlen.du@anu.edu.au\nABSTRACT\nIn-database machine learning has been very popular, almost\nbeing a cliche. However, can we do it the other way around?\nIn this work, we say yes by applying plain old SQL to deep\nlearning, in a sense implementing deep learning algorithms\nwith SQL.\nMost deep learning frameworks, as well as generic ma-\nchine learning ones, share a de facto standard of multidi-\nmensional array operations, underneath fancier infrastruc-\nture such as automatic diﬀerentiation. As SQL tables can\nbe regarded as generalisations of (multi-dimensional) arrays,\nwe have found a way to express common deep learning oper-\nations in SQL, encouraging a diﬀerent way of thinking and\nthus potentially novel models. In particular, one of the lat-\nest trend in deep learning was the introduction of sparsity in\nthe name of graph convolutional networks, whereas we take\nsparsity almost for granted in the database world.\nAs both databases and machine learning involve trans-\nformation of datasets, we hope this work can inspire further\nworks utilizing the large body of existing wisdom, algorithms\nand technologies in the database ﬁeld to advance the state of\nthe art in machine learning, rather than merely integerating\nmachine learning into databases.\n1.\nINTRODUCTION\nBoth machine learning and databases obviously involve\ntransformation of (or computation over) collections of num-\nbers. Combining the two ﬁelds is then an obvious conclusion.\nBut the way of such fusion seems to have been unilateral.\nMuch more eﬀort has been spent towards providing machine\nlearning capabilities in a database context, or so-called In-\nDatabase Machine Learning [18], compared to integeration\nin the opposite direction, which we call In-Machine-Learning\nDatabase.\nWe speculate that the connotation of databases has been\nmore towards systems than towards algorithms, compared to\nthat of machine learning, making it seemingly more natural\nto apply the latter to the former. Modern machine learning,\nin particular deep learning, has been growing into expan-\nsive software systems as well, which suggests us to seriously\nconsider the reverse.\nIn this work we get back at the basic (or not so basic)\nnotion of transforming collections of numbers and try sub-\nstituting the typical operations in machine learning with the\nmost prominent tool in databases, i.e. SQL, to see whatever\nnovel we can ﬁnd under this diﬀerent perspective.\n2.\nRELATED WORK\nIn this section, we review some representative works con-\nnecting the two ﬁelds of machine learning (in particular,\ndeep learning) and databases, so that we can position this\nwork properly in the whole data science landscape. In par-\nticular, reviewing these works helps us with a bird’s-eye\nview of why the relational model, having been ubiquitous\nin databases since the beginning of the ﬁeld, should still\ninterest those at the tip of deep learning research.\n2.1\nMachine Learning in Databases\nMADlib [10] is probably the apex of the classical approach\nwhere machine learning subroutines are provided as black-\nboxes in SQL. MADlib also focuses on conventional machine\nlearning rather than deep learning.\nSciDB [30, 29] substitutes relational tables with multidi-\nmensional arrays. In-database linear algebra and analytics\ncan then be added, resulting in a crossover between a nu-\nmerical library and a database.\nTensor-Relational Model [14] is an elaborated treatise on\nthe role of multidimensional arrays in relational databases.\nMLog [18] provides a domain-speciﬁc language designed\nfor deep learning. The MLog language is integerated into\nRDBMS by mixing with SQL. It operates on multidimen-\nsional arrays (tensors) rather than relational tables. The im-\nplementation compiles MLog into TensorFlow[1] programs.\nIn [25], array operations, automatic diﬀerentiation and\ngradient descent are implemented via SQL extensions.\n[33] envisioned some possible ways to enhance database\nfunctionality with deep learning, beyond ease of access of\ndeep learning in databases.\nA strong argument favouring in-database machine learn-\ning is that databases are often mature distributed systems,\nso distributed machine learning would supposedly require\nlittle extra eﬀort on the user in a database setting.\n[20]\nexplores such a setting.\n2.2\nDatabaseFunctionalityinGeneralMachine\nLearning Settings\n1\narXiv:2004.05366v2  [cs.LG]  14 Apr 2020\nDespite claimed as an in-database framework, AIDA [5,\n4] provides a client interface to a SQL server in the Python\nlanguage, which is the de facto standard in the machine\nlearning world, AIDA also shifts some of the computation\nto the server side, or more precisely, a Python interpreter\nembedded in the database server. So AIDA is best under-\nstood as implementing (low-level computation of) machine\nlearning in a database, and then providing the augmented\ndatabase to machine learning to the user.\nML2SQL [26, 27] compiles a uniﬁed declarative domain-\nspeciﬁc language to both database operations in SQL and\nML-style array operations in python.\nSystemML [3] and its successor SystemDS [2] also provide\na uniﬁed language, but they use non-relational databases.\nTensorLog [12] implements probabilistic logic, essential to\nprobabilistic databases, over typical deep learning infras-\ntructure.\nIn addition to machine learning in databases, [33] also\nenvisions providing system-level facilities and distributed\ncomputation developed in the database community to deep\nlearning.\nFinally, the Pandas [19] library familiar to data scien-\ntist already provides some essential relational functionalities\nsuch as JOIN and SELECT.\n2.3\nNeural Networks Designed for Relational\nModels\nThere are also neural networks speciﬁcally designed for\nlearning relations. [32] summerizes very well the eﬀort in\nthis regard before the ”deep learning takeover”, including\nGraph Neural Networks (GNN) [23], and Relational Neural\nNetworks [31].\nIn the more recent surge of deep learning, [22] explores a\ngeneral deep learning architecture whose outputs are rela-\ntions, with applications to understanding scenes. [21] com-\nbines relational reasoning with recurrent neural networks.\n[35] further applies the architecture in [22] to complex re-\ninforcement learning tasks. [13] employs a modiﬁed logistic\nregression over hidden layers to learn relations. Lifted Rela-\ntional Neural Networks [28] combines ﬁrst-order logic with\nneural networks to learn relational structures.\nNote that all of these models designed to learn relations,\nwhile worth mentioning, overlap little with our claim that\nrelational-model-based SQL can be used as building blocks\nfor general deep learning.\n2.4\nRelational Models versus Graph Convolu-\ntional Networks\nEven being the ”fanciest of the fanciest” topic in Machine\nLearning, Graph convolutional network [16] (GCN) can’t\nescape the link to relational models [24].\n[22] advocates\nrelating relational models and graph convolutional networks,\nas well as deep learning in general, with extensive review.\nOne interesting fact about GCNs is that GPUs no longer\nmake the usual vast speedups. Even without consideration\nof relations, GPUs failed to accelerate beyond one order of\nmagnitude [16]. We speculate that it is something inherent\ngiven the underlying sparsity, posing the same challenge to\ndeep learning and databases alike.\nApparently, edges of graphs are relations. But relations\nare not always edges – they could be hyperedges!\nFrom\nthe point of view of the relational people, it is really a no-\nbrainer that we could have hypergraph variants of GCNs. [7]\ndiscusses them without addressing relational models while\n[34] and [11] address both relational models and hypergraph\nneural networks.\n3.\nARCHITECTURE\nIn this part, we give a big picture of our proposed way\nof doing deep learning with SQL. While the meaning of\ndeep learning may not be exact enough to prevent inten-\ntionally creating a counterexample to our arguments, ac-\ntual instances of deep learning almost universally follow the\nstructures described here, at least in a practical, computa-\ntional sense.\n3.1\nThe (Usual) Way of Deep learning\nA deep learning model can usually be regarded as a scalar\nfunction {(D, P), where D denotes a set of inputs (data) and\nP denotes the model parameters. Our hypothetic goal is to\nﬁnd\nargmin\nP\n{(D0, P)\nwhere D0 can be interpreted as either the set of all possible\ninputs or a test set.\nThe global minimization is usually intractable.\nSo the\nlearning process involves some iterative optimization involv-\ning the gradients ∂{(D, P)\n∂P\n. The iterative optimization takes\nthe steps shown in Algorithm 1.\nThis is the most basic\nLoad training set as D ;\nRandomly initialize P ;\nwhile not meeting stopping criteria do\n(1) Evaluate {(D, P) ;\n(2) Evaluate ∂{(D, P)\n∂P\n(taken care of by automatic\ndiﬀerentiation; not necessarily mathematically\nprecise) ;\n(3) Update P with a new value computed with the\nold P and ∂{(D, P)\n∂P\n(may carry over state from\nprevious iterations) ;\nend\nAlgorithm 1: Typical deep learning control ﬂow\npattern. Some deep learning algorithms follow alternative\nversions. In real world scenarios, it is often only possible\nto train with stochastic gradient descent which follows the\ngeneral pattern outlined in Algorithm 2, where only a subset\nof D is picked for training each iteration. A recurrent neural\nLoad D ; Initialize P ;\nwhile not meeting stopping criteria do\n(1) Evaluate {(D′, P) where D′ ∈D (chosen per\niteration) ;\n(2) Evaluate ∂{(D, P)\n∂P\n;\n(3) Update P ;\nend\nAlgorithm 2: Deep learning control ﬂow that stochastic\ngradient descent uses\nnetwork predicting the next symbol given a preﬁx would be\n2\ntrained with Algorithm 3, in a self-supervised fashion, where\nthe samples are encoded one megasequence of vectors S.\nLoad S ; Initialize P ;\nwhile not meeting stopping criteria do\n(1) Evaluate {(S, P):\n(1a) Initialize hidden states H ( typically with\nzeroes );\n(1b) for S′ (embeddings of) sub-sequence of S do\n(1b1) Evaluate network output (embeddings of\npredicted sub-sequence) S′′ and update hidden\nstates H with (S′′, H) ←f1(S′, H, P) ;\n(1b2) Evaluate per-sub-sequence loss f2(S′′, S)\nby comparing prediction S′′ and the\ncorresponding (embeddings of) sub-sequence of\nS ;\nend\n(1c) compute total loss {(S, P) by summing or\naveraging all per-sub-sequence losses ;\n(2) Evaluate ∂{(S, P)\n∂P\n;\n(3) Update P ;\nend\nAlgorithm 3: Control ﬂow for training recurrent neural\nnetworks\nEven unconventional uses of deep learning are not so un-\nconventional in terms of control ﬂows. Neural style transfer\n[8] follows the pattern in Algorithm 4, essentially just switch-\ning the argument in Algorithm 1 from P to D. Note that\nLoad content image I′ and style image I′′ ;\nInitialize image I ( maybe randomly but usually\nI ←I′);\nLoad (pre-trained) P ;\nwhile not meeting stopping criteria do\n(1) Evaluate {(I, I′, I′′, P) ;\n(2) Evaluate ∂{(I, P)\n∂I\n;\n(3) Update I ;\nend\nAlgorithm 4: Control ﬂow for neural style transfer\nthe content image I′ and style image I′′ are never modiﬁed\nonce loaded. In practice, it is often possible to leave out I′\nin the iterative optimization altogether so long as I′ is used\nas the initial value of I [6]. Adversial example generation,\nlike fast gradient sign attack [9], works in a very similar way\nby perturbing D to maximize {(D, P) in Algorithm 1 rather\nthan perturbing P to minimize it. The phenomenal genera-\ntive adversarial network (GAN) pipes two ordinary networks\nwith parameter sets P1 and P2 together and run two opti-\nmizations in lockstep as shown in Algorithm 5. Function f1\nalong with parameters P1 is the so-called generator network\nproducing fake samples given noise as input, while the dis-\ncriminator network with parameters P2 trying work out a\nscore for each of both these fake samples and the real ones\ngiven as the training set. Then, one number representing\nhow well the scores separate the two types of samples is\nsummarized from the scores. Finally the two sets of param-\neters are optimized with respect to this number, albeit with\nInitialize P1, P2;\nLoad true samples D;\nwhile not meeting stopping criteria do\n(1) Evaluate {(D, N, P1, P2) = f2(D, f1(N, P1), P2)\nwhere N is some kind of noise ;\n(2) Evaluate ∂{(D, N, P1, P2)\n∂P1\nand\n∂{(D, N, P1, P2)\n∂P2\n;\n(3) Update P1 (maximizing) and P2 (minimizing)\naccording to respective gradients;\nend\nAlgorithm 5: Control ﬂow for generative adversarial\nnetworks\nopposite signs. Surely the order of steps (2) and (3) does\nnot matter.\nWhile there could be other ways to code a deep learning\nprogram, the pattern is quite clear.\nThe control ﬂows of\ndeep learning programs are relatively simple, whereas the\nbulk of the eﬀort are distributed to the design of the models,\nmanifesting primarily in Step (1) of each example, among\nthe 3 major steps conveniently partitioned out of the main\nloops.\nAs for Step (2) and (3), there is a separation of concern\nhere. Pragmatically, a major breakthrough that enabled the\nexplosive progress of deep learning is the automatic diﬀeren-\ntiation. While still an active ﬁeld of research, development\nof new deep learning models can be separated from study-\ning automatic diﬀerentiation (Step (2)) itself. We can mix\nand match diﬀerent ﬂavors of control ﬂows with diﬀerent\noptimization algorithms, or more precisely, diﬀerent update\nstrategies (Step (3)). While some combinations work better\nthan others, in general inventors of new deep learning mod-\nels do not concern themselves with which update strategy\nto pick until tuning the performance of the model.\n3.2\nTensor to Relations and back again\nModern deep learning infrastructure has been almost uni-\nversally built upon array-oriented programming paradigms.\nIn this work, we concern ourselves with expressing the deep\nlearning model { in (a very limited set of) SQL.\n4.\nDL-IN-SQL BY EXAMPLES\nWhile we are far from a formal proof that SQL can ex-\npress every possible deep learning model because of the ob-\nvious lack of precise deﬁnition of the latter, we nevertheless\ndemonstrate how the bread-and-butter constructs of deep\nlearning can be expressed in SQL.\nHere, we use an example deep learning task in stark con-\ntrast to typical database-related ones, to demonstrate that\nour architechture is really geared towards deep learning in\ngeneral. We will introduce how frequent layers can be cast\ninto SQL along the way. Without further ado, we begin the\ndemonstration.\n4.1\nImage Classiﬁer Convolutional Networks\nIn this example, we demonstrate how to specify a con-\nvolutional neural network for computer vision in SQL. The\nmodel takes N sample images together as input, where N\nvaries depending on how the model is used. The model is\n3\n𝑁×3×32×32\narray\n(𝑁 sample \nimages)\nconv1\nrelu1\n𝑁×6×28×28\narray\n6×3×5×5\narray\n(convolution \nkernel)\n6-element\narray\n(biases)\n𝑁×6×28×28\narray\npool1\n(2×2)\n𝑁×6×14×14\narray\nconv2\n16×6×5×5\narray\n(convolution \nkernel)\n16-element\narray\n(biases)\n𝑁×16×10×10\narray\nrelu2\n𝑁×16×10×10\narray\npool2\n(2×2)\n𝑁×16×5×5\narray\nﬂatten\n𝑁×400\narray\nfull-connect-1\n120×400 \narray \n(weights)\n120-element\narray\n(biases)\n𝑁×120\narray\nrelu3\n𝑁×120\narray\nfull-connect-2\n84×120\narray \n(weights)\n84-element\narray\n(biases)\n𝑁×84\narray\nrelu4\n𝑁×84\narray\nfull-connect-3\n10×84\narray \n(weights)\n10-element\narray\n(biases)\n𝑁×10\narray\n(prediction\nscores)\n𝑁-element\ninteger array\n(class labels)\ncross-entropy\nloss\n(scalar)\nFigure 1: Structure of the CNN for classifying images. Steps\nof computation are framed , while their inputs and outputs\nare not. Data (parts of D) are marked in red, while param-\neters (parts of P) are marked in blue.\nﬁxed for 10 classes, and 32×32 RGB images. Computation-\nally, the neural network displays the structure illustrated in\nFigure 1. To make things crystal clear, we have drawn all\nparameters of the network explicitly, so each step of com-\nputation in a box does not contain any states. This is quite\ndiﬀerent from many illustrations found elsewhere. For in-\nstance, in deep learning jargon, the ﬁrst convolutional layer\nwould conceptually include both conv1 in the box and the\ntwo parameter arrays (kernel and biases) marked in blue,\nusually not shown explicitly in diagrams.\nNow, we essentially need to express the boxed steps of\ncomputation in SQL, with data D in red and paramters P\nin blue given as SQL tables.\nFirst and foremost, let us see what we can do with the\nconvolution step conv1 . In the SQL context, we provide\nthe 4-dimensional (N ×3×32×32) array of N sample images\nas a relation samples with the following 5 columns.\nimage, channel, r, c\nINTEGER\nval\nREAL\nThe names and types of the columns should be quite self-\nexplanatory. The column image refers to indices in the ﬁrst\ndimension of the original 4D array, taking the values from 0\nto (N −1) (inclusive). Similarly, the column channel refers\nto which one of the three (RGB) channels (2nd dimension),\nwhile r and c refer to which row (3rd dimension) and which\ncolumn (4th dimension) respectively. The column val stores\nthe actual values in the array, obviously.\nSimilarly, the the 6×3×5×5 array of the convolution ker-\nnel is presented as a relation conv1 weight with 5 columns.\nout channel, in channel, r, c\nINTEGER\nweight\nREAL\nAnd the biases to the convolutional layer corresponds to a\n2-column relation conv1 bias.\nout channel\nINTEGER\nbias\nREAL\nWith the input relation ready, we can execute the compu-\ntation of conv1 with CREATE TABLE commands. We do this\nin two steps. Firstly, we put the results of the convolution\nitself into conv1 unbiased.\nCREATE\nTABLE\nconv1_unbiased AS\nSELECT\nimage ,\nout_channel AS channel ,\nsamples.r-conv1_weight .r AS r1 ,\nsamples.c-conv1_weight .c AS c1 ,\nSUM(val * weight) AS val\nFROM samples , conv1_weight\nWHERE\nchannel = in_channel\nAND\nr1 BETWEEN 0 AND 32-5 AND\nc1 BETWEEN 0 AND 32-5\nGROUP BY image ,channel ,r1 ,c1;\nThen we apply the biases to get conv1 out.\nCREATE\nTABLE\nconv1_out AS\nSELECT\nimage , channel , r1 AS r, c1 AS c,\nval + bias AS val\nFROM\nconv1_unbiased ,conv1_bias\nWHERE\nchannel = out_channel;\nThe ReLU layer relu1 is embarrassingly simple to com-\npute.\nCREATE\nTABLE\nrelu1_out AS\nSELECT\nimage , channel , r, c, MAX(0,val) AS val\nFROM\nconv1_out\nExecuting max-pooling ( pool1 ) is also straight-forward.\nCREATE\nTABLE\npool1_out AS\nSELECT\nimage , channel , r/2 AS r, c/2 AS c,\nMAX(val) AS val\nFROM\nrelu1_out\nGROUP BY image ,channel , r, c;\nNow the remaining computation steps up till ﬂatten are\nalmost identical to what we have listed except for table\n4\nnames and array dimensions. We skip them to assume hav-\ning evaluated the output of pool2 .\nﬂatten is almost as\nsimple as ReLU.\nCREATE\nTABLE\nflatten_out AS\nSELECT\nimage , (channel * 5 + r)*5+c AS i, val\nFROM\npool2_out;\nA fully-connected layer like fc1 is treated just like a con-\nvolution layer, only simpler. The weights and biases are put\nin the SQL context as fc1_weight with\nout dim, in dim\nINTEGER\nweight\nREAL\nand fc1_bias with\nout dim\nINTEGER\nbias\nREAL\n.\nThen we can compute fc1_out by applying weights and bi-\nases in two consecutive steps.\nCREATE\nTABLE\nfc1_unbiased AS\nSELECT\nimage , out_dim AS i,\nSUM(val * weight) AS val\nFROM\nflatten_out , fc1_weight\nWHERE i = in_dim\nGROUP BY image , out_dim;\nCREATE\nTABLE\nfc1_out AS\nSELECT image , i, val + bias AS val\nFROM\nfc1_unbiased ,fc1_bias\nWHERE i=out_dim;\nAt this point the computation in SQL up to the output\nof full-connect-3 should be clear. At this stage we could\nclaim that we have speciﬁed the neural network per se. It\nis enough for executing inference. However for training, we\nstill have to show how to compute cross-entropy .\nCross-entropy loss for one sample of computed label weights\nx whose correct label is l is given by\nloss(x, l) = −xl + log(\nX\nj\nexp(xj)) .\nAnd we choose to compute the loss over the N samples as the\nmean over each sample. Assume the output of full-connect-3\nto be fc3_out. First we compute the right side of + with\nCREATE\nTABLE\nx_ent_losses_r AS\nSELECT image , LOG(SUM(EXP(val ))) AS r\nFROM\nfc3_out\nGROUP BY image;\nwhere LOG() and EXP() are obviously (element-wise) natural\nlogarithm and exponentiation.\nThe left hand side is just\nselecting one of 10 element, listed as follows list it for clarity.\nCREATE\nTABLE\nx_ent_losses_l AS\nSELECT\nfc3_out.image ,-val AS l\nFROM fc3_out , labels\nWHERE\nfc3_out.image = labels.image AND i = label;\nThen we combine both sides to get loss for each image\nCREATE\nTABLE\nx_ent_losses AS\nSELECT image , l+r AS val\nFROM\nx_ent_losses_l\nNATURAL\nJOIN\nx_ent_losses_r ;\n𝑁×𝑀\narray\n(𝑁 ﬂat \nsamples)\n  gc1\nrelu1\n𝑁×𝐻\narray\n𝑀×𝐻\narray\n(weights)\n𝐻-element\narray\n(biases)\n𝑁×𝐻\narray\n𝑁×𝐶\narray\n(prediction\nscores)\n𝑁-element\ninteger array\n(class labels)\ncross-entropy\nloss\n(scalar)\n𝑁×𝑁\nsparse\nmatrix 𝐴\n(adjacency)\n  gc2\n𝐻×𝐶\narray\n(weights)\n𝐶-element\narray\n(biases)\nFigure 2: Structure of the simple examplar Graph Convo-\nlutional Network. Steps of computation are framed , while\ntheir inputs and outputs are not.\nData (parts of D) are\nmarked in red, while parameters (parts of P) are marked in\nblue.\nand then ﬁnally the average loss\nCREATE\nTABLE\nx_ent_loss AS\nSELECT SUM(val)/ COUNT(val)\nFROM\nx_ent_losses;\nas an 1-row table. Now we have ﬁnished specifying a deep\nlearning model entirely in SQL.\n4.2\nGraph Convolutional Network\nNow let us see a baisc Graphan Convolutional Network\n(GCN) setup in SQL. This GCN is adapted from a simpliﬁed\nversion of that in [17], as a very neat tutorial provided by the\nsame author [15]. We further remove the dropout to simplify\nthings, which moderately increases the computation load\nand slightly increases overﬁtting while not changing major\nresults.\nThe structure of the whole forward computation is as\nshown in Figure 2. This time we assume N samples of M\nfeatures to be classiﬁed into C classes, with one hidden layer\nof size H in-between. This structure is actually much sim-\npler than the previous example, the only really new things\nbeing the Graph Convolutional layers ( gc1 and gc2 ) and\nthe accompanying (N × N) adjacency matrix A. So we will\nfocus on them.\nFrom a computational point-of-view, what the Graph Con-\nvolutional layer can be considered as two consecutive matrix\nmultiplications plus biasing, despite named convolutional\nlayers. That is, the computation before biasing can be sim-\nply expressed as AXW , where X denotes the input of the\nlayer and W denotes the weights. Take gc1 for example,\nX is an N × M matrix while while W is M × H. Then we\ncan add the biases with\nYi,j = (AXW)i,j + Bj ,\nfor all i ∈{1..N}, j ∈{1..H}, B being the biases of the layer\ngc1 .\n5\nNow continuing with gc1 , we assume the following tables\nin the SQL world.\nsamples(i INTEGER , j INTEGER , val REAL );\ngc1_w\n(i INTEGER , j INTEGER , weight\nREAL );\ngc1_b\n(i INTEGER , bias REAL );\nadj\n(i INTEGER , j INTEGER , val REAL );\nDrawing from previous experience from fully-connected lay-\ners, We can easily reproduce the above forward computation\nin SQL with\nCREATE\nTABLE\ngc1_mid AS\nSELECT\nsamples.i AS i, gc1_w.j AS j,\nSUM(val * weight) AS val\nFROM samples , gc1_w\nWHERE\nsamples.j == gc1_w.i\nGROUP BY samples.i, gc1_w.j;\nfor the intermediate matrix product XW, and subsequently\nCREATE\nTABLE\ngc1_out AS\nSELECT\nadj.i AS i, gc1_mid.j AS j,\nSUM(adj.val * gc1_mid.val) + bias AS val\nFROM adj , gc1_mid , gc1_b\nWHERE\nadj.j == gc1_mid.i AND\ngc1_mid.j == gc1_b.i\nGROUP BY adj.i, gc1_mid.j;\nfor the whole biased output.\nThe symmetric adjacency matrix A has zeroes for pairs\nof vertices without an edge in-between and non-zeroes for\nthose connected by an edge.\nFurthermore, the adjacency\nmatrix is row-normalized from a typical adjacency matrix\nof 0’s and 1’s. That is, each row sums to either 1 if there\nare any edges on the vertex or 0 if the vertex is complete\nisolated. And so does each column because of symmetry.\nThe adjacency matrix here is usually a sparse matrix. Or\nto put it another way, the sparsity is essential to its practical\neﬀectiveness, which in turn was probably an important pre-\ncondition to its current popularity. Yet in the SQL world we\ndo not treat sparsity as something special. Actually we take\nsparsity for granted. While how to create an implementation\nrunning as fast as array-based deep-learning frameworks is\nno easy task, we already have battle-hardened semantics and\nstandards in the database world.\n5.\nDISCUSSION\nThe notion that databases provides the data for dedicated\nmachine learning components to work on has been seldom\nquestioned when uniﬁying the two ﬁelds. But we may as well\ngo to the extent that we build machine learning programs\nin terms of databases.\nIn general, we could consider databases as the most feature-\nrich deep learning framework in the future.\nWe look at\ndatabases and we know what can be added to deep learning.\nOne often overlooked fact is that datasets are processed\nas arrays, implying an order while they are not supposed to.\nRelational algebra and subsequently SQL take care of this\nautomatically.\nThe biggest challenge for implementation would be to port\nautomatic diﬀerentation to relational algebra. However it\ncould also be implemented over yet another layer of ﬂat array\nframework with automatic diﬀerentiation, treated as some\nkind of linear memory to sidestep automatic diﬀerentiation\nfrom the ground up.\nDatabases have been dealing with sparse data to begin\nwith. While directly run deep learning in database engine\nmay not be competitive as random access too much to be\nfast at batch processing, we can certainly continue to bring\nexperience in database to machine learning for a very long\ntime to come.\nPerhaps when new deep learning models\nare proposed, the machine learning researchers will ﬁnd the\ndatabase community waiting for them.\n6.\nREFERENCES\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo,\nZ. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, I. Goodfellow, A. Harp,\nG. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser,\nM. Kudlur, J. Levenberg, D. Man´e, R. Monga,\nS. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens,\nB. Steiner, I. Sutskever, K. Talwar, P. Tucker,\nV. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals,\nP. Warden, M. Wattenberg, M. Wicke, Y. Yu, and\nX. Zheng. TensorFlow: Large-scale machine learning\non heterogeneous systems, 2015. Software available\nfrom tensorﬂow.org.\n[2] M. Boehm, I. Antonov, M. Dokter, R. Ginthoer,\nK. Innerebner, F. Klezin, S. Lindstaedt, A. Phani, and\nB. Rath. Systemds: A declarative machine learning\nsystem for the end-to-end data science lifecycle. 09\n2019.\n[3] M. Boehm, M. W. Dusenberry, D. Eriksson, A. V.\nEvﬁmievski, F. M. Manshadi, N. Pansare,\nB. Reinwald, F. R. Reiss, P. Sen, A. C. Surve, et al.\nSystemml: Declarative machine learning on spark.\nProceedings of the VLDB Endowment,\n9(13):1425–1436, 2016.\n[4] J. V. D’silva, F. De Moor, and B. Kemme. Aida:\nabstraction for advanced in-database analytics.\nProceedings of the VLDB Endowment,\n11(11):1400–1413, 2018.\n[5] J. V. D’silva, F. De Moor, and B. Kemme. Making an\nRDBMS data scientist friendly: Advanced in-database\ninteractive analytics with visualization support.\nPVLDB, 12(12):1930–1933, 2019.\n[6] L. Du. How much deep learning does neural style\ntransfer really need? an ablation study. In The IEEE\nWinter Conference on Applications of Computer\nVision (WACV), March 2020.\n[7] Y. Feng, H. You, Z. Zhang, R. Ji, and Y. Gao.\nHypergraph neural networks. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence,\nvolume 33, pages 3558–3565, 2019.\n[8] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style\ntransfer using convolutional neural networks. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, Jun 2016.\n[9] I. J. Goodfellow, J. Shlens, and C. Szegedy.\nExplaining and harnessing adversarial examples. arXiv\npreprint arXiv:1412.6572, 2014.\n[10] J. M. Hellerstein, C. R´e, F. Schoppmann, D. Z. Wang,\nE. Fratkin, A. Gorajek, K. S. Ng, C. Welton, X. Feng,\n6\nK. Li, et al. The madlib analytics library. Proceedings\nof the VLDB Endowment, 5(12), 2012.\n[11] J. Jiang, Y. Wei, Y. Feng, J. Cao, and Y. Gao.\nDynamic hypergraph neural networks. In Proceedings\nof the 28th International Joint Conference on\nArtiﬁcial Intelligence, pages 2635–2641. AAAI Press,\n2019.\n[12] W. W. C. F. Y. Kathryn and R. Mazaitis. Tensorlog:\nDeep learning meets probabilistic databases. Journal\nof Artiﬁcial Intelligence Research, 1:1–15, 2018.\n[13] S. M. Kazemi and D. Poole. Relnn: A deep neural\nmodel for relational learning. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[14] M. Kim. TensorDB and tensor-relational model\n(TRM) for eﬃcient tensor-relational operations.\nArizona State University, 2014.\n[15] T. N. Kipf. https://github.com/tkipf/pygcn.\n[16] T. N. Kipf and M. Welling. Semi-supervised\nclassiﬁcation with graph convolutional networks. 2016.\n[17] T. N. Kipf and M. Welling. Semi-supervised\nclassiﬁcation with graph convolutional networks.\narXiv preprint arXiv:1609.02907, 2016.\n[18] X. Li, B. Cui, Y. Chen, W. Wu, and C. Zhang. Mlog:\nTowards declarative in-database machine learning.\nProceedings of the VLDB Endowment,\n10(12):1933–1936, 2017.\n[19] W. McKinney. Data structures for statistical\ncomputing in python. In S. van der Walt and\nJ. Millman, editors, Proceedings of the 9th Python in\nScience Conference, pages 51 – 56, 2010.\n[20] S. S. Sandha, W. Cabrera, M. Al-Kateb, S. Nair, and\nM. Srivastava. In-database distributed machine\nlearning: Demonstration using teradata sql engine.\nProc. VLDB Endow., 12(12):18541857, Aug. 2019.\n[21] A. Santoro, R. Faulkner, D. Raposo, J. Rae,\nM. Chrzanowski, T. Weber, D. Wierstra, O. Vinyals,\nR. Pascanu, and T. Lillicrap. Relational recurrent\nneural networks. In Advances in neural information\nprocessing systems, pages 7299–7310, 2018.\n[22] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,\nR. Pascanu, P. Battaglia, and T. Lillicrap. A simple\nneural network module for relational reasoning. In\nI. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n30, pages 4967–4976. Curran Associates, Inc., 2017.\n[23] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner,\nand G. Monfardini. The graph neural network model.\nIEEE Transactions on Neural Networks, 20(1):61–80,\n2008.\n[24] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van\nDen Berg, I. Titov, and M. Welling. Modeling\nrelational data with graph convolutional networks. In\nEuropean Semantic Web Conference, pages 593–607.\nSpringer, 2018.\n[25] M. Sch¨ule, F. Simonis, T. Heyenbrock, A. Kemper,\nS. G¨unnemann, and T. Neumann. In-database\nmachine learning: Gradient descent and tensor algebra\nfor main memory database systems. BTW 2019, 2019.\n[26] M. E. Sch¨ule, M. Bungeroth, D. Vorona, A. Kemper,\nS. G¨unnemann, and T. Neumann. Ml2sql - compiling\na declarative machine learning language to sql and\npython. In EDBT, 2019.\n[27] M. Schle, M. Bungeroth, A. Kemper, S. Gnnemann,\nand T. Neumann. Mlearn: A declarative machine\nlearning language for database systems. pages 1–4, 06\n2019.\n[28] G. Sourek, V. Aschenbrenner, F. Zelezny,\nS. Schockaert, and O. Kuzelka. Lifted relational neural\nnetworks: Eﬃcient learning of latent relational\nstructures. Journal of Artiﬁcial Intelligence Research,\n62:69–100, 2018.\n[29] M. Stonebraker, P. Brown, J. Becla, and D. Zhang.\nScidb: A database management system for\napplications with complex analytics. Computing in\nScience and Engg., 15(3):5462, May 2013.\n[30] M. Stonebraker, P. Brown, A. Poliakov, and\nS. Raman. The architecture of scidb. In Proceedings of\nthe 23rd International Conference on Scientiﬁc and\nStatistical Database Management, SSDBM11, page\n116, Berlin, Heidelberg, 2011. Springer-Verlag.\n[31] W. Uwents and H. Blockeel. Classifying relational data\nwith neural networks. In S. Kramer and B. Pfahringer,\neditors, Inductive Logic Programming, pages 384–396,\nBerlin, Heidelberg, 2005. Springer Berlin Heidelberg.\n[32] W. Uwents, G. Monfardini, H. Blockeel, M. Gori, and\nF. Scarselli. Neural networks for relational learning:\nan experimental comparison. Machine Learning,\n82:315–349, 03 2011.\n[33] W. Wang, M. Zhang, G. Chen, H. Jagadish, B. C.\nOoi, and K.-L. Tan. Database meets deep learning:\nChallenges and opportunities. ACM SIGMOD Record,\n45(2):17–22, 2016.\n[34] N. Yadati, M. Nimishakavi, P. Yadav, V. Nitin,\nA. Louis, and P. Talukdar. Hypergcn: A new method\nfor training graph convolutional networks on\nhypergraphs. In Advances in Neural Information\nProcessing Systems, pages 1509–1520, 2019.\n[35] V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li,\nI. Babuschkin, K. Tuyls, D. Reichert, T. Lillicrap,\nE. Lockhart, M. Shanahan, V. Langston, R. Pascanu,\nM. Botvinick, O. Vinyals, and P. Battaglia. Deep\nreinforcement learning with relational inductive\nbiases. In International Conference on Learning\nRepresentations, 2019.\n7\n",
  "categories": [
    "cs.LG",
    "cs.DB",
    "stat.ML"
  ],
  "published": "2020-04-11",
  "updated": "2020-04-14"
}