{
  "id": "http://arxiv.org/abs/2004.05512v1",
  "title": "Reinforcement Learning via Reasoning from Demonstration",
  "authors": [
    "Lisa Torrey"
  ],
  "abstract": "Demonstration is an appealing way for humans to provide assistance to\nreinforcement-learning agents. Most approaches in this area view demonstrations\nprimarily as sources of behavioral bias. But in sparse-reward tasks, humans\nseem to treat demonstrations more as sources of causal knowledge. This paper\nproposes a framework for agents that benefit from demonstration in this\nhuman-inspired way. In this framework, agents develop causal models through\nobservation, and reason from this knowledge to decompose tasks for effective\nreinforcement learning. Experimental results show that a basic implementation\nof Reasoning from Demonstration (RfD) is effective in a range of sparse-reward\ntasks.",
  "text": "Reinforcement Learning via Reasoning from Demonstration\nLisa Torrey\nSt. Lawrence University\nABSTRACT\nDemonstration is an appealing way for humans to provide assis-\ntance to reinforcement-learning agents. Most approaches in this\narea view demonstrations primarily as sources of behavioral bias.\nBut in sparse-reward tasks, humans seem to treat demonstrations\nmore as sources of causal knowledge. This paper proposes a frame-\nwork for agents that benefit from demonstration in this human-\ninspired way. In this framework, agents develop causal models\nthrough observation, and reason from this knowledge to decom-\npose tasks for effective reinforcement learning. Experimental results\nshow that a basic implementation of Reasoning from Demonstra-\ntion (RfD) is effective in a range of sparse-reward tasks.\n1\nINTRODUCTION\nWith reinforcement learning (RL), agents can train themselves au-\ntonomously to complete tasks. RL algorithms can allow agents to\ndevelop successful behaviors based only on environmental feedback.\nHowever, tasks in large environments with infrequent feedback,\nknown as sparse-reward tasks, have always been difficult for RL\nagents to learn.\nDemonstration is a practical and human-motivated approach to\ncoping with challenging RL tasks. Learning from demonstration\n(LfD) can either replace RL or augment it. Systems that combine\nLfD and RL typically do so by encouraging agents to behave more\nlike demonstrators as they learn. This usually improves their early\nperformance, which would otherwise be essentially random.\nIt can be effective to use demonstrations as models of behavior.\nThis paper asks whether it could also be effective to use demonstra-\ntions as sources of knowledge. In sparse-reward tasks, knowledge\nseems particularly valuable.\nConsider, in the classic Atari game Montezuma’s Revenge (see\nFigure 1), the task of exiting the first room without losing a life.\nIt takes a sequence of several hundred carefully chosen actions\nto complete this task. The deep Q-learning system that initially\nmastered many Atari games [25] did not learn to complete it. Yet the\ntask is simple for a human to understand: the player must navigate\nto a key, and then to a door, without falling off any platforms, and\nwithout colliding with a rolling skull.\nThis paper proposes a framework in which agents acquire this\nkind of high-level knowledge by observing a demonstration, and\nthen to use that knowledge to learn the task more effectively.\nRecent studies show that rendering video games with textured\ncells rather than recognizable objects hinders human learning sig-\nnificantly [13]. One plausible explanation is that object perception\nfacilitates causal model-building, which cognitive scientists recog-\nnize as a critical component of human learning [22]. This paper asks\nhow LfD agents might incorporate causal reasoning, and thereby\nbenefit from demonstration more like humans seem to do in tasks\nlike Montezuma’s Revenge.\n2\nBACKGROUND\nThis section presents a short introduction to RL and surveys tech-\nniques for making it more effective in challenging tasks. It focuses\non single agents learning single tasks, and even within this lim-\nited context it is far from exhaustive, but it provides the necessary\ncontext for the proposals that follow.\n2.1\nReinforcement learning\nAn RL agent develops a policy for choosing actions based on the\nstate of its environment. It receives a numeric reward from the en-\nvironment as feedback for each step. Balancing between exploiting\nits developing policy and exploring alternatives, the agent grad-\nually improves its policy and becomes capable of earning higher\ncumulative rewards [31].\nQ-learning [37] is one of the simpler RL algorithms. It uses a\nfunction Q(s,a) to estimate the cumulative reward an agent can\nexpect to earn after taking action a from state s. Given an accurate\nQ-function, the optimal action in state s is the one that maximizes\nQ(s,a). Agents can begin with all Q(s,a) ≈0 and then adjust the\nQ-values incrementally. After taking action a in state s, receiving\nreward r, and transitioning to state s′, the Q-learning update is:\nQ(s,a) ←−(1 −α)Q(s,a) + α(r + γmaxa′Q(s′,a′))\n(1)\nThe α parameter is a learning rate, which controls the update size.\nThe γ parameter is a discount factor, which controls the agent’s\npatience for delayed rewards. A simple exploration strategy for\nQ-learning is ϵ-greedy action selection: the agent usually takes the\naction with the highest Q-value, but with probability ϵ it chooses a\nrandom action instead.\n2.2\nSparse rewards\nRL excels in environments with frequent informative rewards,\nwhich allow agents to improve their policies steadily. As non-zero\nrewards become less frequent, it takes longer to discover them, and\nit takes more repetition to determine which actions are responsible\nfor them. Sparse rewards cause combinatorial explosions in the\ntraining time required to develop competent agents.\nTaxi [9] is a small example of a sparse-reward task. In this task,\na taxi moves through a 5x5 grid world (see Figure 1), picking up\na passenger at one stop and dropping it off at another, with both\nstops randomly selected from the four corners of the grid. The\nagent does receive a small negative reward (-1) after each action,\nto discourage loitering, but it only receives a positive reward (+20)\nonce it completes the task. Each episode ends after 200 actions,\nwhether or not the task is complete.\nCourier (see Figure 1) is a scaled-up version of Taxi, with similar\ndynamics but significantly increased size and sparsity. In this task, a\ncourier moves through a 35x35 grid world, collecting four randomly-\nplaced packages and delivering them to a central platform. There is\nno time limit on episodes, but there are 20 randomly-placed moving\narXiv:2004.05512v1  [cs.LG]  12 Apr 2020\nFigure 1: Renderings of four virtual environments: Taxi, Courier, Ms. Pacman, and Montezuma’s Revenge.\nvehicles that the agent must dodge. The only feedback occurs at the\nend of the task: success when the courier delivers the last package,\nor failure if it gets run over.\nMore examples of sparse-reward tasks can be found in the Arcade\nLearning Environment [24], which emulates classic Atari games\nwith large and unstructured state spaces (160x210 pixel arrays).\nFigure 1 illustrates two of these games. Montezuma’s Revenge has\none of the most naturally sparse reward systems; in the first room,\nthe only rewards are +100 for acquiring the key and +300 for exiting\nthe room. Ms. Pacman has frequent small rewards for eating pellets,\nbut the largest rewards can only be achieved by generating and\neating multiple edible ghosts.\n2.3\nScaling techniques\nStandard Q-learning, which assigns a value to every state-action\npair, is only practical in small environments like Taxi. Larger envi-\nronments, like Courier and the Atari games, have too many states.\nThis problem can be addressed by decomposing states into feature\nvectors and replacing Q-tables with approximators, which require\nless memory and allow for generalization across states. Q-learning\nadapts well to linear and neural approximators using updates de-\nrived from gradient descent [31].\nThere is a long history in RL of engineering features that alter\nthe state space, either to abstract away irrelevant information or\nto compose useful information. For example, with sophisticated\nfeatures, linear Q-learning is sufficient to achieve high scores in\nthe first level of Ms. Pacman [34]. However, in the era of deep\nlearning, the field has moved away from human feature engineering\nin favor of implicit feature construction via convolutional neural\nnetworks. Deep Q-networks can reach human-level play in many\nAtari games directly from their raw pixel states [25]. But games like\nMontezuma’s Revenge remain difficult for deep Q-learning because\nof their sparse rewards.\nOne way to cope with sparse rewards is to shape them. Reward\nshaping is a technique for providing extra rewards to encourage\nprogress towards task completion. Shaped rewards must be de-\nsigned appropriately in order to be effective [26]. For example, on\nan open grid, it would be effective to reward a taxi by the amount\nthat it reduces its Manhattan distance from its destination, but\nthis intuitive shaping strategy would be counter-productive on the\nactual Taxi grid, which contains walls that restrict movement. A\nrecent and more abstract form of reward shaping, called intrinsic\nmotivation, encourages exploration by rewarding agents for reach-\ning new areas of the state space; this technique has some success\nin Montezuma’s Revenge [7].\nAnother way to cope with sparsity is to decompose tasks into\nsubtasks and apply hierarchical learning. In the options framework\n[32], subtask policies are treated like extended actions, which may\nsometimes be selected instead of primitive actions. Other frame-\nworks, like Feudal RL [8] and MAXQ [9], more explicitly formulate\ntasks as hierarchies of subtasks.\nLike features and rewards, task decompositions have often been\nengineered. For example, one approach uses human domain knowl-\nedge to define a separate learner for each reward source, and\nchooses actions by aggregating their Q-values [35]. Given some\nhuman engineering of states and rewards as well, this system is\ncapable of outplaying humans in Ms. Pacman. Other approaches\ntrain low-level learners to control transitions in a higher-level plan,\nwhich is derived from abstract states [27] or symbolic rules [23]\ndefined by a human. Both of these techniques are effective in Mon-\ntezuma’s Revenge.\nThere are also approaches in which agents learn to decompose\ntasks themselves. For example, one method for learning options\nhas been applied to Ms. Pacman [1], and one for learning feudal\nhierarchies has been applied to Montezuma’s Revenge [36]. These\nsystems can outperform their non-hierchical baselines, but their\nscores do not yet compete with the human-guided approaches.\n2.4\nObjects\nMany tasks are defined around objects. Important objects tend to\nbe visually displayed for human players, as in Figure 1, but they\nare not always explicitly described to RL agents. For example, Taxi\ntraditionally decomposes states into four features: the row and\ncolumn numbers of the taxi location, and the stop numbers of\nthe passenger and destination. These values uniquely specify each\nstate, but leave much of the visual information implicit. At the other\nextreme, the pixel states of Atari games provide complete visual\ninformation, but in a completely unstructured form.\nSome RL algorithms are designed specifically for object-oriented\nstates. Relational RL [14] trains a first-order logical Q-function\napproximator, using states that are sets of first-order predicates,\ngrounded with objects in the environment. Object-oriented RL\n2\n[12] uses a similar state representation, though not a first-order\nQ-function, to speed up learning in Taxi.\nObject-oriented states can provide natural opportunities for task\ndecomposition. For example, Object-Focused Q-learning [5] trains a\npolicy for interacting with each type of object, and chooses actions\nby aggregating their Q-values. Hierarchical Deep Q-learning [21]\ntrains one network to select an object, and another to navigate\ntowards a selected object; this is another approach that is effective\nin Montezuma’s Revenge.\nOf all the forms of state engineering involved in RL, object-\noriented states are arguably the most intuitive, since humans per-\nceive objects with so little apparent effort. Furthermore, object\ntracking is an active area of research in the field of computer vision,\nand there have been steps towards applying it to object-focused\nQ-learning [17].\n2.5\nDemonstration\nLfD originated in the field of robotics, where demonstrations are a\nnatural form of assistance for humans to provide, and it can be par-\nticularly slow or costly to allow agents to explore autonomously. In\nthis context, LfD may be a form of supervised learning, with demon-\nstrations as the source of training data. For example, an agent can\napproximate the demonstrator’s policy, and use the approximation\nas its own policy [29].\nIn environments where exploration is permissible, LfD may\nalso involve autonomous practice. RL provides opportunities to\nacquire better and more complete policies than can be acquired\nfrom demonstration alone. For example, an agent can use an ap-\nproximate demonstrator policy as a starting point, and then train it\nfurther with RL [30]. Alternatively, the agent can develop its own\npolicy from scratch, but use an approximate demonstrator policy\nto choose some actions in the early stages of training [33]. Failed\ndemonstrations as well as successful ones can be leveraged to form\nuseful training biases [18].\nDemonstrations can also bias the policies of RL agents less di-\nrectly. For example, one approach shapes rewards for agent actions\nbased on how similar they are to demonstrated actions [2]. Another\nsystem includes demonstrated transitions in the batches that it\ncollects to update a deep Q-network [19]. In one curriculum-based\napproach, the agent learns autonomously, but using demonstration\nstates as starting points [28]; this is another effective strategy for\nMontezuma’s Revenge.\nSome LfD approaches use demonstration to facilitate decom-\nposition. For example, one method achieves state abstraction by\nremoving features that appear to be irrelevant to demonstrator\nactions [6]. A subsequent method goes on to define subtasks based\non differences in their state abstractions [4]. Several approaches\nconstruct option policies based on segments of demonstrations\n[16, 20, 39]. In the context of robotics, demonstrations have been\nused to teach object affordances, which roughly correspond to\nmacro-actions that robots can perform upon objects [3].\nDecomposing tasks via demonstration is arguably an appealing\nmiddle ground between designing subtasks by hand and learning\nthem from scratch. Demonstration often requires less expertise\nthan engineering. And even humans are rarely expected to learn\ncomplex tasks without seeing them demonstrated first.\n3\nREASONING FROM DEMONSTRATION\nWhat would a human learn from a demonstration of Montezuma’s\nRevenge? For an anecdote, I asked a colleague with no history of\ngaming to watch and describe a YouTube video of the first room.\nHe reported that “the cowboy had to go get something and bring it\nup there.\" Compared to what most LfD agents acquire from demon-\nstration, what my colleague retained seems at once much less and\nmuch more. He would likely recall no specific states or actions, but\nhe could explain how to succeed in this task.\nThis paper asks how an LfD agent could acquire this kind of\nhigh-level knowledge from a demonstration, and how it could use\nthat knowledge as it trains. Since no previous LfD systems appear\nto be based on these questions, this is a fundamental difference\nbetween the proposed approach and the existing ones. Let us call\nthis approach Reasoning from Demonstration (RfD).\nThe goal of an RfD agent is to benefit from demonstration more\nlike humans seem to do. The core of the approach is causal model-\nbuilding based on objects and their interactions. Given a demonstra-\ntion, an RfD agent generates a set of cause-effect hypotheses. Causes\nare object interactions, and effects are other observable events, such\nas object appearances and environment feedback. Using these hy-\npotheses, an RfD agent can identify desirable and undesirable object\ninteractions. This provides a basis for task decomposition.\nThe rest of this section describes an RfD agent that illustrates\none possible implementation of the broader concept. After motivat-\ning the agent’s design, the section introduces formal notation and\nprocedures. To facilitate reproduction and future work, all of the\ncode for this agent is available on GitHub1, along with all of the\ndata generated in the experiments further below.\n3.1\nObjectives and anti-objectives\nLet an objective be an object interaction that contributes towards\nsuccess in the task. For example, in Montezuma’s Revenge, the\nfirst objective should be to bring the main character, Panama Joe,\ninto contact with the key, causing the door to become openable.\nThe second objective should be to bring Joe into contact with the\nopenable door, causing the success feedback. After observing these\nevents in a demonstration, any RfD agent should form hypotheses\nabout causes and effects, and these hypotheses should allow it to\nreason about objectives.\nHumans seem to instinctively focus our attention and monitor\nour progress in object-oriented ways. For example, if we are trying\nto bring Joe to the key, we will focus primarily on those two objects,\nand we will be aware that we are making progress as they get closer\ntogether. The RfD agent presented in this paper therefore performs\nobject-oriented state abstraction and distance-based reward shaping\nas it pursues objectives.\nNaive distance-based shaping would encourage straight-line\nmovement, which is not always correct. For example, Joe’s only\nviable path to the key nearly circles the entire room. Progress to-\nwards objectives needs to be judged along paths that respect the\nenvironmental terrain. The RfD agent presented in this paper there-\nfore develops a map of the region connectivity of its environment,\nso that it can shape rewards effectively.\n1https://github.com/lisatorrey/reasoning-from-demonstration\n3\nLet an anti-objective be an object interaction that causes fail-\nure. For example, the task in Montezuma’s Revenge fails if Joe\ncomes into contact with the skull. Since a successful demonstration\nwould not convey this knowledge, it must be discovered during\nautonomous practice. Any RfD agent should therefore continue to\nacquire cause-effect hypotheses as it trains.\nAnti-objectives need to be avoided during the pursuit of objec-\ntives. However, there is neurological evidence that human brains\nprocess risks separately from rewards [38]. The RfD agent presented\nin this paper therefore develops separate policies with respect to\nobjectives and anti-objectives, and consults both when choosing\nactions.\n3.2\nObjects and events\nLet an environment consist of a state space, an action space, and\nan unknown probability distribution over state transitions. Let a\ntask be defined over an environment by choosing four subsets\nof the state space: the state sets where task attempts begin, end,\nreceive SUCCESS feedback, and receive FAILURE feedback. In this task\nformulation, there are no intermediate rewards.\nAny RfD agent needs to be able to perceive objects and events\nin its environment, but agents may differ in how they represent\nthese elements. Here is the representation used by the RfD agent\npresented in this paper.\nLet objects(s) be the set of objects present in state s. Given an\nobject, let type(object) be a descriptor that similar objects would\nshare. Let location(s, object) and velocity(s, object) describe\nthe perceived position and momentum of the object in state s. Let\nregion(s, object) indicate which region of the environment the\nobject occupies in state s.\nLet events(s, s′) be the set of object interactions observed at the\ntransition from state s to s′. Given an event, let type(event) be a\ndescriptor that similar events would share. Let actor(event) be the\nobject responsible for the event, and let subject(event), if it exists,\nbe the object acted upon.\nGiven an event, let template(event) be composed of three types:\ntype(event), type(actor(event)), and type(subject(event)). Let a\ntemplate be called instantiable in states that contain appropriately\ntyped objects to fill the actor and subject roles. Let instantiated\ntemplates be called possible events. Let instances(s, templates) be\nthe set of possible events generated by instantiating one or more\ntemplates in state s.\nGiven a possible event, let distance(s, event) be the distance\nbetween its objects in state s. Let focus(s, event) be an abstract\nstate representation for the possible event. In this paper, abstract\nstates consist of the velocities of the objects and the location of the\nactor (relative to the subject, if there is one) in state s.\n3.3\nTraining procedures\nThe main procedure for the RfD agent presented in this paper is\nAlgorithm 1, which augments the standard RL loop for episodic\ntraining. The agent develops three kinds of knowledge: a theory,\na map, and a set of policies. It begins to develop some of these\ncomponents based on a demonstrated state sequence, and improves\nthem all as it practices the task autonomously.\nAlgorithm 1 Outlines an RfD agent.\nprocedure rfd\nfor each demonstrated transition (s, s′) do\nUPDATE-MAP(s, s′)\nUPDATE-THEORY(s, s′)\nwhile more training is desirable do\ns ←−random choice from initial task states\nwhile s is non-terminal and attempt time < τ do\nanti-objectives ←−instances(s, causes(FAILURE))\nobjectives ←−instances(s, CONTRIBUTORS(SUCCESS))\nobjective ←−CHOOSE-OBJECTIVE(objectives)\nif objective is multi-regional then\nobjective ←−FIRST-CHECKPOINT(objective)\na ←−CHOOSE-ACTION(s, objective, anti-objectives)\ns′ ←−result of taking action a\nUPDATE-MAP(s, s′)\nUPDATE-THEORY(s, s′)\nUPDATE-POLICIES(s, a, s′, objective, anti-objectives)\ns ←−s′\nAlgorithm 2 Makes a theory consistent with a transition (s,s′).\nprocedure update-theory(s, s′)\nfor each event in events(s, s′) do\nif template(event) has not been seen before then\nfor each object in objects(s′) −objects(s) do\nadd template(event) −→type(object) to the agent’s theory\nif SUCCESS occurred in s′ then\nadd template(event) −→SUCCESS to the agent’s theory\nif FAILURE occurred in s′ then\nadd template(event) −→FAILURE to the agent’s theory\nfor each cause −→effect in the agent’s theory do\nif events(s, s′) contains event s.t. template(event) = cause then\nif effect did not occur in s′ then\nremove cause −→effect from the agent’s theory\nAlgorithm 3 Generates objectives contributing to an effect E.\nprocedure contributors(s, E)\ntemplates ←−∅\nfor each C in causes(E) do\nif C is instantiable in s then\nadd C to templates\nelse\nfor each object-type required to make C instantiable in s do\nmerge CONTRIBUTORS(s, object-type) into templates\nreturn templates\nA theory is a set of cause-effect hypotheses. The agent constructs\na theory through repeated application of Algorithm 2, which re-\nflects a few simple assumptions about causality. It assumes object\ninteractions are potential causes, while object appearances and en-\nvironment feedback are potential effects. Furthermore, it assumes\ncauses and effects coincide in time; if it observes potential cause C\nand potential effect E in the same state, it generates the hypothesis\nC −→E. Finally, it assumes the rule of logical implication; if it\nobserves C but not E, it rejects the hypothesis C −→E.\nThe agent uses its theory to identify objectives and anti-objectives.\nAnti-objectives are possible events to be avoided, since they may\ncause failure. Objectives are possible events to be pursued, because\nthey contribute along some causal path towards success, as identi-\nfied in Algorithm 3. In this algorithm, causes(E) indicates the set\nof all C such that C −→E is in the agent’s theory.\nA map is a graph of the regions of the environment and their\nconnectivity. The agent constructs this map through repeated ap-\nplication of Algorithm 4, which keeps track of where objects move\n4\nAlgorithm 4 Adds to a map based on a state transition (s,s′).\nprocedure update-map(s, s′)\nfor each object in objects(s) ∩objects(s′) do\nR ←−region(s, object)\nR′ ←−region(s′, object)\nif R , R′ and no transition R −→R′ exists in the agent’s map then\nadd transition R −→R′ to the agent’s map at location(s′, object)\nacross regions. To keep the map small, the agent stores just one\ntransition for each pair of regions.\nThe agent uses its map to break down multi-region objectives,\nwhich involve objects in different regions. It applies Dijkstra’s al-\ngorithm [11] to find the shortest path from actor to subject, using\nregion-transitions as intermediate points. The length of a path is\nthe sum of the lengths of its segments. The FIRST-CHECKPOINT pro-\ncedure constructs an intermediate objective of moving the actor\nto the first region-transition along this path. The agent also uses\nits map whenever there are multiple objectives to choose from;\nits CHOOSE-OBJECTIVE procedure selects an objective with minimal\npath length, breaking ties randomly. (The FIRST-CHECKPOINT and\nCHOOSE-OBJECTIVE subprocedures are the only ones for which pseu-\ndocode is omitted due to space constraints.)\nA policy evaluates actions with respect to a possible event. The\nagent represents all of its policies with simple Q-functions, and\nassociates one Q-function with each event template. Whenever it\ndiscovers a new event template, it creates a new zero-valued Q-\nfunction. As it trains, it generates its own rewards to update the\nappropriate Q-functions, as shown in Algorithm 5. Objective and\ncheckpoint rewards are based on progress, and all policies have a\nbonus ω for completion.\nThe agent chooses actions by consulting the policies of its current\nobjective and anti-objectives. It uses the corresponding Q-functions\nto compute both a reward estimate and a cumulative risk estimate\nfor each action, as shown in Algorithm 6. When it decides to explore,\nit randomly chooses a minimal-risk action. Otherwise, it chooses\nan action that maximizes the gap between reward and risk, using a\nweight β to control risk tolerance.\nExploration rates (ϵ) and risk tolerances (β) are specific to the\ncurrent objective. New Q-functions for objectives begin with ϵQ =\nϵmax . Whenever an objective is completed, its ϵ decays, with a floor\nof ϵmin. The agent therefore explores less the more an objective suc-\nceeds. New Q-functions for objectives also begin with βQ = βmax.\nWhenever an objective is exploited, its β decays, but whenever it is\ncompleted, its β resets to βmax . The agent therefore tolerates more\nrisk the longer it pursues an objective without completing it.\nAll of the experiments in this paper use the following settings for\nthe Greek-letter parameters: α = 0.1, γ = 0.9, ω = 100, τ = 10000,\nϵmax = 0.1, ϵmin = 0.01, λϵ = 0.99, βmax = 100, λβ = 0.99.\n4\nBENCHMARK\nTaxi makes a good benchmark for comparing RfD with LfD. It is\nsmall enough to apply any approach, but sparse enough to reveal\ndifferences between approaches. There are two main categories of\nLfD that should be considered: approaches that use demonstration\nto influence a single policy, and approaches that use demonstration\nto decompose the task. This section identifies one approach in each\nAlgorithm 5 Adjusts policies after a transition (s,a,s′).\nprocedure update-policies(s, a, s′, objective, anti-objectives)\nQ ←−the Q-function associated with template(objective)\nδ ←−distance(s, objective) −distance(s′, objective)\ns ←−focus(s, objective)\ns′ ←−focus(s′, objective)\nif objective is a checkpoint then\nif actor(objective) has entered the checkpoint region then\nupdate Q according to Equation 1 with s, a, s′, and r = δ + ω\nϵQ ←−max(ϵmin, λϵ ϵQ )\nβQ ←−βmax\nelse if actor(objective) has entered a different region then\nupdate Q according to Equation 1 with s, a, s′, and r = δ −ω\nelse\nupdate Q according to Equation 1 with s, a, s′, and r = δ\nelse\nif objective ∈events(s, s′) then\nupdate Q according to Equation 1 with s, a, s′, and r = δ + ω\nϵQ ←−max(ϵmin, λϵ ϵQ )\nβQ ←−βmax\nelse if objective remains possible in s′ then\nupdate Q according to Equation 1 with s, a, s′, and r = δ\nfor each anti-objective do\nQ ←−the Q-function associated with template(anti-objective)\ns ←−focus(s, anti-objective)\ns′ ←−focus(s′, anti-objective)\nif anti-objective ∈events(s, s′) then\nupdate Q according to Equation 1 with s, a, s′, and r = −ω\nelse if anti-objective remains possible in s′ then\nupdate Q according to Equation 1 with s, a, s′, and r = 0\nAlgorithm 6 Chooses an action in state s.\nprocedure choose-action(s, objective, anti-objectives)\nfor each a in the action space do\nrisk[a] ←−0\nfor each anti-objective do\nQ ←−the Q-function associated with template(anti-objective)\ns ←−focus(s, anti-objective)\nrisk[a] ←−risk[a] −Q(s, a)\nQ ←−the Q-function associated with template(objective)\ns ←−focus(s, objective)\nfor each a in the action space do\nreward[a] ←−Q(s, a)\nif random(0,1) < ϵQ then\nsafest ←−set of a minimizing risk[a]\nreturn random choice from safest\nelse\nbest ←−set of a maximizing reward[a] −βQ risk[a]\nβQ ←−λβ βQ\nreturn random choice from best\ncategory that seems best suited to the Taxi problem, and shows\nhow these two LfD approaches compare with RfD.\nIt is easy to generate good demonstrations for Taxi. Training\na standard Q-learner to convergence takes about 100,000 actions,\nand then it makes a reliable demonstrator. However, a Taxi demon-\nstration provides very limited information. There are 500 unique\nstates in the environment, and good demonstrations encounter only\n5 to 20 of them. Furthermore, the traditional state representation\nprovides little opportunity to generalize beyond a demonstration.\nStates that match in some or even most of their feature values still\noften have different optimal actions.\nIn this situation, a single-policy LfD agent should benefit most\nby favoring imitation over generalization. The imitation agents\nin this experiment therefore use standard Q-learning, but always\ntake demonstrated actions in demonstrated states, unless they are\nexploring. Figure 2 confirms that demonstrations are useful in Taxi:\n5\nthe imitation agents learn faster than the standard Q-learner, and\nthe effect increases with the number of demonstrations.\nTaxi does provide opportunities for task decomposition, as well\nas further state decomposition within subtasks. Among the exist-\ning approaches, Automatic Decomposition and Abstraction (ADA)\nseems best suited to take advantage of both opportunities [4].\nADA partitions a state space into subtasks along numeric feature\nboundaries, and removes irrelevant features in subtasks based on\ntheir mutual information with demonstrator actions. In Taxi, the\npassenger feature has values in {0, 1, 2, 3, 4}, where 4 means the\npassenger is inside the taxi, and the other values represent the stops.\nThus it is appropriate to use the boundary passenger < 4 to divide\nTaxi into two subtasks. One side is the pickup subtask, in which\nthe destination feature should be ignored. The other side is the\ndropoff subtask, in which the passenger feature can be ignored.\nIn 100 trials of up to 32 demonstrations generated one at a time\nby a trained Q-learner, ADA produced the correct decomposition\n72 times, after a minimum of 3 demonstrations and an average\nof 12. To establish an upper bound on the potential of ADA, the\ndecomposition agents in this experiment all use the correct decom-\nposition, and they train the two subtasks via imitation. Figure 2\nconfirms that decomposition is useful in Taxi: given the same num-\nber of demonstrations, decomposition agents quickly outperform\nimitation agents.\nThe RfD agents each receive just one demonstration, which I\nperformed from the initial state shown in Figure 1. Instead of the\nusual numeric rewards of the Taxi environment, the RfD agents\nreceive only SUCCESS or FAILURE feedback at the end of each attempt.\nThe regions of the environment are the five rectangles suggested by\nthe walls. Each object has a (row, column) location and a velocity\nof 0. The object types and event templates are evident from these\nhypotheses, which the RfD agents generate for Taxi:\npicks(Taxi, Passenger) −→Taxi+Passenger\npicks(Taxi, Passenger) −→Stop\ndrops(Taxi+Passenger, Stop) −→Taxi\ndrops(Taxi+Passenger, Stop) −→Passenger\ndrops(Taxi+Passenger, Destination) −→SUCCESS\ndrops(Taxi+Passenger, Destination) −→Stop\nDifferences in the forms and amounts of assistance that the RfD\nand decomposition agents receive complicate the comparison of\ntheir learning curves. That said, the differences in their learning\ncurves are dramatic. The RfD agents converge in about 2% of the\ntime that the decomposition agents take, regardless of the number\nof demonstrations. Figure 3 shows the RfD learning curves on an\nappropriate scale.\nAlthough they represent the information differently, the RfD\nand decomposition agents are both solving the same subtasks in\nthe same state space. The difference in their performance is mainly\nattributable to the reward shaping that the RfD agents perform. Of\ncourse, it is no surprise that reward shaping speeds up learning.\nBut agents need causal and spatial knowledge in order to perform\nreward shaping themselves, without human engineering.\nOverall, the comparison of ADA and RfD suggests that decompo-\nsition based on causal reasoning, rather than statistical reasoning,\nhas the potential to facilitate significantly faster learning after sig-\nnificantly less demonstration.\nFigure 2: Learning curves for non-RfD agents in Taxi. Curves are\nsmoothed over 400 episodes and averaged over 10 different agents.\nFigure 3: Learning curves for ten RfD agents in Taxi. Curves are\n300 attempts long and smoothed over a 30-attempt window.\n5\nCHALLENGES\nThis section shows how RfD scales to larger and sparser environ-\nments: Courier, Ms. Pacman, and Montezuma’s Revenge.\n5.1\nCourier\nWith four packages and 20 vehicles on a 35x35 grid, just the ini-\ntial states of the Courier task number more than 1035. My single\ndemonstration begins at the state shown in Figure 1, collects the\ntwo packages on one side, collects the two packages on the other\nside, and delivers them all at once.\nThe regions of the environment are the three rectangles sug-\ngested by the walls. Each object has a (row, column) location. The\nobject types and event templates are evident from these initial\nhypotheses that the RfD agents generate for Courier:\narrives(Courier, Package) −→Courier+1\narrives(Courier+1, Package) −→Courier+2\narrives(Courier+2, Package) −→Courier+3\narrives(Courier+3, Package) −→Courier+4\narrives(Courier+4, Platform) −→SUCCESS\narrives(Courier+4, Platform) −→Platform+4\narrives(Courier+4, Platform) −→Courier\nAgents add additional hypotheses as they train. Most impor-\ntantly, they discover that collides(Courier+k, Vehicle) causes FAILURE\nfor any k. They also tend to discover the possibility of delivering\nfewer than four packages at once. These discoveries lead some\nagents to develop alternate strategies.\n6\nThe ADA approach is not practically applicable in Courier due\nto the dimensionality of the state space. And because Courier is es-\nsentially designed to need object-oriented decomposition, no other\nLfD approach seems more applicable. However, Figure 4 shows\nthat training RfD agents to convergence in Courier takes less than\n100,000 actions. These results indicate that combinatorially huge\nenvironment sizes and arbitrarily long subtask sequences are sur-\nmountable via RfD. Furthermore, they show that RfD agents can\ndevelop logical solutions that were not demonstrated.\n5.2\nMs. Pacman\nIn Ms. Pacman, agents can accumulate rewards through a sequence\nof levels just by eating densely placed pellets. Regular pellets are\nworth 10 points, while the rarer power-pellets are worth 50 points.\nHowever, the highest rewards are only available for a short period\nafter consuming a power-pellet, when the ghosts become temporar-\nily edible. Catching one turns it into a pair of eyes and yields 200\npoints, and this reward doubles for each additional catch during\nthe same period.\nLet us define a task in Ms. Pacman that focuses on catching\nedible ghosts in the first level. It provides SUCCESS feedback when\nMs. Pacman catches an edible ghost and FAILURE feedback when\nshe collides with a regular ghost. The task ends upon failure, but\nsuccess is achievable up to 16 times per attempt, because there are\nfour power-pellets and four ghosts.\nThis game always begins in the same state, and the Atari emula-\ntor is, unfortunately, inherently deterministic; the same sequence\nof player actions always produces the same sequence of states. A\ncommon way to introduce some stochasticity is to delay the start\nof the game, allowing the ghosts to move for a random number of\nframes before Ms. Pacman can start moving.\nThe environment provides only pixels, but objects in Ms. Pacman\ncan be identified based on their colors. Each object has an (x,y)\nlocation that approximates its center. Since the typical practice in\nAtari games is to repeat actions for four frames each, any mov-\ning object has a velocity, which is expressed as UP, DOWN, LEFT, or\nRIGHT. The object types and event templates are evident from these\nhypotheses, which the RfD agents generate for Ms. Pacman:\narrives(Pacman, Power) −→Edible\ncatches(Pacman, Edible) −→SUCCESS\ncatches(Pacman, Edible) −→Eyes\ncollides(Pacman, Ghost) −→FAILURE\nDemonstrations for this task could vary widely. I performed a\nshort one, taking Ms. Pacman directly to the lower left pellet and\nkeeping her in that corner, where one edible ghost soon stumbles\ninto her, and a bit later a regular ghost finds her. Unlike my demon-\nstrations for other tasks, this one provides the basis for a complete\ncausal model, but for only a partial map of the regions (which are\nthe 36 corridors). The RfD agents discover about 60% of the regions\nand about 80% of the transitions as they train.\nFigure 5 shows that navigation-intensive problems are approach-\nable via RfD. The RfD agents learn to average about 8 successes per\nattempt within a few hundred thousand actions. Since most agents\nin Ms. Pacman focus on maximizing points across multiple levels,\ncomparisons to previous results are not necessarily meaningful.\nHowever, one agent that was heavily engineered to maximize its\nFigure 4: Learning curves for ten RfD agents in Courier. Curves are\n500 attempts long and smoothed over a 50-attempt window.\nFigure 5: Learning curves for ten RfD agents in Ms. Pacman. Curves\nare 1000 attempts long and smoothed over a 100-attempt window.\nscore on the first level of Ms. Pacman averaged about 3800 points\nat its asymptote [34], while at their asymptotes, the RfD agents\naverage about 5600 points on that level. Causal modeling allows\nthem to pursue large rewards in more deliberate ways.\n5.3\nMontezuma’s Revenge\nThe first-room task in Montezuma’s Revenge provides SUCCESS feed-\nback when Joe exits the room and FAILURE feedback when he loses\na life. In both cases, the task ends.\nRegions correspond to the visual elements of the terrain: the\nplatforms, ladders, floor, and rope. Objects are identified based on\ntheir colors, and each object has an (x,y) location that approxi-\nmates its center. Moving objects have velocities that are expressed\nas (∆x, ∆y) tuples. The object types and event templates are ev-\nident from these hypotheses, which the RfD agents generate for\nMontezuma’s Revenge:\narrives(Joe, Key) −→Door+Key\narrives(Joe, Door+Key) −→SUCCESS\ncollides(Joe, Skull) −→FAILURE\nfalls(Joe) −→FAILURE\nThere is little variation among successful demonstrations for this\ntask, because they must all take approximately the same route down\nto the key and back up to the door. To introduce some variation,\nthe start of each game is randomly delayed by up to 400 frames, so\nthat Joe might reach the skull at any point in its cyclical patrol. My\ndemonstration provides the basis for a complete map, but only a\npartial theory; the RfD agents must learn the causes of failure as\nthey train.\n7\nFigure 6 shows that the RfD agents reach a success rate of about\n90% in this task after about 25 million actions. Comparisons to pre-\nvious results are not necessarily meaningful, because most agents\nin Montezuma’s Revenge are permitted to use all five lives. Since\nthe skull disappears after one collision, even having two lives to\nspend makes it much easier to exit the room.\nOne approach called Deep Abstract Q-Networks does consider\nthe single-life condition [27]. Using hierarchical deep RL with\nhuman-engineered abstract states, it rises to a peak average of ap-\nproximately 300 points in approximately 50 million frames. These\nresults seem comparable to the RfD results. The similarity suggests\nthat when learning sparse-reward tasks, it may be less important\nto use a sophisticated RL algorithm than it is to acquire, in some\nway, an effective decomposition.\n5.4\nExtended actions\nThe task in Montezuma’s Revenge is only slightly longer than the\nother tasks examined in this paper. The difference is far too small\nto explain the steep increase in learning time. Why is Montezuma’s\nRevenge so hard to learn, even for agents who are well-equipped\nto handle sparse rewards? One recent study notes that the environ-\nment contains many dead ends, or states from which an agent will\ncertainly lose a life, despite continuing to act for a while before it\noccurs [15]. For example, if Joe steps off the top platform, he falls\nto his death, and although the emulator continues to accept actions\non the way down, none of them have any effect.\nFrom an RL agent’s perspective, this problem can be charac-\nterized in a different way: actions in Montezuma’s Revenge have\nunpredictable durations. Normally each action lasts for four frames,\nbut if an action begins a jump or a fall, it really lasts until Joe returns\nto a surface again. Humans understand this fact intuitively, since\nour experience in the physical world makes us expect to be power-\nless to change our trajectory in mid-air. Because of our intuition,\nwe perceive the actions in this environment differently than RL\nagents do.\nFigure 7 shows that this difference in perception has a dramatic\neffect on the learning process. If the actions in Montezuma’s Re-\nvenge are extended until Joe stops falling, to correspond with hu-\nman perceptions, the RfD agents can reach the same asymptote in\nabout 2% of the time. The knowledge that RfD agents develop is in-\nsufficient for them to make this adjustment autonomously, so these\nresults should not be considered achievements of RfD. They are\nincluded here just to clarify the difficulty of Montezuma’s Revenge,\nwhich is due not only to the nature of its rewards, but also to the\nnature of its actions.\n6\nCONCLUSIONS\nThis paper proposes a framework called Reasoning from Demon-\nstration. It is inspired by the way that humans seem to approach\nsparse-reward tasks with object-oriented causal reasoning. Agents\nin this framework acquire causal knowledge from demonstration\nand use it to decompose tasks effectively and practice them delib-\nerately. The paper presents experimental evidence that in sparse-\nreward tasks, RfD approaches have the potential for much faster\nlearning than traditional LfD approaches.\nFigure 6: Learning curves for ten RfD agents in Montezuma’s Re-\nvenge. Curves are 100,000 attempts long and smoothed over a 10,000-\nattempt window.\nFigure 7: Learning curves for ten RfD agents in Montezuma’s Re-\nvenge with extended actions. Curves are 2000 attempts long and\nsmoothed over a 200-attempt window.\nAnother potential benefit of RfD agents is their natural resilience\nto low-quantity and low-quality demonstrations. One successful\ndemonstration is often enough to establish the causal dynamics\nof a task. And as long as a demonstration ultimately reveals those\ncausal dynamics, it can be arbitrarily sub-optimal without having\nany detrimental effect on RfD agents.\nThe agent described in this paper implements the RfD concept\nusing simple components and procedures. Many of them could\nclearly be improved by other agents within the RfD framework. For\nexample, richer representations of objects and policies would allow\nfor finer control, and a more sophisticated treatment of causality\nwould produce more robust reasoning.\nMost importantly, although the agent in this paper is assisted\nin perceiving objects, events, and regions, future RfD agents could\napproach these problems autonomously. In fact, the main conclu-\nsion of this paper is that research on perception should be better\nintegrated into research on learning in sparse-reward tasks. Struc-\ntured perception is necessary for causal modeling, which clearly\nhas potential benefits for sparse-reward learning.\nThis work joins ongoing discussions about combining statistical\nand symbolic AI [10] and about making agents learn more like\nhumans do [22]. These discussions seem particularly relevant to\nthe area of LfD, which is already human-inspired.\n8\nREFERENCES\n[1] Pierre-Luc Bacon, Jean Harb, and Doina Precup. 2017. The Option-Critic Archi-\ntecture. In Proceedings of the 31st AAAI Conference on Artificial Intelligence.\n[2] Tim Brys, Anna Harutyunyan, Halit Bener Suay, Sonia Chernova, Matthew Taylor,\nand Ann Nowé. 2015. Reinforcement Learning from Demonstration Through\nShaping. In Proceedings of the 24th Joint International Conference on Artificial\nIntelligence.\n[3] Vivian Chu, Baris Akgun, and Andrea Thomaz. 2016. Learning Haptic Affordances\nfrom Demonstration and Human-Guided Exploration. In Proceedings of the IEEE\nHaptics Symposium.\n[4] Luis Cobo, Charles Isbell, and Andrea Thomaz. 2012. Automatic Task Decom-\nposition and State Abstraction from Demonstration. In Proceedings of the 11th\nInternational Conference on Autonomous Agents and Multiagent Systems.\n[5] Luis Cobo, Charles Isbell, and Andrea Thomaz. 2013. Object Focused Q-learning\nfor Autonomous Agents. In Proceedings of the 12th International Conference on\nAutonomous Agents and Multi-agent Systems.\n[6] Luis Cobo, Peng Zang, Charles Isbell, and Andrea Thomaz. 2011. Automatic State\nAbstraction from Demonstration. In Proceedings of the 22nd International Joint\nConference on Artificial Intelligence.\n[7] Michael Dann, Fabio Zambetta, and John Thangarajah. 2019. Deriving Subgoals\nAutonomously to Accelerate Learning in Sparse Reward Domains. In Proceedings\nof the 33rd AAAI Conference on Artificial Intelligence.\n[8] Peter Dayan and Geoffrey Hinton. 1992. Feudal Reinforcement Learning. In\nAdvances in Neural Information Processing Systems 5.\n[9] Thomas Dietterich. 2000. Hierarchical Reinforcement Learning with the MAXQ\nValue Function Decomposition. Journal of Artificial Intelligence Research 13, 1\n(2000), 227–303.\n[10] Thomas Dietterich. 2019.\n(10 2019).\nhttps://medium.com/@tdietterich/\nwhat-does-it-mean-for-a-machine-to-understand-555485f3ad40\n[11] Edsger Dijkstra. 1959. A note on two problems in connexion with graphs. Numer.\nMath. 1 (1959), 269âĂŞ–271.\n[12] Carlos Diuk, Andre Cohen, and Michael Littman. 2008. An Object-oriented\nRepresentation for Efficient Reinforcement Learning. In Proceedings of the 25th\nInternational Conference on Machine Learning.\n[13] Rachit Dubey, Pulkit Agrawal, Deepak Pathak, Thomas Griffiths, and Alexei\nEfros. 2018. Investigating Human Priors for Playing Video Games. In Proceedings\nof the 35th International Conference on Machine Learning.\n[14] Sašo Džeroski, Luc De Raedt, and Kurt Driessens. 2001. Relational Reinforcement\nLearning. Machine Learning 43, 1 (2001), 7–52.\n[15] Mehdi Fatemi, Shikhar Sharma, Harm van Seijen, and Samira Ebrahimi Kahou.\n2019. Dead-ends and Secure Exploration in Reinforcement Learning. In Proceed-\nings of the 36th International Conference on Machine Learning.\n[16] Roy Fox, Sanjay Krishnan, Ion Stoica, and Ken Goldberg. 2017. Multi-Level\nDiscovery of Deep Options. CoRR abs/1703.08294 (2017).\n[17] Marta Garnelo, Kai Arulkumaran, and Murray Shanahan. 2016. Towards Deep\nSymbolic Reinforcement Learning. arXiv 1609.05518 (2016).\n[18] D. Grollman and A. Billard. 2012. Robot Learning from Failed Demonstrations.\nInternational Journal of Social Robotics 4 (2012), 331âĂŞ342.\n[19] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot,\nDan Horgan, John Quan, Andrew Sendonaris, Ian Osband, Gabriel Dulac-Arnold,\nJohn Agapiou, Joel Z. Leibo, and Audrunas Gruslys. 2018. Deep Q-learning\nFrom Demonstrations. In Proceedings of the 32nd AAAI Conference on Artificial\nIntelligence.\n[20] George Konidaris, Scott Kuindersma, Roderic Grupen, and Andrew Barto. 2012.\nRobot Learning from Demonstration by Constructing Skill Trees. International\nJournal of Robotics Research 31, 3 (2012), 360–375.\n[21] Tejas Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. 2016.\nHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction\nand Intrinsic Motivation. In Advances in Neural Information Processing Systems\n29.\n[22] Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. 2017.\nBuilding Machines that Learn and Think Like People. Behavioral and Brain\nSciences 40 (2017).\n[23] Daoming Lyu, Fangkai Yang, Bo Liu, and Steven Gustafson. 2019. SDRL: Inter-\npretable and Data-Efficient Deep Reinforcement Learning Leveraging Symbolic\nPlanning. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence.\n[24] Marlos Machado, Marc Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht,\nand Michael Bowling. 2018. Revisiting the Arcade Learning Environment. In\nProceedings of the 27th International Joint Conference on Artificial Intelligence.\n[25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness,\nMarc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Os-\ntrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen\nKing, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015.\nHuman-level Control through Deep Reinforcement Learning. Nature 518, 7540\n(2015), 529–533.\n[26] Andrew Ng, Daishi Harada, and Stuart Russell. 1999. Policy Invariance Un-\nder Reward Transformations: Theory and Application to Reward Shaping. In\nProceedings of the 16th International Conference on Machine Learning.\n[27] Melrose Roderick, Christopher Grimm, and Stefanie Tellex. 2018. Deep Abstract\nQ-Networks. In Proceedings of the 17th International Conference on Autonomous\nAgents and MultiAgent Systems.\n[28] Tim Salimans and Richard Chen. 2018. Learning Montezuma’s Revenge from a\nSingle Demonstration. In Proceedings of the NeurIPS Deep Reinforcement Learning\nWorkshop.\n[29] Claude Sammut, Scott Hurst, Dana Kedzier, and Donald Michie. 1992. Learning\nto Fly. In Proceedings of the 9th International Workshop on Machine Learning.\n[30] Stefan Schaal. 1997. Learning from Demonstration. In Advances in Neural Infor-\nmation Processing Systems 9.\n[31] Richard Sutton and Andrew Barto. 2018. Reinforcement Learning: An Introduction\n(2 ed.). MIT Press, Cambridge, MA.\n[32] Richard Sutton, Doina Precup, and Satinder Singh. 1999. Between MDPs and\nSemi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning.\nArtificial Intelligence 112, 1 (1999), 181 – 211.\n[33] Matthew Taylor, Halit Bener Suay, and Sonia Chernova. 2011. Integrating Rein-\nforcement Learning with Human Demonstrations of Varying Ability. In Proceed-\nings of the 10th International Conference on Autonomous Agents and Multi-agent\nSystems.\n[34] Lisa Torrey and Matthew Taylor. 2013. Teaching on a Budget: Agents Advis-\ning Agents in Reinforcement Learning. In Proceedings of the 12th International\nConference on Autonomous Agents and Multiagent Systems.\n[35] Harm van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes,\nand Jeffrey Tsang. 2017. Hybrid Reward Architecture for Reinforcement Learning.\nIn Advances in Neural Information Processing Systems 30.\n[36] Alexander Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jader-\nberg, David Silver, and Koray Kavukcuoglu. 2016. FeUdal Networks for Hierarchi-\ncal Reinforcement Learning. In Proceedings of the 34th International Conference\non Machine Learning.\n[37] Christopher Watkins and Peter Dayan. 1992. Q-learning. Machine Learning 8, 3\n(1992), 279–292.\n[38] Gui Xue, Zhonglin Lu, Irwin Levin, Joshua Weller, Xiangrui Li, and Antoine\nBechara. 2009. Functional Dissociations of Risk and Reward Processing in the\nMedial Prefrontal Cortex. Cerebral Cortex 19, 5 (2009), 1019âĂŞ1027.\n[39] Peng Zang, Peng Zhou, David Minnen, and Charles Isbell. 2009. Discovering\nOptions from Example Trajectories. In Proceedings of the 26th International Con-\nference on Machine Learning.\n9\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2020-04-12",
  "updated": "2020-04-12"
}