{
  "id": "http://arxiv.org/abs/2004.05991v2",
  "title": "A Simple Approach to Learning Unsupervised Multilingual Embeddings",
  "authors": [
    "Pratik Jawanpuria",
    "Mayank Meghwanshi",
    "Bamdev Mishra"
  ],
  "abstract": "Recent progress on unsupervised learning of cross-lingual embeddings in\nbilingual setting has given impetus to learning a shared embedding space for\nseveral languages without any supervision. A popular framework to solve the\nlatter problem is to jointly solve the following two sub-problems: 1) learning\nunsupervised word alignment between several pairs of languages, and 2) learning\nhow to map the monolingual embeddings of every language to a shared\nmultilingual space. In contrast, we propose a simple, two-stage framework in\nwhich we decouple the above two sub-problems and solve them separately using\nexisting techniques. The proposed approach obtains surprisingly good\nperformance in various tasks such as bilingual lexicon induction, cross-lingual\nword similarity, multilingual document classification, and multilingual\ndependency parsing. When distant languages are involved, the proposed solution\nillustrates robustness and outperforms existing unsupervised multilingual word\nembedding approaches. Overall, our experimental results encourage development\nof multi-stage models for such challenging problems.",
  "text": "arXiv:2004.05991v2  [cs.CL]  20 Apr 2020\nA Simple Approach to Learning\nUnsupervised Multilingual Embeddings\nPratik Jawanpuria\nMayank Meghwanshi\nBamdev Mishra\nMicrosoft India\nEmails: {pratik.jawanpuria, mamegh, bamdevm}@microsoft.com\nAbstract\nRecent progress on unsupervised learning of cross-lingual embeddings in bilingual setting has\ngiven impetus to learning a shared embedding space for several languages without any supervision.\nA popular framework to solve the latter problem is to jointly solve the following two sub-problems:\n1) learning unsupervised word alignment between several pairs of languages, and 2) learning how\nto map the monolingual embeddings of every language to a shared multilingual space. In contrast,\nwe propose a two-stage framework in which we decouple the above two sub-problems and solve\nthem separately using existing techniques. Though this seems like a simple baseline approach, we\nshow that the proposed approach obtains surprisingly good performance in various tasks such as\nbilingual lexicon induction, cross-lingual word similarity, multilingual document classiﬁcation, and\nmultilingual dependency parsing. When distant languages are involved, the proposed solution illus-\ntrates robustness and outperforms existing unsupervised multilingual word embedding approaches.\nOverall, our experimental results encourage development of multi-stage models for such challenging\nproblems.\n1\nIntroduction\nLearning cross-lingual word representations has been the focus of many recent works [Klementiev et al.,\n2012, Mikolov et al., 2013, Faruqui and Dyer, 2014, Ammar et al., 2016, Artetxe et al., 2016, Conneau et al.,\n2018]. It aims at learning a shared embedding space for words across two (bilingual word embeddings\nor BWE) or more languages (multilingual word embeddings or MWE), by mapping similar words (or\nconcepts) across different languages close to each other in the common embedding space. Such a rep-\nresentation is useful in various applications such as cross-lingual text classiﬁcation [Klementiev et al.,\n2012, Ammar et al., 2016], building bilingual lexicons [Mikolov et al., 2013], cross-lingual informa-\ntion retrieval [Vuli´c and Moens, 2015], sentiment analysis [Zhou et al., 2015], and machine translation\n[Gu et al., 2018, Lample et al., 2018, Artetxe et al., 2018c], to name a few.\nMikolov et al. [2013] observe that word embeddings exhibit similar structure across different lan-\nguages. In particular, they show that the geometric arrangement of word embeddings can be (approxi-\nmately) preserved by linearly transforming the word embeddings from one language space to another.\nThereafter, several works have explored learning BWEs in supervised [Xing et al., 2015, Artetxe et al.,\n2016, 2018a, Smith et al., 2017, Jawanpuria et al., 2019] as well as unsupervised [Zhang et al., 2017a,b,\nConneau et al., 2018, Artetxe et al., 2018b, Alvarez-Melis and Jaakkola, 2018, Hoshen and Wolf, 2018,\nGrave et al., 2019] settings. Supervision is usually in the form of a bilingual lexicon. Unsupervised\nBWE approaches enjoy the advantage of not requiring a bilingual lexicon during the training stage.\nGeneralization from bilingual to multilingual setting is desirable in various multilingual applica-\ntions such as document classiﬁcation, dependency parsing [Ammar et al., 2016], machine translation,\netc. Representing the word embeddings of several languages in a common shared space can allow\n1\nknowledge transfer across languages wherein a single classiﬁer may be learned on multilingual datasets\n[Heyman et al., 2019].\nIn this work, we propose a two-stage framework for learning a shared multilingual space in the\nunsupervised setting. The two stages aim at solving the following sub-problems: a) learning unsu-\npervised word alignment between a few pairs of languages, and subsequently b) learning how to map\nthe monolingual embeddings of each language to a shared multilingual space. The sub-problems are\nseparately solved using existing techniques. In contrast, existing state-of-the-art unsupervised MWE ap-\nproaches [Chen and Cardie, 2018, Heyman et al., 2019, Alaux et al., 2019] aim at solving the above sub-\nproblems jointly. Though it appears like a simple baseline approach, the proposed framework provides\nthe robustness and versatility often desirable while learning an effective shared MWE space for distant\nlanguages. Unsupervised alignment often fail for distant languages [Søgaard et al., 2018, Glavaˇs et al.,\n2019, Vuli´c et al., 2019]. Our approach can seamlessly work in “hybrid” settings, where supervision\nis available for some languages (but not for others). Integrating such hybrid setups in the joint opti-\nmization framework of existing unsupervised MWE approaches [Chen and Cardie, 2018, Heyman et al.,\n2019, Alaux et al., 2019] may not1 always be feasible.\nWe evaluate our approach on different tasks such as bilingual lexicon induction (BLI), cross-lingual\nword similarity, and two downstream multilingual tasks: document classiﬁcation and dependency pars-\ning. We summarize our ﬁndings below.\n• For a group consisting of similar languages, we observe that the proposed approach achieves BLI\nscore similar to the existing unsupervised MWE approaches. We also observe that all multilingual\napproaches, including ours, beneﬁt from transfer learning across languages in such a setting.\n• In a group comprising of distant languages, we observe sub-optimal BLI performance of existing\nunsupervised MWE approaches. The presence of distant languages sometimes adversely affect\nthe alignment of even similar language pairs in such methods. The proposed approach, however,\nis robust and outperforms other multilingual approaches in such settings.\n• The proposed approach performs better than other MWE baselines on cross-lingual word similar-\nity, document classiﬁcation, and dependency parsing tasks.\nThe rest of the paper is organized as follows. Section 3 discusses the proposed unsupervised MWE\nframework. Section 4 describes our experimental setup and Section 5 presents and analyzes the empirical\nresults. Section 6 concludes the paper. We begin by discussing related works in the next section.\n2\nRelated Work\nBilingual setting: The problem of learning bilingual mapping of word embeddings in the supervised\nsetup is as follows. Let xi ∈Rd and zj ∈Rd denote the i-th and j-th word embeddings of the source\nand the target languages, respectively. A seed translation dictionary Y ∈{0, 1}n×n is available during\nthe training phase such that Y(i, j) := yij = 1 if xi corresponds to zj and yij = 0 otherwise. The aim is\nto learn a mapping from the source language embedding space to the target language embedding space.\nA popular approach to learn this mapping W ∈Rd×d is by solving the following Orthogonal Procrustes\nproblem:\nmin\nW∈Od\nX\ni,j\nyij ∥Wxi −zj∥2 ,\n(1)\n1Heyman et al. [2019], for example, state that their approach is impractical in the supervised setting as it requires pairwise\ndictionaries for all pair of languages.\n2\nwhere Od is the set of d-dimensional orthogonal matrices. Problem (1) admits a closed-form solu-\ntion [Sch¨onemann, 1966]. Several improvements of above have been explored in this direction includ-\ning pre/post-processing of embeddings [Artetxe et al., 2018a], additional transformations [Doval et al.,\n2018], different loss functions [Joulin et al., 2018, Jawanpuria et al., 2019], to name a few.\nIn the unsupervised setting, the seed dictionary (matrix Y) is unknown. Popular unsupervised BWE\nframeworks include self-learning [Artetxe et al., 2018b], adversarial [Zhang et al., 2017a, Conneau et al.,\n2018], and optimal transport [Zhang et al., 2017b, Alvarez-Melis and Jaakkola, 2018] based approaches.\nMost unsupervised algorithms aim at learning both Y and W simultaneously, in a joint optimization\nframework. A few such the Gromov-Wasserstein approach [Peyr´e et al., 2016, Alvarez-Melis and Jaakkola,\n2018] explicitly aim at learning only the bilingual word alignment, i.e., the Y matrix, and they suggest\nusing (1) to learn the mapping operator (W). A few recent works have analyzed the effectiveness of\nsupervised and unsupervised BWE approaches [Søgaard et al., 2018, Glavaˇs et al., 2019, Vuli´c et al.,\n2019].\nMultilingual setting: In the supervised setting, a popular multilingual approach is to bilingually map\nthe embeddings of all other languages to a chosen pivot language [Ammar et al., 2016, Smith et al.,\n2017]. Kementchedjhieva et al. [2018] propose to employ the generalized Procrustes analysis method\n[Gower, 1975] to learn a MWE space. This, however, requires n-way dictionary for n languages, which\nis a stringent constraint in real-world applications. Recently, Jawanpuria et al. [2019] have proposed\na geometric approach for learning a latent shared MWE space using only a few bilingual dictionaries\n(n −1 bilingual dictionaries for n languages sufﬁce).\nChen and Cardie [2018] are among the ﬁrst to propose learning MWE in the unsupervised setting.\nThey extend the unsupervised BWE approach of [Conneau et al., 2018] – adversarial training and it-\nerative reﬁnement procedure – to the multilingual setting.\nHowever, the GAN-based procedure of\nConneau et al. [2018] has known concerns related to optimization stability with distant language pairs\n[Søgaard et al., 2018]. Alaux et al. [2019] propose a joint optimization framework for learning word\nalignment (Y) and mapping (W) between several pair of languages. It aims to learn the shared MWE\nspace by optimizing direct mappings between pairs of languages as well as indirect mappings (via a pivot\nlanguage). The bilingual alignments are learned as doubly-stochastic matrices and are modeled using the\nGromov-Wasserstein loss function. The mapping operators are modeled using the non-smooth RCSLS\nloss function [Joulin et al., 2018]. For efﬁcient optimization, they employ alternate minimization in a\nstochastic setting [Grave et al., 2019]. Heyman et al. [2019] propose to learn the shared MWE space by\nincrementally adding languages to it, one in each iteration. Their approach is based on a reformulation\nof the unsupervised BWE approach of Artetxe et al. [2018b].\n3\nUnsupervised Multilingual Multi-stage Framework\nWe develop a two-stage algorithm for unsupervised learning of multilingual word embeddings (MWEs).\nThe key idea is as follows:\n• learn unsupervised word alignment between a few pairs of languages, and then\n• use the above (learned) knowledge to learn the shared MWE space.\nWe propose to solve the above two stages sequentially, using existing techniques. This is in contrast\nto the existing unsupervised MWE works [Alaux et al., 2019, Chen and Cardie, 2018, Heyman et al.,\n2019] that aim at learning the unsupervised word alignments and cross-lingual word embedding map-\npings jointly. Though the proposed approach appears simple, we empirically observe that it has better\ngeneralization ability and robustness. We summarize the proposed approach, termed as Unsupervised\nMultilingual Multi-stage Learning (UMML), in Algorithm 1 and discuss the details below.\n3\nAlgorithm 1 Unsupervised Multilingual Multi-stage Learning (UMML)\nInput: Monolingual embeddings Xi for each language Li and an undirected, connected graph G(V, E)\nwith V = {L1, . . . , Ln}\n/*Stage 1: Generating unsupervised bilingual lexicons Yij*/\nfor each unordered pair (Li, Lj) ∈E do\nYij ←UnsupWordAlign(Xi, Xj)\nend for\n/*Stage 2: Learning multilingual word embeddings in a shared latent space*/\nRun GeoMM on G(V, E) with monolingual embeddings Xi for all languages Li and bilingual lexicons\nYij for all language pairs (Li, Lj) ∈E\nOutput of GeoMM:\na) metric B (a positive deﬁnite matrix), and\nb) orthogonal matrices Ui ∀i = 1, . . . , n\n/*Representing word embedding x of language Li in the common multilingual space*/\nx →B\n1\n2U⊤\ni x\n3.1\nStage 1: Generating Bilingual Lexicons\nThe ﬁrst stage of our framework is to generate bilingual lexicons for a few pairs of languages. These\nlexicons are used in learning a shared MWE space in the second stage. We employ existing unsuper-\nvised bilingual word alignment algorithms [Artetxe et al., 2018b, Alvarez-Melis and Jaakkola, 2018]\nto generate the bilingual lexicons.\nIt should be noted that these lexicons are learned in the bilin-\ngual setting independent of each other.\nAdditionally, they can be learned in parallel.\nOur frame-\nwork allows usage of different unsupervised bilingual word alignment algorithms [Artetxe et al., 2018b,\nAlvarez-Melis and Jaakkola, 2018, Conneau et al., 2018] for different pairs of languages. More gener-\nally, bilingual lexicon for different pairs of languages may even be obtained using different class of al-\ngorithms/resources: unsupervised, weakly-supervised with bootstrapping [Artetxe et al., 2017], human\nsupervision, etc. This is because the second stage of our framework is agnostic of how the lexicons are\nobtained. Such ﬂexibility in obtaining bilingual lexicons is desirable for learning a good quality shared\nMWE space for real-world applications since it has been observed that existing unsupervised bilingual\nword embedding algorithms may fail when languages are from distant families [Søgaard et al., 2018,\nJawanpuria et al., 2019, Glavaˇs et al., 2019, Vuli´c et al., 2019]. To the best of our knowledge, existing\nunsupervised MWE approaches do not discuss extensibility to such hybrid settings.\nWe use two unsupervised bilingual word alignment algorithms [Alvarez-Melis and Jaakkola, 2018,\nArtetxe et al., 2018b] for generating bilingual lexicons, described in Section 3.3. It should be empha-\nsized that these bilingual lexicons are learned only for a few pairs of languages. Such pairs may be\nrandomly chosen but should satisfy a very simple graph-connectivity criterion mentioned in the follow-\ning section. In our experiments, n −1 bilingual lexicons are generated for n languages.\n3.2\nStage 2: Learning Multilingual Word Embeddings\nAs stated earlier, we learn the MWEs using the bilingual lexicons obtained from the ﬁrst stage. To\nachieve our objective, we propose to employ the Geometry-aware Multilingual Mapping (GeoMM) al-\ngorithm [Jawanpuria et al., 2019].\n4\nThe setting of GeoMM may be formalized as an undirected, connected graph G, whose nodes repre-\nsent languages and edges between nodes imply availability of bilingual dictionaries (for the correspond-\ning pair of languages). GeoMM represents multiple languages in a common latent space by learning\nlanguage-speciﬁc rotations for each language (d × d orthogonal matrix Ui for each language Li) and\na similarity metric common across languages (a d × d symmetric positive-deﬁnite matrix B), where d\nis the dimensionality of the monolingual word embeddings. The rotation matrices align the language\nembeddings to a common latent space, while the (common) metric B governs how distances are mea-\nsured in this latent space. Both the language-speciﬁc parameters (Ui ∀Li) and the shared parameter\n(B) are learned via a joint optimization problem [refer Jawanpuria et al., 2019, Equation 3]. The func-\ntion that maps a word embedding x from language Li’s space to the common latent space is given by:\nx →B\n1\n2U⊤\ni x.\n3.3\nImplementation Details\nWe develop two variants of the proposed approach which differ only in the unsupervised bilingual word\nalignment algorithm employed in the ﬁrst stage.\nUMML-SL: In this method, the UnsupWordAlign subroutine in Algorithm 1 employs the un-\nsupervised self-learning algorithm developed by Artetxe et al. [2018b].\nIt should be noted that for\nUMML-SL, we do not employ various pre-processing and post-processing steps (such as whitening,\nde-whitening, symmetric re-weighting, etc.) that are included in the pipeline proposed by Artetxe et al.\n[2018b]. Hence, the simpliﬁed version of the self-learning algorithm used in our ﬁrst stage only involves\nunsupervised initialization followed by stochastic dictionary induction [Artetxe et al., 2018b]. This is\ndone to ensure that all the compared approaches (for experiments discussed in Sections 4 & 5) have the\nsame input monolingual embeddings for their main algorithm.\nUMML-GW: We employ the Gromov-Wasserstein (GW) algorithm [Alvarez-Melis and Jaakkola,\n2018] in the ﬁrst stage of this method. The GW approach formulates the bilingual word alignment prob-\nlem within the optimal transport framework [M´emoli, 2011, Peyr´e et al., 2016, Peyr´e and Cuturi, 2019].\nIt learns a doubly stochastic matrix.We then run a reﬁnement procedure to obtain a bilingual word align-\nment [Conneau et al., 2018]. The key idea in the reﬁnement procedure is that given the (probabilistic)\nword alignment obtained from the GW algorithm, a Procrustes cross-lingual mapping operator W is\nlearned by solving (1). The mapping operator W, in turn, is used to induce a (reﬁned) bilingual lexicon\nY, using cross-domain similarity local scaling (CSLS) similarity measure [Conneau et al., 2018]. The\nvocabularies used for the reﬁnement procedure is the same as those provided to the GW algorithm (i.e.,\ntop 20 000 most frequent words).\n4\nExperimental Setup\nThe experiments are aimed the following key questions on learning multilingual embeddings via unsu-\npervised word alignment:\n1. How does the proposed approach, learning bilingual word alignments and shared multilingual\nspace sequentially, fare against existing methods that learn them jointly?\n2. How robust are the existing multilingual methods when distant languages are involved? Does the\npresence of a distant language affect the learning between two close-by languages?\nWe perform rigorous evaluations on a number of tasks to answer the above questions.\nBilingual lexicon induction (BLI): A popular task to evaluate the learned cross-lingual mappings\n[Artetxe et al., 2018b, Chen and Cardie, 2018, Alaux et al., 2019, Heyman et al., 2019]. We perform\nevaluations on the MUSE [Conneau et al., 2018] and the VecMap [Dinu and Baroni, 2015, Artetxe et al.,\n5\n2017, 2018a] test datasets. Both the datasets contain pre-trained monolingual embeddings (but trained\non different corpora). Both also provide test bilingual dictionaries for various language pairs involving\nEnglish (en). MUSE also has dictionaries between a few non-English European languages: Spanish\n(es), French (fr), German (de), Italian (it), and Portuguese (pt). VecMap contains English to other lan-\nguage dictionaries for four languages: de, it, es, and Finnish (ﬁ). MUSE contains more number of\nlanguages, many of which are included in our experiments: Arabic (ar), Bulgarian (bg), Czech (cs),\nDanish (da), Dutch (nl), Finnish(ﬁ), Greek (el), Hindi (hi), Hungarian (hu), Polish (po), Russian (ru),\nand Swedish (sv). In a given dataset, if the test bilingual dictionary of a language pair is missing, (e.g.,\npo-cs in MUSE), we follow [Alaux et al., 2019] and use the intersection of their full dictionaries with\nEnglish (e.g., po-en and en-cs in MUSE) to construct a test set. Please refer [Dinu and Baroni, 2015,\nArtetxe et al., 2018a] and [Conneau et al., 2018] for more details on VecMap and MUSE, respectively.\nFollowing existing works [Chen and Cardie, 2018, Heyman et al., 2019, Alaux et al., 2019], we report\nPrecision@1 in the BLI experiments. For inference, we employ the cross-domain similarity local scal-\ning (CSLS) score [Conneau et al., 2018] in the nearest neighbor search. The BLI results on the VecMap\ndataset are discussed in Appendix A.\nCross-lingual word similarity (CLWS): We also evaluate the quality of multilingual word embeddings\non the CLWS task using the SemEval 2017 dataset [Camacho-Collados et al., 2017].\nOne of the main goals for learning multilingual word embeddings is to enable transfer learning across\nlanguages for various downstream natural language applications. Hence, we also evaluate the methods\non two other tasks: multilingual document classiﬁcation (MLDC) and multilingual dependency parsing\n(MLDP) [Ammar et al., 2016, Duong et al., 2017, Heyman et al., 2019]. Ammar et al. [2016] provide a\nplatform to evaluate MWEs on the two tasks.\nMLDP: We evaluate the quality of learned multilingual embeddings on MLDP dataset, MLParsing, sam-\npled from the Universal Dependencies 1.1 corpus [Agi´c et al., 2015]. The dataset has twelve languages:\nBulgarian, Czech, Danish, German, Greek, English, Spanish, Finnish, French, Hungarian, Italian, and\nSwedish. It has training and test sets, containing 6748 and 1200 sentences, respectively. While the test\nset for each language contains 100 sentences, the sentences in the training set of each language vary\nfrom 98 to 6694. The stack-LSTM parser [Dyer et al., 2015] used in this setup is conﬁgured to not use\nany part-of-speech/morphology attributes and to keep the input word embeddings ﬁxed [Ammar et al.,\n2016].\nMLDC: This task is evaluated on the ReutersMLDC document classiﬁcation dataset, which has docu-\nments in seven languages: Danish, German, English, Spanish, French, Italian, and Swedish. The train-\ning and the test sets contains 7000 and 13 058 documents, respective, well balanced across languages\n[Ammar et al., 2016, Heyman et al., 2019]. The document classiﬁer is based on the average perceptron\n[Klementiev et al., 2012].\nCompared methods: In addition to the proposed methods UMML-GW and UMML-SL, dis-\ncussed in Section 3.3, we consider other unsupervised multilingual word embeddings baselines: UMWE\n[Chen and Cardie, 2018] and UMH [Alaux et al., 2019]. For BLI experiments, we also evaluate state-\nof-the-art unsupervised bilingual word embeddings approach of Artetxe et al. [2018b], BilingUnsup, to\nobserve the effect of transfer learning in multilingual approaches. As in existing works, all the unsuper-\nvised methods use the top 20 000 most frequent words of each language to learn the shared embedding\nspace. We use the same hyper-parameters for UMWE, UMH, and BilingUnsup as suggested by their\nauthors.\n5\nResults and discussion\nWe now discuss and analyze the results obtained on the experimental setup described in Section 4.\n6\nde-xx\nen-xx\nes-xx\nfr-xx\nit-xx\npt-xx\nxx-de\nxx-en\nxx-es\nxx-fr\nxx-it\nxx-pt\navg.\nBilingUnsup\n60.9\n76.9\n75.6\n72.7\n75.2\n75.3\n61.6\n76.1\n77.2\n75.9\n73.6\n72.2\n72.8\nUMML-GW\n69.3\n80.2\n81.2\n78.9\n80.3\n79.9\n69.0\n81.7\n81.7\n82.0\n78.7\n76.7\n78.3\nUMML-SL\n70.5\n80.0\n81.7\n79.7\n80.9\n80.9\n69.9\n80.6\n82.3\n83.1\n79.6\n78.2\n79.0\nUMWE\n70.4\n80.6\n82.0\n79.8\n80.6\n80.6\n69.5\n77.4\n83.5\n84.1\n80.4\n79.0\n79.0\nUMH\n69.2\n79.9\n81.8\n79.4\n80.6\n80.6\n69.0\n80.7\n82.3\n82.8\n79.0\n77.6\n78.6\nTable 1: Average Precision@1 for BLI obtained by multilingual algorithms on six European languages\n(German, English, Spanish, French, Italian, and Portuguese) from the MUSE dataset. The results are\nobtained for every combination of source-target pair. The column de-xx (xx-de) denotes average per-\nformance when German is considered the source (target) language. It can be observed that all the\napproaches obtain similar generalization on the test set. The ﬁnal average of each method is computed\non thirty results.\ncs-xx\nda-xx\nde-xx\nen-xx\nes-xx\nfr-xx\nit-xx\nnl-xx\npl-xx\npt-xx\nru-xx\nBilingUnsup\n61.8\n58.7\n58.4\n64.9\n65.0\n63.3\n64.5\n63.7\n62.0\n64.4\n59.3\nUMML-GW\n64.6\n61.7\n64.1\n70.0\n69.3\n68.0\n68.7\n67.1\n65.6\n68.3\n62.4\nUMML-SL\n65.1\n61.3\n64.1\n70.2\n69.3\n68.1\n68.7\n67.4\n66.0\n68.7\n63.3\nUMWE\n57.6\n54.1\n56.8\n63.1\n62.9\n61.5\n61.9\n0.0\n58.6\n61.6\n56.3\nUMH\n63.7\n60.8\n62.8\n68.8\n68.9\n67.5\n68.0\n66.1\n64.2\n67.8\n61.9\nxx-cs\nxx-da\nxx-de\nxx-en\nxx-es\nxx-fr\nxx-it\nxx-nl\nxx-pl\nxx-pt\nxx-ru\navg.\nBilingUnsup\n51.0\n56.6\n64.5\n69.6\n71.7\n70.1\n67.7\n66.1\n53.6\n68.8\n46.3\n62.4\nUMML-GW\n53.1\n62.1\n69.3\n74.6\n76.3\n75.6\n72.3\n70.0\n55.1\n73.8\n47.6\n66.3\nUMML-SL\n53.6\n61.9\n69.5\n75.0\n76.3\n75.7\n72.4\n70.1\n55.2\n74.0\n48.4\n66.6\nUMWE\n49.5\n57.6\n60.3\n63.1\n68.4\n67.9\n65.2\n0.0\n51.0\n66.3\n45.0\n54.0\nUMH\n52.9\n60.4\n68.3\n74.1\n75.6\n74.6\n71.4\n68.6\n54.8\n72.6\n47.4\n65.5\nTable 2: Average Precision@1 for BLI on eleven European languages from the MUSE dataset. The\nlanguages are from different families such as Latin (Spanish, French, Italian, Portuguese), Germanic\n(Danish, German, English, Dutch), and Slavic (Czech, Polish, Russian). The results are obtained for\nevery combination of source-target pair. The proposed methods, UMML-SL and UMML-GW, obtain\nbetter performance than UMH and UMWE in each column. This illustrates the robustness of the pro-\nposed framework with respect to diversity in languages. The ﬁnal average of each method is computed\non 110 results.\nBilingual Lexicon Induction (BLI) Results\nExperiment 1: We begin with BLI task on a group of six relatively close European languages – German,\nEnglish, Spanish, French, Italian, and Portuguese – from the MUSE dataset. We experiment on every\npair of languages and in both directions, leading to thirty results for each method. Table 1 provides\nsummarized results of this experiment. We observe that the proposed two-stage methods, UMML-GW\nand UMML-SL, obtain scores on par with state-of-the-art methods, UMWE and UMH. This shows\nthat in case of similar/close-by languages, all the methods are able to learn a shared multilingual space\nwith similar generalization performance. We also observe that all the multilingual methods outperform\nBilingUnsup, highlighting the beneﬁt of transfer learning in this scenario.\nExperiment 2: We next expand the language set in the ﬁrst experiment to include ﬁve other European\nlanguages (Czech, Danish, Dutch, Polish, Russian) from diverse language families (all from the MUSE\ndataset). This group of eleven languages has also been employed by Alaux et al. [2019] in their BLI\nexperiments. Table 2 reports the summarized results. We observe that the proposed two-stage meth-\n7\nar-de\nar-en\nar-fr\nar-hi\nar-ru\nde-en\nde-fr\nde-hi\nde-ru\nen-fr\nen-hi\nen-ru\nfr-hi\nfr-ru\nhi-ru\nBilingUnsup\n46.5\n46.4\n55.0\n36.9\n35.2\n70.8\n61.9\n31.8\n43.6\n79.8\n31.3\n44.1\n36.1\n44.9\n24.9\nUMML-GW\n46.5\n50.5\n58.1\n0.0\n33.6\n74.0\n75.5\n0.0\n44.4\n82.5\n0.0\n47.7\n0.0\n46.2\n0.0\nUMML-SL\n46.2\n49.5\n56.5\n39.4\n34.1\n74.6\n75.2\n38.4\n45.2\n82.5\n39.0\n49.7\n42.7\n47.1\n29.7\nUMWE\n0.0\n45.0\n58.4\n41.4\n0.0\n0.0\n0.0\n0.0\n40.2\n81.9\n36.4\n0.0\n42.6\n0.0\n0.0\nUMH\n0.1\n0.1\n0.3\n0.0\n0.1\n74.7\n72.5\n0.0\n44.7\n82.0\n0.0\n46.5\n0.0\n44.5\n0.0\nde-ar\nen-ar\nfr-ar\nhi-ar\nru-ar\nen-de\nfr-de\nhi-de\nru-de\nfr-en\nhi-en\nru-en\nhi-fr\nru-fr\nru-hi\navg.\nBilingUnsup\n30.8\n29.4\n37.7\n28.7\n35.0\n72.0\n61.3\n42.0\n59.6\n78.7\n37.6\n59.2\n45.4\n62.6\n32.3\n46.7\nUMML-GW\n31.5\n35.6\n37.5\n0.0\n32.8\n74.6\n70.5\n0.0\n61.3\n83.1\n0.0\n62.9\n0.0\n65.8\n0.0\n37.2\nUMML-SL\n31.1\n35.5\n37.4\n29.7\n33.9\n75.1\n70.7\n45.5\n61.7\n82.9\n47.6\n65.6\n51.9\n66.6\n39.9\n50.8\nUMWE\n0.1\n37.6\n39.8\n23.7\n0.1\n0.0\n0.0\n0.0\n55.7\n79.5\n34.1\n0.0\n48.4\n0.0\n0.0\n22.2\nUMH\n0.2\n0.1\n0.2\n0.0\n0.1\n74.3\n69.9\n0.0\n60.8\n83.2\n0.0\n62.5\n0.0\n65.1\n0.0\n26.1\nTable 3: Precision@1 for BLI obtained by multilingual algorithms on a highly diverse group of six\nlanguages (Arabic, German, English, French, Hindi, and Russian) from the MUSE dataset. The ‘avg’\ncolumn reports the average performance of each method. Both the proposed methods, UMML-GW and\nUMML-SL, obtain better results than UMH and UMWE in this challenging BLI setup, with UMML-SL\noutperforming every method.\nods, UMML-GW and UMML-SL, perform better than UMH and outperforms UMWE. In fact both the\nproposed methods obtain better results than UMH and UMWE in the every column of Table 2. In this\nmultilingual setting, UMWE fails to satisfactorily map the Dutch language word embeddings in the\nshared multilingual space, though Dutch is similar to English. It should be noted that in the bilingual\nsetup UMWE learns an effective English-Dutch cross-lingual space (obtaining average en-nl and nl-en\nscore of 75.2). It, therefore, appears that UMWE has limitations in such multilingual settings which lead\nto its poor performance.\nExperiment 3:\nWe next aim to learn the multilingual space for a more diverse group of languages\nfrom the MUSE dataset: Arabic, German, English, French, Hindi, and Russian. Table 3 reports the BLI\nperformance for each pair of languages. We observe that, except the proposed UMML-SL, all other\nmultilingual methods fail to learn a reasonably good shared multilingual space for all languages. The\nproposed UMML-GW fails to obtain a reasonable BLI score (< 1 Precision@1) in 10 out of 30 pairs.\nUMWE and UMH suffer from such failure on 16 and 18 pairs, respectively. On the other hand, the pro-\nposed UMML-SL obtains effective alignment on all pairs of languages, illustrating its robustness in such\nchallenging setting. It obtains better performance than BilingUnsup, beneﬁting from transfer learning\nin such a diverse group as well. In the following, we analyze the results of the other three multilingual\nmethods:\n• The Gromov-Wasserstein (GW) alignment algorithm [Alvarez-Melis and Jaakkola, 2018], which is\nused in the ﬁrst stage of the proposed UMML-GW, does not learn an effective alignment of English\nand Hindi words. However, in its second stage, this “misalignment” does not adversely affect the BLI\nperformance of language pairs not involving Hindi. This can be concluded from the observation that its\nBLI score for all language pairs not involving Hindi is similar to the corresponding scores obtained by\nUMML-SL. Overall, UMML-GW is able to learn suitable multilingual embeddings for ﬁve languages\n(ar, de, en, fr, and ru).\n• UMH also employs GW based formulation [Alvarez-Melis and Jaakkola, 2018] in its joint optimiza-\ntion framework and it, too, does not learn suitable Hindi embeddings in the shared multilingual space.\nHowever, UMH also fails to learn suitable Arabic embeddings in the shared multilingual space. This is\nsurprising since the GW algorithm (employed in the ﬁrst stage of UMML-GW) learns an effective align-\nment of English and Arabic words. Hence, it appears that jointly learning the unsupervised alignment\n8\nen-de\nen-es\nde-es\nen-it\nde-it\nes-it\navg.\nLuminoso [Speer and Lowry-Duda, 2017]\n0.769\n0.772\n0.735\n0.787\n0.747\n0.767\n0.763\nNASARI [Camacho-Collados et al., 2016]\n0.594\n0.630\n0.548\n0.647\n0.557\n0.592\n0.595\nUMML-GW\n0.726\n0.724\n0.723\n0.728\n0.706\n0.742\n0.725\nUMML-SL\n0.725\n0.723\n0.721\n0.728\n0.705\n0.742\n0.724\nUMWE\n0.713\n0.706\n0.698\n0.708\n0.684\n0.727\n0.706\nUMH\n0.719\n0.721\n0.710\n0.727\n0.696\n0.732\n0.718\nTable 4: Spearman correlation (ρ) on the SemEval 2017 cross-lingual word similarity task. Among un-\nsupervised methods, the proposed UMML-GW and UMML-SL perform better than UMWE and UMH.\nMLDC\nMLDP\nUMML-GW\n89.7\n69.9\nUMML-SL\n90.3\n71.0\nUMWE\n88.3\n71.0\nUMH\n90.0\n70.6\nTable 5: Average accuracy and unlabeled attachment score (UAS) across languages on ReutersMLDC\n(MLDC) and MLParsing (MLDP) datasets, respectively. The proposed UMML-SL obtain the best re-\nsults in both downstream applications.\nand multilingual mapping can adversely affect distant languages (Arabic in this case). Overall, UMH\nlearns suitable multilingual embeddings for four languages (de, en, fr, and ru).\n• The GAN-based approach, UMWE, learns two groups of alignment in the shared multilingual space.\nThe ﬁrst group consists of Arabic, English, French, and Hindi language embeddings, which are suitably\naligned with each other. However, these are “misaligned” with German and Russian language embed-\ndings in the shared space. On the other hand, the German and Russian language embeddings are suitably\naligned with each other (but not with any other language). Such grouping cannot be attributed to lan-\nguage similarity (since English and German are closer than, for e.g., English and Arabic) and may result\nfrom optimizing instability [Søgaard et al., 2018].\nCross-lingual Word Similarity (CLWS) Results\nExperiment 4:\nTable 4 reports performance on the SemEval 2017 CLWS task for four languages:\nEnglish, German, Spanish, and Italian. For evaluating the unsupervised MWE approaches, we consider\nthe multilingual word embeddings of the four languages learned in BLI Experiment 2 (corresponding\nto Table 2). Among the unsupervised MWE approaches, we observe that UMML-GW and UMML-SL\nobtain the best results. For reference, we also include the results of the SemEval 2017 baseline and the\nbest reported system, NASARI [Camacho-Collados et al., 2016] and Luminoso [Speer and Lowry-Duda,\n2017], respectively, in Table 4. However, it should be noted that both NASARI and Luminoso use\nadditional knowledge sources like the Europarl and the OpenSubtitles2016 parallel corpora.\nResults on Downstream Applications\nExperiment 5:\nFor each of the four methods, we learn the shared multilingual space for twelve lan-\nguages in the multilingual dependency parsing (MLDP) dataset: Bulgarian, Czech, Danish, German,\nGreek, English, Spanish, Finnish, French, Hungarian, Italian, and Swedish. We employ the 300 di-\nmensional pre-trained monolingual embeddings from the MUSE dataset, vocabulary list being the top\n200 000 words in each language, as in the BLI experiments. The learned multilingual embeddings are\nalso employed in the multilingual document classiﬁcation (MLDC) evaluation, which has seven lan-\n9\nguages: Danish, German, English, Spanish, French, Italian, and Swedish. It should be noted that for this\nMLDP and MLDP tasks, related works [Ammar et al., 2016, Duong et al., 2017, Heyman et al., 2019]\ntrained 512 dimensional monolingual embeddings on the datasets used by Ammar et al. [2016] and\nDuong et al. [2017]. Hence, the presented results are not comparable with previously reported results.\nTable 5 reports the performance on the MLDP and MLDC tasks. We observe that the proposed\ntwo-stage approaches perform well on the downstream tasks with UMML-SL obtaining the best results.\n6\nDiscussion and Conclusion\nIn this work, we propose a two-stage framework for learning unsupervised multilingual word embed-\ndings (MWE). The two stages correspond to ﬁrst learning unsupervised word alignment between a few\npairs of languages and subsequently learning a latent shared multilingual space. The two problem are\nsolved using existing techniques: the ﬁrst stage is solved using the self-learning [Artetxe et al., 2018b]\nor the Gromov-Wasserstein alignment [Alvarez-Melis and Jaakkola, 2018] algorithms and the second\nstage is solved using the GeoMM algorithm [Jawanpuria et al., 2019].\nThough the two-stage framework seems a simple approach compared to the existing joint optimiza-\ntion methods [Chen and Cardie, 2018, Alaux et al., 2019], our main contribution has been to show that\nit is a strong performer. We observe that the proposed approach (UMML-SL) outperforms existing ap-\nproaches in various tasks such as bilingual lexicon induction, cross-lingual word similarity, multilingual\ndocument classiﬁcation, and multilingual document parsing. The proposed approach also exhibit ro-\nbustness while learning the MWE space for highly diverse groups of languages, a challenging setting\nfor existing approaches. The experiments were focused on the unsupervised setting, but the proposed\ntwo-stage framework has the necessary ﬂexibility to be easily employed in fully supervised setting or in\nhybrid setups where supervision is available for some languages but may be unavailable for others. Our\nresults encourage development of multi-stage models for learning multilingual word embeddings.\nReferences\nˇZeljko Agi´c, Maria Jesus Aranzabe, Aitziber Atutxa, Cristina Bosco, Jinho Choi, Marie-Catherine\nde Marneffe, Timothy Dozat, Richrd Farkas, Jennifer Foster, Filip Ginter, Iakes Goenaga, Koldo Go-\njenola, Yoav Goldberg, Jan Hajiˇc, Anders Trrup Johannsen, Jenna Kanerva, Juha Kuokkala, Veronika\nLaippala, Alessandro Lenci, Krister Lind´en, Nikola Ljubeˇsi´c, Teresa Lynn, Christopher Manning, Hc-\ntor Alonso Mart´ınez, Ryan McDonald, Anna Missil¨a, Simonetta Montemagni, Joakim Nivre, Hanna\nNurmi, Petya Osenova, Slav Petrov, Jussi Piitulainen, Barbara Plank, Prokopis Prokopidis, Sampo\nPyysalo, Wolfgang Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi, Kiril Simov, Aaron Smith,\nReut Tsarfaty, Veronika Vincze, and Daniel Zeman. Universal Dependencies 1.1, 2015.\nJean Alaux, Edouard Grave, Marco Cuturi, and Armand Joulin.\nUnsupervised hyperalignment for\nmultilingual word embeddings. In Proceedings of the International Conference on Learning Rep-\nresentations, 2019. URL: https://github.com/facebookresearch/fastText/tree/\nmaster/alignment.\nDavid Alvarez-Melis and Tommi S. Jaakkola.\nGromov-wasserstein alignment of word embedding\nspaces. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,\n2018. URL: https://github.com/dmelis/otalign.\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A. Smith.\nMassively multilingual word embeddings. Technical report, arXiv preprint arXiv:1602.01925, 2016.\nURL: https://github.com/wammar/multilingual-embeddings-eval-portal.\n10\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\nLearning principled bilingual mappings of word\nembeddings while preserving monolingual invariance. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing, pages 2289–2294, 2016.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. Learning bilingual word embeddings with (almost) no\nbilingual data. In Proceedings of the Annual Meeting of the Association for Computational Linguistics,\npages 451–462, 2017.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. Generalizing and improving bilingual word embed-\nding mappings with a multi-step framework of linear transformations. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, pages 5012–5019, 2018a.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. A robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. In Proceedings of the Annual Meeting of the Association\nfor Computational Linguistics, pages 789–798, 2018b. URL: https://github.com/artetxem/vecmap.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\nUnsupervised statistical machine translation.\nIn\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages\n3632–3642, 2018c.\nJos´e Camacho-Collados, Mohammad Taher Pilehvar, and Roberto Navigli. Nasari: Integrating explicit\nknowledge and corpus statistics for a multilingual representation of concepts and entities. Artiﬁcial\nIntelligence, 240:36–64, 2016.\nJose Camacho-Collados, Mohammad Taher Pilehvar, Nigel Collier, and Roberto Navigli.\nSemeval-\n2017 task 2: Multilingual and cross-lingual semantic word similarity. In Proceedings of the 11th\nInternational Workshop on Semantic Evaluation, 2017.\nXilun Chen and Claire Cardie.\nUnsupervise multilingual word embeddings.\nIn Proceedings\nof the Conference on Empirical Methods in Natural Language Processing,\n2018.\nURL:\nhttps://github.com/ccsasuke/umwe.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv´e J´egou. Word\ntranslation without parallel data. In Proceedings of the International Conference on Learning Repre-\nsentations, 2018. URL: https://github.com/facebookresearch/MUSE.\nGeorgiana Dinu and Marco Baroni. Improving zero-shot learning by mitigating the hubness problem.\nIn Workshop track of International Conference on Learning Representations, 2015.\nYerai Doval, Jose Camacho-Collados, Luis Espinosa-Anke, and Steven Schockaert. Improving cross-\nlingual word embeddings by meeting in the middle. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing, 2018. URL: https://github.com/yeraidm/meemi.\nLong Duong, Hiroshi Kanayama, Tengfei Ma, Steven Bird, and Trevor Cohn. Multilingual training of\ncrosslingual word embeddings. In Proceedings of the Conference of the European Chapter of the\nAssociation for Computational Linguistics, pages 894–904, 2017.\nChris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. Transition-based\ndependency parsing with stack long short-term memory. In Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics and the 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), pages 334–343, 2015.\nManaal Faruqui and Chris Dyer. Improving vector space word representations using multilingual correla-\ntion. In Proceedings of the Conference of the European Chapter of the Association for Computational\nLinguistics, pages 462–471, 2014.\n11\nGoran Glavaˇs, Robert Litschko, Sebastian Ruder, and Ivan Vuli´c. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, comparative analyses, and some misconceptions. In\nProceedings of the Annual Meeting of the Association for Computational Linguistics, pages 710–712,\n2019.\nJohn C. Gower. Generalized procrustes analysis. Psychometrika, 40(1):33–51, 1975.\nEdouard Grave, Armand Joulin, and Quentin Berthet. Unsupervised alignment of embeddings with\nWasserstein Procrustes.\nIn International Conference on Artiﬁcial Intelligence and Statistics (AIS-\nTATS), 2019.\nJiatao Gu, Hany Hassan, Jacob Devlin, and Victor OK Li. Universal neural machine translation for\nextremely low resource languages. In Proceedings of the Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, 2018.\nGeert Heyman, Bregt Verreet, Ivan Vuli´c, and Marie-Francine Moens. Learning unsupervised multilin-\ngual word embeddings with incremental multilingual hubs. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Computational Linguistics, pages 1890–1902, 2019.\nYedid Hoshen and Lior Wolf. Non-adversarial unsupervised word translation. Technical report, arXiv\npreprint arXiv:1801.06126v3, 2018.\nPratik Jawanpuria, Arjun Balgovind, Anoop Kunchukuttan, and Bamdev Mishra. Learning multilingual\nword embeddings in latent metric space: A geometric approach. Transactions of the Association for\nComputational Linguistics, 7:107–120, 2019.\nArmand Joulin, Piotr Bojanowski, Tomas Mikolov, Edouard Grave, and Herv´e J´egou. Loss in translation:\nLearning bilingual word mapping with a retrieval criterion. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing, 2018.\nY. Kementchedjhieva, S. Ruder, R. Cotterell, and A. Søgaard. Generalizing Procrustes Analysis for Bet-\nter Bilingual Dictionary Induction. In The SIGNLL Conference on Computational Natural Language\nLearning, 2018.\nAlexandre Klementiev, Ivan Titov, and Binod Bhattarai. Inducing crosslingual distributed representa-\ntions of words. In Proceedings of the International Conference on Computational Linguistics: Tech-\nnical Papers, pages 1459–1474, 2012.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsupervised ma-\nchine translation using monolingual corpora only. In Proceedings of the International Conference on\nLearning Representations, 2018.\nFacundo M´emoli. Gromov–Wasserstein distances and the metric approach to object matching. Founda-\ntions of computational mathematics, 11(4):417–487, 2011.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among languages for machine\ntranslation. Technical report, arXiv preprint arXiv:1309.4168, 2013.\nGabriel Peyr´e and Marco Cuturi. Computational optimal transport. Foundations and Trends in Machine\nLearning, 11(5-6):355–602, 2019.\nGabriel Peyr´e, Marco Cuturi, and Justin Solomon. Gromov-Wasserstein averaging of kernel and distance\nmatrices. In Proceedings of the International Conference on Machine Learning, 2016.\n12\nPeter H Sch¨onemann. A generalized solution of the orthogonal Procrustes problem. Psychometrika, 31\n(1):1–10, 1966.\nSamuel L. Smith, David H. P. Turban, Steven Hamblin, and Nils Y. Hammerla. Aligning the fastText vec-\ntors of 78 languages, 2017. URL: https://github.com/Babylonpartners/fastText_\nmultilingual.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli´c. On the limitations of unsupervised bilingual dictio-\nnary induction. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics, pages 778–788, 2018.\nRobert Speer and Joanna Lowry-Duda. ConceptNet at SemEval-2017 Task 2: Extending word embed-\ndings with multilingual relational knowledge. In Proceedings of the 11th International Workshop on\nSemantic Evaluations, 2017.\nIvan Vuli´c and Marie-Francine Moens. Monolingual and cross-lingual information retrieval models\nbased on (bilingual) word embeddings. In Proceedings of the 38th International ACM SIGIR Confer-\nence on Research and Development in Information Retrieval, pages 363–372, 2015.\nIvan Vuli´c, Goran Glavaˇs, Roi Reichart, and Anna Korhonen. Do we really need fully unsupervised\ncross-lingual embeddings? In Proceedings of the Conference on Empirical Methods in Natural Lan-\nguage Processing, 2019.\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. Normalized word embedding and orthogonal trans-\nform for bilingual word translation. In Proceedings of the Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, pages 1006–1011,\n2015.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Adversarial training for unsupervised bilin-\ngual lexicon induction. In Proceedings of the Annual Meeting of the Association for Computational\nLinguistics, pages 1959–1970, 2017a.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. Earth mover’s distance minimization for\nunsupervised bilingual lexicon induction. In Proceedings of the Conference on Empirical Methods in\nNatural Language Processing, pages 1934–1945, 2017b.\nHuiwei Zhou, Long Chen, Fulin Shi, and Degen Huang. Learning bilingual sentiment word embeddings\nfor cross-language sentiment classiﬁcation. In Proceedings of the Annual Meeting of the Association\nfor Computational Linguistics and the International Joint Conference on Natural Language Process-\ning, pages 430–440, 2015.\n13\nde-xx\nen-xx\nes-xx\nﬁ-xx\nit-xx\nxx-de\nxx-en\nxx-es\nxx-ﬁ\nxx-it\navg.\nUMML-SL\n49.5\n40.4\n45.6\n45.8\n51.3\n52.4\n37.1\n52.9\n37.2\n53.0\n46.5\nUMWE\n37.8\n0.2\n38.0\n35.1\n40.5\n39.4\n0.1\n42.8\n28.6\n40.7\n30.3\nTable 6: Average Precision@1 for BLI on ﬁve European languages (German, English, Spanish, Finnish,\nItalian) from the VecMap dataset. The results are reported for every combination of source-target pair.\nThe proposed method, UMML-SL, signiﬁcantly improves the results of UMWE.\nA\nAdditional results\nBilingual Lexicon Induction (BLI) Results\nExperiment 6:\nIn addition to the MUSE dataset, we also perform BLI experiment on the VecMap\ndataset. Table 6 (page 2 of this draft) reports result on VecMap for the proposed UMML-SL and UMWE\nmethods. We observe that UMWE learns suitable alignments for only four languages (de, es, ﬁ, and it).\nIt is not able to effectively align the English language embeddings in the shared multilingual space. On\nthe other hand, the proposed UMML-SL suitably aligns all the ﬁve languages, obtaining best perfor-\nmance in every summarized result (Table 6). On this dataset, we do not observe suitable alignment\nbetween pairs of language using the GW algorithm [Alvarez-Melis and Jaakkola, 2018]. This might be\ndue to the challenging nature of this dataset, which Alvarez-Melis and Jaakkola [2018] mention. In their\nwork, Alvarez-Melis and Jaakkola [2018] have alluded to some speciﬁc normalization performed on the\ncovariance matrices of this dataset. The authors have not responded to our query on this. Hence, without\nthe required normalization, both UMML-GW and UMH fail to learn a good alignment on any pair of\nlanguage.\nDetailed results of Experiment 1: The detailed results of Experiment 1 (of the main paper) are pro-\nvided in Table 7 in Page 2 of this draft.\nDetailed results of Experiment 2: The detailed results of Experiment 2 (of the main paper) are provided\nin Tables 8, 9, 10, and 11. The languages are from different families such as Latin (Spanish, French,\nItalian, Portuguese), Germanic (Danish, German, English, Dutch), and Slavic (Czech, Polish, Russian).\n14\nsrc\ntrg\nBilingUnsup\nUMML-GW\nUMML-SL\nUMWE\nUMH\nde\nen\n70.8\n75.7\n74.0\n71.8\n75.3\nde\nes\n59.2\n68.3\n68.1\n68.9\n68.3\nde\nfr\n61.9\n73.0\n75.5\n76.6\n73.3\nde\nit\n60.1\n70.0\n72.3\n72.2\n68.6\nde\npt\n52.6\n59.5\n62.9\n62.5\n60.6\nen\nde\n72.0\n75.3\n75.1\n76.0\n75.7\nen\nes\n80.1\n81.7\n81.7\n82.9\n82.1\nen\nfr\n79.8\n82.5\n82.5\n83.1\n82.1\nen\nit\n76.0\n79.5\n78.6\n79.0\n77.9\nen\npt\n76.4\n82.0\n81.9\n82.0\n81.6\nes\nde\n58.9\n68.3\n69.2\n68.5\n68.4\nes\nen\n79.5\n85.7\n84.1\n81.7\n84.1\nes\nfr\n78.3\n84.7\n85.9\n87.1\n85.8\nes\nit\n78.7\n82.0\n82.8\n84.7\n83.5\nes\npt\n82.5\n85.5\n86.3\n87.8\n87.1\nfr\nde\n61.1\n69.7\n70.8\n70.1\n70.0\nfr\nen\n78.7\n84.4\n83.0\n81.0\n83.1\nfr\nes\n75.6\n81.5\n82.9\n84.4\n82.7\nfr\nit\n75.4\n81.6\n82.7\n83.3\n83.1\nfr\npt\n72.6\n77.4\n79.0\n80.3\n78.0\nit\nde\n59.0\n68.5\n69.5\n68.6\n67.7\nit\nen\n75.0\n80.4\n79.7\n75.3\n79.5\nit\nes\n83.6\n86.6\n87.2\n88.4\n87.4\nit\nfr\n81.3\n86.6\n87.0\n88.3\n87.7\nit\npt\n77.1\n79.4\n80.9\n82.5\n80.9\npt\nde\n56.9\n63.3\n65.0\n64.2\n63.3\npt\nen\n76.5\n82.2\n82.1\n77.1\n81.5\npt\nes\n87.4\n90.5\n91.4\n93.1\n91.3\npt\nfr\n78.3\n83.1\n84.5\n85.6\n85.1\npt\nit\n77.6\n80.4\n81.4\n82.8\n81.7\naverage\n72.8\n78.3\n79.0\n79.0\n78.6\nTable 7: Precision@1 for BLI obtained by multilingual algorithms on six European languages (German,\nEnglish, Spanish, French, Italian, and Portuguese) from the MUSE dataset. The ‘average’ row reports\nthe average performance of each method.\n15\nsrc\ntrg\nBilingUnsup\nUMML-GW\nUMML-SL\nUMWE\nUMH\ncs\nda\n56.3\n60.8\n61.0\n62.4\n58.8\ncs\nde\n66.1\n66.6\n67.7\n65.5\n65.9\ncs\nen\n60.8\n64.5\n65.9\n61.1\n65.1\ncs\nes\n66.1\n69.6\n69.8\n68.7\n68.3\ncs\nfr\n63.5\n67.4\n67.8\n66.2\n65.8\ncs\nit\n60.0\n63.7\n64.2\n63.9\n62.5\ncs\nnl\n62.7\n65.8\n66.3\n0.0\n63.9\ncs\npl\n64.0\n64.2\n64.7\n64.6\n65.1\ncs\npt\n65.0\n68.7\n68.9\n68.7\n66.8\ncs\nru\n53.4\n54.4\n54.3\n54.5\n54.7\nda\ncs\n49.1\n49.6\n49.8\n50.5\n48.9\nda\nde\n69.5\n71.0\n70.6\n69.6\n70.6\nda\nen\n61.8\n68.5\n66.2\n62.1\n66.9\nda\nes\n65.2\n68.9\n68.8\n67.7\n68.0\nda\nfr\n61.4\n66.0\n65.4\n64.5\n64.0\nda\nit\n60.7\n63.2\n62.7\n63.7\n62.2\nda\nnl\n67.3\n71.0\n70.2\n0.0\n69.5\nda\npl\n48.2\n49.3\n48.8\n51.2\n49.2\nda\npt\n63.6\n68.8\n68.5\n68.8\n67.6\nda\nru\n40.4\n40.4\n42.1\n42.5\n41.5\nde\ncs\n51.1\n52.8\n52.9\n53.5\n51.5\nde\nda\n63.0\n67.2\n67.0\n69.3\n66.1\nde\nen\n70.8\n75.0\n74.9\n71.4\n73.4\nde\nes\n59.2\n68.4\n67.7\n67.1\n66.7\nde\nfr\n61.9\n74.7\n74.6\n74.6\n72.6\nde\nit\n60.1\n71.6\n71.7\n71.4\n68.5\nde\nnl\n71.0\n73.4\n73.4\n0.0\n72.8\nde\npl\n51.0\n52.0\n51.5\n52.7\n52.0\nde\npt\n52.6\n61.3\n61.5\n62.2\n59.4\nde\nru\n43.6\n44.7\n45.4\n46.1\n45.3\nTable 8: Precision@1 for BLI on eleven European languages from the MUSE dataset (part 1).\n16\nsrc\ntrg\nBilingUnsup\nUMML-GW\nUMML-SL\nUMWE\nUMH\nen\ncs\n47.1\n54.9\n54.8\n55.4\n52.2\nen\nda\n50.5\n61.1\n61.0\n65.7\n59.0\nen\nde\n72.0\n75.1\n75.4\n75.2\n75.3\nen\nes\n80.1\n81.7\n82.1\n82.7\n82.5\nen\nfr\n79.8\n82.7\n82.3\n83.2\n81.9\nen\nit\n76.0\n78.9\n78.8\n78.3\n77.5\nen\nnl\n70.0\n76.7\n76.1\n0.0\n75.3\nen\npl\n53.3\n59.2\n59.9\n58.5\n55.7\nen\npt\n76.4\n81.3\n81.7\n81.7\n81.3\nen\nru\n44.2\n48.8\n50.1\n50.7\n46.9\nes\ncs\n48.9\n50.2\n51.0\n53.1\n50.6\nes\nda\n57.7\n63.7\n62.8\n66.4\n62.3\nes\nde\n58.9\n69.1\n69.2\n67.4\n67.4\nes\nen\n79.5\n84.6\n84.5\n81.1\n83.9\nes\nfr\n78.3\n85.3\n85.6\n85.7\n85.8\nes\nit\n78.7\n82.5\n82.7\n83.5\n83.3\nes\nnl\n67.0\n70.4\n70.5\n0.0\n69.3\nes\npl\n51.2\n52.7\n52.5\n54.8\n52.7\nes\npt\n82.5\n86.0\n86.1\n86.4\n86.5\nes\nru\n46.9\n48.1\n48.5\n50.4\n47.0\nfr\ncs\n48.8\n50.2\n50.8\n52.8\n50.1\nfr\nda\n56.8\n62.4\n61.9\n64.8\n60.7\nfr\nde\n61.1\n70.2\n70.3\n70.3\n69.8\nfr\nen\n78.7\n84.1\n83.7\n80.3\n83.1\nfr\nes\n75.6\n81.9\n82.3\n83.0\n82.3\nfr\nit\n75.4\n82.5\n82.4\n82.5\n82.9\nfr\nnl\n69.2\n72.3\n72.3\n0.0\n71.1\nfr\npl\n49.6\n51.7\n51.6\n53.7\n51.9\nfr\npt\n72.6\n79.1\n79.2\n78.6\n77.9\nfr\nru\n44.9\n46.0\n46.8\n48.8\n45.4\nTable 9: Precision@1 for BLI on eleven European languages from the MUSE dataset (part 2).\n17\nsrc\ntrg\nBilingUnsup\nUMML-GW\nUMML-SL\nUMWE\nUMH\nit\ncs\n46.5\n49.9\n50.9\n52.8\n49.6\nit\nda\n57.0\n62.1\n61.5\n64.9\n60.3\nit\nde\n59.0\n69.3\n69.3\n66.6\n66.9\nit\nen\n75.0\n80.0\n79.5\n74.1\n79.5\nit\nes\n83.6\n87.3\n87.2\n88.0\n86.9\nit\nfr\n81.3\n87.1\n87.0\n87.1\n87.5\nit\nnl\n67.3\n71.4\n71.4\n0.0\n70.1\nit\npl\n52.9\n53.5\n53.6\n55.9\n53.4\nit\npt\n77.1\n80.9\n81.1\n80.8\n81.0\nit\nru\n44.8\n45.8\n45.7\n48.6\n45.1\nnl\ncs\n51.1\n52.8\n53.3\n0.0\n52.3\nnl\nda\n64.3\n69.0\n68.6\n0.0\n67.9\nnl\nde\n77.8\n79.5\n79.9\n0.0\n79.2\nnl\nen\n69.7\n76.6\n77.6\n0.0\n76.3\nnl\nes\n71.0\n75.0\n74.8\n0.0\n73.9\nnl\nfr\n70.6\n74.2\n74.1\n0.0\n72.8\nnl\nit\n67.6\n71.3\n71.2\n0.0\n69.8\nnl\npl\n52.0\n53.3\n53.3\n0.0\n51.9\nnl\npt\n70.6\n74.6\n75.0\n0.0\n73.2\nnl\nru\n42.2\n44.6\n45.7\n0.0\n43.8\npl\ncs\n61.8\n62.5\n63.0\n65.0\n63.7\npl\nda\n53.2\n57.5\n57.6\n60.3\n55.2\npl\nde\n63.9\n66.3\n66.2\n64.8\n64.7\npl\nen\n64.2\n68.3\n69.8\n64.0\n68.3\npl\nes\n66.3\n71.1\n71.1\n69.4\n69.3\npl\nfr\n63.1\n68.7\n69.1\n67.4\n66.7\npl\nit\n62.4\n66.6\n66.7\n65.8\n64.6\npl\nnl\n62.3\n66.3\n66.9\n0.0\n64.1\npl\npt\n65.7\n70.9\n71.1\n70.1\n67.6\npl\nru\n56.8\n57.5\n58.3\n59.0\n58.4\nTable 10: Precision@1 for BLI on eleven European languages from the MUSE dataset (part 3).\n18\nsrc\ntrg\nBilingUnsup\nUMML-GW\nUMML-SL\nUMWE\nUMH\npt\ncs\n46.8\n50.1\n50.7\n52.0\n49.7\npt\nda\n56.3\n62.7\n62.3\n64.5\n60.7\npt\nde\n56.9\n64.3\n64.9\n62.5\n62.7\npt\nen\n76.5\n81.3\n82.4\n76.3\n81.1\npt\nes\n87.4\n90.9\n91.2\n90.6\n91.5\npt\nfr\n78.3\n83.9\n84.3\n84.0\n83.8\npt\nit\n77.6\n81.1\n81.2\n81.0\n81.5\npt\nnl\n65.6\n70.5\n70.6\n0.0\n69.0\npt\npl\n52.1\n52.0\n52.2\n54.7\n52.4\npt\nru\n46.3\n45.8\n46.8\n49.2\n46.0\nru\ncs\n58.3\n58.1\n58.4\n59.6\n60.2\nru\nda\n50.7\n54.3\n55.3\n57.1\n53.3\nru\nde\n59.6\n61.2\n62.0\n61.5\n60.6\nru\nen\n59.2\n63.5\n65.9\n60.5\n63.0\nru\nes\n62.9\n67.7\n68.2\n67.0\n66.6\nru\nfr\n62.6\n66.1\n66.8\n65.9\n65.0\nru\nit\n58.2\n62.3\n62.8\n62.4\n61.0\nru\nnl\n58.2\n62.1\n62.9\n0.0\n60.5\nru\npl\n61.9\n62.8\n63.6\n63.6\n63.7\nru\npt\n61.7\n65.9\n67.1\n65.6\n64.7\naverage\n62.4\n66.3\n66.6\n54.0\n65.5\nTable 11: Precision@1 for BLI on eleven European languages from the MUSE dataset. The languages\nare from different families such as Latin (Spanish, French, Italian, Portuguese), Germanic (Danish,\nGerman, English, Dutch), and Slavic (Czech, Polish, Russian). This is the fourth part of the results,\nthe ﬁrst, second, and third parts being reported in Tables 8, 9, and 10, respectively. The ‘average’ row\nreports the average performance of each method, the average being computed over 110 BLI results.\n19\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-04-10",
  "updated": "2020-04-20"
}