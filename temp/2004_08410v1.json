{
  "id": "http://arxiv.org/abs/2004.08410v1",
  "title": "Deep Reinforcement Learning for Adaptive Learning Systems",
  "authors": [
    "Xiao Li",
    "Hanchen Xu",
    "Jinming Zhang",
    "Hua-hua Chang"
  ],
  "abstract": "In this paper, we formulate the adaptive learning problem---the problem of\nhow to find an individualized learning plan (called policy) that chooses the\nmost appropriate learning materials based on learner's latent traits---faced in\nadaptive learning systems as a Markov decision process (MDP). We assume latent\ntraits to be continuous with an unknown transition model. We apply a model-free\ndeep reinforcement learning algorithm---the deep Q-learning algorithm---that\ncan effectively find the optimal learning policy from data on learners'\nlearning process without knowing the actual transition model of the learners'\ncontinuous latent traits. To efficiently utilize available data, we also\ndevelop a transition model estimator that emulates the learner's learning\nprocess using neural networks. The transition model estimator can be used in\nthe deep Q-learning algorithm so that it can more efficiently discover the\noptimal learning policy for a learner. Numerical simulation studies verify that\nthe proposed algorithm is very efficient in finding a good learning policy,\nespecially with the aid of a transition model estimator, it can find the\noptimal learning policy after training using a small number of learners.",
  "text": "arXiv:2004.08410v1  [cs.LG]  17 Apr 2020\nXiao Li\ndepartment of educational psychology\nuniversity of illinois at urbana-champaign\nHanchen Xu\ndepartment of electrical and computer engineering\nuniversity of illinois at urbana-champaign\nJinming Zhang\ndepartment of educational psychology\nuniversity of illinois at urbana-champaign\nHua-hua Chang\ndepartment of educational studies\npurdue university\nApril 21, 2020\nPsychometrika Submission\nApril 21, 2020\n2\nDEEP REINFORCEMENT LEARNING FOR ADAPTIVE LEARNING SYSTEMS\nAbstract\nIn this paper, we formulate the adaptive learning problem—the problem of how to\nﬁnd an individualized learning plan (called policy) that chooses the most appropriate\nlearning materials based on learner’s latent traits—faced in adaptive learning systems\nas a Markov decision process (MDP). We assume latent traits to be continuous with an\nunknown transition model. We apply a model-free deep reinforcement learning\nalgorithm—the deep Q-learning algorithm—that can eﬀectively ﬁnd the optimal\nlearning policy from data on learners’ learning process without knowing the actual\ntransition model of the learners’ continuous latent traits. To eﬃciently utilize available\ndata, we also develop a transition model estimator that emulates the learner’s learning\nprocess using neural networks. The transition model estimator can be used in the deep\nQ-learning algorithm so that it can more eﬃciently discover the optimal learning policy\nfor a learner. Numerical simulation studies verify that the proposed algorithm is very\neﬃcient in ﬁnding a good learning policy, especially with the aid of a transition model\nestimator, it can ﬁnd the optimal learning policy after training using a small number of\nlearners.\nKey words: adaptive learning system, transition model estimator, Markov decision\nprocess, deep reinforcement learning, deep Q-learning, neural networks, model-free\nPsychometrika Submission\nApril 21, 2020\n3\nIntroduction\nIn a traditional classroom, a teacher uses the same learning material (e.g. textbook,\ninstruction pace, etc.) for all students. However, the selected material may be too hard for some\nstudents and too easy for some other students. Further, some students may take longer time in\nlearning than the others. Such a learning process may not be eﬃcient. These issues can be solved\nif the teacher can make an individualized learning plan for each individual student: Select an\nappropriate learning material according to each student’ ability and let a student learn at her/his\nown pace. Considering that a very low teacher-student ratio is required, such an individualized\nadaptive learning plan may be too expensive to be applied to all students. As such, adaptive\nlearning systems are developed to provide individualized adaptive learning for all\nstudents/learners. In particular, with the fast growth of digital platforms, globally integrated\nresources, and machine learning algorithms, the adaptive learning systems are becoming\nincreasingly more aﬀordable, applicable, and eﬃcient (Zhang and Chang, 2016).\nAn adaptive learning system—also referred to as a personalized/individualized learning or\nintelligent tutoring system—aims at providing a learner with optimal and individualized learning\nexperience or instructional materials so that the learner can reach a certain achievement level in a\nshortest time or reach as high as possible an achievement level in a ﬁxed period of time. First,\nlearners’ historical data are used to estimate her/his proﬁciency. Then, according to the level of\nher/his proﬁciency, the system selects the most appropriate learning material for the learner.\nAfter the learner ﬁnishes the learning material, an assessment is given to the learner and her/his\nproﬁciency level is updated and is used by the adaptive learning system to choose the next most\nappropriate learning material for the learner. Such process repeats until the learner achieves a\ncertain proﬁciency level.\nIn previous studies, the proﬁciencies or latent traits were typically characterized as vectors of\nbinary latent variables (Chen et al., 2018b; Li et al., 2018; Tang et al., 2019). However, it is\nimportant to consider the granularity of the latent traits in a complicated learning and assessment\nenvironment in which a knowledge domain consists of several ﬁne-grained abilities. In some cases,\nit would be too simple to model learners’ abilities as mastery or non-mastery. For example, when\nPsychometrika Submission\nApril 21, 2020\n4\nan item is designed to measure several latent traits and a learner regarded as mastering all related\ntraits of the item cannot be assured to answer the item correctly. A possible reason is that the\nso-called mastery is not full mastery of a latent trait. By measuring learners’ traits as continuous\nscales, the adaptive learning system can be designed to help learners to learn and improve until\nthey reach the target levels of certain abilities so that the learners can achieve target scores in\nassessments. Especially in practice, most assessments are designed to measure learners’ latent\ntraits (McGlohen and Chang, 2008). In such scenarios, it is better to use a continuous scale to\nmeasure the latent traits as the item response theory (IRT) does. In this paper, we will develop\nan adaptive learning system that estimate learners’ abilities using measurement models in order\nto provide them with most appropriate materials for further improvements.\nExisting research studies have focused on modeling learners’ learning paths (Chen et al.,\n2018a; Wang et al., 2018), accelerating learners’ memory speed (Reddy et al., 2017), providing\nmodel-based sequence recommendation (Chen et al., 2018b; Lan and Baraniuk, 2016; Xu et al.,\n2016), tracing learners’ concept knowledge state transitions over time (Lan et al., 2014), and\nselecting materials for learners optimally based on model-free algorithms (Li et al., 2018;\nTang et al., 2019). However, explicit models are typically needed to characterize learners’ learning\nprogresses in these studies. While there exist research studies that aim to ﬁnd the optimal\nlearning strategy/plan (called policy in the rest of the paper) which chooses the most appropriate\nlearning materials for learners using model-free algorithms, they all assume discrete latent traits.\nIn addition, when the number of learners is too small for the system to learn an optimal policy,\nthese algorithms are not applicable. This paper studies the important, yet less addressed adaptive\nlearning problem—the problem of ﬁnding the optimal learning policy—based on continuous latent\ntraits, and applies machine learning algorithms to deal with the tackle challenges such as only a\nsmall number of learners available in historical data.\nIn this paper, we formulate the adaptive learning problem as a Markov decision process\n(MDP), in which the state is the (continuous) latent traits of a learner, the action is the (discrete)\nlearning material given to the learner. Yet, the state transition model is unknown in practice,\nthus making the MDP unsolvable using conventional model-based algorithms such as the value\niteration algorithm (Sutton and Barto, 2018). To solve the issue, we apply a model-free deep\nPsychometrika Submission\nApril 21, 2020\n5\nreinforcement learning (DRL) algorithm, the so-called deep Q-learning algorithm, to search for\nthe optimal learning policy. The model-free DRL algorithm is a class of machine learning\nalgorithms that solve an MDP by learning an optimal policy represented by neural networks from\na sequence of state transitions directly when the transition model itself is are unknown\n(Fran¸cois-Lavet et al., 2018). DRL algorithms have been widely applied in solving a variety of\nproblems in many diﬀerent ﬁelds such as playing Atari games (Mnih et al., 2015), bidding and\npricing in electricity market (Xu et al., 2019), manipulating robotics (Gu et al., 2017), and\nlocalizing objects (Caicedo and Lazebnik, 2015). We refer interested readers to\nFran¸cois-Lavet et al. (2018) for a detailed review on the theories and applications of DRL.\nTherefore, the adaptive learning system is embedded with the well-developed measurement\nmodels and the model-free DRL algorithm so as to be more ﬂexible.\nHowever, a deep Q-learning algorithm typically requires a large amount of state transition\ndata so as to ﬁnd an optimal policy, which may be diﬃcult to obtain in practice. To cope with\nthe challenge of insuﬃcient state transition data, we develop a transition model estimator that\nemulates the learner’s learning process using neural networks. The transition model that is ﬁtted\nusing available historical transition data can be used in the deep Q-learning algorithm to further\nimprove its performance with no additional cost.\nThe purpose of this paper is to develop a fully adaptive learning system in which (i) the\nlearning material given to a learner is based on her/his continuous latent traits that indicate the\nlevels of certain abilities, and (ii) the learning policy that maps the learner’s latent traits to the\nlearning materials is found adaptively with minimal assumption on the learners’ learning process.\nFirst, an MDP formulation for the adaptive learning problem by representing latent traits in a\ncontinuum is developed. Second, a model-free DRL algorithm—the deep Q-learning algorithm—is\napplied, to the best of our knowledge, for the ﬁrst time, in solving the adaptive learning problem.\nThird, a neural network based transition model estimator is developed, which can greatly improve\nthe performance of the deep Q-learning algorithm when the number of learners is inadequate.\nLast, some interesting simulation studies are conducted to serve as demonstration cases for the\ndevelopment of adaptive learning systems.\nThe remainder of this paper is organized as follows. In the Preliminaries section, we brieﬂy\nPsychometrika Submission\nApril 21, 2020\n6\nreview measurement models and make some assumptions on the adaptive learning problem. In\nthe Adaptive Learning Problem section, we introduce the conventional adaptive learning systems\nand develop a MDP formulation for the adaptive learning problem. Then, we apply the deep\nQ-learning algorithm to solve the MDP in the Optimal Learning Policy Discovery Algorithm\nsection, where a transition model estimator that emulates the learners is also developed. Two\nsimulation studies are conducted in the Numerical Simulation section and some concluding\nremarks are made at the end of the paper.\nPreliminaries\nIn this section, we give a brief introduction on measurement models for continuous latent\ntraits, which is an important component in adaptive learning systems. The representation of\nlearners’ latent traits and assumptions on them are also presented.\nMeasurement Models\nIn an adaptive learning system, a test is given to a learner/student after each learning cycle.\nThe learner’s responses to the test items are collected by the system and her/his latent traits are\nestimated using measurement models, speciﬁcally IRT models (Rasch, 1960; Lord et al., 1968).\nAn appropriate IRT model needs to be chosen based on the test’s features such as the test’s\ndimensional structure (Zhang, 2013) and its response categories. To be more speciﬁc, in the case\nwhen item responses are recorded as binary values indicating correct or incorrect answers, the test\nthat evaluates only one latent trait will use the unidimensional item response theory IRT models\n(Rasch, 1960; Birnbaum, 1968; Lord, 1980), whereas tests that associate more than one trait will\nuse the multidimensional item response theory (MIRT) models (Reckase, 1972; Mulaik, 1972;\nSympson, 1978; Whitely, 1980). When item responses have more than two categories, polytomous\nIRT models such as the partial credit model (Masters, 1982), the generalized partial credit model\n(Muraki, 1992), and the graded response model (Samejima, 1969) are used for unidimensional\ncase. Their extensions can be applied in multidimensional cases.\nPsychometrika Submission\nApril 21, 2020\n7\nThe basic representation of an IRT model is expressed as\nP(U = u|θ) = f(θ, η, u),\n(1)\nwhere P denotes probability, U is a random variable representing the score on the test item, u is\nthe possible value of U, θ is a vector of parameters describing the learner’s latent traits, η is a\nvector of parameters indicating the characteristic of the item, and f denotes a function that maps\nθ, η, u to a probability in [0, 1]. As pointed out in Ackerman et al. (2003), many educational tests\nare inherently multidimensional. Therefore, we will use the MIRT as the intrinsic model to build\nup the adaptive learning system. As an illustration, the multidimensional two-parameter logistic\nIRT (M2PL) model is given by\nP(Uij = 1|θi, aj, dj) =\nea⊤\nj θi+dj\n1 + ea⊤\nj θi+dj ,\n(2)\nwhere Uij is the response given by ith test taker to jth item, θi = [θi1, θi2, · · · , θiD]⊤is a vector in\nRD describing a set of D latent traits, aj is a vector of D discrimination parameters for the jth\nitem, indicating the relative importance of each trait in correctly answering the item, and the\nintercept parameter dj is a scalar for item j. An applicable item j takes each element of aj to be\nnonnegative. Therefore, as each element’s value of θi increases, the probability of correct response\nincreases.\nWith an online calibration design, an accurately calibrated item bank can be acquired using\nprevious learners’ response data for an adaptive learning system without large pretest subject\npools (Makransky and Glas, 2014; Zhang and Chang, 2016). After item parameters are\npre-calibrated, a variety of latent trait estimation methods can be applied to estimate learners’\nabilities. Conventional methods such as maximum likelihood estimation (Lord et al., 1968),\nweighted likelihood estimation and Bayesian methods (e.g. expected a posteriori estimation\n(EAP), maximum a posteriori (MAP)) can accurately estimate latent traits in MIRT models.\nTheir variations are also extended for estimating the latent traits in multiple dimensions. Many\nlatent trait estimation methods result in a bias on the order of as small as O(n−1), where n\ndenotes test length, while approaches that further reduce the bias as well as the variance of\nPsychometrika Submission\nApril 21, 2020\n8\nestimates have also been identiﬁed and proposed (Firth, 1993; Tseng and Hsu, 2001; Wang, 2015;\nWarm, 1989; Zhang et al., 2011).\nAssumptions\nDenoted θ(t) = [θ(t)\n1 , · · · , θ(t)\nD ]⊤as learner’s latent traits at time step t, where D is the number\nof dimensions. Throughout this paper, we make the following simplifying yet practical\nassumptions:\nA1.No retrogression exists in latent traits. That is, θ(t+1)\nd\n≥θ(t)\nd , ∀d ∈{1, · · · , D}.\nA2.The number of learning materials is ﬁnite.\nAdaptive Learning Problem\nIn this section, we ﬁrst describe the adaptive learning problem and then formulate this\nproblem as an MDP.\nProblem Statement\nLearner\nlearning\nmaterial\nLearning\nPolicy\nLatent Traits \nEstimator\nTeacher\ntest\nresponses\nlatent\ntraits\nFigure 1.\nConventional adaptive learning system.\nA conventional adaptive learning system is illustrated in Figure 1. Such an adaptive learning\nsystem is typical in traditional classrooms and online courses like Massive Open Online Courses\n(MOOCs) (Lan and Baraniuk, 2016). In the adaptive learning system, the learner takes some\nlearning materials to improve her/his latent traits. After the learner ﬁnishes learning the\nmaterials, a test or homework is assigned to the learner. Then, the learner’s latent traits are\nPsychometrika Submission\nApril 21, 2020\n9\nestimated. Based on the estimated latent traits, the learning system adaptively determines the\nnext learning material for the learner, which may be one of many forms including a textbook\nchapter, a lecture video, an interactive task, an instructor support, or an instruction pace. Such\ncyclic learning process continues until the learner’s latent traits reach or are close to a prespeciﬁed\nlevels of proﬁciency.\nThe tests in an adaptive learning system can be computerized adaptive testing (CAT). The\nCAT is a test mode that administers tests adapted to test takers’ trait levels (Chang, 2015). It\nprovides more accurate trait estimates with much smaller number of items (Weiss, 1982) by\nsequentially selecting and administering items tailored to each individual learner. Therefore, a\nrelatively short test can assess learners’ latent traits with high accuracy.\nConventionally, the learning policy (or plan) is provided by a teacher as illustrated in Figure\n1. As aforementioned, however, it is too expensive for teachers to make an individualized adaptive\nlearning policy for each learner. In this paper, we use a DRL algorithm to search for an optimally\nindividualized adaptive learning policy for each learner. The algorithm selects the most\nappropriate learning material among all available materials for each learner based on her/his\nprovisional estimated latent traits that are obtained from her/his learning history and\nperformances in tests. The adaptive selection of learning materials guarantees the learner reaches\na prespeciﬁed proﬁciency level in a shortest number of learning cycles or reaches proﬁciency level\nas high as possible in a ﬁxed number of learning cycles. That is, instead of resorting to an\nexperienced teacher for the construction of a learning policy as illustrated in Figure 1, we will\ndevelop a systematic method to enable the adaptive learning system to discover an optimal\nlearning policy from the data that have been collected, which include historical learning materials,\ntest responses, and estimated latent traits, etc.\nMarkov Decision Process Formulation\nPrimer on Markov Decision Process\nBefore presenting the formulation for the adaptive learning problem, we ﬁrst brieﬂy review\nMDPs. An MDP is characterized by a 5-tuple (S, A, P, R, γ), where S is a set of states, A is a set\nPsychometrika Submission\nApril 21, 2020\n10\nof actions, P is a Markovian transition model, R : S × A × S →R is a reward function, and\nγ ∈[0, 1) is a discount factor (Sutton and Barto, 2018).\nA transition sample is deﬁned as\n(s, a, r, s′), where s, s′ ∈S and a ∈A, r = R(s, a, s′) is a scalar reward when the state\ntransitions into state s′ from state s after taking action a.\nLet S(t) and A(t) denote the state and action at time step t, respectively, and R(t) denote the\nreward obtained after taking action A(t) at state S(t). Note that S(t), A(t), and R(t) are random\nvariables. When both S and A are ﬁnite, the transition model P can be represented by\nconditional probability, that is,\nP(t)(s′|s, a) = P(S(t+1) = s′|S(t) = s, A(t) = a).\n(3)\nThe Markovian property of the transition model is that, for any time step t,\nP(S(t+1)|A(t), S(t), . . . , A(0), S(0)) = P(S(t+1)|A(t), S(t)).\n(4)\nEssentially, the Markovian property requires that a future state is independent of all past states\ngiven the current state. Assume P is time-homogeneous, i.e., for any two time steps t1 and t2,\nP(t1)(s′|s, a) = P(t2)(s′|s, a).\n(5)\nThen, we can drop the superscript t and write the transition model as P(s′|s, a). Note that when\nS is continuous, the transition model can be represented by a conditional probability density\nfunction.\nLet π : S →A denote a deterministic policy for the MDP deﬁned above. The action-value\nfunction for the MDP under policy π is deﬁned as follows:\nQπ(s, a) = E[\n∞\nX\nt=0\nγtR(t)|S(0) = s, A(0) = a; π],\n(6)\nwhere E denotes the expectation. The action-value function Qπ(s, a) is the expected cumulative\ndiscounted reward when the system starts from state s, takes action a, and follows policy π\nthereafter. The maximum action-value function over all policies is deﬁned as\nQ(s, a) = maxπ Qπ(s, a). A policy π is said to be optimal if Qπ(s, a) = Q(s, a) for any s ∈S and\na ∈A. In particular, the greedy policy with respect to Q(s, a), deﬁned as\nPsychometrika Submission\nApril 21, 2020\n11\nπ∗(s) = arg maxa Q(s, a), is an optimal policy (Sutton and Barto, 2018). The MDP is solved if\nwe ﬁnd π∗.\nTheorem 1. (Bertsekas and Tsitsiklis, 1996) The optimal action-value function Q(s, a)\nsatisﬁes the Bellman optimality equation:\nQ(s, a) = E[R(0)] + γ\nX\ns′∈S\nP(s′|s, a) max\na′∈A Q(s′, a′).\n(7)\nFurthermore, there is only one Q function that solves the Bellman optimality equation.\nThe Bellman optimality equation is of central importance to solving the MDP. When both S and\nA are ﬁnite and P is known, model-based based algorithms such as the value iteration algorithm\ncan be applied to solve the MDP (Sutton and Barto, 2018).\nAdaptive Learning Problem as MDP\nWe next formulate the adaptive learning problem as an MDP as follows.\nState Space: Deﬁne the vector of parameters describing the learner’s latent traits as the state, i.e.,\ns = θ, which has D continuous variables, where D represents the dimension of the latent traits.\nFor the simplicity of the algorithm construction in the following, the state space is deﬁned as\nS = [0, 1]D when each element of θ satisﬁes θ ∈[0, 1], in which a smaller value of θ indicates a\nlower ability and a larger value indicates a higher ability. Although a latent trait variable is\ntypically deﬁned on R in IRT, a closed interval, say [−5, 5], is used as the range of a latent trait\nvariable in practice. Let hd be the prespeciﬁed target proﬁciency level of the dth latent trait,\nwhich is the level the learners try to reach, where d = 1, . . . , D. Because of the fact that there is a\nbijection between [−5, hd] and [0, 1], an estimated trait θ ∈[−5, hd] can be directly transformed\ninto the scale of [0, 1]. Thus, without loss of generality, we consider the state space as S = [0, 1]D.\nAction Space: Let the learning materials available in the adaptive learning system be indexed by\n1, 2, · · · , L. The action a in the adaptive learning system is the index of a learning material,\nwhich is discrete, and the action space is A = {1, · · · , L}.\nReward Function: Recall that the objective of the adaptive learning system is to minimize the\nlearning steps it takes before a learner’s latent traits reach the maximum, i.e., for θ to reach 1D,\nPsychometrika Submission\nApril 21, 2020\n12\nwhere 1D is an all-ones vector in RD. As such, the reward function is deﬁned as follows:\nr = R(s, a, s′) =\n\n\n\n−1,\nif ||s′ −1D||∞≥10−3,\n0,\notherwise,\n(8)\nwhere || · ||∞indicates the inﬁnite norm. Intuitively, the sum of rewards over one episode (the\nentire learning process of a learner) is to the negative of the total steps a learner takes before all\nof her/his latent traits are very close to 1, which indicates that the learner has reached target\nlevels of all prespeciﬁed abilities.\nTransition Model: The probability distributions of the latent trait as well as the change of trait\nare unknown. As a result, the transition model P(s′|s, a) is not known a priori.\nBased on this MDP formulation, the adaptive learning problem is essentially to ﬁnd an\noptimal learning policy, denoted by π∗: S →A, that determines the action (learning material\nselection) based on the state (latent traits), such that the expected cumulative discounted reward\nis maximized. Note that the larger the expected cumulative discounted reward is, the less the\ntotal learning steps a learner takes to reach the target level(s) of an ability/abilities is. Since the\ntransition model P is unknown, the MDP cannot be solved using model-based algorithms such as\nthe value iteration algorithm. We will resort to a model-free DRL algorithm to solve it in the next\nsection.\nOptimal Learning Policy Discovery Algorithm\nIn this section, we solve the adaptive learning problem by using the deep Q-learning\nalgorithm, which can learn the action-value function directly from historical transition data\nwithout knowing the underlying transition model. To utilize the available transition information\nmore eﬃciently, we further develop a transition model estimator and use it to train the deep\nQ-learning algorithm.\nAction-Value Function As Deep Q-Network\nRecall that the optimal learning policy can be readily obtained if we know the action-value\nfunction. When the state is continuous and the action is discrete, which is the case in the\nPsychometrika Submission\nApril 21, 2020\n13\nadaptive learning problem, the action-value function Q(s, a) cannot be exactly represented in a\ntabular form. In such cases, the action-value function can be approximated by some functions,\nsuch as linear functions (Sutton and Barto, 2018) or artiﬁcial neural networks (simply referred to\nas neural networks) (Mnih et al., 2015). In the former case, the approximate action-value function\nis represented as an inner product of the parameter vector and a feature vector that is\nconstructed from the state. It is important to point out the choice of the features is critical to the\nperformance of the approximate action-value function. Meanwhile, neural networks are capable of\nextracting useful features from the state directly, and have stronger representation power than\nlinear functions (Goodfellow et al., 2016).\nInput Layer\nHidden Layer\nOutput Layer\nFigure 2.\nAn illustrative neural network with one hidden layer.\nAs an example for neural networks, Fig. 2 shows an illustrative neural network that consists\nof an input layer that has 3 units, a hidden layer that has 4 units, and an output layer with 2\nunits. Let x = [x1, x2, x3]⊤, h = [h1, h2, h3, h4]⊤, and y = [y1, y2]⊤denote the vectors that come\nout of the input layer, the hidden layer, and the output layer, respectively. In the neural network,\nthe output of one layer is the input for the next layer. To be more speciﬁc, h can be computed\nfrom x, and y can be computed from h as follows:\nh = φ(Whxx + bh),\n(9)\ny = Wyhh + by,\n(10)\nwhere Whx ∈R4×3 and Wyh ∈R2×4 are two weight matrices, bh ∈R4 and by ∈R2 are two bias\nvectors, and φ(·) is the so-called activation function, which is applied to its argument\nPsychometrika Submission\nApril 21, 2020\n14\nelement-wise. A popular choice of the activation function φ is the rectiﬁer, i.e., φ(x) = max(x, 0).\nConceptually, we can write the output y as a function of y = ϕ(x), where ϕ(·) is parameterized\nby Whx, Wyh, bh, and by, which can be collectively denoted as a parameter vector w. Given a set\nof input-output values denoted by {(x(i), y(i)) : i = 1, · · · , M}, the optimal value of w can be\nfound by solving the following problem:\nmin\nw\nM\nX\ni=1\n||ϕ(x(i)) −y(i)||2,\n(11)\nwhere || · || is the L2-norm. Problem (11) can be solved by using gradient descent algorithm or its\nvariants, in which the gradient of the objective function with respect to w can be computed using\nthe famous backpropagation technique. Neural networks can also be trained using a variety of\nother optimization algorithms such as Adam and RMSProp (see, Goodfellow et al., 2016). Note\nthat there may be several hidden layers and the more hidden layers there are, the deeper the\nneural network is. We refer interested readers to Goodfellow et al. (2016) for a more\ncomprehensive details about neural networks.\nRecall that in the adaptive learning problem, the state is continuous in [0, 1]D, while the\naction is discrete A = {1, · · · , L}. The approximate action-value function, denoted by ˆQ(s, a),\ncan be represented using a neural network as follows. The input layer is the state s, or\nequivalently, the latent trait vector θ, which has D units. The output has L units, each of which\ncorresponds to the action-value for one action. To more be speciﬁc, the ℓth unit in the output\nlayer gives ˆQ(s, a = ℓ), i.e., the action-value for state s and action ℓ. The number of hidden layers\nand the number of units in each hidden layer can be determined through simulation, which is to\nbe detailed in the numerical simulation section. Such a neural network is also referred to as a\ndeep Q-network (DQN) (Mnih et al., 2013). Let w denote the parameter vector of the DQN,\nwhich includes all weights and biases in the DQN. To emphasize that ˆQ(s, a) is parameterized by\nw, we write ˆQ(s, a) as ˆQ(s, a; w).\nOnce we have ˆQ(s, a; w), the optimal learning policy becomes readily available, which is\nπ∗(s) = arg maxa ˆQ(s, a; w). Then, the “Teacher” block in Figure 1 can be replaced with the\nDQN as shown in Figure 3.\nPsychometrika Submission\nApril 21, 2020\n15\nLearner\nLearning\nPolicy\nLatent Traits \nEstimator\nDeep Q-Network\nlatent\ntraits\nlearning\nmaterial\nlatent\ntraits\ntest\nresponses\nFigure 3.\nAdaptive adaptive learning system with DQN.\nLearning Policy Discovery with Deep Q-Learning\nThe parameters of the DQN can be learned from the the sequence of latent traits and\nlearning materials using the deep Q-learning algorithm proposed by Mnih et al. (2013). The\noptimal value of the parameter vector of the DQN, w, can be found by minimizing the mean\nsquared error between the approximate action-value function and the true action-value function:\nmin\nw E[( ˆQ(S, A; w) −Q(S, A))2].\n(12)\nHowever, solving (12) is extremely diﬃcult if not impossible since both Q(S, A) and the\ntransition model are unknown and thus, the expectation of the mean squared error cannot be\ncomputed. The deep Q-learning algorithm adopts two measures to cope with these challenges.\nFirst, the expectation is replaced with the sample average that can be computed from a set of\nhistorical transitions, denote by M = {(s, a, r, s′) : s, s′ ∈S, a ∈A}, with |M| = M, where | · |\ndenotes the cardinality of a set. That is, (12) is now replaced by the following problem:\nmin\nw\nX\n(s,a,r,s′)∈M\n( ˆQ(s, a; w) −Q(s, a))2.\n(13)\nAt time step t, the parameter vector is updated using the gradient descent algorithm as follows:\nw(t+1) = w(t) −α\nX\n(s,a,r,s′)∈M\n( ˆQ(s, a; w(t)) −Q(s, a))∂ˆQ(s, a; w(t))\n∂w\n,\n(14)\nwhere α > 0 is the learning rate and w(t) denotes the value of w at time step w. Second, the\nunknown Q(s, a) is further substituted by r + γ maxa′ ˆQ(s′, a′; w(t)) based on the Bellman\nPsychometrika Submission\nApril 21, 2020\n16\noptimality equation in (7). Note that when ||s′ −1D||∞< 10−3, which indicates the learning\nprocess has ended, Q(s′, a′) = 0. Therefore, (14) is now becomes\nw(t+1) = w(t) −α\nX\n(s,a,r,s′)∈M\n( ˆQ(s, a; w(t)) −y)∂ˆQ(s, a; w(t))\n∂w\n,\n(15)\nwhere\ny =\n\n\n\nr,\nif ||s′ −1D||∞< 10−3,\nr + γ maxa′ ˆQ(s′, a′; w(t)),\notherwise.\n(16)\nThe detailed deep Q-learning algorithm that is used to search the optimal parameter vector\nfor the DQN is presented in Algorithm 1, where one episode represents a complete learning\nprocess of one learner and the number of episodes is the number of learners. In order to obtain a\ngood approximate action-value function, the state-action space needs to be suﬃciently explored.\nTo achieve this, the so-called ǫ-greedy exploration is adopted in the deep Q-learning algorithm.\nSpeciﬁcally, at time step t, a random action a(t) is selected with probability ǫ(t), and a greedy\naction a(t) = maxa ˆQ(s(t), a; w(t)) is with probability 1 −ǫ(t). In this paper, we adaptively decay\nǫ(t) from ǫ to ǫ in τǫ time steps. In addition, the parameter vector is updated at each time step\nusing a set of transitions M that is resampled from the historical transitions denoted by H with\n|H| = H so as to reduce the bias that may be caused by the samples.\nTransition Model Estimator\nThe deep Q-learning algorithm requires a suﬃciently large historical transition data in order\nto ﬁnd a good approximate of the action-value function, based on which the learning policy is\nthen derived. However, we may not be able to obtain adequate transitions due to several reasons\nincluding the lack of adequate learners, and the long time it takes to acquire an individual\nlearner’s learning path (transitions). Thus, it is more desirable to develop an adaptive learning\nsystem which can eﬃciently discover the optimal learning policy after training on a relatively\nsmall number of learners. To this end, we develop a transition model estimator which emulates\nthe learning behavior of learners. Speciﬁcally, the transition model estimator can take a state s\nand an action a as inputs, and output the next state s′. This can be cast as a supervised learning\nPsychometrika Submission\nApril 21, 2020\n17\ntask, (a regression task), which can be solved using neural networks. The input layer of the neural\nnetwork that represents the transition model is a pair of state and action, and the output layer is\nthe next state. The number of hidden layers can be adjusted through the parameter tuning\nprocess (see, e.g., Goodfellow et al., 2016, for more details).\nConceptually, we can write the neural network that represents the transition model as\ns′ = ψ(s, a), the parameter vector of which is denoted by v. The optimal value of v can be found\nby solving the following problem using the backpropagation algorithm:\nmin\nv\nX\n(s,a,r,s′)∈H\n||ψ(s, a) −s′||2,\n(17)\nwhere H is the set of historical transition (data).\nThe adaptive learning system with the DQN and a transition model estimator is shown in\nFig. 4, where the DQN is trained against the transition model, instead of the actual learners.\nTransition Model \nEstimator\nLearner\nLearning\nPolicy\nLatent Traits \nEstimator\nDeep Q-Network\nlatent\ntraits\nlearning\nmaterial\nlatent\ntraits\ntest\nresponses\nlatent\ntraits\nlearning\nmaterial\nFigure 4.\nAdaptive adaptive learning system with DQN and transition model estimator.\nNumerical Simulation\nIn this section, we show the performance of the adaptive learning system with and without\nthe transition model estimator, and also investigate the impacts of latent trait estimation errors\nthrough two simulation studies.\nSimulation Overview\nConsider a group of learners in a two-dimensional assessment and a learning environment\nwith three sets of learning materials. We model the group of learners as a homogeneous MDP. Let\nPsychometrika Submission\nApril 21, 2020\n18\nthe random vector Θ(t) = [Θ(t)\n1 , Θ(t)\n2 ]⊤denote a learner’s state S(t) at time step t, which\nrepresents the latent traits in our study. Consider three sets of learning materials regarding the\ntwo-dimensional latent trait levels, that is, A = {1, 2, 3}. Each set of learning materials contain\ncontents with regards to diﬀerent latent traits. Denote the change of the latent traits from time\nstep t to t + 1 by ∆Θ(t) = [∆Θ(t)\n1 , ∆Θ(t)\n2 ]⊤. The probability of having ∆θ = [∆θ1, ∆θ2]⊤\ntransitioning from state θ = [θ1, θ2]⊤to θ′ = [θ′\n1, θ′\n2]⊤can be represented as\nP(θ′|θ, a) = P(∆Θ(t) = ∆θ|Θ(t) = θ, A(t) = a),\n(18)\nwhere a is the index of the set which the selected learning material belongs to. In the following\nnotations, we only consider the set which the selected learning material belongs to, denoted as a.\nAssume θ1, θ2 ∈[0, 1], where the value of 0 indicates extremely low ability on the corresponding\ndimension and the value of 1 indicates the target ability.\nIn addition, under Assumption A1 of no retrogression, we have ∆θ1 ∈[0, 1 −θ1] and\n∆θ2 ∈[0, 1 −θ2]. As we model the transition of the latent traits to be a continuous-state MDP,\nthe change of ∆θ1 and ∆θ2 only depends on current latent trait θ and the selected learning\nmaterial a. Therefore, we let ∆θ1 and ∆θ2 follow Beta distributions such that\n∆θ1 ∼Beta(1, g1(θ, a)), where a ∈{1, 3}, and ∆θ2 ∼Beta(1, g2(∆θ1, θ, a)), where a ∈{2, 3}.\n∆θ2 = 0 when a = 1 and ∆θ1 = 0 when a = 2, which means the ﬁrst set of materials only helps\nimproving θ1 while the second set is only related to θ2. Parameters of g1(θ, a) and g2(∆θ1, θ, a)\nin the Beta distribution are calculated by\ng1(θ, a) =\n\n\n\n3 + 8θ1 −0.2θ2,\na = 1\n15 + 15θ1 −0.4θ2,\na = 3\n(19)\nand\ng2(θ, a) =\n\n\n\n10 −θ1 + 5θ2,\na = 2\n20 −28θ1e−(θ1−0.6)2\n0.3\n+ 30θ2 −0.3∆θ1,\na = 3.\n(20)\nAn intuitive example is how a learner learns addition “+” and subtraction “–”. A learning\nprocess usually takes a long time and thus a monotonic decreasing, zero-concentrated distribution\nis adopted to simulate the ability increase. In that case, each learning step will most likely lead to\nPsychometrika Submission\nApril 21, 2020\n19\na small increase of the ability/abilities. Besides, in the distribution Beta(1, b), the larger b is, the\nmore the curve approaches 0, which results in a higher chance in generating a smaller ∆θ. It\nimplies that a higher ability the learner has on either dimension, the harder for him/her to\nfurther improve the corresponding ability. Thus, g1(θ, a) and g2(∆θ1, θ, a) have positive\ncoeﬃcients in front of θ1 and θ2, respectively. Meanwhile, we assume that a higher ability on one\ndimension helps to increase the other dimension’s ability, which results in a negative coeﬃcient\nahead of θ2 in g1(θ, a) and a negative coeﬃcient ahead of θ1 in g2(∆θ1, θ, a). In particular,\nassume the third learning material contains contents related to both abilities, and especially helps\nlearners with intermediate or high ability level of addition to improve further on subtraction. This\nassumption is included in calculating g2(θ, a) when a = 3 in equation (20). In addition, if the\nlearner makes a big progress in mastering the ability of addition, there is a higher chance for the\none to improve more on learning subtraction. Thus, the coeﬃcient of ∆θ1 in g2(∆θ1, θ, a) is\nnegative which gives a curve that is less zero-concentrated as ∆θ1 increases. Consequently, ∆θ2\nhas a higher possibility in increasing more as ∆θ1 is large. Note that the transition model is not\nrequired for adaptive learning system. The simulation gives an example in validating the\nmodel-free deep Q-learning algorithm in discovering the optimal learning policy.\nEstimation errors ranging from 1% to 15% are also added to estimated latent traits to\nevaluate their impacts on the adaptive learning system. Denote the estimation error vector by\ne = [e1, e2]⊤, where e1 and e2 are generated by the same normal distribution such that\ne1, e2 ∼N(0, σ2). As a result, 99.7% of e1, e2 lie in the range of (−3σ, 3σ). In the simulation, the\nestimated latent traits are calculated by the sum of the true latent traits and the estimation\nerrors, which are [θ1 + e1, θ2 + e2]⊤. For instance, if the standard deviation σ is 0.03, the\nobservation is [θ1 + e1, θ2 + e2]⊤, where e1, e2 ∼N(0, 0.032), and 99.7% of e1, e2 lie in the range of\n(−0.09, 0.09).\nTwo simulations cases are studied. In the ﬁrst case, the DQN is trained against actual\nlearners whose abilities’ changes follow the MDP with kernel distributions described above. In\nthis case, it is presumed that the optimal learning policy can be trained on suﬃcient number of\nlearners. The resulting optimal learning policy is compared with a heuristic learning policy, which\nselects the next learning material that can improve the not-fully-mastered ability, and a random\nPsychometrika Submission\nApril 21, 2020\n20\nlearning policy which selects any material randomly from the set of three. The impact of diﬀerent\nestimation errors is also assessed. In the second case, the DQN is trained against an estimated\ntransition model learning that is obtained using a small group of learners. The resulting optimal\nlearning policy is compared with that obtained by training against actual learners.\nSimulation Study I\n0\n200\n400\n600\n800\n1000\n1200\n1400\nEpisode\n−30\n−25\n−20\n−15\nEpisode Reward\nSmoothed Reward\nFigure 5.\nSmoothed rewards under the deep Q-learning algorithm.\nAssume all learners are beginners on the two latent traits when using the adaptive learning\nsystem, i.e. Θ(0) = [0, 0]⊤. The DQN has two hidden layers, the ﬁrst of which has 64 units and\nthe second of which has 32 units. The DQN is trained against 2000 learners that are simulated\naccording to the method discussed earlier, i.e. E = 2000. Other parameters are chosen as follows:\nγ = 0.9, α = 6 × 10−4, ǫ = 1.0, ǫ = 0.1, τǫ = 2000, M = 256. The Adam algorithm is adopted for\nthe training of the DQN.\nFigure 5 presents the smoothed reward under the deep Q-learning algorithm across the ﬁrst\n1500 episodes with a smoothing window of 20. It can be seen that the reward converges to −15\nafter 600 episodes, which indicates the optimal learning policy is found after the DQN is trained\nusing 600 learners.\nPsychometrika Submission\nApril 21, 2020\n21\n25\n50\n75\n100\n125\n150\n175\n200\nEpisode\n−35\n−30\n−25\n−20\n−15\n−10\nEpisode Reward\nDQN Smoothed Reward\nHeuristic Smoothed Reward\nRandom Smoothed Reward\nFigure 6.\nSmoothed rewards under DQN, heuristic, and random learning policies.\nFigure 6 and Table 1 compare smoothed rewards across 200 new learners, labeled as episodes\nin Figure 6, with a smoothing window of 20 between the optimal learning policy found by the\ndeep Q-learning algorithm after being trained in 2000 episodes—referred to as the DQN learning\npolicy, the heuristic learning policy, and the random learning policy. The larger the reward is, the\nfewer learning steps a learner takes to fully master the two latent traits, or in another word, the\nbetter the learning policy is. Clearly, the rewards obtained by the deep Q-learning algorithm have\na higher mean and smaller standard deviation (SD) than those obtained by the heuristic learning\npolicy and the random learning policy. These results show that the learning policy found by the\ndeep Q-learning algorithm is much better than the other two.\nPsychometrika Submission\nApril 21, 2020\n22\nState Path\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nθ1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nθ2\nFigure 7.\nAn example of state transition path with action sequence of 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2.\nFigure 7 presents an example of a state transition path that shows how the latent traits\nchange with a sequence of actions taken under the DQN learning policy obtained without\nconsidering estimation error. Take the addition and subtraction test as an example. The ﬁrst\nlearning material is repeatedly selected to improve the learner’s ability of addition at the\nbeginning. Then the third material related to both addition and subtraction is selected. In the\nlast few steps, the second learning material is chosen to further improve the learner’s ability of\nsubtraction.\nPsychometrika Submission\nApril 21, 2020\n23\n0%\n0.5%\n1%\n2%\n3%\n3.3%\n4%\n5%\nError Standard Deviation\n−40\n−30\n−20\n−10\nEpisode Reward\nDQN\nHeuristic\nFigure 8.\nComparison of rewards under DQN and heuristic learning policies with various estimation errors.\nFigure 8 compares rewards under the DQN and the heuristic learning policies when\nestimation errors with various standard deviations (σ) exist. It shows that the mean rewards\nobtained by the DQN learning policy under various estimation errors are consistently higher than\nthose of the heuristic learning policy when estimation errors exist. That is, the DQN learning\npolicy still outperforms the heuristic learning policy even with the presence of estimation errors,\nwhich demonstrates that the deep Q-learning algorithm is reliable and stable in ﬁnding optimal\nlearning policy with the presence of estimation errors.\nSimulation Study II\nNext, we show the performance of the adaptive learning system with a transition model\nestimator, which is represented using a neural network with one hidden layer that has 32 units.\nThe prediction accuracy indices are presented in Table 2. The train and test scores are deﬁned as\nthe coeﬃcient of determination in the training and test sets respectively, calculated by\n1 −\nPH\ni=1 ||s(i) −ˆs(i)||2\nPH\ni=1 ||s(i) −s||2 ,\n(21)\nwhere s is the true state, s is average value of the true state, ˆs is the predicted state using\nprevious state and the action taken, and H is the number of the transitions. The best possible\nPsychometrika Submission\nApril 21, 2020\n24\nscore is 1. The root mean square error (RMSE) is calculated by\nRMSE =\nsPH\ni=1 ||s(i) −ˆs(i)||2\nH\n.\n(22)\n10\n20\n30\n40\n50\n100\n150\n200\n2000\nEpisodes\n−30\n−20\n−10\nEpisode Reward\nActual\nVirtual\nFigure 9.\nComparison of rewards under actual and virtual DQN learning policies.\nA DQN is trained on 2000 episodes against the estimated transition model that is ﬁtted\nusing a certain number of actual learners; the learning policy corresponding to this DQN is\nreferred to as the virtual DQN learning policy. For the purpose of comparison, another DQN is\ntrained on the same number of actual learners; the learning policy corresponding to this DQN is\nreferred to as the actual DQN learning policy. Essentially, these two learning policies diﬀer in the\nway how the same set of actual learners are utilized. The actual learners are simulated according\nto the method discussed in “Simulation Overview” section and are used to train the actual DQN\nlearning policy directly. In contrast, these actual learners are used to ﬁrst ﬁt a transition model,\nwhich is then used to train the virtual DQN learning policy; this allows the virtual DQN learning\npolicy to be trained over as many episodes as it needs. Figure 9 compares rewards obtained by\nthe two DQN learning policies when various numbers of actual learners are utilized. It is shown\nthat with no more than 200 actual learners, the utilization of the transition model can\nsigniﬁcantly improve the performance of the learning policy, generating much larger mean rewards\nPsychometrika Submission\nApril 21, 2020\n25\ncompared than the algorithm without using the transition model. When the number of learners is\nlarge enough, both two approaches found optimal learning policies and yield similar rewards.\nConcluding Remarks and Future Directions\nIn this paper, we developed an MDP formulation for an adaptive learning system by\ndescribing learners’ latent traits as continuous instead of simply classifying learners as mastery or\nnon-mastery of certain skills. The objective of the system is to improve learners’ abilities to the\nprespeciﬁed target levels. We developed a deep Q-learning algorithm, which is a model-free DRL\nalgorithm that can eﬀectively ﬁnd the optimal learning policy from data on learners’ learning\nprocess without knowing the transition model of the learner’s latent traits. To cope with the\nchallenge of insuﬃcient state transition data, which may result in a poor performance of the deep\nQ-learning algorithm, we developed a transition model estimator that emulates the learner’s\nlearning process using neural networks, which can be used to further train the DQN and improve\nthe its performance.\nThe two simulation studies presented in the paper veriﬁed that the proposed methodology is\nvery eﬃcient in ﬁnding a good learning policy for adaptive learning systems without any help\nfrom a teacher. The optimal learning policy found by the DQN algorithm outperformed the\nheuristic and random methods with much higher rewards, or equivalently, much fewer learning\nsteps/cycles for learners to reach the target levels of all prespeciﬁed abilities. Particularly, with\nthe aid of a transition model estimator, the adaptive learning system can ﬁnd a good learning\npolicy eﬃciently after training using a few learners.\nThe directions for extending the adaptive learning research include applying the adaptive\nlearning system on actual learners to further assess the eﬃciency of the proposed methodology.\nBoth the DQN algorithm and the transition model estimator can be adopted and evaluated\nthrough real data analysis on an online learning platform. Second, the adaptive learning system\nhere consists of a latent trait estimator which uses measurement models to estimate latent traits\nand a learning policy. Instead, some research construct the system assuming that learning\nmaterials inﬂuence learners’ responses to test items directly, without the latent trait estimator\nincorporated (Lan et al., 2014; Lan and Baraniuk, 2016). As such, learners’ learning process is\nPsychometrika Submission\nApril 21, 2020\n26\nmodeled and traced directly and model-free algorithms can be proposed to ﬁnd the optimal\nlearning policy. Third, because each group of learners assumes to follow a homogeneous MDP,\nfurther researches can be conducted to classify learners into groups before they use the adaptive\nlearning system in order to ﬁnd the optimal learning policy for each group. Fourth, machine\nlearning algorithms for recommendation systems (e.g., collaborative ﬁltering, matrix\ndecomposition, etc.) can be incorporated with the DRL algorithm to better recommend not only\noptimal but also preferred materials to learners (Li et al., 2010). Fifth, it would be interesting to\nfurther examine how much better is the DQN learning policy than the random and heuristic\nlearning policies under diﬀerent scenarios and constrains. Sixth, it is also interesting to formulate\nthe adaptive learning problem as a partially observable Markov decision process (POMDP) and\nexplore solutions to the problem. Finally, future studies can include the modeled learning paths\n(Chen et al., 2018a; Wang et al., 2018) as learners’ historical data to search the optimal learning\npolicy more eﬃciently.\nPsychometrika Submission\nApril 21, 2020\n27\nReferences\nAckerman, T. A., Gierl, M. J., and Walker, C. M. (2003). Using multidimensional item response\ntheory to evaluate educational and psychological tests. Educational Measurement: Issues and\nPractice, 22(3):37–51.\nBertsekas, D. P. and Tsitsiklis, J. N. (1996). Neuro-dynamic programming, volume 5. Athena\nScientiﬁc Belmont, MA.\nBirnbaum, A. (1968). Some latent trait models and their use in inferring an examinee’s ability.\nStatistical theories of mental test scores, pages 397–472.\nCaicedo, J. C. and Lazebnik, S. (2015). Active object localization with deep reinforcement\nlearning. In Proceedings of the IEEE International Conference on Computer Vision, pages\n2488–2496.\nChang, H.-H. (2015). Psychometrics behind computerized adaptive testing. Psychometrika,\n80(1):1–20.\nChen, Y., Culpepper, S. A., Wang, S., and Douglas, J. (2018a). A hidden markov model for\nlearning trajectories in cognitive diagnosis with application to spatial rotation skills. Applied\nPsychological Measurement, 42(1):5–23.\nChen, Y., Li, X., Liu, J., and Ying, Z. (2018b). Recommendation system for adaptive learning.\nApplied Psychological Measurement, 42(1):24–41.\nFirth, D. (1993). Bias reduction of maximum likelihood estimates. Biometrika, 80(1):27–38.\nFran¸cois-Lavet, V., Henderson, P., Islam, R., Bellemare, M. G., Pineau, J., et al. (2018). An\nintroduction to deep reinforcement learning. Foundations and Trends R\n⃝in Machine Learning,\n11(3-4):219–354.\nGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep learning. MIT press.\nPsychometrika Submission\nApril 21, 2020\n28\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2017). Deep reinforcement learning for robotic\nmanipulation with asynchronous oﬀ-policy updates. In 2017 IEEE International Conference on\nRobotics and Automation (ICRA), pages 3389–3396. IEEE.\nLan, A. S. and Baraniuk, R. G. (2016). A contextual bandits framework for personalized learning\naction selection. In Proceedings of the 9th International Conference on Educational Data\nMining, Raleigh, NC: EDM, pages 424–429.\nLan, A. S., Studer, C., and Baraniuk, R. G. (2014). Time-varying learning and content analytics\nvia sparse factor analysis. In Proceedings of the 20th ACM SIGKDD international conference\non Knowledge discovery and data mining, pages 452–461. ACM.\nLi, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to\npersonalized news article recommendation. In Proceedings of the 19th international conference\non World wide web, pages 661–670. ACM.\nLi, X., Xu, H., Zhang, J., and Chang, H.-h. (2018). Optimal hierarchical learning path design\nwith reinforcement learning. arXiv preprint arXiv:1810.05347.\nLord, F. M. (1980). Application of item response theory to practical testing problems.\nLord, F. M., Novick, M. R., and Birnbaum, A. (1968). Statistical theories of mental test scores.\n1968. Reading: Addison-Wesley Google Scholar.\nMakransky, G. and Glas, C. A. (2014). An automatic online calibration design in adaptive\ntesting. Journal of Applied Testing Technology, 11(1):1–20.\nMasters, G. N. (1982). A rasch model for partial credit scoring. Psychometrika, 47(2):149–174.\nMcGlohen, M. and Chang, H.-H. (2008). Combining computer adaptive testing technology with\ncognitively diagnostic assessment. Behavior Research Methods, 40(3):808–821.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller,\nM. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\nPsychometrika Submission\nApril 21, 2020\n29\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A.,\nRiedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through\ndeep reinforcement learning. Nature, 518(7540):529.\nMulaik, S. (1972). A mathematical investigation of some multidimensional rasch models for\npsychological tests. In Annual Meeting of the Psychometric Society, Princeton, NJ.\nMuraki, E. (1992). A generalized partial credit model: Application of an em algorithm. ETS\nResearch Report Series, 1992(1):i–30.\nRasch, G. (1960). Probabilistic models for some intelligence and attainment tests. Copenhagen:\nDanish Institute for Educational Research.\nReckase, M. D. (1972). Development and application of a multivariate logistic latent trait model.\nReddy, S., Levine, S., and Dragan, A. (2017). Accelerating human learning with deep\nreinforcement learning. In NIPS’17 Workshop: Teaching Machines, Robots, and Humans.\nSamejima, F. (1969). Estimation of latent ability using a response pattern of graded scores.\nPsychometrika monograph supplement.\nSutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\nSympson, J. B. (1978). A model for testing with multidimensional items. In Proceedings of the\n1977 computerized adaptive testing conference, number 00014. University of Minnesota,\nDepartment of Psychology, Psychometric Methods.\nTang, X., Chen, Y., Li, X., Liu, J., and Ying, Z. (2019). A reinforcement learning approach to\npersonalized learning recommendation systems. British Journal of Mathematical and Statistical\nPsychology, 72(1):108–135.\nTseng, F.-L. and Hsu, T.-C. (2001). Multidimensional adaptive testing using the weighted\nlikelihood estimation: a comparison of estimation methods. In Annual meeting of NCME,\nSeattle.\nPsychometrika Submission\nApril 21, 2020\n30\nWang, C. (2015). On latent trait estimation in multidimensional compensatory item response\nmodels. Psychometrika, 80(2):428–449.\nWang, S., Yang, Y., Culpepper, S. A., and Douglas, J. A. (2018). Tracking skill acquisition with\ncognitive diagnosis models: A higher-order, hidden markov model with covariates. Journal of\nEducational and Behavioral Statistics, 43(1):57–87.\nWarm, T. A. (1989). Weighted likelihood estimation of ability in item response theory.\nPsychometrika, 54(3):427–450.\nWeiss, D. J. (1982). Improving measurement quality and eﬃciency with adaptive testing. Applied\nPsychological Measurement, 6(4):473–492.\nWhitely, S. E. (1980). Multicomponent latent trait models for ability tests. Psychometrika,\n45(4):479–494.\nXu, H., Sun, H., Nikovski, D., Kitamura, S., Mori, K., and Hashimoto, H. (2019). Deep\nreinforcement learning for joint bidding and pricing of load serving entity. IEEE Transactions\non Smart Grid, pages 1–1.\nXu, J., Xing, T., and Van Der Schaar, M. (2016). Personalized course sequence recommendations.\nIEEE Transactions on Signal Processing, 64(20):5340–5352.\nZhang, J. (2013). A procedure for dimensionality analyses of response data from various test\ndesigns. Psychometrika, 78(1):37–58.\nZhang, J., Xie, M., Song, X., and Lu, T. (2011). Investigating the impact of uncertainty about\nitem parameters on ability estimation. Psychometrika, 76(1):97–118.\nZhang, S. and Chang, H.-H. (2016). From smart testing to smart learning: how testing\ntechnology can assist the new generation of education. International Journal of Smart\nTechnology and Learning, 1(1):67–92.\nPsychometrika Submission\nApril 21, 2020\n31\nAlgorithm 1: Deep Q Learning Algorithm for Adaptive Learning Problem\nInput: γ, α, ǫ, ǫ, τǫ, M, E\nOutput: w\nRandomly initialize w and set total time step counter τ = 0\nfor episode = 1, · · · , E do\nReceive initial state s0\nfor t = 0, 1, · · · , do\nCompute ǫ(t) = ǫ −(ǫ −ǫ) × min(τ/τǫ, 1) and increase τ by 1\nWith probability ǫ(t) select a random action a(t) otherwise select\na(t) = maxa ˆQ(s(t), a; w(t))\nSend the learning material determined by a(t) to the learner\nGiven the learner a test and collect test response\nReceive new state s(t+1) estimated from test response by latent trait estimator\nCompute reward r(t) according to\nr(t) =\n\n\n\n−1,\nif ||s(t+1) −1D||∞≥10−3\n0,\notherwise\nStore transition (s(t), a(t), r(t), s(t+1)) into H\nSample M transitions from H and store them into M\nUpdate w according to\nw(t+1) = w(t) −α\nX\n(s,a,r,s′)∈M\n( ˆQ(s, a; w(t)) −y)∂ˆQ(s, a; w(t))\n∂w\nwhere\ny =\n\n\n\nr,\nif ||s′ −1D||∞< 10−3\nr + γ maxa′ ˆQ(s′, a′; w(t)),\notherwise\nif ||s′ −1D||∞< 10−3, break\nend\nend\nPsychometrika Submission\nApril 21, 2020\n32\nTable 1.\nMean and Standard Deviation (SD) of Rewards under DQN, Heuristic, and Random Learning Policies.\nMethods\nDQN\nHeuristic\nRandom\nReward mean\n-13.49\n-21.55\n-24.85\nReward SD\n4.59\n4.76\n5.59\nTable 2.\nAccuracy of Transition Model Trained against Various Numbers of Learners.\nNo. of learners\n10\n20\n30\n40\n50\n100\n150\n200\n2000\nTrain Score\n0.96\n0.97\n0.97\n0.97\n0.97\n0.97\n0.97\n0.97\n0.97\nTest Score\n0.95\n0.97\n0.96\n0.96\n0.97\n0.97\n0.97\n0.97\n0.97\nRMSE\n0.11\n0.08\n0.09\n0.09\n0.08\n0.08\n0.08\n0.08\n0.08\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-04-17",
  "updated": "2020-04-17"
}