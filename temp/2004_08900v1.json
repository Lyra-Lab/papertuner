{
  "id": "http://arxiv.org/abs/2004.08900v1",
  "title": "The Cost of Training NLP Models: A Concise Overview",
  "authors": [
    "Or Sharir",
    "Barak Peleg",
    "Yoav Shoham"
  ],
  "abstract": "We review the cost of training large-scale language models, and the drivers\nof these costs. The intended audience includes engineers and scientists\nbudgeting their model-training experiments, as well as non-practitioners trying\nto make sense of the economics of modern-day Natural Language Processing (NLP).",
  "text": "THE COST OF TRAINING NLP MODELS\nA CONCISE OVERVIEW\nOr Sharir\nAI21 Labs\nors@ai21.com\nBarak Peleg\nAI21 Labs\nbarakp@ai21.com\nYoav Shoham\nAI21 Labs\nyoavs@ai21.com\nApril 2020\nABSTRACT\nWe review the cost of training large-scale language models, and the drivers of these costs. The\nintended audience includes engineers and scientists budgeting their model-training experiments, as\nwell as non-practitioners trying to make sense of the economics of modern-day Natural Language\nProcessing (NLP).1\n1\nCosts: Not for the faint hearted\nThe cost of ﬂoating-point operations (FLOPs), the basic Neural Network (NN) operation, has been decreasing. For\nexample, Google reported [1] a 38% cost decrease in ResNet-50 training costs2. This was achieved with optimized\nhardware (moving from GPUs to TPUs) coupled with framework-level optimizations, exploiting parallelism opportu-\nnities. This kind of cost reduction isn’t an isolated occurrence – we’re seeing the costs of training large models fall\nas hardware innovations and training techniques improve. Despite this, overall costs have increased, and can run into\nthe millions. We’ll explain why this is occurring and what factors play a signiﬁcant role in the costs of training3 NLP\nmodels.\nJust how much does it cost to train a model? Two correct answers are “depends” and “a lot”. More quantitatively,\nhere are current ballpark list-price costs of training differently sized BERT [4] models on the Wikipedia and Book\ncorpora (15 GB). For each setting we report two numbers - the cost of one training run, and a typical fully-loaded cost\n(see discussion of \"hidden costs\" below) with hyper-parameter tuning and multiple runs per setting (here we look at a\nsomewhat modest upper bound of two conﬁgurations and ten runs per conﬁguration).4\n• $2.5k - $50k (110 million parameter model)\n• $10k - $200k (340 million parameter model)\n• $80k - $1.6m (1.5 billion parameter model)\nThese already are signiﬁcant ﬁgures, but what they imply about the cost of training the largest models of today is\neven more sobering. Exact ﬁgures are proprietary information of the speciﬁc companies, but one can make educated\n1We thank Barak Lenz, Shai Shalev-Shwartz and other members of AI21 Labs, as well as Jack Clark, Jeff Dean, Deep Ganguli,\nChris Re, Sebastian Ruder and Lior Wolf, who generously commented on previous drafts. Further comments on the document are\nwelcome, and the document will be updated as appropriate. Note: While the comments of our colleagues from other organizations\ngreatly improved the document, they were not representing their organizations, did not share any proprietary information, and may\nnot necessarily agree with everything written here.\n2It also reported a dramatic 27× decrease in training time. While training time is not our focus, it is relevant indirectly:\nCompressed time makes it realistic to train larger models, which costs more.\n3There is a whole other discussion to be had on the costs of NLP models at inference time. These are quite related to the training\ncosts, but deserve a separate discussion. In particular, the inference phase allows for post-training model optimizations, for example\nvia model distillation [2, 3]. This discussion is beyond the scope of this article.\n4The following ﬁgures are based on internal AI21 Labs data. They can be somewhat lower due to discounts, or using preemptible\nversions of the system. The ﬁgures also assume the use of cloud solutions such as GCP or AWS, and on-premise implementations\nare sometimes cheaper. Still, the ﬁgures provide a general sense of the costs.\narXiv:2004.08900v1  [cs.CL]  19 Apr 2020\nThe Cost of Training NLP Models: A Concise Overview\nguesses. For example, based on information released by Google, we estimate that, at list-price, training the 11B-\nparameter variant5 of T5 [5] cost well above $1.3 million for a single run. Assuming 2-3 runs of the large model and\nhundreds of the small ones, the (list-)price tag for the entire project may have been $10 million6.\nNot many companies – certainly not many startups – can afford this cost. Some argue that this is not a severe issue; let\nthe Googles of the world pre-train and publish the large language models, and let the rest of the world ﬁne-tune them\n(a much cheaper endeavor) to speciﬁc tasks. Others (e.g., Etchemendy and Li [6]) are not as sanguine.\n2\nCost Drivers: Size Matters\nWe are not aware of a formula that tells you how many FLOPs are needed in a given NLP setting to achieve a given\nperformance7. However, there are several variables that impact this number, all of which have increased dramatically\nin the past few years, far surpassing the once-deemed “massive” vision-focused ML models.8\nHere are some of the relevant variables, which fall into three categories: (a) size of dataset, (b) model size (we use\nthe number of parameters as a proxy), and (c) training volume (we use as proxy the total number of tokens processed\nduring pre-training). The top row applies to all models, and the bottom row zooms in on transformer-based models.\nZoom-in on Transformer-speciﬁc Attributes\nLayers\n0\n20\n40\n60\n80\nGPT\nBERT-Large\nGPT2-1.5B\nRoBERTa\nXLNet\nELECTRA-1.75M\nMegatronLM\nT5-11B\nTuring-NLG\n78\n48\n72\n24\n24\n24\n48\n24\n12\nAttention Heads\n0\n35\n70\n105\n140\nGPT\nBERT-Large\nGPT2-1.5B\nRoBERTa\nXLNet\nELECTRA-1.75M\nMegatronLM\nT5-11B\nTuring-NLG\n28\n128\n32\n16\n16\n16\n12\n16\n12\nContextual-Embedding \nDimension\n0K\n1K\n3K\n4K\n5K\nGPT\nBERT-Large\nGPT2-1.5B\nRoBERTa\nXLNet\nELECTRA-1.75M\nMegatronLM\nT5-11B\nTuring-NLG\n4256\n1024\n3072\n1024\n1024\n1024\n1600\n1024\n768\nFeed-Forward Dimension\n0K\n18K\n36K\n54K\n72K\nGPT\nBERT-Large\nGPT2-1.5B\nRoBERTa\nXLNet\nELECTRA-1.75M\nMegatronLM\nT5-11B\nTuring-NLG\n17K\n66K\n12K\n4K\n4K\n4K\n6K\n4K\n3K\ncredit:\nBird’s-eye View\nData Size\n(billion words)\n0\n10\n20\n30\n40\nWSJ\nWikipedia\nOpenWebText\nC4\n35\n8.5\n2.5\n0.03\nModel Size\n(billion parameters)\n0\n5\n10\n15\n20\nGPT\nBERT-Large\nGPT2-1.5B\nRoBERTa\nXLNet\nELECTRA-1.75M\nMegatronLM\nT5-11B\nTuring-NLG\n17.0\n11.0\n8.3\n0.3\n0.4\n0.4\n1.5\n0.3\n0.1\nTraining Volume\n(trillion tokens)\n0\n0.6\n1.2\n1.8\n2.4\nGPT\nBERT-Large\nGPT2-1.5B\nRoBERTa\nXLNet\nELECTRA-1.75M\nMegatronLM\nT5-11B\nTuring-NLG\n0.2\n1\n0.2\n1.8\n2.1\n2.1\n0.5\n0.1\n.03\nIn NLP,\nEverything is Big\nand Getting\nBigger\n† The total number of tokens processed during pre-training. This is the product of three different attributes affecting total FLOPs cost of training (beyond the \ncost attributed to model size): input sequence length, training steps, and batch size.\n†\n5 With context lengths of 512 for both encoding and decoding, 128 attention heads, and 65k-dimensional feed-forward layers.\n6These $ ﬁgures come with substantial error bars, but we believe they are in the right ballpark.\n7 It is worth noting the work of [7], which analyzes the impact of various variables, including model size and amount of compute,\non performance, as measured by perplexity. Although the paper does not directly address the question we are after, the methodology\nit offers may provide useful hints. Other relevant papers include [8, 9, 10].\n8Although computer vision is not our focus here, the contrast with NLP is striking, and we discuss it brieﬂy in Appendix A.\n2\nThe Cost of Training NLP Models: A Concise Overview\nThe exact ways in which these increases impact the number of FLOPs are subtle, and depend on the speciﬁc training\nscheme and architecture. For example, fewer FLOPs are needed when training BERT-style models versus GPT-2 [11]\nmodels with comparable model and data sizes, and training steps. Other training schemes can introduce additional\nfactors that dictate cost; for example, the adversarial training scheme of ELECTRA [12] uses an additional “generator”\nmodel during training. This increases the relative per-step costs, but requires fewer steps, thus reducing the overall\ncosts. Despite these subtleties, however, it is clear that all these growing numbers correlate with an overall trend\ntowards a greater number of FLOPs, which determine the bottom line.\nOn top of the above, there are also additional hidden costs, which are often overlooked. Each model must be trained\nmultiple times – this is in order to minimize random effects (each run is inherently stochastic), and to search over a\ncombinatorially large hyper-parameter search space. This means there can be a large multiple over the cost of a single\ntraining episode (although signiﬁcant cost savings can be had by conducting most of the experiments on the smaller\nmodels ﬁrst, before training the large models in the optimized conﬁguration).\n3\nThe Future\nThe reason the community has adopted the mega-scale, brute-force statistical approach is that it works; it has yielded\nbetter performance than any alternative. And since NLP has substantial economic value, no cost is too high in pursuit\nof good performance. We do not see an end to the use of large NN models operating on massive corpora, and one\ncan imagine the costs escalating further, as the community develops more elaborate architectures in pursuit of more\nambitious tasks. As you go from sentences to whole documents and beyond, you can imagine the need for more\ndimensions per token, longer contexts, and potentially more layers. Adding external knowledge sources, although\npotentially reducing the sole reliance on the network (see below), could also contribute to expanding the size of the\nnetwork in order to reﬂect the external knowledge in the embedding space. Indeed, there is already discussion [13] of\n100B-parameter models. That said, we see several factors that may help tame this explosion and prevent things from\ngetting out of hand. In increasing order of importance:\n• Further reduction of raw-compute prices due to increased competition. According to this (admittedly self-\ninterested) blog post [14], the prices on AWS were reduced over 65 times since its launch in 2006, and by as\nmuch as 73% between 2014 and 2017. We expect the same trend for AI-oriented compute offerings.\n• More efﬁcient NN architectures, driven in part by economics and partly by environmental considerations. For\nexample, the Reformer [15] architecture uses heuristics to reduce the complexity of the attention mechanism\nof transformers from quadratic to O(n log n). Similarly, ALBERT [16] achieves better accuracy with fewer\nparameters by factorizing the embedding matrix and weight sharing across layers. We expect to see more of\nthis.\n• Ending the State-of-the-Art (SOTA) race. There is increasing recognition in the community that signiﬁcant\namount of compute is sunk into reaching the top of leaderboards of the many challenge datasets, often in-\nvolving many (in some reported cases, thousands) of runs, just so that one instance will luck into ﬁrst place.\nSuch overﬁtting is of course of little value, and we expect to see less of it.\n• Maxing out on useful data. There is just that much (useful) text that has been written, or that will be. At some\npoint, we will have trained on Borges’ Universal Library.\n• Useful as NNs are, there is a school of thought that holds that statistical ML is necessary but insufﬁcient,\nand will get you just that far. Instead, the thinking goes, you need to incorporate structured knowledge and\nsymbolic methods into the mix, and that in turn depends on brain rather than (only) brawn. This is a view we\nsubscribe to at AI21 Labs (see [17] as an example).\nReferences\n[1]\nGoogle, ResNet-50 training cost comparison, https://cloud.google.com/images/products/tpu/\nmachine-learning-performance_2x.png, Accessed: 2020-04-12, 2020.\n[2]\nV. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled version of BERT: Smaller, faster, cheaper\nand lighter,” in NeurIPS EMC2 Workshop, 2019.\n[3]\nX. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu, TinyBERT: Distilling BERT for natural\nlanguage understanding, 2019. arXiv: 1909.10351 [cs.CL].\n3\nThe Cost of Training NLP Models: A Concise Overview\n[4]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers\nfor language understanding,” in Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-\npers), Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171–4186. DOI:\n10.18653/v1/N19-1423. [Online]. Available: https://www.aclweb.org/anthology/N19-1423.\n[5]\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the\nlimits of transfer learning with a uniﬁed text-to-text transformer,” ArXiv e-prints, 2019. arXiv: 1910.10683.\n[6]\nJ. Etchemendy and F.-F. Li, National research cloud: Ensuring the continuation of american innovation, https:\n//hai.stanford.edu/news/national- research- cloud- ensuring- continuation- american-\ninnovation, Accessed: 2020-04-12, 2020.\n[7]\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D.\nAmodei, Scaling laws for neural language models, 2020. arXiv: 2001.08361 [cs.LG].\n[8]\nJ. Dodge, S. Gururangan, D. Card, R. Schwartz, and N. A. Smith, “Show your work: Improved reporting of\nexperimental results,” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong\nKong, China: Association for Computational Linguistics, Nov. 2019, pp. 2185–2194. DOI: 10.18653/v1/D19-\n1224. [Online]. Available: https://www.aclweb.org/anthology/D19-1224.\n[9]\nZ. Li, E. Wallace, S. Shen, K. Lin, K. Keutzer, D. Klein, and J. E. Gonzalez, Train large, then compress:\nRethinking model size for efﬁcient training and inference of transformers, 2020. arXiv: 2002.11794 [cs.CL].\n[10]\nJ. S. Rosenfeld, A. Rosenfeld, Y. Belinkov, and N. Shavit, “A constructive prediction of the generalization error\nacross scales,” in International Conference on Learning Representations, 2020. [Online]. Available: https:\n//openreview.net/forum?id=ryenvpEKDr.\n[11]\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, Language models are unsupervised multi-\ntask learners, OpenAI’s Blog: https://d4mucfpksywv.cloudfront.net/better-language-models/\nlanguage_models_are_unsupervised_multitask_learners.pdf, 2019.\n[12]\nK. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “ELECTRA: Pre-training text encoders as discriminators\nrather than generators,” in International Conference on Learning Representations, 2020. [Online]. Available:\nhttps://openreview.net/forum?id=r1xMH1BtvB.\n[13]\nSambaNova, A new state of the art in NLP: Beyond gpus, https://sambanova.ai/a-new-state-of-the-\nart-in-nlp-beyond-gpus/, Accessed: 2020-04-12, 2020.\n[14]\nA. Rallo, New research from TSO Logic shows aws costs get lower every year, https://aws.amazon.\ncom/blogs/apn/new-research-from-tso-logic-shows-aws-costs-get-lower-every-year/,\nAccessed: 2020-04-12, 2018.\n[15]\nN. Kitaev, L. Kaiser, and A. Levskaya, “Reformer: The efﬁcient transformer,” in International Conference on\nLearning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=rkgNKkHtvB.\n[16]\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “ALBERT: A lite BERT for self-\nsupervised learning of language representations,” in International Conference on Learning Representations,\n2020. [Online]. Available: https://openreview.net/forum?id=H1eA7AEtvS.\n[17]\nY. Levine, B. Lenz, O. Dagan, D. Padnos, O. Sharir, S. Shalev-Shwartz, A. Shashua, and Y. Shoham, “Sense-\nBERT: Driving some sense into BERT,” in Proceedings of the 2010 Conference of the Association for Compu-\ntational Linguistics (ACL), 2020. [Online]. Available: https://arxiv.org/pdf/1908.05646.pdf.\n[18]\nH. Touvron, A. Vedaldi, M. Douze, and H. Jegou, “Fixing the train-test resolution discrepancy,” in Advances in\nNeural Information Processing Systems 32, Curran Associates, Inc., 2019, pp. 8252–8262. [Online]. Available:\nhttp://papers.nips.cc/paper/9035-fixing-the-train-test-resolution-discrepancy.pdf.\n[19]\nS. Ruder, NLP’s ImageNet moment has arrived, https://thegradient.pub/nlp-imagenet/, Accessed:\n2020-04-12, 2018.\n[20]\nD. Geman, S. Geman, N. Hallonquist, and L. Younes, “Visual Turing test for computer vision systems,” Pro-\nceedings of the National Academy of Sciences, vol. 112, no. 12, pp. 3618–3623, 2015, ISSN: 0027-8424. DOI:\n10.1073/pnas.1422953112. eprint: https://www.pnas.org/content/112/12/3618.full.pdf.\n[Online]. Available: https://www.pnas.org/content/112/12/3618.\n4\nThe Cost of Training NLP Models: A Concise Overview\nA\nNLP versus CV\nWith a few notable exceptions9, you do not see in computer vision (CV) the large numbers and cost escalations you\ndo in NLP, and it is natural to ask why. We enter this discussion with some trepidation. While some of the folks at\nAI21 Labs have experience in CV, it is not our core competence as a company. Furthermore, some of the CV experts\nwith whom we spoke did not have ﬁrm opinions here, and the opinions they did have did not always agree with each\nother. Still, since we have been asked this question we feel we should address it, but please treat the following more\nas a beginning of a discussion rather than deﬁnitive answers.10\nWe believe that there are fundamentally two reasons why training CV models is cheaper than training NLP models:\n• Images versus sentences. Images are smooth and local, in that by and large the value of a pixel depends\nmostly on its close neighborhood and less so on other parts of the image, and furthermore the value does\nnot change drastically from one pixel to its neighbor. Moreover, images are iconic, by which we mean that\n\"what you see is what you get\"; an image of chair and a desk represents a chair and desk. Language is very\ndifferent. Words far apart can be coupled probabilistically, and language is compositional; the way you string\nwords together carries as much meaning as the semantic content of the words themselves.\n• Object recognition versus – what?\nThe canonical problem in computer vision – object recogni-\ntion/classiﬁcation – is, while by no means trivial, relatively simple. It has no direct analog in NLP. One\ncould argue that topic- or sentiment-analysis are somewhat analogous at the document level, and word-sense\ndisambiguation is at the sentence level. But the analogy is weak, and neither of these plays the same central\nrole that object recognition does in vision. Another telling analogy is between object identity in vision (is\nthe person seen in this image the same as the person in this other image?) and noun-phrase co-reference in\nNLP (does “the president” refer to the same entity as “Mr. Trump”?). Here the separation between vision and\nlanguage is stark; object identity is close to being a solved problem, while co-reference is still unsolved. And\nthis is leaving aside the issue that even once solved, co-reference on its own would not bring the same value\nthat object recognition does in CV.\nThese differences manifest themselves in several ways, including these:\n• CNNs versus transformers. CV problems lend themselves to Convolutional Neural Networks (CNNs),\nwhile the canonical NLP approach has centered around transformer models, which are inherently more ex-\npensive than CNNs. The different choice of architecture is directly related to the differences between images\nand sentences; the locality property matches with the local windows of convolutional layers, and smoothness\nwith the sub-sampling operation in pooling layers. Since language does not enjoy these properties, we must\nuse a more general, but less efﬁcient, architecture such as the transformer.\n• Supervised versus semi-supervised versus self-supervised learning. NLP and computer vision employ\nall of these learning regimes, but the balance is different. Unlike in computer vision, most of the training\ntime of NLU models is devoted to self-supervised learning of language itself, and only a small portion is\ndevoted to (supervised) ﬁne-tuning of the model to solve a speciﬁc task. This is related to the inherent\ncomplexity of the structure of language and the nature of NLU and NLG tasks. Much larger datasets are\nneeded in order to provide useful signal, and, just as bad, the tasks are inherently more ambiguous and the\ndata is harder for people to annotate than in image classiﬁcation; it is easier to answer the question “Is that\na person or a car” than “Does this sentence imply that sentence”. Furthermore, data augmentation is much\nmore successful in vision than in NLP, and semi-supervised learning aided by data augmentation has led to\nmany recent SOTA results in vision. In contrast, NLP has been driven toward purely self-supervised learning\n(“the NLP revolution will not be supervised!”). This in turn translates into larger training datasets compared\nto the supervised setting, as well as longer training cycles.\nAgain, important caveats apply to all of the above. Even in object recognition, the larger context of the image can\nmatter when determining what is depicted in a given image patch. Furthermore, object recognition is not the sole focus\nof CV, and more elaborate tasks, such as scene understanding [20], certainly do not have the smooth, local properties\nmentioned (to use a famous example, object recognition techniques do not tell you the interesting part about an image\ndepicting a piano dropping through the air and about to land on someone’s head). As another example, in the area\n9There have been a few attempts to create “mega-models” for CV, e.g., FixResNet [18] has 830M parameters and was trained\non nearly a billion weakly-labeled images. However, the gains are not as great compared to the added costs, and such approaches\nhave not become the norm just yet.\n10See also this article [19] for an interesting discussion circa 2018.\n5\nThe Cost of Training NLP Models: A Concise Overview\nof image synthesis, which often requires accommodating complex logical, real-world constraints in the synthesized\nimage, CNNs give way to inherently more expensive models such as GANs.\nDespite these important caveats, we feel the above analysis is fair, for two reasons. First, it is the case that among CV\ntechnologies, object recognition has brought the most commercial value to date, and CNNs have been the main driver\nbehind its success. And second, more ambitious tasks such as scene understanding are getting close to NLP in being\nless well deﬁned and less well solved. They also call for the same commonsense reasoning as does NLP, and thus are\nlikely require the elaborate techniques – and costs – currently associated with NLP.\n6\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "cs.NE"
  ],
  "published": "2020-04-19",
  "updated": "2020-04-19"
}