{
  "id": "http://arxiv.org/abs/2004.09107v2",
  "title": "A Minimal Model of the Interaction of Social and Individual learning",
  "authors": [
    "Kingsley Cox",
    "Paul Adams"
  ],
  "abstract": "Social, supervised, learning from others might amplify individual, possibly\nunsupervised, learning by individuals, and might underlie the development and\nevolution of culture. We studied a minimal model of the interaction of\nindividual unsupervised and social supervised learning by interacting agents.\nAgents attempted to learn to track a hidden fluctuation \"source\", which,\nlinearly mixed with other masking fluctuations, generated observable input\nvectors. Learning was driven either solely by direct observation of inputs\n(unsupervised, Hebbian) or, in addition, by observation of another agent's\noutput (supervised, Delta rule). To enhance biological realism, the learning\nrules were made slightly connection-inspecific, so that incorrect learning\nsometimes occurs. We found that social interaction can foster both correct and\nincorrect learning. Useful social learning therefore presumably involves\nadditional factors some of which we outline.",
  "text": "1 \nA Minimal Model of the Interaction of Social and Individual learning \n \nKingsley J.A. Cox and Paul R. Adams \n \nDept Neurobiology, Stony Brook University, Stony Brook, NY 11794 \n \nCorrespondence to: K.J.A. Cox   Correspondence and requests for materials should be addressed to \nK.J.A. Cox (email: kcox@syndar.org) \n \nAbstract \nSocial, supervised, learning from others might amplify individual, possibly unsupervised, learning by \nindividuals, and might underlie the development and evolution of culture. We studied a minimal model \nof the interaction of individual unsupervised and social supervised learning by interacting agents. Agents \nattempted to learn to track a hidden fluctuating “source”, which, linearly mixed with other masking \nfluctuations, generated observable input vectors. Learning was driven either solely by direct observation \nof inputs (unsupervised, Hebbian) or, in addition, by observation of another agent’s output (supervised, \nDelta rule). To enhance biological realism, the learning rules were made slightly connection-inspecific, so \nthat incorrect learning sometimes occurs. We found that social interaction can foster both correct and \nincorrect learning. Useful social learning therefore presumably involves additional factors some of which \nwe outline. \n \nKeywords \nNeural networks; supervised learning; unsupervised learning \nmulti-agent learning; Independent Components Analysis; Social learning \n \nINTRODUCTION \n \nAn important aspect of intelligence is the ability to detect regularities in complex, seemingly \nunpatterned, data. Brains can achieve this via learning, largely by activity-dependent adjustment of the \nrelative strengths of synaptic connections between pairs of neurons. Since postsynaptic activity is \ntypically caused by presynaptic activity, a learning rule operating at individual synapses that depends on \nboth pre- and postsynaptic activity, in a broadly Hebbian manner, can detect and represent statistical \nregularities over an entire set of inputs, since it’s driven by input correlations, both second and higher \norder. Learning can sometimes also be facilitated by supervisory signals, especially when a critic or \nteacher already has access to hidden structure.  \n \n\"There are 2 contrasting but not incompatible ideas about individual human intelligence. Firstly, humans \nmight have special brain circuits that directly underpin components of intelligence (Fodor, 1983; Pinker, \n2010; Preuss et al., 1999). Second, humans may have enhanced ability to learn, either individually or \nfrom others. In particular, using symbolic communication, especially language, humans may learn about \nthe world using guidance from other humans (Tomasello 2009), in addition to direct observation. Part \nof the distinctive power of human cognition, compared to other animals, might derive from the way that \nindividual and collective learning are coupled by symbolic communication so that individual insight can \nspread socially, and even evolve (Richerson and Boyd, 2008) (Boyd et al., 2011) (Galef and Laland, \n2005). Here we develop a minimal quantitative model of the leveraging between individual learning and \nsocial learning. Previous models of multiagent learning have been largely based on supervised learning \n2 \n(Sen and Weiss, 1999) (Denaro and Parisi, 1997). However, our model postulates unsupervised \nindividual learning combined with supervised social learning from others.  \n \nOur model is based on the mathematically-transparent Independent Components Analysis (ICA) \nparadigm (Hyvärinen et al., 2004). In this model, input regularities are generated in the simplest possible \nway, by linear mixing of unstructured “causes” or “sources” (independently fluctuating numbers), at \nleast one of which has a nonGaussian distribution, to generate observed “effects” or “observations”. In \nthis situation even a single neuron can infer a cause from observed effects, merely by adjusting its \nsynaptic weights to parallel a column of the mixing matrix (row of the inverse), using Hebbian learning. \nThis model has been described as ‘the simplest, cleanest, and thus most robust learning scenario \nimaginable’ ((Elliott, 2012)). \n.  \nProvided that at least one of the sources has a nonGaussian distribution, these input vectors can exhibit \nboth second-order and higher-order correlations. In our model each agent is a single “neuron” which \nreceives the same input vectors as every other agent, and is attempting to learn weights that permit it \nto track the only source which is nonGaussian, by responding to the higher-order input correlations \ninduced by mixing. If the mixing matrix is orthogonal, only higher than second order input correlations \nare generated, and a nonlinear Hebbian rule can always learn the appropriate unmixing weights \n(Hyvärinen and Oja, 1998). In this situation cooperative learning, via agent interaction, is not needed, \nsince individual agents can always successfully learn. However biologically even this elementary task \ncannot always be achieved, because (1) residual misleading second-order statistics may be present \n(mixing is non-orthogonal, e.g. because of inadequate preprocessing) and (2) the learning rule might not \nbe completely synapse-specific. We and others have shown that under these conditions spurious or \nirrelevant solutions can be learned, especially if the starting weights are unfavorable. If weights are \ninitialized randomly, only those agents that happen to start close to the correct solution may succeed \n(Cox and Adams, 2014; Elliott, 2012). However, if agents could learn from each other, in a supervised \nmanner, as well as directly from the input data, then successful learning by one agent might seed \nlearning by other agents. One could refer to these 2 learning styles as “discovery” (private individual \nlearning directly from observations) and “education” (public learning by observation of discoveries). A \nkey to human cognition might be the harnessing of individual discovery to collective learning (Boyd et \nal., 2011) (Adams and Cox, 2012). While local minima/plateau phenomena are particularly transparent \nin the ICA model ((Rattray, 2002)) (Elliott, 2012), they can hinder most other more sophisticated \nunsupervised learning models. Our results suggest that communication is unlikely to be a panacea for \nsuboptimal learning: populations can get trapped in plausible but parasitic myths. We briefly discuss \npossible ways to ameliorate this Achilles heel of collective learning. \n \n \n \n \n \n \n \n \n \n \n \n \n \n3 \n \n \nMETHODS \n \nUnsupervised learning with ICA \nThe problem in ICA is to unmix a set of independent unit-variance sources that have been mixed by a \nmixing matrix \n \nx = Ms \n \nwhere x is the observable vector of mixed inputs, s is a vector of hidden independent sources, and M is \nthe mixing matrix. In all our simulations M was orthogonal and there were only two sources, one with a \nGaussian distribution and the other Laplacian. This ensures that, in the absence of crosstalk,  there is \nonly one learnable solution, a synaptic weight vector pointing in the same direction as the column of the \nmixing matrix associated with the nonGaussian source (Rattray, 2002). For simplicity we only consider \none output neuron which tries to find a weight vector that will ‘unmix’ the sources so that the output \nneuron tracks one of them (see figure 1a). \n \n  \n \n \nFigure 1  Panel a   A source vector s is mixed by an orthogonal mixing matrix M generating an input vector x. An unmixing \nvector w is sought that will produce an output y that will track the left hand, nonGaussian source.  \nPanel b   Single agent basins of attraction for the PC and the IC. This shows the fixed points of the learning rule as a function of \nthe crosstalk parameter e (ordinate) and the angle  For e/ pairs that lie in the green area, there is only one stable fixed point, \nthe IC, and in the red area only the PC. In the yellow area there are two fixed points, the IC and the PC. In the grey area there is \na smooth change (a mixture) from the IC to the PC as e increases. The dotted vertical line marks the critical angle  below which \nthere is no possibility of a crosstalk threshold, which in our simulations is 69° (for the cubic nonlinearity). The pink line \nrepresents an example of a randomly chosen example of M where  is between 69° and 90°. As the dimension of the network \ngets  will likely become closer and closer to 90° (see text). \n \n \n \nWe use the one unit negentropy-maximizing rule (Hyvärinen and Oja, 1998) \n \nΔw   k σ x f(wT x), normalize w \n4 \n \nwhere k is a learning rate, w is the weight vector, y is the output, and σ is either +1 or -1 depending on \nthe nonlinearity and the nonGaussian source statistics (Cichocki et al., 1997). In our case, with n=2, \nnormalization after each iteration means that the ‘basin’ of attraction lies on the unit circle. \n \nThus the output neuron sums the inputs weighted by the current weight vector w and then passes this \ntotal ‘activation’ through a nonlinearity f(y) (in our case either (y)3 or tanh(y)). The current weight vector \nis then increased by an amount proportional to f(y) and finally the updated weight vector is normalized \nso that no individual weight can become too large. M is constrained to be orthogonal so that there are \nno pairwise correlations introduced into the input vectors x (i.e. the source vectors are only rotated). \nEven if the original mixing is nonorthogonal this condition can be achieved by suitable preprocessing. \nThe weight vector that enables y to track the nonGaussian source is called the independent component \n(IC). \n \nWe only used 2 sources, however the single Gaussian source can be replaced by multiple Gaussian \nsources with the same outcome (Elliott, 2012). \n \n \n \nErrors \nInspecificity in the update rule is introduced by modifying the update in w by using an error matrix E as \ndescribed in (Cox and Adams, 2009; Cox and Adams, 2014; Radulescu et al., 2009) \n \n       \n \n \n     \n \n \nwhere e is the, typically very low, crosstalk level. \n \nThe mixing matrix \nThe mixing matrix is orthogonal and is varied by changing the angle θ that the first column of M makes \nwith the vector [1,0]T.  \n \n        \n     \n    \n      \n \n \n \nPrincipal component of E \nWith crosstalk, an eigenvector of E becomes a possible fixed point of the dynamics in addition to the IC. \nWith non-zero e, the eigenvectors of E are the vectors [0.707, 0.707] and [0.707, -0.707] regardless of \nthe amount of error (since E is a symmetric matrix). When using a tanh nonlinearity  is-1, and with \ncrosstalk the PC associated with the least eigenvalue is the stable fixed point of the dynamics [0.707, -\n0.707] and we call this the ‘PC’. With the cubic nonlinearity is +1 and the PC associated with the \nlargest eigenvalue is the stable fixed point i.e. [0.707, 0.707]. \nThe angle between the first column of M and the PC is denoted  and  is varied by varying θ with the \nmaximum angle, 90°, being achieved when θ is 45°. The angle between the PC and x-axis is fixed at 135° \n5 \nand so the angle between the PC and the IC is (135°- θ). For example to get an angle of 90° between the \nPC and the IC, θ is 45°. θ is measured counterclockwise and as θ increases  decreases. \n \n \n \nSupervised learning \nWe define an agent as a single neuron feed-forward neural network (as in figure 1a) that is trying to \nsolve the ICA problem, i.e. tracking the nonGaussian source of the input data.  It can do this either \ndirectly from observing input fluctuations and using unsupervised Hebbian learning or by supervised \nlearning, i.e. by also observing the current output of another agent that may have already learned the \ncorrect solution, in addition to directly the observing input vectors. \nFor supervised learning, agents use the Delta rule (Widrow and Hoff, 1960). The difference between the \nagent’s output y is compared against a teacher’s output yT (in this case another agent’s estimate of the \ncurrent value of the nonGaussian source). In this case the output y is the dot product of the input vector \nand the weight vector and has not been put though a nonlinearity \n \nΔw =  k x(y - yT ) \n \nwhere Δw is the update in the weight and k is a learning rate. Crosstalk slows learning for the supervised \nagent but does not change the direction of its weight vector (Figure 3b and 3d). \n \n \nTo understand the network when using a cubic nonlinearity we refer to the phase diagram in figure 2b \nshowing the possible outcomes of learning for different values of the mixing angle  and crosstalk \nparameter e, which is described in detail in (Cox and Adams, 2014; Elliott, 2012) \nWith crosstalk (and/or for nonorthogonal mixing; see (Cox and Adams, 2014)), in a critical range of , 2 \nfixed points exist, one corresponding to an approximate IC, the other to an approximate PC (a ‘false \nminimum’). Depending on the values of  and e, these can be either stable or unstable. In the green \nzone, only the IC fixed point is stable, in the red zone only the PC fixed point is stable, and in the yellow \nzone both are stable. In the gray zone there is only one, approximate IC, fixed point which is a smooth \nblend of the IC and PC. \nThe pink line represents a particular example mixing matrix and the closer the pink line is to the y axis \nthe further apart are the IC and the PC (in this study measured by the angle , with the maximum \ndistance being 90°). The greater the dimensionality the greater the chance that a randomly chosen \nmatrix (the pink line) is close to  = /2. \n \nIn the yellow parameter region there are basins of attraction for both the PC and the IC (see figure 3) \nand the relative sizes of the basins change with error (i.e. as one moves up the y axis).  Qualitatively \nsimilar behaviour is seen using the tanh nonlinearity ((Cox and Adams, 2014) and Results). \n \nTo determine the basins of attraction for a particular M the weights were started from a particular point \non the unit circle and allowed to converge (to either the PC or the IC). This was done for many points on \nthe circle for different error rates (see Results). \n \nInteracting agents \nWe studied situations in which 2 or 4 potentially interacting agents can switch their learning strategies \nbetween unsupervised and supervised, while all observing the same input vectors, after a variable \nnumber of iterations which defines a Duty Cycle. Each run breaks down the simulation into sections \n6 \nwhere the agents simultaneously change their learning strategy randomly. A simulation typically runs for \n200,000 epochs and this will be split into for example 10 groups of 20,000 epochs each of which has a \nrandomly assigned learning type (U, S1, S2, S3 each with probability of ¼, see below). And so in this case \nthe ‘Duty Cycle’ would be 20,000 iterations. In other runs the duty was just 1. \n \n2 agents \nWith 2 agents there are 4 possible learning setups.  \n \nType 1 (U) \nBoth Agents learn on their own, i.e. unsupervised, each agent using the ICA one-unit rule algorithm to \nfind the IC. \n \nType 2 (S1) \nAgent 1 learns from agent 2 and agent 2 learns unsupervised. Here agent 2 does ICA but agent 1 is being \ntaught by agent 2 using the Delta rule. \n \nType 3 (S2) \nAgent 2 learns from agent 1 and agent 1 learns unsupervised. Here agent 1 does ICA but agent 2 is being \ntaught by agent 1 using the Delta rule. \n \nType 4 (S3) \nAgents 1 and 2 learn from each other using the Delta rule. In this case learning becomes ineffective. \n \n4 agents \nThe Duty Cycle was 1 and at each iteration each agent chose randomly between supervised and \nunsupervised learning: the supervised learning used the Delta Rule as before but this time an average of \nthe outputs of the other agents was used instead of the output of a randomly chosen individual agent. \nThis \"meanfield\" simplification gave similar results to runs under otherwise identical conditions where \nan agent learned from one randomly chosen other agent. \n \n \n \n7 \n \nRESULTS \n \nWe present our results as follows: \nIn Section 1 we explore the basins of attraction for this ICA model as crosstalk is varied since these data \nwill form the PC/IC ‘landscape’ upon which agents evolve as they are exposed to inputs. In Section 2 we \nlook at how two agents evolve without crosstalk (both non-interacting and interacting), and then how \nagents evolve on the landscape (elucidated in Section 1) with crosstalk, again both non-interacting and \ninteracting, and compare the two situations. In Section 3 we extend the model to four agents. \n \nSection 1 \n1.1  One agent basins of attraction  \n \nIf independently fluctuating signals are linearly combined to form input vectors, a single nonlinear \nneuron-like unit equipped with Hebbian synapses can always learn weights that unmix the inputs so as \nto track a hidden source signal, provided the inputs are uncorrelated at second order, and the learning \nrule is connection-specific and correctly signed (Hyvärinen and Oja, 1998). However if the rule isn’t \ncompletely synapse-specific or the inputs are not perfectly decorrelated, or both, instead of learning \ncorrect unmixing weights, the unit can learn can learn weights that do not unmix, and correspond to the \nprincipal eigenvector of EC (the product of matrices describing the pattern of crosstalk or correlations), \nespecially if the weights start close to this incorrect solution (Elliott, 2012); see Methods. In order to test \nwhether communication between 1-neuron agents could overcome this type of difficulty, we first \ncharacterized the behaviour of individual non-interacting agents. \n In the case of the cubic nonlinearity, an elegant mathematical analysis has delineated the fixed-point \nstructure of learning (Elliott, 2012). As expected from this analysis, we found that depending on the \ninitial weights either the correct (“IC”) or incorrect (“PC”) solution was learned. Although a tanh \nnonlinearity is considered to be more robust (Hyvärinen et al., 2004), no analysis of its behaviour is \navailable, so we then explored its basins of attraction in more detail.  \n \n \n1.2 Cubic nonlinearity \nAn initial assessment of the IC and PC basins using the cubic nonlinearity showed that, for  = 90°, a \ncrosstalk level of 0.165 divided the PC and IC basins into equal halves (data not shown) so that initial \nweight vectors between [-1/√2,1/√2] and [0,1] would converge to the IC (represented by angles 135° to \n90°) and initial weight vectors between [1/√2,1/√2] and [0,1] (represented by angles 45° to 90°) would \nconverge to the PC. \n \n \n1.3 tanh nonlinearity \nSince analytical results are not available for the tanh nonlinearity, we explored the 1-agent basin of \nattraction in more detail. Due to symmetry, we only need to look at the fate of the initial weight vectors \nrepresented by angles between 45° (the PC) and 135° (the IC) – see the legend of Table 1. The crosstalk \nwas varied from 0 to 0.16. \n \nWe looked at three different mixing matrices where the angle between the first column of M and the \n“PC” was 78°, 83° and 90°. The PC is [-0.707, 0.707] so for 78° the first column of M is [0.54, 0.84], for \n83° [0.62, 0.78] and 90° [0.707, 0.707]. For these angles 2 fixed points coexisted (yellow zone in figure \n8 \n1b) and the fixed point that is actually learned depends on the initial weight vector and the amount of \ncrosstalk. In this section we characterize this dependence, by systematically varying the initial weights. \n \nFor an angle of 78° between the PC and IC as error increased above the lower threshold  the PC basin \nincreased from 0 to 30%, expanding rapidly to 100 % on reaching the upper threshold, and the IC basin \nshrank from 100% to 70%. For an angle of 83° the range of the PC was from 0 – 40% and the IC shrank \nfrom 100 to 60%. For an angle of 90° between the IC and PC the PC basin expanded to nearly 100% in a \nsmooth manner before the upper threshold was reached (0.16) (see figure 3 and table 1). \n \n \ndegrees \n15pi/20 \n(135⁰) \n14pi/20 \n(127⁰) \n13pi/20 \n(118⁰) \n12pi/20 \n(108⁰) \n11pi/20 \n(99⁰) \n10pi/20 \n(90⁰) \n9pi/20 \n(81⁰) \n8pi/20 \n(72⁰) \n7pi/20 \n(63⁰) \n6pi/20 \n(54⁰) \n5pi/20 \n(45⁰) \nerror \n \n \n \n \n \n \n \n \n \n \n \n \n0.0 \n \nic \nic \nic \nic \nic \nic \nic \nic \nic \nic \nIc \n0.01 \n \npc \nic \nic \nic \nic \nic \nic \nic \nic \nic \nIc \n0.02 \n \nPc \nPc \nic \nic \nic \nic \nic \nic \nic \nic \nIc \n0.03 \n \nPc \nPc \nic \nic \nic \nic \nic \nic \nic \nic \nIc \n0.04 \n \nPc \nPc \nPc \nic \nic \nic \nic \nic \nic \nic \nIc \n0.05 \n \nPc \nPc \nPc \nic \nic \nic \nic \nic \nic \nic \nIc \n0.06 \n \nPc \nPc \nPc \nPc \nic \nic \nic \nic \nic \nic \nIc \n0.07 \n \nPc \npc \nPc \nPc \nic \nic \nic \nic \nic \nic \nIc \n0.08 \n \nPc \nPc \nPc \nPc \nPc \nic \nic \nic \nic \nic \nIc \n0.09 \n \nPc \nPc \npc \npc \nPc \nic \nic \nic \nic \nic \nIc \n0.10 \n \nPc \npc \npc \nPc \nPc \nic \nic \nic \nic \nic \nIc \n0.11 \n \npc \npc \npc \npc \npc \npc \nIc \nIc \nIc \nIc \nIc \n0.12 \n \npc \npc \npc \npc \npc \npc \nIc \nIc \nIc \nIc \nIc \n0.13 \n \npc \npc \npc \npc \npc \npc \npc \npc \nIc \nIc \nIc \n0.14 \n \npc \npc \npc \npc \npc \npc \npc \npc \npc \npc \nIc \n0.15 \n \npc \npc \npc \npc \npc \npc \npc \npc \npc \npc \nIc \n0.16 \n \npc \npc \npc \npc \npc \npc \npc \npc \npc \npc \npc \n \nTable 1  Basins of attraction with an angle of 90°between the PC and the IC, using tanh nonlinearity. The angles (between the \nstarting weight vectors and the PC of E top row) span a sector of a circle from 45° to 135° (anticlockwise from the x-axis). The \npoint at 135° is the PC and at 45° the IC. The table shows where the network converges to when initialized to a starting vector \nwhich is represented by the angle. For example when the start vector is at [-0.47, 0.88] which is an angle of 118°, and the error \nis 0.04, the weight vector converges to the PC (see red disc in figure 3)  but when the error is 0.02 it converge to the IC (blue \ndisc in figure 3).  \n9 \n \n \n \n \n \n \nFigure 2   Basins of attraction for different initial weight vectors starting at various angles between the IC and the \nPC, using a tanh nonlinearity. The red disk corresponds to the direction of the PC vector, and the blue disk to 3 \ndifferent  directions of the IC vector relative to the fixed PC vector (alpha = 78 degrees in panel a; 84 in b; 90 in c; \nnote that as alpha decreases from c through a, the distance between the red and blue disks decreases \ncorrespondingly). The arcs represent various weight vector starting directions. The pink arcs show the directions of \nstarting weights that converge to the PC for various crosstalk levels, the shading corresponding to different levels. \nThe blue arcs show the IC basin at zero crosstalk. Where the pink and blue arcs overlap, PC and IC basins co-\nexist.  If the crosstalk level is below the indicated pink shading level, the corresponding starting weights converge \nto the IC, at or above that level, they converge to the PC. In a and b the upper crosstalk thresholds (above which \nonly the PC is stable) are 0.095 and 0.09 respectively and so at the indicated crosstalk levels the PC basin did not \ninvade the IC basin very far. In panel c, on the other hand, the PC basin expanded in a smooth way as crosstalk \nincreased all the way up to the upper error threshold, above which the IC is never stable; see also Table 1. \n \n \nSince the 90° angle case showed a smooth increase in the PC basin all the way up to the IC as error \nincreased we decided to use this matrix to explore multi-agent learning, in the next section. \n \nSection 2 \nInteracting agents \n \n2.1 \n ‘teacher’ and ‘student’ \nAn initial natural situation in when there is a ‘teacher’ who is at the solution and a ‘student’ who is not. \nUnder these circumstances we find that the student learns ten times faster when learning from the \nteacher than by learning on its own, i.e. unsupervised (figure 3a and 3b). Further, if there is some \ncrosstalk and the student is in the PC basin of attraction, then the student will never learn the IC but \nwhen learning from the teacher it will get to the IC, albeit more slowly (figure 3d).  \n10 \n \n \n \nFigure 3   Teacher and student. Panel a shows the rapid progress of the student agent as it learns (zero crosstalk) using the \nDelta rule (with the target output being the teacher agent’s output) – the inset graph is a blow-up of the first 5000 interations \n(rectangle). Panel b shows a comparison of the speed of convergence to the IC of the student agent learning (zero crosstalk) \nunsupervised (red trace) and supervised by the teacher (blue trace – the same one as in panel a). Panel c shows the fluctuations \nof the student (after reaching the IC) during both supervised (left) and unsupervised (right) learning (zero crosstalk). Panel d \nshows how the trajectories of the student agent differ when there is crosstalk and the student agent starts from the PC basin – \nthe red trace is unsupervised and the blue trace is supervised by the teacher. \n \n \n2.2  No crosstalk, 2 agents \n \nThe above scenario, in which where there is an agent (teacher) that only does unsupervised learning, \nteaching another agent (student) that only does supervised learning, appears to show that interaction \nbetween agents can help with an increase in convergence speed (figure 3b), and, even more striking, \nallowing the student agent to get to the IC when learning unsupervised would simply mean staying in \nthe PC basin (figure 3d).  \n11 \nHowever, more realistically, teachers themselves must learn, and may not have found correct solutions \n(i.e. could equally well be at the PC), and in the following results we take an even-handed approach in \nwhich there is no privileged agent in that they all learn in the same way. \n \n \nWe begin with the simplest case, orthogonal mixing and no crosstalk, with only one fixed point, the IC. \nSeveral runs were done for both non-interacting and interacting agents to see if interaction had any \neffect on the speed of convergence for the agents. We found that the average time to convergence for \nthe agents did not differ substantially between non-interacting and interacting agents. This was true for \nboth cubic and tanh nonlinearities. A typical set of runs is shown in figure 4. Notice how the interacting \nagents synchronise their learning after about 7,000 iterations. The duty time in these runs was 20,000, \nbut similar results were obtained for different duty times. In these runs learning types U, S1 and S2 were \nused (but not S3) with probability 1/3 each. \n \n \n... \n  \n \n \nFigure 4  Non-interacting and interacting agents with zero crosstalk (tanh nonlinearity). The top 2 panels show the two non-\ninteracting agents; the cosine of the angle the weight vectors of agent 1 and agent 2 make with the IC are shown. The weight \nvectors start from random values and thus converge to the IC at different times. The bottom 2 panels are interacting agents \nwhich again both also start from random weight values. Note that the interacting agents synchonise after around 10,000 \ninterations. \n12 \n \n2.2 Two Interacting agents with crosstalk \n \nWith crosstalk (but still orthogonal mixing) there are now 2 fixed points for each agent to converge to, \nthe PC and the IC. When each agent is learning unsupervised then it will converge to the fixed point of \nthe basin in which it initially started. However, when the agents are interacting there are further \npossibilities: both could end up at the IC, both at the PC, or one at the PC and one at the IC. Factors that \nare likely to affect the outcome are the length of the Duty Time and the initial starting points of the \nweight vectors. The following results explore these possibilities. \n \n2.2.1 Cubic  \nInitially we studied interacting agents using a crosstalk level of 0.165 and initial weight vectors that put \nagent 1 in the IC basin and agent 2 in the PC (i.e. either side of 90°). A typical run is shown in figure 5. In \nfigure 5 the 200,000 epoch run was broken up into ten bouts of 20,000 iterationss according to the \nlearning types U S1 U U S3 S2 S2 S2 S1 U. In figure 5 agent 1 eventually pulls agent 2 into the IC basin \nafter three consecutive phases of S2 after initially being pulled towards the PC by agent 2 during the \nearlier S1 phase. When both agents are simultaneously learning from each other (S3 starting at 80,000 \nepochs) then the components of the weight vector of agent 1 will move towards the weights of agent 2 \n(and vice versa) until they are the same. \n \n \n \n \n \n \n \n \nFigure 5  2 interacting; cubic nonlinearity. The top left and right plots are of the cosine of agent 1 and agent 2 weight vectors \nwith the IC (blue) and PC (red). The lower two plots are of the weight vectors for agent 1 (left) and agent 2. Learning types \nswitch after every 20,000 epochs according to U S1 U U S3 S2 S2 S2 S1 U. \n \nCos angle between weight \n vector and IC direction  \n \nagent 1 \nagent 2 \nepochs \nepochs \nWeight s \n13 \n \n \nWe then examined agent interaction using the tanh nonlinearity since it is a more robust learning rule \n(Hyvärinen et al., 2004) and also because we wanted to explore the nature of the basins using a \ndifferent nonlinearity. However no mathematical analysis of the tanh case is available. \n \n2.3 Tanh nonlinearity \nFor all the experiments in this section an angle of 90° was chosen between the PC and the IC. For figures \n6 and 7 runs were 200,000 epochs split into sections of 20,000 iterations with random reassignment \nbetween the 4 possible learning scenarios. The error rate was fixed at 0.05.  \n \nIn the example below (figure 6) the initial weight vectors for both the agents were close to [0,1] which is \non the 50:50 basin threshold. Both agents finally converged on the PC in this run. \n \n \n \n14 \n \n \n \n \n \n \n \n \nFigure 6 \n200,000 epoch run with agent 1 and agent 2 interacting. The vertical lines show the periods (each of 20,000 epochs) that the \nagents are in each learning mode.  Agent 1 is the top graph and agent 2 the bottom. The error rate was 0.05 and agent 1 started \nin the IC basin and agent 2 started in the PC basin. The blue plots are the angles of the weight vector with the IC and the red \nplots the angle with the PC. Here the agent in the PC basin drags the agent starting in the IC basin into the PC solution. \n \n \n \n \n \nU \nU \nS1 \nS2 \nS3 \nS3 \nS1 \nU \nS3 \nS3 \nepochs \nCos angle between agent 1 weight \n vector and IC direction \nCos angle between agent 2 \nweight vector and IC direction \n(blue) \nU \nU \nS1 \nS2 \nS3 \nS3 \nS1 \nU \nS3 \nS3 \nExample of both agents converging on the PC \n15 \nIn figure 6 the 200,000 epoch run was broken up into ten bouts of 20,000 epochs according to the \nlearning types U U S1 S2 S3 S3 S1 S3 S3 U. \nThe initial weights were [0.0878, 0.9961] (about 85°) for agent 1 and [-0.2825, 0.9593] (about 106°) for \nagent 2 which, for the error rate of 0.05, puts agent 1 in the IC basin and agent 2 in the PC basin (see \nTable 1). Two bouts of unsupervised learning allowed each agent to progress towards their respective \nsolutions (agent 1 the IC and agent 2 the PC). Then at 40,000 epochs each agent switched to S1 learning \nwhere agent 1 learns from agent 2 (which has already made good progress in learning the PC) resulting \nin agent 1 rapidly changing to learning the PC. After 20,000 epochs of S1 learning both agents were in \nthe PC basin and the type of learning that followed did not affect the eventual fate of each agent, \nnamely converging on the PC solution. As expected when both agents are learning from each other (i.e. \nS3), the components of the weight vectors move towards each other - if the components of the weight \nvectors are agent 1 and agent 2 are already (due to previous learning) close to each other then very little \nchange is seen.  \n \nIn the next example (figure 7) the 200,000 epoch run was broken up into ten bouts of 20,000 epochs \naccording to the learning types U S2 S2 S1 S2 S1 S3 S2 U U. \n \n \n \n \n \n16 \n \n \n \n \n \n \n \nFigure 7 \n200,000 epoch run with agent1 and agent 2 interacting. The vertical lines show the periods (each of 20,000 epochs) that the \nagents are in each learning mode.  Agent 1 is the top graph and agent 2 the bottom. The error rate was 0.05 and agent 1 started \nin the IC basin and Agent 2 started in the PC basin. The blue plots are the angles of the weight vector with the IC \n \n \n \n \nepochs \nCos angle between agent  1 weight \nvector and IC direction \nCos angle between agent 2 weight \nvector and IC direction \nU \nS2 \nS2 \nS1 \nS2 \nS1 \nS3 \nU \nS2 \nU \nU \nS2 \nS2 \nS1 \nS2 \nS1 \nS3 \nU \nS2 \nU \nExample of both Agents converging on the IC \n17 \nThe initial starting vectors were agent 1 [-0.0382    0.9993] and agent 2 [0.4078    0.9131]  which from \nTable 1 shows that agent 1 started in the IC basin (about 92°) and agent 2 started in the PC (about 113 °) \nbasin. \nIn the run shown in figure 7 agent 1 begins in the IC basin and agent 2 is in the PC basin. For the first \n20,000 epochs both agents learn unsupervised resulting in agent 1 moving closer towards the IC and \nagent 2 moving closer towards the PC. The second batch of 20,000 epochs has each agent learning via S2 \nlearning which results in agent 2 being dragged out of the PC basin and into the IC basin. As soon as both \nAgents are in the IC basin they stay there and eventually both converge to the IC solution.   \n \nFor a certain crosstalk level what an agent will learn depends on the initial weights of the network, i.e. \nwhether it is in the basin of attraction of the PC or the IC. If one agent starts in the PC basin and the \nother in the IC basin then depending on the particular sequence of learning cycles (supervised or \nunsupervised) then each agent can pull the other agent into its basin of attraction. We looked at the \ncase where the IC was as far away as possible from the PC (90°) which we show allows a crosstalk level \n(0.15) that splits the basins 50:50 so that there is an equal chance for an agent to learn the IC or PC \n(since it has an equal chance of starting in the PC or IC basin). If the agents did not interact then each \nagent would half of the time find the IC and half the time the PC.  \n \n \nAgents starting from PC and IC, duty time = 1 \n \nWe then studied interacting agents from the extreme viewpoint of starting the interacting agents at the \nPC or IC (one exactly at the PC and the other exactly at the IC) and then studying their trajectories at \nvarious crosstalk levels. The duty cycle was set at 1, i.e. the extreme condition where the agent changes \nlearning type randomly at each iteration. Data was collected over many hundreds of runs for two \ndifferent learning rates, 0.0002 and 0.0001 – see figure 8. \nFor every run, both agents always ended up in either the PC basin or the IC basin, depending on the \ncrosstalk level. As expected at low crosstalk, both converge to the IC, and at high crosstalk both \nconverge to the PC, while for intermediate crosstalk, both can converge to either the IC or the PC, \nthough this intermediate range narrows as the learning rate decreases. Presumably with extremely slow \nlearning, there is an abrupt transition from both-IC to both-PC.   \n \n  \n \nFigure 8   Fate of interacting agents as crosstalk is varied. One agent was started at the IC and the other agent at \nthe PC. Each run was for 200,000 iterations and at each iteration both agents changed learning type. The left hand \ngraph has a learning rate of 0.0001 and the right hand graph 0.0002. The plots show the percent of time both \nagents went to the IC or PC at different crosstalk levels. \n \n18 \nAs the learning rate was reduced the fate of the agents became more binary in that for a particular \ncrosstalk level they would both go to either the PC or the IC. For instance take crosstalk at 0.065: for a \nlearning rate of 0.0002 10% of the time both agents would go to the PC but at a learning rate of 0.0001 \nthey both always went to the IC.  \n \nSection 3 \n \n3.1   4 interacting agents, zero crosstalk \nWith 4 agents a modified program was used in which when an agent was doing supervised learning, it \nwould use the average of the output of the other agents in the delta rule (since otherwise the number of \nlearning types becomes very large). Crosstalk was zero (so the only fixed point was the IC) and agents \nstarting with random weight vectors and the average time it took for the agents to converge compared. \nThe Duty Time of 1 as in the previous section.. From our limited number (40 unsupervised and 100 \nsupervised) of runs we found that the average time to convergence was 1.5 times slower for interacting \nagents. As with the two agent case, all four agents at some point synchronized and travelled as a ‘group’ \nto the IC. \n \n \n \nFigure 9   A typical example of 4 interacting agents without crosstalk. The above plots are of each agent as it converges to the \nIC with the y-axis the cosine of the angle of the weight vector with the IC.  Each agent started from random weights and after \neach iteration changed randomly to either unsupervised learning or supervised learning. The number of iterations was 50,000. \nNote how after only about 2,000 iterations all 4 agents began to synchronise their learning. \n \n19 \n3.2   4 interacting agents with crosstalk \n \nIncreasing crosstalk from zero changes the sizes of the PC and IC basins so we looked at the case with \ncrosstalk = 0.05 to see if different basin sizes would alter the convergence times of U and S. Under these \ncircumstances in the non-interacting learning mode the agents will simply converge to bottom of the \nbasin they start in. In the interacting learning mode the situation is more complicated since where all \nthe agents end up depends on the position of each agent’s weight vector in the basin it starts in as well \nas the number of agents in each basin at the start of the run. We found that, as in the 2 agent case, \nwhen interacting all 4 agents either went to the IC or PC, with no mixed outcomes. Synchronisation of \nthe agents also occurred (at around 1,000 iterations in Figure 10). \n \n \n \nFigure 10   \nA typical example of 4 agents interacting with crosstalk (0.05). In this case all the agents went to the IC (blue line) and began \nmoving as a group at around 5,000 iterations. \n \n \n \n \n \n \n \n \n20 \n \nDISCUSSION \n \nHow animals, especially those living in social groups, learn about the world is of relevance to \nunderstanding how Homo sapiens and culture have evolved. Survival strategies can be coded into the \ngenome or learned through experience, and learning can be done independently or by instruction.  The \nmachinery, such as symbolic communication, selective attention, imitation, cooperation, trust etc is of \ngreat interest, but we bypass these issues by assuming perfect attention and communication. \n \nInformation is probably stored in brains by synaptic strength changes (Bartol et al., 2015), but it seems \nunlikely that social learning is accomplished by direct copying of synaptic weights, not least because an \nindividual does not have access to her own weights. It seems more likely that social learning employs \nessentially the same mechanisms as individual learning, in particular close observation of the physical \nenvironment, but with an additional, supervisory, component (Bengio, 2012). In our model, using the \ndelta rule, this is explicit: supervised learning is driven by output, input and an explicit (though possibly \nincorrect) target. Learning is often slow, difficult or unsuccessful, or incorrect solutions may be learned, \neven when unlimited amounts of relevant data are available. \n \nIn the ICA model a ‘hard’ problem can be thought of one in which the IC basin of attraction is small and \nso without crosstalk (or second order correlations) all problems are ‘easy’ since only the IC basin exists. \nWith crosstalk and/or color a competing PC basin appears, and if this basin increases in size (the IC basin \ndecreases in size), the probability of starting near the IC may be lowered. Deep learning can be slowed \nby the existence of plateaus and/or false minima in weight space (Atakulreka and Sutivong, 2007; \nBengio, 2009; Erhan et al., 2010; LeCun et al., 2015; Saxe et al., 2013; Wessels and Barnard, 1992) and in \nthis related sense the problem is hard. One important objective of deep learning is to get into a \nfavourable part of weight space where gradient descent can be effective. For our model utilizing a lucky \nagent that is in a ‘good’ part of weight space (IC basin) to teach other agents in ‘bad’ parts (PC basin) \nmakes the problem easy for the latter agents. \n \nIn our model both individual and social learning are driven by observation of the mixed input vectors, \nbut individual learning is unsupervised, with no access to the underlying source values, while social \nlearning is also driven by access to, possibly incorrect, estimates of underlying source values. In a \npopulation of interacting agents, some individual learners will learn veridical solutions (e.g. IC) while \nothers will learn misleading (e.g. PC) solutions. The larger the population the more likely it becomes that \nat least one agent will unsupervisedly learn the correct solution, and one might hope that this individual \ncan then seed correct learning by the whole group. However, our results suggest social learning is a \ndouble-edged sword, equally facilitating veridical and misleading learning. A related situation underlies \nthe “Rogers Paradox” (Rogers, 1988) (Boyd and Richerson, 1995) (Boyd et al., 2011). In Rogers’s \nevolutionary game model, reproducing agents can learn 2 alternative behaviors, only 1 of which confers \na fitness advantage in the current environment. They can learn these behaviors either individually or \nsocially. As a result, the degree of fitness advantage conferred by social learning depends on the \nproportion of the population that uses individual learning - when most of the population is composed of \nindividual learners (who always match their behavior to the current environment), social learning \nprovides an advantage, but as the proportion of social learners increases, learning mismatched behavior \nbecomes more likely. Rogers shows that the population will evolve to contain a mix of individual and \nsocial learners such that average fitness is identical to that of pure individual learners, so that social \nlearning confers no advantage. Similarly, in our model social learning does not selectively accelerate \nveridical learning. In both cases one must postulate that veridical learning confers a further advantage - \n21 \nin our case the ability to track a source might increase supervisory effectiveness. The IC allows inference \nwhich is useful in guiding behaviour whereas the PC describes the data compactly but does not reveal \nunderlying causes and thus gives a lower fitness advantage (Field, 1989). Of course ICA is a linear mixing \nmodel and the real world is not, but doing ICA on e.g. natural images could make progress towards \nfinding the causes  (Bell and Sejnowski, 1997; Chen and Gopinath, 2000).  \n \nIndeed it has been suggested (Nicholls et al., 2012) that the goal of sensory neocortical circuitry is to \ninfer causes: to convert sensations to perceptions. One possible iterative approach is using successive \nlayers of ICA-like learning followed by alternating layers of marginal Gaussianization and whitening \n(Chen and Gopinath, 2000; Laparra et al., 2011; Lyu and Simoncelli, 2009). Clearly such a deep, iterative \nstrategy would be imperilled by failure of the ICA-like step, for example because of synaptic crosstalk or \nimperfect whitening, and such failure would be more likely for hard problems.  More generally it's likely \nthat the learning that allows improved inference requires connection-specific learning. Our simplified \nmodel may throw light on these issues, especially in the context of social learning. \n \nWe have studied a minimal model of learning in which two agents are able to learn a problem in either \nunsupervised or supervised fashions. In this model each agent learns a task using a neural network \ninvolving incremental weight changes based on data from the environment (unsupervised ICA) or also \nusing information from another agent (supervised using the delta rule).  We look at the simplest possible \ncase where the observed data are generated from a mixture of two independent sources one of which is \nLaplacian and the other Gaussian. In the problem we study, multiple interacting agent ICA learning, the \ndata are usually assumed to be ‘white’ (meaning the data  have no pairwise correlations) and perfect ICA \nlearning will always find the IC (Hyvärinen et al., 2004) so that the behaviour of the underlying \nnonGaussian “cause” can be accurately inferred.  In real life the ability to correctly infer the cause of \nobservations (e.g. a tiger in the grass) will confer fitness advantages. In the real world individuals cannot \nreliably learn for a variety of reasons, including (a) imperfect whitening and (b) crosstalk (i.e. an \nimperfect learning rule).  \nIt has been shown previously that the simple picture of a neural network learning the IC becomes more \ncomplicated when the learning rule used (the Hebb rule) is no longer exactly specific and/or the \nwhitening is imperfect (Cox and Adams, 2009; Cox and Adams, 2014; Elliott, 2012) ; if the weight \nchanges leak slightly to other connections (synapses) then it is possible for the network to learn the \neigenvectors of EC where C is the correlation matrix (in the white case C = I, as in this paper). In the \ncurrent work we use perfect whitening but allow crosstalk.   However, it’s likely that similar results \nwould be found if imperfect whitening, either rather than or in addition to crosstalk, was a problem: \nlearning can get stuck at the wrong solution if the starting weights are unfavorable (see equation 8 in \n(Cox and Adams, 2014; Elliott, 2012)). Note that we are defining the correct solution as one that allows \nveridical tracking of the underlying source fluctuations. The underlying assumption is that identifying \ntrue causes is useful, and confers a fitness advantage. Many neural network theorists instead use a \ndifferent criterion: learning is deemed successful if it allows the brain to spontaneously generate (via an \ninternal generative model) outputs that have the same statistical regularities as the real word (Hinton, \n2007). In this narrow sense PC learning might be useful, because (especially when input statistics are \nGaussian), the output of a few neurons that can represent the first few principal components of the \ninput data can quite successful reconstruct the entire input. However, these outputs would not track the \nsource fluctuations. \n \n \n \n22 \nOur preliminary studies of 4 interacting agents suggests that one can probably extrapolate our results to \nthe case of large numbers of agents, in situations where a slight majority of agents' initial weights lie in \nthe IC basin of attraction.  A particular agent that engages in supervised learning would then be slightly \nmore likely to move to the IC, and as more agents find the IC, the overall movement to the IC would \naccelerate. This would be analogous to the meanfield ferromagnet model, where if even a tiny majority \nof spins align, there is a cascading movement to the majority direction. Of course once all the agents \nreach either the IC or PC they will remain there, since when an agent shifts to supervised learning, it will \nbe kept at the fixed point. \n \n \nWe have previously noted that the role of crosstalk (i.e. the limited synapse specificity of connection \nstrength updates) is analogous to that of polynucleotide copying errors in Darwinian evolution. In both \ncases the ability to acquire new information about the environment is limited by the accuracy of the \nelementary learning process (base replication or synapse strengthening) (Eigen, 1971). In the case of \nDarwinian evolution, this limit imposed on evolution by asexually-reproducing prokaryotic organisms is \ngreatly relaxed in sexually-reproducing eukaryotic organisms, which store much more information in \ntheir genomes (Ridley, 2000), though the exact mechanisms by which sex supercharges evolution \nremains controversial (Otto, 2009). In particular sexual reproduction, which is essentially a protocol for \nthe exchange of genetic information between members of the same breeding population (a “species”), \ncan easily be exploited by “selfish genes”. A well-known example is mitochondria. Selfish male \nmitochondrial genes that act to destroy female mitochondria (and vice versa) would undermine \norganismal fitness while promoting their own proliferation. Nuclear genes avoid this conflict by \neliminating all male mitochondrial genes by both pre- and post-fertilization processes (Ridley, 2000) \n(Hurst, 1995). More generally sexual reproduction affords many opportunities for parasitic genes to \nflourish at the expense of organismal fitness, and sexually reproducing organisms have developed \nmultiple mechanisms preventing free-riding. (Butcher and Deng, 1994; Hurst, 1993). \nOur results hint that an analogous situation exists for social learning. While social learning, by \nsupervision by others, can accelerate the spread of individual discoveries (see figure 3), it can equally \nassist in the spread of misleading ideas (in our model, the PC), which function as parasites. It has been \nsuggested that coupled learning by multiple agents could escape local minima in deep learning networks \n(Bengio, 2012), but this might suffer the same problem as we find, i.e. symmetrical communication \ndoesn’t help when teachers are also learners. In order to selectively enhance collective learning of useful \nweight vectors (e.g. ICs), their successful learning must confer an advantage. The learning population \nneeds not merely a shared protocol for the exchange of information (a “language”) but also shared \nprocedures for testing the utility and veridicality of concepts developed by means of unsupervised \nlearning from observations. An extreme example of such a procedure is “science”: empirical testing and \ndissemination of hypotheses developed by observation and experiment. However, human societies \npresumably developed previous less formal, but still highly cooperative, prosocial ways of testing the \nvalue of claimed “discoveries” (Marean, 2014; Marean, 2016). In particular, observationally-based ideas \nthat do not confer actual ability to infer underlying causes and predict outcomes, must somehow by \npenalized, while the adoption of observationally-based ideas that lead to successful outcomes must be \nenhanced. Future models of the interaction of individual and social learning should test this idea.  \nOne possible modification of our current model that might clarify this issue would be to incorporate \nsome sort of advantage accruing to those who successfully learn the correct solution. In the simplest \ncase one could allow occasional ‘cheating’: an agent obtains a ‘glimpse’ of the true source values, which \nit could use for supervised learning. One could view this as performing empirical experiments to check \nthe validity of one’s inferences. A more interesting and subtle variation of this idea would be to allow \nonly other agents, but not the glimpsing agent, see a measure of its success. In other words, no agent \n23 \ncould directly even glimpse the source, but nevertheless agents would be able to display a scalar \nmeasure of their success to other agents (but perhaps not to themselves), analogous to ‘reputation’. \nWe have tried to construct a minimal model to capture the essence of how supervised and unsupervised \nagents interact. The model is simple but has the advantage of being mathematically transparent, and \nthe issues that arise may be relevant more generally in more complex but less transparent models. We \nconclude that symbolic communication alone cannot reliably boost individual intelligence and that \nadditional social mechanisms such as trust, reputation, cooperation and science are also necessary. \n \n \nReferences \n \nAdams, P. R., Cox, K. J., 2012. From Life to Mind: 2 Prosaic Miracles? Springer, Berlin, Heidelberg. \nAtakulreka, A., Sutivong, D., 2007. Avoiding Local Minima in Feedforward Neural Networks by \nSimultaneous Learning. In: Orgun, M. A., Thornton, J., Eds.), AI 2007: Advances in Artificial \nIntelligence: 20th Australian Joint Conference on Artificial Intelligence, Gold Coast, Australia, \nDecember 2-6, 2007. Proceedings. Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 100-109. \nBartol, T. M., Bromer, C., Kinney, J., Chirillo, M. A., Bourne, J. N., Harris, K. M., Sejnowski, T. J., 2015. \nNanoconnectomic upper bound on the variability of synaptic plasticity. eLife 2015;4:e10778, \ndoi: 10.7554/eLife.10778. \nBell, A. J., Sejnowski, T. J., 1997. The \"independent components\" of natural scenes are edge filters. \nVision Res 37, 3327-38. \nBengio, Y., 2009. Learning Deep Architectures for AI. Found. Trends Mach. Learn. 2, 1-127. \nBengio, Y., 2012. Evolving Culture vs Local Minima. Studies in Computational Intelligence 557, \ndoi:10.1007/978-3-642-55337-0_3. \nBoyd, R., Richerson, P. J., 1995. Why does culture increase human adaptability? Ethology and \nSociobiology 16, 125-143, doi:https://doi.org/10.1016/0162-3095(94)00073-G. \nBoyd, R., Richerson, P. J., Henrich, J., 2011. The cultural niche: Why social learning is essential for human \nadaptation. Proceedings of the National Academy of Sciences 108, 10918-10925, \ndoi:10.1073/pnas.1100290108. \nButcher, D. L., Deng, H. W., 1994. Hypothetical SisterKiller. Nature 369, 26, doi:10.1038/369026a0. \nChen, S. S., Gopinath, R. A., 2000. Gaussianization. NIPS, 2000, 6. \nCichocki, A., Chen, T., Amari, S., 1997. Stability Analysis of Learning Algorithms for Blind Source \nSeparation. Neural Netw 10, 1345-1351. \nCox, K. J., Adams, P. R., 2009. Hebbian crosstalk prevents nonlinear unsupervised learning. Front Comput \nNeurosci 3, 11, doi:10.3389/neuro.10.011.2009. \nCox, K. J., Adams, P. R., 2014. Hebbian learning from higher-order correlations requires crosstalk \nminimization. Biol Cybern 108, 405-22, doi:10.1007/s00422-014-0608-4. \nDenaro, D., Parisi, D., 1997. Cultural Evolution in a Population of Neural Networks. In: Marinaro M., T. R. \ne., (Ed.), In: Neural Nets WIRN VIETRI-96. Perspectives in Neural Computing. . Springer, London. \nEigen, M., 1971. Selforganization of matter and the evolution of biological macromolecules. \nNaturwissenschaften 58, 465-523. \nElliott, T., 2012. Cross-Talk Induces Bifurcations in Nonlinear Models of Synaptic Plasticity. Neural \ncomputation 24, 1-68. \nErhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., Bengio, S., 2010. Why Does Unsupervised \nPre-training Help Deep Learning? J. Mach. Learn. Res. 11, 625-660. \nField, D., 1989. What the statistics of natural images tell us about visual coding. SPIE 1077, 7. \nHinton, G. E., 2007. To recognize shapes, first learn to generate images. Prog Brain Res 165, 535-47, \ndoi:10.1016/S0079-6123(06)65034-6. \n24 \nHurst, L. D., 1993. Evolutionary genetics. Drunken walk of the diploid. Nature 365, 206-7, \ndoi:10.1038/365206a0. \nHurst, L. D., 1995. Selfish genetic elements and their role in evolution: the evolution of sex and some of \nwhat that entails. Philos Trans R Soc Lond B Biol Sci 349, 321-32, doi:10.1098/rstb.1995.0120. \nHyvärinen, A., Oja, E., 1998. Independent component analysis by general nonlinear Hebbian-like learning \nrules. Signal Processing 64, 301-313, doi:10.1016/s0165-1684(97)00197-7. \nHyvärinen, A., Karhunen, J., Oja, E., 2004. Independent Component Analysis. Wiley. \nLaparra, V., Camps-Valls, G., Malo, J., 2011. Iterative Gaussianization: From ICA to Random Rotations. \nIEEE Trans. Neural Networks 4, 537-549. \nLeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature 521, 436-44, doi:10.1038/nature14539. \nLyu, S., Simoncelli, E. P., 2009. Nonlinear extraction of independent components of natural images using \nradial gaussianization. Neural Comput 21, 1485-519, doi:10.1162/neco.2009.04-08-773. \nMarean, C. W., 2014. The origins and significance of coastal resource use in Africa and Western Eurasia. J \nHum Evol 77, 17-40, doi:10.1016/j.jhevol.2014.02.025. \nMarean, C. W., 2016. The transition to foraging for dense and predictable resources and its impact on \nthe evolution of modern humans. Philos Trans R Soc Lond B Biol Sci 371, \ndoi:10.1098/rstb.2015.0239. \nNicholls, J. G., Martin, A. R., Brown, D. A., Diamond, M. E., Weisblat, D. A., , 2012. From Neuron to Brain. \nSinauer Associates. \nOtto, S. P., 2009. The evolutionary enigma of sex. Am Nat 174 Suppl 1, S1-S14, doi:10.1086/599084. \nRadulescu, A., Cox, K., Adams, P., 2009. Hebbian errors in learning: An analysis using the Oja model. \nJournal of Theoretical Biology 258, 489-501, doi:https://doi.org/10.1016/j.jtbi.2009.01.036. \nRattray, M., 2002. Stochastic trapping in a solvable model of on-line independent component analysis. \nNeural computation 14, 421-35, doi:10.1162/08997660252741185. \nRidley, M., 2000. The Cooperative Gene: How Mendel's Demon Explains the Evolution of Complex \nBeings. Free Press. \nRogers, A. R., 1988. Does Biology Constrain Culture? American Anthropologist 90, 819-831, \ndoi:10.1525/aa.1988.90.4.02a00030. \nSaxe, A. M., McClelland, J. M., Ganguli, S., 2013. Learning hierarchical category structure in deep neural \nnetworks. Proceedings of the Annual Meeting of the Cognitive Science Society 35. \nWessels, L. A., Barnard, E., 1992. Avoiding false local minima by proper initialization of connections. IEEE \nTrans Neural Netw 3, 899-905, doi:10.1109/72.165592. \nWidrow, B., Hoff, M. E., 1960. Adaptive Switching Circuits. IRE WESCON Convention Record 4, 96-104. \n \n \n IC \n",
  "categories": [
    "q-bio.NC",
    "q-bio.PE"
  ],
  "published": "2020-04-20",
  "updated": "2020-05-20"
}