{
  "id": "http://arxiv.org/abs/2004.10581v1",
  "title": "When and Why is Unsupervised Neural Machine Translation Useless?",
  "authors": [
    "Yunsu Kim",
    "Miguel Graça",
    "Hermann Ney"
  ],
  "abstract": "This paper studies the practicality of the current state-of-the-art\nunsupervised methods in neural machine translation (NMT). In ten translation\ntasks with various data settings, we analyze the conditions under which the\nunsupervised methods fail to produce reasonable translations. We show that\ntheir performance is severely affected by linguistic dissimilarity and domain\nmismatch between source and target monolingual data. Such conditions are common\nfor low-resource language pairs, where unsupervised learning works poorly. In\nall of our experiments, supervised and semi-supervised baselines with\n50k-sentence bilingual data outperform the best unsupervised results. Our\nanalyses pinpoint the limits of the current unsupervised NMT and also suggest\nimmediate research directions.",
  "text": "When and Why is Unsupervised Neural Machine Translation Useless?\nYunsu Kim\nMiguel Graça†\nHermann Ney\nHuman Language Technology and Pattern Recognition Group\nRWTH Aachen University, Aachen, Germany\n{surname}@cs.rwth-aachen.de\nAbstract\nThis paper studies the practicality of the\ncurrent state-of-the-art unsupervised meth-\nods in neural machine translation (NMT).\nIn ten translation tasks with various data\nsettings, we analyze the conditions un-\nder which the unsupervised methods fail\nto produce reasonable translations.\nWe\nshow that their performance is severely af-\nfected by linguistic dissimilarity and do-\nmain mismatch between source and tar-\nget monolingual data.\nSuch conditions\nare common for low-resource language\npairs, where unsupervised learning works\npoorly. In all of our experiments, super-\nvised and semi-supervised baselines with\n50k-sentence bilingual data outperform the\nbest unsupervised results.\nOur analyses\npinpoint the limits of the current unsuper-\nvised NMT and also suggest immediate re-\nsearch directions.\n1\nIntroduction\nStatistical methods for machine translation (MT)\nrequire a large set of sentence pairs in two lan-\nguages to build a decent translation system (Resnik\nand Smith, 2003; Koehn, 2005). Such bilingual\ndata is scarce for most language pairs and its\nquality varies largely over different domains (Al-\nOnaizan et al., 2002; Chu and Wang, 2018). Neu-\nral machine translation (NMT) (Bahdanau et al.,\n2015; Vaswani et al., 2017), the standard paradigm\nof MT these days, has been claimed to suffer from\nthe data scarcity more severely than phrase-based\nMT (Koehn and Knowles, 2017).\nUnsupervised NMT, which trains a neural trans-\nlation model only with monolingual corpora, was\n† The author is now at DeepL GmbH.\nc⃝2020 The authors. This article is licensed under a Creative\nCommons 3.0 licence, no derivative works, attribution, CC-\nBY-ND.\nproposed for those scenarios which lack bilingual\ndata (Artetxe et al., 2018b; Lample et al., 2018a).\nDespite its progress in research, the performance\nof the unsupervised methods has been evalu-\nated mostly on high-resource language pairs, e.g.\nGerman↔English or French↔English (Artetxe et\nal., 2018b; Lample et al., 2018a; Yang et al., 2018;\nArtetxe et al., 2018a; Lample et al., 2018b; Ren et\nal., 2019b; Artetxe et al., 2019; Sun et al., 2019;\nSen et al., 2019). For these language pairs, huge\nbilingual corpora are already available, so there\nis no need for unsupervised learning in practice.\nEmpirical results in these tasks do not carry over\nto low-resource language pairs; they simply fail to\nproduce any meaningful translations (Neubig and\nHu, 2018; Guzmán et al., 2019).\nThis paper aims for a more comprehensive and\npragmatic study on the performance of unsuper-\nvised NMT. Our experiments span ten translation\ntasks in the following ﬁve language pairs:\n• German↔English: similar languages, abun-\ndant bilingual/monolingual data\n• Russian↔English: distant languages, abun-\ndant bilingual/monolingual data, similar sizes\nof the alphabet\n• Chinese↔English: distant languages, abun-\ndant bilingual/monolingual data, very differ-\nent sizes of the alphabet\n• Kazakh↔English: distant languages, scarce\nbilingual data, abundant monolingual data\n• Gujarati↔English: distant languages, scarce\nbilingual/monolingual data\nFor each task, we compare the unsupervised per-\nformance with its supervised and semi-supervised\ncounterparts. In addition, we make the monolin-\ngual training data vary in size and domain to cover\nmany more scenarios, showing under which con-\nditions unsupervised NMT works poorly.\nHere is a summary of our contributions:\narXiv:2004.10581v1  [cs.CL]  22 Apr 2020\n• We thoroughly evaluate the performance of\nstate-of-the-art unsupervised NMT in numer-\nous real and artiﬁcial translation tasks.\n• We provide guidelines on whether to employ\nunsupervised NMT in practice, by showing\nhow much bilingual data is sufﬁcient to out-\nperform the unsupervised results.\n• We clarify which factors make unsupervised\nNMT weak and which points must be im-\nproved, by analyzing the results both quan-\ntitatively and qualitatively.\n2\nRelated Work\nThe idea of unsupervised MT dates back to word-\nbased decipherment methods (Knight et al., 2006;\nRavi and Knight, 2011). They learn only lexicon\nmodels at ﬁrst, but add alignment models (Dou et\nal., 2014; Nuhn, 2019) or heuristic features (Naim\net al., 2018) later. Finally, Artetxe et al. (2018a)\nand Lample et al. (2018b) train a fully-ﬂedged\nphrase-based MT system in an unsupervised way.\nWith neural networks, unsupervised learning of\na sequence-to-sequence NMT model has been pro-\nposed by Lample et al. (2018a) and Artetxe et al.\n(2018b). Though having slight variations (Yang et\nal., 2018; Sun et al., 2019; Sen et al., 2019), un-\nsupervised NMT approaches commonly 1) learn\na shared model for both source→target and\ntarget→source 2) using iterative back-translation,\nalong with 3) a denoising autoencoder objective.\nThey are initialized with either cross-lingual word\nembeddings or a cross-lingual language model\n(LM). To further improve the performance at the\ncost of efﬁciency, Lample et al. (2018b), Ren et\nal. (2019b) and Artetxe et al. (2019) combine un-\nsupervised NMT with unsupervised phrase-based\nMT. On the other hand, one can also avoid the\nlong iterative training by applying a separate de-\nnoiser directly to the word-by-word translations\nfrom cross-lingual word embeddings (Kim et al.,\n2018; Pourdamghani et al., 2019).\nUnsupervised NMT approaches have been so\nfar evaluated mostly on high-resource language\npairs, e.g.\nFrench→English, for academic pur-\nposes. In terms of practicality, they tend to un-\nderperform in low-resource language pairs, e.g.\nAzerbaijani→English (Neubig and Hu, 2018) or\nNepali→English (Guzmán et al., 2019).\nTo the\nbest of our knowledge, this work is the ﬁrst to\nsystematically evaluate and analyze unsupervised\nlearning for NMT in various data settings.\n3\nUnsupervised NMT\nThis section reviews the core concepts of the re-\ncent unsupervised NMT framework and describes\nto which points they are potentially vulnerable.\n3.1\nBidirectional Modeling\nMost of the unsupervised NMT methods share\nthe model parameters between source→target and\ntarget→source directions. They also often share a\njoint subword vocabulary across the two languages\n(Sennrich et al., 2016b).\nSharing a model among different translation\ntasks has been shown to be effective in multilin-\ngual NMT (Firat et al., 2016; Johnson et al., 2017;\nAharoni et al., 2019), especially in improving per-\nformance on low-resource language pairs.\nThis\nis due to the commonality of natural languages;\nlearning to represent a language is helpful to rep-\nresent other languages, e.g. by transferring knowl-\nedge of general sentence structures. It also pro-\nvides good regularization for the model.\nUnsupervised learning is an extreme scenario\nof MT, where bilingual information is very weak.\nTo supplement the weak and noisy training signal,\nknowledge transfer and regularization are crucial,\nwhich can be achieved by the bidirectional sharing.\nIt is based on the fact that a translation problem is\ndual in nature; source→target and target→source\ntasks are conceptually related to each other.\nPrevious works on unsupervised NMT vary in\nthe degree of sharing: the whole encoder (Artetxe\net al., 2018b; Sen et al., 2019), the middle layers\n(Yang et al., 2018; Sun et al., 2019), or the whole\nmodel (Lample et al., 2018a; Lample et al., 2018b;\nRen et al., 2019a; Conneau and Lample, 2019).\nNote that the network sharing is less effective\namong linguistically distinct languages in NMT\n(Kocmi and Bojar, 2018; Kim et al., 2019a). It still\nworks as a regularizer, but transferring knowledge\nis harder if the morphology or word order is quite\ndifferent. We show how well unsupervised NMT\nperforms on such language pairs in Section 4.1.\n3.2\nIterative Back-Translation\nUnsupervised learning for MT assumes no bilin-\ngual data for training. A traditional remedy for the\ndata scarcity is generating synthetic bilingual data\nfrom monolingual text (Koehn, 2005; Schwenk,\n2008; Sennrich et al., 2016a). To train a bidirec-\ntional model of Section 3.1, we need bilingual data\nof both translation directions. Therefore, most un-\nsupervised NMT methods back-translate in both\ndirections, i.e. source and target monolingual data\nto target and source language, respectively.\nIn unsupervised learning, the synthetic data\nshould be created not only once at the beginning\nbut also repeatedly throughout the training. At the\nearly stages of training, the model might be too\nweak to generate good translations. Hence, most\nmethods update the training data as the model gets\nimproved during training.\nThe improved model\nfor source→target direction back-translates source\nmonolingual data, which improves the model for\ntarget→source direction, and vice versa. This cy-\ncle is called dual learning (He et al., 2016) or itera-\ntive back-translation (Hoang et al., 2018). Figure 1\nshows the case when it is applied to a fully shared\nbidirectional model.\nencoder\ndecoder\nsource/target\njoint vocabulary\nsource/target\njoint vocabulary\nsource\nsentence\ntarget\ntranslation\ntarget\ntranslation\nsource\nsentence\n1)\n2)\n(a)\nencoder\ndecoder\nsource/target\njoint vocabulary\nsource/target\njoint vocabulary\ntarget\nsentence\nsource\ntranslation\nsource\ntranslation\ntarget\nsentence\n1)\n2)\n(b)\nFigure 1: Iterative back-translation for training a bidirec-\ntional sequence-to-sequence model. The model ﬁrst translates\nmonolingual sentences (solid arrows), and then gets trained\nwith the translation as the input and the original as the out-\nput (dashed arrows). This procedure alternates between (a)\nsource→target and (b) target→source translations.\nOne can tune the amount of back-translations\nper iteration: a mini-batch (Artetxe et al., 2018b;\nYang et al., 2018; Conneau and Lample, 2019; Ren\net al., 2019a), the whole monolingual data (Lam-\nple et al., 2018a; Lample et al., 2018b; Sun et\nal., 2019), or some size in between (Artetxe et al.,\n2019; Ren et al., 2019b).\nHowever, even if carefully scheduled, the itera-\ntive training cannot recover from a bad optimum if\nthe initial model is too poor. Experiments in Sec-\ntion 4.5 highlight such cases.\n3.3\nInitialization\nTo kickstart the iterative training, the model should\nbe able to generate meaningful translations already\nin the ﬁrst iteration. We cannot expect the training\nto progress from a randomly initialized network\nand the synthetic data generated by it.\nCross-lingual embeddings give a good starting\npoint for the model by deﬁning a joint continu-\nous space shared by multiple languages. Ideally, in\nsuch a space, close embedding vectors are seman-\ntically related to each other regardless of their lan-\nguages; they can be possible candidates for transla-\ntion pairs (Mikolov et al., 2013). It can be learned\neither in word level (Artetxe et al., 2017; Conneau\net al., 2018) or in sentence level (Conneau and\nLample, 2019) using only monolingual corpora.\nIn the word level, we can initialize the em-\nbedding layers with cross-lingual word embed-\nding vectors (Artetxe et al., 2018b; Lample et al.,\n2018a; Yang et al., 2018; Lample et al., 2018b;\nArtetxe et al., 2019; Sun et al., 2019). On the other\nhand, the whole encoder/decoder parameters can\nbe initialized with cross-lingual sequence training\n(Conneau and Lample, 2019; Ren et al., 2019a;\nSong et al., 2019).\nCross-lingual word embedding has limited per-\nformance among distant languages (Søgaard et al.,\n2018; Nakashole and Flauger, 2018) and so does\ncross-lingual LM (Pires et al., 2019). Section 4.5\nshows the impact of a poor initialization.\n3.4\nDenoising Autoencoder\nInitializing the word embedding layers furnishes\nthe model with cross-lingual matching in the lex-\nical embedding space, but does not provide any\ninformation on word orders or generation of text.\nCross-lingual LMs encode word sequences in dif-\nferent languages, but they are not explicitly trained\nto reorder source words to the target language syn-\ntax. Both ways do not initialize the crucial param-\neters for reordering: the encoder-decoder attention\nand the recurrence on decoder states.\nAs a result, an initial model for unsupervised\nNMT tends to generate word-by-word translations\nwith little reordering, which are very non-ﬂuent\nwhen source and target languages have distinct\nword orders. Training on such data discourages the\nmodel from reordering words, which might cause\na vicious cycle by generating even less-reordered\nsynthetic sentence pairs in the next iterations.\nAccordingly, unsupervised NMT employs an\nde-en\nru-en\nzh-en\nkk-en\ngu-en\nGerman\nEnglish\nRussian\nEnglish\nChinese\nEnglish\nKazakh\nEnglish\nGujarati\nEnglish\nLanguage family\nGermanic\nGermanic\nSlavic\nGermanic\nSinitic\nGermanic\nTurkic\nGermanic\nIndic\nGermanic\nAlphabet Size\n60\n52\n66\n52\n8,105\n52\n42\n52\n91\n52\nMonolingual\nSentences\n100M\n71.6M\n30.8M\n18.5M\n4.1M\nWords\n1.8B\n2.3B\n1.1B\n2.0B\n1.4B\n699M\n278.5M\n421.5M\n121.5M\n93.8M\nBilingual\nSentences\n5.9M\n25.4M\n18.9M\n222k\n156k\nWords\n137.4M\n144.9M\n618.6M\n790M\n440.3M\n482.9M\n1.6M\n1.9M\n2.3M\n1.5M\nTable 1: Training data statistics.\nadditional training objective of denoising autoen-\ncoding (Hill et al., 2016). Given a clean sentence,\nartiﬁcial noises are injected, e.g. deletion or per-\nmutation of words, to make a corrupted input. The\ndenoising objective trains the model to reorder the\nnoisy input to the correct syntax, which is essen-\ntial for generating ﬂuent outputs. This is done for\neach language individually with monolingual data,\nas shown in Figure 2.\nencoder\ndecoder\nsource/target\njoint vocabulary\nsource/target\njoint vocabulary\nnoisy\nsource\nnoisy\ntarget\nsource\nsentence\ntarget\nsentence\nFigure 2: Denoising autoencoder training for source or target\nlanguage.\nOnce the model is sufﬁciently trained for de-\nnoising, it is helpful to remove the objective or re-\nduce its weight (Graça et al., 2018). At the later\nstages of training, the model gets improved in re-\nordering and translates better; learning to denoise\nmight hurt the performance in clean test sets.\n4\nExperiments and Analysis\nData\nOur experiments were conducted on\nWMT 2018 German↔English and Russian↔En-\nglish, WMT 2019 Chinese↔English, Kazakh↔\nEnglish, and Gujarati↔English (Table 1). We pre-\nprocessed the data using the MOSES1 tokenizer\nand a frequent caser. For Chinese, we used the\nJIEBA segmenter2.\nLastly, byte pair encoding\n(BPE) (Sennrich et al., 2016b) was learned jointly\nover source and target languages with 32k merges\nand applied without vocabulary threshold.\nModel\nWe used 6-layer Transformer base ar-\nchitecture (Vaswani et al., 2017) by default:\n512-dimension embedding/hidden layers, 2048-\ndimension feedforward sublayers, and 8 heads.\nDecoding and Evaluation\nDecoding was done\nwith beam size 5. We evaluated the test perfor-\nmance with SACREBLEU (Post, 2018).\nUnsupervised Learning\nWe ran XLM3 by\nConneau and Lample (2019) for the unsupervised\nexperiments.\nThe back-translations were done\nwith beam search for each mini-batch of 16k to-\nkens. The weight of the denoising objective started\nwith 1 and linearly decreased to 0.1 until 100k up-\ndates, and then decreased to 0 until 300k updates.\nThe model’s encoder and decoder were both\ninitialized with the same pre-trained cross-lingual\nLM. We removed the language embeddings from\nthe encoder for better cross-linguality (see Section\n4.6). Unless otherwise speciﬁed, we used the same\nmonolingual training data for both pre-training and\ntranslation training. For the pre-training, we set the\nbatch size to 256 sentences (around 66k tokens).\nTraining was done with Adam (Kingma and Ba,\n2014) with an initial learning rate of 0.0001, where\ndropout (Srivastava et al., 2014) of probability 0.1\nwas applied to each layer output and attention\ncomponents. With a checkpoint frequency of 200k\nsentences, we stopped the training when the val-\nidation perplexity (pre-training) or BLEU (trans-\nlation training) was not improved for ten check-\n1http://www.statmt.org/moses\n2https://github.com/fxsjy/jieba\n3https://github.com/facebookresearch/XLM\nBLEU [%]\nApproach\nde-en\nen-de\nru-en\nen-ru\nzh-en\nen-zh\nkk-en\nen-kk\ngu-en\nen-gu\nSupervised\n39.5\n39.1\n29.1\n24.7\n26.2\n39.6\n10.3\n2.4\n9.9\n3.5\nSemi-supervised\n43.6\n41.0\n30.8\n28.8\n25.9\n42.7\n12.5\n3.1\n14.2\n4.0\nUnsupervised\n23.8\n20.2\n12.0\n9.4\n1.5\n2.5\n2.0\n0.8\n0.6\n0.6\nTable 2: Comparison among supervised, semi-supervised, and unsupervised learning. All bilingual data was used for the\n(semi-)supervised results and all monolingual data was used for the unsupervised results (see Table 1). All results are computed\non newstest2019 of each task, except for de-en/en-de and ru-en/en-ru on newstest2018.\npoints. We extensively tuned the hyperparameters\nfor a single GPU with 12GB memory, which is\nwidely applicable to moderate industrial/academic\nenvironments.\nAll other hyperparameter values\nfollow the recommended settings of XLM.\nSupervised Learning\nSupervised experiments\nused the same hyperparameters as the unsuper-\nvised learning, except 12k tokens for the batch\nsize, 0.0002 for the initial learning rate, and 10k\nbatches for each checkpoint.\nIf the bilingual training data contains less than\n500k sentence pairs, we reduced the BPE merges\nto 8k, the batch size to 2k, and the checkpoint\nfrequency to 4k batches; we also increased the\ndropout rate to 0.3 (Sennrich and Zhang, 2019).\nSemi-supervised Learning\nSemi-supervised\nexperiments continued the training from the super-\nvised baseline with back-translations added to the\ntraining data. We used 4M back-translated sen-\ntences for the low-resource cases, i.e. if the orig-\ninal bilingual data has less than 500k lines, and\n10M back-translated sentences otherwise.\n4.1\nUnsupervised vs. (Semi-)Supervised\nWe ﬁrst address the most general question of this\npaper: For NMT, can unsupervised learning re-\nplace semi-supervised or supervised learning? Ta-\nble 2 compares the unsupervised performance to\nsimple supervised and semi-supervised baselines.\nIn all tasks, unsupervised learning shows much\nworse performance than (semi-)supervised learn-\ning.\nIt produces readable translations in two\nhigh-resource language pairs (German↔English\nand Russian↔English), but their scores are only\naround half of the semi-supervised systems.\nIn\nother three language pairs, unsupervised NMT\nfails to converge at any meaningful optimum,\nreaching less than 3% BLEU scores. Note that,\nin these three tasks, source and target languages\nare very different in the alphabet, morphology, and\n104\n105\n106\n107\nbilingual training sentence pairs\n0\n10\n20\n30\n40\nBLEU [%]\nsupervised\nsemi-supervised\nunsupervised\n(a) German→English\n104\n105\n106\n107\nbilingual training sentence pairs\n0\n10\n20\n30\n40\nBLEU [%]\nsupervised\nsemi-supervised\nunsupervised\n(b) Russian→English\nFigure 3:\nSupervised and semi-supervised learning over\nbilingual training data size. Unsupervised learning (horizon-\ntal line) uses all monolingual data of Table 1.\nword order, etc. The results in Kazakh↔English\nand Gujarati↔English show that the current unsu-\npervised NMT cannot be an alternative to (semi-\n)supervised NMT in low-resource conditions.\nTo discover the precise condition where the\nunsupervised learning is useful in practice, we\nvary the size of the given bilingual training data\nfor (semi-)supervised learning and plot the re-\nsults in Figure 3.\nOnce we have 50k bilingual\nsentence pairs in German↔English, simple semi-\nsupervised learning already outperforms unsuper-\nvised learning with 100M monolingual sentences\nin each language. Even without back-translations\n(supervised), 100k-sentence bilingual data is sufﬁ-\ncient to surpass unsupervised NMT.\nIn the Russian↔English task, the unsupervised\nlearning performance can be more easily achieved\nwith only 20k bilingual sentence pairs using semi-\nsupervised learning. This might be due to that Rus-\nsian and English are more distant to each other\nthan German and English, thus bilingual training\nsignal is more crucial for Russian↔English.\nNote that for these two language pairs, the bilin-\ngual data for supervised learning are from many\ndifferent text domains, whereas the monolingual\ndata are from exactly the same domain of the test\nsets. Even with such an advantage, the large-scale\nunsupervised NMT cannot compete with super-\nvised NMT with tiny out-of-domain bilingual data.\n4.2\nMonolingual Data Size\nIn this section, we analyze how much monolin-\ngual data is necessary to make unsupervised NMT\nproduce reasonable performance. Figure 4 shows\nthe unsupervised results with different amounts of\nmonolingual training data. We keep the equal size\nfor source and target data, and the domain is also\nthe same for both (web-crawled news).\n104\n105\n106\n107\n108\nmonolingual training sentences\n0\n5\n10\n15\n20\n25\nBLEU [%]\nde-en\nru-en\nFigure 4: Unsupervised NMT performance over the size of\nmonolingual training data, where source and target sides have\nthe same size.\nFor German→English, training with only 1M\nsentences already gives a reasonable performance,\nwhich is only around 2% BLEU behind the 100M-\nsentence case. The performance starts to saturate\nalready after 5M sentences, with only marginal im-\nprovements by using more than 20M sentences.\nWe observe a similar trend in Russian→English.\nThis shows that, for the performance of unsu-\npervised NMT, using a massive amount of mono-\nlingual data is not as important as the similarity\nof source and target languages. Comparing to su-\npervised learning (see Figure 3), the performance\nsaturates faster when increasing the training data,\ngiven the same model size.\n4.3\nUnbalanced Data Size\nWhat if the size of available monolingual data is\nlargely different for source and target languages?\nThis is often the case for low-resource language\npairs involving English, where there is plenty of\ndata for English but not for the other side.\nOur experiments so far intentionally use the\nsame number of sentences for both sides. In Fig-\nure 5, we reduced the source data gradually while\nkeeping the large target data ﬁxed. To counteract\nthe data imbalance, we oversampled the smaller\nside to make the ratio of source-target 1:1 for\nBPE learning and mini-batch construction (Con-\nneau and Lample, 2019). We compare such un-\nbalanced data settings to the previous equal-sized\nsource/target settings.\n104\n105\n106\n107\nmonolingual training sentences\n0\n5\n10\n15\n20\n25\nBLEU [%]\nde-en (equal)\nde-en (unbalanced)\nru-en (equal)\nru-en (unbalanced)\nFigure 5: Unsupervised NMT performance over source train-\ning data size, where the target training data is ﬁxed to 20M\nsentences (dashed line). Solid line is the case where the target\ndata has the same number of sentences as the source side.\nInterestingly, when we decrease the target data\naccordingly (balanced, solid line), the performance\nis similar or sometimes better than using the full\ntarget data (unbalanced, dashed line). This means\nthat it is not beneﬁcial to use oversized data on one\nside in unsupervised NMT training.\nIf the data is severely unbalanced, the distribu-\ntion of the smaller side should be much sparser\nthan that of the larger side. The network tries to\ngeneralize more on the smaller data, reserving the\nmodel capacity for smoothing (Olson et al., 2018).\nThus it learns to represent a very different distribu-\ntion of each side, which is challenging in a shared\nmodel (Section 3.1). This could be the reason for\nno merit in using larger data on one side.\n4.4\nDomain Similarity\nIn high-resource language pairs, it is feasible to\ncollect monolingual data of the same domain on\nboth source and target languages. However, for\nlow-resource language pairs, it is difﬁcult to match\nthe data domain of both sides on a large scale.\nFor example, our monolingual data for Kazakh is\nmostly from Wikipedia and Common Crawl, while\nthe English data is solely from News Crawl. In\nthis section, we study how the domain similarity\nof monolingual data on the two sides affects the\nperformance of unsupervised NMT.\nIn Table 3, we artiﬁcially change the domain of\nthe source side to politics (UN Corpus4) or random\n(Common Crawl), while keeping the target domain\nﬁxed to newswire (News Crawl). The results show\nthat the domain matching is critical for unsuper-\nvised NMT. For instance, although German and\nEnglish are very similar languages, we see the per-\nformance of German↔English deteriorate down to\n-11.8% BLEU by the domain mismatch.\nDomain\nDomain\nBLEU [%]\n(en)\n(de/ru)\nde-en\nen-de\nru-en\nen-ru\nNewswire\nNewswire\n23.3\n19.9\n11.9\n9.3\nPolitics\n11.5\n12.2\n2.3\n2.5\nRandom\n18.4\n16.4\n6.9\n6.1\nTable 3: Unsupervised NMT performance where source and\ntarget training data are from different domains. The data size\non both sides is the same (20M sentences).\nTable 4 shows a more delicate case where we\nkeep the same domain for both sides (newswire)\nbut change the providers and years of the news\narticles. Our monolingual data for Chinese (Ta-\nble 1) consist mainly of News Crawl (from years\n2008-2018) and Gigaword 4th edition (from years\n1995-2008).\nWe split out the News Crawl part\n(1.7M sentences) and trained an unsupervised\nNMT model with the same amount of English\nmonolingual data (from News Crawl 2014-2017).\nSurprisingly, this experiment yields much better\nresults than using all available data. Even if the\nsize is small, the source and target data are col-\nlected in the same way (web-crawling) from sim-\nilar years (2010s), which seems to be crucial for\nunsupervised NMT to work.\nOn the other hand, when using the Gigaword\npart (28.6M sentences) on Chinese, unsupervised\n4https://conferences.unite.un.org/uncorpus\nYears\nYears\n#sents\nBLEU [%]\n(en)\n(zh)\n(en/zh)\nzh-en\nen-zh\n2014-2017\n2008-2018\n1.7M\n5.4\n15.1\n1995-2008\n28.6M\n1.5\n1.9\nTable 4: Unsupervised NMT performance where source and\ntarget training data are from the same domain (newswire) but\ndifferent years.\nlearning again does not function properly. Now the\nsource and target text are from different decades;\nthe distribution of topics might be different. Also,\nthe Gigaword corpus is from traditional newspaper\nagencies which can have a different tone from the\nonline text of News Crawl. Despite the large scale,\nunsupervised NMT proves to be sensitive to a sub-\ntle discrepancy of topic, style, period, etc. between\nsource and target data.\nThese results agree with Søgaard et al. (2018)\nwho show that modern cross-lingual word embed-\nding methods fail in domain mismatch scenarios.\n4.5\nInitialization vs. Translation Training\nThus far, we have seen a number of cases where\nunsupervised NMT breaks down. But which part\nof the learning algorithm is more responsible for\nthe performance:\ninitialization (Section 3.3) or\ntranslation training (Section 3.2 and 3.4)?\nIn Figure 6, we control the level of each of\nthe two training stages and analyze its impact on\nthe ﬁnal performance. We pre-trained two cross-\nlingual LMs as initializations of different quality:\nbad (using 10k sentences) and good (using 20M\nsentences). For each initial point, we continued the\ntranslation training with different amounts of data\nfrom 10k to 20M sentences.\n104\n105\n106\n107\nmonolingual training sentences\n0\n5\n10\n15\n20\n25\nBLEU [%]\nde-en (init 20M)\nde-en (init 10k)\nFigure 6: Unsupervised NMT performance over the training\ndata size for translation training, where the pre-training data\nfor initialization is ﬁxed (10k or 20M sentences).\nFrom the bad initialization, unsupervised learn-\ning cannot build a reasonable NMT model, no mat-\nTask\nBLEU [%]\nSource input\nSystem output\nReference output\nde-en\n23.8\nSeit der ersten Besichtigung wurde die\n1.000 Quadratfuß große ...\nSince the ﬁrst Besichtigung, the 3,000\nsquare fueled ...\nSince the ﬁrst viewing, the 1,000sq\nft ﬂat has ...\n10.4\nMünchen 1856:\nVier Karten, die Ihren\nBlick auf die Stadt verändern\nAustrailia 1856: Eight things that can\nkeep your way to the UK\nMunich 1856: Four maps that will\nchange your view of the city\nru-en\n12.0\nВ ходе первоочередных оператив-\nно-следственных мероприятий ус-\nтановлена личность роженицы\nThe первоочередных оператив-\nно-следственных мероприятий\nhave been established by the dolphin\nThe identity of the mother was de-\ntermined during preliminary inves-\ntigative and operational measures\nzh-en\n1.5\n... 调整要兼顾生产需要和消费需求。\n... 调整要兼顾生产需要and 消费需\n求.\n... adjustment must balance produc-\ntion needs with consumer demands.\nTable 5: Problematic translation outputs from unsupervised NMT systems (input copying, ambiguity in the same context).\nter how much data is used in translation training.\nWhen the initial model is strong, it is possible to\nreach 20% BLEU by translation training with only\n100k sentences. Using 1M sentences in transla-\ntion training, the performance is already compa-\nrable to its best. Once the model is pre-trained\nwell for cross-lingual representations, ﬁne-tuning\nthe translation-speciﬁc components seems man-\nageable with relatively small data.\nThis demonstrates the importance of initializa-\ntion over translation training in the current unsu-\npervised NMT. Translation training relies solely\non model-generated inputs, i.e. back-translations,\nwhich do not reﬂect the true distribution of the in-\nput language when generated with a poor initial\nmodel. On Figure 7, we plot all German→English\nunsupervised results we conducted up to the pre-\nvious section. It shows that the ﬁnal performance\ngenerally correlates with the initialization quality.\n24\n25\n26\n27\n28\n29\n210\ninitial LM perplexity\n0\n5\n10\n15\n20\n25\nBLEU [%]\nFigure 7: Unsupervised NMT performance over the valida-\ntion perplexity of the initial cross-lingual LM (de-en).\n4.6\nQualitative Examples\nIn this section, we analyze translation outputs of\nunsupervised systems to ﬁnd out why they record\nsuch low BLEU scores. Do unsupervised systems\nhave particular problems in the outputs other than\nlimited adequacy/ﬂuency?\nTable 5 shows translation examples from the un-\nsupervised systems. The ﬁrst notable problem is\ncopying input words to the output. This happens\nwhen the encoder has poor cross-linguality, i.e.\ndoes not concurrently model two languages well\nin a shared space. The decoder then can easily de-\ntect the input language by reading the encoder and\nmay emit output words in the same language.\nA good cross-lingual encoder should not give\naway information on the input language to the de-\ncoder. The decoder must instead rely on the ouptut\nlanguage embeddings or an indicator token (e.g.\n<2en>) to determine the language of output to-\nkens. As a simple remedy, we removed the lan-\nguage embeddings from the encoder and obtained\nconsistent improvements, e.g. from 4.3% to 11.9%\nBLEU in Russian→English. However, the problem\nstill remains partly even in our best-performing un-\nsupervised system (the ﬁrst example).\nThe copying occurs more often in inferior sys-\ntems (the last example), where the poor initial\ncross-lingual LM is the main reason for the worse\nperformance (Section 4.5).\nNote that the auto-\nencoding (Section 3.4) also encourages the model\nto generate outputs in the input language. We pro-\nvide more in-depth insights on the copying phe-\nnomenon in the appendix (Section A).\nAnother problem is that the model cannot distin-\nguish words that appear in the same context. In the\nsecond example, the model knows that Vier in Ger-\nman (Four in English) is a number, but it generates\na wrong number in English (Eight). The initial LM\nis trained to predict either Four or Eight given the\nsame surrounding words (e.g. 1856, things) and\nhas no clue to map Four to Vier.\nThe model cannot learn these mappings by itself\nwith back-translations. This problem can be partly\nsolved by subword modeling (Bojanowski et al.,\n2017) or orthographic features (Riley and Gildea,\n2018; Artetxe et al., 2019), which are however not\neffective for language pairs with disjoint alphabets.\n5\nConclusion and Outlook\nIn this paper, we examine the state-of-the-art un-\nsupervised NMT in a wide range of tasks and data\nsettings. We ﬁnd that the performance of unsuper-\nvised NMT is seriously affected by these factors:\n• Linguistic similarity of source and target lan-\nguages\n• Domain similarity of training data between\nsource and target languages\nIt is very hard to fulﬁll these in low-/zero-resource\nlanguage pairs, which makes the current unsuper-\nvised NMT useless in practice. We also ﬁnd that\nthe performance is not improved by using massive\nmonolingual data on one or both sides.\nIn practice, a simple, non-tuned semi-supervised\nbaseline with only less than 50k bilingual sen-\ntence pairs is sufﬁcient to outperform our best\nlarge-scale unsupervised system. At this moment,\nwe cannot recommend unsupervised learning for\nbuilding MT products if there are at least small\nbilingual data.\nFor the cases where there is no bilingual data\navailable at all, we plan to systematically com-\npare the unsupervised NMT to pivot-based meth-\nods (Kim et al., 2019b; Currey and Heaﬁeld, 2019)\nor multilingual zero-shot translation (Johnson et\nal., 2017; Aharoni et al., 2019).\nTo make unsupervised NMT useful in the future,\nwe suggest the following research directions:\nLanguage-/Domain-agnostic LM\nWe show in\nSection 4.5 that the initial cross-lingual LM actu-\nally determines the performance of unsupervised\nNMT. In Section 4.6, we argue that the poor perfor-\nmance is due to input copying, for which we blame\na poor cross-lingual LM. The LM pre-training\nmust therefore handle dissimilar languages and do-\nmains equally well. This might be done by careful\ndata selection or better regularization methods.\nRobust Translation Training\nOn the other\nhand, the current unsupervised NMT lacks a mech-\nanism to bootstrap out of a poor initialization. In-\nspired by classical decipherment methods (Section\n2), we might devalue noisy training examples or\nartiﬁcially simplify the problem ﬁrst.\nReferences\nAharoni, Roee, Melvin Johnson, and Orhan Firat.\n2019. Massively multilingual neural machine trans-\nlation. In NAACL-HLT, pages 3874–3884.\nAl-Onaizan, Yaser, Ulrich Germann, Ulf Hermjakob,\nKevin Knight, Philipp Koehn, Daniel Marcu, and\nKenji Yamada. 2002. Translation with scarce bilin-\ngual resources. Machine Translation, 17(1):1–17.\nArtetxe, Mikel, Gorka Labaka, and Eneko Agirre.\n2017. Learning bilingual word embeddings with (al-\nmost) no bilingual data. In ACL, pages 451–462.\nArtetxe, Mikel, Gorka Labaka, and Eneko Agirre.\n2018a. Unsupervised statistical machine translation.\nIn EMNLP, page 3632–3642.\nArtetxe, Mikel, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018b. Unsupervised neural ma-\nchine translation. In ICLR.\nArtetxe, Mikel, Gorka Labaka, and Eneko Agirre.\n2019.\nAn effective approach to unsupervised ma-\nchine translation. In ACL, pages 194–203.\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Ben-\ngio.\n2015.\nNeural machine translation by jointly\nlearning to align and translate. In ICLR.\nBojanowski, Piotr, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. TACL, 5:135–146.\nChu, Chenhui and Rui Wang. 2018. A survey of do-\nmain adaptation for neural machine translation. In\nCOLING, pages 1304–1319.\nConneau, Alexis and Guillaume Lample. 2019. Cross-\nlingual language model pretraining.\nIn NeurIPS,\npages 7057–7067.\nConneau, Alexis, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018.\nWord translation without parallel data. In ICLR.\nCurrey, Anna and Kenneth Heaﬁeld.\n2019.\nZero-\nresource neural machine translation with monolin-\ngual pivot data. In WNGT, pages 99–107.\nDou, Qing, Ashish Vaswani, and Kevin Knight. 2014.\nBeyond parallel data: Joint word alignment and deci-\npherment improves machine translation. In EMNLP,\npages 557–565.\nFirat, Orhan, Kyunghyun Cho, and Yoshua Bengio.\n2016.\nMulti-way, multilingual neural machine\ntranslation with a shared attention mechanism.\nIn\nNAACL-HLT, pages 866–875.\nGraça, Miguel, Yunsu Kim, Julian Schamper, Jiahui\nGeng, and Hermann Ney. 2018. The RWTH aachen\nuniversity English-German and German-English un-\nsupervised neural machine translation systems for\nWMT 2018. In WMT.\nGuzmán, Francisco, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato.\n2019.\nThe FLORES evaluation datasets for low-resource\nmachine translation: Nepali–English and Sinhala–\nEnglish. In EMNLP-IJCNLP, pages 6097–6110.\nHe, Di, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,\nTie-Yan Liu, and Wei-Ying Ma. 2016. Dual learning\nfor machine translation. In NIPS, pages 820–828.\nHill, Felix, Kyunghyun Cho, and Anna Korhonen.\n2016. Learning distributed representations of sen-\ntences from unlabelled data. In NAACL-HLT, pages\n1367–1377.\nHoang, Vu Cong Duy, Philipp Koehn, Gholamreza\nHaffari, and Trevor Cohn.\n2018.\nIterative back-\ntranslation for neural machine translation. In WNGT,\npages 18–24.\nJohnson, Melvin, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Viégas, Martin Wattenberg, Greg Corrado,\net al. 2017. Google’s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nTACL, 5(1):339–351.\nKim, Yunsu, Jiahui Geng, and Hermann Ney. 2018.\nImproving unsupervised word-by-word translation\nwith language model and denoising autoencoder. In\nEMNLP, pages 862–868.\nKim, Yunsu, Yingbo Gao, and Hermann Ney. 2019a.\nEffective cross-lingual transfer of neural machine\ntranslation models without shared vocabularies. In\nACL, pages 1246–1257.\nKim, Yunsu, Petre Petrov, Pavel Petrushkov, Shahram\nKhadivi, and Hermann Ney.\n2019b.\nPivot-based\ntransfer learning for neural machine translation be-\ntween non-English languages. In EMNLP-IJCNLP,\npages 866–876.\nKingma, Diederik P and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.\nKnight, Kevin, Anish Nair, Nishit Rathod, and Kenji\nYamada. 2006. Unsupervised analysis for decipher-\nment problems. In COLING/ACL, pages 499–506.\nKocmi, Tom and Ondˇrej Bojar. 2018. Trivial transfer\nlearning for low-resource neural machine translation.\nIn WMT, pages 244–252.\nKoehn, Philipp and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation.\nIn WNMT,\npages 28–39.\nKoehn, Philipp. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In MT Summit, pages\n79–86.\nLample,\nGuillaume,\nLudovic\nDenoyer,\nand\nMarc’Aurelio Ranzato.\n2018a.\nUnsupervised\nmachine translation using monolingual corpora only.\nIn ICLR.\nLample, Guillaume, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018b.\nPhrase-based & neural unsupervised machine trans-\nlation. In EMNLP, pages 5039–5049.\nLiu, Yinhan, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer.\n2020.\nMultilingual denoising\npre-training for neural machine translation.\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space.\nNaim, Iftekhar, Parker Riley, and Daniel Gildea. 2018.\nFeature-based decipherment for machine translation.\nComputational Linguistics, 44(3):525–546.\nNakashole, Ndapandula and Raphael Flauger.\n2018.\nCharacterizing departures from linearity in word\ntranslation. In ACL, pages 221–227.\nNeubig, Graham and Junjie Hu. 2018. Rapid adapta-\ntion of neural machine translation to new languages.\nIn EMNLP, pages 875–880.\nNuhn, Malte. 2019. Unsupervised Training with Appli-\ncations in Natural Language Processing. Ph.D. the-\nsis, Computer Science Department, RWTH Aachen\nUniversity.\nOlson, Matthew, Abraham Wyner, and Richard Berk.\n2018. Modern neural networks generalize on small\ndata sets. In NIPS, pages 3619–3628.\nPires, Telmo, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual bert?\nIn ACL,\npages 4996–5001.\nPost, Matt. 2018. A call for clarity in reporting bleu\nscores. In WMT, pages 186–191.\nPourdamghani,\nNima,\nNada\nAldarrab,\nMarjan\nGhazvininejad, Kevin Knight, and Jonathan May.\n2019.\nTranslating translationese:\nA two-step\napproach to unsupervised machine translation.\nIn\nACL, pages 3057–3062.\nRavi, Sujith and Kevin Knight. 2011. Deciphering for-\neign language. In ACL, pages 12–21.\nRen, Shuo, Yu Wu, Shujie Liu, Ming Zhou, and Shuai\nMa.\n2019a.\nExplicit cross-lingual pre-training\nfor unsupervised machine translation. In EMNLP-\nIJCNLP, pages 770–779.\nRen, Shuo, Zhirui Zhang, Shujie Liu, Ming Zhou, and\nShuai Ma.\n2019b.\nUnsupervised neural machine\ntranslation with smt as posterior regularization.\nResnik, Philip and Noah A. Smith. 2003. The web\nas a parallel corpus.\nComputational Linguistics,\n29(3):349–380.\nRiley, Parker and Daniel Gildea. 2018. Orthographic\nfeatures for bilingual lexicon induction.\nIn ACL,\npages 390–394.\nSchwenk, Holger.\n2008.\nInvestigations on large-\nscale lightly-supervised training for statistical ma-\nchine translation. In IWSLT.\nSen, Sukanta, Kamal Kumar Gupta, Asif Ekbal, and\nPushpak Bhattacharyya. 2019. Multilingual unsu-\npervised NMT using shared encoder and language-\nspeciﬁc decoders. In ACL, pages 3083–3089.\nSennrich, Rico and Biao Zhang. 2019. Revisiting low-\nresource neural machine translation: A case study.\nIn ACL, pages 211–221.\nSennrich, Rico, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation mod-\nels with monolingual data. In ACL, pages 86–96.\nSennrich, Rico, Barry Haddow, and Alexandra Birch.\n2016b.\nNeural machine translation of rare words\nwith subword units. In ACL, pages 1715–1725.\nSøgaard, Anders, Sebastian Ruder, and Ivan Vuli´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In ACL, pages 778–788.\nSong, Kaitao, Xu Tan, Tao Qin, Jianfeng Lu, and\nTie-Yan Liu.\n2019.\nMass: Masked sequence to\nsequence pre-training for language generation.\nIn\nICML, pages 5926–5936.\nSrivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov.\n2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nSun, Haipeng, Rui Wang, Kehai Chen, Masao Utiyama,\nEiichiro Sumita, and Tiejun Zhao. 2019. Unsuper-\nvised bilingual word embedding agreement for unsu-\npervised neural machine translation. In ACL, pages\n1235–1245.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nYang, Zhen, Wei Chen, Feng Wang, and Bo Xu.\n2018. Unsupervised neural machine translation with\nweight sharing. In ACL, pages 46–55.\nA\nInput Copying\nThis supplement further investigates why the input\ncopying (Section 4.6) occurs in the current unsu-\npervised NMT. We discover its root cause in the\nunsupervised loss function, present possible reme-\ndies, and illustrate the relation to model cross-\nlinguality with a toy experiment.\nA.1\nReconstruction Loss\nTraining loss of the unsupervised NMT (Figure 1)\nis basically reconstruction of a monolingual sen-\ntence via an intermediate representation, created as\nthe most probable output sequence of the current\nmodel’s parameters. This introduces an intrinsic\ndivergence of the model’s usage between in train-\ning (creating an intermediate sequence facilitates\nthe reconstruction) and in testing (producing a cor-\nrect translation).\nNote that there are no constraints on the interme-\ndiate space in training. This gives rise to a plethora\nof solutions to the loss optimization, which might\nbe not aligned with the actual goal of translation.\nIn principle, a model could learn any bijective\nfunction from a monolingual corpus to a set of dis-\ntinct sentences of the same size.\nHere, input copying (Table 5) is a trivial action\nfor the bidirectional model with a shared vocab-\nulary, which is reinforced by training on copied\nback-translations. It is easier than performing any\nkind of translation which might intrinsically re-\nmove information from the input sentence.\nA.2\nRemedies\nTo avoid the copying behavior, we should con-\nstrain the search space of the intermediate hy-\npotheses to only meaningful sequences in the de-\nsired language. This is rather clear in unsupervised\nphrase-based MT (Lample et al., 2018b; Artetxe et\nal., 2018a), where the search space is limited via\nthe choice of applicable rules and a monolingual\nlanguage model of the output language.\nFor unsupervised NMT, it is more difﬁcult due\nto the sharing of model parameters and vocabular-\nies over source and target languages (Section 3.1).\nA good initialization (Section 3.3) and denoising\nautoencoder (Section 3.4) bias the model towards\nﬂuent outputs, but they do not prevent the model\nfrom emitting the input language. The following\ntechniques help to control the output language in\nunsupervised NMT:\n• Restrict the output vocabulary to the desired\nlanguage (Liu et al., 2020)\n• Use language-speciﬁc decoders (Artetxe et\nal., 2018b; Sen et al., 2019)\n• Improve cross-linguality of the encoder, e.g.\nby adversarial training (Lample et al., 2018a)\nNote that these remedies might also harm the over-\nall training process in another respect, e.g. induc-\ning less regularization.\nA.3\nToy Example: Case Conversion\nWe empirically investigate the input copying prob-\nlem with a simple task of case conversion.\nIn\nthis task, the source and target languages con-\nsist only of 1-character words in lower- or up-\npercase respectively. Without any constraints in\nback-translation, the unsupervised NMT may learn\ntwo optimal solutions to the reconstruction loss: 1)\ncopy the casing (undesired) or 2) perform a trans-\nlation from uppercase to lowercase and vice versa\n(desired). We trained 1-layer, 64-dimension Trans-\nformer models with 100-sentence data on each\nside, and measure how often the model fails to con-\nverge to the desired solution.\nTo see the impact of cross-linguality, we com-\npare two initializations where lower- and upper-\ncase character embeddings are equally or sepa-\nrately initialized. When they are equal, the model\nalways found the desired solution (case conver-\nsion) in 10 out of 10 trials, whereas the separate\nvariant only found it in 2 out of 10 trials.\nOn\nconvergence, all experiments achieved zero recon-\nstruction loss.\nThese results are in line with the language\nembedding removal (Section 4.6); better cross-\nlinguality guides the model to refer to the case in-\ndicator on the target side, which leads to case con-\nversion.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-04-22",
  "updated": "2020-04-22"
}