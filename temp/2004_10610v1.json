{
  "id": "http://arxiv.org/abs/2004.10610v1",
  "title": "R-VGAE: Relational-variational Graph Autoencoder for Unsupervised Prerequisite Chain Learning",
  "authors": [
    "Irene Li",
    "Alexander Fabbri",
    "Swapnil Hingmire",
    "Dragomir Radev"
  ],
  "abstract": "The task of concept prerequisite chain learning is to automatically determine\nthe existence of prerequisite relationships among concept pairs. In this paper,\nwe frame learning prerequisite relationships among concepts as an unsupervised\ntask with no access to labeled concept pairs during training. We propose a\nmodel called the Relational-Variational Graph AutoEncoder (R-VGAE) to predict\nconcept relations within a graph consisting of concept and resource nodes.\nResults show that our unsupervised approach outperforms graph-based\nsemi-supervised methods and other baseline methods by up to 9.77% and 10.47% in\nterms of prerequisite relation prediction accuracy and F1 score. Our method is\nnotably the first graph-based model that attempts to make use of deep learning\nrepresentations for the task of unsupervised prerequisite learning. We also\nexpand an existing corpus which totals 1,717 English Natural Language\nProcessing (NLP)-related lecture slide files and manual concept pair\nannotations over 322 topics.",
  "text": "R-VGAE: Relational-variational Graph Autoencoder\nfor Unsupervised Prerequisite Chain Learning\nIrene Li1, Alexander Fabbri1, Swapnil Hingmire2 and Dragomir Radev1\n1Yale University, USA\n2Tata Consultancy Services Limited (TCS), India\nAbstract\nThe task of concept prerequisite chain learning is to automatically determine the existence of\nprerequisite relationships among concept pairs. In this paper, we frame learning prerequisite\nrelationships among concepts as an unsupervised task with no access to labeled concept pairs\nduring training. We propose a model called the Relational-variational Graph AutoEncoder (R-\nVGAE) to predict concept relations within a graph consisting of concept and resource nodes.\nResults show that our unsupervised approach outperforms graph-based semi-supervised methods\nand other baseline methods by up to 9.77% and 10.47% in terms of prerequisite relation pre-\ndiction accuracy and F1 score. Our method is notably the ﬁrst graph-based model that attempts\nto make use of deep learning representations for the task of unsupervised prerequisite learning.\nWe also expand an existing corpus which totals 1, 717 English Natural Language Processing\n(NLP)-related lecture slide ﬁles and manual concept pair annotations over 322 topics.\n1\nIntroduction\nWith the increasing amount of information available online, there is a rising need for structuring how one\nshould process that information and learn knowledge efﬁciently in a reasonable order. As a result, recent\nwork has tried to learn prerequisite relations among concepts, or which concept is needed to learn another\nconcept within a concept graph (Liang et al., 2017; Gordon et al., 2016; AlSaad et al., 2018). Figure 1\nshows an illustration of prerequisite chains as a directed graph. In such a graph, each node is a concept,\nand the direction of each edge indicates the prerequisite relation. Consider two concepts p and q, we\ndeﬁne p →q as p is a prerequisite concept of q. For example, the concept Variational Autoencoders is a\nprerequisite concept of the concept Variational Graph Autoencoders. If someone wants to learn about the\nconcept Variational Graph Autoencoders, the prerequisite concept Variation Autoencoder should appear\nin the prerequisite concept graph in order to create a proper study plan.\nRecent work has attempted to extract such prerequisite relationships from various types of materials\nincluding Wikipedia articles, university course dependencies or MOOCs (Massive Open Online Courses)\n(Pan et al., 2017; Gordon et al., 2016; Liang et al., 2017). However, these materials either need additional\nsteps for pre-processing and cleaning, or contain too many noisy free-texts, bringing more challenges to\nprerequisite relation learning or extracting. Recently, Li et al. (2019) presented a collection of university\nlecture slide ﬁles mainly in NLP lectures with related prerequisite concept annotations. We expanded\nthis dataset as we believe these lecture slides offer a concise yet comprehensive description of advanced\ntopics.\nDeep models such as word embeddings (Mikolov et al., 2013) and more recently contextualized word\nembeddings (Devlin et al., 2018) have achieved great success in the NLP tasks as demonstrate a stronger\nability to represent the semantics of the words than other traditional models. However, recent prereq-\nuisite learning approaches fail to make use of distributional semantics and advances in deep learning\nrepresentations (Labutov et al., 2017; Pan et al., 2017). In this paper, we investigate deep node embed-\ndings within a graph structure to better capture the semantics of concepts and resources, in order to learn\naccurate the prerequisite relations.\narXiv:2004.10610v1  [cs.CL]  22 Apr 2020\nGraph\nConvolutional \nNetworks\nVariational\nAutoencoders\nVariational\nGraph \nAutoencoders\nAutoencoders\nLaplacian \nMatrix\nConvolutional \nNetworks\nFigure 1: An illustration of prerequisite chains: we show six concepts and the relations. For exam-\nple, the concept Variational Autoencoders is a prerequisite concept of the concept Variational Graph\nAutoencoders.\nIn addition to learning node representations, there has been growing research in geometric deep learn-\ning (Bronstein et al., 2017) and graph neural networks (Gori et al., 2005), which apply the representa-\ntional power of neural networks to graph-structured data. Notably, Kipf and Welling (2017) proposed\nGraph Convolutional Networks (GCNs) to perform deep learning on graphs, yielding competitive results\nin semi-supervised learning settings. TextGCN was proposed by (Yao et al., 2018) to model a corpus as\na heterogeneous graph in order to jointly learn word and document embeddings for text classiﬁcation.\nWe build upon these ideas for constructing a resource-concept graph1. Additionally, most of the men-\ntioned methods require a subset of labels for training, a setting which is often infeasible in the real world.\nLimited research has been investigated learning prerequisite relations without using human annotated\nrelations during training (AlSaad et al., 2018). In practice, it is very challenging to obtain annotated\nconcept-concept relations, as the complexity for annotating is O(n2) given n concepts. To tackle this\nissue, we propose a method to learn prerequisite chains without any annotated concept-concept relations,\nwhich is more applicable in the real word.\nOur contributions are two-fold: 1) we expand upon previous annotations to increase coverage for pre-\nrequisite chain learning in ﬁve categories, including AI (artiﬁcial intelligence), ML (machine learning),\nNLP, DL (deep learning) and IR (information retrieval). We also expand a previous corpus of lecture\nﬁles to include an additional 5000 more lecture slides, totaling 1,717 ﬁles. More importantly, we add\nadditional concepts, totaling 322 concepts, as well as the corresponding annotations of each concept pair,\nwhich totals 103,362 relations. 2) we present a novel graph neural model for learning prerequisite rela-\ntions in an unsupervised way using deep representations as input. We model all concepts and resources\nin the corpus as nodes in a single heterogeneous graph and deﬁne a propagation rule to consider multi-\nple edge types by eliminating concept-concept relations during training, making it possible to perform\nunsupervised learning. Our model leads to improved performance over a number of baseline models.\nNotably, it is the ﬁrst graph-based model that attempts to make use of deep learning representations for\nthe task of unsupervised prerequisite learning. Resources, annotations and code are publicly available\nonline2.\n1We use the term resource instead of document for generalization.\n2https://github.com/Yale-LILY/LectureBank/tree/master/LectureBank2\nDomain\n#courses\n#ﬁles\n#tokens\n#pages\n#tokens/page\nNLP\n45\n953\n1,521,505\n37,213\n40.886\nML\n15\n312\n722,438\n12,556\n57.537\nDL\n7\n259\n450,879\n7,420\n60.765\nAI\n5\n98\n139,778\n3,732\n37.454\nIR\n5\n95\n205,359\n4,107\n50.002\nOverall\n77\n1,717\n3,039,959\n65,028\n46.748\nTable 1: Dataset Statistics. In each category, we have a given number of courses (#courses); each course\nconsists of lecture ﬁles (#ﬁles); each lecture ﬁle has a number of individual slides (#pages). We also\nshow the number of total tokens (#tokens) and average token number per slide (#tokens/page).\n2\nRelated Work\n2.1\nDeep Models for Graph-structured Data\nThere has been much research focused on graph-structured data such as social networks and citation\nnetworks (Sen et al., 2008; Akoglu et al., 2015; Defferrard et al., 2016), and many deep models have\nachieved satisfying results. Deepwalk (Perozzi et al., 2014) was a breakthrough model which learns node\nrepresentations using random walks. Node2vec (Grover and Leskovec, 2016) was an improved scalable\nframework, achieving promising results on multi-label classiﬁcation and link prediction. Besides, there\nhas been some work like graph convolution neural networks (GCNs), which target on deep-based prop-\nagation rules within graphs. A recent work applied GCN for text classiﬁcation (Yao et al., 2018) by\nconstructing a single text graph for a corpus based on word co-occurrence and document word relations.\nThe experimental results showed that the proposed model achieved state-of-the-art methods on many\nbenchmark datasets. We are inspired by this work in that we also attempt to construct a single graph for\na corpus, however, we have different types of nodes and edges.\n2.2\nPrerequisite Chain Learning\nLearning prerequisite relations between concepts has attracted much recent work in machine learning\nand NLP ﬁeld. Existing research focuses on machine learning methods (i.e., classiﬁers) to measure\nthe prerequisite relations among concepts (Liang et al., 2018; Liu et al., 2016; Liang et al., 2017).\nSome research integrates feature engineering to represent a concept, inputting these features to a classic\nclassiﬁer to predict relationship of a given concept pair (Liang et al., 2017; Liang et al., 2018). The\nresources to learn those concept features include university course descriptions and materials as well\nas online educational data (Liu et al., 2016; Liang et al., 2017). Recently, Li et al. (2019) introduced\na dataset containing 1,352 English lecture ﬁles collected from university-level lectures as well as 208\nmanually-labeled prerequisite relation topics, initially introduced in (Fabbri et al., 2018). To avoid feature\nengineering, they applied graph-based methods including GAE and VGAE (Kipf and Welling, 2017)\nwhich treat each concept as a node thus building a concept graph. They pretrained a Doc2vec model (Le\nand Mikolov, 2014) to infer each concept as a dense vector, and then trained the concept graph in a semi-\nsupervised way. Finally, the model was able to recover unseen edges of a concept graph. Different from\ntheir work, we wish to do the prerequisite chain learning in an unsupervised manner, while in training,\nno concept relations will be provided to the model.\n3\nDataset\n3.1\nResources\nWe manually collected English lecture slides mainly on NLP-related courses in recent years from known\nuniversities. We treated them as individual slide ﬁle in PDF or PowerPoint Presentations format. Our\nnew collection has 529 additional ﬁles from 17 courses, which we combined with the data provided by\n(Li et al., 2019). We ended up with a total number of 77 courses with 1,717 English lecture slide ﬁles,\nGraph\nConvolutional \nNetworks\nVariational\nAutoencoders\nVariational\nGraph \nAutoencoders\nAutoencoders\nLaplacian \nMatrix\nConvolutional \nNetworks\nR1\nR2\nR3\nR4\nR5\nFigure 2: Concept-resource graph for prerequisite chain learning: oval nodes indicate concept nodes, the\nblue rectangular nodes indicate resource nodes. We show three types of edges: the blue edge between\ntwo resource nodes Ar, the black solid edge between a concept node and a resource node Acr and the\nblack dashed edge between two concept nodes Ac. In the graph, the resource nodes R1 to R5 are example\nresources used to illustrate the idea. In practice there may be more edges, we show a part of the them for\nsimplicity.\ncovering ﬁve domains. We show the ﬁnal statistics in Table 1. For our experiments, we converted those\nﬁles into TXT format which allowed us to load the free texts directly.\n3.2\nConcepts\nWe manually expanded the size of concept list proposed by (Li et al., 2019) from 208 to 322. We included\nconcepts which were not found in their version like restricted boltzmann machine and neural parsing.\nAlso, we re-visited their topic list and corrected a small number of the topics. For example, we combined\ncertain topics (e.g. BLUE and ROUGE) into a single topic (machine translation evaluation). We asked\ntwo NLP PhD students to re-evaluate existing annotations from the old corpus and to provide labels for\neach added concept pair in the new corpus. A Cohen kappa score (Cohen, 1960) of 0.6283 achieved\nbetween our annotators which can be considered as a substantial agreement. We then took the union of\nthe annotations, where if at least one judge stated that a given concept pair (A, B) had A as a prerequisite\nof B, then we deﬁne it a positive relation. We believe that the union of annotations makes more sense for\nour downstream application, where we want users to be able to mark which concepts they already know\nand displaying all potential concepts is essential. We have 1,551 positive relations on the 322 concept\nnodes.\n4\nMethod\n4.1\nProblem Deﬁnition\nIn our corpus, every concept c is a single word or a phrase; every resource r is free text extracted from\nthe lecture ﬁles. We then wish to determine for a given concept pair (ci, cj), whether ci is a prerequisite\nconcept of cj. We deﬁne the concept-resource graph as G = (X, A), where X denotes node features\nor representations and A denotes the adjacency matrix. In our case, the adjacency matrix is the set\nof relations between each node pair, or the edges between the nodes. In Figure 2, we build a single,\nlarge graph consisting of concepts (oval nodes) and resources (rectangular nodes) as nodes, and the\ncorresponding relations as edges. So there are three types of edges in A: the edge between two resource\nnodes Ar (blue line), the edge between a concept node and a resource node Acr (black solid line), and\nthe edge between two concept nodes Ac (black dashed line). Our goal is to learn the relations between\nconcepts only (Ac), so prerequisite chain learning can be formulated as a link prediction problem. Our\nunsupervised setting is to exclude any direct concept relations (Ac) during training, and we wish to\npredict these edges through message passing via the resource nodes indirectly.\n4.2\nPreliminaries\nGraph Convolutional Networks (GCN) (Kipf and Welling, 2017) is a semi-supervised learning ap-\nproach for node classiﬁcation on graphs. It aims to learn the node representation H = {h1, h2, ..hn} in\nthe hidden layers, given the initial node representation X and the adjacency matrix A. The model in-\ncorporates local graph neighborhoods to represent a current node. In a simple GCN model, a layer-wise\npropagation rule can be deﬁned as the following:\nH(l+1) = σ(AH(l)W (l))\n(1)\nwhere l is the current layer number, σ(·) is a non-linear activation function, and W is a parameter\nmatrix that can be learned during training. We eliminate the σ(·) for the last layer output. For the task\nof node classiﬁcation, the loss function is cross-entropy loss. Typically, a two-layer GCN (by plugging\nEquation 1 in) is deﬁned as:\nGCN(X, A) = H2 = ˜AH1W 1 = ˜Aσ(AXW 0)W 1\n(2)\nwhere ˜A is the new adjacency matrix at the second graph layer.\nRelational Graph Convolutional Networks (R-GCNs) (Schlichtkrull et al., 2018) expands the types\nof graph nodes and edges based on the GCN model, allowing operations on large-scale relational data. In\nthis model, an edge between a node pair i and j is denoted as (vi, rel, vj), where rel ∈Rel is considered\na relation type, while in GCN, there is only one type. Similarly, to obtain the hidden representation of\nthe node i, we consider the local neighbors and itself; when multiple types of edges exist, different sets\nof weight will be considered. So the layer-wise propagation rule is deﬁned as:\nh(l+1)\ni\n= σ\n\n1\nM\nX\nrel∈Rel\nX\nj∈Nrel\ni\n(W (l)\nr h(l)\nj\n+ W (l)\n0 h(l)\ni )\n\n\n(3)\nwhere Rel is the set of relations or edge types in the graph, Nrel\ni\ndenotes the neighbors of node i with\nrelation rel, W (l)\nr\nis the weight matrix at layer l for nodes in Nrel\ni\n, W (l)\n0\nis the shared weight matrix at\nlayer l, M is the number of weight matrices in each layer.\nVariational Graph Auto-Encoders (V-GAE) (Kipf and Welling, 2016) is a framework for unsuper-\nvised learning on graph-structured data based on variational auto-encoders (Kingma and Welling, 2013).\nIt takes the adjacency matrix and node features as input and tries to recover the graph adjacency ma-\ntrix A through the hidden layer embeddings Z. Speciﬁcally, the non-probabilistic graph auto-encoder\n(GAE) model calculates embeddings via a two-layer GCN encoder: Z = GCN(X, A), which is given\nby Equation 2.\nThen, in the variational graph auto-encoder, the goal is to sample the latent parameters zi ∈Z from a\nnormal distribution:\nq (zi|X, A) = N\n\u0000zi|µi, diag\n\u0000σ2\ni\n\u0001\u0001\n(4)\nwhere µ = GCNµ(X, A) is the matrix of mean vectors, and logσ = GCNσ(X, A). The training loss\nthen is given as the KL-divergence between the normal distribution and the sampled parameters Z:\nLlatent =\nX\ni∈N\nKL\n\u0010\nN\n\u0010\nµi, diag (σi)2\u0011\n∥N(0, I)\n\u0011\n(5)\nIn the inference stage, the reconstructed adjacency matrix ˆA is the inner product of the latent parame-\nters Z: ˆA = σ(ZZT ).\n4.3\nProposed Model\nTo take multiple relations into consideration and make it possible to do unsupervised learning for con-\ncept relations, we propose our R-VGAE model. Our model builds upon R-GCN and VGAE by taking the\nadvantages of both: R-GCN is a supervised model that deals with multiple relations; VGAE is an unsu-\npervised graph neural network. We then make it possible to directly to train on a heterogeneous graph in\nan unsupervised way for link prediction, in order to learn the prerequisite relations for the concept pairs.\nOur model ﬁrst applies the R-GCN in Equation 3 as the encoder to obtain the latent parameters\nZ, given the initial node features X and adjacency matrix A: Z = R-GCN(X, A).\nIn terms of\nthe variational verison, as opposed to the standard VGAEs, we parameterize µ by the RGCN model:\nµ = R-GCNµ(X, A), and logσ = R-GCNσ(X, A).\nTo predict the link between a concept pair (ci, cj), we followed the DistMult (Yang et al., 2014)\nmethod: we take the last layer output node features ˆX, and deﬁne the following score function to recover\nthe adjacency matrix ˆA by learning a trainable weight matrix R:\nˆA = ˆX⊺R ˆX\n(6)\nThe loss consists of the cross-entropy reconstruction loss of adjacency matrix (Lcross) and the loss\nfrom the latent parameters deﬁned in Equation 5:\nL = Lcross(A, ˆA) + Llatent\n(7)\nWe compare two variations of our R-GAE model. Unsupervised: only the concept-resource edges\nAcr and resource-resource edges Ar are provided during training. This is an unsupervised model because\nno concept-concept edges are used. Semi-supervised: the model has access to concept-resource edges\nAcr and resource-resource edges Ar, as well as a percentage of the available concept-concept edges Ac,\ndescribed later.\n4.4\nNode Features X\nSparse Embeddings We used TFIDF (term frequencyinverse document frequency) to get sparse embed-\ndings for all nodes. We restricted the global vocabulary to be the 322 concept terms only, which means\nthat the dimension of the node features is 322, as we aim to model keywords.\nDense Embeddings As the concepts in our corpus often consist of phrases such as dynamic pro-\ngramming, we made use of Phrase2vec (Artetxe et al., 2018). Phrase2vec (P2V) is a generalization of\nskip-gram models (Mikolov et al., 2013) which learns n-gram embeddings during training, and here we\naim to infer the embeddings of the concepts in our corpus. We trained the P2V model using only our\ncorpus by treating each slide ﬁle as a short document as a sequence of tokens. For each resource node,\nwe take an element-wise average of the P2V embeddings of each single token and phrases that resource\ncovered. Similarly, for each concept node, we took element-wise average of the embeddings of each\nindividual token and the concept phrase. In addition, we then utilized the BERT model (Devlin et al.,\n2018) as another type of dense embedding. We ﬁne-tuned the masked language modeling of BERT using\nour corpus.\n4.5\nAdjacency Matrix A\nTo construct the adjacency matrix A, for each node pair (vi, vj), we applied cosine similarity based on\nenriched TFIDF features3 as the value Aij. Previous work has applied cosine similarity for vector space\nmodels (Garc´ıa-Pablos et al., 2018; Zuin et al., 2018; Bhatia et al., 2016), so we believe it is a suitable\nmethod in our case. This way we were able to generate concept-resource edge values (Acr) and resource-\nresource edge values (Ar). Note that for concept-concept edge values Ac: 1 if ci is a prerequisite of cj,\n0 otherwise. These values are not computed in the unsupervised setting.\n3This means that the TFIDF features are calculated on an extended vocabulary that includes all possible tokens appeared in\nthe corpus.\nMethod\nAcc\nF1\nMAP\nAUC\nConcept embedding + classiﬁer\nP2V (lb1)\n0.5927\n0.5650\n0.5623\n0.5929\nP2V (lb2)\n0.6369\n0.5961\n0.6282\n0.6370\nBERT (lb1)\n0.6540\n0.6099\n0.6475\n0.6540\nBERT (lb2)\n0.6558\n0.6032\n0.6553\n0.6558\nBERT (original)\n0.7088\n0.6963\n0.6779\n0.7090\nGraph-based methods\nDeepWalk (Perozzi et al., 2014)\n0.6292\n0.5860\n0.6270\n0.6281\nNode2vec (Grover and Leskovec, 2016)\n0.6209\n0.6181\n0.5757\n0.6259\nVGAE (Li et al., 2019)\n0.6055\n0.6030\n0.5628\n0.6055\nGAE (Li et al., 2019)\n0.6307\n0.6275\n0.5797\n0.6307\nR-GCN (Schlichtkrull et al., 2018)\n0.5387\n0.4784\n0.5203\n0.5387\nR-VGAE (Our proposed model)\nUS+BERT (ﬁne-tuned)\n0.5704\n0.5704\n0.5579\n0.5955\nUS+BERT (original)\n0.5669\n0.5668\n0.5658\n0.6164\nUS+TFIDF\n0.6495\n0.6458\n0.7069\n0.5507\nUS+P2V\n0.7694*\n0.7638*\n0.8919*\n0.9126*\nSS+BERT (ﬁne-tuned)\n0.6942\n0.6942\n0.6613\n0.7412\nSS+BERT (original)\n0.6839\n0.6839\n0.6556\n0.7372\nSS+TFIDF\n0.7252\n0.7082\n0.8181\n0.7625\nSS+P2V\n0.8065\n0.8010\n0.9380\n0.9454\nTable 2: Accuracy (Acc), macro F1, MAP and AUC scores on balanced test set including 10% of prereq-\nuisite edges. Bold values are the best results within its experiment group. Underscored values indicate a\nbetter performance compared with the trained corpus. Values with an asterisk mean the best performance\nin the unsupervised setting.\n5\nEvaluation\nWe compare our proposed model with two groups of baseline models. We report accuracy, F1 scores, the\nmacro averaged Mean Average Precision (MAP) and Area under the ROC Curve (AUC) scores in Table\n2, as done by previous research (Chaplot et al., 2016; Pan et al., 2017; Li et al., 2019). We split the 1, 551\npositive relations into 9:1 (train/test), and randomly select negative relations as negative training samples,\nand then we run over ﬁve random seeds and report the average scores, following the same setting with\nKipf and Welling (2016) and Li et al. (2019).\nConcept embedding + classiﬁer The ﬁrst group is the concept embedding with traditional classiﬁers\nincluding Support Vector Machines, Logistic Regression, Na¨ıve Bayes and Random Forest. For a given\nconcept pair, we concatenate the dense embeddings for both concepts as input to train the classiﬁers,\nand then we report the best result. We compare Phrase2Vec (P2V) and BERT embeddings. We have two\ncorpora: one is the old version (lb1) provided by (Li et al., 2019), another one is our version (lb2). For the\nBERT model, we applied both the original version from Google (original) 4, and the ﬁne-tuned language\nmodels version on our corpora (lb1, lb2) from Xiao (2018), and perform inference on the concepts.\nThe P2V embeddings have 150 dimension, and the BERT embeddings have 768 dimensions. We show\nimprovements on the BERT and P2V baselines by using our additional data via the underscored values.\nThis indicates that the concept relations can be more accurately predicted when enriching the training\ncorpus to train better embeddings. In our following experiments, if not speciﬁed, we applied lb2 as the\ntraining corpus.\nGraph-based methods We apply the classic graph-based embedding methods DeepWalk (Perozzi et\nal., 2014) and Node2vec (Grover and Leskovec, 2016), by considering the concept nodes only. Then the\n4https://github.com/google-research/bert\npositive concept relations in training set are the known sequences, allowing to train both models to infer\nnode features. Similarly, in the testing phrase, we concatenate the node embeddings given a concept pair,\nand utilize the mentioned classiﬁers to predict the relation and report the performance of the best one. We\nthen include VGAE and GAE methods for prerequisite chain learning following Li et al. (2019). Both\nmethods construct the concept graph in a semi-supervised way. We apply P2V embeddings to replicate\ntheir methods, though it is possible to try additional embeddings, this is not our main focus. Finally, we\ncompare with the original R-GCN model for link prediction proposed by Schlichtkrull et al. (2018) and\napply the same embeddings with the VGAE and GAE methods. Other semi-supervised graph methods\nsuch as GCNs require node labels and thus are not applicable to our setting. We can see that the GAE\nmethod achieves the best results among the baselines. Compare with the ﬁrst group, BERT (original)\nstill has a better performance due to its ability to represent phrases.\nR-VGAE Our model can be trained in both unsupervised (US+*) and semi-supervised (SS+*) way.\nWe also utilize various types of embeddings include P2V, TFIDF, BERT (ﬁne-tuned) and BERT (orig-\ninal). The best performed model in the unsupervised setting is with P2V embeddings, marked with\nasterisks, and it is better than all the baseline supervised methods with a large margin. In addition, our\nsemi-supervised setting models boost the overall performance. We show that the SS+P2V model per-\nforms the best among all the mentioned methods, with a signiﬁcant improvement of 9.77% in accuracy\nand 10.47% in F1 score compared with the best baseline model BERT (original). This indicates that\nR-VGAE model does better on link prediction by bringing extra resource nodes into the graph, while\nthe concept relation can be improved and enhanced indirectly via the connected resource nodes. We also\nobserve that with BERT embeddings, the performance lags behind the other embedding methods for our\napproach. A reason might be that the dimensionality of the BERT embeddings is relatively large com-\npared to P2V and may cause overﬁtting, especially when the edges are sparse; and it might not suitable\nto represent resources as they are a list of keywords when ﬁne-tuning the language modeling. The P2V\nembeddings outperform TFIDF for both unsupervised and semi-supervised models. This shows that\ncompared with sparse embeddings, dense embeddings can better preserve the semantic features when\nintegrated within the R-GAE model, thus boosting the performance. Besides, as a variation of R-GCN\nand GAE, our model surpasses them by taking the advantages of both, comparing with R-GCN and GAE\nresults reported in the second group.\n6\nAnalysis\nWe then take the recovered concept relations from our best performed model R-VGAE (SS+P2V) in Table\n2), and compare them with the gold annotated relations. Note that here we only look at concept nodes.\nThe average degree for gold graph concept nodes is 9.79, while our recovered one has an average degree\nof 6.10, and this means our model predicts fewer edges. We also check the most popular concepts that\nhave the most degrees. We select dependency parsing and tree adjoining grammar as examples. In Table\n3, we show a comparison of the prerequisites from the annotations and our model’s output. The upper\ngroup illustrates results for dependency parsing, where one can notice that the predicted concepts all\nappear in the gold results, missing only a single concept. This shows that even though our model predicts\nless number of relations, it still predicts correct relations. The lower group shows the comparison for the\nconcept tree adjoining grammar, our model gives precise prerequisite concepts among all eight concepts\nfrom the gold set. When a concept has a certain amount number of prerequisite concepts, our model\nis able to provide a comprehensive concept set with a good quality. In the real word, especially in a\nlearner’s scenario, he or she wants to learn the new concept with enough prerequisite knowledge, which\nour model tends to provide.\n7\nConclusion and Future Work\nIn this paper we introduced an expanded dataset for prerequisite chain learning with additional an 5,000\nlecture slides, totaling 1,717 ﬁles. We also provided prerequisite relation annotations for each concept\npair among 322 concepts. Additionally, we proposed an unsupervised learning method which makes\nuse of advances in graph-based deep learning algorithms. Our method avoids any feature engineering\nConcept\nGold Prerequisite Concepts\nModel Output Concept\ndependency\nparsing\nsyntax\nsyntax\nclassic parsing methods\nclassic parsing methods\nlinguistics basics\nlinguistics basics\nparsing\nparsing\nnlp introduction\nnlp introduction\nchomsky hierarchy\nchomsky hierarchy\nlinear algebra\nlinear algebra\nconditional probability\ntree\nadjoining\ngrammar\nclassic parsing methods\nclassic parsing methods\nlinguistics basics\nlinguistics basics\nparsing\nparsing\nnlp introduction\nnlp introduction\ncontext free grammar\ncontext free grammar\nprobabilistic context free grammars\nprobabilistic context free grammars\nchomsky hierarchy\nchomsky hierarchy\ncontext sensitive grammar\ncontext sensitive grammar\nTable 3: A comparison of prerequisite concepts of dependency parsing (upper group) and tree adjoining\ngrammar (lower group) from our annotated gold labels and labels recovered by our best unsupervised\nmodel.\nto learn concept representations. Experimental results demonstrate that our model performs well in\nan unsupervised setting and is able to further beneﬁt when labeled data is available. In future work,\nwe would like to perform a more comprehensive model comparison and evaluation by bringing other\npossible variations of graph-based models to learn a concept graph. Another interesting direction is to\napply multi-task learning to the proposed model by adding a node classiﬁcation task if there are node\nlabels available. A part of the future work would also include developing educational applications for\nlearners to ﬁnd out their study path for certain concepts.\nReferences\nLeman Akoglu, Hanghang Tong, and Danai Koutra. 2015. Graph based anomaly detection and description: a\nsurvey. Data mining and knowledge discovery, 29(3):626–688.\nFareedah AlSaad, Assma Boughoula, Chase Geigle, Hari Sundaram, and ChengXiang Zhai. 2018. Mining MOOC\nLecture Transcripts to Construct Concept Dependency Graphs. International Educational Data Mining Society.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. Unsupervised Statistical Machine Translation. In Pro-\nceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 3632–3642.\nShraey Bhatia, Jey Han Lau, and Timothy Baldwin. 2016. Automatic labelling of topics with neural embeddings.\narXiv preprint arXiv:1612.05340.\nMichael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. 2017. Geometric Deep\nLearning: Going Beyond Euclidean Data. IEEE Signal Process. Mag., 34(4):18–42.\nDevendra Singh Chaplot, Yiming Yang, Jaime Carbonell, and Kenneth R Koedinger. 2016. Data-driven Auto-\nmated Induction of Prerequisite Structure Graphs. International Educational Data Mining Society.\nJacob Cohen. 1960. A Coefﬁcient of Agreement for Nominal Scales. Educational and psychological measure-\nment, 20(1):37–46.\nMicha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs\nwith fast localized spectral ﬁltering. In Advances in neural information processing systems, pages 3844–3852.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nAlexander Fabbri, Irene Li, Prawat Trairatvorakul, Yijiao He, Weitai Ting, Robert Tung, Caitlin Westerﬁeld, and\nDragomir Radev. 2018. Tutorialbank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction\nand Resource Recommendation. In Proceedings of the 56th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages 611–620.\nAitor Garc´ıa-Pablos, Montse Cuadros, and German Rigau. 2018. W2vlda: almost unsupervised system for aspect\nbased sentiment analysis. Expert Systems with Applications, 91:127–137.\nJonathan Gordon, Linhong Zhu, Aram Galstyan, Prem Natarajan, and Gully Burns. 2016. Modeling Concept\nDependencies in a Scientiﬁc Corpus. In Proceedings of the 54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), volume 1, pages 866–875.\nMarco Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A new model for learning in graph domains. In\nProceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2, pages 729–734.\nIEEE.\nAditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\nDiederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\nThomas N Kipf and Max Welling. 2016. Variational Graph Auto-Encoders. Bayesian Deep Learning Workshop\n(NIPS 2016).\nThomas N. Kipf and Max Welling. 2017. Semi-Supervised Classiﬁcation with Graph Convolutional Networks.\nIn 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings.\nIgor Labutov, Yun Huang, Peter Brusilovsky, and Daqing He. 2017. Semi-Supervised Techniques for Mining\nLearning Outcomes and Prerequisites. In Proceedings of the 23rd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pages 907–915. ACM.\nQuoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International\nconference on machine learning, pages 1188–1196.\nIrene Li, Alexander R Fabbri, Robert R Tung, and Dragomir R Radev. 2019. What Should I Learn First: Introduc-\ning Lecturebank for NLP Education and Prerequisite Chain Learning. In 33rd AAAI Conference on Artiﬁcial\nIntelligence (AAAI-19).\nChen Liang, Jianbo Ye, Zhaohui Wu, Bart Pursel, and C Lee Giles. 2017. Recovering Concept Prerequisite\nRelations from University Course Dependencies. In Thirty-First AAAI Conference on Artiﬁcial Intelligence.\nChen Liang, Jianbo Ye, Shuting Wang, Bart Pursel, and C Lee Giles. 2018. Investigating active learning for\nconcept prerequisite learning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.\nHanxiao Liu, Wanli Ma, Yiming Yang, and Jaime Carbonell. 2016. Learning concept graphs from online educa-\ntional data. Journal of Artiﬁcial Intelligence Research, 55:1059–1090.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of\nWords and Phrases and Their Compositionality. In Advances in neural information processing systems, pages\n3111–3119.\nLiangming Pan, Chengjiang Li, Juanzi Li, and Jie Tang. 2017. Prerequisite Relation Learning for Concepts in\nMOOCs. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 1447–1456.\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In\nProceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\nKDD ’14, pages 701–710, New York, NY, USA. ACM.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. 2018.\nModeling Relational Data with Graph Convolutional Networks. In European Semantic Web Conference, pages\n593–607. Springer.\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008. Collec-\ntive classiﬁcation in network data. AI magazine, 29(3):93–93.\nHan Xiao. 2018. bert-as-service. https://github.com/hanxiao/bert-as-service.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2014. Embedding entities and relations for\nlearning and inference in knowledge bases. arXiv preprint arXiv:1412.6575.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2018. Graph Convolutional Networks for Text Classiﬁcation. In\n33rd AAAI Conference on Artiﬁcial Intelligence (AAAI-19).\nGianlucca Zuin, Luiz Chaimowicz, and Adriano Veloso. 2018. Learning transferable features for open-domain\nquestion answering. In 2018 International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-04-22",
  "updated": "2020-04-22"
}