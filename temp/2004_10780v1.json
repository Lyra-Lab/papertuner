{
  "id": "http://arxiv.org/abs/2004.10780v1",
  "title": "Diagram Image Retrieval using Sketch-Based Deep Learning and Transfer Learning",
  "authors": [
    "Manish Bhattarai",
    "Diane Oyen",
    "Juan Castorena",
    "Liping Yang",
    "Brendt Wohlberg"
  ],
  "abstract": "Resolution of the complex problem of image retrieval for diagram images has\nyet to be reached. Deep learning methods continue to excel in the fields of\nobject detection and image classification applied to natural imagery. However,\nthe application of such methodologies applied to binary imagery remains limited\ndue to lack of crucial features such as textures,color and intensity\ninformation. This paper presents a deep learning based method for image-based\nsearch for binary patent images by taking advantage of existing large natural\nimage repositories for image search and sketch-based methods (Sketches are not\nidentical to diagrams, but they do share some characteristics; for example,\nboth imagery types are gray scale (binary), composed of contours, and are\nlacking in texture).\n  We begin by using deep learning to generate sketches from natural images for\nimage retrieval and then train a second deep learning model on the sketches. We\nthen use our small set of manually labeled patent diagram images via transfer\nlearning to adapt the image search from sketches of natural images to diagrams.\nOur experiment results show the effectiveness of deep learning with transfer\nlearning for detecting near-identical copies in patent images and querying\nsimilar images based on content.",
  "text": "Diagram Image Retrieval\nusing Sketch-Based Deep Learning and Transfer Learning\nManish Bhattarai1,2,∗, Diane Oyen2, Juan Castorena2, Liping Yang1, and Brendt Wohlberg2\n1 University of New Mexico, Albuquerque, NM, USA\n2 Los Alamos National Laboratory, Los Alamos, NM, USA\n∗Corresponding author: Manish Bhattarai, ceodspspectrum@lanl.gov\nAbstract\nResolution of the complex problem of image retrieval\nfor diagram images has yet to be reached. Deep learning\nmethods continue to excel in the ﬁelds of object detection\nand image classiﬁcation applied to natural imagery. How-\never, the application of such methodologies applied to bi-\nnary imagery remains limited due to lack of crucial features\nsuch as textures,color and intensity information. This pa-\nper presents a deep learning based method for image-based\nsearch for binary patent images by taking advantage of ex-\nisting large natural image repositories for image search and\nsketch-based methods (Sketches are not identical to dia-\ngrams, but they do share some characteristics; for exam-\nple, both imagery types are gray scale (binary), composed\nof contours, and are lacking in texture). We begin by us-\ning deep learning to generate sketches from natural images\nfor image retrieval and then train a second deep learning\nmodel on the sketches. We then use our small set of man-\nually labeled patent diagram images via transfer learning\nto adapt the image search from sketches of natural images\nto diagrams. Our experiment results show the effectiveness\nof deep learning with transfer learning for detecting near-\nidentical copies in patent images and querying similar im-\nages based on content.\n1. Introduction and motivation\nThe patent industry involves the management and track-\ning of an enormous amount of data, much of which takes\nthe form of scientiﬁc drawings, technical diagrams and\nhand sketched models. The comparison of ﬁgures across\nthis dataset and subsequent retrieval based on similarity in\nreal-time is extremely challenging [15], [29], [30].\nWe\naim to track the spread of technical information by ﬁnd-\ning copies and modiﬁed copies of technical diagrams in\npatent databases and academic journals. Machine Learning\n(ML), and especially Deep Learning (DL) techniques offer\nthe possibility of performing thousands of diagram/diagram\ncomparisons across multiple databases in seconds.\nWe\npresent an ML approach that offers a high comparison ac-\ncuracy with very little training data and, given a speciﬁc\ndiagram and image of interest, under single shot and zero\nshot conditions can scan a database and retrieve all of the\nclosest matches in that database for further review. Here,\nwe present a deep learning approach that takes advantage\nof existing natural image repositories for image search and\nsketch-based methods applied to binary patent imagery.\nThe success of conventional CNN frameworks is widely\nacknowledged in image classiﬁcation and cross-domain re-\nconstruction applied to natural imagery when images have\ncontextually rich information such as texture and pattern\n[3],[36]. These state-of-the-art frameworks fail when ap-\nplied to diagrams, due to their contextually poor imagery.\nStandard One [28], Zero-Shot(ZS) [25] and Few-Shot(FS)\n[24] techniques, originally developed for small datasets\nstruggle to perform well on diagram-type imagery due to the\ndomain variation and huge non-overlap in representation\nacross these domains. Most technical diagrams, sketches\nand scientiﬁc drawings found in patents are binary images.\nThey lack signiﬁcant features such as texture, color and\ncontrast. Also, there is structural variation because of rigid\nbody transformations such as translation, rotation or per-\nspective variations (i.e. viewpoint change). Classical im-\nage processing and computer vision tools such as key-point\nmatching do not perform well given such transformations\n[12]. Typically, DL performance is robust against such rigid\nbody transformations when trained with data augmentation\ntechniques [21]. However, due to the lack of sufﬁciently\nlabeled patent image data available, DL models can easily\nover-ﬁt when trained on the small datasets typical of patent-\nrelated imagery.\nDomain generalization[16] and domain adaptation [18]\ntechniques are gaining popularity as methods to address this\narXiv:2004.10780v1  [cs.CV]  22 Apr 2020\ndata gap. Domain generalization provides a method to gen-\neralize the trained model over a broader dataset. Here, we\napply the concept of domain generalization by pre-training\nan unsupervised DL model on a large set of sketches gen-\nerated from natural images. We aim to achieve a general-\nized representation of the latent space with the edge maps\nand then project the target patent dataset to this domain-\ninvariant representation where differences between training\ndomains are minimized by incorporating the proper loss\nfunctions. We explore this method in both few-shot and\nzero-shot conditions where the model is able to general-\nize the matches and make similarity predictions based on\na small subset of the dataset for training. The model learns\nto recognize unseen matching pairs based on knowledge ac-\nquired from training of labeled similarity pairs.\nWhile most image retrieval methods and algorithms are\ndesigned around natural imagery, sketch-based retrieval\n[13] provides promise as a means to further image retrieval\nrelated to patents.\nOur methods further extend their ap-\nproach through the following steps:\n1. We use deep learning to generate sketches from natural\nimages (using existing natural image repositories for\nimage retrieval/image search/ image comparison).\n2. The large dataset of sketches created in (1) is used in\nthe training for image retrieval (because if the original\nnatural images match, we assume the corresponding\nsketches will match as well).\n3. The unsupervised deep learning model is trained on the\nsketches dataset for domain generalization.\n4. We use transfer learning (our small labeled subset of\ndata for image query is used at this stage) to complete\nthe image retrieval task based on the model trained in\n(3).\nWe show that even under zero-shot and one-shot conditions,\nthis framework surpasses classical retrieval frameworks for\nretrieval of similar binary images.\n2. Related work\nThe requirements of a patent image retrieval sys-\ntem include full-image, sub-image, category-based image,\nrotation-, scale- and afﬁne-invariant image searches, real-\ntime performance, scalability, on-line learning, and seman-\ntic level interpretation. Although the combined set of re-\nquirements present signiﬁcant challenges, we aim to ad-\ndress most of them in our approach.\nContent-based Image Retrieval (CBIR) makes use of low\nlevel visual features such as color, edges, texture, and shape\nto represent and retrieve images [1, 23, 36]. Relational\nskeletons [8], consider features such as relational angle\nand relational position between lines. The use of line seg-\nments in representing the image makes this approach sensi-\ntive to rigid body transformations such as rotations, trans-\nlations and scaling. The Edge Orientation AutoCorrel-\nogram (EOAC) approach used in the US patent retrieval\nsystem PATSEEK [27] claims to be insensitive to transla-\ntion and scaling; however the approach is computationally\nexpensive and the complexity grows with the feature vec-\ntor size. The use of user-deﬁned thresholds makes the ap-\nproach scale variant.\nThe Contour Description Matrix\n(CDM) approach [35] uses canny edge detection for ex-\ntracting contour information followed by converting each\nedge point to a polar coordinate system. While this ap-\nproach is invariant to rigid body transformations, the size\nof the CDM is dependent on image resolution and the re-\nsulting processes are inefﬁcient both computationally and\nmemory-wise. The Adaptive Hierarchical Density His-\ntogram (AHDH) method [22] along with the retrieval\nframework PATMEDIA [30] exploits both local and global\ncontent.\nIt uses both content-based (i.e image-based) as\nwell as concept-based (text-based) retrieval and claims joint\nretrieval using both text and image give better retrieval per-\nformance. The algorithm calculates the adaptive hierarchi-\ncal density histogram by computing the density of black\npixels on a white plane after reducing noise and normaliz-\ning at the pre-processing stage. The ADHD process is made\nto retrieve the images belonging to the same category in the\ndatabase and fails to retrieve similar images belonging to\na different category. Besides, one needs to also manually\nset two different thresholds to make the system scale invari-\nant which contradicts the idea of scale invariance. Fisher\nvectors based patent retrieval [2] uses Fisher vectors [19]\nto represent patent images as low level features. For a pair\nof images, a dot product of ﬁsher vectors is computed to\nmeasure the similarity between them. Similar to the ADHD\napproach, this approach does categorical based retrieval in-\nstead of similarity based retrieval.\n3. Datasets\nThe dataset used to train and test our model is taken from\na patent image search benchmark [29]. About 2000 sketch-\ntype images are manually extracted from approximately 300\npatents belonging to A43B and A63C IPC subclasses and\ncontain types of foot-wear or portions thereof (henceforth\ntermed ”concepts”). The dataset consists of 8 concepts for\nthis domain: cleat, ski boot, high heel, lacing closure, heel\nwith spring, tongue, toe cap and roller blade. The details for\nthe dataset can be found in [29]. The concepts dataset con-\ntains many dissimilarities within each class and is not suit-\nable to train a classiﬁer model to be used as a retrieval and\nmatching framework. An example of the concepts dataset is\nshown in Figure 1. To ground-truth this concept dataset, we\nevaluated image similarity through manual pairwise com-\nFigure 1. Two example images from each class of the Concept\nDataset.\nparisons made by three different non-experts and then de-\ntermined a median out of all similarities. The pairwise sim-\nilarity was quantiﬁed in the score range of 0-5 where, 5 -\nSame match, 4 - Slightly different, 3 - different perspective,\n2 - sub-image, 1 - slightly different sub-image and 0 - dis-\nsimilar.\nWe used the UT Zappos50K shoe dataset [34] and the\nGenerative Fashion dataset [20] to generate the sketches\nfor domain generalization. The ﬁrst dataset contains a total\nof 50K catalog images that were collected by Zappos.com\nwhile the second contains 293K high resolution fashion im-\nages. Using these two datasets, a retrieval performance was\nmeasured on the concepts dataset and fashion-MNIST [32]\ndataset respectively. The Fashion-MNIST dataset contains\n70k images of gray scale fashion products in 10 categories.\n4. Methods\nLabeled benchmark datasets of natural images are easily\naccessible online, but labeled datasets of patent diagrams\nare more limited. To generate the sketches/edge-maps in-\ntended for usage as our custom shoe training dataset, we\nprocess the collection of natural images through the use of\nthe Holistically Nested Convolutional Neural Nets(HCNN)\n[33]. We train a Variational Auto-Encoder (VAE) [9], an\nunsupervised representation learning model, to approximate\nthe distribution of the newly generated sketch dataset with\na multi-dimensional Gaussian distribution with ﬁnite mean\nand variance. Once the model learns the representation of\nthe data, we reuse this model via transfer learning on our\nsmall dataset for domain generalization [17]. The idea of\ndomain generalization is to learn from one or multiple train-\ning domains, to extract a domain-agnostic model which can\nbe applied to an unseen domain. We show that, on passing\nthe dataset through this learned model, it is able to achieve\na minimal clustering of similar matches of the dataset. We\naugment this model with extra blocks of neural nets to con-\nstruct a Siamese framework [10] for ﬁne tuning of the fea-\ntures on the latent space using triplet loss [5] that bring\nlikely samples closer and push dissimilar samples farther\naway. During the training of the Siamese framework, the\naugmented block is ﬁne tuned with a small subset of the\nsimilarity matrix from the entire dataset. At the test phase,\nthe samples that are used to query may/may not have been\npresent during the training. If the similarity metric corre-\nsponding to the queried sample was used during training,\nthen it is called Few-Shot/One-shot learning whereas if the\nno similarity metric corresponding to the queried sample\nwas used during training, then it is called Zero-Shot learn-\ning. This applicability of one shot and zero shot retrieval\nwith our framework relies on the knowledge gained during\nthe domain generalization followed by intelligent ﬁne tun-\ning of the features.\nStep1\n• Source domain natural images\nUse the UT Zappos50K (UT-Zap50K) i.e. a large shoe dataset consisting of 50,025 catalog \nmages comprising shoes, sandals, slippers, and boots.\nStep2\n• Edge map generation\nUse Holistically nested CNN for edge map generation.\nStep3\n• Domain Generalization(DG) with CNN\nTrain variational autoencoders(VAE) with the edge maps to learn the manifold of edge space \nfor domain generalization.\nStep4\n• Transfer learning with the  DG framework for cross domain similarity measure\nUse the trained VAE to compute latent features from the learned manifold for the patent \nshoes dataset.\nStep5\n• Siamese framework for latent features finetuning\nProject the VAE computed latent features into another manifold where latent features \ncorresponding to similar image move closer and dissimilar ones move distant.\nStep6\n• Similarity measure\nCompute the similarity matrix based on l2 distance of  the final  fine tuned latent features and \nalso perform K-nearest neighbor/spectral clustering in this latent space. \nStep7\n• Diagram image retrieval\nReturn the closest matches for any query image based on the similarity matrix or clustering \nneighborhood. \nFigure 2. Proposed model ﬂow-chart\nOnce we have ideal clustering of the samples in the latent\nspace via domain generalization and Siamese triplet loss\nbased ﬁne-tuning, we can use k-nearest neighbour(kNN)\nclustering to return the more closely matched pairs. This\ncould be incorporated into a retrieval tool to return the best\nset of matching images from the queried database.\nTo measure image similarity between two images X, Y\nwith corresponding pixels {x} ∈X and {y} ∈Y we use\nthe mathematical expression:\nS(X, Y ) =\nX\nx∈X\nX\ny∈Y\nK(x, y)\n(1)\n=\nX\nx∈X\nX\ny∈Y\nφ(x)T φ(y)\n(2)\n= ψ(X)T ψ(Y )\n(3)\nwhere K : R →R is the operator denoting pixel similarity.\nNote that Eq. 1 is equivalent to the Kernel factorization of\n[6] where image similarity is computed from features de-\nﬁned by the operator φ : R →R. Alternately, the similar-\nity is a dot product between the transformed features as de-\nscribed by the function ψ(). These three expressions (1), (2)\nand (3) dictate the process of feature extraction, feature en-\ncoding and aggregation and database indexing respectively.\n4.1. Holistically Nested Convolutional Neural Nets\n(HCN) for edge map generation\nWe exploit a state-of-the-art edge detection algorithm\ncalled Holistically Nested Convolutional Neural Nets\n(HCN) [33] to generate the edge maps. This model uti-\nlizes an end-to-end deep CNN framework for image to im-\nage prediction where the input and output are natural image\nand edge map respectively.\nThe model comprises a modiﬁed VGG16 network [11]\nwhere the ﬁnal pooling and fully connected layer is pruned.\nA deep supervision is established by connecting the side\noutput layer to the last convolutional layer in each stage,\nConv1 2, Conv2 2, Conv3 3, Conv4 3, and Conv5 3, re-\nspectively. A convolutional layer with a kernel size of 1 is\noperated on the output of each of the previous layer out-\nputs to compute side outputs which are all then connected\nto a ﬁnal fusion layer. The framework is trained with image\nsketch pairs and then tested with the shoes natural images.\nFigure 3 demonstrates the HCNN framework for edge map\ngeneration. The overall loss function is given by\nL(I, G, W, w) = Lside(I, G, W, w) + Lfuse(I, G, W, w)\n(4)\nWhere,\nLfuse = fusion layer loss function,\nLside = side output layer loss function,\nI = raw input image,\nG = ground truth binary segmentation map,\nW = collection of all other network layer parameters,\nw = w(1), w(2), ..w(M) : corresponding weights for each\nside output layer\n4.2. Domain generalization via Variational Auto En-\ncoder(VAE)\nWe adopt the concept of domain generalization for rep-\nresentation learning. This refers to the learning represen-\ntation of a domain dataset which makes it easier to extract\nsigniﬁcant information when building the matching frame-\nwork. This kind of representation learning is usually done\nin unsupervised settings by leveraging the potential of the\nexcess unlabeled dataset. Domain generalization tries to\napproximate the latent space/manifold over which the data\nof interest can be projected for categorization,clustering, or\nmatching.\nWe aim to combine a self supervised data representa-\ntion achieved via a pre-training process and ﬁne tune the\nmodel through transfer learning. We use a Variational Auto-\nEncoder (VAE) which is implemented on an explicit recon-\nstruction loop that focuses on achieving per-pixel recon-\nstruction. This VAE is trained for the purpose of unsuper-\nvised data representation and uses the encoder framework in\na Siamese framework to achieve a benchmark performance\nof image matching/retrieval via transfer learning.\nThe VAE builds generative models of complex distri-\nbutions of the sketched shoes dataset.\nIt uses a CNN\nbased function approximator to approximate an otherwise\nintractable function. The encoder encodes the input data\ninto the mean and variance statistics of the latent space\nand then samples data points from the Gaussian distribu-\ntion computed from the statistics. The decoder tries to re-\nconstruct the input data based on the sampled points. The\nframework trains in an end-to-end fashion where the objec-\ntive for the encoder is to generate the statistical encoding in\nsuch a way that the difference between the input image and\nthe reconstructed image are minimized.\nThe VAE is incorporated to generate an observation x\nfrom some hidden variable z such that p(z|x)(intractable\ndistribution) is approximated by another distribution q(z|x)\nvia approximate inference. With the objective to minimize\nthe KL divergence between these two distributions q(z|x)\nand p(z|x) and also minimize the reconstruction error, we\ncan write the overall loss function as\nθ(x) = KL(qφ(z|x)||p(z|x)) + L(pθ, qθ),\n(5)\nwhere\nL(pθ, qθ) = Eqφ(z|x)[logpθ(x, z) −logqθ(z|x)]\n(6)\nThe ﬁrst term represents the reconstruction error reconstruc-\ntion likelihood and the second term ensures that our learned\ndistribution q is similar to the true prior distribution p.\n4.3. Single shot/zero shot training for image re-\ntrieval using a Siamese framework\nOnce we have obtained the optimal latent representation\nof the latent space with the VAE, the encoder framework\ncan be used to extract the optimal latent representation\nfor our small dataset. To achieve a better cluster between\nsimilar image pairs, we implement triplet loss for a Siamese\nnetwork. To compute triplet loss, we consider an anchor i.e.\na reference from which the distance will be calculated to a\npositive sample (e.g. sample with a large paired similarity\nscore) and negative sample (e.g.\nsample with 0 paired\nsimilarity score). If we consider the anchor, positive and\nnegative sample images as xa\ni , xp\ni , xn\ni and corresponding\nembedding vectors as f a\ni , f p\ni , f n\ni , then the triplet loss\nltriplet is given as\nFigure 3. An overview of the Holistically Nested Edge Detection framework\nFigure 4.\nAn overview to the single-shot/zero-shot framework. i. Learning the latent representation of the 50K edge dataset with the\nVariational Auto-Encoder(VAE) for domain generalization. ii. Transfer learning of the trained Encoder block into the Siamese framework\nfor training with concept dataset. iii. t-distributed stochastic neighbor embedding(t-SNE) projection and similarity matrix computation\nbased on latent features generated by the trained Siamese block.\nltriplet = 1/N\nN\nX\ni=0\nmax(0, ||f a\ni −f p\ni ||2\n2 −||f a\ni −f n\ni ||2\n2 +α)\n(7)\nWhere N is the batch size and α is a constant factor.\nFigure 4 gives a broader overview of the implemented\nmethodology. In ﬁgure 5, we can see TSNE embedding\ncorresponding to latent features at different stages of the\nframework.\n5. Experiments and results\nWe used the Keras framework with a Tensorﬂow back-\nend to train and ﬁne tune the proposed model.\nAll ex-\nperiments were conducted on the Darwin cluster at the\n(a)\n(b)\n(c)\nFigure 5. (a)TSNE Projection of image space (b)TSNE representation of VAE output feature space (c)TNSE representation of Siamese\ntuned output feature space\nLos Alamos National Labs. This cluster is equipped with\nIntel(R) Xeon(R) Gold 6138 CPUs and 8 GeForce RTX\n2080Ti GPUs.\nThe dataset for training was constructed\nfrom the similarity matrix as triplets. The split was done\n60%(training+validation) and 40%(test). A relatively larger\ntest set was chosen to measure the performance of the model\nunder One-Shot and Zero-Shot conditions.\nThe implementation of the HCNN framework was bor-\nrowed from 1. Considering the pertained model, the model\nused to generate the pseudo-sketches from the natural image\ndatasets as discussed in the Dataset section.\nWe constructed the deep VAE from the standard VGG16\narchitecture by trimming the fully connected layer of\nVGG16 and augmenting the left model with a single layer\nVAE model to form an encoder and then combining with an\nequivalent decoder as shown in bottom of ﬁg 4. We vali-\ndated the VAE model with the following hyper-parameters:\na number of convolutional layers and encoding dimensions\nfor mean and variance. The conﬁguration including a 5-\nlayered Convolutional block and 128 encoding dimensions\nachieved the best reconstruction accuracy on the pseudo\nsketch datasets. We used the batch size of 64, Adam op-\ntimizer and learning rate of 0.001 for training the VAE.\nNext, the trained encoder model from the previous step\nwas augmented with a fully connected block(FC) to con-\nstruct a Siamese network whose triplet loss is computed\nfrom the set of three input images. We experimented with\n1https://github.com/moabitcoin/holy-edge\nadditional loss functions including the Contrastive loss [4],\nConstellation loss[14] and n-pair loss[26]. Triplet loss pro-\nvided the best similarity performance. We also investigated\nthe performance of the Siamese framework by i) training the\nFC while freezing the encoder block and ii) training both\nFC and encoder in end-to-end fashion. We observed that\nthe second approach achieves superior performance. This\nis due to a larger allowance for parameter learning for tun-\ning the feature space. From ﬁgure 5, considering differ-\nent groupings of similar items, the distance between sample\npoints decrease in the feature space from a to c as shown\nby the TSNE[?] projection. The data points are randomly\ndistributed in the original pixel space and the pre-trained\nVAE achieved an improved level of closeness between sim-\nilar samples without any knowledge of the patent dataset.\nFurthermore, with ﬁne tuning utilizing the Siamese frame-\nwork, the grouping of the samples based on their similarity\nis achieved.\nBefore any retrieval task, we construct a full similarity\nmatrix that encodes all computed pairwise similarities be-\ntween the elements of the dataset. Pairwise similarities can\nbe quantiﬁed as the cosine similarity or as the Euclidean\ndistance between the features collected at the output of the\nSiamese layer followed by normalization of values to fall\non a scale of 0-5. For the patent shoes concept dataset,\nthe similarity matrix is of the size 1042x1042. To query\nany image from the row of the matrix, we process the cor-\nresponding columns and sort them based on the score (i.e\nhighest to lowest score) and return the ﬁrst k elements of\nthe sorted array. Figure 6 is an example comparing ground\ntruth and Structural Similarity Index Measure(SSIM) [7]\nbased similarity matrices for the retrieval of a queried im-\nage. However, if one needs to perform retrieval for addi-\ntional images outside of the database, then the results of the\nquery will be based on a newly computed similarity matrix\nthat includes the newly added images. Figure 7 shows the\nground truth similarity matrix and predicted similarity ma-\ntrix based on both One-Shot and Zero-Shot conditions. Fig-\nure 8 demonstrates the retrieval results under One-Shot and\nZero-Shot conditions. To generalize our retrieval frame-\nFigure 6. i) The pairwise ground truth similarity matrix corre-\nsponding to the class ski boot. ii) Illustration of the database re-\nturns for the given query image. iii) Pairwise Similarity Matrix\nbased on the Structural Similarity Index Measure (SSIM) between\nthe images iv) Query returns based on the similarity matrix iii.\nwork to a dataset with no available baseline similarities,\nwe train using a binary similarity matrix with a score of\n0 for intra-class and a score of 1 for inter-class images.\nThis framework is likewise trained in zero-shot and one-\nshot conditions. The output features of the Siamese net-\nwork in the test dataset were clustered with k-NN to ob-\ntain the nearest k features given a test input. To measure\nretrieval performance we use the mean average precision\n(MAP) score computed as:\nMAP =\nPQ\nq=1 Ave(P(q))\nQ\n(8)\nover all retrievals. Here, Q is the number of queries and\nAve(P(q)) is the average of the precision score for each\nquery q.\nTo justify the effectiveness of the proposed approach,\nwe ﬁrst performed retrieval on variants of our model and\nmeasured the MAP results on the concept patent dataset.\nWhen the VAE was ﬁrst trained on natural images instead of\n(a) Ground truth Similarity ma-\ntrix\n(b)\nPredicted\nSimilarity\nmatrix(one-shot and zero-shot)\nFigure 7. Similarity matrix predictions. In ii) the similarity matrix\nis predicted by the Siamese framework in Few-Shot and Zero-Shot\nconditions.\nsketches followed by the training of the Siamese framework\non top of that model, we achieved an overall MAP of 0.75\nfor the ﬁrst 10 retrievals. We also tried training a randomly\ninitialized encoder block for the Siamese network and the\nMAP dropped to 0.6 for the same retrieval set. Implement-\ning our proposed approach, the overall MAP for the same\nretrieval set resulted in a MAP score of 0.83.\nFor baseline comparisons, we use the standard SSIM and\nGoldberg(GB) similarity [31] to compute the structural sim-\nilarity between all the image pairs on the test data partition\nand then return the set of k-NN images with the highest\nsimilarity measure. Table 1 summarizes the retrieval perfor-\nmance of the proposed framework in comparison to SSIM\nmethod for two benchmark datasets: 1) the concept shoe\nand 2) fashion-MNIST datasets. While SSIM is still used\nfor image matching, classiﬁcation and retrieval, GB is used\nas a tool for elastic search in larger datasets23. Here, re-\ntrieval scores are measured with MAP estimates for 10, 20\nand 30 retrieved images per image query. Figure 9 instead\nshows a more detailed trend of the retrieval performance as\na function of the number of retrieved images where our pro-\nposed framework outperforms SSIM and GB based retrieval\nin both datasets. Notice also that our method performance\ndecreases smoothly with increases in items retrieved irre-\nspective of image view-point and intensity variations.\nIn both Zero-Shot and One-Shot retrieval cases in the\nconcept dataset, performance of the proposed approach is\nsigniﬁcantly better than SSIM and GB which drops in per-\nformance exponentially with increase in the number of re-\ntrieved items. Also, note that performance is lower in the\nZero-Shot compared to the One-Shot framework caused\nmainly because of the additional knowledge acquired by the\nmodel with regard to the query set in the One-Shot case.\nAlso, note that for fashion-MNIST similar retrieval perfor-\n2https://github.com/EdjoLabs/image-match\n3https://github.com/dsys/match\nmances are achieved in both proposed and SSIM methods.\nThis is mainly due to high intra-class similarity, the lack\nof multiple viewpoints and the binary scoring used in this\ncase. In contrast, for the concept patent dataset, the similar-\nity scoring which ranges from 0-5 complicates the retrieval\nprocess for any other models that are not trained with such\na scoring scheme. In both datasets, GB fails to perform well\nfor the retrieval in binary and gray-scale images in contrast\nto its efﬁcacy for retrieval in large RGB datasets.\n(a) Ground truth Similarity ma-\ntrix based retrieval.\n(b) Predicted Similarity matrix\nbased retrieval(one shot).\n(c) Ground truth Similarity ma-\ntrix based retrieval.\n(d) Predicted Similarity matrix\nbased retrieval (zero shot).\nFigure 8. Predicted Similarity matrix based retrieval\n6. Conclusion and future work\nWe have demonstrated that domain generalization with\nthe edge maps-based inductive short term learning and la-\ntent space ﬁne-tuning based transductive long term learning\naids to improve retrieval performance. This two-step pro-\ncess helps to ﬁne tune the feature space by appropriately\nlearning the data manifold. This provides a more meaning-\nful structure of technical diagrams which naive image pro-\ncessing/computer vision techniques are unable to extract.\nAlso, we have demonstrated image retrieval using the do-\nmain generalization concept on shoe patent images in One-\nShot and Zero-Shot settings. We can extend this frame-\nwork to other scientiﬁc drawings and patent images by pre-\ntraining the framework with related datasets.\nTo construct the similarity matrix to perform retrieval,\nwe ﬁrst computed the Euclidean distance/ Cosine similarity\nbetween the learning based deep features. These deep fea-\ntures were obtained by transfer learning of a deep learning\nmodel for patent images. It was observed that the learned\ndeep features from the supervised classiﬁcation based task\nFigure 9. mean average precision(mAP) analysis on Concept\nPatent dataset(One-Shot and Zero-Shot) and Fashion dataset\nor unsupervised latent representation did not reﬂect well on\nthe retrieval performance as the learned features were more\nbiased towards the learning objective the framework was\ntrained for.\nIn our approach, we build a pipeline where\nwe ﬁne tune the features obtained with transfer learning\nwith the objective of achieving an improved similarity mea-\nsure between the features corresponding to different im-\nages. This resulted in a better retrieval performance. Also,\nbecause of the difﬁculty involved in quantifying the re-\ntrieval performance on training the deep learning model,\nwe instead use a similarity metric implemented via a scor-\ning measure of similarity between image pairs to train the\nframework.\nIn the future, we plan to implement a Bidirectional Gen-\nerative Adversarial Networks (BiGANs) to learn generative\nmodels mapping from simple latent distributions to arbitrar-\nily complex data distributions. This framework would be\nable to perform domain generalization across more broad\nimage domains including natural images, sketches, scien-\ntiﬁc drawings and patent images.\n7. Acknowledgments\nResearch supported by the Laboratory Directed Re-\nsearch and Development program of Los Alamos National\nLaboratory (LANL) under project number 20200041ER.\nMB supported by the LANL Applied Machine Learning\nSummer Research Fellowship(2019).\nReferences\n[1] S. Antani, R. Kasturi, and R. Jain. A survey on the use\nof pattern recognition methods for abstraction, index-\ning and retrieval of images and video. Pattern recog-\nnition, 35(4):945–965, 2002. 2\n[2] G. Csurka, J.-M. Renders, and G. Jacquet.\nXrce’s\nparticipation at patent image classiﬁcation and image-\nbased patent retrieval tasks of the clef-ip 2011.\nIn\nTable 1. Retrieval Performances\nDataset\nmAP@10\nmAP@20\nmap@30\nConcept\n0.816(ZS on Proposed)\n0.667(ZS on Proposed)\n0.581(ZS on Proposed)\n0.842(OS on proposed)\n0.721(OS on Proposed)\n0.643(OS on Proposed)\n0.348(GB)\n0.280(GB)\n0.247(GB)\n0.273(SSIM)\n0.216(SSIM)\n0.193(SSIM)\nFashion-MNIST\n0.86(Proposed )\n0.81(Proposed)\n0.77(Proposed)\n0.807(SSIM)\n0.770(SSIM)\n0.750(SSIM)\n0.702(GB)\n0.650(GB)\n0.625(GB)\nCLEF (Notebook Papers/Labs/Workshop), volume 2,\n2011. 2\n[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and\nL. Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In 2009 IEEE conference on computer\nvision and pattern recognition, pages 248–255. Ieee,\n2009. 1\n[4] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality\nreduction by learning an invariant mapping. In 2006\nIEEE Computer Society Conference on Computer Vi-\nsion and Pattern Recognition (CVPR’06), volume 2,\npages 1735–1742. IEEE, 2006. 6\n[5] E. Hoffer and N. Ailon.\nDeep metric learning us-\ning triplet network.\nIn International Workshop on\nSimilarity-Based Pattern Recognition, pages 84–92.\nSpringer, 2015. 3\n[6] T. Hofmann, B. Sch¨olkopf, and A. J. Smola. Kernel\nmethods in machine learning. The annals of statistics,\npages 1171–1220, 2008. 3\n[7] A. Hore and D. Ziou.\nImage quality metrics: Psnr\nvs. ssim. In 2010 20th International Conference on\nPattern Recognition, pages 2366–2369. IEEE, 2010.\n6\n[8] B. Huet, N. J. Kern, G. Guarascio, and B. Merialdo.\nRelational skeletons for retrieval in patent drawings.\nIn Proceedings 2001 International Conference on Im-\nage Processing (Cat. No. 01CH37205), volume 2,\npages 737–740. IEEE, 2001. 2\n[9] D. P. Kingma and M. Welling. Auto-encoding varia-\ntional bayes. In ICLR, 2014. 3\n[10] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese\nneural networks for one-shot image recognition. In\nICML deep learning workshop, volume 2, 2015. 3\n[11] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Im-\nagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in Neural Information Process-\ning Systems, pages 1097–1105, 2012. 4\n[12] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning.\nnature, 521(7553):436, 2015. 1\n[13] L. Liu, F. Shen, Y. Shen, X. Liu, and L. Shao. Deep\nsketch hashing: Fast free-hand sketch-based image re-\ntrieval.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2862–\n2871, 2017. 2\n[14] A. Medela and A. Picon.\nConstellation loss: Im-\nproving the efﬁciency of deep metric learning loss\nfunctions for optimal embedding.\narXiv preprint\narXiv:1905.10675, 2019. 6\n[15] M. Mogharrebi, M. C. Ang, A. S. Prabuwono,\nA. Aghamohammadi, and K. W. Ng. Retrieval system\nfor patent images. Procedia Technology, 11:912–918,\n2013. 1\n[16] K. Muandet, D. Balduzzi, and B. Sch¨olkopf. Domain\ngeneralization via invariant feature representation. In\nInternational Conference on Machine Learning, pages\n10–18, 2013. 1\n[17] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learn-\ning and transferring mid-level image representations\nusing convolutional neural networks. In Computer Vi-\nsion and Pattern Recognition (CVPR), IEEE Confer-\nence on, pages 1717–1724, 2014. 3\n[18] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang.\nDomain adaptation via transfer component analysis.\nIEEE Transactions on Neural Networks, 22(2):199–\n210, 2010. 1\n[19] F. Perronnin and C. Dance. Fisher kernels on visual\nvocabularies for image categorization. In 2007 IEEE\nconference on computer vision and pattern recogni-\ntion, pages 1–8. IEEE, 2007. 2\n[20] N.\nRostamzadeh,\nS.\nHosseini,\nT.\nBoquet,\nW. Stokowiec, Y. Zhang, C. Jauvin, and C. Pal.\nFashion-gen:\nThe generative fashion dataset and\nchallenge. arXiv preprint arXiv:1806.08317, 2018. 3\n[21] C. Shorten and T. M. Khoshgoftaar. A survey on im-\nage data augmentation for deep learning. Journal of\nBig Data, 6(1):60, 2019. 1\n[22] P. Sidiropoulos, S. Vrochidis, and I. Kompatsiaris.\nContent-based binary image retrieval using the adap-\ntive hierarchical density histogram. Pattern Recogni-\ntion, 44(4):739–750, 2011. 2\n[23] A. W. Smeulders, M. Worring, S. Santini, A. Gupta,\nand R. Jain. Content-based image retrieval at the end\nof the early years. IEEE Transactions on Pattern Anal-\nysis & Machine Intelligence, (12):1349–1380, 2000. 2\n[24] J. Snell, K. Swersky, and R. Zemel. Prototypical net-\nworks for few-shot learning.\nIn Advances in neu-\nral information processing systems, pages 4077–4087,\n2017. 1\n[25] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng.\nZero-shot learning through cross-modal transfer. In\nAdvances in neural information processing systems,\npages 935–943, 2013. 1\n[26] K. Sohn. Improved deep metric learning with multi-\nclass n-pair loss objective.\nIn Advances in neural\ninformation processing systems, pages 1857–1865,\n2016. 6\n[27] A. Tiwari and V. Bansal. Patseek: Content based im-\nage retrieval system for patent database.\nIn ICEB,\npages 1167–1171, 2004. 2\n[28] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra,\net al. Matching networks for one shot learning. In\nAdvances in neural information processing systems,\npages 3630–3638, 2016. 1\n[29] S. Vrochidis, A. Moumtzidou, and I. Kompatsiaris.\nConcept-based patent image retrieval. World Patent\nInformation, 34(4):292–303, 2012. 1, 2\n[30] S. Vrochidis, A. Moumtzidou, G. Ypma, and I. Kom-\npatsiaris. Patmedia: augmenting patent search with\ncontent-based image retrieval.\nIn Information Re-\ntrieval Facility Conference, pages 109–112. Springer,\n2012. 1, 2\n[31] H. C. Wong, M. Bern, and D. Goldberg. An image\nsignature for any kind of image. In Proceedings. Inter-\nnational Conference on Image Processing, volume 1,\npages I–I. IEEE, 2002. 7\n[32] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a\nnovel image dataset for benchmarking machine learn-\ning algorithms.\narXiv preprint arXiv:1708.07747,\n2017. 3\n[33] S. Xie and Z. Tu. Holistically-nested edge detection.\nIn Proceedings of the IEEE International Conference\non Computer Vision, 2015. 3, 4\n[34] A. Yu and K. Grauman. Fine-grained visual compar-\nisons with local learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, pages 192–199, 2014. 3\n[35] Z. Zhiyuan, Z. Juan, and X. Bin.\nAn outward-\nappearance patent-image retrieval approach based on\nthe contour-description matrix. In 2007 Japan-China\nJoint Workshop on Frontier of Computer Science and\nTechnology (FCST 2007), pages 86–89. IEEE, 2007.\n2\n[36] X. S. Zhou and T. S. Huang. Relevance feedback in\nimage retrieval: A comprehensive review. Multimedia\nsystems, 8(6):536–544, 2003. 1, 2\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-04-22",
  "updated": "2020-04-22"
}