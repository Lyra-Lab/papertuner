{
  "id": "http://arxiv.org/abs/2004.11020v1",
  "title": "SimUSR: A Simple but Strong Baseline for Unsupervised Image Super-resolution",
  "authors": [
    "Namhyuk Ahn",
    "Jaejun Yoo",
    "Kyung-Ah Sohn"
  ],
  "abstract": "In this paper, we tackle a fully unsupervised super-resolution problem, i.e.,\nneither paired images nor ground truth HR images. We assume that low resolution\n(LR) images are relatively easy to collect compared to high resolution (HR)\nimages. By allowing multiple LR images, we build a set of pseudo pairs by\ndenoising and downsampling LR images and cast the original unsupervised problem\ninto a supervised learning problem but in one level lower. Though this line of\nstudy is easy to think of and thus should have been investigated prior to any\ncomplicated unsupervised methods, surprisingly, there are currently none. Even\nmore, we show that this simple method outperforms the state-of-the-art\nunsupervised method with a dramatically shorter latency at runtime, and\nsignificantly reduces the gap to the HR supervised models. We submitted our\nmethod in NTIRE 2020 super-resolution challenge and won 1st in PSNR, 2nd in\nSSIM, and 13th in LPIPS. This simple method should be used as the baseline to\nbeat in the future, especially when multiple LR images are allowed during the\ntraining phase. However, even in the zero-shot condition, we argue that this\nmethod can serve as a useful baseline to see the gap between supervised and\nunsupervised frameworks.",
  "text": "SimUSR: A Simple but Strong Baseline for Unsupervised Image Super-resolution\nNamhyuk Ahn†∗\nAjou University\naa0dfg@ajou.ac.kr\nJaejun Yoo∗\nEPFL\njaejun.yoo88@gmail.com\nKyung-Ah Sohn‡\nAjou University\nkasohn@ajou.ac.kr\nAbstract\nIn this paper, we tackle a fully unsupervised super-\nresolution problem, i.e., neither paired images nor ground\ntruth HR images. We assume that low resolution (LR) im-\nages are relatively easy to collect compared to high resolu-\ntion (HR) images. By allowing multiple LR images, we build\na set of pseudo pairs by denoising and downsampling LR\nimages and cast the original unsupervised problem into a\nsupervised learning problem but in one level lower. Though\nthis line of study is easy to think of and thus should have\nbeen investigated prior to any complicated unsupervised\nmethods, surprisingly, there are currently none. Even more,\nwe show that this simple method outperforms the state-of-\nthe-art unsupervised method with a dramatically shorter la-\ntency at runtime, and signiﬁcantly reduces the gap to the\nHR supervised models. We submitted our method in NTIRE\n2020 super-resolution challenge and won 1st in PSNR, 2nd\nin SSIM, and 13th in LPIPS. This simple method should be\nused as the baseline to beat in the future, especially when\nmultiple LR images are allowed during the training phase.\nHowever, even in the zero-shot condition, we argue that this\nmethod can serve as a useful baseline to see the gap be-\ntween supervised and unsupervised frameworks.\n1. Introduction\nSingle image super-resolution (SISR) is a longstanding\ntask in computer vision area, which focuses on recovering\na high-resolution (HR) image from a single low-resolution\n(LR) image. Since this task has to solve a one-to-many map-\nping problem, building an effective SISR method is chal-\nlenging. Despite the difﬁculties, thanks to the developments\nof deep learning and large-scale datasets with high qual-\nity images, many learning-based SR methods [2, 5, 12, 14,\n16, 30] have recently shown prominent performance gains\nover traditional optimization-based approaches [24]. With\nthe power of the deep neural network, they enjoy the dra-\n† Most work was done while in NAVER Corp.\n* indicates equal contribution. ‡ indicates corresponding author.\nOriginal\nZSSR+BM3D\nOurs+BM3D\nGround Truth\nFigure 1. NTIRE 2020 Super-resolution challenge (×4) [19] im-\nages (1st column). ZSSR [22] with BM3D [4] results (2nd col-\numn). Our SimUSR with BM3D results (3rd column). Ground\ntruth HR image (4th column) is not available in this setup. Our\nmethod achieves superior SR performance for all the cases.\nmatic boost of the SR performance by stacking a lot of lay-\ners [30] or widen the network [16].\nWhile most of the deep learning-based SR methods\nheavily rely on a large number of image pairs (super-\nvised SR), unfortunately, such a large-scale and high qual-\nity dataset is not always accessible, especially when we deal\nwith a real environment. A few recent works have proposed\na workaround solving an unpaired SR task [7, 17, 28]. Since\nthis setup does not require full supervision of LR and HR\nimage pairs but images from each domain, it is a more real-\nistic scenario for many real-world applications. However, in\nmany applications (e.g., medical image), gathering HR (or\nclean) images itself requires a lot of efforts or sometimes\neven impossible.\n1\narXiv:2004.11020v1  [cs.CV]  23 Apr 2020\nTo address this, Shocher et al. [22] have proposed a fully\nunsupervised method, called zero-shot super-resolution\n(ZSSR), which performs both training and testing at run-\ntime using only a single LR test image. By learning a map-\nping from a scale-down version of the LR image to itself,\nZSSR learns to exploit internal image statistics to super-\nresolve the given image. It outperformed the previous inter-\nnal SR methods [11] in a huge margin with a high ﬂexibility\nbecause the model can easily be adapted to any unknown\ndegradation or downsample kernel.\nHowever, ZSSR has several drawbacks. 1) It requires an\nonline optimization procedure at runtime. Since it needs at\nleast 1K steps (both forward and backward propagation),\nthe latency of the ZSSR is extremely high. 2) It is difﬁcult to\nbeneﬁt from a large capacity network. Because ZSSR has to\nperform online training on a single image, the model should\nbe able to quickly adapt to the given image while avoid-\ning the overﬁtting issue, which limits ZSSR to use a shal-\nlow network architecture. 3) When noises are present in LR\nimages, ZSSR shows deteriorated performance because the\nmodel can never learn to denoise, and even after adding a\ndenoising module, it suffers from its restrictive framework.\n4) It does not utilize any prior information at all, which is\nan excessively restrictive constraint. While collecting LR-\nHR image pairs are difﬁcult, acquiring LR images only is\nrelatively easy and feasible in many real-world scenarios.\nSince the internal-based SR methods generally show worse\nSR performance than the external-based models, it is desir-\nable to exploit every available prior information as long as\nit stays in the unsupervised regime.\nTo mitigate these limitations, we propose a simple base-\nline for unsupervised SR (SimUSR) that relaxes the ZSSR\ninto a supervised setting. Instead of using a single image,\nour SimUSR make pseudo-pairs using multiple LR images.\nTo correctly guide the model, we employ BM3D [4] to re-\nmove noises from the LR images when preparing the pairs.\nThough these are very simple corrections, they bring several\nbeneﬁts: our framework can now exploit every beneﬁt of su-\npervised learning. Thanks to this pseudo-supervision, am-\nple prior information enables a model to reduce the perfor-\nmance gap between the unsupervised (only LR is available)\nand the supervised setting (HR is available). SimUSR can\nutilize recently developed network architectures and tech-\nniques that provide huge performance gains (Figure 1). In\naddition, since the online training is not necessary, SimUSR\ncan signiﬁcantly reduce its runtime latency as well. The dif-\nferences of the supervised SR, ZSSR, and SimUSR are sum-\nmarized in Figure 2.\nWe argue that our assumption is fairly practical while\nstill remaining under the unsupervised learning setup; we\nonly use LR images. Our approach is meaningful in that it\ninvestigates the blind spot of the ﬁeld that should have been\naddressed but overlooked.\nOur contributions are summarized as follows:\n1. We propose a simple but strong baseline for unsuper-\nvised SR (SimUSR) (NTIRE Challenge: 1st in PSNR,\n2nd in SSIM, and 13th in LPIPS).\n2. By casting the unsupervised SR into the supervised\nSR, SimUSR provides stable ofﬂine learning with a\ndramatically decreased latency at runtime.\n3. We provide a comprehensive analysis on the effect of\nusing various SR networks and techniques. By taking\nthe state-of-the-art techniques and SR networks as our\nbackbone, our method shows further enhancements.\n2. Related work\nSupervised Super-resolution. Recently, deep learning-\nbased super-resolution models [2, 5, 12, 16, 30] have shown\na dramatic leap over the traditional algorithms [24]. Most of\nthe successful deep SR approaches fall into the supervised\nsetting, where a network is trained on an external dataset\nhaving low- and high-resolution pairs. As long as the size\nof the dataset and the network capacity are large enough, it\nis well known that the supervised approach provides a better\nchance to enhance the SR performance [16, 30]. However, it\nis also true that their performance and generalizability dete-\nriorate dramatically when the dataset size is small and when\nthere exists mismatch between training and testing environ-\nments [27, 6]. To mitigate this issue, recent approaches fo-\ncus on blind SR, which assumes that there exist LR and HR\npairs but with unknown degradation and downsample ker-\nnel [9]. Unlike the aboves, our proposed method can train a\nnetwork even when there are no LR and HR pairs.\nUnpaired Super-resolution. A few recent works have ad-\ndressed an unpaired SR task [7, 17, 28] that does not assume\na paired setting. Since this setup does not require a full su-\npervision, it is a more realistic scenario for many real-world\napplications. Most of the methods employ generative adver-\nsarial framework [8] so that a generator learns to map HR\nimages into their distorted LR version. Using this gener-\nated pairs, they train an SR network in a supervised setting.\nHowever, in practice, there are cases where HR images are\nnot even available, which requires a fully unsupervised SR.\nUnsupervised Super-resolution. Though the unpaired SR\nis sometimes considered as an unsupervised SR, we ﬁrst\nclarify that unsupervised SR should strictly denote the task\nwithout any supervision neither paired images nor HR im-\nages. Under this deﬁnition, there are only a handful of stud-\nies [25, 10, 22] and zero-shot super-resolution (ZSSR) [22]\nfalls into this. ZSSR uses LR sons that are downsampled im-\nages of the given LR test image (a.k.a LR father). Using this\npseudo pairs, they train the model in a supervised manner\nbut only exploiting the internal statistics of the given test\n2\nfθ\nILR\nISR\nfθ\nILR\nISR\nfθ\nI′ LR\nI′ SR\nI′ HR\n…\nLR\n…\nfθ\nILR\nISR\nIHR\nX\nfθ\nILR\nISR\n…\nLR\n…\nLR\nHR\n…\n{(ILR1, IHR1), …, (ILRN, IHRN)}\n{ILR1, …, ILRN}\nILR\nTraining \ndataset\nOffline\nOnline\nSupervised SR\nZSSR\nSimUSR (Ours)\nfθ\nI′ LR\nI′ SR\nI′ HR\nFigure 2. Schematic comparison of the supervised SR, ZSSR [22], and our SimUSR. We analyze current SR approaches in terms of the\ntraining dataset, ofﬂine phase, and online phase. The ofﬂine phase is operated beforehand the user’s inference request (i.e. training process\nof the supervised SR). Online phase denotes runtime. (1st row) While supervised SR requires the LR-HR pairs, ZSSR and SimUSR use\nLR images only, making them more applicable to the real-world SR scenarios. ZSSR utilizes only a single test LR image and performs both\noptimization and inference at runtime. (2nd, 3rd rows) On the other hand, SimUSR exploits additional LR images and follows a similar\nprocedure to the supervised setup, where the model is ﬁrst trained ofﬂine and inference is done online.\nimage. Because every procedure is performed at runtime,\nZSSR suffered from high latency. To overcome this, Soh et\nal. [23] have proposed meta-transfer ZSSR (MZSR). They\nadded a meta-transfer learning phase to exploit the infor-\nmation of the external dataset, which decreased the number\nof the steps required at runtime. Still, to quickly optimize\nthe network, MZSR was limited to use a simple 8-layer\nnetwork. Unlike the aforementioned methods, our SimUSR\ncan beneﬁt from the larger capacities of recently developed\nSR models and short latency at runtime by removing the on-\nline update phase, while remaining in the fully unsupervised\nregime in that it only utilizes the LR images.\n3. Zero-shot super-resolution\nThe zero-shot super-resolution (ZSSR) [22] tackles the\nfully unsupervised SR task, where only low-resolution im-\nages (ILR) are available. To do that, ZSSR performs both\noptimization and inference at runtime using a single test im-\nage (Figure 2). During the online optimizing phase, they use\nan test input image (ILR) as LR father (Ifather\nLR\n) and gen-\nerates LR son (Ison\nLR ) by downsampling LR father with an\narbitrary kernel k. By doing so, they create pseudo-pair\n(I′\nLR, I′\nHR) = (Ison\nLR , Ifather\nLR\n),\nwhere Ison\nLR = ILR ↓s,k and Ifather\nLR\n= ILR. Here, ↓s,k\ndenotes a downsampling operation with an arbitrary kernel\nk and scale factor s.\nWith this pseudo-pair, optimizing a SR model now be-\ncomes a standard supervised setting. The core idea of ZSSR\nis to make the model learn internal image-speciﬁc statistics\nof a given test image during the online training. For infer-\nence, it generates ﬁnal SR output (ISR) by feeding ILR to\nthe trained image-speciﬁc network.\n4. Our method\nWe introduce a simple baseline for a fully unsupervised\nsuper-resolution task (SimUSR). Similar to the ZSSR [22],\nour method does not use any HR images for training the\nnetwork. However, we slightly relax the constraint of ZSSR\nand assumes that it is relatively easy to collect the LR im-\nages, {ILR1, . . . , ILRN }, where N is the number of LR im-\nages. This allows our method to exploit multiple pseudo-\npairs:\n(I′\nLRk, I′\nHRk) = (Ison\nLRk, Ifather\nLRk\n), for k = 1 . . . N.\nHere, we generate Ison\nLR and Ifather\nLR\nwith the same protocol\nthat used in ZSSR.\n3\nTable 1. Quantitative comparison (PSNR/SSIM) on the bicubic SR (scale ×4) benchmark datasets. We boldface the best performance of\nboth supervised SR and ours.\nDataset\nSupervised SR\nZSSR\nSimUSR (Ours)\nCARN\nRCAN\nEDSR\nCARN\nRCAN\nEDSR\nSet5\n32.13/0.8937\n32.63/0.9002\n32.46/0.8968\n31.13/0.8796\n31.94/0.8908\n32.40/0.8962\n32.37/0.8955\nSet14\n28.60/0.7806\n28.87/0.7889\n28.80/0.7876\n28.01/0.7651\n28.44/0.7786\n28.71/0.7860\n28.70/0.7855\nB100\n27.58/0.7349\n27.77/0.7436\n27.71/0.7420\n27.12/0.7211\n27.49/0.7324\n27.68/0.7394\n27.66/0.7389\nUrban100\n26.07/0.7837\n26.82/0.8087\n26.64/0.8033\n24.61/0.7282\n25.70/0.7740\n26.45/0.7986\n26.31/0.7940\nManga109\n-\n31.22/0.9173\n31.02/0.9148\n27.84/0.8657\n30.03/0.9014\n30.73/0.9124\n30.59/0.9107\nTable 2. Quantitative comparison (PSNR) on SR (scale ×4) task\nwith mixture of augmentation (MoA) [27]. We show the effect of\nMoA on our SimUSR and supervised SR (SSR) model. Note that\nSSR results are provided to show the improved upper limit again.\nType\nModel\nSet14\nUrban\nManga\nSimUSR\nRCAN\n28.80\n26.60\n30.85\n(+MoA)\nSSR\nRCAN\n28.92\n26.93\n31.46\n(+MoA)\nThough we now lose the generalizability over a single\ntest image, compared to the cost of the relaxation, the ben-\neﬁts are very huge: we can fully enjoy the advantages of\nthe supervised learning framework. More speciﬁcally, using\nthese multiple pairs, we can now train a network ofﬂine and\nperform inference online as any supervised model usually\ndoes. Our method can be implemented by a simple modi-\nﬁcation of the supervised SR approach, it gives high ﬂexi-\nbility and extensibility. For example, unlike the ZSSR and\nMZSR [23], which inevitably use shallow networks, we can\nuse any off-the-shelf SR network and technique available,\nsuch as data augmentation [27] (Section 5.1). In addition,\nsince the runtime of our SimUSR only depends on the net-\nwork’s inference speed, this also gives a huge acceleration\nin terms of the runtime latency (Section 5.3).\n5. Experiments\nIn this section, we describe our experimental settings and\ncompare the performance of our method with the supervised\nSR models and the ZSSR [22]. In Section 5.1, we analyze\nhow much our SimUSR improves the performance over the\nZSSR and how far we are left to reach the supervised per-\nformance. Then, in Section 5.2, we apply our method on the\nNTIRE 2020 SR dataset [19].\nBaselines. We use ZSSR [22] as our major baseline method.\nHowever, since ZSSR and SimUSR are not designed to han-\ndle noisy cases, we attach BM3D [4] as a pre-processing\nstep. For our SimUSR, we use various models as our\nbackbone network. We use three SR models: CARN [2],\nRCAN [30] and EDSR [16]. Each of the model have differ-\nent numbers of parameters from 1.1M to 43.2M (million).\nDataset and evaluation. We use the DF2K [1, 16] dataset\nfor the bicubic degradation SR task. However, unlike the\nLim et al. [16], we only use the LR images when we train\nthe models. For evaluation, we use Set5 [3], Set14 [26],\nB100 [20], Urban100 [11], and Manga109 [21] for bicubic\nSR task. To evaluate our method on the real-world SR task,\nwe use NTIRE 2020 dataset [19]. This dataset is generated\nwith unknown degradation operation to simulate the realis-\ntic image processing artifacts. In addition, only non-paired\nLR and HR images are given so that the model should be\ntrained via unsupervised setup. Same as DF2K, we do not\nuse any of HR images at the training phase. We use PSNR\nand SSIM to measure the performance. We calculate both\nmetrics on RGB channels for the NTIRE dataset while only\nusing the Y channel for the bicubic SR task.\n5.1. Bicubic super-resolution\nHere, we compare SimUSR with the ZSSR and the su-\npervised SR models. Though the classical bicubic SR task\nis not our main task, it provides a testbed to analyze every\nmodel simultaneously. This also shows how much gap there\nexists between the supervised and unsupervised frame-\nworks. For fair comparison, we report our performance us-\ning different SR networks as our backbone (CARN [2],\nRCAN [30], and EDSR [16]). The quantitative comparison\non various benchmark dataset is shown in Table 1. Exploit-\ning the additional LR images, our SimUSR shows large im-\nprovements over the ZSSR in every case.\nMore interestingly, by exploiting the recent develop-\nment of supervised SR techniques, such as data augmen-\ntation, SimUSR further reduces the gap toward the super-\nvised learning models (Table 2). Note that, while the super-\nvised models can use HR images as ground truth, SimUSR\nonly uses LR images. Therefore, the model should gener-\nalize over the learned scale and pixel distributions. Toward\nthis, we used mixture of augmentation (MoA) [27], which\nis a recent data augmentation method for low-level vision\ntask. MoA is known to not only improve the performance\nbut also enhance the generalization power of the model. By\nemploying the MoA, which ZSSR does not beneﬁt from (re-\nsults not shown), our performance again increases by 0.09\ndB (Set14), 0.15 dB (Urban100), and 0.12 dB (Manga109),\n4\nLR \n(22.57/0.6563)\nOurs + CARN (Δ) \n(25.68/0.7621)\nSet14: Zebra\nZSSR (Δ) \n(24.61/0.7404)\nHR \n(PSNR/SSIM)\nOurs + RCAN (Δ) \n(26.17/0.7728)\nOurs + EDSR (Δ) \n(26.10/0.7714)\nLR \n(23.53/0.7224)\nOurs + CARN (Δ) \n(24.88/0.7836)\nB100: image003\nZSSR (Δ) \n(24.18/0.7571)\nHR \n(PSNR/SSIM)\nOurs + RCAN (Δ) \n(25.32/0.7980)\nOurs + EDSR (Δ) \n(25.23/0.7957)\nLR \n(19.77/0.6511)\nOurs + CARN (Δ) \n(21.85/0.8068)\nUrban100: image004\nZSSR (Δ) \n(21.20/0.7580)\nHR \n(PSNR/SSIM)\nOurs + RCAN (Δ) \n(23.24/0.8507)\nOurs + EDSR (Δ) \n(23.39/0.8493)\nLR \n(15.19/0.4105)\nOurs + CARN (Δ) \n(17.11/0.6043)\nUrban100: image092\nZSSR (Δ) \n(15.47/0.4621)\nHR \n(PSNR/SSIM)\nOurs + RCAN (Δ) \n(17.93/0.6586)\nOurs + EDSR (Δ) \n(17.02/0.6165)\nLR \n(21.09/0.8155)\nOurs + CARN (Δ) \n(26.74/0.9383)\nManga109: Hamlet\nZSSR (Δ) \n(24.83/0.9155)\nHR \n(PSNR/SSIM)\nOurs + RCAN (Δ) \n(26.87/0.9388)\nOurs + EDSR (Δ) \n(26.87/0.9389)\nFigure 3. Qualitative comparison of using our proposed method on the various benchmark datasets which are generated by the Bicubic\ndownsample kernel. ∆is the absolute residual intensity map between the network output and the ground-truth HR image.\n5\nTable 3. Quantitative comparison (PSNR/SSIM) on the NTIRE\n2020 dataset [19]. We analyze the effect of denoising (w/ BM3D)\nand afﬁne transformations (w/o Afﬁne). We also analyze the ad-\nvantage of applying SimUSR.\nMethod\nw/\nw/o\nPSNR / SSIM\nBM3D\nAfﬁne\nZSSR\n25.82 / 0.6898\n✓\n26.45 / 0.7320\n✓\n✓\n26.55 / 0.7344\nSimUSR+CARN\n✓\n✓\n27.19 / 0.7520\nSimUSR+RCAN\n✓\n✓\n27.24 / 0.7550\nSimUSR+EDSR\n✓\n✓\n27.28 / 0.7554\nwhich are upto 3.63 dB (Manga109) improvements over the\nZSSR. Therefore, from now on, we use MoA with SimUSR\nby default unless it is speciﬁed.\nThe qualitative results also shows the superior results\nof SimUSR over the ZSSR (Figure 3). In all the cases,\nSimUSR beneﬁts from the increased performance by us-\ning external LR images. This tendency is clearly shown in\nthe residual intensity map between the SR and HR image.\nFor example, our method (with any backbone) successfully\nrestores the replicating patterns (1st, 3rd, and 4th rows)\nwhile ZSSR has difﬁculty of recovering distortions. Note\nthat ZSSR is supposed to better learn the internal statistics\nby repeatedly seeing the same LR image patches, which is\nin principle good at recovering replicating patterns.\n5.2. Real-world super-resolution\nIn this section, we compare ZSSR [22] and our method\non the NTIRE 2020 dataset [19]. We found two observa-\ntions that 1) ZSSR suffers from noise, and 2) the data aug-\nmentation methods, which are used in the original ZSSR,\nactually harm its SR performance (Table 3). Based on this\nobservation, we decided to attach BM3D [4] before the\nZSSR network optimization. For a fair comparison, we also\nuse BM3D with our SimUSR. Regarding the data augmen-\ntation, we suspect that this is due to ZSSR network’s small\ncapacity and the severe spatial distortion by applying strong\nafﬁne transformations [27].\nBy adding an ad-hoc denoiser (BM3D), ZSSR perfor-\nmance is dramatically improved by 0.63dB and 0.0422 in\nPSNR and SSIM, respectively. And by discarding afﬁne\naugmentation, we can further enhance the ZSSR to achieve\n26.55dB in PSNR (3rd row). With the same setting, our pro-\nposed SimUSR outperforms the ZSSR in a huge margin.\nFor example, SimUSR with the lightweight SR network,\nCARN [2], already boosts the SR performance of the ZSSR\nby 0.64dB and 0.0176 in PSNR and SSIM, respectively.\nMoreover, thanks to the high ﬂexibility of our method, we\ncan easily improve the performance by simply changing the\nbackbone to any other SR network. For instance, we get\nTable 4. The number of the parameters and runtime comparison of\n480×320 LR images with scale factor ×4.\nMethod\nZSSR\nSimUSR (Ours)\nCARN\nEDSR\nRCAN\n# Params.\n0.23M\n1.14M\n15.6M\n43.2M\nRuntime\n300.83s\n0.12s\n1.93s\n1.07s\nanother 0.09dB improvement in PSNR by just replacing a\nbackbone network from CARN to RCAN [30]. Figure 4\nshows the qualitative comparison between the ZSSR and\nour method with different backbone networks. Similar to\nthe bicubic SR task, SimUSR (with any backbone) provides\nbetter restoration results across various cases.\n5.3. Execution time\nIn this section, we evaluate and compare the latency of\nZSSR and our SimUSR (Table 4). Note that we benchmark\nthe runtime speed on the environment of NVIDIA TITAN X\nGPU by generating a 1080p SR image on scale factor ×4.\nAlthough ZSSR has only 0.23M parameters, it requires a\nhuge amount of runtime (300.83s) since it has to perform\noptimization and inference at runtime. In contrast, our pro-\nposed SimUSR only takes less than two seconds (1.93s)\neven if we use a heavy SR network (EDSR) as a backbone\nmodel. Comparing to the ZSSR, our method is at least 155\ntimes faster than the ZSSR and if we use a lightweight SR\nnetwork (CARN), 2,500 times faster (0.12s vs. 300.83s).\nTo embed the SR method to the real application, it is\nobvious that both SR performance and the latency are im-\nportant aspects (e.g. SR system for the streaming service).\nHowever, the above analysis reﬂects that although ZSSR\nhas nice properties, which does not need an HR image, ap-\nplying it to the real application is challenging because of its\nhigh latency. On the other hand, our approach can meet the\ncriteria that real applications demand (on both the perfor-\nmance and speed) by taking advantage of supervised SR. In\naddition, if necessary, we can further reduce the latency by\nreplacing the backbone to more lightweight network thanks\nto the ﬂexibility of our method.\n5.4. NTIRE 2020 super-resolution challenge\nThis work is proposed to participate to the NTIRE 2020\nsuper-resolution challenge [19]. This challenge aims to de-\nvelop an algorithm for the real-world SR task similar to the\nprior challenge in AIM 2019 [18]. However, unlike the pre-\nvious challenge, there exist no LR and HR image pairs in\nthe dataset akin to the setup that we experiment in Sec-\ntion 5.2. We submitted our SimUSR to the ﬁrst track (image\nprocessing artifact), where the clean image is degraded by\nthe unknown image artifact and downsample kernel. In this\nchallenge, models are evaluated using the PSNR, SSIM and\nLPIPS [29] metrics.\n6\nLR \n(24.60/0.6436)\nOurs + CARN (Δ) \n(25.91/0.7176)\nDIV2K: image0804\nZSSR (Δ) \n(25.23/0.6895)\nHR \n(PSNR/SSIM)\nOurs + RCAN (Δ) \n(26.12/0.7244)\nOurs + EDSR (Δ) \n(26.04/0.7230)\nLR \n(25.85/0.7069)\nOurs + CARN (Δ) \n(28.03/0.7997)\nDIV2K: image0831\nZSSR (Δ) \n(27.14/0.7762)\nHR \n(PSNR/SSIM)\nOurs + RCAN (Δ) \n(28.09/0.8018)\nOurs + EDSR (Δ) \n(28.17/0.8029)\nLR \n(21.82/0.6297)\nOurs + CARN (Δ) \n(24.10/0.7211)\nDIV2K: image0846\nZSSR (Δ) \n(22.58/0.6822)\nHR \n(PSNR/SSIM)\nOurs + RCAN (Δ) \n(24.42/0.7323)\nOurs + EDSR (Δ) \n(24.45/0.7317)\nLR \n(25.76/0.6843)\nOurs + CARN (Δ) \n(27.42/0.7694)\nDIV2K: image0850\nZSSR (Δ) \n(26.72/0.7484)\nHR \n(PSNR/SSIM)\nOurs + RCAN (Δ) \n(27.47/0.7710)\nOurs + EDSR (Δ) \n(27.51/0.7719)\nLR \n(24.56/0.7300)\nOurs + CARN (Δ) \n(27.66/0.8522)\nDIV2K: image0892\nZSSR (Δ) \n(24.04/0.7394)\nHR \n(PSNR/SSIM)\nOurs + RCAN (Δ) \n(28.07/0.8605)\nOurs + EDSR (Δ) \n(28.02/0.8601)\nFigure 4. Qualitative comparison of using our proposed method on the NTIRE 2020 dataset [19]. ∆is the absolute residual intensity map\nbetween the network output and the ground-truth HR image.\n7\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nLPIPS (lower is better)\n22\n23\n24\n25\n26\n27\nPSNR (higher is better)\nOurs\nFigure 5. Performance comparison of each entry in the NTIRE\n2020 super-resolution challenge [19] (track one). Our proposed\nSimUSR is marked as a red circle. Our method achieves the best\nPSNR with a reasonable LPIPS (13th rank).\nFinal result on the test dataset is shown in Figure 5 and\nTable 5. As shown in Figure 5, our method achieves the\nbest PSNR score among all entries with a reasonable LPIPS\nscore. Note that since we directly optimize the network us-\ning pixel-based loss, the LPIPS score of our SimUSR is\nlower than the rank of PSNR. We also report the challenge\nresult sorted on the PSNR (Table 5). We get the best PSNR\nand second-best on SSIM with the 13th rank of LPIPS.\n6. Discussion\nIn this section we discuss about the limitation of our\nmethod and the future direction.\nLimitation. Though the accessibility to multiple LR images\nis a mild and reasonable relaxation in many cases, there\nare still many applications and domains that cannot resort\non such assumption where collecting the data is very ex-\npensive, e.g., medical imaging. In addition, SimUSR heav-\nily relies on the generalizability of a model over different\nscales and pixel distributions, which can cause unexpected\nartifacts [27]. Because SimUSR uses bicubic downsampling\nto prepare the pseudo pairs, this may also cause an implicit\nbias in the SR model during the training. Last but not least, it\nis true that SimUSR is a basic approach that one would eas-\nily come up with but overlooked until now. We argue that it\nshould be by no means a new state-of-the-art but serve as a\nreasonable baseline to beat in the future.\nFuture work. We showed that our SimUSR framework is a\nstrong baseline but it still has a plenty of room to improve its\nperformance. For example, we used the BM3D as the pre-\nprocessing module for removing the noise. This pre-module\ncan be replaced to more effective models [13].\nTable 5. Performance comparison of each entry in the NTIRE 2020\nsuper-resolution challenge [19] (track one) sorted on the PSNR.\nThe number in the parenthesis denotes the rank. Our proposed\nSimUSR is ranked the best in PSNR and second-best in SSIM.\nMethod\nPSNR ↑\nSSIM ↑\nLPIPS ↓\nOurs\n27.09 (1)\n0.77 (2)\n0.369 (13)\nAnonymous 1\n27.08 (2)\n0.78 (1)\n0.325 ( 8)\nAnonymous 2\n26.73 (3)\n0.75 (5)\n0.379 (15)\nAnonymous 3\n26.71 (4)\n0.76 (4)\n0.280 ( 6)\nAnonymous 4\n26.54 (5)\n0.75 (8)\n0.302 ( 7)\nAnonymous 5\n26.23 (6)\n0.75 (7)\n0.327 (10)\n7. Conclusion\nWe have introduced a simple but effective baseline for a\nfully unsupervised super-resolution task (SimUSR). we ﬁrst\nclarify that unsupervised SR should strictly denote the task\nwithout any access to HR images. While complying with\nthis deﬁnition, we assume that low resolution (LR) images\nare relatively easy to obtain in the real-world. Exploiting\nmultiple LR images, we generated a pseudo-pair dataset of\nLR images and their down-scaled version and use this to\ntrain a SR model. This simple conversion allows us to en-\njoy the advantages of supervised learning. We demonstrated\nthat our SimUSR outperforms previous unsupervised SR\nmethod while having very short latency. Moreover, by in-\ntegrating the recently developed SR architectures and tech-\nniques, we showed that SimUSR successfully close the per-\nformance gap between the unsupervised and the supervised\nSR methods. Though our approach is simple, we argue that\naccessibility to multiple LR images is a legitimate setting\nand SimUSR serves as a strong baseline of unsupervised SR\nin this regime, which should be investigated prior to consid-\nering other complicated methods.\nAcknowledgement. This work was supported by NAVER\nCorporation and also by the National Research Foundation\nof Korea grant funded by the Korea government (MSIT)\n(no.NRF-2019R1A2C1006608)\nReferences\n[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge\non single image super-resolution: Dataset and study. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition Workshops, pages 126–135, 2017. 4\n[2] Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn. Fast,\naccurate, and lightweight super-resolution with cascading\nresidual network. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), pages 252–268, 2018. 1,\n2, 4, 6\n[3] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and\nMarie-Line Alberi-Morel.\nLow-complexity single-image\nsuper-resolution based on nonnegative neighbor embedding.\n8\nIn Proceedings of the British Machine Vision Conference\n(BMVC), 2012. 4\n[4] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and\nKaren Egiazarian. Image denoising by sparse 3-d transform-\ndomain collaborative ﬁltering. IEEE Transactions on image\nprocessing, 16(8):2080–2095, 2007. 1, 2, 4, 6\n[5] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Image super-resolution using deep convolutional net-\nworks. IEEE transactions on pattern analysis and machine\nintelligence, 38(2):295–307, 2015. 1, 2\n[6] Ruicheng Feng, Jinjin Gu, Yu Qiao, and Chao Dong. Sup-\npressing model overﬁtting for image super-resolution net-\nworks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition Workshops, pages 0–0, 2019.\n2\n[7] Manuel Fritsche, Shuhang Gu, and Radu Timofte. Frequency\nseparation for real-world super-resolution.\narXiv preprint\narXiv:1911.07850, 2019. 1, 2\n[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances\nin neural information processing systems, pages 2672–2680,\n2014. 2\n[9] Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong.\nBlind super-resolution with iterative kernel correction.\nIn\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1604–1613, 2019. 2\n[10] Reinhard Heckel and Paul Hand.\nDeep decoder: Concise\nimage representations from untrained non-convolutional net-\nworks. International Conference on Learning Representa-\ntions, 2019. 2\n[11] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single\nimage super-resolution from transformed self-exemplars. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5197–5206, 2015. 2, 4\n[12] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate\nimage super-resolution using very deep convolutional net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 1646–1654, 2016. 1, 2\n[13] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug.\nNoise2void-learning denoising from single noisy images. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2129–2137, 2019. 8\n[14] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken,\nAlykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-\nrealistic single image super-resolution using a generative ad-\nversarial network. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4681–4690,\n2017. 1\n[15] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli\nLaine,\nTero\nKarras,\nMiika\nAittala,\nand\nTimo\nAila.\nNoise2noise: Learning image restoration without clean data.\narXiv preprint arXiv:1803.04189, 2018.\n[16] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced deep residual networks for single\nimage super-resolution. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition workshops,\npages 136–144, 2017. 1, 2, 4\n[17] Andreas Lugmayr, Martin Danelljan, and Radu Timofte. Un-\nsupervised learning for real-world super-resolution. arXiv\npreprint arXiv:1909.09629, 2019. 1, 2\n[18] Andreas Lugmayr, Martin Danelljan, Radu Timofte, et al.\nAim 2019 challenge on real-world image super-resolution:\nMethods and results. In ICCV Workshops, 2019. 6\n[19] Andreas Lugmayr, Martin Danelljan, Radu Timofte, et al.\nNtire 2020 challenge on real-world image super-resolution:\nMethods and results. CVPR Workshops, 2020. 1, 4, 6, 7, 8\n[20] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\nMalik. A database of human segmented natural images and\nits application to evaluating segmentation algorithms and\nmeasuring ecological statistics.\nIn Proceedings of the In-\nternational Conference on Computer Vision (ICCV), 2001.\n4\n[21] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto,\nToru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa.\nSketch-based manga retrieval using manga109 dataset. Mul-\ntimedia Tools and Applications, 76(20):21811–21838, 2017.\n4\n[22] Assaf Shocher, Nadav Cohen, and Michal Irani. “zero-shot”\nsuper-resolution using deep internal learning. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3118–3126, 2018. 1, 2, 3, 4, 6\n[23] Jae Woong Soh, Sunwoo Cho, and Nam Ik Cho.\nMeta-\ntransfer learning for zero-shot super-resolution.\narXiv\npreprint arXiv:2002.12213, 2020. 3, 4\n[24] Radu Timofte, Vincent De Smet, and Luc Van Gool. A+:\nAdjusted anchored neighborhood regression for fast super-\nresolution. In Asian conference on computer vision, pages\n111–126. Springer, 2014. 1, 2\n[25] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.\nDeep image prior. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 9446–\n9454, 2018. 2\n[26] Jianchao Yang, John Wright, Thomas S Huang, and Yi Ma.\nImage super-resolution via sparse representation.\nIEEE\ntransactions on image processing, 19(11):2861–2873, 2010.\n4\n[27] Jaejun Yoo, Namhyuk Ahn, and Kyung-Ah Sohn. Rethink-\ning data augmentation for image super-resolution: A com-\nprehensive analysis and a new strategy.\narXiv preprint\narXiv:2004.00448, 2020. 2, 4, 6, 8\n[28] Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang,\nChao Dong, and Liang Lin.\nUnsupervised image super-\nresolution using cycle-in-cycle generative adversarial net-\nworks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition Workshops, pages 701–710,\n2018. 1, 2\n[29] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 586–595, 2018. 6\n[30] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng\nZhong, and Yun Fu. Image super-resolution using very deep\n9\nresidual channel attention networks. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), pages\n286–301, 2018. 1, 2, 4, 6\n10\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-04-23",
  "updated": "2020-04-23"
}