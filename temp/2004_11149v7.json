{
  "id": "http://arxiv.org/abs/2004.11149v7",
  "title": "A Comprehensive Overview and Survey of Recent Advances in Meta-Learning",
  "authors": [
    "Huimin Peng"
  ],
  "abstract": "This article reviews meta-learning also known as learning-to-learn which\nseeks rapid and accurate model adaptation to unseen tasks with applications in\nhighly automated AI, few-shot learning, natural language processing and\nrobotics. Unlike deep learning, meta-learning can be applied to few-shot\nhigh-dimensional datasets and considers further improving model generalization\nto unseen tasks. Deep learning is focused upon in-sample prediction and\nmeta-learning concerns model adaptation for out-of-sample prediction.\nMeta-learning can continually perform self-improvement to achieve highly\nautonomous AI. Meta-learning may serve as an additional generalization block\ncomplementary for original deep learning model. Meta-learning seeks adaptation\nof machine learning models to unseen tasks which are vastly different from\ntrained tasks. Meta-learning with coevolution between agent and environment\nprovides solutions for complex tasks unsolvable by training from scratch.\nMeta-learning methodology covers a wide range of great minds and thoughts. We\nbriefly introduce meta-learning methodologies in the following categories:\nblack-box meta-learning, metric-based meta-learning, layered meta-learning and\nBayesian meta-learning framework. Recent applications concentrate upon the\nintegration of meta-learning with other machine learning framework to provide\nfeasible integrated problem solutions. We briefly present recent meta-learning\nadvances and discuss potential future research directions.",
  "text": "A COMPREHENSIVE OVERVIEW AND SURVEY OF RECENT\nADVANCES IN META-LEARNING\nA PREPRINT\nHuimin Peng∗\nhpeng2@ncsu.edu\npeng.huimin.pennie@gmail.com\nABSTRACT\nThis article reviews meta-learning also known as learning-to-learn which seeks rapid and accurate\nmodel adaptation to unseen tasks with applications in highly automated AI, few-shot learning, natural\nlanguage processing and robotics. Unlike deep learning, meta-learning can be applied to few-shot\nhigh-dimensional datasets and considers further improving model generalization to unseen tasks.\nDeep learning is focused upon in-sample prediction and meta-learning concerns model adaptation for\nout-of-sample prediction. Meta-learning can continually perform self-improvement to achieve highly\nautonomous AI. Meta-learning may serve as an additional generalization block complementary for\noriginal deep learning model. Meta-learning seeks adaptation of machine learning models to unseen\ntasks which are vastly different from trained tasks. Meta-learning with coevolution between agent and\nenvironment provides solutions for complex tasks unsolvable by training from scratch. Meta-learning\nmethodology covers a wide range of great minds and thoughts. We brieﬂy introduce meta-learning\nmethodologies in the following categories: black-box meta-learning, metric-based meta-learning,\nlayered meta-learning and Bayesian meta-learning framework. Recent applications concentrate upon\nthe integration of meta-learning with other machine learning framework to provide feasible integrated\nproblem solutions. We brieﬂy present recent meta-learning advances and discuss potential future\nresearch directions.\nKeywords Meta-Learning · General AI · Few-Shot Learning · Meta-Reinforcement Learning · Meta-Imitation\nLearning\n1\nIntroduction\n1.1\nBackground\nMeta-learning was ﬁrst proposed by Jürgen Schmidhuber in 1987 [1] which considers the interaction between agent\nand environment driving self-improvement in agent. It integrates evolutionary algorithms and Turing machine to\nachieve continual self-improvement so that agents adapt to dynamic environment conditions rapidly and precisely. RNN\n(Recurrent Neural Network) [2] contains self-connected recursive neurons that include memory cells to store model\nexperiences and can be updated sequentially to model self-improvement. LSTM (Long Short-Term Memory) is a main\nform of RNN that can be used to account for long-range dependence in sequential data. Forget gates [3] in LSTM are\ncrucial to avoiding memory explosion and improving model generalization capability. LSTM itself can be applied as\na meta-learning system [4] and parameters in LSTM can be updated to reﬂect upon base learner and meta-learners.\nLSTM can also be applied as a meta-learner [5] under the framework of layered meta-learning to account for all base\nlearners optimized with stochastic gradient descent. Coevolution [6] considers cooperation and competition between\nagents, and interaction between agents and evolving environments. Coevolution [7] generalizes deep model solutions to\ncomplex environments which cannot be solved by training from scratch. It demonstrates the necessity of meta-learning\n∗Thank you for all helpful comments! Feel free to leave a message about comments on this manuscript. In case I did not receive\nemail, my personal email is 974630998@qq.com. Thanks to github.com/kourgeorge/arxiv-style for this pdf latex template. I realize\nthat this manuscript may be better named as recent advances in few-shot meta-learning.\narXiv:2004.11149v7  [cs.LG]  26 Oct 2020\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nsince meta-learning with coevolution is capable of providing solutions unreachable by training from scratch. Most\nearly developments in meta-learning are from meta-reinforcement learning seeking to improve degree of autonomy and\nsmartness in robots. Training a reinforcement learning model from scratch is costly, and meta-learning saves the trouble\nby adapting a well-trained model from a similar task. Sometimes tasks are too complex and training from scratch does\nnot provide a sufﬁciently predictive solution. However, it is mentioned in [7] and [6] that meta-learning with generative\nevolution scheme provides good solutions to complex settings which cannot be solved through training from scratch.\nIn [8], meta-learning and transfer learning are regarded as synonyms. Later developments in meta-learning and transfer\nlearning diverge and they emerge as two different academic areas. Meta-learning is known as LTL (Learning-To-Learn)\nmodels which allow rapid and precise adaptation to unseen tasks [9]. Meta-learning is widely applied to few-shot\ndatasets and adapts deep models for mining high-dimensional input from few-shot tasks. LTL gains attention under the\npopularity of continual learning research for strong AI. Transfer learning is concentrated upon adapting pre-trained deep\nmodel to solve similar tasks. In a sense, meta-learning seeks methodology with better adaptation to vastly different tasks\nby uncovering the deep similarity relation between tasks. Recently there have been several research integrating domain\nadaptation into meta-learning to improve prediction accuracy in few-shot image classiﬁcation task, as in [10] and [11].\nSimilar to lifelong learning which accumulates knowledge and builds one model applicable to all, meta-learning aims\nto develop a general framework that can provide task-speciﬁc learners across vastly varying settings. Meta-learning\nmines fundamental rules and depends upon machine learning structures adaptable to a range of vastly different tasks.\nMeta-learning has two functionalities: achieving self-improvement along continual adaptations [1] and providing\ngeneralization framework for machine learning [12].\nIn recent developments, the most important application of meta-learning is few-shot learning [13, 14, 5, 15, 16, 17, 18,\n19, 10]. Actually most advances in few-shot learning lie in meta-learning, as mentioned in a survey paper of few-shot\nlearning [20]. Most recently developed meta-learning methods in 2017-2019 focus upon few-shot image classiﬁcation.\nAs we know, training of deep learning models demands a large amount of labelled data. For few-shot high-dimensional\ndatasets, we resort to meta-learning which incorporates a generalization block based upon deep learners. By utilizing\npast model experiences, meta-learning provides representation models of moderate complexity for few-shot tasks\nwithout demanding large amount of labelled data. In image classiﬁcation, few-shot learning is applicable to tasks where\nthere are K (K< 10) images in each image class known as N-class K-shot problems. It is observed that humans can\nlearn new concepts from few demonstrations and utilize past knowledge to identify a new category efﬁciently. The\nobjective of strong AI is for machine learning models to approach the learning capabilities of human beings. In few-shot\nlearning, researchers seek human-like machines that can make fast and accurate classiﬁcation based upon few images.\nAlso for ancient languages where we only have few observations, and for dialects spoken by only a small group of\npeople, we resort to few-shot meta-learning for fast and accurate predictive modeling. [20] provides an illustrative\noverview of meta-learning methods developed for few-shot learning.\nMeta-learning models how human beings learn. Early research on meta-learning gains inspiration and motivation from\ncognitive science experiments explaining the process in which people learn new concept. The primary component on\nlearning how to solve different tasks is to model the similarity between tasks. In metric-based meta-learning, feature\nextraction and distance measure are jointly applied to represent similarity and are jointly tuned in the training process.\nIn layered meta-learning, base learner models task-speciﬁc characteristics and meta-learner models features shared by\ntasks. To represent similarity with appropriate complexity, hyperparameters implying the complexity of meta-learner\ncan be tuned in training. By modeling task similarity explicitly or implicitly, meta-learning introduces a ﬂexible\nframework designed to be applicable to vastly different tasks. For example, MAML (Model-Agnostic Meta-Learning)\n[12] is applicable to all learners that can be solved with SGD (Stochastic Gradient Descent). On the other hand, based\nupon pre-trained deep models, meta-learning adapts task-speciﬁc learners to unseen tasks rapidly without precision loss\n[21]. Both reinforcement learner [22] and neural network [23] have high representation capability and can be applied to\nsearch for an optimizer autonomously. In both cases, a general representation of an optimizer search space can be layed\nout explicitly.\nIn recent applications, meta-learning is focused upon integration with other machine learning frameworks and forms\nmeta-reinforcement learning [24, 25, 26, 27, 28, 29, 30, 31] and meta-imitation learning [32, 10] which are closely\nassociated with robotics research. The beneﬁts of introducing meta-learning are twofold: to save computation by\navoiding the need to re-train deep models from scratch; to increase reaction speed by adapting deep models to\ndynamic environment conditions. Reinforcement learning estimates optimal actions based upon given policy and\nenvironment [33]. Imitation learning evaluates the reward function from observing behaviors of another agent in a\nsimilar environment [34]. Few-shot learning helps an agent make predictions based upon only few demonstrations\nfrom other agents [12]. Recent application of meta-learning integrates reinforcement learning, imitation learning and\nfew-shot meta-learning for robots to learn basic skills and react timely to rare situations [35]. Meta-learning is feasible\nin tackling situations where out-of-distribution task predictive performance is required. Most deep learning models\n2\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\ndo not inherently embed any scheme like this and have to be integrated into a meta-learning framework to obtain this\ngeneralization capability to out-of-distribution situations.\nA typical assumption behind meta-learning is that tasks share similarity so that they can be solved under the same\nmeta-learning framework. To relax this assumption and improve model generalization, similarity model should be taken\ninto consideration. Feature extraction model and distance measure can be jointly trained to represent similarity of any\ncomplexity. Meta-learner model speciﬁed as neural network also represents similarity shared by vastly different tasks.\nSimilarity is an indisposable component in meta-learning whether it is implicitly or explicitly present. In applications,\nstatistical learning models such as convex linear classiﬁers and logistic regression are widely applied. Statistical models\nare less prone to over-ﬁtting and can be designed to be robust to model misspeciﬁcation. Deep learning models are\nintensively data-driven and highly integrated. Under the framework of meta-learning, we can combine statistical models\nand deep learning for fast and accurate adaptation. For few-shot datasets with high-dimensional input, dimension\nreduction techniques in statistical learning can be speciﬁed as base learner and be integrated with meta-learner. Base\nlearner and meta-learner approach is proposed with the concept of meta-learning, both proposed in this paper [1]. In\nthis survey paper, Base learner and meta-learner approach is called layered meta-learning, since it includes several\nlayers from task-speciﬁc layer to task-generalization layer.\nThe structure of our article is as follows. Section 1.2 presents history of meta-learning research. Section 1.3 provides\nan outline of benchmark datasets for few-shot tasks and meta-learning formulation composed of meta-training, meta-\nvalidation and meta-testing. Section 2 summarizes meta-learning models into four categories. The distinction between\nmethodologies is not exact. Different methods can be combined to solve application problems. Section 2.1 presents\nblack-box meta-learning methods that generalize deep learning models to unseen tasks. Section 2.2 reviews metric-\nbased meta-learning which utilizes feature extraction and distance measure jointly to represent similarity between\ntasks. Section 2.3 summarizes meta-learning models composed of task-speciﬁc base learner and meta-learners that\nexplore similarity shared by vastly different tasks. Section 2.4 surveys Bayesian meta-learning methods which integrate\nprobabilistic methodology into meta-learning. Section 3 brings attention to applications of meta-learning by integrating\nit with other machine learning frameworks. Section 3.1 brieﬂy reviews meta-reinforcement learning and section 3.2\nsurveys meta-imitation learning. Section 3.3 brieﬂy reviews online meta-learning and section 3.4 summarizes methods\nin unsupervised meta-learning. Section 4 discusses future research directions based upon meta-learning.\n1.2\nHistory\nJürgen Schmidhuber proposes meta-learning in his diploma thesis [1] in 1987. [1] achieves learning-to-learn with\nself-improvement using evolutionary algorithm and Turing machine. [1] applies evolutionary algorithms to continually\nupdate learner so that it achieves higher-level of autonomy in AI. [1] builds the ﬁrst meta-learning framework upon\ninspirations from meta-cognition and introduces meta-hierarchy which constitutes base learner and many levels of meta-\nlearners to represent task decomposition for a complex mission. [1] motivates many recent advances in meta-learning\nmethodology. Later metareasoning proposed in [36] lays out an integrated framework combining reinforcement learning,\ncausal learning and meta-learning. Metareasoning is the procedure for computers to autonomously make updated\ncomputation decision based upon causal reasoning under limited computation resources. Rather than evolutionary\nalgorithm, self-improvement in [36] relies upon causal reasoning which is goal-based and reward-driven. [37] lays\nout the framework for learning how to learn and proposes autonomous self-improvement in policy. The origin of\nmeta-learning stems from policy self-improvement driven by interaction between agents and environment which dates\nback to [1] and [38]. Meta-learning does not necessarily involve the role of environment. For reinforcement learning\ntasks, reward-driven self-improvement in policy should be considered. For other complex missions, task decomposition\nis required to simplify the problem.\nEarlier developments in meta-learning research are concentrated upon hyper-parameter optimization, as in [39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50]. All machine learning algorithms can be framed as a base learner and all\nhyper-parameter optimization scheme can be seen as a meta-learner that guides, accelerates and contributes to the base\nlearner optimization. Hyper-parameter optimization is a natural application of meta-learning. In [40], neural network\nis trained using many distinct but related tasks simultaneously in order to improve model generalization capability.\nIn [41], gradients of a cross-validation loss function is used for joint optimization of several hyper-parameters. In\n[24], meta-learning is integrated into reinforcement learning but only to tune hyper-parameters such as learning rate,\nexploration-exploitation tradeoff and discount factor of future reward.\nLater meta-learning can be applied to conduct autonomous model selection [51, 52, 47, 53] such as neural architecture\noptimization [43, 45, 54, 25, 55]. Nowadays AutoML (Automated Machine Learning) is a hot topic where automatic\nsearch of a invincible deep neural network model is examined in various aspects. AutoML is expected to beat any\nhuman-designed deep neural model so that we (non-experts) do not need to go though the excruciating labor of\nhyper-parameter tuning ourselves. Recently AutoML also seeks integration with meta-learning in order to make the\n3\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nneural model search process more efﬁcient, as in [56] and [57]. In [51], meta-learning is applied to ranking and\nclustering, where algorithms are trained on meta-samples and the optimal model with highest prediction accuracy is\nselected. [52] considers using feedforward neural network, decision tree or support vector machine as learner model.\nThen it selects the class of models with the best performance on time series forecast. In [47], meta-learning is applied\nto select parameters in support vector machine (SVM), which demonstrates improved generalization capability in data\nmodeling. Multi-objective particle swarm optimization is integrated with meta-learning to select parameters.\nOne of the most common methodologies for meta-learning is learning how to learn by identifying the best optimizer. In\ndeep learning applications, complex components which need high degree of representation power are modelled with\ndeep neural networks. Searching through all neural network models can identify the optimal neural network for the\ncomplex components to bring out the best performance of deep learning models. For example, in deep reinforcement\nlearning and deep imitation learning, a policy is modelled with deep neural network models, since neural network model\nis known to be a good universal function approximator. [22] considers all ﬁrst-order and second-order optimization\nmethods under the framework of meta-imitation learning and minimizes the distance between predicted and target\nactions. Policy update per iteration can be approximated using neural networks where weight parameters are estimated\njointly with step direction and step size. By learning the optimizer autonomously, algorithms converge faster and\noutperform stochastic gradient descent.\nIn recent developments, meta-learning is concentrated upon model adaptation between vastly different tasks which\nshare certain similarity structure. Though in transfer learning, transferring between vastly different tasks may trigger\nnegative transfer worsening model predictive performance. For out-of-distribution tasks, we can extract the most\nsimilar experience from a large memory, and design predictive models based upon few-shot datasets collected in an\nunseen task. Similarity metric based end-to-end training approach is also mentioned in the early work of [1]. [1]\nindeed guides the development of meta-learning ever since and contains the essential philosophy behind meta-learning\nmethodology. Classiﬁcation of recent meta-learning methodologies is not exact since meta-learning models exhibit the\ntendency to be more ﬂexible and integrated in recent developments. But with classiﬁcation, we can roughly outline\nrecent research directions for later mixing. Recent meta-learning methodology can be categorized into four classes\nwhich are model-based, metric-based, optimization-based and generative AI-GAs methods, as in [58, 6]. MANN\n(Memory-augmented neural networks) [59] belongs to the model-based category. It stores all model training history in\nan external memory and loads the most relevant model parameters from external memory every time a new unseen task\nis present. Task similarity is reﬂected in the metric-based relevance of memory items extracted for model adaptation.\nSecond, convolutional Siamese neural network [13] is within the metric-based category. Metric refers to the similarity\nbetween tasks. Siamese network designs a metric that depends upon the similarity measure between convolutional\nfeatures of different images. Matching networks [14], relation network [17] and prototypical network [15] are all\nmetric-based methods. Feature extraction mechanism and distance measure of embeddings jointly model task similarity.\nIn our work, we categorize memory-based and metric-based methods into metric-based meta-learning.\nOptimization-based technique includes a learner for model estimation at task level and a meta-learner for model\ngeneralization across tasks. Meta-learners may consist of many layers of models for different communication patterns\nand generalization scopes according to task decomposition structure. Many layers of meta-learners may collapse\ninto one layer of meta-learner depending upon task complexity and the corresponding model complexity. In our\nwork, we summarize methods containing base learner and meta-learners to be layered meta-learning. In meta-LSTM\n[5], a meta-learner updates learner parameters on different batches of training data and validation data. For learners\noptimized with stochastic gradient descent, a meta-learner can be speciﬁed to be a long short-term memory model [4].\nMAML (Model-Agnostic Meta-Learning) proposed by Chelsea Finn in 2017 [12] is famous and applicable to many\nreal-life applications. MAML provides fast and accurate generalization of deep neural network models and does not\nimpose any model assumption. MAML is applicable to any learner model optimized with stochastic gradient descent.\nFirst-order meta-learning algorithms FOMAML and Reptile [60] are also optimization-based, where iterative updates\non parameters are designed to be the difference between previous estimates and new sample average estimate.\nFrom probabilistic perspective, meta-learning can be formulated under Bayesian inference framework. Bayesian\ninference provides an efﬁcient estimation of the uncertainty in few-shot meta prediction and extends methodology to be\nmore widely applicable. Bayesian approach is statistical and has a long history. For complex tasks, resorting to Bayesian\nprocedure is worthwhile. In [61], a Bayesian generative model is combined with deep Siamese convolutional network\nto make classiﬁcation on hand-written characters. Bayesian generative model is very intriguing since it simulates\nsamples that look real to augment training and validation data. The sample-generating scheme is estimated from training\ndata itself. So no additional information is generated from generating more samples. However, augmenting data this\nway greatly improves prediction accuracy in few-shot image classiﬁcation. In [62], a Bayesian extension of MAML\nis proposed, where stochastic gradient descent (SGD) in MAML is replaced with Stein variational gradient descent\n(SVGD). SVGD offers an efﬁcient combination of MCMC and variational inference, which are two main approaches in\nBayesian approach. Combination of MCMC and variational inference is ideal since the advantages and disadvantages\n4\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nof MCMC and variational inference complement each other. In [18], a Bayesian graphical model is embedded in\nthe task-speciﬁc parameters and meta-parameters in meta-learning framework. An amortization network is used to\nmap training data to weights in linear classiﬁer. Amortization network is also used to map input data to task-speciﬁc\nstochastic parameter for further sampling. It utilizes an end-to-end stochastic training to compute approximate posterior\ndistributions of task-speciﬁc parameters in meta-learner and unknown labels on new tasks.\nOne of the recent meta-learning applications is in robotics, where meta-imitation learning [34, 32, 10, 35] and meta-\nreinforcement learning [63, 64, 65, 26, 66, 67, 29, 68] are of primary interest. The objective of general AI is for machine\nlearning to reach human intelligence so that machine learning can handle dangerous tasks for human. Children is\ncapable of learning basic movements from only one or two demonstrations so that researchers hope robots can do the\nsame through meta-imitation learning. Imitation of action, reward and policy is achieved by minimization of regret\nfunction which measures the distance between current state and imitation target. In [34], MAML for one-shot imitation\nlearning is proposed. Minimization of cloning loss leads to closely minimick target action that robots try to follow. It\nestimates a policy function that maps visual inputs to actions. Imitation learning is more efﬁcient than reinforcement\nlearning from scratch and is popular in robotics research. [10] also integrates MAML into one-shot imitation learning.\nIt collects one human demonstration video and one robot demonstration video for robots to imitate. The objective here\nis to minimize behavioral cloning loss with inner MAML parameter adaptation. It also considers domain adaptation\nwith generalization to different objects or environment in the imitation task.\nMeta-reinforcement learning (Meta-RL) is designed for RL tasks such as reward-driven situations with sparse reward,\nsequential decision and clear task deﬁnition [27]. RL considers the interaction between agent and environment through\npolicy and reward. By maximizing reward, robots select an optimal sequential decision. In robotics, meta-RL is applied\nin cases where robots need rapid reaction to rare situations based upon previous experiences. [64] provides an overview\nof meta-RL models in multi-bandit problems. Meta-learned RL models demonstrate better performance than RL models\nfrom scratch. [29] constructs a highly integrated meta-RL method PEARL which combines variational inference and\nlatent context embedding in off-policy meta-RL. In addition, reward-driven neuro-activities in animals can be explained\nwith meta-RL. In [27], phasic dopamine (DA) release is viewed as reward and meta-RL model explains well the DA\nregulations in guiding animal behaviors with respect to the changing environment in animal experiment.\nIn addition to meta-RL and meta-imitation learning, meta-learning can be ﬂexibly combined with machine learning\nmodels for applications in real-life problems. For example, unsupervised meta-learning conducts rapid model adaptation\nusing unlabelled data. Online meta-learning analyzes streaming data and performs real-time model adaptation. The\nfeature of meta-learning application is small sample size and high dimensional input. In meta-RL, number of trajectories\nfor interaction between agent and environment is small. In meta imitation learning, number of demonstrations from\nhuman or agent for a similar task is only one or two, thus small sample size. First, unsupervised meta-learning\n[69, 26, 19, 70, 71, 72] is for modelling unlabelled data. Unsupervised clustering methods such as adversarially\nconstrained autoencoder interpolation (ACAI) [73], bidirectional GAN (BiGAN) [73], DeepCluster [74] and InfoGAN\n[75] are applied to cluster data and estimate data labels [71]. Afterwards meta-learning methods are used on unlabelled\ndata and predicted labels obtained through unsupervised clustering. It is mentioned in [71] that unsupervised meta-\nlearning may perform better than supervised meta-learning. Another combination of unsupervised learning and\nmeta-learning is in [72]. It replaces supervised parameter update in inner loop with unsupervised update using\nunlabelled data. Meta-learner in the outer loop applies supervised learning using labeled data to update the unsupervised\nweight update rule. It demonstrates that this unique combination performs better in model generalization.\nSecond, online meta-learning analyzes streaming data so that the model should respond to changing conditions rapidly\nusing a small batch of data in each model adaptation [76, 67, 77]. Robots are supposed to react real-time to dynamic\nenvironment so that robots should learn to update deep model each time with real-time obtained data, which is of small\nsample size. [76] proposes a Bayesian online learning model ALPaCA where kernel-based Gaussian process (GP)\nregression is performed on the last layer of neural network for fast adaptation. It trains an ofﬂine model to estimate GP\nregression parameters which stay ﬁxed through all online model adaptation. [67] applies MAML to continually update\nthe task-speciﬁc parameter in prior distribution so that the Bayesian online model adapts rapidly to streaming data.\n[77] integrates MAML into an online algorithm follow the leader (FTL) and creates an online meta-learning method\nfollow the meta-leader (FTML). MAML updates meta-parameters which are inputs into FTL and this integrated online\nalgorithm generalizes better than previously developed methods.\nMeta-learning algorithms are hybrid, ﬂexible and can be combined with machine learning models such as Bayesian deep\nlearning, RL, imitation learning, online algorithms, unsupervised learning and graph models. In these combinations,\nmeta-learning adds a model generalization module to existing machine learning methods. Many real-life applications\nrequire deep neural network models and integrating meta-learning methods such as MAML, Reptile and Prototypical\nNets into existing deep models can bring additional beneﬁts. Integration of MAML into deep models is convenient\nsince all deep neural network models use SGD to update weight parameters. For complex tasks, we can decompose\n5\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\ncomplex tasks into simpler sub-tasks which can be solved with base learners of less complexity. We may utilize complex\nmeta-learner to represent complex similarity between vastly different tasks. AI-GAs [6] contain coevolution between\nagents and environment and reach feasible solutions for very complex tasks which are otherwise unsolvable by training\nfrom scratch. I like the integration of meta-learning methodology and high-dimensional research in statistics. Recent\ndevelopments in meta-learning are focused upon small sample with high dimensional input. High-dimensional research\nin statistics handles data where number of features is greater than sample size, ie small sample with high dimensional\ninput. They approach the same kind of application problems from different perspectives and more integration is expected\nin this intersection area.\n1.3\nDatasets and Formulation\nFew-shot datasets used as benchmarks for performance comparison in meta-learning literature are reviewed in\n[78]. Many meta-datasets are available at https://github.com/google-research/meta-dataset. Though\nmost datasets are large and annotated, the number of few-shot meta-learning datasets is constantly growing. Still,\nmeta-learning datasets are not as mainstream as large annotated high-dimensional dataset, where deeper and more\ncomplex neural network model is expected in pursuit of higher predictive accuracy. As for application, recent advances\nin meta-learning are concentrated upon few-shot high-dimensional data. I feel that meta-learning datasets are abundant\nin real life and there will be many more challenging baseline datasets available in the future. For general AI application,\nmost datasets are either for vision or for language. Currently commonly used meta-learning datasets are brieﬂy listed as\nfollows.\n• Omniglot [79] is available at https://github.com/brendenlake/omniglot. Omniglot is a large dataset\nof hand-written characters with 1623 characters and 20 examples for each character. These characters are\ncollected based upon 50 alphabets from different countries. It contains both images and strokes data. Stroke\ndata are coordinates with time in miliseconds.\n• ImageNet [80] is available at http://www.image-net.org/. ImageNet contains 14 million images and 22\nthousand classes for these images. Large scale visual recognition challenge 2012 (ILSVRC2012) dataset is\na subset of ImageNet. It contains 1,281,167 images and labels in training data, 50,000 images and labels in\nvalidation data, and 100,000 images in testing data.\n• miniImageNet [14, 81] is a subset of ILSVRC2012. It contains 60,000 images which are of size 84×84.\nThere are 100 classes and 600 images within each class. [82] splits 64 classes as training data, 16 as validation\ndata, and 20 as testing data.\n• tieredImageNet [82] is also a subset of ILSVRC2012 with 34 classes and 10-30 sub-classes within each. It\nsplits 20 classes as training data, 6 as validation data and 8 as testing data.\n• CIFAR-10/CIFAR-100 [83, 84] is available at https://www.cs.toronto.edu/~kriz/cifar.html.\nCIFAR-10 contains 60,000 colored images which are of size 32×32. There are 10 classes, each contains\n6,000 images. CIFAR-100 contains 100 classes, each includes 600 images. CIAFR-FS [85] is randomly\nsampled from CIFAR-100 for few-shot learning in the same mechanism as miniImageNet. FC100 [84] is also\na few-shot subset of CIFAR-100. It splits 12 superclasses as training data, 5 superclasses as validation data\nand 5 superclasses as testing data.\n• Penn Treebank (PTB) [86] is available at https://catalog.ldc.upenn.edu/LDC99T42. PTB is a large\ndataset of over 4.5 million American English words, which contain part-of-speech (POS) annotations. Over\nhalf of all words have been given syntactic tags. It is used for sentiment analysis and classiﬁcation of words,\nsentences and documents.\n• CUB-200 [78, 87] is available at http://www.vision.caltech.edu/visipedia/CUB-200.html. CUB-\n200 is an annotated image dataset that contains 200 bird species, a rough image segmentation and image\nattributes.\n• CelebA (CelebFaces Attributes Dataset) is available at http://mmlab.ie.cuhk.edu.hk/projects/\nCelebA.html. CelebA is an open-source facial image dataset that contains 200,000 images, each with\n40 attributes including identities, locations and facial expressions.\n• YouTube Faces database is available at https://www.cs.tau.ac.il/~wolf/ytfaces/. YouTube Faces\ncontains 3,425 face videos from 1,595 different individuals. Number of frames in each video clip varies from\n48 to 6070.\nAmong these datasets, miniImageNet [14, 81] and tieredImageNet [82] are the most useful few-shot image classiﬁca-\ntion datasets. They are used to compare predictive classiﬁcation accuracy of many meta-learning few-shot methods.\n6\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nThe application of meta-learning is not limited to few-shot learning. These meta-learning methodologies may serve as\nuseful ingredients for constructing highly integrated models to suit the need of real-life complex missions. For few-shot\nimage classiﬁcation, datasets listed here are widely applied in literature as comparison benchmarks.\nTraining and testing framework for few-shot image classiﬁcation used in meta-learning are outlined in ﬁgure 1. This\nmeta-learning framework is after episodic training used in recent few-shot meta-learning research papers. Task data\nare randomly sampled from large annotated dataset. For few-shot image classiﬁcation, image classes are sampled\nﬁrst and images within each class are sampled afterwards. On each task, task-speciﬁc parameters are updated. After\ntraining one or several tasks, meta-parameters are updated. Meta-parameters depict the common feature shared across\nseveral tasks. Within each task, there are training data Dtr, validation data Dval and testing data Dtest. Support set S is\nthe set of all labelled data. Training data and validation data are randomly sampled from support set. Training data\nare used to update task-speciﬁc parameters by minimizing loss function on training data. Validation data are used to\nupdate meta-parameters by minimizing loss function on validation data. Loss function on validation data measures\ngeneralization capability of deep model. Minimization of loss function on validation data maximizes generalization\ncapability of deep models. Query set Q is the set of all unlabelled data and test data are randomly sampled from\nquery set. Test data are unlabelled and we can view the predictive results on test data to see whether the results match\nintuition or not. Meta-learning datasets include non-overlapping meta-training data Dmeta−train, meta-validation data\nDmeta−val and meta-testing data Dmeta−test which consists of tasks. As in the case of few-shot image classiﬁcation,\nthe image classes in training data and validation data are non-overlapping, so that validation data can be viewed as an\nunseen task compared with training data. Loss function on validation data measures model capability to solve unseen\ntasks. Within each task, there is a training dataset, a validation dataset and a testing dataset. For few-shot tasks, the\nsample size in training dataset is small.\nFigure 1: Upper part shows data in each task: training data, validation data and testing data. Lower part shows\nmeta-training data, meta-validation data and meta-testing data that consists of tasks. Support set is the set of all labelled\ndata. Query set is the set of all unlabelled data. Due to limited square spaces, I wrote ’train data’ instead of ’training\ndata’ and ’test data’ instead of ’testing data’.\nIn supervised meta-learning, input is labelled data (xxx, y), where xxx is an image or a feature embedding vector and y is a\nlabel. Data model is y = hθ(xxx) parameterized by meta-parameter θ. As in [33], a task is deﬁned as\nT = {p(xxx), p(y|xxx), L},\nwhere L is a loss function, p(xxx) and p(y|xxx) are data-generating distributions of inputs and labels respectively. Task\nfollows a task distribution T ∼p(T ). K-shot N-class learning is a typical problem setting in few-shot image\nclassiﬁcation, where there are N image classes each with K examples and K< 10.\n2\nMeta-learning\nFor fast and accurate adaptation to unseen tasks with meta-learning, we need to balance exploration and exploitation. In\nexploration, we deﬁne a complete model search space which covers all algorithms for the task. In exploitation, we\noptimize over the search space, identify the optimal learner and estimate learner parameter. For example, learning-\noptimizers methods proposed in [22, 88] deﬁne an extensive search space of optimizers for model exploration. In [89],\nmean average precision deﬁned as the precision in predicted similarity is a proposed loss function used for model\nexploitation. More exploitation is necessary when data distribution is multi-modal distribution, fat-tailed distribution,\nhighly skewed distribution or others. Efﬁcient exploration strategies are required when the computational cost is high.\n7\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nWe may apply meta-learners to predict performance, provide guidance for efﬁcient exploration and to exclude areas\nwhere superior performance is unlikely, as in [90] and [91].\nOn the other hand, meta-learning models can combine ofﬂine deep learning and online algorithms. In ofﬂine deep\nmodel, we aggregate past experiences by training a deep model on large historical datasets. In online algorithms,\nwe continually adapt a deep ofﬂine model to conduct predictive analysis on few-shot datasets from novel tasks. For\ninstance, memory-based meta-learning model in [92] stores ofﬂine training results in memory so that they can be\nretrieved efﬁciently in online model adaptation. Online Bayesian regression in [76] uses ofﬂine training results to\ninitiate task-speciﬁc parameters in prior distributions and update these parameters continually for rapid adaptation to\nonline streaming data. Based upon pre-trained deep learning models, meta-learning methods adapt to new unseen tasks\nefﬁciently.\nA typical assumption behind meta-learning is that tasks share similarity structure, and model generalization between\ntasks can be performed efﬁciently. Degree of similarity between tasks depends upon the similarity metric function which\nis an important component in the meta-learning objective function. Similarity is explicitly or implicitly incorporated in\nmeta-learning models and similarity of any complexity should be allowed to achieve model generalization between\ndifferent tasks. Reliable adaptation between different tasks relies upon identifying the similarity structure between them.\nIn meta-learning research, primary interest lies in relaxing the requirements upon degree of similarity between tasks\nand improving model adaptivity. Similarity is difﬁcult to measure. First, we should know which part of one task is\nrelated to which part of another task and align the tasks. Second, we should know how the parts of different tasks are\nrelated to each other and estimate a representational model between parts.\nIn this section, we brieﬂy summarize meta-learning frameworks that emerge in recent literature into the following\ncategories: black-box meta-learning [33], metric-based meta-learning, layered meta-learning and Bayesian meta-\nlearning. This classiﬁcation of meta-learning frameworks is not exact and the boundaries are vague between different\nclasses of methodologies. This paper brieﬂy summarizes research directions in meta-learning methods. Every method\nis a combination of several meta-learning routes mentioned in this section. There is no clear boundary between these\nmethods which classiﬁes them into different methodology lines. Components of one method belong to several different\nresearch lines. We review these methods to summarize how these meta-learning routes are integrated into the real\nmeta-learning methods.\nBlack-box meta-learning utilizes neural network as the main focus to improve model generalization capability:\n• Apply neural network to model policy and policy self-improvement.\nUsually policy and policy self-\nimprovement are modelled with neural network. Neural network represents all kinds of policy update\nstrategies including stochastic gradient descent (SGD). An optimal policy update strategy from extensive\nsearch should outperform SGD. Similarity between tasks is implicitly considered in black-box neural network.\n• Activation to parameter [93] generalizes to unseen tasks by only updating the mapping between activation to\nweight parameters in output layers. This mapping captures the relation between feature to classiﬁer output.\nParameters in the feature extraction module remain ﬁxed. Predictive accuracy on trained tasks does not\ndecrease after model adaptation to unseen tasks.\n• AdaResNet and AdaCNN [21] re-design neurons in ResNet and CNN to include task-speciﬁc parameters in\nactivation or weights. Meta-parameters are for representation of the similarity shared by tasks and task-speciﬁc\nparameters are for representation of task-speciﬁc information. In generalization to unseen tasks, only task-\nspeciﬁc parameters are updated to reﬂect upon task data. Memory module stores task features and parameter\nestimates to accelerate the adaptation of task-speciﬁc parameters.\n• Meta-learning is widely applicable in the area of AutoML to accelerate the search for an optimal neural\nnetwork model. For different hyper-parameter combinations, meta-learner can predict model performance\nbased upon previous training experiences. Meta-learner points base learner (AutoML) to search through only\npromising regions in hyper-parameter surface. Besides, for an unseen task, meta-learner can provide a good\ninitialization for model selection and parameter speciﬁcation.\nMetric-based meta-learning relies upon the similarity measure between unseen tasks and trained tasks to ﬁnd the most\nrelevant past model training experiences to refer to:\n• SNAIL [94] includes an attention block which identiﬁes the most relevant item from memory. The attention\nblock measures the similarity shared by tasks and contains only meta-parameters. Attention blocks typically\nconsist of two parts: feature extractor neural network and deep metric neural network measuring task similarity.\nMeta-parameters are in feature extractor blocks and supervised deep metric learning is utilized to train metric\nparameters.\n8\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\n• Relation Network [17] contains meta-parameters in feature extraction neural network and meta-parameters\nin distance metrics. Feature extraction model and distance measure are jointly tuned to represent similarity\nbetween tasks. These metric-based meta-learning methods are very similar in structure but include different\ndegrees of ﬂexibility in training.\n• Prototypical Network [15] measures distance between extracted feature embeddings and these centroids from\ndifferent classes. There are many methods built upon Prototypical Network which show superior performance\nin few-shot image classiﬁcation, as in [11] and [95]. [11] integrates domain adaptation into Prototypical\nNetwork and shifts and scales class centroids in adaptation to unseen tasks. [95] utilizes adaptive margin loss\nin deep metric learning of Prototypical Network to improve classiﬁcation power.\n• Dynamic few-shot [96] integrates a ConvNet-based classiﬁer, a few-shot classiﬁcation weight generator and\na classiﬁer based upon cosine distance metric. Classiﬁcation weight remains ﬁxed across all seen classes\nand can be generalized to unseen tasks. For unseen classes, simply add extra neurons to the output layer and\nuse the few-shot classiﬁcation weight generator to estimate additional parameters. Feature extraction neural\nnetwork and classiﬁcation weight generator for testing data jointly represent similarity shared by different\ntasks. Complexity of this framework may be tuned to match the complexity of real similarity between tasks.\n• MAP [89] (Mean Average Precision) designs a similarity-ranking based measure as the meta-objective function\nto estimate meta-parameters. It measures the difference between predicted similarity and real similarity between\ntasks. For each individual sample in a batch of data, the other samples in the same batch are ranked by predicted\nsimilarity with this individual sample. When processing each individual sample, relevant model experiences\nfrom relevant samples are referred to. In mAP, memory module scheme is used much more often than other\nmethods, since memory module is only consulted on the unit of tasks not on the unit of sample points.\nLayered meta-learning relies upon task decomposition to construct generalization tree and consists of base learner and\nmeta-learners for different generalization scopes:\n• MAML [12] is the most popular methods within GBML (Gradient-based meta-learning) methods. Meta-\nparameter is the initial value of task-speciﬁc parameter on each task. Meta-parameter is updated in meta-\nlearner by minimizing loss function on validation data. Task-speciﬁc parameter is updated in base learner by\nminimizing loss function on training data. MAML and GBML are compatible with all learners optimized using\nstochastic gradient descent. Most deep neural networks can be optimized through gradient back-propagation\nand can use MAML or GBML to improve model generalization capability. Since MAML includes SGD\nupdate in both base learner and meta-learner, MAML introduces second-order derivative of loss function which\ncosts computational time. Later Reptile [97] and FOMAML [60] are proposed as ﬁrst-order approximation of\nMAML. Reptile and FOMAML are faster but less accurate than MAML.\n• Meta-LSTM [5] uses LSTM as meta-learner and is also compatible with any learner optimized by SGD.\nLSTM is one of the most important forms of RNN. LSTM contains memory cells, self-connected recursive\nneurons, multiplicative gates and forget gates. LSTM ensures that gradients do not explode or vanish in\nback-propagation and can account for long-range dependence in sequential data. Memory cells contain critical\npast model training experiences and forget gates ensure that memory do not explode. LSTM can model\ntask similarity of any complexity and self-connected recursive neurons allow self-improvement along the\nmeta-training process. There is correspondence between LSTM cell calculations and stochastic gradient\ndescent. Specifying gates in LSTM properly, calculations in LSTM cell are exactly stochastic gradient descent.\n• R2-D2 and LR-D2 [85] considers using widely applicable base learners such as ridge regression and logistic\nregression. These base learners are of less complexity than deep models, but they are also widely applied in\nreal-life applications. Meta-learner is speciﬁed to be neural network model that has high representation power\nfor task similarity of any complexity. Base learner of lower complexity and Meta-learner of high complexity is\nan efﬁcient way to specify layered meta-learning model. Base learner is fast to train on each few-shot task and\nmeta-learner slowly uncovers shared features of all tasks to update meta-parameter.\n• TPN [98] applies end-to-end transductive learning between meta-training data and meta-testing data. Feature\nextraction model and label propagation model based upon graph are jointly trained to reﬂect the similarity\nbetween unseen tasks and trained tasks. Transductive label propagation model based upon graph is regarded\nas the meta-learner. Data variance is taken into account when modelling similarity metric between samples.\nMeta-parameters are parameters in feature extraction module and parameters in data variance model. Label\npropagation for each task predicts labels jointly for all input data using an explicit solution formula, which\nconstitutes a very efﬁcient base learner.\n• LEO [99] utilizes a dimension reduction encoder network, a relation network and a decoder function. Encoder\nnetwork performs feature extraction, relation network measures similarity between samples with data variance\n9\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\naccounted, decoder network projects features to classiﬁer. For generalization to unseen tasks, parameters in\nall these modules are updated with MAML. Meta-learner provides initial values for these parameters in base\nlearner during generalization to unseen tasks. In the inner loop, base learner considers task training data and\nupdates these parameters from initial values supplied from meta-learner.\nBayesian meta-learning recasts meta-learning to be in the probabilistic framework and integrates Bayesian great minds\nthoughts into meta-learning:\n• Bayesian program learning [61] generates hand-written characters indistinguishable from real hand writings\nusing visual checks. Joint distribution of character types, image-speciﬁc parameters and images are utilized to\nsimulate data indistinguishable from real data. Simulated data is augmentation to the original few-shot dataset\nand contributes to improvement in predictive accuracy in few-shot learning. Joint posterior distribution of\nimage-speciﬁc parameters and character types is updated during model generalization to unseen tasks.\n• AI-GAs [6] consist of meta-learning architecture, learner algorithms and generation of more complex en-\nvironment. AI-GAs consider coevolution between agents and environment. AI-GAs provide solutions for\ncomplex environments which are otherwise unsolvable by training from scratch. AI-GAs contains important\ninteraction between agent and environment. Within evolution, as environment becomes more complex, so does\nthe solver agent. Each small change in environment corresponds to each small improvement in solver agent.\nEvolution of environments guides solver agent to grow and to tackle complex tasks which cannot be solved by\ntranining from scratch. AI-GAs are optimization techniques which use meta-learner to guide base learner to\nﬁnd optimum closer to global optimum.\n• Neural Statistician [100] applies variational autoencoder (VAE) to approximate the posterior distribution of\ntask-speciﬁc context. Given posterior distribution, context corresponding to highest posterior probability\nis the predicted label of input data. From posterior distribution, we can also obtain conﬁdence interval of\nunknow label on input data. VAE is an efﬁcient Bayesian inference tool which not only provides estimation on\nunknown quantity but also the uncertainty in predicted value.\n• LLAMA [101] recasts MAML as probabilistic inference in a hierarchical Bayesian model. It uses ﬁrst-order\nand second-order Laplace approximation to Gaussian distribution in log likelihood function. For other data\ndistributions such as multi-modal distribution, highly skewed distribution or fat-tailed distribution, Laplace\napproximation does not work well. Laplace approximation shows best results for tightly distributed, symmetric\ndistributions such as Gaussian distrbution. All distributions in Bayesian framework are taken to be Gaussian so\nthat log likelihood function can be well approximated by Laplace approximation, which uses point estimation\nat the mode to approximate integral of Gaussian distribution.\n• BMAML [62] replaces SGD (Stochastic gradient descent) in MAML with SVGD (Stein variational gradient\ndescent), where SVGD is an efﬁcient Bayesian inference method combining MCMC and variational inference.\nSVGD does not only work for all learners optimized with SGD. It uses chaser loss based upon SVGD to\nestimate meta-parameter and Bayesian fast adaptation to update task-speciﬁc parameters.\n• VERSA [18] relies upon distributional Bayesian decision theory for amortized variational inference. Posterior\ndistribution of true label is approximated and updated during model generalization to unseen tasks.\n• Under Bayesian framework, not only task-speciﬁc parameters and meta-parameters are regarded as random\nvariables, task can also be viewed as a random variable. Task can be considered as countably inﬁnitely many\nand Chinese restaurant process is used as the prior distribution for task. Posterior distribution of task is\ncontinually updated and maximum posterior estimate of task is provided in the model generalization process.\nAll task-speciﬁc parameters are taken to be functions of task. Estimation and uncertainty measure for task-\nspeciﬁc parameter are derived from posterior distribution of task. Meta-parameters should be updated using\naggregation of all task posterior distributions.\n2.1\nBlack-Box Meta-Learning\nHyperparameter optimization can be achieved through random grid search or manual search [48]. Model search space\nis usually indexed by hyperparameters [22]. In adaptation to novel tasks, hyperparameters are re-optimized using data\nfrom the novel task. Optimizers can be approximated with neural networks or reinforcement learners [88]. Neural\nnetworks can approximate any function with good convergence results. By using neural networks, the optimizer\nsearch space represents a wide range of functions that guarantee better potential optima. For neural network with\nvarying architectures, hyperparameters include number of layers, number of block modules, number of neurons etc.\nThese hyperparameters are discrete and optimization techniques are different from continuous hyperparameters such\nas learning rate, momentum etc. Bayesian optimization, reinforcement learning optimization, genetic programming\noptimization may be employed to optimize both discrete and continuous hyperparameters in deep neural network model.\n10\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nIn [22], optimization is through guided policy search and neural network is used to model policy update strategy which\nis usually stochastic gradient descent. This algorithm searches for the optimal policy update strategy represented by\nneural network instead of using SGD directly. Policy update is formulated as\n∆x ←π(f, {x0, · · · , xi−1}),\nwhere f is a neural network to model policy update strategy π and ∆x, γ is the step size. In SGD, policy update\nis through gradients ∆x = −γ Pi−1\nj=0 αj ▽xj f(xj), where α is a discount factor. In this case, policy update is\napproximated with a neural network and is continually adapted using task data.\nAnother approach is the adaptation of a pre-trained neural network from ofﬂine deep model to unseen tasks, as in ﬁgure\n2. It is commonly used in transfer learning. From this perspective, meta-learning and transfer learning are the same\nconcept. For few-shot unseen tasks, training data is limited so that black-box adaptation is only for a small portion of all\nparameters in neural network model. For similar few-shot image classiﬁcation tasks, feature extraction modules remain\nintact and only classiﬁer parameters are adapted for novel tasks. For unseen tasks with sufﬁciently large annotated\ndataset, black-box adaptation is for almost all parameters, hyperparameters, and even neural network architecture\ndepending on the amount of labelled new data available. For similar tasks, solution models are also similar, therefore\nblack-box adaptation does not need to be for all parameters in the deep model. Only updating task-speciﬁc parameters\nis sufﬁcient for adaptation to few-shot unseen tasks. For vastly different tasks, solution models are also vastly different.\nBlack-box adaptation should cover all of parameters, hyperparameters and network architecture. Unseen task should\ninclude sufﬁciently large labelled dataset to achieve accurate adaptation to vastly different tasks.\nFigure 2: Black-box adaptation. Pre-trained model is an ofﬂine deep learning model on a large historical dataset.\nBlack-box adaptation is applied to adapt the pre-trained deep model to a lightweight novel task.\nIn deep neural network, weights and activations are highly correlated causing severe parameter redundancy in the model,\nas a result we can use a few parameters to predict the others. In unseen tasks, we estimate a few parameters rapidly\nand use the pre-trained predictive mapping to estimate the output directly. The pre-requisite for this approach is that\npre-trained predictive mapping stays ﬁxed across all unseen few-shot tasks and represents shared features of all tasks.\nActivation to parameter [93] proposes using a feedforward pass that maps activations to parameters in the last layer\nof a pre-trained deep neural network. It applies to few-shot learning where the number of categories is large and the\nsample size per category is small.\nIn ofﬂine pre-trained deep model, a deep neural network is trained on a large dataset Dlarge with categories Clarge.\nFew-shot adaptation model is trained on a small dataset Dfew with categories Cfew. Denote the activations before\nfully connected layer as a(xxx). Denote the set of activations for label y as Ay = {a(xxx)|xxx ∈Dlarge ∩Dfew, Y (xxx) = y}.\nDenote the mean of activations in Ay as ¯ay. Denote the parameter for category y in fully connected layer as wy and wy\nis used to compute classiﬁer output directly. The pre-trained mapping from activations to parameters in deep neural\nnetwork is\nφ : ¯ay →wy.\nActivation sy is sampled from Ay ∪¯ay with probability p to be ¯ay and 1 −p to be uniform in Ay. Pre-trained mapping\nfrom activation to classiﬁer parameter can be re-written as φ : sy →wy. Deﬁne Slarge = {s1, · · · , s|Clarge|}. The\npre-trained mapping φ is estimated by minimizing cross-entropy loss function:\nL(φ) =\nX\n(xxx,y)∈Dlarge\nESlarge[−φ(sy)a(xxx) + log\nX\nk∈Clarge\nexp {φ(sk)a(xxx)}],\nmaxmizing probability of belonging to the right class and minimizing probability of being in the wrong classes. In\nfew-shot adaptation of pre-trained mapping φ, deﬁne C = Clarge ∪Cfew and D = Dlarge ∪Dfew. This is a typical\nsetting in adaptation of pre-trained deep model, a large annotated dataset Dlarge is used to pre-train a deep model and to\nestimate these generalizable ﬁxed mappings from parameter to classiﬁer output. Deﬁne S = {s1, · · · , s|C|}. Category\nprediction in few-shot data is represented as the probability of xxx in category y:\nP(y|xxx) = exp {ES[φ(sy)a(xxx)]} /\nX\nk∈C\nexp [ES (φ(sk)a(xxx))] .\n11\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nPre-training of a deep neural network consumes time, memory and electricity. In order for the pre-trained deep model\nto be accurate, a large labelled dataset is required. The degree of similarity between tasks should be reasonably high for\nprecise adaptation of the pre-trained mapping. In image classiﬁcation tasks, prediction accuracy is high implying that\npre-training the ofﬂine deep learner is worthwhile. Nowadays there are many well-known large labelled datasets to\npre-train deep model for typical AI missions such as computer vision or language processing. The pre-trained model is\navailable online for free use so that they can be adapted easily to generalize to similar smaller tasks.\nIn order to adapt pre-trained deep neural network, we can parameterize it with meta-parameter and task-speciﬁc\nparameters so that the network itself allows for rapid adaptation. Task-speciﬁc parameters are updated by minimizing\nloss function on task training data. Meta-parameter is updated by minimizing generalization error function on multiple\ntasks. Given only few-shot data on unseen task, only a small proportion of parameters can be updated for model\nadaptation. Network model proposed in [21] speciﬁes neural network neurons to be conditionally shifted neurons (CSN),\nwhich contain activations with task-speciﬁc parameters obtained from linear combinations of training experiences in an\nexternal memory module. Activation of CSN is deﬁned to be\nhl =\n\u001aσ(al) + σ(φl),\nl ̸= M,\nsoftmax(al + φl),\nl = M,\nwhere M is the index of output layer in neural network, al is the pre-activation and φl is the layer-wise task-speciﬁc\nconditional shift parameter. Pre-activation refers to the linear combination of previous layer neuron states right before\nactivation operation of this layer. For ResNet block, we can use CSN as activations and stack these blocks to construct\na deep adaptive ResNet model (AdaResNet). Similarly, we can also stack LSTM layers with CSN activations to\nbuild a deep adaptive LSTM model. We may re-design CNN with CSN activations to be a deep adaptive CNN model\n(AdaCNN). Indeed all famous deep neural network models can be modiﬁed to include CSNs for efﬁcient adaptation to\nfew-shot datasets.\nFor task T , training data is denoted as Dtr\nT = {(xi, yi}n\ni=1 and validation data is denoted as Dval\nT\n= {(x∗\ni , y∗\ni }m\ni=1. Base\nlearner is a mapping from input data (xi, yi) to output label ˆyi. Information in neural network can be speciﬁed as a\nlayer-wise amortized error gradient or a direct feedback (DF) which measures layer-wise contribution to the current\ndifference between predicted output label and true label. Higher information implies higher individual inﬂuence of the\nnetwork component upon output label. In black-box adaptation, parameters with higher inﬂuence in most relevant tasks\nare retrieved from external memory module. Meta-learner gθ is an MLP with ith neuron in lth layer conditional upon\ninformation Il,i. For validation input data x∗\nj, task-speciﬁc parameter φl in CSN activation represents the task-speciﬁc\nconditional shift:\nVl,i = gθ(Il,i), αi = softmax(cos(f(x∗\nj), f(xi))),\nφl = [α1, · · · , αn]T [Vl,1, · · · , Vl,n],\nwhere Il,i is the conditioning information for each neuron stored in external memory module, f is an MLP with a linear\noutput layer for memory query, and cos is the cosine similarity between extracted features for memory query. αi is\nthe individual attention mechanism for efﬁcient information retrieval. Experiences in more similar tasks receive more\nweight in retrieved linear combination. Neural network is trained by minimizing the cross-entropy loss for task T :\nLT = P\nj L(ˆy∗\nj , y∗\nj ), minimizing distance between predicted label and true label on validation data. Ada+network\nmodel is computationally efﬁcient with high prediction accuracy in few-shot image classiﬁcation. Ada+network\ncombines conditionally shifted neurons, base learner+meta-learner, external memory module. No meta-learning method\nreﬂects only one meta-learning approach mentioned in this paper. We have no intention to classify these methods\naccurately. We want to use these meta-learning methods as examples to illustrate how these meta-learning approaches\nare used to solve few-shot meta-learning missions.\nTable 1: Testing accuracy of black-box adaptation meta-learning methods on 5-way 5-shot miniImageNet classiﬁcation.\nMethod\nAccuracy\n[93] Activation to parameter with neural network\n67.87 ± 0.20%\n[93] with wide residual network WRN\n73.74 ± 0.19%\n[21] AdaCNN with DF\n62.00 ± 0.55%\n[21] AdaResNet with DF\n71.94 ± 0.57%\nBlack-box adaptation applies deep learners at meta-level aggregating all task-speciﬁc information and provides a\ndirection for model adaptation to out-of-distribution tasks. There are many approaches for black-box adaptation such as\nstochastic gradient descent, Bayesian optimization, reinforcement learners, genetic algorithms etc. Table 1 presents\ntesting accuracy of these methods on 5-way 5-shot miniImageNet. In miniImageNet classiﬁcation, wide deep residual\n12\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nnetwork performs better than neural network and CNN. We can see that performance highly depends upon deep model\nspeciﬁcations and the ﬁne tuning of pre-trained deep learner. Performance of black-box adaptation is comparable to\nsimilarity-based meta-learning models in the next section.\nAutoML (Automated machine learning) seeks to identify an optimal neural network model which outperforms human-\ndesigned network. AutoML is about searching through all hyperparameter combinations to ﬁnd a global optimum neural\nnetwork model. At each hyperparameter combination, neural network model is trained to convergence to see whether\nit outperforms current best target. Hyperparameter includes both continuous hyperparameters such as learning rate,\nmomentum etc and discrete hyperparameters regarding neural model architecture. Searching efﬁciency and training\nefﬁciency are two main factors to work on for improving global optimum found by AutoML. Meta-learning can be\nintegrated into AutoML to improve either searching efﬁciency or training efﬁciency.\n[90] uses a meta-learner to aggregate learning curves in neural network training process. A parametric model or\nnonparametric model is applied to model various learning curves and to predict ﬁnal prediction accuracy at training\nconvergence from early-stage learning curve. At each hyperparameter combination, after only some training iterations,\nmeta-learner provides an estimation of prediction accuracy at convergence, and whether this combo beats current\nbest performance. It improves training efﬁciency at each hyperparameter combination. Accurate prediction model of\nlearning curve is vital in this algorithm and both parametric and nonparametric models have been developed to improve\nprediction accuracy.\n[91] uses a meta-learner to aggregate the relation between hyperparameter combination and neural network model\nprediction accuracy. Meta-learner learns to predict network prediction performance for new hyperparameter combination.\nMeta-learner supplies hyperparameter combinations which are likely to outperform current best results for the base\nlearner to explore. Base learner is to train neural network model at each hyperparameter combination, which is viewed\nas one task. This meta-learning system increases searching efﬁciency of global optimum hyperparameter setting in\nAutoML. Prediction accuracy in meta-learner is vital for improving searching efﬁciency.\n[102] uses a meta-learner to aggregate datasets and their corresponding optimal machine learning model. Meta-learner\nextract features from datasets and use these features to determine the best machine learning model for it. In aggregation\nof previous training experience, meta-learner learns the set of dataset features which are closely related to optimal\nmodel selection and parameter initialization. For an unseen task, meta-learner computes the set of dataset features\nand provides an optimal model recommendation with initial parameters speciﬁed. Meta-learner determines the most\nappropriate model and parameter initialization after extracting relevant features from dataset. This algorithm improves\nsearching efﬁciency of AutoML by providing a good initial model.\n[57] develops a progressive neural architecture search algorithm which adds units to neural network model through\ngenetic programming and decision making. This algorithm starts from a simple neural network, adds neurons or\nweights to network, measures network prediction accuracy after adding each unit, then chooses the action with the\nbest performance gain. Optimal network mutates to several other network models of the same complexity, called\nchildren. Then children continue the progressive process of further adding more units. Meta-learner aggregates network\nmodel and action choice at each step. Meta-learner learns to predict action performance based upon current network\nstate. Meta-learner recommends actions which are likely to bring the best performance outcome to base learner. This\nalgorithm improves search efﬁciency at each step by predicting performance gain of each action choice. [56] is also a\nprogressive neural architecture search algorithm like [57]. It starts from a null model, uses genetic programming and\nthe neural network grows to be more complex after adding more units to it. Progressive AutoML algorithms never\nlook back or delete previous actions, never evaluate several previous steps jointly to see whether they are still optimal.\nStill, progressive search improves searching efﬁciency in AutoML although it may not reach the global optimal neural\nnetwork model. Isn’t it interesting, forgetting and regret are time-consuming but are the keys to global optimum.\n2.2\nMetric-Based Meta-Learning\nIn this section, we review a class of meta-learning models which depend upon the similarity measure between unseen\ntasks and previously learned models. From [1], end-to-end training by directly referring to experiences of most similar\ntasks is also an efﬁcient scheme for meta-learning. As in ﬁgure 3, distances between an unseen task and the centroids of\nall trained models are compared and the most similar experiences are applied directly to unseen task. The pre-requisite\nis that unseen tasks share similarity structure with previous tasks. Otherwise unseen tasks are not identiﬁable under the\nframework of similarity-based meta-learning methods.\nA subclass of methods resort to an additional external memory module with an efﬁcient critical information retrieval\nmechanism. LSTM itself is a complete meta-learning system with an internal memory cell, a self-recursive neuron for\nself adaptation to unseen tasks, and gates to control information ﬂow. We can also add an external memory module to\nany deep learning model so that they can efﬁciently use previous experiences to accelerate task solving. An external\n13\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nFigure 3: Metric-based meta-learning. ci is the centroid of class i. Distances between a novel task and centroids are\ncompared. A new sample joins the closest class.\nmemory module has several features: information storage and indexing, information retrieval and query, information\ndeleting and forgetting etc. MANN (Memory-augmented neural networks) in [59] adds an external memory module\nto store most important training experience for query later in unseen tasks. Memory Mod proposed in [92] adds an\nexternal memory module M to all kinds of classiﬁcation network so that training experience can be used later to\naccelerate model convergence. Memory module in ’Memory Mod’ stores model training results and has two properties:\nan efﬁcient nearest neighbor search mechanism and a memory update rule. Memory module M consists of three\ncomponents: a matrix of memory keys K|M|×key−size, a vector of memory values V|M| and a vector of memory\nitem ages A|M|. Memory keys are the query index through which we can quickly ﬁnd the most relevant task training\nexperience. Memory values are task training experiences that are closely related to model output, inference of important\nparameters etc. The nearest neighbor of a query q from an unseen task in memory module M is\nNN(q, M) = argmaxi cos(q, K[i]),\nwhere cos is the cosine similarity measure and K[i] is the key value at location i. The k nearest neighbor of query\nq in M is deﬁned as NNk(q, M) = (n1, n2, · · · , nk), where K[ni] is the key with ith highest similarity to query q.\nWe may also use supervised deep metric model to estimate an optimal metric which assigns highest similarity to truly\nrelevant tasks. Memory update ﬁnds items with longest ages and writes to these item locations randomly. Items which\nhave been recently used may also be replaced with new experience, since they may not be referred to for a long time.\nIdeally we expect to store all training experiences in memory module. However, memory space is limited, we must\nlearn to forget to avoid memory explosion. Besides, searching for the most relevant experience may be of low efﬁciency\nif there is too much memory to search through. Memory module can be added to any neural network model such as\nCNN, LSTM and ResNet. Memory module is simple, efﬁcient, useful and is compatible with all machine learning\nmodels. For any machine learning model, we only need to specify the most important training experience to be stored\nin memory and how to query them using similarity metrics. By adding a memory module with memory update and\nefﬁcient nearest neighbor search, it enables all classiﬁcation networks to rapidly and accurately adapt to novel tasks.\nFor out-of-distribution tasks, we accumulate experience in external memory module for a long time so that there will be\nsufﬁcient experience to use for solving these rarely seen tasks.\nAnother subclass of methodology is to apply an embedding mapping f from input x to feature z, which can then\nbe used to compute a similarity measure between tasks. Embedding mapping f performs dimension reduction for\nhigh-dimensional input x. Similarity measure can be output from an attention block, distance between query input and\nprototype of each class, or complex similarity score. Choices of embedding mapping function and similarity measure\nmay affect performance of these methods. For each dataset, more options of feature extractor and distance metric\nfunction should be explored to ﬁnd model speciﬁcations producing higher prediction accuracy.\nSNAIL (Simple Neural Attentive Meta-Learner) [94] applies temporal convolutions in a dense block to extract features\nfrom images. An attention block using soft attention function is applied to identify critical features from past experiences\nand make classiﬁcation. SNAIL processes sequential data and current neuron state only depends upon past states not\nfuture ones. Attention mechanism is embedded within neural network to identify the most relevant previous states to\ncompute current states. For high-dimensional input, attention mechanism is very efﬁcient in dimension reduction and\nimprove data ﬂow efﬁciency in neural network. Attention mechanism can be used before linear combination of previous\nstates and before activation of linear combination. In sequence-to-sequence tasks such as language translation, inputs\nfor task Ti are denoted as {xs}Hi\ns=1. Loss function is Li(xs, as), where input feature is xs ∼Pi(xs|xs−1, as−1) and\noutput from attention block is as ∼π(as|x1, · · · , xs; θ). Meta-parameter in attention block θ is updated by minimizing\nloss function:\nmin\nθ\nETi∼p(T )[\nHi\nX\ns=1\nLi {xs, as(x1, · · · , xs; θ)}].\n14\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nIn 5-way 5-shot miniImageNet classiﬁcation task, SNAIL shows testing accuracy of 68% [94] comparable to other\nmeta-learning methods reviewed in black-box adaptation. Obviously SNAIL is not only for few-shot tasks and can be\nused for large-sample high-dimensional input as well. SNAIL consists of two parts for dimension reduction: temporal\nconvolution layers and causal attention blocks. Convolution layers reduce dimension through averaging data to extact\ninvariant features. Attention layers extract most relevant previous states to compute current state. SNAIL only has\ndimension reduction layers and ﬁnal output classiﬁer layer. SNAIL can be viewed as modiﬁcations to deep convolutional\nnetwork by embedding attention mechanism between layers and enforcing current state to depend only on previous\nstates to process sequential data.\nIn addition, RN (Relation Network) [17] applies supervised deep metric learning to train a similarity metric between\nextracted features of high-dimensional input. Denote an embedding function (feature extractor) as fφ parameterized by\nφ and a relation function (similarity metric) as gθ parameterized by θ. Relation score ri,j measures the similarity metric\nbetween query xxx∗\nj and training data xxxi:\nri,j = gθ[fφ(xxxi), fφ(xxx∗\nj)],\nwhere feature extractor fφ is ﬁrst applied to input data and then similarity metric gθ is computed. Higher relation score\nindicates greater similarity between query and training data. Parameters in the embedding function and relation function\nare jointly estimated through the minimization of classiﬁcation error function:\nmin\nφ,θ\nn\nX\ni=1\nm\nX\nj=1\n{ri,j −I(yi = y∗\nj )}2,\nwhere I(yi = y∗\nj ) is 1 if yi = y∗\nj and 0 otherwise. Minimization of classiﬁcation error implies that relation score is\nhighest (closest to 1) when two samples are in the same class and lowest (closest to 0) when two samples are in different\nclasses. Relation score itself is a similarity measure between different samples. Trainable parameters in relation score\ninclude parameters in feature extractor and parameters in similarity metric function. Usually metric-based meta-learning\nmethods only include trainable parameters in feature extractor when designing similarity measures. On 5-way 5-shot\nminiImageNet, RN shows prediction accuracy of 65% comparable to SNAIL but lower than black-box adaptation in\n[93] and [21]. Relation network model can be embedded within larger, more complex meta-learning framework.\nSimilarly, prototypical network proposed in [15] is also based upon deep metric learning. Prototype in class k is the\ncentroid ck, average of all data features in this class:\nck =\n1\n|Sk|\nX\n(xxxi,yi)∈Sk\nfφ(xxxi),\nwhere Sk is the set of all data in class k. Distance function g measures the distance between data and centroid of each\nclass. Softmax mapping from distance metric g to class probability is\npφ(y = k|xxx) =\nexp(−g[fφ(xxx), ck])\nP\nk′ exp(−g[fφ(xxx), ck′]).\nDifferent from relation network, distance metric g contains no trainable parameter. All parameters are located in feature\nextraction model. Finally, we minimize the negative log-probability loss function J(φ) = −log pφ(y = k|xxx) via SGD.\nMinimization of this loss function maximizes the probability for the input data to be assigned to its true class. The setup\nin prototypical network is simple, efﬁcient and can be integrated into various more complex meta-learning framework.\nAM3 (Adaptive Modality Mixture Mechanism) [103] combines multi-modal learned image and text data to perform\nfew-shot learning. AM3 uses semantic text to help with few-shot image classiﬁcation. TRAML (Task-Relevant\nAdaptive Margin Loss) [95] is based upon prototypical network with adaptive margin loss. AM3 data used with\nTRAML algorithm achieves better few-shot image classiﬁcation than algorithms purely based upon images. The gain\ncomes from adaptive margin loss and multi-modal image data in few-shot tasks. TRAML achieves 67.10 ± 0.52% on\n5-way 1-shot miniImageNet and reaches 79.54 ± 0.60% on 5-way 5-shot miniImageNet. By combining multi-modal\nlearning (image+text) and metric-based meta-learning (Prototypical Network), AM3+TRAML provides better prediction\naccuracy.\nDAPNA (domain adaptation prototypical network with attention) [11] integrates domain adaptation into Prototypical\nNetwork and shifts and scales class centroids in adaptation to unseen tasks. DAPNA makes more components in\nprototypical network to be task-dependent in order to gain better prediction accuracy in few-shot tasks. In DAPNA, for\neach unseen task, centroids in prototypical network is now task-dependent. In addition, DAPNA utilizes margin disparity\ndiscrepancy to train deep distance metric which also contributes to higher prediction accuracy. DAPNA compares the\nperformance of few-shot meta-learning methods and lists out the feature extraction models they use. DAPNA uses\n15\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nwide resnet to be feature extraction model. Compared to deep resnet and deep convolutional network, generally wide\nresnet [104] has better performance than resnet, than deep convolutional network. Better feature extraction model also\nimproves few-shot image classiﬁcation accuracy.\n[105] also combines similarity-based metric learning and meta-learning. The setup of [105] is similar to prototypical\nnetwork [15], where cluster centroids are benchmarks in classiﬁcation. [105] is the ﬁrst paper which applies Mahalanobis\ndistance to make few-shot classiﬁcation within meta-learning framework. Mahalanobis distance is popular in deep\nmetric learning. It contains a parameter matrix which includes many trainable parameters. Here in [105] Mahalanobis\ndistance between extracted features xxxi and xxxj is deﬁned as\ng(xxxi,xxxj) = ∥xxxi −xxxj∥2\n2\nσ2\n,\nwhere cluster-wise data variance σ2 is estimated and updated in adaptation to unseen tasks. In Mahalanobis distance,\nparameter matrix is set to be an inversion of cluster-wise variance. Considering cluster-wise data variation leads to\nmore accurate classiﬁcation prediction especially when data variation is vastly different in different clusters. In addition,\nin feature extraction model, [105] proposes a relative-feature extractor in addition to absolute deep-feature extractor\nfφ. Relative-feature extractor is a modiﬁed version of usual feature extraction model. It ensures that the number of\nextracted features is less than sample size for few-shot tasks. Few-shot tasks use few-shot high-dimensional input where\nthe number of input features is greater than sample size. Over-ﬁtting occurrs when the number of extracted features is\ngreater than sample size. To avoid over-ﬁtting, dimension of relative-feature extractor does not exceed sample size in\nfew-shot novel tasks. However, generally higher dimension of extracted features will improve classiﬁcation prediction\naccuracy in supervised deep metric learning. Usually in loss function, a regularization constraint is imposed upon\nfeature extraction model to limit its complexity so to avoid over-ﬁtting. Usually there is no strict constraint that the\nnumber of extracted features is no greater than sample size.\nLastly, TADAM [84] is an extension of prototypical network and is also based upon supervised deep metric learning.\nOn mimiImageNet 5-shot 5-way, TADAM shows testing accuracy of 76%, greater than SNAIL, RN and prototypical\nnetwork. Similar to prototypical network, a softmax function is applied to compute the probability of belonging to class\nk:\npφ,α(y = k|xxx) = Softmax(−αg[fφ(xxx), ck]),\nwhere g is a distance metric with no trainable parameter, α is the temperature parameter. Parameters φ and α are jointly\nestimated by minimizing the class-wise cross-entropy loss function Jk(φ, α):\nX\nxxxi\n\n\nαg[fφ(xxxi), ck] + log\nX\nj\ne−αg[fφ(xxxi),cj]\n\n\n.\nBy introducing an additional parameter α in softmax function and through joint optimization of embedding parameter\nφ and metric hyperparameter α, prediction accuracy of TADAM on 5-way 5-shot miniImageNet is 8% greater than\nprototypical network. TADAM applies task-dependent centroids in prototypical network. Given centroids, TADAM\nuses fully-connected residual neural network and task-dependent centroids to model scaling and shifting parameter\nof features extracted from previous layer. All layers in the feature extractor of TADAM are task-dependent, therefore\ncentroids are task-dependent as well. By allowing more components in prototypical network to be task-dependent,\nTADAM achieves better adaptation to unseen task and shows higher prediction accuracy per task.\n[96] proposes Dynamic Few-Shot algorithm which consists of a ConvNet-based classiﬁcation model, a few-shot\nclassiﬁcation weight generator and a cosine-similarity based classiﬁer. Compared with former similarity metric based\nmethods, it contains an additional classiﬁcation weight generator which adapts ConvNet parameters to novel tasks. It\ncombines black-box adaptation with similarity-based approach and an external memory module. ’Dynamic Few-Shot’\npreserves model performance upon all trained tasks and seeks higher prediction accuracy on unseen few-shot tasks.\nConvNet trained parameters and their corresponding class-wise feature centroids are all stored in the external memory.\nFor an unseen task with unseen image classes, attention mechanism is applied to form a linear combination of ConvNet\nparameters from most similar tasks and classes. In ﬁnal layer classiﬁer, extra neurons corresponding to unseen image\nclasses are added, and link weights come from the linear combination of ConvNet parameters created by attention\nmechanism.\nTraining data contains Ntr base categories each with K examples. Training data is denoted as Dtr = ∪Ntr\nb=1{xb,i}K\ni=1.\nTesting data contains Nts novel categories each with K examples. Testing data is denoted as Dtest = ∪Nts\nn=1{x∗\nn,i}K\ni=1.\nEmbedding feature z∗\nn = {z∗\nn,i}K\ni=1, where z∗\nn,i = fφ(x∗\nn,i). Denote Wtr as the classiﬁcation weight of base categories.\nGenerate a classiﬁcation weight vector for novel categories W ∗\nn = G(z∗\nn, Wtr|θ) using testing data features and base\ncategories weights. Denote W ∗\nts = {W ∗\nn}Nts\nn=1 as a classiﬁcation weight vector of novel categories. Let W ∗= Wtr∪W ∗\nts.\n16\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nClassiﬁcation weight is W ∗= (w∗\n1, w∗\n2, · · · , w∗\nN). Raw classiﬁcation score of query z is based upon cosine similarity:\ngk = τ cos(z, w∗\nk), where k = 1, · · · , N and τ > 0. Probability of belonging to class k is pk = Softmax(gk).\nEmbedding parameter φ and parameter in classiﬁcation weight generator θ are jointly estimated through the minimization\nof loss function:\nmin\nφ,θ\n1\nNtr\nNtr\nX\nb=1\n1\nK\nK\nX\ni=1\nLφ,θ(xb,i, b),\nwhere xb,i is the ith training example in category b. Minimization of loss function maximizes probability of xb,i\nbelonging to its true class b and minimizes probability of wrong classes. Feature extractor options and classiﬁcation\nweight generators may be explored to achieve higher adaptation performance. ’Dynamic few-shot’ designs a unique\nway of adapting pre-trained deep model to unseen tasks: by adding extra neurons for unseen classes to ﬁnal layer\nclassifer, by generating weights for corresponding extra links using attention mechanism to extract parameters from\nexternal memory module. It modiﬁes both network architecture and weight parameters to adapt pre-trained neural\nnetwork. For adaptation to vastly different tasks, a greater portion of parameters and hyperparameters should be allowed\nto change during adaptation to unseen tasks.\nIn addition, distance metric measure may be designed to estimate similarity structure between tasks, as in [89]. It\nproposes a similarity ranking based measure mAP (mean Average Precision). ’mAP’ identiﬁes most similar samples for\neach sample, and use model experiences from these most similar samples to accelerate training on each sample. Rather\nthan searching for relevant experience to refer to when training each task, ’mAP’ performs this process much more\nfrequently by doing so for each sample within task. Support set is denoted as S = {(xxx1, y1), · · · , (xxxN, yN)}. Deﬁne\ninput data B = (xxx1, · · · ,xxxN). Let Rxxxi = {xxxj ∈B : yj = yi} be the set of all samples in class yi. Let Oxxxi be the\nranking of predicted similarity between xxxi and the other points in B, where Oxxxi\nj is the jth element in Oxxxi with the jth\nhighest similarity to xxxi. Let P(j,xxxi) be the proportion of points that are truly relevant to xxxi in the ﬁrst j items in Oxxxi.\nAverage Precision (AP) of this ranking is deﬁned as\nAPxxxi = |Rxxxi|−1\nX\nj:O\nxxxi\nj ∈Rxxxi\nP(j,xxxi),\nwhere P(j,xxxi) = j−1|{k ≤j : Oxxxi\nk ∈Rxxxi}|.\nMean Average Precision (mAP) is the mean of AP for all points: mAP = |B|−1 P\ni∈B APxxxi. ’mAP’ measures quality\nof ranking samples based upon predicted similarity, since samples from same class should exhibit highest similarity\nand samples from different classes should show lowest similarity. Higher ’mAP’ implies that predicted similarity is\nmore reliable. We may use supervised deep metric learning to ﬁnd an optimal similarity measure which maximizes\n’mAP’. Let ϕφ(xxxi,xxxj) be the cosine similarity between xxxi and xxxj: ϕφ(xxxi,xxxj) = cos[fφ(xxxi), fφ(xxxj)]. Denote Pxxxi as\nthe set of all points with predicted similarity to xxxi. Denote N xxxi as the set of all points without predicted similarity to xxxi.\nIndicator yi\nkj is 1 if ϕφ(xxxi,xxxk) > ϕφ(xxxi,xxxj) and 0 otherwise. For each query xxxi, score function is deﬁned as\nFφ(B, y) = P\nxxxi∈B Fxxxi\nφ (B, y), where\nFxxxi\nφ (B, y) =\n1\n|Pxxxi||Nxxxi|\nP\nk∈Pxxxi\\i\nP\nj∈Nxxxi yi\nkj {ϕφ(xxxi,xxxk) −ϕφ(xxxi,xxxj)} .\nMaximization of score function estimates the optimal ranking most compatible with predicted similarity ranking for\neach sample. Indicator pi\ng is 1 if xxxg is truly relevant to query xxxi and −1 otherwise. Denote pi to be a vector of all pi\ng\nand denote ˆpi to be a vector of all predicted indicator ˆpi\ng. AP loss for query xxxi is formulated as\nLxxxi(pi, ˆpi) = 1 −\n1\n|Pxxxi|\nX\nj:ˆpi\nj=1\nP(j,xxxi).\nmAP loss is the mean of AP loss over all query points. Minimization of AP loss maximizes accuracy of predicted\nsimilarity, ie samples from the same class have highest similarity and samples from different classes have lowest\nsimilarity. Loss-augmented label prediction for query xxxi is a linear combination of score function and AP loss:\nmax\nˆyi\nn\nFxxxi\nφ (B, y) −ϵLxxxi(pi, ˆpi)\no\n,\nwhere ϵ > 0. Simultaneous maximization of score function Fxxxi\nφ (B, y) and minimization of AP loss Lxxxi(pi, ˆpi)\nestimates predicted similarity such that samples from the same class have highest similarity and samples from different\nclasses have lowest similarity.\n17\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nTable 2: Testing accuracy of metric-based meta-learning methods on 5-way 5-shot miniImageNet classiﬁcation.\nMethod\nAccuracy\n[14] Matching Net\n60.0%\n[94] SNAIL\n68.88 ± 0.92%\n[17] Relation Net\n65.32 ± 0.70%\n[15] Prototypical Net\n68.20 ± 0.66%\n[105]\n70.91 ± 0.85%\n[95] Prototypical Net+TRAML\n77.94 ± 0.57%\n[95] AM3+TRAML\n79.54 ± 0.60%\n[11] DAPNA\n84.07 ± 0.16%\n[84] TADAM with α, AT and TC\n76.7 ± 0.3%\n[84] TADAM without tuning\n74.2 ± 0.2%\n[96] Dynamic few-shot with C128F feature extractor\n73.00 ± 0.64%\n[96] with ResNet feature extractor\n70.13 ± 0.68%\n[96] with cosine classiﬁer and attention based weight generator\n74.92 ± 0.36%\n[96] with cosine classiﬁer and no weight generator\n72.83 ± 0.35%\n[89] mAP-SSVM\n63.94 ± 0.72%\n[89] mAP-DLM\n63.70 ± 0.70%\nFrom table 2 which presents testing accuracy of similarity-based methods on 5-way 5-shot miniImageNet, we can see\nthat TADAM and DAPNA show highest accuracy. Both TADAM and DAPNA are modiﬁcations from prototypical\nnetwork by making more components in it to be task-dependent so that the algorithm ﬁts each unseen task better. On\naverage, performance of similarity-based methods is slightly better than black-box adaptation which requires heavy\npre-training computation. Similarity-based methods are more ﬂexible and allow choosing from various options of\nfeature extractor, similarity measure, loss function and hyperparameters. Similarity-based methods require ﬁne tuning\non these choices for best adaptation performance. Better feature extractor, better similarity measure, more discriminative\nloss function and more variable hyperparameters generally improve prediction accuracy on few-shot tasks.\n2.3\nLayered Meta-Learning\n[1] proposes meta-learning, base layer (base learner), meta layer (meta-learner), meta-meta layer etc. This line of\nmethodology is referred to as layered meta-learning here in this paper. It may have other names elsewhere. [106]\nprovides an overview of two-level few-shot meta-learning models which consist of a base model that learns rapidly\nfrom few-shot data, and a meta-model that optimizes the base learner across few-shot tasks. As in ﬁgure 4, base\nlearner is designed for solving each task. Base learner contains two parts of parameters: task-speciﬁc parameters\nand meta-parameters. Base learner updates task-speciﬁc parameters in adaptation to different tasks. Meta-learner\naccumulates experiences from multiple tasks, mines their shared features, minimizes loss function on validation data to\nminimize generalization error and updates meta-parameters in base learner. Base learner learns task-speciﬁc features\nby updating task-speciﬁc parameters in adaptation to each unseen task. Meta-learner models features shared by all\ntasks, guides base learner to better generalize to unseen tasks, maximizes generalization capability of base learner. Base\nlearner supplies task-speciﬁc information and generalization error on task validation data to meta-learner. Meta-learner\naggregates task-speciﬁc information to learn features shared by all tasks and minimizes generalization error of base\nlearner to update meta-parameters. Meta-learner supplies updated meta-parameters to base learner. Communication\nbetween base learner and meta-learner can be designed to be quite complicated to allow closer cooperation between\nbase layer and meta layer. Meta layer (Meta-learner) is used to guide training of base layer (base learner). Meta meta\nlayer (meta-meta learner) may be applied to guide training of meta layer (meta-learner).\nUsually there is tradeoff between model accuracy on training data and model accuracy on validation data. Model\naccuracy on training data evaluates quality of model estimation. Model accuracy on validation data measures gen-\neralization capability of deep model. That is, usually there is tradeoff between model estimation quality and model\ngeneralization capability. Overﬁtting refers to the case where model generalization capability is very low and model\nestimation quality is very high. Base learner concentrates upon efﬁcient and accurate model estimation on each unseen\ntask. Meta-learner focuses upon minimizing generalization error of base learner. By introducing base learner and\nmeta-learner, layered meta-learning approach separates model generalization from model estimation and allows the\ncombination of different variations in adaptation speciﬁcations and estimation procedures. Optimization on base learner\nmaximizes quality of model estimation and optimization on meta-learner maximizes model generalization capability.\nThrough optimization on layered meta-learning, both model estimation quality and model generalization capability\n18\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nFigure 4: Learner and meta-learner framework of meta-learning. Base learner is trained on each task. Meta-learner\nupdates task-speciﬁc components in base learner for adaptation across different tasks.\nare maximized. In layered meta-learning, there is no tradeoff between model estimation and model generalization\ncapability. Meta-LSTM [4], Meta Network (MetaNet) [23], MetaOptNet [81], Meta-SGD [16] and MAML [12] are all\nmethods under this meta-learning framework. Base learner is designed for each task and can be speciﬁed as a machine\nlearning algorithm or a statistical model. The goal of few-shot meta-learning is to ﬁnd an efﬁcient and accurate solution\non each task. As a result, base learner should be efﬁcient and accurate on each task. Meta-learner can be slow to\naggregate model training experiences on multiple tasks. In MetaOptNet [81], base learner is speciﬁed to be a convex\nlinear classiﬁer and meta-learner is posited as a neural network. MetaOptNet shows superior prediction performance in\nminiImageNet classiﬁcation.\nCommunication between base learner and meta-learner should be efﬁcient, supporting effective adaptation of base\nlearner to unseen tasks. More complex communication leads to increased complexity in communication but may help\nreduce training time of base learner. [107] considers the setup of meta-learning under federated learning where data\nprivacy is of primary concern. Data privacy limits the degree of communication between base learner of individual\ntasks and meta-learner across all tasks. [108] uses meta-learner to adapt feature transformation matrix module and\npixel-wise prediction module in a single model for image cropping under pre-speciﬁed aspect ratio.\nGBML (Gradient-based meta-learning) is a group of meta-learning methods that update learner parameters using\nstochastic gradient descent to provide efﬁcient model generalization. In GBML, base learner is speciﬁed to include both\ntask-speciﬁc parameters and meta-parameters. Here meta-parameters are the initial values of task-speciﬁc parameters.\nMeta-learner minimizes the sum of loss function on validation data across all tasks to compute meta-parameters.\nMeta-learner provides meta-parameters to base learner as good initial values for task-speciﬁc parameters. On each\nfew-shot task, after only few rounds of SGD iterations, task-speciﬁc parameters are updated to achieve sufﬁciently high\nprediction accuracy on the task. Meta-learner provides good initial task-speciﬁc parameters to base learner so that base\nlearner meets high prediction accuracy efﬁciently after only few rounds of SGD updates on task-speciﬁc parameters.\nGBML only applies to similar tasks with similar data structure. For example, in few-shot image classiﬁcation tasks,\nunseen tasks have the same data structure as trained tasks with the same number of classes and same number of images\nper class. Unseen tasks should also be few-shot image classiﬁcation tasks and images should have the same number of\npixels as trained tasks. In transfer learning, generalization to vastly different tasks is difﬁcult and may cause negative\ntransfer leading to inferior performance. In meta-learning, researchers have been trying to develop meta-learning\nframework that generalizes better and solves vastly different tasks.\nTypical GBML methods include MAML [12], Reptile [97], FOMAML [60], HF-MAML [109], TMAML [110],\nES-MAML [111] and PROMP [112]. GBML is applicable to any learner optimized with stochastic gradient descent.\nReptile [97], FOMAML [60] and HF-MAML [109] are ﬁrst-order approximations of MAML. They are more efﬁcient\nthan MAML in training but are less accurate than MAML in prediction accuracy. TMAML [110] and PROMP [112]\npropose low-variance and high-quality estimators of gradient and Hessian, which also improve training process of\nmeta-learning framework. ES-MAML [111] applies evolution strategies (ES) in the searching process for a globally\noptimum solution and extends MAML to nonsmooth adaptation operations. GBML is widely applied for generalization\nof neural network to complex tasks.\nMAML (Model-Agnostic Meta-Learning) [12] is applicable to any learner that can be optimized with SGD (Stochastic\nGradient Descent). It is the most famous meta-learning method in recent literature. MFR (Meta Face Recognition)\n[113] applies MAML to generalize face recognition model across different human races. [113] regards different\nhuman races as different domains. Generalization between domains can be easily conducted through direct application\nof well-trained deep model with domain adaptation embedded. GBML can be integrated within any deep model\nto maintain model estimation quality and gain additionally model generalization capability. After integration with\nGBML, deep model maintains high prediction accuracy on in-distribution tasks and has higher prediction accuracy on\n19\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nout-of-distribution tasks. [114] integrates MAML in deep neural network for scene-adaptive video frame interpolation.\nThere are many deep learning applications where integration with GBML improves deep model performance and\nreaches new state-of-the-art accuracy in that application area. We will review Bayesian extensions of MAML in next\nsection about Bayesian meta-learning framework and integrated learning frameworks based upon MAML in later section\nabout applications of meta-learning methodology. Base learner is denoted as hθ parameterized by meta-parameter θ.\nTask-speciﬁc parameter is denoted as φ. Task Ti follows task distribution p(T ). In the inner loop of MAML, base\nlearner is trained where task-speciﬁc parameter φ is updated using\nφi = θ −α ▽θ LTi(hθ),\nwhere α is the trainable step size parameter or learning rate parameter, θ is the meta-parameter which is the initial\nvalues of task-speciﬁc parameter φ in base learner. LTi(hθ) is loss function on task training data. ▽θLTi(hθ) is training\ndata loss gradient evaluated at meta-parameter θ. In the outer loop, meta-learner is trained where meta-parameter θ is\nupdated using (meta-update)\nθ ←θ −β ▽θ\nX\nTi∼p(T )\nLTi(hφi),\nwhere β is the step size or learning rate. LTi(hφi) is loss function on task validation data evaluated at trained task-\nspeciﬁc parameter φi for task Ti. Trained task-speciﬁc parameter φi is a function of its initial value (meta-parameter) θ.\nIn this meta-update, by chain rule, derivative with respect to θ includes loss gradient and derivative ∂φi/∂θ, which leads\nto higher-order derivative of loss. MAML is applicable to RL (reinforcement learning) tasks as well. After deﬁning loss\nfunction, and specifying task-speciﬁc parameters and meta-parameters, we can easily write out MAML framework\nfor few-shot RL tasks. Base learner updates task-speciﬁc parameter φi using task training data Ti and meta-learner\nupdates meta-parameter θ based upon all task validation data. In MAML, both base learner and meta-learner are SGD\noptimizers which ensure efﬁcient adaptation to novel tasks. [115] provides valuable empirical experiences for training\nMAML framework, since in MAML higher-order gradient of loss function is included in parameter update which may\ncause instability in training.\nMeta-SGD [16] also belongs to GBML and has the same structure as MAML. But meta-SGD tunes the base learner\nSGD step size α jointly with meta-parameter θ. In the meta-learner, step size α and meta-parameter θ are jointly\nupdated using (meta-update)\n(θ, α) ←(θ, α) −β ▽(θ,α)\nX\nTi∼p(T )\nLTi(hφi),\nwhere β is step size of meta-learner SGD iteration and α is step size of base learner SGD iteration. Trained task-speciﬁc\nparameter φi is a function of both its initial values (meta-parameter) θ and SGD step size α. Similar to MAML,\nMeta-SGD also includes higher-order gradient of loss function in meta-update. Training art of meta-SGD should\nbe similar to MAML and experiences listed out in [115] should be helpful. On 5-way 5-shot miniImageNet, testing\naccuracy of meta-SGD is 64% which is 0.9% greater than MAML.\nIn addition, Reptile [97] belongs to GBML and also has the same structure as MAML. But the meta-update on\nmeta-parameter θ is based upon ﬁrst-order approximation to loss gradient so to avoid computation of higher-order\nderivative on loss function. MetaPix [116] applies Reptile combined with Pix2PixHD [117] in adapting human pose\nskeletons from one person to another person standing in slightly different backgrounds. Meta-parameter θ is estimated\nthrough the minimization of expected loss function minθ ET [▽θLT (hθ)] . Episodic training is applied and sampled\ntasks are denoted as Ti, i = 1, 2, · · · , J. Task-speciﬁc parameters φi are neural network weights and they are updated\nusing SGD:\nφi = SGD(LTi, θ, k),\nwhere k is the number of gradient iterations taken to update neural network weights φi from its initial values (meta-\nparameter) θ. In the meta-learner, meta-paramater θ is updated using (meta-update)\nθ ←θ + ϵ 1\nJ\nJ\nX\ni=1\n(φi −θ),\nwhere ϵ > 0 is step size or learning rate parameter and 1\nJ\nPJ\ni=1(φi −θ) is an approximation to ﬁrst-order loss gradient.\nFor Euclidean loss which uses L2 norm to measure the distance between predicted value and true value, ﬁrst-order\nloss gradient looks like this formula. It does not compute the derivative of ∂φi/∂θ which introduces computation of\nhigher-order loss gradient. MAML, Reptile and Meta-SGD are all gradient-based meta-learning methods (GBML). They\nshare similar structure and can be easily integrated with any deep learning application model for better performance.\n20\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nDifferent from introduced GBML methods, Meta-LSTM [5] uses a deep neural network classiﬁer as base learner and an\nLSTM as meta-learner. Though LSTM itself is a complete meta-learning system [4] with memory cell and self-recursive\nneuron, LSTM is applied merely as a meta-learner in meta-LSTM. [2] points out using RNN as meta-learner or\nbase learner to achieve higher level of autonomy in robots. Communication between base learner and meta-learner\nensures that mutual information between them is high. Efﬁciency in either base learner or meta-learner should improve\nefﬁciency of whole meta-learning sytem overall. Meta-learner supplies initial values (meta-parameter) of task-speciﬁc\nparameters that contain shared features among tasks. Base learner takes this parameter as input and converges faster\non novel few-shot tasks. Main gradient-based optimization techniques in LSTM include momentum [118], Adagrad\n[119], Adadelta [120] and ADAM [121]. There is correspondence between gradient-based optimization techniques\nand computations in LSTM self-recursive cell. In LSTM, by setting proper values for multiplicative input gate,\nmultiplicative output gate, forget gate, candidate cell state and cell state, computations in recursive cell are equivalent\nto iterations in gradient-based optimization techniques. In meta-LSTM, LSTM is a meta-learner and meta-parameter\nupdate is conducted using computations in recursive cells. In convex optimization, convergence speed is guaranteed. In\nnon-convex optimization such as genetic programming and simulated annealing, convergence holds under assumptions\nas well. In base learner, task-speciﬁc parameters in neural network are updated with MAML. For task Ti, tth SGD\niteration to update task-speciﬁc parameter is\nφt = φt−1 −αt ▽φt−1 Lt(hφt−1).\nNeural network weights φt for task Ti in base learner are updated with loss gradients corresponding to cell state update\nin LSTM:\nct = ft ⊙ct−1 + it ⊙˜ct,\nct = φt,\n˜ct = ▽φt−1Lt,\nit = σ(WI[▽φt−1Lt, Lt, φt−1, it−1] + bI),\nft = σ(WF [▽φt−1Lt, Lt, φt−1, ft−1] + bF ).\nThese formulas show that cell update computations in LSTM recursive neuron are equivalent to SGD parameter update\niterations. Cell state of LSTM is the parameter to be updated and cell state update corresponds to SGD update iteration\nfor the parameter. Due to the correspondence between SGD iteration and LSTM cell state update, LSTM can be applied\nto provide task-speciﬁc parameter update in base model adaptation. Both meta-learner and base learner in MAML\ncontains SGD parameter update procedure. Both meta-learner and base learner in MAML can be speciﬁed to be LSTM\nusing formulas above. In meta-LSTM, meta-learner is speciﬁed to be LSTM which uses this correspondence to conduct\nmeta-parameter update (meta-update) in LSTM recursive cell. Then for task Tt, meta-parameter θt is updated using\nSGD iterations in LSTM meta-learner. LSTM is very stable in training even when LSTM depth is large.\nIn this few-shot meta-learning framework, there are nested loops, where the inner loop is a base learner from each\nindividual task and the outer loop is a meta-learner from all available tasks. Usually, between-task adaptation depends\nupon the quality of similarity measure. [1] proposes meta-learning, base layer (base learner), meta layer (meta-learner)\netc. Later [122] proposes that neural network models with low Kolmogorov complexity exhibit high generalization\ncapability. Efﬁcient lightweight base learner demonstrates improvement in generalization. [85] uses this layered\nmeta-learning framework [1], in which base learner concentrates upon model ﬁtting and meta-learner focuses upon\nmodel adaptation. According to [122], [85] speciﬁes base learner as an efﬁcient and differentiable learner preferably\nwith an explicit closed-form solution. Later [81] follows this line of research and speciﬁes base learner to be convex\noptimizers which can be solved efﬁciently. [98] applies transductive inference in base learner which has closed-form\nsolution for joint prediction of the whole validation dataset. Efﬁcient base learner for fast and accurate adaptation to\nunseen tasks + Relatively slower meta-learner to aggregate training experiences and minimize generalization error\nacross many tasks = An efﬁcient layered meta-learning system. Under this speciﬁcation, over-ﬁtting is reduced in base\nlearner and generalization to unseen tasks is more efﬁcient. In R2D2 (Ridge Regression Differentiable Discriminator)\n[85], base learner is speciﬁed to be ridge regression which has closed-form solution. In LR-D2 (Logistic Regression\nDifferentiable Discriminator), base learner is speciﬁed as iteratively reweighted least squares (IRLS) derived from\nlogistic regression.\nMetaOptNet [81] is developed under layered meta-learning framework [85], where base learner is formulated as a\nregularized linear classiﬁer solvable with convex optimization. This idea of base learner speciﬁcation is similar to\nthat in [85], where an efﬁcient and differentiable statistical base learner is preferred. MetaOptNet is an extension of\n[85] in the sense that it explores more options of base learner speciﬁcation under similar framework as in [85]. Base\nlearner options for convex optimization include linear classiﬁers such as kNN classiﬁer, SVM classiﬁer with nonlinear\nkernels and ridge regression with hinge loss etc. Quadratic programming (QP) can be applied for convex optimization\nwhere Karush-Kuhn-Tucker (KKT) condition is derived and then back-propagation using implicit function theorem is\n21\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nperformed. Parameter θ is estimated using\nmin\nθ [Lbase(Dtr; θ, φ) + R(θ)],\nwhere Lbase is a loss function of base learner and R(θ) is an L2 regularization on parameter θ so to avoid overﬁtting.\nThis optimizer in base learner can be solved efﬁciently with convex optimization. Meta-learning objective is to learn\nan embedding model fφ such that base learner generalizes well across tasks. The meta-objective is to minimize\ngeneralization error on testing / validation dataset:\nmin\nφ ET ∼p(T )[Lmeta(Dtest; θ, φ)],\nwhere Lmeta is a loss function of meta-learner. Meta-learner should be more complex to represent complex similarity\nfeatures shared by many tasks. For example, meta-loss function can be speciﬁed as\nLmeta(Dtest; θ, φ, γ) =\nX\n(xxx,y)∈Dtest\n[−γθyfφ(xxx) + log\nX\nk\nexp(γθkfφ(xxx))].\nMinimization of this cross entropy meta-loss function maximizes probability of belonging to the correct class and\nminimizes probability of belonging to wrong classes. Note that φ is meta-parameter and θ is task-dependent in\nMetaOptNet, contrary to notations in MAML. We may choose statistical models as base learners since they are less\nprone to over-ﬁtting. For example, in MetaOptNet-SVM, base learner can be speciﬁed as an SVM classiﬁer. Auto-Meta\n[123] also follows this line of layered meta-learning framework and speciﬁes meta-learner as a deep CNN model.\nAuto-Meta trains an optimal CNN structure to be an optimal meta-learner model. By optimizing meta-learner deep\nmodel through AutoML, Auto-Meta reduces the number of parameters in meta-learner and increases prediction accuracy\nin few-shot image classiﬁcation.\nTPN (Transductive Propagation Network) [98] learns adaptation to novel classiﬁcation tasks using transductive inference\nin few-shot learning. Transductive inference predicts on whole test data simultaneously, which is different from inductive\ninference which predicts test data one-by-one. TPN considers all data from training set and validation set to construct a\ngraph model, the link weight of which is deﬁned to be a trained similarity metric with data variance included. TPN\nuses labelled data from training set and a joint graph model of both training and validation data to perform transductive\ninference for unknown labels of validation data. Generalization of TPN from training to validation data is different from\npreviously introduced methods, since TPN uses both training and validation data to build a graph model rather than\nrelying on only training data to build a predictive model. TPN maps directly from meta-training data to meta-testing\ndata, constructs a joint graph network model on input data from both support set S and query set Q to propagate labels\nbetween them. Transductive meta-learning framework has two components: a deep CNN feature embedding function\nfφ and a learned manifold structure for the space of unseen classes from S ∪Q. The cross-entropy loss is computed\nusing feature embedding and graph parameter with end-to-end updates on all parameters through back-propagation.\nSupport set is denoted as S = {(xxx1, y1), · · · , (xxxn, yn)}. Query set is denoted as Q = {(xxx∗\n1, y∗\n1), · · · , (xxx∗\nm, y∗\nm)}. The\ngraph edge weight is deﬁned as Gaussian similarity measure:\nWij = exp\n\u001a\n−g[fφ(xxxi), fφ(xxxj)]\n2σ2\n\u001b\n,\nwhere σ2 is data variance modelled as σ = gϕ(fφ(xxx)) parameterized by ϕ, g is similarity function between extracted\nfeature embeddings fφ(xxxi) and fφ(xxxj). Only the k largest edge weights in each row are retained to create a k-nearest\nneighbor graph. W is a row-normalized matrix of all weights in graph. Indicator Yij is 1 if xxxi is in the task training set\nand has label yi = j. Indicator Yij is 0 if xxxi is in the task validation / testing set or does not have label yi = j. Y is\na vector of indicator Yij. Label propagation in the graph network learned from support set S and validation set Q is\nspeciﬁed to be\nFt+1 = θWFt + (1 −θ)Y,\nwhere Ft is the predicted labels at timestamp t and the sequence of {Ft} converges to\nF ∗= (I −θW)−1Y.\nF ∗is an explicit closed-form solution to predicted labels of task validation dataset. Computation of F ∗is in base\nlearner and label prediction in base learner is very efﬁcient. In meta-learner, meta-objective function is deﬁned as\nJ(φ, θ, σ) = Pm\ni=1\nPN\nj=1 −Yij log P(ˆyi = j|xxxi),\nwhere P(ˆyi = j|xxxi) = exp(F ∗\nij)/ PN\nj=1 exp(F ∗\nij),\n22\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nWe minimize meta-loss function J(φ, θ, σ) to estimate meta-parameters φ, θ and σ, where φ is feature extractor /\nembedding parameter, θ is hyperparameter of base learner and σ is data variance model parameter. yi and ˆyi are the\ntrue and predicted label of xxxi respectively. Minimization of meta-loss function maximizes the probability for data to be\nassigned to its true class. All parameters in TPN are regarded as meta-parameters and are updated in meta-update. Base\nlearner plugs in meta-parameters and all task data to directly calculate predicted labels on validation data. Base learner\ndoes not include any parameter update and is very efﬁcient.\nLEO (latent embedding optimization) [99] is also for K-shot N-class few-shot learning, where K is small and N is\nlarge. LEO consists of an encoding process and a decoding process, where encoding process is a feed-forward mapping\nfollowed by a relation network, and decoding process is a softmax classiﬁer. Then parameters in encoder-decoder\nframework are updated using base learner and meta-learner procedure. To train this meta-learning framework including\nan encoder-decoder model, we may explore the following options. (1) Encoder-decoder model is updated efﬁciently in\nbase learner during adaptation to unseen tasks. Hyperparameters in the encoder-decoder model are meta-parameters\nthat are updated in meta-learner. Hyperparameters in encoder-decoder model reﬂect shared features of many tasks and\nparameters in encoder-decoder model reﬂect task-speciﬁc features of each unseen task. (2) To make base learner as\nefﬁcient as possible, all parameters in encoder-decoder model are regarded as meta-parameters. In base learner, for\neach task, ﬁtted encoder-decoder model is utilized to directly calculate predicted labels by plugging in task validation\ndata and meta-parameters from meta-learner. In meta-learner, across many task validation data, generalization error is\nminimized in meta-objective function to estimate all parameters in encoder-decoder model. (3) LEO applies MAML\ntraining framework here. Meta-parameters are the initial values of all parameters in encoder-decoder model. In base\nlearner, all parameters in encoder-decoder model are updated for only few SGD iterations from good initial values. In\nmeta-learner, loss on validation data is minimized to update meta-parameters.\nDimension reduction encoder network is denoted as fφe : Rnx →Rnh parameterized by φe, where nx > nh. Training\ndata for class n is denoted as Dtr\nn = {xk\nn, yk\nn}K\nk=1. Relation network is denoted as gφr parameterized by φr. Denote zn\nas a dimension reduced feature embedding from original input Dtr\nn . Distribution of zn is a Gaussian distribution:\nzn ∼q(zn|Dtr\nn ) = N(µe\nn, diag(σe2\nn )),\nwhere µe\nn, σe\nn =\n1\nNK2\nPK\nkn=1\nPN\nm=1\nPK\nkm=1 gφr\n\u0002\nfφe(xkn\nn ), fφe(xkm\nm )\n\u0003\n.\nEncoder module is a mapping from original high-dimensional input data to feature embedding. Decoder function is\ndenoted as fφd : Z →Θ parameterized by φd where Z is of lower dimension than Θ. Denote wn as a sample from the\ndecoded distribution, which is also a Gaussian distribution:\nwn ∼p(w|zn) = N(µd\nn, diag(σd2\nn )), where µd\nn, σd\nn = fφd(zn).\nDecoder module is a mapping from feature embedding to a vector of probabilities for belonging to each class. In the\nbase learner within inner loop, loss function of LEO for task Ti is\nLtr\nTi(φi) =\nX\n(xxx,y)∈Dtr\n\n−wy · xxx + log\n\n\nN\nX\nj=1\newj·xxx\n\n\n\n,\nwhere φi = (φei, φdi, φri). Apply MAML in inner loop to update feature embedding zn\nz′\nn = zn −α ▽zn Ltr\nTi.\nIn the meta-learner within outer loop, use meta-training data to update the parameters in encoder, relation net and\ndecoder:\nmin\nφe,φr,φd\nX\nTi∼p(T )\n[Lval\nTi (φi) + βDKL(q(zn|Dtr\nn )∥p(zn)) + γ∥stopgrad(z′\nn) −zn∥2\n2]\n+λ1\n\u0000∥φe∥2\n2 + ∥φr∥2\n2 + ∥φd∥2\n2\n\u0001\n+ λ2∥Cd −I∥2,\nwhere meta-parameters φe, φr, φd are the initial values of task-speciﬁc parameters φi = (φei, φdi, φri). Besides\nminimization of model generalization error, meta-objective also puts regularization constraints upon meta-parameters\nto avoid overﬁtting. Cd is the correlation matrix of rows in decoder parameter matrix φd. LEO is a highly integrated\nmeta-learning method based upon base learner and meta-learner. From LEO, we know that base learner model can\nbe of any complexity. Meta-learner makes the whole system much more efﬁcient by training meta-parameters and\naccelerating adaptation of base learner to unseen tasks. The setup of base learner model may be complex but the\ncomplex parameter updates are all in the meta-learner.\nTable 3 summarizes testing accuracy of layered meta-learning methods on 5-way 5-shot miniImageNet classiﬁcation. We\ncan see that MetaOptNet shows highest accuracy. With better feature extraction module applied, better trained similarity\n23\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nTable 3: Testing accuracy of layered meta-learning methods on 5-way 5-shot miniImageNet classiﬁcation.\nMethod\nAccuracy\n[12] MAML\n63.11 ± 0.92%\n[16] Meta-SGD\n64.03 ± 0.94%\n[97] Reptile without transduction\n61.98 ± 0.69%\n[97] Reptile with transduction\n66.00 ± 0.62%\n[5] Meta-LSTM\n60.60 ± 0.71%\n[123] Auto-Meta (without Transduction F=10)\n65.09 ± 0.24%\n[123] Auto-Meta (with Transduction F=12)\n74.65 ± 0.19%\n[85] R2-D2\n68.4 ± 0.2%\n[85] LR-D2 with 5 iterations\n68.7 ± 0.2%\n[85] LR-D2 with 1 iteration\n65.6 ± 0.2%\n[81] MetaOptNet-RidgeReg\n77.88 ± 0.46%\n[81] MetaOptNet-SVM\n78.63 ± 0.46%\n[81] MetaOptNet-SVM-trainval\n80.00 ± 0.45%\n[98] TPN\n69.86 ± 0.65%\n[99] LEO\n77.59 ± 0.12%\nmetrics, more discriminative classiﬁcation loss function, all methods get better predictive performance. Comparison of\nfew-shot image classiﬁcation accuracy here is not strictly based upon the same baseline feature extraction module /\ntrained similarity metric / discriminative classiﬁcation loss function etc. Comparison listed here is rough and not exact.\nHigher prediction accuracy does not necessarily indicate that one line of meta-learning methodology is superior to\nanother. In real meta-learning applications, all these lines of meta-learning methodologies are integrated. Prediction\naccuracy of highly integrated LEO is next to MetaOptNet. Combination of statistical models and machine learning\nmethods improves model generalization capability, since statistical models can adjust for multiplicity in highly correlated\nfeatures, adjust for noise in features and avoid overﬁtting. More integrated and sophisticatedly designed meta-learning\nframeworks demonstrate better prediction accuracy in few-shot image classiﬁcation.\n2.4\nBayesian Meta-Learning\nFrom frequentist perspective, meta-parameter θ and task-speciﬁc parameter φ are regarded as unknown ﬁxed values of\ninterest to be predicted. Under the Bayesian framework, both θ and φ are treated as random variables. Random variables\nhave distributions and distributions may be represented by samples. Updating meta-parameters and task-speciﬁc\nparameters equals updating their distributions or updating samples from their distributions. Distributions or samples\npresent much more information about unknown parameters than just an estimated value. From distributions or samples,\nwe can view that parameter values lie in which region with highest probability, and how much uncertainty the parameter\ncontains. In Bayesian meta-learning, we compute the posterior distributions of model parameters and provide inference\nupon the predictions on out-of-distribution tasks. In few-shot tasks, task-speciﬁc parameters are updated using few\ntraining data and might be subject to large uncertainty. As a result, introducing Bayesian thinking into few-shot\nmeta-learning framework is necessary to provide uncertainty estimation for predicted labels and parameters of interest.\nAt least three Bayesian variations of MAML are presented in this section, since MAML is popular and there have been\nmany literature surrounding extensions of MAML.\nHuman learns new concepts through creative thinking and meta cognition by linking data from several sources to\ncreate understanding about a new concept. In few-shot tasks where training data is limited, human mind can perform\ndata augmentation by brainstorm and free imagnination to link limited data to previous experiences and create psuedo\nsimulated augmented data. Generative model under Bayesian framework simulates innovative imagination in human\nmind to learn new concepts. This subclass of methodology focuses upon integration of a generative model into an\nexisting meta-learning framework. In [61], a generative model BPL (Bayesian program learning) is proposed to classify\nhand-written characters using images and strokes data. A Bayesian generative model is integrated into deep Siamese\nconvolutional network. Generated hand-written characters are indistinguishable from real hand writings using visual\nchecks.\nCharacter type is denoted as θ. A set of M parameters from these types is denoted as Φ = (φ1, · · · , φM) corresponding\nto binary images I = (I1, · · · , IM). The joint distribution of character types, image-speciﬁc parameters and images is\nP(θ, Φ, I) = P(θ)\nM\nY\nm=1\nP(Im|φm)P(φm|θ).\n24\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nBased upon the joint distribution, we compute a discrete approximation to the joint posterior distribution P(θ, φm|Im)\nwhich is updated for adaptation to different tasks. Under Bayesian framework, we work from the joint distribution of data\nand parameters. Under proper independence and conditional independence assumptions between data and parameters,\njoint distribution can be decomposed into important conditional distributions. We aggregate all task training data to\nget accurate empirical estimates of all these important conditional distributions. Based upon estimated conditional\ndistributions, we simulate samples that look like real data and add them into few-shot tasks for data augmentation.\nThough the process of data simulation does not create any additional information, data augmentation does contribute to\nimprovement in prediction accuracy in few-shot image classiﬁcation tasks.\nNeural Statistician method [100] also contains a generative model but this generative model is used to approximate\nposterior distributions of key parameters. Bayesian thinking integrated within meta-learning algorithm can provide\nefﬁcient uncertainty estimation for important parameters and unknown labels. Bayesian variational inference is a key\nmethodology for efﬁcient inference. Bayesian variational inference provides an efﬁcient approximation to posterior\ndistribution and uses the approximated posterior to estimate parameter uncertainty. For task Ti, a task-speciﬁc generative\nmodel ˆpi is estimated. The generative distribution is deﬁned as pi = p(·|ci), where ci is the task-speciﬁc context. Then\nthe approximate posterior of task-speciﬁc context q(c|D) is computed using variational autoencoder (VAE).\nIn VAE, decoder with meta-parameter θ is denoted as p(D|c; θ) and encoder with task-speciﬁc parameter φ is denoted\nas q(c|D; φ). Decoder p(D|c; θ) is also the probability distribution of data which is equal to the value of likelihood\nfunction. Encoder q(c|D; φ) is also the posterior distribution of context c. Context c can be viewed as data label or\ndata class. For adaptation to different tasks, posterior distribution of c modelled as an inference network q(c|D; φ) is\nupdated. The standard variational lower bound of log likelihood function is\nL = Eq(c|D;φ)[log p(D|c; θ)] + αDKL(q(c|D; φ)∥p(c)).\nIn maximum likelihood estimation, we maximize log likelihood function to estimate key parameters. In VAE, we\nmaximize standard variational lower bound of log likelihood function to estimate θ and φ. It is assumed that log\nlikelihood function is close to global optimum when its variational lower bound is maximized. We maximize variational\nlower bound instead of log likelihood, as a result, estimated posterior distribution is only an approximation to true\nposterior. The instance encoder f is a feedforward neural network for feature extraction such that zi = f(xi). In\nexchangeable instance pooling layer, the mapping is from encoded features to a pooled vector: [z1, · · · , zk] →v.\nIn post-pooling network, the mapping is from a pooled vector to a decoded distribution: v →a diagonal Gaussian\n(decoder). After estimating an approximated posterior distribution q(c|D; φ), predicted label is argmaxcq(c|D; φ) where\nposterior is at its maximum.\nThe following three papers extend MAML to Bayesian framework. First, [101] proposes LLAMA (Lightweight\nLaplace Approximation for Meta-Adaptation) which formulates MAML as probabilistic inference in a hierarchical\nBayesian model. It considers ﬁrst-order and second-order Laplace approximations of log likelihood function to\nconstruct inference with quadratic curvature estimation. In a hierarchical Bayesian model, all marginal distributions\nand conditional distributions are speciﬁed to be Gaussian. Log likelihood function is represented as an integral of\nGaussian density kernel. Laplace approximation uses point estimation to approximate the integral in log likelihood\nfunction. Gaussian distribution is tight and symmetric, and its integral can be well approximated by a point estimate in\nLaplace approximation. With Laplace approximation, log likelihood can be computed more efﬁciently, consequently\ncomputation of maximum log likelihood is also more efﬁcient.\nIn task Tj, training data is denoted as xxxj = (xj1, · · · , xjn), and validation data is denoted as xxx∗\nj = (x∗\nj1, · · · , x∗\njm).\nFor probabilistic inference, marginal likelihood of observed data from J tasks xxx = (xxx1, · · · ,xxxJ) can be written as\np(xxx|θ) = QJ\nj=1\n\bR\np(xxxj|φj)p(φj|θ)dφj\n\t\n,\nwhere φj is task-speciﬁc parameter and θ is meta-parameter. In meta-learner, meta-parameter θ is estimated using\nmaximum likelihood maxθ p(xxx|θ). In base learner, task-speciﬁc parameter φj for task Tj can be estimated with MAML\nwhere task-speciﬁc parameter φj is estimated using one SGD iteration from meta-parameter θ:\nˆφj = θ + α ▽θ log p(xxxj|θ).\nIn [101], it is shown that update of task-speciﬁc parameter φj is equivalent to estimating φj using a maximum a\nposteriori (MAP) estimator. MAP estimator is to derive the posterior distribution of φj and MAP is where the posterior\nis at its global maximum. From Bayesian perspective, posterior distribution of φj is p(φj|xxxj, θ) ∝p(xxxj|φj)p(φj|θ),\nwhere p(φj|θ) is the prior distribution of task-speciﬁc parameter and p(xxxj|φj) is the distribution of task training data\nxxxj. Maximum a posteriori estimate of φj is the global mode of p(φj|xxxj, θ):\nˆφj = argmaxφjp(φj|xxxj, θ).\n25\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nBased upon ﬁrst-order Laplace approximation, log likelihood function of validation data xxx∗\nj is approximated with\nlog p(xxx|θ) ≈PJ\nj=1\nn\nlog p\n\u0010\nxxx∗\nj|ˆφj\n\u0011o\n,\nwhere approximate log likelihood for task Tj is given by Exxx∗\nj [log p(xxx∗\nj|φj)] ≈m−1 Pm\nd=1 log p(x∗\njd|ˆφj).\nSecond-order Laplace approximation to likelihood function of task Tj data is\np(xxxj|θ) =\nZ\np(xxxj|φj)p(φj|θ)dφj ≈p(xxxj|ˆφj)p(ˆφj|θ)det(Hj/2π)−1/2,\nwhere Hj is the Hessian of log likelihood function:\nHj = ▽2\nφj[log p(xxxj|φj)] + ▽2\nφj[log p(φj|θ)].\nFrom second-order Laplace approximation, ﬁrst-order approximation log p(xxxj|φj) is replaced with second-order\napproximation log p(xxxj|φj) −η log det( ˆHj). Instead of just a point estimate in ﬁrst-order approximation, second-order\napproximation considers an additional piece ie Hessian matrix. Both ﬁrst-order and second-order Laplace approximation\nof log likelihood function can be applied to update task-speciﬁc parameters in fast model adaptation to unseen tasks.\nHessian matrix takes second-derivative of log likelihood function and requires much more computation than ﬁrst-order\nLaplace approximation. We may assume Hessian matrix to be diagonal, block-wise diagonal, sparse etc to accelerate\nits estimation. Laplace only works for tightly distributed data and parameters. For multi-modal distribution, highly\nskewed distribution, fat-tailed distributions etc, their integrals cannot be estimated using a point estimator and Laplace\napproximation does not work well for them.\nAnother Bayesian extension of MAML is BMAML (Bayesian Model-Agnostic Meta-Learning) proposed in [62], where\nstochastic gradient descent (SGD) in base learner is replaced with Stein variational gradient descent (SVGD) method\n[124, 125]. SVGD is an efﬁcient sampling method which is a combination of MCMC and variational inference. Both\nMCMC and Bayesian variational inference are popular inference methods under Bayesian framework. MCMC relies\nupon obtaining samples from target distribution to provide uncertainty estimate of key parameters. Bayesian variational\ninference maximizes variational lower bound of log likelihood to estimate an approximate posterior distribution of\nkey parameters. Uncertainty estimate and conﬁdence interval are derived from the approximate posterior distribution.\nBayesian variational inference is more efﬁcient than MCMC since it does not require sampling or convergence. MCMC\nis based upon sampling and works for a wider class of data and parameter distributions, such as multi-modal distribution,\nhighly skewed distribution, fat-tailed distributions etc. SVGD inherits advantages and avoids disadvantages of both\nMCMC and Bayesian variational inference. We ﬁrst draw a sample Θ0 = {θ0\ni }n\ni=1 from target distribution p(θ) as the\ninitial particles. In the lth iteration, particles Θl = {θl\ni}n\ni=1 are updated using\ng(θ) = n−1\nn\nX\nj=1\nn\nk(θl\nj, θ) ▽θl\nj log p(θl\nj) + ▽θl\njk(θl\nj, θ)\no\n,\nθl+1\ni\n←θl\ni + ϵlg(θl\ni), for i = 1, · · · , n,\nwhere ϵl is the step size, k(·, ·) is a positive kernel function and ▽θl\njk(θl\nj, θ) is a repulsive force so that θl+1\ni\n̸= θl\ni. In\norder for the samples to reach as much region as possible in target distribution p(θ), samples are expected to be farther\naway from each other and do not collapse to be around the same spot so that samples better represent properties of p(θ).\nDuring meta-training process, we update these particles to update posterior distribution of task-speciﬁc parameters. In\nBMAML, we use this SVGD procedure to update samples rather than use SGD to update parameter values. SVGD\nrequires that target distribution p(θ) is differentiable. SGD requires that loss function is differentiable. BMAML\nextends MAML in the sense that MAML is applicable to cases optimized with SGD and BMAML is applicable to cases\noptimized with SVGD.\nIn base learner, Bayesian Fast Adaptation (BFA) is performed, where we update task-speciﬁc parameter particles\nensemble Φi = {φm\ni }M\nm=1 for task Ti using SVGD based likelihood approximation:\nΦi ←SVGDn(Θ0, Dtr\ni , α),\nwhere after n SVGD iterations, particles ensemble representing distribution of task-speciﬁc parameter is updated to be\nΦi. Hyperparameters controlling each SVGD iteration is denoted as α. The contribution of task Ti to meta-objective is\nto maximize its generalization capability\np(Dval\ni\n|Θ0, Dtr\ni ) =\nR\np(Dval\ni\n|Θ0, Φi)p(Φi|Θ0, Dtr\ni )dΦi ≈M −1 PM\nm=1 p(Dval\ni\n|Θ0, φm\ni ),\n26\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nwhere φm\ni\n∼p(Φi|Θ0, Dtr\ni ) and Θ0 is the initial particle ensemble to generate Φi using SVGD. After a batch of\ntasks T1, · · · , TJ are trained, meta-parameter p(θ) in meta-learner is updated. In one way, θ can be updated through\nminimization of generalization error. In another, initial particles ensemble Θ0 may be updated using SVGD. Meta-\nobjective in meta-learner is to maximize\nlog p(Dval|Θ0, Dtr) ≈PJ\ni=1 LBF A(Φi; Dval\ni\n),\nwhere LBF A(Φi; Dval\ni\n) ≈log\nn\nM −1 PM\nm=1 p(Dval\ni\n|Θ0, φm\ni )\no\n.\nIn base learner, for task Ti, task-speciﬁc particle ensemble Φi is updated using Φi ←SVGD(Θ0; Dtr\ni , α), where α is\nthe hyperparameters in SVGD iteration such as step size or learning rate etc. In meta-learner, meta-parameter particles\nensemble initialization Θ0 is evaluated using\nΘ0 ←Θ0 + β ▽Θ0\nhPJ\ni=1 LBF A(Φi; Dval\ni\n)\ni\n,\nwhere initial particles ensemble is updated using SVGD. It may be better represented as\nΘ0 ←SVGD(Θ0; Dval\ni\n, β).\nMeta-learner described above is similar to meta-learner of MAML. BMAML presents another way of deﬁning meta-\nobjective function: a chaser loss meta-objective. Meta-objective function is speciﬁed as the dissimilarity between\napproximate and true posterior distributions of task-speciﬁc parameter Φi. Posterior of task-speciﬁc parameter after n\nSVGDs from initial particles ensemble Θ0 is denoted as pn\ni ≡pn(Φi|Dtr\ni ; Θ0), from which we randomly sample Φn\ni .\nTrue task-speciﬁc parameter posterior is p∞\ni\n≡p(Φi|Dtr\ni ∪Dval\ni\n), from which we randomly sample Φ∞\ni . BMAML\nmeta-learner seeks the minimization of the dissimilarity between estimated approximate posterior and true posterior:\nminΘ0\nPJ\ni=1 dp(pn\ni ∥p∞\ni ) ≈minΘ0\nPJ\ni=1 ds(Φn\ni ∥Φ∞\ni ),\nwhere dp and ds are the distance measures between two distributions and two particles ensembles respectively.\nFor deﬁnition of chaser loss, true posterior Φ∞\ni\nis approximated with a posterior estimate after s more SVGD\niterations Φn+s\ni\n, where s > 0. Chaser is deﬁned as Φn\ni = SVGDn(Θ0; Dtr\ni , α). Leader is deﬁned as Φn+s\ni\n=\nSVGDs(Φn\ni ; Dtr\ni ∪Dval\ni\n, α). Meta-objective in BMAML based upon chaser loss is\nLBMAML(Θ0) = PJ\ni=1 ds(Φn\ni ∥Φn+s\ni\n).\nFinally, meta-parameter Θ0 is updated using Θ0 ←Θ0 −β ▽Θ0 [LBMAML(Θ0)], where ▽Θ0 is also SVGD iteration\nrather than SGD iteration. Prediction accuracy of BMAML is slightly better than LLAMA in 5-way 1-shot miniImageNet\nclassiﬁcation. However, LLAMA is more lightweight than BMAML.\nThird, PLATIPUS (Probabilistic LATent model for Incorporating Priors and Uncertainty in few-Shot learning based\nupon MAML) [126] is also an extension to MAML under Bayesian framework. PLATIPUS constructs a Bayesian\nnetwork for data and parameters and utilizes Bayesian variational inference to estimate uncertainty in key param-\neters. Through maximizing variational lower bound of log likelihood function, PLATIPUS computes approximate\nposterior distributions of task-speciﬁc parameters and meta-parameters. MAP estimates at the global maximum\nof approximate posterior distributions are used to update task-speciﬁc parameters and meta-parameter. Training\ndata is {(xxx1, y1), · · · , (xxxn, yn)}. Validation data is {(xxx∗\n1, y∗\n1), · · · , (xxx∗\nm, y∗\nm)}. For task Ti, inference network is\nqi(θ, φi) = qi(θ)qi(φi|θ). Conditional distribution is speciﬁed as qi(φi|θ) = qψ(φi|θ,xxxi, yi,xxx∗\ni , y∗\ni ) and prior distri-\nbution of θ is qi(θ) = qψ(θ|xxxi, yi,xxx∗\ni , y∗\ni ). Both components in inference network are parameterized by ψ. These\ntwo inference networks are used to model approximate posterior distributions of key parameters. All marginal and\nconditional distributions are speciﬁed to be Gaussian. Variational lower bound of log likelihood is\nlog p(y∗\ni |xxx∗\ni ,xxxi, yi) ≥Eθ,φi∼qψ[log p(yi|xxxi, φi)\n+ log p(y∗\ni |xxx∗\ni , φi) + log p(φi|θ) + log p(θ)]\n+H(qψ(φi|θ,xxxi, yi,xxx∗\ni , y∗\ni )) + H(qψ(θ|xxxi, yi,xxx∗\ni , y∗\ni )),\nwhere H is regularization upon the approximate distributional model, such that its complexity is moderate and it’s\nreasonably close to its true distribution etc. These inference networks are posited to be Gaussian distributions\nqψ(θ|xxxi, yi,xxx∗\ni , y∗\ni ) = N(µθ + γp ▽µθ log p(yi|xxxi, µθ)\n+γq ▽µθ log p(y∗\ni |xxx∗\ni , µθ), vq).\nIn PLATIPUS, we ﬁrst initialize all meta-parameters Θ = {µθ, σ2\nθ, vq, γp, γq} in inference networks. Based upon these\ninference networks, we sample meta-parameters θ ∼qψ = N(µθ −γq ▽µθ L(µθ, Dtest), vq). Based upon MAML,\n27\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\ntask-speciﬁc parameter in the inner loop is updated using φi = θ −α ▽θ L(θ, Dtr). Gaussian inference network is\nspeciﬁed as p(θ|Dtr) = N(µθ −γp ▽µθ L(µθ, Dtr), σ2\nθ). Finally, meta-parameters Θ are updated using ADAM\n▽Θ {P\ni L(φi, Dtest) + DKL(q(θ|Dtest)∥p(θ|Dtr))} .\nPrediction accuracy of PLATIPUS in 5-way 1-shot miniImageNet classiﬁcation is comparable to LLAMA and BMAML\nwhich are also Bayesian extensions of MAML.\nVERSA (Versatile and Efﬁcient Amortization of Few-Shot Learning) [18] uses Bayesian decision theory (BDT)\nin integration with Bayesian variational inference to estimate approximate posterior distribution of unknown label.\nPredicted label is where approximate posterior distribution of unknown label is at its global maximum. We can estimate\nthe interval where unknown label has the highest probability to be. We can also estimate the uncertainty in unknown\nlabel using approximate posterior distribution. Training data is {(xxxk, yk)}n\nk=1. Validation data is {(xxx∗\nk, y∗\nk)}m\nk=1. Task\ndata is the union of training data and validation data denoted by {(xxxk, yk)}N\nk=1. Task training data of task Ti is (xxxi, yi).\nTask validation data of task Ti is (xxx∗\ni , y∗\ni ). Meta-parameter is θ and task-speciﬁc parameter for task Ti is φi. Joint\ndistribution of all data and all parameters is\np\n\u0000{yi, φi}N\ni=1|{xxxi}N\ni=1, θ\n\u0001\n=\nN\nY\ni=1\np(φi|θ)\nn\nY\nj=1\np(yj|xxxj, φj, θ)\nm\nY\nk=1\np(y∗\nk|xxx∗\nk, φk, θ) .\nJoint distribution of all data and all parameters is decomposed into a multiplication of three parts: conditional distribution\nof parameters, distribution of task training data, distribution of task validation data. For neural network base learners,\ntask-speciﬁc parameters φi = {Wi, bi} are composed of weights and biases. In task testing / validation data, predicted\nlabel is ˆy and unknown true label is ˜y. Unknown label is predicted with\nmin\nˆy\nZ\nL(˜y, ˆy)p(˜y|Di)d˜y,\nwhere Di is all data in task Ti, and L(˜y, ˆy) is the distance between predicted and true label. Posterior distribution of\nunknown true label is\np(˜y|Di) =\nR\np(˜y|φi)p(φi|Di)dφi,\nwhere p(φi|Di) is the posterior distribution of task-speciﬁc parameter. In distributional BDT, true label ˜y follows\ndistribution q(˜y) and the objective is to estimate a predictive distribution of unknown label:\nmin\nq∈Q\nZ\nL(˜y, q(˜y))p(˜y|Di)d˜y,\nwhere predictive label distribution q lies within a pre-speciﬁed distribution family Q. Amortized variational training is\napplied to make estimation based upon distributional BDT. Distribution of unknown label is speciﬁed as q(˜y) = qφ(˜y|D)\nwhere φ is the unknown parameter of interest. Parameter estimator in predictive label distribution is\nmin\nφ Ep(D,˜y)L(˜y, qφ(˜y|D)).\nLoss function L is speciﬁed as negative log likelihood:\nL(qφ) = Ep(D,˜y)[−log qφ(˜y|D)].\nMinimization of loss function L maximizes the probability of data being in its true category. Variational lower bound of\nlog likelihood function used to estimate unknown label is\nEp(D,˜y)[DKL{p(˜y|D)∥qφ(˜y|D)}] + H[qφ(˜y|D)].\nPrediction based upon distributional Bayesian decision theory and amortized variational inference is precise.\nTable 4 presents testing accuracy of meta-learning methods on 5-way 1-shot miniImageNet classiﬁcation. We can\nsee that performance of Bayesian meta-learning methods is comparable to black-box adaptation, similarity-based\nmodels and layered meta-learning approach. Among Bayesian meta-learning methods, BMAML and VERSA show\nbest prediction performance. Among layered meta-learning models reviewed, MetaOptNet shows highest prediction\naccuracy. Comparison of these meta-learning methods on few-shot image classiﬁcation tasks is not based upon the\nsame baseline model such as feature extraction model, discriminative loss function etc. Comparison listed here is rough\nand not exact. We can see that prediction accuracy is far from 100% and there is room for improvement in few-shot\nimage classiﬁcation.\n28\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nTable 4: Testing accuracy of meta-learning methods on 5-way 1-shot miniImageNet classiﬁcation.\nMethod\nAccuracy\n[101] LLAMA\n49.40 ± 1.83%\n[62] BMAML\n53.8 ± 1.46%\n[126] PLATIPUS\n50.13 ± 1.86%\n[18] VERSA\n53.40 ± 1.82%\n[93] Activation to parameter with neural network\n54.53 ± 0.40%\n[93] wide residual network WRN\n59.60 ± 0.41%\n[21] AdaCNN with DF\n48.34 ± 0.68%\n[21] AdaResNet with DF\n56.88 ± 0.62%\n[14] Matching Net\n46.6%\n[15] Prototypical Net\n46.61 ± 0.78%\n[105] PVRT\n52.68 ± 0.51%\n[95] Prototypical Network+TRAML\n60.31 ± 0.48%\n[17] Relation Net\n50.44 ± 0.82%\n[94] SNAIL\n45.1%\n[84] TADAM with α, AT and TC\n58.5 ± 0.3%\n[84] TADAM without tuning\n56.5 ± 0.4%\n[96] Dynamic few-shot with C128F feature extractor\n55.95 ± 0.84%\n[96] with ResNet feature extractor\n55.45 ± 0.89%\n[96] with cosine classiﬁer and attention based weight generator\n58.55 ± 0.50%\n[96] with cosine classiﬁer and no weight generator\n54.55 ± 0.44%\n[89] mAP-SSVM\n50.32 ± 0.80%\n[89] mAP-DLM\n50.28 ± 0.80%\n[12] MAML\n48.7 ± 1.84%\n[23] MetaNet\n49.21 ± 0.96%\n[60] Reptile\n49.97 ± 0.32%\n[16] Meta-SGD\n50.47 ± 1.87%\n[5] Meta-LSTM\n43.44 ± 0.77%\n[123] Auto-Meta (without Transduction F=10)\n49.58 ± 0.20%\n[123] Auto-Meta (with Transduction F=12)\n57.58 ± 0.20%\n[85] R2-D2\n51.8 ± 0.2%\n[85] LR-D2 with 5 iterations\n51.9 ± 0.2%\n[85] LR-D2 with 1 iteration\n51.0 ± 0.2%\n[81] MetaOptNet-RidgeReg\n61.41 ± 0.61%\n[81] MetaOptNet-SVM\n62.64 ± 0.61%\n[81] MetaOptNet-SVM-trainval\n64.09 ± 0.62%\n[98] TPN\n55.51 ± 0.86%\n[99] LEO\n61.76 ± 0.08%\n29\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\n3\nApplications of Meta-learning\nMeta-learning serves as a complementary generalization block for deep-learning models, which have been widely\napplied to tackle challenging problems and to achieve autonomous AI. Though deep-learning models provide accurate in-\nsample prediction, it does not guarantee accurate out-of-sample prediction results. Meta-learning, however, concentrates\nupon out-of-sample generalization of deep-learning models. With MAML embedded, base learner is deep model, and\nmeta-learner provides initial values of task-speciﬁc parameters in base learner. Meta-learner guides base learner to\nsearch for an optimal model training procedure, in which performances upon in-distribution tasks are preserved and\nperformances upon out-of-distribution tasks are improved. Meta-learning is extremely helpful in cases where deep\nmodel is required and data amount is little, since deep model contains lots of parameters that cannot be estimated\naccurately with few-shot data.\nFirst, meta-learning is widely applied in robotics research where robots are expected to reach higher level of autonomy\nin general AI. Robots acquire basic skills from one or two human or video demonstrations using meta-imitation learning.\nRobots perform decision making and make optimal response to different environments through meta-RL. RL model is\nhard to train and it takes a long time for robots to learn an optimal action strategy from training RL model from scratch.\nMeta-RL model does not train RL model from scratch and it needs only one or two interaction trajectories between\nagent and slightly different environment to adapt a pre-trained deep RL model to solve the current task. Imitation\nlearning makes robots learn an optimal policy efﬁciently by imitating many human demonstrations for an exactly same\ntask. Generally imitation learning achieves the same objective but is more efﬁcient than RL model. Meta imitation\nlearning makes robots learn an optimal policy by imitating only one demonstration under a slightly different task, such\nas different background color, different light condition, different target shape etc.\nSimilarly, for trading robot, with imitation learning, robots can imitate human trading and learn complex trading\ndecision making scheme. With meta-learning, trading robots can autonomously adapt decision making mechanism to\nfast changing market conditions continually. Under the same environmental market condition, trading robots learn one\nbehavioral skill from human demonstration through imitation learning. Under a slightly different market condition,\ntrading robots learn from one human demonstration which is obtained under the previous market condition. To be\nmore precise and to perform better, robots should capture complex interaction mechanisms between human behaviors\nand market conditions. Human interacts with market by trading on two alternative assumptions: (1) human trading\nbehaviors affect market conditions, (2) human trading behaviors do not affect market conditions. Market conditions\ndetermine the scheme through which human obtains reward, and further determine optimal trading behaviors taken by\nhuman. Under meta imitation scheme, robots imitate human trading strategies under a slightly different background. In\naddition, meta-learning can handle unusual situations in ﬁnancial market such as sudden market crashes. Proper trading\nstrategies to minimize expected loss can be developed under meta-learning framework. This scheme can be used as a\nfail-safe and be integrated into the current trading machines, preserving prediction accuracy on in-distribution market\nconditions and improving prediction accuracy on out-of-distribution market conditions.\nSecond application of meta-learning is in drug discovery for handling high-dimensional data with small sample size.\nSample size in medical studies is usually small especially for rare disease or gene SNPs. Feature dimension in such\ndatasets, on the other hand, is very large. A task is regarded as ﬁnding an effective treatment for a virus which might\nmutate frequently or identifying a set of features that are truly associated with response of interest. Computation for\ndrug candidate is expensive thus adapting previously trained models to new tasks should be considered to save time and\nexpense in drug discovery. Under such situations, deep models may be applied or not applied. But for processing image\ndata, deep models have to be considered in the whole decision system. In such situations of high-dimensional data with\nsmall sample size, meta-learning system is helpful in adapting pre-trained deep models to solving unseen tasks.\nThird application is in translation of rarely used languages. Rare language may be in these two cases: (1) rarely used\nwords or characters in commonly used languages, (2) rarely used languages. Frequently used words constitute only\na small proportion of all words. Most words appear less frequently and their translation depends upon a relatively\nsmaller sample. Recent meta-learning models are applicable to few-shot datasets where translation can be learned\nusing adaptation of pre-trained deep models. Though sample size is small for ancient or rare languages, a few-shot\nmeta-learning model can be trained to translate these scripts. Over a sufﬁciently long time of experience accumulation,\nmeta-learner has stored relevant experiences for rare language translation tasks.\nFor applications where an explicit deﬁnition of task and label can be clearly speciﬁed, meta-learning provides a\nfeasible plan for problem-solving. Meta-learning framework is ﬂexible and can be conveniently integrated with most\nmachine learning algorithms to provide feasible solutions. For tasks which generally require heavy computation,\nmeta-learning presents the option of aggregating or adapting previous results to save computation. For application of\nmeta-learning framework in deep learning models, all components in meta-learning framework including objective\n30\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nfunctions, task-speciﬁc parameters, meta-parameters, task training data, task validation data etc should be explicitly\ndeﬁned under deep learning framework.\n3.1\nMeta-Reinforcement Learning\nReinforcement learning (RL) is based upon markov decision process (MDP) (S, A, p, r, γ, ρ0, H) where S is the set of\nall states, A is the set of all actions, p(s′|s, a) is the transition process, r(s, a) is a reward function, γ is the discount\nfactor on future reward, ρ0(s) is the initial state distribution, and H is the time horizon [31]. A trajectory between\ntimestamp i and timestamp j is denoted as τ(i, j) ≡(si, ai, · · · , sj, aj, sj+1). Policy is deﬁned as π : S →A.\nObjective is to identify an optimal policy that maximizes the summation of all rewards from a trajectory. An RL task\n[12] is deﬁned as\nTi ≡(Si, A(s), pi(s), pi(s′|s, a), ri(s, a)),\nwhere Si is the state space, A(s) is the action space, pi(s) is the initial state distribution, pi(s′|s, a) is the transition\nprocess and ri(s, a) is the reward process. In an RL task, we observe states, actions and ﬁnd an optimal policy to\nmaximize the estimated sparse reward function.\nEarly research on meta-learning [1] is focused upon reinforcement learning settings. Meta-learning is applied to achieve\ncontinual learning or lifelong learning for robots in RL settings. Recently meta-learning is applied to train deep RL\nmodel when there are not sufﬁciently large number of observed trajectories in each RL task. Meta-reinforcement\nlearning (meta-RL) considers the interaction between agent and changing environment, as in ﬁgure 5. Main objective\nof meta-RL is training robots to handle unusual situations in reward-driven tasks with only one or two observed\ntrajectories. Meta-RL concerns data-efﬁcient fast adaptation to out-of-distribution RL tasks after only one or two\nobserved trajectories [28]. All meta-learning models reviewed in section 2 can be extended to RL tasks.\nFigure 5: Meta-reinforcement learning. Robots interact with environment to get state and reward. Environment\nis different after interaction with robots. Environmental conditions vary in different tasks. In few-shot RL tasks,\nreinforcement learning is integrated into meta-learning framework to train deep RL policy using few-shot data.\n3.1.1\nLifelong RL with Self-Modiﬁcations\nSSA (Success-Story Algorithm) [38] with SMP (Self-Modifying Policy) is for lifelong RL that continually improves\npolicy. Action decision points along the whole lifetime are denoted by c1, · · · , cH, which constitute a solution path for\nsequential decision making. At each decision point, SMP-modiﬁcation candidates are generated using GP (Genetic\nProgramming) and evaluated to see whether they accelerate reward production. GP evolves slowly and is not aggressive\nfor identifying useful SMP-modiﬁcations, making it stable and ideal for lifelong RL. SSC (Success-Story Criterion) is\nR(t)\nt\n< R(t) −R(c1)\nt −c1\n< R(t) −R(c2)\nt −c2\n< · · · < R(t) −R(c|Vt|)\nt −c|Vt|\n,\nwhere Vt is the set of all checkpoints up to time t. Ratio of reward and time is a measure of RL algorithm efﬁciency.\nUseful SMP-modiﬁcations meet SSC and are kept in RL algorithm. As in [127], G¨odel machines based upon useful\nincremental self-improvement theoretically achieve global optimum and is never trapped in any local optimum. SSA\nwith SMP is a computationally stable G¨odel machine that is suitable for lifelong RL. [128] points out that evolutionary\nalgorithms can be faster than backpropagation in estimating neural network weights.\n3.1.2\nMetaRL with GBML\nGradient-based meta-learning models such as MAML and Reptile can be extended to meta-RL framework for efﬁcient\nadaptation of policy over changing environments. Actually GBML can be easily integrated into almost all deep learning\nsettings. For modiﬁcation of GBML to be in RL settings, the following routes may be pursued: (1) re-deﬁne loss\nfunction to be maximization of reward function in RL setting, (2) re-deﬁne task-speciﬁc parameters to be in deep\npolicy model and meta-parameters are the initial values of task-speciﬁc parameters in GBML, (3) re-design algorithms\nto update task-speciﬁc parameters in base learner and algorithms to update meta-parameters in meta-learner etc. An\nextension of MAML to RL tasks is presented in [12], where the loss function of task Ti is written as\nLTi(fφ) = −ESi,Ai\nnPH\nt=1 γtri(st, at)\no\n,\n31\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nwhere fφ is a deep model speciﬁed for policy, and φ is the task-speciﬁc policy parameter of interest. In base learner,\ntask training data is plugged into this loss function and minimization of loss function leads to update on task-speciﬁc\nparameters. In meta-learner, task validation data is plugged into this loss function and minimization of loss function\nleads to update on meta-parameters. This is the direct application of MAML to few-shot RL tasks. For RL tasks,\nwe minimize the loss function LTi(fφ) to estimate an optimal policy fφ and seek fast adaptation to novel tasks with\nchanging environment. Here GBML is applied to solve few-shot RL tasks, as a result, unseen few-shot RL tasks should\nbe similar to trained tasks and have the same data structure as trained tasks.\nAnother application of MAML in meta-RL is E-MAML [129] which adds an exploratory term to MAML. E-MAML\nconsiders the inﬂuence of sampling πθ upon future rewards R(τ), where τ ∼πU(θ). MAML applies stochastic gradient\nof\nR\nR(τ)πU(θ)(τ)dτ. E-MAML uses stochastic gradient of\nR R\nR(τ)πU(θ)(τ)πθ(¯τ)d¯τdτ, where ¯τ ∼πθ.\n3.1.3\nMetaRL with Q-Learning\nMeta-RL in CNN architecture optimization is surveyed in [25]. Reinforcement learning can be applied to solve complex\noptimization problem which includes optimization of both discrete and continuous hyperparameters. In such situations,\nreward function is taken to be the prediction accuracy of candidate models along optimization process. Action option is\ntaken to be the value of hyperparameter to be explored. Through deep RL model, an optimal policy is estimated such\nthat model has the highest prediction accuracy with hyperparameter values chosen by the optimal policy. Deep RL\nmodels are not as efﬁcient to train so that meta-RL models are applied to accelerate the training of deep RL models\nunder slightly different settings using only few-shot RL task data. Evolutionary algorithms provide candidates for\nhyperparameter search. We may utilize a goal-based Q-learning model to update parameters in adaptation. Reward\nfunction is denoted as\nV π(s) = E(r|s, π).\nBellman update on Q function is written as\nQ(st, at) ←Q(st, at) + α[rt+1 + γ max\na\nQ(st+1, a) −Q(st, at)].\nThe optimal action that maximizes Q-function is the best CNN architecture design. Auto-Meta [123] can use neural\nnetwork architecture search algorithms in [25] to optimize meta-learner models speciﬁed as CNN or RNN.\nMQL (Meta-Q-Learning) [130] considers meta-RL using Q-learning embedded in context. Meta-parameter is estimated\nby maximizing average reward from training data and minimizing temporal-difference error from Bellman equation.\nAdaptation is achieved by propensity-score weighted sampling from previous training replay buffer. In openAI-gym\n[131] environments, MQL is efﬁcient and demonstrates performance comparable to PEARL.\n3.1.4\nMetaRL with Actor-Critic\nPEARL (Probabilistic Embeddings of Actor-critic RL) [29] deﬁnes loss function with actor-critic RL and applies\nMAML for few-shot update on parameters in adaptation. Actor-critic RL contains policy estimation (actor) and value\nevaluation (critic). First, variational inference is applied to infer hidden state z. Inference network qφ(z|c) is speciﬁed\nas\nqφ(z|c1, · · · , cN) ∝QN\nn=1 Ψφ(z|cn),\nwhere context cn = (sn, an, s′\nn, rn) consists of state-action pairs, Ψφ(z|cn) = N(f µn\nφ (cn), f σn\nφ (cn)) is a Gaussian\ndistribution and fφ is a neural network that predicts µn and σn for each context cn. Variational lower bound optimized\nto estimate z is\nET Ez[R(T , z) + βDKL{qφ(z|c)∥p(z)}],\nwhere T ∼p(T ), z ∼qφ(z|c), p(z) is a prior distribution over z and R(T , z) is a task-speciﬁc objective. This meta-RL\nalgorithm is based upon soft actor-critic (SAC) method where we consider the joint optimization of inference network\nqφ(z|c), actor πθ(a|s, z) and critic Qξ(s, a, z). Critic loss function is deﬁned to be\nLcritic = E z∼qφ(z|c)\n(s,a,r,s′)∼B\n[Qξ(s, a, z) −(r + ¯V (s′, ¯z))]2,\nwhere Qξ(s, a, z) is the value of current state-action pair, ¯z is the average condition, r is current reward and ¯V (s′, ¯z)\nis the optimal target value derived from all possible future states. Critic loss minimizes the value difference between\ncurrent state-action and optimal target value. Actor loss function [132] is deﬁned as\nLactor = Ea∼πθ\ns∼B DKL[πθ(a|s, ¯z)∥exp(Qξ(s,a,¯z))\nZθ(s)\n],\n32\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nwhere Zθ(s) =\nR\nexp(Qξ(s, a, ¯z))da is a normalizing constant. Actor loss function minimizes the distance between\npolicy πθ(a|s, ¯z) and value-directed action weight speciﬁed as exp(Qξ(s, a, ¯z))/Zθ(s) which assigns more weight to\naction with higher value.\nJoint estimation of parameters in inference network, policy and value function is conducted with MAML. First, sample\nRL task bi = {sj, aj}N\nj=1 ∼Bi and context ci ∼Sc(Bi) where ci = {sj, aj, s′\nj, rj}N\nj=1. Then sample hidden state z ∼\nqφ(z|ci). Compute loss functions: Li\nactor = Lactor(bi, z), Li\ncritic = Lcritic(bi, z) and Li\nKL = βDKL(qφ(z|ci)∥r(z)).\nMAML updates on parameters are as follows:\nφ ←φ −α1 ▽φ\nP\ni(Li\ncritic + Li\nKL),\nθ ←θ −α2 ▽θ\nP\ni Li\nactor,\nξ ←ξ −α3 ▽ξ\nP\ni Li\ncritic.\nActor loss function is used to update policy parameter and critic loss function is used to update inference network and\nvalue function.\n3.1.5\nMetaRL with DDPG\nMetaGenRL [133] improves generalization to vastly different few-shot RL tasks through combining multiple agents\nto train a meta-learner. MetaGenRL considers DDPG (Deep Deterministic Policy Gradient) actor-critic model [134,\n135] which applies DQN (deep Q network) to continuous action domain. [133] demonstrates that DDPG supports\ngeneralization to vastly different environments. MetaGenRL explicitly separates adaptation of policy πθ and value Qξ\nmodel which contributes to improvement in generalization capability. Application of second-order gradients improves\nsampling efﬁciency in RL. Critic loss is\nLcritic = P\ns,a,r,s′[Qξ(s, a) −(r + γ ¯V (s′, πθ(s′)))]2.\nCritic loss is minimized to estimate value parameter ξ. Policy parameter θ can be updated by taking the gradient of\nEτ[PH\nt=1 log πθ(a|s)A(τ, V, t)], where A(τ, V, t) is a GAE (generalized advantage estimate) and V : S →R is a\nvalue function. In MetaGenRL, meta-objective function is speciﬁed as a neural network Lα(τ, πθ, V ), where πθ is an\nauxiliary input and α is neural network parameter. During adaptation, ξ is updated by minimizing critic loss Lcritic,\nθ′ ←θ −▽θLα(τ, x(θ), V ),\nα ←α + ▽αQξ(s, πθ′(s)).\n3.1.6\nContinual MetaRL with Dynamic Environments\nRobots learn to survive dynamic environments through continual meta-RL method GrBAL (Gradient-Based Adaptive\nLearner) [31]. Transition probability estimator is ˆpθ(s′|s, a) where ˆθ is a maximum likelihood estimate. In meta-learning\nprocedure, a general equation of task-speciﬁc parameter update is\nφT = uψ(Dtr\nT , θ),\nwhere θ is meta-parameter, and uψ(Dtr\nT , θ) = θ −ψ ▽θ L(Dtr\nT , θ) in MAML. Parameters are estimated by minimizing\nthe meta-loss function:\nmin\nθ,ψ ET ∼ρ(T )[L(Dval\nT , φT )].\nIn total, M trajectories are sampled from a novel task. We use the experience of K time steps as training data to predict\nnext K time steps which is testing data. Let E denote the set of all dynamic real-world environments. Under these\nconcrete settings, meta-objective is\nmin\nθ,ψ EτE(t−M,t+K)∼D[L(τE(t, t + K), θ′\nE)],\nwhere τE(t −M, t + K) is the task trajectories data between timestamps t −M and t + K, τE(t, t + K) is the task\nvalidation trajectories data between t and t + K and τE(t −M, t −1) is the task training trajectories data. Task\nvalidation data between t and t + K is taken to update meta-parameters: initial values of task-speciﬁc parameters and\nbase learner hyperparameters. Task training data between t −M and t −1 is taken to update task-speciﬁc parameters.\nUpdate on task-speciﬁc parameters in base learner is written as\nφE = uψ(τE(t −M, t −1), θ).\n33\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nHere base learner update on task-speciﬁc parameter using task training can be written as GrBAL:\nφE = θE + ψ ▽θE\n1\nM\nt−1\nX\nk=t−M\nlog ˆpθE(sk+1|sk, ak),\nwhere ψ is the step size. Recurrence-Based Adaptive Learner (ReBAL) update parameters ψ using a recurrent model.\nMeta-loss function using task validation data is\nL(τE(t, t + K), φE) ≡−1\nK\nt+K\nX\nk=t\nlog ˆpφE(sk+1|sk, ak).\nAnother meta-RL framework based upon continual self-improvement is proposed in [136]. [136] performs supervised\nlearning augmented by artiﬁcial curiosity [137] and boredom. Meta-loss minimization creates dynamic attention which\nguides algorithm exploration in artiﬁcial curiosity. Boredom refers to avoidance of environments which have been\nperfectly solved. Layered meta-learning in section 2.3 can be applied in meta-RL tasks as well. Internal feedback\ncorresponding to base learner comes from stepwise interaction between agents and known environments under the\ninﬂuence of agent actions. External feedback corresponding to meta-learner is from all interactions between agents and\nunknown environments, which shapes the whole embedded credit assignment mechanism. Evaluation of internal and\nexternal feedback leads to update on base learner and meta-learners for adaptation to unseen tasks.\n3.1.7\nMeta-RL with Coevolution\nCoevolution [138] involves cooperation and competition between agents and environments. In POET (Paired Open-\nEnded Trailblazer) [7], more complex environment is generated from coevolution between agent and environment, in\nwhich POET trains an accurate RL model for a difﬁcult environment which cannot be solved otherwise by training from\nscratch. In the ith sampled trajectory, reward from interaction between agent and environment is R(θi) parametrized by\nθi ∼pθ,σ(θi) = N(θ, σ2). Optimization to estimate hyperparameter θ is\nmax\nθ,σ Ew∼pθ,σ(w)R(w) ≈1\nn\nn\nX\ni=1\nR(θi).\n(1)\nTo simplify, variance in sampled trajectories σ can be set to be 1. POET consists of three components: generation\nof more complex environment R(θ) from coevolution between agent θ and environment R, train an RL model for\ncurrent agent and environment pair, and adaptation of current agent to varying environment. POET belongs to AI-GA\n(AI-generating algorithm) [6] for self-supervised learning [136]. In general, AI-GA consists of three components:\nmeta-learning architecture, learner algorithms and generation of more complex environment.\n3.1.8\nMetaRL with Task Decomposition\nTask decomposition [139] decomposes complex tasks into a hierarchy of easier subtasks under a pre-speciﬁed or learned\ndependence structure. Meta-learning with task decomposition makes adaptation in complex tasks more accurate and\nmore efﬁcient. Dependencies rely upon the sequence of subtasks following a natural logical order, where the next\nsubtask can only be performed after the previous one is ﬁnished. The execution of a subtask may also depend upon the\nknowledge acquired from several other subtasks. Task decomposition follows either an observable logical hierarchy for\nthe subtasks or a dependency structure estimated from unsupervised learning. Plan Parsing [140] may be performed in\na top-down or bottom-up fashion. An optimal task decomposition plan may be learned progressively using genetic\nprogramming optimization, choosing an optimal action at every step. LfD (Learning from demonstration) may apply\nTACO (Task Decomposition via Temporal Alignment for Control) [141] to decompose complex tasks so that robots can\nlearn subtasks more easily from demonstrations. Among different subtasks with few-shot data, in order to estimate\nparameters more accurately, parameter sharing may be assumed for subtasks and data from these subtasks can be\naggregated to estimate shared parameter. Many-layered learning [142] proposes STL (Stream-To-Layers) which learns\na hierarchical structure from online streaming data based upon knowledge dependency.\nMSGI (Meta-Learner with Subtask Graph Inference) [143] models dependency structure in subtasks using the subtask\ngraph G with subtask precondition Gc and subtask reward Gr. Transition model with subtask graph is factorized into\np(s′|s, a) =\nY\ni\npGi\nc(s′\ni|s, a).\nReward with subtask graph is the summation of reward from all subtasks\nR(s, a) =\nX\ni\nRGir(s, a).\n34\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nSubtask precondition is estimated using decision tree where critical information required for the subtask is found via\ndimension reduction and model selection. For an unseen task, subtask graph is learned before adaptation is made with\nMAML.\n3.2\nMeta-Imitation Learning\nFigure 6: Meta-imitation learning. Find a policy such that robots act like demonstration. Interaction between robots and\nenvironment is the same as in demonstration. Robots clone policy and reward in demonstration.\nIn complex unstructured environments where there is no explicit task or reward deﬁnition, we can learn through\nimitation learning. With only one or few demonstrations, meta-learning is introduced to achieve fast adaptation of\nimitation model to changing environment or varying target policy, as in ﬁgure 6. Similar here, meta-learning framework\nis applied to solve few-shot imitation task with deep imitation learning model. Meta-imitation learning applies similar\npast experiences to a novel task in sparse-reward situations. Applications of meta-imitation learning concentrate upon\nrobotics where robots acquire skills from one visual demonstration. [34] proposes a one-shot visual imitation learning\nmodel built upon MAML. Few-shot meta-imitation learning is realized through MAML updates on parameters in novel\nsettings. Demonstration trajectory data is denoted as τ = {o1, a1, · · · , oH, aH} which consists of observation-action\npairs. Similarly, an imitation task is deﬁned as\nTi = [τ ∼π∗\ni , L(a1, · · · , aH, ˆa1, · · · , ˆaH), H],\nwhere H is the ﬁnite time horizon, π∗\ni is an expert policy we target to imitate, and L is an imitation loss function. Loss\nfunction is mean squared error if actions are continuous, and cross-entropy otherwise. Generally imitation loss function\nis speciﬁed as behavior-cloning loss:\nLTi(fφ) = P\nτ (j)∼Ti\nPH\nt=1 ∥fφ(o(j)\nt ) −a(j)\nt ∥2\n2,\nwhere τ (j) is a trajectory from RL task Ti, and fφ is a CNN policy mapping from observation to action. Policy CNN\nis parameterized by weights W and bias b of last layer, that is φ = (W, b). Then behavior-cloning loss function is\nrewritten as\nL∗\nTi(fφ) = P\nτ (j)∼Ti\nP\nt ∥Wy(j)\nt\n+ b −a(j)\nt ∥2\n2.\nHere in [34], policy CNN fφ is trained on one demonstration from target policy π∗\ni and tested on another demonstration\nfrom the same target policy π∗\ni to compute testing error. Meta-objective with MAML update on parameters is written as\nmin\nθ,W,b\nX\nTi∼p(T )\nL∗\nTi(fφi), where L∗\nTi(fφi) = L∗\nTi(fθ−α▽θL∗\nTi(fθ)).\nAfter MAML update on parameters, model imitates one demonstration from a new expert policy. Additionally, in cases\nwithout expert action to imitate, imitation loss function is written as\nL∗\nTi(fφ) =\nX\nτ (j)∼Ti\nX\nt\n∥Wy(j)\nt\n+ b∥2\n2.\nPolicy CNN fφ is directly estimated from one demonstration video.\nA similar meta-imitation model for the case where robots learn from both human and robot demonstrations is proposed\nin [10]. Human demonstration data is denoted as Dh\nTi. Robot demonstration data is Dr\nTi. Human video dh ∼Dh\nTi.\nRobot video dr = {o1, · · · , oH, s1, · · · , sH, a1, · · · , aH} ∼Dr\nTi. MAML update on task-speciﬁc policy parameter is\nφT = θ −α ▽θ Lψ(θ, dh),\nwhere Lψ parameterized by ψ is the adaptation objective. Behaviorial cloning loss function is\nLBC(φ, dr) = P\nt log πφ(at|ot, st).\nMeta-objective is the combination of MAML policy parameter adaptation and behavioral cloning loss function:\nmin\nθ,ψ\nX\nT ∼p(T )\nX\ndh∈Dh\nT\nX\ndr∈Dr\nT\nLBC(φT , dr).\n35\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nThen MAML update on meta-parameter is\n(θ, ψ) ←(θ, ψ) −β ▽θ,ψ LBC(φT , dr).\nFrom the probabilistic perspective, φT is an MAP estimate of φ based upon approximate inference on log p(φ|Dtr\nT , θ)\n[101].\nHigh-ﬁdelity imitation model MetaMimic proposed in [144] stores memory of off-policy RL and replays memory for\nimitation learning. Policy model is speciﬁed to be a wide deep neural network with batch normalization. Usually larger\ndeeper neural network policy model in off-policy RL brings better adaptation to novel tasks.\nExperience replay memory is denoted as M. Memory items are constructed in the following procedure. Action\nat ∼π(ot, dt+1), where dt+1 is the demonstration video that leads the imitation. Execute at. Then observe ot+1 and\ntask reward rtask\nt\n. Compute imitation reward rimitate\nt\n= g(ot+1, dt+1), which measures the similarity between learned\nobservation and video demonstration. The following item (ot, at, ot+1, rtask\nt\n, rimitate\nt\n, dt+1) is stored in memory M.\nOne demonstration is denoted as d = {d1, · · · , dH} ∼p(d). Imitation policy is speciﬁed as πθ parameterized by θ.\nAction at = πθ(ot, d). Environment renderer E(π) maps action to observation E(π) = {o1, · · · , oH}. Policy parameter\nθ is estimated by maximizing imitation reward:\nmax\nθ\nEd∼p(d)\n\" H\nX\nt=1\nγtg [dt, E(πθ(ot, d))t]\n#\n,\nwhere γ > 0 is the discount factor, and g is a similarity measure. By maximizing imitation reward, similarity between\nlearned observation and demonstration is maximized. Imitation reward is the Euclidean distance between learned\nobservation and demonstration\nrimitate\nt\n= β1 exp(−∥oimage\nt+1\n−dimage\nt+1\n∥2\n2) + β2 exp(−∥obody\nt+1 −dbody\nt+1 ∥2\n2),\nwhere obody\nt\nis the body position coordinate and velocity, and oimage\nt\nis the image pixel capturing interactions between\nbody and objects.\nWTL (Watch-Try-Learn) [35] is a meta-imitation learning (MIL) model which combines MIL with trial-and-error\nRL. It includes a trial procedure which generates a policy from one demonstration and a retrial process that compares\ntrials with demonstration to identify a successful policy. Identiﬁability issue may arise in which policy is not uniquely\ndetermined by one demonstration.\nA trajectory from RL task Ti is denoted as τi = [(s1, a1, ri(s1, a1)), · · · , (sH, aH, ri(sH, aH))]. Demonstration\ntrajectory is denoted as d = [(s1, a1), · · · , (sH, aH)]. Training demonstrations {di,k} are sampled from Di. Trial\npolicy πθ(a|s, {di,k}) is proposed based upon one demonstration. Trial objective to estimate trial policy πθ is\nL(θ, Di) = E{di,k}∼DiEdtest\ni\n∼Di\\{di,k}E(st,at)∼dtest\ni\n[−log πθ(at|st, {di,k})].\nMAML is applied to update θ with trial objective:\nθ ←θ −α ▽θ L(θ, Di).\nTrial trajectories are generated from trial policy {τi,l} ∼πθ(a|s, {di,k}).\nDemonstration-trial trajectory pairs\n({di,k}, {τi,l}) are sampled from D∗\ni . A retrial policy integrates generated trials with demonstration to learn a successful\npolicy πφ(a|s, {di,k}, {τi,l}). Retrial objective to estimate a successful policy πφ is\nL(φ, Di, D∗\ni ) = E({di,k},{τi,l})∼D∗\ni Edtest\ni\n∼Di\\{di,k}E(st,at)∼dtest\ni\n[−log πφ(at|st, {di,k}, {τi,l})].\nMAML is applied to update φ with retrial objective:\nφ ←φ −β ▽φ L(φ, Di, D∗\ni ).\n3.3\nOnline Meta-learning\nOnline learning is applied to streaming data where a small batch of data is obtained each time and fast adaptation is\nrequired for timely reaction to continually changing environment. In online meta-learning, a small batch of streaming\ndata obtained each time is regarded as a task dataset, as in ﬁgure 7. In [77], FTML (Follow The Meta Leader) is\nconstructed by integrating a classical online algorithm follow the leader (FTL) with MAML. FTL is proved to guarantee\nstrong performance for smooth convex loss functions.\nFor task Ti, MAML updates parameter θ using\nUi(θ) = θ −α ▽θ LTi(hθ).\n36\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nRegret function from FTL measures the distance between current value and optimal target value\nR(θ) =\nJ\nX\ni=1\nLTi(hUi(θ)) −min\nθ\nJ\nX\ni=1\nLTi(hUi(θ)).\n(2)\nBy estimating optimal task-speciﬁc parameter that minimizes regret function, current value is closest to optimum. Since\noptimum is a ﬁxed target, minimization of regret function is equivalent to minimizing ﬁrst component in equation (2).\nTasks are streaming data and follow a natural time order so that FTML updates task-speciﬁc parameters by minimizing\nthe summation of task-loss up to task Ti:\nφi+1 = argminθ\ni\nX\nk=1\nLTk(hUk(θ)).\nIn order to solve this optimization online, we apply practical online gradient computation:\ndi(θ) = ▽θEk∼vtL(Dval\nk , Uk(θ)), where Uk(θ) ≡θ −α ▽θ L(Dtr\nk , θ),\nand vt is a sampling distribution from tasks T1, · · · , Ti.\nFigure 7: Online meta-learning. Each data batch in streaming data is regarded as a task dataset. Base learner is applied\nto each task and meta-learner updates base learner across tasks.\nIn robotics, robots need continually rapid and precise adaptation to online streaming data especially when simulated\nmechanical failures or extreme environmental conditions suddenly occur. In such out-of-distribution tasks, fast\nadaptation of trained problem-solving model is required. In [67], deep online meta-learning and nonparametric Bayes\nare integrated to provide a feasible solution for robots to survive. In meta-learning, we concern adaptation of deep\nlearning models to unseen few-shot in-distribution or out-of-distribution tasks. Task-speciﬁc parameters reﬂect task-\nspeciﬁc features of each unseen task. Meta-parameters reﬂect features shared by multiple tasks. We can see that the\nwhole meta-learning system is around the task distribution. This online meta-learning utilizes nonparametric Bayesian\ntheories to compute posterior task distribution. This method regards tasks to be countably inﬁnitely many and assumes\ntask distribution to be one of nonparametric Bayesian distribution types. After obtaining task posterior distribution,\nwe can judge whether each unseen task is in-distribution or out-of-distribution and obtain posterior distributions of\ntask-speciﬁc parameters and meta-parameters.\nTask distribution is P(Tt = Ti|xxxt, yt), where the set of all tasks is a countable inﬁnite set. Expectation-maximization\n(EM) is applied to maximize log likelihood:\nL = ETi∼p(Tt|xxxt,yt)[log pφt(Ti)(yt|xxxt, Ti)],\nwhere φt(Ti) is the task-speciﬁc parameter. Posterior distribution of task is\np(Tt = Ti|xxxt, yt) ∝pφt(Ti)(yt|xxxt, Tt = Ti)P(Tt = Ti).\nNonparameteric Bayesian prior Chinese restaurant process (CRP) is used as the prior distribution on task:\nP(Tt = Ti) =\nPt−1\nk=1 P (Tk=Ti)\nt−1+α\n,\nP(Tt = Tnew) =\nα\nt−1+α,\n37\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nwhere α > 0. Task prior distribution CRP is\nP(Tt = Ti) =\n(t−1\nX\nk=1\nP(Tk = Ti) + δ(Tk = Tnew)α\n)\n.\nMeta-parameter θ is the prior of task-speciﬁc parameter φt. One gradient update of φt from prior θ is built upon FTML\nin [77] where loss function depends upon all trained tasks before t\nφt+1(Ti) = θ −β\nt\nX\nk=0\n[P(Tk = Ti|xxxk, yk) · ▽φk(Ti) log pφk(Ti)(yk|xxxk)].\nMAML update on task-speciﬁc parameter φt(Ti) is\nφt+1(Ti) = φt(Ti) −βP(Tt = Ti|xxxt, yt) · ▽φt(Ti) log pφt(Ti)(yt|xxxt).\nMAP estimator from posterior task distribution is\nT ∗= argmaxTipφt+1(Ti)(yt|xxxt, Tt = Ti).\nPrediction is made based upon task-speciﬁc parameter φt+1(T ∗). Estimation of task-speciﬁc parameters and meta-\nparameters depends upon the MAP estimator from posterior task distribution. MAP estimator of task is a task\nrepresentative of the whole task distribution. Posterior distribution of task-speciﬁc parameters and meta-parametes can\nbe derived from posterior task distribution.\nAnother application of meta-online learning is fast online adaptation of neural network model. [76] proposes ALPaCA\n(Adaptive Learning for Probabilistic Connectionist Architectures). The last layer of a neural network is speciﬁed as\nBayesian linear regression which we need to update in fast adaptation. Posterior distribution of output label from last\nlayer is\np(y|xxx, D∗\nt ) =\nZ\np(y|xxx, θ)p(θ|D∗\nt )dθ,\nwhere D∗\nt = {(xxxi, yi)}t\ni=1 is training data including streaming data before t. A surrogate function qξ(y|xxxt+1, D∗\nt ) is an\napproximation to the posterior distribution of output label. To estimate surrogate function ξ, we minimize KL distance\nmin\nξ\nDKL(p(y|xxxt+1, D∗\nt )∥qξ(y|xxxt+1, D∗\nt )).\nBayesian linear regression at last layer of neural network is yT\nt = φT (xxxt; w)K + ϵt, where ϵt ∼N(0, Σϵ) and Σϵ is\nassumed known. Rewrite regression in matrix form:\nY = ΦK + E,\nwhere K is an unknown parameter and Λ is the precision matrix of pre-trained Φ. Online adaptation of parameters in\nBayesian linear regression can be explicitly expressed. Online adaptation of parameters in Bayesian linear regression is\nthrough the minimization of loss function:\nL( ¯K0, Λ0, w, θ∗)\n=\n−Exxx,y,D∗∼p(xxx,y,D∗|θ∗)[log qξ(y|xxx, D∗)].\nOnline adaptation of parameters in Bayesian linear regression are explicitly expressed as follows\nΛ−1\nt\n←Λ−1\nt−1 −\n(Λ−1\nt−1φt)(Λ−1\nt−1φt)T\n1+φT\nt Λ−1\nt−1φt\n,\nQt ←φtyT\nt + Qt−1, ¯Kt ←Λ−1\nt Qt, ¯yt+1 ←KT\nt φt+1,\nand Σt+1 ←(1 + φT\nt+1Λ−1\nt φt+1)Σϵ.\n3.4\nUnsupervised Meta-learning\nUnsupervised meta-learning applies to cases without labelled data. Unsupervised clustering can provide estimated labels\nfor unlabelled data [71] then supervised meta-learning can be applied regarding estimated labels as true labels. On the\nother hand, unsupervised learning allows parameter adaptation in the inner loop to be independent of data labels [72].\n[19] proposes UMTRA (Unsupervised Meta-learning with Tasks constructed by Random Sampling and Augmentation).\nUnlabeled data is denoted as U. In UMTRA, training data is unlabelled and unsupervised learning method is applied. In\n38\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nrandom labelling, x1, · · · , xN is randomly sampled from training data and is labeled as (x1, 1), · · · , (xN, N), which\nimplies that there are N classes with one image per class. Every sampled image is itself a class. Validation data is\nconstructed from applying data augmentation function to training data x∗\ni = A(xi). Data augmentation is performed by\ngenerating another image in each class and generated image should be indistinguishable from the real image in that\nclass. That is, in validation data, there are two images per class. Then MAML is applied on this task data with N image\nclasses and 2 images under each class. Few-shot image classiﬁcation performance of UMTRA is not as good as MAML\nand Prototypical Network in supervised setting.\nIn MAML, update on task-speciﬁc parameter in the inner loop base learner is through supervised learning using labelled\ndata. In [72], unsupervised learning is utilized in base learner to conduct parameter adaptation to novel tasks. In\n[72], meta-learner still uses supervised learning to update meta-parameters. Base model is speciﬁed as a feedforward\nmultilayer perceptron parameterized by φi. Adaptation of parameters in the inner loop uses unlabeled data and is\nformulated as\nφk+1 = d(φk,xxxi; θ),\nwhere d is the unsupervised update rule of task-speciﬁc parameter φi, and θ is the meta-parameter, φk+1 is the updated\ntask-speciﬁc parameter after k iterations. On each task, base learner updates task-speciﬁc parameter using unlabelled\ntask data. In meta-learner, labelled data are divided into training data and validation data. For example in meta-learner\ntraining data, coefﬁcients are estimated using ridge regression\nˆv = argminv\n\b\n∥y −vTxxx∥2 + λ∥v∥2\t\n.\nMeta-objective is deﬁned to be the cosine similarity between observed labels and predicted labels on meta-learner\nvalidation data:\nMetaObjective(θ) = CosDist(y, ˆvTxxx).\nMeta-parameter is estimated through the optimization of the meta-objective\nθ∗= argminθEtask\n(X\ni\nMetaObjective(φi)\n)\n.\nFor unlabeled data, another perspective is to ﬁrst apply unsupervised clustering to estimate labels and then apply\nsupervised meta-learning. In CACTUs (clustering to automatically construct tasks for unsupervised meta-learning)\n[71], unsupervised learning is ﬁrst applied to estimate data labels which map data to embedding space. Learned cluster\nCk centroid is denoted as ck. Partition from unsupervised clustering and cluster centroids are jointly estimated using\nP, {ck} = argmin{Ck},{ck}\nn\nX\nk=1\nX\nz∈Ck\n∥z −ck∥2\nA,\nwhich minimizes the distance between embeddings and cluster centroids. BiGAN, DeepCluster and Oracle are\ntypical unsupervised clustering methods, which can be combined with MAML or ProtoNets to construct CACTUs.\nOracle+ProtoNets achieves 62.29% accuracy on 5-way 5-shot miniImageNet, comparable to the performance of\nsupervised meta-learning methods. Performance of unsupervised deep clustering models directly affect prediction\nperformance of CACTUs. With better unsupervised deep clustering methods used to estimate data labels, few-shot\nimage classiﬁcation prediction accuracy of CACTUs will be even higher.\n3.5\nMeta-Learning for Machine Cognition\nFor multi-agent problems, interactions between agents include competition and cooperation, which can be modelled\nwith game theory under information asymmetry or RL under limited resources. Other homogenous agents may not be\nthe same from the cognition of one agent. Different agents may view the same task to be different and reach vastly\ndifferent solutions. Meta-learning can be applied to generalize a model for this group of agents to another group.\nMeta-learning can also generalize the model for an agent in this group to another agent in the same group.\nMachine theory of mind [145] proposes ToMnet (Theory of Mind neural network) using meta-learning to understand\ncognition of others based upon self-cognition. Under meta-learning framework, others’ cognition is a few adaptations\nfrom self-cognition. ToMnet is used to predict the future behavior of opponents based upon prior observations for\ndecision making in multi-agent AI model. Each prediction in ToMnet combines a pre-trained neural network model\nupon individual agents and a neural network model upon the current mental status of this agent. Prediction from ToMnet\nis approximately optimal from the perspective of Bayesian online learning. As like all meta-learning algorithms such as\nbase learner and meta-learner framework where base learner captures individual information and meta-learner learns\nshared information, ToMnet combines prior belief on all agents and online agent-speciﬁc information to make efﬁcient\nadaptation.\n39\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\n3.6\nMeta-Learning for Statistical Theories\nIn R2D2 [85] and MetaOptNet [81], statistical learners such as SVM and logistic regression are used as base learner\nunder meta-learning framework. They achieve high prediction accuracy in few-shot image classiﬁcation. This is\none positive aspect of integrating statistical learning and meta-learning. Since meta-learning focuses upon improving\ngeneralization capability of machine learning algorithms, it can also be applied to broaden the scope of statistical\nmethods. [146] proposes MetaVAE (Meta Variational Autoencoders) which improves generalization of traditional\nstatistical variational inference methods to new generative distributions. Let x denote input training data, z denote\nlearned feature from input, pDi(x) denote marginal data distribution. Meta-distribution pM is a distribution over all\nmarginal data distributions pDi(x). Meta-inference model is a mapping from data distribution and input training data to\nlearned feature vector gφ : pM × x →z. The objective function is to optimize\nmax\nφ\nEpDi∼pM\n\u0014\nEpDi(x)\n\u0014\nEgφ(pDi,x) log\npθi(x, z)\ngφ(pDi, x)(z)\n\u0015\u0015\n.\nMetaVAE is formulated as\nmax\nφ\nEpDi∼pM\n\u0014\nmax\nθi Lφ,θi(pDi)\n\u0015\n,\nwhere Lφ,θi(pDi) = −DKL(pDi(x)gφ(pDi, x)∥p(z)pθi(x|z)).\nInference designed this way is robust to a large family of data distributions pM. Since parametric data distribution is\nconsidered here, it is possible to construct sufﬁcient statistics of data that is used to build learned feature vector.\n4\nConclusion and Future Research\nOur survey provides an overview upon meta-learning research including black-box adaptation, similarity-based approach,\nbase learner and meta-learner model, Bayesian meta-learning, meta-RL, meta-imitation learning, online meta-learning\nand unsupervised meta-learning. There are more branches of meta-learning covering integration with causal reasoning,\nchange-point estimation, Bayesian deep learning and real-life applications to explore in the future.\nMeta-learning concentrates upon adaptation to out-of-distribution tasks. For in-distribution tasks, similarity structure\nbetween tasks is more explicit. Models for adaptation to in-distribution tasks should emphasize upon mining similarity\nstructure to solve few-shot unseen tasks more efﬁciently. Methods developed for out-of-distribution tasks should\nmaximize model generalization ability. There exists a tradeoff between model ﬁtting and model generalization. Base\nlearner focuses upon adaptation to each task and meta-learner concentrates upon generalization, which offer a ﬂexible\ncombination to strike a balance in between. We can obtain an estimate of task distribution and judge whether an unseen\ntask is in-distribution or out-of-distribution.\nIn terms of application, meta-learning presents an opportunity for improving model adaptivity across changing\nenvironments. For theoretical research, meta-learning provides a ﬂexible framework of integrating different methods\ncombining advantages while complementing disadvantages of each other. Statistical models are less prone to over-\nﬁtting and can be combined with machine learning methods under meta-learning framework. Meta-learning allows\nus to partition tasks into levels, apply base learner, meta-learners and other learners at different levels constructing\nan integrated solution for a complex mission. Communication between base learner and meta-learners at different\nlevels is not limited to initial values, aggregating gradients, objectives and learned optimizers. More complex model\nspeciﬁcations can be designed to suit the need of real-life applications.\nMeta-learning is not just about solving few-shot in-distribution or out-of-distribution tasks with deep learning models.\nHowever, with the amount of data available nowadays and wide applications of deep learning models, most application\nproblems can be solved with sufﬁciently large amount of data and sufﬁciently complex deep models to obtain satisfactory\nprediction accuracy. We do not see as many few-shot tasks. Actually it depends on how we deﬁne ’few-shot’. With\nhigher prediction accuracy, the number of cases where deep models fail is small. These failing cases constitute a\n’few-shot’ task. As mentioned earlier, with meta-learning framework or components embedded, predictive performance\nof deep models on in-distribution tasks is still high, and predictive performance of deep models on out-of-distribution\ntasks is improved. Meta-learning framework or components can usually accelerate training of deep models and improve\nperformance of deep models on out-of-distribution tasks.\nWhat remains unclear about the objective of meta-learning is whether we should allow meta-learning framework to\nevolve by itself to more complex tasks and corresponding solution to complex tasks. It is a promising ﬁeld, since\nwe can actually solve complex tasks which are unsolvable by training from scratch. But uncontrolled evolution of\nmeta-learning framework may create too high autonomy in machine intelligence bringing ethical concerns. From the\n40\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\nperspective of optimization, meta-learning framework can be viewed as an optimization technique. What it presents\nseems to be solving complex tasks using genetic algorithm for ﬁnding global optimization. Evolution of complex tasks\ncan be seen as a meta-learner accumulating problem-solving experience by solving easier and simpler tasks. Then\nmeta-learner guides base learner to ﬁnd a solution much closer to global optimum. This is actually very similar to\napplication of meta-learning in AutoML where deep neural network model grows to solve more complex problems and\nreach higher prediction accuracy. In the end, we have a deep neural network model with highest prediction accuracy,\neven higher than human-designed/tuned deep neural network model. In the searching for the global optimum deep\nneural network model, current model with highest prediction accuracy is a moving target for meta-learner to beat.\nTask is taken to be beating current model with highest prediction accuracy. Task evolves to be more complex along\nthe searching process of AutoML. Solution is taken to be a deep neural network model that can beat the current best\nmodel. With progressive evolution of task, solution also becomes more complex. As a result, AI-GAs can be viewed as\nAutoML in deep RL, where AutoML is for modelling policy in deep RL.\nMeta-learning seeks solving vastly different few-shot tasks with deep learning models. Meta-learning is a ﬂexible\nframework with clear objectives but no clear form of speciﬁc models or protocols. Many meta-learning modelling\nroutes may be combined to form an integrated model. Meta-learning can also be integrated into other machine learning\nframeworks or deep learning models. This is not a paper about methodology classiﬁcation, but a paper about introducing\nmeta-learning frameworks and applications with famous meta-learning methods as examples.\nAcknowledgment\nThanks to Debasmit Das, Louis Kirsch and Luca Bertinetto (in alphabetical order) for useful and valuable comments on\nthis manuscript.\nReferences\n[1] Jürgen Schmidthuber. Evolutionary principles in self-referential learning (Diploma Thesis). 1987.\n[2] Jürgen Schmidhuber. On Learning to Think: Algorithmic Information Theory for Novel Combinations of\nReinforcement Learning Controllers and Recurrent Neural World Models. 2015.\n[3] Felix A. Gers, Jurgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with LSTM.\nIEE Conference Publication, 2(470):850–855, 1999.\n[4] Steven Younger, Sepp Hochreiter, and Peter Conwell. Meta-learning with backpropagation. Proceedings of the\nInternational Joint Conference on Neural Networks, pages 2001–2006, 2001.\n[5] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In 5th International Conference\non Learning Representations, ICLR - Conference Track Proceedings, pages 1–11, 2017.\n[6] Jeff Clune. AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artiﬁcial intelligence.\n2019.\n[7] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley. Paired open-ended trailblazer (POET): Endlessly\ngenerating increasingly complex and diverse learning environments and their solutions. pages 1–28, 2019.\n[8] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data\nEngineering, 22(10):1345–1359, 2010.\n[9] Frank Hutter. Automated machine learning. 2019.\n[10] Tianhe Yu, Chelsea Finn, Annie Xie, Sudeep Dasari, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot\nimitation from observing humans via domain-adaptive meta-learning. 6th International Conference on Learning\nRepresentations, ICLR - Workshop Track Proceedings, 2018.\n[11] Jiechao Guan, Zhiwu Lu, Tao Xiang, and Ji-Rong Wen. Few-Shot Learning as Domain Adaptation: Algorithm\nand Analysis. 2020.\n[12] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. 34th International Conference on Machine Learning, ICML, 3:1856–1868, 2017.\n[13] Gregory Koch and Gregory Koch. Siamese Thesis. PhD thesis, 2015.\n[14] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks\nfor one shot learning. Advances in Neural Information Processing Systems, pages 3637–3645, 2016.\n[15] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in Neural\nInformation Processing Systems, pages 4078–4088, 2017.\n41\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\n[16] Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-SGD: Learning to Learn Quickly for Few-Shot\nLearning. 2017.\n[17] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales. Learning to\ncompare: relation network for few-shot learning. Proceedings of the IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition, pages 1199–1208, 2018.\n[18] Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E Turner. VERSA: Versatile\nand efﬁcient few-shot learning. Advances in Neural Information Processing Systems, pages 1–9, 2018.\n[19] Siavash Khodadadeh, Ladislau Bölöni, and Mubarak Shah. Unsupervised meta-learning for few-shot image\nclassiﬁcation. 2018.\n[20] Yaqing Wang and Quanming Yao. Few-shot Learning: A Survey. Arxiv, pages 1–41, 2019.\n[21] Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler. Rapid adaptation with conditionally\nshifted neurons. 35th International Conference on Machine Learning, ICML, 8:5898–5909, 2018.\n[22] Ke Li and Jitendra Malik. Learning to optimize. In 5th International Conference on Learning Representations,\nICLR - Conference Track Proceedings, 2017.\n[23] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In 34th International Conference on Machine Learning,\nICML, volume 5, pages 3933–3943, 2017.\n[24] Nicolas Schweighofer and Kenji Doya. Meta-learning in reinforcement learning. Neural Networks, 16(1):5–9,\n2003.\n[25] Yesmina Jaafra, Jean Luc Laurent, Aline Deruyver, and Mohamed Saber Naceur.\nA Review of Meta-\nReinforcement Learning for Deep Neural Networks Architecture Search. pages 1–29, 2018.\n[26] Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Unsupervised Meta-Learning for\nReinforcement Learning. pages 1–15, 2018.\n[27] Jane X. Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Demis Has-\nsabis, and Matthew Botvinick. Prefrontal cortex as a meta-reinforcement learning system. Nature Neuroscience,\n21(6):860–868, 2018.\n[28] Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A. Ortega, Yee Whye Teh, and Nicolas Heess.\nMeta reinforcement learning as task inference. 2019.\n[29] Kate Rakelly, Aurick Zhou, Deirdre Quiilen, Chelsea Finn, and Sergey Levine. Efﬁcient off-policy meta-\nreinforcement learning via probabilistic context variables. 36th International Conference on Machine Learning,\nICML, pages 9291–9301, 2019.\n[30] Ishita Dasgupta, Jane Wang, Silvia Chiappa, Jovana Mitrovic, Pedro Ortega, David Raposo, Edward Hughes,\nPeter Battaglia, Matthew Botvinick, and Zeb Kurth-Nelson. Causal Reasoning from Meta-reinforcement\nLearning. 2019.\n[31] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, and Chelsea\nFinn. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. 7th\nInternational Conference on Learning Representations, ICLR, pages 1–17, 2019.\n[32] Yan Duan, Marcin Andrychowicz, Bradly Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel,\nand Wojciech Zaremba. One-shot imitation learning. In Advances in Neural Information Processing Systems,\npages 1088–1099, 2017.\n[33] Chelsea Finn. Meta-learning: from few-shot learning to rapid reinforcement learning. 36th International\nConference on Machine Learning ICML Tutorial session, 2019.\n[34] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep\nnetworks. 34th International Conference on Machine Learning, ICML 2017, 3:1856–1868, 2017.\n[35] Allan Zhou, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal\nKalakrishnan, Sergey Levine, and Chelsea Finn. Watch, Try, Learn: Meta-Learning from Demonstrations and\nReward. 2019.\n[36] Stuart Russell and Eric Wefald. Principles of metareasoning. Artiﬁcial Intelligence, 49(1-3):361–395, 1991.\n[37] Jürgen H. Schmidhuber. On Learning how to Learn Learning Strategies. Technical Report FKI-198-94, pages\n1–20, 1994.\n[38] Jürgen Schmidhuber, Jieyu Zhao, and Nicol N. Schraudolph. Reinforcement Learning with Self-Modifying\nPolicies. Learning to Learn, pages 293–309, 1998.\n42\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\n[39] Jürgen Schmidhuber. A Neural Network the Embeds its own Meta-Levels. In IEEE International Conference on\nNeural Networks, pages 407–412, 1993.\n[40] Rich Caruana. Learning many related tasks at the same time with backpropagation. Technical report, 1995.\n[41] Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural Computation, 12(8):1889–1900, 2000.\n[42] Donald R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global\nOptimization, 21(4):345–383, 2001.\n[43] Kenneth O. Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topologies. Evolu-\ntionary Computation, 10(2):99–127, 2002.\n[44] Eric Pellerin, Luc Pigeon, and Sylvain Delisle. A meta-learning system based on genetic algorithms. Data\nMining and Knowledge Discovery: Theory, Tools, and Technology VI, 2004.\n[45] Pavel Kordík, Jan Koutník, Jan Drchal, Oleg Kováˇrík, Miroslav ˇCepek, and Miroslav Šnorek. Meta-learning\napproach to neural network optimization. Neural Networks, 23(4):568–582, 2010.\n[46] James Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl. Algorithms for hyper-parameter optimization.\n25th Annual Conference on Neural Information Processing Systems (NIPS), 2011.\n[47] Pericles B.C. Miranda, Ricardo B.C. Prudencio, Andre Carlos P.L.F. De Carvalho, and Carlos Soares. Multi-\nobjective optimization and Meta-learning for SVM parameter selection. Proceedings of the International Joint\nConference on Neural Networks, 2012.\n[48] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine\nLearning Research, 13:281–305, 2012.\n[49] Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter optimization through\nreversible learning. feb 2015.\n[50] S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray Kavukcuoglu, and\nGeoffrey E. Hinton. Attend, infer, repeat: Fast scene understanding with generative models. Advances in Neural\nInformation Processing Systems, pages 3233–3241, 2016.\n[51] Marcilio C.P. De Souto, Ricardo B.C. Prudêncio, Rodrigo G.F. Soares, Daniel S.A. De Araujo, Ivan G. Costa,\nTeresa B. Ludermir, and Alexander Schliep. Ranking and selecting clustering algorithms using a meta-learning\napproach. Proceedings of the International Joint Conference on Neural Networks, pages 3729–3735, 2008.\n[52] Christiane Lemke and Bogdan Gabrys. Meta-learning for time series forecasting and forecast combination.\nNeurocomputing, 73(10-12):2006–2016, 2010.\n[53] Enrique Leyva, Antonio González, and Raúl Pérez. A set of complexity measures designed for applying\nmeta-learning to instance selection. IEEE Transactions on Knowledge and Data Engineering, 27(2):354–367,\n2015.\n[54] Tom Bosc. Learning to learn neural networks. Technical report, 2016.\n[55] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using\nreinforcement learning. 5th International Conference on Learning Representations, ICLR- Conference Track\nProceedings, 2017.\n[56] Esteban Real, Chen Liang, David R. So, and Quoc V. Le. AutoML-Zero: Evolving Machine Learning Algorithms\nFrom Scratch. 2020.\n[57] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li Jia Li, Li Fei-Fei, Alan Yuille,\nJonathan Huang, and Kevin Murphy. Progressive Neural Architecture Search. Lecture Notes in Computer\nScience (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), 11205\nLNCS:19–35, 2018.\n[58] L Weng. Meta-Learning: Learning to Learn Fast, 2018.\n[59] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-Learning\nwith Memory-Augmented Neural Networks. 33rd International Conference on Machine Learning, ICML 2016,\n4:2740–2751, 2016.\n[60] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms. arXiv, pages 1–11,\n2018.\n[61] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through\nprobabilistic program induction. Science, 350(6266):1332–1338, 2015.\n43\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\n[62] Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. Bayesian\nmodel-agnostic meta-learning. In Advances in Neural Information Processing Systems, pages 7332–7342, 2018.\n[63] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. RLˆ2: Fast Reinforce-\nment Learning via Slow Reinforcement Learning. pages 1–14, 2016.\n[64] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell,\nDharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. pages 1–17, 2016.\n[65] Nir Levine, Tom Zahavy, Daniel J. Mankowitz, Aviv Tamar, and Shie Mannor. Shallow updates for deep\nreinforcement learning. Advances in Neural Information Processing Systems, pages 3136–3146, 2017.\n[66] Bradly C. Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever.\nSome considerations on learning to explore via meta-reinforcement learning. In Advances in Neural Information\nProcessing Systems, pages 9280–9290, 2018.\n[67] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning: Continual\nadaptation for model-based RL. In 7th International Conference on Learning Representations, ICLR, 2019.\n[68] Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep\nreinforcement learning with a latent variable model. pages 1–19, 2019.\n[69] Yoshua Bengio. Deep Learning of Representations for Unsupervised and Transfer Learning, 2011.\n[70] Vikas K. Garg and Adam Kalai. Supervising unsupervised learning. Advances in Neural Information Processing\nSystems, pages 4991–5001, 2018.\n[71] Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-learning. 7th International\nConference on Learning Representations, ICLR, 2019.\n[72] Luke Metz, Jascha Sohl-Dickstein, Niru Maheswaranathan, and Brian Cheung. Meta-learning update rules for\nunsupervised representation learning. 7th International Conference on Learning Representations, ICLR, pages\n1–27, 2019.\n[73] David Berthelot, Ian Goodfellow, Colin Raffel, and Aurko Roy. Understanding and improving interpolation in\nautoencoders via an adversarial regularizer. 7th International Conference on Learning Representations ICLR,\n2019.\n[74] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised\nlearning of visual features. In European Conference on Computer Vision (ECCV), pages 139–156, 2018.\n[75] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN: Interpretable\nrepresentation learning by information maximizing generative adversarial nets. Advances in Neural Information\nProcessing Systems, pages 2180–2188, 2016.\n[76] James Harrison, Apoorva Sharma, and Marco Pavone. Meta-Learning Priors for Efﬁcient Online Bayesian\nRegression. 2018.\n[77] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online Meta-Learning. 2019.\n[78] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles\nGelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset of datasets for\nlearning to learn from few examples. 2019.\n[79] Brenden M Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B Tenenbaum. One shot learning of simple\nvisual concepts. In {Proceedings of the 33rd Annual Conference of the Cognitive Science Society}, 2011.\n[80] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual\nRecognition Challenge. International Journal of Computer Vision, 115(3):211–252, 2015.\n[81] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable\nconvex optimization. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition, 2019-June:10649–10657, 2019.\n[82] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum, Hugo\nLarochelle, and Richard S. Zemel. Meta-learning for semi-supervised few-shot classiﬁcation. 6th International\nConference on Learning Representations, ICLR - Conference Track Proceedings, pages 1–15, 2018.\n[83] Alex Krizhevsky. Convolutional deep belief networks on CIFAR-10.\n[84] Boris N. Oreshkin, Pau Rodriguez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved\nfew-shot learning. In Advances in Neural Information Processing Systems, pages 721–731, 2018.\n44\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\n[85] Luca Bertinetto, Philip H.S. Torr, João Henriques, and Andrea Vedaldi. Meta-learning with differentiable\nclosed-form solvers. 7th International Conference on Learning Representations, ICLR, pages 1–15, 2019.\n[86] M. Marcus, B. Santorini, and M. Marcinkiewicz. Building a Large Annotated Corpus of English: The Penn\nTreebank. Computational linguistics, 19(2):313, 1993.\n[87] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, and Florian Schroff. Caltech-ucsd Birds 200.\nCaltech-UCSD Technical Report, pages 1–15, 2010.\n[88] Olga Wichrowska, Niru Maheswaranathan, Matthew W. Hoffman, Sergio Gómez Colmenarejo, Misha Denii,\nNando De Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and generalize. 34th International\nConference on Machine Learning, ICML, pages 5744–5753, 2017.\n[89] Eleni Triantaﬁllou, Richard Zemel, and Raquel Urtasun. Few-shot learning through an information retrieval lens.\nAdvances in Neural Information Processing Systems, pages 2256–2266, 2017.\n[90] Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter optimization\nof deep neural networks by extrapolation of learning curves. IJCAI International Joint Conference on Artiﬁcial\nIntelligence, 2015-Janua(Ijcai):3460–3468, 2015.\n[91] Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Accelerating neural architecture search using\nperformance prediction. 6th International Conference on Learning Representations, ICLR 2018 - Workshop\nTrack Proceedings, 2, 2018.\n[92] Lukasz Kaiser, Aurko Roy, Oﬁr Nachum, and Samy Bengio. Learning to remember rare events. 5th International\nConference on Learning Representations, ICLR 2017 - Conference Track Proceedings, 2019.\n[93] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan Yuille. Few-Shot Image Recognition by Predicting Parameters\nfrom Activations. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition, pages 7229–7238, 2018.\n[94] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. 6th\nInternational Conference on Learning Representations, ICLR - Conference Track Proceedings, 2018.\n[95] Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li, and Liwei Wang. Boosting Few-Shot Learning\nWith Adaptive Margin Loss. 2020.\n[96] Spyros Gidaris and Nikos Komodakis. Dynamic Few-Shot Visual Learning Without Forgetting. In Proceedings\nof the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 4367–4375, 2018.\n[97] Alex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv, pages 1–11, 2018.\n[98] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. Learning to\npropagate labels: Transductive propagation network for few-shot learning. 7th International Conference on\nLearning Representations, ICLR, pages 1–11, 2019.\n[99] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and\nRaia Hadsell. Meta-learning with latent embedding optimization. 7th International Conference on Learning\nRepresentations, ICLR, pages 1–17, 2019.\n[100] Harrison Edwards and Amos Storkey. Towards a neural statistician. pages 1–13, 2016.\n[101] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Grifﬁths. Recasting gradient-based meta-\nlearning as hierarchical bayes. 6th International Conference on Learning Representations, ICLR - Conference\nTrack Proceedings, pages 1–13, 2018.\n[102] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, and Frank\nHutter. Efﬁcient and robust automated machine learning. Advances in Neural Information Processing Systems,\n2015-Janua:2962–2970, 2015.\n[103] Chen Xing, Negar Rostamzadeh, Boris N. Oreshkin, and Pedro O. Pinheiro. Adaptive Cross-Modal Few-Shot\nLearning. 2019.\n[104] Sergey Zagoruyko and Nikos Komodakis. Wide Residual Networks. British Machine Vision Conference 2016,\nBMVC 2016, 2016-Septe:87.1–87.12, 2016.\n[105] Debasmit Das and C. S.George Lee. A Two-Stage Approach to Few-Shot Learning for Image Recognition. IEEE\nTransactions on Image Processing, 29:3336–3350, 2020.\n[106] R Vilalta and Y Drissi. A perspective view and survey of meta-learning. Artiﬁcial Intelligence Review, pages\n77–95, 2002.\n[107] Jeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially Private Meta-Learning. 2019.\n45\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\n[108] Debang Li, Junge Zhang, and Kaiqi Huang. Learning to Learn Cropping Models for Different Aspect Ratio\nRequirements. Cvpr, 2020.\n[109] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the Convergence Theory of Gradient-Based\nModel-Agnostic Meta-Learning Algorithms. 2019.\n[110] Hao Liu, Richard Socher, and Caiming Xiong. Taming MAML: Efﬁcient unbiased meta-reinforcement learning.\n36th International Conference on Machine Learning, ICML 2019, 2019-June:7156–7169, 2019.\n[111] Xingyou Song, Wenbo Gao, Yuxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, and Yunhao Tang. ES-\nMAML: Simple Hessian-Free Meta Learning. 2019.\n[112] Jonas Rothfuss, Tamim Asfour, Dennis Lee, Ignasi Clavera, and Pieter Abbeel. PrOMP: Proximal meta-policy\nsearch. In 7th International Conference on Learning Representations, ICLR, 2019.\n[113] Jianzhu Guo, Xiangyu Zhu, Chenxu Zhao, Dong Cao, Zhen Lei, and Stan Z. Li. Learning Meta Face Recognition\nin Unseen Domains. 2020.\n[114] Myungsub Choi, Janghoon Choi, Sungyong Baik, Tae Hyun Kim, and Kyoung Mu Lee. Scene-Adaptive Video\nFrame Interpolation via Meta-Learning. 2020.\n[115] Antreas Antoniou, Amos Storkey, and Harrison Edwards. How to train your MAML. In 7th International\nConference on Learning Representations, ICLR, 2019.\n[116] Jessica Lee, Deva Ramanan, and Rohit Girdhar. MetaPix: Few-Shot Video Retargeting. 2019.\n[117] Ting Chun Wang, Ming Yu Liu, Jun Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-Resolution\nImage Synthesis and Semantic Manipulation with Conditional GANs. Proceedings of the IEEE Computer Society\nConference on Computer Vision and Pattern Recognition, pages 8798–8807, 2018.\n[118] Y. NESTEROV. A method for solving the convex programming problem with convergence rate O(1/kˆ2).\nAmerican Mathematical Society, 27(2):372–376, 1983.\n[119] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic\noptimization. Technical report, 2010.\n[120] Matthew D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. dec 2012.\n[121] Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In 3rd International\nConference on Learning Representations, ICLR - Conference Track Proceedings. International Conference on\nLearning Representations, ICLR, 2015.\n[122] Jürgen Schmidhuber. Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization\nCapability. Neural Networks, 10(5):857–873, 1997.\n[123] Jaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee, Youngduck Choi, Yongseok Choi,\nDong-Yeon Cho, and Jiwon Kim. Auto-meta: Automated gradient based meta learner search. pages 1–8, 2018.\n[124] Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference algorithm.\nAdvances in Neural Information Processing Systems, pages 2378–2386, 2016.\n[125] Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. Uncertainty in\nArtiﬁcial Intelligence - Proceedings of the 33rd Conference, UAI 2017, 2017.\n[126] Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. 2018.\n[127] Jürgen Schmidhuber. Goedel Machines: Self-Referential Universal Problem Solvers Making Provably Optimal\nSelf-Improvements. 2003.\n[128] Xin Yao. A Review of Evolutionary Artiﬁcial Neural Networks. International Journal of Intelligent Systems,\n4:203–222, 1993.\n[129] Bradly Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Hya Sutskever.\nThe importance of sampling in meta-reinforcement learning. 32nd Conference on Neural Information Processing\nSystems (NIPS), Montreal, Canada, 2018.\n[130] Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander Smola. Meta-Q-Learning. In ICLR, 2020.\n[131] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech\nZaremba. OpenAI gym. jun 2016.\n[132] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum\nentropy deep reinforcement learning with a stochastic actor. 35th International Conference on Machine Learning,\nICML 2018, 5:2976–2989, 2018.\n46\nA Comprehensive Overview and Survey of Recent Advances in Meta-Learning\nA PREPRINT\n[133] Louis Kirsch, Sjoerd van Steenkiste, and Jürgen Schmidhuber. Improving Generalization in Meta Reinforcement\nLearning using Learned Objectives. 2019.\n[134] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic\npolicy gradient algorithms. 31st International Conference on Machine Learning, ICML 2014, 1:605–619, 2014.\n[135] Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and\nDaan Wierstra. Continuous Control with Deep Reinforcement Learning. ICLR, 2016.\n[136] Jürgen Schmidhuber. Making the World Differentiable : On Using Self-Supervised Fully Recurrent Neural\nNetworks for Dynamic Reinforcement Learning and Planning in Non-Stationary Environments ( TR FKI-126-90\n). Neural Networks, pages 1–26, 1990.\n[137] Ferran Alet, Martin F. Schneider, Tomas Lozano-Perez, and Leslie Pack Kaelbling. Meta-learning curiosity\nalgorithms. 2020.\n[138] Jürgen Schmidhuber. Artiﬁcial curiosity based on discovering novel algorithmic predictability through coevolu-\ntion. Proceedings of the 1999 Congress on Evolutionary Computation, CEC 1999, 3:1612–1618, 1999.\n[139] Shimon Whiteson, Nate Kohl, Risto Miikkulainen, and Peter Stone. Evolving soccer keepaway players through\ntask decomposition. Machine Learning, 59(1-2):5–30, 2005.\n[140] Anthony Barrett and Daniel S. Weld. Task-decomposition via plan parsing. Proceedings of the National\nConference on Artiﬁcial Intelligence, 2:1117–1122, 1994.\n[141] Kyriacos Shiarlis, Markus Wulfmeier, Sasha Salter, Shimon Whiteson, and Ingmar Posner. TACO: Learning task\ndecomposition via temporal alignment for control. 35th International Conference on Machine Learning, ICML\n2018, 10:7419–7431, 2018.\n[142] P. E. Utgoff and D. J. Stracuzzi. Many-layered learning. Proceedings - 2nd International Conference on\nDevelopment and Learning, ICDL 2002, pages 141–146, 2002.\n[143] Sungryull Sohn, Hyunjae Woo, Jongwook Choi, and Honglak Lee. Meta Reinforcement Learning with Au-\ntonomous Inference of Subtask Dependencies. 2020.\n[144] Tom Le Paine, Sergio Gómez Colmenarejo, Ziyu Wang, Scott Reed, Yusuf Aytar, Tobias Pfaff, Matt W. Hoffman,\nGabriel Barth-Maron, Serkan Cabi, David Budden, and Nando de Freitas. One-shot high-ﬁdelity imitation:\nTraining large-scale deep nets with RL. 2018.\n[145] Neil C. Rabinowitz, Frank Perbet, H. Francis Song, Chiyuan Zhang, and Matthew Botvinick. Machine Theory of\nmind. 35th International Conference on Machine Learning, ICML 2018, 10:6723–6738, 2018.\n[146] Mike Wu, Kristy Choi, Noah Goodman, and Stefano Ermon. Meta-Amortized Variational Inference and Learning.\n2019.\n47\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-04-17",
  "updated": "2020-10-26"
}