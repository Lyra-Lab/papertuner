{
  "id": "http://arxiv.org/abs/2004.11694v1",
  "title": "Identifying Semantically Duplicate Questions Using Data Science Approach: A Quora Case Study",
  "authors": [
    "Navedanjum Ansari",
    "Rajesh Sharma"
  ],
  "abstract": "Identifying semantically identical questions on, Question and Answering\nsocial media platforms like Quora is exceptionally significant to ensure that\nthe quality and the quantity of content are presented to users, based on the\nintent of the question and thus enriching overall user experience. Detecting\nduplicate questions is a challenging problem because natural language is very\nexpressive, and a unique intent can be conveyed using different words, phrases,\nand sentence structuring. Machine learning and deep learning methods are known\nto have accomplished superior results over traditional natural language\nprocessing techniques in identifying similar texts. In this paper, taking Quora\nfor our case study, we explored and applied different machine learning and deep\nlearning techniques on the task of identifying duplicate questions on Quora's\ndataset. By using feature engineering, feature importance techniques, and\nexperimenting with seven selected machine learning classifiers, we demonstrated\nthat our models outperformed previous studies on this task. Xgboost model with\ncharacter level term frequency and inverse term frequency is our best machine\nlearning model that has also outperformed a few of the Deep learning baseline\nmodels. We applied deep learning techniques to model four different deep neural\nnetworks of multiple layers consisting of Glove embeddings, Long Short Term\nMemory, Convolution, Max pooling, Dense, Batch Normalization, Activation\nfunctions, and model merge. Our deep learning models achieved better accuracy\nthan machine learning models. Three out of four proposed architectures\noutperformed the accuracy from previous machine learning and deep learning\nresearch work, two out of four models outperformed accuracy from previous deep\nlearning study on Quora's question pair dataset, and our best model achieved\naccuracy of 85.82% which is close to Quora state of the art accuracy.",
  "text": "Identifying Semantically Duplicate Questions Using Data\nScience Approach: A Quora Case Study\nNavedanjum Ansari\nInstitute of Computer Science, University of Tartu\nEstonia\nRajesh Sharma\nInstitute of Computer Science, University of Tartu\nEstonia\nABSTRACT\nIdentifying semantically identical questions on, Question and An-\nswering(Q&A) social media platforms like Quora is exceptionally\nsignificant to ensure that the quality and the quantity of content\nare presented to users, based on the intent of the question and\nthus enriching overall user experience. Detecting duplicate ques-\ntions is a challenging problem because natural language is very\nexpressive, and a unique intent can be conveyed using different\nwords, phrases, and sentence structuring. Machine learning and\ndeep learning methods are known to have accomplished superior\nresults over traditional natural language processing techniques in\nidentifying similar texts.\nIn this paper, taking Quora for our case study, we explored and\napplied different machine learning and deep learning techniques\non the task of identifying duplicate questions on Quora’s question\npair dataset. By using feature engineering, feature importance tech-\nniques, and experimenting with seven selected machine learning\nclassifiers, we demonstrated that our models outperformed previ-\nous studies on this task. Xgboost model with character level term\nfrequency and inverse term frequency is our best machine learn-\ning model that has also outperformed a few of the Deep learning\nbaseline models.\nWe applied deep learning techniques to model four different deep\nneural networks of multiple layers consisting of Glove embeddings,\nLong Short Term Memory, Convolution, Max pooling, Dense, Batch\nNormalization, Activation functions, and model merge. Our deep\nlearning models achieved better accuracy than machine learning\nmodels. Three out of four proposed architectures outperformed\nthe accuracy from previous machine learning and deep learning\nresearch work, two out of four models outperformed accuracy from\nprevious deep learning study on Quora’s question pair dataset, and\nour best model achieved accuracy of 85.82% which is close to Quora\nstate of the art accuracy.\nAuthors addresses: Navedanjum Ansari, Institute of Computer Science, University\nof Tartu, 50090 Estonia; Rajesh Sharma, Institute of Computer Science, University of\nTartu, 50090 Estonia.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nACM Conference, ,\n© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\nhttps://doi.org/0000001.0000001\nKEYWORDS\nQuora, Duplicate question, Machine learning, Deep learning, model,\nneural network\nACM Reference Format:\nNavedanjum Ansari and Rajesh Sharma. 2019. Identifying Semantically\nDuplicate Questions Using Data Science Approach: A Quora Case Study.\nIn Proceedings of ACM Conference. ACM, New York, NY, USA, 11 pages.\nhttps://doi.org/0000001.0000001\n1\nINTRODUCTION\nSocial media platforms are a great success as can be witnessed\nby the number of the active user base. In the age of internet and\nsocial media, there has been a plethora of social media platforms,\nfor example, we have Facebook, for user interaction, LinkedIn, for\nprofessional networking, WhatsApp for chat and video calling,\nStack Overflow for technical queries, Instagram for photo sharing.\nAlong the line, Quora is a Question & Answer platform and builds\naround a community of users to share knowledge and express their,\nopinion and expertise on a variety of topics.\nQuestion Answering sites like Yahoo and Google Answers ex-\nisted over a decade however they failed to keep up the content\nvalue [32] of their topics and answers due to a lot of junk informa-\ntion posted; thus their user base declined. On the other hand, Quora\nis an emerging site for the quality content, launched in 2009 and as\nof 2019, it is estimated to have 300 million active users1. Quora has\n400,000 unique topics2 and domain experts as its user so that the\nusers get the first-hand information from the experts in the field.\nWith the growing repository of the knowledge base, there is\na need for Quora to preserve the trust of the users, maintain the\ncontent quality, by discarding the junk, duplicate and insincere\ninformation. Quora has successfully overcome this challenge by\norganizing the data effectively by using modern data science\napproach to eliminate question duplication.\n1.1\nResearch Problem\nAs for any Q&A, it has become imperative to organize the content\nin a specific way to appeal users to be an active participant by\nposting questions and share their knowledge in respective domain\nof expertise. In keeping the users ’ interest, it is also essential that\nusers do not post duplicate questions and thus multiple answers\nfor a semantically similar question, this is avoided if semantically\nduplicate questions are merged then all the answers are made\navailable under the same subject. Detecting semantically duplicate\nquestions and finding the probability of matching also helps the\nQ&A platform to recommend questions to the user instead of\n1Vox - https://www.vox.com/recode/2019/5/16/18627157/quora-value-billion-question-\nanswer\n2Statistics 2019 - https://foundationinc.co/lab/quora-statistics/\nnavedanjum.ansari@gmail.com\nrajesh.sharma@ut.ee\nACM Conference, ,\nNavedanjum Ansari and Rajesh Sharma\nposting a new one. Given our focus of study, we defined the\nfollowing two research questions:\nRQ1: How can we detect duplicate questions on Quora using\nmachine learning and deep learning methods?\nRQ2: How can we achieve the best possible prediction results\non detecting semantically similar questions ?\nResearch questions one and two have been studied on the first\ndataset released by Quora3, however our aim is to achieve the\nhigher accuracy on this task.\n1.2\nThis Work\nWe have extracted different features from the existing question pair\ndataset and applied various machine learning techniques. After\nemploying feature engineering upon raw dataset, we experimented\nwith different machine learning algorithms to draw our baseline.\nWe also showed that not all features were useful in predicting\nduplicate question and after analyzing and dropping a few of the\nfeatures, our result for ML models slightly improved but did not\ndegrade at all. We also have the existing baseline from the works\nof literature, which we have surpassed. We then tried many deep\nlearning methods to finally experiment with our four best deep\nlearning architectures. With our experiment results, we have shown\nthat deep learning methods are suitable for solving the problem of\ndetecting semantically similar questions. Our deep learning neural\nnetworks performed better than baselines from previous research\nstudies.\nMoreover, our machine learning ensemble model TF-IDF\nachieved the accuracy of 82.33% and higher F1 score compared\nto literature [31]. Also, our best deep learning model achieved an\naccuracy of 85.82%. Three out of four presented deep learning mod-\nels outperformed the results from the literature [1, 6, 30, 31] and\nour best result achieved close to Quora’s state of the art accuracy\npresented by Quora engineering team on their blogpost [20].\n2\nLITERATURE REVIEW\nThe previous work to detect duplicate question pairs using Deep\nlearning approach [1], shows that deep learning approach achieved\nsuperior performance than traditional NLP approach. They used\ndeep learning methods like convolutional neural network(CNN),\nlong term short term memory networks (LSTMs), and a hybrid\nmodel of CNN and LSTM layers. Their best model is LSTM network\nthat achieved accuracy of 81.07% and F1 score of 75.7%. They\nused GloVe word vector of 200 dimensions trained using 27 billion\nTwitter words in their experiments.\nThe method proposed in [17] makes use of Siamese GRU neu-\nral network to encode each sentence and apply different distance\nmeasurements to the sentence vector output of the neural network.\nTheir approach involves a few necessary steps. The first step was\ndata processing, which involves tokenizing the sentences in the\nentire dataset using the Stanford Tokenizer4 . This step also in-\nvolved changing each question to a fixed length for allowing batch\ncomputation using matrix operations. The second step involves\n3https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs\n4https://nlp.stanford.edu/software/tokenizer.shtml\nsentence encoding, where they used both recurrent neural net-\nwork(RNN) and gated recurrent unit (GRU). They initialized the\nword embedding to the 300-dimensional GloVe vectors [27].\nThe next step was determining the distance measure [21] that\nare used in combining the sentence vectors to determine if they\nare semantically equivalent. There were two approaches for this\nstep, the first being calculating distances between the sentence\nvectors and running logistic regression to make the prediction. The\npaper has tested cosine distance, Euclidean distance, and weighted\nManhattan distance. The problem here is that it is difficult to know\nthe natural distance measure encoded by the neural network. To\ntackle this issue, they replaced the distance function with a neural\nnetwork, leaving it up to this neural network to learn the correct\ndistance function. They provided a row concatenated vector as\ninput to the neural network and also experimented using one\nlayer and two- layer in the neural network. The paper utilized\ndata augmentation as an approach to reduce overfitting. They\nalso did a hyperparameter search by tuning the size of the neural\nnetwork hidden layer (to 250) and the standardized length of the\ninput sentences (to 30 words) which led to better performance.\nIn the literature [30], authors have used word ordering and word\nalignment using a long-short-term-memory(LSTM) recurrent neu-\nral network [10], and the decomposable attention model respec-\ntively and tried to combine them into the LSTM attention model to\nachieve their best accuracy of 81.4%. Their approach involved im-\nplementing various models proposed by various papers produced\nto determine sentence entailment on the SNLI dataset. Some of\nthese models are Bag of words model, RNN with GRU and LSTM\ncell, LSTM with attention, Decomposable attention model.\nLSTM attention model performed well in classifying sentences\nwith words tangentially related. However, in cases were words in\nthe sentences have a different order; the decomposable attention\nmodel [26] achieves better performance. This paper [26] tried to\ncombine the GRU/LSTM model with the decomposable attention\nmodel to gain from the advantage of both and come up with\nbetter models with better accuracy like LSTM with Word by Word\nAttention, and LSTM with Two Way Word by Word Attention.\nIn the relevant literature [31], the authors have experimented\nwith six traditional machine learning classifiers. They used a\nsimple approach to extract six simple features such as word counts,\ncommon words, and term frequencies(TF-IDF) [28] on question\npairs to train their models. The best accuracy reported in this work\nis 72.2% and 71.9% obtained from binary classifiers random forest\nand KNN, respectively.\nFinally, we reviewed the experiments by Quora’s engineering\nteam [20]. In production, they use the traditional machine learning\napproach using random forest with tens of manually extracted\nfeatures. Three architectures presented in their work use LSTM in\ncombination with attention, angle, and distances. The point noted\nfrom this literature is that Quora uses the word embedding from\nits Quora Corpus whereas all other selected baselines from the\nliterature review used GloVe [27] pre-trained word to vectors from\nthe glove project5.\n5https://nlp.stanford.edu/projects/glove\nIdentifying Semantically Duplicate Questions Using Data Science Approach: A Quora Case Study\nACM Conference, ,\nTable 1: Performance Baseline from selected literature\nPaper\nModel\nTechnique\nAcc\nF1 score\nDetection of Duplicates\nin Quora and Twitter\nCorpus [31]\nLogistic Regression\nMachine Learning\n0.671\n0.66\nDecision Tree\n0.693\n0.69\nKNN\n0.719\n0.72\nRandom Forest\n0.722\n0.73\nDetermining Entailment\nof Questions in the\nQuora Dataset [30]\nLSTM\nDeep learning\n0.784\n0.8339\nLSTM with Attention\n0.81\n0.8516\nLSTM with Two Way Word by Word\nAttention\n0.814\n0.8523\nDecomposable Attention Model\n0.798\n0.8365\nQuora Question\nDuplication [6]\nSiamese with bag of words\nDeep learning\n77.3\n73.2\nSiamese with LSTM\n83.2\n79.3\nSeq2Seq LSTM with Attention\n80.8\n76.4\nEnsemble\n83.8\n79.5\nDuplicate Question Pair\nDetection\nwith\nDeep\nLearning [1]\nLSTM (twitter word embedding 200d)\nDeep learning\n0.8107\n0.757\nQuora State of the\nArt [20]\nLSTM with concatenation\nDeep learning\n0.87\n0.87\nLSTM with distance and angle\n0.87\n0.88\nDecomposable attention\n0.86\n0.87\nThe results achieved in each of the previous studies on Quora\nduplicate question pair dataset is summarized as presented in\nTable 1\n3\nDATASET\nIn this section, we briefly describe the data collection, exploratory\ndata analysis, data visualization, and data cleaning process.\n3.1\nData collection\nThe data for this research work is taken from the First Quora Dataset\nrelease hosted on Amazon S36. There is a total of 404290 rows in\nthe dataset, which indicates that there are total 404290 question\npairs, and the overall file size is 55.4 MB.\nGloVe pre-trained word vectors are used for word embeddings.\nGloVe [27] pre-trained vectors are available at SNLI project site\nGlove. To convert word to vector for distance calculation, we used\nGoogle news vectors [25]GoogleNews-vectors-negative300.bin.gz, of\n3 million words and 300 dimensions.\n3.2\nData Exploration\nWe performed the necessary statistics on the dataset, which helps\nus to give a more detailed understanding of the duplicate Quora\nquestion dataset. There is a total of six columns in the dataset. Each\nof the columns is meaningful and describe the characteristic of\nthe row. The description of the columns is as described below in\nTable 2.\n3.3\nDataset Representation\nTable 3 contains the total number of question pairs and the dis-\ntribution of class labels. Positive samples are those identified as\n6http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\nTable 2: Description of columns in dataset\nColum Name\nDescription\nid\nA unique identifier assigned to each row in\nthe dataset. The first row has an id of 0, and\nthe last row has id 404289\nqid1\nA unique identifier for the question in ques-\ntion1 column.\nqid2\nA unique identifier for the question in ques-\ntion2 column.\nquestion1\nquestion1 contains the actual question to be\ncompare d with question2\nquestion2\nquestion2 contains the actual question to be\ncompare d with question2\nis_duplicate\nis_duplicate is the result of a semantical com-\nparison of question pair. 0 indicates false i.e.\nquestion pair is not duplicate 1 indicates true\ni.e. question pair is duplicate\nsemantically duplicates and negative samples are non-duplicate\npairs.\nTable 3: Class Label distribution\nPositive Sample (1)\n149263\nNegative Sample (0)\n255027\nTotal Question Pairs\n404290\nIn the histogram plot Figure 1, the x-axis represents the number\nof times question occurs, and the y-axis or height of the bar\nrepresents how many such questions with occurrence count exist\nin the dataset. As can be visualized from the graph the majority\nof questions occurs less than 60 times, and the first bar shows the\nACM Conference, ,\nNavedanjum Ansari and Rajesh Sharma\nFigure 1: Distribution of question occurrence in dataset\nunique occurrence and second bar the number of the appearance\nof question twice and so on.\n3.4\nData Cleaning\nWe computed, additional stats on our dataset that helps us explore\nthe data and make decision in eliminating redundant data rows.\nTable 4: Statistics on question1 and question2\nStatistics\nAverage\nSum\nCount\nq1 length\n59.53672\n24070099\n404290\nq2 length\n60.10838\n24301217\n404290\nMax\nlength(char\ncount) q1\n623\nMax\nlength(char\ncount) q2\n1169\nq1 length - q2\nlength\n-0.57166\n-231118\n404290\nq1 length <=5\n-\n-\n53\nq2 length <=5\n-\n-\n19\nMostly these questions short length questions are one word, one\nand two length questions are just the question marks and special\ncharacters, foreign characters. We discard as these data rows in the\ndata cleaning process. In Table 4 we can see that the q2 length on\nan average is greater, and therefore, we have an average negative\ndifference. We dropped a total of 72 rows from our raw dataset\nbased on the logic that both question1 length and question2 less\nthan 6 or either one of the question length is less than six.\nThus, we have 404218 data rows in our machine learning experi-\nments, and we continue with the usual data with 404290 rows for\nour deep learning experiments.\n4\nBACKGROUND\nThis section briefly explains the features extracted from the raw\ndataset and various machine learning and deep learning neural\nlayers used in the experiments.\n4.1\nFeature Engineering\nWe dropped the first three columns id, qid1, and qid2 from the\ninitial raw dataset and created additional useful features so that we\nhave two columns question1, question2, and class label is_duplicate\nand 28 new derived features, Therefore initially, we have total of\nthirty-one columns in dataset provided as input to the machine\nlearning classifiers.\nSet 1 Original Feature\n1. Question 1 dataset: This is the question1, column in the\ndataset.\n2. Question 2 dataset: This is the question2, column in the\ndataset.\n3. Is duplicate: Class label represented as 1 for duplicates and 0\nfor non-duplicates.\nSet 2 Basic Features\n4. Length of question1: Length of the question1, includes all\nthe characters, punctuation and white spaces.\n5. Length of question 2: Length of the question2, includes all\nthe characters, punctuation and white spaces.\n6. Difference in the length of questions: Difference between\nthe length of corresponding question1 and question2.\n7. Number of characters in q1: Distinct number of characters\nexcluding white spaces in corresponding question1.\n8. Number of characters in q2: Distinct number of characters\nexcluding white spaces in corresponding question2.\n9. Number of words in q1: Number of words in question1\nincluding repeated words.\n10. Number of words in q2: Number of words in question2\nincluding repeated words.\n11. Number of common words in q1 and q2: Distinct com-\nmon words in corresponding question1 and question2.\nSet 3 Fuzzy Feature\n12. Qratio: Qratio feature is the quick ratio comparison of the\ntwo question strings and has value range from 0 to 100. More similar\nquestions have a higher score.\n13. Wratio: Wratio feature is the weighted ratio that uses\ndifferent algorithms to calculate the matching score and returns\nthe best ratio for two question strings. Score range from 0 to 100.\n14. Partial ratio: Partial ratio feature calculates the best score\nfor partial string matching against all sub strings of the greater\nlength and returns the best score. Score range from 0 to 100.\n15. Token set ratio: Token set ratio [33] feature is calculated\non the strings by segregating the strings into three parts. First part\nof common strings which are then arranged as sorted intersection,\nand other parts from each of the questions as sorted remainders. It\nthen computes scores from compares sorted intersection with each\nof combination of sorted intersection and sorted remainders of that\nstring. Score range from 0 to 100.\n16. Token sort ratio: Token sort feature tokenizes the strings\nand then sort the strings alphabetically and join back into strings. It\nthen compares the transformed strings using ratio to return score.\nScore range from 0 to 100.\n17. Partial token set ratio: Partial token set feature is similar\nto token set ratio except that after it tokenizes string it uses partial\nratio in place of ratio to calculate the matching score. Score range\nfrom 0 to 100.\n18. Partial token sort ratio: Partial token sort ratio is similar\nto token sort ratio except that it uses partial ratio in place of ratio,\nafter sorting the token to compute matching score. Score range\nfrom 0 to 100.\nIdentifying Semantically Duplicate Questions Using Data Science Approach: A Quora Case Study\nACM Conference, ,\nSet 4 Distance Features\n19. Word mover’s distance(wmd): Word mover’s distance [23]\nfeature calculates the distance between two documents, in our case,\nit gives the distance between two corresponding questions in our\ndataset. It uses word2vec embedding to find the distance between\nsimilar or semantically similar words. The stop words like ‘ the,’\n‘to’ etc. are removed using nltk [2] library.\n20. Normalized word mover’s distance (norm wmd ): Nor-\nmalized word mover’s is similar to word mover’s distance just that\nword2vec vectors are normalized, normalizing helps in reducing\nrisk of incorrect computation.\n21. Cosine distance: Cosine distance feature calculates the\nangle between the word vectors of two question sentences.\n22. Minkowski distance: Minkowski distance feature is a\ngeneric distance metric that can be computed as the summation of\ndifferences of vector dimensions raise to the power p and whole\nraise to the inverse of power p. We have used p=3 to calculate the\nMinkowski distance.\n23. Cityblock distance: Cityblock distance feature is a special\ncase of Minkowski distance metric when we use the value of p=1\nin the equation of Minkowski distance.\n24. Euclidean distance: Euclidean distance feature is also a\nspecial case of Minkowski distance metric when we use the value\nof p=2 in the equation of Minkowski distance.\n25. Jaccard distance: Jaccard distance [7] feature is computed\nas a ratio of intersection between two vectors sets to the union\nof two vector sets. The two vector sets are derived from the two\nquestion sentences in our dataset.\n26. Canberra distance : Canberra distance is computed as the\nsum of the absolute difference of two vector points divided by the\nabsolute sum of individual vector points.\n27. Braycurtis distance: Braycurtis distance [34] is also called\nas Sorenson distance. It is also a variant of Manhattan distance\nnormalized by the sum of the vector points in two objects x and y.\nSet 5 Vectors Features\n28. Skew question1 vector: Skewness is the measure of dis-\ntribution [24] . Skewness indicates a deviation tendency from the\nmean in one of the direction. Skewness is computed over question1\nvector. A normal distribution has a skew value equal to 0.\n29. Skew question2 vector: Skewness is computed over ques-\ntion 1 vector.\n30. Kurtosis question1 vector: Kurtosis distance is the mea-\nsure of dense distribution towards the tails of the distribution [24].\nA normal distribution has a value equal to 0. Kurtosis vector is\ncomputed over question1 vector.\n31. Kurtosis question2 vector: Kurtosis is computed over\nquestion2 vectors.\n4.2\nMachine Learning Models\nWe have selected the following seven machine learning classifiers\nand a statistical feature TF-IDF.\nK-Nearest neighbors: The k-nearest neighbors (KNN) [13]\nalgorithm is a simple, easy-to-implement supervised machine\nlearning algorithm that can be used to solve both classification\nand regression problems.\nDecision Tree: Decision tree [29] is the most powerful and\naccessible tool for classification and prediction.\nRandom forest: Decision trees are the building blocks of the\nrandom forest model. Random forest [16], like its name implies,\nconsists of a large number of individual decision trees that operate\nas an ensemble.\nExtra Trees: Extra tree [11] classifier is a type of ensemble learn-\ning technique which aggregates the results of multiple uncorrelated\ndecision trees collected in a “ forest ” to output its classification\nresult.\nAdaboost: AdaBoost [8] is a popular boosting technique which\nhelps you combine multiple “ weak classifiers ” into a single “ strong\nclassifier ”. A weak classifier is simply a classifier that performs\npoorly but performs better than random guessing.\nGradient Boosting Machine: Gradient boosting [9] is a ma-\nchine learning technique for regression and classification problems,\nwhich produces a prediction model in the form of an ensemble of\nweak prediction models, typically decision trees.\nXGBoost: XGBoost [3] is an implementation of gradient boosted\ndecision trees designed for speed and performance. XGBoost is a\ndecision-tree-based ensemble Machine Learning algorithm that\nuses a gradient boosting framework. XGBoost is short for extreme\ngradient boosting.\nTF-IDF : TF-IDF [28] stands for term frequency -inverse docu-\nment frequency, is a scoring measure widely used in information\nretrieval (IR). TF-IDF is intended to reflect how relevant a term is\nin a given document.\n4.3\nElements of Neural Network Layers\n1. LSTM [10]: Long short-term memory (LSTM) is an artificial\nrecurrent neural network (RNN) architecture used in the field of\ndeep learning. Unlike standard feed forward neural networks, LSTM\nhas feedback connections. It can process not only single data but\nalso entire sequences of data. LSTM networks are well-suited to\nclassifying, processing, and making predictions based on time series\ndata since there can be lags of unknown duration between essential\nevents in a time series.\n2. Word Embedding [23] : Word embeddings are a family of\nnatural language processing techniques aiming at mapping seman-\ntic meaning into a geometric space. This is done by associating a\nnumeric vector to every word in a dictionary, such that the dis-\ntance between any two vectors would capture part of the semantic\nrelationship between the two associated words.\n3. Glove Embedding [27] : GloVe is used for obtaining vector\nrepresentations for words. Training is performed on aggregated\nglobal word-word co-occurrence statistics from a corpus, and the\nresulting representations showcase interesting linear substructures\nof the word vector space.\n4. Time Distributed(Dense) : Time distributed dense layer is\nused on RNN, including LSTM, to keep one-to-one relations on\ninput and output. Assume we have 60 - time steps with 100 samples\nof data (60 x 100 in another word) and you want to use Recurrent\nNeural Network(RNN) with the output of 200. If we do not use time\ndistributed dense layer, we will get 100 x 60 x 200 tensors. So we\nhave the output flattened with each time step mixed.\nACM Conference, ,\nNavedanjum Ansari and Rajesh Sharma\n5. Lambda: Lambda layer is a layer that wraps an arbitrary\nexpression. For example, at a point, we want to calculate the square\nof a variable, but we can not only put the expression into our model\nbecause it only accepts layer, so we need Lambda function to make\nour expression be a valid layer in keras.\n6. Convolution 1D : A CNN works well for identifying simple\npatterns within our data that will then be used to form more\ncomplex patterns within higher layers. A 1D CNN is handy when\nwe expect to derive interesting features from shorter but mostly\nfixed-length segments of the overall data set and where the location\nof the feature within the segment is not of high relevance.\n7. GlobalMaxPooling 1D [12] : This block performs precise ly\nthe same operation as the 1D Max pooling block except that the\npool size is the size of the entire input of the block, i.e., it computes\na single max value for all the incoming data. The 1D Global max\npooling block takes a vector and computes the max value of all\nvalues for each of the input channels. The output is thus a tensor of\nsize is 1 x 1 x (input channels). Using 1D Global max pooling block\ncan replace the fully connected blocks of our CNN\n8. Merge [5] : Merge is used to join multiple neural networks\ntogether. A good example would be where we have two types of\ninput, for example, tags and an image To combine these networks\ninto one prediction and train them together, w e merge these Dense\nlayers before the final classification.\n9. Dense [18] : A dense layer is just a regular layer of neurons in\na neural network. Each neuron receives input from all the neurons\nin the previous layer, thus densely connected. The layer has a weight\nmatrix W, a bias vector b, and the activations of previous layer a.\n10. Batch Normalization [19] : Batch normalization is a tech-\nnique for improving the performance and stability of neural net-\nworks, and also makes more sophisticated deep learning architec-\ntures work in practice. The idea is to normalize the inputs of each\nlayer in such a way that they have a mean output activation of\nzero and standard deviation of one. This is comparable to how the\ninputs to networks are standardized. How does this help? We know\nthat normalizing the inputs to a network helps it learns. However,\na network is just a series of layers, where the output of one layer\nbecomes the input to the next. That means we can think of any\nlayer in a neural network as the first layer of a smaller subsequent\nnetwork. Thought of as a series of neural networks feeding into\neach other, we normalize the output of one layer before applying\nthe activation function, and then feed it into the following layer\n(sub-network).\n11. Dropout [15] : Dropout is a regularization technique, which\naims to reduce the complexity of the model to prevent overfitting.\nUsing “dropout,\" we randomly deactivate specific units (neurons) in\na layer with a certain probability p from a Bernoulli distribution. So,\nif we set half of the activations of a layer to zero, the neural network\nw ill no t be able to rely on particular activations in a given feed-\nforward pass during training. As a consequence, the neural network\nwill learn different, redundant representations; the network can no\nt rely on the particular neurons and the combination (or interaction)\nof these to be present. Another good side effect is that the training\nwill be faster. Dropout is a technique used to tackle overfitting.\n12. PreLU [14] : Parametric Rectified Linear Unit(PreLU), Para-\nmetric ReLU is inspired by ReLU, which, as mentioned before, has\na negligible impact on accuracy compared to ReLU. Based on the\nsame ideas that of ReLU, PreLU has the same goals: increase the\nlearning speed by not deactivating some neurons. The primary\nargument for Parametric ReLu’s over standard ReLu ’s is that they\ndo not saturate as we approach the ramp. In most other ways, they\ndo not offer a distinct advantage. Think of it as an advantage in\nbeing able to tell the difference between a wrong answer and a\nhorrible answer. The effect may not seem dramatic, but in some\ninstances, it can be genuinely advantageous.\n13. Activation [22] : Applies an activation function to the\noutput of a layer such as tanh, sigmoid activation. It takes into\nconsideration the effects of different parameter interaction and\napplies the transformation where it filters the value from which\nneuron to be passed to the next layer or the output.\n5\nMETHODOLOGY\nIn this section, a general approach to training our machine learning\nclassifiers, the process flow for feature importance analysis, the\nprocess of TF-IDF with ML classifiers and four different deep\nlearning architectures that we modeled for our experiments are\npresented.\n5.1\nExperimental and Research Design\nInfluenced by the literature and the previous study, we started our\nexperiments with the binary classification of whether a given pair\nof question is a semantically duplicate question. We began with\nfeature engineering to produce as many as 28 new features from the\ngiven question pair dataset and apply different machine learning\nclassifiers.\n5.2\nFeature Importance\nWe analyzed and studied the features extracted using feature\nengineering to validate the positive contributions from each of\nthe features, and then we retrain our models by dropping the least\nimportant features. We have a total of 28 new features extracted in\nthe experiment stage of section 4.1. We analyze and select the top\ntwenty features that are helpful to our machine learning classifiers,\nand then dropped eight features.\n5.3\nMachine Learning Pipeline with TF-IDF\nTF-IDF character level\nThe flow of TF-IDF character level feature with machine learning\nclassifiers are presented inFigure 2. TF-IDF character level as the\nname suggests computes TF-IDF at character level in the document,\nin our case, it is a question.\nThe model learns the inverse frequency of characters from the\nset of combined unique question1 and question2 character set. The\ncorresponding TF-IDF, character level feature, obtained for each\nof the questions in the pair is then passed as input to the different\nmachine learning classifiers. The classifiers are then trained on the\ntraining dataset, which is 80% of total dataset and tested on 20% of\nthe validation set. We also experimented with word level TF-IDF in\na similar way as character level.\n5.4\nDeep Neural Network Design\nArchitecture-1 : In this simple neural network architecture, we use\na pair of questions as the two inputs. The architecture consists of\nIdentifying Semantically Duplicate Questions Using Data Science Approach: A Quora Case Study\nACM Conference, ,\nFigure 2: The flow of TFIDF character level feature as an\ninput to machine learning classifiers\nthe Embedding layer, LSTM layer applied separately on each of the\nquestion inputs, and then the model is merged using the Merge layer\nfrom keras library [4]. The output from the merged model layer\nis then passed through the series of Batch Normalization, Dense,\nParametric rectified linear unit, Dropout and Sigmoid Activation\nfunction is applied at the final output layer. Embedding layers is\nthe first hidden layer of a network that uses word embedding to\nrepresent a word as a dense vector, and we specify three arguments\nto the Embedding function, the input dimension, output dimension,\nand the input length. We use the input length, i.e. number of words\nas 40 and output dimension as 300. Input dimension is computed\nas the index of words + 1 in the sequence.\nIn this model, we are not using any special pre-trained vectors\nlike GloVe. The output of Embedding layer is fed to the LSTM layer.\nWe used the dropout weight of 0.2 within LSTM to avoid overfitting.\nEach of the models merged as passed through a sequence of layers,\nas shown inFigure 3 The output from the intermediate Dense layer\nis 300, and the final Dense layer always has output dimension one,\nwhich then fed to sigmoid activation to give us the classification\nresult.\nArchitecture-2: Neural network architecture-2 is modeled\nslightly different before applying to merge of different models oth-\nerwise after merge it very similar and trained on exactly same\nhyper-parameters as simple neural network presented as in Fig-\nure 3. In Architecture-2, we increase the number of independent\nmodels before merge to four, which are then merged and trained to\nproduce the classification result. Architecture-2 with four inputs,\ntwo different networks are used for each of the questions as can be\nseen in Figure 4\nAdditional models before the merge, consist of Embedding layer\nusing GloVe pre-trained vector of 300 dimension s with 840B tokens.\nEmbeddings are then fed to Time distributed dense layer to maintain\nFigure 3: Architecture-1 Simple Neural network architec-\nture with two inputs\nFigure 4: Architecture-2 Deep neural network architecture\nwith four inputs\nACM Conference, ,\nNavedanjum Ansari and Rajesh Sharma\none to one relationship over time- distribution. Lambda sum is\napplied along the axis to produce the output of 300 dimensions.\nThus, all the four independent models producing the output of\n300d are then merged and passed through hidden layers of Batch\nNormalization, Dense, PreLu, Dropout, Batch Normalization, Dense\nand Sigmoid Activation to produce the classification result.\nArchitecture-3 : Architecture-3 uses four sub-model or inde-\npendent model from Architecture-2 with all the hyper-parameters\ntuned with the exact same value; the model differs after the merge\nof the four independent models. The modeled neural network\narchitecture-3 can be visualized, as presented in Figure 5\nFigure 5: Architecture-3 Deep neural network with four in-\nputs and dense hidden layers\nArchitecture 4 : The deep neural network architecture-4 is\nmodeled in such a way that it takes the six input which are then\npassed through six independent models and then merged into a\nsingle model consisting of twenty-three layers.\nThe deep neural network architecture-4 is modeled in such a\nway that it takes the six input which are then passed through six\nindependent models and then merged into a single model consisting\nof twenty-three layers.\nFour out of six independent or sub-models are similar to that\nof the four sub- models before the merge as presented in Figure\n12. The two new sub-models that we added consist of GloVe\nFigure 6: Architecture-4 Deep neural network with six in-\nputs and dense hidden layers\nbased Embedding layer, Convolution Neural Network layer applied\nmultiple times before and after Dropout layer. The output from the\nConvolution 1D layer is maxed out using Global Max Pooling 1D\nlayer. Global Max Pooling output is then passed through hidden\nlayers of Batch Normalization, Dense and Dropout. The Dropout\nlayer has shown to perform well within our experiments with a\nweight of 0.2; therefore, throughout our neural network modeling\n; dropout weigh used is 0.2. All six layers produce the output\nof dimension 300 which is then merged as a single model and\npassed through another twenty-six layer consisting of repeated\nunits of Dense, Dropout and Batch Normalization and finally a\nDense layer with the output of dimension size one which is fed\nto sigmoid Activation to predict the classification result. We have\nused tensorFlow keras [4] python library to model each of the\nneural network architecture presented in this section. All models\nare trained on the batch size of 300 and number of epoch iterations\nas 150.\n6\nDESCRIPTION OF MODELS AND RESULTS\nEVALUATION\nThis section discusses evaluation metrics and comparative analysis\nof the results.\nIdentifying Semantically Duplicate Questions Using Data Science Approach: A Quora Case Study\nACM Conference, ,\n6.1\nEvaluation Metrics\nThe selection of metrics is the most crucial step in the evaluation\nof our models as it influences how we measure the performance of\nour model against each other and the baselines.\nAccuracy: Accuracy is the ratio of the total number of correct\npredictions made by the models to the total number of predictions\nrequested to the model.\nF1-Score: F1-score or F1-measure is harmonic mean of precision\nand recall. To understand F1-Score, we need to understand Precision,\nalso known as Specificity and Recall, also known as Sensitivity.\nPrecision: Precision or Specificity is the ratio of predicted\npositive samples that are actually positive to the total number of\npositive predictions made by the models.\nRecall: Recall or Sensitivity is the ratio of predicted positive\nsamples that are actually positive to the total number of actual\npositive predictions in total sample.\nLog loss: Log loss is also known as cross-entropy, and when the\nclassification type is of binary as in our research, then it is known\nas binary cross-entropy. Log Loss value lies in the range of {0,1}\nwhere ideal models will have log loss of 0, and the worst model will\nhave log loss of 1. Log loss indicates how badly our model predicted\nthe probability of our classification.\n6.2\nBaseline Model Classifiers\nWe trained our model and then evaluated the prediction on our test\ndata set to achieve the baseline for our machine learning algorithms\nused in this research. Table 5 shows test accuracy and F1 score of\nour baseline machine learning models.\nTable 5: The baseline performance of traditional machine\nlearning classifiers on the dataset with 30 features predicted\non test dataset\nClassifiers\nAcc\nF1-Score\nK Nearest Neighbors\n0.7275\n0.7031\nAdaBoost\n0.7041\n0.6936\nXGBoost\n0.7417\n0.7326\nGradient Boost\n0.7271\n0.7176\nDecision Tree\n0.7054\n0.6992\nRandom Forest\n0.7099\n0.7016\nExtraTrees\n0.7039\n0.6849\nAs can be observed from Table 5, clearly the Xgboost model\noutperforms all the other selected classifiers with the Accuracy of\n0.7416 and F1 score of 0.7326.\n6.3\nFeature Importance Analysis\nWe analyzed the feature importance value of all seven machine\nlearning classifiers used in the experiments and executed the\nexperiments. Based on our feature importance values, we selected\nthe top 20 features out of 28 derived features . The performance\nresult achieved after feature importance analysis and feature drop\nis as presented in Table 6\nXgboost, Gbm and KNN after feature drop still stood to be the\ntop three performers in our baseline model set, and none of the\nclassifiers suffers from any degradation. However, the gain achieved\nTable 6: Performance of traditional machine learning classi-\nfiers after feature drop on test dataset\nClassifiers\nAcc\nF1-Score\nK Nearest Neighbors\n0.7311\n0.7076\nAdaBoost\n0.7048\n0.6938\nXGBoost\n0.7431\n0.7349\nGradient Boost\n0.7289\n0.7196\nDecision Tree\n0.7054\n0.6992\nRandom Forest\n0.7085\n0.7021\nExtraTrees\n0.7069\n0.6914\nFigure 7: Accuracy comparison of ML classifiers Before ver-\nsus After feature drop\nafter feature drop is minimal. Figures 7 and 8 show the comparative\nvisualization of Accuracy and F1 score before and after the feature\ndrop. The eight dropped features are difference in the length, WRatio,\njaccard distance, braycurtis distance, Euclidean distance, cityblock\ndistance, partial token set ratio, partial token sort ratio.\n6.4\nTF-IDF with ML Models\nXgboost algorithm achieved an F1 score of 80.44 % compared to F1\nscore 79% published in [30] The accuracy achieved is 82.44%, which\nis very close to that of 83.7% achieved by the same literature. Thus,\nour result s show that ML models like Xgboost can also produce\neffective results similar to the Deep learning algorithms like LSTM.\nPresented in Table 8, training and test accuracy and log loss\nmetrics obtained from the deep neural network architectures\npresented in Figures 3, 4, 5 and 6.\nSince we modeled and experimented with applied deep learn-\ning techniques using tensorflow keras library which offers only\naccuracy as the metrics at the end of each epoch and finding addi-\ntional metrics like F1 score require us to run additional tests on test\ndataset and, calculate other metrics from prediction results.\nACM Conference, ,\nNavedanjum Ansari and Rajesh Sharma\nFigure 8: F1 score comparison of ML classifiers Before ver-\nsus After feature drop\nTable 7: Performance of ML classifiers with TF-IDF word and\nTF-IDF character level on test dataset\nWord TF-IDF\nChar TF-IDF\nClassifiers\nAcc\nF1-\nScore\nAcc\nF1-\nScore\nK Nearest Neighbors\n0.7513\n0.7359\n0.7845\n0.7543\nAdaBoost\n0.6883\n0.6076\n0.6871\n0.6201\nXGBoost\n0.7881\n0.7596\n0.8244\n0.8044\nGradient Boost\n0.6756\n0.5339\n0.6951\n0.6009\nDecision Tree\n0.6677\n0.5651\n0.6672\n0.5767\nRandom Forest\n0.6284\n0.3866\n0.6484\n0.4066\nExtraTrees\n0.6281\n0.3864\n0.6581\n0.4059\nTable 8: Accuracy and Log loss performance of deep neural\nnetwork architectures evaluated on 20% of test dataset\nNetwork\nTrain\nLoss\nTrain\nAcc\nTest\nLoss\nTest Acc\nArchitecture-\n1\n0.2902\n0.8715\n0.4062\n0.8133\nArchitecture-\n2\n0.2502\n0.9012\n0.4172\n0.8312\nArchitecture-\n3\n0.1728\n0.9127\n0.4393\n0.8522\nArchitecture-\n4\n0.0997\n0.9674\n0.38501\n0.8582\n7\nCONCLUSIONS AND FUTURE WORK\nWe ensure that, the train and test data is split into 80/20 respectively\nthroughout the experiments. We also ensure that the class labels\nin the test data set has proportionate distribution of samples as in\nour original dataset. All the hyper-parameters are selected based\non grid search performed on the 10% of dataset from the training\nset, thus we ensure that our result do not suffer from overfitting.\nOur results with TF-IDF and ML classifiers show that not\nall models performed well in ensemble with TF-IDF character\nlevel, but our best model Xgboost achieved the accuracy of 82.44\n% and F1 score of 0.8044. This has demonstrated that machine\nlearning models are efficient in solving natural language problem\nof detecting semantically similar question and compared to other\nbaseline achieved from few of the deep learning methods such\nas LSTM and LSTM with Siamese listed in Table 1, our machine\nlearning TF-IDF with Xgboost outperformed them.\nFinally, we experimented with many different deep network lay-\ners and chose the four architecture to present which outperformed\nthe results obtained by literature [6], our best performance from\narchitecture-4 has achieved accuracy of 85.82 %. We used log loss\nmeasures for our neural networks along with accuracy. We reached\nthe best training accuracy of 96.74% and log loss of 0.09 ; however,\nin our work, the test accuracy and test loss is our main focus. We\nachieved a better result from the previous study on the duplicate\nquestion pair dataset. Our best performance from this research\nwork is the accuracy of 85.82% and log loss of 0.385.\nOur accuracy result is very near to the Quora state of the art [20]\naccuracy of 87%. The main reason for difference in results exist\nbecause Quora has used their own word embedding’s from the\nQuora corpus dataset which is very specific to the Quora’s question\nformat, etc. whereas we have used the GloVe general embedding\n; thus our results are methods are more relevant to any general\nquestion and answering system.\nAnother way, Quora could achieve a better by pre-processing the\noriginal question pair dataset. Since knowing the context in which\nquestion is asked, a proper replacement of some of the pronouns\ncan be done, and higher accuracy can be achieved. For example,\npronoun like us, we, they can be replaced if the topic under which\nquestion exists thus replacing it with their relative context like\n“American,” “ Programmers ” and “ Prisoners ’ etc. during the pre-\nprocessing data stage can help achieve a better result. As we are\nunaware in which context questions were asked we could not do\nsuch pre-processing on the original dataset.\nThe limitations expressed in the paragraph above, if known in\nany context in case of any other social Media platforms or Quora\ncan be used as the future development of this research. As also we\nworked on standard Intel core seven laptop, 32 GB RAM, without\nadditional GPU capacity it took over 700 hours approx. to train all\nour four deep learning models and also the TF-IDF+Xgboost model\ntraining process took close to 7 hours. With better GPU capacity,\nwe assume to achieve a slightly better result, and the experiment\ncould have been performed with constructing more deep learning\nmodels and hyper parameter tuning.\nREFERENCES\n[1] Travis Addair. 2017. Duplicate question pair detection with deep learning. Stanf.\nUniv. J (2017).\nIdentifying Semantically Duplicate Questions Using Data Science Approach: A Quora Case Study\nACM Conference, ,\n[2] Steven Bird, Ewan Klein, and Edward Loper. [n.d.]. Natural language processing\nwith Python: analyzing text with the natural language toolkit.\n[3] T Chen and C Guestrin. 2016. XGBoost : Reliable Large-scale Tree Boosting\nSystem. arXiv.\n[4] François Chollet et al. 2015. Keras.\n[5] Y M Chou, Y M Chan, J H Lee, C Y Chiu, and C S Chen. 2018. Unifying and\nmerging well-trained deep neural networks for inference stage. IJCAI Int. Jt.\nConf. Artif. Intell (2018), 2049–2056.\n[6] E Dadashov, S Sakshuwong, and K Yu. 2017. Quora Question Duplication. ,\n9 pages.\n[7] Raihana Ferdous et al. 2009. An efficient k-means algorithm integrated with\nJaccard distance measure for document clustering. In 2009 First Asian Himalayas\nInternational Conference on Internet. 1–6.\n[8] Y Freund and R E Schapire. 1996. Experiments with a New Boosting Algorithm.\nProc. 13th Int. Conf. Mach. Learn (1996).\n[9] J H Friedman. 1999. Greedy Function Approximation : A Gradient Boosting\nMachine 1 Function estimation 2 Numerical optimization in function space.\n[10] F Gers. 2001. Long short-term memory in recurrent neural networks. Neural\nComput (2001).\n[11] P Geurts, D Ernst, and L Wehenkel. 2006. Extremely randomized trees. Mach.\nLearn 63, 1 (2006), 3–42.\n[12] I J Goodfellow, D Warde-Farley, M Mirza, A Courville, and Y Bengio. 2013. Maxout\nNetworks.\n[13] G Guo, H Wang, D Bell, Y Bi, and K Greer. 2010. KNN Model-Based Approach in\nClassification.\n[14] K He, X Zhang, S Ren, and J Sun. 2015. Delving deep into rectifiers. Proc. IEEE\nInt. Conf. Comput. Vis (2015).\n[15] G Hinton. 2014. Dropout : A Simple Way to Prevent Neural Networks from\nOverfitting. , 1929-1958 pages.\n[16] T K Ho. 1995. Random decision forests. Proceedings of the International Conference\non Document Analysis and Recognition, ICDAR (1995).\n[17] Y Homma, S Sy, and C Yeh. 2016. Detecting Duplicate Questions with Deep\nLearning. 30th Conf. Neural Inf. Process. Syst. (NIPS 2016), no. Nips (2016), 1–8.\n[18] G Huang, Z Liu, L Van Der Maaten, and K Q Weinberger. 2017. Densely connected\nconvolutional networks. Proceedings - 30th IEEE Conference on Computer Vision\nand Pattern Recognition (2017).\n[19] S Ioffe and C Szegedy. 2015. Batch Normalization: Accelerating Deep Network\nTraining by Reducing Internal Covariate Shift.\n[20] N Jiang, Lili, Chang, and Dandekar Shuo. 2019. , 4-4 pages.\n[21] J O Josephsen. 1956. Similarity Measures for Text Document Clustering. Nord.\nMed 56, 37 (1956), 1335–1339.\n[22] B Karlik and A Vehbi. 2011. Performance Analysis of Various Activation Functions\nin Generalized MLP Architectures of Neural Networks. Int. J. Artif. Intell. Expert\nSyst 1 (2011), 111–122.\n[23] M J Kusner, Y Sun, I K Nicholas, and Q W Kilian. 2015. From Word Embeddings\nTo Document Distances Matt, Vol. 63130. St. Louis, 1 Brookings Dr., St. Louis,\nMO.\n[24] Kanti V Mardia. 1970. Measures of multivariate skewness and kurtosis with\napplications. Biometrika 57, 3 (1970), 519–530.\n[25] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. GoogleNews-\nvectors-negative300.bin.gz - Efficient estimation of word representations in vector\nspace. arXiv preprint arXiv:1301.3781 (2013).\n[26] A Parikh, O Tckstrm, D Das, and J Uszkoreit. 2016. A Decomposable Attention\nModel for Natural Language Inference. Proc. 2016 Conf. Empir. Methods Nat. Lang.\nProcess (2016), 2249–2255.\n[27] J Pennington, R Socher, and C Manning. 2014. Glove: Global Vectors for Word\nRepresentation. Proc. 2014 Conf. Empir. Methods Nat. Lang. Process (2014), 1532–\n1543.\n[28] S Robertson. 2004. Understanding inverse document frequency: On theoretical\narguments for IDF. J. Doc (2004).\n[29] Philip H Swain and Hans Hauska. 1977. The decision tree classifier: Design and\npotential. IEEE Transactions on Geoscience Electronics 15, 3 (1977), 142–147.\n[30] A Tung and E Xu. 2017. Determining Entailment of Questions in the Quora\nDataset. , 8 pages.\n[31] S Viswanathan, N Damodaran, and A Simon. 2019. Advances in Big Data and\nCloud Computing, Vol. 750. Springer.\n[32] G Wang, K Gill, M Mohanlal, H Zheng, and B Y Zhao. 2013. Wisdom in the social\ncrowd: An analysis of Quora. WWW 2013 - Proc. 22nd Int. Conf. World Wide Web\n(2013), 1341–1351.\n[33] Jiannan Wang, Guoliang Li, and Jianhua Fe. 2011.\nFast-join: An efficient\nmethod for fuzzy token matching based string similarity join. In 2011 IEEE 27th\nInternational Conference on Data Engineering. 458–469.\n[34] David I Warton, Stephen T Wright, and Yi Wang. 2012. Distance-based multi-\nvariate analyses confound location and dispersion effects. Methods in Ecology\nand Evolution 3, 1 (2012), 89–101.\n",
  "categories": [
    "cs.IR",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-04-18",
  "updated": "2020-04-18"
}