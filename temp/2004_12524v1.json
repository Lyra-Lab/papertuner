{
  "id": "http://arxiv.org/abs/2004.12524v1",
  "title": "Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data",
  "authors": [
    "Benjamin Shickel",
    "Parisa Rashidi"
  ],
  "abstract": "Deep learning continues to revolutionize an ever-growing number of critical\napplication areas including healthcare, transportation, finance, and basic\nsciences. Despite their increased predictive power, model transparency and\nhuman explainability remain a significant challenge due to the \"black box\"\nnature of modern deep learning models. In many cases the desired balance\nbetween interpretability and performance is predominately task specific.\nHuman-centric domains such as healthcare necessitate a renewed focus on\nunderstanding how and why these frameworks are arriving at critical and\npotentially life-or-death decisions. Given the quantity of research and\nempirical successes of deep learning for computer vision, most of the existing\ninterpretability research has focused on image processing techniques.\nComparatively, less attention has been paid to interpreting deep learning\nframeworks using sequential data. Given recent deep learning advancements in\nhighly sequential domains such as natural language processing and physiological\nsignal processing, the need for deep sequential explanations is at an all-time\nhigh. In this paper, we review current techniques for interpreting deep\nlearning techniques involving sequential data, identify similarities to\nnon-sequential methods, and discuss current limitations and future avenues of\nsequential interpretability research.",
  "text": "Sequential Interpretability: Methods, Applications, and Future \nDirection for Understanding Deep Learning Models in the \nContext of Sequential Data \n \n \nBenjamin Shickel1,3, Parisa Rashidi2,3 \n \n \n1 Department of Computer and Information Science and Engineering \n2 Department of Biomedical Engineering \n3 University of Florida, Gainesville, Florida, United States \n \n \n \nABSTRACT \n \nDeep learning continues to revolutionize an ever-growing number of critical application \nareas including healthcare, transportation, finance, and basic sciences. Despite their \nincreased predictive power, model transparency and human explainability remain a \nsignificant challenge due to the “black box” nature of modern deep learning models. In \nmany cases the desired balance between interpretability and performance is \npredominately task specific. Human-centric domains such as healthcare necessitate a \nrenewed focus on understanding how and why these frameworks are arriving at critical \nand potentially life-or-death decisions. Given the quantity of research and empirical \nsuccesses of deep learning for computer vision, most of the existing interpretability \nresearch has focused on image processing techniques. Comparatively, less attention has \nbeen paid to interpreting deep learning frameworks using sequential data. Given recent \ndeep learning advancements in highly sequential domains such as natural language \nprocessing and physiological signal processing, the need for deep sequential \nexplanations is at an all-time high. In this paper, we review current techniques for \ninterpreting deep learning techniques involving sequential data, identify similarities to \nnon-sequential methods, and discuss current limitations and future avenues of sequential \ninterpretability research. \n \n \nI. INTRODUCTION \n \nThe past decade has seen an explosion in the amount of machine learning research \nemploying deep learning techniques. Built on the foundation of learning task-specific, \nnonlinear, and increasingly abstract feature representations directly from raw data1, these \nmodern techniques have proven highly effective in a variety of domains, particularly those \ninvolving data that exhibits inherent local structure, such as spatial pixel relationships in \ncomputer vision2,3 and sequential character-level or word-level associations in natural \nlanguage processing4,5. Deep learning techniques have also been successfully applied in \ndomains such as healthcare6,7, finance8, genomics9, drug discovery10, and speech \nrecognition11.  \n \nWhile the results from the deep learning revolution speak for themselves, a common \nlimitation underpinning all deep learning methods in their purest form is the inherent \ndifficulty or inability to precisely determine why a model arrives at a particular output or \nprediction. Along with fundamental properties of deep learning algorithms, this essence \nof explainability can be viewed as part of a two-sided coin: the hierarchical nonlinearities \npresent in deep learning methods that hinder natural explanation of its processes are \nprecisely why these models are so effective at developing high-dimensional and latent \nrepresentations of raw data that yield superior results over more human-understandable \nmethods. Often referred to broadly as interpretability, transparent explanatory processes \nfor a model’s inner workings are missing from fundamental deep learning algorithms and \nhave led to their reputation as “black box” methodologies. \n \nAlthough interpretability is widely viewed as a global limitation and a necessary element \nof future deep learning research, at current time there is still little consensus on a precise, \nformal definition of interpretability12–14. In a unique effort to ground the discussion, Lipton12 \noutlined both objectives and properties of interpretable models, in which associated \nmethods were categorized as either improving transparency, i.e. shedding light on a \nmodel’s inner workings, or providing post-hoc interpretations, where explainable \ninformation is gleaned from already-trained models. In this paper, we build on Lipton’s \ngroundwork and utilize this taxonomy for discussing modern approaches for deep \nlearning interpretability.  \n \nGiven the widely publicized and overwhelmingly successful results of deep learning \napproaches for computer vision tasks, it comes as no surprise that the majority of \ninterpretability research has also focused on the prevalent domain of image processing. \nSeveral works have reviewed interpretable methods for computer vision that capitalize on \nthe spatial aspect of pixel-based image data15–17. Contrarily, in this paper we focus on \nmethods for explaining and understanding sequential deep learning models that use \nsequential data inputs such as text, time series, and other data types that imply a notion \nof order or time. While some technical methodologies for processing sequential data \noverlap with those used for images, there is a distinct difference in application and \ninterpretation, and in this paper, we highlight unique aspects of sequential understanding \nas it relates to deep learning frameworks. \n \nSequential data can be viewed as an ordered collection of samples S = {S1, S2, S3, …, \nSN}, where N refers to the total number of instances, and each item Si = {xi1, xi2, …, xiD} \nrefers to a single sequential instance comprised of D features, where D can be one or \nmany values per time step. For real-valued time series such as stock prices or clinical \nvital signs, the local structure imposed within each sample is based on time. Alternatively, \nfor textual data types the structure is based on the ordering of distinct characters or words \nthat naturally carry meaning based on our understanding of language. \n \n \nFigure 1. Brief overview of sequential data types used by deep learning studies included in this review.  \n \nFigure 2. Histograms of techniques, data types, architectures, and publication count by year for deep sequential \ninterpretability studies included in this review. \n \nIn the simplest case, sequential data is comprised of a list of single elements with \ncorresponding time stamps. However, sequential data can also be multivariate; that is, it \ncan be comprised of multiple values at each time step. \n \nIn this review, we provide an overview of published research involving the interpretability \nof deep learning methods in the context of processing sequential data types. Along with \ndescribing the methodology and contribution of each technique, we place all studies in \ncontext by deriving a goal-oriented taxonomy and categorization of techniques based on \nthe nature of the interpretable information provided to practitioners. We conclude with a \ndiscussion of current trends and similarities, limitations, and future direction of deep \nsequential interpretability research. \n \n \nII. METHODS FOR DEEP SEQUENTIAL INTERPRETABILITY \n \nThere is a lack of consensus on a clear definition of interpretability, not solely for \nsequential data and applications, but across the entire field of deep learning. In fact, the \nnotions of transparency, explainability, and the broad concept of interpretability can refer \nto several different aspects of algorithmic modeling. The desired outcome is often a \nhuman-understandable insight into the input-output data relationship, obtained from an \nopaque but effective sequential deep learning model. The specific types of interpretability \ntechniques and information gleaned from them are widely varied and task, model, and  \n \nFigure 3. Interpretability models can potentially identify important input features. \n \ndata dependent. For example, a number of techniques identify important input features \n(Figure 2).   \n \nWe guide our overview of deep learning interpretability techniques for sequential data \nbased on a categorization of their human-understandable interpretations. Based on a \nreview of deep learning interpretability research over the past decade, we have identified \nthree primary trends in the application of interpretable techniques to sequential modeling: \n(1) network analysis information, (2) sequential saliency, and (3) multivariate attribution \ninformation (Figure 2). For techniques that fall under multiple categories, we include them \nseparately and discuss technical aspects relevant to the category of interest. \n \nA. Network Analysis \nNetwork analysis techniques shed light on the precise mechanisms and data \ntransformations that occur within a deep sequential neural network. These techniques \nexamine individual neurons, layers, or regions of a trained deep learning model, and \ncharacterize the types of inputs that each have learned to identify. We also include class \nprototypes in this category, where synthetic inputs are generated that are most \ncharacteristic of a given class in a supervised learning setting – which can be viewed as \nthe analysis of the final layer of a trained deep sequential model. All methods in this \nsection operate on a trained deep model, and do not require modification of the underlying \narchitecture. \n \n \nFigure 4. Relationships between specific sequential interpretability techniques and target of increased understanding. \n \ni. Weight inspection \nPerhaps the simplest technique to gain insight into a fully trained deep learning model is \nby directly examining its learned weights. These approaches typically constrain analysis \nto the weights of the first layer of a deep network where there is a direct interaction on the \nraw data inputs. We can inspect the weights in a regular fully connected network (Fig.1.a), \nin filters of a convolutional neural network (CNN, Fig.1.b), or in weights of a word \nembedding network (Fig.1.c).   \n \nThis approach is common as a baseline step in image processing applications where \nweights of a trained CNN can be visualized and interpreted as edge, line, or shape  \n \nFigure 5. High-level operational overview of sequential interpretability methods in a supervised learning setting. \n \ndetectors21. Karpathy et al.19 extended this technique to analyzing sequences of image \nframes in their multiresolution CNNs for video classification, where they found qualitative \ndifferences between the types of colors and frequencies included in filter responses. \n \nIn the context of univariate time series, Lasko et al.22 visualized the weights of a trained \nautoencoder for classifying uric acid sequences as exhibiting either gout or leukemia, \nwhere they found the first layer weights detected functional elements such as uphill and \ndownhill ramps and other repeated motifs. \n \nMehrabi et al.18 implemented a Deep Boltzmann Machine for analyzing sequences of two \nsets of medical codes, International Classification of Diseases (ICD-9) and Healthcare \nCost and Utilization Project (HCUP), to identify common sequential patterns among \npatients with similar diagnoses. In a visualization heatmap of the first layer’s hidden \nweights, they found commonalities between codes and identified trends relating to chronic \ndisease. \n \nLi et al.20 visualized word and phrase embedding representations as both heatmaps and \nvia t-SNE, finding particular dimensions of the learned representations corresponded to \nvarious syntactic and semantic aspects, and found qualitative clustering of locally \ncompositional phrase types such as negation. \n \n \n \nFigure 6. As a baseline Interpretability Technique (IT), weight inspection methods inspect the weights, e.g. this can be \ndone by visualizing (a) the weights in a fully connected network , (b) the filters in a CNN, or as (c) the  embedding vector \nof a word embedding network  Example Applications of each : (d) Medical  code sequence: weight visualization of the \nfirst hidden layer with [ICD9 x Diagnosis year] matrices as input18, (e) sequence of video frames: filter visualization of \nvideo streams 19, (f) sequence of words: embedding visualization of sentiment in natural language20. \n \nii. Maximum activation \nSimilar to visualizing fixed network weights of a trained model, the network activations \nwithin a deep model can also be examined. Unlike weight analysis, which is usually \nrestricted to the first layer, activations at any stage of a deep network can be visualized \nalong with their driving inputs. For each unit of a network, particular inputs can be \ndetermined – whether present in real data or generated – that maximally activate that \nunit23, providing qualitative understanding of the types of inputs each portion of a network \nis looking for. It should be noted that individual neuron activation might not be very \nmeaningful. As previously shown, it is a subset of neurons, rather than the individual units, \nthat contain the semantic information in the high layers of neural networks.24 \n \nHermans and Schrauwen25 and Karpathy et al.26 both take this approach for character-\nlevel recurrent neural networks, where cell activations were aligned with the inputs with \nhighest activations to discover interpretable textual patterns such as quoted text and \nlonger-range character interactions. \n \nDong et al.27 analyzed driving styles using spatiosequential GPS time series using \nrecurrent neural networks and visualized the activations of select neurons with maximum  \n \n \nFigure 7. For each unit of a network, particular inputs can be found, whether (a) present in real data or (b) generated. Examples: \n(c) Activation of an LSTM cell as processing the sequence of characters in a text passage, (d) The influence values of a time \nseries. \n \nactivations, showing that particular neurons react to patterns such as angular trends, \nspeed changes, and potential GPS failures. \n \nChe et al.28 and Kale et al.29 utilized autoencoders and fully-connected networks along \nwith prior-based regularization and causal associations for predicting disease from \nmultivariate time series, in which physiological patterns from inputs that maximally \nactivated hidden units were visualized to discover sequential phenotypes for individual \ndiseases. \n \nIn the aforementioned studies, interpretations were based on the real dataset inputs that \nresulted in a unit’s largest activation. However, gradient-based techniques can also be \nused to generate a synthetic input that maximally activates a given neuron23. This \napproach was taken by Lanchantin et al.30,31, who applied convolutional neural networks \nfor extracting motifs from genomic sequences. Once their model was trained, they \noptimized each class output with respect to an input sequence, and through \nbackpropagation were able to generate class-specific motifs for understanding how their \nmodels were predicting transcription factor binding sites. Siddiqui et al.32 take a similar \napproach in the context of univariate and multivariate time series. \n \n \nB. Sequential Saliency \nThe following techniques are designed to visualize the timesteps or sub-patterns of an \ninput sequence that most contributed to the overall sequential representation used for \nfinal model prediction in a supervised learning environment. Application of these methods \ncan be used to understand why a model has made a particular prediction by pointing back \nto specific elements of the input sequence. In contrast to the previous section, the \nfollowing methods are focused more on characterizing the relationship between input time \nsteps and task labels (“why was the prediction made?”), and are less concerned with \ndetermining the internal mechanisms by which this happens (“how was the prediction \nmade?”). \n \ni. Class activation maps \nWang et al.33 utilized fully convolutional networks for a variety of univariate and \nmultivariate time series classification tasks. They used a class activation map to assign a \nsaliency score to each element of the input time series, where the activations of a single \ntime step are summed over all filters in the penultimate convolutional layer. A class \nactivation map can be computed for each available class as shown below: \n \n𝐶𝑙𝑎𝑠𝑠 𝐴𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛 𝑀𝑎𝑝= 0 𝑤!\n\"𝐴!\n!\n \n \nBy superimposing saliency scores along with the original sequence, they were able to \nhighlight time series segments that were most aligned with each available classification \ntarget. \n \nii. Gradient-based saliency \nIn addition to previously mentioned maximum activation interpretations based on \noptimizing an input sequence with respect to a given class prediction, Siddiqui et al.32 \nalso visualized the importance of each of a series of convolutional filters by computing \nthe gradient of each filter’s output with respect to the input sequence. Similarly, \nLanchantin et al.30,31 compute the gradient of the output layer with respect to the input \nlayer to determine the relative importance of each element of genomic sequences. \n \nIn general, we can compute a score function by computing the first order Taylor \nexpansion. 30 We can then compute the derivative using backpropagation in the network. \nA point-wise multiplication of the saliency map with the input features will provide the \ninfluence of each individual feature in the sequence on the output score.  \n \n \n \n \n \n \nFigure 8. (a) The class activation map technique multiplies weights wi by activation maps Ak to generate the activation maps. \nGlobal average pooling is used to summarize each activation map of the final convolutional layer. Class activation map is applied \non a time series dataset to highlight discriminative segments. (b) Example of applying class activation maps to highlight \nsalient timesteps of a univariate time series. \n \nUnlike the maximum activation approach, this approach does not examine activation of \nindividual hidden units, rather it examines saliency of the input features. \n \n𝑆(𝑋) ≈𝑤#𝑋+ 𝑏 \n𝑆𝑎𝑙𝑖𝑒𝑛𝑐𝑦 𝑀𝑎𝑝= 𝜕𝑆\n𝜕𝑋 \n \nWang et al.34 also took this approach, where their multilevel Wavelet Decomposition \nNetwork, which hierarchically decomposed a time series into low and high-frequency \nsignals in the time-frequency domain across several stages, utilized a similar gradient-\nbased derivation of important elements of both the input and intermediate layers for a \nvariety of time series classification and forecasting tasks. \n \niii. Attention \nPerhaps the most widespread form of sequential interpretability in recent years, attention \nmechanisms have found their way into a variety of models and applications. \n \nConceptually, attention mechanisms are designed to allow a model to focus on smaller \nsubsets of an input that are most influential with respect to an output task. Examining how \na model has decided to focus its attention weights for a given input allows improved \ninterpretability and explanation towards a model’s overall prediction. \n \n \nFigure 9. (a) Gradient-based saliency map examine saliency of individual features by superimposing the saliency map on the \nfeatures. (b) Example using saliency maps to identify important DNA-base characters of genetic sequences. \n \n \nAttention mechanisms can take several forms, but all typically operate on hidden state \nvectors at an intermediate or penultimate layer in a deep neural network. For facilitating \ndiscussion, suppose we have an input sequence 𝑥 of length 𝑇, with corresponding hidden \nstate vector ℎ also of length T. \n \n𝛼$ =\n𝑒(&∙(!)\n∑\n𝑒(&∙(\")\n#\n*+,\n \n \nIn the above example, attention – which is normalized to sum to 1 – is distributed to every \nelement in a sequence based on each time step’s compatibility with a global learned \nattention matrix  𝑊, evaluated here via dot product. The sequence can then be weighted \nby each time step’s attention weight, and in some sequential tasks, an overall \nrepresentation of the sequence (often referred to as a context vector) is computed by \nsumming over all time steps as opposed to taking the final time step representation as \nwould be done without attention: \n \n \n \n \n \nFigure 10. (a) Attention-based mechanism (b) Example using attention to discover relative word weights between \nsentence pairs in two languages for a machine translation task. \n \nℎ= 0 𝛼*ℎ*\n#\n*+,\n \n \nEarliest use of attention mechanisms can be traced to the machine translation work by \nBahdanau, et al.35, whose attention-based method closely resembles the above \nequations in the context of their RNN-based encoder-decoder architecture for sequence-\nto-sequence translation. Attention scores over word representations in the source \nlanguage were computed when generating each successive word in the target language, \nand were based on an alignment score from a feedforward neural network. Prior to the \nuse of attention mechanisms for machine translation, word decoding typically used the \nfinal recurrent token representation as the source context vector. This had the potential \nto “forget” very early words. When using attention, a context vector could instead be \nderived using a weighted sum of every input token. \n \nExtensions of this early work for machine translation include Luong et al.’s notion of local \nvs. global attention36. For each decoding step in their model, global attention attended to \nevery word in the source sentence in a similar spirit to Bahdanau et al35; local attention \ninstead assigned word-level alignment scores within a fixed-size window surrounding a \npredicted position. The concepts of local and global attention exhibit parallels to the \nnotions of soft vs. hard attention from the image captioning work of Xu et al.37, in which \nsoft attention generated differentiable alignments over all input elements, whereas hard \nattention selected specific patches of the input before attending in a nondifferentiable \nmanner. The work of Luong et al. additionally proposed alternative alignment functions \nfor assessing representation compatibility, including dot product and forms of general \nattention based on learned weight matrices.36 \n \nIn the context of sequential deep learning, these early attention mechanisms allowed \nmodels to piecewise utilize the hidden representation of every individual element from an \ninput sequence, rather than operating solely on the aggregated context vector from the \nfinal hidden state representation of a recurrent neural network. One perspective of \nsequential attention can be seen as exposing the entire history of the input \nrepresentations to the model, in effect yielding a notion of internal memory38. In the \nimplementation of memory networks for the task of question-answering by Sukhbaatar et \nal.39, an input sequence’s tokens were individually and simultaneously embedded into \nmemory vectors. Compatibility between each input token’s memory vector and a given \nquery vector was computed using an inner product and was used to weight the input \nsequence into a single context vector as described in previous attention works. As \nsequence elements were individually embedded, relative order was preserved by adding \na positional encoding to each element based on a learned embedding of its relative index \nin the sequence. For the task of question answering, their model involved recurrent hops \nacross an external memory. Given the state of modern attention mechanisms, the design \nof this memory network was notable for its elimination of recurrent processing of \nsequential inputs, and for the use of a learned position embedding to account for relative \nsequential ordering. \n \nUtilizing a similar style of non-recurrent processing of sequential inputs, Vaswani et al.40 \nintroduced the attention-based Transformer model for the task of machine translation, \nwhich outperformed traditional recurrent encoder-decoder paradigms. The Transformer \nviewed attention as a function of keys K, queries Q, and values V. In their work, all three \nelements came from the same input sequence, and is why their style of attention is \nreferred to as self-attention. In a similar manner to previously described works, \ncompatibility between a key and query is used to weight the value, and in the case of self-\nattention, each element of an input sequence is represented as a contextual sum of the \nalignment between itself and every other element. Similar to the memory networks of \nSukhbaatar et al.39, the Transformer also involves the addition of a positional encoding \nvector to preserve relative order information between input tokens. The recent NLP \nmethod BERT41 is based on Transformers and at present time whose variants represent \nstate of the art in a variety of natural language processing tasks. \n \nGiven the brief evolution of attention mechanisms above, it is no surprise that various \nforms of attention have seen most widespread use in a variety of NLP-adjacent \napplications, such as machine translation35,36,40,42–44, sentiment analysis45–47, text \nentailment45,47,48, question answering49, text summarization50, recommender systems51, \nimage captioning37, and visual question answering52–54.  \n \nAs attention mechanisms evolved primarily for improved NLP task performance, other \nstudies noted that such methods could be used as a window into the most impactful input \nsequence elements contributing to a contextual representation for classification. This \nperspective on attention is a form of sequential saliency, and can be applied to many \ntypes of sequential deep learning aside from natural language processing applications.  \n \nChorowski et al.55 implemented attention in a sequence-to-sequence framework for \nspeech recognition, where a context of attended time steps improved the ability to \ngenerate precise phoneme locations from audio recordings. \n \nSeveral works have implemented attention mechanisms for the prediction of clinical \noutcomes from sequences of discrete medical codes. In their GRAM model for electronic \nhealth record prediction tasks, Choi et al.56 constructed a directed acyclic graph from \nmedical ontologies and used attention to select relevant graph segments for predicting \ndiagnoses with a recurrent neural network from sequences of discrete clinical codes.  \n \nMa et al.57’s Dipole framework utilized attention with bidirectional RNNs for modeling \nlongitudinal hospital visits.  \n \nSha and Wang58 implemented a hierarchical attention mechanism that focused on \nrelevant visit-level and code-level representations in a GRU for predicting hospital \nmortality. A similar approach was taken by Zhang et al.59 in their Patient2Vec system for \npredicting hospital readmission risk. \n \nLin et al.60’s HA-TCN model stacked several sequential convolutional layers with \nincreasing dilation, and utilized both within-layer and across-layer attention mechanisms \nfor predicting myotonic dystrophy diagnosis to capture patterns of variable scale. \n \nIn a general warning, Pruthi et al.61 outlined two primary drawbacks  of relying on token-\nlevel attention scores as a means of model interpretability or quantifying the most \npredictive input features. In their work, they illustrate how attention scores can be easily \nmanipulated by small modifications to the objective function. They additionally highlight \nthe issues of attending to positional hidden representations, which contain information \nfrom neighboring words in the input sequence. Their experiments showed that even when \npreventing the assignment of attention to crucial words in a sentence, original \nperformance does not suffer.  \n \niv. Perturbation and sensitivity \nWhile many aforementioned methods have utilized weights and activations of a trained \nnetwork to derive sequential explanations, Alvarez-Melis and Jaakkola62 generate input-\noutput token dependencies for a sequence-to-sequence framework without using a \ntrained model’s parameters. They describe a multi-stage framework in which input \nsequences are perturbed using a pretrained variational autoencoder to generate \nsemantically similar sentences, and for each token in the target sequence, a probabilistic \nmodel is estimated to learn dependency coefficients between the original input and target \noutput, taking the form of a dense bipartite dependency graph. In their final explanation  \n \nFigure 11. (a) Intermediate or final model outputs of perturbed sequences can be examined to discover influential \nelements in a prediction task. (b) Example on a machine translation task. \n \nstep, they apply graph partitioning algorithms that incorporate dependency uncertainties \nto discover strong word dependency explanations. \n \nAlipanahi et al.9’s DeepBind framework employed a type of sensitivity analysis in their \nmutation maps for predicting DNA and RNA binding protein specificities, where sequence \nelements were replaced by molecular mutations and overall predictions compared with \nthat of the original input sequence.  \n \nv. Sequential masking  \nIn an effort to better explain the prediction of multi-aspect sentiment analysis and retrieval \nof semantically similar questions, Lei et al.63 augmented their RCNN model64 with a jointly \ntrained probabilistic generator network that learned a binary vector over input words most \ninfluential for a model’s prediction. Along with regularization terms to enforce short, \ncontiguous phrases, they interpreted selected subsequences as an interpretable \nprediction rationale. \n \nChoi et al.65 implemented a form of masking for their context-dependent word \nembeddings, where a sigmoid-activated mask over sequence words was learned jointly \nbefore word embeddings were fed into an LSTM network for machine translation. They \nfound their masking procedure allowed the translation model to focus on particular \nembedding dimensions of similar words whose meaning depended on surrounding \ncontext. \n \n \nFigure 12. (a) By learning to mask or ignore elements of input sequences and comparing outputs with unmasked models, one \ncan reveal elements crucial to a given prediction. (b) Example using masking for chemical molecules represented as SMILES \nsequences. \n \nSimilarly, using an already-trained recurrent and convolutional model for predicting \nchemical properties from SMILES string sequences, Goh et al.66 trained a separate \nresidual network to learn a continuous sequential mask over input elements in an effort \nto mask as much input data as possible without changing the output of the trained model. \nWhen examining the learned masks, they found localized patterns that conformed to \nknown chemistry concepts. \n \nvi. Erasure \nUsing the technique of representation erasure, Li et al.67 introduced a general method for \nmeasuring the influence of particular words or phrases, word representation dimensions, \nand intermediate hidden units and layers in their framework applicable to a variety of NLP \ntasks. In their derivation, the log-likelihood of a trained model to predict the correct label \n𝑐 for an input sequence 𝑒∈𝐸 can be represented as 𝑆(𝑒, 𝑐) = −log 𝑃(𝐿- = 𝑐), where 𝐿- \nis the model’s predicted label. When analyzing a particular dimension of interest 𝑑 within \na trained model, representations involving that dimension are set to zero, with the \nresulting prediction log-likelihood represented as 𝑆(𝑒, 𝑐, ¬𝑑). The importance of \ndimension 𝑑 is defined by the relative change in log-likelihood over the entire corpus 𝐸 as \nshown below: \n \n𝐼(𝑑) = 1\n|𝐸| 0 𝑆(𝑒, 𝑐) −𝑆(𝑒, 𝑐, ¬𝑑)\n𝑆(𝑒, 𝑐)\n-∈/\n \n \n \n \nFigure 13. Input elements or hidden layer representations can be set to zero to measure their relative impact. The importance of \nthe erased dimension is defined by the relative change in log-likelihood over the entire corpus. \n \nThe method of representation erasure yielded important words, embedding dimensions, \nand hidden units for a variety of NLP tasks including sequence tagging, ontological \nclassification, and sentiment analysis. They also explored reinforcement learning for \ndetermining the minimum number of removed input words to change their model’s \nprediction decision. \n \n \nvii. Causality \nKale et al.29 employ the Pairwise LiNGAM method of causal inference to deep \nrepresentations of physiological time series to discover the most causal latent dimensions \nof autoencoder-based representations. For the most causal hidden features, they \nvisualized the mean of the select input time series that resulted in maximum activation of \nthe most causal units and interpreted causal clinical phenotypes for two healthcare \nprediction tasks. \n \nFigure 14. Important features with respect to the output can be identified using causal inference. 29  \n \n \nviii. Sequential decomposition \nMurdoch et al.68 derived a method to decompose the output and cell states of LSTMs to \nisolate the contribution of a current phrase from the remainder of a textual sequence. \nQualitative comparison with several related studies67,69,70 yielded more sensible \nprediction rationale for a few select examples in sentiment analysis tasks, and when \ncomparing the distribution of positive and negative phrases, they found their method of \ncontextual decomposition to produce more separated sentiment densities. \n \nix. Timestep decay \nFor retrieving semantically related questions in a question-answering forum, Lei et al.64 \nmodified a convolutional neural network with a context-dependent gating scheme, similar \nin spirit to an LSTM, but involving a decay term 𝜆 for adaptively ignoring irrelevant textual \ntokens. When visualizing a heatmap of word-level decay terms for several dataset \nexamples, they found increased focus on domain-specific words important for their \nretrieval task. \n \n \nC. Multivariate Attribution \nMany of the aforementioned studies have focused on inputs of a single dimension, such \nas univariate real-valued time series or natural language text. An additional form of \nsequential interpretability arises when considering multivariate inputs, where at each time \nstep only a subset of variables are relevant. Similar to sequential saliency, methods for \nmultivariate attribution often take the form of visualization and pointing to specific \nsegments of input sequences. \n \ni. Attention \nThe RETAIN framework by Choi et al.71,72 made interpretability a primary goal, decoupling \nthe standard attention mechanism from time step representations to derive a precise \nformulation of the contribution of each time step and feature dimension towards predicting \nheart failure from sequences of discrete clinical codes. In their work, a separate attention \nmechanism was used for assessing the importance of each sequential timestep, as well \nas each variable at a given timestep. \n \nXu et al.73’s RAIM model took a similar approach to distinguish between time and feature-\nbased attention mechanisms in their guided attention method. Their model was used for \npredicting decompensation and length of stay using both discrete clinical codes and \nphysiological time series in the intensive care unit. They also used two distinct attention \nmechanisms: one each for timestep and variable-level importance. \n \nSimilar to RETAIN71 and RAIM73, Qin et al.74 implemented dual attention mechanisms \nalong with LSTMs for forecasting real-valued financial time series that focused on both \nthe timestep and the feature dimension of input multivariate sequences. The two attention \nmechanisms were designed to highlight both timestep and variable-level importance. \nSimilarly, the GeoMAN framework of Liang et al.75 incorporated both spatial and \nsequential attention mechanisms for forecasting geo-sensory time series. \n \nShickel et al.76 implemented a self-attention mechanism in conjunction with a GRU \nnetwork for the real-time prediction of intensive care unit mortality based on a collection \nof physiological signals, and mapped attention scores back to original time series inputs \nto provide interpretable explanations of each input variable at each time step to clinical \npractitioners. \n \nii. Backwards neuron attribution \nIntroduced by Bach et al.77 in their image processing work with the goal of quantifying the \ncontribution of each input pixel towards the final classification prediction, the technique of \nlayer-wise relevance propagation (LRP) frames a model’s output 𝑓(𝑥) as a sum of \nrelevance scores 𝑅 over all dimensions of an input 𝑥∈ℝ0. In the context of images, each \ndimension 𝑑 is a pixel: \n \n \nFigure 15. The backward attribution methods distribute the relevance of an upper-level neuron to all lower connected \nneurons based on the relative neuron activations. \n \n𝑓(𝑥) ≈0 𝑅1\n1∈2\n \n \nLRP operates by distributing neuron relevance at each layer 𝑙 of a network of 𝑘 layers \nsuch that the sum of a layer’s neuron relevance scores 𝑅1 is equivalent at each layer of \na network, including at the input layer: \n \n𝑓(𝑥) = 0 𝑅1\n3#\n1∈3#\n= 0 𝑅1\n3#$%\n1∈3#$%\n= 0 𝑅1\n3#$&\n1∈3#$&\n= ⋯= 0 𝑅1\n1∈2\n \n \nLayer relevance scores are computed in a backwards fashion, beginning with the quantity \n𝑓(𝑥) of the output neuron in the case of binary classification. Overall, the relevance of a \nneuron in layer 𝑙! is determined by the relevance of each neuron in adjacent upper layer \n𝑙!45 for which a connection exists between the two. An a priori function 𝑅*←7\n3#←3#'% is used \nto quantify the relevance distributed to a neuron 𝑖 in lower layer 𝑙! from a neuron 𝑗 in \nupper layer 𝑙!45. The total relevance of a neuron 𝑖 in layer 𝑙! is computed by the sum of \npartial relevance scores passed from connected neurons in the following layer:   \n \n𝑅*\n3# = 0 𝑅*←7\n3#←3#'%\n7∈3#'%\n \n \nOne potential form77 of a relevance passing function is shown below, which distributes \nthe relevance of an upper-level neuron to all lower connected neurons based on the \nrelative neuron activations 𝑎: \n \n𝑅*←7\n3#←3#'% = 𝑅7\n3#'% ∙\n𝑎*𝑤*!\n∑𝑎(𝑤(!\n(\n \n \nSturm et al.78 applied LRP for understanding the decisions made by a fully-connected \nnetwork for classifying EEG time series, and found LRP-derived relevant time series \nsegments to align with neurophysiologically plausible patterns.  \n \nAlong with gradient-based saliency techniques, Arras et al.79 also used LRP to discover \ninfluential words for sentiment analysis tasks. Along with visualizing relevant words for \nsingle example documents and aggregating top words across an entire corpus, they also \nperformed a quantitative experiment involving deletion of LRP-based influential words \nand found a significant performance decrease. \n \nSimilar in spirit to layer-wise relevance propagation, the DeepLIFT technique proposed \nby Shrikumar et al.80 involves backwards neuron attribution assignments based on a unit’s \nactivation in relation to a reference value computed from a forward pass through the \nnetwork. Caicedo-Torres and Gutierrez81 implemented DeepLIFT in their multi-scale \nconvolutional networks for predicting mortality in the intensive care unit from multivariate \nphysiological time series, and found influential subsequences for single prediction as well \nas population-aggregated features corresponding to both positive and negative overall \nimportance. \n \nWhile not primarily used for multivariate attribution, we incude these techniques due to \ntheir inherent capability to do so. Backwards neuron attribution methods can also be used \nfor highlighting sequential saliency.  \n \niii. Feature decomposition \nWith a distinct focus on sequential data, Hsu et al.82 developed a factorized variational \nautoencoder model that isolates the global sequence-level and local segment-level \nattributes of an input sequence. They visualized qualitative impact of the disentangled, \nfactorized attributes by varying the resulting latent dimensions for speech generation and \nevaluated quantitative results on speaker identification and phoneme recognition tasks. \n \nBased on the work of He et al.83, the MV-LSTM84 and IMV-LSTM85 architectures from \nGuo et al. refactor the traditional hidden state ℎ$ into a tensor of separate hidden states \nper input dimension [ℎ$\n5, … , ℎ$\n8]#. By deriving a set of complementary state update rules \nand aggregating context with a standard attention mechanism, the hidden states with \nrespect to each input variable were isolated and visualized for several multivariate time \nseries classification tasks.  \n \n \n \n \nFigure 16. A traditional machine learning model is trained on output from the deep learning model. The traditional \nmachine learning model can highlight important features. \n \niv. Feature decay \nChe et al.86’s GRU-D model included jointly trained, per-variable decay terms 𝛾 for both \ninput and hidden states for handling irregularly sampled and missing data in clinical time \nseries. By examining the learned sequential decay vectors for each input variable, they \nwere able to identify both the variables whose current value was important for predicting \nmortality, and the variables for which the model relied more on past observations. \n \nv. Model approximation \nIn the Interpretable Mimic Learning framework of Che et al.87–89, a deep learning model \nsuch as DNN, SDA, or LSTM, was trained to predict health-related outcomes based on \nmultivariate clinical time series. In one variant, a separate gradient boosted tree (GBT) \nmodel was trained on the original input time series to minimize the mean squared error \nbetween the true classification target and the raw output from the deep learning model. \nAnother variant included an intermediate logistic regression classifier trained to predict \nthe true target based on the raw deep learning outputs, in which the intermediate model’s \nclass predictions served as classification target for the GBT model based on the original \ninput time series. In both cases, they were able to derive human-interpretable decision \ntrees based on the learned knowledge from sequential deep learning. \n \nSimilarly, Wu et al.90 developed a collection of regularization techniques involving the \ntraining of decision trees to predict the output of a GRU network given the original input \ntime series. As with the work of Che et al.87–89, explainable feature importance were \navailable given the modeling of the deep network with an interpretable decision tree \nformulation. \n \nIn their LSTM framework for sentiment analysis and question answering, Murdoch and \nSzlam69 derived a decomposition of an LSTM’s output into a product of contribution \nfactors for each word in an input sentence describing their influence towards the \nprediction of each class. They scored relevant phrases for each classification target \nacross an entire corpus by measuring average contribution for each possible class in \nrelation to all other classes. Following the training of the LSTM, they trained a simple \npattern-matching model based on extracted relevant phrases for final prediction, and \nfound competitive, although lower, results compared with the standard LSTM. \n \nKrakovna and Doshi-Velez91 trained hidden Markov models to approximate hidden states \nof an LSTM for character-level language modeling from text. They train decision trees to \npredict hidden states from their hybrid algorithm, allowing for insight on how the language \nmodel is constructed. \n \nIn their CNN framework for processing EEG signals, Schirrmeister et al.92 correlate \ninterpretable, known features such as mean frequency band envelopes with given units’ \nactivations within their receptive field. They analyzed the degree of correlation to interpret \nwhether the CNN was approximating known predictive features or was learning \nrepresentations that contained additional information. With their feature correlation maps, \nthey discovered plausible localized patterns that supported existing research on the \ninfluence of EEG spectral power bands.   \n \nSimilar to backwards neuron propagation, model approximation techniques are more \ngeneralized and can be used for both sequential saliency as well as multivariate \nattribution. \n \n \nD. Tools \nAlthough this review has focused exclusively on interpreting deep learning methods in the \ncontext of sequential input data, we briefly note the presence of several open source tools \nand packages for improving general explainability for a variety of input data and model \ntypes. For Pytorch and Tensorflow, framework-specific tools with a suite of interpretability \ntechniques include Facebook Captum1 and tf-explain2, respectively. Specific to natural \nlanguage processing, exBERT3 can be used for visualizing self-attention patterns in \nTransformers, and AllenNLP4 is useful for a variety of gradient-based methods. For \n \n1 captum.ai \n2 github.com/sicara/tf-explain \n3 github.com/bhoov/exbert \n4 allennlp.org/interpret \ngeneralized and model-agnostic suites of tools and techniques, we encourage interested \nreaders to explore Lime5, SHAP6, IBM Explainability 3607, ALIBI8, Skater9, iNNvestigate10 \n \n \nIII. DISCUSSION \n \nThe works included in this review span a wide range of sequential applications of deep \nlearning involving a variety of sequential data types. While differing in execution, shared \namongst all techniques is the desire to improve some aspect of human interpretability of \nthe traditionally opaque deep models.  \n \nA. Goal-Oriented Taxonomy \nAt current time, the notion of interpretability in machine learning remains vague and \nprecisely undefined. In an effort to further the discussion and provide a more concrete \nfoundation for integration with existing sequential applications, we broadly categorized \nmethods according to the nature of the additional information which they make available. \nGiven the rise of deep sequential learning, its recent trend towards human-critical \napplications, and the corresponding renewed research focus on explainability, we feel a \ngoal-oriented classification of techniques is best suited for practitioners wishing to \nincorporate aspects of interpretability that are often domain and task specific.  \n \nA more traditional view of interpretability is embodied by the methods in our “Network \nAnalysis” section, where the goal is to provide additional understanding of the \nmechanisms by which a deep sequential model transforms data between its two \ninterpretable endpoints. Such techniques can be viewed as improving the transparency \nof the black box itself. Methods like weight inspection and maximum activation allow \npractitioners to understand the patterns and subsequences in the input space that each \ncomponent of a deep neural network is trained to respond to. In essence, network \nanalysis involves the notion of input/output correlations performed at a neuron, layer, or \nmodel-wise perspective and provide insight on how particular predictions are made on a \nmore global scale. These techniques are not only useful for tracing the flow of information \nthrough a deep sequential network, but they also provide a clear view of the sequential \npattern differences a model has learned between targets in the form of class prototypes. \n \n \n \n \n \n \n \n5 github.com/albermax/innvestigate \n6 github.com/slundberg/shap \n7 aix360.mybluemix.net \n8 github.com/SeldonIO/alibi \n9 github.com/oracle/Skater \n10 github.com/albermax/innvestigate \n \nInterpretability Goal \nInterpretability Method \nRequires Model Change? \nSequential Implementations \nNetwork Analysis \nWeight inspection \nNo \nKarpathy et al. [18] \nLasko et al. [19] \nMehrabi et al. [20] \nLi et al. [21] \nWang et al. [31] \nMaximum activation \nNo \nHermans & Schrauwen [23] \nKarpathy et al. [24] \nDong et al. [25] \nChe et al. [26] \nKale et al. [27] \nLanchantin et al. [28], [29] \nSiddiqui et al. [30] \nSequential Saliency \nClass activation maps \nNo \nWang et al. [31] \nLanchantin et al. [28,29] \nGradient-based saliency \nNo \nLanchantin et al. [28,29] \nSiddiqui et al. [30] \nWavelet Decomposition Network, Wang \net al. [32] \nAttention \nYes \nChorowski et al. [67] \nDipole, Ma et al. [69] \nGRNN-HA, Sha & Wang [70] \nPatient2Vec, Zhang et al. [71] \nRETAIN, Choi et al. [72], [73] \nRAIM, Xu et al. [74] \nDA-RNN, Qin et al. [75] \nDeepSOFA, Shickel et al. [76] \nGeoMAN, Liang et al. [77] \nHA-TCN, Lin et al. [78] \nPerturbation and sensitivity \nNo \nAlvarez-Melis & Jaakkola [45] \nDeepBind, Alipanahi et al. [9] \nSequential masking \nYes \nRCNN, Lei et al. [83], [85] \nChoi et al. 86] \nGoh et al. [47] \nErasure \nNo \nLi et al. [46] \nCausality \nNo \nKale et al. [27] \nSequential decomposition \nNo \nMurdoch et al. [48] \nTimestep decay \nYes \nLei et al. [83] \nGRU-D, Che et al. [84] \nMultivariate \nattribution \nAttention \nYes \nRETAIN, Choi et al. [72], [73] \nDA-RNN, Qin et al. [75] \nGeoMAN, Liang et al. [77] \nDeepSOFA, Shickel et al. [76] \nMV-LSTM, Guo et al. [81] \nBackwards neuron attribution \nNo \nLayer-wise relevance propagation, Bach \net al. [33], [34], [35] \nDeepLIFT, Shrikumar et al. [36], [37] \nFeature decomposition \nYes \nHsu et al. [79] \nHe et al. [84] \nMV-LSTM, Guo et al. [81] \nIMV-LSTM, Guo et al. [82] \nFeature decay \nYes \nGRU-D, Che et al. [84] \nModel approximation \nNo \nMimic learning, Che et al. [38], [39], [40] \nWu et al. [41] \nMurdoch & Szlam [42] \nKrakovna & Doshi-Velez [43] \nSchirrmeister et al. [44] \nTable 1. Overview and implementation frameworks of goal-based taxonomy of sequential interpretability methods. \n \nIn contrast to more dataset-wide insights from network analysis, the goal of techniques in \nour “Sequential Saliency” section focus on understanding relevant subsequences of a \nsingle sample that drive its output prediction. While network analysis can provide a view \nof input patterns that various regions of a deep learning model use to construct an overall \nsequence representation, they do not draw target-specific conclusions regarding \nimportant patterns between a single input and its corresponding output. In effect, many \nnetwork analysis techniques can be viewed as an unsupervised form of interpretability \nusing a supervised learning model. On the other hand, sequential saliency is solely \nfocused on corelating exact patterns of an input to the prediction class. Such techniques \noften take the form of highlighting or visualizing subsequences of an input that are most \nresponsible for its prediction. If a practitioner’s goal is to provide an explanation for a \nprediction, sequential saliency methods are those that apply.  \n \nWhile the two aforementioned categories of interpretable methods are straightforward to \nunderstand in a univariate setting such as text or a single time series, many sequential \napplications involve multiple data points at each time step. While methods involving \nsequential saliency can discover which time steps were most influential, they are unable \nto identify and disentangle the importance of individual input variables at the salient time \nsteps. In our section “Multivariate Attribution”, we described techniques for isolating and \nquantifying importance at a per-dimension scale. These individual dimensions can refer \nto either the input variables themselves or such specific dimensions of a model’s hidden \ninternal representation.  \n \nB. Comparison with Existing Interpretability Classifications \nIn the context of recently published overviews of interpretable machine learning \nmethods12,93 that tend to divide approaches as intrinsic or post-hoc, the majority of current \ntechniques for sequential interpretability are decidedly post-hoc in nature; that is, given \nan already-trained deep learning model, these methods seek an understanding of what \nexactly a model has learned by either examining trained model parameters or by \nmeasuring the model’s response to particular input sequences. This contrasts with \nLipton’s notion of transparency, which can generally be viewed as a complete \nunderstanding of the mechanistic data transformations occurring within the model itself. \nExemplified by the relative lack of research in this area, precisely tracing inputs to outputs \nin deep learning frameworks is inherently difficult due to the fundamental nonlinear and \nhierarchical data transformations. While some may argue that post-hoc techniques \ncannot be fully trusted because they still rely on deep learning as a black box, we note \nLipton’s analogy12 that if humans are considered interpretable, it is only in a post-hoc \nmanner; while it is impossible to completely understand the precise biological \nmechanisms of human brain activity leading to an external expression or decision, it may \nnot be strictly necessary, as useful information can still be gleaned from a variety of \nexplanatory methods. \n \nC. Limitations and Future Direction \nIn this work we have provided an overview of current techniques that can be used to \nbetter understand deep learning frameworks in the context of sequential data types. After \na comprehensive analysis of the body of related literature, we have identified several \ndata-driven limitations and opportunities for future research that can be categorized as \npertaining to either (1) data modalities, (2) application domains, and (3) interpretable \ntechniques. These limitations represent a gap in current literature that are either natural \nextensions of existing techniques, or logical extensions of existing ideas and techniques. \nBased on the contents of this review, we recommend further research and exploration \npertaining to these issues. \n \nReferring to Figure 2, the most common data type among deep sequential interpretability \nliterature is text (29 studies), followed by time series (24), discrete sequences (13), and \nvideo (1). Given the prevalence of deep learning for video processing, it is surprising to \nsee the lack of interpretable frameworks for this modality.  \n \nAdditionally, while several works on this review aim to identify influential time steps of \nsequential input sequences, nearly all are viewed in isolation; that is, salient \nsubsequences are identified by contiguous, single salient time steps rather than by \ndirectly modeling a sequence as a series of arbitrary length subsequences. \n \nMissing from current literature are sequential applications of several domain and model-\nagnostic approaches to deep learning interpretability, such as local interpretable model-\nagnostic explanations (LIME)94,95, meta learning96, deep nearest neighbors97, and \nadversarial techinques98,99. Such methods are general in nature, and while there exists \nno theoretical obstacle for sequential data types, at current time these techniques have \nnot been explored or demonstrated for sequential data and applications. \n \n \n \nREFERENCES \n1. \nLecun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444 (2015). \n2. \nGuo, Y. et al. Deep learning for visual understanding: A review. Neurocomputing \n187, 27–48 (2016). \n3. \nVoulodimos, A., Doulamis, N., Doulamis, A. & Protopapadakis, E. Deep Learning \nfor Computer Vision: A Brief Review. Comput. Intell. Neurosci. 2018, 1–13 (2018). \n4. \nYoung, T., Hazarika, D., Poria, S. & Cambria, E. Recent trends in deep learning \nbased natural language processing. IEEE Comput. Intell. Mag. 13, 55–75 (2018). \n5. \nDeng, L. & Liu, Y. Deep Learning in Natural Language Processing. (Springer, \n2018). \n6. \nShickel, B., Tighe, P. J., Bihorac, A. & Rashidi, P. Deep EHR: A Survey of Recent \nAdvances in Deep Learning Techniques for Electronic Health Record (EHR) \nAnalysis. IEEE J. Biomed. Heal. Informatics 22, 1589–1604 (2018). \n7. \nMiotto, R., Wang, F., Wang, S., Jiang, X. & Dudley, J. T. Deep learning for \nhealthcare: review, opportunities and challenges. Brief. Bioinform. 1–11 (2017). \ndoi:10.1093/bib/bbx044 \n8. \nHeaton, J. B., Polson, N. G. & Witte, J. H. Deep learning for finance: deep \nportfolios. Appl. Stoch. Model. Bus. Ind. 33, 3–12 (2017). \n9. \nAlipanahi, B., Delong, A., Weirauch, M. T. & Frey, B. J. Predicting the sequence \nspecificities of DNA- and RNA-binding proteins by deep learning. Nat. Biotechnol. \n33, 831–838 (2015). \n10. \nGawehn, E., Hiss, J. A. & Schneider, G. Deep Learning in Drug Discovery. Mol. \nInform. 35, 3–14 (2016). \n11. \nHinton, G. et al. Deep neural networks for acoustic modeling in speech \nrecognition. IEEE Signal Process. Mag. 29, 82–97 (2012). \n12. \nLipton, Z. C. The Mythos of Model Interpretability. (2016). \n13. \nDoshi-Velez, F. & Kim, B. Towards A Rigorous Science of Interpretable Machine \nLearning. 1–13 (2017). \n14. \nLage, I., Chen, E., He, J., Narayanan, M. & Kim, B. An Evaluation of the Human-\nInterpretability of Explanation. (2019). \n15. \nHohman, F., Kahng, M., Pienta, R. & Chau, D. H. Visual Analytics in Deep \nLearning : An Interrogative Survey for the Next Frontiers. 25, 1–20 (2019). \n16. \nChoo, J. & Liu, S. Visual Analytics for Explainable Deep Learning. IEEE Comput. \nGraph. Appl. 38, 84–92 (2018). \n17. \nZhang, Q. & Zhu, S.-C. Visual Interpretability for Deep Learning: a Survey. 19, \n27–39 (2018). \n18. \nMehrabi, S. et al. Sequential Pattern and Association Discovery of Diagnosis \nCodes Using Deep Learning. in Proceedings of 2015 International Conference on \nHealthcare Informatics (ICHI) 408–416 (2015). doi:10.1109/ICHI.2015.58 \n19. \nKarpathy, A. & Leung, T. Large-scale Video Classification with Convolutional \nNeural Networks. 10–20 (2014). doi:10.1109/CVPR.2014.223 \n20. \nLi, J., Chen, X., Hovy, E. & Jurafsky, D. Visualizing and Understanding Neural \nModels in NLP. in Proceedings of the 2016 Conference of the North American \nChapter of the Association for Computational Linguistics: Human Language \nTechnologies 681–691 (Association for Computational Linguistics, 2016). \n21. \nLee, H., Grosse, R., Ranganath, R. & Ng, A. Y. Convolutional deep belief \nnetworks for scalable unsupervised learning of hierarchical representations. in \nProceedings of the 26th Annual International Conference on Machine Learning - \nICML ’09 1–8 (2009). doi:10.1145/1553374.1553453 \n22. \nLasko, T. A., Denny, J. C. & Levy, M. A. Computational Phenotype Discovery \nUsing Unsupervised Feature Learning over Noisy, Sparse, and Irregular Clinical \nData. PLoS One 8, (2013). \n23. \nErhan, D., Bengio, Y., Courville, A. & Vincent, P. Visualizing Higher-Layer \nFeatures of a Deep Network. (2009). doi:10.2464/jilm.23.425 \n24. \nSzegedy, C. et al. Intriguing properties of neural networks. (2014). \ndoi:10.1017/S0269888998214044 \n25. \nHermans, M. & Schrauwen, B. Training and Analysing Deep Recurrent Neural \nNetworks. 1, 190–198 (2013). \n26. \nKarpathy, A., Johnson, J. & Fei-Fei, L. Visualizing and understanding recurrent \nnetworks. arXiv Prepr. arXiv1506.02078 (2015). \n27. \nDong, W. et al. Characterizing Driving Styles with Deep Learning. (2016). \n28. \nChe, Z., Kale, D., Li, W., Taha Bahadori, M. & Liu, Y. Deep Computational \nPhenotyping. in Proceedings of the 21st ACM SIGKDD International Conference \non Knowledge Discovery and Data Mining 507–516 (2015). \ndoi:10.1145/2783258.2783365 \n29. \nKale, D. C. et al. Causal Phenotype Discovery via Deep Networks. AMIA Annu. \nSymp. Proc. 2015, 677–86 (2015). \n30. \nLanchantin, J., Singh, R., Lin, Z. & Qi, Y. Deep Motif: Visualizing Genomic \nSequence Classifications. 1–5 (2016). \n31. \nLanchantin, J., Singh, R., Wang, B. & Qi, Y. Deep Motif Dashboard: Visualizing \nand Understanding Genomic Sequences Using Deep Neural Networks. (2016). \n32. \nSiddiqui, S. A., Mercier, D., Munir, M., Dengel, A. & Ahmed, S. TSViz: \nDemystification of Deep Learning Models for Time-Series Analysis. 1–13 (2018). \n33. \nWang, Z., Yan, W. & Oates, T. Time series classification from scratch with deep \nneural networks: A strong baseline. Proc. Int. Jt. Conf. Neural Networks 2017-\nMay, 1578–1585 (2017). \n34. \nWang, J., Wang, Z., Li, J. & Wu, J. Multilevel Wavelet Decomposition Network for \nInterpretable Time Series Analysis. 2437–2446 (2018). \n35. \nBahdanau, D., Cho, K. & Bengio, Y. Neural Machine Translation by Jointly \nLearning to Align and Translate. arXiv (2014). doi:10.1007/s10107-014-0839-0 \n36. \nLuong, M.-T., Pham, H. & Manning, C. D. Effective Approaches to Attention-\nbased Neural Machine Translation. (2015). \n37. \nXu, K. et al. Show, Attend and Tell: Neural Image Caption Generation with Visual \nAttention. (2015). \n38. \nBritz, D. Attention and Memory in Deep Learning and NLP. WildML (2016). \n39. \nSukhbaatar, S. & Szlam, A. End-To-End Memory Networks. in Advances in neural \ninformation processing systems 2440–2448 (2015). \n40. \nVaswani, A. et al. Attention Is All You Need. Adv. Neural Inf. Process. Syst. 5998–\n6008 (2017). doi:10.1017/S0952523813000308 \n41. \nDevlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep \nBidirectional Transformers for Language Understanding. (2018). \ndoi:arXiv:1811.03600v2 \n42. \nCohn, T. et al. Incorporating Structural Alignment Biases into an Attentional \nNeural Translation Model. 876–885 (2016). \n43. \nTu, Z., Lu, Z., Liu, Y., Liu, X. & Li, H. Modeling Coverage for Neural Machine \nTranslation. (2016). \n44. \nChoi, H., Cho, K. & Bengio, Y. Fine-grained attention mechanism for neural \nmachine translation. Neurocomputing 284, 171–176 (2018). \n45. \nLin, Z. et al. A Structured Self-attentive Sentence Embedding. 1–15 (2017). \n46. \nYang, Z. et al. Hierarchical Attention Networks for Document Classification. in \nProceedings of the 2016 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technologies 1480–\n1489 (Association for Computational Linguistics, 2016). \n47. \nCheng, J., Dong, L. & Lapata, M. Long Short-Term Memory-Networks for Machine \nReading. (2016). \n48. \nRocktäschel, T., Grefenstette, E., Hermann, K. M., Kočiský, T. & Blunsom, P. \nReasoning about Entailment with Neural Attention. 1–9 (2015). \n49. \nHermann, K. M. et al. Teaching Machines to Read and Comprehend. 1–14 \n(2015). \n50. \nRush, A. M., Chopra, S. & Weston, J. A Neural Attention Model for Abstractive \nSentence Summarization. (2015). \n51. \nSeo, S., Huang, J., Yang, H. & Liu, Y. Interpretable Convolutional Neural \nNetworks with Dual Local and Global Attention for Review Rating Prediction. 297–\n305 (2017). doi:10.1145/3109859.3109890 \n52. \nChen, K. et al. ABC-CNN: An Attention Based Convolutional Neural Network for \nVisual Question Answering. (2015). \n53. \nXu, H. & Saenko, K. Ask, attend and answer: Exploring question-guided spatial \nattention for visual question answering. in Lecture Notes in Computer Science \n(including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in \nBioinformatics) 9911 LNCS, 451–466 (2016). \n54. \nYang, Z., He, X., Gao, J., Deng, L. & Smola, A. Stacked attention networks for \nimage question answering. in Proceedings of the IEEE Computer Society \nConference on Computer Vision and Pattern Recognition 2016-Decem, 21–29 \n(2016). \n55. \nChorowski, J., Bahdanau, D., Serdyuk, D., Cho, K. & Bengio, Y. Attention-Based \nModels for Speech Recognition. (2015). \n56. \nChoi, E., Bahadori, M. T., Song, L., Stewart, W. F. & Sun, J. GRAM: Graph-based \nAttention Model for Healthcare Representation Learning. 1–15 (2016). \n57. \nMa, F. et al. Dipole: Diagnosis Prediction in Healthcare via Attention-based \nBidirectional Recurrent Neural Networks. (2017). doi:10.1145/3097983.3098088 \n58. \nSha, Y. & Wang, M. D. Interpretable Predictions of Clinical Outcomes with An \nAttention-based Recurrent Neural Network. 233–240 (2017). \ndoi:10.1145/3107411.3107445 \n59. \nZhang, J., Kowsari, K., Harrison, J. H., Lobo, J. M. & Barnes, L. E. Patient2Vec: A \nPersonalized Interpretable Deep Representation of the Longitudinal Electronic \nHealth Record. IEEE Access 6, 65333–65346 (2018). \n60. \nLin, L., Xu, B., Wu, W., Richardson, T. & Bernal, E. A. Medical Time Series \nClassification with Hierarchical Attention-based Sequential Convolutional \nNetworks: A Case Study of Myotonic Dystrophy Diagnosis. (2019). \n61. \nPruthi, D., Gupta, M., Dhingra, B., Neubig, G. & Lipton, Z. C. Learning to Deceive \nwith Attention-Based Explanations. arXiv (2014). \n62. \nAlvarez-Melis, D. & Jaakkola, T. S. A causal framework for explaining the \npredictions of black-box sequence-to-sequence models. (2017). \n63. \nLei, T., Barzilay, R. & Jaakkola, T. Rationalizing Neural Predictions. (2016). \n64. \nLei, T. et al. Semi-supervised Question Retrieval with Gated Convolutions. (2015). \n65. \nChoi, H., Cho, K. & Bengio, Y. Context-dependent word representation for neural \nmachine translation. Comput. Speech Lang. 45, 149–160 (2017). \n66. \nGoh, G. B., Hodas, N. O., Siegel, C. & Vishnu, A. SMILES2Vec: An Interpretable \nGeneral-Purpose Deep Neural Network for Predicting Chemical Properties. \n(2017). doi:10.475/123 \n67. \nLi, J., Monroe, W. & Jurafsky, D. Understanding Neural Networks through \nRepresentation Erasure. (2016). \n68. \nMurdoch, W. J., Liu, P. J. & Yu, B. Beyond Word Importance: Contextual \nDecomposition to Extract Interactions from LSTMs. 1–14 (2018). \n69. \nMurdoch, W. J. & Szlam, A. Automatic Rule Extraction from Long Short Term \nMemory Networks. (2017). \n70. \nSundararajan, M., Taly, A. & Yan, Q. Axiomatic Attribution for Deep Networks. \n(2017). \n71. \nChoi, E., Bahadori, M. T., Schuetz, A., Stewart, W. F. & Sun, J. RETAIN: \nInterpretable Predictive Model in Healthcare using Reverse Time Attention \nMechanism. \n72. \nKwon, B. C. et al. RetainVis: Visual Analytics with Interpretable and Interactive \nRecurrent Neural Networks on Electronic Medical Records. (2018). \n73. \nXu, Y., Biswal, S., Deshpande, S. R., Maher, K. O. & Sun, J. RAIM: Recurrent \nAttentive and Intensive Model of Multimodal Patient Monitoring Data. (2018). \n74. \nQin, Y. et al. A dual-stage attention-based recurrent neural network for time series \nprediction. IJCAI Int. Jt. Conf. Artif. Intell. 2627–2633 (2017). \n75. \nLiang, Y., Ke, S., Zhang, J., Yi, X. & Zheng, Y. Geoman: Multi-level attention \nnetworks for geo-sensory time series prediction. IJCAI Int. Jt. Conf. Artif. Intell. \n2018-July, 3428–3434 (2018). \n76. \nShickel, B. et al. DeepSOFA: A Continuous Acuity Score for Critically Ill Patients \nusing Clinically Interpretable Deep Learning. Sci. Rep. 9, 1879 (2019). \n77. \nBach, S. et al. On pixel-wise explanations for non-linear classifier decisions by \nlayer-wise relevance propagation. PLoS One 10, 1–46 (2015). \n78. \nSturm, I., Lapuschkin, S., Samek, W. & Müller, K. R. Interpretable deep neural \nnetworks for single-trial EEG classification. J. Neurosci. Methods 274, 141–145 \n(2016). \n79. \nArras, L., Montavon, G., Müller, K.-R. & Samek, W. Explaining Recurrent Neural \nNetwork Predictions in Sentiment Analysis. 159–168 (2017). \n80. \nShrikumar, A., Greenside, P. & Kundaje, A. Learning Important Features Through \nPropagating Activation Differences. (2017). \n81. \nCaicedo-Torres, W. & Gutierrez, J. ISeeU: Visually interpretable deep learning for \nmortality prediction inside the ICU. 1–24 (2019). \n82. \nHsu, W.-N., Zhang, Y. & Glass, J. Unsupervised Learning of Disentangled and \nInterpretable Representations from Sequential Data. (2017). \n83. \nHe, Z. et al. Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for \nSequence Learning. 1–11 (2017). \n84. \nGuo, T., Lin, T. & Lu, Y. An interpretable LSTM neural network for autoregressive \nexogenous model. 1–6 (2018). \n85. \nGuo, T., Lin, T. & Antulov-Fantulin, N. Exploring Interpretable LSTM Neural \nNetworks over Multi-Variable Data. (2019). \n86. \nChe, Z., Purushotham, S., Cho, K., Sontag, D. & Liu, Y. Recurrent Neural \nNetworks for Multivariate Time Series with Missing Values. Sci. Rep. 8, 1–12 \n(2018). \n87. \nChe, Z., Purushotham, S., Khemani, R. & Liu, Y. Distilling Knowledge from Deep \nNetworks with Applications to Healthcare Domain. arXiv 1–13 (2015). \n88. \nChe, Z., Purushotham, S., Khemani, R. & Liu, Y. Interpretable Deep Models for \nICU Outcome Prediction. AMIA ... Annu. Symp. proceedings. AMIA Symp. 2016, \n371–380 (2016). \n89. \nChe, Z. & Liu, Y. Deep learning solutions to computational phenotyping in health \ncare. IEEE Int. Conf. Data Min. Work. ICDMW 2017-Novem, 1100–1109 (2017). \n90. \nWu, M. et al. Beyond Sparsity: Tree Regularization of Deep Models for \nInterpretability. (2017). \n91. \nKrakovna, V. & Doshi-Velez, F. Increasing the Interpretability of Recurrent Neural \nNetworks Using Hidden Markov Models. (2016). \n92. \nSchirrmeister, R. T. et al. Deep learning with convolutional neural networks for \nEEG decoding and visualization. Hum. Brain Mapp. 38, 5391–5420 (2017). \n93. \nDu, M., Liu, N. & Hu, X. Techniques for Interpretable Machine Learning. Commun. \nACM 63, 68–77 (2019). \n94. \nRibeiro, M. T., Singh, S. & Guestrin, C. Model-Agnostic Interpretability of Machine \nLearning. (2016). \n95. \nRibeiro, M. T., Singh, S. & Guestrin, C. ‘Why Should I Trust You?’: Explaining the \nPredictions of Any Classifier. (2016). \n96. \nLiu, X., Wang, X. & Matwin, S. Interpretable Deep Convolutional Neural Networks \nvia Meta-learning. in Proceedings of the International Joint Conference on Neural \nNetworks 2018-July, (2018). \n97. \nPapernot, N. & McDaniel, P. Deep k-Nearest Neighbors: Towards Confident, \nInterpretable and Robust Deep Learning. (2018). \n98. \nDong, Y., Bao, F., Su, H. & Zhu, J. Towards Interpretable Deep Neural Networks \nby Leveraging Adversarial Examples. (2017). \n99. \nChen, X. et al. InfoGAN: Interpretable Representation Learning by Information \nMaximizing Generative Adversarial Nets. Nips (2016). doi:10.1007/978-3-319-\n16817-3 \n \n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-04-27",
  "updated": "2020-04-27"
}