{
  "id": "http://arxiv.org/abs/2004.12974v1",
  "title": "Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning",
  "authors": [
    "Archit Sharma",
    "Michael Ahn",
    "Sergey Levine",
    "Vikash Kumar",
    "Karol Hausman",
    "Shixiang Gu"
  ],
  "abstract": "Reinforcement learning provides a general framework for learning robotic\nskills while minimizing engineering effort. However, most reinforcement\nlearning algorithms assume that a well-designed reward function is provided,\nand learn a single behavior for that single reward function. Such reward\nfunctions can be difficult to design in practice. Can we instead develop\nefficient reinforcement learning methods that acquire diverse skills without\nany reward function, and then repurpose these skills for downstream tasks? In\nthis paper, we demonstrate that a recently proposed unsupervised skill\ndiscovery algorithm can be extended into an efficient off-policy method, making\nit suitable for performing unsupervised reinforcement learning in the real\nworld. Firstly, we show that our proposed algorithm provides substantial\nimprovement in learning efficiency, making reward-free real-world training\nfeasible. Secondly, we move beyond the simulation environments and evaluate the\nalgorithm on real physical hardware. On quadrupeds, we observe that locomotion\nskills with diverse gaits and different orientations emerge without any rewards\nor demonstrations. We also demonstrate that the learned skills can be composed\nusing model predictive control for goal-oriented navigation, without any\nadditional training.",
  "text": "Emergent Real-World Robotic Skills via\nUnsupervised Off-Policy Reinforcement Learning\nArchit Sharmaα , Michael Ahn, Sergey Levine, Vikash Kumar, Karol Hausmanβ, Shixiang Guβ\nGoogle Research\nAbstract—Reinforcement learning provides a general frame-\nwork for learning robotic skills while minimizing engineering\neffort. However, most reinforcement learning algorithms assume\nthat a well-designed reward function is provided, and learn a\nsingle behavior for that single reward function. Such reward\nfunctions can be difﬁcult to design in practice. Can we instead\ndevelop efﬁcient reinforcement learning methods that acquire\ndiverse skills without any reward function, and then re-purpose\nthese skills for downstream tasks? In this paper, we demonstrate\nthat a recently proposed unsupervised skill discovery algorithm\ncan be extended into an efﬁcient off-policy method, making it suit-\nable for performing unsupervised reinforcement learning in the\nreal world. Firstly, we show that our proposed algorithm provides\nsubstantial improvement in learning efﬁciency, making reward-\nfree real-world training feasible. Secondly, we move beyond the\nsimulation environments and evaluate the algorithm on real\nphysical hardware. On quadrupeds, we observe that locomotion\nskills with diverse gaits and different orientations emerge without\nany rewards or demonstrations. We also demonstrate that the\nlearned skills can be composed using model predictive control\nfor goal-oriented navigation, without any additional training.\nI. INTRODUCTION\nReinforcement learning (RL) has the potential of enabling\nautonomous agents to exhibit intricate behaviors and solve\ncomplex tasks from high-dimensional sensory input without\nhand-engineered policies or features [54, 48, 37, 35, 16].\nThese properties make this family of algorithms particularly\napplicable to the ﬁeld of robotics where hand-engineering\nfeatures and control policies have proven to be challenging\nand difﬁcult to scale [30, 28, 49, 29, 17, 26]. However,\napplying RL to real-world robotic problems has not fully\ndelivered on its promise. One of the reasons for this is that\nthe assumptions that are required in a standard RL formulation\nare not fully compatible with the requirements of real-world\nrobotics systems. One of these assumptions is the existence\nof a ground truth reward signal, provided as part of the task.\nWhile this is easy in simulation, in the real world this often\nrequires special instrumentation of the setup, as well as the\nability to reset the environment after every learning episode,\nwhich often requires tailored reset mechanisms or manual\nlabor. If we could relax some of these assumptions, we may\nbe able to fully utilize the potential of RL algorithms in real-\nworld robotic problems.\nIn this context for robotics, the recent work in unsupervised\nlearning becomes relevant — we can learn skills without any\nα Work done in Google AI residency\nβ equal contribution\nContact: architsh@google.com\nFig. 1. A 12 degree of freedom quadruped (D’Kitty) discovers diverse\nlocomotion skills without any rewards or demonstrations. We develop\noff-DADS, an asynchronous and off-policy version of Dynamics-\nAware Discovery of Skills (DADS) [52], that enables sample-efﬁcient\nskill discovery based on mutual-information based optimization.\nexternal reward supervision and then re-purpose those skills to\nsolve downstream tasks using only a limited amount of inter-\naction. Of course, when learning skills without any reward su-\npervision, we have limited control over the kinds of skills that\nemerge. Therefore, it is critical for unsupervised skill learning\nframeworks to optimize for diversity, so as to produce a large\nenough repertoire of skills such that potentially useful skills\nare likely to be part of this repertoire. In addition, a framework\nlike this needs to offer the user some degree of control over the\ndimensions along which the algorithm explores. Prior works in\nunsupervised reinforcement learning [43, 38, 45, 2, 13, 14, 52]\nhave demonstrated that interesting behaviors can emerge from\nreward-free interaction between the agent and environment.\nIn particular, [13, 14, 52] demonstrate that the skills learned\nfrom such unsupervised interaction can be harnessedto solve\ndownstream tasks. However, due to their sample-inefﬁciency,\nthese prior works in unsupervised skill learning have been\nrestricted to simulation environments (with a few exceptions\nsuch as Baranes and Oudeyer [5], Pong et al. [46], Lee et al.\n[33]) and their feasibility of executing on real robots remains\nunexplored.\nIn this paper, we address the limiting sample-inefﬁciency\nchallenges of previous reward-free, mutual-information-based\nlearning methods and demonstrate that it is indeed feasible to\ncarry out unsupervised reinforcement learning for acquisition\narXiv:2004.12974v1  [cs.RO]  27 Apr 2020\nof robotic skills. To this end, we build on the work of Sharma\net al. [52] and derive a sample-efﬁcient, off-policy version\nof a mutual-information-based, reward-free RL algorithm,\nDynamics-Aware Discovery of Skills (DADS), which we refer\nto as off-DADS. Our method uses a mutual-information-based\nobjective for diversity of skills and speciﬁcation of task-\nrelevant dimensions (such as x-y location of the robot) to\nspecify where to explore. Moreover, we extend off-DADS to\nbe able to efﬁciently collect data on multiple robots, which\ntogether with the efﬁcient, off-policy nature of the algorithm,\nmakes reward-free real-world robotics training feasible. We\nevaluate the asynchronous off-DADS method on D’Kitty, a\ncompact cost-effective quadruped, from the ROBEL robotic\nbenchmark suite [3]. We demonstrate that diverse skills with\ndifferent gaits and navigational properties can emerge, without\nany reward or demonstration. We present simulation experi-\nments that indicate that our off-policy algorithm is up to 4x\nmore efﬁcient than its predecessor. In addition, we conduct\nreal-world experiments showing that the learned skills can\nbe harnessed to solve downstream tasks using model-based\ncontrol, as presented in [52].\nII. RELATED WORK\nOur work builds on a number of recent works [17, 32, 26,\n36, 19, 41] that study end-to-end reinforcement learning of\nneural policies on real-world robot hardware, which poses\nsigniﬁcant challenges such as sample-efﬁciency, reward en-\ngineering and measurements, resets, and safety [17, 11, 57].\nGu et al. [17], Kalashnikov et al. [26], Haarnoja et al.\n[19], Nagabandi et al. [41] demonstrate that existing off-policy\nand model-based algorithms are sample efﬁcient enough for\nreal world training of simple manipulation and locomotion\nskills given reasonable task rewards. Eysenbach et al. [12], Zhu\net al. [57] propose reset-free continual learning algorithms and\ndemonstrate initial successes in simulated and real environ-\nments. To enable efﬁcient reward-free discovery of skills, our\nwork aims to address the sample-efﬁciency and reward-free\nlearning jointly through a novel off-policy learning framework.\nReward engineering has been a major bottleneck not only in\nrobotics, but also in general RL domains. There are two kinds\nof approaches to alleviate this problem. The ﬁrst kind involves\nrecovering a task-speciﬁc reward function with alternative\nforms of speciﬁcations, such as inverse RL [42, 1, 58, 22]\nor preference feedback [8]; however, these approaches still\nrequire non-trivial human effort. The second kind proposes\nan intrinsic motivation reward that can be applied to differ-\nent MDPs to discover useful policies, such as curiosity for\nnovelty [50, 43, 51, 6, 44, 9], entropy maximization [23, 46,\n33, 15], and mutual information [27, 25, 10, 14, 13, 38, 52].\nOurs extends the dynamics-based mutual-information objec-\ntive from Sharma et al. [52] to sample-efﬁcient off-policy\nlearning.\nOff-policy extension to DADS [52] poses challenges beyond\nthose in standard RL [47, 24, 56, 39]. Since we learn an\naction abstraction that can be related to a low-level policy\nin hierarchical RL (HRL) [55, 53, 31, 4, 40], we encounter\nsimilar difﬁculties as in off-policy HRL [40, 34]. We took\ninspirations from the techniques introduced in [40] for stable\noff-policy learning of a high-level policy; however, on top of\nthe non-stationarity in policy, we also need to deal with the\nnon-stationarity in reward function as our DADS rewards are\ncontinually updated during policy learning. We successfully\nderive a novel off-policy variant of DADS that exhibits stable\nand sample-efﬁcient learning.\nIII. BACKGROUND\nIn this section, we setup the notation to formally intro-\nduce the reinforcement learning problem and the algorithmic\nfoundations of our proposed approach. We work in a Markov\ndecision process (MDP) M = (S, A, p, r), where S denotes\nthe state space of the agent, A denotes the action space of the\nagent, p : S×S×A →[0, ∞) denotes the underlying (stochas-\ntic) dynamics of the agent-environment which can be sampled\nstarting from the initial state distribution p0 : S →[0, ∞),\nand a reward function r : S × A →[0, ∞). The goal of the\noptimization problem is to learn a controller π(at | st) which\nmaximizes E[P\nt γtr(st, at)] for a discount factor γ ∈[0, 1).\nWithin deep reinforcement learning, there are several\nmethods\nto\noptimize\nthis\nobjective.\nIn\nparticular,\noff-\npolicy methods centered around learning a Q-function [35,\n20, 16, 26] are known to be suitable for reinforcement\nlearning on robots. At a high level, algorithms estimate\nQπ(st, at) = E[P\ni≥t γi−tr(si, ai)], where the expectation is\ntaken over state-action trajectories generated by the execut-\ning policy π in the MDP M after taking action at in the\nstate st. Crucially, Qπ can be estimated using data col-\nlected from arbitrary policies using the temporal-difference\nlearning (hence off-policy learning). For continuous or large\ndiscrete action spaces, a parametric policy can be updated\nto π′(a | s) ←argmaxaQπ(s, a), which can be done approx-\nimately using stochastic gradient descent when Q is differ-\nentiable with respect to a [35, 21, 18]. While the off-policy\nmethods differ in speciﬁcs of each step, they alternate between\nestimating Qπ and updating π using the Qπ till convergence.\nThe ability to use trajectories sampled from arbitrary policies\nenables these algorithms to be sample efﬁcient.\nA. Unsupervised Reinforcement Learning\nIn an unsupervised learning setup, we assume a MDP\nM = (S, A, p) without any reward function r, retaining the\nprevious deﬁnitions and notations. The objective is to sys-\ntematically acquire diverse set of behaviors using autonomous\nexploration, which can subsequently be used to solve down-\nstream tasks efﬁciently. To this end, a skill space Z is\ndeﬁned such that a behavior z ∈Z is deﬁned by the policy\nπ(a|s, z). To learn these behaviors in a reward-free setting,\nthe information theoretic concept of mutual information is\ngenerally employed. Intuitively, mutual information I(x, y)\nbetween two random variables x, y is high when given x, the\nuncertainty in value of y is low and vice-versa. Formally,\nI(x, y) =\nZ\np(x, y) log p(x, y)\np(x)p(y)dxdy\nDynamics-aware Discovery of Skills (DADS) [52] uses\nthe concept of mutual information to encourage skill dis-\ncovery with predictable consequences. It uses the follow-\ning conditional mutual information formulation to motivate\nthe algorithm: I(s′, z\n|\ns) where s′ denotes the next\nstate observed after executing the behavior z from the\nstate s. The joint distribution can be factorized as follows:\np(z, s, s′) = p(z)p(s | z)p(s′ | s, z), where p(z) denotes the\nprior distribution over Z, p(s | z) denotes the stationary\ndistribution induced by π(a | s, z) under the MDP M and\np(s′ | s, z) =\nR\np(s′ | s, a)π(a | s, z)da denotes the transition\ndynamics. The conditional mutual information can be written\nas\nI(s′, z | s) =\nZ\np(z, s, s′) log p(s′ | s, z)\np(s′ | s) dzdsds′\nAt a high level, optimizing for I(s′, z\n| s) encourages\nπ to generate trajectories such that s′ can be determined\nfrom s, z (predictability) and simultaneously encourages π to\ngenerate trajectories where s′ cannot be determined well from\ns without z (diversity). Note, computing I is intractable due\nto intractability of p(s′ | s, z) and p(s′ | s). However, one can\nmotivate the following reinforcement learning maximization\nfor π using variational inequalities and approximations as\ndiscussed in [52]:\nJ(π) = Ez,s,s′∼p(z,s,s′)[r(s, z, s′)]\nr(s, z, s′) =\nqφ(s′ | s, z)\nPL\ni=1 qφ(s′ | s, zi)\n+ log L\nfor {zi}L\ni=1 ∼p(z) where qφ maximizes\nJ(qφ) = Ez,s,s′∼p(z,s,s′)[log qφ(s′ | s, z)]\nSharma et al. [52] propose an on-policy alternating optimiza-\ntion: At iteration t, collect a batch B(t) of trajectories from\nthe current policy π(t) to simulate samples from p(z, s, s′),\nupdate q(t)\nφ\n→q(t+1)\nφ\non B(t) using stochastic gradient descent\nto approximately maximize J(qφ), label the transitions with\nreward r(t+1)(s, z, s′) and update π(t) →π(t+1) on B(t)\nusing any reinforcement learning algorithm to approximately\nmaximize J(π). Note, the optimization encourages the policy\nπ to produce behaviors predictable under qφ(s′|s, z), while\nrewarding the policy for producing diverse behaviors for\ndifferent z ∈Z. This can be seen from the deﬁnition of\nr(s, z, s′): The numerator will be high when the transition\ns →s′ has a high log probability under the current skill\nz (high qφ(s′ | s, z) implies high predictability), while the\ndenominator will be lower if the transition has low probability\nunder zi (low qφ(s′ | s, zi) implies qφ is expecting a different\ntransition under the skill zi).\nInterestingly, the variational approximation qφ(s′ | s, z),\ncalled skill dynamics, can be used for model-predictive con-\ntrol. Given a reward function at test-time, the sequence of\nskill z ∈Z can be determined online using model-predictive\ncontrol by simulating trajectories using skill dynamics qφ.\nIV. TOWARDS REAL-WORLD UNSUPERVISED LEARNING\nThe broad goal of this section is to motivate and present\nthe algorithmic choices required for accomplishing reward-\nfree reinforcement learning in the real-world. We address\nthe issue of sample-efﬁciency of learning algorithms, which\nis the main bottleneck to running the current unsupervised\nlearning algorithms in the real-world. In the same vein, an\nasynchronous data-collection setup with multiple actors can\nsubstantially accelerate the real-world execution. We exploit\nthe off-policy learning enterprise to demonstrate unsupervised\nlearning in the real world, which allows for both sample-\nefﬁcient and asynchronous data collection through multiple\nactors [26].\nA. Off-Policy Training of DADS\nWe develop the off-policy variant of DADS, which we\ncall off-DADS. For clarity, we can restate J(π) in the more\nconventional form of expected discounted sum of rewards.\nUsing the deﬁnition of the stationary distribution p(s | z) =\nPT\nt=0 γtp(st = s | z) for a γ-discounted episodic setting of\nhorizon T, we can write:\nJ(π) = E[\nT −1\nX\nt=0\nγtr(st, z, st+1)]\nwhere the expectation has been taken with respect to trajec-\ntories generated by π(a | s, z) for z ∼p(z). This has been\nexplicitly shown in Appendix A. Now, we can write the Q-\nvalue function as\nQπ(st, at) = E[\nX\ni≥t\nγi−tr(st, z, st+1)]\nFor problems with a ﬁxed reward function, we can use off-\nthe-shelf off-policy reinforcement learning algorithms like soft\nactor-critic [20, 21] or deep deterministic policy gradient [35].\nAt a high level, we use the current policy π(t) to sample a\nsequence of transitions from the environment and add it to the\nreplay buffer R. We uniformly sample a batch of transitions\nB(t) = {(si, zi, ai, s′\ni)}B\ni=0 from R and use it to update π(t)\nand Qπ(t).\nHowever, in this setup: (a) the reward is non-stationary as\nr(s, z, s′) depends upon qφ, which is learned simultaneously\nto π, Qπ and (b) learning qφ involves maximizing J(qφ) which\nimplicitly relies on the current policy π and the induced\nstationary distribution p(s | z). For (a), we recompute the\nreward r(s, z, s′) for the batch B(t) using the current iterate\nq(t)\nφ . For (b), we propose two alternative methods:\n• We use samples from current policy π(t) to maximize\nJ(qφ). While this does not introduce any additional bias,\nit does not take advantage of the off-policy data available\nin the replay buffer.\n• Reuse off-policy data while maximizing J(qφ).\nTo re-use off policy data for learning qφ, we have to consider\nimportance sampling corrections, as the data has been sampled\nfrom a different distribution. While we can derive an unbiased\ngradient estimator, as discussed in Appendix B, we motivate\nAlgorithm 1: Asynchronous off-DADS\nInitialize parameters π(a | s, z), qφ(s′ | s, z);\nInitialize replay buffer R;\n// collector threads\nwhile not done do\nSync πc ←π ;\nSample z ∼p(z);\nCollect episode using πc; // store πc(a | s, z)\nend\n// training thread\ns ←0, n ←0;\nwhile not done do\nwhile n < s + newsteps do\nSync B; n+ = size(B); // get data from actors\nR ←R ∪B;\nend\nfor i ←1 to Tq do\nSample {sj, zj, aj, s′\nj, πc(aj | sj, zj)}Bq\nj=1 ∼R;\nwj ←clip( π(aj|sj,zj)\nπc(aj|sj,zj), 1\nα, α);\nUpdate qφ using {sj, zj, s′\nj, wj)}Bq\nj=1;\nend\nfor i ←1 to Tπ do\nSample {sj, zj, aj, s′\nj, πc(aj | sj, zj)}Bπ\nj=1 ∼R;\nrj ←r(sj, zj, s′\nj); // DADS reward\nUpdate π using {sj, zj, aj, s′\nj, rj)}Bπ\nj=1;\nend\ns ←n;\nend\nan alternate estimator which is simpler and more stable nu-\nmerically, albeit biased. Consider the deﬁnition of J(qφ):\nJ(qφ) = Ez,s,s′∼p\n\u0002\nlog qφ(s′ | s, z)\n\u0003\n=\nZ\np(z)p(s | z)p(s′ | s, z) log qφ(s′ | s, z)dzdsds′\n=\nZ\np(z)p(s | z)π(a | s, z)p(s′ | s, a)\nlog qφ(s′ | s, z)dzdsdads′\nwhere\nwe\nhave\nused\nthe\nfact\nthat\np(s′ | s, z) =\nR\np(s′ | s, a)π(a | s, z)da. Now, consider that\nthe samples have been generated by a behavior policy πc(a |\ns, z). The corresponding generating distribution can be written\nas:\npπc(z, s, s′) =\nR\np(z)pc(s | z)π(a | s, z)p(s′ | s, a)da,\nwhere the prior p(z) over Z and the dynamics p(s′ | s, a) are\nshared across all policies, and pc(s | z) denotes the stationary\nstate distribution induced by πc. We can rewrite J(qφ) as\nJ(qφ) =\nZ\np(z)pc(s | z)πc(a | s, z)p(s′ | s, a)\np(s | z)π(a | s, z)\npc(s | z)πc(a | s, z) log qφ(s′ | s, z)dzdsdads′\nwhich is equivalent to\nJ(qφ) = Ez,s,a,s′∼pπc\nh p(s | z)π(a | s, z)\npc(s | z)πc(a | s, z) log qφ(s′ | s, z)\ni\nThus, the gradient for J(qφ) with respect to φ can be written\nas:\n∇φJ(qφ) = E\nh p(s | z)π(a | s, z)\npc(s | z)πc(a | s, z)∇φ log qφ(s′ | s, z)\ni\n≈1\nBq\nBq\nX\ni=1\nh π(ai | si, zi)\nπc(ai | si, zi)∇φ log qφ(s′\ni | si, zi)\ni\nThe estimator is biased because we compute the importance\nsampling correction as wi = clip( π(ai|si,zi)\nπc(ai|si,zi), 1\nα, α) which\nignores the intractable state-distribution correction p(s|z)\npc(s|z) [18].\nThis considerably simpliﬁes the estimator while keeping the\nestimator numerically stable (enhanced by clipping) as com-\npared to the unbiased estimator derived in Appendix B. In\ncontext of off-policy learning, the bias due to state-distribution\nshift can be reduced using a shorter replay buffer.\nOur ﬁnal proposed algorithm is summarized in the Algo-\nrithm 1. At a high level, we use n actors in the environment\nwhich use the latest copy of the policy to collect episodic data.\nThe centralized training script keeps adding new episodes to\nthe shared replay buffer R. When a certain threshold of new\nexperience has been added to R, the buffer is uniformly sam-\npled to train qφ to maximize J(qφ). To update π, we sample\nthe buffer uniformly again and compute r(s, z, s′) for all the\ntransitions using the latest qφ. The labelled transitions can\nthen be passed to any off-the-shelf off-policy reinforcement\nlearning algorithm to update π and Qπ.\nV. EXPERIMENTS\nIn this section, we experimentally evaluate our robotic\nlearning method, off-DADS, for unsupervised skill discovery.\nFirst, we evaluate the off-DADS algorithm itself in isolation,\non a set of standard benchmark tasks, to understand the gains\nin sample efﬁciency when compared to DADS proposed in\n[52], while ablating the role of hyperparameters and variants\nof off-DADS. Then, we evaluate our robotic learning method\non D’Kitty from ROBEL [3], a real-world robotic benchmark\nsuite. We also provide preliminary results on D’Claw from\nROBEL, a manipulation oriented robotic setup in Appendix D.\nA. Benchmarking off-DADS\nWe benchmark off-DADS and its variants on continuous\ncontrol environments from OpenAI gym [7], similar to [52].\nWe use the HalfCheetah, Ant, and Humanoid environments,\nwith state-dimensionality 18, 29, and 47 respectively. We also\nconsider the setting where the skill-dynamics only observes the\nglobal x, y coordinates of the Ant. This encourages the agent\nto discover skills which diversify in the x, y space, yielding\nskills which are more suited for locomotion [13, 52].\nTo evaluate the performance of off-DADS and the role of\nhyperparameters, we consider the following variantions:\n• Replay Buffer Size: We consider two sizes for the replay\nbuffer R: 10,000 (s) and 1,000,000 (l). As alluded to,\n(a) Half-Cheetah\n(b) Ant\n(c) Ant (x-y)\n(d) Humanoid\nFig. 2.\nSample efﬁciency comparison of off-DADS with DADS\n(red). We control the effect of state-distribution shift using length\nof replay buffers (s implies short and l implies long replay buffer)\nand importance sampling corrections (1 and 10 being the values of\nthe clipping parameter). We observe that all variants of off-DADS\noutperforms DADS in terms of sample efﬁciency, and using a short\nreplay buffer with importance sampling clip parameter set to 10\nconsistently gives the best performance.\nthis controls how on-policy the algorithm is. A smaller\nreplay buffer will have lower bias due to state-distribution\nshift, but can lose sample efﬁciency as it discards samples\nfaster [18].\n• Importance Sampling: We consider two settings for the\nclipping parameter in the importance sampling correction:\nα = 1 and α = 10. The former implies that there\nis no correction as all the weights are clipped to 1.\nThis helps evaluate whether the suggested importance\nsampling correction gives any gains in terms of sample\nefﬁciency.\nThis gives us four variants abbreviated as s1, s10, l1 and\nl10. We also evaluate against the off-DADS variant where\nthe skill-dynamics is trained on on-policy samples from the\ncurrent policy. This helps us evaluate whether training skill-\ndynamics on off-policy data can beneﬁt the sample efﬁciency\nof off-DADS. Note, while this ablation helps us understand\nthe algorithm, this scheme would be wasteful of samples in\nasynchronous off-policy real world training, where the data\nfrom different actors could potentially be coming from differ-\nent (older) policies. Finally, we benchmark against the baseline\nDADS, as formulated in [52]. The exact hyperparameters for\neach of the variants are listed in Appendix C. We record\ncurves for ﬁve random seeds for the average intrinsic reward\nr(s, z, s′) as a function of samples from the environment and\nreport the average curves in Figure 2.\nWe observe that all variants of off-DADS consistently out-\nperform the on-policy baseline DADS on all the environments.\nThe gain in sample efﬁciency can be as high as four times,\nas is the case for Ant (x-y) environment where DADS takes\n16 million samples to converge to the same levels as shown\nfor off-DADS (about 0.8 average intrinsic reward). We also\nnote that irrespective of the size of the replay buffer, the\nimportance sampling correction with α = 10 outperforms or\nmatches α = 1 on all environments. This positively indicates\nthat the devised importance sampling correction makes a better\nbias-variance trade-off than no importance sampling. The best\nperforming variant on every environment except Ant (x-y) is\nthe s10. While training skill-dynamics on-policy provides a\ncompetitive baseline, the short replay buffer and the clipped\nimportance sampling counteract the distribution shift enough\nto beneﬁt the overall sample efﬁciency of the algorithm.\nInterestingly on Ant (x-y), the best performing variant is l10.\nThe long replay buffer variants are slower than the short replay\nbuffer variants but reach a higher average intrinsic reward. This\ncan be attributed to the smaller state-space for skill-dynamics\n(only 2-dimensional) and thus, the state-distribution correction\nrequired is potentially negligible but at the same time the off-\npolicy data is helping learn better policies.\nB. Real-world Training\nWe now demonstrate the off-DADS can be deployed for\nreal world reward-free reinforcement learning. To this end, we\nchoose the ROBEL benchmark [3]. In particular, we deploy\noff-DADS on D’Kitty shown in the Figure 3. D’Kitty is a\n12 DOF compact quadruped capable of executing diverse\ngaits. We also provide preliminary results for D’Claw, a\nmanipulation-oriented setup from ROBEL in Appendix D.\nFig. 3.\n(Left) D’Kitty robot from the ROBEL benchmark. (Right)\nD’Kitty with the LED conﬁguration for PhaseSpace tracking.\nC. D’Kitty Experimental Setup\nTo run real-world training, we constructed a walled 4m×4m\ncordoned area, shown in Figure 4. The area is equipped\nwith 5 PhaseSpace Impulse X2 cameras that are equidistantly\nmounted along two bounding orthogonal walls. These cameras\nare connected to a PhaseSpace Impulse X2E motion capture\nsystem which performs 6 DOF rigid body tracking of the\nD’Kitty robots’ chassis at 480Hz. We use two D’Kitty robots\nfor data collection and training in experiment. Each D’Kitty,\nwe attach one PhaseSpace LED controller which controls 8\nactive LED markers that are attached to the top surface of the\nD’Kitty chassis as shown in Figure 3. Each D’Kitty is tethered\nFig. 4.\nTwo quadrupeds in a cordoned area. The LEDs allow the\nrobots and the object to be tracked using the PhaseSpace cameras.\nvia 3 cables: USB serial to the computer running off-DADS,\n12V/20A power to the D’Kitty robot, and USB power to the\nLED controller. To reduce wire entanglement with the robot,\nwe also have an overhead support for the tethering wires.\nDynamixel Property\nValue\nModel\nXM-W210\nControl Mode\nPosition Control\nBaudrate\n1 Mbps\nPWM Limit\n450 (50.85%)\nVoltage Range\n9.5V to 16V\nFig. 5.\nDynamixel motor conﬁguration for the D’Kitty robots.\nD. Algorithmic Details\nWe ﬁrst test the off-DADS algorithm variants in simulation.\nFor the D’Kitty observation space, we use the Cartesian posi-\ntion and Euler orientation (3 + 3), joint angles and velocities\n(12 + 12), the last action executed by the D’Kitty (12) and the\nupright (1), which is the cosine of the orientation with global\nz-axis. The concatenated observation space is 43-dimensional.\nHyperparameter details for off-DADS (common to all variants)\nare as follows: The skill space Z is 2D with support over\n[−1, 1]2. We use a uniform prior p(z) over Z. We parameterize\nπ(a | s, z), Qπ(s, a, z) and qφ(s′ | s, z) using neural networks\nwith two hidden layers of size 512. The output of π(a | s, z)\nis parameterized by a normal distribution N(µ, Σ) with a\ndiagonal covariance which is scaled to [−1, 1] using tanh\ntransformation. For qφ, we reduce the observation space to the\nD’Kitty co-ordinates (x, y). This encourages skill-discovery\nfor locomotion behaviors [52, 13]. We parameterize qφ to\npredict ∆s = s′ −s, a general trick in model-based control\nwhich does not cause any loss in representational power as\nthe next state can be recovered by adding the prediction to the\ncurrent state. We use soft-actor critic [20] to optimize π, Qπ.\nTo learn qφ, we sample batches of size 256 and use the Adam\noptimizer with a ﬁxed learning rate of 0.0003 for Tq = 8\nsteps. For soft-actor critic, we again use Adam optimizer with\na ﬁxed learning rate of 0.0003 while sampling batches of size\n256 for 128 steps. Discount factor γ = 0.99, with a ﬁxed\nentropy coefﬁcient of 0.1. For computing the DADS reward\nr(s, z, s′), we set L = 100 samples from the prior p(z). We\nset the episode length to be 200, which terminates prematurely\nif the upright coefﬁcient falls below 0.9 (that is the D’Kitty is\ntilting more than 25 degrees from the global z-axis).\nFig. 6.\n(Left) Training curves for D’Kitty in both simulation and\nreal-world. We ﬁnd the off-DADS with a short replay and importance\nsampling clipping parameter α = 10 to be the most suitable for the\nreal-world learning. We ﬁnd the real-world learning curve closely\nfollows the simulation learning curve.\nIn terms of off-DADS variants, we evaluate the four variants\ndiscussed in the previous section. For all the variants, we\ncollect at least 500 steps in the simulation before updating\nqφ and π. The observations for the variants resemble those\nof the Ant (x-y) environment. We observe that the variants\nwith a replay buffer of size 10, 000 are much faster to learn\nthan the replay buffer of size 1, 000, 000. Asymptotically, we\nobserve the long replay buffer outperforms the short replay\nbuffer though. We also observe setting α = 10 beneﬁts the\ncause of sample efﬁciency.\nFor the real robotic experiment, we choose the hyperpa-\nrameters R to be of size 10, 000 and we set α = 10.\nWhile asymptotically better performance is nice, we prioritized\nsample efﬁciency. For the real experiment, we slightly modify\nthe collection condition. For every update of qφ and π, we\nensure there are 200 new steps and at least 3 new episodes in\nthe replay buffer R.\nE. Emergent Locomotion Behaviors\nWith the setup and hyperparameters described in the previ-\nous sections, we run the real-world experiment. The experi-\nment was ran over 3 days, with the effective training time on\nthe robot being 20 hours (including time spent in maintaining\nthe hardware). We collected around 300, 000 samples in total\nas shown in the learning curve in Figure 6. We capture\nthe emergence of locomotion skills in our video supplement.\nFigure 1 and Figure 7 show some of the diversity which\nemerges in skills learned by D’Kitty using off-DADS, in terms\nof orientation and gaits.\nFig. 7.\nDiverse gaits learned by the D’Kitty in our real world\nexperiments.\nBroadly, the learning occurs in the following steps: (a)\nD’Kitty ﬁrst tries to learn how to stay upright to prolong\nthe length of the episode. This happens within the ﬁrst hour\nof the episode. (b) It spends the next few hours trying to\nmove around while trying to stay upright. These few hours,\nthe movements are most random and the intrinsic reward is\nrelatively low as they do not correlate well with z. (c) About\n5-6 hours into the training, it starts showing a systematic gait\nwhich it uses to move in relatively random directions. This\nis when the intrinsic reward starts to rise. (d) A few more\nhours of training and this gait is exploited to predictably move\nin speciﬁc directions. At this point the reward starts rising\nrapidly as it starts diversifying the directions the agent can\nmove in predictably. Interestingly, D’Kitty uses two different\ngaits to capture and grow in two different directions of motion,\nwhich can be seen in the video supplement. (e) At about 16\nhours of training, it can reliably move in different directions\nand it is trying to further increase the directions it can\nmove in predictably. Supplementary videos are available here:\nhttps://sites.google.com/view/dads-skill\nOne interesting difference from simulation where the\nD’Kitty is unconstrained, is that the harnesses and tethering\ndespite best attempts restrain the movement of the real robot\nto some extent. This encourages the agent to invest in multiple\ngaits and use simpler, more reliable motions to move in\ndifferent directions.\nF. Challenges in real-world training\nWe discuss some of the challenges encountered during\nreal-world reinforcement learning, particularly in context of\nlocomotive agents.\n• Reset & Autonomous operation: A good initial state\ndistribution is necessary for the exploration to proceed\ntowards the desirable state distribution. In context of\nlocomotion, a good reset comprises of being in an upright\nposition and relocating away from the extremities of the\narea. For the former, we tried two reset mechanisms: (a)\nscripted mechanism, which is shown in the supplemen-\ntary video and (b) reset detector which would continue\ntraining if the D’Kitty was upright (based on height\nand tilt with z-axis), else would wait (for human to\nreset). However, (a) being programmatic is not robust and\ndoes not necessarily succeed in every conﬁguration, in\naddition to being slow. (b) can be really fast considering\nthat D’Kitty is reasonably compact, but requires human\noversight. Despite human oversight, the reset detector\ncan falsely assume the reset is complete and initiate the\nepisode, which requires an episode ﬁlter to be written.\nRelocating from the extremities back to the center is\na harder challenge. It is important because the tracker\nbecomes noisy in those regions while also curbing the\nexploration of the policy. However, this problem only\narises when the agent shows signiﬁcant skills to navigate.\nThere are other challenges besides reset which mandate\nhuman oversight into the operation. Primarily, random\nexploration can be tough on the robot, requiring mainte-\nnance in terms of tightening of screws and sometimes,\nreplacing motors. We found latter can be avoided by\nkeeping the motor PWMs low (450 is good). While we\nmake progress towards reward-free learning in this work,\nwe leave it to future work to resolve problems on the way\nto fully autonomous learning.\n• Constrained space: For the full diversity of skills to\nemerge, an ideal operation would have unconstrained\nspace with accurate tracking. However, realistically that\nis infeasible. Moreover, real operation adds unmodelled\nconstraints over simulation environments. For example,\nthe use of harness to reduce wire entanglement with\nthe robot adds tension depending on the location. When\noperating multiple D’Kitties in the same space, there can\nbe collisions during the operation. Likelihood of such\nan event progressively grows through the training. About\ntwo-ﬁfths through the training, we started collecting with\nonly one D’Kitty in the space to avoid future collisions.\nHalfway through the training, we decided to expand the\narea to its limits and re-calibrate our tracking system for\nthe larger area. Despite the expansion, we were still short\non space for the operation of just one D’Kitty. To remedy\nthe situation, we started decreasing the episode length.\nWe went from 200 steps to 100, then to 80 and then\nto 60. However, we observed that short episodes started\naffecting the training curve adversarially (at about 300k\nsamples). Constrained by the physical limits, we ﬁnished\nthe training. While a reasonable skill diversity already\nemerges in terms of gaits and orientations within the\ntraining we conduct, as shown in Figure 1, more skills\nshould be discovered with more training (as suggested\nby the simulation results as well as the fact the reward\ncurve has not converged). Nonetheless, we made progress\ntowards the conveying the larger point of reward-free\nlearning being realized in real-world.\nDistance Travelled\n2.17 ± 0.59\nPercentage of Falls\n5%\nFig. 8.\nDistance (in m) travelled by skills sampled randomly from\nthe prior in 100 steps, which is 10s of execution time. We also report\nthe number of times D’Kitty falls before completing 100 steps. The\nresults have been averaged over 20 trials.\nFig. 9.\nNavigation via model-predictive-control over skills learned\nby off-DADS. Using the skill-dynamics learned in the unsupervised\ntraining, the planner composes skills to move towards the goal. The\ntrajectories visualized show movement of D’Kitty towards the goal\nbox, marked by a set of LED markers.\nG. Harnessing Learned Skills\nQualitatively, we see that a diverse set of locomotive skills\ncan emerge from reward-free training. However, as has been\ndiscussed in [52], these skills can be harnessed for downstream\ntasks using model-based control on the learned skill-dynamics\nqφ(s′ | s, z). First, we partly quantify the learned skills in\nFigure 8. We execute skills randomly sampled from the prior\nand collect statistics for these runs. In particular, we ﬁnd\nthat despite limited training, the skills are relatively robust\nand fall in only 5% of the runs, despite being proﬁcient in\ncovering distance. Interestingly, the learned skills can also be\nharnessed for model-based control as shown in Figure 9. The\ndetails for model-predictive control follow directly from [52],\nwhich elucidates on how to to do so using skill-dynamics qφ\nand π. We have included video supplements showing model-\npredictive control in the learned skill space for goal navigation.\nVI. CONCLUSION\nIn this work, we derived off-DADS, a novel off-policy\nvariant to mutual-information-based reward-free reinforcement\nlearning framework. The improved sample-efﬁciency from off-\npolicy learning enabled the algorithm to be applied on a\nreal hardware, a quadruped with 12 DoFs, to learn various\nlocomotion gaits under 20 hours without human-design reward\nfunctions or hard-coded primitives. Given our dynamics-based\nformulation from [52], we further demonstrate those acquired\nskills are directly useful for solving downstream tasks such\nas navigation using online planning with no further learning.\nWe detail the successes and challenges encountered in our\nexperiments, and hope our work could offer an important\nfoundation toward the goal of unsupervised, continual rein-\nforcement learning of robots in the real world for many days\nwith zero human intervention.\nREFERENCES\n[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learn-\ning via inverse reinforcement learning. In Proceedings\nof the twenty-ﬁrst international conference on Machine\nlearning, page 1, 2004.\n[2] Joshua Achiam, Harrison Edwards, Dario Amodei, and\nPieter Abbeel. Variational option discovery algorithms.\narXiv preprint arXiv:1807.10299, 2018.\n[3] Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo\nPonte, Abhishek Gupta, Sergey Levine, and Vikash Ku-\nmar. Robel: Robotics benchmarks for learning with low-\ncost robots. arXiv preprint arXiv:1909.11639, 2019.\n[4] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The\noption-critic architecture. In Thirty-First AAAI Confer-\nence on Artiﬁcial Intelligence, 2017.\n[5] Adrien Baranes and Pierre-Yves Oudeyer. Active learn-\ning of inverse models with intrinsically motivated goal\nexploration in robots. Robotics and Autonomous Systems,\n61(1):49–73, 2013.\n[6] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski,\nTom Schaul, David Saxton, and Remi Munos. Unifying\ncount-based exploration and intrinsic motivation. In Ad-\nvances in neural information processing systems, pages\n1471–1479, 2016.\n[7] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas\nSchneider, John Schulman, Jie Tang, and Wojciech\nZaremba. Openai gym, 2016.\n[8] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,\nShane Legg, and Dario Amodei.\nDeep reinforcement\nlearning from human preferences. In Advances in Neural\nInformation Processing Systems, pages 4299–4307, 2017.\n[9] C´edric Colas, Pierre Fournier, Olivier Sigaud, Mohamed\nChetouani, and Pierre-Yves Oudeyer. Curious: intrinsi-\ncally motivated modular multi-goal reinforcement learn-\ning. arXiv preprint arXiv:1810.06284, 2018.\n[10] Christian Daniel, Gerhard Neumann, and Jan Peters.\nHierarchical relative entropy policy search. In Artiﬁcial\nIntelligence and Statistics, pages 273–281, 2012.\n[11] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hes-\nter.\nChallenges of real-world reinforcement learning.\narXiv preprint arXiv:1904.12901, 2019.\n[12] Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and\nSergey Levine. Leave no trace: Learning to reset for safe\nand autonomous reinforcement learning.\nInternational\nConference on Learning Representations (ICLR), 2018.\n[13] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz,\nand Sergey Levine.\nDiversity is all you need: Learn-\ning skills without a reward function.\narXiv preprint\narXiv:1802.06070, 2018.\n[14] Carlos Florensa, Yan Duan, and Pieter Abbeel. Stochastic\nneural networks for hierarchical reinforcement learning.\narXiv preprint arXiv:1704.03012, 2017.\n[15] Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and\nShixiang Gu.\nA divergence minimization perspective\non imitation learning methods.\nConference on Robot\nLearning (CoRL), 2019.\n[16] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and\nSergey Levine. Continuous deep q-learning with model-\nbased acceleration. In International Conference on Ma-\nchine Learning, pages 2829–2838, 2016.\n[17] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey\nLevine.\nDeep reinforcement learning for robotic ma-\nnipulation with asynchronous off-policy updates.\nIn\n2017 IEEE international conference on robotics and\nautomation (ICRA), pages 3389–3396. IEEE, 2017.\n[18] Shixiang Shane Gu, Timothy Lillicrap, Richard E Turner,\nZoubin Ghahramani, Bernhard Sch¨olkopf, and Sergey\nLevine. Interpolated policy gradient: Merging on-policy\nand off-policy gradient estimation for deep reinforcement\nlearning. In Advances in neural information processing\nsystems, pages 3846–3855, 2017.\n[19] Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan,\nGeorge Tucker, and Sergey Levine.\nLearning to\nwalk via deep reinforcement learning.\narXiv preprint\narXiv:1812.11103, 2018.\n[20] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and\nSergey Levine.\nSoft actor-critic: Off-policy maximum\nentropy deep reinforcement learning with a stochastic\nactor. arXiv preprint arXiv:1801.01290, 2018.\n[21] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen,\nGeorge Tucker, Sehoon Ha, Jie Tan, Vikash Kumar,\nHenry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft\nactor-critic algorithms and applications. arXiv preprint\narXiv:1812.05905, 2018.\n[22] Dylan Hadﬁeld-Menell, Smitha Milli, Pieter Abbeel, Stu-\nart J Russell, and Anca Dragan. Inverse reward design.\nIn Advances in neural information processing systems,\npages 6765–6774, 2017.\n[23] Elad Hazan, Sham M Kakade, Karan Singh, and Abby\nVan Soest. Provably efﬁcient maximum entropy explo-\nration. arXiv preprint arXiv:1812.02690, 2018.\n[24] Nan Jiang and Lihong Li. Doubly robust off-policy value\nevaluation for reinforcement learning.\narXiv preprint\narXiv:1511.03722, 2015.\n[25] Tobias Jung, Daniel Polani, and Peter Stone. Empower-\nment for continuous agentenvironment systems. Adaptive\nBehavior, 19(1):16–39, 2011.\n[26] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian\nIbarz, Alexander Herzog, Eric Jang, Deirdre Quillen,\nEthan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke,\net al.\nQt-opt: Scalable deep reinforcement learning\nfor vision-based robotic manipulation.\narXiv preprint\narXiv:1806.10293, 2018.\n[27] Alexander S Klyubin, Daniel Polani, and Chrystopher L\nNehaniv.\nAll else being equal be empowered.\nIn\nEuropean Conference on Artiﬁcial Life, pages 744–753.\nSpringer, 2005.\n[28] Jens Kober and Jan R Peters. Policy search for motor\nprimitives in robotics. In Advances in neural information\nprocessing systems, pages 849–856, 2009.\n[29] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforce-\nment learning in robotics: A survey. The International\nJournal of Robotics Research, 32(11):1238–1274, 2013.\n[30] Nate Kohl and Peter Stone. Policy gradient reinforcement\nlearning for fast quadrupedal locomotion.\nIn IEEE\nInternational Conference on Robotics and Automation,\n2004. Proceedings. ICRA’04. 2004, volume 3, pages\n2619–2624. IEEE, 2004.\n[31] George Konidaris, Scott Kuindersma, Roderic Grupen,\nand Andrew Barto. Autonomous skill acquisition on a\nmobile manipulator. In Twenty-Fifth AAAI Conference\non Artiﬁcial Intelligence, 2011.\n[32] Vikash Kumar, Emanuel Todorov, and Sergey Levine.\nOptimal control with learned local models: Application\nto dexterous manipulation. In 2016 IEEE International\nConference on Robotics and Automation (ICRA), pages\n378–383. IEEE, 2016.\n[33] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric\nXing, Sergey Levine, and Ruslan Salakhutdinov. Efﬁcient\nexploration via state marginal matching. arXiv preprint\narXiv:1906.05274, 2019.\n[34] Andrew Levy, George Konidaris, Robert Platt, and Kate\nSaenko. Learning multi-level hierarchies with hindsight.\narXiv preprint arXiv:1712.00948, 2017.\n[35] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel,\nNicolas Heess, Tom Erez, Yuval Tassa, David Silver, and\nDaan Wierstra. Continuous control with deep reinforce-\nment learning. arXiv preprint arXiv:1509.02971, 2015.\n[36] A Rupam Mahmood, Dmytro Korenkevych, Gautham\nVasan, William Ma, and James Bergstra. Benchmarking\nreinforcement learning algorithms on real-world robots.\narXiv preprint arXiv:1809.07731, 2018.\n[37] Volodymyr Mnih, Koray Kavukcuoglu, David Silver,\nAlex Graves, Ioannis Antonoglou, Daan Wierstra, and\nMartin Riedmiller. Playing atari with deep reinforcement\nlearning. arXiv preprint arXiv:1312.5602, 2013.\n[38] Shakir Mohamed and Danilo Jimenez Rezende.\nVari-\national information maximisation for intrinsically mo-\ntivated reinforcement learning.\nIn Advances in neural\ninformation processing systems, pages 2125–2133, 2015.\n[39] R´emi Munos, Tom Stepleton, Anna Harutyunyan, and\nMarc Bellemare.\nSafe and efﬁcient off-policy rein-\nforcement learning. In Advances in Neural Information\nProcessing Systems, pages 1054–1062, 2016.\n[40] Oﬁr Nachum, Shixiang Shane Gu, Honglak Lee, and\nSergey Levine. Data-efﬁcient hierarchical reinforcement\nlearning. In Advances in Neural Information Processing\nSystems, pages 3303–3313, 2018.\n[41] Anusha Nagabandi, Kurt Konoglie, Sergey Levine, and\nVikash Kumar. Deep dynamics models for learning dex-\nterous manipulation. arXiv preprint arXiv:1909.11652,\n2019.\n[42] Andrew Y Ng, Stuart J Russell, et al. Algorithms for\ninverse reinforcement learning. In Icml, volume 1, pages\n663–670, 2000.\n[43] Pierre-Yves Oudeyer and Frederic Kaplan.\nWhat is\nintrinsic motivation? a typology of computational ap-\nproaches. Frontiers in neurorobotics, 1:6, 2009.\n[44] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and\nTrevor Darrell.\nCuriosity-driven exploration by self-\nsupervised prediction.\nIn Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\nWorkshops, pages 16–17, 2017.\n[45] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and\nTrevor Darrell.\nCuriosity-driven exploration by self-\nsupervised prediction. In ICML, 2017.\n[46] Vitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin\nNair, Shikhar Bahl, and Sergey Levine. Skew-ﬁt: State-\ncovering self-supervised reinforcement learning. arXiv\npreprint arXiv:1903.03698, 2019.\n[47] Doina Precup.\nEligibility traces for off-policy policy\nevaluation. Computer Science Department Faculty Pub-\nlication Series, page 80, 2000.\n[48] Martin Riedmiller. Neural ﬁtted q iteration–ﬁrst experi-\nences with a data efﬁcient neural reinforcement learning\nmethod. In European Conference on Machine Learning,\npages 317–328. Springer, 2005.\n[49] Martin Riedmiller, Thomas Gabel, Roland Hafner, and\nSascha Lange. Reinforcement learning for robot soccer.\nAutonomous Robots, 27(1):55–73, 2009.\n[50] J¨urgen Schmidhuber.\nCurious model-building control\nsystems.\nIn Proc. international joint conference on\nneural networks, pages 1458–1463, 1991.\n[51] J¨urgen Schmidhuber. Formal theory of creativity, fun,\nand intrinsic motivation (1990–2010).\nIEEE Transac-\ntions on Autonomous Mental Development, 2(3):230–\n247, 2010.\n[52] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Ku-\nmar, and Karol Hausman. Dynamics-aware unsupervised\ndiscovery of skills. International Conference on Learning\nRepresentations (ICLR), 2020.\n[53] Martin Stolle and Doina Precup.\nLearning options in\nreinforcement learning.\nIn International Symposium\non abstraction, reformulation, and approximation, pages\n212–223. Springer, 2002.\n[54] Richard S Sutton, Andrew G Barto, et al. Introduction\nto reinforcement learning, volume 135.\nMIT press\nCambridge, 1998.\n[55] Richard S Sutton, Doina Precup, and Satinder Singh.\nBetween mdps and semi-mdps: A framework for tem-\nporal abstraction in reinforcement learning.\nArtiﬁcial\nintelligence, 112(1-2):181–211, 1999.\n[56] Philip Thomas and Emma Brunskill. Data-efﬁcient off-\npolicy policy evaluation for reinforcement learning. In\nInternational Conference on Machine Learning, pages\n2139–2148, 2016.\n[57] Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah,\nKristian Hartikainen, Avi Singh, Vikash Kumar, and\nSergey Levine.\nThe ingredients of real world robotic\nreinforcement learning.\nInternational Conference on\nRepresentation Learning (ICLR), 2020.\n[58] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and\nAnind K Dey. Maximum entropy inverse reinforcement\nlearning. In Aaai, volume 8, pages 1433–1438. Chicago,\nIL, USA, 2008.\nAPPENDIX\nA. Expanding the stationary distribution\nRecall that the stationary distribution can be written as\np(s | z) =\nT\nX\nt=0\nγtp(st = s | z)\nfor a γ-discounted episodic setting of horizon T. Here,\np(st | z) =\nZ\np0(s0)\nt−1\nY\ni=0\nπ(ai | si, z)p(si+1 | si, z)daidsi\nNow, using this in the deﬁnition of J(π), we get\nJ(π) =\nZ\np(z)\nh T −1\nX\nt=0\nγt\nZ\np0(s0)\nt−1\nY\ni=0\nπ(ai | si)\np(si+1 | si, ai)dsidai\ni\np(s′ | s, z)r(s, z, s′)dzdsds′\nNow, we can switch the integral for variables s, s′ and the sum-\nmation over index t, distributing the terms p(s′ | s, z)r(s, z, s′)\nover T in the summation. For each term, we can rename the\nvariable s →st and s′ →st+1. We get,\nJ(π) =\nZ\np(z)\nh T −1\nX\nt=0\nZ\np0(s0)\ntY\ni=0\nπ(ai | si)p(si+1 | si, ai)\nγtr(st, z, st+1)dsidai\ni\ndz\nwhere we have also expanded p(st+1 | st, z) =\nR\np(st+1 |\nst, at)π(at | st, z)dat. Now, consider\n\u0010 Z\np0(s0)\ntY\ni=0\nπ(ai | si)p(si+1 | si, ai)dsidai\n\u0011\n\u0010 Z\nT −1\nY\ni=t+1\nπ(ai | si)p(si+1 | si, ai)dsidai\n\u0011\n=\nZ\np0(s0)\nT −1\nY\ni=0\nπ(ai | si)p(si+1 | si, ai)dsidai\nHowever,\nR QT −1\ni=t+1 π(ai | si)p(si+1 | si, ai)dsidai = 1, as\nintegral of a probability distribution over its support integrates\nto one. Therefore by introducing the dummy variables for the\nleftover trajectory in each term, the probability distribution\nin every term of the sum becomes the same. Exchanging the\nsum and the integral again, we can rewrite J(π) as the desired\nexpectation over all possible trajectories:\nJ(π) =\nZ\np(z)p0(s0)\nT −1\nY\nt=0\nπ(at | st)p(st+1 | st, at)\n\u0010 T −1\nX\nt=0\nγtr(st, z, st+1)\n\u0011\ndzds0da0 . . . dsT\nThis same exercise can be repeated for J(qφ).\nB. Importance sampling corrections for skill dynamics\nTo derive the unbiased off-policy update for qφ, consider the\ndeﬁntion again:\nJ(qφ) =\nZ\np(z)p(s | z)p(s′ | s, z) log qφ(s′ | s, z)dzdsds′\n= Ea|s∼π\n\u0002 T −1\nX\nt=0\nγt log qφ(st+1 | st, z)\n\u0003\nThis results from the expansion of the state distribution p(s |\nz), as discussed in Appendix A. Note, the expectation is taken\nwith respect to trajectories generated by π(a | s, z) for z ∼\np(z), and is contracted for convenience. Assuming the samples\nare generated from a behavior policy πc(a | s, z), we can\nrewrite J(qφ) as follows:\nJ(qφ) = Ea|s∼πc\nh T −1\nX\nt=0\nγt\u0010\ntY\ni=0\nπ(ai | si, z)\nπc(ai | si, z)\n\u0011\nlog qφ(st+1 | st, z)\ni\nThis results from the fact that the state at time t only depends\non actions preceding time t. Note, while this estimator is\nunbiased, it can be extremely unstable computationally as it\nis a product of probabilities which can become diminishingly\nsmall.\nC. Hyperparameters\nThe hyperparameters for all experiments heavily borrow\nfrom those in subsection V-B.\n1) HalfCheetah: We learn a 3D skill space with a uniform\nprior [−1, 1]2. For on-policy DADS, we collect a batch of 2000\nsteps in every iteration. The π, Qπ is trained for 64 steps with a\nbatch size of 256 sampled uniformly from the 2000 steps. The\noff-policy version, has similar hyperparameters, except that π\nsamples from a replay buffer of size 100, 000, while the skill\ndynamics qφ is only trained on the new batch of 2000 steps.\nFor all versions of off-DADS, we collect a fresh 1000 samples\nin every iteration. Rest of the hyperparameters are the same.\n2) Ant: For the Ant environment on the full state-space,\nwe learn a 3D skill space with a uniform prior [−1, 1]2. For\non-policy DADS, we collect a batch of 2000 steps in every\niteration. The π, Qπ is trained for 64 steps with a batch size\nof 256 sampled uniformly from the 2000 steps. The off-policy\nversion, has similar hyperparameters, except that π samples\nfrom a replay buffer of size 100, 000, while the skill dynamics\nqφ is only trained on the new batch of 2000 steps. For all\nversions of off-DADS, we collect a fresh 1000 samples in\nevery iteration. Rest of the hyperparameters are the same.\nFor the Ant environment on the x, y space for the skill\ndynamics, we learn a 2D skill space with a uniform prior\n[−1, 1]2. For on-policy DADS, we collect a batch of 2000\nsteps in every iteration. The π, Qπ is trained for 64 steps with\na batch size of 256 sampled uniformly from the 2000 steps.\nThe off-policy version, has similar hyperparameters, except\nthat π samples from a replay buffer of size 100, 000, while\nthe skill dynamics qφ is only trained on the new batch of\n2000 steps. For all versions of off-DADS, we collect a fresh\n500 samples in every iteration. Rest of the hyperparameters\nare the same.\n3) Humanoid: We learn a 5D space with a uniform prior\n[−1, 1]2. For on-policy DADS, we collect a batch of 4000\nsteps in every iteration. The π, Qπ is trained for 64 steps with\na batch size of 256 sampled uniformly from the 2000 steps.\nThe off-policy version, has similar hyperparameters, except\nthat π samples from a replay buffer of size 100, 000, while\nthe skill dynamics qφ is only trained on the new batch of\n4000 steps. For all versions of off-DADS, we collect a fresh\n2000 samples in every iterations Rest of the hyperparameters\nare the same.\nD. D’Claw (Simulation)\nFig. 10.\nD’Claw from ROBEL [3] is a three-armed manipulatation oriented\nrobotic setup. In this environment, it is provided with a three-pronged valve\nwhich can rotate on its axis. The goal position can be speciﬁed by the motor\nat the top.\nWe also provide additional results for the application of off-\nDADS to the manipulation domains. We choose the D’Claw\nenvironment from ROBEL, shown in Figure 10. D’Claw is\nthree independent arms with 9DOF capable of interacting and\nmanipulating objects. For our setup, we have a three-pronged\nvalve which can rotate about the z-axis. For the observation\nspace in the environment, we use the euler angle of the valve\n(1), joint angles and velocities (9 + 9). The concatenated\nobservation space is 19-dimensional.\nWe provide results for the parallel simulation environment,\nwhich we will extend to hardware in the future. The hyper-\nparameters details are as follows: We have a continuous 1D\nskill space Z with support over [−1, 1]. We use a uniform\nprior p(z) over Z. We parameterize π(a | s, z), Qπ(s, a, z)\nand qφ(s′ | s, z) using neural networks with two hidden layers\nof size 512. The output of π(a | s, z) is parameterized by\na normal distribution N(µ, Σ) with a diagonal covariance\nmatrix. The output of the normal distribution is scaled to\n[−1, 1] using tanh transformation. For qφ, we reduce the\nobservation space to the valve orientation (θ). This encourages\nskill-discovery for behaviors which manipulate the object, and\nnot just randomly move around their arms. We parameterize\nqφ to predict ∆θ = θ′−θ. When predicting the next state using\nθ′ = θ + ∆θ, we map the resulting orientation to (−π, π]. To\nlearn qφ, we sample batches of size 256 and use the Adam\noptimizer with a ﬁxed learning rate of 0.0003 for Tq = 8 steps.\nFor soft-actor critic, we again use Adam optimizer with a ﬁxed\nlearning rate of 0.0003 while sampling batches of size 256\nfor 64 steps. Discount factor γ = 0.99, with a ﬁxed entropy\ncoefﬁcient of 0.1. For computing the DADS reward r(s, z, s′),\nwe set L = 100 samples from the prior p(z). We set the\nepisode length to be 200. We use a replay buffer R of size\n100, 000 and set importance sampling clipping factor α = 1.\nFig. 11. We use off-DADS to enable skill discovery for D’Claw to manipulate\nthe three-pronged valve. The ﬁrst row represents a skill which turns the valve\nclockwise and the second row represents a skill which turns the valve counter-\nclockwise.\nAs we can see in Figure 11, the claw can learn to ma-\nnipulate the valve and provide rotational skills in different\ndirections. These skills can be used to manipulate the valve\nfrom random starting positions to random turn-goal positions\nusing model-predictive control (MPC). The skills and model-\npredictive control for goal-turning of valve are available here:\nhttps://sites.google.com/view/dads-skill\n",
  "categories": [
    "cs.RO",
    "cs.LG"
  ],
  "published": "2020-04-27",
  "updated": "2020-04-27"
}