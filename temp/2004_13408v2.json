{
  "id": "http://arxiv.org/abs/2004.13408v2",
  "title": "Time Series Forecasting With Deep Learning: A Survey",
  "authors": [
    "Bryan Lim",
    "Stefan Zohren"
  ],
  "abstract": "Numerous deep learning architectures have been developed to accommodate the\ndiversity of time series datasets across different domains. In this article, we\nsurvey common encoder and decoder designs used in both one-step-ahead and\nmulti-horizon time series forecasting -- describing how temporal information is\nincorporated into predictions by each model. Next, we highlight recent\ndevelopments in hybrid deep learning models, which combine well-studied\nstatistical models with neural network components to improve pure methods in\neither category. Lastly, we outline some ways in which deep learning can also\nfacilitate decision support with time series data.",
  "text": "rsta.royalsocietypublishing.org\nResearch\nArticle submitted to journal\nSubject Areas:\nDeep learning, time series modelling\nKeywords:\nDeep neural networks, time series\nforecasting, uncertainty estimation,\nhybrid models, interpretability,\ncounterfactual prediction\nAuthor for correspondence:\nBryan Lim\ne-mail: blim@robots.ox.ac.uk\nTime Series Forecasting With\nDeep Learning: A Survey\nBryan Lim1 and Stefan Zohren1\n1Department of Engineering Science, University of\nOxford, Oxford, UK\nNumerous deep learning architectures have been\ndeveloped to accommodate the diversity of time series\ndatasets across different domains. In this article, we\nsurvey common encoder and decoder designs used\nin both one-step-ahead and multi-horizon time series\nforecasting – describing how temporal information is\nincorporated into predictions by each model. Next, we\nhighlight recent developments in hybrid deep learning\nmodels, which combine well-studied statistical models\nwith neural network components to improve pure\nmethods in either category. Lastly, we outline some\nways in which deep learning can also facilitate decision\nsupport with time series data.\n1. Introduction\nTime series modelling has historically been a key area\nof academic research – forming an integral part of\napplications in topics such as climate modelling [1],\nbiological sciences [2] and medicine [3], as well as\ncommercial decision making in retail [4] and ﬁnance [5] to\nname a few. While traditional methods have focused on\nparametric models informed by domain expertise – such\nas autoregressive (AR) [6], exponential smoothing [7, 8]\nor structural time series models [9] – modern machine\nlearning methods provide a means to learn temporal\ndynamics in a purely data-driven manner [10]. With\nthe increasing data availability and computing power in\nrecent times, machine learning has become a vital part of\nthe next generation of time series forecasting models.\nDeep learning in particular has gained popularity\nin recent times, inspired by notable achievements in\nimage classiﬁcation [11], natural language processing\n[12] and reinforcement learning [13]. By incorporating\nbespoke architectural assumptions – or inductive biases\n[14] – that reﬂect the nuances of underlying datasets,\ndeep neural networks are able to learn complex data\nrepresentations [15], which alleviates the need for manual\nfeature engineering and model design. The availability\nof open-source backpropagation frameworks [16, 17] has\nalso simpliﬁed the network training, allowing for the\ncustomisation for network components and loss functions.\n©\nThe Authors.\nPublished by the Royal Society under the terms of the\nCreative Commons Attribution License http://creativecommons.org/licenses/\nby/4.0/, which permits unrestricted use, provided the original author and\nsource are credited.\narXiv:2004.13408v2  [stat.ML]  27 Sep 2020\n2\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nGiven the diversity of time-series problems across various domains, numerous neural network\ndesign choices have emerged. In this article, we summarise the common approaches to time\nseries prediction using deep neural networks. Firstly, we describe the state-of-the-art techniques\navailable for common forecasting problems – such as multi-horizon forecasting and uncertainty\nestimation. Secondly, we analyse the emergence of a new trend in hybrid models, which combine\nboth domain-speciﬁc quantitative models with deep learning components to improve forecasting\nperformance. Next, we outline two key approaches in which neural networks can be used to\nfacilitate decision support, speciﬁcally through methods in interpretability and counterfactual\nprediction. Finally, we conclude with some promising future research directions in deep learning\nfor time series prediction – speciﬁcally in the form of continuous-time and hierarchical models.\nWhile we endeavour to provide a comprehensive overview of modern methods in deep learning,\nwe note that our survey is by no means all-encompassing. Indeed, a rich body of literature exists for\nautomated approaches to time series forecasting - including automatic parametric model selection\n[18], and traditional machine learning methods such as kernel regression [19] and support vector\nregression [20]. In addition, Gaussian processes [21] have been extensively used for time series\nprediction – with recent extensions including deep Gaussian processes [22], and parallels in deep\nlearning via neural processes [23]. Furthermore, older models of neural networks have been used\nhistorically in time series applications, as seen in [24] and [25].\n2. Deep Learning Architectures for Time Series Forecasting\nTime series forecasting models predict future values of a target yi,t for a given entity i at time t.\nEach entity represents a logical grouping of temporal information – such as measurements from\nindividual weather stations in climatology, or vital signs from different patients in medicine – and\ncan be observed at the same time. In the simplest case, one-step-ahead forecasting models take the\nform:\nˆyi,t+1 = f(yi,t−k:t, xi,t−k:t, si),\n(2.1)\nwhere ˆyi,t+1 is the model forecast, yi,t−k:t = {yi,t−k, . . . , yi,t}, xi,t−k:t = {xi,t−k, . . . , xi,t} are\nobservations of the target and exogenous inputs respectively over a look-back window k, si is\nstatic metadata associated with the entity (e.g. sensor location), and f(.) is the prediction function\nlearnt by the model. While we focus on univariate forecasting in this survey (i.e. 1-D targets), we\nnote that the same components can be extended to multivariate models without loss of generality\n[26, 27, 28, 29, 30]. For notational simplicity, we omit the entity index i in subsequent sections\nunless explicitly required.\n(a) Basic Building Blocks\nDeep neural networks learn predictive relationships by using a series of non-linear layers to\nconstruct intermediate feature representations [15]. In time series settings, this can be viewed as\nencoding relevant historical information into a latent variable zt, with the ﬁnal forecast produced\nusing zt alone:\nf(yt−k:t, xt−k:t, s) = gdec(zt),\n(2.2)\nzt = genc(yt−k:t, xt−k:t, s),\n(2.3)\nwhere genc(.), gdec(.) are encoder and decoder functions respectively, and recalling that that\nsubscript i from Equation (2.1) been removed to simplify notation (e.g. yi,t replaced by yt). These\nencoders and decoders hence form the basic building blocks of deep learning architectures, with\nthe choice of network determining the types of relationships that can be learnt by our model. In\nthis section, we examine modern design choices for encoders, as overviewed in Figure 1, and their\nrelationship to traditional temporal models. In addition, we explore common network outputs and\nloss functions used in time series forecasting applications.\n3\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n(a) CNN Model.\n(b) RNN Model.\n(c) Attention-based Model.\nFigure 1: Incorporating temporal information using different encoder architectures.\n(i) Convolutional Neural Networks\nTraditionally designed for image datasets, convolutional neural networks (CNNs) extract local\nrelationships that are invariant across spatial dimensions [11, 31]. To adapt CNNs to time series\ndatasets, researchers utilise multiple layers of causal convolutions [32, 33, 34] – i.e. convolutional\nﬁlters designed to ensure only past information is used for forecasting. For an intermediate feature\nat hidden layer l, each causal convolutional ﬁlter takes the form below:\nhl+1\nt\n= A\n\u0012\n(W ∗h) (l, t)\n\u0013\n,\n(2.4)\n(W ∗h) (l, t) =\nk\nX\nτ=0\nW (l, τ)hl\nt−τ,\n(2.5)\nwhere hl\nt ∈RHin is an intermediate state at layer l at time t, ∗is the convolution operator, W (l, τ) ∈\nRHout×Hin is a ﬁxed ﬁlter weight at layer l, and A(.) is an activation function, such as a sigmoid\nfunction, representing any architecture-speciﬁc non-linear processing. For CNNs that use a total of\nL convolutional layers, we note that the encoder output is then zt = hL\nt .\nConsidering the 1-D case, we can see that Equation (2.5) bears a strong resemblance to ﬁnite\nimpulse response (FIR) ﬁlters in digital signal processing [35]. This leads to two key implications\nfor temporal relationships learnt by CNNs. Firstly, in line with the spatial invariance assumptions\nfor standard CNNs, temporal CNNs assume that relationships are time-invariant – using the same\nset of ﬁlter weights at each time step and across all time. In addition, CNNs are only able to use\ninputs within its deﬁned lookback window, or receptive ﬁeld, to make forecasts. As such, the\nreceptive ﬁeld size k needs to be tuned carefully to ensure that the model can make use of all\nrelevant historical information. It is worth noting that a single causal CNN layer with a linear\nactivation function is equivalent to an auto-regressive (AR) model.\nDilated Convolutions\nUsing standard convolutional layers can be computationally challenging\nwhere long-term dependencies are signiﬁcant, as the number of parameters scales directly with the\nsize of the receptive ﬁeld. To alleviate this, modern architectures frequently make use of dilated\ncovolutional layers [32, 33], which extend Equation (2.5) as below:\n(W ∗h) (l, t, dl) =\n⌊k/dl⌋\nX\nτ=0\nW (l, τ)hl\nt−dlτ,\n(2.6)\nwhere ⌊.⌋is the ﬂoor operator and dl is a layer-speciﬁc dilation rate. Dilated convolutions can hence\nbe interpreted as convolutions of a down-sampled version of the lower layer features – reducing\nresolution to incorporate information from the distant past. As such, by increasing the dilation rate\nwith each layer, dilated convolutions can gradually aggregate information at different time blocks,\nallowing for more history to be used in an efﬁcient manner. With the WaveNet architecture of [32]\nfor instance, dilation rates are increased in powers of 2 with adjacent time blocks aggregated in\neach layer – allowing for 2l time steps to be used at layer l as shown in Figure 1a.\n4\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n(ii) Recurrent Neural Networks\nRecurrent neural networks (RNNs) have historically been used in sequence modelling [31],\nwith strong results on a variety of natural language processing tasks [36]. Given the natural\ninterpretation of time series data as sequences of inputs and targets, many RNN-based architectures\nhave been developed for temporal forecasting applications [37, 38, 39, 40]. At its core, RNN cells\ncontain an internal memory state which acts as a compact summary of past information. The\nmemory state is recursively updated with new observations at each time step as shown in Figure\n1b, i.e.:\nzt = ν (zt−1, yt, xt, s) ,\n(2.7)\nWhere zt ∈RH here is the hidden internal state of the RNN, and ν(.) is the learnt memory update\nfunction. For instance, the Elman RNN [41], one of the simplest RNN variants, would take the\nform below:\nyt+1 = γy(Wyzt + by),\n(2.8)\nzt = γz(Wz1zt−1 + Wz2yt + Wz3xt + Wz4s + bz),\n(2.9)\nWhere W., b. are the linear weights and biases of the network respectively, and γy(.), γz(.) are\nnetwork activation functions. Note that RNNs do not require the explicit speciﬁcation of a lookback\nwindow as per the CNN case. From a signal processing perspective, the main recurrent layer – i.e.\nEquation (2.9) – thus resembles a non-linear version of inﬁnite impulse response (IIR) ﬁlters.\nLong Short-term Memory\nDue to the inﬁnite lookback window, older variants of RNNs can\nsuffer from limitations in learning long-range dependencies in the data [42, 43] – due to issues with\nexploding and vanishing gradients [31]. Intuitively, this can be seen as a form of resonance in the\nmemory state. Long Short-Term Memory networks (LSTMs) [44] were hence developed to address\nthese limitations, by improving gradient ﬂow within the network. This is achieved through the use\nof a cell state ct which stores long-term information, modulated through a series of gates as below:\nInput gate:\nit = σ(Wi1zt−1 + Wi2yt + Wi3xt + Wi4s + bi),\n(2.10)\nOutput gate:\not = σ(Wo1zt−1 + Wo2yt + Wo3xt + Wo4s + bo),\n(2.11)\nForget gate:\nft = σ(Wf1zt−1 + Wf2yt + Wf3xt + Wf4s + bf),\n(2.12)\nwhere zt−1 is the hidden state of the LSTM, and σ(.) is the sigmoid activation function. The gates\nmodify the hidden and cell states of the LSTM as below:\nHidden state:\nzt = ot ⊙tanh(ct),\n(2.13)\nCell state:\nct = ft ⊙ct−1\n+ it ⊙tanh(Wc1zt−1 + Wc2yt + Wc3xt + Wc4s + bc),\n(2.14)\nWhere ⊙is the element-wise (Hadamard) product, and tanh(.) is the tanh activation function.\nRelationship to Bayesian Filtering\nAs examined in [39], Bayesian ﬁlters [45] and RNNs are both\nsimilar in their maintenance of a hidden state which is recursively updated over time. For Bayesian\nﬁlters, such as the Kalman ﬁlter [46], inference is performed by updating the sufﬁcient statistics\nof the latent state – using a series of state transition and error correction steps. As the Bayesian\nﬁltering steps use deterministic equations to modify sufﬁcient statistics, the RNN can be viewed\nas a simultaneous approximation of both steps – with the memory vector containing all relevant\ninformation required for prediction.\n5\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n(iii) Attention Mechanisms\nThe development of attention mechanisms [47, 48] has also lead to improvements in long-term\ndependency learning – with Transformer architectures achieving state-of-the-art performance in\nmultiple natural language processing applications [12, 49, 50]. Attention layers aggregate temporal\nfeatures using dynamically generated weights (see Figure 1c), allowing the network to directly\nfocus on signiﬁcant time steps in the past – even if they are very far back in the lookback window.\nConceptually, attention is a mechanism for a key-value lookup based on a given query [51], taking\nthe form below:\nht =\nk\nX\nτ=0\nα(κt, qτ)vt−τ,\n(2.15)\nWhere the key κt, query qτ and value vt−τ are intermediate features produced at different time\nsteps by lower levels of the network. Furthermore, α(κt, qτ) ∈[0, 1] is the attention weight for\nt −τ generated at time t, and ht is the context vector output of the attention layer. Note that\nmultiple attention layers can also be used together as per the CNN case, with the output from the\nﬁnal layer forming the encoded latent variable zt.\nRecent work has also demonstrated the beneﬁts of using attention mechanisms in time series\nforecasting applications, with improved performance over comparable recurrent networks [52,\n53, 54]. For instance, [52] use attention to aggregate features extracted by RNN encoders, with\nattention weights produced as below:\nα(t) = softmax(ηt),\n(2.16)\nηt = Wη1tanh(Wη2κt−1 + Wη3qτ + bη),\n(2.17)\nwhere α(t) = [α(t, 0), . . . α(t, k)] is a vector of attention weights, κt−1, qt are outputs from LSTM\nencoders used for feature extraction, and softmax(.) is the softmax activation function. More\nrecently, Transformer architectures have also been considered in [53, 54], which apply scalar-dot\nproduct self-attention [49] to features extracted within the lookback window. From a time series\nmodelling perspective, attention provides two key beneﬁts. Firstly, networks with attention are\nable to directly attend to any signiﬁcant events that occur. In retail forecasting applications, for\nexample, this includes holiday or promotional periods which can have a positive effect on sales.\nSecondly, as shown in [54], attention-based networks can also learn regime-speciﬁc temporal\ndynamics – by using distinct attention weight patterns for each regime.\n(iv) Outputs and Loss Functions\nGiven the ﬂexibility of neural networks, deep neural networks have been used to model both\ndiscrete [55] and continuous [37, 56] targets – by customising of decoder and output layer of the\nneural network to match the desired target type. In one-step-ahead prediction problems, this\ncan be as simple as combining a linear transformation of encoder outputs (i.e. Equation (2.2))\ntogether with an appropriate output activation for the target. Regardless of the form of the target,\npredictions can be further divided into two different categories – point estimates and probabilistic\nforecasts.\nPoint Estimates\nA common approach to forecasting is to determine the expected value of a\nfuture target. This essentially involves reformulating the problem to a classiﬁcation task for\ndiscrete outputs (e.g. forecasting future events), and regression task for continuous outputs – using\nthe encoders described above. For the binary classiﬁcation case, the ﬁnal layer of the decoder then\nfeatures a linear layer with a sigmoid activation function – allowing the network to predict the\nprobability of event occurrence at a given time step. For one-step-ahead forecasts of binary and\ncontinuous targets, networks are trained using binary cross-entropy and mean square error loss\n6\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nfunctions respectively:\nLclassification = −1\nT\nT\nX\nt=1\nyt log(ˆyt) + (1 −yt) log(1 −ˆyt)\n(2.18)\nLregression = 1\nT\nT\nX\nt=1\n(yt −ˆyt)2\n(2.19)\nWhile the loss functions above are the most common across applications, we note that the\nﬂexibility of neural networks also allows for more complex losses to be adopted - e.g. losses for\nquantile regression [56] and multinomial classiﬁcation [32].\nProbabilistic Outputs\nWhile point estimates are crucial to predicting the future value of a target,\nunderstanding the uncertainty of a model’s forecast can be useful for decision makers in different\ndomains. When forecast uncertainties are wide, for instance, model users can exercise more caution\nwhen incorporating predictions into their decision making, or alternatively rely on other sources\nof information. In some applications, such as ﬁnancial risk management, having access to the full\npredictive distribution will allow decision makers to optimise their actions in the presence of rare\nevents – e.g. allowing risk managers to insulate portfolios against market crashes.\nA common way to model uncertainties is to use deep neural networks to generate parameters\nof known distributions [27, 37, 38]. For example, Gaussian distributions are typically used for\nforecasting problems with continuous targets, with the networks outputting means and variance\nparameters for the predictive distributions at each step as below:\nyt+τ ∼N(µ(t, τ), ζ(t, τ)2),\n(2.20)\nµ(t, τ) = WµhL\nt + bµ,\n(2.21)\nζ(t, τ) = softplus(WΣhL\nt + bΣ),\n(2.22)\nwhere hL\nt is the ﬁnal layer of the network, and softplus(.) is the softplus activation function to\nensure that standard deviations take only positive values.\n(b) Multi-horizon Forecasting Models\nIn many applications, it is often beneﬁcial to have access to predictive estimates at multiple points\nin the future – allowing decision makers to visualise trends over a future horizon, and optimise\ntheir actions across the entire path. From a statistical perspective, multi-horizon forecasting can be\nviewed as a slight modiﬁcation of one-step-ahead prediction problem (i.e. Equation (2.1)) as below:\nˆyt+τ = f(yt−k:t, xt−k:t, ut−k:t+τ, s, τ),\n(2.23)\nwhere τ ∈{1, . . . , τmax} is a discrete forecast horizon, ut are known future inputs (e.g. date\ninformation, such as the day-of-week or month) across the entire horizon, and xt are inputs\nthat can only be observed historically. In line with traditional econometric approaches [57, 58],\ndeep learning architectures for multi-horizon forecasting can be divided into iterative and direct\nmethods – as shown in Figure 2 and described in detail below.\n(i) Iterative Methods\nIterative approaches to multi-horizon forecasting typically make use of autoregressive deep\nlearning architectures [37, 39, 40, 53] – producing multi-horizon forecasts by recursively feeding\nsamples of the target into future time steps (see Figure 2a). By repeating the procedure to generate\nmultiple trajectories, forecasts are then produced using the sampling distributions for target values\nat each step. For instance, predictive means can be obtained using the Monte Carlo estimate\nˆyt+τ = PJ\nj=1 ˜y(j)\nt+τ/J, where ˜y(j)\nt+τ is a sample taken based on the model of Equation (2.20). As\nautoregressive models are trained in the exact same fashion as one-step-ahead prediction models\n7\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n(a) Iterative Methods\n(b) Direct Methods\nFigure 2: Main types of multi-horizon forecasting models. Colours used to distinguish between\nmodel weights – with iterative models using a common model across the entire horizon and direct\nmethods taking a sequence-to-sequence approach.\n(i.e. via backpropagation through time), the iterative approach allows for the easy generalisation\nof standard models to multi-step forecasting. However, as a small amount of error is produced\nat each time step, the recursive structure of iterative methods can potentially lead to large error\naccumulations over longer forecasting horizons. In addition, iterative methods assume that all\ninputs but the target are known at run-time – requiring only samples of the target to be fed into\nfuture time steps. This can be a limitation in many practical scenarios where observed inputs exist,\nmotivating the need for more ﬂexible methods.\n(ii) Direct Methods\nDirect methods alleviate the issues with iterative methods by producing forecasts directly using all\navailable inputs. They typically make use of sequence-to-sequence architectures [52, 54, 56], using\nan encoder to summarise past information (i.e. targets, observed inputs and a priori known inputs),\nand a decoder to combine them with known future inputs – as depicted in Figure 2b. As described\nin [59], alternative approach is to use simpler models to directly produce a ﬁxed-length vector\nmatching the desired forecast horizon. This, however, does require the speciﬁcation of a maximum\nforecast horizon (i.e. τmax), with predictions made only at the predeﬁned discrete intervals.\n3. Incorporating Domain Knowledge with Hybrid Models\nDespite its popularity, the efﬁcacy of machine learning for time series prediction has historically\nbeen questioned – as evidenced by forecasting competitions such as the M-competitions [60]. Prior\nto the M4 competition of 2018 [61], the prevailing wisdom was that sophisticated methods do not\nproduce more accurate forecasts, and simple models with ensembling had a tendency to do better\n[59, 62, 63]. Two key reasons have been identiﬁed to explain the underperformance of machine\nlearning methods. Firstly, the ﬂexibility of machine learning methods can be a double-edged sword\n– making them prone to overﬁtting [59]. Hence, simpler models may potentially do better in low\ndata regimes, which are particularly common in forecasting problems with a small number of\nhistorical observations (e.g. quarterly macroeconomic forecasts). Secondly, similar to stationarity\nrequirements of statistical models, machine learning models can be sensitive to how inputs are\npre-processed [26, 37, 59], which ensure that data distributions at training and test time are similar.\nA recent trend in deep learning has been in developing hybrid models which address these\nlimitations, demonstrating improved performance over pure statistical or machine learning models\nin a variety of applications [38, 64, 65, 66]. Hybrid methods combine well-studied quantitative\ntime series models together with deep learning – using deep neural networks to generate model\nparameters at each time step. On the one hand, hybrid models allow domain experts to inform\nneural network training using prior information – reducing the hypothesis space of the network\nand improving generalisation. This is especially useful for small datasets [38], where there is a\ngreater risk of overﬁtting for deep learning models. Furthermore, hybrid models allow for the\nseparation of stationary and non-stationary components, and avoid the need for custom input\npre-processing. An example of this is the Exponential Smoothing RNN (ES-RNN) [64], winner\nof the M4 competition, which uses exponential smoothing to capture non-stationary trends and\n8\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nlearns additional effects with the RNN. In general, hybrid models utilise deep neural networks\nin two manners: a) to encode time-varying parameters for non-probabilistic parametric models\n[64, 65, 67], and b) to produce parameters of distributions used by probabilistic models [38, 40, 66].\n(a) Non-probabilistic Hybrid Models\nWith parametric time series models, forecasting equations are typically deﬁned analytically and\nprovide point forecasts for future targets. Non-probabilistic hybrid models hence modify these\nforecasting equations to combine statistical and deep learning components. The ES-RNN for\nexample, utilises the update equations of the Holt-Winters exponential smoothing model [8] –\ncombining multiplicative level and seasonality components with deep learning outputs as below:\nˆyi,t+τ = exp(WEShL\ni,t+τ + bES) × li,t × γi,t+τ,\n(3.1)\nli,t = β(i)\n1 yi,t/γi,t + (1 −β(i)\n1 )li,t−1,\n(3.2)\nγi,t = β(i)\n2 yi,t/li,t + (1 −β(i)\n2 )γi,t−κ,\n(3.3)\nwhere hL\ni,t+τ is the ﬁnal layer of the network for the τth-step-ahead forecast, li,t is a level\ncomponent, γi,t is a seasonality component with period κ, and β(i)\n1 , β(i)\n2\nare entity-speciﬁc static\ncoefﬁcients. From the above equations, we can see that the exponential smoothing components\n(li,t, γi,t) handle the broader (e.g. exponential) trends within the datasets, reducing the need for\nadditional input scaling.\n(b) Probabilistic Hybrid Models\nProbabilistic hybrid models can also be used in applications where distribution modelling is\nimportant – utilising probabilistic generative models for temporal dynamics such as Gaussian\nprocesses [40] and linear state space models [38]. Rather than modifying forecasting equations,\nprobabilistic hybrid models use neural networks to produce parameters for predictive distributions\nat each step. For instance, Deep State Space Models [38] encode time-varying parameters for linear\nstate space models as below – performing inference via the Kalman ﬁltering equations [46]:\nyt = a(hL\ni,t+τ)T lt + φ(hL\ni,t+τ)ϵt,\n(3.4)\nlt = F (hL\ni,t+τ)lt−1 + q(hL\ni,t+τ) + Σ(hL\ni,t+τ) ⊙Σt,\n(3.5)\nwhere lt is the hidden latent state, a(.), F (.), q(.) are linear transformations of hL\ni,t+τ, φ(.), Σ(.)\nare linear transformations with softmax activations, ϵt ∼N(0, 1) is a univariate residual and\nΣt ∼N(0, I) is a multivariate normal random variable.\n4. Facilitating Decision Support Using Deep Neural Networks\nAlthough model builders are mainly concerned with the accuracy of their forecasts, end-users\ntypically use predictions to guide their future actions. For instance, doctors can make use of clinical\nforecasts (e.g. probabilities of disease onset and mortality) to help them prioritise tests to order,\nformulate a diagnosis and determine a course of treatment. As such, while time series forecasting is\na crucial preliminary step, a better understanding of both temporal dynamics and the motivations\nbehind a model’s forecast can help users further optimise their actions. In this section, we explore\ntwo directions in which neural networks have been extended to facilitate decision support with\ntime series data – focusing on methods in interpretability and causal inference.\n(a) Interpretability With Time Series Data\nWith the deployment of neural networks in mission-critical applications [68], there is a increasing\nneed to understand both how and why a model makes a certain prediction. Moreover, end-users can\n9\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nhave little prior knowledge with regards to the relationships present in their data, with datasets\ngrowing in size and complexity in recent times. Given the black-box nature of standard neural\nnetwork architectures, a new body of research has emerged in methods for interpreting deep\nlearning models. We present a summary below – referring the reader to dedicated surveys for\nmore in-depth analyses [69, 70].\nTechniques for Post-hoc Interpretability\nPost-hoc interpretable models are developed to\ninterpret trained networks, and helping to identify important features or examples without\nmodifying the original weights. Methods can mainly be divided into two main categories. Firstly,\none possible approach is to apply simpler interpretable surrogate models between the inputs and\noutputs of the neural network, and rely on the approximate model to provide explanations. For\ninstance, Local Interpretable Model-Agnostic Explanations (LIME) [71] identify relevant features\nby ﬁtting instance-speciﬁc linear models to perturbations of the input, with the linear coefﬁcients\nproviding a measure of importance. Shapley additive explanations (SHAP) [72] provide another\nsurrogate approach, which utilises Shapley values from cooperative game theory to identify\nimportant features across the dataset. Next, gradient-based method – such as saliency maps [73, 74]\nand inﬂuence functions [75] – have been proposed, which analyse network gradients to determine\nwhich input features have the greatest impact on loss functions. While post-hoc interpretability\nmethods can help with feature attributions, they typically ignore any sequential dependencies\nbetween inputs – making it difﬁcult to apply them to complex time series datasets.\nInherent Interpretability with Attention Weights\nAn alternative approach is to directly design\narchitectures with explainable components, typically in the form of strategically placed attention\nlayers. As attention weights are produced as outputs from a softmax layer, the weights are\nconstrained to sum to 1, i.e. Pk\nτ=0 α(t, τ) = 1. For time series models, the outputs of Equation (2.15)\ncan hence also be interpreted as a weighted average over temporal features, using the weights\nsupplied by the attention layer at each step. An analysis of attention weights can then be used to\nunderstand the relative importance of features at each time step. Instance-wise interpretability\nstudies have been performed in [53, 55, 76], where the authors used speciﬁc examples to show how\nthe magnitudes of α(t, τ) can indicate which time points were most signiﬁcant for predictions. By\nanalysing distributions of attention vectors across time, [54] also shows how attention mechanisms\ncan be used to identify persistent temporal relationships – such as seasonal patterns – in the dataset.\n(b) Counterfactual Predictions & Causal Inference Over Time\nIn addition to understanding the relationships learnt by the networks, deep learning can also help\nto facilitate decision support by producing predictions outside of their observational datasets, or\ncounterfactual forecasts. Counterfactual predictions are particularly useful for scenario analysis\napplications – allowing users to evaluate how different sets of actions can impact target trajectories.\nThis can be useful both from a historical angle, i.e. determining what would have happened if a\ndifferent set of circumstances had occurred, and from a forecasting perspective, i.e. determining\nwhich actions to take to optimise future outcomes.\nWhile a large class of deep learning methods exists for estimating causal effects in static\nsettings [77, 78, 79], the key challenge in time series datasets is the presence of time-dependent\nconfounding effects. This arises due to circular dependencies when actions that can affect the\ntarget are also conditional on observations of the target. Without any adjusting for time-dependent\nconfounders, straightforward estimations techniques can results in biased results, as shown in [80].\nRecently, several methods have emerged to train deep neural networks while adjusting for time-\ndependent confounding, based on extensions of statistical techniques and the design of new loss\nfunctions. With statistical methods, [81] extends the inverse-probability-of-treatment-weighting\n(IPTW) approach of marginal structural models in epidemiology – using one set of networks to\nestimate treatment application probabilities, and a sequence-to-sequence model to learn unbiased\npredictions. Another approach in [82] extends the G-computation framework, jointly modelling\n10\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\ndistributions of the target and actions using deep learning. In addition, new loss functions have\nbeen proposed in [83], which adopts domain adversarial training to learn balanced representations\nof patient history.\n5. Conclusions and Future Directions\nWith the growth in data availability and computing power in recent times, deep neural networks\narchitectures have achieved much success in forecasting problems across multiple domains. In\nthis article, we survey the main architectures used for time series forecasting – highlighting the\nkey building blocks used in neural network design. We examine how they incorporate temporal\ninformation for one-step-ahead predictions, and describe how they can be extended for use in\nmulti-horizon forecasting. Furthermore, we outline the recent trend of hybrid deep learning models,\nwhich combine statistical and deep learning components to outperform pure methods in either\ncategory. Finally, we summarise two ways in which deep learning can be extended to improve\ndecision support over time, focusing on methods in interpretability and counterfactual prediction.\nAlthough a large number of deep learning models have been developed for time series\nforecasting, some limitations still exist. Firstly, deep neural networks typically require time series to\nbe discretised at regular intervals, making it difﬁcult to forecast datasets where observations can be\nmissing or arrive at random intervals. While some preliminary research on continuous-time models\nhas been done via Neural Ordinary Differential Equations [84], additional work needs to be done\nto extend this work for datasets with complex inputs (e.g. static variables) and to benchmark them\nagainst existing models. In addition, as mentioned in [85], time series often have a hierarchical\nstructure with logical groupings between trajectories – e.g. in retail forecasting, where product\nsales in the same geography can be affected by common trends. As such, the development of\narchitectures which explicit account for such hierarchies could be an interesting research direction,\nand potentially improve forecasting performance over existing univariate or multivariate models.\nCompeting Interests. The author(s) declare that they have no competing interests.\nReferences\n1 Mudelsee M. Trend analysis of climate time series: A review of methods. Earth-Science Reviews.\n2019;190:310 – 322.\n2 Stoffer DS, Ombao H. Editorial: Special issue on time series analysis in the biological sciences.\nJournal of Time Series Analysis. 2012;33(5):701–703.\n3 Topol EJ. High-performance medicine: the convergence of human and artiﬁcial intelligence.\nNature Medicine. 2019 Jan;25(1):44–56.\n4 Böse JH, Flunkert V, Gasthaus J, Januschowski T, Lange D, Salinas D, et al. Probabilistic Demand\nForecasting at Scale. Proc VLDB Endow. 2017 Aug;10(12):1694–1705.\n5 Andersen TG, Bollerslev T, Christoffersen PF, Diebold FX. Volatility Forecasting. National\nBureau of Economic Research; 2005. 11188.\n6 Box GEP, Jenkins GM. Time Series Analysis: Forecasting and Control. Holden-Day; 1976.\n7 Gardner Jr ES. Exponential smoothing: The state of the art. Journal of Forecasting. 1985;4(1):1–28.\n8 Winters PR. Forecasting Sales by Exponentially Weighted Moving Averages. Management\nScience. 1960;6(3):324–342.\n9 Harvey AC. Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge\nUniversity Press; 1990.\n10 Ahmed NK, Atiya AF, Gayar NE, El-Shishiny H. An Empirical Comparison of Machine Learning\nModels for Time Series Forecasting. Econometric Reviews. 2010;29(5-6):594–621.\n11 Krizhevsky A, Sutskever I, Hinton GE. ImageNet Classiﬁcation with Deep Convolutional\nNeural Networks. In: Pereira F, Burges CJC, Bottou L, Weinberger KQ, editors. Advances in\nNeural Information Processing Systems 25 (NIPS); 2012. p. 1097–1105.\n12 Devlin J, Chang MW, Lee K, Toutanova K.\nBERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In: Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers); 2019. p. 4171–4186.\n11\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13 Silver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driessche G, et al. Mastering the\ngame of Go with deep neural networks and tree search. Nature. 2016;529:484–503.\n14 Baxter J. A Model of Inductive Bias Learning. J Artif Int Res. 2000;12(1):149â ˘A¸S198.\n15 Bengio Y, Courville A, Vincent P. Representation Learning: A Review and New Perspectives.\nIEEE Transactions on Pattern Analysis and Machine Intelligence. 2013;35(8):1798–1828.\n16 Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al.. TensorFlow: Large-Scale\nMachine Learning on Heterogeneous Systems; 2015. Software available from tensorﬂow.org.\nAvailable from: http://tensorflow.org/.\n17 Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch: An Imperative Style,\nHigh-Performance Deep Learning Library. In: Advances in Neural Information Processing\nSystems 32; 2019. p. 8024–8035.\n18 Hyndman RJ, Khandakar Y. Automatic time series forecasting: the forecast package for R.\nJournal of Statistical Software. 2008;26(3):1–22.\n19 Nadaraya EA.\nOn Estimating Regression.\nTheory of Probability and Its Applications.\n1964;9(1):141–142.\n20 Smola AJ, SchÃ˝ulkopf B. A Tutorial on Support Vector Regression. Statistics and Computing.\n2004;14(3):199–222.\n21 Williams CKI, Rasmussen CE. Gaussian Processes for Regression. In: Advances in Neural\nInformation Processing Systems (NIPS); 1996. .\n22 Damianou A, Lawrence N. Deep Gaussian Processes. In: Proceedings of the Conference on\nArtiﬁcial Intelligence and Statistics (AISTATS); 2013. .\n23 Garnelo M, Rosenbaum D, Maddison C, Ramalho T, Saxton D, Shanahan M, et al. Conditional\nNeural Processes. In: Proceedings of the International Conference on Machine Learning (ICML);\n2018. .\n24 Waibel A. Modular Construction of Time-Delay Neural Networks for Speech Recognition.\nNeural Comput. 1989;1(1):39â ˘A¸S46.\n25 Wan E. Time Series Prediction by Using a Connectionist Network with Internal Delay Lines. In:\nTime Series Prediction. Addison-Wesley; 1994. p. 195–217.\n26 Sen R, Yu HF, Dhillon I. Think Globally, Act Locally: A Deep Neural Network Approach to\nHigh-Dimensional Time Series Forecasting. In: Advances in Neural Information Processing\nSystems (NeurIPS); 2019. .\n27 Wen R, Torkkola K. Deep Generative Quantile-Copula Models for Probabilistic Forecasting. In:\nICML Time Series Workshop; 2019. .\n28 Li Y, Yu R, Shahabi C, Liu Y. Diffusion Convolutional Recurrent Neural Network: Data-\nDriven Trafﬁc Forecasting. In: (Proceedings of the International Conference on Learning\nRepresentations ICLR); 2018. .\n29 Ghaderi A, Sanandaji BM, Ghaderi F. Deep Forecast: Deep Learning-based Spatio-Temporal\nForecasting. In: ICML Time Series Workshop; 2017. .\n30 Salinas D, Bohlke-Schneider M, Callot L, Medico R, Gasthaus J. High-dimensional multivariate\nforecasting with low-rank Gaussian Copula Processes. In: Advances in Neural Information\nProcessing Systems (NeurIPS); 2019. .\n31 Goodfellow I, Bengio Y, Courville A.\nDeep Learning.\nMIT Press; 2016.\nhttp://www.\ndeeplearningbook.org.\n32 van den Oord A, Dieleman S, Zen H, Simonyan K, Vinyals O, Graves A, et al. WaveNet: A\nGenerative Model for Raw Audio. arXiv e-prints. 2016 Sep;p. arXiv:1609.03499.\n33 Bai S, Zico Kolter J, Koltun V. An Empirical Evaluation of Generic Convolutional and Recurrent\nNetworks for Sequence Modeling. arXiv e-prints. 2018;p. arXiv:1803.01271.\n34 Borovykh A, Bohte S, Oosterlee CW. Conditional Time Series Forecasting with Convolutional\nNeural Networks. arXiv e-prints. 2017;p. arXiv:1703.04691.\n35 Lyons RG. Understanding Digital Signal Processing (2nd Edition). USA: Prentice Hall PTR;\n2004.\n36 Young T, Hazarika D, Poria S, Cambria E.\nRecent Trends in Deep Learning Based\nNatural Language Processing [Review Article]. IEEE Computational Intelligence Magazine.\n2018;13(3):55–75.\n37 Salinas D, Flunkert V, Gasthaus J. DeepAR: Probabilistic Forecasting with Autoregressive\nRecurrent Networks. arXiv e-prints. 2017;p. arXiv:1704.04110.\n38 Rangapuram SS, Seeger MW, Gasthaus J, Stella L, Wang Y, Januschowski T. Deep State Space\nModels for Time Series Forecasting. In: Advances in Neural Information Processing Systems\n(NIPS); 2018. .\n12\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39 Lim B, Zohren S, Roberts S. Recurrent Neural Filters: Learning Independent Bayesian Filtering\nSteps for Time Series Prediction. In: International Joint Conference on Neural Networks (IJCNN);\n2020. .\n40 Wang Y, Smola A, Maddix D, Gasthaus J, Foster D, Januschowski T. Deep Factors for Forecasting.\nIn: Proceedings of the International Conference on Machine Learning (ICML); 2019. .\n41 Elman JL. Finding structure in time. Cognitive Science. 1990;14(2):179 – 211.\n42 Bengio Y, Simard P, Frasconi P. Learning long-term dependencies with gradient descent is\ndifﬁcult. IEEE Transactions on Neural Networks. 1994;5(2):157–166.\n43 Kolen JF, Kremer SC. In: Gradient Flow in Recurrent Nets: The Difﬁculty of Learning LongTerm\nDependencies; 2001. p. 237–243.\n44 Hochreiter S, Schmidhuber J.\nLong Short-Term Memory.\nNeural Computation. 1997\nNov;9(8):1735–1780.\n45 Srkk S. Bayesian Filtering and Smoothing. Cambridge University Press; 2013.\n46 Kalman RE. A New Approach to Linear Filtering and Prediction Problems. Journal of Basic\nEngineering. 1960;82(1):35.\n47 Bahdanau D, Cho K, Bengio Y. Neural Machine Translation by Jointly Learning to Align and\nTranslate. In: Proceedings of the International Conference on Learning Representations (ICLR);\n2015. .\n48 Cho K, van Merriënboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, et al. Learning\nPhrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In:\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP); 2014. .\n49 Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is All you\nNeed. In: Advances in Neural Information Processing Systems (NIPS); 2017. .\n50 Dai Z, Yang Z, Yang Y, Carbonell J, Le Q, Salakhutdinov R. Transformer-XL: Attentive Language\nModels beyond a Fixed-Length Context. In: Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics (ACL); 2019. .\n51 Graves A, Wayne G, Danihelka I. Neural Turing Machines. CoRR. 2014;abs/1410.5401.\n52 Fan C, Zhang Y, Pan Y, Li X, Zhang C, Yuan R, et al. Multi-Horizon Time Series Forecasting with\nTemporal Attention Learning. In: Proceedings of the ACM SIGKDD international conference\non Knowledge discovery and data mining (KDD); 2019. .\n53 Li S, Jin X, Xuan Y, Zhou X, Chen W, Wang YX, et al. Enhancing the Locality and Breaking\nthe Memory Bottleneck of Transformer on Time Series Forecasting. In: Advances in Neural\nInformation Processing Systems (NeurIPS); 2019. .\n54 Lim B, Arik SO, Loeff N, Pﬁster T. Temporal Fusion Transformers for Interpretable Multi-horizon\nTime Series Forecasting. arXiv e-prints. 2019;p. arXiv:1912.09363.\n55 Choi E, Bahadori MT, Sun J, Kulas JA, Schuetz A, Stewart WF. RETAIN: An Interpretable\nPredictive Model for Healthcare using Reverse Time Attention Mechanism. In: Advances in\nNeural Information Processing Systems (NIPS); 2016. .\n56 Wen R, et al. A Multi-Horizon Quantile Recurrent Forecaster. In: NIPS 2017 Time Series\nWorkshop; 2017. .\n57 Taieb SB, Sorjamaa A, Bontempi G. Multiple-output modeling for multi-step-ahead time series\nforecasting. Neurocomputing. 2010;73(10):1950 – 1957.\n58 Marcellino M, Stock J, Watson M. A Comparison of Direct and Iterated Multistep AR Methods\nfor Forecasting Macroeconomic Time Series. Journal of Econometrics. 2006;135:499–526.\n59 Makridakis S, Spiliotis E, Assimakopoulos V. Statistical and Machine Learning forecasting\nmethods: Concerns and ways forward. PLOS ONE. 2018 03;13(3):1–26.\n60 Hyndman R. A brief history of forecasting competitions. International Journal of Forecasting.\n2020;36(1):7–14.\n61 The M4 Competition: 100,000 time series and 61 forecasting methods. International Journal of\nForecasting. 2020;36(1):54 – 74.\n62 Fildes R, Hibon M, Makridakis S, Meade N. Generalising about univariate forecasting methods:\nfurther empirical evidence. International Journal of Forecasting. 1998;14(3):339 – 358.\n63 Makridakis S, Hibon M.\nThe M3-Competition: results, conclusions and implications.\nInternational Journal of Forecasting. 2000;16(4):451 – 476. The M3- Competition.\n64 Smyl S. A hybrid method of exponential smoothing and recurrent neural networks for time\nseries forecasting. International Journal of Forecasting. 2020;36(1):75 – 85. M4 Competition.\n65 Lim B, Zohren S, Roberts S. Enhancing Time-Series Momentum Strategies Using Deep Neural\nNetworks. The Journal of Financial Data Science. 2019;.\n13\nrsta.royalsocietypublishing.org Phil. Trans. R. Soc. A 0000000\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66 Grover A, Kapoor A, Horvitz E. A Deep Hybrid Model for Weather Forecasting. In: Proceedings\nof the ACM SIGKDD international conference on knowledge discovery and data mining (KDD);\n2015. .\n67 Binkowski M, Marti G, Donnat P.\nAutoregressive Convolutional Neural Networks for\nAsynchronous Time Series.\nIn: Proceedings of the International Conference on Machine\nLearning (ICML); 2018. .\n68 Moraffah R, Karami M, Guo R, Raglin A, Liu H. Causal Interpretability for Machine Learning –\nProblems, Methods and Evaluation. arXiv e-prints. 2020;p. arXiv:2003.03934.\n69 Chakraborty S, Tomsett R, Raghavendra R, Harborne D, Alzantot M, Cerutti F, et al.\nInterpretability of deep learning models: A survey of results. In: 2017 IEEE SmartWorld\nConference Proceedings); 2017. p. 1–6.\n70 Rudin C. Stop explaining black box machine learning models for high stakes decisions and use\ninterpretable models instead. Nature Machine Intelligence. 2019 May;1(5):206–215.\n71 Ribeio M, Singh S, Guestrin C. \"Why Should I Trust You?\" Explaining the Predictions of Any\nClassiﬁer. In: KDD; 2016. .\n72 Lundberg S, Lee SI. A Uniﬁed Approach to Interpreting Model Predictions. In: Advances in\nNeural Information Processing Systems (NIPS); 2017. .\n73 Simonyan K, Vedaldi A, Zisserman A. Deep Inside Convolutional Networks: Visualising Image\nClassiﬁcation Models and Saliency Maps. arXiv e-prints. 2013;p. arXiv:1312.6034.\n74 Siddiqui SA, Mercier D, Munir M, Dengel A, Ahmed S. TSViz: Demystiﬁcation of Deep Learning\nModels for Time-Series Analysis. IEEE Access. 2019;7:67027–67040.\n75 Koh PW, Liang P. Understanding Black-box Predictions via Inﬂuence Functions. In: Proceedings\nof the International Conference on Machine Learning(ICML; 2017. .\n76 Bai T, Zhang S, Egleston BL, Vucetic S. Interpretable Representation Learning for Healthcare\nvia Capturing Disease Progression through Time.\nIn: Proceedings of the ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining (KDD); 2018. .\n77 Yoon J, Jordon J, van der Schaar M. GANITE: Estimation of Individualized Treatment Effects\nusing Generative Adversarial Nets. In: International Conference on Learning Representations\n(ICLR); 2018. .\n78 Hartford J, Lewis G, Leyton-Brown K, Taddy M.\nDeep IV: A Flexible Approach for\nCounterfactual Prediction. In: Proceedings of the 34th International Conference on Machine\nLearning (ICML); 2017. .\n79 Alaa AM, Weisz M, van der Schaar M. Deep Counterfactual Networks with Propensity Dropout.\nIn: Proceedings of the 34th International Conference on Machine Learning (ICML); 2017. .\n80 Mansournia MA, Etminan M, Danaei G, Kaufman JS, Collins G. Handling time varying\nconfounding in observational research. BMJ. 2017;359.\n81 Lim B, Alaa A, van der Schaar M. Forecasting Treatment Responses Over Time Using Recurrent\nMarginal Structural Networks. In: NeurIPS; 2018. .\n82 Li R, Shahn Z, Li J, Lu M, Chakraborty P, Sow D, et al. G-Net: A Deep Learning Approach to\nG-computation for Counterfactual Outcome Prediction Under Dynamic Treatment Regimes.\narXiv e-prints. 2020;p. arXiv:2003.10551.\n83 Bica I, Alaa AM, Jordon J, van der Schaar M. Estimating counterfactual treatment outcomes\nover time through adversarially balanced representations. In: International Conference on\nLearning Representations(ICLR); 2020. .\n84 Chen RTQ, Rubanova Y, Bettencourt J, Duvenaud D. Neural Ordinary Differential Equations.\nIn: Proceedings of the International Conference on Neural Information Processing Systems\n(NIPS); 2018. .\n85 Fry C, Brundage M. The M4 Forecasting Competition – A Practitioner’s View. International\nJournal of Forecasting. 2019;.\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2020-04-28",
  "updated": "2020-09-27"
}