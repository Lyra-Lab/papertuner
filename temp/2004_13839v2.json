{
  "id": "http://arxiv.org/abs/2004.13839v2",
  "title": "Neural translation and automated recognition of ICD10 medical entities from natural language",
  "authors": [
    "Louis Falissard",
    "Claire Morgand",
    "Sylvie Roussel",
    "Claire Imbaud",
    "Walid Ghosn",
    "Karim Bounebache",
    "Grégoire Rey"
  ],
  "abstract": "The recognition of medical entities from natural language is an ubiquitous\nproblem in the medical field, with applications ranging from medical act coding\nto the analysis of electronic health data for public health. It is however a\ncomplex task usually requiring human expert intervention, thus making it\nexpansive and time consuming. The recent advances in artificial intelligence,\nspecifically the raise of deep learning methods, has enabled computers to make\nefficient decisions on a number of complex problems, with the notable example\nof neural sequence models and their powerful applications in natural language\nprocessing. They however require a considerable amount of data to learn from,\nwhich is typically their main limiting factor. However, the C\\'epiDc stores an\nexhaustive database of death certificates at the French national scale,\namounting to several millions of natural language examples provided with their\nassociated human coded medical entities available to the machine learning\npractitioner. This article investigates the applications of deep neural\nsequence models to the medical entity recognition from natural language\nproblem.",
  "text": "Neural translation and automated \nrecognition of ICD10 medical entities \nfrom natural language \n \nAuthors \nLouis Falissard (corresponding author) \nCépiDc Inserm, Paris Saclay University, le Kremlin Bicêtre, France \nPostal address: 31 rue du Général Leclerc, 94270, Le Kremlin Bicêtre, France \nEmail address: louis.falissard@inserm.fr \nPhone number: +33679649178 \nClaire Morgand: CépiDc, Inserm, Le Kremlin Bicêtre, France  \nClaire Imbaud: CépiDc, Inserm, Le Kremlin Bicêtre, France \nWalid Ghosn: CépiDc, Inserm, Le Kremlin Bicêtre, France  \nKarim Bounebache: CépiDc, Inserm, Le Kremlin Bicêtre, France \nGrégoire Rey: CépiDc, Inserm, Le Kremlin Bicêtre, France \n \nWord count : 4200 \n \n \n \n \n \n \n \n \n \n \nAbstract \nBackground \nThe recognition of medical entities from natural language is an ubiquitous problem in the medical field, \nwith applications ranging from medical act coding to the analysis of electronic health data for public \nhealth. It is however a complex task usually requiring human expert intervention, thus making it \nexpansive and time consuming. \nThe recent advances in artificial intelligence, specifically the raise of deep learning methods, has \nenabled computers to make efficient decisions on a number of complex problems, with the notable \nexample of neural sequence models and their powerful applications in natural language processing. \nThey however require a considerable amount of data to learn from, which is typically their main \nlimiting factor. However, the CépiDc stores an exhaustive database of death certificates at the French \nnational scale, amounting to several millions of natural language examples provided with their \nassociated human coded medical entities available to the machine learning practitioner. This article \ninvestigates the applications of deep neural sequence models to the medical entity recognition from \nnatural language problem. \nMethods \nThe investigated dataset is based on every French death certificate from 2011 to 2016, containing \ninformation such as the subject’s age, gender, and the chain of events leading to his or her death both \nin French and encoded as ICD10 medical entities, for a total of around 3 million observations. The task \nof automatically recognizing ICD10 medical entities from the French natural language based chain of \nevent is then formulated as a type of predictive modelling problem known as a sequence-to-sequence \nmodelling problem. A deep neural network based model known as the Transformer is then slightly \nadapted and fit to the dataset. Its performance is then assessed on an exterior dataset and compared \nto the current state of the art. Confidence intervals for derived measurements are derived via \nbootstrap. \nResults \nThe proposed approach resulted in a test F-measure of .952 [.946, .957], which constitutes a significant \nimprovement on the current state of the art and its previously reported 82.5 F-measure assessed on a \ncomparable dataset. Such an improvement opens a whole field of new applications, from nosologist \nlevel automated coding to temporal harmonization of death statistics. \nConclusion \nThis article shows that deep artificial neural network can directly learn from voluminous datasets \ncomplex relationships between natural language and medical entities, without any explicit prior \nknowledge. Although not entirely free from mistakes, the derived model constitutes a powerful tool \nfor automated coding of medical entities from medical language with promising potential applications.  \n \nKeywords: machine learning, deep learning, machine translation, mortality statistics, automated \nmedical entity recognition, ICD10 coding \n \n \n1 Introduction \n \nThe democratization of electronic health record databases has opened countless opportunities to gain \nprecious insights in fields ranging from precision medicine to public health and epidemiology.  \nHowever, they still present many challenges, both technical and methodological, that make their \nexploitation cumbersome. As an example, natural language is extensively present in some health \nrelated databases, while being notoriously difficult to handle with traditional statistical methods, and \npreventing most international comparisons due to language barrier. In order to counter these \nundesirable properties, several approaches have been devised. For instance, by encapsulating most \nmedical entities in a standardized hierarchical tree structure, the ICD10 classification [1] offers a \npowerful and expressive way of organizing analytics compatible health databases. On the other hand, \nICD10 entities are significantly less intuitive for human users than natural language, and require years \nof training and practice to handle fluently. As a consequence, the data production of classification \nbased medical data is usually handmade, expansive and time consuming. Several attempts have been \nmade to design artificial intelligence based systems able to automatically derive medical entities from \nnatural languages, some with quite promising performance[2]–[4]. However, all of them fall short in \nautomating the complex production schemes inherent to medical databases, specifically in regard to \ntheir high data quality standards.  \nHowever, recent innovation in deep artificial neural networks have achieved significant progress in \nnatural language processing [5], [6]. In particular, their applications in the field of machine \ntranslation[7]–[9], fuelled by increases in both data and computing power, repeatedly bring automated \nsystems closer and closer to human level performances. Several attempts have been made to apply \nthese powerful techniques in an electronic health database setting, most of them with mitigated \nsuccess. As an example, the current state of the art in ICD10 entity recognition from natural language \nin death certificates still remains a combination of expert system and SVM based classical machine \nlearning[2]. Several explanations exist for this discrepancy between traditional machine translation \nand medical entity recognition. First, deep artificial neural network based methods are known to \nrequire huge amount of data for optimal performances. However, most experiment were either \nperformed with slightly out-of-date neural architectures, or with dataset sizes at least an order of \nmagnitude under what would be typically required[10]. On the other hand, the “Centre for \nEpidemiology on Medical Causes of Death” (CépiDc) has been storing French death certificates at the \nnational scale since 2011 in both natural language and ICD10 converted format. The entire database \namounts to just under 3 million death examples, thus providing with sensibly better settings to \ninvestigate the potential applications of deep neural networks in medical entities recognition. \nThe following article formulates the process of ICD10 entity recognition from natural language as a \nsequence to sequence statistical modelling problem (better known as seq2seq models in the academic \nliterature) and proposes to solve it with a variation one of the state of the art machine translation \nneural architecture, the Transformer. The following section focuses on describing the aforementioned \nstatistical modelling problem and overall methodology. Section 3 reports the result of experiments \ndone on the French CépiDc dataset as well as a comparison with the current state of the art. Section 4 \npresents a discussion on the model’s potential limitation through an error analysis and describes \npotential leads for improvement.  \n \n2 Material and methods \n \n \n2.1 Material \n \nThe dataset used during this study consists of every available death certificate found in the CépiDc \ndatabase for the years 2011 to 2016, representing just under 3 million training examples. These \ndocuments record various information about their subjects, including the chain of events leading to \nthe subject’s death, written by a medical practitioner.  \n2.1.1 Causal chain of death \n \nThe causal chain of death constitutes the main source of information available on a death certificate \nin order to devise mortality statistics. It typically sums up the sequence of events that led to the \nsubject’s death, starting from immediate causes (such as cardiac arrest) and progressively expanding \ninto the individual’s past to the underlying causes of death. WHO provides countries with a \nstandardized causal chain of events format, which France follows, alongside most developed countries. \nThis WHO standard asks of the medical practitioner in charge of reporting the events leading to the \nsubject’s passing to fill out a two-part form in natural language. The first part is comprised of 4 lines, \nin which the practitioner is asked to report the chain of events, from immediate to underlying cause, \nin inverse causal order (immediate causes are reported on the first lines, and underlying causes on the \nlast lines). Although 4 lines are available for reporting, they need not all be filled. In fact, the last \navailable lines are rarely used by the practitioner. The second part is comprised of two lines in which \nthe practitioner is asked to report “any other significant conditions contributing to death but not \nrelated to the disease or condition causing it” [11] that the subject may have been suffering from.  \nIn order to counter the language dependent variability of death certificates across countries, a pre-\nprocessing step is typically applied to the causal chain of events leading to the individual’s death, where \neach natural language based line on the certificate is converted into a sequence of codes defined by \nthe 10th revision of the International Statistical Classification of Diseases and Related Health Problems \n(ICD-10)[1]. ICD-10 is a medical classification created by WHO defining 14199 medical entities (e.g. \ndiseases, signs and symptoms…) distributed over 22 chapters and encoded with 3 or 4 alpha decimal \nsymbols (one letter and 2 or 3 digits), 5615 of which are present in the investigated dataset. Table 1 \nshows an example of a causal chain of events taken from an American death certificate, in both natural \nlanguage and ICD10 formats. \n \nLine Natural language \nICD10 encoding \n1 \nSTROKE IN SEPTEMBER LEFT HEMIPARESIS \nI64 G819 \n2 \nFALL SCALP LACERATION FRACTURE HUMERUS S010 W19 S423 \n3 \nCORONARY ARTERY DISEASE \nI251 \n4 \nACUTE INTRACRANIAL HEMORRHAGE \nI629 \n6 \nDEMENTIA DEPRESSION HYPERTENSION \nF03 F329 I10 \n Table 1: Example of cause chain of death, in natural language and as ICD10 \ncodes. Some natural language lines correspond to several ICD10 codes, whose \norders matter in the overall coding process \n \nAs aforementioned, the process of converting the natural language based causal chain of events \nleading to death in an ICD10 format is the main focus of this article. Consequently, the latter will be \nselected as target variable and the former as the main explanatory variable for the neural network \nbased predictive model defined further. \nFor reasons related to the underlying cause of death production process, the natural language based \nchain of events and its ICD10 encoded counterpart suffer from alignment errors at the line level, as \nshown in table 2. Although qualitatively deemed quite rare, this misalignment phenomenon brings \nsufficient noise in the dataset to prevent model convergence while fitting models with line level \nsentence pairs.  \n \n \n \n \n \nLine Natural language \nICD10 encoding \n1 \nSTROKE IN SEPTEMBER LEFT HEMIPARESIS \nI64 G819 \n2 \nFALL SCALP LACERATION FRACTURE HUMERUS S010 W19 S423 \n3 \nCORONARY ARTERY DISEASE \nI629 I251 \n4 \nACUTE INTRACRANIAL HEMORRHAGE \n \n6 \nDEMENTIA DEPRESSION HYPERTENSION \nF03 F329 I10 \n \nTable 1: Same certificate as displayed in table 1 showcasing the \nmisalignment phenomenon. The ICD10 code related to line 4 (both in red) has \nbeen moved to line 3 by a human coder. Concatenating lines in a backward \nfashion restores alignment while preserving ordering  \n \nIn order to bypass this critical flaw in the investigated dataset, a decision was chosen to consider as \ninput and target variables the certificates lines concatenated in a backward fashion (from line 6 to \nline 1), as can be seen in figure 1. This slight change in data format does not significantly alter the \nproblematic at hand, as the investigated model is still trained to recognize ICD10 encoded medical \nentities from natural language. If anything, the modified modelling problem can be expected to be \nmore difficult, as both the variance and dimensionality of both input and target variables have \nincreased. Several methods are available to retrieve line level aligned predictions from a model \ntrained in such a configuration, for instance using a combination of transfer learning and pruned tree \nsearch. \n \n \nFig. 1 Left: the original modelling problem. Each certificate line is taken as an \ninput variable to predict its corresponding ICD10 code line. Right: The modified \ninvestigated problem. All certificate lines are concatenated and taken as an input \nvariable to predict the corresponding concatenated ICD10 code line \n \n2.1.2 Miscellaneous variables \n \nFrom gender to place of birth, a death certificate contains various additional information on its subject \nbesides the chain of events leading to death. As some of these items are typically used by both expert \nsystems and human coders to detect ICD10 entities in the chain of events, they present an interest as \nexplanatory variable for the investigated predictive model. After consultation with expert coders, the \nfollowing items available on French death certificate were selected as additional exogenous variables:  \n \ngender (2 states categorical variables), \n \nyear of death (6 states categorical variables), \n \nage, factorized into 5 years’ intervals with the exception of subject less than one-year-old, \nwhich were divided into two classes following whether they were more than 28-day-old, \n \norigin of the death certificate (2 states categorical variables, either from the electronic or \npaper based death certification pipeline). \nStrictly speaking, the subject’s year of passing should only have a limited effect on the relationship \nbetween natural language and its contained medical entities. However, the WHO defined coding rules, \nas well as their interpretations by human coders slightly evolve over the years. As a consequence, the \nmodel should benefit, in term of predictive performance, from being able to differentiate between \ndifferent years. \nSimilarly, the impact of the certificate’s origin on the model’s predictive power is not entirely obvious \nat first sight. However, the paper based certificates data entry process is handled by human through \nspeech recognition technology. In addition, the entry clerks are asked to apply a small set of \nnormalization rules to the natural language. Electronic death certificates, however, are received \ndirectly from the medical practitioner as is. As a consequence, distribution shifts are to be expected \nfrom paper to electronic based chain of events, and including this information as an explanatory \nvariable might be beneficial to the model’s predictive power. \n \n2.2 Method \n \nWith both the explanatory and target variables well defined, the investigated modelling problem can \nbe defined as follows: \n𝑃(𝐼𝐶𝐷|𝑁𝐿, 𝐴, 𝐺, 𝑌, 𝐸, 𝜃) = 𝑓𝜃(𝑁𝐿, 𝐴, 𝑌, 𝐺, 𝐸) \nWith: \n \n \n𝐼𝐶𝐷∈⟦0, 1⟧561620One line of a certificate encoded as ICD10 entities \n \n𝑁𝐿∈⟦0, 1⟧𝑉𝐿 the line in natural language, tokenized with a vocabulary 𝑉 and of maximum \nsequence length 𝐿 \n \n𝐴∈⟦0, 1⟧25 the categorized age \n \n𝑌∈⟦0, 1⟧6 the year of death \n \n⟦0, 1⟧∈ℝ2 the gender \n \n⟦0, 1⟧∈ℝ2 the death certificate’s origin \n \n𝑓𝜃 a mapping from the problem’s input space to its output space, parameterized in 𝜃 a real-\nvalued vector (typically a neural network) \nTheoretically, the derived modelling problem is typical of traditional statistical modelling problems, \nand could be solved using multinomial logistic regression. In practice, however, this approach presents \na significant drawback. In this setting, the investigated target variable constitutes a categorical variable \nwith 561620 (20 ICD10 codes sequences, each of which can take 5616 values) distinct states, thus \nrendering the analysis untractable both in term of computational expanses and sample size \nrequirements. This type of approach, however, makes no use of the data’s inherent sequential nature, \nwhich allows to rewrite the investigated modelling problem as follows: \n𝑃(𝐼𝐶𝐷|𝑁𝐿, 𝐴, 𝐺, 𝑌, 𝐸, 𝜃) = 𝑃((𝐼𝐶𝐷1, −, 𝐼𝐶𝐷𝑛)|𝑁𝐿, 𝐴, 𝐺, 𝑌, 𝐸, 𝜃) ∀𝑛∈⟦1, 20⟧  \n= ∏𝑃(𝐼𝐶𝐷𝑖|(𝐼𝐶𝐷1,−,𝐼𝐶𝐷𝑖−1),𝑁𝐿,𝐴,𝐺,𝑌,𝐸,𝜃) ∀𝑛∈⟦1,20⟧\n𝑛\n𝑖=1\n \nWith: \n \n𝐼𝐶𝐷𝑖∈⟦0, 1⟧5616 ∀𝑖∈⟦1, 20⟧ the ith code present on the code line \n \nFactors in the right hand side of equation 2 can be interpreted as distinct predictive modelling problem, \nall with an output variable distributed across all ICD10 codes. Although still highly dimensional, \npredicting output variables of such dimensionality is typically tractable with modern machine learning \ntechniques[7]. They present however two significant drawbacks for traditional modelling techniques: \n \nThe number of output variables to predict varies across observations in the dataset (not all \ndeath certificates have 20 ICD10 codes) \n \nThe output variables’ distributions are conditioned on previous ones \nThis particular formulation is known in the deep artificial neural network community as a sequence to \nsequence modelling problem[7], and has been an active area of research for the past few years. As one \nof the state of the art neural architecture devised in the field, the Transformer[9] was chosen as the \npredictive model investigated in the following experiments. It was recently outperformed by the \nEvolved Transformer[12], a variation on the former. However, both approaches were investigated and \nyielded similar results. The Transformer architecture was retained due to its availability of official and \nmaintained implementations, and the final results further displayed were obtained using an ensemble \nof 7 such models. \nSeveral specificities in the aforedefined modelling problem required small adaptations to the \nTransformer architecture. However, the authors feel their technicity fall outside the scope of this \narticle. The interested reader will however find a complete description of these modifications in the \nannex documents. \n2.3 Training and evaluation methodology \n \nThe investigated model was trained using all French death certificates from years 2011 to 2016. 5000 \ncertificates were randomly excluded from each year and distributed into a validation set for hyper-\nparameter fine-tuning, and a test dataset for unbiased prediction performance estimation (2500 each), \nresulting in three datasets with following sample sizes: \n \nTraining dataset: 3240109 records \n \nValidation and test dataset: 30000 records each \nThe model was adapted from Tensorflow’s (a python-based distributed machine learning framework) \nofficial Transformer implementation. Training was performed on three NVidia RTX 2070 GPUs \nsimultaneously using a mirrored distribution strategy using a variant of stochastic gradient descent, \nthe Adam optimization algorithm. \nHyper-parameters were first initialized following the Transformer’s authors in their base setting. \nFurther fine tuning of a selected number of hyper-parameters was performed using a random search \nguided on the validation set. The interested reader will find a complete description of the training \nprocess and hyper-parameter values defining this model in annex. \nAfter training, the model’s predictive performance was assessed on the test dataset (excluded prior to \ntraining, as mentioned earlier), and compared to the current state of the art, obtained by the \n“Laboratoire d’Informatique et de Mécanique pour les Sciences de L’ingénieur” (LIMSI) during the 2017 \nCLEF eHealth challenge[2]. As the CLEF eHealth challenge only provided electronic certificates to the \ncontestants, and in order to ensure comparability, the model’s performances were assessed on paper \nand electronic certificates separately. For the same reason, the performance metrics used for model \nevaluation were selected as follows: \n \n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛= \n𝑇𝑟𝑢𝑒 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑠\n𝑇𝑟𝑢𝑒 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑠+ 𝐹𝑎𝑙𝑠𝑒 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑠 \n𝑅𝑒𝑐𝑎𝑙𝑙= \n𝑇𝑟𝑢𝑒 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑠\n𝑇𝑟𝑢𝑒 𝑝𝑜𝑠𝑖𝑡𝑖𝑣𝑒𝑠+ 𝐹𝑎𝑙𝑠𝑒 𝑛𝑒𝑔𝑎𝑡𝑖𝑣𝑒𝑠 \n𝐹-𝑚𝑒𝑎𝑠𝑢𝑟𝑒 = 2 ∙ 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 ∙𝑅𝑒𝑐𝑎𝑙𝑙 \n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 +  𝑅𝑒𝑐𝑎𝑙𝑙 \nWith: \n \nTrue positives the number of codes predicted by the model that are present in the test set’s \ntrue output target, \n \nFalse positives the number of codes predicted by the model that are not present in the test \nset’s true output target, \n \nFalse negatives the number of codes not predicted by the model that are present in the test \nset’s true output. \nThe informed reader might find these metrics stray away from common machine translation system \nbenchmarking metrics such as BLEU or negative log perplexity scores[7]–[9], [13], but the former were \nthe only ones used in comparable work. As BLEU and negative log perplexity have close to no absolute \ninterpretability without comparisons to alternative methods, their use was discarded from the \nexperiment. In order to present the reader with a more comprehensive view of the proposed \napproaches’ performances, these accuracy metrics were also derived on a per chapter basis, again on \nthe same test set, and confidence intervals were computed using bootstrap. \n3 Results \n \nThe ensemble of transformer models were trained as aforedescribed for approximately 3 weeks, and \nthe final ensemble’s predictive performance as well as the current state of the arts’ are reported in \nTable 1. As previously mentioned, the current state of the arts’ performances were assessed on \nelectronic certificates only, and should as a consequence be compared to the proposed approach \nperformance on a similar situation. Because paper based certificates are still sensibly more common \nthan their electronic counterparts in France (approximately 90% of certificates in the dataset are paper \nbased), overall and paper specific performances are also displayed.   \nApproach \nF-measure \nPrecision \nRecall \nCurrent state of the art (LIMSI) \n.825 \n.872 \n.784 \nProposed approach (electronic \ncertificates) \n.952 [.946, .957] .955 [.95, .96] \n.948 [.943, .954] \nProposed approach (paper certificates) \n.942 [.941, .944] \n.949 [.947, .95] \n.936 [.934, .937] \nProposed approach (all certificates) \n.943 [.941, .944] \n.949 [.948, .951] \n.937 [.935, .938] \nTable 3 F-measure of the current state of the art and the proposed approach, with \ntheir corresponding 95% confidence intervals, derived by bootstrap. Confidence \nintervals were not provided in the LIMSI’s publication and are therefore not displayed.  \n \nThe proposed approach shows an F measure 73% closer to a perfect score when compared to the \ncurrent state of the art. In addition to its substantial improvement in F-measure, the proposed \napproach displays significantly more balanced precision and recall scores than the LIMSI’s method \n(from 5% relative difference to less than 1%). \nA surprising result, however, lies in the model’s lower performances on paper certificates. Indeed, the \nstandardization they receive due to their voice based data collecting process considerably reduces \nvariance and prevents any misspelling of words in the data potentially present in electronic based \ncertificates. As a consequence, model performance on the former should be expected to be higher. A \npotential explanation for this phenomenon lies in the potential for missing data in paper based \ncertificates. Indeed, when confronted to poorly written words, data clerks are allowed to replace them \nwith a “!” symbol when the word is estimated unreadable (present in approximately 10% of paper \nbased certificates). Medical coders, however, are usually more efficient in guessing the words from the \nwritten certificates (typically with the addition of contextual clues). A purely text based approach \nhowever, is then limited to pure guess on those observations with missing data, logically leading to \npoorer performance. This phenomenon being absent from electronic based certificates, it constitutes \na promising candidate in explaining this unexpected difference of performance. In addition, model \nperformances on paper certificates not containing any “!” symbol in the test set led to 96.2% F-\nmeasure, thus providing strong evidence to support this hypothesis. \n4 Discussion \n \nAlthough the proposed approach significantly outperforms the current state of the art, neural network \nbased methods are known to present several drawbacks that can significantly limit their application in \nsome situations. Typically, the current lack of systematic methods to interpret and understand neural \nnetwork based model and their decision processes can lead the former to perform catastrophically on \nill predicted cases, independently from their high predictive performances. As a consequence, the \nproposed model behaviour in ill predicted cases require careful analysis. In addition, such an \ninvestigation can lead to significant insights potentially relevant when applying the derived model in \npractical applications.  \n \n4.1 Per-chapter quantitative analysis \n \nOne simple, straightforward approach to understanding the model’s weakness, lies in assessing its \nperformance on a finer grain level, for instance by identifying false positives and negatives not only at \nthe global level, but per ICD10 chapters, as can be seen in table 4.  \nIt appears from these graphs that although the most prevalent medical entities are associated with \nlow false positive and negative rates, some rarer chapters are associated with unreasonably high error \nrates. Depending on their prevalence and accuracies, these chapters can be classified into two distinct \ncategories: \n \nChapters associated with unreasonably high error rates but extremely low prevalence such as \n“diseases for the ear and mastoid process” or “pregnancy, childbirth and the puerperium”. \nHowever, these entity groups remain rare enough within the dataset to allow for alternative \ntreatments, like manual evaluation, for instance. \n \nChapters associated with high error rates (although lower than the former) but with significant \nprevalence such as “External causes of morbidity and mortality” or “Injury, poisoning and \ncertain other consequences of external causes”.  \nThe task of identifying these potential mistakes, however, is not entirely trivial depending on whether \nmistakes are of false positive or false negative types. Indeed, potential false positives errors are directly \nidentifiable within the predicted ICD10 code sequences. As a consequence, coding quality control for \nthis mistake type should be fairly straightforward to implement (one could for instance manually \nreview all code sequences containing codes related to “Pregnancy, childbirth and the puerperium” \nsystematically. Potential false negative errors, however, are inherently significantly harder to identify, \nand require further investigation, for instance through association rules analysis.  \nA number of promising leads are already available and should reasonably improve upon the proposed \napproach: \n \nTraining methods adapted to imbalanced datasets such as up sampling or loss weighting \n \nData augmentation for rare medical entities \n \nAddition of information to the model (prenatal related death, for instance, are explicitly \ndefined as such on certificates) \n \nHybrid approach with traditional NLP approaches, typically less expensive in term of sample \nsize requirements \n \nICD10 chapter \nFalse \npositives (%)  \nFalse \nnegatives (%) \nPrevalence \n(%) \nDiseases of the circulatory system \n3.75 \n4.98 \n22.4 \nSymptoms, signs and abnormal clinical and \nlaboratory findings, not elsewhere classified \n3.87 \n4.12 \n21.8 \nNeoplasms \n4.07 \n5.07 \n15.9 \nDiseases of the respiratory system \n3.02 \n4.00 \n8.76 \nEndocrine, nutritional and metabolic diseases \n2.17 \n3.44 \n4.83 \nDiseases of the nervous system \n2.70 \n4.12 \n3.89 \nMental and behavioural disorders \n2.88 \n4.14 \n3.58 \nDiseases of the digestive system \n5.72 \n8.10 \n3.53 \nFactors influencing health status and contact with \nhealth services \n19.2 \n19.6 \n3.08 \nDiseases of the genitourinary system \n5.45 \n7.59 \n2.71 \nExternal causes of morbidity and mortality \n16.6 \n23.5 \n2.57 \nCertain infectious and parasitic diseases \n7.98 \n9.23 \n2.55 \nInjury, poisoning and certain other consequences of \nexternal causes \n14.0 \n19.8 \n2.07 \nDiseases of the blood and blood-forming organs and \ncertain disorders involving the immune mechanism \n6.72 \n12.2 \n0.77 \nDiseases of the musculoskeletal system and \nconnective tissue \n12.2 \n17.3 \n0.62 \nDiseases of the skin and subcutaneous tissue \n8.72 \n8.16 \n0.51 \nCertain conditions originating in the perinatal period \n14.5 \n20.5 \n0.16 \nCongenital malformations, deformations and \nchromosomal abnormalities \n22.4 \n25.6 \n0.15 \nDiseases of the eye and adnexa \n4.93 \n13.6 \n0.076 \nCodes for special purposes \n24.0 \n34.0 \n0.047 \nDiseases of the ear and mastoid process \n5.6 \n33.3 \n0.017 \nPregnancy, childbirth and the puerperium \n50 \n33.3 \n0.0056 \n \nTable 4 Prevalence, false positives and negatives rates for each ICD10 chapter, sorted \nin descending order by prevalence \n \n \n \n \n \n \n \n \n4.2 Score calibration fitness assessment \n \nThe model being fit in a similar fashion to multinomial logistic regression, it not only yields a prediction, \nbut an associated score similar to a confidence probability. If properly calibrated, this score can offer \npowerful insights regarding the prediction’s quality at the individual level. Typically, a “good” score \nwould be expected to show higher values in cases where the predicted ICD10 sequence is of good \nquality (typically in term of F-measure) and a low one when mispredicting. Efficient assessment of such \nscores in traditional machine learning problems is typically done through visualization of ROC curves. \nHowever, the sequential, multinomial nature of the investigated problem renders this approach ill \ndefined. The plot found in figure 2, while conceptually similar to a ROC curve, was derived following a \nslightly different approach in order to efficiently appreciate the model score’s quality: \n \nA grid of score threshold values was defined (uniform grid with 0.01 intervals) \n \nFor every given threshold value were computed the percentage of predictions with inferior or \nequal scores (considered as rejected predictions due to poor score), as well as the F-measure \nperformance on the accepted predictions \n \nPercentage of accepted certificates and F-measurement were scatter plotted against each \nother, with threshold value displayed as points’ colour  \n \nFig. 2 Percentage of rejected predictions versus F-measure on accepted ones. The \nscore threshold value defining the accepted predictions are displayed as point colo urs \n \nBy showing a clean, increasing relationship between number of rejected predictions and F-measure on \nthe remaining, figure 2 strongly indicates good score calibration. As an example, setting a threshold \nscore of approximately 50% allows for a fully automated coding of 80% of the certificates presents in \nthe test set with a corresponding F-measure exceeding 98%. \n4.3 Human based qualitative error analysis \n \nThe error analysis carried on so far allowed for the assessment of the model’s strength and weaknesses \non the global level. They however fail to yield any interesting insight regarding potential model biases, \nfor instance towards specific coding rules. Indeed, the coding of medical entity from natural language, \nespecially with regard to mortality statistics, is subject to a number of coding rules depending on \ncontext or pathology, with a level of specificity oftentimes reaching casuistry.  \nIn addition, all results presented so far with a model error defined as a disagreement between the \nmodel’s output and the information contained in the database. However, building a medical database \nis a complex, mostly human based process. As such, an inevitable amount of noise is to be expected in \nthe ICD10 coding process, potentially leading to negative bias toward our performance and error \nevaluations, the aforementioned presence of missing data in the natural language being a perfect \nexample of such phenomenon. \nOne straightforward, although fairly time-consuming approach to address these two considerations \ncan be derived from human observation of disagreement cases by a ICD10 coding specialist. Two \nexperiments were conducted following this idea.  \nFirst, 99 ill predicted certificates were selected at random from the test set and shown to the medical \npractitioner referent and final decision-maker on ICD10 mortality coding in France, who was asked for \neach certificates to: \n \nExtract all the ICD10 medical entities present on each death certificates by herself, from the \ninformation the proposed model had access to, \n \nGive a qualitative comment on the investigated model and database’s outputs compared to \nhers. \nThe derived ICD10 sequences were then compared to both the actual values contained in the dataset \nand those predicted by the derived model with the aforedefined metrics used a similarity distance. For \nbetter comparability, these statistics are reported both on: \n \nAll certificates in table 5 \n \nCertificates without missing data in the natural language based causal chain of death (by \nexcluding certificates containing a “!” symbol) in table 6 \n \nDatabase or prediction \nF-measure \nPrecision \nRecall \nDatabase against \nmedical expert \n.891 [.859, .920]  \n.868 [.827, .905] \n.916 [.888, .940] \nPrediction against \nmedical expert \n.894 [.867, .918] \n.894 [.860, .923] \n.894 [.868, .920] \nTable 5. F-measure, precision and recall (with their 95% confidence intervals) of both \nthe database and the model’s predictions against the medical expert for all sampled \ncertificates \n \nDatabase or prediction \nF-measure \nPrecision \nRecall \nDatabase against \nmedical expert \n.909 [.873, .939]  \n.901 [.855, .938] \n.917 [.888, .940] \nPrediction against \nmedical expert \n.877 [.845, .910] \n.877 [.837, .911] \n.877 [.847, .906] \nTable 5. F-measure, precision and recall (with their 95% confidence intervals) of both \nthe database and the model’s predictions against the medical expert for sampled \ncertificates without missing data \n \nTables 5 and 6, show no significant difference in prediction performance between the proposed \napproach and the current data production process (based on a combination of expert system and \nhuman coders). When including certificates containing missing text, the proposed model slightly \noutperforms the current system (although not significantly), further confirming the hypothesis that \nthe performance metrics reported in the result section are negatively biased. When excluding these \nproblematic observations, the expert system/human coder combination slightly outperforms the \nproposed approach, although still not significantly. The observed drop in the investigated model’s \nperformance when excluding missing text certificates might seem surprising at first, as the information \navailable to both the medical practitioner and the model remains the same regarding of this exclusion. \nHowever, this performance gap might be explained through selection bias, as faulty predictions were \nselected by their difference with the database’s ICD10 content. The observed F-measure of .974 \nobserved between model prediction and medical expert opinion on certificates with missing text \nprovides strong evidence to confirm this hypothesis.   \nFrom the qualitative comments made by the medical experts, three major types of model errors could \nbe defined: \n \nIn 16% of cases, disagreement between the current data production process and the proposed \napproach was due to missing information in the input text. On these specific cases, the F-\nmeasure between model output and medical expert decision was measured at .974 (an \nexample of such error case can be seen in the annex, in table MA4) \n \nIn 14% of cases, the correct ICD10 sequence is dependent on highly contextual cues or external \nknowledge of world behaviour (e.g. Someone found dead at the bottom of stairs is quite likely \nto have suffered a fall. An example of such error case can be seen in the annex, in table MA3) \n \nIn 12% of cases, the correct ICD10 sequence is dependent on highly nonlinear, almost casuistic \nrules and are typical examples of scenarios where a hybridized deep learning and expert-based \nsystem should be beneficial (an example of such error case can be seen in the annex, in table \nMA4) \n \nThe remaining cases did not elicit any comment from the medical expert. \nFinally, in a second experiment, the medical expert’s ability to discriminate between human coding \nand the proposed approach was assessed, in a Turing test-like approach. To do so, a hundred \nadditional ill-predicted certificates were sampled at random from the test set, and associated with \ntheir two corresponding, anonymized and shuffled ICD10 sequences (from both the proposed \napproach and the database). The medical expert was then asked, after careful reviewing of each \ncertificate, to answer the question “Which of these ICD10 sequences candidates was derived by the \nhuman/expert system combination”. After exclusion of certificates containing missing text data \n(where the human coder was easily identifiable due to the apparently out of context additional codes \nas seen in table MA2), the medical expert was able to correctly identify the human in 62.0% [50.7, \n73.2] of cases, which is significantly better than random guessing (although barely). \n \n5 Conclusion \n \nIn this article, the task of automatic recognition of ICD10 medical entities from natural language in \nFrench was presented as a seq2seq modelling problem, well known in the deep artificial neural \nnetwork academic literature. From this consideration, the performances of a well-known approach in \nthe field, consisting of an ensemble of Tranformer models, was investigated using the CépiDc database \nand shown to obtain a new state of the art. The derived model’s behaviour was thoroughly assessed \nfollowing different approaches in order to identify potential weaknesses and leads for improvements. \nAlthough the proposed approach significantly outperforms any other existing automated ICD10 \nrecognition systems on French free-text, the question of method transferability to other languages \nrequire more investigations.  \nThe substantial performances reported in this article open an entire range of promising applications in \nvarious medical related fields, from medical act automated coding to advanced natural language based \nanalysis for epidemiology. However, these interesting opportunities are oftentimes prohibited by \nthese methods’ massive drawbacks, mostly their requirement for millions of annoted observations to \nperform well. Mortality datasets, in spite of their specificity, provide researchers with huge, clean and \nmultilingual medical text data perfectly fit for the application of deep neural networks. As a \nconsequence, and keeping in mind neural network’s strong transfer learning capability, the authors \nfirmly believe that mortality data constitutes one of the most promising point of entry into modern \nnatural language processing methods applications in the biomedical sciences.     \n \n6 Bibliography \n \n \n[1] \n‘WHO | ICD-10 online versions’. [Online]. Available: \nhttps://www.who.int/classifications/icd/icdonlineversions/en/. [Accessed: 21-Jan-2020]. \n[2] \nA. Névéol et al., ‘CLEF eHealth 2017 Multilingual Information Extraction task Overview: ICD10 \nCoding of Death Certificates in English and French.’, in Workshop of the Cross-Language \nEvaluation Forum, Dublin, Ireland, 2017. \n[3] \nA. Névéol et al., ‘Clinical Information Extraction at the CLEF eHealth Evaluation lab 2016’, CEUR \nWorkshop Proc, vol. 1609, pp. 28–42, Sep. 2016. \n[4] \n‘A Deep Learning Method for ICD-10 Coding of Free-Text Death Certificates | SpringerLink’. \n[Online]. Available: https://link.springer.com/chapter/10.1007/978-3-319-65340-2_12. \n[Accessed: 24-Oct-2019]. \n[5] \nT. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, ‘Distributed Representations of \nWords and Phrases and Their Compositionality’, in Proceedings of the 26th International \nConference on Neural Information Processing Systems - Volume 2, USA, 2013, pp. 3111–3119. \n[6] \nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘BERT: Pre-training of Deep Bidirectional \nTransformers for Language Understanding’, arXiv:1810.04805 [cs], May 2019. \n[7] \nK. Cho et al., ‘Learning Phrase Representations using RNN Encoder-Decoder for Statistical \nMachine Translation’, arXiv:1406.1078 [cs, stat], Jun. 2014. \n[8] \nJ. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, ‘Convolutional Sequence to \nSequence Learning’, arXiv:1705.03122 [cs], Jul. 2017. \n[9] \nA. Vaswani et al., ‘Attention Is All You Need’, arXiv:1706.03762 [cs], Dec. 2017. \n[10] J. Sˇeva, M. Sanger, and U. Leser, ‘WBI at CLEF eHealth 2018 Task 1: Language-independent \nICD-10 coding using multi-lingual embeddings and recurrent neural networks’, p. 14. \n[11] ‘Completing a medical certificate of cause of death (MCCD)’, GOV.UK. [Online]. Available: \nhttps://www.gov.uk/government/publications/guidance-notes-for-completing-a-medical-\ncertificate-of-cause-of-death. [Accessed: 24-Apr-2019]. \n[12] D. R. So, C. Liang, and Q. V. Le, ‘The Evolved Transformer’, arXiv:1901.11117 [cs, stat], May \n2019. \n[13] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‘Bleu: a Method for Automatic Evaluation of \nMachine Translation’, in Proceedings of the 40th Annual Meeting of the Association for \nComputational Linguistics, Philadelphia, Pennsylvania, USA, 2002, pp. 311–318, doi: \n10.3115/1073083.1073135. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nNeural \ntranslation \nand \nautomated \nrecognition of ICD10 medical entities from \nnatural language – Annex \n \n1 Pre-processing \n1.1 Text standardization \n \nMinimal standardization was applied to the text data. The 6 lines present on the death certificates \nwere concatenated with a “, ” separator, and the two following steps were applied as the only text \ncleaning treatments: \n \nAll letters were put to lower case \n \nAll space based separator were collapsed to “ “ \n1.2 Tokenization for rare words \n \nIn order to reduce the problem’s dimensionality and handle rare words in the dataset, Byte pair \nencoding[1], the standard methodology used in the recent machine translation academic literature, \nwas used. Its implementation used for the experiments reported in this article can be found in the \nofficial Tensorflow Transformer repository[2]. \nThe algorithm was applied on the entire training dataset and the derived tokenization were of \ndimensionality 500 and 2033 for the ICD10 and French corpora, respectively. \n \n2 Model definition \n \nThe model itself follows the traditional transformer architecture[3]. The model’s official Tensorflow \nimplementation was used for the experiments[3]. However, the traditional Transformer model doesn’t \nallow for the treatment of additional conditional variables. In order to include the latter in the model, \na similar methodology than that followed in [4] was chosen: \n \nEach conditional variable was linearly projected linearly in an embedding space whose \ndimensionality match the transformer’s hidden size hyper-parameter \n \nThese linear projections are added in an element-wise fashion to the embedded token \nsequence \n \nThe transformer model is used as defined in its original article on the resulting embedded \nsequence  \n \nFig. 1 Transformer adaptation for the handling of conditional variables  \n \n \n3 Hyperparameter search \n \n \nHyperparameter tuning was done with a random search guided by validation set’s F-measure results. \nThe following variable were randomly sampled from the further specified probability distributions: \n \nModel’s hidden size: sampled from a uniform random distribution between 256 and 512 \n \nBatch size: For computational reasons, the batch size was defined as (100 * 512 / hidden_size) \n \nLearning rate: Uniformly sampled from discrete values 1. or 2. (note that this value doesn’t \nconstitute the actual learning rate, which is modified by the function “get_learning_rate” \n \nLayer_postprocess_dropout: sampled from a uniform random distribution between 0 and 0.2 \n \nAttention_dropout: sampled from a uniform random distribution between 0 and 0.2 \n \nRelu_dropout: sampled from a uniform random distribution between 0 and 0.2 \n \nAll other parameters were fixed as recommended by the BASE_MULTI_GPU settings provided in the \ntensorflow transformer official implementation. \n40 models were trained with different hyperparameters sampled from these distributions, the best set \nof hyperparameter was then used to train a new set of model for ensembling. \n \n4 Ensembling method \n \nDue to computational reasons, the traditional method for ensembling neural translation model (logits \naveraging during the beam search process) could not be used. The following alternative was used \ninstead: \n \nGet the prediction from each model \n \nCompute F-measurements between all prediction candidates \n \nSelect the prediction that shows highest F-measurements with other candidates on average \nThe ensemble of models was selected by a greedy search on all the models trained for the experiment \n(40 models trained during the hyperparameters search and additional models trained with the best \nhyperparameter set) guided by the F-measurement on the validation set \nThe derived score was taken as the prediction scores’ average. \n \n5 Final ensemble hyperparameters \n \nThe final ensemble found by greedy exploration consisted of 7 different models, 5 of which were \ntrained with the best set of hyperparameters revealed by the random hyperparameter searchs. The \nthree distinct sets of hyperparameters can be found in table 1. \n \nHyperparameter \nSet 1 (best set) Set 2 Set 3 \nBatch size  \n172 \n152 \n164 \nHidden size \n296 \n336 \n312 \nLearning rate \n2. \n2. \n2. \nLayer postprocess dropout .073 \n.12 \n.005 \nAttention dropout \n.105 \n.030 \n.017 \nRelu dropout \n.173 \n.030 \n.20 \nTable MA1. Sets of hyperparameters for the different models used in the final \nensemble \n \n \n6 Error examples \n \nText \nhta, insuffisance cardiaque, anévrisme aorte !, !, asystolie ! \nPredicted ICD10 \nI10 I509 I714 I500 \nDatabase ICD10 \nI10 I509 I714 H570 I500 R068 \nTrue ICD10  \nI10 I509 I714 I500 \nTable MA2. Example of “missing data” type error. The database shows two additional \ncodes that are not present in the text according to the medical expert. These codes \nare probably associated with the “!” present in the text, and were derived from a \nhuman coder reading the handwritten death certificate.  \n \n \nText \nacfa, hta, connu vertige, retrouvé terre bas escalier \nICD10 prediction \nI48 I10 R42 R98 \nDatabase ICD10 \nI48 I10 R42 W10 \nTrue ICD10  \nI48 I10 R42 W10 \nTable MA3. Example of contextual error. The proposed approach converts “retrouvé \nterre bas escalier” (which roughly translates to “found at the bottom of the stairs”) to \nR98 “unattended death”. Both human coders are able to deduce that the subject \nprobably fell down the stairs and use the ICD10 code W10 “Fall on and from steps”  \n \n \nText \ncardiopathie ischemique avec triple pontage, anevrisme aortique, cancer de la \nvessie, hta, arret cardio - respiratoir \nPredicted ICD10 \nI259 Z951 I719 C679 I10 R092 \nDatabase ICD10 \nI259 I251 I719 C679 I10 R092 \nTrue ICD10  \nI259 I251 I719 C679 I10 R092 \nTable MA4. Example of error caused by a coding rule. I251 and Z951 are both suitable \nfor “triple pontage” (Coronary artery bypass surgery). However, the M4 mortality \ncoding rule (Special instructions on surgery and other medical procedures) dictates the \ncode choice   \n \n \n \n \n \n \n \n \n \n \n \n7 Bibliography \n \n[1] R. Sennrich, B. Haddow, and A. Birch, ‘Neural Machine Translation of Rare Words with Subword \nUnits’, in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics \n(Volume 1: Long Papers), Berlin, Germany, 2016, pp. 1715–1725, doi: 10.18653/v1/P16-1162. \n[2] ‘tensorflow/models’, \nGitHub. \n[Online]. Available: \nhttps://github.com/tensorflow/models. \n[Accessed: 22-Jan-2020]. \n[3] ‘[1706.03762] Attention Is All You Need’. [Online]. Available: https://arxiv.org/abs/1706.03762. \n[Accessed: 04-Feb-2019]. \n[4] L. Falissard et al., ‘A deep artificial neural network based model for underlying cause of death \nprediction from death certificates’, arXiv:1908.09712 [cs, stat], Aug. 2019. \n \n",
  "categories": [
    "cs.CL",
    "cs.CY",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-03-27",
  "updated": "2020-05-06"
}