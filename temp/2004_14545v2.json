{
  "id": "http://arxiv.org/abs/2004.14545v2",
  "title": "Explainable Deep Learning: A Field Guide for the Uninitiated",
  "authors": [
    "Gabrielle Ras",
    "Ning Xie",
    "Marcel van Gerven",
    "Derek Doran"
  ],
  "abstract": "Deep neural networks (DNNs) have become a proven and indispensable machine\nlearning tool. As a black-box model, it remains difficult to diagnose what\naspects of the model's input drive the decisions of a DNN. In countless\nreal-world domains, from legislation and law enforcement to healthcare, such\ndiagnosis is essential to ensure that DNN decisions are driven by aspects\nappropriate in the context of its use. The development of methods and studies\nenabling the explanation of a DNN's decisions has thus blossomed into an\nactive, broad area of research. A practitioner wanting to study explainable\ndeep learning may be intimidated by the plethora of orthogonal directions the\nfield has taken. This complexity is further exacerbated by competing\ndefinitions of what it means ``to explain'' the actions of a DNN and to\nevaluate an approach's ``ability to explain''. This article offers a field\nguide to explore the space of explainable deep learning aimed at those\nuninitiated in the field. The field guide: i) Introduces three simple\ndimensions defining the space of foundational methods that contribute to\nexplainable deep learning, ii) discusses the evaluations for model\nexplanations, iii) places explainability in the context of other related deep\nlearning research areas, and iv) finally elaborates on user-oriented\nexplanation designing and potential future directions on explainable deep\nlearning. We hope the guide is used as an easy-to-digest starting point for\nthose just embarking on research in this field.",
  "text": "Explainable Deep Learning:A Field Guide for the Uninitiated\nExplainable Deep Learning:\nA Field Guide for the Uninitiated\nGabriëlle Ras∗\ng.ras@donders.ru.nl\nDonders Institute for Brain, Cognition and Behaviour\nRadboud University Nijmegen\n6525 HR Nijmegen, the Netherlands\nNing Xie∗†\nxining@amazon.com\nAmazon\nSeattle, WA 98109, USA\nMarcel van Gerven\nm.vangerven@donders.ru.nl\nDonders Institute for Brain, Cognition and Behaviour\nRadboud University Nijmegen\n6525 HR Nijmegen, the Netherlands\nDerek Doran†\nderek.doran@tenet3.com\nTenet3, LLC\nDayton, OH, USA\nAbstract\nDeep neural networks (DNNs) have become a proven and indispensable machine learn-\ning tool. As a black-box model, it remains diﬃcult to diagnose what aspects of the model’s\ninput drive the decisions of a DNN. In countless real-world domains, from legislation and\nlaw enforcement to healthcare, such diagnosis is essential to ensure that DNN decisions are\ndriven by aspects appropriate in the context of its use. The development of methods and\nstudies enabling the explanation of a DNN’s decisions has thus blossomed into an active,\nbroad area of research. A practitioner wanting to study explainable deep learning may be\nintimidated by the plethora of orthogonal directions the ﬁeld has taken. This complexity\nis further exacerbated by competing deﬁnitions of what it means “to explain” the actions\nof a DNN and to evaluate an approach’s “ability to explain”. This article oﬀers a ﬁeld\nguide to explore the space of explainable deep learning aimed at those uninitiated in the\nﬁeld. The ﬁeld guide: i) Introduces three simple dimensions deﬁning the space of founda-\ntional methods that contribute to explainable deep learning, ii) discusses the evaluations\nfor model explanations, iii) places explainability in the context of other related deep learn-\ning research areas, and iv) ﬁnally elaborates on user-oriented explanation designing and\npotential future directions on explainable deep learning. We hope the guide is used as an\neasy-to-digest starting point for those just embarking on research in this ﬁeld.\n1. Introduction\nArtiﬁcial intelligence (AI) systems powered by deep neural networks (DNNs) are pervasive\nacross society: they run in our pockets on our cell phones [Georgiev et al., 2017], in cars\nto help avoid car accidents [Jain et al., 2015], in banks to manage our investments [Chong\n∗. Gabriëlle Ras and Ning Xie are co-ﬁrst authors on this work.\n†. Ning Xie and Derek Doran completed some of this work while at the Dept. of Computer Science and\nEngineering, Wright State University, Dayton, OH.\n1\narXiv:2004.14545v2  [cs.LG]  13 Sep 2021\nRas, Xie, van Gerven, & Doran\net al., 2017] and evaluate loans [Pham and Shen, 2017], in hospitals to help doctors diagnose\ndisease symptoms [Nie et al., 2015], at law enforcement agencies to help recover evidence\nfrom videos and images to help law enforcement [Goswami et al., 2014], in the military of\nmany countries [Lundén and Koivunen, 2016], and at insurance agencies to evaluate coverage\nsuitability and costs for clients [Dong et al., 2016, Sirignano et al., 2016]. However, when\nmedical treatment is to be assigned or when a signiﬁcant ﬁnancial decision must be made, an\nAI that suggests a course of action with reasonable evidence, rather than to merely prescribe\none, is desired.\nFor the human ultimately responsible for the action taken, the use of\nDNNs leaves an important question unanswered: how can a person that is held accountable\nfor a decision trust a DNN’s recommendation and justify its use?\nAchieving trust and\nﬁnding justiﬁcation can hardly be achieved if the user does not have access to a satisfactory\nexplanation for the process that led to the recommendation.\nConsider, for example, a\nhypothetical scenario in which a medical system runs a DNN in the backend. Assume that\nthe system makes life-altering predictions about whether or not a patient has a terminal\nillness. It is desirable if this system could also provide a rationale behind its predictions.\nEqually important is for the system to give a rationale that both physicians and patients\ncan understand and trust. Trust in a decision is built upon a rationale that is: (i) easily\ninterpretable; (ii) relatable to the user; (iii) connects the decision with contextual information\nabout the choice or to the user’s prior experiences; and (iv) reﬂects the intermediate thinking\nof the user in reaching a decision. Given the qualitative nature of these characteristics, it\nmay come as no surprise that there is great diversity in the deﬁnitions, approaches, and\ntechniques used by researchers to provide a rationale for the decisions of a DNN. This\ndiversity is further compounded by the fact that the form of a rationale often conforms to\na researcher’s personal notion of what constitutes an “explanation”. For a newcomer to the\nﬁeld, whether a seasoned researcher or students in a discipline that DNN’s are impacting,\njumping into the ﬁeld is a daunting task.\nThis article oﬀers a starting point for researchers and practitioners who are embarking\ninto the ﬁeld of explainable deep learning. This ﬁeld guide is designed to help an uninitiated\nresearcher understand:\n• A set of dimensions characterizing the space of foundational work in explainable deep\nlearning, and a description of such methods. This space summarizes the core aspects\nof explainable DNN techniques that a majority of present work is inspired by or built\nfrom (Section 2).\n• Methods for evaluating explanation methods (Section 3).\n• Complementary research topics that are aligned with explainability. Topics that are\ncomplementary to explainability may involve the development of mechanisms that\nmathematically explain how DNNs learn to generalize, or approaches to reduce a\nDNN’s sensitivity to particular input features. Such topics are indirectly associated\nwith explainability in the sense that they investigate how a DNN learns or performs\ninference, even though the intention of the work is not directly to investigate explana-\ntions (Section 4).\n• The considerations of a designer developing an explainable DNN system (Section 5).\n2\nExplainable Deep Learning:A Field Guide for the Uninitiated\nFigure 1: Outline of the ﬁeld guide.\n• Future directions in explainability research (Section 6).\nOur taxonomy of explainable DNN techniques clariﬁes the technical ideas underpinning\nmost modern explainable deep learning techniques. The discussion of fundamental explain-\nable deep learning methods, emblematic of each framework dimension, provides further\ncontext for the modern work that builds on or takes inspiration from them. Following the\ntaxonomy is a brief discussion on the evaluations of model explanations. Complementary\nDNN topics are reviewed, and the relationships between explainable DNNs and other re-\nlated research areas are developed. The ﬁeld guide then turns to essential considerations\nthat need to be made when building an explainable DNN system in practice, considering\nthe end-user. Finally, the overview of our current limitations and seldom-looked at aspects\nof explainable deep learning suggests new research directions. This information captures\nwhat a newcomer needs to know to successfully navigate the current research literature on\nexplainable deep learning and identify new research problems.\nThere are many reviews on the topic of model explainability. Most of them focus on\nexplanations of general artiﬁcial intelligence methods [Arrieta et al., 2020, Carvalho et al.,\n3\nRas, Xie, van Gerven, & Doran\n2019, Mueller et al., 2019, Tjoa and Guan, 2019, Gilpin et al., 2018, Adadi and Berrada,\n2018, Miller, 2019, Guidotti et al., 2018, Lipton, 2018, Liu et al., 2017, Došilović et al.,\n2018, Doshi-Velez and Kim, 2017, Molnar, 2020, Carvalho et al., 2019], and some on deep\nlearning [Ras et al., 2018, Montavon et al., 2018, Zhang and Zhu, 2018, Samek et al.,\n2017, Erhan et al., 2010].\nThe unique contributions of this ﬁeld guide are as follows.\nFirst, it speciﬁcally tar-\ngets explanations for deep learning systems, while existing reviews focus on explanations of\ngeneral artiﬁcial intelligence methods or have a narrower focus on particular types of deep\nlearning architectures. Second, it speciﬁcally targets researchers uninitiated in explainable\ndeep learning and aims to lower the bar to enter this ﬁeld. Third, it introduces a novel\ncategorization scheme to systematically organize numerous explanation methods, with an\neye towards simplicity and focus on the ﬁeld’s foundations. Finally, it elaborates on other\ntopics closely related to explainable deep learning. The review connects related ﬁelds to\nexplainable deep learning to better understand how they contribute to existing work to\nimprove DNN transparency, robustness, and reliability.\n1.1 A Word of Caution\nAlthough this paper focuses on explaining DNNs, it does not mean that DNNs are the only\nproblem-solving tools in a machine learning toolbox. DNNs have signiﬁcant potential for\nmisuse when applied prematurely or incorrectly. Extreme caution must be taken, by all\nparties involved, to ensure that the DNN technology and derivatives are properly tested\nbefore production and commercial use. It is also recommended to review the perceived need\nfor DNNs and consider if other algorithms can serve the same purpose [Rudin, 2019]. One\nrecent example of this is the wrongful use of facial recognition technology by federal law\nenforcement agencies. Studies have shown that facial recognition technology is signiﬁcantly\nless accurate on non-white, non-male persons [Buolamwini and Gebru, 2018, Garvie, 2016],\nyet the immature technology was used in the real world with negative outcomes.\nOn the other hand, DNN applications in domains such as medicine [Rajpurkar et al.,\n2018, Esteva et al., 2017] or applications that can beneﬁt climate change [Rolnick et al., 2019]\nhave been meaningful and beneﬁcial to society as a whole. This paper looks at explainability\nthrough the lens of the DNNs. However, that does not mean that explanations are only\nrequired for DNNs. The need for explanations extend to the whole of scientiﬁc reasoning.\n1.2 But What is an Explanation?\nIn general, deﬁning explanation is a philosophical activity that, while worth undertaking,\ndoes not align with the goal of this paper. Instead, we will give examples of what explana-\ntions can look like in the context of deep learning. In its most general form, an explanation\nis any information that can help the user understand and communicate to others why the\nmodel exhibits a particular pattern of decision-making and how individual decisions come\nabout.\nThe goal of any explanation can roughly fall into one of the following two categories:\n(i) explanations that give insight into model training and generalization. These explanations\ngive a practitioner additional information that can be used to make decisions about the\ncomponents in the model training and validation process, e.g., the number of labeled data,\n4\nExplainable Deep Learning:A Field Guide for the Uninitiated\nFigure 2: Examples of what explanations can look like in practice. The explanation depends\non the type of data used and the method used to create the explanation. In general\nan explanation is any information that aids the user in understanding the model\nrationale behind the model prediction.\nvalue of the hyperparameters, model choice. The other category is (ii) explanations that\ngive insight into model predictions. Most explanations fall into this category and help prac-\ntitioners explain why the model made a particular prediction, usually in terms of the model\ninput. These explanations can be used to communicate to others (potentially non-experts)\nabout model predictions. Many individual predictions can be analyzed to reveal patterns\nin overall model prediction behavior. This category of explanations can further be broken\ndown in more speciﬁc categories such as counterfactual explanations [Verma et al., 2020]\nand contrastive explanations [Miller, 2018].\nMost explanations bear a strong resemblance to the data type that was used to train\nthe DNN. If the datatype is an image, the explanation can be a saliency or heatmap. A\nsaliency map depicts regions in the image that the explanation method determined was\nimportant for the network’s prediction. If the datatype is text-based, the explanation can\nlook like highlighted words in the text. The explanation method (for instance, attention\nvisualization) determines which words are highlighted. If the data is composed of attributes,\ni.e., data that can be represented as a table, the explanation can be a set of rules that describe\nwhich combinations of diﬀerent attribute values lead to which predictions. The illustrations\n5\nRas, Xie, van Gerven, & Doran\nin Figure 2 reﬂect diﬀerent explanation representations based on data type. It is useful to\nkeep in mind that while heatmaps are one of the most common and natural ways to present a\nmodel explanation, it is also subject to interpretation by the practitioner. Practitioners can\ninterpret the explanation diﬀerently, especially when the explanation method is unknown.\n2. Methods for Explaining DNNs\nThere are countless surveys on explainable AI [Arrieta et al., 2020, Carvalho et al., 2019,\nMueller et al., 2019, Tjoa and Guan, 2019, Gilpin et al., 2018, Adadi and Berrada, 2018,\nMiller, 2019, Guidotti et al., 2018, Lipton, 2018, Liu et al., 2017, Došilović et al., 2018, Doshi-\nVelez and Kim, 2017] and explainable deep learning [Ras et al., 2018, Montavon et al.,\n2018, Zhang and Zhu, 2018, Samek et al., 2017, Erhan et al., 2010]. The many surveys cover\na large body of work that may prove hard to navigate and synthesize into a broad view of\nthe ﬁeld. Instead, this article investigates a simple space of foundational explainable DNN\nmethods. We say a method is foundational if it is often used in practice or if it introduces\na concept that modern work builds upon. Understanding this smaller space of foundational\nmethods will support a reader as they study modern approaches.\nSince diﬀerent users at diﬀerent stages of the software pipeline have diﬀerent require-\nments, it is only possible to represent relative advantages given the explainability goal that\nneeds to be achieved. The users that we will take into account for this discussion are the\nexpert users as described in [Ras et al., 2018].\nWe present a simple three-dimensional space encompassing:\n• Visualization methods: Visualization methods express an explanation by highlight-\ning, through a scientiﬁc visualization, characteristics of an input that strongly inﬂuence\nthe output of a DNN.\n• Model distillation: Model distillation develops a separate, “white-box” machine\nlearning model that is trained to mimic the input-output behavior of the DNN. The\nwhite-box model, which is inherently explainable, is meant to identify the decision\nrules or input features inﬂuencing DNN outputs.\n• Intrinsic methods: Intrinsic methods are DNNs that have been speciﬁcally created\nto render an explanation along with its output. As a consequence of its design, in-\ntrinsically explainable deep networks can jointly optimize model performance and a\nquality of the explanations produced.\n2.1 Visualization Methods\nVisualization methods associate the degree to which a DNN considers input features to a\ndecision. This association is often referred to as attribution. A common explanatory form of\nvisualization methods is saliency maps or heatmaps, where oftentimes a transparent colored\nheatmap is overlaid on the original input image. These maps identify input features that are\nmost salient, in the sense that they cause a maximum response or stimulation inﬂuencing\nthe model’s output [Yosinski et al., 2015, Ozbulak, 2019, Olah et al., 2017, Olah et al.,\n2018, Carter et al., 2019]. We break down visualization methods into two types, namely\n6\nExplainable Deep Learning:A Field Guide for the Uninitiated\nExplaining\nDNNs\nMethods\nVisualization\nBackpropagation\nActivation Maximization\n[Erhan et al., 2009]\nDeconvolution\n[Zeiler et al., 2011]; [Zeiler and Fergus, 2014]\nCAM and Grad-CAM\n[Zhou et al., 2016]; [Selvaraju et al., 2017]\nLRP\n[Bach et al., 2015]; [Lapuschkin et al., 2016]\n[Arras et al., 2016]; [Arras et al., 2017]\n[Ding et al., 2017]; [Montavon et al., 2017]\nDeepLIFT\n[Shrikumar et al., 2017]\nIntegrated Gradients\n[Sundararajan et al., 2016, Sundararajan et al., 2017]\nPerturbation\nOcclusion Sensitivity\n[Zeiler and Fergus, 2014]; [Zhou et al., 2014]\nRepresentation Erasure\n[Li et al., 2016]\nMeaningful Perturbation\n[Fong and Vedaldi, 2017]\nPrediction Diﬀerence Analysis\n[Zintgraf et al., 2017]\n[Robnik-Šikonja and Kononenko, 2008]\nDistillation\nLocal Approximation\nLIME\n[Ribeiro et al., 2016c, Ribeiro et al., 2016a]\nAnchor-LIME\n[Ribeiro et al., 2016b]\nAnchors\n[Ribeiro et al., 2018]\nSTREAK\n[Elenberg et al., 2017]\nSHAP\n[Lundberg and Lee, 2017]\nCausal SHAP\n[Heskes et al., 2020]\nModel Translation\nTree Based\n[Frosst and Hinton, 2017]\n[Tan et al., 2018]; [Zhang et al., 2019a]\nFSA Based\n[Hou and Zhou, 2020]\nGraph Based\n[Zhang et al., 2017, Zhang et al., 2018]\nRule Based\n[Murdoch and Szlam, 2017]; [Harradon et al., 2018]\nIntrinsic\nAttention Mechanisms\nSingle-Modal Weighting\n[Bahdanau et al., 2015]; [Luong et al., 2015]\n[Wang et al., 2016]; [Vaswani et al., 2017]\n[Letarte et al., 2018]; [He et al., 2018]\n[Devlin et al., 2019]\nMulti-Modal Interaction\n[Vinyals et al., 2015]; [Xu et al., 2015]\n[Antol et al., 2015]; [Park et al., 2016]\n[Goyal et al., 2017]; [Teney et al., 2018]\n[Mascharka et al., 2018]; [Anderson et al., 2018]\n[Xie et al., 2019]\nJoint Training\nText Explanation\n[Hendricks et al., 2016]; [Camburu et al., 2018]\n[Park et al., 2018]; [Kim et al., 2018b]\n[Zellers et al., 2019]; [Liu et al., 2019]; [Hind et al., 2019]\nExplanation Association\n[Lei et al., 2016]; [Dong et al., 2017]\n[Melis and Jaakkola, 2018]; [Iyer et al., 2018]\nModel Prototype\n[Li et al., 2018a]; [Chen et al., 2019]\nFigure 3: Methods for explaining DNNs.\n7\nRas, Xie, van Gerven, & Doran\nVisualization Methods\nSummary\nReferences\nBackpropagation-based\nVisualize feature relevance from vol-\nume of gradient passed through lay-\ners during network training.\n[Erhan et al., 2009, Zeiler et al., 2011, Zeiler and\nFergus, 2014, Zhou et al., 2016, Selvaraju et al.,\n2017, Bach et al., 2015, Lapuschkin et al., 2016,\nArras et al., 2016, Arras et al., 2017, Ding et al.,\n2017, Montavon et al., 2017, Shrikumar et al.,\n2017, Sundararajan et al., 2017, Sundararajan\net al., 2016]\nPerturbation-based\nVisualize feature relevance by com-\nparing network output of an input\nand a modiﬁed copy of the input.\n[Zeiler and Fergus, 2014, Zhou et al., 2014, Li\net al., 2016, Fong and Vedaldi, 2017, Robnik-\nŠikonja and Kononenko, 2008, Zintgraf et al.,\n2017]\nTable 1: Visualization methods.\nbackpropagation and perturbation-based visualization. The types are summarized in Table 1\nand will be discussed further below.\nDNN Visualization w.r.t. Model Prediction \nOpaque DNN\nHidden \nOutput \nInput \n...\n...\n...\n...\n...\n...\nCalculate\n\"Saliency\"\nScore \nVisualize Interested Areas\nInput \n（Depict which part of input is\nrelevant w.r.t. given model prediction.）\nHidden \n（Depict what kind of features are\ncaptured by these hidden states.）\n...\nFigure 4: Visualization Methods. The to-be-visualized element E can be from either the\nmodel input X or hidden states H.\nVisualization is based on the calculated\nsaliency score S(E), which varies along with diﬀerent visualization methods.\n2.1.1 Backpropagation-based methods\nBackpropagation-based methods identify the saliency of input features based on some eval-\nuation of gradient signals passed from output to input during network training. A baseline\ngradient-based approach visualizes the partial derivative of the network output with respect\nto each input feature scaled by its value [Simonyan et al., 2013, Springenberg et al., 2014],\nthus quantifying the sensitivity of the network’s output with respect to input features. In a\nscene recognition task, for example, a high relevance score for pixels representing a bed in\na CNN that decides the image is of class “bedroom” may suggest that the decision made by\nthe CNN is highly sensitive to the presence of the bed in the image. Other gradient-based\nmethods may evaluate this sensitivity with respect to the output, but from diﬀerent collec-\ntions of feature maps at intermediate CNN network layers [Zeiler and Fergus, 2014, Bach\net al., 2015, Montavon et al., 2017, Shrikumar et al., 2017].\n8\nExplainable Deep Learning:A Field Guide for the Uninitiated\nActivation maximization. One of the earliest works on visualization in deep architectures\nis proposed by [Erhan et al., 2009]. This seminal study introduces the activation maximiza-\ntion method to visualize important features in any layer of a deep architecture by optimizing\nthe input X such that the activation a of the chosen unit i in a layer j is maximized:\narg max\nX\nai,j(X, θ)\n(1)\nParameters θ of a trained network are kept ﬁxed during activation maximization. The\noptimal X is found by computing the gradient of ai,j(X, θ) and updating X in the direc-\ntion of the gradient. The practitioner decides the values of the hyperparameters for this\nprocedure, i.e., the learning rate and how many iterations to run. The optimized X will\nbe a representation, in the input space, of the features that maximize the activation of a\nspeciﬁc unit, or if the practitioner chooses so, multiple units in a speciﬁc network layer.\nBy visualizing the internal representations, the practitioner can check if concepts learned\nby the model are human interpretable. The quality of the concepts can be used as an in-\ndication of model generalizability and determine if, for instance, additional labeled data is\nneeded to train the model. Activation maximization can give insight into the model training\nand generalization process but does not lend itself to explaining individual model predictions.\nDeconvolution. Deconvolution was originally introduced as an algorithm to learn image\nfeatures in an unsupervised manner [Zeiler et al., 2011]. However, the method gained popu-\nlarity because of its applications in visualizing higher layer features in the input space [Zeiler\nand Fergus, 2014], i.e., visualizing higher layer features in terms of the input. Deconvolution\nassumes that the model being explained is a neural network consisting of multiple convo-\nlutional layers. We will refer to this model as CNN. The consecutive layers of this network\nconsist of a convolution of the previous layer’s output (or the input image in the case of the\nﬁrst convolutional layer) with a set of learned convolutional ﬁlters, followed by the appli-\ncation of the rectiﬁed linear function (ReLU) ReLU(A) = max(A, 0) on the output of the\naforementioned convolution\nAℓ, sℓ= maxpool\n\u0010\nReLU\n\u0010\nAℓ−1 ∗Kℓ+ bℓ\u0011\u0011\n(2)\nwhere ℓindicates the respective layer, Aℓis the output of the previous layer, K is\nthe learned ﬁlter, and b is the bias. If the outputs from the ReLU are passed through a\nlocal max-pooling function, it additionally stores the output sℓcontaining the indices of the\nmaximum values for a later unpooling operation. In the original paper, the set of sℓ’s are\nreferred to as switches. A deconvolutional neural network, referred to as DeCNN, consists of\nthe inverse operations of the original convolutional network CNN. DeCNN takes the output\nof CNN as its input. In other words, DeCNN runs the CNN in reverse, from top-down. This is\nwhy the deconvolution method is classiﬁed as a backpropagation method. The convolutional\nlayers in CNN are replaced with deconvolutions and the max-pooling layers are replaced with\nunpooling layers. A deconvolution is also called a transposed convolution, meaning that the\nvalues of Kℓare transposed and then copied to the deconvolution ﬁlters KℓT . If the CNN\nincluded max-pooling layers, they are replaced with unpooling layers which approximately\nupscales the feature map, retaining only the maximum values. This is done by retrieving\n9\nRas, Xie, van Gerven, & Doran\nthe indices stored in sℓat which the maximum values were located when the max-pooling\nwas originally applied in CNN.\nAs an example let us see the calculations involved in deconvolving Equation 2:\nAℓ−1 = unpool\n\u0010\nReLU\n\u0010\u0010\nAℓ−bℓ\u0011\n∗KℓT \u0011\n, sℓ\u0011\n(3)\nUsing Equation 3 one or multiple learned ﬁlters K in any layer of the network can be\nvisualized by reverse propagating the values of K all the way back to the input space. Finally,\nthis study also describes how the visualizations can be used for architecture selection.\nPractitioners can use this method to visualize how much information from the original\ninput the extracted features retain, and gain insight on how information is extracted from\ndata at diﬀerent network layers. From this insight actions can be taken to improve the model\ntraining process. Deconvolution does not lend itself to explaining single model predictions.\nCAM and Grad-CAM. [Zhou et al., 2016] describes a visualization method for creating\nclass activation maps (CAM) using global average pooling (GAP) in CNNs. [Lin et al.,\n2013] proposes the idea to apply a global average pooling on the activation maps of the last\nconvolutional layer, right before the fully connected (FC) output layer. This results in the\nfollowing conﬁguration at the end of the CNN: GAP(Conv) →FC →softmax. The FC layer\nhas C nodes, one for each class. The CAM method combines the activations A from Conv,\ncontaining K convolutional ﬁlters, and weights wk,c from FC, where the (k, c) pair indicates\nthe speciﬁc weighted connection from Conv to FC, to create relevance score map:\nmapc =\nK\nX\nk\nwk,cAk\n(4)\nThe map is then upsampled to the size of the input image and overlaid on the input image,\nvery similar to a heat map, resulting in the class activation map. Each class has a unique\nCAM, indicating the image regions important to network prediction for that class. CAM\ncan only be applied on CNNs that employ the GAP(Conv) →FC →softmax conﬁguration.\nGradient-weighted Class Activation Map (Grad-CAM) [Selvaraju et al., 2017] is a gen-\neralization CAM using the gradients of the network output with respect to the last convolu-\ntional layer to achieve the class activation map. This allows Grad-CAM to be applicable to a\nbroader range of CNNs compared to CAM, only requiring that the ﬁnal activation function\nused for network prediction to be a diﬀerentiable function, e.g., softmax. For each feature\nmap Ak in the ﬁnal convolutional layer of the network, a gradient of the score yc (the value\nbefore softmax, also known as logit) of class c with respect to every node in Ak is computed\nand averaged to get an importance score αk,c for feature map Ak:\nαk,c =\n1\nm · n\nm\nX\ni=1\nn\nX\nj=1\n∂yc\n∂Ak,i,j\n(5)\n10\nExplainable Deep Learning:A Field Guide for the Uninitiated\nwhere Ak,i,j is a neuron positioned at (i, j) in the m × n feature map Ak. Grad-CAM\nlinearly combines the importance scores of each feature map and passes them through a\nReLU to obtain an m × n-dimensional relevance score map\nmapc = ReLU\n K\nX\nk\nαk,cAk\n!\n(6)\nThe relevance score map is then upsampled via bilinear interpolation to be of the same\ndimension as the input image to produce the class activation map. Figure 5 shows a visual\nrepresentation of the grad-CAM method and an example of what a grad-CAM heatmap\nlooks like for the prediction “cat”.\nPractitioners can use the CAM family of methods to determine, given an input and a\nclass, what is the information in the input that gives evidence for that class. Based on\nthis information the practitioner can determine to what extent model predictions can be\ninterpreted and assess for which classes consistent model predictions can expected. For ex-\nample, if we have two models where both models have the same accuracy score, a model that\nproduces heatmaps consistent with human experience is often considered more trustworthy\ncompared to one where the heatmaps correspond poorly to human experience. Practition-\ners can also use the CAM family of methods to determine if there is an unfavorable class\nbias that the model is picking up on, e.g., skin color. The reader should note that only pos-\nitive attribution can be computed with this method due to the ReLU function in Equation 6.\nLayer-Wise Relevance Propagation. LRP methods create a saliency map that, rather\nthan measuring sensitivity, represents the relevance of each input feature to the output of\nthe network [Bach et al., 2015, Lapuschkin et al., 2016, Arras et al., 2016, Arras et al.,\n2017, Ding et al., 2017, Montavon et al., 2017]. While sensitivity measures the change in\nresponse in the network’s output as a result of changing attributes in the input [Kindermans\net al., 2019], relevance measures the strength of the connection between the input features to\nnetwork output, without making any changes to the input or the components of the network.\nLRP methods decompose the output value f(x) of a deep network f across input features\nx = (x1, x2, . . . , xN), such that f(x) = P\ni ri where ri is the relevance score of feature xi.\nPerhaps the most generic type of LRP is called Deep Taylor Decomposition [Montavon\net al., 2017]. The method is based on the fact that f is diﬀerentiable and hence can be\napproximated by a Taylor expansion of f at some root ˆx for which f(ˆx) = 0:\nf(x) = f(ˆx) + ∇ˆxf · (x −ˆx) + ϵ\n=\nN\nX\ni\n∂f\n∂xi\n( ˆxi) · (xi −ˆxi) + ϵ\n(7)\nwhere ϵ encapsulates all second order and higher terms in the Taylor expansion. A good\nroot point is one that is as minimally diﬀerent from x and that causes the function f(x) to\noutput a diﬀerent prediction. The relevance score for inputs can then be seen as the terms\ninside of the summation:\nri = ∂f\n∂xi\n( ˆxi) · (xi −ˆxi)\n(8)\n11\nRas, Xie, van Gerven, & Doran\nFigure 5: Visual explanation of how grad-CAM works. Top: Visualization of Equation 5\nfor calculating the importance scores αi,j for each feature map Ak. Middle: The\nheatmap for a speciﬁc class is computed by multiplying the importance score with\neach feature map and taking the sum. Afterwards the heatmap is upsampled and\noverlaid on the original image. Bottom: Heatmap for the prediction “cat”.\nTo extend this idea to a DNN, the deep Taylor decomposition algorithm considers a\nconservative decomposition of relevance scores across layers of the network, starting from\nthe output, through each hidden layer, back to the input. Thus, the method requires that\nthe relevance score of a node i at layer l, denoted rl\ni be decomposable into\nrℓ\ni =\nM\nX\nj\nrℓ\ni,j\n(9)\n12\nExplainable Deep Learning:A Field Guide for the Uninitiated\nwhere the summation is taken over all M nodes in layer ℓ+ 1 that node i in layer ℓ\nconnects or contributes to. This indicates that the relevance score of the later layers can be\nbackpropagated to generate the relevance score of former layers. The relevance score with\nrespect to the input space can thus be calculated by conducting this decomposition rule\nlayer by layer. Further details can be found in the original paper [Montavon et al., 2017].\nIn practice, the results of applying LRP are similar to the results of the CAM family of\nmethods: given an input and a prediction, both methods tell the practitioner the regions\nin the input that are most relevant for the prediction. LRP allows the heatmap to display\nnegative attributions in addition to positive ones. Practitioners can use the information to\nassess and further investigate things like model bias, prediction consistency and model trust.\nHowever, with LRP the practitioner has to additionally supply the method with a reference\ninput (root) ˆx, which in some cases can either be unsolvable or expensive to compute. It is\nworth noting that the visualization results of LRP rely on the root choices: depending on\nthe input space restrictions, a diﬀerent root can be chosen, and for diﬀerent roots chosen,\nthe relevance propagation rule varies, which ultimately yields diﬀerent appearances of the\nheatmap. Compared to CAM methods, the LRP heatmap could be of higher quality and\nmore precise. This is because LRP assigns each individual pixel a relevance score, as op-\nposed to CAM, which looks at activation maps in the ﬁnal layer. The ﬁnal CAM heatmap\nwill be an upsampled image indicating an approximate relevant region.\nDeepLIFT. Deep Learning Important FeaTures (DeepLIFT) is an important approach\nbased on backpropagation by [Shrikumar et al., 2017]. It assigns relevance scores to input\nfeatures based on the diﬀerence between an input x and a “reference” input x′. The reference\nshould be chosen according to the problem at hand and can be found by answering the\nquestion “What am I interested in measuring diﬀerences against?”. In an example using\nMNIST the reference chosen is an input of all zeros as this is the background value in the\nimages. Deﬁne ∆t = f(x) −f(x′) as the diﬀerence-from-reference of an interested neuron\noutput of the network between x and reference x′, and ∆x = x−x′ as the diﬀerence between\nx and x′. DeepLIFT assigns a relevance score R∆xi∆t for input feature xi:\n∆t =\nN\nX\ni=1\nR∆xi∆t\n(10)\nwhere N is the number of input neurons that are necessary to compute t.\nIn this\nformulation, R∆xi∆t can be thought of as a weight denoting how much inﬂuence ∆xi had\non ∆t. According to Equation 10 the sum of the all weights is equal to the diﬀerence-from-\nreference output ∆t. The relevance score can be calculated via the Linear rule, Rescale rule,\nor RevealCancel rule, as elaborated in their study. A multiplier m∆x∆t is deﬁned as\nm∆x∆t = R∆x∆t\n∆x\n(11)\nindicating the relevance of ∆x with respect to ∆t, averaged by ∆x. Given a hidden\nlayer ℓof nodes aℓ= (aℓ\n1, aℓ\n2, . . . aℓ\nK), whose upstream connections are the input nodes\n13\nRas, Xie, van Gerven, & Doran\nx = (x1, x2, . . . xN), and a downstream target node is t, the DeepLIFT paper proves the\neﬀectiveness of the “chain rule” as illustrated below:\nm∆xi∆t =\nK\nX\nj=1\nm∆xi∆aℓ\njm∆aℓ\nj∆t\n(12)\nThis “chain rule” allows for layer-by-layer computation of the relevance scores of each hid-\nden layer node via backpropagation. The DeepLIFT paper and appendix specify particular\nrules for computing m∆xi∆aℓ\nj based on the architecture of the hidden layer aℓ.\nDeepLIFT resembles LRP because the practitioner has to choose a reference point and\nbecause the algorithm assigns pixel-wise relevance scores. The heatmap results of DeepLIFT\nexplain the diﬀerence in prediction between the reference image and the original prediction.\nHeatmap interpretation depends on which reference image is chosen. By being creative with\nthe choice of the reference image, the practitioner can use DeepLIFT to probe the network\nin diﬀerent ways. Heatmaps obtained with DeepLIFT can also display negative attribution\nin addition to positive attribution, unlike the CAM family.\nIntegrated Gradients. Integrated gradients [Sundararajan et al., 2017] is an “axiomatic\nattribution” map that satisﬁes two axioms for input feature relevance scoring on a network f.\nThe ﬁrst axiom is sensitivity: compared to some baseline input x′, when input x diﬀers from\nx′ along feature xi and f(x) ̸= f(x′), then xi should have a non-zero relevance score. The\nsecond axiom is implementation invariance: for two networks f1 and f2 whose outputs are\nequal for all possible inputs, the relevance score for every input feature xi should be identical\nover f1 and f2. The break of the second axiom may potentially result in the sensitivity of\nrelevance scores on irrelevant aspects of a model.\nGiven a deep network f whose codomain is [0, 1], an input x, and a baseline input x′,\nthe relevance of feature xi of input x over f is taken as the integral of the gradients of f\nalong the straight line path from x′ to x:\nIGi(x) = (xi −x′\ni)\nZ 1\n0\n∂f(x′ + α(x −x′))\n∂xi\ndα\n(13)\nwhere α is associated with the path from x′ to x, and is smoothly distributed in range\n[0, 1]. An interpretation of IGi is the cumulative sensitivity of f to changes in feature i in\nall inputs on a straight line between x′ to x going in direction i. Intuitively, xi should have\nincreasing relevance if gradients are large between a “neutral” baseline point x′ and x along\nthe direction of xi. IG can be approximated by a Riemann summation of the integral:\nIGi(x) ≊(xi −x′\ni)\nM\nX\nk=1\n∂F(x′ + k\nM (x −x′))\n∂xi\n1\nM\n(14)\nwhere M is the number of steps in the Riemann approximation of this integral. In the\noriginal paper the authors propose setting M somewhere between 20 and 300 steps.\nIG is similar to LRP and DeepLIFT: the practitioner needs to supply a reference (base-\nline) image, and the algorithm assigns pixel-wise relevance scores. For image input problems,\nthe baseline image is set to a black image, while for text input, the baseline input can be a\n14\nExplainable Deep Learning:A Field Guide for the Uninitiated\nzero embedding vector. IG targets the sensitivity axiom not considered by gradient-based\nattribution methods such as [Simonyan et al., 2013, Springenberg et al., 2014] and the\nimplementation invariance axiom not considered by methods like DeepLIFT and LRP.\n2.1.2 Perturbation-based Methods\nPerturbation-based methods compute input feature relevance by altering or removing the\ninput feature and comparing the diﬀerence in network output between the original and al-\ntered one. Perturbation methods can compute the marginal relevance of each feature with\nrespect to how a network responds to a particular input.\nOcclusion Sensitivity. The approach proposed by [Zeiler and Fergus, 2014], applicable\nfor spatial data, sweeps a ‘grey patch’ that occludes spatial values (i.e., pixels) over the\ninput and sees how the model prediction varies when the patch covers diﬀerent regions in\nthe input. The reasoning behind this approach is that the model’s performance decreases\nwhen the model does not have access to the relevant information. Thus, the more the model\nperformance decreases, the more relevant the occluded region is assumed to be. When a\nsigniﬁcant portion of the image is swept, the information can be used to create a sensitivity\nheatmap. In Figure 6 an example is given where a gray patch is swept across an image of a\nhummingbird. A variant of this method is implemented in [Zhou et al., 2014], where small\ngray squares are used to occlude image patches in a dense grid. All other methods in this\ncategory work similarly. The methods vary in the information that the patch provides or\nremoves, the size of the patch, and how the patches are sampled.\nThe practitioner can use this method to measure how sensitive a model is to a particular\npart of the removed input. This information can serve as a perfunctory indication of how\nregions in the input are correlated with model predictions. Since this method does not make\nuse of model internals, it can be used on any DNN. It is worth noting that the higher the\ndesired resolution of heatmaps, the smaller the patch should be, thus the longer it takes to\ncompute the heatmap. Certain features in the input might co-occur, and the joint presence\nof these features is important. However, occlusion sensitivity is unable to handle this be-\ncause only one region at a time is occluded.\nRepresentation Erasure. [Li et al., 2016] is an example of a perturbation-based method for\nnatural language input. To measure the eﬀectiveness of each input word or each dimension\nof intermediate hidden activations, the method erases the information by deleting a word or\nsetting a dimension to zero and observes the inﬂuences on model predictions correspondingly.\nReinforcement learning (RL) is adopted to evaluate the inﬂuence of multiple words or phrases\ncombined by ﬁnding the minimum changes in the text that causes a ﬂipping of a neural\nnetwork’s decision.\nPractitioners can use representation erasure to achieve the same goals as occlusion sen-\nsitivity: to measure how sensitive a model is to a particular part of the removed input.\nEven though this method focuses on natural language explanations, it can be modiﬁed and\napplied to other types of problems. Compared to occlusion sensitivity, this method has a\ncouple of beneﬁts. First, representation erasure can handle combinations of erasures. It\nbecomes possible to take into account co-occurring input features. Second, representation\n15\nRas, Xie, van Gerven, & Doran\nFigure 6: An illustration of how perturbation methods work on images.\nerasure is eﬃcient because RL is used to ﬁnd the minimum change that causes the model’s\nprediction to change. In contrast, occlusion sensitivity requires the practitioner to sweep\nthe entire image in a brute force manner.\nMeaningful Perturbation. [Fong and Vedaldi, 2017] deﬁnes an explanation as a meta-\npredictor, which is a rule that predicts the output of a black box f to certain inputs x. For\nexample, the explanation for a classiﬁer that identiﬁes a bird in an image can be deﬁned as\nB(x; f) = {x ∈Xc ⇔f(x) = +1}\n(15)\nwhere f(x) = +1 means a bird is present and Xc is the set of all images that the\nDNN predicts a bird exists. Given a speciﬁc image x0 and a DNN f, the visualization is\ngenerated via perturbation to identify sensitive areas of x0 with respect to the output f(x0)\nformulated as a local explanation (“local” to x0) by the author. The author deﬁnes three\nkinds of perturbations to delete information from image, i) constant, replacing region with\na constant value ii) noise, adding noise to the region, and iii) blur, blurring the region area,\nand generating explainable visualization respectively.\nA practitioner can use meaningful perturbation to ﬁnd regions in the input that the\nmodel’s output is sensitive to, similar to the previous perturbation-based methods. In con-\ntrast to said methods, information is never entirely removed from the image, and the amount\nof perturbation is kept to a minimum. Because of this, the resulting heatmap is more con-\ncentrated around the region of interest, and the number of spurious areas is far less compared\nto the other methods. From a practitioner’s perspective, this makes for more easily inter-\npretable heatmaps.\nPrediction Diﬀerence Analysis. [Zintgraf et al., 2017] proposes a rigorous approach\nto delete information from an input and measure its inﬂuence accordingly. The method\nis based on [Robnik-Šikonja and Kononenko, 2008], which evaluates the eﬀect of feature\n16\nExplainable Deep Learning:A Field Guide for the Uninitiated\nModel Distillation\nComments\nReferences\nLocal Approximation\nLearns a simple model whose in-\nput/output behavior mimics that of\na DNN for a small subset of input\ndata.\n[Ribeiro et al., 2016c, Ribeiro et al., 2016a,\nRibeiro et al., 2016b, Ribeiro et al., 2018, Elen-\nberg et al., 2017]\nModel Translation\nTrain an alternative smaller model\nthat mimics the input/output be-\nhavior of a DNN.\n[Frosst and Hinton, 2017, Tan et al., 2018,\nZhang et al., 2019a, Hou and Zhou, 2020, Zhang\net al., 2017, Zhang et al., 2018, Harradon et al.,\n2018, Murdoch and Szlam, 2017]\nTable 2: Model distillation.\nxi with respect to class c by calculating the prediction diﬀerence between p(c | x−i) and\np(c | x) using the marginal probability\np(c | x−i) =\nX\nxi\np(xi | x−i)p(c | x−i, xi)\n(16)\nwhere x denotes all input features, x−i denotes all features except xi, and the sum\niterates over all possible values of xi. The prediction diﬀerence, also called relevance value\nin the paper, is then calculated by\nDiﬀi(c | x) = log2(odds(c | x)) −log2(odds(c | x−i))\n(17)\nwhere odds(c | x) =\np(c|x)\n1−p(c|x). The magnitude of Diﬀi(c | x) measures the importance of\nfeature xi. Diﬀi(c | x) measures the inﬂuence direction of feature xi, where a positive value\nmeans for decision c and a negative value means against decision c. Compared to [Robnik-\nŠikonja and Kononenko, 2008], Zintgraf et al. improves prediction diﬀerence analysis in three\nways: by i) sampling patches instead of pixels given the high pixel dependency nature of\nimages; ii) removing patches instead of individual pixels to measure the prediction inﬂuence\ngiven the robustness nature of neutral networks on individual pixels;\niii) adapting the\nmethod to measure the eﬀect of intermediate layers by changing the activations of a given\nintermediate layer and evaluating the inﬂuence on down-streaming layers. The heatmaps\nproduced with this method contain both evidence for and against the predicted class.\nThe practitioner can use this method to gain insight into which regions in the input\nthe model is sensitive to and how perturbations in various model regions aﬀect the output.\nCompared to the other perturbation-based methods, prediction diﬀerence analysis applies\na conditional sampling algorithm to determine which regions are perturbed with Gaussian\nnoise. From a practical perspective, this leads to heatmaps, indicating both positive and\nnegative regions of interest for a speciﬁc class.\n2.2 Model Distillation\nModel distillation refers to a class of post-training explanation methods where the knowledge\nencoded within a trained DNN is distilled into a representation amenable for explanation by a\nuser. This representation can take the form of more interpretable machine learning methods,\ne.g., decision trees. In this setting, as illustrated in Figure 7, an inherently transparent or\nwhite box machine learning model g is trained to mimic the input/output behavior of a\n17\nRas, Xie, van Gerven, & Doran\nFigure 7: High level view of model distillation. The behavior of a trained deep learning\nmodel f used as training data for an explainable model g.\ntrained opaque deep neural network f so that g(x) ≈f(x). Subsequent explanation of\nhow g maps inputs to outputs may serve as a surrogate explanation of f’s mapping. Note\nthat [Hinton et al., 2015] outlines a method, with the same name, that implements a speciﬁc\nform of model distillation, namely distilling the knowledge learned by an ensemble of DNNs\ninto a single DNN. In fact, an entire class of explainable deep learning techniques has emerged\nbased on the notion of model distillation.\nA distilled model in general learns to imitate the actions or qualities of an opaque DNN\nover the same data. It is a myth that the distilled form of a DNN necessarily underperforms\ncompared to the original DNN [Liu et al., 2018b].\nConceptually, this may be because\na distilled model has access to information from the trained DNN, including the input\nfeatures it found to be most discriminatory and feature or output correlations relevant for\nclassiﬁcation. The distilled model can use this information directly during training, thus\nreducing the needed capacity of the distilled model. Since the distilled model still takes the\noriginal data as input, it may be further useful as a transparent view of how input features\nbecome related to the actions of the DNN. Interpreting the distilled model will not give deep\ninsights into the internal representation of the data a DNN learns, or say anything about\nthe DNN’s learning process, but can at least provide insight into the features, correlations,\nand relational rules that explain how the DNN operates. Put another way, the explanation\nof a distilled model can be seen as a hypothesis as to why a DNN has assigned some class\nlabel to an input.\nSometimes distillation methods can appear very similar to occlusion methods because\ndistillation methods can also use occlusion in their algorithms. However, they diﬀer in the\nfollowing key aspect: While occlusion methods explicitly aim to make heatmaps, distillation\nmethods aim to capture (local) model behavior by linking the information learned by oc-\nclusion with a more general representation. Moreover, compared to visualization methods,\na much larger space of explanation forms become available to the practitioner that requires\nspecialized knowledge about both the application domain as well as the interpretable model\nthat will be distilled to.\n18\nExplainable Deep Learning:A Field Guide for the Uninitiated\nWe organize model distillation techniques into two categories:\n• Local Approximation. A local approximation method learns a simple model whose\ninput/output behavior mimics that of a DNN for a small subset of the input data.\nThis method is motivated by the idea that the model a DNN uses to discriminate\nwithin a local area of the data manifold is simpler than the discriminatory model over\nthe entire surface. Given a suﬃciently high local density of input data to approximate\nthe local manifold with piecewise linear functions, the DNN’s behavior in this local\narea may be distilled into a set of explainable linear discriminators.\n• Model Translation.\nModel translations train an alternative smaller model that\nmimics the total. input/output behavior of a DNN. They contrast local approximation\nmethods in replicating the behavior of a DNN across an entire dataset rather than\nsmall subsets. The smaller models may be directly explainable, may be smaller and\neasier to deploy, or could be further analyzed to gain insights into the causes of the\ninput/output behavior that the translated model replicates.\n2.2.1 Local Approximation\nA local approximation method learns a distilled model that mimics DNN decisions on inputs\nwithin a small subset of the input examples. Local approximations are made for data subsets\nwhere feature values are very similar, so that a simple and explainable model can make\ndecisions within a small area of the data manifold. While the inability to explain every\ndecision of a DNN may seem unappealing, it is often the case that an analyst or practitioner\nis most interested in interpreting DNN actions under a particular subspace (for example,\nthe space of gene data related to a particular cancer or the space of employee performance\nindicators associated with those ﬁred for poor performance).\nThe idea of applying local approximations may have originated from [Baehrens et al.,\n2010]. These researchers present the notion of an “explainability vector”, deﬁned by the\nderivative of the conditional probability a datum is of a class given some evidence x0 by a\nBayes classiﬁer. The direction and magnitude of the derivatives at various points x0 along\nthe data space deﬁne a vector ﬁeld that characterizes ﬂow away from a corresponding class.\nThe work imitates an opaque classiﬁer in a local area by learning a classiﬁer that has the\nsame form as a Bayes estimator for which the explanation vectors can be estimated.\nLocal Interpretable Model-Agnostic Explanations (LIME). Perhaps the most pop-\nular local approximation method is LIME developed by [Ribeiro et al., 2016c]. Figure 8\nvisually outlines the LIME process. From a global, black-box model f and an input of inter-\nest x ∈Rd, LIME deﬁnes an interpretable model g from a class of inherently interpretable\nmodels g ∈G with diﬀerent domain Rd′ that approximates f well in the local area around x.\nExamples of models in G may be decision trees or regression models whose weights explain\nthe relevance of an input feature to a decision. Note that the domain of g is diﬀerent from\nthat of f. The model g operates over an interpretable representation x′ of the input data\npresented to the unexplainable model f, which could for example be a binary vector denot-\ning the presence or absence of words in text input, or a binary vector denoting if a certain\npixel or color pattern exists in an image input. In Figure 8 examples of the interpretable\n19\nRas, Xie, van Gerven, & Doran\nrepresentation x′ can be seen. In this case x′ is a binary array indicating if a pixel belongs\nto a pattern or not by respectively assigning a 1 or a 0 to each pixel location. Noting that\ng could be a decision tree with very high depth, or a regression model with many co-variate\nweights, an interpretable model that is overly complex may still not be useful or usable to\na human. Thus, LIME also deﬁnes a measure of complexity Ω(g) on g. Ω(g) could measure\nthe depth of a decision tree or the number of higher order terms in a regression model, for\nexample, or it could be coded as if to check that a hard constraint is satisﬁed (e.g., Ω(g) = ∞\nif g is a tree and its depth exceeds some threshold). Let Πx(z) be a similarity kernel between\nperturbed data point z and a original data point x ∈Rd and a loss L(f, g, Πx) deﬁned to\nmeasure how poorly g approximates f on data in the area Πx around the data point x. To\ninterpret f(x), LIME identiﬁes the model g satisfying:\narg min\ng\n{L(f, g, Πx) + Ω(g)}\n(18)\nwhere Ω(g) serves as a type of complexity regularization, or as a guarantee that the\nreturned model will not be too complex when Ω(g) codes a hard constraint. So that LIME\nremains model agnostic, L is approximated by uniform sampling over the non-empty space\nof Rd′. For each sampled data point z′ ∈Rd′, LIME recovers the x ∈Rd corresponding to\nz′, computes f(z), and compares this to g(z′) using L. To make sure that the g minimizing\nEquation 18 ﬁts well in the area local to the original reference point x, the comparison of\nf(z) to g(z′) in L is weighted by Πx(z) so that samples farther from x contribute less to\nthe loss. The sampling process is repeated until the satisfactory dataset of locally perturbed\nsamples Z = {z′, f(z), Πx(z)} is obtained to train interpretable model g on.\nLIME gained popularity because it was one of the earlier methods that combined a\nmodel-agnostic framework with local model explanations. The strength of LIME lies in the\nvalidation of the method with non-expert human practitioners. The original paper describes\nmultiple experiments where non-expert users were asked to use the LIME explanations in\ndiﬀerent tasks. From an algorithmic perspective, LIME makes use of occlusion to ﬁnd re-\ngions in the input that the model is sensitive to, similar to the perturbation-based methods\ndiscussed earlier.\nHowever, LIME aims to generalize the explanations in a local region\naround a reference image by learning an interpretable model. In practice, this means that\nthe practitioner only needs one local LIME model for each set of similar inputs. In contrast\nto perturbation-based methods, where for each image, a new heatmap is computed, once\nthe practitioner has a local LIME model, assuming the input domain does not drastically\nchange, the practitioner does not need to retrain a new local LIME model. This can be\nparticularly useful when the data distribution has low variance.\nWe mention LIME in detail because other popular local approximation models [Ribeiro\net al., 2016a, Ribeiro et al., 2016b, Ribeiro et al., 2018, Elenberg et al., 2017] follow LIME’s\ntemplate and make their extensions or revisions. One drawback of LIME, which uses a linear\ncombination of input features to provide local explanations, is the precision and coverage of\nsuch explanations are not guaranteed. Since the explanations are generated in a locally linear\nfashion, for an unseen instance, which might lie outside of the region where a linear combina-\ntion of input features could represent, it is unclear if an explanation generated linearly and\nlocally still applies. To address this issue, anchor methods [Ribeiro et al., 2016b, Ribeiro\n20\nExplainable Deep Learning:A Field Guide for the Uninitiated\nFigure 8: An image example explaining the process behind LIME. The interpretable repre-\nsentation x′ is a binary array indicating if a pixel belongs to a pattern or not by\nrespectively assigning a 1 or a 0 to each pixel. z′ contains a subset of the nonzero\nelements of x′. z is a perturbed version of x where z′ indicates the visible pixels or\ninput features. This process of sampling subsets of the original input is repeated\nuntil the desired local perturbed dataset Z around x is collected. Finally Z is\nused to train interpretable model g.\net al., 2018] extend LIME to produce local explanations based on if-then rules, such that\nthe explanations are locally anchored upon limited yet suﬃciently stable input features for\nthe given instance and the changes to the rest of input features won’t make an inﬂuence.\nAnother drawback of LIME is, the interpretable linear model is trained based on a large set\nof randomly perturbed instances and the class label of each perturbed instance is assigned\nby inevitably calling the complex opaque model, which is computationally costly. To reduce\nthe time complexity, [Elenberg et al., 2017] introduces STREAK, which is similar to LIME\nbut limits the time of calling complex models, and thus runs much faster. Instead of ran-\ndomly generating instances and training an interpretable linear model, STREAK directly\nselects critical input components (for example, superpixels of images) by greedily solving a\ncombinatorial maximization problem. Taking the image classiﬁcation task as an example,\nan input image which is predicted as a class by the opaque model, is ﬁrst segmented into\nsuperpixels via image segmentation algorithm [Achanta et al., 2012]. In every greedy step, a\nnew superpixel is added to the superpixels set if by containing it in the image will maximize\nthe probability of the opaque model on predicting the given class. The set of superpixels\nindicating the most important image regions of the given input image for the opaque model\nto make its decision. Despite the technical details, some common characteristics are shared\n21\nRas, Xie, van Gerven, & Doran\namong all aforementioned local approximation methods, i) input instance is segmented into\nsemantic meaningful parts for selection, ii) function calls of the original opaque model is\ninevitable iii) the explanations and model behavior are explored in a local fashion.\nShapley Additive Explanations. More commonly referred to as SHAP [Lundberg and\nLee, 2017], this method computes Shapley values [Shapley, 1953] for input feature sets. The\nShapley value explanations are represented as the coeﬃcients in a linear model. SHAP bears\na resemblance to perturbation-based methods because an incomplete (perturbed) input z is\ngiven to the model. The eﬀects of the perturbation are measured, and a score is assigned\nbased on the feature contribution amount. Only in this case, the contribution of adding\na feature is measured and as opposed to measuring the removal of a feature as is the\ncase in perturbation-based methods. The paper presents a framework grounded in game\ntheory that provides a result guaranteeing a unique solution for additive feature attribution\nmethods. According to the paper, various other explanation methods can be considered\nadditive feature attribution methods.\nIn essence, each feature is viewed as a member of a group. The method calculates how\nmuch each member contributes to the group. At the heart of the SHAP lies a function that\nassigns contribution values to each input feature. These are called the Shapley values φi:\nφi =\nX\nS⊆F\\{i}\n|S|!(|F| −|S| −1)!\n|F|!\n[fS∪{i}(xS∪{i}) −fS(xS)]\n(19)\nwhere F is the set of all features, S ⊆F, fS∪{i}(xS∪{i}) is a model trained with a subset\nof features that does not include feature x′\ni and fS(xS) is a model trained with a subset of\nfeatures that does include feature x′\ni. In practice this is implemented as:\nφi(f, x) =\nX\nz′⊆x′\n|z′|!(M −|z′| −1)!\nM!\n[fx(z′) −fx(z′ \\ i)]\n(20)\nwhere z′ ∈{0, 1}M is a simpliﬁed representation of the perturbed sample z indicating\nthe presence of input features x′\ni and M is the number of simpliﬁed input features.\nφi\nis calculated by sampling various combinations of features and measuring the change in\nprediction. In the end, a linear model g is ﬁtted to the features and their eﬀects:\ng(z′) = φ0 +\nM\nX\ni=1\nφiz′\ni\n(21)\nAn example of this process is illustrated in Figure 9.\nThere exist diﬀerent implementations of SHAP. KernelSHAP repeatedly samples fea-\ntures from the input, replacing a subset of the values by random values present in the data.\nThis perturbed input is fed into the model, and the prediction is recorded. Each sampled\nfeature set is assigned a weight using the SHAP kernel. After the sampling is done, a linear\nmodel is ﬁtted. The Shapley values are extracted as the coeﬃcients from the linear model.\nKernelSHAP samples from the marginal distribution and assumes that features are inde-\npendent from each other. However, this is often not the case with real-world data such as\n22\nExplainable Deep Learning:A Field Guide for the Uninitiated\nFigure 9: A visualization of the SHAP algorithm. In this example we have M = 8 simpliﬁed\ninput features and the contribution of feature x′\n4 is being measured.\nnaturalistic images. TreeSHAP was introduced as a faster alternative to KernelSHAP [Lund-\nberg et al., 2018]. Note that TreeSHAP is applicable only to tree-based ML models. The\ncritical diﬀerence and why we mention TreeSHAP is that it samples from the conditional\ndistribution instead of the marginal distribution, i.e., it does not assume that features are\nindependent. Features that on themselves are not relevant for prediction can get a non-\nzero value assigned because they can now be correlated to other features relevant for model\nprediction. This is problematic because it violates the Shapley axiom stating that features\nthat do not contribute to the prediction should have a value of zero. In practice, it means\nthat the explanations produced by TreeSHAP are unreliable. [Heskes et al., 2020] proposes a\ncausal variation on SHAP called causal Shapley values. Causal Shapley values give a causal\ninterpretation to Shapley values, allowing for the diﬀerentiation between direct and indirect\nfeature contributions.\nEven though SHAP is considered a local approximation method, we can run it multiple\ntimes to obtain global explanations. This is where SHAP becomes a compelling method\nsince the global explanations are faithful to the local ones. Assuming suﬃcient Shapley\nvalues are calculated, SHAP becomes an explanation model in itself that can explain any\ninstance. Strong examples are given by [Molnar, 2020].\nAs mentioned before, SHAP uses occlusion to ﬁnd regions in the input that the model’s\noutput is sensitive to, similar to LIME. The other similarity that they share is that the\nexplanation is an interpretable model. In the case of LIME, the choice of model is left to the\npractitioner, while with SHAP, the interpretable model is always a linear model as deﬁned in\nEquation 21. SHAP’s main strength comes from the way it deﬁnes an explanation as additive\nfeature attribution grounded in a game-theoretical perspective. Viewed through this lens,\nvarious explanation methods ﬁt into this framework, and the relationships between methods\nbecome apparent. Like LIME, SHAP was also validated with real human practitioners, and\n23\nRas, Xie, van Gerven, & Doran\nthe results show that SHAP explanations are better aligned with human intuition compared\nto several other methods, including LIME.\n2.2.2 Model Translation\nCompared to local approximation methods, model translation replicates the behavior of a\nDNN across an entire dataset rather than small subsets, through a smaller model that is eas-\nier for explanation. The smaller model could be easier to deploy [Hinton et al., 2015], faster\nto converge [Yim et al., 2017], or even be easily explainable, such as a decision tree [Frosst\nand Hinton, 2017, Bastani et al., 2017, Tan et al., 2018], Finite State Automaton (FSA) [Hou\nand Zhou, 2020], graphs [Zhang et al., 2017, Zhang et al., 2018], or causal and rule-based\nclassiﬁer [Harradon et al., 2018, Murdoch and Szlam, 2017]. We highlight the diversity of\nmodel types DNNs have been distilled into below.\nDistillation to Decision Trees.\nRecent work has been inspired by the idea of tree-\nbased methods for DNNs. [Frosst and Hinton, 2017] proposes “soft decision trees” which\nuse stochastic gradient descent for training based on the predictions and learned ﬁlters of\na given neural network. The performance of the soft decision trees is better than normal\ntrees trained directly on the given dataset, but is worse compared to the given pre-trained\nneural networks. Another recent work is proposed by [Tan et al., 2018] which generates\nglobal additive explanations for fully connected neural networks trained on tabular data\nthrough model distillation. Global additive explanations [Sobol, 2001, Hooker, 2004, Hoos\nand Leyton-Brown, 2014] have been leveraged to study complex models, including analyz-\ning how model parameters inﬂuence model performance and decomposing black box models\ninto lower-dimensional components. In this work, the global additive explanations are con-\nstructed by following previous work [Hooker, 2007] to decompose the black-box model into\nan additive model such as spline or bagged tree. They follow [Craven and Shavlik, 1996] to\ntrain the additive explainable model. [Zhang et al., 2019a] trains a decision tree to depict\nthe reasoning logic of a prep-trained DNN with respect to given model predictions. The\nauthors ﬁrst mine semantic patterns, such as objects, parts, and “decision modes” as funda-\nmental blocks to build the decision tree. The tree is then trained to quantitatively explain\nwhich fundamental components are used for a prediction and the percentage of contribution\nrespectively. The decision tree is organized in a hierarchical coarse-to-ﬁne way, thus nodes\nclose to the tree top correspond to common modes shared by multiple examples, while nodes\nat the bottom represent ﬁne-grained modes with respect to speciﬁc examples.\nFrom a practitioner’s perspective, distilling into decision trees has some advantages, ar-\nguably the most important one being that, compared to potentially subjective heatmaps,\ntrees produce objective rules. Furthermore, formulating explanations as rules has the bene-\nﬁt that other algorithms can directly interpret the rules. The practitioner can then further\nautomate the processing of the resulting explanations in their decision systems.\nDistillation to Finite State Automata. [Hou and Zhou, 2020] introduces a new distil-\nlation of RNNs to explainable Finite State Automata (FSA). It is worth noting that this\nmethod is very specialized, only applicable to RNNs that perform binary classiﬁcations. An\nFSA consists of ﬁnite states and transitions between the states, and the transition from one\n24\nExplainable Deep Learning:A Field Guide for the Uninitiated\nstate to another is a result of external input inﬂuence. FSA is formally deﬁned as a 5-tuple\n(E, S, s0, δ, F), where E is a ﬁnite non-empty set of elements existing in input sequences, S\nis a ﬁnite non-empty set of states, s0 ∈S is an initial state, δ : S × E →S deﬁnes the state\ntransmission function, and F ⊆S is the set of ﬁnal states. The transition process of FSA\nis similar to RNNs in the sense that both methods accept items from some sequence one by\none and transit between (hidden) states accordingly. The idea to distillate an RNN to FSA\nis based on the fact that the hidden states of an RNN tend to form clusters, which can be\nleveraged to build FSA. Two clustering methods, k-means++ and k-means-x are adopted\nto cluster the hidden states of RNN towards constructing the explainable FSA model. The\nauthors follow the structure learning technique and translate an RNN into an FSA, which\nis easier to interpret in two aspects, i) FSA can be simulated by humans; ii) the transi-\ntions between states in FSA have real physical meanings. Such a model translation helps to\nunderstand the inner mechanism of the given RNN model.\nThis is one of the few methods that try to understand the inner states of RNNs. The\nmain beneﬁt of distilling into an FSA is that an FSA can be represented graphically and in\nan objective manner. Similar to rules produced by a decision tree, this leads to objective\nexplanations.\nDistillation into Graphs. Both [Zhang et al., 2017] and [Zhang et al., 2018] build an\nobject parts graph for a pre-trained CNN to provide model explanations. Similar to [Zhang\net al., 2019a], the authors ﬁrst extract semantic patterns in the input and then gradually\nconstruct the graph for an explanation. Each node in the graph represents a part pattern,\nwhile each edge represents co-activation or spatial adjacent between part patterns. The\nexplanatory graph explains the knowledge hierarchy inside the model, which can depict\nwhich nodes/part patterns are activated and the location of the parts in the corresponding\nfeature maps.\nThe beneﬁt of distilling into graphs is that graphs can capture and make transparent\nrelational data. For this method speciﬁcally, the information contained in the graph can\nbe represented in several ways, e.g., as intuitive heatmaps that show which semantically\ninterpretable part of the image was relevant to the prediction of the original model, or a\ngraph that displays the connections between features in the input and how these connections\nrelate to the model prediction.\nDistillation into Causal and Rule-based Models.\nWe also note work on distilling\na DNN into symbolic rules and causal models. [Harradon et al., 2018] constructs causal\nmodels based on concepts in a DNN. The semantics are deﬁned over an arbitrary set of\n“concepts”, that could range from recognition of groups of neuron activations up to labeled\nsemantic concepts. To construct the causal model, concepts of intermediate representations\nare extracted via an autoencoder. Based on the extracted concepts, a graphical Bayesian\ncausal model is constructed to build association for the models’ inputs to concepts, and\nconcepts to outputs. The causal model is ﬁnally leveraged to identify the input features of\nsigniﬁcant causal relevance with respect to a given classiﬁcation result.\nThe practitioner can use this method to relate the model prediction to the model’s\nlearned concepts. So far, the other methods that do this as well were activation minimiza-\ntion and deconvolution. It is diﬃcult to say whether one method is better than the other\n25\nRas, Xie, van Gerven, & Doran\nIntrinsic Methods\nComments\nReferences\nAttention Mechanisms\nLeverage\nattention\nmechanisms\nto\nlearn\nconditional\ndistribution\nover given input units, composing\na weighted contextual vector for\ndownstream processing. The atten-\ntion visualization reveals inherent\nexplainability.\n[Bahdanau et al., 2015, Luong et al., 2015,\nVaswani et al., 2017, Wang et al., 2016, Letarte\net al., 2018, He et al., 2018, Devlin et al.,\n2019, Vinyals et al., 2015, Xu et al., 2015, An-\ntol et al., 2015, Goyal et al., 2017, Teney et al.,\n2018, Mascharka et al., 2018, Anderson et al.,\n2018, Xie et al., 2019, Park et al., 2016]\nJoint Training\nAdd additional explanation “task” to\nthe original model task, and jointly\ntrain the explanation task along\nwith the original task.\n[Zellers et al., 2019, Liu et al., 2019, Park et al.,\n2018, Kim et al., 2018b, Hendricks et al., 2016,\nCamburu et al., 2018, Hind et al., 2019, Melis\nand Jaakkola, 2018, Iyer et al., 2018, Lei et al.,\n2016, Dong et al., 2017, Li et al., 2018a, Chen\net al., 2019]\nTable 3: Intrinsic methods.\nbecause the approaches vary signiﬁcantly: activation minimization generates an image that\nmaximally activates a target region in the network, sometimes producing strange images.\nDeconvolution uses translated convolutions to map the target activations back to the input,\nresulting in patterns representing concepts in the input. The diﬀerence between these two\nmethods and [Harradon et al., 2018] is that the latter visualizes concepts as heatmaps over-\nlaid on the input, rather than generating a potentially uninterpretable image. However, this\napproach can sometimes lead to coarse explanations, especially if a concept is a relatively\nsmall part of the input. Another drawback of this method is that the practitioner needs to\nprovide semantic concept labels, which is not necessary with the two previously mentioned\nmethods.\nIn another example, [Murdoch and Szlam, 2017] leverages a simple rule-based classiﬁer\nto mimic the performance of an LSTM model. This study runs experiments on two natu-\nral language processing tasks, sentiment analysis, and question answering. The rule-based\nmodel is constructed via the following steps: i) decompose the outputs of an LSTM model,\nand generate important scores for each word; ii) based on the word level important score,\nimportant simple phrases are selected according to which jointly have high important scores;\niii) The extracted phrase patterns are then used in the rule-based classiﬁer, approximating\nthe output of the LSTM model. Similar to using decision trees, rules have the advantage of\nbeing objectively interpretable and can be further processed as part of a decision system.\n2.3 Intrinsic Methods\nIdeally, we would like to have models that provide explanations for their decisions as part of\nthe model output, or that the explanation can easily be derived from the model architecture.\nIn other words, explanations should be intrinsic to the process of designing model architec-\ntures and during training. The ability for a network to intrinsically express an explanation\nmay be more desirable compared to post-hoc methods that seek explanations of models that\nwere never designed to be explainable in the ﬁrst place. This is because an intrinsic model\nhas the capacity to learn not only accurate outputs per input but also outputs expressing\nan explanation of the network’s action that is optimal with respect to some notion of ex-\nplanatory ﬁdelity. [Ras et al., 2018] previously deﬁned a category related to this approach\nas intrinsic methods and identiﬁed various methods that oﬀer explainable extensions of the\n26\nExplainable Deep Learning:A Field Guide for the Uninitiated\nmodel architecture or the training scheme. In this section, we extend the notion of intrin-\nsic explainability with models that actually provide an explanation for their decision even\nas they are being trained. Figure 10 shows the diﬀerence between the process of deriving\nexplanations post-hoc compared to intrinsic explanations. The main diﬀerence is that with\nintrinsic explanations, the explanations are part of the model or part of the model output,\ngiving the practitioner additional information during the model training phase that is not\navailable during the post-hoc derivation of explanation.\nLike distillation methods, the practitioner needs to have explicit knowledge about the\nﬁeld of application and the models required to learn explanations intrinsically. Intrinsic\nmethods are arguably the most diﬃcult methods to apply compared to visualization and\ndistillation methods because the practitioner needs additional specialized knowledge and\nbecause the training process will involve multiple models that can make optimization more\nchallenging. Additionally, more computing power is likely required to train a larger pipeline\nof models jointly.\nWe observe methods in the literature on intrinsically explainable DNNs to follow two\ntrends: (i) they introduce attention mechanisms to a DNN, and the attention visualization\nreveals inherent explainability; (ii) they add additional explanation “task” to the original\nmodel task, and jointly train the explanation task along with the original task. We explain\nthe trends and highlight the representative methods below.\n2.3.1 Attention Mechanisms\nDNNs can be endowed with attention mechanisms that simultaneously preserve or even\nimprove their performance and have explainable outputs expressing their operations. An\nattention mechanism [Vaswani et al., 2017, Devlin et al., 2019, Teney et al., 2018, Xie\net al., 2019] learns conditional distribution over given input units, composing a weighted\ncontextual vector for downstream processing. The attention weights can be generated in\nmultiple ways, such as by calculating cosine similarity [Graves et al., 2014], adding addi-\ntive model structure, such as several fully connected layers, to explicitly generate attention\nweights [Bahdanau et al., 2015], leveraging the matrix dot-product [Luong et al., 2015] or\nscaled dot-product [Vaswani et al., 2017], and so on. Attention mechanisms have shown to\nimprove DNN performance for particular types of tasks, including tasks on ordered inputs\nas seen in natural language processing [Vaswani et al., 2017, Devlin et al., 2019] and multi-\nmodal fusion such as visual question answering [Anderson et al., 2018]. It is worth noting\nthat recently some interesting discussions have been raised on whether or not attention can\nbe counted as an explanation tool [Jain and Wallace, 2019, Wiegreﬀe and Pinter, 2019]; this\nwill be discussed in Section 3.\nSingle-Modal Weighting. The output of attention mechanisms during a forward pass\ncan inform a practitioner about how diﬀerent input features are weighted at diﬀerent phases\nof model inference. In pure text processing tasks such as language translation [Bahdanau\net al., 2015, Luong et al., 2015, Vaswani et al., 2017] or sentiment analysis [Wang et al.,\n2016, Letarte et al., 2018, He et al., 2018], attention mechanism allows the downstream\nmodules, a decoder for language translation or fully connected layers for classiﬁcation tasks,\nto concentrate on diﬀerent words in the input sentence by assigning learned weights to\n27\nRas, Xie, van Gerven, & Doran\nFigure 10: A process comparison between deriving an explanation post-hoc vs. a model\nwhere the explanations are intrinsic. The goal is to produce a trained, explain-\nable model. When extracting explanations post-hoc, the process consists of sep-\narate training and explanation stages. The explanations can potentially guide\nthe original model for performance improvement (grey arrows going backward).\nFor intrinsic explanation, generating explanations is integrated into the train-\ning process. It is worth noting that using intrinsic explanations could be more\ntime-consuming since the model needs to learn a more complex problem.\nthem [Vaswani et al., 2017, Wang et al., 2016]. To provide straightforward explanations,\nthe attention weights can be visualized as heatmaps, depicting the magnitude and the sign\n(positive or negative) of each weight value, showing how input elements weighted combined\nto inﬂuence the model latter processing and the ﬁnal decisions.\nFigure 11 gives a visual example of how attention can be used for explanation for single-\nmodal weighting. The left-hand side of the ﬁgure depicts a basic RNN encoder-decoder\narchitecture where the dense representation of text input is associated with the attention\nweights. These weights can be plotted as a matrix, and each input’s importance as it relates\nto the output can directly be interpreted. This type of explanation can help the practitioner\nmonitor model predictions during training and give insight into whether the model utilizes\nundesirable correlations in the dataset. However, the practitioner needs to keep in mind\nthat it will become cumbersome to monitor individual model predictions as the input size\n28\nExplainable Deep Learning:A Field Guide for the Uninitiated\nFigure 11: Left: High-level overview of using attention as explanation. The model takes\nan English sentence as input and outputs a Dutch translation. During the for-\nward pass, the alignment scores α (attention weights) are calculated as part of\nthe training process and can immediately be visualized as a heatmap. The α\nmaps the correlation between the diﬀerent parts of the input (hidden states h)\nand output (hidden states s). Alignment score function f determines how α is\ncomputed. For a detailed explanation on how attention works, see [Bahdanau\net al., 2015]. Right: The attention matrix where each value is the alignment\nscore αn,m between encoder hidden state hm and decoder hidden state sn.\nincreases.\nMulti-Modal Interaction.\nIn multi-modal interaction tasks, such as image caption-\ning [Vinyals et al., 2015, Xu et al., 2015], visual question answering [Antol et al., 2015, Goyal\net al., 2017, Johnson et al., 2017, Teney et al., 2018] or visual entailment [Xie et al., 2019],\nattention mechanisms play an important role in feature alignment and fusion across diﬀerent\nfeature spaces (for instance, between text and images). For example, Park et al. propose the\nPointing and Justiﬁcation model that uses multiple attention mechanisms to explain the an-\nswer of a VQA task with natural language explanations and image region alignments [Park\net al., 2016]. Xie et al. use attention mechanisms to recover semantically meaningful areas\nof an image that correspond to the reason a statement is, is not, or could be entailed by the\nimage’s conveyance [Xie et al., 2019]. [Mascharka et al., 2018] aims to close the gap between\nperformance and explainability in visual reasoning by introducing a neural module network\nthat explicitly models an attention mechanism in image space. By passing attention masks\nbetween modules it becomes explainable by being able to directly visualize the masks. This\nshows how the attention of the model shifts as it considers the diﬀerent components of the\ninput.\nMulti-modal interaction methods go one step beyond single-modal weighting by combin-\ning multiple attention mechanisms with supplementary tasks that increase (i) the model’s\n29\nRas, Xie, van Gerven, & Doran\ninterpretability and (ii) give the practitioner additional opportunities for creating explana-\ntions that cater to the area of application. However, compared to single-modal weighting,\nmulti-modal interaction can be more diﬃcult to apply due to the higher complexity accom-\npanying the increased combination of attention components and multiple tasks.\n2.3.2 Joint Training\nThis type of intrinsic method is to introduce an additional “task” besides the original model\ntask, and jointly train the additional task together with the original one. Here we generalize\nthe meaning of a “task” by including preprocessing or other steps involved in the model\noptimization process. The additional task is designed to provide model explanations directly\nor indirectly. Such additional task can be in the form of i) text explanation, which is a task\nthat directly provides explanations in natural language format; ii) explanation association,\nwhich is a step that associates input elements or latent features with human-understandable\nconcepts or objects, or even directly with model explanations; iii) model prototype which\nlearns a prototype that has clear semantic meanings as a preprocessing step. Explanations\nare generated based on the comparison between the model behavior and the prototype.\nIn Figure 12 we see a high-level overview of the joint training with, in this case, an\nexample of an image captioning task. The objective\narg min\nθ\n1\nN\nN\nX\nn=1\nαL(yn, y′) + L(en, e′)\n(22)\nis a very general form of the function that has to be minimized and is composed of at\nleast two losses: the prediction loss and the explanation component loss. By weighting each\nloss, a balance can be found between having a model that gives good predictions and a\nmodel that gives good explanations.\nText Explanation. A group of recent work [Zellers et al., 2019, Liu et al., 2019, Park\net al., 2018, Kim et al., 2018b, Hendricks et al., 2016, Camburu et al., 2018, Hind et al.,\n2019] achieve the explainable goal via augmenting the original DNN architecture with an\nexplanation generation component and conducting joint training to provide natural language\nexplanations along with the model decisions, similar to what is illustrated in Figure 12.\nSuch explainable methods are quite straightforward and the explanations that they produce\nare layman-friendly since the explanations are presented directly using natural language\nsentences, instead of ﬁgures or statistical data that usually require professional knowledge\nto digest. The explanation could be either generated word by word similar to a sequence\ngeneration task [Hendricks et al., 2016, Park et al., 2018, Camburu et al., 2018, Kim et al.,\n2018b, Liu et al., 2019], or be predicted from multiple candidate choices [Zellers et al., 2019].\nThe primary advantage of joint training text explanations is that the practitioner can\ntailor the explanations to the users’ needs while using state-of-the-art models. This way, the\npractitioner can get the best of both worlds. On the other hand, obtaining the appropriate\n(labeled) dataset for the explanation generation component is diﬃcult and time-consuming.\nFurthermore, the practitioner must overcome the additional diﬃculties that joint training\npresents when training multiple models jointly instead of training a single model. Finally, it\n30\nExplainable Deep Learning:A Field Guide for the Uninitiated\nFigure 12: High level overview of how joint training works. The algorithm tries to min-\nimize the average combined weighted loss over the output prediction and the\nexplanation generation, where α denotes the weight.\nis known that the generated explanations exhibit some inconsistencies [Oana-Maria et al.,\n2019] which undermines the trust in the explanations provided by the model.\n[Hendricks et al., 2016] is an early work that provides text justiﬁcations along with\nits image classiﬁcation results. The approach combines image captioning, sampling, and\ndeep reinforcement learning to generate textual explanations. The class information is in-\ncorporated into the text explanations, which makes this method distinct from normal image\ncaptioning models that only consider visual information, via i) include class as an additional\ninput for text generation and ii) adopt a reinforcement learning based loss that encourages\ngenerated sentences to include class discriminative information.\n[Liu et al., 2019] proposes a Generative Explanation Framework (GEF) for text clas-\nsiﬁcations. The framework is designed to generate ﬁne-grained explanations such as text\njustiﬁcations. During training, both the class labels and ﬁne-grained explanations are pro-\nvided for supervision, and the overall loss of GEF contains two major parts, classiﬁcation\nloss and explanation generation loss.\nTo make the generated explanations class-speciﬁc,\n“explanation factor” is designed in the model structure to associate explanations with classi-\nﬁcations. The “explanation factor” is intuitively based on directly taking the explanations as\n31\nRas, Xie, van Gerven, & Doran\ninput for classiﬁcation and adding constraints on the classiﬁcation softmax outputs. Specif-\nically, “explanation factor” is formulated to minimize the pairwise discrepancy in softmax\noutputs for diﬀerent input pairs, i) generated explanations and ground-truth explanations,\nand ii) generated explanations and original input text.\nUnlike the previously methods which generate text explanations, [Zellers et al., 2019]\nprovides explanations in a multichoice fashion. They propose a visual reasoning task named\nVisual Commonsense Reasoning (VCR), which is to answer text questions based on given\nvisual information (image), and provide reasons (explanations) accordingly. Both the an-\nswers and reasons are provided in a multichoice format. Due to the multichoice nature,\nreasonable explanations should be provided during testing, in contrast to other works which\ncould generate explanations along with model decisions. Thus VCR is more suitable to be\napplied for prototype model debugging to audit model reasoning process, instead of real-life\napplications where explanations are usually lacking and remain to be generated.\nExplanation Association. This type of joint training method associates input elements or\nlatent features with human-understandable concepts or objects, or even directly with model\nexplanations, which helps to provide model explanations intrinsically [Melis and Jaakkola,\n2018, Iyer et al., 2018, Lei et al., 2016, Dong et al., 2017]. Such methods usually achieve\nexplanations by adding regularization term [Melis and Jaakkola, 2018, Lei et al., 2016,\nDong et al., 2017] and/or revising model architecture [Melis and Jaakkola, 2018, Iyer et al.,\n2018, Lei et al., 2016]. The explanations are provided in the form of i) associating input\nfeatures or latent activations with semantic concepts [Melis and Jaakkola, 2018, Dong et al.,\n2017]; ii) associating model prediction with a set of input elements [Lei et al., 2016]; iii)\nassociating explanations with object saliency maps in a computer vision task [Iyer et al.,\n2018]. Regardless of the format of explanations and the technical details, methods belonging\nto this type commonly share the characteristics of associating hard-to-interpret elements to\nhuman-understandable atoms in an intrinsic joint training fashion.\n[Melis and Jaakkola, 2018] proposes an intrinsic method which associates input features\nwith semantically meaningful concepts and regards the coeﬃcient as the importance of\nsuch concepts during inference. A regularization based general framework for creating self-\nexplaining neural networks (SENNs) is introduced. Given raw input, the network jointly\nlearns to generate the class prediction and to generate explanations in terms of an input\nfeature-to-concept mapping. The framework is based on the notion that linear regression\nmodels are explainable and generalizes the respective model deﬁnition to encompass complex\nclassiﬁcation functions, such as a DNN. A SENN consists of three components: i) A “concept\nencoder” that transforms the raw input into a set of explainable concepts. Essentially this\nencoder can be understood as a function that transforms low-level input into high-level\nmeaningful structure, which predictions and explanations can be built upon. ii) An “input-\ndependent parametrizer”, which is a procedure to get the coeﬃcient of explainable concepts,\nlearns the relevance of the explainable concepts for the class predictions. The values of\nthe relevance scores quantify the positive or negative contribution of the concept to the\nprediction. iii) Some “aggregation function” (e.g. a sum) that combines the output of the\nconcept encoder and the parametrizer to produce a class prediction.\n[Iyer et al., 2018] introduces Object-sensitive Deep Reinforcement Learning (O-DRL),\nwhich is an explanation framework for reinforcement learning tasks that takes videos as\n32\nExplainable Deep Learning:A Field Guide for the Uninitiated\ninput.\nO-DRL adds a pre-processing step (template matching) to recognize and locate\nspeciﬁc objects in the input frame. For each detected object, an extra channel is added to\nthe input frame’s RGB channels. Each object channel is a binary map that has the same\nheight and width as the original input frame, 1’s encoding for the location of the detected\nobject. The binary maps are later used to generate object saliency maps (as opposed to\npixel saliency maps) that indicate the relevance of the object to action generation. It is\nargued that object saliency maps are more meaningful and explainable than pixel saliency\nmaps since the objects encapsulate a higher-level visual concept.\n[Lei et al., 2016] integrates explainability in their neural networks for sentiment anal-\nysis by learning rationale extraction during the training phase in an unsupervised manner.\nRationale extraction is done by allowing the network to learn to identify a small subset of\nwords that all lead to the same class prediction as the entire text. They achieve this by\nadding mechanisms that use a combination of a generator and an encoder. The generator\nlearns which text fragments could be candidate rationales and the encoder uses these can-\ndidates for prediction. Both the generator and the encoder are jointly trained during the\noptimization phase. The model explanation is provided by associating the model prediction\nwith a set of critical input words.\n[Dong et al., 2017] focuses on providing intrinsic explanations for models on video\ncaptioning tasks. An interpretive loss function is deﬁned to increase the visual ﬁdelity of the\nlearned features. This method is based on the nature of the used dataset, which contains rich\nhuman descriptions along with each video, and the rich text information can be leveraged to\nadd constraint towards explainability. To produce an explanation, semantically meaningful\nconcepts are ﬁrst pre-extracted from human descriptions via Latent Dirichlet Allocation,\nwhich covers a variety of visual concepts such as objects, actions, relationships, etc. Based\non the pre-extracted semantic topic, an interpretive loss is added to the original video\ncaptioning DNN model, for jointly training to generate video captions along with forcing\nthe hidden neurons to be associated with semantic concepts.\nFrom a practitioner’s perspective, explanation association can be powerful because se-\nmantically meaningful concepts are directly interpretable by humans. Explanations based\non these concepts can be represented in various forms, e.g., a relational graph or a heatmap,\nunlike text explanations which are limited to text only. Similar to text explanations, ex-\nplanation association can also require specialized (labeled) datasets, which are diﬃcult to\nobtain. Unlike text explanations, which mostly use an external component to generate the\nexplanations, explanation associations often use internal model representations to associate\nhigh-level concepts. The practitioner may need to modify existing models to gain access\nto these internal representations, or in some cases, the internal representations might not\nbe accessible to the practitioner. Furthermore, various methods in this category require a\npipeline of several models where each model can become a bottleneck in the joint training\nprocess.\nModel Prototype. This type of intrinsic method is speciﬁcally for classiﬁcation tasks, and\nis derived from a classical form of case-based reasoning [Kolodner, 1992] called prototype\nclassiﬁcation [Marchette and Socolinsky, 2003, Bien and Tibshirani, 2011, Kim et al., 2014].\nA prototype classiﬁer generates classiﬁcations based on the similarity between the given\ninput and each prototype observation in the dataset. In prototype classiﬁcation applications,\n33\nRas, Xie, van Gerven, & Doran\nthe word “prototype” is not limited to an observation in the dataset, but can be generalized\nto a combination of several observations or a latent representation learned in the feature\nspace. To provide intrinsic explanations, the model architecture is designed to enable joint\ntraining the prototypes along with the original task. The model explainability is achieved\nby tracing the reasoning path for the given prediction back to each prototype learned by the\nmodel.\n[Li et al., 2018a] proposes an explainable prototype-based image classiﬁer that can trace\nthe model classiﬁcation path to enable reasoning transparency. The model contains two\nmajor components; an autoencoder and a prototype classiﬁer. The autoencoder, containing\nan encoder and a decoder, is to transform raw input into a latent feature space, and the latent\nfeature is later used by the prototype classiﬁer for classiﬁcation. The prototype classiﬁer, on\nthe other hand, generates a classiﬁcation via i) ﬁrst calculating the distances in the latent\nspace between a given input image and each prototype, ii) then passing through a fully-\nconnected layer to compute the weighted sum of the distances, and iii) ﬁnally normalizing the\nweighted sums by the softmax layer to generate the classiﬁcation result. Because the network\nlearns prototypes during the training phase, each prediction always has an explanation that\nis faithful to what the network actually computes. Each prototype can be visualized by\nthe decoder, and the reasoning path of the prototype classiﬁer can be partially traced given\nthe fully-connected layer weights and the comparison between input and each visualized\nprototype, providing intrinsic model explanations.\n[Chen et al., 2019] introduces an explainable DNN architecture called Prototypical Part\nNetwork (ProtoPNet) for image classiﬁcation tasks. Similar to [Li et al., 2018a], ProtoP-\nNet also contains two components; a regular convolutional neural network and a prototype\nclassiﬁer.\nThe regular convolutional neural network projects the raw image into hidden\nfeature space, where prototypes are learned. The prototype classiﬁer is to generate model\npredictions based on the weighted sum of each similarity score between an image patch and\na learned prototype. Unlike [Li et al., 2018a] where learned prototypes are corresponding\nto the entire image, the prototypes in [Chen et al., 2019] are more ﬁne-grained and are\nlatent representations of parts/patches of the image. To provide a model explanation, the\nlatent representation of each prototype is associated with an image patch in the training\nset, shedding light on the reasoning clue of ProtoPNet.\nCompared to the other methods in this category, prototypes adopt a diﬀerent approach\ntowards creating interpretable model architecture. Other methods tend to use existing model\narchitectures and make adaptations that either grant the practitioner access to either internal\nmodel components or add models to the existing model pipeline. In the case of prototypes,\nthe practitioner often creates a novel architecture with traceable paths of reasoning. In some\nsense, model prototypes avoid end-to-end architectures where a single DNN learns the entire\ntask. Instead, the DNN is constructed having explicitly interpretable components baked in\nas part of its design.\n2.4 A Methods Lookup Table\nMethods discussed in this ﬁeld guide are categorized by distinct philosophies on eliciting\nand expressing an explanation from a DNN. This organization is ideal to understand the\n“classes” of methods that are being investigated in research and gradually implemented in\n34\nExplainable Deep Learning:A Field Guide for the Uninitiated\npractice.\nThis does not, however, resolve an obvious question from a machine learning\npractitioner: What is the “right” type of explanatory method I should use when building\na model to solve my speciﬁc kind of problem?\nIt is diﬃcult to match the methods with\na particular situation because the type of explanation method suitable to that particular\nsituation is often dependent on many variables including the type of DNN architecture, data,\nproblem, and desired form of explanation.\nWe propose Table 4 and Table 5 as a starting point in answering this question. All of\nthe papers in Figure 3 are organized in Table 4 and Table 5. Each table is titled with the\nmain category of explanation method. Both tables are organized into ﬁve columns. The ﬁrst\ncolumn indicates the subcategory of the explanation method and the second column displays\nthe reference to the explanation paper. The following three columns contain summarized in-\nformation taken directly from the explanation paper. The third and fourth columns contain\none or more icons representing the type of data used and the type of problem(s) presented\nin the paper respectively. The meaning of the icons can be found at the bottom of the\ntable. The ﬁnal column displays information about the speciﬁc DNN model on which the\nexplanation method has been used in the paper.\nThe practitioner can make use of Table 4 and Table 5 by considering what type of\ndata, problem, and DNN architecture they are using in their speciﬁc situation. Then the\npractitioner can ﬁnd an appropriate explanation method by matching the type of data,\nproblem, and DNN architecture with the ones in the tables. For example, if a practitioner\nis using an image dataset to train a CNN on a classiﬁcation problem, the practitioner can\nmake use of all the explanation methods for which the é icon and the © icon and “CNN”\nare present in the respective rows. Note that in the “DNN Type” column we use “no speciﬁc\nrequirements” to indicate that the DNN used in the respective paper does not need to meet\nany other speciﬁc requirements other than being a DNN. We use “model agnostic” to indicate\nthat the type of model does not matter, i.e., the model does not have to be a DNN.\n3. Evaluating Explanations\nThere is a growing body of research on the objective comparison of explanation methods\nand their quality. It is important to be able to evaluate the factual quality of the generated\nexplanation. Evidence suggests that when humans and AI collaborate, humans often make\nbetter decisions when the AI provides a correct explanation [Ray et al., 2019]. When the\nexplanation is incorrect it can lead to bad outcomes [Jacovi and Goldberg, 2020]. In addi-\ntion, practitioners need to know whether they can trust the explanation that the methods\nreturn. It is well known that explanation methods are subject to misinterpretation, espe-\ncially visual explanation methods [Kindermans et al., 2019, Nie et al., 2018, Adebayo et al.,\n2018, Alvarez-Melis and Jaakkola, 2018]. There are concerns regarding the factual correct-\nness of explanation methods such as deconvolution, guided backpropagation and LRP [Kin-\ndermans et al., 2018]. One main challenge for explanation evaluations is the lack of ground\ntruth for most cases. In addition, the favorable evaluation metric may vary a lot according\nto the speciﬁc evaluation goal and oriented user groups.\n35\nRas, Xie, van Gerven, & Doran\na) Visualization Methods\nExplanation Paper\nData\nType\nProblem\nType\nDNN Type\nBack-Propagation\n[Erhan et al., 2009]\né\n©\nclassiﬁer has to be diﬀerentiable\n[Zeiler et al., 2011]\né\n©\nCNN with max-pooling + relu\n[Zeiler and Fergus, 2014]\né\n©\nCNN with max-pooling + relu\n[Selvaraju et al., 2017]\né\n©\nÐ Â\nCNN\n[Zhou et al., 2016]\né Z\n©\n®\nÐ )\nCNN with global average pooling\n+ softmax output\n[Bach et al., 2015]\né Z\n©\nmultilayer network\n[Lapuschkin et al., 2016]\né\n©\nCNN\n[Arras et al., 2016]\nY\n©\nCNN\n[Arras et al., 2017]\nY\n©\nCNN\n[Ding et al., 2017]\nZ\n\u001c\nattention-based encoder decoder\n[Montavon et al., 2017]\nagnostic\n©\nno speciﬁc requirements\n[Shrikumar et al., 2017]\né \u0007\n©\nCNN\n[Sundararajan\net\nal.,\n2017]\né Z\nâ\n©\n\u001c\nno speciﬁc requirements\n[Sundararajan\net\nal.,\n2016]\né Z\nâ\n©\nno speciﬁc requirements\nPerturbation\n[Zeiler and Fergus, 2014]\né\n©\nCNN\n[Li et al., 2016]\nY\n©\nS\nno speciﬁc requirements\n[Fong and Vedaldi, 2017]\né\n©\nmodel agnostic\n[Zintgraf et al., 2017]\né\n©\nCNN\n[Robnik-Šikonja\nand\nKononenko, 2008]\n\u0012\n©\nmodels has to output probabilities\n[Dabkowski\nand\nGal,\n2017]\né\n©\nclassiﬁer has to be diﬀerentiable\nb) Model Distillation\nLoc. Appr.\n[Ribeiro et al., 2016c]\né Z\n\u0012\n©\nmodel agnostic\n[Ribeiro et al., 2016b]\né Z\n\u0012\n©\nmodel agnostic\n[Ribeiro et al., 2018]\né Z\nL\n©\n\f \u001c\nmodel agnostic\n[Elenberg et al., 2017]\né\n©\nmodel agnostic\n[Baehrens et al., 2010]\né Y \u0012\n©\nmodel agnostic\n[Lundberg and Lee, 2017]\né Z\n\u0012\n©\nmodel agnostic\n[Heskes et al., 2020]\né Z\n\u0012\n©\nmodel agnostic\nModel Translation\n[Hou and Zhou, 2020]\nZ\n©\nRNN\n[Murdoch\nand\nSzlam,\n2017]\nZ\n©\nØ\nLSTM\n[Harradon et al., 2018]\né\n©\nCNN\n[Frosst and Hinton, 2017]\né\n©\nCNN\n[Zhang et al., 2019a]\né\n©\nCNN\n[Tan et al., 2018]\nL\n©\n®\nno speciﬁc requirements\n[Zhang et al., 2017]\né\n©\nCNN\n[Zhang et al., 2018]\né\n©\nCNN, GANs\nData Types\nProblem Types\né\nimage\n©\nclassiﬁcation\nZ\ntext\n)\nlocalization\nâ\nmolecular graph\nÐ\nvisual question answering\n\u0007\nDNA sequence\n®\nregression\nY\nembedding\n\u001c\nlanguage translation\n\u0012\ncategorical data\nS\nsequence tagging\nL\ntabular data\n\f\nstructured prediction\n\u001c\ntext generation\nØ\nquestion answering\nÂ\ncaptioning\nTable 4: Lookup table for the (a) visualization and (b) model distillation methods.\n36\nExplainable Deep Learning:A Field Guide for the Uninitiated\nIntrinsic Methods\nExplanation Paper\nData\nType\nProblem\nType\nDNN Type\nAttention Mechanisms\n[Vaswani et al., 2017]\nZ\n\u001c\ntransformer\n[Devlin et al., 2019]\nZ\ni\ntransformer\n[Bahdanau et al., 2015]\nZ\n\u001c\nRNN encoder-decoder\n[Luong et al., 2015]\nZ\n\u001c\nstacking LSTM\n[Wang et al., 2016]\nZ\n♥\nattention-based LSTM\n[Letarte et al., 2018]\nZ\n♥\n©\nself-attention network\n[He et al., 2018]\nZ\n♥\nattention-based LSTM\n[Teney et al., 2018]\né Z\nÐ\nCNN + GRU combination\n[Mascharka et al., 2018]\né\nÐ\nvarious specialized modules\n[Xie et al., 2019]\né\n£\nvarious specialized modules\n[Park et al., 2016]\né Z\nÐ\nvarious specialized modules\n[Vinyals et al., 2015]\né\nÂ\nCNN + LSTM combination\n[Xu et al., 2015]\né\nÂ\nCNN + RNN combination\n[Antol et al., 2015]\né Z\nÐ\nCNN + MLP, CNN + LSTM\ncombinations\n[Goyal et al., 2017]\né Z\nÐ\nCNN + LSTM combination\n[Anderson et al., 2018]\né Z\nÐ Â\nregion proposal network +\nresnet combo, LSTM\nJoint Training\n[Camburu et al., 2018]\nZ\ni\nLSTM\n[Hind et al., 2019]\nY\n©\nmodel agnostic\n[Hendricks et al., 2016]\né\n©\nCNN\n[Zellers et al., 2019]\né Z\n\u0010\nrecognition to cognition net-\nwork\n[Liu et al., 2019]\nZ\n©\nencoder-predictor\n[Park et al., 2018]\né Z\n©\nÐ\npointing\nand\njustiﬁcation\nmodel\n[Kim et al., 2018b]\né\nx\nCNN\n[Lei et al., 2016]\nZ\n♥\nencoder-generator\n[Melis\nand\nJaakkola,\n2018]\né \u0012 L\n©\nself-explaining\nneural\nnet-\nwork\n[Iyer et al., 2018]\né\nÄ\ndeep q-network\n[Dong et al., 2017]\n¿\nÂ\nattentive encoder-decoder\n[Li et al., 2018a]\né\n©\nautoencoder\n+\nprototype\nlayer combination\n[Chen et al., 2019]\né\n©\nprototypical part network\nData Types\nProblem Types\né\nimage\n©\nclassiﬁcation\nZ\ntext\nÐ\nvisual question answering\nY\nembedding\n\u001c\nlanguage translation\n\u0012\ncategorical data\nÂ\ncaptioning\nL\ntabular data\n♥\nsentiment analysis\n¿\nvideo\n£\nvisual entailment\n\u0010\nvisual commonsense reasoning\ni\nlanguage understanding\nÄ\nreinforcement learning\nx\ncontrol planning\nTable 5: Lookup table for the intrinsic methods.\n37\nRas, Xie, van Gerven, & Doran\n3.1 What Makes a Good Explanation?\nThe answer to this question depends on the user, the context of use, the type of model and\ndata, and the desired explanation form. To address this need, literature has come up with\nvarious desiderata [Ras et al., 2018, Robnik-Šikonja and Bohanec, 2018, Carvalho et al., 2019,\nJacovi and Goldberg, 2020]. The traits ﬁdelity, consistency, stability and comprehensibility\nare most commonly scrutinized and discussed.\n3.2 Methods for Evaluating Explanation Methods and their Explanations\nPractically there are two main approaches for evaluating explanations. The ﬁrst is to devise\nan objective metric or benchmark to evaluate the explanations without human interven-\ntion [Samek et al., 2016, Hooker et al., 2019, Vu et al., 2019, Adebayo et al., 2018, Alvarez-\nMelis and Jaakkola, 2018]. This approach has the beneﬁt of being able to compare numerous\nexplanation methods with each other. Using objective benchmarks we can investigate to\nwhat extent desiderata such as ﬁdelity, consistency and stability are being satisﬁed. Given\nthat visualization methods that produce heatmaps are a popular and intuitive type of expla-\nnation method, it has gained most of the attention in the subﬁeld of evaluation explanation.\nSpeciﬁcally, evaluating heatmaps generated for image classiﬁcation networks is the focus of\nvarious evaluation work. There also exists a small body of work in evaluating explanations\nin NLP that we will discuss shortly [DeYoung et al., 2020]. The second approach is to let a\nhuman evaluate the explanations [Prasad et al., 2020, Hase and Bansal, 2020, Jesus et al.,\n2021]. By using humans to evaluate the explanations, we can investigate to what degree the\nfollowing desiderata are satisﬁed: clarity, parsimony, comprehensibility and importance.\n3.2.1 Evaluating Heatmaps\nEven though the following evaluations focus on heatmaps derived from image classiﬁers,\nthey can also be applied to heatmaps of text in NLP explanations, e.g., LIME or SHAP.\n[Samek et al., 2016] and variations [Kindermans et al., 2018, Petsiuk et al., 2018] introduce a\nperturbation-based method for evaluating the quality of heatmaps. Using the method, they\ncompare the quality of heatmaps generated by sensitivity analysis [Simonyan et al., 2013],\ndeconvolution [Zeiler and Fergus, 2014] and LRP [Bach et al., 2015] by replacing the regions\nof the image that correspond to the location of the heatmap with randomly uniform data\nand checking how much the classiﬁcation score changes. According to their metric, the more\nthe classiﬁcation score changes, the better the heatmap corresponds to class-discriminating\nfeatures. Their results show that the heatmaps produced by LRP correspond better to the\nclass features compared to heatmaps produced by sensitivity analysis and deconvolution.\nHowever, [Hooker et al., 2019] argues that the perturbation-based method violates the\nassumption that the training and evaluation data come from the same distribution.\nIn\nresponse, [Hooker et al., 2019] proposes a benchmark for evaluating feature importance\nestimates in DNNs. Their benchmark is called ROAR: RemOve And Retrain. The goal of\nROAR is to determine whether the removal of important information caused classiﬁcation\ndegradation or whether the introduction of the so-called uninformative information caused\nthe modiﬁed images to go out of distribution, thereby causing classiﬁcation degradation.\nIt replaces the fraction of pixels deemed important according to some heatmap with the\nchannel mean, similar to the perturbation-based method [Samek et al., 2016]. However,\n38\nExplainable Deep Learning:A Field Guide for the Uninitiated\nthere is an important diﬀerence: to deal with the fallacy of introducing out of distribution\nimages in the evaluation phase, ROAR applies similar modiﬁcations to all the images in\nboth training and test set. The explanation method is applied to all the train and test\nset images to obtain a heatmap for each image. Then they remove the same percentage\nof the deemed important pixels from the image and replace it with the channel mean of\nthat image.\nFinally, they train separate models on the modiﬁed data and evaluate the\nclassiﬁcation accuracy. If the accuracy of the re-trained models goes down, we can say with\nsome certainty that the removed information in the modiﬁed image is indeed the cause of\nthe classiﬁcation degradation. Methods like [Kindermans et al., 2019, Kindermans et al.,\n2016] investigate the reliability of heatmaps by modifying the input with information that\ndoes not change the classiﬁcation result and checking how the heatmaps change as a result.\nThey ﬁnd that various visualization methods are vulnerable to input modiﬁcation and return\nincorrect heatmaps as a result. The main conclusion is that many visualization methods are\nunreliable because they do not satisfy input invariance.\nIn contrast to the previous methods, [Vu et al., 2019] suggests a metric to evaluate\nheatmaps based on perturbing regions that are not indicated as important. Their metric is\ncalled c-Eval, where c is a number that indicates how robust the classiﬁer is to perturba-\ntions in regions deemed as not important by the explanation method. This method indirectly\nmeasures how accurate the heatmaps are: the larger c, the more robust the classiﬁer, the\nmore accurate the explanation method is at identifying class-discriminating features. Us-\ning c-Eval they compare various explanation methods and ﬁnd that there is a signiﬁcant\ndiﬀerence in the quality of heatmaps produced by black-box methods (e.g., SHAP, LIME)\ncompared to back-propagation based methods (e.g., LRP, DeepLIFT).\nIn an alternative approach, [Adebayo et al., 2018] proposes two sanity checks for evalu-\nating the quality of heatmaps. The ﬁrst is the model parameter randomization test, and it\ncompares heatmaps generated by a trained model with heatmaps generated by a randomly\ninitialized model. If the outputs are similar, the explanation method is insensitive to model\nproperties such as the weights. The second sanity check is the data randomization test, and\nit compares heatmaps generated by a model trained on the original dataset with heatmaps\ngenerated by a model trained on a version of the dataset where all the labels have been ran-\ndomly permuted. If the heatmaps are similar, it indicates that the explanation method is\nnot dependent on the relationship between the data and the labels that exist in the original\ndata. The distance between the heatmaps are measured using various similarity metrics.\n3.2.2 Evaluating NLP Explanations\nThe use of attention-based deep learning models for NLP have increased signiﬁcantly [Vaswani\net al., 2017, Brown et al., 2020]. Attention has often been argued to be intrinsically inter-\npretable, see Section 2.3. However, recent studies [Jain and Wallace, 2019, Serrano and\nSmith, 2019, Baan et al., 2019] show that attention is not always interpretable and that\nattention does not always lead to insight into model prediction. [Jain and Wallace, 2019]\nﬁrst raises the point that, while we make the assumption that attention is implicitly inter-\npretable because directly it provides insight into which words are important, this assumption\nhas never been formally evaluated. That is, the relationship between attention weights and\nmodel output is not clear. The experiments of their results show that correlation between\n39\nRas, Xie, van Gerven, & Doran\nfeature importance measures, like heatmapping, and the learned attention weights is weak.\nThe results suggest that the ability of attention modules to provide meaningful explana-\ntions into model prediction is questionable at best. [Serrano and Smith, 2019] manipulates\nthe attention weights in trained models and analyzes the resulting diﬀerence in model pre-\ndictions. Their ﬁndings are mixed: sometimes higher attention weights correlate to model\npredictions but not always. [Baan et al., 2019] reveals that some attention heads tend to\nspecialize towards interpretable parts of a document, but this ability does not generalize to\nall documents. Also the specializations are not consistent over diﬀerently initialized models.\nTo reconcile these conﬂicting views some studies [Vashishth et al., 2019, Wiegreﬀe and\nPinter, 2019] conduct experiments to ﬁnd situations where attention can be used to gain\ninsight into model prediction. [Vashishth et al., 2019] presents experiments over a range of\nNLP tasks justifying both observations. They identify conditions when the attention weights\nare interpretable and correlate with text heatmaps. Their results also reveal that attention\nweights are not interpretable when the input only has a single sequence by showing that in\nthis situation the attention weights function as a gating unit. [Wiegreﬀe and Pinter, 2019]\nprovides a set of experiments that show that attention can be used to gain insight into\nmodel predictions. Their conclusion is that the results from [Jain and Wallace, 2019] do not\ndisprove that attention can serve as an explanation.\n3.2.3 Using Humans to Evaluate Explanations\nResearch in this explanation sub-ﬁeld is still in its infancy and given that there is signiﬁcant\nvariation among people, contexts and their needs, the results of the papers in this section\nshould not be taken as absolute. In contrast to the previous evaluation methods, this section\naddresses methods that are concerned with how interpretable explanations are to humans.\nThis approach has the beneﬁt of revealing to what extent the speciﬁc setting and explanation\nmethod is interpretable and useful to people who will use them. Research has revealed that\nthere is still a big gap between the perceived and actual usefulness of explanations.\nModel interpretability can be understood as how easy it is for a human to predict the\noutput of the model on new input based on past predictions. This concept is called sim-\nulatability. It was found that LIME improves simulatability of models trained on tabular\ndata [Hase and Bansal, 2020], however, subjective ratings about the explanations did not\npredict how useful the explanations actually were.\nAnother way to judge explanations using a human baseline is by investigating how much\nmodel explanations align with human explanations. In an investigation of model alignment\nof transformer models for Natural Language Inference (NLI) it was found that BERT-based\ntransformer models score the highest on alignment [Prasad et al., 2020]. It should also be\nnoted that the number of parameters in the model lead to worse model alignment and that\nalignment was not predicted by accuracy on NLI tasks.\nSometimes explanations like LIME and SHAP can hurt user performance, albeit not\nby very much. In a recent study by [Jesus et al., 2021] the accuracy and decision time\nwas measured when participants need to make decisions. The accuracy of the participants\nwas higher when only the basic data was given compared to when both the data and the\nexplanation was given. However, the accuracy gap was not signiﬁcant. The decision time\nsigniﬁcantly decreases when explanations are given.\n40\nExplainable Deep Learning:A Field Guide for the Uninitiated\nAn evaluation of which factors in explanations make them human interpretable came to\nthe conclusion that explanations might have more in common with design principles [Lage\net al., 2019]. One factor that drove the results was the complexity of the explanations. The\npaper further identiﬁes that regularizers can be used to optimize for the interpretability of\nML systems.\n4. Topics Associated with Explainability\nWe next review research topics closely aligned with explainable deep learning. A survey,\nvisualized in Figure 13, identiﬁes four broad related classes of research. Work on learning\nmechanism (Section 4.1) investigates the backpropagation process to establish a theory\naround weight training. These studies, in some respects, try to establish a theory to ex-\nplain how and why DNNs converge to some decision-making process. Research on model\ndebugging (Section 4.2) develops tools to recognize and understand the failure modes\nof a DNN. It emphasizes the discovery of problems that limit the training and inference\nprocess of a DNN (e.g., dead ReLUs, mode collapse, etc.).\nTechniques for adversarial\nattack and defense (Section 4.3) search for diﬀerences between regular and unexpected\nactivation patterns. This line of work promotes deep learning systems that are robust and\ntrustworthy; traits that also apply to explainability. Research on fairness and bias in\nDNNs (Section 4.4) is related to the ethics trait discussed above, but more narrowly con-\ncentrates on ensuring DNN decisions do not over-emphasize undesirable input data features.\nWe elaborate on the connection between these research areas and explainable DNNs next.\n4.1 Learning Mechanism\nThe investigation of the learning mechanism tries to derive principles explaining the evolu-\ntion of a model’s parameters during training. Many existing approaches can be categorized\nas being semantics-related, in that the analysis tries to associate a model’s learning pro-\ncess with concepts that have a concrete semantic meaning. They generally assign semantic\nconcepts to a DNNs’ internal ﬁlters (weights) or representations (activations), in order to\nuncover a human-interpretable explanation of the learning mechanism. Semantically inter-\npretable descriptions are rooted in the ﬁeld of neuro-symbolic computing [Garcez et al.,\n2012]. An early work is [Zhou et al., 2014] which assigns semantic concepts, such as objects,\nobject parts, etc, to the internal ﬁlters of a convolutional neural network (CNN) image\nscene classiﬁer. Those semantic concepts are generated based on the visualization of re-\nceptive ﬁelds of each internal unit in the given layers. The authors also discovered that\nobject detectors are embedded in a scene classiﬁer without explicit object-level supervision\nfor model training. [Gonzalez-Garcia et al., 2018] further explores this problem in a quan-\ntitative fashion. Two quantitative evaluations are conducted to study whether the internal\nrepresentations of CNNs really capture semantic concepts. Interestingly, the authors’ exper-\nimental results show that the association between internal ﬁlters and semantic concepts is\nmodest and weak. But this association improves for deeper layers of the network, matching\nthe conclusion of [Zhou et al., 2014]. [Kim et al., 2018a] quantiﬁes the importance of a given\nsemantic concept with respect to a classiﬁcation result via Testing with Concept Activation\nVector (TCAV), which is based on multiple linear classiﬁers built with internal activations\non prepared examples. The prepared examples contain both positive examples representing\n41\nRas, Xie, van Gerven, & Doran\nTopics\nAssociated\nwith\nExplainability\nLearning\nMechanism\n[Zhou et al., 2014]; [Zhang et al., 2016]\n[Raghu et al., 2017]; [Arpit et al., 2017]\n[Gonzalez-Garcia et al., 2018]; [Kim et al., 2018a]\nModel\nDebugging\n[Amershi et al., 2015]\n[Alain and Bengio, 2017]; [Fuchs et al., 2018]; [Kang et al., 2018]\nAdversarial\nAttack\n&\nDefense\nAdversarial\nAttack\nblack-box attack\n[Chen et al., 2017]; [Zhao et al., 2017]\n[Papernot et al., 2017]; [Brendel et al., 2018]\n[Dong et al., 2018]; [Su et al., 2019]\nwhite-box attack\n[Szegedy et al., 2013]\n[Goodfellow et al., 2014]\n[Sabour et al., 2015]\n[Nguyen et al., 2015]\n[Kurakin et al., 2016]\n[Rozsa et al., 2016]\n[Papernot et al., 2016b]\n[Tabacof and Valle, 2016]\n[Papernot et al., 2016a]\n[Moosavi-Dezfooli et al., 2016]\n[Carlini and Wagner, 2017]\n[Moosavi-Dezfooli et al., 2017]\n[Carlini et al., 2017]\nAdversarial\nDefense\n[Papernot et al., 2016b]; [Madry et al., 2017]\n[Meng and Chen, 2017]; [Xie et al., 2017a]\n[Samangouei et al., 2018]; [Li et al., 2018b]\n[Liu et al., 2018a]\nFairness\n&\nBias\nFairness\nDeﬁnition\nGroup Fairness: [Calders et al., 2009]\nIndividual Fairness: [Dwork et al., 2012]\nEqualized Odds and Equal Opportunity: [Hardt et al., 2016]\nDisparate Mistreatment: [Zafar et al., 2017a]\nFairness\nSolution\nPre-processing Methods\n[Kamiran and Calders, 2010, Kamiran and Calders, 2012]\n[Zemel et al., 2013]; [Louizos et al., 2015]\n[Adebayo and Kagal, 2016]; [Calmon et al., 2017]\n[Gordaliza et al., 2019]\nIn-processing Methods\n[Calders et al., 2009]; [Kamishima et al., 2011]\n[Zafar et al., 2017a]; [Woodworth et al., 2017]\n[Zafar et al., 2017b]; [Bechavod and Ligett, 2017]\n[Pérez-Suay et al., 2017]; [Berk et al., 2017]\n[Kearns et al., 2018]; [Olfat and Aswani, 2018]\n[Agarwal et al., 2018]; [Menon and Williamson, 2018]\n[Donini et al., 2018]; [Dwork et al., 2018]\nPost-processing Methods\n[Feldman et al., 2015]; [Hardt et al., 2016]\n[Pleiss et al., 2017]; [Beutel et al., 2017]\nFigure 13: Topics associated with explainability.\na semantic concept and randomly sampled negative examples that do not represent the con-\ncept. Directional derivatives are used to calculate TCAV, which measures the proportion of\nexamples that belong to a given class that are positively inﬂuenced by a given concept.\nOther methods to interpret the learning process of a DNN searches for statistical pat-\nterns indicative of convergence to a learned state. Those learning patterns include but are\n42\nExplainable Deep Learning:A Field Guide for the Uninitiated\nnot limited to: i) how layers evolve along with the training process [Raghu et al., 2017]; ii)\nthe convergence of diﬀerent layers [Raghu et al., 2017]; and iii) the generalization and memo-\nrization properties of DNNs [Zhang et al., 2016, Arpit et al., 2017]. In studying the learning\ndynamics during training, [Raghu et al., 2017] makes a comparison between two diﬀerent\nlayers or networks via Singular Vector Canonical Correlation Analysis (SVCCA). For a neu-\nron in a selected layer of a DNN, the neuron’s vector representation is generated in a “global\nfashion”, i.e. all examples from a given ﬁnite dataset are used, and each element in the neu-\nron’s vector representation is an activation for an example. The vector representations for all\nneurons in a selected layer form a vector set, representing this layer. To compare two layers,\nSVCCA takes the vector set of each layer as input and calculates a canonical correlation sim-\nilarity to make the alignment. The nature of SVCCA makes it a useful tool to monitor how\nlayer activations evolve along with the training process. The authors further discover that\nearlier layers converge faster than later layers. Thus, the weights for earlier layers can be\nfrozen earlier to reduce computational cost during training. Layer-wise convergence is also\nstudied in work such as [Zhang et al., 2016] using systematic experimentation. Keeping the\nmodel structure and hyper-parameters ﬁxed, the authors’ experiments are conducted only\nwith diﬀerent input modiﬁcation settings, either on input labels or image pixels. The exper-\nimental results indicate that DNNs can perfectly ﬁt training data with both random feature\nvalues and labels, while the degree of generalization on testing data reduces as randomness\nincreases. The authors also hypothesize that explicit regularization (such as dropout, weight\ndecay, data augmentation, etc.) may improve generalization and stochastic gradient descent\ncould act as an implicit regularizer for linear models. In a similar study, [Arpit et al., 2017]\nexamines memorization by DNNs via quantitative experiments with real and random data.\nThe study ﬁnds that DNNs do not simply memorize all real data; instead, patterns that are\ncommonly shared among the data are leveraged for memorization. Interestingly, the authors\nclaim that explicit regularization does make a diﬀerence in the speed of memorization for\nrandom data, which is diﬀerent from the conclusions in [Zhang et al., 2016]. Besides the\naforementioned research work, we would like to refer readers to a recent review paper [Bahri\net al., 2020], which covers the intersection between statistical mechanics and deep learning,\nand derives the success of deep learning from a theoretical perspective.\n4.2 Model Debugging\nSimilar to the concept of software debugging, the concept of model debugging applies tech-\nniques from traditional programming to ﬁnd out when and where model-architecture, data\nprocessing, and training related errors occur. A “probe” is leveraged to analyze the internal\npattern of a DNN, to provide further hints towards performance improvement. A probe is\nusually an auxiliary model or a structure such as a linear classiﬁer, a parallel branch of the\nmodel pipeline, etc. The training process of the probe is usually independent of the training\nprocess of the master model (a DNN) that the probe serves for. Regardless of the form of\nthe probe, the ultimate goal is model improvement.\n[Kang et al., 2018] uses model assertions, or Boolean functions, to verify the state of\nthe model during training and run time. The assertions can be used to ensure the model\noutput is consistent with meta observations about the input. For example, if a model is\ndetecting cars in a video, the cars should not disappear and reappear in successive frames\n43\nRas, Xie, van Gerven, & Doran\nof the video. Model debugging is thus implemented as a veriﬁcation system surrounding\nthe model and is implicitly model-agnostic. The model assertions are implemented as user-\ndeﬁned functions that operate on a recent history of the model input and output. The\nauthors explore several ways that model assertions can be used during both run-time and\ntraining time, in correcting wrong outputs and in collecting more samples to perform active\nlearning. [Amershi et al., 2015] proposes ModelTracker, a debugging framework revolving\naround an interactive visual interface. This visual interface summarizes traditional summary\nstatistics, such as AUC and confusion matrices, and presents this summary to the user\ntogether with a visualization of how close data samples are to each other in the feature\nspace. The interface also has an option to directly inspect prediction outliers in the form of\nthe raw data with its respective label, giving users the ability to directly correct mislabeled\nsamples.\nThe goal of this framework is to provide a uniﬁed, model-agnostic, inspection\ntool that supports debugging of three speciﬁc types of errors: mislabeled data, inadequate\nfeatures to distinguish between concepts and insuﬃcient data for generalizing from existing\nexamples. [Alain and Bengio, 2017] uses linear classiﬁers to understand the predictive power\nof representations learned by intermediate layers of a DNN. The features extracted by an\nintermediate layer of a deep classiﬁer are fed as input to the linear classiﬁer. The linear\nclassiﬁer has to predict which class the given input belongs to. The experimental results\nshow that the performance of the linear classiﬁer improves when making predictions using\nfeatures from deeper layers, i.e., layers close to the ﬁnal layer. This suggests that task-\nspeciﬁc representations are encoded in the deeper layers. [Fuchs et al., 2018] proposes the\nidea of neural stethoscopes, which is a general-purpose framework used to analyze the DNN\nlearning process by quantifying the importance of speciﬁc inﬂuential factors in the DNN\nand inﬂuence the DNN learning process by actively promoting and suppressing information.\nNeural stethoscopes extend a DNN’s architecture with a parallel branch containing a two-\nlayer perceptron. It is important to note that the main network branch does not need to\nbe changed to be able to use the neural stethoscope. This parallel branch takes the feature\nrepresentation from an arbitrary layer from the main network as input and is trained on a\nsupplemental task given known complementary information about the dataset. Speciﬁcally,\nin this study the experiments are conducted on the ShapeStacks dataset [Groth et al., 2018],\nwhich introduces a vision-based stability prediction task for block towers.\nThe dataset\nprovides information on both the local and global stability of a stack of blocks. In this\nspeciﬁc study the stethoscope was used to investigate the internal representations contained\nin the network layers that lead to the prediction of the global stability of a stack of blocks,\nwith local stability as complementary information. The stethoscope can be tuned to three\ndiﬀerent modes of operation: analytic, auxiliary, and adversarial. Each mode determines\nhow the stethoscope loss LS is propagated, e.g., in the analytical mode, LS is not propagated\nthrough the main network. The auxiliary and adversarial modes are used to promote and\nsuppress information respectively.\nThe paper shows that the method was successful in\nimproving network performance and mitigating biases that are present in the dataset.\n4.3 Adversarial Attack and Defense\nAn adversarial example is an artiﬁcial input engineered to intentionally disturb the judgment\nof a DNN [Goodfellow et al., 2014]. Developing defenses to adversarial examples requires\n44\nExplainable Deep Learning:A Field Guide for the Uninitiated\na basic understanding of the space that inputs are taken from and the shape and form of\nboundaries between classes. Interpretations of this space inform the construction of defenses\nto better discriminate between classes and forms the basis of explaining input/output be-\nhavior. Moreover, an “explanation” from a model that is not reasonable given its input and\noutput may be indicative of an adversarial example.\nThe study of adversarial examples [Yuan et al., 2019, Zhang et al., 2019b] are from\nthe perspective of attack and defense. Adversarial attack methods are about generating\nadversarial examples that can fool a DNN. From the model access perspective, there are two\nmain types of adversarial attack: black-box\n[Chen et al., 2017, Zhao et al., 2017, Papernot\net al., 2017, Brendel et al., 2018, Dong et al., 2018, Su et al., 2019] and white-box [Szegedy\net al., 2013, Goodfellow et al., 2014, Sabour et al., 2015, Nguyen et al., 2015, Kurakin et al.,\n2016, Rozsa et al., 2016, Papernot et al., 2016a, Moosavi-Dezfooli et al., 2016, Tabacof\nand Valle, 2016, Kurakin et al., 2016, Carlini and Wagner, 2017, Moosavi-Dezfooli et al.,\n2017, Carlini et al., 2017, Eykholt et al., 2018] attacks. In the black-box setting the attacker\nhas no access to the model parameters or intermediate gradients whereas these are available\nfor the white-box settings. Adversarial defense [Madry et al., 2017, Papernot et al., 2016b,\nMeng and Chen, 2017, Xie et al., 2017a, Samangouei et al., 2018, Li et al., 2018b, Liu\net al., 2018a], on the other hand, seeks solutions to make a DNN robust against generated\nadversarial examples.\nRecent work on adversarial attack reveals vulnerabilities by perturbing input data with\nimperceptible noise [Goodfellow et al., 2014, Carlini and Wagner, 2017, Madry et al., 2017] or\nby adding “physical perturbations” to objects under analysis (i.e. black and white stickers\non objects captured by computer vision systems) [Eykholt et al., 2018]. Among numer-\nous adversarial attack methods, the C&W attack [Carlini and Wagner, 2017] and PGD\nattack [Madry et al., 2017] are frequently used to evaluate the robustness of DNNs.\nC&W attack [Carlini and Wagner, 2017] casts the adversarial attack task as an optimiza-\ntion problem and is originally proposed to challenge an adversarial defense method called\ndefensive distillation [Papernot et al., 2016b]. Variants of C&W attacks are based on the dis-\ntance metrics (ℓ0, ℓ2, or ℓ∞). [Carlini and Wagner, 2017], for example, can successfully defeat\ndefensive distillation with high-conﬁdence adversarial examples generated via C&W attack.\nProjected Gradient Descent (PGD) attack [Madry et al., 2017] in brief is an iterative version\nof an early stage adversarial attack called Fast Gradient Sign Method (FGSM) [Goodfellow\net al., 2014]. As indicated in its name, PGD attack generates adversarial examples based\non the gradients of the loss with respect to the input. PGD attack is more favorable than\nC&W attack when direct control of input distortion is needed [Liu et al., 2018a].\nAdversarial defense is challenging due to the diversity of the adversarial example crafting\nprocesses and a DNN’s high-dimensional feature space. There exist two typical groups of\nadversarial defense methods, i) adversarial training [Madry et al., 2017, Goodfellow et al.,\n2014, Szegedy et al., 2013], which is to augment the training dataset with generated adver-\nsarial examples such that the trained model is more robust against adversarial attack, and\nii) removal perturbations [Samangouei et al., 2018, Meng and Chen, 2017], which dismisses\nadversarial perturbations from input data. [Madry et al., 2017] integrates the PGD attack\ninto the model training process, such that the model is optimized on both benign examples\nand challenging adversarial examples. The optimization is conducted in a min-max fashion,\nwhere the loss for adversarial attack process is maximized in order to generate strong adver-\n45\nRas, Xie, van Gerven, & Doran\nsarial examples, while the loss for the classiﬁcation process is minimized, in order to get a\nrobust and well-performed model. [Samangouei et al., 2018], on the other hand, tackles the\nadversarial defense problem by ﬁltering out adversarial perturbations. Generative Adversar-\nial Networks (GANs) are leveraged to project a given input image, potentially polluted by\nadversarial perturbations, into a pseudo original image, where adversarial artifacts are di-\nminished. Model decisions are made from the GAN generated “original” image. Experiments\nindicate this defense technique is eﬀective against both black-box and white-box attacks.\n4.4 Fairness and Bias\nModel fairness aims to build DNN models that objectively consider each input feature and is\nnot unduly biased against a particular subset of the input data. Although a ﬁrm deﬁnition\nof what it means for a DNN to be “fair” is evolving, common themes are emerging in the\nliterature [Heidari et al., 2018]. Group fairness [Calders et al., 2009], also called demographic\nparity or statistical parity, focuses on fairness with respect to a group (based on race, gender,\netc.). The goal of group fairness is to ensure each group receives equalized percentage of\nbeneﬁt. Consider a loan application as an example. Suppose we are monitoring the loan\napproval situation of two cities, city A and city B. The population of city A is twice as much\nas that of city B. Based on the deﬁnition of group fairness, twice as many loan applications\nshould be approved in A compared to city B. Individual fairness [Dwork et al., 2012] aims\nto treat similar inputs similarly based on a metric to measure the closeness of their features.\nTo compare group fairness and individual fairness, let’s return to the loan request example.\nUnder the restriction of group fairness, an individual from city A may not be approved for a\nloan request just because of the group percentage limitation, even though this individual is\nmore qualiﬁed based on economic metrics than other approved ones from city B. However,\nindividual fairness requires that individuals with similar characteristics should have the same\nchance to be approved for a loan request, regardless of which city individuals come from.\nThis is in antithesis with group fairness. Further notions of fairness, such as equalized odds\nand equal opportunity [Hardt et al., 2016], disparate mistreatment [Zafar et al., 2017a], and\nothers [Heidari et al., 2018, Woodworth et al., 2017] are also studied in the literature.\nThe fairness problem is currently addressed by three types of methods [Calmon et al.,\n2017]: (i) pre-processing methods revise input data to remove information correlated to sen-\nsitive attributes; (ii) in-process methods add fairness constraints into the model learning\nprocess; and (iii) post-process methods adjust model predictions after the model is trained.\nPre-processing methods [Kamiran and Calders, 2010, Kamiran and Calders, 2012, Zemel\net al., 2013, Louizos et al., 2015, Adebayo and Kagal, 2016, Calmon et al., 2017, Gordaliza\net al., 2019] learn an alternative representation of the input data that removes information\ncorrelated to the sensitive attributes (such as race or gender) while maintaining the model\nperformance as much as possible. For example, [Calmon et al., 2017] proposes a proba-\nbilistic framework to transform input data to prevent unfairness in the scope of supervised\nlearning. The input transformation is conducted as an optimization problem, aiming to\nbalance discrimination control (group fairness), individual distortion (individual fairness),\nand data utility. In-process methods [Calders et al., 2009, Kamishima et al., 2011, Zafar\net al., 2017a, Woodworth et al., 2017, Zafar et al., 2017b, Bechavod and Ligett, 2017, Kearns\net al., 2018, Pérez-Suay et al., 2017, Berk et al., 2017, Olfat and Aswani, 2018, Agarwal et al.,\n46\nExplainable Deep Learning:A Field Guide for the Uninitiated\n2018, Menon and Williamson, 2018, Donini et al., 2018, Dwork et al., 2018] directly intro-\nduce fairness learning constraints to the model in order to punish unfair decisions during\ntraining. [Kamishima et al., 2011] achieves the fairness goal by adding a fairness regularizer,\nfor example, such that the inﬂuence of sensitive information on model decisions is reduced.\nPost-process methods [Feldman et al., 2015, Hardt et al., 2016, Pleiss et al., 2017, Beutel\net al., 2017] are characterized by adding ad-hoc fairness procedures to a trained model.\nOne example is [Hardt et al., 2016] which constructs non-discriminating predictors as a\npost-processing step to achieve equalized odds and equal opportunity (two fairness notions\nproposed in their study). They introduce the procedure to construct non-discriminating pre-\ndictors for two scenarios of the original model, binary predictor and score function, where\nin the latter scenario the original model generates real score values in range [0, 1]. A non-\ndiscriminating predictor is constructed for each protected group, with a deﬁned threshold\nto achieve a fairness goal.\n5. Designing Explanations for Users\nThe foundations of explaining DNNs discussed in this survey are seldom enough to achieve\nexplanations useful to users in practice. ML engineers designing explainable DNNs in prac-\ntice will thus often integrate an explanatory method into their DNN and then reﬁne the\npresentation of the explanation to a form useful for the end-user. A useful explanation must\nconform to some deﬁnition of what constitutes a satisfactory explanation of the network’s\ninner workings depending on the user, the conditions of use, and the task at hand. These\ndeﬁnitions are often qualitative (e.g., one user is better swayed by visual over textual expla-\nnations for a task). User requirements for an explanation may further vary by preferences\nbetween explanations that are of high ﬁdelity versus those that are parsimonious. The qual-\nity of an explanation depends on user- and context-speciﬁc utility and makes the evaluation\nof explanations a diﬃcult problem.\nThis suggests that explanations, grounded in the methods discussed in this ﬁeld guide,\nneed to be designed by engineers on a case-by-case basis for the user and task at hand.\nThis section describes the following important design questions when engineers apply the\nmethods in this ﬁeld guide in practice:\n1. Who is the end user? The kind of end-user, and in particular their expertise in\ndeep learning and their domain-speciﬁc requirements, deﬁne the appropriate trade-oﬀ\nbetween ﬁdelity and parsimony in an explanation’s presentation.\n2. How practically impactful are the decisions of the DNN? Here impact corre-\nsponds to the consequence of right and wrong decisions on people and society. Time-\ncritical scenarios require explanations that can be rapidly generated and processed by\na user should there be a need to intervene (e.g., in self-driving cars). Decision-critical\nscenarios require explanations that are trustworthy, that is, an explanation that a user\ntrusts to be faithful to the actual decision-making process of the DNN.\n3. How extendable is an explanation? It is expensive to design a form of explanation\nfor only a single type of user who faces a single type of problem. A good design should\nthus be grounded on a single user’s preferences that can be applied to multiple types\n47\nRas, Xie, van Gerven, & Doran\nof problems, or be ﬂexible enough to appeal to multiple user types examining the same\nproblem type. It may not be feasible to devise the presentation of an explanation that\nappeals to a broad set of users tailed to a diverse set of problems.\n5.1 Understanding the End User\nOne of the primary tasks to design an explanation is to determine the type of end-user using\nthe system. The literature has documented cases of designs that provide both low-level tech-\nnical speciﬁc explanations targeting on deep learning experts [Zeiler and Fergus, 2014, Sun-\ndararajan et al., 2017, Anderson et al., 2018, Li et al., 2016, Fong and Vedaldi, 2017, Zintgraf\net al., 2017], and high-level reasoning extracted explanations catering normal users [Har-\nradon et al., 2018, Zhang et al., 2019a, Zhang et al., 2017, Zhang et al., 2018]. DNN experts\ncare mostly about technical details and potential hints for model revising and performance\nimprovement. Ideal explanations for them could be in form of input feature inﬂuence analyt-\nics [Adler et al., 2018, Koh and Liang, 2017, Li et al., 2016, Fong and Vedaldi, 2017], hidden\nstates interaction and visualizations [Anderson et al., 2018, Vaswani et al., 2017, Vinyals\net al., 2015, Zeiler and Fergus, 2014, Selvaraju et al., 2017, Bach et al., 2015, Sundarara-\njan et al., 2017], etc. DNN experts, for instance, could check if the model is emphasizing\nreasonable image areas [Zeiler and Fergus, 2014] or text elements/words [Vaswani et al.,\n2017, Sundararajan et al., 2017] towards generating corresponding model decisions, and\npropose model revision strategies accordingly. Normal users, on the other hand, mainly\nfocus on the high-level functionality of the model, instead of technical details. Their main\nconcern is if the model is working reasonably and not violating human logic. The explanation\ncan be represented in the form of extracted reasoning logic [Harradon et al., 2018, Zhang\net al., 2019a, Zhang et al., 2017, Zhang et al., 2018] or some easy to understandable input\nclues with respect to given prediction [Ribeiro et al., 2016c]. If the decision is generated\non unexpected input elements or not following a logical reasoning process, a doubt could\nbe raised by the user to deny such a model decision. Considering the diﬀerent user exper-\ntise level on DNN knowledge, the designing of a general model explanation system, which\nsatisﬁes both DNN experts and normal users, is challenging and remains to be explored.\nThe domain a user operates in is another important consideration. For example, the\nexplanation needs of a medical doctor require that the explanation representation be de-\ntailed enough such that the doctor can understand the reasoning process behind the speciﬁc\ndiagnosis and be conﬁdent about said diagnosis [Lipton, 2017], e.g., the patient needs this\nspeciﬁc treatment because it identiﬁes features of cancer at a particular stage.\nBut no\nexplainable method is able to automatically tailor its explanations to end-users for a spe-\nciﬁc domain. One possible way to obtain such explanations using the present art is if the\nfeatures of the input data that are expressed by an explanation method have an intuitive\ndomain-speciﬁc interpretation, which is built upon a systematic knowledge base constructed\nby domain experts.\n5.2 The Impact of DNN Decisions\nThe need for and characteristics of an explanation depends on the impact of a DNN’s\noperation on human life and society. This impact can be realized based on the impact and\nspeed of a decision. In time-critical scenarios [Grigorescu et al., 2020] where users must\n48\nExplainable Deep Learning:A Field Guide for the Uninitiated\nprocess and react to DNN decisions in limited time, explanations must be produced that\nare simple to interpret and understand and are not computationally intense to perform.\nThis is a particularly important aspect of explanations that is seldom investigated in the\nliterature. For example, a DNN providing recommendations during a military operation,\nor sensing upcoming hazards on a vehicle, needs to support their output with explanations\nwhile giving the user enough time to process and react accordingly. In a decision-critical\nscenario [Grigorescu et al., 2020, Nemati et al., 2018, Ahmad et al., 2018], the ability to\nnot only interpret but deeply inspect a decision grows in importance. Any user decision\nbased on a DNN’s recommendation should be supported with evidence and explanations\nothers can understand. At the same time, should a DNN’s recommendation turn out to be\nincorrect or lead to an undesirable outcome for the user, the model should be inspected post-\nhoc to hypothesize root causes and identify “bugs” in the DNN’s actions. Deep, technical\ninspections of the neural network guided by comprehensive interpretations of its inference\nand training actions are necessary for such post-hoc analysis.\nFew current model explanations are designed with time- and decision-critical scenarios in\nmind. The computational cost of many model explanations tends to be high and may require\nextra human labor, which is undesirable if an automatic instant explanation is needed. For\ninstance, for explanations presented in form of model visualization [Zeiler and Fergus, 2014,\nSelvaraju et al., 2017, Shrikumar et al., 2017, Sundararajan et al., 2017, Montavon et al.,\n2017, Zhou et al., 2016], extra human eﬀort is needed for veriﬁcation, which is potentially\ncostly. Besides, some explanation methods with post-hoc training involved [Ribeiro et al.,\n2016c, Frosst and Hinton, 2017, Krakovna and Doshi-Velez, 2016, Hou and Zhou, 2020] may\nbe limited in its utility on providing explanations for real-time input. The study for decision-\ncritical scenarios is still under development. In order to increase the ﬁdelity and reliability of\nmodel decisions, a variety of topics are explored besides model explanations, including model\nrobustness [Papernot et al., 2016b, Meng and Chen, 2017, Xie et al., 2017a, Samangouei\net al., 2018, Li et al., 2018b], fairness and bias [Heidari et al., 2018, Calders et al., 2009, Hardt\net al., 2016, Zafar et al., 2017a, Calmon et al., 2017, Gordaliza et al., 2019, Agarwal et al.,\n2018, Menon and Williamson, 2018, Donini et al., 2018, Dwork et al., 2018, Pleiss et al.,\n2017, Beutel et al., 2017], model trustworthiness [Jiang et al., 2018, Heo et al., 2018], etc.\nThe study of the aforementioned topics, together with model explanations, may jointly shed\nlight on potential new solutions for applications on decision-critical scenarios.\n5.3 Design Extendability\nModularity and reusability are important extendability traits in the architecture of large-\nscale software systems: modularity promotes the ability of an engineer to replace and alter\nsystem components as necessary, while reusability promotes the use of already proven soft-\nware modules. In a similar vein, highly reliable and performant DNN systems should also be\nconstructed with reusable and highly modular components. Modularity is a trait of the func-\ntional units of a DNN that may be adaptable for multiple architectures. For example, the\nform of an attention mechanism suitable for any kind of sequential data processing [Vaswani\net al., 2017, Devlin et al., 2019]. A highly modular DNN architecture may be one that con-\ntains many “plug and play” components in each layer so that its complete design can be seen\nas a composition of interconnected functional units. Reusability speaks to complete DNN\n49\nRas, Xie, van Gerven, & Doran\nsystems, perhaps already trained, that can be reused in multiple problem domains. One\nexample of reusability is the common application of a pre-trained YOLO [Redmon et al.,\n2016] model for object localization in frames in a deep learning video processing pipeline.\nDNN explanation methods that also exhibit these extendability traits are more likely\nto be broadly useful over a variety of DNN models and application domains. Modularized\nexplainable models will crucially reduce the overhead in implementing and deploying ex-\nplainability in new domains and may lead to explanatory forms a user is familiar with over\nmultiple types of models. Reusability plays a role in risk control, such that the ﬁdelity of\nan explanation remains consistent however the explanatory method is applied.\nNeither modularity nor reusability is the focus of explainable methods in the literature.\nHowever, existing methods could be divided by how modular they potentially are. Model-\nagnostic methods [Ribeiro et al., 2016c, Ribeiro et al., 2016b, Ribeiro et al., 2016a, Fong\nand Vedaldi, 2017, Jha et al., 2017], which do not take the type of model into account,\nare modular by deﬁnition in the sense that the explanatory module is independent of the\nmodel it is producing explanations for. On the other hand, The second category contains\nexplanation methods that are very speciﬁc to the model [Shrikumar et al., 2017, Zeiler\nand Fergus, 2014, Bach et al., 2015, Montavon et al., 2017, Zhou et al., 2016, Murdoch\net al., 2018, Xie et al., 2017b, Park et al., 2018, Hendricks et al., 2016]. This aspect is\nimportant for expert users that are developing deep learning models and need to understand\nspeciﬁcally which aspect of the deep learning model is inﬂuencing the predictions, e.g. in\nmodel debugging. However, these methods by their very nature lack modularity.\n6. Future Directions\nThe ﬁeld guide concludes by introducing research directions whose developments can con-\ntribute to improving explainable deep learning.\nA Unifying Approach to Explainability. There have been several eﬀorts to come\nup with a framework for explainable artiﬁcial intelligence (XAI) or interpretable machine\nlearning [Gilpin et al., 2018, Doshi-Velez and Kim, 2017, Doshi-Velez and Kim, 2018]. Each\none of these papers considers the literature from a diﬀerent perspective, with little consider-\nation to unify various methods. Currently, there is still a lack of systematic general theory\nin the realm of DNN explanation [Arrieta et al., 2020, Díez et al., 2013]. A systematic the-\nory should be able to beneﬁt the overall model explanation studies, and once formed, some\ncurrent challenging explainable problems may be able to be handled properly by nature, and\nsome novel directions may be proposed based on the systematic theories. However, one of\nthe main reasons why it is so diﬃcult to establish a formal theory of explanation is that the\nbasic concepts of explanations in AI are diﬃcult or impossible to formalize [Wolf et al., 2019].\nUser-friendly Explanations. User-friendly explanations are needed to minimize the tech-\nnical understanding of a user to correctly interpret explanations. As the concern of the\nopaque nature of DNNs is raising increasing attention in the society and even required by\nlaw, model explanations would inevitably be mandatory in a wide range of real-life appli-\ncations [Goodman and Flaxman, 2017]. Given the varied backgrounds of model users, the\nfriendliness would be a future trend towards constructing explanations of high qualities.\n50\nExplainable Deep Learning:A Field Guide for the Uninitiated\nMost explainable methods are still catering towards expert users instead of laymen [Ras\net al., 2018], in the sense that knowledge about the method, for instance the DNN process,\nis needed to understand the explanation. The requirement on model knowledge limits the\nwide usage of such explanation models, since in real scenarios the chance of the end-users\nbeing machine learning experts is very low. Assuming the end-user has been correctly deter-\nmined, the next step is to determine what needs to be explained, i.e., which step or decision\ndoes the system need to explain?\nProducing Explanations Eﬃciently.\nTime and decision-critical explanations [Grig-\norescu et al., 2020, Ahmad et al., 2018, Nemati et al., 2018], as discussed in Section 5.2,\nmust be produced with enough time for a user to react to a DNN’s decision. An eﬃcient\nmanner to produce explanations further saves computational power, which is favorable in\nindustrial applications or when explanations are required in environments with low comput-\ning resources.\nDeveloping Methods for Trustworthiness. The vulnerability of a DNN to adversarial\nexamples [Yuan et al., 2019, Goodfellow et al., 2014, Carlini and Wagner, 2017, Madry et al.,\n2017] and poisoned training sets [Saha et al., 2019] raises much concern on trustworthiness.\nAs more and more DNNs are leveraged in real-life applications, the demand for model\ntrustworthiness would undoubtedly increase, especially for decision-critical scenarios where\nundesired decisions may cost severe consequences. This thread of research is only beginning\nto be developed [Jiang et al., 2018, Heo et al., 2018].\n7. Conclusions\nThe rapid advancements in deep neural networks have stimulated innovations in a wide range\nof applications such as facial recognition [Masi et al., 2018], voice assistance [Tulshan and\nDhage, 2018], driving system [Jain et al., 2015], etc. The ﬁeld of deep learning explainability\nis motivated by the opaque nature of DNN systems and the increasing demand on model\ntransparency and trustworthiness in the society. Government policies, i.e., the EU’s General\nData Protection Regulation (GDPR) [Goodman and Flaxman, 2017], allude to a future\nwhere the explainability aspects of deep networks will become a legal concern.\nWe hope this ﬁeld guide has distilled the essential topics, related work, methods, and\nconcerns associated with explainable deep learning for an initiate. A wide range of existing\nmethods on deep learning explainability is introduced and organized by a novel categoriza-\ntion scheme, depicting the ﬁeld clearly and straightforwardly. Topics closely associated with\nDNN explainability, including model learning mechanism, model debugging, adversarial at-\ntack and defense, and model fairness and bias, are reviewed as related work. A discussion\non user-oriented explanation designing and future trends of this ﬁeld is provided at the end\nof this survey, shedding light on potential directions on model explainability. Given the\ncountless papers in this ﬁeld and the rapid development of explainable methods, we admit\nthat we are unable to cover every paper or every aspect that belongs to this realm. We\ncarefully designed hierarchical categories for the papers covered such that the skeleton of\nthis ﬁeld is visualized.\n51\nRas, Xie, van Gerven, & Doran\nIn the end, the important thing is to explain the right thing to the right person in the\nright way at the right time.1 We are excited to continue to observe how the ﬁeld evolves to\ndeliver the appropriate explanation to the right audience who need it the most. We hope the\nnumerous solutions actively being explored will lead to the fairer, safer, and more conﬁdent\nuse of deep learning across society.\nAcknowledgments\nWe acknowledge project support from the Ohio Federal Research Network, the Multidis-\nciplinary Research Program of the Department of Defense (MURI N00014-00-1-0637), and\nthe organizers and participants of the Schloss Dagstuhl −Leibniz Center for Informatics\nSeminar 17192 on Human-Like Neural-Symbolic Computing for providing the environment\nto develop the ideas in this paper. Parts of this work was completed under a Fulbright-NSF\nFellowship for Cyber Security and Critical Infrastructure. We would also like to thank Erdi\nÇallı and Pim Haselager for the helpful discussions and general support.\nReferences\n[Achanta et al., 2012] Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., and Süsstrunk,\nS. (2012).\nSLIC superpixels compared to state-of-the-art superpixel methods.\nIEEE\ntransactions on pattern analysis and machine intelligence, 34(11):2274–2282.\n[Adadi and Berrada, 2018] Adadi, A. and Berrada, M. (2018). Peeking inside the black-box:\nA survey on explainable artiﬁcial Iitelligence (XAI). IEEE Access, 6:52138–52160.\n[Adebayo et al., 2018] Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., and\nKim, B. (2018). Sanity checks for saliency maps. In Advances in Neural Information\nProcessing Systems, pages 9525–9536.\n[Adebayo and Kagal, 2016] Adebayo, J. and Kagal, L. (2016). Iterative orthogonal feature\nprojection for diagnosing bias in black-box models. ArXiv, abs/1611.04967.\n[Adler et al., 2018] Adler, P., Falk, C., Friedler, S. A., Nix, T., Rybeck, G., Scheidegger, C.,\nSmith, B., and Venkatasubramanian, S. (2018). Auditing black-box models for indirect\ninﬂuence. Knowledge and Information Systems, 54(1):95–122.\n[Agarwal et al., 2018] Agarwal, A., Beygelzimer, A., Dudik, M., Langford, J., and Wallach,\nH. (2018). A reductions approach to fair classiﬁcation. In International Conference on\nMachine Learning, pages 60–69.\n[Ahmad et al., 2018] Ahmad, M. A., Eckert, C., and Teredesai, A. (2018). Interpretable\nmachine learning in healthcare. In Proceedings of the 2018 ACM International Conference\non Bioinformatics, Computational Biology, and Health Informatics, pages 559–560.\n[Alain and Bengio, 2017] Alain, G. and Bengio, Y. (2017).\nUnderstanding intermediate\nlayers using linear classiﬁer probes. ArXiv, abs/1610.01644.\n1. Paraphrased from Dr. Silja Renooij at BENELEARN2019\n52\nExplainable Deep Learning:A Field Guide for the Uninitiated\n[Alvarez-Melis and Jaakkola, 2018] Alvarez-Melis, D. and Jaakkola, T. S. (2018). On the\nrobustness of interpretability methods. ArXiv, abs/1806.08049.\n[Amershi et al., 2015] Amershi, S., Chickering, M., Drucker, S. M., Lee, B., Simard, P.,\nand Suh, J. (2015). Modeltracker: Redesigning performance analysis tools for machine\nlearning.\nIn Proceedings of the 33rd Annual ACM Conference on Human Factors in\nComputing Systems, pages 337–346. ACM.\n[Anderson et al., 2018] Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould,\nS., and Zhang, L. (2018). Bottom-up and top-down attention for image captioning and\nvisual question answering. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 6077–6086.\n[Antol et al., 2015] Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick,\nC., and Parikh, D. (2015). VQA: Visual question answering. In Proceedings of the IEEE\ninternational conference on computer vision, pages 2425–2433.\n[Arpit et al., 2017] Arpit, D., Jastrzębski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,\nM. S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et al. (2017). A closer look at\nmemorization in deep networks. In International Conference on Machine Learning, pages\n233–242.\n[Arras et al., 2016] Arras, L., Horn, F., Montavon, G., Müller, K.-R., and Samek, W. (2016).\nExplaining predictions of non-Linear classiﬁers in NLP. In Proceedings of the 1st Workshop\non Representation Learning for NLP, pages 1–7.\n[Arras et al., 2017] Arras, L., Horn, F., Montavon, G., Müller, K.-R., and Samek, W. (2017).\n\"What is relevant in a text document?\": An interpretable machine learning approach.\nPloS one, 12(8):e0181142.\n[Arrieta et al., 2020] Arrieta, A. B., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik,\nS., Barbado, A., García, S., Gil-López, S., Molina, D., Benjamins, R., et al. (2020). Ex-\nplainable artiﬁcial intelligence (XAI): Concepts, taxonomies, opportunities and challenges\ntoward responsible AI. Information Fusion, 58:82–115.\n[Baan et al., 2019] Baan, J., ter Hoeve, M., van der Wees, M., Schuth, A., and de Rijke, M.\n(2019). Do transformer attention heads provide transparency in abstractive summariza-\ntion? ArXiv, abs/1907.00570.\n[Bach et al., 2015] Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller, K.-R., and\nSamek, W. (2015). On pixel-wise explanations for non-linear classiﬁer decisions by layer-\nwise relevance propagation. PloS one, 10(7):e0130140.\n[Baehrens et al., 2010] Baehrens, D., Schroeter, T., Harmeling, S., Kawanabe, M., Hansen,\nK., and MÃžller, K.-R. (2010). How to explain individual classiﬁcation decisions. Journal\nof Machine Learning Research, 11(Jun):1803–1831.\n[Bahdanau et al., 2015] Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine\ntranslation by jointly learning to align and translate. CoRR, abs/1409.0473.\n53\nRas, Xie, van Gerven, & Doran\n[Bahri et al., 2020] Bahri, Y., Kadmon, J., Pennington, J., Schoenholz, S. S., Sohl-\nDickstein, J., and Ganguli, S. (2020). Statistical mechanics of deep learning. Annual\nReview of Condensed Matter Physics, 11:501–528.\n[Bastani et al., 2017] Bastani, O., Kim, C., and Bastani, H. (2017). Interpreting blackbox\nmodels via model extraction. ArXiv, abs/1705.08504.\n[Bechavod and Ligett, 2017] Bechavod, Y. and Ligett, K. (2017). Penalizing unfairness in\nbinary classiﬁcation. ArXiv, abs/1707.00044.\n[Berk et al., 2017] Berk, R., Heidari, H., Jabbari, S., Joseph, M., Kearns, M., Morgen-\nstern, J., Neel, S., and Roth, A. (2017). A convex framework for fair regression. ArXiv,\nabs/1706.02409.\n[Beutel et al., 2017] Beutel, A., Chen, J., Zhao, Z., and Chi, E. H. (2017).\nData deci-\nsions and theoretical implications when adversarially learning fair representations. ArXiv,\nabs/1707.00075.\n[Bien and Tibshirani, 2011] Bien, J. and Tibshirani, R. (2011). Prototype selection for in-\nterpretable classiﬁcation. The Annals of Applied Statistics, 5:2403–2424.\n[Brendel et al., 2018] Brendel, W., Rauber, J., and Bethge, M. (2018). Decision-based ad-\nversarial attacks: Reliable attacks against black-box machine learning models. ArXiv,\nabs/1712.04248.\n[Brown et al., 2020] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal,\nP., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are\nfew-shot learners. ArXiv, abs/2005.14165.\n[Buolamwini and Gebru, 2018] Buolamwini, J. and Gebru, T. (2018). Gender shades: In-\ntersectional accuracy disparities in commercial gender classiﬁcation. In Conference on\nfairness, accountability and transparency, pages 77–91.\n[Calders et al., 2009] Calders, T., Kamiran, F., and Pechenizkiy, M. (2009). Building clas-\nsiﬁers with independency constraints. In 2009 IEEE International Conference on Data\nMining Workshops, pages 13–18. IEEE.\n[Calmon et al., 2017] Calmon, F., Wei, D., Vinzamuri, B., Ramamurthy, K. N., and Varsh-\nney, K. R. (2017). Optimized pre-processing for discrimination prevention. In Advances\nin Neural Information Processing Systems, pages 3992–4001.\n[Camburu et al., 2018] Camburu, O.-M., Rocktäschel, T., Lukasiewicz, T., and Blunsom,\nP. (2018). e-SNLI: Natural language inference with natural language explanations. In\nAdvances in Neural Information Processing Systems, pages 9560–9572.\n[Carlini et al., 2017] Carlini, N., Katz, G., Barrett, C., and Dill, D. L. (2017). Ground-truth\nadversarial examples. ArXiv, abs/1709.10207.\n[Carlini and Wagner, 2017] Carlini, N. and Wagner, D. (2017).\nTowards evaluating the\nrobustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP),\npages 39–57. IEEE.\n54\nExplainable Deep Learning:A Field Guide for the Uninitiated\n[Carter et al., 2019] Carter, S., Armstrong, Z., Schubert, L., Johnson, I., and Olah, C.\n(2019). Activation atlas. Distill, 4(3):e15.\n[Carvalho et al., 2019] Carvalho, D. V., Pereira, E. M., and Cardoso, J. S. (2019). Machine\nlearning interpretability: A survey on methods and metrics. Electronics, 8(8):832.\n[Chen et al., 2019] Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., and Su, J. K. (2019).\nThis looks like that: Deep learning for interpretable image recognition. In Advances in\nNeural Information Processing Systems, pages 8928–8939.\n[Chen et al., 2017] Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J. (2017).\nZoo: Zeroth order optimization based black-box attacks to deep neural networks with-\nout training substitute models. In Proceedings of the 10th ACM Workshop on Artiﬁcial\nIntelligence and Security, pages 15–26. ACM.\n[Chong et al., 2017] Chong, E., Han, C., and Park, F. C. (2017). Deep learning networks\nfor stock market analysis and prediction: Methodology, data representations, and case\nstudies. Expert Systems with Applications, 83:187–205.\n[Craven and Shavlik, 1996] Craven, M. and Shavlik, J. W. (1996).\nExtracting tree-\nstructured representations of trained networks. In Advances in neural information pro-\ncessing systems, pages 24–30.\n[Dabkowski and Gal, 2017] Dabkowski, P. and Gal, Y. (2017). Real time image saliency\nfor black box classiﬁers. In Advances in Neural Information Processing Systems, pages\n6967–6976.\n[Devlin et al., 2019] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT:\nPre-training of deep bidirectional transformers for language understanding. In Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\npages 4171–4186.\n[DeYoung et al., 2020] DeYoung, J., Jain, S., Rajani, N. F., Lehman, E., Xiong, C., Socher,\nR., and Wallace, B. C. (2020). Eraser: A benchmark to evaluate rationalized nlp models.\nArXiv, abs/1911.03429.\n[Díez et al., 2013] Díez, J., Khalifa, K., and Leuridan, B. (2013). General theories of expla-\nnation: Buyer beware. Synthese, 190(3):379–396.\n[Ding et al., 2017] Ding, Y., Liu, Y., Luan, H., and Sun, M. (2017). Visualizing and un-\nderstanding neural machine translation. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages\n1150–1159.\n[Dong et al., 2016] Dong, W., Li, J., Yao, R., Li, C., Yuan, T., and Wang, L. (2016). Char-\nacterizing driving styles with deep learning. ArXiv, abs/1607.03611.\n55\nRas, Xie, van Gerven, & Doran\n[Dong et al., 2018] Dong, Y., Liao, F., Pang, T., Su, H., Zhu, J., Hu, X., and Li, J. (2018).\nBoosting adversarial attacks with momentum. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 9185–9193.\n[Dong et al., 2017] Dong, Y., Su, H., Zhu, J., and Zhang, B. (2017).\nImproving inter-\npretability of deep neural networks with semantic information.\nIn Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 4306–4314.\n[Donini et al., 2018] Donini, M., Oneto, L., Ben-David, S., Shawe-Taylor, J. S., and Pontil,\nM. (2018). Empirical risk minimization under fairness constraints. In Advances in Neural\nInformation Processing Systems, pages 2791–2801.\n[Doshi-Velez and Kim, 2017] Doshi-Velez, F. and Kim, B. (2017). Towards a rigorous science\nof interpretable machine learning. ArXiv, abs/1702.08608.\n[Doshi-Velez and Kim, 2018] Doshi-Velez, F. and Kim, B. (2018). Considerations for evalua-\ntion and generalization in interpretable machine learning. In Explainable and interpretable\nmodels in computer vision and machine learning, pages 3–17. Springer.\n[Došilović et al., 2018] Došilović, F. K., Brčić, M., and Hlupić, N. (2018). Explainable arti-\nﬁcial intelligence: A survey. In 2018 41st International Convention on Information and\nCommunication Technology, Electronics and Microelectronics (MIPRO), pages 0210–0215.\nIEEE.\n[Dwork et al., 2012] Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. (2012).\nFairness through awareness. In Proceedings of the 3rd innovations in theoretical computer\nscience conference, pages 214–226. ACM.\n[Dwork et al., 2018] Dwork, C., Immorlica, N., Kalai, A. T., and Leiserson, M. (2018). De-\ncoupled classiﬁers for group-fair and eﬃcient machine learning. In Conference on Fairness,\nAccountability and Transparency, pages 119–133.\n[Elenberg et al., 2017] Elenberg, E., Dimakis, A. G., Feldman, M., and Karbasi, A. (2017).\nStreaming weak submodularity: Interpreting neural networks on the ﬂy. In Advances in\nNeural Information Processing Systems, pages 4044–4054.\n[Erhan et al., 2009] Erhan, D., Bengio, Y., Courville, A., and Vincent, P. (2009). Visual-\nizing higher-layer features of a deep network. Départment d’Informatique et Recherche\nOpérationnelle, University of Montreal, QC, Canada, Tech. Rep, 1341.\n[Erhan et al., 2010] Erhan, D., Courville, A., and Bengio, Y. (2010). Understanding repre-\nsentations learned in deep architectures. Départment d’Informatique et Recherche Opéra-\ntionnelle, University of Montreal, QC, Canada, Tech. Rep, 1355.\n[Esteva et al., 2017] Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau,\nH. M., and Thrun, S. (2017). Dermatologist-level classiﬁcation of skin cancer with deep\nneural networks. Nature, 542(7639):115–118.\n56\nExplainable Deep Learning:A Field Guide for the Uninitiated\n[Eykholt et al., 2018] Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao,\nC., Prakash, A., Kohno, T., and Song, D. (2018). Robust physical-world attacks on deep\nlearning visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 1625–1634.\n[Feldman et al., 2015] Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., and\nVenkatasubramanian, S. (2015). Certifying and removing disparate impact. In Proceed-\nings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining, pages 259–268. ACM.\n[Fong and Vedaldi, 2017] Fong, R. C. and Vedaldi, A. (2017). Interpretable explanations\nof black boxes by meaningful perturbation.\nIn Proceedings of the IEEE International\nConference on Computer Vision, pages 3429–3437.\n[Frosst and Hinton, 2017] Frosst, N. and Hinton, G. (2017). Distilling a neural network into\na soft decision tree. ArXiv, abs/1711.09784.\n[Fuchs et al., 2018] Fuchs, F. B., Groth, O., Kosoriek, A. R., Bewley, A., Wulfmeier, M.,\nVedaldi, A., and Posner, I. (2018). Neural stethoscopes: Unifying analytic, auxiliary and\nadversarial network probing. ArXiv, abs/1806.05502.\n[Garcez et al., 2012] Garcez, A. S. d., Broda, K. B., and Gabbay, D. M. (2012). Neural-\nsymbolic learning systems: Foundations and applications. Springer Science & Business\nMedia.\n[Garvie, 2016] Garvie, C. (2016). The perpetual line-up: Unregulated police face recognition\nin America. Georgetown Law, Center on Privacy & Technology.\n[Georgiev et al., 2017] Georgiev, P., Bhattacharya, S., Lane, N. D., and Mascolo, C. (2017).\nLow-resource multi-task audio sensing for mobile and embedded devices via shared deep\nneural network representations. Proceedings of the ACM on Interactive, Mobile, Wearable\nand Ubiquitous Technologies, 1(3):50.\n[Gilpin et al., 2018] Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., and Kagal,\nL. (2018). Explaining explanations: An overview of interpretability of machine learning. In\n2018 IEEE 5th International Conference on data science and advanced analytics (DSAA),\npages 80–89. IEEE.\n[Gonzalez-Garcia et al., 2018] Gonzalez-Garcia, A., Modolo, D., and Ferrari, V. (2018). Do\nsemantic parts emerge in convolutional neural networks? International Journal of Com-\nputer Vision, 126(5):476–494.\n[Goodfellow et al., 2014] Goodfellow, I. J., Shlens, J., and Szegedy, C. (2014). Explaining\nand harnessing adversarial examples. ArXiv, abs/1412.6572.\n[Goodman and Flaxman, 2017] Goodman, B. and Flaxman, S. (2017).\nEuropean Union\nregulations on algorithmic decision-making and a \"right to explanation\". AI magazine,\n38(3):50–57.\n57\nRas, Xie, van Gerven, & Doran\n[Gordaliza et al., 2019] Gordaliza, P., Del Barrio, E., Fabrice, G., and Loubes, J.-M. (2019).\nObtaining fairness using optimal transport theory. In International Conference on Ma-\nchine Learning, pages 2357–2365.\n[Goswami et al., 2014] Goswami, G., Bhardwaj, R., Singh, R., and Vatsa, M. (2014). MDL-\nFace: Memorability augmented deep learning for video face recognition. In IEEE Inter-\nnational Joint Conference on Biometrics, pages 1–7. IEEE.\n[Goyal et al., 2017] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D.\n(2017). Making the V in VQA matter: Elevating the role of image understanding in\nvisual question answering. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6904–6913.\n[Graves et al., 2014] Graves, A., Wayne, G., and Danihelka, I. (2014). Neural turing ma-\nchines. ArXiv, abs/1410.5401.\n[Grigorescu et al., 2020] Grigorescu, S., Trasnea, B., Cocias, T., and Macesanu, G. (2020).\nA survey of deep learning techniques for autonomous driving. Journal of Field Robotics,\n37(3):362–386.\n[Groth et al., 2018] Groth, O., Fuchs, F. B., Posner, I., and Vedaldi, A. (2018). Shapestacks:\nLearning vision-based physical intuition for generalised object stacking. In Proceedings of\nthe European Conference on Computer Vision (ECCV), pages 702–717.\n[Guidotti et al., 2018] Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F.,\nand Pedreschi, D. (2018). A survey of methods for explaining black box models. ACM\nComputing Surveys (CSUR), 51(5):93.\n[Hardt et al., 2016] Hardt, M., Price, E., Srebro, N., et al. (2016). Equality of opportunity\nin supervised learning. In Advances in neural information processing systems, pages 3315–\n3323.\n[Harradon et al., 2018] Harradon, M., Druce, J., and Ruttenberg, B. (2018).\nCausal\nlearning and explanation of deep neural networks via autoencoded activations. ArXiv,\nabs/1802.00541.\n[Hase and Bansal, 2020] Hase, P. and Bansal, M. (2020). Evaluating explainable AI: Which\nalgorithmic explanations help users predict model behavior? ArXiv, abs/2005.01831.\n[He et al., 2018] He, R., Lee, W. S., Ng, H. T., and Dahlmeier, D. (2018). Eﬀective attention\nmodeling for aspect-level sentiment classiﬁcation. In Proceedings of the 27th International\nConference on Computational Linguistics, pages 1121–1131.\n[Heidari et al., 2018] Heidari, H., Ferrari, C., Gummadi, K., and Krause, A. (2018). Fairness\nbehind a veil of ignorance: A welfare analysis for automated decision making. In Advances\nin Neural Information Processing Systems, pages 1265–1276.\n[Hendricks et al., 2016] Hendricks, L. A., Akata, Z., Rohrbach, M., Donahue, J., Schiele,\nB., and Darrell, T. (2016). Generating visual explanations. In European Conference on\nComputer Vision, pages 3–19. Springer.\n58\nExplainable Deep Learning:A Field Guide for the Uninitiated\n[Heo et al., 2018] Heo, J., Lee, H. B., Kim, S., Lee, J., Kim, K. J., Yang, E., and Hwang,\nS. J. (2018). Uncertainty-aware attention for reliable interpretation and prediction. In\nAdvances in Neural Information Processing Systems, pages 909–918.\n[Heskes et al., 2020] Heskes, T., Sijben, E., Bucur, I. G., and Claassen, T. (2020). Causal\nshapley values: Exploiting causal knowledge to explain individual predictions of complex\nmodels. In Advances in Neural Information Processing Systems, pages 4778–4789.\n[Hind et al., 2019] Hind, M., Wei, D., Campbell, M., Codella, N. C., Dhurandhar, A., Mo-\njsilović, A., Natesan Ramamurthy, K., and Varshney, K. R. (2019). TED: Teaching AI to\nexplain its decisions. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics,\nand Society, pages 123–129. ACM.\n[Hinton et al., 2015] Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the knowledge\nin a neural network. ArXiv, abs/1503.02531.\n[Hooker, 2004] Hooker, G. (2004). Discovering additive structure in black box functions. In\nProceedings of the tenth ACM SIGKDD international conference on Knowledge discovery\nand data mining, pages 575–580.\n[Hooker, 2007] Hooker, G. (2007).\nGeneralized functional anova diagnostics for high-\ndimensional functions of dependent variables. Journal of Computational and Graphical\nStatistics, 16(3):709–732.\n[Hooker et al., 2019] Hooker, S., Erhan, D., Kindermans, P.-J., and Kim, B. (2019).\nA\nbenchmark for interpretability methods in deep neural networks. In Advances in Neural\nInformation Processing Systems, pages 9734–9745.\n[Hoos and Leyton-Brown, 2014] Hoos, H. and Leyton-Brown, K. (2014). An eﬃcient ap-\nproach for assessing hyperparameter importance. In International Conference on Machine\nLearning, pages 754–762.\n[Hou and Zhou, 2020] Hou, B.-J. and Zhou, Z.-H. (2020). Learning with interpretable struc-\nture from gated RNN.\nIEEE transactions on neural networks and learning systems,\n31(7):2267–2279.\n[Iyer et al., 2018] Iyer, R., Li, Y., Li, H., Lewis, M., Sundar, R., and Sycara, K. (2018).\nTransparency and explanation in deep reinforcement learning neural networks. In Pro-\nceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 144–150.\nACM.\n[Jacovi and Goldberg, 2020] Jacovi, A. and Goldberg, Y. (2020).\nTowards faithfully in-\nterpretable NLP systems: How should we deﬁne and evaluate faithfulness?\nArXiv,\nabs/2004.03685.\n[Jain et al., 2015] Jain, A., Koppula, H. S., Raghavan, B., Soh, S., and Saxena, A. (2015).\nCar that knows before you do: Anticipating maneuvers via learning temporal driving\nmodels. In Proceedings of the IEEE International Conference on Computer Vision, pages\n3182–3190.\n59\nRas, Xie, van Gerven, & Doran\n[Jain and Wallace, 2019] Jain, S. and Wallace, B. C. (2019). Attention is not explanation.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 3543–3556.\n[Jesus et al., 2021] Jesus, S., Belém, C., Balayan, V., Bento, J., Saleiro, P., Bizarro, P., and\nGama, J. (2021). How can I choose an explainer? An application-grounded evaluation of\npost-hoc explanations. ArXiv, abs/2101.08758.\n[Jha et al., 2017] Jha, S., Raman, V., Pinto, A., Sahai, T., and Francis, M. (2017). On\nlearning sparse boolean formulae for explaining AI decisions. In NASA Formal Methods\nSymposium, pages 99–114. Springer.\n[Jiang et al., 2018] Jiang, H., Kim, B., Guan, M., and Gupta, M. (2018). To trust or not to\ntrust a classiﬁer. In Advances in neural information processing systems, pages 5541–5552.\n[Johnson et al., 2017] Johnson,\nJ.,\nHariharan,\nB.,\nvan der Maaten,\nL.,\nFei-Fei,\nL.,\nLawrence Zitnick, C., and Girshick, R. (2017). CLEVR: A diagnostic dataset for compo-\nsitional language and elementary visual reasoning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 2901–2910.\n[Kamiran and Calders, 2010] Kamiran, F. and Calders, T. (2010). Classiﬁcation with no\ndiscrimination by preferential sampling. In Proc. 19th Machine Learning Conf. Belgium\nand The Netherlands, pages 1–6. Citeseer.\n[Kamiran and Calders, 2012] Kamiran, F. and Calders, T. (2012). Data preprocessing tech-\nniques for classiﬁcation without discrimination.\nKnowledge and Information Systems,\n33(1):1–33.\n[Kamishima et al., 2011] Kamishima, T., Akaho, S., and Sakuma, J. (2011). Fairness-aware\nlearning through regularization approach. In 2011 IEEE 11th International Conference\non Data Mining Workshops, pages 643–650. IEEE.\n[Kang et al., 2018] Kang, D., Raghavan, D., Bailis, P., and Zaharia, M. (2018).\nModel\nassertions for debugging machine learning. In NeurIPS MLSys Workshop.\n[Kearns et al., 2018] Kearns, M., Neel, S., Roth, A., and Wu, Z. S. (2018).\nPreventing\nfairness gerrymandering: Auditing and learning for subgroup fairness. In International\nConference on Machine Learning, pages 2569–2577.\n[Kim et al., 2014] Kim, B., Rudin, C., and Shah, J. A. (2014). The Bayesian case model:\nA generative approach for case-based reasoning and prototype classiﬁcation. In Advances\nin Neural Information Processing Systems, pages 1952–1960.\n[Kim et al., 2018a] Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F.,\net al. (2018a).\nInterpretability beyond feature attribution: Quantitative testing with\nconcept activation vectors (TCAV). In International Conference on Machine Learning,\npages 2673–2682.\n60\nExplainable Deep Learning:A Field Guide for the Uninitiated\n[Kim et al., 2018b] Kim, J., Rohrbach, A., Darrell, T., Canny, J., and Akata, Z. (2018b).\nTextual explanations for self-driving vehicles. In Proceedings of the European conference\non computer vision (ECCV), pages 563–578.\n[Kindermans et al., 2019] Kindermans, P.-J., Hooker, S., Adebayo, J., Alber, M., Schütt,\nK. T., Dähne, S., Erhan, D., and Kim, B. (2019). The (un)reliability of saliency methods.\nIn Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pages 267–280.\nSpringer.\n[Kindermans et al., 2018] Kindermans, P.-J., Schütt, K., Alber, M., Müller, K.-R., Erhan,\nD., Kim, B., and Dähne, S. (2018). Learning how to explain neural networks: Patternnet\nand Patternattribution. In International Conference on Learning Representations.\n[Kindermans et al., 2016] Kindermans, P.-J., Schütt, K., Müller, K.-R., and Dähne, S.\n(2016). Investigating the inﬂuence of noise and distractors on the interpretation of neural\nnetworks. ArXiv, abs/1611.07270.\n[Koh and Liang, 2017] Koh, P. W. and Liang, P. (2017).\nUnderstanding black-box pre-\ndictions via inﬂuence functions. In Proceedings of the 34th International Conference on\nMachine Learning-Volume 70, pages 1885–1894. JMLR. org.\n[Kolodner, 1992] Kolodner, J. L. (1992). An introduction to case-based reasoning. Artiﬁcial\nintelligence review, 6(1):3–34.\n[Krakovna and Doshi-Velez, 2016] Krakovna, V. and Doshi-Velez, F. (2016).\nIncreasing\nthe interpretability of recurrent neural networks using hidden Markov models. ArXiv,\nabs/1606.05320.\n[Kurakin et al., 2016] Kurakin, A., Goodfellow, I., and Bengio, S. (2016). Adversarial ex-\namples in the physical world. ArXiv, abs/1607.02533.\n[Lage et al., 2019] Lage, I., Chen, E., He, J., Narayanan, M., Kim, B., Gershman, S., and\nDoshi-Velez, F. (2019). An evaluation of the human-interpretability of explanation. ArXiv,\nabs/1902.00006.\n[Lapuschkin et al., 2016] Lapuschkin, S., Binder, A., Montavon, G., Muller, K.-R., and\nSamek, W. (2016). Analyzing classiﬁers: Fisher vectors and deep neural networks. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n2912–2920.\n[Lei et al., 2016] Lei, T., Barzilay, R., and Jaakkola, T. (2016). Rationalizing neural predic-\ntions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language\nProcessing, pages 107–117.\n[Letarte et al., 2018] Letarte, G., Paradis, F., Giguère, P., and Laviolette, F. (2018). Impor-\ntance of self-attention for sentiment analysis. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 267–275.\n[Li et al., 2016] Li, J., Monroe, W., and Jurafsky, D. (2016). Understanding neural networks\nthrough representation erasure. ArXiv, abs/1612.08220.\n61\nRas, Xie, van Gerven, & Doran\n[Li et al., 2018a] Li, O., Liu, H., Chen, C., and Rudin, C. (2018a). Deep learning for case-\nbased reasoning through prototypes: A neural network that explains its predictions. In\nThirty-Second AAAI Conference on Artiﬁcial Intelligence.\n[Li et al., 2018b] Li, Y., Min, M. R., Yu, W., Hsieh, C.-J., Lee, T., and Kruus, E. (2018b).\nOptimal transport classiﬁer: Defending against adversarial attacks by regularized deep\nembedding. ArXiv, abs/1811.07950.\n[Lin et al., 2013] Lin, M., Chen, Q., and Yan, S. (2013).\nNetwork in network.\nArXiv,\nabs/1312.4400.\n[Lipton, 2017] Lipton, Z. C. (2017).\nThe doctor just won’t accept that!\nArXiv,\nabs/1711.08037.\n[Lipton, 2018] Lipton, Z. C. (2018). The mythos of model interpretability. Communications\nof the ACM, 61(10):36–43.\n[Liu et al., 2019] Liu, H., Yin, Q., and Wang, W. Y. (2019). Towards explainable NLP:\nA generative explanation framework for text classiﬁcation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pages 5570–5581.\n[Liu et al., 2017] Liu, S., Wang, X., Liu, M., and Zhu, J. (2017). Towards better analysis of\nmachine learning models: A visual analytics perspective. Visual Informatics, 1(1):48–56.\n[Liu et al., 2018a] Liu, X., Li, Y., Wu, C., and Hsieh, C.-J. (2018a). Adv-bnn: Improved\nadversarial defense through robust bayesian neural network. ArXiv, abs/1810.01279.\n[Liu et al., 2018b] Liu, X., Wang, X., and Matwin, S. (2018b). Improving the interpretabil-\nity of deep neural networks with knowledge distillation.\nIn 2018 IEEE International\nConference on Data Mining Workshops (ICDMW), pages 905–912. IEEE.\n[Louizos et al., 2015] Louizos, C., Swersky, K., Li, Y., Welling, M., and Zemel, R. (2015).\nThe variational fair autoencoder. ArXiv, abs/1511.00830.\n[Lundberg et al., 2018] Lundberg, S. M., Erion, G. G., and Lee, S.-I. (2018). Consistent\nindividualized feature attribution for tree ensembles. ArXiv, abs/1802.03888.\n[Lundberg and Lee, 2017] Lundberg, S. M. and Lee, S.-I. (2017). A uniﬁed approach to\ninterpreting model predictions. In Advances in Neural Information Processing Systems,\npages 4765–4774.\n[Lundén and Koivunen, 2016] Lundén, J. and Koivunen, V. (2016).\nDeep learning for\nHRRP-based target recognition in multistatic radar systems. In IEEE Radar Confer-\nence, pages 1–6. IEEE.\n[Luong et al., 2015] Luong, M.-T., Pham, H., and Manning, C. D. (2015). Eﬀective ap-\nproaches to attention-based neural machine translation. ArXiv, abs/1508.04025.\n[Madry et al., 2017] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. (2017).\nTowards deep learning models resistant to adversarial attacks. ArXiv, abs/1706.06083.\n62\nExplainable Deep Learning:A Field Guide for the Uninitiated\n[Marchette and Socolinsky, 2003] Marchette, C. E. P. D. J. and Socolinsky, J. G. D. D. A.\n(2003). Classiﬁcation using class cover catch digraphs. Journal of Classiﬁcation, 20:3–23.\n[Mascharka et al., 2018] Mascharka, D., Tran, P., Soklaski, R., and Majumdar, A. (2018).\nTransparency by design: Closing the gap between performance and interpretability in\nvisual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 4942–4950.\n[Masi et al., 2018] Masi, I., Wu, Y., Hassner, T., and Natarajan, P. (2018).\nDeep face\nrecognition: A survey. In 2018 31st SIBGRAPI conference on graphics, patterns and\nimages (SIBGRAPI), pages 471–478. IEEE.\n[Melis and Jaakkola, 2018] Melis, D. A. and Jaakkola, T. (2018).\nTowards robust inter-\npretability with self-explaining neural networks. In Advances in Neural Information Pro-\ncessing Systems, pages 7775–7784.\n[Meng and Chen, 2017] Meng, D. and Chen, H. (2017). Magnet: A two-pronged defense\nagainst adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on\nComputer and Communications Security, pages 135–147. ACM.\n[Menon and Williamson, 2018] Menon, A. K. and Williamson, R. C. (2018). The cost of fair-\nness in binary classiﬁcation. In Conference on Fairness, Accountability and Transparency,\npages 107–118.\n[Miller, 2018] Miller, T. (2018).\nContrastive explanation: A structural-model approach.\nArXiv, abs/1811.03163.\n[Miller, 2019] Miller, T. (2019).\nExplanation in artiﬁcial intelligence: Insights from the\nsocial sciences. Artiﬁcial intelligence, 267:1–38.\n[Molnar, 2020] Molnar, C. (2020). Interpretable Machine Learning. Lulu.com.\n[Montavon et al., 2017] Montavon, G., Lapuschkin, S., Binder, A., Samek, W., and Müller,\nK.-R. (2017). Explaining nonlinear classiﬁcation decisions with deep Taylor decomposi-\ntion. Pattern Recognition, 65:211–222.\n[Montavon et al., 2018] Montavon, G., Samek, W., and Müller, K.-R. (2018). Methods for\ninterpreting and understanding deep neural networks. Digital Signal Processing, 73:1–15.\n[Moosavi-Dezfooli et al., 2017] Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., and Frossard,\nP. (2017). Universal adversarial perturbations. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1765–1773.\n[Moosavi-Dezfooli et al., 2016] Moosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. (2016).\nDeepfool: A simple and accurate method to fool deep neural networks. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 2574–2582.\n[Mueller et al., 2019] Mueller, S. T., Hoﬀman, R. R., Clancey, W., Emrey, A., and Klein,\nG. (2019). Explanation in human-AI systems: A literature meta-review, synopsis of key\nideas and publications, and bibliography for explainable AI. ArXiv, abs/1902.01876.\n63\nRas, Xie, van Gerven, & Doran\n[Murdoch et al., 2018] Murdoch, W. J., Liu, P. J., and Yu, B. (2018).\nBeyond word\nimportance:\nContextual decomposition to extract interactions from LSTMs.\nArXiv,\nabs/1801.05453.\n[Murdoch and Szlam, 2017] Murdoch, W. J. and Szlam, A. (2017). Automatic rule extrac-\ntion from long short term memory networks. ArXiv, abs/1702.02540.\n[Nemati et al., 2018] Nemati, S., Holder, A., Razmi, F., Stanley, M. D., Cliﬀord, G. D., and\nBuchman, T. G. (2018). An interpretable machine learning model for accurate prediction\nof sepsis in the ICU. Critical care medicine, 46(4):547–553.\n[Nguyen et al., 2015] Nguyen, A., Yosinski, J., and Clune, J. (2015). Deep neural networks\nare easily fooled: High conﬁdence predictions for unrecognizable images. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 427–436.\n[Nie et al., 2015] Nie, L., Wang, M., Zhang, L., Yan, S., Zhang, B., and Chua, T.-S. (2015).\nDisease inference from health-related questions via sparse deep learning. IEEE Transac-\ntions on Knowledge and Data Engineering, 27(8):2107–2119.\n[Nie et al., 2018] Nie, W., Zhang, Y., and Patel, A. (2018). A theoretical explanation for\nperplexing behaviors of backpropagation-based visualizations. In International Conference\non Machine Learning, pages 3806–3815.\n[Oana-Maria et al., 2019] Oana-Maria, C., Brendan, S., Pasquale, M., Thomas, L., and Phil,\nB. (2019). Make up your mind! Adversarial generation of inconsistent natural language\nexplanations. ArXiv, abs/1910.03065.\n[Olah et al., 2017] Olah, C., Mordvintsev, A., and Schubert, L. (2017). Feature visualiza-\ntion. Distill, 2(11):e7.\n[Olah et al., 2018] Olah, C., Satyanarayan, A., Johnson, I., Carter, S., Schubert, L., Ye, K.,\nand Mordvintsev, A. (2018). The building blocks of interpretability. Distill, 3(3):e10.\n[Olfat and Aswani, 2018] Olfat, M. and Aswani, A. (2018). Spectral algorithms for comput-\ning fair support vector machines. In International Conference on Artiﬁcial Intelligence\nand Statistics, pages 1933–1942.\n[Ozbulak, 2019] Ozbulak, U. (2019). PyTorch CNN visualizations. https://github.com/\nutkuozbulak/pytorch-cnn-visualizations.\n[Papernot et al., 2017] Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., and\nSwami, A. (2017). Practical black-box attacks against machine learning. In Proceedings\nof the 2017 ACM on Asia conference on computer and communications security, pages\n506–519. ACM.\n[Papernot et al., 2016a] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B.,\nand Swami, A. (2016a). The limitations of deep learning in adversarial settings. In 2016\nIEEE European Symposium on Security and Privacy (EuroS&P), pages 372–387. IEEE.\n64\nExplainable Deep Learning:A Field Guide for the Uninitiated\n[Papernot et al., 2016b] Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami, A.\n(2016b). Distillation as a defense to adversarial perturbations against deep neural net-\nworks. In 2016 IEEE Symposium on Security and Privacy (SP), pages 582–597. IEEE.\n[Park et al., 2018] Park, D. H., Hendricks, L. A., Akata, Z., Rohrbach, A., Schiele, B.,\nDarrell, T., and Rohrbach, M. (2018). Multimodal explanations: Justifying decisions and\npointing to the evidence.\nIn 31st IEEE Conference on Computer Vision and Pattern\nRecognition.\n[Park et al., 2016] Park, D. H., Hendricks, L. A., Akata, Z., Schiele, B., Darrell, T., and\nRohrbach, M. (2016). Attentive explanations: Justifying decisions and pointing to the\nevidence. ArXiv, abs/1612.04757.\n[Pérez-Suay et al., 2017] Pérez-Suay, A., Laparra, V., Mateo-García, G., Muñoz-Marí, J.,\nGómez-Chova, L., and Camps-Valls, G. (2017). Fair kernel learning. In Joint European\nConference on Machine Learning and Knowledge Discovery in Databases, pages 339–355.\nSpringer.\n[Petsiuk et al., 2018] Petsiuk, V., Das, A., and Saenko, K. (2018). Rise: Randomized input\nsampling for explanation of black-box models. ArXiv, abs/1806.07421.\n[Pham and Shen, 2017] Pham, T. T. and Shen, Y. (2017). A deep causal inference approach\nto measuring the eﬀects of forming group loans in online non-proﬁt microﬁnance platform.\nArXiv, abs/1706.02795.\n[Pleiss et al., 2017] Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., and Weinberger, K. Q.\n(2017). On fairness and calibration. In Advances in Neural Information Processing Sys-\ntems, pages 5680–5689.\n[Prasad et al., 2020] Prasad, G., Nie, Y., Bansal, M., Jia, R., Kiela, D., and Williams, A.\n(2020). To what extent do human explanations of model behavior align with actual model\nbehavior? ArXiv, abs/2012.13354.\n[Raghu et al., 2017] Raghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J. (2017).\nSVCCA: Singular vector canonical correlation analysis for deep learning dynamics and\ninterpretability. In Advances in Neural Information Processing Systems, pages 6076–6085.\n[Rajpurkar et al., 2018] Rajpurkar, P., Irvin, J., Ball, R. L., Zhu, K., Yang, B., Mehta,\nH., Duan, T., Ding, D., Bagul, A., Langlotz, C. P., et al. (2018).\nDeep learning for\nchest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to\npracticing radiologists. PLoS medicine, 15(11):e1002686.\n[Ras et al., 2018] Ras, G., van Gerven, M., and Haselager, P. (2018). Explanation methods\nin deep learning: Users, values, concerns and challenges. In Explainable and Interpretable\nModels in Computer Vision and Machine Learning, pages 19–36. Springer.\n[Ray et al., 2019] Ray, A., Yao, Y., Kumar, R., Divakaran, A., and Burachas, G. (2019).\nCan you explain that? Lucid explanations help human-AI collaborative image retrieval.\nIn Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, vol-\nume 7, pages 153–161.\n65\nRas, Xie, van Gerven, & Doran\n[Redmon et al., 2016] Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). You\nonly look once: Uniﬁed, real-time object detection. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 779–788.\n[Ribeiro et al., 2016a] Ribeiro, M. T., Singh, S., and Guestrin, C. (2016a). Model-agnostic\ninterpretability of machine learning. ArXiv, abs/1606.05386.\n[Ribeiro et al., 2016b] Ribeiro, M. T., Singh, S., and Guestrin, C. (2016b).\nNothing\nelse matters: Model-agnostic explanations by identifying prediction invariance. ArXiv,\nabs/1611.05817.\n[Ribeiro et al., 2016c] Ribeiro, M. T., Singh, S., and Guestrin, C. (2016c).\nWhy should\nI trust you?: Explaining the predictions of any classiﬁer.\nIn Proceedings of the 22nd\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages\n1135–1144. ACM.\n[Ribeiro et al., 2018] Ribeiro, M. T., Singh, S., and Guestrin, C. (2018). Anchors: High-\nprecision model-agnostic explanations. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence.\n[Robnik-Šikonja and Bohanec, 2018] Robnik-Šikonja,\nM.\nand\nBohanec,\nM.\n(2018).\nPerturbation-based explanations of prediction models. In Human and machine learning,\npages 159–175. Springer.\n[Robnik-Šikonja and Kononenko, 2008] Robnik-Šikonja, M. and Kononenko, I. (2008). Ex-\nplaining classiﬁcations for individual instances. IEEE Transactions on Knowledge and\nData Engineering, 20(5):589–600.\n[Rolnick et al., 2019] Rolnick, D., Donti, P. L., Kaack, L. H., Kochanski, K., Lacoste, A.,\nSankaran, K., Ross, A. S., Milojevic-Dupont, N., Jaques, N., Waldman-Brown, A., et al.\n(2019). Tackling climate change with machine learning. ArXiv, abs/1906.05433.\n[Rozsa et al., 2016] Rozsa, A., Rudd, E. M., and Boult, T. E. (2016). Adversarial diversity\nand hard positive generation. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops, pages 25–32.\n[Rudin, 2019] Rudin, C. (2019). Stop explaining black box machine learning models for\nhigh stakes decisions and use interpretable models instead. Nature Machine Intelligence,\n1(5):206.\n[Sabour et al., 2015] Sabour, S., Cao, Y., Faghri, F., and Fleet, D. J. (2015). Adversarial\nmanipulation of deep representations. ArXiv, abs/1511.05122.\n[Saha et al., 2019] Saha, A., Subramanya, A., and Pirsiavash, H. (2019). Hidden trigger\nbackdoor attacks. ArXiv, abs/1910.00033.\n[Samangouei et al., 2018] Samangouei, P., Kabkab, M., and Chellappa, R. (2018). Defense-\ngan: Protecting classiﬁers against adversarial attacks using generative models. ArXiv,\nabs/1805.06605.\n66\nExplainable Deep Learning:A Field Guide for the Uninitiated\n[Samek et al., 2016] Samek, W., Binder, A., Montavon, G., Lapuschkin, S., and Müller, K.-\nR. (2016). Evaluating the visualization of what a deep neural network has learned. IEEE\ntransactions on neural networks and learning systems, 28(11):2660–2673.\n[Samek et al., 2017] Samek, W., Wiegand, T., and Müller, K.-R. (2017). Explainable ar-\ntiﬁcial intelligence: Understanding, visualizing and interpreting deep learning models.\nArXiv, abs/1708.08296.\n[Selvaraju et al., 2017] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D.,\nand Batra, D. (2017). Grad-CAM: Visual explanations from deep networks via gradient-\nbased localization. In Proceedings of the IEEE International Conference on Computer\nVision, pages 618–626.\n[Serrano and Smith, 2019] Serrano, S. and Smith, N. A. (2019). Is attention interpretable?\nArXiv, abs/1906.03731.\n[Shapley, 1953] Shapley, L. (1953). A value for n-person games. Contributions to the Theory\nof Games, 2.28:307–317.\n[Shrikumar et al., 2017] Shrikumar, A., Greenside, P., and Kundaje, A. (2017). Learning\nimportant features through propagating activation diﬀerences. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70, pages 3145–3153. JMLR. org.\n[Simonyan et al., 2013] Simonyan, K., Vedaldi, A., and Zisserman, A. (2013). Deep inside\nconvolutional networks: Visualising image classiﬁcation models and saliency maps. ArXiv,\nabs/1312.6034.\n[Sirignano et al., 2016] Sirignano, J., Sadhwani, A., and Giesecke, K. (2016). Deep learning\nfor mortgage risk. ArXiv, abs/1607.02470.\n[Sobol, 2001] Sobol, I. M. (2001).\nGlobal sensitivity indices for nonlinear mathematical\nmodels and their Monte Carlo estimates.\nMathematics and computers in simulation,\n55(1-3):271–280.\n[Springenberg et al., 2014] Springenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller,\nM. (2014). Striving for simplicity: The all convolutional net. ArXiv, abs/1412.6806.\n[Su et al., 2019] Su, J., Vargas, D. V., and Sakurai, K. (2019). One pixel attack for fooling\ndeep neural networks. IEEE Transactions on Evolutionary Computation, 23(5):828–841.\n[Sundararajan et al., 2016] Sundararajan, M., Taly, A., and Yan, Q. (2016). Gradients of\ncounterfactuals. ArXiv, abs/1611.02639.\n[Sundararajan et al., 2017] Sundararajan, M., Taly, A., and Yan, Q. (2017).\nAxiomatic\nattribution for deep networks. In Proceedings of the 34th International Conference on\nMachine Learning-Volume 70, pages 3319–3328. JMLR. org.\n[Szegedy et al., 2013] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Good-\nfellow, I., and Fergus, R. (2013).\nIntriguing properties of neural networks.\nArXiv,\nabs/1312.6199.\n67\nRas, Xie, van Gerven, & Doran\n[Tabacof and Valle, 2016] Tabacof, P. and Valle, E. (2016). Exploring the space of adversar-\nial images. In 2016 International Joint Conference on Neural Networks (IJCNN), pages\n426–433. IEEE.\n[Tan et al., 2018] Tan, S., Caruana, R., Hooker, G., Koch, P., and Gordo, A. (2018).\nLearning global additive explanations for neural nets using model distillation.\nArXiv,\nabs/1801.08640.\n[Teney et al., 2018] Teney, D., Anderson, P., He, X., and van den Hengel, A. (2018). Tips\nand tricks for visual question answering: Learnings from the 2017 challenge. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pages 4223–4232.\n[Tjoa and Guan, 2019] Tjoa, E. and Guan, C. (2019). A survey on explainable artiﬁcial\nintelligence (XAI): Towards medical XAI. ArXiv, abs/1907.07374.\n[Tulshan and Dhage, 2018] Tulshan, A. S. and Dhage, S. N. (2018).\nSurvey on virtual\nassistant: Google Assistant, Siri, Cortana, Alexa. In International Symposium on Signal\nProcessing and Intelligent Recognition Systems, pages 190–201. Springer.\n[Vashishth et al., 2019] Vashishth, S., Upadhyay, S., Tomar, G. S., and Faruqui, M. (2019).\nAttention interpretability across nlp tasks. ArXiv, abs/1909.11218.\n[Vaswani et al., 2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all you need. In Advances in\nneural information processing systems, pages 5998–6008.\n[Verma et al., 2020] Verma, S., Dickerson, J., and Hines, K. (2020). Counterfactual Expla-\nnations for Machine Learning: A Review. ArXiv, abs/2010.10596.\n[Vinyals et al., 2015] Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015). Show and\ntell: A neural image caption generator. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 3156–3164.\n[Vu et al., 2019] Vu, M. N., Nguyen, T. D., Phan, N., Gera, R., and Thai, M. T. (2019).\nc-Eval: A uniﬁed metric to evaluate feature-based explanations via perturbation. ArXiv,\nabs/1906.02032.\n[Wang et al., 2016] Wang, Y., Huang, M., Zhao, L., et al. (2016). Attention-based LSTM for\naspect-level sentiment classiﬁcation. In Proceedings of the 2016 conference on empirical\nmethods in natural language processing, pages 606–615.\n[Wiegreﬀe and Pinter, 2019] Wiegreﬀe, S. and Pinter, Y. (2019). Attention is not not expla-\nnation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 11–20.\n[Wolf et al., 2019] Wolf, L., Galanti, T., and Hazan, T. (2019). A formal approach to ex-\nplainability. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Soci-\nety, pages 255–261.\n68\nExplainable Deep Learning:A Field Guide for the Uninitiated\n[Woodworth et al., 2017] Woodworth, B., Gunasekar, S., Ohannessian, M. I., and Srebro,\nN. (2017). Learning non-discriminatory predictors. ArXiv, abs/1702.06081.\n[Xie et al., 2017a] Xie, C., Wang, J., Zhang, Z., Ren, Z., and Yuille, A. (2017a). Mitigating\nadversarial eﬀects through randomization. ArXiv, abs/1711.01991.\n[Xie et al., 2019] Xie, N., Lai, F., Doran, D., and Kadav, A. (2019). Visual entailment: A\nnovel task for ﬁne-grained image understanding. ArXiv, abs/1901.06706.\n[Xie et al., 2017b] Xie, N., Sarker, M. K., Doran, D., Hitzler, P., and Raymer, M. (2017b).\nRelating input concepts to convolutional neural network decisions. ArXiv, abs/1711.08006.\n[Xu et al., 2015] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel,\nR., and Bengio, Y. (2015). Show, attend and tell: Neural image caption generation with\nvisual attention. In International conference on machine learning, pages 2048–2057.\n[Yim et al., 2017] Yim, J., Joo, D., Bae, J., and Kim, J. (2017). A gift from knowledge\ndistillation: Fast optimization, network minimization and transfer learning. In The IEEE\nConference on Computer Vision and Pattern Recognition.\n[Yosinski et al., 2015] Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., and Lipson, H. (2015).\nUnderstanding neural networks through deep visualization. ArXiv, abs/1506.06579.\n[Yuan et al., 2019] Yuan, X., He, P., Zhu, Q., and Li, X. (2019). Adversarial examples:\nAttacks and defenses for deep learning. IEEE transactions on neural networks and learning\nsystems, 30(9):2805–2824.\n[Zafar et al., 2017a] Zafar, M. B., Valera, I., Gomez Rodriguez, M., and Gummadi, K. P.\n(2017a). Fairness beyond disparate treatment & disparate impact: Learning classiﬁcation\nwithout disparate mistreatment. In Proceedings of the 26th International Conference on\nWorld Wide Web, pages 1171–1180. International World Wide Web Conferences Steering\nCommittee.\n[Zafar et al., 2017b] Zafar, M. B., Valera, I., Rodriguez, M., Gummadi, K., and Weller, A.\n(2017b). From parity to preference-based notions of fairness in classiﬁcation. In Advances\nin Neural Information Processing Systems, pages 229–239.\n[Zeiler and Fergus, 2014] Zeiler, M. D. and Fergus, R. (2014). Visualizing and understand-\ning convolutional networks. In European conference on computer vision, pages 818–833.\nSpringer.\n[Zeiler et al., 2011] Zeiler, M. D., Taylor, G. W., and Fergus, R. (2011). Adaptive deconvolu-\ntional networks for mid and high level feature learning. In 2011 International Conference\non Computer Vision, pages 2018–2025. IEEE.\n[Zellers et al., 2019] Zellers, R., Bisk, Y., Farhadi, A., and Choi, Y. (2019). From recognition\nto cognition: Visual commonsense reasoning. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 6720–6731.\n69\nRas, Xie, van Gerven, & Doran\n[Zemel et al., 2013] Zemel, R., Wu, Y., Swersky, K., Pitassi, T., and Dwork, C. (2013).\nLearning fair representations. In International Conference on Machine Learning, pages\n325–333.\n[Zhang et al., 2016] Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2016).\nUnderstanding deep learning requires rethinking generalization. ArXiv, abs/1611.03530.\n[Zhang et al., 2018] Zhang, Q., Cao, R., Shi, F., Wu, Y. N., and Zhu, S.-C. (2018). Inter-\npreting cnn knowledge via an explanatory graph. In Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence.\n[Zhang et al., 2017] Zhang, Q., Cao, R., Wu, Y. N., and Zhu, S.-C. (2017). Growing inter-\npretable part graphs on ConvNets via multi-shot learning. In Proc. of AAAI Conference\non Artiﬁcial Intelligence, pages 2898–2906.\n[Zhang et al., 2019a] Zhang, Q., Yang, Y., Ma, H., and Wu, Y. N. (2019a). Interpreting\nCNNs via decision trees. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 6261–6270.\n[Zhang and Zhu, 2018] Zhang, Q.-s. and Zhu, S.-C. (2018).\nVisual interpretability for\ndeep learning: A survey.\nFrontiers of Information Technology & Electronic Engineer-\ning, 19(1):27–39.\n[Zhang et al., 2019b] Zhang, W. E., Sheng, Q. Z., and Alhazmi, A. A. F. (2019b). Gen-\nerating textual adversarial examples for deep learning models:\nA survey.\nArXiv,\nabs/1901.06796.\n[Zhao et al., 2017] Zhao, Z., Dua, D., and Singh, S. (2017). Generating natural adversarial\nexamples. ArXiv, abs/1710.11342.\n[Zhou et al., 2014] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. (2014).\nObject detectors emerge in deep scene CNNs. ArXiv, abs/1412.6856.\n[Zhou et al., 2016] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. (2016).\nLearning deep features for discriminative localization. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages 2921–2929.\n[Zintgraf et al., 2017] Zintgraf, L. M., Cohen, T. S., Adel, T., and Welling, M. (2017).\nVisualizing deep neural network decisions:\nPrediction diﬀerence analysis.\nArXiv,\nabs/1702.04595.\n70\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2020-04-30",
  "updated": "2021-09-13"
}