{
  "id": "http://arxiv.org/abs/2004.14876v2",
  "title": "Analyzing the Surprising Variability in Word Embedding Stability Across Languages",
  "authors": [
    "Laura Burdick",
    "Jonathan K. Kummerfeld",
    "Rada Mihalcea"
  ],
  "abstract": "Word embeddings are powerful representations that form the foundation of many\nnatural language processing architectures, both in English and in other\nlanguages. To gain further insight into word embeddings, we explore their\nstability (e.g., overlap between the nearest neighbors of a word in different\nembedding spaces) in diverse languages. We discuss linguistic properties that\nare related to stability, drawing out insights about correlations with\naffixing, language gender systems, and other features. This has implications\nfor embedding use, particularly in research that uses them to study language\ntrends.",
  "text": "Analyzing the Surprising Variability in Word Embedding Stability\nAcross Languages\nLaura Burdick, Jonathan K. Kummerfeld and Rada Mihalcea\nComputer Science & Engineering\nUniversity of Michigan, Ann Arbor\n{lburdick,jkummerf,mihalcea}@umich.edu\nAbstract\nWord embeddings are powerful representa-\ntions that form the foundation of many natu-\nral language processing architectures, both in\nEnglish and in other languages. To gain fur-\nther insight into word embeddings, we explore\ntheir stability (e.g., overlap between the near-\nest neighbors of a word in different embedding\nspaces) in diverse languages. We discuss lin-\nguistic properties that are related to stability,\ndrawing out insights about correlations with\nafﬁxing, language gender systems, and other\nfeatures. This has implications for embedding\nuse, particularly in research that uses them to\nstudy language trends.\n1\nIntroduction\nWord embeddings have become an established part\nof natural language processing (NLP) (Collobert\net al., 2011; Wang et al., 2020a). Stability, deﬁned\nas the overlap between the nearest neighbors of\na word in different embedding spaces, was intro-\nduced to measure variations in local embedding\nneighborhoods across changes in data, algorithms,\nand word properties (Antoniak and Mimno, 2018;\nWendlandt et al., 2018). These studies found that\nmany common English embedding spaces are sur-\nprisingly unstable, which has implications for work\nthat uses embeddings as features in downstream\ntasks, and work that uses embeddings to study spe-\nciﬁc properties of language.\nHowever, research to date on word embedding\nstability has been exclusively done on English and\nso is not representative of all languages. In this\nwork, we explore the stability of word embeddings\nin a wide range of languages. Better understanding\nthe differences caused by diverse languages will\nprovide a foundation for building embeddings and\nNLP tools in all languages.1\n1Code is available at https://lit.eecs.umich.\nedu/downloads.html.\nIn English and other very high resource lan-\nguages, it has become common practice to use\ncontextualized word embeddings, such as BERT\n(Devlin et al., 2019) and XLNet (Yang et al., 2019).\nThese algorithms require huge amounts of computa-\ntional resources and data. For example, it takes 2.5\ndays to train XLNet with 512 TPU v3 chips. In ad-\ndition to requiring heavy computational resources,\nmost contextualized embedding algorithms need\nlarge amounts of data. BERT uses 3.3 billion words\nof training data. In contrast to these large corpora,\nmany datasets from low-resource languages are\nfairly small (Maxwell and Hughes, 2006). To sup-\nport scenarios where using huge amounts of data\nand computational resources is not feasible, it is im-\nportant to continue developing our understanding\nof context-independent word embeddings, such as\nword2vec (Mikolov et al., 2013) and GloVe (Pen-\nnington et al., 2014). These algorithms continue to\nbe used in a wide variety of situations, including\nthe computational humanities (Abdulrahim, 2019;\nHellrich et al., 2019) and languages where only\nsmall corpora are available (Joshi et al., 2019).\nIn this work, we consider how stability varies for\ndifferent languages, and how linguistic properties\nare related to stability—a previously understudied\nrelationship. Using regression modeling, we cap-\nture relationships between linguistic properties and\naverage stability of a language, and we draw out\ninsights about how linguistic features relate to sta-\nbility. For instance, we ﬁnd that embeddings in\nlanguages with more afﬁxing tend to be less stable.\nOur ﬁndings provide crucial context for research\nthat uses word embeddings to study language prop-\nerties and trends (e.g., Heyman and Heyman, 2019;\nAbdulrahim, 2019), which often rely on raw em-\nbeddings created by GloVe or word2vec. If these\nembeddings are unstable, then research using them\nneeds to take this into account in terms of method-\nologies and error analysis.\narXiv:2004.14876v2  [cs.CL]  9 Sep 2021\n2\nRelated Work\nWord embeddings are low-dimensional vectors\nused to represent words, normally in downstream\ntasks, such as word sense disambiguation (Scarlini\net al., 2020) and text summarization (Moradi et al.,\n2020). They have been shown to capture both syn-\ntactic and semantic properties of words, making\nthem useful in a wide range of NLP tasks (Wang\net al., 2020b). In this work, we explore word em-\nbeddings that generate one embedding per word,\nregardless of the word’s context. We consider two\nwidely used algorithms: word2vec (Mikolov et al.,\n2013) and GloVe (Pennington et al., 2014).\nOur work analyzes embeddings in multiple lan-\nguages, which is important because embeddings\nare commonly used across many languages. In par-\nticular, there has been interest in embeddings for\nlow-resource languages (Chimalamarri et al., 2020;\nStringham and Izbicki, 2020).\nIn this work, we use stability to measure the\nquality of word embeddings. Similar to the work\nwe present here on stability, other research looks\nat how nearest neighbors vary as properties of the\nembedding spaces change. Pierrejean and Tanguy\n(2018) found that the lowest frequency and the\nhighest frequency words have the highest variation\namong nearest neighbors. Additional research has\nexplored how semantic and syntactic properties\nof words change with different embedding algo-\nrithm and parameter choices (Artetxe et al., 2018;\nYaghoobzadeh and Schütze, 2016). Unlike our\nwork, previous studies only considered English.\nFinally, while our work is not a form of embed-\nding evaluation, it is related to the topic (Chiu et al.,\n2016; Rogers et al., 2018; Qiu et al., 2018). There\nhas been extensive work on evaluating word em-\nbeddings, seen in the recent RepEval workshops\n(Rogers et al., 2019), and going back to work com-\nparing them with counting based methods (Baroni\net al., 2014). Our ﬁndings indicate that work on\nembedding evaluation should take into considera-\ntion stability, using multiple training runs to con-\nﬁrm results. Similarly, stability should be con-\nsidered when studying the impact of embeddings\non downstream tasks. Leszczynski et al. (2020)\nspeciﬁcally looked at the downstream instability\nof word embeddings, and found that there is a\nstability-memory tradeoff, and higher stability can\nbe achieved by increasing the embedding dimen-\nsion.\n3\nData\nIn order to explore the stability of word em-\nbeddings in different languages, we work with\ntwo datasets, Wikipedia and the Bible.\nWhile\nWikipedia has more data, the Bible covers more lan-\nguages. Wikipedia is a comparable corpus, whereas\nthe Bible is a parallel corpus.\nWikipedia Corpus.\nWe use pre-processed\nWikipedia dumps in 40 languages taken from Al-\nRfou’ et al. (2013).2 The size of these Wikipedia\ncorpora varies from 329,136 sentences (Tagalog)\nto 75,241,648 sentences (English), with an average\nof 9,292,394 sentences. For all of our experiments,\nwe downsample each corpus to work with compa-\nrably sized data (details in Section 4.2).\nBible Corpus. We consider 97 languages from\nthe pre-processed Bible corpus (McCarthy et al.,\n2020):3 all languages for which at least 75% of the\nBible (≥23, 326 verses) is present.4 This excludes\nmany languages for which there is only a partial\nBible, e.g., just the New Testament, which would\nbe insufﬁcient for training word vectors. We con-\nsider two sets of languages with the Bible corpus:\nlanguages that overlap with the set of Wikipedia\nlanguages (26 languages), and all languages in the\nBible corpus (97 languages).\nWALS. To gain linguistic properties of these lan-\nguages, we use the World Atlas of Language Struc-\ntures (WALS),5 a database of phonological, lex-\nical, and grammatical properties for over 2,000\nlanguages (Dryer and Haspelmath, 2013). This\nexpert-curated resource contains 192 language fea-\ntures. For example, WALS records subject, object,\nand verb word order for various languages.\n4\nCalculating Stability in Many\nLanguages\nThe ﬁrst part of our work is a comparison of stabil-\nity across languages. Before presenting our mea-\nsurements, we deﬁne stability and analyze some\nimportant methodological decisions.\n2Available online at https://sites.google.com/\nsite/rmyeid/projects/polyglot.\n3Available by contacting McCarthy et al. (2020).\n4To work with a maximum number of languages, we only\nconsider the complete Protestant Bible (i.e., all of the verses\nthat appear in the English King James Version of the Bible).\n5Available online at https://wals.info.\nModel 1: indie, punk, progressive, pop, roll, band,\nblues, brass, class, alternative\nModel 2: punk, indie, alternative, progressive, band,\nsedimentary, bands, psychedelic, climbing, pop\nModel 3: punk, pop, indie, alternative, band, roll,\nprogressive, folk, climbing, metal\nTable 1:\nTen nearest neighbors for the word rock\nin three GloVe models trained on different subsets of\nLarge English Wikipedia. Words in all lists are in bold;\nwords in only two lists are italicized. Models 1 and 2\nhave 6 words (60%) in common, models 1 and 3 have\n7, and models 2 and 3 have 7. Therefore, this word has\na stability of 66.7%, the average word overlap between\nthe three models.\n4.1\nDeﬁning Stability\nStability is deﬁned as the percent overlap between\nnearest neighbors in an embedding space. To calcu-\nlate stability, given a word W and two embedding\nspaces A and B, take the ten nearest neighbors\n(measured using cosine similarity) of W in both\nA and B. The stability of W is the percent over-\nlap between these two lists of nearest neighbors.6\n100% stability indicates perfect agreement between\nthe two embedding spaces, while 0% stability in-\ndicates complete disagreement. Table 1 shows a\nsimple example. This deﬁnition of stability can be\ngeneralized to more than two embedding spaces\nby considering the average overlap between pairs\nof embedding spaces. Let X and Y be two sets\nof embedding spaces. Then, for every pair of em-\nbedding spaces (x, y), where x ∈X and y ∈Y ,\ntake the ten nearest neighbors of W in both x and\ny and calculate percent overlap. Let the stability\nbe the average percent overlap over every pair of\nembedding spaces (x, y).\nPrevious work has explored stability for English\nword embeddings. For instance, it was found that\nthe presence of certain documents in the train-\ning corpus affects stability (Antoniak and Mimno,\n2018), and that training and evaluating embeddings\non separate domains is less stable than training and\nevaluating on the same domain (Wendlandt et al.,\n2018). In this work, we expand this analysis to a\nmore diverse set of languages.\n6While alternative deﬁnitions of stability are possible, e.g.,\nconsidering a vector of similarities with a large set of words,\nwe chose to use a prior deﬁnition of stability that has been\nrigorously studied. Similarly, sets of nearest neighbors smaller\nand larger than ten have been tried previously, with compara-\nble results (Wendlandt et al., 2018).\n4.1.1\nThe Effect of Downsampling on\nStability\nStability measures how changes to the input data\nor training algorithm affect the resulting embed-\ndings. Sometimes we make changes with the goal\nof shifting the embeddings, such as increasing the\ncontext window size to try to get embeddings that\ncapture semantics more than syntax. In other cases,\nwe would hope a change would not substantially\nchange embeddings, such as changing the random\nseed for the algorithm. For our experiments, we\nconsider a previously unstudied source of instabil-\nity: different data samples from the same distribu-\ntion. This is a case where we hope embeddings\nremain stable, given a sufﬁciently large sample.\nWe generate data samples by downsampling a\ncorpus to create multiple smaller corpora; we then\nmeasure stability across these downsamples. The\nchoice of sampling with or without replacement,\nand the size of the sample are subtle methodolog-\nical choices. In this section, we consider whether\nstability across downsamples produces consistent\nresults that we can compare across languages.\nFirst, we consider downsampling with replace-\nment, shown in Figure 1a. We use data drawn\nfrom an English Wikipedia corpus of 5,269,686\nsentences (denoted “Large English Wikipedia\").7\nWe randomly sample ﬁve sets of 500,000 sentences\nmultiple times, controlling the amount of overlap\nbetween downsamples (from 10% to 60% shared\nacross all ﬁve samples). For a speciﬁc overlap\namount X%, X% of 500,000 sentences is randomly\nsampled and included in all of the ﬁve downsam-\nples. The remaining (100-X)% sentences are ran-\ndomly sampled for each downsample.\nStability is calculated using GloVe embeddings\nand the words that occur in every downsample for\nevery overlap percentage. In Figure 1a, we group\nstability into buckets of size 5% (i.e., 0-5%, 5-10%,\netc). This allows us to see patterns in stability that\nare not visible from a single statistic, such as the\noverall average. We see that while stability trends\nare similar for different overlap amounts, stabil-\nity is consistently higher as the overlap amount\nincreases. This means that if we use downsam-\npling with replacement, we cannot reliably com-\npare stability across multiple corpora of varying\nsizes (e.g., Wikipedia and the much smaller Bible\ncorpus). The overlap amount would change de-\n7This data was used in Tsvetkov et al. (2016) and is avail-\nable by contacting the authors of that paper.\nExperiment\nMachine\nTiming\nTraining one w2v embedding on one Wikipedia corpus (Section 4)\nMachine 1\n13 sec.\nTraining one GloVe embedding on one Wikipedia corpus (Section 4)\nMachine 1\n12 min.\nCalculating stability on one Wikipedia corpus (Section 4)\nMachine 1\n17 sec.\nTraining one w2v embedding on one Bible corpus (Section 4)\nMachine 1\n5 sec.\nCalculating stability on one Bible corpus (Section 4)\nMachine 1\n12 sec.\nTraining regression model (Section 5)\nMachine 2\n< 7 sec.\nLeave-one-out cross-validation (Section 6)\nMachine 2\n< 4 sec.\nTable 2: Runtimes for different experimental portions of this work. Machine 1 is four Intel(R) Xeon(R) CPU\nE5-1603 v3 @ 2.80 GHz processors. Machine 2 is a 2.9GHz Dual-Core Intel Core i5.\n0\n20\n40\n60\n80\n100\n% Stability (bucketed)\n10\n2\n10\n1\n100\n101\n102\n% of words (log scale)\n10%\n40%\n60%\n(a) Sampling with replacement, varying percentage over-\nlap between samples.\n0\n20\n40\n60\n80\n100\n% Stability (bucketed)\n10\n2\n10\n1\n100\n101\n102\n% of words (log scale)\n50,000\n100,000\n250,000\n500,000\n(b) Sampling without replacement, varying sample size.\nFigure 1: Measuring the impact of data sampling pa-\nrameters on stability measurements. Results when sam-\npling with replacement consistently increase as overlap\nincreases (a). This poses a problem, as results may re-\nﬂect corpus size rather than intrinsic stability. Results\nwhen sampling without replacement do show a consis-\ntent pattern, even when the sample is only 50,000 sen-\ntences, a tenth of the largest sample size (b).\npending on the size of the corpus, changing our\nstability measurement.\nInstead of downsampling with replacement,\nwe consider downsampling without replacement,\nshown in Figure 1b for different downsample sizes.\nWe see that varying the size of the downsample\ndoes not have a large effect on the patterns of sta-\nbility. Particularly when looking at lower stability,\nthe trends are remarkably consistent, even when\nthe downsample size varies from 50,000 sentences\nto 500,000 sentences. The pattern grows less con-\nsistent when looking at higher stability, especially\nwith smaller downsample sizes.\nThis comparison (Figures 1a and 1b) shows that\ndownsampling without replacement produces more\nconsistent (and thus comparable) stability results\nthan downsampling with replacement. Thus, we\nonly consider downsampling without replacement.\n4.2\nStability for Wikipedia and the Bible\nOur ﬁrst study, shown in Figure 2, considers sta-\nbility across the 26 languages included in both\nWikipedia and the Bible. These results show three\nsettings for Wikipedia: (1) Stability of GloVe em-\nbeddings across ﬁve downsampled corpora, (2)\nStability of word2vec (w2v) embeddings across\nﬁve downsampled corpora, and (3) Stability of\nword2vec embeddings using ﬁve random seeds on\none downsampled corpus. For the Bible, we only\nshow the third case, since it is too small for down-\nsampling.\nEach downsampled corpus is 100,000 sentences,\nand words that occur with a frequency less than ﬁve\nare ignored. Previous work (Pierrejean and Tanguy,\n2018) has indicated that words that appear this in-\nfrequently will be very unstable. We use standard\nparameters for both embedding algorithms.8 For\neach embedding, we calculate the ten nearest neigh-\nbors of every word using FAISS9 (Johnson et al.,\n2019). Finally, for each language, we calculate the\nstability for every word in that language across all\nﬁve embedding spaces. Experimental runtimes are\nlisted in Table 2.\nFigure 2 shows bucketed stability for both\nWikipedia and the Bible. Most languages have\nthe same overall trend: a large number of relatively\nunstable word embeddings, then a fairly ﬂat distri-\n8For GloVe (Pennington et al., 2014), we use 100 itera-\ntions, 300 dimensions, a window size of 5, and a minimum\nword count of 5; these parameters led to good performance in\nWendlandt et al. (2018). For word2vec (Mikolov et al., 2013),\nwe use 300 dimensions, a window size of 5, and a minimum\nword count of 5.\n9We use exact, not approximate, search.\n0\n1\n100\n% of words\n(log scale)\nStandard Arabic\n0.52%\n1.12%\n0.82%\n0.88%\nBulgarian\n0.65%\n1.06%\n0.97%\n1.69%\nCzech\n0.47%\n0.76%\n0.83%\n1.85%\nDanish\n0.65%\n1.08%\n0.83%\n2.28%\n0\n1\n100\n% of words\n(log scale)\nStandard German\n0.68%\n1.15%\n0.8%\n2.1%\nGreek\n0.85%\n1.39%\n0.94%\n2.63%\nEnglish\n0.79%\n1.34%\n0.84%\n1.39%\nFinnish\n0.47%\n0.77%\n0.73%\n2.29%\n0\n1\n100\n% of words\n(log scale)\nFrench\n0.85%\n1.38%\n0.87%\n2.34%\nCroatian\n0.4%\n0.65%\n0.76%\n2.37%\nHungarian\n0.52%\n0.81%\n0.86%\n2.27%\nIndonesian\n0.86%\n1.56%\n1.1%\n2.36%\n0\n1\n100\n% of words\n(log scale)\nItalian\n0.8%\n1.36%\n0.85%\n1.85%\nKorean\n0.33%\n0.65%\n0.58%\n1.38%\nLithuanian\n0.5%\n0.84%\n0.96%\n2.28%\nMalay\n0.9%\n1.52%\n1.32%\n0.56%\n0\n1\n100\n% of words\n(log scale)\nDutch\n0.82%\n1.2%\n1.09%\n1.37%\nNorwegian\n0.84%\n1.29%\n0.93%\n4.46%\nPortuguese\n0.81%\n1.35%\n0.91%\n1.78%\nRomanian\n0.74%\n1.21%\n0.94%\n0.88%\n0\n1\n100\n% of words\n(log scale)\nRussian\n0.56%\n0.87%\n0.93%\n0.88%\nSwedish\n0.7%\n1.06%\n0.98%\n0.73%\nTagalog\n0.67%\n1.03%\n0.92%\n0.79%\nTurkish\n0.54%\n0.95%\n0.89%\n3.33%\n0\n25\n50\n75\n100\n% Stability (bucketed)\n0\n1\n100\n% of words\n(log scale)\nUkrainian\n0.55%\n0.87%\n0.91%\n3.33%\n0\n25\n50\n75\n100\n% Stability (bucketed)\nVietnamese\n2.17%\n3.14%\n2.46%\n2.62%\nWikipedia - w2v downsamples\nWikipedia - w2v random seeds\nWikipedia - GloVe downsamples\nBible - w2v random seeds\nFigure 2: Percentage of words that occur in each stability bucket for four different methods, three on Wikipedia\nand one on the Bible. The 26 languages in common are shown here. The average stability for each method is\nshown on the individual graphs.\nbution between 25% and 75%, and a sharp drop\nat high stability. This indicates that the conclu-\nsions from prior work on English apply to other\nlanguages as well. In particular, it means that any\nwork that uses embeddings to study a language\nshould train multiple embedding spaces to ensure\nrobust ﬁndings.\nSome languages have substantially more stable\nembeddings than others. Comparing GloVe down-\nsamples on Wikipedia, Vietnamese has the most\nstable embeddings (avg. 2.46%), while Korean has\nthe least stable embeddings (avg. 0.58%). The plot\nfor Vietnamese has a different trend than many of\nthe other plots in Figure 2. Vietnamese is the only\nAustro-Asiatic language in our dataset, so there\ncould be multiple distinctives that are related to\nit exhibiting different patterns than the other lan-\nguages.\nFinally, varying the training algorithm has a\nsmaller impact than changing the dataset. Keeping\n0\n25\n50\n75\n100\n% Stability (bucketed)\n10\n2\n10\n1\n100\n101\n102\n% of words (log scale)\nElberfelder 1871\nElberfelder 1905\nElberfelder 1905\nGrunewalder\nLuther 1545\nLetztehand\nLuther 1545\nLuther 1912\nNeue\nPattloch\nSchlacter\nTafelbibel\nTextbibel\nZuercher\n(a) German\n0\n25\n50\n75\n100\n% Stability (bucketed)\n10\n2\n10\n1\n100\n101\n102\n% of words (log scale)\nBonnet\nCrampon\nDarby\nDavid Martin\nJerusalem 2004\nKing James\nLouis Segond\nOstervald 1867\nParole de Vie\nPerret\nPirot Clamer\n(b) French\nFigure 3: Percentage of words that occur in each stabil-\nity bucket for different Bible translations.\nthe dataset ﬁxed (Wikipedia) and varying the algo-\nrithm, we see similar trends. Keeping the algorithm\nﬁxed (w2v random seeds) and varying the dataset,\nwe often see substantial shifts. This means that in\norder to compare languages we need to carefully\ncontrol for the content of the corpus (which the\nBible data allows us to do). While the Bible is too\nsmall to support downsampling, these results on\nWikipedia suggest that experiments varying the ran-\ndom seed lead to similar variations to experiments\nvarying the data sample.\nTo conﬁrm this ﬁnding, we consider two lan-\nguages with multiple Bible translations: German\nand French.\nWe average stability across ﬁve\nword2vec embeddings using ﬁve random seeds on\none downsampled corpus. The downsampled cor-\npus is 100,000 sentences, randomly sampled. Fig-\nure 3 shows the stability patterns for each. The\nresults are very consistent, indicating that varia-\ntions in translator behavior do not impact stability\nthe way shifting from one corpus to another does.\nThe largest shift is for the French Parole de Vie\ntranslation (top line in yellow in Figure 3b), which\nintentionally uses simpler, everyday language. For\nfurther experiments on languages with multiple\nBible translation, we choose the Bible translation\nwith the highest average stability.\nIt is difﬁcult to infer more from these ﬁgures\nalone. In the next section, we use regression mod-\neling to identify patterns in the results. Based on\nthe observations above, we use results from GloVe\nacross ﬁve downsampled corpora for Wikipedia,\nand results across ﬁve random seeds for the Bible.\n5\nRegression Modeling\nWe now explore linguistic factors that correlate\nwith stability. To draw conclusions about speciﬁc\nlinguistic features, we use a ridge regression model\n(Hoerl and Kennard, 1970)10 to predict the average\nstability of all words in a language given features\nreﬂecting language properties. Regression models\nhave previously been used to measure the impact\nof individual features (Singh et al., 2016). Ridge\nregression regularizes the magnitude of the model\nweights, producing a more interpretable model than\nnon-regularized linear regression. We experiment\nwith different regularization strengths and use the\nbest-performing value (α = 10).11 We choose to\nuse a linear model here because of its interpretabil-\nity. While more complicated models might yield\nadditional insight, we show that there are interest-\ning connections to be drawn from a linear model.\n5.1\nModel Input and Output\nOur model takes linguistic features of a language as\ninput and predicts stability as output. Since WALS\nproperties are categorical, we turn each property\ninto a set of binary features. If a particular language\ndoes not have a known value for a given property,\nthen all of these features are marked zero.\nIn order to draw out important correlations be-\ntween linguistic features and stability, we ﬁlter the\nlanguages and WALS properties that we consider.\nWe only include languages that have at least 25%\nof all WALS properties. Then, we only consider\nWALS properties that cover at least 25% of the\nﬁltered languages. We remove all WALS proper-\nties that do not have at least two features that each\ninclude at least ﬁve languages. Note that because\nall of our input features are binary, all weights are\neasily comparable. After this ﬁltering, we end up\n10Run using the Python package sklearn.linear.\nmodel.Ridge (Pedregosa et al., 2011) with default parame-\nters except α = 10.\n11We run leave-one-language-out cross-validation, de-\nscribed in Section 5.2, using the α values of 0.0001, 0.001,\n0.01, 0.1, 1, 10, 100, and 1000, choosing the α value with the\nlowest average absolute error.\nwith 37 languages,12 and 97 WALS properties.\nWe also group highly correlated WALS features.\nWe create the groupings by combining features\nwith a Pearson correlation greater than 0.8. A fea-\nture is included in a particular grouping if it corre-\nlates highly with any of the features already in the\ngroup. Each grouped feature is marked as one if\nany of the included features are marked as one.\nFor each model, we bootstrap over the input fea-\ntures 1, 000 times, allowing us to calculate standard\nerror for the R2 score and the model weights. Cal-\nculating signiﬁcance for each feature allows us to\ndiscard highly variable weights and focus on fea-\ntures that consistently contribute to the regression\nmodel, giving us more conﬁdence in the results.\nThe output of our model is the average stability\nof a language, which is calculated by averaging to-\ngether the stability of all of the words in a language.\nIf a language is present in both corpora, we average\nthe stabilities from the two corpora.\n5.2\nEvaluation\nWe evaluate our model in two ways. First, we\nmeasure goodness of ﬁt using the coefﬁcient of\ndetermination R2.13 This measures how much vari-\nance in the dependent variable y (average stability)\nis captured by the independent variables x (WALS\nproperties). A model that always predicts the ex-\npected value of y, regardless of the input features,\nwill have an R2 score of 0. The highest possible\nR2 score is 1, and R2 can be negative. Second,\nin addition to the R2 score, we run leave-one-out\ncross-validation across all languages, and report ab-\nsolute error on the left-out language. We compare\nthis to a baseline of choosing the average stability\nover all training languages.\nWe use the individual feature weights to measure\nhow much a particular feature contributes to the\noverall model. When reporting weights, we train\nthe model using all 37 languages. Because we\nare primarily using regression modeling to learn\nassociations between certain features and stability,\nno test data are necessary. The emphasis is on the\nmodel itself and the feature weights it learns, not\n12Bengali, Bulgarian, Cherokee, Comanche, English, Esto-\nnian, Finnish, Haitian, Haitian Creole, Hebrew, Hindi, Hmong\nNjua, Hungarian, Indonesian, Italian, Japanese, Korean, Latin,\nLatvian, Linda, Lithuanian, Ma’di, Mam, Mandarin, Maybrat,\nNorwegian, Persian, Pohnpeian, Polish, Portuguese, Russian,\nSomali, Spanish, Swedish, Thai, Turkish, Ukrainian, Viet-\nnamese\n13Measured using the Python package\nsklearn.linear_model.Ridge.score.\n0\n1\n2\n3\n% Average Stability\nCase suffixes\n(9 languages)\nNo case affixes\nor adpositional clitics\n(5 languages)\n(a) Position of Case Afﬁxes\n0\n1\n2\n3\n% Average Stability\nSuffixing grouping\n(24 languages)\nWeakly suffixing\n(5 languages)\nLittle affixation\n(5 languages)\n(b) Preﬁxing vs. Sufﬁxing in Inﬂectional Morphology\n0\n1\n2\n3\n% Average Stability\nSuffixing Grouping\n(24 languages)\nMixed Type\n(5 languages)\n(c) Position of Tense-Aspect Afﬁxes\nFigure 4: Afﬁxing properties compared using box-and-\nwhisker plots.\non the model’s performance on a task.\n6\nResults and Discussion\nOur regression model has a high R2 score of\n0.96 ± 0.00, indicating that the model ﬁts the data\nwell. Signiﬁcant weights with the highest magni-\ntude are shown in Table 3. Running leave-one-out\ncross-validation across all languages, we get an\naverage absolute error of 0.62 ± 0.53.14 For com-\nparison, using the average stability gives an average\nabsolute error of 0.86 ± 0.55. (A two sample t-test\ncomparison gives a p-value of 0.060.)\nTable 4 breaks down the regression results by\nbroad WALS category, listing both the number of\nbinary features per category, as well as the average\nmagnitude of weights for features in that category.\nThe two most important groups of features are\nNominal Categories and Verbal Categories. Both\nof these categories have a large number of features\nand a high average magnitude. While the Lexicon\ncategory has a high average magnitude, it contains\n14Cross-validation has an average R2 score of 0.92 on the\ntraining data.\nCat.\nWALS Attribute\nWeight\nVC,\nM\nSufﬁxing Grouping:\n·Preﬁxing vs. Sufﬁxing in Inﬂectional Morphology: Strongly Sufﬁxing;\n·Position of Tense-Aspect Afﬁxes: Tense-aspect sufﬁxes\n−0.14 ± 0.0\nL\nHand and Arm: Different\n−0.11 ± 0.0\nCS\nRelativization on Obliques: Gap\n−0.10 ± 0.0\nVC\nOverlap between Situational & Epistemic Modal Marking: Overlap for both possibility & neces-\nsity\n−0.09 ± 0.0\nNC\nOrdinal Numerals: First, second, three-th\n−0.08 ± 0.0\nNC\nComitatives and Instrumentals: Differentiation\n−0.08 ± 0.0\nP\nRhythm Types: Trochaic\n−0.08 ± 0.0\nWO\nOrder of Adjective and Noun: Adjective-Noun\n−0.07 ± 0.0\nWO\nOrder of Adposition and Noun Phrase: Postpositions\n−0.07 ± 0.0\nNC\nNo Gender Grouping:\n· Systems of Gender Assignment: No gender;\n· Sex-based and Non-sex-based Gender Systems: No gender;\n· Gender Distinctions in Independent Personal Pronouns: No gender distinctions;\n· Number of Genders: None\n0.05 ± 0.0\nP\nVoicing and Gaps in Plosive Systems: Other\n0.06 ± 0.0\nM\nPreﬁxing vs. Sufﬁxing in Inﬂectional Morphology: Little afﬁxation\n0.06 ± 0.0\nCS\n‘Want’ Complement Subjects: Subject is expressed overtly\n0.06 ± 0.0\nVC\nThe Morphological Imperative: No second-person imperatives\n0.06 ± 0.0\nCS\nPurpose Clauses: Balanced\n0.06 ± 0.0\nWO\nPrepositions Grouping:\n·Order of Adposition and Noun Phrase: Prepositions;\n·Relationship between the Order of Object and Verb and the Order of Adposition and Noun Phrase:\nVO and Prepositions\n0.06 ± 0.0\nWO\nOrder of Demonstrative and Noun: Noun-Demonstrative\n0.07 ± 0.0\nNC\nPosition of Case Afﬁxes: No case afﬁxes or adpositional clitics\n0.11 ± 0.0\nTable 3: Weights with the highest magnitude in the regression model. Negative weights correspond with low\nstability, and positive weights correspond with high stability.\nWALS Category\nNum.\nAvg.\nFeatures\nMagnitude\nSimple Clauses (SC)\n30\n0.019\nNominal Syntax (NS)\n2\n0.021\nOther (O)\n2\n0.023\nComplex Sentences (CS)\n11\n0.028\nMorphology (M)\n18\n0.031\nWord Order (WO)\n32\n0.031\nPhonology (P)\n21\n0.032\nNominal Categories (NC)\n40\n0.036\nVerbal Categories (VC)\n27\n0.036\nLexicon (L)\n6\n0.039\nTable 4: Number of binary features and average magni-\ntude of weights in the regression model for different\nWALS categories. Grouped features are included in\neach category that they cover.\nvery few features. To further explore these results,\nwe highlight a few WALS property in more detail.\nSufﬁxes and preﬁxes. Table 3 shows that three\nof the top features are related to afﬁxes (sufﬁxes\nand preﬁxes). Speciﬁcally, three main properties\ndeal with afﬁxes: Position of Case Afﬁxes (Dryer,\n2013a), Preﬁxing vs. Sufﬁxing in Inﬂectional Mor-\nphology (Dryer, 2013c), and Position of Tense-\nAspect Afﬁxes (Dryer, 2013b). Distributions of\nthese features in the 37 languages used for the re-\ngression model are shown in Figure 4 (categories\nwith fewer than ﬁve languages are not shown).\nFor all three of these properties, more afﬁxing is\nassociated with lower stability. When considering\nword embeddings, this result makes intuitive sense.\nAfﬁxes cause there to be many different word vari-\nations (e.g., walk, walked, walking, walker), which\nmay not be handled consistently by the embedding\nalgorithm, leading to lower average stability.\nGendered Languages. Table 3 also highlights a\ngrouping of WALS properties related to whether\na language is gendered or not. Four WALS prop-\nerties are relevant to this: Systems of Gender As-\nsignment (Corbett, 2013c), Sex-based and Non-\nsex-based Gender Systems (Corbett, 2013b), Gen-\nder Distinctions in Independent Personal Pronouns\n(Siewierska, 2013), and Number of Genders (Cor-\nbett, 2013a). In general, a language is considered\nto have a gender system if different parts-of-speech\nare required to agree in gender (as opposed to sim-\nply having gendered nouns). Distributions of these\nfeatures are shown in Figure 5.\nFor all of these properties, languages with no\ngender system tend to have higher average stability.\nAgain, this result makes sense in the context of\n0\n1\n2\n3\n% Average Stability\nGender Grouping\n(9 languages)\nNo Gender Grouping\n(12 languages)\n(a) Systems of Gender Assignment; Sex-based and Non-\nsex-based Gender Systems (Gender Grouping: No gender;\nSex-based)\n0\n1\n2\n3\n% Average Stability\n3rd person singular only\n(7 languages)\nNo Gender Grouping\n(12 languages)\n(b) Gender Distinctions in Independent Personal Pronouns\n0\n1\n2\n3\n% Average Stability\nTwo\n(5 languages)\nNo Gender Grouping\n(12 languages)\n(c) Number of Genders\nFigure 5: Gender properties compared using box-and-\nwhisker plots. Note, the 12 languages with “No Gender\nGrouping” are not the same across the three plots.\nword embeddings. Languages with gender systems\nwill have more word forms (e.g., both male and\nfemale word forms), which may not be handled\nconsistently by the embedding algorithm.\n7\nConclusion\nIn this paper, we considered how stability varies\nacross different languages. This work is important\nbecause algorithms such as GloVe and word2vec\ncontinue to be effective methods in a wide vari-\nety of scenarios (Arora et al., 2020), particularly\nthe computational humanities and languages where\nlarge corpora are not available. We studied the rela-\ntionship between linguistic properties and stability,\nsomething that has been previously understudied.\nWe drew out several aspects of this relationship,\nincluding that languages with more afﬁxing tend to\nhave less stable embeddings, and languages with\nno gender systems tend to have more stable embed-\ndings. These insights can be used in future work\nto inform the design of embeddings in many lan-\nguages. For example, this work suggests that future\nembedding space designs need to take into account\ngendered words and morphologically rich words\nwith afﬁxes.\n8\nAcknowledgements\nThis material is based in part upon work sup-\nported by the National Science Foundation (grant\n#1815291) and by the John Templeton Founda-\ntion (grant #61156). Any opinions, ﬁndings, and\nconclusions or recommendations expressed in this\nmaterial are those of the author and do not nec-\nessarily reﬂect the views of the National Science\nFoundation or John Templeton Foundation.\nReferences\nAbdul Z Abdulrahim. 2019. Ideological drifts in the\nUS constitution: Detecting areas of contention with\nmodels of semantic change. In NeurIPS Joint Work-\nshop on AI for Social Good, Vancouver, Canada.\nRami Al-Rfou’, Bryan Perozzi, and Steven Skiena.\n2013.\nPolyglot: Distributed word representations\nfor multilingual NLP. In Proceedings of the Seven-\nteenth Conference on Computational Natural Lan-\nguage Learning, pages 183–192, Soﬁa, Bulgaria.\nAssociation for Computational Linguistics.\nMaria Antoniak and David Mimno. 2018. Evaluating\nthe stability of embedding-based word similarities.\nTransactions of the Association for Computational\nLinguistics, 6:107–119.\nSimran Arora, Avner May, Jian Zhang, and Christopher\nRé. 2020. Contextual embeddings: When are they\nworth it? In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2650–2663, Online. Association for Computa-\ntional Linguistics.\nMikel Artetxe, Gorka Labaka, Iñigo Lopez-Gazpio,\nand Eneko Agirre. 2018.\nUncovering divergent\nlinguistic information in word embeddings with\nlessons for intrinsic and extrinsic evaluation. In Pro-\nceedings of the 22nd Conference on Computational\nNatural Language Learning, pages 282–291, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nMarco\nBaroni,\nGeorgiana\nDinu,\nand\nGermán\nKruszewski. 2014.\nDon’t count, predict!\nA\nsystematic\ncomparison\nof\ncontext-counting\nvs.\ncontext-predicting semantic vectors. In Proceedings\nof the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong\nPapers),\npages 238–247,\nBaltimore,\nMaryland.\nAssociation for Computational Linguistics.\nSantwana Chimalamarri, Dinkar Sitaram, and Ashritha\nJain. 2020. Morphological segmentation to improve\ncrosslingual word embeddings for low resource lan-\nguages.\nACM Transactions on Asian and Low-\nResource Language Information Processing (TAL-\nLIP), 19(5):1–15.\nBilly Chiu, Anna Korhonen, and Sampo Pyysalo. 2016.\nIntrinsic evaluation of word vectors fails to predict\nextrinsic performance.\nIn Proceedings of the 1st\nWorkshop on Evaluating Vector-Space Representa-\ntions for NLP, pages 1–6, Berlin, Germany. Asso-\nciation for Computational Linguistics.\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011.\nNatural language processing (almost) from\nscratch.\nJournal of Machine Learning Research,\n12(Aug):2493–2537.\nGreville G. Corbett. 2013a. Number of genders. In\nMatthew S. Dryer and Martin Haspelmath, editors,\nThe World Atlas of Language Structures Online.\nMax Planck Institute for Evolutionary Anthropol-\nogy, Leipzig.\nGreville G. Corbett. 2013b.\nSex-based and non-sex-\nbased gender systems.\nIn Matthew S. Dryer and\nMartin Haspelmath, editors, The World Atlas of Lan-\nguage Structures Online. Max Planck Institute for\nEvolutionary Anthropology, Leipzig.\nGreville G. Corbett. 2013c. Systems of gender assign-\nment. In Matthew S. Dryer and Martin Haspelmath,\neditors, The World Atlas of Language Structures On-\nline. Max Planck Institute for Evolutionary Anthro-\npology, Leipzig.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMatthew S. Dryer. 2013a.\nPosition of case afﬁxes.\nIn Matthew S. Dryer and Martin Haspelmath, ed-\nitors, The World Atlas of Language Structures On-\nline. Max Planck Institute for Evolutionary Anthro-\npology, Leipzig.\nMatthew S. Dryer. 2013b. Position of tense-aspect af-\nﬁxes. In Matthew S. Dryer and Martin Haspelmath,\neditors, The World Atlas of Language Structures On-\nline. Max Planck Institute for Evolutionary Anthro-\npology, Leipzig.\nMatthew S. Dryer. 2013c.\nPreﬁxing vs. sufﬁxing in\ninﬂectional morphology. In Matthew S. Dryer and\nMartin Haspelmath, editors, The World Atlas of Lan-\nguage Structures Online. Max Planck Institute for\nEvolutionary Anthropology, Leipzig.\nMatthew S. Dryer and Martin Haspelmath, editors.\n2013. WALS Online. Max Planck Institute for Evo-\nlutionary Anthropology, Leipzig.\nJohannes Hellrich, Sven Buechel, and Udo Hahn.\n2019.\nModeling word emotion in historical lan-\nguage: Quantity beats supposed stability in seed\nword selection.\nIn Proceedings of the 3rd Joint\nSIGHUM Workshop on Computational Linguistics\nfor Cultural Heritage, Social Sciences, Humanities\nand Literature, pages 1–11, Minneapolis, USA. As-\nsociation for Computational Linguistics.\nTom Heyman and Geert Heyman. 2019.\nCan\nprediction-based distributional semantic models pre-\ndict typicality? Quarterly Journal of Experimental\nPsychology, pages 2084–2109.\nArthur E. Hoerl and Robert W. Kennard. 1970. Ridge\nregression:\nBiased estimation for nonorthogonal\nproblems. Technometrics, 12(1):55–67.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs.\nIEEE\nTransactions on Big Data.\nIshani Joshi, Purvi Koringa, and Suman Mitra. 2019.\nWord embeddings in low resource Gujarati lan-\nguage. In 2019 International Conference on Doc-\nument Analysis and Recognition Workshops (IC-\nDARW), volume 5, pages 110–115. IEEE.\nMegan Leszczynski, Avner May, Jian Zhang, Sen Wu,\nChristopher R. Aberger, and Christopher Ré. 2020.\nUnderstanding the downstream instability of word\nembeddings. In Proceedings of Machine Learning\nand Systems 2020, MLSys 2020, Austin, TX, USA,\nMarch 2-4, 2020. mlsys.org.\nMike Maxwell and Baden Hughes. 2006. Frontiers in\nlinguistic annotation for lower-density languages. In\nProceedings of the Workshop on Frontiers in Lin-\nguistically Annotated Corpora 2006, pages 29–37,\nSydney, Australia. Association for Computational\nLinguistics.\nArya D. McCarthy, Rachel Wicks, Dylan Lewis, Aaron\nMueller, Winston Wu, Oliver Adams, Garrett Nico-\nlai, Matt Post, and David Yarowsky. 2020.\nThe\nJohns Hopkins University Bible corpus:\n1600+\ntongues for typological exploration.\nIn Proceed-\nings of the 12th Language Resources and Evaluation\nConference, pages 2884–2892, Marseille, France.\nEuropean Language Resources Association.\nTomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed rep-\nresentations of words and phrases and their compo-\nsitionality. In 27th Annual Conference on Neural\nInformation Processing Systems, pages 3111–3119,\nLake Tahoe, Nevada.\nMilad Moradi, Maedeh Dashti, and Matthias Samwald.\n2020.\nSummarization of biomedical articles us-\ning domain-speciﬁc word embeddings and graph\nranking.\nJournal of Biomedical Informatics,\n107:103452.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron\nWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-\ndre Passos, David Cournapeau, Matthieu Brucher,\nMatthieu Perrot, and Édouard Duchesnay. 2011.\nScikit-learn: Machine learning in Python. Journal\nof Machine Learning Research, 12:2825–2830.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014.\nGloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1532–1543,\nDoha,\nQatar. Association for Computational Linguistics.\nBénédicte Pierrejean and Ludovic Tanguy. 2018. To-\nwards qualitative word embeddings evaluation: Mea-\nsuring neighbors variation.\nIn Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Stu-\ndent Research Workshop, pages 32–39, New Or-\nleans, Louisiana, USA. Association for Computa-\ntional Linguistics.\nYuanyuan Qiu, Hongzheng Li, Shen Li, Yingdi Jiang,\nRenfen Hu, and Lijiao Yang. 2018. Revisiting cor-\nrelations between intrinsic and extrinsic evaluations\nof word embeddings.\nIn Chinese Computational\nLinguistics and Natural Language Processing Based\non Naturally Annotated Big Data, pages 209–221.\nSpringer.\nAnna Rogers, Aleksandr Drozd, Anna Rumshisky, and\nYoav Goldberg, editors. 2019. Proceedings of the\n3rd Workshop on Evaluating Vector Space Represen-\ntations for NLP. Association for Computational Lin-\nguistics, Minneapolis, USA.\nAnna Rogers, Shashwath Hosur Ananthakrishna, and\nAnna Rumshisky. 2018. What’s in your embedding,\nand how it predicts task performance. In Proceed-\nings of the 27th International Conference on Com-\nputational Linguistics, pages 2690–2703, Santa Fe,\nNew Mexico, USA. Association for Computational\nLinguistics.\nBianca Scarlini, Tommaso Pasini, and Roberto Nav-\nigli. 2020.\nWith more contexts comes better per-\nformance:\nContextualized sense embeddings for\nall-round word sense disambiguation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n3528–3539, Online. Association for Computational\nLinguistics.\nAnna Siewierska. 2013. Gender distinctions in inde-\npendent personal pronouns.\nIn Matthew S. Dryer\nand Martin Haspelmath, editors, The World Atlas of\nLanguage Structures Online. Max Planck Institute\nfor Evolutionary Anthropology, Leipzig.\nAbhinav Deep Singh, Poojan Mehta, Samar Husain,\nand Rajkumar Rajakrishnan. 2016.\nQuantifying\nsentence complexity based on eye-tracking mea-\nsures.\nIn Proceedings of the Workshop on Com-\nputational Linguistics for Linguistic Complexity\n(CL4LC), pages 202–212, Osaka, Japan. The COL-\nING 2016 Organizing Committee.\nNathan Stringham and Mike Izbicki. 2020.\nEvaluat-\ning word embeddings on low-resource languages. In\nProceedings of the First Workshop on Evaluation\nand Comparison of NLP Systems, pages 176–186,\nOnline. Association for Computational Linguistics.\nYulia Tsvetkov, Manaal Faruqui, Wang Ling, Brian\nMacWhinney, and Chris Dyer. 2016.\nLearning\nthe curriculum with Bayesian optimization for task-\nspeciﬁc word representation learning. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 130–139, Berlin, Germany. Association\nfor Computational Linguistics.\nShirui Wang, Wenan Zhou, and Chao Jiang. 2020a. A\nsurvey of word embeddings based on deep learning.\nComputing, 102(3):717–740.\nYuxuan Wang, Yutai Hou, Wanxiang Che, and Ting Liu.\n2020b. From static to dynamic word representations:\na survey. International Journal of Machine Learn-\ning and Cybernetics, pages 1–20.\nLaura Wendlandt, Jonathan K. Kummerfeld, and Rada\nMihalcea. 2018. Factors inﬂuencing the surprising\ninstability of word embeddings. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 2092–2102, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nYadollah Yaghoobzadeh and Hinrich Schütze. 2016.\nIntrinsic subspace evaluation of word embedding\nrepresentations.\nIn Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 236–\n246, Berlin, Germany. Association for Computa-\ntional Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Thirty-third Conference\non Neural Information Processing Systems, pages\n5754–5764, Vancouver, Canada.\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-04-30",
  "updated": "2021-09-09"
}