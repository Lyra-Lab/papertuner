{
  "id": "http://arxiv.org/abs/2004.14958v1",
  "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning",
  "authors": [
    "Mikel Artetxe",
    "Sebastian Ruder",
    "Dani Yogatama",
    "Gorka Labaka",
    "Eneko Agirre"
  ],
  "abstract": "We review motivations, definition, approaches, and methodology for\nunsupervised cross-lingual learning and call for a more rigorous position in\neach of them. An existing rationale for such research is based on the lack of\nparallel data for many of the world's languages. However, we argue that a\nscenario without any parallel data and abundant monolingual data is unrealistic\nin practice. We also discuss different training signals that have been used in\nprevious work, which depart from the pure unsupervised setting. We then\ndescribe common methodological issues in tuning and evaluation of unsupervised\ncross-lingual models and present best practices. Finally, we provide a unified\noutlook for different types of research in this area (i.e., cross-lingual word\nembeddings, deep multilingual pretraining, and unsupervised machine\ntranslation) and argue for comparable evaluation of these models.",
  "text": "A Call for More Rigor in Unsupervised Cross-lingual Learning\nMikel Artetxe†∗, Sebastian Ruder‡∗, Dani Yogatama‡, Gorka Labaka†, Eneko Agirre†\n†HiTZ Center, University of the Basque Country (UPV/EHU)\n‡DeepMind\n{mikel.artetxe,gorka.labaka,e.agirre}@ehu.eus\n{ruder,dyogatama}@google.com\nAbstract\nWe review motivations, deﬁnition, approaches,\nand methodology for unsupervised cross-\nlingual learning and call for a more rigorous\nposition in each of them. An existing rationale\nfor such research is based on the lack of par-\nallel data for many of the world’s languages.\nHowever, we argue that a scenario without any\nparallel data and abundant monolingual data is\nunrealistic in practice. We also discuss differ-\nent training signals that have been used in pre-\nvious work, which depart from the pure unsu-\npervised setting. We then describe common\nmethodological issues in tuning and evalua-\ntion of unsupervised cross-lingual models and\npresent best practices. Finally, we provide a\nuniﬁed outlook for different types of research\nin this area (i.e., cross-lingual word embed-\ndings, deep multilingual pretraining, and un-\nsupervised machine translation) and argue for\ncomparable evaluation of these models.\n1\nIntroduction\nThe study of the connection among human lan-\nguages has contributed to major discoveries in-\ncluding the evolution of languages, the reconstruc-\ntion of proto-languages, and an understanding of\nlanguage universals (Eco and Fentress, 1995). In\nnatural language processing, the main promise of\nmultilingual learning is to bridge the digital lan-\nguage divide, to enable access to information and\ntechnology for the world’s 6,900 languages (Ruder\net al., 2019). For the purpose of this paper, we\ndeﬁne “multilingual learning” as learning a com-\nmon model for two or more languages from raw\ntext, without any downstream task labels. Common\nuse cases include translation as well as pretraining\nmultilingual representations. We will use the term\ninterchangeably with “cross-lingual learning”.\n∗Equal contribution.\nRecent work in this direction has increasingly\nfocused on purely unsupervised cross-lingual learn-\ning (UCL)—i.e., cross-lingual learning without any\nparallel signal across the languages. We provide\nan overview in §2. Such work has been motivated\nby the apparent dearth of parallel data for most\nof the world’s languages. In particular, previous\nwork has noted that “data encoding cross-lingual\nequivalence is often expensive to obtain” (Zhang\net al., 2017a) whereas “monolingual data is much\neasier to ﬁnd” (Lample et al., 2018a). Overall,\nit has been argued that unsupervised cross-lingual\nlearning “opens up opportunities for the processing\nof extremely low-resource languages and domains\nthat lack parallel data completely” (Zhang et al.,\n2017a).\nWe challenge this narrative and argue that the\nscenario of no parallel data and sufﬁcient monolin-\ngual data is unrealistic and not reﬂected in the real\nworld (§3.1). Nevertheless, UCL is an important\nresearch direction and we advocate for its study\nbased on an inherent scientiﬁc interest (to better\nunderstand and make progress on general language\nunderstanding), usefulness as a lab setting, and\nsimplicity (§3.2).\nUnsupervised cross-lingual learning permits no\nsupervisory signal by deﬁnition. However, pre-\nvious work implicitly includes monolingual and\ncross-lingual signals that constitute a departure\nfrom the pure setting. We review existing train-\ning signals as well as other signals that may be\nof interest for future study (§4). We then discuss\nmethodological issues in UCL (e.g., validation, hy-\nperparameter tuning) and propose best evaluation\npractices (§5). Finally, we provide a uniﬁed out-\nlook of established research areas (cross-lingual\nword embeddings, deep multilingual models and\nunsupervised machine translation) in UCL (§6),\nand conclude with a summary of our recommenda-\ntions (§7).\narXiv:2004.14958v1  [cs.CL]  30 Apr 2020\n2\nBackground\nIn this section, we brieﬂy review existing work\non UCL, covering cross-lingual word embeddings\n(§2.1), deep multilingual pre-training (§2.2), and\nunsupervised machine translation (§2.3).\n2.1\nCross-lingual word embeddings\nCross-lingual word embedding methods tradition-\nally relied on parallel corpora (Gouws et al., 2015;\nLuong et al., 2015). Nonetheless, the amount of\nsupervision required was greatly reduced via cross-\nlingual word embedding mappings, which work by\nseparately learning monolingual word embeddings\nin each language and mapping them into a shared\nspace through a linear transformation. Early work\nrequired a bilingual dictionary to learn such a trans-\nformation (Mikolov et al., 2013a; Faruqui and Dyer,\n2014). This requirement was later reduced with\nself-learning (Artetxe et al., 2017), and ultimately\nremoved via unsupervised initialization heuristics\n(Artetxe et al., 2018a; Hoshen and Wolf, 2018) and\nadversarial learning (Zhang et al., 2017a; Conneau\net al., 2018a). Finally, several recent methods have\nformulated cross-lingual embedding alignment as\nan optimal transport problem (Zhang et al., 2017b;\nGrave et al., 2019; Alvarez-Melis and Jaakkola,\n2018).\n2.2\nDeep multilingual pretraining\nFollowing the success in learning shallow word em-\nbeddings (Mikolov et al., 2013b; Pennington et al.,\n2014), there has been an increasing interest in learn-\ning contextual word representations (Dai and Le,\n2015; Peters et al., 2018; Howard and Ruder, 2018).\nRecent research has been dominated by BERT (De-\nvlin et al., 2019), which uses a bidirectional trans-\nformer encoder trained on masked language mod-\neling and next sentence prediction, which led to\nimpressive gains on various downstream tasks.\nWhile the above approaches are limited to a sin-\ngle language, a multilingual extension of BERT\n(mBERT) has been shown to also be effective at\nlearning cross-lingual representations in an unsu-\npervised way.1 The main idea is to combine mono-\nlingual corpora in different languages, upsampling\nthose with less data, and training a regular BERT\nmodel on the combined data. Conneau and Lam-\nple (2019) follow a similar approach but perform a\nmore thorough evaluation and report substantially\n1https://github.com/google-research/\nbert/blob/master/multilingual.md\nstronger results,2 which was further scaled up by\nConneau et al. (2019). Several recent studies (Wu\nand Dredze, 2019; Pires et al., 2019; Artetxe et al.,\n2020b; Wu et al., 2019) analyze mBERT to get a\nbetter understanding of its capabilities.\n2.3\nUnsupervised machine translation\nEarly attempts to build machine translation systems\nusing monolingual data alone go back to statistical\ndecipherment (Ravi and Knight, 2011; Dou and\nKnight, 2012, 2013). However, this approach was\nonly shown to work in limited settings, and the ﬁrst\nconvincing results on standard benchmarks were\nachieved by Artetxe et al. (2018c) and Lample et al.\n(2018a) on unsupervised Neural Machine Transla-\ntion (NMT). Both approaches rely on cross-lingual\nword embeddings to initialize a shared encoder,\nand train it in conjunction with the decoder using\na combination of denoising autoencoding, back-\ntranslation, and optionally adversarial learning.\nSubsequent work adapted these principles to un-\nsupervised phrase-based Statistical Machine Trans-\nlation (SMT), obtaining large improvements over\nthe original NMT-based systems (Lample et al.,\n2018b; Artetxe et al., 2018b). This alternative ap-\nproach uses cross-lingual n-gram embeddings to\nbuild an initial phrase table, which is combined\nwith an n-gram language model and a distortion\nmodel, and further reﬁned through iterative back-\ntranslation. There have been several follow-up\nattempts to combine NMT and SMT based ap-\nproaches (Marie and Fujita, 2018; Ren et al., 2019;\nArtetxe et al., 2019b). More recently, Conneau and\nLample (2019), Song et al. (2019) and Liu et al.\n(2020) obtain strong results using deep multilingual\npretraining rather than cross-lingual word embed-\ndings to initialize unsupervised NMT systems.\n3\nMotivating fully unsupervised learning\nIn this section, we challenge the narrative of moti-\nvating UCL based on a lack of parallel resources.\nWe argue that the strict unsupervised scenario can-\nnot be motivated from an immediate practical per-\nspective, and elucidate what we believe should be\nthe true goals of this research direction.\n2The full version of their model (XLM) requires parallel\ncorpora for their translation language modeling objective, but\nthe authors also explore an unsupervised variant using masked\nlanguage modeling alone.\n3.1\nHow practical is the strict unsupervised\nscenario?\nMonolingual resources subsume parallel resources.\nFor instance, each side of a parallel corpus effec-\ntively serves as a monolingual corpus. From this ar-\ngument, it follows that monolingual data is cheaper\nto obtain than parallel data, so unsupervised cross-\nlingual learning should in principle be more gener-\nally applicable than supervised learning.\nHowever, we argue that the common claim that\nthe requirement for parallel data “may not be met\nfor many language pairs in the real world” (Xu\net al., 2018) is largely inaccurate. For instance,\nthe JW300 parallel corpus covers 343 languages\nwith around 100,000 parallel sentences per lan-\nguage pair on average (Agi´c and Vuli´c, 2019), and\nthe multilingual Bible corpus collected by Mayer\nand Cysouw (2014) covers 837 language varieties\n(each with a unique ISO 639-3 code). Moreover,\nthe PanLex project aims to collect multilingual lex-\nica for all human languages in the world, and al-\nready covers 6,854 language varieties with at least\n20 lexemes, 2,364 with at least 200 lexemes, and\n369 with at least 2,000 lexemes (Kamholz et al.,\n2014). While 20 or 200 lexemes might seem insuf-\nﬁcient, weakly supervised cross-lingual word em-\nbedding methods already proved effective with as\nlittle as 25 word pairs (Artetxe et al., 2017). More\nrecent methods have focused on completely remov-\ning this weak supervision (Conneau et al., 2018a;\nArtetxe et al., 2018a), which can hardly be justiﬁed\nfrom a practical perspective given the existence\nof such resources and additional training signals\nstemming from a (partially) shared script (§4.2).\nFinally, given the availability of sufﬁcient monolin-\ngual data, noisy parallel data can often be obtained\nby mining bitext (Schwenk et al., 2019a,b).\nIn addition, large monolingual data is difﬁcult\nto obtain for low-resource languages. For instance,\nrecent work on cross-lingual word embeddings has\nmostly used Wikipedia as its source for monolin-\ngual corpora (Gouws et al., 2015; Vuli´c and Korho-\nnen, 2016; Conneau et al., 2018a). However, as of\nNovember 2019, Wikipedia exists in only 307 lan-\nguages3 of which nearly half have less than 10,000\narticles. While one could hope to overcome this by\ntaking the entire web as a corpus, as facilitated by\nCommon Crawl4 and similar initiatives, this is not\n3https://en.wikipedia.org/wiki/List_\nof_Wikipedias\n4https://commoncrawl.org/\nalways feasible for low-resource languages. First,\nthe presence of less resourced languages on the web\nis very limited, with only a few hundred languages\nrecognized as being used in websites.5 This situa-\ntion is further complicated by the limited coverage\nof existing tools such as language detectors (Buck\net al., 2014; Grave et al., 2018), which only cover\na few hundred languages. Alternatively, speech\ncould also serve as a source of monolingual data\n(e.g., by recording public radio stations). However,\nthis is an unexplored direction within UCL, and\ncollecting, processing and effectively capitalizing\non speech data is far from trivial, particularly for\nlow-resource languages.\nAll in all, we conclude that the alleged scenario\ninvolving no parallel data and sufﬁcient monolin-\ngual data is not met in the real world in the terms\nexplored by recent UCL research. Needless to say,\neffectively exploiting unlabeled data is important\nin any low-resource setting. However, refusing to\nuse an informative training signal—which paral-\nlel data is—when it does indeed exist, cannot be\njustiﬁed from a practical perspective if one’s goal\nis to build the strongest possible model. For this\nreason, we believe that semi-supervised learning\nis a more suitable paradigm for truly low-resource\nlanguages, and UCL should not be motivated from\nan immediate practical perspective.\n3.2\nA scientiﬁc motivation\nDespite not being an entirely realistic setup, we\nbelieve that UCL is an important research direction\nfor the reasons we discuss below.\nInherent scientiﬁc interest.\nThe extent to which\ntwo languages can be aligned based on independent\nsamples—without any cross-lingual signal—is an\nopen and scientiﬁcally relevant problem per se. In\nfact, it is not entirely obvious that UCL should be\npossible at all, as humans would certainly strug-\ngle to align two unknown languages without any\ngrounding. Exploring the limits of UCL could help\nto understand the limits of the principles that the\ncorresponding methods are based on, such as the\ndistributional hypothesis. Moreover, this research\nline could bring new insights into the properties\nand inner workings of both language acquisition\nand the underlying computational models that ulti-\nmately make UCL possible. Finally, such methods\nmay be useful in areas where supervision is impos-\n5https://w3techs.com/technologies/\noverview/content_language\nsible to obtain, such as when dealing with unknown\nor even non-human languages.\nUseful as a lab setting.\nThe strict unsupervised\nscenario, although not practical, allows us to isolate\nand better study the use of monolingual corpora for\ncross-lingual learning. We believe lessons learned\nin this setting can be useful in the more practical\nsemi-supervised scenario. In a similar vein, mono-\nlingual language models, although hardly useful on\ntheir own, have contributed to large improvements\nin other tasks. From a research methodology per-\nspective, unsupervised systems also set a competi-\ntive baseline, which any semi-supervised method\nshould improve upon.\nSimplicity as a value.\nAs we discussed previ-\nously, refusing to use an informative training signal\nwhen it does exist can hardly be beneﬁcial, so we\nshould not expect UCL to perform better than semi-\nsupervised learning. However, simplicity is a value\nin its own right. Unsupervised approaches could be\npreferable to their semi-supervised counterparts if\nthe performance gap between them is small enough.\nFor instance, unsupervised cross-lingual embed-\nding methods have been reported to be competitive\nwith their semi-supervised counterparts in certain\nsettings (Glavaš et al., 2019), while being easier to\nuse in the sense that they do not require a bilingual\ndictionary.\n4\nWhat does unsupervised mean?\nIn its most general sense, unsupervised cross-\nlingual learning can be seen as referring to any\nmethod relying exclusively on monolingual text\ndata in two or more languages. However, there\nare different training signals—stemming from com-\nmon assumptions and varying amounts of linguistic\nknowledge—that one can potentially exploit under\nsuch a regime. This has led to an inconsistent use\nof this term in the literature. In this section, we\ncategorize different training signals available both\nfrom a monolingual and a cross-lingual perspec-\ntive and discuss additional scenarios enabled by\nmultiple languages.\n4.1\nMonolingual training signals\nFrom a computational perspective, text is modeled\nas a sequence of discrete symbols. In UCL, the\ntraining data consists of a set of such sequences in\neach of the languages. In principle, without any\nknowledge about the languages, one would have no\nprior information of the nature of such sequences\nor the possible relations between them. In prac-\ntice, however, sets of sequences are assumed to\nbe independent, and existing work differs whether\nthey assume document-level sequences (Conneau\nand Lample, 2019) or sentence-level sequences\n(Artetxe et al., 2018c; Lample et al., 2018a).\nNature of atomic symbols.\nA more important\nconsideration is the nature of the atomic symbols\nin such sequences.\nTo the best of our knowl-\nedge, previous work assumes some form of word\nsegmentation or tokenization (e.g., splitting by\nwhitespaces or punctuation marks). Early work\non cross-lingual word embeddings considered such\ntokens as atomic units. However, more recent work\n(Hoshen and Wolf, 2018; Glavaš et al., 2019) has\nprimarily used fastText embeddings (Bojanowski\net al., 2017) which incorporate subword informa-\ntion into the embedding learning, although the vo-\ncabulary is still deﬁned at the token level. In ad-\ndition, there have also been approaches that incor-\nporate character-level information into the align-\nment learning itself (Heyman et al., 2017; Riley and\nGildea, 2018). In contrast, most work on contextual\nword embeddings and unsupervised machine trans-\nlation operates with a subword vocabulary (Devlin\net al., 2019; Conneau and Lample, 2019).\nWhile the above distinction might seem irrel-\nevant from a practical perspective, we think that\nit is important from a more fundamental point of\nview (e.g. in relation to the distributional hypoth-\nesis as discussed in §3.2). Moreover, some of the\nunderlying assumptions might not generalize to dif-\nferent writing systems (e.g. logographic instead\nof alphabetic). For instance, subword tokenization\nhas been shown to perform poorly on reduplicated\nwords (Vania and Lopez, 2017). In relation to that,\none could also consider the text in each language as\na stream of discrete character-like symbols without\nany notion of tokenization. Such a tabula rasa ap-\nproach is potentially applicable to any arbitrary lan-\nguage, even when its writing system is not known,\nbut has so far only been explored for a limited num-\nber of languages in a monolingual setting (Hahn\nand Baroni, 2019).\nLinguistic information.\nFinally, one can exploit\nadditional linguistic knowledge through linguistic\nanalysis such as lemmatization, part-of-speech tag-\nging, or syntactic parsing. For instance, before\nthe advent of unsupervised NMT, statistical deci-\npherment was already shown to beneﬁt from incor-\nporating syntactic dependency relations (Dou and\nKnight, 2013). For other tasks such as unsuper-\nvised POS tagging (Snyder et al., 2008), monolin-\ngual tag dictionaries have been used. While such\napproaches could still be considered unsupervised\nfrom a cross-lingual perspective, we argue that the\ninterest of this research direction is greatly limited\nby two factors: (i) from a theoretical perspective,\nit assumes some fundamental knowledge that is\nnot directly inferred from the raw monolingual cor-\npora; and (ii) from a more practical perspective, it\nis not reasonable to assume that such resources are\navailable in the less resourced settings where this\nresearch direction has more potential for impact.\n4.2\nCross-lingual training signals\nPure UCL should not use any cross-lingual signal\nby deﬁnition. When we view text as a sequence\nof discrete atomic symbols (either characters or to-\nkens), a strict interpretation of this principle would\nconsider the set of atomic symbols in different lan-\nguages to be disjoint, without prior knowledge of\nthe relationship between them.\nNeedless to say, any form of learning requires\nmaking assumptions, as one needs some criterion\nto prefer one mapping over another. In the case\nof UCL, such assumptions stem from the struc-\ntural similarity across languages (e.g. semanti-\ncally equivalent words in different languages are\nassumed to occur in similar contexts). In practice,\nthese assumptions weaken as the distribution of\nthe datasets diverges, and some UCL models have\nbeen reported to break under a domain shift (Sø-\ngaard et al., 2018; Guzmán et al., 2019; Marchisio\net al., 2020). Similarly, approaches that leverage\nlinguistic features such as syntactic dependencies\nmay assume that these are similar across languages.\nIn addition, one can also assume that the sets\nof symbols that are used to represent different lan-\nguages have some commonalities. This departs\nfrom the strict deﬁnition of UCL above, establish-\ning some prior connections between the sets of sym-\nbols in different languages. Such an assumption\nis reasonable from a practical perspective, as there\nare a few scripts (e.g. Latin, Arabic or Cyrillic) that\ncover a large fraction of languages. Moreover, even\nwhen two languages use different writing systems\nor scripts, there are often certain elements that are\nstill shared (e.g. Arabic numerals, named entities\nwritten in a foreign script, URLs, certain punctua-\ntion marks, etc.). In relation to that, several models\nhave relied on identically spelled words (Artetxe\net al., 2017; Smith et al., 2017; Søgaard et al., 2018)\nor string-level similarity across languages (Riley\nand Gildea, 2018; Artetxe et al., 2019b) as train-\ning signals. Other methods use a joint subword\nvocabulary for all languages, indirectly exploiting\nthe commonalities in their writing system (Lample\net al., 2018b; Conneau and Lample, 2019).\nHowever, past work greatly differs on the nature\nand relevance that is attributed to such a training\nsignal. The reliance on identically spelled words\nhas been considered as a weak form of supervi-\nsion in the cross-lingual word embedding literature\n(Søgaard et al., 2018; Ruder et al., 2018), and sig-\nniﬁcant effort has been put into developing strictly\nunsupervised methods that do not rely on such sig-\nnal (Conneau et al., 2018a). In contrast, the un-\nsupervised machine translation literature has not\npayed much attention to this factor, and has often\nrelied on identical words (Artetxe et al., 2018c),\nstring-level similarity (Artetxe et al., 2019b), or a\njoint subword vocabulary (Lample et al., 2018b;\nConneau and Lample, 2019) under the unsuper-\nvised umbrella. The same is true for unsupervised\ndeep multilingual pretraining, where a shared sub-\nword vocabulary has been a common component\n(Pires et al., 2019; Conneau and Lample, 2019),\nalthough recent work shows that it is not important\nto share vocabulary across languages (Artetxe et al.,\n2020b; Wu et al., 2019).\nOur position is that making assumptions on lin-\nguistics universals is acceptable and ultimately nec-\nessary for UCL. However, we believe that any con-\nnection stemming from a (partly) shared writing\nsystem belongs to a different category, and should\nbe considered a separate cross-lingual signal. Our\nrationale is that a given writing system pertains to\na speciﬁc form to encode a language, but cannot be\nconsidered to be part of the language itself.6\n4.3\nMultilinguality\nWhile most work in unsupervised cross-lingual\nlearning considers two languages at a time, there\nhave recently been some attempts to extend these\nmethods to multiple languages (Duong et al., 2017;\nChen and Cardie, 2018; Heyman et al., 2019),\nand most work on unsupervised cross-lingual pre-\ntraining is multilingual (Pires et al., 2019; Conneau\n6As a matter of fact, languages existed well before writing\nwas invented, and a given language can have different writing\nsystems or new ones can be designed.\nMonolingual signal\nCross-lingual signal\nSequence of symbols\nShared writing system\nSets of sentences/documents\nIdentical words\nTokens/subwords\nString similarity\nLinguistic analysis\nTable 1: Different types of monolingual and cross-\nlingual signals that have been used for unsupervised\ncross-lingual learning, ordered roughly from least to\nmost linguistic knowledge (top to bottom).\nand Lample, 2019). When considering parallel\ndata across a subset of the language pairs, mul-\ntilinguality gives rise to additional scenarios. For\ninstance, the scenario where two languages have no\nparallel data between each other but are well con-\nnected through a third (pivot) language has been\nexplored by several authors in the context of ma-\nchine translation (Cheng et al., 2016; Chen et al.,\n2017). However, given that the languages in ques-\ntion are still indirectly connected through parallel\ndata, this scenario does not fall within the unsuper-\nvised category, and is instead commonly known as\nzero-resource machine translation.\nAn alternative scenario explored in the contem-\nporaneous work of Liu et al. (2020) is where a set of\nlanguages are connected through parallel data, and\nthere is a separate language with monolingual data\nonly. We argue that, when it comes to the isolated\nlanguage, such a scenario should still be considered\nas UCL, as it does not rely on any parallel data for\nthat particular language nor does it assume any pre-\nvious knowledge of it. This scenario is easy to\njustify from a practical perspective given the abun-\ndance of parallel data for high-resource languages,\nand can also be interesting from a more theoretical\npoint of view. This way, rather than considering\ntwo unknown languages, this alternative scenario\nwould assume some knowledge of how one partic-\nular language is connected to other languages, and\nattempt to align it to a separate unknown language.\n4.4\nDiscussion\nAs discussed throughout the section, there are dif-\nferent training signals that we can exploit depend-\ning on the available resources of the languages\ninvolved and the assumptions made regarding their\nwriting system, which are summarized in Table 1.\nMany of these signals are not speciﬁc to work\non UCL but have been observed in the past in al-\nlegedly language-independent NLP approaches, as\ndiscussed by Bender (2011). Others, such as a re-\nliance on subwords or shared symbols are more\nrecent phenomena.\nWhile we do not aim to open a terminological\ndebate on what UCL encompasses, we advocate for\nfuture work being more aware and explicit about\nthe monolingual and cross-lingual signals they em-\nploy, what assumptions they make (e.g. regarding\nthe writing system), and the extent to which these\ngeneralize to other languages.\nIn particular, we argue that it is critical to con-\nsider the assumptions made by different methods\nwhen comparing their results. Otherwise the blind\nchase for state-of-the-art performance may bene-\nﬁt models making stronger assumptions and ex-\nploiting all available training signals, which could\nultimately conﬂict with the eminently scientiﬁc mo-\ntivation of this research area (see §3.2).\n5\nMethodological issues\nIn this section, we describe methodological issues\nthat are commonly encountered when training and\nevaluating unsupervised cross-lingual models and\npropose measures to ameliorate them.\n5.1\nValidation and hyperparameter tuning\nIn conventional supervised or semi-supervised set-\ntings, we use a separate validation set for develop-\nment and hyperparameter tuning. However, this\nbecomes tricky in unsupervised cross-lingual learn-\ning, where we ideally should not use any parallel\ndata other than for testing purposes.\nPrevious work has not paid much attention to\nthis aspect, and different methods are evaluated\nwith different validation schemes. For instance,\nArtetxe et al. (2018b,c) use a separate language\npair with a parallel validation set to make all devel-\nopment and hyperparameter decisions. They test\ntheir ﬁnal system on other language pairs without\nany parallel data. This approach has the advantage\nof being strictly unsupervised with respect to the\ntest language pairs, but the optimal hyperparameter\nchoice might not necessarily transfer well across\nlanguages. In contrast, Conneau et al. (2018a) and\nLample et al. (2018a) propose an unsupervised\nvalidation criterion that is deﬁned over monolin-\ngual data and shown to correlate well with test per-\nformance. This enables systematic tuning on the\nlanguage pair of interest, but still requires parallel\ndata to guide the development of the unsupervised\nvalidation criterion itself. A parallel validation\nset has also been used for systematic tuning in\nthe context of unsupervised machine translation\n(Marie and Fujita, 2018; Marie et al., 2019; Sto-\njanovski et al., 2019). While this is motivated as\na way to abstract away the issue of unsupervised\ntuning—which the authors consider to be an open\nproblem—we argue that any systematic use of par-\nallel data should not be considered UCL. Finally,\nprevious work often does not report the validation\nscheme used. In particular, unsupervised cross-\nlingual word embedding methods have almost ex-\nclusively been evaluated on bilingual lexicons that\ndo not have a validation set, and presumably use\nthe test set to guide development to some extent.\nOur position is that a completely blind develop-\nment model without any parallel data is unrealistic.\nSome cross-lingual signals to guide development\nare always needed. However, this factor should be\ncarefully controlled and reported with the neces-\nsary rigor as a part of the experimental design. We\nadvocate for using one language pair for develop-\nment and evaluating on others when possible. If\nparallel data in the target language pair is used, the\ntest set should be kept blind to avoid overﬁtting,\nand a separate validation should be used. In any\ncase, we argue that the use of parallel data in the\ntarget language pair should be minimized if not\ncompletely avoided, and it should under no circum-\nstances be used for extensive tuning. Instead, we\nrecommend to use unsupervised validation criteria\nfor systematic tuning in the target language.\n5.2\nEvaluation practices\nWe argue that there are also several issues with\ncommon evaluation practices in UCL.\nEvaluation\non\nfavorable\nconditions.\nMost\nwork on UCL has focused on relatively close lan-\nguages with large amounts of high-quality parallel\ncorpora from similar domains. Only recently have\napproaches considered more diverse languages as\nwell as language pairs that do not involve English\n(Glavaš et al., 2019; Vuli´c et al., 2019), and some\nexisting methods have been shown to completely\nbreak in less favorable conditions (Guzmán et al.,\n2019; Marchisio et al., 2020). In addition, most\napproaches have focused on learning from simi-\nlar domains, often involving Wikipedia and news\ncorpora, which are unlikely to be available for low-\nresource languages. We believe that future work\nshould pay more attention to the effect of the ty-\npology and linguistic distance of the languages\ninvolved, as well as the size, noise and domain\nsimilarity of the training data used.\nOver-reliance on translation tasks.\nMost work\non UCL focuses on translation tasks, either at the\nword level (where the problem is known as bilin-\ngual lexicon induction) or at the sentence level\n(where the problem is known as unsupervised ma-\nchine translation). While translation can be seen\nas the ultimate application of cross-lingual learn-\ning and has a strong practical interest on its own,\nit only evaluates a particular facet of a model’s\ncross-lingual generalization ability. In relation to\nthat, Glavaš et al. (2019) showed that bilingual\nlexicon induction performance does not always cor-\nrelate well with downstream tasks. In particular,\nthey observe that some mapping methods that are\nspeciﬁcally designed for bilingual lexicon induc-\ntion perform poorly on other tasks, showing the risk\nof relying excessively on translation benchmarks\nfor evaluating cross-lingual models.\nMoreover, existing translation benchmarks have\nbeen shown to have several issues on their own.\nIn particular, bilingual lexicon induction datasets\nhave been reported to misrepresent morphologi-\ncal variations, overly focus on named entities and\nfrequent words, and have pervasive gaps in the\ngold-standard targets (Czarnowska et al., 2019; Ke-\nmentchedjhieva et al., 2019). More generally, most\nof these datasets are limited to relatively close lan-\nguages and comparable corpora.\nLack of an established cross-lingual bench-\nmark.\nAt the same time, there is no de facto\nstandard benchmark to evaluate cross-lingual mod-\nels beyond translation. Existing approaches have\nbeen evaluated in a wide variety of tasks including\ndependency parsing (Schuster et al., 2019), named\nentity recognition (Rahimi et al., 2019), sentiment\nanalysis (Barnes et al., 2018), natural language\ninference (Conneau et al., 2018b), and document\nclassiﬁcation (Schwenk and Li, 2018). XNLI (Con-\nneau et al., 2018b) and MLDoc (Schwenk and Li,\n2018) are common choices, but they have their own\nproblems: MultiNLI, the dataset from which XNLI\nwas derived, has been shown to contain superﬁ-\ncial cues that can be exploited (Gururangan et al.,\n2018), while MLDoc can be solved by keyword\nmatching (Artetxe et al., 2020b). There are non-\nEnglish counterparts for more challenging tasks\nsuch as question answering (Cui et al., 2019; Hsu\net al., 2019), but these only exist for a handful of\nlanguages. More recent datasets such as XQuAD\nMethodological issues\nExamples\nValidation and\nhyperparameter tuning\nSystematic tuning with\nparallel data or on test data\nEvaluation on\nfavorable conditions\nTypologically similar languages;\nalways including English;\ntraining on the same domain\nOver-reliance on\ntranslation tasks\nOverﬁtting to bilingual lexicon\ninduction; known issues with\nexisting datasets\nLack of an established\nbenchmark\nEvaluation on many different\ntasks; problems with common\ntasks (MLDoc and XNLI)\nTable 2: Methodological issues pertaining to validation\nand hyperparameter tuning and evaluation practices in\ncurrent work on unsupervised cross-lingual learning.\n(Artetxe et al., 2020b), MLQA (Lewis et al., 2019)\nand TyDi QA (Clark et al., 2020) cover a wider\nset of languages, but a comprehensive benchmark\nthat evaluates multilingual representations on a di-\nverse set of tasks—in the style of GLUE (Wang\net al., 2018)—and languages has been missing un-\ntil very recently. The contemporaneous XTREME\n(Hu et al., 2020) and XGLUE (Liang et al., 2020)\nbenchmarks try to close this gap, but they are\nstill restricted to languages where existing labelled\ndata is available. Finally, an additional issue is\nthat a large part of these benchmarks were created\nthrough translation, which was recently shown to\nintroduce artifacts (Artetxe et al., 2020a).\nWe present a summary of the methodological\nissues discussed in Table 2.\n6\nBridging the gap between unsupervised\ncross-lingual learning ﬂavors\nThe three categories of UCL (§2) have so far been\ntreated as separate research topics by the commu-\nnity. In particular, cross-lingual word embeddings\nhave a long history (Ruder et al., 2019), while deep\nmultilingual pretraining has emerged as a separate\nline of research with its own best practices and eval-\nuation standards. At the same time, unsupervised\nmachine translation has been considered a separate\nproblem in its own right, where cross-lingual word\nembeddings and deep multilingual pretraining have\njust served as initialization techniques.\nWhile each of these families have their own\ndeﬁning features, we believe that they share a\nstrong connection that should be considered from\na more holistic perspective. In particular, both\ncross-lingual word embeddings and deep mul-\ntilingual pretraining share the goal of learning\n(sub)word representations, and essentially differ on\nwhether such representations are static or context-\ndependent. Similarly, in addition to being a down-\nstream application of the former, unsupervised ma-\nchine translation can also be useful to develop\nother multilingual applications or learn better cross-\nlingual representations. This has previously been\nshown for supervised machine translation (McCann\net al., 2017; Siddhant et al., 2019) and recently for\nbilingual lexicon induction (Artetxe et al., 2019a).\nIn light of these connections, we call for a more\nholistic view of UCL, both from an experimental\nand theoretical perspective.\nEvaluation.\nMost work on cross-lingual word\nembeddings focuses on bilingual lexicon induc-\ntion. In contrast, deep multilingual pretraining has\nnot been tested on this task, and is instead typi-\ncally evaluated on zero-shot cross-lingual transfer.\nWe think it is important to evaluate both families—\ncross-lingual word embeddings and deep multilin-\ngual representations—in the same conditions to bet-\nter understand their strengths and weaknesses. In\nthat regard, Artetxe et al. (2020b) recently showed\nthat deep pretrained models are much stronger in\nsome downstream tasks, while cross-lingual word\nembeddings are more efﬁcient and sufﬁcient for\nsimpler tasks. However, this could partly be at-\ntributed to a particular integration strategy, and we\nadvocate for using a common evaluation frame-\nwork in future work to allow a direct comparison\nbetween the different families.\nTheory.\nFrom a more theoretical perspective, it\nis still not well understood in what ways cross-\nlingual word embeddings and deep multilingual\npretraining differ. While one could expect the latter\nto be learning higher-level multilingual abstrac-\ntions, recent work suggests that deep multilingual\nmodels might mostly be learning a lexical-level\nalignment (Artetxe et al., 2020b). For that reason,\nwe believe that further research is needed to under-\nstand the relation between both families of models.\n7\nRecommendations\nTo summarize, we make the following practical\nrecommendations for future cross-lingual research:\n• Be rigorous when motivating UCL. Do not\npresent it as a practical scenario unless sup-\nported by a real use case.\n• Be explicit about the monolingual and cross-\nlingual signals used by your approach and the\nassumptions it makes, and take them into con-\nsiderations when comparing different models.\n• Report the validation scheme used. Minimize\nthe use of parallel data by preferring an unsu-\npervised validation criterion and/or using only\none language for development. Always keep\nthe test set blind.\n• Pay attention to the conditions in which you\nevaluate your model. Consider the impact\nof typology, linguistic distance, and the do-\nmain similarity, size and noise of the training\ndata. Be aware of known issues with common\nbenchmarks, and favor evaluation on a diverse\nset of tasks.\n• Keep a holistic view of UCL, including cross-\nlingual word embeddings, deep multilingual\npretraining and unsupervised machine transla-\ntion. To the extent possible, favor a common\nevaluation framework for these different fami-\nlies.\n8\nConclusions\nIn this position paper, we review the status quo of\nunsupervised cross-lingual learning—a relatively\nrecent ﬁeld. UCL is typically motivated by the\nlack of cross-lingual signal for many of the world’s\nlanguages, but available resources indicate that a\nscenario with no parallel data and sufﬁcient mono-\nlingual data is not realistic. Instead, we advocate\nfor the importance of UCL for scientiﬁc reasons.\nWe also discuss different monolingual and cross-\nlingual training signals that have been used in the\npast, and advocate for carefully reporting them to\nenable a meaningful comparison across different\napproaches. In addition, we describe methodolog-\nical issues related to the unsupervised setting and\npropose measures to ameliorate them. Finally, we\ndiscuss connections between cross-lingual word\nembeddings, deep multilingual pre-training, and\nunsupervised machine translation, calling for an\nevaluation on an equal footing.\nWe hope that this position paper will serve to\nstrengthen research in UCL, providing a more rigor-\nous look at the motivation, deﬁnition, and method-\nology. In light of the unprecedented growth of our\nﬁeld in recent times, we believe that it is essential to\nestablish a rigorous foundation connecting past and\npresent research, and an evaluation protocol that\ncarefully controls for the use of parallel data and\nassesses models in diverse, challenging settings.\nAcknowledgments\nThis research was partially funded by a Face-\nbook Fellowship, the Basque Government ex-\ncellence research group (IT1343-19), the Span-\nish MINECO (UnsupMT TIN2017-91692-EXP\nMCIU/AEI/FEDER, UE) and Project BigKnowl-\nedge (Ayudas Fundación BBVA a equipos de inves-\ntigación cientíﬁca 2018).\nReferences\nŽeljko Agi´c and Ivan Vuli´c. 2019. JW300: A wide-\ncoverage parallel corpus for low-resource languages.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n3204–3210, Florence, Italy. Association for Compu-\ntational Linguistics.\nDavid Alvarez-Melis and Tommi Jaakkola. 2018.\nGromov-wasserstein alignment of word embedding\nspaces. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1881–1890, Brussels, Belgium. Association\nfor Computational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 451–462,\nVancouver, Canada. Association for Computational\nLinguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018a. A robust self-learning method for fully un-\nsupervised cross-lingual mappings of word embed-\ndings. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 789–798, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018b.\nUnsupervised statistical machine transla-\ntion.\nIn Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3632–3642, Brussels, Belgium. Association\nfor Computational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2019a.\nBilingual lexicon induction through unsu-\npervised machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5002–5007, Florence,\nItaly. Association for Computational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2019b. An effective approach to unsupervised ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 194–203, Florence, Italy. Association\nfor Computational Linguistics.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2020a. Translation artifacts in cross-lingual transfer\nlearning. arXiv preprint arXiv:2004.04721.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018c. Unsupervised neural ma-\nchine translation. In Proceedings of the 6th Inter-\nnational Conference on Learning Representations\n(ICLR 2018).\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020b.\nOn the Cross-lingual Transferability of\nMonolingual Representations.\nIn Proceedings of\nACL 2020.\nJeremy Barnes, Roman Klinger, and Sabine Schulte im\nWalde. 2018.\nBilingual sentiment embeddings:\nJoint projection of sentiment across languages. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2483–2493, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nEmily M. Bender. 2011. On Achieving and Evaluating\nLanguage-Independence in NLP. Linguistic Issues\nin Language Technology, 6(3):1–26.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nChristian Buck, Kenneth Heaﬁeld, and Bas van Ooyen.\n2014. N-gram counts and language models from the\ncommon crawl. In Proceedings of the Ninth Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC’14), pages 3579–3584, Reykjavik,\nIceland. European Language Resources Association\n(ELRA).\nXilun Chen and Claire Cardie. 2018.\nUnsupervised\nmultilingual word embeddings. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 261–270, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nYun Chen, Yang Liu, Yong Cheng, and Victor O.K.\nLi. 2017.\nA teacher-student framework for zero-\nresource neural machine translation. In Proceedings\nof the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1925–1935, Vancouver, Canada. Association\nfor Computational Linguistics.\nYong Cheng, Yang Liu, Qian Yang, Maosong Sun, and\nWei Xu. 2016.\nNeural machine translation with\npivot languages. arXiv preprint arXiv:1611.04928.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. Tydi qa: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining.\nIn Advances\nin Neural Information Processing Systems 32, pages\n7057–7067.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018a.\nWord translation without parallel data. In Proceed-\nings of the 6th International Conference on Learning\nRepresentations (ICLR 2018).\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018b.\nXNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2019.\nCross-lingual\nmachine reading comprehension. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1586–1595, Hong Kong,\nChina. Association for Computational Linguistics.\nPaula Czarnowska, Sebastian Ruder, Edouard Grave,\nRyan Cotterell, and Ann Copestake. 2019.\nDon’t\nforget the long tail! A comprehensive analysis of\nmorphological generalization in bilingual lexicon in-\nduction. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n973–982, Hong Kong, China. Association for Com-\nputational Linguistics.\nAndrew M. Dai and Quoc V. Le. 2015.\nSemi-\nsupervised sequence learning. In Advances in Neu-\nral Information Processing Systems 28, pages 3079–\n3087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nQing Dou and Kevin Knight. 2012. Large scale deci-\npherment for out-of-domain machine translation. In\nProceedings of the 2012 Joint Conference on Empir-\nical Methods in Natural Language Processing and\nComputational Natural Language Learning, pages\n266–275, Jeju Island, Korea. Association for Com-\nputational Linguistics.\nQing Dou and Kevin Knight. 2013. Dependency-based\ndecipherment for resource-limited machine transla-\ntion.\nIn Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1668–1676, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nLong Duong, Hiroshi Kanayama, Tengfei Ma, Steven\nBird, and Trevor Cohn. 2017. Multilingual training\nof crosslingual word embeddings. In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n1, Long Papers, pages 894–904, Valencia, Spain. As-\nsociation for Computational Linguistics.\nUmberto Eco and James Fentress. 1995. The search for\nthe perfect language. Blackwell Oxford.\nManaal Faruqui and Chris Dyer. 2014. Improving vec-\ntor space word representations using multilingual\ncorrelation. In Proceedings of the 14th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 462–471, Gothenburg,\nSweden. Association for Computational Linguistics.\nGoran Glavaš, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 710–721,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nStephan Gouws, Yoshua Bengio, and Greg Corrado.\n2015.\nBilBOWA: Fast bilingual distributed repre-\nsentations without word alignments. In Proceedings\nof the 32nd International Conference on Machine\nLearning, volume 37 of Proceedings of Machine\nLearning Research, pages 748–756, Lille, France.\nPMLR.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nEdouard Grave, Armand Joulin, and Quentin Berthet.\n2019. Unsupervised alignment of embeddings with\nwasserstein procrustes. In Proceedings of Machine\nLearning Research, volume 89, pages 1880–1890.\nPMLR.\nSuchin Gururangan,\nSwabha Swayamdipta,\nOmer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A.\nSmith. 2018.\nAnnotation artifacts in natural lan-\nguage inference data. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 107–112, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nFrancisco Guzmán, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. The\nFLORES evaluation datasets for low-resource ma-\nchine translation:\nNepali–English and Sinhala–\nEnglish. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n6097–6110, Hong Kong, China. Association for\nComputational Linguistics.\nMichael Hahn and Marco Baroni. 2019.\nTabula\nnearly rasa: Probing the linguistic knowledge of\ncharacter-level neural language models trained on\nunsegmented text. Transactions of the Association\nfor Computational Linguistics, 7:467–484.\nGeert Heyman, Bregt Verreet, Ivan Vuli´c, and Marie-\nFrancine Moens. 2019. Learning unsupervised mul-\ntilingual word embeddings with incremental multi-\nlingual hubs. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 1890–1902, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nGeert Heyman, Ivan Vuli´c, and Marie-Francine Moens.\n2017.\nBilingual lexicon induction by learning to\ncombine word-level and character-level representa-\ntions. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Volume 1, Long Papers, pages\n1085–1095, Valencia, Spain. Association for Com-\nputational Linguistics.\nYedid Hoshen and Lior Wolf. 2018. Non-adversarial\nunsupervised word translation.\nIn Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 469–478, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nTsung-Yuan Hsu, Chi-Liang Liu, and Hung-yi Lee.\n2019. Zero-shot reading comprehension by cross-\nlingual transfer learning with multi-lingual lan-\nguage representation model. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5933–5940, Hong Kong,\nChina. Association for Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A Massively Multilingual Multi-\ntask Benchmark for Evaluating Cross-lingual Gener-\nalization. arXiv preprint arXiv:2003.11080.\nDavid Kamholz, Jonathan Pool, and Susan Colow-\nick. 2014.\nPanLex: Building a resource for pan-\nlingual lexical translation.\nIn Proceedings of the\nNinth International Conference on Language Re-\nsources and Evaluation (LREC’14), pages 3145–\n3150, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nYova Kementchedjhieva, Mareike Hartmann, and An-\nders Søgaard. 2019. Lost in evaluation: Misleading\nbenchmarks for bilingual dictionary induction. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3327–\n3332, Hong Kong, China. Association for Computa-\ntional Linguistics.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018a.\nUnsupervised\nmachine translation using monolingual corpora only.\nIn Proceedings of the 6th International Conference\non Learning Representations (ICLR 2018).\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018b.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5039–5049, Brussels, Belgium. Association\nfor Computational Linguistics.\nPatrick Lewis, Barlas O˘guz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2019. MLQA: Evalu-\nating Cross-lingual Extractive Question Answering.\narXiv preprint arXiv:1910.07475.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, Xiaodong Fan, Bruce\nZhang, Rahul Agrawal, Edward Cui, Sining Wei,\nTaroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie\nWu, Shuguang Liu, Fan Yang, Rangan Majumder,\nand Ming Zhou. 2020. Xglue: A new benchmark\ndataset for cross-lingual pre-training, understanding\nand generation. arXiv preprint arXiv:2004.01401.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020.\nMultilingual denoising\npre-training for neural machine translation. arXiv\npreprint arXiv:2001.08210.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015.\nBilingual word representations with\nmonolingual quality in mind. In Proceedings of the\n1st Workshop on Vector Space Modeling for Natural\nLanguage Processing, pages 151–159, Denver, Col-\norado. Association for Computational Linguistics.\nKelly Marchisio, Kevin Duh, and Philipp Koehn. 2020.\nWhen does unsupervised machine translation work?\narXiv preprint arXiv:2004.05516.\nBenjamin Marie and Atsushi Fujita. 2018.\nUnsuper-\nvised neural machine translation initialized by un-\nsupervised statistical machine translation.\narXiv\npreprint arXiv:1810.12703.\nBenjamin Marie, Haipeng Sun, Rui Wang, Kehai Chen,\nAtsushi Fujita, Masao Utiyama, and Eiichiro Sumita.\n2019.\nNICT’s unsupervised neural and statistical\nmachine translation systems for the WMT19 news\ntranslation task. In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 294–301, Florence, Italy.\nAssociation for Computational Linguistics.\nThomas Mayer and Michael Cysouw. 2014. Creating\na massively parallel Bible corpus. In Proceedings\nof the Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 3158–\n3163, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems 30, pages 6294–6305.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a.\nExploiting similarities among languages for ma-\nchine translation. arXiv preprint arXiv:1309.4168.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-\nrado, and Jeff Dean. 2013b. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111–3119.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014.\nGloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1532–1543,\nDoha,\nQatar. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations.\nIn Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT?\nIn Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER.\nIn Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 151–164, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nSujith Ravi and Kevin Knight. 2011. Deciphering for-\neign language. In Proceedings of the 49th Annual\nMeeting of the Association for Computational Lin-\nguistics: Human Language Technologies, pages 12–\n21, Portland, Oregon, USA. Association for Compu-\ntational Linguistics.\nShuo Ren, Zhirui Zhang, Shujie Liu, Ming Zhou, and\nShuai Ma. 2019.\nUnsupervised neural machine\ntranslation with SMT as posterior regularization. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 33, pages 241–248.\nParker Riley and Daniel Gildea. 2018. Orthographic\nfeatures for bilingual lexicon induction. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 390–394, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nSebastian Ruder, Ryan Cotterell, Yova Kementched-\njhieva, and Anders Søgaard. 2018. A discriminative\nlatent-variable model for bilingual lexicon induction.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n458–468, Brussels, Belgium. Association for Com-\nputational Linguistics.\nSebastian Ruder, Ivan Vuli´c, and Anders Søgaard.\n2019. A Survey of Cross-lingual Word Embedding\nModels. Journal of Artiﬁcial Intelligence Research,\n65:569–631.\nTal Schuster, Ori Ram, Regina Barzilay, and Amir\nGloberson. 2019. Cross-lingual alignment of con-\ntextual word embeddings, with applications to zero-\nshot dependency parsing.\nIn Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1599–1613, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nHolger Schwenk,\nVishrav Chaudhary,\nShuo Sun,\nHongyu Gong,\nand Francisco Guzmán. 2019a.\nWikiMatrix:\nMining 135M Parallel Sentences.\narXiv preprint arXiv:1907.05791.\nHolger Schwenk and Xian Li. 2018. A corpus for mul-\ntilingual document classiﬁcation in eight languages.\nIn Proceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), Miyazaki, Japan. European Language Re-\nsources Association (ELRA).\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov,\nEdouard Grave, and Armand Joulin. 2019b.\nCC-\nMatrix:\nMining Billions of High-Quality Par-\nallel Sentences on the WEB.\narXiv preprint\narXiv:1911.04944.\nAditya Siddhant, Melvin Johnson, Henry Tsai, Naveen\nArivazhagan, Jason Riesa, Ankur Bapna, Orhan Fi-\nrat, and Karthik Raman. 2019.\nEvaluating the\nCross-Lingual Effectiveness of Massively Multilin-\ngual Neural Machine Translation.\narXiv preprint\narXiv:1909.00437.\nSamuel L. Smith, David H. P. Turban, Steven Ham-\nblin, and Nils Y. Hammerla. 2017.\nOfﬂine bilin-\ngual word vectors, orthogonal transformations and\nthe inverted softmax. In Proceedings of the 5th Inter-\nnational Conference on Learning Representations\n(ICLR 2017).\nBenjamin Snyder, Tahira Naseem, Jacob Eisenstein,\nand Regina Barzilay. 2008. Unsupervised multilin-\ngual learning for POS tagging. In Proceedings of the\n2008 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1041–1050, Honolulu,\nHawaii. Association for Computational Linguistics.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 778–\n788, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019.\nMASS: Masked sequence to se-\nquence pre-training for language generation. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, volume 97, pages 5926–5936,\nLong Beach, California, USA. PMLR.\nDario Stojanovski, Viktor Hangya, Matthias Huck, and\nAlexander Fraser. 2019. The LMU munich unsuper-\nvised machine translation system for WMT19. In\nProceedings of the Fourth Conference on Machine\nTranslation (Volume 2: Shared Task Papers, Day\n1), pages 393–399, Florence, Italy. Association for\nComputational Linguistics.\nClara Vania and Adam Lopez. 2017. From characters\nto words to in between: Do we capture morphol-\nogy? In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2016–2027, Vancouver,\nCanada. Association for Computational Linguistics.\nIvan Vuli´c, Goran Glavaš, Roi Reichart, and Anna Ko-\nrhonen. 2019.\nDo we really need fully unsuper-\nvised cross-lingual embeddings? In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4407–4418, Hong Kong,\nChina. Association for Computational Linguistics.\nIvan Vuli´c and Anna Korhonen. 2016. On the role of\nseed lexicons in learning bilingual word embeddings.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 247–257, Berlin, Germany.\nAssociation for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding.\nIn Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nShijie Wu, Alexis Conneau, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019.\nEmerging\ncross-lingual structure in pretrained language mod-\nels. arXiv preprint arXiv:1911.01464.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nRuochen Xu, Yiming Yang, Naoki Otani, and Yuexin\nWu. 2018.\nUnsupervised cross-lingual transfer of\nword embedding spaces. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2465–2474, Brussels, Bel-\ngium. Association for Computational Linguistics.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017a. Adversarial training for unsupervised\nbilingual lexicon induction. In Proceedings of the\n55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1:\nLong Papers),\npages 1959–1970, Vancouver, Canada. Association\nfor Computational Linguistics.\nMeng Zhang, Yang Liu, Huanbo Luan, and Maosong\nSun. 2017b. Earth mover’s distance minimization\nfor unsupervised bilingual lexicon induction. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1934–\n1945, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-04-30",
  "updated": "2020-04-30"
}