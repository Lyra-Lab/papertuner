{
  "id": "http://arxiv.org/abs/2005.00318v1",
  "title": "Can Multilingual Language Models Transfer to an Unseen Dialect? A Case Study on North African Arabizi",
  "authors": [
    "Benjamin Muller",
    "Benoit Sagot",
    "Djamé Seddah"
  ],
  "abstract": "Building natural language processing systems for non standardized and low\nresource languages is a difficult challenge. The recent success of large-scale\nmultilingual pretrained language models provides new modeling tools to tackle\nthis. In this work, we study the ability of multilingual language models to\nprocess an unseen dialect. We take user generated North-African Arabic as our\ncase study, a resource-poor dialectal variety of Arabic with frequent\ncode-mixing with French and written in Arabizi, a non-standardized\ntransliteration of Arabic to Latin script. Focusing on two tasks,\npart-of-speech tagging and dependency parsing, we show in zero-shot and\nunsupervised adaptation scenarios that multilingual language models are able to\ntransfer to such an unseen dialect, specifically in two extreme cases: (i)\nacross scripts, using Modern Standard Arabic as a source language, and (ii)\nfrom a distantly related language, unseen during pretraining, namely Maltese.\nOur results constitute the first successful transfer experiments on this\ndialect, paving thus the way for the development of an NLP ecosystem for\nresource-scarce, non-standardized and highly variable vernacular languages.",
  "text": "Can Multilingual Language Models Transfer to an Unseen Dialect?\nA Case Study on North African Arabizi\nBenjamin Muller\nBenoˆıt Sagot\nDjam´e Seddah\nInria, Paris, France\nfirstname.lastname@inria.fr\nAbstract\nBuilding natural language processing systems\nfor non standardized and low resource lan-\nguages is a difﬁcult challenge. The recent suc-\ncess of large-scale multilingual pretrained lan-\nguage models provides new modeling tools to\ntackle this. In this work, we study the abil-\nity of multilingual language models to pro-\ncess an unseen dialect. We take user gener-\nated North-African Arabic as our case study, a\nresource-poor dialectal variety of Arabic with\nfrequent code-mixing with French and writ-\nten in Arabizi, a non-standardized translitera-\ntion of Arabic to Latin script.\nFocusing on\ntwo tasks, part-of-speech tagging and depen-\ndency parsing, we show in zero-shot and unsu-\npervised adaptation scenarios that multilingual\nlanguage models are able to transfer to such\nan unseen dialect, speciﬁcally in two extreme\ncases: (i) across scripts, using Modern Stan-\ndard Arabic as a source language, and (ii) from\na distantly related language, unseen during pre-\ntraining, namely Maltese. Our results consti-\ntute the ﬁrst successful transfer experiments\non this dialect, paving thus the way for the de-\nvelopment of an NLP ecosystem for resource-\nscarce, non-standardized and highly variable\nvernacular languages.\n1\nIntroduction\nAccurately modeling low resource and non-\nstandardized languages exhibiting a high degree of\nvariation is extremely challenging. Recent releases\nof multilingual language models trained on large\ncorpora (Devlin et al., 2019; Lample and Conneau,\n2019) provide an interesting opportunity to address\nthis challenge in new ways. We frame our work\nas a cross-lingual transfer learning analysis; we\nstudy the capacity of a system trained as a language\nmodel on a source set of languages to transfer to a\ntarget language and task. More precisely, we inves-\ntigate the ability of multilingual language models\nto process a language that is absent from their pre-\ntraining set. For brevity, we simply refer to such\nlanguages as unseen.\nOur work focuses on the multilingual version of\nBERT (mBERT) (Devlin et al., 2019). The cross-\nlingual modeling ability of mBERT has been re-\ncently studied by Pires et al. (2019), who show\nthat cross-lingual transfer is very efﬁcient between\npretrained languages. In our work, we address a dif-\nferent and more challenging question: can mBERT\ntransfer to an unseen and non-standardized dialect?\nWe take North-African Arabizi, hereafter Narabizi,\nas our case study. We deﬁne Narabizi as the Arabic\ndialect spoken in Algeria, found ubiquitously on\nsocial media and written in Latin script, although\nwith no standard spelling and no standard translit-\neration of Arabic letters. It is a non-standardized\ndialect and shows a high degree of code-mixing\nwith French (Amazouz et al., 2019). This makes\nNarabizi highly variable across users and therefore\nvery challenging for Natural Language Processing.\nFor our experiments, we use the Narabizi raw\ncorpus and treebank recently released by Seddah\net al. (2020) and focus on two tasks, namely part-\nof-speech (POS) tagging and dependency parsing.\nAfter a detailed cross-lingual performance analysis,\nour results show that multilingual models are able\nto transfer to unseen, highly variable data. More\nprecisely, we make the following contributions:\n• We push the zero-shot cross-lingual abilities\nof mBERT to the extreme and show that it can\ntransfer to unseen Narabizi in POS tagging\nand parsing, even when the source is another\nunseen and related language such as Maltese\n• By running comparison across source lan-\nguages and diverse BERT models, we demon-\nstrate that mBERT is using its multilingual\nrepresentations to process Narabizi.\n• We show the positive impact of unsuper-\nvised ﬁne-tuning on cross-lingual transfer and\ndemonstrate its ability to make transfer possi-\nble, even across scripts, in a scenario where\narXiv:2005.00318v1  [cs.CL]  1 May 2020\nthe target language is not in the pre-training\ncorpora.\n2\nRelated Work\nWord embedding & Cross lingual Transfer\nRecently, cross lingual transfer has beneﬁted from\nmultilingual language models. We refer to (Lam-\nple and Conneau, 2019; Eisenschlos et al., 2019;\nVania et al., 2019; Wu et al., 2019; Conneau et al.,\n2019; Wu and Dredze, 2019) who demonstrate the\nefﬁciency of language models in zero-shot transfer\nsettings for a variety of tasks. In this regard, Pires\net al. (2019) analyze in detail the zero-shot trans-\nfer ability of mBERT on sequence labeling. Wang\net al. (2019) suggest that cross-lingual transfer of\nmultilingual models rely on structural properties of\nlanguages. Both studies focus on transfer between\nlanguages that are part of the pretraining corpora.\nIn our work, we study the ability of mBERT to\ntransfer to an unseen language.\nCode-Switching\nis a hard challenge for NLP as\nshown in the myriad of works that have tackled\nthis phenomenon for more than 10 years, see for\nexample (Solorio and Liu, 2008; Vyas et al., 2014;\nC¸ etino˘glu and C¸ ¨oltekin, 2016; Lynn and Scannell,\n2019). Ball and Garrette (2018) and Pires et al.\n(2019) analyzed the performance of neural mod-\nels for sequence labeling showing that those ap-\nproaches can cope with such a complexity. In our\nwork, we face both code-switched and highly vari-\nable data.\nUnsupervised Adaptation of Language Models\nHan and Eisenstein (2019) show that ﬁne-tuning\nBERT in an unsupervised way using its masked\nlanguage objective brings signiﬁcant improvement\nto downstream sequence labeling tasks for out-of-\ndomain Old English. Studying the speciﬁc case\nof English-Spanish code-mixing, Gonen and Gold-\nberg (2018) show how to adapt bilingual language\nmodels to code-mixed data. In our work, we focus\non unsupervised adaptation and analyze its impact\non the even more challenging case of Narabizi.\n3\nNarabizi\nArabic varieties are often classiﬁed into three cat-\negories (Habash, 2010): (i) Classical Arabic, as\nfound in the Qur’an and related canonical texts,\n(ii) Modern Standard Arabic (MSA), the ofﬁcial\nlanguage of the vast majority of Arabic speaking\ncountries and (iii) Dialectal Arabic. This work\nfocuses on North-African dialectal Arabic in its Al-\ngerian form, understood and spoken by more than\n40 million people in the Maghreb (Sayahi, 2014).\nIn its written form, it is mostly found online and in\nLatin script. For simplicity we refer to this North-\nAfrican Arabic dialect as North-African Arabizi\n(Farrag, 2012) or Narabizi, illustrated here:\nsource: Mrhba, Ana 3rbi mn dzaye\ntranslation:“Hey, Im Arab from Algeria”\nLike other written languages found on social me-\ndia and even more importantly as it is not standard-\nized,1 Narabizi shows a high degree of variability\nacross writers. As part of its variability, Narabizi\nfrequently involves code-switching with French.\nMoreover, Narabizi does not belong to the pre-\ntraining corpora of mBERT. For this reason, we\ntake Narabizi as our case study to analyze the abil-\nity of mBERT to handle an unseen, highly variable\nand code-mixed dialect.\nData\nThe data we use comes from two main\nsources. The ﬁrst one, described by Cotterell et al.\n(2014), is a collection of 9000 raw Algerian roman-\nized Arabic sentences, a sample of which has been\nannotated with Universal Dependency trees (Mc-\nDonald et al., 2013) and word-level language iden-\ntiﬁcation2 by Seddah et al. (2020) totalling 1,434\n(1172/146/178) annotated sentences. Our second\nsource, also released by Seddah et al. (2020), is a\ncollection of 49,546 raw Narabizi sentences.\nBaselines\nTo grasp the complexity of Narabizi,\nwe run some preliminary experiments. We take\nQi et al. (2019)’s tagger and parser as our strong\nbaselines (named StanfordNLP3), and as our bot-\ntom lines the majority class predictor for POS tag-\nging and the left predictor4 for dependency parsing.\nCompetitive taggers perform on datasets of similar\nsize above 90%. StanfordNLP only reaches 84.20%\non our data for POS tagging and 52.84% for pars-\ning, as measured by the unlabeled attachment score\n(UAS; cf. Table 1).\n4\nModel\nmBERT is a Transformer (Vaswani et al., 2017)\ntrained as a joint masked-language and a next sen-\n1I.e. no writing rules are ofﬁcially deﬁned.\n2Narabizi and French prevalence in the train set (% to-\nken): Narabizi 64.64%, French 33.84%, then MSA, English\n& Spanish.\n3Ranked top 3 (after correction) in POS tagging and pars-\ning at the 2018 UD shared task, trained using French fastText\nvectors (Mikolov et al., 2018).\n4Whereby each word is attached to its immediate left neigh-\nbor.\ntence prediction model on sub-word level tokenized\nsentences. More details can be found in (Devlin\net al., 2019). We use the multilingual cased version\nof BERT. mBERT was trained on the concatenation\nof the Wikipedia corpora for 104 languages.\nPOS tagging and dependency parsing with\nmBERT\nFollowing Devlin et al. (2019), we turn\nmBERT into a POS tagger by appending a softmax\non top of its last layer. For parsing, we append the\nbiafﬁne graph parser layers described by Dozat and\nManning (2016). In both cases, we ﬁne-tune the\noverall model by backpropagating only through the\nﬁrst sub-word token of each word. We call these\narchitectures mBERT+POS and mBERT+PARSE,\nand use them in our zero-shot learning experiments\nby applying models trained on a source language\nto data in our target language.\nUnsupervised Adaptation\nWe call unsuper-\nvised adaptation the process of ﬁne-tuning mBERT\nin an unsupervised manner using its Masked-\nLanguage Model (MLM) objective trained on\nraw sentences. We refer to mBERT ﬁne-tuned\non raw data as mBERT+MLM.\nWe deﬁne as\nmBERT+MLM+TASK, with TASK referring to\nPOS, resp. PARSE, to point to mBERT+MLM ﬁne-\ntuned as a POS tagger, resp. parser.\n5\nExperiments\nOur goal is to measure how well mBERT makes\nuse of its multilingual pre-training on an unseen\ndialect. We deﬁned a source language as a language\non which a POS tagger or a parser are trained, and\nwill report the performance of the resulting models\nwhen applied to Narabizi data.\n5.1\nSource Languages\nWe study transfer along two independent direc-\ntions. The ﬁrst one is the relatedness of the source\nlanguage to Narabizi. The more different they are,\nthe worse we expect the transfer to be. Our second\ndirection distinguishes between source languages\nincluded in mBERT pre-training corpora and those\nthat are not. We expect the transfer to be better\nwhen the source language is included in the pre-\ntraining corpora. To cover the full scope of cases,\nwe pick Modern Standard Arabic, French, English\nand Vietnamese.\nAs recalled in (Habash, 2010; ˇC´epl¨o et al., 2016),\nMaltese is related to the Arabic continuum of lan-\nguages. It is standardized and written in an ex-\ntended Latin script. This makes Maltese a promis-\ning candidate for transferring to Narabizi.\nWe also use French in order to study the im-\npact of code-mixing. In addition, we experiment\nwith English as another European language writ-\nten in Latin script, but which is not code-mixed\nwith Narabizi. Finally, we use Vietnamese as the\nmost unrelated language to test the cross-lingual\npower of the model in the most extreme case. We\nrefer the reader to the Appendix (Table 3) for an\noverview of the source languages. We sample the\ntraining datasets to have 1,200 sentences for each\nsource language.5 Additionally, we report results\nin the standard supervised setting in which we ﬁne-\ntune mBERT+TASK on Narabizi and evaluate on\nNarabizi. This provides us with an upper bound on\nhow we can expect mBERT to perform on such a\nlanguage.\n5.2\nOptimisation\nFor supervised ﬁne-tuning, we use the same\nrange of hyper-parameters as Devlin et al. (2019).\nFor unsupervised adaptation, we run preliminary\nexperiments to measure the impact of the raw cor-\npus among the 49,000 sentences Narabizi corpus,\nand Narabizi mixed with a sub-sample extracted\nfrom mBERT pre-training corpora. As reported\nby Gonen and Goldberg (2018), we found that, if\ncarefully optimized, ﬁne-tuning mBERT with its\nmasked language objective directly on the target\ndata leads to the best models.6\n6\nResults and Discussion\nWe present our results in Table 1. We report the ac-\ncuracy for POS tagging and the Unlabeled Attach-\nment Score (UAS) for parsing.7 Any performance\nabove the bottom line demonstrates that transfer\nis happening from the pre-training or ﬁne-tuning\nstages to process Narabizi. For both POS tagging\nand parsing, mBERT+TASK performs outperforms\nthe baselines by a large margin when the source\nis Maltese, French and English. This shows that\nmBERT is able to transfer to Narabizi even without\nhaving been trained on any Narabizi tokens at any\nstage of the training process.\nWe\nreport\nan\naverage\nboost\nwith\nmBERT+MLM+TASK\nof\n+10.15\npoints\nin\n5We pick the ﬁrst 1,200 training sentences. More informa-\ntion on the datasets used is given in Appendix 5.1.\n6Cf. Appendix § A.2 for details on hyper-parameters.\n7For parsing, we focus on UAS only and do not report\nLabeled Attachment Scores (LAS). We do so because we no-\nticed diverging conventions in the labeling scheme between\nthe Narabizi treebank and the other treebanks we use, which\nresult from different annotation choices allowed by the Uni-\nversal Dependencies framework.\n<60\n60-78\n78-100\n100\n% narabizi words in each sentence\n20\n30\n40\n50\n60\n70\nUPOS\n(A)\n(E)\n(C)\n(B)\n(D)\n<60\n60-78\n78-100\n100\n% narabizi words in each sentence\n20\n30\n40\n50\n60\n70\nUAS\n(I)\n(J)\n(K)\n(L)\n(M)\nmBERT+Task French\nmBERT+MLM+Task French\nmBERT+Task Maltese\nmBERT+MLM+Task Maltese\nFigure 1: Performance with regard to code-mixing rate,\nreported on Narabizi train set to have enough data per\nbucket. (5 seeds). (X) markers commented in sec. 6.2.\nNB: no Narabizi annotated training data seen during\nﬁne-tuning.\nPOS tagging and +4.10 in UAS across all source\nlanguages when compared with mBERT+TASK.\nThis means that the unsupervised adaptation on\n49,546 raw Narabizi sentences is efﬁcient even\non such an out-of-domain language. In all these\nsettings, StanfordNLP, the very strong neural\nbaseline, designed speciﬁcally for POS tagging\nand parsing, is outperformed by mBERT (it only\nreaches 31.90 and 33.74 (resp. 15.53) for parsing\nin UAS (resp. in LAS) on Narabizi when trained\non French)\n6.1\nCross-script Transfer\nIn zero-shot settings, cross-script transfer does\nnot perform above the bottom lines when the source\nis MSA in POS tagging. Our hypothesis is that such\ntransfer requires the target language to be in the pre-\ntraining corpora as reported by Pires et al. (2019)\nin the case of Urdu and Hindi. Nevertheless, to\nour surprise, we observe an impressive +13 boost\nin tagging and +6.11 in parsing performance after\nunsupervised adaptation when the source is MSA,\noutperforming the baselines. This means that in the\ncase of MSA, cross-script transfer happens when\nthe target language is seen during unsupervised\nadaptation, and 49k sentences are enough to lead\nto such a transfer. Moreover, cross-script transfer\nis better with MSA than Vietnamese, suggesting\nthat the multilingual model is making use of the\nproximity of MSA and Narabizi.\n6.2\nImpact of code-mixing\nWe hypothesize that the high level of transfer\nwhen the source is French is due to the high code-\nmixing proportion of Narabizi. To test our hypoth-\nesis, we present in Figure 1 the performance of\nthe model with respect to the code-mixing ratio.\nWe split the dataset into four buckets of around\n25% of the full dataset, according to the ratio\nof native Narabizi tokens in each sentence (be-\ntween less than 60% to stricly 100%) as opposed\nto French tokens. We compare French and Mal-\ntese as source languages. We conﬁrm our intuition\nthat code-mixing explains the good performance of\nthe model trained on French. Indeed, on sentences\nthat have 100% Narabizi tokens, mBERT+TASK\ntrained on French performs poorly (cf.\nﬁg. 1\n(E) for POS and (L) for parsing). On the other\nside, for sentences that include at least 40% of\nFrench tokens, scores reach 54% (cf. (A)) for\nPOS tagging and 47% for parsing (cf. (I)). More-\nover, for French, mBERT+MLM+TASK leads to\nan impressive 21.2% error reduction compared to\nmBERT+TASK for POS tagging (33.12 vs. 47.32)\nand an 8.5% error reduction for parsing (cf. Ta-\nble 1). We observe in Fig. 1 (cf. (B) and (K)) that\nthis improvement mostly comes from a better accu-\nracy on Narabizi tokens. Interestingly, we observe\nthat unsupervised ﬁne-tuning leads to the closing\nof the gap between the performance of the models\ntuned on French and Maltese on native Narabizi\ntokens (+15: (B)-(E) vs. +2.4: (B)-(C) for POS\ntagging and +5.6: (K)-(L) vs. +2.2: (J)-(K) for\nparsing). This demonstrates the capacity of un-\nsupervised ﬁne-tuning to close lexical mismatch\nbetween distant languages such as native Narabizi\nand French.\n6.3\nTransfer between unseen languages\nSurprisingly, mBERT+TASK tuned on Maltese\ndoes not perform poorly. It leads to the best per-\nformance for mBERT+TASK for both POS tagging\nand parsing. It outperforms StanfordNLP in the\nzero-shot scenario by 5 points in POS tagging and\n6 points in parsing. As seen in Figure 1 (C) and (J),\nit performs the best on native Narabizi sentences\n(with no code-mixing). This result is surprising\nas Maltese is absent from the pre-training corpora.\nIt shows that mBERT is able to capture structural\nproperties shared by related languages even if they\nare absent from the pre-training corpora, thereby\nextending the observations described by Wang et al.\n(2019).\nIs the multilingualism of mBERT at play?\nFi-\nnally, we want to show that the ability of mBERT\nto achieve cross-lingual transfer is related to the\n104 languages it is pre-trained on, rather than be-\ncause a pre-trained Transformer is an inherently\nmBERT+TASK\nmBERT+MLM+TASK\nSource\nPOS\nUAS\nPOS\nUAS\nMaltese\n35.13\n40.04\n38.94\n42.32\nFrench\n33.12\n38.54\n47.32\n43.77\nEnglish\n30.67\n32.40\n44.59\n38.83\nMSA\n16.55\n28.23\n28.08\n34.34\nViet.\n16.92\n13.98\n23.21\n14.44\nNarabizi\n81.60\n66.84\n82.61\n67.12\nBaselines\nStanfordNLP\nBottom lines\nNarabizi\n84.20\n52.84\n20.49\n18.71\nFrench\n27.00\n33.74\n-\n-\nTable 1: Cross-Lingual performance averaged on 5\nseeds on the Narabizi test set. Baselines are described\nin Section 3.\ngood POS tagger or parser. To do so, we com-\npare mBERT with three other models: Roberta,\nthe optimized English version of BERT (Liu et al.,\n2019), CamemBERT (C.BERT) the French version\nof BERT (Martin et al., 2019), and a randomly ini-\ntialized mBERT-like Transformer (Rand.). We fo-\ncus our analysis on French and Maltese. mBERT is\nthe model that leads to the most successful transfer\nin both cases and for both tasks, by a very large\nmargin in the case of Maltese. This shows that\npre-training on such a diversity of languages is at\nthe core of the transfer to Narabizi.\nmBERT\nRoBERTA\nC.BERT\nRand.\nPOS\nUAS\nPOS UAS\nPOS UAS\nPOS\nUAS\nfr\n33.12 38.54\n29.75 27.41\n31.77 32.55\n30.29 25.30\nmt\n35.13 40.04\n25.45 17.27\n31.62 34.65\n19.81 19.04\nTable 2: Zero-shot transfer from French (fr) and Mal-\ntese (mt) to Narabizi. 5 averaged seeds.\n7\nConclusion\nOur work on Narabizi reveals novel properties of\nmultilingual language models. We have shown that\ntransfer learning approaches can be used success-\nfully on this language, both in zero-shot scenarios,\nwhere no target language data is used at any stage\nof the training process, and in unsupervised adapta-\ntion scenarios, where only raw target language data\nis used.\nThis is remarkable, because Narabizi, an in-\ncreasingly used language on social media, is an\nextremely challenging language for NLP in general\nand transfer learning approaches in particular, for\nat least three reasons: (i) it is written in a script\ndifferent to its closest resourced relative (Modern\nStandard Arabic), (ii) it displays a high degree of\nvariation because of the lack of spelling standard,\nand (iii) it involves frequent code-switching with\nan unrelated language (French, in our case).\nOur results pave the way to using transfer learn-\ning approaches to build NLP tools not only for\nNarabizi but also for other vernacular varieties\nof Arabic written in Latin script, and more gen-\nerally for any low resource language, even when it\ndisplays some of the challenging properties listed\nabove. Our paper therefore sheds light on a way\nto initiate the development of NLP ecosystems for\nlanguages and language varieties that are increas-\ningly used online, for which NLP is badly needed,\nbut for which few resources, if any, are available to\ndate.\nAcknowledgments\nWe want to thanks Yanai Elazar, Ganesh Jawa-\nhar and Louis Martin for proofreading and in-\nsightful comments. This work was partly funded\nby two French National funded projects granted\nto Inria and other partners by the Agence Na-\ntionale de la Recherche, namely projects PAR-\nSITI (ANR-16-CE33-0021) and SoSweet (ANR-\n15-CE38-0011), as well as by the second author’s\nchair in the PRAIRIE institute funded by the\nFrench national agency ANR as part of the “In-\nvestissements davenir” programme under the ref-\nerence ANR-19-P3IA-0001. This project also re-\nceived support from the French Ministry of Indus-\ntry and Ministry of Foreign Affairs via the PHC\nMaimonide France-Israel cooperation programme.\nReferences\nDjegdjiga Amazouz, Martine Adda-Decker, and Lori\nLamel. 2019.\nAddressing code-switching in\nfrench/algerian arabic speech. In Interspeech 2017,\npages 62–66.\nKelsey Ball and Dan Garrette. 2018.\nPart-of-speech\ntagging for code-switched, transliterated texts with-\nout explicit language identiﬁcation.\nIn Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 3084–3089,\nBrussels, Belgium. Association for Computational\nLinguistics.\nSlavom´ır\nˇC´epl¨o, J´an B´atora, Adam Benkato, Jiˇr´ı\nMiliˇcka, Christophe Pereira, and Petr Zem´anek.\n2016.\nMutual intelligibility of spoken maltese,\nlibyan arabic, and tunisian arabic functionally tested:\nA pilot study. Folia Linguistica, 50(2):583–628.\n¨Ozlem C¸ etino˘glu and C¸ a˘grı C¸ ¨oltekin. 2016.\nPart\nof speech annotation of a Turkish-German code-\nswitching corpus. In Proceedings of the 10th Lin-\nguistic Annotation Workshop held in conjunction\nwith ACL 2016 (LAW-X 2016), pages 120–130,\nBerlin, Germany. Association for Computational\nLinguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nRyan Cotterell, Adithya Renduchintala, Naomi Saphra,\nand Chris Callison-Burch. 2014.\nAn algerian\narabic-french code-switched corpus.\nIn Workshop\non Free/Open-Source Arabic Corpora and Corpora\nProcessing Tools Workshop Programme.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTimothy Dozat and Christopher D Manning. 2016.\nDeep biafﬁne attention for neural dependency pars-\ning. arXiv preprint arXiv:1611.01734.\nJulian Eisenschlos, Sebastian Ruder, Piotr Czapla,\nMarcin\nKardas,\nSylvain\nGugger,\nand\nJeremy\nHoward. 2019.\nMultiﬁt:\nEfﬁcient multi-lingual\nlanguage\nmodel\nﬁne-tuning.\narXiv\npreprint\narXiv:1909.04761.\nMona Farrag. 2012. Arabizi: a writing variety worth\nlearning? an exploratory study of the views of for-\neign learners of arabic on arabizi. Master’s thesis,\nSchool of Humanities and Social Sciences, Ameri-\ncan University in Cairo, Cairo, Egypt.\nHila Gonen and Yoav Goldberg. 2018.\nLanguage\nmodeling for code-switching: Evaluation, integra-\ntion of monolingual data, and discriminative train-\ning. arXiv preprint arXiv:1810.11895.\nNizar Habash. 2010. Introduction to Arabic Natural\nLanguage Processing. Morgan and Claypool.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsuper-\nvised domain adaptation of contextualized embed-\ndings: A case study in early modern english. arXiv\npreprint arXiv:1904.02817.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTeresa Lynn and Kevin Scannell. 2019.\nCode-\nswitching in irish tweets: A preliminary analysis.\nIn Proceedings of the Celtic Language Technology\nWorkshop, pages 32–40, Dublin, Ireland. European\nAssociation for Machine Translation.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Su´arez, Yoann Dupont, Laurent Romary, ´Eric\nVillemonte de la Clergerie, Djam´e Seddah, and\nBenoˆıt Sagot. 2019.\nCamemBERT: a Tasty\nFrench Language Model.\narXiv e-prints, page\narXiv:1911.03894.\nRyan McDonald, Joakim Nivre, Yvonne Quirmbach-\nBrundage, Yoav Goldberg, Dipanjan Das, Kuz-\nman Ganchev,\nKeith Hall,\nSlav Petrov,\nHao\nZhang, Oscar T¨ackstr¨om, Claudia Bedini, N´uria\nBertomeu Castell´o, and Jungmee Lee. 2013. Uni-\nversal dependency annotation for multilingual pars-\ning. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 2: Short Papers), pages 92–97, Soﬁa, Bulgaria.\nAssociation for Computational Linguistics.\nTomas Mikolov, Edouard Grave, Piotr Bojanowski,\nChristian Puhrsch, and Armand Joulin. 2018. Ad-\nvances in pre-training distributed word representa-\ntions. In Proceedings of the International Confer-\nence on Language Resources and Evaluation (LREC\n2018).\nJoakim Nivre, Marie-Catherine De Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Hajic, Christopher D Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, et al. 2016. Universal dependencies\nv1: A multilingual treebank collection. In Proceed-\nings of the Tenth International Conference on Lan-\nguage Resources and Evaluation (LREC’16), pages\n1659–1666.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT?\narXiv\npreprint arXiv:1906.01502.\nPeng Qi, Timothy Dozat, Yuhao Zhang, and Christo-\npher D Manning. 2019. Universal dependency pars-\ning from scratch. arXiv preprint arXiv:1901.10457.\nSayahi. 2014.\nThe languages of the Maghreb. In\nDiglossia and Language Contact: Language Varia-\ntion and Change in North Africa. Cambridge: Cam-\nbridge University Press.\nDjam´e Seddah, Farah Essaidi, Amal Fethi, Matthieu\nFuteral, Benjamin Muller, Pedro Javier Ortiz Su´arez,\nBenoˆıt Sagot, and Abhishek Srivastava. 2020. Build-\ning a user-generated content north-african arabizi\ntreebank: Tackling hell.\nIn 59th Annual Meeting\nof the Association for Computational Linguistics\n(ACL), Seattle, USA.\nThamar Solorio and Yang Liu. 2008. Part-of-Speech\ntagging for English-Spanish code-switched text. In\nProceedings of the 2008 Conference on Empirical\nMethods in Natural Language Processing, pages\n1051–1060, Honolulu, Hawaii. Association for Com-\nputational Linguistics.\nClara Vania, Yova Kementchedjhieva, Anders Søgaard,\nand Adam Lopez. 2019. A systematic comparison\nof methods for low-resource dependency parsing on\ngenuinely low-resource languages. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 1105–1116, Hong\nKong, China. Association for Computational Lin-\nguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYogarshi Vyas, Spandana Gella, Jatin Sharma, Kalika\nBali, and Monojit Choudhury. 2014. POS tagging\nof English-Hindi code-mixed social media content.\nIn Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 974–979, Doha, Qatar. Association for Com-\nputational Linguistics.\nZihan Wang, Stephen Mayhew, Dan Roth, et al. 2019.\nCross-lingual ability of multilingual BERT: An em-\npirical study. arXiv preprint arXiv:1912.07840.\nShijie Wu, Alexis Conneau, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019.\nEmerging\ncross-lingual structure in pretrained language mod-\nels. arXiv preprint arXiv:1911.01464.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nA\nAppendix\nA.1\nSource Languages\nLanguage\nscript\nrelatedness\n∈ΩmBERT\nNarabizi\nLatin\n-\nno\nFrench\nLatin\ncode-mixed\nyes\nEnglish\nLatin\nnone\nyes\nMaltese\nLatin\nshared root\nno\nMS Arabic\nArabic\nshared root\nyes\nVietnamese\nLatin\nnone\nyes\nTable 3: Source language in regard to Narabizi based\non languages relatedness and inclusion in model pre-\ntraining corpora (∈ΩmBERT for languages included in\nthe 104 pre-training languages of mBERT).\nFrench\nfr gsd\nMS Arabic\nar padt\nEnglish\nen ewt\nMaltese\nmt mudt\nVietnamese\nvi vtb\nTable 4: Universal Dependencies (Nivre et al., 2016)\nDatasets used for cross-lingual experiments\nA.2\nFine-tuning hyper-parameters\nWe list here all the hyper-parameters used for\nﬁne-tuning in a supervised way on POS tagging\nand in an unsupervised way on raw Narabizi data\n(cf. Table 5 and 6). For the supervised setting,\nwe run a grid search on all the combination of\nhyper-parameters and select the best model on the\nvalidation set of the source language for both POS\ntagging and parsing.\nbatch size\n{32,16}\nlearning rate\n{1e-5,5e-5,1e-4}\noptimizer\nAdam\nepochs (best of)\n30\nTable 5: Supervised ﬁne-tuning hyper-parameters.\nbatch size\n64\nlearning rate\n5e-5\noptimizer\nAdam\nwarmup\nlinear\nwarmup steps\n10% total\nepochs (best of)\n10\nTable 6: Unsupervised ﬁne-tuning hyper-parameters\nA.3\nBuckets : Detailed data and scores\nProportion Arabizi\n<60\n60-78\n78-100\n=100\n% of word in sentence\ntrain set number sents\n322\n286\n283\n276\nTable 7: Code-mixed Buckets\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-05-01",
  "updated": "2020-05-01"
}