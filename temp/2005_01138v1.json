{
  "id": "http://arxiv.org/abs/2005.01138v1",
  "title": "Off-Policy Adversarial Inverse Reinforcement Learning",
  "authors": [
    "Samin Yeasar Arnob"
  ],
  "abstract": "Adversarial Imitation Learning (AIL) is a class of algorithms in\nReinforcement learning (RL), which tries to imitate an expert without taking\nany reward from the environment and does not provide expert behavior directly\nto the policy training. Rather, an agent learns a policy distribution that\nminimizes the difference from expert behavior in an adversarial setting.\nAdversarial Inverse Reinforcement Learning (AIRL) leverages the idea of AIL,\nintegrates a reward function approximation along with learning the policy, and\nshows the utility of IRL in the transfer learning setting. But the reward\nfunction approximator that enables transfer learning does not perform well in\nimitation tasks. We propose an Off-Policy Adversarial Inverse Reinforcement\nLearning (Off-policy-AIRL) algorithm which is sample efficient as well as gives\ngood imitation performance compared to the state-of-the-art AIL algorithm in\nthe continuous control tasks. For the same reward function approximator, we\nshow the utility of learning our algorithm over AIL by using the learned reward\nfunction to retrain the policy over a task under significant variation where\nexpert demonstrations are absent.",
  "text": "OFF-POLICY ADVERSARIAL INVERSE REINFORCEMENT\nLEARNING\nA PREPRINT\nSamin Yeasar Arnob\nDepartment of Electrical and Computer Engineering\nMcGill University\nsamin.arnob@mail.mcgill.ca\nMay 5, 2020\nABSTRACT\nAdversarial Imitation Learning (AIL) is a class of algorithms in Reinforcement learning (RL), which\ntries to imitate an expert without taking any reward from the environment and does not provide expert\nbehavior directly to the policy training. Rather, an agent learns a policy distribution that minimizes the\ndifference from expert behavior in adversarial setting. Adversarial Inverse Reinforcement Learning\n(AIRL) leverages the idea of AIL, integrates a reward function approximation along with learning\nthe policy and shows the utility of IRL in the transfer learning setting. But the reward function\napproximator that enables transfer learning does not perform well in imitation tasks. We propose\nan Off-Policy Adversarial Inverse Reinforcement Learning (Off-policy-AIRL) algorithm which is\nsample efﬁcient as well as gives good imitation performance compared to the state-of-the-art AIL\nalgorithm in the continuous control tasks. For the same reward function approximator, we show the\nutility of learning our algorithm over AIL by using the learned reward function to retrain the policy\nover a task under signiﬁcant variation where expert demonstrations are absent.\nKeywords Reinforcement Learning · Inverse Reinforcement Learning · Transfer Learning\n1\nIntroduction\nReinforcement learning (RL) is a very useful framework for learning complex behavior in control, where an agent\ninteracts with an environment and tries to learn the expected behavior by optimizing an objective function that ensures\nthe highest cumulative reward over speciﬁc time steps. A reward is often human-engineered function that tries to ensure\nhigher rewards for expected behavior within the environment. Designing a reward function for a complex environment\ncan be tricky. Poorly designed reward function may lead to a fatal mistake when applied in real-world applications.\nGenerative Adversarial Imitation Learning (GAIL) [1] introduces AIL where a generator (policy) is used to compute\ntrajectory and a binary classiﬁer called discriminator classiﬁes generated behavior from the expert. As imitation learning\nalgorithms only learn policy, they miserably fail when there is a change in environmental dynamics.\nInverse reinforcement learning (IRL) [2, 3] addresses the importance of learning reward function and thus learn\nreward function approximator along with policy. Adversarial Inverse Reinforcement Learning (AIRL) [4] proposes an\nIRL algorithm in adversarial learning, which shows a promising result when there is considerable variability in the\nenvironment from the demonstration setting. But compared to AIL algorithms, AIRL does not perform well in imitation\nperformance [5, 4]. It is also important to note the best performing reward function approximator conﬁguration for\nimitation and transfer learning tasks in AIRL algorithm are different. In our work we perform transfer learning using\nthe same conﬁguration that works best in imitation performance.\nThe paper makes the following contributions: (i) we show the scope of using a stochastic policy (SAC) in DAC\nalgorithm, and it gives improved imitation performance (ablation study added on the appendix Figure 10). (ii) Ee\npropose an Off-Policy Adversarial Inverse Reinforcement Learning algorithm (off-policy-AIRL) which is sample\narXiv:2005.01138v1  [cs.LG]  3 May 2020\nA PREPRINT - MAY 5, 2020\nefﬁcient and improves imitation performance of prior IRL algorithms through comparing imitation performance with\nstate-of-the-art AIL algorithm on continuous control tasks. (iii) We show utility of learning IRL algorithm over imitation\nby reusing the learned reward function to train policy under certain changes in dynamics where expert demonstrations\nare absent. We use Soft Actor-Critic (SAC), which makes our algorithm more sample efﬁcient. (iv) We also show using\nMultiplicative Compositional Policies (MCP) allows more ﬂexibility in retraining policy in transfer task and at the same\ntime improves imitation performance.\n2\nBackground\nWe deﬁne a Markov decision process (MDP) as a tuple (S, A, T, r, γ, ρ0), where S,A are the state and action space\nrespectively, γ ∈[0, 1) is the discount factor. We consider the dynamic or transition distribution T(s′|s, a), initial state\ndistribution ρ0(s) and reward function r(s, a) are unknown in IRL setup and can be learned through interaction with\nthe MDP.\nAIL evolved from maximum casual entropy IRL framework [6], which considers an entropy regularized MDP with the\ngoal to ﬁnd the optimal policy π∗that maximizes the expected entropy-regularized discounted reward under π,T and\nρ0:\nπ∗= arg max\nπ\nEτ∼π\nh\nT\nX\nt=0\nγtr(st, at) + H(π(.|st))\ni\n.\n(1)\nHere τ = (s0, a0...sT , aT ) deﬁnes the sequence of states and actions induced by policy and H(π(.|st)) is the discounted\ncausal entropy of policy π. In imitation task, expert demonstrations D = τ1, τ2...τn are given and an agent is expected\nto produce trajectory which are similar to the expert and infer reward function. Under certain constrain of feature\nmatching [7] derived following objective function:\nPψ(τ) ∝eR(τ) = 1\nz eR(τ),\n∝p(s0)\nY\np(st+1|st, at)eγtrψ(st,at),\n(2)\nwhere z =\nR\nτ eR(τ) is the partition function and can be expressed as probability of trajectories with equal reward/cost\nare equal likely and probability of trajectory with higher rewards are exponentially more likely. Equation (2) can be\ninterpreted as solving a maximum likelihood problem: maxψEτ∼D[log pψ(τ)]. But the assumptions that are made in\n[6] are model / transition function T(s′|s, a) is known and applied on small state-action space.\nGAIL [1], a model-free algorithm, is the ﬁrst framework to draw connection between generative adversarial network\n(GAN) and imitation learning which works in continuous state-action space. GAIL uses a generative model or generator,\nG that acts as the policy. Purpose of the generator is to learn policy πθ such that the state-action visitation frequency\nρπθ is similar to expert’s ρπE without directly being provided with the expert demonstrations. A discriminator D is a\nbinary classiﬁer which tries to distinguish the data distribution of the generator ρπ from the expert’s ρπE. Objective for\na generator is to minimize the difference with expert visitation frequency, while a discriminator wants to maximize the\ndifference and thus derived the entropy regularized objective as following:\ndistanceminπθ maxDψ (ρπ, ρπE) = min\nπθ max\nDψ\nh\nEπθ[log Dψ(s, a)] + EπE[log(1 −Dψ(s, a))] −λH(πθ)\ni\n,\nwhere entropy term H is policy regularizer controlled by λ ≥0. GAIL uses two separate neural-networks to represent\ngenerator and discriminator. For any given state generator tries to take expert like action. Discriminator takes state-action\npairs as input and computes the likelihood of the input coming from an expert. Generator uses a reward function\nR(s, a) = −log Dψ(s, a) in order to train it’s network parameters.\nGAIL is sample efﬁcient in terms of number of expert demonstrations required but not quite sample efﬁcient in terms\nof required interaction with the environment to learn the policy. Thus GAIL is not suitable for many real-world\napplications. Discriminator Actor critic (DAC) [5] uses Twin-delayed deep deterministic actor-critic (TD3) [8], an\noff-policy algorithm as generator to improve upon the sample efﬁciency of existing methods, and it extends the learning\nenvironment with absorbing states. DAC criticises AIL algorithms (i.e GAIL) for inducing reward biases by using strict\npositive or negative reward function and proposes a reward function r(s, a) = log(Dθ(s, a)) −log(1 −Dθ(s, a)) in\norder to achieve unbiased rewards. Combined, these changes remove the survival bias and solves sample inefﬁciency\nproblem of GAIL.\n2\nA PREPRINT - MAY 5, 2020\nFor being an imitation learning algorithm, instead of learning cost/reward function, both GAIL and DAC only recover\nexpert policy and thus will fail miserably in dynamic environments. A discriminator function, that learns policy as well\nas the cost/reward function, is proposed by [9] by optimizing following objective for discriminator network:\nDψ =\npψ(τ)\npψ(τ) + q(τ)\n(a)\n=\n1/z ∗eRψ\n1/z ∗eRψ + q(τ).\n(3)\nAnd thus,\nLdis = max\nDψ\nh\nEτ∼p[log Dψ(τ)] + Eτ∼q[log(1 −Dψ(τ))]\ni\n,\nwhere p(τ) is the data distribution and q(τ) is the policy distribution and (a) consider derived objective function (2)\nfrom [6]. But [9] contains no experimental demonstration of imitation performance. Later on AIRL [4] shows, when\nwe consider a whole trajectory to compute discriminator update it suffers from high variance issue and that leads to a\npoor performing generator. Instead AIRL computes discriminator update using (s, a, s′) tuple:\nDψ,ω =\npψ,ω(s, a, s′)\npψ,ω(s, a, s′) + q(s, a),\n=\nefψ,ω(s,a,s′)\nefψ,ω(s,a,s′) + q(s, a),\n=\nefψ,ω(s,a,s′)\nefψ,ω(s,a,s′) + πθ(a|s),\n(4)\nwhere fψ,ω(s, a, s′) = rψ(s, a) + γΦω(s′) −Φω(s) is a potential-based reward function, rψ(s, a) is a reward function\napproximator and Φω is reward shaping term controlled by the dynamics. In [4] fψ,ω(s, a, s′) is referred as disentangled\nreward function [10] due to rψ(s) being indifferent to the changes in dynamics T.\n3\nOff-policy Adversarial Inverse Reinforcement Learning\nOff-policy-AIRL algorithm is inspired from DAC [5], which is an adversarial imitation learning algorithm. DAC\naddresses two existing problem with AIL algorithms, which are reward function bias and sample inefﬁciency. Reward\nfunctions used in these algorithms induce an implicit bias by using strict positive or negative reward functions, which\nmay work for some environments but in many cases become a reason for sub-optimal behavior.\nFurthermore, [5] shows adversarial methods improperly handle terminal states. This introduces implicit reward priors\nthat can affect the policy‘s performance. In particular, many imitation learning implementations [11, 1, 4] and MDPs\nomit absorbing states sa. Thus they implicitly assign 0 reward to terminal/absorbing states and adds a bias to the reward\nlearning process. Thus we use the same approach to learn the reward for absorbing states. Return of the terminal state is\ndeﬁned as RT = r(sT , aT ) + P∞\nt=T +1 γt−T r(sa, .), where r(sa, 0) is learned reward, instead of just RT = r(sT , aT ).\nTo make the implementation more stable, the terminal reward is analytically derived using the following equation:\nRT = r(sT , aT ) + γ r(sa, .)\n1 −γ\n(5)\nLike any other imitation learning algorithm DAC only learns the policy. We want to use the best of both worlds and thus\nreplace the binary discriminator of DAC and use formulated discriminator function from AIRL [4] to learn a reward\nfunction approximator along with the policy. From [4] it is evident that AIRL provides a poor imitation performance\ncompared to GAIL. It is important to note that the best performing policy in [4] for imitation task is trained using\nstate-action dependent reward function rψ(s, a), which fails to re-train policy in the transfer learning task. For state\ndependent reward function rψ(s) [4] successfully completes transfer learning task but it comes with a cost of poor\nimitation performance. In this work, we perform transfer learning using the same conﬁguration that works best in\nimitation performance. We improve upon the prior IRL algorithm performance and while still being sample efﬁcient by\nusing Soft-Actor-Critic (SAC) [12] as the generator.\nWe store our experience in a replay buffer R to utilize them during the off-policy update of the generator. After each\ntime the environment reaches a terminal condition and resets, we update our discriminator and generator for the same\nepisodic timesteps. We use an expert buffer RE to store the expert trajectory and sample the (sE, aE, sE′) pair in batch\nwhile update our discriminator.\n3\nA PREPRINT - MAY 5, 2020\n3.1\nDiscriminator update rule\nDuring discriminator update we randomly sample mini-batch of state-action-next-state pairs (s, a, ., s′)b from both\nexpert buffer RE and replay buffer R. We use following equation derived in [4] to compute the discriminator output,\nDψ,ω =\nefψ,ω(s,a,s′)\nefψ,ω(s,a,s′) + πθ(a|s).\n(6)\nwhere,\nfψ,ω = rψ(s) + γΦω(s′) −Φω(s),\n= rψ(s) + γVω(s′) −Vω(s).\n(7)\nHere reward approximator rψ can be a function of state, state-action or state-action-next-state and Φ can be any function\nthat gives a measure of being at any state s. Similar to [4] we use value function Vω as reward shaping terms. We ﬁnd\nthe best imitation performance using state dependent reward function.\nOutput of the discriminator will predict likelihood of being an expert and thus objective of the discriminator is to\nminimize following binary cross entropy loss:\nmin Lψ,ω = min\nB\nX\nb=1\n[−log Dψ,ω(sb, ab, s\n′\nb) −log(1 −Dψ,ω(sbE, abE, s\n′\nbE))].\n(8)\nWith in few iterations of the discriminator update, it easily can classify the expert from the generator data. Thus to\nmake the generator learning more stable we use gradient penalty [13, 5].\n3.2\nGenerator update rule\nIn IRL setting, we consider the actual reward from the environment is non-existent and rather formulate a reward\napproximator to train policy. AIRL [4] algorithm uses a on-policy policy-gradient algorithm as its generator and\nproposes following reward approximator ˆr(s, a, s′) to update the policy:\nˆrψ,ω(s, a, s′) = log Dψ,ω(s, a, s′) −log(1 −Dψ,ω(s, a, s′)).\n(9)\nUsing equation (6) it is also referred as entropy regularized reward function:\nˆrψ,ω(s, a, s′) = log\nefψ,ω(s,a,s′)\nefψ,ω(s,a,s′) + π(a|s) −log\nπθ(a|s)\nefψ,ω(s,a,s′) + πθ(a|s),\n= fψ,ω(s, a, s′) −log πθ(a|s).\nFor improving imitation performance and sample efﬁciency we use SAC as our generator. SAC being an off-policy\nalgorithm, samples (s, a, s′, done) tuple from buffer when trains the network. For our algorithm we experiment different\nreward approximator which is discussed in following section and use trained reward approximator to update critic or\nQφ function in SAC with entropy term through following gradient step:\n∇φJQ(φ) = ∇θQφ(st, at)\nh\nQφ(st, at) −ˆrψ,ω(s, a, s′) + γQφ′(st+1, at+1) −α log πθ(at+1|st+1)\ni\n.\nSimilar to [8, 12] we update actor or policy πθ for every second update of our Qφ function using following gradient\nstep:\n∇θ[Eat∼πθ[α log πθ(at|st) −Qφ(st, at)]].\n(10)\n3.3\nReward function selection\nImplementation of AIRL discriminator [4] does not work off the shelf as now we have to train an off-policy generator.\nWe experiment on different way to compute the reward function for the generator update.\nIn AIRL, the author uses the value function of the states Vω(s) to measure additional shaping terms. We experiment on\ndifferent variations of reward approximator ˆrψ,ω to update policy and disentangled reward function fψ,ω to compute\n4\nA PREPRINT - MAY 5, 2020\ndiscriminator output so that we ﬁnd the combination of these two approximators that gives the best performance in\noff-policy-AIRL.\nImplementation 1:\n• Policy update using : ˆrψ,ω(s, a, s′) = log Dψ,ω(s, a, s′) −log(1 −Dψ,ω(s, a, s′))\n• Disentangled reward function : fψ,ω = rψ(s, a) + γVω(s′) −Vω(s)\nImplementation 2\n• Policy update using : ˆrψ,ω(s, a, s′) = log Dψ,ω(s, a, s′) −log(1 −Dψ,ω(s, a, s′))\n• Disentangled reward function : fψ,ω = rψ(s) + γVω(s′) −Vω(s)\nImplementation 3\n• Policy update using : ˆrψ,ω(s, a, s′) = rψ(s, a)\n• Disentangled reward function : fψ,ω = rψ(s, a) + γVω(s′) −Vω(s)\nImplementation 4\n• Policy update using : ˆrψ,ω(s, a, s′) = rψ(s)\n• Disentangled reward function : fψ,ω = rψ(s) + γVω(s′) −Vω(s)\nFor implementation (1) and (2), we use the exact policy update rule which is used in AIRL [4]. In implementation\n(1) we consider a state dependent reward function rψ(s) and for implementation (2) we consider reward to be a\nfunction of state-action pair rψ(s, a). As we see from equation (9), reward computed to update policy is equivalent\nto entropy regularized reward function. We evaluate the disentangled reward function for both state dependent rψ(s)\nand state-action dependent rψ(s, a). For implementation (3) and (4) we follow similar comparative study where policy\nupdate discard entropy regularization and use direct output from our reward approximator rψ. As discussed in DAC [5],\nstrict negative or positive reward approximator induces bias. Similar to DAC, our reward approximator rψ gives both\npositive and negative rewards thus does not suffer reward biasing.\n4\nTransfer learning task\nAdvantage of IRL over imitation learning is that we can leverage the learned reward function to retrain a new policy.\nIf the reward function captures the underlying objective of an agent, then it is possible to use the reward function\nunder robust changes of the dynamics. Imitation learning successfully learn policy in the training domain but it is\nnot possible to use the policy when there is a signiﬁcant domain shift [4]. We use the learned reward function from\noff-policy-AIRL algorithm to re-train our policy in transfer learning setting. To the best of our knowledge AIRL [4]\nis the only IRL framework that shows utility of reward function in transfer learning for continuous control tasks and\nthus we demonstrate our experiments using same environments from [4] (see Figure 1) and we also consider expert\ndemonstrations to be absent for the transfer tasks.\nFor transfer learning experiments we consider dynamic changes in following two criterion, where either (1) dynamics\nof the agent or (2) dynamics of the environment changes while the goal/objective of the agent and action dimension\nremain the same. Through theses experiments we show the learned reward function rψ indeed can help the policy to\nbehave under dynamic changes even when expert behavior is completely absent. For transfer learning experiments we\nuse the same hyper-parameter and update rules described in prior.\n4.1\nCriterion:1\nBy changing dynamics of an agent, we refer to a signiﬁcant structural change of an agent. To evaluate transfer learning\nperformance under this criterion we use Customized Ant [4] environment. In both imitation and transfer task the\nquadrupedal Ant requires to run forward, while dynamics of the agent during test session is changed by disabling and\nshrinking two legs. In MuJoCo setting, [4] sets the gear ratios to 1, whereas the other joints were set to 150. The gear\nratio is a parameter which translates actions to torques on the joints, so the disabled joints are ∼150x more difﬁcult to\nmove. This signiﬁcantly changes its gait. In a MDP this puts a restriction in action space A, which results in a new\ntransition dynamics T ′(s|s, a). Thus optimal policy π∗is changed even though the objective and action-dimension\nremains the same.\n5\nA PREPRINT - MAY 5, 2020\n4.2\nCriterion:2\nBy changing dynamics of the environment, we consider notable variation in the trained environment such that it directly\nresults in a different transition dynamics T ′(s|s, a) without putting restriction in action-space A. Objective of Shifting\nMaze [4] task is to reach to a speciﬁc goal position, where during imitation learning, agent is trained to go around the\nwall on the left side, but during transfer task environment setup is changed and must go around the wall on the right.\nThus optimal policy π∗is changed even though the objective and the dynamics of the agent remains same.\nFigure 1: Top row: Ant environment is used to perform experiment under criterion 1 Bottom row: For shifting maze\ntask, the agent (blue) is required to reach at the goal (green) and is being used for experiment under criterion 2\n5\nMultiplicative Compositional Policies\nPolicy often gets overﬁtted over one task and thus become harder to re-train during transfer learning task. Different\ntechniques such as early stopping or regularization can be helpful in this case. An alternative solution is provided by\n[14] through using variational bottleneck technique to control over the information that is fed to the discriminator\nusing mutual information theory, which allows more controlled update in the generator. But this technique is highly\ndependent on hyper-parameter of the bottleneck itself, thus performance can be varied vastly if not tuned properly for\neach task. We want to learn a policy that gives a good performance during transfer learning without being dependent on\nexact hyper-parameter tuning. We propose using Multiplicative Compositional Policy (MCP) [15] to improve policy\nperformance in transfer learning task. MCP was introduced to combine multiple primitive skills in RL setting and these\nprimitve skills are controlled using a gating function. We propose using multiple primitive networks to learn single skill\nin IRL setting. Using a weighted multiplicative composition of Gaussian primitives improves imitation performance\nand allows more ﬂexibility in composing into new policy in transfer learning.\nMCP [15] tries to learn different skills with different primitive policies and then reuse those learned skills by combining\nthem to do a more sophisticated task. In hierarchical learning [16, 17, 18, 19] it is common to learn premitives and then\nﬁnd a composite policy π(a|s) by weighted sum of distribution from premitives πi(a|s). A gating function computes\nthe weight wi, that determines the probability of activating each premitive for a given state, s. Composite policy can be\nwritten as:\nπ(a|s) =\nk\nX\ni=1\nwi(s)πi(a|s),\nk\nX\ni=1\nwi(s) = 1, wi(s) > 0.\n(11)\nHere k denotes the number of primitives and this can be referred as additive model. Standard hierarchical learning\nmodels can sequentially select particular skill over time. In other words, can activate single primitive at each timestep.\nMCP [15] proposes an multiplicative model, where multiple primitives are activated at a given time-step. This allows an\nagent to learn a complex task requiring to perform more than one sub-task at a time.\nMCP [15] decomposes agent’s policy into premitives, train those primitive policies on different sub-tasks and then\nobtain a composite-policy by multiplicative composition of these premitives. Here each primitive is considered as\ndistribution over actions and the composite policy is a multiplicative composition of these distribution. Thus,\nπ(a|s, g) =\n1\nZ(s, g)\nk\nY\ni=1\nπi(a|s, g)wi(s,g), wi(s, g) ≥0.\n(12)\nMCP enables an agent to activate multiple primitives simultaneously with each primitive specializing in different\nbehaviors that can be composed to produce a continuous spectrum of skills, whereas, standard hierarchical algorithm\nonly activate single primitive at a time.\n6\nA PREPRINT - MAY 5, 2020\n6\nMCP in off-policy-AIRL\nMCP learns premitive policies πi by training them into different sub-tasks and then try to learn more complex task\nleveraging the learned skills. But in transfer learning experiments, we use MCP to learn single task. The underlying\nidea is to observe whether it can relearn the composition of different action parameters when we put them in dynamic\nscenarios. For example, during transfer learning experiments [4] disables two legs and thus it is important for the agent\nto re-optimise it’s policy to walk in this transfer learning setup. Our initial objective is to see if MCP can decompose\nmotor skills for a single task and re-optimise the learned policy by re-training the gating function. We refer this new\npolicy as SAC-MCP.\nIn MCP [15], experiments were conducted using on-policy algorithms in RL setting and the policy parameters were\nconsidered to be state and goal dependent. We explore this concept to decompose motor skill over single task in IRL\nsetting using off-policy algorithm.\n512\n256\n256\n256\n256\n256\nstate input\nrelu\ntanh\npremitives\nActor\n512\n256\nstate input\n256\nsigmoid\nw\nGating function\nk\nk\nNo of primitive action\nFigure 2: Generator architecture for SAC-MCP.\n7\nExperimental Setup\nSimilar to [1, 5] discriminator is 2 layer MLP of 100 hidden units with tanh activation. Our generator consists of\nseparate Actor and Critic neural network and follows the architecture used in [5, 8], where both of these networks have\n2 layer MLP of 400 and 300 hidden units with ReLU activation. To implement SAC-MCP, we modify Actor network\n(see Figure 2) by adding multiple premitive networks with same conﬁguration described in [15].\nWe have trained all networks with the Adam optimizer [20] from PyTorch, which uses default learning rate of 1e−3. For\nour experiment we trained our algorithms on MuJoCo [21] continuous control task environments. For transfer learning\nexperiments we use Custom-Ant and Shifting Maze environments from [4]. Performance curve is obtained using the\nmean over 10 experiments for 0-9 seeds and evaluated after each 5000 interaction with the environment. During each\nevaluation we have stored the average performance of 10 runs.\n8\nResults\n8.1\nReward function selection\nWe conduct experiment on CustomAnt-v0 [4] to select reward function for policy update. We train SAC policy and\ncollected 50 trajectory from the expert. We keep our seed ﬁxed to 0. Figure 3(a) gives the performance curve using\nactual reward from the environment and it is evident that reward signal directly from the reward function approximator\n(Implementation 4) gives better policy performance in imitation task. We also see the cumulative reward achieved from\nreward approximator rθ (see Figure 3(b)) replicates the actual performance curve. As our experiment shows drastic\ndifference in performance we have not done comparative study for multiple seeds or other environments.\n7\nA PREPRINT - MAY 5, 2020\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nCumulative Episodic Reward\nEnvironment: CustomAnt-v0\nImplementation: 1\nImplementation: 2\nImplementation: 3\nImplementation: 4\nExpert\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n12000\n10000\n8000\n6000\n4000\n2000\n0\n2000\nCumulative Episodic Reward\nEnvironment: CustomAnt-v0\nImplementation: 1\nImplementation: 2\nImplementation: 3\nImplementation: 4\nFigure 3: Performance comparison of Off-policy AIRL for different update rules. Here ﬁgure (a) gives the performance\ncurve using actual reward from the environment (b) shows the cumulative reward achieved from reward approximator rθ\nAs demonstrated in Figure 4 using implementation-4 off-policy AIRL gives a better imitation performance than\nDAC in Hopper-v2 and Walker2d-v2 environments within 1e6 iterations, while provides comparable performance for\nHalfCheetah-v2 and Ant-v2 environments after 2e6 iterations.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTimesteps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Rewards\nEnvironment-HalfCheetah-v2\nDAC (Generator: TD3)\nOff-Policy-AIRL(Generator:SAC- fixed temp)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTimesteps\n1e6\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nCumulative Rewards\nEnvironment-Ant-v2\nDAC (Generator: TD3)\nOff-Policy-AIRL(Generator:SAC- learned temp)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTimesteps\n1e6\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Rewards\nEnvironment-Hopper-v2\nDAC (Generator: TD3)\nOff-Policy-AIRL(Generator:SAC- fixed temp)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTimesteps\n1e6\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nCumulative Rewards\nEnvironment-Walker2d-v2\nDAC (Generator: TD3)\nOff-Policy-AIRL(Generator:SAC- fixed temp)\nFigure 4: Performance Comparison of DAC and Off-policy AIRL over continuous control task\n8.2\nPerformance in transfer learning\nWe test trained agent in transfer learning task under criterion (1) and (2). We demonstrate the utility of learning reward\nfunction which gives us an advantage of learning IRL over imitation learning. In IRL setting, Off-policy AIRL learns\nexpert like behavior in Custom Ant environment but suffers from variance in Shifting maze task. We do not tune\nseparate hyper-parameter for individual environment but doing so may reduce performance variance for Shifting maze\nenvironment.\nAs illustrated in Figure 1 during transfer learning under criterion-1, agent’s gait changes signiﬁcantly and as can be\nseen in Figure 4 off-policy-AIRL fails to re-learn the policy using reward function. But we do not conclude that it is not\n8\nA PREPRINT - MAY 5, 2020\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime Steps\n1e6\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Rewards\nEnvironment-CustomAnt-v0\nOff-policy-AIRL(Generator:SAC learned temp)\nExpert: SAC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime Steps\n1e6\n4\n3\n2\n1\nCumulative Rewards\nEnvironment-PointMazeLeft-v0\nOff-policy-AIRL(Generator:SAC learned temp)\nExpert: SAC\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime Steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Rewards\nEnvironment-DisabledAnt-v0\nOff-policy AIRL (Generator: SAC learned temperature)\nExpert: SAC\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime Steps\n1e6\n4\n3\n2\n1\n0\nCumulative Rewards\nEnvironment-PointMazeRight-v0\nOff-policy-AIRL(Generator:SAC learned temp)\nExpert: SAC\nFigure 5: Top row:Performance Off-policy AIRL over continuous control task Bottom row:Performance Off-policy\nAIRL during transfer learning task.\npossible to relearn policy under this criterion using off-policy-AIRL. It is demonstrated in [22] that changing a better\nstructure of agent’s body not only is better suited for the task but also facilitates policy learning. The environment that\nwe use in criterion-1 was introduced in [4], where the structure of the body of the quadrupedal Ant was changed [4] for\ntransfer learning by shrinking two legs and making other two larger. But the speciﬁc increment or decrement of the legs\nmay have favored for [4] to re-learn policy when two legs are paralyzed in transfer learning. A hyper-parameter tuning\nof the structural change may work for our proposed algorithm as well. Experiments with more structural ﬂexibility are\nrequired to come to a solid conclusion under this criterion.\nOn the other hand for Shifting maze environment (see Figure 4) it successfully completes the task but suffer from high\nvariance in performance.\n8.3\nPerformance comparison for using MCP\nPerformance of our MCP implementation in IRL setting is shown in Figure 6. Using SAC-MCP improves the imitation\nperformance in multiple (HalfCheetah-v2 and Ant-v2) MuJoCo control task.\nWe again conduct our transfer learning experiments on Shifting-Maze tasks. Here we do not load prior gating function.\nWe compare performance with and without re-training the actor. Our initial hypothesis is that having multiplicative\ncomposition of Gaussian primitives should allow us with diverse behavior by only training the gating function during\ntransfer task. But Figure 7 shows we are not able to learn shifting maze task without retraining the actor networks. We\ncompare performance of SAC-MCP for k = {4, 8} premitives (see Figure 7). For K = 8 it reduces the performance\nvariance suffered by SAC.\n8.4\nRandom State Initialization\nWe initialize the environment from a random state. As we retrain already learned policy over one task, it does not\ngive much exploratory action. Thus initializing environment at different state will help to explore the new task. We\nuse expert agent to run for 0-100 steps and then stop at random, thus enabling the learning agent to start at random\nstate every time the environment resets. It allows an agent to explore from a state that is expected to be visited by an\n9\nA PREPRINT - MAY 5, 2020\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime Steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nCumulative Rewards\nEnvironment-HalfCheetah-v2\nOff-policy-AIRL(Generator: SAC)\nOff-policy-AIRL(Generator: SAC-MCP k=8)\nExpert: SAC\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime Steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nCumulative Rewards\nEnvironment-Ant-v2\nOff-policy-AIRL(Generator: SAC)\nOff-policy-AIRL(Generator: SAC-MCP k=8)\nExpert: SAC\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime Steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nCumulative Rewards\nEnvironment-Hopper-v2\nOff-policy-AIRL(Generator: SAC)\nOff-policy-AIRL(Generator: SAC-MCP k=8)\nExpert: SAC\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime Steps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nCumulative Rewards\nEnvironment-Walker2d-v2\nOff-policy-AIRL(Generator: SAC)\nOff-policy-AIRL(Generator: SAC-MCP k=8)\nExpert: SAC\nFigure 6: Performance Comparison of SAC and SAC-MCP as generator in off-policy-AIRL over continuous control\ntask.\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime Steps\n1e6\n6\n5\n4\n3\n2\n1\n0\nCumulative Rewards\nEnvironment-PointMazeRight-v0\nOff-policy-AIRL(Generator:SAC learned temp)\nOff-policy-AIRL(Generator:SAC-MCP learned temp K = 8 )\nOff-policy-AIRL(Generator:SAC-MCP learned temp K = 4 )\nExpert: SAC\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime Steps\n1e6\n5\n4\n3\n2\n1\n0\nCumulative Rewards\nEnvironment-PointMazeRight-v0\nOff-policy-AIRL(Generator:SAC learned temp)\nOff-policy-AIRL(Generator:SAC-MCP learned temp K = 8 )\nOff-policy-AIRL(Generator:SAC-MCP learned temp K = 4 )\nExpert: SAC\nFigure 7: Performance of off-policy-AIRL (Generator: SAC-MCP) during transfer learning. Left:Train only gating\nfunction. Right:Train both policy and gating function\nexpert. At the same time, using learned policy (from prior imitation task) to take actions will result into better quality of\nsequential observations. We see in Figure 8 random initialization improves performance and also reduces performance\nvariance for both SAC and SAC-MCP on transfer learning task for shifting maze environment.\n10\nA PREPRINT - MAY 5, 2020\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime Steps\n1e6\n5\n4\n3\n2\n1\nCumulative Rewards\nEnvironment-PointMazeRight-v0\nOff-policy-AIRL(Generator:SAC learned temp)\nOff-policy-AIRL(Generator:SAC-MCP learned temp K = 8 )\nOff-policy-AIRL(Generator:SAC-MCP learned temp K = 4 )\nExpert: SAC\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nTime Steps\n1e6\n4\n3\n2\n1\nCumulative Rewards\nEnvironment-PointMazeRight-v0\nOff-policy-AIRL(Generator:SAC learned temp)\nOff-policy-AIRL(Generator:SAC-MCP learned temp K = 8 )\nOff-policy-AIRL(Generator:SAC-MCP learned temp K = 4 )\nExpert: SAC\nFigure 8: Performance Comparison of SAC and SAC-MCP as generator during transfer learning with random state\ninitialization. Left:Train only gating function of SAC-MCP. Right:Train both policy and gating function of SAC-MCP\n9\nDiscussion and Conclusion\nWe propose an off-policy IRL algorithm in adversarial learning which learns policy using its approximated reward\nfunction. We use SAC as generator to improve imitation performance while still being sample efﬁcient. We compare\ndifferent way to compute reward functions for off-policy generator update and experimentally ﬁnd simple state\ndependent reward network without entropy regularization works best for our algorithm. We compare our imitation\nperformance with state-of-the-art imitation learning algorithm to show our persistent expert like performance over\ncontinuous control environments.\nWe experimentally demonstrate the utility of learned reward function using off-policy-AIRL when the expert demon-\nstrations are non-existent in transfer learning. We compare performance over two transfer learning criterion, where\neither (1) the dynamics of the agent or (2) the dynamics of the environment changes while the goal/objective and action\ndimension remain the same.\nFurthermore, we propose using MCP model to learn single primitive to achieve more ﬂexibility in retraining policy\nin transfer learning. We observe an improved performance for using SAC-MCP on both imitation performance and\ntransfer learning. We also conduct experiments under random state initialization, which helps the agent to explore from\nan state that is expected to be visited in the new environment and reduces performance variance over prior experiments.\nBut unfortunately, our algorithm (using both SAC and SAC-MCP as generator) fails when the (criterion 1) dynamics\nof the agent is changed in transfer learning. The quadrupedal Ant has a very speciﬁc increment and decrement of the\nlegs, which may have favored for [4] to re-learn policy when two legs are paralyzed in transfer learning. As speciﬁc\nbody structure of an agent can facilitate learning policy over a task [22], experiments using environments with more\nstructural ﬂexibility is required to come to a conclusion under this criterion.\n10\nAcknowledgements\nThe author thank Aditya Mahajan for reviewing the paper and providing valuable feedback. Implementation of\nDiscriminator Actor-Critic algorithm used in this paper is initially implemented as ICLR-2019 Reproducibility\nChallenge along with Sheldon Benard and Vincent Luczkow [23]. The author also thank Ilya Kostrikov for looking into\nthe code and helping further on the implementation. The code that is used in this paper is an extension of the project.\nExperiments and respective results mentioned are computed for the purpose of this paper.\nReferences\n[1] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. CoRR, abs/1606.03476, 2016.\n[2] Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and\napplication to reward shaping. In ICML, volume 99, pages 278–287, 1999.\n[3] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of\nthe twenty-ﬁrst international conference on Machine learning, page 1. ACM, 2004.\n11\nA PREPRINT - MAY 5, 2020\n[4] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforcement learning.\nIn International Conference on Learning Representations, 2018.\n[5] Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson. Discriminator-\nactor-critic: Addressing sample inefﬁciency and reward bias in adversarial imitation learning. In International\nConference on Learning Representations, 2019.\n[6] Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of maximum causal\nentropy. 2010.\n[7] Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement\nlearning. 2008.\n[8] Scott Fujimoto, Herke van Hoof, and Dave Meger. Addressing function approximation error in actor-critic\nmethods. CoRR, abs/1802.09477, 2018.\n[9] Chelsea Finn, Paul F. Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative adversarial\nnetworks, inverse reinforcement learning, and energy-based models. CoRR, abs/1611.03852, 2016.\n[10] Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations: Theory and\napplication to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning,\nICML ’99, pages 278–287, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc.\n[11] Kee-Eung Kim and Hyun Soo Park. Imitation learning via kernel mean embedding. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n[12] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry\nZhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algorithms and applications. CoRR,\nabs/1812.05905, 2018.\n[13] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training\nof wasserstein gans. In Advances in neural information processing systems, pages 5767–5777, 2017.\n[14] Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine. Variational discriminator\nbottleneck: Improving imitation learning, inverse rl, and gans by constraining information ﬂow. arXiv preprint\narXiv:1810.00821, 2018.\n[15] Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. MCP: learning composable\nhierarchical control with multiplicative compositional policies. CoRR, abs/1905.09808, 2019.\n[16] Petros Faloutsos, Michiel Van de Panne, and Demetri Terzopoulos. Composable controllers for physics-based\ncharacter animation. In Proceedings of the 28th annual conference on Computer graphics and interactive\ntechniques, pages 251–260. ACM, 2001.\n[17] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal\nabstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):181–211, 1999.\n[18] Michael I Jordan and Robert A Jacobs. Hierarchies of adaptive experts. In Advances in neural information\nprocessing systems, pages 985–992, 1992.\n[19] Xue Bin Peng, Glen Berseth, and Michiel Van de Panne. Terrain-adaptive locomotion skills using deep reinforce-\nment learning. ACM Transactions on Graphics (TOG), 35(4):81, 2016.\n[20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n[21] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012\nIEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012.\n[22] David Ha. Reinforcement learning for improving agent design. 2018. https://designrl.github.io.\n[23] Vincent Luczkow Sheldon Benard and Samin Yeasar Arnob. Iclr 2019 reproducibility-challenge discriminator-\nactor-critic: Addressing sample inefﬁciency and reward bias in adversarial imitation learning. ICLR 2019\nREPRODUCIBILITY-CHALLENGE, December 2018.\n[24] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and\nMartin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n12\nA PREPRINT - MAY 5, 2020\n11\nAppendix: Off-Policy Adversarial Inverse Reinforcement Learning\n11.1\nOff-policy-AIRL algorithm\nAlgorithm 1 off-policy-AIRL Algorithm\n1: Input: Expert Replay Buffer RE\n2: while n ̸= 106 do\n3:\nepisodic timestep = 0\n4:\nwhile not done do\n5:\na ←π(a|s)\n6:\nR ←R ∪s, a, ., s′\n7:\nepisodic timestep += 1\n8:\nn += 1\n9:\nif done WarpAbsorbingState\n10:\nfor episodic timesteps do\n11:\nsample random mini-batch {(sb, ab, ., s\n′\nb)}B\nb=1 ∈R\n12:\nsample random mini-batch {(sbE, abE, ., s\n′\nbE)}B\nb=1 ∈RE\n13:\nCompute loss: Lψ,ω = PB\nb=1[−log Dψ,ω(sb, ab, s\n′\nb) −log(1 −Dψ,ω(sbE, abE, s\n′\nbE))]\n14:\nupdate Dψ,ω + gradient-penalty\n15:\nfor episodic timesteps do\n16:\nsample random mini-batch {(sb, ab, ., s\n′\nb)}B\nb=1 ∈R\n17:\nuse current reward function {(sb, ab, rb, s\n′\nb)}B\nb=1 ←rψ(sb)\n18:\nupdate SAC\n11.2\nMore Experimental Details\n11.2.1\nExpert Data Collection\nWe require expert demonstrations to train policy in imitation task. So we train TD3 [8], SAC[12] to compare the\nperformance. To collect the expert data we run experiments on MuJoCo continuous control task and also on environments\nused in [4] in RL setting (consider reward function available from the environment) for 1 million iterations. The\ntemperature parameter α controls the stochasticity in SAC, thus we experiment treating the temperature as both ﬁxed\nand learnable parameter. We select 0.2 as the ﬁxed temperature value for SAC. We get the performance curve by seeding\n10 experiment from 0-9.\nAs can be seen from Figure 9, except for Ant-v2 in MuJoCo tasks, keeping the temperature value ﬁxed for SAC\ngives better performance and for [4] environments SAC with learned temperature seem to perform with marginal\nimprovement. Thus we select SAC as our expert for all the following experiments.\nWe collect 50 trajectories from 5 best performing expert policies. As the policy update is done by random sampling\nfrom the buffer, that itself creates randomness and help the learning policy to better generalize on the task [24]. We use\nthe same hyper-parameters (i.e. temperature) that gives the best results for the expert for all the experiments discussed\nin this paper.\nEnvironment\nAvg returns over seed 0-9\nTemperature parameter\nHalfCheetah-v2\n11083\nﬁxed 0.2\nAnt-v2\n4294\nlearned\nHopper-v2\n3544\nﬁxed 0.2\nWalker2d-v2\n4490\nﬁxed 0.2\nCustomAnt-v0\n2430\nﬁxed 0.2\nDisabledAnt-v0\n1089\nlearned\nPointMazeLeft-v0\n-7.37\nlearned\nPointMazeRight-v0\n-7.33\nlearned\nTable 1: Expert Average Performance over 0-9 seed after 1e6 iterations.\n13\nA PREPRINT - MAY 5, 2020\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n2000\n4000\n6000\n8000\n10000\n12000\nCumulative Rewards\nEnvironment-HalfCheetah-v2\nTD3\nSAC (learned temperature)\nSAC (fixed temperature)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nCumulative Rewards\nEnvironment-Ant-v2\nTD3\nSAC (learned temperature)\nSAC (fixed temperature)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nCumulative Rewards\nEnvironment-Hopper-v2\nTD3\nSAC (learned temperature)\nSAC (fixed temperature)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n0\n1000\n2000\n3000\n4000\n5000\n6000\nCumulative Rewards\nEnvironment-Walker2d-v2\nTD3\nSAC (learned temperature)\nSAC (fixed temperature)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n500\n1000\n1500\n2000\n2500\nCumulative Rewards\nEnvironment-CustomAnt-v0\nSAC (learned temperature)\nSAC (fixed temperature)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n200\n400\n600\n800\n1000\n1200\nCumulative Rewards\nEnvironment-DisabledAnt-v0\nSAC (learned temperature)\nSAC (fixed temperature)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n25\n20\n15\n10\nCumulative Rewards\nEnvironment-PointMazeRight-v0\nSAC (learned temperature)\nSAC (fixed temperature)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n30\n25\n20\n15\n10\nCumulative Rewards\nEnvironment-PointMazeLeft-v0\nSAC (learned temperature)\nSAC (fixed temperature)\nFigure 9: Performance of policy gradient algorithms (TD3, SAC) over continuous control task in RL setting.\n11.2.2\nPerformance of DAC with Different Policies\nNo ablation study is conducted in [5] with different policy as generator. Thus we also investigate the performance of\nDAC with SAC as generator. SAC helps us with providing a entropy regularized objective function like GAIL [1]. Our\n14\nA PREPRINT - MAY 5, 2020\nexperiment using SAC in Figure 10 also proves the sample efﬁciency of DAC. Also it is important to have a stochastic\npolicy to compute discriminator output (Dψ,ω =\nefψ,ω(s,a,s′)\nefψ,ω(s,a,s′)+πθ(a|s)) in our off-policy-AIRL algorithm.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Rewards\nEnvironment-HalfCheetah-v2\nDAC (Generator: TD3)\nDAC (Generator: SAC- fixed temperature)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nCumulative Rewards\nEnvironment-Ant-v2\nDAC (Generator: TD3)\nDAC (Generator: SAC- fixed temperature)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Rewards\nEnvironment-Hopper-v2\nDAC (Generator: TD3)\nDAC (Generator: SAC- fixed temperature)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTimesteps\n1e6\n0.2\n0.4\n0.6\n0.8\n1.0\nCumulative Rewards\nEnvironment-Walker2d-v2\nDAC (Generator: TD3)\nDAC (Generator: SAC- fixed temperature)\nFigure 10: Performance of DAC for different generator (TD3, SAC).\n11.3\nFurther details to compute premitives in MCP\nEach primitive πi(a|s, g) = N\n\u0000µi(s, g), Σi(s, g)\n\u0001\nis modeled by a Gaussian with mean µi(s, g) and diagonal covari-\nance matrix Σi(s, g) = diag\n\u0000σ1\ni (s, g), σ2\ni (s, g)...σ|A|\ni\n(s, g)\n\u0001\n, where σj\ni (s, g) denotes variance of jth action parameter\nfrom primitive i and |A| represents the dimensionality of the action space. A multiplicative comparison of Gaussian\nprimitives yields yet another Gaussian policy π(a|s, g) = N\n\u0000µi(s, g), Σi(s, g)\n\u0001\n. Since the primitives construct each\naction parameter with an independent Gaussian, the action parameters of the composite policy π will also assume the\nform of independent Gaussians with component-wise mean µj(s, g) and variance σj(s, g). Component-wise mean and\nvariance can be written as:\nµj(s, g) =\n1\nPk\nl=1\nwl(s,g)\nσj\nl (s,g)\nk\nX\ni=1\nwi(s, g)\nσj\ni (s, g)\nµj\ni(s, g),\n(13)\nσj(s, g) =(\nX wi(s, g)\nσj\ni (s, g)\n)−1.\n(14)\n15\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.RO",
    "stat.ML"
  ],
  "published": "2020-05-03",
  "updated": "2020-05-03"
}