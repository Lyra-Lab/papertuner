{
  "id": "http://arxiv.org/abs/2005.01259v2",
  "title": "Noise Pollution in Hospital Readmission Prediction: Long Document Classification with Reinforcement Learning",
  "authors": [
    "Liyan Xu",
    "Julien Hogan",
    "Rachel E. Patzer",
    "Jinho D. Choi"
  ],
  "abstract": "This paper presents a reinforcement learning approach to extract noise in\nlong clinical documents for the task of readmission prediction after kidney\ntransplant. We face the challenges of developing robust models on a small\ndataset where each document may consist of over 10K tokens with full of noise\nincluding tabular text and task-irrelevant sentences. We first experiment four\ntypes of encoders to empirically decide the best document representation, and\nthen apply reinforcement learning to remove noisy text from the long documents,\nwhich models the noise extraction process as a sequential decision problem. Our\nresults show that the old bag-of-words encoder outperforms deep learning-based\nencoders on this task, and reinforcement learning is able to improve upon\nbaseline while pruning out 25% text segments. Our analysis depicts that\nreinforcement learning is able to identify both typical noisy tokens and\ntask-specific noisy text.",
  "text": "Noise Pollution in Hospital Readmission Prediction:\nLong Document Classiﬁcation with Reinforcement Learning\nLiyan Xu†\nJulien Hogan‡\nRachel Patzer‡\nJinho D. Choi†\n†Department of Computer Science, ‡Department of Surgery\nEmory University, Atlanta, US\n{liyan.xu,julien.hogan,rpatzer,jinho.choi}@emory.edu\nAbstract\nThis paper presents a reinforcement learning\napproach to extract noise in long clinical doc-\numents for the task of readmission prediction\nafter kidney transplant. We face the challenges\nof developing robust models on a small dataset\nwhere each document may consist of over 10K\ntokens with full of noise including tabular text\nand task-irrelevant sentences. We ﬁrst exper-\niment four types of encoders to empirically\ndecide the best document representation, and\nthen apply reinforcement learning to remove\nnoisy text from the long documents, which\nmodels the noise extraction process as a se-\nquential decision problem. Our results show\nthat the old bag-of-words encoder outperforms\ndeep learning-based encoders on this task, and\nreinforcement learning is able to improve upon\nbaseline while pruning out 25% text segments.\nOur analysis depicts that reinforcement learn-\ning is able to identify both typical noisy tokens\nand task-speciﬁc noisy text.\n1\nIntroduction\nPrediction of hospital readmission has always been\nrecognized as an important topic in surgery. Pre-\nvious studies have shown that the post-discharge\nreadmission takes up tremendous social resources,\nwhile at least a half of the cases are preventable\n(Basu Roy et al., 2015; Jones et al., 2016). Clini-\ncal notes, as part of the patients’ Electronic Health\nRecords (EHRs), contain valuable information but\nare often too time-consuming for medical experts\nto manually evaluate. Thus, it is of signiﬁcance to\ndevelop prediction models utilizing various sources\nof unstructured clinical documents.\nThe task addressed in this paper is to predict 30-\nday hospital readmission after kidney transplant,\nwhich we treat it as a long document classiﬁcation\nproblem without using speciﬁc domain knowledge.\nThe data we use is the unstructured clinical docu-\nments of each patient up to the date of discharge.\nIn particular, we face three types of challenges in\nthis task. First, the document size can be very long;\ndocuments associated with these patients can have\ntens of thousands of tokens. Second, the dataset\nis relatively small with fewer than 2,000 patients\navailable, as kidney transplant is a non-trivial med-\nical surgery. Third, the documents are noisy, and\nthere are many target-irrelevant sentences and tabu-\nlar data in various text forms (Section 2).\nThe lengthy documents together with the small\ndataset impose a great challenge on representation\nlearning. In this work, we experiment four types\nof encoders: bag-of-words (BoW), averaged word\nembedding, and two deep learning-based encoders\nthat are ClinicalBERT (Huang et al., 2019) and\nLSTM with weight-dropped regularization (Merity\net al., 2018). To overcome the long sequence issue,\ndocuments are split into multiple segments for both\nClinicalBERT and LSTM (Section 4).\nAfter we observe the best performed encoders,\nwe further propose to combine reinforcement learn-\ning (RL) to automatically extract out task-speciﬁc\nnoisy text from the long documents, as we observe\nthat many text segments do not contain predictive\ninformation such that removing these noise can po-\ntentially improve the performance. We model the\nnoise extraction process as a sequential decision\nproblem, which also aligns with the fact that clini-\ncal documents are received in time-sequential order.\nAt each step, a policy network with strong entropy\nregularization (Mnih et al., 2016) decides whether\nto prune the current segment given the context, and\nthe reward comes from a downstream classiﬁer af-\nter all decisions have been made (Section 5).\nEmpirical results show that the best performed\nencoder is BoW, and deep learning approaches suf-\nfer from severe overﬁtting under huge feature space\nin contrast of the limited training data. RL is ex-\nperimented on this BoW encoder, and able to im-\nprove upon baseline while pruning out around 25%\narXiv:2005.01259v2  [cs.CL]  23 May 2020\nType\nP\nT\nDescription\nCO\n1,354\n4,395.3\nReport for every outpatient consultation before transplantation\nDS\n514\n1,296.7\nSummary at the time of discharge from every hospital admission happened before transplant\nEC\n1,110\n1,073.6\nResults of echocardiography\nHP\n1,422\n3,025.1\nSummary of the patient’s medical history and clinical examination\nOP\n1,472\n4,224.8\nReport of surgical procedures\nPG\n1,415\n13,723.4\nMedical note during hospitalization summarizing the patient’s medical status each day\nSC\n2,033\n1,189.2\nReport from the evaluation of each transplant candidate by the selection committee\nSW\n1,118\n1,407.6\nReport from encounters with social workers\nTable 1: Statistics of our dataset with respect to different types of clinical notes. P: # of patients, T: avg. # of tokens,\nCO: Consultations, DS: Discharge Summary, EC: Echocardiography, HP: History and Physical, OP: Operative, PG:\nProgress, SC: Selection Conference, SW: Social Worker. The report for SC is written by the committee that consists\nof surgeons, nephrologists, transplant coordinators, social workers, etc. at the end of the transplant evaluation. All\n8 types follow the approximately 3:7 positive-negative class distribution.\ntext segments (Section 6). Further analysis shows\nthat RL is able to identify traditional noisy tokens\nwith few document frequencies (DF), as well as\ntask-irrelevant tokens with high DF but of little\ninformation (Section 7).\n2\nData\nThis work is based on the Emory Kidney Transplant\nDataset (EKTD) that contains structured chart data\nas well as unstructured clinical notes associated\nwith 2,060 patients. The structured data comprises\n80 features that are lab results before the discharge\nas well as the binary labels of whether each patient\nis readmitted within 30 days after kidney transplant\nor not where 30.7% patients are labeled as positive.\nThe unstructured data includes 8 types of notes\nsuch that all patients have zero to many documents\nfor each note type. It is possible to develop a more\naccurate prediction model by co-training the struc-\ntured and unstructured data; however, this work\nfocuses on investigating the potentials of unstruc-\ntured data only, which is more challenging.\n2.1\nPreprocessing\nAs the clinical notes are collected through various\nsources of EMRs, many noisy documents exist in\nEKTD such that 515 documents are HTML pages\nand 303 of them are duplicates. These documents\nare removed during preprocessing. Moreover, most\ndocuments contain not only written text but also\ntabular data, because some EMR systems can only\nexport entire documents in the table format.\nWhile there are many tabular texts in the documents\n(e.g., lab results and prescription as in Table 2), it is\nimpractical to write rules to ﬁlter them out, as the\nexported formats are not consistent across EMRs.\nThus, any tokens containing digits or symbols, ex-\ncept for one-character tokens, are removed during\nLab Fishbone (BMP, CBC, CMP, Diff) and\ncritical labs - Last 24 hours 03/08/2013 12:45\n142(Na) 104(Cl) 70H(BUN) - 10.7L(Hgb) <\n92(Glu) 6.5(WBC) 137L(Plt) 3.6(K) 26(CO2)\nTable 2: An example of tabular text in EKTD.\npreprocessing. Although numbers may provide use-\nful features, most quantitative measurements are\nalready included in the structured data so that those\nfeatures can be better extracted from the structured\ndata if necessary. The remaining tabular text con-\ntains headers and values that do not provide much\nhelpful information and become another source of\nnoise, which we handle by training a reinforcement\nlearning model to identify them (Section 5).\nTable 1 gives the statistics of each clinical note\ntype after preprocessing. The average number of\ntokens is measured by counting tokens in all doc-\numents from the same note type of each patient.\nGiven this preprocessed dataset, our task is to take\nall documents in each note type as a single input\nand predict whether or not the patient associated\nwith those documents will be readmitted.\n3\nRelated Work\nShin et al. (2019) presented ensemble models uti-\nlizing both the structured and the unstructured data\nin EKTD, where separate logistic regression (LR)\nmodels are trained on the structured data and each\ntype of notes respectively, and the ﬁnal prediction\nof each patient is obtained by averaging predictions\nfrom each models. Since some patients may lack\ndocuments from certain note types, prediction on\nthese note types are simply ignored in the averaging\nprocess. For the unstructured notes, concatenation\nof Term Frequency-Inverse Document Frequency\n(TF-IDF) and Latent Dirichlet Allocation (LDA)\nrepresentation is fed into LR. However, we have\nfound that the representation from LDA only con-\ntributes marginally, while LDA takes signiﬁcantly\nmore inferring time. Thus, we drop LDA and only\nuse TF-IDF as our BoW encoder (Section 4.1).\nVarious deep learning models regarding text clas-\nsiﬁcation have been proposed in recent years. Pre-\ntrained language models like BERT have shown\nstate-of-the-art performance on many NLP tasks\n(Devlin et al., 2019). ClinicalBERT is also intro-\nduced on the medical domain (Huang et al., 2019).\nHowever, deep learning approaches have two draw-\nbacks on this particular dataset. First, deep learn-\ning requires large dataset to train, whereas most of\nour unstructured note types only have fewer than\n2,000 samples. Second, these approaches are not\ndesigned for long documents, and difﬁcult to keep\nlong-term dependencies over thousands of tokens.\nReinforcement learning has been explored to\ncombat data noise by previous work (Zhang et al.,\n2018; Qin et al., 2018) on the short text setting. A\npolicy network makes decision left-to-right over\ntokens, and is jointly trained with another classiﬁer.\nHowever, there is little investigation of using RL on\nthe long text setting, as it still requires an effective\nencoder to give meaningful representation of long\ndocuments. Therefore, in our experiments, the ﬁrst\nstep is to select the best encoder, and then apply\nRL on the long document classiﬁcation.\n4\nDocument Representation\n4.1\nBag-of-Words\nFor the baseline model, the bag-of-words represen-\ntation with TF-IDF scores, excluding stopwords\n(Nothman et al., 2018), is fed into logistic regres-\nsion (LR). The objective is to minimize the negative\nlog likelihood of the gold label yi:\n−1\nm\nm\nX\ni=1\n[yi log p(gi)+(1−yi) log 1 −p(gi)] (1)\nwhere gi is the TF-IDF representation of Di. In\naddition, we experiment two common techniques\nin the encoder to reduce feature space: token stem-\nming, and document frequency cutoff.\n4.2\nAveraged Word Embedding\nWord embeddings generated by fastText are used\nto establish another baseline, that utilizes subwords\nto better represent unseen terms (Bojanowski et al.,\n2017). It is suitable for this task as unseen terms\nor misspellings frequently appear in these clinical\nnotes. The averaged word embedding is used to\nrepresent the input document consisting of multi-\nple notes, which gets fed into LR with the same\ntraining objective.\n4.3\nClinicalBERT\nFollowing Huang et al. (2019), the pretrained lan-\nguage BERT model (Devlin et al., 2019) is ﬁrst\ntuned on the MIMIC-III clinical note corpus (John-\nson et al., 2016), which has shown to provide better\nrelated word similarities in medical domains. Then,\na dense layer is added on the CLS token of the last\nBERT layer. The entire parameters are ﬁne-tuned\nto optimize the binary cross entropy loss, that is the\nsame objective as Equation 1.\nSince BERT has a limit on the input length, the\ninput document of each patient is split into multi-\nple subsequences. Each subsequence is within the\nBERT length limit, and serves as an independent\nsample with the same label of the patient. The\ntraining data is therefore noisily inﬂated. The ﬁnal\nprobability of readmission is computed as follows:\np(yi = 1|gi) = pni\nmax + pni\nmeanni/c\n1 + ni/c\n(2)\nwhere gi is the BERT representation of patient i, ni\nis the corresponding number of subsequences, and\nc is a hyperparameter to control the inﬂuence of ni.\npni\nmax and pni\nmean are the max and mean probability\nacross the subsequences, respectively.\nThe motivation behind balancing between the\nmax and mean probability is that subsequences do\nnot contain equal information. pni\nmax represents the\nbest potential, while longer text should give more\nimportance to pni\nmean, because pni\nmax is more easily af-\nfected by noise as the text length grows. Although\nEquation 2 seems intuitive, the use of pseudo labels\non subsequences becomes another source of noise,\nespecially when there are thousands of tokens; thus,\nthe performance is uncertain. Section 6.2 provides\ndetailed empirical analysis for this model.\n4.4\nWeight-dropped LSTM\nWe split documents of each patient into multiple\nshort segments, and feed the segment representa-\ntion to long short-term memory network (LSTM)\nat each time step:\nhj ←LSTM(sj, hj−1; θ)\n(3)\nst ⟶at\na1\na2\n⋯\nat\n⋯\naT\ns1 →s2 →⋯→st →⋯→sT\n↓\nat\nst+1\ngτ ⟶p(y|gτ)\ngτ\nSelected Text\nAction\nState\nPrediction \nReward\nPruning \nReward\nPolicy Network\nEnvironment\nDownstream Classiﬁer\nπθ\n↓\n↓\n↓\nFigure 1: Overview of our reinforcement learning approach. Rewards are calculated and sent back to the policy\nnetwork after all actions a1:T have been sampled for the given episode.\nwhere hj is the hidden state at time step j, sj is the\njth segment, and θ is the set of parameters.\nAlthough segmentation of documents is still nec-\nessary, no pseudo labels are needed. We get the\nsegment representation by averaging its token em-\nbedding from the last layer of BERT. The ﬁnal\nhidden state at each step j is the concatenated hid-\nden states of a single-layer Bi-directional LSTM.\nAfter we get the hidden state for each segment, a\nmax-pooling operation is performed on h1:n over\nthe time dimension to obtain a ﬁxed-length vector,\nsimilar to Kim (2014); Adhikari et al. (2019). A\ndense layer is immediately followed.\nIt is particularly important to strengthen regu-\nlarization on this dataset with small sample size.\nDropout (Srivastava et al., 2014) as a way of regu-\nlarization has been shown effective in deep learning\nmodels, and Merity et al. (2018) has successfully\napplied dropout-like technique in LSTM: the use of\nDropConnect (Wan et al., 2013) is applied on the\nfour hidden-to-hidden matrices, preventing overﬁt-\nting from occurring on the recurrent weights.\n5\nReinforcement Learning\nReinforcement learning is applied to the best per-\nforming encoder in Section 4 to prune noisy text,\nwhich can lead to comparable or even better per-\nformance, as many text segments in these clinical\nnotes are found to be irrelevant to this task. Fig-\nure 1 describes the overview of our reinforcement\nlearning approach. The pruning process is mod-\neled as a sequential decision problem, for the fact\nthat these notes are received in time-order. It con-\nsists of two separate components: a policy network,\nand a downstream classiﬁer. To avoid having too\nmany time steps, the policy is performed on the seg-\nment level instead of token level. For each patient,\ndocuments are split into short segments g1:T =\n{g1, g2, · · · , gT }, and the policy network conducts\na sequence of decisions a1:T = {a1, a2, · · · , aT }\nover segments. The downstream classiﬁer is re-\nsponsible for the reward, and the REINFORCE al-\ngorithm is used to train the policy (Williams, 1992).\nState\nAt each time step, the state st is the con-\ncatenation of two parts: the representation of previ-\nously selected text, and the current segment repre-\nsentation gi. The previously selected text serves as\nthe context and provides a prior importance. Both\nparts are represented by an effective encoder, e.g.\nthe best performing encoder from Section 4.\nAction\nThe action space at each step is binary:\n{Keep, Prune}. If the action is Keep, the current\nsegment is added to the selected text; otherwise, it\nis discarded. The ﬁnal selected text for a patient is\nthe concatenated segments selected by the policy.\nReward\nThe reward comes at the end when all\nactions are sampled for the entire sequence. The\nﬁnal selected text is fed to the downstream classi-\nﬁer, and negative log-likelihood of the gold label is\nused as the reward R. In addition, we also include\na reward term Rp to encourage pruning, as follows:\nRp = c · α · [2σ( l\nβ ) −1]\n(4)\nwhere c and β are hyperparameters to control the\nscale of Rp, l is the number of segments, α is the\nratio of pruned segments |{ak = Prune}| /l, σ\nis the sigmoid function. The value of the term\n2σ( l\nβ) −1 falls into range (0, 1). When l is small,\nit downgrades the encouragement of pruning; when\nl is large, it also gives an upper bound of Rp. Addi-\ntionally, we apply exponential decay on the reward.\nThe ﬁnal reward is dlR+Rp. d is the discount rate.\nPolicy Network\nThe policy network maintains a\nstochastic policy π(at|st; θ):\nπ(at|st; θ) = σ(Wst + b)\n(5)\nwhere θ is the set of policy parameters W and b, at\nand st are the action and state at the time step t re-\nspectively. During training, an action is sampled at\nEncoder\nCO\nDS\nEC\nHP\nOP\nPG\nSC\nSW\nBag-of-Words (§4.1)\n58.6\n62.1\n52.0\n58.9\n51.8\n61.2\n59.3\n51.6\n+ Cutoff\n58.6\n62.3\n52.8\n59.0\n51.9\n61.3\n59.3\n51.9\n+ Stemming\n58.9\n61.8\n53.4\n59.4\n51.9\n61.5\n59.3\n51.6\nAveraged Embedding (§4.2)\n56.3\n53.7\n52.4\n54.0\n53.4\n54.7\n54.2\n46.6\nClinicalBERT (§4.3)\n51.9\n53.3\n-\n52.7\n-\n-\n52.3\n-\nWeight-dropped LSTM (§4.4)\n53.7\n55.8\n-\n54.2\n-\n-\n54.5\n-\nTable 3: The Area Under the Curve (AUC) scores achieved by different encoders on the 5-fold cross-validation.\nSee the caption in Table 1 for the descriptions of CO, DS, EC, HP, OP, PG, SC, and SW. For deep learning encoders,\nonly four types are selected in experiments (Section 6.2).\neach step with the probability from the policy. After\nthe sampling is performed over the entire sequence,\nthe delayed reward is computed. During evaluation,\nthe action is picked by argmaxaπ(a|st; θ).\nThe training is guided by the REINFORCE algo-\nrithm (Williams, 1992), which optimizes the policy\nto maximize the expected reward:\nJ(θ) = Ea1:T ∼πRa1:T\n(6)\nand the gradient has the following form:\n∇θJ(θ) = Eτ\nT\nX\nt=1\n∇θ log π(at|st; θ)Rτ\n(7)\n≈1\nN\nN\nX\ni=1\nT\nX\nt=1\n∇θ log π(ait|sit; θ)Rτi\n(8)\nwhere\nτ\nrepresents\nthe\nsampled\ntrajectory\n{a1, a2, · · · , aT }, N is the number of sampled tra-\njectories. Rτi here equals the delayed reward from\nthe downstream classiﬁer at the last step.\nTo encourage exploration and avoid local op-\ntima, we add the entropy regularization (Mnih et al.,\n2016) on the policy loss:\nJreg(θ) = λ\nN\nN\nX\ni=1\n1\nTi\n∇θH(π(sit; θ))\n(9)\nwhere H is the entropy, and λ is the regularization\nstrength, Ti is the trajectory length.\nFinally, the downstream classiﬁer and policy net-\nwork are warm-started by separate training, and\nthen jointly trained together.\n6\nExperiments\nBefore experiments, we perform the preprocessing\ndescribed in Section 2.1, and then randomly split\npatients in every note type by 5 folds to perform\ncross-validation as suggested by Shin et al. (2019).\nTo evaluate each fold Fi, 12.5% of the training set,\nthat is the combined data of the other 4 folds, are\nheld out as the development set and the best conﬁg-\nuration from this development set is used to decode\nFi. The same split is used across all experiments\nfor fair comparison. Following Shin et al. (2019),\nthe averaged Area Under the Curve (AUC) across\nthese 5 folds is used as the evaluation metric.\n6.1\nBaseline\nBag-of-Words\nWe ﬁrst conduct experiments us-\ning the bag-of-words encoder (BoW; Section 4.1)\nto establish the baseline. Many experiments are per-\nformed on all note types using the vanilla TF-IDF,\ndocument frequency (DF) cutoff at 2 (removing\nall tokens whose DF ≤2), and token stemming.\nFor every experiment, the class weight is assigned\ninversely proportional to class frequencies, and the\ninverse of regularization strength C is searched\nfrom {0.01, 0.1, 1, 10}, where the best results are\nachieved with C = 1 on the development set.\nTable 3 describes the cross-validation results on\nevery note type. The top AUC is 62.3%, which is\nwithin expectation given the difﬁculty of this task.\nSome note types are not as predictive as the others,\nsuch as Operative (OP) and Social Worker (SW),\nwith the AUC under 52%. Most note types have\nthe standard deviations in range 0.02 to 0.03.\nIn comparison to the previous work (Shin et al.,\n2019), we achieve 0.671 AUC combining both\nstructured and unstructured data, despite without\nthe use of LDA in our encoder.\nNoise Observation\nThe DF cutoff coupled with\ntoken stemming signiﬁcantly reduce feature space\nfor the BoW model. As shown in Table 4, the DF\ncutoff itself can achieve about 50% reduction of the\nfeature space. Furthermore, applying the DF cutoff\nleads to slightly higher AUCs on most of the note\ntypes, despite almost a half of the tokens are re-\nmoved from the vocabulary. This implies that there\nexists a large amount of noisy text that appears only\nin few documents, causing the models to be over-\nﬁtted more easily. These results further verify our\nprevious observation and strengthen the necessity\nto extract noise from these long documents using\nreinforcement learning (Section 6.3).\nAveraged Word Embedding\nFor the averaged\nword embedding encoder (AWE; Section 4.2), em-\nbeddings generated by FastText trained on the Com-\nmon Crawl and the English Wikipedia with the 300\ndimension is used.1 AWE is outperformed by BoW\non every note type except Operative (OP; Table 3).\nThis empirical result implies that AWE over thou-\nsands of tokens is not so effective in generating the\ndocument representation so that the averaged em-\nbeddings are less discriminative than the sparse vec-\ntors generated by BoW for such long documents.\nType\nVanilla\n+ Cutoff\n+ Stemming\nCO\n28,213\n15,022 (46.8)\n12,243 (56.6)\nDS\n11,029\n6,117 (44.5)\n5,228 (52.6)\nHP\n20,245\n11,276 (44.3)\n9,329 (53.9)\nSC\n19,050\n9,873 (48.2)\n8,200 (57.0)\nTable 4: The dimensions of the feature spaces used by\neach BoW model with respect to the four note types.\nThe numbers in the parentheses indicate the percentage\nreduction from the vanilla model, respectively.\n6.2\nDeep Learning-based Encoders\nFor deep learning encoders, the four note types with\ngood baseline performance (≈60% AUC) and rea-\nsonable sequence length (< 5000) are selected to\nuse in the following experiments, which are Con-\nsultations (CO), Discharge Summary (DS), History\nand Physical (HP), and Selection Conference (SC)\n(see Tables 1 and 3).\nSegmentation\nFor both ClinicalBERT and the\nLSTM models, the input document is split into\nsegments as described in Section 4.3. For LSTM,\nwe set the maximum segment length to be 128 for\nCO and HP, 64 for DS and SC, to balance between\nsegment length and sequence length. The segment\nlength for ClinicalBERT is set to 318 (approach-\ning 500 after BERT tokenization) to avoid noise\nbrought by too many pseudo labels. More statistics\nabout segmentation are summarized in Table 5.\n1https://fasttext.cc/docs/en/crawl-vectors.html\nFor the ClinicalBERT, we use the PyTorch BERT\nimplementation with the base conﬁguration:2 768\nembedding dimensions and 12 transformer layers,\nand we load the weights provided by Huang et al.\n(2019) whose language model has been ﬁnetuned\non large-scale clinical notes.3 We ﬁnetune the en-\ntire ClinicalBERT with batch size 4, learning rate\n2 × 10−5, and weight decay rate 0.01.\nFor the weight-dropped LSTM, we set the batch\nsize to 64, the learning rate to 10−3, the weight-\ndrop rate to 0.5, and search the hidden state dimen-\nsion from {128, 256, 512} on the development set.\nEarly stop is used for both approaches.\nType + Model\nSEN\nSEQ\nINST\nCO + BERT\n318\n14.8\n11,376\nCO + LSTM\n128\n36.8\n948\nDS + BERT\n318\n4.6\n1,588\nDS + LSTM\n64\n22.5\n371\nHP + BERT\n318\n10.1\n8,364\nHP + LSTM\n128\n27.3\n987\nSC + BERT\n318\n3.7\n5,206\nSC + LSTM\n64\n25.4\n1,422\nTable 5: SEN: maximum segment length (number of\ntokens) allowed by the corresponding model, SEQ: av-\nerage sequence length (number of segments), INST: av-\nerage number of samples in the training set.\nResult Analysis\nTable 3 shows the ﬁnal results\nachieved by the ClinicalBERT and LSTM models.\nThe AUCs of both models experience a non-trivial\ndrop from the baseline. After further investigation,\nthe issue is that both models suffer from severe\noverﬁtting under the huge feature spaces, and strug-\ngle to learn generalized decision boundaries from\nthis data. Figure 2 shows an example of the weak\ncorrelation between the training loss and the AUC\nscores on the development set.\nAs more steps are processed, the training loss grad-\nually decreases to 0. However, the model has high\nvariance and it does not necessarily give better per-\nformance on the development set as the training\nloss drops. This issue is more apparent with Clini-\ncalBERT on CO because there are too many pseudo\nlabels acting as noise, which makes it harder for\nthe model to distinguish useful patterns from noise.\n2https://github.com/huggingface/transformers\n3https://github.com/kexinhuang12345/clinicalBERT\n     0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n      0   500    1k   1.5k   2k   2.5k   3k   3.5k 4\n(a) Training loss\n     0       40       80     120     160    200\n 44\n 46\n 48\n 50\n 52\n 54\n 56\n 58\n 60\n 62\n(b) AUC on dev-set\nFigure 2: Training loss and AUC scores on the develop-\nment set during the LSTM training on the CO type. The\nAUC scores depict high variance while showing weak\ncorrelation to the training loss.\n6.3\nReinforcement Learning\nAccording to Table 3, the BoW model achieves\nthe best performance. Therefore, we decide to use\nTF-IDF to represent the long text of each patient,\nalong with logistic regression as the classiﬁer for\nreinforcement learning. Document segmentation\nis the same as LSTM (Table 5). During training,\nsegments within each note are shufﬂed to reduce\noverﬁtting risks, and sequences with more than 36\nsegments are truncated.\nThe downstream classiﬁer is warm-started by\nloading weights from the logistic regression model\nin the previous experiment. The policy network\nis then trained for 400 episodes while freezing the\ndownstream classiﬁer. After the warm start, both\nmodels are jointly trained. We set the number of\nsampling N as 10 episodes, learning rate 2 × 10−4,\nand ﬁx the scaling factor β in Equations 4 as 8,\nand discount rate as 0.95. Moreover, we search the\nreward coefﬁcient c in {0.02, 0.1, 0.4}, and entropy\ncoefﬁcient λ in {2, 4, 6, 8}.\nCO\nDS\nHP\nSC\nBest\n58.9\n62.3\n59.4\n59.3\nRL\n59.8\n62.4\n60.6\n60.2\nPruning\n26%\n5%\n19%\n23%\nTable 6: The AUC scores and the pruning ratios of re-\ninforcement learning (RL). Best: AUC scores from the\nbest performing models in Table 3.\nThe AUC scores and the pruning ratios (the number\nof pruned segments divided by the sequence length)\nare shown in Table 6. Our reinforcement learning\napproach outperforms the best performing models\nin Table 3, achieving around 1% higher AUC scores\non three note types, CO, HP, and SC, while pruning\nout up to 26% of the input documents.\nTuning Analysis\nWe ﬁnd that two hyperparame-\nters are essential to the ﬁnal success of reinforce-\nment learning (RL). The ﬁrst is the reward discount\nrate d. The scale of the policy gradient ∇θJ(θ) de-\npends on the sequence length T, while the delayed\nreward Rτ is always on the same scale regardless\nof T. Therefore, different sequence length across\nepisodes causes turbulence on the policy gradient,\nleading to unstable training. It is important to apply\nreward decay to stabilize the scale of ∇θJ(θ).\nThe second is the entropy regularization coefﬁ-\ncient λ, which forces the model to add bias towards\nuncertainty. Without strong entropy regularization,\nthe training is easy to fall into local optima in early\nstage, which is to keep all segments, as shown by\nFigure 3(a). λ = 6 gives the model descent incen-\ntive to explore aggressively, as shown by Figure\n3(b), and ﬁnally leads to higher AUC.\n0.82\n0.84\n0.86\n0.88\n  0.9\n0.92\n0.94\n0.96\n0.98\n     1\n       0   500   1k  1.5k  2k  2.5k  3k  3.5k  4k\n(a) Without Entropy Reg.\n    0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n    1\n     0   500   1k  1.5k  2k  2.5k  3k  3.5k  4k\n(b) With Entropy Reg.\nFigure 3: Retaining ratios on the development set of SC\nwhile training the reinforcement learning model. En-\ntropy regularization encourages more exploration.\n7\nNoise Analysis\nTo investigate the noise extracted by RL, we an-\nalyze the pruned segments on the validation sets\nof the Consultations type (CO), and compare the\nresults with other basic noise removal techniques.\nQualitative Analysis\nTable 7 demonstrates the\npotential of the learned policy to automatically\nidentify noisy text from the long documents. The\noriginal notes of shown examples are tabular text\nwith headers and values, mostly lab results and\nmedical prescription. After the data cleaning step,\nthe text becomes broken and does not make much\nsense for humans to evaluate. The learned policy\ncan identify noisy segments by looking at the pres-\nence of headers such as “lab ﬁshbone”, “lab report”,\nand certain medical terms that frequently appear\nin tabular reports such as “chloride”, “creatinine”,\n“hemoglobin”, “methylprednisolone”, etc. We ﬁnd\nthat many pruned segments have strong indicators\nlab ﬁshbone ( bmp , cbc , cmp , diff ) and critical labs - last hours ( not an ofﬁcial lab report .\nplease see ﬂowsheet ( or printed ofﬁcial lab reports ) for ofﬁcial lab results . ) ( na ) ( cl ) h ( bun ) -\n( hgb ) ( glu ) ( wbc ) ( plt ) ( ) h ( cr ) ( hct ) na = not applicable a = abnormal ( ftn ) = footnote .\nlaboratory studies : sodium , potassium , chloride , . , bun , creatinine , glucose . total bilirubin\n1 , phos of , calcium , ast 9 , alt , alk phos . parathyroid hormone level . white blood cell count ,\nhemoglobin , hematocrit , platelets . inr , ptt , and pt .\nmethylprednisolone ivpb : mg , ivpb , give in surgery , routine , / , infuse over : minute . mycophe-\nnolate mofetil : mg = 4 cap , po , capsule , once , now , / , stop date / , ml . documented medications\ndocumented accupril : mg , po , qday , 0 reﬁll , substitution allowed .\nTable 7: Examples of pruned segments by the learned policy. Tokens that have feature importance lower than\n−0.001 (towards Prune action) are marked bold.\nthe social worker met with this pleasant year old caucasian male on this date for kidney transplant\nevaluation . the patient was alert , oriented and easily engaged in conversation with the social\nworker today . he resides in atlanta with his spouse of years , who he describes as very supportive .\nhe reports occasional alcohol drinks per month but denies any illicit drug use . he has a grade\neducation . he has been married for years . he is working full - time while on peritoneal dialysis as\na business asset manager . he has medicare and an aarp prescriptions supplement . family history :\nmother deceased at age with complications of obesity , high blood pressure and heart disease .\nTable 8: Examples of kept segments by the learned policy. Tokens that have feature importance greater than 0.0005\n(towards Keep action) are marked bold.\nof headers and speciﬁc medical terms, which ap-\npear mostly in tabular text rather than written notes.\nTable 8 shows examples that are kept by the pol-\nicy. Tokens that contribute towards Keep action are\nwords related with human and social life, such as\n“social worker”, “engaged”, “drinks”, “married”,\n“medicare”, and terms related with health condi-\ntions, such as “obesity”, “heart”, “high blood pres-\nsure”. These terms indeed appear mostly in written\ntext rather than tabular data.\nIn addition, we also notice that the policy is able\nto remove certain duplicate segments. Medical\nprofessionals sometimes repeat certain description\nfrom previous notes to a new document, causing\nduplicate content. The policy learns to make use of\nthe already selected context, and assigns negative\ncoefﬁcients to certain tokens. Duplicate segments\nare only selected once if the segment contains many\ntokens that have opposite feature importance in the\ncontext and segment vectors.\nQuantitative Analysis\nWe examine tokens that\nare pruned by RL and compare with document fre-\nquency (DF) cutoff. We select 3000 unique tokens\nin the vocabulary that have the top negative feature\nimportance (towards Prune action) in the segment\nvector of CO. Figure 4 shows the DF distribution\nof these tokens.\n\u0013\n\u0015\u0013\u0013\n\u0017\u0013\u0013\n\u0019\u0013\u0013\n\u001b\u0013\u0013\n'RFXPHQW\u0003)UHTXHQF\\\n\u0014\u0013\u0013\n\u0014\u0013\u0014\n\u0014\u0013\u0015\n\u0014\u0013\u0016\n1XPEHU\u0003RI\u00037RNHQV\nFigure 4: Log scale distribution on document frequency\nof tokens with top negative feature importance.\nWe observe that the majority of those tokens have\nsmall DF values. It shows that the learned policy is\nable to identify certain tokens with small DF values\nas noise, which aligns with DF cutoff. Moreover,\nthe distribution also shows a non-trivial amount of\ntokens with large DF values, demonstrating that\nRL can also identify task-speciﬁc noisy tokens that\ncommonly appear in documents, which in this case\nare certain tokens in noisy tabular text.\nEither RL or DF cutoff achieves higher AUC\nwhile reducing input features, proving that given\nthe small sample size, the extracted text is more\nlikely to cause overﬁt than being generalizable pat-\ntern, which also veriﬁes our initial hypothesis.\n8\nConclusion\nIn this paper, we address the task of 30-day readmis-\nsion prediction after kidney transplant, and propose\nto improve the performance by applying reinforce-\nment learning with noise extraction capability. To\novercome the challenge of long document represen-\ntation with a small dataset, four different encoders\nare experimented. Empirical results show that bag-\nof-words is the most suitable encoder, surpassing\noverﬁtted deep learning models, and reinforcement\nlearning is able to improve the performance, while\nbeing able to identify both traditional noisy tokens\nthat appear in few documents, and task-speciﬁc\nnoisy text that commonly appear.\nAcknowledgments\nWe gratefully acknowledge the support of the Na-\ntional Institutes of Health grant R01MD011682,\nReducing Disparities among Kidney Transplant Re-\ncipients. Any opinions, ﬁndings, and conclusions\nor recommendations expressed in this material are\nthose of the authors and do not necessarily reﬂect\nthe views of the National Institutes of Health.\nReferences\nAshutosh Adhikari, Achyudh Ram, Raphael Tang, and\nJimmy Lin. 2019. Rethinking complex neural net-\nwork architectures for document classiﬁcation. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages\n4046–4051, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nSenjuti Basu Roy, Ankur Teredesai, Kiyana Zolfaghar,\nRui Liu, David Hazel, Stacey Newman, and Albert\nMarinez. 2015. Dynamic hierarchical classiﬁcation\nfor patient risk-of-readmission. In Proceedings of\nthe 21th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, KDD\nâ ˘A´Z15, page 1691â ˘A¸S1700, New York, NY, USA.\nAssociation for Computing Machinery.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nIn Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nKexin\nHuang,\nJaan\nAltosaar,\nand\nRajesh\nRan-\nganath. 2019.\nClinicalbert:\nModeling clinical\nnotes and predicting hospital readmission.\nCoRR,\nabs/1904.05342.\nAlistair EW Johnson,\nTom J Pollard,\nLu Shen,\nH\nLehman\nLi-wei,\nMengling\nFeng,\nMoham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntiﬁc data, 3:160035.\nCaroline Jones, Robert Hollis, Tyler Wahl, Brad Oriel,\nKamal Itani, Melanie Morris, and Mary Hawn. 2016.\nTransitional care interventions and hospital readmis-\nsions in surgical populations: A systematic review.\nThe American Journal of Surgery, 212.\nYoon Kim. 2014.\nConvolutional neural networks\nfor sentence classiﬁcation.\nIn Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Lin-\nguistics.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models.\nIn International Conference on\nLearning Representations.\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi\nMirza, Alex Graves, Timothy Lillicrap, Tim Harley,\nDavid Silver, and Koray Kavukcuoglu. 2016. Asyn-\nchronous methods for deep reinforcement learning.\nIn Proceedings of The 33rd International Confer-\nence on Machine Learning, volume 48 of Proceed-\nings of Machine Learning Research, pages 1928–\n1937, New York, New York, USA. PMLR.\nJoel Nothman, Hanmin Qin, and Roman Yurchak.\n2018. Stop word lists in free open-source software\npackages.\nIn Proceedings of Workshop for NLP\nOpen Source Software (NLP-OSS), pages 7–12, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nPengda Qin, Weiran Xu, and William Yang Wang.\n2018.\nRobust distant supervision relation extrac-\ntion via deep reinforcement learning. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2137–2147, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nBonggun Shin, Julien Hogan, Andrew B. Adams, Ray-\nmond J. Lynch, and Jinho D. Choi. 2019.\nMul-\ntimodal ensemble approach to incorporate various\ntypes of clinical notes for predicting readmission.\nIn 2019 IEEE EMBS International Conference on\nBiomedical & Health Informatics (BHI).\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15(56):1929–1958.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun,\nand Rob Fergus. 2013.\nRegularization of neu-\nral networks using dropconnect.\nIn Proceedings\nof the 30th International Conference on Interna-\ntional Conference on Machine Learning - Volume\n28, ICMLâ ˘A´Z13, page IIIâ ˘A¸S1058â ˘A¸SIIIâ ˘A¸S1066.\nJMLR.org.\nRonald\nJ.\nWilliams.\n1992.\nSimple\nstatistical\ngradient-following\nalgorithms\nfor\nconnection-\nist\nreinforcement\nlearning.\nMach.\nLearn.,\n8(3â ˘A¸S4):229â ˘A¸S256.\nTianyang Zhang, Minlie Huang, and Li Zhao. 2018.\nLearning structured representation for text classiﬁca-\ntion via reinforcement learning. In AAAI Conference\non Artiﬁcial Intelligence.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-05-04",
  "updated": "2020-05-23"
}