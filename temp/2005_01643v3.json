{
  "id": "http://arxiv.org/abs/2005.01643v3",
  "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems",
  "authors": [
    "Sergey Levine",
    "Aviral Kumar",
    "George Tucker",
    "Justin Fu"
  ],
  "abstract": "In this tutorial article, we aim to provide the reader with the conceptual\ntools needed to get started on research on offline reinforcement learning\nalgorithms: reinforcement learning algorithms that utilize previously collected\ndata, without additional online data collection. Offline reinforcement learning\nalgorithms hold tremendous promise for making it possible to turn large\ndatasets into powerful decision making engines. Effective offline reinforcement\nlearning methods would be able to extract policies with the maximum possible\nutility out of the available data, thereby allowing automation of a wide range\nof decision-making domains, from healthcare and education to robotics. However,\nthe limitations of current algorithms make this difficult. We will aim to\nprovide the reader with an understanding of these challenges, particularly in\nthe context of modern deep reinforcement learning methods, and describe some\npotential solutions that have been explored in recent work to mitigate these\nchallenges, along with recent applications, and a discussion of perspectives on\nopen problems in the field.",
  "text": "Ofﬂine Reinforcement Learning: Tutorial, Review,\nand Perspectives on Open Problems\nSergey Levine1,2, Aviral Kumar1, George Tucker2, Justin Fu1\n1UC Berkeley, 2Google Research, Brain Team\nAbstract\nIn this tutorial article, we aim to provide the reader with the conceptual tools\nneeded to get started on research on ofﬂine reinforcement learning algorithms:\nreinforcement learning algorithms that utilize previously collected data, without\nadditional online data collection. Ofﬂine reinforcement learning algorithms hold\ntremendous promise for making it possible to turn large datasets into powerful de-\ncision making engines. Effective ofﬂine reinforcement learning methods would be\nable to extract policies with the maximum possible utility out of the available data,\nthereby allowing automation of a wide range of decision-making domains, from\nhealthcare and education to robotics. However, the limitations of current algorithms\nmake this difﬁcult. We will aim to provide the reader with an understanding of\nthese challenges, particularly in the context of modern deep reinforcement learning\nmethods, and describe some potential solutions that have been explored in recent\nwork to mitigate these challenges, along with recent applications, and a discussion\nof perspectives on open problems in the ﬁeld.\n1\nIntroduction\nReinforcement learning provides a mathematical formalism for learning-based control. By utilizing\nreinforcement learning, we can automatically acquire near-optimal behavioral skills, represented by\npolicies, for optimizing user-speciﬁed reward functions. The reward function deﬁnes what an agent\nshould do, and a reinforcement learning algorithm determines how to do it. While the reinforcement\nlearning algorithms have been an active area of research for decades, the introduction of effective\nhigh-capacity function approximators – deep neural networks – into reinforcement learning, along\nwith effective algorithms for training them, has allowed reinforcement learning methods to attain\nexcellent results along a wide range of domains (Tesauro, 1994; Hafner and Riedmiller, 2011; Levine\nand Koltun, 2013; Mnih et al., 2013; Levine et al., 2016; Silver et al., 2017; Kalashnikov et al., 2018).\nHowever, the fact that reinforcement learning algorithms provide a fundamentally online learning\nparadigm is also one of the biggest obstacles to their widespread adoption. The process of reinforce-\nment learning involves iteratively collecting experience by interacting with the environment, typically\nwith the latest learned policy, and then using that experience to improve the policy (Sutton and Barto,\n1998). In many settings, this sort of online interaction is impractical, either because data collection is\nexpensive (e.g., in robotics, educational agents, or healthcare) and dangerous (e.g., in autonomous\ndriving, or healthcare). Furthermore, even in domains where online interaction is feasible, we might\nstill prefer to utilize previously collected data instead – for example, if the domain is complex and\neffective generalization requires large datasets.\nIndeed, the success of machine learning methods across a range of practically relevant problems over\nthe past decade can in large part be attributed to the advent of scalable data-driven learning methods,\nwhich become better and better as they are trained with more data. Online reinforcement learning is\ndifﬁcult to reconcile with this paradigm. While this was arguably less of an issue when reinforcement\nlearning methods utilized low-dimensional or linear parameterizations, and therefore relied on small\ndatasets for small problems that were easy to collect or simulate (Lange et al., 2012), once deep\nnetworks are incorporated into reinforcement learning, it is tempting to consider whether the same\nkind of data-driven learning can be applied with reinforcement learning objectives, thus resulting in\ndata-driven reinforcement learning that utilizes only previously collected ofﬂine data, without any\narXiv:2005.01643v3  [cs.LG]  1 Nov 2020\n(a) online reinforcement learning\nrollout(s)\nupdate\nrollout data\n(b) off-policy reinforcement learning\nrollout(s)\nupdate\nrollout data\nbuffer\n(c) offline reinforcement learning\nrollout(s)\nlearn\nbuffer\ndata collected once \nwith any policy\ndeployment\ntraining phase\nFigure 1: Pictorial illustration of classic online reinforcement learning (a), classic off-policy reinforcement\nlearning (b), and ofﬂine reinforcement learning (c). In online reinforcement learning (a), the policy πk is updated\nwith streaming data collected by πk itself. In the classic off-policy setting (b), the agent’s experience is appended\nto a data buffer (also called a replay buffer) D, and each new policy πk collects additional data, such that D is\ncomposed of samples from π0, π1, . . . , πk, and all of this data is used to train an updated new policy πk+1. In\ncontrast, ofﬂine reinforcement learning employs a dataset D collected by some (potentially unknown) behavior\npolicy πβ. The dataset is collected once, and is not altered during training, which makes it feasible to use large\nprevious collected datasets. The training process does not interact with the MDP at all, and the policy is only\ndeployed after being fully trained.\nadditional online interaction (Kumar, 2019; Fu et al., 2020). See Figure 1 for a pictorial illustration.\nA number of recent works have illustrated the power of such an approach in enabling data-driven\nlearning of policies for dialogue (Jaques et al., 2019), robotic manipulation behaviors (Ebert et al.,\n2018; Kalashnikov et al., 2018), and robotic navigation skills (Kahn et al., 2020).\nUnfortunately, such data-driven ofﬂine reinforcement learning also poses major algorithmic challenges.\nAs we will discuss in this article, many commonly used reinforcement learning methods can learn\nfrom off-policy data, but such methods often cannot learn effectively from entire ofﬂine data, without\nany additional on-policy interaction. High-dimensional and expressive function approximation\ngenerally exacerbates this issue, since function approximation leaves the algorithms vulnerable to\ndistributional shift, one of the central challenges with ofﬂine reinforcement learning. However,\nthe appeal of a fully ofﬂine reinforcement learning framework is enormous: in the same way that\nsupervised machine learning methods have enabled data to be turned into generalizable and powerful\npattern recognizers (e.g., image classiﬁers, speech recognition engines, etc.), ofﬂine reinforcement\nlearning methods equipped with powerful function approximation may enable data to be turned\ninto generalizable and powerful decision making engines, effectively allowing anyone with a large\nenough dataset to turn this dataset into a policy that can optimize a desired utility criterion. From\nhealthcare decision-making support to autonomous driving to robotics, the implications of a reliable\nand effective ofﬂine reinforcement learning method would be immense.\nIn some application domains, the lack of effective ofﬂine reinforcement learning methods has driven\nresearch in a number of interesting directions. For example, in robotics and autonomous driving, a\nrapidly growing research topic is the study of simulation to real-world transfer: training policies with\nreinforcement learning in simulation and then transferring these policies into the real world (Sadeghi\nand Levine, 2017; Tan et al., 2018; Chebotar et al., 2019). While this approach is very pragmatic (and\noften effective), its popularity highlights the deﬁciency in ofﬂine reinforcement learning methods: if\nit was possible to simply train policies with previously collected data, it would likely be unnecessary\nin many cases to manually design high-ﬁdelity simulators for simulation-to-real-world transfer. After\nall, outside of reinforcement learning (e.g., in computer vision, NLP, or speech recognition), transfer\nfrom simulation is comparatively much less prevalent, since data-driven learning is so effective.\nThe goal of this article is to provide the reader with the conceptual tools needed to get started on\nresearch in the ﬁeld of ofﬂine reinforcement learning (also called batch reinforcement learning (Ernst\net al., 2005; Riedmiller, 2005; Lange et al., 2012)), so as to hopefully begin addressing some of these\ndeﬁciencies. To this end, we will present the ofﬂine reinforcement learning problem formulation, and\ndescribe some of the challenges associated with this problem setting, particularly in light of recent\nresearch on deep reinforcement learning and the interaction between reinforcement learning and\nhigh-dimensional function approximator, such as deep networks. We will cover a variety of ofﬂine\nreinforcement learning methods studied in the literature. For each one, we will discuss the conceptual\nchallenges, and initial steps taken to mitigate these challenges. We will then discuss some of the\napplications of ofﬂine reinforcement learning techniques that have already been explored, despite\n2\nthe limitations of current methods, and conclude with some perspectives on future work and open\nproblems in the ﬁeld.\n2\nOfﬂine Reinforcement Learning Problem Statement and Overview\nIn this section, we will introduce the mathematical formalism of reinforcement learning and deﬁne\nour notation, and then set up the ofﬂine reinforcement learning problem setting, where the goal is\nto learn near-optimal policies from previously collected data. Then, we will brieﬂy discuss some\nof the intuition behind why the ofﬂine reinforcement learning problem setting poses some unique\nchallenges, using a supervised behavioral cloning example.\n2.1\nReinforcement Learning Preliminaries\nIn this section, we will deﬁne basic reinforcement learning concepts, following standard textbook\ndeﬁnitions (Sutton and Barto, 1998). Reinforcement learning addresses the problem of learning\nto control a dynamical system, in a general sense. The dynamical system is fully deﬁned by a\nfully-observed or partially-observed Markov decision process (MDP).\nDeﬁnition 2.1 (Markov decision process). The Markov decision process is deﬁned as a tuple\nM = (S, A, T, d0, r, γ), where S is a set of states s ∈S, which may be either discrete or con-\ntinuous (i.e., multi-dimensional vectors), A is a set of actions a ∈A, which similarly can be discrete\nor continuous, T deﬁnes a conditional probability distribution of the form T(st+1|st, at) that de-\nscribes the dynamics of the system,1 d0 deﬁnes the initial state distribution d0(s0), r : S × A →R\ndeﬁnes a reward function, and γ ∈(0, 1] is a scalar discount factor.\nWe will use the fully-observed formalism in most of this article, though the deﬁnition for the partially\nobserved Markov decision process (POMDP) is also provided for completeness. The MDP deﬁnition\ncan be extended to the partially observed setting as follows:\nDeﬁnition 2.2 (Partially observed Markov decision process). The partially observed Markov decision\nprocess is deﬁned as a tuple M = (S, A, O, T, d0, E, r, γ), where S, A, T, d0, r, and γ are deﬁned\nas before, O is a set of observations, where each observation is given by o ∈O, and E is an emission\nfunction, which deﬁnes the distribution E(ot|st).\nThe ﬁnal goal in a reinforcement learning problem is to learn a policy, which deﬁnes a distribution\nover actions conditioned on states, π(at|st), or conditioned on observations in the partially observed\nsetting, π(at|ot). The policy may also be conditioned on an observation history, π(at|o0:t). From\nthese deﬁnitions, we can derive the trajectory distribution. The trajectory is a sequence of states and\nactions of length H, given by τ = (s0, a0, . . . , sH, aH), where H may be inﬁnite. The trajectory\ndistribution pπ for a given MDP M and policy π is given by\npπ(τ) = d0(s0)\nH\nY\nt=0\nπ(at|st)T(st+1|st, at).\nThis deﬁnition can easily be extended into the partially observed setting by including the observations\not and emission function E(ot|st). The reinforcement learning objective, J(π), can then be written\nas an expectation under this trajectory distribution:\nJ(π) = Eτ∼pπ(τ)\n\" H\nX\nt=0\nγtr(st, at)\n#\n.\n(1)\nWhen H is inﬁnite, it is sometimes also convenient to assume that the Markov chain on (st, at)\ndeﬁned by π(at|st)T(st+1|st, at) is ergodic, and deﬁne the objective in terms of the expected reward\nunder the stationary distribution of this Markov chain (Sutton and Barto, 1998). This deﬁnition is\nsomewhat complicated by the role of the discount factor. For a full discussion of this topic, we refer\nthe reader to prior work (Thomas, 2014).\n1We will sometimes use time subscripts (i.e., st+1 follows st), and sometimes “prime” notation (i.e., s′ is\nthe state that follows s). Explicit time subscripts can help clarify the notation in ﬁnite-horizon settings, while\n“prime” notation is simpler in inﬁnite-horizon settings where absolute time step indices are less meaningful.\n3\nIn many cases, we will ﬁnd it convenient to refer to the marginals of the trajectory distribution pπ(τ).\nWe will use dπ(s) to refer to the overall state visitation frequency, averaged over the time steps, and\ndπ\nt (st) to refer to the state visitation frequency at time step t.\nIn this section, we will brieﬂy summarize different types of reinforcement learning algorithms and\npresent deﬁnitions. At a high level, all standard reinforcement learning algorithms follow the same\nbasic learning loop: the agent interacts with the MDP M by using some sort of behavior policy,\nwhich may or may not match π(a|s), by observing the current state st, selecting an action at, and\nthen observing the resulting next state st+1 and reward value rt = r(st, at). This may repeat for\nmultiple steps, and the agent then uses the observed transitions (st, at, st+1, rt) to update its policy.\nThis update might also utilize previously observed transitions. We will use D = {(si\nt, ai\nt, si\nt+1, ri\nt)} to\ndenote the set of transitions that are available for the agent to use for updating the policy (“learning”),\nwhich may consist of either all transitions seen so far, or some subset thereof.\nPolicy gradients.\nOne of the most direct ways to optimize the RL objective in Equation 1 is to\ndirectly estimate its gradient. In this case, we typically assume that the policy is parameterized by a\nparameter vector θ, and therefore given by πθ(at|st). For example, θ might denote the weights of\na deep network that outputs the logits for the (discrete) actions at. In this case, we can express the\ngradient of the objective with respect to θ as:\n∇θJ(πθ)=Eτ∼pπθ (τ)\n\" H\nX\nt=0\nγt∇θ log πθ(at|st)\n H\nX\nt′=t\nγt′−tr(st′, at′) −b(st)\n!\n|\n{z\n}\nreturn estimate ˆ\nA(st,at)\n#\n,\n(2)\nwhere the return estimator ˆA(st, at) can itself be learned as a separate neural network critic, as\ndiscussed below, or it can simply be estimated with Monte Carlo samples, in which case we simply\ngenerate samples from pπθ(τ), and then sum up the rewards over the time steps of the sampled\ntrajectory. The baseline b(st) can be estimated as the average reward over the sampled trajectories, or\nby using a value function estimator V (st), which we discuss in the dynamic programming section.\nWe can equivalently write this gradient expression as an expectation with respect to dπ\nt (st) as\n∇θJ(πθ)=\nH\nX\nt=0\nEst∼dπ\nt (st),at∼πθ(at|st)\nh\nγt∇θ log πθ(at|st) ˆA(st, at)\ni\n.\nA common modiﬁcation is to drop the γt term in front of the gradient, which approximates an average\nreward setting (Thomas, 2014). Dropping this term and adopting an inﬁnite-horizon formulation, we\ncan further rewrite the policy gradient as expectation under dπ(s) as\n∇θJ(πθ)=\n1\n1 −γ Es∼dπ(st),a∼πθ(a|s)\nh\n∇θ log πθ(a|s) ˆA(s, a)\ni\n.\nThe constant scaling term\n1\n1−γ is often disregarded. This inﬁnite-horizon formulation is often\nconvenient to work with for analyzing and deriving policy gradient methods. For a full derivation of\nthis gradient, we refer the reader to prior work (Sutton et al., 2000; Kakade, 2002; Schulman et al.,\n2015). We can summarize a basic Monte Carlo policy gradient algorithm as follows:\nAlgorithm 1 On-policy policy gradient with Monte Carlo estimator\n1: initialize θ0\n2: for iteration k ∈[0, . . . , K] do\n3:\nsample trajectories {τi} by running πθk(at|st) ▷each τi consists of si,0, ai,0, . . . , si,H, ai,H\n4:\ncompute Ri,t = PH\nt′=t γt′−tr(si,t, ai,t)\n5:\nﬁt b(st) to {Ri, t}\n▷use constant bt = 1\nN\nP\ni Ri, t, or ﬁt b(st) to {Ri, t}\n6:\ncompute ˆA(si,t, ai,t) = Ri,t −b(st)\n7:\nestimate ∇θkJ(πθk) ≈P\ni,t ∇θk log πθk(ai,t|si,t) ˆA(si,t, ai,t)\n8:\nupdate parameters: θk+1 ←θk + α∇θkJ(πθk)\n9: end for\nFor additional details on standard on-policy policy gradient methods, we refer the reader to prior\nwork (Sutton et al., 2000; Kakade, 2002; Schulman et al., 2015).\n4\nApproximate dynamic programming.\nAnother way to optimize the reinforcement learning objec-\ntive is to observe that, if we can accurately estimate a state or state-action value function, it is easy to\nthen recover a near-optimal policy. A value function provides an estimate of the expected cumulative\nreward that will be obtained by following some policy π(at|st) when starting from a given state st,\nin the case of the state-value function V π(st), or when starting from a state-action tuple (st, at), in\nthe case of the state-action value function Qπ(st, at). We can deﬁne these value functions as:\nV π(st) = Eτ∼pπ(τ|st)\n\" H\nX\nt′=t\nγt′−tr(st, at)\n#\nQπ(st, at) = Eτ∼pπ(τ|st,at)\n\" H\nX\nt′=t\nγt′−tr(st, at)\n#\n.\nFrom this, we can derive recursive deﬁnitions for these value functions, which are given as\nV π(st) = Eat∼π(at|st) [Qπ(st, at)]\nQπ(st, at) = r(st, at) + γEst+1∼T (st+1|st,at) [V π(st+1)] .\nWe can combine these two equations to express the Qπ(st, at) in terms of Qπ(st+1, at+1):\nQπ(st, at) = r(st, at) + γEst+1∼T (st+1|st,at),at+1∼π(at+1|st+1) [Qπ(st+1, at+1))] .\n(3)\nWe can also express these in terms of the Bellman operator for the policy π, which we denote Bπ.\nFor example, Equation (3) can be written as ⃗Qπ = Bπ ⃗Qπ, where ⃗Qπ denotes the Q-function Qπ\nrepresented as a vector of length |S|×|A|. Before moving on to deriving learning algorithms based on\nthese deﬁnitions, we brieﬂy discuss some properties of the Bellman operator. This Bellman operator\nhas a unique ﬁxed point that corresponds to the true Q-function for the policy π(a|s), which can\nbe obtained by repeating the iteration ⃗Qπ\nk+1 = Bπ ⃗Qπ\nk, and it can be shown that limk→∞⃗Qπ\nk = ⃗Qπ,\nwhich obeys Equation (3) (Sutton and Barto, 1998). The proof for this follows from the observation\nthat Bπ is a contraction in the ℓ∞norm (Lagoudakis and Parr, 2003).\nBased on these deﬁnitions, we can derive two commonly used algorithms based on dynamic program-\nming: Q-learning and actor-critic methods. To derive Q-learning, we express the policy implicitly\nin terms of the Q-function, as π(at|st) = δ(at = arg max Q(st, at)), and only learn the Q-function\nQ(st, at). By substituting this (implicit) policy into the above dynamic programming equation, we\nobtain the following condition on the optimal Q-function:\nQ⋆(st, at) = r(st, at) + γEst+1∼T (st+1|st,at)\n\u0014\nmax\nat+1 Q⋆(st+1, at+1)\n\u0015\n.\n(4)\nWe can again express this as ⃗Q = B⋆⃗Q in vector notation, where B⋆now refers to the Bellman\noptimality operator. Note however that this operator is not linear, due to the maximization on the\nright-hand side in Equation (4). To turn this equation into a learning algorithm, we can minimize\nthe difference between the left-hand side and right-hand side of this equation with respect to the\nparameters of a parametric Q-function estimator with parameters φ, Qφ(st, at). There are a number\nof variants of this Q-learning procedure, including variants that fully minimize the difference between\nthe left-hand side and right-hand side of the above equation at each iteration, commonly referred to\nas ﬁtted Q-iteration (Ernst et al., 2005; Riedmiller, 2005), and variants that take a single gradient\nstep, such as the original Q-learning method (Watkins and Dayan, 1992). The commonly used\nvariant in deep reinforcement learning is a kind of hybrid of these two methods, employing a replay\nbuffer (Lin, 1992) and taking gradient steps on the Bellman error objective concurrently with data\ncollection (Mnih et al., 2013). We write out a general recipe for Q-learning methods in Algorithm 2.\nClassic Q-learning can be derived as the limiting case where the buffer size is 1, and we take G = 1\ngradient steps and collect S = 1 transition samples per iteration, while classic ﬁtted Q-iteration runs\nthe inner gradient descent phase to convergence (i.e., G = ∞), and uses a buffer size equal to the\nnumber of sampling steps S. Note that many modern implementations also employ a target network,\nwhere the target value ri + γ maxa′ Qφk(s′, a′) actually uses φL, where L is a lagged iteration\n(e.g., the last k that is a multiple of 1000). Note that these approximations violate the assumptions\nunder which Q-learning algorithms can be proven to converge. However, recent work suggests that\nhigh-capacity function approximators, which correspond to a very large set Q, generally do tend to\nmake this method convergent in practice, yielding a Q-function that is close to ⃗Qπ (Fu et al., 2019;\nVan Hasselt et al., 2018).\n5\nAlgorithm 2 Generic Q-learning (includes FQI and DQN as special cases)\n1: initialize φ0\n2: initialize π0(a|s) = ϵU(a) + (1 −ϵ)δ(a = arg maxa Qφ0(s, a))\n▷Use ϵ-greedy exploration\n3: initialize replay buffer D = ∅as a ring buffer of ﬁxed size\n4: initialize s ∼d0(s)\n5: for iteration k ∈[0, . . . , K] do\n6:\nfor step s ∈[0, . . . , S −1] do\n7:\na ∼πk(a|s)\n▷sample action from exploration policy\n8:\ns′ ∼p(s′|s, a)\n▷sample next state from MDP\n9:\nD ←D ∪{(s, a, s′, r(s, a))}\n▷append to buffer, purging old data if buffer too big\n10:\nend for\n11:\nφk,0 ←φk\n12:\nfor gradient step g ∈[0, . . . , G −1] do\n13:\nsample batch B ⊂D\n▷B = {(si, ai, s′\ni, rt)}\n14:\nestimate error E(B, φk,g) = P\ni\n\u0000Qφk,g −(ri + γ maxa′ Qφk(s′, a′))\n\u00012\n15:\nupdate parameters: φk,g+1 ←φk,g −α∇φk,gE(B, φk,g)\n16:\nend for\n17:\nφk+1 ←φk,G\n▷update parameters\n18: end for\nActor-critic algorithms.\nActor-critic algorithms combine the basic ideas from policy gradients\nand approximate dynamic programming. Such algorithms employ both a parameterized policy and\na parameterized value function, and use the value function to provide a better estimate of ˆA(s, a)\nfor policy gradient calculation. There are a number of different variants of actor-critic methods,\nincluding on-policy variants that directly estimate V π(s) (Konda and Tsitsiklis, 2000), and off-policy\nvariants that estimate Qπ(s, a) via a parameterized state-action value function Qπ\nφ(s, a) (Haarnoja\net al., 2018, 2017; Heess et al., 2015). We will focus on the latter class of algorithms, since they\ncan be extended to the ofﬂine setting. The basic design of such an algorithm is a straightforward\ncombination of the ideas in dynamic programming and policy gradients. Unlike Q-learning, which\ndirectly attempts to learn the optimal Q-function, actor-critic methods aim to learn the Q-function\ncorresponding to the current parameterized policy πθ(a|s), which must obey the equation\nQπ(st, at) = r(st, at) + γEst+1∼T (st+1|st,at),at+1∼πθ(at+1|st+1) [Qπ(st+1, at+1)] .\nAs before, this equation can be expressed in vector form in terms of the Bellman operator for the\npolicy, ⃗Qπ = Bπ ⃗Qπ, where ⃗Qπ denotes the Q-function Qπ represented as a vector of length |S|×|A|.\nWe can now instantiate a complete algorithm based on this idea, shown in Algorithm 3.\nFor more details, we refer the reader to standard textbooks and prior works (Sutton and Barto, 1998;\nKonda and Tsitsiklis, 2000). Actor-critic algorithms are closely related with another class of methods\nthat frequently arises in dynamic programming, called policy iteration (PI) (Lagoudakis and Parr,\n2003). Policy iteration consists of two phases: policy evaluation and policy improvement. The\npolicy evaluation phase computes the Q-function for the current policy π, Qπ, by solving for the\nﬁxed point such that Qπ = BπQπ. This can be done via linear programming or solving a system\nof linear equations, as we will discuss in Section 4, or via gradient updates, analogously to line\n15 in Algorithm 3. The next policy iterate is then computed in the policy improvement phase, by\nchoosing the action that greedily maximizes the Q-value at each state, such that πk+1(a|s) = δ(a =\narg maxa Qπk(s, a)), or by using a gradient based update procedure as is employed in Algorithm 3\non line 24. In the absence of function approximation (e.g., with tabular representations) policy\niteration produces a monotonically improving sequence of policies, and converges to the optimal\npolicy. Policy iteration can be obtained as a special case of the generic actor-critic algorithm in\nAlgorithm 3 when we set GQ = ∞and Gπ = ∞, when the buffer D consists of each and every\ntransition of the MDP.\nModel-based reinforcement learning.\nModel-based reinforcement learning is a general term that\nrefers to a broad class of methods that utilize explicit estimates of the transition or dynamics function\nT(st+1|st, at), parameterized by a parameter vector ψ, which we will denote Tψ(st+1|st, at). There\nis no single recipe for a model-based reinforcement learning method. Some commonly used model-\nbased reinforcement learning algorithms learn only the dynamics model Tψ(st+1|st, at), and then\n6\nAlgorithm 3 Generic off-policy actor-critic\n1: initialize φ0\n2: initialize θ0\n3: initialize replay buffer D = ∅as a ring buffer of ﬁxed size\n4: initialize s ∼d0(s)\n5: for iteration k ∈[0, . . . , K] do\n6:\nfor step s ∈[0, . . . , S −1] do\n7:\na ∼πθk(a|s)\n▷sample action from current policy\n8:\ns′ ∼p(s′|s, a)\n▷sample next state from MDP\n9:\nD ←D ∪{(s, a, s′, r(s, a))}\n▷append to buffer, purging old data if buffer too big\n10:\nend for\n11:\nφk,0 ←φk\n12:\nfor gradient step g ∈[0, . . . , GQ −1] do\n13:\nsample batch B ⊂D\n▷B = {(si, ai, s′\ni, rt)}\n14:\nestimate error E(B, φk,g) = P\ni\n\u0000Qφk,g −(ri + γEa′∼πk(a′|s′)Qφk(s′, a′))\n\u00012\n15:\nupdate parameters: φk,g+1 ←φk,g −αQ∇φk,gE(B, φk,g)\n16:\nend for\n17:\nφk+1 ←φk,GQ\n▷update Q-function parameters\n18:\nθk,0 ←θk\n19:\nfor gradient step g ∈[0, . . . Gπ −1] do\n20:\nsample batch of states {si} from D\n21:\nfor each si, sample ai ∼πθk,g(a|si)\n▷do not use actions in the buffer!\n22:\nfor each (si, ai), compute ˆA(si, ai) = Qφk+1(si, ai) −Ea∼πk,g(a|si)[Qφk+1(si, a)]\n23:\n∇θk,gJ(πθk,g) ≈1\nN ∇θk,g log πθk,g(si, ai) ˆA(si, ai)\n24:\nθk,g+1 ←θk,g + απ∇θk,gJ(πθk,g)\n25:\nend for\n26:\nθk + 1 ←θk,Gπ\n▷update policy parameters\n27: end for\nutilize it for planning at test time, often by means of model-predictive control (MPC) (Tassa et al.,\n2012) with various trajectory optimization methods (Nagabandi et al., 2018; Chua et al., 2018). Other\nmodel-based reinforcement learning methods utilize a learned policy πθ(at|st) in addition to the\ndynamics model, and employ backpropagation through time to optimize the policy with respect to\nthe expected reward objective (Deisenroth and Rasmussen, 2011). Yet another set of algorithms\nemploy the model to generate “synthetic” samples to augment the sample set available to model-\nfree reinforcement learning methods. The classic Dyna algorithm uses this recipe in combination\nwith Q-learning and one-step predictions via the model from previously seen states (Sutton, 1991),\nwhile a variety of recently proposed algorithms employ synthetic model-based rollouts with policy\ngradients (Parmas et al., 2019; Kaiser et al., 2019a) and actor-critic algorithms (Janner et al., 2019).\nSince there are so many variants of model-based reinforcement learning algorithms, we will not\ngo into detail on each of them in this section, but we will discuss some considerations for ofﬂine\nmodel-based reinforcement learning in Section 5.\n2.2\nOfﬂine Reinforcement Learning\nThe ofﬂine reinforcement learning problem can be deﬁned as a data-driven formulation of the\nreinforcement learning problem. The end goal is still to optimize the objective in Equation (1).\nHowever, the agent no longer has the ability to interact with the environment and collect additional\ntransitions using the behavior policy. Instead, the learning algorithm is provided with a static dataset\nof transitions, D = {(si\nt, ai\nt, si\nt+1, ri\nt)}, and must learn the best policy it can using this dataset. This\nformulation more closely resembles the standard supervised learning problem statement, and we can\nregard D as the training set for the policy. In essence, ofﬂine reinforcement learning requires the\nlearning algorithm to derive a sufﬁcient understanding of the dynamical system underlying the MDP\nM entirely from a ﬁxed dataset, and then construct a policy π(a|s) that attains the largest possible\ncumulative reward when it is actually used to interact with the MDP. We will use πβ to denote the\ndistribution over states and actions in D, such that we assume that the state-action tuples (s, a) ∈D\n7\nare sampled according to s ∼dπβ(s), and the actions are sampled according to the behavior policy,\nsuch that a ∼πβ(a|s).\nThis problem statement has been presented under a number of different names. The term “off-\npolicy reinforcement learning” is typically used as an umbrella term to denote all reinforcement\nlearning algorithms that can employ datasets of transitions D where the corresponding actions in\neach transition were collected with any policy other than the current policy π(a|s). Q-learning\nalgorithms, actor-critic algorithms that utilize Q-functions, and many model-based reinforcement\nlearning algorithm are off-policy algorithms. However, off-policy algorithms still often employ\nadditional interaction (i.e., online data collection) during the learning process. Therefore, the term\n“fully off-policy” is sometimes used to indicate that no additional online data collection is performed.\nAnother commonly used term is “batch reinforcement learning” (Ernst et al., 2005; Riedmiller, 2005;\nLange et al., 2012). While this term has been used widely in the literature, it can also cause some\namount of confusion, since the use of a “batch” in an iterative learning algorithm can also refer to a\nmethod that consumes a batch of data, updates a model, and then obtains a different batch, as opposed\nto a traditional online learning algorithm, which consumes one sample at a time. In fact, Lange et al.\n(2012) further introduces qualiﬁers “pure” and “growing” batch reinforcement learning to clarify this.\nTo avoid this confusion, we will instead use the term “ofﬂine reinforcement learning” in this tutorial.\nThe ofﬂine reinforcement learning problem can be approached using algorithms from each of the\nfour categories covered in the previous section, and in principle any off-policy RL algorithm could be\nused as an ofﬂine RL algorithm. For example, a simple ofﬂine RL method can be obtained simply by\nusing Q-learning without additional online exploration, using D to pre-populate the data buffer. This\ncorresponds to changing the initialization of D in Algorithm 2, and setting S = 0. However, as we\nwill discuss later, not all such methods are effective in the ofﬂine setting.\n2.3\nExample Scenarios\nBefore delving deeper into the technical questions surrounding ofﬂine RL, we will ﬁrst discuss a few\nexample scenarios where ofﬂine RL might be utilized. These scenarios will help us to understand the\nfactors that we must consider when designing ofﬂine RL methods that are not only convergent and\nprincipled, but also likely to work well in practice. A more complete discussion of actual applications\nis provided in Section 6.\nDecision making in health care.\nAn example health care scenario might formulate a Markov\ndecision process to model the process of diagnosing and treating a patient, where actions correspond\nto various available interventions (e.g., diagnostic tests and treatments), and observations correspond\nto the patient’s symptoms and results of diagnostic tests. A partially observed MDP formulation may\nbe most suitable in such cases. Conventional active reinforcement learning in such scenarios might be\nprohibitively dangerous – even utilizing a fully trained policy to treat a patient is a difﬁcult prospect\nfor clinicians, and deploying a partially trained policy would be out of the question. Therefore, ofﬂine\nRL might be the only viable path to apply reinforcement learning in such settings. Ofﬂine data would\nthen be obtained from treatment histories of real patients, with the “actions” that were selected by\ntheir physician.\nLearning goal-directed dialogue policies.\nDialogue can be viewed as interactive sequential de-\ncision making problem, which can also be modeled as an MDP, particularly when the dialogue\nis goal-directed (e.g., a chat bot on an e-commerce website that is offering information about a\nproduct to persuade a user to make a purchase). However, since the goal for such agents is to interact\nsuccessfully with real humans, collecting trials requires interacting with live humans, which may be\nprohibitively expensive at the scale needed to train effective conversational agents. However, ofﬂine\ndata can be collected directly from humans, and may indeed be natural to collect in any application\ndomain where the aim is to partially or completely supplant human operators, by recording the\ninteractions that are already taking place with the human-operated system.\nLearning robotic manipulation skills.\nIn a robotic manipulation setting, active reinforcement\nlearning may in fact be quite feasible. However, we might want to learn policies for a variety of\nrobotic skills (e.g., all of the steps necessary to prepare a variety of meals for a home cooking robot)\nthat generalize effectively over different environments and settings. In that case, each skill by itself\nmight require a very large amount of interaction, as we would not only need to collect enough data to\n8\nlearn the skill, but enough data such that this skill generalizes effectively to all the situations (e.g., all\nthe different homes) in which the robot might need to perform it. With ofﬂine RL, we could instead\nimagine including all of the data the robot has ever collected for all of its previously learned skills in\nthe data buffer for each new skill that it learns. In this way, some skills could conceivably be acquired\nwithout any new data collection, if they can be assembled from parts of previously learned behaviors\n(e.g., cooking a soup that includes onions and carrots can likely be learned from experience cooking a\nsoup with onions and meat, as well as another soup with carrots and cucumbers). In this way, ofﬂine\nRL can effectively utilize multi-task data.\n2.4\nWhat Makes Ofﬂine Reinforcement Learning Difﬁcult?\nOfﬂine reinforcement learning is a difﬁcult problem for multiple reasons, some of which are reason-\nably clear, and some of which might be a bit less clear. Arguably the most obvious challenge with\nofﬂine reinforcement learning is that, because the learning algorithm must rely entirely on the static\ndataset D, there is no possibility of improving exploration: exploration is outside the scope of the\nalgorithm, so if D does not contain transitions that illustrate high-reward regions of the state space, it\nmay be impossible to discover those high-reward regions. However, because there is nothing that we\ncan do to address this challenge, we will not spend any more time on it, and will instead assume that\nD adequately covers the space of high-reward transitions to make learning feasible.2\nA more subtle but practically more important challenge with ofﬂine reinforcement learning is that,\nat its core, ofﬂine reinforcement learning is about making and answering counterfactual queries.\nCounterfactual queries are, intuitively, “what if” questions. Such queries require forming hypotheses\nabout what might happen if the agent were to carry out a course of action different from the one\nseen in the data. This is a necessity in ofﬂine RL, since we if we want the learned policy to perform\nbetter than the behavior seen in the dataset D, we must execute a sequence of actions that is in\nsome way different. Unfortunately, this strains the capabilities of many of our current machine\nlearning tools, which are designed around the assumption that the data is independent and identically\ndistributed (i.i.d.). That is, in standard supervised learning, the goal is to train a model that attains\ngood performance (e.g., high accuracy) on data coming from the same distribution as the training\ndata. In ofﬂine RL, the whole point is to learn a policy that does something differently (presumably\nbetter) from the pattern of behavior observed in the dataset D.\nThe fundamental challenge with making such counterfactual queries is distributional shift: while our\nfunction approximator (policy, value function, or model) might be trained under one distribution, it\nwill be evaluated on a different distribution, due both to the change in visited states for the new policy\nand, more subtly, by the act of maximizing the expected return. This latter point is discussed in more\ndetail in Section 4. Distributional shift issues can be addressed in several ways, with the simplest one\nbeing to constrain something about the learning process such that the distributional shift is bounded.\nFor example, by constraining how much the learned policy π(a|s) differs from the behavior policy\nπβ(a|s), we can bound state distributional shift (Kakade and Langford, 2002; Schulman et al., 2015).\nIn this section, we will provide a short theoretical illustration of how harmful distributional shift can\nbe on the performance of policies in MDPs. In this example, based on Ross et al. (2011), we will\nassume that we are provided with optimal action labels a⋆at each state s ∈D. One might expect\nthat, under such a strong assumption, the performance of our learned policy should be at least as\ngood as the policies that we can learn with reinforcement learning without such optimal action labels.\nThe goal in this analysis will be to bound the number of mistakes made by the learned policy π(a|s)\nbased on this labeled dataset, denoted as\nℓ(π) = Epπ(τ)\n\" H\nX\nt=0\nδ(at ̸= a⋆\nt )\n#\n.\nIf we train π(a|s) with supervised learning (i.e., standard empirical risk minimization) on this labeled\ndataset, we have the following result from Ross et al. (2011):\nTheorem 2.1 (Behavioral cloning error bound). If π(a|s) is trained via empirical risk minimization\non s ∼dπβ(s) and optimal labels a⋆, and attains generalization error ϵ on s ∼dπβ(s), then\nℓ(π) ≤C + H2ϵ is the best possible bound on the expected error of the learned policy.\n2It is worth noting that deﬁning this notion formally is itself an open problem, and to the best knowledge of\nthe authors, there are no known non-trivial “sufﬁciency” conditions on D that allows us to formulate guarantees\nthat any ofﬂine reinforcement learning algorithm will recover an optimal or near-optimal policy.\n9\nProof. The proof follows from Theorem 2.1 from Ross et al. (2011) using the 0-1 loss, and the bound\nis the best possible bound following the example from Ross and Bagnell (2010).\nInterestingly, if we allow for additional data collection, where we follow the learned policy π(a|s)\nto gather additional states s ∼dπ(s), and then access optimal action labels for these new on-policy\nstates, the best possible bound becomes substantially better:\nTheorem 2.2 (DAgger error bound). If π(a|s) is trained via empirical risk minimization on s ∼dπ(s)\nand optimal labels a⋆, and attains generalization error ϵ on s ∼dπ(s), then ℓ(π) ≤C + Hϵ is the\nbest possible bound on the expected error of the learned policy.\nProof. The proof follows from Theorem 3.2 from Ross et al. (2011). This is the best possible bound,\nbecause the probability of a mistake at any time step is at least ϵ, and PH\nt=1 ϵ = Hϵ.\nThis means that, even with optimal action labels, we get an error bound that is at best quadratic in\nthe time horizon H in the ofﬂine case, but linear in H in the online case. Intuitively, the reason for\nthis gap in performance is that, in the ofﬂine case, the learned policy π(a|s) may enter into states\nthat are far outside of its training distribution, since dπ(s) may be very different from dπβ(s). In\nthese out-of-distribution states, the generalization error bound ϵ no longer holds, since standard\nempirical risk minimization makes no guarantees about error when encountering out-of-distribution\ninputs that were not seen during training. Once the policy enters one of these out-of-distribution\nstates, it will keep making mistakes and may remain out-of-distribution for the remainder of the trial,\naccumulating O(H) error. Since there is a non-trivial chance of entering an out-of-distribution state\nat every one of the H time steps, the overall error therefore scales as O(H2). In the on-policy case,\nsuch out-of-distribution states are not an issue. Of course, this example is somewhat orthogonal to\nthe main purpose of this tutorial, which is to study ofﬂine reinforcement learning algorithms, rather\nthan ofﬂine behavioral cloning methods. However, it should serve as a warning, as it indicates that\nthe challenges of distributional shift are likely to cause considerable harm to any policy trained from\nan ofﬂine dataset if care is not taken to minimize its detrimental effects.\n3\nOfﬂine Evaluation and Reinforcement Learning via Importance Sampling\nIn this section, we survey ofﬂine reinforcement learning algorithms based on direct estimation of\npolicy return. These methods generally utilize some form of importance sampling to either evaluate\nthe return of a given policy, or to estimate the corresponding policy gradient, corresponding to an\nofﬂine variant of the policy gradient methods discussed in Section 2.1. The most direct way to\nutilize this idea is to employ importance sampling to estimate J(π) with trajectories sampled from\nπβ(τ). This is known as off-policy evaluation. In principle, once we can evaluate J(π), we can select\nthe most performant policy. In this section, we review approaches for off-policy evaluation with\nimportance sampling and then discuss how these ideas can be used for ofﬂine reinforcement learning.\n3.1\nOff-Policy Evaluation via Importance Sampling\nWe can naïvely use importance sampling to derive an unbiased estimator of J(π) that relies on\noff-policy trajectories:\nJ(πθ) = Eτ∼πβ(τ)\n\"\nπθ(τ)\nπβ(τ)\nH\nX\nt=0\nγtr(s, a)\n#\n= Eτ∼πβ(τ)\n\" H\nY\nt=0\nπθ(at|st)\nπβ(at|st)\n! H\nX\nt=0\nγtr(s, a)\n#\n≈\nn\nX\ni=1\nwi\nH\nH\nX\nt=0\nγtri\nt,\n(5)\nwhere wi\nt = 1\nn\nQt\nt′=0\nπθ(ai\nt′|si\nt′)\nπβ(ai\nt′|si\nt′) and {(si\n0, ai\n0, ri\n0, si\n1, . . .)}n\ni=1 are n trajectory samples from πβ(τ)\n(Precup, 2000). Unfortunately, such an estimator can have very high variance (potentially unbounded\nif H is inﬁnite) due to the product of importance weights. Self-normalizing the importance weights\n(i.e., dividing the weights by Pn\ni=1 wi\nH) results in the weighted importance sampling estimator (Pre-\ncup, 2000), which is biased, but can have much lower variance and is still a strongly consistent\nestimator.\n10\nTo improve this estimator, we need to take advantage of the statistical structure of the problem.\nBecause rt does not depend on st′ and at′ for t′ > t, we can drop the importance weights from future\ntime steps, resulting in the per-decision importance sampling estimator (Precup, 2000):\nJ(πθ) = Eτ∼πβ(τ)\n\" H\nX\nt=0\n \ntY\nt′=0\nπθ(at|st)\nπβ(at|st)\n!\nγtr(s, a)\n#\n≈1\nn\nn\nX\ni=1\nH\nX\nt=0\nwi\ntγtri\nt.\nAs before, this estimator can have high variance, and we can form a weighted per-decision importance\nestimator by normalizing the weights. Unfortunately, in many practical problems, the weighted\nper-decision importance estimator still has too much variance to be effective.\nIf we have an approximate model that can be used to obtain an approximation to the Q-values for each\nstate-action tuple (st, at), which we denote ˆQπ(st, at), we can incorporate it into this estimate. Such\nan estimate can be obtained, for example, by estimating a model of the MDP transition probability\nT(st+1|st, at) and then solving for the corresponding Q-function, or via any other method for\napproximating Q-values. We can incorporate these estimates as control variates into the importance\nsampled estimator to get the best of both:\nJ(πθ) ≈\nn\nX\ni=1\nH\nX\nt=0\nγt \u0010\nwi\nt\n\u0010\nri\nt −ˆQπθ(st, at)\n\u0011\n−wi\nt−1Ea∼πθ(a|st)\nh\nˆQπθ(st, a)\ni\u0011\n.\n(6)\nThis is known as the doubly robust estimator (Jiang and Li, 2015; Thomas and Brunskill, 2016)\nbecause it is unbiased if either πβ is known or if the model is correct. We can also form a weighted\nversion by normalizing the weights. More sophisticated estimators can be formed by training the\nmodel with knowledge of the policy to be evaluated (Farajtabar et al., 2018), and by trading off bias\nand variance more optimally (Thomas and Brunskill, 2016; Wang et al., 2017).\nBeyond consistency or unbiased estimates, we frequently desire a (high-probability) guarantee on the\nperformance of a policy. Thomas et al. (2015) derived conﬁdence bounds based on concentration\ninequalities specialized to deal with the high variance and potentially large range of the importance\nweighted estimators. Alternatively, we can construct conﬁdence bounds based on distributional\nassumptions (e.g., asymptotic normality) (Thomas et al., 2015) or via bootstrapping (Thomas et al.,\n2015; Hanna et al., 2017) which may be less conservative at the cost of looser guarantees.\nSuch estimators can also be utilized for policy improvement, by searching over policies with respect\nto their estimated return. In safety-critical applications of ofﬂine RL, we would like to improve over\nthe behavior policy with a guarantee that with high probability our performance is no lower than a\nbound. Thomas et al. (2015) show that we can search for policies using lower conﬁdence bounds\non importance sampling estimators to ensure that the safety constraint is met. Alternatively, we can\nsearch over policies in a model of the MDP and bound the error of the estimated model with high\nprobability (Ghavamzadeh et al., 2016; Laroche et al., 2017; Nadjahi et al., 2019).\n3.2\nThe Off-Policy Policy Gradient\nImportance sampling can also be used to directly estimate the policy gradient, rather than just\nobtaining an estimate of the value for a given policy. As discussed in Section 2.1, policy gradient\nmethods aim to optimize J(π) by computing estimates of the gradient with respect to the policy\nparameters. We can estimate the gradient with Monte Carlo samples, as in Equation (2), but this\nrequires on-policy trajectories (i.e., τ ∼πθ(τ)). Here, we extend this approach to the ofﬂine setting.\nPrevious work has generally focused on the off-policy setting, where trajectories are sampled from a\ndistinct behavior policy πβ(a|s) ̸= π(a|s). However, in contrast to the ofﬂine setting, such methods\nassume we can continually sample new trajectories from πβ, while old trajectories are reused for\nefﬁciency. We begin with the off-policy setting, and then discuss additional challenges in extending\nsuch methods to the ofﬂine setting.\n11\nNoting the similar structure between J(π) and the policy gradient, we can adapt the techniques for\nestimating J(π) off-policy to the policy gradient\n∇θJ(πθ) = Eτ∼πβ(τ)\n\"\nπθ(τ)\nπβ(τ)\nH\nX\nt=0\nγt∇θ log πθ(at|st) ˆA(st, at)\n#\n= Eτ∼πβ(τ)\n\" H\nY\nt=0\nπθ(at|st)\nπβ(at|st)\n! H\nX\nt=0\nγt∇θ log πθ(at|st) ˆA(st, at)\n#\n≈\nn\nX\ni=1\nwi\nH\nH\nX\nt=0\nγt∇θ log πθ(ai\nt|si\nt) ˆA(si\nt, ai\nt),\nwhere {(si\n0, ai\n0, ri\n0, si\n1, . . .)}n\ni=1 are n trajectory samples from πβ(τ) (Precup, 2000; Precup et al.,\n2001; Peshkin and Shelton, 2002). Similarly, we can self-normalize the importance weights resulting\nin the weighted importance sampling policy gradient estimator (Precup, 2000), which is biased, but\ncan have much lower variance and is still a consistent estimator.\nIf we use the Monte Carlo estimator with baseline for ˆA (i.e., ˆA(si\nt, ai\nt) = PH\nt′=t γt′−trt′ −b(si\nt)),\nthen because rt does not depend on st′ and at′ for t′ > t, we can drop importance weights in the\nfuture, resulting in the per-decision importance sampling policy gradient estimator (Precup, 2000):\n∇θJ(πθ) ≈\nn\nX\ni=1\nH\nX\nt=0\nwi\ntγt\n H\nX\nt′=t\nγt′−t wi\nt′\nwi\nt\nrt′ −b(si\nt)\n!\n∇θ log πθ(ai\nt|si\nt).\nAs before, this estimator can have high variance, and we can form a weighted per-decision importance\nestimator by normalizing the weights. Paralleling the development of doubly robust estimators for\npolicy evaluation, doubly robust estimators for the policy gradient have also been derived (Gu et al.,\n2017a; Huang and Jiang, 2019; Pankov, 2018; Cheng et al., 2019). Unfortunately, in many practical\nproblems, these estimators have too high variance to be effective.\nPractical off-policy algorithms derived from such estimators can also employ regularization, such that\nthe learned policy πθ(a|s) does not deviate too far from the behavior policy πβ(a|s), thus keeping the\nvariance of the importance weights from becoming too large. One example of such a regularizer is\nthe soft max over the (unnormalized) importance weights (Levine and Koltun, 2013). This regularized\ngradient estimator ∇θ ¯J(πθ) has the following form:\n∇θ ¯J(πθ) ≈\n n\nX\ni=1\nwi\nH\nH\nX\nt=0\nγt∇θ log πθ(ai\nt|si\nt) ˆA(si\nt, ai\nt)\n!\n+ λ log\n n\nX\ni=1\nwi\nH\n!\n.\nIt is easy to check that P\ni wi\nH →1 as n →∞, meaning that this gradient estimator is consistent.\nHowever, with a ﬁnite number of samples, such an estimator automatically adjusts the policy πθ\nto ensure that at least one sample has a high (unnormalized) importance weight. More recent deep\nreinforcement learning algorithms based on importance sampling often employ a sample-based\nKL-divergence regularizer (Schulman et al., 2017), which has a functional form mathematically\nsimilar to this one when also utilizing an entropy regularizer on the policy πθ.\n3.3\nApproximate Off-Policy Policy Gradients\nThe importance-weighted policy objective requires multiplying per-action importance weights over\nthe time steps, which leads to very high variance. We can derive an approximate importance-sampled\ngradient by using the state distribution of the behavior policy, dπβ(s), in place of that of the current\npolicy, dπ(s). This results in a biased gradient due to the mismatch in state distributions, but can\nprovide reasonable learning performance in practice. The corresponding objective, which we will\ndenote Jπβ(πθ) to emphasize its dependence on the behavior policy’s state distribution, is given by\nJπβ(πθ) = Es∼dπβ (s) [V π(s)] .\nNote that Jπβ(πθ) and J(πθ) differ in the distribution of states under which the return is estimated\n(dπβ(s) vs. dπ(s)), making Jπβ(πθ) a biased estimator for J(πθ). This may lead to suboptimal\nsolutions in certain cases (see Imani et al. (2018) for some examples). However, expectations under\n12\nstate distributions from dπβ(s) can be calculated easily by sampling states from the dataset D in the\nofﬂine case, removing the need for importance sampling.\nRecent empirical work has found that this bias is acceptable in practice (Fu et al., 2019). Differentiat-\ning the objective and applying a further approximation results in the off-policy policy gradient (Degris\net al., 2012):\n∇θJπβ(πθ) = Es∼dπβ (s),a∼πθ(a|s) [Qπθ(s, a)∇θ log πθ(a|s) + ∇θQπθ(s, a)]\n≈Es∼dπβ (s),a∼πθ(a|s) [Qπθ(s, a)∇θ log πθ(a|s)] .\nDegris et al. (2012) show that under restrictive conditions, the approximate gradient preserves the\nlocal optima of Jπβ(π). This approximate gradient is used as a starting point in many widely used\ndeep reinforcement learning algorithms (Silver et al., 2014; Lillicrap et al., 2015; Wang et al., 2016;\nGu et al., 2017b).\nTo compute an estimate of the approximate gradient, we additionally need to estimate Qπθ(s, a)\nfrom the off-policy trajectories. Basic methods for doing this were discussed in Section 2.1, and we\ndefer a more in-depth discussion of ofﬂine state-action value function estimators to Section 4. Some\nestimators use action samples, which required an importance weight to correct from πβ samples\nto πθ samples. Further improvements can be obtained by introducing control variates and clipping\nimportance weights to control variance (Wang et al., 2016; Espeholt et al., 2018).\n3.4\nMarginalized Importance Sampling\nIf we would like to avoid the bias incurred by using the off-policy state distribution and the high\nvariance from using per-action importance weighting, we can instead attempt to directly estimate\nthe state-marginal importance ratio ρπθ(s) = dπθ (s)\ndπβ (s). An estimator using state marginal importance\nweights can be shown to be have no greater variance than using the product of per-action importance\nweights. However, computing this ratio exactly is generally intractable. Only recently have practical\nmethods for estimating the marginalized importance ratio been introduced (Sutton et al., 2016; Yu,\n2015; Hallak et al., 2015, 2016; Hallak and Mannor, 2017; Nachum et al., 2019a; Zhang et al., 2020a;\nNachum et al., 2019b). We discuss some key aspects of these methods next.\nMost of these methods utilize some form of dynamic programming to estimate the importance ratio\nρπ. Based on the form of the underlying Bellman equation used, we can classify them into two\ncategories: methods that use a “forward” Bellman equation to estimate the importance ratios directly,\nand methods that use a “backward” Bellman equation on a functional that resembles a value function\nand then derive the importance ratios from the learned value functional.\nForward Bellman equation based approaches.\nThe state-marginal importance ratio, ρπ(s), sat-\nisﬁes a kind of “forward” Bellman equation:\n∀s′,\ndπβ(s′)ρπ(s′)\n|\n{z\n}\n:=(dπβ ◦ρπ)(s′)\n= (1 −γ)d0(s′) + γ\nX\ns,a\ndπβ(s)ρπ(s)π(a|s)T(s′|s, a)\n|\n{z\n}\n:=( ¯\nBπ◦ρπ)(s′)\n.\n(7)\nThis relationship can be leveraged to perform temporal difference updates to estimate the state-\nmarginal importance ratio under the policy.\nFor example, when stochastic approximation is used, Gelada and Bellemare (2019) use the following\nupdate rule in order to estimate ρπ(s′) online:\nˆρπ(s′) ←ˆρπ(s′) + α\n\u0014\n(1 −γ) + γ π(a|s)\nπβ(a|s) ˆρπ(s) −ˆρπ(s′)\n\u0015\n,\n(8)\nwith s ∼dπβ(s), a ∼πβ(a|s), s′ ∼T(s′|s, a). Several additional techniques, including using TD(λ)\nestimates and automatic adjustment of feature dimensions, have been used to stabilize learning. We\nrefer the readers to Hallak and Mannor (2017) and Gelada and Bellemare (2019) for a detailed discus-\nsion. Gelada and Bellemare (2019) also discusses several practical tricks, such as soft-normalization\nand discounted evaluation, to adapt these methods to deep Q-learning settings, unlike prior work that\nmainly focuses on linear function approximation. Wen et al. (2020) view to problem from the lens of\npower iteration, and propose a variational power method approach to combine function approximation\n13\nand power iteration to estimate ρπ. Imani et al. (2018); Zhang et al. (2019) show that similar methods\ncan be used to estimate the off-policy policy gradient and thus be used in an off-policy actor critic\nmethod.\nAlternatively, Liu et al. (2018) propose to use an adversarial approach to obtain the state-marginal\nimportance ratios. From Eq. 7, they derive a functional\nL(ρ, f) = γEs,a,s′∼D\n\u0014\u0012\nρ(s) π(a|s)\nπβ(a|s) −ρ(s′)\n\u0013\nf(s′)\n\u0015\n+ (1 −γ)Es0∼d0[(1 −ρ(s))f(s)]\n(9)\nsuch that L(ρ, f) = 0, ∀f if and only if ρ = ρπ. Therefore, we can learn ρ by minimizing a worst-\ncase estimate of L(ρ, f)2, by solving an adversarial, saddle-point optimization: minρ maxf L(ρ, f)2.\nRecent work (Mousavi et al., 2020; Kallus and Uehara, 2019a,b; Tang et al., 2019; Uehara and Jiang,\n2019) has reﬁned this approach, in particular removing the need to have access to πβ. Once ρ∗is\nobtained, Liu et al. (2019) use this estimator to compute the off-policy policy gradient.\nZhang et al. (2020a) present another off-policy evaluation method that computes the importance ratio\nfor the state-action marginal, ρπ(s, a) :=\ndπ(s,a)\ndπβ (s,a), by directly optimizing a variant of the Bellman\nresidual error corresponding to a modiﬁed forward Bellman equation, that includes actions, shown in\nEquation 10.\ndπβ(s′, a′)ρπ(s′, a′)\n|\n{z\n}\n:=(dπβ ◦ρπ)(s′,a′)\n=(1 −γ)d0(s′)π(a′|s′)+γ\nX\ns,a\ndπβ(s, a)ρπ(s, a)π(a|s)T(s′|s, a)\n|\n{z\n}\n:=( ¯\nBπ◦ρπ)(s′,a′)\n.\n(10)\nTheir approach can be derived by applying a divergence metric (such as an f-divergence, which\nwe will review in Section 4) between the two sides of the modiﬁed forward Bellman equation in\nEquation 10, while additionally constraining the importance ratio, ρπ(s, a), to integrate to 1 in\nexpectation over the dataset, D, to prevent degenerate solutions, as follows\nmin\nρπ\nDf\n\u0000\u0000 ¯Bπ ◦ρπ\u0001\n(s, a), (dπβ ◦ρπ) (s, a)\n\u0001\ns.t. Es,a,s′∼D[ρπ(s, a)] = 1.\n(11)\nThey further apply tricks inspired from dual embeddings (Dai et al., 2016) to make the objective\ntractable, and to avoid the bias caused due to sampled estimates. We refer the readers to Zhang et al.\n(2020a) for further discussion.\nZhang et al. (2020b) show that primal-dual solvers may not be able to solve Eqn. 11, and modify the\nobjective in by replacing the f-divergence with a norm induced by 1/dπβ. This creates an optimization\nproblem that is provably convergent under linear function approximation.\nmin\nρπ\n1\n2||\n\u0000\u0000 ¯Bπ ◦ρπ\u0001\n(s, a) −(dπβ ◦ρπ) (s, a)\n\f\f |2\n(dπβ )−1 + λ\n2 (Es,a,s′∼D[ρπ(s, a)] −1)2.\n(12)\nBackward Bellman equation based approaches via convex duality.\nFinally, we discuss methods\nfor off-policy evaluation and improvement that utilize a backward Bellman equation – giving rise to\na value-function like functional – by exploiting convex duality. Because these methods start from\nan optimization perspective, they can bring to bear the mature tools of convex optimization and\nonline learning. Lee and He (2018) extend a line of work applying to convex optimization techniques\nto policy optimization (Chen and Wang, 2016; Wang and Chen, 2016; Dai et al., 2017a,b) to the\noff-policy setting. They prove sample complexity bounds in the off-policy setting, however, extending\nthese results to practical deep RL settings has proven challenging.\nNachum et al. (2019a) develop similar ideas for off-policy evaluation. The key idea is to devise a\nconvex optimization problem with ρπ as its optimal solution. The chosen optimization problem is\nρπ = arg\nmin\nx:S×A→R\n1\n2Es,a,s′∼D\n\u0002\nx(s, a)2\u0003\n−Es∼dπ(s),a∼π(a|s) [x(s, a)] .\n(13)\nUnfortunately, this objective requires samples from the on-policystate-marginal distribution, dπ(s).\nThe key insight is a change of variables, x(s, a) = ν(s, a) −Es′∼T (s′|s,a),a′∼π(a′|s′)[ν(s′, a′)] and\nintroduce the variable ν(s, a) to simplify the expression in Equation 13, and obtain the “backward”\ndynamic programming procedure shown in Equation 14. For brevity, we deﬁne a modiﬁed Bellman\n14\noperator, ˜Bπν(s, a) := Es′∼T (s′|s,a),a′∼π(a′|s′)[ν(s′, a′)], that is similar to the expression for Bπ\nwithout the reward term r(s, a).\nmin\nν:S×A→R\n1\n2Es,a,s′∼D\n\u0014\u0010\nν(s, a) −˜Bπν(s, a)\n\u00112\u0015\n−Es0∼d0(s0),a∼π(a|s0) [ν(s0, a)] .\n(14)\nRemarkably, Equation 14 does not require on-policy samples to evaluate. Given an optimal solution\nfor the objective in Equation 14, denoted as ν∗, we can obtain the density ratio, ρπ, using the relation,\nρπ(s, a) = ν∗(s, a) −˜Bπν∗(s, a). The density ratio can then be used for off-policy evaluation and\nimprovement.\nNachum et al. (2019b); Nachum and Dai (2020) build on a similar framework to devise an off-policy\nRL algorithm. The key idea is to obtain an estimate of the on-policy policy gradient for a state-\nmarginal regularized RL objective by solving an optimization problem. The regularizer applied in\nthis family of methods is the f-divergence between the state(-action) marginal of the learned policy\nand the state-action marginal of the dataset. We will cover f-divergences in detail in Section 4. The\nf-divergence regularized RL problem, with a tradeoff factor, α, is given by:\nmax\nπ\nEs∼dπ(s),a∼π(·|s) [r(s, a)] −αDf(dπ(s, a), dπβ(s, a)).\n(15)\nBy exploiting the variational (dual) form of the f-divergence shown below\nDf(p, q) =\nmax\nx:S×A→R\n\u0000Ey∼p(y)[x(y)] −Ey∼q(y)[f ∗(x(y))]\n\u0001\n,\n(16)\nand then applying a change of variables from x to Q (c.f., (Nachum et al., 2019a)) where Q sat-\nisﬁes Q(s, a) = Es′∼T (s,a),a′∼π(a′|s′) [r(s, a) −αx(s, a) + γQ(s′, a′)], we obtain a saddle-point\noptimization problem for optimizing the regularized RL objective,\nmax\nπ\nmin\nQ\nL(Q, πβ, π) := Es0∼d0(s0),a∼π(·|s0) [Q(s0, a)]\n+ αEs,a∼dπβ (s,a)\n\u0014\nf ∗\n\u0012r(s, a) + γEs′∼T (s,a),a′∼π(a′|s′)[Q(s′, a′)] −Q(s, a)\nα\n\u0013\u0015\n.\nWhen f(x) = x2, f ∗(x) = x2 and this objective reduces to applying a regular actor-critic algorithm\nas discussed in Section 2.1 along with an additional term minimizing Q-values at the initial state s0.\nIt can also be shown that the derivative with respect to the policy of L(Q∗, πβ, π), at the optimal\nQ-function, Q∗, is precisely equal to the on-policy policy gradient in the regularized policy gradient\nproblem:\n∂\n∂π L(Q∗, πβ, π) = Es∼dπ(s),a∼π(·|s)\nh\n˜Qπ(s, a) · ∇π log π(a|s)\ni\n,\n(17)\nwhere ˜Qπ is the action-value function corresponding to the regularized RL problem.\nFinally, we note that this is a rapidly developing area and recent results suggest that many of these\nmethods can be uniﬁed under a single framework (Tang et al., 2019; Nachum and Dai, 2020).\n3.5\nChallenges and Open Problems\nThe methods discussed in this section utilize some form of importance sampling to either estimate\nthe return of the current policy πθ, or estimate the gradient of this return. The policy improvement\nmethods discussed in this section have been developed primarily for a classic off-policy learning\nsetting, where additional data is collected online, but previously collected data is reused to improve\nefﬁciency. To the best of our knowledge, such methods have not generally been applied in the\nofﬂine setting, with the exception of the evaluation and high-conﬁdence improvement techniques in\nSection 3.1.\nApplying such methods in the fully ofﬂine setting poses a number of major challenges. First,\nimportance sampling already suffer from high variance, and this variance increases dramatically\nin the sequential setting, since the importance weights at successive time steps are multiplied\ntogether (see, e.g., Equation (5)), resulting in exponential blowup. Approximate and marginalized\nimportance sampling methods alleviate this issue to some degree by avoiding multiplication of\nimportance weights over multiple time steps, but the fundamental issue still remains: when the\n15\nbehavior policy πβ is too different from the current learned policy πθ, the importance weights will\nbecome degenerate, and any estimate of the return or the gradient will have too much variance to be\nusable, especially in high-dimensional state and action spaces, or for long-horizon problems. For this\nreason, importance-sampled estimators are most suitable in the case where the policy only deviates\nby a limited amount from the behavior policy. In the classic off-policy setting, this is generally the\ncase, since new trajectories are repeatedly collected and added to the dataset using the latest policy,\nbut in the ofﬂine setting this is generally not the case. Thus, the maximum improvement that can\nbe reliably obtained via importance sampling is limited by (i) the suboptimality of the behavior\npolicy; (ii) the dimensionality of the state and action space; (iii) the effective horizon of the task. The\nsecond challenge is that the most effective of these off-policy policy gradient methods either requires\nestimating the value function, or the state-marginal density ratios via dynamic programming. As\nseveral prior works have shown, and as we will review in Section 4, dynamic programming methods\nsuffer from issues pertaining to out-of-distribution queries in completely ofﬂine settings, making it\nhard to stably learn the value function without additional corrections. This problem is not as severe\nin the classic off-policy setting, which allows additional data collection. Nonetheless, as we will\ndiscussion in Section 6, a number of prior applications have effectively utilized importance sampling\nin an ofﬂine setting.\n4\nOfﬂine Reinforcement Learning via Dynamic Programming\nDynamic programming methods, such as Q-learning algorithms, in principle can offer a more\nattractive option for ofﬂine reinforcement learning as compared to pure policy gradients. As discussed\nin Section 2.1, dynamic programming methods aim to learn a state or state-action value function,\nand then either use this value function to directly recover the optimal policy or, as in the case of\nactor-critic methods, use this value function to estimate a gradient for the expected returns of a policy.\nBasic ofﬂine dynamic programming algorithms can be constructed on the basis of ﬁtted Q-learning\nmethods (Ernst et al., 2005; Riedmiller, 2005; Hafner and Riedmiller, 2011), as well as policy iteration\nmethods (Sutton and Barto, 1998). The generic Q-learning and actor-critic algorithms presented\nin Algorithm 2 and Algorithm 3 in Section 2.1 can in principle be utilized as ofﬂine reinforcement\nlearning, simply by setting the number of collection steps S to zero, and initializing the buffer to be\nnon-empty. Such algorithms can be viable for ofﬂine RL, and indeed have been used as such even in\nrecent deep reinforcement learning methods. We will discuss an older class of sucm methods, based\non linear function approximation, in Section 4.1, but such techniques have also been used effectively\nwith deep neural network function approximators. For example, Kalashnikov et al. (2018) describes a\nQ-learning algorithm called QT-Opt that was able to learn effective vision-based robotic grasping\nstrategies from about 500,000 grasping trials logged over the course of previous experiments, but\nobserves that additional online ﬁne-tuning still improved the performance of the policy considerably\nover the one trained purely from logged data. Some prior works on ofﬂine RL have also noted that,\nfor some types of datasets, conventional dynamic programming algorithms, such as deep Q-learning\nor deterministic actor-critic algorithms, can in fact work reasonably well (Agarwal et al., 2019).\nHowever, as we will discuss in Section 4.2, such methods suffer from a number of issues when\nall online collection is halted, and only ofﬂine data is used. These issues essentially amount to\ndistributional shift, as discussed in Section 2.4. Solutions to this issue can be broadly grouped into\ntwo categories: policy constraint methods, discussed in Section 4.3, which constrain the learned\npolicy π to lie close to the behavior policy πβ, thus mitigating distributional shift, and uncertainty-\nbased methods, discussed in Section 4.4, which attempt to estimate the epistemic uncertainty of\nQ-values, and then utilize this uncertainty to detect distributional shift. We will discuss both classes\nof distributional shift corrections, and then conclude with perspectives on remaining challenges and\nopen problems in Section 4.6.\n4.1\nOff-Policy Value Function Estimation with Linear Value Functions\nWe ﬁrst discuss standard ofﬂine reinforcement learning methods based on value function and policy\nestimation with linear function approximators, which do not by themselves provide any mitigate\nfor distributional shift, but can work well when good linear features are available. While modern\ndeep reinforcement learning methods generally eschew linear features in favor of non-linear neural\nnetwork function approximators, linear methods constitute an important class of ofﬂine reinforcement\nlearning algorithms in the literature (Lange et al., 2012; Lagoudakis and Parr, 2003). We begin\n16\nwith algorithms that use a linear function to approximate the Q-function, such that Qφ ≈f(s, a)T φ,\nwhere f(s, a) ∈Rd is a feature vector associated with a state-action pair. This parametric Q-function\ncan then be estimated for a given policy π(a|s) using data from a different behavior policy πβ(a|s),\nwith state visitation frequency dπβ(s). As discussed in Section 2.1, the Q-function Qπ for a given\npolicy π(a|s) must satisfy the Bellman equation, given in full in Equation (3), and written in Bellman\noperator notation as ⃗\nQπ = Bπ ⃗\nQπ.\nWhen linear function approximation is used to represent the Q-function, the Q-function for a policy\ncan be represented as the solution of a linear system, and hence can be computed via least squares\nminimization, since the Bellman operator Bπ is linear. This provides a convenient way to compute\nQπ directly in closed form, as compared to using gradient descent in Algorithm 3. The resulting\nQ-value estimates can then be used to improve the policy. We start with a discussion of different\nways of solving the linear system for computing Qπ. To recap, the Q-function is a linear function on\na feature vector f(s, a), which we can express in tabular form as F ∈R|S||A|×d, such that ⃗Qφ = Fφ.\nMultiple procedures that can be used to obtain a closed form expression for the optimal φ for a\ngiven policy π and, as discussed in by Sutton et al. (2009) and Lagoudakis and Parr (2003), these\nprocedures may each yield different solutions under function approximation. We ﬁrst summarize two\nmethods for selecting φ to approximate Qπ for a given policy π, and then discuss how to utilize these\nmethods in a complete reinforcement learning method.\nBellman residual minimization.\nThe ﬁrst approach selects φ such that the resulting linear Q-\nfunction satisﬁes the Bellman equation as closely as possible in terms of squared error, which can\nbe obtained via the least squares solution. To derive the corresponding least squares problem, we\nﬁrst write the Bellman equation in terms of the Bellman operator, and expand it using the vectorized\nexpression for the reward function, ⃗R, and a linear operator representing the dynamics and policy\nbackup, which we denote as P π, such that (P π ⃗Q)(s, a) = Es′∼T (s′|s,a),a′∼π(a′|s′)[Q(s′, a′)]. The\ncorresponding expression of the Bellman equation is given by\nFφ ≈BπFφ = ⃗R + γP πFφ =⇒(F −γP πF) φ ≈⃗R.\nWriting out the least squares solution using the normal equations, we obtain\nφ =\n\u0000(F −γP πF)T (F −γP πF)\n\u0001−1 (F −γP πF)T ⃗R.\nThis expression minimizes the ℓ2 norm of the Bellman residual (the squared difference between the\nleft-hand side and right-hand side of the Bellman equation), and is referred to as the Bellman residual\nminimizing solution.\nLeast-squares ﬁxed point approximation.\nAn alternate approach is use projected ﬁxed-point\niteration, rather than direct minimization of the Bellman error, which gives rise to the least-squares\nﬁxed point approximation. In this approach, instead of minimizing the squared difference between the\nleft-hand side and right-hand side of the Bellman equation, we instead attempt to iterate the Bellman\noperator to convergence. In the tabular case, as discussed in Section 2.1, we know that iterating\n⃗Qk+1 ←Bπ ⃗Qk converges, as k →∞, to ⃗Qπ. In the case where we use function approximation to\nrepresent ⃗Qk, we cannot set ⃗Qk+1 to Bπ ⃗Qk precisely, because there may not be any value of the\nweights φ that represent Bπ ⃗Qk exactly. We therefore must employ a projected ﬁxed point iteration\nmethod, where each iterate Bπ ⃗Qk = BπFφk is projected onto the span of F to obtain φk+1. We can\nexpress this projection via a projection operator, Π, such that the projected Bellman iteration is given\nby ⃗Qk+1 = ΠBπ ⃗Qk. We can obtain a solution for this operator by solving the normal equation, and\nobserve that Π = F(FT F)−1FT . We can expand out the projected Bellman iteration expression as:\n⃗Qk+1 = F(FT F)−1FT (⃗R + γP π ⃗Qk)\nFφk+1 = F(FT F)−1FT (⃗R + γP πFφk)\nφk+1 = (FT F)−1FT (⃗R + γP πFφk).\n(18)\nBy repeatedly applying this recurrence, we can obtain the ﬁxed point of the projected Bellman\noperator as k →∞(Sutton et al., 2009).\nBoth methods have been used in the literature, and there is no clear consensus on which approach\nis preferred, though they yield different solutions in general when the true Q-function Qπ is not\n17\nin the span of F (i.e., cannot be represented by any parameter vector φ) (Lagoudakis and Parr,\n2003). We might at ﬁrst surmise that a good linear ﬁtting method should ﬁnd the Q-function Fφ\nthat corresponds to a least-squares projection of the true ⃗Qπ onto the hyperplane deﬁned by F.\nUnfortunately, neither the Bellman residual minimization method nor the least-squares ﬁxed point\nmethod in general obtains this solution. The Bellman residual minimization does not in general\nproduce a ﬁxed point of either the Bellman operator or the projected Bellman operator. However, the\nsolution obtained via Bellman residual minimization may be closer to the true Q-function in terms\nof ℓ2 distance, since it is explicitly obtained by minimizing Bellman residual error. Least-squares\nﬁxed point iteration obtains a Q-function that is a ﬁxed point of the projected Bellman operator, but\nmay be arbitrarily suboptimal. However, least-squares ﬁxed point iteration can learn solutions that\nlead, empirically, to better-performing policies (Sutton et al., 2009; Lagoudakis and Parr, 2003). In\ngeneral, there are no theoretical arguments that justify the superiority of one approach over the other.\nIn practice, least-squares ﬁxed-point iteration can give rise to more effective policies as compared\nto the Bellman residual, while the Bellman residual minimization approach can be more stable and\npredictable (Lagoudakis and Parr, 2003).\nLeast squares temporal difference Q-learning (LSTD-Q).\nNow that we have covered different\napproaches to solve a linear system of equations to obtain an approximation to Qπ, we describe\nLSTD-Q, a method for estimating Qπ from a static dataset, directly from samples. This method\nincrementally estimates the terms FT (F −γP πF) and FT ⃗R, which appear in Equation (18), and\nthen inverts this sampled estimate to then obtain φ. We defer further discussion on LSTD-Q to\nLagoudakis and Parr (2003), which also describes several computationally efﬁcient implementations\nof the LSTD-Q algorithm. Note that the LSTD-Q algorithm is not directly applicable to estimating\nQ∗, the optimal Q-function, since the optimal Bellman equation for Q∗is not linear due to the\nmaximization, and thus cannot be solved in closed form.\nLeast squares policy iteration (LSPI).\nFinally, we discuss least-squares policy iteration (LSPI), a\nclassical ofﬂine reinforcement learning algorithm that performs approximate policy iteration (see\ndiscussion in Section 2.1) using a linear approximation to the Q-function. LSPI uses LSTD-Q as an\nintermediate sub-routine to perform approximate policy evaluation, thereby obtaining an estimate\nfor Qπ, denoted Qφ. Then, the algorithm sets the next policy iterate to be equal to the greedy\npolicy corresponding to the approximate Qφ, such that πk+1(a|s) = δ(a = arg maxa Qφ(s, a)).\nAn important and useful characteristic of LSPI is that it does not require a separate approximate\nrepresentation for the policy when the actions are discrete, and hence removes any source of error\nthat arises due to function approximation in the actor in actor-critic methods. However, when the\naction space is continuous, a policy gradient method similar to the generic actor-critic algorithm in\nAlgorithm 3 can be used to optimize a parametric policy.\n4.2\nDistributional Shift in Ofﬂine Reinforcement Learning via Dynamic Programming\nBoth the linear and non-linear dynamic programming methods discussed so far, in Section 2.1\nand Section 4.1 above, can in principle learn from ofﬂine data, collected according to a different\n(unknown) behavioral policy πβ(a|s), with state visitation frequency dπβ(s). However, in practice,\nthese procedures can result in very poor performance, due to the distributional shift issues alluded to\nin Section 2.4.\nDistributional shift affects ofﬂine reinforcement learning via dynamic programming both at test time\nand at training time. At test time, the state visitation frequency dπ(s) induced by a policy trained with\nofﬂine RL will differ systematically from the state visitation frequency of the training data, dπβ(s).\nThis means that, as in the case of policy gradients, a policy trained via an actor-critic method may\nproduce unexpected and erroneous actions in out-of-distribution states s ∼dπβ(s), as can the implicit\ngreedy policy induced by a Q-function in a Q-learning method. One way to mitigate this distributional\nshift is to limit how much the learned policy can diverge from the behavior policy. For example, by\nconstraining π(a|s) such that DKL(π(a|s)∥πβ(a|s)) ≤ϵ, we can bound DKL(dπ(s)∥dπβ(s)) by δ,\nwhich is O(ϵ/(1 −γ)2) (Schulman et al., 2015). This bound is very loose in practice, but nonetheless\nsuggests that the effects of state distribution shift can potentially be mitigated by bounding how much\nthe learned policy can deviate from the behavior policy that collected the ofﬂine training data. This\nmay come at a substantial cost in ﬁnal performance however, as the behavior policy – and any policy\nthat is close to it – may be much worse than the best policy that can be learned from the ofﬂine data.\n18\nIt should be noted that, for the algorithms discussed previously, state distribution shift affects test-\ntime performance, but has no effect on training, since neither the policy nor the Q-function is ever\nevaluated at any state that was not sampled from dπβ(s). However, the training process is affected by\naction distribution shift, because the target values for the Bellman backups in Equation (3) depend on\nat+1 ∼π(at+1|st+1). While this source of distribution shift may at ﬁrst seem insigniﬁcant, it in fact\npresents one of the largest obstacles for practical application of approximate dynamic programming\nmethods to ofﬂine reinforcement learning. Since computing the target values in Equation (3) requires\nevaluating Qπ(st+1, at+1), where at+1 ∼π(at+1|st+1), the accuracy of the Q-function regression\ntargets depends on the estimate of the Q-value for actions that are outside of the distribution of\nactions that the Q-function was ever trained on. When π(a|s) differs substantially from πβ(a|s),\nthis discrepancy can result in highly erroneous target Q-values. This issue is further exacerbated by\nthe fact that π(a|s) is explicitly optimized to maximize Ea∼π(a|s)[Qπ(s, a)]. This means that, if the\npolicy can produce out-of-distribution actions for which the learned Q-function erroneously produces\nexcessively large values, it will learn to do so. This is true whether the policy is represented explicitly,\nas in actor-critic algorithms, or implicitly, as the greedy policy π(a|s) = δ(a = arg maxa′ Qπ(s, a′)).\nIn standard online reinforcement learning, such issues are corrected naturally when the policy acts in\nthe environment, attempting the transitions it (erroneously) believes to be good, and observing that\nin fact they are not. However, in the ofﬂine setting, the policy cannot correct such over-optimistic\nQ-values, and these errors accumulate over each iteration of training, resulting in arbitrarily poor\nﬁnal results.\n0.0K\n0.2K\n0.4K\n0.6K\n0.8K\n1.0K\nTrainSteps\n−1000\n−750\n−500\n−250\n0\n250\n500\n750\n1000\nHalfCheetah-v2: AverageReturn\nn=1000\nn=10000\nn=100000\nn=1000000\n0.0K\n0.2K\n0.4K\n0.6K\n0.8K\n1.0K\nTrainSteps\n0\n5\n10\n15\n20\n25\n30\nHalfCheetah-v2: log(Q)\nn=1000\nn=10000\nn=100000\nn=1000000\nFigure 2: Performance of SAC (Haarnoja et al., 2018),\nan actor-critic method, on the HalfCheetah-v2 task in\nthe ofﬂine setting, showing return as a function of gra-\ndient steps (left) and average learned Q-values on a log\nscale (right), for different numbers of training points (n).\nNote that an increase the number of samples does not\ngenerally prevent the “unlearning effect,” indicating that\nit is distinct from overﬁtting. Figure from Kumar et al.\n(2019).\nThis problem manifests itself in practice as an\n“unlearning” effect when running ofﬂine RL via\ndynamic programming. The experiments in Fig-\nure 2, from Kumar et al. (2019), show this un-\nlearning effect. The horizontal axis corresponds\nto the number of gradient updates to the Q-\nfunction, and the vertical axis shows the actual\nreturn obtained by running the greedy policy\nfor the learned Q-function. The learning curve\nresembles what we might expect in the case of\noverﬁtting: the return ﬁrst improves, and then\nsharply falls as training progresses. However,\nthis “overﬁtting” effect remains even as we in-\ncrease the size of the dataset, suggesting that the\nphenomenon is distinct from classic statistical\noverﬁtting. As the Q-function is trained longer\nand longer, the target values become more and\nmore erroneous, and the entire Q-function de-\ngrades.\nThus, to effectively implement ofﬂine reinforcement learning via dynamic programming, it is crucial\nto address this out-of-distribution action problem. Previous works have observed several variants\nof this problem. Fujimoto et al. (2018) noted that restricting Q-value evaluation only to actions in\nthe dataset avoids erroneous Q-values due to extrapolation error. Kumar et al. (2019) described the\nout-of-distribution action problem in terms of distributional shift, which suggests a less restrictive\nsolution based on limiting distributional shift, rather than simply constraining to previously seen\nactions. A number of more recent works also observe that a variety of constraints can be used to\nmitigate action distribution shift (Wu et al., 2019a). We provide a uniﬁed view of such “policy\nconstraint” methods in the following section.\n4.3\nPolicy Constraints for Off-Policy Evaluation and Improvement\nThe basic idea behind policy constraint methods for ofﬂine reinforcement learning via dynamic\nprogramming is to ensure, explicitly or implicitly, that regardless of the target values in Equation (3),\nthe distribution over actions under which we compute the target value, π(a′|s′), is “close” to\nthe behavior distribution πβ(a′|s′). This ensures that the Q-function is never queried on out-of-\ndistribution actions, which may introduce errors into the computation of the Bellman operator. If all\nstates and actions fed into the Q-function for target value calculations are always in-distribution with\nrespect to the Q-function training set, errors in the Q-function should not accumulate, and standard\n19\ngeneralization results from supervised learning should apply. Since the Q-function is evaluated on the\nsame states as the ones on which it is trained, only the action inputs can be out of distribution, when we\ncompute Ea′∼π(a′|s′)[Q(s′, a′)], and therefore it is sufﬁcient to keep π(a′|s′) close to πβ(a′|s′). Of\ncourse, in practice, the distributions do need to deviate in order for the learned policy to improve over\nthe behavior policy, but by keeping this deviation small, errors due to out-of-distribution action inputs\ncan be kept under control. The different methods in this family differ in terms of the probability metric\nused to deﬁne “closeness” and how this constraint is actually introduced and enforced. We can broadly\ncategorize these methods along these two axes. We will discuss explicit f-divergence constraints,\nwhich directly add a constraint to the actor update to keep the policy π close to πβ in terms of\nan f-divergence (typically the KL-divergence), implicit f-divergence constraints, which utilize\nan actor update that, by construction, keeps π close to πβ, and integral probability metric (IPM)\nconstraints, which can express constraints with more favorable theoretical and empirical properties\nfor ofﬂine RL. Furthermore, the constraints can be enforced either as direct policy constraints on\nthe actor update, or via a policy penalty added to the reward function or target Q-value.\nFormally, we can express the family of policy iteration methods with policy constraints as a ﬁxed\npoint iteration involving iterative (partial) optimization of the following objectives:\nˆQπ\nk+1 ←arg min\nQ E(s,a,s′)∼D\n\u0014\u0010\nQ(s, a) −\n\u0010\nr(s, a) + γEa′∼πk(a′|s′)[ ˆQπ\nk(s′, a′)]\n\u0011\u00112\u0015\nπk+1 ←arg max\nπ\nEs∼D\nh\nEa∼π(a|s)[ ˆQπ\nk+1(s, a)]\ni\ns.t. D(π, πβ) ≤ϵ.\nWhen the min and max optimizations are not performed to convergence, but instead for a limited\nnumber of gradient steps, we recover the general actor-critic method in Algorithm 3, with the\nexception of the constraint D(π, πβ) ≤ϵ on the policy update. A number of prior methods instantiate\nthis approach with different choices of D (Kumar et al., 2019; Fujimoto et al., 2018; Siegel et al.,\n2020). We will refer to this class of algorithms as policy constraint methods.\nIn policy penalty methods, the actor-critic algorithm is modiﬁed to incorporate the constraint into\nthe Q-values, which forces the policy to not only avoid deviating from πβ(a|s) at each state, but to\nalso avoid actions that may lead to higher deviation from πβ(a|s) at future time steps. This can be\naccomplished by adding a penalty term αD(π(·|s), πβ(·|s)) to the reward function r(s, a) leading\nto the modiﬁed reward function ¯r(s, a) = r(s, a) −αD(π(·|s), πβ(·|s)). The most well-known\nformulation of policy penalty methods stems from the linearly solvable MDP framework (Todorov,\n2006), or equivalently the control as inference framework (Levine, 2018), where D is chosen to be\nthe KL-divergence. An equivalent formulation adds the penalty term αD(π(·|s), πβ(·|s)) directly to\nthe target Q-values and the actor objective (Wu et al., 2019a; Jaques et al., 2019), resulting in the\nfollowing algorithm:\nˆQπ\nk+1 ←arg min\nQ\nE(s,a,s′)∼D\n\u0014\u0010\nQ(s, a)−\n\u0010\nr(s, a)+γEa′∼πk(a′|s′)[ ˆQπ\nk(s′, a′)]−αγD(πk(·|s′), πβ(·|s′))\n\u0011\u00112\u0015\nπk+1 ←arg max\nπ\nEs∼D\nh\nEa∼π(a|s)[ ˆQπ\nk+1(s, a)] −αD(π(·|s), πβ(·|s))\ni\n.\nWhile the basic recipe for policy constraint and policy penalty methods is similar, the particular\nchoice of how the constraints are deﬁned and how they are enforced can make a signiﬁcant difference\nin practice. We will discuss these choices next, as well as their tradeoffs.\nExplicit f-divergence constraints.\nFor any convex function f, the corresponding f-divergence is\ndeﬁned as:\nDf(π(·|s), πβ(·|s)) = Ea∼π(·|s)\n\u0014\nf\n\u0012 π(a|s)\nπβ(a|s)\n\u0013\u0015\n.\n(19)\nKL-divergence, χ2-divergence, and total-variation distance are some commonly used members of\nthe f-divergence family, corresponding to different choices of function f (Nowozin et al., 2016). A\nvariational form for the f-divergence can also be written as\nDf(π(·|s), πβ(·|s)) =\nmax\nx:S×A→R Ea∼π(·|s) [x(s, a)] −Ea′∼πβ(·|s) [f ∗(x(s, a′))] ,\n(20)\n20\nwhere f ∗is the convex conjugate for the convex function f. Both the primal form (Equation 19)\nand the dual variational form (Equation 20) of the f-divergence has been used to specify policy\nconstraints. In the dual form, an additional neural network is used to represent the function x. The\nstandard form of “passive dynamics” in linearly solvable MDPs (Todorov, 2006) or the action prior\nin control as inference (Levine, 2018) corresponds to the KL-divergence (primal form), which has\nalso been used in recent work that adapts such a KL-divergence penalty for ofﬂine reinforcement\nlearning (Jaques et al., 2019; Wu et al., 2019a). The KL-divergence, given by DKL(π, πβ) =\nEa∼π(·|s)[log π(a|s) −log πβ(a|s)], can be computed by sampling action samples a ∼π(·|s), and\nthen computing sample-wise estimates of the likelihoods inside the expectation. It is commonly used\ntogether with “policy penalty” algorithms, by simply adding an estimate of −α log πβ(a|s) to the\nreward function, and employing an entropy regularized reinforcement learning algorithm, which\nanalytically adds H(π(·|s)) to the actor objective (Levine, 2018). One signiﬁcant disadvantage of\nthis approach is that it requires explicit estimation of the behavior policy (e.g., via behavioral cloning)\nto ﬁt log πβ(a|s).\nAdditionally, the sub-family of asymmetrically-relaxed f-divergences can be used for the policy\nconstraint. For any chosen convex function f, these divergences modify the expression for Df to\nintegrate over only those a such that the density ration π(·|s)/πβ(a|s) is larger than a pre-deﬁned\nthreshold, thereby not penalizing small density ratios. Wu et al. (2019a) brieﬂy discuss this divergence\nsub-family, and we refer readers to Wu et al. (2019b) for a detailed description.\nImplicit f-divergence constraints.\nThe KL-divergence constraint can also be enforced implicitly,\nas in the case of AWR (Peng et al., 2019), AWAC (Nair et al., 2020), and ABM (Siegel et al.,\n2020). These methods ﬁrst solve for the optimal next policy iterate under the KL-divergence\nconstraint, indicated as ¯πk+1, non-parameterically, and then project it onto the policy function class\nvia supervised regression, implementing the following procedure:\n¯πk+1(a|s) ←1\nZ πβ(a|s) exp\n\u0012 1\nαQπ\nk(s, a)\n\u0013\nπk+1 ←arg min\nπ DKL(¯πk+1, π)\nIn practice, the ﬁrst step can be implemented by weighting samples from πβ(a|s) (i.e., the data\nin the buffer D) by importance weights proportional to exp\n\u0000 1\nαQπ\nk(s, a)\n\u0001\n, and the second step can\nbe implemented via a weighted supervised learning procedure employing these weights. In this\nway, importance sampling effectively implements a KL-divergence constraint on the policy, with α\ncorresponding to the Lagrange multiplier. The Q-function Qπ\nk corresponding to the previous policy\nπk can be estimated with any Q-value or advantage estimator. We refer the reader to related work for\na full derivation of this procedure (Peng et al., 2019; Nair et al., 2020).\nIntegral probability metrics (IPMs).\nAnother choice of the policy constraint D is an integral\nprobability metric, which is a divergence measure with the following functional form dependent on a\nfunction class Φ:\nDΦ(π(·|s), πβ(·|s)) =\nsup\nφ∈Φ,φ:S×A→R\n\f\fEa∼π(·|s)[φ(s, a)] −Ea′∼πβ(·|s)[φ(s, a′)]\n\f\f .\n(21)\nDifferent choices for the function class Φ give rise to different divergences. For example, when Φ\nconsists of all functions with a unit Hilbert norm in a reproducing kernel Hilbert space (RKHS), DΦ\nrepresents the maximum mean discrepancy (MMD) distance, which can alternatively be computed\ndirectly from samples as following:\nMMD2(π(·|s), πβ(·|s)) = Ea∼π(·|s),a′∼π(·|s) [k(a, a′)] −\n2Ea∼π(·|s),a′∼πβ(·|s) [k(a, a′)] + Ea∼πβ(·|s),a′∼πβ(·|s) [k(a, a′)] ,\nwhere k(·, ·) represents any radial basis kernel, such as the Gaussian or Laplacian kernel. As another\nexample, when Φ consists of all functions φ with a unit Lipschitz constant, then DΦ is equivalent to\nthe ﬁrst order Wasserstein (W1) or Earth-mover’s distance:\nW1(π(·|s), πβ(·|s)) =\nsup\nf,||f||L≤1\n\f\fEa∼π(·|s)[f(a)] −Ea∼πβ(·|s[f(a)]\n\f\f .\n(22)\nThese metrics are appealing because they can often be estimated with non-parametric estimators. We\nrefer the readers to Sriperumbudur et al. (2009) for a detailed discussion on IPMs. BEAR (Kumar\net al., 2019) uses the MMD distance to represent the policy constraint, while Wu et al. (2019a)\nevaluates an instantiation of the ﬁrst order Wasserstein distance.\n21\nTradeoffs between different constraint types.\nThe KL-divergence constraint, as well as other f-\ndivergences, represent a particularly convenient class of constraints, since they readily lend themselves\nto be used in a policy penalty algorithm by simply augmenting the reward function according to\n¯r(s, a) = r(s, a) −αf(π(a|s)/πβ(a|s)), with the important special case of the KL-divergence\ncorresponding to ¯r(s, a) = r(s, a) + α log πβ(a|s) −α log π(a|s), with the last term subsumed\ninside the entropy regularizer H(π(·|s)) when using a maximum entropy reinforcement learning\nalgorithm (Levine, 2018). However, KL-divergences and other f-divergences are not necessarily ideal\nfor ofﬂine reinforcement learning. Consider a setting where the behavior policy is uniformly random.\nIn this case, ofﬂine reinforcement learning should, in principle, be very effective. In fact, even\nstandard actor-critic algorithms without any policy constraints can perform very well in this setting,\nsince when all actions have equal probability, there are no out-of-distribution actions (Kumar et al.,\n2019). However, a KL-divergence constraint in this setting would restrict the learned policy π(a|s)\nfrom being too concentrated, requiring the constrained algorithm to produce a highly stochastic (and\ntherefore highly suboptimal) policy. Intuitively, an effective policy constraint should prevent the\nlearned policy π(a|s) from going outside the set of actions that have a high probability in the data, but\nwould not prevent it from concentrating around a subset of high-probability actions. This suggests\nthat a KL-divergence constraint is not in general the ideal choice.\nFigure 3: A comparison of support and distribution constraints\non a simple 1D lineworld from Kumar (2019). The task requires\nmoving to the goal location (marked as ’G’) starting from ’S’. The\nbehavior policy strongly prefers the left action at each state (b),\nsuch that distribution constraint is unable to ﬁnd the optimal policy\n(c), whereas support-constraint can successfully obtain the optimal\npolicy (d). We refer to Kumar (2019) for further discussion.\nIn contrast, as argued by Kumar et al.\n(2019) and Laroche et al. (2017), re-\nstricting the support of the learned\npolicy π(a|s) to the support of the\nbehavior distribution πβ(a|s) may be\nsufﬁcient to theoretically and empir-\nically obtain an effective ofﬂine RL\nmethod. In the previous example, if\nonly the support of the learned pol-\nicy is constrained to lie in the sup-\nport of the behavior policy, the learned\npolicy can be deterministic and opti-\nmal. However, when the behavior pol-\nicy does not have full support, a sup-\nport constraint will still prevent out-\nof-distribution actions. Kumar (2019)\nformalize this intuition and present a\nsimple example of a 1D-lineworld environment, which we reproduce in Figure 3 where constraining\ndistributions can lead to highly suboptimal behavior, that strongly prefers moving leftwards, thus\nreaching the goal location with only very low likelihood over the course of multiple intermediate\nsteps of decision making, while support constraints still allow for learning the optimal policy, since\nthey are agnostic to the probability density function in this setting.\nWhich divergence metrics can be used to constrain policy supports? In a ﬁnite sample setting, the\nfamily of f-divergences measures the difference in the probability densities, which makes it unsuitable\nfor support matching. In an MDP with a discrete action-space, a simple choice for constraining the\nsupport is a metric that penalizes the total amount of probability mass on out-of-distribution actions\nunder the π distribution, as shown below:\nDsupport,ε(π(·|s), πβ(·|s)) =\nX\na∈A,πβ(a|s)≤ε\nπ(a|s).\n(23)\nThis metric has been used in the context of off-policy bandits (Sachdeva et al., 2020), but not in the\ncontext of ofﬂine reinforcement learning. When the underlying MDP has a continuous action space\nand exact support cannot be estimated, Kumar et al. (2019) show that a ﬁnite sample estimate of the\nMMD distance can be used to approximately constrain supports of the learned policy. Similar to\nf-divergences, the MMD distance still converges to a divergence estimate between the distribution\nfunctions of its arguments. However, Kumar et al. (2019) show experimentally (Figure 7) that, in\nthe ﬁnite sample setting, MMD resembles a support constraining metric. The intuition is that the\nMMD distance does not depend on the speciﬁc densities of the behavior distribution or the policy,\nand can be computed via a kernel-based distance on samples from each distribution, thus making it\njust sufﬁcient enough for constraining supports when ﬁnite samples are used. Some variants of the\n22\nasymmetric f-divergence that asymmetrically penalize the density ratios π(·|s)/πβ(·|s) can also be\nused to approximately constrain supports (Wu et al., 2019b,a).\n4.4\nOfﬂine Approximate Dynamic Programming with Uncertainty Estimation\nAs discussed in Section 4.3, policy constraints can prevent out-of-distribution action queries to the\nQ-function when computing the target values. Aside from directly constraining the policy, we can\nalso mitigate the effect of out-of-distribution actions by making the Q-function resilient to such\nqueries, via effective uncertainty estimation. The intuition behind these uncertainty-based methods\nis that, if we can estimate the epistemic uncertainty of the Q-function, we expect this uncertainty\nto be substantially larger for out-of-distribution actions, and can therefore utilize these uncertainty\nestimates to produce conservative target values in such cases. In this section, we brieﬂy review such\nuncertainty-aware formulations of ofﬂine approximate dynamic programming methods.\nFormally, such methods require learning an uncertainty set or distribution over possible Q-functions\nfrom the dataset D, which we can denote PD(Qπ). This can utilize explicit modeling of conﬁdence\nsets, such as by maintaining upper and lower conﬁdence bounds (Jaksch et al., 2010), or directly repre-\nsenting samples from the distribution over Q-functions, for example via bootstrap ensembles (Osband\net al., 2016), or by parameterizing the distribution over Q-values using a known (e.g., Gaussian)\ndistribution with learned parameters (O’Donoghue et al., 2018). If calibrated uncertainty sets can\nbe learned, then we can improve the policy using a conservative estimate of the Q-function, which\ncorresponds to the following policy improvement objective:\nπk+1 ←arg max\nπ\nEs∼D\nh\nEa∼π(a|s)\nh\nEQπ\nk+1∼PD(Qπ)[Qπ\nk+1(s, a)] −αUnc(PD(Qπ))\nii\n|\n{z\n}\nconservative estimate\n,\n(24)\nwhere Unc denotes a metric of uncertainty, such that subtracting it provides a conservative estimate\nof the actual Q-function. The choice of the uncertainty metric Unc depends on the particular choice\nof uncertainty estimation method, and we discuss this next.\nWhen bootstrap ensembles3 are used to represent the Q-function, as is common in prior work (Osband\net al., 2016; Eysenbach et al., 2017; Kumar et al., 2019; Agarwal et al., 2019), typical choices of\n‘Unc’ include variance across Q-value predictions of an ensemble of Q-functions (Kumar et al., 2019),\nor maximizing the Q-value with respect to the worst case or all convex combinations of the Q-value\npredictions of an ensemble (Agarwal et al., 2019). When a parametric distribution, such as a Gaussian,\nis used to represent the Q-function (O’Donoghue et al., 2018), a choice of Unc, previously used for\nexploration, is to use a standard deviation above the mean as an optimistic Q-value estimate for policy\nimprovement. When translated to ofﬂine RL, an analogous estimate would be to use conservative\nQ-values, such as one standard deviation below the mean, for policy improvement.\n4.5\nConservative Q-Learning and Pessimistic Value Functions\nAs an alternative to imposing constraints on the policy in an actor-critic framework, an effective\napproach to ofﬂine RL can also be developed by regularizing the value function or Q-function directly\nto avoid overestimation for out-of-distribution actions (Kumar et al., 2020b). This approach can be\nappealing for several reasons, such as being applicable to both actor-critic and Q-learning methods,\neven when a policy is not represented explicitly, and avoiding the need for explicit modeling of\nthe behavior policy. The practical effect of such a method resembles the conservative estimate in\nEquation (24), but without explicit uncertainty estimation. As discussed by Kumar et al. (2020b), one\nsimple way to ensure a conservative Q-function is to modify the objective for ﬁtting the Q-function\nparameters φ on Line 14 of the Q-learning method in Algorithm 2 or Line 14 of the actor-critic\nmethod in Algorithm 3 to add an additional conservative penalty term, yielding a modiﬁed objective\n˜E(B, φ) = αC(B, φ) + E(B, φ),\n3It is known in the deep learning literature that, although a true bootstrap ensemble requires resampling the\ndataset with replacement for every bootstrap, omitting this resampling step and simply using different random\ninitialization for every neural network in the ensemble is sufﬁcient to differentiate the models and provide\nreasonable uncertainty estimates (Osband et al., 2016). In fact, previous work has generally observed little\nbeneﬁt from employ proper resampling (Osband et al., 2016), which technically means that all of these methods\nsimply employ regular (non-bootstrapped) ensembles, although the quality of their uncertainty estimates is\nempirically similar.\n23\nwhere different choices for C(B, φ) lead to algorithms with different properties. A basic example\nis the penalty CCQL0(B, φ) = Es∼B,a∼µ(a|s)[Qφ(s, a)]. Intuitively, this penalty minimizes the Q-\nvalues at all of the states in the buffer, for actions selected according to the distribution µ(a|s). If\nµ(a|s) is chosen adversarially, for example by maximizing the penalty CCQL0(B, φ), the effect is\nthat the conservative penalty will push down on high Q-values. Note that the standard Bellman\nerror term E(B, φ) will still enforce that the Q-values obey the Bellman backup for in-distribution\nactions. Therefore, if the penalty weight α is chosen appropriately, the conservative penalty should\nmostly push down on Q-values for out-of-distribution actions for which the Q-values are (potentially\nerroneously) high, since in-distribution actions would be “anchored” by the Bellman error E(B, φ).\nIndeed, Kumar et al. (2020b) show that, for an appropriately chosen value of α, a Q-function trained\nwith this conservative penalty will represent a lower bound on the true Q-function Q(s, a) for the\ncurrent policy, both in theory and in practice. This results in a provably conservative Q-learning or\nactor-critic algorithm. A simple and practical choice for µ(a, s) is to use a regularized adversarial\nobjective, such that\nµ = arg max\nµ\nEs∼D\n\u0002\nEa∼µ(a|s)[Qφ(s, a)] + H(µ(·|s))\n\u0003\n.\nThis choice inherits the lower bound guarantee above, and is simple to compute. The solution to the\nabove optimization is given by µ(a|s) ∝exp(Q(s, a)), and we can express CCQL0(B, φ) under this\nchoice of µ(a|s) in closed form as CCQL0(B, φ) = Es∼B[log P\na exp(Qφ(s, a))]. This expression\nhas a simple intuitive interpretation: the log-sum-exp is dominated by the action with the largest\nQ-value, and hence this type of penalty tends to minimize whichever Q-value is largest at each\nstate. When the action space is large or continuous, we can estimate this quantity by sampling and\nreweighting. For example, Kumar et al. (2020b) propose sampling from the current actor (in an\nactor-critic algorithm) and computing importance weights.\nWhile the approach described above has the appealing property of providing a learned Q-function\nQφ(s, a) that lower bounds the true Q-function, and therefore provably avoids overestimation, it tends\nto suffer from excessive underestimation – that is, it is too conservative. A simple modiﬁcation, which\nwe refer to as CCQL1(B, φ), is to also add a value maximization term to balance out the minimization\nterm under µ(a|s), yielding the following expression:\nCCQL1(B, φ) = Es∼B,a∼µ(a|s)[Qφ(s, a)] −E(s,a)∼B[Qφ(s, a)].\nThis conservative penalty minimizes Q-values under the adversarially chosen µ(a|s) distribution,\nand maximizes the values for state-action tuples in the batch. Intuitively, this acts to ensure that high\nQ-values are only assigned to in-distribution actions. When µ(a|s) is equal to the behavior policy, the\npenalty is zero on average. While this penalty does not produce a Q-function that is a pointwise lower\nbound on the true Q-function, it is a lower bound in expectation under the current policy, thereby\nstill providing appealing conservatism guarantees, while substantially reducing underestimation in\npractice. As shown by Kumar et al. (2020b), this variant produces the best performance in practice.\n4.6\nChallenges and Open Problems\nAs we discussed in Section 4.1, basic approximate dynamic programming algorithms can perform\nvery poorly in the ofﬂine setting due to distributional shift, primarily due to the distributional shift\nof the actions due to the discrepancy between the behavior policy πβ(a|s) and the current learned\npolicy π(a|s), which is used in the target value calculation for the Bellman backup. We discussed\nhow policy constraints and explicit uncertainty estimation can in principle mitigate this problem, but\nboth approaches have a number of shortcomings.\nWhile such explicit uncertainty-based methods are conceptually attractive, it is often very hard to\nobtain calibrated uncertainty estimates to evaluate PD( ˆQπ) and Unc in practice, especially with\nmodern high-capacity function approximators, such as deep neural networks. In practice, policy\nconstraint and conservative value function methods so far seem to outperform pure uncertainty-based\nmethods (Fujimoto et al., 2018). This might at ﬁrst appear surprising, since uncertainty estimation\nhas been a very widely used tool in another subﬁeld of reinforcement learning – exploration. In\nexploration, acting optimistically with respect to estimated uncertainty, or utilizing posterior sampling,\nhas been shown to be effective in practice (Osband and Van Roy, 2017). However, in the setting\nof ofﬂine reinforcement learning, where we instead must act conservatively with respect to the\nuncertainty set, the demands on the ﬁdelity of the uncertainty estimates are much higher. Exploration\n24\nalgorithms only require the uncertainty set to include good behavior – in addition, potentially, to a\nlot of bad behavior. However, ofﬂine reinforcement learning requires the uncertainty set to directly\ncapture the trustworthiness of the Q-function, which is a much higher bar. In practice, imperfect\nuncertainty sets can give rise to either overly conservative estimates, which impedes learning, or\noverly loose estimates, which results in exploitation of out-of-distribution actions. Of course, the\nrelatively performance of policy constraint and uncertainty-based methods may change in the future,\nas the community develops better epistemic uncertainty estimation techniques or better algorithms to\nincorporate more suitable distribution metrics into ofﬂine RL.\nPolicy constraint methods do however suffer from a number of challenges. First, most of these\nmethods ﬁt an estimated model of the behavior policy πβ(·|s) from the dataset and constrain the\nlearned policy π(·|s) against this estimated behavior policy. This means that the performance of these\nalgorithms is limited by the accuracy of estimation of the behavior policy, which may be hard in\nscenarios with highly multimodal behaviors, as is the case in practice with real-world problems. For\nexample, if a unimodal Gaussian policy is used to model a highly multi-modal action distribution,\naveraging across different modes while ﬁtting this behavior policy may actually be unable to prevent\nthe learned policy, π, from choosing out-of-distribution actions. Methods that enforce the constraint\nimplicitly, using only samples and without explicit behavior policy estimation, are a promising way\nto alleviate this limitation (Peng et al., 2019; Nair et al., 2020).\nEven when the behavior policy can be estimated exactly, a number of these methods still suffer\nfrom the undesirable effects of function approximation. When neural networks are used to represent\nQ-functions, undesirable effects of function approximation may prevent the Q-function from learning\nmeaningful values even when out-of-distribution actions are controlled for in the target values. For\nexample, when the size of the dataset is limited, approximate dynamic programming algorithms tend\nto overﬁt on the small dataset, and this error is then accumulated via Bellman backups (Fu et al., 2019;\nKumar et al., 2020a). Moreover, if the dataset state-action distribution is narrow, neural network\ntraining may only provide brittle, non-generalizable solutions. Unlike online reinforcement learning,\nwhere accidental overestimation errors arising due to function approximation can be corrected via\nactive data collection, these errors cumulatively build up and affect future iterates in an ofﬂine RL\nsetting.\nMethods that estimate a conservative or pessimistic value function (Kumar et al., 2020b) present a\nsomewhat different set of tradeoffs. While they avoid issues associated with estimating the behavior\npolicy and can effectively avoid overestimation, they can instead suffer from underestimation and a\nform of overﬁtting: if the dataset is small, the conservative regularizer can assign values that are too\nlow to actions that are undersampled in the dataset. Indeed, excessive pessimism may be one of the\nbigger issues preventing better performance on current benchmarks, and an important open question\nis how to dynamically modulate the degree of conservatism to balance the risks of overestimation\nagainst the downside of avoiding any unfamiliar action.\nA further issue that afﬂicts both constraint-based and conservative methods is that, while the Q-\nfunction is never evaluated on out-of-distribution states during training, the Q-function is still affected\nby the training set state distribution dπβ(s), and this is not accounted for in current ofﬂine learning\nmethods. For instance, when function approximation couples the Q-value at two distinct states with\nvery different densities under dπβ(s), dynamic programming with function approximation may give\nrise to incorrect solutions on the state that has a lower probability dπβ(s). A variant of this issue was\nnoted in the standard RL setting, referred to as an absence of “corrective feedback” by Kumar et al.\n(2020a) (see Kumar and Gupta (2020) for a short summary), and such a problem may affect ofﬂine\nRL algorithms with function approximation more severely, since they have no control over the data\ndistribution at all.\nAnother potential challenge for all of these ofﬂine approximate dynamic programming methods is that\nthe degree of improvement beyond the behavior policy is restricted by error accumulation. Since the\nerror in policy performance compounds with a factor that depends on 1/(1 −γ)2 ≈H2 (Farahmand\net al., 2010; Kumar et al., 2019; Kidambi et al., 2020), a small divergence from the behavior policy at\neach step can give rise to a policy that diverges away from the behavior distribution and performs very\npoorly. Besides impacting training, this issue can also lead to severe state distribution shift at test\ntime, which can lead the policy to perform very poorly. Therefore, policy constraints must be strong,\nso as to ensure that the overall error is small. This can restrict the amount of policy improvement that\ncan be achieved. We might expect that directly constraining the state-action marginal distribution of\n25\nthe policy, such as the methods explored in recent work (Nachum et al., 2019b) might not suffer from\nthe error accumulation issue, however, Kidambi et al. (2020) proved a lower-bound result showing\nthat quadratic scaling with respect to horizon is inevitable in the worst case for any ofﬂine RL method.\nMoreover, as previously discussed in Section 3.5, representing and enforcing constraints on the\nstate-action marginal distributions themselves requires Bellman backups, which can themselves suffer\nfrom out-of-distribution actions. Besides the worst-case dependence on the horizon, an open question\nthat still remains is is the development of constraints that can effectively trade off error accumulation\nand suboptimality of the learned policy in most “average”-case MDP instances, and can be easily\nenforced and optimized in practice via standard optimization techniques without requiring additional\nfunction approximators to ﬁt the behavior policy.\n5\nOfﬂine Model-Based Reinforcement Learning\nThe use of predictive models can be a powerful tool for enabling effective ofﬂine reinforcement\nlearning. Since model-based reinforcement learning algorithms primarily rely on their ability to\nestimate T(st+1|st, at) via a parameterized model Tψ(st+1|st, at), rather than performing dynamic\nprogramming or importance sampling, they beneﬁt from convenient and powerful supervised learning\nmethods when ﬁtting the model, allowing them to effectively utilize large and diverse datasets.\nHowever, like dynamic programming methods, model-based ofﬂine RL methods are not immune\nto the effects of distribution shift. In this section, we brieﬂy discuss how distributional shift affects\nmodel-based reinforcement learning methods, then survey a number of works that utilize models for\nofﬂine reinforcement learning, and conclude with a brief discussion of open challenges. A complete\ntreatment of all model-based reinforcement learning work, as well as work that learns predictive\nmodels of physics but does not utilize them for control (e.g., Lerer et al. (2016); Battaglia et al.\n(2016)), is outside the scope of this tutorial, and we focus primarily on work that performs both\nmodel-ﬁtting and control from ofﬂine data.\n5.1\nModel Exploitation and Distribution Shift\nAs discussed in Section 2.1, the model in a model-based RL algorithm can be utilized either for\nplanning (including online, via MPC) or for training a policy. In both cases, the model must provide\naccurate predictions for state-action tuples that are more optimal with respect to the current task. That\nis, the model will be queried at s ∼dπ(s), where π is either an explicit policy, or an implicit policy\nproduced by running planning under the model. In general dπ(s) ̸= dπβ(s), which means that the\nmodel is itself susceptible to distribution shift. In fact, the model suffers from distribution shift both\nin terms of the state distribution dπ(s), and the action distribution π(a|s).\nSince the policy (or action sequence, in the case of planning) is optimized to obtain the highest\npossible expected reward under the current model, this optimization process can lead to the policy\nexploiting the model, intentionally producing out-of-distribution states and actions at which the model\nTψ(st+1|st, at) erroneously predicts successor states st+1 that lead to higher returns than the actual\nsuccessor states that would be obtained in the real MDP. This model exploitation problem can lead\nto policies that produce substantially worse performance in the real MDP than what was predicted\nunder the model. Prior work in online model-based RL has sought to address this issue primarily\nvia uncertainty estimation, analogously to the uncertainty-based methods discussed in Section 4.4,\nbut this time modeling the epistemic uncertainty of the learned model Tψ(st+1|st, at). In low-\ndimensional MDPs, such uncertainty estimates can be produced by means of Bayesian models such\nas Gaussian processes (Deisenroth and Rasmussen, 2011), while for higher-dimensional problems,\nBayesian neural networks and bootstrap ensembles can be utilized (Chua et al., 2018; Janner et al.,\n2019). Effective uncertainty estimation is generally considered an important component of modern\nmodel-based reinforcement learning methods, for the purpose of mitigating model exploitation.\nTheoretical analysis of model-based policy learning can provide bounds on the error incurred from\nthe distributional shift due to the divergence between the learned policy π(a|s) and the behavior\npolicy πβ(a|s) (Sun et al., 2018b; Luo et al., 2018; Janner et al., 2019). This analysis resembles the\ndistributional shift analysis provided in the DAgger example in Section 2.4, except that now both the\npolicy and the transition probabilities experience distributional shift. Following Janner et al. (2019),\nif we assume that the total variation distance (TVD) between the learned model Tψ and true model T\nis bounded by ϵm = maxt Edπ\nt DT V (Tψ(st+1|st, at)∥T(st+1|st, at)), and the TVD between π and\n26\nπβ is likewise bounded on sampled states by ϵπ, then the true policy value J(π) is related to the\npolicy estimate under the model, Jψ(π), according to\nJ(π) ≥Jψ(π) −\n\u00142γrmax(ϵm + 2ϵπ)\n(1 −γ)2\n+ 4rmaxϵπ\n1 −γ\n\u0015\n.\nIntuitively, the second term represents error accumulation due to the distribution shift in the policy,\nwhile the ﬁrst term represents error accumulation due to the distribution shift in the model. The\nﬁrst term also includes a dependence on ϵπ, because policies that diverge more from πβ will lead to\nstates that are further outside of the data distribution, which in turn will make errors in the model\nmore likely. Janner et al. (2019) also argue that a modiﬁed model-based RL procedure that resembles\nDyna Sutton (1991), where only short-horizon rollouts from the model are generated by “branching”\noff of states seen in the data, can mitigate this accumulation of error. This is also intuitively natural:\nif applying the learned model recursively leads to compounding error, then shorter rollouts should\nincur substantially less error.\n5.2\nBrief Survey of Model-Based Ofﬂine Reinforcement Learning\nA natural and straightforward way to utilize model-based reinforcement learning algorithms in\nthe ofﬂine setting is to simply train the model from ofﬂine data, with minimal modiﬁcation to\nthe algorithm. As with Q-learning and actor-critic methods, model-based reinforcement learning\nalgorithms can be applied to the ofﬂine setting naïvely. Furthermore, many effective model-based\nreinforcement learning methods already take steps to limit model exploitation via a variety of\nuncertainty estimation methods (Deisenroth and Rasmussen, 2011; Chua et al., 2018), making them\nreasonably effective in the ofﬂine setting. Many such methods have been known to exhibit excellent\nperformance in conventional off-policy settings, where additional data collection is allowed, but\nprior data is also utilized (Sutton, 1991; Watter et al., 2015; Zhang et al., 2018; Hafner et al., 2018;\nJanner et al., 2019). Indeed, Yu et al. (2020) showed that MBPO (Janner et al., 2019) actually attains\nreasonable performance on standard ofﬂine RL settings without modiﬁcation, whereas naïve dynamic\nprogramming methods (e.g., soft actor-critic (Haarnoja et al., 2018)) fail to learn meaningful policies\nwithout policy constraints. This suggests that model-based RL algorithms are likely to lead to an\neffective class of ofﬂine reinforcement learning methods.\nOfﬂine RL with standard model-based RL methods.\nA number of recent works have also demon-\nstrated effective ofﬂine learning of predictive models and their application to control in complex and\nhigh-dimensional domains, including settings with image observations. Some of these methods have\ndirectly utilized high-capacity predictive models on high-dimensional observations, such as images,\ndirectly employing for online trajectory optimization (i.e., MPC). Action-conditional convolutional\nneural networks (Oh et al., 2015) have been used to provide accurate, long-term prediction of behavior\nin Atari games and have been combined with RL to improve sample-efﬁciency over model-free\nmethods (Kaiser et al., 2019b). The visual foresight method involves training a video prediction\nmodel to predict future image observations for a robot, conditioned on the current image and future\nsequence of actions. The model is represented with a recurrent neural network, and trained on\nlarge amounts of ofﬂine data, typically collected with an uninformative randomized policy (Finn\nand Levine, 2017; Ebert et al., 2018). More recent work has demonstrated this approach on very\nlarge and diverse datasets, collected from multiple viewpoints, many objects, and multiple robots,\nsuggesting a high degree of generalization, though the particular behaviors are comparatively simple,\ntypically relocating individual objects by pushing or grasping Dasari et al. (2019). A number of\nprior works have also employed “hybrid” methods that combine elements of model-free and model-\nbased algorithms, predicting future rewards or reward features conditioned on a sequence of future\nactions, but avoiding direct prediction of future observations. A number of such methods have been\nexplored in the conventional online RL setting (Tamar et al., 2016; Dosovitskiy and Koltun, 2016; Oh\net al., 2017), and in the ofﬂine RL setting, such techniques have been applied effectively to learning\nnavigational policies for mobile robots from previously collected data (Kahn et al., 2018, 2020).\nOff-policy evaluation with models.\nModel learning has also been explored considerably in the\ndomain of off-policy evaluation (OPE), as a way to reduce the variance of importance sampled\nestimators of the sort discussed in Section 3.1. Here, the model is used to provide a sort of baseline\nfor the expected return inside of an importance sampled estimator, as illustrated in Equation (6) in\n27\nSection 3.1. In these settings, the model is typically trained from ofﬂine data (Jiang and Li, 2015;\nThomas and Brunskill, 2016; Farajtabar et al., 2018; Wang et al., 2017).\nDistribution and safe region constraints.\nAs with the policy constraint methods in Section 4.3,\nmodel-based RL algorithms can also utilize constraints imposed on the policy or planner with the\nlearned model. Methods that guarantee Lyapunov stability (Berkenkamp et al., 2017) of the learned\nmodel can be used to constrain policies within “safe” regions of the state space where the chance\nof failure is small. Another example of such an approach is provided by deep imitative models\n(DIMs) (Rhinehart et al., 2018), which learn a normalizing ﬂow model over future trajectories from\nofﬂine data, conditioned on a high-dimensional observation. Like the hybrid methods described\nabove, DIMs avoid direct prediction of observations. The learned distribution over dataset trajectories\ncan then be used to both provide predictions for a planner, and provide a distributional constraint,\npreventing the planner from planning trajectories that deviate signiﬁcantly from the training data,\nthus limiting distributional shift.\nConservative model-based RL algorithms.\nMore recently, two concurrent methods, MoREL (Ki-\ndambi et al., 2020) and MOPO (Yu et al., 2020), have proposed ofﬂine model-based RL algorithms\nthat aim to utilize conservative value estimates to provide analytic bounds on performance. Concep-\ntually, these methods resemble the conservative estimation approach described in Section 4.5, but\ninstead of regularizing a value function, they modify the MDP model learned from data to induce\nconservative behavior. The basic principle is to provide the policy with a penalty for visiting states\nunder the model where the model is likely to be incorrect. If the learned policy takes actions that re-\nmain in regions where the model is accurate, then the model-based estimate of the policy’s value (and\nits gradient) will likely be accurate also. Let u(s, a) denote an error oracle that provides a consistent\nestimate of the accuracy of the model at state-action tuple (s, a). For example, as proposed by Yu et al.\n(2020), this oracle might satisfy the property that D(Tψ(st+1|st, at)∥T(st+1|st, at)) ≤u(s, a) for\nsome divergence measure D(·, ·). Then, conservative behavior can be induced either by modifying\nthe reward function to obtain a conservative reward of the form ˜r(s, a) = r(s, a) −λu(s, a), as\nin MOPO (Yu et al., 2020), or by modifying the MDP under the learned model so that the agent\nenters an absorbing state with a low reward value when u(s, a) is below some threshold, as in\nMoREL (Kidambi et al., 2020). In both cases, it is possible to show that the model-based estimate of\nthe policy’s performance under the model the modiﬁed reward function or MDP structure bounds that\npolicy’s true performance in the real MDP, providing appealing theoretical guarantees on performance.\nNote, however, that such approaches still require a consistent estimator for the error oracle u(s, a).\nPrior work has used disagreement in a bootstrap ensemble to provide this estimate, but it is not\nguaranteed to be consistent under sampling error, and resolving this limitation is likely to be an\nimportant direction for future work.\n5.3\nChallenges and Open Problems\nAlthough model-based reinforcement learning appears to be a natural ﬁt for the ofﬂine RL problem\nsetting, current methods for fully ofﬂine model-based reinforcement learning generally rely on explicit\nuncertainty estimation for the model to detect and quantify distributional shift, for example by using a\nbootstrap ensemble. Although uncertainty estimation for models is in some ways more straightforward\nthan uncertainty estimation for value functions, current uncertainty estimation methods still leave\nmuch to be desired, and it seems likely that ofﬂine performance of model-based RL methods can be\nimproved substantially by carefully accounting for distributional shift.\nModel-based RL methods also present their own set of challenges: while some MDPs are easy to\nmodel accurately, others can be exceedingly difﬁcult. Modeling MDPs with very high-dimensional\nimage observations and long horizons is a major open problem, and current predictive modeling\nmethods generally struggle with long-horizon prediction. Hybrid methods that combine model-based\nand model-free learning, for example by utilizing short rollouts (Sutton, 1991; Janner et al., 2019) or\navoiding prediction of full observations (Dosovitskiy and Koltun, 2016; Oh et al., 2017; Kahn et al.,\n2020) offer some promise in this area.\nIt is also still an open theoretical question as to whether model-based RL methods even in theory can\nimprove over model-free dynamic programming algorithms. The reasoning behind this question is\nthat, although dynamic programming methods do not explicitly learn a model, they essentially utilize\nthe dataset as a “non-parametric” model. Fundamentally, both dynamic programming methods and\n28\nmodel-based RL methods are solving prediction problems, with the former predicting future returns,\nand the latter predicting future states. Moreover, model-free methods can be modiﬁed to predict even\nmore general quantities (Sutton et al., 2011), such as return values with different discount factors,\nreturn values for different number of steps into the future, etc. In the linear function approximation\ncase, it is known that model-based updates and ﬁtted value iteration updates actually produce identical\niterates (Vanseijen and Sutton, 2015; Parr et al., 2008), though it is unknown whether this relationship\nholds under non-linear function approximation. Therefore, exploring the theoretical bounds on the\noptimal performance of ofﬂine model-based RL methods under non-linear function approximation,\nas compared to ofﬂine dynamic programming methods, remains an open problem.\n6\nApplications and Evaluation\nIn this section, we survey and discuss evaluation methods, benchmarks, and applications for ofﬂine\nreinforcement learning methods. As discussed in Section 1, and as we will discuss further in\nSection 7, it is very likely that the full potential of ofﬂine reinforcement learning methods has yet\nto be fully realized, and perhaps the most exciting applications of such methods are still ahead of\nus. Nonetheless, a number of prior works have applied ofﬂine reinforcement learning in a range of\nchallenging domains, from safety-critical real-world physical systems to large-scale learning from\nlogged data for recommender systems. We ﬁrst discuss how ofﬂine reinforcement learning algorithms\nhave been evaluated in prior work, and then discuss speciﬁc application domains where such methods\nhave already made an impact.\n6.1\nEvaluation and Benchmarks\nWhile individual application domains, such as recommender systems and healthcare, discussed below,\nhave developed particular domain-speciﬁc evaluations, the general state of benchmarking for modern\nofﬂine reinforcement learning research is less well established. In the absence of well-developed\nevaluation protocols, one approach employed in recent work is to utilize training data collected via a\nstandard online reinforcement learning algorithm, using either the entire replay buffer for an off-policy\nalgorithm for training (Kumar et al., 2019; Agarwal et al., 2019; Fujimoto et al., 2018), or even data\nfrom the optimal policy. However, this evaluation setting is rather unrealistic, since the entire point of\nutilizing ofﬂine reinforcement learning algorithms in the real world is to obtain a policy that is better\nthan the best behavior in the dataset, potentially in settings where running reinforcement learning\nonline is impractical due to cost or safety concerns. A simple compromise solution is to utilize data\nfrom a “suboptimal” online reinforcement learning run, for example by stopping the online process\nearly, saving out the buffer, and using this buffer as the dataset for ofﬂine RL (Kumar et al., 2019).\nHowever, even this formulation does not fully evaluate capabilities of ofﬂine reinforcement learning\nmethods, and the statistics of the training data have a considerable effect on the difﬁcult of ofﬂine\nRL (Fu et al., 2020), including how concentrated the data distribution is around a speciﬁc set of\ntrajectories, and how multi-modal the data is. Broader data distributions (i.e., ones where πβ(a|s) has\nhigher entropy) are generally easier to learn from, since there are fewer out-of-distribution actions.\nOn the other hand, highly multi-modal behavior policies can be extremely difﬁcult to learn from for\nmethods that require explicit estimation of πβ(a|s), as discussed in Section 4.3 and 4.6. Our recently\nproposed set of ofﬂine reinforcement learning benchmarks aims to provide standardized datasets and\nsimulations that cover such difﬁcult cases (Fu et al., 2020).\nFigure 4: An example of exploiting compo-\nsitional structure in trajectories to ﬁnd short-\nest paths in the Maze2D environment in the\nD4RL benchmark suite (Fu et al., 2020).\nA reasonable question we might ask in regard to datasets\nfor ofﬂine RL is: in which situations might we actually\nexpect ofﬂine RL to yield a policy that is signiﬁcantly better\nthan any trajectory in the training set? While we cannot\nexpect ofﬂine RL to discover actions that are better than any\naction illustrated in the data, we can expect it to effectively\nutilize the compositional structure inherent in any temporal\nprocess. This idea is illustrated in Figure 4: if the dataset\ncontains a subsequence illustrating a way to arrive at state 2\nfrom state 1, as well as a separate subsequence illustrating\nhow to arrive at state 3 from state 2, then an effective ofﬂine\nRL method should be able to learn how to arrive at state 3\nfrom state 1, which might provide for a substantially higher\n29\nﬁnal reward than any of the subsequences in the dataset. When we also consider the capacity of\nneural networks to generalize, we could imagine this sort of “transitive induction” taking place\non a portion of the state variables, effectively inferring potentially optimal behavior from highly\nsuboptimal components. This capability can be evaluated with benchmarks that explicitly provide\ndata containing this structure, and the D4RL benchmark suite provides a range of tasks that exercise\nthis capability (Fu et al., 2020).\nAccurately evaluating the performance of ofﬂine RL algorithms can be difﬁcult, because we are\ntypically interested in maximizing the online performance of an algorithm. When simulators are\navailable, online evaluations can be cheaply performed within the simulator order to benchmark the\nperformance of algorithms. Off-policy evaluation (OPE) can also be used to estimate the performance\nof policies without explicit online interaction, but it is an active area of research as discussed in\nSection 3.1. Nevertheless, OPE is a popular tool in areas such as online advertising (Li et al.,\n2010) or healthcare (Murphy et al., 2001) where online evaluation can have signiﬁcant ﬁnancial or\nsafety consequences. In certain domains, human experts can be used to assess the quality of the\ndecision-making system. For example, Jaques et al. (2019) uses crowd-sourced human labeling to\njudge whether dialogue generated by an ofﬂine RL agent is ﬂuent and amicable, and Raghu et al.\n(2017) evaluates using a qualitative analysis based one domain experts for sepsis treatment.\n6.2\nApplications in Robotics\nRobotics is an appealing application domain for ofﬂine reinforcement learning, since RL has the\npotential to automate the acquisition of complex behavioral skills for robots – particularly with\nraw sensory observations, such as camera images – but conducting online data collection for each\nrobotic control policy can be expensive and impractical. This is especially true for robots that\nmust learn to act intelligently in complex open-world environments, since the challenge of robust\nvisual perception alone already necessitates large training sets. The ImageNet Large-Scale Visual\nRecognition Challenge (Russakovsky et al., 2015) stipulates a training set of 1.5 million images for\nopen-world object recognition, and it seems reasonable that the sample complexity for a robotic RL\nalgorithm that must act in similar real-world settings should be at least of comparable size. For this\nreason, utilizing previous collected data can be of critical importance in robotics.\nSeveral prior works have explored ofﬂine RL methods for learning robotic grasping, which is a partic-\nularly appealing task ofﬂine RL methods, since it requires the ability to generalize to a wide range of\nFigure 5: Large-scale robotic grasping data collection.\nKalashnikov et al. (2018) describes how a dataset of over\n500,000 grasp trials collected from multiple robots was\nused to train a vision-based grasping policy, comparing\nfully ofﬂine training and online ﬁne-tuning.\nobjects (Pinto and Gupta, 2016; Levine et al.,\n2018; Kalashnikov et al., 2018; Zeng et al.,\n2018).\nSuch methods have utilized approxi-\nmate dynamic programming (Kalashnikov et al.,\n2018) (see Figure 5), as well as more domain-\nspeciﬁc algorithms, such as a single-step bandit\nformulation (Pinto and Gupta, 2016). Outside\nof robotic grasping, Ebert et al. (2018) propose\na model-based algorithm based on prediction\nof future video frames for learning a variety\nof robotic manipulation skills from ofﬂine data,\nwhile Dasari et al. (2019) expand on this ap-\nproach with a large and diverse dataset of robotic\ninteraction. Cabi et al. (2019) propose to use\nreward learning from human preferences com-\nbined with ofﬂine RL to provide a user-friendly\nmethod for controlling robots for object manip-\nulation tasks. In the domain of robotic navigation, Mo et al. (2018) propose a dataset of visual\nindoor scenes for reinforcement learning, collected via a camera-mounted-robot, and Mandlekar\net al. (2020) proposes a dataset of human demonstrations for robotic manipulation. Kahn et al.\n(2020) discuss how a method based on prediction of future reward signals, blending elements of\nmodel-based and model-free learning, can learn effective navigation policies from data collected in\nthe real world using a random exploration policy. Pure model-based methods in robotics typically\ninvolve training a model on real or simulated data, and then planning within the model to produce a\npolicy that is executed on a real system. Approaches have included using Gaussian process models\n30\nfor controlling blimps (Ko et al., 2007), and using linear regression (Koppejan and Whiteson, 2009)\nand locally-weighted Bayesian regression (Bagnell and Schneider, 2001) for helicopter control.\n6.3\nApplications in Healthcare\nUsing ofﬂine reinforcement learning in healthcare poses several unique challenges (Gottesman et al.,\n2018). Safety is a major concern, and largely precludes any possibility of online exploration. Datasets\ncan also be signiﬁcantly biased towards serious outcomes (Gottesman et al., 2019), since minor cases\nrarely require treatment, and can lead naïve agents to erroneous conclusions, for example that any\ndrug treatment may cause death simply because it is not prescribed to otherwise healthy individuals.\nThe MIMIC-III dataset (Johnson et al., 2016), which contains approximately 60K medical records\nfrom ICUs, has been inﬂuential in enabling data-driven research in healthcare treatment. Q-learning\nmethods on this dataset has been applied to problems such as the treatment of sepsis (Raghu et al.,\n2017) and optimizing the use of ventilators (Prasad et al., 2017). Wang et al. (2018) apply actor-critic\nmethods on MIMIC-III to determine drug recommendations.\nFigure 6: A real-time epilepsy treatment sys-\ntem, train using ofﬂine reinforcement learn-\ning (Guez et al., 2008).\nOutside of ICU settings, ofﬂine RL applications include\nlearning from recordings of seizure activity in mouse\nbrains in order to determine optimal stimulation frequen-\ncies for reducing epileptic seizures (Guez et al., 2008).\nOfﬂine RL has also been used for optimizing long term\ntreatment plans. Shortreed et al. (2011) uses ofﬂine ﬁtted\nQ-iteration for optimizing schizophrenia treatment, Nie\net al. (2019) uses doubly-robust estimators to safely de-\ntermine proper timings of medical treatments, and Tseng\net al. (2017) uses a model-based approach for lung cancer\ntreatment. Careful application of ofﬂine RL that can han-\ndle such challenges may offer healthcare providers powerful assistive tools for optimizing the care of\npatients and ultimately improving outcomes.\n6.4\nApplications in Autonomous Driving\nAs in healthcare, a signiﬁcant barrier to applying reinforcement learning in the domain of self-driving\nvehicles is safety. In the online setting, exploratory agents can select actions that lead to catastrophic\nfailure, potentially endangering the lives of the passengers. Thus, ofﬂine RL is potentially a promising\ntool for enabling, safe, effective learning in autonomous driving.\nFigure\n7:\nA\nroad\nfollowing\nsystem\ntrained end-to-end via reinforcement learn-\ning (Kendall et al., 2019).\nWhile ofﬂine RL has not yet found signiﬁcant application\nin actual real-world self-driving vehicles (Yurtsever et al.,\n2020), learning-based approaches have been gaining in\npopularity. RobotCar (Maddern et al., 2017) and BDD-\n100K (Yu et al., 2018) are both large video datasets contain-\ning thousands of hours of real-life driving activity. Imita-\ntion learning has been a popular approach towards end-to-\nend, data-driven methods in autonomous driving (Bojarski\net al., 2016; Sun et al., 2018a; Pan et al., 2017; Codevilla\net al., 2018). Reinforcement learning approaches have\nbeen applied in both simulation (Sallab et al., 2017) and in\nthe real world, with human interventions in case the vehi-\ncle violates a safety constraint (Kendall et al., 2019) (see\nFigure 7). Model-based reinforcement learning methods\nthat employ constraints to keep the agent close to the training data for the model, so as to avoid\nout-of-distribution inputs as discussed in Section 5, can effectively provide elements of imitation\nlearning when training on driving demonstration data, as for example in the case of deep imitative\nmodels (DIMs) (Rhinehart et al., 2018). Indeed, with the widespread availability of high-quality\ndemonstration data, it is likely that effective methods for ofﬂine RL in the ﬁeld of autonomous driving\nwill, explicitly or implicitly, combine elements of imitation learning and reinforcement learning.\n31\n6.5\nApplications in Advertising and Recommender Systems\nRecommender systems and advertising are particularly suitable domains for ofﬂine RL because\ndata collection is easy and efﬁcient, and can be obtained by logging user behavior. However, these\ndomains are also “safety critical,” in the sense that a highly suboptimal policy may result in large\nmonetary losses, thereby making unconstrained online exploration problematic. Thus, ofﬂine RL\napproaches have a long history of application in this area.\nOff-policy evaluation is commonly used as a tool for performing A/B tests and estimating the perfor-\nmance of advertising and recommender systems without additional interaction with the environment.\nIn contrast to the other applications discussed, policy evaluation for recommender systems is typically\nformulated within a contextual bandit problem (Langford et al., 2008; Li et al., 2010), where states\nmay correspond to user history and actions correspond to recommendations. This approximation\nremoves the need for sequential decision making, but can introduce approximation errors if actions\nhave temporal dependence as in domains such as robotics or healthcare.\nApplications of ofﬂine RL for recommender systems include slate and whole-page optimiza-\ntion (Swaminathan et al., 2017), applying doubly robust estimation to estimate website visits Dudík\net al. (2014), and A/B testing for click optimization (Gilotte et al., 2018). Policy learning from\nlogged, ofﬂine data has included studies on optimizing newspaper article click-through-rates (Strehl\net al., 2010; Garcin et al., 2014), advertisement ranking on search pages (Bottou et al., 2013), and\npersonalized ad recommendation for digital marketing (Theocharous et al., 2015; Thomas et al.,\n2017).\n6.6\nApplications in Language and Dialogue\nInteraction via natural language is not typically thought of as a reinforcement learning problem,\nbut in fact the formalism of sequential decision making can provide a powerful tool for natural\nlanguage interaction: when dialogue is modeled as a purposeful interaction, the RL framework\ncan in principle offer an effective mechanism for learning policies for outputting natural language\nresponses to human interlocutors. The most direct way to utilize standard online RL methods for\nnatural language interaction – by having machines engage in dialogue with real humans – can be\nexceedingly tedious, especially in the early stages of training, when the policy would produce mostly\nnon-sensical dialogue. For this reason, ofﬂine RL offers a natural avenue to combine the optimal\ndecision making formalism of RL with the kinds of large datasets of human-to-human conversations\navailable in NLP.\nIn prior work, ofﬂine RL approaches have been applied in the areas of dialogue and language\ninterfaces, where datasets consist of logged interactions, such as agent-customer transcripts (Zhou\net al., 2017). An example of an application is dialogue management, which is typically concerned\nFigure 8: A dialogue agent trained via ofﬂine reinforcement learning inter-\nacting with a human user, with the aim of elicit responses with positive senti-\nment (Jaques et al., 2019).\nwith accomplishing a spe-\nciﬁc goal, such as retriev-\ning information. Examples\nof this have included ap-\nplications of ofﬂine RL to\nthe problem of ﬂight book-\ning (Henderson et al., 2008),\nrestaurant information re-\ntrieval Pietquin et al. (2011),\nand restaurant recommen-\ndation (Kandasamy et al.,\n2017). Jaques et al. (2019)\napplies ofﬂine RL to the\nproblem of dialogue generation, and focuses on producing natural responses that elicit positive\nfeedback from human users. An example of an interaction with a trained agent is shown in Figure 8.\n7\nDiscussion and Perspectives\nOfﬂine reinforcement learning offers the possibility of turning reinforcement learning – which is\nconventionally viewed as a fundamentally active learning paradigm – into a data-driven discipline,\n32\nsuch that it can beneﬁt from the same kind of “blessing of scale” that has proven so effective across a\nrange of supervised learning application areas (LeCun et al., 2015). However, making this possible\nwill require new innovations that bring to bear sophisticated statistical methods and combine them\nwith the fundamentals of sequential decision making that are conventionally studied in reinforcement\nlearning. Standard off-policy reinforcement learning algorithms have conventionally focused on\ndynamic programming methods that can utilize off-policy data, as discussed in Section 2.1 and\nSection 4, and importance sampling methods that can incorporate samples from different sampling\ndistributions, as discussed in Section 3.2.\nHowever, both of these classes of approaches struggle when scaled up to complex high-dimensional\nfunction approximators, such as deep neural networks, high-dimensional state or observation spaces,\nand temporally extended tasks. As a result, the standard off-policy training methods in these two\ncategories have generally proven unsuitable for the kinds of complex domains typically studied\nin modern deep reinforcement learning. More recently, a number of improvements for ofﬂine RL\nmethods have been proposed that take into account the statistics of distributional shift, via either\npolicy constraints or uncertainty estimation, such as the policy constraint formulations that we discuss\nin Section 4.3. These formulations have the potential to mitigate the shortcomings of early methods,\nby explicitly account for the key challenge in ofﬂine RL: distributional shift due to differences\nbetween the learned policy and the behavior policy.\nMore generally, such methods shed light on the fact that ofﬂine RL is, at its core, a counter-factual\ninference problem: given data that resulted from a given set of decisions, infer the consequence of\na different set of decisions. Such problems are known to be exceptionally challenging in machine\nlearning, because they require us to step outside of the commonly used i.i.d. framework, which\nassumes that test-time queries involve the same distribution as the one that produced the training\ndata (Schölkopf, 2019). It therefore stands to reason that the initial solutions to this problem,\nproposed in recent work, should aim to reduce distributional shift, either by constraining the policy’s\ndeviation from the data, or by estimating (epistemic) uncertainty as a measure of distributional shift.\nMoving forward, we might expect that a variety of tools developed for addressing distributional\nshift and facilitating generalization may ﬁnd use in ofﬂine RL algorithms, including techniques from\ncausal inference (Schölkopf, 2019), uncertainty estimation (Gal and Ghahramani, 2016; Kendall\nand Gal, 2017), density estimation and generative modeling (Kingma et al., 2014), distributional\nrobustness (Sinha et al., 2017; Sagawa et al., 2019) and invariance (Arjovsky et al., 2019). More\nbroadly, methods that aim to estimate and address distributional shift, constrain distributions (e.g.,\nvarious forms of trust regions), and evaluate distribution support from samples are all potentially\nrelevant to developing improved methods for ofﬂine reinforcement learning.\nThe counter-factual inference perspective becomes especially clear when we consider model-based\nofﬂine RL algorithms, as discussed brieﬂy in Section 5. In this case, the model answers the question:\n“what would the resulting state be if the agent took an action other than the one in the dataset?” Of\ncourse, the model suffers from distributional shift in much the same way as the value function, since\nout-of-distribution state-action tuples can result in inaccurate prediction. Nonetheless, prior work\nhas demonstrated good results with model-based methods, particularly in regard to generalization\nwith real-world data (Finn and Levine, 2017; Ebert et al., 2018), and a range of works on predicting\nphysical phenomena have utilized ofﬂine datasets (Lerer et al., 2016; Battaglia et al., 2016). This\nsuggests that model learning may be an important component of effective future ofﬂine reinforcement\nlearning methods.\nTo conclude our discussion of ofﬂine reinforcement learning, we will leave the reader with the\nfollowing thought. While the machine learning community frequently places considerable value on\ndesign of novel algorithms and theory, much of the amazing practical progress that we have witnessed\nover the past decade has arguably been driven just as much by advances in datasets as by advances in\nmethods. Indeed, widely deployed techniques in computer vision and NLP utilize learning methods\nthat are relatively old and well understood, and although improvements in architectures and models\nhave driven rapid increase in performance, the increasing size and diversity of datasets – particularly\nin real-world applications – have been an instrumental driver of progress. In real-world applications,\ncollecting large, diverse, representative, and well-labeled datasets is often far more important than\nutilizing the most advanced methods. In the standard active setting in which most reinforcement\nlearning methods operate, collecting large and diverse datasets is often impractical, and in many\napplications, including safety-critical domains such as driving, and human-interactive domains such\nas dialogue systems, it is prohibitively costly in terms of time, money, and safety considerations.\n33\nTherefore, developing a new generation of data-driven reinforcement learning may usher in a new\nera of progress in reinforcement learning, both by making it possible to bring it to bear on a range of\nreal-world problems that have previously been unsuited for such methods, and by enabling current\napplications (e.g., in robotics or autonomous driving) to beneﬁt from much larger, more diverse, and\nmore representative datasets that can be reused effectively across experiments.\nReferences\nAgarwal, R., Schuurmans, D., and Norouzi, M. (2019). An optimistic perspective on ofﬂine rein-\nforcement learning. arXiv preprint arXiv:1907.04543.\nArjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2019). Invariant risk minimization. arXiv\npreprint arXiv:1907.02893.\nBagnell, J. A. and Schneider, J. G. (2001). Autonomous helicopter control using reinforcement\nlearning policy search methods. In Proceedings 2001 ICRA. IEEE International Conference on\nRobotics and Automation (Cat. No. 01CH37164), volume 2, pages 1615–1620. IEEE.\nBattaglia, P., Pascanu, R., Lai, M., Rezende, D. J., et al. (2016). Interaction networks for learning\nabout objects, relations and physics. In Advances in neural information processing systems, pages\n4502–4510.\nBerkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A. (2017). Safe model-based reinforcement\nlearning with stability guarantees. In Advances in neural information processing systems, pages\n908–918.\nBojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort,\nM., Muller, U., Zhang, J., et al. (2016). End to end learning for self-driving cars. arXiv preprint\narXiv:1604.07316.\nBottou, L., Peters, J., Quiñonero-Candela, J., Charles, D. X., Chickering, D. M., Portugaly, E., Ray, D.,\nSimard, P., and Snelson, E. (2013). Counterfactual reasoning and learning systems: The example\nof computational advertising. The Journal of Machine Learning Research, 14(1):3207–3260.\nCabi, S., Colmenarejo, S. G., Novikov, A., Konyushkova, K., Reed, S., Jeong, R., ˙Zołna, K., Aytar,\nY., Budden, D., Vecerik, M., et al. (2019). A framework for data-driven robotics. arXiv preprint\narXiv:1909.12200.\nChebotar, Y., Handa, A., Makoviychuk, V., Macklin, M., Issac, J., Ratliff, N., and Fox, D. (2019).\nClosing the sim-to-real loop: Adapting simulation randomization with real world experience. In\n2019 International Conference on Robotics and Automation (ICRA), pages 8973–8979. IEEE.\nChen, Y. and Wang, M. (2016). Stochastic primal-dual methods and sample complexity of reinforce-\nment learning. arXiv preprint arXiv:1612.02516.\nCheng, C.-A., Yan, X., and Boots, B. (2019). Trajectory-wise control variates for variance reduction\nin policy gradient methods. arXiv preprint arXiv:1908.03263.\nChua, K., Calandra, R., McAllister, R., and Levine, S. (2018). Deep reinforcement learning in\na handful of trials using probabilistic dynamics models. In Advances in Neural Information\nProcessing Systems, pages 4754–4765.\nCodevilla, F., Miiller, M., López, A., Koltun, V., and Dosovitskiy, A. (2018). End-to-end driving via\nconditional imitation learning. In 2018 IEEE International Conference on Robotics and Automation\n(ICRA), pages 1–9. IEEE.\nDai, B., He, N., Pan, Y., Boots, B., and Song, L. (2016). Learning from conditional distributions via\ndual embeddings. arXiv preprint arXiv:1607.04579.\nDai, B., Shaw, A., He, N., Li, L., and Song, L. (2017a). Boosting the actor with dual critic. arXiv\npreprint arXiv:1712.10282.\nDai, B., Shaw, A., Li, L., Xiao, L., He, N., Liu, Z., Chen, J., and Song, L. (2017b). Sbeed: Convergent\nreinforcement learning with nonlinear function approximation. arXiv preprint arXiv:1712.10285.\n34\nDasari, S., Ebert, F., Tian, S., Nair, S., Bucher, B., Schmeckpeper, K., Singh, S., Levine, S., and Finn,\nC. (2019). Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215.\nDegris, T., White, M., and Sutton, R. S. (2012).\nOff-policy actor-critic.\narXiv preprint\narXiv:1205.4839.\nDeisenroth, M. and Rasmussen, C. E. (2011). Pilco: A model-based and data-efﬁcient approach to\npolicy search. In Proceedings of the 28th International Conference on machine learning (ICML-11),\npages 465–472.\nDosovitskiy, A. and Koltun, V. (2016). Learning to act by predicting the future. arXiv preprint\narXiv:1611.01779.\nDudík, M., Erhan, D., Langford, J., Li, L., et al. (2014). Doubly robust policy evaluation and\noptimization. Statistical Science, 29(4):485–511.\nEbert, F., Finn, C., Dasari, S., Xie, A., Lee, A., and Levine, S. (2018). Visual foresight: Model-based\ndeep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568.\nErnst, D., Geurts, P., and Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.\nJournal of Machine Learning Research, 6(Apr):503–556.\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley,\nT., Dunning, I., et al. (2018). Impala: Scalable distributed deep-rl with importance weighted\nactor-learner architectures. arXiv preprint arXiv:1802.01561.\nEysenbach, B., Gu, S., Ibarz, J., and Levine, S. (2017). Leave no trace: Learning to reset for safe and\nautonomous reinforcement learning. arXiv preprint arXiv:1711.06782.\nFarahmand, A.-m., Szepesvári, C., and Munos, R. (2010). Error propagation for approximate policy\nand value iteration. In Advances in Neural Information Processing Systems, pages 568–576.\nFarajtabar, M., Chow, Y., and Ghavamzadeh, M. (2018). More robust doubly robust off-policy\nevaluation. arXiv preprint arXiv:1802.03493.\nFinn, C. and Levine, S. (2017). Deep visual foresight for planning robot motion. In 2017 IEEE\nInternational Conference on Robotics and Automation (ICRA), pages 2786–2793. IEEE.\nFu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2020). D4rl: Datasets for deep data-driven\nreinforcement learning. In arXiv.\nFu, J., Kumar, A., Soh, M., and Levine, S. (2019). Diagnosing bottlenecks in deep Q-learning\nalgorithms. arXiv preprint arXiv:1902.10250.\nFujimoto, S., Meger, D., and Precup, D. (2018). Off-policy deep reinforcement learning without\nexploration. arXiv preprint arXiv:1812.02900.\nGal, Y. and Ghahramani, Z. (2016). Dropout as a bayesian approximation: Representing model\nuncertainty in deep learning. In international conference on machine learning, pages 1050–1059.\nGarcin, F., Faltings, B., Donatsch, O., Alazzawi, A., Bruttin, C., and Huber, A. (2014). Ofﬂine and\nonline evaluation of news recommender systems at swissinfo. ch. In Proceedings of the 8th ACM\nConference on Recommender systems, pages 169–176.\nGelada, C. and Bellemare, M. G. (2019). Off-policy deep reinforcement learning by bootstrapping\nthe covariate shift. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33,\npages 3647–3655.\nGhavamzadeh, M., Petrik, M., and Chow, Y. (2016). Safe policy improvement by minimizing robust\nbaseline regret. In Advances in Neural Information Processing Systems, pages 2298–2306.\nGilotte, A., Calauzènes, C., Nedelec, T., Abraham, A., and Dollé, S. (2018). Ofﬂine a/b testing for\nrecommender systems. In Proceedings of the Eleventh ACM International Conference on Web\nSearch and Data Mining, pages 198–206.\n35\nGottesman, O., Johansson, F., Komorowski, M., Faisal, A., Sontag, D., Doshi-Velez, F., and Celi,\nL. A. (2019). Guidelines for reinforcement learning in healthcare. Nat Med, 25(1):16–18.\nGottesman, O., Johansson, F., Meier, J., Dent, J., Lee, D., Srinivasan, S., Zhang, L., Ding, Y., Wihl,\nD., Peng, X., et al. (2018). Evaluating reinforcement learning algorithms in observational health\nsettings. arXiv preprint arXiv:1805.12298.\nGu, S., Holly, E., Lillicrap, T., and Levine, S. (2017a). Deep reinforcement learning for robotic\nmanipulation with asynchronous off-policy updates. In 2017 IEEE international conference on\nrobotics and automation (ICRA), pages 3389–3396. IEEE.\nGu, S. S., Lillicrap, T., Turner, R. E., Ghahramani, Z., Schölkopf, B., and Levine, S. (2017b).\nInterpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep\nreinforcement learning. In Advances in neural information processing systems, pages 3846–3855.\nGuez, A., Vincent, R. D., Avoli, M., and Pineau, J. (2008). Adaptive treatment of epilepsy via\nbatch-mode reinforcement learning. In AAAI, pages 1671–1678.\nHaarnoja, T., Tang, H., Abbeel, P., and Levine, S. (2017). Reinforcement learning with deep energy-\nbased policies. In Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pages 1352–1361. JMLR. org.\nHaarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft actor-critic: Off-policy maximum\nentropy deep reinforcement learning with a stochastic actor. In arXiv.\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. (2018). Learning\nlatent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551.\nHafner, R. and Riedmiller, M. (2011). Reinforcement learning in feedback control. Machine learning,\n84(1-2):137–169.\nHallak, A. and Mannor, S. (2017). Consistent on-line off-policy evaluation. In Proceedings of the\n34th International Conference on Machine Learning-Volume 70, pages 1372–1383. JMLR. org.\nHallak, A., Tamar, A., and Mannor, S. (2015). Emphatic td bellman operator is a contraction. arXiv\npreprint arXiv:1508.03411.\nHallak, A., Tamar, A., Munos, R., and Mannor, S. (2016). Generalized emphatic temporal difference\nlearning: Bias-variance analysis. In Thirtieth AAAI Conference on Artiﬁcial Intelligence.\nHanna, J. P., Stone, P., and Niekum, S. (2017). Bootstrapping with models: Conﬁdence intervals for\noff-policy evaluation. In Thirty-First AAAI Conference on Artiﬁcial Intelligence.\nHeess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and Tassa, Y. (2015). Learning continuous\ncontrol policies by stochastic value gradients. In Advances in Neural Information Processing\nSystems, pages 2944–2952.\nHenderson, J., Lemon, O., and Georgila, K. (2008). Hybrid reinforcement/supervised learning of\ndialogue policies from ﬁxed data sets. Computational Linguistics, 34(4):487–511.\nHuang, J. and Jiang, N. (2019). From importance sampling to doubly robust policy gradient. arXiv\npreprint arXiv:1910.09066.\nImani, E., Graves, E., and White, M. (2018). An off-policy policy gradient theorem using emphatic\nweightings. In Advances in Neural Information Processing Systems, pages 96–106.\nJaksch, T., Ortner, R., and Auer, P. (2010). Near-optimal regret bounds for reinforcement learning.\nJournal of Machine Learning Research, 11(Apr):1563–1600.\nJanner, M., Fu, J., Zhang, M., and Levine, S. (2019). When to trust your model: Model-based policy\noptimization. In Advances in Neural Information Processing Systems, pages 12498–12509.\nJaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, C., Lapedriza, A., Jones, N., Gu, S., and Picard,\nR. (2019). Way off-policy batch deep reinforcement learning of implicit human preferences in\ndialog. arXiv preprint arXiv:1907.00456.\n36\nJiang, N. and Li, L. (2015). Doubly robust off-policy value evaluation for reinforcement learning.\narXiv preprint arXiv:1511.03722.\nJohnson, A. E., Pollard, T. J., Shen, L., Li-wei, H. L., Feng, M., Ghassemi, M., Moody, B., Szolovits,\nP., Celi, L. A., and Mark, R. G. (2016). Mimic-iii, a freely accessible critical care database.\nScientiﬁc data, 3:160035.\nKahn, G., Abbeel, P., and Levine, S. (2020). Badgr: An autonomous self-supervised learning-based\nnavigation system. arXiv preprint arXiv:2002.05700.\nKahn, G., Villaﬂor, A., Abbeel, P., and Levine, S. (2018). Composable action-conditioned predictors:\nFlexible off-policy learning for robot navigation. arXiv preprint arXiv:1810.07167.\nKaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D.,\nFinn, C., Kozakowski, P., Levine, S., et al. (2019a). Model-based reinforcement learning for atari.\narXiv preprint arXiv:1903.00374.\nKaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D.,\nFinn, C., Kozakowski, P., Levine, S., et al. (2019b). Model-based reinforcement learning for atari.\narXiv preprint arXiv:1903.00374.\nKakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning. In\nInternational Conference on Machine Learning (ICML), volume 2.\nKakade, S. M. (2002). A natural policy gradient. In Advances in neural information processing\nsystems, pages 1531–1538.\nKalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakr-\nishnan, M., Vanhoucke, V., et al. (2018). Scalable deep reinforcement learning for vision-based\nrobotic manipulation. In Conference on Robot Learning, pages 651–673.\nKallus, N. and Uehara, M. (2019a). Efﬁciently breaking the curse of horizon: Double reinforcement\nlearning in inﬁnite-horizon processes. arXiv preprint arXiv:1909.05850.\nKallus, N. and Uehara, M. (2019b). Intrinsically efﬁcient, stable, and bounded off-policy evaluation\nfor reinforcement learning. In Advances in Neural Information Processing Systems, pages 3320–\n3329.\nKandasamy, K., Bachrach, Y., Tomioka, R., Tarlow, D., and Carter, D. (2017). Batch policy gradient\nmethods for improving neural conversation models. arXiv preprint arXiv:1702.03334.\nKendall, A. and Gal, Y. (2017). What uncertainties do we need in bayesian deep learning for computer\nvision? In Advances in neural information processing systems, pages 5574–5584.\nKendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., Lam, V.-D., Bewley, A., and\nShah, A. (2019). Learning to drive in a day. In 2019 International Conference on Robotics and\nAutomation (ICRA), pages 8248–8254. IEEE.\nKidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T. (2020). Morel: Model-based ofﬂine\nreinforcement learning. arXiv preprint arXiv:2005.05951.\nKingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. (2014). Semi-supervised learning with\ndeep generative models. In Advances in neural information processing systems, pages 3581–3589.\nKo, J., Klein, D. J., Fox, D., and Haehnel, D. (2007). Gaussian processes and reinforcement learning\nfor identiﬁcation and control of an autonomous blimp. In Proceedings 2007 ieee international\nconference on robotics and automation, pages 742–747. IEEE.\nKonda, V. R. and Tsitsiklis, J. N. (2000). Actor-critic algorithms. In Advances in neural information\nprocessing systems, pages 1008–1014.\nKoppejan, R. and Whiteson, S. (2009). Neuroevolutionary reinforcement learning for generalized\nhelicopter control. In Proceedings of the 11th Annual conference on Genetic and evolutionary\ncomputation, pages 145–152.\n37\nKumar, A. (2019). Data-driven deep reinforcement learning. https://bair.berkeley.edu/\nblog/2019/12/05/bear/. BAIR Blog.\nKumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. (2019). Stabilizing off-policy q-learning\nvia bootstrapping error reduction. In Advances in Neural Information Processing Systems, pages\n11761–11771.\nKumar, A. and Gupta, A. (2020). Does on-policy data collection ﬁx errors in reinforcement learning?\nhttps://bair.berkeley.edu/blog/2020/03/16/discor/. BAIR Blog.\nKumar, A., Gupta, A., and Levine, S. (2020a). Discor: Corrective feedback in reinforcement learning\nvia distribution correction. arXiv preprint arXiv:2003.07305.\nKumar, A., Zhou, A., Tucker, G., and Levine, S. (2020b). Conservative q-learning for ofﬂine\nreinforcement learning. In Neural Information Processing Systems (NeurIPS).\nLagoudakis, M. G. and Parr, R. (2003). Least-squares policy iteration. Journal of machine learning\nresearch, 4(Dec):1107–1149.\nLange, S., Gabel, T., and Riedmiller, M. (2012). Batch reinforcement learning. In Reinforcement\nlearning, pages 45–73. Springer.\nLangford, J., Strehl, A., and Wortman, J. (2008). Exploration scavenging. In Proceedings of the 25th\ninternational conference on Machine learning, pages 528–535.\nLaroche, R., Trichelair, P., and Combes, R. T. d. (2017). Safe policy improvement with baseline\nbootstrapping. arXiv preprint arXiv:1712.06924.\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. nature, 521(7553):436–444.\nLee, D. and He, N. (2018). Stochastic primal-dual q-learning. arXiv preprint arXiv:1810.08298.\nLerer, A., Gross, S., and Fergus, R. (2016). Learning physical intuition of block towers by example.\narXiv preprint arXiv:1603.01312.\nLevine, S. (2018). Reinforcement learning and control as probabilistic inference: Tutorial and review.\narXiv preprint arXiv:1805.00909.\nLevine, S., Finn, C., Darrell, T., and Abbeel, P. (2016). End-to-end training of deep visuomotor\npolicies. The Journal of Machine Learning Research, 17(1):1334–1373.\nLevine, S. and Koltun, V. (2013). Guided policy search. In International Conference on Machine\nLearning, pages 1–9.\nLevine, S., Pastor, P., Krizhevsky, A., Ibarz, J., and Quillen, D. (2018). Learning hand-eye coordi-\nnation for robotic grasping with deep learning and large-scale data collection. The International\nJournal of Robotics Research, 37(4-5):421–436.\nLi, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to personalized\nnews article recommendation. In Proceedings of the 19th international conference on World wide\nweb, pages 661–670.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.\n(2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.\nLin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and\nteaching. Machine learning, 8(3-4):293–321.\nLiu, Q., Li, L., Tang, Z., and Zhou, D. (2018). Breaking the curse of horizon: Inﬁnite-horizon\noff-policy estimation. In Advances in Neural Information Processing Systems, pages 5356–5366.\nLiu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. (2019). Off-policy policy gradient with\nstate distribution correction. arXiv preprint arXiv:1904.08473.\nLuo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. (2018). Algorithmic framework for model-\nbased deep reinforcement learning with theoretical guarantees. arXiv preprint arXiv:1807.03858.\n38\nMaddern, W., Pascoe, G., Linegar, C., and Newman, P. (2017). 1 year, 1000 km: The oxford robotcar\ndataset. The International Journal of Robotics Research, 36(1):3–15.\nMandlekar, A., Xu, D., Martín-Martín, R., Savarese, S., and Fei-Fei, L. (2020). Learning to generalize\nacross long-horizon tasks from human demonstrations. arXiv preprint arXiv:2003.06085.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.\n(2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\nMo, K., Li, H., Lin, Z., and Lee, J.-Y. (2018). The adobeindoornav dataset: Towards deep reinforce-\nment learning based real-world indoor robot visual navigation. arXiv preprint arXiv:1802.08824.\nMousavi, A., Li, L., Liu, Q., and Zhou, D. (2020). Black-box off-policy estimation for inﬁnite-horizon\nreinforcement learning. arXiv preprint arXiv:2003.11126.\nMurphy, S. A., van der Laan, M. J., Robins, J. M., and Group, C. P. P. R. (2001). Marginal mean\nmodels for dynamic regimes. Journal of the American Statistical Association, 96(456):1410–1423.\nNachum, O., Chow, Y., Dai, B., and Li, L. (2019a). Dualdice: Behavior-agnostic estimation of\ndiscounted stationary distribution corrections. In Advances in Neural Information Processing\nSystems, pages 2315–2325.\nNachum, O. and Dai, B. (2020). Reinforcement learning via fenchel-rockafellar duality. arXiv\npreprint arXiv:2001.01866.\nNachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D. (2019b). Algaedice: Policy\ngradient from arbitrary experience. arXiv preprint arXiv:1912.02074.\nNadjahi, K., Laroche, R., and Combes, R. T. d. (2019). Safe policy improvement with soft baseline\nbootstrapping. arXiv preprint arXiv:1907.05079.\nNagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2018). Neural network dynamics for\nmodel-based deep reinforcement learning with model-free ﬁne-tuning. In 2018 IEEE International\nConference on Robotics and Automation (ICRA), pages 7559–7566. IEEE.\nNair, A., Dalal, M., Gupta, A., and Levine, S. (2020). Accelerating online reinforcement learning\nwith ofﬂine datasets. arXiv preprint arXiv:2006.09359.\nNie, X., Brunskill, E., and Wager, S. (2019). Learning when-to-treat policies. arXiv preprint\narXiv:1905.09751.\nNowozin, S., Cseke, B., and Tomioka, R. (2016). f-gan: Training generative neural samplers using\nvariational divergence minimization. In Advances in neural information processing systems, pages\n271–279.\nOh, J., Guo, X., Lee, H., Lewis, R. L., and Singh, S. (2015). Action-conditional video prediction\nusing deep networks in atari games. In Advances in neural information processing systems, pages\n2863–2871.\nOh, J., Singh, S., and Lee, H. (2017). Value prediction network. In Advances in Neural Information\nProcessing Systems, pages 6118–6128.\nOsband, I., Blundell, C., Pritzel, A., and Van Roy, B. (2016). Deep exploration via bootstrapped dqn.\nIn Advances in neural information processing systems, pages 4026–4034.\nOsband, I. and Van Roy, B. (2017). Why is posterior sampling better than optimism for reinforcement\nlearning? In Proceedings of the 34th International Conference on Machine Learning-Volume 70,\npages 2701–2710. JMLR. org.\nO’Donoghue, B., Osband, I., Munos, R., and Mnih, V. (2018). The uncertainty bellman equation and\nexploration. In International Conference on Machine Learning, pages 3836–3845.\nPan, Y., Cheng, C.-A., Saigol, K., Lee, K., Yan, X., Theodorou, E., and Boots, B. (2017). Agile\nautonomous driving using end-to-end deep imitation learning. arXiv preprint arXiv:1709.07174.\n39\nPankov, S. (2018). Reward-estimation variance elimination in sequential decision processes. arXiv\npreprint arXiv:1811.06225.\nParmas, P., Rasmussen, C. E., Peters, J., and Doya, K. (2019). Pipps: Flexible model-based policy\nsearch robust to the curse of chaos. arXiv preprint arXiv:1902.01240.\nParr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., and Littman, M. L. (2008). An analysis of linear\nmodels, linear value-function approximation, and feature selection for reinforcement learning. In\nProceedings of the 25th international conference on Machine learning, pages 752–759.\nPeng, X. B., Kumar, A., Zhang, G., and Levine, S. (2019). Advantage-weighted regression: Simple\nand scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177.\nPeshkin, L. and Shelton, C. R. (2002). Learning from scarce experience. arXiv preprint cs/0204043.\nPietquin, O., Geist, M., Chandramohan, S., and Frezza-Buet, H. (2011). Sample-efﬁcient batch\nreinforcement learning for dialogue management optimization. ACM Transactions on Speech and\nLanguage Processing (TSLP), 7(3):1–21.\nPinto, L. and Gupta, A. (2016). Supersizing self-supervision: Learning to grasp from 50k tries and\n700 robot hours. In 2016 IEEE international conference on robotics and automation (ICRA), pages\n3406–3413. IEEE.\nPrasad, N., Cheng, L.-F., Chivers, C., Draugelis, M., and Engelhardt, B. E. (2017). A reinforcement\nlearning approach to weaning of mechanical ventilation in intensive care units. arXiv preprint\narXiv:1704.06300.\nPrecup, D. (2000). Eligibility traces for off-policy policy evaluation. Computer Science Department\nFaculty Publication Series, page 80.\nPrecup, D., Sutton, R. S., and Dasgupta, S. (2001). Off-policy temporal-difference learning with\nfunction approximation. In ICML, pages 417–424.\nRaghu, A., Komorowski, M., Ahmed, I., Celi, L., Szolovits, P., and Ghassemi, M. (2017). Deep\nreinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602.\nRhinehart, N., McAllister, R., and Levine, S. (2018). Deep imitative models for ﬂexible inference,\nplanning, and control. arXiv preprint arXiv:1810.06544.\nRiedmiller, M. (2005). Neural ﬁtted Q iteration–ﬁrst experiences with a data efﬁcient neural re-\ninforcement learning method. In European Conference on Machine Learning, pages 317–328.\nSpringer.\nRoss, S. and Bagnell, D. (2010). Efﬁcient reductions for imitation learning. In International\nConference on Artiﬁcial Intelligence and Statistics (AISTATS), pages 661–668.\nRoss, S., Gordon, G., and Bagnell, D. (2011). A reduction of imitation learning and structured\nprediction to no-regret online learning. In International Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS), pages 627–635.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla,\nA., Bernstein, M., et al. (2015). Imagenet large scale visual recognition challenge. International\njournal of computer vision, 115(3):211–252.\nSachdeva, N., Su, Y., and Joachims, T. (2020). Off-policy bandits with deﬁcient support.\nSadeghi, F. and Levine, S. (2017). CAD2RL: Real single-image ﬂight without a single real image. In\nRobotics: Science and Systems.\nSagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. (2019). Distributionally robust neural\nnetworks. In International Conference on Learning Representations.\nSallab, A. E., Abdou, M., Perot, E., and Yogamani, S. (2017). Deep reinforcement learning framework\nfor autonomous driving. Electronic Imaging, 2017(19):70–76.\n40\nSchölkopf, B. (2019). Causality for machine learning. arXiv preprint arXiv:1911.10500.\nSchulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust region policy optimiza-\ntion. In International conference on machine learning, pages 1889–1897.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347.\nShortreed, S. M., Laber, E., Lizotte, D. J., Stroup, T. S., Pineau, J., and Murphy, S. A. (2011).\nInforming sequential clinical decision-making through reinforcement learning: an empirical study.\nMachine learning, 84(1-2):109–136.\nSiegel, N. Y., Springenberg, J. T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner,\nR., and Riedmiller, M. (2020). Keep doing what worked: Behavioral modelling priors for ofﬂine\nreinforcement learning. arXiv preprint arXiv:2002.08396.\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic\npolicy gradient algorithms. In International Conference on Machine Learning (ICML).\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker,\nL., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge. Nature,\n550(7676):354–359.\nSinha, A., Namkoong, H., and Duchi, J. (2017). Certifying some distributional robustness with\nprincipled adversarial training. arXiv preprint arXiv:1710.10571.\nSriperumbudur, B. K., Fukumizu, K., Gretton, A., Schölkopf, B., and Lanckriet, G. R. (2009).\nOn integral probability metrics,\\phi-divergences and binary classiﬁcation.\narXiv preprint\narXiv:0901.2698.\nStrehl, A., Langford, J., Li, L., and Kakade, S. M. (2010). Learning from logged implicit exploration\ndata. In Advances in Neural Information Processing Systems, pages 2217–2225.\nSun, L., Peng, C., Zhan, W., and Tomizuka, M. (2018a). A fast integrated planning and control\nframework for autonomous driving via imitation learning. In Dynamic Systems and Control\nConference, volume 51913, page V003T37A012. American Society of Mechanical Engineers.\nSun, W., Gordon, G. J., Boots, B., and Bagnell, J. (2018b). Dual policy iteration. In Advances in\nNeural Information Processing Systems, pages 7059–7069.\nSutton, R. S. (1991). Dyna, an integrated architecture for learning, planning, and reacting. ACM\nSigart Bulletin, 2(4):160–163.\nSutton, R. S. and Barto, A. G. (1998). Introduction to Reinforcement Learning. MIT Press, Cambridge,\nMA, USA, 1st edition.\nSutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesvári, C., and Wiewiora,\nE. (2009). Fast gradient-descent methods for temporal-difference learning with linear function\napproximation. In Proceedings of the 26th Annual International Conference on Machine Learning,\npages 993–1000.\nSutton, R. S., Mahmood, A. R., and White, M. (2016). An emphatic approach to the problem of\noff-policy temporal-difference learning. The Journal of Machine Learning Research, 17(1):2603–\n2631.\nSutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000). Policy gradient methods for\nreinforcement learning with function approximation. In Advances in neural information processing\nsystems, pages 1057–1063.\nSutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., and Precup, D. (2011).\nHorde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor\ninteraction. In The 10th International Conference on Autonomous Agents and Multiagent Systems-\nVolume 2, pages 761–768.\n41\nSwaminathan, A., Krishnamurthy, A., Agarwal, A., Dudik, M., Langford, J., Jose, D., and Zitouni,\nI. (2017). Off-policy evaluation for slate recommendation. In Advances in Neural Information\nProcessing Systems, pages 3632–3642.\nTamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel, P. (2016). Value iteration networks. In\nAdvances in Neural Information Processing Systems, pages 2154–2162.\nTan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y., Hafner, D., Bohez, S., and Vanhoucke, V. (2018).\nSim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332.\nTang, Z., Feng, Y., Li, L., Zhou, D., and Liu, Q. (2019). Doubly robust bias reduction in inﬁnite\nhorizon off-policy estimation. arXiv preprint arXiv:1910.07186.\nTassa, Y., Erez, T., and Todorov, E. (2012). Synthesis and stabilization of complex behaviors through\nonline trajectory optimization. In 2012 IEEE/RSJ International Conference on Intelligent Robots\nand Systems, pages 4906–4913. IEEE.\nTesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level play.\nNeural computation, 6(2):215–219.\nTheocharous, G., Thomas, P. S., and Ghavamzadeh, M. (2015). Personalized ad recommendation\nsystems for life-time value optimization with guarantees. In Twenty-Fourth International Joint\nConference on Artiﬁcial Intelligence.\nThomas, P. (2014). Bias in natural actor-critic algorithms. In International Conference on Machine\nLearning (ICML).\nThomas, P. and Brunskill, E. (2016). Data-efﬁcient off-policy policy evaluation for reinforcement\nlearning. In International Conference on Machine Learning, pages 2139–2148.\nThomas, P. S., Theocharous, G., and Ghavamzadeh, M. (2015). High-conﬁdence off-policy evaluation.\nIn Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence.\nThomas, P. S., Theocharous, G., Ghavamzadeh, M., Durugkar, I., and Brunskill, E. (2017). Predictive\noff-policy policy evaluation for nonstationary decision problems, with applications to digital\nmarketing. In Twenty-Ninth IAAI Conference.\nTodorov, E. (2006). Linearly-solvable markov decision problems. In Advances in Neural Information\nProcessing Systems (NIPS).\nTseng, H.-H., Luo, Y., Cui, S., Chien, J.-T., Ten Haken, R. K., and El Naqa, I. (2017). Deep\nreinforcement learning for automated radiation adaptation in lung cancer.\nMedical physics,\n44(12):6690–6705.\nUehara, M. and Jiang, N. (2019). Minimax weight and q-function learning for off-policy evaluation.\narXiv preprint arXiv:1910.12809.\nVan Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J. (2018). Deep\nreinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648.\nVanseijen, H. and Sutton, R. (2015).\nA deeper look at planning as learning from replay.\nIn\nInternational conference on machine learning, pages 2314–2322.\nWang, L., Zhang, W., He, X., and Zha, H. (2018). Supervised reinforcement learning with recurrent\nneural network for dynamic treatment recommendation. In Proceedings of the 24th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining, pages 2447–2456.\nWang, M. and Chen, Y. (2016). An online primal-dual method for discounted markov decision\nprocesses. In 2016 IEEE 55th Conference on Decision and Control (CDC), pages 4516–4521.\nIEEE.\nWang, Y.-X., Agarwal, A., and Dudik, M. (2017). Optimal and adaptive off-policy evaluation in\ncontextual bandits. In Proceedings of the 34th International Conference on Machine Learning-\nVolume 70, pages 3589–3597. JMLR. org.\n42\nWang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., and de Freitas, N. (2016).\nSample efﬁcient actor-critic with experience replay. arXiv preprint arXiv:1611.01224.\nWatkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8(3-4):279–292.\nWatter, M., Springenberg, J., Boedecker, J., and Riedmiller, M. (2015). Embed to control: A locally\nlinear latent dynamics model for control from raw images. In Advances in neural information\nprocessing systems, pages 2746–2754.\nWen, J., Dai, B., Li, L., and Schuurmans, D. (2020). Batch stationary distribution estimation. arXiv\npreprint arXiv:2003.00722.\nWu, Y., Tucker, G., and Nachum, O. (2019a). Behavior regularized ofﬂine reinforcement learning.\narXiv preprint arXiv:1911.11361.\nWu, Y., Winston, E., Kaushik, D., and Lipton, Z. (2019b). Domain adaptation with asymmetrically-\nrelaxed distribution alignment. arXiv preprint arXiv:1903.01689.\nYu, F., Xian, W., Chen, Y., Liu, F., Liao, M., Madhavan, V., and Darrell, T. (2018). Bdd100k: A\ndiverse driving video database with scalable annotation tooling. arXiv preprint arXiv:1805.04687.\nYu, H. (2015). On convergence of emphatic temporal-difference learning. In Conference on Learning\nTheory, pages 1724–1751.\nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma, T. (2020). Mopo:\nModel-based ofﬂine policy optimization. In Neural Information Processing Systems (NeurIPS).\nYurtsever, E., Lambert, J., Carballo, A., and Takeda, K. (2020). A survey of autonomous driving:\nCommon practices and emerging technologies. IEEE Access.\nZeng, A., Song, S., Welker, S., Lee, J., Rodriguez, A., and Funkhouser, T. (2018). Learning synergies\nbetween pushing and grasping with self-supervised deep reinforcement learning. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pages 4238–4245. IEEE.\nZhang, M., Vikram, S., Smith, L., Abbeel, P., Johnson, M. J., and Levine, S. (2018). Solar: deep struc-\ntured representations for model-based reinforcement learning. arXiv preprint arXiv:1808.09105.\nZhang, R., Dai, B., Li, L., and Schuurmans, D. (2020a). Gendice: Generalized ofﬂine estimation of\nstationary values. In International Conference on Learning Representations.\nZhang, S., Boehmer, W., and Whiteson, S. (2019). Generalized off-policy actor-critic. In Advances\nin Neural Information Processing Systems, pages 1999–2009.\nZhang, S., Liu, B., and Whiteson, S. (2020b). Gradientdice: Rethinking generalized ofﬂine estimation\nof stationary values. arXiv preprint arXiv:2001.11113.\nZhou, L., Small, K., Rokhlenko, O., and Elkan, C. (2017). End-to-end ofﬂine goal-oriented dialog\npolicy learning via policy gradient. arXiv preprint arXiv:1712.02838.\n43\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2020-05-04",
  "updated": "2020-11-01"
}