{
  "id": "http://arxiv.org/abs/2005.02612v1",
  "title": "Deep Divergence Learning",
  "authors": [
    "Kubra Cilingir",
    "Rachel Manzelli",
    "Brian Kulis"
  ],
  "abstract": "Classical linear metric learning methods have recently been extended along\ntwo distinct lines: deep metric learning methods for learning embeddings of the\ndata using neural networks, and Bregman divergence learning approaches for\nextending learning Euclidean distances to more general divergence measures such\nas divergences over distributions. In this paper, we introduce deep Bregman\ndivergences, which are based on learning and parameterizing functional Bregman\ndivergences using neural networks, and which unify and extend these existing\nlines of work. We show in particular how deep metric learning formulations,\nkernel metric learning, Mahalanobis metric learning, and moment-matching\nfunctions for comparing distributions arise as special cases of these\ndivergences in the symmetric setting. We then describe a deep learning\nframework for learning general functional Bregman divergences, and show in\nexperiments that this method yields superior performance on benchmark datasets\nas compared to existing deep metric learning approaches. We also discuss novel\napplications, including a semi-supervised distributional clustering problem,\nand a new loss function for unsupervised data generation.",
  "text": "Deep Divergence Learning\nKubra Cilingir 1 Rachel Manzelli 1 Brian Kulis 1\nAbstract\nClassical linear metric learning methods have re-\ncently been extended along two distinct lines:\ndeep metric learning methods for learning em-\nbeddings of the data using neural networks, and\nBregman divergence learning approaches for ex-\ntending learning Euclidean distances to more gen-\neral divergence measures such as divergences over\ndistributions. In this paper, we introduce deep\nBregman divergences, which are based on learn-\ning and parameterizing functional Bregman diver-\ngences using neural networks, and which unify\nand extend these existing lines of work. We show\nin particular how deep metric learning formula-\ntions, kernel metric learning, Mahalanobis met-\nric learning, and moment-matching functions for\ncomparing distributions arise as special cases of\nthese divergences in the symmetric setting. We\nthen describe a deep learning framework for learn-\ning general functional Bregman divergences, and\nshow in experiments that this method yields supe-\nrior performance on benchmark datasets as com-\npared to existing deep metric learning approaches.\nWe also discuss novel applications, including a\nsemi-supervised distributional clustering problem,\nand a new loss function for unsupervised data\ngeneration.\n1. Introduction\nThe goal of metric learning is to use supervised data in or-\nder to learn a distance function (or more general divergence\nmeasure) that is tuned to the data and task at hand. Classical\napproaches to metric learning are generally focused on the\nlinear regime, where one learns a linear mapping of the\ndata and then applies the Euclidean distance in the mapped\nspace for downstream tasks such as clustering, ranking, and\nclassiﬁcation (Davis et al., 2007; Weinberger & Saul, 2009;\n1Department of Electrical and Computer Engineering,\nBoston University, Boston, Massachusetts, USA. Correspon-\ndence to: Kubra Cilingir <kubra@bu.edu>, Rachel Manzelli\n<manzelli@bu.edu>, Brian Kulis <bkulis@bu.edu>.\nGoldberger et al., 2004). These methods, known as Ma-\nhalanobis metric learning approaches, have been analyzed\ntheoretically, are scalable, and usually involve convex opti-\nmization problems that can be solved globally (Kulis, 2013;\nBellet et al., 2015).\nClassical metric learning methods have been extended along\nvarious axes; two important directions are deep metric learn-\ning and Bregman divergence learning. Deep metric learning\napproaches replace the linear mapping learned in Maha-\nlanobis metric learning methods with more general map-\npings that are learned via neural networks (Hoffer & Ailon,\n2015; Chopra et al., 2005). On the other hand, Bregman\ndivergence methods replace the squared Euclidean distance\nwith arbitrary Bregman divergences (Bregman, 1967), and\nlearn the underlying generating function of the Bregman\ndivergence via piecewise linear approximators (Siahkamari\net al., 2019) or convex combinations of existing basis func-\ntions (Wu et al., 2009). These two extensions of classical\nmetric learning are complementary and disjoint. For in-\nstance, Bregman divergence approaches can be utilized in\nscenarios where one needs to compare distributions (the\nwell-known KL-divergence arises as a special case), but the\nlearning problems are not directly applicable to the deep\nlearning setting. Similarly, deep metric learning methods\nstill employ Euclidean distances, and are thus not directly\namenable to problems where one needs to compare distribu-\ntions.\nIn this paper, we introduce a framework for studying Breg-\nman divergences that can naturally be learned in the deep set-\nting. Figure 1 gives a high-level overview of our approach,\nwhich we term as deep Bregman divergences, in comparison\nto existing metric learning approaches. These divergences\nare based on functional Bregman divergences (Frigyik et al.,\n2008), which were introduced as a extension of classical\nBregman divergences but with functional inputs instead of\nvector inputs. In this functional setting, the underlying Breg-\nman divergence is parameterized by a convex functional\nwhose input itself is a function.\nWe ﬁrst perform an analysis for the symmetric divergence\ncase. In this setting, we prove a result about the form for any\nfunctional Bregman divergence and observe that many exist-\ning metric learning models can be seen to arise from special\ncases of this form. These include deep learning methods,\narXiv:2005.02612v1  [cs.LG]  6 May 2020\nDeep Divergence Learning\nclassical linear metric learning methods, and kernel metric\nlearning. There are also special cases that include moment-\nmatching functions, which yields connections to the Wasser-\nstein distance (Arjovsky et al., 2017), maximum mean dis-\ncrepancy (MMD), and kernel MMD (Gretton, 2012).\nWe then turn our attention to the strictly more general case,\nwhere the divergences need not be symmetric; the KL-\ndivergence is a classical example of such an asymmetric\nBregman divergence. In this setting, we describe a frame-\nwork for learning an arbitrary deep Bregman divergence.\nOur approach is based on appropriately parameterizing the\nconvex functional governing the underlying Bregman di-\nvergence with a neural network, and learning the resulting\nparameters of that network.\nWe describe several applications of our proposed deep Breg-\nman divergence framework. First, we can extend existing\ndeep metric learning formulations to learn more general\ndeep Bregman divergences. Second, since our divergences\ncan naturally be applied to compare distributions on data,\nanother application is in unsupervised generative learning,\nwhere the goal is to minimize a learned distributional diver-\ngence between real and generated data. In particular, we dis-\ncuss connections to GAN models and describe some novel\nalgorithms for unsupervised data generation. Third, we de-\nscribe a semi-supervised distributional clustering problem.\nHere, the problem is to cluster data where each data point is\nrepresented as a distribution—for example, a movie’s rating\nmay be represented as a distribution over user scores—using\ntraining data where we know whether pairs of distributions\nshould be clustered together or not.\nIn all three of the above settings, we show empirical results\nthat highlight the beneﬁts of our framework. In particular,\nwe show that learning asymmetric divergences offers perfor-\nmance gains over existing symmetric models on benchmark\ndata, and achieve state-of-the-art classiﬁcation performance\nin some settings. We also show that our clustering algorithm\noutperforms existing baselines on a simple proof-of-concept\ndataset as well as several human activity sensor data sets,\nand that our data generation results suggest that there may\nbe value in further developing and studying new learned\ndistributional divergence measures.\n2. Related Work\nMuch of the early work on metric learning focused on the\nlinear setting, often referred to as Mahalanobis metric learn-\ning. In this setting, the goal is to learn a global linear trans-\nformation of the data and apply standard distances such as\nthe Euclidean distance on top of the learned transformation.\nThis is often expressed as learning a distance function of the\nform dA(x, y) = (x−y)T A(x−y), where A is a positive\nsemi-deﬁnite matrix. This is equivalent to learning a lin-\near transformation G, where A = GT G, since dA(x, y) =\nLinear(Mahalanobis)\nMetric Learning\nBeyond Euclidean \nBregman Divergence\nLearning\nDeep Metric\nLearning\nDeep Divergence\nLearning\nWeights\nto \nFunctions\nFigure 1. Overview of our framework in comparison to existing\nmetric learning approaches. Deep Bregman divergences feature\nboth the ability to learn divergences beyond Euclidean (such as\ndivergences over distributions) while encompassing parameteriza-\ntions that are amenable to deep learning architectures.\n(x−y)T GT G(x−y) = ∥Gx−Gy∥2\n2. Examples of this ap-\nproach to metric learning include MMC (Xing et al., 2003),\nMCML (Globerson & Roweis, 2005), LMNN (Weinberger\n& Saul, 2009), ITML (Davis et al., 2007), POLA (Shalev-\nShwartz et al., 2004), LEGO (Jain et al., 2008), and others.\nSee the surveys by Kulis (2013) and Bellet et al. (2015) for\nfurther references and details on some of these approaches.\nNote that one of the advantages of the linear approach is\nthat one can often provide performance guarantees—for in-\nstance, a signiﬁcant amount of work has gone into proving\nregret bounds in the online setting (Shalev-Shwartz et al.,\n2004), as well as generalization bounds (Bellet & Habrard,\n2015; Cao et al., 2016) for some Mahalanobis metric learn-\ning models.\nWhile linear methods are simpler and can often be analyzed\ntheoretically, in practice it is often useful to learn other, non-\nlinear, approaches to metric learning. For instance, one can\nshow that many linear models can be appropriately adapted\nto run in kernel space (Jain et al., 2012; Chatpatanasiri et al.,\n2010). Another more recent approach to moving beyond\nlinear metric learning is the Bregman divergence learning\nframework discussed in the introduction (Siahkamari et al.,\n2019; Wu et al., 2009). Here we move beyond learning\nMahalanobis metrics, but instead focus on a strictly larger\nclass of divergences that includes asymmetric divergences\nsuch as the KL-divergence, Itakura-Saito divergence, and\nothers. These may be considered as non-linear approaches\n(since the resulting divergence does not involve linear trans-\nformations in general). The Bregman learning framework is\nthus more powerful than linear approaches but also remains\nwell-principled: one can prove generalization bounds in this\nframework.\nThe third, and by far the most well-studied, approach to\nnon-linear metric learning is known as deep metric learning,\nDeep Divergence Learning\nand involves learning a neural network to embed data into\nsome new space, where standard distances such as Euclidean\ndistance are used. If f is a function that maps an input x\nto an embedding f(x), then the resulting learned metric is\ntypically ∥f(x) −f(y)∥2\n2. Several popular loss functions\nhave been proposed to learn such a metric—the two main\nones are the contrastive loss (Chopra et al., 2005) and the\ntriplet loss (Hoffer & Ailon, 2015). Both utilize supervision\n(pairwise for the contrastive loss and relative constraints for\nthe triplet loss) and use the learned distance ∥f(x)−f(y)∥2\n2.\nMoreover, there has been considerable follow-up work that\nexplores how best to choose pairs or triples of points from\na training set to achieve the best results (Hermans et al.,\n2017). There has also been work on deep metric learning\nusing other losses, such as the angular loss (Wang et al.,\n2017) or the average precision for deep metric learning to\nrank (Cakir et al., 2019).\nOur work also has ties to methods involving comparing\ndistributions. Examples of such measures that are relevant\nto our work include the maximum mean discrepancy met-\nric (also known as the integral probability metric) (Gretton,\n2012), the kernel MMD, and the Wasserstein distance (Ar-\njovsky et al., 2017). Several notions of divergences over\ndistributions have been used for unsupervised data gener-\nation in GAN-type models, including the Jensen-Shannon\ndivergence (Goodfellow et al., 2014), the Wasserstein dis-\ntance (Arjovsky et al., 2017), and the MMD (Li et al., 2015;\n2017).\n3. Deep Bregman Divergences\nWe now turn our attention to functional Bregman diver-\ngences, the main tool for our learning problems. Our goal is\ntwo-fold: we ﬁrst prove a result that characterizes the form\nfor a symmetric functional Bregman divergence and show\nconnections between this form and existing metric learn-\ning models. Second, we consider a parameterization for\narbitrary functional Bregman divergences that will permit\nlearning via neural networks.\n3.1. Bregman Divergences and Functional Bregman\nDivergences\nA Bregman divergence is a generalized measure of distance\nbetween objects, parameterized by a strictly convex function\nφ (Bregman, 1967). Let φ : Ω→R, where Ωis a closed,\nconvex set. The Bregman divergence with respect to φ (for\nvector inputs) is deﬁned as\nDφ(x, y) = φ(x) −φ(y) −(x −y)T ∇φ(y).\nNote that the last term represents the derivative of φ in the\ndirection of x −y. Examples of Bregman divergences in-\nclude the squared Euclidean distance, parameterized by\nφ(x) =\n1\n2∥x∥2\n2; the KL-divergence, parameterized by\nφ(x) = P\ni xi log xi; and the Itakura-Saito distance, pa-\nrameterized by φ(x) = −P\ni log xi.\nBregman divergences arise in many settings in machine\nlearning and related areas. In the study of exponential fam-\nily distributions, there is a bijection between the class of\nregular Bregman divergences and regular exponential fami-\nlies (see Banerjee et al. (2005)). In optimization, Bregman\ndivergences arise frequently; for instance, mirror descent\nutilizes Bregman divergences, and Bregman divergences\nwere originally proposed as part of constrained optimiza-\ntion (Bregman, 1967). In the study of clustering, Bregman\ndivergences offer a straightforward way to extend the k-\nmeans algorithm beyond the use of the squared Euclidean\ndistance (Banerjee et al., 2005). A consequence of this is a\nway to cluster multivariate Gaussians in a k-means frame-\nwork (Davis & Dhillon, 2006); we will use this algorithm\nas a baseline later in the paper.\nMore recently, Frigyik et al. (2008) proposed and studied\nan extension to standard Bregman divergences called func-\ntional Bregman divergences, where instead of vector inputs,\nwe compute a divergence between pairs of functions (or\ndistributions). In this case, given two functions p and q, and\na strictly convex functional φ whose input space is a convex\nset of functions and whose output is in R, the corresponding\nBregman divergence is\nDφ(p, q) = φ(p) −φ(q) −\nZ\n[p(x) −q(x)]δφ(q)(x)dx.\nHere δφ(q) is the functional derivative of φ at q and the\nintegral term calculates this derivative in the direction of\np −q.1 An example of a functional Bregman divergence\narises when we choose φ(p) =\nR\np(x)2dx; in this case, one\ncan work out that the functional derivative of φ at p is 2p\nand that the resulting functional divergence is\nR\n[p(x) −\nq(x)]2dx.\n3.2. The Symmetric Setting\nOur ﬁrst goal is to relate functional Bregman divergences\nback to concepts in metric learning and other related learn-\ning models. To do this, let us deﬁne a symmetric functional\nBregman divergence as a functional Bregman divergence\nsuch that Dφ(p, q) = Dφ(q, p) for all p and q.\nOur ﬁrst result characterizes the form of an arbitrary sym-\nmetric functional Bregman divergence. This result can be\nstated as follows:\nTheorem 3.1. A functional Bregman divergence Dφ(p, q)\nis a symmetric functional Bregman divergence if and only if\nit has the following form:\nDφ(p, q) =\nZZ\n(p(x) −q(x))(p(y) −q(y))ψ(x, y)dxdy,\n1Note that Frigyik et al. (2008) utilize the more general Fr´echet\nderivative. Also, for simplicity, we limit ourselves to Riemann\nintegrals unless otherwise noted. See Appendix for more details.\nDeep Divergence Learning\nwhere ψ(x, y) is some symmetric positive semi-deﬁnite func-\ntion.\nFor instance, the example from above, where φ(p) =\nR\np(x)2dx, can be seen as a special case where ψ(x, y) = 1\nif x = y and 0 otherwise. The proof of the theorem appears\nin the appendix. In essence, this result extends an anal-\nogous result known from the vector setting, which states\nthat any symmetric Bregman divergence must be a Maha-\nlanobis distance, namely Dφ(x, y) = (x−y)T A(x−y) for\nsome positive semi-deﬁnite matrix A (Bauschke & Borwein,\n2001).\nNext we must show that, for particular choices of the\nsymmetric positive semi-deﬁnite function ψ, as well as\nrestrictions on p and q, the resulting divergence yields\nfamiliar forms.\nDeep Metric Learning and Moment-Matching. Let us\nconsider ψ(x, y) = fW (x)T fW (y), where fW (x) is an\nembedding given by a neural network parameterized by\nweights W. This is clearly a positive semi-deﬁnite function,\nas it is an inner product between embedded data points.\nFurther, assume p and q are distributions.\nFirst, from Fubini’s theorem, observe that we can re-write\nthe functional Bregman divergence in this special case as\nDφ(p, q) = ∥Ep[fW ] −Eq[fW ]∥2.\nThis is a moment-matching type of metric. Note the similar-\nity to the Wasserstein distance (Arjovsky et al., 2017) and\nthe maximum mean discrepancy (Gretton, 2012). (In those\ncases, one further takes a supremum over the function fW ,\nwhich we would also do when performing optimization to\nlearn fW .)\nThis general form of the divergence is typically difﬁcult to\ncompute. We can consider the case when we have ﬁnite\nsamples or, equivalently, we let p and q be given by empiri-\ncal distributions over sets of points P and Q, respectively.\nIn this case, the resulting divergence simpliﬁes to\nDφ(p, q) =\n\r\r\r\r\n1\n|P|\nX\nx∈P\nfW (x) −1\n|Q|\nX\ny∈Q\nfW (y)\n\r\r\r\r\n2\n.\nThis yields a divergence measure between distributions p\nand q that matches the ﬁrst moment, similar to how MMD\noperates.\nTo make connections to deep metric learning, consider the\ncase where P and Q are of size one, namely Dirac delta\nfunctions at points x and y, respectively. Then the diver-\ngence is simply\nDφ(p, q) = ∥fW (x) −fW (y)∥2,\nor just the squared Euclidean distance after embedding the\ndata via a neural network. This form is precisely what\nnearly all deep metric learning methods employ: they learn a\nneural network to embed data, apply the (squared) Euclidean\ndistance in the mapped space, and then apply a loss function\nsuch as a contrastive or triplet loss on top of this mapped\ndistance (Chopra et al., 2005; Hoffer & Ailon, 2015).\nLinear Metric Learning. If we replace the integral in the\nfunctional Bregman divergence with a Lebesgue integral (as\nit was deﬁned in the original functional Bregman divergence\npaper), then use the counting measure for integration, the\nintegral in the functional Bregman divergence simply be-\ncomes a sum over the elements in the measure space. In this\ncase, ψ(x, y) is then replaced by a positive semi-deﬁnite\nmatrix A, and function inputs to the divergence are replaced\nby vectors x and y. Then the resulting divergence is the\nusual Mahalanobis distance\nDφ(x, y) = (x −y)T A(x −y).\nThus, we can recover the usual Mahalanobis metric used in\nlinear metric learning under our framework.\nKernel Metric Learning. We can also recover familiar\nkernel forms of the preceding functions. In the case of a\nkernel function ψ(x, y) = κ(x, y), the divergence recovers\nthe moment-matching objective but with the norm induced\nby the kernel’s reproducing kernel Hilbert space, similar\nto kernel MMD (Gretton, 2012). Further, in the case of\na kernel function κ(x, y) = g(x)T Ag(x), where g(x) is\nan embedding to a reproducing kernel Hilbert space, and\nA is a positive-deﬁnite operator, the resulting divergence\nin the single-sample case yields the divergence studied for\nMahalanobis metric learning in kernel space (Kulis et al.,\n2009).\nA summary of some of the special cases described in this\nsection appear in Table ??.\n3.3. The General Setting\nNext we consider the more general setting, i.e., when the\nfunctional divergence may not be symmetric. Here our goal\nis to introduce a parameterization of the functional diver-\ngences that are amenable to learning via neural networks.\nWe term the resulting divergences as deep Bregman diver-\ngences.\nA key insight of Siahkamari et al. (2019) was that one can\napproximate a strictly convex function arbitrarily well with\na piecewise linear function. In particular, they chose to\nparameterize the generating function φ of a vector Bregman\ndivergence by the following max-afﬁne function:\nφ(x) = max\nc (xT wc + bc).\nHere c ranges from 1 to K, where K is the number of hyper-\nDeep Divergence Learning\nCase\nIntegral Setting\nψ(x, y)\nInputs to Dφ\nDφ\nMahalanobis Distance\nLebesgue + Count. Meas.\nA ⪰0\nVectors x, y\n(x −y)T A(x −y)\nDeep Metric Learning\nRiemann\nfW (x)T fW (y)\nDirac Deltas at x, y\n∥fW (x) −fW (y)∥2\nMoment Matching\nRiemann\nfW (x)T fW (y)\nDistributions p, q\n∥Ep[fW ] −Eq[fW ]∥2\nTable 1. Some of the special cases of Dφ(p, q) for the symmetric divergence setting.\nplanes used to approximate the underlying strictly convex\nfunction. Such functions can be used to approximate any\nvector Bregman divergence arbitrarily well. Thus, learning\na Bregman divergence amounts to learning the weights wi\nand biases bi given appropriate supervision.\nWe can perform an analogous parameterization in the func-\ntional divergence setting. By generalizing the piecewise\nlinear functions of Siahkamari et al, we can deﬁne a convex\ngenerating functional. The following theorem demonstrates\nthat every convex generating functional can be expressed\nin terms of linear functionals, thus justifying our choice of\nparameterization:\nTheorem 3.2. Let φ(p) be a convex generating functional\ncorresponding to a functional Bregman divergence Dφ.\nThen φ can be formulated as\nφ(p) =\nsup\n(w,bw)∈A\nZ\np(x)w(x)dx + bw,\nwhere A is a set of linear functionals in which each member\nis characterized by w and bw.\nFor our parameterization, we replace supremum with maxi-\nmum, and denote each function pair as (wc, bc) in a count-\nable set of functionals A. See Appendix A.2 and B for\nthe proof and details. In the case where p and q are dis-\ntributions, we may write this more succinctly as φ(p) =\nmax\n\u0000Ep[wc] + bc\n\u0001\n, where the expectation is taken with\nrespect to the subscript distribution p. Note that straight-\nforward application of the calculus of variations reveals\nthat the functional derivative of φ(q) is simply wq∗, where\nq∗= argmaxc[\nR\nq(x)wc(x)dx + bc]. Consequently, the\nfunctional Bregman divergence between p and q can be\nexpressed as Dφ(p, q) =\n\u0012 Z\np(x)wp∗(x)dx+bp∗\n\u0013\n−\n\u0012 Z\np(x)wq∗(x)dx+bq∗\n\u0013\n. (1)\nFor distributions, this is more succinctly Dφ(p, q) =\n(Ep[wp∗] + bp∗) −(Ep[wq∗] + bq∗).\nThis parameterization of the functional φ is now amenable\nto learning a functional divergence given data. In particular,\nwe now parameterize a divergence by the corresponding\nweight functions w1, ..., wK and biases b1, ..., bK. If we\nassume that each of these weight functions are given by deep\nneural networks, then it becomes natural to set up learning\nproblems where we aim to learn the underlying divergence\ngiven data. The resulting deep Bregman divergences will be\nshown to yield novel learning problems and strong empirical\nperformance on benchmark metric learning tasks. In the\nnext section we will detail our approach to extend deep\nmetric learning to this setting.\n4. Learning Problems and Applications\nIn the previous section, we saw in the symmetric setting\nhow different choices of the functions related to a func-\ntional Bregman divergence yield existing forms, as well as\nhow one may parameterize a general asymmetric functional\nBregman divergence using deep neural networks. Now we\nconnect the divergences discussed in the previous section\nto particular learning problems. In particular, we describe\nseveral novel applications and learning problems that arise\nfrom learning deep Bregman divergences.\n4.1. From Deep Metric Learning to Deep Divergence\nLearning\nConsider a learning problem where we aim to learn a deep\ndivergence given supervised data. As with deep metric learn-\ning, we will consider the case when p and q are empirical\ndistributions over single points x and y, respectively. We\nsaw in the previous section that we will parameterize our\ndeep divergence by weight functions w1, ..., wK and biases\nb1, ..., bK. To make things simpler, let us encompass all of\nthese weight functions into a single large neural network\nwith weights W. The network will have K different outputs,\none for each weight function. Many possible architectures\nare possible to capture this type of network; we consider an\narchitecture where several layers are shared in the network,\nand then the network branches into K subnetworks, each\nwith its own independent set of weights. See Figure 2 for\nthe network that we employ in our benchmark experiments.\nNow, suppose we pass x through the network. Each subnet-\nwork c produces a single output wc(x) + bc, and there are\nK total outputs, one per subnetwork. The index of the max-\nimum output is p∗. Similarly, pass y through the network;\nthe index of the maximum output across the K subnetworks\nis q∗. Then, by (1), the divergence is the difference be-\ntween the output of x at p∗and the output of x at q∗. For\ninstance, suppose that each of the K outputs corresponds to\na different class. Then the divergence will be zero if both\npoints achieve a maximum value for the same class (i.e.,\nDeep Divergence Learning\nInput image (x)\nConvolutional layers\nDense layers\nSplit into k \nsubnetworks\n1\n2\n...\nk\nk outputs\n...\n...\n       ...\nFigure 2. The general architecture we employ for deep Bregman\ndivergences on image data. For K functionals, we produce K\nseparate outputs, which are then used to compute the divergence\nover pairs of inputs.\nthey are both classiﬁed into the same class). The divergence\nis non-zero if the points are assigned to different classes,\nand the divergence grows as the two outputs become more\ndisparate.\nOne can now set up a divergence learning problem over\npairs or triples of points under this framework. Suppose we\nare given triples of points (x, y, z), where x should have a\nsmaller divergence to y than to z. One can easily apply exist-\ning loss deep metric learning loss functions—the triplet loss\nor contrastive loss are the two most common ones—with the\nlearned divergence in place of the usual squared Euclidean\ndistance. See the appendix for deﬁnitions of standard loss\nfunctions for deep metric learning. In experiments, we will\ncompare existing deep metric learning approaches to the\nmore general deep divergence learning problem considered\nhere, and we will see that we obtain gains over the existing\nmodels on standard benchmarks.\n4.2. Learning over Distributions\nA key advantage to our framework is that we need not re-\nstrict ourselves only to divergences between single points.\nAs we saw earlier, we can also capture divergences between\ndistributions of points that are similar to what is used for the\nMMD and the Wasserstein distance. Here we will discuss\napplications involving learning divergences over distribu-\ntions.\nData Generation. Consider the problem encoutered in\nmany GAN applications: we aim to learn a generator\nfor data such that we minimize some distributional diver-\ngence between the real and generated data distributions. In\nexisting GAN literature, divergences considered include\nthe Jensen-Shannon divergence (Goodfellow et al., 2014),\nMMD distance (Li et al., 2015; 2017), and the Wasserstein\ndistance (Arjovsky et al., 2017).\nUnder the deep divergence framework, rather than employ-\ning a ﬁxed divergence, we can learn one from data. In\nthis setting, we consider two distributions psynth and preal,\ncorresponding to distributions of generated and real data,\nrespectively. Assume that psynth is generated by passing\nrandomly-generated input data through a generator g, as is\nstandard with GAN models. As with GAN training, learn-\ning proceeds in an adversarial manner. We aim to learn a\ngenerator to minimize Dφ(psynth, preal), while simultane-\nously we aim to learn weights of the underlying network\nparameterizing Dφ to maximize Dφ(psynth, preal). As with\nGANs, we alternate between gradient updates for these two\nobjectives.\nWe note that, in practice, it is useful to restrict our attention\nto the case when K = 2, as it yields a particularly inter-\npretable model. In this case, we can think of one of the\ntwo subnetworks as outputting a larger value on real data,\nwhile the other subnetwork as outputting a larger value on\nsynthetic data. Thus, the network that parameterizes the di-\nvergence is analogous to the discriminator in a GAN model.\nWhen training the underlying weights of this network W,\nwe can take pairs or triples of real and synthetic data and\nutilize a triplet or contrastive loss to encourage the output on\nthe real data to be larger for one subnetwork and the output\non the synthetic data to be larger for the other subnetwork.\nSimilarly, when training the generator g, we use a loss that\nencourages real and synthetic data to both have the same\nmaximal output.\nSemi-Supervised Distributional Clustering. As another\napplication of learning divergences over distributions, con-\nsider a scenario where instead of clustering a set of data\npoints, we aim to cluster a set of distributions. In this setup,\neach distribution may correspond to an empirical distribu-\ntion over a set of points—for instance, we may have a distri-\nbution of ratings for each item in an online store. The goal is:\ngiven a set of such distributions, to cluster the distributions\ntogether into a set of clusters.\nDavis & Dhillon (2006) considered a version of this prob-\nlem where each distribution was given by a multivariate\nGaussian. Since the KL-divergence between multivariate\nGaussians is itself a Bregman divergence, one can use prop-\nerties of Bregman divergences to generalize the k-means\nalgorithm to this setting. Here, we will consider a version\nof this problem that is both semi-supervised (so pairs of\ndistributions that should or should not be clustered together\nare provided over a training set), and does not assume that\neach distribution is a multivariate Gaussian. Our approach\nalso removes the implicit assumption that the means of the\ndistributions are linearly separable for each cluster.\nAnalogous to Davis and Dhillon, given a functional Breg-\nman divergence deﬁned over distributions, one can apply a\ngeneralization of k-means to cluster the distributions. As\nshown by Frigyik et al. (2008), the mean minimizes the\nexpected functional Bregman divergence over a set of dis-\ntributions, analogous to the ﬁnite-dimensional case. Thus,\nk-means can be generalized to a setting where the squared\nEuclidean distance between vectors is replaced by the corre-\nDeep Divergence Learning\nMetrics\nBaseline Method\nOur Method\nDavid &\nDhillon\nTriplet\nContrastive\nTriplet\nContrastive\nRI\nMean\n0.638\n0.639\n0.997\n0.999\n0.550\nStd\n0.005\n0.005\n0.003\n0.003\n0.009\nARI\nMean\n0.197\n0.198\n0.993\n0.997\n0.005\nStd\n0.012\n0.013\n0.007\n0.006\n0.012\nTable 2. Rand index and adjusted rand index scores for different\nclustering experiments, where the baseline method treats each\ntraining point independently.\nsponding functional Bregman divergence over distributions.\nIf we represent each distribution by an empirical distribution\nover its underlying points, we can easily compute a param-\neterized functional Bregman divergence between pairs of\ndistributions. In our experiments, we will consider in par-\nticular learning a symmetric divergence on supervised data\nusing the moment-matching distance with a contrastive or\ntriplet loss. Then, once we have learned the divergence\nfrom data, we replace the squared Euclidean distance in the\nk-means algorithm with the learned divergence to directly\ncluster data in the test set.\n5. Experimental Results\nWe now empirically compare our proposed deep divergence\nframework to existing models. Due to space considerations,\nsome further details and results are available in the supple-\nmentary material.\n5.1. Clustering\nTo begin, we consider a simple demonstration of the ad-\nvantages of our approach on synthetic data for the semi-\nsupervised distributional clustering problem. We generated\nn = 500 training points, each assigned to one of three\nclusters. Each data point is represented by a multivariate\nGaussian; the means of these Gaussians were uniformly\nsampled over rings of radius .2, .6, and 1 plus Gaussian\nnoise, depending on the cluster identity, and the covariance\nof each Gaussian was .1 times the identity. See Figure 3 for\na plot of sampled means, along with data after generating\nfrom these Gaussians. We also generated n = 200 test\npoints in the same manner.\nWe compare three approaches to cluster the data. Our ﬁrst\nbaseline is the method of Davis & Dhillon (2006), which is\nan unsupervised clustering algorithm designed speciﬁcally\nto cluster multivariate Gaussian distributions. The second\nbaseline applies deep metric learning on all generated points\nfrom all the Gaussians; we apply contrastive and triplet\nlosses separately and learn a 3-layer multilayer perceptron\n(MLP) over the data in each case. The number of units in\neach layer were set to 1000, 500, and 2, and standard ReLU\nactivation was used. The third approach is our method; we\napply the (empirical) moment-matching function from the\nsymmetric setting, treating each distribution as its own data\npoint, in conjunction with a contrastive and triplet losses to\nlearn a 3-layer MLP with the same settings as the baseline\nMLP. On the test set, we use the learned divergence in place\nof the squared Euclidean distance in a k-means algorithm\nfor both the second and third method.\nWe compute the rand index and adjusted rand index scores\non the test set in each case, averaged over 10 runs for each\nof the three methods. The results are given in Table 2. The\nDavis & Dhillon method cannot cluster the multivariate\nGaussians, as their method is restricted to linear separability\nof the means. The baseline deep metric learning method\nfails due to the overlap of the generated data across clusters,\nwhereas the distributional divergence approach is able to\nperfectly cluster the test data in most runs. We can also\nvisualize the embeddings learned by the second and third\nmethod, where we see that our learned embeddings capture\nthe correct cluster structure, as pictured in Figure 3.\nFurther experiments were performed on real datasets, the\nresults of which are enumerated in Appendix C.\n5.2. Deep Metric Learning Comparisons\nNext we consider comparisons between our general deep\ndivergence learning framework and existing deep metric\nlearning models on standard benchmarks, to demonstrate\nthat our approach’s ﬂexibility yields improved performance\non several datasets and tasks.\nWe compare standard deep metric learning approaches to\nour proposed approach on the four benchmark datasets used\nin the original triplet loss paper (Hoffer & Ailon, 2015)—\nMNIST, Cifar10, SVHN, and STL10—as well as Fashion\nMNIST. We use the same basic architecture for the deep\nBregman divergence network as shown in Figure 2; for the\nEuclidean case we do not employ separate subnetworks\nin the dense layers. We treat several architecture choices\nas hyperparameters and validate over these hyperparame-\nters using Bayesian optimization (tuned separately for each\ndataset); Table 4 lists the hyperparameters that we search\nDatasets\nEuclidean\nDeep Bregman\nTriplet\nContrastive\nTriplet\nContrastive\nMNIST\n99.50\n99.63\n99.61\n99.56\nFashion MNIST\n93.24\n93.57\n94.90\n94.00\nSVHN\n92.58\n94.88\n94.03\n94.12\nCifar10\n77.00\n79.40\n81.40\n80.80\nSTL10\n59.97\n63.10\n62.64\n60.91\nTable 3. K-nn classiﬁcation accuracy results on the given datasets\n(without data augmentation or using learned features). The bold\nvalues indicate the best triplet loss (Bregman versus Euclidean)\nand contrastive loss (Bregman versus Euclidean) results.\nDeep Divergence Learning\n1.0\n0.5\n0.0\n0.5\n1.0\nx\n1.0\n0.5\n0.0\n0.5\n1.0\ny\n2\n1\n0\n1\n2\nx\n2.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\ny\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nx\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\ny\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx\n0.4\n0.2\n0.0\n0.2\n0.4\ny\nFigure 3. (Left) Plot of the means of the n = 500 Gaussian distributions, color-coded by cluster identity. (Middle left) Plot of data after\ngenerating 50 points from each Gaussian. (Middle right) Embedding learned by our method using contrastive loss with a moment-matching\nfunction. (Right) Embedding learned by the baseline deep learning approach.\nModel hyperparams\nTraining hyperparams\nlayers\n2 - 5\nmargin\n0.1 - 2.0\nconv ﬁlters\n16 - 128\nepochs\n10 - 40\nconv kernels\n3 - 9\nlearning rate\n10−5 - 10−1\nconv biases\nT / F\nbatch size\n32-128\npoolings\nT / F\noptimizer\nadam / sgd / rms\nbatchnorms\nT / F\nK in k-nn\n5 - 10\ndense units\n50 - 300\nnormalization\nT / F\nTable 4. Hyperparameter intervals used for tuning. First 100 itera-\ntions are used to narrow down the space, then 200 more iterations\nare run for each benchmark. T: True, F: False.\nover, along with the ranges of values considered.\nWe consider separately both triplet loss and contrastive loss,\nand report in bold the best values for each loss. For the\ntriplet loss, we consider all triplets in a batch when com-\nputing the loss. We perform no data augmentation. Results\nare shown in Table 3, where we see small but signiﬁcant\ngains in classiﬁcation accuracy for the Bregman method as\ncompared to the standard deep metric learning approach,\nparticularly in the triplet loss case. On Fashion MNIST, we\noutperform the current state-of-the-art for no data augmen-\ntation (94.23% from Assunc¸ao et al. (2018)), even though\nwe are not directly training a classiﬁer. We also note that we\nwould expect further gains in performance with more sophis-\nticated architectures (e.g., ResNets and other more recent\narchitectures), perhaps yielding near state-of-the-art perfor-\nmance on more datasets; however, the main goal of this\ncomparison is not to achieve state-of-the-art performance\nbut rather to present a fair comparison between the Bregman\nand Euclidean approaches on standard benchmarks.\n5.3. Unsupervised Data Generation\nFinally, we consider some qualitative results where we show\nthat our approach can be used for generating data with sim-\nilar performance to GANs. We consider the problem dis-\ncussed earlier, namely where we train a deep divergence\nmodel to minimize a learned divergence between real and\nFigure 4. Real and generated sample batches from CelebA (Top\nrow) and MNIST (Bottom row) datasets.\nsynthetic data. We apply our approach on 28x28 MNIST\nand CELEBA datasets, as is standard for GAN applications.\nWe adjust the strides to adapt the network for different input\nsizes. We keep model structures close to standard in order to\nshow the effectiveness of the divergence formula we intro-\nduced. We use a generator consisting of 4 deconvolutional\nlayers and a discriminator (i.e., the network parameterizing\nthe deep Bregman divergence) with 4 convolutional layers,\nwith a dropout rate of 0.5 in between the layers as well as\nlrelu and tanh activations. In the discriminator network, the\nconvolutional layers are followed by two 2-layer subnet-\nworks (again similar to Figure 2, where K = 2 in this case).\nFor the discriminator, we use the contrastive loss with a\nmargin of 0.4, whereas the generator directly attempts to\nminimize deep Bregman divergence between the real and\ngenerated images. More hyperparameter details are given\nin the appendix.\nSome randomly chosen results are presented in Figure 4,\nwhere we see that the distribution divergence learned by\nour method is able to generate realistic-looking images with\nno labeled supervision. We note that further theoretical\nanalysis and experimentation of these methods is required\nto determine situations where our loss functions may be\nmore desirable than existing GAN approaches.\nDeep Divergence Learning\n6. Conclusions\nIn this paper, we examined a novel generalization of both\nBregman divergence learning and deep metric learning,\nwhich we call deep divergence learning. This framework\noffers several appealing advantages: it uniﬁes a number of\nexisting ideas in metric learning under a single framework,\nit suggests a way to extend deep metric learning beyond the\nEuclidean setting, and it naturally yields learning problems\ninvolving divergences over distributions. Empirically we\nhave seen advantages of our approach compared to existing\ndeep metric learning methods.\nReferences\nArjovsky, M., Chintala, S., and Bottou, L. Wasserstein\nGAN, 2017. arXiv:1701.07875.\nAssunc¸ao, F., Lourenc¸o, N., Machado, P., and Ribeiro, B.\nDenser: Deep evolutionary network structured represen-\ntation. arXiv preprint arXiv:1801.01563, 2018.\nBanerjee, A., Merugu, S., Dhillon, I. S., and Ghosh, J. Clus-\ntering with Bregman divergences. Journal of Machine\nLearning Research, 6:1705–1749, 2005.\nBanos, O., Garcia, R., Holgado-Terriza, J. A., Damas, M.,\nPomares, H., Rojas, I., Saez, A., and Villalonga, C.\nmhealthdroid: a novel framework for agile development\nof mobile health applications. In International workshop\non ambient assisted living, pp. 91–98. Springer, 2014.\nBanos, O., Villalonga, C., Garcia, R., Saez, A., Damas,\nM., Holgado-Terriza, J. A., Lee, S., Pomares, H., and\nRojas, I. Design, implementation and validation of a\nnovel open framework for agile development of mobile\nhealth applications. Biomedical engineering online, 14\n(2):S6, 2015.\nBauschke, H. H. and Borwein, J. M. Joint and separate con-\nvexity of the Bregman distance. Studies in Computational\nMathematics, 8:23–36, 2001.\nBellet, A. and Habrard, A. Robustness and generalization for\nmetric learning. Neurocomputing, 151:259–267, 2015.\nBellet, A., Habrard, A., and Sebban, M. Metric learning.\nSynthesis Lectures on Artiﬁcial Intelligence and Machine\nLearning, 9(1):1–151, 2015.\nBregman, L. M. The relxation method of ﬁnding the com-\nmon points of convex sets and its application to the so-\nlution of problems in convex programming. USSR Com-\nputational Mathematics and Mathematical Physics, 7(3):\n200–217, 1967.\nBruno, B., Mastrogiovanni, F., and Sgorbissa, A. A pub-\nlic domain dataset for adl recognition using wrist-placed\naccelerometers. In the 23rd IEEE International Sympo-\nsium on Robot and Human Interactive Communication,\npp. 738–743. IEEE, 2014.\nCakir, F., He, K., Xide, X., Kulis, B., and Sclaroff, S. Deep\nmetric learning to rank. In Computer Visiona and Pattern\nRecognition, 2019.\nCao, Q., Guo, Z.-C., and Ying, Y. Generalization bounds for\nmetric and similarity learning. Machine Learning, 102\n(1):115–132, 2016.\nChatpatanasiri, R., Korsrilabutr, T., Tangchanachaianan, P.,\nand Kijsirikul, B. A new kernelization framework for Ma-\nhalanobis distance learning algorithms. Neurocomputing,\n73(10–12):1570–1579, 2010.\nChopra, S., Hadsell, R., and LeCun, Y. Learning a similarity\nmetric discriminatively, with application to face veriﬁca-\ntion. In Proc. IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2005.\nDavis, J. and Dhillon, I. S. Differential entropic cluster-\ning of multivariate Gaussians. In Advances in Neural\nInformation Processing Systems (NIPS), 2006.\nDavis, J., Kulis, B., Jain, P., Sra, S., and Dhillon, I.\nInformation-theoretic metric learning. In Proc. 24th In-\nternational Conference on Machine Learning (ICML),\n2007.\nFr´echet, M. Sur les ensembles de fonctions et les op´erations\nlin´eaires. CR Acad. Sci. Paris, 144:1414–1416, 1907.\nFrigyik, B. A., Srivastava, S., and Gupta, M. R. Functional\nBregman divergences and Bayesian estimation of distri-\nbutions. IEEE Transactions on Information Theory, 54\n(11):5130–5139, 2008.\nGelfand, I. M., Silverman, R. A., et al. Calculus of varia-\ntions. Courier Corporation, 2000.\nGierz, G. Integral representations of linear functionals on\nfunction modules. The Rocky Mountain Journal of Math-\nematics, pp. 545–554, 1987.\nGloberson, A. and Roweis, S. Metric learning by collapsing\nclasses. In Advances in Neural Information Processing\nSystems (NIPS), 2005.\nGoldberger, J., Roweis, S., Hinton, G., and Salakhutdinov,\nR. Neighbourhood components analysis. In Advances in\nNeural Information Processing Systems (NIPS), 2004.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.\nGenerative adversarial networks. In Advances in Neural\nInformation Processing Systems (NIPS), 2014.\nDeep Divergence Learning\nGretton, A. A Kernel Two-Sample Test. Journal of Machine\nLearning Research, 13:723–773, 2012.\nHermans, A., Beyer, L., and Leibe, B. In Defense of the\nTriplet Loss for Person Re-Identiﬁcation. arXiv preprint\narXiv:1703.07737, 2017.\nHoffer, E. and Ailon, N. Deep metric learning using triplet\nnetwork. In International Workshop on Similarity-Based\nPattern Recognition, pp. 84–92. Springer, 2015.\nJain, P., Kulis, B., Dhillon, I., and Grauman, K. Online\nmetric learning and fast similarity search. In Advances in\nNeural Information Processing Systems (NIPS), 2008.\nJain, P., Kulis, B., Davis, J., and Dhillon, I. Metric and\nkernel learning using a linear transformation. Journal of\nMachine Learning Research, 13:519–547, 2012.\nKulis, B. Metric learning: A survey. Foundations and\nTrends R⃝in Machine Learning, 5(4):287–364, 2013.\nKulis, B., Sustik, M., and Dhillon, I. Low-rank kernel\nlearning with Bregman matrix divergences. Journal of\nMachine Learning Research, 10:341–376, 2009.\nLi, C., Chang, W., Cheng, Y., Yang, Y., and Poczos, B.\nMMD-GAN: Towards deeper understanding of moment\nmatching network. In Neural Information Processing\nSystems, 2017.\nLi, Y., Swersky, K., and Zemel, R. Generative moment\nmatching networks. In International Conference on Ma-\nchine Learning, 2015.\nShalev-Shwartz, S., Singer, Y., and Ng, A. Y. Online and\nbatch learning of pseudo-metrics. In Proceedings of the\ntwenty-ﬁrst international conference on Machine learn-\ning, pp. 94. ACM, 2004.\nSiahkamari, A., Saligrama, V., Castanon, D., and Kulis, B.\nLearning Bregman divergences, 2019. arXiv:1905.11545.\nWang, J., Zhou, F., Wen, S., Liu, X., and Lin, Y. Deep metric\nlearning with angular loss. In International Conference\non Computer Vision, 2017.\nWeinberger, K. Q. and Saul, L. K. Distance metric learning\nfor large margin nearest neighbor classiﬁcation. Journal\nof Machine Learning Research, 10:207–244, 2009.\nWeiss, G. M., Yoneda, K., and Hayajneh, T. Smartphone\nand smartwatch-based biometrics using activities of daily\nliving. IEEE Access, 7:133190–133202, 2019.\nWu, L., Jin, R., Hoi, S. C., Zhu, J., and Yu, N. Learning\nBregman distance functions and its application for semi-\nsupervised clustering. In Advances in neural information\nprocessing systems, pp. 2089–2097, 2009.\nXing, E. P., Jordan, M. I., Russell, S. J., and Ng, A. Y.\nDistance metric learning with application to clustering\nwith side-information. In Advances in neural information\nprocessing systems, pp. 521–528, 2003.\nAppendices\nA. Notation and Deﬁnitions\nIn this section, we ﬁrst brieﬂy deﬁne triplet and contrastive\nlosses used in the main paper. Then, we introduce basic\nconcepts from functional analysis and the notation used for\nextending vector spaces to function spaces, which will be\nused for our proofs.\nA.1. Deﬁnitions of contrastive and triplet losses\nAs a reminder for the reader, we provide the deﬁnitions of\ncontrastive and triplet losses. The idea behind these losses\nis to enforce a small distance between similar inputs, and a\nlarge distance for dissimilar inputs. The Euclidean distance\nis utilized as the distance measure, denoted by d.\nContrastive loss. The contrastive loss takes an input pair,\nxa and xb, together with a relationship label y that takes the\nvalue of 1 if the inputs are similar and 0 otherwise. The loss\nfunction for a single xa, xb pair is:\nL(xa, xb) = yd(xa, xb)+(1−y) max{m−d(xa, xb), 0}2,\nwhere m is a margin value to separate dissimilar samples,\nchosen as a hyperparameter.\nTriplet loss. The triplet loss takes an input triplet xa.xp, xn,\nwhere the anchor xa has one similar input xp and one dis-\nsimilar input xn. The loss function for a single triplet is:\nL(xa, xp, xn) = max{d(xp, xa) −d(xn, xa) + m, 0},\nwhere m is again a margin value to separate relative dis-\ntances of the similar and dissimilar pairs.\nTypically, the distance measure d in both loss functions is\nthe Euclidean distance; however, for our loss functions, we\nreplace the distance measure by our learned deep Bregman\ndivergence.\nA.2. Assumptions and deﬁnitions from functional\nanalysis\nWe ﬁrst present basic notation from functional analysis,\nsince we extend vector spaces to function spaces to derive\nthis formulation.\nAssume we have a ﬁnite measure space (χ, Σ, µ) which is\nLebesgue-measurable, and χ ∈Rd. Note that we mainly\nDeep Divergence Learning\nconsider a set of distributions in this paper, which is a special\ncase that uses a Radon measure and a bounded Borel set,\nbut we continue with the more general case. Consider a set\nof measurable functions F ⊆Lp, deﬁned as F = {f ∈\nF | f : χ →R, ||f||p ≤C1 < ∞and f ≥0}, where\nC1 is a constant and 1 ≤p ≤∞. The restriction that\nf ≥0 is not limiting, since it can be easily satisﬁed by\nusing its equivalence class obtained by only applying an\nafﬁne transformation (Frigyik et al., 2008).\nAssume W ⊆Lp is a compact set of functions. All linear\nfunctionals have a continuous integral representation with\nrespect to our focus of measure space (Gierz, 1987), with\na corresponding function w ∈W, w : χ →R. Similarly,\nwe can characterize afﬁne functionals by their function and\nconstant pairs A = {(w, bw) | w ∈W, bw ∈R and |bw| ≤\nC2}, with C2 a constant.\nFor a convex functional φ, we denote its Fr´echet deriva-\ntive as δφ(p) and the epigraph of φ as epi φ; with their\ndeﬁnitions brieﬂy given below (Gelfand et al., 2000) :\nFr´echet derivative of φ. If for every h ∈W, there exists\nδφ(f) s.t.\nlim\n||h||p→0\nφ(f + h) −φ(f) −δφ(f)[h]\n||h||p\n= 0,\nthen φ(f) is Fr´echet differentiable and δφ(f) is the Fr´echet\nderivative of φ at f.\nDirectional Fr´echet derivative of φ. The derivative of a\nfunctional φ at f in the direction of a function g is deﬁned\nas:\nδφ[f; g] =\nZ\nδφ(f)(x)g(x)dx.\nEpigraph of φ. The epigraph of a functional φ is deﬁned\nas:\nepi φ := {(f, c) ∈F × R | φ(f) ≤c}.\nB. Proof of Theorem 3.1\nProof. To prove the result, we can generalize a known\nsymmetry result for standard Bregman divergences seen in\nBauschke & Borwein, Lemma 3.16 (Bauschke & Borwein,\n2001), or this Mathematics Stack Exchange discussion2.\nWe start by establishing that any symmetric functional Breg-\nman divergence has the form given in the statement of the\ntheorem. Let 0f be the zero-function (given, for example\nby the function p −p for any p). We can assume without\nloss of generality that φ(0f) = 0 and δφ(0f) = 0—we can\nalways add a constant to φ to ensure the ﬁrst property, and\n2https://math.stackexchange.com/questions/2242980/bregman-\ndivergence-symmetric-iff-function-is-quadratic\nwe can subtract\nR\np(x)δφ(0f)dx from φ to ensure the sec-\nond property, both without changing the resulting Bregman\ndivergence.\nNext, if Dφ(p, q) = Dφ(q, p) for all p, q, then writing out\nthe Bregman divergences and equating them yields\nφ(p) −φ(q) −\nZ\n(p(x) −q(x))δφ(q)(x)dx\n= φ(q) −φ(p) −\nZ\n(q(x) −p(x))δφ(p)(x)dx.\n(2)\nLetting p = 0f and simplifying the above equation (and\nusing φ(0f) = 0 and δφ(0f) = 0), we obtain the following:\n2φ(q) =\nZ\nq(x)δφ(q)(x)dx.\nNote that this equation holds for any q. Plugging this equa-\ntion (along with the same equation where p has replaced q)\ninto (2), we obtain the following identity:\nZ\np(x)δφ(q)(x)dx =\nZ\nq(x)δφ(p)(x)dx.\n(3)\nThis can be used to establish that δφ is linear. For example,\nto establish that δφ is homogeneous, we must show that\nδφ(αp) = αδφ(p), for non-zero α. Using (3) twice (ﬁrst\nand third line), we can establish the following for any p and\nq:\nZ\nq(z)δφ(αp)(z)dz\n=\nZ\nαp(z)δφ(q)(z)dz\n=\nα\nZ\np(z)δφ(q)(z)dz\n=\nα\nZ\nq(z)δφ(p)(z)dz.\nThis can then be used to show that δφ(αp) = αδφ(p): for\nany point x, suppose p is a Dirac delta function at x. Then\nthe above equation establishes that δφ(αp) equals αδφ(p)\nat x. Since the equation is true for all p, then δφ(αp) equals\nαδφ(p) for all points.\nA similar argument can be used to establish that δφ(p+q) =\nδφ(p) + δφ(q). In particular,\nR\nr(z)δφ(p + q)(z)dz\n=\nZ\n(p(z) + q(z))δφ(r)(z)\n=\nZ\np(z)δφ(r)(z) +\nZ\nq(z)δφ(r)(z)dz\n=\nZ\nr(z)δφ(p)(z)dz +\nZ\nr(z)δφ(q)(z)dz\nfor all r, establishes that δφ(p + q) = δφ(p) + δφ(q) and\nchoosing r as Dirac delta functions ensures this equality for\nall points.\nDeep Divergence Learning\nIn the case of functions, if a gradient function δφ is linear,\nthen the function φ must be quadratic; this is because we\ntake an anti-derivative of a linear function and obtain a\nquadratic function. In the functional case, this means that φ\nmust have the following form:\nφ(p) =\nZZ\np(x)p(y)ψ(x, y)dxdy,\nwhere ψ is a symmetric, positive semi-deﬁnite function.\n(In the vector setting, φ(x) = xT Ax for a positive semi-\ndeﬁnite matrix A, so this is a generalization to the functional\nsetting.) One can verify that the gradient δφ is of the form\nδφ(p)(y) = 2\nZ\np(x)ψ(x, y)dx,\nwhich is indeed a linear function. Given this form for φ, the\nﬁnal step is to plug φ into the deﬁnition for the functional\ndivergence and to simplify the resulting divergence. After\nsimpliﬁcation using the deﬁnition of φ and its derivative,\nalong with the fact that ψ(x, y) = ψ(y, x), we obtain\nDφ(p, q) =\nZZ\n(p(x) −q(x))(p(y) −q(y))ψ(x, y)dxdy.\nNow that we have established one direction of the theorem,\nwe can establish the other. This direction is considerably\nsimpler. We must show that a divergence that has the form\nDφ(p, q) =\nZZ\n(p(x) −q(x))(p(y) −q(y))ψ(x, y)dxdy\nis in fact a symmetric functional Bregman divergence. The\nfact that it is symmetric follows directly. The fact that\nit is a functional Bregman divergence follows from the\nfact that choosing the strictly convex functional φ(p) =\nRR\np(x)p(y)ψ(x, y)dxdy yields the resulting divergence.\nC. Proof of Theorem 3.2\nIn this section, we show that our convex generating func-\ntional form is justiﬁed in that any convex functional can be\nrepresented as a supremum over linear functionals.\nUp to this point, we notated convex functionals as φ(p), in\nterms of only their input functions. Here we will use the\nnotation φ(x; p(x)) for convex functionals, where x refers\nto the input of the function p.\nProof. (⊇) We ﬁrst show that the right hand side is indeed\na convex functional.\nWe will use the standard deﬁnition of convexity since it\ndirectly extends to the functional case. The domain of the\nfunctionals is a convex subset since for all λ ∈[0, 1], and\np, q ∈Lp, ||λp + (1 −λq)||p < ∞, so λp + (1 −λq) ∈Lp\nnaturally.\nFor an arbitrary pair (w, bw), and p, q ∈F we have:\nZ\n(λp(x) + (1 −λ)q(x))w(x)dx + bw ≤\nλ\n\u0014 Z\np(x)w(x)dx + bw\n\u0015\n+ (1 −λ)\n\u0014 Z\nq(x)w(x)dx + bw\n\u0015\n= λφp(x; p(x)) + (1 −λ)φq(x; q(x))\n≤λφ∗(x; p(x)) + (1 −λ)φ∗(x; q(x)),\nwhere φ∗represents the supremum attained for the right-\nhand side of (3.2), and φa(b) =\nR\nb(x)a(x)dx + ba . Since\nthe inequalities hold for all (w, bw), we can take the sup of\nthe ﬁrst line and obtain:\nφ∗(λp + (1 −λ)q(x)) ≤λφ∗(x; p(x))\n+ (1 −λ)φ∗(x; q(x)).\n(⊆) We now show that for a given convex functional\nφ(x; p(x)), we can ﬁnd a set of afﬁne functionals to write it\nas (3.2).\nAssume δφ(x; p(x)) is the Frechet derivative of φ at func-\ntion p ∈W.\nThen δφ(x; p(x)) is a linear operator.\nSince φ is continuous and bounded, we can ﬁnd r(x) =\narg inf δφ(x; p(x))3. Deﬁne δ\n′\nφ(x; p(x)) := δφ(x; p(x)) +\nφ(x; r(x)) ≥0. This positive functional can be represented\nin an integral form by Riesz-Markov-Kakutani representa-\ntion theorem on the measure d(δ\n′\nφ(x)) (Fr´echet, 1907). Note\nthat we can always add or substract properly scaled constant\nterms and preserve the information since these transforma-\ntions are linear. For a given p′, applying Riesz theorem\ngives us the representation below:\nδφ(x; p′(x)) =\nZ\nδφ′(x; p′(x))p′(x)dx,\nwith a support function\nlφp′(x; p(x)) =\nZ\nδφ′(x; p′(x))p(x)dx\n+ φ(x; p′(x)) −δφ(x; p′(x)).\nWe also have lφp′(x; p(x)) ≤\nepi φ(x; p(x)) for all\np, p′ ∈W, since φ is a convex functional. lp′(x; p(x)) ≤\nφ(x; p(x)) for all p since we are on a compact domain and\nφ is continuous and convex. Now for a given convex func-\ntional φ(x; p(x)), consider S\np′∈W lφp′(x; p(x)) as a set of\nafﬁne functionals, further denoted by Lφ for convenience.\nDeﬁne:\nψ(x; p(x)) = sup\nl∈Lφ\nZ\nl(x)p(x)dx + bl.\n3r(x) also can be constructed from the ϵ-subdifferentials of φ\nto ensure existence.\nDeep Divergence Learning\n0\n25\n50\n75\n100\n125\n150\n175\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\nD loss\nD loss with moving average\n0\n25\n50\n75\n100\n125\n150\n175\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\nG loss\nG loss with moving average\nFigure 5. The discriminator (left) and generator (right) losses dur-\ning training for CelebA. Each epoch is split into 25 averaged batch\nlosses. The window size is 2 for the moving average.\nψ is a convex functional by the ﬁrst part of the proof.\nSince φ is convex, for all p ̸= p′ we have φ(x; p\n′(x)) −\nlφ(x; p\n′(x)) ≥0, so for an arbitrary p, ψ(x; p(x)) =\nlφ(x; p(x)). This concludes ψ(x; p(x)) forms a set of func-\ntionals to construct φ.\nNote that if we restrict our space to a set of n distributions,\nthen all we need to know is at most n corresponding maxi-\nmizing afﬁne functionals; in this case the supremum can be\nreplaced by the maximum as we did in the paper.\nD. Applications Details\nD.1. Additional GAN Model Details\nIn this section, we present more training details related to our\nGAN model. We use RMSprop optimizer with a momentum\nvalue of 0.99, and set the learning rates to 10−3 for the\ndiscriminator and 3×10−3 for the generator. The minibatch\nsize is chosen as 64. Our main model has convolutional\nlayers with kernel sizes equal to 5 and ﬁlter sizes equal to\n64. The strides are halved towards the ﬁnal layers. Stride\nsizes are determined based on the input image dimensions.\nIn our experiments, we incorporate contrastive loss into deep\nBregman learning in order to supervise the discriminator.\nHowever, our distributional loss formula has the potential\nto be directly used in the GAN setting, which we leave as a\nfuture work.\nWe provide the loss plots for the generator and the discrim-\ninator through the training phase below in Figure 5. We\nobserve that the discriminator ﬁrst learns the metric, then\nthe training preserves the balance between the two networks.\nWe note that image quality still improves for a while after\nthe losses become saturated, due to the nature of contrastive\nloss.\nD.2. Applications of Clustering on Sensor Data\nIn order to demonstrate the capabilities of our distributional\nclustering method on real data, we chose to experiment\nwith activity classiﬁcation using time-varying sensor data.\nThough many datasets are applicable, we chose to use\nDataset\nBaseline Method\nOur Method\nDavid &\nDhillon\nWHARF\nRI\nMean\n0.832\n0.887\n0.876\nStd\n0.002\n0.004\n0.007\nARI\nMean\n0.098\n0.364\n0.327\nStd\n0.006\n0.022\n0.026\nMHEALTH\nRI\nMean\n0.849\n0.860\n0.664\nStd\n0.005\n0.007\n0.006\nARI\nMean\n0.106\n0.149\n0.023\nStd\n0.008\n0.018\n0.001\nWISDM\nRI\nMean\n0.894\n0.907\n0.900\nStd\n0.004\n0.003\n0.003\nARI\nMean\n0.086\n0.127\n0.089\nStd\n0.005\n0.014\n0.009\nTable 5. Rand index and adjusted rand index scores for different\nclustering experiments performed on real data, where the baseline\nmethod treats each training point independently.\nWHARF (Bruno et al., 2014), MHEALTH (Banos et al.,\n2014; 2015), and WISDM (Weiss et al., 2019) in our initial\nexperiments. These datasets are collections of multimodal\nbody sensor recordings as test subjects perform different\nactivities of daily living (ADL), including but not limited to\nsitting, standing, eating, walking, and jogging.\nThe experimental setup is the same as in Section 5.1, where\nwe compute the rand index and adjusted rand index scores\non the test set in each experiment, averaged over 10 runs\nfor each of the three methods. Results are given in Table 5.\nWe note that only experiments using contrastive loss are\nreported here; though our distributional loss formula has the\npotential to be applied directly here, we leave this as future\nwork.\nAs in Section 5.1, we visualize embeddings learned by our\nmethod and the baseline method, where we see that our\nlearned embeddings capture the correct cluster structure, as\npictured in Figure 6.\nD.3. Additional K-nn Classiﬁcation Details\nHere we provide more details regarding our K-nn exper-\niments between deep Bregman and Euclidean cases. All\nfactors in our experimental settings are created by very stan-\ndard choices for a fair comparison. The batches are chosen\nrandomly from the relevant dataset, and then the pairs are\ncreated within that batch at each iteration. We use a vali-\ndation set ratio of 20%. Once the training is complete, we\nobtain test embeddings and run the K-nn algorithm on these\nembeddings.\nWe choose k, the number of subnetworks, to be equal to the\nnumber of classes. Additionally, we run a small experiment\nDeep Divergence Learning\n1\n0\n1\n2\n3\nx\n2.5\n2.0\n1.5\n1.0\n0.5\ny\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nx\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\ny\nFigure 6. (Left) Embedding of the WHARF dataset learned by our\nmethod using contrastive loss with a moment-matching function.\n(Right) Embedding learned by the baseline deep learning approach.\nk\n5\n20\n50\n100\n200\n500\n1000\nacc\n71.9\n77.8\n79.4\n80.0\n77.4\n74.1\n70.8\nTable 6. Accuracy on Cifar10 when varying the number of subnet-\nworks (k).\nover varying k from 5 to 1000 and reported the results in\nTable 6. The results indicate that performance improves to a\npoint, and then the model starts to overﬁt. This suggests that\nan optimal k can be found by adding it as a hyperparameter\nin the experiments.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-05-06",
  "updated": "2020-05-06"
}