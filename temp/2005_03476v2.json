{
  "id": "http://arxiv.org/abs/2005.03476v2",
  "title": "Brain-like approaches to unsupervised learning of hidden representations -- a comparative study",
  "authors": [
    "Naresh Balaji Ravichandran",
    "Anders Lansner",
    "Pawel Herman"
  ],
  "abstract": "Unsupervised learning of hidden representations has been one of the most\nvibrant research directions in machine learning in recent years. In this work\nwe study the brain-like Bayesian Confidence Propagating Neural Network (BCPNN)\nmodel, recently extended to extract sparse distributed high-dimensional\nrepresentations. The usefulness and class-dependent separability of the hidden\nrepresentations when trained on MNIST and Fashion-MNIST datasets is studied\nusing an external linear classifier and compared with other unsupervised\nlearning methods that include restricted Boltzmann machines and autoencoders.",
  "text": "Brain-like approaches to unsupervised learning\nof hidden representations - a comparative study⋆\nNaresh Balaji Ravichandran1, Anders Lansner1,2, and Pawel Herman1\n1 Computational Brain Science Lab, KTH Royal Institute of Technology,\nStockholm, Sweden\n2 Department of Mathematics, Stockholm University, Stockholm, Sweden\n{nbrav,ala,paherman}@kth.se\nAbstract. Unsupervised learning of hidden representations has been\none of the most vibrant research directions in machine learning in recent\nyears. In this work we study the brain-like Bayesian Conﬁdence Prop-\nagating Neural Network (BCPNN) model, recently extended to extract\nsparse distributed high-dimensional representations. The usefulness and\nclass-dependent separability of the hidden representations when trained\non MNIST and Fashion-MNIST datasets is studied using an external\nlinear classiﬁer and compared with other unsupervised learning methods\nthat include restricted Boltzmann machines and autoencoders.\nKeywords: Neural networks · Bio-inspired · Hebbian learning · Unsu-\npervised learning · Structural plasticity.\n1\nIntroduction\nArtiﬁcial neural networks have made remarkable progress in supervised pattern\nrecognition in recent years. In particular, deep neural networks have dominated\nthe ﬁeld largely due to their capability to discover hierarchies of hidden data\nrepresentations. However, most deep learning methods rely extensively on su-\npervised learning from labeled samples for extracting and tuning data represen-\ntations. Given the abundance of unlabeled data there is an urgent demand for\nunsupervised or semi-supervised approaches to learning of hidden representa-\ntions [1]. Although early concepts of greedy layer-wise pretraining allow for ex-\nploiting unlabeled data, ultimately the application of deep pre-trained networks\nto pattern recognition problems rests on label-dependent end-to-end weight ﬁne\ntuning [6]. At the same time, we observe a surge of interest in more brain plausi-\nble networks for unsupervised and semi-supervised learning problems that build\non some fundamental principles of neural information processing in the brain [8].\n⋆Funding for the work is received from the Swedish e-Science Research Centre (SeRC),\nEuropean Commission H2020 program, Grant Agreement No. 800999 (SAGE2), and\nGrant Agreement No. 801039 (EPiGRAM-HS). The simulations were performed on\nresources provided by Swedish National Infrastructure for Computing (SNIC) at the\nPDC Center for High Performance Computing, KTH Royal Institute of Technology.\narXiv:2005.03476v2  [cs.NE]  16 Apr 2021\n2\nN. B. Ravichandran et al.\nMost importantly, these brain-like computing approaches rely on local learning\nrules and label-independent biologically compatible mechanisms to build data\nrepresentations whereas deep learning methods predominantly make use of error\nback-propagation (backprop) for learning the weights. Although backprop as a\nlearning algorithm is highly eﬃcient for ﬁnding good representations from data,\nthere are several issues that make it an unlikely candidate model for synaptic\nplasticity in the brain. The most apparent issue is that the synaptic strength\nbetween two biological neurons is expected to comply with Hebb’s postulate,\ni.e. to depend only on the available local information provided by the activities\nof pre- and postsynaptic neurons. This is violated in backprop since synaptic\nweight updates need gradient signals to be communicated from distant output\nlayers. Please refer to [14] for a detailed review of possible biologically plausible\nimplementations of and alternatives to backprop.\nIn this work we utilize the MNIST and Fashion-MNIST datasets to compare\ntwo classical representation learning networks, the autoencoder (AE) and the\nrestricted Boltzmann machine (RBM), with two brain-like approaches to unsu-\npervised learning of hidden representations, i.e. the recently proposed model by\nKrotov and Hopﬁeld (KH) [9], and the BCPNN model [19]. In particular, we\nqualitatively examine the extracted hidden representations and quantify their\nclass-dependent separability using a simple linear classiﬁer on top of all the net-\nworks under investigation. This classiﬁcation step is not part of the learning\nstrategy, and we use it merely to evaluate the resulting representations.\nSpecial emphasis is on the feedforward BCPNN model with a single hidden\nlayer, which frames the update and learning steps of the neural network as prob-\nabilistic computations. BCPNN has previously been used in abstract models\nof associative memory [11, 21], action selection [3], and in application to brain\nimaging [2] and data mining [18]. Spiking versions of BCPNN with biologically\ndetailed Hebbian synaptic plasticity have also been developed to model diﬀerent\nforms of cortical associative memory [7, 15, 23]. BCPNN architecture comprises\nmany modules, referred to as hypercolumns (HCs), that in turn comprise a set\nof functional minicolumns (MCs) competing in a soft-winner-take-all manner.\nThe abstract view of a HC in this cortical-like network is that it represents some\nattribute, e.g. edge orientation, in a discrete coded manner. A minicolumn con-\nceptually represents one discrete value (a realization of the given attribute) and,\nas a biological parallel, it accounts for a local subnetwork of around a hundred\nrecurrently connected neurons with similar receptive ﬁeld properties [16]. Such\nan architecture was initially generalized from the primary visual cortex, but to-\nday has more support also from later experimental work and has been featured\nin spiking computational models of cortex [10, 20].\nFinally, in this work we highlight an additional mechanism called structural\nplasticity, introduced recently to the BCPNN framework [19], which enables self-\norganization and unsupervised learning of hidden representations. Structural\nplasticity learns a set of sparse connections while simultaneously learning the\nweights of the connections. This is in line with structural plasticity found in\nBrain-like approaches to unsupervised learning\n3\nthe brain, where there is continuous formation and removal of synapses in an\nactivity-dependent manner [4].\n2\nRelated Works\nA popular unsupervised learning approach is to train a hidden layer to reproduce\nthe input data as, for example, in AE and RBM. The AE and RBM networks\ntrained with a single hidden layer are relevant here since learning weights of\nthe input-to-hidden-layer connections relies on local gradients, and the repre-\nsentations can be stacked on top of each other to extract hierarchical features.\nHowever, stacked autoencoders and deep belief nets (stacked RBMs) have typ-\nically been used for pre-training procedures followed by end-to-end supervised\nﬁne-tuning (using backprop) [6]. The recently proposed KH model [9] addresses\nthe problem of learning solely with local gradients by learning hidden repre-\nsentations only using an unsupervised method. In this network, the input-to-\nhidden connections are trained and additional (non-plastic) lateral inhibition\nprovides competition within the hidden layer. For evaluating the representation,\nthe weights are ﬁxed, and a linear classiﬁer trained with labels is used for the\nﬁnal classiﬁcation. Our approach shares some common features with the KH\nmodel, e.g. learning hidden representations solely by unsupervised methods, and\nevaluating the representations by a separate classiﬁer ([8] provides an extensive\nreview of methods with similar goals).\nAll the aforementioned models employ either competition within the hidden\nlayer (KH), or feedback connections from hidden to input (RBM and AE). The\nBCPNN uses only the feedforward connections, along with an implicit compe-\ntition via a local softmax operation, the neural implementation of which would\nbe lateral inhibition within a hypercolumn.\nIt is also observed that, for unsupervised learning, having sparse connectiv-\nity in the feedforward connections performs better than full connectivity [8].\nEven networks employing supervised learning, like convolutional neural net-\nworks (CNNs), force a ﬁxed spatial ﬁlter to obtain this sparse connectivity.\nThe BCPNN model takes an alternative adaptive approach by using structural\nplasticity to obtain a sparse connectivity.\n3\nBayesian Conﬁdence Propagation Neural Network\nHere we describe the BCPNN network architecture and update rules [11, 19,\n21]. The feedforward BCPNN architecture contains two layers, referred to as the\ninput layer and hidden layer (Fig. 1A). A layer consists of a set of HCs, each of\nwhich represents a discrete random variable Xi (upper case). Each HC, in turn,\nis composed of a set of MCs representing a particular value xi (lower case) of Xi.\nThe probability of Xi is then a multinomial distribution, deﬁned as p(Xi = xi),\nsuch that P\nxi p(Xi = xi) = 1. In the neural network, the activity of the MC is\ninterpreted as p(Xi = xi), and the activities of all the MCs inside a HC sum to\none.\n4\nN. B. Ravichandran et al.\nSince the network is a probabilistic graphical model (see Fig. 1B), we can\ncompute the posterior of a target HC in the hidden layer conditioned on all\nthe source HCs in the input layer. We will use x’s and y’s to refer to the\nHCs in the input and hidden layers respectively. Computing the exact poste-\nrior p(Yj|X1, ..., XN) over the target HC is intractable, since it scales exponen-\ntially with the number of units. The naive Bayes assumption p(X1, .., XN|Yj) =\nQN\ni=1 p(Xi|Yj) allows us to write the posterior as follows:\np(Yj|X1, ..., XN) = p(Yj) QN\ni=1 p(Xi|Yj)\np(X1, ..., XN)\n∝p(Yj)\nN\nY\ni=1\np(Xi|Yj)\n(1)\nWhen the network is driven by input data {X1, .., XN} = {xD\n1 , .., xD\nN}, we\ncan write the posterior probabilities of a target MC in terms of the source MCs\nas:\np(yj|xD\n1 , ..., xD\nN) ∝p(yj)\nN\nY\ni=1\np(xD\ni |yj) = p(yj)\nN\nY\ni=1\nY\nxi\np(xi|yj)I(xi=xD\ni )\n(2)\nwhere I(·) is the indicator function that equals 1 if its argument is true, and\nzero otherwise. We have written the posterior of the target MC as a function of\nall the source MCs (all xi’s). The log posterior can be written as:\nlog p(yj|xD\n1 , ..., xD\nN) ∝log p(yj) +\nN\nX\ni=1\nX\nxi\nI(xi =xD\ni ) log p(xi|yj)\n(3)\nSince the posterior is linear in the indicator function of data sample, I(xi =\nxD\ni ) can be approximated by its expected value deﬁned as π(xi) = p(xi = xD\ni ).\nExcept for π(xi), all the terms in the posterior are functions of the marginals\np(yj) and p(xi, yj). We deﬁne the terms bias b(yj) = log p(yj) and weight\nw(xi, yj) = log\np(xi,yj)\np(xi)p(yj) in analogy with artiﬁcial neural networks. Note that\nin the weight term, we have added an additional p(xi) factor in the denominator\nto be consistent with previous BCPNN models [21, 23], but this will not aﬀect\nthe computation since all terms independent of yj will absorbed in the activity\nnormalization.\nThe inference step to calculate the posterior probabilities of the target MCs\nconditioned on the input sample is given by the activity update equations:\nh(yj) = b(yj) +\nN\nX\ni=1\nX\nxi\nπ(xi)w(xi, yj)\n(4)\nπ(yj) =\nexp h(yj)\nP\nk exp h(yk)\n(5)\nwhere h(yj) is the total input received by each target MC from which the pos-\nterior probability π(yj) = p(yj|xD\n1 , ..., xD\nN) is recovered by softmax normalization\nof all MCs within the HC.\nBrain-like approaches to unsupervised learning\n5\nA\nB\nFig. 1. Schematic of the BCPNN architecture with three input HCs and one hidden\nHC. A. The neural network model where the input HCs are binary and the hidden HC\nis multinomial (gray boxes). The MCs within the HC are the discrete values of the hid-\nden variable (open and shaded circles inside the box). B. The equivalent probabilistic\ngraphical model illustrating the generative process with each random variable (circle)\nrepresenting a HC. The input HCs are observable (shaded circle) and the hidden HC is\nlatent (open circle). The naive Bayes assumption renders the likelihood of generating\ninputs factorial.\nAs each data sample is presented, the learning step incrementally updates\nthe marginal probabilities, weights, and biases as follows:\nτp\ndp(xi)\ndt\n= π(xi) −p(xi)\n(6)\nτp\ndp(xi, yj)\ndt\n= π(xi) π(yj) −p(xi, yj)\n(7)\nτp\ndp(yj)\ndt\n= π(yj) −p(yj)\n(8)\nb(yj) = log p(yj)\n(9)\nw(xi, yj) = log\np(xi, yj)\np(xi) p(yj)\n(10)\nwhere the parameter τp is the learning time constant. The sets of Equations\n4-5 and Equations 6-10 deﬁne the activity and learning update equations of\nthe BCPNN architecture respectively. For the network with multiple input HCs\nand one hidden HC (as in Fig. 1A), the computation is equivalent to a mixture\nmodel with each hidden MC representing one mixture component. For learning\ndistributed representations from data, we can train multiple hidden HCs using\nthe same principles. The network for unsupervised representation learning re-\nquires, in addition to the above computation, structural plasticity for learning\na sparse set of connections from the input to hidden layer, which we discuss in\ndetail in the next section.\n6\nN. B. Ravichandran et al.\n3.1\nStructual plasticity\nStructural plasticity builds a sparse set of connections from the input to hidden\nlayer by iteratively improving from randomly initialized connections. We ﬁrst\ndeﬁne connections in terms of input HC to hidden HC, that is, when an input\nHC has an active connection to a hidden HC, all MCs within the input HC are\nconnected to all MCs within the hidden HC. The connections are formulated\nas a connectivity matrix M, where Mij = 1 when the connection from the ith\ninput HC to jth hidden HC is active, or Mij = 0 if silent3 (see Fig. 2). Each\nMij is initialized as active stochastically with probability pih, with pih being the\nhyperparameter that controls the connection density. Once initialized, the total\nnumber of active incoming connections to each hidden HC is ﬁxed whereas the\noutgoing connections from a source HC can be changed. For each connection,\nwe compute the mutual information I(Xi, Yj) between the ith input HC and jth\nhidden HC and normalize by the number of active outgoing connections from\nthe input HC to compute the score ˜I(Xi, Yj):\n˜I(Xi, Yj) =\nI(Xi, Yj)\n1 + P\nk Mik\n(11)\nSince the total number of active incoming connections is ﬁxed, each hidden\nHC greedily maximizes the sum of score ˜I(Xi, Yj) by silencing the active con-\nnection with the lowest score (change Mij from 1 to 0) and activating the silent\nconnection with the highest score (change Mij from 0 to 1), provided that lat-\nter’s score is higher than the former. We call this operation a ﬂip and use a\nhyperparameter nflips to set the number of ﬂips made per training epoch.\nFig. 2. Structural plasticity. The input layer contains N binary HCs, and the hid-\nden layer contains four HCs (gray boxes). The existence of a connection between an\ninput HC and hidden HC is shown as a blue line, i.e., Mij=1. Structural plasticity\ninvolves each hidden HCs silencing an active connection and activating another silent\nconnection. The red arrow shows one such ﬂip operation for the third hidden HC.\n3 In analogy with biological synapses that can be non-existing, silent, or active, we\nadopt the term ’silent’ for inactive connections.\nBrain-like approaches to unsupervised learning\n7\n4\nExperiments\nHere we ﬁrst describe the experimental setup for the BCPNN and three other\nrelated models for unsupervised learning as described in section 2. Next, we\nstudy qualitatively the hidden representations by examining the receptive ﬁelds\nformed by unsupervised learning. Finally, we provide quantitative evaluation of\nthe representations learnt by the four models using class-dependent classiﬁcation\nwith a linear classiﬁer.\n4.1\nData\nWe ran the experiments on the MNIST [12] and Fashion-MNIST [24] datasets.\nBoth datasets contain 60,000 training and 10,000 test samples of 28x28 grey-scale\nimages. The images were ﬂattened to 784 dimensions and the pixel intensities\nwere normalized to the range [0,1]. The images acted as the input layer for the\nmodels and the labels were not used for unsupervised learning.\n4.2\nModels\nWe considered four network architectures: BCPNN (c.f. section 3), AE, RBM\nand, KH. Each model had one hidden layer with 3000 hidden units.\nBCPNN The BCPNN network had an input layer with 784 HCs and 2 MCs\nper HC (pixel intensity was interpretted as probability of a binary variable) and\na hidden layer with nHC=30 and nMC=100. The learning time constant was set\nas τp = 60 to roughly match the training time for one epoch (for details, see\n[19]). The entire list of parameters and their values are listed in Table 1. The\nsimulations were performed on code parallelized using MPI on a cluster of 2.3\nGHz Xeon E5 processors and the training process took approximately ﬁfteen\nminutes per run.\nTable 1. BCPNN model parameters\nSymbol Value Descrption\nnepoch\n5\nNumber of epochs of unsupervised learning\nnHC\n30\nNumber of HCs in hidden layer\nnMC\n100\nNumber of MCs per HC in hidden layer\n∆t\n0.01\nTime-step\nτp\n60\nLearning time-constant\npih\n8%\nConnectivity from input to hidden layer\nnflips\n16\nNumber of ﬂips per epoch for structural plasticity\nµ\n10\nMean of poisson distribution for initializing MCs\n8\nN. B. Ravichandran et al.\nFig. 3. A. Histogram of weights from the input layer to hidden layer. The horizontal\naxis has the minimum to maximum value of the weights as the range, and the vertical\naxis is in log scale. B. Schematic of the four unsupervised learning models under com-\nparison and the supervised classiﬁer. The dotted lines imply we use the representations\nof the hidden layer as input for the classiﬁer.\nKH The KH network was reproduced from the original work using the code\nprovided by Krotov and Hopﬁeld [9]. We kept all the parameters as originally\ndescribed, except for having 3000 hidden units instead of 2000, to be consistent\nin the comparison with other models.\nRBM For the RBM network, we used sigmoidal units for both input and hidden\nlayer. The weights were trained using the Contrastive Divergence algorithm with\none iteration of Gibbs sampling (CD-1). The learning rate α was set as 0.01 and\nthe training was done in minibatches of 256 samples for 300 epochs.\nAE For the AE network, we used sigmoidal units for both hidden layer and\nreconstruction layer and two sets of weights, one for encoding from input to\nhidden layer and another for decoding from hidden to reconstruction layer. The\nweights were trained using the Adam optimizer and L2 reconstruction loss with\nan additional L1 sparsity loss on the hidden layer. The sparsity loss coeﬃcient\nwas determined by maximizing the accuracy of a held-out validation set of 10000\nsamples and set as λ=1e-7 for MNIST and λ=1e-5 for Fashion-MNIST. The\ntraining was in minibatches of 256 samples for 300 epochs.\n4.3\nReceptive ﬁeld comparison\nAs can be observed in Fig. 3A, the distribution of weight values when trained on\ntrained on MNIST considerably diﬀers across the networks. It appears that the\nrange of values for BCPNN corresponds to that reported for AE, whereas for KH\nand RBM, weights lie in a far narrower interval centered around 0. Importantly,\nBrain-like approaches to unsupervised learning\n9\nBCPNN has by far the highest proportion of zero weights (90%), which renders\nthe connectivity truly sparse.\nFig. 4. Receptive ﬁelds of diﬀerent unsupervised learning methods. For each model, the\npositive and negative values are normalized, such that blue, white, and red represent\nthe lowest, zero, and highest value of weights. A. BCPNN: Each row corresponds to\na randomly chosen HC and the constituent MCs of BCPNN. First column shows the\nreceptive ﬁeld of HC (black means Mij=1). The remaining columns show the receptive\nﬁeld of nine randomly chosen MCs out of 100 MCs within the HC. B. KH, C. RBM,\nD. AE: Receptive ﬁelds of 60 randomly chosen hidden units out of 3000.\nIn Fig. 4, we visualize the receptive ﬁelds of the four unsupervised learning\nnetworks trained on the MNIST dataset. Firstly, it is straightforward to see that\nthe receptive ﬁelds of all the networks diﬀer signiﬁcantly. The RBM (Fig. 4C)\nand AE (Fig. 4D) have receptive ﬁelds that are highly localized and span the\ninput space, a characteristic of distributed representations. The KH model (Fig.\n4B) has receptive ﬁelds that resemble the entire image, showing both positive\nand negative values over the image, as a result of Hebbian and anti-Hebbian\nlearning [9]. Generally, local representations like mixture models and competitive\nlearning, as opposed to distributed representations, tend to have receptive ﬁelds\nthat resemble prototypical samples. With this distinction in mind, the receptive\nﬁelds in the BCPNN should be closely examined (Fig. 4A). The receptive ﬁelds of\nHCs (ﬁrst column) are localized and span the input space, much like a distributed\nrepresentation. Within each HC however, the MCs have receptive ﬁelds (each\n10\nN. B. Ravichandran et al.\nrow) resembling prototypical samples, like diverse sets of lines and strokes. This\nsuggests that the BCPNN representations are ”hybrid”, with the higher-level\nHCs coding a distributed representation, and the lower level MCs coding a local\nrepresentation.\n4.4\nClassiﬁcation performance\nFor all the four models of unsupervised learning, we employed the same linear\nclassiﬁer for predicting the labels (see Fig. 3B). This allowed us to consistently\nevaluate the representations learned by the diﬀerent models. The linear classiﬁer\nconsiders the hidden layer representations as the input and the labels as the out-\nput. The output layer consists of softmax units for the 10 labels. The classiﬁer’s\nweights were trained by stochastic gradient descent with the Adam optimizer\nusing cross-entropy loss function. The training procedure used minibatches of\n256 samples and a total of 300 training epochs.\nTable 2. Accuracy comparison for MNIST and Fashion-MNIST datasets\nModel Hyperparameters\nMNIST\nMNIST\nFashion-MNIST Fashion-MNIST\nTrain %\nTest %\nTrain %\nTest %\nBCPNN\nSee Table 1\n99.59 ± 0.01 98.31 ± 0.02\n88.71 ± 0.05\n86.31 ± 0.09\nKH\nSee [9]\n98.75 ± 0.01 97.39 ± 0.064\n87.49 ± 0.03\n85.10 ± 0.05\nRBM\nα = 0.01\n98.92 ± 0.04 97.67 ± 0.10\n88.13 ± 0.06\n86.06 ± 0.12\nAE\nλ = {1e-7,1e-5} 100.0 ± 0.00 97.78 ± 0.09\n88.52 ± 0.02\n86.12 ± 0.08\nThe results of the classiﬁcation are shown in Table 2. All the results presented\nhere are the mean and standard deviation of the classiﬁcation accuracy over ten\nrandom runs. We performed three independent comparisons of BCPNN with KH,\nRBM, and AE using the Kruskal-Wallis test to evaluate statistical signiﬁcance.\nOn both MNIST and Fashion-MNIST, BCPNN outperforms KH, RBM, and AE\n(all with p<0.0001).\nFor the BCPNN model, we set nHC=30 and nMC=100 in order to match the\nsize of the hidden layer with the other models. However, BCPNN also scales well\nwith the size of the hidden layer. From manual inspection, we found the best\nperformance is at nHC=200 and nMC=100, where the test accuracy for MNIST\nis 98.58 ± 0.05 and Fashion-MNIST is 88.87 ± 0.08.\n5\nDiscussion\nWe have evaluated four diﬀerent network models that can perform unsupervised\nrepresentation learning using biologically plausible local learning rules. We made\n4 This is lower than the test accuracy of 98.54% reported by Krotov and Hopﬁeld [9]\nwho used a non-linear classiﬁer with exponentiated ReLU activation and a non-linear\nloss function. Here we used instead a simpler linear classiﬁer with softmax activation\nand cross-entropy loss.\nBrain-like approaches to unsupervised learning\n11\nour assessment relying on the assumption that the usefulness of representations\nis reﬂected in their class separability, which can be quantiﬁed by classiﬁcation\nperformance, similar to recent unsupervised learning methods [17]. Learning rep-\nresentations without supervised ﬁne-tuning is a harder task compared to similar\nnetworks with end-to-end backprop, since the information about the samples’\ncorresponding labels cannot be utilized. Additionally, our unsupervised learning\nmethod relies on local Hebbian-like learning without any global optimisation cri-\nterion as in backprop learning, which makes the task even harder. Yet, we show\nthat the investigated BCPNN model performs well, comparable to the 98.5%\naccuracy of multi-layer perceptron networks on MNIST with one hidden layer\ntrained using end-to-end backprop [13].\nWe also showed that the recently proposed BCPNN model performs compet-\nitively against other unsupervised learning models. The modular structure of the\nBCPNN layer led to “hybrid” representations that diﬀer from the well-known\ndistributed and local representations. In contrast to the minibatch method of\nother unsupervised learning methods, learning in BCPNN was chosen to remain\nincremental using dynamical equations, since such learning is biologically feasible\nand useful in many autonomous engineering solutions. Despite the slow conver-\ngence properties of an incremental approach, BCPNN required only 5 epochs\nof unsupervised training, in comparison to 300 epochs for AE and RBM, and\n1000 epochs for KH. The incremental learning, along with modular architecture,\nsparse connectivity, and scalability of BCPNN is currently also taken advantage\nof in dedicated VLSI design [22].\nOne important diﬀerence between current deep learning architectures and\nthe brain concerns the abundance of recurrent connections in the latter. Deep\nlearning architectures rely predominantly on feedforward connectivity. A typical\ncortical area receives only around 10% of synapses from lower order structures,\ne.g. thalamus, and the rest from other cortical areas [5]. These feedback and re-\ncurrent cortical connections are likely involved in associative memory, constraint-\nsatisfaction e.g. for ﬁgure-ground segmentation, top-down modulation and selec-\ntive attention [5]. Incorporating these important aspects of cortical computation\ncan play a key role in improving our machine learning models and approaches.\nIt is important to note that the unsupervised learning models discussed in this\nwork are proof-of-concept designs and not meant to directly model some speciﬁc\nbiological system or structure. Yet, they may shed some light on the hierarchical\nfunctional organization of e.g. sensory processing streams in the brain. Further\nwork will focus on extending our study to multi-layer architectures.\nReferences\n1. Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and\nnew perspectives. IEEE transactions on pattern analysis and machine intelligence\n35(8), 1798–1828 (2013)\n2. Benjaminsson, S., Fransson, P., Lansner, A.: A novel model-free data analysis tech-\nnique based on clustering in a mutual information space: application to resting-\nstate fmri. Frontiers in systems neuroscience 4, 34 (2010)\n12\nN. B. Ravichandran et al.\n3. Berthet, P., Hellgren Kotaleski, J., Lansner, A.: Action selection performance of\na reconﬁgurable basal ganglia inspired model with hebbian–bayesian go-nogo con-\nnectivity. Frontiers in behavioral neuroscience 6, 65 (2012)\n4. Butz, M., W¨org¨otter, F., van Ooyen, A.: Activity-dependent structural plasticity.\nBrain research reviews 60(2), 287–305 (2009)\n5. Douglas, R.J., Martin, K.A.: Recurrent neuronal circuits in the neocortex. Current\nbiology 17(13), R496–R500 (2007)\n6. Erhan, D., Bengio, Y., Courville, A., Manzagol, P.A., Vincent, P., Bengio, S.: Why\ndoes unsupervised pre-training help deep discriminant learning? (2009)\n7. Fiebig, F., Lansner, A.: A spiking working memory model based on hebbian short-\nterm potentiation. Journal of Neuroscience 37(1), 83–96 (2017)\n8. Illing, B., Gerstner, W., Brea, J.: Biologically plausible deep learning—but how\nfar can we go with shallow networks? Neural Networks 118, 90–101 (2019)\n9. Krotov, D., Hopﬁeld, J.J.: Unsupervised learning by competing hidden units. Pro-\nceedings of the National Academy of Sciences 116(16), 7723–7731 (2019)\n10. Lansner, A.: Associative memory models: from the cell-assembly theory to biophys-\nically detailed cortex simulations. Trends in neurosciences 32(3), 178–186 (2009)\n11. Lansner, A., Benjaminsson, S., Johansson, C.: From ann to biomimetic information\nprocessing. In: Biologically Inspired Signal Processing for Chemical Sensing, pp.\n33–43 (2009)\n12. LeCun, Y.: The mnist database of handwritten digits. http://yann. lecun.\ncom/exdb/mnist/ (1998)\n13. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\n14. Lillicrap, T.P., Santoro, A., Marris, L., Akerman, C.J., Hinton, G.: Backpropaga-\ntion and the brain. Nature Reviews Neuroscience pp. 1–12 (2020)\n15. Lundqvist, M., Herman, P., Lansner, A.: Theta and gamma power increases and\nalpha/beta power decreases with memory load in an attractor network model.\nJournal of cognitive neuroscience 23(10), 3008–3020 (2011)\n16. Mountcastle, V.B.: The columnar organization of the neocortex. Brain: a journal\nof neurology 120(4), 701–722 (1997)\n17. Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748 (2018)\n18. Orre, R., Lansner, A., Bate, A., Lindquist, M.: Bayesian neural networks with\nconﬁdence estimations applied to data mining. Computational Statistics & Data\nAnalysis 34(4), 473–493 (2000)\n19. Ravichandran, N.B., Lansner, A., Herman, P.: Learning representations in bayesian\nconﬁdence propagation neural networks. In: International Joint Conference on Neu-\nral Networks (IJCNN) (2020)\n20. Rockland, K.S.: Five points on columns. Frontiers in Neuroanatomy 4, 22 (2010)\n21. Sandberg, A., Lansner, A., Petersson, K.M., Ekeberg.: A bayesian attractor net-\nwork with incremental learning. Network: Computation in neural systems 13(2),\n179–194 (2002)\n22. Stathis, D., Sudarshan, C., Yang, Y., Jung, M., Weis, C., Hemani, A., Lansner,\nA., Wehn, N.: ebrainii: a 3 kw realtime custom 3d dram integrated asic implemen-\ntation of a biologically plausible model of a human scale cortex. Journal of Signal\nProcessing Systems pp. 1–21 (2020)\n23. Tully, P.J., Hennig, M.H., Lansner, A.: Synaptic and nonsynaptic plasticity ap-\nproximating probabilistic inference. Frontiers in synaptic neuroscience 6, 8 (2014)\n24. Xiao, H., Rasul, K., Vollgraf, R.: Fashion-mnist: a novel image dataset for bench-\nmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 (2017)\n",
  "categories": [
    "cs.NE",
    "cs.LG"
  ],
  "published": "2020-05-06",
  "updated": "2021-04-16"
}