{
  "id": "http://arxiv.org/abs/2005.03812v1",
  "title": "Comparative Analysis of Word Embeddings for Capturing Word Similarities",
  "authors": [
    "Martina Toshevska",
    "Frosina Stojanovska",
    "Jovan Kalajdjieski"
  ],
  "abstract": "Distributed language representation has become the most widely used technique\nfor language representation in various natural language processing tasks. Most\nof the natural language processing models that are based on deep learning\ntechniques use already pre-trained distributed word representations, commonly\ncalled word embeddings. Determining the most qualitative word embeddings is of\ncrucial importance for such models. However, selecting the appropriate word\nembeddings is a perplexing task since the projected embedding space is not\nintuitive to humans. In this paper, we explore different approaches for\ncreating distributed word representations. We perform an intrinsic evaluation\nof several state-of-the-art word embedding methods. Their performance on\ncapturing word similarities is analysed with existing benchmark datasets for\nword pairs similarities. The research in this paper conducts a correlation\nanalysis between ground truth word similarities and similarities obtained by\ndifferent word embedding methods.",
  "text": "COMPARATIVE ANALYSIS OF WORD EMBEDDINGS \nFOR CAPTURING WORD SIMILARITIES \nMartina Toshevska1, Frosina Stojanovska2 and Jovan Kalajdjieski3 \n1Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, \nSkopje, Macedonia \nmartina.toshevska@finki.ukim.mk \n2Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, \nSkopje, Macedonia \nfrosina.stojanovska@finki.ukim.mk \n3Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University, \nSkopje, Macedonia \njovan.kalajdzhieski@finki.ukim.mk \n \n \nABSTRACT \nDistributed language representation has become the most widely used technique for language \nrepresentation in various natural language processing tasks. Most of the natural language processing \nmodels that are based on deep learning techniques use already pre-trained distributed word \nrepresentations, commonly called word embeddings. Determining the most qualitative word embeddings \nis of crucial importance for such models. However, selecting the appropriate word embeddings is a \nperplexing task since the projected embedding space is not intuitive to humans. \nIn this paper, we explore different approaches for creating distributed word representations. We perform \nan intrinsic evaluation of several state-of-the-art word embedding methods. Their performance on \ncapturing word similarities is analysed with existing benchmark datasets for word pairs similarities. The \nresearch in this paper conducts a correlation analysis between ground truth word similarities and \nsimilarities obtained by different word embedding methods. \nKEYWORDS \nWord Embeddings, Distributed Word Representation, Word Similarity \n1. INTRODUCTION \nRepresenting natural language has become one of the main concerns in a wide range of Natural \nLanguage Processing (NLP) tasks. Some of the simplest techniques for representing natural \nlanguage, when the data is textual, are one-hot encoding, Bag of Words (BoW), term-frequency, \nalong with others [1]. These methods have a deficiency of incorporating the necessary \ninformation in the mapped space of the representations. Representations such as one-hot \nencoding create vector representations with enormous size (the size of the vocabulary) that do \nnot preserve the relative closeness of the words, and representations such as BoW do not capture \nthe context and order of the words. \nDistributed representations of textual data as vectors have been proven useful in many \nalgorithms. Each word vector in a given corpus is represented based on the mutual information \nwith other words in the corpus [2]. Vector representation can be computed at different levels \nincluding: characters [3], [4], words [5], [6], [7], phrases [8], [9], sentences [10], document [11], \netc. Word embeddings are representations of words in a continuous Rn space. Each word is \nprojected in n-dimensional vector space as a real-valued vector by keeping the information \nabout the meanings and similarities of the words. Words with similar meanings will be mapped \ncloser in the projected space. Word vectors capture syntactic and semantic regularities in \nlanguage, such that the operation “King - Man + Woman” results in a vector close to “Queen” \n[5], [12]. \nNowadays, several word embeddings methods incorporate different techniques for word \nrepresentation. Training the word embeddings requires large corpus, extensive computational \npower and time. Consequently, the preferred approach is to use the idea of transfer learning, i.e. \nusing already pre-trained embeddings and then training the NLP model with these embeddings. \nHowever, selecting the appropriate word embeddings is a perplexing task since the projected \nembedding space is not intuitive to humans. Nevertheless, deciding on the pre-trained word \nembeddings used as input in the NLP model is the first and crucial step, since the quality of the \nword representation has a significant influence on the global performance of the model.   \nTherefore, offering comparative experimental results for the methods can be valuable to \nresearchers for determining the most appropriate embeddings for their model based on the \ncomparative analysis. The evaluation of word embeddings is not unified. Generally, the \nevaluation methods can be mainly grouped into two different approaches: an intrinsic and \nextrinsic evaluation [13]. Intrinsic evaluations test the quality of a representation independent of \nspecific natural language processing tasks, directly testing the syntactic or semantic \nrelationships between words. Human-created benchmark datasets generated with a particular \ntask of comparing words by similarity, relatedness, or similar, are of crucial importance in the \ncomparing process of intrinsic evaluation. Extrinsic evaluation of word vectors is the evaluation \nof the embeddings on a real NLP task, chosen as an evaluation method, for example, part-of-\nspeech tagging [14], named-entity recognition [15], sentiment analysis [16] and neural machine \ntranslation [17].  \nDetermining the most qualitative word embeddings is the main focus when executing the \nevaluations. Additionally, the dimensionality of the word embeddings is an important segment \nof the method capability to encode notable information in the projected space. Bigger \ndimensionality means more space for incorporating more information in the space, but after \nreaching some point, the marginal gain will diminish [5]. Typically, the dimensionality of the \nvectors is set to be between 50 and 1,000, where 300 is the most frequently used. Consequently, \nevaluating the word embeddings requires additional investigation for finding the most suitable \ndimensionality.  \nIn this paper, we focus on different pre-trained word embeddings used in state-of-the-art models \nfor some NLP tasks and analyse their performance on capturing word similarities with existing \nbenchmark datasets for word pairs similarities1. The datasets WordSim353 [18], SimLex999 \n[19] and SimVerb3500 [20] are used in the experiments to compare the state-of-the-art word \nembeddings. \nThe rest of the paper is organized as follows. Section 2 describes the word embedding methods \nused for comparative analysis. Section 3 gives an overview of the related research for the \nproblem. The comparative analysis along with the discussion of the results of the experiments \nare included in Section 4. Finally, Section 5 concludes the paper and gives future work ideas. \n \n \n \n1 The code is available at https://github.com/mtoshevska/Comparing-Embeddings \n2. WORD EMBEDDINGS \nMainly, word embeddings methods can be divided into three groups according to the technique \nof creating the vectors: neural network based, word matrix based and a combination of the \nprevious two groups with an additional strategy of supplementary information fusion called \nensemble methods. \n \n2.1. Neural Network Based Methods \n2.1.1. Word2Vec \nWord2Vec includes two different models: Continuous Bag Of Words (CBOW) and Skip-gram \n[5], [6]. Both of these methods are neural networks with a hidden layer with N neurons, where \nN is the dimensionality of the created word embeddings. The first method CBOW is a neural \nnetwork where the context of the words is the input of the network. The task is to predict the \ncurrent word as the output of the network [5]. The second method Skip-gram is a neural \nnetwork where the input is the one-hot encoding of the word, and the output is the predicted \ncontext of the word, i.e. the surrounding words [5]. To make these algorithms more efficient \ntechniques such as Hierarchical Softmax and Skip-Gram Negative Sampling (SGNS) are used. \nThe most popular pre-trained Word2Vec word embeddings are 300-dimensional vectors \ngenerated with negative sampling training on the Google News corpus (that contains 100 billion \nwords)2 [6]. These pre-trained embeddings are used in the experiments. \n2.1.2. FastText \nThe FastText model [21], [22] is directly derived from the Skip-gram model of Word2Vec. The \nauthors claim that by using a distinct vector representation for each word, the Skip-gram model \nignores the internal structure of the words. For that purpose, they suggested a different scoring \nfunction that takes into account the internal structure. Their subword model represents each \nword as a bag of character n-gram. Special symbols < and > are added at the beginning and the \nend of words to distinguish prefixes and suffixes from other character sequences. The word is \nalso included in the set of its n-grams to be able to learn a better representation for each word. \nThe authors suggest extracting all n-grams for n greater than or equal to 3 and smaller than or \nequal to 6. \nThis simple approach provides extraction of all prefixes and suffixes of a given word. When all \nthe n-grams of a specific word are extracted, a vector representation is assigned to every n-\ngram. The final word is represented as the sum of the vector representations of all of its n-\ngrams. This model allows sharing the representations across words, thus allowing to learn a \nreliable representation for rare words.  \nTo bound the memory requirements, a hashing function - FNV-1a3 is used to map the n-grams \nto integer numbers. There are also proposed approaches for producing compact architectures \n[23]. \nIn our experiments, we have used pre-trained models both trained with subword information on \nWikipedia 2017 (16B tokens) and trained with subword information on Common Crawl (600B \ntokens)4. \n \n2 https://code.google.com/p/word2vec/, last visited: 02.04.2020 \n3 http://www.isthe.com/chongo/tech/comp/fnv/, last visited: 02.04.2020 \n2.2. Word Matrix Based Methods \n2.2.1. GloVe \nGloVe (Global Vectors for Word Representation) [7] is a log-bilinear regression model for \nunsupervised learning of word representations that combines the advantages of two model \nfamilies: global matrix factorization and local context window methods. The general idea is that \nco-occurrence ratio of any two words, that is the frequency of the words occurring in each \nother’s context, encodes information about the words. It captures meaningful linear \nsubstructures, efficiently leveraging global word-word co-occurrence matrix statistics. The \nmodel is optimized so that the dot product of any pair of vectors is equal to the co-occurrence \nratio of the corresponding words. \nThere are several pre-trained GloVe vectors available for public use5, trained on different \ndatasets such as Twitter and Wikipedia. In our experiments we include 50-dimensional and 200-\ndimensional word vectors pre-trained on the Twitter dataset (27B tokens), and 50-dimensional \nand 300-dimensional word vectors pre-trained on the Wikipedia dataset (6B tokens). \n2.2.2. LexVec \nLexVec [24] is based on the idea of factoring the PPMI matrix [25] using a reconstruction loss \nfunction. This loss function does not weigh all errors equally, unlike SVD, but penalizes errors \nof frequent co-occurrence more heavily while still treating negative co-occurrence, unlike \nGloVe. The authors propose keeping the SGNS [6] weighting scheme by using window \nsampling and negative sampling but explicitly factoring PPMI matrix rather than implicitly \nfactorizing the shifted PMI matrix. The minimization on the two terms of the loss function is \ndone using two approaches: mini-batch, which executes gradient descent in the same way as \nSGNS, and stochastic, where every context window is extended with k negative samples and \niterative gradient descent is then run on pairs for each window. The authors state that the \nevaluation of word similarity and analogy tasks shows that LexVec compares to and often \noutperforms state-of-the-art methods on many of these tasks. \nSeveral pre-trained LexVec vectors are available for public use6, and the experiments in this \npaper include the model trained with subword information on Common Crawl which contained \n2,000,000 words (58B tokens) and the model trained on Wikipedia which contained 368,999 \nwords (7B tokens).  \n2.3. Ensemble Methods \n2.3.1. ConceptNet Numberbatch \nConceptNet Numberbatch represents an ensemble method that produces word embeddings from \nlarge, multilingual vocabulary with a combination of the GloVe and Word2Vec embeddings and \nadditional structured knowledge from the semantic networks ConceptNet [26] and PPDB [27]. \nThis method uses the extended retrofitting technique [28] to adjust the pre-trained word \nembedding matrices with knowledge graphs. The objective is to learn a new embeddings matrix \nsuch that the word embeddings are close by some distance metric to their counterparts in the \nprimary word embedding matrix and adjacent vertices in the knowledge network. \n \n4 https://fasttext.cc/docs/en/english-vectors.html, last visited: 02.04.2020 \n5 https://nlp.stanford.edu/projects/glove/, last visited: 02.04.2020 \n6 https://github.com/alexandres/lexvec, last visited: 02.04.2020 \nThe pre-trained embeddings version ConceptNet Numberbatch 19.087 [29] is used in the \nexperiments in this paper. \n \n3. RELATED WORK \nThere are several attempts to compare different methods for creating word embeddings on \ndifferent datasets for different purposes. Mainly the experiments are conducted on datasets \ncontaining English words, but some are on other languages as well. Berardi et al. [30] have \nconducted a comparison between the Skip-gram model of Word2Vec [5], [6] and GloVe [7] in \nthe Italian language. The models are trained on two datasets: the entire dump of the Italian \nWikipedia and a collection of 31,432 books (mostly novels) written in Italian. The authors \nbelieve that by using two very different datasets, both by purpose and style, they will investigate \nthe impact of the training data on the two models. In this research, they conclude that \nWord2Vec’s Skip-gram model outperforms GloVe on both datasets. However, it does not have \nthe same accuracy as trained on English datasets, which may be a sign of a higher complexity of \nthe Italian language.  \nComparison between different models for word embeddings in different languages is also \nconducted in [31], where the authors first compare Hungarian analogical questions to English \nquestions by training a Skip-gram model on the Hungarian Webcorpus. They obtain similar \nresults in the morphological questions, but the semantic questions are dominated by the English \nmodel. They also conduct a proto dictionary generation and comparison using CBOW \nWord2Vec and GloVe models on Hungarian/Slovenian/Lithuanian languages to English.  \nIn contrast, there have been quite a few studies evaluating word embeddings in quantitatively \nrepresenting word semantics in the English language, mostly comparing the word embeddings \ngenerated by different methods. Baroni et al. [32] conducted a comparison between four \nmodels: Word2Vec’s CBOW8, DISSECT9, Distributional Memory model10 and Collobert and \nWeston11. The evaluation has been done using a corpus of 2.8 billion tokens.  In their research, \nthey concluded that Word2Vec’s CBOW model outperformed other methods for almost all the \ntasks.  \nGhannay et al. [33] evaluated the word embeddings generated by CSLM word embeddings [34], \ndependency-based word embeddings [35], combined word embeddings and Word2Vec’s Skip-\ngram model on multiple NLP tasks. The models were trained on the Gigaword corpus composed \nof 4 billion words, and the authors found that the dependency-based word embeddings gave the \nbest performance. They also concluded that significant improvement can be obtained by a \ncombination of embeddings. Authors in [13] compared Word2Vec’s CBOW model, GloVe, \nTSCCA [36], C&W embeddings [37], Hellinger PCA [38] and Sparse Random Projections [39] \nand concluded that Word2Vec’s CBOW model outperformed the other encodings on 10 out of \nthe 14 test datasets. Apart from comparisons of embeddings in the general NLP domain, there \nhave also been comparisons of word embeddings on many specific domains such as biomedical \ndomains [40], [41]. \n \n \n7 https://github.com/commonsense/conceptnet-numberbatch, last visited: 02.04.2020 \n8 https://code.google.com/p/word2vec/, last visited: 02.04.2020 \n9 http://clic.cimec.unitn.it/composes/, last visited: 02.04.2020 \n10 http://clic.cimec.unitn.it/dm/, last visited: 02.04.2020 \n11 http://ronan.collobert.com/senna/, last visited: 02.04.2020 \n4. COMPARATIVE ANALYSIS \nWord vectors can be evaluated using two different approaches. Extrinsic evaluation is an \nevaluation with real NLP tasks such as natural language inference or sentiment analysis. For \nthat purpose, word vectors are incorporated into an embedding layer of a deep neural network. \nThe model is then trained on a specific NLP task. If the performances are bad, it is uncertain \nwhether the word vectors lack to represent the meaning of the words or simply the model is not \ngood enough for the task. \nThe prior issue justifies the need for intrinsic evaluation. This type of evaluation is an \nintermediate evaluation where word vectors are directly evaluated on different tasks. The \nintrinsic evaluation involves tasks as word vector analogies and word similarities. \nWord vector analogies are used to evaluate word vectors by how well their cosine distance after \naddition captures intuitive semantic and syntactic analogy questions. For example, the operation \n“King - Man + Woman” results in a vector close to “Queen”, the operation “Windows - \nMicrosoft + Google” results in a vector close to “Android”, etc. \nWord similarities are values that measure the similarity of word pairs. These values are \ncollected in several human-generated datasets that are applied as a resource for comparison of \nthe quality of mapping the words in some space with different word representations. Different \ndatasets represent the similarity differently, by either coupling or segregating the word \nsimilarity with word relatedness or word association. \n4.1. Data \nWordSim35312 dataset consists of 353 noun pairs with human similarity ratings [18]. The \ndataset is used as a golden standard in the field of word similarity and relatedness computation. \nThe similarity scores of the word pairs in this resource are a representative measure for the \nsimilarity of the words dependent on the relatedness or association. Later the dataset is \nannotated with semantic relations by distinguishing the concepts of similarity and relatedness \n[42]. \nSimLex99913 has recently become a widely used lexical resource for tracking progress in word \nsimilarity computation. This benchmark dataset is concentrated on measuring the similarity, \nrather than relatedness or association [19]. It consists of 666 noun-noun pairs, 222 verb-verb \npairs and 111 adjective-adjective pairs. The annotation guidelines of the dataset narrow the \nsimilarity to synonymy [43]. SimLex999 can be used to measure the ability of the word \nembedding models to capture the similarity of the words (without the influence of the \nrelatedness or association) while at the same time excluding the dependence of relatedness or \nassociation. \nSimVerb350014 represents a dataset for verb similarity that consists of 3500 verb-verb pairs \n[20]. This dataset is built on the same annotation criteria as the SimLex-999, i.e. there is a \ndifference between similarity and relatedness. Therefore, this method is a broad coverage \nresource for verb similarities. \n \n \n12 http://alfonseca.org/eng/research/wordsim353.html, last visited: 02.04.2020 \n13 https://fh295.github.io/SimLex-999.zip, last visited: 02.04.2020 \n14 https://www.repository.cam.ac.uk/handle/1810/264124, last visited: 02.04.2020 \n4.2. Word Similarity Experiments \n4.2.1. Word Similarities \nWord embeddings represent words in a way that words with similar meaning are represented \nwith similar vectors. The experiments presented in this section evaluate the ability to capture \nword similarities of different pre-trained word embeddings. For each word pair in the datasets, \nthe cosine similarity of the corresponding word embedding vectors is computed. Average cosine \nsimilarities for each dataset are plotted in Figure 1. \nAverage similarity obtained by human ratings is 5.86, 4.56 and 4.29 for WordSim353, \nSimLex999 and SimVerb3500 dataset, respectively. The closest average cosine similarity for \neach dataset is obtained among GloVe and FastText word embeddings. The average cosine \nsimilarities for GloVe embeddings is 5.37, 4.62 and 3.79 for WordSim353, SimLex999 and \nSimVerb3500 dataset, respectively. With the FastText embeddings, average cosine similarity is \n4.69, 4.81 and 4.12 for WordSim353, SimLex999 and SimVerb3500 dataset, respectively. \nThese values direct to the conclusion that FastText and GloVe perform better in capturing \nsimilarities between words. \nIn terms of dimensionality, it is expected that word vectors of higher dimensionality have a \nmore prominent quality of the representation. This statement is confirmed for SimLex999 \ndataset and slightly confirmed for SimVerb3500 dataset, when analysing the GloVe embeddings \nwith different dimensionalities. For SimLex999 dataset, average cosine similarity is closer to \nground truth similarity for 200-dimensional embeddings pre-trained on Twitter and 300-\ndimensional embeddings pre-trained on Wikipedia. For SimVerb3500 dataset, the statement \napplies only for 200-dimensional embeddings pre-trained on Twitter, while the similarities for \nvectors pre-trained on Wikipedia are closer to ground truth similarities when the dimensionality \nis 50. For the WordSim353 dataset, lower vector dimensionality implies closer cosine to \nground-truth similarity. \n4.2.2. Correlation Analysis \nAverage similarity does not always provide good insight into word similarities since similarities \nobtained by one metric can be higher than similarities obtained by another metric. Despite \ndifferent average values, both similarity distributions could be correlated. Therefore, for all pre-\ntrained word embedding vectors, we calculate the correlation coefficient between cosine \nsimilarities of word vectors and ground truth similarities. \nSpearman correlation coefficient, Pearson correlation coefficient and Kendall’s tau correlation \ncoefficient are computed for each pair of ground-truth similarities and cosine similarities. The \nresults are summarized in Table 1, Table 2 and Table 3 for WordSim353, SimLex999 and \nSimVerb3500 dataset, respectively, and are depicted in Figure 2 for the WordSim353 dataset, \nFigure 3 for the SimLex999 dataset, and Figure 4 for the SimVerb3500 dataset. \nResults of the correlation analysis oppose the findings of average similarity. GloVe and \nFastText word embeddings demonstrated better performance for capturing word similarities in \nterms of average cosine similarity. However, they have the lowest correlation coefficient. The \nvalue is around 0, implying no correlation between ground truth and cosine similarities of word \nvectors. FastText despite being developed to overcome weaknesses of Word2Vec, does not \nshow to capture word similarities better since correlation is roughly 0.1 for SimLex999 and \nSimVerb3500 and approximately 0.2 for WordSim353. GloVe also shows weak performance on \nSimLex999 and SimVerb3500, although it is used as a starting point in many high-performance \ndeep learning models for different NLP tasks. However, the correlation on WordSim353 is quite \nhigher, with a maximum value of 0.61 for the Spearman correlation coefficient. \n \nFigure 1. Average cosine similarities. GT - ground truth similarities. F-C-300 - 300-dimensional FastText \nembeddings pre-trained on Crawl dataset. F-W-300 - 300-dimensional FastText embeddings pre-trained \non Wikipedia dataset. G-T-200 - 200-dimensional GloVe embeddings pre-trained on Twitter dataset. G-\nT-50 - 50-dimensional GloVe embeddings pre-trained on Twitter dataset. G-W-300 - 300-dimensional \nGloVe embeddings pre-trained on Wikipedia dataset. G-W-50 - 50-dimensional GloVe embeddings pre-\ntrained on Wikipedia dataset. L-C-300 - 300-dimensional LexVec embeddings pre-trained on Crawl \ndataset. L-W-300 - 300-dimensional LexVec embeddings pre-trained on Wikipedia dataset. N-C-300 - \n300-dimensional ConceptNet Numberbatch embeddings. W-G-300 - 300-dimensional Word2Vec \nembeddings pre-trained on Google News dataset.  \nFor all three datasets, the highest correlation is achieved by ConceptNet Numberbatch word \nembeddings, indicating a positive correlation between similarity distributions. The second \nhighest correlation coefficient belongs to Word2Vec and LexVec word embeddings that are \nabout 0.7 for WordSim353 dataset, around 0.4 for SimLex999 dataset and roughly 0.3 for the \nSimVerb3500 dataset. \nThe assumption about the better quality of word vectors with higher dimensionality corresponds \nwith the correlation results. Correlation coefficients for GloVe word embeddings despite being \nlow for the SimLex999 and SimVerb3500 dataset, show that higher dimensionality entails \nhigher correlation. Furthermore, we can infer that word vectors of higher dimensionality are \nbetter for capturing word similarities. \n \n \n \n \n \nTable 1. Correlation coefficients between ground truth similarities and word vector cosine similarities for \nthe WordSim353 dataset. S - Spearman correlation coefficient. Pr - Pearson correlation coefficient. K - \nKendall’s tau correlation coefficient. \nWord Embeddings \nS \nPr \nK \nFastText-Crawl-300 \n0.25 \n0.26 \n0.17 \nFastText-Wikipedia-300 \n0.19 \n0.19 \n0.13 \nGloVe-Twitter-200 \n0.52 \n0.53 \n0.36 \nGloVe-Twitter-50 \n0.46 \n0.46 \n0.32 \nGloVe-Wikipedia-300 \n0.61 \n0.60 \n0.45 \nGloVe-Wikipedia-50 \n0.50 \n0.51 \n0.36 \nLexVec-Crawl-300 \n0.72 \n0.68 \n0.53 \nLexVec-Wikipedia-300 \n0.66 \n0.63 \n0.48 \nConceptNet-Numberbatch-300 \n0.81 \n0.75 \n0.63 \nWord2Vec-GoogleNews-300 \n0.69 \n0.65 \n0.51 \n \n \n \n \n \n \n \nTable 2. Correlation coefficients between ground truth similarities and word vector cosine similarities for \nthe SimLex999 dataset. S - Spearman correlation coefficient. Pr - Pearson correlation coefficient. K - \nKendall’s tau correlation coefficient. \nWord Embeddings \nS \nPr \nK \nFastText-Crawl-300 \n0.16 \n0.16 \n0.11 \nFastText-Wikipedia-300 \n0.09 \n0.07 \n0.06 \nGloVe-Twitter-200 \n0.13 \n0.14 \n0.08 \nGloVe-Twitter-50 \n0.10 \n0.10 \n0.06 \nGloVe-Wikipedia-300 \n0.37 \n0.39 \n0.30 \nGloVe-Wikipedia-50 \n0.26 \n0.29 \n0.18 \nLexVec-Crawl-300 \n0.44 \n0.45 \n0.31 \nLexVec-Wikipedia-300 \n0.38 \n0.39 \n0.27 \nConceptNet-Numberbatch-300 \n0.63 \n0.65 \n0.46 \nWord2Vec-GoogleNews-300 \n0.44 \n0.45 \n0.31 \n \n \n \n \nTable 3. Correlation coefficients between ground truth similarities and word vector cosine similarities for \nthe SimVerb3500 dataset. S - Spearman correlation coefficient. Pr - Pearson correlation coefficient. K - \nKendall’s tau correlation coefficient. \nWord Embeddings \nS \nPr \nK \nFastText-Crawl-300 \n0.11 0.11 \n0.07 \nFastText-Wikipedia-300 \n0.03 0.02 \n0.02 \nGloVe-Twitter-200 \n0.06 0.07 \n0.04 \nGloVe-Twitter-50 \n0.03 0.04 \n0.02 \nGloVe-Wikipedia-300 \n0.23 0.23 \n0.16 \nGloVe-Wikipedia-50 \n0.15 0.16 \n0.10 \nLexVec-Crawl-300 \n0.30 0.31 \n0.21 \nLexVec-Wikipedia-300 \n0.28 0.28 \n0.19 \nConceptNet-Numberbatch-300 \n0.57 0.59 \n0.41 \nWord2Vec-GoogleNews-300 \n0.36 0.38 \n0.25 \n \n \n \n \n \nFigure 2. Correlation between ground truth similarities and similarities obtained with different word \nembedding methods for the WordSim353 dataset. Gt_sim – ground truth similarities obtained by human \nratings. Cosine_sim – cosine similarities between word embeddings \n \n \n \n \nFigure 3. Correlation between ground truth similarities and similarities obtained with different word \nembedding methods for the SimLex999 dataset. Gt_sim – ground truth similarities obtained by human \nratings. Cosine_sim – cosine similarities between word embeddings. \n \n \n \n \nFigure 4. Correlation between ground truth similarities and similarities obtained with different word \nembedding methods for the SimVerb3500 dataset. Gt_sim – ground truth similarities obtained by human \nratings. Cosine_sim – cosine similarities between word embeddings \n \n5. CONCLUSIONS AND FUTURE WORK \nIn this paper, we have analysed several word embedding methods. These methods could be \ngenerally divided into two groups according to the creation technique: neural network based and \nmatrix based. We further examined an ensemble method which is a compound of the two \nmethods. \nWord embedding similarity experiments were conducted for all of the examined methods. These \nexperiments evaluate the ability to capture word similarities between pairs of words on three \ndifferent datasets using a cosine similarity measure. Analysing only the average similarity \ninformation and comparing these values with the average value of the human ratings, the \nconclusion was that the GloVe and FastText outperformed the other word embedding methods. \nAfter conducting the word embedding similarity experiments, we also carried out correlation \nanalysis. In this analysis, the Spearman correlation coefficient, Pearson correlation coefficient \nand Kendall’s tau correlation coefficient were computed for each pair of ground truth \nsimilarities and cosine similarities of the word embeddings. Using the data obtained in this \nanalysis we have concluded that even with the high cosine similarity values of GloVe and \nFastText, their correlation values are close to 0, implying no correlation between ground truth \nand cosine similarities of word vectors. Moreover, ConceptNet Numberbatch word embeddings \nhave the highest correlation coefficients even though they did not have high average similarity \nvalues. \nA more comprehensive study of word similarities is including the context of the text of the \nwords. Newest word embedding methods, like ELMO [44] or BERT [45], utilize the context of \nthe words as they appear in the text, creating deep contextualized word representations. \nExamining the difference between these different mappings of the words will be part of our \nfuture work. Additionally, combining the different ideas from intrinsic and extrinsic evaluations \nwould provide a more beneficial comparison of the different word representations.  \nREFERENCES \n[1] \nManning, Christopher D., Prabhakar Raghavan, and Hinrich Schütze. “Scoring, term weighting \nand the vector space model.” Introduction to information retrieval 100 (2008): 2-4. \n[2]          Mikolov, Tomáš, Wen-tau Yih, and Geoffrey Zweig. “Linguistic regularities in continuous space \nword representations.” Proceedings of the 2013 conference of the north american chapter of the \nassociation for computational linguistics: Human language technologies. 2013. \n[3]         Santos, Cicero D., and Bianca Zadrozny. “Learning character-level representations for part-of-\nspeech tagging.” Proceedings of the 31st international conference on machine learning (ICML-\n14). 2014. \n[4]         dos Santos, Cıcero, et al. “Boosting Named Entity Recognition with Neural Character \nEmbeddings.” Proceedings of NEWS 2015 The Fifth Named Entities Workshop. 2015. \n[5] \nMikolov, Tomas, et al. “Efficient Estimation of Word Representations in Vector Space.” ICLR \n(Workshop Poster). 2013. \n[6] \nMikolov, Tomas, et al. “Distributed representations of words and phrases and their \ncompositionality.” Advances in neural information processing systems. 2013. \n[7] \nPennington, Jeffrey, Richard Socher, and Christopher D. Manning. “Glove: Global vectors for \nword representation.” Proceedings of the 2014 conference on empirical methods in natural \nlanguage processing (EMNLP). 2014. \n[8] \nYu, Mo, and Mark Dredze. “Learning composition models for phrase embeddings.” \nTransactions of the Association for Computational Linguistics 3 (2015): 227-242. \n[9] \nZhou, Zhihao, Lifu Huang, and Heng Ji. “Learning Phrase Embeddings from Paraphrases with \nGRUs.” Proceedings of the First Workshop on Curation and Applications of Parallel and \nComparable Corpora. 2017. \n[10] \nKiros, Ryan, et al. “Skip-thought vectors.” Advances in neural information processing systems. \n2015. \n[11] \nLe, Quoc, and Tomas Mikolov. “Distributed representations of sentences and documents.” \nInternational conference on machine learning. 2014. \n[12] \nLevy, Omer, and Yoav Goldberg. “Linguistic regularities in sparse and explicit word \nrepresentations.” Proceedings of the eighteenth conference on computational natural language \nlearning. 2014. \n[13]       Schnabel, Tobias, et al. “Evaluation methods for unsupervised word embeddings.” Proceedings \nof the 2015 conference on empirical methods in natural language processing. 2015. \n[14]       Ratnaparkhi, Adwait. “A maximum entropy model for part-of-speech tagging.” Conference on \nEmpirical Methods in Natural Language Processing. 1996. \n[15]       Yadav, Vikas, and Steven Bethard. “A Survey on Recent Advances in Named Entity Recognition \nfrom Deep Learning models.” Proceedings of the 27th International Conference on \nComputational Linguistics. 2018. \n[16]       Liu, Bing. Sentiment analysis: Mining opinions, sentiments, and emotions. Cambridge University \nPress, 2015. \n[17]       Cho, Kyunghyun, et al. “Learning Phrase Representations using RNN Encoder–Decoder for \nStatistical Machine Translation.” Proceedings of the 2014 Conference on Empirical Methods in \nNatural Language Processing (EMNLP). 2014. \n[18] \nFinkelstein, Lev, et al. “Placing search in context: The concept revisited.” Proceedings of the \n10th international conference on World Wide Web. 2001. \n[19] \nHill, Felix, Roi Reichart, and Anna Korhonen. “Simlex-999: Evaluating semantic models with \n(genuine) similarity estimation.” Computational Linguistics 41.4 (2015): 665-695. \n[20] \nGerz, Daniela, et al. “SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity.” \nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. \n2016. \n[21] \nBojanowski, Piotr, et al. “Enriching word vectors with subword information.” Transactions of \nthe Association for Computational Linguistics 5 (2017): 135-146. \n[22] \nJoulin, Armand, et al. “Bag of Tricks for Efficient Text Classification.” Proceedings of the 15th \nConference of the European Chapter of the Association for Computational Linguistics: Volume \n2, Short Papers. 2017. \n[23] \nA. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jegou, and T. Mikolov, “Fasttext. zip: \nCompressing text classification models,” arXiv preprint arXiv:1612.03651 (2016). \n[24] \nSalle, Alexandre, Aline Villavicencio, and Marco Idiart. “Matrix Factorization using Window \nSampling and Negative Sampling for Improved Word Representations.” Proceedings of the 54th \nAnnual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). \n2016. \n[25] \nBullinaria, John A., and Joseph P. Levy. “Extracting semantic representations from word co-\noccurrence statistics: A computational study.” Behavior research methods 39.3 (2007): 510-526. \n[26] \nSpeer, Robyn, and Catherine Havasi. “Representing General Relational Knowledge in \nConceptNet 5.” Proceedings of the Eighth International Conference on Language Resources and \nEvaluation (LREC'12). 2012. \n[27] \nGanitkevitch, Juri, Benjamin Van Durme, and Chris Callison-Burch. “PPDB: The paraphrase \ndatabase.” Proceedings of the 2013 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technologies. 2013. \n[28] \nR. Speer and J. Chin, “An ensemble method to produce high-quality word embeddings.” arXiv \npreprint arXiv:1604.01692 (2016). \n[29] \nSpeer, Robyn, Joshua Chin, and Catherine Havasi. “Conceptnet 5.5: An open multilingual graph \nof general knowledge.” Thirty-First AAAI Conference on Artificial Intelligence. 2017. \n[30]    \nBerardi, Giacomo, Andrea Esuli, and Diego Marcheggiani. “Word Embeddings Go to Italy: A \nComparison of Models and Training Datasets.” IIR. 2015. \n[31]     Makrai, Márton, et al. “Comparison of distributed language models on medium-resourced \nlanguages.” XI. Magyar Számítógépes Nyelvészeti Konferencia (MSZNY 2015) (2015). \n[32]     Baroni, Marco, Georgiana Dinu, and Germán Kruszewski. “Don’t count, predict! a systematic \ncomparison of context-counting vs. context-predicting semantic vectors.” Proceedings of the \n52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long \nPapers). 2014. \n[33]     Ghannay, Sahar, et al. “Word embedding evaluation and combination.” Proceedings of the Tenth \nInternational Conference on Language Resources and Evaluation (LREC'16). 2016. \n[34] \nSchwenk, Holger. “CSLM-a modular open-source continuous space language modeling toolkit.” \nINTERSPEECH. 2013. \n[35] \nLevy, Omer, and Yoav Goldberg. “Dependency-based word embeddings.” Proceedings of the \n52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short \nPapers). 2014. \n[36]     Dhillon, Paramveer S., et al. “Two step CCA: a new spectral method for estimating vector \nmodels of words.” Proceedings of the 29th International Conference on International \nConference on Machine Learning. 2012. \n[37] \nCollobert, Ronan, et al. “Natural language processing (almost) from scratch.” Journal of \nmachine learning research 12.Aug (2011): 2493-2537. \n[38] \nLebret, Rémi, and Ronan Collobert. “Word Embeddings through Hellinger PCA.” Proceedings \nof the 14th Conference of the European Chapter of the Association for Computational \nLinguistics. 2014. \n[39] \nLi, Ping, Trevor J. Hastie, and Kenneth W. Church. “Very sparse random projections.” \nProceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and \ndata mining. 2006. \n[40]     Pakhomov, Serguei VS, et al. “Corpus domain effects on distributional semantic modeling of \nmedical terms.” Bioinformatics 32.23 (2016): 3635-3644. \n[41]     Wang, Yanshan, et al. “A comparison of word embeddings for the biomedical natural language \nprocessing.” Journal of biomedical informatics 87 (2018): 12-20. \n[42] \nAgirre, Eneko, et al. “A Study on Similarity and Relatedness Using Distributional and WordNet-\nbased Approaches.” Proceedings of Human Language Technologies: The 2009 Annual \nConference of the North American Chapter of the Association for Computational Linguistics. \n2009. \n[43] \nKliegr, Tomáš, and Ondřej Zamazal. “Antonyms are similar: Towards paradigmatic association \napproach to rating similarity in SimLex-999 and WordSim-353.” Data & Knowledge \nEngineering 115 (2018): 174-193. \n[44]        Peters, Matthew E., et al. “Deep Contextualized Word Representations.” Proceedings of the \n2018 Conference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.  \n[45]       Devlin, Jacob, et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language \nUnderstanding.” Proceedings of the 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long \nand Short Papers). 2019. \n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-05-08",
  "updated": "2020-05-08"
}