{
  "id": "http://arxiv.org/abs/2005.04646v4",
  "title": "An FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning",
  "authors": [
    "Hirohisa Watanabe",
    "Mineto Tsukada",
    "Hiroki Matsutani"
  ],
  "abstract": "DQN (Deep Q-Network) is a method to perform Q-learning for reinforcement\nlearning using deep neural networks. DQNs require a large buffer and batch\nprocessing for an experience replay and rely on a backpropagation based\niterative optimization, making them difficult to be implemented on\nresource-limited edge devices. In this paper, we propose a lightweight\non-device reinforcement learning approach for low-cost FPGA devices. It\nexploits a recently proposed neural-network based on-device learning approach\nthat does not rely on the backpropagation method but uses OS-ELM (Online\nSequential Extreme Learning Machine) based training algorithm. In addition, we\npropose a combination of L2 regularization and spectral normalization for the\non-device reinforcement learning so that output values of the neural network\ncan be fit into a certain range and the reinforcement learning becomes stable.\nThe proposed reinforcement learning approach is designed for PYNQ-Z1 board as a\nlow-cost FPGA platform. The evaluation results using OpenAI Gym demonstrate\nthat the proposed algorithm and its FPGA implementation complete a CartPole-v0\ntask 29.77x and 89.40x faster than a conventional DQN-based approach when the\nnumber of hidden-layer nodes is 64.",
  "text": "arXiv:2005.04646v4  [cs.LG]  12 Mar 2023\nAN FPGA-BASED ON-DEVICE REINFORCEMENT LEARNING\nAPPROACH USING ONLINE SEQUENTIAL LEARNING\nA PREPRINT\nHirohisa Watanabe\nKeio University\n3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan\nwatanabe@arc.ics.keio.ac.jp\nMineto Tsukada\nKeio University\n3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan\ntsukada@arc.ics.keio.ac.jp\nHiroki Matsutani\nKeio University\n3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan\nmatutani@arc.ics.keio.ac.jp\nMarch 14, 2023\nABSTRACT\nDQN (Deep Q-Network) is a method to perform Q-learning for reinforcement learning using deep\nneural networks. DQNs require a large buffer and batch processing for an experience replay and\nrely on a backpropagation based iterative optimization, making them difﬁcult to be implemented\non resource-limited edge devices. In this paper, we propose a lightweight on-device reinforcement\nlearning approach for low-cost FPGA devices. It exploits a recently proposed neural-network based\non-device learning approach that does not rely on the backpropagation method but uses OS-ELM\n(Online Sequential Extreme Learning Machine) based training algorithm. In addition, we propose a\ncombination of L2 regularization and spectral normalization for the on-device reinforcement learn-\ning so that output values of the neural network can be ﬁt into a certain range and the reinforcement\nlearning becomes stable. The proposed reinforcement learning approach is designed for PYNQ-Z1\nboard as a low-cost FPGA platform. The evaluation results using OpenAI Gym demonstrate that the\nproposed algorithm and its FPGA implementation complete a CartPole-v0 task 29.77x and 89.40x\nfaster than a conventional DQN-based approach when the number of hidden-layer nodes is 64.\nKeywords Reinforcement learning · FPGA · On-device learning · OS-ELM · Spectral normalization\n1\nIntroduction\nReinforcement learning differs from a typical deep learning in that agents themselves explore their environment and\nlearn appropriate actions. This means that it learns correct actions while creating a dataset. In DQN (Deep Q-Network)\n[1], Q-learning for reinforcement learning is replaced with deep neural networks so that it can acquire a high gener-\nalization capability by the deep neural networks. In this case, continuous input values can be used as inputs. Also,\nto reduce a dependence on a sequence of input data, an experience replay technique [2], in which past experiences\nincluding states, actions, and rewards are recorded in a buffer and then randomly picked up for training, is typically\nused for DQNs. However, such DQNs are costly for resource-limited edge devices and a standalone execution on edge\ndevices is not feasible, because they rely on a backpropagation based training algorithm that iteratively optimizes their\nweight parameters and the convergence is sometimes time-consuming.\nIn this paper, we propose a lightweight on-device reinforcement learning approach for resource-limited FPGA devices.\nIt exploits a recently proposed neural-network based on-device learning approach [3] that does not rely on the back-\npropagation methods but uses OS-ELM (Online Sequential Extreme Learning Machine) based training algorithm [4].\nComputational cost for this training algorithm is quite low, because its weight parameters are analytically solved in a\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\none-shot manner without the backpropagation based iterative optimization. In theory, it has been demonstrated that it\ncan satisfy the universal approximation theorem [5] as in deep learning.\nHowever, since the training algorithm of OS-ELM assumes single hidden-layer neural networks, their output values\ntend to be unstable in some cases, e.g., when they are overﬁt to some speciﬁc inputs and/or when unknown patterns are\nfed. In the case of reinforcement learning, one of crucial issues is that an action acquisition with Q-learning becomes\nunstable. To address this issue, this paper proposes a combination of L2 regularization and spectral normalization\n[6] so that output values of the proposed OS-ELM Q-Network can be ﬁt into a certain range and the reinforcement\nlearning becomes stable. This enables us to implement the reinforcement learning on small-sized FPGA devices for\nstandalone execution on resource-limited edge devices. In this paper, the proposed reinforcement learning approach\nis designed for PYNQ-Z1 board. The evaluation results using OpenAI Gym show that the proposed algorithm and its\nFPGA implementation complete a CartPole task 29.77x and 89.40x faster than a conventional DQN when the number\nof hidden-layer nodes is 64.\nThe rest of this paper is organized as follows. Section 2 introduces basic technologies behind our proposal. Section 3\nproposes the lightweight on-device reinforcement learning approach and illustrates an FPGA implementation. In Sec-\ntion 4, it is evaluated in terms of training curve and execution time to complete a CartPole task. Section 5 summarizes\nthis paper.\n2\nPreliminaries\nThis section introduces (1) ELM (Extreme Learning Machine), (2) OS-ELM (Online Sequential ELM), (3) ReOS-\nELM (Regularized OS-ELM), and (4) DQN (Deep Q-Network).\n2.1\nELM\nELM [7] is a batch training algorithm for single hidden-layer neural networks. In this case, the network consists of\ninput layer, hidden layer, and output layer (see Figure 1 in a few pages later). The numbers of their nodes are n, ˜N,\nand m, respectively.\nAssuming an n-dimensional input chunk x ∈Rk×n with batch size k is given, an m-dimensional output chunk\ny ∈Rk×m is computed as follows.\ny = G(x · α + b)β,\n(1)\nwhere G is an activation function, α ∈Rn× ˜\nN is an input weight matrix between input and hidden layers, β ∈R ˜\nN×m\nis an output weight matrix between hidden and output layers, and b ∈R ˜\nN is a bias vector of the hidden layer.\nAssuming this neural network approximates an m-dimensional target chunk (i.e., teacher data) t ∈Rk×m with zero\nerror, the following equation is satisﬁed.\nG(x · α + b)β = t\n(2)\nHere, the hidden layer matrix is deﬁned as H ≡G(x · α + b). The optimal output weight matrix ˆβ is computed as\nfollows.\nˆβ = H†t,\n(3)\nwhere H† is a pseudo inverse matrix of H, which can be computed with matrix decomposition algorithms, such as\nSVD and QRD (QR Decomposition).\nIn ELM algorithm, the input weight matrix α is initialized with random values and not changed thereafter. The opti-\nmization is thus performed only for the output weight matrix β; thus, it is quite simple compared with backpropagation\nbased neural networks that optimize both α and β. In addition, the training algorithm of ELM is not iterative; it an-\nalytically computes the optimal weight matrix β for a given input chunk in one shot, as shown in Equation 3. That\nis, it can always obtain the optimal β in one shot, unlike a typical gradient descent method that iteratively tunes the\nparameters toward the optimal solution.\nPlease note that ELM is a batch training algorithm and it becomes costly when the training data size grows sequentially.\nThis means that, when a new training data arrives, the whole dataset including the new data must be retrained to update\nthe model. This issue is a limiting factor for reinforcement learning, which can be addressed by OS-ELM.\n2.2\nOS-ELM\nOS-ELM [4] is an online sequential version of ELM, which can update the model sequentially using an arbitrary batch\nsize. Assuming that the i-th training chunk {xi ∈Rki×n, ti ∈Rki×m} with batch size ki is given, we need to\n2\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\ncompute an output weight matrix βi that can minimize the following error.\n\n\n\n\n\nH0\n...\nHi\n\nβi −\n\n\nt0\n...\nti\n\n\n\n\n\n2\n,\n(4)\nwhere Hi is deﬁned as Hi ≡G(xi · α + b).\nAssuming Pi ≡\n\n\n\n\n\n\nH0\n...\nHi\n\n\n⊤\n\nH0\n...\nHi\n\n\n\n\n\n\n−1\n(i ≥0), the optimal output weight matrix is computed as follows.\nPi = Pi−1 −Pi−1H⊤\ni\n\u0000I + HiPi−1H⊤\ni\n\u0001−1 HiPi−1\nβi = βi−1 + PiH⊤\ni (ti −Hiβi−1)\n(5)\nIn particular, the initial values P0 and β0 are precomputed as follows. This computation is called initial training.\nP0 =\n\u0000H⊤\n0 H0\n\u0001−1\nβ0 = P0H⊤\n0 t0\n(6)\nAs shown in Equation 5, the output weight matrix βi and its intermediate result Pi are computed from the previous\ntraining results βi−1 and Pi−1. Thus, OS-ELM can sequentially update the model with a newly-arrived target chunk\nin one shot, and there is no need to retrain all the past data unlike ELM.\nIn this approach, the major bottleneck is the pseudo inverse operation\n\u0000I + HiPi−1H⊤\ni\n\u0001−1 in Equation 5. As\nproposed in [3], the batch size k is ﬁxed at 1 in this paper so that the pseudo inverse operation of k × k matrix for the\nsequential training is replaced with a simple reciprocal operation; thus, we can eliminate SVD or QRD computation\nfrom Equation 5.\n2.3\nReOS-ELM\nReOS-ELM [8] is an OS-ELM variant where an L2 regularization is applied to the output weight matrix β so that\nit can mitigate an overﬁtting issue of OS-ELM and improve its generalization capability. The training algorithm of\nReOS-ELM is same as that of OS-ELM, except that the initial training of P0 and β0 is changed as follows.\nP0 =\n\u0000H⊤\n0 H0 + δI\n\u0001−1\nβ0 = P0H⊤\n0 t0,\n(7)\nwhere δ is a regularization parameter that controls an importance of the regularization term.\n2.4\nReinforcement Learning and DQN\nIn DQNs, deep neural networks are used for Q-learning which is a typical reinforcement learning algorithm. In time\nstep t, Qθ1(st, at) represents a value for taking action at in state st, predicted with a set of neural network parameters\nθ1. In this case, θ1 is trained so that the value Qθ1(st, at) can be predicted accurately by the neural network. However,\nif θ1 is trained for each time step t, it is continuously changed and the Q-learning will not be stable. To address this\nissue, DQNs use a ﬁxed target Q-network technique [9], in which another neural network with a set of parameters θ2\nis used for stabilizing the Q-learning, in addition to that with θ1. More speciﬁcally, θ2 is used but ﬁxed for a while,\nand it is updated with θ1 at a predeﬁned interval.\nIn DQNs, an optimization target is computed as follows.\nf(rt, st+1, dt) = rt + (1 −dt)γ max\na∈A Qθ2 (st+1, a),\n(8)\nwhere γ ∈[0, 1] is a discount rate that controls an importance of the next step, rt is a current reward given by an\nenvironment, and dt indicates if the current episode 1 is ﬁnished or not. If dt is equal to 1, the current episode is\n1In this paper, an episode is deﬁned as a complete sequence of states, actions, and rewards.\n3\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\nFigure 1: Extreme Learning Ma-\nchine\nFigure 2: Simpliﬁed output model (numbers of state variables and actions are 4\nand 2 in this example)\nﬁnished and a new episode is started. As shown in Equation 8, the sum of the reward and the maximum Q-value\namong all the possible actions A in one step ahead is regarded as the optimization target. As mentioned above, θ2\nis periodically updated with θ1 by using the ﬁxed target Q-network technique. Speciﬁcally, the loss value for θ1 is\ndenoted as follows [10].\nL(θ1) =\nE\n(st,at,rt,st+1,dt)∼D\n\"\u0012\nQθ1(st, at) −f(rt, st+1, dt)\n\u00132#\n,\n(9)\nwhere D is a buffer for the experience replay technique [1], which is used to suppress impacts of temporal dependence\non input data for training. In this case, past experiences (e.g., st, at, rt, st+1, and dt in Equation 9) are stored in the\nbuffer D. Then, they are randomly picked up from the buffer to form a batch which will be used for updating the\nweight parameters of the neural network.\n2.5\nSpectral Regularization and Spectral Normalization\nTo stabilize an action acquisition with Q-learning, we focus on regularization methods used in deep learning. Speciﬁ-\ncally, for reinforcement learning, a range of neural network outputs should be within a constant multiplication of their\ninput for the stability. Such a property is referred to as Lipschitz continuity. More speciﬁcally, assuming an input value\nis changed from x1 to x2, their output values f(x1) and f(x2) should satisfy the following constraint.\n∀x1, x2, ∥f(x1) −f(x2)∥≤K∥x1 −x2∥,\n(10)\nwhere K ∈R is a constant value called Lipschitz constant. Lipschitz constant of a neural network is derived by partial\nproducts of Lipschitz constants of all the layers, each of which is equal to a product of Lipschitz constant of a weight\nmatrix (i.e., its largest singular value) and that of an activation function (i.e., ≤1 for ReLU and tanh). It should be\nsuppressed for the stable Q-learning. A spectral regularization [11] can be used to suppress the Lipschitz constant of\na neural network, in which the sum of the largest singular value in each weight matrix is added to the loss function as\na penalty term.\nIn practice, a well-known extension of the spectral regularization is spectral normalization [6], in which an output of\na neural network is computed based on partial products of input data and each weight matrix divided by its largest\nsingular value. In this case, the Lipschitz constant is limited to ≤1. Since 1-Lipschitz continuity is required for GANs\n(Generative Adversarial Networks), it is widely used in these applications. In this paper, we use this approach for\nstabilizing the OS-ELM based reinforcement learning.\n3\nOn-Device Reinforcement Learning Approach\nIn Q-learning, the value Qθ(st, at) is approximated with a neural network. Toward the standalone reinforcement\nlearning on resource-limited edge devices, in this paper we propose to use OS-ELM for this purpose.\n3.1\nBaseline OS-ELM Q-Network\nAlgorithm 1 shows the proposed OS-ELM Q-Network. It consists of four states: Determine, Observe, Store, and\nUpdate.\n4\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\nAlgorithm 1: OS-ELM Q-Network\n1 Initialize parameters θ1 = {α0, β0} using random values R ∈[0, 1]\n2 σmax(α0) ←SVD(α0)\n3 α0 ←α0/σmax(α0) // Initialize α0\n4 Initialize parameters θ2 as θ2 ←θ1\n5 Initialize buffer D\n6 Initialize global step t\n7 for episode ∈1 . . . do\n8\nfor step ∈1 . . . do\n9\nt ←t + 1\n// Determine\n10\nif random value r1 < ε1 then\n11\nat ←arg max\na∈A\nQθ1(st, a)\n12\nelse\n13\nat ←random action value\n// Observe\n14\nObserve (st+1, rt, dt) from environment\n15\nif dt == 1 then\n16\nBreak\n// Store\n17\nStore (st, at, rt, st+1, dt) in buffer D\n// Update\n18\nif t == ˜N then\n19\nRetrieve ∀i ∈[1, ˜N], (si, ai, ri, si+1, di) from buffer D\n20\nUpdate ∀i ∈[1, ˜N], Qθ1(si, ai) to clip(−1, ri + (1 −di)γ maxa∈A Qθ2(si+1, a), 1) // Initialize\nβt\n21\nelse if t > ˜N then\n22\nif random value r2 < ε2 then\n23\nUpdate Qθ1(st, at) to clip(−1, rt + (1 −dt)γ maxa∈A Qθ2(st+1, a), 1) // Update βt\n24\nif episode % UPDAT E_ST EP == 0 then\n25\nθ2 ←θ1\n• In Determine state (lines 10-13), a current action at is determined based on the current state st. More\nspeciﬁcally, an action that maximizes the Q-value (line 11) or randomly-selected one (line 13) is selected as\nat.\n• In Observe state (lines 14-16), based on an interaction using at with the environment, the next state st+1,\nreward rt, and ﬂag dt are observed.\n• In Store state (line 17), these observed values, action at, and state st are stored in buffer D so that they can\nbe used in Update state.\n• In Update state (lines 18-23), β is initialized or updated, depending on the global step t. More speciﬁcally, it\nis initially trained with stored values in D (line 20) based on Equation 6 when the number of experiences in\nD is same as ˜N (i.e., t == ˜N). Or, it is sequentially updated with the latest experience (line 23) based on\nEquation 5 when t > ˜N. The former is referred as an initial training and the latter is referred as a sequential\ntraining.\nPlease note that the buffer D is used for the initial training only and it is not used in subsequent sequential training in\nthe case of OS-ELM Q-Network.\nFixed Target Q-Network\nOS-ELM Q-Network uses the ﬁxed target Q-network technique as well as DQNs. At ﬁrst,\ntwo sets of neural network parameters θ1 and θ2 are initialized in lines 1 and 4. θ1 is updated more frequently (lines 20\nand 23) and θ2 is synchronized with θ1 at a certain interval (lines 24-25). Please note that a straightforward algorithm\n5\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\nthat approximates Q(st, at) with OS-ELM is unstable and cannot complete a reinforcement learning task in this paper.\nWe thus introduce some techniques below in order to improve OS-ELM Q-Network.\nSimpliﬁed Output Model\nIn DQNs, the i-th node of an output layer is Q-value of the i-th action, and they are\ntrained so that the i-th node can predict Q(s, ai) accurately. In this case, their input and output sizes are equal to\nthe numbers of state variables and actions, respectively. The left hand side of Figure 2 shows an example of such a\nnetwork when the numbers of state variables and actions are 4 and 2, respectively. Since an action value is fed to the\nmodel in this case, Q-value is calculated for all the possible actions.\nIn Update state of DQNs, a loss value computed with Equations 8 and 9 is used for the backpropagation based iterative\noptimization. In OS-ELM, on the other hand, teacher data t ∈Rm is required to update β when the batch size k is\n1, as shown in Equation 5. To directly use (rt + (1 −dt) γ maxa∈A Qθ2 (st+1, a)) in Equation 8 to update β, in this\npaper we employ a simpliﬁed output model, which is illustrated in the right hand side of Figure 2. In this model, a\nset of state variables and an action value (e.g., -0.5 for action a0 and 0.5 for action a1) is given as an input and its\ncorresponding Q-value is an output, which is scalar (i.e., m = 1). Thus, (rt + (1 −dt) γ maxa∈A Qθ2 (st+1, a)) can\nbe directly used as a teacher data when updating β in the simpliﬁed output model (lines 20 and 23).\nQ-Value Clipping\nOS-ELM Q-Network tends to be unstable especially when unseen inputs are fed to the network,\nand its output values become anomaly in such cases. Such outliers hinder the reinforcement learning, because these\nvalues are signiﬁcantly large and exceed a range of normal reward values. In a typical setting for the reinforcement\nlearning, the maximum reward given by the environment is 1 and the minimum reward is -1. Thus, as shown in\nlines 20 and 23, output values of OS-ELM Q-Network are clipped so that they are ﬁt into the range of −1 ≤rt +\n(1 −dt)γ maxa∈A Qθ2(st+1, a) ≤1. Such a Q-value clipping suppresses outliers and enables a stable reinforcement\nlearning with OS-ELM Q-Network.\nRandom Update\nDQNs typically train their neural network parameters in a batch manner and use the experience\nreplay technique to form a batch randomly so that it can mitigate a dependence on a sequence of input data. On the\nother hand, OS-ELM is a sequential training algorithm that can update its neural network parameters sequentially with\na small batch size k. As mentioned in Section 2.2, the major bottleneck of OS-ELM when implemented for resource-\nlimited FPGA devices is the pseudo inverse matrix operation that may require an SVD or QRD core. In [3], the pseudo\ninverse matrix operation is eliminated by ﬁxing k to 1 for enabling the neural network based on-device learning. In\nthis paper, to reduce the dependence on a sequence of input data while keeping the small batch size k to 1, we adopt\na method of randomly determining whether or not to update the neural network parameters for each step, as shown in\nlines 22-23. More speciﬁcally, depending on a random value r2, the latest experience (i.e., a set of observed values,\naction at, and state st) is sequentially trained so that the batch size is ﬁxed to 1 and the pseudo inverse matrix operation\ncan be eliminated. Assuming that the ﬁrst initial training is done by software and all the subsequent sequential training\nis computed by the FPGA device (see Figure 3), we can eliminate the buffer D in the FPGA part. Thus, a combination\nof the random update with OS-ELM whose batch size is set to 1 [3] can reduce both computational cost and memory\nusage 2.\n3.2\nOS-ELM Q-Network with Regularization/Normalization\nIn Q-learning, a neural network is updated based on comparisons of an expected value of the reward with the next\nstate; thus, it can be expected that Q-values in successive states are basically close to recent ones. As mentioned in\nSection 2.5, the spectral regularization and normalization would be effective in reinforcement learning for improving\nthe generalization capability. As discussed below, our recommendation is that the spectral normalization and the L2\nregularization are applied to weight parameters α (lines 2-3) and β (line 20), respectively.\nSpectral Normalization for β\nLet us start with the spectral normalization for the weight parameter β of OS-ELM\nQ-Network. Let σmax(βi) is the largest singular value in β at step i. In this case, βi is divided by σmax(βi) for every\nfeedforward operation. To obtain σmax(βi), SVD is typically applied to β for every time, which is a costly operation;\nso, we do not use the spectral normalization for β.\nL2 Regularization for β\nIn this paper, we thus use the L2 regularization for β as an alternative to the spectral\nnormalization for β. In this case, the initial training of Equation 6, which is called from line 20 of Algorithm 1, is\nreplaced with Equation 7. This approach is validated below. Assuming A is a general matrix, the following relation is\n2This approach can mitigate temporal dependency, but the sampling efﬁciency is reduced compared to the experience replay.\n6\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\nFigure 3: On-device reinforcement learning on PYNQ-Z1 platform (Steps 4a and 4c are OS-ELM Q-Network core\nimplemented in FPGA part)\nsatisﬁed.\n∥A∥2\n2 = σ2\nmax(A) ≤∥A∥2\nF =\nX\ni\nσ2\ni (A),\n(11)\nwhere ∥· ∥2 and ∥· ∥F denote a spectral norm and an L2 norm, respectively. As shown in Relation 11, the L2 norm\nintroduces a stronger constraint than the spectral norm [11]. This means that the L2 regularization for βi of OS-ELM\ncan introduce the same or stronger effect of the spectral regularization.\nSpectral Normalization for α\nDifferent from β, weight parameter α of OS-ELM is randomly generated at the\ninitialization step and not changed at runtime. Since the initial values of α can be computed at ofﬂine (e.g., by\nsoftware), the spectral normalization can be easily applied to α, as shown in lines 2-3 of Algorithm 1. By applying the\nspectral normalization for α, the Lipschitz constant depending on α is suppressed within 1 or less; thus, the Lipschitz\nconstant of OS-ELM is σmax(βi) or less. More speciﬁcally, it depends on βi and the L2 regularization parameter δ,\nwhich means that the Lipschitz constant can be controlled by these parameters. As a result, by a combination of the\nspectral normalization for α and the L2 regularization for β, the Lipschitz constant of OS-ELM can be kept under\nσmax(βi). 3\n3.3\nFPGA Implementation\nTable 1 shows the target platform in this paper. Figure 3 shows the design overview of FPGA that consists of CPU\nand FPGA parts. The predict and sequential train modules in Steps 4a and 4c are designed with Xilinx Vivado and\nimplemented in a programmable logic part (denoted as FPGA part) of PYNQ-Z1 platform, while the initial train in\nStep 2c is executed by the CPU part (i.e., Cortex-A9 processor). After the initial train (Step 2c), Steps 4a, 4b, and 4c\nare continuously executed as a main loop. We assume that the interactions with environment (Steps 2b and 4b) are\nhandled by the CPU part.\nA low-cost OS-ELM core optimized to batch size 1 was proposed in [3]. In this paper, we redesigned a further\noptimized core that includes both the predict and sequential train modules (i.e., Steps 4a and 4c) in Verilog HDL, and\nit is implemented for the same FPGA platform as in [3]. The target FPGA device is Xilinx XC7Z020-1CLG400C.\nOperating frequency of the programmable logic part is 100MHz, while the CPU is running at 650MHz. Xilinx Vivado\nv2017.4 is used for the implementation.\nAs shown in the right hand side of Figure 2, in the OS-ELM Q-Network core, its input size (i.e., the number of input-\nlayer nodes) is equal to the sum of the numbers of state variables and a single action variable, which is ﬁve in the\nCartPole-v0 task. The output size is 1, which is a scalar. The number of hidden-layer nodes is varied from 32 to\n256 in the evaluations. The predict and sequential train modules can be implemented with matrix add, mult, and div\noperations. SVD or QRD core is not needed as in [3]. For these matrix operations, only a single set of add, mult,\nand div units is implemented in this design for minimizing the area, but a parallel execution using multiple arithmetic\n3As mentioned in Section 2.5, when ReLU or tanh is used as an activation function, Lipschitz constant of a neural network is\nderived as a partial product of Lipschitz constant of each layer. In this case, Lipschitz constant of the original network without\nregularization/normalization at step i is derived as σmax(α)σmax(βi).\n7\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\nTable 1: Speciﬁcation of target platform\nOS\nPYNQ Linux based on Ubuntu 18.04\nCPU\nCortex-A9 processor (650MHz)\nRAM\nDDR3 SDRAM (512MB)\nFPGA\nZynq XC7Z020-1CLG400C (100MHz)\nTable 2: FPGA resource utilization of OS-ELM Q-Network core\n˜N\nBRAM [%]\nDSP [%]\nFF [%]\nLUT [%]\n32\n2.86\n1.82\n1.49\n3.52\n64\n11.43\n1.82\n2.47\n5.00\n128\n45.71\n1.82\n4.50\n7.93\n192\n91.43\n1.82\n6.44\n11.01\nunits is also possible. We use 32-bit Q20 numbers as a ﬁxed-point number format. Input data, weight parameters α\nand β, and intermediate computation results are stored in on-chip BRAMs. As mentioned in Section 3.1, since the\nﬁxed target Q-network technique is used, two sets of neural network parameters θ1 and θ2 are needed. Speciﬁcally,\nthe same α is used for both θ1 and θ2, while different β is needed for θ1 and θ2; thus, two sets of β are implemented\nin the BRAMs.\nTable 2 shows FPGA resource utilization of the OS-ELM Q-Network core that consists of the predict and sequential\ntrain modules when the number of hidden-layer nodes ˜N is changed from 32 to 256. The largest design with 256\nhidden-layer nodes cannot be implemented for PYNQ-Z1 board due to an excessive BRAM requirement. The other\ndesigns can be ﬁt into the FPGA device. The BRAM utilization is thus a limiting factor, and those of the other\nresources are not high.\n4\nEvaluations\nThe proposed OS-ELM Q-Network is evaluated in terms of the execution time to complete a reinforcement learning\ntask. Its variants with and without the spectral normalization and L2 regularization techniques are compared to a\ntypical DQN.\n4.1\nEvaluation Environment\nAs a reinforcement learning task in this experiment, we use OpenAI Gym CartPole-v0 that tries to make an inverted\npendulum stand longer. As simulation parameters, Cart position, Cart velocity, Pole angle, and Pole velocity at tip are\nset to -2.4 to 2.4, −∞to ∞, -41.8°to 41.8°, and −∞to ∞, respectively. The numbers of state variables and actions\nare 4 and 2, respectively.\nThe following designs are compared in terms of (i) training curve and (ii) average execution time to complete the\nreinforcement learning task. The proposed FPGA design is evaluated in terms of FPGA resource utilization.\n1. OS-ELM: The proposed OS-ELM Q-Network with the ﬁxed target Q-network, simpliﬁed output model, Q-\nvalue clipping, and random update techniques (i.e., the L2 regularization and spectral normalization are not\nincluded)\n2. OS-ELM-L2: The above OS-ELM with the L2 regularization for β\n3. OS-ELM-Lipschitz: The above OS-ELM with the spectral normalization for α\n4. OS-ELM-L2-Lipschitz: The above OS-ELM with the spectral normalization for α and L2 regularization\nfor β\n5. DQN: A three-layer DQN with the ﬁxed target Q-network and experience replay\n6. ELM: The above DQN replaced with ELM with the simpliﬁed output model and Q-value clipping\n7. FPGA: Same as OS-ELM-L2-Lipschitz but its prediction and sequential training parts are implemented in\nprogrammable logic using ﬁxed-point numbers as described in Section 3.3\nWe use ReLU as an activation function. As reinforcement learning parameters, we use the following setting: ε1 = 0.7,\nε2 = 0.5, and UPDAT E_ST EP = 2. As the L2 regularization parameter, δ is set to 1 and 0.5 for OS-ELM-L2\nand OS-ELM-L2-Lipschitz, respectively. In DQN, ε2 is not used, the buffer depth for the experience replay is set\n8\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\nFigure 4: Training curve (light-colored lines: # of steps for continuously standing in each episode; highly-colored\nlines: moving average over last 100 episodes)\nto 10,000, the batch size is set to 32, Adam [12] is used as an optimizer, the learning rate is set to 0.01, and Huber\nfunction [13] is used as a loss function.\n4.2\nTraining Curve\nIn this section, algorithm-level evaluations for the reinforcement learning task are conducted. Among the seven designs\nlisted in Section 4.1, ELM, OS-ELM, OS-ELM-L2, OS-ELM-Lipschitz, OS-ELM-L2-Lipschitz, and DQN are\ncompared 4. They are executed as a software on a 650MHz Cortex-A9 processor of the PYNQ-Z1 board. NumPy\nversion 1.17.2 and Pytorch version 1.3.0 are used for DQN and the ELM/OS-ELM based approaches, respectively.\nIn the designs other than DQN, because their dependence on initial weight parameters are high, unpromising weight\nparameters are reset when a given condition is met. Speciﬁcally, the ELM/OS-ELM based approaches are reset if they\ndid not complete the reinforcement learning task after 300 episodes elapsed.\nFigure 4 illustrates training curves of the six designs when the number of hidden-layer nodes ˜N is varied from 32 to\n192. X-axis shows the number of episodes elapsed, and Y-axis shows the number of continuous steps that the inverted\npendulum is standing (higher is better). There are two line types for each design. Light-colored lines show the number\nof steps for continuously standing in each episode, and highly-colored lines show the moving average over the last\n100 episodes. In these graphs, a representative result is picked up for each design for illustration purpose. Average\nexecution time to complete the task is evaluated in Section 4.3.\nThe upper left graph shows the results when the number of hidden-layer nodes is 32. In this case, in addition to the\nbaseline DQN, the proposed OS-ELM Q-Networks with regularization and/or normalization techniques (OS-ELM-\nL2 and OS-ELM-L2-Lipschitz) acquire better actions that can make the inverted pendulum stand longer. In the case\n4Here, OS-ELM-L2-Lipschitz is corresponding to FPGA. Their difference is that FPGA uses 32-bit Q20 ﬁxed-point numbers,\nbut the negative impact was not signiﬁcant in this experiment.\n9\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\nTable 3: Execution time to complete (breakdown of FPGA) [sec]\n˜N\ntrain_seq\npredict_seq\ntrain_init\npredict_init\nTotal\n32\n7.847\n1.466\n0.023\n0.053\n9.389\n64\n22.458\n2.135\n0.047\n0.067\n24.707\n128\n84.038\n4.036\n0.245\n0.166\n88.484\n192\n218.258\n7.005\n0.685\n0.281\n226.230\nof OS-ELM, on the other hand, as the number of episodes increases, the number of steps for continuously standing is\ngetting worse. This result demonstrates that the Q-value clipping technique is not sufﬁcient for the stable reinforcement\nlearning and the regularization and/or normalization techniques are required.\nThe reinforcement learning is stable in OS-ELM-L2-Lipschitz that uses both the L2 regularization and spectral nor-\nmalization. In this case, a generalization capability is improved by the L2 regularization and an output range is limited\nby the spectral normalization. That is, the L2 regularization works directly on weight parameters β which are updated\nat each step. The spectral normalization affects α so that an output value range of OS-ELM-L2-Lipschitz is less than\nor equal to σmax(β); thus, outliers due to α values can be suppressed by the spectral normalization. Please note that\neven if rewards of OS-ELM-L2-Lipschitz are declined once, it can recover the situation and then get right actions.\nThe upper right graph shows the results when the number of hidden-layer nodes is 64. A similar tendency mentioned\nabove is observed in this case too, but ELM also acquires correct actions, because it is expected that this conﬁguration\n( ˜N = 64) is best suited for ELM.\nThe lower two graphs show the results when the numbers of hidden-layer nodes are 128 and 192. These results\nare similar. Only DQN and the proposed OS-ELM-L2-Lipschitz can acquire correct actions. OS-ELM-L2 and\nOS-ELM-Lipschitz fail to learn correct actions, indicating that using either the L2 regularization or the spectral\nnormalization is not sufﬁcient. In summary, OS-ELM-L2-Lipschitz can avoid the overﬁtting situation and acquire\ncorrect actions thanks to the constraints on both α and β.\n4.3\nExecution Time to Complete\nWe evaluate the seven designs in terms of execution times to complete the CartPole-v0 task when the number of\nhidden-layer nodes ˜N is varied from 32 to 192. In this evaluation, an execution was terminated as “impossible” if it\ncould not complete the task after 50,000 episodes. As a result, OS-ELM and OS-ELM-Lipschitz could not complete\nthe task in our evaluation. Also, ELM was not stable. Figure 5 shows the execution times of OS-ELM-L2, OS-ELM-\nL2-Lipschitz, DQN, and FPGA. DQN is separated in the graph since its execution time is quite large compared to\nthe others. Table 3 shows detailed breakdown of the proposed FPGA design.\nIn these graphs, each bar shows the execution time breakdown of each operation: train_seq, predict_seq, train_init,\npredict_init, train_DQN, predict_1, and predict_32.\n• In the OS-ELM based approaches except for FPGA, train_init and train_seq indicate their initial training\nand sequential training, respectively. predict_init and predict_seq are their predictions before and after their\ninitial training is completed, respectively. All these operations are done by the CPU part.\n• In the proposed FPGA, before the initial training, train_init and predict_init (Steps 2a and 2c) are executed\nby the CPU part. After the initial training, train_seq and predict_seq (Steps 4a and 4c) are done by the FPGA\npart. PS and PL parts are connected via AXI bus and DMA transfer is used for their communication though\nnot fully implemented in our design. We assume data transfer latency between the CPU and FPGA parts is\n1 cycle per ﬂoat32. This is an optimistic assumption, but we use this value for simplicity because it varies\ndepending on an underlying hardware platform (e.g., DMA performance).\n• In the baseline DQN, train_DQN is its training. predict_1 and predict_32 indicate its predictions when the\nbatch sizes are 1 and 32, respectively. More speciﬁcally, predict_1 and predict_32 are called from Determine\nand Update states, respectively. All the operations are done by the CPU part.\nExecution time for interactions with a given environment (Steps 2b and 4b) is not considered in this evaluation.\ntrain_init and predict_init exist but are negligible.\nThe breakdown of each operation is computed by (the number of executions of the operation) × (execution time of\nthe single operation). train_seq is dominant compared to train_init because train_init is executed only once for each\nepisode. These execution times are averaged over 150 trials.\n10\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\n(a) OS-ELM based approaches\n(b) DQN\nFigure 5: Execution time to complete [sec]\nWhen the number of hidden-layer nodes is 32, OS-ELM-L2, OS-ELM-L2-Lipschitz, DQN, and FPGA can acquire\ncorrect actions. Their execution times are 132.27sec, 55.02sec, 3232.54sec, and 9.39sec, respectively. When the\nnumber of hidden-layer nodes is 64, OS-ELM-L2, OS-ELM-L2-Lipschitz, DQN, and FPGA can acquire correct\nactions. Their executions times are 647.56sec, 74.20sec, 2208.90sec, and 24.71sec, respectively. The execution\ntimes of OS-ELM-L2, OS-ELM-L2-Lipschitz, and FPGA are increased compared to their previous result having\n32 hidden-layer nodes because of a larger matrix size. In this case, OS-ELM-L2, OS-ELM-L2-Lipschitz, and the\nproposed FPGA are faster than DQN by 3.41x, 29.77x, and 89.40x, respectively.\nThese results demonstrate that FPGA is the fastest followed by OS-ELM-L2-Lipschitz and DQN, because update\nformula of the OS-ELM based approaches is simple as shown in Equations 5 and 7. Although FPGA and OS-ELM-\nL2-Lipschitz use the same algorithm, FPGA is faster, because train_seq and predict_seq are accelerated by dedicated\ncircuits, as shown in Figure 3. Regarding the performance bottleneck, the OS-ELM based approaches spend most of\ntime for train_seq, while DQN spends a certain time for train_DQN, predict_1, and predict_32. As mentioned above,\nthe execution times tend to increase as the number of hidden-layer nodes is increased except for DQN. This is because\nthe size of matrix products is denoted as R ˜\nN× ˜\nN · R ˜\nN× ˜\nN, and the computation cost increases rapidly as the number\nof hidden-layer nodes is increased. Such matrix products can be accelerated efﬁciently by dedicated logic; thus, the\nproposed FPGA design is advantageous for the on-device reinforcement learning on resource-limited edge devices.\n5\nSummary\nTo solve reinforcement learning tasks on resource-limited edge devices, in this paper, we proposed OS-ELM Q-\nNetwork as a lightweight reinforcement learning algorithm that do not rely on a backpropagation based iterative\noptimization. More speciﬁcally, the following techniques were proposed for OS-ELM Q-Network: (1) simpliﬁed out-\nput model, (2) Q-value clipping, (3) random update, and (4) combination of the spectral normalization for α and L2\nregularization for β. Especially, thanks to (4), the Lipschitz constant of OS-ELM can be suppressed under σmax(β)\nand further controlled by adjusting the parameter δ.\nOS-ELM Q-Network with all the above techniques was designed for PYNQ-Z1 board as a low-cost FPGA platform by\nextending an existing on-device learning core [3]. Prediction and sequential training in most of Determine and Update\nstates (i.e., predict_seq and train_seq) are accelerated by the FPGA part, and the others are executed by the CPU part.\nThe evaluation results using OpenAI Gym demonstrated that the proposed OS-ELM-L2-Lipschitz and its FPGA\nimplementation complete a CartPole-v0 task 29.77x and 89.40x faster than a conventional DQN-based approach when\nthe number of hidden-layer nodes is 64. Also, they are robust against the number of hidden-layer nodes thanks to (4).\n11\nAn FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning\nA PREPRINT\nReferences\n[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller, “Playing Atari\nwith Deep Reinforcement Learning,” arXiv:1312.5602, Dec 2013.\n[2] L.-J. Lin, “Reinforcement Learning for Robots Using Neural Networks,” Ph.D. dissertation, Carnegie Mellon\nUniversity, USA, Jan 1993.\n[3] M. Tsukada, M. Kondo, and H. Matsutani, “A Neural Network-Based On-device Learning Anomaly Detector\nfor Edge Devices,” IEEE Transactions on Computers, vol. 69, no. 7, pp. 1027–1044, Jul 2020.\n[4] N.-Y. Liang, G.-B. Huang, P. Saratchandran, and N. Sundararajan, “A Fast and Accurate Online Sequential\nLearning Algorithm for Feedforward Networks,” IEEE Transactions on Neural Networks, vol. 17, no. 6, pp.\n1411–1423, Nov 2006.\n[5] K. Hornik, M. Stinchcombe, and H. White, “Multilayer Feedforward Networks are Universal Approximators,”\nNeural Networks, vol. 2, no. 5, pp. 359 – 366, Jul 1989.\n[6] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “Spectral Normalization for Generative Adversarial Net-\nworks,” in Proceedings of the International Conference on Learning Representations (ICLR’18), Feb 2018.\n[7] G.-B. Huang, Q.-Y. Zhu, and C.-K. Siew, “Extreme Learning Machine: A New Learning Scheme of Feedforward\nNeural Networks,” in Proceedings of the International Joint Conference on Neural Networks (IJCNN’04), Jul\n2004, pp. 985–990.\n[8] H. T. Huynh and Y. Won, “Regularized Online Sequential Learning Algorithm for Single-Hidden Layer Feedfor-\nward Neural Networks,” Pattern Recognition Letters, vol. 32, no. 14, pp. 1930 – 1935, Oct 2011.\n[9] V. Mnih et al., “Human-Level Control through Deep Reinforcement Learning,” Nature, vol. 518, no. 7540, pp.\n529–533, Feb 2015.\n[10] J. Achiam, “Spinning Up in Deep Reinforcement Learning,” https://github.com/openai/spinningup,\n2018.\n[11] Y. Yoshida and T. Miyato, “Spectral Norm Regularization for Improving the Generalizability of Deep Learning,”\narXiv:1705.10941, May 2017.\n[12] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,” in Proceedings of the International\nConference on Learning Representations (ICLR’15), May 2015.\n[13] P. J. Huber, “Robust Estimation of a Location Parameter,” Annals of Mathematical Statistics, vol. 35, no. 1, pp.\n73–101, Mar 1964.\n12\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-05-10",
  "updated": "2023-03-12"
}