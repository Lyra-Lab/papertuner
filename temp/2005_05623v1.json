{
  "id": "http://arxiv.org/abs/2005.05623v1",
  "title": "Unsupervised Multi-label Dataset Generation from Web Data",
  "authors": [
    "Carlos Roig",
    "David Varas",
    "Issey Masuda",
    "Juan Carlos Riveiro",
    "Elisenda Bou-Balust"
  ],
  "abstract": "This paper presents a system towards the generation of multi-label datasets\nfrom web data in an unsupervised manner. To achieve this objective, this work\ncomprises two main contributions, namely: a) the generation of a low-noise\nunsupervised single-label dataset from web-data, and b) the augmentation of\nlabels in such dataset (from single label to multi label). The generation of a\nsingle-label dataset uses an unsupervised noise reduction phase (clustering and\nselection of clusters using anchors) obtaining a 85% of correctly labeled\nimages. An unsupervised label augmentation process is then performed to assign\nnew labels to the images in the dataset using the class activation maps and the\nuncertainty associated with each class. This process is applied to the dataset\ngenerated in this paper and a public dataset (Places365) achieving a 9.5% and\n27% of extra labels in each dataset respectively, therefore demonstrating that\nthe presented system can robustly enrich the initial dataset.",
  "text": "Unsupervised Multi-label Dataset Generation from Web Data\nCarlos Roig\nDavid Varas\nIssey Masuda\nJuan Carlos Riveiro\nElisenda Bou-Balust\nVilynx Inc.\n{carlos,david.varas,issey,eli}@vilynx.com\nAbstract\nThis paper presents a system towards the generation of\nmulti-label datasets from web data in an unsupervised man-\nner. To achieve this objective, this work comprises two main\ncontributions, namely: a) the generation of a low-noise un-\nsupervised single-label dataset from web-data, and b) the\naugmentation of labels in such dataset (from single label to\nmulti label). The generation of a single-label dataset uses\nan unsupervised noise reduction phase (clustering and se-\nlection of clusters using anchors) obtaining a 85% of cor-\nrectly labeled images. An unsupervised label augmentation\nprocess is then performed to assign new labels to the images\nin the dataset using the class activation maps and the uncer-\ntainty associated with each class. This process is applied\nto the dataset generated in this paper and a public dataset\n(Places365) achieving a 9.5% and 27% of extra labels in\neach dataset respectively, therefore demonstrating that the\npresented system can robustly enrich the initial dataset.\n1. Introduction\nDeep learning methods are widely used for many com-\nputer vision tasks. Among these tasks, image classiﬁcation\n-which consists on assigning a label to each image- is one of\nthe most researched by the community due to their myriad\napplications. To perform this task, a dataset consisting of\nimages and their corresponding labels is required. Common\ndatasets for image classiﬁcation are MNIST (handwritten\ndigits) [5], ImageNet (object categories) [4] (e.g. ostrich,\nkite or snail), Places365 (scene categories) [30] (e.g. bed-\nroom, coast or downtown) or LFW (person identities) [13].\nThe aforementioned datasets have one thing in com-\nmon: they are manually generated. Curating a dataset is\na very expensive and time-consuming task due to a) the\nnumber of samples required for classiﬁcation using Deep\nLearning (e.g.\nImageNet consists on 1.6M images) and\nFigure 1: The label augmentation pipeline presented in this\nwork is an unsupervised approach for generating multiple\nlabels using a single labeled dataset. A convolutional neu-\nral network is used to localize salient regions in the image,\ntheir associated probability scores and class uncertainty to\naccurately predict multiple labels for an image.\nb) the necessity to involve ﬁeld experts to manually (or\nsemi-automatically) decide which label corresponds to each\nsample. Because of this, the number of datasets available\nis limited. While the available datasets might be enough\nfor research, their size and lack of diversity is currently\nprecluding the growth of industrial applications. This has\ncaused that nowadays industrial image classiﬁcation is lim-\nited to big corporations who can afford the generation of\nsuch datasets.\nUnsupervised dataset generation from Web Data is fore-\nseen as a solution to mitigate this problem. By using image\nsearch engines, it is possible to gather samples that corre-\nspond to a text query. Moreover, the images can include\nmetadata information which can be used to retrieve the most\nrelevant samples for a given task. This allows the genera-\ntion of datasets in a faster and more cost-effective way[19].\nHowever, due to the nature of web data, such datasets usu-\n1\narXiv:2005.05623v1  [cs.CV]  12 May 2020\nally contain a lot of noisy samples. If the input query is not\nspeciﬁc enough or the number of required samples is too\nlarge, the amount of noisy samples could become exponen-\ntially larger.\nThe noisy samples in unsupervised datasets can be di-\nvided into two groups: 1) incorrectly labeled images (im-\nages with a wrong label) and 2) images partially labeled\n(images with a correct label, but lacking other labels that\ncould also have). This paper presents a novel approach to\nmitigate both types of noise.\nMany approaches have been proposed in the literature to\ndeal with the problem of incorrectly labeled/noisy samples\n[16, 24]. However, these approaches rely on the images\nhaving attached metadata, which is not always available. In\nthis work we present a method to generate an unsupervised\ndataset with noise reduction using only the ranking of the\nsamples coming from their corresponding web searches.\nTo mitigate the second type of noise, the incomplete la-\nbeling of images, some approaches have been found in the\nliterature [6]. However, this problem still remains unsolved,\nbecause the previous approach requires to iterate multiple\ntimes over the dataset to learn new missing labels.\nThis paper proposes a novel system that is able to out-\nput multiple labels from a single-label dataset in an unsu-\npervised manner (Figure 1), therefore providing label aug-\nmentation for samples that are incompletely labeled (single-\nlabeled images).\nOur contribution in this paper is twofold:\n1. A model to unsupervisedly generate a single-label\ndataset using web data with a noise reduction mech-\nanism.\n2. A model to provide unsupervised label augmentation,\nto mitigate the incomplete-labeling noise.\nThis paper is structured as follow: Section 2 reviews pre-\nvious work in unsupervised dataset generation and noise\nreduction, emphasizing the differentiation between previ-\nous work and the main contributions presented in this pa-\nper. Section 3 analyzes the proposed solution and system\noverview. In section 4, the implementation details of such\nsystem are described to encourage reproducibility. Quan-\ntitative and qualitative results obtained from this work (il-\nlustrated with both Places365 and our custom dataset) are\npresented in Section 5. Finally, Section 6 concludes with a\nreview of the work done, the insights obtained during the\ndevelopment of this work and future lines of research.\n2. Related work\nThe work presented in this paper involves three areas of\nresearch: a) Dataset generation from web data (and differ-\nent approaches to deal with incorrectly labeled samples), b)\nlearning from noisy labeled data and (c) label augmenta-\ntion (learning multiple classes from single labeled images\nto deal with incomplete labeling noise).\nDataset generation from web data is a very relevant\nﬁeld of research due to the cost of manually generated\ndatasets from scratch. Moreover, thanks to the huge amount\nof partially annotated data that can be gathered from the\nweb, the applications of web generated datasets are ubiq-\nuitous. Many of the most relevant datasets in computer vi-\nsion have been generated by harvesting information from\nthe web: Image datasets (ImageNet [4], Places365 [30],\nSUN397 [27], PASCAL VOC [7], MS COCO [20] or Open\nImages [17]), video datasets (Youtube-8M [1], ActivityNet\n[8], AVA [11] or Kinetics [15]) and audio event recognition\ndatasets (AudioSet [10]). The primary data source for all\nthe aforementioned datasets is web data. However, all of\nthem have been manually annotated and cleaned to some\nextent (are not unsupervised).\nOther datasets generated from web data are completely\nunsupervised, such as WebVision [19], consisting on 5\nthousand visual concepts mined from public sources reach-\ning a total of 16 million noisy images. This dataset is gath-\nered by using textual search queries in image search en-\ngines. WebVision, while extremely valuable because of the\nnumber of samples and the minimum supervision required,\nhas still some noise. TourTheWorld [28] is also generated\nin an unsupervised manner. In this dataset, labels are gener-\nated for landmark images based on their location metadata.\nThis is done by creating clusters of spatially nearby pictures\nand then clustering the visual features of each sample for\npruning purposes. However, while this process relieves the\nneed of manually annotating the samples, it does not pro-\nvide name labels corresponding to each cluster of nearby\nsamples. Therefore, the problem of autonomous generation\nof unsupervised datasets with named classes remains still\nunsolved.\nPrevious work from the authors, WorldLocations [21],\naddresses this challenge by introducing a step of unsuper-\nvised classname-annotation and noise reduction. This re-\nsulted in a landmarks dataset with named labels that did not\nrequire any human supervision or cleaning. The ﬁst contri-\nbution of this work tackles the generalization of WorldLo-\ncations for its usage with other types of web data.\nBesides the generation of new datasets in unsupervised\nways, many works have focused on learning discriminat-\ning visual features based on web data using deep learn-\ning techniques. Chen and Gupta [3] crawled images from\nthousands of labels in order to train a CNN for object detec-\ntion and localization that matched the results of a network\ntrained with ImageNet. In [23], Sun et al. studied the dif-\nferences between web and standard datasets and then used\nweb data to assist the training of networks for image clas-\nsiﬁcation and localization. Krause et al. [16] outperformed\nFigure 2: Proposed system. The ﬁrst part of the process, Single-label Dataset Generation, uses search engines to ﬁnd images\nbased on queries associated with the dataset labels. Metadata associated with each image is used to select a set of anchors\nand to eliminate noisy samples. This procedure results in a single-label dataset. Multi-label augmentation is then performed\nusing this dataset. In the process, a classiﬁcation layer is trained to do multi-class predictions, by using features extracted\nfrom the image, class scores and uncertainties.\nseveral ﬁne-grained recognition task by increasing the train-\ning samples using noisy web data. Similarly, Joulin et al.\n[14] used captioned images in order to learn text relation-\nships through the visual features of the images mined. All\nthese works prove that web data can be used to train mod-\nels that perform well compared to models trained only with\nhand-crafted datasets.\nMoreover, more recent works analyze the usage of noisy\ndata to assist the training of many vision tasks. Sun et al.\n[22] used the JFT-300M dataset to evaluate the performance\nof current vision tasks, leading to state-of-the-art results in\nimage classiﬁcation, object detection, semantic segmenta-\ntion and pose estimation. Guo et al. [12] used the idea of\ncurriculum learning for their CurriculumNet. By measur-\ning data complexity using cluster densities, they were able\nto implement a training schedule that led them to achieve\nstate-of-the-art results in WebVision, Imagenet, Clothing-\n1M [18] and Food-101 [2]. In this context, this work ad-\ndresses the training of a neural network using an unsuper-\nvised dataset generated from web data.\nMulti-Label Image Recognition is one of the pivotal\nchallenges in computer vision, because of the multi-label\nnature of real-world images (these images always contain\ndifferent labels to be recognized). In order to address this\nproblem, Wang et al. [26] proposed using an LSTM sub-\nnetwork to generate new regions within an image and the\nrelations between these regions. In [32], Zhu et al. ex-\nploited semantic and spatial information using a deep neu-\nral network that captured the relations between them, allow-\ning them to predict multiple labels. Ge et al. [9] presented\na weakly supervised approach to curriculum learning, that\nﬁrst obtains intermediate object localizations and pixel la-\nbels to then perform task-speciﬁc trainings. The previous\nworks relies on attention mechanisms and region proposals\nto identify new labels. All the aforementioned works rely\non poor baselines, which makes it very difﬁcult to compare\nthe different proposed solutions. Wang et al.. In [25] Liu et\nal. addressed this issue by studying data augmentation tech-\nniques and ensembles for multi-label classiﬁcation, used to\nreview three datasets for benchmarking. This work used a\nweakly supervised detection model that was used to distill\ninformation for training the classiﬁcation network. Finally,\nDurand et al. [6] proposed a new classiﬁcation loss that uses\nthe amount of known labels for learning with partially la-\nbeled datasets in an iterative way. Even though the research\ncommunity is very active in multi-label image classiﬁca-\ntion, there is a lack of major multi-label image datasets for\nsolving these problems. Moreover, to the author’s knowl-\nedge there has not been any multi-label dataset generated in\nan unsupervised manner. To this end, the work presented in\nthis paper addresses the unsupervised generation of a multi-\nlabel image dataset through label augmentation.\n3. System Overview\nIn this section, the proposed system for multi-label\ndataset generation is presented.\nThe approach described\nin this paper is composed by two main modules: First, a\nsemi-supervised single-label dataset generation system that,\ngiven a set of queries, extracts images from web data and\nassigns them a label. The output of this system (a single-\nlabel dataset) is the input of the second module, which aims\nto generate multiple labels for images in an unsupervised\nmanner. The complete pipeline is illustrated in Figure 2.\n3.1. Single-label dataset generation from web data\nThe pipeline starts with the set of K labels that represent\nthe classes of the desired dataset. These labels are queried\ninto an image search engine. Due to the noisy nature of the\nresulting images, a cleaning step is further required before\nusing these images to train a classiﬁer. In order to perform\nthis process, an initial seed consisting on a set of anchors is\ngenerated. These anchors correspond to the ﬁrst retrieved\nimages in the aforementioned queries as they usually corre-\nspond to the most reliable results. These anchors are later\nused in the cluster selection step.\nThen, a feature extractor module is used to obtain a rep-\nFigure 3: Architecture of the multi-label prediction system. First, a feature map is extracted from the image using a neural\nnetwork. This map is used to compute the label scores, class uncertainties and Class Activation Maps (CAMs). Then,\nmultiple regions are proposed using the scores and CAMs. Finally, class uncertainty is used to select multiple labels.\nresentation of the downloaded images. Any kind of feature\nextractor could be used as long as it is capable to provide\na reliable semantic representation of the image. In the pro-\nposed system, a ResNet-50 trained with ImageNet is used\nfor this purpose.\nThe feature extraction step is followed by a clustering\nwhich aims to create an over-segmentation of the data in\norder to select those clusters that better represent the desired\nclass. The features of the anchor images are not considered\nin the clustering process. Finally, a cluster selection step\nis performed computing the distances between the anchors\nand the centroids of the clusters. The clustering is proposed\nin contrast of an exhaustive comparison feature-to-anchor\nas it helps to quantize the feature space and thus reduce the\nnoise added by spurious. This selection is key for reducing\nthe noise in the dataset.\n3.2. Unsupervised label augmentation\nDatasets for image classiﬁcation tasks are usually anno-\ntated with the predominant class in the image [4, 30, 19].\nHowever, in real images, more than one class from the\ndataset may be visible in the scene. Two main problems\narise in this situation: First, the model penalizes the pre-\ndiction of correct visible labels during training when these\nlabels are not the predominant class in the scene. Second,\ndatasets containing classes which are hyperonyms of other\nclasses (i.e. stadium and football stadium) only consider as\na correct label a single semantic level while information at\ndifferent levels could be exploited.\nThese two problems appear due to the fact that informa-\ntion associated with classes in the dataset is not fully ex-\nploited for training. Despite having the visual representa-\ntion of several classes in images that do not contain their\nlabel, they have not been annotated with multiple labels due\nto the high cost of this process.\nIn this section, the generation of new labels for annotated\nimages is presented (Figure 3). Although the objective of\nthis work is to generate a multi-label annotation from sin-\ngle labeled images from web data, this process may be ex-\ntended to any number of input and output labels using the\nsame technique. New labels assigned to images are trans-\nferred from existing classes in the dataset. Moreover, when\nnew classes are added to the dataset, their labels may be\neasily transferred to the previous dataset images using this\nprocess.\nFirst, a convolutional neural network is trained for the\ntask of single label classiﬁcation using the initial dataset. A\nsingle classiﬁer per class is trained for this purpose using\nbinary cross entropy loss LCE with sigmoid as activation\nfunction. Following [29], we introduce a term σk in the loss\nto model the uncertainty of the prediction for the kth class:\nL =\nX\nk\n1\n2σ2\nk\nLCE\nk\n+ log σk\n(1)\nUsing this loss function, the model learns to predict the Het-\neroscedastic Uncertainty for each class. This type of uncer-\ntainty depends on the input data and is predicted as a deter-\nministic mapping from inputs to model outputs. As it can\nbe observed, when the model predicts something wrong, it\nlearns to attenuate the residual term by increasing the un-\ncertainty of the class σ2\nk. This information is further used to\ndiscard predictions with large uncertainty for the model.\nThe backbone of the network is used as a feature extrac-\ntor. Then, a Global Average Pooling (GAP) and a classi-\nﬁcation layer with sigmoid activations Ws that outputs the\nnumber of desired classes are stacked on top of the back-\nbone. In this work, GAP is used as its loss encourages the\nnetwork to identify the extent of the objects in the image\nin contrast with Global Max Pooling (GMP), which encour-\nages the system to identify just one discriminative part [29].\nThis is because the average of the feature map is performed,\nso the maximum can be obtained by ﬁnding all the discrim-\ninative parts of the class because all low activations reduce\nthe output of a particular feature map. On the other hand,\nlow scores for all image regions except the most discrimi-\nnative one do not impact the score in the GMP.\nIn order to assign new labels to a given image, several\nconsiderations should be taken into account. First, the class\nshould be visible with a minimum degree of quality. In\nother words, the area of the class must be larger than a cer-\ntain threshold. Second, a single class may be assigned to\neach region of the image. As a consequence, overlapping\nclasses should be suppressed. Third, the model should be\nconﬁdent about its prediction to give a new label to an im-\nage.\nTaking this into account, for each image in the dataset,\nthe Class Activation Map (CAM) of all the classes are used\nto generate new labels. The score associated with a given\nclass k is computed as:\nSk =\nX\ni\nwk\ni\nX\nx,y\nfi(x, y) =\nX\nx,y\nX\ni\nwk\ni fi(x, y)\n(2)\nwhere fi is the ith slice of the feature volume map extracted\nfrom the last convolutional layer of the model and wk\ni is the\nith weight of the classiﬁcation layer of class k. Then, the\nCAM associated with this class is deﬁned as:\nCAMk(x, y) =\nX\ni\nwk\ni fi(x, y)\n(3)\nEach position of the CAM represents the pixel probabil-\nity of belonging to a given class. Then, the kth class is lo-\ncalized in the image by ﬁnding those pixels of CAMk with\na probability larger than Tp.\nA Non Maximum Supression (NMS) step is adopted on\nthe regions with large probability extracted from the CAMs.\nIn the case of two or more regions sharing an area larger\nthan a given IoU threshold TIoU, the region belonging to\nthe class with larger score Sk is kept. Labels with at least\none non suppressed region after this process are assigned to\nthe image under analysis.\nPrecision\nLabels added\nCustom dataset\n83.5%\n9.5%\nPlaces365\n81%\n27%\nTable 1: Results of the multi-label stage. Precision of the\nnew labels has been measured together with the number of\nnew labels added\n4. Implementation details\nIn this section, several details related with the implemen-\ntation of the label augmentation pipeline are presented in\norder to encourage reproducibility.\nFigure 4: Examples of multi-labeled images of our custom\ndataset. The ﬁrst row shows the original image. Second row\nshows the CAMs for the original label whereas third row\nshows correct (green) and incorrect (red) generated labels\nwith their CAMs\nThe backbone network used in the feature extraction step\nin Section 3.2 is a ResNet-50 trained with Places365. The\nfeature maps are extracted from the last layer of this net-\nwork before classiﬁcation.\nThese maps have a shape of\n7x7x2048, due to the ﬁxed input image size of 224x224.\nThe feature map is resized using a bilinear interpola-\ntion. Ideally, this interpolation should be performed to ﬁt\nthe original size of the image. As the processing time of\nthis operation grows exponentially with the size of the re-\nsulting map, it is performed in two steps. First, the feature\nmap is resized to 63x63x2048, from which CAMs are ex-\ntracted. Then, CAMs are resized to the input image shape.\nThe classiﬁcation and uncertainty layers are trained fol-\nlowing the loss in Equation 1. These layers have output size\n96 and 365 for our custom dataset and Places365 respec-\ntively. The training process uses the same data augmenta-\ntion techniques that are used in [30].\n5. Results\nIn this section, a set of experiments is presented in order\nto validate the proposed method. These experiments have\nbeen conducted using the settings presented in Section 4.\nFirst, the Single-label Dataset Generation process is as-\nsessed. To this end, a single-label dataset has been gener-\nated using the aforementioned pipeline for evaluation pur-\nposes. This dataset is composed by 250K images that be-\nlong to 96 classes associated with different types of scenes\nand places.\nThe quality of the automatically generated\nsingle-label dataset has been evaluated selecting a random\nsubset of the images and manually assessing whether the\nlabel associated with each image belongs to the requested\nclass or not. The accuracy given by such pipeline is 85%.\nIn order to assess the Multi-label Data Augmentation\nprocess, two datasets have been used. The ﬁrst dataset is\ncreated using the Single-label Dataset Generation described\nbefore. Also, we used Places365 [31], which is a publicly\navailable single-label dataset that contains several images\nwith more than one visible class. This dataset has been\nselected to allow further comparison with other methods.\nBoth datasets have been evaluated taking a random subset\nof the predicted new labels and manually assessing them.\nThe results can be found in Table 1.\nAs it can be observed, the proposed system increases the\nnumber of labels with a large precision. This increase de-\npends on the nature of the dataset. The results of Table\n1 show that the number of labels added for the Places365\ndataset is larger than the labels generated for the custom\ndataset.\nVisually inspecting the dataset leads to an un-\nderstanding that Places365 contains more images where a\nclass-collision happens, meaning that more than one class\nshould have been annotated but due to the intrinsic single-\nlabel nature of the dataset, only one was taken into account.\nIn contrast, in the custom dataset, as the number of classes\nis smaller than in Places365, fewer classes collide and thus\nthe number of labels added is not as big as in the later.\n5.1. Qualitative results\nIn Figure 4, some example images from our custom\ndataset and their corresponding labels are shown. For each\nimage, the CAMs of the annotated and generated labels are\npresented. Moreover, some examples of correct and incor-\nrect label augmentation are shown. As it can be observed,\nthe highlighted regions of the correct examples focus on re-\ngions that visually represent the generated label. The in-\ncorrect examples show views that are not common for the\ndifferent classes or patterns that are visually similar to other\nclasses from the dataset (e.g. the paw in the ﬁeld of the uni-\nversity campus has colors and lines that are mistaken with a\nbaseball ﬁeld).\nSeveral examples showing images from Places365\ndataset can be found in Figure 5. The correct examples\nshow similar patterns to the examples presented in Figure\n4. Moreover, in some cases, the new labels are even more\naccurate than the original annotation of the image (e.g. the\nimage with a car interior label is more accurate than the\nauto factory label). The incorrect examples show bias prob-\nlems associated with biases in some classes, where several\npatterns may be over-represented (e.g. ﬂorist shop appear\nwhen the network ﬁnds ﬂower patterns).\n6. Conclusions\nUnsupervised datasets generated from web data are fore-\nseen as key in image classiﬁcation due to the volume of\nsamples required in such problems. However, these datasets\nhave some level of noise. This work has addressed two\ntypes of noise present in unsupervised datasets generated\nfrom web data, namely: a) the incorrect labeling of images\nand b) the incomplete labeling of images.\nThis work starts by presenting a system to generate a\nsingle-label dataset from web images in an unsupervised\nFigure 5: Examples of the Places365 dataset. The ﬁrst row\nshows the original image. Second row shows the CAMs for\nthe original label whereas third row shows correct (green)\nand incorrect (red) generated labels with their CAMs\nmanner.\nThe noise of this dataset is drastically reduced\nby performing a clustering step and a selection of clusters\nusing anchors (therefore reducing the incorrect labeling of\nimages). The presented system has been used to generate a\ndataset resulting in 250K images for 96 classes with only a\n15% of incorrectly labeled images. The percentage of cor-\nrect images in the dataset presented in this paper (85%) is\nnotable when compared to WebVision (between 66% and\n80% correctly labeled images).\nThe second contribution of this paper is aimed towards\nsolving the incomplete labeling problem. In this context,\nthis paper presents a novel system to autonomously aug-\nment the number of labels of any dataset by assigning la-\nbels to the corresponding images using the class activation\nmaps and the uncertainty associated with each class. This\nsystem can be used to perform label augmentation in either\nsingle-label or multi-label datasets. To demonstrate the ef-\nfectiveness of this system, this pipeline has been applied\nto the previously generated dataset and to Places365 (for\nreproducibility). Results have shown the addition of 9.5%\nand 27% of labels in each dataset respectively, demonstrat-\ning the applicability of this system for label augmentation\n(to enrich the initial dataset and describe additional visual\ninformation present on the scene).\nReferences\n[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,\nB. Varadarajan, and S. Vijayanarasimhan.\nYoutube-8m:\nA large-scale video classiﬁcation benchmark.\nCoRR,\nabs/1609.08675, 2016. 2\n[2] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101 –\nmining discriminative components with random forests. In\nEuropean Conference on Computer Vision, 2014. 3\n[3] X. Chen and A. Gupta. Webly supervised learning of convo-\nlutional networks. CoRR, abs/1505.01554, 2015. 2\n[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR09, 2009. 1, 2, 4\n[5] L. Deng. The mnist database of handwritten digit images for\nmachine learning research [best of the web]. IEEE Signal\nProcessing Magazine, 29(6):141–142, Nov 2012. 1\n[6] T. Durand, N. Mehrasa, and G. Mori. Learning a deep con-\nvnet for multi-label classiﬁcation with partial labels. CoRR,\nabs/1902.09720, 2019. 2, 3\n[7] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\nWilliams, J. Winn, and A. Zisserman. The pascal visual ob-\nject classes challenge: A retrospective. International Journal\nof Computer Vision, 111(1):98–136, Jan. 2015. 2\n[8] B. G. Fabian Caba Heilbron, Victor Escorcia and J. C.\nNiebles. Activitynet: A large-scale video benchmark for hu-\nman activity understanding. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n961–970, 2015. 2\n[9] W. Ge, S. Yang, and Y. Yu. Multi-evidence ﬁltering and fu-\nsion for multi-label classiﬁcation, object detection and se-\nmantic segmentation based on weakly supervised learning.\nCoRR, abs/1802.09129, 2018. 3\n[10] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen,\nW. Lawrence, R. C. Moore, M. Plakal, and M. Ritter. Audio\nset: An ontology and human-labeled dataset for audio events.\nIn Proc. IEEE ICASSP 2017, New Orleans, LA, 2017. 2\n[11] C. Gu, C. Sun, S. Vijayanarasimhan, C. Pantofaru, D. A.\nRoss, G. Toderici, Y. Li, S. Ricco, R. Sukthankar, C. Schmid,\nand J. Malik. AVA: A video dataset of spatio-temporally lo-\ncalized atomic visual actions. CoRR, abs/1705.08421, 2017.\n2\n[12] S. Guo, W. Huang, H. Zhang, C. Zhuang, D. Dong,\nM. R. Scott, and D. Huang.\nCurriculumnet: Weakly su-\npervised learning from large-scale web images.\nCoRR,\nabs/1808.01097, 2018. 3\n[13] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller.\nLabeled Faces in the Wild: A Database forStudying Face\nRecognition in Unconstrained Environments. In Workshop\non Faces in ’Real-Life’ Images: Detection, Alignment, and\nRecognition, Marseille, France, Oct. 2008. Erik Learned-\nMiller and Andras Ferencz and Fr´ed´eric Jurie. 1\n[14] A. Joulin, L. van der Maaten, A. Jabri, and N. Vasilache.\nLearning visual features from large weakly supervised data.\nCoRR, abs/1511.02251, 2015. 3\n[15] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier,\nS. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev,\nM. Suleyman, and A. Zisserman. The kinetics human action\nvideo dataset. CoRR, abs/1705.06950, 2017. 2\n[16] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev,\nT. Duerig, J. Philbin, and F. Li.\nThe unreasonable effec-\ntiveness of noisy data for ﬁne-grained recognition. CoRR,\nabs/1511.06789, 2015. 2\n[17] A. Kuznetsova, H. Rom, N. Alldrin, J. R. R. Uijlings,\nI. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci,\nT. Duerig, and V. Ferrari. The open images dataset V4: uni-\nﬁed image classiﬁcation, object detection, and visual rela-\ntionship detection at scale. CoRR, abs/1811.00982, 2018. 2\n[18] J. Li, Y. Wong, Q. Zhao, and M. S. Kankanhalli. Learning to\nlearn from noisy labeled data. CoRR, abs/1812.05214, 2018.\n3\n[19] W. Li, L. Wang, W. Li, E. Agustsson, and L. V. Gool. Webvi-\nsion database: Visual learning and understanding from web\ndata. CoRR, abs/1708.02862, 2017. 1, 2, 4\n[20] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B.\nGirshick, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and\nC. L. Zitnick. Microsoft COCO: common objects in context.\nCoRR, abs/1405.0312, 2014. 2\n[21] C. Roig, D. Varas, I. Masuda, M. Sarmiento, G. Floriach,\nJ. Espadaler, J. C. Riveiro, and E. Bou-Balust. Unsupervised\nlarge-scale world locations dataset. 2018. 2\n[22] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisit-\ning unreasonable effectiveness of data in deep learning era.\nCoRR, abs/1707.02968, 2017. 3\n[23] X. Sun, L. Zheng, Y. Lai, and J. Yang. Learning from web\ndata: the beneﬁt of unsupervised object localization. CoRR,\nabs/1812.09232, 2018. 2\n[24] X. Sun, L. Zheng, Y.-K. Lai, and J. Yang. Learning from web\ndata: the beneﬁt of unsupervised object localization. arXiv\npreprint arXiv:1812.09232, 2018. 2\n[25] Q. Wang, N. Jia, and T. P. Breckon. A baseline for multi-\nlabel image classiﬁcation using ensemble deep CNN. CoRR,\nabs/1811.08412, 2018. 3\n[26] Z. Wang, T. Chen, G. Li, R. Xu, and L. Lin. Multi-label\nimage recognition by recurrently discovering attentional re-\ngions. CoRR, abs/1711.02816, 2017. 3\n[27] J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba. Sun\ndatabase: Large-scale scene recognition from abbey to zoo.\npages 3485–3492, 06 2010. 2\n[28] Y. Zheng, M. Zhao, Y. Song, H. Adam, U. Buddemeier,\nA. Bissacco, F. Brucher, T.-S. Chua, and H. Neven. Tour the\nworld: Building a web-scale landmark recognition engine,\n06 2009. 2\n[29] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-\nralba. Learning deep features for discriminative localization.\nIn IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2016. 4\n[30] B. Zhou, A. Khosla, A. Lapedriza, A. Torralba, and A. Oliva.\nPlaces: An Image Database for Deep Scene Understanding.\nArXiv e-prints, Oct. 2016. 1, 2, 4, 5\n[31] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.\nLearning deep features for scene recognition using places\ndatabase. In Advances in neural information processing sys-\ntems, pages 487–495, 2014. 5\n[32] F. Zhu, H. Li, W. Ouyang, N. Yu, and X. Wang.\nLearn-\ning spatial regularization with image-level supervisions for\nmulti-label image classiﬁcation.\nCoRR, abs/1702.05891,\n2017. 3\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-05-12",
  "updated": "2020-05-12"
}