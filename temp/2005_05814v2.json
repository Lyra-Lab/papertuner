{
  "id": "http://arxiv.org/abs/2005.05814v2",
  "title": "A Report on the 2020 Sarcasm Detection Shared Task",
  "authors": [
    "Debanjan Ghosh",
    "Avijit Vajpayee",
    "Smaranda Muresan"
  ],
  "abstract": "Detecting sarcasm and verbal irony is critical for understanding people's\nactual sentiments and beliefs. Thus, the field of sarcasm analysis has become a\npopular research problem in natural language processing. As the community\nworking on computational approaches for sarcasm detection is growing, it is\nimperative to conduct benchmarking studies to analyze the current\nstate-of-the-art, facilitating progress in this area. We report on the shared\ntask on sarcasm detection we conducted as a part of the 2nd Workshop on\nFigurative Language Processing (FigLang 2020) at ACL 2020.",
  "text": "A Report on the 2020 Sarcasm Detection Shared Task\nDebanjan Ghosh1, Avijit Vajpayee1, and Smaranda Muresan2\n1 Educational Testing Service\n2Data Science Institute, Columbia University\n{dghosh, avajpayee}@ets.org\nsmara@columbia.edu\nAbstract\nDetecting sarcasm and verbal irony is critical\nfor understanding peoples actual sentiments\nand beliefs. Thus, the ﬁeld of sarcasm analysis\nhas become a popular research problem in nat-\nural language processing. As the community\nworking on computational approaches for sar-\ncasm detection is growing, it is imperative to\nconduct benchmarking studies to analyze the\ncurrent state-of-the-art, facilitating progress in\nthis area. We report on the shared task on sar-\ncasm detection we conducted as a part of the\n2nd Workshop on Figurative Language Pro-\ncessing (FigLang 2020) at ACL 2020.\n1\nIntroduction\nSarcasm and verbal irony are a type of ﬁgurative\nlanguage where the speakers usually mean the op-\nposite of what they say. Recognizing whether a\nspeaker is ironic or sarcastic is essential to down-\nstream applications for correctly understanding\nspeakers’ intended sentiments and beliefs. Con-\nsequently, in the last decade, the problem of irony\nand sarcasm detection has attracted a considerable\ninterest from computational linguistics researchers.\nThe task has been usually framed as a binary clas-\nsiﬁcation task (sarcastic vs. non-sarcastic) using\neither the utterance in isolation or adding contex-\ntual information such as conversation context, au-\nthor context, visual context, or cognitive features\n(Davidov et al., 2010; Tsur et al., 2010; Gonz´alez-\nIb´a˜nez et al., 2011; Riloff et al., 2013; Maynard\nand Greenwood, 2014; Wallace et al., 2014; Ghosh\net al., 2015; Joshi et al., 2015; Muresan et al., 2016;\nAmir et al., 2016; Mishra et al., 2016; Ghosh and\nVeale, 2017; Felbo et al., 2017; Ghosh et al., 2017;\nHazarika et al., 2018; Tay et al., 2018; Oprea and\nMagdy, 2019; Majumder et al., 2019; Castro et al.,\n2019; Ghosh et al., 2019).\nIn this paper, we report on the shared task on\nsarcasm detection that we conducted as part of the\nTurns\nMessage\nContext1\nThe [govt] just conﬁscated a $180\nmillion boat shipment of cocaine\nfrom drug trafﬁckers.\nContext2\nPeople think 5 tonnes is not a lot of\ncocaine.\nResponse Man, I’ve seen more than that on a\nFriday night!\nTable 1: Sarcastic replies to conversation context in\nReddit. Response turn is a reply to Context2 turn\nthat is a reply to Context1 turn\n2nd Workshop on Figurative Language Processing\n(FigLang 2020) at ACL 2020. The task aims to\nstudy the role of conversation context for sarcasm\ndetection. Two types of social media content are\nused as training data for the two tracks - microblog-\nging platform such as Twitter and online discussion\nforum such as Reddit.\nTable 1 and Table 2 show examples of three turn\ndialogues, where Response is the sarcastic reply.\nWithout using the conversation context Contexti,\nit is difﬁcult to identify the sarcastic intent ex-\npressed in Response. The shared task is designed\nto benchmark the usefulness of modeling the en-\ntire conversation context (i.e., all the prior dialogue\nturns) for sarcasm detection.\nSection 2 discusses the current state of research\non sarcasm detection with a focus on the role of\ncontext. Section 3 provides a description of the\nshared task, datasets, and metrics. Section 4 con-\ntains brief summaries of each of the participating\nsystems whereas Section 5 reports a comparative\nevaluation of the systems and our observations\nabout trends in designs and performance of the\nsystems that participated in the shared task.\narXiv:2005.05814v2  [cs.CL]  4 Jun 2020\nTurns\nMessage\nContext1\nThis is the greatest video in the his-\ntory of college football.\nContext2\nHes gonna have a short career if he\nkeeps smoking . Not good for your\nhealth\nResponse Awesome !!! Everybody does it.\nThats the greatest reason to do\nsomething.\nTable 2: Sarcastic replies to conversation context in\nTwitter. Response turn is a reply to Context2 turn\nthat is a reply to Context1 turn\n2\nRelated Work\nA considerable amount of work on sarcasm de-\ntection has considered the utterance in isolation\nwhen predicting the sarcastic or non-sarcastic la-\nbel. Initial approaches used feature-based machine\nlearning models that rely on different types of fea-\ntures from lexical (e.g., sarcasm markers, word\nembeddings) to pragmatic such as emoticons or\nlearned patterns of contrast between positive senti-\nment and negative situations (Davidov et al., 2010;\nVeale and Hao, 2010; Gonz´alez-Ib´a˜nez et al., 2011;\nLiebrecht et al., 2013; Riloff et al., 2013; Maynard\nand Greenwood, 2014; Joshi et al., 2015; Ghosh\net al., 2015; Ghosh and Muresan, 2018). Recently,\ndeep learning methods have been applied for this\ntask (Ghosh and Veale, 2016; Tay et al., 2018). For\nexcellent surveys on sarcasm and irony detection\nsee (Wallace, 2015; Joshi et al., 2017).\nHowever, when recognizing sarcastic intent even\nhumans have difﬁculties sometimes when consider-\ning an utterance in isolation (Wallace et al., 2014).\nRecently an increasing number of researchers have\nstarted to explore the role of contextual informa-\ntion for irony and sarcasm analysis. The term con-\ntext loosely refers to any information that is avail-\nable beyond the utterance itself (Joshi et al., 2017).\nA few researchers have examined author context\n(Bamman and Smith, 2015; Khattri et al., 2015;\nRajadesingan et al., 2015; Amir et al., 2016; Ghosh\nand Veale, 2017), multi-modal context (Schifanella\net al., 2016; Cai et al., 2019; Castro et al., 2019),\neye-tracking information (Mishra et al., 2016), or\nconversation context (Bamman and Smith, 2015;\nWang et al., 2015; Joshi et al., 2016; Zhang et al.,\n2016; Ghosh et al., 2017; Ghosh and Veale, 2017).\nRelated to shared tasks on ﬁgurative language\nanalysis, recently, Van Hee et al. (2018) have con-\nducted a SemEval task on irony detection in Twit-\nter focusing on utterances in isolation. Besides the\nbinary classiﬁcation task of identifying the ironic\ntweet the authors also conducted a multi-class irony\nclassiﬁcation to identify the speciﬁc type of irony:\nwhether it contains verbal irony, situational irony,\nor other types of irony. In our case, the current\nshared task aims to study the role of conversation\ncontext for sarcasm detection. In particular, we\nfocus on benchmark the effectiveness of modeling\nthe conversation context (e.g., all the prior dialogue\nturns or a subset of the prior dialogue turns) for sar-\ncasm detection.\n3\nTask Description\nThe design of our shared task is guided by two\nspeciﬁc issues. First, we plan to leverage a particu-\nlar type of context — the entire prior conversation\ncontext — for sarcasm detection. Second, we plan\nto investigate the systems’ performance on conver-\nsations from two types of social media platforms:\nTwitter and Reddit. Both of these platforms allow\nthe writers to mark whether their messages are sar-\ncastic (e.g., #sarcasm hashtag in Twitter and “/s”\nmarker in Reddit).\nThe competition is organized in two phases:\ntraining and evaluation. By making available com-\nmon datasets and frameworks for evaluation, we\nhope to contribute to the consolidation and strength-\nening of the growing community of researchers\nworking on computational approaches to sarcasm\nanalysis.\n3.1\nDatasets\n3.1.1\nReddit Training Dataset\nKhodak et al. (2017) introduced the self-annotated\nReddit Corpus which is a very large collection of\nsarcastic and non-sarcastic posts (over one million)\ncurated from different subreddits such as politics,\nreligion, sports, technology, etc. This corpus con-\ntains self-labeled sarcastic posts where users label\ntheir posts as sarcastic by marking “/s” to the end of\nsarcastic posts. For any such sarcastic post, the cor-\npus also provides the full conversation context, i.e.,\nall the prior turns that took place in the dialogue.\nWe select the training data for the Reddit track\nfrom Khodak et al. (2017). We considered a couple\nof criteria. First, we choose sarcastic responses\nwith at least two prior turns. Note, for many re-\nsponses in our training corpus the number of turns\nis much more. Second, we curated sarcastic re-\nsponses from a variety of subreddits such that no\nsingle subreddit (e.g., politics) dominates the train-\ning corpus. In addition, we avoid responses from\nsubreddits that we believe are too speciﬁc and nar-\nrow (e.g., subreddit dedicated to a speciﬁc video\ngame) that might not generalize well. The non-\nsarcastic partition of the training dataset is collected\nfrom the same set of subreddits that are used to\ncollect sarcastic responses. We ﬁnally end up in\nselecting 4,400 posts (as well as their conversation\ncontext) for the training dataset equally balanced\nbetween sarcastic and non-sarcastic posts.\n3.1.2\nTwitter Training Dataset\nFor the Twitter dataset, we have relied upon the\nannotations that users assign to their tweets using\nhashtags. The sarcastic tweets were collected us-\ning hashtags: #sarcasm and #sarcastic. As non-\nsarcastic utterances, we consider sentiment tweets,\ni.e., we adopt the methodology proposed in related\nwork (Muresan et al., 2016). Such sentiment tweets\ndo not contain the sarcasm hashtags but include\nhashtags that contain positive or negative senti-\nment words. The positive tweets express direct\npositive sentiment and they are collected based on\ntweets with positive hashtags such as #happy, #love,\n#lucky. Likewise, the negative tweets express di-\nrect negative sentiment and are collected based on\ntweets with negative hashtags such as #sad, #hate,\n#angry. Classifying sarcastic utterances against\nsentiment utterances is a considerably harder task\nthan classifying against random objective tweets\nsince many sarcastic utterances also contain senti-\nment terms. Here, we are relying on self-labeled\ntweets, thus, it is always possible that sarcastic\ntweets were mislabeled with sentiment hashtags or\nusers did not use the #sarcasm hashtag at all. We\nmanually evaluated around 200 sentiment tweets\nand found very few such cases in the training cor-\npus. Similar to the Reddit dataset we apply a cou-\nple of criteria while selecting the training dataset.\nFirst, we select sarcastic or non-sarcastic tweets\nonly when they appear in a dialogue (i.e., begins\nwith “@”-user symbol) and at least have two or\nmore prior turns as conversation context. Second,\nfor the non-sarcastic posts, we maintain a strict\nupper limit (i.e., not-greater than 10%) for any sen-\ntiment hashtag. Third, we apply heuristics such as\navoiding short tweets, discarding tweets with only\nmultiple URLs, etc. We end up selecting 5,000\ntweets for training balanced between sarcastic and\nnon-sarcastic tweets.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nlength=2\nlength=3\nlength=4\nlength=5\nlength>5\nFigure 1: Plot of Reddit (blue) and Twitter (orange)\ntraining datasets on the basis of context length. X-axis\nrepresents context length (i.e., number of prior turns)\nand Y-axis represents the % of training utterances.\nFigure 1 presents a plot of number of training\nutterances on the basis of context length, for Red-\ndit and Twitter tracks respectively. We notice, al-\nthough the numbers are comparable for utterances\nwith context length equal to two or three, for Twit-\nter corpus, utterances with a higher number of con-\ntext (i.e., prior turns) is much higher.\n3.1.3\nEvaluation Data\nThe Twitter data for evaluation is curated similarly\nto the training data. For Reddit, we do not use\nKhodak et al. (2017) rather collected new sarcastic\nand non-sarcastic responses from Reddit. First, for\nsarcastic responses we utilize the same set of sub-\nreddits utilized in the training dataset, thus, keeping\nthe same genre between the evaluation and train-\ning. For the non-sarcastic partition, we utilized the\nsame set of subreddits and submission threads as\nthe sarcastic partition. For both tracks the evalu-\nation dataset contains 1800 instances partitioned\nequally between the sarcastic and the non-sarcastic\ncategories.\n3.2\nTraining Phase\nIn the ﬁrst phase, data is released for training and/or\ndevelopment of sarcasm detection models (both\nReddit and Twitter). Participants can choose to\npartition the training data further to a validation\nset for preliminary evaluations and/or tuning of\nhyper-parameters. Likewise, they can also elect to\nperform cross-validation on the training data.\n3.3\nEvaluation Phase\nIn the second phase, instances for evaluation are\nreleased. Each participating system generated pre-\ndictions for the evaluation instances, for up to N\nmodels.\n1 Predictions are submitted to the Co-\ndaLab site and evaluated automatically against the\ngold labels. CodaLab is an established platform to\norganize shared-tasks (Leong et al., 2018) because\nit is easy to use, provides easy communication with\nthe participants (e.g., allows mass-emailing) as well\nas tracks all the submissions updating the leader-\nboard in real-time. The metrics used for evaluation\nis the average F1 score between the two categories\n- sarcastic and non-sarcastic. The leaderboards dis-\nplayed the Precision, Recall, and F1 scores in the\ndescending order of the F1 scores, separately for\nthe two tracks - Twitter and Reddit.\n4\nSystems\nThe shared task started on January 19, 2020, when\nthe training data was made available to all the regis-\ntered participants. We released the evaluation data\non February 25, 2020. Submissions were accepted\nuntil March 16, 2020. Overall, we received an\noverwhelming number of submissions: 655 for the\nReddit track and 1070 for the Twitter track. The\nCodaLab leaderboard showcases results from 39\nsystems for the Reddit track and 38 systems for the\nTwitter track, respectively. Out of all submissions,\n14 shared task system papers were submitted. In\nthe following section we summarize each system\npaper. We also put forward a comparative analy-\nsis based on their performance and the choice of\nfeatures/models in Section 5. Interested readers\ncan refer to the individual teams’ papers for more\ndetails. But ﬁrst, we discuss the baseline classiﬁca-\ntion model that we used.\n4.1\nBaseline Classiﬁer\nWe use prior published work as the baseline that\nused conversation context to detect sarcasm from\nsocial media platforms such as Twitter and Reddit\n(Ghosh et al., 2018). Ghosh et al. (2018) proposed a\ndual LSTM architecture with hierarchical attention\nwhere one LSTM models the conversation context\nand the other models sarcastic response. The hier-\narchical attention (Yang et al., 2016) implements\ntwo levels of attention – one at the word level and\nanother at the sentence level. We used their system\nbased on only the immediate conversation context\n(i.e., the immediate prior turn). 2 This is denoted\nas LSTMattn in Table 3 and Table 4.\n1N is set to 999.\n2https://github.com/Alex-Fabbri/deep_\nlearning_nlp_sarcasm\n4.2\nSystem Descriptions\nWe describe the participating systems in the follow-\ning section (in alphabetical order).\nabaruah (Baruah et al., 2020):\nFine-tuned a\nBERT model (Devlin et al., 2018) and reported\nresults on varying maximum sequence length (cor-\nresponding to varying level of context inclusion\nfrom just response to entire context). They also\nreported results of BiLSTM with FastText embed-\ndings (of response and entire context) and SVM\nbased on char n-gram features (again on both re-\nsponse and entire context). One interesting result\nwas SVM with discrete features performed bet-\nter than BiLSTM. They achieved best results with\nBERT on response and most immediate context.\nad6398 (Kumar and Anand, 2020):\nReport re-\nsults comparing multiple transformer architectures\n(BERT, SpanBERT (Joshi et al., 2020), RoBERTa\n(Liu et al., 2019)) both in single sentence classi-\nﬁcation (with concatenated context and response\nstring) and sentence pair classiﬁcation (with con-\ntext and response being separate inputs to a\nSiamese type architecture). Their best result was\nwith using RoBERTa + LSTM model.\naditya604 (Avvaru et al., 2020):\nUsed BERT on\nsimple concatenation of last-k context texts and\nresponse text. The authors included details of data\ncleaning (de-emojiﬁcation, hashtag text extraction,\napostrophe expansion) as well experiments on other\narchitectures (LSTM, CNN, XLNet (Yang et al.,\n2019)) and varying size of context (5, 7, complete)\nin their report. The best results were obtained by\nBERT with 7 length context for Twitter dataset and\nBERT with 5 context for Reddit dataset.\namitjena40 (Jena et al., 2020):\nUsed a time-\nseries analysis inspired approach for integrating\ncontext. Each text in conversational thread (con-\ntext and response) was individually scored using\nBERT and Simple Exponential Smoothing (SES)\nwas utilized to get probability of ﬁnal response be-\ning sarcastic. They used the ﬁnal response label\nas a pseudo-label for scoring the context entries,\nwhich is not theoretically grounded. If ﬁnal re-\nsponse is sarcastic, the previous context dialogue\ncannot be assumed to be sarcastic (with respect to\nits preceding dialogue). However, the effect of this\nerror is attenuated due to exponentially decreasing\ncontribution of context to ﬁnal label under SES\nscheme.\nRank Lb.\nRank\nTeam\nP\nR\nF1\nApproach\n1\n1\nmiroblog\n0.834\n0.838\n0.834\nBERT + BiLSTM + NeXtVLAD + Context En-\nsemble + Data Augmentation\n2\n2\nandy3223\n0.751\n0.755\n0.750\nRoBERTa-Large (all the prior turns)\n3\n6\ntaha\n0.738\n0.739\n0.737\nBERT+ Local Context Focus\n4\n8\ntanvidadu\n0.716\n0.718\n0.716\nRoBERTa-Large (last two prior turns)\n5\n9\nnclabj\n0.708\n0.708\n0.708\nRoBERTa + Multi-Initialization Ensemble\n6\n12\nad6398\n0.693\n0.699\n0.691\nRoBERTa + LSTM\n7\n16\nkalaivani.A 0.679\n0.679\n0.679\nBERT (isolated response)\n8\n17\namitjena40 0.679\n0.683\n0.678\nTorchMoji + ELMO + Simple Exp. Smoothing\n9\n21\nburtenshaw 0.67\n0.677\n0.667\nEnsemble of SVM, LSTM, CNN-LSTM, MLP\n10\n26\nsalokr\n0.641\n0.643\n0.639\nBERT + CNN + LSTM\n11\n31\nadithya604 0.605\n0.607\n0.603\nBERT (concatenation of prior turns and response)\n12\n-\nbaseline\n0.600\n0.599\n0.600\nLSTMattn\n13\n32\nabaruah\n0.595\n0.605\n0.585\nBERT-Large (concatenation of response and its\nimmediate prior turn)\nTable 3: Performance of the best system per team and baseline for the Reddit track. We include two ranks - ranks\nfrom the submitted systems as well as the Leaderboard ranks from the CodaLab site\nAnandKumaR (Khatri and P, 2020):\nExperi-\nmented with using traditional ML classiﬁers like\nSVM and Logisitic Regression over embeddings\nthrough BERT and GloVe (Pennington et al., 2014).\nUsing BERT as a feature extraction method as op-\nposed to ﬁne-tuning it was not beneﬁcial and Lo-\ngisitic Regression over GloVe embeddings outper-\nformed them in their experiment. Context was used\nin their best model but no details were available\nabout the depth of context usage (full vs. imme-\ndiate). Additionally, they only experimented with\nTwitter data and no submission was made to the\nReddit track. They provided details of data clean-\ning measures for their experiments which involved\nstopword removal, lowercasing, stemming, punctu-\nation removal and spelling normalization.\nandy3223\n(Dong\net\nal.,\n2020):\nUsed\nthe\ntransformer-based architecture for sarcasm detec-\ntion, reporting the performance of three architec-\nture, BERT, RoBERTa, and ALBERT (Lan et al.,\n2019). They considered two models, the target-\noriented where only the target (i.e., sarcastic re-\nsponse) is modeled and context-aware, where the\ncontext is also modeled with the target. The authors\nconducted extensive hyper-parameter search, and\nset the learning rate to 3e-5, the number of epochs\nto 30, and use different seed values, 21, 42, 63, for\nthree runs. Additionally, they set the maximum\nsequence length 128 for the target-oriented models\nwhile it is set to 256 for the context-aware models.\nburtenshaw (Lemmens et al., 2020):\nEm-\nployed an ensemble of four models - LSTM (on\nword, emoji and hashtag representations), CNN-\nLSTM (on GloVe embeddings with discrete punc-\ntuation and sentiment features), MLP (on sentence\nembeddings through Infersent (Conneau et al.,\n2017)) and SVM (on character and stylometric fea-\ntures). The ﬁrst three models (except SVM) used\nthe last two immediate contexts along with the re-\nsponse.\nduke DS (Gregory et al., 2020):\nHere the au-\nthors have conducted extensive set of experiments\nusing discrete features, DNNs, as well as trans-\nformer models, however, reporting only the results\non the Twitter track. Regarding discrete features,\none of novelties in their approach is including a\npredictor to identify whether the tweet is political\nor not, since many sarcastic tweets are on political\ntopics. Regarding the models, the best performing\nmodel is an ensemble of ﬁve transformers: BERT-\nbase-uncased, RoBERTa-base, XLNet-base-cased,\nRoBERTa-large, and ALBERT-base-v2.\nkalaivani.A\n(kalaivani\nA\nand\nD,\n2020):\nCompared traditional machine learning clas-\nsiﬁers\n(e.g.,\nLogistic\nRegression/Random\nForest/XGBoost/Linear SVC/ Gaussian Naive\nBayes) on discrete bag-of-word features/Doc2Vec\nfeatures with LSTM models on Word2Vec\nembeddings (Mikolov et al., 2013) and BERT\nmodels.\nFor context usage they report results\non using isolated response, isolated context and\ncontext-response combined (unclear as to how\ndeep the context usage is). The best performance\nfor their experiments was by BERT on isolated\nresponse.\nmiroblog (Lee et al., 2020):\nImplemented a clas-\nsiﬁer composed of BERT followed by BiLSTM and\nNeXtVLAD (Lin et al., 2018) (a differentiable pool-\ning mechanism which empirically performed better\nthan Mean/Max pooling).\n3 They employed an\nensembling approach for including varying length\ncontext and reported that gains in F1 after context\nof length three are negligible. Just with these two\ncontributions alone, their model outperformed all\nothers. Additionally, they devised a novel approach\nof data augmentation (i.e., Contextual Response\nAugmentation) from unlabelled conversational con-\ntexts based on next sentence prediction conﬁdence\nscore of BERT. Leveraging large-scale unlabelled\nconversation data from web, their model outper-\nformed the second best system by 14% and 8.4%\nfor Twitter and Reddit respectively (absolute F1\nscore).\nnclabj (Jaiswal, 2020):\nUsed a majority-voting\nensemble of RoBERTa models with different\nweight-initialization and different levels of context\nlength. Their report shows that previous 3 turns\nof dialogues had the best performance in isolation.\nAdditionally, the present results comparing other\nsentence embedding architectures like Universal\nSentence Encoder (Cer et al., 2018), ELMo (Peters\net al., 2018) and BERT.\nsalokr/vaibhav (Srivastava et al., 2020) :\nEm-\nployed a CNN-LSTM based architecture on BERT\nembeddings to utilize the full context thread and\nthe response. The entire context after encoding\nthrough BERT is passed through CNN and LSTM\nlayers to get a representation of the context. Con-\nvolution and dense layers over this summarized\ncontext representation and BERT encoding of re-\nsponse make up the ﬁnal classiﬁer.\ntaha (ataei et al., 2020):\nReported experiments\ncomparing SVM on character n-gram features,\nLSTM-CNN models, Transformer models as well\nas a novel usage of aspect based sentiment clas-\nsiﬁcation approaches like Interactive Attention\n3VLAD is an acronym of “Vector of Locally Aggregated\nDescriptors” (Lin et al., 2018).\nNetworks(IAN) (Ma et al., 2017), Local Context\nFocus(LCF)-BERT (Zeng et al., 2019) and BERT-\nAttentional Encoder network (AEN) (Song et al.,\n2019). For aspect based approaches, they viewed\nthe last dialogue of conversational context as aspect\nof the target response. LCF-BERT was their best\nmodel for the Twitter task but due to computational\nresource limitations they were not able to try it for\nReddit task (where BERT on just the response text\nperformed best).\ntanvidadu (Dadu and Pant, 2020):\nFine-tuned\nRoBERTa-large model (355 Million parameters\nwith over a 50K vocabulary size) on response and\nits two immediate contexts. They reported results\non three different types of inputs: response-only\nmodel, concatenation of immediate two context\nwith response, and using an explicit separator token\nbetween the response and the ﬁnal context. The\nbest result is reported in the setting where they used\nthe separation token.\n5\nResults and Discussions\nTable 3 and Table 4 present the results for the Red-\ndit track and the Twitter track, respectively. We\nshow the rank of the submitted systems (best result\nfrom their submitted reports) both in terms of the\nsystem submissions (out of 14) as well as their rank\non the Codalab leaderboard. Note, for a couple of\nentries we observe a discrepancy between their best\nreported system(s) and the leaderboard entries. For\nthe sake of fairness, for such cases, we selected the\nleaderboard entries to present in Table 3 and Table\n4. 4\nAlso, out of the 14 system descriptions duke DS\nand AnadKumR report the performance on the\nTwitter dataset, only. For overall results on both\ntracks, we observe majority of the models out-\nperformed the LSTMattn baseline (Ghosh et al.,\n2018). Almost all the submitted systems have used\nthe transformer-architecture that seems to perform\nbetter than RNN-architecture, even without any\ntask-speciﬁc ﬁne-tuning. Although most of the\nmodels are similar and perform comparably, we\nobserve a particular system - miroblog - has out-\nperformed the other models in both the tracks by\nposting an improvement over the 2nd ranked sys-\ntem by more than 7% F1-score in the Reddit track\nand by 14% F1-score in the Twitter track.\n4Also, for such cases (e.g., abaruah, under the Approach\ncolumn we reported the approach described in the system\npaper that is not necessarily reﬂect the scores of Table 3.\nRank Lb.\nRank\nTeam\nP\nR\nF1\nApproach\n1\n1\nmiroblog\n0.932\n0.936\n0.931\nBERT + BiLSTM + NeXtVLAD + Context En-\nsemble + Data Augmentation\n2\n2\nnclabj\n0.792\n0.793\n0.791\nRoBERTa + Multi-Initialization Ensemble\n3\n3\nandy3223\n0.791\n0.794\n0.790\nRoBERTa-Large (all the prior turns)\n4\n5\nad6398\n0.773\n0.774\n0.772\nRoBERTa + LSTM\n5\n6\ntanvidadu\n0.772\n0.772\n0.772\nRoBERTa-Large (last two prior turns)\n6\n8\nduke DS\n0.758\n0.767\n0.756\nEnsemble of Transformers\n7\n11\namitjena40 0.751\n0.751\n0.750\nTorchMoji + ELMO + Simple Exp. Smoothing\n8\n13\nsalokr\n0.742\n0.746\n0.741\nBERT + CNN + LSTM\n9\n16\nburtenshaw 0.741\n0.746\n0.740\nEnsemble of SVM, LSTM, CNN-LSTM, MLP\n10\n21\nabaruah\n0.734\n0.735\n0.734\nBERT-Large (concatenation of response and its\nimmediate prior turn)\n11\n24\ntaha\n0.731\n0.732\n0.731\nBERT\n12\n27\nkalaivani.A 0.722\n0.722\n0.722\nBERT (isolated response)\n13\n28\nadithya604 0.719\n0.721\n0.719\nBERT (concatenation of prior turns and response)\n14\n35\nAnadKumR 0.690\n0.690\n0.690\nGloVe + Logistic Regression\n15\n-\nbaseline\n0.700\n0.669\n0.680\nLSTMattn\nTable 4: Performance of the best system per team and baseline for the Twitter track. We include two ranks - ranks\nfrom the submitted systems as well as the Leaderboard ranks from the CodaLab site\nIn the following paragraphs, we inspect the per-\nformance of the different systems more closely. We\ndiscuss a couple of particular aspects.\nContext Usage:\nOne of the prime motivating fac-\ntors for conducting this shared task was to investi-\ngate the role of contextual information. We notice\nthe most common approach for integrating context\nwas simply concatenating it with the response text.\nNovel approaches include :\n1. Taking immediate context as aspect for re-\nsponse in Aspect-based Sentiment Classiﬁca-\ntion architectures (taha)\n2. CNN-LSTM based summarization of entire\ncontext thread (salokr)\n3. Time-series fusion with proxy labels for con-\ntext (amitjena40)\n4. Ensemble of multiple models with different\ndepth of context (miroblog)\n5. Using explicit separator between context and\nresponse when concatenating (tanvidadu)\nDepth of Context:\nResults suggest that beyond\nthree context turns, gains from context information\nare negligible and may also reduce the performance\ndue to sparsity of long context threads. The depth\nof context required is dependent on the architecture\nand CNN-LSTM based summarization of context\nthread (salokr) was the only approach that effec-\ntively used the whole dialogue.\nDiscrete vs. Embedding Features\nThe leader-\nboard was dominated by Transformer based archi-\ntectures and we saw submissions using BERT or\nRoBERTa and other variants. Other sentence em-\nbedding architectures like Infersent, CNN/LSTM\nover word embeddings were also used but had\nmiddling performances. Discrete features were in-\nvolved in only two submissions (burtenshaw and\nduke DS) and were the focus of burtenshaw sys-\ntem.\nLeveraging other datasets\nThe large difference\nbetween the best model (miroblog) and other sys-\ntems can be attributed to their dataset augmenta-\ntion strategies. Using just the context thread as a\nnegative example when the context+response is a\npositive example, is a straight-forward approach\nfor augmentation from labeled dialogues. Their\nnovel contribution lies in leveraging large-scaled\nunlabelled dialogue threads, showing another use\nof BERT by using NSP conﬁdence score for assign-\ning pseudo-labels.\nAnalysis of predictions:\nFinally, we conducted\nan error analysis based on the predictions of the\nsystems. We particularly focused on addressing\ntwo questions. First, we investigate whether any\nparticular pattern exists in the evaluation instances\nthat are wrongly classiﬁed by the majority of the\nsystems. Second, we compare the predictions of\nthe top-performing systems to identify instances\ncorrectly classiﬁed by the candidate system but\nmissed by the remaining systems. Here, we attempt\nto recognize speciﬁc characteristics that are unique\nto a model, if any.\nInstead of looking at the predictions of all the\nsystems we decided to analyze only the top-three\nsubmissions in both tracks because of their high\nperformances. We identify 80 instances (30 sar-\ncastic) from the Reddit evaluation dataset and 20\ninstances (10 sarcastic) from the Twitter evalua-\ntion set, respectively, that are missed by all the\ntop-performing systems. Our interpretation of this\nﬁnding is that all these test instances more or less\nbelong to a variety of topics including sarcastic re-\nmarks on baseball teams, internet bills, vaccination,\netc., that probably do not generalize well during\nthe training. For both Twitter and Reddit, we also\nfound many sarcastic examples that contain com-\nmon non-sarcastic markers such as laughs (e.g.,\n“haha”), jokes, positive-sentiment emoticons (e.g.,\n:)) in terms of Twitter track. We did not ﬁnd any\ncorrelation to context length. Most of the instances\ncontain varied context length, from two to six.\nWhile analyzing the predictions of individual\nsystems we noted that miroblog correctly identi-\nﬁes the most number of predictions for both the\ntracks. In fact, miroblog has successfully predicted\nover two hundred examples (with almost equal dis-\ntribution of sarcastic and non-sarcastic instances) in\ncomparison to the second-ranked and third-ranked\nsystems for both tracks. As stated earlier, this can\nbe attributed to their data augmentation strategies\nthat have assisted miroblog’s models to generalize\nbest. However, we still notice that instances with\nsubtle humor or positive sentiment are missed by\nthe best-performing models even if they are pre-\ntrained on a very large-scale corpora. We foresee\nmodels that are able to detect subtle humor or witty\nwordplay will perform even better in a sarcasm\ndetection task.\n6\nConclusion\nThis paper summarizes the results of the shared\ntask on sarcasm detection using conversation from\ntwo social media platforms (Reddit and Twitter),\norganized as part of the 2nd Workshop on the Fig-\nurative Language Processing at ACL 2020. This\nshared task aimed to investigate the role of con-\nversation context for sarcasm detection. The goal\nwas to understand how much conversation context\nis needed or helpful for sarcasm detection. For\nReddit, the training data was sampled from the\nstandard corpus from Khodak et al. (2017) whereas\nwe curated a new evaluation dataset. For Twitter,\nboth the training and the test datasets are new and\ncollected using standard hashtags. We received\n655 submissions (from 39 unique participants) and\n1070 submissions (from 38 unique participants) for\nReddit and Twitter tracks, respectively. We pro-\nvided brief descriptions of each of the participating\nsystems who submitted a shared task paper (14\nsystems).\nWe notice that almost every submitted system\nhave used transformer-based architectures, such as\nBERT and RoBERTa and other variants, emphasiz-\ning the increasing popularity of using pre-trained\nlanguage models for various classiﬁcation tasks.\nThe best systems, however, have employed a clever\nmix of ensemble techniques and/or data augmenta-\ntion setups, which seem to be a promising direction\nfor future work. We hope that some of the teams\nwill make their implementations publicly available,\nwhich would facilitate further research on improv-\ning performance on the sarcasm detection task.\nReferences\nkalaivani A and Thenmozhi D. 2020. Sarcasm iden-\ntiﬁcation and detection in conversion context using\nBERT. In Proceedings of the Second Workshop on\nFigurative Language Processing, Seattle, WA, USA.\nSilvio Amir, Byron C Wallace, Hao Lyu, and Paula Car-\nvalho M´ario J Silva. 2016. Modelling context with\nuser embeddings for sarcasm detection in social me-\ndia. arXiv preprint arXiv:1607.00976.\nTaha Shangipour ataei, Soroush Javdan, and Behrouz\nMinaei-Bidgoli. 2020. Applying Transformers and\naspect-based sentiment analysis approaches on sar-\ncasm detection. In Proceedings of the Second Work-\nshop on Figurative Language Processing, Seattle,\nWA, USA.\nAdithya Avvaru, Sanath Vobilisetty, and Radhika\nMamidi. 2020. Detecting sarcasm in conversation\ncontext using Transformer based model.\nIn Pro-\nceedings of the Second Workshop on Figurative Lan-\nguage Processing, Seattle, WA, USA.\nDavid Bamman and Noah A Smith. 2015. Contextual-\nized sarcasm detection on twitter. In Ninth Interna-\ntional AAAI Conference on Web and Social Media.\nArup Baruah, Kaushik Das, Ferdous Barbhuiya, and\nKuntal Dey. 2020. Context-aware sarcasm detection\nusing BERT. In Proceedings of the Second Work-\nshop on Figurative Language Processing, Seattle,\nWA, USA.\nYitao Cai, Huiyu Cai, and Xiaojun Wan. 2019. Multi-\nmodal sarcasm detection in twitter with hierarchical\nfusion model. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2506–2515.\nSantiago Castro, Devamanyu Hazarika, Ver´onica P´erez-\nRosas, Roger Zimmermann, Rada Mihalcea, and\nSoujanya Poria. 2019. Towards multimodal sarcasm\ndetection (an obviously perfect paper). In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4619–4629.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018.\nUniversal sentence encoder.\narXiv\npreprint arXiv:1803.11175.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic\nBarrault, and Antoine Bordes. 2017.\nSupervised\nlearning of universal sentence representations from\nnatural language inference data.\narXiv preprint\narXiv:1705.02364.\nTanvi Dadu and Kartikey Pant. 2020. Sarcasm detec-\ntion using context separators in online discourse. In\nProceedings of the Second Workshop on Figurative\nLanguage Processing, Seattle, WA, USA.\nDmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.\nSemi-supervised recognition of sarcastic sentences\nin twitter and amazon. In Proceedings of the Four-\nteenth Conference on Computational Natural Lan-\nguage Learning, CoNLL ’10.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nXiangjue Dong, Changmao Li, and Jinho D. Choi.\n2020. Transformer-based context-aware sarcasm de-\ntection in conversation threads from social media. In\nProceedings of the Second Workshop on Figurative\nLanguage Processing, Seattle, WA, USA.\nBjarke Felbo, Alan Mislove, Anders Søgaard, Iyad\nRahwan, and Sune Lehmann. 2017. Using millions\nof emoji occurrences to learn any-domain represen-\ntations for detecting sentiment, emotion and sarcasm.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1615–1625. Association for Computational Linguis-\ntics.\nAniruddha Ghosh and Tony Veale. 2016.\nFracking\nsarcasm using neural network.\nIn Proceedings of\nNAACL-HLT, pages 161–169.\nAniruddha Ghosh and Tony Veale. 2017. Magnets for\nsarcasm: Making sarcasm detection timely, contex-\ntual and very personal. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 482–491. Association for\nComputational Linguistics.\nDebanjan Ghosh, Alexander R Fabbri, and Smaranda\nMuresan. 2018.\nSarcasm analysis using conversa-\ntion context. arXiv preprint arXiv:1808.07531.\nDebanjan Ghosh, Alexander Richard Fabbri, and\nSmaranda Muresan. 2017. The role of conversation\ncontext for sarcasm detection in online interactions.\narXiv preprint arXiv:1707.06226.\nDebanjan Ghosh, Weiwei Guo, and Smaranda Muresan.\n2015. Sarcastic or not: Word embeddings to predict\nthe literal or sarcastic meaning of words.\nIn Pro-\nceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1003–\n1012, Lisbon, Portugal. Association for Computa-\ntional Linguistics.\nDebanjan Ghosh and Smaranda Muresan. 2018. ” with\n1 follower i must be awesome: P”. exploring the role\nof irony markers in irony recognition. arXiv preprint\narXiv:1804.05253.\nDebanjan Ghosh, Elena Musi, Kartikeya Upasani,\nand Smaranda Muresan. 2019. Interpreting verbal\nirony: Linguistic strategies and the connection to\nthe type of semantic incongruity.\narXiv preprint\narXiv:1911.00891.\nRoberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and\nNina Wacholder. 2011. Identifying sarcasm in twit-\nter: A closer look. In ACL (Short Papers), pages\n581–586. Association for Computational Linguis-\ntics.\nHunter Gregory, Steven Li, Pouya Mohammadi, Na-\ntalie Tarn, Rachel Ballantyne, and Cynthia Rudin.\n2020.\nA Transformer approach to contextual sar-\ncasm detection in twitter. In Proceedings of the Sec-\nond Workshop on Figurative Language Processing,\nSeattle, WA, USA.\nDevamanyu Hazarika, Soujanya Poria, Sruthi Gorantla,\nErik Cambria, Roger Zimmermann, and Rada Mi-\nhalcea. 2018. Cascade: Contextual sarcasm detec-\ntion in online discussion forums. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 1837–1848. Association for Com-\nputational Linguistics.\nNikhil Jaiswal. 2020. Neural sarcasm detection using\nconversation context. In Proceedings of the Second\nWorkshop on Figurative Language Processing, Seat-\ntle, WA, USA.\nAmit Kumar Jena, Aman Sinha, and Rohit Agarwal.\n2020. C-net: Contextual network for sarcasm de-\ntection. In Proceedings of the Second Workshop on\nFigurative Language Processing, Seattle, WA, USA.\nAditya Joshi, Pushpak Bhattacharyya, and Mark J Car-\nman. 2017. Automatic sarcasm detection: A survey.\nACM Computing Surveys (CSUR), page 73.\nAditya Joshi, Vinita Sharma, and Pushpak Bhat-\ntacharyya. 2015. Harnessing context incongruity for\nsarcasm detection. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), volume 2, pages 757–762.\nAditya\nJoshi,\nVaibhav\nTripathi,\nPushpak\nBhat-\ntacharyya, and Mark Carman. 2016. Harnessing se-\nquence labeling for sarcasm detection in dialogue\nfrom tv series friends. CoNLL 2016, page 146.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nAkshay Khatri and Pranav P. 2020. Sarcasm detection\nin tweets with BERT and GloVe embeddings.\nIn\nProceedings of the Second Workshop on Figurative\nLanguage Processing, Seattle, WA, USA.\nAnupam Khattri, Aditya Joshi, Pushpak Bhattacharyya,\nand Mark Carman. 2015. Your sentiment precedes\nyou:\nUsing an author’s historical tweets to pre-\ndict sarcasm. In Proceedings of the 6th Workshop\non Computational Approaches to Subjectivity, Senti-\nment and Social Media Analysis, pages 25–30, Lis-\nboa, Portugal. Association for Computational Lin-\nguistics.\nMikhail Khodak, Nikunj Saunshi, and Kiran Vodrahalli.\n2017.\nA large self-annotated corpus for sarcasm.\narXiv preprint arXiv:1704.05579.\nAmardeep Kumar and Vivek Anand. 2020. Transform-\ners on sarcasm detection with context. In Proceed-\nings of the Second Workshop on Figurative Lan-\nguage Processing, Seattle, WA, USA.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations.\narXiv preprint\narXiv:1909.11942.\nHankyol Lee, Youngjae Yu, and Gunhee Kim. 2020.\nAugmenting data for sarcasm detection with unla-\nbeled conversation context. In Proceedings of the\nSecond Workshop on Figurative Language Process-\ning, Seattle, WA, USA.\nJens Lemmens, Ben Burtenshaw, Ehsan Lotﬁ, Ilia\nMarkov, and Walter Daelemans. 2020. Sarcasm de-\ntection using an ensemble approach. In Proceedings\nof the Second Workshop on Figurative Language\nProcessing, Seattle, WA, USA.\nChee Wee Leong, Beata Beigman Klebanov, and Eka-\nterina Shutova. 2018.\nA report on the 2018 vua\nmetaphor detection shared task. In Proceedings of\nthe Workshop on Figurative Language Processing,\npages 56–66.\nCC Liebrecht, FA Kunneman, and APJ van den Bosch.\n2013. The perfect solution for detecting sarcasm in\ntweets# not.\nIn Proceedings of the 4th Workshop\non Computational Approaches to Subjectivity, Senti-\nment and Social Media Analysis.\nRongcheng Lin, Jing Xiao, and Jianping Fan. 2018.\nNextvlad: An efﬁcient neural network to aggregate\nframe-level features for large-scale video classiﬁca-\ntion. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 0–0.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nDehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng\nWang. 2017.\nInteractive attention networks for\naspect-level sentiment classiﬁcation. arXiv preprint\narXiv:1709.00893.\nNavonil Majumder, Soujanya Poria, Haiyun Peng,\nNiyati Chhaya, Erik Cambria, and Alexander Gel-\nbukh. 2019.\nSentiment and sarcasm classiﬁcation\nwith multitask learning. IEEE Intelligent Systems,\n34(3):38–43.\nDiana Maynard and Mark A Greenwood. 2014. Who\ncares about sarcastic tweets? investigating the im-\npact of sarcasm on sentiment analysis. In Proceed-\nings of LREC.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, pages 3111–3119.\nAbhijit Mishra, Diptesh Kanojia, Seema Nagar, Kun-\ntal Dey, and Pushpak Bhattacharyya. 2016.\nHar-\nnessing cognitive features for sarcasm detection. In\nProceedings of the 54th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1095–\n1104, Berlin, Germany.\nSmaranda Muresan, Roberto Gonzalez-Ibanez, Deban-\njan Ghosh, and Nina Wacholder. 2016. Identiﬁca-\ntion of nonliteral language in social media: A case\nstudy on sarcasm.\nJournal of the Association for\nInformation Science and Technology, 67(11):2725–\n2737.\nSilviu Oprea and Walid Magdy. 2019. Exploring au-\nthor context for detecting intended vs perceived sar-\ncasm.\nIn Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2854–2859.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. Proceedings of the Empiricial Methods\nin Natural Language Processing (EMNLP 2014),\n12.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nAshwin Rajadesingan, Reza Zafarani, and Huan Liu.\n2015. Sarcasm detection on twitter: A behavioral\nmodeling approach. In Proceedings of the Eighth\nACM International Conference on Web Search and\nData Mining, pages 97–106. ACM.\nEllen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra\nDe Silva, Nathan Gilbert, and Ruihong Huang. 2013.\nSarcasm as contrast between a positive sentiment\nand negative situation. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 704–714.\nRossano Schifanella, Paloma de Juan, Joel Tetreault,\nand Liangliang Cao. 2016.\nDetecting sarcasm in\nmultimodal social platforms. In Proceedings of the\n2016 ACM on Multimedia Conference, pages 1136–\n1145. ACM.\nYouwei Song, Jiahai Wang, Tao Jiang, Zhiyue Liu, and\nYanghui Rao. 2019.\nAttentional encoder network\nfor targeted sentiment classiﬁcation. arXiv preprint\narXiv:1902.09314.\nHimani Srivastava, Vaibhav Varshney, Surabhi Kumari,\nand Saurabh Srivastava. 2020. A novel hierarchical\nBERT architecture for sarcasm detection.\nIn Pro-\nceedings of the Second Workshop on Figurative Lan-\nguage Processing, Seattle, WA, USA.\nYi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian\nSu. 2018. Reasoning with sarcasm by reading in-\nbetween. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1010–1020. Associ-\nation for Computational Linguistics.\nOren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.\nIcwsm-a great catchy name: Semi-supervised recog-\nnition of sarcastic sentences in online product re-\nviews. In ICWSM.\nCynthia Van Hee, Els Lefever, and V´eronique Hoste.\n2018. Semeval-2018 task 3: Irony detection in en-\nglish tweets. In Proceedings of The 12th Interna-\ntional Workshop on Semantic Evaluation, pages 39–\n50.\nTony Veale and Yanfen Hao. 2010. Detecting ironic\nintent in creative comparisons.\nIn European Con-\nference on Artiﬁcial Intelligence, volume 215, pages\n765–770, Lisbon, Portugal.\nByron C Wallace. 2015. Computational irony: A sur-\nvey and new perspectives. Artiﬁcial Intelligence Re-\nview, 43(4):467–483.\nByron C Wallace, Do Kook Choe, Laura Kertz, and\nEugene Charniak. 2014. Humans require context to\ninfer ironic intent (so computers probably do, too).\nIn ACL (2), pages 512–516.\nZelin Wang, Zhijian Wu, Ruimin Wang, and Yafeng\nRen. 2015. Twitter sarcasm detection exploiting a\ncontext-based model. In International Conference\non Web Information Systems Engineering, pages 77–\n91, Miami, Florida. Springer.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchical\nattention networks for document classiﬁcation. In\nProceedings of NAACL-HLT, pages 1480–1489.\nBiqing Zeng, Heng Yang, Ruyang Xu, Wu Zhou, and\nXuli Han. 2019. Lcf: A local context focus mecha-\nnism for aspect-based sentiment classiﬁcation. Ap-\nplied Sciences, 9(16):3389.\nMeishan Zhang, Yue Zhang, and Guohong Fu. 2016.\nTweet sarcasm detection using deep neural network.\nIn Proceedings of COLING 2016, The 26th Inter-\nnational Conference on Computational Linguistics:\nTechnical Papers, pages 2449–2460.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-05-12",
  "updated": "2020-06-04"
}