{
  "id": "http://arxiv.org/abs/2005.09428v2",
  "title": "Quantum-Classical Machine learning by Hybrid Tensor Networks",
  "authors": [
    "Ding Liu",
    "Jiaqi Yao",
    "Zekun Yao",
    "Quan Zhang"
  ],
  "abstract": "Tensor networks (TN) have found a wide use in machine learning, and in\nparticular, TN and deep learning bear striking similarities. In this work, we\npropose the quantum-classical hybrid tensor networks (HTN) which combine tensor\nnetworks with classical neural networks in a uniform deep learning framework to\novercome the limitations of regular tensor networks in machine learning. We\nfirst analyze the limitations of regular tensor networks in the applications of\nmachine learning involving the representation power and architecture\nscalability. We conclude that in fact the regular tensor networks are not\ncompetent to be the basic building blocks of deep learning. Then, we discuss\nthe performance of HTN which overcome all the deficiency of regular tensor\nnetworks for machine learning. In this sense, we are able to train HTN in the\ndeep learning way which is the standard combination of algorithms such as Back\nPropagation and Stochastic Gradient Descent. We finally provide two applicable\ncases to show the potential applications of HTN, including quantum states\nclassification and quantum-classical autoencoder. These cases also demonstrate\nthe great potentiality to design various HTN in deep learning way.",
  "text": "arXiv:2005.09428v2  [cs.LG]  14 Aug 2024\nQuantum-Classical Machine learning by Hybrid Tensor Networks\nDing Liu,1, ∗Jiaqi Yao,1 Zekun Yao,1 and Quan Zhang1\n1School of Computer Science and Technology, Tiangong University, Tianjin 300387, China\nTensor networks (TN) have found a wide use in machine learning, and in particular, TN and deep\nlearning bear striking similarities. In this work, we propose the quantum-classical hybrid tensor\nnetworks (HTN) which combine tensor networks with classical neural networks in a uniform deep\nlearning framework to overcome the limitations of regular tensor networks in machine learning.\nWe ﬁrst analyze the limitations of regular tensor networks in the applications of machine learning\ninvolving the representation power and architecture scalability. We conclude that in fact the regular\ntensor networks are not competent to be the basic building blocks of deep learning.\nThen, we\ndiscuss the performance of HTN which overcome all the deﬁciency of regular tensor networks for\nmachine learning. In this sense, we are able to train HTN in the deep learning way which is the\nstandard combination of algorithms such as Back Propagation and Stochastic Gradient Descent. We\nﬁnally provide two applicable cases to show the potential applications of HTN, including quantum\nstates classiﬁcation and quantum-classical autoencoder.\nThese cases also demonstrate the great\npotentiality to design various HTN in deep learning way.\nI.\nINTRODUCTION.\nIn recent years, tensor networks (TN) have drawn more\nattention as one of the most powerful numerical tools for\nstudying quantum many-body systems [1–4].\nFurther-\nmore, TN have been recently applied to many research\nareas of machine learning [5–8], such as image classiﬁ-\ncation [9–11], dimensionality reduction [12, 13], gener-\native model [10, 14], data compression [15], improving\ndeep neural network [16], probabilistic graph model [17],\nquantum compressed sensing [18], even the promising\nway to implement quantum circuit [19–23].\nHowever,\nresearchers encounter a serious computing complexity\nproblem and raise the question: Are the tensor networks\nable to be the universal deep learning architecture? As\nwe know, the theoretical foundation of deep neural net-\nworks is the principle of universal approximation which\nstates that a feed-forward network with a single hidden\nlayer is a universal approximator if and only if the activa-\ntion function is not polynomial [24, 25]. In this context,\nthe key point of the question of tensor network machine\nlearning is whether TNs can also be universal approxi-\nmators.\nSome pioneering researches have begun to address this\nfundamental problem. Reference [26] proposes the con-\ncept of generalized tensor networks to outperform regular\ntensor networks, particularly in terms of representation\npower. Speciﬁcally, they try to combine generalized ten-\nsor networks with convolution neural networks together\nand achieve some good results.\nIn this approach, the\nconvolutions are treated as a feature map, and the ten-\nsor network is placed in the ﬁnal layer and used as the\nclassiﬁer.\nReference [27] provides a mathematical analysis of the\nrepresentation power of some typical tensor network fac-\ntorizations of discrete multivariate probability distribu-\n∗liuding@tiangong.edu.cn\ntions involving matrix product states (MPS), Born ma-\nchines, and locally puriﬁed states (LPS). Reference [28]\ndiscusses the equivalence between restricted boltzmann\nmachines (RBM) and tensor network states. They prove\nthat these kinds of speciﬁc neural networks can be trans-\nlated into MPS and outline an eﬃcient algorithm for do-\ning so. Drawing on this insight, they quantify the rep-\nresentational power of RBM through the perspective of\ntensor networks. This insight into tensor networks and\nRBM guides the design of novel quantum deep learning\narchitectures.\nThe intersection of physics and machine learning,\nquantum networks are relatively easy to deploy on quan-\ntum computers. Reference [29] presents the framework\nof hybrid tensor networks, with building blocks including\nmeasurable quantum states and classically contractible\ntensors. Using hybrid tree tensor networks as an exam-\nple, it demonstrates the method of eﬃciently simulat-\ning quantum systems on quantum computers with sig-\nniﬁcantly smaller volumes than the target systems. This\napproach provides insights for simulating large practical\nproblems within medium-scale quantum computers. The\nreview article [30] discusses how various tensor networks\ncan be mapped to quantum computers, their utilization\nin machine learning and data encoding, and which im-\nplementation techniques can enhance their performance.\nAdditionally, when parameters are randomly initialized,\nthe size of initial gradients decreases exponentially with\nthe increase in the number of quantum bits and circuit\ndepth. To address this phenomenon, known as the ”bar-\nren plateau”, Reference [31] proposes the MPS pretrain-\ning method.\nDiﬀerent from these previous works, we propose the\nconcept of Hybrid Tensor Networks (HTN) which com-\nbine tensor networks with classical neural networks into\na uniform deep learning framework.\nWe show the\nschematic of this universal framework in Fig.\n1.\nBy\nvirtue of this framework, people are able to freely de-\nsign HTN by adding any speciﬁc tensor network and\nany classical neural network at any part of the HTN.\n2\nFIG. 1. Universal framework of Hybrid Tensor Networks.\n(a)\n(b)\n(c)\nFIG. 2. Diﬀerent ways of message passing on Neural Networks and Tensor Networks. The directions of message passing are\ndenoted by blue arrows. For tensor network, the message passing is implemented by the operation of tensor contraction; (a)\nNeural Network ; (b) Regular tensor network; (c) Generalized tensor network, and the operation of copy is marked by red dot;\nTABLE I. Number of parameters of each model on MNIST\nclassiﬁcation.\nmodel\nMPS TTN LeNet-5 FCN HTN\nTest accuracy\n98% 95% 99%\n95% 98%\nBond dimension 20\n6\n-\n-\n3\nNumber of\n6.3\n1.4\n1.2\n2.4\n7.7\nparameters\n×107 ×109 ×104\n×106 ×105\nTABLE II. Number of parameters of each model on MNIST\nregression\nLower bounds of MSE Loss O(10−1) O(10−2)\nFCN\n8.6 × 103 6 × 104\nCNN\n6 × 102\n3 × 103\nTTN\n4.3 × 104 1.4 × 106\nHTN\n6.5 × 103 2.6 × 104\nAnd then train the whole network using the standard\nBack Propagation (BP) algorithm and Stochastic Gra-\ndient Descent (SGD). Therefore by introducing neurons\nwith nonlinear activation, HTN will be a kind of univer-\nsal approximator just like neural networks. More impor-\ntantly, HTN are capable of dealing with both quantum\nentanglement states and product states. In this way, the\nHTN will be a good choice for the implementation of\na hybrid quantum-classical deep learning model, mak-\ning it a promising choice for the implementation of hy-\nbrid quantum-classical deep learning models. In this pa-\nper, we discuss some preliminary ideas to design HTN\nand provide some applicable cases and numerical exper-\niments.\nAt the end, we give a brief discussion on the\nquantum feature engineering.\nII.\nLIMITATIONS OF REGULAR TENSOR\nNETWORKS MACHINE LEARNING\nAlthough, as a kind of popular and powerful numer-\nical tool in quantum many-body physics, regular tensor\nnetworks expose some limitations on machine learning,\nsuch as the limitations on representation and scalability\nin architecture. All of these limitations restrict the ap-\nplication of regular tensor networks in machine learning,\nespecially for deep learning. In this section, we conclude\nand analyze some main points.\n3\n0 10\n30\n50\n70\n90\n110\n130\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\nbond dimension\ntime (ms)\n \n \nIntel Xeon(R) \nTesla K80\nFIG. 3. Speeding up on triangle tensor network contraction\nby GPU platform. The time cost is plotted on logarithmic\ny-axis.\n0\n200\n400\n600\n800\n1000\n0\n0.05\n0.1\nepoch\nloss\n \n \n2*2 grid\n4*4 grid\n8*8 grid\n(a)\n0\n200\n400\n600\n800\n1000\n0\n0.05\n0.1\n0.15\n0.2\n0.25\nepoch\nloss\n \n \n2*2 grid\n4*4 grid\n8*8 grid\n(b)\nFIG. 4. Training loss of quantum-classical autoencoder; (a)\nMNIST ; (b) Fashion-MNIST ;\nA.\nRepresentation\nGeneral neural networks (NNs) are characterized by\nthe universal approximation theorem which states that\nthe feed-forward networks are capable of approximating\nany continuous function, owing to the use of nonlinear\nactivation.\nSo we treat it as a kind of so-called uni-\nversal approximator.\nBased on this, NNs become the\nfundamental building blocks of deep learning.\nIn con-\ntrast, TNs are considered as multi-linear functions and\ntherefore obey the superposition principle in quantum\nmechanics. This is characterized as the intrinsic feature\nof TNs in quantum many-body systems, but an obstacle\nto being a powerful universal approximator in machine\nlearning. Therefore, nonlinear feature map functions are\nrequired to map all data points from the original feature\nspace to the high-dimensional Hilbert space.\nIn some\nprevious works [9, 11], people use the feature map which\nis introduced by (1) ﬁrstly as:\nCvs(x) =\ns\u0012d −1\ns −1\n\u0013\n(cos(π\n2 x))d−s(sin(π\n2 x))s−1\n(1)\nwhere d denotes the dimension of the physical index, and\ns runs from 1 to d. By using a larger d, the TTN has\nthe potential to approximate a richer class of functions.\nFurthermore, reference [26] discusses some other optional\ncomplex feature maps, even including neural networks\nsuch as CNNs. These works highlight the crucial role of\nthe feature map in tensor network machine learning, as\nit determines whether and how well a tensor network can\napproximate a nonlinear function.\nThis signiﬁcant diﬀerence between TNs and NNs is il-\nlustrated in Fig. 1. Deep neural networks are equipped\nwith lots of non-linear activation in each layer and each\nneuron which guarantees its power of approximation.\nBut in contrast, TNs strongly depend on the choice of the\nfeature map, rendering them a multi-linear model in the\nhigh dimensional feature space, which strictly limits their\npower of approximation. It is also easy to understand\nthis from the perspective of statistical machine learning\ntheory such as Support Vector Machine (SVM). In the\ncontext of SVM, people always need to map original data\npoints into a high-dimensional feature space and ﬁnd a\nkernel function while addressing the nonlinear issue, and\nit is called the “kernel trick”. However, in the context\nof tensor network machine learning, it is unreasonable to\nendow the TNs with the capacity of universal approxima-\ntion just by this “kernel trick”, especially when we want\nto build a complex and deep tensor network model.\nMoreover, for a speciﬁc machine learning task, we al-\nways have to train a large-scale regular tensor network\nwith more parameters than its corresponding classical\nneural network can pose a challenge. Taking the previ-\nous works as examples [11, 32], we employed the TTN\nand MPS on the benchmark of handwritten digits clas-\nsiﬁcation. The parametric complexity is around O(D5)\nfor TTN and O(D3) for MPS, in which D represents the\nbond dimension. This may result in the trouble of high\nparametric complexity since we often need to use a large\nbond dimension.\nThe experimental results in Table I\nshow us the scale of a number of parameters in both TTN\nand MPS are far more than almost any classical model\nsuch as CNN and fully-connected network (FCN). For\ncomparison, we also implement a HTN model and ﬁnd\nthe number of parameters it needs is less than FCN’s, and\nof course far less than TTN’s and MPS’s. From the per-\nspective of quantum simulation, we understand that sim-\nulating quantum computing on a classical computer al-\nways requires exponential growth of parameters with the\nsize of the systems. It shows us a large number of param-\neters intrinsically leads to a severe problem -–compared\nwith the existing classical deep learning model, it is diﬃ-\ncult to train a regular tensor network that has the same\nor better performance, even if it is impossible.\nWe also verify this conclusion by some preliminary re-\ngression experiments which directly show us how well the\nmodel can reach in the curve ﬁtting. Table II presents\nthe benchmark results on the MNIST dataset. In this\ncase, we change the classiﬁcation task to a simple regres-\nsion issue by setting the label as a corresponding scalar.\nTaking the class of image “6” as an example, we need\nto train a model that outputs a scalar which closes to\n“6” as soon as possible, rather than a classiﬁcation vec-\ntor. We then determine the lower bound of the Mean\nSquare Error loss function (MSE) and ﬁnd the minimum\n4\nmodel that could reach this lower bound. Indeed, the\nlower bound of the loss function characterizes how well\nthe model can ﬁt the curve. Clearly, the TTN contains\nmany more parameters than FCN and CNN to reach the\nsame level of the lower bound. In this case, it has ranged\nfrom around 10 to 103 times larger than that of the FCN\nor CNN. It will lead to severe time-consuming problems\nand even in some worst cases, the training is likely to\nfail. Similar to the last case, we also ﬁnd the number of\nparameters HTN needs is less than FCN’s, and far less\nthan TTN’s.\nIt is worth noting that references [15, 16, 33–35] pro-\npose tensorizing neural networks or tensor regression net-\nworks. These are other feasible ways to take advantage of\ntensor networks in deep learning. But they are very dif-\nferent from the tensor network learning we talked about\nhere. The motivation for tensorizing neural networks is\nto compress the weight matrix of neural networks using\ntensor decomposition to reduce computing complexity or\nsave storage space. In this context, the models of ten-\nsorizing neural networks neither take quantum data into\nconsideration nor involve the implementation of a quan-\ntum model.\nB.\nScalability\nWe also evaluate regular tensor networks from the per-\nspective of scalability in architecture design. As we know,\nthere are many deep learning models developed to tackle\nchallenges in computer vision, natural language process-\ning and speech recognition etc., such as the popular CNN\n[36], RNN, LSTM [37], GAN [38], Attention model and\nTransformer [39] etc. So just like playing the Jenga game,\npeople are always able to assemble all these models to-\ngether depending on the engineering applications in prac-\ntice, even for the designing of extremely deep and com-\nplex architecture. In contrast, the scale of tensor net-\nworks must be strictly restricted while applying them to\nquantum many-body systems or machine learning, given\nthe rapid growth of computational complexity. So, it is\nhard to imagine a huge tensor network with thousands of\nlayers could be simulated on classical computers and ap-\nplied in machine learning. However, Reference [40] pro-\nposed that Pan Zhang’s team utilized a computing clus-\nter equipped with 512 GPUs, dedicating 15 hours to suc-\ncessfully complete a sampling task of Google’s Sycamore\nquantum supremacy circuit.\nSpeciﬁcally, we take the process of message passing\ninto consideration and observe a signiﬁcant diﬀerence in\nbehavior between the tensor networks and neural net-\nworks. As we show in Fig. 2(a), the input message passes\nthrough a neural network from the input side to the out-\nput side layer by layer. Speciﬁcally, for any single neu-\nron, the message passing could be divided into two parts:\nweighted sum and fan-out. The operation of fan-out gen-\nerates lots of copies of the output message and distributes\nthem to the next layer. This mechanism guarantees the\nFIG. 5. HTN for quantum states classiﬁcation. We embed\ntwo tree tensor network layers and three dense neural network\nlayers.\nFIG. 6. HTN for Quantum-classical autoencoder. We create\nthe encoder by two tensor network layers, and design three\ndiﬀerent decoders by using three diﬀerent setups of deconvo-\nlutional layers.\none-to-many mapping could be implemented easily by\nneural networks but becomes an obstacle to regular ten-\nsor networks for machine learning. Fig. 2(b) shows that\nmessage passing is implemented by contraction in regular\ntensor networks, and it is essentially the inverse opera-\ntion of tensor decomposition. Reference [26] proposed the\ngeneralized tensor networks to overcome this limitation\nby introducing the operation of copy which is marked by\nthe red dot in Fig.2(c).\nThis mechanism deﬁnitely limits the scalability of reg-\nular tensor networks in machine learning, especially in\nthe case that we need to build a deep hierarchy.\nBased on all these observations, we understand the lim-\nitation on architecture scalability is another severe prob-\nlem for regular tensor network machine learning. Due to\nthis, we think it’s not a good way to build a huge, deep\nand complex deep learning model by regular tensor net-\nworks for the practical applications of machine learning.\nTherefore, how can we take the advantage of tensor net-\nworks for deep learning? The solution we try to oﬀer is\nthe Hybrid Tensor Network.\nIII.\nHYBRID TENSOR NETWORKS\nWe propose the concept of Hybrid Tensor Networks\n(HTN) to overcome the limitations of both representa-\ntion and scalability of regular tensor networks in machine\nlearning. The basic idea is to introduce nonlinearity by\nthe combination of tensor networks and neural networks.\nBy doing so, we are able to embed a local tensor network\ninto any existing popular deep learning framework very\neasily, involving both model and algorithm such as CNN,\nRNN, LSTM etc.. Then we could train a HTN by the\n5\nstandard Back Propagation algorithm (BP) and Stochas-\ntic Gradient Descend (SGD) which can be easily found\nin any deep learning literature [41]. Suppose we have a\nHTN which formed in the sequence of n tensor network\nlayers T1, T2, ..., Tn, and subsequent m neural network\nlayers L1, L2, ..., Lm.\nThe cost function is denoted as\nCost. Then we could compute the partial derivative to\nthe ith tensor network layer owing to the BP algorithm\nby (2).\nC ∂Cost\n∂Ti\n= ∂Cost\n∂Lm\n·\n∂Lm\n∂Lm−1\n· · · ∂L1\n∂Tn\n·\n∂Tn\n∂Tn−1\n· · · ∂Ti+1\n∂Ti\n(2)\nSince the operation of tensor contraction deﬁned as (3)\nis doubtless diﬀerentiable,\nCT [k]\ni+1 =\nX\nα1 ... αp\nT\n[1]\ni,α1 T\n[2]\ni,α2 ... T\n[p]\ni,αp\n(3)\nwhere T [k]\ni+1 represents the kth tensor in the i + 1 layer.\nSo the last term of (2) can be deduced as (4),\nC\n∂T\n[k]\ni+1\n∂Ti [j] =\n∂\nP\nα1 ... αp\nT\n[1]\ni,α1 ... T\n[j]\ni,αj ... T\n[p]\ni,αp\n∂Ti [j]\n=\nX\n{α1 ... αp}\\{αj}\nT\n[1]\ni,α1 ... T\n[j−1]\ni,αj−1 T\n[j+1]\ni,αj+1 ... T\n[p]\ni,αp\n(4)\nwhere T [j]\ni\nrepresents the jth tensor in the ith layer. And\nthe rest terms of (2) could be calculated easily according\nto the principle of neural networks. Then, we can update\nthis tensor by using the gradient descend method as (5),\nCT\n′\ni = Ti −η ∂Cost\n∂Ti\n(5)\nwhere η denotes the learning rate. Indeed all tensors in\nHTN could be updated layer by layer following this way.\nTherefore, it guarantees that the HTN can be trained\nin the uniform optimization framework which combines\nBP and SGD. Some popular deep learning open-source\nsoftware libraries such as Tensorﬂow [42] and Pytorch [43]\noﬀer powerful automatic diﬀerentiation program libraries\nwhich could help us implement HTN very easily.\nFurthermore, we test the speedup of tensor contraction\non the GPU platform, which is shown in Fig. 3. It con-\nﬁrms the feasibility of implementing the HTN model by\nutilizing the GPU platform, and sheds light on the po-\ntential, complex and practical applications of large-scale\nHTN model in the real world.\nIt is worth noting that, in contrast to previous work\nthat combines tensor networks and neural networks [26],\nwe treat tensor networks as the ”quantum units” respon-\nsible for extracting quantum features from input states.\nSo for the designing of a deep HTN, the ﬁrst consider-\nation is to determine the role that tensor networks will\nplay. We present our two preliminary attempts on quan-\ntum states classiﬁcation and quantum-classical autoen-\ncoder in the following.\nA.\nQuantum states classiﬁcation\nWe design a simple HTN architecture with two tree\ntensor network layers followed by three dense neural net-\nwork layers to verify its practicability in classiﬁcation\nproblems. In this case, we ﬁrst transform the input im-\nages into quantum product states without entanglement,\nwhich is formed as (6),\nC |Φ⟩= |φ(x 1)⟩⊗|φ(x 2)⟩· · · ⊗· |φ(x n)⟩\n(6)\nwhere x1, x2, ..., xn represents each pixel; |Φ⟩is the prod-\nuct states we get in the high dimensional Hilbert space;\nφ denotes the feature map we mentioned by (1).\nWe\ndeﬁne the tree tensor network as |Ψ⟩, so these two tree\ntensor network layers encode the |Φ⟩into the interme-\ndiate low dimensional states |C⟩by tensor contraction,\ni.e. |C⟩= ⟨Ψ |Φ⟩. Afterward, these intermediate states\ncould be processed by neural networks.\nFinally, the subsequent dense neural network layers\nclassify the intermediate C into 10 corresponding cate-\ngories by using the cross entropy cost function and the\npopular Adam training algorithm [44] which is derived\nfrom the standard SGD. The cross entropy is deﬁned as\n(7)\nCCroEn(L, P) = −\nn\nX\ni=1\nL (C i )log (P (C i ))\n(7)\nwhere L refers to the label and P is the predicted output\nby HTN. As can be observed, in analogy with the clas-\nsical CNNs, the tensor network layers play a similar role\nto the convolutional layers.But diﬀerent from it, tensor\nnetworks are more applicable to quantum state process-\ning because they naturally represent the structure and\nproperties of quantum states. Quantum states typically\nexhibit highly entangled properties, and tensor networks\nprovide an eﬀective method for describing and process-\ning this entanglement. We benchmark it on the popular\nMNIST and Fashion-MNIST datasets. The training set\nconsists of 60 000 (28 × 28) gray-scale images, with 10\n000 testing examples. For the simplicity of coding, we\nrescaled them to (32×32) images by padding zeros pix-\nels. We show the schematic in Fig. 5. It is easy to get\n98% test accuracy on MNIST and 90% test accuracy on\nFashion-MNIST by using this simple HTN architecture\nwithout using any deep learning tricks.\nThe\noverview\nof\nexperimental\nresults\nfor\nnu-\nmerous\nclassical\nmodels\non\nthese\ntasks\ncan\nbe\nfound\non\nthe\noﬃcial\nwebsites:\nof\nMNIST\n(http://yann.lecun.com/exdb/mnist/)\nand\nFashion-\n6\nFIG. 7. Quantum-Classical Autoencoder on MNIST;\nFIG. 8. Quantum-Classical Autoencoder on Fashion-MNIST;\nMNIST\n(https://github.com/zalandoresearch/fashion-\nmnist).\nThough our method applies to the complex\nnumber HTN, we assume all tensors are real for simplic-\nity. Our code of the implementation is available at [45],\nand people can ﬁnd the setup of parameters in detail\nfrom it.\nB.\nQuantum-Classical Autoencoder\nWe then demonstrate the application of quantum au-\ntoencoder by using a variety of HTNs. For simplicity,\nwe still benchmark all models on MNIST and Fashion-\nMNIST datasets. In this case, the encoder is formed by\na tensor network which compresses the input quantum\nstates into low-dimensional intermediate states.\nNext,\nthese compressed intermediate states could be recovered\nby a series of typical classical neural networks. We con-\n7\ntinue to use the Adam training algorithm, but change the\ncost function as MSE (Mean Square Error):\nCMSE = 1\nn\nn\nX\ni=1\n\u0000I i −O i\n\u00012\n(8)\nwhere I is the input data, and O denotes the recon-\nstructed data.\nFig.\n6 shows us the basic architecture\nand we can ﬁnd the detailed setup of parameters in our\ncode which is available at [45].\nWe show a series of experimental results in both Fig.\n7 and Fig. 8, and provide the evaluation indicators Com-\npression Ratio (CR) and PSNR (Peak Signal-to-Noise\nRatio) which is deﬁned as (9):\nCPSNR = 10∗log 10\n\n\n\n\nmax 2\nI\n1\nmn\nm\nP\ni=1\nnP\nj=1\n(O i,j −P i,j )2\n\n\n\n(9)\nwhere maxI indicates the max value of input data.\nWe compress input product states into intermediate\nrepresentations in three diﬀerent scales i.e.\n8*8 grids,\n4*4 grids and 2*2 grids. It should be noted that larger\ngrids beneﬁt from saving more original input informa-\ntion so that we can reconstruct better images from it\nand have a better PSNR score. This is evident in both\nFig. 7 and Fig. 8. In contrast, the smaller intermediate\nrepresentations will have higher CR. So it is necessary to\nstrike a balance between them in the practical quantum\ninformation application. We also plot the loss curve of\nboth cases in Fig.4, and it clearly shows us the process\nof training a HTN. Moreover, it deﬁnitely shows us the\ncase of 8*8 will reach a lower loss value and produce a\nbetter recovery image.\nC.\nQuantum feature engineering\nAbove two cases we present the potential of developing\nthe new concept of quantum feature engineering which is\nthe quantum version of feature engineering in machine\nlearning. It is generally recognized that deep learning is\nan eﬀective way to perform feature engineering since it is\ncapable of extracting feature information automatically\nfrom the raw data. Such as during the process of training\na convolutional neural network, the convolutional kernels\nwill be trained as feature detectors to recognize, extract\nand assemble valuable feature information which will be\nused in the subsequent machine learning tasks such as\nclassiﬁcation, regression or sequential analysis etc..\nIn\nanalogy to this standpoint, HTN could be treated as a\nwell-deﬁned hybrid quantum-classical model that is ap-\npropriate for quantum feature engineering. In which, the\ntensor network component we say it “quantum unit” is\nin charge of the recognition, extraction or assembling of\nquantum feature, and then transform it into the form of\nclassical data.\nAlthough we have no formal deﬁnition of what quan-\ntum feature exactly is in machine learning, we still\nstarted to investigate it involving quantum entanglement\nand ﬁdelity by using TTN in our previous work [11]. Ref.\n[46] proposed the machine learning based approach in\nterms of quantum control and ﬁrst proposed the concept\nof quantum feature engineering. Based on these, we think\nthat quantum feature engineering will be a promising and\nsigniﬁcant area in quantum machine learning, and we an-\nticipate that HTN will be an excellent choice of quantum\nfeature engineering.\nIn future work, it will help us to\nunderstand better how quantum features such as entan-\nglement and ﬁdelity aﬀect the performance of machine\nlearning.\nIV.\nPARAMETERIZED QUANTUM CIRCUITS\nThe article employs Parameterized Quantum Circuits\n(PQCs) to replicate the Hybrid Tensor Networks (HTN)\ndiscussed in the paper, utilizing the PennyLane [47] and\nPytorch packages to construct and simulate the HTN.\nEach pixel is allocated to the corresponding qubit during\nthe PQCs construction, but the MNIST dataset is down-\nsampled to 4*4 grayscale images due to hardware restric-\ntions on the quantum simulation. The speciﬁc quantum\nexperiments are segregated into two categories: classiﬁ-\ncation and image reconstruction.\nA.\nClassiﬁcation on sixteen qubits\nThe MNIST dataset is downsampled to 16 (4*4\ngrayscale images). And we select 400 samples from 2 dif-\nferent classes (200/class) to create smaller datasets (from\nMNIST classes 0, 1). We divide these 400 samples into a\ntrain (300) and a test set (100).\nBased on the HTN framework presented in Fig. 1, we\nprovide a comprehensive description of the quantum cir-\ncuit that we implemented. The input stage performs nor-\nmalization of the 4*4 image, and each pixel corresponds\nto a qubit.\nThe Feature Map and Product States are\nconstructed using the RY and Hadamard gates to achieve\nstate preparation. The Tree Tensor Network applies the\nTNN wrapper network in PennyLane for initial model\ntraining on the dataset. We use the two-layer TNN func-\ntion for simulation in this part and choose the RY gate for\nbetter ﬁtting ability in network training. We added the\nmeasurement function at the end of the quantum circuit\nthat can accept either speciﬁed wires or an observable\nthat rotates the computational basis [47]. Speciﬁc PQCs\nare illustrated in Fig. 9(a).\nIn our experiment, we employed Mean Squared Error\n(MSE) as the loss function for training PQCs. After 15\nepochs, the loss function and accuracy of the network in\nthe training set are shown in Fig. 10(a), and we achieved\n100% accuracy on the test set.\n8\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nH\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\nRY\n(a)\n(b)\nFIG. 9. Quantum circuits of PQCs; (a) Classiﬁcation ; (b) Image reconstruction ;\n9\n0\n2\n4\n6\n8\n10\n12\n14\nepoch\n0.2\n0.4\n0.6\n0.8\nloss\naccuracy\n(a)\n0\n10\n20\n30\n40\n50\n60\nepoch\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nloss\n10\n12\n14\n16\n18\n20\n22\n24\nPSNR\nloss\nPSNR\n(b)\nFIG. 10. Training loss of PQCs; (a) Classiﬁcation ; (b) Image\nreconstruction.\nB.\nImage reconstruction on sixteen qubits\nWe conduct experiments using the handwritten digit 1\nfrom the MNIST dataset to implement image reconstruc-\ntion with the Hybrid Tensor Networks (HTN) framework.\nThe speciﬁc network model structure can be found in\nFig. 9(b), and the diﬀerence from the classiﬁcation ex-\nperiments lies in the measurement stage where we intro-\nduce four measurement nodes and reconstruct them into\na 2*2 grayscale image. Afterward, we utilize three layers\nof deconvolution layers to achieve image reconstruction.\nIn this experiment, we use the PSNR as the evaluation\nmetric for the reconstructed graph.\nThe loss function\ngraph from the experiment is presented in Fig. 10(b).\nWe achieved a PSNR of 22.72 on the test set.\nV.\nDISCUSSION AND FUTURE WORK\nWe propose hybrid tensor networks that combine ten-\nsor networks with classical neural networks in a uniform\nframework in order to overcome the limitations of regu-\nlar tensor networks in machine learning. Based on the\nnumerical experiments, we conclude with the following\nobservations. (1) Regular tensor networks are not com-\npetent to be the basic building block of deep learning\ndue to the limitations of representation power i.e. the\nabsence of nonlinearity, and the restriction of scalabil-\nity. (2) HTN overcomes the deﬁciency in representation\npower of the regular tensor network by the nonlinear\nfunction from neural network units, and oﬀers good per-\nformance in scalability. (3) HTN could be trained by the\nstandard combination of BP and SGD algorithms, allow-\ning for inﬁnite possibilities in designing HTN following\ndeep learning principles. (4) HTN serves as an applica-\nble implementation of quantum feature engineering that\ncould be simulated on classical computers.\nThere are some interesting and potential research sub-\njects to be left in our future works. The ﬁrst one is to do\ndeep learning on quantum entanglement data by HTN.\nOur preliminary experiments in this paper focus on deal-\ning with product quantum states without entanglement,\nbut it is natural to extend HTN to the scenario of quan-\ntum entanglement data formed by MPS or PEPS etc.,\nwhich neural network is incapable of. Moreover, there\nare some works focusing on tensor network based quan-\ntum circuits which demonstrates an interesting way to\ndo quantum machine learning [19]. Additionally, some\nworks focus on quantum-classical machine learning by us-\ning parameter quantum circuit [48–52]. Inspired by these\nworks, the HTN is able to be implemented by parameter\nquantum circuits in the future. In this case, the training\nalgorithm should be revised to guarantee the isometry of\neach local tensor in the HTN.\nAcknowledgments.—\nDL is grateful to Shi-ju Ran for helpful discussions.\nAnd this work was supported by Tianjin Natural Science\nFoundation of China (20JCYBJC00500) and the Science\n& Technology Development Fund of Tianjin Education\nCommission for Higher Education (2018KJ217).\n[1] Frank Verstraete, Valentin Murg, and J. Ignacio Cirac,\n“Matrix product states, projected entangled pair states,\nand variational renormalization group methods for quan-\ntum spin systems,” Advances in Physics 57, 143–224\n(2008), arXiv:0907.2796.\n[2] Rom´an Or´us, “A practical introduction to tensor net-\nworks:\nMatrix product states and projected entan-\ngled pair states,” Annals of Physics 349, 117 (2014),\narXiv:1306.2164.\n[3] Rom´an Or´us, “Advances on tensor network theory:\nsymmetries, fermions, entanglement, and holography,”\nThe European Physical Journal B 87, 280 (2014),\narXiv:1407.6552.\n[4] Shi-Ju Ran, Emanuele Tirrito, Cheng Peng, Xi Chen,\nGang Su,\nand Maciej Lewenstein, “Review of ten-\nsor network contraction approaches,” arXiv preprint\narXiv:1708.09213 (2017).\n[5] Rom´an Or´us, “Tensor networks for complex quantum\nsystems,” Nature Reviews Physics , 1–13 (2019).\n[6] Stavros Efthymiou,\nJack Hidary,\nand Stefan Le-\nichenauer, “Tensornetwork for machine learning,” arXiv\npreprint arXiv:1906.06329 (2019).\n[7] Chase Roberts, Ashley Milsted, Martin Ganahl, Adam\nZalcman, Bruce Fontaine, Yijian Zou, Jack Hidary,\nGuifre Vidal, and Stefan Leichenauer, “Tensornetwork:\nA library for physics and machine learning,” arXiv\npreprint arXiv:1905.01330 (2019).\n[8] Zheng-Zhi Sun, Shi-Ju Ran,\nand Gang Su, “Tangent-\nspace gradient optimization of tensor network for ma-\nchine learning,” arXiv preprint arXiv:2001.04029 (2020).\n[9] E. Miles Stoudenmire and David J. Schwab, “Super-\nvised learning with tensor networks,” Advances in Neural\nInformation Processing Systems 29, 4799–4807 (2016),\n1605.05775.\n[10] Zhao-Yu Han, Jun Wang, Heng Fan, Lei Wang, and Pan\nZhang, “Unsupervised generative modeling using matrix\n10\nproduct states,” Physical Review X 8, 031012 (2018).\n[11] Ding Liu, Shi-Ju Ran, Peter Wittek, Cheng Peng,\nRaul Bl´azquez Garc´ıa, Gang Su, and Maciej Lewenstein,\n“Machine learning by unitary tensor network of hierar-\nchical tree structure,” New Journal of Physics 21, 073059\n(2019).\n[12] Andrzej Cichocki, Namgil Lee, Ivan Oseledets, Anh-Huy\nPhan, Qibin Zhao, Danilo P. Mandic, and Others, “Ten-\nsor networks for dimensionality reduction and large-scale\noptimization: Part 1 low-rank tensor decompositions,”\nFoundations and Trends® in Machine Learning 9, 249–\n429 (2016).\n[13] Andrzej Cichocki, Anh-Huy Phan, Qibin Zhao, Namgil\nLee,\nIvan Oseledets,\nMasashi Sugiyama,\nDanilo P.\nMandic,\nand Others, “Tensor networks for dimension-\nality reduction and large-scale optimization: Part 2 ap-\nplications and future perspectives,” Foundations and\nTrends® in Machine Learning 9, 431–673 (2017).\n[14] Song Cheng, Lei Wang, Tao Xiang,\nand Pan Zhang,\n“Tree tensor networks for generative modeling,” Physical\nReview B 99, 155131 (2019).\n[15] Zhuan Li and Pan Zhang, “Shortcut matrix prod-\nuct\nstates\nand\nits\napplications,”\narXiv\npreprint\narXiv:1812.05248 (2018).\n[16] Jean Kossaiﬁ, Zachary Lipton, Aran Khanna, Tommaso\nFurlanello, and Anima Anandkumar, “Tensor regression\nnetworks,” arXiv (2017).\n[17] Shi-Ju Ran, “Bayesian tensor network and optimiza-\ntion algorithm for probabilistic machine learning,” arXiv\npreprint arXiv:1912.12923 (2019).\n[18] Shi-Ju Ran, Zheng-Zhi Sun, Shao-Ming Fei, Gang Su,\nand Maciej Lewenstein, “Quantum compressed sensing\nwith unsupervised tensor network machine learning,”\narXiv preprint arXiv:1907.10290 (2019).\n[19] William Huggins, Piyush Patel, K. Birgitta Whaley, and\nE. Miles Stoudenmire, “Towards quantum machine learn-\ning with tensor networks,” Quantum Science and Tech-\nnology (2018).\n[20] Marcello Benedetti, Delﬁna Garcia-Pintos, Oscar Per-\ndomo, Vicente Leyton-Ortega, Yunseong Nam,\nand\nAlejandro Perdomo-Ortiz, “A generative modeling ap-\nproach for benchmarking and training shallow quantum\ncircuits,” npj Quantum Information 5, 45 (2019).\n[21] Amandeep Singh Bhatia, Mandeep Kaur Saggi, Ajay Ku-\nmar,\nand Sushma Jain, “Matrix product state–based\nquantum classiﬁer,” Neural computation 31, 1499–1517\n(2019).\n[22] Shi-Ju Ran, “Eﬃcient encoding of matrix product states\ninto quantum circuits of one-and two-qubit gates,” arXiv\npreprint arXiv:1908.07958 (2019).\n[23] Kunkun Wang, Lei Xiao, Wei Yi, Shi-Ju Ran, and Peng\nXue, “Quantum image classiﬁer with single photons,”\narXiv preprint arXiv:2003.08551 (2020).\n[24] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shi-\nmon Schocken, “Multilayer feedforward networks with a\nnonpolynomial activation function can approximate any\nfunction,” Neural networks 6, 861–867 (1993).\n[25] Kurt Hornik, “Approximation capabilities of multilayer\nfeedforward networks,” Neural networks 4, 251–257\n(1991).\n[26] Ivan Glasser, Nicola Pancotti,\nand J Ignacio Cirac,\n“Supervised learning with generalized tensor networks,”\narXiv preprint arXiv:1806.05964 (2018).\n[27] Ivan Glasser, Ryan Sweke, Nicola Pancotti, Jens Eisert,\nand Ignacio Cirac, “Expressive power of tensor-network\nfactorizations for probabilistic modeling,” in Advances in\nNeural Information Processing Systems (2019) pp. 1496–\n1508.\n[28] Jing Chen, Song Cheng, Haidong Xie, Lei Wang,\nand\nTao Xiang, “Equivalence of restricted boltzmann ma-\nchines and tensor network states,” Physical Review B\n97, 085104 (2018).\n[29] Xiao Yuan, Jinzhao Sun, Junyu Liu, Qi Zhao,\nand\nYou Zhou, “Quantum simulation with hybrid tensor net-\nworks,” Physical Review Letters 127, 040501 (2021).\n[30] Hans-Martin Rieser, Frank K¨oster,\nand Arne Peter\nRaulf, “Tensor networks for quantum machine learn-\ning,” Proceedings of the Royal Society A 479, 20230218\n(2023).\n[31] James Dborin, Fergus Barratt,\nVinul Wimalaweera,\nLewis Wright,\nand Andrew G Green, “Matrix product\nstate pre-training for quantum machine learning,” Quan-\ntum Science and Technology 7, 035014 (2022).\n[32] Zheng-Zhi Sun, Cheng Peng, Ding Liu, Shi-Ju Ran, and\nGang Su, “Generative tensor network classiﬁcation model\nfor supervised machine learning,” Physical Review B\n101, 075135 (2020).\n[33] Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin,\nand Dmitry P Vetrov, “Tensorizing neural networks,”\nin Advances in neural information processing systems\n(2015) pp. 442–450.\n[34] Timur\nGaripov,\nDmitry\nPodoprikhin,\nAlexander\nNovikov,\nand Dmitry Vetrov, “Ultimate tensorization:\ncompressing convolutional and fc layers alike,” arXiv\npreprint arXiv:1611.03214 (2016).\n[35] Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuex-\nian Hou, Ming Zhou,\nand Dawei Song, “A tensorized\ntransformer for language modeling,” in Advances in Neu-\nral Information Processing Systems (2019) pp. 2232–\n2242.\n[36] Yann LeCun, Yoshua Bengio, et al., “Convolutional net-\nworks for images, speech, and time series,” The handbook\nof brain theory and neural networks 3361, 1995 (1995).\n[37] Sepp Hochreiter and J¨urgen Schmidhuber, “Long short-\nterm memory,” Neural computation 9, 1735–1780 (1997).\n[38] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio, “Generative adversarial nets,” in Ad-\nvances in neural information processing systems (2014)\npp. 2672–2680.\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez,  Lukasz Kaiser,\nand Illia Polosukhin, “Attention is all you need,” in Ad-\nvances in neural information processing systems (2017)\npp. 5998–6008.\n[40] Feng Pan, Keyang Chen, and Pan Zhang, “Solving the\nsampling problem of the sycamore quantum circuits,”\nPhysical Review Letters 129, 090502 (2022).\n[41] Ian Goodfellow, Yoshua Bengio,\nand Aaron Courville,\nDeep learning (MIT press, 2016).\n[42] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanjay\nGhemawat, Geoﬀrey Irving, Michael Isard, et al., “Ten-\nsorﬂow: A system for large-scale machine learning,” in\n12th {USENIX} Symposium on Operating Systems De-\nsign and Implementation ({OSDI} 16) (2016) pp. 265–\n283.\n[43] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\n11\nChanan, Edward Yang, Zach DeVito, Zeming Lin, Alban\nDesmaison, Luca Antiga, and Adam Lerer, “Automatic\ndiﬀerentiation in pytorch,” (2017).\n[44] Diederik\nP\nKingma\nand\nJimmy\nBa,\n“Adam:\nA\nmethod for stochastic optimization,” arXiv preprint\narXiv:1412.6980 (2014).\n[45] Stanford Scanning Repository, “The code of the imple-\nmentation is available at,”\n(2020), https://github.\ncom/dingliu0305/Hybrid-Tensor-Network.\n[46] Akram Youssry, Gerardo A Paz-Silva, and Christopher\nFerrie, “Beyond quantum noise spectroscopy: modelling\nand mitigating noise with quantum feature engineering,”\narXiv preprint arXiv:2003.06827 (2020).\n[47] Ville Bergholm, Josh Izaac, Maria Schuld, Christian\nGogolin, Shahnawaz Ahmed, Vishnu Ajith, M Sohaib\nAlam, Guillermo Alonso-Linaje, B AkashNarayanan, Ali\nAsadi, et al., “Pennylane: Automatic diﬀerentiation of\nhybrid quantum-classical computations,” arXiv preprint\narXiv:1811.04968 (2018).\n[48] Rongxin Xia and Sabre Kais, “Hybrid quantum-classical\nneural network for generating quantum states,” arXiv\npreprint arXiv:1912.06184 (2019).\n[49] JS Otterbach, R Manenti, N Alidoust, A Bestwick,\nM Block, B Bloom, S Caldwell, N Didier, E Schuyler\nFried, S Hong, et al., “Unsupervised machine learn-\ning on a hybrid quantum computer,” arXiv preprint\narXiv:1712.05771 (2017).\n[50] Daiwei Zhu,\nNorbert M Linke,\nMarcello Benedetti,\nKevin A Landsman,\nNhung H Nguyen,\nC Huerta\nAlderete,\nAlejandro Perdomo-Ortiz,\nNathan\nKorda,\nA Garfoot, Charles Brecque, et al., “Training of quan-\ntum circuits on a hybrid quantum computer,” Science\nadvances 5, eaaw9918 (2019).\n[51] Ryan Sweke, Frederik Wilde, Johannes Meyer, Maria\nSchuld,\nPaul\nK\nF¨ahrmann,\nBarth´el´emy\nMeynard-\nPiganeau,\nand Jens Eisert, “Stochastic gradient de-\nscent for hybrid quantum-classical optimization,” arXiv\npreprint arXiv:1910.01155 (2019).\n[52] Walter Vinci, Lorenzo Buﬀoni, Hossein Sadeghi, Amir\nKhoshaman, Evgeny Andriyash,\nand Mohammad H\nAmin, “A path towards quantum advantage in training\ndeep generative models with quantum annealers,” arXiv\npreprint arXiv:1912.02119 (2019).\n",
  "categories": [
    "cs.LG",
    "quant-ph",
    "stat.ML"
  ],
  "published": "2020-05-15",
  "updated": "2024-08-14"
}