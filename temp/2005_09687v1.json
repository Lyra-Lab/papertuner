{
  "id": "http://arxiv.org/abs/2005.09687v1",
  "title": "Deep learning approaches for neural decoding: from CNNs to LSTMs and spikes to fMRI",
  "authors": [
    "Jesse A. Livezey",
    "Joshua I. Glaser"
  ],
  "abstract": "Decoding behavior, perception, or cognitive state directly from neural\nsignals has applications in brain-computer interface research as well as\nimplications for systems neuroscience. In the last decade, deep learning has\nbecome the state-of-the-art method in many machine learning tasks ranging from\nspeech recognition to image segmentation. The success of deep networks in other\ndomains has led to a new wave of applications in neuroscience. In this article,\nwe review deep learning approaches to neural decoding. We describe the\narchitectures used for extracting useful features from neural recording\nmodalities ranging from spikes to EEG. Furthermore, we explore how deep\nlearning has been leveraged to predict common outputs including movement,\nspeech, and vision, with a focus on how pretrained deep networks can be\nincorporated as priors for complex decoding targets like acoustic speech or\nimages. Deep learning has been shown to be a useful tool for improving the\naccuracy and flexibility of neural decoding across a wide range of tasks, and\nwe point out areas for future scientific development.",
  "text": "Deep learning approaches for neural decoding: from CNNs to\nLSTMs and spikes to fMRI\nJesse A. Livezey1,2,* and Joshua I. Glaser3,4,5,*\njlivezey@lbl.gov, j.glaser@columbia.edu\n*equal contribution\n1Biological Systems and Engineering Division, Lawrence Berkeley National Laboratory,\nBerkeley, California, United States\n2Redwood Center for Theoretical Neuroscience, University of California, Berkeley,\nBerkeley, California, United States\n3Department of Statistics, Columbia University, New York, United States\n4Zuckerman Mind Brain Behavior Institute, Columbia University, New York, United States\n5Center for Theoretical Neuroscience, Columbia University, New York, United States\nMay 21, 2020\nAbstract\nDecoding behavior, perception, or cognitive state directly from neural signals has applications\nin brain-computer interface research as well as implications for systems neuroscience. In the last\ndecade, deep learning has become the state-of-the-art method in many machine learning tasks\nranging from speech recognition to image segmentation. The success of deep networks in other\ndomains has led to a new wave of applications in neuroscience. In this article, we review deep\nlearning approaches to neural decoding. We describe the architectures used for extracting useful\nfeatures from neural recording modalities ranging from spikes to EEG. Furthermore, we explore\nhow deep learning has been leveraged to predict common outputs including movement, speech,\nand vision, with a focus on how pretrained deep networks can be incorporated as priors for\ncomplex decoding targets like acoustic speech or images. Deep learning has been shown to be a\nuseful tool for improving the accuracy and ﬂexibility of neural decoding across a wide range of\ntasks, and we point out areas for future scientiﬁc development.\n1\nIntroduction\nUsing signals from the brain to make predictions about behavior, perception, or cognitive state,\ni.e., “neural decoding”, is becoming increasingly important within neuroscience and engineering.\nOne common goal of neural decoding is to create brain computer interfaces, where neural signals\nare used to control an output in real time. This could allow patients with neurological or motor\ndiseases or injuries to, for example, control a robotic arm or cursor on a screen, or produce speech\nthrough a synthesizer. Another common goal of neural decoding is to gain a better scientiﬁc\nunderstanding of the link between neural activity and the outside world. To provide insight,\ndecoding accuracy can be compared across brain regions, cell types, diﬀerent types of subjects\n(e.g., with diﬀerent diseases or genetics), and diﬀerent experimental conditions [1–8]. Plus, the\nrepresentations learned by neural decoders can be probed to better understand the structure of\nneural computation [9–12]. These uses of neural decoding span many diﬀerent neural recording\nmodalities and span a wide range of behavioral outputs (Fig. 1A).\n1\narXiv:2005.09687v1  [q-bio.NC]  19 May 2020\nWithin the last decade, many researchers have begun to successfully use deep learning ap-\nproaches for neural decoding. A decoder can be thought of as a function approximator, doing\neither regression or classiﬁcation depending on whether the output is a continuous or categor-\nical variable. Given the great successes of deep learning at learning complex functions across\nmany domains [13–22], it is unsurprising that deep learning has become a popular approach\nin neuroscience. Here, we will review the many uses of deep learning for neural decoding. We\nwill emphasize how diﬀerent deep learning architectures can induce biases that can be beneﬁcial\nwhen decoding from diﬀerent neural recording modalities and when decoding diﬀerent behav-\nioral outputs. We hope this will prove useful to deep learning researchers aiming to understand\ncurrent neural decoding problems and to neuroscience researchers aiming to understand the\nstate-of-the-art in neural decoding.\n2\nDeep learning architectures\nAt their core, deep learning models share a common structure across architectures: 1) simple\ncomponents formed from linear operations (typically matrix multiplication or convolution) plus\na nonlinear operation (for example, rectiﬁcation or a sigmoid nonlinearity); and 2) composition\nof these simple components to form complex, layered architectures. There are many formats\nof neural networks, each with their own set of assumptions. In addition to feedforward neural\nnetworks, which have the basic structure described above, common architectures for neural\ndecoding are convolutional neural networks (CNNs) and recurrent neural networks (RNNs).\nWhile more complex deep network layer types, e.g., graph neural networks [23] or networks\nthat use attention mechanisms [24], have been developed, they have not seen as much use in\nneuroscience. Additionally, given that datasets in neuroscience typically have limited numbers\nof trials, simpler, more shallow deep networks (e.g., a standard convolutional network versus a\nresidual convolutional network [21]) are often used for neural decoding.\nRNNs typically use a sequence of inputs. RNNs are also capable of processing inputs that\nare sequences of varying lengths, which occurs in neuroscience data (e.g., trials of diﬀering\nduration). This is unlike a fully-connected network, which requires a ﬁxed dimensionality input.\nIn an RNN, the inputs are then projected into a hidden layer, which connects to itself across\ntime (Fig. 1B). Thus, recurrent networks are commonly used for decoding since they can ﬂexibly\nincorporate information across time. Finally, the hidden layer projects to an output, which can\nitself be a sequence (Fig. 1B), or just a single data point.\nCNNs can be adapted to input and output data in many diﬀerent formats. For example,\nconvolutional architectures can take in structured data (1d timeseries, 2d images, 3d volumes) of\narbitrary size. The convolutional layers will then learn ﬁlters of the corresponding dimensions,\nin order to extract meaningful local structure (Fig. 1C). The convolutional layers will be par-\nticularly useful if there are important features that are translation invariant, as in images. This\nis done hierarchically, in order to learn ﬁlters of varying scales (i.e., varying temporal or spatial\nfrequency content). Next, depending on the output that is being predicted, the convolutional\nlayers are fed into other types of layers to produce the ﬁnal output (e.g., into fully connected\nlayers to classify an image). In general, hierarchically combining local features is a useful prior\nfor image-like datasets.\nWeight-sharing, where the weights of some parameters are constrained to be the same, is\noften used for neural decoding. For instance, the parameters of a convolutional (in time) layer\ncan be made the same for diﬀering input channels or neurons, so that these inputs are ﬁltered\nin the same way. This is analogous to CNN parameters being shared across space or time in\n2d or 1d convolutions. For neural decoding, this can be beneﬁcial for learning a shared set\nof data-driven features for diﬀerent recording channels as an alternative to human-engineered\nfeatures.\nTraining a neural decoder uses supervised learning, where the network’s parameters are\nlearned to predict target outputs based on the inputs. Recent work has combined supervised\ndeep networks with unsupervised learning techniques. These unsupervised methods learn (typ-\n2\nically) lower dimensional representations that reproduce one data source (either the input or\noutput), and are especially prevalent when decoding images. One common method, generative\nadversarial networks (GANs)\n[25, 26], generate an output, e.g. an image, given a vector of\nnoise as input. GANs are trained to produce images that fool a classiﬁer deep network about\nwhether they are real versus generated images. Another method is convolutional autoencoders,\nwhich are trained to encode an image into a latent state, and then reconstruct a high ﬁdelity\nversion [27]. These unsupervised methods can produce representations of the decoding input or\noutput that are sometimes more conducive for decoding.\n3\nThe inputs of decoding: neural recording modalities and\nfeature engineering\n3.1\nNeural recording modalities\nTo understand how varying neural network architectures can be preferable for processing dif-\nferent neural signals, it is important to understand the basics of neural recording modalities.\nThese modalities diﬀer in their invasiveness, and their spatial and temporal precision.\nThe most invasive recordings involve inserting electrodes into the brain to record voltages.\nThis allows experimentalists to record spikes or action potentials, the fast electrical transients\nthat individual neurons use to signal, and the basic unit of neural signaling. To get binary spik-\ning events, the recorded signals are high-pass ﬁltered and thresholded. Datasets with spikes are\nthus binary time courses from all of the recording channels (Fig. 1A). These invasive measure-\nments also allow recording local ﬁeld potentials (LFPs), which are the low-pass ﬁltered version\n(typically below ∼200Hz) of the same recorded voltage. LFPs are thought to be the sum of input\nactivity of local neurons [32]. When all voltage is included across frequency bands, the voltage\nis generally referred to as wide-band activity. Datasets with LFP and wide-band are continuous\ntime courses of voltages from all the recording channels (Fig. 1A). Note that traditionally, due to\nthe distance between recording electrodes being greater than the spatial precision of recording,\nspatial relationships between electrodes are not utilized for decoding. Spikes, LFP, and wide-\nband are more commonly recorded from animal models than humans because of their invasive\nnature.\nAnother invasive technique for recording individual neurons’ activities is calcium imaging,\nwhich uses microscopy to capture images of ﬂuorescent calcium indicators that are sensitive to\nneurons’ spiking activity [33]. The raw outputs of calcium imaging are videos: pixels measure\nﬂuorescence at the times when, and locations where, neurons are active. Calcium imaging is\nonly used with animal models.\nElectrical potentials measured from outside of the brain, that is electrocorticography (ECoG)\nand electroencephalography (EEG), are common neural recording modalities used in humans.\nECoG recordings are from grids that record electrical potentials from the surface of the cortex,\nrequire surgical implantation, and often cover large function areas of cortex. EEG is a non-\ninvasive method that records from the surface of the scalp from up to hundreds of spatially\ndistributed channels. Like LFPs, datasets from ECoG and EEG recordings are continuous time\ncourses of electrical potentials across recording channels (Fig. 1A), but here the spatial layout\nof the channels is also sometimes used in decoding.\nNote that as these electrical recording\nmethods get less invasive, spatial precision decreases (from spikes to LFP to ECoG to EEG),\nwhich can lead to inferior decoding performance [34, 35]. Still, all these electrical signals can be\nrecorded at high temporal resolution (100s-1000s of Hz) which make them good candidates for\nfast time-scale decoding.\nMagnetoencephalography (MEG), functional near infrared spectroscopy (fNIRS), and func-\ntional magnetic resonance imaging (fMRI ) are also noninvasive recording modalities which are\nmost often used in human decoding experiments. MEG measures the weak magnetic ﬁelds that\nare induced by electrical currents in the brain. Like EEG and ECoG, MEG can be recorded\nwith high temporal precision. fNIRS and fMRI measure blood oxygenation (a proxy for neural\n3\nA\nB\nC\nDecoder Inputs, X\nSpikes\nLFP\nEEG/ECoG\nfMRI...\nMovement\nSpeech\nVision...\nDecoder Outputs, Y\nDecoder:\nY=f (X)\nGrid-x\nGrid-y\nTime\nVx\nVY\nUnits\nTime\nTime\nChannels\nVoxels\nVoxels\nTime\nvia ﬁlter\nX1\nX\n...\nY\nX2\nX3\nX4\nY4\nY3\nY2\nY1\nRNN Decoder\nCNN Decoder\nSlices\nFigure 1: Schematics. A: Schematics of neural decoding, which can use many diﬀerent neural\nmodalities as input (top) and can predict many diﬀerent outputs (bottom). Embedded ﬁgures\nare adapted from [28–30]. B: A schematic of a standard recurrent neural network (RNN). Each\narrow represents a linear transformation followed by a nonlinearity.\nArrows of the same color\nrepresent the same transformations occurring. The circles representing the hidden layer typically\ncontain many hidden units. More sophisticated versions of RNNs, which include gates that control\ninformation ﬂow through various parts of the network, are commonly used. For example, see [31]\nfor a schematic of an LSTM. C: A schematic of a convolutional neural network. A convolutional\ntransformation takes a learned ﬁlter and convolves it with the input (here, a 2d input), and then\npasses this through a nonlinearity. This means that here, a 2×2 ﬁlter will be multiplied pixel-wise\nwith all 2×2 blocks to get the values of the next layer in the network.\n4\nTime\nFrequency\nA\nB\nC\nGrid-x\nGrid-y\nTime\nTime\nElectrodes\nHand-engineered\nLearned\nHigh gamma amplitude\nWavelet amplitude\nRaw voltage\nFigure 2: Feature engineering for neural decoding. For all plots, the red box indicates a set of\nfeatures across time, space, or frequency which will be ﬁltered together by the ﬁrst layer’s convo-\nlutional or recurrent window. The red arrows indicate axes along which convolution or recurrence\nmay be performed. Sample data from [29]. A: High gamma amplitude, which is selected from a\nlarge ﬁlterbank of features from B, is shown spatially laid out in the ECoG grid locations. Deep\nnetwork ﬁlters combine hand-engineered high gamma features across space and time. B: Spec-\ntrotemporal wavelet decomposition of the raw data, from C, may be used as the input to a deep\nnetwork. The deep network ﬁlter shown combines features across frequency and time and can be\nshared across channels. C: Raw electrical potential recorded using ECoG across channels. The\ndeep network ﬁlter shown combines features across time and can be shared across channels.\nactivity), through its absorption of light and with resonance imaging respectively, and their\ntemporal resolution are temporally limited by its dynamics. fNIRS and fMRI datasets contain\nactivity signals in diﬀerent “voxels (locations) of the brain over time. Due to the limited tempo-\nral resolution, sometimes the temporal continuity of this data is not used for decoding purposes\n(Fig. 1A).\n3.2\nFeature engineering\nFor each of these recording modalities, the raw data are processed to create features that are ben-\neﬁcial for decoding. Sometimes, these features are hand-engineered based on previous knowledge,\ntraditionally with the goal of creating features that are most compatible with linear decoders.\nOther times, this feature engineering is part of the deep learning architecture. That is, a more\nraw form of the input is provided into the decoder, and a ﬁrst stage of the deep network decoder\nwill automatically learn to extract relevant features. Speciﬁc neural network architectures can\nbe beneﬁcial for this automatic feature engineering (Fig. 2).\nFor use in decoding, spikes are typically ﬁrst converted into ﬁring rates by determining the\nnumber of spikes in time bins. Then, these ﬁring rates are fed into the decoder. This general\napproach of decoding based on ﬁring rates (an assumption of “rate coding”) is standard. While\nusing precise temporal timing of spikes (“temporal coding”) for decoding has been done [36],\nwe are not aware of examples using deep learning. Given that ﬁring rates are used as inputs,\nadditional neural network architectures are not used to extract unknown features from the input.\nHowever, in future research, it might be advantageous to provide a more raw form of spiking as\ninput, and use deep learning architectures to do feature engineering. For rate coding, the best\n5\nsize and temporal placement of time bins could be automatically determined, and for temporal\ncoding, features related to the precise timing of spikes could be learned.\nWhen analyzing calcium imaging data, the videos are typically preprocessed to extract time\ntraces of ﬂuorescences over time for each neuron [37]. Sometimes, additional processing will be\ndone to estimate spiking events from the calcium traces [38]. Deep learning tools exist for both\nof these processing steps [39, 40]. For decoding, either the ﬂuorescences, or the estimated ﬁring\nrates (via the estimated spike trains), are then used as input. While it could be possible to\ndevelop an end-to-end decoder that works with the videos as input, this may prove challenging\ngiven the potential for overﬁtting with high-dimensional input.\nWhen decoding from wide-band, LFP, EEG, and ECoG data, it is common to ﬁrst extract\nspectrotemporal features from the data, for example the signals in speciﬁc frequency bands.\nSometimes, only “task-relevant” frequencies will be used for decoding - for instance, using\nhigh gamma frequencies in ECoG to decode speech [41, 42] (Fig. 2A). More frequently, many\nfrequencies will be included, to better understand which are contributing to decoding [12, 43].\nSimilar to frequency selection based on domain knowledge, ECoG grid electrodes and fMRI\nvoxels are often subselected by hand or with statistical tests. In general, these extracted features\ncan then be put into almost any type of decoder, such as linear (or logistic) regression or a deep\nneural network (e.g. [44]).\nIt is also possible to let a deep learning architecture do more of the feature extraction. One\napproach is to ﬁrst convert each electrode’s signal into a frequency domain representation over\ntime (i.e., a spectrogram), often via a wavelet transform. Then, this 2-dimensional representation\n(like an image) is provided as input to a CNN [35, 45–47] (Fig. 2B). If multiple electrode channels\nare being used for decoding, each channel can be fed into an independent CNN, or alternatively,\nthe CNN weights for each channel can be shared [35]. The CNN will then learn the relevant\nfrequency domain representation for the decoding.\nAnother approach is to provide the raw input signals into a deep learning architecture\n(Fig. 2C). To learn temporal features, typically the signal is fed into a 1-dimensional CNN,\nwhere the convolutions occur in the time domain. This has been done with a standard CNN\n[48], in addition to variant architectures. Ahmadi et al. [49] used a temporal convolutional net-\nwork, which is a more complex version of a 1-dimensional CNN that (among other things) allows\nfor multiple timescales of inputs to aﬀect the output. Li et al. [50] used parameterized versions\nof temporal ﬁlters that target synchrony between electrodes. These convolutional approaches\nwill automatically learn temporal ﬁlters (like frequency bands) that are relevant for decoding.\nIn addition to temporal structure, there is often spatial structure of the electrode channels\nthat can also be leveraged for decoding (Fig. 2A). Convolutional ﬁlters can be used in the spatial\ndomain to learn spatial representations that are relevant for decoding, for example local func-\ntional correlation structure. It is common for the temporal ﬁlters and spatial ﬁlters to be learned\nin successive layers of the network, either temporal followed by spatial [51, 52] or vice-versa [53].\nAdditionally, 3-dimensional convolutional ﬁlters can be learned that simultaneously incorporate\nboth temporal and (2-dimensional) spatial dimensions [54] or 3 spatial dimensions [55]. Includ-\ning spatial ﬁlters, which is most common in EEG and ECoG, can help learn spatial motifs that\nare most relevant for the task. Moreover, from a practical perspective, convolutional networks\nare an eﬃcient way of processing high-dimensional spatial data.\n4\nThe outputs of decoding\nNeural decoding is used to predict many outputs, including movement, speech, vision, and\nmore. Sometimes, the output variable will be directly predicted from the neural inputs, e.g.,\nwhen predicting movement velocities. Other times, the decoder may be trained to predict some\nintermediate representation, which has a predetermined mapping to the output (Fig. 3). For\nexample, a GAN can be trained to generate an image using a small number of latent variables.\nThis mapping from the low-dimensional variables to images can be learned without having\nto simultaneously record neural activity. Then, to decode an image from neural activity, one\n6\ncan train the decoder to predict the latent variables to be fed into the GAN, rather than the\nentire high-dimensional image. This two-step approach can be especially beneﬁcial when the\noutput data is complex and high-dimensional, as is often the case in vision or speech. In eﬀect,\nthe generative model can act as a prior on the underconstrained decoding solution. Across the\nfollowing decoding outputs, researchers have used both the “direct” and “intermediate mapping”\napproaches (Fig. 3).\n4.1\nMovement\nSome of the earliest uses of neural decoding were in the motor system [56]. Researchers have used\nneural activity from motor cortex to predict many diﬀerent motor outputs, such as movement\nkinematics (e.g., position and velocity), muscle activity (EMG), and broad type of movement.\nTraditionally, this decoding has used methods (e.g., Kalman Filter or Wiener Filter) that as-\nsumed a linear mapping from neural activity to the motor output, which has led to many\nsuccesses [57–60].\nTo improve the decoders, these methods were extended to allow speciﬁc\nnonlinearities (e.g., Unscented Kalman Filter and Wiener Cascade [61–64]). Within the last\ndecade, deep learning methods have become more common, frequently outperforming linear\nmethods and their direct nonlinear extensions when compared (e.g., [28, 53, 65, 66]).\nDeep learning methods for decoding movement have been applied to a wide range of prob-\nlems. Researchers have used many input signals that have high temporal resolution, including\nspikes [28, 65–70], wide-band [71, 72], LFP [44, 49], EEG [73, 74], and ECoG [53, 75–77]. Ad-\nditionally, deep learning has been used to predict many diﬀerent outputs. Often the output\nis a continuous variable, such as the position, angle, or velocity of a limb, joint, or cursor\n[28, 44, 49, 53, 65, 66, 69, 70, 73], or a muscles EMG [67] (Fig. 3B). Rather than predicting\na continuous variable, sometimes the goal is to classify diﬀerent movement types [71, 72, 74–\n77], for example, classifying which ﬁnger is moving [75]. Finally, deep learning decoders have\nbeen used to predict movements from eﬀectors across diﬀerent parts of the body, including arm\n[28, 44, 49, 65, 66, 68, 70], leg [65, 69, 73], wrist [67, 71, 72], and ﬁnger movements [53, 71, 72, 75–\n77]. Thus, deep learning methods have shown to be a very ﬂexible tool for movement decoding.\nRNNs are by far the most common deep learning architecture for movement decoding. When\npredicting a continuous movement variable, there is generally a linear mapping from the RNNs\noutput to the movement variable. When classifying movements, there is an additional softmax\nnonlinearity that determines the movement with the highest probability. From a deep learning\nperspective, given that this is a problem of converting one sequence (a temporal trace of neural\nactivities) into another sequence (motor outputs), it would be expected that an RNN would be an\nappropriate architecture. Recurrent architectures also make sense from a scientiﬁc perspective:\nmotor cortical activity has dynamics that are important for producing movements [78], plus\nmovements themselves have dynamics.\nLSTMs have generally been the most common and successful type of RNN for decoding\n[28, 44, 53, 65, 67–69, 75–77], although other standard types of RNN architectures (e.g., GRUs\n[73] and echostate networks [70]) have also proven successful. Additionally, researchers have\nfound that stacking multiple layers of LSTMs [65, 75] can improve performance beyond a single\nLSTM [65]. LSTMs are likely successful because they are able to learn long-term dependencies\nbetter than a standard “vanilla” RNN [31].\nA common goal of neural decoding of movement is to be able to create a usable brain\ncomputer interface for patients. While the majority of deep learning uses have been in oﬄine\nscenarios (decoding after the neural recording), there are several successful examples of real-\ntime uses of deep learning for movement decoding [66, 70–72]. For example, in human patients\nwith tetraplegia who had implanted electrode arrays, Schwemmer et al. [71] were able to classify\nplanned movements of wrist extension, wrist ﬂexion, index extension, and index ﬂexion. They\nthen applied functional electrical stimulation to activate muscles according to this decoder, so\nthat the patient was able to make these movements in real time. In Sussillo et al. [70], monkeys\nwith implanted electrode arrays were able to control the velocity of a cursor on a screen in real\ntime.\n7\nI - T - _ - I - S - _ - O - U - R - ...\nSeq2Seq generation\nIntermediate\nfeature vector\nCNN\nGAN\nAcoustic model\nConcurrent behavior\nRNN\n. . .\n. . .\nNeural data\nNeural data\nA\nC\nB\nE\nF\nG\nRNN\n. . .\nDirect Decoding\nDecoding Through\nIntermediate Variables\nRNN intermediate state\nD\nSpectrogram\nH\nFigure 3: Architectures and outputs of decoding. A: Sequential inputs can be processed by RNNs\nwhich can use past context (or past and future in bi-directional RNNs). B: RNN outputs at each\ntimestep can be mapped to behaviors, e.g., movements, measured concurrently. C: The ﬁnal output\nof an RNN can be used as the input to a decoding network which can produce a second sequence\nof a diﬀerent length, such as text. D: RNNs can produce an intermediate state to be used in a\nsecond decoding step. E: Intermediate states can often be structured, such as a spectrogram in\nthis example. F: Intermediate states can be fed into an acoustic model which produces acoustic\nwaveforms.\nG: Image-like inputs can be processed by CNNs to produce intermediate feature\nvectors. H: Feature vectors can be fed into generative image models, e.g., a GAN, to produce a\nmore realistic looking image.\n8\nWhile there has been great initial success, there are several challenges associated with using\ndeep learning for real-time decoding for brain computer interfaces. One challenge is that the\nsource of the recorded neural activity can change across days, for example due to slight movement\nof implanted electrodes. One approach that has dealt with this is the multiplicative RNN, which\nallows mappings from the neural input to the motor output to partially change across days [66].\nAnother challenge is computation time, as there is the need to make predictions through the deep\nlearning architecture at very high temporal resolution. When using a less complicated echostate\nnetwork, Sussillo et al. [70] were able to decode with less than 25 ms temporal resolution.\nHowever, when using a more complex architecture of LSTMs followed by CNNs, Schwemmer\net al. [71] decoded at 100 ms resolution, slower than our perception. Relatedly, for linear methods\nthat can be ﬁt rapidly, researchers are able to adapt the decoder in real time to better match\nthe subjects intention (trying to get to a target) to improve performance [58, 62]. Developing\nsimilar approaches for deep learning based decoders is an exciting, unexplored area.\n4.2\nSpeech\nVocal articulation is a complex behavior that engages a large functional area of the brain to\nproduce movements that have a high degree of articulatory temporal and spatial precision [79].\nIt is also a uniqely human ability which limits the recording modalities and neuroscientiﬁc\ninterventions that can be used to study it. Due to the functional and temporal requirements\nof decoding speech, cortical surface electrical potentials recorded using ECoG is the typical\nrecording modality used, although penetrating electrodes, MEG, EEG, and fNIRS are also\nused [80–83]. When decoding from ECoG or EEG, researchers commonly use the signals’ high\ngamma amplitude [41], although some use more broad spectrotemporal features as well [41, 43,\n84].\nMany approaches to decoding speech from neural signals have used some combination of\nlinear methods and shallow probabilistic models. Clustering, SVMs, LDA, linear regression,\nand probabilistic models have been used with spectrotemporal features of electrical potentials\nto decode vowel acoustics, speech articulator movements, phonemes, whole words, and semantic\ncategories [41, 43, 80, 85–88].\nDeep learning approaches to decoding speech from neural signals have emerged that can\npotentially learn nonlinear mappings. Some of these approaches have operated on temporally\nsegmented neural data and have thus used fully connected neural network architectures. For\nexample, spectrotemporal features derived from ECoG or EEG have been used to reconstruct\nperceived spectrograms, classify words or syllables, or classify entire phrases [12, 42, 82–84].\nThese examples with temporally segmented neural data are useful for increasing understanding\nabout neural representations, and as a step towards decoding natural speech.\nMapping directly from continuous, time-varying neural signals to speech is the goal of speech\nbrain-computer interfaces [89, 90]. Both convolutional and recurrent networks are able to ﬂex-\nibly decode timeseries data and are often used for decoding naturalistic speech. Heelan et al.\n[91] reconstructed perceived speech audio from multi-unit spike counts from a non-human pri-\nmate and found that LSTM-based networks outperformed other traditional and deep models.\nSpeech represented as text does not have a simple one-to-one temporal alignment to regularly\nsampled neural signals. For this reason, speech-to-text decoding networks often use architec-\ntures and methods like sequence-to-sequence models or the connectionist temporal classiﬁcation\nloss [20, 92], which are commonly used in machine translation or automated speech recognition\napplications. As such, several groups have decoded directly from neural signals to text using\nrecurrent networks such as sequence-to-sequence models [93, 94] (Fig. 3C).\nFor decoding intelligible acoustic speech, it is also common to split decoding into a more\nconstrained neural-to-intermediate mapping, followed by a second stage that maps this interme-\ndiate format into an acoustic waveform using acoustic priors for speech based on deep learning or\nhand-engineered methods. For instance, high gamma features recorded using ECoG have been\nused to decode spectrograms and speech articulator dynamics [54, 95] as intermediate states.\nThen, either a WaveNet deep network [96] was used to directly produce an acoustic waveform\n9\nfrom the spectrogram [54], or an RNN was used to produce acoustic features which were fed into\na speech synthesizer [95]. These second stages do not require invasive neural data for training\nand were trained on a larger second corpus.\nDeep learning models have improved the accuracy of primarily oﬄine speech decoding tasks.\nMany of the preprocessing and decoding methods reviewed here are done oﬄine using acausal or\nhigh-latency deep learning models. Developing deep learning methods, software, and hardware\nfor real-time speech decoding is important for clinical applications of brain computer inter-\nfaces [88, 97].\n4.3\nVision\nSimilar to decoding acoustic speech, decoding visual stimuli from neural signals requires strong\nimage priors due to the large variability of natural scenes and the relatively small bit-rate of\nneural recordings. Early attempts to reconstruct the full visual experience restricted decoding\nto simple images [98] or relied on a ﬁlterbank encoding model and a large set of natural images\nas a sampled prior [99]. Qiao et al. [100] solved the simpler task of classifying perceived object\ncategory using one CNN to select a small set of fMRI voxels which were fed into a second RNN\nfor classiﬁcation. Similarly, Ellis and Michaelides [101] classiﬁed among many visual scenes from\ncalcium imaging data using feedforward or convolutional neural networks.\nAs mentioned in Deep learning architectures, deep generative image models, such as GANs,\ncan produce realistic images. In addition, CNNs trained to classify large naturalistic image\ndatabases [102] (discriminative models) have been shown to encode a large amount of textural\nand semantic meaning in their activations [103], which can be used as an image prior. Due to the\nvariety of ways that natural image priors can be created with deep networks, there exist decoding\nmethods that combine diﬀerent aspects of both generative and discriminative networks.\nGiven a deep generative model of images, a simpler decoder can be trained to map from\nneural data to the latent space of the model [104, 105], and the generative model can be used\nfor image reconstruction. Similarly, a linear stage reconstruction followed by a deep network\nthat cleans-up the image has been used with retinal ganglion cell output [27]. Generative models\ncan also be trained to reconstruct images directly from fMRI responses on real data with data\naugmentation from a simulated encoding model [106].\nAlternatively, generative and discriminative models can be used together. By leveraging a\npretrained CNN, a simple decoder can be trained to map neural data to CNN activations that can\nthen be passed into a convolutional image reconstruction model [107]. Additionally, the input\nimage in a pretrained CNN can be optimized so that the CNN activations match predictions\ngiven by the fMRI responses [108].\nResearchers have also used an end-to-end approach in\nwhich they train the generative part directly on neural data with both an adversarial loss and\na pretrained CNN feature loss [109]. Along with acoustic speech, decoding naturalistic visual\nstimuli presents one of the best cases to study the use of data-driven priors derived from deep\nnetworks.\n4.4\nOther outputs\nWhile we have chosen to focus on a few decoding outputs that are prevalent in the literature, deep\nlearning has been used for a myriad of decoding applications. RNNs such as LSTMs have been\nused to decode an animals location [28, 35, 110, 111] and direction [112] from spiking activity in\nthe hippocampus and head-direction cells, respectively. LSTMs have been used to decode what\nis being remembered in a working memory task from human fMRI [113]. Researchers have used\nLSTMs [114] and feedforward neural networks [115] to classify diﬀerent classes of behaviors,\nusing spiking activity in animals [115] and fNIRS measurements in humans [114].\nLSTMs\n[116, 117] and CNNs [118] have been used to classify emotions from EEG signals. Feedforward\nneural networks have been used to determine the source of a subjects attention, using EEG in\nhumans [119, 120] and spiking activity in monkeys [121]. CNNs [46–48], along with LSTMs [48]\n10\nhave been used to predict a subject’s stage of sleep from their EEG. For almost any behavioral\nsignal that can be decoded, someone has tried to use deep learning.\n5\nDiscussion\nDeep learning is an attractive method for use in neural decoding because of its ability to learn\ncomplex, nonlinear transformations from data. In many of the examples above, deep networks\ncan outperform linear or shallow methods even on relatively small datasets; however, examples\nexist where this is not the case, especially when using fMRI [122, 123] or fNIRS data [124].\nRelatedly, there are many times in which using hand-engineered features can outperform an\nend-to-end neural network that will learn the features. This is more likely with limited amounts\nof data, and also when there is strong prior knowledge about the relevant features. One general\nmachine learning approach to eﬃciently use limited data is transfer learning, in which a neural\nnetwork trained in one scenario (typically with more data) is used a separate scenario. This has\nbeen used in neural decoding to more eﬀectively train decoders for new subjects [77, 94] and\nfor new predicted outputs [71]. As the capability to generate ever larger datasets develops with\nautomated, long-term experimental setups for single animals [125] and large scale recordings\nacross multiple animals [126], deep learning is well poised to take advantage of this ﬂood of\ndata. As dataset sizes increase, this will also allow more features to be learned through data-\ndriven network training rather than being selected by-hand.\nAlthough deep learning will inevitably improve decoding accuracy as neuroscientists collect\nlarger datasets, extracting scientiﬁc knowledge from trained networks is still an area of active\nresearch. That is, can we understand the transformations deep networks are learning? In com-\nputer vision, layers that include spatial attention [127] and methods for performing feature\nattribution [128] have been developed to understand what parts of the input are important for\nprediction, although the latter are an active area of research [129]. These methods could be\nused to attribute what channels, neurons, or time-points are most salient for decoding [128].\nAdditionally, there are methods for understanding deep network representations in computer\nvision that examine the representations networks have learned across layers [130, 131]. Using\nthese methods may help to understand the transformations that occur within neural decoders,\nhowever results may be sensitive to the decoder’s architecture and not purely the data’s struc-\nture. While deep learning interpretability methods are not commonly used on decoders trained\non neural data, there are a few examples of networks that were built with interpretability in\nmind or were investigated after training [12, 50, 51, 113].\nWhen interpreting decoders, it is often assumed that the decoder reveals the information\ncontained in the brain about the decoded variable. It is important to note that this is only\npartially true when priors are being used for decoding [132], which is often the case when\ndecoding a full image or acoustic speech. In these scenarios, the decoded outputs will be a\nfunction of both neural activity and the prior, so one cannot simply determine what information\nthe brain has about the output.\nThe software used to create, train, and evaluate deep networks has been steadily developed\nand is now almost as easy to use as other standard machine learning methods. A wide range\nof cost functions, layer types, and parameter optimization algorithms are implemented and\naccessible in deep learning libraries such as PyTorch or Tensorﬂow [133, 134] and libraries in\nother programming languages. Like other machine learning methods, care must be taken to\ncarefully cross-validate results as deep networks can easily overﬁt to the training data.\nIn addition to their use in neural decoding, deep learning has other prominent uses within\nneuroscience [135, 136]. Neural networks have a long history in neuroscience as models of neural\nprocessing [137, 138]. More recently, there has also been a surge of papers using deep networks\nas encoding models [9, 11, 139]. There has been a speciﬁc focus on using the representations\nlearned by deep networks trained to perform behavioral tasks (e.g., image recognition) to predict\nneural responses in corresponding brain areas (e.g., across the visual hierarchy [140]). Combining\nthese multiple complementary approaches is one promising approach to understanding neural\n11\ncomputation.\nAcknowledgements\nWe would like to thank Ella Batty and Charles Frye for very helpful comments on this manuscript.\nFunding\nJIG was supported by National Science Foundation NeuroNex Award DBI-1707398 and The\nGatsby Foundation AT3708. JAL was supported by the LBNL Laboratory Directed Research\nand Development program.\nReferences\n[1] Rodrigo Quian Quiroga, Lawrence H Snyder, Aaron P Batista, He Cui, and Richard A\nAndersen. Movement intention is better predicted than attention in the posterior parietal\ncortex. Journal of neuroscience, 26(13):3615–3620, 2006.\n[2] Stephenie A Harrison and Frank Tong. Decoding reveals the contents of visual working\nmemory in early visual areas. Nature, 458(7238):632–635, 2009.\n[3] Soumyadipta Acharya, Matthew S Fifer, Heather L Benz, Nathan E Crone, and Nitish V\nThakor. Electrocorticographic amplitude predicts ﬁnger positions during slow grasping\nmotions of the hand. Journal of neural engineering, 7(4):046002, 2010.\n[4] Martin Weygandt, Carlo R Blecker, Axel Sch¨afer, Kerstin Hackmack, John-Dylan Haynes,\nDieter Vaitl, Rudolf Stark, and Anne Schienle.\nfmri pattern recognition in obsessive–\ncompulsive disorder. Neuroimage, 60(2):1186–1193, 2012.\n[5] Erin L Rich and Jonathan D Wallis. Decoding subjective decisions from orbitofrontal\ncortex. Nature neuroscience, 19(7):973, 2016.\n[6] Joshua I Glaser, Matthew G Perich, Pavan Ramkumar, Lee E Miller, and Konrad P\nKording. Population coding of conditional probability distributions in dorsal premotor\ncortex. Nature communications, 9(1):1–14, 2018.\n[7] Liberty S Hamilton, Erik Edwards, and Edward F Chang. A spatial map of onset and\nsustained responses to speech in the human superior temporal gyrus. Current Biology, 28\n(12):1860–1871, 2018.\n[8] Nora Brackbill, Colleen Rhoades, Alexandra Kling, Nishal P Shah, Alexander Sher,\nAlan M Litke, and EJ Chichilnisky.\nReconstruction of natural images from responses\nof primate retinal ganglion cells. bioRxiv, 2020.\n[9] Lane McIntosh, Niru Maheswaranathan, Aran Nayebi, Surya Ganguli, and Stephen Bac-\ncus. Deep learning models of the retinal response to natural scenes. In Advances in neural\ninformation processing systems, pages 1369–1377, 2016.\n[10] Tasha Nagamine and Nima Mesgarani. Understanding the representation and computation\nof multilayer perceptrons: A case study in speech recognition. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70, pages 2564–2573. JMLR. org,\n2017.\n12\n[11] Alexander JE Kell, Daniel LK Yamins, Erica N Shook, Sam V Norman-Haignere, and\nJosh H McDermott. A task-optimized neural network replicates human auditory behavior,\npredicts brain responses, and reveals a cortical processing hierarchy. Neuron, 98(3):630–\n644, 2018.\n[12] Jesse A Livezey, Kristofer E Bouchard, and Edward F Chang. Deep learning as a tool\nfor neural data analysis: speech classiﬁcation and cross-frequency coupling in human\nsensorimotor cortex. PLoS computational biology, 15(9):e1007091, 2019.\n[13] Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J Frey. Predict-\ning the sequence speciﬁcities of dna-and rna-binding proteins by deep learning. Nature\nbiotechnology, 33(8):831–838, 2015.\n[14] Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami,\nLeonidas J Guibas, and Jascha Sohl-Dickstein. Deep knowledge tracing. In Advances\nin neural information processing systems, pages 505–513, 2015.\n[15] Michela Paganini, Luke de Oliveira, and Benjamin Nachman. Calogan: Simulating 3d\nhigh energy particle showers in multilayer electromagnetic calorimeters with generative\nadversarial networks. Physical Review D, 97(1):014021, 2018.\n[16] Thorsten Kurth, Sean Treichler, Joshua Romero, Mayur Mudigonda, Nathan Luehr, Ev-\nerett Phillips, Ankur Mahesh, Michael Matheson, Jack Deslippe, Massimiliano Fatica,\net al. Exascale deep learning for climate analytics. In SC18: International Conference for\nHigh Performance Computing, Networking, Storage and Analysis, pages 649–660. IEEE,\n2018.\n[17] Kristof T Sch¨utt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R\nM¨uller. Schnet–a deep learning architecture for molecules and materials. The Journal of\nChemical Physics, 148(24):241722, 2018.\n[18] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[19] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation with deep\nconvolutional neural networks.\nIn Advances in neural information processing systems,\npages 1097–1105, 2012.\n[20] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in neural information processing systems, pages 3104–3112, 2014.\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[22] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Bat-\ntenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al.\nDeep speech 2: End-to-end speech recognition in english and mandarin. In International\nconference on machine learning, pages 173–182, 2016.\n[23] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu\nPhilip. A comprehensive survey on graph neural networks. IEEE Transactions on Neural\nNetworks and Learning Systems, 2020.\n[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in\nneural information processing systems, pages 5998–6008, 2017.\n13\n[25] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\nneural information processing systems, pages 2672–2680, 2014.\n[26] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning\nwith deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434,\n2015.\n[27] Nikhil Parthasarathy, Eleanor Batty, William Falcon, Thomas Rutten, Mohit Rajpal,\nEJ Chichilnisky, and Liam Paninski.\nNeural networks for eﬃcient bayesian decoding\nof natural images from retinal neurons. In Advances in Neural Information Processing\nSystems, pages 6434–6445, 2017.\n[28] Joshua I Glaser, Raeed H Chowdhury, Matthew G Perich, Lee E Miller, and Konrad P\nKording. Machine learning for neural decoding. arXiv preprint arXiv:1708.00909, 2017.\n[29] Kristofer E. Bouchard and Edward F Chang.\nHuman ecog speaking consonant-vowel\nsyllables, 2019. URL https://doi.org/10.6084/m9.figshare.c.4617263.v4.\n[30] Samaneh Kazemifar, Kathryn Y Manning, Nagalingam Rajakumar, Francisco A Gomez,\nAndrea Soddu, Michael J Borrie, Ravi S Menon, Robert Bartha, Alzheimers Disease Neu-\nroimaging Initiative, et al. Spontaneous low frequency bold signal variations from resting-\nstate fmri are decreased in alzheimer disease. PloS one, 12(6), 2017.\n[31] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\n[32] Gy¨orgy Buzs´aki, Costas A Anastassiou, and Christof Koch. The origin of extracellular\nﬁelds and currentseeg, ecog, lfp and spikes. Nature reviews neuroscience, 13(6):407–420,\n2012.\n[33] Tsai-Wen Chen, Trevor J Wardill, Yi Sun, Stefan R Pulver, Sabine L Renninger, Amy\nBaohan, Eric R Schreiter, Rex A Kerr, Michael B Orger, Vivek Jayaraman, et al. Ultra-\nsensitive ﬂuorescent proteins for imaging neuronal activity. Nature, 499(7458):295–300,\n2013.\n[34] Robert D Flint, Christian Ethier, Emily R Oby, Lee E Miller, and Marc W Slutzky. Local\nﬁeld potentials allow accurate decoding of muscle activity. Journal of neurophysiology,\n108(1):18–24, 2012.\n[35] Markus Frey, Sander Tanni, Catherine Perrodin, Alice OLeary, Matthias Nau, Jack Kelly,\nAndrea Banino, Christian F Doeller, and Caswell Barry. Deepinsight: a general framework\nfor interpreting wide-band neural activity. bioRxiv, page 871848, 2019.\n[36] Andr´e Maia Chagas, Lucas Theis, Biswa Sengupta, Maik Christopher St¨uttgen, Matthias\nBethge, and Cornelius Schwarz. Functional analysis of ultra high information rates con-\nveyed by rat vibrissal primary aﬀerents. Frontiers in neural circuits, 7:190, 2013.\n[37] Andrea Giovannucci, Johannes Friedrich, Pat Gunn, Jeremie Kalfon, Brandon L Brown,\nSue Ann Koay, Jiannis Taxidis, Farzaneh Najaﬁ, Jeﬀrey L Gauthier, Pengcheng Zhou,\net al. Caiman an open source tool for scalable calcium imaging data analysis. Elife, 8:\ne38173, 2019.\n[38] Joshua T Vogelstein, Adam M Packer, Timothy A Machado, Tanya Sippy, Baktash\nBabadi, Rafael Yuste, and Liam Paninski. Fast nonnegative deconvolution for spike train\ninference from population calcium imaging. Journal of neurophysiology, 104(6):3691–3704,\n2010.\n14\n[39] Somayyeh Soltanian-Zadeh, Kaan Sahingur, Sarah Blau, Yiyang Gong, and Sina Farsiu.\nFast and robust active neuron segmentation in two-photon calcium imaging using spa-\ntiotemporal deep learning.\nProceedings of the National Academy of Sciences, 116(17):\n8554–8563, 2019.\n[40] Artur Speiser, Jinyao Yan, Evan W Archer, Lars Buesing, Srinivas C Turaga, and Jakob H\nMacke. Fast amortized inference of neural activity from calcium imaging data with varia-\ntional autoencoders. In Advances in Neural Information Processing Systems, pages 4024–\n4034, 2017.\n[41] Kristofer E Bouchard and Edward F Chang.\nNeural decoding of spoken vowels from\nhuman sensory-motor cortex with high-density electrocorticography. In 2014 36th Annual\nInternational Conference of the IEEE Engineering in Medicine and Biology Society, pages\n6782–6785. IEEE, 2014.\n[42] Minda Yang, Sameer A Sheth, Catherine A Schevon, Guy M Mckhann Ii, and Nima\nMesgarani. Speech reconstruction from human auditory cortex with deep neural networks.\nIn Sixteenth Annual Conference of the International Speech Communication Association,\n2015.\n[43] Emily M Mugler, James L Patton, Robert D Flint, Zachary A Wright, Stephan U Schuele,\nJoshua Rosenow, Jerry J Shih, Dean J Krusienski, and Marc W Slutzky. Direct classiﬁca-\ntion of all american english phonemes using signals from functional speech motor cortex.\nJournal of neural engineering, 11(3):035015, 2014.\n[44] Nur Ahmadi, Timothy G Constandinou, and Christos-Savvas Bouganis. Decoding hand\nkinematics from local ﬁeld potentials using long short-term memory (lstm) network. In\n2019 9th International IEEE/EMBS Conference on Neural Engineering (NER), pages\n415–419. IEEE, 2019.\n[45] Hosein M Golshan, Adam O Hebb, and Mohammad H Mahoor. Lfp-net: A deep learning\nframework to recognize human behavioral activities using brain stn-lfp signals. Journal\nof Neuroscience Methods, 335:108621, 2020.\n[46] Jialin Wang, Yanchun Zhang, Qinying Ma, Huihui Huang, and Xiaoyuan Hong. Deep\nlearning for single-channel eeg signals sleep stage scoring based on frequency domain\nrepresentation. In International Conference on Health Information Science, pages 121–\n133. Springer, 2019.\n[47] Zeke Barger, Charles G Frye, Danqian Liu, Yang Dan, and Kristofer E Bouchard. Robust,\nautomated sleep scoring by a compact neural network with distributional shift correction.\nPloS one, 14(12), 2019.\n[48] Akara Supratak, Hao Dong, Chao Wu, and Yike Guo. Deepsleepnet: a model for automatic\nsleep stage scoring based on raw single-channel eeg. IEEE Transactions on Neural Systems\nand Rehabilitation Engineering, 25(11):1998–2008, 2017.\n[49] Nur Ahmadi, Timothy G Constandinou, and Christos-Savvas Bouganis.\nEnd-to-end\nhand kinematic decoding from lfps using temporal convolutional network. In 2019 IEEE\nBiomedical Circuits and Systems Conference (BioCAS), pages 1–4. IEEE, 2019.\n[50] Yitong Li, Kafui Dzirasa, Lawrence Carin, David E Carlson, et al.\nTargeting eeg/lfp\nsynchrony with neural nets. In Advances in Neural Information Processing Systems, pages\n4620–4630, 2017.\n15\n[51] Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer,\nMartin Glasstetter, Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wol-\nfram Burgard, and Tonio Ball. Deep learning with convolutional neural networks for eeg\ndecoding and visualization. Human brain mapping, 38(11):5391–5420, 2017.\n[52] Vernon J Lawhern, Amelia J Solon, Nicholas R Waytowich, Stephen M Gordon, Chou P\nHung, and Brent J Lance. Eegnet: a compact convolutional neural network for eeg-based\nbrain–computer interfaces. Journal of neural engineering, 15(5):056013, 2018.\n[53] Ziqian Xie, Odelia Schwartz, and Abhishek Prasad. Decoding of ﬁnger trajectory from\necog using deep learning. Journal of neural engineering, 15(3):036009, 2018.\n[54] Miguel Angrick, Christian Herﬀ, Emily Mugler, Matthew C Tate, Marc W Slutzky, Dean J\nKrusienski, and Tanja Schultz. Speech synthesis from ecog using densely connected 3d\nconvolutional neural networks. Journal of neural engineering, 16(3):036019, 2019.\n[55] Liang Zou, Jiannan Zheng, Chunyan Miao, Martin J Mckeown, and Z Jane Wang. 3d cnn\nbased automatic diagnosis of attention deﬁcit hyperactivity disorder using functional and\nstructural mri. IEEE Access, 5:23626–23636, 2017.\n[56] Apostolos P Georgopoulos, Roberto Caminiti, John F Kalaska, and Joseph T Massey.\nSpatial coding of movement: a hypothesis concerning the coding of movement direction\nby motor cortical populations. Experimental Brain Research, 49(Suppl. 7):327–336, 1983.\n[57] Wei Wu, Michael J Black, Yun Gao, M Serruya, A Shaikhouni, JP Donoghue, and Elie\nBienenstock. Neural decoding of cursor motion using a kalman ﬁlter. In Advances in\nneural information processing systems, pages 133–140, 2003.\n[58] Vikash Gilja, Paul Nuyujukian, Cindy A Chestek, John P Cunningham, M Yu Byron,\nJoline M Fan, Mark M Churchland, Matthew T Kaufman, Jonathan C Kao, Stephen I\nRyu, et al. A high-performance neural prosthesis enabled by control algorithm design.\nNature neuroscience, 15(12):1752, 2012.\n[59] Mijail D Serruya, Nicholas G Hatsopoulos, Liam Paninski, Matthew R Fellows, and John P\nDonoghue. Instant neural control of a movement signal. Nature, 416(6877):141–142, 2002.\n[60] Jose M Carmena, Mikhail A Lebedev, Roy E Crist, Joseph E O’Doherty, David M San-\ntucci, Dragan F Dimitrov, Parag G Patil, Craig S Henriquez, and Miguel AL Nicolelis.\nLearning to control a brain–machine interface for reaching and grasping by primates. PLoS\nbiology, 1(2), 2003.\n[61] Zheng Li, Joseph E O’Doherty, Timothy L Hanson, Mikhail A Lebedev, Craig S Henriquez,\nand Miguel AL Nicolelis. Unscented kalman ﬁlter for brain-machine interfaces. PloS one,\n4(7), 2009.\n[62] Trieu Phat Luu, Yongtian He, Samuel Brown, Sho Nakagome, and Jose L Contreras-\nVidal. Gait adaptation to visual kinematic perturbations using a real-time closed-loop\nbrain–computer interface to a virtual reality avatar. Journal of neural engineering, 13(3):\n036006, 2016.\n[63] Eric A Pohlmeyer, Sara A Solla, Eric J Perreault, and Lee E Miller. Prediction of upper\nlimb muscle activity from motor cortical discharge during reaching. Journal of neural\nengineering, 4(4):369, 2007.\n[64] Christian Ethier, Emily R Oby, Matthew J Bauman, and Lee E Miller. Restoration of\ngrasp following paralysis through brain-controlled stimulation of muscles. Nature, 485\n(7398):368–371, 2012.\n16\n[65] Po-He Tseng, N´uria Armengol Urpi, Mikhail Lebedev, and Miguel Nicolelis. Decoding\nmovements from cortical ensemble activity using a long short-term memory recurrent\nnetwork. Neural computation, 31(6):1085–1113, 2019.\n[66] David Sussillo, Sergey D Stavisky, Jonathan C Kao, Stephen I Ryu, and Krishna V Shenoy.\nMaking brain–machine interfaces robust to future neural variability. Nature communica-\ntions, 7:13749, 2016.\n[67] Stephanie Naufel, Joshua I Glaser, Konrad P Kording, Eric J Perreault, and Lee E Miller.\nA muscle-activity-dependent gain between motor cortex and emg. Journal of neurophys-\niology, 121(1):61–73, 2019.\n[68] Jisung Park and Sung-Phil Kim. Estimation of speed and direction of arm movements\nfrom m1 activity using a nonlinear neural decoder.\nIn 2019 7th International Winter\nConference on Brain-Computer Interface (BCI), pages 1–4. IEEE, 2019.\n[69] Yinong Wang, Wilson Truccolo, and David A Borton. Decoding hindlimb kinematics from\nprimate motor cortex using long short-term memory recurrent neural networks. In 2018\n40th Annual International Conference of the IEEE Engineering in Medicine and Biology\nSociety (EMBC), pages 1944–1947. IEEE, 2018.\n[70] David Sussillo, Paul Nuyujukian, Joline M Fan, Jonathan C Kao, Sergey D Stavisky,\nStephen Ryu, and Krishna Shenoy. A recurrent neural network for closed-loop intracortical\nbrain–machine interface decoders. Journal of neural engineering, 9(2):026027, 2012.\n[71] Michael A Schwemmer, Nicholas D Skomrock, Per B Sederberg, Jordyn E Ting, Gaurav\nSharma, Marcia A Bockbrader, and David A Friedenberg. Meeting brain–computer in-\nterface user performance expectations using a deep neural network decoding framework.\nNature medicine, 24(11):1669–1676, 2018.\n[72] Nicholas D Skomrock, Michael A Schwemmer, Jordyn E Ting, Hemang R Trivedi, Gaurav\nSharma, Marcia A Bockbrader, and David A Friedenberg. A characterization of brain-\ncomputer interface performance trade-oﬀs using support vector machines and deep neural\nnetworks to decode movement intent. Frontiers in neuroscience, 12:763, 2018.\n[73] Sho Nakagome, Trieu Phat Luu, Yongtian He, Akshay Sujatha Ravindran, and Jose L\nContreras-Vidal. An empirical comparison of neural networks and machine learning algo-\nrithms for eeg gait decoding. Scientiﬁc Reports, 10(1):1–17, 2020.\n[74] Ewan Nurse, Benjamin S Mashford, Antonio Jimeno Yepes, Isabell Kiral-Kornek, Stefan\nHarrer, and Dean R Freestone. Decoding eeg and lfp signals using deep learning: heading\ntruenorth. In Proceedings of the ACM International Conference on Computing Frontiers,\npages 259–266, 2016.\n[75] Anming Du, Shuqin Yang, Weijia Liu, and Haiping Huang. Decoding ecog signal with\ndeep learning model based on lstm. In TENCON 2018-2018 IEEE Region 10 Conference,\npages 0430–0435. IEEE, 2018.\n[76] Gang Pan, Jia-Jun Li, Yu Qi, Hang Yu, Jun-Ming Zhu, Xiao-Xiang Zheng, Yue-Ming\nWang, and Shao-Min Zhang. Rapid decoding of hand gestures in electrocorticography\nusing recurrent neural networks. Frontiers in neuroscience, 12:555, 2018.\n[77] Venkatesh Elango, Aashish N Patel, Kai J Miller, and Vikash Gilja. Sequence transfer\nlearning for neural decoding. bioRxiv, page 210732, 2017.\n[78] Krishna V Shenoy, Maneesh Sahani, and Mark M Churchland. Cortical control of arm\nmovements: a dynamical systems perspective. Annual review of neuroscience, 36:337–359,\n2013.\n17\n[79] Kristofer E Bouchard, Nima Mesgarani, Keith Johnson, and Edward F Chang. Functional\norganization of human sensorimotor cortex for speech articulation. Nature, 495(7441):327,\n2013.\n[80] Alexander M Chan, Eric Halgren, Ksenija Marinkovic, and Sydney S Cash. Decoding word\nand category-speciﬁc spatiotemporal representations from meg and eeg. Neuroimage, 54\n(4):3028–3039, 2011.\n[81] Christian Herﬀand Tanja Schultz. Automatic speech recognition from neural signals: a\nfocused review. Frontiers in neuroscience, 10:429, 2016.\n[82] Alborz Rezazadeh Sereshkeh, Robert Trott, Aur´elien Bricout, and Tom Chau. Eeg classi-\nﬁcation of covert speech using regularized neural networks. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 25(12):2292–2300, 2017.\n[83] Jun Wang, Myungjong Kim, Angel W Hernandez-Mulero, Daragh Heitzman, and Paul\nFerrari. Towards decoding speech production from single-trial magnetoencephalography\n(meg) signals. In 2017 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 3036–3040. IEEE, 2017.\n[84] Hassan Akbari, Bahar Khalighinejad, Jose L Herrero, Ashesh D Mehta, and Nima Mes-\ngarani. Towards reconstructing intelligible speech from the human auditory cortex. Sci-\nentiﬁc reports, 9(1):1–12, 2019.\n[85] David F Conant, Kristofer E Bouchard, Matthew K Leonard, and Edward F Chang.\nHuman sensorimotor cortex control of directly measured vocal tract movements during\nvowel production. Journal of Neuroscience, 38(12):2955–2966, 2018.\n[86] Spencer Kellis, Kai Miller, Kyle Thomson, Richard Brown, Paul House, and Bradley\nGreger. Decoding spoken words using local ﬁeld potentials recorded from the cortical\nsurface. Journal of neural engineering, 7(5):056007, 2010.\n[87] Christian Herﬀ, Dominic Heger, Adriana De Pesters, Dominic Telaar, Peter Brunner,\nGerwin Schalk, and Tanja Schultz. Brain-to-text: decoding spoken phrases from phone\nrepresentations in the brain. Frontiers in neuroscience, 9:217, 2015.\n[88] Frank H Guenther, Jonathan S Brumberg, E Joseph Wright, Alfonso Nieto-Castanon,\nJason A Tourville, Mikhail Panko, Robert Law, Steven A Siebert, Jess L Bartels, Dinal S\nAndreasen, et al. A wireless brain-machine interface for real-time speech synthesis. PloS\none, 4(12), 2009.\n[89] Jonathan R Wolpaw, Niels Birbaumer, Dennis J McFarland, Gert Pfurtscheller, and\nTheresa M Vaughan. Brain–computer interfaces for communication and control. Clinical\nneurophysiology, 113(6):767–791, 2002.\n[90] Tanja Schultz, Michael Wand, Thomas Hueber, Dean J Krusienski, Christian Herﬀ, and\nJonathan S Brumberg. Biosignal-based spoken communication: A survey. IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, 25(12):2257–2271, 2017.\n[91] Christopher Heelan, Jihun Lee, Ronan OShea, Laurie Lynch, David M Brandman, Wilson\nTruccolo, and Arto V Nurmikko. Decoding speech from spike-based neural population\nrecordings in secondary auditory cortex of non-human primates. Communications biology,\n2(1):1–12, 2019.\n[92] Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connec-\ntionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural\nnetworks. In Proceedings of the 23rd international conference on Machine learning, pages\n369–376, 2006.\n18\n[93] Pengfei Sun, Gopala K Anumanchipalli, and Edward F Chang.\nBrain2char: A deep\narchitecture for decoding text from brain recordings. arXiv preprint arXiv:1909.01401,\n2019.\n[94] Joseph G Makin, David A Moses, and Edward F Chang. Machine translation of cortical\nactivity to text with an encoder–decoder framework. Technical report, Nature Publishing\nGroup, 2020.\n[95] Gopala K Anumanchipalli, Josh Chartier, and Edward F Chang. Speech synthesis from\nneural decoding of spoken sentences. Nature, 568(7753):493, 2019.\n[96] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex\nGraves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A genera-\ntive model for raw audio. arXiv preprint arXiv:1609.03499, 2016.\n[97] David A Moses, Matthew K Leonard, Joseph G Makin, and Edward F Chang. Real-time\ndecoding of question-and-answer speech dialogue using human cortical activity. Nature\ncommunications, 10(1):1–14, 2019.\n[98] Yoichi Miyawaki, Hajime Uchida, Okito Yamashita, Masa-aki Sato, Yusuke Morito, Hi-\nroki C Tanabe, Norihiro Sadato, and Yukiyasu Kamitani. Visual image reconstruction\nfrom human brain activity using a combination of multiscale local image decoders. Neu-\nron, 60(5):915–929, 2008.\n[99] Shinji Nishimoto, An T Vu, Thomas Naselaris, Yuval Benjamini, Bin Yu, and Jack L\nGallant. Reconstructing visual experiences from brain activity evoked by natural movies.\nCurrent Biology, 21(19):1641–1646, 2011.\n[100] Kai Qiao, Jian Chen, Linyuan Wang, Chi Zhang, Lei Zeng, Li Tong, and Bin Yan. Cate-\ngory decoding of visual stimuli from human brain activity using a bidirectional recurrent\nneural network to simulate bidirectional information ﬂows in human visual cortices. Fron-\ntiers in neuroscience, 13, 2019.\n[101] Randall Jordan Ellis and Michael Michaelides. High-accuracy decoding of complex visual\nscenes from neuronal calcium responses. BioRxiv, page 271296, 2018.\n[102] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009.\n[103] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.\nImage style transfer using\nconvolutional neural networks. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2414–2423, 2016.\n[104] Katja Seeliger, Umut G¨u¸cl¨u, Luca Ambrogioni, Yagmur G¨u¸cl¨ut¨urk, and Marcel AJ van\nGerven. Generative adversarial networks for reconstructing natural images from brain\nactivity. NeuroImage, 181:775–785, 2018.\n[105] Ya˘gmur G¨u¸cl¨ut¨urk, Umut G¨u¸cl¨u, Katja Seeliger, Sander Bosch, Rob van Lier, and Mar-\ncel AJ van Gerven. Reconstructing perceived faces from brain activations with deep ad-\nversarial neural decoding. In Advances in Neural Information Processing Systems, pages\n4246–4257, 2017.\n[106] Ghislain St-Yves and Thomas Naselaris.\nGenerative adversarial networks conditioned\non brain activity reconstruct seen images. In 2018 IEEE International Conference on\nSystems, Man, and Cybernetics (SMC), pages 1054–1061. IEEE, 2018.\n19\n[107] Haiguang Wen, Junxing Shi, Yizhen Zhang, Kun-Han Lu, Jiayue Cao, and Zhongming\nLiu. Neural encoding and decoding with deep learning for dynamic natural vision. Cerebral\nCortex, 28(12):4136–4160, 2018.\n[108] Guohua Shen, Tomoyasu Horikawa, Kei Majima, and Yukiyasu Kamitani. Deep image\nreconstruction from human brain activity. PLoS computational biology, 15(1):e1006633,\n2019.\n[109] Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, and Yukiyasu Kami-\ntani.\nEnd-to-end deep image reconstruction from human brain activity.\nFrontiers in\nComputational Neuroscience, 13, 2019.\n[110] Ardi Tampuu, Tambet Matiisen, H Freyja ´Olafsd´ottir, Caswell Barry, and Raul Vicente.\nEﬃcient neural decoding of self-location with a deep recurrent network. PLoS computa-\ntional biology, 15(2):e1006822, 2019.\n[111] Mohammad R Rezaei, Anna K Gillespie, Jennifer A Guidera, Behzad Nazari, Saeid Sadri,\nLoren M Frank, Uri T Eden, and Ali Youseﬁ. A comparison study of point-process ﬁlter\nand deep learning performance in estimating rat position using an ensemble of place cells.\nIn 2018 40th Annual International Conference of the IEEE Engineering in Medicine and\nBiology Society (EMBC), pages 4732–4735. IEEE, 2018.\n[112] Zishen Xu, Wei Wu, Shawn S Winter, Max L Mehlman, William N Butler, Christine M\nSimmons, Ryan E Harvey, Laura E Berkowitz, Yang Chen, Jeﬀrey S Taube, et al. A\ncomparison of neural decoding methods and population coding across thalamo-cortical\nhead direction cells. Frontiers in Neural Circuits, 13, 2019.\n[113] Hongming Li and Yong Fan.\nInterpretable, highly accurate brain decoding of subtly\ndistinct brain states from functional mri using intrinsic functional networks and long\nshort-term memory recurrent neural networks. NeuroImage, 202:116059, 2019.\n[114] So-Hyeon Yoo, Seong-Woo Woo, and Zafar Amad. Classiﬁcation of three categories from\nprefrontal cortex using lstm networks: fnirs study. In 2018 18th International Conference\non Control, Automation and Systems (ICCAS), pages 1141–1146. IEEE, 2018.\n[115] Eleanor Batty, Matthew Whiteway, Shreya Saxena, Dan Biderman, Taiga Abe, Simon\nMusall, Winthrop Gillis, Jeﬀrey Markowitz, Anne Churchland, John P Cunningham, et al.\nBehavenet: nonlinear embedding and bayesian neural decoding of behavioral videos. In\nAdvances in Neural Information Processing Systems, pages 15680–15691, 2019.\n[116] Simon M Hofmann, Felix Klotzsche, Alberto Mariola, Vadim V Nikulin, Arno Villringer,\nand Michael Gaebler.\nDecoding subjective emotional arousal during a naturalistic vr\nexperience from eeg using lstms. In 2018 IEEE International Conference on Artiﬁcial\nIntelligence and Virtual Reality (AIVR), pages 128–131. IEEE, 2018.\n[117] Anumit Garg, Ashna Kapoor, Anterpreet Kaur Bedi, and Ramesh K Sunkaria. Merged\nlstm model for emotion classiﬁcation using eeg signals. In 2019 International Conference\non Data Science and Engineering (ICDSE), pages 139–143. IEEE, 2019.\n[118] Samarth Tripathi, Shrinivas Acharya, Ranti Dev Sharma, Sudhanshu Mittal, and Samit\nBhattacharya. Using deep and convolutional neural networks for accurate emotion classi-\nﬁcation on deap dataset. In Twenty-Ninth IAAI Conference, 2017.\n[119] Gregory Ciccarelli, Michael Nolan, Joseph Perricone, Paul T Calamia, Stephanie Haro,\nJames OSullivan, Nima Mesgarani, Thomas F Quatieri, and Christopher J Smalt. Com-\nparison of two-talker attention decoding from eeg with nonlinear neural networks and\nlinear methods. Scientiﬁc reports, 9(1):1–10, 2019.\n20\n[120] Tobias de Taillez, Birger Kollmeier, and Bernd T Meyer. Machine learning for decoding\nlisteners attention from electroencephalography evoked by continuous speech. European\nJournal of Neuroscience, 2017.\n[121] Elaine Astrand, Pierre Enel, Guilhem Ibos, Peter Ford Dominey, Pierre Baraduc, and Su-\nliann Ben Hamed. Comparison of classiﬁers for decoding sensory and cognitive information\nfrom prefrontal neuronal populations. PloS one, 9(1), 2014.\n[122] Marc-Andre Schulz, Thomas Yeo, Joshua Vogelstein, Janaina Mourao-Miranada, Jakob\nKather, Konrad Kording, Blake A Richards, and Danilo Bzdok. Deep learning for brains?:\nDiﬀerent linear and nonlinear scaling in uk biobank brain images vs. machine-learning\ndatasets. bioRxiv, page 757054, 2019.\n[123] Rajat Mani Thomas, Selene Gallo, Leonardo Cerliani, Paul Zhutovsky, Ahmed El-Gazzar,\nand Guido van Wingen. Classifying autism spectrum disorder using the temporal statistics\nof resting-state functional mri data with 3d convolutional neural networks. Frontiers in\nPsychiatry, 11:440, 2020.\n[124] Johannes Hennrich, Christian Herﬀ, Dominic Heger, and Tanja Schultz. Investigating deep\nlearning for fnirs based bci. In 2015 37th Annual international conference of the IEEE\nEngineering in Medicine and Biology Society (EMBC), pages 2844–2847. IEEE, 2015.\n[125] Ashesh K Dhawale, Rajesh Poddar, Steﬀen BE Wolﬀ, Valentin A Normand, Evi\nKopelowitz, and Bence P ¨Olveczky. Automated long-term recording and analysis of neural\nactivity in behaving animals. Elife, 6:e27702, 2017.\n[126] Allen Brain Observatory. Available at: http://observatory.brain-map.org/visualcoding,\n2016.\n[127] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudi-\nnov, Rich Zemel, and Yoshua Bengio.\nShow, attend and tell: Neural image caption\ngeneration with visual attention. In International conference on machine learning, pages\n2048–2057, 2015.\n[128] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep net-\nworks. In Proceedings of the 34th International Conference on Machine Learning-Volume\n70, pages 3319–3328. JMLR. org, 2017.\n[129] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been\nKim. Sanity checks for saliency maps. In Advances in Neural Information Processing\nSystems, pages 9505–9515, 2018.\n[130] Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine\nYe, and Alexander Mordvintsev. The building blocks of interpretability. Distill, 3(3):e10,\n2018.\n[131] The OpenAI microscope. https://microscope.openai.com/models, 2020. Accessed: 2020-\n05-12.\n[132] Nikolaus Kriegeskorte and Pamela K Douglas. Interpreting encoding and decoding models.\nCurrent opinion in neurobiology, 55:167–179, 2019.\n[133] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:\nAn imperative style, high-performance deep learning library. In Advances in Neural In-\nformation Processing Systems, pages 8024–8035, 2019.\n21\n[134] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey Dean,\nMatthieu Devin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard, et al. Tensorﬂow:\nA system for large-scale machine learning. In 12th {USENIX} Symposium on Operating\nSystems Design and Implementation ({OSDI} 16), pages 265–283, 2016.\n[135] Tim Christian Kietzmann, Patrick McClure, and Nikolaus Kriegeskorte.\nDeep neural\nnetworks in computational neuroscience. BioRxiv, page 133504, 2018.\n[136] Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz,\nAmelia Christensen, Claudia Clopath, Rui Ponte Costa, Archy de Berker, Surya Ganguli,\net al. A deep learning framework for neuroscience. Nature neuroscience, 22(11):1761–1770,\n2019.\n[137] John J Hopﬁeld. Neural networks and physical systems with emergent collective com-\nputational abilities. Proceedings of the national academy of sciences, 79(8):2554–2558,\n1982.\n[138] David Zipser and Richard A Andersen. A back-propagation programmed network that\nsimulates response properties of a subset of posterior parietal neurons. Nature, 331(6158):\n679–684, 1988.\n[139] David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. A\nneural network that ﬁnds a naturalistic solution for the production of muscle activity.\nNature neuroscience, 18(7):1025–1033, 2015.\n[140] Daniel LK Yamins and James J DiCarlo.\nUsing goal-driven deep learning models to\nunderstand sensory cortex. Nature neuroscience, 19(3):356, 2016.\n22\n",
  "categories": [
    "q-bio.NC",
    "cs.LG"
  ],
  "published": "2020-05-19",
  "updated": "2020-05-19"
}