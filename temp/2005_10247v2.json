{
  "id": "http://arxiv.org/abs/2005.10247v2",
  "title": "Model-Based Robust Deep Learning: Generalizing to Natural, Out-of-Distribution Data",
  "authors": [
    "Alexander Robey",
    "Hamed Hassani",
    "George J. Pappas"
  ],
  "abstract": "While deep learning has resulted in major breakthroughs in many application\ndomains, the frameworks commonly used in deep learning remain fragile to\nartificially-crafted and imperceptible changes in the data. In response to this\nfragility, adversarial training has emerged as a principled approach for\nenhancing the robustness of deep learning with respect to norm-bounded\nperturbations. However, there are other sources of fragility for deep learning\nthat are arguably more common and less thoroughly studied. Indeed, natural\nvariation such as lighting or weather conditions can significantly degrade the\naccuracy of trained neural networks, proving that such natural variation\npresents a significant challenge for deep learning.\n  In this paper, we propose a paradigm shift from perturbation-based\nadversarial robustness toward model-based robust deep learning. Our objective\nis to provide general training algorithms that can be used to train deep neural\nnetworks to be robust against natural variation in data. Critical to our\nparadigm is first obtaining a model of natural variation which can be used to\nvary data over a range of natural conditions. Such models may be either known a\npriori or else learned from data. In the latter case, we show that deep\ngenerative models can be used to learn models of natural variation that are\nconsistent with realistic conditions. We then exploit such models in three\nnovel model-based robust training algorithms in order to enhance the robustness\nof deep learning with respect to the given model. Our extensive experiments\nshow that across a variety of naturally-occurring conditions and across various\ndatasets, deep neural networks trained with our model-based algorithms\nsignificantly outperform both standard deep learning algorithms as well as\nnorm-bounded robust deep learning algorithms.",
  "text": "Model-Based Robust Deep Learning: Generalizing to\nNatural, Out-of-Distribution Data\nAlexander Robey, Hamed Hassani, and George J. Pappas\nDepartment of Electrical and Systems Engineering\nUniversity of Pennsylvania\nAbstract\nWhile deep learning has resulted in major breakthroughs in many application domains, the\nframeworks commonly used in deep learning remain fragile to artiﬁcially-crafted, imperceptible\nchanges in the data. In response to this fragility, adversarial training has emerged as a principled\napproach for enhancing the robustness of deep learning with respect to norm-bounded pertur-\nbations. However, there are other sources of fragility for deep learning that are arguably more\ncommon and less thoroughly studied. Indeed, natural variation such as changes in lighting or\nweather conditions can signiﬁcantly degrade the accuracy of trained neural networks, proving\nthat such natural variation presents a signiﬁcant challenge for deep learning.\nIn this paper, we propose a paradigm shift from perturbation-based adversarial robustness\nto model-based robust deep learning. Our objective is to provide general training algorithms that\ncan be used to train deep neural networks to be robust against natural variation in data. Critical\nto our paradigm is ﬁrst obtaining a model of natural variation which can be used to vary data\nover a range of natural conditions. Such models of natural variation may be either known a\npriori or else learned from data. In the latter case, we show that deep generative models can be\nused to learn models of natural variation that are consistent with realistic conditions. We then\nexploit such models in three novel model-based robust training algorithms in order to enhance\nthe robustness of deep learning with respect to the given model.\nOur extensive experiments show that across a variety of naturally-occurring conditions\nin twelve distinct datasets including MNIST, SVHN, GTSRB, and CURE-TSR, ImageNet, and\nImageNet-c, deep neural networks trained with our model-based algorithms signiﬁcantly\noutperform classiﬁers trained via empirical risk minimization, perturbation-based adversarial\ntraining, data augmentation methods, and domain adaptation techniques. Speciﬁcally, when\ntraining on ImageNet and testing on various subsets of ImageNet-c, our algorithms improve\nover baseline methods by up to 30 percentage points in top-1 accuracy. Furthermore, we\ndemonstrate the broad applicability of our paradigm in four challenging robustness applications.\n(1) Firstly, we show that our algorithms can signiﬁcantly improve robustness against natural,\nout-of-distribution data, resulting in accuracy improvements as large as 20-30 percentage points\ncompared to state-of-the-art classiﬁers. (2) Secondly, we show that models of natural variation\ncan be effortlessly composed to provide robustness against multiple simultaneous distributional\nshifts. To evaluate this property, we curate several new datasets containing multiple sources\nof natural variation. (3) Thirdly, we show that models of natural variation trained on one\ndataset can be applied to new datasets to provide signiﬁcant levels of robustness against unseen\ndistributional shifts. (4) Finally, we show that in the setting of unsupervised domain adaptation,\nour algorithms outperform traditional domain adaptation techniques.\nOur results suggest that exploiting models of natural variation can result in signiﬁcant\nimprovements in the robustness of deep learning when deployed in natural environments.\nThis paves the way for a plethora of interesting future research directions, both algorithmic\nand theoretical, as well as numerous applications in which enhancing the robustness of deep\nlearning will enable it’s wider adoption.\nCode is available at the following link: https://github.com/arobey1/mbrdl.\n1\narXiv:2005.10247v2  [cs.LG]  2 Nov 2020\nContents\n1\nIntroduction\n5\n2\nPerturbation-based robustness in deep learning\n7\n3\nModel-based robust deep learning\n9\n3.1\nAdversarial examples versus natural variation . . . . . . . . . . . . . . . . . . . . . .\n10\n3.2\nFormulating the model-based robust optimization problem . . . . . . . . . . . . . . .\n11\n3.3\nGeometry of model-based robust training . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n4\nModels of natural variation\n14\n4.1\nKnown models G(x, δ) of natural variation . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4.2\nLearning unknown models of natural variation G(x, δ) from data . . . . . . . . . . .\n16\n4.3\nUsing deep generative models to learn models of natural variation\n. . . . . . . . . .\n17\n4.4\nA gallery of learned models of natural variation\n. . . . . . . . . . . . . . . . . . . . .\n19\n5\nModel-based robust training algorithms\n19\n5.1\nModel-based Robust Training (MRT) . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n5.2\nModel-based Adversarial Training (MAT) . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n5.3\nModel-based Data Augmentation (MDA) . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n6\nExperiments\n24\n6.1\nOut-of-distribution robustness\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n6.2\nModel-based robustness on the shift from ImageNet to ImageNet-c . . . . . . . . . .\n27\n6.3\nRobustness to simultaneous distributional shifts . . . . . . . . . . . . . . . . . . . . .\n28\n6.4\nTransferability of model-based robustness . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n6.5\nModel-based robust deep learning for unsupervised domain adaptation . . . . . . .\n30\n7\nDiscussion\n31\n7.1\nImpact of model quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n7.2\nAlgorithm and hyperparameter selection criteria . . . . . . . . . . . . . . . . . . . . .\n32\n8\nRelated works\n34\n8.1\nPerturbation-based adversarial robustness . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n8.2\nGenerative models in the context of robustness . . . . . . . . . . . . . . . . . . . . . .\n35\n8.3\nA broader view of robustness in deep learning . . . . . . . . . . . . . . . . . . . . . .\n35\n8.4\nDomain adaptation and domain generalization . . . . . . . . . . . . . . . . . . . . . .\n36\n9\nConclusion and future directions\n37\nReferences\n39\nA Training details\n49\nA.1 Baseline and model-based implementation details . . . . . . . . . . . . . . . . . . . .\n49\nA.2 Classiﬁer training details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nA.3 MUNIT framework overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n2\nA.4 Hyperparameters and implementation of MUNIT . . . . . . . . . . . . . . . . . . . .\n51\nB\nA gallery of learned models of natural variation\n52\nC Details concerning datasets and domains\n59\nC.1\nNatural vs. synthetic variation in data . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\nC.1.1\nNaturally-occurring variation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\nC.1.2\nArtiﬁcially-generated variation . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\nC.2\nDatasets introduced in this paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\nList of Figures\n1\nA new notion of robustness. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2\nModels of natural variation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3\nGeometry of perturbation-based and model-based robustness . . . . . . . . . . . . .\n13\n4\nKnown models of natural variation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n5\nLearned models of natural variation . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n6\nLearning models of natural variation via disentangled representations . . . . . . . .\n18\n7\nCURE-TSR snow challenge levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n8\nA better models implies more robustness\n. . . . . . . . . . . . . . . . . . . . . . . . .\n32\n9\nSVHN contrast (low→high) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n10\nSVHN brightness (low→high) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n11\nCURE-TSR snow (no→yes)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\n12\nCURE-TSR haze (no→yes) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n13\nCURE-TSR decolorization (no→yes) . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n14\nCURE-TSR rain (no→yes) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n15\nImageNet brightness (low→high) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n16\nImageNet contrast (high→low) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n17\nImageNet snow (no→yes) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n18\nImageNet fog (no→yes)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n19\nImageNet frost (no→yes)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n20\nGTSRB brightness (low→high) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n21\nGTSRB contrast (low→high) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n58\n22\nSVHN brightness thresholding overview\n. . . . . . . . . . . . . . . . . . . . . . . . .\n61\n23\nSVHN contrast thresholding overview . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n24\nGTSRB brightness thresholding overview . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n25\nSVHN contrast thresholding overview . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n26\nImageNet dataset with brightness and snow\n. . . . . . . . . . . . . . . . . . . . . . .\n64\n27\nImageNet dataset with brightness and contrast . . . . . . . . . . . . . . . . . . . . . .\n65\n28\nImageNet dataset with brightness and fog . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n29\nImageNet dataset with contrast and fog . . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n3\nList of Tables\n1\nA gallery of learned models of natural variation\n. . . . . . . . . . . . . . . . . . . . .\n20\n2\nOut-of-distribution robustness\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3\nImageNet to ImageNet-c robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n4\nComposing models of natural variation . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5\nTransferability of model-based robustness . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n6\nModel-based training in the setting of unsupervised domain adaptation . . . . . . .\n30\n7\nThe impact of varying k in the model-based algorithms . . . . . . . . . . . . . . . . .\n33\n8\nMUNIT hyperparameters\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n9\nPassing images from other datasets through a model learned on MNIST . . . . . . .\n59\n10\nThresholds for SVHN and GTSRB\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\nList of Algorithms\n1\nKnown model for background color . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n2\nModel-based Robust Training (MRT) . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3\nModel-based Adversarial Training (MAT) . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4\nModel-Based Data Augmentation (MDA) . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4\n1\nIntroduction\nOver the last decade, we have witnessed unprecedented breakthroughs in deep learning [1].\nRapidly growing bodies of work continue to improve the state-of-the-art in generative modeling\n[2, 3, 4], computer vision [5, 6, 7], and natural language processing [8, 9]. Indeed, the signiﬁcant\nprogress made in these ﬁelds has prompted large-scale integration of deep learning techniques\ninto a myriad of application domains, including autonomous vehicles, medical diagnostics, and\nrobotics [10, 11]. Importantly, many of these domains are safety-critical, meaning that the detections,\nrecommendations, or decisions made by deep learning systems can directly impact the well-being\nof humans [12]. For this reason, it is essential that the deep learning systems used in safety-critical\napplications are robust and trustworthy [13].\nDespite the remarkable progress made toward improving the state-of-the-art in deep learning,\nit is well-known that many deep learning frameworks including neural networks are fragile\nto seemingly innocuous and imperceptible changes to their input data [14]. Well-documented\nexamples of fragility to carefully-designed noise can be found in the context of image detection\n[15], video analysis [16, 17], trafﬁc sign misclassiﬁcation [18], machine translation [19], clinical trials\n[20], and robotics [21]. In response to this vulnerability, a growing body of work has focused on\nimproving the robustness of deep learning. More speciﬁcally, the literature concerning adversarial\nrobustness has sought to improve robustness against small, imperceptible perturbations of data,\nwhich have been shown to cause misclassiﬁcation [14]. Over the last ﬁve years, this literature has\nincluded the development of robust training algorithms [22, 23, 24, 25, 26, 27, 28] and certiﬁable\ndefenses [29, 30, 31]. In particular, these robust training approaches, i.e. the method of adversarial\ntraining [32], typically perturb input data via adversarially-chosen, norm-bounded noise in a robust\noptimization formulation [22, 23], and have been shown to be effective at improving the robustness\nof deep learning against norm-bounded perturbations [33].\nWhile the adversarial training paradigm has provided a rigorous framework for analyzing and\nimproving the robustness of deep learning, the algorithms used in this paradigm have notable\nlimitations. Speciﬁcally, most adversarial training algorithms are only applicable for robustness\napplications in which data is perturbed by norm-bounded, artiﬁcially-generated, imperceptible\nnoise. Thus while adversarial training algorithms can resolve security threats arising from artiﬁcial\ntampering of the data, these schemes cannot provide similar levels of robustness to changes that\nmay arise due to other kinds of perturbations or variation [15]. And to this end, numerous recent\npapers have unanimously shown that deep learning is extremely fragile to unbounded shifts in the\ndata-distribution which commonly occur due to a wide range of natural phenomena [34, 35, 36, 15]\nand which cannot be modeled by additive, norm-bounded perturbations. Such phenomena include\nunseen distributional shifts such as changes in image lighting, variable weather conditions, or\nblurring [37, 38]. And while such unseen distributional shifts are arguably more common in safety-\ncritical domains than norm-bounded perturbations, there are remarkably few general, principled\ntechniques that provide robustness against these forms of out-of-distribution, naturally-occurring\nvariation [39]. Thus, it is of critical importance for the deep learning community to design novel\nalgorithms that are robust against natural, out-of-distribution shifts in data.\nIn this paper, we formulate the ﬁrst general-purpose algorithms that (1) use unlabeled data\nto learn models that describe arbitrary forms of natural variation and (2) exploit these models to\nprovide signiﬁcant robustness against natural, out-of-distribution shifts in data. To this end, we\npropose a paradigm shift from perturbation-based adversarial robustness to model-based robust\n5\ndeep learning. In this paradigm, following the observation that data can vary in highly nonlinear\nand unbounded ways in real-world, safety-critical environments, we ﬁrst obtain models of natural\nvariation which describe how data varies in natural environments. Such models of natural variation\nmay be known a priori, as is the case for geometric transformations such as rotation or scaling.\nAlternatively, in some settings a model of natural variation may not be known beforehand and\ntherefore must be learned from data; for example, there are no analytic models that describe how\nto change the weather conditions in images. Once such models of natural variation have been\nobtained, in this paradigm, we formulate a novel robust optimization problem that exploits models\nof natural variation to produce neural networks that are robust to the source of natural variation\ncaptured by the model. In this way, the goal of the model-based robust paradigm is to develop\ngeneral-purpose algorithms that can be used to train neural networks to be robust against natural,\nout-of-distribution shifts in data.\nOur experiments show that across a variety of naturally-occurring and challenging conditions,\nsuch as changes in lighting, background color, haze, decolorization, snow, rain, frost, fog, and\ncontrast, in twelve distinct datasets including MNIST, SVHN, GTSRB, CURE-TSR, ImageNet, and\nImageNet-c, neural networks trained with our model-based algorithms signiﬁcantly outperform\nclassiﬁers trained via empirical risk minimization, norm-bounded robust deep learning algorithms,\ndata augmentation methods, and, when applicable, domain adaptation techniques. In particular,\nwe show that classiﬁers trained on ImageNet using our model-based algorithms and tested on\nvarious subsets of ImageNet-c improve over state-of-the-art classiﬁers by up to 30 percentage\npoints. Furthermore, we show that the model-based robust deep learning paradigm is model-\nagnostic and adaptable, meaning that it can be used to provide robustness against arbitrary forms\nof natural variation in data and regardless of whether models of natural variation are known a\npriori or learned from data. To demonstrate the broad applicability of our approach, we present\napply our paradigm to four novel settings. (1) First, we show that our algorithms are the ﬁrst to\nprovide out-of-distribution robustness on a range of challenging settings. (2) Next, we show that\nmodels of natural variation can be composed to provide robustness against multiple simultaneous\ndistribution shifts. To evaluate this property, we curate several new datasets, each of which has two\nsimultaneous natural shifts. (3) Thirdly, we show that models of natural variation trained on one\ndataset can be used to provide robustness on datasets that are entirely unseen while training the\nmodel. (4) Lastly, we show that in the setting of unsupervised domain adaptation, our algorithms\noutperform traditional domain adaptation techniques by signiﬁcant margins.\nWhile the experiments in this paper focus on image classiﬁcation tasks subject to challenging\nnatural conditions, our model-based robust deep learning paradigm is much broader and can, in\nprinciple, be applied to many other application domains as long as one can obtain accurate models\ndescribing how data naturally varies. In that sense, we believe that this approach will open up\nnumerous directions for future research.\nContributions. The contributions of our paper can be summarized as follows:\n• Paradigm shift. We propose a paradigm shift from perturbation-based adversarial robustness\nto model-based robust deep learning, in which models of natural variation express changes\ndue to natural conditions that frequently appear in data.\n• Learning models of natural variation. For many forms of natural variation that are com-\nmonly encountered in safety-critical applications, we use deep generative models to learn\nmodels of natural variation from unlabelled data that are consistent with realistic conditions.\n6\n• Robust-optimization-based formulation. We formulate a novel training procedure by con-\nstructing a general robust optimization problem that searches for challenging out-of-distribution\nshifts in data to train classiﬁers to be robust against natural variation.\n• Novel model-based training algorithms. We propose a family of novel training algorithms\nthat exploit models of natural variation to improve the robustness of deep learning against\nchallenging natural conditions captured my models of natural variation.\n• ImageNet-c robustness. We show that our algorithms improve the robustness of classiﬁers\ntrained on ImageNet and tested on ImageNet-c by as much as 30 percentage points on a\nvariety of challenging settings, including changes in snow, contrast, brightness, and frost.\n• Out-of-distribution robustness. We show that our algorithms are the ﬁrst to consistently\nprovide robustness against natural, out-of-distribution shifts in data, including changes in\nsnow, rain, fog, and brightness on SVHN, GTSRB, CURE-TSR, and ImageNet, that frequently\noccur in real-world environments.\n• Robustness to simultaneous distributional shifts. We show that our framework is compos-\nable and thus can be used to improve robustness against multiple simultaneous distributional\nshifts in data. To evaluate this feature, we curate four new datasets, each of which has two\nsimultaneous distributional shifts.\n• Robustness to unseen domains. We show that models of natural variation can be reused on\ndatasets that are entirely unseen during training to improve out-of-distribution generalization.\nThis property demonstrates that model-based robustness is transferrable to unseen domains.\n• Robustness in the setting of unsupervised domain adaptation. We show that in the setting\nof unsupervised domain adaptation, our algorithms provide higher levels of robustness than\ntraditional domain adaptation techniques.\n2\nPerturbation-based robustness in deep learning\nIn this paper, we consider a standard classiﬁcation task in which the data is distributed according to\na joint distribution (x, y) ∼D over instances x ∈Rd and corresponding labels y ∈[k] := {1, . . . , k}.\nWe assume that we are given a suitable loss function ℓ(x, y; w); common examples include the\ncross-entropy or quadratic losses. In this notation, we let w ∈Rp denote the weights of a neural\nnetwork. The goal of the learning task is to ﬁnd the weights w that minimize the risk over D with\nrespect to the loss function ℓ. That is, we wish to solve\nw⋆∈arg min\nw∈Rp\nE(x,y)∼D [ℓ(x, y; w)] .\n(2.1)\nIn a litany of past works, it has been shown empirically that ﬁrst-order methods (e.g. SGD or Adam\n[40]) can be used to approximately solve (2.1) to obtain weights w⋆that engender neural networks\nwhich achieve high classiﬁcation accuracy on a variety of image classiﬁcation tasks [1, 41]\nAs observed in previous works [22, 23], solving the optimization problem stated in (2.1) does\nnot result in robust neural networks. More speciﬁcally, neural networks trained by solving (2.1) are\nknown to be susceptible to adversarial attacks. This means that given a datum x with a corresponding\n7\nlabel y, one can ﬁnd another datum xadv such that (1) x is close to xadv with respect to a given\nEuclidean norm and (2) xadv is predicted by the learned classiﬁer as belonging to a different class\nc ∈[k] where c ̸= y. If such a datum xadv exists, it is called an adversarial example.\nTo address this striking vulnerability, researchers have sought to improve the robustness of\ndeep learning by developing adversarial training algorithms, which inure neural networks against\nsmall, norm-bounded perturbations [32]. The dominant paradigm toward training neural networks\nto be robust against adversarial examples relies on a robust optimization perspective [42]. Indeed,\nthe approach used in [22, 23] to provide robustness to adversarial examples is formalized by\nconsidering a distinct yet related optimization problem to (2.1). In particular, the idea is to train\nneural networks to be robust against a worst-case perturbation of each instance x. This worst-case\nperspective can be formulated in the following way:\nw⋆∈arg min\nw∈Rp\nE(x,y)∼D\n\u0014\nmax\nδ∈∆ℓ(x + δ, y; w)\n\u0015\n.\n(2.2)\nHere the set of allowable perturbation ∆= ∆(ϵ) := {δ ∈Rd : ||δ||p ≤ϵ} is typically a norm-ball\nwith respect to a suitably chosen Euclidean p-norm ||·||p.\nWe can think of the optimization problem in (2.2) as comprising two coupled optimization\nproblems: an inner maximization problem and an outer minimization problem:\nδ⋆∈arg max\nδ∈∆\nℓ(x + δ, y; w)\n(2.3)\nw⋆∈arg min\nw∈Rp\nE(x,y)∼D [ℓ(x + δ⋆, y; w)]\n(2.4)\nFirst, in the inner maximization problem of (2.3), we seek a perturbation δ ∈∆that results in large\nloss values when we perturb x by the amount δ. When ∆is a norm-ball, any solution δ∗to the\ninner maximization problem of (2.3) is a worst-case, norm-bounded perturbation in so much as the\ndatum x + δ∗is most likely to be classiﬁed as any label c ∈[k] other than the true label y. If indeed\nthe trained classiﬁer predicts any class c other than y for the datum xadv := x + δ∗, then xadv is a\nbona ﬁde adversarial example. After solving this inner maximization problem, we can rewrite the\nouter minimization problem via the expression shown in (2.4). From this point of view, the goal\nof the outer problem is to ﬁnd the weights w ∈Rp that ensure that the worst-case datum x + δ∗\nis classiﬁed by our model as having label y. To connect robust training to the standard training\nparadigm for neural networks given in (2.1), note that if δ∗= 0 or if ∆= {0} is trivial, then the\nouter minimization problem (2.4) reduces to the empirical risk minimization problem of (2.1).\nLimitations of perturbation-based robustness. While there has been signiﬁcant progress toward\ndeveloping algorithms that train neural networks to be robust against norm-bounded perturbations\n[22, 23, 24, 25, 26, 27, 28], there are signiﬁcant limitations to adversarial training. Notably, it has\nbeen unanimously shown in a spate of recent work that deep learning is also fragile to various\nforms of natural variation [34, 35, 36, 15]. In the context of image classiﬁcation, such natural variation\nincludes changes in lighting, weather, or background color [18, 43, 44], spatial transformations such\nas rotation or scaling [45, 46], and sensor-based attacks [27]. These realistic forms of variation in\ndata, which are known as nuisances in the computer vision community, cannot be modeled by the\nnorm-bounded perturbations x 7→x + δ used in the standard adversarial training paradigm of\n(2.2) [47]. And while these natural distributional shifts are ubiquitous in numerous application\ndomains, there are remarkably few general, principled techniques that provide robustness against\nthese forms of out-of-distribution, naturally-occurring variation [39]. Therefore, an important open\n8\nproblem in the deep learning community is to develop algorithms that can train neural networks to\nbe robust against natural and realistic forms of out-of-distribution data that are often inherent in\nsafety-critical applications.\nChallenges in designing a more general robustness paradigm. Given the efﬁcacy of works that\nseek to improve the robustness of deep learning against adversarial perturbations, it is of fun-\ndamental interest to determine whether the adversarial robustness literature can be leveraged\ntoward developing more general notions of robustness. To this end, in this paper we identify two\nfundamental challenges toward achieving this objective.\nFirstly, unlike in the setting of perturbation-based robustness, in real-world environments, data\ncan vary in unknown and highly nonlinear ways. Thus, the ﬁrst step toward building a more\ngeneral robust training procedure must be to design mechanisms that accurately describe how data\nvaries in such environments. Indeed, in many scenarios, known geometric or physical structure\ncan be used to describe how data naturally varies, as is the case for spatial transformations such\nas rotations or scalings. On the other hand, many transformations, such as changes in weather\nconditions in images, cannot be described by analytical mathematical expressions. For this reason,\nit is essential that a more general robustness paradigm be able to leverage known structure and to\nlearn this structure from data when no analytic expression describing how data varies is available.\nThe second challenge underlying the task of developing a more general robustness paradigm is\nto formulate a principled training procedure that leverages suitable models that describe how data\nvaries toward generalizing to out-of-distribution shifts in the data distribution. Indeed, assuming\none has access a suitable model of natural variation, such a procedure should be agnostic to the\nspeciﬁc parameterization of the model and adaptable to both models that are known a priori as well\nas models that are learned from data.\nA unifying solution: Model-Based Robust Deep Learning. In this paper, we present a new\ntraining paradigm for deep learning that improves robustness against natural, out-of-distribution\nshifts in data by addressing both of these unique challenges. Rather than perturbing data in a norm-\nbounded manner, our robust training approach exploits models of natural variation that describe\nhow data changes with respect to particular shifts in the data distribution. However, we emphasize\nthat our approach is model-agnostic in the sense that it provides a paradigm that is applicable across\narbitrary classes of naturally-occurring variation. Indeed, in this paper we will show that even if\na model of natural variation is not explicitly known a priori, one can train neural networks to be\nrobust against natural variation by learning a model of this variation in an ofﬂine and data-driven\nfashion. More broadly, we claim that the framework described in this paper represents a new and\nmore general paradigm for robust deep learning as it provides a methodology for improving the\nrobustness of deep learning against arbitrary sources of natural variation.\n3\nModel-based robust deep learning\nIn the following section, we introduce the model-based robust deep learning paradigm. Motivated\nby past work concerning robustness against adversarially-chosen, norm-bounded perturbations,\nwe formulate a robust optimization problem that characterizes a new notion of robustness with\nrespect to natural variation. To concertize this formulation, we also offer a geometric interpretation\nof this novel notion of robustness.\n9\n(a) Perturbation-based adversarial example. In the\nperturbation-based robustness setting, an input da-\ntum such as the image of the panda on the left is\nperceptually indistinguishable from the adversarial\nexample shown on the right.\n(b) Natural variation. In this paper, we study ro-\nbustness with respect to natural variation. In this\nexample, the image of the street in snowy weather\non the right vis-a-vis the image on the left illustrates\none form of natural variation.\nFigure 1: A new notion of robustness. The adversarial robustness community has predominantly\nfocused on norm-bounded adversaries. Such adversaries add artiﬁcial noise to an input image to\nproduce an adversarial example that looks perceptually similar to the input, but fools a deep neural\nnetwork. In this paper, we focus on adversaries which change an input datum by subjecting it\nto natural variation. Such variation often does not obey norm-bounded constraints and renders\ntransformed data perceptually quite different from the original image.\n3.1\nAdversarial examples versus natural variation\nAs we showed in Section 2, the problem of defending neural networks against adversaries that\ncan perturb data by a small amount δ in some Euclidean p-norm can be formulated as a robust\noptimization problem, as described by equation (2.2). In this way, solving (2.2) engenders neural\nnetworks that are robust to imperceptible noise. This notion of robustness is illustrated in the\ncanonical example shown in Figure 1a, in which the adversary can arbitrarily perturb any pixel\nvalues in the image of the panda bear on the left-hand-side to create a new image as long as the\nperturbation is bounded, meaning that δ ∈∆:= {δ ∈Rd : ||δ||∞≤ϵ}. When ϵ > 0 is small, the\ntwo panda bears in Figure 1a are seemingly identical and yet the small perturbation δ can lead to\nmisclassiﬁcation.\nWhile adversarial training provides robustness against the imperceptible perturbations de-\nscribed in Figure 1a, in natural environments data varies in ways that cannot be captured by\nadditive, norm-bounded perturbations. For example, consider the two trafﬁc signs shown in\nFigure 1b. Note that the images on the left and on the right show the same trafﬁc sign; however, the\nimage on the left shows the sign on a sunny day, whereas the image on the right shows the sign in\nthe middle of a snow storm. This example prompts several relevant questions. How do we ensure\nthat neural networks are robust to such natural variation? How can we rethink adversarial training\nalgorithms to provide robustness against natural-varying and challenging data?\nFormalizing a more general perspective on robustness. In this paper we advocate for a new and\nmore general notion of robustness in deep learning with respect to natural, out-of-distribution\nshifts in the data. Critical to our approach is the existence of a model of natural variation G(x, δ).\nConcretely, a model of natural variation G : Rd × ∆→Rd is a mapping that describes how an input\ndatum x can be naturally varied by a nuisance parameter δ resulting in a new image x′ := G(x, δ).\nAn illustrative example of such a model is shown in Figure 2, where the input image x on the left\n(in this case, in sunny weather) is transformed into a semantically similar image x′ on the right (in\nsnowy weather) by varying the nuisance parameter δ. Ideally, for a ﬁxed input image x, the impact\n10\nFigure 2: Models of natural variation. Throughout this paper, we will use models of natural variation\nto describe a wide variety of natural transformations that data are often subjected to in natural,\nreal-world environments. In our formulation, models of natural variation take the form G(x, δ),\nwhere x is an input datum such as an image and δ is a nuisance parameter that characterizes the\nextent to which the output datum x′ := G(x, δ) is varied.\nof varying the nuisance parameter δ should be to induce different levels of natural variation on the\noutput image x′. For example, in Figure 2, the model of natural variation should be able to produce\nimages with a dusting of snow as well as images with an all-out blizzard simply by varying the\nnuisance parameter δ.\nFor the time being, we assume the existence of a suitable model of natural variation G(x, δ); later,\nin Section 4, we will detail our approach for obtaining models of natural variation that correspond\nto a wide variety of natural shifts in the data distribution. In this way, given a model of natural\nvariation G(x, δ), our immediate goal to develop novel model-based robust training algorithms that\ntrain neural networks to be robust against natural variation by exploiting the model G(x, δ). For\ninstance, if G(x, δ) models variation in the lighting conditions in an image, our model-based training\nalgorithm will provide robustness against lighting discrepancies. On the other hand, if G(x, δ)\nmodels changes in weather conditions such as in Figure 1b, then our model-based algorithms will\nimprove the robustness of trained classiﬁers against varying weather conditions. More generally,\nour model-based robust training formulation is agnostic to the source of natural variation, meaning\nthat our paradigm is broadly applicable to any source of natural variation that a model of natural\nvariation G(x, δ) can capture.\n3.2\nFormulating the model-based robust optimization problem\nIn what follows, we provide a mathematical formulation for the model-based robust deep learning\nparadigm. This formulation retains the fundamental elements of the adversarial training paradigm\ndescribed in Section 2. In this sense, we again consider a classiﬁcation task in which the goal is to\ntrain a neural network with weights w ∈Rp to correctly predict the label y ∈[k] of a corresponding\ninput instance x ∈Rd, where (x, y) ∼D. This setting is identical to the setting described in the\npreamble to equation (2.1).\nOur point of departure from the classical adversarial training formulation of (2.2) is in the\nchoice of the so-called adversarial perturbation. In this paper, we assume that the adversary has\naccess to a model of natural variation G(x, δ), which allows it to transform x into a distinct yet\nrelated instance x′ := G(x, δ) by choosing different values of δ from a given nuisance space ∆. The\ngoal in this setting is to train a classiﬁer that achieves high accuracy both on a test set drawn i.i.d.\n11\nfrom D and on more-challenging test data that has been subjected to the source of natural variation\nthat G models. In this sense, we are proposing a new training paradigm for deep learning that\nprovides robustness against models of natural variation G(x, δ).\nIn order to defend a neural network against such an adversary, we propose the following\nmodel-based robust optimization problem, which will be the central object of study in this paper:\nmin\nw E(x,y)∼D\n\u0014\nmax\nδ∈∆ℓ(G(x, δ), y; w)\n\u0015\n.\n(3.1)\nHere the nuisance space ∆may be problem-dependent; indeed, different parameterizations of the\nmodel of natural variation G(x, δ) may inﬂuence the choice of ∆. We defer a discussion of the\nchoice of the nuisance space ∆until Section 7.2.\nConceptually, the intuition for this formulation is similar to the intuition for (2.2) given in\nSection 2. Indeed, the optimization problem in (3.1) also comprises an inner maximization problem\nand an outer minimization problem:\nδ⋆∈arg max\nδ∈∆\nℓ(G(x, δ), y; w)\n(3.2)\nw⋆∈arg min\nw∈Rp\nE(x,y)∼D [ℓ(G(x, δ⋆), y; w)]\n(3.3)\nIn the inner maximization problem of (3.2), given an instance-label pair (x, y) and a ﬁxed weight\nw ∈Rp, the adversary seeks a nuisance parameter δ∗∈∆that produces a corresponding instance\nx′ := G(x, δ∗) which gives rise to high loss values ℓ(G(x, δ∗), y; w) under the current weight w.\nOne can think of this vector δ∗as characterizing the most-challenging nuisance that can be captured\nby the model G(x, δ∗) for the original instance x. After solving this inner problem, we can rewrite\nthe outer minimization problem via the expression shown in (3.3). In this outer problem, we seek\nthe weight w ∈Rp that minimizes the risk against challenging instances of the form G(x, δ∗). By\ntraining the network to correctly classify these worst-case data, ideally the classiﬁer should become\ninvariant to the model G(x, δ) for any δ ∈∆and consequently to the original source of natural\nvariation.\n3.3\nGeometry of model-based robust training\nTo provide further intuition for (3.1), in Figure 3, we consider the underlying geometry of the\nperturbation-based and model-based robust training paradigms. The geometry of perturbation-\nbased adversarial robustness is shown in Figure 3a, wherein each datum x can be perturbed to any\nother datum xadv contained in a small ϵ-neighborhood around x. That is, the data can be additively\nperturbed via x 7→xadv := x + δ where δ is constrained to lie in a set ∆:= {δ ∈Rd : ||δ||p ≤ϵ}. As\nϵ is generally chosen to be quite small, perturbations generated in the perturbation-based paradigm\ncharacterize a local notion of robustness, meaning that adversarial examples are constrained to be\nclose to the original image x with respect to a Euclidean norm over the data space Rd.\nFigure 3b shows the geometry of the model-based robust training paradigm. Let us consider a\ntask in which our goal is to correctly classify images of street signs in varying weather conditions.\nIn the model-based robust training paradigm, we assume that we are equipped with a model of\nnatural variation G(x, δ) which, by varying the nuisance parameter δ ∈∆, changes the output\nimage x′ := G(x, δ) according to the natural phenomena captured by the model. For example, if\nour data contains images x in sunny weather, the model G(x, δ) may be designed to continuously\n12\n(a) Perturbation-based robustness. In perturbation-\nbased adversarial robustness, an adversary can per-\nturb a datum x into a perceptually similar datum\nxadv := x + δ. When δ is constrained to lie in a set\n∆:= {δ ∈Rd : ||δ||p ≤ϵ}, the underlying geom-\netry of the problem can be used to ﬁnd worst-case\nadditive perturbations.\n(b) Model-based robustness.\nIn our paradigm,\nmodels of natural variation can be though of char-\nacterizing a class of learned image manifolds B(x). By\nsearching over these manifolds, in the model-based\nrobust deep learning paradigm, we seek images\nx′ ∈B(x) that have been subjected to high levels of\nnatural variation.\nFigure 3: Geometry of perturbation-based and model-based robustness. Both the perturbation-\nbased and model-based training paradigms have useful geometric interpretations. Indeed, whereas\nthe perturbation-based paradigm considers a local Euclidean notion of robustness, the model-based\nparadigm considers much larger changes in data induced by models of natural variation.\nvary the weather conditions in these images without changing the scene, other vehicles on the road,\nor the size and shape of the street signs in these images.\nMore generally, such model-based variations around x have a manifold-like structure in the\ndata space Rd. More speciﬁcally, the set of output images corresponding to a ﬁxed input image\nx that can be obtained by varying the nuisance parameter δ can be captured by the learned image\nmanifold B(x), which we deﬁne as follows:\nB(x) = BG(x) := {x′ ∈Rd : x′ = G(x, δ) for some δ ∈∆}\n(3.4)\nNote that the learned image manifold is deﬁned implicitly in terms of a model of natural variation\nG(x, δ). Formally, for a ﬁxed image x, the set B(x) is a parameterized dim(∆)-manifold lying in\nRd, where dim(∆) denotes the dimension of the nuisance space ∆. As we will show, for many\nmodels of natural variation, dim(∆) and therefore the dimension of the learned image manifold\nB(x) will be signiﬁcantly smaller than the dimension d of the data space Rd. Furthermore, given\nthe deﬁnition of the learned image manifold in (3.4), the outer maximization problem (3.3) can be\nrewritten in the following way:\nδ⋆∈arg max\nx′∈B(x)\nℓ(x′, y; w)\n(3.5)\n13\nAlgorithm 1 Known model for background color\nInputs: x ∈RC×H×W, δ := (r, g, b) ∈[0, 255]3\nOutput: new image x\n1: bgd_image ←0C×H×W\n2: bgd_image[0, :, :] ←r\n3: bgd_image[1, :, :] ←g\n4: bgd_image[2, :, :] ←b\n5: x ←where(x ≤12, bgd_image, x)\nFigure 4: Known models of natural variation. In a variety of cases, a model of how data varies in\na robustness problem is known a priori. In these cases, the model can immediately be exploited\nin our model-based training paradigm. For example, a known model of how background colors\nchange for the MNIST digits can be directly leveraged for model-based training.\nThis shows that solving the inner maximization problem of (3.2) corresponds to ﬁnding a point x′\non the learned image manifold B(x) which causes high loss under the current weight w ∈Rp. The\ngoal of the model-based training algorithms we provide in Section 5 will be to search over these\nmanifolds to ﬁnd images with challenging levels of natural variation.\n4\nModels of natural variation\nOur model-based robustness paradigm of (3.1) critically relies on the existence of a model of natural\nvariation G(x, δ) that maps x 7→G(x, δ) := x′ and consequently describes how a datum x can be\ndeformed into x′ via the choice of a nuisance parameter δ ∈∆. In this section, we consider cases\nin which (1) a model G(x, δ) is known a priori, and (2) a model G(x, δ) is unknown and therefore\nmust be learned ofﬂine from data. In this second case in which models of natural variation must be\nlearned from data, we propose a formulation for obtaining such models.\n4.1\nKnown models G(x, δ) of natural variation\nFor many problems, a model G(x, δ) is known a priori due to underlying physical or geometric\nlaws and can be immediately exploited in our model-based robust training formulation. One\nstraightforward example in which a model of natural variation G(x, δ) is known is the classical\nadversarial training paradigm described by equation (2.2). Indeed, by inspecting equations (2.2)\nand (3.1), we can immediately extract the well-known norm-bounded adversarial model:\nG(x, δ) = x + δ\nfor δ ∈∆:= {δ ∈Rd : ||δ||p ≤ϵ}.\n(4.1)\nThe above example of a known model shows that in some sense the perturbation-based adversarial\ntraining paradigm of equation (2.2) is a special case of the model-based robust deep learning\nparadigm (3.1) when G(x, δ) = x + δ. Of course, for this choice of adversarial perturbations there\nis a plethora of robust training algorithms [22, 23, 24, 25, 26, 27, 28].\n14\nAnother example of a known model of natural variation is shown in Figure 4. Consider a\nscenario in which we would like to be invariant to changes in the background color for the MNIST\ndataset [48]. This would require having a model G(x, δ) that takes an MNIST digit x as input\nand reproduces the same digit but with various colorized RGB backgrounds which correspond to\ndifferent values of δ ∈∆. This model is relatively simple to describe; pseudocode is provided in\nAlgorithm 1.\nMore broadly, there are many settings in which naturally-occuring variation in data has geo-\nmetric structure that is known a priori. For example, in image classiﬁcation tasks, there are usually\nintrinsic geometric structures that identify how data can be rotated, translated, or scaled. Indeed,\ngeometric models for rotating an image along a particular axis can be characterized by a one-\ndimensional angular parameter δ. In this case, a known model of natural variation for rotation can\nbe described by\nG(x, δ) = R(δ)x\nfor δ ∈∆:= [0, 2π).\n(4.2)\nwhere R(δ) is a rotation matrix. Such geometric models can facilitate adversarial distortions of\nimages using a low-dimensional parameter δ. In prior work, this idea has been exploited to train\nneural networks to be robust against rotations of the data around a given axis [49, 50, 51].\nAltogether, these examples show that for a variety of problems, known models can be used\nto analytically describe how data changes. In the context of known models, our model-based\napproach offers a more general framework that is model-agnostic in the sense that it is applicable\nto all such models of how data varies. Before describing our approach for learning models of\nnatural variation from data when a known model is not available, we brieﬂy explore the connection\nbetween known models of natural variation and equivariant neural networks.\nConnections to equivariant neural networks. Recently, geometric and spatial transformations\nhave been considered in the development of equivariant neural network architectures. In many\nof these studies, one considers a transformation T : Rd × ∆→Rd where ∆has some algebraic\nstructure [7]. By deﬁnition, we say that a function f is equivariant with respect to T if f (T(x, δ)) =\nT( f (x), δ) for all δ ∈∆. That is, applying T to an input x and then applying f to the result is\nequivalent to applying T to f (x). To this end, there has been a great deal of recent work that involves\ndesigning architectures that are equivariant to various transformations of data [6, 7, 52, 53, 54].\nRecently, this has been extended to leveraging group convolutions, which can be used to provide\nequivariance with respect to certain symmetric groups [55] and to permutations of data [56].\nInterestingly, it has been shown that rotationally equivariant neural networks are signiﬁcantly less\nvulnerable to geometric invariance-based adversarial attacks [57]. In the context of this paper, these\nstructured transformations of data T : Rd × ∆→Rd can be viewed as models of natural variation\nby directly setting G(x, δ) = T(x, δ), where ∆may have additional group structure.\nIn contrast to the literature that concerns equivariance, much of the adversarial robustness\ncommunity has focused on what is often called invariance. A function f is said to be invariant to T\nif f (T(x, δ)) = f (x) for any δ ∈∆, meaning that transforming an input x by T has no impact on\nthe output. While previous approaches exploit such transformations for designing architectures\nthat respect this structure, our goal is to exploit this structure toward developing robust training\nalgorithms.\n15\nFigure 5: Learned models of natural variation. When a known model of natural variation is not\navailable, we advocate for learning models of natural variation G(x, δ) ofﬂine from data. To this\nend, we formulate a statistical procedure that characterizes the problem of learning a suitable\nmodel of natural variation G(x, δ) from unlabelled and unpaired data. The model in this ﬁgure\nwas trained on SVHN using the MUNIT framework to vary the brightness in a given input image;\nin this case, the nuisance space ∆was the cube [−1, 1] × [−1, 1] ⊂R2.\n4.2\nLearning unknown models of natural variation G(x, δ) from data\nWhile geometry and physics may provide analytical models of natural variation G(x, δ) that can be\nexploited in our model-based robust training procedure, in many situations such models are not\nknown or are too costly to obtain. For example, consider Figure 3b in which a model of natural\nvariation G(x, δ) describes the impact of adding snowy weather to an image x. In this case, the\ntransformation G(x, δ) takes an image x of a street sign in sunny weather and maps it to an image\nx′ := G(x, δ) in snowy weather. Even though there is a relationship between the snowy and the\nsunny images, obtaining a model G relating the two images is extremely challenging if we resort to\nphysics or geometric structure. For such problems, we advocate for learning the model G(x, δ) from\ndata prior to model-based robust training. An example of a learned model of natural variation is\nshown in Figure 5.\nIn what follows, we introduce a statistical framework for learning models of natural variation\nfrom data. In particular, we advocate for a procedure in which a model of natural variation G(x, δ)\nis learned ofﬂine using unlabeled and unpaired data prior to performing model-based robust training\non a new and possibly different dataset. We note that while the procedure we describe is quite\ngeneral, there are likely other formulations that may also result in suitable models of natural\nvariation. Indeed, one interesting future direction is to explore approaches in which one learns\na model of natural variation G(x, δ) and trains a classiﬁer via the model-based robust training\nparadigm simultaneously.\nA statistical framework for learning models of natural variation. In order to learn a model of\nnatural variation G(x, δ), we assume that we have access to two unpaired image domains A and B\nthat are drawn from a common dataset or distribution. Generally speaking, in our setting domain\nA will contain the original data without any natural variation, and domain B will contain data that\nhas been transformed by an underlying natural phenomenon. Thus, in the example of Figure 3b,\ndomain A would contain images of trafﬁc signs in sunny weather, and domain B would contain\nimages of street signs in snowy weather. We emphasize that the domains A and B are unpaired,\n16\nmeaning that it may not be possible to select an image of a trafﬁc sign in sunny weather from\ndomain A and ﬁnd a corresponding image of that same street sign in the same scene with snowy\nweather in domain B.\nOur approach toward formalizing the idea of learning a model of natural variation G(x, δ)\nfrom data is to view G as a mechanism that transforms the distribution of data in domain A\nso that it resembles the distribution of data in domain B. More formally, let PA and PB be the\ndata distributions corresponding to domains A and B respectively. Our objective is to learn a\nmapping (x, δ) 7→G(x, δ) that takes as input a datum x ∼PA and a nuisance parameter δ ∈∆and\nthen produces a new datum x′ ∼PB. Statistically speaking, the nuisance parameter δ represents\nthe extra randomness or variation required to engender a multimodal distribution over output\nimages distributed according to PB with different levels of natural variation that correspond\nsemantically to a given input image x. For example, when considering images with varying\nweather conditions, the randomness in the nuisance parameter δ might control whether an image\nof a sunny scene is mapped to a corresponding image with a dusting of snow or to an image in an\nall-out blizzard. In this way, we without loss of generality we assume that the nuisance parameter is\nindependently generated from a simple distribution P∆(e.g. uniform or Gaussian) to represent the\nextra randomness required to generate x′ from x. In this sense, the role of the nuisance parameter\nis somewhat similar to the role of the noise variable in generative adversarial networks [58].\nUsing this formalism, we can view G(·, ·) as a mapping that transforms the distribution PA × P∆\ninto the distribution PB. More speciﬁcally, G pushes forward the measure PA × P∆, which is deﬁned\nover A × ∆, to PB, which is deﬁned over B. That is, ideally a model of natural variation should\napproximately satisfy the following expression:\nPB = G # (PA × P∆)\n(4.3)\nwhere # denote the push-forward measure. Now in order to make this framework for learning\na model of natural variation G concrete, we consider a parametric family of models of natural\nvariation G := {Gθ : θ ∈Θ} deﬁned over a parameter space Θ ⊂Rm. We can express the problem\nof learning a model of natural variation Gθ∗parameterized by the θ∗∈Θ that best ﬁts the above\nformalism in the following way:\nθ∗∈arg min\nθ∈Θ\nd (PB, Gθ # (PA × P∆)).\n(4.4)\nHere d(·, ·) is an appropriately-chosen distance metric that measures the distance between two\nprobability distributions (e.g. the KL-divergence or Wasserstein distance).\nThis problem has received broad interest in the machine learning community thanks to the\nrecent advances in generative modeling. In particular, in the ﬁelds of image-to-image translation\nand style-transfer, learning mappings between unpaired image domains is a well-studied problem\n[4, 59, 60]. In the next subsection, we will show how the breakthroughs in these ﬁelds can be\nused to learn a model of natural variation G(x, δ) that closely approximates underlying natural\nphenomena.\n4.3\nUsing deep generative models to learn models of natural variation\nRecall that in order to learn a model of natural variation from data, we aim to solve the optimization\nproblem in (4.4) and to consequently obtain a model Gθ∗that transforms x ∼PA into corresponding\n17\nFigure 6: Learning models of natural variation via disentangled representations. In this paper,\nwe exploit recent progress in generative modeling toward learning models of natural variation\nG(x, δ) from data. Such architectures generally use an encoder-decoder structure, in which an\nencoding network learns to separate semantic from nuisance content in two latent spaces, and the\ndecoder learns to reconstruct an image from the representations in these latent spaces [4, 61]. Thus,\nby varying the nuisance content (which we refer to as the nuisance parameter), one can produce a\nmultimodal distribution over output images.\nsamples x′ ∼PB. In the literature concerning image-to-image translation networks, a variety of\nworks have sought to solve this problem. Indeed, numerous methods have leveraged disentangled\nrepresentations and cycle-consistency toward achieving this goal [2, 4, 60, 59, 62, 61, 63]. Furthermore,\na closely related line of work has relied on class-conditioning in an image-to-image translation\nframework to generate realistic images [64, 65, 66, 67]. We note that while class-conditional image-\nto-image translation networks have been shown to be successful at generating realistic samples, in\nour framework we do not assume that the data used for training such networks from either domain\nis labeled; an exploration of how conditioning could be used toward learning models of natural\nvariation is a fruitful direction for future work.\nAmong the methods mentioned in the previous paragraph, [61, 62, 4] all seek to learn multimodal\nmappings without relying on class-conditioning, meaning that they seek to disentangle the semantic\ncontent of a datum (i.e. its label or the characterizing component of an input image) from the nuisance\ncontent (e.g. background color, weather conditions, etc.) to produce a multimodal distribution over\nvarying output images. We highlight these methods because learning a multimodal mapping is a\nconcomitant property toward learning models of natural variation that can produce images subject\nto a range of natural conditions. Indeed, any of these architectures are suitable for solving (4.4).\nHowever, throughout the experiments that are presented in Section 6, for simplicity we adhere to a\nparticular choice for the architecture for G.\nAn architecture for models of natural variation. In this paper we will predominantly use the\nMultimodal Unsupervised Image-to-Image Translation (MUNIT) framework [4] to learn models\nof natural variation. At its core, MUNIT combines two autoencoding networks [68] and two\ngenerative adversarial networks (GANs) [58] to learn two mappings: one that maps images from\ndomain A to corresponding images in B and one that maps in the other direction from B to A.\n18\nFor completeness, we provide a complete characterization of the MUNIT framework and the\nhyperparamters we used to train models of natural variation using MUNIT in Appendix A. For the\npurposes of this paper, we will only exploit the mapping from A to B, although one direction for\nfuture work is to incorporate both mappings. Therefore, the map G : A × ∆→B learned in the\nMUNIT framework can be thought of as taking as input an image x ∈A and a nuisance parameter\nδ ∈∆and outputting an image x′ ∈B that has the same semantic content as the input image x but\nthat has a different level of natural variation. This architecture is illustrated in Figure 6. Notice\nthat the encoding network encodes the input image x into a semantic component and a nuisance\nparameter; by varying this nuisance parameter and then decoding, the MUNIT framework can be\nused to vary the natural conditions in the input image.\nWe emphasize that our model-based robust deep learning paradigm is not reliant on any\nparticular feature of the MUNIT framework. Indeed, while our results exhaustively show that\nMUNIT is a suitable method for a variety of datasets and sources of natural variation, an interesting\nfuture direction is to explore the impact of different image-to-image translation architectures toward\nlearning suitable models of natural variation. In Table 1, we show images from several datasets\nand corresponding images generated by models of natural variation learned using the MUNIT\nframework. Each of these learned models of natural variation corresponds to a different source\nof natural variation. For each of these models, we used an eight-dimensional nuisance space\n∆:= [0, 1]8 ⊂R8; the output images are generated by sampling different values from ∆uniformly\nat random.\n4.4\nA gallery of learned models of natural variation\nTo demonstrate the efﬁcacy of the MUNIT framework toward learning models of natural variation\nG(x, δ), in Table 1 we show a gallery of learned models of natural variation learned using MUNIT\nfor various datatsets and sources of natural variation. Importantly, this table shows that our\nframework in conjunction with the MUNIT architecture can be used to learn perceptually realistic\nmodels of natural variation for both low-dimension data (e.g. SVHN) and high-dimensional data\n(e.g. ImageNet). To this end, in Section 7.2, we will more quantitatively evaluate the ability of\nlearned models of natural variation to produce realistic output image distributions.\n5\nModel-based robust training algorithms\nIn the previous section, we described a procedure that can be used to train models of natural\nvariation G(x, δ). In some cases, such models may be known a priori while in other cases such\nmodels may be learned ofﬂine from data. Regardless of their origin, we will now assume that\nwe have access to a suitable model G(x, δ) and shift our attention toward exploiting G in the\ndevelopment of novel robust training algorithms.\nTo begin, recall the optimization-based formulation of (3.1). Given a model of natural variation\nG(x, δ), (3.1) is a nonconvex-nonconcave min-max problem, and is therefore difﬁcult to solve\nexactly. We will therefore resort to approximate methods for solving this challenging optimization\nproblem. To elucidate our approach for solving (3.1), we ﬁrst characterize the problem in the\nﬁnite-sample setting. That is, rather than assuming access to the full joint distribution (x, y) ∼D,\nwe assume that we are given given a ﬁnite number of samples Dn := {(x(j), y(j))}n\nj=1 distributed\ni.i.d. according to the true data distribution D. The empirical version of (3.1) in the ﬁnite-sample\n19\nDataset\nNatural\nVariation\nImages\nOriginal\nGenerated\nSVHN\nBrightness\nContrast\nHue\nGTSRB\nBrightness\nCURE-TSR\nSnow\nImageNet\nSnow\nBrightness\nFog\nTable 1: A gallery of learned models of natural variation. For a range of datasets, we show images\ngenerated by passing data through learned models of natural variation.\n20\nsetting can be expressed in the following way:\nw⋆∈arg min\nw∈Rp\n1\nn\nn\n∑\nj=1\n\u0014\nmax\nδ∈∆ℓ\n\u0010\nG\n\u0010\nx(j), δ\n\u0011\n, y(j); w\n\u0011\u0015\n.\n(5.1)\nConcretely, we search for the parameter w ∈Rp that induces the smallest empirical error while\neach sample (x(j), y(j)) is varied according to the model of natural variation G(x(j), δ). In particular,\nwhile subjecting each instance-label pair (x(j), y(j)) to the source of natural variation modeled by\nG, we search for nuisance parameters δ ∈∆so as to train the classiﬁer on the most challenging\nnatural conditions.\nWhen the learnable weights w ∈Rp parameterize a neural network fw, the outer minimization\nproblem and the inner maximization problem are inherently nonconvex and nonconcave respec-\ntively. Therefore, we will rely on zeroth- and ﬁrst-order optimization techniques for solving this\nproblem to a locally optimal solution. We will propose three algorithmic variants, each of which\ntakes a integer parameter k > 0: (1) Model-based Robust Training (MRT-k), (2) Model-based Adversarial\nTraining (MAT-k), and (3) Model-based Data Augmentation (MDA-k).\nAn overview of the model-based robust training algorithms. Each of the three algorithms we\npropose in this paper – MRT-k, MAT-k, and MDA-k – seeks a solution to (5.1) by alternating between\nsolving the outer minimization problem and solving the inner maximization problem. Indeed,\none similarity amongst these three algorithms is that each procedure seeks a solution to the outer\nproblem by using a standard ﬁrst-order optimization technique (e.g. SGD or Adam). However, the\nalgorithms differ in how they search for a solution to the inner problem; at a high level, each of\nthese methods seeks such a solution by augmenting the original training dataset Dn with new data\ngenerated by a given model of natural variation G(x, δ). In particular, MRT randomly queries G\nto generate several new data points and then selects those generated data that induce the highest\nloss in the inner maximization problem. On the other hand, MAT employs a gradient-based search\nin the nuisance space ∆to ﬁnd loss-maximizing generated data. Finally, MDA augments the\ntraining dataset Dn with generated data by sampling randomly in ∆to produce a wide range of\nnatural conditions. We note that past approaches have used similar adversarial [22] and statistical\n[69] augmentation techniques. However, the main difference between these past works and our\nalgorithms are that our algorithms exploit models of natural variation G(x, δ) to generate new data.\nIn the remainder of this section, we will describe each algorithm in detail and provide psue-\ndocode for each algorithm. Python implementations of each algorithm are available at the following\nlink: https://github.com/arobey1/mbrdl.\n5.1\nModel-based Robust Training (MRT)\nIn general, solving the inner maximization problem in (5.1) is difﬁcult and motivates the need for\nmethods that yield approximate solutions. In this vein, one simple scheme is to sample different\nnuisance parameters δ ∈∆for each instance-label pair (x(j), y(j)) and among those sampled values,\nﬁnd the nuisance parameter δadv that gives the highest empirical loss under G. Indeed, this\napproach is not designed to ﬁnd an exact solution to the inner maximization problem; rather it\naims to ﬁnd a difﬁcult example by sampling in the nuisance space of the model of natural variation.\nOnce we obtain this difﬁcult example by sampling in ∆, the next objective is to solve the\nouter minimization problem. The procedure we propose in this paper for solving this problem\n21\nAlgorithm 2 Model-based Robust Training (MRT)\nInput: weight initialization w, trade-off parameter λ ∈[0, 1], number of steps k\nOutput: learned weight w\n1: repeat\n2:\nfor minibatch Bm := {(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))} ⊂Dn do\n3:\nInitialize max_loss ←0 and δadv := (δ(1)\nadv, δ(2)\nadv, . . . , δ(m)\nadv) ←(0q, 0q, . . . , 0q)\n4:\nfor k steps do\n5:\nSample δ(j) uniformly at random from ∆for j = 1, . . . , m\n6:\ncurrent_loss ←∑m\nj=1 ℓ(G(x(j), δ(j)), y(j); w)\n7:\nif current_loss > max_loss then\n8:\nmax_loss ←current_loss\n9:\nδ(j)\nadv ←δ(j) for j = 1, . . . , m\n10:\nend if\n11:\nend for\n12:\ng ←∇w ∑m\nj=1[ℓ(G(x(j), δ(j)\nadv), y(j); w) + λ · ℓ(x(j), y(j); w)]\n13:\nw ←Update(g, w)\n14:\nend for\n15: until convergence\namounts to using the worst-case nuisance parameter δadv obtained via the inner maximization\nproblem to perform data-augmentation. That is, for each instance-label pair (x(j), y(j)), we treat\n(G(x(j), δadv), y(j)) as a new instance-label pair that can be used to supplement the original dataset\nDn. These training data can be used together with ﬁrst-order optimization methods to solve the\nouter minimization problem to a locally optimal solution w∗.\nAlgorithm 2 contains the pseudocode for the MRT algorithm. In particular, in lines 4-11, we\nsearch for a difﬁcult example by sampling in ∆and picking the parameter δadv ∈∆that induces the\nhighest empirical loss. Then in lines 12-13, we calculate a stochastic gradient of the loss with respect\nto the weights w ∈Rp of the classiﬁer; we then use this gradient to update w using a ﬁrst-order\nmethod. There are a number of potential algorithms for this Update function in line 13, including\nstochastic gradient descent (SGD), Adam [40], and Adadelta [70].\nThroughout the experiments in the forthcoming sections, we will train classiﬁers via MRT with\ndifferent values of k. In this algorithm, k controls the number of data points we consider when\nsearching for a loss-maximing datum. To make clear the role of k in this algorithm, we will refer to\nAlgorithm 2 as MRT-k when appropriate.\n5.2\nModel-based Adversarial Training (MAT)\nAt ﬁrst look, the sampling-based approach used by MRT may not seem as powerful as a ﬁrst-order\n(i.e. gradient-based) adversary that has been shown to be effective at improving the robustness\nof trained classiﬁers against norm-bounded, perturbation-based attacks [71]. Indeed, it is natural\nto extend the ideas encapsulated in this previous work that advocate for ﬁrst-order adversaries\nto the model-based setting. That is, under the assumption that our model of natural variation\nG(x, δ) is differentiable, in principle we can use projected gradient ascent (PGA) in the nuisance\n22\nAlgorithm 3 Model-based Adversarial Training (MAT)\nInput: weight initialization w, trade-off parameter λ ∈[0, 1], number of steps k\nOutput: learned weight w\n1: repeat\n2:\nfor minibatch Bm := {(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))} ⊂Dn do\n3:\nInitialize δadv := (δ(1)\nadv, δ(2)\nadv, . . . , δ(m)\nadv) ←(0q, 0q, . . . , 0q)\n4:\nfor k steps do\n5:\ngadv ←∇δadv ∑m\nj=1 ℓ(G(x(j), δ(j)\nadv), y(j); w)\n6:\nδadv ←Π∆[δadv + αgadv]\n7:\nend for\n8:\ng ←∇w ∑m\nj=1[ℓ(G(x(j), δ(j)\nadv), y(j); w) + λ · ℓ(x(j), y(j); w)]\n9:\nw ←Update(g, w)\n10:\nend for\n11: until convergence\nspace ∆⊂Rq of a given model to solve the inner maximization problem. This idea motivates the\nformulation of our second algorithm, which we call Model-based Adversarial Training (MAT).\nIn Algorithm 3, we present pseudocode for MAT. Notably, by ascending the stochastic gradient\nwith respect to δadv in lines 4-7, we seek a nuisance parameter δ∗\nadv that maximizes the empirical\nloss. In particular, in line 6 we perform the update step of PGA to obtain δadv ∈∆; in this notation,\nΠ∆denotes the projection onto the set ∆. However, performing PGA until convergence at each\niteration leads to a very high computational complexity. Thus, at each training step, we perform k\nsteps of projected gradient ascent. Following this procedure, we use the loss-maximization nuisance\nparameter δ∗\nadv to augment Dn with data G(x(j), δ∗\nadv) that has been subjected to worst-case nuisance\nvariability. The update step is then carried out by computing the stochastic gradient of the loss over\nthe augmented training sample with respect to the learnable weights w ∈Rp in line 8. Finally, we\nupdate w in line 9 in a similar fashion as was done in the description of the MRT algorithm.\nAn empirical analysis of the performance of MAT will be given in Section 6. To emphasize the\nrole of the number of gradient steps k used to ﬁnd a loss maximizing nuisance parameter δ∗\nadv ∈∆,\nwe will often refer to Algorithm 3 as MAT-k.\n5.3\nModel-based Data Augmentation (MDA)\nBoth MRT and MAT adhere to the common philosophy of selecting loss-maximizing data generated\nby a model of natural variation G(x, δ) to augment the original training dataset Dn. That is, in\nkeeping with the min-max formulation of (3.1), both of these methods search adversarially over\n∆to ﬁnd challenging natural variation. More speciﬁcally, for each data point (x(j), y(j)), these\nalgorithms select δ ∈∆such that G(x(j), δ) =: x(j)\nadv maximizes the loss term ℓ(x(j)\nadv, y(j); w). The\nguiding principle behind these methods is that by showing the neural network these challenging,\nmodel-generated data during training, the trained classiﬁer will be able to robustly classify data\nover a wide spectrum of natural conditions.\nAnother interpretation of (3.1) is as follows. Rather than taking an adversarial point of view\nin which we expose neural networks to the most challenging model-generated examples, an\n23\nAlgorithm 4 Model-Based Data Augmentation (MDA)\nInput: weight initialization w, trade-off parameter λ ∈[0, 1], number of steps k\nOutput: learned weight w\n1: repeat\n2:\nfor minibatch Bm := {(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))} ⊂Dn do\n3:\nInitialize x(j)\ni\n←0d for i = 1, . . . , k and for j = 1, . . . , m\n4:\nfor k steps do\n5:\nSample δ(j) randomly from ∆for j = 1, . . . , m\n6:\nx(j)\ni\n←G(x(j), δ(j)) for j = 1, . . . , m\n7:\nend for\n8:\ng ←∇w ∑m\nj=1[∑k\ni=1 ℓ(x(j)\ni , y(j); w) + λ · ℓ(x(j), y(j); w)]\n9:\nw ←Update(g, w)\n10:\nend for\n11: until convergence\nalternative is to expose these networks to a diversity of model-generated data during training. In\nthis approach, by augmenting Dn with model-generated data corresponding to a wide range of\nnatural variations δ ∈∆, one might hope to achieve higher levels of robustness with respect to a\ngiven model of natural variation G(x, δ).\nThis idea motivates the third and ﬁnal algorithm, which we call Model-based Data Aug-\nmentation (MDA). The psuedocode for this algorithm is given in Algorithm 4. Notably, rather\nthan searching adversarially over ∆to ﬁnd model-generated data subject to worst-case (i.e. loss-\nmaximizing) natural variation, in lines 4-7 of MDA we randomly sample in ∆to obtain a diverse\narray of nuisance parameters. For each such nuisance parameter, we augment Dn with a new\ndatum and calculate the stochastic gradient with respect to the weights w ∈Rp in line 8 using both\nthe original dataset Dn and these diverse augmented data.\nIn MDA, the parameter k controls the number of model-based data points per data point in Dn\nthat we append to the training set. To make this explicit, we will frequently refer to Algorithm 4 as\nMDA-k.\n6\nExperiments\nWe present experiments in ﬁve different and challenging settings over twelve distinct datasets to demon-\nstrate the broad applicability of the model-based robust deep learning paradigm.\nExperimental overview. First, in Sections 6.1-6.2, we show that our algorithms are the ﬁrst to\nconsistently provide out-of-distribution robustness across a range of challenging corruptions,\nincluding shifts in brightness, contrast, snow, fog, frost, and haze on CURE-TSR, ImageNet, and\nImageNet-c. Next, in Section 6.3, we show that models of natural variation can be composed to\nprovide robustness against simultaneous shifts. To evaluate this feature, we curate several new\ndatasets containing simultaneous sources of natural variation. Following this, in Section 6.4, we\nshow that models of natural variation G(x, δ) trained on a ﬁxed dataset can be reused to provide\nrobustness on datasets entirely unseen while training G. This demonstrates that model-based\n24\nFigure 7: CURE-TSR snow challenge levels. From left to right we show the same image with\ndifferent levels of snow natural variation. Challenge-level 0 corresponds to no natural variation,\nwhereas challenge-level 5 corresponds to the highest level of natural variation.\nrobustness is highly transferable, meaning that once a model of natural variation has been learned\nfor a particular source of natural variation, it can be reused to provide robustness against the same\nsource of natural variation in future applications. Finally, in Section 6.5, we show that in the setting\nof unsupervised domain adaptation, our algorithm outperforms a well-known domain adaptation\nbaseline.\nNotation for image domains. Throughout this section, we consider a wide range of datasets,\nincluding MNIST [48], SVHN [72], GTSRB [73], CURE-TSR [74], MNIST-m [75], Fashion-MNIST\n[76], EMNIST [77], KMNIST [78], QMNIST [79], USPS [80], ImageNet [81], and ImageNet-c [15]. For\nmany of these datasets, we extract subsets corresponding to different sources of natural variation;\nhenceforth, we will call these subsets domains. To indicate the source of natural variation under\nconsideration in a given experiment, we use the notation “source (A→B)” to denote a distributional\nshift from domain A to domain B. For example, “contrast (low→high)” will denote a shift from\nlow-contrast to high-contrast within a particular dataset. Images from domains A and B for each of\nthe shifts used in this paper are available in Appendix B. We note that our experiments contain\ndomains with both natural and artiﬁcially-generated variation; details concerning how we extracted\nnon-artiﬁcial variation can be found in Appendix C.\nWe emphasize that each image domain used in this paper contains a training set and a test set.\nWhile both the training and test set in each domain come from the same distribution, neither the\nmodels of natural variation nor the classiﬁers have access to test data from any domain during the\ntraining phase. More explicitly, when learning models of natural variation G(x, δ) and training\nclassiﬁers, we use data from the training set of the relevant domains. Conversely, when testing the\nclassiﬁers, we use data from the test set of the relevant domains.\nBaseline algorithms and evaluation metrics. In the experiments, we consider a variety of baseline\nalgorithms. In particular, where approapriate, we compare our model-based algorithms to empirical\nrisk mimization (ERM), the adversarial training algorithm PGD [22], a recently-proposed data-\naugmentation technique called AugMix [39], and the domain adaptation technique known as\nAdversarial Discriminative Domain Adaptation (ADDA) [82]. To evaluate the performance of each\nof these algorithms, we will report the top-1 accuracy (i.e. the standard classiﬁcation accuracy)\nand, where appropriate, the top-5 accuracy (i.e. the frequency with which the ground-truth label\nis one of the top ﬁve predicted classes). Further details concerning architecture selection and the\nhyperparameters used are given in Appendix A.\n25\nCURE-TSR\nsubset\nTest accuracy (top-1) on challenge-levels 3, 4, and 5\nERM + Aug\nPGD + Aug\nMRT\n3\n4\n5\n3\n4\n5\n3\n4\n5\nSnow\n86.5\n74.8\n60.9\n82.9\n77.3\n61.8\n88.0\n77.8\n70.7\nHaze\n55.2\n54.0\n47.5\n83.8\n63.1\n53.4\n83.9\n79.1\n70.1\nDecolorization\n87.9\n85.1\n78.8\n84.7\n75.2\n64.9\n90.5\n89.6\n89.4\nRain\n72.7\n71.7\n66.9\n68.9\n66.4\n60.5\n80.7\n78.7\n74.8\nTable 2: Out-of-distribution robustness. In each experiment, we train a model of natural variation\nG(x, δ) to map from challenge-level 0 to challenge-level 2 data from various subsets of CURE-TSR.\nWe then perform model-based training using challenge-level 0 data and test on challenge-levels 3-5.\nWe allow all baseline classiﬁers access to labeled data from challenge-levels 0 and 2. As our method\ndoes not use labeled data from challenge-level 2, this is in some sense an unfair comparison to our\nalgorithms; despite this, model-based algorithms outperform the baselines across the board.\n6.1\nOut-of-distribution robustness\nIn many applications, one might have data corresponding to low levels of natural variation,\nsuch as a dusting of snow in images of street signs. However, it is often difﬁcult to collect data\ncorresponding to high levels of natural variation, such as images taken during a blizzard. In\nsuch cases, we show that our algorithms can be used to provide signiﬁcant out-of-distribution\nrobustness against data with high levels of natural variation by training on data with relatively low\nlevels of the same source of natural variation. To do so, we use data from the CURE-TSR dataset\n[74], which contains images of street signs divided into subsets according to various sources of\nnatural variation and corresponding severity levels. For example, for images in the “snow” subset,\nchallenge-level 0 corresponds to no snow, whereas challenge-level 5 corresponds to a full blizzard.\nFor each row of Table 2, we use unlabeled data from challenge-levels 0 and 2 to learn a model\nof natural variation corresponding to a given subset of CURE-TSR. We then train classiﬁers using\nMRT-10 with labeled challenge-level 0 data. We also train classiﬁers using ERM and PGD using\nthe labeled data from challenge-levels 0 and 2. To denote the fact that ERM and PGD are trained\nusing an augmented dataset containing labeled challenge-level 2 data in addition to challenge-level\n0 data, we denote these algorithms in Table 2 by ERM+Aug and PGD+Aug. We then test all classiﬁers\non data from challenge-levels 3, 4, and 5. Note that in some sense this is an unfair comparison for\nour methods, given that the model-based algorithms are not given access to labeled challenge-level\n2 data. In spite of this unfair comparison, our algorithms still outperform the baseline algorithms.\nBy considering each row in 2, a general pattern emerges. When tested on challenge-level 3 and\n4 data, MRT improves over ERM+Aug and PGD+Aug, although in some cases the improvements\nare somewhat modest. These modest improvements are due in part to the fact that challenge-level 3\ndata has only slightly more natural variation than challenge-level 2 data, which all of the classiﬁers\nhave seen in either labeled form (in the case of the baselines) or unlabeled form (in the case of MRT).\nHowever, by comparing the performance of the trained classiﬁers on challenge-level 5 data, it is\nclear that MRT improves signiﬁcantly over the baselines by as much as 20 percentage points. This\nshows that as the challenge-level becomes more severe, our model-based algorithms outperform\nthe baselines by relatively larger margins.\n26\nModel dataset\n(classes 0-9)\nTraining dataset\n(classes 10-59)\nTest dataset\n(classes 10-59)\nTest accuracy (top-1/top-5)\nERM\nAugMix\nMDA\nIN-c Snow\nImageNet\nIN-c Snow\n20.9\n49.9\n1.10\n8.3\n31.1\n61.2\nIN-c Contrast\nIN-c Contrast\n41.1\n73.4\n0.72\n6.76\n50.0\n79.5\nIN-c Brightness\nIN-c Brightness\n26.9\n59.2\n0.56\n5.20\n53.0\n81.7\nIN-c Fog\nIN-c Fog\n8.7\n29.5\n29.1\n57.8\n24.6\n50.8\nIN-c Frost\nIN-c Frost\n16.3\n39.0\n29.5\n58.4\n36.0\n67.2\nTable 3: ImageNet to ImageNet-c robustness. In each experiment, we train a model of natural\nvariation to map from classes 0-9 of ImageNet to the same classes from a subset of ImageNet-c.\nNext, we use this model to perform model-based training on classes 10-59 of ImageNet, and we test\neach network on classes 10-59 from the same subset ImageNet-c on which the model was trained.\nTherefore, despite the fact that each model of natural variation G(x, δ) is trained on classes that are\nentirely distinct from the classes considered when training and testing the classiﬁer, the model G\nstill provides signiﬁcant levels of robustness against the shift from ImageNet to ImageNet-c.\n6.2\nModel-based robustness on the shift from ImageNet to ImageNet-c\nTo demonstrate the scalability of our approach, we perform experiments on ImageNet [81] and\nthe recently-curated ImageNet-c dataset [15]. ImageNet-c contains images from the ImageNet test\nset that are corrupted according to artiﬁcial transformations, such as snow, rain, and fog, and are\nlabeled from 1-5 depending on the severity of the corruption. For brevity, in Table 3, we abbreviate\nImageNet-c to “IN-c.”\nFor numerous challenging corruptions, we train models to map from the classes 0-9 of ImageNet\nto the corresponding classes of ImageNet-c. We then train all classiﬁers, each of which uses the\nResNet50 architecture [41], on classes 10-59 of ImageNet, and test on the corresponding classes\nfor various subsets of ImageNet-c. Note that in this setting, the ImageNet classes used to train\nthe model of natural variation are disjoint from those that are used to train the classiﬁer, so\nmany techniques, including most domain adaptation methods, do not apply. To offer a point of\ncomparison, we include the accuracies of classiﬁers trained using AugMix, which is a recently\nproposed method that adds known transformations to the data [39].\nBy considering Table 3, the ﬁrst notable observation is that the classiﬁers trained using ERM\nperform very poorly on the shift from ImageNet to ImageNet-c. That is, when evaluating ResNet50\nclassiﬁers trained on ImageNet on the original ImageNet test set, one would expect to attain a peak\ntop-1 accuracy of more than 80 percentage points [83]; therefore, the drop in classiﬁcation accuracy\nfor the classiﬁers trained using ERM is remarkable given that the corruptions included in this table\nare quite common. This demonstrates the degree to which classiﬁers trained on ImageNet lack\nrobustness to common corruptions and transformations of data [15]. To this end, the authors of\n[39] sought to address this fragility by introducing the AugMix algorithm, which adds randomly\ncorrupted data to augment the training dataset. As shown in Table 3, while this method does\nimprove accuracy on two challenges, for other corruptions, the top-1 and top-5 accuracies plummet.\nOn the other hand, in almost all cases that we considered, MDA-3 signiﬁcantly outperformed both\nmethods, in some cases improving by nearly 30 percentage points in top-1 accuracy.\n27\nDataset\nChallenge 1\n(dom. A1→dom. B1)\nChallenge 2\n(dom. A2→dom. B2)\nTest acc. (top-1)\nERM\nMDA\nSVHN\nBrightness (low→high)\nContrast (low→high)\n54.9\n67.2\nImageNet &\nImageNet-c\nBrightness (low→high)\nContrast (high→low)\n13.6\n49.9\nBrightness (low→high)\nSnow (no→yes)\n53.3\n58.3\nBrightness (low→high)\nFog (no→yes)\n50.3\n58.8\nContrast (high→low)\nFog (no→yes)\n8.40\n23.2\nTable 4: Composing models of natural variation. We consider distributional shifts corresponding\nto two distinct and simultaneous sources of natural variation. To perform model-based training,\nwe ﬁrst train two models of natural variation G1(x, δ) and G2(x, δ) separately using unlabeled data\nto describe two distinct sources of natural variation. We then compose these models to form a new\nmodel G(x, δ) := G1(G2(x, δ), δ), which can vary the natural conditions in a given input image x\naccording to both of the sources of natural variation modeled by G1 and G2.\n6.3\nRobustness to simultaneous distributional shifts\nIn practice, it is common to encounter multiple simultaneous distributional shifts. For example,\nin image classiﬁcation, there may be shifts in both brightness and contrast; yet while there may\nbe examples corresponding to shifts in either brightness or contrast in the training data, there\nmay not be any examples of both shifts occurring simultaneously. To address this robustness\nchallenge, for each row of Table 4, we learn two models of natural variation G1(x, δ) and G2(x, δ)\nusing unlabeled training data corresponding to two separate shifts, which map domains A1→B1\n(e.g. low- to high-brightness) and A2→B2 (e.g. low- to high-contrast). We then compose these\nmodels to form a new model\nG(x, δ) = G1(G2(x, δ), δ)\n(6.1)\nwhich can be used to vary the natural conditions in a given input image x according to both sources\nof natural variation modeled by G1 and G2. We then train classiﬁers on labeled data from A1 ∪A2\n(e.g. data with either low-brightness or low-contrast) and test on data from B1 ∩B2 (e.g. data with\nboth high-brightness and high-contrast).\nTo gather data from B1 ∩B2 containing images with high-brightness and high-contrast from\nSVHN, we threshold the SVHN training set according to both of these sources of natural variation.\nOn the other hand, to create the data from B1 ∩B2 for the ImageNet experiments, we curate four new\ndatasets by applying pairs of transformations that were originally used to create the ImageNet-c\ndatasets. Images corresponding to these dataset, which contain shifts in brightness and contrast,\nbrightness and snow, brightness and fog, and contrast and fog, as well as more details concerning\nhow they were curated are available in Appendix C.\nThe results in Table 4 reveal that across each of the settings we considered, MDA-3 signiﬁcantly\noutperformed ERM with respect to top-1 classiﬁcation accuracy. Despite the fact that neither\nalgorithm has access to data from domain B1 ∩B2 during training, MDA is able to improve by\nbetween 5 and 35 percentage points over ERM across these ﬁve settings. The composable nature of\nour methods is unique to our framework; we plan to explore this feature further in future work.\n28\nTraining\ndataset D1\nTest\ndataset D2\nChallenge\n(dom. A→dom. B)\nTest accuracy (top-1)\nERM\nPGD\nMRT\nMDA\nMAT\nMNIST\nFashion-\nMNIST\nBackground\ncolor\n(blue→red)\n69.3\n67.7\n81.4\n80.1\n76.1\nQ-MNIST\n87.0\n79.9\n98.0\n98.0\n98.0\nE-MNIST\n63.5\n49.3\n86.1\n85.9\n84.1\nK-MNIST\n47.9\n47.7\n89.1\n89.3\n86.8\nUSPS\n89.9\n87.4\n93.3\n93.4\n91.9\nSVHN\nMNIST-m\nDecolorization (no→yes)\n76.1\n75.3\n77.1\n78.3\n79.2\nGTSRB\nCURE-TSR\nBrightness (high→low)\n47.6\n43.6\n73.0\n72.4\n67.8\nImageNet &\nCURE-TSR\nSnow (no→yes)\n52.0\n53.0\n59.4\n62.2\n59.4\nImageNet-c\nBrightness (low→high)\n41.5\n40.2\n46.6\n46.7\n47.5\nTable 5: Transferability of model-based robustness. In each experiment, we train a model of\nnatural variation on a given training dataset D1. Then, we use this model to perform model-based\ntraining on a new dataset D2 entirely unseen during the training of the model. This shows that\nonce they have been trained, models of natural variation can be used to improve robustness in new\napplications without the need for retraining.\n6.4\nTransferability of model-based robustness\nBecause we learn models of natural variation G(x, δ) ofﬂine before training a classiﬁer, our\nparadigm can be applied to domains that are entirely unseen while training the model G. In\nparticular, we show that models can be reused on similar yet unseen datasets to provide robustness\nagainst a common source of natural variation. For example, one might have access to two domains\ncorresponding to the shift from images of European street signs taken during the day to images\ntaken at night. However, one might wish to provide robustness against the same shift from daytime\nto nighttime on a new dataset of American street signs without access to any nighttime images in\nthis new dataset. Whereas many techniques, including most domain adaptation methods, do not\napply in this scenario, in the model-based robust deep learning paradigm, we can simply learn a\nmodel of natural variation G(x, δ) corresponding to the changes in lighting for the European street\nsigns and then apply this model to the dataset of the American signs.\nTable 5 shows several experiments of this stripe in which a model of natural variation G(x, δ)\nis learned on one dataset D1 and then applied on another dataset D2. Notably, classiﬁers trained\nusing our model-based algorithms signiﬁcantly outperform the ERM and PGD baselines in each\ncase, despite the fact that all classiﬁers have access to the same training data from dataset D2. In\nparticular, by using models of natural variation trained on D1, and then training in the model-based\nparadigm on D2, we are able to improve the test accuracy on the shift from domain A to domain B\non D2 by as much as 40 percentage points. This demonstrates that models of natural variation can\nbe reused to provide robustness against domains that are entirely unseen during the training of the\nmodel of natural variation.\n29\nDataset\nChallenge\n(dom. A→dom. B)\nTest accuracy (top-1)\nERM\nPGD\nADDA\nMRT\nMDA\nMAT\nSVHN\nBrightness (low→high)\n30.5\n36.2\n60.1\n70.9\n69.5\n52.2\nContrast (low→high)\n55.9\n57.9\n54.6\n74.3\n74.1\n55.2\nGTSRB\nBrightness (low→high)\n40.3\n34.7\n27.6\n50.4\n48.3\n64.8\nContrast (low→high)\n44.5\n41.9\n14.7\n68.4\n69.4\n55.1\nCURE-TSR\nSnow (no→yes)\n52.0\n53.0\n16.1\n74.0\n74.5\n72.3\nHaze (no→yes)\n57.2\n50.9\n49.2\n72.5\n70.0\n74.6\nRain (no→yes)\n62.6\n62.3\n16.5\n75.2\n73.7\n75.3\nTable 6: Model-based training in the setting of unsupervised domain adaptation. In each exper-\niment, we assume access to labeled data from domain A as well as unlabeled data from domain\nB; we use (unlabelled) data from both domains to train a model of natural variation G(x, δ) for\neach row of this table. We then train classiﬁers on labeled data from domain A, and test classiﬁers\non test data from domain B. We compare to suitable baselines, including the domain adaptation\nmethod ADDA, which uses the labelled domain A data as well as the unlabelled domain B data.\n6.5\nModel-based robust deep learning for unsupervised domain adaptation\nThe results of Sections 6.1, 6.2, 6.3, and 6.4 show that our approach does not require labelled or\nunlabelled data from domain B to improve robustness on the shift from domain A to domain\nB. In particular, in Section 6.1, we trained classiﬁers on low levels of natural variation and then\nevaluated these classiﬁers on higher levels of natural variation. Next, in Section 6.2 we used a\ndifferent distribution of classes to train a model of natural variation G(x, δ) than we used to train\nand evaluate classiﬁers. Further, in Section 6.3, we assumed access to data corresponding to two\nﬁxed distributional shifts, but not to the data corresponding to both shifts occurring simultaneously.\nFinally, in Section 6.4, we trained models of natural variation G(x, δ) on a different dataset D1 from\nthe dataset D2 used to train and evaluate classiﬁers.\nHowever, when unlabelled data corresponding to a ﬁxed domain shift from domain A to\ndomain B is available, it is of interest to evaluate how our approach compares to relevant methods\nsuch as domain adaptation. We emphasize that while this is one of the most commonly studied\nsettings in domain adaptation, it represents only one particular setting to which the model-based\nrobust deep learning paradigm can be applied.\nIn Table 6, for each shift from domain A to B, we assume access to labeled data from domain A\nand unlabeled data from domain B. In each row, we use unlabeled data from both domains to train\na model of natural variation G(x, δ). We then train classiﬁers using our algorithms, ERM, and PGD\nusing data from domain A; we then evaluate these classiﬁers on data from the test set of domain B.\nFurthermore, we compare to ADDA, which is a well-known domain adaptation method [82]. In\nevery scenario, our model-based algorithms signiﬁcantly outperform the baselines, often by 10-20\npercentage points. Notably, while ADDA offers strong performance on SVHN, it fairs signiﬁcantly\nworse on GTSRB and CURE-TSR.\n30\n7\nDiscussion\nFollowing the experiments of Section 6, we now discuss further aspects of model-based training. In\nparticular, we provide an additional experiment which shows that more accurate models of natural\nvariation engender classiﬁers that are more robust against natural variation in our paradigm. We\nalso discuss choices for the integer parameter k in each of the model-based training algorithms.\nFurthermore, we provide details concerning how we chose to deﬁne the nuisance spaces ∆for\nlearned models of natural variation used in the experiments of Section 6.\n7.1\nImpact of model quality\nAn essential yet so far undiscussed piece of the efﬁcacy of our model-based paradigm is the impact\nof the quality of learned models of natural variation on the robustness we are ultimately able to\nprovide. In scenarios where we do not have access to a known model of natural variation, the\nability to provide any sort of meaningful robustness relies on learned models that can accurately\nrender realistic looking data consistent with various sources of natural variation. To this end, it\nis reasonable to expect that models that can more effectively render realistic yet challenging data\nshould result in classiﬁers that are more robust to shifts in natural variation.\nTo examine the impact of the quality of learned models of natural variation in our paradigm, we\nconsider a task performed in Section 6.5, wherein we learned a model of natural variation G(x, δ)\nthat mapped low-contrast samples from SVHN, which comprised domain A, to high-contrast\nsamples from SVHN, which comprised domain B. While learning this model, we saved snapshots\nat various points during the training procedure. In particular, we collected a family of intermediate\nmodels G, where the index refers to the number of MUNIT training iterations that were completed\nbefore saving the snapshot:\nG =\nn\nG10, G100, G250, G500, G1000, G2000, G3000, G4000\no\nIn Figure 8j, we show the result of training classiﬁers with MRT-10 using each model of natural\nvariation G ∈G. Note that the models that are trained for more training steps engender classiﬁers\nthat provide higher levels of robustness against the shift in natural variation. Indeed, as the\nmodel G10 produces random noise, the performance of this classiﬁer performs at effectively the\nlevel as the baseline classiﬁer (shown in Table 6). On the other hand, the model G4000 is able to\naccurately preserve the semantic content of the input data while varying the nuisance content,\nand is therefore able to provide higher levels of robustness. In other words, better models induce\nimproved robustness for classiﬁers trained in the model-based paradigm.\nWhile this experiment demonstrates that models of natural variation G(x, δ) which generate\nmore realistic data ultimately lead to classiﬁers that are more robust against out-of-distribution\nshifts, further questions remain. In particular, in our setting, as learned models of natural variation\ncritically rely on the ability of deep generative models to render realistic images, there is no guarantee\nthat learned models will always accurately reﬂect natural conditions. Indeed, it remains an open\nproblem as to how to ensure that deep generative models generalize effectively [84]. To this end,\none promising direction for future work is to study the conditions under which reliable models of\nnatural variation can be learned.\n31\n(a) Original.\n(b) 10.\n(c) 100.\n(d) 250.\n(e) 500\n(f) 1000.\n(g) 2000.\n(h) 3000.\n(i) 4000.\nOutput images from models in G.\nWe\nshow an example image from domain A\nin (a), and subsequently show the corre-\nsponding output images for each G ∈G for\na randomly chosen δ ∈∆in (b)-(i).\n0\n20\n40\n60\n80\n100\n120\n140\nClassifier training epochs\n20\n30\n40\n50\n60\nTest accuracy\n10\n100\n250\n500\n1000\n2000\n3000\n4000\n(j) MRT using models from G. For each model in G, we run\nMRT-10 for ﬁve trials and show the resulting test accuracy\non samples from the test set from domain B. Note that the\nrobustness of the trained classiﬁer increases as the number\nof training steps used to train the model increases.\nFigure 8: A better model implies more robustness. By learning a family of models G, where each\nmodel of natural variation G(x, δ) in G has been trained for a different number of iterations, we\nshow empirically that models that can more accurately reconstruct input data subject to varying\nnatural conditions engender classiﬁers with higher levels of robustness.\n7.2\nAlgorithm and hyperparameter selection criteria\nSampling versus adversarial perspective. From an optimization perspective, we can group our\nmodel-based algorithms into two categories: sampling (zeroth-order) methods and adversarial (ﬁrst-\norder) methods. Sampling-based methods refer to those that seek to solve the inner maximization\nproblem in equation (3.2) by querying the model of natural variation G(x, δ). This is particularly\nimportant for models that are not differentiable. Both MRT and MDA are sampling-based (zeroth-\norder) methods in that they vary data by randomly sampling different nuisance parameters δ ∈∆\nfor each batch in the training set. On the other hand, MAT varies data via an adversarial (ﬁrst-order)\nmethod, wherein we statistically approximate the gradient ∇δE(x,y)∼D[ℓ(G(x, δ), y; w)] to perform\nthe optimization; that is, we search for the worst-case nuisance parameter δ ∈∆. Importantly, we\nnote that while differentiability is not required in our paradigm, differentiability is required for\ntraining classiﬁers via MAT.\nThroughout the experiments, in general we see that the sampling-based algorithms presented\nin this paper achieve higher levels of robustness against almost all sources of natural variation. This\nﬁnding stands in contrast to ﬁeld of perturbation-based robustness, in which adversarial methods\nhave been shown to be the most effective in improving the robustness against small, norm-bounded\nperturbations [71]. Going forward, an interesting research direction is not only to consider new\nalgorithms but also to understand whether sampling-based or adversarial techniques provide more\n32\nk\nTest accuracy (top-1)\nMAT-k\nMRT-k\nMDA-k\n1\n81.9\n81.2\n81.7\n5\n75.6\n81.7\n81.1\n10\n75.7\n82.6\n81.1\n20\n79.9\n83.2\n80.3\n50\n76.3\n82.5\n79.7\nTable 7: The impact of varying k in the model-based algorithms. We study the impact of varying\nk for each of the model-based algorithms on the brightness (low→high) shift on SVHN.\nrobustness with respect to a given model of natural variation.\nThe impact of k in the model-based training algorithms. The discussion surrounding the differ-\nence between the sampling-based and adversarial mindsets that characterize our model-based\nalgorithms is intimately related to the parameter k in each of the model-based training algorithms.\nNotably, in each of these algorithms, the parameter k serves distinct yet related purposes. In MRT, k\ncontrols the number of nuisance parameters δ that are sampled at each iteration. Similarly, in MAT,\nk determines the number of steps of gradient ascent performed at each iteration. On the other hand,\nin MDA, k controls the number of data points that are added to the training set at each iteration.\nAs we show in our experiments, each of the three model-based algorithms can be used to\nprovide signiﬁcant out-of-distribution robustness against various sources of natural variation. In\nthis subsection, we focus on the impact of varying the parameter k in each of these algorithms. In\nparticular, in Table 7, we see that varying k has a different impact for each of the three algorithms.\nFor MAT, we see that increasing k decreases the accuracy of the trained classiﬁer; one interpretation\nof this phenomenon is that larger values of k allow MAT to ﬁnd more challenging forms of\nnatural variation. On the other hand, the test accuracy of MRT improves slightly as k increases.\nRecall that while both MRT and MAT seek to ﬁnd “worst-case” natural variation, MRT employs\na sampling-based approach to solving the inner maximization problem as opposed to the more\nprecise, gradient-based procedure used by MAT. Thus the differences in the impact of varying\nk between MAT and MRT may be due to the fact that MRT only approximately solves the inner\nproblem at each iteration. Finally, we see that increasing k slightly decreases the test accuracy of\nclassiﬁers trained with MDA.\nThis study can also be used as an algorithm selection criteria. Indeed, when data presents many\nmodes corresponding to different levels of natural variation, it may be more efﬁcacious to use MRT\nor MDA, which will observe a more diverse set of natural conditions due to their sampling-based\napproaches. On the other hand, when facing a single challenging source of natural variation, it may\nbe more useful to use MAT, which seeks to ﬁnd “worst-case,” natural, out-of-distribution data.\nThe dimension and radius of the nuisance space ∆. Given a ﬁxed instance x ∈Rd, ∆character-\nizes the set of images that can be obtained under the mapping of a model of natural variation\nG(x, δ). In Section 3.3, we described a geometric perspective that allowed us to rewrite the inner\nmaximization in (3.2) implicitly in terms of the learned image manifold. This representation of the\ninner maximization problem elucidated the fact that ∆must be rich enough to be able to produce\nrepresentative images on the learned image manifold parameterized by a model of natural variation\n33\nG(x, δ). However, we note that in the extreme case when dim(∆) = d, as is the case in much of the\nadversarial robustness literature, it is well known that δ is difﬁcult to efﬁciently optimize over [22].\nTherefore, the dimension of ∆, henceforth denoted as dim(∆), should be small enough so that ∆\ncan be efﬁciently optimized over and large enough so that it can accurately capture the underlying\nsource of natural variation that the model G(x, δ) describes. In this sense, dim(∆) should reﬂect\nthe complexity of both the source of natural variation and indeed of the data itself.\nTo this end, throughout our experiments, we generally scale dim(∆) with d. For low-dimensional\ndata (e.g. MNIST, SVHN, etc.), we found that dim(∆) = 2 sufﬁced toward capturing the underlying\nsource of natural variation effectively. However, on datasets such as GTSRB, for which we rescaled\ninstances into 64 × 64 × 3 arrays, we found that dim(∆) = 8 was more appropriate for capturing the\nfull range of natural variation. Indeed, on ImageNet, which contains instances of size 224 × 224 × 3,\nwe found that dim(∆) = 8 still produced images that captured the essence of the underlying source\nof natural variation.\nHaving studied different choices for the dimension of ∆, a concomitant question is how to pick\nthe radius of ∆. In every experiment described in this paper, we let ∆:= {x ∈Rq| −1 ⪯x ⪯1}\nwhere q = dim(∆). This choice is not fundamental, and indeed we plan to explore varying this\nradius in future work. In particular, rather than restricting ∆to be compact, one could imagine\nplacing a distribution over the space of nuisance parameters.\n8\nRelated works\nIn this section, we attempt to characterize works from a variety of ﬁelds, including adversarial\nrobustness, generative modeling, equivariant neural networks, and domain adaptation, which\nhave been inﬂuential in the development of the model-based robust deep learning paradigm.\n8.1\nPerturbation-based adversarial robustness\nA rapidly growing body of work has addressed adversarial robustness of neural networks with\nrespect to small norm-bounded perturbations. This problem has motivated an arms-race-like\namalgamation of adversarial attacks and defenses within the scope of norm-bounded adversaries\n[71, 85] and has prompted researchers to closely study the theoretical properties of adversarial\nrobustness [86, 87, 88, 89, 90]. And while some defenses have withstood a variety of strong\nadversaries [22], it remains to be seen as to whether such progress will ultimately lead to deep\nlearning models that are reliably robust against adversarial, perturbation-based attacks.\nSeveral notable works that propose methods for defending against adversarial attacks formulate\nso-called adversarial training algorithms, the goal of which is to defend neural networks against\nworst-case perturbations [22, 23, 24, 25, 26, 27, 28]. Some of the most successful works take a robust\noptimization perspective, in which the goal is to ﬁnd the worst-case adversarial perturbation of\ndata by solving a min-max problem [22, 23]. However, it has also been shown that randomized\nsmoothing based defenses are able to withstand strong attacks [91, 92]. In a different yet related\nline of work, optimization-based methods have been proposed to provide certiﬁable guarantees\non the robustness of neural networks against small perturbations [29, 30, 31, 93]. Others have also\nstudied how different architectural choices can result in classiﬁers that are more robust against\nadversarial examples [94, 95].\n34\nAs adversarial training methods have become more sophisticated, a range of adaptive adversar-\nial attacks, or attacks speciﬁcally targeting a particular defense, have been proposed [85]. Prominent\namong the attacks on robustly-trained classiﬁers have been algorithms that circumvent so-called\nobfuscated gradients [71, 96]. Such attacks generally focus on generating adversarial examples that\nare perceptually similar to a given input image [97, 98]. Very recently, the authors of [99] developed\nthe notion of a “perceptual adversarial attack,” in which an adversary can corrupt a given input\nimage subject to a learned “neural perceptual distance.” As far as the authors are aware, this is the\nstrongest imperceptible adversarial attack for trained classiﬁers.\nIn summary, the commonality among all the approaches mentioned above is that they generally\nconsider norm-bounded adversarial perturbations which are perceptually indistinguishable from\ntrue examples. Contrary to these approaches, in this work we propose a paradigm shift from\nnorm-bounded perturbation-based robustness to model-based robustness. To this end, we have\nprovided training algorithms that improve robustness against natural shifts in the data distribution;\nsuch changes are often perceptually distinct from input data, as is the case for varying lighting or\nweather conditions.\n8.2\nGenerative models in the context of robustness\nAnother line of work has sought to leverage deep generative models in the loop of training to\nfacilitate strategies for generating and defending against adversarial examples. In [100], [101], and\n[102], the authors propose attack strategies that use the generator from a generative adversarial\nnetwork (GAN) to generate additive perturbations that can be used to attack a classiﬁer. On\nthe other hand, a framework called DefenseGAN, which uses a Wasserstein GAN to “denoise”\nadversarial examples [103], has been proposed to defend against perturbation-based attacks. This\ndefense method was later broken by the Robust Manifold Defense [104], which searches over\nthe parameterized manifold induced by a generative model to ﬁnd worst-case perturbations of\ndata. The min-max formulation used in this work is analogous to the PGD defense [22], and was\nfoundational in our development of the model-based robust deep learning paradigm.\nCloser to the approach we describe in this paper are works that use deep generative models to\ngenerate adversarial inputs themselves, rather than generating small perturbations. The authors of\n[105] and [106] use the generator from a GAN to generate adversarial examples that obey Euclidean\nnorm-based constraints. Alternatively, the authors of [107] use GANs to generate adversarial\npatterns that can be used to construct adversarial examples in multiple domains. Similarly, [108]\nand [109] generate unrestricted adversarial examples, or instances that are not subjected to a norm-\nbased constraint [110], via a deep generative model. Finally, two recent papers [111, 112] use a\nGAN to perform data-augmentation by generating perceptually realistic samples.\nIn this work, we use generative networks to learn and model the natural variability within\ndata. This is indeed different than generating norm-based adversarial perturbations or perceptually\nrealistic adversarial examples, as have already been considered in the literature. Our generative\nmodels are designed to capture natural shifts in data, whereas the relevant literature has sought to\ncreate synthetic adversarial perturbations that fool neural networks.\n8.3\nA broader view of robustness in deep learning\nAside from the algorithms we introduced in Section 5, we are not aware of any other algorithms that\ncan be used to address out-of-distribution robustness across the diverse array of tasks presented in\n35\nSection 6. However, several lines of research have sought to address this problem in constrained\nsettings or under highly restrictive assumptions. Of note are works that have sought to provide\nrobustness against speciﬁc transformations of data that are more likely to be encountered in appli-\ncations than norm-bounded perturbations. Transformations that have recently received attention\nfrom the adversarial robustness community include adversarial quilting [113], adversarial patches\nand clothing [114], geometric transformations [49, 50, 51, 115], distortions [116], deformations and\nocclusions [117], and nuisances encountered by unmanned aerial vehicles [118]. In response to\nthese works and motivated by myriad safety-critical applications, ﬁrst steps toward robust defenses\nagainst speciﬁc distributional shifts have recently been proposed [108, 110, 111, 112]. The resulting\nmethodologies generally leverage properties speciﬁc to the transformation of interest.\nAnother line of work has sought to use various mechanisms to create semantically-realistic\nadversarial examples via more general frameworks. For example, in [44], the authors seek to\ncreate “semantic adversarial examples” via color-based shifts, which relates to the perspective\nadvocated for in [119], in which the authors argue that system semantics and speciﬁcations should\nbe considered when generating meaningful disturbances in the data. Similarly, in [120], the authors\nuse differentiable renderers to generate semantically meaningful changes. In the same spirit, the\nidea in [121] is to leverage an information theoretic approach to edit the nuisance content of images\nto create perceptually realistic data that causes misclassiﬁcation.\nWhile this progress has helped to motivate new notions of robustness, defenses against speciﬁc\nthreat models are limited in the sense that they often cannot be generalized to develop a learning\nparadigm that is broadly applicable across different forms of natural variation. This contrasts with\nthe motivation behind this paper, which is to provide general robust training algorithms that can\nimprove the robustness of trained neural networks across a variety of scenarios and applications.\nMore related to the current work are two concurrent papers that formulate robust training\nprocedures under the assumption that data is corrupted according to a ﬁxed generative architecture.\nThe authors of [122] exploit properties speciﬁc to the StyleGAN [123] architecture to formulate a\ntraining algorithm that provides robustness against color-based shifts on MNIST and CelebA [124].\nIn our work, we propose a more general framework and three novel robust training algorithms\nthat can exploit any suitable generative model, and we show improvements on more challenging,\nnaturally-occurring shifts across twelve distinct datasets. The authors of [125] use conditional\nVAEs to learn perturbation sets corresponding to simple corruptions from pairs of images. In\nour framework we improve robustness against more challenging, natural shifts by learning from\nunpaired datasets and we do not rely on class-conditioning to generate realistic images.\n8.4\nDomain adaptation and domain generalization\nIn the domain adaptation literature, various methods have been proposed which rely on the restric-\ntive assumption that unlabeled data corresponding to a ﬁxed distributional shift is available during\ntraining [126, 127, 128, 129]. Several works in this vain use an adversarial min-max formulation to\nadapt a classiﬁer trained on a source domain to perform well on a related target domain, for which\nlabels are unavailable at training time [82, 130, 131, 132]. We note that these “adversarial” methods\ndiffer from the model-based paradigm we introduce in that rather than formulating the problem\nof adapting a classiﬁer to perform well on the target domain, we employ a min-max procedure\nto search for worst-case shifts in data for a ﬁxed generative model. Indeed, the main difference\nbetween domain adaptation techniques and our paradigm is that our solution does not assume\naccess to unlabeled data from a ﬁxed shift and can be applied to datasets that are entirely unseen\n36\nduring training.\nAlso related is the ﬁeld of domain generalization [133, 134, 135], in which one assumes access\nto a variety of training domains, all of which are related to an unseen target domain on which\nthe trained classiﬁer is ultimately evaluated. Such works often rely on transfer learning [136] and\ndistribution matching [137] to improve classiﬁcation accuracy on the unseen domain. While the\nexperiments in Sections 6.4 tackle a similar problem, in which knowledge from one domain is used\nto learn a classiﬁer that performs well on an unseen test domain, the experiments in the other\nsubsections of Section 6 show that our model-based paradigm is much more broadly applicable\nthat techniques domain generalization techniques.\n9\nConclusion and future directions\nIn this paper, we formulated a novel problem that concerns the robustness of deep learning\nwith respect to natural, out-of-distribution shifts in data. Motivated by perceptible nuisances in\ncomputer vision, such as lighting or weather changes, we propose the novel model-based robust deep\nlearning paradigm, the goal of which is to improve the robustness of deep learning systems with\nrespect to naturally occurring conditions. This notion of robustness offers a departure from that\nof norm-bound, perturbation-based adversarial robustness, and indeed our optimization-based\nformulation results in a new family of training algorithms that can be used to train neural networks\nto be robust against arbitrary forms of natural variation. Across a range of diverse experiments and\nacross twelve distinct datasets, we empirically show that the model-based paradigm is broadly\napplicable to a variety of challenging domains.\nOur model-based robust deep learning paradigm open numerous directions for future work. In\nwhat follows, we brieﬂy highlight several of these broad directions.\nLearning a library of nuisance models. One natural future direction is to determine superior ways\nof learning models of natural variations to perform model-based training. In this paper, we used\nthe MUNIT framework [4], but other existing architectures may be better suited for speciﬁc forms\nof natural variation or for different datasets. Indeed, a more rigorous statistical analysis of problem\n(4.4) may lead to the discovery of new architectures designed speciﬁcally for model-based training.\nTo this end, recent work that concerns learning invariances in neural networks may provide insight\ninto learning physically meaningful models [138, 139]. Beyond computer vision, learning such\nmodels in other domains such as robotics would enable new applications.\nModel-based algorithms and architectures. Another important direction involves the develop-\nment of new algorithms for solving the min-max formulation of (3.1). In this paper, we presented\nthree algorithms – MRT, MDA, and MAT – that can be used to approximately solve this problem,\nbut other algorithms are possible and may result in higher levels of robustness. In particular,\nadapting ﬁrst-order methods to search globally over the learned image manifold may provide\nmore efﬁcient, scalable, or robust results. Indeed, one open question is whether it is necessary\nto decouple the procedures used to learn classiﬁers and models of natural variation. Another\ninteresting direction is to follow the line of work that has developed equivariant neural network\narchitectures toward designing new architectures that are invariant to models of natural variation.\nApplications beyond image classiﬁcation. Throughout the paper, we have empirically demon-\nstrated the utility of our approach across many image classiﬁcation tasks. However, our model-\n37\nbased paradigm could also be broadly applied to further applications, both in computer vision\nas well as outside computer vision. Within computer vision, one can consider other tasks such as\nsemantic segmentation in the presence of challenging natural conditions. Outside computer vision,\none exciting area is to exploit physical models of robot dynamics with deep reinforcement learning\nfor applications such as walking in unknown terrains. In any domain where one has access to\nsuitable models of natural variation, our approach allows domain experts to leverage these models\nin order to make deep learning far more robust.\nTheoretical foundations. Finally, we believe that there are many exciting open questions with\nrespect to the theoretical aspects of model-based robust training. What types of models provide\nsigniﬁcant robustness gains in our paradigm? How accurate does a model need to be to engender\nneural networks that are robust against natural variation and out-of-distribution shifts? We would\nlike to address such theoretical questions from geometric, physical, and statistical perspectives with\nan eye toward developing faster algorithms that are both more sample-efﬁcient and that provide\nhigher levels of robustness. A deeper theoretical understanding of our model-based robust deep\nlearning paradigm could result in new approaches that blend model-based and perturbation-based\nmethods and algorithms.\nAcknowledgements\nThis work has been partially supported by the Defense Advanced Research Projects Agency\n(DARPA) Assured Autonomy under Contract No. FA8750-18-C-0090, AFOSR under grant FA9550-\n19-1-0265 (Assured Autonomy in Contested Environments), NSF CPS 1837210, and ARL CRA\nDCIST W911NF-17-2-0181 program.\nAuthors\nAll authors are with the Department of Electrical and Systems Engineering, University of Pennsyl-\nvania, Philadelphia, PA 19104. The authors can be reached at the following email addresses:\n{arobey1, hassani, pappasg}@seas.upenn.edu.\n38\nReferences\n[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,\n2015.\n[2] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks. In Proceedings of the IEEE international\nconference on computer vision, pages 2223–2232, 2017.\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity\nnatural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n[4] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-\nto-image translation. In Proceedings of the European Conference on Computer Vision (ECCV),\npages 172–189, 2018.\n[5] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In\nAdvances in neural information processing systems, pages 3856–3866, 2017.\n[6] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In\nAdvances in neural information processing systems, pages 2017–2025, 2015.\n[7] Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, and Kostas Daniilidis. Polar\ntransformer networks. arXiv preprint arXiv:1709.01889, 2017.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[9] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[10] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining\nthe predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international conference\non knowledge discovery and data mining, pages 1135–1144, 2016.\n[11] Andre Esteva, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark De-\nPristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. A guide\nto deep learning in healthcare. Nature medicine, 25(1):24–29, 2019.\n[12] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Ré. Hidden\nstratiﬁcation causes clinically meaningful failures in machine learning for medical imaging.\nIn Proceedings of the ACM Conference on Health, Inference, and Learning, pages 151–159, 2020.\n[13] Tommaso Dreossi, Alexandre Donzé, and Sanjit A Seshia. Compositional falsiﬁcation of\ncyber-physical systems with machine learning components. Journal of Automated Reasoning,\n63(4):1031–1053, 2019.\n[14] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian\nGoodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint\narXiv:1312.6199, 2013.\n39\n[15] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to com-\nmon corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.\n[16] Xingxing Wei, Jun Zhu, Sha Yuan, and Hang Su. Sparse adversarial perturbations for videos.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 8973–8980,\n2019.\n[17] Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, and Ludwig\nSchmidt. A systematic framework for natural perturbations from videos. arXiv preprint\narXiv:1906.02168, 2019.\n[18] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao,\nAtul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep\nlearning visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 1625–1634, 2018.\n[19] Eric Wallace, Mitchell Stern, and Dawn Song. Imitation attacks and defenses for black-box\nmachine translation systems. arXiv preprint arXiv:2004.15015, 2020.\n[20] Konstantinos Papangelou, Konstantinos Sechidis, James Weatherall, and Gavin Brown. To-\nward an understanding of adversarial examples in clinical trials. In Joint European Conference\non Machine Learning and Knowledge Discovery in Databases, pages 35–51. Springer, 2018.\n[21] Marco Melis, Ambra Demontis, Battista Biggio, Gavin Brown, Giorgio Fumera, and Fabio\nRoli. Is deep learning safe for robot vision? adversarial examples against the icub humanoid.\nIn Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 751–759,\n2017.\n[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian\nVladu.\nTowards deep learning models resistant to adversarial attacks.\narXiv preprint\narXiv:1706.06083, 2017.\n[23] Eric Wong and J Zico Kolter. Provable Defenses Against Adversarial Examples Via the\nConvex Outer Adversarial Polytope. arXiv preprint arXiv:1711.00851, 2017.\n[24] Divyam Madaan and Sung Ju Hwang.\nAdversarial neural pruning.\narXiv preprint\narXiv:1908.04355, 2019.\n[25] Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, and James Storer. Deﬂect-\ning adversarial attacks with pixel deﬂection. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 8571–8580, 2018.\n[26] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I\nJordan. Theoretically principled trade-off between robustness and accuracy. arXiv preprint\narXiv:1901.08573, 2019.\n[27] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical\nworld. arXiv preprint arXiv:1607.02533, 2016.\n40\n[28] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple\nand accurate method to fool deep neural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2574–2582, 2016.\n[29] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial\nexamples. arXiv preprint arXiv:1801.09344, 2018.\n[30] Mahyar Fazlyab, Manfred Morari, and George J Pappas. Safety veriﬁcation and robustness\nanalysis of neural networks via quadratic constraints and semideﬁnite programming. arXiv\npreprint arXiv:1903.01287, 2019.\n[31] Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim,\nParth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zelji´c, et al. The marabou framework\nfor veriﬁcation and analysis of deep neural networks. In International Conference on Computer\nAided Veriﬁcation, pages 443–452. Springer, 2019.\n[32] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing\nadversarial examples. arXiv preprint arXiv:1412.6572, 2014.\n[33] Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, and Jun Zhu.\nBenchmarking adversarial robustness. arXiv preprint arXiv:1912.11852, 2019.\n[34] Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexan-\nder Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan,\net al. On robustness and transferability of convolutional neural networks. arXiv preprint\narXiv:2007.08558, 2020.\n[35] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig\nSchmidt. Measuring robustness to natural distribution shifts in image classiﬁcation. arXiv\npreprint arXiv:2007.00644, 2020.\n[36] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,\nRahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A\ncritical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.\n[37] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore: Automated whitebox\ntesting of deep learning systems. In proceedings of the 26th Symposium on Operating Systems\nPrinciples, pages 1–18, 2017.\n[38] Alesia Chernikova, Alina Oprea, Cristina Nita-Rotaru, and BaekGyu Kim. Are self-driving\ncars secure? evasion attacks against deep neural networks for steering angle prediction. In\n2019 IEEE Security and Privacy Workshops (SPW), pages 132–137. IEEE, 2019.\n[39] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lak-\nshminarayanan. Augmix: A simple data processing method to improve robustness and\nuncertainty. arXiv preprint arXiv:1912.02781, 2019.\n[40] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\n41\n[41] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[42] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization, volume 28.\nPrinceton University Press, 2009.\n[43] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural\nadversarial examples. arXiv preprint arXiv:1907.07174, 2019.\n[44] Hossein Hosseini and Radha Poovendran. Semantic adversarial examples. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1614–1619,\n2018.\n[45] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially\ntransformed adversarial examples. arXiv preprint arXiv:1801.02612, 2018.\n[46] Nikolaos Karianakis, Jingming Dong, and Stefano Soatto. An empirical evaluation of current\nconvolutional architectures’ ability to manage nuisance location and scale variability. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4442–4451,\n2016.\n[47] Mahmood Sharif, Lujo Bauer, and Michael K Reiter. On the suitability of lp-norms for creating\nand preventing adversarial examples. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops, pages 1605–1613, 2018.\n[48] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs\n[Online]. Available: http://yann. lecun. com/exdb/mnist, 2, 2010.\n[49] Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry.\nExploring the landscape of spatial robustness. arXiv preprint arXiv:1712.02779, 2017.\n[50] Mislav Balunovic, Maximilian Baader, Gagandeep Singh, Timon Gehr, and Martin Vechev.\nCertifying geometric robustness of neural networks.\nIn Advances in Neural Information\nProcessing Systems, pages 15287–15297, 2019.\n[51] Sandesh Kamath, Amit Deshpande, and KV Subrahmanyam. Invariance vs. robustness of\nneural networks. arXiv preprint arXiv:2002.11318, 2020.\n[52] Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow.\nHarmonic networks: Deep translation and rotation equivariance. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 5028–5037, 2017.\n[53] Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning\nso (3) equivariant representations with spherical cnns. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 52–68, 2018.\n[54] Taco S Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant\nconvolutional networks and the icosahedral cnn. arXiv preprint arXiv:1902.04615, 2019.\n42\n[55] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International\nconference on machine learning, pages 2990–2999, 2016.\n[56] Nicholas Guttenberg, Nathaniel Virgo, Olaf Witkowski, Hidetoshi Aoki, and Ryota Kanai.\nPermutation-equivariant neural networks applied to dynamics prediction. arXiv preprint\narXiv:1612.04530, 2016.\n[57] Beranger Dumont, Simona Maggio, and Pablo Montalvo. Robustness of rotation-equivariant\nnetworks to adversarial perturbations. arXiv preprint arXiv:1802.06627, 2018.\n[58] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\nneural information processing systems, pages 2672–2680, 2014.\n[59] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang,\nand Eli Shechtman. Toward multimodal image-to-image translation. In Advances in neural\ninformation processing systems, pages 465–476, 2017.\n[60] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning\nfor image-to-image translation. In Proceedings of the IEEE international conference on computer\nvision, pages 2849–2857, 2017.\n[61] Jack Klys, Jake Snell, and Richard Zemel. Learning latent subspaces in variational autoen-\ncoders. In Advances in Neural Information Processing Systems, pages 6444–6454, 2018.\n[62] Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip Bachman, and Aaron Courville.\nAugmented cyclegan: Learning many-to-many mappings from unpaired data. arXiv preprint\narXiv:1802.10151, 2018.\n[63] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation\nnetworks. In Advances in neural information processing systems, pages 700–708, 2017.\n[64] Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, and Qiang Qiu. Stochastic conditional genera-\ntive networks with basis decomposition. arXiv preprint arXiv:1909.11286, 2019.\n[65] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo.\nStargan: Uniﬁed generative adversarial networks for multi-domain image-to-image trans-\nlation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n8789–8797, 2018.\n[66] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image\nsynthesis for multiple domains. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8188–8197, 2020.\n[67] Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking\ngenerative adversarial networks for diverse image synthesis. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 1429–1437, 2019.\n[68] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\n43\n[69] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio\nSavarese. Generalizing to unseen domains via adversarial data augmentation. In Advances in\nNeural Information Processing Systems, pages 5334–5344, 2018.\n[70] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,\n2012.\n[71] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense\nof security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,\n2018.\n[72] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.\nReading digits in natural images with unsupervised feature learning. NIPS Workshop on Deep\nLearning and Unsupervised Feature Learning, 2011.\n[73] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The German Trafﬁc\nSign Recognition Benchmark: A multi-class classiﬁcation competition. In IEEE International\nJoint Conference on Neural Networks, pages 1453–1460, 2011.\n[74] Dogancan Temel, Min-Hung Chen, and Ghassan AlRegib. Trafﬁc sign detection under chal-\nlenging conditions: A deeper look into performance variations and spectral characteristics.\nIEEE Transactions on Intelligent Transportation Systems, 2019.\n[75] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François\nLaviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural\nnetworks. The Journal of Machine Learning Research, 17(1):2096–2030, 2016.\n[76] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for\nbenchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n[77] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending\nmnist to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN),\npages 2921–2926. IEEE, 2017.\n[78] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and\nDavid Ha. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718,\n2018.\n[79] Chhavi Yadav and Léon Bottou. Cold case: The lost mnist digits. In Advances in Neural\nInformation Processing Systems, pages 13443–13452, 2019.\n[80] Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on\npattern analysis and machine intelligence, 16(5):550–554, 1994.\n[81] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npages 248–255. Ieee, 2009.\n[82] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative do-\nmain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 7167–7176, 2017.\n44\n[83] I Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale\nsemi-supervised learning for image classiﬁcation. arXiv preprint arXiv:1905.00546, 2019.\n[84] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and\nequilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.\n[85] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive\nattacks to adversarial example defenses. arXiv preprint arXiv:2002.08347, 2020.\n[86] Edgar Dobriban, Hamed Hassani, David Hong, and Alexander Robey. Provable tradeoffs in\nadversarially robust classiﬁcation. arXiv preprint arXiv:2006.05161, 2020.\n[87] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry.\nAdversarially robust generalization requires more data. In Advances in Neural Information\nProcessing Systems, pages 5014–5026, 2018.\n[88] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Un-\nlabeled data improves adversarial robustness. In Advances in Neural Information Processing\nSystems, pages 11192–11203, 2019.\n[89] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander\nMadry. Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.\n[90] Adel Javanmard, Mahdi Soltanolkotabi, and Hamed Hassani. Precise tradeoffs in adversarial\ntraining for linear regression. arXiv preprint arXiv:2002.10477, 2020.\n[91] Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certiﬁed adversarial robustness via\nrandomized smoothing. arXiv preprint arXiv:1902.02918, 2019.\n[92] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck,\nand Greg Yang. Provably robust deep learning via adversarially trained smoothed classiﬁers.\nIn Advances in Neural Information Processing Systems, pages 11289–11300, 2019.\n[93] Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas.\nEfﬁcient and accurate estimation of lipschitz constants for deep neural networks. In Advances\nin Neural Information Processing Systems, pages 11423–11434, 2019.\n[94] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier.\nParseval networks: Improving robustness to adversarial examples. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70, pages 854–863. JMLR. org, 2017.\n[95] Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples.\nIn Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,\npages 135–147, 2017.\n[96] Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing\nten detection methods. In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and\nSecurity, pages 3–14, 2017.\n[97] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling\ndeep neural networks. IEEE Transactions on Evolutionary Computation, 23(5):828–841, 2019.\n45\n[98] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo\nLi. Boosting adversarial attacks with momentum. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 9185–9193, 2018.\n[99] Cassidy Laidlaw, Sahil Singla, and Soheil Feizi. Perceptual adversarial robustness: Defense\nagainst unseen threat models. arXiv preprint arXiv:2006.12655, 2020.\n[100] Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating\nadversarial examples with adversarial networks. arXiv preprint arXiv:1801.02610, 2018.\n[101] Hyeungill Lee, Sungyeob Han, and Jungwoo Lee. Generative adversarial trainer: Defense to\nadversarial perturbations with gan. arXiv preprint arXiv:1705.03387, 2017.\n[102] Huaxia Wang and Chun-Nam Yu. A direct approach to robust deep learning using adversarial\nnetworks. arXiv preprint arXiv:1905.09591, 2019.\n[103] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classiﬁers\nagainst adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.\n[104] Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, and Alexandros G Dimakis. The ro-\nbust manifold defense: Adversarial training using generative models.\narXiv preprint\narXiv:1712.09196, 2017.\n[105] Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel.\nTowards the ﬁrst\nadversarially robust neural network model on mnist. arXiv preprint arXiv:1805.09190, 2018.\n[106] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial examples.\narXiv preprint arXiv:1710.11342, 2017.\n[107] Muhammad Muzammal Naseer, Salman H Khan, Muhammad Haris Khan, Fahad Shahbaz\nKhan, and Fatih Porikli. Cross-domain transferability of adversarial perturbations. In\nAdvances in Neural Information Processing Systems, pages 12885–12895, 2019.\n[108] Isaac Dunn, Tom Melham, and Daniel Kroening. Generating realistic unrestricted adversarial\ninputs using dual-objective gan training. arXiv preprint arXiv:1905.02463, 2019.\n[109] Xiaosen Wang, Kun He, Chuan Guo, Kilian Q Weinberger, and John E Hopcroft. At-gan: A\ngenerative attack model for adversarial transferring on generative adversarial nets. arXiv\npreprint arXiv:1904.07793, 2019.\n[110] Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversar-\nial examples with generative models. In Advances in Neural Information Processing Systems,\npages 8312–8323, 2018.\n[111] Simon Vandenhende, Bert De Brabandere, Davy Neven, and Luc Van Gool. A three-player\ngan: generating hard samples to improve classiﬁcation networks. In 2019 16th International\nConference on Machine Vision Applications (MVA), pages 1–6. IEEE, 2019.\n[112] Vinicius F Arruda, Thiago M Paixão, Rodrigo F Berriel, Alberto F De Souza, Claudine Badue,\nNicu Sebe, and Thiago Oliveira-Santos. Cross-domain car detection using unsupervised\nimage-to-image translation: From day to night. In 2019 International Joint Conference on Neural\nNetworks (IJCNN), pages 1–8. IEEE, 2019.\n46\n[113] Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering\nadversarial images using input transformations. arXiv preprint arXiv:1711.00117, 2017.\n[114] Zuxuan Wu, Ser-Nam Lim, Larry Davis, and Tom Goldstein. Making an invisibility cloak:\nReal world adversarial attacks on object detectors. arXiv preprint arXiv:1910.14667, 2019.\n[115] Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Geometric robustness of\ndeep networks: analysis and improvement. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 4441–4449, 2018.\n[116] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust\nadversarial examples. arXiv preprint arXiv:1707.07397, 2017.\n[117] Xiaolong Wang, Abhinav Shrivastava, and Abhinav Gupta. A-fast-rcnn: Hard positive\ngeneration via adversary for object detection. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2606–2615, 2017.\n[118] Zhenyu Wu, Karthik Suresh, Priya Narayanan, Hongyu Xu, Heesung Kwon, and Zhangyang\nWang. Delving into robust object detection from unmanned aerial vehicles: A deep nuisance\ndisentanglement approach. In Proceedings of the IEEE International Conference on Computer\nVision, pages 1201–1210, 2019.\n[119] Tommaso Dreossi, Somesh Jha, and Sanjit A Seshia. Semantic adversarial deep learning. In\nInternational Conference on Computer Aided Veriﬁcation, pages 3–26. Springer, 2018.\n[120] Lakshya Jain, Wilson Wu, Steven Chen, Uyeong Jang, Varun Chandrasekaran, Sanjit Seshia,\nand Somesh Jha. Generating semantic adversarial examples with differentiable rendering.\narXiv preprint arXiv:1910.00727, 2019.\n[121] Jörn-Henrik Jacobsen, Jens Behrmann, Richard Zemel, and Matthias Bethge. Excessive\ninvariance causes adversarial vulnerability. arXiv preprint arXiv:1811.00401, 2018.\n[122] Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil, Krishnamurthy Dvijotham, Timo-\nthy Mann, and Pushmeet Kohli. Achieving robustness in the wild via adversarial mixing\nwith disentangled representations. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1211–1220, 2020.\n[123] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 4401–4410, 2019.\n[124] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the\nwild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.\n[125] Eric Wong and J Zico Kolter. Learning perturbation sets for robust machine learning. arXiv\npreprint arXiv:2007.08450, 2020.\n[126] Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand.\nDomain-adversarial neural networks. arXiv preprint arXiv:1412.4446, 2014.\n47\n[127] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation.\nIn International conference on machine learning, pages 1180–1189. PMLR, 2015.\n[128] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classiﬁer\ndiscrepancy for unsupervised domain adaptation. arXiv preprint arXiv:1712.02560, 2017.\n[129] Hal Daumé III. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009.\n[130] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei\nEfros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In\nInternational conference on machine learning, pages 1989–1998. PMLR, 2018.\n[131] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial\ndomain adaptation. In Advances in Neural Information Processing Systems, pages 1640–1650,\n2018.\n[132] Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Multi-adversarial domain\nadaptation. arXiv preprint arXiv:1809.02176, 2018.\n[133] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classiﬁ-\ncation tasks to a new unlabeled sample. In Advances in neural information processing systems,\npages 2178–2186, 2011.\n[134] Krikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain generalization via\ninvariant feature representation. In International Conference on Machine Learning, pages 10–18,\n2013.\n[135] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.\nDeep domain generalization via conditional invariant adversarial networks. In Proceedings of\nthe European Conference on Computer Vision (ECCV), pages 624–639, 2018.\n[136] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott.\nDomain generalization by marginal transfer learning. arXiv preprint arXiv:1711.07910, 2017.\n[137] Isabela Albuquerque, João Monteiro, Tiago H Falk, and Ioannis Mitliagkas. Adversarial target-\ninvariant representation learning for domain generalization. arXiv preprint arXiv:1911.00804,\n2019.\n[138] Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances\nusing the marginal likelihood. In Advances in Neural Information Processing Systems, pages\n9938–9948, 2018.\n[139] Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. Learning invari-\nances in neural networks. arXiv preprint arXiv:2010.11882, 2020.\n[140] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n48\nA\nTraining details\nIn this appendix, we provide details concerning our implementation of both the model-based robust\ndeep learning algorithms as well as models of natural variation. We note that all experiments\ndescribed in Sections 6 and 7 were run on four NVIDIA RTX 5000 GPUs.\nA.1\nBaseline and model-based implementation details\nPGD. When training classiﬁers with PGD [22], we use a perturbation budget of ϵ = 8/255, a step\nsize of α = 0.01, and twenty iterations of gradient ascent per batch. Our implementation of PGD is\nincluded along with our implementation of the model-based robust deep learning algorithms; this\nimplementation can be found at https://github.com/arobey1/mbrdl.\nADDA. We used the following implementation of ADDA [82]: https://github.com/Carl0520/\nADDA-pytorch. We trained each classiﬁer for 100 epochs on the source domain, and we then trained\nfor a further 100 epochs when adaptating the weights of the target encoder. We used the LeNet\n[140] for the encoder networks; to ensure a fair comparison, we used two convolutional layers and\ntwo feed-forward layers, which is the same as the classiﬁcation networks used for model-based\ntraining.\nAugMix. We used the following implementation of AugMix [39]: https://github.com/google-\nresearch/augmix. We used the default parameters for training described in this repository.\nModel-based algorithms. All three model-based algorithms (MRT, MAT, and MDA) are imple-\nmented in our repository: https://github.com/arobey1/mbrdl. Throughout the experiments,\nunless stated otherwise, we ran MRT with k = 10, MAT with k = 10, and MDA with k = 3. We also\nused a trade-off parameter of λ = 1.\nA.2\nClassiﬁer training details\nHere we use the following conventions for describing architectures. c32-3 refers to a 2D convolu-\ntional operator with 32 kernels, each of which has shape 3 × 3. p2 refers to a max-pooling layer\nwith kernel size 2. d0.25 refers to a dropout layer, which drops an activation with probability 0.25.\nflat refers to a ﬂattening layer. fc-128 refers to a fully-connected layer mapping into R128.\nWhen training classiﬁers for MNIST, Q-MNIST, E-MNIST, K-MNIST, Fashion-MNIST, USPS,\nSVHN, GTSRB, and CURE-TSR we use a simple CNN architecture with two convolutional layers\nand two feed forward layers. More speciﬁcally, the architecture we used the following architecture:\nc32-3, c64-3, p2, c128-3, p2, d0.25, flat, fc128, d0.5, fc10.\nFor each of these experiments, we use the Adadelta [70] optimizer with a learning rate of 1.0. We\nalso use a batch size of 64. Images from MNIST, Q-MNIST, E-MNIST, K-MNIST, Fashion-MNIST,\nUSPS, SVHN, and CURE-TSR are resized to 32 × 32 × 3; for grayscale datasets such as MNIST,\nwe repeat the channels three times. Images from GTSRB are resized to 64 × 64 × 3. We train each\nclassiﬁer for 100 epochs.\nWhen training on ImageNet, we use the ResNet-50 [41] architecture. We note that architectural\nchoices are possible and will be explored in future work. For each of the experiments on ImageNet,\n49\nwe use SGD with an initial learning rate of 0.05; we decay the learning rate linearly to 0.001 over\n100 epochs. We use a batch size of 64 for ERM and PGD. For the model-based algorithms, we use a\nbatch size of 32, as these algorithms require the GPU to store multiple copies of each image during\neach training iteration.\nA.3\nMUNIT framework overview\nFor completeness, we give a brief overview of the MUNIT framework [4]. To begin, let xA ∈A\nand xB ∈B be images from two unpaired image domains A and B; in the notation of the previous\nsection, we assume that these images are drawn from two marginal distributions PA and PB.\nFurther, the MUNIT model assumes that each image from either domain can be decomposed into\ntwo components: a style code s that contains information about factors of natural or nuisance\nvariation, and a content code c that contains information about higher level features such as the\nlabel of the image. Further, it is assumed that the content codes for images in either domain are\ndrawn from a common set C, but that the style codes are drawn from domain speciﬁc sets SA\nand SB. In this way, a pair of corresponding images (xA, xB) are of the form xA = DecA(c, sA)\nand xB = DecB(c, sB), where c ∈C, sA ∈SA, sB ∈SB, and where DecA and DecB are unknown\ndecoding networks corresponding to domains A and B respectively. The authors of [4] call this\nsetting a partially shared latent space assumption.\nThe MUNIT model consists of an encoder-decoder pair (EncA, DecA) and (EncB, DecB) for\neach image domain A and B. These encoder-decoder pairs are trained to learn a mapping that\nreconstructs its input. That is, xA ≈DecA(EncA(xA)) and xB ≈DecB(EncB(xB)). More speciﬁcally,\nEncA : A →C × SA is trained to encode xA into a content code c ∈C and a style code sA ∈SA.\nSimilarly, EncB : B →C × SB is trained to encode xB into c ∈C and sB ∈SB. Then the decoding\nnetworks DecA : C × SA →A and DecB : C × SB →B are trained to reconstruct the encoded pairs\n(c, sA) and (c, sB) into the respective images xA and xB.\nInter-domain image translation is performed by swapping the decoders. In this way, to map an\nimage xA from A to B, xA is ﬁrst encoded into EncA(xA) = (c, sA). Then, a new style vector sB is\nsampled from SB from a prior distribution πB on the set SB and the translated image xA→B is equal\nto DecB(c, sB). The translation of xB from B to A can be described via a similar procedure with\nEncB, DecA, and a prior πA supported on SA. In this paper, we follow the convention used in [4] as\nuse a Gaussian distribution for both πA and πB with zero mean and an identity covariance matrix.\nTraining an MUNIT model involves considering four loss terms. First, the encoder-decoder\npairs (EncA, DecA) and (EncB, DecB) are trained to reconstruct their inputs my minimizing the\nfollowing loss:\nℓrecon = ExA∼PA ||DecA(EncA(xA)) −xA||1 + ExB∼PB ||DecB(EncB(xB)) −xB||1\nFurther, when translating an image from one domain to another, the authors of [4] argue that we\nshould be able to reconstruct the style and content codes. By rewriting the encoding networks as\nEncA(xA) = (Encc\nA(xA), Encs\nA(xA)) and EncB(xB) = (Encc\nB(xB), Encs\nB(xB)), the constraint on the\ncontent codes can be expressed in the following way:\nℓc\nrecon = EcA∼P(cA)\nsB∼πB\n||Encc\nB(DecB(cA, sB)) −cA||1 + EcB∼P(cB)\nsA∼πA\n||Encc\nA(DecA(cB, sA)) −cB||1\nwhere P(cA) is the distribution given by cA = Encc\nA(xA) where xA ∼PA and P(cB) is the distri-\nbution given by cB = Encc\nB(xB) where xB ∼PB. Similar, the constraint on the style codes can be\n50\nName\nValue\nBatch size\n1\nWeight decay\n0.0001\nLearning rate\n0.0001\nLearning rate policy\nStep\nγ (learning rate decay amount)\n0.5\nλx (image reconstruction coefﬁcient)\n10\nλc (content cycle-consistency coefﬁcient)\n1\nλs (style cycle-consistency coefﬁcient)\n1\nTable 8: MUNIT hyperparameters.\nwritten as\nℓs\nrecon = EcA∼P(cA)\nsB∼πB\n||Encs\nB(DecB(cA, sB)) −sB||1 + EcB∼P(cB)\nsA∼πA\n||Encs\nA(DecA(cB, sA)) −sA||1 .\nFinally, two GANs corresponding to the two domains A and B are used to form an adversarial loss\nterm. The GANs use the decoders DecA and DecB as the respective generators for domains A and\nB. By denoting the discriminators for these domains by DA and DB, we can write the GANs as\n(DecA, DA) and (DecB, DB). In this way, the ﬁnal loss term takes the following form:\nℓGAN = EcA∼P(cA)\nsB∼πB\n[log (1 −DB(DecB(cA, sB)))] + ExB∼PB[log DB(xB)]\n+ EcB∼P(cB)\nsA∼πA\n[log (1 −DA(DecA(cB, sA)))] + ExA∼PA[log DA(xA)]\nUsing the four loss terms we have described, the MUNIT framework uses ﬁrst-order methods\nto solve the following nonconvex optimization problem:\nmin\nEncA,EncB\nDecA,DecB\nmax\nD1,D2\nℓGAN + λxℓrecon + λcℓc\nrecon + λsℓs\nrecon\nA.4\nHyperparameters and implementation of MUNIT\nIn Table 8, we record the hyperparameters we used for training models of natural variation via the\nMUNIT framework. The hyperparameters we selected are generally in line with those suggested in\n[4]. We use the same architectures for the encoder, decoder, and discriminative networks as are\ndescribed in Appendix B.2 of [4].\n51\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 9: SVHN contrast (low→high).\nB\nA gallery of learned models of natural variation\nWe conclude this section by showing images corresponding to the many distributional shifts used\nin the experiments section. Furthermore, we show images generated by passing domain A images\nthrough learned models of natural variation.\n52\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 10: SVHN brightness (low→high).\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 11: CURE-TSR snow (no→yes).\n53\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 12: CURE-TSR haze (no→yes).\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 13: CURE-TSR decolorization (no→yes).\n54\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 14: CURE-TSR rain (no→yes).\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 15: ImageNet brightness (low→high).\n55\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 16: ImageNet contrast (high→low).\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 17: ImageNet snow (no→yes).\n56\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 18: ImageNet fog (no→yes).\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 19: ImageNet frost (no→yes).\n57\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 20: GTSRB brightness (low→high).\n(a) Domain A images.\n(b) Domain B images.\n(c) Domain A images from (a) passed through a\nlearned model of natural variation G(x, δ).\n(d) One image x from domain A (left) and six im-\nages generated by passing x through G(x, δ) for ran-\ndomly sampled δ vectors.\nFigure 21: GTSRB contrast (low→high).\n58\nDataset D2\nQMNIST\nEMNIST\nKMNIST\nFashion-MNIST\nUSPS\nImages\nfrom D2\nModel-\nbased\nimages\nTable 9: Passing images from other datasets through a model learned on MNIST. The ﬁrst row\nof images in this table are samples taken from colorized versions of Q-MNIST, E-MNIST, K-MNIST,\nFashion-MNIST, and USPS. The second row of images shows samples passed through a model\ntrained on the original MNIST dataset to change the background color from blue to red.\nC\nDetails concerning datasets and domains\nAs mentioned in Section 6, we used twelve different datasets in this work to fully evaluate the\nefﬁcacy of the model-based algorithms we introduced in Section 5. For several of these datasets,\nwe curated subsets corresponding to different factors of natural variation, which we refer to as\ndomains.\nC.1\nNatural vs. synthetic variation in data\nThroughout our experiments, we demonstrate that our methods are able to provide robustness\nagainst many challenging sources of natural variation. Furthermore, our experiments contain\ndomains with both naturally-occurring and artiﬁcially-generated variation. Notably, every experiment\ninvolving data from SVHN or GTSRB used naturally-occurring variation. In what follows, we\ndiscuss both of these categories.\nC.1.1\nNaturally-occurring variation\nThroughout the experiments, we used data from SVHN and GTSRB to train neural networks to\nbe robust against contrast and brightness variation. To extract naturally-occurring variation from\nthese datasets, we used simple metrics to threshold the data into subsets corresponding to different\nlevels of natural variation. Speciﬁcally, we deﬁned the brightness B(x) of an RGB image x to be\nthe mean pixel value of x, and we deﬁne the contrast C(x) to be the difference between the largest\nand smallest pixel values. Table 10 show the thresholds we chose for contrast and brightness on\nSVHN and GTSRB. Note that these thresholds were chosen somewhat subjectively to reﬂect our\nperception of low, medium and high values of brightness and contrast. We intend to experiment\nwith different thresholds in future work.\n59\nSVHN\nGTSRB\nLow\nMedium\nHigh\nLow\nMedium\nHigh\nBrightness\nB < 60\n160 < B < 170\nB > 180\nB < 40\n85 < B < 125\nB > 170\nContrast\nC < 80\n90 < C < 100\nC > 190\nC < 80\n140 < C < 200\nC > 230\nTable 10: Brightness and contrast thresholds. This table shows the thresholds we chose to represent\nlow, medium, and high values of contrast and brightness for SVHN and GTSRB.\nFigure 22 shows a summary of the subsets of SVHN that we compiled corresponding to\nbrightness. In particular, Figure 22a shows a histogram of the brightnesses of images in SVHN. We\nused this histogram to set thresholds for low, medium, and high brightness, which are given in\nTable 10. The images below the histogram correspond to the bins of the histogram; that is, images\nfurther to the left in Figure 22a have lower brightness, whereas images further to the right have\nhigh brightness. In Figures 22b, 22c, and 22d, we show samples from the subsets of low, medium\nand high contrast subsets of SVHN that we compiled. Figure 23 tells the same story as 22 for the\ncontrast nuisances in SVHN. Again, Figure 23a shows a histogram and accompanying images\ncorresponding to different values of contrast. Figures 23b, 23c, and 23d show samples from the\nsubsets of low, medium, and high contrast images we compiled.\nWe repeat this analysis for the brightness and contrast thresholding operations for GTSRB in\nFigures 24 and 25. Again, the difference between high- and low-brightness samples is remarkable,\nas is the difference in the samples corresponding to high- and low-contrast. However, an interesting\ndifference between the distributions of brightness and contrast on GTSRB vis-a-vis SVHN is that\nthe distributions for GTSRB are skewed, whereas the distributions for SVHN are close to being\nsymmetric.\nC.1.2\nArtiﬁcially-generated variation\nThe remainder of the experiments, including those on MNIST, CURE-TSR, and ImageNet, use\nartiﬁcally-generated variation. Indeed, one challenge in addressing deep learning’s lack of robust-\nness to natural variation is that relatively few datasets contain labeled forms of naturally-occurring\nsources of variation. To this end, an important research challenge is to curate datasets with\nnaturally-occurring variation; we plan to pursue this goal in future work.\nWhen data with naturally-occurring variation is not available, artiﬁcially-generated variation\ncan be used as an effective proxy for testing the robustness of deep learning against different forms\nof variation [15, 36]. Indeed, the recently curated CURE-TSR [74] and ImageNet-c [15] were created\nusing pre-deﬁned, artiﬁcal transformations of data. While these transformations are synthetic, the\nimages in Appendix B show that they are indeed quite realistic.\nC.2\nDatasets introduced in this paper\nIn this paper, we introduced several new datasets which contain multiple simultaneous corruptions,\nincluding show, brightness, contrast, and fog. In particular, we used the transforms used to create\nImageNet-c to add multiple corruptions to the ImageNet test set. To do so, we used the open-source\ncode from [15], which can be found at https://github.com/hendrycks/robustness. Images of\nthese datasets are shown in Figures 26-29.\n60\n(a) SVHN brightness histogram. The histogram\nshows the distribution of pixel brightness for\nSVHN. The images below the histogram corre-\nspond to the bins of the histogram, meaning sam-\nples to the left have low brightness whereas sam-\nples further to the right have higher brightness.\n(b) Low brightness samples.\n(c) Medium brightness samples.\n(d) High brightness samples.\nFigure 22: SVHN brightness thresholding overview.\n61\n(a) SVHN contrast histogram. The histogram\nshows the distribution of pixel contrast for\nSVHN. The images below the histogram corre-\nspond to the bins of the histogram, meaning sam-\nples to the left have low contrast whereas sam-\nples further to the right have higher contrast.\n(b) Low contrast samples.\n(c) Medium contrast samples.\n(d) High contrast samples.\nFigure 23: SVHN contrast thresholding overview.\n62\n(a) GTSRB brightness histogram.\nThe his-\ntogram shows the distribution of pixel brightness\nfor GTSRB. The images below the histogram cor-\nrespond to the bins of the histogram, meaning\nsamples to the left have low brightness whereas\nsamples further to the right have higher bright-\nness.\n(b) Low brightness samples.\n(c) Medium brightness samples.\n(d) High brightness samples.\nFigure 24: GTSRB brightness thresholding overview.\n63\n(a) SVHN contrast histogram. The histogram\nshows the distribution of pixel contrast for\nSVHN. The images below the histogram corre-\nspond to the bins of the histogram, mening sam-\nples to the left have low contrast whereas sam-\nples further to the right have higher contrast.\n(b) Low contrast samples.\n(c) Medium contrast samples.\n(d) High contrast samples.\nFigure 25: SVHN contrast thresholding overview.\n(a) Test images from ImageNet.\n(b) Corresponding images from new dataset.\nFigure 26: Brightness and snow. We use the challenge-level 1 transforms from ImageNet-c to\ngenerate an ImageNet test set with shifts in both brightness and snow.\n64\n(a) Test images from ImageNet.\n(b) Corresponding images from new dataset.\nFigure 27: Brightness and contrast. We use the challenge-level 2 transforms from ImageNet-c to\ngenerate an ImageNet test set with shifts in both brightness and contrast.\n(a) Test images from ImageNet.\n(b) Corresponding images from new dataset.\nFigure 28: Brightness and fog. We use the challenge-level 1 transforms from ImageNet-c to\ngenerate an ImageNet test set with shifts in both brightness and fog.\n(a) Test images from ImageNet.\n(b) Corresponding images from new dataset.\nFigure 29: Contrast and fog. We use the challenge-level 1 transforms from ImageNet-c to generate\nan ImageNet test set with shifts in both contrast and fog.\n65\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2020-05-20",
  "updated": "2020-11-02"
}