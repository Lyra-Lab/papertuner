{
  "id": "http://arxiv.org/abs/2005.12533v1",
  "title": "Guiding Symbolic Natural Language Grammar Induction via Transformer-Based Sequence Probabilities",
  "authors": [
    "Ben Goertzel",
    "Andres Suarez Madrigal",
    "Gino Yu"
  ],
  "abstract": "A novel approach to automated learning of syntactic rules governing natural\nlanguages is proposed, based on using probabilities assigned to sentences (and\npotentially longer word sequences) by transformer neural network language\nmodels to guide symbolic learning processes like clustering and rule induction.\nThis method exploits the learned linguistic knowledge in transformers, without\nany reference to their inner representations; hence, the technique is readily\nadaptable to the continuous appearance of more powerful language models. We\nshow a proof-of-concept example of our proposed technique, using it to guide\nunsupervised symbolic link-grammar induction methods drawn from our prior\nresearch.",
  "text": "Guiding Symbolic Natural Language\nGrammar Induction\nvia Transformer-Based Sequence Probabilities\nBen Goertzel1, Andr´es Su´arez-Madrigal1,2, and Gino Yu2\n1 SingularityNET Foundation, Amsterdam, The Netherlands\nhttps://singularitynet.io\nben@goertzel.org\n2 The Hong Kong Polytechnic University, Kowloon, Hong Kong\nsuarezandres@gmail.com\nAbstract. A novel approach to automated learning of syntactic rules\ngoverning natural languages is proposed, based on using probabilities\nassigned to sentences (and potentially longer word sequences) by trans-\nformer neural network language models to guide symbolic learning pro-\ncesses like clustering and rule induction. This method exploits the learned\nlinguistic knowledge in transformers, without any reference to their inner\nrepresentations; hence, the technique is readily adaptable to the contin-\nuous appearance of more powerful language models. We show a proof-of-\nconcept example of our proposed technique, using it to guide unsuper-\nvised symbolic link-grammar induction methods drawn from our prior\nresearch.\nKeywords: Unsupervised grammar induction · Transformers · BERT.\n1\nIntroduction\nUnsupervised grammar induction – learning the grammar rules of a language\nfrom a corpus of text or speech without any labeled examples (e.g. sentences\nannotated with human-created syntax parses) – remains in essence an unsolved\nproblem. Although it has been approached for decades [2], useful applications\nfor restricted domains have been presented [9], and state-of-the-art performance\nis improving [10], the resulting grammars for natural language are still not able\nto properly capture its structure.\nBypassing explicit representations of the grammar rules, recent transformer\nneural network models have shown powerful abilities at language prediction and\ngeneration, indicating that at some level they internally “understand” those\nrules. However, such rules don’t seem to be found in the neural connections\nin these networks in any straightforward manner [3,8], and are not easily ex-\ntractable without supervision. Supervised extraction of grammatical knowledge\nfrom the BERT [4] network reveals that, to map the state of a transformer net-\nwork when parsing a sentence into the sentence’s parse, complex and tangled\nmatrix transformations are needed [7].\narXiv:2005.12533v1  [cs.CL]  26 May 2020\n2\nBen Goertzel, Andr´es Su´arez-Madrigal, and Gino Yu\nHere we explore an alternate approach: Don’t try to milk the grammar out\nof the transformer network directly, rather use the transformer’s language model\nas a sequence probability oracle, a tool for estimating the probabilities of word\nsequences; then use these sequence probability estimates to guide the behavior\nof symbolic learning algorithms performing grammar induction. Our proposal is\nactually agnostic in the mechanism to ﬁnd rules, and could synergize well with\nrelated eﬀorts [12,6]; what we introduce is a novel and powerful way to guide the\ninduction. This is work in progress, but preliminary results have been obtained\nand look quite promising.\nFull human-level AI language processing will clearly involve additional as-\npects not considered here, most critically the grounding of linguistic constructs\nin non-linguistic data [14]. However, the synergy between symbolic and sub-\nsymbolic aspects of language modeling is a key aspect of generally intelligent\nlanguage understanding and generation which has not been adequately cap-\ntured so far, and we feel the work presented here makes signiﬁcant progress in\nthis direction.\n2\nMethodology\nTransformer network models like BERT [4], GPT-2 [11], and their relatives pro-\nvide probabilistic language models which can be used to assess the probability\nof a given sentence. The probability of sentence S according to such a language\nmodel tells you the odds that, if you sampled a random sentence from the model\n(used in a generative way), the output would be S. If S is not grammatical\naccording to the grammar rules of the language modelled by the network, its\nprobability will be very low. If S is grammatical but senseless, we assume from\nexperimentation with these models, that its probability should also be quite low.\nHaving a sentence (or more generally word sequence) probability oracle of\nthis nature for a language provides a way to assess the degree to which a given\ngrammar G models that language. What one wants is that: the high-probability\nsentences according to the oracle tend to be grammatical according to G, the\nlow-probability sentences according to the oracle are less likely to be grammatical\naccording to G, and G is as concise as possible. The grammars that best ﬁt these\nconjuncted factors are the best grammatical models of the language in question.\nThis concept could be used to cast grammar induction as a probabilistic\nprogramming problem, in a relatively straightforward but computationally ex-\norbitant way. Just sample random grammars from some reasonable distribution\non grammar space, and evaluate their quality by the above factors.\nWhat we propose here is conceptually similar but more feasible: Begin with\na symbolic grammar learning algorithm which is capable of incrementally build-\ning up a complex grammar, then use sentence probability estimates from a neu-\nral language model to guide the grammar learning. One could view this as an\ninstance of the probabilistic programming approach, using a linguistic-theory-\nbased heuristic method of sampling grammar space.\nGuiding Grammar Induction via Transformers\n3\nOur prior work on symbolic grammar induction [5] uses two mains steps\nto build a dependency grammar from an unlabeled corpus. First, separate the\nvocabulary of interest into word categories (functionally equivalent to parts of\nspeech, with a certain level of granularity). An implicit sub-step here is the\ndisambiguation of polysemous words in the vocabulary, so that a single word\ncould be assigned to more than one category. Then, perform rule induction to\nﬁnd how words in these categories are connected to form grammatical sentences.\nOur proposed approach, which enhances the aforementioned steps with the use\nof transformer language models, is depicted in Figure 1 and summarized as:\n1. Infer word-senses and parts of speech from vectors built using a neural lan-\nguage model as a sentence probability oracle.\n2. Infer grammatical rules from symbolic pattern-analysis of the corpus tagged\nwith these senses and parts of speech.\n3. Assemble a grammar incrementally from inferred rules. To evaluate whether\na given rule should be included in the grammar:\n– Using a tree transformer network, generate a set of sentences consistent\nwith the given rule, and others that follow mutations of the rule.\n– Use a neural model as a sentence probability oracle to estimate whether\nthe inferred rule leads to better generated sentences than its mutation(s).\nTransformer\nNeural Net\n/Probabilistic\nLanguage Model\nText corpus\nClustering\nWord instance\nembeddings\nWord senses\nClustering\nWord-sense\nembeddings\nParts of Speech\n(PoS)\nPoS-tagged\ncorpus\nSymbolic\nGrammar\nRule Learner\nTree\nTransformer\nNetwork\nGrammar Rule\nEvaluator\nInferred grammar\nGrammar-based\ngenerated sentences\nFig. 1. High-level grammar learning architecture involving symbolic learning guided\nby estimated word sequence probabilities from a transformer network.\nFor our early experiments, we have chosen BERT [4] as the transformer to use,\nbut the idea could easily make use of similar unsupervised pre-trained networks.\n2.1\nAssessing sentence probability\nTo explain details of our approach, we begin with the computation of sentence\nprobability according to a neural language model (illustrated in Fig. 2).\nGiven a sentence S = [w0, w1, ..., wN], composed of N words wi, i ∈[0, 1, ..., N],\nwe want to calculate its probability P(S). A way to decompose that probability\ninto conditional probabilities is:\nPf(S) = P(w0, w1, ..., wN) = P(w0)·P(w1|w0)·P(w2|w0, w1)·...·P(wN|w0, w1, ..., wN−1),\n4\nBen Goertzel, Andr´es Su´arez-Madrigal, and Gino Yu\n*\n*\n=\ntokenizer\nmasking\nBERT\nBERT\nBERT\nFig. 2. Example of forward sentence probability calculation.\nwhich we call forward sentence probability.\nA conditional probability P(wi|wi−1, ..., w0) can be obtained from BERT’s\nmasked word prediction model by taking the whole sentence, masking all the\nwords which are not conditioned in the term (including wi), and obtaining\nBERT’s estimation for the probability of wi.\nTo exemplify the idea, we summarize how to calculate the forward probability\nof the sentence “She answered quickly”. The probability is given by\nPf(She answered quickly) = P(She)·P(She answered|She)·P(She answered quickly|She answered).\nEach factor translates to a BERT Masked Language Model (MLM) prediction\nfor a sentence with masked tokens. For example,\nP(She answered|She) = P(MASK2=answered|She MASK2 MASK3),\nand we get the probability that “answered” is predicted as the second token in\nthe BERT MLM.\nNow, to take advantage of BERT’s bi-directional capabilities, we can estimate\nthe sentence’s backwards probability in a similar fashion:\nPb(S) = P(w0, w1, ..., wN) = P(wN)·P(wN−1|wN)·P(wN−2|wN−1, wN)·...·P(w0|w1, w2, ..., wN)\nWe ﬁnally approximate the sentence probability as the geometric-mean of\nthe two directional ones:\nP(S) =\nq\nPf(S) · Pb(S)\n2.2\nWord Category Formation\nFollowing our prior work on symbolic grammar induction [5], and a number\nof previous works, we propose to generate embeddings for the words in the\nvocabulary and cluster them using a proximity metric in the embedding space.\nEach ﬁnal cluster can be considered a diﬀerent word category, whose connection\nrules to other clusters will be deﬁned in the induced grammar. Unlike prior work,\nwe use sentence probabilities as the embedding features.\nGuiding Grammar Induction via Transformers\n5\nWe expand each sentence in the corpus into N sentences with a “blank”\ntoken in a diﬀerent position, where N is that sentence’s length. Each of those\nsentences with a blank is a feature for the word-vectors we will build. Hence, we\ncan think of a word-sentence matrix M, where rows are unique sentences with\nblanks in them, and columns are the words in the vocabulary (see Fig. 3).\nWe ﬁll each cell in the matrix with the probability of the corresponding\nsentence-with-a-blank (row), when the blank is substituted by the corresponding\nword (column). That is, if S′\ni is the sentence-with-a-blank in row i and wj is the\nword in column j, then the cell Mi,j = P(S′\ni|blank ﬁlled with wj).\nOnce the matrix is ﬁlled, word categories are obtained by clustering the ob-\ntained word vectors (columns of the matrix). Or, if one has performed word\nsense disambiguation (which can be done based on diﬀerent computations from\nthis same matrix, as will be described below), by clustering similar vectors cor-\nresponding to word senses.\n. . .\n. . .\n. . .\n. . .\n...\n...\n...\n...\n...\n...\n...\n...\n. . .\n. . .\n. . .\n. . .\n. . .\nWSD\nrestructure\n. . .\n...\n...\n...\n...\n...\n...\n...\n...\n. . .\n. . .\n. . .\n. . .\nFig. 3. Left: Matrix of words versus sentences-with-one-blank; each cell contains the\nprobability of the given sentence ﬁlled with the given word. Right: The matrix restruc-\ntured after WSD.\n2.3\nWord Sense Disambiguation\nWord embeddings obtained from transformer networks by supervised learning\nhave been used to disentangle word senses [16]; here we attempt this task in\nan unsupervised manner. From an unlabeled training corpus, we obtain a trans-\nformer embedding for each instance of each word in its given context. Then, for\neach word in the vocabulary, we gather all of its embeddings and cluster them;\nwe consider the resulting clusters as diﬀerent word senses.\nSpeciﬁcally, a word-instance can be represented by a vector whose compo-\nnents are given by the probability that the neural language model assigns to\nthe sentences (and discourse contexts) obtained by replacing such word instance\nwith each word in the vocabulary.\nConsider the word-instance “test” in “The test was a success”. If the cor-\npus vocabulary is V = (frog, which, ...) then we can represent this instance’s\nintension (contextual properties) with the vector I:\n– I(test, The\nwas a success)[1] = P(The frog was a success)\n– I(test, The\nwas a success)[2] = P(The which was a success)\n6\nBen Goertzel, Andr´es Su´arez-Madrigal, and Gino Yu\n– . . .\nNoticeably, the matrix obtained this way is the same one used for word-category\nformation; only, instead of performing clustering over the word vectors (columns),\nwe need to independently cluster the rows that belong to instances of the same\nword to ﬁnd their diﬀerent senses.\nWord Category Formation in Depth. Once polysemy is taken care of, we\ncan perform word-categorization over word-senses, allowing the same word to\nbe assigned to diﬀerent parts of speech (PoS) (e.g. “test” as a noun and as a\n“verb”). We need, however, to re-structure the sentence probability matrix to\nexpress word-senses as columns before grouping them into PoS. This is done by\nreassigning the previously-calculated probabilities to the correct word-sense.\nStarting from the original matrix M, we zero-initialize a disambiguated ma-\ntrix M ′ with the same number or rows, and as many columns as word-senses.\nFor a given entry in the original matrix, Mi,j, corresponding to sentence Si and\nvocabulary word wj, we need to decide to which of its senses to assign it to. If wj\nhas only one sense, the decision is trivial; otherwise, we take the embedding for\nsentence Si (that is, the entire row Mi, as in the WSD process), and measure its\ndistance from the centroids of the diﬀerent senses for wj obtained in the WSD\nstep. The closest sense gets assigned the value Mi,j, and the rest keep a zero.\nThis way, we build word-sense embeddings by using the columns of M ′; clustering\nthese embeddings creates PoS categories and ﬁner-grained syntactico-semantic\ncategories. Figure 3 illustrates the disambiguated probability matrix.\n2.4\nGrammar Induction\nAfter word categories are formed, grammar induction can take place by ﬁguring\nout which groups of words are allowed to link with others in grammatical parses.\nA grammar can be accumulated by starting with one rule and adding more\nincrementally, using the neural language model to evaluate the desirability of\neach proposed addition. The choice of candidate rules is made by a symbolic\nrule induction algorithm; so far we have used the Grammar Learner process\ndescribed in [5].\nFor a grammar rule proposed as an addition to the partial grammar already\nlearned, we generate sentences that use that rule within the given grammar\nand obtain their sentence probabilities P(S). Then we corrupt the rule in some\nmanner, adjust the grammar accordingly, generate sentences from this modiﬁed\ngrammar starting with the mutated rule, and evaluate their P(S). If the sen-\ntences from the modiﬁed grammar decrease signiﬁcantly in quality (where the\nthreshold is a parameter), then the original rule is taken as valid. The rationale is\nthat correct grammar rules will produce better sentences than their distortions.\nIn the case of the link grammar formalism [13], which we have used in our\nwork so far, a grammar rule consists of a set of disjuncts of conjunctions of\ntyped “connectors” pointing forward or backward in a sentence. A mutation of\nGuiding Grammar Induction via Transformers\n7\nthis type of rule can be the swapping of each connector in the rule, which also\nimplies a word-order change.\nFor example, if we have a rule R that connects the word “kids” with the\nword “the” on the left and the word “small” also on the left, in that order:\nkids: small- & the-,\nwhich allows the string “the small kids”, then the mutated rule R∗would be\nkids: small+ & the+,\nwhich accepts the string “kids small the”3.\nFig. 4. Link-parse of ”The small kids play football” according to the standard English\nlink grammar dictionary [13].\nThis methodology requires a way to generate sentences from proposed gram-\nmars. One approach is to use a given grammar to guide the attention within a\nTree Transformer [15]. The standard Tree Transformer approach guides atten-\ntion based on word-sequence segmentation that is driven by mutual information\nvalues between pairs of adjacent words. One can replace these probabilities with\nmutual information values between pairs of words that are linked in partial\nparses that agree with a provided grammar.\nCurrently we are using a simpler stochastic sentence generation model in\nour proof-of-concept experiments, and planning to shift to a Tree Transformer\napproach for the next phase of work.\nSo, the rule R guides the generation of sentences like S = “The small kids\nplay football” (see its Link-parse in Fig. 4). The rule R∗guides the generation\nof sentences like S∗=“Kids small the play football”. The language model says\nP(S) > P(S∗), thus arguing in favor of adding R to one’s grammar (and then\ncontinuing the incremental learning process).\nAlternatively, instead of producing mutated rules, one could also compare\nthe probabilities of sentences generated with the rule under evaluation against\nthose of a set of reference sentences of the same length, like those in the corpus\nused to derive the grammar, or the word categories obtained previously.\n3\nProof of concept (POC)\nScalable implementation and testing of the ideas described above is work in\nprogress; here we describe some basic examples we have explored so far, which\n3 Notice that connectors in the rules for small and kids also have to be modiﬁed to\naccommodate this mutation, i.e. they need to swap kids+ to kids-\n8\nBen Goertzel, Andr´es Su´arez-Madrigal, and Gino Yu\nvalidate the basic concepts (but do not yet provide a thorough demonstration).\nWe chose to perform our initial experiments using BERT4, due to its popularity\nin several downstream tasks (e.g. word sense disambiguation [16]).\nFollowing the workﬂow of the grammar induction process, we ﬁrst show an\nexample of word sense disambiguation, then one for word category formation,\nand ﬁnally grammar rule evaluation.\n3.1\nWord sense disambiguation\nFor an initial simple experiment, we created a small corpus of 16 sentences con-\ntaining 146 words, out of which 8 are clearly ambiguous (to an English speaker).\nBoth syntactic and semantic ambiguities were included. We generated embed-\ndings for each word instance in the corpus, as described in section 2.3. Clustering\nwas performed with spherical clustering methods from Spherecluster5 [1], as well\nas out-of-the-box DBSCAN and OPTICS models in Python’s scikit-learn library\nwith the cosine-distance metric.\nWe found that SphericalKMeans clustering did the best job at separating\nword senses in our test corpus. Setting the number of clusters to two, the al-\ngorithm achieved an F1-score of 0.91. As examples, the disambiguation for the\nword “fat”, which was perfect, looks as follows:\nCluster #0 samples:\nsantiago\nbecame FAT after he got\nmarried .\nthere are many\nhealth\nrisks\nassociated\nwith FAT .\nthe\nnegative\nhealth\neffects of FAT last a long time .\nCluster #1 samples:\nthe FAT cat ate the last\nmouse\nquickly .\nthere is a FAT fly in the car with us .\nThe clustering for “time”, on the other hand, placed one instance in the\nwrong category, and looks like this:\nCluster #0 samples:\ni was born and raised in santiago de cuba , a long TIME ago .\nmy mouse\nstopped\nresponding at the same TIME as the\nkeyboard .\nthe\nnegative\nhealth\neffects of fat last a long TIME .\nCluster #1 samples:\nyou will TIME the\nduration of the dress\nfitting\nsession .\nTIME will fly away\nquickly .\nThe disadvantage of using this straightforward implementation of Spheri-\ncalKMeans is that one has to specify the number of clusters to use. When re-\nquesting more clusters than there are senses for a word, the algorithm spreads\ninstances with similar meanings to diﬀerent clusters. This is especially the case\nwith words that we wouldn’t consider ambiguous, like function words (we have\n4 In particular, we use Huggingface’s implementation of BERT, contained in their\n“transformers” package [17] https://huggingface.co/transformers\n5 https://github.com/jasonlaska/spherecluster\nGuiding Grammar Induction via Transformers\n9\nsought to ﬁlter these by explicitly not disambiguating the top 10% most fre-\nquent words in the corpus). However, this may not be a terrible problem in\nour use case, as the word category formation algorithm will simply create more\nword-sense vectors per word, which then it could cluster together in the same\nword category. Future experiments will involve alternatives that automatically\nestimate the number of clusters to use.\n3.2\nWord category formation\nHere, working with the same corpus as for WSD, we used the disambiguation\nresults described above to build word vectors, thus allowing for words to be cat-\nalogued in more than one group. Rather than SphericalKMeans, we found that\nOPTICS, a method that doesn’t require a parameter for the number of clusters\nand can leave vectors uncategorized (shown as Cluster #-1), oﬀers remarkable\nquality in most formed clusters (#0-14), with a good level of granularity.\nCluster #-1: [fat , fat , ate , last , mouse , mouse , quickly , quickly ,\n., there , there , many , many , health , health , associated , with ,\nwith , stopped , responding , same , time , as , will , fly , fly , negative ,\nof , a, a, long , in , us , tomorrow , she , she , was , was , wearing ,\nlovely , brown , brown , dress , attendees , did , not , properly , for ,\noccasion , became , after , got , married , ’, ’, s, deteriorated ,\nand , de , ,, ago , fitting , wasn , t, year , smith , protagonize , ]\nCluster\n#0: [the , my , his , ]\nCluster\n#1: [born , able , ]\nCluster\n#2: [raised , growing , bought , ]\nCluster\n#3: [cat , keyboard , car , session , feed , family , microsoft , ]\nCluster\n#4: [duration , episode , series , ]\nCluster\n#5: [are , is , ]\nCluster\n#6: [morning , night , ]\nCluster\n#7: [away , out , ]\nCluster\n#8: [they , he , i, you , ]\nCluster\n#9: [risks , effects , ]\nCluster\n#10: [at , to , ]\nCluster\n#11: [santiago , cuba , ]\nCluster\n#12: [time , will , long , ]\nCluster\n#13: [dress , and , ]\nCluster\n#14: [of , in , ]\nAn evident problem with this result is that most of the words remain uncate-\ngorized (in Cluster #-1). Although we would expect the full iterative grammar\nlearning algorithm we propose to be able to live with that and cluster some of\nthe remaining words in the next pass, we will ﬁrst try to ﬁne-tune the procedure\nto alleviate this situation, as well as explore some other clustering algorithms.\nAt the same time, we predict that the results will improve when we use a larger\nnumber of features (instead of only 16 sentences for a total of 146 diﬀerent fea-\ntures). A very simple expansion of the vocabulary to cluster (not shown) already\nshowed a similar number of more populated clusters.\n10\nBen Goertzel, Andr´es Su´arez-Madrigal, and Gino Yu\n3.3\nGrammar Rule Evaluation\nWe show a simple use case for grammar rule evaluation, using the basic rule\nmodiﬁcation strategy proposed in the methodology: swapping the direction of\nthe connectors that make up a rule, and comparing the sentences generated with\nand without the mutation.\nFor this experiment, we created a proof-of-concept grammar with 6 words\ndivided in 6 categories: determiner, subject, verb, direct object, adjective, ad-\nverb. Then, we assigned relationships among the classes. Using a semi-random\nsentence generator, this grammar produces sentences like “the small kids eat the\nsmall candy quickly.” (that being the longest possible sentence in this grammar).\nWe then introduced some extra spurious rules to the grammar by hand.\nFrom a total of 21 rules (15 correct ones vs. 6 spurious ones), the grammar can\ngenerate sentences like “kids eat the the small candy kids eat candy the small\nquickly quickly.”, which clearly shows that the grammar is not correct anymore\n(this grammar has loops, so this is not even the longest sentence permitted by\nthese simple modiﬁcation).\nFinally, we ran a ﬁrst version of the grammar rule evaluator, to ﬁnd out that\nall of the spurious rules were rejected, as well as three of the “correct” rules.\nWe notice that among the “correct” rules that were discarded, at least one:\neat: kids -,\ngenerates sentences with no direct object, like “the kids eat.” This sentence,\nalthough valid, might not be very common for the BERT model, and thus obtain\na low probability.\nSimilarly, the reverse of this rule, as modiﬁed by the evaluation algorithm:\neat: kids+,\ngenerates sentences like “eat the kids.”, which is also grammatically valid, and\nmaybe as common as the previous case. This is a sensible explanation for the\nrule’s rejection.\n4\nConclusion and Future Work\nOur proof-of-concept experiments give intuitively strong indication of the via-\nbility of the methodology proposed for synergizing symbolic and sub-symbolic\nlanguage modeling to achieve unsupervised grammar induction. The next step\nis to create a scalable implementation of the approach and apply it to a large\ncorpus, and assess the quality of the results. If successful this will constitute\nsigniﬁcant progress both toward unsupervised grammar induction, and toward\nunderstanding how diﬀerent types of intelligent subsystems can come together\nto more closely achieve human-like language understanding and generation.\nReferences\n1. Banerjee, A., Dhillon, I., Ghosh, J., Sra, S.: Clustering on the unit hypersphere us-\ning von mises-ﬁsher distributions. Journal of Machine Learning Research 6 (2005)\nGuiding Grammar Induction via Transformers\n11\n2. Charniak, E.: Statistical language learning. MIT press (1996)\n3. Clark, K., Khandelwal, U., Levy, O., Manning, C.D.: What Does BERT Look At?\nAn Analysis of BERT’s Attention. arXiv:1906.04341 [cs] (Jun 2019)\n4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidi-\nrectional Transformers for Language Understanding. arXiv:1810.04805 [cs] (2019)\n5. Glushchenko, A., Suarez, A., Kolonin, A., Goertzel, B., Baskov, O.: Programmatic\nLink Grammar Induction for Unsupervised Language Learning. In: Artiﬁcial Gen-\neral Intelligence, vol. 11654, pp. 111–120. Springer International Publishing (2019)\n6. Grave, E., Elhadad, N.: A convex and feature-rich discriminative approach to de-\npendency grammar induction. In: Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics. pp. 1375–1384 (2015)\n7. Hewitt, J., Manning, C.D.: A structural probe for ﬁnding syntax in word represen-\ntations. In: Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics. pp. 4129–4138 (2019)\n8. Htut, P.M., Phang, J., Bordia, S., Bowman, S.R.: Do Attention Heads in BERT\nTrack Syntactic Dependencies? arXiv:1911.12246 [cs] (Nov 2019)\n9. de La Higuera, C., Oates, T., van Zaanen, M.: Introduction: Special issue on ap-\nplications of grammatical inference. Applied Artiﬁcial Intelligence 22(1-2),\n1–3\n(2008)\n10. Li, B., Cheng, J., Liu, Y., Keller, F.: Dependency Grammar Induction with a\nNeural Variational Transition-based Parser. arXiv:1811.05889 [cs] (Nov 2018)\n11. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language\nmodels are unsupervised multitask learners. OpenAI Blog 1(8), 9 (2019)\n12. Schmid, U., Kitzelmann, E.: Inductive rule learning on the knowledge level. Cog-\nnitive Systems Research 12(3-4), 237–248 (2011)\n13. Sleator, D.D., Temperley, D.: Parsing english with a link grammar. arXiv: cmp-\nlg/9508004 (1995)\n14. Tomasello, M.: Constructing a Language: A Usage-Based Theory of Language Ac-\nquisition. Harvard University Press (2003)\n15. Wang, Y.S., yi Lee, H., Chen, Y.N.: Tree transformer: Integrating tree structures\ninto self-attention. In: EMNLP/IJCNLP (2019)\n16. Wiedemann, G., Remus, S., Chawla, A., Biemann, C.: Does BERT Make Any\nSense? Interpretable Word Sense Disambiguation with Contextualized Embed-\ndings. arXiv:1909.10430 [cs] (Oct 2019)\n17. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,\nRault, T., Louf, R., Funtowicz, M., Brew, J.: Huggingface’s transformers: State-\nof-the-art natural language processing. ArXiv abs/1910.03771 (2019)\n",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "published": "2020-05-26",
  "updated": "2020-05-26"
}