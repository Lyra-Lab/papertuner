{
  "id": "http://arxiv.org/abs/2006.00262v3",
  "title": "Data Augmentation with Unsupervised Machine Translation Improves the Structural Similarity of Cross-lingual Word Embeddings",
  "authors": [
    "Sosuke Nishikawa",
    "Ryokan Ri",
    "Yoshimasa Tsuruoka"
  ],
  "abstract": "Unsupervised cross-lingual word embedding (CLWE) methods learn a linear\ntransformation matrix that maps two monolingual embedding spaces that are\nseparately trained with monolingual corpora. This method relies on the\nassumption that the two embedding spaces are structurally similar, which does\nnot necessarily hold true in general. In this paper, we argue that using a\npseudo-parallel corpus generated by an unsupervised machine translation model\nfacilitates the structural similarity of the two embedding spaces and improves\nthe quality of CLWEs in the unsupervised mapping method. We show that our\napproach outperforms other alternative approaches given the same amount of\ndata, and, through detailed analysis, we show that data augmentation with the\npseudo data from unsupervised machine translation is especially effective for\nmapping-based CLWEs because (1) the pseudo data makes the source and target\ncorpora (partially) parallel; (2) the pseudo data contains information on the\noriginal language that helps to learn similar embedding spaces between the\nsource and target languages.",
  "text": "Data Augmentation with Unsupervised Machine Translation Improves\nthe Structural Similarity of Cross-lingual Word Embeddings\nSosuke Nishikawa, Ryokan Ri and Yoshimasa Tsuruoka\nThe University of Tokyo\n7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan\nsosuke-nishikawa@nii.ac.jp\n{li0123,tsuruoka}@logos.t.u-tokyo.ac.jp\nAbstract\nUnsupervised cross-lingual word embedding\n(CLWE) methods learn a linear transformation\nmatrix that maps two monolingual embedding\nspaces that are separately trained with mono-\nlingual corpora. This method relies on the as-\nsumption that the two embedding spaces are\nstructurally similar, which does not necessar-\nily hold true in general. In this paper, we ar-\ngue that using a pseudo-parallel corpus gener-\nated by an unsupervised machine translation\nmodel facilitates the structural similarity of the\ntwo embedding spaces and improves the qual-\nity of CLWEs in the unsupervised mapping\nmethod. We show that our approach outper-\nforms other alternative approaches given the\nsame amount of data, and, through detailed\nanalysis, we show that data augmentation with\nthe pseudo data from unsupervised machine\ntranslation is especially effective for mapping-\nbased CLWEs because (1) the pseudo data\nmakes the source and target corpora (partially)\nparallel; (2) the pseudo data contains informa-\ntion on the original language that helps to learn\nsimilar embedding spaces between the source\nand target languages.\n1\nIntroduction\nCross-lingual word embedding (CLWE) methods\naim to learn a shared meaning space between\ntwo languages (the source and target languages),\nwhich is potentially useful for cross-lingual transfer\nlearning or machine translation (Yuan et al., 2020;\nArtetxe et al., 2018b; Lample et al., 2018a). Al-\nthough early methods for learning CLWEs often uti-\nlize multilingual resources such as parallel corpora\n(Gouws et al., 2015; Luong et al., 2015) and word\ndictionaries (Mikolov et al., 2013), recent studies\nhave focused on fully unsupervised methods that\ndo not require any cross-lingual supervision (Lam-\nple et al., 2018b; Artetxe et al., 2018a; Patra et al.,\n2019). Most unsupervised methods fall into the\ncategory of mapping-based methods, which gen-\nerally consist of the following procedures: train\nmonolingual word embeddings independently in\ntwo languages; then, ﬁnd a linear mapping that\naligns the two embedding spaces. The mapping-\nbased method is based on a strong assumption that\nthe two independently trained embedding spaces\nhave similar structures that can be aligned by a\nlinear transformation, which is unlikely to hold\ntrue when the two corpora are from different do-\nmains or the two languages are typologically very\ndifferent (Søgaard et al., 2018). To address this\nproblem, several studies have focused on improv-\ning the structural similarity of monolingual spaces\nbefore learning mapping (Zhang et al., 2019; Vuli´c\net al., 2020), but few studies have focused on how\nto leverage the text data itself.\nIn this paper, we show that the pseudo sentences\ngenerated from an unsupervised machine transla-\ntion (UMT) system (Lample et al., 2018c) facili-\ntates the structural similarity without any additional\ncross-lingual resources. In the proposed method,\nthe training data of the source and/or target lan-\nguage are augmented with the pseudo sentences\n(Figure 1).\nWe argue that this method facilitates the struc-\ntural similarity between the source and target em-\nbeddings for the following two reasons. Firstly, the\nsource and target embeddings are usually trained on\nmonolingual corpora. The difference in the content\nof the two corpora may accentuate the structural\ndifference between the two resulting embedding\nspaces, and thus we can mitigate that effect by\nmaking the source and target corpora parallel by au-\ntomatically generated pseudo data. Secondly, in the\nmapping-based method, the source and target em-\nbeddings are trained independently without taking\ninto account the other language. Thus, the embed-\nding structures may not be optimal for CLWEs. We\nargue that pseudo sentences generated by a UMT\narXiv:2006.00262v3  [cs.CL]  3 Jun 2021\nFigure 1: Our framework for training CLWEs using un-\nsupervised machine translation (UMT). We ﬁrst train\nUMT models using monolingual corpora for each lan-\nguage. We then translate all the training corpora and\nconcatenate the outputs with the original corpora, and\ntrain monolingual word embeddings independently. Fi-\nnally, we map these word embeddings on a shared em-\nbedding.\nsystem contain some trace of the original language,\nand using them when training monolingual embed-\ndings can facilitate the structural correspondence\nof the two sets of embeddings.\nIn the experiments using the Wikipedia dump\nin English, French, German, and Japanese, we ob-\nserve substantial improvements by our method in\nthe task of bilingual lexicon induction and down-\nstream tasks without hurting the quality as monolin-\ngual embeddings. Moreover, we carefully analyze\nwhy our method improves the performance, and\nthe result conﬁrms that making the source and tar-\nget corpora parallel does contribute to performance\nimprovement, and also suggests that the generated\ntranslation data contain information about the orig-\ninal language.\n2\nBackground and Related Work\nCross-lingual Word Embeddings\nCLWE methods aim to learn a semantic space\nshared between two languages. Most of the cur-\nrent approaches fall into two types of methods:\njoint-training approaches and mapping-based ap-\nproaches.\nJoint-training approaches jointly train a shared\nembedding space given multilingual corpora with\ncross-lingual supervision such as parallel corpora\n(Gouws et al., 2015; Luong et al., 2015), document-\naligned corpora (Vulic and Moens, 2016), or\nmonolingual corpora along with a word dictionary\n(Duong et al., 2016).\nOn the other hand, mapping-based approaches\nutilize monolingual embeddings that are already\nobtained from monolingual corpora. They assume\nstructural similarity between monolingual embed-\ndings of different languages and attempt to obtain\na shared embedding space by ﬁnding a transfor-\nmation matrix W that maps source word embed-\ndings to the target embedding space (Mikolov et al.,\n2013). The transformation matrix W is usually ob-\ntained by minimizing the sum of squared euclidian\ndistances between the mapped source embeddings\nand target embeddings:\nargmin\nW\n|D|\nX\ni\n∥Wxi −yi∥2 ,\n(1)\nwhere D is a bilingual word dictionary that con-\ntains word pairs (xi, yi) and xi and yi represent\nthe corresponding word embeddings.\nAlthough ﬁnding the transformation matrix W\nis straightforward when a word dictionary is avail-\nable, a recent trend is to reduce the amount of cross-\nlingual supervision or to ﬁnd W in a completely un-\nsupervised manner (Lample et al., 2018b; Artetxe\net al., 2018a). The general framework of unsu-\npervised mapping methods is based on heuristic\ninitialization of a seed dictionary D and iterative\nreﬁnement of the transformation matrix W and\nthe dictionary D, as described in Algorithm 1. In\nour experiment, we use the unsupervised mapping-\nbased method proposed by Artetxe et al. (2018a).\nTheir method is characterized by the seed dictio-\nnary initialized with nearest neighbors based on\nsimilarity distributions of words in each language.\nThese mapping-based methods, however, are\nbased on the strong assumption that the two inde-\npendently trained embedding spaces have similar\nstructures that can be aligned by a linear trans-\nformation. Although several studies have tackled\nimproving the structural similarity of monolingual\nspaces before learning mapping (Zhang et al., 2019;\nVuli´c et al., 2020), not much attention has been paid\nto how to leverage the text data itself.\nInput: The source embeddings X, the target embeddings Y\nOutput: The transformation matrix W\nHeuristically induce an initial seed word dictionary D\nwhile not convergence do\nCompute W given the word dictionary D from the equation (1)\nUpdate the word dictionary D by retrieving cross-lingual nearest neighbors in a shared\nembedding space obtained by W\nend\nreturn W\nAlgorithm 1: The general workﬂow of unsupervised mapping methods\nIn this paper, we argue that we can facilitate\nstructural correspondence of two embedding spaces\nby augmenting the source or/and target corpora\nwith the output from an unsupervised machine\ntranslation system (Lample et al., 2018c).\nUnsupervised Machine Translation\nUnsupervised machine translation (UMT) is the\ntask of building a translation system without any\nparallel corpora (Artetxe et al., 2018b; Lample\net al., 2018a,c; Artetxe et al., 2019b). UMT is\naccomplished by three components: (1) a word-\nby-word translation model learned using unsuper-\nvised CLWEs; (2) a language model trained on the\nsource and target monolingual corpora; (3) a back-\ntranslation model where the model uses input and\nits own translated output as parallel sentences and\nlearn how to translate them in both directions.\nMore speciﬁcally, the initial source-to-target\ntranslation model P 0\ns→t is created by the word-by-\nword translation model and the language model of\nthe target language. Then, P 1\nt→s is learned in a\nsupervised setting using the source original mono-\nlingual corpus paired with the synthetic parallel\nsentences of the target language generated by P 0\ns→t.\nAgain, another source-to-target translation model\nP 1\ns→t is trained with the target original monolin-\ngual corpus and the outputs of P 0\ns→t, and in the\nsame way, the quality of the translation models is\nimproved with an iterative process.\nIn our experiments, we adopt an unsupervised\nphrase-based statistical machine translation (SMT)\nmethod to generate a pseudo corpus because it pro-\nduces better translations than unsupervised neu-\nral machine translation on low-resource languages\n(Lample et al., 2018c). The difference of the unsu-\npervised SMT (USMT) model from its supervised\ncounterpart is that the initial phrase table is de-\nrived based on the cosine similarity of unsupervised\nCLWEs, and the translation model is iteratively im-\nproved by pseudo parallel corpora.\nOur proposed method utilizes the output of a\nUSMT system to augment the training corpus for\nCLWEs.\nExploiting UMT for Cross-lingual\nApplications\nThere is some previous work on how to use UMT\nto induce bilingual word dictionaries or improve\nCLWEs. Artetxe et al. (2019a) explored an effec-\ntive way of utilizing a phrase table from a UMT\nsystem to induce bilingual dictionaries. Marie and\nFujita (2019) generate a synthetic parallel corpus\nfrom a UMT system, and jointly train CLWEs\nalong with the word alignment information (Lu-\nong et al., 2015). In our work, we use the synthetic\nparallel corpus generated from a UMT system not\nfor joint-training but for data augmentation to train\nmonolingual word embeddings for each language,\nwhich are subsequently aligned through unsuper-\nvised mapping. In the following sections, we empir-\nically show that our approach leads to the creation\nof improved CLWEs and analyze why these results\nare achieved.\n3\nExperimental Design\nIn this section, we describe how we obtain\nmapping-based CLWEs using a pseudo parallel\ncorpus generated from UMT. We ﬁrst train UMT\nmodels using the source/target training corpora,\nand then translate them to the machine-translated\ncorpora. Having done that, we simply concate-\nnate the machine-translated corpus with the orig-\ninal training corpus, and learn monolingual word\nembeddings independently for each language. Fi-\nnally, we map these embeddings to a shared CLWE\nspace.\nCorpora\nWe implement our method with two similar lan-\nguage pairs:\nEnglish-French (en-fr), English-\nGerman (en-de), and one distant language pair:\nEnglish-Japanese (en-ja). We use plain texts from\nWikipedia dumps1, and randomly extract 10M sen-\ntences for each language. The English, French, and\nGerman texts are tokenized with the Moses tok-\nenizer (Koehn et al., 2007) and lowercased. For\nJapanese texts, we use kytea2 to tokenize and\nnormalize them3.\nTraining mapping-based CLWEs\nGiven tokenized texts, we train monolingual word\nembeddings using fastText4 with 512 dimen-\nsions, a context window of size 5, and 5 negative\nexamles. We then map these word embeddings on\na shared embedding space using the open-source\nimplementation VecMap5 with the unsupervised\nmapping algorithm (Artetxe et al., 2018a).\nTraining UMT models\nTo implement UMT, we ﬁrst build a phrase ta-\nble by selecting the most frequent 300,000 source\nphrases and taking their 200 nearest-neighbors in\nthe CLWE space following the setting of Lample\net al. (2018c). We then train a 5-gram language\nmodel for each language with KenLM (Heaﬁeld\net al., 2013) and combine it with the phrase ta-\nble, which results in an unsupervised phrase-based\nSMT model. Then, we reﬁne the UMT model\nthrough three iterative back-translation steps. At\neach step, we translate 100k sentences randomly\nsampled from the monolingual data set. We use a\nphrase table containing phrases up to a length of 4\nexcept for initialization. The quality of our UMT\nmodels is indicated by the BLEU scores (Papineni\net al., 2002) in Table 1. We use newstest2014 from\nWMT146 to evaluate En-Fr and En-De translation\naccuracy and the Tanaka corpus7 for En-Ja evalua-\ntion.\n1https://dumps.wikimedia.org/\n2http://www.phontron.com/kytea/\nindex-ja.html\n3We convert all alphabets and numbers to half-width, and\nall katakana to full-width with the mojimoji library https:\n//github.com/studio-ousia/mojimoji\n4https://fasttext.cc\n5https://github.com/artetxem/vecmap\n6http://www.statmt.org/wmt14/\ntranslation-task.html\n7http://www.edrdg.org/wiki/index.php/\nTanakaCorpus\nen - fr\nen - de\nen - ja\n→\n←\n→\n←\n→\n←\n19.2\n19.1\n10.3\n13.7\n3.6\n1.4\nTable 1: BLEU scores of UMT.\nTraining CLWEs with pseudo corpora\nWe then translate all the training corpora with the\nUMT system and obtain machine-translated cor-\npora, which we call pseudo corpora. We concate-\nnate the pseudo corpora with the original corpora,\nand learn monolingual word embeddings for each\nlanguage. Finally, we map these word embeddings\nto a shared CLWE space with the unsupervised\nmapping algorithm.\nModels\nWe compare our method with a baseline with no\ndata augmentation as well as the existing related\nmethods: dictionary induction from a phrase table\n(Artetxe et al., 2019a) and the unsupervised joint-\ntraining method (Marie and Fujita, 2019). These\ntwo methods both exploit word alignments in the\npseudo parallel corpus, and to obtain them we use\nFast Align8 (Dyer et al., 2013) with the default\nhyperparameters. For the joint-training method, we\nadopt bivec9 to train CLWEs with the parameters\nused in Upadhyay et al. (2016) using the pseudo\nparallel corpus and the word alignments. To ensure\nfair comparison, we implement all of these methods\nwith the same UMT system.\n4\nEvaluation of Cross-lingual Mapping\nIn this section, we conduct a series of experiments\nto evaluate our method. We ﬁrst evaluate the per-\nformance of cross-lingual mapping in our method\n( 4.1) and investigate the effect of UMT quality (\n4.2). Then, we analyze why our method improves\nthe bilingual lexicon induction (BLI) performance.\nThrough carefully controlled experiments, we ar-\ngue that it is not simply because of data augmenta-\ntion but because: (1) the generated data makes the\nsource and target corpora (partially) parallel ( 4.3);\n(2) the generated data reﬂects the co-occurrence\nstatistics of the original language ( 4.4).\n4.1\nBilingual Lexicon Induction\nFirst, we evaluate the mapping accuracy of word\nembeddings using BLI. BLI is the task of iden-\n8https://github.com/clab/fast_align\n9https://github.com/lmthang/bivec\nMethod\nsource (en)\ntarget\nen→fr\nfr→en\nen→de\nde→en\nen→ja\nja→en\norig.\npsd.\norig.\npsd.\nMRR\nP@1\nMRR\nP@1\nMRR\nP@1\nMRR\nP@1\nMRR\nP@1\nMRR\nP@1\nBLI from\nphrase table\n✓\n-\n-\n✓\n-\n0.673\n-\n0.524\n-\n0.551\n-\n0.486\n-\n0.311\n-\n0.226\n-\n✓\n✓\n-\n-\n0.509\n-\n0.697\n-\n0.302\n-\n0.542\n-\n0.198\n-\n0.259\n✓\n✓\n✓\n✓\n-\n0.673\n-\n0.522\n-\n0.551\n-\n0.486\n-\n0.311\n-\n0.226\njoint\ntraining\n✓\n-\n-\n✓\n0.640\n0.636\n0.615\n0.634\n0.552\n0.509\n0.545\n0.520\n0.347\n0.295\n0.272\n0.227\n-\n✓\n✓\n-\n0.587\n0.579\n0.643\n0.685\n0.535\n0.491\n0.577\n0.549\n0.279\n0.226\n0.305\n0.249\n✓\n✓\n✓\n✓\n0.654\n0.642\n0.642\n0.650\n0.585\n0.532\n0.520\n0.518\n0.325\n0.267\n0.295\n0.234\nmapping\n✓\n-\n✓\n-\n0.670\n0.612\n0.650\n0.614\n0.579\n0.484\n0.587\n0.488\n0.471\n0.378\n0.364\n0.242\nmapping\n(+ pseudo)\n✓\n-\n✓\n✓\n0.709\n0.666\n0.687\n0.688\n0.656\n0.582\n0.635\n0.563\n0.514\n0.405\n0.436\n0.304\n✓\n✓\n✓\n-\n0.728\n0.684\n0.703\n0.700\n0.647\n0.566\n0.636\n0.562\n0.486\n0.392\n0.407\n0.297\n✓\n✓\n✓\n✓\n0.721\n0.677\n0.696\n0.700\n0.652\n0.574\n0.637\n0.563\n0.497\n0.387\n0.426\n0.300\nTable 2: Comparison with previous approaches in BLI. “orig.” and “psd.” indicate original training corpus and\npseudo corpus. In each cell, the left cell shows the result of MRR, and the right cell shows the result of p@1.\ntifying word translation pairs, and is a common\nbenchmark for evaluating CLWE methods. In these\nexperiments, we use Cross-Domain Similarity Lo-\ncal Scaling (Lample et al., 2018b) as the method\nfor identifying translation pairs in the two embed-\nding spaces. For BLI scores, we adopt the mean\nreciprocal rank (MRR) (Glavaˇs et al., 2019) and\nP@1.\nWe use XLing-Eval10 as test sets for En-Fr and\nEn-Ge. For En-Ja. We create the word dictionaries\nautomatically using Google Translate11, following\nRi and Tsuruoka (2020). Other than BLI from a\nphrase table, we train three sets of embeddings with\ndifferent random seeds and report the average of\nthe results.\nWe compare the proposed method with other\nalternative approaches in BLI as shown in Table\n2. In all the language pairs, the mapping method\nwith pseudo data augmentation achieves better per-\nformance than the other methods. Here, one may\nthink that the greater amount of data can lead to\nbetter performance, and thus augmenting both the\nsource and target corpora shows the best perfor-\nmance. However, the result shows that it is not\nnecessarily the case: for our mapping method, aug-\nmenting only either the source or target, not both,\nachieves the best performance in many language\npairs. This is probably due to the presence of two\npseudo corpora with different natures.\nAs for the two methods using word alignments\n(BLI from phrase table; joint training), we ob-\nserve some cases where these models underper-\nform the mapping methods, especially in English\nand Japanese pairs. We attribute this to our rel-\natively low-resource setting where the quality of\nthe synthetic parallel data is not sufﬁcient to per-\n10https://github.com/codogogo/\nxling-eval\n11https://translate.google.com/\nen - fr\nen - de\nBT\nBLI\nBLEU\nBLI\nBLEU\nstep\nMRR P@1\nMRR P@1\n-\n0.670 0.612\n-\n0.579 0.484\n-\n0\n0.711 0.646 14.7\n0.592 0.508 10.7\n1\n0.714 0.651 18.8\n0.615 0.524 13.5\n2\n0.728 0.684 19.2\n0.647 0.566 13.7\nTable 3: Results of BLI score on CLWEs using pseudo\ncorpus generated from different quality UMTs.\nform these methods which require word alignment\nbetween parallel sentences.\n4.2\nEffect of UMT quality\nTo investigate the effect of UMT quality on our\nmethod, we compare the accuracy of BLI on the\nCLWEs using pseudo data generated from UMT\nmodels of different qualities. As a translator with\nlow performance, we prepare models that perform\nfewer iterations on back-translation (BT). Note that\nwe compare the results on the source-side (English)\nextension, where the quality of the translation is\nnotably different. As shown in Table 3, we ﬁnd that\nthe better the quality of generated data, the better\nthe performance of BLI.\n4.3\nEffect of sharing content\nIn the mapping method, word embeddings are inde-\npendently trained by monolingual corpora that do\nnot necessarily have the same content. As a result,\nthe difference in the corpus contents can hurt the\nstructural similarity of the two resulting embedding\nspaces. We hypothesize that using synthetic paral-\nlel data which have common contents for learning\nword embeddings leads to better structural corre-\nspondence, which improves cross-lingual mapping.\nTo verify the effect of sharing the contents using\nparallel data, we compare the extensions with a\nparallel corpus and a non-parallel corpus. More\nconcretely, we ﬁrst split the original training data\nExtension\nen - fr\nen - de\nen - ja\npseudo\nparallel\n-\n-\n0.621 / 711\n0.502 / 877\n0.426 / 1776\n×\n×\n0.630 / 838\n0.509 / 1714\n0.429 / 2301\n✓\n×\n0.686 / 123\n0.569 / 272\n0.454 / 1050\n✓\n✓\n0.695 / 144\n0.585 / 183\n0.459 / 1024\nTable 4: Results of BLI score and eigenvector similarity. In each cell, the left cell shows the result of BLI, and\nthe right cell shows the result of eigenvector similarity. Each row indicates, from top to bottom, no extension,\nextension with non-pseudo data, extension with non-parallel pseudo data, and extension with parallel pseudo data.\nCorpus\nfr-A\nde-A\nja-A\nen\n0.621 / 711\n0.502 / 877\n0.426 / 1776\nen + pseudo (fr-B)\n0.686 / 123\n0.516 / 315\n0.421 / 2194\nen + pseudo (de-B)\n0.621 / 193\n0.569 / 272\n0.423 / 2173\nen + pseudo (ja-B)\n0.568 / 279\n0.454 / 625\n0.454 / 1050\nTable 5: Results of BLI score and eigenvector similarity. Note that lang-A and pseudo (lang-B) are not parallel.\nof the source and target languages evenly (each\ndenoted as Split A and Split B). As the baseline, we\ntrain CLWEs with Split A. We use the translation of\nSplit A of the target language data for the parallel\nextension of the source data, and Split B for the non-\nparallel extension. Also, we compare them with the\nextension with non-pseudo data, which is simply\nincreasing the amount of the source language data\nby raw text.\nAlong with the BLI score, we show eigenvector\nsimilarity, a spectral metric to quantify the struc-\ntural similarity of word embedding spaces (Søgaard\net al., 2018). To compute eigenvector similarity, we\nnormalize the embeddings and construct the nearest\nneighbor graphs of the 10,000 most frequent words\nin each language. We then calculate their Laplacian\nmatrices L1 and L2 from those graphs and ﬁnd the\nsmallest k such that the sum of the k largest eigen-\nvalues of each Laplacian matrices is < 90% of all\neigenvalues. Finally, we sum up the squared differ-\nences between the k largest eigenvalues from L1\nand L2 and derive the eigen similarity. Note that\nsmaller eigenvector similarity values mean higher\ndegrees of structural similarity.\nTable 4 shows the BLI scores and eigenvector\nsimilarity in each extension setting. The parallel\nextension method shows a slightly better BLI per-\nformance than the non-parallel extension. This\nsupports our hypothesis that parallel pseudo data\nmake word embeddings space more suitable for\nbilingual mapping because of sharing content. In\neigenvector similarity, there is no signiﬁcant im-\nprovement between the parallel and non-parallel\ncorpora. This is probably due to large ﬂuctuations\nin eigenvector similarity values. Surprisingly, the\nresults show that augmentation using pseudo data\nis found to be much more effective than the exten-\nsion of the same amount of original training data.\nThis result suggests that using pseudo data as train-\ning data is useful, especially for learning bilingual\nmodels.\n4.4\nEffect of reﬂecting the co-occurrence\nstatistics of the language\nWe hypothesize that the translated sentences re-\nﬂect the co-occurrence statistics of the original lan-\nguage, which makes the co-occurrence information\non training data similar, improving the structural\nsimilarity of the two monolingual embeddings.\nTo verify this hypothesis, we experiment with\naugmenting the source language with sentences\ntranslated from a non-target language. To examine\nonly the effect of the co-occurrence statistics of\nlanguage and avoid the effects of sharing content,\nwe use the extensions with the non-parallel corpus.\nTable 5 shows that BLI performance and eigen-\nvector similarity improve with the extension from\nthe same target language, but that is not the case if\nthe pseudo corpus is generated from a non-target\nlanguage. These results indicate that our method\ncan leverage learning signals on the other language\nin the pseudo data.\n5\nDownstream Tasks\nAlthough CLWEs were evaluated almost exclu-\nsively on the BLI task in the past, Glavaˇs et al.\n(2019) recently showed that CLWEs that perform\nwell on BLI do not always perform well in other\ncross-lingual tasks. Therefore, we evaluate our\nembeddings on the four downstream tasks: topic\nclassiﬁcation (TC), sentiment analysis (SA), depen-\ndency parsing (DP), and natural language inference\nen-fr\nen-de\nen-ja\nTask\nmapping\nmapping\n(+ pseudo)\njoint\ntraining\nmapping\nmapping\n(+ pseudo)\njoint\ntraining\nmapping\nmapping\n(+ pseudo)\njoint\ntraining\nTC\n79.5\n82.2†\n79.7\n79.0\n79.3\n70.4\n70.4\n71.6†\n66.7\n(92.6)\n(93.3)\n(92.5)\n(91.7)\n(92.0)\n(91.4)\n(92.2)\n(93.3)\n(91.9)\nSA\n69.1\n69.5\n66.3\n63.7\n65.1†\n62.5\n63.5\n62.8\n57.3\n(71.8)\n(71.9)\n(69.9)\n(71.1)\n(70.2)\n(70.3)\n(70.7)\n(70.6)\n(66.8)\nDP\n63.9\n64.3\n64.1\n56.7\n57.0\n55.9\n17.8\n18.1\n17.3\n(73.2)\n(73.5)\n(75.1)\n(73.2)\n(73.6)\n(74.7)\n(72.9)\n(73.3)\n(74.8)\nNLI\n54.4\n54.7\n45.0\n55.7\n56.0\n44.7\n-\n-\n-\n(70.3)\n(70.1)\n(68.6)\n(70.2)\n(70.3)\n(69.7)\n-\n-\n-\nTable 6: Results of Downstream tasks. Numbers in parentheses indicate the score of English validation data. The\nscores indicate averages of 20 experiments with different seeds. Statistically signiﬁcant correlations are marked\nwith a dagger (p <0.01).\n(NLI).\nTopic Classiﬁcation\nThis task is classifying the\ntopics of news articles. We use the MLDoc 12\ncorpus compiled by Schwenk and Li (2018). It\nincludes four topics: CCAT (Corporate / Indus-\ntrial), ECAT (Economics), GCAT (Government /\nSocial), MCAT (Markets). As the classiﬁer, we\nimplemented a simple light-weight convolutional\nneural network (CNN)-based classiﬁer.\nSentiment Analysis\nIn this task, a model is used\nto classify sentences as either having a positive or\nnegative opinion. We use the Webis-CLS-10 corpus\n13. This data consists of review texts for amazon\nproducts and their ratings from 1 to 5. We cast the\nproblem as binary classiﬁcation and deﬁne rating\nvalues 1-2 as “negative” and 4-5 as “positive”, and\nexclude the rating 3. Again, we use the CNN-based\nclassiﬁer for this task.\nDependency Parsing\nWe train the deep biafﬁne\nparser (Dozat and Manning, 2017) with the UD\nEnglish EWT dataset14 (Silveira et al., 2014). We\nuse the PUD treebanks15 as test data.\nNatural Language Inference\nWe use the En-\nglish MultiNLI corpus (Williams et al., 2018) for\ntraining and the multilingual XNLI corpus for eval-\nuation (Conneau et al., 2018). XNLI only covers\nFrench and German from our experiment. We train\nthe LSTM-based classiﬁer (Bowman et al., 2015),\nwhich encodes two sentences, concatenated the rep-\nresentations, and then feed them to a multi-layer\nperceptron.\n12https://github.com/facebookresearch/ MLDoc\n13https://webis.de/data/webis-cls-10. html\n14https://universaldependencies.org/\ntreebanks/en_ewt/index.html\n15https://universaldependencies.org/\nconll17/\nIn each task, we train the model using English\ntraining data with the embedding parameters ﬁxed .\nWe then evaluate the model on the test data in other\ntarget languages.\nResult and Discussion\nTable 6 shows the test set accuracy of downstream\ntasks. For topic classiﬁcation, our method obtains\nthe best results in all language pairs. Especially\nin En-Fr and En-Ja, a signiﬁcant difference is ob-\ntained in Students t-test. For sentiment analysis, we\nobserve a signiﬁcant improvement in En-De, but\ncannot observe consistent trends in other languages.\nFor dependency parsing and natural language infer-\nence, we observe a similar trend where the perfor-\nmance of our method outperforms other methods,\nalthough no signiﬁcant difference is observed in\nthe t-test. The cause of the lower performance of\njoint-training compared with the mapping method\nis presumably due to the poor quality of synthetic\nparallel data as described in 4.1. In summary, given\nthe same amount of data, the CLWEs obtained from\nour method tend to show higher performance not\nonly in BLI but also in downstream tasks compared\nwith other alternative methods, although there is\nsome variation.\n6\nAnalysis\nMonolingual Word Similarity\nOur method\nuses a noisy pseudo corpus to learn monolingual\nword embeddings, and it might hurt the quality\nof monolingual embeddings. To investigate this\npoint, we evaluate monolingual embeddings with\nthe word similarity task. This task evaluates the\nquality of monolingual word embeddings by mea-\nsuring the correlation between the cosine similarity\nin a vector space and manually created word pair\nsimilarity. We use simverb-350016 (Gerz et al.,\n16http://people.ds.cam.ac.uk/dsg40/simverb.html\nen-fr\nen-de\nen-ja\ncorpus\nen\nfr\nen\nde\nen\nja\norigin\n1.60 × 10−3\n1.63 × 10−3\n1.51 × 10−3\n3.78 × 10−3\n1.52 × 10−3\n1.03 × 10−3\npseudo\n0.57 × 10−3\n0.57 × 10−3\n0.66 × 10−3\n0.59 × 10−3\n0.19 × 10−3\n0.17 × 10−3\nTable 7: Type-token ratio of the training corpus (origin) and the pseudo-corpus (pseudo)\ncorpus\nsimverb-3500\nmen\nen\n0.259\n0.763\nen + pseudo (fr)\n0.260\n0.767\nen + pseudo (de)\n0.253\n0.768\nen + pseudo (ja)\n0.220\n0.760\nTable 8: Results of word similarity. The scores indicate\naverages of 3 experiments with different seeds.\n2016) consisting of 3500 verb pairs and men17\n(Bruni et al., 2014) consisting of 3000 frequent\nwords extracted from web text.\nTable 8 shows the results of word similarity. The\nscores of monolingual word embeddings using a\nFrench and German pseudo corpus are maintained\nor improved, while they decrease in Japanese. This\nsuggests that the quality of monolingual word em-\nbeddings could be hurt due to the low quality of the\npseudo corpus or differences in linguistic nature.\nNevertheless, the proposed method improves the\nperformance of En-Ja’s CLWE, which suggests that\nthe monolingual word embeddings created with a\npseudo corpus have a structure optimized for cross-\nlingual mapping.\nApplication to UMT\nUMT is one of the impor-\ntant applications of CLWEs. Appropriate initializa-\ntion with CLWEs is crucial to the success of UMT\n(Lample et al., 2018c). To investigate how CLWEs\nobtained from our method affect the performance\nof UMTs, we compare the BLEU scores of UMTs\ninitialized with CLWEs with and without a pseudo\ncorpus at each iterative step. As shown in Table 9,\nwe observe that initialization with CLWE using the\npseudo data result in a higher BLEU score in the\nﬁrst step but does not improve the score at further\nsteps compared to the CLWE without the pseudo\ndata. Marie and Fujita (2019) also demonstrate the\nsame tendency in the CLWE with joint-training.\nTo investigate this point, we compare the lexical\ndensities of the training corpus and the pseudo-\ncorpus used in the above experiments ( 4, 5) using\ntype-token ratio (Table 7). The results demonstrate\nthat the pseudo corpus has a smaller vocabulary\nper word than the training corpus, and thus it is\n17https://staff.fnwi.uva.nl/e.bruni/MEN\nBT\nen→fr\nfr→en\nen→fr\nfr→en\nstep\nCLWE (no pseudo)\nCLWE (+ pseudo)\n0\n14.7\n14.8\n1\n16.7\n18.8\n16.1\n18.2\n2\n18.8\n19.2\n18.2\n18.5\n3\n19.2\n19.1\n18.6\n18.8\nTable 9: BLEU scores of UMT at each back-translation\nstep in En-Fr with a phrase table induced using differ-\nent CLWEs.\nstandardized to some extent as reported in Van-\nmassenhove et al. (2019). As a result, speciﬁc\nwords might be easily mapped in CLWEs using a\npseudo corpus18, and then the translation model\nmakes it easier to translate phrases in more speciﬁc\npatterns. Hence, the model cannot generate diverse\ndata during back-translation, and the accuracy is\nnot improved due to easy learning.\n7\nConclusion and Future Work\nIn this paper, we show that training cross-lingual\nword embeddings with pseudo data augmentation\nimproves performance in BLI and downstream\ntasks. We analyze the reason for this improve-\nment and found that the pseudo corpus reﬂects the\nco-occurrence statistics and content of the other\nlanguage and that the property makes the structure\nof the embedding suitable for cross-lingual word\nmapping.\nRecently, Vuli´c et al. (2019) have shown that\nfully unsupervised CLWE methods fails in many\nlanguage pairs and argue that researchers should\nnot focus too much on the fully unsupervised set-\ntings. Still, our ﬁndings that improve structural\nsimilarity of word embeddings in the fully unsu-\npervised setting could be useful in semi-supervised\nsettings, and thus we would like to investigate this\ndirection in the future.\n18In a preliminary experiment, we investigated the variation\nin performance of cross-lingual mapping with and without\npseudo according to the frequency of words in the source\nlanguage, but there was little correlation between them.\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018a. A robust self-learning method for fully un-\nsupervised cross-lingual mappings of word embed-\ndings. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 789–798.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2019a.\nBilingual lexicon induction through unsu-\npervised machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5002–5007.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2019b. An effective approach to unsupervised ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 194–203.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018b. Unsupervised Neural Ma-\nchine Translation. In Proceedings of the 5th Interna-\ntional Conference on Learning Representations.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642.\nElia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.\nMultimodal distributional semantics. Journal of Ar-\ntiﬁcial Intelligence Research.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018.\nXNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485.\nTimothy Dozat and Christopher D Manning. 2017.\nDeep Biaﬁne Attention for Neural Dependency Pars-\ning. In Proceedings of the International Conference\non Learning Representations.\nLong Duong, Hiroshi Kanayama, Tengfei Ma, Steven\nBird, and Trevor Cohn. 2016.\nLearning Crosslin-\ngual Word Embeddings without Bilingual Corpora.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1285–1295.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013.\nA simple, fast, and effective reparameter-\nization of ibm model 2.\nIn Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 644–648, At-\nlanta, Georgia.\nDaniela Gerz, Ivan Vuli´c, Felix Hill, Roi Reichart, and\nAnna Korhonen. 2016.\nSimVerb-3500: A large-\nscale evaluation set of verb similarity. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2173–2182.\nGoran Glavaˇs, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 710–721.\nStephan Gouws, Yoshua Bengio, and Greg Corrado.\n2015. BilBOWA: Fast Bilingual Distributed Repre-\nsentations without Word Alignments.\nIn Proceed-\nings of the 32nd International Conference on Ma-\nchine Learning, volume 37, pages 748–756, Lille,\nFrance.\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable modiﬁed\nKneser-Ney language model estimation. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 690–696.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan,\nWade Shen,\nChristine Moran,\nRichard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the As-\nsociation for Computational Linguistics Companion\nVolume Proceedings of the Demo and Poster Ses-\nsions, pages 177–180.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018a.\nUnsupervised\nmachine translation using monolingual corpora only.\nIn International Conference on Learning Represen-\ntations.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv Jgou. 2018b.\nWord translation without parallel data. In Interna-\ntional Conference on Learning Representations.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018c.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5039–5049.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015.\nBilingual Word Representations with\nMonolingual Quality in Mind. In Proceedings of the\n1st Workshop on Vector Space Modeling for Natural\nLanguage Processing, pages 151–159.\nBenjamin Marie and Atsushi Fujita. 2019.\nUnsuper-\nvised joint training of bilingual word embeddings.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n3224–3230.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.\nExploiting Similarities among Languages for Ma-\nchine Translation. Computing Research Repository,\narXiv:1309.4168.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318.\nBarun Patra, Joel Ruben Antony Moniz, Sarthak Garg,\nMatthew R. Gormley, and Graham Neubig. 2019.\nBilingual Lexicon Induction with Semi-supervision\nin Non-Isometric Embedding Spaces. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 184–193.\nRyokan Ri and Yoshimasa Tsuruoka. 2020.\nRevisit-\ning the context window for cross-lingual word em-\nbeddings. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 995–1005.\nHolger Schwenk and Xian Li. 2018. A corpus for mul-\ntilingual document classiﬁcation in eight languages.\nIn Proceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018).\nNatalia Silveira,\nTimothy Dozat,\nMarie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor,\nJohn Bauer, and Chris Manning. 2014. A Gold Stan-\ndard Dependency Corpus for English. In Proceed-\nings of the Ninth International Conference on Lan-\nguage Resources and Evaluation (LREC’14), pages\n2897–2904.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli´c.\n2018.\nOn the Limitations of Unsupervised Bilin-\ngual Dictionary Induction.\nIn Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n778–788.\nShyam Upadhyay, Manaal Faruqui, Chris Dyer, and\nDan Roth. 2016. Cross-lingual models of word em-\nbeddings: An empirical comparison. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1661–1670.\nEva Vanmassenhove, Dimitar Shterionov, and Andy\nWay. 2019. Lost in translation: Loss and decay of\nlinguistic richness in machine translation. In Pro-\nceedings of Machine Translation Summit XVII Vol-\nume 1: Research Track, pages 222–232.\nIvan Vuli´c, Goran Glavaˇs, Roi Reichart, and Anna Ko-\nrhonen. 2019.\nDo we really need fully unsuper-\nvised cross-lingual embeddings? In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4407–4418.\nIvan Vuli´c, Anna Korhonen, and Goran Glavaˇs. 2020.\nImproving bilingual lexicon induction with unsuper-\nvised post-processing of monolingual word vector\nspaces. In Proceedings of the 5th Workshop on Rep-\nresentation Learning for NLP, pages 45–54.\nIvan Vulic and Marie-Francine Moens. 2016. Bilingual\nDistributed Word Representations from Document-\naligned Comparable Data. Journal of Artiﬁcial In-\ntelligence Research, 55(1):953–994.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122.\nMichelle Yuan, Mozhi Zhang, Benjamin Van Durme,\nLeah Findlater, and Jordan Boyd-Graber. 2020. In-\nteractive reﬁnement of cross-lingual word embed-\ndings. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 5984–5996.\nMozhi Zhang, Keyulu Xu, Ken-ichi Kawarabayashi,\nStefanie Jegelka, and Jordan Boyd-Graber. 2019.\nAre girls neko or sh¯ojo? cross-lingual alignment of\nnon-isomorphic embeddings with iterative normal-\nization.\nIn Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3180–3189.\n8\nAppendix\nA\nThe hyperparameters for downstream\ntasks\nA.1\nDocument Classiﬁcation and Sentiment\nAnalysis\nhyperparameters\nCNN Classiﬁer\nnumber of ﬁlters\n8\nngram ﬁlter sizes\n2, 3, 4, 5\nMLP hidden size\n32\nTraining\noptimizer\nAdam\nlearning rate\n0.001\nlr scheduler\nhalved each time the dev score stops improving\npatience\n3\nbatch size\n50\nA.2\nDependency Parsing\nhyperparameters\nGraph-based Parser\nLSTM hidden size\n200\nLSTM number of layers\n3\ntag representation dim\n100\narc representation dim\n500\npos tag embedding dim\n50\nTraining\noptimizer\nAdam\nlearning rate\n0.001\nlr scheduler\nhalved each time the dev score stops improving\npatience\n3\nbatch size\n32\nA.3\nNatural Language Inference\nhyperparameters\nSentence Encoder\nLSTM hidden size\n300\nLSTM number of layers\n2\nTraining\noptimizer\nAdam\nlearning rate\n0.001\nlr scheduler\nhalved each time the dev score stops improving\npatience\n3\nbatch size\n64\n",
  "categories": [
    "cs.CL"
  ],
  "published": "2020-05-30",
  "updated": "2021-06-03"
}