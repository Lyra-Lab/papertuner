{
  "id": "http://arxiv.org/abs/2006.02610v1",
  "title": "Semi-supervised and Unsupervised Methods for Heart Sounds Classification in Restricted Data Environments",
  "authors": [
    "Balagopal Unnikrishnan",
    "Pranshu Ranjan Singh",
    "Xulei Yang",
    "Matthew Chin Heng Chua"
  ],
  "abstract": "Automated heart sounds classification is a much-required diagnostic tool in\nthe view of increasing incidences of heart related diseases worldwide. In this\nstudy, we conduct a comprehensive study of heart sounds classification by using\nvarious supervised, semi-supervised and unsupervised approaches on the\nPhysioNet/CinC 2016 Challenge dataset. Supervised approaches, including deep\nlearning and machine learning methods, require large amounts of labelled data\nto train the models, which are challenging to obtain in most practical\nscenarios. In view of the need to reduce the labelling burden for clinical\npractices, where human labelling is both expensive and time-consuming,\nsemi-supervised or even unsupervised approaches in restricted data setting are\ndesirable. A GAN based semi-supervised method is therefore proposed, which\nallows the usage of unlabelled data samples to boost the learning of data\ndistribution. It achieves a better performance in terms of AUROC over the\nsupervised baseline when limited data samples exist. Furthermore, several\nunsupervised methods are explored as an alternative approach by considering the\ngiven problem as an anomaly detection scenario. In particular, the unsupervised\nfeature extraction using 1D CNN Autoencoder coupled with one-class SVM obtains\ngood performance without any data labelling. The potential of the proposed\nsemi-supervised and unsupervised methods may lead to a workflow tool in the\nfuture for the creation of higher quality datasets.",
  "text": "1\nSemi-supervised and Unsupervised Methods for\nHeart Sounds Classiﬁcation in Restricted Data\nEnvironments\nBalagopal Unnikrishnan1, Pranshu Ranjan Singh1, Xulei Yang2, and Matthew Chin Heng Chua1\nAbstract—Automated heart sounds classiﬁcation is a much-\nrequired diagnostic tool in the view of increasing incidences of\nheart related diseases worldwide. In this study, we conduct a com-\nprehensive study of heart sounds classiﬁcation by using various\nsupervised, semi-supervised and unsupervised approaches on the\nPhysioNet/CinC 2016 Challenge dataset. Supervised approaches,\nincluding deep learning and machine learning methods, require\nlarge amounts of labelled data to train the models, which are\nchallenging to obtain in most practical scenarios. In view of the\nneed to reduce the labelling burden for clinical practices, where\nhuman labelling is both expensive and time-consuming, semi-\nsupervised or even unsupervised approaches in restricted data\nsetting are desirable. A GAN based semi-supervised method is\ntherefore proposed, which allows the usage of unlabelled data\nsamples to boost the learning of data distribution. It achieves\na better performance in terms of AUROC over the supervised\nbaseline when limited data samples exist. Furthermore, several\nunsupervised methods are explored as an alternative approach\nby considering the given problem as an anomaly detection\nscenario. In particular, the unsupervised feature extraction using\n1D CNN Autoencoder coupled with one-class SVM obtains good\nperformance without any data labelling. The potential of the\nproposed semi-supervised and unsupervised methods may lead\nto a workﬂow tool in the future for the creation of higher quality\ndatasets.\nIndex Terms—Heart Sounds Classiﬁcation, Semi-supervised\nLearning, Unsupervised Learning, Generative Adversarial Net-\nworks, One-Class Support Vector Machines.\nI. INTRODUCTION\nC\nARDIOVASCULAR diseases (CVDs) have been the\nmain cause of death globally. 17.9 million deaths have\nbeen attributed to CVDs, which represents 31% of all global\ndeaths [1]. There is a need for methods for ﬁrst hand examina-\ntion of cardiovascular system. Auscultation of the heart sounds\nor Phonocardiogram (PCG) signals is a crucial component of\nphysical examination and can help detect cardiac conditions\nsuch as arrhythmia, valve disease, heart failure, and more\n[2]. Heart sound analysis by auscultation has been done by\nphysicians to assess the heart condition over a period of\ntime. However, designing an accurate and automated system\nfor detection of abnormal heart sounds is challenging due to\nunavailability of rigorously validated and high-quality heart\nsounds datasets [3].\nApart from PCG signals, Electrocardiogram (ECG) signals\nhas been used for detecting arrhythmia, myocardial ischemia\n1 Institute of Systems Science, National University of Singapore\n2 Institute for Infocomm Research, A*STAR Singapore\nand chronic alterations [4 - 5]. Although ECG signals can re-\nveal various intricate and abnormal heart behaviors, symptoms\nsuch as heart murmurs are concealed from an ECG signal [6].\nThe use of heart sounds to detect various heart abnormalities\nhas led to the development of wide range of algorithms. In [7],\nPCG signals undergo digital subtraction analysis to detect and\ncharacterize heart murmurs. Automated classiﬁcation methods\nof heart sounds involve approaches such as Support Vector\nMachines (SVM) [8], Neural Networks [9], Probability based\nmethods [10] and ensemble of various classiﬁers [11].\nThe design of supervised methods for heart sounds clas-\nsiﬁcation requires large amount of labelled data. However,\nit is often difﬁcult, expensive, or time-consuming to obtain\nadditional labelled data [12]. There are challenges in obtaining\npatients data in the medical domain. Furthermore, multiple\nphysicians have to perform labelling in order to achieve a\ncommon consensus, etc. Semi-supervised learning and active\nlearning methods deal with this problem by utilizing available\nunlabelled data along with the labelled data to build better\nclassiﬁer models [13]. Chamberlain, Daniel, et al. demonstrate\nautomatic lung sounds classiﬁcation using a semi-supervised\ndeep learning algorithm [14]. Transfer learning for supervised\nheart sounds classiﬁcation and data augmentation for minority\nclass (abnormal category) samples are some of the areas\nbeing explored to improve the performance over traditional\nsupervised classiﬁcation methods [15 - 16].\nIn most cases, the abnormal samples are much lesser\nthan normal samples. This leads to a class imbalance when\nperforming classiﬁcation tasks [17]. It is both time-consuming\nand expensive to collect the abnormal samples.There have\nbeen works that perform clustering on the extracted features\nfrom the heart sounds, followed by classiﬁcation [18]. In\nanomaly detection methods, the model is trained only on\nnormal samples, but tested with both normal and abnormal\nsamples [19].\nIn this work, the focus is on exploring current and new\nsupervised, semi-supervised and unsupervised methods for\nheart sounds classiﬁcation. The main contributions of this\nwork are:\n(i) Analysis of the performance of various supervised methods\nfor heart sounds classiﬁcation;\n(ii) Utilization of the Generative Adversarial Network (GAN)\n-based semi-supervised technique to obtain better performance\nin terms of Area Under the Receiver Operating Characteristic\ncurve (AUROC) as compared to the supervised benchmark,\nand\narXiv:2006.02610v1  [cs.CV]  4 Jun 2020\n2\n(iii) Learning of latent representations from features of heart\nsounds using a 1D Convolutional Neural Network (CNN)\nmodel (Unsupervised method) and anomaly detection algo-\nrithms, and evaluate the classiﬁcation performance using AU-\nROC metric.\nThe methods and experimental analysis are discussed in detail\nin the following sections.\nII. METHODOLOGY\nThis section describes the data and the methods used in this\nstudy. The sub-section Dataset and Data Preparation describe\nthe dataset used and the feature extraction methods for heart\nsounds, respectively. Subsequent sub-sections explain the tech-\nniques used for heart sound classiﬁcation using supervised,\nsemi-supervised and unsupervised methods.\nA. Dataset\nThe heart sounds dataset used for this study was provided\nby the 2016 PhysioNet/Computing in Cardiology Challenge\n[2]. It contains 3,240 labelled heart sounds recordings. The\ndataset is divided into two classes, Normal and Abnormal\nsamples. Fig. 1 shows the heart sounds signal for normal and\nabnormal sample. The duration of heart sounds signal ranges\nfrom 5 seconds (short-period) to 120 seconds (long period).\nThis dataset was obtained by combining various heart sounds\ndatabases. It consists of 6 sub-datasets labelled A, B, C, D, E\nand F as shown in the Fig. 2.\nThe heart sounds recordings in this dataset were collected\nfrom nine different locations of the body. The four major\nlocations are the aortic area, pulmonic area, tricuspid area\nand mitral area.The normal recordings correspond to healthy\nsubjects whereas the abnormal ones were obtained from pa-\ntients with conﬁrmed cardiac diagnosis. The typical illnesses\nof the patients were heart valve defects and coronary artery.\nThe presence of noise in some samples were due to the\nuncontrolled environment of the recordings. The noise sources\nincludes talking, stethoscope motion, breathing and intestinal\nsounds.\nB. Data Preparation\nFor this study, various features obtained from heart sounds\nsignals are used for training different models. The raw signal\nundergoes pre-processing steps such as padding and pruning.\nFor padding operation, all the samples are zero-padded to\nachieve the length of the maximum length signal (120 seconds)\nin the dataset. For pruning operation, all the signals are\ntruncated to achieve the length of minimum length signal (5\nseconds) in the dataset.\nThe different types of features extracted from the heart\nsounds signal are shown in Fig. 3. For semi-supervised meth-\nods, the raw processed signal is used as input. For supervised\nmethods, both the padded and pruned signals are used to\nobtain the spectrogram and mel-spectrogram features. Both\nspectrogram and mel-spectrogram features are plotted with\nthe time as the x-axis and frequency as y-axis. These plots\nare saved in form of color images having resolution 64 x 64\nx 3 and 128 x 128 x 3 respectively.\nFig. 1.\nThe heart sounds signal for normal class (top) and abnormal class\n(bottom). The x-axis represents the time-steps and y-axis represents the signal\nvalue. The sampling rate of the signal is 2000 Hz.\nFig. 2.\nThe 2016 PhysioNet/Computing in Cardiology Challenge dataset\ndistribution. The dataset was obtained by combining heart sounds databases\ncollected independently by various research teams. The individual datasets\nare labelled A, B, C, D, E and F. The distribution of normal and abnormal\nsamples in each sub-dataset is different.\nAudio features such as Mel-Frequency Cepstral Coefﬁ-\ncients (MFCCs), Chroma [20], mel-scaled spectrogram (mel-\nspectrogram), spectral contrast [21] and tonal centroid features\n(tonnetz) [22] were extracted from the heart sounds signals.\nMFCCs, Chroma, mel-spectrogram, Spectral Contrast and\nTonnetz contribute 40, 12, 128, 7 and 6 features, respectively.\nThese features are appended to form a combined feature list\nwith 193 features. These extracted audio features are used in\nsupervised methods and unsupervised methods (for anomaly\ndetection). Since there is a class imbalance, oversampling of\nminority class (Abnormal class) is performed using Synthetic\nMinority Over-sampling Technique (SMOTE) [23]. This over-\nsampling is performed on the audio features.\n3\nFig. 3.\nFeature Extraction from Heart Sounds Signal. Various features are\nextracted for supporting various techniques of heart sounds classiﬁcation.\nspectrogram and Mel-spectrogram are obtained by converting the PCG signals\nto image. Audio features are obtained by appending speciﬁc features such as\nMFCC sequence, Chroma, Mel-spectrogram, Contrast and Tonnetz.\nC. Supervised Methods for Heart Sounds Classiﬁcation\nThe various supervised methods used for performing heart\nsounds classiﬁcation can be grouped in four clusters:\n(i) Transfer Learning using pre-trained deep learning models\non spectrogram/ Mel-spectrogram images;\n(ii) Custom CNN on spectrogram images;\n(iii) Deep Learning models on extracted audio features, and\n(iv) Machine Learning models on extracted audio features.\nThe details of the methods are described below.\n1) Transfer Learning using Pre-trained Deep Learning\nModels on Spectrogram/ Mel-spectrogram Images: Transfer\nlearning in CNNs has shown that the image representations\nlearnt over a large-scale labelled dataset can be transferred to\nclassiﬁcation tasks over limited data samples [24]. ResNet-50\n[25], Inception-v3 [26] and DenseNet-121 [27] have shown\nstate-of-the-art classiﬁcation results on the ImageNet dataset.\nThe spectrograms and mel-spectrograms obtained from heart\nsounds signals are converted to 64 x 64 x 3 images. (from\nData Preparation sub-section) These images are trained on\nImageNet pre-trained ResNet-50, Inception-v3 and DenseNet-\n121 models. The output of the ﬁnal convolutional layer of\nthree models is fed to a fully-connected single node layer for\nclassiﬁcation into Normal or Abnormal class.\n2) Custom CNN on Spectrogram Images: The spectrogram\nobtained from the heart sounds signal is converted to 128 x\n128 x 3 image. These images are fed to a custom designed\nCNN network which follows VGG [28] like architecture. The\ncustom architecture of Custom CNN is provided in the Table\nI. The input spectrogram image passes through a series of\nconvolution and pooling layers, and dense layers towards\nthe end of the network to output the class of the heart\nsounds signal. ReLU activation [29] has been used for the\nconvolutional and dense layers, except for the ﬁnal dense layer,\nwhich uses Sigmoid activation. Dropout layers are added to\nprevent the model from over-ﬁtting to the training set [30].\n3) Deep Learning Models on Extracted Audio Features:\nThe audio features extracted from the heart sounds signals\nTABLE I\nCUSTOM CNN ARCHITECTURE ON SPECTROGRAM IMAGES\nLayers\nAttributes\nConvolution 2D\n16 ﬁlters, 3 x 3 kernel, ReLU activation,\npadding=same\nConvolution 2D\n16 ﬁlters, 3 x 3 kernel, ReLU activation,\npadding=same\nMaxPool 2D\n2 x 2 kernel, stride=2\nConvolution 2D\n32 ﬁlters, 3 x 3 kernel, ReLU activation,\npadding=same\nConvolution 2D\n32 ﬁlters, 3 x 3 kernel, ReLU activation,\npadding=same\nMaxPool 2D\n2 x 2 kernel, stride=2\nConvolution 2D\n64 ﬁlters, 3 x 3 kernel, ReLU activation,\npadding=same\nConvolution 2D\n64 ﬁlters, 3 x 3 kernel, ReLU activation,\npadding=same\nMaxPool 2D\n2 x 2 kernel, stride=2\nConvolution 2D\n128 ﬁlters, 3 x 3 kernel, ReLU activation,\npadding=same\nConvolution 2D\n128 ﬁlters, 3 x 3 kernel, ReLU activation,\npadding=same\nMaxPool 2D\n2 x 2 kernel, stride=2\nFlatten & Dropout\ndropout rate=0.25\nDense\n256 nodes, ReLU activation\nDropout\ndropout rate=0.25\nDense\n1 node, Sigmoid activation\nTABLE II\nNEURAL NETWORK WITH LSTM UNITS ON EXTRACTED AUDIO\nFEATURES\nLayers\nAttributes\nLSTM\n128\nunits,\ndropout=0.2,\nrecurrent\ndropout=0.25\nDropout\ndropout rate=0.25\nLSTM\n64\nunits,\ndropout=0.2,\nrecurrent\ndropout=0.25\nDense\n1 node, Sigmoid activation\nundergo oversampling using SMOTE to obtain the equal\nnumber of samples for both Normal and Abnormal classes.\nThese features are then modeled using Dense Neural Network\n(Dense NN), Neural Network with Long Short Term Mem-\nory units (LSTM NN) [31] and 1D CNN. The Dense NN\narchitecture takes a feature list of dimension 193 as input as\npasses it through a series of densely connected layers. The\nDense NN architecture consists of 4 dense layers, each having\n128 nodes with ReLU activation, followed by a single node\ndensely connected layer with Sigmoid activation. The LSTM\nNN architecture takes a feature list of dimension 193 x 1\nas input and passes it through a series of LSTM units and\ndensely connected layers. The architecture for LSTM NN is\nprovided in Table II. The LSTM units are useful in modeling\nsequential data and ﬁnal densely connected node is used for\nthe classiﬁcation. The 1D CNN architecture takes a feature list\nof dimension 193 x 1 as input and passes it through a series\nof 1D convolutional, 1D pooling and densely connected layers\nas depicted in Table III.\n4) Machine Learning Models on Extracted Audio Features:\nThe extracted audio features are used to ﬁt machine learning\nmodels such as Decision Tree, SVM, Random Forest and\n4\nTABLE III\n1D CNN ON EXTRACTED AUDIO FEATURES\nLayers\nAttributes\nConvolution 1D\n128 ﬁlters, kernel size=3, ReLU activation\nConvolution 1D\n128 ﬁlters, kernel size=3, ReLU activation\nMaxPool 1D\nkernel size=3, stride=3\nConvolution 1D\n256 ﬁlters, kernel size=3, ReLU activation\nConvolution 1D\n256 ﬁlters, kernel size=3, ReLU activation\nMaxPool 1D\nkernel size=3, stride=3\nConvolution 1D\n512 ﬁlters, kernel size=3, ReLU activation\nConvolution 1D\n512 ﬁlters, kernel size=3, ReLU activation\nFlatten\nDense\n256 nodes, ReLU activation\nDense\n128 nodes, ReLU activation\nDense\n1 node, Sigmoid activation\nGradient Boosting. For each machine learning method, the\nmodel was ﬁtted on training set and hyper-parameters of the\nmodels were tuned using a validation set and ﬁnally evaluated\non a test set. The various hyper-parameters and their values\nfor each machine learning model are provided in Table IV. For\ndecision tree modeling, the hyper-parameters are Criterion (the\nfunction that measures the quality of the split), max depth\n(the maximum depth of the decision tree), max leaf nodes\n(maximum number of nodes allowed in leaf/terminal positions)\nand class weight. The class weight hyper-parameter represents\nthe ratio (abnormal:normal ratio) of weights associated with\nthe classes. The hyper-parameters used in SVM modeling are\nC (penalty factor of error term), kernel (the type of kernel used\nin the algorithm), gamma (kernel coefﬁcient) and class weight.\nThe kernel used in SVM model was radial basis function (rbf).\nThe hyper-parameters used in Random Forest modeling are\nCriterion, Number of estimators (the number of trees in the\nforest), max depth and max leaf nodes. For gradient boosting\nmodeling, the hyper-parameters used are Number of estimators\n(the number of boosting stages), max depth and learning rate\n(the reduces contribution of each tree by this rate). The above\nhyper-parameters are tuned for each machine learning model\nfor two cases, without SMOTE balancing and with SMOTE\nbalancing.\nD. Semi-supervised Method: Generative Adversarial Network\nGenerative adversarial networks (GANs) has provided a\nway for generating fake samples and utilizing them for other\ntasks [32]. The semi-supervised method makes use of GANs\nto utilize the unlabelled data samples. The semi supervised\nmodels has access to both the labelled and unlabelled data\nfrom the training set. In theory, such models should perform\nbetter than the supervised methods as they now have access\nto unlabelled training data - provided the semi-supervised\nsmoothness assumption holds i.e. if two points x1, x2 are\nclose in a high density region, their labels y1, y2 are also be\ncloser [33]. This class of semi-supervised algorithms are called\ngenerative models and they are generally trained in a coupled\nfashion, similar to the training procedure of GANs. Fig. 4\nand Fig. 5 provide the GAN training and testing framework.\nA combined loss function as mentioned in equations (1) to\nTABLE IV\nHYPER-PARAMETERS FOR VARIOUS MACHINE LEARNING MODELS ON\nEXTRACTED AUDIO FEATURES\nML Model\nHyper-parameter\nValue\nDecision Tree\nCriterion\nEntropy\nMax Depth\n40\nMax Leaf Nodes\n40\nClass weight\n5:1\nDecision Tree (with SMOTE)\nCriterion\nEntropy\nMax Depth\n60\nMax Leaf Nodes\n40\nSVM\nC\n0.07\nKernel\nrbf\nGamma\nauto\nClass weight\n19:3\nSVM (with SMOTE)\nC\n70\nKernel\nrbf\nGamma\nauto\nRandom Forest\nCriterion\nEntropy\nNo. of estimators\n400\nMax Depth\n10\nMax Leaf Nodes\n50\nClass weight\n5:1\nRandom Forest (with SMOTE)\nCriterion\nEntropy\nNo. of estimators\n100\nMax Depth\n10\nMax Leaf Nodes\n64\nGradient Boosting\nNo. of estimators\n400\nMax Depth\n7\nLearning rate\n0.1\nGradient Boosting (with SMOTE)\nNo. of estimators\n400\nMax Depth\n6\nLearning rate\n0.1\n(4) is used to train the discriminator and generator, and the\nreformulation trick is used as depicted in [34].\nThe generator is trained by matching the features of the\ngenerated samples and the real samples. The supervised loss is\nsimilar to the cross-entropy loss in K-class classiﬁcation prob-\nlems and the unsupervised loss helps in distinguishing between\nreal and fake samples. This coupled training in an adversarial\nsetting is used to train the semi-supervised network. The\nnetwork architecture for the discriminator and generator are\nprovided in Table V and Table VI. 1D convolutions are used in\nboth discriminator and generator network architecture. These\nare highly effective as convolution operations are translation\nand scale invariant and can pickup relevant features anywhere\nwithin the input. This is useful since the heart sounds are\nnot segmented or aligned in any fashion. The ﬁrst 5 seconds\nof the heart sounds data is directly taken as input for the\nsemi-supervised method. In the overall training setup, minimal\namount of annotation or labelling is required.\nE. Unsupervised Method: Anomaly Detection\nFor the purpose of obtaining good performance in restricted\ndata environments, the method of anomaly detection was\nexplored. In anomaly detection scenario, the model is trained\nusing just the normal class samples. Any abnormality or\ndeviation from normality is considered as a disease case\n(abnormal class). This has two major advantages, (i) it can\nperform the entire training without the need of any labels (need\n5\nFig. 4. Semi-supervised GAN Training Framework. The Generator (G) takes a random noise z as input and produces a generated sample xgenerated. The\nDiscriminator (D) takes the generated samples, labelled real samples (xlabelled, y) and unlabelled real samples xunlabelled and produces the prediction of\nthe class label and the Intermediate layer output M(x).\nFig. 5. Semi-supervised GAN Testing Framework. During the testing phase,\nonly Discriminator (D) is used. The test sample is fed to the Discriminator to\nobtain the class prediction. The predicted class along with the ground truth\nclass is used to obtain the AUROC metric.\nTABLE V\nSEMI-SUPERVISED GAN DISCRIMINATOR ARCHITECTURE\nLayers\nAttributes\nConvolution 1D\n64 ﬁlters, kernel size=8, stride=1, Leaky\nReLU activation\nConvolution 1D\n64 ﬁlters, kernel size=8, stride=2, Leaky\nReLU activation\nConvolution 1D\n128 ﬁlters, kernel size=8, stride=2, Leaky\nReLU activation\nConvolution 1D\n256 ﬁlters, kernel size=8, stride=2, Leaky\nReLU activation\nConvolution 1D\n256 ﬁlters, kernel size=8, stride=2, Leaky\nReLU activation\nConvolution 1D\n256 ﬁlters, kernel size=8, stride=2, Leaky\nReLU activation\nConvolution 1D\n256 ﬁlters, kernel size=8, stride=2, Leaky\nReLU activation\nConvolution 1D\n256 ﬁlters, kernel size=8, stride=2, Leaky\nReLU activation\nAdaptive Avg Pooling\n1D\noutput size=1\nFlatten\nIntermediate Layer Output\nDense\n2 nodes (number of classes)\nTABLE VI\nSEMI-SUPERVISED GAN GENERATOR ARCHITECTURE\nLayers\nAttributes\nDense\n256*33(=8448)\nnodes,\nbatch\nnorm\n1D,\nReLU activation\nReshape\nReshape to 256 x 33\nConv Transpose 1D\n256\nﬁlters,\nkernel\nsize=8,\nstride=2,\npadding=0, batch norm 1D, ReLU activation\nConv Transpose 1D\n256\nﬁlters,\nkernel\nsize=8,\nstride=2,\npadding=0, batch norm 1D, ReLU activation\nConv Transpose 1D\n256\nﬁlters,\nkernel\nsize=8,\nstride=2,\npadding=0, batch norm 1D, ReLU activation\nConv Transpose 1D\n256\nﬁlters,\nkernel\nsize=8,\nstride=2,\npadding=0, batch norm 1D, ReLU activation\nConv Transpose 1D\n256\nﬁlters,\nkernel\nsize=8,\nstride=2,\npadding=0, batch norm 1D, ReLU activation\nConv Transpose 1D\n128\nﬁlters,\nkernel\nsize=8,\nstride=2,\npadding=1, batch norm 1D, ReLU activation\nConv Transpose 1D\n64\nﬁlters,\nkernel\nsize=8,\nstride=2,\npadding=1, batch norm 1D, ReLU activation\nConv Transpose 1D\n1 ﬁlter, kernel size=8, stride=1, padding=0,\ntanh activation\nfor abnormal class samples), and (ii) it allows for an anomaly\nscore which can be used to get the relative grade of the\nabnormal samples and can be utilized in various applications\nsuch as triaging.\nTwo anomaly detection algorithms and two sets of features\n(these features are used to train the two algorithms) are con-\nsidered for evaluation. The two anomaly detection algorithms\nused are One-Class SVM [35] and Isolation Forest [36]. In\nOne-Class SVM algorithm, the normal samples are enclosed\nwithin a hyper-sphere or hyper-plane and everything outside\nthis is considered as an anomalous sample. The distance from\nthe separating plane decides the degree of abnormality. In\nIsolation forest, the samples are split randomly during training\nusing isolation trees. The resulting average tree lengths for the\ntree forest is taken as a measure of the abnormality. Anomalous\nsamples are more susceptible to isolation while splitting and\nhence have shorter average tree lengths. This can be used to\ndistinguish between the anomalies and normal samples.\nDuring training, a stack of 1D Convolutions layers and\n1D Convolutions-Upsampling layers are combined to serve as\n6\nLossdiscriminator = Lossunsupervised + Losssupervised\n(1)\nLosssupervised = −E(x, y)[logPD(y|x, y < K + 1)], K = no.ofclasses\n(2)\nLossunsupervised = −Ex[log(1 −PD(y = K + 1|x))] −Exg[logPD(y = K + 1|x)]\n(3)\nLossgenerator = ||Ex[M(x)] −Exg[M(G(z))]||2\n2\n(4)\nFig. 6. Unsupervised Anomaly Detection Framework using 1D CNN Autoencoder. The Autoencoder takes an input sample x and produces the reconstructed\nsample xrecon. The latent representations z from the Autoencoder and the Reconstruction loss are used as features for anomaly detection methods, Isolation\nForest and One-Class SVM.\nan auto-encoder. The audio features extracted from the heart\nsounds are provided to the auto-encoder for reconstruction.\nTwo features from 1D CNN Autoencoder were used for\nserving as the input features for Isolation Forest and One-\nClass SVM:\n(i) Reconstruction loss: The squared difference between the ac-\ntual input and the reconstructed output sample by the 1D CNN\nAutoencoder. The intuition is that, for the anomalous samples,\nthe reconstruction loss would be higher during test time as\nthe actual model cannot accurately reconstruct anomalous\nsamples as it is trained to reconstruct only normal samples.\nThe reconstruction loss is deﬁned in equation 5.\nLossreconstruction = |Xinput −Xrec|2\n(5)\n(ii) Latent Representations : Latent representations or embed-\ndings is the output obtained from the bottleneck layer / the last\nlayer of the encoder. While training, the latent representation\nwould provide a set of features which represent the training\nsamples. These feature set would help discriminate between\nnormal and anomalous samples.\nThe overall anomaly detection framework is shown in Fig.\n6. The autoencoder network structure is provided in Table\nVII. Two modes of training were used for training the 1D\nCNN autoencoder. In the ﬁrst case, the training data consists\npurely of normal samples only. In the second case, the data\nis contaminated with abnormal samples as well. This would\nhelp in evaluating the utility of the method in the use-case\nwhere there are no ﬁlters to prevent abnormal samples from\nbeing used - like screening applications - where the data\ncan have a mix of both normal and abnormal samples, but\nthe proportion of anomalous data is lesser. 8% - 12% is\na reasonable assumption for contamination with anomalous\nsamples as the percentage prevalence of heart diseases among\ngeneral population is roughly 10% [37]. During the training\nphase, extracted latent representations and reconstruction loss\nfor samples obtained from the auto-encoder are used to train\nthe two anomaly detection algorithms. For both the algorithms,\nthe experiments are conducted for clean data (only normal\nsamples) and contaminated data (normal and abnormal mixed).\nIII. COMPUTATIONS AND RESULTS\nThis section describes the experiments performed to eval-\nuate the methods discussed in previous section. Computa-\ntional Setup and Evaluation Metrics sub-section describes the\ntraining setup and the various metrics used to validate the\nperformance on a test set. Subsequent sub-section describes\nthe results obtained for supervised, semi-supervised and unsu-\npervised methods.\nA. Computational Setup\nThe computations of supervised methods for heart sounds\nclassiﬁcation utilize the entire dataset consisting of 3,240\nsamples. 20% of above dataset (648 samples) were used for\ntesting and remaining 80% were used for training. This 80%\nwas further divided into training (90%, 2,333 samples) and\nvalidation (10%, 259 samples) sets. The training set was used\n7\nTABLE VII\n1D CNN AUTOENCODER ARCHITECTURE\nLayers\nAttributes\nConvolution 1D\n64 ﬁlters, kernel size=3, padding=same\nMaxPool 1D\nkernel size=2, stride=2\nConvolution 1D\n64 ﬁlters, kernel size=3, padding=same\nMaxPool 1D\nkernel size=2, stride=2\nConvolution 1D\n32 ﬁlters, kernel size=3, padding=same\nMaxPool 1D\nkernel size=2, stride=2\nConvolution 1D\n16 ﬁlters, kernel size=3, padding=same\nMaxPool 1D\nkernel size=2, stride=2\nConvolution 1D\n8 ﬁlters, kernel size=3, padding=same\nMaxPool 1D\nkernel size=2, stride=2\nFlatten\nLatent Space\nReshape\nReshape to 12 x 8\nConvolution 1D\n8 ﬁlters, kernel size=3, padding=same\nUpsampling 1D\nsize=2\nConvolution 1D\n16 ﬁlters, kernel size=3, padding=same\nUpsampling 1D\nsize=2\nConvolution 1D\n32 ﬁlters, kernel size=3, padding=same\nUpsampling 1D\nsize=2\nConvolution 1D\n64 ﬁlters, kernel size=3, padding=same\nUpsampling 1D\nsize=2\nZero Padding 1D\n0 x 1\nConvolution 1D\n1 ﬁlter, kernel size=3, padding=same\nfor model ﬁtting and validation set was used to tune the hyper-\nparameters of the model.\nThe computations of semi-supervised and unsupervised\nmethods utilize only the sub-dataset E (2141 samples) of the\nentire dataset. Since each of the sub-datasets are collected\nfrom different sources, there may be some bias associated with\neach sub-dataset. 20% of above sub-dataset E (429 samples)\nwere used for testing and remaining 80% were used for\ntraining. This 80% was further divided into training (90%,\n1,540 samples) and validation (10%, 172 samples) sets.\nB. Evaluation Metrics\nThe supervised methods are evaluated using the standard\nclassiﬁcation metrics such as sensitivity, speciﬁcity and ac-\ncuracy. An additional metric MAcc, which is deﬁned as\nthe average of sensitivity and speciﬁcity was also used for\nevaluation [38]. For semi-supervised evaluation, the idea is to\ncompare the performance of the supervised baseline against\nsemi-supervised methods across different amounts of labelled\ndata. The idea is to mimic a clinical use case setting where\nthe number of labelled samples is limited. As there is class\nimbalance, AUROC (Area Under the Receiver Operating Char-\nacteristic curve) is used as the metric for comparison between\nsupervised and semi-supervised models. AUROC not only\ntakes into account the issue of class imbalance, but is also not\nsensitive to the cutoff value taken for the class predictions.\nC. Results and Discussion\nTable VIII shows the results for various supervised methods\ndiscussed in this study. DenseNet-121 on Mel-spectrograms\n(with padding) and Decision Tree on extracted audio fea-\ntures achieved the best performance in terms of speciﬁcity\nand sensitivity respectively. Gradient Boosting on extracted\naudio features (with SMOTE balancing) achieved the best\nperformance in terms of accuracy and MAcc. Table IX shows\nthe comparison among the Gradient Boosting method and the\nmethods reported in the PhysioNet/CinC 2016 Challenge.\nFor the semi-supervised computations, the percentage of\nlabelled data provided to the models is slowly increased across\nthe computation and compared based on the AUROC scores.\nFig. 7 shows the plot of AUROC against the percentage of\nlabelled data used for supervised and semi-supervised method.\nThe performance for the semi-supervised model is better than\nthe supervised baseline even in the case of very few data\nsamples. Even with 4 or 8 data samples, the semi-supervised\nmodel is able to outperform the supervised baseline. This\nobservation in the plot can be explained as:\n(i) The larger unlabelled training set helps approximate the\noverall data distribution, which allows for a much better\ndecision boundary than the supervised method which can only\naccount for labelled samples;\n(ii) The unlabelled data has a regularization effect on the\nclassiﬁcation network as the semi-supervised training follows\na coupled adversarial training procedure.\nA higher performance is obtained in terms of AUROC as more\nand more labelled samples are used for classiﬁcation.\nUse cases for semi-supervised methods: These methods\nare of particular importance in cases of limited annotation\nability. In most clinical cases, large number of labelled training\nsamples for supervised training is not readily available. The\nsemi-supervised methods can be used in two scenarios:\n(i) It can learn better from lesser labelled data and provide\nbetter labels for pseudo-labelling algorithms;\n(ii) It has a better predictive power as compared to supervised\nmethods, and due to the usage of unlabelled samples as well,\nthe model can iteratively select samples that needs to be\nlabelled for better performance.\nTable X and Table XI provide the results of the anomaly\ndetection methods. One-Class SVM achieves a better perfor-\nmance in terms of AUROC for both cases as compared to\nIsolation Forest. Moreover, latent representations or embed-\ndings give a better performance than reconstruction loss. Data\ncontamination in the experiment with autoencoder trained on\nonly normal samples is not a major concern as the latent rep-\nresentations obtained are fairly robust to this issue. However,\nfor the experiment with autoencoder trained on both normal\nand abnormal samples, it is ideal that normal samples be fed\ninto the anomaly detection algorithm.\nUse cases for unsupervised methods: The unsupervised fea-\nture extraction (using 1D CNN Autoencoder) method coupled\nwith anomaly detection methods achieved good performances\neven with no major labelling burden. These methods can be\nused in two scenarios:\n(i) These methods are good for triaging applications since the\nabnormality scores are an indicator of the disease and the ones\nwith the higher abnormality score can be evaluated ﬁrst;\n(ii) These methods can be useful for creating datasets for\nsupervised or semi-supervised training. Samples that are close\nto the classiﬁcation boundary can be chosen in an unsupervised\nsetting, as these samples are more confusing for the models\nto distinguish.\n8\nTABLE VIII\nRESULTS FOR SUPERVISED METHODS OF HEART SOUNDS\nCLASSIFICATION\nMethod\nAccuracy Speciﬁcity\nSensitivity\nMAcc\nResNet-50\non\nMel-\nspectrogram\n(with\npadding)\n0.869\n0.941\n0.604\n0.773\nResNet-50\non\nspectrogram\n(with\npadding)\n0.878\n0.943\n0.640\n0.792\nResNet-50\non\nMel-\nspectrogram\n(with\npruning)\n0.860\n0.919\n0.640\n0.780\nResNet-50 on spec-\ntrogram (with prun-\ning)\n0.847\n0.896\n0.669\n0.782\nInception-v3 on Mel-\nspectrogram\n(with\npadding)\n0.850\n0.931\n0.554\n0.743\nInception-v3\non\nspectrogram\n(with\npadding)\n0.867\n0.947\n0.576\n0.761\nInception-v3 on Mel-\nspectrogram\n(with\npruning)\n0.796\n0.941\n0.266\n0.604\nInception-v3 on spec-\ntrogram (with prun-\ning)\n0.826\n0.953\n0.360\n0.656\nDenseNet-121\non\nMel-spectrogram\n(with padding)\n0.869\n0.965\n0.518\n0.741\nDenseNet-121\non\nspectrogram\n(with\npadding)\n0.818\n0.990\n0.187\n0.589\nCustom\nCNN\non\nspectrogram\n(with\npadding)\n0.909\n0.967\n0.698\n0.832\nDense\nNN\non\nex-\ntracted audio features\n(with SMOTE)\n0.855\n0.880\n0.763\n0.821\nLSTM\nNN\non\nex-\ntracted audio features\n(with SMOTE)\n0.748\n0.770\n0.670\n0.720\n1D CNN on extracted\naudio features (with\nSMOTE)\n0.843\n0.847\n0.827\n0.837\nDecision Tree on ex-\ntracted audio features\n0.824\n0.811\n0.870\n0.841\nDecision Tree on ex-\ntracted audio features\n(with SMOTE)\n0.832\n0.837\n0.813\n0.825\nSVM on extracted au-\ndio features\n0.807\n0.813\n0.784\n0.799\nSVM\non\nextracted\naudio features (with\nSMOTE)\n0.827\n0.953\n0.367\n0.660\nRandom Forest on ex-\ntracted audio features\n0.898\n0.925\n0.798\n0.862\nRandom Forest on ex-\ntracted audio features\n(with SMOTE)\n0.878\n0.888\n0.842\n0.865\nGradient Boosting on\nextracted audio fea-\ntures\n0.913\n0.970\n0.705\n0.838\nGradient Boosting on\nextracted audio fea-\ntures (with SMOTE)\n0.913\n0.935\n0.834\n0.885\nTABLE IX\nCOMPARISON OF PROPOSED METHOD WITH VARIOUS SUPERVISED\nMETHODS REPORTED IN THE PHYSIONET/CINC 2016 CHALLENGE\nMethod\nFeature\nBalancing\ndata\nMAcc\nAdaBoost and CNN [11]\nTime-\nfrequency\nNo\n0.8602\nEnsemble of NN [39]\nTime-\nfrequency\nYes\n0.8590\nDropout\nConnected\nNN\n[40]\nMFCC\nNo\n0.8520\nSVM and KNN [41]\nTime-\nfrequency,\nMFCC\nNo\n0.8454\nCNN [42]\nMFCC\nNo\n0.8399\nSVM and ELM [43]\nAudio\nSignal\nAnalysis\nNo\n0.7869\nGradient Boosting (Current\nstudy)\nExtracted Au-\ndio features\nYes\n0.8850\nFig. 7.\nSemi-supervised Results. The graph shows the AUROC evaluation\nmetric against the percentage of labelled data for supervised baseline and\nsemi-supervised method. The performance of semi-supervised approach is\nbetter than supervised approach throughout the graph.\nTABLE X\nRESULTS FOR ANOMALY DETECTION WHEN AUTOENCODER IS TRAINED\nON ONLY NORMAL SAMPLES\nMethod\nFeatures\nLabels\nAUROC\nIsolation Forest\nEmbeddings\nNormal\n0.644\nContaminated\n0.564\nRec Loss\nNormal\n0.699\nContaminated\n0.687\nOne-Class SVM\nEmbeddings\nNormal\n0.842\nContaminated\n0.552\nRec Loss\nNormal\n0.737\nContaminated\n0.658\n9\nTABLE XI\nRESULTS FOR ANOMALY DETECTION WHEN AUTOENCODER IS TRAINED\nON ENTIRE DATA (BOTH NORMAL AND ABNORMAL SAMPLES)\nMethod\nFeatures\nLabels\nAUROC\nIsolation Forest\nEmbeddings\nNormal\n0.678\nContaminated\n0.630\nRec Loss\nNormal\n0.577\nContaminated\n0.542\nOne-Class SVM\nEmbeddings\nNormal\n0.828\nContaminated\n0.567\nRec Loss\nNormal\n0.671\nContaminated\n0.588\nIV. CONCLUSION AND FUTURE DIRECTIONS\nThis study explores the supervised, semi-supervised and\nunsupervised methods of heart sounds classiﬁcation for the\nuse cases where the availability of labelled data is scarce.\nIn such cases, the supervised methods with large number of\nlabelled samples, plateau out and have similar performances.\nHowever, for smaller number of labelled samples, the semi-\nsupervised algorithm outperforms the supervised baselines.\nFurthermore, the given problem is framed as an anomaly\ndetection problem with unsupervised feature learning. The\nissue of data contamination is also studied and the results are\npresented.\nThese works can be a starting point for various future use\ncases and studies. One promising direction for the utilization\nof these methods is in the case of active learning - where ﬁrst a\nsmall subset of samples is labelled and then iteratively samples\nare chosen to be labelled further to improve performance. The\ngood performance on lower number of labelled samples is\nalso useful in the case for pseudo-labelling, where existing\nsupervised classiﬁcation methods are used with the assumed\nlabels. The heart sounds signals used in this study are not seg-\nmented. Various segmentation algorithms have been developed\nin recent years. Proper segmentation and alignment techniques\ncan be employed to further boost the performance. Apart\nfrom band-pass ﬁlters, other signal processing techniques\ncan be explored to improve performance. Hence, better pre-\nprocessing techniques and feature extraction techniques can be\nanother pathway for exploration.\nAnother more challenging setting is to use data augmenta-\ntion for heart sounds signals. It might be interesting to see how\nsound signals can be augmented using techniques like Mix-Up\n[44], apart from SMOTE for data balancing. However, since\nthe domain of application is health care, it is important to\nensure that the augmented data samples should not introduce\nany wrong features or biases within the model and hence\nshould be undertaken with utmost care. Moreover, all the\nmethods presented in this work can generalize for any 1D\nsignal input. Hence, ECG signals can also be used instead\nof PCG signals. This work is presented with the belief that it\ncan aid both in creation of better models and more importantly,\nbetter datasets which can further improve performance, as in\nmost practical cases, it is the quality of data used that is a\ncrucial factor in obtaining better performance.\nREFERENCES\n[1] “Cardiovascular diseases (cvds),” May 2017. [Online]. Available: www.\nwho.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)\n[2] C. Liu, D. Springer, Q. Li, B. Moody, R. A. Juan, F. J. Chorro,\nF. Castells, J. M. Roig, I. Silva, A. E. Johnson et al., “An open access\ndatabase for the evaluation of heart sound algorithms,” Physiological\nMeasurement, vol. 37, no. 12, p. 2181, 2016.\n[3] R. M. Rangayyan and R. J. Lehner, “Phonocardiogram signal analysis:\na review.” Critical reviews in biomedical engineering, vol. 15, no. 3, pp.\n211–236, 1987.\n[4] N. V. Thakor and Y.-S. Zhu, “Applications of adaptive ﬁltering to ecg\nanalysis: noise cancellation and arrhythmia detection,” IEEE transac-\ntions on biomedical engineering, vol. 38, no. 8, pp. 785–794, 1991.\n[5] R. Silipo and C. Marchesi, “Artiﬁcial neural networks for automatic ecg\nanalysis,” IEEE transactions on signal processing, vol. 46, no. 5, pp.\n1417–1425, 1998.\n[6] W. Phanphaisarn, A. Roeksabutr, P. Wardkein, J. Koseeyaporn, and\nP. Yupapin, “Heart detection and diagnosis based on ecg and epcg\nrelationships,” Medical devices (Auckland, NZ), vol. 4, p. 133, 2011.\n[7] M. A. Akbari, K. Hassani, J. D. Doyle, M. Navidbakhsh, M. Sangargir,\nK. Bajelani, and Z. S. Ahmadi, “Digital subtraction phonocardiography\n(dsp) applied to the detection and characterization of heart murmurs,”\nBiomedical engineering online, vol. 10, no. 1, p. 109, 2011.\n[8] S. Ari, K. Hembram, and G. Saha, “Detection of cardiac abnormality\nfrom pcg signal using lms based least square svm classiﬁer,” Expert\nSystems with Applications, vol. 37, no. 12, pp. 8019–8026, 2010.\n[9] I. Grzegorczyk, M. Soliski, M. epek, A. Perka, J. Rosiski, J. Rymko,\nK. Stpie, and J. Gieratowski, “Pcg classiﬁcation using a neural network\napproach,” in 2016 Computing in Cardiology Conference (CinC), 2016,\npp. 1129–1132.\n[10] F. Plesinger, I. Viscor, J. Halamek, J. Jurco, and P. Jurak, “Heart\nsounds analysis using probability assessment,” Physiological measure-\nment, vol. 38, no. 8, p. 1685, 2017.\n[11] C. Potes, S. Parvaneh, A. Rahman, and B. Conroy, “Ensemble of feature-\nbased and deep learning-based classiﬁers for detection of abnormal heart\nsounds,” in 2016 Computing in Cardiology Conference (CinC).\nIEEE,\n2016, pp. 621–624.\n[12] X. J. Zhu, “Semi-supervised learning literature survey,” University of\nWisconsin-Madison Department of Computer Sciences, Tech. Rep.,\n2005.\n[13] B. Settles, “Active learning literature survey,” University of Wisconsin-\nMadison Department of Computer Sciences, Tech. Rep., 2009.\n[14] D. Chamberlain, R. Kodgule, D. Ganelin, V. Miglani, and R. R. Fletcher,\n“Application of semi-supervised deep learning to lung sound analysis,”\nin 2016 38th Annual International Conference of the IEEE Engineering\nin Medicine and Biology Society (EMBC).\nIEEE, 2016, pp. 804–807.\n[15] A. I. Humayun, M. Khan, S. Ghaffarzadegan, Z. Feng, T. Hasan et al.,\n“An ensemble of transfer, semi-supervised and supervised learning\nmethods for pathological heart sound classiﬁcation,” arXiv preprint\narXiv:1806.06506, 2018.\n[16] A. Ukil, S. Bandyopadhyay, C. Puri, R. Singh, and A. Pal, “Class\naugmented semi-supervised learning for practical clinical analytics on\nphysiological signals,” arXiv preprint arXiv:1812.07498, 2018.\n[17] M. M. Rahman and D. Davis, “Addressing the class imbalance problem\nin medical datasets,” International Journal of Machine Learning and\nComputing, vol. 3, no. 2, p. 224, 2013.\n[18] G. Amit, N. Gavriely, and N. Intrator, “Cluster analysis and classiﬁcation\nof heart sounds,” Biomedical Signal Processing and Control, vol. 4,\nno. 1, pp. 26–36, 2009.\n[19] M. A. Pimentel, D. A. Clifton, L. Clifton, and L. Tarassenko, “A review\nof novelty detection,” Signal Processing, vol. 99, pp. 215–249, 2014.\n[20] D. Ellis, “Chroma feature analysis and synthesis,” Resources of Lab-\noratory for the Recognition and Organization of Speech and Audio-\nLabROSA, 2007.\n[21] D.-N. Jiang, L. Lu, H.-J. Zhang, J.-H. Tao, and L.-H. Cai, “Music\ntype classiﬁcation by spectral contrast feature,” in Proceedings. IEEE\nInternational Conference on Multimedia and Expo, vol. 1. IEEE, 2002,\npp. 113–116.\n[22] C. Harte, M. Sandler, and M. Gasser, “Detecting harmonic change in\nmusical audio,” in Proceedings of the 1st ACM workshop on Audio and\nmusic computing multimedia.\nACM, 2006, pp. 21–26.\n[23] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “Smote:\nsynthetic minority over-sampling technique,” Journal of artiﬁcial intel-\nligence research, vol. 16, pp. 321–357, 2002.\n10\n[24] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, “Learning and transferring\nmid-level image representations using convolutional neural networks,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2014, pp. 1717–1724.\n[25] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[26] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking\nthe inception architecture for computer vision,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition, 2016, pp.\n2818–2826.\n[27] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, 2017, pp. 4700–4708.\n[28] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[29] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted boltz-\nmann machines,” in Proceedings of the 27th international conference on\nmachine learning (ICML-10), 2010, pp. 807–814.\n[30] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: a simple way to prevent neural networks from over-\nﬁtting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp.\n1929–1958, 2014.\n[31] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[32] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in\nAdvances in neural information processing systems, 2014, pp. 2672–\n2680.\n[33] O. Chapelle, B. Scholkopf, and A. Zien, “Semi-supervised learning\n(chapelle, o. et al., eds.; 2006)[book reviews],” IEEE Transactions on\nNeural Networks, vol. 20, no. 3, pp. 542–542, 2009.\n[34] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and\nX. Chen, “Improved techniques for training gans,” in Advances in neural\ninformation processing systems, 2016, pp. 2234–2242.\n[35] L. M. Manevitz and M. Yousef, “One-class svms for document clas-\nsiﬁcation,” Journal of machine Learning research, vol. 2, no. Dec, pp.\n139–154, 2001.\n[36] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation forest,” in 2008 Eighth\nIEEE International Conference on Data Mining.\nIEEE, 2008, pp. 413–\n422.\n[37] E. J. Benjamin, M. J. Blaha, S. E. Chiuve, M. Cushman, S. R. Das,\nR. Deo, J. Floyd, M. Fornage, C. Gillespie, C. Isasi et al., “Heart\ndisease and stroke statistics-2017 update: a report from the american\nheart association.” Circulation, vol. 135, no. 10, pp. e146–e603, 2017.\n[38] G. D. Clifford, C. Liu, B. Moody, J. Millet, S. Schmidt, Q. Li,\nI. Silva, and R. G. Mark, “Recent advances in heart sound analysis,”\nPhysiological measurement, vol. 38, no. 8, pp. E10–E25, 2017.\n[39] M. Zabihi, A. B. Rad, S. Kiranyaz, M. Gabbouj, and A. K. Katsaggelos,\n“Heart sound anomaly and quality detection using ensemble of neural\nnetworks without segmentation,” in 2016 Computing in Cardiology\nConference (CinC).\nIEEE, 2016, pp. 613–616.\n[40] E. Kay and A. Agarwal, “Dropconnected neural network trained with\ndiverse features for classifying heart sounds,” in 2016 Computing in\nCardiology Conference (CinC).\nIEEE, 2016, pp. 617–620.\n[41] I. J. D. Bobillo, “A tensor approach to heart sound classiﬁcation,” in\n2016 Computing in Cardiology Conference (CinC).\nIEEE, 2016, pp.\n629–632.\n[42] J. Rubin, R. Abreu, A. Ganguli, S. Nelaturi, I. Matei, and K. Sricharan,\n“Classifying heart sound recordings using deep convolutional neural\nnetworks and mel-frequency cepstral coefﬁcients,” in 2016 Computing\nin Cardiology Conference (CinC).\nIEEE, 2016, pp. 813–816.\n[43] X. Yang, F. Yang, L. Gobeawan, S. Y. Yeo, S. Leng, L. Zhong, and\nY. Su, “A multi-modal classiﬁer for heart sound recordings,” in 2016\nComputing in Cardiology Conference (CinC).\nIEEE, 2016, pp. 1165–\n1168.\n[44] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond\nempirical risk minimization,” arXiv preprint arXiv:1710.09412, 2017.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-06-04",
  "updated": "2020-06-04"
}