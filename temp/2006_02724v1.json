{
  "id": "http://arxiv.org/abs/2006.02724v1",
  "title": "Characterizing the Weight Space for Different Learning Models",
  "authors": [
    "Saurav Musunuru",
    "Jay N. Paranjape",
    "Rahul Kumar Dubey",
    "Vijendran G. Venkoparao"
  ],
  "abstract": "Deep Learning has become one of the primary research areas in developing\nintelligent machines. Most of the well-known applications (such as Speech\nRecognition, Image Processing and NLP) of AI are driven by Deep Learning. Deep\nLearning algorithms mimic human brain using artificial neural networks and\nprogressively learn to accurately solve a given problem. But there are\nsignificant challenges in Deep Learning systems. There have been many attempts\nto make deep learning models imitate the biological neural network. However,\nmany deep learning models have performed poorly in the presence of adversarial\nexamples. Poor performance in adversarial examples leads to adversarial attacks\nand in turn leads to safety and security in most of the applications. In this\npaper we make an attempt to characterize the solution space of a deep neural\nnetwork in terms of three different subsets viz. weights belonging to exact\ntrained patterns, weights belonging to generalized pattern set and weights\nbelonging to adversarial pattern sets. We attempt to characterize the solution\nspace with two seemingly different learning paradigms viz. the Deep Neural\nNetworks and the Dense Associative Memory Model, which try to achieve learning\nvia quite different mechanisms. We also show that adversarial attacks are\ngenerally less successful against Associative Memory Models than Deep Neural\nNetworks.",
  "text": "Characterizing the Weight Space for Different \nLearning Models \nSaurav Musunuru \nComputer Science & \nEngineering \nIndian Institute of \nTechnology Delhi, India \nemail: \nmusunurusaurav@gmail\n.com \n \n \nJay N. Paranjape \nComputer Science & \nEngineering \nIndian Institute of \nTechnology Delhi, India \nemail: \njay.edutech@gmail.com \n \n \n \nRahul Kumar Dubey \nRobert Bosch \nEngineering and \nBusiness Solutions \nBangalore, India \nemail:RahulKumar.D\nubey@in.bosch.com \n \n \n \nVijendran G. \nVenkoparao \nRobert Bosch \nEngineering and \nBusiness Solutions \nBangalore, India \ne-mail: \nGopalanVijendran.Venk\noparao@in.bosch.com \n \nAbstract—Deep Learning has become one of the primary \nresearch areas in developing intelligent machines. Most of the \nwell-known applications (such as Speech Recognition, Image \nProcessing and NLP) of AI are driven by Deep Learning. Deep \nLearning algorithms mimic human brain using artificial neural \nnetworks and progressively learn to accurately solve a given \nproblem. But there are significant challenges in Deep Learning \nsystems. There have been many attempts to make deep \nlearning models imitate the biological neural network. \nHowever, many deep learning models have performed poorly \nin the presence of adversarial examples. Poor performance in \nadversarial examples leads to adversarial attacks and in turn \nleads to safety and security in most of the applications.  In this \npaper we make an attempt to characterize the solution space of \na deep neural network in terms of three different subsets viz. \nweights belonging to exact trained patterns, weights belonging \nto generalized pattern set and weights belonging to adversarial \npattern sets. We attempt to characterize the solution space \nwith two seemingly different learning paradigms viz. the Deep \nNeural Networks and the Dense Associative Memory Model, \nwhich try to achieve learning via quite different mechanisms. \nWe also show that adversarial attacks are generally less \nsuccessful against Associative Memory Models than Deep \nNeural Networks. \nKeywords—Fully connected feed forward model, Associative \nmemory model, Hyper sphere, Adversarial \nI. INTRODUCTION \nToday, Deep Learning has become one of the primary \nresearch areas in developing intelligent machines. Most of \nthe well-known applications of AI, like Image Classification, \nRecognition, and Computer Vision and so on [1] [2]are \ndriven by Deep Learning. Today, it is being used in many \nreal world applications like Automated Driving[3], Anomaly \nDetection[4] and Healthcare[5]. Deep Learning algorithms \nmimic human brains using artificial neural networks and \nprogressively learn to accurately solve a given problem. But, \nwith great technological advances come complex difficulties \nand hurdles.For the reliable application of Deep Neural \nNetworks in the domain of security, the robustness against \nadversarial attacks must be well established. There had been \nseveral speculative explanation regarding the existence  of  \nadversarial  examples.  Some  of  the  explanations  attribute  \nthis  to  the  non-linearity  of  deep  neural networks, but \nrecently in [6], the   authors   showed   that linear  behavior  \nin  high dimensional  spaces  is  sufficient  to produce \nadversarial examples in neural networks. In this paper, we \nstudy and compare two such models namely  the Deep \nNeural Networks and the Associative Memory, which try to \nachieve learning via quite different mechanisms. \nA. Deep Neural Networks(DNN) \nSupervised learning is a glorified name for a function \nwhich takes some input and gives an output according to the \napplication it is used for. For example, in image \nclassification tasks, an image is given as an input and the \nfunction outputs a set of numbers, the maximum of which is \nthe predicted label of the image. This is but one technique of \nclassification and the task of classification is but one among \nmany, the general idea behind neural networks in a \nsupervised learning setup  remains the same i.e. find a \nmapping from input to output. \nWhat makes neural networks so useful is the fact that \nthese mappings don't have to be identified by the \nprogrammer. Instead, the programmer defines an architecture \nof layers performing simple mathematical functions like \narithmetic, matrix multiplication, convolution, non-linear \nfunctions like tanh, and so on, and also provides this model \nwith a large set of input-output pairs. Through multiple \npasses of the input, and update rules defined by the \nalgorithms such as Statistical Gradient Descent (SGD), the \nmodel 'learns' a set of weights which it uses to perform the \nmathematical operations so that maximum of the given pairs \nare satisfied. \nWhen many such simple mathematical layers are used \nbetween the input and the output layers, the network is \ntermed as 'Deep' Neural Network. These models have \nperformed extremely well in almost all kinds of machine \nlearning tasks. Well known examples include Oxford's \nVGG16[2] , Google's GoogleNet[7] and so on. \nB. Associative Memory \nAn associative memory is a type of Neural Network \nparadigm that allows for the recall of data  based on the \ndegree of similarity between the input pattern and the \npatterns stored in memory.  It refers to a memory \norganization in which the memory is accessed by its content \nas opposed to an explicit address like in the traditional \ncomputer memory system.  Therefore, this type of memory \nallows the recall of information based on partial knowledge \nof its contents. Hopfield Network was first introduced by \nJ.J.Hopfield in 1982[8]. It's structure is based on the \nbiological neuron. Similar to the human brain, the Hopfield \nnetwork memorizes the given patterns. And in the process of \nrecall it tries to recall the memorized pattern which is closest \nto the given pattern. In this sense the Hopfield Network \nfunctions as an associative memory. There are multiple \nlearning rule for Hopfield networks. The training of the \nHopfield network is finding the weight matrix for the \nweights which connects the neurons. The Hebbian learning \nrule is \n \n \n(1) \nWe use an iterative learning rule which is described in \nthe next section. A sample Hopfield Network is shown in \nFig. 1. \n \nFig 1.   A sample Hopfield Network consisting of 6 neurons. \n \nThe Energy of a Hopfield net is described using the \nequation \n \n(2) \n \nTo understand this we try to look at a visualization of the \nenergy landscape which is plotted between the energy vs \npatterns. Since the patterns are N dimensional vectors this \nlandscape is only a visualization and not the exact graph \nwhich is plotted on hypersphere. A visualisation is given in \nFig. 2. \n \nFig. 2.  The 3D energy landscape of a Hopfield network before training. \nII. COMPARISON BETWEEN DNN AND ITERATIVE LEARNING \nASSOCIATIVE MODEL NETWORK \nA. Training Process \n1) DNN \nTraining is the process of finding 'weights' of the model \nwhich will satisfy the input output mapping. Weights are \nparameters of the model which are used along with the inputs \nto produce output. In other words, the output is a function of \nthe inputs as well as the model weights. Before training, \nthese weights may be initialized randomly. During training, \nbatches of inputs are taken and according to the initial \nrandom weights. Then, an objective function, as defined by \nthe programmer, is calculated, which is a function of the \npredicted value as well as the actual value. Then, the weights \nare updated so that this objective function(or loss) is \nminimized. This is done through various optimization rules \nsuch as Stochastic Gradient Descent(SGD)[9], Adam[10], \nRMSProp[11] and so on. This process is continued for many \niterations until a reasonable accuracy is achieved. We have \nused SGD and RMS error as our optimization function and \nobjective function respectively. \nOne important distinction to be made here is that we are \nnot interfering with the inputs of the model during training. \nWe only change the weights of the model as shown in Fig. 3. \nWe can visualize the training process by plotting the \nobjective function against the weights of the model. The \nweights get updated in each iteration of the training and \nfinally converge to a minima. However, no updates are made \nto the input patterns during this process. This is not the case \nfor Associative Memory Models, where modifications are \nmade in the patterns. \n \nFig. 3.   Gradient Descent, where the weights are updated towards \nminima. Here, the weights are initialized randomly and as the training \nproceeds, the path as shown is followed and a minima is reached. \n \n2) Iterative Learning Associative Networks \nThe training process of Iterative Learning Associative \nModel Network depends on the learning rule. In the iterative \nlearning rule, the weight matrix is initialized to the Hebbian \nweight matrix and then a correction value is added in each \niteration. The correction is calculated by flipping states of the \npatterns randomly and checking the lowering of energy since \nthe patterns of the network should be the lowest in energy \ncompared to the vicinity of the pattern in terms of hamming \ndistance. To visualize the change of the weight matrix in \nterms of the energy landscape, see Fig. 2, Fig. 4 and Fig. 5. \nB. Adversarial Inputs \nAdversarial inputs are slightly modified input patterns \nwhich were previously recognized by the model but not \nafter the modification. However, These patterns seem \nsimilar to the human eye. [12] first showed the existence of \nadversarial inputs. The existence of such inputs poses a \nthreat to privacy and security with the increasing application \nof deep learning models in fields concerning human lives. \n1) DNN \nDeep neural networks are shown to be highly prone to \nadversarial attacks [13]. After [12] discovered the existence \nof adversarial inputs, many more such attacks have been \ninvented. A defense mechanism which is robust to all such \nattacks has not been created to the best of our knowledge. \nFor all the existing deep neural networks, there exist attacks \nwhich mislead the model without misleading the human eye. \n \nFig. 4.  The 3D energy landscape of a Hopfield network after training \n \n \nFig. 5.  This figure shows the change in landscape while training in an \niterative rule. The black landscape is the trained landscape. The red \nlandscape inside is the previous landscape. \n \n2) Associative Memory \nAn Iterative Learning Associative Model network is not \nbased on the conventional gradient based optimization. \nHence, most of the adversarial attacks don't work on it. As \nan experiment, we generated 40 adversarial images from the \nMNIST dataset, for a dense network and recalled them using \na Iterative Learning Associative Model Network. We found \nthat the number of images which were misclassified by the \nIterative Learning Associative Model Network was 0.7 . \n \n This experiment proves the model dependence of \nadversarial attacks. We define model dependence of \nadversarial attacks as the property of adversarial attacks \nbeing successful on one type of learning rule but not on \nothers. Similarly, model independence is the property of an \nadversarial attack by which it is successful for any learning \nrule. However, [14] proved that adversarial attacks can be \nviewed as model independent, by creating transfer based \nattacks. Attacks against an ensemble of individual models \nare generally successful against other models as well. Thus, \nin case of Deep Neural Networks, adversarial attacks can be \ntransferred from one model to another.  \nWe claim that a similar model independence can always \nbe created by introducing more types of models in such an \nensemble. \nThus, the attacks generated against such an ensemble \nconsisting of associative memory models and recurrent \nmodels should be model independent in a rough sense. \nHowever, strictly speaking, adversarial attacks are always \nmodel dependent. \nC. Existence of Replicas in the Weight Space \n1) DNN \nUsually almost every architecture has a few dense layers \nbefore the output layers. Dense layers are important to the \nmodel since they do the actual classification from the \nfeatures extracted by the convolutions layers. According to \n[15], there exist transformations which when applied to the \nweight vector of fully connected dense layers, keeps the \noutput unchanged. These transformations are sign flips and \nweight(belonging to the same layer) interchanges. Thus, \nmultiple weight vectors exist for the same mapping. It has \nalso been proved that these weights take the form of a \nwedge and cone in the weight space. These cones are called \nequioutput cones. A visual representation is shown in Fig. 6 \nand Fig. 8. The paper further explains that the higher the \ndimensionality of the weight space, the easier it is to \nconverge to a minimum. For more information, we refer \nreaders to [15]. \n \nFor every set of patterns in the input(pattern-space), we \ndefine the weights learned by a model so as to minimize the \nobjective function, as the corresponding weight vector.  \nThus, we define the relation between weight space and \npattern space to be model dependent. However, for each \nsuch relation we can make interesting observations. \n \nWe define the cone formed in the weight space \ncorresponding to data points in the training set, as the exact \ncone. A similar cone is formed for all those patterns which \nare not in the data-set but are predicted/classified correctly \nby the model. We term this as the generalized cone. These \nequioutput cones are formed by sign flips and interchanges \nin weights just as shown in Fig. 6. Using the correlation \nbetween weight-space and pattern-space as described above, \nwe get equioutput cones for each of the cones using these \ntransformations(sign flips and interchanges). We verified \nthe existence of equioutput cones by defining an order of \nascendance between the weights of a layer in the weight \nvector viz. we initialized them in a particular order, and \nempirically viewed that the order is maintained during any  \n \n \nFig  6. The existence of equioutput cones, visualized, according to [12]. To the left is an example layer of a dense network. Each color represents a weight. \nSince we have a fully connected net, even on interchanging red and blue weights, the output will be the same. Hence, we get an equioutput cone. \n  \n \n \nFig 7. The green points correspond to a minima, mapping to the exact cone(green part in hypersphere), blue points correspond to the generalized \ncone(blue part in the hypersphere), and the red points correspond to the Non Exact, non generalized cone(red region in the hypersphere). This way, all the \npoints on the objective function belong to one of these cones. \n \ntime instance of the training. Thus, a weight vector does \nnot change its cone during training. This was true for \nevery permutation of the weights of a layer in the weight \nvector. \n \nWe term the remaining part of the hypersphere as the \nNon Exact Non Generalized Cone. This corresponds \ntoadversarial inputs as well as irrelevant inputs, which are \nnot images which are formed by making small amount of \nperturbations to the original image so that they are still \nclassifiable by the human eye, but completely mislead the \nmodel. Irrelevant images are all other combinations of the \npixels. For example, a completely black image is \nirrelevant for an MNIST dataset. \n \nA 3D representation of the hypersphere, along with \nexact and generalized cones, is shown in Fig. 8. We also \nshow an analogy between the position on the objective \nfunction and the hypersphere in Fig. 7. The points very \nnear to the minimas correspond to the exact cone. Points a \nlittle further away but closer to the minima than the \nmaxima correspond to the generalized cone. All other \npoints correspond to the Non Exact Non Generalized \nCone. We believe that the points corresponding to the \nmaxima have some relation with being adversarial. \npredicted correctly by the model.    \n \nWe emphasize that we have shown the above results \nempirically and a mathematical proof will be taken up as \nfuture work. However, it is not feasible to separate \nadversarial patterns from irrelevant patterns. This method \nrelies only on the model's prediction, given an image and \nso, to classify a pattern as adversarial, human decision is \nneeded. Hence, even for simple datasets like MNIST, \nwhich have 784 pixels per image and 70,000 images and \nonly considering binary images, it is humanely impossible \nto classify 2784 - 70000 images.  \n \nApplying the transformations pointed out in [12] we \ncan say that equioutput cones exist in case of exact, \ngeneralized as well as non exact and non generalized \ncones. Moreover, we claim that the volume of the entire \nhypersphere of the weight space is filled by these cones, \ni.e. \n \n \nFig. 8 The red region denotes the exact cone. The blue region, \nsurrounding the exact cone denotes the generalized cone. The cones are \nmade for visualization only. In reality, the cones cover a minuscule \nvolume of the hypersphere. \n \nPu = Pt + Pg + Pa + Pr \n \nAnd \n \nWt = We + Wg + Wn \n \nWhere \n \nPu = Total patterns in the pattern space \nPt = Training patterns \nPg = Generalized patterns, which are predicted correctly \nbut not been trained on, by the model \nPa = Adversarial patterns \nPr = irrelevant patterns \nWt = Total possible weights \nWe \n= \nWeights \ncorresponding \nto \nthe \nexact \ncone(corresponding to Pt) \nWg = Weights corresponding to the generalized \ncone(corresponding to Pg) \nWn = Weights corresponding to the Non Exact Non \nGeneralized Cone(corresponding to Pa + Pr) \n \n2) Associative Memory \nThe equioutput transformation given in [12] are only \nfor deep neural networks. If the same transformation is \napplied to the Hopfield weight matrix, the same results \nare not obtained. So there might be some equioutput \ntransformations which are not in mathematically closed \nforms, but there is no evidence of it. \nIII. CONCLUSION AND DISCUSSION \nIn this paper, we attempted to characterize the hyper \nsphere of solution space in terms of weights pertaining to \nexact solution, generalized solutions and weights space \nbelonging to adversarial patterns. We explored two very \ndifferent learning rules used for a pattern recognition and \nclassification tasks viz. feed-forward deep networks and \nassociative \nlearning rule. We \ncompared different \nproperties like the training procedure, existence of replica \nweights and robustness against adversarial examples. We \nsee that recalling into a final output image in case of \nIterative Learning Associative Model Network is much \nmore advantageous against adversarial attacks. In deep \nneural networks, the linear layers at the end of the \nnetwork output scores and are seen to be easily mislead, \nwhich is not the case in Iterative Learning Associative \nModel Neural Networks. Hence, we show the advantage \nof recall against classification. [16] attempted to combine \nclassification with recall and showed that it indeed is \nhelpful in improving robustness against adversarial \nattacks. A mathematical proof for proving the existence of \nreplicas in the weight space is left as future work. Also, a \ndefinitive relation between the maxima of the graph of the \nobjective function against weights or patterns is yet to be \nestablished and is left as future work. \n \nACKNOWLEDGMENT \nSaurav Musunuru and Jay N. Paranjape gratefully \nacknowledge the opportunity to intern at Robert Bosch \nand the permission by the Indian Institute of Technology \nDelhi. The authors are grateful towards K. Srinivasan for \nhelping out with the 3D plots in the paper. \nREFERENCES \n[1] \nA. Krizhevsky, I. Sutskever, and G. E. Hinton. “ImageNet \nclassification with deep convolutional neural networks” Proc. \nAdv. Neural Inf. Process. Syst, pp. 1097–1105, 2012 \n[2] \nK. Simonyan and A. Zisserman, “Very deep convolutional \nnetworks for large-scale image recognition”, [Online]. \nAvailable: https://arxiv.org/abs/1409.1556 , 2014 \n[3] \nBojarski et al. “End to End Learning for Self-Driving Cars”, \n2016 \n[4] \nD. Xu, E. Ricci, Y. Yan, J. Song and N. Sebe “Learning Deep \nRepresentations of Appearance and Motion for Anomalous \nEvent Detection”, CoRR, abs/1510.01533, 2015 \n[5] \nR. Miotto, F.Wang, S. Wang, X. Jiang and JT Dudley “Deep \nlearning \nfor \nhealthcare: \nreview, \nopportunities \nand \nchallenges”, Briefings in bioinformatics (SP), 2017 \n[6] \nI.J. Goodfellow, J. Shlens and C. Szegedy, “Explaining and \nHarnessing Adversarial Examples”, arXiv:1412.6572, 2014 \n[7] \nC. Szegedy et al. , “Going deeper with convolutions”, IEEE \nConference on Computer Vision and Pattern Recognition, \n2015 \n[8] \nJ.J. Hopfield, “Neural networks and physical systems with \nemergent collective computational abilities”, Proceedings of \nthe National Academy of Sciences of the USA, 1982 \n[9] \nJ. Kiefer and J. Wolfowitz, “Stochastic Estimation of the \nMaximum of a Regression Function”, Ann. Math. Statist., vol \nno. 3, pp 462-466, 1952 \n[10] D.P. Kingma and J. Ba, “Adam: A Method for Stochastic \nOptimization”, \nInternational Conference \nfor \nLearning \nRepresentations, 2015 \n[11] G. Hinton, “Neural Networks for Machine Learning - Lecture \n6a - Overview of mini-batch gradient descent”, 2012 \n[12] C. Szegedy et al. , “Intriguing properties of neural networks”, \nCoRR, abs/1312.6199, 2013 \n[13] X. Yuan, P. He, Q. Zhu and X. Li,  “Adversarial Examples: \nAttacks \nand \nDefenses \nfor \nDeep \nLearning”, \narXiv:1712.07107, 2017 \n[14] Y. Liu, X. Chen, C. Liu and D. Song, “Delving into \nTransferable Adversarial Examples and Black-box Attacks”, \nCoRR, abs/1611.02770, 2016 \n[15] R. \nHecht-Nielsen, \n“The \nMunificence \nof \nHigh \nDimensionality”, ICANN-92, pp 1017-1030, 1992 \n[16] J.N. Paranjape, V.G. Venkoparao and R.K. Dubey, \n“Exploring the role of Input and Output Layers of a Deep \nNeural Network in Adversarial Defense”, In Press, 2020 \n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-06-04",
  "updated": "2020-06-04"
}