{
  "id": "http://arxiv.org/abs/2006.02986v1",
  "title": "A Novel Update Mechanism for Q-Networks Based On Extreme Learning Machines",
  "authors": [
    "Callum Wilson",
    "Annalisa Riccardi",
    "Edmondo Minisci"
  ],
  "abstract": "Reinforcement learning is a popular machine learning paradigm which can find\nnear optimal solutions to complex problems. Most often, these procedures\ninvolve function approximation using neural networks with gradient based\nupdates to optimise weights for the problem being considered. While this common\napproach generally works well, there are other update mechanisms which are\nlargely unexplored in reinforcement learning. One such mechanism is Extreme\nLearning Machines. These were initially proposed to drastically improve the\ntraining speed of neural networks and have since seen many applications. Here\nwe attempt to apply extreme learning machines to a reinforcement learning\nproblem in the same manner as gradient based updates. This new algorithm is\ncalled Extreme Q-Learning Machine (EQLM). We compare its performance to a\ntypical Q-Network on the cart-pole task - a benchmark reinforcement learning\nproblem - and show EQLM has similar long-term learning performance to a\nQ-Network.",
  "text": "A Novel Update Mechanism for Q-Networks Based\nOn Extreme Learning Machines\nCallum Wilson\nDepartment of Mechanical and Aerospace Engineering\nUniversity of Strathclyde\nGlasgow, United Kingdom\ncallum.j.wilson@strath.ac.uk\nAnnalisa Riccardi\nDepartment of Mechanical and Aerospace Engineering\nUniversity of Strathclyde\nGlasgow, United Kingdom\nannalisa.riccardi@strath.ac.uk\nEdmondo Minisci\nDepartment of Mechanical and Aerospace Engineering\nUniversity of Strathclyde\nGlasgow, United Kingdom\nedmondo.minisci@strath.ac.uk\nAbstract—Reinforcement learning is a popular machine learn-\ning paradigm which can ﬁnd near optimal solutions to complex\nproblems. Most often, these procedures involve function approx-\nimation using neural networks with gradient based updates to\noptimise weights for the problem being considered. While this\ncommon approach generally works well, there are other update\nmechanisms which are largely unexplored in reinforcement\nlearning. One such mechanism is Extreme Learning Machines.\nThese were initially proposed to drastically improve the training\nspeed of neural networks and have since seen many applications.\nHere we attempt to apply extreme learning machines to a\nreinforcement learning problem in the same manner as gradient\nbased updates. This new algorithm is called Extreme Q-Learning\nMachine (EQLM). We compare its performance to a typical Q-\nNetwork on the cart-pole task - a benchmark reinforcement\nlearning problem - and show EQLM has similar long-term\nlearning performance to a Q-Network.\nI. INTRODUCTION\nM\nACHINE learning methods have developed signiﬁ-\ncantly over many years and are now applied to in-\ncreasingly practical and real world problems. For example,\nthese techniques can optimise control tasks which are often\ncarried out inefﬁciently by basic controllers. The ﬁeld of\nReinforcement Learning (RL) originates in part from the\nstudy of optimal control [1], where a controller is designed\nto maximise, or minimise, a characteristic of a dynamical\nsystem over time. It is often impossible or impractical to derive\nan analytical optimal control solution for environments with\ncomplex or unknown dynamics, which motivates the use of\nmore intelligent methods such as RL. In particular, intelligent\ncontrollers must be capable of learning quickly online to adapt\nto changes. The study of optimal control and RL brought\nc⃝2020 IEEE. Personal use of this material is permitted. Permission from\nIEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works.\nmachine learning into the broader ﬁeld of engineering with\napplications to a wide variety of problems [2].\nThe generalisation performance of RL-derived controllers\nsigniﬁcantly improved with the incorporation of function ap-\nproximators [3]. Unlike the earlier tabular methods, architec-\ntures such as fuzzy logic controllers [4] or more commonly\nNeural Networks (NNs) can exploit similarities in areas of\nthe state space to learn better policies. This comes at a cost:\nNNs usually take a long time to train and in general they do\nnot guarantee convergence. Furthermore, nonlinear function\napproximators can be unstable and cause the learning algo-\nrithm to diverge [5]. Despite this, through careful selection of\nhyperparameters and use of additional stability improvement\nmeasures, as will be discussed later, such function approxi-\nmators can still obtain useful solutions to control problems.\nOf all the algorithms available for tuning network weights,\nbackpropagation is the most widely used in state-of-the-art\nsystems [6], [7], [8], [9]. The most common alternatives\nto this approach involve evolutionary algorithms, which can\nbe used to evolve network weights or replace the function\napproximator entirely [10]. Such algorithms tend to show\nbetter performance but have a much higher computational cost\nwhich can make them infeasible for certain learning problems.\nExtreme Learning Machines (ELMs) are a class of neural\nnetworks which avoid using gradient based updates [11].\nFor certain machine learning problems, ELM has several\nadvantages over other update rules - mainly that it can be con-\nsiderably faster than iterative methods for optimising network\nweights since they are instead calculated analytically. ELM\nhas seen many improvements and adaptations allowing it to\nbe applied to a wide variety of problems involving function\napproximation [12]. These include applications within the\nrealm of RL, such as using a table to provide training data for\nan ELM network [13], approximating system dynamics using\nELM to later apply RL methods [14], or using ELM theory to\nderive an analytical solution for weight updates based on the\narXiv:2006.02986v1  [cs.NE]  4 Jun 2020\nAgent\nReward\nState\nEnvironment\nAction\nFig. 1. Agent-environment interaction in reinforcement learning, where the\nagent observes a state and reward signal from the environment and uses this\ninformation to select an action to take\nloss function of gradient-based updates [15]. Here we aim to\nuse ELM in a conventional RL algorithm by only altering the\nneural network update rule. The algorithm which uses ELM\nin this manner is referred to here as the “Extreme Q-Learning\nMachine” (EQLM).\nIn this paper we develop the EQLM algorithm and compare\nits performance to a standard Q-network of the same complex-\nity. The type of Q-network used here is relatively primitive but\nincorporates some features to improve stability and general\nlearning performance to provide a reasonable comparison. A\nfull stability analysis of each algorithm is outwith the scope\nof this paper, however we compare their performance using\nstandard measures of learning. EQLM uses an incremental\nform of ELM which allows updates to be made online while\nthe RL agent is interacting with the environment. Tests are\ncarried out on a classical RL benchmark known as the cart-\npole problem.\nII. BACKGROUND\nA RL process consists of an agent which senses an envi-\nronment in a certain state and carries out actions to maximise\nfuture reward [2]. The only feedback the agent receives from\nthe environment is a state signal and reward signal and it can\nonly affect the environment by its actions as shown schemat-\nically in Figure 1. The objective is then to maximise the total\ndiscounted reward it receives. One method for optimising the\nreward is Q-Learning, where the agent learns the action-value\nfunction, Q of its policy and uses this to improve the policy in\nan iterative process [16]. The temporal-difference (TD) error\nof Q is deﬁned as shown:\ne = Q(st, at) −\n\u0010\nrt+1 + γ max\na\nQ(st+1, a)\n\u0011\n(1)\nwhere st, at, and rt denote state, action, and reward respec-\ntively at time-step t, γ is the discount factor which determines\nthe affect of long term rewards on the TD error, and Q(s, a) is\nthe estimated action-value function. We approximate Q(s, a)\nusing a feed-forward NN with parameters θ and, for the stan-\ndard Q-network, perform updates using the mean-squared TD\nerror. Approximating the value function and updating using\nTD-error forms the basis for the most rudimentary Q-Learning\nalgorithms. This section details the additional features of the\nQ-Learning algorithm which are employed in EQLM.\nA. ϵ-Greedy Policies\nTo ﬁnd an optimal solution, an agent must visit every state\nin the environment throughout its training, which requires\nthe agent to explore the environment by periodically taking\nrandom actions. This conﬂicts with the agent’s global goal of\ntaking actions deemed optimal by the current control policy\nto improve the policy; thus the well known issue of balancing\nexploration and exploitation. One type of policies which help\nremedy this issue are known as “ϵ-greedy” policies [2]. In\nthese policies, a parameter ϵ dictates the probability of taking\na random action at each time-step, where 0 ≤ϵ ≤1, and\nthis can be tuned to give the desired trade-off between taking\nexploratory or exploitative actions. Exploration becomes less\nnecessary later in the training period once the agent has more\nexperience. Instead, the agent seeks to exploit actions consid-\nered more “optimal” following a period of more exploration.\nTo achieve this in practice, ϵ varies linearly during the training\nperiod from ϵi to ϵf over Nϵ episodes. Following this, ϵ is\nheld constant at ϵ = ϵf for the remainder of training. The\nexploration probability after n episodes is given by Equation\n2.\nϵ =\n(\nϵi −\nn\nNϵ (ϵi −ϵf) ,\nif n < Nϵ\nϵf,\nif n ≥Nϵ\n(2)\nB. Target Network\nA crucial issue with Q-networks is that they are inherently\nunstable and will tend to overestimate action-values, which\ncan cause the predicted action-values to diverge [17]. Several\nmethods to resolve this issue have been proposed, including\nthe use of a target network [7]. This network calculates target\naction-values for updating the policy network and shares its\nstructure with this network. The parameters of the policy\nnetwork, θ are periodically transferred to the target network,\nwhose parameters are denoted θ−, which otherwise remain\nconstant. In practise, the target network values are updated\nevery C time-steps. This slightly decouples the target values\nfrom the policy network which reduces the risk of divergence.\nC. Experience Replay\nThe online methods of learning discussed thus far all\nconventionally make updates on the most recent observed\nstate transition, which has several limitations [7]. For example,\nstates which are only visited once may contain useful update\ninformation which is quickly lost and updating on single state\ntransitions results in low data efﬁciency of the agent’s expe-\nrience. A more data efﬁcient method of performing updates\nutilises experience replay [18]. In this method, experiences\nof transitions are stored in a memory, D which contains the\nstate sj, action taken aj, observed reward rj+1, and observed\nstate sj+1 for each transition. Updates are then made on\na “minbatch” of k experiences selected randomly from the\nmemory at every time-step. To limit the number of state\nFig. 2. Q-Network algorithm\n1: initialise network with random weights\n2: for episode= 1 to Nep do\n3:\ninitialise state st ←s0\n4:\nwhile state st is non-terminal do\n5:\nselect action at according to policy π\n6:\nexecute action at and observe r, st+1\n7:\nupdate memory D with (st, at, rt, st+1)\n8:\nselect\nrandom\nminibatch\nof\nk\nexperiences\n(sj, aj, rj, sj+1) from D\n9:\ntj =\n(\nrj,\nif sj+1 is terminal\nrj + γ maxa QT (sj+1, a),\notherwise\n10:\nej = Q(sj, aj) −(rj+1 + γ maxa QT (sj+1, a))\n11:\nupdate network using the error ej for each transition\nin the minibatch\n12:\nafter C time-steps set θ−←θ\n13:\nend while\n14: end for\ntransitions stored, a maximum memory size Nmem is deﬁned\nsuch that a moving window of Nmem previous transitions are\nstored in the agent’s memory [17].\nD. Q-Network Algorithm\nFigure 2 details the Q-Network algorithm which incorpo-\nrates a target network and experience replay. This algorithm\ngives our baseline performance to which we compare Extreme\nQ-Learning Machine (EQLM) and also provides the basis for\nincorporating ELM as a novel update mechanism.\nIII. ELM THEORY AND DEVELOPMENT\nA. Extreme Learning Machine\nELM in its most widely used form is a type of single-layer\nfeedforward network (SLFN). The description of ELM herein\nuses the same notation as in [11]. Considering an arbitrary\nset of training data (xi, ti) where xi = [xi1, xi2, . . . , xin]\nand ti = [ti1, ti2, . . . , tim], a SLFN can be mathematically\nmodelled as follows\n˜\nN\nX\ni=1\nβig (wi · xj + bi) = oj, j = 1, . . . , N\n(3)\nwhere\n˜N\nis\nthe\nnumber\nof\nhidden\nnodes,\nβi\n=\n[βi1, βi2, . . . , βim]T is the output weight vector which con-\nnects the ith hidden node to the output nodes, g(x) is the\nactivation function, wi = [wi1, wi2, ..., win]T is the input\nweight vector which connects the ith hidden node to the input\nnodes, and bi is the bias of the ith hidden node. Where the\nnetwork output oj has zero error compared to the targets tj\nfor all N samples, P ˜\nN\nj=1 ∥oj −tj∥= 0 it can be written that\n˜\nN\nX\ni=1\nβig (wi · xj + bi) = tj, j = 1, . . . , N\n(4)\nwhich contains the assumption that the SLFN can approximate\nthe N samples with zero error. Writing this in a more compact\nform gives\nHβ = T\n(5)\nwhere H is the hidden layer output matrix, β is the output\nweight vector matrix, and T is the target matrix. These are\ndeﬁned as shown\nH =\n\n\ng(w1 · x1 + b1)\n· · ·\ng(w ˜\nN · x1 + b ˜\nN)\n...\n· · ·\n...\ng(w1 · xN + b1)\n· · ·\ng(w ˜\nN · xN + b ˜\nN)\n\n\nN× ˜\nN(6)\nβ =\n\n\nβT\n1\n...\nβT\n˜\nN\n\n\n˜\nN×m\n(7)\nT =\n\n\ntT\n1\n...\ntT\nN\n\n\nN×m\n(8)\nELM performs network updates by solving the linear system\ndeﬁned in equation 5 for β\nˆβ = H†T\n(9)\nwhere H† here denotes the Moore-Penrose generalised inverse\nof H as deﬁned in equation 10. This is used since, in general,\nH is not a square matrix and so cannot be inverted directly.\nH† =\n\u0000HHT \u0001† HT\n(10)\nThe method used by ELM to update its weights has several\nadvantages over classical methods of updating neural net-\nworks. It is proven in [11] that ˆβ is the smallest norm least\nsquares solution for β in the linear system deﬁned by equation\n5, which is not always the solution reached using classical\nmethods. ELM also avoids many of the issues commonly\nassociated with neural networks such as converging to local\nminima and improper learning rate. Such problems are usually\navoided by using more sophisticated algorithms, whereas ELM\nis far simpler than most conventional algorithms.\nB. Regularized ELM\nDespite the many beneﬁts of ELM, several issues with\nthe algorithm are noted in [19]. Mainly, the algorithm still\ntends to overﬁt and is not robust to outliers in the input data.\nThe authors propose a Regularized ELM which attempts to\nbalance the empirical risk and structural risk to give better\ngeneralisation. This differs to the ELM algorithm which is\nsolely based on empirical risk minimisation.\nThe main feature of regularized ELM is the introduction of\na parameter ¯γ which regulates the amount of empirical and\nstructural risk. This parameter can be adjusted to balance the\nrisks and obtain the best generalisation of the network. Weights\nare calculated as shown:\nβ =\n\u0012I\n¯γ + HT D2H\n\u0013†\nHT T\n(11)\nwhich incorporates the parameter ¯γ and a weighting matrix\nD. Setting D as the identity matrix I yields an expression for\nunweighted regularized ELM:\nβ =\n\u0012I\n¯γ + HT H\n\u0013†\nHT T\n(12)\nwhich is a simpliﬁcation of equation 11. ELM is then the\ncase of equation 12 where ¯γ →∞. Adding the parameter ¯γ\nadds some complexity to the ELM algorithm because of its\ntuning, however regularized ELM still maintains most of the\nadvantages of ELM over conventional neural networks.\nC. Incremental Extreme Learning Machine\nIt is desired to perform network updates sequentially on\nbatches of data which necessitates an incremental form of\nELM. Such an algorithm is presented in [20] whose basis\nis the regularized form of ELM shown in equation 12. The\nalgorithm used for the purposes of EQLM is the least square\nincremental extreme learning machine (LS-IELM).\nFor an initial set of N training samples (xi, ti) the LS-\nIELM algorithm initialises the network weights as shown:\nβ = A†\ntHT T\n(13)\nwhere\nAt = I\n¯γ + HT H\n(14)\nand H and T are given by equations 6 and 8. Suppose new\nsets of training data arrive in chunks of k samples - the hidden\nlayer output matrix and targets for a new set of k samples are\nas shown:\nHIC =\n\n\ng(w1 · xN + b1)\n· · ·\ng(w ˜\nN · xN + b ˜\nN)\n...\n· · ·\n...\ng(w1 · xN+k + b1)\n· · ·\ng(w ˜\nN · xN+k + b ˜\nN)\n\n\nk× ˜\nN\n(15)\nTIC =\n\n\ntT\nN+1\n...\ntT\nN+k\n\n\nk×m\n(16)\nTo perform updates using the most recent data at time t, Kt\nis deﬁned as\nKt = I −A†\ntHT\nIC\n\u0010\nHICA†\ntHT\nIC + Ik×k\n\u0011†\nHIC\n(17)\nand the update rules for β and A are then as follows:\nβt+1 = Ktβt + KtA†\ntHT\nICTIC\n(18)\nA†\nt+1 = KtA†\nt\n(19)\nD. Extreme Q-Learning Machine\nThe algorithm for applying Q-learning using LS-IELM\nbased updates, here referred to as the Extreme Q-Learning\nMachine (EQLM) is shown in Figure 3. Similar to the Q-\nnetwork algorithm in Figure 2, this uses experience replay\nand a target network to improve its performance. Unlike the\nQ-network, the TD-error is not calculated and instead a target\nmatrix, T for the minibatch of data is created which has the\npredicted action-values for all actions in the given states. The\ntarget action-value for each state, sj is then assigned to the\napplicable value in tj. Matrix H is constructed using the states\nin the minibatch and then the update rules are applied. The\nboolean variable step0 is introduced to initialise the network\nat the very ﬁrst update.\nOne further key difference in the EQLM algorithm is\nthe heuristic policy used in initial episodes. The return in\ninitial episodes has a substantial effect on the convergence of\nEQLM as discussed later. This necessitates a simple heuristic\ncontroller for the start of training which does not need to\nperform very well, but can at least prevent the agent from\nconverging on a highly sub-optimal policy. EQLM uses a\nheuristic action selection at = h0(t), which is effectively an\nopen loop control scheme dependant only on the time-step,\nfor Nh episodes. Deﬁnition of this heuristic is discussed in\nthe following section.\nIV. EXPERIMENTS AND RESULTS\nCode\nto\nreproduce\nresults\nis\navailable\nat\nhttps://github.com/strath-ace/smart-ml.\nA. OpenAI Gym Environments\nThe environment used to test the algorithms comes from the\nOpenAI Gym which is a toolkit containing benchmark tests\nfor a variety of machine learning algorithms [21]. The gym\ncontains, among other environments, several classical control\nproblems, control tasks which use the MuJoCo physics engine\n[22], and the Atari2600 games which are used in [7]. Here the\nagents will be tested on the environment named “CartPole-v0”.\nThe cart-pole problem was originally devised in [23] where\nthe authors created an algorithm called “BOXES” to learn\nto control the system. In this task, a pole is attached by a\nhinge to a cart which rolls along a track and is controlled\nby two possible actions - an applied force of ﬁxed magnitude\nin either the positive or negative x-direction along the track.\nThe goal is to keep the pendulum from toppling for as long\nas possible, which yields a very simple reward function of\nr = +1 for every time-step where the pendulum has not\ntoppled. In addition, the track on which the cart is situated\nis ﬁnite and reaching the limits of the track also indicates\nfailure. The dynamics of the system used in the gym are the\nsame as those deﬁned by [24]. The state-space size for this\nenvironment is 4 and the action-space size is 2.\nThis problem can be considered an “episodic” task [2],\nwhere the learning is divided into episodes which have de-\nﬁned ending criteria. An episode terminates either when the\npendulum passes 12◦or the cart reaches either end of the track.\nFig. 3. EQLM algorithm\n1: initialise network with random weights\n2: step0 ←True\n3: for episode= 1 to Nep do\n4:\ninitialise state st ←s0\n5:\nwhile state st is non-terminal do\n6:\nif episode≤Nh then\n7:\nselect action at according to heuristic h0(t)\n8:\nelse\n9:\nselect action at according to policy π\n10:\nend if\n11:\nexecute action at and observe r, st+1\n12:\nupdate memory D with (st, at, rt, st+1)\n13:\nselect\nrandom\nminibatch\nof\nk\nexperiences\n(sj, aj, rj, sj+1) from D\n14:\ntj =\n(\nrj,\nif sj+1 is terminal\nrj + γ maxa Q(sj+1, a),\notherwise\n15:\nconstruct matrix H\n16:\nif step0 then\n17:\nAt = I\n¯γ + HT H\n18:\nβt+1 = A†\ntHT T\n19:\nAt+1 = At\n20:\nstep0 ←False\n21:\nelse\n22:\nKt = I −A†\ntHT \u0010\nHA†\ntHT + Ik×k\n\u0011†\nH\n23:\nβt+1 = Ktβt + KtA†\ntHT T\n24:\nA†\nt+1 = KtA†\nt\n25:\nend if\n26:\nafter C time-steps set θ−←θ\n27:\nend while\n28: end for\nIn this task, a maximum number of time-steps per episode of\n200 is set within the gym.\nB. Heuristic Policy\nAs discussed previously, EQLM is susceptible to converging\non a sub-optimal policy without the use of a heuristic policy\nin the initial episodes. A random policy at the start of training\nwill sometimes produce this sub-optimal result and so we\nneed to deﬁne a simple deterministic policy which does not\nimmediately solve the task but prevents unacceptable long-\nterm performance. For the cart-pole task we consider here\nwhich has a binary action space, we deﬁne the heuristic policy\nas taking alternating actions at each time-step as shown:\nh0(t) = mod(t, 2)\n(20)\nFrom testing, we found Nh = 5 to be a suitable number of\nepisodes over which to use the heuristic. The effect of this\ninitial heuristic policy is shown in Figure 4. This shows the\naveraged rewards over the ﬁrst 200 episodes of training for\nboth networks with and without the heuristic. While the return\nin the initial episodes is still higher for EQLM in both cases, it\nis clear that with the heuristic EQLM shows a more favourable\nperformance. This is due to occasions where, without the\nheuristic, EQLM quickly converges to a sub-optimal policy,\nwhich is mitigated by the heuristic policy. Also shown is the\naverage performance of the heuristic alone, which receives\na reward of 37 per episode. This indicates that although the\nheuristic alone performs very poorly on the task, it is still\nuseful to improve the performance of both algorithms.\nFig. 4. Varying performance with the use of an initial heuristic h0 with the\naverage performance for the heuristic alone shown\nC. Hyperparameter Selection\nThe performance of a Q-learning agent can be very sensitive\nto its hyperparameters. To create a useful comparison of each\nagent’s performance we therefore need to tune the hyperpa-\nrameters for this problem. Here we use the Python library\nHyperopt which is suitable for optimising within combined\ndiscrete- and real-valued search spaces [25]. Hyperparameters\nto be optimised are: learning rate α (Q-Network only), regular-\nisation parameter ¯γ (EQLM only), number of hidden nodes ˜N,\ninitial exploration probability ϵi (with ϵf ﬁxed as 0), number\nof episodes to decrease exploration probability Nϵ, discount\nfactor γ, minibatch size k, and target network update steps C.\nOur main objective to optimise is the ﬁnal performance of\nthe agent, i.e. the total reward per episode, after it converges\nto a solution. In addition, an agent should converge to the\noptimal solution in as few episodes as possible. Both these\nobjectives can be combined into the single metric of area under\nthe learning curve as shown in Figure 5. Since hyperopt uses\na minimisation procedure, we speciﬁcally take the negative\narea under the curve. One of the issues with optimising these\nsystems is their stochastic nature which can result in several\nruns with the same hyperparameters having vastly different\nperformance. To account for this, each evaluation uses 8 runs\nand the loss is the upper 95% conﬁdence interval of the metric\nfrom these runs. This gives a conservative estimate of the\nworst-case performance for a set of hyperparameters.\nTable I shows the best parameters obtained when tuning the\nhyperparameters for this task. Most of the hyperparameters\ncommon to each algorithm are not substantially different with\nFig. 5.\nExample learning curves which show different values for the loss\nfunction\nthe exception of minibatch size, k which is 26 and 2 for the\nQ-network and EQLM respectively. In fact, the performance\nof EQLM tended to decrease for larger values of k which\nwas not the case for the Q-network. This could be a result\nof the matrix inversion in EQLM where the behaviour of the\nnetwork is less stable if the matrix is non-square. Alternatively,\nit is possible that EQLM attempting to ﬁt to a much larger\nnumber of predicted Q-values causes the overall performance\nto decrease. The fact it needs fewer data per time-step than\na standard Q-network could also indicate that EQLM is more\nefﬁcient at extracting information on the environment’s action-\nvalues compared to using gradient descent.\nHyperparameter\nQ-Network\nEQLM\nα\n0.0065\n-\n¯γ\n-\n1.827e-5\n˜\nN\n29\n25\nϵi\n0.670\n0.559\nNϵ\n400\n360\nγ\n0.99\n0.93\nk\n26\n2\nC\n70\n48\nTABLE I\nHYPERPARAMETERS USED FOR EACH AGENT IN THE CART-POLE TASK\nD. Learning Performance\nWith the selected hyperparameters, each agent carried out\n50 runs of 600 episodes in the cart-pole environment to\ncompare their performance. The results of this are shown\nin Figure 6 and Table II. Here we use two measures of\nperformance: mean reward over the ﬁnal 100 episodes and\narea under the learning curve (auc).\nFrom the learning curves, we see EQLM on average\nachieves a superior performance in the earliest episodes of\ntraining followed by a steady increase in return until it\nplateaus. The Q-network begins with comparatively low av-\nerage return but then shows a sharp increase in return before\nits performance plateaus for the remainder of the episodes.\nAfter each of the learning curves plateau at their near-optimal\nFig. 6. Learning curves for EQLM and a standard Q-Network in the cart-pole\ntask. Results are averaged over all 50 runs at each episode and shaded area\nindicates the 95% conﬁdence interval\nMeasure\nQ-Network\nEQLM\nreward\nmean\n160.0 (147.5, 173.7)\n166.9 (160.7, 173.3)\nstd\n47.0 (35.1, 62.2)\n23.1 (20.3, 26.7)\nauc (∗103)\nmean\n84.1 (81.0 87.4)\n83.3 (80.4, 86.2)\nstd\n11.7 (9.1, 14.7)\n10.6 (9.3, 12.4)\nTABLE II\nPERFORMANCE OF EACH ALGORITHM IN THE CART-POLE TASK\nperformance, we see some of the most interesting differences\nbetween the two algorithms. The average return for EQLM\nremains very consistent as do the conﬁdence intervals, how-\never the Q-network displays some temporal variation in its\nperformance as training continues and the conﬁdence intervals\ntend to get larger. This shows that the long-term performance\nof EQLM is more consistent than the equivalent Q-network,\nwhich is backed up by the data in Table II. The standard\ndeviation of the mean reward of EQLM (23.1) is less than\nhalf that of the Q-network (47.0) and both algorithms have\ncomparable mean rewards (160.0 and 166.9 for Q-network\nand EQLM respectively).\nTo ﬁnd a statistical measure of the difference in performance\nof each algorithm, we use a two-tailed t-test [26]. This assumes\nboth algorithms’ performance belongs to the same distribution\nwhich we reject when the p-value is less than a threshold of\n0.05. When comparing the mean reward in the ﬁnal episodes,\nthe t-test yielded values of t = −0.628, p = 0.531. Similarly\nfor the area under the learning curve we obtained t = −1.16,\np = 0.24. As a result, we cannot reject the hypothesis\nthat the performance of both algorithms follows the same\ndistribution. This demonstrates EQLM as being capable of\nachieving similar performance to a standard Q-Network in this\ntask.\nV. CONCLUSION\nThis paper proposed a new method of updating Q-networks\nusing techniques derived from ELM called Extreme Q-\nLearning Machine (EQLM). When compared to a standard\nQ-network on the benchmark cart-pole task, EQLM shows\ncomparable average performance which it achieves more con-\nsistently than the Q-network. EQLM also shows better initial\nlearning performance when initialised using a basic heuristic\npolicy.\nWhile EQLM shows several advantages to standard Q-\nnetworks, it is clear that the conventional gradient descent\nmethods are also capable of learning quickly as they gain\nmore experience. Future work could look at combining the\nstrengths of EQLM’s initial performance and using gradient-\nbased methods to accelerate the learning. In this paper we\nhave tuned the hyperparameters of EQLM for a speciﬁc\nproblem, but a more rigorous parametric study is necessary\nto learn more about the effect of the hyperparameters on\nEQLM’s learning performance. One of the developments in\nELM which was not used here is the ELM-based multilayer\nperceptron [27]. Such a network could similarly be used for\nRL problems since deep networks are generally better suited\nto more complex tasks [28].\nThe results in this paper suggest ELM methods are capable\nof being used within RL with similar performance and greater\nconsistency than conventional gradient-descent for simple RL\nproblems. Additional research is needed on the application of\nEQLM to higher dimensional and adaptive control problems.\nACKNOWLEDGMENT\nThe authors would like to thank the University of Strath-\nclyde Alumni Fund for their support.\nREFERENCES\n[1] R. E. Bellman, “The theory of dynamic programming,” Bulletin of the\nAmerican Mathematical Society, vol. 60, p. 503516, 1954.\n[2] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.\nCambridge: MIT Press, 1998.\n[3] R. S. Sutton, “Generalization in Reinforcement Learning : Successful\nExamples Using Sparse Coarse Coding,” Advances in Neural Informa-\ntion Processing Systems, vol. 8, pp. 1038–1044, 1996.\n[4] R. Davoodi and B. J. Andrews, “Computer simulation of FES standing\nup in paraplegia: A self-adaptive fuzzy controller with reinforcement\nlearning,” IEEE Transactions on Rehabilitation Engineering, 1998.\n[5] J. N. Tsitsiklis and B. Van Roy, “An Analysis of Temporal-Difference\nLearning with Function Approximation,” IEEE Transactions on Auto-\nmatic Control, vol. 42, no. 5, pp. 674–690, 1997.\n[6] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner,\nI. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and\nD. Hassabis, “Mastering the game of Go with deep neural networks and\ntree search,” Nature, vol. 529, 2016.\n[7] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\nS. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,\nD. Wierstra, S. Legg, and D. Hassabis, “Human-level control through\ndeep reinforcement learning,” Nature, vol. 518, 2015.\n[8] V. Mnih, A. Puigdom`enech Badia, M. Mirza, A. Graves, T. Harley, T. P.\nLillicrap, D. Silver, and K. Kavukcuoglu, “Asynchronous Methods for\nDeep Reinforcement Learning,” in International conference on Machine\nLearning, 2016.\n[9] H. Van Hasselt, A. Guez, and D. Silver, “Deep Reinforcement Learning\nWith Double Q-Learning,” in Proceedings of the 30th AAAI Conference\non Artiﬁcial Intelligence, 2016, pp. 2094–2100.\n[10] D. E. Moriarty and A. C. Schultz, “Evolutionary Algorithms for Rein-\nforcement Learning,” Journal of Artiicial Intelligence Research, vol. 11,\npp. 241–276, 1999.\n[11] G. B. Huang, Q. Y. Zhu, and C. K. Siew, “Extreme learning machine:\nTheory and applications,” Neurocomputing, vol. 70, no. 1-3, pp. 489–\n501, 2006.\n[12] G. B. Huang, D. H. Wang, and Y. Lan, “Extreme Learning Machines: A\nSurvey,” International Journal of Machine Learning and Cybernetics,\nvol. 2, no. 2, pp. 107–122, 2011.\n[13] J. M. Lopez-Guede, B. Fernandez-Gauna, and M. Gra˜na, “State-Action\nValue Function Modeled by ELM in Reinforcement Learning for Hose\nControl Problems,” International Journal of Uncertainty, vol. 21, pp.\n99–116, 2013.\n[14] J. M. Lopez-Guede, B. Fernandez-Gauna, and J. A. Ramos-Hernanz,\n“A L-MCRS dynamics approximation by ELM for Reinforcement\nLearning,” Neurocomputing, vol. 150, pp. 116–123, 2014.\n[15] T. Sun, B. He, and R. Nian, “Target Following for an Autonomous Un-\nderwater Vehicle Using Regularized ELM-based Reinforcement Learn-\ning,” in OCEANS’15 MTS/IEEE Washington, 2015, pp. 1–5.\n[16] C. J. C. H. Watkins, “Learning from Delayed Rewards,” Ph.D. disserta-\ntion, King’s College, 1989.\n[17] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-\nstra, and M. Riedmiller, “Playing Atari with Deep Reinforcement\nLearning,” arXiv preprint arXiv:1312.5602, 2013.\n[18] L.-J. Lin, “Self-Improving Reactive Agents Based On Reinforcement\nLearning, Planning and Teaching,” Machine Learning, vol. 8, pp. 293–\n321, 1992.\n[19] W. Deng, Q. Zheng, and L. Chen, “Regularized Extreme Learning\nMachine,” IEEE Symposium on Computational Intelligence and Data\nMining, no. 60825202, pp. 389–395, 2009.\n[20] L. Guo, J. h. Hao, and M. Liu, “An incremental extreme learning\nmachine for online sequential learning problems,” Neurocomputing, vol.\n128, pp. 50–58, 2014.\n[21] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,\nJ. Tang, and W. Zaremba, “OpenAI Gym,” pp. 1–4, 2016.\n[22] E. Todorov, T. Erez, and Y. Tassa, “MuJoCo: A physics engine for\nmodel-based control,” IEEE International Conference on Intelligent\nRobots and Systems, pp. 5026–5033, 2012.\n[23] D. Michie and R. A. Chambers, “BOXES: An Experiment in Adaptive\nControl,” Machine Intelligence, vol. 2, pp. 137–152, 1968.\n[24] A. G. Barto, R. S. Sutton, and C. W. Anderson, “Neuronlike Adaptive\nElements That Can Solve Difﬁcult Learning Control Problems,” IEEE\nTransactions on Systems, Man and Cybernetics, vol. SMC-13, no. 5, pp.\n834–846, 1983.\n[25] J. Bergstra, D. Yamins, and D. D. Cox, “Making a science of model\nsearch: Hyperparameter optimization in hundreds of dimensions for\nvision architectures,” in 30th International Conference on Machine\nLearning, ICML 2013, vol. 28, no. PART 1, 2013, pp. 115–123.\n[26] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger,\n“Deep Reinforcement Learning that Matters,” in The Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, 2018, pp. 3207–3214.\n[27] J. Tang, C. Deng, and G.-B. Guang, “Extreme learning machine for\nmultilayer perceptron,” IEEE Transactions on Neural Networks and\nLearning Systems, vol. 27, no. 4, pp. 809–821, 2015.\n[28] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n",
  "categories": [
    "cs.NE",
    "cs.LG"
  ],
  "published": "2020-06-04",
  "updated": "2020-06-04"
}