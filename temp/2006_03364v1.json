{
  "id": "http://arxiv.org/abs/2006.03364v1",
  "title": "Structure preserving deep learning",
  "authors": [
    "Elena Celledoni",
    "Matthias J. Ehrhardt",
    "Christian Etmann",
    "Robert I McLachlan",
    "Brynjulf Owren",
    "Carola-Bibiane Schönlieb",
    "Ferdia Sherry"
  ],
  "abstract": "Over the past few years, deep learning has risen to the foreground as a topic\nof massive interest, mainly as a result of successes obtained in solving\nlarge-scale image processing tasks. There are multiple challenging mathematical\nproblems involved in applying deep learning: most deep learning methods require\nthe solution of hard optimisation problems, and a good understanding of the\ntradeoff between computational effort, amount of data and model complexity is\nrequired to successfully design a deep learning approach for a given problem. A\nlarge amount of progress made in deep learning has been based on heuristic\nexplorations, but there is a growing effort to mathematically understand the\nstructure in existing deep learning methods and to systematically design new\ndeep learning methods to preserve certain types of structure in deep learning.\nIn this article, we review a number of these directions: some deep neural\nnetworks can be understood as discretisations of dynamical systems, neural\nnetworks can be designed to have desirable properties such as invertibility or\ngroup equivariance, and new algorithmic frameworks based on conformal\nHamiltonian systems and Riemannian manifolds to solve the optimisation problems\nhave been proposed. We conclude our review of each of these topics by\ndiscussing some open problems that we consider to be interesting directions for\nfuture research.",
  "text": "STRUCTURE PRESERVING DEEP LEARNING\nELENA CELLEDONI 1, MATTHIAS J. EHRHARDT 2, CHRISTIAN ETMANN 3,\nROBERT I. MCLACHLAN 4, BRYNJULF OWREN 1, CAROLA-BIBIANE SCH¨ONLIEB 3 AND\nFERDIA SHERRY 3\nAbstract. Over the past few years, deep learning has risen to the foreground as a\ntopic of massive interest, mainly as a result of successes obtained in solving large-scale\nimage processing tasks. There are multiple challenging mathematical problems involved\nin applying deep learning: most deep learning methods require the solution of hard\noptimisation problems, and a good understanding of the tradeoﬀbetween computational\neﬀort, amount of data and model complexity is required to successfully design a deep\nlearning approach for a given problem. A large amount of progress made in deep learning\nhas been based on heuristic explorations, but there is a growing eﬀort to mathematically\nunderstand the structure in existing deep learning methods and to systematically design\nnew deep learning methods to preserve certain types of structure in deep learning. In\nthis article, we review a number of these directions: some deep neural networks can be\nunderstood as discretisations of dynamical systems, neural networks can be designed to\nhave desirable properties such as invertibility or group equivariance, and new algorithmic\nframeworks based on conformal Hamiltonian systems and Riemannian manifolds to solve\nthe optimisation problems have been proposed. We conclude our review of each of these\ntopics by discussing some open problems that we consider to be interesting directions\nfor future research.\n1. Introduction\nStructure preserving numerical schemes have their roots in geometric integration [57], and\nnumerical schemes that build on characterisations of PDEs as metric gradient ﬂows [5], just\nto name a few. The overarching aim of structure preserving numerics is to preserve certain\nproperties of the continuous model, e.g mass or energy conservation, in its discretisation.\nBut structure preservation is not just restricted to play a role in classical numerical analysis\nof ODEs and PDEs. Indeed, through the advent of continuum interpretations of neural\nnetworks [55, 44, 45, 108], structure preservation is also entering the ﬁeld of deep learning.\nHere, the main objectives are to use the continuum model and structure preserving schemes\nto derive stable and converging neural networks and associated training procedures, and\nalgorithms, e.g. neural networks which generalise well.\n1Department of Mathematical Sciences, NTNU, N-7491 Trondheim, Norway. Email addresses:\nelena.celledoni@ntnu.no; brynjulf.owren@ntnu.no\n2Institute for Mathematical Innovation, University of Bath, Bath BA2 7JU, UK. Email ad-\ndress: m.ehrhardt@bath.ac.uk\n3Department of Applied Mathematics and Theoretical Physics, University of Cambridge,\nWilberforce\nRoad,\nCambridge\nCB3\n0WA,\nUK. Email\naddresses:\ncetmann@damtp.cam.ac.uk;\ncbs31@cam.ac.uk; fs436@cam.ac.uk\n4Institute of Fundamental Sciences, Massey University, Private Bag 11-222, Palmerston\nNorth, New Zealand. Email address: r.mclachlan@massey.ac.nz\nDate: June 8, 2020.\n1\narXiv:2006.03364v1  [cs.LG]  5 Jun 2020\n2\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n1.1. Neural Networks. Neural networks are a rich class of machine learning models that\ncan be leveraged for many diﬀerent tasks including regression, classiﬁcation, natural lan-\nguage processing, reinforcement learning and image generation [82]. While it is diﬃcult\nto provide an all-encompassing deﬁnition for neural networks, they can generally be char-\nacterised as a combination of simple, parametric functions between feature spaces. These\nfunctions act as individual building blocks (commonly called the layers of the network).\nThe main mechanism for combining these layers, which we will adopt in this work, is by\nfunction composition.\nFor any k ∈{0, . . . , K −1}, let X k denote a vector space (our feature space). While\nin most applications, these are simply ﬁnite-dimensional Euclidean spaces, we will assume\nmore general structures (such as Banach spaces) when appropriate. With this, we then\ndeﬁne a generic layer\nf k : X k × Θk →X k+1,\nwhere Θk is the set of possible parameter values of this layer. A neural network\nΨ : X × Θ →Y\n(x, θ) 7→zK\n(1)\ncan then be deﬁned via the iteration\nz0 = x\nzk+1 = f k(zk, θk),\nk = 0, . . . , K −1,\n(2)\nsuch that X 0 = X and X K = Y, where θ := (θ0, . . . , θK−1) ∈Θ0 ×· · ·×ΘK−1 =: Θ denotes\nthe entirety of the network’s parameters. The ﬁrst layer is commonly referred to as the\nneural network’s input layer, the ﬁnal layer as the neural network’s output layer, and all of\nthe remaining layers are called hidden layers.\nWhile the above deﬁnitions are still quite general, in practice several standard layer\ntypes are employed. Most ubiquitous are those that can be written as a learnable aﬃne\ncombination of the input, followed by a simple, nonlinear function: the layer’s activation\nfunction. The quintessential example are fully-connected layers\nf : RM × (RM ′×M × RM ′) →RM ′\n(z, (A, b)) 7→σ(Az + b),\n(3)\nwhose parameters are the weight matrix A ∈RM ′×M and the bias vector b ∈RM ′. Its\nactivation function σ : RM ′ →RM ′ is typically applied component-wise, e.g. the hyperbolic\ntangent [tanh(z)]i := tanh(zi) or the rectiﬁed linear unit (ReLU) [relu(z)]i := max(0, zi).\nFor classiﬁcation networks, the most common choice for the output layer’s activation func-\ntion is the softmax activation function given by\n[softmax(z)]i =\nexp(zi)\nPM ′\nj=1 exp(zj)\n,\nsuch that the neural network’s output’s entries can be regarded as the individual class\nmembership probabilities of the input.\nAn important extension to the concept of fully-\nconnected layers lie in convolutional layers [83] where the matrix-vector product is replaced\nby the application of a (multi-channel) convolution operator. These are the main building\nblock of neural networks used in imaging applications.\nSTRUCTURE PRESERVING DEEP LEARNING\n3\nSuppose we are given a set of paired training data (xn, yn)N\nn=1 ⊂X × Y, which is the\ncase for predictive tasks like regression or classiﬁcation. Training the model then amounts\nto solving the optimisation problem\nmin\nθ∈Θ\n(\nE(θ) = 1\nN\nN\nX\nn=1\nLn(Ψ(xn, θ)) + R(θ)\n)\n.\n(4)\nHere Ln(y) := L(y, yn) : Y →R∞is the loss for a speciﬁc data point where L : Y × Y →\nR∞:= R ∪{∞} is a general loss function which usually satisﬁes L ≥0 and L(y1, y2) = 0\nif and only if y1 = y2. The function Ln is usually smooth on its eﬀective domain {y |\nLn(y) < ∞} and convex. R : Θ →R∞acts as a regulariser which penalises and constrains\nunwanted solutions. In this setting, solving (4) is a form of empirical risk minimisation\n[109]. Typically, variants of stochastic gradient descent are employed to solve this task.\nThe calculation of the necessary gradients is performed using the famous backpropagation\nalgorithm, which can be understood as an application of reverse-mode auto-diﬀerentiation\n[86].\n1.2. Residual Networks and Diﬀerential Equations. In the following, we will discuss\nartiﬁcial neural networks architectures that arise from the numerical discretisation of time\nand parameter dependent diﬀerential equations. Diﬀerential equations have a long history\nin the mathematical treatment of neural networks. Initially, neural networks were motivated\nby biological neurons in the brain. Mathematical models for their interactions are based on\nnonlinear, time-dependent diﬀerential equations. These have inspired some famous artiﬁcial\nneural networks such as the Hopﬁeld networks [63].\nOn a time interval [0, T], the values of the approximation to the solution of the diﬀerential\nequation at diﬀerent discrete times 0 = t0 < t1 < · · · < tK = T, e.g. tk = k h and h = T/K,\ncorresponding to the diﬀerent layers of the network architecture. For a ﬁxed ﬁnal time T,\nthe existence of an underlying continuous model guarantees the existence of a continuous\nlimit as the number of layers goes to inﬁnity and h goes to zero.\nIn the spirit of geometric numerical integration [57], we discuss structural properties of\nthe ANN as arising from the structure preserving discretisation of a diﬀerential equation\nwith appropriate qualitative features such as having an underlying symmetry, an energy\nfunction or a Lyapunov function, preserving a volume form or a symplectic structure.\nContrary to the ’classical’ design principle for layers of aﬃne transformations followed\nby activation functions (cf. (3)), so-called residual layers are a variation to this principle,\nwhich has risen to become a standard design concept of neural networks. Here, the output\nof one such layer is again added to its input, which again deﬁnes a network, the ResNet [60],\nΨ : X × Θ →X, Ψ(x, θ) = zK that is now given by the iteration\nz0 = x\nzk+1 = zk + σ(Akzk + bk),\nk = 0, . . . , K −1,\n(5)\nif X = Y. If on the other hand the output space Y diﬀers from X, it is common to add\nanother layer η : X →Y on top, which deﬁnes a network ˆΨ := η ◦Ψ : X →Y. This is for\nexample a common scenario in classiﬁcation, where the dimensionality of Y is determined\nby the number of classes.\nIt is easy to see that (5) corresponds to a particular discretisation of an ODE. To make\nthe connection more precise, denote by zk := z(tk), Ak := A(tk), bk := b(tk) samples of three\n4\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\nk = 0\nk = 15\nk = 30\nk = 40\nhalfmoon2d\nk = 0\nk = 15\nk = 30\nk = 40\ndonut2d\nk = 0\nk = 15\nk = 30\nk = 40\ndonut3d\nFigure 1. Evolutions of the ResNet model (6) for three diﬀerent data sets.\nThe upper two rows show evolutions in 2d and the lower row in 3d. The\nlink function σ is the hyperbolic tangent and the data ﬁt and regulariser\nare the squared 2-norm, Ln(z) = 1\n2∥z −yn∥2\n2, R(θ) = λ\n2 ∥θ∥2\n2.\nfunctions z : [0, T] →RM, A : [0, T] →RM×M and b : [0, T] →RM. With these notations\nwe can write the ResNet (5) as Ψ(x, θ) = z(T) with\nz(tk+1) = z(tk) + hσ(A(tk)z(tk) + b(tk)),\nk = 0, . . . , K −1,\nz(0) = x\n(6)\nif T = K and thus h = 1. For general T, it can be readily seen that (6) corresponds to the\nforward Euler discretisation of\n˙z(t) = f(z(t), θ(t)),\nt ∈[0, T],\nz(0) = x\n(7)\nwith θ := (A, b) : [0, T] →RM×M × RM ∼= RM 2+M,\nf(z(t), θ(t)) = σ(A(t)z(t) + b(t)).\n(8)\nThus, there is a natural connection of the discrete deep learning problem (4)+(5) with the\noptimal control formulation (4)+(7). The dynamics of the ResNet (5) as a discretisation of\n(7) is depicted in Figure 1.\nFrom here on we will suppress the dependence on t whenever it is clear from the context.\nIt is a well-known fact that the optimal control formulation can be phrased as a closed dy-\nnamical system by using Pontryagin’s principle and this results in a constrained Hamiltonian\nboundary value problem [12], the Hamiltonian of this system is given as\nH(z, p, θ) = ⟨p, f(z, θ)⟩\n(9)\nSTRUCTURE PRESERVING DEEP LEARNING\n5\nwhere p ∈T ∗X ≡RM is a vector of Lagrange multipliers. The dynamical system is then\ngiven by\n˙z = ∂pH,\n˙p = −∂zH,\n(10)\nsubject to the constraint\n0 = ∂θH.\n(11)\nThe adjoint equation for p can be expressed as\n˙p = ∂zf T p,\np(T) = 1\nN\nN\nX\nn=1\n∂zL′\nn(z(T)).\n(12)\nIn what follows we will review some of the guiding principles of structure preserving\ndeep learning, and in particular recent contributions for new neural networks architectures\nas discretisations for ODEs and PDEs in Section 2 and the interpretation of the training\nof neural networks as an optimal control problem in Section 3, invertible neural networks\nin Section 4, equivariant neural networks in Section 5, and structure-preserving numerical\nschemes for the training of neural networks in Section 6.\n2. Neural networks inspired by differential equations\n2.1. Structure preserving ODE formulations. In this section, we look at how the ODE\nformulation (7) can be restricted or extended in order to ensure that its ﬂow has favourable\nproperties. We are zooming in on the forward problem itself, assuming that the parameters\nθ are ﬁxed. By abuse of notaton, we will in this section write f(t, z) for f(z, θ(t)) in that\nwe consider θ(t) a known function of t. It is perhaps not so obvious what the desirable\nproperties of the ﬂow should be, and to some extent we here lean on prior work by Haber\nand Ruthotto [55] as well as Chang et al. [22]. It seems desirable that small perturbations\nin the data should not lead to large diﬀerences in the end result. Preferably, the forward\nmodel should have good stability properties, for instance in the sense that for any two nearby\nsolutions z1(t) and z2(t) to (7)\n∥z2(T) −z1(T)∥≤C∥z2(0) −z1(0)∥\n(13)\nfor a moderately sized constant C.\nIt is well-known that this type of estimate can be\nobtained in several diﬀerent ways, depending on the properties of the underlying vector\nﬁeld in (7). If f(t, z) is Lipschitz in its second argument with constant L, then (13) holds\nwith C = eT L. Looking at (8), one can use L = Lσ max\nt ∥A(t)∥where Lσ is a Lipschitz\nconstant for the activation function σ.\nStability can also be studied in terms of Lyapunov functions, that is, functions V (z) that\nare non-increasing along solution trajectories. Functions that are constant along solutions\nare called ﬁrst integrals, and a particular instance is the energy function of autonomous\nHamiltonian systems.\nFor stability of nonlinear ODEs one may for instance consider growth and contractivity in\nthe L2-norm using a one-sided Lipschitz condition. This is similar to the analysis proposed\nin [127]. We assume that there exists a constant ν ∈R such that for all admissible z1, z2,\nand t ∈[0, T] we have\n⟨f(t, z2) −f(t, z1), z2 −z1⟩≤ν∥z2 −z1∥2.\n(14)\n6\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\nIn this case it is easily seen that for any two solutions z1(t), z2(t) to (7) one has\n∥z2(t) −z1(t)∥≤∥z2(0) −z1(0)∥eνt,\nso that the problem is contractive if ν ≤0 [59]. For instance, the vector ﬁeld (8) satisﬁes\n(14) for σ absolutely continuous if\nν ≤sup\nt,D\nλmax\n\u0012DA(t) + (DA(t))T\n2\n\u0013\n,\nwhere the supremum is taken over all diagonal matrices D with diagonal entries in σ′(R).\nSome care should be taken here: It is not so that the sign of the eigenvalues of 1\n2(DA(t) +\n(DA(t))T ) is invariant under the set of all positive diagonal matrices D. In particular, even\nif A(t) is skew-symmetric, the vector ﬁeld (8) may still have a positive one sided Lipschitz\nconstant ν, but if we assume for instance that σ′(s) ∈[0, 1], it holds that ν ≤1\n2 max\nt\n∥A(t)∥∞.\nHaber and Ruthotto [55] suggest to use the eigenvalues of the linearised ODE vector ﬁeld\nto analyse the stability behaviour of the forward problem. If the eigenvalues of the resulting\nJacobian matrix have only non-positive real parts, it may be an indication of stability, yet\nfor non-autonomous systems such an analysis may lead to wrong conclusions.\nClearly, the set of all vector ﬁelds of the form (8) include both stable and unstable cases.\nThere are diﬀerent ways of ensuring that a vector ﬁeld has stable or contractive trajectories.\nOne could be to put restrictions on the family of matrices A(t), another option is to alter\nthe form of (8) either by adding symmetry to it or by embedding it into a larger space where\nit can be given desirable geometric properties, e.g. by doubling the dimension, it is possible\nin several diﬀerent ways to obtain a non-autonomous Hamiltonian vector ﬁeld. In [22] and\n[55] several models are suggested, and we mention a few of them in Section 2.1.2.\n2.1.1. Dissipative models and gradient ﬂows. For diﬀerent reasons it is desirable to use\nvector ﬁelds with good stability properties. This will ensure that data which are close to\neach other in a chosen norm initially remain close as the features propagate through the\nneural network. A model suggested in [55] was to consider weight matrices A(t) of the form\nA(t) = S(t) −S(t)T −γI\nwhere S(t) is arbitrary and γ is a small dissipation parameter. Flows of vector ﬁelds of\nthe form ˙z = A(t)z + b(t) exactly preserve the L2-norm of the ﬂow when A(t) is skew-\nsymmetric. The non-linearity will generally alter this behaviour, but adding a small amount\nof dissipation can improve the stability. It is however not guaranteed to reduce the one-sided\nLipschitz condition.\nZhang and Schaeﬀer [127] analyse the stability of a model where the activation function σ\nis always chosen to be the Rectiﬁed Linear Unit (ReLU) function. The form of the discretised\nmodel is such that in the limit when h tends to zero it must be replaced by a diﬀerential\ninclusion rather than an ODE of the form discussed above, meaning that ˙z −f(t, z) belongs\nto some speciﬁed subdomain of RM and their model vector ﬁeld is\nf(t, z) = −A2(t)σ(A1(t)z(t) + b1(t)) + b2(t),\n(15)\nGrowth and stability estimates are derived for this class of vector ﬁelds as well as for cases\nwhere restrictions are imposed on the parameters, such as A2(t) having positive elements or\nthe case A1(t) = A2(t)T =: A(t) and b2(t) = 0. For this last case, we consider for simplicity\nthe ODE\n˙z = −A(t)T σ(A(t)z + b(t)) = f(t, z),\n(16)\nSTRUCTURE PRESERVING DEEP LEARNING\n7\nwhich is a gradient system in the sense that ˙z = −∇zV with V = γ(A(t)z + b(t))1 where\nγ′ = σ and 1 is the vector of ones.\nTheorem 2.1.\n(1) Let V (t, z) be twice diﬀerentiable and convex in the second argument.\nThen the\ngradient vector ﬁeld f(t, z) = −∇V (t, z) satisﬁes a one-sided Lipschitz condition\n(14) with ν ≤0.\n(2) Suppose that σ(s) is absolutely continuous and 0 ≤σ′(s) ≤1 a.e. in R. Then (16)\nsatisﬁes the one-sided Lipschitz condition (14) for any choice of parameters A(t)\nand b(t) with\n−µ2\n∗≤νσ ≤0\nwhere µ∗= min\nt\nµ(t) and where µ(t) is the smallest singular value of A(t).\nIn\nparticular νσ = −µ2\n∗is obtained when σ(s) = s.\nProof. (1) We compute\n⟨f(t, z2) −f(t, z1), z2 −z1⟩= −⟨∇zV (t, z2) −∇zV (t, z1), z2 −z1⟩\nand deﬁne\nφ(ξ) = d\ndξ V (t, ξz2 + (1 −ξ)z1)\nsuch that\n⟨∇zV (t, z2) −∇zV (t, z1), z2 −z1⟩= φ(1) −φ(0) =\nZ 1\n0\nφ′(ξ) dξ.\nTherefore, by the convexity of V (t, z)\n⟨f(t, z2) −f(t, z1), z2 −z1⟩= −⟨\nZ 1\n0\n∇2\nzV (t, ξz2 + (1 −ξ)z1) dξ (z2 −z1), z2 −z1⟩≤0\n(2) Let z1 and z2 be vectors in RM. Using (16) while suppressing the t-dependence in the\nparameters, we ﬁnd\n⟨f(t, z2) −f(t, z1), z2 −z1⟩= −⟨σ(Az2 + b) −σ(Az1 + b), A(z2 −z1)⟩\n(17)\nFor real scalars ζ, η and β we have\n(σ(ζ + β) −σ(η + β))(ζ −η) =\nZ ζ\nη\nσ′(s + β) ds (ζ −η)\nand since 0 ≤σ′(s + β) ≤1 a.e. we have\n0 ≤(σ(ζ + β) −σ(η + β))(ζ −η) ≤(ζ −η)2.\nUsing this inequality in (17) we obtain\n−∥Az2 −Az1∥2 ≤⟨f(t, z2) −f(t, z1), z2 −z1⟩≤0\nSince ∥A(z2 −z1)∥2 ≥µ2\n∗∥z2 −z1∥2 the result follows. □\nRemark. In Theorem 2.1 we restricted the class of activation functions to be absolutely\ncontinuous with 0 ≤σ′(s) ≤1. This is true for many of the activation functions proposed\nin the literature, in particular for the ReLU function and the sigmoid σ(s) = tanh s.\n8\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n2.1.2. Hamiltonian vector ﬁelds. One may take inspiration from mechanical systems and\nintroduce the Hamiltonian framework. Separable Hamiltonian systems are deﬁned in terms\nof kinetic and potential energy functions T and V\nH(t, z, p) = T(t, p) + V (t, z)\nThe leads to diﬀerential equations of the form\n˙z = ∂pH = ∂pT\n(18)\n˙p = −∂zH = −∂zV\n(19)\nThere are diﬀerent ways to construct a Hamiltonian system from (8). In [22] the following\nmodel is suggested\n˙z = A1(t)T σ1(A1(t)p + b1(t))\n˙p = −A2(t)T σ2(A2(t)z + b2(t))\nLet γi : R →R be such that γ′\ni(t) = σi(t), i = 1, 2. The corresponding Hamiltonian is\nT(t, p) = γ1(A1(t)p + b1(t))1,\nV (t, z) = γ2(A2(t)z + b2(t))1\nwhere 1 = (1, . . . , 1)T . A simple case is obtained by choosing σ1(s) := s, A1(t) ≡I, b1(t) ≡0\nand σ2(s) := σ(s) which after eliminating p yields the second order ODE\n¨z = −∂zV = −A2(t)T σ(A2(t)z + b2(t))\nA second example considered in [22] is obtained by setting σ1 = σ2 = σ.\nFrom the outset, it might not be so clear which geometric features such a non-autonomous\nHamiltonian system has. There seem to be at least two ways to understand this problem\n[90]. Let us assume that u = (z, p) ∈T ∗RM ≡RM ⊕RM with “positions” z and “momenta”\np forming the phase space. We have the natural symplectic form on the phase space ω0 =\ndp ∧dz. This form can be represented by the Darboux-matrix J as\nω0(ξ, η) = ⟨ξ, Jη⟩,\nJ =\n\u0014\n0\nI\n−I\n0\n\u0015\nand the Hamiltonian vector ﬁeld f(t, z, p) is deﬁned via dH(·) = ω0(f(t, z, p), ·).\nLet us now introduce as phase space T ∗RM × R by including the time t as a dependent\nvariable. The natural projection is τ : (z, p, t) 7→(z, p). The space T ∗RM × R can then be\nfurnished with a contact structure deﬁned as\nωH = τ ∗ω0 −dH ∧dt.\nThe ﬁrst term is just the original form, ignoring the t-component of the tangents, the second\nform is only concerned with the “t-direction”. One can write the matrix of ωH by adding a\nrow and a column to J coming from the second term −dH ∧dt\nJH =\n \nJ\n−∇uH\n(∇uH)T\n0\n!\nThe resulting vector ﬁeld is then fH = (f T , ft)T and can be expressed through the equations\nifHωH = 0,\nifHdt = 1\nSTRUCTURE PRESERVING DEEP LEARNING\n9\nwhere ifH stands for the interior derivative of the form by fH, e.g. ifH applied to the two-\nform ωH is the one-form α = ifHωH such that α(η) = ω(fH, η) for all vector ﬁelds η. The\nform ωH is preserved along the ﬂow, but H is not.\nExtended phase space. One can in fact recover an autonomous Hamiltonian problem from\nthe non-autonomous one by adding two extra dependent variables, say t and pt. We do this\nby considering the manifold T ∗(RM × R) which can be identiﬁed with T ∗RM+1. One needs\na new projection µ : (z, p, t, pt) 7→(z, p, t) and we can deﬁne an extended (autonomous)\nHamiltonian on T ∗(RM × R) as\nK = H ◦µ + pt\nwith corresponding two-form\nΩ0 = µ∗τ ∗ω0 + dpt ∧dt\nThe corresponding matrix, JE, is just the original Darboux-matrix J where each of the\nn × n identity matrices has been replaced by corresponding (M + 1) × (M + 1) identity\nmatrices. The extended Hamiltonian K is exactly preserved along the ﬂow so that the new\nconjugate momentum variable pt will keep track of how H(z, p, t) is varying along solutions.\nThe resulting ODE vector feld fE can be written as dK(·) = Ω0(fE, ·) and in coordinates\nthe ODE system becomes\n˙z = ∂pH,\n˙p = −∂zH,\n˙t = 1,\n˙pt = −∂tH.\n(20)\nWe see at once that since the equations for ˙z, ˙p do not depend on pt and since the solution\nfor t is explicitly given, we solve the same problem as before. After solving for z and p,\nwe obtain pt independently by integration. The second thing one may observe is that if a\nnumerical method φh has the property that1\nφh ◦µ = µ ◦φh\nthen what we obtain by just considering the ﬁrst 2M components of the numerical solution\nto the extended system is precisely the same as what we would have obtained applying the\nsame method to the original non-autonomous problem. This observation was used by Asorey\net al. [7] to deﬁne what is meant by canonical transformations in the non-autonomous case,\nand we refer to this paper for further results on the structural connections between the\ntwo systems. In applications to deep learning, one should note that geometric properties\nof the solution can mostly be deduced from the extended system rather than the original\nnon-autonomous one, there are numerical methods which preserve energy or the symplectic\nform Ω0 and rigorous results can be proved for the long time behaviour of such integrators\n[57].\n2.2. Structure preserving numerical methods for the ODE model. The rationale\nbehind proposing ODE formulations with geometric structure is to enforce a controlled\nbehaviour of the solution as it is propagated through the hidden layers of the network. It is\ntherefore also important that when the ODE is approximated by a numerical method, this\nbehaviour should be retained by the numerical scheme.\n1It is common to assume that a given numerical integrator is deﬁned on systems in any dimension\n10\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n2.2.1. Numerical methods preserving dissipativity. When solving numerically ODEs which\nsatisfy the one-sided Lipschitz condition (14) a desirable property of the numerical scheme\nwould be that it contracts two nearby numerical solutions whenever ν ≤0. That is, it\nshould satisfy\n∥zk+1\n1\n−zk+1\n2\n∥≤∥zk\n1 −zk\n2∥,\n(21)\nin each time step k, preferably without too severe restrictions on the time step h. Methods\nwhich can achieve this for any step size exist, and are called B-stable. There are many\nB-stable methods, and for Runge–Kutta methods B-stability can be easily checked by the\ncondition of algebraic stability. Examples of B-stable methods are the implicit Euler method,\nand all implicit Runge–Kutta methods within the classes, Gauss, Radau IA, Radau IIA and\nLobatto IIIC [59]. In deep learning algorithms it has been more usual to consider explicit\nschemes since they are much cheaper per step than implicit ones, but are subject to restric-\ntions on the time step used. Note for instance that the explicit Euler method is unstable\nfor any step size when applied even to linear constant coeﬃcient systems where there are\neigenvalues of the coeﬃcient matrix on the imaginary axis. To consider contractivity of\nexplicit schemes, we need to replace (14) by a diﬀerent monotonicity condition\n⟨f(t, z2) −f(t, z2), z2 −z1⟩≤¯ν∥f(t, z2) −f(t, z1)∥2\nwhere we assume ¯ν < 0. For every Runge–Kutta method with strictly positive weights bi\nthere is a constant r such that the numerical solution is contractive whenever the step size\nsatisﬁes\nh < −2¯νr\nThe value of r can be calculated for every Runge–Kutta method with positive weights, and\ne.g. for the explicit Euler method as well as for the classical 4th order method of Kutta one\nhas r = 1 [36].\n2.2.2. Numerical methods preserving energy or symplecticity. For autonomous Hamiltonian\nsystems there are two important geometric properties which are conserved. One is the energy\nor Hamiltonian H(z, p) the other one is the symplectic structure, a closed non-degenerate\ndiﬀerential two-form. These two properties are also the main targets for structure preserving\nnumerical methods.\nAll Hamiltonian deep learning models presented here can be extended to a separable,\nautonomous canonical system, i.e. a system of the form (18)-(19). Such systems preserve\nthe symplectic two-form dp∧dq and there are many examples of explicit numerical methods\nthat also preserve this same form in the sense that dpk+1 ∧dqk+1 = dpk ∧dqk. The simplest\nexample of such a scheme is the symplectic Euler method, deﬁned for the variables z and p\nas\nzk+1 = zk + h ∂pT(tk+1, pk),\npk+1 = pk −h ∂zV (tk+1, zk+1)\n(22)\nThe symplectic Euler method is explicit for separable Hamiltonian systems and is an ex-\nample of a splitting method [93]. Many other examples and a comprehensive treatment of\nsymplectic integrators can be found in [57]. When applying symplectic integrators to Hamil-\ntonian problems one has the important notion of backward error analysis. The numerical\napproximations obtained can be interpreted as the exact ﬂow of a perturbed system with\nHamiltonian ˜H(z, p) = H(z, p) + hH2(z, p) + · · · 2. This partly explains the popularity of\n2This is a divergent asymptotic series, but truncation is possible at the expense of an exponentially small\nremainder term\nSTRUCTURE PRESERVING DEEP LEARNING\n11\nsymplectic integrators, since many of the characteristics of Hamiltonian ﬂows are inherited\nby the numerical integrator.\nThere exist many numerical methods which preserve energy exactly for autonomous prob-\nlems, for instance there is a large class of schemes based on discrete gradients [94]. A discrete\ngradient of a function H(z) is a continuous map ∇H : RM × RM →RM which satisﬁes the\nfollowing two conditions\nH(z2) −H(z1) = ⟨∇H(z1, z2), z2 −z1⟩,\n∇H(z, z) = ∇H(z),\n∀z, z1, z2 ∈RM\nFor a Hamiltonian problem, ˙z = J∇H(z) it is easily seen that the method deﬁned as\nzk+1 −zk\nh\n= J∇H(zk, zk+1)\nwill be energy preserving in the sense that H(zk) = H(z0) for all k > 0. There are many\nchoices of discrete gradients, but most of them lead to implicit schemes and therefore have\nthe disadvantage of being computationally expensive.\nAnother disadvantage is that even if it makes sense to impose energy conservation for\nthe extended autonomised system explained above for deep learning models, it is not clear\nwhat that would mean for the original problem. It remains an open problem to understand\nthe potential for and the beneﬁts of using energy preserving schemes for non-autonomous\nHamiltonian systems in deep learning.\n2.2.3. Splitting methods and shears. Splitting methods are very popular time-integration\nmethods that can be easily applied to preserve geometric structure of the underlying ODE\nproblems, e.g. symplecticity. The idea of splitting and composition is simply to split the\nvector ﬁeld in the sum of two (or more) vector ﬁelds, to integrate separately each of the\nparts and compose the corresponding ﬂows. For example splitting a Hamiltonian vector\nﬁeld in the sum of two Hamiltonian vector ﬁelds and composing their ﬂows results into a\nsymplectic integrator. If the individual parts are easy to integrate exactly, the resulting\ntime-integration method has often low computational cost. We refer to [93] for an overview\non splitting methods. An ODE on Rd is a shear if there exist a basis of Rd in which the\nODE takes the form\n˙yi = 0,\ni = 1, . . . , k,\n˙yi = fi(y1, . . . , yk),\ni = k + 1, . . . , d.\nA diﬀeomorphism on Rd is called a shear if it is the ﬂow of a shear. Splitting vector ﬁelds\ninto the sum of shears allows to compute their individual ﬂows exactly simply applying the\nforward Euler method.\nConsider the shears Rn × Rn →Rn × Rn:\n(z, p) 7→(z + g(p), p),\n(z, p) 7→(z, p + f(z)).\nIn the case of autonomous, separable, Hamiltonian systems, the symplectic Euler method\n(22) can be seen as the composition of two such maps where\ng(p) := h∂pT(p),\nf(z) := −h∂zV (z).\nAnother popular example is the St¨ormer-Verlet integrator or leapfrog integrator which is\nalso the composition of two shears. It is possible to represent quite complicated functions\n12\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\nwith just two shears. The “standard map” (also known as ChirikovTaylor map) is an area\npreserving, chaotic shear map much-studied in dynamics and accelerator physics.\nShears are useful tools to construct numerical time-integration methods that preserve\ngeometric features. As already mentioned symplecticity is one such property (if f and g are\ngradients), another is the preservation of volume and, as we will see in section 4, shears can\nbe used to obtain invertible neural networks.\n2.3. Features evolving on Lie groups or homogeneous manifolds. If the data belongs\nnaturally to a diﬀerentiable manifold M of ﬁnite dimension and f(z, θ) in (7) is a vector\nﬁeld on M for all θ ∈Θ then z(t) ∈M. Concrete examples of data sets in this category are\nmanifold valued images and signals. One example is diﬀusion tensor imaging consisting of\ntensor data which at each point (i, j) in space corresponds to a 3×3 matrix Ai,j ∈Sym+(3)\nsymmetric and positive deﬁnite, [34]. If m × l is the number of voxels in the image then\nM = Sym+(3)m×l. Another example is InSAR imaging data taking values on the unit circle\nS1, and where M =\n\u0000S1\u0001m×l, [107]. In both these examples neural networks, e.g. denoising\nautoencoders [116], can be used to learn image denoising. The loss function (4) can take\nthe form\nL(Ψ(x, θ), y) =\nl,m\nX\ni,j=1\nd(Ψ(x, θ)i,j, yi,j)\nwhere x = (x1,1, . . . , xl,m) ∈M is the noisy image y = (y1,1, . . . , yl,m) ∈M is the source\nimage and d is a distance function on Sym+(3) or S1 respectively. For example in the case\nof Sym+(3) with TASym+(3) = Sym(3), a possible choice of Riemannian metric is\n⟨X, Y ⟩TAM := trace(A−1\n2 XA−1Y A−1\n2 )\nwhere X and Y are symmetric 3 × 3 and A is symmetric and positive deﬁnite, and with\ntrace denoting the trace of matrices. With this metric Sym+(3) is a Riemannian symmetric\nspace and is geodesically complete [119].\nA third example concerns classiﬁcation tasks where the data are Lie group valued curves\nfor activity recognition, here M = Gm with m the number of points where the curve is\nsampled and G = SO(3) is the group of rotations, [79]. A loss function can be built using\nfor example the following distance between two sampled curves c1 = (c1,1, . . . , c1,m) ∈Gm,\nc2 = (c2,1, . . . , c2,m) ∈Gm,\nd(c1, c2) =\nm−1\nX\ni=1\n\r\r\r\r\r\r\nlog(c1,i+1c−1\n1,i )\n∥log(c1,i+1c−1\n1,i )∥\n1\n2g\n−\nlog(c2,i+1c−1\n2,i )\n∥log(c2,i+1c−1\n2,i )∥\n1\n2g\n\r\r\r\r\r\r\ng\n,\nwhere g is the Lie algebra of G, ∥· ∥g is a norm deduced from an inner product on g, and\nlog : G →g denotes the matrix logarithm. An important feature of this distance is that it\nis re-parametrisation invariant and taking the inﬁmum over all (discrete) parametrisations\nof the second curve one obtains a well deﬁned distance for curves independent on their\nparametrisation, see [18] for details.\nThe ODE (7) should in this setting be adapted to be an ODE on M. If M is a d −p\nsubmanifold of Rd, M := {y | g(y) = 0}, g : Rp →Rd then the ODE (7) can be modiﬁed\nadding constraints, alternatively one could consider intrinsic manifold formulations which\nallow to represent the data with d −p degrees of freedom instead of d.\nThe numerical\nintegration of this ODE must then respect the manifold structure.\nSTRUCTURE PRESERVING DEEP LEARNING\n13\n2.4. Open problems.\n2.4.1. Geometric properties of Hamiltonian models. Autonomous Hamiltonian systems and\ntheir numerical solution are by now very well understood. For such models, one has con-\nservation of energy that is attractive when considering stability of the neural network map.\nThe same cannot, to our knowledge, be said about the non-autonomous case. One can ap-\nproach this problem in diﬀerent ways. One is to consider canonicity in the sense of [7] and\nstudy the geometric properties of canonical transformations via the extended autonomous\nHamiltonian system. This is however not so straightforward for a number of reasons. One\nissue is that every level set of the extended Hamiltonian will be non-compact. Another issue\nis that the added conjugate momentum variable, denoted pt above, is artiﬁcial and is only\nused to balance the time varying energy function.\nOne should also consider the eﬀect of regularisation, since one may expect that smoothing\nof the parameters will cause the system to behave more similarly to the autonomous problem.\nSee subsection 3.2 of this paper.\n2.4.2. Measure preserving models. The most prevalent example of measure to preserve is the\nphase space volume. In invertible networks some attention is given to volume preserving\nmaps, see [105, 38] and also section 4 of this paper. For the ODE model this amounts\nto the vector ﬁeld f(t, z) being divergence free, and there are several ways to parametrise\nsuch vector ﬁelds. All Hamiltonian vector ﬁelds are volume preserving, but the converse\nis not true. Volume preserving integrators can be obtained, for example, via splitting and\ncomposition methods [93].\n2.4.3. Manifold based models. A generalisation of most of the approaches presented in this\nsection to the manifold setting is still missing. For data evolving on manifolds the ODE\n(7) should be adapted to be an ODE on M. Just as an example, a gradient ﬂow on M\nanalogous to (16) can be considered starting from deﬁning a function V : M × Θ →R\nusing the antiderivative of the activation function σ, and the Riemannian metric.\nThe\nHamiltonian formulations of section 2.1.2 could be also generalised to manifolds in a similar\nway, starting from the deﬁnition of the Hamiltonian function.\nAn appropriate numerical time-discretisation of the ODEs for deep learning algorithms\nmust guarantee that also the evolution through the layers remains on the manifold so that\none can make use of the Riemannian structure of M and obtain improved convergence of\nthe gradient descent optimisation, see also section 6.2. The numerical time discretisation\nof this ODE must then respect the manifold structure and there is a vast literature on this\nsubject, see for example [57, Ch. IV]. For numerical integration methods on Lie groups and\nhomogeneous manifolds including symplectic and energy-preserving integrators see [67, 21].\n3. Deep Learning meets Optimal Control\nAs outlined in the introduction supervised deep learning with the ResNet architecture\ncan be seen as solving a discretised optimal control problem. This observation has been\nmade in [55, 44, 28, 87] with further extensions to PDEs are described in [108].\nRecall (4)\nmin\nθ∈Θ\n(\nE(θ) = 1\nN\nN\nX\nn=1\nLn(Ψ(xn, θ)) + R(θ)\n)\n(23)\n14\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\nwhere the neural network Ψ(·, θ) : X →X is either deﬁned by a recursion like (5) or the\nsolution at the ﬁnal time of an ODE (7). Another approach is to view the training as an\noptimisation problem over Θ × X N where X is the space of the dynamics z, i.e.\nmin\n(θ,z)∈Θ×X N\n(\nE(θ, z) = 1\nN\nN\nX\nn=1\nLn(zn(T)) + R(θ)\n)\n(24a)\nsuch that\n˙zn = f(zn, θ(t)),\nzn(0) = xn,\nn = 1, . . . , N.\n(24b)\nIn machine learning the reduced formulation (23) is much more common than (24).\n3.1. Training algorithms. The discrete or continuous optimal control problem can be\nsolved in multiple ways, some of which we will discuss next.\n3.1.1. Derivative-based algorithms. The advantage of the reduced problem formulation (23)\nis that it is a smooth and unconstrained optimisation problem such that if Θ is a Hilbert\nspace, derivative-based algorithms such as ”gradient” descent are applicable. Due to the\nnonlinearity of Ψ we can at most expect to converge to a critical point E′(θ) = 0 with a\nderivative-based algorithm. The most basic algorithm to train neural networks is stochastic\ngradient descent (SGD) [106]. Given an initial estimate θ0, SGD consists of iterating two\nsimple steps. First, sample a set of data points Sj ⊂{1, . . . , N} and then iterate\nθj+1 = θj −τ j 1\n|Sj|\nX\nn∈Sj\n(Ln ◦Ψ(xn, ·))′(θj).\n(25)\nOther ﬁrst-order algorithms used for deep learning are the popular Adam [73] but also\nthe Gauss–Newton method has been used [55]. A discussion on the convergence of SGD\nis out of the scope of this paper.\nInstead we focus on how to compute the derivatives\n(Ln ◦Ψ(xn, ·))′(θj) in the continuous model which is the central building block for all ﬁrst-\norder methods. This following theorem is very common and the main idea dates back to\nPontryagin [101]. This formulation is inspired by [16, Lemma 2.47].\nTheorem 3.1. Assume that f and Ln are of class C1 and that f is Lipschitz with respect\nto z. Let zn ∈W 1,∞([0, T], RM) be the solution of the ODE (24b) with initial condition xn\nand pn the solution of the adjoint equation\n˙pn = −∂zf(zn(t), θ(t))T pn,\npn(T) = L′\nn(zn(T)).\n(26)\nThen the Fr´echet derivative of A := Ln ◦Ψ(xn, ·) : L∞:= L∞([0, T], RM 2+M) →R at\nθ ∈L∞is the linear map B := A′(θ) : L∞→R,\nBh =\nZ T\n0\n⟨∂θf(zn(t), θ(t))T pn(t), h(t)⟩dt.\n(27)\nFor ﬁnite dimensional θ a similar theorem can be proven which dates back to Gr¨onwall\nin 1919, see [54, 58].\nDeﬁning neural networks via ODEs and computing gradients via\ncontinuous formulas similar to Theorem 3.1 has been ﬁrst proposed in [24] with extensions\nin [52] and [42]. In the deep learning community this is being referred to as Neural ODEs.\nSimilar to Theorem 3.1 a discrete version can be derived when the ODE (7) is discretised\nwith a Runge–Kutta method is given in [12], see also [56] for a related discussion about\nthis topic. For simplicity we just state the special case of the explicit Euler discretisation\n(ResNet (5)) here.\nSTRUCTURE PRESERVING DEEP LEARNING\n15\nTheorem 3.2 ([12]). Let zn be the solution of the ResNet (5) with initial condition xn. If\npn satisﬁes the recursion\npk+1\nn\n= pk\nn −h∂zf(zk\nn, θk)T pk+1\nn\n,\nk = 0, . . . , K −1,\npK\nn = L′\nn(zK\nn ),\n(28)\nthen the derivative of A := Ln ◦Ψ(xn, ·) is given by ∂θkA(θ) = h∂θf(zk\nn, θk)T pk+1\nn\n.\nSome readers will spot the similarity between Theorem 3.2 and what is called backprop-\nagation in the deep learning community. This observation was already made in the 1980’s,\ne.g. [80].\nIf all functions in (27) are discretised by constant functions on [tk, tk+1], then the gradient\nof the discrete problem (28) approximates the Fr´echet derivative of the continuous problem\n(27). In more detail, let ek,j ∈L∞be supported on [tk, tk+1] and ek,j(t) = ej ∈RM 2+M\nwith ej\ni = 1 if j = i and 0 else. If we denote by A = Ln ◦Ψ(xn, ·) the data ﬁt using the\nODE solution (7) and ˜A = Ln ◦Ψ(xn, ·) the data ﬁt with the ResNet (5), then\nA′(θ)ek,j =\nZ tk+1\ntk\n⟨∂θf(zn(t), θ(t))T pn(t), ek,j(t)⟩dt\n(29)\n≈h⟨∂θf(zn(tk), θ(tk))T pn(tk+1), ej⟩= ∂θk\nj ˜A(θ).\n(30)\nIn other words, in this case of piecewise constant functions, discretise-then-optimise is the\nsame as the optimise-then-discretise.\n3.1.2. Method of successive approximation. Another strategy to train neural networks has\nbeen recently proposed by [85] and is not based on the gradients of the reduced formulation\n(23) but on the necessary conditions of (24) also known as Pontryagin’s maximum principle\n[101] instead. Given an initial estimate of θ0, each iteration of the method of successive\napproximation(MSA) has three simple steps. First, solve (24b) with θj which we denote by\nzj\nn, i.e.\n˙zj\nn = f(zj\nn, θj(t)),\nzj\nn(0) = xn,\nn = 1, . . . , N.\n(31)\nThen solve the adjoint equation (26) with θj, zj\nn and denote the solution by pj\nn, i.e.\n˙pj\nn = −∂zf(zj\nn(t), θj(t))T pj\nn,\npj\nn(T) = L′\nn(zj\nn(T)),\nn = 1, . . . , N.\n(32)\nThe third and ﬁnal step is to maximise the Hamiltonian (9) given pj\nn, zj\nn, i.e. for any t ∈[0, T]\nthe update is deﬁned as\nθj+1(t) := arg max\nθ\n(\nH(zj\nn(t), pj\nn(t), θ) = 1\nN\nN\nX\nn=1\n⟨pj\nn(t), f(zj\nn(t), θ)⟩\n)\n.\n(33)\nNote that this algorithm is potentially well-deﬁned also in the non-smooth case, i.e. f is\nnot diﬀerentiable with respect to θ. If f is indeed smooth, R = 0 and θj+1 = θj, then\n1\nN\nN\nX\nn=1\n∂θf(zj\nn(t), θj(t))T pj\nn(t) = 0.\n(34)\nand the Fr´echet derivative of (23) vanishes. Analysis, extensions and numerical examples\nof the MSA are presented in [85].\n16\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n3.2. Regularisation. The training of a neural network (4) may include explicit regu-\nlarisation R and several diﬀerent regularisers have been proposed in the literature, e.g.\n[12, 112, 55, 96, 103], which we want to discuss in this section. Before we dive into the\nspeciﬁcs of these regularisers, we would like to answer the question if regularisation is nec-\nessary for training neural networks.\nExample 3.2.1. In order to shed some light on this, we consider the most trivial example\nwhich is taken from [112]. To this end we use ResNet (5) and let N = 1, K = 1, M = 1,\nL1(z) = (z −1)2, x1 = 0 and σ = tanh. When no regulariser is present, R = 0, the training\nproblem (4) simpliﬁes to\nmin\nA∈R,b∈R\n\b\nE(A, b) = (tanh(b) −1)2\t\n.\n(35)\nSince tanh(R) = (−1, 1), there are two possible problems here. First, (35) does not have\na solution, so the task of training does not really make sense. Second, even if we ignore\nthe ﬁrst problem and just apply a descent algorithm on (35), then we encounter another\nproblem: minimising sequences are not bounded. For example, let Aj := 0, bj := j, then\nlimj→∞E(Aj, bj) = 0 but (Aj, bj)j∈N is unbounded and does not even contain a convergent\nsubsequence. Thus, we cannot expect our training algorithm to converge.\nThe key problem in the previous example was that the range of the neural network Ψ(xn, ·)\nwas not closed. The non-closedness of the range is a characterisation of ill-posedness for\nlinear inverse problems in inﬁnite dimensions, see e.g. [29, Theorem 3.7]. While this may\nnever be the case for ﬁnite-dimensional linear inverse problems, the non-linearity in (35)\nresults in exactly this property.\nIn order to overcome this problem, we can either pose constraints on the data ﬁdelity\n(network architecture, link function etc) or we cure the ill-posedness by regularisation as is\nclassically done when considering ill-posed inverse problems, see e.g. [46, 68]. To overcome\nthe problem in (35) it is suﬃcient to add a regulariser R which is coercive, meaning that for\nany sequence of parameters (θj)j∈N it holds that\nlim\nj→∞∥θj∥= ∞\n⇒\nlim\nj→∞R(θj) = ∞.\n(36)\nThis condition implies that minimising sequences, which are sequences (θj)j∈N such that\nlimj→∞E(θj) = infθ∈Θ E(θ), are bounded. In reﬂexive Banach spaces, this it suﬃcient\nto guarantee at least a convergent subsequence. For more information on regularisation of\nnon-linear ill-posed inverse problems we refer to classical textbooks, e.g. [46, 68].\nThe remainder of this section is dedicated to discuss a couple of speciﬁc choices for\nthe regulariser R. In ﬁnite dimensions, due to the equivalence of all norms, any norm is\ncoercive. In addition, the regulariser may impose additional properties on the estimated\nparameters. By now it is standard to use the 1-norm ∥θ∥1 = P\ni |θi| or the squared 2-norm\n∥θ∥2\n2 = P\ni |θi|2 for regularisation in deep learning to promote solutions which are sparse or\nhave small coeﬃcients respectively, see e.g. [96, 103]. The interpretation of a deep neural\nnetwork as a process that changes with time naturally calls for other norms. For instance,\nthe next section relies on the squared H1-norm as a regularisation, i.e. for θ : [0, T] →RM\nit is deﬁned as\n∥θ∥2\nH1 = ∥θ(0)∥2 +\nZ T\n0\n∥∂tθ(t)∥2 dt.\n(37)\nSTRUCTURE PRESERVING DEEP LEARNING\n17\nThis regularisation and its discrete counterpart will promote solutions which are smoothly\nvarying across the layers and were used in [55, 112].\nFinally, the connection of deep neural networks to discretised ODEs motivate other non-\nstandard regularisers, too. To this end we consider the ResNet with time varying discreti-\nsation\nzk+1 = zk + hkσ(Akzk + bk)\n(38)\nand extend the parameters to θ = (Ak, bk, hk)K−1\nk=0 . Then it is natural to ensure that the\ntime steps h := (hk)K−1\nk=0 are nonnegative and sum to T, more precisely we regularise the\ntime steps with R : RK →R∞,\nR(h) =\n(\n0,\nif\nhk ≥0\nk = 0, . . . , K −1,\nPK−1\nk=0 hk = T\n∞,\nelse\n.\n(39)\nNote that R is nonsmooth and convex and since its proximal operator can be computed\neﬃciently [41] proximal algorithms can be eﬃciently employed. This regulariser has been\nused for deep learning in [12].\n3.3. Deep limits. Let θ(K) denote a minimiser of (4) with a ResNet (5) with K layers\nand by θ∞a minimiser of (4) with the ODE constraint (7). In what sense and under which\nconditions do discrete solutions θ(K) converge when the number of layers K tends to inﬁnity?\nIf these converge, do they converge to a solution of the optimal control problem θ∞?\nIn order to answer these questions one needs a topology in which we can compare θ(K) ∈\n(RM 2+M)K and θ∞: [0, T] →RM 2+M. With the discrete measure µ(K) =\n1\nK\nPK−1\nk=0 δkT/K\nwe make the identiﬁcation (RM 2+M)K ∼= L2(µ(K), [0, T], RM 2+M) =: L2(µ(K)) where by\nabusing notation, we associate the discrete object θ(K) with the piecewise constant function\nθ(K) : [0, T] →RM 2+M with θ(K)(t) = θ(K)\nk\nif t ∈[tk, tk+1).\nIt turns out that H1 :=\nH1([0, T], RM 2+M) is a suitable solution space for the optimal control problem, so that\nL2 := L2([0, T], RM 2+M) is a natural candidate for the convergence of θ(K) to θ∞since\nboth L2(µ(K)) and H1 can be embedded into L2.\nThe following theorem is a special case of Theorem 2.1 in [112] which answers the question\nof deep limits for the ResNet (6). Its proof relies on the equivalence of convergence in L2\nand a certain transport metric [51], see [112] for more details.\nTheorem 3.3. Let E(K) : L2(µ(K)) →R\nE(K)(θ) = 1\nN\nN\nX\nn=1\nLn(Ψ(xn, θ)) + λ\n \n∥θ(0)∥2 + K\nK−1\nX\nk=0\n∥θ(tk+1) −θ(tk)∥2\n!\n(40)\nwith Ψ being the discrete ResNet (6) and E∞: H1 →R\nE∞(θ) = 1\nN\nN\nX\nn=1\nLn(Ψ(xn, θ)) + λ\n \n∥θ(0)∥2 +\nZ T\n0\n∥∂tθ(t)∥2 dt\n!\n(41)\nwith Ψ given by the ODE (7). Let σ be Lipschitz continuous with σ(0) = 0 and Ln be\ncontinuous and nonnegative for all n = 1, . . . , N. If λ > 0, then\n(1) minimisers of E(K) and E∞exist for all K ∈N,\n(2) minimal values converge, i.e. limK→∞minL2(µ(K)) E(K) = minH1 E∞, and\n18\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n(3) any sequence of minimisers of E(K), {θ(K)}K∈N ⊂L2, is relatively compact, and\nany limit point of {θ(K)}K∈N is a minimiser of E∞.\n3.4. Open problems. Connecting deep learning to optimal control has opened up new\navenues to advance the ﬁeld of deep learning. In this section we discussed algorithms moti-\nvated by this connection which are based on derivatives and necessary optimality conditions.\nWe discussed the need and potential for variational regularisation of deep learning and un-\nderstanding the behaviour of deep neural networks as we increase the number of layers. All\nof these routes have natural extensions which will pave the way for better understanding of\ndeep learning and even more powerful tools.\n3.4.1. Algorithms with builtin errors. Using ODEs as a network architecture and computing\ngradients via the adjoint, i.e. Theorem 3.1, is theoretically appealing. However, practically\nboth the forward and the adjoint ODE have to be solved numerically which induces er-\nrors into the gradient. When using oﬀ-the-shelf ﬁrst-order methods like SGD then they\nassume that the gradients are computed exactly which may either hinder performance or\nrequire prohibitively accurate computations, see for instance discussion in [52]. That being\nsaid, state-of-the-art algorithms like SGD and Adam are stochastic and the update are not\nguaranteed to decrease the objective so the impact of discretisation errors is not clear. More-\nover, since the numerical solutions of the ODEs can be computed to any given tolerance,\nthis naturally poses the question how to use such knowledge and control over the accuracy\nin optimisation algorithms. Some algorithms have been extended to include such errors, see\nfor instance [33, Chapter 8] and [122].\n3.4.2. Algorithms without gradients. The MSA and its extended version have been proposed\nfor deep learning [85] but potentially more development is needed to fully exploit this direc-\ntion including stochastic updates with respect to the data points and eﬃcient maximisation\nof the Hamiltonian.\nThe MSA exploits the structure of deep learning only to the point of optimal control (24)\nbut is generic in terms of the architecture, e.g. the choice of f. For discretised system a\nmore tailored algorithm has been proposed in [110] but without any theoretical guarantees\non its convergence.\n3.4.3. Architectures, rates and topologies for deep limits. The question if the ResNet con-\nverges with increasing number of layers was satisfactorily answered in [111] but these results\nstill leave a number of questions unanswered. First, do other architectures also have deep\nlimits? The most likely candidates here are discretisations of ODEs. Second, are these\nresults tightly linked to H1-regularisaton or can they be extended to other topologies, e.g.\nthe one induced by the total variation? Third, are there convergence rates for these limits?\nAnother work on the convergence of discretised ODEs with ﬁner discretisations is [56].\nIn particular this work not only proves convergence but also convergence rates. It utilises\nassumptions on smoothness and coercivity of certain Hessians but it is not clear if these\nare met in a deep learning context. It would be interesting to verify or falsify the assump-\ntions for relevant learning problems. In case these are not met, a diﬀerent architecture or\nregularisation might be a way out.\nSTRUCTURE PRESERVING DEEP LEARNING\n19\n4. Invertible neural networks and normalising flows\nIn the previous chapters, we have viewed certain neural networks as discretisations of con-\ntinuous systems. In the following, we will return to the classical view of discrete networks,\nwhich are additionally endowed with a certain structure. Invertible neural networks, i.e.\nbijective neural networks, are such an example. They have been an emerging topic in deep\nlearning over the last few years. They are naturally connected to neural networks that are\ninspired by ordinary diﬀerential equations, as ﬂows of ODEs are themselves invertible. Two\nof the main applications of invertible neural networks lie in memory-eﬃcient backpropaga-\ntion as well as generative modeling with density estimation. For simplicity, in the following\nwe will use the abbreviation fk = f k(·, θk) when appropriate. Much like in general, invert-\nible networks are typically parametrised as a function composition Ψ = fK ◦· · ·◦f1, with the\nadditional constraint that each layer fk is a bijective function. The inverse can then simply\nbe computed via Ψ−1 = f1\n−1 ◦· · · ◦fK\n−1. A main restriction this imposes on the architec-\nture is the fact that for each layer f k : X k × Θk →X k+1, one has dim(X k) = dim(X k+1),\nwhere the X k are the feature spaces. As a consequence, the dimension of the input space\ndetermines the dimensions of the feature spaces in a (fully) invertible neural network.\n4.1. Types of invertible layers. Invertible networks and layers can roughly be divided\ninto two categories: Those that are algebraically invertible and those that are inverted with\na numerical approximation scheme.\n4.1.1. Coupling layers. Coupling layers [38] work by splitting a layer’s input into two parts\nand applying a suitable transformation that is easily invertible for one of the two parts.\nMathematically, for an M-dimensional input, the index set I = {1, . . . , M} is partitioned\ninto two sets. In convolutional networks, the partition is often done channel-wise, i.e. the\nchannels are split into two sets. A diﬀerent type of partitioning is invertible downsampling\n(also known as a masking or squeeze operation [38]). With x = (x1, x2) ∈RM1 × RM2 and\ny = (y1, y2) ∈RM1 × RM2, a coupling layer g : x 7→y is deﬁned via the mapping\ny1 = x1\ny2 = γ(x2, f(x1)),\n(42)\nwhere f : RM1 →RM2 and γ : RM2 × RM1 →RM2 is invertible in the ﬁrst argument. Then\ng−1 : y 7→x is given by\nx1 = y1\nx2 = γ−1(y2, f(y1)),\n(43)\nwhere γ−1 is the inverse of γ in its ﬁrst argument (for ﬁxed second argument). The coupling\nlaw γ can be as simple as γ : (a, b) 7→a + b, with which f is called an additive coupling\nlayer [38], such that γ−1 : (c, b) 7→c −b. Another commonly-used class of coupling layers\nare aﬃne coupling layers [38, 39]. More complex, but in theory more expressive coupling\nlaws can be constructed from strictly monotonic splines [43].\nNote that (42) and (43)\nare shear mappings (Section 2.2.3), which in the case of ODEs are used to construct e.g.\nsymplecticity-preserving numerical solutions.\nThere is in principle no restriction on the function f. This allows for the utilisation of\narbitrarily expressive sub-networks f (e.g. a sequence of convolutional layers with non-linear\nactivation functions), without rendering g (algebraically) non-invertible.\nThe numerical\nstability of this inversion may still pose an issue. Stability guarantees (both for the forward\nand the inverse mapping) can for instance be controlled via the Lipschitz constant of f and\n20\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\nγ, as shown in [10]. This mirrors stability considerations of ODEs (see Section 2.1), where\nguarantees can be formulated as conditions on the Lipschitz constants. Note that while f\nhas to map to RM2, the individual layers which comprise f may change the dimensionality\nthroughout this sub-network – only the ﬁnal layer has to transform the data to the required\nspace RM2. Since coupling layers perform learnable computations only on a part of the\ninput, the partitioning should change throughout the network, e.g. by switching the roles\nof x1 and x2.\n4.1.2. Invertible layers through iterative schemes. Another class of invertible layers are those\nthat are invertible with an iterative scheme. Invertible residual networks [9] are a special\ncase of the commonly-used residual networks [60], which allow for the inversion with a simple\nﬁxed-point iteration. A residual layer can be framed as a function g : RM →RM with\ng(x) = x + f(x),\n(44)\nwhere f : RM →RM is a sub-network. For ﬁxed x, let y := x+f(x), such that x = y−f(x).\nA ﬁxed point z∗of the function Φy : z 7→y −f(z) is thus in the preimage of y under g, i.e.\ng(z∗) = y. Note that\n∥Φy(a) −Φy(b)∥= ∥f(b) −f(a)∥≤Lip(f) · ∥b −a∥,\n(45)\nwhere Lip(f) is the Lipschitz-constant of g. According to Banach’s ﬁxed-point theorem, if\nLip(f) < 1, then Φy is guaranteed to have a unique ﬁxed point, which is the inverse of y\nunder g, i.e. z∗= g−1(y). This inverse can be approximated to arbitrary precision via the\niteration\nxi = y −f(xi−1)\n(46)\nfor any initial value x0. While most common layer types (such as dense or convolutional lay-\ners equipped with activation functions with bounded derivative) are Lipschitz, the condition\nLip(f) < 1 is not necessarily met. In [9], the required Lipschitz constraint is enforced by\nspectral normalisation: Let the linear layer Aθ depend linearly on its parameters θ (e.g. con-\nvolutional layers without non-linear activation functions and biases; these are linear layers\nthat linearly depend on their kernel). Then A˜θ parametrised by\n¯θ := c · θ/∥Aθ∥2\n(47)\nhas Lipschitz constant Lip(A¯θ) = c.\nAs a consequence, the layer f(x) = σ(A¯θ · x + b)\nhas Lipschitz constant Lip(f) ≤c for any activation function σ with Lip(σ) ≤1, where\nb is a bias vector.\nThus, by updating the layers’ parameters via spectral normalisation\naccording to (47) after each gradient step, the required Lipschitz constraint for invertibility\nis guaranteed, if one chooses c < 1. In practice, the spectral norm ∥Aθ∥2 is calculated with\nthe power method. To save computations, the authors in [9] only perform a single power\niteration, but re-use the estimation of the leading singular vector from the previous training\nstep as the initial guess.\nWhile the power method (with a ﬁnite number of iterations)\ntechnically only provides a lower bound to ∥Aθ∥2, the authors in [9] ﬁnd the Lipschitz\nconstraint to still be met in practice.\nIn the case of invertible residual networks, the connection to neural networks as numerical\nsolutions to ODEs is particularly strong. As noted in Section 1.2, ResNets can generally\nbe viewed as Euler discretisations of an ODE, if one views the activations, weights and\nbiases as observations of time-dependent variables. Fittingly, in [9] the authors originally\nmotivate the inversion of such a ResNets by looking at the dynamics of the associated ODE\nbackwards in time.\nSTRUCTURE PRESERVING DEEP LEARNING\n21\n4.1.3. Linear Invertible Components. Above, two general approaches for constructing in-\nvertible, nonlinear layers were presented. In the following, we list a few linear, invertible\nlayers, which are used to increase the expressivity of invertible networks.\nCoupling layers (Section 4.1.1) work by dimension splitting, which in the context of\nconvolutional neural networks usually consists in splitting the channels into two groups,\nwhich are processed independently from one another. This is in contrast to standard, multi-\nchannel convolutions, where each output channel depends on all input channels. In order\nto overcome this limitation, the channels can subsequently be linearly combined (via an\nendomorphism). If the matrix of these coeﬃcients is invertible, the automorphism created\nthis way can be integrated as a linear layer into the invertible network framework. These\ninvertible matrices can be parametrised in diﬀerent ways: One approach [74] is to directly\nparametrise the desired matrix via a LU factorisation (with additional ﬁxed permutation),\nwhich can then be inverted. While not discussed in the original publication, the diagonal\nentries themselves can be e.g. parametrised to be larger than some ε > 0 to enforce stable\ninvertibility. This is known as invertible 1 × 1 convolution, because this ’channel mixing’\ncan equivalently be realised as a convolution with (in 2D) 1-by-1 ﬁlters. An extension to\nthis idea to convolutional layers (with larger ﬁlters) is presented in [62]. A computationally\nparticularly cheaply (and stably) invertible class of matrices are orthogonal matrices. In e.g.\n[102], these are parametrised as a sequence of Householder transformations. Other possible\nparametrisations of (special) orthogonal matrices include exponentials of skew-symmetric\nmatrices, Cayley transforms or sequences of Givens rotations. For a discussion about the\nnumerical optimisation of such classes of parameter matrices compare also the forthcoming\nSection 6 and in particular (59).\nAnother type of linear, invertible layer are invertible down- and upsampling operations\nfor image data. While classical down- and upsampling methods (such as bilinear interpo-\nlation or nearest-neighbour methods) from image processing are inherently non-invertible\n(as they change the dimensionality of the input), it is possible to construct invertible up-\nand downsampling methods. Since the dimensionality of their output must be the same as\nthe dimensionality of their input, decreasing (or increasing) the spatial dimensionality of an\nimage must be accompanied by a suitable increase (respectively decrease) in the number of\nchannels. One such transformation for invertible downsampling is the pixel shuﬄe transform\n[38], which subsamples the pixels and reorders them into new channels. The inverse of this\ntransformation is an invertible upsampling operation. In [48], this is generalised to learnable\ninvertible up- and downsampling operations, which contain the (inverse) pixel shuﬄe as a\nspecial case. The general idea is to construct orthogonal strided convolutional operators,\nwhere the kernel size matches the stride s ∈Nd, with spatial dimensionality d. It can be\nshown that by a speciﬁc reordering of an orthogonal τ-by-τ matrix (where τ = s1 · · · sd) into\na ﬁlter kernel, the resulting convolution is orthogonal. This implies that the inverse is simply\ngiven by the corresponding transposed convolution. The authors propose to parametrise the\nrequired orthogonal matrices via exponentiating skew-symmetric matrices. As the matrix\nexponential is a surjective mapping from the Lie algebra of skew-symmetric matrices to the\nspecial orthogonal matrices, any special orthogonal matrix can be parametrised this way.\n4.2. Applications.\n4.2.1. Memory-eﬃcient backpropagation. One possible area of application for invertible neu-\nral networks is memory-eﬃcient backpropagation [53]. Let zk+1 = f(zk, θk) be the output\n22\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\nof a neural network’s layer with nonlinear mapping f : X k × Θk →X k+1, where θk are the\nlayer’s parameters. Let further L be the loss of the network. Then\n∇θkL =\n\u0000∂θkzk+1\u0001∗∇zk+1L =\n\u0000∂θkf(zk, θk)\n\u0001∗∇zk+1L\nprovides the weight-gradient necessary for the training of the network. For the calculation\nof this gradient, one needs both the gradient of the loss with respect to the output node (i.e.\n∇zk+1L), as well as the ability to calculate the derivative ∂θkf(zk, θk). Unless f is linear,\nthe calculation of the derivative requires access to zk, meaning that zk needs to be stored\nin memory. This typically represents the bulk of the memory demand in training neural\nnetworks. If, however f is invertible, one can simply calculate zk from zk+1 via\nzk = f −1(zk+1, θk)\nfor the additional computational cost of calculating the inverse (where f −1 is the inverse of\nf in its ﬁrst argument). Thus, instead of storing activations in memory during the forward\npass, intermediate activations can simply be successively reconstructed from the output\nlayer’s activation. The memory requirement of training an invertible network using this\nscheme is thus independent of the number of invertible layers.\n4.2.2. Invertible networks as sub-networks. In practice, many classical applications of neural\nnetworks such as classiﬁcation, regression or segmentation do not typically map between\nvector spaces of the same dimensionality. Hence, a bijective function that maps between\nthese two spaces does not exist, which is why a fully invertible neural network typically\ncannot solve the desired task. It is however possible to use an invertible neural networks\nas a subnetwork in a neural network. For example, if Ψ : X →X is an invertible network\nand η : X →Y is a suitable non-invertible layer, the combined network η ◦Ψ : X →Y\ncan for instance be used for a classiﬁcation task of predicting labels in Y from features\nX. As demonstrated in [69], such a neural network can have competitive performance to\na comparably-sized residual network [60] on ImageNet [37]. Adding an output layer which\ntransforms between spaces mirrors the approach for discretised ODEs, see Section 1.2.\nThe use of invertible networks as sub-networks for example allows for the utilisation of\nthe memory eﬃcient backpropagation for these sub-networks (cf. Section 4.2.1), while the\nrespective gradients of non-invertible sub-networks can be calculated conventionally.\n4.2.3. Density estimation and generative modeling. Aside from generative adversarial net-\nworks (GANs) and variational autoencoders (VAEs), normalising ﬂows are another class of\nmachine learning models that can be used to artiﬁcially generate data. While e.g. GANs are\nable to generate realistic-looking images [71], they lack the ability to estimate the likelihood\nof data under the generative model at hand. Likewise, VAEs can only estimate a lower\nbound of the likelihood – the variational lower bound. This is in contrast to normalising\nﬂows, which are trained via maximum likelihood estimation.\nLike most generative models, normalising ﬂows generate data from a simple base distri-\nbution (usually Gaussian) via a learned transformation. Let the random variable z have\nprobability density function q, for which we will write z ∼q(z). For any diﬀeomorphism f,\nit holds that\nx := f −1(z) ∼q(z) ·\n\f\fdet(∂zf −1(z))\n\f\f−1\nSTRUCTURE PRESERVING DEEP LEARNING\n23\ndue to the change-of-variables theorem3 [14]. This means that for the probability density of\nx (denoted p), the log-likelihood of x can be expressed as\nlog p(x) = log q(f(x)) + log |det(∂xf(x))| .\n(48)\nLet f be parametrised by an invertible neural network, i.e. f(x) = Ψ(x, θ) for all x ∈X.\nGiven training data (xn)N\nn=1, the minimiser of\nmin\nθ∈Θ\n(\nE(θ) = −1\nN\n N\nX\nn=1\n−log q(Ψ(xn, θ)) + log |det(∂xnΨ(xn, θ))|\n!)\n.\n(49)\nis a maximum likelihood estimator of the training data under the neural network. Models\ntrained this way are called normalising ﬂows, because they are usually trained to convert\ncomplicated data into normal distributions. In this framework, artiﬁcial data can be gen-\nerated by sampling f −1(z), where z ∼q(z). An example of learning the ’two half-moons’\ndataset this way is provided in Figure 2. This case highlights that continuously transforming\na normal distribution into a distribution whose probability density function has disconnected\nsupport is not possible. However, the normalising ﬂow instead assigns low probability den-\nsity to areas outside of the support, which results in a distribution that resembles the true\ndistribution.\nOne of the main diﬃculties in designing normalising ﬂows lies in the evaluation (respec-\ntively diﬀerentiation) of the determinant term. Note that for a neural network of the form\n(2) it holds that\ndet(∂z0Ψ(z0, θ)) = det(∂zK−1zK) · · · det(∂z0z1),\nsuch that the computation of the determinant for the whole network can be performed in\na layer-by-layer fashion. Still, na¨ıvely computing each determinant by the Leibniz formula\nyields prohibitive computation times. Depending on the types of layers used, the structure of\nthe individial layers’ Jacobian makes employing diﬀerent determinant identities possible. If\ne.g. the Jacobian is lower triangular (as is the case with coupling layers [38]), the determinant\nreduces to the product of the Jacobian diagonal entries. Invertible residual networks [9] on\nthe other hand induce no such structure. By using the Lipschitz constraint on f in equation\n(44) and using the identity from [120], the authors show that in their case\nlog\n\f\fdet(∂z0Ψ(z0, θ))\n\f\f = trace\n\u0000log ∂z0Ψ(z0, θ)\n\u0001\nholds. The matrix logarithm is then approximated as a truncated Taylor series, whereas\nfor the evaluation of the trace, the Hutchinson estimator [65] is employed. An extension\nto this concept is presented in [23], where the truncation with a ﬁxed number of steps is\nreplaced by a ’Russian roulette’ estimator, a type of Monte-Carlo estimator. This estimator\nintroduces a stochastic truncation of the series, which results in an unbiased estimator of\nthe log-likelihood (48).\nIn the context of normalising ﬂows, another connection to ODEs becomes apparent: As\nnoted in [24], a continuous version of the change-of-variables theorem may be used to for-\nmulate a continuous normalising ﬂow, which in turn can be solved numerically via a dis-\ncretisation scheme. A maximum likelihood estimator can in turn be obtained by training a\nNeural ODE [24].\n3Here it suﬃces to consider invertible functions f that are only locally diﬀeomorphic in x. This is of\npractical relevance, because many neural networks are only locally diﬀerentiable due to the use of piecewise\ndiﬀerentiable activation functions such as ReLU.\n24\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\nFigure 2. Left: ’Two half-moons’ dataset.\nMiddle: An approximation\nof the probability density function of the ’two half-moons’ dataset by a\nnormalising ﬂow, generated by the code from [23]. Right: Points sampled\nfrom the approximated distribution.\n4.3. Open problems.\n4.3.1. Fast inverses without coupling. Coupling layers (Section 4.1.1) allow both for quick\nforward and inverse computations. Their expressivity is however hindered by the fact that\nexpressive, learned transformations are only applied to a part of their input, while the\nremaining portion of the input is not transformed. However, while this problem can be\nmitigated by using multiple coupling layers (with diﬀerent partitions of the input), it may\nbe desirable to construct invertible layers, where each output neuron depends on all input\nneurons (as is the case with ’conventional’ fully-connected or convolutional layers). Invertible\nresidual layers (Section 4.1.2) on the other hand fulﬁll this criterion, but they rely on\na numerical inversion via a ﬁxed-point iteration, which requires multiple evaluations of\nthe forward operation. Furthermore, in order to guarantee their invertibility, a Lipschitz\nconstant needs to be controlled, which requires additional computation time. For practical\npurposes, it would be desirable to combine both methods’ advantages in order to obtain\nboth fast and expressive invertible layers.\n4.3.2. Stability guarantees. As discussed in [10], controlling the Lipschitz constant of in-\nvertible layers is a conceptionally simple, but practically costly method of guaranteeing a\ncertain stability of the inverse. Furthermore, as discussed above, they need to be controlled\nin order to guarantee the convergence of the iterative scheme for the inversion of residual\nlayers. In the interest of computational eﬃciency, is there a computationally cheaper way\nto control the Lipschitz constant than via the proposed power method? Work in this direc-\ntion has already been performed in [23], as the authors experiment with Lipschitz constant\ncorresponding to mixed norms.\nIn a similar vein, are there alternative ways of guaranteeing stability other than by\ncontrolling the Lipschitz constant?\n5. Equivariant neural networks\nIn recent years, the use of convolutional architectures in deep neural networks [81] for\nimaging tasks in machine learning has proven to be an extremely fruitful idea.\nA par-\nticularly well known example of this is the work in [78], where deep convolutional neural\nSTRUCTURE PRESERVING DEEP LEARNING\n25\nnetworks (CNNs) were used to achieve state-of-the-art performance in the ImageNet contest\n(a challenging image classiﬁcation task), outperforming other approaches by a large margin.\nAnother striking example of the power of CNNs is given in [114], where it is shown that\neven an untrained CNN can be used as an eﬀective prior for natural images. It is commonly\nunderstood that the eﬀectiveness of CNNs in imaging tasks is in large part due to them\nbeing in some sense right for the problems at hand. CNNs combine the ﬂexibility of neural\nnetworks (in the form of many learnable ﬁlters) with the known symmetries of images: both\nconvolutions and pointwise nonlinearities commute with translations of the underlying do-\nmain, so that the outputs of a CNN transform in a predictable way when their inputs are\ntranslated. By constraining the search space in a principled way (in theory, fully connected\nnetworks are at least as expressive as CNNs), CNNs can make eﬃcient use of training data\nto learn to perform tasks to a higher standard than fully connected networks and it is easier\nto interpret the action of a CNN on its inputs than it is do the same for a fully connected\nnetwork.\nGiven the success of CNNs in machine learning, it is natural to ask whether the concept of\nconvolutional architectures can be generalised to incorporate other symmetries into neural\nnetwork architectures. One current line of research in this direction is the study of group\nequivariant neural networks, which has been gaining considerable traction since the work\non group convolutional neural networks in [32]. A neural network can be thought of as a\nfunction taking inputs from a feature space X 1 and returning outputs from a feature space\nX 2. We will call a function F : X 1 →X 2 G-equivariant if there are group actions of G\non X 1 and X 2 (denoted by T X 1 and T X 2 respectively to emphasise that the group actions\nneed not be of the same nature) such that\nF(T X 1\ng\nx) = T X 2\ng\n[F(x)]\nfor all x ∈X 1, g ∈G.\n(50)\nTo elucidate this deﬁnition, let us note some examples of behaviour covered by it:\n• if G acts trivially on X 2, i.e. T X 2\ng\n= id, we recover invariance of F to group\ntransformations of its input, which is often a desirable property of an image classiﬁer,\n• whereas if X 1 = X 2 and T X 1 = T X 2, the output of F transforms in exactly the same\nway as the input does, which is a useful property to have in many image-to-image\ntasks such as segmentation.\nIf we are given two G-equivariant functions F1 : X 1 →X 2, F2 : X 2 →X 3 (with the same\naction of G on X 2 for both functions), their composition is easily seen to be G-equivariant:\nF2(F1(T X 1\ng\nx)) = F2(T X 2\ng\n[F1(x)]) = T X 3\ng\n[F2(F1(x))].\nAppealing to this result and noting the usual structure of a neural network as a composition\nof an alternating sequence of aﬃne maps and nonlinearities, there is a promising way of de-\nsigning G-equivariant neural networks: design G-equivariant linear maps and G-equivariant\nnonlinearities, add biases as appropriate to get aﬃne maps from the linear maps, and com-\npose the aﬃne maps and nonlinearities as you would in an ordinary neural network.\nAs it turns out, when the group actions considered above are in fact group representa-\ntions (i.e. they act linearly), the problem of ﬁnding and characterising G-equivariant linear\nmaps reduces to the well-studied problem of ﬁnding and characterising intertwiners in rep-\nresentation theory.\nThis insight has recently been used to unify existing approaches to\nG-equivariant neural networks and to show that G-equivariant linear maps necessarily take\n26\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\nthe form of a type of group convolution [30]. Let us describe this work in some more detail\nhere.\n5.1. Equivariant transformations of feature maps on homogeneous spaces.\n5.1.1. Homogeneous spaces. It has been observed [30, 77] that there is a common setting\nunifying many of the existing approaches to G-equivariant neural networks. We are given\na group G which acts continuously and transitively on a domain X (X is a so-called ho-\nmogeneous space of G). With this assumption we can cover the cases where X = Rd and\nG = SE(d) := Rd ⋊SO(d) the group of rotations and translations, and where X = Sd−1 and\nG = SO(d), which are two commonly studied cases. Fixing a point p ∈X as the origin and\ndenoting by Gp = {g ∈G|gp = p} the stabiliser of p, we can identify X with the quotient\nspace G/Gp: since G acts transitively on X, for any q ∈X there is a g ∈G such that gp = q.\nOn the other hand if g1p = g2p, then g−1\n2 g1p = p, so that g−1\n2 g1 ∈Gp, or g2Gp = g1Gp.\nHence, there is a well-deﬁned bijective map X →G/Gp mapping q to gGp, where g ∈G is\nsuch that gq = p. Conversely, given a closed subgroup H < G, G acts transitively on the\nquotient space G/H by left multiplication, making it into a homogeneous space of G. From\nthis reasoning, we see that the homogeneous spaces of G can be identiﬁed precisely with the\nquotient spaces G/H as H ranges over closed subgroups of G. Henceforth, we will consider\ntwo arbitrary subgroups H1, H2 < G and the associated homogeneous spaces G/H1, G/H2.\n5.1.2. Equivariant linear maps. Scalar-valued features on these spaces can be modelled as\nfunctions G/Hi →R and these can be arbitrarily stacked to get feature maps x : G/Hi →\nRC that transform under the action of G according to [π1(g)x](p) = x(g−1p), but in this\nsetting one can also consider more general features: given a representation (Vi, ρi) of Hi,\nwe can consider signals to be ﬁelds of Vi-valued features on G/Hi, which are strictly more\ngeneral than the stacks of scalar-valued ﬁelds since their components are mixed under the\naction of G. This is mathematically formalised by noting that G is a principal Hi-bundle,\nconstructing from this the associated vector bundle Pi with ﬁber space Vi, the sections of\nwhich, Γ(Pi), are the signals of interest. Under the action of the group G, these signals\nnaturally transform according to the representation of G induced by H, πG\nHi. To put this\nin the notation of (50), we are taking X 1 = Γ(P1), X 2 = Γ(P2) and T X 1\ng\n= πG\nH1(g), T X 2\ng\n=\nπG\nH2(g) and we are asking what the general form is of a linear map F : X 1 →X 2 that\nsatisﬁes (50) in this case. There are multiple ways in which Γ(Pi) and πG\nHi can be modelled,\nbut for this purpose it is easiest to consider Mackey functions: we identify x ∈Γ(Pi) with\nx : G →Vi satisfying x(gh) = ρi(h−1)x(g) for all h ∈Hi, in which case the induced\nrepresentation is just given by [πG\nHi(g)x](g′) = x(g−1g′). Writing F as integration against a\nkernel K : G × G →Hom(V1, V2), the equivariance condition tells us that\nZ\nG\nK(g′−1g, g′′)x(g′′) dg′′ = [πG\nH2(g′)F(x)](g)\n= F([πG\nH1(g′)x])(g)\n=\nZ\nG\nK(g, g′′)x(g′−1g′′) dg′′.\nSTRUCTURE PRESERVING DEEP LEARNING\n27\nThe ﬁnal expression on the right hand side can be rewritten assuming left-invariance of the\nmeasure (as can be ensured if we have a Haar measure on G) to give\nZ\nG\n(K(g′−1g, g′′) −K(g, g′g′′))x(g′′) dg = 0\n∀x ∈Γ(P1),\nwhich is the case if and only if\nK(g′−1g, g′′) = K(g, g′g′′)\n∀g, g′, g′′ ∈G.\nThis ﬁnal condition tells us that K(g, g′) = K(e, g−1g′) =: k(g−1g′), so we ﬁnd that F is\nequivariant and only if it is given by a convolution type operation:\nF(x)(g) =\nZ\nG\nk(g−1g′)x(g′) dg′.\n(51)\nSince we have assumed that F(x) ∈Γ(P2) is a Mackey function, we must have\nZ\nG\nρ2(h−1\n2 )k(g−1g′)x(g′) dg′ = ρ2(h−1\n2 )F(x)(g) = F(x)(gh2)\n=\nZ\nG\nk((gh2)−1g′)x(g′) dg′\n=\nZ\nG\nk(h−1\n2 g−1g′)x(g′) dg′.\nHence ρ2(h2)k(g) = k(h2g) for all h2 ∈H2, g ∈G. On the other hand, x ∈Γ(P1) is also a\nMackey function, so for any h1 ∈H1\nZ\nG\nk(g−1g′)x(g′) dg′ =\nZ\nG\nk(g−1g′)ρ1(h1)ρ1(h−1\n1 )x(g′) dg′\n=\nZ\nG\nk(g−1g′)ρ1(h1)x(g′h1) dg′\n=\nZ\nG\nk(g−1g′h−1\n1 )ρ1(h1)x(g′) dg′.\nHere we have assumed right invariance of the measure. This implies that k(gh1) = k(g)ρ1(h1)\nfor h1 ∈H1, g ∈G. Taking together the above results, the condition for equivariance of a\nlinear map F : Γ(P1) →Γ(P2) is that we perform a convolution type operation against a\nkernel k satisfying the linear constraints\nk(h2gh1) = ρ2(h2)k(g)ρ1(h1)\n∀h1 ∈H1, g ∈G, h2 ∈H2.\nThese constraints can be solved once the type of features Vi have been chosen (so before\ntraining time) to ﬁnd a basis for the convolution kernels that give rise to equivariant linear\nmaps, and at training time we can learn equivariant linear maps by learning the parameters\nof an expansion in this basis.\n28\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n5.1.3. Equivariant nonlinearities. Having established the general form for equivariant linear\nmaps, the question remains how to choose equivariant nonlinearities. If we are to propose\na nonlinearity F : Γ(P1) →Γ(P1), we can not in general just apply a pointwise nonlinearity\nas in ordinary neural networks: this will only work if the chosen representation of H1 does\nnot mix components, as is the case for instance if the representation of H1 is trivial (which\nis always the case if H1 = {e}) or if the regular representation of H1 is chosen [118]. One\nway to make this work in general is by having the ﬁrst layer of the network be a linear layer\nmapping feature maps deﬁned on the base domain X = G/H1 to feature maps deﬁned on\nthe group G = G/{e} and letting all feature maps after that be deﬁned on the group [32, 11].\nIf the chosen representation does not work well with pointwise nonlinearities, another thing\nthat can be done is to take the pointwise norm of the feature map (which is a scalar-valued\nfeature map), pass it through a pointwise nonlinearity, and multiply this by the feature\nmap: F(x)(p) = f(∥x(p)∥)x(p), as is done for instance in [121, 111]. Note that f could\ninclude a learnable parameter such as a bias parameter.\nAnother option is to combine\nfeatures of diﬀerent types in a tensor product [76], which is particularly convenient when\nworking in Fourier space: in Fourier space the convolution operation becomes a pointwise\nmultiplication, but it is not immediately obvious how to apply equivariant nonlinearities, so\nother methods performing the convolution in Fourier space have to transform back to ”real”\nspace (which is computationally expensive) before applying the nonlinearity [31, 47].\n5.2. A numerical demonstration of the use of equivariant neural networks. In this\nsection, let us consider an example that demonstrates some of the advantages that can be\ngained by using equivariant neural networks. We will use a supervised learning approach\nto learn a denoiser: we assume that we are given pairs random variables representing clean\nand noisy images (x∗, x) and attempt to solve the following optimisation problem:\nmin\nΨ∈C\n1\n2E\nh\n∥Ψ(x) −x∗∥2\n2 + λ∥∇(Ψ(x) −x∗)∥1,ε\ni\n.\n(52)\nHere, C is a class of functions, λ ≥0 is a small constant, ∇is a ﬁnite diﬀerence image\ngradient operator and ∥·∥1,ε is a smoothed version of the 1-norm. In this speciﬁc experiment,\nwe let the clean images x∗be of size 60×60, containing random rectangles with sides aligned\nto the grid and with random colours and we let the noisy images be generated from clean\nimages by adding Gaussian white noise (as shown in the top right row of Figure 3). It\nis natural to ask for the denoiser to commute with rotations and translations, that is, for\nthe denoiser to be equivariant with respect to rotations and translations. We compare two\nchoices of C in Problem (52), which we will refer to as the class of ordinary and equivariant\ndenoiser respectively. In both cases, the functions in C the form of a ResNet (as deﬁned in\n(5)) preceded by a learnable lifting layer and succeeded by a learnable projection layer. The\ndistinction between the ordinary and equivariant denoisers, is that the ordinary denoiser uses\nordinary convolution operations and a pointwise ReLU nonlinearity (resulting in translation\nequivariance), whereas the equivariant denoiser uses the rotation and translation equivariant\nversions of these operations as deﬁned in [121]. To give a fair comparison, we ﬁx the same\nwidth and depth in both classes. In this case, the equivariant denoiser has a number of\ndegrees of freedom that is less than 10% of the number of degrees of freedom of the ordinary\ndenoiser (62994 versus 741891). The denoisers are trained by performing 1500 iterations of\nAdam [73] on minibatches of size 64, decreasing the step size when validation error stagnates,\nand for each denoiser we perform 5 training runs. The results are shown in Figure 3: as\nwe see in the plot of the training errors, the equivariant denoiser consistently converges in\nSTRUCTURE PRESERVING DEEP LEARNING\n29\nfewer iterations than the ordinary denoiser and achieves a better objective function value.\nBesides this, we can be conﬁdent that the equivariant denoiser will generalise to rotated\nexamples despite not having seen them in training, whereas it is hard to say anything about\nhow the ordinary denoiser generalises to rotated examples.\n500\n1,000\n1,500\n10−4\n10−3\n10−2\n10−1\n100\nIteration number\nTraining Error\nEquivariant\nOrdinary\nClean\nNoisy\nOrdinary\nEquivariant\nClean\nNoisy\nOrdinary\nEquivariant\nOn-grid example\nRotated example\nFigure 3. A demonstration of the use of equivariance in a denoising task.\nOn the left, we have plotted training errors for 5 runs of each denoiser. On\nthe right, we have displayed the outputs of the denoisers on an example (the\n”on-grid” example) similar to the ones used for training and on a rotated\nversion.\n5.3. Open problems.\n5.3.1. Sample complexity bounds. One of the main motivations that is given for the use of\nequivariant neural networks is that they should be able to use training data more eﬃciently\nthan neural networks that are not designed to be equivariant. This is particularly important\nin applications in which training data is in short supply, as is often the case in inverse\nproblems [6]. In the machine learning community, the number of samples needed to estimate\na given object is known as the sample complexity. There is recent work [40] establishing\nsample complexity bounds for some simple CNN models, and showing that these bounds\ncompare favourably to the corresponding ones for fully connected networks. In a similar\nvein, it would be interesting to establish rigorous sample complexity bounds for equivariant\nneural network models that guarantee their data eﬃciency.\n5.3.2. Approximation properties. When applying existing group equivariant neural network\narchitectures as in the framework described above, there are a large number of choices that\nneed to be made: the domains on which the feature maps are deﬁned, the choices of the\ntypes of features (the representation of H that is used), the choice of nonlinearity. While\nthere is a vast amount of literature on the approximation properties for ordinary neural\nnetworks (including older works such as [35, 64] and some more recent works that apply\nto CNNs [15, 100]), there is not yet the same theoretical guidance on how the choices of\nthe various aspects of group equivariant neural networks can be made to guarantee that\nthe networks are suﬃciently expressive. There is some theoretical work on hypothetical\nequivariant neural networks and approximation results relating to them [124], but it is not\nyet of practical use in choosing an equivariant architecture.\n30\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n5.3.3. Approximate equivariance. Many of the symmetries we would like to work with in\nequivariant neural networks are continuous, but when we implement them in practice it is\nnecessary to discretise them. Generally in the literature, one of two approaches is taken:\nthe group is discretised and exact group convolutions are taken with respect to the discrete\nsubgroup (in which case we have exact equivariance to the discrete symmetry), or the group\nconvolutions are derived in the continuous setting and eventually discretised (so that we\nhave approximate equivariance to the continuous symmetry). In either case, it has not been\nstudied in detail how much of an error we make when we make these discretisations and\nwhether there is an optimal discretisation to use. It would be of great interest to provide\nbounds on the equivariance error, as these could be used to decide, for example, whether a\ntheoretically invariant classiﬁer is actually invariant in practice.\n6. Structure-exploiting learning\nThe training of neural networks amounts to the numerical optimisation of typically\nsmooth, but high-dimensional and highly non-linear objectives as in (4), and as discussed\nin the optimal control framework in Section 3. The most widely used numerical method for\n(4) is Adam [73].\nAs before let Ψ : X × Θ →X be a (deep) neural network that depends on the data and\nthe parameters θ ∈Θ. The training of Ψ amounts to the optimisation over the parameters\nθ with respect to a loss function\nmin\nθ∈Θ\n(\nE(θ) = 1\nN\nN\nX\nn=1\nL(η ◦Ψ(xn, θ), yn) + R(θ)\n)\n,\nas in (4).\nWhile the loss function L usually is a convex function, the dependency of\nthe parameter θ on the network Ψ is in general highly nonlinear which makes the over-\nall optimisation problem in (4) non-convex. For what follows, let us denote by Lj(θ) =\n1\n|Sj|\nP\nn∈Sj(L(η◦Ψ(xn, θ), yn)+ 1\nN R(θ)), for j = 0, 1, . . . and where Sj is a randomly chosen\nset of indices from the N training samples. For ﬁxed positive parameters α and β1, β2 < 1,\nand for appropriate initialisations for the parameters and the auxiliary moment functions\nm0 and v0, Adam amounts to the following iteration\nmj =\n1\n(1 −βj\n1)\n\u0000β1mj−1 + (1 −β1)∇Lj(θj−1)\n\u0001\nvj =\n1\n(1 −βj\n2)\n\u0000β2vj−1 + (1 −β2)∇Lj(θj−1) · ∇Lj(θj−1)\n\u0001\nθj = θj−1 −α\nmj\n√\nvj + ε\n,\n(53)\nfor a small ε > 0 and where\n√\nvj is taken component-wise. Stochasticity, in the form of\nchoosing subsets Sj randomly in every iteration, is crucial to deal with high-dimensionality\nof the problem.\nAdam is being used for almost all of neural network training because of its easy im-\nplementation, its robustness to rescaling, its computational eﬃciency and small memory\nrequirements and for its suitability for problems which are large-scale in terms of training\ndata and parameters. On the other hand, its theoretical convergence properties do in gen-\neral not guarantee convergence to a solution of (4), cf. [104] where the authors propose a\nSTRUCTURE PRESERVING DEEP LEARNING\n31\nconvergent variant in the convex setting and [125] which provides convergence guarantees\nin the non-convex and stochastic setting.\nThe literature for optimising smooth (non-convex) objectives is, however, much richer\nthan Adam alone. It is tightly linked to developments in convex analysis and operations\nresearch, as well as the numerical discretisation of dynamical systems, ODEs and PDEs\nas discussed in parts in Sections 2 and 3. Also in the context of neural network training\nother optimisation schemes have been investigated recently. Here, we will mainly focus on\nthose which have some structure-preserving property such as Hamiltonian descent [88, 98]\nwhich are guaranteed to dissipate a Hamiltonian energy, optimisation approaches when the\nfeatures or network parameters are elements in a Riemannian rather than Euclidean space\n[1, 113, 84], and – as a special case of the latter – information geometry approaches that\ndescribe optimisation on statistical manifolds [4].\n6.1. Conformal Hamiltonian systems. The most classic approach for minimising E(·)\nover RL is gradient descent, i.e. to seek a stationary point of E by evolving\n˙θ = −∇E(θ).\n(54)\nSeveral optimisation methods for E can then be derived through diﬀerent discretisations of\n(28), with the simplest one being explicit gradient descent and its stochastic versions [106].\nAnother route for the derivation of optimisers for E is obtained by replacing the gradient\nﬂow (54) with a Hamiltonian ﬂow as in (10) with dissipation, for example a conformal\nHamitonian system, i.e.\n˙p = −∇E(θ) −γp,\n˙θ = p\nµ,\n(55)\nwith γ, µ > 0. Rewriting the system (55) in one equation gives\n¨θ + γ ˙θ = −1\nµ∇E(θ),\nwhich is gradient descent accelerated by momentum. More generally, (55) is a special case\nof a conformal Hamiltonian system of the form\n˙p = −∇θH(p, θ) −γp\n˙θ = ∇pH(p, θ),\n(56)\nfor a separable Hamiltonian function H(θ, p) = T(p) + E(θ) as in Section 2.1.2. Taking\nT(p) =\n1\n2µ∥p∥2\n2 we get (55). Taking T(p) =\np\n∥p∥2 + ε we get a new optimisation approach\nfor E which is called relativistic gradient descent [49]\n˙p = −∇E(θ) −γp\n˙θ =\np\np\nε + ∥p∥2\n(57)\nWhile the gradient system (54) dissipates E, for a conformal Hamiltonian system (56) the\nHamiltonian H is dissipated as\nd\ndtH(p, θ) = −γ\n2µpT p.\n32\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\nFigure 4. Optimisation of V (x, y) = 2x2 −1.05x4 + 1/6x6 + xy + y2.\nComparison of gradient descent, Hamiltonian descent and Adam iterations,\ninitial guess [−0.5, 0.8], global minimum at [0, 0]. Methods and parameters:\nGradient descent (GD) learning rate h = 0.01, Heavy ball (HB) µ = 0.9,\nh = 0.01, Nesterov Accelerated Gradient (NaG) µ = 0.012, h = 0.01,\nRelativistic Gradient Descent (RGD) [49] h = 0.0001, µ = 0.9259, Adam\nβ1 = 0.9, β2 = 0.999, ε = 1e −8, α = h = 0.1. Left: value of the loss\nfunction versus number of iterations. Right: trajectory of approach to the\noptimum (NaG h = 0.01 yellow, RGD h = 0.0001 magenta, Adam h = 0.1\ngreen).\nFor separable Hamiltonians with the kinetic energy T chosen so it has a global minimiser in\n0, limit points of (56) recover stationary points of E. More precisely, for H = T(p) + E(θ)\nbeing separable the equilibria of (56) fulﬁll\nγ p = −∇E(θ)\n0 = ∇T(p).\nIf T is chosen so it has a unique global minimum in p∗= 0 (e.g. in the case of (55)), we have\nthat (0, θ∗) is a zero of ∇H(p, θ) if and only if u∗is a zero of ∇E(θ), i.e. θ∗is a stationary\npoint of E(θ). Moreover, we have that (0, θ∗) is the solution of the conformal Hamiltonian\nsystem as t →∞. Then, using a numerical integration method that preserves the property\nof Hamiltonian energy dissipation (such a numerical method is called conformal symplectic\nscheme [13]), we get an approximation of (θ∗, 0), where (θ∗, 0) is an equilibrium of (56) and\nθ∗a stationary point of E [92].\nIn the recent work [49] the authors take advantage of the connection between optimisation\nschemes for E and conformal symplectic Hamiltonian schemes for H = T +E for the design\nof new optimisation approaches for E - similar to the connections we have seen before, e.g.\nbetween Adam and conformal Hamiltonian descent. See also Figure 4 for a comparison\nbetween diﬀerent optimisation methods applied to the camelback function.\n6.2. Learning in Riemannian metric spaces. After identifying the dissipation of an\nappropriate Hamiltonian as a structure worth preserving for numerical optimisation of neural\nnetworks, as discussed in the last section, explicit conditions on the parameter matrices\nthemselves, such as orthogonality, seem to impose structure on the optimisation that can be\nof advantage and in particular lead to better (at least empirical) convergence rates, better\nSTRUCTURE PRESERVING DEEP LEARNING\n33\ngeneralisability and accuracy [84]. Such conditions usually pose a parameter optimisation\nproblem within a Riemannian metric space rather than Euclidean space. Moreover, there\nare several important applications, cf. in particular the next subsection, where the training\ndata and the parameters naturally lie in a Riemannian space.\nWhile in section 2.3 we discussed the case when the training data and features lie on a\nRiemannian manifold, in this section we consider the setting when the parameters belong\nto a manifold. This in particular means that ordinary gradient descent a la (54), i.e. with\nthe gradient ∇being the ordinary gradient in RL, does in general not describe steepest\ndescent of E in such a Riemannian metric space. Instead descent with respect to a metric-\ninduced gradient needs to be considered [5]. In what follows, we consider some representative\nexamples of how Riemannian optimisation could arise in the context of deep learning.\n6.2.1. Network parameters evolving on manifolds. Assuming that the parameters to be\nlearned evolve on a manifold arises when additional structure or constraints are imposed\non the parameters. This is done to improve stability of the training algorithm and of the\ntrained model. Pioneering work advocating the use of Riemannian gradient and orthogo-\nnality constraints can be found in [2, 3, 17] and there is an extensive follow up literature.\nExamples of this procedure in the context of deep learning have recently appeared in [70]\nand earlier in the context of CNNs in [27, 8]. It is crucial to implement eﬃciently the Rie-\nmannian gradient descent making good use of the tensor structure of the data and of the\nlayers of the neural network, avoiding undesired increase in computational complexity. One\nway to proceed is to introduce an evolution equation for θ and replace (7) by the extended\nsystem of ODEs\n˙z = f(z, θ),\n(58)\n˙θ = g(θ).\n(59)\nOf particular importance is the case where the second equation evolves on a Lie group G\nor on a homogeneous manifold M = G/H.4 In [3] G is simply the general linear group,\nwhile orthogonality constraints have been adopted in [66] in the context of independent\ncomponent analysis. Other concrete examples that we have in mind include the aﬃne group,\nthe special Euclidean group SE(n), the special orthogonal group SO(n), the Stiefel manifold\nVn,p = SO(n)/SO(n −p) and the Grassmann manifold Gn,p = SO(n)/(SO(n −p) × SO(p))\nincluding the n-sphere.\nFor matrix Lie groups G (and homogeneous manifolds) (59) can simply take the form of\na linear matrix diﬀerential equation\n˙θ = M(t) θ,\nθ ∈M\nand M(t) ∈g, and the optimisation can be performed in terms of the variable M. To\npreserve the manifold structure one can consider a discretisations of this equation of the\nform\nθk+1 = exp(h Mk)θk,\nwhere exp : g →G is the matrix exponential. This discretisation can be seen as the proto-\ntype for constructing local coordinates5 on the manifold of parameters. Alternatively any\n4Here H is a closed Lie subgroup of G, and M := G/H the quotient with the manifold structure turning\nπ : G →G/H π : g 7→gH into a submersion. Then M becomes a homogeneous space for G with respect to\nthe transitive Lie group action induced by left multiplication.\n5Otherwise called retraction maps [1].\n34\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\napproximation of the exponential map φ ≈exp (e.g. by a rational approximant) preserving\nthe property φ : g →G, can be used to construct local parametrisations of the manifold6.\nOn homogeneous manifolds such as Vn,p and Gn,p additional structure on M(t) can be\nexploited to reduce the computational cost of matrix exponentials and similar mappings\n[19].\nAfter time discretisation, this approach guarantees that the parameters at each layer of\nthe network belong to manifolds which are all naturally equipped with Riemannian metrics\nand some of which are compact. In particular, a metric on G which is H-right invariant (i.e.\nthe right multiplication Rh with h ∈H is an isometry) descends to a Riemannian metric\non M = G/H, [50]. Gradient descent techniques to train the network should then exploit\nthe Riemannian structure [2, 70].\nA suﬃcient condition for convergence of Riemannian\ngradient descent is that the manifolds are geodesically complete, [119, 113]. All Riemannian\nhomogeneous manifolds as well as compact Riemannian manifolds are geodesically complete\n[75, IV.4].\n6.2.2. Information Geometry. A special case of the Riemannian structure discussed in the\nprevious paragraph arises when taking into account the inherent statistical properties of\nthe underlying unknown distributions of training pairs and, connected to this, statistical\nproperties of the network parameters θ.\nTreating the parameters θ as probability distributions, they can be modelled as elements\non a statistical manifold with an appropriate metric. A statistical manifold is a Riemannian\nmanifold whose points correspond to probability distributions. Gradient descent on statis-\ntical manifolds is studied in information geometry. Here, the so-called natural gradient is\nthe proposed notion for gradient on a statistical manifold [2]. For an L-dimensional param-\neter space, equipped with a Riemannian metric tensor G = G(θ) = (gij(θ)) ∈RL×L that\ndepends on θ, the natural gradient of E(θ) is deﬁned as\n˜∇E(θ) = G−1(θ)∇E(θ),\nwhere ∇E(θ) denotes the ordinary gradient of E in RL. The natural metric considered in\nthis context is the Fisher information, with G being the Fisher information matrix of the\nparameters θ. Natural gradient descent then reads\n˙θ = −G−1(θ)∇E(θ).\n(60)\nIn the case of E(θ) =\n1\n2N\nPN\nn=1 ∥Ψ(xn, θ)−yn∥2 a squared error loss and G being the Fisher\ninformation matrix, we have\nG(θ) = 1\nN\nN\nX\nn=1\nJT\nn Jn,\nwhere Jn is the Jacobian of Ψ(xn, θ) with respect to θ, cf. [89]. Note that in this case\nnatural gradient descent, discretised with forward-Euler, is equivalent to Gauss-Newton\niteration [97]. This connection between natural gradient descent and (extended) Gauss-\nNewton methods can be extended to more general losses as well [99]. In the continuum\nlimit, (60) is a gradient ﬂow with respect to the Fisher-Rao metric, see [95, Deﬁnition 3.1.].\nSeveral works [2, 4, 99] have demonstrated advantages of using the natural gradient over\nthe ordinary gradient in (54) for neural network training. The Riemannian structure seems\n6An example for the case of SO(n) and Sp(n) is the Cayley transformation, used in the context of\ninvertible networks in section 4.1.3\nSTRUCTURE PRESERVING DEEP LEARNING\n35\nto help against the gradient descent being trapped in ﬂat areas of the loss function’s surface\n[123], and as a result the network to feature better generalisation capabilities, cf. [61, 72]\nfor diﬀerent characterisations of ﬂatness of the loss function’s surface.\n6.3. Optimisation of 2-layer ReLU neural networks as Wasserstein gradient ﬂows.\nThe inherent gradient ﬂow structure of training neural networks also appears when studying\nglobal optimality and generalisation properties of trained networks. Bach and Chizat [25]\npick up the gradient ﬂow formulation of neural network learning and study convergence of the\nlearning problem (4) to a global minimiser of E for 2-layer ReLU neural networks Φ in the ∞-\nwidth limit. In particular, their analysis makes use of the structure of a Wasserstein gradient\nﬂow formulation of (54) over the space of probability measures in the ∞-width network\nlimit. Proving convergence to a global minimiser of E also allows the study of optimal\ngeneralisation capabilities of the trained 2-layer ReLU neural network by characterising the\nlimit (for a certain class of loss functions with exponential tails) as a max-margin classiﬁer\n[26].\nIn [25] they consider a 2-layer neural network of the form\nΦ(θ, x) = 1\nJ\nJ\nX\nj=1\nf(θj, x),\n(61)\nwhere\nf(θj, x) = cj max{at\njx + bj, 0},\nfor x ∈RM and θj = (aj, bj, cj) ∈RM+2. The weights θ in (61) are learned by minimising a\nloss function such as (4) with optional regularisation R(θ) = λ\nL\nPL\nj=1 ∥θj∥2\n2. In this setting,\nthey investigate the performance (generalisation capabilities) of the learned network Φ by\nstudying the associated gradient ﬂow of the loss function E for initial weights θ(0) = θ0 ∼\ni.i.d. µ0 ∈P2(Rn+1)\n˙θ = −m∇E(θ, x).\n(62)\nIn particular, they analyse convergence of (62) in the ∞-width limit, which they prove can\nbe written as a Wasserstein gradient ﬂow. More precisely, they parametrise the network Φ\nwith probability measures µ ∈P2(Rd+2) as\nΦ(µ, x) =\nZ\nΦ(θ, x) dµ(θ),\nand the associated loss function as\nE(µ) = 1\nN\nN\nX\nn=1\nL(Φ(µ, xn), yn) + λ\nZ\n∥θ∥2\n2 dµ(θ).\n(63)\nIn this setting they prove the following results.\nTheorem 6.1 ([25]). Assume that\nspt(µ0) ⊂{|c|2 = ∥a∥2\n2 + |b|2}.\nAs L →∞, µt,L =\n1\nL\nPL\nj=1 δθj(t) converges in P2(Rd+1) to µt, the unique Wasserstein\ngradient ﬂow of E in (63) starting in µ0.\nMoreover, assuming µ0 is ‘diverse’ enough (cf. [25] for details). If µt converges to µ∞\nin P2(Rd+1), then µ∞is a global minimiser of E.\n6.4. Open problems.\n36\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n6.4.1. Port-Hamiltonian optimisation methods. Generally, investigating new optimisation\nmethods for E by considering diﬀerent instances of Hamiltonian descent is a promising re-\nsearch direction. Here, diﬀerent choices of Hamiltonians or special cases of Hamiltonian\nsystems might be advantageous for classes of loss functions and network architectures, im-\nposing diﬀerent descent dynamics. A more concrete example are numerical schemes that\narise when symplectic numerical integration is applied to port-Hamiltonian systems (with\ndiﬀerent Hamiltonians), cf. [115] for an introduction to port-Hamiltonian systems and [20]\nfor the development of structure preserving numerical integrators for port-Hamiltonian sys-\ntems by using discrete gradient and splitting approaches. In [91] the authors design a loss\nfunction and parameter optimisation dynamics of the network in such a way that the neural\nnetwork itself behaves like an autonomous port-Hamiltonian system. This in turn allows a\nproof of convergence of the optimisation algorithm to a minimum of the loss. Taking this a\nstep further, port-Hamiltonian systems also lend themselves to the design of locally adaptive\noptimisation schemes as the port-Hamiltonian structure is preserved under concatenation\nof port-Hamiltonians with orthogonal input-output relation.\n6.4.2. Convergence analysis for natural gradient optimisation. While several papers seem\nto suggest (mostly empirically or for linear networks) that natural gradient descent helps\nto mitigate nuisance curvature in the neural network parameter optimisation, very little\nseems to have been done on this theoretically, in particular for nonlinear neural networks\n[126]. In general, while for particular choices of the Riemannian metric (60) boils down to\nclassical optimisation schemes, such as Gauss-Newton for G being the Fisher information\nmetric, analytic results on convergence properties of natural gradient ﬂow discretisations are\ngenerally open. Indeed, the study of (60) for diﬀerent choices of G could be very interesting.\nFor instance, if G is a BFGS approximation to the Hessian of E, then a stochastic method\nwas proposed and analysed in [117].\n6.4.3. Studying generalization properties of neural networks by metric gradient ﬂows. As\nthe example of the Wasserstein gradient ﬂow in section 6.3 has shown, metric gradient ﬂows\ncan serve as a useful tool for studying convergence of the network training and for the study\nof generalisation properties of the minimisers [25, 26].\nIt would be interesting to see if\nother metric gradient ﬂows would also lend themselves to such an analysis. In connection\nwith information geometry in section 6.2.2 we have seen the gradient ﬂow with respect to\nthe Fisher-Rao metric appearing. Could this be used to study convergence properties for\nother network architectures, beyond 2-layer ReLU? Are there other metrics that could be\ninteresting to investigate for that purpose?\n7. Conclusion\nStructure-preserving approaches to deep learning are a mean to design neural networks\nwith guaranteed mathematical properties, as well as derive optimisation schemes that im-\nprove their training and provide means for analysing their global optimality and generalisa-\ntion capabilities. In this paper we are discussing some recent examples from this emerging\ntopic of structure-preserving deep learning.\nThese include ODE and PDE parametrisa-\ntions of neural networks for improved stability properties, an optimal control formulation\nfor neural network training that gives rise to systematic training and regularisation proce-\ndures, invertible neural networks for large-scale deep learning, equivariant neural network\narchitectures for the design of neural networks that preserve group transformations, and\nSTRUCTURE PRESERVING DEEP LEARNING\n37\nstructure-preserving training of neural networks by means of Hamiltonian descent and Rie-\nmannian metric gradient ﬂows. Together with the discussion of state-of-the-art results we\nalso suggest a range of open problems that we identiﬁed as interesting mathematical avenues\nthat could help to shed some more light onto the systematic design and training of deep\nneural networks.\nAcknowledgements\nMJE would like to thank Matt Thorpe for fruitful discussions. MJE acknowledges support\nfrom the EPSRC grants EP/S026045/1 and EP/T026693/1, the Faraday Institution via\nEP/T007745/1, and the Leverhulme Trust fellowship ECF-2019-478.\nCE and CBS acknowledge support from the Wellcome Innovator Award RG98755.\nCBS acknowledges support from the Leverhulme Trust project on Breaking the non-\nconvexity barrier, the Philip Leverhulme Prize, the EPSRC grants EP/S026045/1 and\nEP/T003553/1, the EPSRC Centre Nr.\nEP/N014588/1, European Union Horizon 2020\nresearch and innovation programmes under the Marie Sk lodowska-Curie grant agreement\nNo. 777826 NoMADS and No. 691070 CHiPS, the Cantab Capital Institute for the Math-\nematics of Information and the Alan Turing Institute.\nFS acknowledges support from the Cantab Capital Institute for the Mathematics of In-\nformation.\nEC and BO thank the SPIRIT project (No.\n231632) under the Research Council of\nNorway FRIPRO funding scheme.\nThe authors would like to thank the Isaac Newton Institute for Mathematical Sciences,\nCambridge, for support and hospitality during the programmes Variational methods and\neﬀective algorithms for imaging and vision (2017) and Geometry, compatibility and structure\npreservation in computational diﬀerential equations (2019) where work on this paper was\nundertaken, EPSRC grant EP/K032208/1.\nReferences\n[1] Pierre-Antoine Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix\nmanifolds. Princeton University Press, Princeton, NJ, 2008. With a foreword by Paul Van Dooren.\n[2] Shun-Ichi Amari. Natural gradient works eﬃciently in learning. Neural Computation, 10(2):251–276,\n1998.\n[3] Shun-Ichi Amari, Andrzej Cichocki, and Howard Hua Yang. A new learning algorithm for blind signal\nseparation. Advances in neural information processing systems, pages 757–763, 1996.\n[4] Shun-Ichi Amari and Scott C. Douglas. Why natural gradient? In Proceedings of the 1998 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing, ICASSP’98 (Cat. No. 98CH36181),\nvolume 2, pages 1213–1216. IEEE, 1998.\n[5] Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar´e. Gradient ﬂows: in metric spaces and in the space\nof probability measures. Springer Science & Business Media, 2008.\n[6] Simon Arridge, Peter Maass, Ozan ¨Oktem, and Carola-Bibiane Sch¨onlieb. Solving inverse problems\nusing data-driven models. Acta Numerica, 28:1–174, 2019.\n[7] Manuel Asorey, Jos´e F. Cari˜nena, and Luis A. Ibort. Generalized canonical transformations for time-\ndependent systems. J. Math. Phys., 24(12):2745–2750, 1983.\n[8] Gary B´ecigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. In Interna-\ntional Conference on Learning Representations, 2019.\n[9] Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David Duvenaud, and Joern-Henrik Jacobsen.\nInvertible residual networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings\nof the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine\nLearning Research, pages 573–582, Long Beach, California, USA, 09–15 Jun 2019. PMLR.\n38\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n[10] Jens Behrmann, Paul Vicol, Kuan-Chieh Wang, Roger B. Grosse, and J¨orn-Henrik Jacobsen. On the\ninvertibility of invertible neural networks, 2020.\n[11] Erik J. Bekkers, Maxime W. Lafarge, Mitko Veta, Koen A. J. Eppenhof, Josien P. W. Pluim, and\nRemco Duits. Roto-Translation Covariant Convolutional Networks for Medical Image Analysis. In\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention, pages\n440–448. Springer, Cham, 2018.\n[12] Martin Benning, Elena Celledoni, Matthias J. Ehrhardt, Brynjulf Owren, and Carola-Bibiane\nSch¨onlieb. Deep learning as optimal control problems: models and numerical methods. Journal of\nComputational Dynamics, 6(2):171–198, 2019.\n[13] Ashish Bhatt, Dwayne Floyd, and Brian E Moore. Second order conformal symplectic schemes for\ndamped Hamiltonian systems. Journal of Scientiﬁc Computing, 66(3):1234–1259, 2016.\n[14] Vladimir I. Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007.\n[15] Helmut B¨olcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Optimal Approximation with\nSparsely Connected Deep Neural Networks. SIAM Journal on Mathematics of Data Science, 1(1):8–45,\n2019.\n[16] J. Fr´ed´eric Bonnans. Course on Optimal Control, 2019. http://www.cmap.polytechnique.fr/ bon-\nnans/notes/oc/ocbook.pdf.\n[17] Jean-Fran¸cois Cardoso and Beate Hvam Laheld. Equivariant adaptive source separation. IEEE Trans-\nactions on signal processing, 44:3017–3030, 1992.\n[18] Elena Celledoni, Markus Eslitzbichler, and Alexander Schmeding. Shape analysis on Lie groups with\napplications in computer animation. J. Geom. Mech., 8(3):273–304, 2016.\n[19] Elena Celledoni and Simone Fiori. Neural learning by geometric integration of reduced ‘rigid-body’\nequations. J. Comput. Appl. Math., 172(2):247–269, 2004.\n[20] Elena Celledoni and Eirik Hoel Høiseth. Energy-Preserving and Passivity-Consistent Numerical Dis-\ncretization of Port-Hamiltonian Systems. arXiv preprint arXiv:1706.08621, 2017.\n[21] Elena Celledoni, H˚akon Marthinsen, and Brynjulf Owren. An introduction to Lie group integrators—\nbasics, new developments and applications. J. Comput. Phys., 257(part B):1040–1061, 2014.\n[22] Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot Holtham. Reversible\narchitectures for arbitrarily deep residual neural networks. In Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence, 2018.\n[23] Tian Qi Chen, Jens Behrmann, David Duvenaud, and J¨orn-Henrik Jacobsen. Residual ﬂows for invert-\nible generative modeling. In Advances in Neural Information Processing Systems, pages 9913–9923,\n2019.\n[24] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary diﬀerential\nequations. In Advances in Neural Information Processing Systems, pages 6572–6583, 2018.\n[25] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized\nmodels using optimal transport. In Advances in neural information processing systems, pages 3036–\n3046, 2018.\n[26] L´ena¨ıc Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks\ntrained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.\n[27] Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In Advances in\nNeural Information Processing Systems, pages 5225–5235, 2017.\n[28] Marco Ciccone, Marco Gallieri, Jonathan Masci, Christian Osendorfer, and Faustino Gomez. NAIS-\nNet: Stable deep networks from non-autonomous diﬀerential equations. In Advances in Neural Infor-\nmation Processing Systems, pages 3025–3035, 2018.\n[29] Christian Clason. Regularization of Inverse Problems, 2020. arXiv:2001.00617.\n[30] Taco Cohen, Mario Geiger, and Maurice Weiler. A General Theory of Equivariant CNNs on Homoge-\nneous Spaces. In Advances in Neural Information Processing Systems 32, pages 9145—-9156, 2019.\n[31] Taco\nS.\nCohen,\nMario\nGeiger,\nJonas\nKoehler,\nand\nMax\nWelling.\nSpherical\nCNNs,\n2018.\narxiv:1801.10130.\n[32] Taco S. Cohen and Max Welling. Group Equivariant Convolutional Networks. In International con-\nference on machine learning, pages 2990–2999, 2016.\n[33] Andrew R. Conn, Nicholas I. M. Gould, and Philippe L. Toint. Trust-Region Methods, volume 1 of\nMPS-SIAM Series on Optimization. MPS/SIAM, Philadelphia, 2000.\nSTRUCTURE PRESERVING DEEP LEARNING\n39\n[34] Phil Cook, Yu Bai, Shahrum Nedjati-Gilani, Kiran Seunarine, Matt Hall, Geoﬀrey Parker, and Daniel\nAlexander. Camino: Open-Source Diﬀusion-MRI Reconstruction and Processing. In Proc.14th Sci.\nMeeting of ISMRM, Seattle WA, USA, volume 2759, 2006.\n[35] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,\nSignals, and Systems, 2(4):303–314, 1989.\n[36] Germund Dahlquist. Generalized disks of contractivity for explicit and implicit Runge-Kutta methods.\nTechnical report, CM-P00069451, 1979.\n[37] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:\nA large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npages 248–255. Ieee, 2009.\n[38] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components esti-\nmation. arXiv preprint arXiv:1410.8516, 2014.\n[39] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv\npreprint arXiv:1605.08803, 2016.\n[40] Simon S. Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Ruslan Salakhutdinov, and Aarti\nSingh. How Many Samples are Needed to Estimate a Convolutional or Recurrent Neural Network?\narxiv:1805.07883, 2018.\n[41] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Eﬃcient Projections onto\nthe l1-ball for Learning in High dimensions. In Proceedings of the 25th International Conference on\nMachine Learning - ICML, pages 272–279, 2008.\n[42] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented Neural ODEs. In Advances in Neural\nInformation Processing Systems, 2019.\n[43] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline ﬂows. In Ad-\nvances in Neural Information Processing Systems, pages 7509–7520, 2019.\n[44] Weinan E. A Proposal on Machine Learning via Dynamical Systems. Communications in Mathematics\nand Statistics, 5(1):1–11, 2017.\n[45] Weinan E, Jiequn Han, and Qianxiao Li. A Mean-Field Optimal Control Formulation of Deep Learning.\narXiv:1807.01083v1, 2018.\n[46] Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of Inverse Problems. Math-\nematics and Its Applications. Springer, 1996.\n[47] Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning SO(3)\nEquivariant Representations with Spherical CNNs. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 52–68, 2018.\n[48] Christian Etmann, Rihuan Ke, and Carola-Bibiane Sch¨onlieb. iUNets: Fully invertible U-Nets with\nlearnable up-and downsampling. arXiv preprint arXiv:2005.05220, 2020.\n[49] Guilherme Fran¸ca, Jeremias Sulam, Daniel P. Robinson, and Ren´e Vidal. Conformal symplectic and\nrelativistic optimization. arXiv preprint arXiv:1903.04100, 2019.\n[50] Sylvestre Gallot, Dominique Hulin, and Jacques Lafontaine. Riemannian geometry. Universitext.\nSpringer-Verlag, Berlin, third edition, 2004.\n[51] Nicol´as Garc´ıa Trillos and Dejan Slepˇcev. Continuum Limit of Total Variation on Point Clouds. Archive\nfor Rational Mechanics and Analysis, 220(1):193–241, 2016.\n[52] Amir Gholami, Kurt Keutzer, and George Biros. ANODE: Unconditionally accurate memory-eﬃcient\ngradients for neural ODEs. In IJCAI International Joint Conference on Artiﬁcial Intelligence, volume\n2019-Augus, pages 730–736, 2019.\n[53] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B. Grosse. The reversible residual network:\nBackpropagation without storing activations. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing\nSystems 30, pages 2214–2224. Curran Associates, Inc., 2017.\n[54] Thomas H. Gr¨onwall. Note on the Derivatives with Respect to a Parameter of the Solutions of a\nSystem of Diﬀerential Equations. Annals of Mathematics, 20(4):292–296, 1919.\n[55] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse Problems,\n34(1):014004, 2017.\n[56] William W. Hager. Runge-Kutta methods in optimal control and the transformed adjoint system.\nNumerische Mathematik, 87(2):247–282, 2000.\n40\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n[57] Ernst Hairer, Christian Lubich, and Gerhard Wanner. Geometric numerical integration: structure-\npreserving algorithms for ordinary diﬀerential equations, volume 31. Springer Science & Business\nMedia, 2006.\n[58] Ernst Hairer, Syvert P. Nørsett, and Gerhard Wanner. Solving Ordinary Diﬀerential Equations I.\nSpringer Series in Computational Mathematics. Springer-Verlag Berlin Heidelberg, 2 edition, 1993.\n[59] Ernst Hairer and Gerhard Wanner. Solving ordinary diﬀerential equations. II, volume 14 of Springer\nSeries in Computational Mathematics. Springer-Verlag, Berlin, 2010. Stiﬀand diﬀerential-algebraic\nproblems, Second revised edition, paperback.\n[60] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recog-\nnition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\n[61] Sepp Hochreiter and J¨urgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.\n[62] Emiel Hoogeboom, Rianne Van Den Berg, and Max Welling. Emerging convolutions for generative\nnormalizing ﬂows. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the\n36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning\nResearch, pages 2771–2780, Long Beach, California, USA, 09–15 Jun 2019. PMLR.\n[63] John J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abil-\nities. Proceedings of the national academy of sciences, 79(8):2554–2558, 1982.\n[64] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks,\n4(2):251–257, 1991.\n[65] Michael F. Hutchinson. A stochastic estimator of the trace of the inﬂuence matrix for laplacian smooth-\ning splines. Communications in Statistics-Simulation and Computation, 19(2):433–450, 1990.\n[66] Aapo Hyv¨arinen and Erkki Oja. Independent component analysis: algorithms and applications. Neural\nNetworks, 13:411–430, 2000.\n[67] Arieh Iserles, Hans Z. Munthe-Kaas, Syvert P. Nørsett, and Antonella Zanna. Lie-group methods. In\nActa numerica, 2000, volume 9 of Acta Numer., pages 215–365. Cambridge Univ. Press, Cambridge,\n2000.\n[68] Kazufumi Ito and Bangti Jin. Inverse Problems - Tikhonov Theory and Algorithms. World Scientiﬁc,\n2014.\n[69] Jrn-Henrik Jacobsen, Arnold W.M. Smeulders, and Edouard Oyallon. i-RevNet: Deep invertible net-\nworks. In International Conference on Learning Representations, 2018.\n[70] Sinisa Todorovic Jun Li, Li Fuxin. Eﬃcient Riemannian optimization on the Stiefel manifold via the\nCayley transform. In ICLR 2020, 2020.\n[71] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adver-\nsarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 4401–4410, 2019.\n[72] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter\nTang. On large-batch training for deep learning: Generalization gap and sharp minima. In ICLR,\n2017.\n[73] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[74] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions.\nIn Advances in Neural Information Processing Systems, pages 10215–10224, 2018.\n[75] Shoshichi Kobayashi and Katsumi Nomizu. Foundations of diﬀerential geometry. Vol. I. Wiley Classics\nLibrary. John Wiley & Sons, Inc., New York, 1996. Reprint of the 1963 original, A Wiley-Interscience\nPublication.\n[76] Risi Kondor, Zhen Lin, and Shubhendu Trivedi. ClebschGordan Nets: a Fully Fourier Space Spherical\nConvolutional Neural Network, 2018.\n[77] Risi Kondor and Shubhendu Trivedi. On the Generalization of Equivariance and Convolution in Neural\nNetworks to the Action of Compact Groups. arxiv:1802.03690, 2018.\n[78] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E. Hinton. ImageNet Classiﬁcation with Deep Convo-\nlutional Neural Networks, 2012.\n[79] Carnegie Mellon University Graphics Lab. Motion capture database. http://mocap.cs.cmu.edu/, 2003.\n[80] Yann LeCun. A theoretical framework for back-propagation. In Proceedings of the 1988 connectionist\nmodels summer school, volume 1, pages 21–28. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988.\n[81] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series, 1998.\n[82] Yann LeCun, Yoshua Bengio, and Geoﬀrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.\nSTRUCTURE PRESERVING DEEP LEARNING\n41\n[83] Yann LeCun, Bernhard Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne Hub-\nbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural\ncomputation, 1(4):541–551, 1989.\n[84] Jun Li, Fuxin Li, and Sinisa Todorovic. Eﬃcient Riemannian Optimization on the Stiefel Manifold via\nthe Cayley Transform. In International Conference on Learning Representations, 2019.\n[85] Qianxiao Li, Long Chen, Cheng Tai, and Weinan E. Maximum principle based algorithms for deep\nlearning. Journal of Machine Learning Research, 18:1–29, 2018.\n[86] Seppo Linnainmaa. The representation of the cumulative rounding error of an algorithm as a taylor\nexpansion of the local rounding errors. Master’s Thesis (in Finnish), Univ. Helsinki, pages 6–7, 1970.\n[87] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond ﬁnite layer neural networks: Bridging\ndeep architectures and numerical diﬀerential equations. In 6th International Conference on Learning\nRepresentations, ICLR 2018 - Workshop Track Proceedings, 2018.\n[88] Chris J Maddison, Daniel Paulin, Yee Whye Teh, Brendan O’Donoghue, and Arnaud Doucet. Hamil-\ntonian descent methods. arXiv preprint arXiv:1809.05042, 2018.\n[89] James Martens. New insights and perspectives on the natural gradient method. arXiv preprint\narXiv:1412.1193, 2014.\n[90] H˚akon Marthinsen and Brynjulf Owren. Geometric integration of non-autonomous linear Hamiltonian\nproblems. Adv. Comput. Math., 42(2):313–332, 2016.\n[91] Stefano Massaroli, Michael Poli, Federico Califano, Angela Faragasso, Jinkyoo Park, Atsushi Ya-\nmashita, and Hajime Asama. Port-Hamiltonian approach to neural network training. arXiv preprint\narXiv:1909.02702, 2019.\n[92] Robert McLachlan and Matthew Perlmutter. Conformal Hamiltonian systems. Journal of Geometry\nand Physics, 39(4):276–300, 2001.\n[93] Robert I. McLachlan and G. Reinout W. Quispel. Splitting methods. Acta Numer., 11:341–434, 2002.\n[94] Robert I. McLachlan, G. Reinout W. Quispel, and Nicolas Robidoux. Geometric integration using\ndiscrete gradients. R. Soc. Lond. Philos. Trans. Ser. A Math. Phys. Eng. Sci., 357(1754):1021–1045,\n1999.\n[95] Klas Modin. Geometry of matrix decompositions seen through optimal transport and information\ngeometry. arXiv preprint arXiv:1601.01875, 2016.\n[96] Andrew Y. Ng. Feature selection, L1 vs. L2 regularization, and rotational invariance. Proceedings of\nthe 21 st International Conference on Machine Learning, 2004.\n[97] Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006.\n[98] Brendan O’Donoghue and Chris J. Maddison. Hamiltonian descent for composite objectives. In Ad-\nvances in Neural Information Processing Systems, pages 14443–14453, 2019.\n[99] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint\narXiv:1301.3584, 2013.\n[100] Philipp Petersen and Felix Voigtlaender. Equivalence of approximation by convolutional neural net-\nworks and fully-connected networks. Proceedings of the American Mathematical Society, 148(4):1567–\n1581, 2019.\n[101] L S Pontryagin. Mathematical Theory of Optimal Processes. Classics of Soviet Mathematics. Taylor\n& Francis, 1987.\n[102] Patrick Putzky and Max Welling. Invert to learn to invert. In Advances in Neural Information Pro-\ncessing Systems 32, pages 446–456. Curran Associates, Inc., 2019.\n[103] Marc’Aurelio Ranzato, Y-Lan Boureau, and Yann Le Cun. Sparse feature learning for deep belief net-\nworks. Advances in Neural Information Processing Systems 20 - Proceedings of the 2007 Conference,\n2009.\n[104] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In ICLR,\n2018.\n[105] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ﬂows. In Pro-\nceedings of the 32nd International Conference on International Conference on Machine Learning -\nVolume 37, ICML15, page 15301538. JMLR.org, 2015.\n[106] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical\nstatistics, pages 400–407, 1951.\n[107] Fabio Rocca, Claudio Maria Prato, and Alessandro Ferretti. An overview of ERS-SAR interferometry.\nIn Proceedings of the 3rdERS Symposium on Space at the Service of our Environment, Florence, Italy,\n1997.\n42\nCELLEDONI, EHRHARDT, ETMANN, MCLACHLAN, OWREN, SCH ¨ONLIEB AND SHERRY\n[108] Lars Ruthotto and Eldad Haber. Deep Neural Networks Motivated by Partial Diﬀerential Equations.\nJournal of Mathematical Imaging and Vision, 2019.\n[109] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-\nrithms. Cambridge university press, 2014.\n[110] Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom Goldstein. Training\nNeural Networks Without Gradients: A Scalable ADMM Approach. In ICML, 2016.\n[111] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoﬀ, and Patrick Riley.\nTensor ﬁeld networks: Rotation- and translation-equivariant neural networks for 3D point clouds.\narxiv:1802.08219, 2018.\n[112] Matthew Thorpe and Yves van Gennip. Deep limits of residual neural networks. arXiv preprint\narXiv:1810.11741, 2018.\n[113] Constantin Udri¸ste. Convex functions and optimization methods on Riemannian manifolds, volume\n297 of Mathematics and its Applications. Kluwer Academic Publishers Group, Dordrecht, 1994.\n[114] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep Image Prior. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 9446–9454, 2018.\n[115] Arjan van der Schaft and Dimitri Jeltsema. Port-Hamiltonian systems theory:\nAn introductory\noverview. Foundations and Trends in Systems and Control, 1(2-3):173–378, 2014.\n[116] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.\nStacked denoising autoencoders: learning useful representations in a deep network with a local denois-\ning criterion. J. Mach. Learn. Res., 11:3371–3408, 2010.\n[117] Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Lu. Stochastic Quasi-Newton Methods for Non-\nconvex Stochastic Optimization. SIAM Journal on Optimization, 27(2):927–956, 2017.\n[118] Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learning Steerable Filters for Rotation Equi-\nvariant CNNs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 849–858, 2018.\n[119] Andreas Weinmann, Laurent Demaret, and Martin Storath. Total variation regularization for manifold-\nvalued data. SIAM J. Imaging Sci., 7(4):2226–2257, 2014.\n[120] Christopher S. Withers and Saralees Nadarajah. log det A = tr log A. International Journal of\nMathematical Education in Science and Technology, 41(8):1121–1124, 2010.\n[121] Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow. Harmonic\nNetworks: Deep Translation and Rotation Equivariance, 2017.\n[122] Yuchen Xie, Richard H. Byrd, and Jorge Nocedal. Analysis of the BFGS Method with Errors. SIAM\nJournal on Optimization, 30(1):182–209, 2020.\n[123] Howard Hua Yang and Shun-ichi Amari. Natural gradient descent for training multi-layer perceptrons.\nSubmitted to IEEE Tr. on Neural Networks, 1997.\n[124] Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. arxiv:1804.10306,\n2018.\n[125] Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods\nfor nonconvex optimization. In Advances in neural information processing systems, pages 9793–9803,\n2018.\n[126] Guodong Zhang, James Martens, and Roger B Grosse. Fast convergence of natural gradient descent\nfor over-parameterized neural networks. In Advances in Neural Information Processing Systems, pages\n8080–8091, 2019.\n[127] Linan Zhang and Hayden Schaeﬀer. Forward Stability of ResNet and Its Variants. Journal of Mathe-\nmatical Imaging and Vision, 62(3):328–351, 2020.\n",
  "categories": [
    "cs.LG",
    "cs.NA",
    "math.NA",
    "stat.ML"
  ],
  "published": "2020-06-05",
  "updated": "2020-06-05"
}