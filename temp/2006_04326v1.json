{
  "id": "http://arxiv.org/abs/2006.04326v1",
  "title": "Semi-Supervised Contrastive Learning with Generalized Contrastive Loss and Its Application to Speaker Recognition",
  "authors": [
    "Nakamasa Inoue",
    "Keita Goto"
  ],
  "abstract": "This paper introduces a semi-supervised contrastive learning framework and\nits application to text-independent speaker verification. The proposed\nframework employs generalized contrastive loss (GCL). GCL unifies losses from\ntwo different learning frameworks, supervised metric learning and unsupervised\ncontrastive learning, and thus it naturally determines the loss for\nsemi-supervised learning. In experiments, we applied the proposed framework to\ntext-independent speaker verification on the VoxCeleb dataset. We demonstrate\nthat GCL enables the learning of speaker embeddings in three manners,\nsupervised learning, semi-supervised learning, and unsupervised learning,\nwithout any changes in the definition of the loss function.",
  "text": "arXiv:2006.04326v1  [eess.AS]  8 Jun 2020\nSemi-Supervised Contrastive Learning with\nGeneralized Contrastive Loss and\nIts Application to Speaker Recognition\nNakamasa Inoue and Keita Goto\nTokyo Institute of Technology, Tokyo, Japan\nE-mail: inoue@c.titech.ac.jp\nAbstract—This paper introduces a semi-supervised contrastive\nlearning framework and its application to text-independent\nspeaker veriﬁcation. The proposed framework employs general-\nized contrastive loss (GCL). GCL uniﬁes losses from two different\nlearning frameworks, supervised metric learning and unsuper-\nvised contrastive learning, and thus it naturally determines the\nloss for semi-supervised learning. In experiments, we applied the\nproposed framework to text-independent speaker veriﬁcation on\nthe VoxCeleb dataset. We demonstrate that GCL enables the\nlearning of speaker embeddings in three manners, supervised\nlearning, semi-supervised learning, and unsupervised learning,\nwithout any changes in the deﬁnition of the loss function.\nI. INTRODUCTION\nWith the development of various optimization techniques,\ndeep learning has become a powerful tool for numerous\napplications, including speech and image recognition. To build\nhigh-performance models, supervised learning is the most\npopular methodology, in which labeled samples are used for\noptimizing model parameters. It is known that deep neural\nnetworks (e.g., ResNet [1]) having more than a million pa-\nrameters outperform hand-crafted feature extraction methods.\nAs such, optimizing parameters with a well-designed objective\nfunction is one of the most important research topics in deep\nlearning.\nIn recent years, supervised metric learning methods for deep\nneural networks have attracted attention. Examples of these\ninclude triplet loss [2] and prototypical episode loss [3], which\npredispose a network to minimize within-class distance and\nmaximize between-class distance. They are also effective for\ntext-independent speaker veriﬁcation, as shown in [4], because\ncosine similarity between utterances from the same speaker is\ndirectly maximized in the training phase.\nNevertheless, unsupervised learning methods have grown\ngreatly, thanks to large-scale collections of unlabeled sam-\nples. Some studies have recently proven that self-supervised\nlearning achieves performance very close to that of supervised\nlearning. For example, the simple framework for contrastive\nlearning of representations (SimCLR) [5] provides superior\nimage representation by introducing contrastive NT-Xent loss\nusing data augmentation on unlabeled images. For speaker\nveriﬁcation, these methods motivate us to explore unsuper-\nvised and semi-supervised ways to learn speaker embeddings\nby effectively using unlabeled utterances.\nIn general, supervised learning and unsupervised learning\ndepend on different methodologies. However, supervised met-\nric learning and unsupervised contrastive learning share a\ncommon idea to maximize or minimize the similarity between\nsamples. This implies the possibility of unifying these two\nlearning frameworks.\nIn this paper, we propose a semi-supervised contrastive\nlearning framework based on generalized contrastive loss\n(GCL). GCL provides a uniﬁed formulation of two different\nlosses from supervised metric learning and unsupervised con-\ntrastive learning. Thus, it naturally works as a loss function\nfor semi-supervised learning. In experiments, we applied the\nproposed framework to text-independent speaker veriﬁcation\non the VoxCeleb dataset. We demonstrated that GCL enables\nthe network to learn speaker embeddings in three manners,\nsupervised learning, semi-supervised learning, and unsuper-\nvised learning, without any changes in the deﬁnition of the\nloss function.\nII. RELATED WORK\nA. Supervised Metric Learning\nSupervised metric learning is a framework to learn a metric\nspace from a given set of labeled training samples. For\nrecognition problems, such as audio and image recognition,\nthe goal is typically to learn the semantic distance between\nsamples.\nA recent trend in supervised metric learning is to design a\nloss function at the top of a deep neural network. Examples\ninclude contrastive loss for Siamese networks [6], triplet loss\nfor triplet networks [2], and episode loss for prototypical\nnetworks [3]. To measure the distance between samples,\nEuclidean distance is often used with these losses.\nFor face identiﬁcation from images, measuring similarity by\ncosine similarity often improves the performance. ArcFace [7],\nCosFace [8], and SphereFace [9] are its popular implementa-\ntions. Their effectiveness is also shown in speaker veriﬁcation\nfrom audio samples with some extended loss deﬁnitions, such\nas ring loss [10], [11]. One of the best choices for speaker\nveriﬁcation is angle-prototypical loss [4], which introduces\ncosine similarity to episode loss, as shown in [4] with thorough\nexperiments.\nB. Unsupervised Contrastive Learning\nUnsupervised learning is a framework to train a model from\na given set of unlabeled training samples. Classic methods for\nunsupervised learning include clustering methods such as K-\nmeans clustering [12]. Most of them are statistical approaches\nwith some objectives based on means and variances.\nRecently, self-supervised learning has proven to be effective\nfor pre-training deep neural networks. For example, Jigsaw\n[13] and Rotation [14] deﬁne a pretext task on unlabeled data\nand pre-train networks for image recognition by solving it.\nDeep InfoMax [15] and its multiscale extension AMDIM [16]\nfocus on mutual information between representations extracted\nfrom multiple views of a context. SimCLR [5] introduces con-\ntrastive learning using data augmentation. The effectiveness\nof contrastive learning is also shown in MoCo V2 [17], [18].\nThese methods achieve performance comparable with that of\nsupervised learning in tasks of image representation learning.\nCross-modal approaches are also effective if more than one\nsource is available. For speaker veriﬁcation, Nagrani et al.\n[19] proposed a cross-modal self-supervised learning method,\nwhich uses face images as supervision of audio signals to\nidentify speakers.\nC. Semi-Supervised Learning\nSemi-supervised learning is a framework to train a model\nfrom a set consisting of both labeled and unlabeled samples.\nTo effectively incorporate information from unlabeled samples\ninto the parameter optimization step, a regularization term\nis often introduced into the objective function. For example,\nconsistency regularization [20] is used to penalize sensitivity\nto augmented unlabeled samples.\nFor speaker veriﬁcation, Stafylakis et al. [21] proposed\nself-supervised speaker embeddings. A pre-trained automatic\nspeech recognition system is utilized to make a supervision\nsignal of phoneme information on unlabeled utterances.\nIII. PRELIMINARY\nA. Supervised Metric Learning\nLet D be a training dataset for supervised learning, which\nconsists of sample pairs x and their discrete class label y.\nThe goal of supervised metric learning is to learn a metric\nfunction d(x, x′), which assigns a small distance between\nsamples belonging to the same class, and relatively large\ndistance between samples from different classes. Assuming\nthat the training phase has iterations for parameter updates,\na mini-batch B is sampled from D at each iteration. For\nconvenience, two-step sampling is often used [4]. First, a\nset of N different classes are randomly sampled from the\nset of training classes. We denote the sampled classes by\ny1, y2, · · · , yN. Second, K independent samples are randomly\nsampled from each of N classes. We denote the samples\nfrom the class yi as x1\ni , x2\ni , · · · , xk\ni . As a result, a mini-batch\nB = {(xk\ni , yi) : i = 1, 2, · · · , N, k = 1, 2, · · · , K} consists\nof NK samples.\nAs an example of supervised metric learning, we show the\ntraining process of a prototypical network [3]. The main idea\nof a prototypical network is to make prototype representations\nof each class and to minimize the distance between a query\nsample and its corresponding prototype. Its loss for parameter\nupdates is computed as follows:\n1) Sample a mini-batch B from D and split it into a query\nset Q = {(x1\ni , yi) ∈B : k = 1} and a support set\nS = {(xk\ni , yi) ∈B : k > 1}.\n2) Extract query representations z1\ni from Q by\nz1\ni = fθ(x1\ni ),\n(1)\nwhere fθ is a neural network for embedding (i.e., a\nnetwork without the ﬁnal loss layer) and θ is a set of\nparameters.\n3) Construct prototype representations z2\ni from S by\nz2\ni =\n1\nK −1\nK\nX\nk=2\nfθ(xk\ni ).\n(2)\n4) From a representation batch Z\n=\n{zk\ni\n:\ni\n=\n1, 2, · · · , N, k = 1, 2}, compute the episode loss deﬁned\nby\nL = −1\nN\nX\ni\ns(z1\ni , z2\ni )\nP\nj s(z1\ni , z2\nj),\n(3)\nwhere s is the exponential function of negative distance\nbetween representations s(z, z′) := exp(−d(z, z′)),\nand d is the squared Euclidean distance.\nB. Unsupervised Contrastive Learning\nLet U be a training dataset for unsupervised learning, which\nconsists of unlabeled samples u. The goal of unsupervised\nlearning is to train networks without any manually attached\nlabels.\nAs an example of unsupervised learning, we show the\ntraining process of SimCLR [5]. SimCLR maximizes the\nsimilarity between representations of two augmented samples\nt1(u) and t2(u), where t1 and t2 are two randomly selected\naugmentation functions. Its loss for parameter updates is\ncomputed as follows:\n1) Sample a mini-batch B = {ui : i = 1, 2, · · · , N} from\nU.\n2) Extract the ﬁrst representation z1\ni by\nz1\ni = fθ(t1(ui)).\n(4)\nNote that t1 is randomly selected from a set of augmen-\ntation functions for each i.\n3) Extract the second representation z2\ni by\nz2\ni = fθ(t2(ui)).\n(5)\n4) From a representation batch Z\n=\n{zk\ni\n:\ni\n=\n1, 2, · · · , N, k = 1, 2}, compute the NT-Xent loss [5]\ndeﬁned by\nLs = 1\n2(ℓ12 + ℓ21),\n(6)\nwhere\nℓ12 = −1\nN\nX\ni\ns(z1\ni , z2\ni )\nP\nj s(z1\ni , z2\nj) + P\nj̸=i s(z1\ni , z1\nj),\n(7)\nℓ21 = −1\nN\nX\ni\ns(z2\ni , z1\ni )\nP\nj s(z2\ni , z1\nj) + P\nj̸=i s(z2\ni , z2\nj).\n(8)\nHere s is the exponential of similarity between represen-\ntations s(z, z′) = exp(cos(gθ′(z), gθ′(z′))/τ), gθ′ is a\nfully connected layer with a parameter θ′, and τ is a\nhyper-parameter.\nWe note that by omitting the second summation in the denom-\ninator of Eq. (7) or (8) we obtain Eq. (3). This opens a way\nto bridge the two losses for supervised metric learning and\nunsupervised contrastive learning.\nIV. PROPOSED METHOD\nThis section presents 1) Generalized contrastive loss (GCL)\nand 2) GCL for semi-supervised learning. GCL uniﬁes losses\nfrom two different learning frameworks, supervised metric\nlearning and unsupervised contrastive learning, and thus it\nnaturally works as a loss function for semi-supervised learning.\nA. Generalized Contrastive Loss\nLet Z = {zk\ni : i = 1, 2, · · · , N, k = 1, 2} be a represen-\ntation batch obtained from a mini-batch for either supervised\nmetric learning or unsupervised contrastive learning (see Step\n4 in Sec. III-A and Sec. III-B). We deﬁne the GCL as\nLα =\n1\n2N\nX\ni,k\nP\nj,l⟨αkl\nij ⟩s(zk\ni , zl\nj)\nP\nj,l |αkl\nij |s(zk\ni , zl\nj) + ǫ,\n(9)\nwhere αkl\nij is a fourth-order afﬁnity tensor, ⟨·⟩denotes the\napplication of Macaulay brackets to the ramp function as\n⟨a⟩= max(0, a), and ǫ ≃0 is a constant to avoid a division\nby zero. Note that a positive value for αkl\nij predisposes two\nrepresentations zk\ni and zl\nj to be close to each other, a negative\nvalue does the opposite. The episode loss can be viewed as a\nspecial case of GCL when Z is made from a mini-batch of\nlabeled samples via prototypes, as shown in Sec. III-A, and\nthe afﬁnity tensor is deﬁned by\nαkl\nij =\n\n\n\n\n\n1\n(k < l, i = j)\n−1\n(k < l, i ̸= j)\n0\n(otherwise)\n.\n(10)\nNote that i is the category index and k is the sample index in\nthis case.\nThe NT-Xent loss can also be viewed as a special case of\nGCL when Z is made from a mini-batch of unlabeled samples\nusing augmentation, as shown in Sec. III-B, and the afﬁnity\ntensor is deﬁned by\nαkl\nij =\n\n\n\n\n\n1\n(k ̸= l, i = j)\n0\n(k = l, i = j)\n−1\n(otherwise)\n.\n(11)\nFig. 1.\nSemi-supervised learning using generalized contrastive loss (GCL).\nFrom a given mini-batch (B0, B1), which includes both labeled and unlabeled\nsamples, a representation batch Z = Z0∪Z1 is constructed. Z0 is constructed\nin the same way as in supervised metric learning, for example, with anchors\nand prototypes. Z1 is constructed in the same way as in unsupervised\ncontrastive learning, for example, with data augmentation functions.\nNote that i is the sample index and k is the augmentation type\nindex in this case.\nOther types of losses, including generalized end-to-end loss\n[22] and angle-prototypical loss [4], can also be obtained by\nchanging the deﬁnitions of Z, α, and s. Note that the complete\ndeﬁnition of GCL includes more instances of metric learning\nmethods, as discussed in the Appendix.\nB. GCL for Semi-Supervised Learning\nIn semi-supervised learning, a training dataset includes both\nlabeled and unlabeled samples. Thus, a mini-batch is given by\na pair B = (B0, B1) of a set of labeled samples B0 and a set of\nunlabeled samples B1. To apply GCL to B, its representation\nbatch is constructed by Z = Z0 ∪Z1, where\nZ0 = {zk\ni|0 : i = 1, 2, · · · , N, k = 1, 2}\n(12)\nis a representation batch of B0 given from a supervised metric\nlearning method and\nZ1 = {zk\ni|1 : i = 1, 2, · · · , N ′, k = 1, 2}\n(13)\nis a representation batch of B1 given from an unsupervised\ncontrastive learning method, as shown in Figure 1.\nThe GCL for semi-supervised learning is then deﬁned on\nZ by\nLα =\nX\ni,k,u\nP\nj,l,v⟨αkl\nij|uv⟩s(zk\ni|u, zl\nj|v)\nP\nj,l,v |αkl\nij|uv|s(zk\ni|u, zl\nj|v),\n(14)\nwhere u, v ∈{0, 1} denote labeled or unlabeled samples.\nNote that afﬁnity tensor αkl\nij|uv becomes a sixth-order tensor\nto predispose similarity between zk\ni|u and zl\nj|v to be close or\nfar.\nHere, we provide an example deﬁnition of αkl\nij|uv for semi-\nsupervised learning. Compared with NT-Xent loss, we relax\nthe afﬁnity between unlabeled samples because some labeled\nTABLE I\nRESULTS OF SEMI-SUPERVISED, UNSUPERVISED, AND SUPERVISED\nLEARNING. EQUAL ERROR RATE (EER) ON THE VOXCELEB 1 TEST IS\nREPORTED.\nMethod\nTrainingScenario\nAdditionalData/Model\nEER(%)\nSSL embedding [21] Semi-supervised\nSpeech recognition\n6.31\nOurs\nSemi-supervised\n-\n6.01\nCross-modal [19]\nUnsupervised\nVideo (face images)\n20.09\nOurs\nUnsupervised\n-\n15.26\nAM-Softmax\nSupervised\n-\n1.81\nOurs\nSupervised\n-\n2.56\nsamples are available for training.\nαkl\nij|00 =\n\n\n\n\n\n1\n(k ̸= l, i = j)\n0\n(k = l, i = j)\n−1\n(otherwise)\n(15)\nαkl\nij|11 =\n\n\n\n\n\n1\n(k ̸= l, i = j)\n0\n(k = l, i = j)\n−1\n(otherwise)\n(16)\nαkl\nij|01 = −1\n(17)\nαkl\nij|10 = −1.\n(18)\nThis deﬁnition is effective for semi-supervised learning for\nspeaker veriﬁcation, where labeled utterances are from a pre-\ndeﬁned set of speakers and unlabeled utterances are from\nanother (different) set of unknown speakers. For the similarity\nmeasure, we use s(z, z′) = exp(γ cos(z, z′) + β). This\ndeﬁnition is used in [4].\nV. EXPERIMENTS\nA. Experimental Settings\nWe used the VoxCeleb dataset [23], [24] for evaluating\nour proposed framework. The training set (voxceleb 2 dev)\nconsists of 1,092,009 utterances of 5,994 speakers. The test set\n(voxceleb 1 test) consists of 37,611 enrollment-test utterance\npairs. The equal error rate (EER) was used as an evaluation\nmeasure.\nFor semi-supervised learning experiments, we randomly\nselected P speakers from the set of 5,994. We used their\nlabeled samples and the remaining unlabeled samples for\ntraining. This is the same evaluation setting proposed in [21].\nFor unsupervised learning experiments, we did not use speaker\nlabels. This evaluation setting is more difﬁcult than the cross-\nmodal self-supervised setting in [25] because we did not use\nvideos (face images) for training. For supervised learning\nexperiments, we used all labeled samples for training. This\nis the ofﬁcial evaluation setting on the VoxCeleb dataset.\nWe used the ResNet18 convolutional network with an input\nof 40-dimensional ﬁlter bank features. For data augmentation\nto construct a representation batch from unlabeled samples,\nwe used four Kaldi data augmentation schemes with the MU-\nSAN (noise, music, and babble) and the RIR (room impulse\nresponse) datasets. For semi-supervised learning, 10 % of\nsamples in each mini-batch were unlabeled and the others were\nlabeled.\nFig. 2.\nResults for semi-supervised experiments. The equal error rate on\nthe VoxCeleb 1 test set is reported. The baseline uses only labeled samples.\nSemi-supervised GCL uses both labeled and unlabeled samples.\nB. Results\nTable I summarizes EERs for semi-supervised, unsuper-\nvised, and supervised learning settings. The results demon-\nstrate that GCL enables the learning of speaker embeddings\nin the three different settings without any changes in the\ndeﬁnition of the loss function.\nFor semi-supervised learning experiments, we compared the\nresults with those of [21] by using the same number of labeled\nspeakers (P = 899). The results show that our framework\nachieves comparable performance. Note that the method in\n[21] uses an automatic speech recognition model pre-trained\non another dataset, but we did not use such pre-trained models.\nComparison with a supervised learning method is shown in\nFigure 2. We see that adding unlabeled utterances improved\nthe performance, in particular when the number of available\nlabeled utterances was small.\nFor unsupervised learning experiments, our method outper-\nformed the cross-modal self-supervised method in [19]. Note\nthat our method did not use any visual information, such\nas face images, for supervision. Audio-visual unsupervised\nlearning with our framework is promising as a next step.\nFor supervised learning experiments, our method achieves\na 2.56 % EER without using data augmentation. However,\nthere is still room for improvement, because training the same\nnetwork with Softmax and AM-Softmax losses (training with\nSoftmax and ﬁne-tuning with AM-Softmax) achieves a 1.81 %\nEER. Introducing a more effective network structure, such\nas ECAPA-TDNN [26] and AutoSpeech-NAS [25], to our\nframework would be also interesting as future work.\nVI. CONCLUSION\nThis paper proposed a semi-supervised contrastive learning\nframework with GCL. We showed via experiments on the\nVoxCeleb dataset that the proposed GCL enables a network\nto learn speaker embeddings in three manners, namely, su-\npervised learning, semi-supervised learning, and unsupervised\nlearning. Furthermore, this was accomplished without making\nany changes to the deﬁnition of the loss function.\nACKNOWLEDGMENT\nThis work was partially supported by the Japan Science\nand Technology Agency, ACT-X Grant JPMJAX1905, and the\nJapan Society for the Promotion of Science, KAKENHI Grant\n19K22865.\nREFERENCES\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep resid-\nual learning for image recognition. In Proceedings of the International\nConference on Computer Vision and Pattern Recognition (CVPR), pages\n770–778, 2016.\n[2] Elad Hoffer and Nir Ailon. Deep metric learning using triplet network.\nIn Proceedings of the International Workshop on Similarity-Based\nPattern Recognition (SIMBAD), pages 84–92, 2015.\n[3] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks\nfor few-shot learning.\nIn Proceedings of the Advances in Neural\nInformation Processing Systems (NeurIPS), pages 4077–4087, 2017.\n[4] Joon Son Chung, Jaesung Huh, Seongkyu Mun, Minjae Lee, Hee Soo\nHeo, Soyeon Choe, Chiheon Ham, Sunghwan Jung, Bong-Jin Lee, and\nIcksang Han.\nIn defence of metric learning for speaker recognition.\narXiv preprint arXiv:2003.11982, 2020.\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.\nA simple framework for contrastive learning of visual representations.\nProceedings of the International Conference on Machine Learning\n(ICML), 2020.\n[6] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction\nby learning an invariant mapping. In Proceedings of the International\nConference on Computer Vision and Pattern Recognition (CVPR), pages\n1735–1742, 2006.\n[7] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface:\nAdditive angular margin loss for deep face recognition. In Proceedings\nof the International Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 4690–4699, 2019.\n[8] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao\nZhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for\ndeep face recognition. In Proceedings of the International Conference\non Computer Vision and Pattern Recognition (CVPR), pages 5265–5274,\n2018.\n[9] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and\nLe Song. Sphereface: Deep hypersphere embedding for face recognition.\nIn Proceedings of the International Conference on Computer Vision and\nPattern Recognition (CVPR), pages 212–220, 2017.\n[10] Yutong Zheng, Dipan K. Pal, and Marios Savvides. Ring loss: Convex\nfeature normalization for face recognition.\nIn Proceedings of the\nInternational Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 5089–5097, 2018.\n[11] Yi Liu, Liang He, and Jia Liu. Large Margin Softmax Loss for Speaker\nVeriﬁcation. In Proceedings of Interspeech, 2019.\n[12] Stuart Lloyd. Least squares quantization in pcm. IEEE Transactions on\nInformation Theory, vol. 28, no. 2, pages 129–137, 1982.\n[13] Mehdi Noroozi and Paolo Favaro.\nUnsupervised learning of visual\nrepresentations by solving jigsaw puzzles.\nIn Proceedings of the\nEuropean Conference on Computer Vision (ECCV), pages 69–84, 2016.\n[14] Spyros Gidaris, Praveer Singh, and Nikos Komodakis.\nUnsupervised\nrepresentation learning by predicting image rotations. In Proceedings of\nthe International Conference on Learning Representations, 2018.\n[15] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan\nGrewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning\ndeep representations by mutual information estimation and maximiza-\ntion.\nIn Proceedings of the International Conference on Learning\nRepresentations, 2019.\n[16] Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning\nrepresentations by maximizing mutual information across views.\nIn\nProceedings of the Advances in Neural Information Processing Systems\n(NeurIPS), pages 15509–15519, 2019.\n[17] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for unsupervised visual representation learning.\narXiv preprint arXiv:1911.05722, 2019.\n[18] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nIm-\nproved baselines with momentum contrastive learning. arXiv preprint\narXiv:2003.04297, 2020.\n[19] Arsha Nagrani, Joon Son Chung, Samuel Albanie, and Andrew Zis-\nserman.\nDisentangled speech embeddings using cross-modal self-\nsupervision. In Proceedings of the International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages 6829–6833, 2020.\n[20] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.\nRegulariza-\ntion with stochastic transformations and perturbations for deep semi-\nsupervised learning. In Proceedings of the Advances in Neural Infor-\nmation Processing Systems (NeurIPS), pages 1163–1171, 2016.\n[21] Themos Stafylakis, Johan Rohdin, Oldrich Plchot, Petr Mizera, and\nLukas Burget.\nSelf-supervised speaker embeddings.\nProceedings of\nInterspeech, 2019.\n[22] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. Gener-\nalized end-to-end loss for speaker veriﬁcation. In Proceedings of the\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 4879–4883, 2018.\n[23] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: A\nlarge-scale speaker identiﬁcation dataset. In Proceedings of Interspeech,\n2017.\n[24] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. Voxceleb2:\nDeep speaker recognition. In Proceedings of Interspeech, 2018.\n[25] Shaojin Ding, Tianlong Chen, Xinyu Gong, Weiwei Zha, and Zhangyang\nWang. Autospeech: Neural architecture search for speaker recognition,\narXiv preprint arXiv:2005.03215, 2020.\n[26] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck. Ecapa-\ntdnn: Emphasized channel attention, propagation and aggregation in tdnn\nbased speaker veriﬁcation. arXiv preprint arXiv:2005.07143, 2020.\nAPPENDIX\nThe complete form of the proposed GCL is deﬁned over\na representation batch Z = {zk\ni\n: i = 1, 2, · · · , N, k =\n1, 2, · · · , N} by\nL = −1\nKN\nX\ni,k\nΨ\n\nX\nj,l\ns(zk\ni , zl\nj; αkl\nij)\n\n,\n(19)\nwhere αkl\nij is an afﬁnity tensor, s(z, z′; α) is the similarity\nbetween z and z′ given an afﬁnity value α, and Ψ is a\nnormalization or clipping function.\nTable II summarizes how to obtain popular loss functions\nfrom GCL. We hope this provides an overview of recent\nprogress and helps other researchers develop new unsuper-\nvised, semi-supervised, and supervised learning methods.\nA. Afﬁnity Type\nFour types of afﬁnity tensor deﬁnitions are used in Table II.\nWith all of them, a positive value for αkl\nij predisposes two\nrepresentations zk\ni and zl\nj to be close to each other, a negative\nvalue does the opposite. The density of αkl\nij increases in the\norder of Types 1 to 4, as shown in Figure 3. Deﬁnitions of\nthe types are given below. Note that K = 2 is assumed for\nsimplicity.\nType 1 makes pairs (z1\ni , z2\nj) and its output is 1 if two\nsamples are from the same class (i.e., i = j) and −1 if two\nsamples are from different classes (i.e., i ̸= j). An example\ndeﬁnition of this type is given by\nαkl\nij =\n\n\n\n\n\n1\n(k < l, i = j)\n−1\n(k > l, i = j −1 mod N)\n0\n(otherwise)\n.\n(20)\nType 2 makes triplets (z1\ni , z2\ni , z2\nj) where i ̸= j. With respect\nto an anchor z1\ni , z2\ni is marked as positive and z2\nj is marked\nTABLE II\nCOMPARISON OF RECENT LOSS DEFINITIONS IN GCL FORMULATION. THE AFFINITY TENSOR MAKES PAIRS, TRIPLETS, (N + 1)-TUPLES, OR\n2N-TUPLES, AS SHOWN IN FIGURE 3. REPRESENTATION BATCH Z IS CONSTRUCTED FROM LABELED SAMPLES, UNLABELED SAMPLES, AND/OR\nPARAMETERS. SEE THE DEFINITION OF GCL IN SEC. IV FOR THE MEANING OF s, ˜α, AND Ψ(v). m IS A MARGIN HYPER-PARAMETER, AND\nM = P\nj,l s(zk\ni , zl\nj; αkl\nij ) + ǫ.\nLoss\nAfﬁnity\nRepresentation Batch Z\nSimilarity s(z, z′; α)\n˜α\nΨ(v)\nContrastive loss [6]\nType 1\nLabeled\nαd(z, z′)\nα\n−(⟨v⟩−χ(v < 0)⟨v + m⟩)/2\nTriplet loss [2]\nType 2\nLabeled\nαd(z, z′)\nα\n−⟨v + m⟩\nArcFace (AAM loss) [7]\nType 3\nLabeled+weights\n|α| exp(cos(∠(z, z′) + m⟨α⟩))\n⟨α⟩\n2v/M\nSphereFace [9]\nType 3\nLabeled+weights\n|α| exp((1 + m⟨α⟩) cos(z, z′))\n⟨α⟩\n2v/M\nCosFace [8]\nType 3\nLabeled+weights\n|α| exp(cos(z, z′) −m⟨α⟩)\n⟨α⟩\n2v/M\nPrototypical episode loss [3]\nType 3\nLabeled\n|α| exp(−d(z, z′))\n⟨α⟩\n2v/M\nAngle-prototypical loss [4]\nType 3\nLabeled\n|α| exp(γ cos(z, z′) + β)\n⟨α⟩\n2v/M\nSimCLR (NT-Xent loss) [5]\nType 4\nUnlabeled\n|α| exp(cos(g(z), g(z′))/τ)\n⟨α⟩\nv/M\nOur experimental setting\nType 4\nLabeled+unlabeled\n|α| exp(γ cos(z, z′) + β)\n⟨α⟩\nv/M\nFig. 3. Four types of the afﬁnity tensor αkl\nij . The values 1 and −1 denote representation pairs predisposed to be close to and far from each other, respectively.\nThe diagonal 0 values are for anchors, and the other 0 values make no restriction on sample pairs.\nas negative. An example deﬁnition of this type is given by\nαkl\nij =\n\n\n\n\n\n1\n(k ̸= l, i = j)\n−1\n(k ̸= l, i = j −1 mod N)\n0\n(otherwise)\n.\n(21)\nType 3 makes (N+1)-tuples (z1\ni , z2\n1, · · · , z2\nN). With re-\nspect to an anchor z1\ni , z2\ni is marked as positive and all the\nothers are marked as negative. The deﬁnition of this type is\ngiven by\nαkl\nij =\n\n\n\n\n\n1\n(k < l, i = j)\n−1\n(k < l, i ̸= j)\n0\n(otherwise)\n.\n(22)\nType 4 makes 2N-tuples (z1\ni , z2\n1, · · · , z2\nN, z1\n1, · · · , z1\ni−1,\nz1\ni+1, z1\nN). With respect to an anchor z1\ni , z2\ni is marked\nas positive and all the others are marked as negative. The\ndeﬁnition of this type is given by\nαkl\nij =\n\n\n\n\n\n1\n(k ̸= l, i = j)\n0\n(k = l, i = j)\n−1\n(otherwise)\n.\n(23)\nB. Representation batch\nTable II gives three types of deﬁnition for the representation\nbatch Z = {zk\ni : i = 1, 2, · · · , N, k = 1, 2}.\nLabeled: With labeled samples for supervised learning, zk\ni\ndenotes the k-th representation from class i. A representation\nzk\ni is deﬁned by sample representation zk\ni = fθ(xk′\ni ) or a\nstatistical representation, such as a mean representation (pro-\ntotype) computed from some samples in B′ ⊂B, speciﬁcally,\nzk\ni\n=\n1\n|B′|\nP\nk′∈B′ fθ(xk′\ni ). Here, B = {(xk′\ni , yi) : i =\n1, 2, · · · , N, k′ = 1, 2, · · · , K′} is a mini-batch of labeled\nsamples.\nLabeled+weights: This type uses parameters as prototypes,\nwhere z1\ni = fθ(x1\ni ) is a representation from class i and z2\ni =\nwi is a weight parameter for class i.\nUnlabeled: With unlabeled samples for unsupervised learn-\ning, zk\ni denotes the representation of the i-th sample with\nthe k-th augmentation. With this type, prototypes can also be\nintroduced in the same way as prototypes are constructed for\nlabeled samples, that is, by taking the mean of representations\nfrom more than one augmentation function.\n",
  "categories": [
    "eess.AS",
    "cs.SD"
  ],
  "published": "2020-06-08",
  "updated": "2020-06-08"
}