{
  "id": "http://arxiv.org/abs/2006.04902v2",
  "title": "What Matters in Unsupervised Optical Flow",
  "authors": [
    "Rico Jonschkowski",
    "Austin Stone",
    "Jonathan T. Barron",
    "Ariel Gordon",
    "Kurt Konolige",
    "Anelia Angelova"
  ],
  "abstract": "We systematically compare and analyze a set of key components in unsupervised\noptical flow to identify which photometric loss, occlusion handling, and\nsmoothness regularization is most effective. Alongside this investigation we\nconstruct a number of novel improvements to unsupervised flow models, such as\ncost volume normalization, stopping the gradient at the occlusion mask,\nencouraging smoothness before upsampling the flow field, and continual\nself-supervision with image resizing. By combining the results of our\ninvestigation with our improved model components, we are able to present a new\nunsupervised flow technique that significantly outperforms the previous\nunsupervised state-of-the-art and performs on par with supervised FlowNet2 on\nthe KITTI 2015 dataset, while also being significantly simpler than related\napproaches.",
  "text": "What Matters in Unsupervised Optical Flow\nRico Jonschkowski1,2, Austin Stone1,2, Jonathan T. Barron2,\nAriel Gordon1,2, Kurt Konolige1,2, and Anelia Angelova1,2\n1Robotics at Google and 2Google AI\n{rjon,austinstone,barron,gariel,konolige,anelia}@google.com\nAbstract. We systematically compare and analyze a set of key compo-\nnents in unsupervised optical ﬂow to identify which photometric loss,\nocclusion handling, and smoothness regularization is most eﬀective. Along-\nside this investigation we construct a number of novel improvements to\nunsupervised ﬂow models, such as cost volume normalization, stopping\nthe gradient at the occlusion mask, encouraging smoothness before up-\nsampling the ﬂow ﬁeld, and continual self-supervision with image resizing.\nBy combining the results of our investigation with our improved model\ncomponents, we are able to present a new unsupervised ﬂow technique\nthat signiﬁcantly outperforms the previous unsupervised state-of-the-\nart and performs on par with supervised FlowNet2 on the KITTI 2015\ndataset, while also being signiﬁcantly simpler than related approaches.\n1\nIntroduction\nOptical ﬂow is a key representation in computer vision that describes the pixel-\nlevel correspondence between two images. Since optical ﬂow is useful for estimating\nmotion, disparity, and semantic correspondence, improvements in optical ﬂow\ndirectly beneﬁt downstream tasks such as visual odometry, stereo depth estima-\ntion, and object tracking. The performance of optical ﬂow techniques has recently\nseen dramatic improvements, due to the widespread adoption of deep learning.\nBecause ground-truth labels for dense optical ﬂow are diﬃcult to obtain for\nreal image pairs, supervised optical ﬂow techniques are primarily trained using\nsynthetic data [5]. Although models trained on synthetic data often generalize\nwell to real images, there is an inherent mismatch between these two data sources\nthat those approaches may struggle to overcome [17,28]\nThough non-synthetic data for training supervised optical ﬂow techniques\nis scarce, the data required to train an unsupervised model is abundant: all\nthat training requires is unlabeled video, of which there are countless hours\nfreely available on the internet. If an unsupervised approach could leverage this\nabundant and diverse real data, it would produce an optical ﬂow model that does\nnot suﬀer from any mismatch between its training data and its test data, and\ncould presumably produce higher-quality results. The core assumption shared\nby unsupervised optical ﬂow techniques is that an object’s appearance does not\nchange as it moves, which allows these models to be trained using unlabeled\nvideo as follows: The model is used to estimate a ﬂow ﬁeld between two images,\narXiv:2006.04902v2  [cs.CV]  14 Aug 2020\n2\nR. Jonschkowski et al.\nthat ﬂow ﬁeld is used to warp one image to match the other, and then the model\nweights are updated so as to minimize the diﬀerence between those two images –\nand to accommodate some form of regularization.\nAlthough all unsupervised optical ﬂow methods share this basic idea, their\ndetails vary greatly. In this work we systematically compare, improve, and\nintegrate key components to further our understanding and provide a uniﬁed\nframework for unsupervised optical ﬂow. Our contributions are:\n1. We systematically compare key components of unsupervised optical ﬂow, such\nas photometric losses, occlusion estimation techniques, self-supervision, and\nsmoothness constraints, and we analyze the eﬀect of other choices, such as\npretraining, image resolution, data augmentation, and batch size.\n2. We propose four improvements to these key components: cost volume normal-\nization, gradient stopping for occlusion estimation, applying smoothness at\nthe native ﬂow resolution, and image resizing for self-supervision.\n3. We integrate the best performing improved components in a uniﬁed framework\nfor unsupervised optical ﬂow (UFlow for short) that sets a new state of the\nart – even compared to substantially more complex methods that estimate\nﬂow from multiple frames or co-train ﬂow with monocular or stereo depth\nestimation. To facilitate future research, our source code is available at https:\n//github.com/google-research/google-research/tree/master/uflow.\n2\nRelated Work\nThe motion between an object and a viewer causes apparent movement of\nbrightness patterns in the image [7]. Optical ﬂow techniques attempt to invert\nthis relationship to recover a motion estimate [16]. Classical methods infer optical\nﬂow for a pair of images by minimizing a loss function that measures photometric\nconsistency and smoothness [10,3,24]. Recent approaches reframe optical ﬂow\nestimation as a learning problem in which a CNN-based model regresses from\na pair of images to a ﬂow ﬁeld [5,11]. Some models incorporate ideas from\nearlier methods, such as cost volumes and coarse-to-ﬁne warping [20,25,32].\nThese supervised approaches require representative training data with accurate\noptical ﬂow labels. Though such data can be generated for rigid objects with\nknown geometry [6,19], recovering this ground truth ﬂow for arbitrary scenes is\nlaborious, and requires approaches as unusual as manually painting scenes with\ntextured ﬂuorescent paint and imaging it under ultraviolet light [1]. Since such\napproaches scale poorly, supervised methods have mainly relied on synthetic\ndata for training, and often for evaluation [2,4,5]. Synthesizing “good” training\ndata (such that learned models generalize to real images) is itself a hard research\nproblem, requiring careful consideration of scene content, camera motion, lens\ndistortion, and sensor degradation [17].\nUnsupervised approaches circumvent the need for labels by optimizing pho-\ntometric consistency with some regularization [22,34], similar to the classical\noptimization-based methods mentioned above. Where traditional methods solve\nan optimization problem for each image pair, unsupervised learning jointly op-\nWhat Matters in Unsupervised Optical Flow\n3\ntimizes an objective across all pairs in a dataset and learns a function that\nregresses a ﬂow ﬁeld from images. This approach has two advantages: 1) inference\nis fast because optimization is only performed during training, and 2) by jointly\noptimizing across the whole train set, information is shared across image pairs\nwhich can potentially improve performance. This unsupervised approach was\nextended to use edge-aware smoothness [30], a bi-directional Census loss [18],\ndiﬀerent forms of occlusion estimation [12,18,30], self-supervision [14,15], and\nestimation from multiple frames [12,15]. Other extensions introduced geometric\nreasoning through epipolar constraints [35] or by co-training optical ﬂow with\ndepth and ego-motion models from monocular [21,33,36] or stereo input [29].\nThese works have pushed the state of the art and generated a range of\nideas for unsupervised optical ﬂow. But since each of them evaluates a diﬀerent\ncombination of ideas, it is unclear how individual ideas compare to each other and\nwhich ideas combine well together. For example, the methods OAFlow [30] and\nDDFlow [14] use diﬀerent photometric losses and diﬀerent ways to mask occlusions,\nand OAFlow uses an edge-aware smoothness loss while DDFlow regularizes\nlearning through self-supervision. DDFlow performs better than OAFlow, but\ndoes this mean that every component of DDFlow is better than every component\nof OAFlow? The ablation studies often presented in these papers show that\neach novel contribution of each work does indeed improve the performance of\neach individual model, but they do not provide a guarantee that each such\ncontribution will always improve performance when added to any other model.\nOur work addresses this problem by systematically comparing and combining\nphotometric losses (L1, Charbonnier [24], Census [14,18,35,36], and structural\nsimilarity [21,29,33]), diﬀerent methods for occlusion estimation [3,30], ﬁrst order\nand second order edge-aware smoothness [27], and self-supervision [14]. Our work\nalso improves cost volume computation, occlusion estimation, smoothness, and\nself-supervision and integrates all components into an state of the art framework\nfor unsupervised optical, while being simpler than many proposed methods to\nform a solid base for future work.\n3\nPreliminaries on Unsupervised Optical Flow\nThe task of estimating optical ﬂow can be deﬁned as follows: Given two color\nimages I(1), I(2) ∈RH×W ×3, we want to estimate the ﬂow ﬁeld V (1) ∈RH×W ×2,\nwhich for each pixel in I(1) denotes the relative position of its corresponding pixel\nin I(2). Note that optical ﬂow is an asymmetric representation of pixel motion:\nV (1) provides a ﬂow vector for each pixel in I(1), but to ﬁnd a mapping from\nimage 2 back to image 1, one would need to estimate V (2).\nIn the context of unsupervised learning, we want to ﬁnd a function V (1) =\nfθ(I(1), I(2)) with parameters θ learned from a set of image sequences D =\n{(I(1), I(2), . . . , I(N))}. Because we lack ground truth ﬂow, we must deﬁne a\nproxy objective L(D, θ), such as photometric consistency between I(1) and\nI(2) after it has been warped according to some estimated V (1). To enforce\nphotometric consistency only for pixels that can be reconstructed from the other\n4\nR. Jonschkowski et al.\nLevel 5\nLevel 4\nLevel 3\nLevel 2\nLevel 1\nImage 1\nImage 2\nFinal flow\nC, F\nW, C, F\nW, C, F\nCN\nFlow\nFlow\nFlow\nFlow\nW, C, F\nCost volume comp.\nWarping\nFlow estimation\nFeatures 1\nFeatures 2\nFlow\nLevel + 1\nContext\nLevel - 1\n+\nFig. 1. Model overview. Left: Feature pyramids feed into a top-down ﬂow estimation.\nRight: A zoomed in view on a “W, C, F” (warping, cost volume, ﬂow estimation) block\nimage, we must also estimate an occlusion mask O(1) ∈RH×W , for example\nbased on the estimated forward and backward ﬂow ﬁelds O(1) = g(V (1), V (2)).\nL(·) might also include other terms for, e.g. for smoothness or self-supervision. If\nL(·) is diﬀerentiable with respect to θ, the parameters that minimize this loss\nθ∗= arg min(L(D, θ)) can be recovered using gradient-based optimization.\n4\nKey Components of Unsupervised Optical Flow\nThis section compares and improves key components of unsupervised optical ﬂow.\nWe will ﬁrst discuss a model fθ(·), which we base on PWC-Net [25], and improve\nthrough cost-volume normalization. Then we go through diﬀerent components\nof the objective function L(·): occlusion-aware photometric consistency, smooth-\nness, and self-supervision. Here, we propose improvements to each component:\nstopping the gradient at the occlusion mask, computing smoothness at the native\nﬂow resolution, and image resizing for self-supervision. We end this section by\ndiscussing data augmentation and optimization.\nAs shown in Fig. 1, our model feeds images I(1) and I(2) into a shared CNN\nthat generates a feature pyramid, where features are used as input for warping\n(W), cost volume computation (C), and ﬂow estimation (F). At each level ℓ, the\nestimated ﬂow V (1,ℓ+1) from the level above is upscaled, passed down to the\nlower level as ˆV (1,ℓ+1), and then used to warp F (2,ℓ), the features of image 2. The\nwarped features w(F (2,ℓ), ˆV (1,ℓ)) together with F (1,ℓ) are used to compute a cost\nvolume. The cost volume considers feature correlations for all pixels and all 81\ncombinations of shifting w(F (2,ℓ), ˆV (1,ℓ)) up to 4 pixels up/down and left/right.\nThis results in a cost volume Cℓ∈R\nW\n2ℓ× H\n2ℓ×81 that describes how closely each\npixel in F (1,ℓ) resembles the 81 pixels around its location in F (2,ℓ). The cost\nvolume, the features from image 1, the higher level ﬂow and the context – the\noutput of the second to last layer of the ﬂow estimation network – are fed into a\nCNN that estimates a ﬂow V (1,ℓ). After a number of ﬂow estimation levels, there\nis a ﬁnal stage of ﬂow reﬁnement at level two in which the ﬂow and context are\nfed into a context network (CN), which is a stack of dilated convolutions.\nModel Shrinking, Level Dropout and Cost Volume Normalization: PWC-Net\nwas designed for supervised learning of optical ﬂow [25]. To deal with increased\nWhat Matters in Unsupervised Optical Flow\n5\nmemory requirements for unsupervised learning due to bi-directional losses,\nocclusion estimation, and self-supervision, we remove level six, use 32 channels\nin all levels, and add residual connections to all ﬂow estimation modules (the “+”\nin the bottom right of Fig. 1). Additionally, we dropout residual ﬂow estimation\nat all levels to further regularize learning, i.e. we randomly pass the resized and\nrescaled ﬂow estimate from the level above directly to the level below.\nAnother diﬀerence when using this model for unsupervised rather than super-\nvised learning is that unsupervised losses are typically only imposed on the ﬁnal\noutput (presumably because photometric consistency and other objectives work\nbetter at higher resolutions). But without supervised losses on intermediate ﬂow\npredictions, the model has diﬃculty learning ﬂow estimation at higher levels. We\nfound that this is caused by very low values in the estimated cost volumes as a\nresult of vanishing feature activations at higher levels.\nWe address this problem by cost volume normalization. Let us denote features\nfor image i at level ℓas F (i,ℓ) ∈R\nH\n2ℓ× W\n2ℓ×d. The cost volume between images 1\nand 2 for all image locations (x, y) and all considered image shifts (u, v) is the\ninner product of the normalized features of the two images:\nC(ℓ)\nx,y,u,v =\nX\nd\n \nF (1,ℓ)\nx,y,d −µ(1,ℓ)\nσ(1,ℓ)\n!  \nF (2,ℓ)\nx+u,y+v,d −µ(2,ℓ)\nσ(2,ℓ)\n!\n.\n(1)\nWhere µ(i,ℓ) and σ(i,ℓ) are the sample mean and standard deviation of F (i,ℓ) over\nits spatial and feature dimensions. We found that cost volume normalization\nimproves convergence and ﬁnal performance in unsupervised optical ﬂow. These\nﬁndings are consistent with prior work that used a similar form of normalization\nto improve geometric matching [23].\nUnsupervised Learning Objectives: Deﬁning a learning objective L(·) that\nspeciﬁes the task of learning optical ﬂow without having access to labels is the\ncore problem of unsupervised optical ﬂow. Similar to related work [14,15,18,30],\nwe train our model by estimating optical ﬂow and applying the respective losses\nin both directions. In this work we consider a learning objective that consists of\nthree terms: occlusion-aware photometric consistency, edge-aware smoothness,\nand self-supervision, which we will now discuss in detail.\nPhotometric Consistency: The photometric consistency term encourages the esti-\nmated ﬂow to align image patches with a similar appearance by penalizing photo-\nmetric dissimilarity. The metric for measuring appearance similarity is critical for\nany unsupervised optical ﬂow technique. Related approaches use three diﬀerent\nobjectives here (sometimes in combination), (i) the generalized Charbonnier\nloss [12,18,22,30,34,35], (ii) the structural similarity index (SSIM) loss [33,21,29],\nand (iii) the Census loss [18,35,36]. We compare all three losses in this paper.\nThe generalized Charbonnier loss [24] is LC = 1\nn\nP \u0000(I(1) −w(I(2))2 + ϵ2\u0001α. Our\nexperiments use ϵ = 0.001 and α = 0.5 and also compare to using a modiﬁed L1\nloss LL1 = P |I(1) −w(I(2)) + ϵ′| with ϵ′ = 10−6. For the SSIM [31] loss, we use\nan occlusion-aware implementation from recent work [9]. For the Census loss, we\n6\nR. Jonschkowski et al.\nuse a soft Hamming distance on Census-transformed image patches [18]. Based\non the empirical results discussed below, we use the Census loss unless otherwise\nstated. All photometric losses are computed using an occlusion-masked average\nover all pixels [30].\nOcclusion Estimation: By deﬁnition, occluded regions do not have a correspon-\ndence in the other image, so they should be discounted when computing the\nphotometric loss. Related approaches estimate occlusions by (i) checking for con-\nsistent forward and backward ﬂow [30], (ii) using the range map of the backward\nﬂow [3], and (iii) learning a model for occlusion estimation [12]. We are considering\nand comparing the ﬁrst two variants here and improve the second variant through\ngradient stopping. In addition to taking into account occlusions, we also mask\n“invalid” pixels whose ﬂow vectors point outside of the frame of the image [30]. The\nforward-backward consistency check deﬁnes occlusions a pixels for which the ﬂow\nand the back-projected backward ﬂow disagree by more than a threshold, such that\nthe occlusion mask is deﬁned as O(1) = 1|V (1)−w(V (2))|2<α1(|V (1)|2−|w(V (2))|2)+α2,\nwhere α1 = 0.01 and α2 = 0.5 [26]. An alternative approach computes a “range\nmap” R(i) ∈RH×W – a soft histogram of how many pixels in the other image\nmap onto a given pixel, which is constructed by having each ﬂow vector distribute\na total weight of 1 to the four pixels around its end point according to a bilinear\nkernel [30]. Pixels that none of the reverse ﬂow vectors point to are assumed\nto have no correspondence in the other image, and are therefore occluded. As\nproposed by Wang et al. [30], we compute an occlusion mask O(i) ∈RW ×H by\nthresholding the range map at 1. Based on the empirical results below, we use\nrange-map based occlusion estimation by default, but use the forward-backward\nconsistency check on KITTI, where it signiﬁcantly improves performance.\nGradient Stopping at Occlusion Masks: Although prior work does not mention\nthis issue [30], we found that propagating the gradient of the photometric loss into\nthe occlusion estimation consistently degraded performance or caused divergence\nwhen the occlusion estimation was diﬀerentiable, as is the case for range-map\nbased occlusion. This behavior is to be expected because when computing the\nocclusion-weighted average over photometric dissimilarity, there should be a gradi-\nent towards masking pixels with high photometric error. We address this problem\nby stopping the gradient at the occlusion mask, which eliminates divergence and\nimproves performance.\nSmoothness: Diﬀerent forms of smoothness are commonly used to regularize\noptical ﬂow in traditional methods [3,10,24] as well as most recent unsupervised\napproaches [34,22,30,18,33,36,21,12,29,35]. In this work, we consider edge-aware\nﬁrst and second order smoothness [27], where ﬂows are encouraged to align their\nboundaries with visual edges in the image I(1). Formally, we deﬁne kth order\nsmoothness as:\nLsmooth(k) = 1\nn\nP exp\n\u0010\n−λ\n3\nP\nc\n\f\f\f ∂I(1,ℓ)\nc\n∂x\n\f\f\f\n\u0011 \f\f\f ∂kV (1,ℓ)\n∂xk\n\f\f\f + exp\n\u0010\n−λ\n3\nP\nc\n\f\f\f ∂I(1,ℓ)\nc\n∂y\n\f\f\f\n\u0011 \f\f\f ∂kV (1,ℓ)\n∂yk\n\f\f\f . (2)\nWhere λ modulates edge weighting based on I(1,ℓ)\nc\nfor color channel c ∈[0, 2]. By\ndefault, we use ﬁrst order smoothness on Flying Chairs and Sintel and second\norder smoothness on KITTI, which we ablate in diﬀerent experiments.\nWhat Matters in Unsupervised Optical Flow\n7\nSmoothness at Flow Resolution: A question that we have not seen addressed is\nat which level ℓ, smoothness should be applied. Since we follow the commonly\nused method of estimating optical ﬂow at ℓ= 2, i.e. at a quarter of the input\nresolution, followed by upsampling through bilinear interpolation, our model\nproduces piece-wise linear ﬂow ﬁelds. As a result, only every fourth pixel can\npossibly have a non-zero second order derivative, which might not be aligned with\nthe corresponding image edge and thereby reduce the eﬀectiveness of edge-aware\nsmoothness. To address this, we apply smoothness at level ℓ= 2 where ﬂow is\ngenerated and downsample the image instead of upsampling the ﬂow. This of\ncourse does not aﬀect evaluation, which is done at the original image resolution.\nSelf-supervision: The idea of self-supervision in unsupervised optical ﬂow is to\ngenerate optical ﬂow labels by applying the learned model on a pair of images,\nthen modify the images to make ﬂow estimation more diﬃcult and train the model\nto recover the originally estimated ﬂow [14,15]. Since we see the main utility of\nthis technique in learning ﬂow estimation for pixels that go out of the image\nboundary – where cost-volume computation is not informative and photometric\nlosses do not apply – we build on and improve ideas about self-supervised image\ncrops [14]. For our self-supervised objective, we apply our model on the full\nimages, crop the images by removing 64 pixels from each edge, apply the model\nagain, and use the cropped estimated ﬂow from the full images as supervision for\nﬂow estimation from the cropped images. We deﬁne the self-supervision objectives\nas an occlusion-weighted Charbonnier loss, that takes into account only pixels\nthat have low forward-backward consistency in the “student” ﬂow from cropped\nimage and high forward-backward consistency in the “teacher” ﬂow from the\noriginal images, similar to DDFlow [14].\nContinual Self-supervision and Image Resizing: Unlike related work, we do not\nﬁrst train and then freeze a teacher model to supervise a separate student\nmodel but rather have a single model that supervises itself, which simpliﬁes\nthe approach, reduces the required memory, and allows the self-supervision\nsignal to improve continually. To stabilize learning, we stop gradients of the\nself-supervision objectives to be propagated into the “teacher” ﬂow. Additionally,\nwe resize the image crops to match the original resolution before feeding them\ninto the model (and we rescale the self-generated ﬂow labels accordingly) to make\nthe self-supervision examples more representative of the problem of extrapolating\nﬂow beyond the image boundary in the original size.\nOptimization: To train our model fθ(·) we minimize a weighted sum of losses:\nL(D, θ) = wphoto · Lphoto + wsmooth · Lsmooth + wself · Lself ,\n(3)\nwhere Lphoto is our photometric loss, Lsmooth is smoothness regularization, and\nLself is the self-supervision Charbonnier loss. We set wphoto to 1 for experiments\nusing the Census loss and to 2 when we compare to the SSIM, Charbonnier, or\nL1 losses. We set wself to 2 when using ﬁrst order, and to 4 for second order\nsmoothness and use an edge-weight of λ = 150. We use wself = 0 during the ﬁrst\n8\nR. Jonschkowski et al.\nhalf of training, linearly increase it 0.3 during the next 10% of gradient steps and\nkeep it constant afterwards.\nRGB image values are scaled to [−1, 1], and augmentated by randomly\nswapping the color channels and randomly shifting the hue. Sintel images are\nadditionally randomly ﬂipped up/down and left/right. All models are trained\nusing with Adam [13] (β1 = 0.9, β2 = 0.999, ϵ = 10−8) with a learning rate of\n10−4 for m steps, followed by another 1\n5m steps during which the learning rate is\nexponentially decayed to 10−8. All ablations use m = 50K with batch size 32,\nbut the ﬁnal model was trained using m = 1M with batch size 1, which produced\nslightly better performance as described below. Either way, the training takes\nabout three days. Experiments on Sintel and KITTI start from a model that was\nﬁrst trained on Flying Chairs.\n5\nExperiments\nWe evaluate our model on the standard optical ﬂow benchmark datasets: Flying\nChairs [5], Sintel [4], and KITTI 2012/2015 [6,19]. We divide Flying Chairs and\nSintel according to its standard train/test split. For KITTI, we train on the\nmulti-view extension on the KITTI 2015 dataset, and we do not train on any\ndata from KITTI 2012 because it does not have moving objects.\nRelated work is inconsistent in their use of train/test splits. For Sintel, it is\ncommon to train on the training set, report the benchmark performance on the\ntest set, and evaluate ablations on the training set only (because test set labels\nare not public), which does not test generalization very well. Others “download\nthe Sintel movie and extract ∼10,000 images” [15] including the test set images,\nwhich is intended to demonstrate the ability of unsupervised methods to train\non raw video data, but unfortunately also includes the benchmark test images in\nthe training set. For KITTI, other works train on the raw KITTI dataset with\nand without excluding the evaluation set, or most commonly train on frames\n1–8 and 13–20 of the multi-view extension of KITTI 2012/2015 datasets and\nevaluate on frames 10/11. But this split can mask overﬁtting to the trained\nsequences – either in the ablation results or also in the benchmark results, when\nthe multiview-extensions of both the train and the test set are used. We therefore\nadopt the training regimen of Zhong et al. [35] and train two models for each\ndataset, one on the training set and one on test set (or for KITTI on their\nmultiview extension) and evaluate these models appropriately.\nFollowing the conventions of the KITTI benchmark, we report endpoint error\n(“EPE”) and error rates (“ER”), where a prediction is considered erroneous if\nits EPE is > 3 pixels and if the distance between the predicted point and the\ntrue end point is > 5% of the length of the true ﬂow vector. We compute these\nmetrics for all pixels (“occ” in the KITTI benchmark, which we call “all” in\nthis paper). We use the common practice of pretraining on the train split of\nthe Flying Chairs dataset before training on Sintel / KITTI. We evaluate on\nall images in the native resolution, but have the model perform inference on\na resolution that is divisible by 32, output at a four times smaller resolution,\nWhat Matters in Unsupervised Optical Flow\n9\nand then resize the output to the original resolution for evaluation. On KITTI,\nwe observe that performance improves when using a square input resolution\ninstead of a resolution in the original aspect ratio – perhaps because KITTI is\ndominated by horizontal motion. Accordingly, we use the following resolutions in\nour experiments: Flying Chairs: 384×512, Sintel: 448×1024, KITTI: 640×640.\n6\nResults\nWe evaluate our model in an extensive comparison and ablation study, from\nwhich we identify the best combination of components, tested in the “full” setting,\nwhich is often diﬀerent from the components that work best individually in our\n“minimal” setting (more details below). We then compare our resulting model\nto the best published methods on unsupervised optical ﬂow, and show that it\noutperforms all methods on all benchmarks.\nAblations and Comparisons of Key Components To determine which\naspects of unsupervised optical ﬂow are most important, we perform an extensive\nseries of ablation studies. We ﬁnd that a) occlusion-masking, self-supervision, and\nsmoothness are all important, b) level dropout and cost volume normalization\nimprove performance, c) the Census loss outperforms other photometric losses,\nd) range-map based occlusion estimation requires gradient stopping to work,\nc) edge-aware smoothness and smoothness level matters signiﬁcantly, d) self\nsupervision helps especially for KITTI, and is improved by our changes, e) losses\nmight be the current performance bottleneck, f) changing the resolution can\nsubstantially improve results, g) data augmentation and pretraining are helpful.\nIn each ablation study we train one model per domain (on Flying Chairs,\nKITTI-test, and Sintel-test), and evaluate those on the corresponding validation\nsplit from the same domain, taking into account occluded and non-occluded\npixels “(all)”. To estimate the noise in our results, we trained models with six\ndiﬀerent random seeds for each domain and computed their standard deviations\nper metric: Flying Chairs: 0.0162, Sintel Clean: 0.0248, Sintel Final: 0.0131,\nKITTI-2015: 0.0704, 0.0718%. We now describe the ﬁndings of each study.\nTable 1. Core components: OM: oc-\nclusion masking, SM: smoothness, SS:\nself-supervision; “div.”: divergence\nChairs\nSintel train\nKITTI-15 train\nOM SM SS\ntest\nClean Final\nall\nnoc\nER%\n–\n–\n–\n3.58\n4.20\n6.80\n13.07 2.47 21.21\n–\n–\n✓\n2.99\n3.34\n5.18\n11.36 2.30 18.61\n–\n✓\n–\n2.84\n3.37\n5.19\n11.37 2.17 19.31\n–\n✓\n✓\n2.74\n3.12\n4.56\n3.28\n2.08\n9.97\n✓\n–\n–\n3.28\n3.78\n5.85\ndiv.\ndiv.\ndiv.\n✓\n–\n✓\n2.91\n3.26\n4.72\n3.02\n2.11\n9.89\n✓\n✓\n–\n2.63\n3.20\n4.63\n4.15\n2.05 13.15\n✓\n✓\n✓\n2.55\n3.00\n4.18\n2.94 1.98 9.65\nCore Components: Table 1 shows how per-\nformance varies as each core component\nof our model (occlusion masking, smooth-\nness, and self-supervision) is removed. We\nsee that every component contributes to\nthe overall performance. Since the utility\nof diﬀerent components depends on what\nother components are used, all following\nexperiments compare to the “minimal”\n(ﬁrst row) and “full” (last row) versions of\nour method. Qualitative results for rows\n9, 4, 3, and 1 are shown in Figure 2 (from left to right). Note how the ﬂow error\n∆V increases with each removal of a core component.\n10\nR. Jonschkowski et al.\n2015\nI\n∆V\nKITTI\nV ∗\nV\nFinal\nI\n∆V\nSintel\nV ∗\nV\nGround truth\nFull\n−Occlusion\n−Self-supervision\n−Smoothness\nFig. 2. Qualitative ablation results of our model on random images not seen during\ntraining. Flow quality deteriorates as we progressively ablate core components\nTable 2. Model improvements. CVN:\ncost volume normalization, LD: level\ndropout\nChairs\nSintel train\nKITTI-15 train\nCVN\nLD\ntest\nClean Final\nall\nnoc\nER%\nMinimal\n–\n–\n5.01\n4.52\n6.67\n13.30\n2.72\n21.69\n–\n✓\n5.29\n4.40\n6.59\n12.75\n2.49\n21.30\n✓\n–\n4.86\n4.19\n6.69\n13.294\n2.59\n21.54\n✓\n✓\n3.58\n4.20\n6.80\n13.07\n2.47 21.21\nFull\n–\n–\n3.78\n3.41\n4.70\n39.09\n30.19 98.77\n–\n✓\n3.21\n3.45\n4.61\n2.96\n1.96\n9.77\n✓\n–\n2.54\n3.07\n4.31\n3.16\n2.04\n10.35\n✓\n✓\n2.55\n3.00\n4.18\n2.94\n1.98\n9.65\nTable 3. Photometric losses. Best re-\nsults of L1 and Charbonnier underlined\nChairs\nSintel train\nKITTI-15 train\nMethod\ntest\nClean Final\nall\nnoc\nER%\nMinimal\nL1\n4.27\n5.51\n7.74\n17.02\n6.11\n32.96\nCharbonnier\n4.31\n5.50\n7.64\n16.94\n6.09\n32.84\nSSIM\n3.51\n4.01\n5.41\n11.99\n2.46\n21.72\nCensus\n3.54\n4.23\n6.98\n11.66 2.37 21.15\nFull\nL1\n2.83\n4.23\n5.75\n5.53\n3.17\n18.65\nCharbonnier\n2.86\n4.24\n5.81\n5.56\n3.21\n18.82\nSSIM\n2.54\n3.08\n4.52\n3.29\n2.04\n10.41\nCensus\n2.61\n3.00\n4.20\n3.08\n2.01 10.01\nTable 4. Occlusion estimation. RM:\nrange-map\nbased\noccllusion,\nFB:\nforward-backward consistency check\nChairs\nSintel train\nKITTI-15 train\nMethod\ntest\nClean Final\nall\nnoc\nER%\nMinimal\nNone\n3.51\n4.15\n6.69\n12.89 2.41\n21.17\nRM (w/o grad stop)\ndiv.\ndiv.\ndiv.\ndiv.\ndiv.\ndiv.\nRM (w/ grad stop)\n3.27\n3.78\n5.86\n10.65 2.29\n18.76\nFB (from step 1)\n3.57\n3.71\n4.83\n8.99\n2.16 17.71\nFB (after 20% steps)\n3.49\n3.76\n4.92\n9.75\n2.13 18.38\nFull\nNone\n2.73\n3.84\n5.13\n3.28\n2.10\n10.07\nRM (w/o grad stop)\ndiv.\ndiv.\ndiv.\ndiv.\ndiv.\ndiv.\nRM (w/ grad stop)\n2.58\n3.01\n4.25\n3.10\n2.04\n9.86\nFB (from step 1)\n3.28\n3.49\n4.45\n2.96\n1.99\n9.65\nFB (after 20% steps)\n3.14\n3.12\n4.13\n2.88 1.95\n9.54\nModel Improvements: Table 2 shows that\nlevel dropout (LD) and cost volume nor-\nmalization (CVN) improve performance\nin the full setting (but not generally in\nthe minimal setting). CVN appears to\nbe more important for Chairs and Sintel\nwhile LD helps most for KITTI.\nPhotometric Losses: Table 3 compares\ncommonly used photometric losses and\nshows that it is important to test every\ncomponent with the full method, rather\nthan looking at isolated performance. By\nitself, the commonly-used Charbonnier\nloss works better, but in the full setting,\nit underperforms the simpler L1 loss. For\nKITTI, Census works best in both set-\ntings. But for Sintel (in particular Sintel\nFinal), the SSIM loss signiﬁcantly out-\nperforms Census in the minimal setting\n(5.41 vs. 6.98) but does not perform as\nwell when used with all components in\nthe full setting.\nOcclusion Estimation: Table 4 compares\ndiﬀerent approaches to occlusion estima-\ntion (forward-backward consistency and\nrange maps). We see that range-map\nbased occlusion consistently diverges un-\nless we stop the gradient of the photomet-\nric loss. But when gradients are stopped,\nthis method works well, especially for Fly-\ning Chairs and Sintel Clean. Forward-backward consistency works best for KITTI,\nespecially if not applied from the beginning.\nWhat Matters in Unsupervised Optical Flow\n11\nTable 5. Level for smoothness loss\nSmoothn.\nChairs\nSintel train\nKITTI-15 train\nlevel\ntest\nClean Final\nall\nnoc\nER%\nMinimal\n0\n3.05\n4.10\n5.22\n12.16\n2.32\n20.33\n1\n2.94\n3.65\n5.07\n11.94\n2.24\n19.98\n2\n2.85\n3.33\n5.21\n11.43 2.23 19.38\nFull\n0\n2.87\n3.65\n4.63\n2.95\n2.02\n9.87\n1\n2.74\n3.13\n4.29\n2.96\n1.99\n9.78\n2\n2.58\n3.00\n4.24\n2.93\n1.99\n9.63\nTable 6. Comparison of weights for\nﬁrst/second order smoothness\nwsmooth\nChairs\nSintel train\nKITTI-15 train\n1st 2nd\ntest\nClean Final\nall\nocc\nER%\nMinimal\n0\n0\n4.55\n4.16\n6.84\ndiv.\ndiv.\ndiv.\n0\n2\n3.13\n3.77\n6.32\n11.37 2.17\n19.33\n0\n8\n4.02\n3.50\n6.08\n7.27 2.11 14.70\n4\n0\n2.85\n3.35\n5.05\n7.23\n2.30\n18.58\n16\n0\n4.37\n4.78\n6.03\n9.58\n4.09\n22.82\nFull\n0\n0\n2.92\n3.27\n4.77\n2.92\n2.07\n9.75\n0\n2\n2.79\ndiv.\ndiv.\n2.93\n1.98\n9.61\n0\n8\n2.75\n3.33\n4.77\n2.94\n1.91\n9.85\n4\n0\n2.60\n3.00\n4.17\n5.39\n2.03\n16.58\n16\n0\n3.68\n4.22\n5.30\n8.71\n4.01\n21.52\nTable 7. Smoothness edge-weights\nChairs\nSintel train\nKITTI-15 train\nλ\ntest\nClean Final\nall\nnoc\nER%\nMinimal\n0\n4.93\n6.00\n6.65\n4.15\n2.36\n12.50\n10\n4.33\n5.32\n6.12\n4.22\n2.17 12.28\n150\n2.83\n3.36\n5.12\n11.41 2.21\n19.37\nFull\n0\n4.87\n5.78\n6.40\n3.86\n2.84\n11.81\n10\n3.75\n4.62\n5.34\n3.14\n2.11\n10.27\n150\n2.56\n3.02\n4.20\n2.87 1.95\n9.59\nTable 8. Self-supervision ablation\nChairs\nSintel train\nKITTI-15 train\nSelf-supervision\ntest\nClean Final\nall\nnoc\nER%\nMinimal\nNone\n3.48\n4.10\n6.62\n13.05 2.48\n21.23\nNo resize\n3.16\n3.53\n5.67\n12.87 2.35\n20.22\nFrozen teacher\n3.10\n3.36\n5.24\n8.11 2.38 13.90\nDefault\n2.99\n3.34\n5.18\n11.36 2.30\n18.61\nFull\nNone\n2.67\n3.18\n4.60\n4.10\n2.02\n12.95\nNo resize\n2.51\n3.14\n4.48\n3.53\n2.02\n11.13\nFrozen teacher\n2.66\n3.04\n4.24\n2.99\n1.99\n9.70\nDefault\n2.61\n2.99\n4.23\n2.86 1.95\n9.57\nTable 9. Losses on Sintel for zero ﬂow,\nground truth ﬂow, and predicted ﬂow\nL1\nSSIM\nCensus\nSM\nCensus + SM\nFlow\nnoc\nall\nnoc\nall\nnoc\nall\nall\nnoc\nall\nClean\nZero\n.146\n.161\n.927\n.946\n3.160\n3.193\n0.\n3.160\n3.193\nGT\n.031 .052\n.191 .241\n2.041 2.122\n.032\n2.073 2.154\nUFlow\n.031 .042\n.203\n.247\n2.06\n2.130\n.024\n2.085\n2.154\nFinal\nZero\n.126\n.142\n.731\n.751\n3.037\n3.075\n0.\n3.037\n3.075\nGT\n.034\n.055\n.185\n.233\n2.086\n2.154\n.063\n2.149\n2.217\nUFlow\n.032 .037\n.167 .226\n2.044 2.091\n.045\n2.089 2.136\nSmoothness: Prior work suggests that\nphotometric and smoothness losses taken\ntogether work better at higher resolu-\ntions [8]. But our analysis of the smooth-\nness loss alone shows an advantage of ap-\nplying this loss at the resolution of ﬂow\nestimation, rather than at the image reso-\nlution, in particular for Flying Chairs and\nSintel (Table 5). Our results also show\nthat ﬁrst order smoothness works bet-\nter on Chairs and Sintel while second\norder smoothness works better on KITTI\n(Table 6). We see that context is impor-\ntant because in the minimal setting, the\nbest second order smoothness weight for\nKITTI is 8, but in the full setting, it\nis 2. Comparing diﬀerent edge-weights λ\n(Eq. 2) in Table 7, we see that nonzero\nedge-weights improve performance, par-\nticularly in the full setting. To our sur-\nprise, the simple strategy of only opti-\nmizing the Census loss and second or-\nder smoothness without edge-awareness,\nocclusion, or self-supervision (ﬁrst row)\nproduces performance on KITTI that im-\nproves on previous the state of the art.\nSelf-Supervision: In Table 8 we ablate the\nuse of self-supervision and our proposed\nchanges, and conﬁrm that self-supervision\non image crops is instrumental in achiev-\ning good results on KITTI, where er-\nrors are dominated by fast motion near\nthe image edges. We also see that self-\nsupervision is most eﬀective when the\nimage crop is resized as proposed by our\nmethod. Freezing the teacher network,\nas done in other works, seems to be im-\nportant only when not using the other\nregularizing components. With these com-\nponents in place, sharing the same model\nfor both student and teacher appears to\nbe beneﬁcial.\nLoss Comparison to Ground Truth: Photometric loss functions used in unsu-\npervised optical ﬂow rely on the brightness consistency assumption: that pixel\nintensities in the camera image are invariant to motion in the world. But pho-\n12\nR. Jonschkowski et al.\ntometric consistency is an imperfect indicator of ﬂow quality (e.g. in regions of\nshadows and specularity). To analyze this issue, we compute photometric and\nsmoothness losses not only for the ﬂow ﬁeld produced by our model, but also\nfor a ﬂow ﬁeld ﬁlled with zeros and for the ground truth ﬂow. Table 9 shows\nthat our model is able to achieve comparable or better photometric consistency\n(and overall loss) than the ground truth ﬂow. This trend is more pronounced on\nSintel Final, which we believe violates the consistency assumption more than\nSintel Clean. This result suggests that the loss functions currently used may be\na limiting factor in unsupervised methods.\nTable 10. Resolution\nKITTI-15 train\nResolution\nall\nnoc\nER%\nMin.\n384×1280\n13.25 2.79 21.38\n640×640\n12.91 2.42 21.17\nFull\n384×1280\n3.80\n2.13 10.88\n640×640\n2.93 1.96 9.61\nResolution: Table 10 shows, perhaps surprisingly, that\nestimating ﬂow at a diﬀerent resolution and aspect\nratio can substantially improve performance on KITTI-\n15 (2.93 vs. 3.80), presumably because the motion ﬁeld\nin this dataset is dominated by horizontal motion. We\nhave not observed this eﬀect in other datasets.\nTable 11. Data augmentation. F: im-\nage ﬂipping up/down and left/right (not\nused for KITTI), C: color augmentation\nChairs\nSintel train\nKITTI-15 train\nF C\ntest\nClean Final\nall\nnoc\nER%\nMinimal\n–\n–\n3.47\n4.39\n6.56\n13.27\n2.56\n22.13\n– ✓\n3.56\n4.38\n6.58\n13.07 2.47 21.21\n✓–\n3.49\n4.23\n6.73\n–\n–\n–\n✓✓\n3.58\n4.20\n6.80\n–\n–\n–\nFull\n–\n–\n2.53\n3.84\n5.14\n3.06\n2.03\n9.82\n– ✓\n2.61\n3.78\n5.23\n2.94\n1.98\n9.65\n✓–\n2.57\n3.02\n4.22\n–\n–\n–\n✓✓\n2.55\n3.00\n4.18\n–\n–\n–\nTable 12. Pretraining on Chairs\nPretraining\nSintel train\nKITTI-15 train\non Chairs\nClean Final\nall\nnoc\nER%\nMin.\n–\n4.41\n7.53\n12.93 2.44 21.24\n✓\n4.20\n6.80\n13.07\n2.47 21.21\nFull\n–\n3.38\n4.81\n3.08\n2.04\n10.00\n✓\n3.00\n4.18\n2.94\n1.98\n9.65\nTable 13. Gradient steps (S) and batch\nsize (B)\nChairs\nSintel train\nKITTI-15 train\nS\nB\ntest\nClean\nFinal\nall\nnoc\nER%\ntest\n60K 32\n{3.16}\n3.04\n4.23\n2.92\n1.96\n9.71\n1.2M\n1\n{2.82}\n3.01\n4.09\n2.84\n1.96\n9.39\ntrain\n60K 32\n2.57\n{2.47} {3.92}\n{2.74} {1.87} {9.04}\n1.2M\n1\n2.55\n{2.50} {3.39}\n{2.71} {1.88} {9.05}\nData Augmentation: Table 11 evaluates\nthe importance of color augmentation\n(color channel swapping and hue random-\nization) for all domains, as well as image\nﬂipping for Sintel. The results show that\nboth augmentation techniques improve\nperformance, in particular image ﬂipping\nfor Sintel (which is a much smaller dataset\nthan Chairs or KITTI).\nPretraining: Pretraining is a common\nstrategy in supervised [5,25] and unsu-\npervised [14,35] optical ﬂow. The results\nin Table 12 conﬁrm that pretraining on\nChairs improves performance on Sintel\nand KITTI.\nGradient Steps and Batch Size: All exper-\niments up to this point have trained the\nmodel for 60K steps at a batch size of 32.\nTable 13 shows a comparison to another\ntraining regime that trains longer with\nsmaller batches, which consistently im-\nproves performance. We use this regime\nfor our comparison to other published\nmethods.\nComparison to State of the Art: We show qualitative results in Figure 3\nand quantitatively evaluate our model trained on KITTI and Sintel data in the\ncorresponding benchmarks in Table 14, where we compare against state-of-the-art\ntechniques for unsupervised and supervised optical ﬂow. Results not reported by\nprior work are indicated with “–”.\nWhat Matters in Unsupervised Optical Flow\n13\nTable 14. Our model (yellow) compared to state of the art. Supervised models in gray\nﬁne-tune on their evaluation domain, which is often not possible in practice. Braces\nindicate models whose training set includes its evaluation set, and so are not comparable:\n“()” trained on the labeled evaluation set, “{}” trained on the unlabeled evaluation set,\nand “[]” trained on data related to the evaluation set (e.g. < 5 frames away in KITTI,\nor having the same content in Sintel). The best unsupervised and supervised (without\nﬁnetuning) results are in bold. Methods that use additional modalities are denoted with\nMDM: mono depth/motion, SDM: stereo depth/motion, MF: multi-frame ﬂow\nSintel Clean [4]\nSintel Final [4]\nKITTI 2012 [6]\nKITTI 2015 [19]\nEPE\nEPE\nEPE\nEPE\nEPE (noc)\nER in %\nMethod\ntrain\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntrain\ntrain\ntest\nSupervised\n(A) FlowNet2-ft [11]\n(1.45)\n4.16\n(2.01)\n5.74\n(1.28)\n1.8\n(2.30)\n–\n(8.61)\n11.48\n(B) PWC-Net-ft [25]\n(1.70)\n3.86\n(2.21)\n5.13\n(1.45)\n1.7\n(2.16)\n–\n(9.80)\n9.60\n(C) SelFlow-ft [15]\n(1.68)\n[3.74]\n(1.77)\n{4.26}\n(0.76)\n1.5\n(1.18)\n–\n–\n8.42\n(D) VCN-ft [32]\n(1.66)\n2.81\n(2.24)\n4.40\n–\n–\n(1.16)\n–\n(4.10)\n6.30\n(E) FlowNet2 [11]\n2.02\n3.96\n3.14\n6.02\n4.09\n–\n9.84\n–\n28.20\n–\n(F) PWC-Net [25]\n2.55\n–\n3.93\n–\n4.14\n–\n10.35\n–\n33.67\n–\n(G) VCN [32]\n2.21\n–\n3.62\n–\n–\n–\n8.36\n–\n25.10\n–\nUnsupervised\n(H) Back2Basics [34]\n–\n–\n–\n–\n11.30\n9.9\n–\n–\n–\n–\n(I)\nDSTFlow [22]\n{6.16}\n10.41\n{7.38}\n11.28\n[10.43]\n12.4\n[16.79]\n[6.96]\n[36.00]\n[39.00]\n(J)\nOAFlow [30]\n{4.03}\n7.95\n{5.95}\n9.15\n[3.55]\n[4.2]\n[8.88]\n–\n–\n[31.20]\n(K) UnFlow [18]\n–\n–\n7.91\n10.21\n3.29\n–\n8.10\n–\n23.27\n–\n(L) GeoNet [33] (MDM)\n–\n–\n–\n–\n–\n–\n10.81\n8.05\n–\n–\n(M) DF-Net [36] (MDM)\n–\n–\n–\n–\n3.54\n4.4\n{8.98}\n–\n{26.01} {25.70}\n(N) CCFlow [21] (MDM)\n–\n–\n–\n–\n–\n–\n5.66\n–\n20.93\n25.27\n(O) MFOccFlow [12] (MF) {3.89}\n7.23\n{5.52}\n8.81\n–\n–\n[6.59]\n[3.22]\n–\n22.94\n(P) UnOS [29] (SDM)\n–\n–\n–\n–\n1.64\n1.8\n5.58\n–\n–\n18.00\n(Q) EPIFlow [35]\n3.94\n7.00\n5.08\n8.51\n2.61\n3.4\n5.56\n2.56\n–\n16.95\n(R) DDFlow [14]\n{2.92}\n6.18\n{3.98}\n7.40\n[2.35]\n3.0\n[5.72]\n[2.73]\n–\n14.29\n(S)\nSelFlow [15] (MF)\n[2.88]\n[6.56]\n{3.87} {6.57}\n[1.69]\n2.2\n[4.84]\n[2.40]\n–\n14.19\n(T) UFlow-test\n3.01\n–\n4.09\n–\n1.58\n–\n2.84\n1.96\n9.39\n–\n(U) UFlow-train\n{2.50}\n5.21\n{3.39}\n6.50\n1.68\n1.9\n{2.71}\n{1.88}\n{9.05}\n11.13\nAmong unsupervised approaches (H-U), our model sets a new state of the\nart for Sintel Clean (5.21 vs. 6.18), Sintel Final (6.50 vs. 7.40), and KITTI-15\n(11.13% vs. 14.19%) – where, for a lack of comparability, we had to disregard\nresults in braces that came from (partially) training on the test set. UFlow is\nonly outperformed (1.8 vs. 1.9) on KITTI-12, which does not include moving\nobjects, by a stereo-depth and motion based approach (P).\nThe top-performing supervised models ﬁnetuned on data from the evaluation\ndomain (models A-D) do outperform our unsupervised model, as one may expect.\nBut on KITTI-15, our model performs on par with the supervised FlowNet2. Of\ncourse, ﬁne-tuning on the domain is only possible because the KITTI training data\nalso contains ground-truth ﬂow, which we ignore but which supervised techniques\nrequire. This sort of supervision is hard to obtain (KITTI being virtually the\nonly non-synthetic dataset with this information), which demonstrates the value\nof unsupervised ﬂow techniques such as ours. Without access to the ground truth\nlabels of the test domain, our unsupervised method compares more favorably\nto its supervised counterparts, signiﬁcantly outperforming them on KITTI. Our\nﬁnal experiment analyses cross-domain generalization in more detail.\n14\nR. Jonschkowski et al.\n2015\nKITTI\nClean\nSintel\nFinal\nSintel\nInput RGB\nTrue Flow\nPredicted Flow\nEndpoint Error\nTrue Occlusions\nPredicted Occlusions\nFig. 3.\nResults for our model on random examples not seen during training taken\nfrom KITTI 2015 and Sintel Final. These qualitative results show the model’s ability\nto estimate fast motions, relatively ﬁne details, and substantial occlusions\nTable 15. Generalization across datasets.\nPerformance when training on one dataset\nand testing on diﬀerent one (gray if same)\nChairs\nSintel train\nKITTI-15 train\nMethod\ntest\nClean\nFinal\nall\nnoc\nER%\nTrain on\nChairs\nPWC-Net [25]\n2.00\n3.33\n4.59\n13.20\n–\n41.79\nDDFlow [14]\n2.97\n4.83\n4.85\n17.26\n–\n–\nUFlow-test\n{2.82}\n4.36\n5.12\n15.68\n7.96\n32.69\nUFlow-train\n2.55\n3.43\n4.17\n11.27\n5.66\n30.31\nTrain on\nSintel\nPWC-Net [25]\n3.69\n(1.86)\n(2.31)\n10.52\n–\n30.49\nDDFlow [14]\n3.46\n{2.92}\n{3.98}\n12.69\n–\n–\nUFlow-test\n3.39\n3.01\n4.09\n7.67\n3.77\n17.41\nUFlow-train\n3.25\n{2.50}\n{3.39}\n9.40\n4.53\n20.02\nTrain on\nKITTI\nDDFlow [14]\n6.35\n6.20\n7.08\n[5.72]\n–\n–\nUFlow-test\n5.25\n6.34\n7.01\n2.84\n1.96\n9.39\nUFlow-train\n5.05\n5.58\n6.31\n{2.71}\n{1.88}\n{9.05}\nTable 15 evaluates out-of-domain\ngeneralization by training and eval-\nuating models across three datasets.\nWhile performance is best when train-\ning and test data are from the same\ndomain, our model shows good gener-\nalization. It consistently outperforms\nDDFlow and it outperforms the su-\npervised PWC-Net in all but one gen-\neralization task (training on Chairs\nand testing on Sintel Clean).\n7\nConclusion\nWe have presented a study into what matters in unsupervised optical ﬂow that\nsystematically analyzes, compares, and improves a set of key components. This\nstudy results in a range of novel observations about these components and their\ninteractions, from which we integrate the best components and improvements into\na uniﬁed framework for unsupervised optical ﬂow. Our resulting UFlow model\nsubstantially outperforms the state of the art among unsupervised methods and\nperforms on par with the supervised FlowNet2 on the challenging KITTI 2015\nbenchmark, despite not using any labels. In addition to its strong performance,\nour method is also signiﬁcantly simpler than many related approaches, which we\nhope will make it useful as a starting point for further research into unsupervised\noptical ﬂow. Our code is available at https://github.com/google-research/\ngoogle-research/tree/master/uflow.\nWhat Matters in Unsupervised Optical Flow\n15\nReferences\n1. Simon Baker, Daniel Scharstein, J. P. Lewis, Stefan Roth, Michael J. Black, and\nRichard Szeliski. A database and evaluation methodology for optical ﬂow. IJCV,\n2011.\n2. John L. Barron, David J. Fleet, and Steven S. Beauchemin. Performance of optical\nﬂow techniques. IJCV, 1994.\n3. Thomas Brox, Andr´es Bruhn, Nils Papenberg, and Joachim Weickert. High accuracy\noptical ﬂow estimation based on a theory for warping. ECCV, 2004.\n4. Daniel J. Butler, Jonas Wulﬀ, Garrett B. Stanley, and Michael J. Black.\nA\nnaturalistic open source movie for optical ﬂow evaluation. ECCV, 2012.\n5. Alexey Dosovitskiy, Philipp Fischery, Eddy Ilg, Philip H¨ausser, Caner Hazırba¸s,\nVladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox.\nFlownet: Learning optical ﬂow with convolutional networks. ICCV, 2015.\n6. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous\ndriving? The KITTI vision benchmark suite. CVPR, 2012.\n7. James J. Gibson. The Perception of the Visual World. Houghton Miﬄin, 1950.\n8. Cl´ement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow.\nDigging into self-supervised monocular depth estimation. ICCV, 2019.\n9. Ariel Gordon, Hanhan Li, Rico Jonschkowski, and Anelia Angelova. Depth from\nvideos in the wild: Unsupervised monocular depth learning from unknown cameras.\nICCV, 2019.\n10. Berthold K. P. Horn and Brian G. Schunck. Determining optical ﬂow. Artiﬁcial\nIntelligence, 1981.\n11. Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy,\nand Thomas Brox. Flownet 2.0: Evolution of optical ﬂow estimation with deep\nnetworks. CVPR, 2017.\n12. Joel Janai, Fatma G¨uney, Anurag Ranjan, Michael J. Black, and Andreas Geiger.\nUnsupervised learning of multi-frame optical ﬂow with occlusions. ECCV, 2018.\n13. Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\nICLR, 2015.\n14. Pengpeng Liu, Irwin King, Michael R. Lyu, and Jia Xu. DDFlow: Learning optical\nﬂow with unlabeled data distillation. AAAI, 2019.\n15. Pengpeng Liu, Michael R. Lyu, Irwin King, and Jia Xu. Selﬂow: Self-supervised\nlearning of optical ﬂow. CVPR, 2019.\n16. Bruce D. Lucas and Takeo Kanade. An iterative image registration technique with\nan application to stereo vision. DARPA Image Understanding Workshop, 1981.\n17. Nikolaus Mayer, Eddy Ilg, Philipp Fischer, Caner Hazirbas, Daniel Cremers, Alexey\nDosovitskiy, and Thomas Brox. What makes good synthetic training data for\nlearning disparity and optical ﬂow estimation? IJCV, 2018.\n18. Simon Meister, Junhwa Hur, and Stefan Roth. Unﬂow: Unsupervised learning of\noptical ﬂow with a bidirectional census loss. AAAI, 2018.\n19. Moritz Menze, Christian Heipke, and Andreas Geiger.\nJoint 3d estimation of\nvehicles and scene ﬂow. ISPRS Workshop on Image Sequence Analysis, 2015.\n20. Anurag Ranjan and Michael J. Black. Optical ﬂow estimation using a spatial\npyramid network. CVPR, 2017.\n21. Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun, Jonas\nWulﬀ, and Michael J. Black. Competitive collaboration: Joint unsupervised learning\nof depth, camera motion, optical ﬂow and motion segmentation. CVPR, 2019.\n22. Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang, and Hongyuan Zha.\nUnsupervised deep learning for optical ﬂow estimation. AAAI, 2017.\n16\nR. Jonschkowski et al.\n23. Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. Convolutional neural network\narchitecture for geometric matching. CVPR, 2017.\n24. Deqing Sun, Stefan Roth, and Michael J. Black. Secrets of optical ﬂow estimation\nand their principles. CVPR, 2010.\n25. Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. PWC-Net: CNNs for\noptical ﬂow using pyramid, warping, and cost volume. CVPR, 2018.\n26. Narayanan Sundaram, Thomas Brox, and Kurt Keutzer. Dense point trajectories\nby gpu-accelerated large displacement optical ﬂow. ECCV, 2010.\n27. Carlo Tomasi and Roberto Manduchi. Bilateral ﬁltering for gray and color images.\nICCV, 1998.\n28. A. Torralba and A. A. Efros. Unbiased look at dataset bias. CVPR, 2011.\n29. Yang Wang, Peng Wang, Zhenheng Yang, Chenxu Luo, Yi Yang, and Wei Xu.\nUnos: Uniﬁed unsupervised optical-ﬂow and stereo-depth estimation by watching\nvideos. CVPR, 2019.\n30. Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang, and Wei Xu.\nOcclusion aware unsupervised learning of optical ﬂow. CVPR, 2018.\n31. Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image\nquality assessment: From error visibility to structural similarity. IEEE Transactions\non Image Processing, 2004.\n32. Gengshan Yang and Deva Ramanan. Volumetric correspondence networks for\noptical ﬂow. NeurIPS, 2019.\n33. Zhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth,\noptical ﬂow and camera pose. CVPR, 2018.\n34. Jason J. Yu, Adam W. Harley, and Konstantinos G. Derpanis. Back to basics: Un-\nsupervised learning of optical ﬂow via brightness constancy and motion smoothness.\nECCV Workshop, 2016.\n35. Yiran Zhong, Pan Ji, Jianyuan Wang, Yuchao Dai, and Hongdong Li. Unsupervised\ndeep epipolar ﬂow for stationary or dynamic scenes. CVPR, 2019.\n36. Yuliang Zou, Zelun Luo, and Jia-Bin Huang. DF-Net: Unsupervised joint learning\nof depth and ﬂow using cross-task consistency. ECCV, 2018.\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "eess.IV"
  ],
  "published": "2020-06-08",
  "updated": "2020-08-14"
}