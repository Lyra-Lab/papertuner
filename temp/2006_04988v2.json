{
  "id": "http://arxiv.org/abs/2006.04988v2",
  "title": "Object Segmentation Without Labels with Large-Scale Generative Models",
  "authors": [
    "Andrey Voynov",
    "Stanislav Morozov",
    "Artem Babenko"
  ],
  "abstract": "The recent rise of unsupervised and self-supervised learning has dramatically\nreduced the dependency on labeled data, providing effective image\nrepresentations for transfer to downstream vision tasks. Furthermore, recent\nworks employed these representations in a fully unsupervised setup for image\nclassification, reducing the need for human labels on the fine-tuning stage as\nwell. This work demonstrates that large-scale unsupervised models can also\nperform a more challenging object segmentation task, requiring neither\npixel-level nor image-level labeling. Namely, we show that recent unsupervised\nGANs allow to differentiate between foreground/background pixels, providing\nhigh-quality saliency masks. By extensive comparison on standard benchmarks, we\noutperform existing unsupervised alternatives for object segmentation,\nachieving new state-of-the-art.",
  "text": "Object Segmentation Without Labels with Large-Scale Generative Models\nAndrey Voynov 1 Stanislav Morozov 1 Artem Babenko 1\nAbstract\nThe recent rise of unsupervised and self-\nsupervised learning has dramatically reduced the\ndependency on labeled data, providing effective\nimage representations for transfer to downstream\nvision tasks. Furthermore, recent works employed\nthese representations in a fully unsupervised setup\nfor image classiﬁcation, reducing the need for hu-\nman labels on the ﬁne-tuning stage as well. This\nwork demonstrates that large-scale unsupervised\nmodels can also perform a more challenging ob-\nject segmentation task, requiring neither pixel-\nlevel nor image-level labeling. Namely, we show\nthat recent unsupervised GANs allow to differen-\ntiate between foreground/background pixels, pro-\nviding high-quality saliency masks. By extensive\ncomparison on standard benchmarks, we outper-\nform existing unsupervised alternatives for object\nsegmentation, achieving new state-of-the-art. Our\nmodel and implementation are available online2.\n1. Introduction\nReducing the reliance on labeled data is a long-standing\ngoal of machine learning research. The recent studies on\nunsupervised and self-supervised learning for both discrimi-\nnative (Chen et al., 2020b; He et al., 2020; Caron et al., 2020;\nGrill et al., 2020) and generative (Donahue & Simonyan,\n2019; Chen et al., 2020a) models have demonstrated that\none does not need labeled data on the pretraining stage\nto produce image representations for typical computer vi-\nsion problems. While these representations require some\nlabeled data to ﬁnetune to a particular downstream task,\nrecent works (Van Gansbeke et al., 2020; Zheltonozhskii\net al., 2020) exploit these representations to solve the image\nclassiﬁcation problem without labels at all.\n1Yandex, Moscow, Russia. Correspondence to: Andrey Voynov\n<an.voynov@yandex.ru>.\nPreprint, to be published in Proceedings of the 38 th International\nConference on Machine Learning, PMLR 139, 2021. Copyright\n2021 by the author(s).\n2https://github.com/anvoynov/\nBigGANsAreWatching\nThis paper employs state-of-the-art unsupervised generative\nmodels to perform label-free object segmentation, where\ngroundtruth pixel-level labels are expensive to collect be-\ncause of labor-intensive human efforts. This problem cur-\nrently receives much research attention and is typically ad-\ndressed by methods based on GANs (Chen et al., 2019;\nBielski & Favaro, 2019; Benny & Wolf, 2020). However,\ntraining high-quality GANs can be both time-consuming\nand unstable. Moreover, the protocols in (Chen et al., 2019;\nBielski & Favaro, 2019; Benny & Wolf, 2020) typically\ninclude a large number of hyperparameters that are tricky to\ntune in the completely unsupervised setup when a labeled\nvalidation set is not available. In contrast, we propose an\nalternative, much simpler approach that does not require ad-\nversarial training or heavy hyperparameter tuning for each\nparticular segmentation task.\nOur work is partially inspired by the ﬁndings from Voynov\n& Babenko (2020), which has shown that the latent space\nof BigGAN (Brock et al., 2019) possess the direction “re-\nsponsible” for the background removal, and this direction\ncan be used to produce training data for saliency detection.\nHowever, the approach (Voynov & Babenko, 2020) is not\nunsupervised since (i) BigGAN is trained with known Ima-\ngenet labels, therefore, it is not an unsupervised model; (ii) it\nrequires manual inspecting of several latent transformations.\nThis paper eliminates external supervision mentioned above\nand demonstrates that large off-the-shelf GANs can segment\nimages, being completely unsupervised. As a main technical\nnovelty, we introduce an automatic procedure identifying\nthe “segmenting” latent directions in the pretrained GANs.\nThis procedure reveals such directions in the state-of-the-\nart publicly available BigBiGAN (Donahue & Simonyan,\n2019), which is trained on the Imagenet (Deng et al., 2009)\nwithout labels. These directions allow to distinguish ob-\nject/background pixels in the generated images, providing\ndecent segmentation masks. These masks are then used to\nsupervise a discriminative U-Net model (Ronneberger et al.,\n2015), which is stable and easy to train. As another advan-\ntage, our approach also provides a straightforward way to\ntune hyperparameters. Since an amount of synthetic data is\nunlimited, its hold-out subset can be used as validation.\nOur work conﬁrms the promise of using GANs to produce\nsynthetic training data, which is a long-standing goal of\narXiv:2006.04988v2  [cs.LG]  11 Jun 2021\nObject Segmentation Without Labels with Large-Scale Generative Models\nresearch on generative modeling. In extensive experiments,\nwe show that the approach often outperforms the existing un-\nsupervised alternatives for object segmentation and saliency\ndetection. Our results provide additional evidence to the\ncommon trend that more data and larger models often can\nreduce the requirements of human labels.\nOverall, the contributions of our paper are the following:\n1. We propose to perform unsupervised object segmenta-\ntion using off-the-shelf Imagenet-pretrained GANs.\n2. We introduce an automatic method to identify “seg-\nmenting” directions in the GAN latent space.\n3. We show that our method outperforms the state-of-the-\nart in most operating points. Given its simplicity, the\nmethod can serve as a baseline in the future.\n2. Related work\nIn this paper, we address the binary object segmentation\nproblem, i.e., for each pixel, we aim to predict if it belongs\nto the object or the background. This problem is typically\nreferred to as saliency detection (Wang et al., 2019) and\nforeground object segmentation (Chen et al., 2019; Bielski\n& Favaro, 2019; Benny & Wolf, 2020). While most prior\nworks propose fully-supervised or weakly-supervised meth-\nods, we focus on the most challenging unsupervised setup,\nwhere only a few approaches have been developed.\nExisting unsupervised approaches. Before the rise of\ndeep learning models, a large number of “shallow” unsuper-\nvised techniques were developed (Zhu et al., 2014b; Jiang\net al., 2013; Peng et al., 2016; Cong et al., 2017; Cheng\net al., 2014; Wei et al., 2012). These earlier techniques were\nmostly based on hand-crafted features and heuristics, e.g.,\ncolor contrast (Cheng et al., 2014), or certain background\npriors (Wei et al., 2012). Often these approaches also utilize\ntraditional computer vision routines, such as super-pixels\n(Yang et al., 2013; Wang et al., 2016), object proposals (Guo\net al., 2017), CRF (Kr¨ahenb¨uhl & Koltun, 2011). These\nheuristics, however, are not completely learned from data,\nand the corresponding methods are inferior to the more\nrecent “deep” approaches.\nRegarding unsupervised deep models, several works have\nrecently been proposed by the saliency detection community\n(Wang et al., 2017b; Zhang et al., 2018; 2017; Nguyen et al.,\n2019). Their main idea is to combine or fuse the predic-\ntions of several heuristic saliency methods, typically using\nthem as a source of noisy groundtruth for deep CNN models.\nHowever, these methods are not entirely unsupervised since\nthey rely on the supervised-pretrained classiﬁcation or seg-\nmentation networks or utilize a limited number of labeled\ndata (Zhang et al., 2020). In contrast, in this work, we focus\non the methods that do not require labeled data.\nGenerative models for object segmentation. The recent\nline of unsupervised methods (Chen et al., 2019; Bielski &\nFavaro, 2019) employs generative modeling to decompose\nthe image into the object/background. In a nutshell, these\nmethods exploit the idea that the object’s location or ap-\npearance can be perturbed without affecting image realism.\nThis inductive bias is formalized in the training protocols,\nwhich include learning of GANs. Therefore, for each new\nsegmentation task, one has to perform adversarial learning,\nwhich can be unstable, time-consuming, and sensitive to\nhyperparameters. In contrast, our approaches avoid these\ndisadvantages, being much simpler and easier to reproduce.\nIn essence, we propose to use the “inner knowledge” of\npretrained large-scale GANs to produce the saliency masks.\nLatent spaces of large-scale GANs. Recent study (Voynov\n& Babenko, 2020) has shown that the latent space of Big-\nGAN (Brock et al., 2019) can be used to obtain saliency\nmasks for synthetic images. However, such an ability was\ndiscovered only for BigGAN trained under the supervision\nfrom the image class labels. For unconditional GANs, it\nwas not discovered in (Voynov & Babenko, 2020), hence,\nit is not clear if the supervision from the class labels is\nnecessary for the GAN latent space to distinguish between\nobject/background pixels. This paper shows that this super-\nvision is not necessary, contributing novel knowledge to the\ngeneral trend to unsupervised learning.\n3. Latent Segmenters in Unsupervised GANs\nVoynov & Babenko (2020) has shown that the BigGAN’s\nlatent space contains a direction hbg responsible for a back-\nground removal: once a latent code z of a generated image\nG(z) is shifted by hbg, the background pixels of the shifted\nimage G(z + hbg) become white, while the foreground\nones remain almost unchanged. As we will show, the la-\ntent spaces of other large-scale GANs also have directions\nthat have different effects on background/foreground pixels.\nThe following section provides a principled framework to\nidentify such “segmenting” directions automatically.\n3.1. Modeling a segmenting direction\nFormally, we consider a latent shift h to be a segmenting\ndirection if there are two afﬁne operators A1, A2 : R3 →R3\nsuch that for each latent z and pixel G(z)x,y we have\nG(z + h)x,y = Ai(x,y)(G(z)x,y), i(x, y) ∈{1, 2}\n(1)\nthat is, the latent shift acts on each pixel as one of the\ntwo ﬁxed maps. Intuitively, this deﬁnition formalizes our\ndesire that the latent segmenter should affect the back-\nground/foreground pixels differently.\nNow we explain how to ﬁnd these afﬁne operators A1, A2\nfor a given latent direction h from the generator G. In\nObject Segmentation Without Labels with Large-Scale Generative Models\naddition, we also show how to identify the directions that are\nthe most appropriate for an object segmentation task. Given\ntwo afﬁne operators A1, A2, for a pair of pixel intensities\nc, c′ let us deﬁne a map\nSA1,A2(c, c′) = arg min\nA∈{A1,A2}\n(∥A(c) −c′∥2) · c\n(2)\nthat is SA1,A2 chooses an operator that maps c closer\nto c′ and applies it to c.\nWe extend this action to the\ngenerated images space by setting (SA1,A2 · G(z))x,y =\nSA1,A2(G(z)x,y, G(z + h)x,y). Then we deﬁne the restora-\ntion loss:\nLh(A1, A2) = E\nz\nX\nx,y\n∥(SA1,A2 · G(z))x,y−G(z+h)x,y∥2\n(3)\nhere we sum over all pixels of an image G(z).\nThis\nquantity indicates how good one can approximate the map\nσh : G(z) →G(z + h) by choosing the optimal A1, A2 for\neach pixel. If this map can be represented in a form of (1),\nthe quantity Lh possesses a global minimum equal to 0.\nThus, for a given direction h one can ﬁnd the optimal A1, A2\nby solving\nA1, A2 = argminA1,A2Lh(A1, A2)\n(4)\nThese operators also deﬁne a binary segmentation of a gen-\nerated image by assigning a label computed as\narg min\ni∈{1,2}\n∥Ai · G(z)x,y −G(z + h)x,y∥2\n(5)\nfor each pixel (x, y).\n3.2. Exploring segmenting directions\nNow we explain how to identify the segmenting direction in\nthe latent space of a given pretrained GAN. First, we ﬁnd\na set of interpretable directions using the technique from\n(Voynov & Babenko, 2020) with the default hyperparam-\neters. This results in a set of latent directions h1, . . . , hN.\nFor each hk we then optimize (4) with the stochastic gra-\ndient descent. Since the number of learnable parameters\nis only 24 and the loss is averaged over all image pixels,\nwe use a small mini-batch of four images and 200 steps of\nAdam optimizer with a learning rate 0.005. The optimiza-\ntion converges rapidly, and we did not observe any beneﬁts\nfrom larger batches or larger numbers of steps. Overall,\nthis optimization takes a few minutes on the Nvidia-1080ti\nGPU card. Thus, for each hk we obtain a pair of afﬁne\noperators (A(hk)\n1\n, A(hk)\n2\n). The optimal loss value Lhk in-\ndicates how good a particular transform σhk can be ap-\nproximated by two pixelwise afﬁne operators. In practice,\nthis ranking is not sufﬁcient to identify directions suitable\nfor segmentation as the transforms σk may induce almost\nidentical or a global lighting transformation with A(hk)\n1\nclose to A(hk)\n2\n. If so, the masking based on these operators\nbecomes noisy and inadequate for downstream tasks. To\novercome this issue, for each hk we compute the mean dis-\ntance Dk = ∥A(hk)\n1\n· G(z)x,y −A(hk)\n2\n· G(z)x,y∥2 over the\npixels of generated images. Intuitively, the direction that\ninduces the most distant A1, A2 should produce adequate\nsegmentation masks for synthetic images.\nWe apply the above approach to the Imagenet-pretrained\nunsupervised BigBiGAN, which parameters are available\nonline. After using the method from (Voynov & Babenko,\n2020), we extract 120 latent directions to serve as candidates\nto be the latent segmenters. We also scale them by a mul-\ntiplier 5 as the unit-length latent shifts commonly induce\nminor image transformation leading to noisy restoration\nloss Lh optimization process. After solving the optimiza-\ntion problem (4) and computing Dk values for each h, we\nchoose the direction with the highest Dk among the best-\n70% in terms of the restoration loss. We use this direction\nas a latent segmenter to produce the saliency masks.\nOn the Figure 1 we plot the optimal values of the restoration\nloss Lhk and the operators mean distance Dk for all the\ncandidate directions. Notably, the background/foreground\ndirection, which we utilize for the saliency generation, has\nthe highest mean distance Dk while possessing low restora-\ntion loss Lhk. On the Figure 2 we illustrate the images\nSA1,A2 · G(z) that approximate the shifted G(z + h) by the\npixelwise operators A1, A2 that minimize Lhbg. In our ex-\nperiments, the operators A1, A2 corresponding to the back-\nground saliency direction have a form:\nA1(c) =\n\n\n0.13\n−0.12\n0.06\n0.01\n0.00\n0.04\n0.02\n−0.20\n0.22\n\n· c +\n\n\n0.78\n0.76\n0.69\n\n;\nA2(c) =\n\n\n0.31\n−0.05\n0.05\n0.04\n0.19\n0.06\n0.01\n−0.06\n0.31\n\n· c −\n\n\n0.1\n0.15\n0.19\n\n\nThe ﬁrst operator performs aggressive lightening, while the\nsecond one downscales the channels and applies a minor\nnegative shift resulting in a darkening. In practice, the\nintensity of the pixels handled by the ﬁrst operator increases\nwhile the intensity of the pixels handled by the second one\ndecreases. This direction, along with the corresponding\noperators, is used to produce the saliency masks as shown in\nFigure 2. For efﬁciency, we set the generated image mask at\npixel (x, y) to be equal to [∥G(z + hbg)x,y∥> ∥G(z)x,y∥]\nas in practice it appears to be a decent approximation of (5).\n3.3. Adaptation to the particular segmentation task.\nSince BigBiGAN was trained on the Imagenet, sampling\nthe latent codes from the standard Gaussian distribution\nObject Segmentation Without Labels with Large-Scale Generative Models\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nrestoration loss Lhk\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nmean distance Dk\nSaliency lighting\nZoom\nLighting direction\nGlobal shading\nHeavy deformation\n- Global Shading +\n- Light Direction +\n- Zoom +\n- Heavy Deformation +\n- Saliency Lighting +\nFigure 1. Left: BigBiGAN latent directions restoration loss values and the operators A1, A2 dissimilarity. Right: examples of generated\nimage transformations induced by moving a latent z along some of directions. The central image in each row corresponds to the original\nsample G(z).\nFigure 2. From top to bottom: generated image; shifted image;\nshifted image approximation by pixelwise operators A1, A2; mask\ngenerated by these operators assignment.\nz ∼N(0, I) will result in the distribution of the synthetic\ndata that resembles the Imagenet. However, this distribution\ncan be suboptimal for the particular segmentation task. To\nmitigate this issue, we introduce a simple additional step\nin the process of synthetic data generation. To make the\ndistribution of generated images closer to the particular\ndataset I={I1, . . . , IN}, we sample z from the latent space\nregions that are close to the latent codes of I. To this\nend, we use the BigBiGAN encoder to compute the latent\nrepresentations {E(I1), . . . , E(IN)} ⊂R120 and sample\nthe codes from the neighborhood of these representations.\nFormally, the samples have a form:\n{E(Ii)+αξ | i ∼U{1, N}, ξ ∼N(0, I)}\n(6)\nHere α denotes the neighborhood size, and it should be\nlarger for small I to prevent overﬁtting. In particular, we\nuse α=0 for Imagenet and α=0.2 for all other cases. In the\nexperimental section, we demonstrate that this simple and\nefﬁcient modiﬁcation of the data generation process results\nin a dramatic performance boost.\n3.4. Improving saliency masks.\nHere we describe a few simple heuristics that increase the\nmasks’ quality for the particular segmentation task. The\nablation of each component is presented in Section 4.4.\nMask size ﬁltering.\nSince some of the BigBiGAN-\nproduced images are low-quality and do not contain well-\ndeﬁned objects, the corresponding masks can result in very\nnoisy supervision. To alleviate this, we apply simple ﬁlter-\ning that excludes the images where the ratio of foreground\npixels exceeds 0.5.\nHistogram ﬁltering. Since G(z+hbg) should have mostly\ndark and light pixels, we ﬁlter out the images that are not\ncontrastive enough. Formally, we compute the intensity\nhistogram with 12 bins for the grayscaled G(z+hbg). Then\nwe smooth it by taking the moving average with a window of\n3 and ﬁlter out the samples that have local maxima outside\nthe ﬁrst/last buckets of the histogram.\nConnected components ﬁltering.\nFor each generated\nmask M we group the foreground pixels into connected\n(by edges) groups forming clusters M1, . . . , Mk. Assuming\nthat M1 is the cluster with the maximal area, we exclude\nall the clusters Mi with |Mi| < 0.2 · |M1|. This technique\nallows to remove visual artifacts from the synthetic data.\nObject Segmentation Without Labels with Large-Scale Generative Models\nFigure 3. Examples of mask improvement. Left: the sample rejected by the mask size ﬁlter. Middle: the sample rejected by the histogram\nﬁltering. Right block: mask pixels removed by the connected components ﬁlter are shown in blue and the remaining mask pixels are\nshown in red.\nWe present samples of images rejected by each ﬁltering step\nin Figure 3.\n3.5. Training the model on synthetic data\nGiven a large amount of synthetic data, one can train one of\nthe existing image-to-image CNN architectures in the fully\nsupervised regime. The whole pipeline is schematically\npresented in Figure 4. In all our experiments, we employ\na standard U-net architecture (Ronneberger et al., 2015).\nWe train U-net on the synthetic dataset with the Adam op-\ntimizer and the binary cross-entropy objective applied on\nthe pixel level. We perform 12 · 103 steps with batch 95.\nThe initial learning rate equals 0.001 and is decreased by\n0.2 on step 8 · 103. During inference, we rescale an input\nimage to have a size 128 along its shorter side. Compared to\nexisting unsupervised alternatives, the training of our model\nis straightforward and does not include a large number of\nhyperparameters. The only hyperparameters in our protocol\nare batch size, learning rate schedule, and a number of op-\ntimizer steps, and we tune them on the hold-out validation\nset of synthetic data. We set the batch size to guarantee\nthe maximal GPU memory utilization. Figure 5 reports the\nsegmentation quality on the hold-out synthetic data for the\ndifferent hyperparameter values. Notably, they have a minor\naffect on the model quality. Figure 5 demonstrates that the\noptimal hyperparameters chosen for synthetic data are typi-\ncally optimal for real datasets as well. Training with online\nsynthetic data generation takes approximately seven hours\non two Nvidia 1080Ti cards.\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n10k \n12k \n14k \ntrain steps\n0.01\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nrate\n0.001\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n4000      6000 \n8000 \nsteps per rate decay\n0.1\n0.2\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nrate decay\nSynthetic ECSSD DUT-OMRON DUTS\nFigure 5. Model performance for different hyperparameters. We\ntake the hyperparameters that performs best on the synthetic data.\nz = E(Ireal)\nIreal\nG\nG\nz + hbg\nLoss\nFiltering\nU-net\n>\nFigure 4. Schematic representation of our approach. First, we generate an image G(z) and the shifted image G(z + hbg). Then these\nimages induce a synthetic mask. This mask is passed through a ﬁltering (Section 3.4) and a U-net segmentation model learns to predict it,\ngiven the original generated image.\nObject Segmentation Without Labels with Large-Scale Generative Models\n4. Experiments\nThis section aims to conﬁrm that the usage of GAN-\nproduced synthetic data is a promising direction for unsu-\npervised saliency detection and object segmentation. To this\nend, we extensively compare our approach to the existing\nunsupervised counterparts on the standard benchmarks.\nEvaluation metrics. All the methods are compared in terms\nof the three measures described below.\n• F-measure\nis\nan\nestablished\nmeasure\nin\nthe\nsaliency detection literature.\nIt is deﬁned as\nFβ= (1+β2)P recision×Recall\nβ2P recision+Recall\n. Here Precision and Re-\ncall are calculated based on the binarized predicted\nmasks and groundtruth masks as Precision=\nT P\nT P +F P\nand Recall=\nT P\nT P +F N , where TP, TN, FP, FN denote\ntrue-positive, true-negative, false-positive, and false-\nnegative, respectively. We compute F-measure for 255\nuniformly distributed binarization thresholds and re-\nport its maximum value max Fβ. We use β=0.3 for\nconsistency with existing works.\n• IoU (Intersection over Union) is calculated on\nthe binarized predicted masks and groundtruth as\nIoU(s, m)=µ(s ∩m)\nµ(s ∪m), where µ denotes the area. The\nbinarization threshold is set to 0.5.\n• Accuracy measures the proportion of pixels that have\nbeen correctly assigned to the object/background. The\nbinarization threshold for masks is set to 0.5.\nSince the existing literature uses different benchmark\ndatasets for saliency detection and object segmentation, we\nperform a separate comparison for each task below.\n4.1. Object segmentation.\nDatasets. We use two following datasets from the literature\nof segmentation with generative models.\n• Caltech-UCSD Birds 200-2011 (Wah et al., 2011)\ncontains 11,788 photographs of birds with segmen-\ntation masks. We follow (Chen et al., 2019), and use\n10,000 images for our training subset and 1,000 for\nthe test subset from splits provided by (Chen et al.,\n2019). Unlike (Chen et al., 2019), we do not use any\nimages for validation and simply omit the remaining\n788 images.\n• Flowers (Nilsback & Zisserman, 2007) contains 8,189\nimages of ﬂowers equipped with saliency masks gener-\nated automatically via the method developed for ﬂow-\ners. We do not apply the mask area ﬁlter in our method\nwith this dataset, as it rejects most of the samples.\nOn these two datasets, we compare the following methods:\n• PerturbGAN (Bielski & Favaro, 2019) segments an\nimage based on the idea that object location can be\nperturbed without affecting the scene realism. For\ncomparison, we use the numbers reported in (Bielski\n& Favaro, 2019).\n• ReDO (Chen et al., 2019) produces segmentation\nmasks based on the idea that object appearance can be\nchanged without affecting image quality. For compar-\nison, we report the numbers from (Chen et al., 2019).\nNote, (Chen et al., 2019) use hold-out labeled sets to\nset hyperparameters.\n• OneGAN (Benny & Wolf, 2020) which simultane-\nously learning a conditional image generator, fore-\nground extraction and segmentation, clustering, and\nobject removal and background completion. Note that\n(Benny & Wolf, 2020) uses bounding boxes from im-\nage annotations to cut background patches, which is a\nsource of weak supervision.\n• BigBiGAN is our method where the latent codes are\nsampled from z ∼N(0, I). For Flowers dataset we\nMethod\nCUB-200-2011\nFlowers\nmax Fβ\nIoU Accuracy maxFβ\nIoU\nAccuracy\nPerturbGAN\n—\n0.380\n—\n—\n—\n—\nReDO\n—\n0.426\n0.845\n—\n0.764\n0.879\nOneGAN\n—\n0.555\n—\n—\n—\n—\nBigBiGAN\n0.794\n0.683\n0.930\n0.760\n0.540\n0.765\nE-BigBiGAN (w/o z-noising)\n0.750\n0.619\n0.918\n0.814\n0.689\n0.874\nE-BigBiGAN (with z-noising)\n0.834\n0.710\n0.940\n0.878\n0.804\n0.904\nstd\n0.005\n0.007\n0.002\n0.001 <0.001 <0.001\nTable 1. The comparison of unsupervised object segmentation methods. For our model, we report the performance averaged over ten runs.\nFor the best model, we also report the standard deviation values.\nObject Segmentation Without Labels with Large-Scale Generative Models\nMethod\nECSSD\nDUTS\nDUT-OMRON\nmaxFβ\nIoU Accuracy maxFβ\nIoU Accuracy maxFβ\nIoU Accuracy\nHS\n0.673 0.508\n0.847\n0.504 0.369\n0.826\n0.561 0.433\n0.843\nwCtr\n0.684 0.517\n0.862\n0.522 0.392\n0.835\n0.541 0.416\n0.838\nWSC\n0.683 0.498\n0.852\n0.528 0.384\n0.862\n0.523 0.387\n0.865\nDeepUSPS\n0.584 0.440\n0.795\n0.425 0.305\n0.773\n0.414 0.305\n0.779\nBigBiGAN\n0.782 0.672\n0.899\n0.608 0.498\n0.878\n0.549 0.453\n0.856\nE-BigBiGAN 0.797 0.684\n0.906\n0.624 0.511\n0.882\n0.563 0.464\n0.860\nTable 2. The comparison of unsupervised saliency detection methods. For BigBiGAN and E-BigBiGAN we report the mean values over\n10 independent runs.\nfound it beneﬁcial to generate the saliency masks by\nthresholding the shifted image G(z + hbg) with its\nmean value. Thus, for Flowers the masks are generated\nas M = [G(z + hbg) > mean(G(z + hbg))].\n• E-BigBiGAN (w/o z-noising) is our method where\nthe latent codes of synthetic data are sampled from the\noutputs of the encoder E applied to the train images of\nthe dataset at hand.\n• E-BigBiGAN (with z-noising) same as above with la-\ntent codes sampled from the vicinity of the embeddings\nwith the neighborhood size α set to 0.2.\nFollowing the prior works, we apply image preprocessing\nby extracting a central crop and resizing it to 128×128. The\ncomparison results are provided in Table 1, which demon-\nstrates the signiﬁcant advantage of our scheme. Note, since\nboth datasets in this comparison are small-scale, z-noising\nconsiderably improves the performance, increasing the di-\nversity of training images.\n4.2. Saliency detection.\nDatasets. We use the following established benchmarks\nfor saliency detection. For all the datasets, groundtruth\npixel-level saliency masks are available.\n• ECSSD (Shi et al., 2015) contains 1,000 images with\nstructurally complex natural contents.\n• DUTS (Wang et al., 2017a) contains 10,553 train and\n5,019 test images. The train images are selected from\nthe ImageNet detection train/val set. The test images\nare selected from the ImageNet test, and the SUN\ndataset (Xiao et al., 2010). We always report the per-\nformance on the DUTS-test subset.\n• DUT-OMRON (Yang et al., 2013) contains 5,168 im-\nages of high content variety.\nBaselines. While there are a large number of papers on\nunsupervised deep saliency detection, all of them employ\npretrained supervised models in their training protocols.\nFigure 6. Top: Images from the DUTS-test dataset. Middle: Groundtruth masks. Bottom: Masks produced by the E-BigBiGAN method.\nObject Segmentation Without Labels with Large-Scale Generative Models\nMethod\nECSSD\nDUTS\nDUT-OMRON\nmaxFβ IoU Accuracy maxFβ IoU Accuracy maxFβ IoU Accuracy\n(Voynov & Babenko, 2020) 0.778 0.648\n0.904\n0.604 0.478\n0.889\n0.56\n0.444\n0.878\nBigBiGAN (base)\n0.737 0.626\n0.859\n0.575 0.454\n0.817\n0.498 0.389\n0.758\nE-BigBiGAN\n0.797 0.684\n0.906\n0.624 0.511\n0.882\n0.563 0.464\n0.860\nTable 3. Comparison of our method with the weakly-supervised BigGAN-based approach.\nMethod\nECSSD\nDUTS\nDUT-OMRON\nmaxFβ IoU Accuracy maxFβ IoU Accuracy maxFβ IoU Accuracy\nBase\n0.737 0.626\n0.859\n0.575 0.454\n0.817\n0.498 0.389\n0.758\n+Imagenet embeddings\n0.773 0.657\n0.874\n0.616 0.483\n0.832\n0.533 0.413\n0.772\n+Size ﬁlter\n0.781 0.670\n0.900\n0.62\n0.499\n0.871\n0.552 0.443\n0.842\n+Histogram\n0.779 0.670\n0.900\n0.621 0.503\n0.875\n0.555 0.450\n0.850\n+Connected components 0.797 0.684\n0.906\n0.624 0.511\n0.882\n0.563 0.464\n0.860\nTable 4. Impact of different components in the E-BigBiGAN pipeline.\nFor example, DeepUSPS (Nguyen et al., 2019) uses a seg-\nmentation model pretrained on CityScapes dataset (Cordts\net al., 2016). Therefore, we mostly use the recent “shallow”\nmethods HS (Yan et al., 2013), wCtr (Zhu et al., 2014a),\nand WSC (Li et al., 2015) as the baselines. These three\nmethods were chosen based on their state-of-the-art per-\nformance reported in the literature and publicly available\nimplementations. To compare with deep saliency detection\nmodels, we also add DeepUSPS (Nguyen et al., 2019) to\nthe list of baselines. However, to perform a fair comparison,\nwe train the DeepUSPS model without pretraining on the\nCityScapes dataset. The results for all methods are reported\nin Table 2. In this table, BigBiGAN denotes the version\nof our method where the latent codes of synthetic images\nare sampled from z ∼N(0, I). In turn, in E-BigBiGAN, z\nare sampled from the latent codes of Imagenet-train images\nfor all three datasets. Since the Imagenet dataset is large\nenough, we do not employ z-noising in this comparison.\nAs one can see, our method mostly outperforms the competi-\ntors by a considerable margin, which conﬁrms the promise\nof using synthetic imagery in unsupervised scenarios. Note\nthat DeepUSPS shows weak results using the same amount\nof supervision as our method. Several qualitative segmenta-\ntion samples are provided on Figure 6.\n4.3. Is BigGAN’s supervision necessary for the\nsegmentation performance?\nIn Table 3 we compare our method with the approach pro-\nposed in (Voynov & Babenko, 2020). Though this method\nis not fully unsupervised, it is interesting to compare syn-\nthetic from supervised and unsupervised GANs. (Voynov\n& Babenko, 2020) utilized the “background removal” direc-\ntion in the BigGAN’s latent space to generate foreground\n/ background masks. As BigGAN has no encoder, we also\ncompare it with a weaker version of our method that uses\nthe prior latent distribution without any ﬁltering (see Ta-\nble 4, ﬁrst line). Notably, even without any adaptation to\nthe particular dataset and ﬁltering, our method performs on\npar with the “supervised” one. Enriched with the adaptation\nstep, our approach outperforms (Voynov & Babenko, 2020)\nwhile being unsupervised. These results are quite surprising\nsince BigGAN has remarkably higher generation quality\nwith the Fr´echet Inception Distance (FID) of 10.2 facing\n23.3 for BigBiGAN.\n4.4. Ablation.\nIn Table 4 we demonstrate the impact of individual compo-\nnents in our method. First, we start with a saliency detec-\ntion model trained on the synthetic data pairs {G(z), M =\n[G(z+hbg) > G(z)]} with z ∼N(0, I). Then we add one\nby one the components listed in Section 3.3 and Section 3.4.\nThe most signiﬁcant performance impact comes from using\nthe latent codes of the real images from the Imagenet.\n4.5. Synthetic Data Quality\nThis section compares the quality of saliency masks ob-\ntained with our method with the real ones. First, we eval-\nuate the consistency of the real and generated masks. We\nuse the SOTA publicly available saliency model1 to evalu-\nate the quality of our synthetic masks used for the best E-\n1https://github.com/NathanUA/U-2-Net\nObject Segmentation Without Labels with Large-Scale Generative Models\nMethod\nECSSD\nDUTS\nDUT-OMRON\nmaxFβ IoU Accuracy maxFβ IoU Accuracy maxFβ IoU Accuracy\nE-BigBiGAN with U2Net 0.813 0.674\n0.911\n0.654 0.525\n0.906\n0.663 0.559\n0.915\nE-BigBiGAN\n0.797 0.684\n0.906\n0.624 0.511\n0.882\n0.563 0.464\n0.860\nTable 5. Comparison of masks generation with supervised U2Net-guided synthetic labeling\nBigBiGAN run with the Imagenet embeddings. The model\nresults in 0.412 IoU and 0.720 accuracy on 105 random\nsamples, which is lower than our scheme’s performance on\nthe real datasets. We attribute such behavior to the fact that\nour synthetic data is often noisy, and the model trained on\nhuman-provided masks is not robust to the noise. In other\nwords, it is more beneﬁcial to train on difﬁcult, noisy data\nand to test on reﬁned, clean data rather than otherwise.\nTo understand whether the performance bottleneck of our\nmethod is the quality of generated images or the quality\nof masks, we perform the following experiment. Using\nthe same SOTA supervised model U2Net : R3×128×128 →\n{0, 1}128×128 as above, we take randomly sampled latent z\nformed by the E-BigBiGAN pipeline and form the dataset of\nthe pairs {G(z), U2Net(G(z))} were G is the BigBiGAN\ngenerator. In other words, we take the same images as in\nour best method and form the masks with a high-quality\nsaliency model pretrained on the real data. Then we train a\nU-net segmentation model on this data following the proto-\ncol described in Section 3.5. The comparison of the original\nmodel with the U2Net-guided model is presented in Table 5.\nNotably, the U2Net-guided model performs better, though\nit does not demonstrate a break-through outperformance on\ntwo datasets out of three. This result indicates that the bot-\ntleneck of our method mainly lies in the quality of generated\nimages rather than the mask generation approach.\n4.6. Independent Masks Estimation\nIn principle, the optimization problem (4) that induces the\nsegmentation masks could be solved independently for each\ngenerated image. We have tried to optimize the operators\nA1, A2 independently for each generated sample G(z) dur-\ning the synthetic data generation. In this case, the resulting\nmasks appear to be almost the same as masks produced by\nthe operators optimized for all synthetic images simultane-\nously. In Figure 7 we present the masks generated in the\noriginal protocol (red) and the masks generated with the\nper-sample optimization (green). Mutual IoU and accuracy\nof these masks computed over 128 randomly generated sam-\nples is 0.86 and 0.96 respectfully, therefore, they are very\nsimilar. Even though the latent shift may indeed act with a\ndifferent strength, the optimal operators’ indices from Equa-\ntion 2 seem to be rather persistent. We should also note that\nthis per-sample training protocol would require an enormous\nextra time as single batch generation takes approximately\n30 seconds.\nFigure 7. First row: generated images G(z); second row: masks\ngenerated with operators A1, A2 that optimize the common ob-\njective from Equation 3; third row: masks generated with the\noperators A1, A2 independently optimized for each sample.\n5. Conclusion\nIn our paper, we continue the line of works on unsuper-\nvised object segmentation with the aid of generative models.\nWhile the existing unsupervised techniques require adversar-\nial training, we introduce an alternative research direction\nbased on the high-quality synthetic data from the off-the-\nshelf GAN. Namely, we utilize the images produced by the\nBigBiGAN model, which is trained on the Imagenet dataset.\nExploring BigBiGAN, we have discovered that its latent\nspace semantics automatically create the saliency masks for\nsynthetic images via latent space manipulations. We pro-\npose to use the BigBiGAN’s encoder to ﬁt this pipeline for\na particular dataset. As shown in experiments, this synthetic\ndata is an excellent source of supervision for discriminative\ncomputer vision models. The main feature of our approach\nis its simplicity and reproducibility since our model does\nnot rely on a large number of components/hyperparameters.\nOn several standard benchmarks, we demonstrate that our\nmethod achieves superior performance compared to existing\nunsupervised competitors.\nReferences\nBenny, Y. and Wolf, L. Onegan: Simultaneous unsupervised\nlearning of conditional image generation, foreground seg-\nmentation, and ﬁne-grained clustering. ECCV, 2020.\nObject Segmentation Without Labels with Large-Scale Generative Models\nBielski, A. and Favaro, P. Emergence of object segmentation\nin perturbed generative models. In Advances in Neural\nInformation Processing Systems, pp. 7254–7264, 2019.\nBrock, A., Donahue, J., and Simonyan, K. Large scale\nGAN training for high ﬁdelity natural image synthesis. In\nInternational Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=B1xsqj09Fm.\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski,\nP., and Joulin, A. Unsupervised learning of visual fea-\ntures by contrasting cluster assignments. arXiv preprint\narXiv:2006.09882, 2020.\nChen, M., Arti`eres, T., and Denoyer, L.\nUnsupervised\nobject segmentation by redrawing. In Advances in Neural\nInformation Processing Systems, pp. 12705–12716, 2019.\nChen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan,\nD., and Sutskever, I. Generative pretraining from pixels.\nIn International Conference on Machine Learning, pp.\n1691–1703. PMLR, 2020a.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. arXiv preprint arXiv:2002.05709, 2020b.\nCheng, M.-M., Mitra, N. J., Huang, X., Torr, P. H., and\nHu, S.-M. Global contrast based salient region detection.\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence, 37(3):569–582, 2014.\nCong, R., Lei, J., Fu, H., Huang, Q., Cao, X., and Hou, C.\nCo-saliency detection for rgbd images based on multi-\nconstraint feature matching and cross label propagation.\nIEEE Transactions on Image Processing, 27(2):568–579,\n2017.\nCordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler,\nM., Benenson, R., Franke, U., Roth, S., and Schiele,\nB. The cityscapes dataset for semantic urban scene un-\nderstanding. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 3213–3223,\n2016.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\nFei, L. ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR09, 2009.\nDonahue, J. and Simonyan, K. Large scale adversarial rep-\nresentation learning. In Advances in Neural Information\nProcessing Systems, pp. 10541–10551, 2019.\nGrill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond,\nP. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo,\nZ. D., Azar, M. G., et al. Bootstrap your own latent: A\nnew approach to self-supervised learning. arXiv preprint\narXiv:2006.07733, 2020.\nGuo, F., Wang, W., Shen, J., Shao, L., Yang, J., Tao, D.,\nand Tang, Y. Y. Video saliency detection using object\nproposals. IEEE transactions on cybernetics, 48(11):\n3159–3170, 2017.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 9729–\n9738, 2020.\nJiang, H., Wang, J., Yuan, Z., Wu, Y., Zheng, N., and Li,\nS. Salient object detection: A discriminative regional\nfeature integration approach. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 2083–2090, 2013.\nKr¨ahenb¨uhl, P. and Koltun, V. Efﬁcient inference in fully\nconnected crfs with gaussian edge potentials. In Advances\nin neural information processing systems, pp. 109–117,\n2011.\nLi, N., Sun, B., and Yu, J. A weighted sparse coding frame-\nwork for saliency detection. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 5216–5223, 2015.\nNguyen, T., Dax, M., Mummadi, C. K., Ngo, N., Nguyen,\nT. H. P., Lou, Z., and Brox, T. Deepusps: Deep robust\nunsupervised saliency prediction via self-supervision. In\nAdvances in Neural Information Processing Systems, pp.\n204–214, 2019.\nNilsback, M.-E. and Zisserman, A. Delving into the whorl\nof ﬂower segmentation. In BMVC, volume 2007, pp.\n1–10, 2007.\nPeng, H., Li, B., Ling, H., Hu, W., Xiong, W., and May-\nbank, S. J. Salient object detection via structured matrix\ndecomposition. IEEE transactions on pattern analysis\nand machine intelligence, 39(4):818–832, 2016.\nRonneberger, O., Fischer, P., and Brox, T. U-net: Convolu-\ntional networks for biomedical image segmentation. In In-\nternational Conference on Medical image computing and\ncomputer-assisted intervention, pp. 234–241. Springer,\n2015.\nShi, J., Yan, Q., Xu, L., and Jia, J. Hierarchical image\nsaliency detection on extended cssd. IEEE transactions\non pattern analysis and machine intelligence, 38(4):717–\n729, 2015.\nVan Gansbeke, W., Vandenhende, S., Georgoulis, S., Proes-\nmans, M., and Van Gool, L. Scan: Learning to classify\nimages without labels. In European Conference on Com-\nputer Vision, pp. 268–285. Springer, 2020.\nObject Segmentation Without Labels with Large-Scale Generative Models\nVoynov, A. and Babenko, A. Unsupervised discovery of\ninterpretable directions in the gan latent space. arXiv\npreprint arXiv:2002.03754, 2020.\nWah, C., Branson, S., Welinder, P., Perona, P., and Belongie,\nS. The caltech-ucsd birds-200-2011 dataset. 2011.\nWang, L., Lu, H., Wang, Y., Feng, M., Wang, D., Yin, B.,\nand Ruan, X. Learning to detect salient objects with\nimage-level supervision. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npp. 136–145, 2017a.\nWang, L., Lu, H., Wang, Y., Feng, M., Wang, D., Yin, B.,\nand Ruan, X. Learning to detect salient objects with\nimage-level supervision. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npp. 136–145, 2017b.\nWang, W., Shen, J., Shao, L., and Porikli, F. Correspondence\ndriven saliency transfer. IEEE Transactions on Image\nProcessing, 25(11):5025–5034, 2016.\nWang, W., Lai, Q., Fu, H., Shen, J., and Ling, H. Salient\nobject detection in the deep learning era: An in-depth\nsurvey. arXiv preprint arXiv:1904.09146, 2019.\nWei, Y., Wen, F., Zhu, W., and Sun, J. Geodesic saliency\nusing background priors. In IEEE, ICCV, 2012.\nXiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A.\nSun database: Large-scale scene recognition from abbey\nto zoo. In 2010 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition, pp. 3485–3492.\nIEEE, 2010.\nYan, Q., Xu, L., Shi, J., and Jia, J. Hierarchical saliency\ndetection. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 1155–1162,\n2013.\nYang, C., Zhang, L., Lu, H., Ruan, X., and Yang, M.-H.\nSaliency detection via graph-based manifold ranking. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 3166–3173, 2013.\nZhang, D., Han, J., and Zhang, Y. Supervision by fusion: To-\nwards unsupervised learning of deep salient object detec-\ntor. In Proceedings of the IEEE International Conference\non Computer Vision, pp. 4048–4056, 2017.\nZhang, D., Tian, H., and Han, J. Few-cost salient object\ndetection with adversarial-paced learning. In Larochelle,\nH., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin,\nH. (eds.), Advances in Neural Information Processing\nSystems, volume 33, pp. 12236–12247. Curran Asso-\nciates, Inc., 2020.\nURL https://proceedings.\nneurips.cc/paper/2020/file/\n8fc687aa152e8199fe9e73304d407bca-Paper.\npdf.\nZhang, J., Zhang, T., Dai, Y., Harandi, M., and Hartley,\nR. Deep unsupervised saliency detection: A multiple\nnoisy labeling perspective. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npp. 9029–9038, 2018.\nZheltonozhskii, E., Baskin, C., Bronstein, A. M., and\nMendelson, A.\nSelf-supervised learning for large-\nscale unsupervised image clustering.\narXiv preprint\narXiv:2008.10312, 2020.\nZhu, W., Liang, S., Wei, Y., and Sun, J. Saliency optimiza-\ntion from robust background detection. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pp. 2814–2821, 2014a.\nZhu, W., Liang, S., Wei, Y., and Sun, J. Saliency optimiza-\ntion from robust background detection. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pp. 2814–2821, 2014b.\n",
  "categories": [
    "cs.LG",
    "cs.CV",
    "stat.ML"
  ],
  "published": "2020-06-08",
  "updated": "2021-06-11"
}