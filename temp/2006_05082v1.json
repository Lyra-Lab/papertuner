{
  "id": "http://arxiv.org/abs/2006.05082v1",
  "title": "Learning to Stop While Learning to Predict",
  "authors": [
    "Xinshi Chen",
    "Hanjun Dai",
    "Yu Li",
    "Xin Gao",
    "Le Song"
  ],
  "abstract": "There is a recent surge of interest in designing deep architectures based on\nthe update steps in traditional algorithms, or learning neural networks to\nimprove and replace traditional algorithms. While traditional algorithms have\ncertain stopping criteria for outputting results at different iterations, many\nalgorithm-inspired deep models are restricted to a ``fixed-depth'' for all\ninputs. Similar to algorithms, the optimal depth of a deep architecture may be\ndifferent for different input instances, either to avoid ``over-thinking'', or\nbecause we want to compute less for operations converged already. In this\npaper, we tackle this varying depth problem using a steerable architecture,\nwhere a feed-forward deep model and a variational stopping policy are learned\ntogether to sequentially determine the optimal number of layers for each input\ninstance. Training such architecture is very challenging. We provide a\nvariational Bayes perspective and design a novel and effective training\nprocedure which decomposes the task into an oracle model learning stage and an\nimitation stage. Experimentally, we show that the learned deep model along with\nthe stopping policy improves the performances on a diverse set of tasks,\nincluding learning sparse recovery, few-shot meta learning, and computer vision\ntasks.",
  "text": "Learning to Stop While Learning to Predict\nXinshi Chen 1 Hanjun Dai 2 Yu Li 3 Xin Gao 3 Le Song 1 4\nAbstract\nThere is a recent surge of interest in designing\ndeep architectures based on the update steps in\ntraditional algorithms, or learning neural networks\nto improve and replace traditional algorithms.\nWhile traditional algorithms have certain stop-\nping criteria for outputting results at different iter-\nations, many algorithm-inspired deep models are\nrestricted to a “ﬁxed-depth” for all inputs. Similar\nto algorithms, the optimal depth of a deep architec-\nture may be different for different input instances,\neither to avoid “over-thinking”, or because we\nwant to compute less for operations converged al-\nready. In this paper, we tackle this varying depth\nproblem using a steerable architecture, where a\nfeed-forward deep model and a variational stop-\nping policy are learned together to sequentially\ndetermine the optimal number of layers for each\ninput instance. Training such architecture is very\nchallenging. We provide a variational Bayes per-\nspective and design a novel and effective training\nprocedure which decomposes the task into an or-\nacle model learning stage and an imitation stage.\nExperimentally, we show that the learned deep\nmodel along with the stopping policy improves\nthe performances on a diverse set of tasks, in-\ncluding learning sparse recovery, few-shot meta\nlearning, and computer vision tasks.\n1. Introduction\nRecently, researchers are increasingly interested in the con-\nnections between deep learning models and traditional algo-\nrithms: deep learning models are viewed as parameterized\nalgorithms that operate on each input instance iteratively,\nand traditional algorithms are used as templates for design-\ning deep learning architectures. While an important con-\n1Georgia Institute of Technology, USA 2Google Research, USA\n3King Abdullah University of Science and Technology, Saudi\nArabia 4Ant Financial, China. Correspondence to: Xinshi Chen\n<xinshi.chen@gatech.edu>, Le Song <lsong@cc.gatech.edu>.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by\nthe author(s).\nTask 1\nFixed-depth Learned Algorithm\n𝒙\n…\n𝑓#$ 𝑓#%\n𝑓#&\n𝒙𝑻\n(a):  Learning-based Algorithm design\nTask 2\n𝜃)\n𝜃*+,-.\n𝜃*+,-/\n∇#ℒ.\n∇#ℒ/\n(b):  Task-imbalanced Meta Learning\n…\n𝒙𝒕\n(output)\nDynamic-depth Traditional Algorithm\n𝒙\n𝒙𝒕\nsatisfied\nnot satisfied\ncriteria\nhand-designed\nupdate step\nD\nFigure 1. Motivation for learning to stop.\ncept in traditional algorithms is the stopping criteria for\noutputting the result, which can be either a convergence\ncondition or an early stopping rule, such stopping criteria\nhas been more or less ignored in algorithm-inspired deep\nlearning models. A “ﬁxed-depth” deep model is used to\noperate on all problem instances (Fig. 1 (a)). Intuitively,\nfor deep learning models, the optimal depth (or the opti-\nmal number of steps to operate on an input) can also be\ndifferent for different input instances, either because we\nwant to compute less for operations converged already, or\nwe want to generalize better by avoiding “over-thinking”.\nSuch motivation aligns well with both the cognitive science\nliterature (Jones et al., 2009) and many examples below:\n• In learning to optimize (Andrychowicz et al., 2016; Li &\nMalik, 2016), neural networks are used as the optimizer\nto minimize some loss function. Depending on the initial-\nization and the objective function, an optimizer should\nconverge in different number of steps;\n• In learning to solve statistical inverse problems such as\ncompressed sensing (Chen et al., 2018; Liu et al., 2019),\ninverse covariance estimation (Shrivastava et al., 2020),\nand image denoising (Zhang et al., 2019), deep mod-\nels are learned to directly predict the recovery results.\nIn traditional algorithms, problem-dependent early stop-\nping rules are widely used to achieve regularization for a\nvariance-bias trade-off. Deep learning models for solving\nsuch problems maybe also achieve a better recovery ac-\ncuracy by allowing instance-speciﬁc computation steps;\n• In meta learning, MAML (Finn et al., 2017) used an\nunrolled and parametrized algorithm to adapt a common\narXiv:2006.05082v1  [cs.LG]  9 Jun 2020\nLearning to Stop While Learning to Predict\n𝒙\n𝜃#\n𝒙𝟏\n𝜃%\n𝒙𝟐\n𝜙\n𝜋#\n< 0.5\n𝜃-\n𝜙\n𝜋%\n< 0.5\n…\n𝜃.\n𝒙𝒕\n𝜙\n𝜋.≥0.5\nstop, output 𝒙𝒕\n…\nFigure 2. Two-component model: learning to predict (blue) while\nlearning to stopping (green).\nparameter to a new task. However, depending on the\nsimilarity of the new task to the old tasks, or, in a more\nrealistic task-imbalanced setting where different tasks\nhave different numbers of data points (Fig. 1 (b)), a task-\nspeciﬁc number of adaptation steps is more favorable to\navoid under or over adaption.\nTo address the varying depth problem, we propose to learn\na steerable architecture, where a shared feed-forward model\nfor normal prediction and an additional stopping policy\nare learned together to sequentially determine the optimal\nnumber of layers for each input instance. In our framework,\nthe model consists of (see Fig. 2)\n• A feed-forward or recurrent mapping Fθ, which trans-\nforms the input x to generate a path of features (or states)\nx1, · · · , xT ; and\n• A stopping policy πφ : (x, xt) 7→πt ∈[0, 1], which se-\nquentially observes the states and then determines the\nprobability of stopping the computation of Fθ at layer t.\nThese two components allow us to sequentially predict the\nnext targeted state while at the same time determining when\nto stop. In this paper, we propose a single objective function\nfor learning both θ and φ, and we interpret it from the per-\nspective of variational Bayes, where the stopping time t is\nviewed as a latent variable conditioned on the input x. With\nthis interpretation, learning θ corresponds to maximizing\nthe marginal likelihood, and learning φ corresponds to the\ninference step for the latent variable, where a variational\ndistribution qφ(t) is optimized to approximate the posterior.\nA natural algorithm for solving this problem could be the\nExpectation-Maximization (EM) algorithm, which can be\nvery hard to train and inefﬁcient.\nHow to learn θ and φ effectively and efﬁciently? We propose\na principled and effective training procedure, where we\ndecompose the task into an oracle model learning stage and\nan imitation learning stage (Fig. 3). More speciﬁcally,\n• During the oracle model learning stage, we utilize a\nclosed-form oracle stopping distribution q∗|θ which can\nleverage label information not available at testing time.\n• In the imitation learning stage, we use a sequential policy\nπφ to mimic the behavior of the oracle policy obtained in\nthe ﬁrst stage. The sequential policy does not have access\nto the label so that it can be used during testing phase.\nThis procedure provides us a very good initial predictive\nmax\n$  𝒥𝛽−VAE(ℱ𝜃, 𝑞𝜙)\nAlternating Updates\nℱ𝜽\n𝒒𝝓\nVAE-based method:\nStage I.\nOur method:\n𝑞∗|𝜃\nℱ𝜽\nmin\n𝝓KL(        ,         )\nmax\n6  𝒥𝛽−VAE(ℱ𝜃, 𝑞𝜙)\noracle\nmax\n6  𝒥𝛽−VAE(ℱ𝜃, 𝑞∗|𝜃)\noptimal 𝜽∗\nStage II.\n𝑞∗|𝜽∗\noracle\n𝒒𝝓\noptimal 𝝓∗\nFigure 3. Two-stage training framework.\nmodel and a stopping policy. We can either directly use these\nlearned models, or plug them back to the variational EM\nframework and reiterate to further optimize both together.\nOur proposed learning to stop method is a generic frame-\nwork that can be applied to a diverse range of applications.\nTo summarize, our contribution in this paper includes:\n1. a variational Bayes perspective to understand the pro-\nposed model for learning both the predictive model and\nthe stopping policy together;\n2. a principled and efﬁcient algorithm for jointly learning\nthe predictive model and the stopping policy; and the\nrelation of this algorithm to reinforcement learning;\n3. promising experiments on various tasks including learn-\ning to solve sparse recovery problems, task-imbalanced\nfew-shot meta learning, and computer vision tasks, where\nwe demonstrate the effectiveness of our method in terms\nof both the prediction accuracy and inference efﬁciency.\n2. Related Works\nUnrolled algorithm. A line of recent works unfold and\ntruncate iterative algorithms to design neural architectures.\nThese algorithm-based deep models can be used to automat-\nically learn a better algorithm from data. This idea has been\ndemonstrated in different problems including sparse signal\nrecovery (Gregor & LeCun, 2010; Sun et al., 2016; Borg-\nerding et al., 2017; Metzler et al., 2017; Zhang & Ghanem,\n2018; Chen et al., 2018; Liu et al., 2019), sparse inverse\ncovariance estimation (Shrivastava et al., 2020), sequential\nBayesian inference (Chen et al., 2019), parameter learning\nin graphical models (Domke, 2011), non-negative matrix\nfactorization (Yakar et al., 2013), etc. Unrolled algorithm\nbased deep module has also be used for structured prediction\n(Belanger et al., 2017; Ingraham et al., 2019; Chen et al.,\n2020). Before the training phase, all these works need to\nassign a ﬁxed number of iterations that is used for every\nLearning to Stop While Learning to Predict\ninput instance regardless of their varying difﬁculty level.\nOur proposed method is orthogonal and complementary to\nall these works, by taking the variety of the input instances\ninto account via adaptive stopping time.\nMeta learning. Optimization-based meta learning techniq-\nues are widely applied for solving challenging few-shot\nlearning problems (Ravi & Larochelle, 2017; Finn et al.,\n2017; Li et al., 2017). Several recent advances proposed\ntask-adaptive meta-learning models which incorporate task-\nspeciﬁc parameters (Qiao et al., 2018; Lee & Choi, 2018;\nNa et al., 2020) or task-dependent metric scaling (Oreshkin\net al., 2018). In parallel with these task-adaptive methods,\nwe propose a task-speciﬁc number of adaptation steps and\ndemonstrate the effectiveness of this simple modiﬁcation\nunder the task-imbalanced scenarios.\nOther adaptive-depth deep models. In image recognition,\n‘early exits’ is proposed mainly aimed at improving the\ncomputation efﬁciency during the inference phase (Teer-\napittayanon et al., 2016; Zamir et al., 2017; Huang et al.,\n2018), but these methods are based on speciﬁc architectures.\nKaya et al. (2019) proposed to avoiding “over-thinking” by\nearly stopping. However, the same as all the other ‘early\nexits’ models, some heuristic policies are adopted to choose\nthe output layer by conﬁdence scores of internal classiﬁers.\nAlso, their algorithms for training the feed-forward model\nFθ do not take into account the effect of the stopping policy.\nOptimal stopping. In optimal control literature, optimal\nstopping is a problem of choosing a time to take a given ac-\ntion based on sequentially observed random variables in or-\nder to maximize an expected payoff (Shiryaev, 2007). When\na policy for controlling the evolution of random variables\n(corresponds to the output of Fθ) is also involved, it is called\na “mixed control” problem, which is highly related to our\nwork. Existing works in this area ﬁnd the optimal controls\nby solving the Hamilton-Jacobi-Bellman (HJB) equation,\nwhich is theoretically grounded (Pham, 1998; Ceci & Bas-\nsan, 2004; Dumitrescu et al., 2018). However, they focus\non stochastic differential equation based model and the pro-\nposed algorithms suffer from the curse of dimensionality\nproblem. Becker et al. (2019) use DL to learn the optimal\nstopping policy, but the learning of θ is not considered. Be-\nsides, Becker et al. (2019) use reinforcement learning (RL)\nto solve the problem. In Section 4, we will discuss how our\nvariational inference formulation is related to RL.\n3. Problem Formulation\nIn this section, we will introduce how we model the stopping\npolicy together with the predictive deep model, deﬁne the\njoint optimization objective, and interpret this framework\nfrom a variational Bayes perspective.\n3.1. Steerable Model\nThe predictive model, Fθ, is a typical T-layer deep model\nthat generates a path of embeddings (x1, · · · , xT ) through:\nPredictive model:\nxt = fθt(xt−1), for t= 1, · · · , T (1)\nwhere the initial x0 is determined by the input x. We denote\nit by Fθ = {fθ1, · · · , fθT } where θ ∈Θ are the parameters.\nStandard supervised learning methods learn θ by optimizing\nan objective estimated on the ﬁnal state xT . In our model,\nthe operations in Eq. 1 can be stopped earlier, and for differ-\nent input instance x, the stopping time t can be different.\nOur stopping policy, πφ, determines whether to stop at t-th\nstep after observing the input x and its ﬁrst t states x1:t\ntransformed by Fθ. If we assume the Markov property, then\nπφ only needs to observe the most recent state xt. In this\npaper, we only input x and xt to πφ at each step t, but it\nis trivial to generalize it ππ(x, x1:t). More precisely, πφ is\ndeﬁned as a randomized policy as follows:\nStopping policy: πt = πφ(x, xt), for t= 1, · · · , T −1 (2)\nwhere πt ∈[0, 1] is the probability of stopping. We abuse\nthe notation π to both represent the parametrized policy and\nalso the probability mass.\nThis stopping policy sequentially makes a decision when-\never a new state xt is observed. Conditioned on the states\nobserved until step t, whether to stop before t is independent\non states after t. Therefore, once it decides to stop at t, the\nremaining computations can be saved, which is a favorable\nproperty when the inference time is a concern, or for some\noptimal stopping problems such as option trading where\ngetting back to earlier states is not allowed.\n3.2. From Sequential Policy To Stop Time Distribution\nThe stopping policy πφ makes sequential actions based on\nthe observations, where πt := πφ(x, xt) is the probability\nof stopping when xt is observed. These sequential actions\nπ1, · · · , πT −1 jointly determines the random time t at which\nthe stop occurs. Induced by πφ, the probability mass func-\ntion of the stop time t, denoted as qφ, can be computed by\nVariational stop time distribution:\n(\nqφ(t) = πt\nQt−1\nτ=1(1 −πτ)\nif t < T,\nqφ(T) = QT −1\nτ=1 (1 −πτ)\nelse.\n(3)\nIn this equation, the product Qt−1\nτ=1(1 −πτ) indicates the\nprobability of ‘not stopped before t’, which is the survival\nprobability. Multiply this survival probability with πt, we\nhave the stop time distribution qφ(t). For the last time step\nT, the stop probability qφ(T) simply equals to the survival\nprobability at T, which means if the process is ‘not stopped\nbefore T’, then it must stop at T.\nLearning to Stop While Learning to Predict\nNote that we only use πφ in our model to sequentially de-\ntermine whether to stop. However, we use the induced\nprobability mass qφ to help design the training objective and\nalso the algorithm.\n3.3. Optimization Objective\nNote that the stop time t is a discrete random variable with\ndistribution determined by qφ(t). Given the observed label\ny of an input x, the loss of the predictive model stopped at\nposition t can computed as ℓ(y, xt; θ) where ℓ(·) is a loss\nfunction. Taking into account all possible stopping positions,\nwe will be interested in the loss in expectation over t,\nL(θ, qφ; x, y) := Et∼qφℓ(y, xt; θ) −βH(qφ),\n(4)\nwhere H(qφ) := −P\nt qφ(t) log qφ(t) is an entropy regu-\nlarization and β is the regularization coefﬁcient. Given a\ndata set D = {(x, y)}, the parameters of the predictive\nmodel and the stopping policy can be estimated by\nminθ,φ\n1\n|D|\nP\n(x,y)∈D L(θ, qφ; x, y).\n(5)\nTo better interpret the model and objective, in the following,\nwe will make a connection from the perspective of vari-\national Bayes, and how the objective function deﬁned in\nEq. 4 is equivalent to the β-VAE objective.\n3.4. Variational Bayes Perspective\nIn the Bayes’ framework, a probabilistic model typically\nconsists of prior, likelihood function and posterior of the\nlatent variable. We ﬁnd the correspondence between our\nmodel and a probabilistic model as follows (also see Table 1)\n• we view the adaptive stopping time t as a latent variable\nwhich is unobserved;\n• The conditional prior p(t|x) of t is a uniform distribution\nover all the layers in this paper. However, if one wants\nto reduce the computation cost and penalize the stopping\ndecisions at deeper layers, a prior with smaller probability\non deeper layers can be deﬁned to regularize the results;\n• The likelihood function pθ(y|t, x) of the observed label\ny is controlled by θ, since Fθ determines the states xt;\n• The posterior distribution over the stopping time t can be\ncomputed by Bayes’ rule pθ(t|y, x) ∝pθ(y|t, x)p(t|x),\nbut it requires the observation of the label y, which is\ninfeasible during testing phase.\nTable 1. Corresponds between our model and Bayes’ model.\nstop time t\nlatent variable\nlabel y\nobservation\nloss ℓ(y, xt; θ)\nlikelihood pθ(y|t, x)\nstop time distribution qφ\nposterior pθ(t|y, x)\nregularization\nprior p(t|x)\nIn this probabilistic model, we need to learn θ to better ﬁt\nthe observed data and learn a variational distribution qφ over\nt that only takes x and the transformed internal states as\ninputs to approximate the true posterior.\nMore speciﬁcally, the parameters in the likelihood function\nand the variational posterior can be optimized using the vari-\national autoencoder (VAE) framework (Kingma & Welling,\n2013). Here we consider a generalized version called β-\nVAE (Higgins et al., 2017), and obtain the optimization\nobjective for data point (x, y)\nJβ-VAE(θ, qφ; x, y) :=\nEqφ log pθ(y|t,x) −βKL(qφ(t)||p(t|x)),\n(6)\nwhere KL(·||·) is the KL divergence. When β = 1, it\nbecomes the original VAE objective, i.e., the evidence lower\nbound (ELBO). Now we are ready to present the equivalence\nrelation between the β-VAE objective and the loss deﬁned\nin Eq. 4. See Appendix A.1 for the proof.\nLemma 1. Under assumptions: (i) the loss function ℓin\nEq. 4 is deﬁned as the negative log-likelihood (NLL), i.e.,\nℓ(y, xt; θ) := −log pθ(y|t, x);\n(ii) the prior p(t|x) is a uniform distribution over t;\nthen minimizing the loss L in Eq. 4 is equivalent to maxi-\nmizing the β-VAE objective Jβ-VAE in Eq. 6.\nFor classiﬁcation problems, the cross-entropy loss is aligned\nwith NLL. For regression problems with mean squared error\n(MSE) loss, we can deﬁne the likelihood as pθ(y|t, x) ∼\nN(xt, I). Then the NLL of this Gaussian distribution is\n−log pθ(y|t, x) =\n1\n2∥y −xt∥2\n2 + C, which is equiva-\nlent to MSE loss. More generally, we can always deﬁne\npθ(y|t, x) ∝exp(−ℓ(y, xt; θ)).\nThis VAE view allows us to design a two-step procedure\nto effectively learn θ and φ in the predictive model and\nstopping policy, which is presented in the next section.\n4. Effective Training Algorithm\nVAE-based methods perform optimization steps over θ (M\nstep for learning) and φ (E step for inference) alternatively\nuntil convergence, which has two limitations in our case:\ni. The alternating training can be slow to converge and\nrequires tuning the training scheduling;\nii. The inference step for learning qφ may have the mode col-\nlapse problem, which in this case means qφ only captures\nthe time step t with highest averaged frequency.\nTo overcome these limitations, we design a training proce-\ndure followed by an optional ﬁne-tuning stage using the\nvariational lower bound in Eq. 6. More speciﬁcally,\nStage I. Find the optimal θ by maximizing the conditional\nmariginal likelihood when the stop time distribution follows\nan oracle distribution q∗\nθ.\nLearning to Stop While Learning to Predict\nStage II. Fix the optimal θ learned in Stage I, and only learn\nthe distribution qφ to mimic the oracle by minimizing the\nKL divergence between qφ and q∗\nθ.\nStage III. (Optional) Fine-tune θ and φ jointly towards the\njoint objective in Eq. 6.\nThe overall algorithm steps are summarized in Algorithm 1.\nIn the following sections, we will focus on the derivation\nof the ﬁrst two training steps. Then we will discuss several\nmethods to further improve the memory and computation\nefﬁciency for training.\n4.1. Oracle Stop Time Distribution\nWe ﬁrst give the deﬁnition of the oracle stop time distribu-\ntion q∗\nθ. For each ﬁxed θ, we can ﬁnd a closed-form solution\nfor the optimal q∗\nθ that optimizes the joint objective.\nq∗\nθ(·|y, x) := arg maxq∈∆T −1 Jβ-VAE(θ, q; x, y)\nAlternatively, q∗\nθ(·|y, x) = arg minq∈∆T −1 L(θ, q; x, y).\nUnder the mild assumptions in Lemma 1, these two opti-\nmizations lead to the same optimal oracle distribution.\nOracle stop time distribution:\nq∗\nθ(t|y, x) =\npθ(y|t, x)\n1\nβ\nPT\nt=1 pθ(y|t, x)\n1\nβ\n(7)\n=\nexp(−1\nβ ℓ(y, xt; θ))\nPT\nt=1 exp(−1\nβ ℓ(y, xt; θ))\n(8)\nThis closed-form solution makes it clear that the oracle picks\na step t according to the smallest loss or largest likelihood\nwith an exploration coefﬁcient β.\nRemark: When β = 1, q∗\nθ is the same as the posterior\ndistribution pθ(t|y, x) ∝pθ(y|t, x)p(t|x).\nNote that there are no new parameters in the oracle dis-\ntribution. Instead, it depends on the parameters θ in the\npredictive model. Overall, the oracle q∗\nθ is a function of θ,\nt, y and x that has a closed-form. Next, we will introduce\nhow we use this oracle in the ﬁrst two training stages.\n4.2. Stage I. Predictive Model Learning\nIn Stage I, we optimize the parameters θ in the predictive\nmodel by taking into account the oracle stop distribution\nq∗\nθ . This step corresponds to the M step for learning θ, by\nmaximizing the marginal likelihood. The difference with\nthe normal M step is that here qφ is replaced by the oracle\nq∗\nθ that gives the optimal stopping distribution so that the\nmarginal likelihood is independent on φ. More precisely,\nstage I ﬁnds the optimum of:\nmax\nθ\n1\n|D|\nX\n(x,y)∈D\nT\nX\nt=1\nq∗\nθ(t|y, x) log pθ(y|t, x),\n(9)\nAlgorithm 1 Overall Algorithm\nRandomly initialized θ and φ.\nFor itr = 1 to #iterations do\n▷Stage I.\nSample a batch of data points B ∼D.\nTake an optimization step to update θ towards the\nmarginal likelihood function deﬁned in Eq. 9.\nFor itr = 1 to #iterations do\n▷Stage II.\nSample a batch of data points B ∼D.\nTake an optimization step to update φ towards the re-\nverse KL divergence deﬁned in Eq. 10.\nFor itr = 1 to #iterations do\n▷Optional Step\nSample a batch of data points B ∼D.\nUpdate both θ and φ towards β-VAE objective in Eq. 6.\nreturn θ, φ\nwhere the summation over t is the expectation of the like-\nlihood, Et∼q∗\nθ(t|y,x) log pθ(y|t, x). Since q∗\nθ has a differ-\nentiable closed-form expression in terms of θ, x, y and t,\nthe gradient can also propagate through q∗\nθ, which is also\ndifferent from the normal M step.\nTo summarize, in Stage I., we learn the predictive model\nparameter θ, by assuming that the stop time always follows\nthe best stopping distribution that depends on θ. In this case,\nthe learning of θ has already taken into account the effect of\nthe data-speciﬁc stop time.\nHowever, we note that the oracle q∗\nθ is not in the form of\nsequential actions as in Eq. 2 and it requires the access to\nthe true label y, so it can not be used for testing. However,\nit plays an important role in obtaining a sequential policy\nwhich will be explained next.\n4.3. Stage II. Imitation With Sequential Policy\nIn Stage II, we learn the sequential policy πφ that can best\nmimic the oracle distribution q∗\nθ, where θ is ﬁxed to be\nthe optimal θ learned in Stage I. The way of doing so is\nto minimize the divergence between the oracle q∗\nθ and the\nvariational stop time distribution qφ induced by πφ (Eq. 3).\nThere are various variational divergence minimization ap-\nproaches that we can use (Nowozin et al., 2016). For exam-\nple, a widely used objective for variational inference is the\nreverse KL divergence:\nKL(qφ||q∗\nθ) = PT\nt=1 −qφ(t) log q∗\nθ(t|y, x) −H(qφ).\nRemark. We write qφ(t) instead of qφ(t|x1:T , x) for nota-\ntion simplicity, but qφ is dependent on x and x1:T (Eq. 3).\nIf we rewrite qφ using π1, · · · , πT −1 as deﬁned in Eq. 3,\nwe can ﬁnd that minimizing the reverse KL is equivalent to\nﬁnding the optimal policy πφ in a reinforcement learning\n(RL) environment, where the state is xt, action at ∼πt :=\nπφ(x, xt) is a stop/continue decision, the state transition is\nLearning to Stop While Learning to Predict\ndetermined by θ and at, and the reward is deﬁned as\nr(xt, at; y) :=\n(\n−βℓ(y, xt; θ)\nif at = 0 (i.e. stop)\n0\nif at = 1 (i.e. continue)\nwhere ℓ(y, xt; θ) = −log pθ(y|t, x). More detials and also\nthe derivation are given in Appendix A.2 to show that min-\nimizing KL(qφ||q∗\nθ) is equivalent to solving the following\nmaximum-entropy RL:\nmax\nφ\nEπφ\nPT\nt=1 [r(xt, at; y) + H(πt)] .\nIn some related literature, optimal stopping problem is often\nformulated as an RL problem (Becker et al., 2019). Above\nwe bridge the connection between our variational inference\nformulation and the RL-based optimal stopping literature.\nAlthough reverse KL divergence is a widely used objective,\nit suffers from the mode collapse issue, which in our case\nmay lead to a distribution qφ that captures only a common\nstopping time t for all x that on average performs the best,\ninstead of a more spread-out stopping time. Therefore, we\nconsider the forward KL divergence:\nKL(qφ||q∗\nθ) = −\nT\nX\nt=1\nq∗\nθ(t|y, x) log qφ(t) −H(q∗\nθ), (10)\nwhich is equivalent to the cross-entropy loss, since the term\nH(q∗\nθ) can be ignored as θ is ﬁxed in this step. Experimen-\ntally, we ﬁnd forward KL leads to a better performance.\n4.4. The Optional Fine Tuning Stage\nIt is easy to see that our two-stage training procedure also\nhas an EM ﬂavor. However, with the oracle q∗\nθ incorporated,\nthe training of θ has already taken into account the effect of\nthe optimal stopping distribution. Therefore, we can save a\nlot of alternation steps. After the two-stage training, we can\nﬁne-tune θ and φ jointly towards the β-VAE objective. Ex-\nperimentally, we ﬁnd this additional stage does not improve\nmuch the performance trained after the ﬁrst two stages.\n4.5. Implementation Details For Efﬁcient Training\nSince both objectives in oracle learning stage (Eq. 9) and\nimitation stage (Eq. 10) involve the summation over T lay-\ners, the computation and memory costs during training are\nhigher than standard learning methods. The memory issue is\nespecially important in meta learning. In the following, we\nintroduce several ways of improving the training efﬁciency.\nFewer output channels. Instead of allowing the model to\noutput xt at any layer, we can choose a smaller number of\noutput channels that are evenly placed along with the layers.\nStochastic sampling in Step I. A Monte Carlo method can\nbe used to approximate the expectation over q∗\nθ in Step I.\nMore precisely, for each (x, y) we can randomly sample\na layer ts ∼q∗\nθ(t|y, x) from the oracle, and only compute\nlog pθ(y|ts, x) at ts, instead of summing over all t ∈[T].\nNote that, in this case, the gradient will not back-propagate\nthrough q∗\nθ(t|y, x).\nMAP estimate in Step II. Instead of approximating the\ndistribution q∗\nθ, we can approximate the maximum a pos-\nterior (MAP) estimate ˆt(x, y) = arg maxt∈[T ] q∗\nθ(t|y, x)\nso that the objective for each sample is −log qθ(ˆt(x, y)),\nwhich does not involve the summation over t. Except for\nefﬁciency, we also ﬁnd this MAP estimate can lead to a\nhigher accuracy, by encouraging the learning of qφ to focus\nmore on the sample-wise best layer.\n5. Experiments\nWe conduct experiments on (i) learning-based algorithm for\nsparse recovery, (ii) few-shot meta learning, and (iii) image\ndenoising. The comparison is in an ablation study fashion to\nbetter examine whether the stopping policy can improve the\nperformances given the same architecture for the predictive\nmodel, and whether our training algorithm is more effective\ncompared to the alternating EM algorithm. In the end, we\nalso discuss our exploration of the image recognition task.\n5.1. Learning To Optimize: Sparse Recovery\nWe consider a sparse recovery task which aims at recovering\nx∗∈Rn from its noisy linear measurements b = Ax∗+ ϵ,\nwhere A ∈Rm×n, ϵ ∈Rm is Gaussian white noise, and\nm ≪n. A popular approach is to model the problem as the\nLASSO formulation minx 1\n2∥b−Ax∥2\n2 +ρ∥x∥1 and solves\nit using iterative methods such as the ISTA (Blumensath &\nDavies, 2008) and FISTA (Beck & Teboulle, 2009) algo-\nrithms. We choose the most popular model named Learned\nISTA (LISTA) as the baseline and also as our predictive\nmodel. LISTA is a T-layer network with update steps:\nxt = ηλt(W 1\nt b + W 2\nt xt−1),\nt = 1, · · · , T,\n(11)\nwhere θ = {(λt, W 1\nt , W 2\nt )}T\nt=1 are leanable parameters.\nExperiment setting. We follow Chen et al. (2018) to gen-\nerate the samples. The signal-to-noise ratio (SNR) for each\nsample is uniformly sampled from 20, 30, and 40. The train-\ning loss for LISTA is PT\nt=1 γT −t∥xt −x∗∥2\n2 where γ ≤1.\nIt is commonly used for algorithm-based deep learning, so\nthat there is a supervision signal for every layer. For ISTA\nand FISTA, we use the training set to tune the hyperparam-\neters by grid search. See Appendix B.1 for more details.\nRecovery performance. (Table 2) We report the NMSE\n(in dB) results for each model/algorithm evaluated on 1000\nﬁxed test samples per SNR level. It is revealed in Table 2\nthat learning-based methods have better recovery perfor-\nLearning to Stop While Learning to Predict\nTable 2. Recovery performances of different algorithms/models.\nSNR\nmixed\n20\n30\n40\nFISTA (T = 100)\n-18.96\n-16.75\n-20.46\n-20.97\nISTA (T = 100)\n-14.66\n-13.99\n-14.99\n-15.07\nISTA (T = 20)\n-9.17\n-9.12\n-9.24\n-9.16\nFISTA (T = 20)\n-11.12\n-10.98\n-11.19\n-11.19\nLISTA (T = 20)\n-17.53\n-16.53\n-18.07\n-18.20\nLISTA-stop (T ⩽20)\n-22.41\n-20.29\n-23.90\n-24.21\nmances, especially for the more difﬁcult tasks (i.e. when\nSNR is 20). Compared to LISTA, our proposed adaptive-\nstopping method (LISTA-stop) signiﬁcantly improve recov-\nery performance. Also, LISTA-stop with ⩽20 iterations\nperforms better than ISTA and FISTA with 100 iterations,\nwhich indicates a better convergence.\nStopping distribution. The stop time distribution qφ(t) in-\nduced by πφ can be computed via Eq. 3. We report in Fig. 4\nthe stopping distribution averaged over the test samples,\nfrom which we can see that with a high probability LISTA-\nstop terminates the process before arriving at 20-th iteration.\n14\n15\n16\n17\n18\n19\n20\niteration t\n0.0\n0.2\n0.4\n0.6\n0.8\nq (t)\n0.001\n0.683\n0.267\n0.0370.0060.0040.002\n0\n5\n10\n15\n20\niteration t\n20\n15\n10\n5\n0\naveraged NMSE\nLISTA-stop\nLISTA\nISTA\nFISTA\n(a) stop time distribution\n(b) convergence\nFigure 4. Left: Stop time distribution\n1\n|Dtest|\nP\nx∈Dtest qφ(t|x)\naveraged over the test set. Right: Convergence of different al-\ngorithms. For LISTA-stop, the NMSE weighted by the stopping\ndistribution qφ is plotted. In the ﬁrst 13 iterations qφ(t) = 0, so\nno red dots are plotted.\nConvergence comparison. Fig. 4 shows the change of\nNMSE as the number of iterations increases. Since LISTA-\nstop outputs the results at different iteration steps, it is not\nmeaningful to draw a uniﬁed convergence curve. Therefore,\nwe plot the NMSE weighted by the stopping distribution qφ,\ni.e., 10 log10(\nPN\ni=1 qφ(t|i)∥xt−x∗,i∥2\n2\nPN\ni=1 qφ(t|i)\n/(\nPN\ni=1 ∥x∗,i∥2\n2\nN\n), using\nthe red dots. We observe that for LISTA-stop the expected\nNMSE increases as the number of iterations increase, this\nmight indicate that the later stopped problems are more\ndifﬁcult to solve. Besides, at 15th iteration, the NMSE in\nFig. 4 (b) is the smallest, while the averaged stop probability\nmass qφ(15) in Fig. 4 (a) is the highest.\nTable 3. Different algorithms for training LISTA-stop.\nSNR\nmixed\n20\n30\n40\nAEVB algorithm\n-21.92\n-19.92\n-23.27\n-23.58\nStage I. + II.\n-22.41\n-20.29\n-23.90\n-24.21\nStage I.+II.+III.\n-22.78\n-20.59\n-24.29\n-24.73\nAblation study on training algorithms. To show the ef-\nfectiveness of our two-stage training, in Table 3, we com-\npare the results with the auto-encoding variational Bayes\n(AEVB) algorithm (Le et al., 2018) that jointly optimizes\nFθ and qφ. We observe that the distribution qφ in AEVB\ngradually becomes concentrated on one layer and does not\nget rid of this local minimum, making its ﬁnal result not as\ngood as the results of our two-stage training. Moreover, it is\nrevealed that Stage III does not improve much of the perfor-\nmance of the two-stage training, which also in turn shows\nthe effectiveness of the oracle-based two-stage training.\n5.2. Task-imbalanced Meta Learning\nIn this section, we perform meta learning experiments in the\nfew-short learning domain (Ravi & Larochelle, 2017).\nExperiment setting. We follow the setting in MAML (Finn\net al., 2017) for the few-shot learning tasks. Each task is an\nN-way classiﬁcation that contains meta-{train, valid, test}\nsets. On top of it, the macro dataset with multiple tasks is\nsplit into train, valid and test sets. We consider the more re-\nalistic task-imbalanced setting proposed by Na et al. (2020).\nUnlike the standard setting where the meta-train of each\ntask contains k-shots for each class, here we vary the num-\nber of observation to perform k1- k2-shot learning where\nk1 < k2 are the minimum/maximum number of observa-\ntions per class, respectively. Build on top of MAML, we\ndenote our variant as MAML-stop which learns how many\nadaptation gradient descent steps are needed for each task.\nIntuitively, the tasks with less training data would prefer\nfewer steps of gradient-update to prevent overﬁtting. As we\nmainly focus on the effect of learning to stop, the neural\narchitecture and other hyperparameters are largely the same\nas MAML. Please refer to Appendix B.2 for more details.\nDataset. We use the benchmark datasets Omniglot (Lake\net al., 2011) and MiniImagenet (Ravi & Larochelle, 2017).\nOmniglot consists of 20 instances of 1623 characters from\n50 different alphabets, while MiniImagenet involves 64\ntraining classes, 12 validation classes, and 24 test classes.\nWe use exactly the same data split as Finn et al. (2017). To\nconstruct the imbalanced tasks, we perform 20-way 1-5\nshot classiﬁcation on Omniglot and 5-way 1-10 shot clas-\nsiﬁcation on MiniImagenet. The number of observations\nper class in each meta-test set is 1 and 5 for Omniglot and\nMiniImagenet, respectively. For evaluation, we construct\n600 tasks from the held-out test set for each setting.\nTable 4. Task-imbalanced few-shot image classiﬁcation.\nOmniglot\nMiniImagenet\n20-way, 1-5 shot\n5-way, 1-10 shot\nMAML\n97.96 ± 0.3%\n57.20 ± 1.1%\nMAML-stop\n98.45 ± 0.2%\n60.67 ± 1.0%\nResults. Table 4 summarizes the accuracy and the 95% con-\nLearning to Stop While Learning to Predict\nTable 5. Few-shot classiﬁcation in vanilla meta learning setting (Finn et al., 2017) where all tasks have the same number of data points.\nOmniglot 5-way\nOmniglot 20-way\nMiniImagenet 5-way\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nMAML\n98.7 ± 0.4%\n99.1 ± 0.1%\n95.8 ± 0.3%\n98.9 ± 0.2%\n48.70 ± 1.84%\n63.11 ± 0.92%\nMAML-stop\n99.62 ± 0.22%\n99.68 ± 0.12%\n96.05 ± 0.35%\n98.94 ± 0.10 %\n49.56 ± 0.82%\n63.41 ± 0.80%\nﬁdence interval on the held-out tasks for each dataset. The\nmaximum number of adaptation gradient descent steps is 10\nfor both MAML and MAML-stop. We can see the optimal\nstopping variant of MAML outperforms the vanilla MAML\nconsistently. For a more difﬁcult task on MiniImagenet\nwhere the imbalance issue is more severe, the accuracy\nimprovement is 3.5%. For completeness, we include the\nperformance on vanilla meta learning setting where all tasks\nhave the same number of observations in Table 5. MAML-\nstop still achieves comparable or better performance.\n5.3. Image Denoising\nIn this section, we perform the image denoising experiments.\nMore implementation details are provided in Appendix B.3.\nDataset. The models are trained on BSD500 (400 images)\n(Arbelaez et al., 2010), validated on BSD12, and tested on\nBSD68 (Martin et al., 2001). We follow the standard setting\nin (Zhang et al., 2019; Lefkimmiatis, 2018; Zhang et al.,\n2017) to add Gaussian noise to the images with a random\nnoise level σ ⩽55 during training and validation phases.\nExperiment setting. We compare with two DL models,\nDnCNN (Zhang et al., 2017) and UNLNet5 (Lefkimmiatis,\n2018), and two traditional methods, BM3D (Dabov et al.,\n2007) and WNNM (Gu et al., 2014). Since DnCNN is one\nof the most widely-used models for image denoising, we\nuse it as our predictive model. All deep models including\nours are considered in the blind Gaussian denoising setting,\nwhich means the noise-level is not given to the model, while\nBM3D and WNNM require the noise-level to be known.\nTable 6. PSNA performance comparison. The sign * indicates that\nnoise levels 65 and 75 do not appear in the training set.\nσ\nDnCNN-stop DnCNN UNLNet5\nBM3D\nWNNM\n35\n27.61\n27.60\n27.50\n26.81\n27.36\n45\n26.59\n26.56\n26.48\n25.97\n26.31\n55\n25.79\n25.71\n25.64\n25.21\n25.50\n*65\n23.56\n22.19\n-\n24.60\n24.92\n*75\n18.62\n17.90\n-\n24.08\n24.39\nResults. The performance is evaluated by the mean peak\nsignal-to-noise ratio (PSNR). Table 6 shows that DnCNN-\nstop performs better than the original DnCNN. Especially,\nfor images with noise levels 65 and 75 which are unseen dur-\ning training phase, DnCNN-stop generalizes signiﬁcantly\nbetter than DnCNN alone. Since there is no released code\nfor UNLNet5, its performances are copied from the pa-\nper (Lefkimmiatis, 2018), where results are not reported\nfor σ = 65 and 75. For traditional methods BM3D and\nWNNM, the test is in the noise-speciﬁc setting. That is,\nthe noise level is given to both BM3D and WNNM, so the\ncomparison is not completely fair to learning based methods\nin blind denoising setting.\nGround Truth\nWNNM\nDnCNN\nDnCNN-stop\nFigure 5. Denoising results of an image with noise level 65. (See\nAppendix B.3.2 for more visualization results.)\n5.4. Image Recognition\nWe explore the potential of our idea for improving the recog-\nnition performances on Tiny-ImageNet, using VGG16 (Si-\nmonyan & Zisserman, 2014) as the predictive model. With\n14 internal classiﬁers, after Stage I training, if the oracle q∗\nθ\nis used to determine the stop time t, the accuracy of VGG16\ncan be improved to 83.26%. Similar observation is provided\nin SDN (Kaya et al., 2019), but their loss P\nt wtℓt depends\non very careful hand-tuning on the weight wt for each layer,\nwhile we directly take an expectation using the oracle, which\nis more principled and leads to higher accuracy (Table 7).\nHowever, it reveals to be very hard to mimic the behavior of\nthe orcale q∗\nθ by πφ in Stage II, either due to the need of a\nbetter parametrization for πφ or more sophisticated reasons.\nOur learned πφ leads to similar accuracy as the heuristic\npolicy in SDN, which becomes the bottleneck in our ex-\nploration. However, based on the large performance gap\nbetween the oracle and the original VGG16, our result still\nprovides a potential direction for breaking the performance\nbottleneck of DL on image recognition.\nTable 7. Image recognition with oracle stop distribution.\nVGG16\nSDN training\nOur Stage I. training\n58.60%\n77.78% (best layer)\n83.26% (best layer)\nLearning to Stop While Learning to Predict\n6. Conclusion\nIn this paper, we introduce a generic framework for mod-\nelling and training a deep learning model with input-speciﬁc\ndepth, which is determined by a stopping policy πφ. Ex-\ntensive experiments are conducted to demonstrate the ef-\nfectiveness of both the model and the training algorithm,\non a wide range of applications. In the future, it will be\ninteresting to see whether other aspects of algorithms can\nbe incorporated into deep learning models either to improve\nthe performance or for better theoretical understandings.\nReferences\nAndrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W.,\nPfau, D., Schaul, T., Shillingford, B., and De Freitas, N.\nLearning to learn by gradient descent by gradient descent.\nIn Advances in Neural Information Processing Systems,\npp. 3981–3989, 2016.\nArbelaez, P., Maire, M., Fowlkes, C., and Malik, J. Contour\ndetection and hierarchical image segmentation. IEEE\ntransactions on pattern analysis and machine intelligence,\n33(5):898–916, 2010.\nBeck, A. and Teboulle, M.\nA fast iterative shrinkage-\nthresholding algorithm for linear inverse problems. SIAM\njournal on imaging sciences, 2(1):183–202, 2009.\nBecker, S., Cheridito, P., and Jentzen, A. Deep optimal\nstopping. Journal of Machine Learning Research, 20(74):\n1–25, 2019.\nBelanger, D., Yang, B., and McCallum, A. End-to-end learn-\ning for structured prediction energy networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 429–439. JMLR. org, 2017.\nBlumensath, T. and Davies, M. E. Iterative thresholding for\nsparse approximations. Journal of Fourier analysis and\nApplications, 14(5-6):629–654, 2008.\nBorgerding, M., Schniter, P., and Rangan, S. Amp-inspired\ndeep networks for sparse linear inverse problems. IEEE\nTransactions on Signal Processing, 65(16):4293–4308,\n2017.\nCeci, C. and Bassan, B. Mixed optimal stopping and stochas-\ntic control problems with semicontinuous ﬁnal reward for\ndiffusion processes. Stochastics and Stochastic Reports,\n76(4):323–337, 2004.\nChen, X., Liu, J., Wang, Z., and Yin, W. Theoretical linear\nconvergence of unfolded ista and its practical weights and\nthresholds. In Advances in Neural Information Process-\ning Systems, pp. 9061–9071, 2018.\nChen, X., Dai, H., and Song, L. Particle ﬂow bayes rule.\nIn International Conference on Machine Learning, pp.\n1022–1031, 2019.\nChen, X., Li, Y., Umarov, R., Gao, X., and Song, L. RNA\nsecondary structure prediction by learning unrolled algo-\nrithms. arXiv preprint arXiv:2002.05810, 2020.\nDabov, K., Foi, A., Katkovnik, V., and Egiazarian, K. Image\ndenoising by sparse 3-d transform-domain collaborative\nﬁltering. IEEE Transactions on image processing, 16(8):\n2080–2095, 2007.\nDomke, J.\nParameter learning with truncated message-\npassing. In CVPR 2011, pp. 2937–2943. IEEE, 2011.\nDumitrescu, R., Reisinger, C., and Zhang, Y. Approximation\nschemes for mixed optimal stopping and control problems\nwith nonlinear expectations and jumps. arXiv preprint\narXiv:1803.03794, 2018.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 1126–1135. JMLR. org, 2017.\nGregor, K. and LeCun, Y. Learning fast approximations\nof sparse coding. In Proceedings of the 27th Interna-\ntional Conference on International Conference on Ma-\nchine Learning, pp. 399–406. Omnipress, 2010.\nGu, S., Zhang, L., Zuo, W., and Feng, X. Weighted nuclear\nnorm minimization with application to image denoising.\nIn Proceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pp. 2862–2869, 2014.\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,\nBotvinick, M., Mohamed, S., and Lerchner, A. beta-\nVAE: Learning basic visual concepts with a constrained\nvariational framework. ICLR, 2(5):6, 2017.\nHuang, G., Chen, D., Li, T., Wu, F., van der Maaten, L., and\nWeinberger, K. Multi-scale dense networks for resource\nefﬁcient image classiﬁcation. In International Conference\non Learning Representations, 2018. URL https://\nopenreview.net/forum?id=Hk2aImxAb.\nIngraham, J., Riesselman, A., Sander, C., and Marks, D.\nLearning protein structure with a differentiable simulator.\nIn International Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=Byg3y3C9Km.\nJones, M., Kinoshita, S., and Mozer, M. C. Optimal re-\nsponse initiation: Why recent experience matters. In\nAdvances in neural information processing systems, pp.\n785–792, 2009.\nLearning to Stop While Learning to Predict\nKaya, Y., Hong, S., and Dumitras, T. Shallow-deep net-\nworks: Understanding and mitigating network overthink-\ning. In International Conference on Machine Learning,\npp. 3301–3310, 2019.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nLake, B., Salakhutdinov, R., Gross, J., and Tenenbaum, J.\nOne shot learning of simple visual concepts. In Proceed-\nings of the annual meeting of the cognitive science society,\nvolume 33, 2011.\nLe, T. A., Igl, M., Rainforth, T., Jin, T., and Wood, F. Auto-\nencoding sequential monte carlo. In International Con-\nference on Learning Representations, 2018.\nLee, Y. and Choi, S. Gradient-based meta-learning with\nlearned layerwise metric and subspace. arXiv preprint\narXiv:1801.05558, 2018.\nLefkimmiatis, S. Universal denoising networks: a novel\ncnn architecture for image denoising. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pp. 3204–3213, 2018.\nLi, K. and Malik, J. Learning to optimize. arXiv preprint\narXiv:1606.01885, 2016.\nLi, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning\nto learn quickly for few-shot learning. arXiv preprint\narXiv:1707.09835, 2017.\nLiu, J., Chen, X., Wang, Z., and Yin, W. ALISTA: Analytic\nweights are as good as learned weights in LISTA. In\nInternational Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=B1lnzn0ctQ.\nMartin, D., Fowlkes, C., Tal, D., and Malik, J. A database\nof human segmented natural images and its application\nto evaluating segmentation algorithms and measuring\necological statistics. In Proceedings Eighth IEEE Inter-\nnational Conference on Computer Vision. ICCV 2001,\nvolume 2, pp. 416–423. IEEE, 2001.\nMetzler, C., Mousavi, A., and Baraniuk, R. Learned d-\namp: Principled neural network based compressive image\nrecovery. In Advances in Neural Information Processing\nSystems, pp. 1772–1783, 2017.\nNa, D., Lee, H. B., Lee, H., Kim, S., Park, M., Yang, E.,\nand Hwang, S. J. Learning to balance: Bayesian meta-\nlearning for imbalanced and out-of-distribution tasks. In\nInternational Conference on Learning Representations,\n2020. URL https://openreview.net/forum?\nid=rkeZIJBYvr.\nNowozin, S., Cseke, B., and Tomioka, R. f-gan: Training\ngenerative neural samplers using variational divergence\nminimization. In Advances in neural information process-\ning systems, pp. 271–279, 2016.\nOreshkin, B., L´opez, P. R., and Lacoste, A. Tadam: Task de-\npendent adaptive metric for improved few-shot learning.\nIn Advances in Neural Information Processing Systems,\npp. 721–731, 2018.\nPham, H. Optimal stopping of controlled jump diffusion\nprocesses: a viscosity solution approach. In Journal of\nMathematical Systems, Estimation and Control. Citeseer,\n1998.\nQiao, S., Liu, C., Shen, W., and Yuille, A. L. Few-shot im-\nage recognition by predicting parameters from activations.\nIn Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pp. 7229–7238, 2018.\nRavi, S. and Larochelle, H. Optimization as a model for\nfew-shot learning. 2017.\nShiryaev, A. N. Optimal stopping rules, volume 8. Springer\nScience & Business Media, 2007.\nShrivastava, H., Chen, X., Chen, B., Lan, G., Aluru, S., Liu,\nH., and Song, L. GLAD: Learning sparse graph recovery.\nIn International Conference on Learning Representations,\n2020. URL https://openreview.net/forum?\nid=BkxpMTEtPB.\nSimonyan, K. and Zisserman, A.\nVery deep convolu-\ntional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\nSun, J., Li, H., Xu, Z., et al. Deep admm-net for com-\npressive sensing mri. In Advances in neural information\nprocessing systems, pp. 10–18, 2016.\nTeerapittayanon, S., McDanel, B., and Kung, H.-T.\nBranchynet: Fast inference via early exiting from deep\nneural networks. In 2016 23rd International Conference\non Pattern Recognition (ICPR), pp. 2464–2469. IEEE,\n2016.\nYakar, T. B., Litman, R., Sprechmann, P., Bronstein, A. M.,\nand Sapiro, G. Bilevel sparse models for polyphonic\nmusic transcription. In ISMIR, pp. 65–70, 2013.\nZamir, A. R., Wu, T.-L., Sun, L., Shen, W. B., Shi, B. E.,\nMalik, J., and Savarese, S. Feedback networks. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 1308–1317, 2017.\nZhang, J. and Ghanem, B.\nIsta-net:\nInterpretable\noptimization-inspired deep network for image compres-\nsive sensing. In Proceedings of the IEEE Conference\nLearning to Stop While Learning to Predict\non Computer Vision and Pattern Recognition, pp. 1828–\n1837, 2018.\nZhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L.\nBeyond a gaussian denoiser: Residual learning of deep\ncnn for image denoising. IEEE Transactions on Image\nProcessing, 26(7):3142–3155, 2017.\nZhang, X., Lu, Y., Liu, J., and Dong, B. Dynamically un-\nfolding recurrent restorer: A moving endpoint control\nmethod for image restoration. In International Confer-\nence on Learning Representations, 2019. URL https:\n//openreview.net/forum?id=SJfZKiC5FX.\nLearning to Stop While Learning to Predict\nA. Derivations\nA.1. Proof of Lemma 1\nProof. Under the assumptions that\nℓ(y, xt; θ) := −log pθ(y|t, x);\nand the prior p(t|x) is a uniform distribution over t, the β-VAE objective can be written as\nJβ-VAE(θ, qφ; x, y) :=\nEqφ log pθ(y|t, x) −βKL(qφ(t)||p(t|x))\n= −Eqφℓ(y, xt; θ) −βEqφ(t) log qφ(t)\np(t|x)\n= −Eqφℓ(y, xt; θ) −βEqφ(t) log qφ(t)\n+ βEqφ(t) log p(t|x)\n= −\n\u0000Eqφℓ(y, xt; θ) −βH(qφ)\n\u0001\n+ βEqφ(t) log 1\nT\n= −L(θ, qφ; x, y) −β log T.\nSince the second term −β log T is a constant, maximizing Jβ-VAE(θ, qφ; x, y) is equivalent to minimizing L(θ, qφ; x, y).\nA.2. Equivalence of reverse KL and maximum-entropy RL\nThe variational distribution qφ actually depends on the input instance x. For notation simplicity, we only write qφ(t) instead\nof qφ(t|x).\nmin\nφ KL(qφ(t)||q∗\nθ(t|y, x))\n(12)\n= min\nφ −\nT\nX\nt=1\nqφ(t) log q∗\nθ(t|y, x)) −H(qφ)\n(13)\n= min\nφ −\nT\nX\nt=1\nqφ(t) log pθ(y|t, x)β) −H(qφ)\n(14)\n+\nT\nX\nt=1\nqφ(t) log\nT\nX\nτ=1\npθ(y|τ, x)β\n(15)\n= min\nφ −\nT\nX\nt=1\nqφ(t) log pθ(y|t, x)β −H(qφ)\n(16)\n+\nT\nX\nt=1\nqφ(t)C(x, y)\n(17)\n= min\nφ −\nT\nX\nt=1\nqφ(t) log pθ(y|t, x)β −H(qφ)\n(18)\n+ C(x, y)\n(19)\n= min\nφ −\nT\nX\nt=1\nqφ(t) log pθ(y|t, x)β −H(qφ)\n(20)\n= max\nφ\nT\nX\nt=1\nqφ(t) log pθ(y|t, x)β + H(qφ)\n(21)\n= max\nφ\nT\nX\nt=1\nqφ(t)βℓ(y, xt; θ) + H(qφ)\n(22)\nLearning to Stop While Learning to Predict\n= max\nφ\nEt∼qφ [−βℓ(y, xt; θ) −log qφ(t)]\n(23)\nDeﬁne the action as at ∼πt = πφ(x, xt), the reward function as\nr(xt, at; y) :=\n(\n−βℓ(y, xt; θ)\nif at = 1 (i.e. stop),\n0\nif at = 0 (i.e. continue),\nand the transition probability as\nP(xt+1|xt, at) =\n(\n1\nif xt+1 = Fθ(xt) and at = 0,\n0\nelse.\nThen the above optimization can be written as\nmax\nφ\nEt∼qφ [−βℓ(y, xt; θ) −log qφ(t)]\n(24)\n= max\nφ\nEπφ\nT\nX\nt=1\nr(xt, at; y) −log πt(at|x, xt)\n(25)\n= max\nφ\nEπφ\nT\nX\nt=1\n[r(xt, at; y) + H(πt)] .\n(26)\nB. Experiment Details\nB.1. Learning To Learn: Sparse Recovery\nSynthetic data. We follow Chen et al. (2018) to choose m = 250, n = 500, sample the entries of A i.i.d. from the standard\nGaussian distribution, i.e., Aij ∼N(0, 1\nm), and then normalize its columns to have the unit ℓ2 norm. To generate y∗, we\ndecide each of its entry to be non-zero following the Bernoulli distribution with pb = 0.1. The values of the non-zero entries\nare sampled from the standard Gaussian distribution. The noise ϵ is Gaussian white noise. The signal-to-noise ratio (SNR)\nfor each sample is uniformly sampled from 20, 30 and 40. For the testing phase, a test set of 3000 samples are generated,\nwhere there are 1000 samples for each noise level. This test set is ﬁxed for all experiments in our simulations.\nEvaluation metric. The performance is evaluated by NMSE (in dB), which is deﬁned as 10 log10(\nPN\ni=1 ∥ˆxi−x∗,i∥2\n2\nPN\ni=1 ∥x∗,i∥2\n2\n) where\nˆxi is the estimator returned by an algorithm or deep model.\nB.2. Task-imbalanced Meta Learning\nB.2.1. DETAILS OF SETUP\nHyperparameters\nWe train MAML with batch size 16 on Omniglot imbalanced and batch size 2 on MiniImagenet\nimbalanced datasets. In both scenario we train with 60000 of mini-batch updates for the outer-loop of MAML. We report the\nresults with 5 inner SGD steps for Omniglot imbalanced and 10 inner SGD steps for MiniImagenet imbalanced with other\nbest hyperparameters suggested in (Finn et al., 2017), respectively. For MAML-stop we run 10 inner SGD steps for both\ndatasets, with the inner learning rate to be 0.1 and 0.05 for Omniglot and MiniImagenet, respectively. The outer learning\nrate for MAML-stop is 1e−4 as we use batch size 1 for training.\nWhen generating each meta-training dataset, we randomly select the number of observations within k1 to k2 for k1-k2-shot\nlearning. The number of observations in test set is always kept the same within each round of experiment.\nB.2.2. MEMORY EFFICIENT IMPLEMENTATION\nAs our MAML-stop allows the automated decision of optimal stopping, it is preferable that the maximum number of SGD\nupdates per each task is set to a larger number to fully utilize the capacity of the approach. This brings the challenge during\ntraining, as the loss on each meta-test set during training is required for each single inner update step. That is to say, if we\nallow maximumly 10 steps of inner SGD update, then the memory cost for running CNN prediction on meta-test set is 10x\nlarger than vanilla MAML. Thus a straightforward implementation will not give us a feasible training mechanism.\nLearning to Stop While Learning to Predict\nTo make the training of MAML-stop feasible on a single GPU, we utilize the following techniques:\n• We use stochastic EM for learning the predictive model, as well as the stopping policy. Speciﬁcally, we sample\nt ∼q∗\nθ(·|y, x) in each round of training, and only maximize pθ(y|t, x) in this round.\n• As the auto differentiation in PyTorch is unable to distinguish between ‘no gradient’ and ‘zero gradient’, it causes extra\nstorage for the unnecessary gradient computation. To overcome this, we ﬁrst calculate q∗\nθ(t|y, x) for each t without any\ngradient storage (which corresponds to no grad() in PyTorch), then recompute pθ(y|t, x) for the sampled t.\nWith the above techniques, we can train MAML-stop almost as (memory) efﬁcient as MAML.\nB.2.3. STANDARD META-LEARNING TASKS\nFor completeness, we also include the MAML-stop in the standard setting of few-shot learning. We mainly compared with\nthe vanilla MAML for the sake of ablation study.\nHyperparameters\nThe hyperparameter setup mainly follows the vanilla MAML paper. For both MAML and MAML-\nstop, we use the same batch size, number of training epochs and the learning rate. For Omniglot 20-way experiments and\nMiniImagenet 5-way experiments, we tune the number of unrolling steps in {5, 6, . . . , 10}, β in {0, 0.1, 0.01, 0.001} and\nthe learning rate of inner update in {0.1, 0.05}. We simply use grid search with a random held-out set with 600 tasks to\nselect the best model conﬁguration.\nB.3. Image Denoising\nB.3.1. IMPLEMENTATION DETAILS\nWhen training the denoising models, the raw images were cropped and augmented into 403K 50 ∗50 patchs. The training\nbatch size was 256. We used Adam optimizer with the initial learning rate as 1e −4. We ﬁrst trained the deep learning\nmodel with the unweighted loss for 50 epochs. Then, we further train the model with the weighted loss for another 50\nepoches. After hyper-parameter searching, we set the exploration coefﬁcient β as 0.1. When training the policy network, we\nused the Adam optimizer with the learning rate as 1e −4. We reused the above hyper-parameters during joint training.\nB.3.2. VISUALIZATION\nGround Truth\nNoisy Image\nBM3D\nWNNM\nDnCNN\nDnCNN-stop\nFigure 6. Denoising results of an image with noise level 65.\nLearning to Stop While Learning to Predict\nGround Truth\nNoisy Image\nBM3D\nWNNM\nDnCNN\nDnCNN-stop\nFigure 7. Denoising results of an image with noise level 65.\nB.4. Computing infrastructure\nMost of the experiments were run a hetergeneous GPU cluster. For each experiment, we typically used one or two V100\ncards, with the typical CPU processor as Intel Xeon Platinum 8260L. We assigned 6 threads and 64 GB CPU memory for\neach V100 card to maximize the utilization of the card.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-06-09",
  "updated": "2020-06-09"
}