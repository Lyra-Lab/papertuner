{
  "id": "http://arxiv.org/abs/2006.05082v1",
  "title": "Learning to Stop While Learning to Predict",
  "authors": [
    "Xinshi Chen",
    "Hanjun Dai",
    "Yu Li",
    "Xin Gao",
    "Le Song"
  ],
  "abstract": "There is a recent surge of interest in designing deep architectures based on\nthe update steps in traditional algorithms, or learning neural networks to\nimprove and replace traditional algorithms. While traditional algorithms have\ncertain stopping criteria for outputting results at different iterations, many\nalgorithm-inspired deep models are restricted to a ``fixed-depth'' for all\ninputs. Similar to algorithms, the optimal depth of a deep architecture may be\ndifferent for different input instances, either to avoid ``over-thinking'', or\nbecause we want to compute less for operations converged already. In this\npaper, we tackle this varying depth problem using a steerable architecture,\nwhere a feed-forward deep model and a variational stopping policy are learned\ntogether to sequentially determine the optimal number of layers for each input\ninstance. Training such architecture is very challenging. We provide a\nvariational Bayes perspective and design a novel and effective training\nprocedure which decomposes the task into an oracle model learning stage and an\nimitation stage. Experimentally, we show that the learned deep model along with\nthe stopping policy improves the performances on a diverse set of tasks,\nincluding learning sparse recovery, few-shot meta learning, and computer vision\ntasks.",
  "text": "Learning to Stop While Learning to Predict\nXinshi Chen 1 Hanjun Dai 2 Yu Li 3 Xin Gao 3 Le Song 1 4\nAbstract\nThere is a recent surge of interest in designing\ndeep architectures based on the update steps in\ntraditional algorithms, or learning neural networks\nto improve and replace traditional algorithms.\nWhile traditional algorithms have certain stop-\nping criteria for outputting results at different iter-\nations, many algorithm-inspired deep models are\nrestricted to a â€œï¬xed-depthâ€ for all inputs. Similar\nto algorithms, the optimal depth of a deep architec-\nture may be different for different input instances,\neither to avoid â€œover-thinkingâ€, or because we\nwant to compute less for operations converged al-\nready. In this paper, we tackle this varying depth\nproblem using a steerable architecture, where a\nfeed-forward deep model and a variational stop-\nping policy are learned together to sequentially\ndetermine the optimal number of layers for each\ninput instance. Training such architecture is very\nchallenging. We provide a variational Bayes per-\nspective and design a novel and effective training\nprocedure which decomposes the task into an or-\nacle model learning stage and an imitation stage.\nExperimentally, we show that the learned deep\nmodel along with the stopping policy improves\nthe performances on a diverse set of tasks, in-\ncluding learning sparse recovery, few-shot meta\nlearning, and computer vision tasks.\n1. Introduction\nRecently, researchers are increasingly interested in the con-\nnections between deep learning models and traditional algo-\nrithms: deep learning models are viewed as parameterized\nalgorithms that operate on each input instance iteratively,\nand traditional algorithms are used as templates for design-\ning deep learning architectures. While an important con-\n1Georgia Institute of Technology, USA 2Google Research, USA\n3King Abdullah University of Science and Technology, Saudi\nArabia 4Ant Financial, China. Correspondence to: Xinshi Chen\n<xinshi.chen@gatech.edu>, Le Song <lsong@cc.gatech.edu>.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by\nthe author(s).\nTask 1\nFixed-depth Learned Algorithm\nğ’™\nâ€¦\nğ‘“#$ ğ‘“#%\nğ‘“#&\nğ’™ğ‘»\n(a):  Learning-based Algorithm design\nTask 2\nğœƒ)\nğœƒ*+,-.\nğœƒ*+,-/\nâˆ‡#â„’.\nâˆ‡#â„’/\n(b):  Task-imbalanced Meta Learning\nâ€¦\nğ’™ğ’•\n(output)\nDynamic-depth Traditional Algorithm\nğ’™\nğ’™ğ’•\nsatisfied\nnot satisfied\ncriteria\nhand-designed\nupdate step\nD\nFigure 1. Motivation for learning to stop.\ncept in traditional algorithms is the stopping criteria for\noutputting the result, which can be either a convergence\ncondition or an early stopping rule, such stopping criteria\nhas been more or less ignored in algorithm-inspired deep\nlearning models. A â€œï¬xed-depthâ€ deep model is used to\noperate on all problem instances (Fig. 1 (a)). Intuitively,\nfor deep learning models, the optimal depth (or the opti-\nmal number of steps to operate on an input) can also be\ndifferent for different input instances, either because we\nwant to compute less for operations converged already, or\nwe want to generalize better by avoiding â€œover-thinkingâ€.\nSuch motivation aligns well with both the cognitive science\nliterature (Jones et al., 2009) and many examples below:\nâ€¢ In learning to optimize (Andrychowicz et al., 2016; Li &\nMalik, 2016), neural networks are used as the optimizer\nto minimize some loss function. Depending on the initial-\nization and the objective function, an optimizer should\nconverge in different number of steps;\nâ€¢ In learning to solve statistical inverse problems such as\ncompressed sensing (Chen et al., 2018; Liu et al., 2019),\ninverse covariance estimation (Shrivastava et al., 2020),\nand image denoising (Zhang et al., 2019), deep mod-\nels are learned to directly predict the recovery results.\nIn traditional algorithms, problem-dependent early stop-\nping rules are widely used to achieve regularization for a\nvariance-bias trade-off. Deep learning models for solving\nsuch problems maybe also achieve a better recovery ac-\ncuracy by allowing instance-speciï¬c computation steps;\nâ€¢ In meta learning, MAML (Finn et al., 2017) used an\nunrolled and parametrized algorithm to adapt a common\narXiv:2006.05082v1  [cs.LG]  9 Jun 2020\nLearning to Stop While Learning to Predict\nğ’™\nğœƒ#\nğ’™ğŸ\nğœƒ%\nğ’™ğŸ\nğœ™\nğœ‹#\n< 0.5\nğœƒ-\nğœ™\nğœ‹%\n< 0.5\nâ€¦\nğœƒ.\nğ’™ğ’•\nğœ™\nğœ‹.â‰¥0.5\nstop, output ğ’™ğ’•\nâ€¦\nFigure 2. Two-component model: learning to predict (blue) while\nlearning to stopping (green).\nparameter to a new task. However, depending on the\nsimilarity of the new task to the old tasks, or, in a more\nrealistic task-imbalanced setting where different tasks\nhave different numbers of data points (Fig. 1 (b)), a task-\nspeciï¬c number of adaptation steps is more favorable to\navoid under or over adaption.\nTo address the varying depth problem, we propose to learn\na steerable architecture, where a shared feed-forward model\nfor normal prediction and an additional stopping policy\nare learned together to sequentially determine the optimal\nnumber of layers for each input instance. In our framework,\nthe model consists of (see Fig. 2)\nâ€¢ A feed-forward or recurrent mapping FÎ¸, which trans-\nforms the input x to generate a path of features (or states)\nx1, Â· Â· Â· , xT ; and\nâ€¢ A stopping policy Ï€Ï† : (x, xt) 7â†’Ï€t âˆˆ[0, 1], which se-\nquentially observes the states and then determines the\nprobability of stopping the computation of FÎ¸ at layer t.\nThese two components allow us to sequentially predict the\nnext targeted state while at the same time determining when\nto stop. In this paper, we propose a single objective function\nfor learning both Î¸ and Ï†, and we interpret it from the per-\nspective of variational Bayes, where the stopping time t is\nviewed as a latent variable conditioned on the input x. With\nthis interpretation, learning Î¸ corresponds to maximizing\nthe marginal likelihood, and learning Ï† corresponds to the\ninference step for the latent variable, where a variational\ndistribution qÏ†(t) is optimized to approximate the posterior.\nA natural algorithm for solving this problem could be the\nExpectation-Maximization (EM) algorithm, which can be\nvery hard to train and inefï¬cient.\nHow to learn Î¸ and Ï† effectively and efï¬ciently? We propose\na principled and effective training procedure, where we\ndecompose the task into an oracle model learning stage and\nan imitation learning stage (Fig. 3). More speciï¬cally,\nâ€¢ During the oracle model learning stage, we utilize a\nclosed-form oracle stopping distribution qâˆ—|Î¸ which can\nleverage label information not available at testing time.\nâ€¢ In the imitation learning stage, we use a sequential policy\nÏ€Ï† to mimic the behavior of the oracle policy obtained in\nthe ï¬rst stage. The sequential policy does not have access\nto the label so that it can be used during testing phase.\nThis procedure provides us a very good initial predictive\nmax\n$  ğ’¥ğ›½âˆ’VAE(â„±ğœƒ, ğ‘ğœ™)\nAlternating Updates\nâ„±ğœ½\nğ’’ğ“\nVAE-based method:\nStage I.\nOur method:\nğ‘âˆ—|ğœƒ\nâ„±ğœ½\nmin\nğ“KL(        ,         )\nmax\n6  ğ’¥ğ›½âˆ’VAE(â„±ğœƒ, ğ‘ğœ™)\noracle\nmax\n6  ğ’¥ğ›½âˆ’VAE(â„±ğœƒ, ğ‘âˆ—|ğœƒ)\noptimal ğœ½âˆ—\nStage II.\nğ‘âˆ—|ğœ½âˆ—\noracle\nğ’’ğ“\noptimal ğ“âˆ—\nFigure 3. Two-stage training framework.\nmodel and a stopping policy. We can either directly use these\nlearned models, or plug them back to the variational EM\nframework and reiterate to further optimize both together.\nOur proposed learning to stop method is a generic frame-\nwork that can be applied to a diverse range of applications.\nTo summarize, our contribution in this paper includes:\n1. a variational Bayes perspective to understand the pro-\nposed model for learning both the predictive model and\nthe stopping policy together;\n2. a principled and efï¬cient algorithm for jointly learning\nthe predictive model and the stopping policy; and the\nrelation of this algorithm to reinforcement learning;\n3. promising experiments on various tasks including learn-\ning to solve sparse recovery problems, task-imbalanced\nfew-shot meta learning, and computer vision tasks, where\nwe demonstrate the effectiveness of our method in terms\nof both the prediction accuracy and inference efï¬ciency.\n2. Related Works\nUnrolled algorithm. A line of recent works unfold and\ntruncate iterative algorithms to design neural architectures.\nThese algorithm-based deep models can be used to automat-\nically learn a better algorithm from data. This idea has been\ndemonstrated in different problems including sparse signal\nrecovery (Gregor & LeCun, 2010; Sun et al., 2016; Borg-\nerding et al., 2017; Metzler et al., 2017; Zhang & Ghanem,\n2018; Chen et al., 2018; Liu et al., 2019), sparse inverse\ncovariance estimation (Shrivastava et al., 2020), sequential\nBayesian inference (Chen et al., 2019), parameter learning\nin graphical models (Domke, 2011), non-negative matrix\nfactorization (Yakar et al., 2013), etc. Unrolled algorithm\nbased deep module has also be used for structured prediction\n(Belanger et al., 2017; Ingraham et al., 2019; Chen et al.,\n2020). Before the training phase, all these works need to\nassign a ï¬xed number of iterations that is used for every\nLearning to Stop While Learning to Predict\ninput instance regardless of their varying difï¬culty level.\nOur proposed method is orthogonal and complementary to\nall these works, by taking the variety of the input instances\ninto account via adaptive stopping time.\nMeta learning. Optimization-based meta learning techniq-\nues are widely applied for solving challenging few-shot\nlearning problems (Ravi & Larochelle, 2017; Finn et al.,\n2017; Li et al., 2017). Several recent advances proposed\ntask-adaptive meta-learning models which incorporate task-\nspeciï¬c parameters (Qiao et al., 2018; Lee & Choi, 2018;\nNa et al., 2020) or task-dependent metric scaling (Oreshkin\net al., 2018). In parallel with these task-adaptive methods,\nwe propose a task-speciï¬c number of adaptation steps and\ndemonstrate the effectiveness of this simple modiï¬cation\nunder the task-imbalanced scenarios.\nOther adaptive-depth deep models. In image recognition,\nâ€˜early exitsâ€™ is proposed mainly aimed at improving the\ncomputation efï¬ciency during the inference phase (Teer-\napittayanon et al., 2016; Zamir et al., 2017; Huang et al.,\n2018), but these methods are based on speciï¬c architectures.\nKaya et al. (2019) proposed to avoiding â€œover-thinkingâ€ by\nearly stopping. However, the same as all the other â€˜early\nexitsâ€™ models, some heuristic policies are adopted to choose\nthe output layer by conï¬dence scores of internal classiï¬ers.\nAlso, their algorithms for training the feed-forward model\nFÎ¸ do not take into account the effect of the stopping policy.\nOptimal stopping. In optimal control literature, optimal\nstopping is a problem of choosing a time to take a given ac-\ntion based on sequentially observed random variables in or-\nder to maximize an expected payoff (Shiryaev, 2007). When\na policy for controlling the evolution of random variables\n(corresponds to the output of FÎ¸) is also involved, it is called\na â€œmixed controlâ€ problem, which is highly related to our\nwork. Existing works in this area ï¬nd the optimal controls\nby solving the Hamilton-Jacobi-Bellman (HJB) equation,\nwhich is theoretically grounded (Pham, 1998; Ceci & Bas-\nsan, 2004; Dumitrescu et al., 2018). However, they focus\non stochastic differential equation based model and the pro-\nposed algorithms suffer from the curse of dimensionality\nproblem. Becker et al. (2019) use DL to learn the optimal\nstopping policy, but the learning of Î¸ is not considered. Be-\nsides, Becker et al. (2019) use reinforcement learning (RL)\nto solve the problem. In Section 4, we will discuss how our\nvariational inference formulation is related to RL.\n3. Problem Formulation\nIn this section, we will introduce how we model the stopping\npolicy together with the predictive deep model, deï¬ne the\njoint optimization objective, and interpret this framework\nfrom a variational Bayes perspective.\n3.1. Steerable Model\nThe predictive model, FÎ¸, is a typical T-layer deep model\nthat generates a path of embeddings (x1, Â· Â· Â· , xT ) through:\nPredictive model:\nxt = fÎ¸t(xtâˆ’1), for t= 1, Â· Â· Â· , T (1)\nwhere the initial x0 is determined by the input x. We denote\nit by FÎ¸ = {fÎ¸1, Â· Â· Â· , fÎ¸T } where Î¸ âˆˆÎ˜ are the parameters.\nStandard supervised learning methods learn Î¸ by optimizing\nan objective estimated on the ï¬nal state xT . In our model,\nthe operations in Eq. 1 can be stopped earlier, and for differ-\nent input instance x, the stopping time t can be different.\nOur stopping policy, Ï€Ï†, determines whether to stop at t-th\nstep after observing the input x and its ï¬rst t states x1:t\ntransformed by FÎ¸. If we assume the Markov property, then\nÏ€Ï† only needs to observe the most recent state xt. In this\npaper, we only input x and xt to Ï€Ï† at each step t, but it\nis trivial to generalize it Ï€Ï€(x, x1:t). More precisely, Ï€Ï† is\ndeï¬ned as a randomized policy as follows:\nStopping policy: Ï€t = Ï€Ï†(x, xt), for t= 1, Â· Â· Â· , T âˆ’1 (2)\nwhere Ï€t âˆˆ[0, 1] is the probability of stopping. We abuse\nthe notation Ï€ to both represent the parametrized policy and\nalso the probability mass.\nThis stopping policy sequentially makes a decision when-\never a new state xt is observed. Conditioned on the states\nobserved until step t, whether to stop before t is independent\non states after t. Therefore, once it decides to stop at t, the\nremaining computations can be saved, which is a favorable\nproperty when the inference time is a concern, or for some\noptimal stopping problems such as option trading where\ngetting back to earlier states is not allowed.\n3.2. From Sequential Policy To Stop Time Distribution\nThe stopping policy Ï€Ï† makes sequential actions based on\nthe observations, where Ï€t := Ï€Ï†(x, xt) is the probability\nof stopping when xt is observed. These sequential actions\nÏ€1, Â· Â· Â· , Ï€T âˆ’1 jointly determines the random time t at which\nthe stop occurs. Induced by Ï€Ï†, the probability mass func-\ntion of the stop time t, denoted as qÏ†, can be computed by\nVariational stop time distribution:\n(\nqÏ†(t) = Ï€t\nQtâˆ’1\nÏ„=1(1 âˆ’Ï€Ï„)\nif t < T,\nqÏ†(T) = QT âˆ’1\nÏ„=1 (1 âˆ’Ï€Ï„)\nelse.\n(3)\nIn this equation, the product Qtâˆ’1\nÏ„=1(1 âˆ’Ï€Ï„) indicates the\nprobability of â€˜not stopped before tâ€™, which is the survival\nprobability. Multiply this survival probability with Ï€t, we\nhave the stop time distribution qÏ†(t). For the last time step\nT, the stop probability qÏ†(T) simply equals to the survival\nprobability at T, which means if the process is â€˜not stopped\nbefore Tâ€™, then it must stop at T.\nLearning to Stop While Learning to Predict\nNote that we only use Ï€Ï† in our model to sequentially de-\ntermine whether to stop. However, we use the induced\nprobability mass qÏ† to help design the training objective and\nalso the algorithm.\n3.3. Optimization Objective\nNote that the stop time t is a discrete random variable with\ndistribution determined by qÏ†(t). Given the observed label\ny of an input x, the loss of the predictive model stopped at\nposition t can computed as â„“(y, xt; Î¸) where â„“(Â·) is a loss\nfunction. Taking into account all possible stopping positions,\nwe will be interested in the loss in expectation over t,\nL(Î¸, qÏ†; x, y) := Etâˆ¼qÏ†â„“(y, xt; Î¸) âˆ’Î²H(qÏ†),\n(4)\nwhere H(qÏ†) := âˆ’P\nt qÏ†(t) log qÏ†(t) is an entropy regu-\nlarization and Î² is the regularization coefï¬cient. Given a\ndata set D = {(x, y)}, the parameters of the predictive\nmodel and the stopping policy can be estimated by\nminÎ¸,Ï†\n1\n|D|\nP\n(x,y)âˆˆD L(Î¸, qÏ†; x, y).\n(5)\nTo better interpret the model and objective, in the following,\nwe will make a connection from the perspective of vari-\national Bayes, and how the objective function deï¬ned in\nEq. 4 is equivalent to the Î²-VAE objective.\n3.4. Variational Bayes Perspective\nIn the Bayesâ€™ framework, a probabilistic model typically\nconsists of prior, likelihood function and posterior of the\nlatent variable. We ï¬nd the correspondence between our\nmodel and a probabilistic model as follows (also see Table 1)\nâ€¢ we view the adaptive stopping time t as a latent variable\nwhich is unobserved;\nâ€¢ The conditional prior p(t|x) of t is a uniform distribution\nover all the layers in this paper. However, if one wants\nto reduce the computation cost and penalize the stopping\ndecisions at deeper layers, a prior with smaller probability\non deeper layers can be deï¬ned to regularize the results;\nâ€¢ The likelihood function pÎ¸(y|t, x) of the observed label\ny is controlled by Î¸, since FÎ¸ determines the states xt;\nâ€¢ The posterior distribution over the stopping time t can be\ncomputed by Bayesâ€™ rule pÎ¸(t|y, x) âˆpÎ¸(y|t, x)p(t|x),\nbut it requires the observation of the label y, which is\ninfeasible during testing phase.\nTable 1. Corresponds between our model and Bayesâ€™ model.\nstop time t\nlatent variable\nlabel y\nobservation\nloss â„“(y, xt; Î¸)\nlikelihood pÎ¸(y|t, x)\nstop time distribution qÏ†\nposterior pÎ¸(t|y, x)\nregularization\nprior p(t|x)\nIn this probabilistic model, we need to learn Î¸ to better ï¬t\nthe observed data and learn a variational distribution qÏ† over\nt that only takes x and the transformed internal states as\ninputs to approximate the true posterior.\nMore speciï¬cally, the parameters in the likelihood function\nand the variational posterior can be optimized using the vari-\national autoencoder (VAE) framework (Kingma & Welling,\n2013). Here we consider a generalized version called Î²-\nVAE (Higgins et al., 2017), and obtain the optimization\nobjective for data point (x, y)\nJÎ²-VAE(Î¸, qÏ†; x, y) :=\nEqÏ† log pÎ¸(y|t,x) âˆ’Î²KL(qÏ†(t)||p(t|x)),\n(6)\nwhere KL(Â·||Â·) is the KL divergence. When Î² = 1, it\nbecomes the original VAE objective, i.e., the evidence lower\nbound (ELBO). Now we are ready to present the equivalence\nrelation between the Î²-VAE objective and the loss deï¬ned\nin Eq. 4. See Appendix A.1 for the proof.\nLemma 1. Under assumptions: (i) the loss function â„“in\nEq. 4 is deï¬ned as the negative log-likelihood (NLL), i.e.,\nâ„“(y, xt; Î¸) := âˆ’log pÎ¸(y|t, x);\n(ii) the prior p(t|x) is a uniform distribution over t;\nthen minimizing the loss L in Eq. 4 is equivalent to maxi-\nmizing the Î²-VAE objective JÎ²-VAE in Eq. 6.\nFor classiï¬cation problems, the cross-entropy loss is aligned\nwith NLL. For regression problems with mean squared error\n(MSE) loss, we can deï¬ne the likelihood as pÎ¸(y|t, x) âˆ¼\nN(xt, I). Then the NLL of this Gaussian distribution is\nâˆ’log pÎ¸(y|t, x) =\n1\n2âˆ¥y âˆ’xtâˆ¥2\n2 + C, which is equiva-\nlent to MSE loss. More generally, we can always deï¬ne\npÎ¸(y|t, x) âˆexp(âˆ’â„“(y, xt; Î¸)).\nThis VAE view allows us to design a two-step procedure\nto effectively learn Î¸ and Ï† in the predictive model and\nstopping policy, which is presented in the next section.\n4. Effective Training Algorithm\nVAE-based methods perform optimization steps over Î¸ (M\nstep for learning) and Ï† (E step for inference) alternatively\nuntil convergence, which has two limitations in our case:\ni. The alternating training can be slow to converge and\nrequires tuning the training scheduling;\nii. The inference step for learning qÏ† may have the mode col-\nlapse problem, which in this case means qÏ† only captures\nthe time step t with highest averaged frequency.\nTo overcome these limitations, we design a training proce-\ndure followed by an optional ï¬ne-tuning stage using the\nvariational lower bound in Eq. 6. More speciï¬cally,\nStage I. Find the optimal Î¸ by maximizing the conditional\nmariginal likelihood when the stop time distribution follows\nan oracle distribution qâˆ—\nÎ¸.\nLearning to Stop While Learning to Predict\nStage II. Fix the optimal Î¸ learned in Stage I, and only learn\nthe distribution qÏ† to mimic the oracle by minimizing the\nKL divergence between qÏ† and qâˆ—\nÎ¸.\nStage III. (Optional) Fine-tune Î¸ and Ï† jointly towards the\njoint objective in Eq. 6.\nThe overall algorithm steps are summarized in Algorithm 1.\nIn the following sections, we will focus on the derivation\nof the ï¬rst two training steps. Then we will discuss several\nmethods to further improve the memory and computation\nefï¬ciency for training.\n4.1. Oracle Stop Time Distribution\nWe ï¬rst give the deï¬nition of the oracle stop time distribu-\ntion qâˆ—\nÎ¸. For each ï¬xed Î¸, we can ï¬nd a closed-form solution\nfor the optimal qâˆ—\nÎ¸ that optimizes the joint objective.\nqâˆ—\nÎ¸(Â·|y, x) := arg maxqâˆˆâˆ†T âˆ’1 JÎ²-VAE(Î¸, q; x, y)\nAlternatively, qâˆ—\nÎ¸(Â·|y, x) = arg minqâˆˆâˆ†T âˆ’1 L(Î¸, q; x, y).\nUnder the mild assumptions in Lemma 1, these two opti-\nmizations lead to the same optimal oracle distribution.\nOracle stop time distribution:\nqâˆ—\nÎ¸(t|y, x) =\npÎ¸(y|t, x)\n1\nÎ²\nPT\nt=1 pÎ¸(y|t, x)\n1\nÎ²\n(7)\n=\nexp(âˆ’1\nÎ² â„“(y, xt; Î¸))\nPT\nt=1 exp(âˆ’1\nÎ² â„“(y, xt; Î¸))\n(8)\nThis closed-form solution makes it clear that the oracle picks\na step t according to the smallest loss or largest likelihood\nwith an exploration coefï¬cient Î².\nRemark: When Î² = 1, qâˆ—\nÎ¸ is the same as the posterior\ndistribution pÎ¸(t|y, x) âˆpÎ¸(y|t, x)p(t|x).\nNote that there are no new parameters in the oracle dis-\ntribution. Instead, it depends on the parameters Î¸ in the\npredictive model. Overall, the oracle qâˆ—\nÎ¸ is a function of Î¸,\nt, y and x that has a closed-form. Next, we will introduce\nhow we use this oracle in the ï¬rst two training stages.\n4.2. Stage I. Predictive Model Learning\nIn Stage I, we optimize the parameters Î¸ in the predictive\nmodel by taking into account the oracle stop distribution\nqâˆ—\nÎ¸ . This step corresponds to the M step for learning Î¸, by\nmaximizing the marginal likelihood. The difference with\nthe normal M step is that here qÏ† is replaced by the oracle\nqâˆ—\nÎ¸ that gives the optimal stopping distribution so that the\nmarginal likelihood is independent on Ï†. More precisely,\nstage I ï¬nds the optimum of:\nmax\nÎ¸\n1\n|D|\nX\n(x,y)âˆˆD\nT\nX\nt=1\nqâˆ—\nÎ¸(t|y, x) log pÎ¸(y|t, x),\n(9)\nAlgorithm 1 Overall Algorithm\nRandomly initialized Î¸ and Ï†.\nFor itr = 1 to #iterations do\nâ–·Stage I.\nSample a batch of data points B âˆ¼D.\nTake an optimization step to update Î¸ towards the\nmarginal likelihood function deï¬ned in Eq. 9.\nFor itr = 1 to #iterations do\nâ–·Stage II.\nSample a batch of data points B âˆ¼D.\nTake an optimization step to update Ï† towards the re-\nverse KL divergence deï¬ned in Eq. 10.\nFor itr = 1 to #iterations do\nâ–·Optional Step\nSample a batch of data points B âˆ¼D.\nUpdate both Î¸ and Ï† towards Î²-VAE objective in Eq. 6.\nreturn Î¸, Ï†\nwhere the summation over t is the expectation of the like-\nlihood, Etâˆ¼qâˆ—\nÎ¸(t|y,x) log pÎ¸(y|t, x). Since qâˆ—\nÎ¸ has a differ-\nentiable closed-form expression in terms of Î¸, x, y and t,\nthe gradient can also propagate through qâˆ—\nÎ¸, which is also\ndifferent from the normal M step.\nTo summarize, in Stage I., we learn the predictive model\nparameter Î¸, by assuming that the stop time always follows\nthe best stopping distribution that depends on Î¸. In this case,\nthe learning of Î¸ has already taken into account the effect of\nthe data-speciï¬c stop time.\nHowever, we note that the oracle qâˆ—\nÎ¸ is not in the form of\nsequential actions as in Eq. 2 and it requires the access to\nthe true label y, so it can not be used for testing. However,\nit plays an important role in obtaining a sequential policy\nwhich will be explained next.\n4.3. Stage II. Imitation With Sequential Policy\nIn Stage II, we learn the sequential policy Ï€Ï† that can best\nmimic the oracle distribution qâˆ—\nÎ¸, where Î¸ is ï¬xed to be\nthe optimal Î¸ learned in Stage I. The way of doing so is\nto minimize the divergence between the oracle qâˆ—\nÎ¸ and the\nvariational stop time distribution qÏ† induced by Ï€Ï† (Eq. 3).\nThere are various variational divergence minimization ap-\nproaches that we can use (Nowozin et al., 2016). For exam-\nple, a widely used objective for variational inference is the\nreverse KL divergence:\nKL(qÏ†||qâˆ—\nÎ¸) = PT\nt=1 âˆ’qÏ†(t) log qâˆ—\nÎ¸(t|y, x) âˆ’H(qÏ†).\nRemark. We write qÏ†(t) instead of qÏ†(t|x1:T , x) for nota-\ntion simplicity, but qÏ† is dependent on x and x1:T (Eq. 3).\nIf we rewrite qÏ† using Ï€1, Â· Â· Â· , Ï€T âˆ’1 as deï¬ned in Eq. 3,\nwe can ï¬nd that minimizing the reverse KL is equivalent to\nï¬nding the optimal policy Ï€Ï† in a reinforcement learning\n(RL) environment, where the state is xt, action at âˆ¼Ï€t :=\nÏ€Ï†(x, xt) is a stop/continue decision, the state transition is\nLearning to Stop While Learning to Predict\ndetermined by Î¸ and at, and the reward is deï¬ned as\nr(xt, at; y) :=\n(\nâˆ’Î²â„“(y, xt; Î¸)\nif at = 0 (i.e. stop)\n0\nif at = 1 (i.e. continue)\nwhere â„“(y, xt; Î¸) = âˆ’log pÎ¸(y|t, x). More detials and also\nthe derivation are given in Appendix A.2 to show that min-\nimizing KL(qÏ†||qâˆ—\nÎ¸) is equivalent to solving the following\nmaximum-entropy RL:\nmax\nÏ†\nEÏ€Ï†\nPT\nt=1 [r(xt, at; y) + H(Ï€t)] .\nIn some related literature, optimal stopping problem is often\nformulated as an RL problem (Becker et al., 2019). Above\nwe bridge the connection between our variational inference\nformulation and the RL-based optimal stopping literature.\nAlthough reverse KL divergence is a widely used objective,\nit suffers from the mode collapse issue, which in our case\nmay lead to a distribution qÏ† that captures only a common\nstopping time t for all x that on average performs the best,\ninstead of a more spread-out stopping time. Therefore, we\nconsider the forward KL divergence:\nKL(qÏ†||qâˆ—\nÎ¸) = âˆ’\nT\nX\nt=1\nqâˆ—\nÎ¸(t|y, x) log qÏ†(t) âˆ’H(qâˆ—\nÎ¸), (10)\nwhich is equivalent to the cross-entropy loss, since the term\nH(qâˆ—\nÎ¸) can be ignored as Î¸ is ï¬xed in this step. Experimen-\ntally, we ï¬nd forward KL leads to a better performance.\n4.4. The Optional Fine Tuning Stage\nIt is easy to see that our two-stage training procedure also\nhas an EM ï¬‚avor. However, with the oracle qâˆ—\nÎ¸ incorporated,\nthe training of Î¸ has already taken into account the effect of\nthe optimal stopping distribution. Therefore, we can save a\nlot of alternation steps. After the two-stage training, we can\nï¬ne-tune Î¸ and Ï† jointly towards the Î²-VAE objective. Ex-\nperimentally, we ï¬nd this additional stage does not improve\nmuch the performance trained after the ï¬rst two stages.\n4.5. Implementation Details For Efï¬cient Training\nSince both objectives in oracle learning stage (Eq. 9) and\nimitation stage (Eq. 10) involve the summation over T lay-\ners, the computation and memory costs during training are\nhigher than standard learning methods. The memory issue is\nespecially important in meta learning. In the following, we\nintroduce several ways of improving the training efï¬ciency.\nFewer output channels. Instead of allowing the model to\noutput xt at any layer, we can choose a smaller number of\noutput channels that are evenly placed along with the layers.\nStochastic sampling in Step I. A Monte Carlo method can\nbe used to approximate the expectation over qâˆ—\nÎ¸ in Step I.\nMore precisely, for each (x, y) we can randomly sample\na layer ts âˆ¼qâˆ—\nÎ¸(t|y, x) from the oracle, and only compute\nlog pÎ¸(y|ts, x) at ts, instead of summing over all t âˆˆ[T].\nNote that, in this case, the gradient will not back-propagate\nthrough qâˆ—\nÎ¸(t|y, x).\nMAP estimate in Step II. Instead of approximating the\ndistribution qâˆ—\nÎ¸, we can approximate the maximum a pos-\nterior (MAP) estimate Ë†t(x, y) = arg maxtâˆˆ[T ] qâˆ—\nÎ¸(t|y, x)\nso that the objective for each sample is âˆ’log qÎ¸(Ë†t(x, y)),\nwhich does not involve the summation over t. Except for\nefï¬ciency, we also ï¬nd this MAP estimate can lead to a\nhigher accuracy, by encouraging the learning of qÏ† to focus\nmore on the sample-wise best layer.\n5. Experiments\nWe conduct experiments on (i) learning-based algorithm for\nsparse recovery, (ii) few-shot meta learning, and (iii) image\ndenoising. The comparison is in an ablation study fashion to\nbetter examine whether the stopping policy can improve the\nperformances given the same architecture for the predictive\nmodel, and whether our training algorithm is more effective\ncompared to the alternating EM algorithm. In the end, we\nalso discuss our exploration of the image recognition task.\n5.1. Learning To Optimize: Sparse Recovery\nWe consider a sparse recovery task which aims at recovering\nxâˆ—âˆˆRn from its noisy linear measurements b = Axâˆ—+ Ïµ,\nwhere A âˆˆRmÃ—n, Ïµ âˆˆRm is Gaussian white noise, and\nm â‰ªn. A popular approach is to model the problem as the\nLASSO formulation minx 1\n2âˆ¥bâˆ’Axâˆ¥2\n2 +Ïâˆ¥xâˆ¥1 and solves\nit using iterative methods such as the ISTA (Blumensath &\nDavies, 2008) and FISTA (Beck & Teboulle, 2009) algo-\nrithms. We choose the most popular model named Learned\nISTA (LISTA) as the baseline and also as our predictive\nmodel. LISTA is a T-layer network with update steps:\nxt = Î·Î»t(W 1\nt b + W 2\nt xtâˆ’1),\nt = 1, Â· Â· Â· , T,\n(11)\nwhere Î¸ = {(Î»t, W 1\nt , W 2\nt )}T\nt=1 are leanable parameters.\nExperiment setting. We follow Chen et al. (2018) to gen-\nerate the samples. The signal-to-noise ratio (SNR) for each\nsample is uniformly sampled from 20, 30, and 40. The train-\ning loss for LISTA is PT\nt=1 Î³T âˆ’tâˆ¥xt âˆ’xâˆ—âˆ¥2\n2 where Î³ â‰¤1.\nIt is commonly used for algorithm-based deep learning, so\nthat there is a supervision signal for every layer. For ISTA\nand FISTA, we use the training set to tune the hyperparam-\neters by grid search. See Appendix B.1 for more details.\nRecovery performance. (Table 2) We report the NMSE\n(in dB) results for each model/algorithm evaluated on 1000\nï¬xed test samples per SNR level. It is revealed in Table 2\nthat learning-based methods have better recovery perfor-\nLearning to Stop While Learning to Predict\nTable 2. Recovery performances of different algorithms/models.\nSNR\nmixed\n20\n30\n40\nFISTA (T = 100)\n-18.96\n-16.75\n-20.46\n-20.97\nISTA (T = 100)\n-14.66\n-13.99\n-14.99\n-15.07\nISTA (T = 20)\n-9.17\n-9.12\n-9.24\n-9.16\nFISTA (T = 20)\n-11.12\n-10.98\n-11.19\n-11.19\nLISTA (T = 20)\n-17.53\n-16.53\n-18.07\n-18.20\nLISTA-stop (T â©½20)\n-22.41\n-20.29\n-23.90\n-24.21\nmances, especially for the more difï¬cult tasks (i.e. when\nSNR is 20). Compared to LISTA, our proposed adaptive-\nstopping method (LISTA-stop) signiï¬cantly improve recov-\nery performance. Also, LISTA-stop with â©½20 iterations\nperforms better than ISTA and FISTA with 100 iterations,\nwhich indicates a better convergence.\nStopping distribution. The stop time distribution qÏ†(t) in-\nduced by Ï€Ï† can be computed via Eq. 3. We report in Fig. 4\nthe stopping distribution averaged over the test samples,\nfrom which we can see that with a high probability LISTA-\nstop terminates the process before arriving at 20-th iteration.\n14\n15\n16\n17\n18\n19\n20\niteration t\n0.0\n0.2\n0.4\n0.6\n0.8\nq (t)\n0.001\n0.683\n0.267\n0.0370.0060.0040.002\n0\n5\n10\n15\n20\niteration t\n20\n15\n10\n5\n0\naveraged NMSE\nLISTA-stop\nLISTA\nISTA\nFISTA\n(a) stop time distribution\n(b) convergence\nFigure 4. Left: Stop time distribution\n1\n|Dtest|\nP\nxâˆˆDtest qÏ†(t|x)\naveraged over the test set. Right: Convergence of different al-\ngorithms. For LISTA-stop, the NMSE weighted by the stopping\ndistribution qÏ† is plotted. In the ï¬rst 13 iterations qÏ†(t) = 0, so\nno red dots are plotted.\nConvergence comparison. Fig. 4 shows the change of\nNMSE as the number of iterations increases. Since LISTA-\nstop outputs the results at different iteration steps, it is not\nmeaningful to draw a uniï¬ed convergence curve. Therefore,\nwe plot the NMSE weighted by the stopping distribution qÏ†,\ni.e., 10 log10(\nPN\ni=1 qÏ†(t|i)âˆ¥xtâˆ’xâˆ—,iâˆ¥2\n2\nPN\ni=1 qÏ†(t|i)\n/(\nPN\ni=1 âˆ¥xâˆ—,iâˆ¥2\n2\nN\n), using\nthe red dots. We observe that for LISTA-stop the expected\nNMSE increases as the number of iterations increase, this\nmight indicate that the later stopped problems are more\ndifï¬cult to solve. Besides, at 15th iteration, the NMSE in\nFig. 4 (b) is the smallest, while the averaged stop probability\nmass qÏ†(15) in Fig. 4 (a) is the highest.\nTable 3. Different algorithms for training LISTA-stop.\nSNR\nmixed\n20\n30\n40\nAEVB algorithm\n-21.92\n-19.92\n-23.27\n-23.58\nStage I. + II.\n-22.41\n-20.29\n-23.90\n-24.21\nStage I.+II.+III.\n-22.78\n-20.59\n-24.29\n-24.73\nAblation study on training algorithms. To show the ef-\nfectiveness of our two-stage training, in Table 3, we com-\npare the results with the auto-encoding variational Bayes\n(AEVB) algorithm (Le et al., 2018) that jointly optimizes\nFÎ¸ and qÏ†. We observe that the distribution qÏ† in AEVB\ngradually becomes concentrated on one layer and does not\nget rid of this local minimum, making its ï¬nal result not as\ngood as the results of our two-stage training. Moreover, it is\nrevealed that Stage III does not improve much of the perfor-\nmance of the two-stage training, which also in turn shows\nthe effectiveness of the oracle-based two-stage training.\n5.2. Task-imbalanced Meta Learning\nIn this section, we perform meta learning experiments in the\nfew-short learning domain (Ravi & Larochelle, 2017).\nExperiment setting. We follow the setting in MAML (Finn\net al., 2017) for the few-shot learning tasks. Each task is an\nN-way classiï¬cation that contains meta-{train, valid, test}\nsets. On top of it, the macro dataset with multiple tasks is\nsplit into train, valid and test sets. We consider the more re-\nalistic task-imbalanced setting proposed by Na et al. (2020).\nUnlike the standard setting where the meta-train of each\ntask contains k-shots for each class, here we vary the num-\nber of observation to perform k1- k2-shot learning where\nk1 < k2 are the minimum/maximum number of observa-\ntions per class, respectively. Build on top of MAML, we\ndenote our variant as MAML-stop which learns how many\nadaptation gradient descent steps are needed for each task.\nIntuitively, the tasks with less training data would prefer\nfewer steps of gradient-update to prevent overï¬tting. As we\nmainly focus on the effect of learning to stop, the neural\narchitecture and other hyperparameters are largely the same\nas MAML. Please refer to Appendix B.2 for more details.\nDataset. We use the benchmark datasets Omniglot (Lake\net al., 2011) and MiniImagenet (Ravi & Larochelle, 2017).\nOmniglot consists of 20 instances of 1623 characters from\n50 different alphabets, while MiniImagenet involves 64\ntraining classes, 12 validation classes, and 24 test classes.\nWe use exactly the same data split as Finn et al. (2017). To\nconstruct the imbalanced tasks, we perform 20-way 1-5\nshot classiï¬cation on Omniglot and 5-way 1-10 shot clas-\nsiï¬cation on MiniImagenet. The number of observations\nper class in each meta-test set is 1 and 5 for Omniglot and\nMiniImagenet, respectively. For evaluation, we construct\n600 tasks from the held-out test set for each setting.\nTable 4. Task-imbalanced few-shot image classiï¬cation.\nOmniglot\nMiniImagenet\n20-way, 1-5 shot\n5-way, 1-10 shot\nMAML\n97.96 Â± 0.3%\n57.20 Â± 1.1%\nMAML-stop\n98.45 Â± 0.2%\n60.67 Â± 1.0%\nResults. Table 4 summarizes the accuracy and the 95% con-\nLearning to Stop While Learning to Predict\nTable 5. Few-shot classiï¬cation in vanilla meta learning setting (Finn et al., 2017) where all tasks have the same number of data points.\nOmniglot 5-way\nOmniglot 20-way\nMiniImagenet 5-way\n1-shot\n5-shot\n1-shot\n5-shot\n1-shot\n5-shot\nMAML\n98.7 Â± 0.4%\n99.1 Â± 0.1%\n95.8 Â± 0.3%\n98.9 Â± 0.2%\n48.70 Â± 1.84%\n63.11 Â± 0.92%\nMAML-stop\n99.62 Â± 0.22%\n99.68 Â± 0.12%\n96.05 Â± 0.35%\n98.94 Â± 0.10 %\n49.56 Â± 0.82%\n63.41 Â± 0.80%\nï¬dence interval on the held-out tasks for each dataset. The\nmaximum number of adaptation gradient descent steps is 10\nfor both MAML and MAML-stop. We can see the optimal\nstopping variant of MAML outperforms the vanilla MAML\nconsistently. For a more difï¬cult task on MiniImagenet\nwhere the imbalance issue is more severe, the accuracy\nimprovement is 3.5%. For completeness, we include the\nperformance on vanilla meta learning setting where all tasks\nhave the same number of observations in Table 5. MAML-\nstop still achieves comparable or better performance.\n5.3. Image Denoising\nIn this section, we perform the image denoising experiments.\nMore implementation details are provided in Appendix B.3.\nDataset. The models are trained on BSD500 (400 images)\n(Arbelaez et al., 2010), validated on BSD12, and tested on\nBSD68 (Martin et al., 2001). We follow the standard setting\nin (Zhang et al., 2019; Lefkimmiatis, 2018; Zhang et al.,\n2017) to add Gaussian noise to the images with a random\nnoise level Ïƒ â©½55 during training and validation phases.\nExperiment setting. We compare with two DL models,\nDnCNN (Zhang et al., 2017) and UNLNet5 (Lefkimmiatis,\n2018), and two traditional methods, BM3D (Dabov et al.,\n2007) and WNNM (Gu et al., 2014). Since DnCNN is one\nof the most widely-used models for image denoising, we\nuse it as our predictive model. All deep models including\nours are considered in the blind Gaussian denoising setting,\nwhich means the noise-level is not given to the model, while\nBM3D and WNNM require the noise-level to be known.\nTable 6. PSNA performance comparison. The sign * indicates that\nnoise levels 65 and 75 do not appear in the training set.\nÏƒ\nDnCNN-stop DnCNN UNLNet5\nBM3D\nWNNM\n35\n27.61\n27.60\n27.50\n26.81\n27.36\n45\n26.59\n26.56\n26.48\n25.97\n26.31\n55\n25.79\n25.71\n25.64\n25.21\n25.50\n*65\n23.56\n22.19\n-\n24.60\n24.92\n*75\n18.62\n17.90\n-\n24.08\n24.39\nResults. The performance is evaluated by the mean peak\nsignal-to-noise ratio (PSNR). Table 6 shows that DnCNN-\nstop performs better than the original DnCNN. Especially,\nfor images with noise levels 65 and 75 which are unseen dur-\ning training phase, DnCNN-stop generalizes signiï¬cantly\nbetter than DnCNN alone. Since there is no released code\nfor UNLNet5, its performances are copied from the pa-\nper (Lefkimmiatis, 2018), where results are not reported\nfor Ïƒ = 65 and 75. For traditional methods BM3D and\nWNNM, the test is in the noise-speciï¬c setting. That is,\nthe noise level is given to both BM3D and WNNM, so the\ncomparison is not completely fair to learning based methods\nin blind denoising setting.\nGround Truth\nWNNM\nDnCNN\nDnCNN-stop\nFigure 5. Denoising results of an image with noise level 65. (See\nAppendix B.3.2 for more visualization results.)\n5.4. Image Recognition\nWe explore the potential of our idea for improving the recog-\nnition performances on Tiny-ImageNet, using VGG16 (Si-\nmonyan & Zisserman, 2014) as the predictive model. With\n14 internal classiï¬ers, after Stage I training, if the oracle qâˆ—\nÎ¸\nis used to determine the stop time t, the accuracy of VGG16\ncan be improved to 83.26%. Similar observation is provided\nin SDN (Kaya et al., 2019), but their loss P\nt wtâ„“t depends\non very careful hand-tuning on the weight wt for each layer,\nwhile we directly take an expectation using the oracle, which\nis more principled and leads to higher accuracy (Table 7).\nHowever, it reveals to be very hard to mimic the behavior of\nthe orcale qâˆ—\nÎ¸ by Ï€Ï† in Stage II, either due to the need of a\nbetter parametrization for Ï€Ï† or more sophisticated reasons.\nOur learned Ï€Ï† leads to similar accuracy as the heuristic\npolicy in SDN, which becomes the bottleneck in our ex-\nploration. However, based on the large performance gap\nbetween the oracle and the original VGG16, our result still\nprovides a potential direction for breaking the performance\nbottleneck of DL on image recognition.\nTable 7. Image recognition with oracle stop distribution.\nVGG16\nSDN training\nOur Stage I. training\n58.60%\n77.78% (best layer)\n83.26% (best layer)\nLearning to Stop While Learning to Predict\n6. Conclusion\nIn this paper, we introduce a generic framework for mod-\nelling and training a deep learning model with input-speciï¬c\ndepth, which is determined by a stopping policy Ï€Ï†. Ex-\ntensive experiments are conducted to demonstrate the ef-\nfectiveness of both the model and the training algorithm,\non a wide range of applications. In the future, it will be\ninteresting to see whether other aspects of algorithms can\nbe incorporated into deep learning models either to improve\nthe performance or for better theoretical understandings.\nReferences\nAndrychowicz, M., Denil, M., Gomez, S., Hoffman, M. W.,\nPfau, D., Schaul, T., Shillingford, B., and De Freitas, N.\nLearning to learn by gradient descent by gradient descent.\nIn Advances in Neural Information Processing Systems,\npp. 3981â€“3989, 2016.\nArbelaez, P., Maire, M., Fowlkes, C., and Malik, J. Contour\ndetection and hierarchical image segmentation. IEEE\ntransactions on pattern analysis and machine intelligence,\n33(5):898â€“916, 2010.\nBeck, A. and Teboulle, M.\nA fast iterative shrinkage-\nthresholding algorithm for linear inverse problems. SIAM\njournal on imaging sciences, 2(1):183â€“202, 2009.\nBecker, S., Cheridito, P., and Jentzen, A. Deep optimal\nstopping. Journal of Machine Learning Research, 20(74):\n1â€“25, 2019.\nBelanger, D., Yang, B., and McCallum, A. End-to-end learn-\ning for structured prediction energy networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 429â€“439. JMLR. org, 2017.\nBlumensath, T. and Davies, M. E. Iterative thresholding for\nsparse approximations. Journal of Fourier analysis and\nApplications, 14(5-6):629â€“654, 2008.\nBorgerding, M., Schniter, P., and Rangan, S. Amp-inspired\ndeep networks for sparse linear inverse problems. IEEE\nTransactions on Signal Processing, 65(16):4293â€“4308,\n2017.\nCeci, C. and Bassan, B. Mixed optimal stopping and stochas-\ntic control problems with semicontinuous ï¬nal reward for\ndiffusion processes. Stochastics and Stochastic Reports,\n76(4):323â€“337, 2004.\nChen, X., Liu, J., Wang, Z., and Yin, W. Theoretical linear\nconvergence of unfolded ista and its practical weights and\nthresholds. In Advances in Neural Information Process-\ning Systems, pp. 9061â€“9071, 2018.\nChen, X., Dai, H., and Song, L. Particle ï¬‚ow bayes rule.\nIn International Conference on Machine Learning, pp.\n1022â€“1031, 2019.\nChen, X., Li, Y., Umarov, R., Gao, X., and Song, L. RNA\nsecondary structure prediction by learning unrolled algo-\nrithms. arXiv preprint arXiv:2002.05810, 2020.\nDabov, K., Foi, A., Katkovnik, V., and Egiazarian, K. Image\ndenoising by sparse 3-d transform-domain collaborative\nï¬ltering. IEEE Transactions on image processing, 16(8):\n2080â€“2095, 2007.\nDomke, J.\nParameter learning with truncated message-\npassing. In CVPR 2011, pp. 2937â€“2943. IEEE, 2011.\nDumitrescu, R., Reisinger, C., and Zhang, Y. Approximation\nschemes for mixed optimal stopping and control problems\nwith nonlinear expectations and jumps. arXiv preprint\narXiv:1803.03794, 2018.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 1126â€“1135. JMLR. org, 2017.\nGregor, K. and LeCun, Y. Learning fast approximations\nof sparse coding. In Proceedings of the 27th Interna-\ntional Conference on International Conference on Ma-\nchine Learning, pp. 399â€“406. Omnipress, 2010.\nGu, S., Zhang, L., Zuo, W., and Feng, X. Weighted nuclear\nnorm minimization with application to image denoising.\nIn Proceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pp. 2862â€“2869, 2014.\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,\nBotvinick, M., Mohamed, S., and Lerchner, A. beta-\nVAE: Learning basic visual concepts with a constrained\nvariational framework. ICLR, 2(5):6, 2017.\nHuang, G., Chen, D., Li, T., Wu, F., van der Maaten, L., and\nWeinberger, K. Multi-scale dense networks for resource\nefï¬cient image classiï¬cation. In International Conference\non Learning Representations, 2018. URL https://\nopenreview.net/forum?id=Hk2aImxAb.\nIngraham, J., Riesselman, A., Sander, C., and Marks, D.\nLearning protein structure with a differentiable simulator.\nIn International Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=Byg3y3C9Km.\nJones, M., Kinoshita, S., and Mozer, M. C. Optimal re-\nsponse initiation: Why recent experience matters. In\nAdvances in neural information processing systems, pp.\n785â€“792, 2009.\nLearning to Stop While Learning to Predict\nKaya, Y., Hong, S., and Dumitras, T. Shallow-deep net-\nworks: Understanding and mitigating network overthink-\ning. In International Conference on Machine Learning,\npp. 3301â€“3310, 2019.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nLake, B., Salakhutdinov, R., Gross, J., and Tenenbaum, J.\nOne shot learning of simple visual concepts. In Proceed-\nings of the annual meeting of the cognitive science society,\nvolume 33, 2011.\nLe, T. A., Igl, M., Rainforth, T., Jin, T., and Wood, F. Auto-\nencoding sequential monte carlo. In International Con-\nference on Learning Representations, 2018.\nLee, Y. and Choi, S. Gradient-based meta-learning with\nlearned layerwise metric and subspace. arXiv preprint\narXiv:1801.05558, 2018.\nLefkimmiatis, S. Universal denoising networks: a novel\ncnn architecture for image denoising. In Proceedings\nof the IEEE conference on computer vision and pattern\nrecognition, pp. 3204â€“3213, 2018.\nLi, K. and Malik, J. Learning to optimize. arXiv preprint\narXiv:1606.01885, 2016.\nLi, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning\nto learn quickly for few-shot learning. arXiv preprint\narXiv:1707.09835, 2017.\nLiu, J., Chen, X., Wang, Z., and Yin, W. ALISTA: Analytic\nweights are as good as learned weights in LISTA. In\nInternational Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=B1lnzn0ctQ.\nMartin, D., Fowlkes, C., Tal, D., and Malik, J. A database\nof human segmented natural images and its application\nto evaluating segmentation algorithms and measuring\necological statistics. In Proceedings Eighth IEEE Inter-\nnational Conference on Computer Vision. ICCV 2001,\nvolume 2, pp. 416â€“423. IEEE, 2001.\nMetzler, C., Mousavi, A., and Baraniuk, R. Learned d-\namp: Principled neural network based compressive image\nrecovery. In Advances in Neural Information Processing\nSystems, pp. 1772â€“1783, 2017.\nNa, D., Lee, H. B., Lee, H., Kim, S., Park, M., Yang, E.,\nand Hwang, S. J. Learning to balance: Bayesian meta-\nlearning for imbalanced and out-of-distribution tasks. In\nInternational Conference on Learning Representations,\n2020. URL https://openreview.net/forum?\nid=rkeZIJBYvr.\nNowozin, S., Cseke, B., and Tomioka, R. f-gan: Training\ngenerative neural samplers using variational divergence\nminimization. In Advances in neural information process-\ning systems, pp. 271â€“279, 2016.\nOreshkin, B., LÂ´opez, P. R., and Lacoste, A. Tadam: Task de-\npendent adaptive metric for improved few-shot learning.\nIn Advances in Neural Information Processing Systems,\npp. 721â€“731, 2018.\nPham, H. Optimal stopping of controlled jump diffusion\nprocesses: a viscosity solution approach. In Journal of\nMathematical Systems, Estimation and Control. Citeseer,\n1998.\nQiao, S., Liu, C., Shen, W., and Yuille, A. L. Few-shot im-\nage recognition by predicting parameters from activations.\nIn Proceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pp. 7229â€“7238, 2018.\nRavi, S. and Larochelle, H. Optimization as a model for\nfew-shot learning. 2017.\nShiryaev, A. N. Optimal stopping rules, volume 8. Springer\nScience & Business Media, 2007.\nShrivastava, H., Chen, X., Chen, B., Lan, G., Aluru, S., Liu,\nH., and Song, L. GLAD: Learning sparse graph recovery.\nIn International Conference on Learning Representations,\n2020. URL https://openreview.net/forum?\nid=BkxpMTEtPB.\nSimonyan, K. and Zisserman, A.\nVery deep convolu-\ntional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\nSun, J., Li, H., Xu, Z., et al. Deep admm-net for com-\npressive sensing mri. In Advances in neural information\nprocessing systems, pp. 10â€“18, 2016.\nTeerapittayanon, S., McDanel, B., and Kung, H.-T.\nBranchynet: Fast inference via early exiting from deep\nneural networks. In 2016 23rd International Conference\non Pattern Recognition (ICPR), pp. 2464â€“2469. IEEE,\n2016.\nYakar, T. B., Litman, R., Sprechmann, P., Bronstein, A. M.,\nand Sapiro, G. Bilevel sparse models for polyphonic\nmusic transcription. In ISMIR, pp. 65â€“70, 2013.\nZamir, A. R., Wu, T.-L., Sun, L., Shen, W. B., Shi, B. E.,\nMalik, J., and Savarese, S. Feedback networks. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 1308â€“1317, 2017.\nZhang, J. and Ghanem, B.\nIsta-net:\nInterpretable\noptimization-inspired deep network for image compres-\nsive sensing. In Proceedings of the IEEE Conference\nLearning to Stop While Learning to Predict\non Computer Vision and Pattern Recognition, pp. 1828â€“\n1837, 2018.\nZhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L.\nBeyond a gaussian denoiser: Residual learning of deep\ncnn for image denoising. IEEE Transactions on Image\nProcessing, 26(7):3142â€“3155, 2017.\nZhang, X., Lu, Y., Liu, J., and Dong, B. Dynamically un-\nfolding recurrent restorer: A moving endpoint control\nmethod for image restoration. In International Confer-\nence on Learning Representations, 2019. URL https:\n//openreview.net/forum?id=SJfZKiC5FX.\nLearning to Stop While Learning to Predict\nA. Derivations\nA.1. Proof of Lemma 1\nProof. Under the assumptions that\nâ„“(y, xt; Î¸) := âˆ’log pÎ¸(y|t, x);\nand the prior p(t|x) is a uniform distribution over t, the Î²-VAE objective can be written as\nJÎ²-VAE(Î¸, qÏ†; x, y) :=\nEqÏ† log pÎ¸(y|t, x) âˆ’Î²KL(qÏ†(t)||p(t|x))\n= âˆ’EqÏ†â„“(y, xt; Î¸) âˆ’Î²EqÏ†(t) log qÏ†(t)\np(t|x)\n= âˆ’EqÏ†â„“(y, xt; Î¸) âˆ’Î²EqÏ†(t) log qÏ†(t)\n+ Î²EqÏ†(t) log p(t|x)\n= âˆ’\n\u0000EqÏ†â„“(y, xt; Î¸) âˆ’Î²H(qÏ†)\n\u0001\n+ Î²EqÏ†(t) log 1\nT\n= âˆ’L(Î¸, qÏ†; x, y) âˆ’Î² log T.\nSince the second term âˆ’Î² log T is a constant, maximizing JÎ²-VAE(Î¸, qÏ†; x, y) is equivalent to minimizing L(Î¸, qÏ†; x, y).\nA.2. Equivalence of reverse KL and maximum-entropy RL\nThe variational distribution qÏ† actually depends on the input instance x. For notation simplicity, we only write qÏ†(t) instead\nof qÏ†(t|x).\nmin\nÏ† KL(qÏ†(t)||qâˆ—\nÎ¸(t|y, x))\n(12)\n= min\nÏ† âˆ’\nT\nX\nt=1\nqÏ†(t) log qâˆ—\nÎ¸(t|y, x)) âˆ’H(qÏ†)\n(13)\n= min\nÏ† âˆ’\nT\nX\nt=1\nqÏ†(t) log pÎ¸(y|t, x)Î²) âˆ’H(qÏ†)\n(14)\n+\nT\nX\nt=1\nqÏ†(t) log\nT\nX\nÏ„=1\npÎ¸(y|Ï„, x)Î²\n(15)\n= min\nÏ† âˆ’\nT\nX\nt=1\nqÏ†(t) log pÎ¸(y|t, x)Î² âˆ’H(qÏ†)\n(16)\n+\nT\nX\nt=1\nqÏ†(t)C(x, y)\n(17)\n= min\nÏ† âˆ’\nT\nX\nt=1\nqÏ†(t) log pÎ¸(y|t, x)Î² âˆ’H(qÏ†)\n(18)\n+ C(x, y)\n(19)\n= min\nÏ† âˆ’\nT\nX\nt=1\nqÏ†(t) log pÎ¸(y|t, x)Î² âˆ’H(qÏ†)\n(20)\n= max\nÏ†\nT\nX\nt=1\nqÏ†(t) log pÎ¸(y|t, x)Î² + H(qÏ†)\n(21)\n= max\nÏ†\nT\nX\nt=1\nqÏ†(t)Î²â„“(y, xt; Î¸) + H(qÏ†)\n(22)\nLearning to Stop While Learning to Predict\n= max\nÏ†\nEtâˆ¼qÏ† [âˆ’Î²â„“(y, xt; Î¸) âˆ’log qÏ†(t)]\n(23)\nDeï¬ne the action as at âˆ¼Ï€t = Ï€Ï†(x, xt), the reward function as\nr(xt, at; y) :=\n(\nâˆ’Î²â„“(y, xt; Î¸)\nif at = 1 (i.e. stop),\n0\nif at = 0 (i.e. continue),\nand the transition probability as\nP(xt+1|xt, at) =\n(\n1\nif xt+1 = FÎ¸(xt) and at = 0,\n0\nelse.\nThen the above optimization can be written as\nmax\nÏ†\nEtâˆ¼qÏ† [âˆ’Î²â„“(y, xt; Î¸) âˆ’log qÏ†(t)]\n(24)\n= max\nÏ†\nEÏ€Ï†\nT\nX\nt=1\nr(xt, at; y) âˆ’log Ï€t(at|x, xt)\n(25)\n= max\nÏ†\nEÏ€Ï†\nT\nX\nt=1\n[r(xt, at; y) + H(Ï€t)] .\n(26)\nB. Experiment Details\nB.1. Learning To Learn: Sparse Recovery\nSynthetic data. We follow Chen et al. (2018) to choose m = 250, n = 500, sample the entries of A i.i.d. from the standard\nGaussian distribution, i.e., Aij âˆ¼N(0, 1\nm), and then normalize its columns to have the unit â„“2 norm. To generate yâˆ—, we\ndecide each of its entry to be non-zero following the Bernoulli distribution with pb = 0.1. The values of the non-zero entries\nare sampled from the standard Gaussian distribution. The noise Ïµ is Gaussian white noise. The signal-to-noise ratio (SNR)\nfor each sample is uniformly sampled from 20, 30 and 40. For the testing phase, a test set of 3000 samples are generated,\nwhere there are 1000 samples for each noise level. This test set is ï¬xed for all experiments in our simulations.\nEvaluation metric. The performance is evaluated by NMSE (in dB), which is deï¬ned as 10 log10(\nPN\ni=1 âˆ¥Ë†xiâˆ’xâˆ—,iâˆ¥2\n2\nPN\ni=1 âˆ¥xâˆ—,iâˆ¥2\n2\n) where\nË†xi is the estimator returned by an algorithm or deep model.\nB.2. Task-imbalanced Meta Learning\nB.2.1. DETAILS OF SETUP\nHyperparameters\nWe train MAML with batch size 16 on Omniglot imbalanced and batch size 2 on MiniImagenet\nimbalanced datasets. In both scenario we train with 60000 of mini-batch updates for the outer-loop of MAML. We report the\nresults with 5 inner SGD steps for Omniglot imbalanced and 10 inner SGD steps for MiniImagenet imbalanced with other\nbest hyperparameters suggested in (Finn et al., 2017), respectively. For MAML-stop we run 10 inner SGD steps for both\ndatasets, with the inner learning rate to be 0.1 and 0.05 for Omniglot and MiniImagenet, respectively. The outer learning\nrate for MAML-stop is 1eâˆ’4 as we use batch size 1 for training.\nWhen generating each meta-training dataset, we randomly select the number of observations within k1 to k2 for k1-k2-shot\nlearning. The number of observations in test set is always kept the same within each round of experiment.\nB.2.2. MEMORY EFFICIENT IMPLEMENTATION\nAs our MAML-stop allows the automated decision of optimal stopping, it is preferable that the maximum number of SGD\nupdates per each task is set to a larger number to fully utilize the capacity of the approach. This brings the challenge during\ntraining, as the loss on each meta-test set during training is required for each single inner update step. That is to say, if we\nallow maximumly 10 steps of inner SGD update, then the memory cost for running CNN prediction on meta-test set is 10x\nlarger than vanilla MAML. Thus a straightforward implementation will not give us a feasible training mechanism.\nLearning to Stop While Learning to Predict\nTo make the training of MAML-stop feasible on a single GPU, we utilize the following techniques:\nâ€¢ We use stochastic EM for learning the predictive model, as well as the stopping policy. Speciï¬cally, we sample\nt âˆ¼qâˆ—\nÎ¸(Â·|y, x) in each round of training, and only maximize pÎ¸(y|t, x) in this round.\nâ€¢ As the auto differentiation in PyTorch is unable to distinguish between â€˜no gradientâ€™ and â€˜zero gradientâ€™, it causes extra\nstorage for the unnecessary gradient computation. To overcome this, we ï¬rst calculate qâˆ—\nÎ¸(t|y, x) for each t without any\ngradient storage (which corresponds to no grad() in PyTorch), then recompute pÎ¸(y|t, x) for the sampled t.\nWith the above techniques, we can train MAML-stop almost as (memory) efï¬cient as MAML.\nB.2.3. STANDARD META-LEARNING TASKS\nFor completeness, we also include the MAML-stop in the standard setting of few-shot learning. We mainly compared with\nthe vanilla MAML for the sake of ablation study.\nHyperparameters\nThe hyperparameter setup mainly follows the vanilla MAML paper. For both MAML and MAML-\nstop, we use the same batch size, number of training epochs and the learning rate. For Omniglot 20-way experiments and\nMiniImagenet 5-way experiments, we tune the number of unrolling steps in {5, 6, . . . , 10}, Î² in {0, 0.1, 0.01, 0.001} and\nthe learning rate of inner update in {0.1, 0.05}. We simply use grid search with a random held-out set with 600 tasks to\nselect the best model conï¬guration.\nB.3. Image Denoising\nB.3.1. IMPLEMENTATION DETAILS\nWhen training the denoising models, the raw images were cropped and augmented into 403K 50 âˆ—50 patchs. The training\nbatch size was 256. We used Adam optimizer with the initial learning rate as 1e âˆ’4. We ï¬rst trained the deep learning\nmodel with the unweighted loss for 50 epochs. Then, we further train the model with the weighted loss for another 50\nepoches. After hyper-parameter searching, we set the exploration coefï¬cient Î² as 0.1. When training the policy network, we\nused the Adam optimizer with the learning rate as 1e âˆ’4. We reused the above hyper-parameters during joint training.\nB.3.2. VISUALIZATION\nGround Truth\nNoisy Image\nBM3D\nWNNM\nDnCNN\nDnCNN-stop\nFigure 6. Denoising results of an image with noise level 65.\nLearning to Stop While Learning to Predict\nGround Truth\nNoisy Image\nBM3D\nWNNM\nDnCNN\nDnCNN-stop\nFigure 7. Denoising results of an image with noise level 65.\nB.4. Computing infrastructure\nMost of the experiments were run a hetergeneous GPU cluster. For each experiment, we typically used one or two V100\ncards, with the typical CPU processor as Intel Xeon Platinum 8260L. We assigned 6 threads and 64 GB CPU memory for\neach V100 card to maximize the utilization of the card.\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-06-09",
  "updated": "2020-06-09"
}