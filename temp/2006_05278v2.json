{
  "id": "http://arxiv.org/abs/2006.05278v2",
  "title": "An Overview of Deep Semi-Supervised Learning",
  "authors": [
    "Yassine Ouali",
    "Céline Hudelot",
    "Myriam Tami"
  ],
  "abstract": "Deep neural networks demonstrated their ability to provide remarkable\nperformances on a wide range of supervised learning tasks (e.g., image\nclassification) when trained on extensive collections of labeled data (e.g.,\nImageNet). However, creating such large datasets requires a considerable amount\nof resources, time, and effort. Such resources may not be available in many\npractical cases, limiting the adoption and the application of many deep\nlearning methods. In a search for more data-efficient deep learning methods to\novercome the need for large annotated datasets, there is a rising research\ninterest in semi-supervised learning and its applications to deep neural\nnetworks to reduce the amount of labeled data required, by either developing\nnovel methods or adopting existing semi-supervised learning frameworks for a\ndeep learning setting. In this paper, we provide a comprehensive overview of\ndeep semi-supervised learning, starting with an introduction to the field,\nfollowed by a summarization of the dominant semi-supervised approaches in deep\nlearning.",
  "text": "An Overview of Deep Semi-Supervised Learning\nYassine Ouali \u0000∗\nCéline Hudelot\nMyriam Tami\nUniversité Paris-Saclay, CentraleSupélec, MICS, 91190, Gif-sur-Yvette, France\n{yassine.ouali,celine.hudelot,myriam.tami}@centralesupelec.fr\nAbstract\nDeep neural networks demonstrated their ability to provide remarkable performances on a wide range\nof supervised learning tasks (e.g., image classiﬁcation) when trained on extensive collections of labeled\ndata (e.g., ImageNet). However, creating such large datasets requires a considerable amount of resources,\ntime, and eﬀort. Such resources may not be available in many practical cases, limiting the adoption and\nthe application of many deep learning methods. In a search for more data-eﬃcient deep learning methods\nto overcome the need for large annotated datasets, there is a rising research interest in semi-supervised\nlearning and its applications to deep neural networks to reduce the amount of labeled data required,\nby either developing novel methods or adopting existing semi-supervised learning frameworks for a deep\nlearning setting. In this paper, we provide a comprehensive overview of deep semi-supervised learning,\nstarting with an introduction to the ﬁeld, followed by a summarization of the dominant semi-supervised\napproaches in deep learning1.\nKeywords: semi-supervised learning, deep learning, neural networks, consistency training, entropy mini-\nmization, proxy labeling, generative models, graph neural networks.\n1\nIntroduction\nIn recent years, semi-supervised learning (SSL) has emerged as an exciting new research direction in deep\nlearning. Such methods deal with the situation where few labeled training examples are available together\nwith a signiﬁcant number of unlabeled samples. In such a setting, SSL methods are more applicable to real-\nworld applications where the unlabeled data are readily available and easy to acquire, while labeled instances\nare often hard, expensive, and time-consuming to collect. SSL is capable of building better classiﬁers that\ncompensate for the lack of labeled training data. However, in order to avoid a lousy matching of the problem\nstructure with the model assumption, which can lead to a degradation in classiﬁcation performance [193],\nSSL is only eﬀective under certain assumptions, such as assuming that the decision boundary should avoid\nregions with high density, facilitating the extraction of additional information from the unlabeled instances\nto regularize training. In this paper, we will start by an introduction to SSL with its main assumptions and\nmethods, followed by a summarization of the dominant semi-supervised approaches in deep learning. For a\ndetailed and comprehensive review of the ﬁeld, Semi-Supervised Learning Book [20] is a good resource.\n1.1\nSemi-supervised learning\n∗Corresponding author, any corrections, contributions or suggestions are welcomed.\n1A curated and an up-to-date list of SSL papers is available at this link.\n1\narXiv:2006.05278v2  [cs.LG]  6 Jul 2020\n-Model\nPseudo-Labeling\nVAT\nEntropy Minimization\nSupervised\nClass A\nClass B\nUnlabeled\nFigure 1: SSL toy example. The decision boundaries obtained on two-moons dataset, with a supervised\nand diﬀerent SSL approaches using 6 labeled examples, 3 for each class, and the rest of the points as unlabeled\ndata.\n“Semi-supervised learning (SSL) is halfway between supervised and unsupervised learning.\nIn\naddition to unlabeled data, the algorithm is provided with some supervision information – but\nnot necessarily for all examples. Often, this information will be the targets associated with some\nof the examples. In this case, the data set X = (xi); i ∈[n] can be divided into two parts:\nthe points Xl := (x1, . . . , xl), for which labels Yl := (y1, . . . , yl) are provided, and the points\nXu := (xl+1, . . . , xl+u), the labels of which are not known.” – Chapelle et al. [20].\nAs stated in the deﬁnition above, in SSL, we are provided with a dataset containing both labeled and\nunlabeled examples.\nThe portion of labeled examples is usually quite small compared to the unlabeled\nexample (e.g., 1 to 10% of the total number of examples). So with a dataset D containing a labeled subset\nDl and an unlabeled subset Du, the objective, or rather hope, is to leverage the unlabeled examples to train\na better performing model than what can be obtained using only the labeled portion. And hopefully, get\ncloser to the desired optimal performance, in which all of the dataset D is labeled.\nMore formally, the goal of SSL is to leverage the unlabeled data Du to produce a prediction function\nfθ with trainable parameters θ, that is more accurate than what would have been obtained by only using\nthe labeled data Dl. For instance, Du might provide us with additional information about the structure\nof the data distribution p(x) to better estimate the decision boundary between the diﬀerent classes. For\nexample, as shown in ﬁg. 1, where the data points with distinct labels are separated with a low-density\nregion, leveraging unlabeled data with a SSL approach can provide us with additional information about\nthe shape of the decision boundary between two classes, and reduce the ambiguity present in the supervised\ncase.\nSSL ﬁrst appeared in the form of self-training [20], which is also known as self-labeling or self-teaching.\nA model is ﬁrst trained on labeled data. Then, iteratively, a portion of the unlabeled data is annotated using\nthe trained model and added to the training set for the next training iteration. SSL took oﬀin the 1970s\nafter its success with iterative algorithms such as the expectation-maximization algorithm [109], in which\nthe labeled and unlabeled data are jointly used to maximize the likelihood of the model.\n1.2\nSSL Methods\nThere have been many SSL methods and approaches that have been introduced over the years.\nThese\nalgorithms can be broadly divided into the following categories:\n• Consistency Regularization (a.k.a Consistency Training). Based on the assumption that if\na realistic perturbation was applied to the unlabeled data points, the prediction should not change\nsigniﬁcantly. The model can then be trained to have a consistent prediction on a given unlabeled\nexample and its perturbed version.\n2\n• Proxy-label Methods. Such methods leverage a trained model on the labeled set to produce addi-\ntional training examples by labeling instances of the unlabeled set based on some heuristics. These\napproaches can also be referred to as bootstrapping [14] algorithms. We follow Ruder et al. [133] and\nrefer to them as proxy-label methods. Some examples of such methods are Self-training, Co-training\nand Multi-View Learning.\n• Generative Models. Similar to the supervised setting, where the learned features on one task can\nbe transferred to other downstream tasks. Generative models that are able to generate images from\nthe data distribution p(x) must learn transferable features to a supervised task p(y|x) for a given task\nwith targets y.\n• Graph-Based Methods. The labeled and unlabeled data points can be considered as nodes of a\ngraph, and the objective is to propagate the labels from the labeled nodes to the unlabeled ones by\nutilizing the similarity of two nodes xi and xj, which is reﬂected by how strong the edge eij between\nthe two nodes.\nIn addition to these main categories, there is also some SSL work on entropy minimization, where we\nforce the model to make conﬁdent predictions by minimizing the entropy of the predictions. Consistency\ntraining can also be considered a proxy-label method, with a subtle diﬀerence, instead of considering the\npredictions as ground-truths and compute the cross-entropy loss, we enforce consistency of predictions by\nminimizing a given distance between the outputs.\nSSL methods can also be categorized based on two dominant learning paradigms, transductive learn-\ning and inductive learning. Transductive learning aims to apply the trained classiﬁer on the unlabeled\ninstances observed at training time; in this case, it does not generalize to unobserved instances. This type of\nalgorithm is mainly used on graphs, such as random walks for node embedding [119, 59], where the objective\nis to label the unlabeled nodes of the graph that are present at training time. The more popular paradigm,\ninductive learning, aims to learn a classiﬁer capable of generalizing to unobserved instances at test time.\n1.3\nMain Assumptions in SSL\nThe ﬁrst question we need to answer is under what assumptions can we apply SSL algorithms? SSL al-\ngorithms only work under some assumptions about the structure of the data need to hold. Without such\nassumptions, it would not be possible to generalize from a ﬁnite training set to a set of possibly inﬁnitely\nmany unseen test cases. The main assumptions in SSL are:\n• The Smoothness Assumption. If two points x1, x2 reside in a high-density region are close, then\nso should be their corresponding outputs y1, y2 [20].\nMeaning that if two inputs are of the same\nclass and belong to the same cluster, which is a high-density region of the input space, then their\ncorresponding outputs need to be close. The inverse also holds true; if the two points are separated\nby a low-density region, the outputs must be distant from each other. This assumption can be quite\nhelpful in a classiﬁcation task, but not so much for regression.\n• The Cluster Assumption. If points are in the same cluster, they are likely to be of the same class\n[20]. In this particular case of the smoothness assumption, we suppose that input data points form\nclusters, and each cluster corresponds to one of the output classes. The cluster assumption can also\nbe seen as the low-density separation assumption: The decision boundary should lie in the low-density\nregions. The relation between the two assumptions is easy to see, if a given decision boundary lies in\na high-density region, it will likely cut a cluster into two diﬀerent classes, resulting in samples from\ndiﬀerent classes belonging to the same cluster, which is a violation of the cluster assumption. In this\n3\ncase, we can restrict our model to have consistent predictions on the unlabeled data over some small\nperturbations pushing its decision boundary to low-density regions.\n• The Manifold Assumption. The (high-dimensional) data lie (roughly) on a low-dimensional man-\nifold [20].\nIn high dimensional spaces, where the volume grows exponentially with the number of\ndimensions, it can be quite hard to estimate the true data distribution for generative tasks. For dis-\ncriminative tasks, the distances are similar regardless of the class type, making classiﬁcation quite\nchallenging. However, if our input data lies on some lower-dimensional manifold, we can try to ﬁnd\na low dimensional representation using the unlabeled data and then use the labeled data to solve the\nsimpliﬁed task.\n1.4\nRelated Problems\nActive Learning\nIn active learning [140, 63], the learning algorithm is provided with a large pool of\nunlabeled data points, with the ability to request the labeling of any given examples from the unlabeled\nset in an interactive manner. As opposed to classical passive learning, in which the examples to be labeled\nare chosen randomly from the unlabeled pool, active learning aims to carefully choose the examples to be\nlabeled to achieve a higher accuracy while using as few requests as possible, thereby minimizing the cost of\nobtaining labeled data. This is of particular interest in problems where data may be abundant, but labels\nare scarce or expensive to obtain.\nAlthough it is not possible to obtain a universally good active learning strategy [33], there exist many\nheuristics [140], which have been proven to be eﬀective in practice. The two widely used selection criteria are\ninformativeness and representativeness [72, 188]. Informativeness measures how well an unlabeled instance\nhelps reduce the uncertainty of a statistical model, while representativeness measures how well an instance\nhelps represent the structure of input patterns.\nActive learning and SSL are naturally related, since both aim to use a limited amount of data to improve\na learner. Several works considered combining SSL and AL in diﬀerent tasks. [41] demonstrates a signiﬁcant\nerror reduction with limited labeled data for speech understanding, [129] proposes an active semi-supervised\nlearning system for pedestrian detection, [192] combines AL and SSL using Gaussian ﬁelds applied to syn-\nthetic datasets, and [51] exploits both labeled and unlabeled data using SSL to distill information from\nunlabeled data that improves representation learning and sample selection.\nTransfer Learning and Domain Adaptation\nTransfer learning [116, 162] is used to improve a learner\non one domain, called the target domain, by transferring the knowledge learned from a related domain,\nreferred to as the source domain. For instance, we may wish to train the model on a synthetic, cheap-to-\ngenerate data, with the goal of using it on real data. In this case, the source domain used to train the model\nis related but diﬀerent from the target domain used to test the model. When the source and target diﬀer\nbut are related, then transfer learning can be applied to obtain higher accuracy on the target data.\nOne popular type of transfer learning is domain adaptation [122, 118, 166]. Domain adaptation is a type\nof transductive transfer learning, where the target task remains the same as the source, but the domain\ndiﬀers. The objective of domain adaptation is to train a learner capable of generalizing across diﬀerent\ndomains of diﬀerent distributions in which the labeled data are available for the source domain. As for the\ntarget domain, we refer to the case where no labeled data is available on target as unsupervised domain\nadaptation, while semi-supervised and supervised domain adaptation refers to situations where we have a\nlimited or a fully labeled target domain receptively [10].\nSSL and unsupervised domain adaptation are closely related; in both cases, we are provided with la-\nbeled and unlabeled data, with the objective of learning a function capable of generalizing to the unlabeled\n4\ndata and unseen examples.\nHowever, in SSL, both the labeled and unlabeled sets come from the same\ndistribution, while in unsupervised domain adaptation, the target and source distributions diﬀer. Methods\nin both subjects can be leveraged interchangeably. In SSL, [104] proposed to use adversarial distribution\nalignment [50] for semi-supervised image classiﬁcation using only a small amount of labeled samples. As for\nunsupervised domain adaptation, semi-supervised methods, such as consistency regularization [142, 95, 47],\nco-regularization [91] or proxy labeling [134, 133] demonstrated their eﬀectiveness in domain adaptation.\nWeakly-Supervised Learning\nTo overcome the need for large hand-labeled and expensive training sets,\nmost sizeable deep learning systems use some form of weak supervision: lower-quality, but larger-scale\ntraining sets constructed via strategies such as using cheap annotators [126]. In weakly-supervised learning,\nthe objective is the same as in supervised learning, however, instead of a ground-truth labeled training\nset, we are provided with one or more weakly annotated examples, that could come from crowd workers,\nbe the output of heuristic rules, the result of distant supervision [106], or the output of other classiﬁers.\nFor example, in weakly-supervised semantic segmentation, pixel-level labels, which are harder and more\nexpensive to acquire, are substituted for inexact annotations, e.g., image labels [159, 184, 161, 97, 94], points\n[9], scribbles [100] and bounding boxes [144, 31]. In such a scenario, SSL approaches can be used to enhance\nthe performance further if a limited number of strongly labeled examples are available while still taking\nadvantage of the weakly labeled examples.\nLearning with Noisy Labels\nLearning from noisy labels [46, 52] can be challenging given the negative\nimpact label noise can have on the performance of deep learning methods if the noise is signiﬁcant. To\novercome this, most existing methods for training deep neural networks with noisy labels seek to correct the\nloss function. One type of correction consists of treating all the examples as equal and relabeling the noisy\nexamples, where proxy labels methods can be used for the relabeling procedure [174, 101, 127]. Another\ntype of correction applies a reweighing to the training examples to distinguish between the clean and noisy\nsamples [28, 149]. Other works [35, 69, 87, 96] have shown that SSL can be useful in learning from noisy\nlabels, where the noisy labels are discarded, and the noisy examples are considered as unlabeled data and\nused to regularize training using SSL methods.\n1.5\nEvaluating SSL Approaches\nThe conventional experimental procedure used to evaluate SSL methods consists of choosing a dataset (e.g.,\nCIFAR-10 [88], SVHN [110], ImageNet [34], IMDb [103], Yelp review [180]) commonly used for supervised\nlearning, a large portion of the labels are then ignored, resulting in a small labeled set Dl and a larger\nunlabeled Du. A deep learning model is trained with a given SSL approach, and the results are reported\non the original test set over various and standardized portions of labeled examples. In order to make this\nprocedure applicable to real-world settings, Oliver et al. [113] proposed the following ways to improve this\nexperimental methodology:\n• A Shared Implementation. For a realistic comparison of diﬀerent SSL methods, they must share\nthe same underlying architectures and other implementation details (e.g., hyperparameters, parameter\ninitialization, data augmentation, regularization, etc.).\n• High-Quality Supervised Baseline. The main objective of SSL is to obtain better performance\nthan what can be obtained in a supervised manner. This is why it is essential to provide a strong\nbaseline consisting of training the same model on the labeled set Dl in a supervised way, with modiﬁed\nhyperparameters to report the best-case performance of the fully-supervised model.\n5\n• Comparison to Transfer Learning. Another robust baseline to compare SSL methods to can be\nobtained by training the model on large labeled datasets, and then ﬁne-tune it on the small labeled\nset Dl.\n• Considering Class Distribution Mismatch.\nThe possible distribution mismatch between the\nlabeled and unlabeled examples can be ignored when doing evaluation since both sets come from the\nsame dataset. Still, such a mismatch is prevalent in real-world applications, where the unlabeled data\ncan have diﬀerent class distributions compared to the labeled data. The eﬀect of this discrepancy needs\nto be addressed for better real-world adoption of SSL.\n• Varying the Amount of Labeled and Unlabeled Data. A common practice in SSL is varying\nthe number of labeled examples, but also varying the size Du in a systematic way to simulate realistic\nscenarios, such as training on a relatively small unlabeled set, can provide additional insights into the\neﬀectiveness of SSL approaches.\n• Realistically Small Validation Sets. In many cases where a fully annotated dataset if used for\nevaluation, we might end-up with a validation set that is signiﬁcantly larger than the labeled set Dl\nused for training, in such a setting, extensive hyperparameter tuning might result in an overﬁtting to\nthe validation set. In contrast, small validation sets constrain the ability to select models [20, 45],\nresulting in a more realistic assessment of the performance of SSL methods.\n2\nConsistency Regularization\nA recent line of works in deep semi-supervised learning utilizes the unlabeled data to enforce the trained\nmodel to be in line with the cluster assumption, i.e., , the learned decision boundary must lie in low-density\nregions. These methods are based on a simple concept that, if a realistic perturbation was to be applied to an\nunlabeled example, the prediction should not change signiﬁcantly, given that under the cluster assumption,\ndata points with distinct labels are separated with low-density regions, so the likelihood of one example to\nswitch classes after a perturbation is small (e.g., ﬁg. 1).\nMore formally, with consistency regularization, we are favoring functions fθ that give consistent predic-\ntions for similar data points. So rather than minimizing the classiﬁcation cost at the zero-dimensional data\npoints of the inputs space, the regularized model minimizes the cost on a manifold around each data point,\npushing the decision boundaries away from the unlabeled data points and smoothing the manifold on which\nthe data resides [193]. Concretely, given an unlabeled data point x ∈Du and its perturbed version ˆxu, the\nobjective is to minimize the distance between the two outputs d(fθ(x), fθ(ˆx)). The popular distance mea-\nsures d are mean squared error (MSE), Kullback-Leiber divergence (KL) and Jensen-Shannon divergence\n(JS). For two outputs fθ(x) and fθ(ˆx) in the form of a probability distribution over the C classes, and\nm = 1\n2(fθ(x) + fθ(ˆx)), we can compute these measures as follows:\ndMSE(fθ(x), fθ(ˆx)) = 1\nC\nC\nX\nk=1\n(fθ(x)k −fθ(ˆx)k)2\n(2.1)\ndKL(fθ(x), fθ(ˆx)) = 1\nC\nC\nX\nk=1\nfθ(x)k log fθ(x)k\nfθ(ˆx)k\n(2.2)\ndJS(fθ(x), fθ(ˆx)) = 1\n2dKL(fθ(x), m) + 1\n2dKL(fθ(ˆx), m)\n(2.3)\nNote that we can also enforce a consistency over two perturbed versions of x, ˆx1 and ˆx2.\n6\nDenoising Decoder\nEncoder\nNoisy Encoder\nGaussian Noise\nFigure 2: Ladder Networks. An illustration of one forward pass of Ladder Networks. The objective is to\nreconstruct the clean activations of the encoder using a denoising decoder that takes as input the corrupted\nactivations of the noisy encoder.\n2.1\nLadder Networks\nTo take any well-performing feed-forward network on supervised data and augment it with additional\nbranches to be able to utilize additional unlabeled data. Rasmus et al. [125] propose to use Ladder Networks\n[153] with an additional encoder and decoder for SSL. As illustrated in ﬁg. 2, the network consists of two\nencoders, a corrupted and clean one, and a decoder. At each training iteration, the input x is passed through\nboth encoders. In the corrupted encoder, Gaussian noise is injected at each layer after batch normalization,\nproducing two outputs, a clean prediction y and a prediction based on corrupted activations ˜y. The output\n˜y is then fed into the decoder to reconstruct the uncorrupted input and the clean hidden activations. The\nunsupervised training loss Lu is then computed as the MSE between the activations of the clean encoder\nz and the reconstructed activations ˆz (i.e., after batch normalization), computed over all layers, from the\ninput to the last layer L, with a weighting λl for each layer’s contribution to the total loss:\nLu =\n1\n|D|\nX\nx∈D\nL\nX\nl=0\nλldMSE(z(l), ˆz(l))\n(2.4)\nIf the input is a labeled data point, x ∈Dl, with a label y, a supervised cross-entropy loss H(˜y, t) term\ncan be added to Lu to obtain the ﬁnal loss.\nL = Lu + Ls = Lu +\n1\n|Dl|\nX\nx,t∈Dl\nH(˜y, t)\n(2.5)\nThe method can be easily adapted for convolutional neural networks (CNNs) by replacing the fully-\nconnected layers with convolutional layers for semi-supervised vision tasks. However, the ladder network is\nquite computationally heavy, approximately tripling the computation needed for one training iteration. To\nmitigate this, the authors propose a variant of ladder networks called Γ-Model where λl = 0 when l < L.\nIn this case, the decoder is omitted, and the unsupervised loss is computed as the MSE between the two\noutputs y and ˜y.\n7\nNetwork\nwith dropout\nAugmentations\nCross\nEntropy\nLoss\nFigure 3: Loss computation for Π-Model. The MSE between the two outputs is computed for the unsu-\npervised loss, and if the input is a labeled example, we add the supervised loss to the weighted unsupervised\nloss.\n2.2\nPi-Model\nThe Π-Model [92] is a simpliﬁcation of the Γ-Model of Ladder Networks, where the corrupted encoder is\nremoved, and the same network is used to get the prediction for both corrupted and uncorrupted inputs.\nSpeciﬁcally, Π-Model takes advantage of the stochastic nature of the prediction function fθ in neural networks\ndue to common regularization techniques, such as data augmentation and dropout, that typically don’t alter\nthe model predictions. For any given input x, the objective is to reduce the distances between two predictions\nof fθ with x as input in both forward passes. Concretely, as illustrated in ﬁg. 3, we would like to minimize\nd(y, ˜y), where we consider one of the two outputs as a target. Given the stochastic nature of the predictions\nfunction (e.g., using dropout as a noise source), the two outputs fθ(x) = ˜y1 and fθ(x) = ˜y2 will be distinct,\nand the objective is to obtain consistent predictions for both of them. In case the input x is a labeled data\npoint, we also compute the cross-entropy supervised loss using the provided labels y:\nL = w\n1\n|Du|\nX\nx∈Du\ndMSE(˜y1, ˜y2) +\n1\n|Dl|\nX\nx,y∈Dl\nH(y, f(x))\n(2.6)\nwith w as a weighting function, starting from 0 up to a ﬁxed weight λ (e.g., 30) after a given number of\nepochs (e.g., 20% of training time). This way, we avoid using the untrained and random prediction function,\nproviding us with unstable predictions at the start of training.\n2.3\nTemporal Ensembling\nΠ-Model can be divided into two stages, we ﬁrst classify all of the training data without updating the weights\nof the model, obtaining the predictions y, and in the second stage, we consider the predictions y as targets for\nthe unsupervised loss and enforce consistency of predictions by minimizing the distance between the current\noutputs ˜y and the outputs of the ﬁrst stage y under diﬀerent dropouts and augmentations.\nThe problem with this approach is that the targets y are based on a single evaluation of the network\nand can rapidly change. This instability in the targets can lead to an instability during training and reduces\nthe amount of training signal that can be extracted from the unlabeled examples. To solve this, Laine et\nal. [92] propose a second version of Π-Model called Temporal Ensembling, where the targets yema are\nthe aggregation of all the previous predictions. This way, during training, we only need a single forward\npass to get the current predictions ˜y and the aggregated targets yema, speeding up the training time by\napproximately 2×. The training process is illustrated in ﬁg. 4.\nFor a target ˜y, at each training iteration, the current output ˜y is accumulated into the ensemble output\nyema by an exponentially moving average update:\nyema = αyema + (1 −α)˜y\n(2.7)\nwhere α is a momentum term that controls how far the ensemble reaches into training history. ˜y can also\n8\nNetwork\nwith dropout\nAugmentations\nCross\nEntropy\nLoss\nEMA\nFigure 4: Loss computation for Temporal Ensembling. The MSE between the current prediction and\nthe aggregated target is computed for the unsupervised loss, and if the input is a labeled example, we add\nthe supervised loss to the weighted unsupervised loss.\nbe seen as the output of an ensemble network f from previous training epochs, with the recent ones having\ngreater weight than the distant ones.\nAt the start of training, temporal ensembling reduces to Π-Model since the aggregated targets are very\nnoisy, to overcome this, similar to the bias correction used in Adam optimizer [82], the targets ˜y are corrected\nfor the startup bias at a training step t as follows:\nyema = (αyema + (1 −α)˜y)/(1 −αt)\n(2.8)\nThe loss computation in temporal ensembling remains the same as in Π-Model, but with two essential\nbeneﬁts. First, the training is faster since we only need a single forward pass through the network to obtain\n˜y, while maintaining an exponential moving average (EMA) of label predictions on each training example\nand penalizing predictions that are inconsistent with these targets. Second, the targets are more stable\nduring training, yielding better results. The downside of such a method is a large amount of memory needed\nto keep an aggregate of the predictions for all of the training examples, which can become quite memory\nintensive for large datasets and dense tasks (e.g., semantic segmentation).\n2.4\nMean teachers\nΠ-Model and its improved version with Temporal Ensembling provides a better and more stable teacher\nmodel by maintaining an EMA of the predictions of each example, formed by an ensemble of the model’s\ncurrent version and those earlier versions evaluated at the same example. This ensembling improves the\nquality of the predictions and using them as the teacher predictions improve results. However, the newly\nlearned information is incorporated into the training at a slow pace, since each target is updated only once\nper epoch, and the larger the dataset, the bigger the span between the updates gets.\nAdditionally, in the previous approaches, the same model plays a dual role, as a teacher and a student.\nGiven a set of unlabeled data, as a teacher, the model generates the targets, which are then used by itself as\na student for learning using a consistency loss. These targets may very well be misclassiﬁed. If the weight\nof the unsupervised loss outweighs that of the supervised loss, the model is prevented from learning new\ninformation, predicting the same targets, and resulting in a form of conﬁrmation bias. To solve this, the\nquality of the targets must be improved. The quality of targets can be improved by either: (1) carefully\nchoosing the perturbations instead of merely injecting additive or multiplicative noise, or, (2) carefully\nchoosing the teacher model responsible for generating the targets, instead of using a replica of the student\nmodel.\nTo overcome these limitations, Mean Teacher [148] proposes using a teacher model for a faster incorpo-\nration of the learned signal, and to avoid the problem of conﬁrmation bias. A training iteration of Mean\n9\nStudent\nCross\nEntropy\nLoss\nTeacher\nEMA\nNoise\nNoise\nFigure 5: Mean Teacher. The teacher model, which is an EMA of the student model, is responsible for\ngenerating the targets for consistency training. The student model is then trained to minimize the supervised\nloss over labeled examples and the consistency loss over unlabeled examples. At each training iteration, both\nmodels are evaluated with an injected noise (η, η′), and the weights of the teacher model are updated using\nthe current student model to incorporate the learned information at a faster pace.\nTeacher (ﬁg. 5) is very similar to previous methods; the main diﬀerence is that Π-Model uses the same model\nas a student and a teacher θ′ = θ, and Temporal Ensembling approximate a stable teacher fθ′ as an ensemble\nfunction with a weighted average of successive predictions. While Mean Teacher deﬁnes the weights θ′\nt of\nthe teacher model fθ′ at a training step t as an EMA of successive student’s weights θ:\nθ′\nt = αθ′\nt−1 + (1 −α)θt\n(2.9)\nThe loss computation in this case is the sum of the supervised and unsupervised loss, where the teacher\nmodel is used to obtain the targets for the unsupervised loss for a given input x:\nL = w\n1\n|Du|\nX\nx∈Du\ndMSE(fθ(x), fθ′(x)) +\n1\n|Dl|\nX\nx,y∈Dl\nH(y, fθ(x))\n(2.10)\n2.5\nDual Students\nOne of the main drawbacks of using a Mean Teacher is that given a large number of training iterations, the\nteacher model weights will converge to that of the student model, and any biased and unstable predictions\nwill be carried over to the student.\nTo solve this, Ke et al. [80] propose a dual students step-up. Two student models with diﬀerent initial-\nization are simultaneously trained, and at a given iteration, one of them provides the targets for the other.\nTo choose which one, we test for the most stable predictions that satisfy the following stability conditions:\n• The predictions using two input versions, a clean x and a perturbed version ˜x give the same results:\nf(x) = f(˜x).\n• Both predictions are conﬁdent, i.e., are far from the decision boundary. This can be tested by seeing\nif f(x) (resp. f(˜x)) is greater than a conﬁdence threshold ϵ, e.g., ϵ = 0.1.\nGiven two student models, fθ1 and fθ2, an unlabeled input x ∈Du and its perturbed version ˜x. We\ncompute four predictions: fθ1(x), fθ1(˜x), fθ2(x), and fθ2(˜x) . In addition to training each model to minimize\n10\nboth the supervised and unsupervised losses:\nL = Ls + λ1Lu =\n1\n|Dl|\nX\nx,y∈Dl\nH(y, fθi(x)) + λ1\n1\n|Du|\nX\nx∈Du\ndMSE(fθi(x), fθi(˜x))\n(2.11)\nwe also force one of the students to have similar predictions to its counterpart. To chose which one to update\nits weights, we check for both models’ stability constraint; if the predictions one of the models is unstable, we\nupdate its weights. If both are stable, we update the model with the largest variation Ei = ∥fi(x) −fi(˜x)∥2,\ni.e., the least stable. In this case, the least stable model is trained with an additional loss:\nλ2\nX\nx∈Du\ndMSE(fθi(x), fθj(x))\n(2.12)\nwhere λ1 and λ2 are hyperparameters specifying the contribution of each loss term.\n2.6\nFast-SWA\nAthiwaratkun et al. [5] observed that Π-Model and Mean Teacher continue taking signiﬁcant steps in the\nweight space at the end of training, given that the models stochastic gradient descent (SGD) traverses a large\nﬂat region of the weight space late in training, continuing to actively explore the set of plausible solutions\nand producing diverse predictions on the test set even in the late stages of training. Based on this behavior,\naveraging the SGD iterates can lead to ﬁnal weights closer to the center of the ﬂat region, stabilizing the\nSGD trajectory, and leading to signiﬁcant gains in performance and better generalization.\nOne way to produce an ensemble of the model late in training is Stochastic Weight Averaging (SWA)\n[75], an approach based on averaging the weights traversed by SGD at the end of training with a cyclic\nlearning rate (ﬁg. 6). After a given number of epochs, the learning rate changes to a cyclic learning rate and\nthe training repeats for several cycles, the weights at the end of each cycle corresponding to the minimum\nvalues of the learning rate are stored, and averaged together to obtain a model with the averaged weights\nfθSWA, which is then used to make predictions.\n`0\n`\nEpoch\nLearning Rate\n0\n`0\n`\nEpoch\nLearning Rate\n0\n1Cycle\n1Cycle\nSWA\nfast-SWA\nfast-SWA\nSWA\nFigure 6: SWA and fast-SWA. Left and Center. Cyclical cosine learning rate schedule used at the end of\ntraining for SWA and fast-SWA with diﬀerent averaging strategies. Right. 2d illustration of the impact of\nSWA and fast-SWA averaging strategies on the ﬁnal weights. Based on [5].\nMotivated by the observation that the beneﬁts of averaging are the most prominent when the distance\nbetween the averaged points is large, and given that SWA only collects the weights once per cycle, which\nmeans that many additional training epochs are needed in order to collect enough weights for averaging.\nThe authors propose fast-SWA, a modiﬁcation of SWA that averages the networks corresponding to many\npoints during the same cycle, resulting in a better ﬁnal model and a faster ensembling procedure.\n11\n2.7\nVirtual Adversarial Training\nThe previous approaches focused on applying random perturbations to each input to generate artiﬁcial input\npoints, encouraging the model to assign similar outputs to the unlabeled data points and their perturbed\nversions. This way, we push for a smoother output distribution. As a result, the generalization performance\nof the model can be improved. However, such random noise and random data augmentation often leaves\nthe predictor particularly vulnerable to small perturbations in a speciﬁc direction, that is, the adversarial\ndirection, which is the direction in the input space in which the label probability p(y|x) of the model is most\nsensitive.\nTo overcome this, and inspired by adversarial training [56] that trains the model to assign to each input\ndata a label that is similar to the labels of its neighbors in the adversarial direction. Miyato et al. [108]\npropose Virtual Adversarial Training (VAT), a regularization technique that enhances the model’s robustness\naround each input data point against random and local perturbations, the term virtual comes from the fact\nthat the adversarial perturbation is approximated without any label information, and is hence applicable to\nSSL to smooth the output distribution.\nConcretely, VAT trains the output distribution to be identically smooth around each data point, by\nselectively smoothing the model in its most adversarial direction. For a given data point x, we would like\nto compute the adversarial perturbation radv that will alter the model’s predictions the most. We start by\nsampling a Gaussian noise r of the same dimensions as the input x, we then compute its gradients gradr\nwith respect the loss between the two predictions, with and without the injections of the noise r (i.e., KL-\ndivergence is used as a distance measure d(., .)). radv can then be obtained by normalizing and scaling gradr\nby a hyperparameter ϵ. The computation can be summarized in the following steps:\n1. r ∼N(0,\nξ\n√\ndim(x)I)\n2. gradr = ∇rdKL(fθ(x), fθ(x + r))\n3. radv = ϵ gradr\n∥gradr∥\nNote that the computation above is a single iteration of the approximation of radv, for a more accurate\nestimate, we consider radv = r and recompute radv following the last two steps. But in general, given how\ncomputationally expensive this computation is, requiring an additional forward and backward passes, we only\napply a single power iteration for computing the adversarial perturbation. With the optimal perturbation\nradv, we can then compute the unsupervised loss as the MSE between the two predictions of the model, with\nand without the injection of radv:\nLu = w\n1\n|Du|\nX\nx∈Du\ndMSE(fθ(x), fθ(x + radv))\n(2.13)\nFor a more stable training, a Mean Teacher can be used to generate stable targets by replacing fθ(x)\nwith fθ′(x), where fθ′ is an EMA of the student fθ.\n2.8\nAdversarial Dropout\nInstead of using an additive adversarial noise as VAT, Park et al. [117] propose adversarial dropout (AdD),\na.k.a, element-wise adversarial dropout (EAdD), in which dropout masks are adversarially optimized to alter\nthe model’s predictions. With this type of perturbations, we induce a sparse structure of the neural network,\nwhile the other forms of additive noise does not make changes to the structure of the neural network directly.\n12\nPerturbed images\nClean images\nFigure 7: Virtual Adversarial Examples. Examples of the perturbed ImagetNet images for diﬀerent\nvalues of the scaling hyperparameter ϵ.\nThe ﬁrst step is to ﬁnd the dropout conditions that are most sensitive to the model’s predictions. In a\nSSL setting, where we do not have access to the true labels, we use the model’s predictions on the unlabeled\ndata points to approximate the adversarial dropout mask ϵadv, which is subject to the boundary condition:\n∥ϵadv −ϵ∥2 ≤δH with H as the dropout layer dimension and a hyperparameter δ, which restricts adversarial\ndropout masks to be inﬁnitesimally diﬀerent from the random dropout mask ϵ. Without this constraint, the\nadversarial dropout might induce a layer without any connections. By restricting the adversarial dropout\nto be similar to the random dropout, we prevent ﬁnding such an irrational layer, which does not support\nbackpropagation.\nSimilar to VAT, we start from a random dropout mask, we compute a KL-divergence loss between the\noutputs, with and without dropout, and given the gradients of the loss with respect to the activations\nbefore the dropout layer, we update the random dropout mask in an adversarial manner. The prediction\nfunction fθ is divided into two parts, fθ1 and fθ2, where fθ(x, ϵ) = fθ2(fθ1(x) ⊙ϵ), we start by computing\nan approximation of the Jacobian matrix as follows:\nJ(x, ϵ) ≈fθ1(x) ⊙∇fθ1(x)dKL(fθ(x), fθ(x, ϵ))\n(2.14)\nUsing J(x, ϵ), we can then update the random dropout mask ϵ to obtain ϵadv, so that if ϵ(i) = 0 and\nJ(x, ϵ)(i) > 0 or ϵ(i) = 1 and J(x, ϵ)(i) < 0 at a given position i, we inverse the value of ϵ at that location.\nResulting in ϵadv, which can then be used to compute the unsupervised loss:\nLu = w\n1\n|Du|\nX\nx∈Du\ndMSE(fθ(x), fθ(x, ϵadv))\n(2.15)\nChannel-wise Adversarial Dropout\nThe element-wise adversarial dropout (EAdD) introduced by Park\net al. [117] is limited to fully-connected networks, to use AdD in a wider range of tasks, Lee et al. [95]\nproposed channel-wise AdD (CAdD), an extension the element-wise masking in AdD to convolutional layers\n(ﬁg. 8).\nIn these layers, standard dropout is relatively ineﬀective due to the strong spatial correlation\nbetween individual activations of a feature map [150]. EAdD dropout suﬀers from the same issues when\n13\nElement-wise AdD (EAdD)\nChannel-wise AdD (CAdD)\nFigure 8: EAdD and CAdD. EAdD drops activation individually regardless of the spatial correlation,\nwhile CAdD drops entire feature maps, making it more suitable for convolutional layers. Image Source: [95].\nCross\nEntropy\nLoss\nFigure 9: ICT. A student model is trained to have consistent predictions at diﬀerent interpolations of\nunlabeled data points, where a teacher is used to generate the targets before the Mixup operation.\nnaively applied to convolutional layers. To solve this, EAdD adversarially drops entire feature maps rather\nthan individual activations. While the general procedure is similar to that of EAdD, an additional constraint\nis imposed on the mask to represent spatial dropout [150]. In this case, the mask ϵ ∈RC×H×W is of the\nsame shape as the activations; the adversarial dropout mask ϵadv is approximated under the following new\ncondition:\n1\nHW\nC\nX\ni=1\n∥ϵadv(i) −ϵ(i)∥≤δC\n(2.16)\nwhere δ is a hyperparameter to restrict the diﬀerent between the two masks to be small, and ϵ(i) is the mask\ncorresponding to the i-th activation map. The process of ﬁnding the channel-wise adversarial dropout mask\nis similar to those of element-wise adversarial dropout, but with a per activation map approximation.\n2.9\nInterpolation Consistency Training\nAs discussed earlier, the random perturbations are ineﬃcient in high dimensions, given that only a limited\nsubset of the input perturbations are capable of pushing the decision boundary into low-density regions. VAT\nand AdD ﬁnd the adversarial perturbations that will maximize the change in the model’s predictions, which\ninvolve multiple forward and backward passes to compute these perturbations. This additional computation\ncan be restrictive in many cases and makes such methods less appealing. As an alternative, Verma et al.\n[156] propose Interpolation Consistency Training (ICT) as an eﬃcient consistency regularization technique\nfor SSL.\nGiven a MixUp operation [178]: Mixλ(a, b) = λ · a + (1 −λ) · b that outputs an interpolation between\nthe two inputs with a weight λ ∼Beta(α, α) for α ∈[0, ∞]. As shown in ﬁg. 9, ICT trains the prediction\nfunction fθ to provide consistent predictions at diﬀerent interpolations of unlabeled data points xi and xj,\n14\nwhere the targets are generated using a teacher model fθ′ which is an EMA of fθ:\nfθ(Mixλ(xi, xj)) ≈Mixλ(fθ′(xi), fθ′(xj))\n(2.17)\nThe unsupervised objective is to have similar values between the student model’s prediction given a\nmixed input of two unlabeled data points, and the mixed outputs of the teacher model.\nLu = w\n1\n|Du|\nX\nxi,xj∈Du\ndMSE(fθ(Mixλ(xi, xj)), Mixλ(fθ′(xi), fθ′(xj))\n(2.18)\nThe beneﬁt of ICT compared to random perturbations can be analyzed by considering the mixup oper-\nation as a perturbation applied to a given unlabeled example: xi + δ = Mixλ(xi, xj), for a large number of\nclasses and with a similar distribution of examples per class, it is likely that the pair of points (xi, xj) lie in\ndiﬀerent clusters and belong to diﬀerent classes. If one of these two data points lies in a low-density region,\napplying an interpolation toward xj points to a low-density region, which is a good direction to move the\ndecision boundary toward.\n2.10\nUnsupervised Data Augmentation\nUnsupervised Data Augmentation [169] uses advanced data augmentation methods, such as AutoAugment\n[29], RandAugment [30] and Back Translation [43, 139], as perturbations for consistency training based SSL.\nSimilar to supervised learning, advanced data augmentation methods can also provide extra advantages over\nsimple augmentations and random noise injection in consistency training, given that; (1) it generates realistic\naugmented examples, making it safe to encourage the consistency between predictions on the original and\naugmented examples, (2) it can generate a diverse set of examples improving the sample eﬃciency, and (3)\nit is capable of providing the missing inductive biases for diﬀerent tasks.\nMotivated by these points, Xie et al. [169] propose to apply the following augmentations to generate\ntransformed versions of the unlabeled inputs:\n• RandAugment for Image Classiﬁcation. Consists of uniformly sampling from the same set of\npossible transformations in Python Imaging Library (PIL), without requiring any labeled data to\nsearch for a good augmentation strategy.\n• Back-translation for Text Classiﬁcation. Consists of translating an existing example in language\nA into another language B, and then translating it back into A to obtain an augmented example.\nAfter deﬁning the augmentations to be applied during training, the training procedure (ﬁg. 10) is straight-\nforward. The objective is to have the correct predictions over the labeled set and consistent predictions on\nthe original and augmented examples from the unlabeled set.\n3\nEntropy Minimization\nIn the previous section, in a setting where the cluster assumption is maintained, we enforce consistency of\npredictions to push the decision boundary into low-density regions to avoid classifying samples from the same\ncluster with distinct classes, which is a violation of the cluster assumption. Another way to enforce this is to\nencourage the network to make conﬁdent (i.e., low-entropy) predictions on unlabeled data regardless of the\npredicted class, discouraging the decision boundary from passing near data points where it would otherwise\nbe forced to produce low-conﬁdence predictions. This is done by adding a loss term which minimizes the\n15\nCross\nEntropy\nLoss\nAugmentations\nBack\ntranslation RandAugment TF-IDF word\nreplacement\nFigure 10: UDA. The training procedure consists of computing the supervised loss for the labeled examples\nand the consistency loss between the two outputs of the augmented and clean input.\nentropy of the prediction function fθ(x). For a categorical output space with C possible classes, the entropy\nminimization term [58] is:\n−\nC\nX\nk=1\nfθ(x)k log fθ(x)k\n(3.1)\nHowever, with high capacity models such as neural networks, the model can quickly overﬁt to low conﬁdent\ndata points by simply outputting large logits, resulting in a model with very conﬁdent predictions [113]. On\nits own, entropy minimization doesn’t produce competitive results compared to other SSL methods, but can\nproduce state-of-the-art results when combined with diﬀerent approaches.\n4\nProxy-label Methods\nProxy label methods are the class of SSL algorithms that produce proxy labels on unlabeled data, using the\nprediction function itself or some variant of it without any supervision. These proxy labels are then used as\ntargets together with the labeled data, providing some additional training information even if the produced\nlabels are often noisy or weak and do not reﬂect the ground truth. These methods can be divided mainly\ninto two groups: self-training, where the model itself produces the proxy labels, and multi-view learning,\nwhere the proxy labels are produced by models trained on diﬀerent views of the data.\n4.1\nSelf-training\nIn self-training [173, 138, 131, 132], the small amount of labeled data Dl is ﬁrst used to train a prediction\nfunction fθ. The trained model is then used to assign pseudo-labels to the unlabeled data points x ∈Du.\nGiven an output fθ(x) for an unlabeled data point x in the form of a probability distribution over the classes,\nthe pair (x, argmaxfθ(x)) is added to the labeled set if the probability assigned to its most likely class is\nhigher than a predetermined threshold τ. The process of training the model using the augmented labeled\nset, and then set using it to label the remaining of Du is repeated until the model is incapable of producing\nconﬁdent predictions. Other heuristics can be used to decide which proxy labeled examples to retain, such as\nusing the relative conﬁdence instead of the absolute conﬁdence, where the top n unlabeled samples predicted\nwith the highest conﬁdence after every epoch are added to the labeled training dataset Dl. The impact\nof self-training is similar to that of entropy minimization; in both cases, the network is forced to output\nmore conﬁdent predictions. The main downside of such methods is that the model is unable to correct its\n16\nFigure 11: The MPL training procedure. At each training iteration, the teacher model is trained along with\na student model to set the student’s target distributions and adapt to the student’s learning state. Image\nSource: [120].\nown mistakes, and any biased and wrong classiﬁcations can be quickly ampliﬁed resulting in conﬁdent but\nerroneous proxy labels on the unlabeled data points.\nYalnizet et al. [171] propose to use self-training to improve ResNet-50 [67] top-1 accuracy and enhance\nthe robustness of the trained model to various perturbations (e.g., perturbations used in ImageNet-A, C and\nP [68]). The model is ﬁrst trained on unlabeled images and their proxy labels, and then ﬁne-tuned on labeled\nimages in the ﬁnal stage. Instead of using the same model for both proxy labels generation and training,\nXie et al. [170] propose to use the student-teacher setting. In an iterative manner, the teacher model is ﬁrst\ntrained on the labeled examples and used to generate soft proxy labels on the unlabeled data. The student\ncan then be trained on both the labeled set and the proxy labels while aggressively injecting noise to obtain\na more robust model. In the next iteration, the student is considered as a teacher, and a bigger version of\nEﬃcientNet [146] is used for the student, and the same procedure is repeated up to the largest model.\nIn addition to image classiﬁcation, self-training was also successfully applied to a variety of tasks, such as\nsemantic segmentation [7], text classiﬁcation [98, 79], machine translation [139, 64, 25, 65] and when learning\nfrom noisy data [154].\nPseudo-labeling\n[93, 2, 74, 141], similar to self-training, the objective of pseudo-labeling is to generate\nproxy labels to enhance the learning process. A ﬁrst attempt at adapting pseudo-labeling [93] for deep\nlearning constrained the usage of the proxy labels to a ﬁne-tuning stage after pretraining the network. Shi et\nal. [141] propose to adapt Transductive SSL [77, 78, 181, 160] by treating the labels of unlabeled examples\nas variables and trying to determine their optimal labels together with the optimal model parameters, by\nminimizing the proposed loss function through the iterative training process. The generated proxy labels are\nconsidered as hard labels for the unlabeled examples, an uncertainty weight is then introduced, with large\nweights for examples with distant k-nearest neighbors in the feature space, in additiont to two loss terms\nencouraging intra-class compactness and inter-class separation, and a consistency term between samples with\ndiﬀerent perturbations. Iscen et al. [74] integrated label-propagation [190, 164, 54] within pseudo-labeling.\nThe method alternates between training the network on the labeled examples and pseudo-labels and then\nleveraging the learned representations to build a nearest neighbor graph where label propagation is applied\nto reﬁne the hard pseudo-labels. They also introduce two uncertainty scores, one for every sample based on\nthe entropy of the output probabilities to overcome the unequal conﬁdence in the predictions, and a per-class\nscored based class population to deal with class-imbalance. Arazo et al. [2] showed that a naive pseudo-\nlabeling overﬁts to incorrect pseudo-labels due to the so-called conﬁrmation bias, and demonstrate that\nMixUp [178] and setting a minimum number of labeled samples per mini-batch are eﬀective regularization\ntechniques for reducing this bias.\n17\nMeta Pseudo Labels\nGiven how important the heuristics used to select which the proxy labels to add\nto the training set, where a proper method could lead to a sizable gain. Pham et al. [120] propose to use\nthe student-teacher setting, where the teacher model is responsible for producing the proxy labels based\non an eﬃcient meta-learning algorithm called Meta Pseudo Labels (MPL), which encourages the teacher to\nadjust the target distributions of training examples in a manner that improves the learning of the student\nmodel. The teacher is updated by policy gradients computed by evaluating the student model on a held-out\nvalidation set.\nA given training step of MPL consists of two phases (ﬁg. 11):\n• Phase 1: The Student learns from the Teacher. In this phase, given a single input example x ∈Dl,\nthe teacher fθ′ produces a target class-distribution to train the student fθ, where the pair (x, fθ′(x))\nis shown to the student to update its parameters by back-propagating from the cross-entropy loss.\n• Phase 2: The Teacher learns from the Student’s Validation Loss.\nAfter the student updates its\nparameters in ﬁrst step, its new parameter θ(t + 1) are evaluated on an example (xval, yval) from the\nheld-out validation dataset using the cross-entropy loss. Since the validation loss depends on θ′ via\nthe ﬁrst step, this validation cross-entropy loss is also a function of the teacher’s weights θ′. This\ndependency allows us to compute the gradients of the validation loss with respect to the teacher’s\nweights, and then update θ′ to minimize the validation loss using policy gradients.\nWhile the student’s performance allows the teacher to adjust and adapt to the student’s learning state,\nthis signal alone is not suﬃcient to train the teacher since when the teacher has observed enough evidence\nto produce meaningful target distributions to teach the student, the student might have already entered a\nbad region of parameters. To overcome this, the teacher is also trained using the pair of labeled data points\nfrom the held-out validation set.\n4.2\nMulti-view training\nMulti-view training (MVL) [89, 182] utilizes multi-view data that are very common in real-world applications,\nwhere diﬀerent views can be collected by diﬀerent measuring methods (e.g., color information and texture\ninformation for images) or by creating limited views of the original data. In such a setting, MVL aims to\nlearn a distinct prediction function fθi to model a given view vi(x) of a data point x, and jointly optimize all\nthe functions to improve the generalization performance. Ideally, the possible views complement each other\nso that the produced models can collaborate in improving each other’s performance.\n4.2.1\nCo-training\nCo-training [16] requires that each data point x can be represented using two conditionally independent\nviews v1(x) and v2(x), and that each view is suﬃcient to train a good model. After training two prediction\nfunctions fθ1 and fθ2 on a speciﬁc view on the labeled set Dl. We start the proxy labeling procedure. At\neach iteration, an unlabeled data point is added to the training set of the model fθi if the other model fθj\noutputs a conﬁdent prediction with a probability higher than a threshold τ. This way, one of the models\nprovides newly labeled examples where the other model is uncertain. Co-training has been combined with\ndeep learning for some applications, such as object recognition [24] by utilizing RGB-D data, with RGB and\ndepth as the two views used to train the two models, or for combining multi-modal data [3] (i.e., image\nand text) by training each model on a given modality and use it to provide pseudo-labels for other models.\nHowever, in many cases, the data have only one view rather than two, in this instance, diﬀerent learning\nalgorithms or diﬀerent parameter conﬁgurations to learn two diﬀerent classiﬁers can be employed. The two\nviews v1(x) and v2(x) can also be generated by injecting noise or by applying diﬀerent augmentations, for\n18\nexample, Qiao et al. [121] used adversarial perturbations to produce new views for deep co-training for image\nclassiﬁcation, where the models are encouraged to have the same predictions on Dl but make diﬀerent errors\nwhen they are exposed to adversarial attacks.\nDemocratic Co-training\n[187]. An extension of Co-training, consists of replacing the diﬀerent views of\nthe input data with a number of models with diﬀerent architectures and learning algorithms, which are ﬁrst\ntrained on the labeled examples. The trained models are then used to label a given example x if a majority\nof models conﬁdently agree on its label.\n4.2.2\nTri-Training\nTri-training [189] tries to overcome the lack of data with multiple views and reduce the bias of the predictions\non unlabeled data produced with self-training by utilizing the agreement of three independently trained\nmodels instead of a single model. First, the labeled data Dl is used to train three prediction functions: fθ1,\nfθ2 and fθ3. An unlabeled data point x ∈Du is then added to the supervised training set of the function fθi\nif the other two models agree on its predicted label. The training stops if no data points are being added\nto any of the models’ training sets. Tri-training requires neither the existence of multiple views nor unique\nlearning algorithms, making it more generally applicable. Using Tri-training with neural networks can be\nvery expensive, requiring predictions for each one of the three models on all the unlabeled data. Ruder et\nal. [133] propose to sample a limited number of unlabeled data points at each training epoch, the candidate\npool size is increased as the training progresses and the models become more accurate.\nMulti-task tri-training\n[133] can also be used to reduce the time and sample complexity, where all three\nmodels share the same feature-extractor with model-speciﬁc classiﬁcation layers. This way, the models are\ntrained jointly with an additional orthogonality constraint on two of the three classiﬁcation layers to be\nadded to loss term, to avoid learning similar models and falling back to the standard case of self-training.\nTri-Net [39] also falls in this category, with a shared module for joint learning and three output modules for\ntri-training, in addition to utilizing output smearing [17] to initialize these modules. After the proxy labeling\niteration, a ﬁne-tuning stage is conducted on the labeled data to augment diversity and eliminate unstable\nand suspicious pseudo-labeled data.\nCross-View Training\nIn self-training, the model plays a dual role of a teacher and a student, producing\nthe predictions it is being trained on, resulting in very moderate performance gains. As a solution, and\ntaking inspiration from multi-view learning and consistency training, Clark et al. [27] propose Cross-View\nTraining, where the model is trained to produce consistent predictions across diﬀerent views of the inputs.\nInstead of using a single model as a teacher and a student, they propose to use a shared encoder, and then\nadd auxiliary prediction modules that transform the encoder representations into predictions, these modules\nare then divided into auxiliary student modules and a primary teacher module. The input to each student\nprediction module is a subset of the model’s intermediate representations corresponding to a restricted view\nof the input, such as feeding one of the student only the forward LSTM from a given Bi-LSTM layer, so it\nmakes predictions without seeing any tokens to the right of the current one (ﬁg. 12). The primary teacher\nmodule in trained only on labeled examples, and is responsible of generating the pseudo-labels taking as\ninput the full view of the unlabeled inputs, the students are trained to have consistent predictions with the\nteacher module. Given an encoder e, a teacher module t and K student modules si with i ∈[0, K], where\neach student receives a limited view of the input, the training objective is written as follows:\nL = Lu + Ls =\n1\n|Du|\nX\nx∈Du\nK\nX\ni=1\ndMSE(t(e(x)), si(e(x))) +\n1\n|Dl|\nX\nx,y∈Dl\nH(t(e(x)), y)\n(4.1)\n19\nx1\nx2\nx3\nEmbed\nBackward LSTM\nForward LSTM\npθ\nPredict\npfwd\nθ\npfuture\nθ\npbwd\nθ\nppast\nθ\nAuxiliary\nPrediction\nModules\nPrimary\nPrediction\nModule\nLoss\nFigure 12: Cross-view Training. An example of auxiliary student prediction modules. Each student sees\na restricted view of the input. For instance, the forward prediction module does not see any context to the\nright of the current token when predicting that tokens label. Image Source: [27]\nFigure 13: MixMatch. The procedure of label guessing process used in MixMatch, taking as input a batch\nof unlabeled examples, and outputting a batch of K augmented version of each input, with a corresponding\nsharpened proxy labels. Image Source: [120].\nCross-view training takes advantage of unlabeled data by improving the encoder’s representation learning.\nThe student prediction modules can learn from the teacher module predictions because this primary module\nhas a better, unrestricted view of the inputs. As the student modules learn to make accurate predictions\ndespite their restricted views of the input, they improve the quality of the representations produced by the\nencoder. Which, in turn, improves the full model, which uses the same shared representations.\n5\nHolistic Methods\nAn emerging line of work in SSL is a set of holistic approaches that try to unify the current dominant\nmethods in SSL in a single framework, achieving better performances.\n5.1\nMixMatch\nBerthelot et al. [13] propose a holistic approach which gracefully uniﬁes ideas and components from the\ndominant paradigms for SSL, resulting in an algorithm that is greater than the sum of its parts and surpasses\nthe performance of the traditional approaches.\nMixMatch takes as input a batch from the labeled set Dl containing pairs of inputs and their corre-\nsponding one-hot targets, a batch from the unlabeled set Du containing only unlabeled data, and a set of\nhyperparameters: the sharpening softmax temperature T, the number of augmentations K, and the Beta\n20\ndistribution parameter α for MixUp. Producing a batch of augmented labeled examples and a batch of\naugmented unlabeled examples with their proxy labels. These augmented examples can then be used to\ncompute the losses and train the model. Precisely, MixMatch consists of the following steps:\n• Step 1: Data Augmentation. Using a given transformation, a labeled example x ∈Dl from the\nlabeled batch is transformed, producing its augmented versions ˜x. For an unlabeled example x ∈Du,\nthe augmentation function is applied K times, resulting in K augmented versions of the unlabeled\nexamples ˜x1, ..., ˜xK.\n• Step 2: Label Guessing. The second step consists of producing proxy labels for the unlabeled\nexamples. First, we generate the predictions for the K augmented versions of each unlabeled example\nusing the predictions function fθ. The K predictions are then averaged together, obtaining a proxy\nor a pseudo label ˆy = 1/K PK\nk=1(ˆyk) for each one of the augmentations of the unlabeled example x:\n(˜x1, ˆy), ..., (˜xK, ˆy).\n• Step 3: Sharpening. To push the model to produce conﬁdent predictions and minimize the entropy\nof the output distribution, the generated proxy labels ˆy in step 2 in the form of a probability distribution\nover C classes are sharpened by adjusting the temperature of the categorical distribution, computed\nas follows where (ˆyu)k refers to the probability of class k out of C classes:\n(ˆy)k = (ˆy)\n1\nT\nk /\nC\nX\nk=1\n(ˆy)\n1\nT\nk\n(5.1)\n• Step 4 MixUp. The previous steps resulted in two new augmented batches, a batch L of augmented\nlabeled examples and their target, and a batch U of augmented unlabeled examples and their sharpened\nproxy labels. Note that the size of U is K times larger than the original batch given that each example\nx ∈Du is replaced by its K augmented versions. In the last step, we mix these two batches. First,\na new batch merging both batches is created W = Shuﬄe(Concat(L, U)). W is then divided into two\nbatches: W1 of the same size as L and W2 of the same size as L. Using the Mixup operation that is\nslightly adjusted so that the mixed example is closer the labeled examples, the ﬁnal step is to create\nnew labeled and unlabeled batches by mixing the produced batches together using Mixup as follows:\nL′ = MixUp(L, W1)\n(5.2)\nU′ = MixUp(U, W2)\n(5.3)\nAfter creating two augmented batches L′ and U′ using MixMatch, we can then train the model using\nthe standard SSL losses by computing the CE loss for the supervised loss, and the consistency loss for the\nunsupervised loss using the augmented batches as follows:\nL = Ls + wLu =\n1\n|L′|\nX\nx,y∈L′\nH(y, fθ(x))) + w 1\n|U′|\nX\nx,ˆy∈U′\ndMSE(ˆy, fθ(x))\n(5.4)\n5.2\nReMixMatch\nBerthelot et al.\n[12] propose to improve MixMatch by introducing two new techniques: distribution\nalignment and augmentation anchoring. Distribution alignment encourages the marginal distribution of\npredictions on unlabeled data to be close to the marginal distribution of ground-truth labels. Augmentation\nanchoring feeds multiple strongly augmented versions of the input into the model, encouraging each output\nto be close to the prediction for a weakly-augmented version of the same input.\n21\nFigure 14: ReMixMatch. Left. Distribution alignment adjusts the guessed labels distributions to match\nthe ground-truth class distribution divided by the average model predictions on Du. Right. Augmentation\nanchoring uses the prediction obtained using a weakly augmented image as targets for a strongly augmented\nversion of the same image. Image Source: [12].\nDistribution alignment. In order to force that the aggregate of predictions on unlabeled data matches\nthe distribution of the provided labeled data. Over the course of training, a running average ˜y of the model’s\npredictions on unlabeled data is maintained over the last 128 batches. For the marginal class distribution p(y),\nit is estimated based on the labeled examples seen during training. Given a prediction fθ(x) on the unlabeled\nexample x, the output probability distribution is aligned as follows: fθ(x) = Normalize (fθ(x) × p(y)/˜y).\nAugmentation Anchoring. MixMatch uses a simple ﬂip-and-crop augmentation strategy, ReMixMatch\nreplaces the weak augmentations with strong augmentations learned using a control theory based augmen-\ntation strategy following AutoAugment.\nWith such augmentations, the model’s prediction for a weakly\naugmented unlabeled image is used as a proxy label for many strongly augmented versions of the same\nimage in a standard cross-entropy loss.\nFor training, MixMatch is applied to the unlabeled and labeled batches, with the application of dis-\ntribution alignment and replacing the K weakly augmented example with a strongly augmented example,\nin addition to using the weakly augmented examples for predicting proxy labels for the unlabeled strongly\naugmented examples. With two augmented batches L′ and U′, the supervised and unsupervised losses are\ncomputed both using the cross-entropy loss as follows:\nL = Ls + wLu =\n1\n|L′|\nX\nx,y∈L′\nH(y, fθ(x))) + w 1\n|U′|\nX\nx,ˆy∈U′\nH(ˆy, fθ(x)))\n(5.5)\nIn addition to these losses, the authors add a self-supervised loss. First, a new unlabeled batch ˆU′ of\nexamples is created by rotating all of the examples with an angle r ∼{0, 90, 180, 270}. The rotated examples\nare then used to compute a self-supervised loss, where the classiﬁcation layer on top of the model predicts\nthe correct applied rotation, in addition to the cross-entropy loss over the rotated examples:\nLSL = w′ 1\n| ˆU′|\nX\nx,ˆy∈ˆU′\nH(ˆy, fθ(x))) + λ 1\n| ˆU′|\nX\nx∈ˆU′\nH(r, fθ(x)))\n(5.6)\n5.3\nFixMatch\nFixMatch [143] proposes a simple SSL algorithm that combines consistency regularization and pseudo-\nlabeling. In FixMatch (ﬁg. 15), both the supervised and unsupervised losses are computed using a cross-\nentropy loss. For labeled examples, the provided targets are used. For unlabeled examples x ∈Du, a weakly\naugmented version is ﬁrst computed using weak augmentation function Aw. As in self-training, the predicted\n22\nFigure 15: FixMatch. The model prediction on a weakly augmented input is considered as target if the\nmaximum output class probability is above threshold, this target can then be used to train the model on a\nstrongly augmented version of the same input using standard cross-entropy loss. Image Source: [12].\nlabel is then considered as a proxy label if the highest class probability is greater than a threshold τ. With\na proxy label, K strongly augmented examples are generated using a strong augmentation function As. We\nthen assign to these augmented versions the proxy label obtained with the weakly labeled version. The\nunsupervised loss can be written as follows:\nLu = w\n1\nK|Du|\nX\nx∈Du\nK\nX\ni=1\n1(max(fθ(Aw(x))) ≥τ)H(fθ(Aw(x)), fθ(As(x)))\n(5.7)\nAugmentations. Weak augmentations consist of a standard ﬂip-and-shift augmentation strategy. Speciﬁ-\ncally, the images are ﬂipped horizontally with a probability of 50% on all datasets except SVHN, in addition\nto randomly translating images by up to 12.5% vertically and horizontally. For the strong augmentations,\nRandAugment and CTAugment [12] are used where a given transformation (e.g., color inversion, translation,\ncontrast adjustment, etc.) is randomly selected for each sample in a batch of training examples, and the\namplitude of the transformation is a hyperparameter that is optimized during training.\nOther important factors in the FixMatch are the usage of Adam optimizer [82], weight decay regularization\nand the learning rate schedule, where the authors propose to use a cosine learning rate decay with a decay\nof η cos( 7πt\n16T ), where η is the initial learning rate, t is the current training step, and T is the total number of\ntraining iterations.\n6\nGenerative Models\nIn unsupervised learning, we are provided with samples x drawn i.i.d. from an unknown data distribution\nwith density p(x), and the objective is to estimate this density. Supervised learning, on the other hand,\nconsists of estimating a functional relationship between the inputs x and the labels y with the goal of\nminimizing the functional of the joint distribution p(x, y) [20]. Classiﬁcation can be treated as a special\ncase of estimating p(x, y), where we are only interested in the conditional distributions p(y|x), without the\nneed to estimate the input distribution p(x) since x will always be given at prediction time. Semi-supervised\nlearning with generative models can be viewed as either an extension of supervised learning, classiﬁcation in\naddition to information about p(x) provided by Du, or as an extension of unsupervised learning, clustering\nin addition to the provided labels from Dl. In this section, we explore some generative approaches for deep\nSSL.\n23\n6.1\nVariational Autoencoders for SSL\nVariational Autoencoders (VAEs) [83, 36] have emerged as one of the most popular approaches to unsuper-\nvised learning of complicated distributions. A standard VAE is an autoencoder trained with a reconstruction\nobjective between the inputs and their reconstructed versions, in addition to a variational objective term\nthat attempts to learn a latent space that roughly follows a unit Gaussian distribution, this objective is\nimplemented as the KL-divergence between the latent space and the standard Gaussian. With an input x,\nthe conditional distribution qφ(z|x) modeled by an encoder, the standard Gaussian distribution p(z) and the\nreconstructed input ˆx generated using a decoder pθ(x|z). The parameters φ and θ are trained to minimize\nthe following objective:\nL = dMSE(x, ˆx) + dKL(qφ(z|x), p(z))\n(6.1)\n6.1.1\nVariational Autoencoder\nKingma et al. [84] expanded the work on variational generative techniques [83, 128] for SSL, that exploit\ngenerative descriptions of the data to improve upon the classiﬁcation performance that would be obtained\nusing the labeled data alone.\nStandard VAEs for SSL (M1 Model)\nThe ﬁrst model consists of an unsupervised pretraining stage,\nin which the VAE is trained using the labeled and unlabeled examples. Using a fully trained VAE, the\nobserved labeled data x ∈Dl are transformed into the latent space deﬁned by z, the standard supervised\ntask can then be solved using (z, y) where y are the labels of x. With this approach, the classiﬁcation can\nbe performed in a lower dimensional space since the dimensionality of the latent variables z is much less\nthan that of the observations. These low dimensional embeddings are more easily separable since the latent\nspace is formed by independent Gaussian posteriors parameterized by an encoder, built by a sequence of\nnon-linear transformations of the inputs.\nExtending VAEs for SSL (M2 Model)\nIn the M1 model, the labels of Dl were ignored when training\nthe VAE. With the second model, the labels are also used during training. If the class labels are not available,\ny is treated as a latent variable in addition to the latent variable z. The network in this case contains three\ncomponents, qφ(y|x) modeled by a classiﬁcation network, qφ(z|y, x) modeled by an encoder, and pθ(x|y, z)\nmodeled by a decoder, with parameters φ and θ. The training is similar to a standard VAE with the addition\nof the posterior on y and loss terms to train qφ(y|x) if the labels are available. The distribution qφ(y|x) can\nthen be used at test time to get the predictions on unseen data.\nStacked VAEs (M1+M2 Model)\nThe two previous models can be concatenated to form a joint model.\nIn this case, the model M1 is ﬁrst trained to obtain the latent variables z1, the model M2 then uses the\nlatent variables z1 from model M1 as new representations of the data as opposed to raw values x. The ﬁnal\nmodel can be described as follows:\npθ(x, y, z1, z2) = p(y)p(z2)pθ(z1|y, z2)pθ(x|z1)\n(6.2)\n6.1.2\nVariational Auxiliary Autoencoder\nVariational Auxiliary Autoencoder [102, 124] extends the variational distribution with auxiliary variables a:\nq(a, z|x) = q(z|a, x)q(a|x), such that the marginal distribution q(z|x) can ﬁt more complicated posteriors\np(z|x) while improving the ﬂexibility of inference. In order to have an unchanged generative model p(x|z),\n24\nGenerator\nLatent space\nDiscriminator\nReal samples \nGenerated\nsamples\n\"real\"\nor\n\"fake\"\nFigure 16: GAN framework.\nDuring training, the discriminator D alternates between receiving real\nsamples from the data distribution p(x), with the goal of correctly classifying them as real, i.e., D(x) = 1, and\ngenerated samples G(z) with the aim of correctly classifying them as fake, i.e., D(G(z)) = 0, while competing\nwith the generator, trying to generate real-looking samples to fool the discriminator, i.e., D(G(z)) = 1.\nit is required that the joint mode p(x, z, a) gives back the original p(x, z) under marginalization over a, thus\np(x, z, a) = p(a|x, z)p(x, z), with p(a|x, z) ̸= p(a) to avoid falling back to the original VAE model.\nIn SSL, to incorporate the class information, an additional latent variable y is introduced, the generative\nmodel become p(y)p(z)p(a|z, y, x)p(x|y, z), with a, y, z as the auxiliary variable, class label, and latent\nfeatures respectively. In this case, the auxiliary unit a introduces a latent feature extractor to the inference\nmodel giving a richer mapping between x and y. The resulting model is parametrized by 5 neural networks: 1)\nan auxiliary inference model q(a|x), 2) a latent inference model q(z|a, y, x), 3) a classiﬁcation model q(y|a, x),\n4) a generative model p(a|.), and 5) a generative model p(x|.), which are trained on both a generative and\ndiscriminative tasks simultaneously.\n6.1.3\nInﬁnite Variational Autoencoder\nAnother variation of VAEs for SSL is Inﬁnite Variational Autoencoder [44], to overcome the limitation of\nVAEs of having a ﬁxed dimension of the latent space and a ﬁxed number of parameters in the generative\nmodel in advance, in which the capacity of the model must be chosen a priori with some foreknowledge of\nthe training data characteristics. Inﬁnite VAE solves this by producing an inﬁnite mixture of autoencoders\ncapable of growing with the complexity of the data to best capture its intrinsic structure. After training the\ngenerative model using unlabeled data, this model can then be combined with the available labeled data to\ntrain a discriminative model, which is also a mixture of experts, for classiﬁcation. For a given test example\nx, each discriminative expert produces a tentative output that is then weighted by the generative model. As\nsuch, each discriminative expert learns to perform better with instances that are more structurally similar\nfrom the generative model’s perspective. With a higher modeling capability, the inﬁnite VAE is able to\ncapture the distribution of the unlabeled data more accurately. Therefore, it provides a generative model\nthat allows the discriminative model, which is trained based on its output, to be more eﬀectively learned\nusing a small number of samples.\n6.2\nGenerative Adversarial Networks for SSL\nA Generative Adversarial Network (GAN) [55] (ﬁg. 16) consists of a generator network G and a discriminator\nnetwork D. The generator receives a latent variable z ∼p(z) sampled from the prior distribution p(z) and\nmaps to the input space.\nThe discriminator takes an input, either coming from the real data p(x) or\ngenerated by G and outputs the probability of the input being from either G or the real data distribution\np(x), represented with an empirical distribution D. The standard training procedure of GANs minimizes\n25\ntwo objectives by alternating between training the discriminator D and the generator G:\nLD = max\nD Ex∼p(x)[log D(x)] + Ez∼p(z)[1 −log D(G(z))]\nLG = min\nG −Ez∼p(z)[log D(G(z))]\n(6.3)\nwhere p(z) is usually chosen as a standard normal distribution. Other formulations have been proposed to\nimprove and stabilize the training procedure, such as the hinge-loss version of the adversarial loss [99, 151]\nand Wassertein GAN (WGAN) [4]. Which are subsequently improved in several ways [176, 107, 177], such as\nusing spectral normalization [107] on both the generator and the discriminator, or consistency regularization\non the discriminator [177].\n6.2.1\nCatGAN\nCategorical generative adversarial networks (CatGAN) [145] consist of combining both the generative and\nthe discriminative perspectives within the training procedure. The discriminator D in this case plays the role\nof C classiﬁers and is trained to maximize the mutual information between the inputs x and the predicted\nlabels for a number of C unknown classes. To aid these classiﬁers in their task of discovering categories that\ngeneralize well to unseen data, and avoid overﬁtting to spurious correlations in the data, the adversarial\ngenerative network comes into play and provides the examples the discriminator must become robust to.\nThe traditional two-player game in the GAN framework can be extended to CatGAN by having a dis-\ncriminator that assign all examples to one the C classes instead of a probability of x belonging to p(x), while\nstaying uncertain of the class assignments for the generated samples by G. After training such a classiﬁer-\ngenerator pair where the discovered C classes coincide with the classiﬁcation problem we are interested in,\nthe classiﬁer can then be used during inference being trained only on unlabeled data.\nCatGAN objective dictates three requirements for the discriminator and two requirements that the gen-\nerator should fulﬁlled:\n• Discriminator requirements: should (1) be certain of class assignment for samples from p(x), (2) be\nuncertain of assignment for generated samples, and (3) by assuming a uniform prior p(y) over classes,\nall classes must be distributed equally.\n• Generator requirements: should (1) generate samples with highly certain class assignments, and (2)\nsimilar to the discriminator, equally distribute samples across all classes.\nIn order to have the output class distribution D(x) = p(y|x, D) to be highly peaked where D is certain\nabout the class assignment, the entropy H(D(x)) of the class distribution must be low. For the generated\nsamples D(G(z)) = p(y|G(z), D), the predictions should be highly uncertain with a uniform class distribu-\ntion, in this case, the entropy H(D(G(z))) must be high. The ﬁrst two requirements can then be enforced by\nsimply minimizing H(D(x)) and maximizing the H(D(G(z))). To meet the third requirement that all classes\nshould be used equally, the entropy of the marginal class distribution as measured empirically for both D\nand G needs to be maximized:\nHD = H\n \n1\nN\nN\nX\ni=1\nD(xi)\n!\nHG ≈H\n \n1\nM\nM\nX\ni=1\nD(G(zi))\n!\n(6.4)\nCombining these requirements, CatGAN objective for the discriminator and the generator is:\nLD = max\nD −Ex∼p(x)[H(D(x))] + Ez∼p(z)[H(D(G(z)))] + HD\nLG = min\nG Ez∼p(z)[H(D(G(z)))] −HG\n(6.5)\n26\nIn SSL, if the input x comes from the labeled set Dl with a label y in the form of a one-hot vector, the\ndiscriminator D is trained with a cross-entropy loss in addition to LD:\nLD + λE(x,y)∼p(x)l[−y log G(x)]\n(6.6)\nwhere λ is a cost weighting term.\n6.2.2\nDCGAN\nAnother way of using GANs for SSL is to leverage the unlabeled examples to learn good and transferable\nintermediate representations, which can then be used on a variety of supervised learning tasks such as image\nclassiﬁcation based on a small labeled set Dl. Radford et al. [123] propose to build good image representations\nby training GANs, and later reusing parts of the generator and discriminator networks as feature extractors\nfor supervised tasks. The authors propose Deep Convolutional GANs (DCGAN), a class of architectures\nwith a set of constraints on the architectural topology of convolutional GANs to be able to scale them while\nmaintaining a stable training in most settings, such as replacing polling layers with strided convolutions for\nthe discriminator, fractional-strided convolutions for generator, using batchnorm [73] in both the generator\nand the discriminator, and removing fully connected layers for deeper architectures.\nAfter training DCGANs for image generation, the representations learned by DCGANs can be utilized\nfor downstream tasks, by either ﬁne tuning the discriminator features with an additional classiﬁcation layer\nadded on top and trained on Dl, or by ﬂattening and concatenating the learned features and training a linear\nclassiﬁer on top of them.\n6.2.3\nSGAN\nDCGAN demonstrated the utility of the learned representations for SSL, but it has several undesirable\nproperties. Using the learned representations of the discriminator after the fact doesn’t allow for training\nthe classiﬁer and the generator simultaneously, doing this is more eﬃcient, but more importantly, improving\nthe discriminator improves the classiﬁer, and improving the classiﬁer improves the discriminator, which\nimproves the generator.\nSemi-Supervised GAN (SGAN) [112] takes advantage of this feedback loop by\nallowing to learn a generative model and a classiﬁer simultaneously, signiﬁcantly improving the classiﬁcation\nperformance, the quality of the generated samples, and reducing training time.\nInstead of a discriminator network outputting an estimated probability that the input image is drawn\nfrom the data distribution. For C classes, SGAN consists of a discriminator with C + 1 output, with per\nclass output in addition to a fake class output. Training an SGAN is similar to training a GAN; the only\ndiﬀerence is using the labels to train the discriminator if the input x is drawn for the labeled set Dl. The\ndiscriminator is trained to minimize the negative log-likelihood with respect to the given labels, and the\ngenerator is trained to maximize it.\n6.2.4\nFeature Matching GAN\nTraining GANs consists if ﬁnding a Nash equilibrium to a two-player non-cooperative game, with each player\ntrying to minimize its cost function. To solve this, GAN training consists of applying gradient descent on\neach player’s cost simultaneously, but with such a training procedure, there is no guarantee of convergence.\nFeature matching [135] was proposed to encourage convergence. Feature matching addresses the instability\nof GANs by specifying a new objective for the generator that prevents it from over training on the current\ndiscriminator. Instead of directly maximizing the output of the discriminator, the new objective requires the\ngenerator to generate data that matches the ﬁrst-order feature statistics between of the data distribution,\n27\ni.e., the hidden representations of the discriminator. For some activations h(x) of a given intermediate layer,\nthe new objective is deﬁned as:\n∥Ex∼p(x)[h(x)] −Ez∼p(z)[h(G(z))]∥2\n(6.7)\nThe problem of the generator mode collapse, where it always emits the same point, is still present\neven with feature matching because the discriminator processes each example independently, so there is no\ncoordination between its gradients, and thus no mechanism to tell the outputs of the generator to become\nmore dissimilar to each other.\nTo avoid this, in addition to feature matching, a new technique called\nminibatch discrimination is also integrated into the training procedure to allow the discriminator to look\nat multiple data examples in combination, where the discriminator still classiﬁes single examples as real or\ngenerated data, but it is now able to use the other examples in the minibatch as side information.\nFor SSL, similar to SGAN, the discriminator in feature matching GAN employs a (C + 1)-class objective\ninstead of binary classiﬁcation, where true samples are classiﬁed into the ﬁrst C classes and generated samples\nare classiﬁed into the (C +1)-th fake class, the probability of x being fake in this case is p(y = C +1|G(z), D),\ncorresponding to 1 −D(x) in the original GAN framework. The loss function for training the classiﬁer then\nbecomes L = Ls + Lu where:\nLs = −Ex,y∼p(x)l[log p(y|x, y < K + 1, D)]\nLu = −Ex∼p(x)u log[1 −p(y = K + 1|x, D)] −Ez∼p(z) log[p(y = K + 1|G(z), D))]\n(6.8)\nThe above objective is similar to the original GAN formulation by considering p(y = K + 1|G(z), D) to\nbe the probability of fake samples, while the only diﬀerence is that the probability of true samples if split\ninto C sub-classes. This (C + 1)-class discriminator objective lead to strong empirical results, and was later\nwidely used to evaluate the eﬀectiveness of generative models [42, 152]. The main drawback is that feature\nmatching works well in classiﬁcation but fails to generate indistinguishable samples, while the other objective\nof minibatch discrimination is good at realistic image generation but cannot predict labels accurately.\n6.2.5\nBad GAN\nFeature matching GAN formulation raises two questions. First, it is not clear why the formulation of the\ndiscriminator can improve the performance when combined with a generator. Second, it seems that good\nsemi-supervised learning and a good generator cannot be obtained at the same time. Dai et al. [32] addressed\nthese questions by showing that for a (C +1)-class discriminator formulation of GAN-based SSL, good semi-\nsupervised learning requires a bad generator that does not match the true data distribution, but simply\nplays the role of a complement generator to help the discriminator obtain correct decision boundaries in\nhigh-density areas in the feature space.\nTo overcome the drawbacks of feature matching GANs, the new objective function of the generator is:\nmin\nG −H(pG) + Ex∼pG log p(x)I[p(x) > ϵ] + ∥Ex∼pGh(x) −Ex∼p(x)h(x)∥2\n(6.9)\nwhere pG is the distribution induced by the generator G, I[·] is an indicator function and ϵ is a threshold.\nThe ﬁrst term maximizes the entropy of generator to avoid the collapsing issues that are a clear sign of\nlow entropy, but given that for implicit generative models, GANs only provide samples rather than an\nanalytic density form, the entropy can either optimized in the input space i.e., H(pG(x)) using variational\ninference or the feature space i.e., H(pG(h(x))) using a pull-away term (PT) [183] as an auxiliary cost for the\nentropy. The second term enforces the generation of samples with low density in the input space by pushing\nthe generated samples to move towards low-density regions deﬁned by p(x), this probability distribution\nover images is estimated using PixelCNN++ [136] model, which pretrained on the training set, and ﬁxed\nduring semi-supervised training. The last term is the feature matching objective. This method substantially\n28\nimproves the performance of image classiﬁcation over vanilla feature matching GANs on several benchmark\ndatasets.\n6.2.6\nTriple-GAN\nAs discussed in Bad GAN, the generator and the discriminator (i.e., the classiﬁer) may not be optimal at the\nsame time, since that for an optimal generator, i.e., p(x) = pg(x), an optimal discriminator should identify x\nas fake. Still, as a classiﬁer, the discriminator should predict the correct class of x conﬁdently since x ∼p(x),\nindicating that the discriminator and generator may not be optimal at the same time. Instead of learning\na complement generator for classiﬁcation, Triple-GAN [26] is designed to achieve simultaneously a good\ngeneration of realistically-looking samples conditioned on class labels, and produce a good classiﬁer with the\nsmallest possible prediction error.\nTriple-GAN consists of three components: (1) a classiﬁer C that characterizes the conditional distribution\npc(y|x) ≈p(y|x); (2) a class-conditional generator G that characterizes the conditional distribution in the\nother direction pg(x|y) ≈p(x|y); and (3) a discriminator D that distinguishes whether a pair of data (x, y)\ncomes from the true distribution p(x, y). All the components are parameterized as neural networks. The\ndesired equilibrium is that the joint distributions deﬁned by the classiﬁer and the generator both converge\nto the true data distribution.\nFor p(x) as the empirical distribution of inputs x ∈D and p(y) as a uniform distribution which is\nassumed to be the same as the distribution of labels on labeled data, the classiﬁer produces pseudo-labels\npc(y|x) given x, in this case, the examples x and the pseudo-labels y are drown from the joint distribution\npc(x, y) = p(x)pc(y|x). Similarly, the generator produces examples x = G(y, z), with y ∼p(y) and the\nlatent variables z ∼p(z), the generated examples x and labels y are drown from the joint distribution\npg(x, y) = p(y)pg(x|y). These pseudo input-label pairs (x, y) generated by both C and G are sent to the\nsingle discriminator D. The objective function is formulated as:\nL = min\nC,G max\nD E(x,y)∼p(x,y)[log D(x, y)] + αE(x,y)∼pc(x,y)[log(1 −D(x, y))]\n+ (1 −α)E(x,y)∼pg(x,y)[log(1 −D(G(y, z), y))]\n(6.10)\nwhere α ∈[0, 1] is a constant that controls the relative importance of generation and classiﬁcation. To prop-\nerly leverage unlabeled data, an additional regularization is enforced on classiﬁer C, consisting of minimizing\nthe conditional entropy of pc(y|x), the cross-entropy between p(y) and pc(y), and a consistency regulariza-\ntion with a dropout as the source of noise. In such a setting, the classiﬁer achieves high accuracy with only\nvery few labeled examples, while the generator produces state-of-the-art images, even when conditioned on\ny labels.\nEnhanced TripleGAN (EnhancedTGAN) [167] improves Triple-GAN by adopting a class-wise mean fea-\nture matching to regularize the generator and a semantic matching term to ensure the semantics consistency\nof the synthesized data between the generator and the classiﬁer, further improving the state-of-the-art results\nin both SSL and instance synthesis.\n6.2.7\nBiGAN\nOne of the limitations of the traditional GAN framework is not being able to infer latent representations\nz that can be used as rich representations of the data x for a more eﬃcient training. Unlike VAEs with\nan inference network (i.e., decoder) p(.) that can learn a variational posterior over latent variables, the\ngenerator is typically a directed, latent variable model with latent variables z and observed variables x,\nmaking it unable to infer the latent feature representations for a given data point. BiGAN [38] solves this by\nintroducing an encoder E as an additional component in the GAN framework, which maps data x to latent\n29\nrepresentations z. The BiGAN discriminator D discriminates not only in data space between x and G(z),\nbut jointly in data and latent space, between pairs (x, E(x)) and (G(z), z), where the latent component is\neither an encoder output E(x) or a generator input z. A trained BiGAN encoder can then serve as feature\nextractor for downstream tasks. The BiGAN training objective is deﬁned as a minimax objective:\nL = min\nG,E max\nD Ex∼p(x)(log D(x, E(x))) + Ez∼p(z)(1 −log D(G(z), z))\n(6.11)\nKumar et al. [90] proposed Augmented-BiGAN, an improved version of BiGAN for SSL. The Augmented-\nBiGAN is similar to other GAN frameworks used for SSL, treating the generated samples as an additional\nclass to the regular classes that the classiﬁer aims to label, with an additional Jacobian-based regularization\nthat is introduced to encourage the classiﬁer to be robust to local variations in the tangent space of the\ninput manifold. The BiGAN trained encoder is used in calculating these Jacobians, resulting in an eﬃcient\nestimation of the tangents space at each training sample, and avoiding the expensive SVD-based method\nused in contractive autoencoders [130].\n7\nGraph-Based SSL\nGraphs are a powerful tool to model interactions and relations between diﬀerent entities, in order to under-\nstand the represented system in both a global and local manner. In Graph-based SSL [193] methods, each\ndata point xi, be it labeled or unlabeled, is represented as a node in the graph, and the edge connecting each\npair of nodes reﬂects their similarity. Formerly, A graph G(V, E) is a collection of V = {x1, . . . , xn} vertices\nor nodes and E = {eij}n\ni,j=1 edges. The n × n adjacency matrix A of a graph G describes the structure of\nthe graph, with each element as a non-negative weight associated with each edge, if two nodes xi and xj are\nnot connected to each other, then Aij = 0. The adjacency matrix A can either be derived using a similarity\nmeasure between the data points [191, 74], or be explicitly derived from external data, such as a knowledge\ngraph [165], and provided as input. Graph-based tasks can be broadly categorized into four categories [57]:\nnode classiﬁcation, link prediction, clustering, and visualization. Graph methods can also be transductive or\ninductive in nature; transductive methods are only capable of producing labels assignments of the examples\nseen during training (i.e., the unlabeled nodes of the graph), while inductive methods are more generalizable,\nand can be transferred and applied to unseen examples. In this section, we will discuss node classiﬁcation\napproaches, given that the objective in SSL is to assign labels to the unlabeled examples. Node classiﬁcation\napproaches can be broadly grouped into methods which propagate the labels from labeled nodes to unlabeled\nnodes based on the assumption that nearby nodes tend to have the same labels [6, 191, 185], and methods\nwhich learn node embeddings based on the assumption that nearby nodes should have similar embeddings\nin vector space and then apply classiﬁers on the learned embeddings [59]. First, we start with some graph\nconstruction approaches and then discuss several popular methods for graph-based SSL.\n7.1\nGraph Construction\nTo apply graph-based SSL, we ﬁrst need a graph. The graph can either be presented as an input in the form\nof an adjacency matrix A or can be constructed to reﬂect the similarity of the nodes. A useful graph should\nreﬂect our prior knowledge about the domain and is the practitioner’s responsibility to feed a good graph to\ngraph-based SSL algorithms in order to produce valuable outputs (for more details, see Ch3 & 7 [191]).\nIn case we have limited domain knowledge about the dataset at hand, Zhu et al. [191] describes some\ncommon ways to create graphs:\n• Fully connected graphs. A simple form the graph can take is being fully connected with weighted\nedges between all pairs of data. With full connectivity, the derivatives of the graph w.r.t., the weights\n30\ncan be computed to update the weights of the edges, but the computational cost, in this case, will be\nhigh.\n• Sparse graphs. A sparse graph can be constructed so that each node is only connected to a few\nsimilar nodes, while the connections to dissimilar nodes are removed. Examples of sparse graphs are\nkNN graphs where nodes i and j are connected if i is one of k-nearest neighbors [157] of j or vice\nversa. A possible way to obtain the edge weight Aij between xi and xj is to use a Gaussian kernel [15]:\nWij = exp{−∥xi −xj∥2/2σ2} with a hyperparameter σ. Another approach is ϵNN graphs where nodes\ni and j are connected if the distance d(i, j) ≤ϵ. These graphs can be created using either the raw data\nor representations extracted from a trained network and updated iteratively (e.g., CNN features [74]).\n7.2\nLabel Propagation\nThe main assumption in label propagation is that the data points in the same manifold are very likely to\nshare the same semantic label [185]. To this end, label propagation propagates labels of the labeled data\npoints to the unlabeled data points according to the data manifold structures and the in-between node\nsimilarity.\nIn label propagation [191, 185, 48], the labeling scores are deﬁned as the optimal solution that minimizes\nthe loss function. Let a n × C matrix ˆY corresponds to the new classiﬁcation scores for each data point,\nwhere each row ˆYi is a probability distribution over C classes, and Y is a n × C matrix containing the labels\nfor the labeled data points, where each row Yi is a one-hot vector if xi is a labeled data point, and a vector\nof zeros otherwise. The loss function to be minimized for label propagation [191] is:\nL = 1\n2\nn\nX\ni,j=1\nAij(ˆyi −ˆyj)2 = ˆY T L ˆY\n(7.1)\nwhere L = D −A is the graph Laplacian matrix that measures the smoothness of the graph, with D =\nPn\nj=1 Aij as the degree matrix, this loss function can be viewed as a graph Laplacian regularization which\nincurs a large penalty when similar nodes with a large weight Aij are predicted to have diﬀerent labels\nˆyi ̸= ˆyj. By deﬁning a n × n probabilistic transition matrix P = D−1A, where Pij is the probability of\ntransit from node i to j, and spliting the matrices P, Y and ˆY into labeled and unlabeled sub-matrices:\nP =\n\u0012 Pll\nPlu\nPul\nPuu\n\u0013\nY =\n\u0012 Yl\nYu\n\u0013\nˆY =\n \nˆYl\nˆYu\n!\n(7.2)\nthe optimal solution for eq. (7.1) is:\nˆYl = Yl\nˆYu = (I −Puu)−1PulYl\n(7.3)\nwhere I is an identity matrix. The labeling score computation involves the matrix inversion operation, which\nis computationally heavy for large graphs. As an alternative, Zhu et al. [191] propose an iterative approach\nto converge to the same solution:\n1. Propagate ˆY ←P ˆY .\n2. Preserve the labeled data ˆYl = Yl.\n3. Repeat from step 1 until convergence.\nAnother similar label propagation algorithm was proposed by Zhou et al. [185], where, in addition to\nthe contribution a node i receives from its neighbors j, it receives an additional small contribution given by\n31\nits initial value. In this case, the labels of the labeled nodes might change to better reﬂect the ﬁnal labels,\nwhich can be helpful if the initial labels are noisy. The loss function in this instance is:\nL = 1\n2\nn\nX\ni,j=1\nAij∥\nˆYi\n√Dii\n−\nˆYi\np\nDjj\n∥2 + (1/α −1)\nn\nX\ni=1\n∥ˆYi −Yi∥2\n(7.4)\nwith a hyperparameter α. The ﬁrst and second terms in the loss function correspond to the smoothness\nconstraint and the ﬁtting constraint, respectively. The smoothness constraint results in labels that do not\nchange too much between nearby points, while the ﬁtting constraint forces the ﬁnal labels of the labeled\nnodes to be similar to their initial value. The optimal solution that minimizes the loss function is:\nˆY = (I −αS)−1Y\n(7.5)\nwhere S = D−1/2AD−1/2. Similar the the ﬁrst algorithm, Zhou et al. [185] propose a less computationally\nexpensive iterative approach:\n1. Propagate ˆY ←αS ˆY + (1 −α)Y .\n2. Repeat step 1 until convergence.\nIt is worth noting that even though the iterative method is the standard approach for label propagation, it\ndoes not output the same labeling results as the the optimal solution.\n7.3\nGraph Embedding\nThe term graph embedding has been used in the literature in two ways: to represent an entire graph in\nvector space, or to represent each individual node in vector space [57]. In this paper, we are interested in\nlearning node embeddings since such a representation can be used for SSL tasks, such as node classiﬁcation.\nThe goal of node embedding is to encode the nodes as low dimensional vectors that reﬂect their positions and\nthe structure of their local neighborhood. These low dimensional embeddings can be viewed as encoding or\nprojecting, nodes into a latent space, where geometric relations in this latent space correspond to interactions\n(e.g., edges) in the original graph [62, 71]. Factorization-based approaches such as Laplacian Eigenmaps [11]\nand Locally Linear Embedding (LLE) [18] are examples of algorithms based on this rationale, but they have\nscalability issues for large graphs, other more scalable embedding techniques which leverage the sparsity of\nreal-world networks have been proposed. For example, LINE [147] and HOPE [115] attempt to preserve high\norder proximities (e.g., the edge weights of a given node, and the similarity of edge weights of each pair of\nnodes). Intuitively, the goal of these methods is simply to learn embeddings for each node such that the\ninner product between the learned embedding vectors approximates some deterministic measure of graph\nproximity [62].\nAnother family of method is random walks introduced by [119] and its variants [59, 49, 21, 1]. Instead of\nusing a deterministic measure of graph proximity like factorization-based approaches, these methods optimize\nthe embeddings so that nodes have similar embeddings if they tend to co-occur within short random walks\nover the graph, making them especially useful when one can either only partially observe the graph or the\ngraph is too large to measure in its entirety. Random walks consist of starting from a randomly samples node\nx0 ∈sample(V ), and then repeatedly sampling an edge to transition to the next node xi+1 = sample(N(xi)),\nwith N(xi) as the neighboring nodes of xi. The resulting sequence of random walks x0 →x1 →x2 →. . .\ncan then be passed to word2vec algorithm [105] with the objective to embed each node xi within the random\nwalk sequences to be close in the vector space to its neighboring nodes. With a context window of size T,\nwith T usually deﬁned to be in the range T ∈{2, . . . , 10}, the representation of the anchor node xi is brought\ncloser to the embeddings of its next neighbors {xi−T/2, . . . , xi, . . . , xi+T/2}.\n32\nRandom Walk\nBiased random walks\nDFS\nBFS\nFigure 17: Random Walks.\nLeft.\nAn example of a random walk of length 4 starting from node x1:\nx1 →x3 →x5 →x7 →x6 Right. Breadth ﬁrst search (BFS) and depth ﬁrst search (DFS) strategies from\nnode x3.\nFormally, random walk method learn embeddings zi of a given node xi so that:\npT (xj|xi) ≈\nez⊤\ni zj\nP\nxk∈V ez⊤\ni zk\n(7.6)\nwhere pT (xj|xi) is the probability of visiting xj on a length-T random walk starting at xi. To learn such\nembeddings, the following loss is optimized:\nL =\nX\n(xi,xj)∈RW\n−log\n \nez⊤\ni zj\nP\nxk∈V ez⊤\ni zk\n!\n(7.7)\nwhere RW is the set of the length-T generated random walks. Evaluating the loss is prohibitively expensive,\nsince assessing the denominator requires a computation over all the nodes of the graph. Thus, diﬀerent\nmethods use diﬀerent optimization to approximate the loss in eq. (7.7). For example, DeepWalk [119] uses a\nhierarchical softmax to compute the denominator, while node2vec approximates eq. (7.7) using negative sam-\npling similar to word2vec. The diﬀerent methods also diﬀer in the construction of random walk, DeepWalk\nuses simple unbiased random walks over the graph, while node2vec introduces two random walk hyperpa-\nrameters, p and q, to smoothly interpolate between walks that are more akin to breadth-ﬁrst or depth-ﬁrst\nsearch (ﬁg. 17). The hyperparameter p controls the likelihood of the walk immediately revisiting a node,\nwhile q controls the likelihood of the walk revisiting its neighborhood [62].\nFor SSL, the learned embeddings can then be used as inputs to train a classiﬁer over the labeled nodes and\nthen applied over the unlabeled node. Alternatively, a cross-entropy term can be added to the unsupervised\nloss in eq. (7.7) for SSL based random walks, in order to jointly train a classiﬁer on top of the node embeddings\nover the labeled nodes. For example, Planetoid [172] introduces a hyperparameter r to control the sampled\ninstances for a given training iteration, alternating between sampling random pairs from a given random\nwalk for the unsupervised loss if r < random, and a couple of nearby labeled nodes with the same label for\nthe supervised loss if r ≥random. The embeddings are trained using both the supervised and unsupervised\nloss, while the classiﬁer is only trained with a supervised loss.\n7.4\nGraph Neural Networks\nRandom walks based method, with their expressivity (i.e., incorporating both local and higher-order neigh-\nborhood information) and eﬃciency (i.e., do not need to consider all node pairs when training), suﬀer from\nsome limitations, such as the lack of parameter sharing where every node has its own unique embedding,\nand the inherent transductive nature of these approaches, in which the embeddings are only generated for\nnodes seen during training. This is especially problematic for evolving graphs, massive graphs that cannot\n33\nFigure 18: Context Aggregation. An example of a three-step context aggregation. The context of x1 at\nk = 2 depends not only on its neighboring node x2, but also the neighbors of x2 due to the ﬁrst aggregation\nstep.\nbe fully stored in memory, or domains that require generalizing to new graphs after training [62]. To solve\nthese issues, a number of methods use deep neural networks based methods applied to graphs [186, 168, 8].\nDNGR [19] and SDNE [158] propose the ﬁrst application of deep networks for graphs by using deep autoen-\ncoders [70] in order to compress the information about a node’s local neighborhood. A high dimensional\nrepresentation si ∈R|V | of a node xi, which describes the proximity of node xi to all other nodes in the\ngraph is ﬁrst extracted, and then fed through an autoencoder for dimensionality reduction and trained us-\ning a reconstruction loss. After training, the bottleneck low dimensional representation is then used as an\nembedding for xi. However, these approaches suﬀer from similar limitations as random walks methods, with\ninputs of size |V |, which can be extremely costly and even intractable for large graphs, in addition to their\ntransductive nature.\nSeveral recent node embedding approaches aim to solve the main limitations of the random walks and\nautoencoder based methods by designing functions that rely on a node’s local neighborhood (ﬁg. 18), but\nnot necessarily the entire graph. Unlike the previously discussed methods, graph neural networks use the\nnode features, e.g., proﬁle information for a social network or even simple statistics such as node degree [61]\nor one-hot vectors [86], to generate the embeddings. These methods are often called convolutional because\nthey represent a node as a function of its surrounding neighborhood, similar to CNNs [62]. The training\nprocedure starts by initializing the ﬁrst hidden states h0\ni using the nodes features xi: h0\ni ←xi, ∀xi ∈V . For\nK training iterations, at each step, the hidden stated are updated by aggregating the hidden states of the\nneighboring nodes, with an AGGREGATE, COMBINE, a non-linearity σ, and a NORMALIZE functions as\nfollows [61]:\nFor k = 1 . . . K:\nFor xi ∈V :\n1. h′ ←AGGREGATEk({hk−1\nj\n, ∀xj ∈N(xi)})\n2. hk\ni ←σ(W k · COMBINE(hk−1\ni\n, h′))\n3. hk\ni ←NORMALIZE(hk\ni )\nand at the end, the embeddings zi of node xi are the ﬁnal hidden states: zi ←hK\ni .\nThe aggregation\nfunction and the set of trainable parameters W k, ∀k ∈[1, K] specify how to aggregate the local neighborhood\ninformation. The diﬀerent approaches such as GCN [137, 147, 81, 85], GraphSAGE [61] and GAT [155] follow\nthe same procedure but diﬀer primarily in how the aggregation, the combination and the normalization are\nperformed. For example, GraphSAGE uses concatenation as a combination function and experiment with\nvarious general aggregation functions, i.e., the element-wise mean, max-pooling, and LSTMs, while GCN\n34\nuses a weighted sum as a combination function and element-wise mean as an aggregate. The weight are then\ntrained using an unsupervised loss similar to random walks based methods, and for SSL [85], a classiﬁer is\ntrained on top of the node embeddings (i.e., the ﬁnal hidden state) to predict the class labels for the labeled\nnodes, which can then applied on the unlabeled nodes for node classiﬁcation.\n8\nSelf-Supervision for SSL\nSelf-supervised learning [163, 76] is a form of unsupervised learning, where the model is trained using a\nstandard supervised loss, but on a pretext task where the supervision comes from the data itself.\nThe\nobjective, in this case, is not to maximize ﬁnal performance on the pretext task, but rather to learn rich\nand transferable features for downstream tasks. A variety of pretext tasks were proposed, where the model\nis ﬁrst trained on one or multiple tasks with unlabeled examples, the resulting model is either used for\ngenerating representations for the raw data, which are utilized for training a shallow classiﬁer on Dl, or\ndirectly ﬁne-tuned for a downstream task with labeled images. Examples of such pretext tasks for computer\nvision are:\n• Exemplar-CNN [40]. For a given image, a set of N patches are generated using diﬀerent transfor-\nmations, all these patches are then considered as a separate class, and the model is trained to predict\nthe correct class for a given input patch.\n• Rotation [53]. A given rotation out of four possible rotations of multiple of 90◦, i.e., [0◦, 90◦, 180◦, 270◦],\nis applied to the input image, and the model is trained to predict the correct rotation that was applied.\n• Patches [37]. A ﬁrst patch is randomly extracted from the input image, this patch is considered as the\ncenter, and eight diﬀerent neighboring and non-overlapping patches are extracted with small jitters at\nthe eight neighboring locations, the model is then trained to predict the position of one of the second\npatches with regard to the ﬁrst one. Other versions of this pretext task were proposed, such as jigsaw\npuzzle [111] where a random permutation of the nine patches are fed into the model, and the objective\nis to predict the correct permutation that was applied to get the correct ordering of the patches.\n• Colorization [179]. The input image is ﬁrst transformed from RGB to Lab color space, an input with\nonly the luminance information contained within the L component or the color information with ab\ncomponents is fed into the model, and the objective is to predict the rest of the information, i.e., either\nthe luminance or the coloring of the image. The task can either be considered as a regression problem\nor a classiﬁcation problem by quantizing the Lab color space.\n• Contrastive Predictive Coding [114]. Using a contrastive loss based on Noise Contrastive Esti-\nmation [60] and its recent versions such as Momentum Contrast [66] and SimCLR [22], the model\nis trained to diﬀerentiate between positive and negative samples, the positives can be a given input\nimage and its transformed versions, or a given patch and its neighboring patches, while the negatives\nare randomly sampled images or patches.\nSuch pretext tasks can easily be utilized for SSL, where the model is trained on the whole dataset on the\npretext task with self-supervision, and then adapted to the labeled set Dl using the standard cross-entropy\nloss, either simultaneously as demonstrated by [175, 12] with rotation as the pretext task, or iteratively, by\nﬁrst training the model using self-supervision and then ﬁne tuning it on Dl as demonstrated by [22, 23] using\ncontrastive learning.\n35\n9\nConclusion\nIn this paper, we introduced semi-supervised learning, with its main approaches and assumptions, with\nSSL techniques within deep learning framework. Speciﬁcally, this review covered four broad categories of\napproaches for SSL: consistency regularization, generative models, graph-based methods, and holistic ap-\nproaches. With the growing research interest in data-eﬃcient deep learning algorithms, it is foreseeable that\ndeep SSL methods could approach the performance of fully supervised methods, and have board applications\nintegrated into diﬀerent systems and learning paradigms.\nAcknowledgements\nY. Ouali is supported by Randstad corporate research in collaboration with Université Paris-Saclay, Cen-\ntraleSupélec, MICS. We thank Victor Bouvier for his helpful feedback on an earlier version.\nReferences\n[1] Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A Alemi. Watch your step: Learning node embeddings\nvia graph attention. In Advances in Neural Information Processing Systems, pages 9180–9190, 2018.\n[2] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Pseudo-labeling and conﬁrmation bias\nin deep semi-supervised learning. arXiv preprint arXiv:1908.02983, 2019.\n[3] Ehsan Mohammady Ardehaly and Aron Culotta. Co-training for demographic classiﬁcation using deep learning from\nlabel proportions. In 2017 IEEE International Conference on Data Mining Workshops (ICDMW), pages 1017–1024.\nIEEE, 2017.\n[4] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.\n[5] Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. There are many consistent explanations of\nunlabeled data: Why you should average. arXiv preprint arXiv:1806.05594, 2018.\n[6] Arik Azran. The rendezvous algorithm: Multiclass semi-supervised learning with markov random walks. In Proceedings\nof the 24th international conference on Machine learning, pages 49–56, 2007.\n[7] Yauhen Babakhin, Artsiom Sanakoyeu, and Hirotoshi Kitamura. Semi-supervised segmentation of salt bodies in seismic\nimages using an ensemble of convolutional neural networks. In German Conference on Pattern Recognition, pages 218–\n231. Springer, 2019.\n[8] Davide Bacciu, Federico Errica, Alessio Micheli, and Marco Podda. A gentle introduction to deep learning for graphs.\narXiv preprint arXiv:1912.12693, 2019.\n[9] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. What’s the point: Semantic segmentation with point\nsupervision. In European conference on computer vision, pages 549–565. Springer, 2016.\n[10] Oscar Beijbom. Domain adaptations for computer vision applications. arXiv preprint arXiv:1211.4860, 2012.\n[11] Mikhail Belkin and Partha Niyogi.\nLaplacian eigenmaps and spectral techniques for embedding and clustering.\nIn\nAdvances in neural information processing systems, pages 585–591, 2002.\n[12] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raﬀel. Remixmatch:\nSemi-supervised learning with distribution alignment and augmentation anchoring. arXiv preprint arXiv:1911.09785,\n2019.\n[13] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raﬀel. Mixmatch: A\nholistic approach to semi-supervised learning. In Advances in Neural Information Processing Systems, pages 5050–5060,\n2019.\n[14] Christian Biemann. Unsupervised and knowledge-free natural language processing in the structure discovery paradigm.\nPhD thesis, Leipzig University, Germany, 2007.\n[15] Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.\n[16] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh\nannual conference on Computational learning theory, pages 92–100, 1998.\n[17] Leo Breiman. Randomizing outputs to increase prediction accuracy. Machine Learning, 40(3):229–242, 2000.\n36\n[18] Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global structural information.\nIn Proceedings of the 24th ACM international on conference on information and knowledge management, pages 891–900,\n2015.\n[19] Shaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural networks for learning graph representations. In Thirtieth AAAI\nconference on artiﬁcial intelligence, 2016.\n[20] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book\nreviews]. IEEE Transactions on Neural Networks, 20(3):542–542, 2009.\n[21] Haochen Chen, Bryan Perozzi, Yifan Hu, and Steven Skiena. Harp: Hierarchical representation learning for networks. In\nThirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[22] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. A simple framework for contrastive learning of\nvisual representations. arXiv preprint arXiv:2002.05709, 2020.\n[23] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoﬀrey Hinton. Big self-supervised models are\nstrong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020.\n[24] Yanhua Cheng, Xin Zhao, Rui Cai, Zhiwei Li, Kaiqi Huang, Yong Rui, et al. Semi-supervised multimodal deep learning\nfor rgb-d object recognition. 2016.\n[25] Yong Cheng. Semi-supervised learning for neural machine translation. In Joint Training for Neural Machine Translation,\npages 25–40. Springer, 2019.\n[26] LI Chongxuan, Tauﬁk Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In Advances in neural information\nprocessing systems, pages 4088–4098, 2017.\n[27] Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc V Le. Semi-supervised sequence modeling with\ncross-view training. arXiv preprint arXiv:1809.08370, 2018.\n[28] Martin Cooke, Phil Green, Ljubomir Josifovski, and Ascension Vizinho. Robust automatic speech recognition with missing\nand unreliable acoustic data. Speech communication, 34(3):267–285, 2001.\n[29] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation\nstrategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 113–123,\n2019.\n[30] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical data augmentation with no\nseparate search. arXiv preprint arXiv:1909.13719, 2019.\n[31] Jifeng Dai, Kaiming He, and Jian Sun.\nBoxsup: Exploiting bounding boxes to supervise convolutional networks for\nsemantic segmentation. 2015 IEEE International Conference on Computer Vision (ICCV), Dec 2015.\n[32] Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Russ R Salakhutdinov. Good semi-supervised learning that\nrequires a bad gan. In Advances in neural information processing systems, pages 6510–6520, 2017.\n[33] Sanjoy Dasgupta. Analysis of a greedy active learning strategy. In Advances in neural information processing systems,\npages 337–344, 2005.\n[34] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\nImagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.\n[35] Yifan Ding, Liqiang Wang, Deliang Fan, and Boqing Gong. A semi-supervised two-stage approach to learning from noisy\nlabels. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1215–1224. IEEE, 2018.\n[36] Carl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.\n[37] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction.\nIn Proceedings of the IEEE international conference on computer vision, pages 1422–1430, 2015.\n[38] JeﬀDonahue, Philipp Krähenbühl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782,\n2016.\n[39] WeiWang Dong-DongChen and Zhi-HuaZhou WeiGao.\nTri-net for semi-supervised deep learning.\nIn Proceedings of\nTwenty-Seventh International Joint Conference on Artiﬁcial Intelligence, pages 2014–2020, 2018.\n[40] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with exemplar convolutional neural networks. IEEE transactions on pattern analysis and\nmachine intelligence, 38(9):1734–1747, 2015.\n[41] Thomas Drugman, Janne Pylkkonen, and Reinhard Kneser. Active and semi-supervised learning in asr: Beneﬁts on the\nacoustic and language models. arXiv preprint arXiv:1903.02852, 2019.\n[42] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron Courville.\nAdversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.\n37\n[43] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. arXiv preprint\narXiv:1808.09381, 2018.\n[44] M Ehsan Abbasnejad, Anthony Dick, and Anton van den Hengel. Inﬁnite variational autoencoder for semi-supervised\nlearning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5888–5897, 2017.\n[45] Dennis Forster, Abdul-Saboor Sheikh, and Jörg Lücke.\nNeural simpletrons: Learning in the limit of few labels with\ndirected generative networks. Neural computation, 30(8):2113–2174, 2018.\n[46] Benoît Frénay and Michel Verleysen. Classiﬁcation in the presence of label noise: a survey. IEEE transactions on neural\nnetworks and learning systems, 25(5):845–869, 2013.\n[47] Geoﬀrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation. arXiv preprint\narXiv:1706.05208, 2017.\n[48] Yasuhiro Fujiwara and Go Irie. Eﬃcient label propagation. In International Conference on Machine Learning, pages\n784–792, 2014.\n[49] Soumyajit Ganguly, Manish Gupta, Vasudeva Varma, Vikram Pudi, et al. Author2vec: Learning author representations\nby combining content and link information. In Proceedings of the 25th International Conference Companion on World\nWide Web, pages 49–50. International World Wide Web Conferences Steering Committee, 2016.\n[50] Yaroslav Ganin and Victor Lempitsky.\nUnsupervised domain adaptation by backpropagation.\narXiv preprint\narXiv:1409.7495, 2014.\n[51] Mingfei Gao, Zizhao Zhang, Guo Yu, Sercan O Arik, Larry S Davis, and Tomas Pﬁster. Consistency-based semi-supervised\nactive learning: Towards minimizing labeling cost. arXiv preprint arXiv:1910.07153, 2019.\n[52] Salvador García, Julián Luengo, and Francisco Herrera. Data preprocessing in data mining. Springer, 2015.\n[53] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations.\narXiv preprint arXiv:1803.07728, 2018.\n[54] Chen Gong, Dacheng Tao, Wei Liu, Liu Liu, and Jie Yang. Label propagation via teaching-to-learn and learning-to-teach.\nIEEE transactions on neural networks and learning systems, 28(6):1452–1465, 2016.\n[55] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680,\n2014.\n[56] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.\nExplaining and harnessing adversarial examples.\narXiv\npreprint arXiv:1412.6572, 2014.\n[57] Palash Goyal and Emilio Ferrara. Graph embedding techniques, applications, and performance: A survey. Knowledge-\nBased Systems, 151:78–94, 2018.\n[58] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances in neural infor-\nmation processing systems, pages 529–536, 2005.\n[59] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM\nSIGKDD international conference on Knowledge discovery and data mining, pages 855–864, 2016.\n[60] Michael Gutmann and Aapo Hyvärinen.\nNoise-contrastive estimation: A new estimation principle for unnormalized\nstatistical models. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics,\npages 297–304, 2010.\n[61] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in neural\ninformation processing systems, pages 1024–1034, 2017.\n[62] William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods and applications. arXiv\npreprint arXiv:1709.05584, 2017.\n[63] Steve Hanneke.\nTheoretical foundations of active learning.\nTechnical report, CARNEGIE-MELLON UNIV PITTS-\nBURGH PA MACHINE LEARNING DEPT, 2009.\n[64] Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma.\nDual learning for machine\ntranslation. In Advances in neural information processing systems, pages 820–828, 2016.\n[65] Junxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio Ranzato. Revisiting self-training for neural sequence generation.\narXiv preprint arXiv:1909.13788, 2019.\n[66] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual repre-\nsentation learning. arXiv preprint arXiv:1911.05722, 2019.\n[67] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n38\n[68] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturba-\ntions. arXiv preprint arXiv:1903.12261, 2019.\n[69] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A\nsimple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019.\n[70] Geoﬀrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. science,\n313(5786):504–507, 2006.\n[71] Peter D Hoﬀ, Adrian E Raftery, and Mark S Handcock. Latent space approaches to social network analysis. Journal of\nthe american Statistical association, 97(460):1090–1098, 2002.\n[72] Sheng-Jun Huang, Rong Jin, and Zhi-Hua Zhou. Active learning by querying informative and representative examples.\nIn Advances in neural information processing systems, pages 892–900, 2010.\n[73] Sergey Ioﬀe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate\nshift. arXiv preprint arXiv:1502.03167, 2015.\n[74] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5070–5079, 2019.\n[75] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights\nleads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018.\n[76] Longlong Jing and Yingli Tian.\nSelf-supervised visual feature learning with deep neural networks: A survey.\narXiv\npreprint arXiv:1902.06162, 2019.\n[77] Thorsten Joachims. Transductive inference for text classiﬁcation using support vector machines. In Icml, volume 99,\npages 200–209, 1999.\n[78] Thorsten Joachims.\nTransductive learning via spectral graph partitioning.\nIn Proceedings of the 20th International\nConference on Machine Learning (ICML-03), pages 290–297, 2003.\n[79] Giannis Karamanolakis, Daniel Hsu, and Luis Gravano. Leveraging just a few keywords for ﬁne-grained aspect detection\nthrough weakly supervised co-training. arXiv preprint arXiv:1909.00415, 2019.\n[80] Zhanghan Ke, Daoye Wang, Qiong Yan, Jimmy Ren, and Rynson WH Lau. Dual student: Breaking the limits of the\nteacher in semi-supervised learning. In Proceedings of the IEEE International Conference on Computer Vision, pages\n6728–6736, 2019.\n[81] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular graph convolutions: moving\nbeyond ﬁngerprints. Journal of computer-aided molecular design, 30(8):595–608, 2016.\n[82] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[83] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n[84] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling.\nSemi-supervised learning with deep\ngenerative models. In Advances in neural information processing systems, pages 3581–3589, 2014.\n[85] Thomas N Kipf and Max Welling.\nSemi-supervised classiﬁcation with graph convolutional networks.\narXiv preprint\narXiv:1609.02907, 2016.\n[86] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.\n[87] Kyeongbo Kong, Junggi Lee, Youngchul Kwak, Minsung Kang, Seong Gyun Kim, and Woo-Jin Song. Recycling: Semi-\nsupervised learning with noisy labels in deep neural networks. IEEE Access, 7:66998–67005, 2019.\n[88] Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n[89] Abhishek Kumar and Hal Daumé. A co-training approach for multi-view spectral clustering. In Proceedings of the 28th\ninternational conference on machine learning (ICML-11), pages 393–400, 2011.\n[90] Abhishek Kumar, Prasanna Sattigeri, and Tom Fletcher. Semi-supervised learning with gans: Manifold invariance with\nimproved inference. In Advances in Neural Information Processing Systems, pages 5534–5544, 2017.\n[91] Abhishek Kumar, Prasanna Sattigeri, Kahini Wadhawan, Leonid Karlinsky, Rogerio Feris, Bill Freeman, and Gregory\nWornell. Co-regularized alignment for unsupervised domain adaptation. In Advances in Neural Information Processing\nSystems, pages 9345–9356, 2018.\n[92] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.\n[93] Dong-Hyun Lee. Pseudo-label: The simple and eﬃcient semi-supervised learning method for deep neural networks. In\nWorkshop on challenges in representation learning, ICML, volume 3, page 2, 2013.\n[94] Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, and Sungroh Yoon.\nFicklenet: Weakly and semi-supervised\nsemantic image segmentation\\\\using stochastic inference. arXiv preprint arXiv:1902.10421, 2019.\n39\n[95] Seungmin Lee, Dongwan Kim, Namil Kim, and Seong-Gyun Jeong. Drop to adapt: Learning discriminative features\nfor unsupervised domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages\n91–100, 2019.\n[96] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning.\narXiv preprint arXiv:2002.07394, 2020.\n[97] Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Tell me where to look: Guided attention inference\nnetwork. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Jun 2018.\n[98] Xinzhe Li, Qianru Sun, Yaoyao Liu, Qin Zhou, Shibao Zheng, Tat-Seng Chua, and Bernt Schiele. Learning to self-train\nfor semi-supervised few-shot classiﬁcation. In Advances in Neural Information Processing Systems, pages 10276–10286,\n2019.\n[99] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017.\n[100] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun. Scribblesup: Scribble-supervised convolutional networks for\nsemantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n3159–3167, 2016.\n[101] Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudanthi Wijewickrema, and\nJames Bailey. Dimensionality-driven learning with noisy labels. arXiv preprint arXiv:1806.02612, 2018.\n[102] Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxiliary deep generative models. arXiv\npreprint arXiv:1602.05473, 2016.\n[103] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word\nvectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics:\nHuman language technologies-volume 1, pages 142–150. Association for Computational Linguistics, 2011.\n[104] Christoph Mayer, Matthieu Paul, and Radu Timofte.\nAdversarial feature distribution alignment for semi-supervised\nlearning. arXiv preprint arXiv:1912.10428, 2019.\n[105] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and JeﬀDean. Distributed representations of words and\nphrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n[106] Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction without labeled data.\nIn Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference\non Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational\nLinguistics, 2009.\n[107] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial\nnetworks. arXiv preprint arXiv:1802.05957, 2018.\n[108] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method\nfor supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–\n1993, 2018.\n[109] Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13(6):47–60, 1996.\n[110] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural\nimages with unsupervised feature learning. 2011.\n[111] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European\nConference on Computer Vision, pages 69–84. Springer, 2016.\n[112] Augustus Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint arXiv:1606.01583, 2016.\n[113] Avital Oliver, Augustus Odena, Colin A Raﬀel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep\nsemi-supervised learning algorithms. In Advances in Neural Information Processing Systems, pages 3235–3246, 2018.\n[114] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018.\n[115] Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. Asymmetric transitivity preserving graph embedding.\nIn Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages\n1105–1114, 2016.\n[116] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering,\n22(10):1345–1359, 2009.\n[117] Sungrae Park, JunKeon Park, Su-Jin Shin, and Il-Chul Moon. Adversarial dropout for supervised and semi-supervised\nlearning. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[118] Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Visual domain adaptation: A survey of recent\nadvances. IEEE signal processing magazine, 32(3):53–69, 2015.\n40\n[119] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of\nthe 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701–710, 2014.\n[120] Hieu Pham, Qizhe Xie, Zihang Dai, and Quoc V Le. Meta pseudo labels. arXiv preprint arXiv:2003.10580, 2020.\n[121] Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and Alan Yuille. Deep co-training for semi-supervised image recogni-\ntion. In Proceedings of the european conference on computer vision (eccv), pages 135–152, 2018.\n[122] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine\nlearning. The MIT Press, 2009.\n[123] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative\nadversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n[124] Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In International Conference on Machine\nLearning, pages 324–333, 2016.\n[125] Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised learning with ladder\nnetworks. In Advances in neural information processing systems, pages 3546–3554, 2015.\n[126] Alex Ratner, P Varma, B Hancock, and Chris Ré. Weak supervision: A new programming paradigm for machine learning\n(2019).\n[127] Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training\ndeep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.\n[128] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference\nin deep generative models. arXiv preprint arXiv:1401.4082, 2014.\n[129] Phill Kyu Rhee, Enkhbayar Erdenee, Shin Dong Kyun, Minhaz Uddin Ahmed, and Songguo Jin.\nActive and semi-\nsupervised learning for object detection with imperfect data. Cognitive Systems Research, 45:109–123, 2017.\n[130] Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent classiﬁer. In\nAdvances in neural information processing systems, pages 2294–2302, 2011.\n[131] Ellen Riloﬀ. Automatically generating extraction patterns from untagged text. In Proceedings of the national conference\non artiﬁcial intelligence, pages 1044–1049, 1996.\n[132] Ellen Riloﬀand Janyce Wiebe.\nLearning extraction patterns for subjective expressions.\nIn Proceedings of the 2003\nconference on Empirical methods in natural language processing, pages 105–112, 2003.\n[133] Sebastian Ruder and Barbara Plank. Strong baselines for neural semi-supervised learning under domain shift. arXiv\npreprint arXiv:1804.09530, 2018.\n[134] Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training for unsupervised domain adaptation. In\nProceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2988–2997. JMLR. org, 2017.\n[135] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for\ntraining gans. In Advances in neural information processing systems, pages 2234–2242, 2016.\n[136] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized\nlogistic mixture likelihood and other modiﬁcations. arXiv preprint arXiv:1701.05517, 2017.\n[137] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network\nmodel. IEEE Transactions on Neural Networks, 20(1):61–80, 2008.\n[138] H Scudder.\nProbability of error of some adaptive pattern-recognition machines.\nIEEE Transactions on Information\nTheory, 11(3):363–371, 1965.\n[139] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual\ndata. arXiv preprint arXiv:1511.06709, 2015.\n[140] Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison Department of Com-\nputer Sciences, 2009.\n[141] Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng MaXiaoyu Tao, and Nanning Zheng.\nTransductive semi-supervised\ndeep learning using min-max features. In Proceedings of the European Conference on Computer Vision (ECCV), pages\n299–315, 2018.\n[142] Rui Shu, Hung H Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain adaptation. arXiv\npreprint arXiv:1802.08735, 2018.\n[143] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han\nZhang, and Colin Raﬀel. Fixmatch: Simplifying semi-supervised learning with consistency and conﬁdence. arXiv preprint\narXiv:2001.07685, 2020.\n[144] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. Box-driven class-wise region masking and ﬁlling rate guided\nloss for weakly supervised semantic segmentation, 2019.\n41\n[145] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks.\narXiv preprint arXiv:1511.06390, 2015.\n[146] Mingxing Tan and Quoc V Le. Eﬃcientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint\narXiv:1905.11946, 2019.\n[147] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network\nembedding. In Proceedings of the 24th international conference on world wide web, pages 1067–1077, 2015.\n[148] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve\nsemi-supervised deep learning results. In Advances in neural information processing systems, pages 1195–1204, 2017.\n[149] Sunil Thulasidasan, Tanmoy Bhattacharya, JeﬀBilmes, Gopinath Chennupati, and Jamal Mohd-Yusof. Combating label\nnoise in deep learning using abstention. arXiv preprint arXiv:1905.10964, 2019.\n[150] Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. Eﬃcient object localization using\nconvolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n648–656, 2015.\n[151] Dustin Tran, Rajesh Ranganath, and David M Blei.\nDeep and hierarchical implicit models.\narXiv preprint\narXiv:1702.08896, 7:3, 2017.\n[152] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. It takes (only) two: Adversarial generator-encoder networks.\nIn Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n[153] Harri Valpola. From neural pca to deep unsupervised learning. In Advances in independent component analysis and\nlearning machines, pages 143–171. Elsevier, 2015.\n[154] Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie. Learning from noisy large-scale\ndatasets with minimal supervision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 839–847, 2017.\n[155] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention\nnetworks. arXiv preprint arXiv:1710.10903, 2017.\n[156] Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation consistency training for\nsemi-supervised learning. arXiv preprint arXiv:1903.03825, 2019.\n[157] Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416, 2007.\n[158] Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD\ninternational conference on Knowledge discovery and data mining, pages 1225–1234, 2016.\n[159] Xiang Wang, Shaodi You, Xi Li, and Huimin Ma.\nWeakly-Supervised Semantic Segmentation by Iteratively Mining\nCommon Object Features. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition, pages 1354–1362, 2018.\n[160] Zhengxia Wang, Xiaofeng Zhu, Ehsan Adeli, Yingying Zhu, Chen Zu, Feiping Nie, Dinggang Shen, and Guorong Wu.\nProgressive graph-based transductive learning for multi-modal classiﬁcation of brain disorder disease. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 291–299. Springer, 2016.\n[161] Yunchao Wei, Huaxin Xiao, Honghui Shi, Zequn Jie, Jiashi Feng, and Thomas S. Huang. Revisiting dilated convolution:\nA simple approach for weakly- and semi-supervised semantic segmentation. 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, Jun 2018.\n[162] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of Big data, 3(1):9, 2016.\n[163] Lilian Weng. Self-supervised representation learning. lilianweng.github.io/lil-log, 2019.\n[164] Max Whitney and Anoop Sarkar. Bootstrapping via graph propagation. In Proceedings of the 50th Annual Meeting of\nthe Association for Computational Linguistics: Long Papers-Volume 1, pages 620–628. Association for Computational\nLinguistics, 2012.\n[165] Derry Wijaya, Partha Pratim Talukdar, and Tom Mitchell. Pidgin: ontology alignment using web text as interlingua. In\nProceedings of the 22nd ACM international conference on Information & Knowledge Management, pages 589–598, 2013.\n[166] Garrett Wilson and Diane J Cook. A survey of unsupervised deep domain adaptation. arXiv preprint arXiv:1812.02849,\n2018.\n[167] Si Wu, Guangchang Deng, Jichang Li, Rui Li, Zhiwen Yu, and Hau-San Wong. Enhancing triplegan for semi-supervised\nconditional instance synthesis and classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 10091–10100, 2019.\n[168] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on\ngraph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 2020.\n[169] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation for consis-\ntency training. 2019.\n42\n[170] Qizhe Xie, Eduard Hovy, Minh-Thang Luong, and Quoc V Le.\nSelf-training with noisy student improves imagenet\nclassiﬁcation. arXiv preprint arXiv:1911.04252, 2019.\n[171] I Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised learning for\nimage classiﬁcation. arXiv preprint arXiv:1905.00546, 2019.\n[172] Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings.\narXiv preprint arXiv:1603.08861, 2016.\n[173] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd annual meeting of the\nassociation for computational linguistics, pages 189–196, 1995.\n[174] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 7017–7025, 2019.\n[175] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi-supervised learning. In\nProceedings of the IEEE international conference on computer vision, pages 1476–1485, 2019.\n[176] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena.\nSelf-attention generative adversarial networks.\narXiv preprint arXiv:1805.08318, 2018.\n[177] Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee.\nConsistency regularization for generative adversarial\nnetworks. arXiv preprint arXiv:1910.12027, 2019.\n[178] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization.\narXiv preprint arXiv:1710.09412, 2017.\n[179] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European conference on computer\nvision, pages 649–666. Springer, 2016.\n[180] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classiﬁcation. In Advances\nin neural information processing systems, pages 649–657, 2015.\n[181] Yan-Ming Zhang, Kaizhu Huang, and Cheng-Lin Liu. Fast and robust graph-based transductive learning via minimum\ntree cut. In 2011 IEEE 11th International Conference on Data Mining, pages 952–961. IEEE, 2011.\n[182] Jing Zhao, Xijiong Xie, Xin Xu, and Shiliang Sun. Multi-view learning overview: Recent progress and new challenges.\nInformation Fusion, 38:43–54, 2017.\n[183] Junbo Zhao, Michael Mathieu, and Yann LeCun.\nEnergy-based generative adversarial network.\narXiv preprint\narXiv:1609.03126, 2016.\n[184] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative\nlocalization. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2016.\n[185] Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Schölkopf. Learning with local and global\nconsistency. In Advances in neural information processing systems, pages 321–328, 2004.\n[186] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun.\nGraph neural networks: A review of methods and applications. arXiv preprint arXiv:1812.08434, 2018.\n[187] Yan Zhou and Sally Goldman. Democratic co-learning. In 16th IEEE International Conference on Tools with Artiﬁcial\nIntelligence, pages 594–602. IEEE, 2004.\n[188] Zhi-Hua Zhou. A brief introduction to weakly supervised learning. National Science Review, 5(1):44–53, 2018.\n[189] Zhi-Hua Zhou and Ming Li.\nTri-training: Exploiting unlabeled data using three classiﬁers.\nIEEE Transactions on\nknowledge and Data Engineering, 17(11):1529–1541, 2005.\n[190] Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. 2002.\n[191] Xiaojin Zhu, Zoubin Ghahramani, and John D Laﬀerty. Semi-supervised learning using gaussian ﬁelds and harmonic\nfunctions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pages 912–919, 2003.\n[192] Xiaojin Zhu, John Laﬀerty, and Zoubin Ghahramani.\nCombining active learning and semi-supervised learning using\ngaussian ﬁelds and harmonic functions. In ICML 2003 workshop on the continuum from labeled to unlabeled data in\nmachine learning and data mining, volume 3, 2003.\n[193] Xiaojin Jerry Zhu. Semi-supervised learning literature survey. Technical report, University of Wisconsin-Madison De-\npartment of Computer Sciences, 2005.\n43\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-06-09",
  "updated": "2020-07-06"
}