{
  "id": "http://arxiv.org/abs/2006.05579v1",
  "title": "Deep reinforcement learning for optical systems: A case study of mode-locked lasers",
  "authors": [
    "Chang Sun",
    "Eurika Kaiser",
    "Steven L. Brunton",
    "J. Nathan Kutz"
  ],
  "abstract": "We demonstrate that deep reinforcement learning (deep RL) provides a highly\neffective strategy for the control and self-tuning of optical systems. Deep RL\nintegrates the two leading machine learning architectures of deep neural\nnetworks and reinforcement learning to produce robust and stable learning for\ncontrol. Deep RL is ideally suited for optical systems as the tuning and\ncontrol relies on interactions with its environment with a goal-oriented\nobjective to achieve optimal immediate or delayed rewards. This allows the\noptical system to recognize bi-stable structures and navigate, via trajectory\nplanning, to optimally performing solutions, the first such algorithm\ndemonstrated to do so in optical systems. We specifically demonstrate the deep\nRL architecture on a mode-locked laser, where robust self-tuning and control\ncan be established through access of the deep RL agent to its waveplates and\npolarizers. We further integrate transfer learning to help the deep RL agent\nrapidly learn new parameter regimes and generalize its control authority.\nAdditionally, the deep RL learning can be easily integrated with other control\nparadigms to provide a broad framework to control any optical system.",
  "text": "Deep reinforcement learning for optical systems: A case study of mode-locked lasers\nChang Sun‚àó, Eurika Kaiser‚àó‚àó, Steven L. Brunton‚àó‚àó,‚Ä†, and J. Nathan Kutz‚àó,‚Ä†\n‚àóDepartment of Physics, University of Washington, Seattle, WA 98115\n‚àó‚àóDepartment of Mechanical Engineering, University of Washington, Seattle, WA 98115\n‚Ä† Department of Applied Mathematics, University of Washington, Seattle, WA 98115\nWe demonstrate that deep reinforcement learning (deep RL) provides a highly eÔ¨Äective strategy for\nthe control and self-tuning of optical systems. Deep RL integrates the two leading machine learning\narchitectures of deep neural networks and reinforcement learning to produce robust and stable\nlearning for control. Deep RL is ideally suited for optical systems as the tuning and control relies\non interactions with its environment with a goal-oriented objective to achieve optimal immediate\nor delayed rewards. This allows the optical system to recognize bi-stable structures and navigate,\nvia trajectory planning, to optimally performing solutions, the Ô¨Årst such algorithm demonstrated to\ndo so in optical systems. We speciÔ¨Åcally demonstrate the deep RL architecture on a mode-locked\nlaser, where robust self-tuning and control can be established through access of the deep RL agent\nto its waveplates and polarizers. We further integrate transfer learning to help the deep RL agent\nrapidly learn new parameter regimes and generalize its control authority. Additionally, the deep\nRL learning can be easily integrated with other control paradigms to provide a broad framework to\ncontrol any optical system.\nMachine learning (ML) and artiÔ¨Åcial intelligence (AI)\nalgorithms are transforming the scientiÔ¨Åc landscape [1,\n2].\nFrom self-driving cars and autonomous vehicles to\ndigital twins and manufacturing, there are few scientiÔ¨Åc\nand engineering disciplines that have not been profoundly\nimpacted by the rise of ML/AI methods. Optics is no\nexception, with a signiÔ¨Åcant growth of ML/AI methods\ndeveloped for applications ranging from imaging to op-\ntical communications [3, 4]. For control applications, a\nvariety of ML strategies have been developed for stabi-\nlizing optical systems such as mode-locked lasers [5‚Äì12].\nFrom genetic algorithms to deep neural networks, these\nstudies provide a broad perspective on how a diverse set\nof optimization algorithms can be used to automate the\ncontrol and self-tuning of a given optical device. How-\never, one of the most successful ML architectures has yet\nto be implemented for mode-locked lasers: reinforcement\nlearning (RL) [13]. RL is a rapidly growing branch of\nML/AI that is based upon goal-oriented algorithms in\nwhich an agent learns from interactions with the envi-\nronment. It is the algorithmic basis for the popular AI\nsuccess stories on games like chess and Go [14]. Given\nits leading status as a control and goal-oriented strategy,\nwe show that RL can be integrated with optical systems,\nspeciÔ¨Åcally mode-locked lasers, to produce an architec-\nture for intelligent and stable self-tuning operation.\nThe power of RL lies in its ability to learn from interac-\ntions with the environment with goal-oriented objectives.\nThis is unlike the two other dominant ML paradigms of\nsupervised and unsupervised learning [1, 2]. With a trial-\nand-error search, a RL agent learns to sense the state of\nits environment and take actions accordingly to achieve\noptimal immediate or delayed rewards. SpeciÔ¨Åcally, the\nRL agent arrives at diÔ¨Äerent states by performing actions,\nwith the selected actions leading to positive or negative\nrewards for learning. Importantly, the RL agent is capa-\nble of learning delayed rewards, which is critical for many\noptical systems since a trajectory to the optimal solution\nmust be learned. This is equivalent to mapping out a\nset of moves, or long term strategy, to win a chess game.\nRL targets optimal policies for reinforcement learners to\nmaximize the total reward across an episode. Each state\nfollows a Markov property by assumption, i.e., each state\nis determined only by the previous state and the transi-\ntion taken to the current state. Thus a large number of\ntrials must be evaluated in order to determine an opti-\nmal policy. This is accomplished in chess and Go by self-\nplay [14], which is exactly what the mode-locked laser is\nallowed to do to learn an optimal policy.\nIn context of mode-locked lasers, the RL agent is given\naccess to the components of the laser typically used for\ngenerating stable operation: the waveplates and polar-\nizer (See Fig. 1). The RL agent then explores the ways\nto maximize the policy information, which is centered\naround stable mode-locking of the laser cavity. Specif-\nically, the highest-energy mode-locked pulse is typically\nsought in the high-dimensional space generated by the\nwaveplates and polarizer. We show that the RL agent\ncan learn to stabilize a mode-locked laser in a robust\nmanner. More than that, it can learn pathways to cir-\ncumvent regions in parameter space where bi-stabilities\nexist. Indeed, the delayed reward structure of the RL\nagent allows the system to learn how to maneuver around\nbi-stabilities in order to achieve optimal mode-locking\nperformance.\nSuch a trajectory cannot be discovered\nwith the variety of ML methods used so far on laser sys-\ntems [5‚Äì12]. The RL framework is especially valuable for\nsystems where enough self-exploration can be promoted\nin order to sample the entirety of parameter space. This\ncan be done with mode-locked lasers so that the RL ar-\nchitecture provides a clear pathway towards technological\nimplementation and more robust turn-key technologies.\narXiv:2006.05579v1  [eess.SP]  10 Jun 2020\n2\nAction network\nLoss function\nTarget network\nDeep reinforcement learning controller\nùõº!\"#$%& = ùõº+ ‚àÜùõº\n‚àÜùõº\nu‚Ä≤, v‚Ä≤, ùõº‚Ä≤\nu, v, ùõº\nGain\nBirefringence K \nFiber\nMode Locked \nFiber Laser\nŒ±' Œ±( Œ±) Œ±*\nReward\nFIG. 1. Schematic of the self-tuning Ô¨Åber laser. The mode-\nlocked Ô¨Åber laser, including the laser cavity and optical com-\nponents, is discussed in detail in the Methods Section B.\nThe deep reinforcement learning controller is discussed in the\nMethods Section C.\n(a)\n(b)\nFIG. 2. The variation of the rewards and loss function during\ntraining shows that the deep reinforcement learning controller\nadapts to improved policies as training proceeds.\nRESULTS\nWe demonstrate the eÔ¨Écacy of deep reinforcement\nlearning control on mode-locked Ô¨Åber lasers in Fig. 1. We\nÔ¨Årst demonstrate the deep RL strategy for a single-input\ncontrol (Œ±1). The deep RL controller is then applied in a\nmulti-input control to Ô¨Ånd the optimal orientation of the\nwaveplates (Œ±1, Œ±2, Œ±3) and polarizer (Œ±p). Finally, the\ncontroller is generalized to Ô¨Ånd optimal solutions with\nvarying values of the Ô¨Åber birefringence, which is an un-\nmeasured latent variable that dictates the performance\nof the laser cavity. RL is shown to be a robust and stable\nway to enact control. The loss function, or optimization\nobjective, is detailed in the Methods section. Previous\nwork [5, 6] has found the loss function to be well mod-\neled by the cavity energy divided by the kurtosis (fourth-\nmoment) of the spectrum.\nSingle-input control for Ô¨Åxed birefringence\nFigure 2 shows the variation of rewards and loss func-\ntion during training process of the deep RL controller for\na single-input, single-output (SISO) case. The quarter-\nwaveplate angle Œ±1 is the control variable, which can be\nvaried in 2‚ó¶steps. The search starts with an initial value\nof Œ±1 = 15‚ó¶, and all other angles are held Ô¨Åxed at pre-\ndetermined, locally maximizing values. The deep rein-\nforcement learning agent takes an action from a state\nusing epsilon-greedy policy and the exploration rate œµ\nexponentially decays during training. We observe an in-\ncrease of the total reward over a complete episode as the\ntraining proceeds, as shown in Fig. 2 (a). Note that the\ndeep RL agent adapts to a new policy when the loss rises\nas shown in Fig. 2 (b).\nExtending the initial values of Œ±1 from 15‚ó¶during\ntraining enables us to train a model that drives the laser\ndynamics to mode-locking with diÔ¨Äerent initial values of\nŒ±1, as shown in Fig.\n3.\nWith Ô¨Åxed birefringence pa-\nrameter K, the deep reinforcement learning controller\ncorrectly interprets and extracts features from the in-\nput states, and takes the action to eÔ¨Éciently drive the\nlaser dynamics to mode-locked solutions with Œ±1 starting\nfrom [‚àí40‚ó¶, ‚àí10‚ó¶]. For example, with the initial value of\nŒ±1 = ‚àí12‚ó¶, the reward of each step continues increasing\nfrom the initial value as the deep RL controller drives\nthe intra-cavity dynamics to mode-locking, as shown in\nFig. 3 (iii). Interestingly, we observe hysteresis in the\ncorresponding change of Œ±1 while the reward is consis-\ntently increasing, as shown in Fig. 4 (a). The deep rein-\nforcement learning controller correctly identiÔ¨Åes the bi-\nstability of the intra-cavity dynamics and arrives eventu-\nally at the globally maximizing solution in this case. No\nother ML architecture to date has been able to identify\nbistability. RL achieves this due to its deferred reward\nstructure which allows it to plan a path around the in-\nstability. Figure 4 (b) describes the system bi-stability\nwith two controllers Œ±1 and Œ±2. Our deep RL agent suc-\ncessfully discovers the path to drive the laser dynamics\nto mode-locking, marked as path (i) in Fig. 4 (b). The\ncorresponding Ô¨Ånal states of the electric Ô¨Åelds u and v are\nalso pictured in Fig. 4 (b), with a high reward r = 0.2062.\nWe compare this path selected by our deep RL agent to\nthe direct connection of the start and end points, which\nis marked as path (ii) in Fig. 4 (b). The corresponding\nÔ¨Ånal states of the electric Ô¨Åelds u and v are also pictured,\nof which we observe constant waveforms (plane waves)\nwith r = 0.0066. Note that in this work we only extend\nthe initial values of Œ±1 to be in the range of [‚àí40‚ó¶, ‚àí10‚ó¶]\nduring training, but the deep RL agent has the ability to\nbe generalized and further expanded to a larger range of\ninitial values of Œ±1.\n3\nRewards r                           ùõº!(deg)\n-36.0\n-40.0\n1                                  10                              20\n-28.0\n-12.0\n-10.0\n0.04          0.08          0.12          0.16          0.20\nSteps in each episode\ni\nii\niii\nùõº!(deg)\nSteps in each episode\nSteps in each episode\nr\n(i)\n(ii)\n(iii)\nFIG. 3. The deep reinforcement learning controller eÔ¨Äectively drives the laser dynamics to mode-locked solutions with Œ±1\nstarting from [‚àí40‚ó¶, ‚àí10‚ó¶]. Left panel demonstrates the change of rewards in each episode starting with diÔ¨Äerent initial values\nof Œ±1. The deep reinforcement learning controller adaptively selects actions to continue with the current waveplate orientation,\nor increase/decrease Œ±1 by 2‚ó¶. Control results for experiments starting with Œ±1 = ‚àí36‚ó¶, ‚àí28‚ó¶, and ‚àí12‚ó¶are shown in detail\nin Ô¨Ågures (i)-(iii). Note that the intra-cavity electric Ô¨Åelds u and v start as hyperbolic secant pulses in each experiment.\nMulti-input control for Ô¨Åxed birefringence\nFigure 5 shows the deep reinforcement learning con-\ntroller for the multiple-input, single-output (MISO) case\nwhere we control all four waveplate orientations simul-\ntaneously. This multi-input control is more complicated\nthan the single-input case as the number of possible ac-\ntions is signiÔ¨Åcantly larger.\nThus transfer learning is\nleveraged to prevent the model from diverging at the\nearly stage of training. The number of controllers is grad-\nually increased until the desired performance is achieved.\nThe search starts with initial values of Œ±1 = 15‚ó¶, Œ±2 =\n‚àí5‚ó¶, Œ±3 = 20‚ó¶, and Œ±p = 84‚ó¶. With Ô¨Åxed Ô¨Åber birefrin-\ngence, the deep reinforcement learning controller takes\nthe correct actions to drive the laser dynamics from con-\nstant waveforms (plane waves) to the mode-locked solu-\ntions, as shown in Fig. 5. After the mode-locked state\nis achieved, the deep reinforcement learning agent con-\ntinues searching through the action space with the re-\nward oscillating near the optimal performance. Because\nthe waveplate orientations are varied simultaneously, the\nlarge action space results in a slow search for the deep\nRL agent, and we Ô¨Ånd it diÔ¨Écult to eliminate such os-\ncillations. Other adaptive controllers, for example, ex-\ntremum seeking control [5], can be combined to stabilize\nand attenuate such oscillations for better performance.\nMulti-input control for varying birefringence\nWe Ô¨Ånd the neural network parameters of the deep RL\ncontroller trained with birefringence K = 0.1 can be gen-\neralized to control the mode-locking dynamics at diÔ¨Äerent\nvalues of K. Except for a few cases, the deep reinforce-\nment learning controller successfully drives the laser dy-\nnamics to mode-locking with diÔ¨Äerent values of birefrin-\ngence K from ‚àí0.2 to 0.4, as shown in Fig. 6 (a). Note\nthat in such generalized cases, the deep RL controller\ntakes more steps on average to drive the laser dynam-\nics to mode-locking, and in some cases the mode-locked\nsolutions achieved are not tightly-conÔ¨Åned as shown pre-\nviously.\nMoreover, among a few cases, the deep rein-\nforcement learning controller successfully achieves mode-\nlocking but gradually wanders afterwards.\nOne feasible solution to improve the control perfor-\nmance in such cases is to retrain the model completely for\ndiÔ¨Äerent values of birefringence K. However, we Ô¨Ånd the\nneural network parameters of our deep RL agent adapt\nwell and quickly with transfer learning [15] to diÔ¨Äerent\nvalues of the birefringence K.\nIn other words, there\nis no need to retrain the model completely with vary-\ning K, but instead we can slightly increase the explo-\nration rate of the current model and collect more ex-\nperiences by interacting with the new environment of\n4\n(a)\nùõº!(deg)\n-10\n-30\n0.200\n0.050\n-15                                                  5\ni\n0.125\n-20        -10          0         10          20\nt\n2.0\n1.0\n0.0\n(i)\n(ii)\nùõº\"(deg)\n2.0\n1.0\n0.0\nr=0.2062 \nr=0.0066\nii\nRewards\nŒ±!\nBistability of laser dynamics\n(a)\n(b)\nFIG. 4. (a) The deep reinforcement learning controller for a\nsingle-input, single-output (SISO) case. The reward r rises\nfrom the initial value as the controller drives the intra-cavity\ndynamics to mode-locking and we observe hysteresis in the\ncorresponding change of Œ±1 while the reward r is consis-\ntently increasing. (b) The deep reinforcement learning con-\ntrol for two controllers Œ±1 and Œ±2.\nStart with Œ±1 = ‚àí15‚ó¶\nand Œ±2 = ‚àí3‚ó¶, the laser dynamics successfully arrives at\nthe mode-locked solution with r = 0.2062 following the path\n(i) selected by the deep reinforcement learning controller,\nwhereas we observe the plane wave solution (r = 0.0066) fol-\nlowing path (ii) as comparison.\nchanged birefringence K.\nThese newly collected expe-\nriences enable the model to quickly update and adapt to\nthe new environment. Thus it provides the possibility of\nbuilding an online model for what is typically a stochas-\ntic and slowly-varying birefringence. In our experiments,\ntraining the neural network parameters with the birefrin-\ngence K = 0.1 takes at least 2000 episodes for the deep\nRL controller to converge to the optimal policy, whereas\nthe transfer learning takes only 300 episodes on average\nto adapt to environments with varying birefringence K.\nWith Ô¨Åned-tuned parameters for K ‚àà{‚àí0.2, 0, 0.2, 0.4},\nwe improve the performance of the deep reinforcement\nlearning controller to drive the laser dynamics to mode-\nlocking with other values of the birefringence K ranging\nfrom ‚àí0.2 to 0.4, as shown in Fig. 6 (b). Note only four\nsets of neural network parameters are used to generalize\nthe controller to a range of K values.\nAs previously noted, we Ô¨Ånd in some cases that the\ndeep RL controller successfully achieves mode-locking,\nbut gradually walks apart from the desired solution. In\nsuch cases we can rely on extremum seeking controller\n[5] or other adaptive controllers for better performance.\nExtremum-seeking control, for example, is a form of\nperturb-and-observe control that estimates the gradient\n0.2\n0.1\n0.0\n-16\n-20\n-24\n-5\n-10\n-15\n28\n24\n20\n84\n81\n78\nRewards\nùõº!(deg)\nùõº\"(deg)\nùõº#(deg)\nùõº$(deg)\nSteps in each episode\n0\n20\n40\n60\nFIG. 5. The deep reinforcement learning controller for the\nmultiple-input, single-output (MISO) case where we are con-\ntrolling all four waveplate orientations simultaneously (K =\n0). The experiment starts with hyperbolic secant pulses u and\nv in cavity, which are promptly attenuated to constant wave-\nforms with initial values of Œ±1 = 15‚ó¶, Œ±2 = ‚àí5‚ó¶, Œ±3 = 20‚ó¶,\nand Œ±p = 84‚ó¶. The four controllers Œ±1, Œ±2, Œ±3, and Œ±4 either\nhold on to the current orientations or increase/decrease by\n0.5‚ó¶in each step.\nof an objective function by injecting an additional si-\nnusoidal signal as input.\nThe signal converges more\nrapidly when the objective function has a large gradient.\nExtremum-seeking control can lock the system to the lo-\ncal maximum and reacts rapidly to moderate changes\nof intra-cavity dynamics [16]. However, it relies on ini-\ntial conditions of the parameters and state of the system\nsince it only Ô¨Ånds local maxima. Moreover, extremum-\nseeking control cannot recover in cases when the sys-\ntem is knocked far from the desired local maximum with\ndrastic perturbations. Therefore, a combination of the\ndeep RL agent with the extremum-seeking controller is\na viable integrative strategy since it can evade the poor\nlocal maximum, and stabilize the intra-cavity dynamics\naround the mode-locked solution. To implement this in-\ntegrated strategy, the deep RL controller is Ô¨Årst executed\nin order to Ô¨Ånd a good mode-locked solution in a rapid\nmanner.\nIndeed, deep RL can Ô¨Ånd near optimal sta-\nble mode-locking with several steps of propagation. The\nextremum-seeking controller is then turned on to stabi-\nlize the system. The schematic is shown in Fig. 7.\n5\nSteps in each episode\n1                                             30                                         60\n-0.2\nK\n0.0\n0.2\n0.4\n0.20\n0.16\n0.12\n0.08\n0.04\n-0.2\nK\n0.0\n0.2\n0.4\n(a)\n(b)\nFIG. 6. (a) Neural network parameters of the deep RL con-\ntroller trained with birefringence K = 0.1 can be generalized\nto environments with diÔ¨Äerent values of K. (b) With transfer\nlearning, neural network parameters of the deep RL controller\ncan be rapidly Ô¨Åne-tuned with a small amount of newly col-\nlected experiences and updated to control eÔ¨Äectively in the\nnew environments with diÔ¨Äerent values of K.\nDISCUSSION\nDeep reinforcement learning is a learning paradigm\nthat integrates the power of reinforcement learning and\ndeep neural networks. It is an ideal ML paradigm for\ncomplex dynamical systems where the learning agent is\nallowed to explore the system and for which trajectory\nplanning is critical for success: both aspects typically\nmanifest in optical systems.\nHere, we demonstrate a\nfast, reliable self-tuning controller for the passive mode-\nlocked Ô¨Åber laser with deep RL. The controller varies\nall four waveplate orientations simultaneously to achieve\na tightly-conÔ¨Åned, high-energy mode-locked state.\nIn-\nterestingly, the control paths selected by the deep rein-\nforcement learning controller reÔ¨Çect the bi-stability of the\nlaser dynamics, and demonstrate the eÔ¨Écacy of the deep\nlearning control to correctly sense the state of the envi-\nronment in a bistable system. Although no new optical\nYes\nùú∂ùíñùíëùíÖùíÇùíïùíÜ= ùú∂+ ‚àÜùú∂\nState u, v\nùõº'()*+,\n‚àÜùõº\nGain\nBirefringence K\nFiber\nMode Locked \nFiber Laser\nGood \nperformance? \n(r>threshold?)\nNo\nReward r\nDeep RL controller\nInput\nExtremum-seeking controller (ESC)\nùëÑ-\n‚Ä¶\nùëÑ.\nùëÑ/\n‚Ä¶\nŒ±! Œ±\" Œ±# Œ±$\nFIG. 7.\nCombining the deep reinforcement learning con-\ntroller with the extremum-seeking controller stabilizes the\nintra-cavity dynamics and achieves mode-locking with varying\nbirefringence K.\nphysics is demonstrated in this work, we have provided\na principled control strategy to escape from the poor lo-\ncal maxima by interacting with the environments, which\nis important for building controllers in systems with bi-\nstabiltiy. Moreover, the deep RL controller architecture\nprovided here can be easily integrated and generalized to\nexperimental environments and other optical systems, in-\ncluding for instance, managing instabilities from disper-\nsion management [17], controlling pulse compression [18],\nand/or circumventing Q-switching instabilities [19]. Im-\nportantly, given a well-deÔ¨Åned reward criteria and state-\nspace, the deep RL architecture generates experiences\nwith its environment in order to train the deep reinforce-\nment learning controller.\nThe deep RL framework demonstrated here can be\ncombined and integrated with other control paradigms,\nfor example, the extremum-seeking controller, for a bet-\nter control performance. Since the deep RL controller\ncontinues searching the entire space even with a good\nmode-locked solution already found, it is diÔ¨Écult to elim-\ninate the oscillation, and in some cases the controller\neventually walks apart from the mode-locked solutions.\nAn integration with the extremum-seeking controller sta-\nbilizes the control performance around the optimal solu-\ntion discovered, even with slowly varying birefringence.\nWith drastic perturbations to the birefringence, our deep\nreinforcement learning controller can be promptly Ô¨Åne-\ntuned to adapt to the new environments using transfer\nlearning. Once a new mode-locked solution is found by\nthe deep RL controller, the extremum-seeking controller\nis turned on instead to stabilize the system. This hybrid\napproach marries the ability of deep RL to search glob-\nally in a large control space with the increased stability\nprovided by extremum-seeking control via local optimiza-\ntion.\n6\nMETHODS\nA.\nReinforcement learning\nAs noted earlier, RL is a branch of machine learning\nthat uses a goal-oriented algorithm that learns from in-\nteractions with its environment. Using a trial-and-error\nsearch, an agent learns to sense the state of its envi-\nronment and take actions accordingly to achieve optimal\nimmediate or delayed rewards. SpeciÔ¨Åcally, the RL agent\narrives at diÔ¨Äerent states by performing actions, with the\nselected actions leading to positive or negative rewards\nfor learning. The agent‚Äôs behaviors are deÔ¨Åned by poli-\ncies of the reinforcement learning algorithms in the envi-\nronments, and we target at the optimal policies for rein-\nforcement learners to maximize total rewards across an\nepisode (or trajectories generated by tuning the optical\nsystem).\nQ-learning\nWe leverage deep Q-learning, speciÔ¨Åcally the deep oÔ¨Ä-\npolicy temporal diÔ¨Äerence control algorithm [20, 21],\nwhich approximates the current estimations based on the\npreviously learned estimations. In reinforcement learn-\ning algorithms, the state-action value function, or Q-\nfunction, is deÔ¨Åned as the expected discounted return\nof rewards starting from the state s with the action a ac-\ncording to policy œÄ. The Q-function speciÔ¨Åes the agent‚Äôs\nperformance taking a particular action and transit from\nthe current state to the next with the policy we choose.\nDuring training the reinforcement learning agent learns\nand converges to the optimal policy that maximizes the\ntotal reward across an episode.\nQ-learning [22] is a particular approach to learn opti-\nmal actions in such sequential decision problems and has\nbeen recognized as a form of temporal diÔ¨Äerence learn-\ning [23]. Suppose we take action a in the current state s\nand arrive at state s‚Ä≤, Q-learning obtains\nQ(s, a) = r(s, a) + Œ≥ max\na‚Ä≤ Q(s‚Ä≤, a‚Ä≤),\n(1)\nwhere r(s, a) is the reward collected performing action a\nto move from state s to s‚Ä≤, Œ≥ ‚àà[0, 1] is the discount factor\nthat controls the contribution of the rewards collected in\nthe future to the total reward after the episode is Ô¨Ån-\nished. During an episode, the agent proceeds by either\nchoosing the action with the highest Q value (exploita-\ntion), or selecting randomly an action to explore other\npossible states which may return higher delayed rewards\n(exploration). The agent moves forward to the next state\ns‚Ä≤ with the selected action a and collects the associated\nreward r. Q-learning updates the current Q value of the\nexperienced state-action pair with the collected reward\nafter transitioning to s‚Ä≤ and the possible future rewards\ntaking the optimal action thereafter:\nQnew(s, a) = Q(s, a) + Œ±(r + Œ≥ max\na‚Ä≤\nQ(s‚Ä≤, a‚Ä≤) ‚àíQ(s, a)), (2)\nwhere Œ± ‚àà[0, 1] is the learning rate. Note that the diÔ¨Äer-\nence between the actual reward r + Œ≥ max Q(s‚Ä≤, a‚Ä≤) and\nthe expected reward Q(s, a) is taken to update the value\nof Q(s, a). The parameter Œ± is important for convergence\nsince it determines to what extent the current Q function\nis updated by the newly explored information. The Q\nfunction is arbitrarily initialized and updated following\nEq. (2) until the Q-learning algorithm has converged.\nDeep Q neural networks (DQN)\nIn discrete environments represented by a Ô¨Ånite num-\nber of possible states and actions, we often search\nthrough all possible state-action pairs exhaustively to\nÔ¨Ånd the optimal Q(s, a) values and the associated policy.\nHowever, this is computationally expensive and becomes\ninfeasible with more than a small number of state-actions\npairs.\nIn continuous environments, it is impossible to\nlist and search through each state with diÔ¨Äerent actions.\nIn contrast, deep Q learning [21] allows one to approxi-\nmate the tabular Q function Q(s, a) as a parameterized\nfunction Q(s, a; Œ∏). Considering that neural networks can\nprovide good approximations to possibly very complex\nfunctions, we utilize here deep neural networks as the es-\ntimator of the Q value function. In particular, Q(s, a; Œ∏)\nis modeled as a multi-layered neural network with pa-\nrameters Œ∏ that takes a given state s as input and yields\na vector of values Q(s, ¬∑; Œ∏), each associated with a par-\nticular action a.\nFollowing the Q-learning updating rule deÔ¨Åned in Eq.\n2, we refer to r + Œ≥ max Q(s‚Ä≤, a‚Ä≤; Œ∏) as the target value,\nQ(s, a; Œ∏) as the predicted value, and the diÔ¨Äerence be-\ntween the target and prediction is minimized when the\ncurrent policy converges to the optimum.\nIn deep Q\nlearning, we can deÔ¨Åne analogously the loss function as\nthe squared diÔ¨Äerence between the target and predicted\nvalue:\nL = (r + Œ≥ max\na‚Ä≤ Q(s‚Ä≤, a‚Ä≤; Œ∏) ‚àíQ(s, a; Œ∏))2.\n(3)\nThe loss is minimized by learning updates to the deep\nneural network parameters Œ∏ that converge to the opti-\nmal policy. In summary, we use neural networks for the\napproximation of the Q function in deep Q learning, and\nconverge to the optimal policy by minimizing the loss.\nIn particular, we employ as deep reinforcement learn-\ning agent an adaptation of the double deep Q neural\nnetwork (DDQN) [24, 25] to the self-tuning laser con-\ntrol problem. The architecture of DDQNs is shown in\nFig. 8, where the inputs fed into the action network de-\nscribe the current state that the deep RL agent is in, and\nthe output of the action network is the approximated\nQ function, speciÔ¨Åcally the Q values for all possible ac-\ntions given the current state. Following the loss function\ndeÔ¨Åned in Eq. (3), we would observe strong divergence\nduring training since the same neural network with pa-\nrameters Œ∏ calculates both the predicted value and target\n7\nAction network\nAction a\nSoft update of \nparameters\nInput ùê¨\nùëÑ!\n‚Ä¶\nùëÑ\"\nùëÑ#\n‚Ä¶\nTarget network\nInput ùê¨‚Ä≤\nTarget   \nPrediction\nùëÑ!\n$\n‚Ä¶\nùëÑ\"\n$\nùëÑ#$\n‚Ä¶\nùêãùê®ùê¨ùê¨= (ùê´+ ùõÑùê¶ùêöùê±ùêö%ùêê(ùê¨\", ùêö\"; ùõâùê¢\n\") ‚àíùêê(ùê¨, ùêö; ùõâùê¢))ùüê\nFIG. 8. The architecture of the double deep Q neural network.\nA target network is included to stabilize the training. More\ndetails are discussed in Section A.\nvalue. To diminish the divergence, two separate networks\nare employed, one for selecting an action and the other\nfor evaluating the selected action. SpeciÔ¨Åcally, the target\nnetwork with parameters Œ∏‚Ä≤ is used to calculate the target\nvalue, while the action network with parameters Œ∏ yields\nthe predicted Q values associated with each action. The\nnew loss function is deÔ¨Åned as:\nL = (r + Œ≥ max\na‚Ä≤ Q(s‚Ä≤, a‚Ä≤; Œ∏‚Ä≤) ‚àíQ(s, a; Œ∏))2,\n(4)\nwhere Œ∏‚Ä≤ and Œ∏ stand for the diÔ¨Äerent set of parameters\nof the target network and the action network, respec-\ntively. The parameters of the target network are peri-\nodically frozen for several episode during training before\nbeing updated by copying the parameters from the action\nnetwork, or partially updated with parameters from the\naction network in each episode to stabilize the training.\nTo stabilize the training and reduce the overÔ¨Åtting\ncaused by correlation between the deep RL agent‚Äôs ex-\nperiences, we train the DDQN with an experience replay\nbuÔ¨Äer [26], which is usually deÔ¨Åned as a queue that saves\na Ô¨Åxed number of the recent experiences. The experi-\nence < s, a, r, s‚Ä≤ > of the deep RL agent is deÔ¨Åned as the\nconcatenation of the current state s, the action selected\na, the next state s‚Ä≤ after performing the action, and the\nassociated reward r received in this transition. During\ntraining, rather than directly train with the newest ex-\nperiences collected, we sample a random batch of the\nexperiences < s, a, r, s‚Ä≤ > from the replay buÔ¨Äer and feed\nthe sampled batch to the neural network for parameter\nupdates.\nThe deep RL agent beneÔ¨Åts from the replay\nbuÔ¨Äers by learning from an enlarged range of random\nand less correlated experiences.\nB.\nMode-locked Ô¨Åber laser model\nOur model of the laser cavity is a well-established com-\nputational model which treats the cavity in a component\nby component manner by separately applying the nonlin-\near optical propagation to the laser dynamics with dis-\ncrete waveplates and polarizer in each round trip. This\nmodel produces a rich set of dynamics that we wish to\ncontrol [27]. We model the propagation of intra-cavity\nÔ¨Åelds with the coupled nonlinear Schr¬®odinger equation\nwith modiÔ¨Åcations to account for the bandwidth limited\ngain and cavity losses [28‚Äì30]:\ni‚àÇu\n‚àÇz + D\n2\n‚àÇ2u\n‚àÇt2 ‚àíKu+(|u|2+A|v|2)u+Bv2u‚àó= iRu, (5a)\ni‚àÇv\n‚àÇz + D\n2\n‚àÇ2v\n‚àÇt2 +Kv+(A|u|2+|v|2)v+Bu2v‚àó= iRv, (5b)\nwhere u(z, t) and v(z, t) are often referred to as the fast\nand slow components of the two intra-cavity electric Ô¨Åeld\nenvelopes, which are orthogonally polarized. The prop-\nagation distance z is non-dimensionalized by the cavity\nlength, and the dimensionless time t is normalized by\nthe full width at half maximum of the pulse. D is the\naverage group velocity dispersion, A and B, determined\nby physical properties of the laser Ô¨Åber, are the nonlin-\near coupling parameters characterizing the cross-phase\nmodulation and the four-wave mixing, respectively. In\nthis work we consider a silica Ô¨Åber with A = 2/3 and\nB = 1/3. The Ô¨Åber birefringence, quantiÔ¨Åed by K, rep-\nresents a major disturbance to the laser dynamics due\nto its sensitivity to thermal Ô¨Çuctuations. The dissipative\nterm R, characterizing the bandwidth-limited gain and\nattenuation arising from the Yb-doped ampliÔ¨Åcation, is\ndeÔ¨Åned as\nR =\n2g0(1 + œÑ‚àÇ2\nt )\n1 + (1/e0)\nR ‚àû\n‚àí‚àû(|u|2 + |v|2)dt ‚àíŒì,\n(6)\nwhere g0 is the dimensionless pumping strength, and e0 is\nthe dimensionless saturation energy of the gain medium.\nLosses caused by output coupling and Ô¨Åber attenuation\nare characterized by the pump bandwidth œÑ and Œì.\nThe eÔ¨Äect of the waveplates and polarizer during each\nround trip is modeled by the discrete application of Jones\nmatrices:\nWŒª/4 =\n\u0014\ne‚àíiœÄ/4\n0\n0\neiœÄ/4\n\u0015\n,\n(7.1)\nWŒª/2 =\n\u0014\n‚àíi 0\n0\ni\n\u0015\n, Wp =\n\u0014\n1 0\n0 0\n\u0015\n.\n(7.2)\nNote that WŒª/4 characterizes the eÔ¨Äects of quarter-\nwaveplates Œ±1 and Œ±2, WŒª/2 is for the half-waveplate Œ±3,\nand Wp is for the polarizer Œ±p. An additional rotation\nmatrix R(Œ±) is necessary to account for the oÔ¨Äset be-\ntween the direction of the intra-cavity fast Ô¨Åeld and the\nprincipal axes of the waveplates and polarizer, and we\ndeÔ¨Åne\nJj = R(Œ±j)WjR(‚àíŒ±j),\n(8.1)\nR(Œ±j) =\n\u0014\ncos(Œ±j) ‚àísin(Œ±j)\nsin(Œ±j)\ncos(Œ±j)\n\u0015\n,\n(8.2)\n8\nEnvironment\nGain\nBirefringence K \nFiber\nAgent\n(deep RL \ncontroller)\nŒ±!\nŒ±\"\nŒ±#\nŒ±$\nAction = (Œ±!, Œ±\", Œ±#, Œ±$)\nReward = E/M\nMode Locked \nFiber Laser\n++\n|v|\n|u|\nŒ±! Œ±\" Œ±# Œ±$\nState = (Œ±!, Œ±\", Œ±#, Œ±$, u, v)\nFIG. 9. Schematic of the self-tuning mode-locked laser with deep reinforcement learning control. The input to the deep RL\ncontroller describes the current state that the controller is in, and is deÔ¨Åned as the concatenated electric Ô¨Åelds u, v and waveplate\norientations Œ±1, Œ±2, Œ±3, Œ±p. The control inputs to the laser cavity are then updated by the selected action of the deep RL\ncontroller, which result in changes of the laser cavity dynamics and returns new electric Ô¨Åelds u, v, and reward r as deÔ¨Åned in\nEq. 9 to the deep RL agent. Given the updated inputs and the associated reward r, the deep RL controller adjusts its strategy\naccordingly to select the next action and optimize the control inputs to the laser cavity.\nwhere Œ±j (j = 1, 2, 3, p) is a waveplate or polarizer an-\ngle.\nThese rotation angles are easily manipulated via\nelectronic control [31], and are considered as the con-\ntrol variables of the deep reinforcement learning agent\nfor driving the laser dynamics to mode-locking in this\nwork.\nC.\nDeep reinforcement learning control\nA schematic of the self-tuning mode-locked laser with\ndeep reinforcement learning control is shown in Fig. 9,\nhighlighted with a deep RL controller and a mode locked\nÔ¨Åber laser cavity of passive nonlinear polarization rota-\ntion (NPR). The mode-locking laser cavity, which is dis-\ncussed in details in the previous section, is interpreted as\nthe interactive environment in the reinforcement learn-\ning framework, and the waveplate angles Œ±1, Œ±2, Œ±3 and\npolarizer angle Œ±p are considered as the controllable ac-\ntions of the deep reinforcement learning agent. We take\nconcatenated components of the electric Ô¨Åelds u, v, and\nthe current waveplate orientations Œ±1, Œ±2, Œ±3 and Œ±p as\nthe input to the deep reinforcement learning agent. The\ndeep reinforcement learning controller is built with al-\nternatively stacked convolutional layers and max pool-\ning layers, followed by fully connected layers with leaky-\nReLU as activation functions. The convolutional layers\nextract features from the input state by identifying the\nsolitons inside the electric Ô¨Åelds u and v, and the max\npooling layers detect existence of the solitons and reduce\nthe input dimensionality before feeding into the fully con-\nnected layers. Note that we demonstrate in this work the\neÔ¨Écacy of the deep RL architecture in a numerical sim-\nulation of the laser cavity, but it is possible to train the\ndeep RL controller directly in an experiment, as the con-\ntroller only relies on information that is readily available\nin experiments.\nThe performance of the deep reinforcement learning\ncontroller is evaluated in terms of a reward r. In particu-\nlar, we seek to steer the system to high-energy mode-\nlocked states.\nHowever, the reward/cost landscape is\nvery complex and exhibits many local optima.\nIn ad-\ndition, evaluating energy is not suÔ¨Écient, as there are\nmany chaotic solutions which have signiÔ¨Åcantly higher\nenergy than mode-locked states [5]. To deÔ¨Åne the reward\nr, we consider including the fourth-moment (kurtosis) M\nof the Fourier spectrum of the waveform, which is large\nfor chaotic solutions but relatively small for the desired\nmode-locked states. To have a large reward r only for\ntightly conÔ¨Åned temporal wave packets with relatively\nlarge energy, we deÔ¨Åne [5]\nr = E/M.\n(9)\nTo penalize the ineÔ¨Äective actions more eÔ¨Éciently during\ntraining, we rescale the reward to be centered around\nzero, so that the desired actions result in positive rewards\nwhile the ineÔ¨Äective ones return negative rewards. We\nrescale it back as deÔ¨Åned in Eq. (9) after training for\n9\nconsistency and interpretability.\nOur deep RL agent uses an œµ-greedy policy to balance\nbetween exploration and exploitation, and parameters Œ∏‚Ä≤\nof the target network are partially updated in each train-\ning step to improve stability.\nNote that the deep RL\nagent spans a large action space in the multiple-input\nsingle-output (MISO) case, when the three waveplates\nand polarizer orientations Œ±1, Œ±2, Œ±3, and Œ±p are consid-\nered as controllers. Thus we observe convergence diÔ¨É-\nculty in training the model directly with randomly ini-\ntialized neural network parameters.\nTo deal with this\nproblem, we start training our deep RL agent with a\nsingle controller Œ±1, and gradually increase the number\nof controllers by initializing with previously trained pa-\nrameters for the current model of increased number of\ncontrollers. Such parameter initialization strategy eÔ¨É-\nciently prevents the model from diverging, especially at\nthe early stages of training.\nThe deep RL agent takes as input the electric Ô¨Åelds\nand waveplate orientations, and the output yields Q val-\nues associated with all possible actions given the current\nstate that the deep RL agent is in. Following the œµ-greedy\npolicy, our RL agent either randomly selects an action for\nexploration, or greedily selects the action with the high-\nest Q value for exploitation and moves to the next state\naccordingly. SpeciÔ¨Åcally, parameters Œ±1, Œ±2, Œ±3, and Œ±p\nare adjusted according to the action selected, and conse-\nquently we observe changes of electric Ô¨Åelds u and v in\nthe Ô¨Åber laser cavity. The reward r of this transition, as\ndeÔ¨Åned in Eq. 9, is taken of the new intra-cavity Ô¨Åelds\nu and v after transition. The procedure is repeated un-\ntil the completion of the entire episode, and then a new\nepisode is started with the same initial conditions of the\nelectric Ô¨Åelds and waveplate/polarizer orientations to col-\nlect more samples. Thus, the deep RL agent learns from\ndiÔ¨Äerent trials using exploration and exploitation, and\neventually converges to the optimal policy that leads to\nthe highest total reward across the entire episode. Note\nthat after the training stage, the learned policy of the\ndeep RL agent is evaluated by greedily selecting the ac-\ntion with the highest Q value.\nACKNOWLEDGEMENTS\nSLB acknowledges support from the Army Research\nOÔ¨Éce (ARO W911NF-19-1-0045;\nProgram Manager\nMatthew Munson). SLB and EK acknowledge support\nfrom the Army Research OÔ¨Éce (ARO W911NF-17-1-\n0306; Program Managers Matthew Munson and Samuel\nStanton). SLB, EK, and JNK acknowledge support from\nthe UW Engineering Data Science Institute, NSF HDR\naward #1934292. JNK acknowledges support from the\nAir Force OÔ¨Éce of ScientiÔ¨Åc Research (FA9550-17-1-\n0200).\n[1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville.\nDeep learning. MIT press, 2016.\n[2] Steven L Brunton and J Nathan Kutz. Data-driven sci-\nence and engineering: Machine learning, dynamical sys-\ntems, and control. Cambridge University Press, 2019.\n[3] Darko Zibar, Henk Wymeersch, and Ilya Lyubomirsky.\nMachine learning under the spotlight. Nature Photonics,\n11(12):749‚Äì751, 2017.\n[4] Rachel Won. Intelligent learning with light. Nature Pho-\ntonics, 12(10):571‚Äì573, 2018.\n[5] Steven L Brunton,\nXing Fu,\nand J Nathan Kutz.\nExtremum-seeking control of a mode-locked laser. IEEE\nJournal of Quantum Electronics, 49(10):852‚Äì861, 2013.\n[6] Steven L Brunton, Xing Fu, and J Nathan Kutz. Self-\ntuning Ô¨Åber lasers. IEEE Journal of Selected Topics in\nQuantum Electronics, 20(5):464‚Äì471, 2014.\n[7] Xing Fu, Steven L Brunton, and J Nathan Kutz. Classi-\nÔ¨Åcation of birefringence in mode-locked Ô¨Åber lasers using\nmachine learning and sparse representation. Optics ex-\npress, 22(7):8585‚Äì8597, 2014.\n[8] J Nathan Kutz and Steven L Brunton. Intelligent systems\nfor stabilizing mode-locked lasers and frequency combs:\nmachine learning and equation-free control paradigms for\nself-tuning optics. Nanophotonics, 4(1):459‚Äì471, 2015.\n[9] U Andral, R Si Fodil, F Amrani, F Billard, E Hertz, and\nPh Grelu. Fiber laser mode locked through an evolution-\nary algorithm. Optica, 2(4):275‚Äì278, 2015.\n[10] RI Woodward and Edmund JR Kelleher. Towards ?smart\nlasers?: self-optimisation of an ultrafast pulse source us-\ning a genetic algorithm. ScientiÔ¨Åc reports, 6:37616, 2016.\n[11] U Andral, J Buguet, R Si Fodil, F Amrani, F Billard,\nE Hertz, and Ph Grelu. Toward an autosetting mode-\nlocked Ô¨Åber laser cavity. JOSA B, 33(5):825‚Äì833, 2016.\n[12] Thomas Baumeister, Steven L Brunton, and J Nathan\nKutz.\nDeep learning and model predictive control for\nself-tuning mode-locked lasers. JOSA B, 35(3):617‚Äì626,\n2018.\n[13] Richard S Sutton and Andrew G Barto. Reinforcement\nlearning: An introduction. MIT press, 2018.\n[14] David Silver, Thomas Hubert, Julian Schrittwieser, Ioan-\nnis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanc-\ntot, Laurent Sifre, Dharshan Kumaran, Thore Graepel,\net al. A general reinforcement learning algorithm that\nmasters chess, shogi, and go through self-play. Science,\n362(6419):1140‚Äì1144, 2018.\n[15] Sinno Jialin Pan and Qiang Yang. A survey on trans-\nfer learning. IEEE Transactions on knowledge and data\nengineering, 22(10):1345‚Äì1359, 2009.\n[16] Miroslav Krsti¬¥c and Hsin-Hsiung Wang. Stability of ex-\ntremum seeking feedback for general nonlinear dynamic\nsystems. Automatica, 36(4):595‚Äì601, 2000.\n[17] Jared C Bronski and J Nathan Kutz. Modulational sta-\nbility of plane waves in nonreturn-to-zero communica-\ntions systems with dispersion management. Optics let-\nters, 21(13):937‚Äì939, 1996.\n[18] Qian Li, J Nathan Kutz, and PKA Wai.\nHigh-degree\npulse compression and high-coherence supercontinuum\ngeneration in a convex dispersion proÔ¨Åle. Optics Com-\n10\nmunications, 301:29‚Äì33, 2013.\n[19] Joshua L Proctor and J Nathan Kutz. Theory of q switch-\ning in actively mode-locked lasers. JOSA B, 23(4):652‚Äì\n662, 2006.\n[20] Christopher JCH Watkins and Peter Dayan. Q-learning.\nMachine learning, 8(3-4):279‚Äì292, 1992.\n[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, An-\ndrei A Rusu, Joel Veness, Marc G Bellemare, Alex\nGraves, Martin Riedmiller, Andreas K Fidjeland, Georg\nOstrovski, et al. Human-level control through deep rein-\nforcement learning. Nature, 518(7540):529‚Äì533, 2015.\n[22] C. J. C. H. Watkins.\nLearning from delayed rewards.\nPhD thesis, 1989.\n[23] R. S. Sutton.\nLearning to predict by the methods of\ntemporal diÔ¨Äerences. Machine learning, 3(1):9‚Äì44, 1988.\n[24] Hado V Hasselt.\nDouble q-learning.\nIn Advances in\nneural information processing systems, pages 2613‚Äì2621,\n2010.\n[25] Hado Van Hasselt, Arthur Guez, and David Silver. Deep\nreinforcement learning with double q-learning. In Thir-\ntieth AAAI conference on artiÔ¨Åcial intelligence, 2016.\n[26] Long-Ji Lin. Reinforcement learning for robots using neu-\nral networks.\nTechnical report, Carnegie-Mellon Univ\nPittsburgh PA School of Computer Science, 1993.\n[27] Kristin M Spaulding, Darryl H Yong, Arnold D Kim,\nand J Nathan Kutz. Nonlinear dynamics of mode-locking\noptical Ô¨Åber ring lasers. JOSA B, 19(5):1045‚Äì1054, 2002.\n[28] Edwin Ding, Eli Shlizerman, and J Nathan Kutz. Gen-\neralized master equation for high-energy passive mode-\nlocking: the sinusoidal ginzburg‚Äìlandau equation. IEEE\njournal of quantum electronics, 47(5):705‚Äì714, 2011.\n[29] Edwin Ding and J Nathan Kutz. Operating regimes and\nperformance optimization in mode-locked Ô¨Åber lasers.\nOptics and Spectroscopy, 111(2):166, 2011.\n[30] Andrey\nKomarov,\nHerve\nLeblond,\nand\nFran¬∏cois\nSanchez.\nMultistability and hysteresis phenomena in\npassively mode-locked Ô¨Åber lasers.\nPhysical review A,\n71(5):053809, 2005.\n[31] Xuling Shen,\nWenxue Li,\nMing Yan,\nand Heping\nZeng.\nElectronic control of nonlinear-polarization-\nrotation mode locking in yb-doped Ô¨Åber lasers. Optics\nLetters, 37(16):3426‚Äì3428, 2012.\n",
  "categories": [
    "eess.SP",
    "physics.optics"
  ],
  "published": "2020-06-10",
  "updated": "2020-06-10"
}