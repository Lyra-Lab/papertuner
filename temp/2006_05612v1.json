{
  "id": "http://arxiv.org/abs/2006.05612v1",
  "title": "Deep Learning for Change Detection in Remote Sensing Images: Comprehensive Review and Meta-Analysis",
  "authors": [
    "Lazhar Khelifi",
    "Max Mignotte"
  ],
  "abstract": "Deep learning (DL) algorithms are considered as a methodology of choice for\nremote-sensing image analysis over the past few years. Due to its effective\napplications, deep learning has also been introduced for automatic change\ndetection and achieved great success. The present study attempts to provide a\ncomprehensive review and a meta-analysis of the recent progress in this\nsubfield. Specifically, we first introduce the fundamentals of deep learning\nmethods which arefrequently adopted for change detection. Secondly, we present\nthe details of the meta-analysis conducted to examine the status of change\ndetection DL studies. Then, we focus on deep learning-based change detection\nmethodologies for remote sensing images by giving a general overview of the\nexisting methods. Specifically, these deep learning-based methods were\nclassified into three groups; fully supervised learning-based methods, fully\nunsupervised learning-based methods and transfer learning-based techniques. As\na result of these investigations, promising new directions were identified for\nfuture research. This study will contribute in several ways to our\nunderstanding of deep learning for change detection and will provide a basis\nfor further research.",
  "text": "arXiv:2006.05612v1  [cs.CV]  10 Jun 2020\nDEEP LEARNING FOR CHANGE DETECTION IN REMOTE\nSENSING IMAGES: COMPREHENSIVE REVIEW AND\nMETA-ANALYSIS\nA PREPRINT\nLazhar kheliﬁ∗\nDepartment of Computer Science\nand Operations Research\nMontreal University\nMontreal, Quebec, Canada\nkhelifil@iro.umontreal.ca\nMax Mignotte\nDepartment of Computer Science\nand Operations Research\nMontreal University\nMontreal, Quebec, Canada\nmignotte@iro.umontreal.ca\nJune 11, 2020\nABSTRACT\nDeep learning (DL) algorithms are considered as a methodology of choice for remote-sensing image analysis\nover the past few years. Due to its effective applications, deep learning has also been introduced for automatic\nchange detection and achieved great success. The present study attempts to provide a comprehensive review and\na meta-analysis of the recent progress in this subﬁeld. Speciﬁcally, we ﬁrst introduce the fundamentals of deep\nlearning methods which are frequently adopted for change detection. Secondly, we present the details of the\nmeta-analysis conducted to examine the status of change detection DL studies. Then, we focus on deep learning-\nbased change detection methodologies for remote sensing images by giving a general overview of the existing\nmethods. Speciﬁcally, these deep learning-based methods were classiﬁed into three groups; fully supervised\nlearning-based methods, fully unsupervised learning-based methods and transfer learning-based techniques. As\na result of these investigations, promising new directions were identiﬁed for future research. This study will\ncontribute in several ways to our understanding of deep learning for change detection and will provide a basis\nfor further research.\nKeywords Change detection · Remote sensing images · Deep learning · Feature learning · Weakly supervised\nlearning · Review\n1\nIntroduction\nDeep learning (DL) has seen an increasing trend and a great interest over the past decade due to its powerful ability\nto represent learning. Deep learning allows models that are built, based on multiple processing layers, to learn rep-\nresentations of data samples with several levels of abstraction [1]. Deep learning enables models that are composed,\nbased on multiple layers, to learn representations of data samples with several ranges of abstraction levels [1]. It may\nalso be considered as the analysis of models that either require a greater composition of learned concepts or functions,\ncompared to conventional machine learning models such as naive Bayes [2] [3], support vector machine (SVM) [4]\n[5], random forests, [6] [7] and the decision tree[8][9].\nOn the basis of its state-of-the-art performance, deep learning has been consequently applied to various domains, such\nas computer vision [10], speech recognition [11], and information retrieval [12]. Particularly, in the computer vision\nﬁeld, deep learning has taken great leaps thanks to the recent advances of processing power, the improvements in\ngraphics processors and the increased data volumes (i.e., videos and images). Notably, the science of remote sensing\n(RS) has seen a massive increase in the generation and enhancement of digital images captured from airplanes or\nsatellites that cover almost each angle of the surface of the earth. This growth in data has pushed the community\n∗webpage: https://www.lazharkheliﬁ.com\nA PREPRINT - JUNE 11, 2020\nof the geoscience and remote sensing (RS) to apply deep learning algorithms to solve different remote sensing tasks.\nAmong these tasks, stands out the change detection (CD) task deﬁned in [13] as ’the process of identifying differences\nin the state of an object or phenomenon by observing it at different times’. In another word, change detection refers to\nidentifying the differences between images acquired over the same geographical zone but taken at two distinct times\n[14].\nChange detection techniques are extensively utilized in various applications [15] including; disaster assessment [16],\nenvironmental monitoring [17], land management [18] and urban change analysis [19], etc. Currently, the number\nof extreme disasters caused by climate change such as drought, ﬂoods, hurricanes, and heat waves, has revealed at\nthe same time a new challenge for researchers and a need for developing more effective automated change detection\nmethods. Motivated by those aforementioned observations, deep learning has been introduced for change detection in\nremote sensing and achieved good performance.\nRecently, various reviews that focus on deep learning for remote sensing data have been published. These studies\nhave summarized the deep learning techniques adopted in all major remote sensing sub-areas including classiﬁcation,\nrestoration, denoising, target recognition, scene understanding, and other tasks (for further details we refer the reader\nto [20] [21] [22]). To the best of our knowledge, however, there is no work that has studied the recent progress of deep\nlearning for the task of change detection in a speciﬁc and extensive way. Therefore, the purpose of this present report\nis to provide an overview of the state of deep learning algorithms as applied in remote sensing images for change\ndetection. Hence, by performing a meta-analysis, we selected and categorized the relevant papers related to DL and\nchange detection. By doing so, then we provide a technical review of these studies that shed more light on the advance\nof deep learning for change detection. This review will serve as a base for future studies in this subﬁeld of research.\nThe rest of this paper is structured as follows. Section 2 presents the deﬁnition of the change detection problem.\nSection 3 gives a brief overview of deep learning as well as the typical deep models used for change detection. Section\n4 describes the methods and data used to review the state-of-the-art. In Section 5, we divide these previous works into\nthree categories; fully supervised learning-based methods, fully unsupervised learning-based methods, and transfer\nlearning-based methods. Section 6 suggests two interesting research directions to further advance the ﬁeld. Finally,\nSection 7 outlines the conclusions.\n2\nChange detection in remote sensing\nChange detection is the operation of quantitative analysis and determination of surface changes from phenomena or\nobjects over two distinct periods [13]. This process, which is a basic technology in the ﬁeld of earth observation,\nattempts to distinguish the changed and unchanged pixels of bi-temporal or multi-temporal remote sensing images\nacquired from the same geographical zone or area, but at different times, respectively [23] [24]. Assigning to each\npixel a binary label based on a pair or series of co-registered images represents the main purpose of the change\ndetection system. A positive label thus means that the area of that pixel has changed, while a null label represents an\nunchanged area (See Figs. 1 and 2) [25]. Actually, change detection represents a powerful tool for video surveillance,\nmapping urban areas, and other forms of multi-temporal analysis.\nFormally, let I1 and I2 be two co-registered images, which share the same size W × L and taken over the same\ngeographical region at two separate periods t1 and t2, respectively, using the same sensor, in the classic monomodal\ncase:\nI1 = {I1(x, y), 1 ≤x ≤W, 1 ≤y ≤L}\n(1)\nI2 = {I2(x, y), 1 ≤x ≤W, 1 ≤y ≤L}.\n(2)\nThe primary purpose of a change detection system is to generate an accurate binary change map (CM):\nCM = {CM(x, y) ∈{0, 1}, 1 ≤x ≤W, 1 ≤y ≤L},\n(3)\nwhere (x, y) represents the position coordinates of the pixel indexed i. In traditional methods, this change map can\nbe obtained by a difference image (DI) operation, based on differencing or log-rationing function (DI = |I1 −I2|),\nfollowed by a ﬁnal analysis of the DI result.\nChange detection has been successfully used in a wide variety of applications. In particular, in the agricultural sector,\nchange detection is adopted for deforestation monitoring, disaster assessment and shifting cultivation monitoring.\nIn the military ﬁeld, it is now utilized in collecting information about new military installations, movement of the\nenemy’s military forces, battleﬁeld area, and damage assessment [26]. In the civil ﬁeld, change detection is used to\ncontrol urban area development and city extension [27]. Also, it is actually adopted to monitor the effects of climate\n2\nA PREPRINT - JUNE 11, 2020\nChange detection\nmethod\nT1 image\nT2 image\nChanged area\nUnchanged area\nChange map\nFigure 1: Graphical illustration of the change detection problem.\nImage at time T1\nImage at time T2\nChange map\nFigure 2: An illustration of a typical change detection results within a high-resolution satellite image [29] [30].\nchanges usually associated with the increase of levels of greenhouse gas (GHG) emissions in the atmosphere, such as\nchanges in mass balance and glacier facies or sea-level change.\nWhile the change detection algorithms have shown many beneﬁts in various ﬁelds of applications, it faces some serious\nchallenges. Among these challenges, we can consider the variation in data acquisition parameters which can affect the\nprocess of ﬁnding the relevant changes by adding an irrelevant information into the data. In addition, this unwanted\nchange can be emerged as atmospheric features, like fog, clouds, and dust. For example, a cloud present in one image\n(at time t1) but not in the other one (at time t2) leads to a bright patch that can be registered as a difference and\nconsequently affects the quality of the resulting change map. Angles of sunlight may also present another problem\nrelated to the presence and the direction of the shadows on the scene [26]. Besides, vegetation growth and surface\nreﬂectance of objects such as soil before and after rain can also affect the result of a change [28]. Thus, a robust change\ndetection method must be able to differentiate between relevant changes and irrelevant changes in satellite images in\naddition to the detection of temporal changes. Motivated by those successful applications, recently deep learning\ntechniques, capable of extracting information from data (image or video), have been applied to solve this problem and\nhave achieved good performances.\n3\nA PREPRINT - JUNE 11, 2020\n3\nBrief Overview of deep learning\nDeep learning (DL) algorithms, aiming at learning representative and discriminatory features from a set of data in a\nhierarchical way, have received much attention from worldwide geoscience and remote sensing communities, in recent\nyears. In the ﬁrst part of this section, we brieﬂy present the deep learning history to explain the trend in its growth.\nIn the second part, we outline different deep network models widely designed for change detection in remote sensing\nimages. These deep networks incorporate deep belief networks (DBNs), stacked autoencoders (SAEs), generative\nadversarial networks (GANs), recurrent neural networks (RNNs), and convolutional neural networks (CNNs).\n3.1\nHistory\nDeep learning (DL) is a particular approach of machine learning which takes advantage of the knowledge of the\nstatistics, human brain, and applied mathematics statistics, as it advanced over the last years [31]. By gathering these\npieces of knowledge, this approach relieved human experts from formally deﬁning all the knowledge that the computer\nmachine requires to resolve a particular problem. This powerful approach reaches good ﬂexibility and scalability by\nrepresenting the world as an embedded hierarchical structure of concepts. This concept hierarchy enables the machine\nto recognize complex concepts by developing them from simpler ones [31]. DL is driven from the connectivity\ntheory related to the functionality of our brain cells, also called neurons, leading to the concept of artiﬁcial neural\nnetworks (ANN). ANN is designed based on artiﬁcial neuron layers to receive input data and transform it into outputs\nby applying an activation function and learning progressively higher-level features. The intermediate layers (in the\nmiddle of the input and output) are often called “hidden layers” because they are not directly observable from the\ninputs and outputs of the system [32]. In practice, to solve complex tasks such as change detection in remote sensing\nimages, a neural network that contains multiple hidden layers is applied. This multiple-layered structure is addressed\nas a “deep” neural networks (DNNs), therefore, the word “deep learning”.\nAs described in [31], the development of deep learning has followed three main waves reﬂecting different philosoph-\nical viewpoints. The ﬁrst wave refers to cybernetics in the 1940s-1960s, characterized by the concepts advance of\nbiological learning [33] [34] and application of the ﬁrst models such as the perceptron [35] which allows the training\nof an architecture based on a unique neuron. The second stage began with the connectionist1 approach expanded in\nthe 1980s-1995s period, with back-propagation [36] to train a neural network using one or two hidden layers. This\nfundamental building block updates the weights of the connections in the network for multiple times, by minimizing\na measure of the gap among the actual output vector of the net and the aimed output vector [37]. While this approach\nworks quite well when dealing with simple applications, especially, the community of computer vision has found some\nissues to apply this approach to complex problems. The main challenge was the lack of speciﬁc computing hardware\nto train efﬁciently deep neural networks (DNNs). The third wave started in 2006 under the name of deep learning [38]\n[39]. Since that time, we have seen a renewed importance in deep neural networks beneﬁtted to the availability of\npowerful computer systems, expanded databases and new training techniques. Currently, deep learning has received\nmuch focus in different research areas of computer vision, including the analysis of remote sensing images.\n3.2\nDeep models\n3.2.1\nDBNs\nDeep belief networks (DBNs) are mainly built based on a layerwise training model called restricted Boltzmann ma-\nchine (RBM). RBMs are stochastic undirected graphical models containing a layer of visible variables and a unique\nlayer of hidden variables. Fig. 3 illustrates the graph structure of the RBM. It is a bipartite graph that involves the link\nof visible units representing observations, to hidden units that learn to describe features based on undirected weighted\nconnections [40]. In this model, there are no connections permitted among any variables in the visible layer or between\nany units in the hidden layer. Mathematically, let suppose that the visible layer v contains a set of nv binary random\nvariables, and the hidden layer h consists of nh binary random variables. The energy function of the canonical RBM\ncan be formulated as [31]:\nE(v,h) = −b⊤v −c⊤h −v⊤Wh\n(4)\nwhere W, b and c represent learnable parameters. The weights W on the connections and the biases b The weights on\nthe connexions and the biases of the individual units express a distribution of probability through an energy function\nover the joint states of the visible and hidden units [41]. The probability (i.e., energy) of a joint conﬁguration is then\ndeﬁned as:\np(v,h) = Z exp(−E(v, h))\n(5)\n4\nA PREPRINT - JUNE 11, 2020\nHidden units\nVisible units\nFigure 3: A general illustration of RBM.\nInput layer\nRBM1\nRBM2\nRBM3\nOutput layer\nFigure 4: General illustration of DBN.\nwhere Z is the normalizing constant usually referred to the partition function:\nZ =\nX\nv\nX\nh\nexp {−E(v, h)}\n(6)\nBecause of the restricted characteristic (i.e., feature) representation capability of a unique RBM, several RBMs can be\nstacked one by one forming a DBN that may effectively trained to obtain a deep hierarchical modeling of the training\ndata [42]. Fig. 4 presents a DBN composed by stacking multiple RBM layers.\n3.2.2\nSAEs\nAutoencoder (AE) is considered as the principal building piece of the stacked autoencoder (SAE) [44]. An autoencoder\nis a feedforward neural network model that applies backpropagation, setting the objective values to be consistent (or\nequal) to the inputs. This model consists of two steps an encoder h = f(x) and a decoder that attempts to provide a\nreconstruction r = g(h). On the one hand, based on a non-linear function, the encoder side projects the input vector\n(x) to the hidden layer:\nh = s(whx + bh)\n(7)\n1Connectionism represents a movement in cognitive science that aims to interpret intellectual abilities through artiﬁcial neural\nnetworks [43].\n5\nA PREPRINT - JUNE 11, 2020\nInput layer\nOutput layer\nHidden layer\nEncoder\nDecoder\nFigure 5: Auto-Encoders.\nInput layer\nEA\nEA\nOutput layer\nFigure 6: Stacked Auto-Encoders.\nOn the other hand, the decoder maps the hidden layer back to the output layer that contains an identical number of\nunits as the input layer:\ny = s(wyh + by)\n(8)\nwhere s(.) denotes the logistic sigmoid function (1 + exp(−x)). wh and wy represent the input to hidden and the\nhidden to output weights, respectively. In addition, bh and by identify the bias of the hidden and output units. With the\npurpose of reconstructing the error between x and y, a metric based on the Euclidean distance is generally minimized.\nThis reconstruction loss is deﬁned by:\nd(x,y) = ∥y −x∥2\n(9)\nA typical architecture of autoencoder is presented in Fig. 5. A stacked Autoencoder (SAE) is a neural network built\non the top of several layers of autoencoders where the output of each hidden layer is connected to the input of the next\nhidden layer. Fig. 6 shows a simple representation of a SAE.\n3.2.3\nCNNs\nConvolutional neural networks, also known as CNNs [45], are a special form of neural network designed for processing\ndata that has a known grid-like representation, for example image data, which can be considered a two dimension (2D)\ngrid of pixels. Generally, the CNNs can be thought of as an extractor of hierarchical characteristics, which, on the\none hand, extracts features of diverse abstraction layers, and on the other hand, maps the raw pixel intensities into\na feature vector. [46]. An architecture of a typical CNNs is illustrated in Fig. 7, where conv, mp, and fc denote\nconvolutional, max-pooling and fully-connected layers, respectively. Convolutional layer represents the fundamental\ncomponent of the CNN architecture [47]. In this layer, several trainable convolution kernels (called also ﬁlters) are\napplied to the previous layer. The weights of these kernels aim to connect units in a feature map with the previous\nlayer. As a result of convolution, local conjunctions of features are detected and their appearance is mapped to the\nfeature maps. The stacking of various convolutional layers increases the depth of networks which makes the extracted\nmaps more abstract. The earlier layers enhance features, for example edges, however, the following layers aggregate\n6\nA PREPRINT - JUNE 11, 2020\nFigure 7: A ﬂowchart of a conventional CNN, which consists of two convolutional layers (C1, C2), two pooling layers\n(P1, P2), two fully connected layers (F1, F2) and a softmax layer (output).\nthese features in the form of motifs, parts, or objects. Formally, suppose that ml represents the ﬁlters convolution\nnumber in layer l of the network, and xn\nl−1 the 2D array related to the n−th input of layer l. The k −th output feature\nvector of layer l, denoted zk\nl , can be computed as follows:\nzk\nl =\nhml−1\nX\nn=1\nwk,n\nl\n⊗xn\nl−1\ni\n+ bk\nl\n(10)\nwhere bk\nl is the bias matrix, wk,n\nl\nrepresents a ﬁlter connecting the n-th feature map in the previous layer (l −1) with\nthe k-th feature map in layer l, and ⊗denotes the convolution operator. Typically, after the convolution operation a\nnonlinear activation function is performed on each element of the convolution result.\nyk\nl = f\n\u0010\nzk\nl\n\u0011\n(11)\nA range of activation functions has been proposed in the literature to improve the performance of CNNs, for example\nthe sigmoid function [48], hyperbolic tangent function (tanh) [49], adaptive piecewise linear activation (APL) [50],\nand the popular rectiﬁed linear unit (ReLU) [51]. The convolution process is followed by a max-pooling operation.\nThis step aims to replace the output of the network at some particular positions with a summary statistic relating to the\nneighborhood of this location [31]. The pooling operation aims to gradually minimize, the spatial size of the output\nfeature maps, and hence, decreases the parameters number of the network. Generally, there are two standard choices\nfor the operation of pooling: max and average. Formally, for a v × v window-size neighbor represented by N. The\naverage takes the arithmetic mean of the elements in each pooling region as follows:\npk\nj = | 1\nRk\nj\n|\nX\ni∈Rk\nj\nyk\ni,j\n(12)\nwhile the max operation takes the largest element:\npk\nj = max\ni∈Rk\ni\nyk\ni,j\n(13)\nwhere Rj is pooling region (i.e., the number of elements in N) and yk\ni,j is the activation value related to the position\n(i, j). After the pooling operation, the output feature maps of the previous layer are ﬂattened and provided to fully\nconnected layers. These layers are exploited to extract more high-level information by reshaping feature maps into\nan n-dimension vector [42]. At the last layer of the network, called the classiﬁcation layer, neurons are gathered\nautomatically into C output feature maps that correspond to the number of classes. Then, using a softmax function,\nthe output of the classiﬁcation layer L is converted into (normalized) probability distribution errors. Speciﬁcally, the\nprobability distribution of classes is produced via the following function:\npc =\nexp(yc\nL)\nPC\nc′=1 exp(yc′\nL )\n(14)\nwhere the calculated probabilities are within a [0, 1] range, and the sum of all the probabilities is equal to 1. Convolu-\ntional Neural Networks (CNNs) have been well established as a powerful class of models from a variety of computer\nvision tasks [52] including change detection in remote sensing images. Hence, different successful CNNs architectures\nhave been suggested in the literature. The current surge of the CNNs in many tasks heavily relies on the use of modern\nnetwork architectures, such as the AlexNet [53], VGG [54], and RESNET [55]. These modern architectures explore\nnew and innovative ways for constructing convolutional layers that guarantee more efﬁcient learning [56].\n7\nA PREPRINT - JUNE 11, 2020\nFigure 8: An unrolled recurrent neural network.\n3.2.4\nRNNs\nRecurrent neural networks, also known as RNNs [37], are a class of neural networks that allows processing sequential\ndata. Particularly, this model is enhanced by the integration of edges that spanning adjacent time steps which introduces\nthe notion of time [57]. Compared to the convolutional neural network that is specialized for processing a grid of values\nX such as an input image, a recurrent neural network allows operating over a sequence of vectors or values with the\nhelp of a recurrent hidden state (see Fig. 8). Formally, suppose that x(1), ..., x(n) is a sequence of vectors where xt\nrepresents the data at the tth time step. Two activation functions deﬁne all calculations required for computation at\neach temporal sequence t:\nht = f(Whxt + Uhht−1 + bh)\n(15)\nyt = f2(Wyht + by)\n(16)\nwhere Uh is the same matrix utilized at each time step. Via this matrix, the hidden units in the previous step ht−1 is\nused to compute ht, while the current observation provides a weighted term Whxt, which is summed with Uhht−1 and\na bias term by. Both Wh and by are typically replicated over time. The output layer is represented by a conventional\nneural network activation function applied to the linear transformation of the hidden units, and the process is repeated\nfor every time phase [58]. Unfortunately, standard RNNs suffer from a critical drawback related to the vanishing\ngradient problem, which makes the neural network hard to be trained properly. To overcome this serious problem,\nlong short-term memory (LSTM) [59] and gated recurrent unit (GRU) [60] were suggested. One advantage of LSTM\nis that it introduces the notion of memory cell, a unit of computation that replaces classical nodes in the hidden layer of\na network. This capability of memory cells able to overcome difﬁculties with training encountered by earlier recurrent\nnetworks. Like the LSTM unit, the GRU is characterized by units which control the information ﬂow within the unit,\nnevertheless, without having a distinct memory cell [61].\n3.2.5\nGANs\nGenerative adversarial networks (GANs) were proposed by Goodfellow et al. [62]. Given a real data (e.g., images),\nthis generative technique learns to produce novel data with the same statistics as the original data. GANs are based\non a game theoretical scenario in which the generator network must compete against an adversary [31]. A general\nillustration of the structure of a GAN is shown in Fig. 9. Formally, from training data x and a provided a priori\ndistribution (i.e., random noise) v, the generator network directly generates fake samples G(v). Its adversary, the\ndiscriminator network, aims to differentiate between samples provided by the training data and samples produced by\nthe generator. While the discriminator D is trained to maximize the value of log(D(x)), indicating the probability of\nselecting the correct labels to the training samples, the generator block G is trained to minimize log(1−D(G(z)) [42].\nThus, D and G play a two-player minimax game as follows:\nGAN = arg min\nD max\nG LGAN(G, D)\n(17)\nwhere\nLGAN(G, D) = Ex∼p(x)[log D(x)] + Ev∼(v)[log(1 −D(G(v)))]\nHere, E denotes the expectation operator. The main goal of training generative networks is to produce examples\nthat appear realistic compared to the original data. Based on that assumption, GANs have been successfully used in\ndifferent computer vision and image processing applications.\n8\nA PREPRINT - JUNE 11, 2020\nFigure 9: General illustration of the structure of a Generative Adversarial Network (GAN).\n4\nMethods and data used to review DL for CD in remote sensing images\n4.1\nA meta-analysis process for data extraction\nWe searched and collected all published studies relevant to change detection in remote-sensing images using the\ndeep learning approach. The search for studies was conducted using the web of science database2. It is the most\ntrusted publisher global citation database. The generated dataset was built using an advanced search option (search\ndate: April 18th, 2020) with a relevant controlled vocabulary included; deep learning, change detection and remote\nsensing topic, etc. All of the studies included in our research had been published up to 2014. Ignored from the\nsearch query were prefaces, article summaries, interviews, discussions, news items, correspondences, readers’ let-\nters, comments, summaries of tutorials, panels, workshops, and poster sessions. This search strategy resulted in\na total of 160 unique papers, including 110 journal articles and 47 conference papers, two early access paper and\none editorial material paper. All these included studies are summarized in one ﬁle publicly accessible via this link:\n\"http://www.lazharkheliﬁ.com/?publications=rev_cd_dl.zip\". It is worth noting that every article included in the re-\nview was read in detail by the authors.\n4.2\nReferred journals and conference papers\nAmong the set of 110 peer-reviewed journal papers, a larger part of articles were published in the ten journals shown\nin Table 1. Note that journals with only one publication are not listed here. Overall, these 10 journals include 82\narticles peer-reviewed journal papers related to DL change detection and remote sensing. Regarding the number of\narticles published per journal, the top ﬁve peer-reviewed journal papers are; Remote Sensing, IEEE Transactions on\nGeoscience and Remote Sensing (TGRS), IEEE Access, IEEE Journal of Selected Topics in Applied Earth Observations\nand Remote Sensing and IEEE Geoscience and Remote Sensing Letters.\nWe found that the topic is now well represented at the major international remote sensing conferences. Thus, among\nthe set of 47 conference papers, a majority of the articles were published by the two remote sensing academic societies\nlisted in Table 2, namely the IEEE Geoscience and Remote Sensing Society (IGARSS) and the Society of Photo-\nOptical Instrumentation Engineers (SPIE). The conferences with only one publication are not listed in this table. The\nreader should bear in mind that conference papers were excluded from the present meta-analysis because many were\nexpanded into journal papers after presenting at the conferences (as in [20]). In addition, after a deep understanding\nof the content of all the papers, 20 journal papers that not cover the subject of this study were also excluded.\n4.3\nBrief interpretation of the results\nSeveral general conclusions may be drawn from the conducted statistical analysis to examine the trend in the use of\nDL for change detection. Trends and projections are illustrated in this study using histogram graphs in order to better\nvisualize the distribution of the data. Figure 10 reveals that there has been a marked increase in the number of scientiﬁc\npapers released on the topic since 2015. The number of published papers is expected to grow even more tremendously\n2The web of science database is accessible via the following link: https://www.webofknowledge.com/\n9\nA PREPRINT - JUNE 11, 2020\nFigure 10: Growing number of published papers related to deep learning for change detection in remote sensing (we\npredict more than 100 papers in 2020).\nFigure 11: The number of citations per year for papers related to deep learning for change detection in remote sensing\n(we predict more than 1000 citations in 2020).\nin the coming years. Similarly, the graph of Fig. 11 shows that there has been an important increase in the number\nof citations of those papers. Table 3 highlights the top three most-cited papers. This exponential growth, both for the\nnumber of published papers and the number of citations, validates the rapid growth of interest in the study of deep\nlearning for change detection in remote sensing images. Notably, the number of journal papers on this topic now\nexceeds the number of conference papers. This indicates the technical maturity of this research area. As can be seen\nfrom Fig. 12, the CNN model has been the most widely applied for change detection, followed by the SAE, DBN,\nRNN, AEs, RBM and GAN models. This higher popularity of CNN is probably because it is more suitable to learn\nhierarchical image representations from the input data by sequentially abstracting higher-level features [47]. Looking\nat Fig. 13, it is apparent that the SAR image type has been the most commonly used within deep learning model for\nchange detection, followed by multispectral, arial, optic, heterogeneous (i.e., multi-modal), and hyperspectral images.\nThe reason for this is, that synthetic-aperture radar captures images using microwave signals which can enter through\nclouds [63], and is therefore more likely to have a signiﬁcant advantage of being insensitive to sunlight and complex\natmospheric conditions [64].\n5\nDeep learning for change detection in remote Sensing images\nDeep learning has recently become the focus of considerable interest in the change detection ﬁeld [20]. It aims to\nautomatically learn high-level features from various remote sensing data compared to traditional hand-crafted features-\nbased methods [46]. Deep learning approaches for change detection can be grouped in several ways by considering\ndifferent perspectives. In this study, therefore, the deep learning approaches used for change detection are classiﬁed\ninto three groups based on the learning technique and the availability of a training data that can be either labeled or\nunlabeled. The ﬁrst type contains fully supervised methods which solve the problem by learning from a labeled training\ndataset. The second type of methods contains fully unsupervised methods that learn from unlabeled datasets. Both\nsupervised and unsupervised methods serve at selecting the available features that consistent with the target concept.\nHence, in supervised learning, the target concept is explicitly correlated to class afﬁliation, while in unsupervised\n10\nA PREPRINT - JUNE 11, 2020\nFigure 12: Distribution of DL models used in the studies.\nFigure 13: Distribution of types of remote sensing images used in the studies.\nTable 1: Journals identiﬁed as pertinent, and number of relevant papers.\nName of journal\n#\n-Remote Sensing-\n(31)\n-IEEE Transactions on Geoscience and Remote Sensing (TGRS)-\n(13)\n-IEEE Access-\n(7)\n-IEEE Journal of Selected Topics in Applied Earth Observations and\nmote Sensing-\n(7)\n-IEEE Geoscience and Remote Sensing Letters-\n(5)\n-ISPRS Journal of Photogrammetry and Remote Sensing-\n(5)\n-Journal of Applied Remote Sensing-\n(5)\n-IEEE Transactions on Neural Networks and Learning Systems-\n(4)\n-Applied Sciences-Basel-\n(3)\n-International Journal of Image and Data Fusion-\n(2)\nlearning the target concept typically targeted through inherent structures of the data [68]. The third type of methods\ncontains transfer learning based methods. Transfer learning is an important machine learning technique which attempts\nto utilize the knowledge learned from one task and to apply it on another, but associated, task with the purpose to either\nreduce the necessary ﬁne-tuning data size or improve performances [69]. Sections 5.1, 5.2, and 5.3 will outline these\ntypes of methods in detail.\n11\nA PREPRINT - JUNE 11, 2020\nTable 2: Conferences and proceedings determined as pertinent, and number of relevant papers.\nTitle of conference/Proceedings\n#\n-International Geoscience and Remote Sensing Symposium (IGARSS)-\n(17)\n-Proceedings of Society of Photo-Optical Instrumentation Engineers (SPIE)-\n(9)\nTable 3: The top three most-cited papers.\nAuthors\nTitle\nYear of publication\nTimes cited\n-Gong et al. [65]-\n-Change Detection in Synthetic Aperture Radar\nImages Based on Deep Neural Networks-\n2016\n(224)\n-Lyu et al. [66]-\n-Learning a Transferable Change Rule from a Re-\ncurrent Neural Network for Land Cover Change\nDetection-\n2016\n(126)\n-Zhang et al. [67]-\n-Change detection based on deep feature repre-\nsentation and mapping transformation for multi-\nspatial-resolution remote sensing images-\n2016\n(108)\n5.1\nFully Supervised learning based-methods\nFor a long time, it was commonly assumed that the process of training deep supervised neural networks is challeng-\ning, time-consuming, and too difﬁcult to perform [46]. While the standard learning strategy consisting of randomly\ninitializing the weights, recently, it was found that deep supervised networks can be trained by proper weight initial-\nization. This novel strategy just adequate enough for improving the gradient ﬂow as well as the transmission of useful\ninformation by the activations [70] [22]. The efﬁciency of supervised deep networks is particularly evident in case of\nthe availability of large amount of labeled data used to properly train it.\nIn recent years, some pure supervised DL methods have been suggested for change detection in RS images relying\non CNNs [71] [46]. These CNNs based studies have shown superior performances to the classical state-of-art meth-\nods. The CNNs are hierarchical models which converts the input image into multiple layers of feature maps. These\ngenerated maps consist of high-level discriminatory features that reﬂect the original input data [22]. Based on the\nfully convolutional networks U-Net is considered as one of the standard CNNs architectures used for change detection\ntask. The general network architecture of U-Net is symmetric, having an encoder that extracts spatial features from\nthe image, and a decoder that builds the segmentation map from the encoded feature [72].\nJaturapitpornchai et al. [63] have proposed in detail a U-Net-based network, which detects the novel buildings con-\nstruction in developing regions using two SAR images captured at different times. Subsequently, the U-Net architec-\nture was extended through a few modiﬁcations in other works. In this regard, Hamdi et al. [73] have developed an\nalgorithm using a modiﬁed U-Net model for automatic detection and mapping of damaged areas in an ArcGIS envi-\nronment. Their model was trained based on a database of a forest area in Bavaria, Germany. Recently, an improved\nUNet++ architecture was proposed by Peng et al. [74] for end-to-end change detection of VHR satellite images. In\norder, to learn multi-scale feature maps dense skip connections were established between the different layers of this\narchitecture. In addition, a residual block strategy was followed to facilitate gradient convergence of the network.\nFor change detection in hyperspectral image, a general end-to-end two-dimensional CNNs framework, called GET-\nNET, was presented by Wang et al. [75]. In addition, a conventional change vector analysis (CVA) method [76]\nwas adopted to generate pseudo-training sets with labels. Wiratama et al. [77] proposed a dual-dense convolutional\nnetwork for recognizing pixel-wise change on the basis of a dissimilarity analysis of neighborhood pixels on high reso-\nlution panchromatic (PAN) images. In their suggested algorithm, two fully convolutional neural networks are utilized\nto compute the dissimilarity of neighboring pixels. Further, a dense connection in convolution layers is performed\nto reuse preceding feature maps by connecting them to all subsequent layers. Zhang et al. [78] have introduced a\nfully atrous convolutional neural network (FACNN). In this FACNN, ﬁrst, an encoder which consists of fully atrous\nconvolution layers, is used for extracting scale features from VHR images. Afterwards, a change map based on pixel\nis generated using the classiﬁcation map of current images and an outdated land cover geographical information sys-\ntem (GIS) map. Daudt et al. [79] have proposed an integrated network based on deep FCNNs that performs a land\ncover mapping and change detection simultaneously, using information from the land cover mapping branches to help\nwith change detection. Zhang et al. [80] presented a spectral-spatial joint learning network (SSJLN). At the ﬁrst\n12\nA PREPRINT - JUNE 11, 2020\npart of this model, the spectral-spatial joint representation is derived from the network similar to the Siamese CNN\n(S-CNN) [81]. Second, these extracted features are combined together using a feature fusion block. To explore the\nunderlying information of the combined features, discrimination learning is then performed at the last step. Liu et al.\n[25] have demonstrated the complementarity of CNNs and bidirectional long short-term memory network (BiLSTM)\nby combining them into one uniﬁed architecture. While, the former is useful in extracting the rich spectral-spatial\nfeatures from bi-temporal images, the latter is powerful in analyzing the temporal dependence of bi-temporal images\nand transferring the features of images. Similarly, Cao et al. [82] have combined a deep denoising model trained on\na huge number of simulated SAR images patches with a CNNs model. While, the deep denoising network is adopted\nto keep useful information and suppress noise simultaneously, a three layers of a CNN model are built to establish the\nfeature learning process. Contrary to previous approaches, that rely on CNNs based models Wiratama et al. [83] have\nproposed a fusion architecture combining front-end and back-end neural networks. In order to accomplish low-level\nand high-level differential detection, the fusion network contains both single-path and dual-path networks. In addition\n, based on the two dual outputs, a two-stage decision algorithm was proposed by authors to efﬁciently provide the\nﬁnal change detection result. This method has shown a good performance for the identiﬁcation of changed/unchanged\nareas in high-resolution panchromatic images.\n5.2\nFully unsupervised learning based-methods\nSupervised deep learning methods such as the CNNs and its modiﬁed models have achieved satisfactory result in\nmany computer vision tasks due the availability of large annotated datasets [46]. Unfortunately, for change detection\ntask, there are often not enough training data to build such models. In addition, building a ground-truth map reﬂecting\nthe real change information of ground objects costs lots of time and effort [84]. Therefore, in many cases, it is more\nefﬁcient to learn the change features generated from a remote sensing image in an unsupervised manner [85].\nUnsupervised feature-learning methods are mainly based on models which may learn feature representations from the\npatches (of images, par example) without any necessary supervision [22]. There have been numerous enhancements\nand evolution to the unsupervised deep learning approach that has been successfully applied to recognizing remote\nsensing (RS) scenes and targets. One of the most well-known and signiﬁcant approaches is to stack (or to combine)\ntogether different shallow feature-learning methods like the Gaussian Mixture model, AEs, sparse coding and RBMs\n[46]. In this regard, for change detection in multispectral images, Zhang et al. [86] have proposed a new unsupervised\nmethod combining the DBN and the feature change analysis (FCA). Thus, to capture the useful information for discrim-\nination between changed and unchanged regions and to also suppress the irrelevant variations, the available spectral\nchannels are transformed into an abstract feature space via the DBN. Then, using these learned features, an FCA is\nperformed to identify the different types of change. Similarly, Su et al. [87] have introduced a novel deep learning and\nmapping (DLM) framework oriented to the ternary change detection task for information unbalanced images. In their\nmethod, two types of neural networks are used. First, a stacked denoising autoencoder is applied to two input images,\nserving as a feature extractor. Then, after a selection step of relevant samples, mapping functions are generated by a\nstacked mapping network, establishing the relationship between the features of each class. Afterwards, a comparison\nbetween the features is performed and the ﬁnal ternary map is generated via a clustering process of the comparison\nresult. Gao et al. [88] have proposed a novel SAR image change detection method based on deep semi-nonnegative\nmatrix factorization (Deep Semi-NMF) [89] and singular value decomposition (SVD) networks [90]. In their sug-\ngested method, the deep Semi-NMF is used as a pre-classiﬁcation step. Following this, the SVD network of two SVD\nconvolutional layers is applied to obtain reliable features, where good quality of these obtained features effectively\nimproves the classiﬁcation performance. To achieve more precise ternary change detection without any supervision,\nGong et al.[14] have combined SAE, CNN and an unsupervised clustering algorithm. First, noise is removed and key\nchange information are extracted by transforming difference image into a suitable feature space using SAE. Next, an\nunsupervised clustering is established on the feature maps learned by SAE. This ﬁnal step aims to provide reliable\npseudo labels for training the CNN as a change feature classiﬁer. Lv et al. [85] have presented a feature learning\nmethod based on the combination of a stacked contractive autoencoder (sCAE) and a simple clustering algorithm. In\nthis method, ﬁrst, an afﬁliated temporal change image is built using three different metrics. the aim of this strategy is to\nprovide more information about the temporal difference on the pixel level. Second, homogeneous change samples are\nprovided by generating a set of superpixels using a simple linear iterative clustering algorithm. Third, these generated\nsuperpixel-samples are used as input to train a sCAE network. Then, the encoded features results from the sCAE model\nare binary classiﬁed to create the change result map. Gong et al. [91] have developed a generative discriminatory clas-\nsiﬁed network (GDCN) for multispectral image change detection. The generative adversarial networks represent the\nkey block of this proposed model by providing three types of data; labeled data, unlabeled data, and new fake data.\nMore precisely, this GDCN composes of a discriminatory classiﬁed network (DCN) and a generator (G). While the\nDCN divides the input data into changed class, unchanged class, and extra class (i.e., fake class), the generator recovers\nthe real data from input noises to provide additional training samples. Finally, the bitemporal multispectral images are\n13\nA PREPRINT - JUNE 11, 2020\nfed to the DCN to get a ﬁnal reliable change map. For change detection in SAR images, Gen et al. [92] have proposed\nSGDNNs, an unsupervised saliency guided deep neural networks. The ﬁrst step in this model consists of extracting\na salient region from the difference image (DI), which probably belongs to the changed object. Then, a hierarchical\nfuzzy C-means (HFCM) clustering [93] is established to select samples with higher probabilities to be changed and\nunchanged. Using these pseudotraining samples, a DNNs based on the nonnegative-and Fisher-constrained autoen-\ncoder are applied to get reliable ﬁnal detection. Li et al. and [94] performed change detection for hyperspectral images\nusing a novel noise modeling-based unsupervised fully convolutional network (FCN) framework. Speciﬁcally, their\nsuggested deep CNN is trained using the change detection maps of existing unsupervised change detection methods,\nwhile the noise is removed during the end-to-end training process. Recently, Huang et al. [95] have proposed a new\nunsupervised algorithm based on deep learning called ABCDHIDL to automatically detect the building changes from\nmulti-temporal high-resolution remote sensing (HRRS) images. In this algorithm, initially, a convolution operation is\nadopted for two reasons; ﬁrst, to extract the spatial, texture and spectral features and second to generate a combined\nlow-level feature vector for each pixel. Then, the unlabeled samples are injected to pre-train a DBN network, where\nits parameters are optimized by jointly using the extreme learning machine (ELM) classiﬁer [96]. To further improve\nthe detection process, labeled samples are offered by an automatic selection based on a morphological operation.\n5.3\nDeep transfer learning based-methods\nIn many remote sensing applications, it is so expensive or impossible to recollect the required training data and rebuild\nthe models [97]. In particular, for the change detection task, there are often not enough training data that accurately\nrepresent the real change information of ground objects. Therefore, it is important to reduce the requirement and effort\nto recollect the training data. In that context, transfer learning or knowledge transfer among task domains can be a\nreliable solution.\nTransfer learning is deﬁned as the capability of extracting knowledge from one or more source tasks and applying it to\na novel or target task [97]. Formally, given a source domain DS with a related source task TS and a target domain DT\nwith a corresponding task TT , transfer learning is the proceeding of improving the target predictive function fT (.) by\nutilizing the corresponding information from DS and TS, where DS ̸= DT or TS ̸= TT [98].\nThere are two basic approaches currently being adopted in research into transfer learning. The ﬁrst approach consists\nof using the outputs of one or more layers of a network (such as AlexNet or resnet-101) trained on a different task as\ngeneric high dimensional feature detectors and training a new shallow model based on these features [99]. The second\napproach is more involved, which consists of ﬁne-tuning the network pre-trained in general images. Hence, ﬁnal layer\n(for classiﬁcation/regression) is not just replaced, but also, previous layers are retained again [100]. Following this\nformer approach, Hou et al. [101] have transferred a CNNs already pre-trained on large-scale natural image data set\n(e.g., ImageNet [102]), to a RS domain. Speciﬁcally, to get better results they ﬁne-tune the VGG-16 [54] to adapt it\nto their optical RS images on an aerial image dataset (AID) [103]. Similarly, Venugopal et al. [104], have resorted to\na ResNet-101 [105] network as a pretrained model, and they ﬁne-tuned parameters based on a dilated convolutional\nneural network (DCNN) which detects the changes between the two images. Afterwards, the classiﬁed result is\ndetermined from the ﬁnal feature map as unchanged and changed areas. To solve the change detection problem in\noptical aerial images, Zhang et al. [106] proposed a new method based on deep Siamese semantic network trained\nusing an improved triplet loss function. First, a DeepLabv2 [107] model pretrained on large-scale image data set (e.g.,\nPASCAL VOC 2012 dataset [108]), was transferred to the network, due to the difﬁculty of directly training the Siamese\nnetwork. Based on this strategy, the network has achieved a comparable performance with limited computational cost\nand minimum training samples. This change detection method is based on four steps; First, In order to perform\na radiant correction to the two coregistered images, the input bitemporal images are preprocessed using histogram\nmatching. Second, the preprocessed pair images are fed to the deep Siamese semantic network in order to generate two\nfeature maps. Following this, a resizing operation is applied for two semantic feature maps by a bilinear interpolation.\nAfterwards, a distance map is obtained by computing the Euclidean distance between semantic feature maps. Finally, a\nsimple threshold segmentation method is used to separate the distance map, and therefore, to generate the ﬁnal change\ndetection result. Fang et al. [109] proposed a novel hybrid end-to-end framework named dual learning-based Siamese\nframework (DLSF) for change detection from very high resolution (VHR) images. This framework consists of two\nparallel streams which are dual learning-based domain transfer and Siamese-based change decision. While the ﬁrst\npath is aimed at reducing the domain differences between two paired images and maintaining the intrinsic information\nby translating them into each other’s domain, the second path is aimed at learning a decision strategy to decide the\nchanges in two domains, respectively. Yang, et al. [64] have adopted the concept of change that is learned from the\nsource domain to the target domain by reducing the distribution discrepancy between two domains. In their model,\nthe pretraining stage includes two tasks; a supervised change detection in the source domain using U-Net architecture\nand a reconstruction network in the target domain without labels. The lower layers are shared between the two tasks,\nhowever, the ﬁnal layers related to each task are trained separately. After the pretraining step, reliable labels that are\n14\nA PREPRINT - JUNE 11, 2020\nchosen from a CD map, are used to ﬁne-tune the change detection network for the target domain. Although training\ndata are limited in the task of sea ice change detection, in the work of Gao et al.\n[110] a large data set was used\nto train a transferred multilevel fusion network MLFN, in addition, a ﬁne-tune strategy was utilized to optimize the\nnetwork parameters.\n6\nPromising research directions\nTo advance the progress of the change detection task, in this section, we suggest two important directions for research,\nspeciﬁcally deep reinforcement learning and weakly supervised change detection.\n6.1\nDeep reinforcement learning\nDue to the lack of sufﬁcient labeled training databases for the supervised change detection task, the description ca-\npability of the features generated by deep learning methods may become limited or even impoverished. Recently,\ndeep reinforcement learning [111] [112] [113] has become the focus of considerable interest in the ﬁeld of machine\nlearning and has shown an excellent potential and great performance in various domains of computer vision such as\nautonomous driving [114] [115], object tracking [116] [117], person re-identiﬁcation [118] [119], etc.\nDeep reinforcement learning combines deep neural networks with a reinforcement learning architecture, where intelli-\ngent machines can learn from their actions similar to the way humans learn from experience. Reinforcement learning\nenables software-deﬁned agents to learn from the environment on the basis of random exploration and to adjust the\nbest possible actions based on continuous feedback in order to attain their goals. Actions that get them to the target\noutcome are rewarded (i.e., exploitation) [120]. Formally, it consists of a ﬁnite number of states si which represent\nagents and the environment, actions ai realized by the agent, probability Pa of moving from one state to another on the\nbasis of action ai, and reward Ra(si, si+1) corresponded to the move to the next state si+1 with action a. To predict\nthe best action as given by the function D(s, a), balancing and maximizing the current reward R and future reward α\n· max[D(s′, a′] is necessary. Where α in the equation denotes a ﬁxed discount factor. Hence, this function D(s, a) is\nrepresented as the summation of current reward R and future reward α · max[D(s′, a′)] in the following way [120]:\nD(s, a) = R + α · max[D(s′, a′)]\n(18)\nReinforcement learning is particularly dedicated to solve problems consisting of both short-term and long-term re-\nwards, for example, games such as go and chess, etc. However, combining reinforcement learning and deep network\narchitecture together yields deep reinforcement learning (DRL), which extends the use of reinforcement learning to\nrobustly solve more difﬁcult games and other challenging problems [121]. Deep reinforcement learning not only pro-\nvides rich representations characterized by a higher number of hidden layers of deep networks, but also, presents a\nreinforcement learning-based Q-learning algorithm 3 that maximizes the reward for actions taken by the agent [121].\nFu et al. [122] have shown the feasibility of using deep reinforcement learning for remote sensing ship detection task.\nRecently, Li et al. [123] have proposed an interesting aircraft detection framework based on the combination of a\nCNN model with reinforcement learning. Similarly, the change detection process can be solved as an action-decision\nproblem based on a sequence of actions reﬁning the size of the changed regions between two input images.\n6.2\nWeakly supervised change detection\nConsidering the high cost of the data labeling operation, in many computer vision tasks, it is hard to get strong\nsupervision information, (e.g., a dataset with fully ground-truth labels) [125]. Notably, in remote sensing images, the\nmanual annotation of objects is generally expensive and sometimes unreliable. Particularly for the change detection\ntask, the changed regions are very small, the background is often cluttered and complex, and the images may be taken\nby different sensors [126]. However, training a change detection framework based on weakly supervised learning\n(WSL) can alleviate the need for manual annotation. Weakly supervised data include a small quantity of accurate label\ninformation, that differs from data in traditional supervised learning [127]. In general, there are three classes of weak\nsupervision [125]:\n• Incomplete supervision when a minimum quantity data (among the training data) is provided with labels, which is\ninadequate to successfully train a learner.\n3Q-learning is a reinforcement learning algorithm required to ﬁnd an optimal action-selection strategy to maximize the sum of\nthe discounted rewards. [124].\n15\nA PREPRINT - JUNE 11, 2020\n• Inexact supervision is when some supervision information is available, however, not as accurate as required (i.e.,\nonly coarse-grained label information is provided).\n• Inaccurate supervision relates the case in which the outlined labels are not really ground-truth and suffer from errors\n(i.e., learning with label noise).\nRecent progress in the geospatial object detection ﬁeld [128] [129] has shown the feasibility of using weakly super-\nvised learning. Similarly, it will be interesting to explore the potential of WSL-based change detection models accu-\nrately for identifying the changed regions between two images. However, the performance of existing WSL-based\nmethods in remote sensing images is still far from satisfactory. For example, accurate position of the change cannot be\nyielded in detection of building changes [130]. Much effort also needs to be made to establish more efﬁcient methods\nto improve the detection accuracy [126].\n7\nConclusion\nRecently, deep learning-based change detection in remote sensing ﬁeld has drawn signiﬁcant attention and obtained\ngood performances. Deep learning based methods can automatically learn complex features of remote sensing images\non the basis of a huge number of hierarchical layers, in contrast to traditional hand-crafted feature-based methods. In\nthis work, publications related to DL in remote sensing images were systematically analyzed through a metaanalysis.\nIn addition, a deeper review was conducted to describe and discuss the use of DL algorithms speciﬁcally in the\nﬁeld of change detection, which differentiates our study from previous reviews on DL and remote sensing. Thus,\nseveral deep models that are often used for change detection are described. In addition, we concentrate on deep\nlearning-based change detection approaches for remote sensing images by providing a general overview of the existing\nmethods. Speciﬁcally, these deep learning-based methods were divided into three groups; fully supervised learning-\nbased methods, fully unsupervised learning-based methods and transfer learning-based methods. Besides, we have also\nproposed two promising future research directions. Therefore, a further study with more focus on deep reinforcement\nlearning and weakly supervised change detection methods are strongly suggested.\nReferences\n[1] L. Yann, B. Yoshua, and H. Geoffrey, “Deep learning,” Nature, vol. 521, pp. 436 – 444, 2015.\n[2] I. Rish, “An empirical study of the naive bayes classiﬁer,” in IJCAI 2001 workshop on empirical methods in\nartiﬁcial intelligence, vol. 3, no. 22.\nIBM New York, 2001, pp. 41–46.\n[3] D. D. Lewis, “Naive (bayes) at forty: The independence assumption in information retrieval,” in Machine\nLearning: ECML-98, C. Nédellec and C. Rouveirol, Eds.\nBerlin, Heidelberg: Springer Berlin Heidelberg,\n1998, pp. 4–15.\n[4] J. Suykens, L. Lukas, P. V. Dooren, B. D. Moor, and J. Vandewalle, “Least squares support vector machine\nclassiﬁers: a large scale algorithm,” 1999.\n[5] G. Cauwenberghs and T. A. Poggio, “Incremental and decremental support vector machine learning,” in NIPS,\nT. K. Leen, T. G. Dietterich, and V. Tresp, Eds.\nMIT Press, 2000, pp. 409–415.\n[6] L. Breiman, “Random forests,” Machine Learning, vol. 45, no. 1, pp. 5–32, 2001.\n[7] P. O. Gislason, J. A. Benediktsson, and J. R. Sveinsson, “Random forests for land cover classiﬁcation,” Pattern\nRecognition Letters, vol. 27, no. 4, pp. 294 – 300, 2006.\n[8] S. R. Safavian and D. Landgrebe, “A survey of decision tree classiﬁer methodology,” IEEE Transactions on\nSystems, Man, and Cybernetics, vol. 21, no. 3, pp. 660–674, 1991.\n[9] M. Friedl and C. Brodley, “Decision tree classiﬁcation of land cover from remotely sensed data,” Remote Sens-\ning of Environment, vol. 61, no. 3, pp. 399 – 409, 1997.\n[10] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, “Deep learning for computer vision: A\nbrief review,” Computational Intelligence and Neuroscience, vol. 2018, pp. 1–13, 02 2018.\n[11] L. Deng, G. Hinton, and B. Kingsbury, “New types of deep neural network learning for speech recognition and\nrelated applications: an overview,” in 2013 IEEE International Conference on Acoustics, Speech and Signal\nProcessing, 2013, pp. 8599–8603.\n[12] H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, and R. Ward, “Deep sentence embedding using\nlong short-term memory networks: Analysis and application to information retrieval,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing, vol. 24, no. 4, pp. 694–707, 2016.\n16\nA PREPRINT - JUNE 11, 2020\n[13] A. SINGH, “Review article digital change detection techniques using remotely-sensed data,” International Jour-\nnal of Remote Sensing, vol. 10, no. 6, pp. 989–1003, 1989.\n[14] M. Gong, H. Yang, and P. Zhang, “Feature learning and change feature classiﬁcation based on deep learning for\nternary change detection in sar images,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 129, pp.\n212 – 225, 2017.\n[15] R. Liu, D. Jiang, L. Zhang, and Z. Zhang, “Deep depthwise separable convolutional network for change de-\ntection in optical aerial images,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote\nSensing, vol. 13, pp. 1109–1118, 2020.\n[16] F. Bovolo and L. Bruzzone, “A split-based approach to unsupervised change detection in large-size multitem-\nporal images: Application to tsunami-damage assessment,” IEEE Transactions on Geoscience and Remote\nSensing, vol. 45, no. 6, pp. 1658–1670, 2007.\n[17] P. Coppin, I. Jonckheere, K. Nackaerts, B. Muys, and E. Lambin, “Digital change detection methods in ecosys-\ntem monitoring: a review,” International Journal of Remote Sensing, vol. 25, no. 9, pp. 1565–1596, 2004.\n[18] J. Feranec, G. Hazeu, S. Christensen, and G. Jaffrain, “Corine land cover change detection in europe (case\nstudies of the netherlands and slovakia),” Land Use Policy, vol. 24, no. 1, pp. 234 – 247, 2007.\n[19] C. M. Viana, S. Oliveira, S. C. Oliveira, and J. Rocha, “29 - land use/land cover change detection and urban\nsprawl analysis,” in Spatial Modeling in GIS and R for Earth and Environmental Sciences, H. R. Pourghasemi\nand C. Gokceoglu, Eds.\nElsevier, 2019, pp. 621 – 651.\n[20] “Deep learning in remote sensing applications: A meta-analysis and review,” ISPRS Journal of Photogrammetry\nand Remote Sensing, vol. 152, pp. 166 – 177, 2019.\n[21] X. X. Zhu, D. Tuia, L. Mou, G. Xia, L. Zhang, F. Xu, and F. Fraundorfer, “Deep learning in remote sensing: A\ncomprehensive review and list of resources,” IEEE Geoscience and Remote Sensing Magazine, vol. 5, no. 4, pp.\n8–36, 2017.\n[22] L. Zhang, L. Zhang, and B. Du, “Deep learning for remote sensing data: A technical tutorial on the state of the\nart,” IEEE Geoscience and Remote Sensing Magazine, vol. 4, no. 2, pp. 22–40, 2016.\n[23] S. Liu, L. Bruzzone, F. Bovolo, and P. Du, “Hierarchical unsupervised change detection in multitemporal hy-\nperspectral images,” IEEE Transactions on Geoscience and Remote Sensing, vol. 53, no. 1, pp. 244–260, 2015.\n[24] G. Yang, H. Li, W. Wang, W. Yang, and W. J. Emery, “Unsupervised change detection based on a uniﬁed\nframework for weighted collaborative representation with rddl and fuzzy clustering,” IEEE Transactions on\nGeoscience and Remote Sensing, vol. 57, no. 11, pp. 8890–8903, 2019.\n[25] R. Liu, Z. Cheng, L. Zhang, and J. Li, “Remote sensing image change detection based on information transmis-\nsion and attention mechanism,” IEEE Access, vol. 7, pp. 156 349–156359, 2019.\n[26] K. L. de Jong and A. S. Bosman, “Unsupervised change detection in satellite images using convolutional neural\nnetworks,” CoRR, vol. abs/1812.05815, 2018.\n[27] N. Kadhim, M. Mourshed, and M. Bray, “Advances in remote sensing applications for urban sustainability,”\nEuro-Mediterranean Journal for Environmental Integration, vol. 1, no. 7, 2016.\n[28] Z. Dianjun and Z. Guoqing, “Estimation of soil moisture from optical and thermal remote sensing: A review,”\nSensors (Basel), vol. 16, no. 8, 2016.\n[29] J. Sublime and E. Kalinicheva, “Automatic post-disaster damage mapping using deep-learning techniques for\nchange detection: Case study of the tohoku tsunami,” Remote Sensing, vol. 11, no. 9, p. 1123, May 2019.\n[30] M. Kolos, A. Marin, A. Artemov, and E. Burnaev, “Procedural synthesis of remote sensing images for robust\nchange detection with neural networks,” in Advances in Neural Networks – ISNN 2019, H. Lu, H. Tang, and\nZ. Wang, Eds.\nCham: Springer International Publishing, 2019, pp. 371–387.\n[31] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning.\nThe MIT Press, 2016.\n[32] R. D. Reed and R. J. Marks, Neural Smithing: Supervised Learning in Feedforward Artiﬁcial Neural Networks.\nMIT Press, 1999.\n[33] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent in nervous activity,” Bulletin of\nMathematical Biophysics, vol. 5, pp. 115–133, 1943.\n[34] D. O. Hebb, The Organization of Behavior: A Neuropsychological Theory.\nNew York: Wiley, 1949.\n[35] F. Rosenblatt, “The perceptron: A probabilistic model for information storage and organization in the brain.”\nPsychological Review, vol. 65, no. 6, pp. 386–408, 1958.\n17\nA PREPRINT - JUNE 11, 2020\n[36] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning Representations by Back-propagating Errors,”\nNature, vol. 323, no. 6088, pp. 533–536, 1986.\n[37] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Neurocomputing: Foundations of research,” J. A. Anderson\nand E. Rosenfeld, Eds.\nCambridge, MA, USA: MIT Press, 1988, ch. Learning Representations by Back-\npropagating Errors, pp. 696–699.\n[38] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of data with neural networks,” Science,\nvol. 313, pp. 504 – 507, 2006.\n[39] Y. Bengio and Y. LeCun, “Scaling learning algorithms towards ai,” in Large Scale Kernel Machines, L. Bottou,\nO. Chapelle, D. DeCoste, and J. Weston, Eds.\nCambridge, MA: MIT Press, 2007.\n[40] A. Mohamed, G. E. Dahl, and G. Hinton, “Acoustic modeling using deep belief networks,” IEEE Transactions\non Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 14–22, 2012.\n[41] A. Mohamed, T. N. Sainath, G. Dahl, B. Ramabhadran, G. E. Hinton, and M. A. Picheny, “Deep belief networks\nusing discriminative features for phone recognition,” in 2011 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2011, pp. 5060–5063.\n[42] S. Li, W. Song, L. Fang, Y. Chen, P. Ghamisi, and J. A. Benediktsson, “Deep learning for hyperspectral image\nclassiﬁcation: An overview,” IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 9, pp. 6690–\n6709, 2019.\n[43] C. Buckner and J. Garson, “Connectionism,” E. N. Zalta (Ed.), The stanford encyclopedia of philosophy\n(Fall 2019). Stanford:\nMetaphysics Research Lab,\nStanford University.,\n2019. [Online]. Available:\nhttps://plato.stanford.edu/entries/connectionism/\n[44] J. Zabalza, J. Ren, J. Zheng, H. Zhao, C. Qing, Z. Yang, P. Du, and S. Marshall, “Novel segmented stacked\nautoencoder for effective dimensionality reduction and feature extraction in hyperspectral imaging,” Neurocom-\nputing, vol. 185, pp. 1 – 10, 2016.\n[45] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, “Backpropagation\napplied to handwritten zip code recognition,” Neural Computation, vol. 1, pp. 541–551, 1989.\n[46] L. Zhang, F. Yang, Y. Daniel Zhang, and Y. J. Zhu, “Road crack detection using deep convolutional neural\nnetwork,” in 2016 IEEE International Conference on Image Processing (ICIP), Sep. 2016, pp. 3708–3712.\n[47] Y. Rikiya, D. R. K. Gian, and T. Kaori, “Convolutional neural networks: an overview and application in radiol-\nogy,” Insights into Imaging, vol. 9, pp. 115–133, 2018.\n[48] Y. Ito, “Representation of functions by superpositions of a step or sigmoid function and their applications to\nneural network theory,” Neural Networks, vol. 4, no. 3, pp. 385 – 394, 1991.\n[49] G. A. Anastassiou, “Univariate hyperbolic tangent neural network approximation,” Mathematical and Computer\nModelling, vol. 53, no. 5, pp. 1111 – 1132, 2011.\n[50] F. Agostinelli, M. D. Hoffman, P. J. Sadowski, and P. Baldi, “Learning activation functions to improve deep\nneural networks,” CoRR, vol. abs/1412.6830, 2014.\n[51] B. Xu, N. Wang, T. Chen, and M. Li, “Empirical Evaluation of Rectiﬁed Activations in Convolutional Network,”\n2015.\n[52] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei, “Large-scale video classiﬁcation\nwith convolutional neural networks,” in Proceedings of International Computer Vision and Pattern Recognition\n(CVPR 2014), 2014.\n[53] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural net-\nworks,” in Advances in Neural Information Processing Systems 25, F. Pereira, C. J. C. Burges, L. Bottou, and\nK. Q. Weinberger, Eds.\nCurran Associates, Inc., 2012, pp. 1097–1105.\n[54] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in\nInternational Conference on Learning Representations, 2015.\n[55] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June 2016, pp. 770–778.\n[56] J. Jordan, “Common architectures in convolutional neural networks,” in https://www.jeremyjordan.me/convnet-\narchitectures/, 2018.\n[57] Z. C. Lipton, J. Berkowitz, and C. Elkan, “A critical review of recurrent neural networks for sequence learning,”\n2015.\n18\nA PREPRINT - JUNE 11, 2020\n[58] I. H. Witten, E. Frank, and M. A. Hall, Data Mining: Practical Machine Learning Tools and Techniques, 3rd ed.,\nser. Morgan Kaufmann Series in Data Management Systems.\nAmsterdam: Morgan Kaufmann, 2011.\n[59] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, no. 8, pp. 1735–\n1780, 1997.\n[60] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of gated recurrent neural networks on\nsequence modeling,” 2014.\n[61] C. Junyoung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of gated recurrent neural networks on\nsequence modeling,” in NIPS 2014 Workshop on Deep Learning, December 2014, 2014.\n[62] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-\ngio, “Generative adversarial nets,” in Advances in Neural Information Processing Systems 27, Z. Ghahramani,\nM. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds.\nCurran Associates, Inc., 2014, pp.\n2672–2680.\n[63] R. Jaturapitpornchai, M. Matsuoka, N. Kanemoto, S. Kuzuoka, R. Ito, and R. Nakamura, “Newly built construc-\ntion detection in sar images using deep learning,” Remote Sensing, vol. 11, no. 12, pp. 1–24, Jun 2019.\n[64] M. Yang, L. Jiao, F. Liu, B. Hou, and S. Yang, “Transferred deep learning-based change detection in remote\nsensing images,” IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 9, pp. 6960–6973, Sep.\n2019.\n[65] M. Gong, J. Zhao, J. Liu, Q. Miao, and L. Jiao, “Change detection in synthetic aperture radar images based\non deep neural networks,” IEEE Transactions on Neural Networks and Learning Systems, vol. 27, no. 1, pp.\n125–138, 2016.\n[66] H. Lyu, H. Lu, and L. Mou, “Learning a transferable change rule from a recurrent neural network for land cover\nchange detection,” Remote Sensing, vol. 8, no. 6, p. 506, Jun 2016.\n[67] P. Zhang, M. Gong, L. Su, J. Liu, and Z. Li, “Change detection based on deep feature representation and\nmapping transformation for multi-spatial-resolution remote sensing images,” ISPRS Journal of Photogrammetry\nand Remote Sensing, vol. 116, pp. 24 – 41, 2016.\n[68] Z. Zhao and H. Liu, “Spectral feature selection for supervised and unsupervised learning,” in ICML ’07, 2007.\n[69] Y.-A. Chung, H.-Y. Lee, and J. Glass, “Supervised and unsupervised transfer learning for question answering,”\nin Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, vol. 1.\nNew Orleans, Louisiana: Association for Computational\nLinguistics, Jun. 2018, pp. 1585–1594.\n[70] Y. Bengio, “Deep learning of representations: Looking forward,” in Statistical Language and Speech Pro-\ncessing, A.-H. Dediu, C. Martín-Vide, R. Mitkov, and B. Truthe, Eds.\nBerlin, Heidelberg: Springer Berlin\nHeidelberg, 2013, pp. 1–37.\n[71] O. A. B. Penatti, K. Nogueira, and J. A. dos Santos, “Do deep features generalize from everyday objects\nto remote sensing and aerial scenes domains?” in 2015 IEEE Conference on Computer Vision and Pattern\nRecognition Workshops (CVPRW), June 2015, pp. 44–51.\n[72] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,”\nin Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, N. Navab, J. Hornegger,\nW. M. Wells, and A. F. Frangi, Eds.\nCham: Springer International Publishing, 2015, pp. 234–241.\n[73] Z. M. Hamdi, M. Brandmeier, and C. Straub, “Forest Damage Assessment Using Deep Learning on High\nResolution Remote Sensing Data,” Remote Sensing, vol. 11, no. 17, SEP 1 2019.\n[74] D. Peng, Y. Zhang, and H. Guan, “End-to-End Change Detection for High Resolution Satellite Images Using\nImproved UNet plus,” Remote Sensing, vol. 11, no. 11, JUN 1 2019.\n[75] Q. Wang, Z. Yuan, Q. Du, and X. Li, “GETNET: A General End-to-End 2-D CNN Framework for Hyperspectral\nImage Change Detection,” IEEE Transactions on Geoscience and Remote Sensing , vol. 57, no. 1, pp. 3–13, JAN\n2019.\n[76] W. A. Malila, “Change vector analysis: An approach for detecting forest changes with landsat,” in LARS Symp.,\n1980.\n[77] W. Wiratama, J. Lee, S.-E. Park, and D. Sim, “Dual-Dense Convolution Network for Change Detection of\nHigh-Resolution Panchromatic Imagery,” Applied Sciences-Basel, vol. 8, no. 10, OCT 2018.\n[78] C. Zhang, S. Wei, S. Ji, and M. Lu, “Detecting Large-Scale Urban Land Cover Changes from Very High\nResolution Remote Sensing Images Using CNN-Based Classiﬁcation,” ISPRS International Journal of Geo-\nInformation, vol. 8, no. 4, APR 2019.\n19\nA PREPRINT - JUNE 11, 2020\n[79] R. C. Daudt, B. L. Saux, A. Boulch, and Y. Gousseau, “Multitask learning for large-scale semantic change\ndetection,” Computer Vision and Image Understanding, vol. 187, p. 102783, 2019.\n[80] W. Zhang and X. Lu, “The Spectral-Spatial Joint Learning for Change Detection in Multispectral Imagery,”\nRemote Sensing, vol. 11, no. 3, FEB 1 2019.\n[81] Z. Zhang, G. Vosselman, M. Gerke, D. Tuia, and M. Y. Yang, “Change detection between multimodal remote\nsensing data using siamese CNN,” CoRR, vol. abs/1807.09562, 2018.\n[82] X. Cao, Y. Ji, L. Wang, B. Ji, L. Jiao, and J. Han, “Sar image change detection based on deep denoising and\ncnn,” IET Image Processing, vol. 13, no. 9, pp. 1509–1515, 2019.\n[83] W. Wiratama and D. Sim, “Fusion Network for Change Detection of High-Resolution Panchromatic Imagery,”\nApplied Sciences-Basel, vol. 9, no. 7, APR 1 2019.\n[84] C. Cao, S. Dragicevic, and S. Li, “Land-use change detection with convolutional neural network methods,”\nEnvironments, vol. 6, 2019.\n[85] N. Lv, C. Chen, T. Qiu, and A. K. Sangaiah, “Deep learning and superpixel feature extraction based on con-\ntractive autoencoder for change detection in sar images,” IEEE Transactions on Industrial Informatics, vol. 14,\nno. 12, pp. 5530–5538, Dec 2018.\n[86] H. Zhang, M. Gong, P. Zhang, L. Su, and J. Shi, “Feature-level change detection using deep representation\nand feature change analysis for multispectral imagery,” IEEE Geoscience and Remote Sensing Letters, vol. 13,\nno. 11, pp. 1666–1670, Nov 2016.\n[87] L. Su, M. Gong, P. Zhang, M. Zhang, J. Liu, and H. Yang, “Deep learning and mapping based ternary change\ndetection for information unbalanced images,” Pattern Recognition, vol. 66, pp. 213 – 228, 2017.\n[88] F. Gao, X. Liu, J. Dong, G. Zhong, and M. Jian, “Change detection in sar images based on deep semi-nmf and\nsvd networks,” Remote Sensing, vol. 9, p. 435, 05 2017.\n[89] Y. Wang and Y. Zhang, “Nonnegative matrix factorization: A comprehensive review,” IEEE Transactions on\nKnowledge and Data Engineering, vol. 25, no. 6, pp. 1336–1353, 2013.\n[90] J. Xue, J. Li, and Y. Gong, “Restructuring of deep neural network acoustic models with singular value decom-\nposition.” in INTERSPEECH, F. Bimbot, C. Cerisara, C. Fougeron, G. Gravier, L. Lamel, F. Pellegrino, and\nP. Perrier, Eds.\nISCA, 2013, pp. 2365–2369.\n[91] M. Gong, Y. Yang, T. Zhan, X. Niu, and S. Li, “A generative discriminatory classiﬁed network for change\ndetection in multispectral imagery,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote\nSensing, vol. 12, no. 1, pp. 321–333, Jan 2019.\n[92] J. Geng, X. Ma, X. Zhou, and H. Wang, “Saliency-guided deep neural networks for sar image change detection,”\nIEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 10, pp. 7365–7377, 2019.\n[93] A. B. Geva, “Hierarchical unsupervised fuzzy clustering,” IEEE Transactions on Fuzzy Systems, vol. 7, no. 6,\npp. 723–733, 1999.\n[94] X. Li, Z. Yuan, and Q. Wang, “Unsupervised deep noise modeling for hyperspectral image change detection,”\nRemote Sensing, vol. 11, no. 3, 2019.\n[95] F. Huang, Y. Yu, and T. Feng, “Automatic building change image quality assessment in high resolution re-\nmote sensing based on deep learning,” Journal of Visual Communication and Image Representation, vol. 63, p.\n102585, 2019.\n[96] S. Suresh, R. V. Babu, and H. J. Kim, “No-reference image quality assessment using modiﬁed extreme learning\nmachine classiﬁer,” Applied Soft Computing, vol. 9, no. 2, pp. 541 – 552, 2009.\n[97] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transactions on Knowledge and Data Engineering,\nvol. 22, no. 10, pp. 1345–1359, 2010.\n[98] K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A survey of transfer learning,” Journal of Big Data, vol. 3, 2016.\n[99] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep learning: A review,” 2018.\n[100] A. Abdalla, H. Cen, L. Wan, R. Rashid, H. Weng, W. Zhou, and Y. He, “Fine-tuning convolutional neural\nnetwork with transfer learning for semantic segmentation of ground-level oilseed rape images in a ﬁeld with\nhigh weed pressure,” Computers and Electronics in Agriculture, vol. 167, p. 105091, 2019.\n[101] B. Hou, Y. Wang, and Q. Liu, “Change detection based on deep features and low rank,” IEEE Geoscience and\nRemote Sensing Letters, vol. PP, pp. 1–5, 11 2017.\n20\nA PREPRINT - JUNE 11, 2020\n[102] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A Large-Scale Hierarchical Image\nDatabase,” in CVPR09, 2009.\n[103] G. Xia, J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong, L. Zhang, and X. Lu, “Aid: A benchmark data set for performance\nevaluation of aerial scene classiﬁcation,” IEEE Transactions on Geoscience and Remote Sensing, vol. 55, no. 7,\npp. 3965–3981, 2017.\n[104] N. Venugopal, “Sample selection based change detection with dilated network learning in remote sensing im-\nages,” Sensing and Imaging, vol. 20, 12 2019.\n[105] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” CoRR, vol.\nabs/1512.03385, 2015.\n[106] M. Zhang, G. Xu, K. Chen, M. Yan, and X. Sun, “Triplet-based semantic relation learning for aerial remote\nsensing image change detection,” IEEE Geoscience and Remote Sensing Letters, vol. 16, no. 2, pp. 266–270,\nFeb 2019.\n[107] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab: Semantic image segmentation\nwith deep convolutional nets, atrous convolution, and fully connected crfs,” CoRR, vol. abs/1606.00915, 2016.\n[108] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes\n(voc) challenge,” International Journal of Computer Vision, vol. 88, no. 2, pp. 303–338, 2010.\n[109] B. Fang, L. Pan, and R. Kou, “Dual learning-based siamese framework for change detection using bi-temporal\nvhr optical remote sensing images,” Remote Sensing, vol. 11, p. 1292, 2019.\n[110] Y. Gao, F. Gao, J. Dong, and S. Wang, “Transferred deep learning for sea ice change detection from synthetic-\naperture radar images,” IEEE Geoscience and Remote Sensing Letters, vol. 16, no. 10, pp. 1655–1659, Oct\n2019.\n[111] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.\nFidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra,\nS. Legg, and D. Hassabis, “Human-level control through deep reinforcement learning,” Nature, vol. 518, pp.\n529–533, 2015.\n[112] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, “Target-driven visual navigation\nin indoor scenes using deep reinforcement learning,” in 2017 IEEE International Conference on Robotics and\nAutomation (ICRA), 2017, pp. 3357–3364.\n[113] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning for multiagent systems: A review\nof challenges, solutions, and applications,” IEEE Transactions on Cybernetics, pp. 1–14, 2020.\n[114] C. Hoel, K. Driggs-Campbell, K. Wolff, L. Laine, and M. Kochenderfer, “Combining planning and deep re-\ninforcement learning in tactical decision making for autonomous driving,” IEEE Transactions on Intelligent\nVehicles, pp. 1–12, 2019.\n[115] Y. Dai, D. Xu, K. Zhang, S. Maharjan, and Y. Zhang, “Deep reinforcement learning and permissioned\nblockchain for content caching in vehicular edge computing and networks,” IEEE Transactions on Vehicular\nTechnology, vol. 69, no. 4, pp. 4312–4324, 2020.\n[116] Z. Teng, B. Zhang, and J. Fan, “Three-step action search networks with deep q-learning for real-time object\ntracking,” Pattern Recognition, vol. 101, pp. 1–11, 2020.\n[117] W. Luo, P. Sun, F. Zhong, W. Liu, T. Zhang, and Y. Wang, “End-to-end active object tracking and its real-world\ndeployment via reinforcement learning,” IEEE Transactions on Pattern Analysis and Machine Intelligence, pp.\n1–14, 2019.\n[118] Z. Liu, J. Wang, S. Gong, H. Lu, and D. Tao, “Deep reinforcement active learning for human-in-the-loop person\nre-identiﬁcation,” in The IEEE International Conference on Computer Vision (ICCV), October 2019.\n[119] W. Zhang, X. He, W. Lu, H. Qiao, and Y. Li, “Feature aggregation with reinforcement learning for video-based\nperson re-identiﬁcation,” IEEE Transactions on Neural Networks and Learning Systems, vol. 30, no. 12, pp.\n3847–3852, 2019.\n[120] A. Shrestha and A. Mahmood, “Review of deep learning algorithms and architectures,” IEEE Access, vol. 7, pp.\n53 040–53065, 2019.\n[121] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, “Asyn-\nchronous methods for deep reinforcement learning,” in Proceedings of The 33rd International Conference on\nMachine Learning, ser. Proceedings of Machine Learning Research, M. F. Balcan and K. Q. Weinberger, Eds.,\nvol. 48.\nNew York, New York, USA: PMLR, 20–22 Jun 2016, pp. 1928–1937.\n21\nA PREPRINT - JUNE 11, 2020\n[122] K. Fu, Y. D. Li, H. Sun, X. Yang, G. Xu, Y. Li, and X. Sun, “A ship rotation detection model in remote sensing\nimages based on feature fusion pyramid network and deep reinforcement learning,” Remote Sensing, vol. 10,\npp. 1–26, 2018.\n[123] Y. Li, K. Fu, H. Sun, and X. Sun, “An aircraft detection framework based on reinforcement learning and\nconvolutional neural networks in remote sensing images,” Remote Sensing, vol. 10, pp. 1–17, 2018.\n[124] J. Peng and R. J. Williams, “Incremental multi-step q-learning,” in Machine Learning Proceedings 1994, W. W.\nCohen and H. Hirsh, Eds.\nSan Francisco (CA): Morgan Kaufmann, 1994, pp. 226 – 232.\n[125] Z.-H. Zhou, “A brief introduction to weakly supervised learning,” National Science Review, vol. 5, no. 1, pp.\n44–53, 08 2017.\n[126] G. Cheng and J. Han, “A survey on object detection in optical remote sensing images,” ISPRS Journal of\nPhotogrammetry and Remote Sensing, vol. 117, pp. 11 – 28, 2016.\n[127] Y. Li, L. Guo, and Z. Zhou, “Towards safe weakly supervised learning,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, pp. 1–14, 2019.\n[128] P. Zhou, G. Cheng, Z. Liu, S. Bu, and X. Hu, “Weakly supervised target detection in remote sensing images\nbased on transferred deep features and negative bootstrapping,” Multidimensional Systems and Signal Process-\ning, vol. 27, pp. 925–944, 2016.\n[129] P. Zhou, D. Zhang, G. Cheng, and J. Han, “Negative bootstrapping for weakly supervised target detection in\nremote sensing images,” in 2015 IEEE International Conference on Multimedia Big Data, 2015, pp. 318–323.\n[130] H. Jiang, X. Hu, K. Li, J. Zhang, J. Gong, and M. Zhang, “Pga-siamnet: Pyramid feature-based attention-guided\nsiamese network for remote sensing orthoimagery building change detection,” Remote Sensing, vol. 12, no. 3,\n2020.\n22\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-06-10",
  "updated": "2020-06-10"
}