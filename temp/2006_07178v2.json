{
  "id": "http://arxiv.org/abs/2006.07178v2",
  "title": "Meta-Reinforcement Learning Robust to Distributional Shift via Model Identification and Experience Relabeling",
  "authors": [
    "Russell Mendonca",
    "Xinyang Geng",
    "Chelsea Finn",
    "Sergey Levine"
  ],
  "abstract": "Reinforcement learning algorithms can acquire policies for complex tasks\nautonomously. However, the number of samples required to learn a diverse set of\nskills can be prohibitively large. While meta-reinforcement learning methods\nhave enabled agents to leverage prior experience to adapt quickly to new tasks,\ntheir performance depends crucially on how close the new task is to the\npreviously experienced tasks. Current approaches are either not able to\nextrapolate well, or can do so at the expense of requiring extremely large\namounts of data for on-policy meta-training. In this work, we present model\nidentification and experience relabeling (MIER), a meta-reinforcement learning\nalgorithm that is both efficient and extrapolates well when faced with\nout-of-distribution tasks at test time. Our method is based on a simple\ninsight: we recognize that dynamics models can be adapted efficiently and\nconsistently with off-policy data, more easily than policies and value\nfunctions. These dynamics models can then be used to continue training policies\nand value functions for out-of-distribution tasks without using\nmeta-reinforcement learning at all, by generating synthetic experience for the\nnew task.",
  "text": "Meta-Reinforcement Learning Robust to\nDistributional Shift via Model Identiﬁcation and\nExperience Relabeling\nRussell Mendonca * 1, Xinyang Geng * 1, Chelsea Finn 2, Sergey Levine 1\n1 University of California, Berkeley 2 Stanford University\n{russellm, young.geng}@berkeley.edu\ncbfinn@cs.stanford.edu, svlevine@eecs.berkeley.edu\nAbstract\nReinforcement learning algorithms can acquire policies for complex tasks au-\ntonomously. However, the number of samples required to learn a diverse set of\nskills can be prohibitively large. While meta-reinforcement learning methods have\nenabled agents to leverage prior experience to adapt quickly to new tasks, their\nperformance depends crucially on how close the new task is to the previously\nexperienced tasks. Current approaches are either not able to extrapolate well, or\ncan do so at the expense of requiring extremely large amounts of data for on-policy\nmeta-training. In this work, we present model identiﬁcation and experience rela-\nbeling (MIER), a meta-reinforcement learning algorithm that is both efﬁcient and\nextrapolates well when faced with out-of-distribution tasks at test time. Our method\nis based on a simple insight: we recognize that dynamics models can be adapted\nefﬁciently and consistently with off-policy data, more easily than policies and value\nfunctions. These dynamics models can then be used to continue training policies\nand value functions for out-of-distribution tasks without using meta-reinforcement\nlearning at all, by generating synthetic experience for the new task.\n1\nIntroduction\nRecent advances in reinforcement learning (RL) have enabled agents to autonomously acquire policies\nfor complex tasks, particularly when combined with high-capacity representations such as neural\nnetworks [16, 29, 20, 15]. However, the number of samples required to learn these tasks is often very\nlarge. Meta-reinforcement learning (meta-RL) algorithms can alleviate this problem by leveraging\nexperience from previously seen related tasks [4, 36, 7], but the performance of these methods\non new tasks depends crucially on how close these tasks are to the meta-training task distribution.\nMeta-trained agents can adapt quickly to tasks that are similar to those seen during meta-training, but\nlose much of their beneﬁt when adapting to tasks that are too far away from the meta-training set.\nThis places a signiﬁcant burden on the user to carefully construct meta-training task distributions that\nsufﬁciently cover the kinds of tasks that may be encountered at test time.\nMany meta-RL methods either utilize a variant of model-agnostic meta-learning (MAML) and adapt\nto new tasks with gradient descent [7, 24, 21], or use an encoder-based formulation that adapt\nby encoding experience with recurrent models [4, 36], attention mechanisms [19] or variational\ninference [23]. The latter class of methods generally struggle when adapting to out-of-distribution\ntasks, because the adaptation procedure is entirely learned and carries no performance guarantees with\nout-of-distribution inputs (as with any learned model). Methods that utilize gradient-based adaptation\nhave the potential of handling out-of-distribution tasks more effectively, since gradient descent\ncorresponds to a well-deﬁned and consistent learning process that has a guarantee of improvement\n* Equal contribution. Preprint. Under review.\narXiv:2006.07178v2  [cs.LG]  15 Jun 2020\nFigure 1: Overview of our approach. The model context variable (φ) is adapted using gradient descent, and the\nadapted context variable (φT ) is fed to the policy alongside state so the policy can be trained with standard RL\n(Model Identiﬁcation). The adapted model is used to relabel the data from other tasks by predicting next state\nand reward, generating synthetic experience to continue improving the policy (Experience Relabeling).\nregardless of the task [6]. However, in the RL setting, these methods [7, 24] utilize on-policy\npolicy gradient methods for meta-training, which require a very large number of samples during\nmeta-training [23].\nIn this paper, we aim to develop a meta-RL algorithm that can both adapt effectively to out-of-\ndistribution tasks and be meta-trained efﬁciently via off-policy value-based algorithms. One straight-\nforward idea might be to directly develop a value-based off-policy meta-RL method that uses\ngradient-based meta-learning. However, this is very difﬁcult, since the ﬁxed point iteration used\nin value-based RL algorithms does not correspond to gradient descent, and to our knowledge no\nprior method has successfully adapted MAML to the off-policy value-based setting. We further\ndiscuss this difﬁculty in Appendix D. Instead, we propose to leverage a simple insight: dynamics\nand reward models can be adapted consistently, using gradient based update rules with off-policy\ndata, even if policies and value functions cannot. These models can then be used to train policies for\nout-of-distribution tasks without using meta-RL at all, by generating synthetic experience for the new\ntasks.\nBased on this observation, we propose model identiﬁcation and experience relabeling (MIER), a\nmeta-RL algorithm that makes use of two independent novel concepts: model identiﬁcation and\nexperience relabeling. Model identiﬁcation refers to the process of identifying a particular task from\na distribution of tasks, which requires determining its transition dynamics and reward function. We\nuse a gradient-based supervised meta-learning method to learn a dynamics and reward model and a\n(latent) model context variable such that the model quickly adapts to new tasks after a few steps of\ngradient descent on the context variable. The context variable must contain sufﬁcient information\nabout the task to accurately predict dynamics and rewards. The policy can then be conditioned on\nthis context [27, 13] and therefore does not need to be meta-trained or adapted. Hence it can be\nlearned with any standard RL algorithm, avoiding the complexity of meta-reinforcement learning.\nWe illustrate the model identiﬁcation process in the left part of Figure 1.\nWhen adapting to out-of-distribution tasks at meta-test time, the adapted context variable may itself be\nout of distribution, and the context-conditioned policy might perform poorly. However, since MIER\nadapts the model with gradient descent, we can continue to improve the model using more gradient\nsteps. To continue improving the policy, we leverage all data collected from other tasks during\nmeta-training, by using the learned model to relabel the next state and reward on every previously\nseen transition, obtaining synthetic data to continue training the policy. We call this process, shown in\nthe right part of Figure 1, experience relabeling. This enables MIER to adapt to tasks outside of the\nmeta-training distribution, outperforming prior meta-reinforcement learning methods in this setting.\n2\nPreliminaries\nFormally, the reinforcement learning problem is deﬁned by a Markov decision process (MDP). We\nadopt the standard deﬁnition of an MDP, T = (S, A, p, µ0, r, γ), where S is the state space, A is the\naction space, p(s′|s, a) is the unknown transition probability of reaching state s′ at the next time step\nwhen an agent takes action a at state s, µ0(s) is the initial state distribution, r(s, a) is the reward\n2\nfunction, and γ ∈(0, 1) is the discount factor. An agent acts according to some policy π(a|s) and\nthe learning objective is to maximize the expected return, Est,at∼π[P\nt γtr(st, at)].\nWe further deﬁne the meta-reinforcement learning problem. Meta-training uses a distribution over\nMDPs, ρ(T ), from which tasks are sampled. Given a speciﬁc task T , the agent is allowed to collect a\nsmall amount of data D(T )\nadapt, and adapt the policy to obtain πT . The objective of meta-training is to\nmaximize the expected return of the adapted policy ET ∼ρ(T ),st,at∼πT [P\nt γtr(st, at)].\nMIER also makes use of a learned dynamics and reward model, similar to model-based reinforcement\nlearning. In model-based reinforcement learning, a model ˆp(s′, r|s, a) that predicts the reward and\nnext state from current state and action is trained using supervised learning. The model may then\nbe used to generate data to train a policy, using an objective similar to the RL objective above:\narg maxπ Est,at∼π,ˆp[P\nt γtr(st, at)]. Note that the expectation is now taken with respect to the\npolicy and learned model, rather than the policy and the true MDP transition function p(s′|s, a).\nIn order to apply model-based RL methods in meta-RL, we need to solve a supervised meta-learning\nproblem. We brieﬂy introduce the setup of supervised meta-learning and the model agnostic meta-\nlearning approach, which is an important foundation of our work. In supervised meta-learning,\nwe also have a distribution of tasks ρ(T ) similar to the meta-RL setup, except that the task T is\nnow a pair of input and output random variables (XT , YT ). Given a small dataset D(T )\nadapt sampled\nfrom a speciﬁc task T , the objective is to build a model that performs well on the evaluation data\nD(T )\neval sampled from the same task. If we denote our model as f(X; θ), the adaptation process as\nA(θ, D(T )\nadapt) and our loss function as L, the objective can be written as:\nmin\nf,A ET ∼ρ(T )\nh\nL\n\u0010\nf\n\u0010\nXT ; A\n\u0010\nθ, D(T )\nadapt\n\u0011\u0011\n, YT\n\u0011i\nModel agnostic meta-learning [7] is an approach to solve the supervised meta-learning problem.\nSpeciﬁcally, the model f(X; θ) is represented as a neural network, and the adaptation process is\nrepresented as few steps of gradient descent. For simplicity of notation, we only write out one step of\ngradient descent:\nAMAML\n\u0010\nθ, D(T )\nadapt\n\u0011\n= θ −α∇θEX,Y ∼D(T )\nadapt [L (f (X; θ) , Y )]\nThe training process of MAML can be summarized as optimizing the loss of the model after few\nsteps of gradient descent on data from the new task. Note that because AMAML is the standard gradient\ndescent operator, our model is guaranteed to improve under suitable smoothness conditions regardless\nof the task distribution ρ(T ), though adaptation to in-distribution tasks is likely to be substantially\nmore efﬁcient.\n3\nMeta Training with Model Identiﬁcation\nAs discussed in Section 1, MIER is built on top of two concepts, which we call model identiﬁcation\nand experience relabeling. We ﬁrst discuss how we can reformulate the meta-RL problem into a\nmodel identiﬁcation problem, where we train a fast-adapting model to rapidly identify the transition\ndynamics and reward function for a new task. We parameterize the model with a latent context\nvariable, which is meta-trained to encapsulate all of the task-speciﬁc information acquired during\nadaptation. We then train a universal policy that, conditioned on this context variable, can solve all of\nthe meta-training tasks. Training this policy is a standard RL problem instead of a meta-RL problem,\nso any off-the-shelf off-policy algorithms can be used. The resulting method can immediately be used\nto adapt to new in-distribution tasks, simply by adapting the model’s context via gradient descent,\nand conditioning the universal policy on this context. We illustrate the model identiﬁcation part of\nour algorithm in the left part of Figure 1 and provide pseudo-code for our meta-training procedure in\nAlgorithm 1.\nIn a meta-RL problem, where tasks are sampled from a distribution of MDPs, the only varying factors\nare the dynamics p and the reward function r. Therefore, a sufﬁcient condition for identifying the task\nis to learn the transition dynamics and the reward function, and this is exactly what model-based RL\nmethods do. Hence, we can naturally formulate the meta-task identiﬁcation problem as a model-based\nRL problem and solve it with supervised meta-learning methods.\n3\nSpeciﬁcally, we choose the MAML method for its simplicity and consistency. Unlike the standard\nsupervised MAML formulation, we condition our model on a latent context vector, and we only\nchange the context vector when adapting to new tasks. Since all task-speciﬁc information is thus\nencapsulated in the context vector, conditioning the policy on this context should provide it with\nsufﬁcient information to solve the task. This architecture is illustrated in the left part of Figure 1. We\ndenote the model as ˆp(s′, r|s, a; θ, φ), where θ is the neural network parameters and φ is the latent\ncontext vector that is passed in as input to the network.\nOne step of gradient adaptation can be written as follows:\nφT = AMAML\n\u0010\nθ, φ, D(T )\nadapt\n\u0011\n= φ −α∇φE(s,a,s′,r)∼D(T )\nadapt[−log ˆp (s′, r|s, a; θ, φ)].\nWe use the log likelihood as our objective for the probabilistic model. We then evaluate the model\nusing the adapted context vector φT , and minimize its loss on the evaluation dataset to learn the\nmodel. Speciﬁcally, we minimize the model meta-loss function Jˆp(θ, φ, D(T )\nadapt, D(T )\neval) to obtain the\noptimal parameter θ and context vector initialization φ:\narg min\nθ,φ Jˆp\n\u0010\nθ, φ, D(T )\nadapt, D(T )\neval\n\u0011\n= arg min\nθ,φ E(s,a,s′,r)∼D(T )\neval[−log ˆp (s′, r|s, a; θ, φT )]\nThe main difference between our method and previously proposed meta-RL methods that also use\ncontext variables [23, 4] is that we use gradient descent to adapt the context. Adaptation will be\nmuch faster for tasks that are in-distribution, since the meta-training process explicitly optimizes for\nthis objective, but the model will still adapt to out-of-distribution tasks given enough samples and\ngradient steps, since the adaptation process corresponds to a well-deﬁned and convergent learning\nprocess. However, for out-of-distribution tasks, the adapted context could be out-of-distribution for\nthe policy. We address this problem in Section 4.\nAlgorithm 1 Model Identiﬁcation Meta-Training\nInput: task distribution ρ(T ), training steps N,\nlearning rate α\nOutput: policy parameter ψ, model parameter θ, model\ncontext φ\nRandomly initialize ψ, θ, φ\nInitialize multitask replay buffer R(T ) ←∅\nwhile θ, φ, ψ not converged do\nSample task T ∼ρ(T )\nCollect D(T )\nadapt using πψ and φ\nCompute φT = AMAML(θ, φ, D(T )\nadapt)\nCollect D(T )\neval using π and φT\nR(T ) ←R(T ) ∪D(T )\nadapt ∪D(T )\neval\nfor i = 1 to N do\nSample task T ∼R\nSample D(T )\nadapt, D(T )\neval ∼R(T )\nUpdate θ ←θ−α∇θJˆp(θ, φ, D(T )\nadapt, D(T )\neval)\nUpdate ψ ←ψ −α∇ψJπ(ψ, D(T )\neval, φT )\nend\nend\nGiven the latent context variable from the\nadapted model φT , the meta-RL problem can be\neffectively reduced to a standard RL problem, as\nthe task speciﬁc information has been encoded\nin the context variable. We can therefore apply\nany standard model-free RL algorithm to obtain\na policy, as long as we condition the policy on\nthe latent context variable.\nIn our implementation, we utilize the soft actor-\ncritic (SAC) algorithm [9], though any efﬁcient\nmodel-free RL method could be used. We brieﬂy\nintroduce the policy optimization process for a\ngeneral actor-critic method. Let us parameterize\nour policy πψ by a parameter vector ψ. Actor-\ncritic methods maintain an estimate of the Q\nvalues for the current policy, Qπψ(s, a, φT ) =\nEst,at∼πψ[P\nt γtr(st, at)|s0 = s, a0 = a, T ],\nvia Bellman backups, and improve the policy\nby maximizing the expected Q values under the\npolicy, averaged over the dataset D. The policy\nloss can be written as:\nJπ(ψ, D, φT ) = −Es∼D,a∼π[Qπψ(s, a, φT )]\nNote that we condition our value function Qπψ(s, a, φT ) and policy πψ(a|s, φT ) on the adapted task\nspeciﬁc context vector φT , so that the policy and value functions are aware of which task is being\nperformed [27, 13]. Aside from incorporating the context φT , the actual RL algorithm is unchanged,\nand the main modiﬁcation is the concurrent meta-training of the model to produce φT .\n4\nImproving Out-of-Distribution Performance by Experience Relabeling\nAt meta-test time, when our method must adapt to a new unseen task T , it will ﬁrst sample a small\nbatch of data and obtain the latent context φT by running the gradient descent adaptation process on\n4\nthe context variable, using the model identiﬁcation process introduced in the previous section. While\nour model identiﬁcation method is already a complete meta-RL algorithm, it has no guarantees of\nconsistency. That is, it might not be able to adapt to out-of-distribution tasks, even with large amounts\nof data: although the gradient descent adaptation process for the model is consistent and will continue\nto improve, the context variable φT produced by this adaptation may still be out-of-distribution for\nthe policy when adapting to an out-of-distribution task. However, with an improved model, we can\ncontinue to train the policy with standard off-policy RL, by generating synthetic data using the model.\nIn practice we adapt the model for as many gradient steps as necessary, and then use this model to\ngenerate synthetic transitions using states from all previously seen meta-training tasks, with new\nsuccessor states and rewards. We call this process experience relabeling. Since the model is adapted\nvia gradient descent, it is guaranteed to eventually converge to a local optimum for any new task, even\na task that is outside the meta-training distribution. We illustrate the experience relabeling process in\nthe right part of Figure 1, and provide pseudo-code in Algorithm 2.\nAlgorithm 2 Experience Relabeling Adaptation\nInput: test task ˆT , multitask replay buffer R(T ),\nAdaptation steps for context Nfast, Training steps for\npolicy Np, Training steps for model Nm\nOutput: policy parameter ψ\nCollect D( ˆ\nT )\nadapt from ˆT using πψ and φ\nfor i = 1 to Nfast do\nUpdate φT according to Eq. 3\nend\nwhile ψ not converged do\nfor i = 1 to Np do\nSample T ∼R and D(T ) ∼R(T )\nˆD( ˆ\nT ) ←Relabel(D(T ), θ, φ ˆ\nT )\nTrain policy ψ ←ψ −α∇ψJπ(ψ, ˆD( ˆ\nT ), φT )\nend\nend\nWhen using data generated from a learned\nmodel to train a policy, the model’s predicted\ntrajectory often diverges from the real dynam-\nics after a large number of time steps, due to\naccumulated error [12]. We can mitigate this\nissue in the meta-RL case by leveraging all\nof the data from other tasks that was avail-\nable during meta-training. Although new task\nis previously unseen, the other training tasks\nshare the same state space and action space,\nand so we can leverage the large set of diverse\ntransitions collected from these tasks. Using\nthe adapted model and policy, we can relabel\nthese transitions, denoted (s, a, s′, r), by sam-\npling new actions with our adapted policy, and\nby sampling next states and rewards from the\nadapted model. The relabeling process can be\nwritten as:\nRelabel(D, θ, φT ) = {(s, a, s′, r)|s ∈D; a ∼π(a|s, φT ), (s′, r) ∼ˆp(s′, r|s, a; θ, φT )}.\nWe use these relabeled transitions to continue training the policy. The whole adaptation process\nis summarized in Algorithm 2. Since the learned model is only used to predict one time step into\nthe future, our approach does not suffer from compounding model errors. The MQL algorithm\n[5] also reuses data from other training tasks, but requires re-weighting them with an estimated\nimportance ratio instead of relabeling them with an adapted model. We will show empirically that\nour approach performs better on out-of-distribution tasks, and we also note that such reweighting\ncould be infeasible in some environments, which we discuss in Appendix C. We also note that our\nexperience relabeling method is a general tool for adapting to out-of-distribution tasks, and could be\nused independently of our model identiﬁcation algorithm. For example, we could apply a standard,\nnon-context-based dynamics and reward model to generate synthetic experience to ﬁnetune a policy\nobtained from any source, including other meta-RL methods.\n5\nRelated Work\nMeta-reinforcement learning algorithms extend the framework of meta-learning [28, 34, 22, 1] to\nthe reinforcement learning setting. Model-free encoder-based methods, encode the transitions seen\nduring adaptation into a latent context variable, and the policy is conditioned on this context to adapt it\nto the new task. The context encoding process is often done via a recurrent network [4, 36, 5, 30], an\nattention mechanism [19], or via amortized variational inference [23, 11]. While inference is efﬁcient\nfor handling in-distribution tasks (Fig. 2), it is not effective for adaptation to out-of-distribution\ntasks (Fig. 4). On the other hand, MIER can handle out-of-distribution tasks through the use of\na consistent gradient descent learner for the model, followed by a consistent (non-meta-trained)\noff-policy reinforcement learning method.\nModel-free gradient-based meta-RL methods [7, 24, 38, 25, 17, 8, 31, 10], implement gradient\ndescent as the adaptation process. However, they are based on on-policy RL algorithms, and thus\n5\nrequire a large number of samples for training and adaptation (Fig. 2). There are also works that\ncombine gradient-based and context-based methods [14]. However, such methods still suffer from\nthe same sample efﬁciency problem as other gradient based methods, because of the use of on-policy\npolicy gradients. Our method mitigates this problem by combining a gradient-based supervised\nmeta-learning algorithm with a regular RL algorithm to achieve better sample efﬁciency at meta-\ntraining time. There has been some work that uses off-policy policy gradients for sample efﬁcient\nmeta-training [18], but this still requires quite a few trajectories for policy gradient based adaptation\nat test time. MIER avoids this by reusing the experiences collected during training to enable fast\nadaptation with minimal amount of additional data.\nModel based meta-RL methods meta-train a model rather than a policy [21, 26]. At test time, when\nthe model is adapted to a particular task, standard planning techniques, such as model predictive\ncontrol [37, 3], are often applied to select actions. Unfortunately, purely model-based meta-RL\nmethods typically attain lower overall returns than their model-free counter-parts, particularly for\nlong-horizon tasks. Our method can attain comparatively higher ﬁnal returns, since we only use one-\nstep predictions from our model to provide synthetic data for a model-free RL method (Fig. 4). This\nresembles methods that combine model learning with model-free RL in single-tasks settings [32, 12].\n6\nExperimental Evaluation\nWe aim to answer the following questions in our experiments: (1) Can MIER meta-train efﬁciently on\nstandard meta-RL benchmarks, with meta-training sample efﬁciency that is competitive with state-of-\nthe-art methods? (2) How does MIER compare to prior meta-learning approaches for extrapolation to\nmeta-test tasks with out-of-distribution (a) reward functions and (b) dynamics? (3) How important is\nexperience relabeling in leveraging the model to train effective policies for out-of-distribution tasks?\nTo answer these questions, we ﬁrst compare the meta-training sample efﬁciency of MIER to existing\nmethods on several standard meta-RL benchmarks. We then test MIER on a set of out-of-distribution\nmeta-test tasks to analyze extrapolation performance. We also compare against a version of our\nmethod without experience relabeling, in order to study the importance of this component for\nadaptation. All experiments are run with OpenAI gym [2] and use the mujoco simulator [35]. We\nhave released code to run our experiments at https://github.com/russellmendonca/mier_\npublic.git/, and additional implementation and experiment details including hyperparameters are\nincluded in Appendix A.\n6.1\nMeta-Training Sample Efﬁciency on Meta-RL Benchmarks\nWe ﬁrst evaluate MIER on standard meta-RL benchmarks, which were used in prior work [7, 23, 5].\nResults are shown in Figure 2. We compare to PEARL [23], which uses an off-policy encoder-based\nmethod, but without consistent adaptation, meta Q-learning (MQL) [5], which also uses an encoder,\nMAML [7] and PRoMP [24], which use MAML-based adaptation with on-policy policy gradients,\nand RL2 [4], which uses an on-policy algorithm with an encoder. We plot the meta-test performance\nafter adaptation (on in-distribution tasks) against the number of meta-training samples, averaged\nacross 3 random seeds. On these standard tasks, we run a variant of our full method which we call\nMIER-wR (MIER without experience relabeling), which achieves performance that is comparable\nto or better than the best prior methods, indicating that our model identiﬁcation method provides\na viable meta-learning strategy that compares favorably to state-of-the-art methods. However, the\nprimary focus of our paper is on adaptation to out-of-distribution tasks, which we analyze next.\n6.2\nAdaptation to Out-of-Distribution Tasks\nNext, we evaluate how well MIER can adapt to out-of-distribution, both on tasks with varying reward\nfunctions and tasks with varying dynamics. We compare the performance of our full method (MIER),\nand MIER without experience relabeling (MIER-wR), to prior meta-learning methods for adaptation\nto out-of-distribution tasks. All algorithms are meta-trained with the same number of samples (2.5M\nfor Ant Negated Joints, and 1.5M for all other domains) before evaluation. For performance of\nalgorithms as a function of data used for meta-training, see Figure 6 in Appendix B.\n6\nFigure 2: Performance on standard meta-RL benchmarks. Return is evaluated over the course of the meta-\ntraining process on meta-test tasks that are in-distribution.\n(a) Cheetah Velocity\n(b) Ant Direction\n(c) Cheetah Negated Joints\n(d) Ant Negated Joints\nFigure 3: Illustration of out-of-distribution adaptation tasks: (a) Cheetah-Velocity Medium (target velocity\ntraining set in blue, test set in red) and Cheetah-Velocity Hard (target velocity training set in green, test set in\nred), (b) Ant Direction (target direction training tasks in green, test tasks in red), (c) Cheetah Negated Joints\nand (d) Ant Negated Joints. Training and test sets are indicated in the ﬁgure for (a) and (b). In the negated joint\nenvironments, the control is negated to a set of randomly selected joints, and the movement direction when\ncontrol is applied is depicted for both negated and normal joints.\nExtrapolation over reward functions:\nTo evaluate extrapolation to out-of-distribution rewards,\nwe ﬁrst test on the half cheetah velocity extrapolation environments introduced by Fakoor et al.\n[5].1 Half-Cheetah-Vel-Medium has training tasks where the cheetah is required to run at target\nspeeds ranging from 0 to 2.5 m/s, while Half-Cheetah-Hard has training tasks where the target\nspeeds are sampled from 0 to 1.5 m/s, as depicted in Figure 3(a). In both settings, the test set has\ntarget speeds sampled from 2.5 to 3 m/s. In Figure 4, we see that our method matches MQL on the\neasier Half-Cheetah-Vel-Medium environment, and outperforms all prior methods including MQL on\nthe Half-Cheetah-Vel-Hard setting, where extrapolation is more difﬁcult. Furthermore we see that\nexperience relabeling improves performance for our method for both settings.\nWe also evaluate reward function extrapolation for an Ant that needs to move in different directions,\nwith the training set comprising directions sampled from 3 quarters of a circle, and the test set\ncontaining tasks from the last quadrant, as shown in Figure 3(b). We see in Figure 4 that our method\noutperforms prior algorithms by a large margin in this setting. We provide a more ﬁne-grained\nanalysis of adaptation performance on different tasks in the test set in Figure 5. We see that while the\nperformance of all methods degrades as validation tasks get farther away from the training distribution,\nMIER and MIER-wR perform consistently better than MAML and PEARL.\nExtrapolation over dynamics:\nTo study adaptation to out-of-distribution dynamics, we con-\nstructed variants of the HalfCheetah and Ant environments where we randomly negate the control of\nrandomly selected groups of joints as shown in Figures 3(c) and 3(d). During meta-training, we\nnever negate the last joint, such that we can construct out-of-distribution tasks by negating this last\njoint, together with a randomly chosen subset of the others. For the HalfCheetah, we negate 3 joints\nat a time from among the ﬁrst 5 during meta-training, and always negate the 6th joint (together with\na random subset of 2 of the other 5) for testing, such that there are 10 meta-training tasks and 10\nout-of-distribution evaluation tasks. For the Ant, we negate 4 joints from among the ﬁrst 7 during\nmeta-training, and always negate the 8th (together with a random subset of 3 of the other 7) for\n1Since we do not have access to the code used by Fakoor et al. [5], quantitative results for the easier cheetah\ntasks are taken from their paper, but we cannot evaluate MQL on other more challenging tasks.\n7\nFigure 4: Performance on out-of-distribution tasks. All algorithms are meta-trained with the same amount of\ndata, and then evaluated on out-of-distribution tasks. Cheetah-Velocity and Ant-Direction environments have\nvarying reward functions, while Cheetah-Negated-Joints and Ant-Negated-Joints have different dynamics.\nFigure 5: Performance evaluated on validation tasks of varying difﬁculty. For Cheetah Velocity, the training\ndistribution consists of target speeds from 0 to 1.5 m/s, and so tasks become harder left to right along the x axis.\nAnt Direction consists of training tasks ranging from 0 to 1.5 π radians, so the hardest tasks are in the middle.\nevaluation, resulting in 35 meta-training tasks and 35 evaluation tasks, out of which we randomly\nselect 15.\nIn addition to PEARL and MAML, we compare against GrBAL [21], a model based meta-RL method.\nWe note that we could not evaluate GrBAL on the reward extrapolation tasks, since it requires the\nanalytic reward function to be known during planning, but we can compare to this method under\nvarying dynamics. From Figure 4, we see that performance on Cheetah-Negated-Joints with just\ncontext adaptation (MIER-wR) is substantially better than PEARL and MAML and GrBAL, and\nthere is further improvement by using the model for relabeling (MIER). On the more challenging\nAnt-Negated-Joints environment, MIER-wR shows similar performance to PEARL, and leveraging\nthe model for relabeling again leads to better performance for MIER.\n7\nConclusion\nIn this paper, we introduce a consistent and sample efﬁcient meta-RL algorithm by reformulating\nthe meta-RL problem as model identiﬁcation, and then described a method for adaptation to out-\nof-distribution tasks based on experience relabeling. Our algorithm can adapt to new tasks by\ndetermining the parameters of the model, which predicts the reward and future transitions, and\ncan adapt consistently to out-of-distribution tasks by adapting the model ﬁrst, relabeling all data\nfrom the meta-training tasks with this model, and then ﬁnetuning on that data using a standard\noff-policy RL method. While model identiﬁcation identiﬁcation and experience relabeling can\nbe used independently, with the former providing for a simple meta-RL framework, and the latter\n8\nproviding for adaptation to out-of-distribution tasks, we show that combining these components\nleads to good results across a range of challenging meta-RL problems that require extrapolating to\nout-of-distribution tasks at meta-test time.\nReferences\n[1] Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. In IJCNN-91-Seattle\nInternational Joint Conference on Neural Networks, volume ii, pages 969 vol.2–, 1991.\n[2] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2017.\n[3] E. F. Camacho and C. B. Alba. Model predictive control. In Learning to learn. Springer Science\nand Business Media, 2013.\n[4] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2:\nFast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779,\n2016.\n[5] Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J. Smola. Meta-q-learning. In\nInternational Conference on Learning Representations, 2020. URL https://openreview.\nnet/forum?id=SJeD3CEFPH.\n[6] Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and\ngradient descent can approximate any learning algorithm. In International Conference on\nLearning Representations, 2018. URL https://openreview.net/forum?id=HyjC5yWCW.\n[7] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-\ntation of deep networks. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, pages 1126–1135. JMLR. org, 2017.\n[8] Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-\nreinforcement learning of structured exploration strategies. In Advances in Neural Information\nProcessing Systems, pages 5302–5311, 2018.\n[9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint\narXiv:1801.01290, 2018.\n[10] Rein Houthooft, Richard Y Chen, Phillip Isola, Bradly C Stadie, Filip Wolski, Jonathan Ho,\nand Pieter Abbeel. Evolved policy gradients. arXiv preprint arXiv:1802.04821, 2018.\n[11] Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A Ortega, Yee Whye Teh, and\nNicolas Heess. Meta reinforcement learning as task inference. arXiv preprint arXiv:1905.06424,\n2019.\n[12] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:\nModel-based policy optimization. arXiv preprint arXiv:1906.08253, 2019.\n[13] Leslie Pack Kaelbling. Learning to achieve goals. In IJCAI, 1993.\n[14] Lin Lan, Zhenguo Li, Xiaohong Guan, and Pinghui Wang. Meta reinforcement learning with\ntask embedding and shared policy. arXiv preprint arXiv:1905.06527, 2019.\n[15] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep\nvisuomotor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016.\n[16] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval\nTassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning,\n2015.\n[17] Hao Liu, Richard Socher, and Caiming Xiong.\nTaming maml: Efﬁcient unbiased meta-\nreinforcement learning. In International Conference on Machine Learning, pages 4061–4071,\n2019.\n[18] Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, and Chelsea\nFinn. Guided meta-policy search. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-\nBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,\npages 9656–9667. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/\n9160-guided-meta-policy-search.pdf.\n9\n[19] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive\nmeta-learner. arXiv preprint arXiv:1707.03141, 2017.\n[20] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n[21] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine,\nand Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-\nreinforcement learning. arXiv preprint arXiv:1803.11347, 2018.\n[22] Devang K Naik and RJ Mammone. Meta-neural networks that learn by learning. In [Proceedings\n1992] IJCNN International Joint Conference on Neural Networks, volume 1, pages 437–442.\nIEEE, 1992.\n[23] Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efﬁcient\noff-policy meta-reinforcement learning via probabilistic context variables. arXiv preprint\narXiv:1903.08254, 2019.\n[24] Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal\nmeta-policy search. arXiv preprint arXiv:1810.06784, 2018.\n[25] Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon\nOsindero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint\narXiv:1807.05960, 2018.\n[26] Steindór Sæmundsson, Katja Hofmann, and Marc Peter Deisenroth. Meta reinforcement\nlearning with latent variable gaussian processes. arXiv preprint arXiv:1803.07551, 2018.\n[27] Tom Schaul, Dan Horgan, Karol Gregor, and David Silver. Universal value function approxi-\nmators. In Proceedings of the 32nd International Conference on International Conference on\nMachine Learning - Volume 37, ICML’15, page 1312–1320. JMLR.org, 2015.\n[28] Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to\nlearn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.\n[29] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust\nregion policy optimization, 2015.\n[30] Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and\nIlya Sutskever. Some considerations on learning to explore via meta-reinforcement learning.\narXiv preprint arXiv:1803.01118, 2018.\n[31] Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, and Yongxin Yang. Learning to learn:\nMeta-critic networks for sample efﬁcient learning. arXiv preprint arXiv:1706.09529, 2017.\n[32] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM\nSigart Bulletin, 2(4):160–163, 1991.\n[33] O. Tange. Gnu parallel - the command-line power tool. ;login: The USENIX Magazine, 36(1):\n42–47, Feb 2011. URL http://www.gnu.org/s/parallel.\n[34] Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to\nlearn, pages 3–17. Springer, 1998.\n[35] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based\ncontrol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages\n5026–5033. IEEE, 2012.\n[36] Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Rémi Munos,\nCharles Blundell, Dharshan Kumaran, and Matthew M Botvinick. Learning to reinforcement\nlearn. ArXiv, abs/1611.05763, 2016.\n[37] Grady Williams, Andrew Aldrich, and Evangelos A. Theodorou. Model predictive path integral\ncontrol using covariance variable importance sampling. CoRR, abs/1509.01149, 2015. URL\nhttp://arxiv.org/abs/1509.01149.\n[38] Luisa M Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson.\nCaml: Fast context adaptation via meta-learning. arXiv preprint arXiv:1810.03642, 2018.\n10\nAppendices\nA\nImplementation Details\nPlease see the released codebase for code to meta-train models and extrapolate to out-of-distribution\ntasks. We also include code for the simulation environments included in the paper.\nA.1\nDatasets\nAll experiments are run with OpenAI gym [2], use the mujoco simulator [35] and are run with 3 seeds\n(We meta-train 3 models, and run extrapolation for each). The metric used to evaluate performance\nis the average return (sum of rewards) over a test rollout. The horizon for all environments is 200.\nFor the meta-RL benchmarks (Fig. 2), performance on test tasks is plotted versus number of samples\nmeta-trained on. The out-of-distribution plots (Fig. 4 and 5) report performance of all algorithms\nmeta-trained with the same number of samples (2.5M for Ant Negated Joints, and 1.5M for all other\ndomains). For the standard meta-RL benchmark tasks, we use the settings from PEARL [23] for\nnumber of meta-train tasks, meta-test tasks and data points for adaptation on a new test task. For the\nout-of-distribution experiments, the values used for datasets are listed in Table 1. The description of\nthe meta-train and meta-test task sets for out-of-distribution tasks is included in Section 6.2.\nA.2\nExtrapolation Experiment Details\nFor the settings with varying reward functions, the state dynamics does not differ across tasks, and\nso we only meta-train a reward prediction model. We only relabel rewards and preserve the (state,\naction, next state) information from cross task data while relabelling experience in this setting. For\ndomains with varying dynamics, we meta-learn both reward and state models.\nWhen continually adapting the model to out of distribution tasks, we ﬁrst take a number of gradient\nsteps (N) that only affect the context , followed by another number of gradient steps (M) that affect\nall model parameters. We also note that if the model adaptation process overﬁts to the adaptation\ndata, using generated synthetic data will lead to worse performance for the policy. To avoid this, we\nonly use 80% of the adaptation data to learn the model, and use the rest for validation. The model is\nused to produce synthetic data for a task only if the total model loss on the validation set is below a\nthreshold (set to -3).\nTable 1: Settings for out-of-distribution environments\nEnvironment\nMeta-train tasks\nMeta-test tasks\nData points for adaptation\nN\nM\nCheetah-vel-medium\n100\n30\n200\n10\n100\nCheetah-vel-hard\n100\n30\n200\n10\n100\nAnt-direction\n100\n10\n400\n20\n0\nCheetah-negated-joints\n10\n10\n400\n10\n0\nAnt-negated-joints\n10\n10\n400\n10\n0\nWalker-rand-params\n40\n20\n400\n10\n100\nA.3\nHyper-parameters\nFor the MIER experiments hyper-parameters are kept mostly ﬁxed across all experiments, with the\nmodel-related hyperparameters set to default values used in the Model Based Policy Optimization\ncodebase [12], and the policy-related hyperparameters set to default settings in PEARL [23], and\ntheir values are detailed in Table 2. We also ran sweeps on some hyper-parameters, detailed in\nTable 3.\nFor the baselines, we used publicly released logs for the benchmark results, and ran code released by\nthe authors for the out-of-distribution tasks. Hyper-parameters were set to the default values in the\ncodebases. We also swept on number of policy optimization steps and context vector dimension for\nPEARL, similar to the sweep in Table 3.\n11\nTable 2: Default Hyper-parameters\n(a) Model-related\nHyperparameter\nValue\nModel arch\n200-200-200-\n200\nMeta batch size\n10\nInner adaptation steps\n2\nInner learning rate\n0.01\nNumber of cross tasks for re-\nlabelling\n20\nBatch-size for cross task sam-\npling\n1e5\nDataset train-val ratio for\nmodel adaptation\n0.8\n(b) Policy-related\nHyperparameter\nValue\nCritic arch\n300-300-300\nPolicy arch\n300-300-300\nDiscount factor\n0.99\nLearning rate\n3e-4\nTarget update interval\n1\nTarget update rate\n0.005\nSac reward scale\n1\nSoft temperature\n1.0\nPolicy training batch-size\n256\nRatio of real to synthetic data\nfor continued training\n0.05\nNumber of policy optimiza-\ntion steps per synthetic batch\ngeneration\n250\nTable 3: Hyper-parameter sweeps\nHyper-parameter\nValue\nSelected Values\nNumber of policy optimization steps per meta-training iteration\n1000, 2000, 4000\n1000\nContext vector dimension\n5, 10\n5\nGradient norm clipping\n10, 100\n10\nAll experiments used GNU parallel [33] for parallelization, and were run on GCP instances with\nNVIDIA Tesla K80 GPUS.\nB\nTest-Time Performance Curves for Extrapolation Tasks\nFigure 6: Extrapolation performance on OOD tasks. In all experiements, we see our method exceeds or matches\nthe performance of previous state-of-the-art methods. We also observe that experience relabeling is crucial to\ngetting good performance on out-of-distribution tasks.\n12\nC\nComparison of the Data Reuse Methods of MIER and MQL\nBoth MQL [5] and the experience relabel process of MIER reuses data collected during meta-training\ntime to continue improve the policy during adaptation. However, the way these two methods reuse\ndata is completely different. Given a new adaptation dataset Dadapt and replay buffer R containing\ndata from other tasks, MQL estimates a density ratio p(s,a,s′,r)\nq(s,a,s′,r), where p and q are the corresponding\nprobability density on Dadapt and R. MQL then re-weight the transitions in the replay buffer R\nusing this density ratio to compute the loss for the policy and value function. Note that this method\nis inherently limited since it assumes that the data distributions of different tasks share the same\nsupport in a meta-RL problem. That is, it assumes p(s, a, s′, r) > 0 for transitions sampled from\nother tasks in the replay buffer R. This assumption may not be true in main practical domains. If\nthis assumption does not hold true in the domain, the true importance ratio would be strictly 0 and\ntherefore any quantity computed using this importance ratio will be 0 too.\nOur method avoids this problem by using an adapted reward and dynamics model to relabel the data\ninstead of using an importance ratio to re-weight them. By generating a synthetic next state and\nreward, the relabeled transition could have non-zero probability density under the new task even if\nthe original transition is sampled from a different task. The only assumption for our method is that\ndifferent tasks share the same state and action space, which is true for most meta-RL domains.\nD\nThe Difﬁculty of Combining Gradient-Based Meta-Learning with\nValue-Based RL Methods\nOne straightforward idea of building a sample efﬁcient off-policy meta-RL algorithm that adapts well\nto out-of-distribution task is to simply combine MAML with an off-policy actor-critic RL algorithm.\nHowever, this seemingly simple idea is very difﬁcult in practice, mainly because of the difference\nbetween Bellman backup iteration used in actor-critic methods and gradient descent. Consider the\nBellman backup of Q function Qπ for policy π,\nQπ(s, a) ←r(s, a) + γEs′∼p(s′|s,a),a′∼π(a′|s′)[Qπ(s′, a′)]\nwhich backs up the next state Q value to the current state Q value. One iteration of Bellman backup\ncan only propagate value information backward in time for one timestep. Therefore, given a trajectory\nwith horizon T, even if we can perform the backup operation exactly at every iteration, at least T\niterations of Bellman backup is required for the Q function to converge. Therefore, it cannot be used\nas the inner loop objective for MAML, where only a few steps of gradient descent is allowed. In\npractice T is usually 200 for MuJoCo based meta-RL domains, and applying MAML with 200 steps\nof inner loop is certainly intractable. If we only perform K steps of Bellman backup for the inner\nloop, where K is a small number, we would obtain a Q function that is greedy in K steps, which\ngives us very limited performance. In fact, we realized this limitation only after implementing this\nmethod, where we were never able to get it to work in even the easiest domain.\n13\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-06-12",
  "updated": "2020-06-15"
}