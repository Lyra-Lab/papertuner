{
  "id": "http://arxiv.org/abs/2006.07262v3",
  "title": "A Brief Look at Generalization in Visual Meta-Reinforcement Learning",
  "authors": [
    "Safa Alver",
    "Doina Precup"
  ],
  "abstract": "Due to the realization that deep reinforcement learning algorithms trained on\nhigh-dimensional tasks can strongly overfit to their training environments,\nthere have been several studies that investigated the generalization\nperformance of these algorithms. However, there has been no similar study that\nevaluated the generalization performance of algorithms that were specifically\ndesigned for generalization, i.e. meta-reinforcement learning algorithms. In\nthis paper, we assess the generalization performance of these algorithms by\nleveraging high-dimensional, procedurally generated environments. We find that\nthese algorithms can display strong overfitting when they are evaluated on\nchallenging tasks. We also observe that scalability to high-dimensional tasks\nwith sparse rewards remains a significant problem among many of the current\nmeta-reinforcement learning algorithms. With these results, we highlight the\nneed for developing meta-reinforcement learning algorithms that can both\ngeneralize and scale.",
  "text": "A Brief Look at Generalization in Visual Meta-Reinforcement Learning\nSafa Alver 1 Doina Precup 1 2\nAbstract\nDue to the realization that deep reinforcement\nlearning algorithms trained on high-dimensional\ntasks can strongly overﬁt to their training envi-\nronments, there have been several studies that\ninvestigated the generalization performance of\nthese algorithms. However, there has been no\nsimilar study that evaluated the generalization per-\nformance of algorithms that were speciﬁcally de-\nsigned for generalization, i.e. meta-reinforcement\nlearning algorithms. In this paper, we assess the\ngeneralization performance of these algorithms by\nleveraging high-dimensional, procedurally gen-\nerated environments. We ﬁnd that these algo-\nrithms can display strong overﬁtting when they\nare evaluated on challenging tasks. We also ob-\nserve that scalability to high-dimensional tasks\nwith sparse rewards remains a signiﬁcant problem\namong many of the current meta-reinforcement\nlearning algorithms. With these results, we high-\nlight the need for developing meta-reinforcement\nlearning algorithms that can both generalize and\nscale.\n1. Introduction\nIn recent years, deep reinforcement learning (RL) algo-\nrithms have achieved signiﬁcant success in a wide variety of\nchallenging tasks, ranging from board games (Silver et al.,\n2017; 2018) to video games (Mnih et al., 2015; Vinyals\net al., 2017). Despite the ever-increasing successes, these\nalgorithms require a substantial amount of data for achiev-\ning good performance in a narrowly-deﬁned domain, and\nthey can perform very poorly even when slight modiﬁca-\ntions occur in the environment. This indicates that RL al-\ngorithms tend to overﬁt to the tasks on which they were\ntrained (Zhang et al., 2018b; Farebrother et al., 2018). If\nwe want RL algorithms that can learn multiple tasks and\nquickly adapt to new ones, such algorithms need to learn\n1Mila - McGill University 2Google DeepMind. Correspondence\nto: Safa Alver <safa.alver@mail.mcgill.ca>.\n4th Lifelong Learning Workshop at ICML 2020, Vienna, Austria,\n2020. Copyright 2020 by the author(s).\nthe common structure across many tasks and then use this\ninformation to quickly generalize to new tasks.\nRecent studies in the ﬁeld of meta-reinforcement learning\n(meta-RL) have shown promising results in this direction\n(Duan et al., 2016; Wang et al., 2016; Finn et al., 2017).\nMeta-RL algorithms are trained on multiple related envi-\nronments, in order to learn a learning algorithm that can\nperform quick adaptation to new unseen tasks. While these\nalgorithms have shown promise, due to the lack of well-\ndesigned benchmarks, they have often been trained and eval-\nuated on very narrow and simple task distributions, leaving\ntheir true generalization capabilities unclear. For instance,\none of the popular benchmarks involves two tasks which re-\nquire training a simulated legged robot to run either forward\nor backward, and another one involves different parametriza-\ntions of these robots in terms of their limb conﬁgurations\n(Finn et al., 2017; Houthooft et al., 2018; Rakelly et al.,\n2019). To overcome this problem and study the capabilities\nof these algorithms, recently Yu et al. (2019) have designed\na simulated robot benchmark with 50 qualitatively-diverse\nmanipulation tasks. However, despite the diversity, the\nbenchmark lacks challenging high-dimensional tasks with\nsparse rewards, leaving their capabilities in these domains\nunclear.\nIn this study, we examine the generalization performance of\nmeta-RL algorithms using challenging vision-based sparse-\nreward environments. To achieve this, we leverage proce-\ndurally generated environments (Cobbe et al., 2019), which\nallow generating an inﬁnite amount of game levels with\ngreatly varying difﬁculty. We ﬁnd that current meta-RL\nalgorithms show strong signs of overﬁtting when evaluated\non challenging environments. We also observe that scalabil-\nity to high-dimensional tasks remains a signiﬁcant problem\namong current meta-RL algorithms, as most of them either\nperform poorly or run very slowly. Our main contribution\nis the empirical study of the generalization performance\nof meta-RL algorithms in vision-based sparse-reward envi-\nronments. We hope that our ﬁndings will stimulate further\nresearch progress in improving generalization.\nThe rest of the paper is structured as follows. In section 2,\nwe provide a brief preliminary about the formulation we\nassume in this paper. In section 3, we review the studies\nthat are most relevant to this study. In section 4, we describe\narXiv:2006.07262v3  [cs.LG]  3 Jul 2020\nA Brief Look at Generalization in Visual Meta-Reinforcement Learning\nModifiedCoinrun\nHeist\nAgent's view\nRandom levels\nFigure 1. (Left) Example observations from the ModiﬁedCoinrun and Heist environments that the agent receives. Observations consist of\nonly a small patch of space surrounding the agent, making these environments partially observable. (Right) Bird’s-eye view of some of\nthe levels from these environments. The difﬁculty of the levels increases from left to right.\nour experimental setting and provide our empirical ﬁndings.\nFinally, in section 5, we provide a discussion and suggest\npossible future directions.\n2. Preliminaries\nReinforcement Learning. We assume the standard formu-\nlation of RL in Sutton & Barto (2018), where a task is\nrepresented by a Markov Decision Process (MDP), a tu-\nple T = (S, A, p, r, γ, T). Here S is the state space, A is\nthe action space, p : S × A × S →R+ is the transition\ndistribution, r : S × A →[−Rmax, +Rmax] is the reward\nfunction, γ ∈(0, 1) is the discount factor and T is the hori-\nzon. The aim of a RL agent is to learn a stochastic policy\nπθ : S × A →[0, 1] that maximizes the expected sum\nof discounted rewards, which is also known as the return\nEτ[PT −1\nk=0 γkrt+k+1], where τ denotes trajectories.\nMeta-Reinforcement Learning. In meta-RL, we assume\na distribution over tasks p(T ), where each task is a separate\nMDP as described above. Importantly, it is often assumed\nthat these tasks share either S and A or only A as in our\ncase. In this setting, the aim of the meta-RL agent is to learn\na an update method U from training tasks, which can be\nlearned to generate new stochastic policies πU(θ), allowing\nquick adaptation to testing tasks. Both training and testing\ntasks are assumed to be sampled from p(T ).\n3. Related Work\nOverﬁtting in Reinforcement Learning. Due to the real-\nization that training and testing RL agents in the same envi-\nronments can prevent the detection of overﬁtting, there have\nbeen a variety of generalization studies where RL agents\nare evaluated on different environments (mostly different\nlevels and modes of a single game) than those on which\nthey are trained (Cobbe et al., 2018; Justesen et al., 2018;\nFarebrother et al., 2018; Zhang et al., 2018a;b; Cobbe et al.,\n2019). Among these studies, the works of Justesen et al.\n(2018) and Cobbe et al. (2018; 2019) are closest to our\nstudy. These works use procedural content generation to\nevaluate generalization in regular RL algorithms. Although\nour evaluation is inspired by these studies, we look at the\ngeneralization problem in the context of meta-RL rather\nthan regular RL.\nCurriculum Learning in Reinforcement Learning. The\nidea of training RL agents using curricula has been explored\nwidely in the RL literature (Schmidhuber, 2013; Graves\net al., 2017; Justesen et al., 2018; Matiisen et al., 2019;\nWang et al., 2019). In these approaches, the agent is trained\nwith an increasing difﬁculty of levels. However, in this\nwork we do not consider a structured curriculum, but rather\nconsider one with mixed difﬁculties (the default setting of\nProcgen environments (Cobbe et al., 2019)). It might also be\ninteresting to explore the option of increasing task difﬁculty\nin the context of meta-RL, but we leave this for future work.\nOur approach has the advantage of forcing the agent to\nperform well at all levels of difﬁculty at the same time.\nMeta-Reinforcement Learning Evaluation. Meta-RL al-\ngorithms have been evaluated on a wide range of simu-\nlated environments which include 3D maze navigation tasks\n(Duan et al., 2016; Wang et al., 2016; Mishra et al., 2017),\ncontinuous control tasks with parametric variations (Finn\net al., 2017; Rothfuss et al., 2018; Rakelly et al., 2019;\nHouthooft et al., 2018; Kirsch et al., 2019), bandit/MDP\nproblems (Duan et al., 2016; Wang et al., 2016; Mishra\nA Brief Look at Generalization in Visual Meta-Reinforcement Learning\nEpisode 1\nEpisode 2\nEpisode 1\nEpisode 2\nFigure 2. Example visualizations of the RL2 agent’s behavior in randomly chosen levels. In both of the environments, the agent follows\nthe exploratory red path in the ﬁrst episode. Then, after discovering where the coin/gem is, in the second episode it follows the yellow\npath which directly leads to the target. This behavior only arises when given enough training levels.\net al., 2017), challenging gridworlds (Stadie et al., 2018)\nand robotic manipulation tasks (Yu et al., 2019). However,\nexcept for the 3D maze navigation tasks, these domains lack\ntwo important challenges: high-dimensionality and reward\nsparsity. Thus, the generalization performance of these al-\ngorithms beyond 3D maze navigation tasks remains unclear.\nIn order to investigate this, we evaluate meta-RL algorithms\non challenging vision-based, sparse-reward, procedurally\ngenerated environments (Cobbe et al., 2019).\n4. Experiments\n4.1. Experimental Setting\nEnvironments. As a testbed, we used two different games\nfrom the Procgen environments (Cobbe et al., 2019).1 Our\nﬁrst choice is a platformer game, Coinrun. However, since\nthe original game requires no exploration, we modiﬁed it\nso that the agent has to perform exploration to ﬁnd the coin,\nwhich is placed either on the far right or far left side of the\nlevel. We refer to this modiﬁed version as ModiﬁedCoinrun.\nOur second choice is a navigation game called Heist, where\nthe agent has to ﬁrst collect scattered keys to unlock doors,\nand then reach the gem. In both of these environments, the\nagent receives a reward of +10 when the coin/gem is picked\nand the levels end without any reward if the agent dies (only\nin ModiﬁedCoinrun) or the 1,000 timesteps limit is reached.\nBoth games are also partially observable, requiring memory\n(see left side of Fig. 1).\nSince meta-RL algorithms are often evaluated in environ-\nments where only the level layouts change (Duan et al.,\n2016; Mishra et al., 2017), as opposed to the additional\nchanging colors in the Procgen environments, we also cre-\nated versions of the above environments where only the\nlayout changes across different levels. We refer to these\nenvironments as ModiﬁedCoinrun (Easy Mode) and Heist\n(Easy Mode). More information on the environments we\n1We have chosen only two environments as the computational\ncost for our experiments is very high.\nused can be found in Appendix A.\nAn important thing to note is that the levels of the games\nin Procgen environments are generated deterministically\nfrom a given seed, allowing generation of an inﬁnite amount\nof training and testing levels. They also greatly vary in\ndifﬁculty, providing a natural curriculum for learning. Some\nof the levels, ranging from the easiest to the hardest, are\ndepicted in the right side of Figure 1.\nAlgorithm Choice. For the evaluation process, we experi-\nmented with many different meta-RL algorithms that meta-\nlearn a policy. However, we only evaluated RL2 (Duan et al.,\n2016; Wang et al., 2016) as it was the only algorithm that\nwe were able to successfully train. In our experiments, we\nfound E-RL2 (Stadie et al., 2018) to perform very poorly\n(possibly due to the very delayed reward), SNAIL (Mishra\net al., 2017) to require a very large trajectory (which requires\nan infeasible amount of memory), and PEARL (Rakelly\net al., 2019) to run very slowly. We also do not include\nMAML (Finn et al., 2017), and the algorithms that build on\ntop of it (Stadie et al., 2018; Rothfuss et al., 2018; Gupta\net al., 2018), as Mishra et al. (2017) has found the compu-\ntational expense of training MAML in high-dimensional\ntasks to be prohibitively high. To support our observations,\nwe would also like to note that except for RL2 and SNAIL,\nthere has been no study that we are aware, at the time of\nthis paper, that reported success of meta-RL algorithms in\nhigh-dimensional tasks.\nImplementation Details. Our RL2 implementation, builds\non top of the PPO (Schulman et al., 2017) implementation\nof Liang et al. (2017) and uses the IMPALA CNN (Espeholt\net al., 2018) as its visual feature extractor. We use this CNN\nover the one in Mnih et al. (2015), as Cobbe et al. (2019)\nhas shown that it performs signiﬁcantly better on Procgen\nenvironments. As in Duan et al. (2016), the previous actions,\nrewards and done signals are fed to the network along with\nthe current observation. We concatenate 2 episodes to form\na trial, as we found that adding more episodes does not\nmake any difference in the ﬁnal performance. In all of\nA Brief Look at Generalization in Visual Meta-Reinforcement Learning\n101\n102\n103\n104\n105\nNumber of Training Levels\n0\n2\n4\n6\n8\n10\nScore\nModifiedCoinrun (Easy Mode)\nTrain\nTest\n101\n102\n103\n104\n105\nNumber of Training Levels\n0\n2\n4\n6\n8\n10\nScore\nModifiedCoinrun\nTrain\nTest\n101\n102\n103\n104\n105\nNumber of Training Levels\n0\n2\n4\n6\n8\n10\nScore\nHeist (Easy Mode)\nTrain\nTest\n101\n102\n103\n104\n105\nNumber of Training Levels\n0\n2\n4\n6\n8\n10\nScore\nHeist\nTrain\nTest\nEasy\nMixed\nHard\nTraining Level Difficulty\n0\n2\n4\n6\n8\n10\nScore\nModifiedCoinrun (Easy Mode)\nTrain\nTest\nEasy\nMixed\nHard\nTraining Level Difficulty\n0\n2\n4\n6\n8\n10\nScore\nModifiedCoinrun\nTrain\nTest\nEasy\nMixed\nHard\nTraining Level Difficulty\n0\n2\n4\n6\n8\n10\nScore\nHeist (Easy Mode)\nTrain\nTest\nEasy\nMixed\nHard\nTraining Level Difficulty\n0\n2\n4\n6\n8\n10\nScore\nHeist\nTrain\nTest\nFigure 3. (Top row) The ﬁnal training and test performance of RL2 as a function of the number training tasks. The horizontal black and\nvertical green dashed lines indicate the maximum achievable scores and recommended training levels, respectively. (Bottom row) The\ntraining and test performance for each of the training level difﬁculties. For each of the plots, we report the mean score in the last episode\nof the trial. The means and standard deviations are computed using 3 independent runs.\nour experiments, we trained the RL2 agent for 25M (easy\nmodes) and 100M (regular modes) timesteps. Training more\ndid not increase the performance. More information on the\nnetwork architecture and hyperparameters can be found in\nAppendix B and C, respectively.\n4.2. Evaluation\nQualitative Pre-evaluation. Before running generalization\nexperiments, we trained the RL2 agent with 100,000 train-\ning levels and tested it on unseen levels to see if it can\nachieve the optimal behavior, which consists of exploring\nthe level in the ﬁrst episode, and then, after locating the\ntarget, using this information to quickly reach it in the sec-\nond episode. We indeed observe successful learning of this\nbehavior. Examples of this type of optimal behavior are\ndepicted in Figure 2.2\nGeneralization Experiment I. We start our generalization\nevaluation by looking at the effect of training set size on\ngeneralization, as was done in Cobbe et al. (2019) for regular\nRL algorithms. To investigate this, we constructed sets of\ntraining levels, with 10 to 100,000 levels. We then trained\nRL2 agents on each of these sets for 25M (for easy modes)\n2Videos of the trained agents on additional levels can\nbe found at https://www.youtube.com/playlist?\nlist=PLcOHCuu_gh7nVISSFKtgT_JZ0iNiIbTqn.\nand 100M (for regular modes) timesteps and tested them on\nrandomly sampled held-out levels. The results are shown in\nthe top row of Figure 3.\nOur results show that small training sets can cause signiﬁ-\ncant overﬁtting in the meta-RL setting, just as in the regular\nRL setting. Like regular RL algorithms, meta-RL algo-\nrithms also require as many as 10,000 levels to close the\ngeneralization gap. More importantly, however, we see that\nthere exists a large generalization gap for the recommended\n200 (easy mode) and 500 (regular mode) training levels\n(Cobbe et al., 2019) (see the vertical green dashed lines in\nthe top row of Fig. 3). In the Heist environments, this gap is\nrelatively lower only because the training score is also low.\nThis indicates that meta-RL algorithms fail to generalize in\nthe Procgen benchmark, showing strong signs of overﬁtting\ninstead.\nGeneralization Experiment II. Next, we investigate gen-\neralization on a higher level, by evaluating the effect of\nhaving a training set composed of ﬁxed difﬁculty levels.\nThis time, as the number of levels is not the main concern,\nwe constructed three sets of training levels, each containing\n100,000 levels, where the ﬁrst one contains easy levels, the\nsecond one contains mixed difﬁculty levels and the last one\ncontains hard ones. The details of these sets are available in\nAppendix A. We again trained RL2 agents on each of these\nA Brief Look at Generalization in Visual Meta-Reinforcement Learning\nsets for 25M (for easy mode) and 100M (for regular mode)\ntimesteps and tested them on held out levels with mixed\ndifﬁculty.\nThe results are depicted in the bottom row of Figure 3. We\nﬁnd that while training on hard levels can prevent learning\nfrom happening at all, training on easy/mixed levels can\nhave different effects depending on the game. In the Heist\nenvironments, training only on easy levels can allow gener-\nalization to unseen mixed levels. This also aligns with the\nobservations of Duan et al. (2016) that RL2 agents trained\non 5×5 mazes can often generalize to 9×9 ones. How-\never, in the ModiﬁedCoinrun environments, this is not the\ncase, and training on mixed levels is strictly required for\ngeneralization. We attribute this ﬁnding to the nature of the\nenvironments. In the Heist environments, the agent is able to\nrandomly explore in the ﬁrst episode without dying, whereas\nin the ModiﬁedCoinrun environments, random exploration\ncan cause death, with no useful information being passed\nto the second episode. This suggests that generalization\nacross different level difﬁculties is possible only in certain\nenvironments and strong overﬁtting at a higher level can\noccur in certain environments. Investigating more formally\nwhat environment characteristics are helpful or detrimental\ncould aid the development of better algorithms.\n5. Discussion and Future Work\nThe results of our experiments show that current meta-RL\nalgorithms can show strong overﬁtting, despite their explicit\ngoal of generalizing well. We see this behavior even in the\nsimplest settings (easy modes) of the Procgen environments.\nThis matches the recent ﬁndings of Yu et al. (2019), in which\nthey have found that current meta-RL algorithms can fail to\ngeneralize even in the simplest settings of the Meta-World\nbenchmark. If our purpose is to create algorithms that can\nactually generalize, rather than evaluating them on simple\ntasks, such as different parametrizations of continuous con-\ntrol tasks or simple maze navigation tasks, we believe that\nthey must be evaluated on challenging environments like\nProcgen or Meta-World, which can pose signiﬁcant general-\nization challenges and can help differentiate different agents.\nBy doing so, we can develop algorithms that can achieve\nthe promise of meta-RL.\nAnother interesting observation is that the scalability of cur-\nrent meta-RL algorithms to high dimensional tasks remains\na signiﬁcant problem. Our experiments with a large selec-\ntion of algorithms have shown that RL2 is the only algorithm\nthat was able to scale to the environments presented in this\nstudy. With this, we also highlight the need for developing\nalgorithms with better scaling properties.\nIn future work, these experiments can be extended to the\nother games in the Procgen benchmark and possibly to other\nenvironments where task creation is easily achieved, result-\ning in more diverse and challenging benchmarks to evaluate\nthe generalization performance of meta-RL algorithms. An-\nother possible future direction is to investigate the general-\nization performance of algorithms that are not referred to as\nmeta-RL algorithms, but nevertheless promise generaliza-\ntion.\nReferences\nCobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman,\nJ. Quantifying generalization in reinforcement learning.\narXiv preprint arXiv:1812.02341, 2018.\nCobbe, K., Hesse, C., Hilton, J., and Schulman, J. Lever-\naging procedural generation to benchmark reinforcement\nlearning. arXiv preprint arXiv:1912.01588, 2019.\nDuan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever,\nI., and Abbeel, P.\nRl2:\nFast reinforcement learn-\ning via slow reinforcement learning.\narXiv preprint\narXiv:1611.02779, 2016.\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih,\nV., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning,\nI., et al. Impala: Scalable distributed deep-rl with impor-\ntance weighted actor-learner architectures. arXiv preprint\narXiv:1802.01561, 2018.\nFarebrother, J., Machado, M. C., and Bowling, M. Gen-\neralization and regularization in dqn.\narXiv preprint\narXiv:1810.00123, 2018.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 1126–1135. JMLR.org, 2017.\nGraves, A., Bellemare, M. G., Menick, J., Munos, R., and\nKavukcuoglu, K. Automated curriculum learning for\nneural networks. In Proceedings of the 34th International\nConference on Machine Learning-Volume 70, pp. 1311–\n1320. JMLR. org, 2017.\nGupta, A., Mendonca, R., Liu, Y., Abbeel, P., and Levine,\nS. Meta-reinforcement learning of structured exploration\nstrategies. In Advances in Neural Information Processing\nSystems, pp. 5302–5311, 2018.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\nHouthooft, R., Chen, Y., Isola, P., Stadie, B., Wolski, F.,\nHo, O. J., and Abbeel, P. Evolved policy gradients. In\nAdvances in Neural Information Processing Systems, pp.\n5400–5409, 2018.\nA Brief Look at Generalization in Visual Meta-Reinforcement Learning\nJustesen, N., Torrado, R. R., Bontrager, P., Khalifa, A.,\nTogelius, J., and Risi, S. Illuminating generalization\nin deep reinforcement learning through procedural level\ngeneration. arXiv preprint arXiv:1806.10729, 2018.\nKirsch, L., van Steenkiste, S., and Schmidhuber, J. Improv-\ning generalization in meta reinforcement learning using\nlearned objectives.\narXiv preprint arXiv:1910.04098,\n2019.\nLiang, E., Liaw, R., Moritz, P., Nishihara, R., Fox, R., Gold-\nberg, K., Gonzalez, J. E., Jordan, M. I., and Stoica, I.\nRllib: Abstractions for distributed reinforcement learning.\narXiv preprint arXiv:1712.09381, 2017.\nMatiisen, T., Oliver, A., Cohen, T., and Schulman, J.\nTeacher-student curriculum learning. IEEE transactions\non neural networks and learning systems, 2019.\nMishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.\nA simple neural attentive meta-learner. arXiv preprint\narXiv:1707.03141, 2017.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529–533, 2015.\nRakelly, K., Zhou, A., Quillen, D., Finn, C., and Levine,\nS.\nEfﬁcient off-policy meta-reinforcement learning\nvia probabilistic context variables.\narXiv preprint\narXiv:1903.08254, 2019.\nRothfuss, J., Lee, D., Clavera, I., Asfour, T., and Abbeel,\nP. Promp: Proximal meta-policy search. arXiv preprint\narXiv:1810.06784, 2018.\nSchmidhuber, J. Powerplay: Training an increasingly gen-\neral problem solver by continually searching for the sim-\nplest still unsolvable problem. Frontiers in psychology, 4:\n313, 2013.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and\nKlimov, O. Proximal policy optimization algorithms.\narXiv preprint arXiv:1707.06347, 2017.\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou,\nI., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,\nBolton, A., et al. Mastering the game of go without\nhuman knowledge. Nature, 550(7676):354–359, 2017.\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai,\nM., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae-\npel, T., et al. A general reinforcement learning algorithm\nthat masters chess, shogi, and go through self-play. Sci-\nence, 362(6419):1140–1144, 2018.\nStadie, B. C., Yang, G., Houthooft, R., Chen, X., Duan, Y.,\nWu, Y., Abbeel, P., and Sutskever, I. Some considerations\non learning to explore via meta-reinforcement learning.\narXiv preprint arXiv:1803.01118, 2018.\nSutton, R. S. and Barto, A. G. Reinforcement learning: An\nintroduction. 2018.\nVinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhn-\nevets, A. S., Yeo, M., Makhzani, A., K¨uttler, H., Agapiou,\nJ., Schrittwieser, J., et al. Starcraft ii: A new challenge for\nreinforcement learning. arXiv preprint arXiv:1708.04782,\n2017.\nWang, J. X., Kurth-Nelson, Z., Tirumala, D., Soyer, H.,\nLeibo, J. Z., Munos, R., Blundell, C., Kumaran, D., and\nBotvinick, M. Learning to reinforcement learn. arXiv\npreprint arXiv:1611.05763, 2016.\nWang, R., Lehman, J., Clune, J., and Stanley, K. O. Paired\nopen-ended trailblazer (poet): Endlessly generating in-\ncreasingly complex and diverse learning environments\nand their solutions. arXiv preprint arXiv:1901.01753,\n2019.\nYu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C.,\nand Levine, S. Meta-world: A benchmark and evaluation\nfor multi-task and meta reinforcement learning. arXiv\npreprint arXiv:1910.10897, 2019.\nZhang, A., Ballas, N., and Pineau, J. A dissection of over-\nﬁtting and generalization in continuous reinforcement\nlearning. arXiv preprint arXiv:1806.07937, 2018a.\nZhang, C., Vinyals, O., Munos, R., and Bengio, S.\nA\nstudy on overﬁtting in deep reinforcement learning. arXiv\npreprint arXiv:1804.06893, 2018b.\nSupplementary Material\nA. Environment Details\nModifiedCoinrun\n(Easy Mode)\nHeist\n(Easy Mode)\nFigure A.1. Bird’s-eye view of some of the levels from the ModiﬁedCoinrun (Easy Mode) and Heist (Easy Mode) environments. The\ndifﬁculty increases from left to right. Different from the regular versions of these environments (see Figure 1), the background color,\nagent appearance and wall themes do not change across levels.\nModiﬁedCoinrun. The ModiﬁedCoinrun environment is a modiﬁed version of the Coinrun environment (hard mode) where\nthe agent spawns on top of a block in the middle of the level, as opposed to always spawning from the far left. This is the\nonly difference from the original environment and it is modiﬁed in this way to test the agent’s ability to transfer knowledge\nbetween episodes. The goal of the agent is again to reach the coin and get a reward of +10. The episode terminates if the\nagent touches the enemies, lava or saws, or it is not able to get the coin within 1,000 timesteps.\nThe levels in easy and hard sets are created by setting the max difficulty parameter in the source code of Coinrun to 1\n(min) and 3 (max), respectively.\nModiﬁedCoinrun (Easy Mode). The ModiﬁedCoinrun (Easy Mode) environment is a modiﬁed version of the Coinrun\nenvironment (easy mode) where again the agent spawns on top a block in the middle of the level. The only difference\nfrom the ModiﬁedCoinrun environment above is that it is created with the ﬂag distribution mode=easy rather than\ndistribution mode=hard. This ﬂag allows to create different levels where only the level layouts differs between\nthem (see the top row of Fig. A.1).\nThe levels in easy and hard sets are again created by setting the max difficulty parameter in the source code of Coinrun\nto 1 (min) and 3 (max), respectively.\nHeist. The Heist environment is just the original Heist environment with two minor modiﬁcations. To make it partially\nobservable and tractable, we create it with the distribution mode=memory ﬂag and change the world dimension\n(controlled by the world dim parameter) from 21 to 13, respectively. To goal of the agent is to again pick up the colored\nkeys, unlock the corresponding colored doors and reach the gem to get a reward of +10. The episode terminates after 1,000\ntimesteps.\nThe levels in easy and hard sets are created by setting the difficulty parameter in the source code of Heist to 1 (min)\nand 4 (max), respectively.\nHeist (Easy Mode). The Heist (Easy Mode) environment is an easier version of the above Heist environment where the\nbackground color is set to a constant image and the world dimension is changed from 13 to 9 (see the bottom row of\nFig. A.1).\nThe levels in easy and hard sets are created by setting the difficulty parameter in the source code of Heist to 1 (min)\nA Brief Look at Generalization in Visual Meta-Reinforcement Learning\nand 2 (max), respectively.\nB. Network Architecture\nFollowing Cobbe et al. (2019), we use the IMPALA CNN architecture (Espeholt et al., 2018) for the visual feature extraction\npart of our network. We then concatenate the output of the CNN with the vector containing the previous action (one-hoted),\nprevious reward and previous done signal, and pass it through a fully connected layer with 256 units. Finally, we pass this\n256 dimensional vector through an LSTM (Hochreiter & Schmidhuber, 1997) with 256 units and the output of the LSTM is\nthen fed to two separate fully connected layers (with size 15 and 1) corresponding to the logits for the policy and the value\nof the state. Except for the last layers, all layers have a ReLU nonlinearity.\nC. Hyperparameters\nThe hyperparameters of the RL2 agent, which is built on top of PPO, is given in Table C.1.\nTable C.1. Hyperparameters for the RL2 agent.\nLearning rate\n5e−4\nDiscount\n0.99\nGAE λ\n0.99\nKL coefﬁcient\n0.5\nTarget for KL divergence\n0.01\nEntropy coefﬁcient\n0.01\nPPO clip parameter\n0.2\nGradient clip\n0.5\nMax sequence length\n2000\n# of workers\n8\n# of environments per worker\n16\n# of episodes in a trial\n2\n# of SGD iterations\n1\nBatch size\n16000\nValue function loss coefﬁcient\n0.5\n",
  "categories": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ],
  "published": "2020-06-12",
  "updated": "2020-07-03"
}