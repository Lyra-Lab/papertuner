{
  "id": "http://arxiv.org/abs/2006.07698v2",
  "title": "Transferring Monolingual Model to Low-Resource Language: The Case of Tigrinya",
  "authors": [
    "Abrhalei Tela",
    "Abraham Woubie",
    "Ville Hautamaki"
  ],
  "abstract": "In recent years, transformer models have achieved great success in natural\nlanguage processing (NLP) tasks. Most of the current state-of-the-art NLP\nresults are achieved by using monolingual transformer models, where the model\nis pre-trained using a single language unlabelled text corpus. Then, the model\nis fine-tuned to the specific downstream task. However, the cost of\npre-training a new transformer model is high for most languages. In this work,\nwe propose a cost-effective transfer learning method to adopt a strong source\nlanguage model, trained from a large monolingual corpus to a low-resource\nlanguage. Thus, using XLNet language model, we demonstrate competitive\nperformance with mBERT and a pre-trained target language model on the\ncross-lingual sentiment (CLS) dataset and on a new sentiment analysis dataset\nfor low-resourced language Tigrinya. With only 10k examples of the given\nTigrinya sentiment analysis dataset, English XLNet has achieved 78.88% F1-Score\noutperforming BERT and mBERT by 10% and 7%, respectively. More interestingly,\nfine-tuning (English) XLNet model on the CLS dataset has promising results\ncompared to mBERT and even outperformed mBERT for one dataset of the Japanese\nlanguage.",
  "text": "Transferring Monolingual Model to Low-Resource Language: The Case of\nTigrinya\nAbrhalei Tela1, Abraham Woubie2, Ville Hautam¨aki1\n1School of Computing, University of Eastern Finland, Joensuu, Finland\n2Department of Signal Processing and Acoustics, Aalto University, Espoo, Finland\nabrht@uef.fi, abraham.zewoudie@aalto.fi, villeh@cs.uef.fi\nAbstract\nIn recent years, transformer models have achieved great\nsuccess in natural language processing (NLP) tasks.\nMost\nof the current state-of-the-art NLP results are achieved by us-\ning monolingual transformer models, where the model is pre-\ntrained using a single language unlabelled text corpus. Then,\nthe model is ﬁne-tuned to the speciﬁc downstream task. How-\never, the cost of pre-training a new transformer model is high for\nmost languages. In this work, we propose a cost-effective trans-\nfer learning method to adopt a strong source language model,\ntrained from a large monolingual corpus to a low-resource lan-\nguage. Thus, using XLNet language model, we demonstrate\ncompetitive performance with mBERT and a pre-trained target\nlanguage model on the cross-lingual sentiment (CLS) dataset\nand on a new sentiment analysis dataset for low-resourced lan-\nguage Tigrinya. With only 10k examples of the given Tigrinya\nsentiment analysis dataset, English XLNet has achieved 78.88%\nF1-Score outperforming BERT and mBERT by 10% and 7%,\nrespectively. More interestingly, ﬁne-tuning (English) XLNet\nmodel on the CLS dataset has promising results compared to\nmBERT and even outperformed mBERT for one dataset of the\nJapanese language.\nIndex Terms: transformer model, sentiment analysis, transfer\nlearning\n1. Introduction\nNatural language processing (NLP) [1] problems like machine\ntranslation [2], sentiment analysis [3], and question answer-\ning [4] have achieved great success with the emergence of trans-\nformer models [5, 6, 7], and availability of large corpora and\nintroduction of modern computing infrastructures. Compared\nto the traditional neural network methods, transformer models\nachieve not only lower error rates but also reduce the training\ntime required on down streaming tasks, which makes them eas-\nier to be used by a wide range of applications.\nHowever, most languages (especially the low-resource lan-\nguages) in the world have limited available corpora [8] to train\nlanguage-speciﬁc transformer models [5] from scratch. Train-\ning such a model from scratch can also be quite expensive\nin terms of computational power used [9].\nThus, the simi-\nlar explosion of state-of-the-art NLP models than in English\nlanguage has not been materialized for many other languages.\nThen, naturally, we would like to ﬁnd a way how to push these\nNLP models for multiple languages in a cost-effective manner.\nTo tackle this problem, researchers have proposed multilingual\ntransformer models such as mBERT [6] and XLM [10]. These\nmodels share a common vocabulary of multiple languages and\npre-trained on a large text corpus of the given set of languages\ntokenized using the shared vocabulary. The multilingual trans-\nformer models have helped to push the state-of-the-art results\non cross-lingual NLP tasks [6, 10, 11]. However, most mul-\ntilingual models have performance trade-off between low and\nhigh-resource languages [11]. High-resource languages dom-\ninate the performance of such models, but it usually under-\nperforms when compared to the monolingual models [12, 13].\nMoreover, only ∼100 languages are used for pre-training such\nmodels, which can be ineffective for other unrepresented lan-\nguages [13].\nIt was hypothesized in [14] that the lexical overlap between\ndifferent languages has a negligible role while the structural\nsimilarities, like morphology and word order, among languages\nto have a crucial role in cross-lingual success. In this work, our\napproach is to transfer a monolingual transformer model into a\nnew target language. We transfer the source model at the lexical\nlevel by learning the target language’s token embeddings. Our\nwork provides additional evidence that strong monolingual rep-\nresentations are a useful initialization for cross-lingual transfer\nin line with [15].\nWe show that monolingual models on language A can learn\nabout language B without any shared vocabulary or shared pre-\ntraining data. This gives us new insights on using transformer\nmodels trained on single language to be ﬁne-tuned using a la-\nbeled dataset of new unseen target languages. Furthermore, this\nhelps the low-resource languages to use a monolingual trans-\nformer model pre-trained with high-resource language’s text\ncorpus. By using this approach, we can eliminate the cost of\npre-training a new transformer model from scratch. Moreover,\nwe empirically examined the ability of BERT, mBERT, and XL-\nNet to generalize on a new target language. Based on our exper-\niments, XLNet model can generalize more on new target lan-\nguages. Finally, we publish the ﬁrst publicly available senti-\nment analysis dataset for the Tigrinya language.\n2. Tigrinya\nTigrinya is a language commonly used in Eritrea and Ethiopia,\nwith more than 7 million speakers worldwide [16]. It is one of\nthe Semitic languages with the likes of Amharic, Arabic, and\nHebrew [17]. While these languages have received a reason-\nably good focus from the NLP research community, Tigrinya\nhas been one of the under-studied languages in NLP with no\npublicly available datasets and tools for NLP tasks such as ma-\nchine translation, question answering, and sentiment analysis.\nTigrinya has its own alphabet chart called Fidel with some\nletters shared by other languages such as Amharic and Ti-\ngre [18]. The writing system is derived from Geez language, in\nwhich each letter has a syllable of consonant + vowel, except in\nrare cases [17, 19]. Tigrinya alphabet has 35 base letters with 7\nvowels and some extra letters formed by variation of those base\nletters with 5 vowels to form a list of around 275 unique sym-\nbols [20]. By default, Tigrinya has subject-object-verb (SOV)\narXiv:2006.07698v2  [cs.CL]  19 Jun 2020\nFigure 1: Transfer a monolingual transformer model to new target language\nword order, although this is not always the case with some ex-\nception [16, 21]. The writing system is from left to right with\neach word is separated by a space; however, Tigrinya is a mor-\nphologically rich language where multiple morphemes can be\npacked together in a single word.\nThe morphology of Tigrinya is similar to Semitic lan-\nguages, including Amharic and Arabic, which have a root-and-\npattern type [16]. Moreover, Tigrinya has a complex morpho-\nlogical structure with many variations of a given root verb not\nonly by its preﬁx and sufﬁx but also its internal inﬂections of the\nroot-and-pattern type [16, 18]. For example, from a given root\nword terefe (fail), we can change its internal morphemes to have\nother new forms terifu (he failed) and terifa (she failed). Such\ngender differences make structural change in the root word. Be-\nsides this, Tigrinya has conjunctions and prepositions as part of\nthe word itself.\nTedla and Yamamoto [16] studied a detailed morphologi-\ncal segmentation of Tigrinya language. The authors chose to\nuse Latin transliterations of the given Geez scripts due to the\nsyllabic properties of Tigrinyas letters, which can result in al-\nterations of characters in the segmentation boundaries. How-\never, in this work, we have used the natural Geez script text for\nour sentiment analysis task. A language-independent tokenizer,\nSentencePiece [22], is trained using a large Tigrinya text cor-\npus used for training our TigXLNet model to segment a natural\nGeez script based Tigrinya text input.\n3. Cross-lingual transformer models\n3.1. Background\nMultilingual transformer models are designed to have one com-\nmon model representing multiple languages and then ﬁne-tune\non a downstream task of those languages [6, 10, 11]. Multilin-\ngual BERT uses the same masked language model (MLM) [6]\nobjective used for monolingual BERT trained using multiple\nlanguages. XLM, in contrast, tries to leverage parallel data by\nproposing a translation language model (TLM) [10]. XLM-\nR [11] has pushed the state-of-the-art results in many cross-\nlingual tasks by following the approach used by XLM, but scal-\ning up the amount of training data and uncovering the low-\nresource vs. high-resource trade-off.\nOn the other hand, Chi et al. [23] proposed a teacher-student\nframework based ﬁne-tuning technique on a new target lan-\nguages text classiﬁcation task, while Artetxe et al. [15] pro-\nposed a zero-shot shot based ﬁne-tuning method to transfer a\nmonolingual model into a new target language. Those zero-\nshot techniques are relatively cost-effective with less or zero\nnumbers of labeled data required on target languages. How-\never, none of them uses permutation language model (PLM) [7]\nbased XLNet, which, based on our experiment, could lead to\nbetter performance for unseen languages.\n3.2. Language model objectives\nMLM is an auto-encoding based pre-training language mod-\neling objective, in which the model is trained to predict a set\nof corrupted tokens represented by [MASK] from a given sen-\ntence. From a given set of tokens of an input sentence with size\nT, x = [x1, x2, x3, ... xT ], BERT ﬁrst masks some tokens, y =\n[y1, y2, y3, ... yN], of the total given tokens where N < T. Then\nthe learning objective will be to predict the masked tokens back\nwith:\nmax\nθ\nlog pθ(y|x) =\nN\nX\nt=1\nlog pθ(yt|x)\nTLM objective is an extension for MLM to take advantage\nof the parallel corpus for multilingual language representa-\ntions. While the mathematical formulation is kept the same with\nMLM, TLM has more contextual information to learn from dur-\ning pre-training. PLM is auto-regressive language modeling,\nwhich has access to bi-directional context while keeping the na-\nture of auto-regressive language modeling. This way, PLM tries\nto resolve the limitations of MLM, independence assumption,\nand input noise pointed out by Yang et al. [7].\n3.3. Proposed method\nIn this work, we have proposed a new transfer learning ap-\nproach to use an already existing English monolingual trans-\nformer model to tackle downstream tasks of other unseen tar-\nget languages.\nHence, the language model pre-training for\nthe source language is not a necessary step, making the pro-\nposed method more cost-efﬁcient. The transformer models con-\nsidered as source model in this work are BERT, XLNet, and\nmBERT. Figure 1 shows the graphical illustration of the pro-\nposed method.\nTo transfer a monolingual transformer model into a new tar-\nget language, we followed three different steps. First, we gen-\nerate a vocabulary for the target language using SentencePiece\nmodel trained on the language’s unlabelled dataset. Then, we\ntrain a context-independent Word2Vec [24] based token embed-\ndings for the vocabulary generated in the previous step. Finally,\nthe given transformer model is ﬁne-tuned on a labeled dataset\nof the target language with frozen token embeddings. By freez-\ning the token embeddings of the model during ﬁne-tuning, the\ntransformer model can preserve the learned embeddings. This\nis necessary because the embedding technique used is a context-\nindependent token embedding; however, in practice, it does not\nseem to have more performance difference.\n4. Experimental Setup and Result\nIn this work, we conducted our experiment for the Tigrinya sen-\ntiment classiﬁcation task on a newly created Tigrinya sentiment\nanalysis dataset. Furthermore, we have tested our experiment\non one of the standard cross-lingual datasets for sentiment anal-\nysis, the cross-lingual sentiment (CLS) dataset [25].\n4.1. Datasets\nWe have constructed a sentiment analysis dataset for Tigrinya\nwith two classes as positive and negative. The data has been\ncollected from YouTube comments of Eritrean and Ethiopian\nmusic videos and short movie channels. It consists of around\n30k automatically labeled training set, and two professionals\nhad labeled the test set independently and considered only when\nthey gave the same label to form the ﬁnal 4k test examples with\n2k positive and 2k negative. Additionally, we have used the\nCLS dataset for testing our proposed method on languages like\nGerman, French, and Japanese. It consists of English, German,\nFrench and Japanese languages collected from Amazon reviews\non three different domains (Music, Books, and DVD).\nText augmentation methods such as [26, 27] are shown\nto increase the performance of text classiﬁcation tasks with\nless available data. Back translation based data augmentation\nproposed by Sugiyama and Yoshinaga [26] requires a good\nmachine translation model which is not always available for\nlow-resourced languages like Tigrinya. Alternatively, Wei and\nZou [27] proposed a natural but effective data augmentation us-\ning four different operations: synonym replacement, random\nswap, random insertion, and random deletion.\nWe follow a\nsimilar approach by Wei and Zou [27]; however, we have used\nWord2Vec embeddings-based synonym replacement. In all our\nexperiments, we have used the human-labeled 4k dataset as our\ntest set while the augmented ∼50k dataset for training unless\notherwise stated.\n4.2. Baseline Models\nWhen evaluating our proposed method using the CLS dataset,\nwe used mBERT ﬁne-tuned on the same sized training data.\nThis way, we can examine the ability of XLNet to under-\nstand new languages during ﬁne-tuning comparatively with\nthe mBERT trained on 104 languages, including the four lan-\nguages of the CLS dataset. For Tigrinya sentiment analysis,\nwe have evaluated the proposed method against a new trans-\nformer model, TigXLNet, which is purely pre-trained in a sin-\ngle Tigrinya language text corpus, then used to ﬁne-tune on our\nnew dataset. Furthermore, we have compared the generaliza-\ntion of BERT, XLNet, and mBERT on unseen target language,\nTigrinya, with different conﬁgurations.\n4.3. Results on Tigrinya Sentiment Analysis Dataset\nAs shown in Table 1, ﬁne-tuning XLNet on Tigrinya senti-\nment analysis dataset is comparable to the result of ﬁne-tuning\nTigXLNet and better than mBERT ﬁne-tuned on the same\ndataset. Using the English XLNet model as the source trans-\nformer model, the proposed method has achieved 81.62% in\nF1-Score, while the same method using mBERT source model\nhas a lower F1-Score at 77.51%. Furthermore, the effectiveness\nof initializing the model with language-dependent embeddings\ninstead of using random embeddings (source language embed-\ndings) is also presented in the Table 1. Both XLNet and mBERT\nunder-performed when using random token embeddings com-\npared to their corresponding models initialized with Word2Vec\ntoken embeddings. This shows that transferring a monolingual\nTable 1: Fine-tuning TigXLNet, mBERT and XLNet using\nTigrinya sentiment analysis dataset.\nModels\nEmbedding\nF1-Score\nTigXLNet\n-\n83.29\nmBERT\n+random token embed.\n76.01\n+word2vec token embed.\n77.51\nXLNet\n+random token embed.\n77.83\n+word2vec token embed.\n81.62\nXLNet model into a new language like Tigrinya can result in\na good performance at a little cost without needing to train\nlanguage-speciﬁc transformer models.\n4.4. XLNet Frozen Weights\nWe have tested the performance of XLNet model on Tigrinya\nsentiment analysis with different conﬁgurations, as presented\nin Table 2.\nIn the ﬁrst setup, we randomize the pre-trained\nXLNet model weights to examine if the performance we gain\nin an unseen language is from the learned XLNet weights, not\njust from the XLNet neural network architecture, and its abil-\nity to learn new features during ﬁne-tuning. As we may ex-\npect, the model’s performance decreases drastically compared\nto the model started with the pre-trained XLNet weights. The\nXLNet model initialized with randomized weights results in\n53.93% F1-Score, which is close to a result of a random model\ntrained on binary classiﬁcation tasks.\nIn the second conﬁg-\nuration, when all the transformer layers of XLNet are frozen\nduring ﬁne-tuning, the performance of the model has increased\nsigniﬁcantly from the previous conﬁguration of randomly ini-\ntialized weights by ∼15% (F1-Score). From these results, we\ncan conclude that XLNet (English) model (initialized with ran-\ndom weights) cannot learn from the given labeled dataset at the\nﬁne-tuning stage. On the other hand, the pre-trained weights of\nXLNet have a general understanding of unseen languages like\nTigrinya. Lastly, by ﬁne-tuning XLNet on a labeled dataset of\nthe target language, the model’s performance gets better.\nTable 2: Fine-tuning XLNet using Tigrinya sentiment analysis\ndataset with different settings.\nModel\nSettings\nF1-Score\nXLNet\n+Random XLNet weights\n53.93\n+Frozen XLNet weights\n68.14\n+Fine-tune XLNet weights\n81.62\n4.5. Result on CLS Dataset\nIn this experiment, monolingual XLNet is compared with\nmBERT. As in Table 3, monolingual XLNet pre-trained using\nEnglish text corpus has abstract representations of other unseen\nlanguages such as German, French, and Japanese. Although the\nF1-Score of mBERT is expected to be higher for all datasets\nof those languages (mBERT pre-training language set includes\nthose CLS languages), XLNet has achieved comparable results,\nTable 3: F1-Score on CLS dataset, note that we have used the same hyper-parameters and same dataset size for all models (all train\nand the unprocessed dataset of CLS is used for training, and the model is evaluated on the given test set)\nModels\nEnglish\nGerman\nFrench\nJapanese\nAvg.\nBooks\nDVD\nMusic\nBooks\nDVD\nMusic\nBooks\nDVD\nMusic\nBooks\nDVD\nMusic\nXLNet\n92.90\n93.31\n92.02\n85.23\n83.30\n83.89\n73.05\n69.80\n70.12\n83.20\n86.07\n85.24\n83.08\nmBERT\n92.78\n90.30\n91.88\n88.65\n85.85\n90.38\n91.09\n88.57\n93.67\n84.35\n81.77\n87.53\n88.90\nespecially with German and Japanese dataset.\nFurthermore,\nXLNet even outperforms mBERT in one of the experiments\nfor the Japanese language. From the results of this, we can\ndeduce that XLNet is strong enough, compared to mBERT, to\nlearn about unseen languages during ﬁne-tuning of the new tar-\nget language.\n4.6. BERT vs. XLNet on New Language\nIn this experiment, as presented in Table 4, MLM based BERT\nand mBERT are compared to PLM based XLNet on a new tar-\nget language: Tigrinya. By freezing all parameters of BERT,\nmBERT, and XLNet, except corresponding embedding and ﬁ-\nnal linear layers, we can observe that BERT and mBERT are\nclose to a random model with binary classiﬁcation task. While\nfrozen XLNet model results in more than 10% F1-Score in-\ncrease to both BERT and mBERT. This clearly shows that the\npre-trained weights of XLNet have better generalization ability\non unseen target language compared to both BERT and mBERT\npre-trained weights. Furthermore, the positive effect of initializ-\ning all models with language-speciﬁc token embeddings can be\nobserved from the Table 4. By initializing BERT and mBERT\npre-trained models with Word2Vec token embeddings, the per-\nformance on ﬁne-tuning Tigrinya sentiment analysis dataset has\nincreased by around 2% (F1-Score) when compared to their cor-\nresponding pre-trained models with random weights. Finally,\nwe can observe that PLM based XLNet has outperformed MLM\nbased BERT and mBERT in all different settings.\nTable 4: Comparison of BERT, mBERT, and XLNet models\nﬁne-tuned using the Tigrinya sentiment analysis dataset. All\nhyper-parameters are the same for all models, including a\nlearning rate of 2e-5, batch size of 32, the sequence length of\n180, and 3 number of epochs.\nModels\nConﬁguration\nF1-Score\nBERT\n+Frozen BERT weights\n54.91\n+Random embeddings\n74.26\n+Frozen token embeddings\n76.35\nmBERT\n+Frozen mBERT weights\n57.32\n+Random embeddings\n76.01\n+Frozen token embeddings\n77.51\nXLNet\n+Frozen XLNet weights\n68.14\n+Random embeddings\n77.83\n+Frozen token embeddings\n81.62\n4.7. Effect of Dataset Size\nFigure 2 shows the effects of training dataset size for the per-\nformance of BERT, mBERT, XLNet, and TigXLNet based on\nthe Tigrinya sentiment analysis dataset. By randomly selecting\n1k, 5k, 10k, 20k, 30k, 40k, and full dataset of ∼50k exam-\nples, the performance of XLNet is dominant when compared to\nBERT and mBERT. All hyper-parameters of the models during\nﬁne-tuning are stayed ﬁxed for all models except for TigXLNet,\nwhere the number of epochs is one as it tends to overﬁt if the\nnumber of epochs is larger. XLNet has achieved an F1-Score\nof 77.19% with just 5k training examples while BERT and\nmBERT require full dataset size (∼50k examples) to achieve\n76.35% and 77.51% respectively. The performance of both XL-\nNet and TigXLNet has increased by less than 3%, with a dataset\nincrease of 40k (10k to 50k). Based on this experiment, around\n10k training examples could be enough to get a comparably\ngood XLNet model ﬁne-tuned for new language (Tigrinya) text\nclassiﬁcation tasks. Finally, with ∼2 hours of ﬁne-tuning XL-\nNet using Google Colab’s GPU, we can save the computational\ncost of pre-training TigXLNet from scratch, which takes 7 days\nusing TPU v3-8 of 8 cores and 128GB memory.\nFigure 2: The effect of dataset size for ﬁne-tuning XLNet,\nTigXLNet, BERT, and mBERT on the Tigrinya sentiment\nanalysis dataset.\n5. Conclusion\nIn this research, we have performed an empirical study on the\nability of XLNet to generalize on a new language. Interestingly,\ntransferring an English XLNet model to a new target language,\nTigrinya, we achieve comparable performance to a monolin-\ngual XLNet model (TigXLNet) pre-trained on Tigrinya text cor-\npus. Computational saving of performing transfer learning only\nis enormous. The proposed method also has comparable re-\nsults to mBERT on CLS dataset, especially on Japanese and\nGerman languages. In our experiment, it is shown that PLM\nbased XLNet has better performance in the case of unseen lan-\nguages when compared to MLM based BERT and mBERT. Fur-\nthermore, we have released a new Tigrinya sentiment analysis\ndataset and a new XLNet model speciﬁcally for Tigrinya lan-\nguage, TigXLNet, which could help NLP downstream tasks of\nTigrinya. Our experiment results also hint that training multi-\nlingual transformer models using PLM could achieve better per-\nformance boost across a range of downstream NLP tasks. This\nis due to the advantages of PLM over other language models\nlike MLM to discover more insights about languages that are\nnot even in the pre-training corpus.\n6. References\n[1] S. Bird, E. Klein, and E. Loper, Natural Language Processing\nwith Python, 1st ed.\nOReilly Media, Inc., 2009.\n[2] Y. Wu,\nM. Schuster,\nZ. Chen,\nQ. V. Le,\nM. Norouzi,\nW. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,\nJ. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws,\nY. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil,\nW. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals,\nG. Corrado,\nM. Hughes,\nand J. Dean,\n“Google’s neural\nmachine translation system: Bridging the gap between human\nand machine translation,” CoRR, vol. abs/1609.08144, 2016.\n[Online]. Available: http://arxiv.org/abs/1609.08144\n[3] B. Liu, Sentiment Analysis and Opinion Mining. Morgan & Clay-\npool Publishers, 2012.\n[4] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad:\n100,000+\nquestions\nfor\nmachine\ncomprehension\nof\ntext,”\nProceedings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, 2016. [Online]. Available:\nhttp://dx.doi.org/10.18653/v1/D16-1264\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is\nall you need,” in Advances in Neural Information Processing\nSystems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, Eds.\nCurran\nAssociates, Inc., 2017, pp. 5998–6008. [Online]. Available:\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n[6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding,” in Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers).\nMinneapolis, Minnesota: Association for\nComputational Linguistics, Jun. 2019, pp. 4171–4186. [Online].\nAvailable: https://www.aclweb.org/anthology/N19-1423\n[7] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov,\nand Q. V. Le, “Xlnet: Generalized autoregressive pretraining\nfor language understanding,” in Advances in Neural Information\nProcessing Systems 32, H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, Eds.\nCurran\nAssociates, Inc., 2019, pp. 5753–5763. [Online]. Available:\nhttps://arxiv.org/pdf/1906.08237.pdf\n[8] S. Ruder, A. Søgaard, and I. Vuli´c, “Unsupervised cross-lingual\nrepresentation learning,” in Proceedings of ACL 2019, Tutorial\nAbstracts, 2019, pp. 31–38.\n[9] C. Wang, M. Li, and A. J. Smola, “Language models with\ntransformers,”\nCoRR, vol. abs/1904.09408,\n2019. [Online].\nAvailable: http://arxiv.org/abs/1904.09408\n[10] A. Conneau and G. Lample, “Cross-lingual language model\npretraining,” in Advances in Neural Information Processing\nSystems 32,\nH. Wallach,\nH. Larochelle,\nA. Beygelzimer,\nF. d'Alch´e-Buc, E. Fox, and R. Garnett, Eds.\nCurran Associates,\nInc., 2019, pp. 7059–7069. [Online]. Available: http://papers.nips.\ncc/paper/8928-cross-lingual-language-model-pretraining.pdf\n[11] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek,\nF. Guzmn, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov,\n“Unsupervised cross-lingual representation learning at scale,”\n2019.\n[12] W. de Vries, A. van Cranenburgh, A. Bisazza, T. Caselli, G. van\nNoord, and M. Nissim, “Bertje: A dutch bert model,” 2019.\n[13] A. Virtanen, J. Kanerva, R. Ilo, J. Luoma, J. Luotolahti,\nT. Salakoski, F. Ginter, and S. Pyysalo, “Multilingual is not\nenough: Bert for ﬁnnish,” 2019.\n[14] K. K, Z. Wang, S. Mayhew, and D. Roth, “Cross-lingual ability of\nmultilingual bert: An empirical study,” 2019.\n[15] M. Artetxe, S. Ruder, and D. Yogatama, “On the cross-lingual\ntransferability of monolingual representations,” 2019.\n[16] Y. Tedla and K. Yamamoto, “Morphological segmentation with\nlstm neural networks for tigrinya,” 2018.\n[17] R. Hetzron, The Semitic Languages.\n270 Madison Ave, New\nYork USA 10016: Routledge, 1997, pp. 426–430.\n[18] O. Osman and Y. Mikami, “Stemming Tigrinya words for\ninformation\nretrieval,”\nin\nProceedings\nof\nCOLING\n2012:\nDemonstration Papers.\nMumbai, India: The COLING 2012\nOrganizing Committee, Dec. 2012, pp. 345–352. [Online].\nAvailable: https://www.aclweb.org/anthology/C12-3043\n[19] M. Tadesse and Y. Assabie,\n“Trilingual sentiment analy-\nsis on social media,” Master’s thesis, Univeristy of Addis\nAbaba, 2018. [Online]. Available: http://etd.aau.edu.et/handle/\n123456789/17926\n[20] Y. K. Tedla, K. Yamamoto, and A. Marasinghe, “Tigrinya\npart-of-speech tagging with morphological patterns and the new\nnagaoka tigrinya corpus,” International Journal of Computer\nApplications, vol. 146, no. 14, pp. 33–41, Jul 2016. [On-\nline]. Available: http://www.ijcaonline.org/archives/volume146/\nnumber14/25468-2016910943\n[21] A. Sahle, A comprehensive Tigrinya grammar.\nAsmara, Eritrea:\nLawrenceville NJ: Red Sea Press, Inc., 1998, pp. 71–72.\n[22] T. Kudo and J. Richardson, “Sentencepiece: A simple and lan-\nguage independent subword tokenizer and detokenizer for neural\ntext processing,” 2018.\n[23] Z. Chi, L. Dong, F. Wei, X.-L. Mao, and H. Huang, “Can monolin-\ngual pretrained models help cross-lingual classiﬁcation?” 2019.\n[24] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estima-\ntion of word representations in vector space,” 2013.\n[25] P. Prettenhofer and B. Stein, “Cross-language text classiﬁcation\nusing structural correspondence learning,” in Proceedings of the\nACL, 2010.\n[26] A. Sugiyama and N. Yoshinaga, “Data augmentation using\nback-translation for context-aware neural machine translation,” in\nProceedings of the Fourth Workshop on Discourse in Machine\nTranslation (DiscoMT 2019).\nHong Kong, China: Association\nfor Computational Linguistics, Nov. 2019, pp. 35–44. [Online].\nAvailable: https://www.aclweb.org/anthology/D19-6504\n[27] J. Wei and K. Zou, “Eda:\nEasy data augmentation tech-\nniques for boosting performance on text classiﬁcation tasks,” in\nEMNLP/IJCNLP, 2019.\n",
  "categories": [
    "cs.CL",
    "cs.LG"
  ],
  "published": "2020-06-13",
  "updated": "2020-06-19"
}