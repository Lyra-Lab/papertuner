{
  "id": "http://arxiv.org/abs/2006.09213v2",
  "title": "A Hybrid Natural Language Generation System Integrating Rules and Deep Learning Algorithms",
  "authors": [
    "Wei Wei",
    "Bei Zhou",
    "Georgios Leontidis"
  ],
  "abstract": "This paper proposes an enhanced natural language generation system combining\nthe merits of both rule-based approaches and modern deep learning algorithms,\nboosting its performance to the extent where the generated textual content is\ncapable of exhibiting agile human-writing styles and the content logic of which\nis highly controllable. We also come up with a novel approach called HMCU to\nmeasure the performance of the natural language processing comprehensively and\nprecisely.",
  "text": "A Hybrid Natural Language Generation System Integrating Rules and\nDeep Learning Algorithms\nWei Wei1, Bei Zhou2, Georgios Leontidis1\n1Department of Computing Science,\nUniversity of Aberdeen\nw.wei.19@abdn.ac.uk\ngeorgios.leontidis@abdn.ac.uk\n2School of Computer Science,\nThe University of Sydney\nbei.zhou@sydney.edu.au\nAbstract— This paper proposes an enhanced natural lan-\nguage generation system combining the merits of both rule-\nbased approaches and modern deep learning algorithms, boost-\ning its performance to the extent where the generated textual\ncontent is capable of exhibiting agile human-writing styles and\nthe content logic of which is highly controllable. We also come\nup with a novel approach called HMCU to measure the per-\nformance of the natural language processing comprehensively\nand precisely.\nI. INTRODUCTION\nNowadays,\nmainstream\nnatural\nlanguage\ngeneration\n(NLG) techniques fall into two categories, i.e. conventional\nrule-based approaches and deep learning algorithm-based\napproaches, each of which carries exclusive pros and cons.\nThe former approaches can produce high-quality text with\ncontrollable context logic but suffering from the fact that\nthe generated text style is inﬂexible which directly abates its\npossibility of wide industrial applications, such as Search\nEngine Optimization (SEO). Albeit the test yielded from\ndeep learning-based methods demonstrates a style very close\nto the one written by a human, its pronounced pitfall that\nthe text context is not under control commonly causes the\ngenerated text to be meaningless.\nThe approach we propose below builds upon the advan-\ntages of both and eliminates their respective disadvantage\nso that the generated text holds human-writing styles whilst\nthe context logic being controllable. The architecture can be\nseen through Fig. 1.\nFig. 1.\nEnhanced Nature Language Generation System Architecture\nEven though the rule-based and deep-learning-based mod-\nels work collaboratively when generating text, their model\nbuilding and training processes are completely different.\nThe rule-based model is constructed through predeﬁned\ninstructions with no training required. To compare it against\nthe deep learning model, the parameters of the deep learning\nmodel need to be trained. The dataset utilized to train the\ndeep learning model is not the data derived from the rule-\nbased model but rather the generic data after cleansing and\npreprocessing to ensure that the content and the paragraphing\nof which are semantically and grammatically correct and\nmaintain human-writing style so that the trained deep learn-\ning model can yield text, satisfying our expectation.\nII. PRELIMINARIES\nThis section presents the HMCU analysis model that is\nadopted to compare and evaluate the performance of various\nNLG model, along with the brief introduction of the essential\nconcepts regarding rule-based as well as deep learning-\nbased NLG techniques that are conducive to understand our\nproposed model coherently.\nA. HMCU Analysis Model\nThe measurement of the NLG system cannot be conducted\nquantitively, via using metrics such as Mean Square Error\nor Mean Absolute Error which are quantitative metrics that\nreﬂect the loss between predicted outcomes and true labels\nof data samples. Therefore, in this paper we propose a novel\nmethod that evaluates the NLG system in terms of four\npredeﬁned metrics. The metrics applied to measure the NLG\nsystem can be summarized into four broad categories, i.e. H,\nM, C, U, the indications of which are listed below:\n• H(Human language): The similarity between the gener-\nated text and human writing text.\n• M(Machine style): The similarity between the generated\ntext and machine writing text.\n• C(Controllable context logic): The degree to which the\ncontext logic is under control.\n• U(Uncontrollable context logic): The degree to which\nthe content logic is out of control.\nH and M evaluate the style of the generated text. H stands\nfor the extent to which the texts manifest Human-writing\nstyle. Similarly, M represents the extent to which the texts\nmanifest Machine-writing style. C and U evaluate the extent\nto which the context logic is under control. These four\nmetrics are not designed to be used in a stand-alone but,\nrather in an aggregated manner. Any NLG model could be\nmeasured by utilizing the combination of these metrics, as\nshown in Table I.\narXiv:2006.09213v2  [cs.CL]  17 Jun 2020\nTABLE I\nHMCU EVALUATION APPROACH\nC\nU\nControllable context logic\nUncontrollable context logic\nH\nHC\nHU\nHuman\nlanguage\nThe generated text has a\nhuman-writing style and\nthe context logic is\ncontrollable\nThe generated text has a\nhuman-writing style and\nthe context logic is hard\nto control\nM\nMC\nMU\nMachine\nstyle\nThe generated text has a\nmachine-writing style and\nthe context logic is\ncontrollable\nThe generated text has a\nmachine-writing style and\nthe context logic is hard\nto control\nCommonly, the text generated from the rule-based NLG\nmodel tends to fall into the MC category whereas the one\ngenerated from the deep learning-based NLG model is likely\nto be categorized into HU when the models are constructed\nand trained appropriately.\nThe ideal output-text of the NLG system should fall in\nthe HC category, which is a rather improbable outcome.\nThus, we must balance different constraints out and con-\nsider the characteristics and peculiarities of the real-world\napplications before deploying the model into production. In\nthe empirical research section, the applications of utilizing\nHMCU approaches to evaluate different NLG systems are\ndemonstrated.\nB. Rule-based NLG Techniques\nThe incentives for using NLG techniques is to obtain\nnatural language text for a wide spectrum of inputs rather\nthan canned and ﬁxed text with simple templates. Namely,\nthe rule-based NLG techniques generate text by predeﬁned\nrules. Albeit high quality in terms of human readability is\nguaranteed, the stiff style is the most salient difﬁculty to\novercome.\nRule-based NLG systems were introduced by Reiter and\nDale [1]. They listed several tasks falling into the problems\nthat the NLG system could resolve, including content de-\ntermination, discourse planning, sentence aggregation, lex-\nicalization, referring expression generation, and linguistic\nrealization. In addition to these tasks, they also proposed a\npipeline with three stages to tackle these tasks just described,\nwhich composes of text planning, sentence planning, linguis-\ntic realization.\nThe core of rule-based NLG techniques is the idea of\ngeneration as a deterministic decision-making process elim-\ninating some forms of stochasticity. For example, the rule-\nbased grammar credits the capacity to yield previously un-\nseen output through recursive rules[2]. Although these rules\npreserve the randomness of generated texts to some extent,\nthe outputs are still deterministic. Given the same inputs, the\noutputs are always identical under speciﬁc rules.\nC. Deep Learning-Based NLG techniques\nNowadays, to the best of our knowledge, the majority of\ndeep learning techniques that are adopted to implement natu-\nral language generation applications fall into two categories,\neither recurrent neural network(RNN) such as Long Short-\nTerm Memory[3] or Transformer[4] models, as listed below:\n• The rapid innovation and development of deep learning\ntechniques enabled their applications on NLG systems.\nLong Short-Term Memory (LSTM) models and their\nmultiple variations have been extensively applied to\nNature Language Processing (NLP) tasks because of\ntheir prowess in handling time series data. Recently,\nstudies have shown that LSTMs can be leveraged to\ngenerate whole sentences that are analogous to the\nones written by a human. For instance, this paper[5]\nproposed an encoder-decoder structure complemented\nby a discriminator that could produce high-readable and\ncontext awareness sentences, which is the state-of-the-\nart approach.\n• In the domain of deep learning, sequence-to-sequence\nmodels are commonly using an encoder-decoder archi-\ntecture to complete tasks like machine translation where\none sequence written in one language is converted to a\nsequence of another language while the meaning is fully\npreserved, which is considered a similar task to para-\nphrasing. Transformer models that adopt the encoder-\ndecoder architecture have been applied to a wide range\nof NLG applications due to its unique architecture and\nhigh performance. GPT-3[6] is one of the remarkable\nexamples of the transformer model being utilized on\nsome NLP tasks and has very recently demonstrated\nbrilliant results. However, its tremendous size becomes\nthe major hindrance that thwarts its popularity.\nDue to the maturity and reliability of the LSTM model in\nNLG related tasks, we adopt it as the deep-learning technique\nto further process the text generated from the rule-based\nmodel. The transformed model could also be an option that\nis used as an alternative to the LSTM model.\nIII. ENHANCED NATURAL GENERATION SYSTEM WITH\nHYBRID APPROACHES\nMachines can generate text with controllable context logic\nto accord with the intention of the human through Rule-based\nNLG techniques; however the generated output probably\nmaintain a similar writing style (machine writing style) since\nthe output text is based on presets templates. On the other\nhand, machines can generate human writing style text based\non deep learning techniques but the context logic of the\noutput cannot be controlled by a human. The purpose of this\npaper is to propose a novel hybrid method to enhance NLG\nsystems and make them capable of performing with a highly\ncontrollable context logic and a ﬂexible human writing style.\nFig. 2 shows our proposed text quality analysis model\ncalled HMCU, along with the output quality of the text gen-\nerated by machines that can be analyzed through this model.\nGiven that HMCU is a text quality evaluation method, it can\nbe used in actual applications independently or combined\nwith other evaluation methods.\nThe quality of the text generated by machines through\nrule-based techniques is usually in the fourth quadrant in\nFig. 2, and the quality of the text generated based on deep\nFig. 2.\nHMCU analysis model\nlearning techniques usually in the second quadrant in Fig. 2.\nHowever, ideally, text quality should be in the ﬁrst quadrant,\ni.e. closer to human writing style, with the context logic\ncontrollable by humans. Therefore, the main task of the\nenhanced NLG system is to allow the ﬁnal output text quality\nto move to the ﬁrst quadrant as much as possible.\nIn general, our proposed NLG system works in the form\nof a pipeline, as demonstrated in ﬁg. 1. It ﬁrstly takes the\ngenerated text with high MC from the rule-based model and\nfeeds it to a deep learning model that is able to yield text\nwith high HC. The generated text from the deep learning\nmodels has high HU because it is unrestrained from any\nestablished rules leading to output styles that are not similar\nto the preset templates. Therefore the style of generated text\nfrom the deep learning models is ﬂexible and closer to the\nhuman writing style.\nAlthough the context logic cannot be controlled by humans\nif the text is generated based on deep learning models, people\ncan restrict the deep learning part to only paraphrasing the\ntext generated from the rule-based part. This means that in\nour scheme, the deep learning part is not used for the creation\nof text, but rather for paraphrasing of sentences. To make the\nﬁnal context logic controllable, the deep learning part should\nparaphrase the text sentence by sentence rather than the entire\ntext in one go. It is worth pointing out that the training data\nof the deep learning model is generic data rather than text\ndata generated based on rules to avoid making the writing\nstyle inﬂexible, which would be the case if people were to\nuse rule-based output as training data. This means that the\ndeep learning model can paraphrase any sentence, and make\nthe style of the paraphrasing sentence closer to the human\nstyle while maintaining the original meaning(context logic).\nThe NLG system we propose can utilize the advantages of\nboth methods while eliminating their disadvantages to some\nextent. As shown in ﬁg. 1, in this example, our system takes\ninput as structure data that consists of some attributes, like\nthe time the event occurred, the people involved, the location\nwhere the event happened, etc. The rule-based system then\nproduces text in accord with these given attributes, which\nis subsequently used as an input to the deep learning-based\nsystem aiming for the ﬁnal output to both contain human-\nwriting style and controllable context logic.\nIV. EMPIRICAL RESEARCH\nA real-time news generation system will be used to test\nthese ideas in this section, and the output text from rule-\nbased system, deep learning-based system, and a proposed\nhybrid system will be compared to evaluate the performance\nof the proposed system.\nA. Case Study\nOne of the many application domains of the NLG system\nis the real-time news generation that curtails enormously\nthe time for the news to be written properly[7]. We have\ndeveloped 3 systems for news generation using rule-based,\ndeep learning-based, and hybrid-based techniques separately.\nThe output text will be used to evaluate the performance of\nthese systems from two aspects, i.e. controllability of context\nlogic and ﬂexibility of writing style. We assume that a more\nﬂexible writing style indicates attributes closer to the human\nwriting style.\nTo make the experimental results more credible, the testing\ndata has been split into 5 groups. From the perspective of the\ncontrollability of context logic, the text generated based on\nrules has the highest average context logic similarity, which\nis 0.744. In contrast, the text generated based on LSTM has\nthe lowest average context logic similarity at 0.059. Besides,\nthe average context logic similarity of the text generated by\nthe hybrid method is 0.173 (detailed evaluation methods and\nsteps will be shown in Section F). The experimental results\nshow that the text generated based on rules still has the\nhighest controllable context logic, but the hybrid method can\ngreatly improve the controllability of the context logic of the\ntext generated by the deep learning approaches.\nFrom the perspective of the ﬂexibility of writing style,\nthe text generated using the LSTM technique is the farthest\nfrom the machine writing style, with the similarity to the\nmachine writing style being 0.043. On the contrary, the text\ngenerated using the rule-based approaches has the highest\nsimilarity with a machine writing style, at 0.242. The sim-\nilarity to a machine writing style of the text generated by\nthe hybrid approach is 0.098, which is much lower than the\nsimilarity achieved by the rule-based technique. This means\nthat compared to the rule-based text generation technique,\nthe hybrid approach can greatly improve the ﬂexibility of\nthe writing style of the text.\nThe hybrid approach is a way to balance the performance\nof the generated text between the controllability of context\nlogic and ﬂexibility of writing style. It can make the output\ntext not only reach the degree of human acceptance and\nunderstanding of the context logic of the text but also keep\nthe output text within a ﬂexible writing style.\nB. Rule-based Natural Language Generation system\nWe got 900 short news data from a news website(i.e.\nindiatimes.com) and split it into 2 parts, i.e. 675 rows for\ntraining data and 225 rows for testing data. The testing data\nhas been divided into 5 groups, where each group includes\n45 rows of testing data. Notice that the training data here is\nnot prepared for the paraphrasing system, but for the system\nthat only uses the LSTM technique for text generation.\nThen, we prepared a one-to-one structured data for each\nrow of the testing data. The structure of the structured data\nas used in this project has been designed as follows:\n[{”subject” : ””, ”verb” : ””, ”object” : ””, ”reason” :\n””, ”purpose” : ””, ”area” : ””, ”date” : ””, ”week” :\n””, ”year” : ””, ”month” : ””}, {”subject” : ””, ”verb” :\n””, ”object” : ””, ”reason” : ””, ”purpose” : ””, ”area” :\n””, ”date” : ””, ”week” : ””, ”year” : ””, ”month” : ””}]\nEach dictionary in the list corresponds to a sentence of\nnews, and each dictionary contains various elements of the\nnews, such as date, subject, and area. This is not the only way\nto design structured data. In actual projects, people can also\ndesign different structured data according to their preferences\nand needs.\nWith structured data, people can design detailed rules\nand templates to complete the entire Rule-based natural\nlanguage generation system[8]. For example, the following\nis an example generated by a Rule-based natural language\ngeneration system implemented by us.\n“Germany is well placed avoid wave of coronavirus.\nMoreover, the absence of a vaccine meant social distancing\nwere necessary because Scholz said on Friday. Moreover,\nwe are living with the virus. Besides, it will change we can\nhave new medical therapies. Moreover, we have to organise\nour lives in order to avoid a second wave.”\nThrough this example, people can know that the context\nlogic of these texts can be understood by humans, and the\ncontext logic of the content can be controlled by the NLG\nsystem designer. However, when the number of samples\ngenerated becomes large, people will ﬁnd that the writing\nstyle of these texts is very similar (i.e. inﬂexible) and has a\nvery high machine writing style. Numbers of problems will\noccur in the actual use scenarios, such as the user’s aesthetic\nfatigue in reading, not conducive to SEO, and so on.\nC. Natural Language Generation system based on LSTM\nThis part is not needed in the hybrid approach we pro-\nposed, but to be able to compare the performance of text\ngenerated by various techniques, we developed an NLG\nsystem using the LSTM technique.\nThe LSTM NLG model needs to be trained before using\nit. The training data used in this for this part is one described\nin subsection B. After training the model, one can enter\nkeywords into the model, and the system will automatically\ncreate sentences or news based on the trained LSTM model.\nTo evaluate the model developed, we extracted key\ninformation from each row of the testing data and entered\nit into the LSTM NLG system. After that, the system\nautomatically generated different pieces of news. One of\nthe news samples generated by the NLG system based on\nLSTM is as follows:\n“Germany is well to the bodies. The absence of the virus\nto the bodies. We are living to function. It will change to\nMedical to proper to proper to proper to proper to proper to\nproper to proper to proper to proper to proper to proper to\nproper to proper to proper to proper. We have to the country.”\nIt can be seen from the news sample that although this\nnews looks very ﬂexible in writing style the context logic\nof the content is uncontrollable. Thus, this news has no real\nmeaning. Many people try to use the LSTM technique to\ngenerate fake news, but these fake news cannot be applied\nin the actual environment, because in practice real news are\nwritten based on factual information.\nD. Paraphrasing system based on Deep Learning techniques\nThe paraphrasing system can be realized based on deep\nlearning techniques. In this paper we used the LSTM\ntechnique as the paraphrasing system[9]. Notice that the\nparaphrasing system and the system presented in subsection\nC are not the same systems, despite both being based on\nLSTMs.\nThe Sequence-to-Sequence model will be used in this\nsection. The purpose of the paraphrasing part is to allow the\nmodel to automatically paraphrase arbitrary sentences, rather\nthan just paraphrasing news-related sentences[10]. Therefore,\nthe training data of the paraphrasing system is not the speciﬁc\nnews data mentioned in subsection B, but generic data.\nThe data set, called ParaNMT-50M, will be used as the\ntraining data for the paraphrasing system in this paper[11].\nHowever, people can choose other data-sets as the training\ndata for the paraphrasing section according to their prefer-\nences.\nAfter the paraphrasing system has been developed and\ntrained, any input sentence can be paraphrased. For example,\nwhen entering \"how are you\", the paraphrasing system can\nparaphrase the sentence to “how are you doing?”, “how do\nyou feel?” and so on.\nE. Integrated NLG system\nThe integrated system will incorporate the systems pre-\nsented in subsections B and D. Speciﬁcally, we ﬁrst design\nthe structured data of news and then we add them as input\ninto the system of subsection B. Following this, the system\nof subsection B will produce an output text, which will be\nentered into the system of subsection D. After that, the sys-\ntem of subsection D will produce an output text, which will\nthen be grammatically checked to correct any grammatical\nerrors generated by the paraphrasing part, thereby producing\nthe ﬁnal output result.\nTABLE II\nCONTEXTUAL LOGIC SIMILARITY\nGroup\nThe contextual\nlogic similarity\nof Rule-based\ntechniques\nThe contextual\nlogic similarity\nof LSTM\ntechniques\nThe contextual\nlogic similarity\nof the\nHybrid approach\n1\n0.553\n0.005\n0.15\n2\n0.709\n0.111\n0.182\n3\n0.893\n0.012\n0.167\n4\n0.835\n0.029\n0.231\n5\n0.728\n0.138\n0.136\naverage\n0.744\n0.059\n0.173\nThe integrated NLG system combines a rule and deep\nlearning-based techniques, which can generate text with\ncontrollable context logic and a ﬂexible writing style. For\nexample, the following sentence is an example output as\ngenerated by the proposed system:\n“Germany is well placed away from the virus. As if they\nwere not given any possibility of having a vaccine, did\nit require to be a social denial because Scholz said. We\nlive with the virus. Besides, it will change a new medical\ntreatment. In addition, we have to organize our lives to\navoid the second wave.”\nFrom this output text, it can be seen that the context logic\nof the text is roughly consistent with the real news events.\nOn the premise of ensuring that the context logic is control-\nlable, the writing style of news texts becomes very ﬂexible.\nHowever, it is worth noting that there are still some sentences\nwhose context logic deviates from the original news events,\ne.g. the system paraphrased the sentence \"Moreover, the\nabsence of a vaccine meant social distancing was necessary\nbecause Scholz said on Friday.\" to the sentence “As if they\nwere not given any possibility of having a vaccine, did it\nrequire to be a social denial because Scholz said. ”, which\nchanges a bit the original context logic. Therefore, we need\nto further study how to solve or avoid such small deviations\nin the future.\nF. Discussion of Results and Evaluation\nThe performance of the text will be evaluated from two\naspects, controllability of context logic and ﬂexibility of\nwriting style.\nControllability of context logic can be quantiﬁed via\ncalculating the similarity between the generated text and\nreal news. Firstly, we set the text generated based on rules,\nthe text generated based on LSTM and the text generated\nby the proposed hybrid approach as the corpus, and then\nused the real news corresponding to the text as the input,\nin order to calculate the similarity between them (we named\nthis similarity as contextual logic similarity). The contextual\nlogic similarity of each news(testing data) can be calculated\nﬁrst, and then the average contextual logic similarity of each\ngroup can be calculated. Then, the contextual logic similarity\nof these 5 groups can be averaged again to obtain the ﬁnal\naverage contextual logic similarity.\nTABLE III\nMACHINE WRITING STYLE SIMILARITY\nGroup\nMachine writing\nstyle similarity\nof Rule-based\ntechniques\nMachine writing\nstyle similarity\nof LSTM\ntechniques\nMachine writing\nstyle similarity\nof the\nHybrid approach\n1\n0.254\n0.037\n0.108\n2\n0.247\n0.037\n0.103\n3\n0.226\n0.032\n0.09\n4\n0.243\n0.082\n0.087\n5\n0.238\n0.025\n0.104\naverage\n0.242\n0.043\n0.098\nAnalyzing the results presented in Table II, one can see\nthat the text generated based on Rule-based techniques has\na very high contextual logic similarity, with a similar trend\nobserved across all groups. The contextual logical similarity\nof the text generated based on the LSTM technique is very\nlow and unstable, e.g. the values of group 1 and group 3 are\nvery low, which are 0.005 and 0.012 respectively. This means\nthat it is likely that the text generated by these two groups\nis completely inconsistent with the actual news event. The\ncontextual logic similarity between the text generated by the\nhybrid approach and the real news is relatively stable and is\nbetween 0.13 and 0.25.\nTherefore, the contextual logic similarity of the text gen-\nerated based on the rule is the highest, followed by the text\ngenerated by a hybrid approach. Through actual case studies,\nwe can ﬁnd that the context logic of text generated based on\nthe hybrid approach is understandable, though there are a\nfew sentences that still have problems. The contextual logic\nsimilarity of the text generated based on LSTM is extremely\nlow, which does not coincide with the original news event.\nOn the other hand, the ﬂexibility of writing style can be\nquantiﬁed by comparing the similarity between the text gen-\nerated by each approach. The higher the similarity between\nthe texts generated based on the same approach, the less\nﬂexible the writing style of the text will be, that is, the\ngenerated text will be closer to the machine writing style.\nThis similarity can be called machine writing style similarity.\nWe calculated the similarity for 5 groups of the testing\ndata separately. In each group, people can arbitrarily select\na generated text as a benchmark. Hereafter, the similarity\nbetween the other generated texts and the benchmark can be\ncalculated, and then the average machine writing style sim-\nilarity within each group can be calculated. Finally, people\ncan calculate the average machine writing style similarity\nbetween these 5 groups.\nTable III shows the machine writing style similarity of the\ntext generated by each group based on different approaches.\nThe text generated via the rule-based technique demonstrated\nthe highest machine writing style similarity, which indicates\nthat the writing style of the text generated via the rule-based\ntechnique is inﬂexible. The proposed hybrid approach can\nincrease the ﬂexibility of the generated text, as evidenced\nthrough the fact that the the machine writing style similarity\ndropped from 0.242 (rule-based techniques) to 0.098 (hybrid\napproach). Although the LSTM approach has the lowest\nmachine writing style similarity, its context logic is uncon-\ntrollable and hard to apply in actual environments currently.\nTherefore, the hybrid approach based on rule and deep\nlearning-based techniques we proposed in this paper can\nmake the Natural Language Generation system achieve a\nbalance between the controllability of context logic and\nﬂexibility of writing style. The hybrid approach can make\nthe generated text have controllable context logic, along with\na ﬂexible writing style.\nV. CONCLUSION & FUTURE WORK\nAlthough using a hybrid approach can make most of the\ncontext logic of the resulting text controllable, there are still\nsome sentences where the context logic cannot be controlled.\nWe will do more studies for this research challenge in the\nfuture.\nBesides, we put forward an evaluation approach which\ncalled HMCU in this paper, but the quantitative indicators\nand calculation methods are still not mature enough. We will\ndesign more mature quantitative indicators and calculation\nmethods in the future.\nVI. ACKNOWLEDGE\nWe are very grateful for the very useful suggestions and\nguidance that Professor Ehud Reiter offered throughout the\ndevelopment and writing phases of this paper.\nREFERENCES\n[1] E. Reiter and R. Dale, “Building applied natural language generation\nsystems,” Natural Language Engineering, vol. 3, no. 1, pp. 57–87,\n1997.\n[2] S. Varges and C. Mellish, “Instance-based natural language genera-\ntion,” Natural Language Engineering, vol. 16, no. 3, pp. 309–346,\n2010.\n[3] A. Prakash, S. A. Hasan, K. Lee, V. Datla, A. Qadir, J. Liu, and\nO. Farri, “Neural paraphrase generation with stacked residual lstm\nnetworks,” arXiv preprint arXiv:1610.03098, 2016.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\nAdvances in neural information processing systems, pp. 5998–6008,\n2017.\n[5] B. N. Patro, V. K. Kurmi, S. Kumar, and V. P. Namboodiri, “Learning\nsemantic sentence embeddings using sequential pair-wise discrimina-\ntor,” arXiv preprint arXiv:1806.00807, 2018.\n[6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language\nmodels are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020.\n[7] L. Leppänen, M. Munezero, M. Granroth-Wilding, and H. Toivonen,\n“Data-driven news generation for automated journalism,” in Pro-\nceedings of the 10th International Conference on Natural Language\nGeneration, pp. 188–197, 2017.\n[8] B. Starkie, “Inferring attribute grammars with structured data for natu-\nral language processing,” in International Colloquium on Grammatical\nInference, pp. 237–248, Springer, 2002.\n[9] B. N. Patro, V. K. Kurmi, S. Kumar, and V. P. Namboodiri, “Learning\nsemantic sentence embeddings using sequential pair-wise discrimina-\ntor,” arXiv preprint arXiv:1806.00807, 2018.\n[10] A. Prakash, S. A. Hasan, K. Lee, V. Datla, A. Qadir, J. Liu, and\nO. Farri, “Neural paraphrase generation with stacked residual lstm\nnetworks,” arXiv preprint arXiv:1610.03098, 2016.\n[11] J. Wieting and K. Gimpel, “Paranmt-50m: Pushing the limits of para-\nphrastic sentence embeddings with millions of machine translations,”\narXiv preprint arXiv:1711.05732, 2017.\n",
  "categories": [
    "cs.CL",
    "cs.AI",
    "I.2.7; I.2.6"
  ],
  "published": "2020-06-15",
  "updated": "2020-06-17"
}