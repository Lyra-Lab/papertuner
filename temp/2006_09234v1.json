{
  "id": "http://arxiv.org/abs/2006.09234v1",
  "title": "Model Embedding Model-Based Reinforcement Learning",
  "authors": [
    "Xiaoyu Tan",
    "Chao Qu",
    "Junwu Xiong",
    "James Zhang"
  ],
  "abstract": "Model-based reinforcement learning (MBRL) has shown its advantages in\nsample-efficiency over model-free reinforcement learning (MFRL). Despite the\nimpressive results it achieves, it still faces a trade-off between the ease of\ndata generation and model bias. In this paper, we propose a simple and elegant\nmodel-embedding model-based reinforcement learning (MEMB) algorithm in the\nframework of the probabilistic reinforcement learning. To balance the\nsample-efficiency and model bias, we exploit both real and imaginary data in\nthe training. In particular, we embed the model in the policy update and learn\n$Q$ and $V$ functions from the real data set. We provide the theoretical\nanalysis of MEMB with the Lipschitz continuity assumption on the model and\npolicy. At last, we evaluate MEMB on several benchmarks and demonstrate our\nalgorithm can achieve state-of-the-art performance.",
  "text": "Model Embedding Model-Based Reinforcement Learning\nXiaoyu Tan ∗1, Chao Qu∗1, Junwu Xiong1, and James Zhang1\n1Ant Financial Services Group\nAbstract\nModel-based reinforcement learning (MBRL) has shown its advantages in sample-\neﬃciency over model-free reinforcement learning (MFRL). Despite the impressive re-\nsults it achieves, it still faces a trade-oﬀbetween the ease of data generation and model\nbias. In this paper, we propose a simple and elegant model-embedding model-based\nreinforcement learning (MEMB) algorithm in the framework of the probabilistic rein-\nforcement learning. To balance the sample-eﬃciency and model bias, we exploit both\nreal and imaginary data in the training. In particular, we embed the model in the\npolicy update and learn Q and V functions from the real data set. We provide the\ntheoretical analysis of MEMB with the Lipschitz continuity assumption on the model\nand policy. At last, we evaluate MEMB on several benchmarks and demonstrate our\nalgorithm can achieve state-of-the-art performance.\n1\nIntroduction\nReinforcement learning can be generally classiﬁed into two categories: model-free rein-\nforcement learning (MFRL) and model-based reinforcement learning (MBRL). There is\na surge of interest in MBRL recently due to its higher sample-eﬃciency comparing with\nMFRL (Kurutach et al., 2018; Heess et al., 2015; Asadi et al., 2018). Despite its suc-\ncess, MBRL still faces a challenging problem, i.e., the model-bias, where the imperfect\ndynamics model would degrade the performance of the algorithm (Kurutach et al., 2018).\nUnfortunately, such things always happen when the environment is suﬃciently complex.\nThere are a few eﬀorts to mitigate such issues by combining model-based and model-free\napproaches. Heess et al. (2015) compute the value gradient along real system trajectories\ninstead of planned ones to avoid the compounded error. Kalweit and Boedecker (2017)\nmix the real data and imaginary data from the model and then train Q function. An\nensemble of neural networks can be applied to model the environment dynamics, which\neﬀectively reduces the error of the model (Kurutach et al., 2018; Clavera et al., 2018; Chua\net al., 2018).\nIndeed, how to exploit the real and imaginary data is a key question in model-based\nreinforcement learning. Recent model-based algorithms applying Dyna-style updates have\ndemonstrated promising results (Sutton, 1990; Kurutach et al., 2018; Luo et al., 2018).\nThey collect real data using the current policy to train the dynamics model. Then the\npolicy is improved using state-of-the-art model-free reinforcement learning algorithms with\nimagined data generated by the learned model. Our argument is that why not directly\nembed the model into the policy improvement? To this end, we derive a reinforcement\n∗Equal contribution with randomized order.\n1\narXiv:2006.09234v1  [cs.LG]  16 Jun 2020\nlearning algorithm called model-embedding model-based reinforcement learning (MEMB)\nin the framework of the probabilistic reinforcement learning (Levine, 2018).\nWe provide the theoretical result on the error of the long term return in MEMB, which\nis caused by the model bias and policy distribution shift given the Lipschitz continuity\ncondition of the model and policy. In addition, our analysis takes consideration of the\nlength of the rollout step, which helps us to design the algorithm. In MEMB, the dynamics\nmodel and reward model are trained with the real data set collected from the environment.\nThen we simply train Q and V function using the real data set with the update rule derived\nfrom the maximum entropy principle (several other ways to include the imaginary data\ncan also be applied, see discussions in Section 3). In the policy improvement step, the\nstochastic actor samples an action with the real state as the input, and then the state\nswitches from s to s′ according to the learned dynamics model.\nWe link the learned dynamics model, reward model, and policy to compute an analytic\npolicy gradient by the back-propagation. Comparing with the likelihood-ratio estimator\nusually used in the MFRL method, such value gradient method would reduce the variance\nof the policy gradient (Heess et al., 2015). The other merit of MEMB is its computational\neﬃciency. Several state-of-the-art MBRL algorithms generate hundreds of thousands imag-\ninary data from the model and a few real samples (Luo et al., 2018; Janner et al., 2019).\nThen the huge imaginary data set feeds into MFRL algorithms, which may be sample-\neﬃcient in terms of real samples but not computational-friendly. On the contrary, our\nalgorithm embeds the model in the policy update. Thus we can implement it eﬃciently\nby computing policy gradient several times in each iteration (see our algorithm 1) and do\nnot need to do the calculation on the huge imaginary data set.\nNotice SVG (Heess et al., 2015) also embeds the model to compute the policy gradient.\nHowever, there are several key diﬀerences between MEMB and SVG.\n• To alleviate the issue of the compounded error, SVG proposes a conservative algo-\nrithm where just real data is used to evaluate policy gradients and the imaginary\ndata is wasted. However, our theorem shows that the imaginary data from the short\nrollout from the learned model can be trusted. In our work, the policy is trained\nwith the model and imaginary dataset m times in each iteration of the algorithm.\nIn the ablation study (appendix B), we demonstrate such diﬀerence leads to a large\ngap in the performance.\n• We provide a theoretical guarantee of the algorithm, which is not included in SVG.\n• We derive our algorithm in the framework of the probabilistic reinforcement learning.\nThe entropy term would encourage the exploration, prevent the early convergence to\nthe sub-optimal policies, and show state-of-the-art performance in MFRL (Haarnoja\net al., 2018).\nIn addition, MEMB avoids the importance sampling in the oﬀ-policy setting by sampling\nthe action from π and transition from the learned model, which further reduces the variance\nof the gradient estimation.\nContributions: We derive an elegant, sample-eﬃcient, and computational-friendly 1\nDyna-style MBRL algorithm in the framework of the probabilistic reinforcement learning\nin a principled way. Diﬀerent from the traditional MBRL algorithm, we directly embed\nthe model into the policy improvement, which could reduce the variance in the gradient\nestimation and avoid the computation on the huge imaginary data set. In addition, since\n1We can ﬁnish one trial of the experiment around one or two hours on a laptop.\n2\nthe algorithm is oﬀ-policy, it is sample-eﬃcient. At last, we provide theoretical results of\nour algorithm on the long term return considering the model bias and policy distribution\nshift. We test our algorithm on several benchmark tasks in Mujoco simulation environment\n(Todorov et al., 2012) and demonstrate that our algorithm can achieve state-of-the-art\nperformance. We provide our code anonymously for the reproducibility 2.\nRelated work: There are a plethora of works on MBRL. They can be classiﬁed\ninto several categories depending on the way to utilize the model, to search the optimal\npolicy or the function approximator of the dynamics model. We leave the comprehensive\ndiscussion on the related work in appendix A.\n2\nPreliminaries\nIn this section, we ﬁrst present some backgrounds on the Markov decision process. Then\nwe introduce the knowledge on the probabilistic reinforcement learning with entropy reg-\nularization (Ziebart et al., 2008; Levine, 2018) and stochastic value gradient (Heess et al.,\n2015) since parts of them are the building blocks of our algorithm.\n2.1\nMDP\nMarkov Decision Process (MDP) can be described by a 5-tuple (S, A, r, p, γ): S is the\nstate space, A is the action space, p is the transition probability, r is the expected reward,\nand γ ∈[0, 1) is the discount factor. That is for s ∈S and a ∈A, r(s, a) is the expected\nreward, p(s′|s, a) is the probability to reach the state s′. A policy is used to select actions\nin the MDP. In general, the policy is stochastic and denoted by π, where π(at|st) is the\nconditional probability density at at associated with the policy. The state value evaluated\non policy π could be represented by V π(s) = Eπ[P∞\nt=0 γtr(st, at)|s0 = s] on immediate\nreward return r with discount factor γ ∈(0, 1) along the horizon t. When the entropy of\nthe policy is incorporated in the probabilistic reinforcement learning (Ziebart et al., 2008),\nwe could redeﬁne the reward function r(s, a) ←r(s, a) −log π(a|s). When the model of\nthe environment is learned from the data, we use ˆp(s′|s, a) to denote the learned dynamic\nmodel, and ˆr(s, a) as the learned reward model.\nWe denote the true long term return as η(π) := Eπ\nP∞\nt=0 γtr(st, at), where the expec-\ntation corresponds to the policy, true transition and true reward. In the model based rein-\nforcement learning, we denote the model long term return as ˆη(π) := ˆEπ\nP∞\nt=0 γtˆr(st, at),\nwhere ˆE means the expectation over the policy, learned model ˆp and ˆr.\n2.2\nProbabilistic Reinforcement Learning\nLevine (2018) formulate reinforcement learning as a probabilistic inference problem. The\ntrajectory τ up to time step T is deﬁned as\nτ = ((s0, a0), (s1, a1), ..., (sT , aT )).\nThe probability of the trajectory with the optimal policy is deﬁned as\nρ = [p(s0)\nT\nY\nt=0\np(st+1|st, at)] exp\n\u0000T\nX\nt=0\nr(st, at)\n\u0001\n.\n2Code is submitted at https://github.com/MEMB-anonymous1/MEMB\n3\nThe probability of the trajectory induced by the policy π(a|s) is\n˜ρ = p(s0)\nT\nY\nt=0\np(st+1|st, at)π(at|st).\nThe objective is to minimize the KL divergence KL(˜ρ, ρ), which leads to the entropy\nregularized reinforcement learning maxπ\nPT\nt=0 E(st,at)∼ρπ[r(st, at) + αH(π(·|st))], where\nH(π(·|st)) is an entropy term scaled by α (Ziebart et al., 2008).\nThe optimal policy\ncan be obtained by the following soft-Q update (Fox et al., 2016).\nQ(st, at) ←−r(st, at) + γEst+1∼p[V (st+1)], V (st) ←α log(\nZ\nA\nexp( 1\nαQ(st, at))dat).\nAbove iterations deﬁne the soft Q operator, which is a contraction. The optimal policy\nπ∗(a|s) can be recovered by π⋆(at|st) =\nexp( 1\nαQ∗(st,at))\nR\nA exp( 1\nαQ∗(st,at))dat , where Q∗is the ﬁxed point\nof the soft-Q update. We refer readers to the work (Ziebart et al., 2008; Haarnoja et al.,\n2017) for more discussions. In soft actor-critic (Haarnoja et al., 2018), the optimal policy\nπ∗(at|st) is approximated by a neural network πθ(at|st), which is obtained by solving the\nfollowing optimization problem\nmax\nπθ(at|st) Est∼p(st)Eat∼πθ(at|st)[Q(st, at) −α log πθ(at|st))].\n2.3\nStochastic Value Gradient\nStochastic value gradient method is a model-based algorithm which is designed to avoid the\ncompounded model errors by only using the real-world observation and gradient informa-\ntion from the model (Heess et al., 2015). The algorithm directly substitutes the dynamics\nmodel and reward model in the Bellman equation and calculates the gradient. To perform\nthe backpropagation in the stochastic Bellman equation, the re-parameterization trick is\napplied to evaluate the gradient on real-world data.\nIn SVG(1), the stochastic policy\nπ(a|s; θ) with parameter θ could be optimized by the policy gradient in the following way\n∂V (s)\n∂θ\n≈Eη,ζ[∂ˆr(s, a)\n∂a\n∂π(a|s)\n∂θ\n+ γ(∂V ′(s′)\n∂s′\n∂f(s, a)\n∂a\n∂π(a|s)\n∂θ\n)],\n(1)\nwhere η and ζ are the policy and model re-parameterization noise which could be directly\nsampled from a prior distribution or inferred from a generative model g(η, ζ|s, a, s′). The\nf(s, a) and ˆr(s, a) are dynamics model and reward model respectively. In the oﬀ-policy up-\ndate, SVG includes the important weight w = π(ak|sk,θt)\nπ(ak|sk,θk), where θt represent the parameter\nof the current policy and k is the index of the data from the replay buﬀer.\nNotions: Given two metric space (X, dX) and (Y, dY ), we say a function f is L Lips-\nchitz if dY (f(x1), f(x2)) ≤LdX(x1, x2), ∀x1, x2 ∈X. Give a meritc space (M, d) and the\nset P(M) of probability measures on M, the Wasserstein metric between two probability\ndistributions µ1 and µ2 in P(M) is deﬁned as W(µ1, µ2) := infj∈Σ\nR R\np(x, y)d(x, y)dxdy,\nwhere Σ denotes the collection of all joint distributions p with marginal µ1 and µ2.\n3\nMEMB\nIn this section, we introduce our model-embedding model-based reinforcement learning\nalgorithm (MEMB). Particularly, we optimize the following model long term return with\n4\nthe entropy regularization.\nmax\nπ\nT\nX\nt=0\nˆE[ˆr(st, at) + H(π(at|st))],\n(2)\nwhere we omit the regularizer parameter α of the entropy term in the following dis-\ncussion to ease the exposition.\nRemind that optimizing the entropy regularzied rein-\nforcement learning is equivalent to minimize the KL divergence between the distribu-\ntion of ρ and ˜ρ in Section 2.2.\nNow we replace the true model by the learned model\nˆp(stt+1|st, at) and ˆr(s, a).\nTherefore we have ˜ρπ = p(s0) QT\nt=0 ˆp(st+1|st, at)π(at|st) and\nρπ = [p(s0) QT\nt=0 ˆp(st+1|st, at)] PT\nt=0 exp(ˆr(st, at)). We then optimize the KL divergence\nKL(˜ρ, ρ), w.r.t to π(at|st). Using the backward view as that in (Levine, 2018), we have\nthe optimal policy. We defer the derivation to the appdendix E.\nπ∗(at|st) =\nexp(Q(st, at))\nR\na exp(Q(st, at)dat\n,\nwhere Q(st, at) = ˆr(st, at) + γEst+1∼ˆp[V (st+1)], V (st) = Eπ(at|st)[Q(st, at) −log π(at|st)].\n(3)\nIn the policy improvement step, the optimal policy π∗can be approximated by a\nparametric function πθ(at|st). In the MFRL, this can be obtained by solving\nmax\nπθ(at|st) Est∼p(st)Eat∼πθ(at|st)[Q(st, at) −log πθ(at|st))]\n(Levine, 2018; Haarnoja et al., 2018). A straightforward way is to optimize Q, V and\nπ using the imaginary data from the rollout, which reduces to Luo et al. (2018); Janner\net al. (2019) and many others. However such way used in the MFRL cannot leverage the\nmodel information. We leave the our derivation and discussion on the policy improvement\nin Section 3.3.\n3.1\nModel Learning\nThe transition dynamics and rewards could be modeled by non-linear function approxima-\ntions as two independent regression tasks which have the same input but diﬀerent output.\nParticularly, we train two independent deep neural networks with parameter ω and ϕ to\nrepresent the dynamics model ˆp and reward model ˆr respectively. In our analysis, we\nassume ˆp is not far from p, i.e., W(p(s′|s, a), ˆp(s′|s, a)) ≤ϵm, which can be estimated in\npractice by cross validation.\nTo better represent the stochastic nature of the dynamic transitions and rewards, we\nimplement re-parameterization trick on both ˆp and ˆr with input noises ζω and ζϕ sampled\nfrom Gaussian distribution N(0, 1). Thus we can denote dynamic model s′ = f(s, a, ζw)\nand reward as ˆr(s, a, ζϕ). In practice, we use neural networks to generate mean: µω, µϕ,\nand variance: σω, σϕ separately for the transition model and reward model, respectively.\nThen, we compute the result by µω + σωζω and µϕ + σϕζϕ, respectively.\nWe optimize above two models by sampling the data (s, a, s′, r) from the (real data)\nreplay buﬀer D and minimizing the mean square error:\nJ(ω) = 1\n2ED,ζω[(f(s, a, ζω) −s′)2], J(ϕ) = 1\n2ED,ζϕ[(ˆr(s, a, ζϕ) −r)2].\n(4)\n5\n3.2\nValue function learning\nAlthough in equation (3), the Q function is computed w.r.t. the learned model, this way\nwould cause the model bias in practice. In addition, in the policy update, this bias would\nresult in an additional error of the policy gradient (since roughly speaking, the gradient\nof the policy is weighted by the Q function). To avoid this model error, we update V ,\nQ using equation (3) with the real transition (s, a, r, s′) from the real data replay buﬀer.\nParticularly, we minimize the following error w.r.t to V and Q.\nJ(ψ) = Est∼D[1\n2(Vψ(st) −Eat∼π[Qφ(st, at) −log π(at|st)])2].\n(5)\nJ(φ) = E(st,at)∼D[1\n2\n\u0000Qφ(st, at) −rt −γVψ(st+1)\n\u00012].\n(6)\nA straightforward way to incorporate the imaginary data is the value expansion (Feinberg\net al., 2018). However in our ablation study, we ﬁnd that the training with real data\ngives the best result. Thus we just brieﬂy introduce the value expansion here. If Q func-\ntion and V function are parameterized by φ and ψ respectively, they could be updated\nby minimizing the new objective function with the value expansion on imaginary rollout:\nJ(φ) = 1\nH\nPH−1\nt=0\n\u0000Qφ(ˆst, ˆat) −(PH−1\nk=t γk−tˆrk + γH−tVψ(ˆsH))\n\u00012, where only the initial tu-\nple τ0 = (ˆs0, ˆa0, ˆr0, ˆs1) is sampled from replay buﬀer D with real-world data, and later\ntransitions are sampled from the imaginary rollout from the model. H here is the time\nstep of value expansion using imaginary data. τ is the training tuple and τ0 is the initial\ntraining tuple. Note that when H = 1, it reduces to the case where just real data is used.\n3.3\nPolicy Learning\nThen we consider the policy improvement step, i.e., to calculate the optimal policy at each\ntime step. One straightforward way is to optimize the following problem\nmax\nπ(at|st) Est∼p(st)Eat∼π(at|st)[Q(st, at) −log π(at|st))]\nas that in MFRL but with the imaginary data set. This way reduces to the work (Luo\net al., 2018; Chua et al., 2018; Janner et al., 2019). However, such way cannot leverage\nthe learned dynamics model and reward model. To incorporate the model information,\nnotice that V (st) = Ea∼π(at|st)[Q(st, at) −log π(at|st)], thus the policy improvement step\nis equal to\nmax\nπ(at|st) Est∼p(st)V (st).\n(7)\nIn the following, we connect the dynamics model, reward model, and value function to-\ngether by the soft Bellman equation. Recall we have re-parameterized the dynamics model\ns′ = f(s, a, ζω) in Section 3.1. Now we re-parameterize the policy as a = κ(s, η; θ) with\nnoise variables η ∼ρ(η). Now we can write the soft Bellman equation in the following\nway.\nV (s) = Eη[Eζϕˆr(s, κ(s, η; θ), ζϕ) −log π(a|s) + γEζwV ′(f(s, κ(s, η; θ), ζω))]\n(8)\nTo optimize (7) and leverage gradient information of the model, we sample s from the real\ndata replay buﬀer D and take the gradient of V (s) w.r.t. θ\nEs∼D\n∂V (s)\n∂θ\n= Es∼D,η,ζw,ζϕ[∂ˆr\n∂a\n∂κ\n∂θ −1\nπ\n∂π\n∂θ + γ(∂V ′(s′)\n∂s′\n∂f\n∂a\n∂κ\n∂θ )].\n(9)\n6\nFor the Gaussian noise case, we can further simplify ∂π\n∂θ by plugging in the density function\nof the normal distribution. Clearly, we can unroll the Bellman equation with k steps and\nobtain similar result but here we focus on k = 1.\nThe equation (9) demonstrates an\ninteresting connection between our algorithm and SVG. Notice that the transition from\n(s, a) to s′ is sampled from the learned dynamics model f, while the SVG(1) just utilizes\nthe real data. Thus in the algorithm we can update policy several times in each iteration\nto fully utilized the model rather than just use the real transition once. Compared with\nthe policy gradient step taken by SVG(1) algorithm (Heess et al., 2015), equation (9)\nincludes one extra term −(1/π)(∂π/∂θ) to maximize the entropy of policy. We also drop\nthe importance sampling weights by sampling from the current policy.\n3.4\nMEMB algorithm\nWe summarize our MEMB in Algorithm 1.\nAt the beginning of each step, we train\ndynamics model f and reward model ˆr by minimizing the L2 loss shown in (4). Then\nthe agent interacts with the environment and stores the data in the real data replay\nbuﬀer D. Actor samples sk from D and collects sk+1 according to the dynamics model\nf(sk, ak, ζω). Such imaginary transition is stored in Dimg. Then we train Q, V , and π\naccording to the update rule in Section 3. Similar to other value-based RL algorithms,\nour algorithm also utilizes two Q functions to further reduce the overestimation error by\ntraining them simultaneously with the same data but only selecting the minimum target\nin value updates (Fujimoto et al., 2018). We use the target function for V like that in\ndeep Q-learning algorithm (Mnih et al., 2015), and update it with an exponential moving\naverage. We train policy using the gradient in (9). Remark that our s′ is sampled from\nthe dynamic model f(s, a, ζω), while in SVG, it uses the true transition.\n4\nTheoretical Analysis\nIn this section, we provide a theoretical analysis for our algorithm. Notice our algorithm\nbasically samples a state from the real data replay buﬀer and then unrolls the trajectory\nwith several steps. Then using this imaginary data from rollout and the value function\ntrained from real data, we update the policy with our policy learning formulation in 3.3.\nIn the following, we ﬁrst give a general result on how accurate the model long term return\nis regardless of how many rollout step is used in our algorithm. Later, we provide a more\nsubtle analysis considering the rollout procedure.\nWe ﬁrst investigate the diﬀerence between the true long term return η and the model\nlong term return ˆη, which is induced by the model bias and the distribution shift due\nto the updated policy encountering states not seen during model training. Particularly,\nwe denote that the model bias as W(p(s′|s, a), ˆp(s′|s, a)), i.e., the Wasserstein distance\nbetween the true model and the learned model.\nWe denote the distribution shift as\nW(π(a|s), πD(a|s)), where πD is the data-collecting policy and π is intermediate policy\nduring the update of the algorithm. For instance, in our algorithm 1, πD corresponds\nto the replay buﬀer of the true data while π is the policy in the imaginary rollout. We\nassume W(p(s′|s, a), ˆp(s′|s, a)) ≤ϵm, ∀s, a and W(π(a|s), πD(a|s)) ≤ϵπ, ∀s. Comparing\nwith the total variation used in (Janner et al., 2019), the Wasserstein distance has better\nrepresentation in the sense of how close ˆp approximate p (Asadi et al., 2018). For instance,\nif p and ˆp has disjoint supports, the total variation is always 1 regardless of how far the\nsupports are from each other. Such case could always happen in the high-dimensional\nsetting (Gulrajani et al., 2017).\n7\nAlgorithm 1 MEMB\nInputs: Replay buﬀer D, imaginary replay buﬀer Dimg, policy πθ, value function Vψ,\ntarget value function V ¯ψ. Two Q functions with parameters φ0 and φ1, dynamic model\nf with parameter ω, and reward model ˆr with parameter ϕ\nfor each iteration do\n1. Train the dynamics model and reward model\nCalculate the gradients ∇ωJ(ω), ∇ϕJ(ϕ) using (4) with D, update ω and ϕ\n2. Interact with environment\nSample at ∼π(at|st), get reward rt, and observe the next state st+1\nAppend the tuple (st, at, rt, st+1) into D\n3. Update the actor, critics m times\nEmpty Dimg\nSample (s0, a0, r0, s1) ∼D\nfor each imaginary rollout step k do\nSample ak ∼π(ak|sk), get reward rk = ˆr(sk, ak, ζϕ), and sample sk+1 ∼f(sk, ak, ζω)\nAppend the tuple (sk, ak, rk, sk+1) into Dimg\nend for\nCalculate the gradient ∇φJ(φ) using (6) with ¯ψ and Dimg.\nCalculate the gradient ∇ψJ(ψ) using (5) with D\nCalculate the gradient ∇θV (s) using (9) with Dimg.\nUpdate φ, ψ, and θ, update ¯ψ with Polyak averaging\nend for\nTo bound the error of the long term return, we follow the Lipschitz assumption on\nthe model and policy in (Asadi et al., 2018). Particularly, a transition model ˆp belongs\nto Lipschitz class model if it is represented by ˆp(s′|s, a) = P\nfm 1(fm(s, a) = s′)gm(fm),\nwhich says the transition probabilities can be decomposed in to a distribution g over a set\nof deterministic function fm. The model is called Km Lipschitz, if fm is a Km Lipschitz\nfunction. It is easy to understand this in the context of our work when we re-parametrize\nthe model function. For instance, if the transition is deterministic, i.e., s′ = fm(s, a),\nthen Km is the Lipschitz constant of fm. Similarly the policy π associated with Lipschitz\nclass is given by π(a|s) = P\nfπ 1(fπ(s) = a)gπ(fπ) and fπ(s) is Kπ Lipschitz. If we use\nneural network to approximate model and policy, Lipschitz continuity means the gradient\nw.r.t the input is bounded by a constant. Here for simplicity, we assume gm and gπ are\nindependent with state and action. The similar bound including such dependence can be\nproved but with more involved assumption and notations.\nNotice the analysis in (Asadi et al., 2018) just considers the error caused by the model\nbias and neglect the the eﬀect of distribution shift of the policy. Therefore they assume\nπD = π and thus do not need the Lipschitz assumption on π. In addition, we give a bound\nconsidering the k step rollout in Theorem 2, which is not covered by (Asadi et al., 2018).\nIn the following, we ﬁrst give a general result to bound true long term return η and model\nlong term return ˆη, where we assume the reward model is known and mainly focus on the\neﬀect of the model bias and distribution shift.\nTheorem 1. Let the true transition model p and the learned transition model ˆp both be\nKm Lipschitz. We also assume policy π(a|s) and reward function r(s, a) are Kπ and Kr\nLipschitz respectively. Suppose W(ˆp(s′|s, a), p(s′|s, a)) ≤ϵm, ∀(s, a), W(π(a|s), πD(a|s)) ≤\n8\nϵπ, ∀s. Let ¯K := KπKm and assume ¯K ∈[0, 1), then the diﬀerence between true return\nand model return is bounded as |η(π) −ˆη(π)| ≤2\n\u0002\nγKr ¯\nK\n(1−γ)(1−γ ¯\nK) + Kr\n1−γ\n\u0003\nϵπ +\nγKr ¯\nK\n(1−γ)(1−γ ¯\nK)ϵm.\nNotice above theorem is a generic result on the model based reinforcement learning.\nSuch analysis is based on running full rollout of the learned model, which results in the\ncompounded error. However, notice in our algorithm, we actually start a rollout from a\nstate with the distribution induced by the previous policy πD and then run imaginary\nrollout for k steps using the current policy π on learned transition model ˆp. Follow the\nnotion in (Janner et al., 2019), we call it k-step branched rollout.\nWe use ηbranch to\ndescribe the long term return of this branched rollout and have a ﬁne-grained analysis in\nthe following.\nTheorem 2. Let the true transition model p and learned transition model ˆp both be Km\nLipschitz. We also assume policy π(a|s) and reward function r(s, a) are Kπ and Kr Lip-\nschitz respectively. Suppose W(ˆp(s′|s, a), p(s′|s, a)) ≤ϵm, ∀(s, a), W(π(a|s), πD(a|s)) ≤\nϵπ, ∀s.\nIf ¯K := KπKm ∈[0, 1), then the diﬀerence between true return and branched\nreturn is bounded as |η(π) −ηbranch(π)| ≤Kr[\nγk+1 ¯\nK\n(1−γ)(1−¯\nKγ) +\nγk\n1−γ ]ϵπ + Kr[ Kπ(1−γk)\n(1−¯\nK)(1−γ) −\nKπ(1−(γ ¯\nK)k)\n(1−γ ¯\nK)(1−¯\nK) + γk Kπ(1−¯\nKk)\n(1−¯\nK)(1−γ)]ϵm.\nClearly, on the right hand side of the bound, some terms increase with the rollout step k\nwhile the others decreases. Thus there exist a best k∗which depends on the discount factor\nγ, ϵm and ϵπ. In general, the imaginary data from short rollout is still trustful. Recall\nwe can apply the rollout of Bellman equation in two diﬀerent ways in our algorithm: (1)\nPolicy update (equation (8)). (2) value function learning in section 3.2. So we do ablation\nstudy on these two ways and ﬁnd that the imaginary data in value function learning would\ndegrade the performance while improves the learning a lot in policy update, which also\nexplains why MEMB is much better than SVG. Another interesting result is on ϵm. In\nour algorithm 1, in each iteration, we can update policy m times using the imaginary data\nfrom the model. If m is too large, it will cause a large distribution shift ϵπ and degrade\nthe performance. As such, typically we choose m as 3 to 5.\n5\nExperimental results\nIn this section, we would like to answer two questions: (1) How does MEMB perform\non some benchmark reinforcement learning tasks comparing with other state-of-the-art\nmodel-based and model-free reinforcement learning algorithms? (2) Whether we should\nuse the imaginary data generated by the model embedding in the policy learning in Section\n3.3. How many imaginary data we should use in the value function update in Section 3.2?\nWe leave the answer of second question in the ablation study in appendix B.\nEnvironment: To answer these two questions, we experiment in the Mujoco simula-\ntion environment (Todorov et al., 2012): InvertedPendulum-v2, HalfCheetah-v2, Reacher-\nv2, Hopper-v2, Swimmer-v2, and Walker2d-v2. Each experiment is tested on ﬁve trials\nusing ﬁve diﬀerent random seeds and initialized parameters. The details of the tasks and\nexperiment implementations can be found in appendix C.\nComparison to state-of-the-art: We compare our algorithm with state-of-the-art\nmodel-free and model-based reinforcement learning algorithms in terms of sample com-\nplexity and performance. DDPG (Lillicrap et al., 2015) and SAC (Haarnoja et al., 2018)\nare two model-free reinforcement learning algorithms on continuous action tasks. SAC has\nshown its reliable performance and robustness on several benchmark tasks. Our algorithm\n9\n0\n5\n10\n15\n20\n25\n30\n35\nEpochs\n0\n200\n400\n600\n800\n1000\n1200\nAverage return\nMEMB\nDDPG\nSAC\nSLBO\nSVG\nPOPLIN\nMBPO\n(a) Inverted Pendulum\n0\n25\n50\n75\n100\n125\n150\n175\n200\nsteps (1e3)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nAverage return\nMEMB\nDDPG\nSAC\nSLBO\nSVG\nPOPLIN\nMBPO\n(b) HalfCheetah\n0\n10\n20\n30\n40\n50\nEpochs\n60\n50\n40\n30\n20\n10\n0\nAverage return\nMEMB\nDDPG\nSAC\nSLBO\nSVG\nPOPLIN\nMBPO\n(c) Reacher\n0\n25\n50\n75\n100\n125\n150\n175\n200\nsteps (1e3)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAverage return\nMEMB\nDDPG\nSAC\nSLBO\nSVG\nPOPLIN\nMBPO\n(d) Hopper\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\nsteps (1e3)\n0\n50\n100\n150\n200\n250\n300\nAverage return\nMEMB\nDDPG\nSAC\nSLBO\nSVG\nPOPLIN\nMBPO\n(e) Swimmer\n0\n50\n100\n150\n200\n250\n300\nsteps (1e3)\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\nAverage return\nMEMB\nDDPG\nSAC\nSLBO\nSVG\nPOPLIN\nMBPO\n(f) Walker2D\nFigure 1: Performance of MEMB and other baselines in benchmark tasks. The x-axis is the\ntraining step (epoch or step). Each experiment is tested on ﬁve trials using ﬁve diﬀerent\nrandom seeds and initialized parameters. For a simple task, i.e., InvertedPendulum, we\nlimit the training steps at 40 epochs. For the other three complex tasks, the total training\nsteps are 200K or 300K. The solid line is the mean of the average return. The shaded\nregion represents the standard deviation. On HalfCheetah, Hopper, and Swimmer, MEMB\noutperforms the other baselines signiﬁcantly. In the task Walker2d, SLBO is slightly better\nthan MEMB. They both surpass other algorithms. On Reacher, MEMB and SAC perform\nbest.\nalso builds on the maximum entropy reinforcement learning framework and beneﬁts from\nincorporating the model in the policy update. Four model-based reinforcement learning\nbaselines are SVG (Heess et al., 2015),SLBO (Luo et al., 2018), MBPO (Janner et al.,\n2019) and POPLIN (Wang and Ba, 2019). Notice in SVG, the algorithm just computes\nthe gradient in the real trajectory, while our MEMB updates policy using the imaginary\ndata m times generated from the model. At the same time, we avoid the importance\nsampling by using the data from the learned model. SLBO is a model-base algorithm\nwith performance guarantees that applies TRPO (Schulman et al., 2015) on the data set\ngenerated from the rollout of the model. MBPO has the similar spirit but with SAC as\nthe learning algorithm on the imaginary data.\nFor fairness, we compare the baseline without the ensemble learning techniques (Chua\net al., 2018).\nThese techniques are known to reduce the model bias.\nWe do not use\ndistributed RL either to accelerate the training. We believe that the above-mentioned skills\nare orthogonal to our work and could be integrated into the future work to further improve\nthe performance.\nWe just compare this pure version of MEMB with other baselines.\nWe also notice that some recent works in MBRL modify the benchmarks to shorten the\ntask horizons and simplify the model problem while some work assume the true terminal\ncondition is known to the algorithm (Wang et al., 2019). On the contrary, we test our\nalgorithm in the full-length tasks and do not have assumptions on the terminal condition.\nWe present experimental results in Figure 1.\nIn a simple task, InvertedPendulum,\nMEMB achieves the asymptotic result just using 16 epochs. In HalfCheetah, MEMB’s\n10\nperformance is at around 8000 at 200k steps, while all the other baselines’ performance\nis below 5300. In Reacher, MEMB and SAC have similar performance. Both of them\nare better than other algorithms. In Hopper, the ﬁnal performance of MEMB is around\n3300. The runner-up is POPLIN whose ﬁnal performance is around 2300. In Swimmer,\nthe performance of MEMB is the best. In Walker2d, SLBO is slighter better than MEMB.\nBoth of them achieve the average return of 2900 at 300k timesteps.\nReferences\nKavosh Asadi, Dipendra Misra, and Michael L Littman. Lipschitz continuity in model-\nbased reinforcement learning. ICML 2018, 2018.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie\nTang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforce-\nment learning in a handful of trials using probabilistic dynamics models. In Advances\nin Neural Information Processing Systems, pages 4754–4765, 2018.\nIgnasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and\nPieter Abbeel. Model-based reinforcement learning via meta-policy optimization. arXiv\npreprint arXiv:1809.05214, 2018.\nMarc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-eﬃcient approach\nto policy search. In Proceedings of the 28th International Conference on machine learn-\ning (ICML-11), pages 465–472, 2011.\nV Feinberg, A Wan, I Stoica, MI Jordan, JE Gonzalez, and S Levine. Model-based value\nexpansion for eﬃcient model-free reinforcement learning.\nIn Proceedings of the 35th\nInternational Conference on Machine Learning (ICML 2018), 2018.\nChelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse opti-\nmal control via policy optimization. In International Conference on Machine Learning,\npages 49–58, 2016.\nRoy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via\nsoft updates. UAI, 2016.\nScott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation\nerror in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C\nCourville. Improved training of wasserstein gans. In Advances in neural information\nprocessing systems, pages 5767–5777, 2017.\nTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learn-\ning with deep energy-based policies. In Proceedings of the 34th International Conference\non Machine Learning-Volume 70, pages 1352–1361. JMLR. org, 2017.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Oﬀ-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv\npreprint arXiv:1801.01290, 2018.\n11\nNicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval\nTassa. Learning continuous control policies by stochastic value gradients. In Advances\nin Neural Information Processing Systems, pages 2944–2952, 2015.\nMichael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:\nModel-based policy optimization. In Advances in Neural Information Processing Sys-\ntems, pages 12498–12509, 2019.\nGabriel Kalweit and Joschka Boedecker. Uncertainty-driven imagination for continuous\ndeep reinforcement learning. In Conference on Robot Learning, pages 195–206, 2017.\nThanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-\nensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.\nSergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and\nreview. arXiv preprint arXiv:1805.00909, 2018.\nSergey Levine and Pieter Abbeel. Learning neural network policies with guided policy\nsearch under unknown dynamics. In Advances in Neural Information Processing Sys-\ntems, pages 1071–1079, 2014.\nSergey Levine and Vladlen Koltun. Guided policy search. In International Conference on\nMachine Learning, pages 1–9, 2013.\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval\nTassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement\nlearning. arXiv preprint arXiv:1509.02971, 2015.\nYuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma.\nAlgorithmic framework for model-based deep reinforcement learning with theoretical\nguarantees. arXiv preprint arXiv:1807.03858, 2018.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,\net al. Human-level control through deep reinforcement learning. Nature, 518(7540):529,\n2015.\nAnusha Nagabandi, Gregory Kahn, Ronald S Fearing, and Sergey Levine. Neural network\ndynamics for model-based deep reinforcement learning with model-free ﬁne-tuning. In\n2018 IEEE International Conference on Robotics and Automation (ICRA), pages 7559–\n7566. IEEE, 2018.\nArthur George Richards. Robust constrained model predictive control. PhD thesis, Mas-\nsachusetts Institute of Technology, 2005.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust\nregion policy optimization.\nIn International conference on machine learning, pages\n1889–1897, 2015.\nRichard S Sutton. Integrated architectures for learning, planning, and reacting based on\napproximating dynamic programming. In Machine Learning Proceedings 1990, pages\n216–224. Elsevier, 1990.\n12\nYuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex\nbehaviors through online trajectory optimization.\nIn 2012 IEEE/RSJ International\nConference on Intelligent Robots and Systems, pages 4906–4913. IEEE, 2012.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-\nbased control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and\nSystems, pages 5026–5033. IEEE, 2012.\nC´edric Villani. Optimal transport: old and new. Bull. Amer. Math. Soc, 47:723–727, 2010.\nTingwu Wang and Jimmy Ba.\nExploring model-based planning with policy networks.\nICLR 2020, 2019.\nTingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois,\nShunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-\nbased reinforcement learning. arXiv preprint arXiv:1907.02057, 2019.\nBrian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy\ninverse reinforcement learning. 2008.\n13\nA\nRelated work\nThere are a plethora of works on MBRL. They can be classiﬁed into several categories\ndepending on the way to utilize the model, to search the optimal policy or the function\napproximator of the dynamics model. Iterative Linear Quadratic-Gaussian (iLQG) (Tassa\net al., 2012) assumes that the true dynamics are known to the agent. It approximates\nthe dynamics with linear functions and the reward function with quadratic functions.\nHence the problem can be transferred into the classic LQR problem. In Guided Policy\nSearch (Levine and Koltun, 2013; Levine and Abbeel, 2014; Finn et al., 2016), the system\ndynamics are modeled with the time-varying Gaussian-linear model. It approximated the\npolicy with a neural network π by minimizing the KL divergence between iLQG and π. A\nregularization term is augmented into the reward function to avoid the over-conﬁdence on\nthe policy optimization. Nonlinear function approximators can be leveraged to model more\ncomplicated dynamics. Deisenroth and Rasmussen (2011) use Gaussian processes to model\nthe dynamics of the environment. The policy gradient can be computed analytically along\nthe training trajectory. However, it may suﬀer from the curse of dimensionality which\nhinders its applicability in the real problem. Recently, more and more works incorporate\nthe deep neural network into MBRL. Heess et al. (2015) model the dynamics and reward\nwith neural networks, and compute the gradient with the true data. Richards (2005);\nNagabandi et al. (2018) optimize the action sequence to maximize the expected planning\nreward along with the learned dynamics model and then the policy is ﬁne-tuned with\nTRPO. Luo et al. (2018); Chua et al. (2018); Kurutach et al. (2018); Janner et al. (2019)\nuse the current policy to gather the data from the interaction with the environment and\nthen learn the dynamics model.\nIn the next step, the policy is improved (trained by\nthe model-free reinforcement learning algorithm) with a large amount of imaginary data\ngenerated by the learned model. MEMB may reduce to their work by updating the policy\nwith a model-free algorithm. Janner et al. (2019) provide an error bound on the long\nterm return of the k-step rollout given that the total variation of model bias and policy\ndistribution are bounded by ϵ. However, as we discussed, total variation may not be a good\nmeasure to describe the diﬀerence of learned model and true model especially when the\nsupport of the distributions is disjoint. Ensemble learning can also be applied to further\nreduce the model error. Asadi et al. (2018) leverage the Lipschitz model to analyze the\nmodel bias in the long term return.\nOur work consider the both model bias and the\ndistribution shift on the policy. In addition, we analyze the k-step rollout while Asadi\net al. (2018) just gives the result on full rollout.\nB\nAblation Study\nB.1\nHow to utilize the imaginary data\nIn this section, we make the ablation study to understand how much imaginary data we\nshould include in the algorithm. Remind that in our algorithm, the model is embedded\nin the Soft Bellman equation in the policy update step, which means we fully trust the\nmodel to compute the policy gradient.\nThis way to utilize the imaginary data would\nimprove the sample-eﬃciency. To conﬁrm our claim, we compare the MEMB with SVG\non the task HalfCheetah. Notice when we just utilize the real trajectory in the policy\nupdate, MEMB reduces to the SVG3. To remove the eﬀect of the entropy regularization,\n3It still has some diﬀerence such as the update rule of Q and V function\n14\n0\n25\n50\n75\n100\n125\n150\n175\n200\nsteps (1e3)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nAverage return\nalpha=0.2\nalpha=0.1\nalpha=0\nSVG\n(a) HalfCheetah (policy)\n0\n10\n20\n30\n40\n50\nEpochs\n1600\n1400\n1200\n1000\n800\n600\n400\n200\n0\nAverage return\nH=1\nH=2\nH=5\n(b) Pendulum (value)\n0\n25\n50\n75\n100\n125\n150\n175\n200\nsteps (1e3)\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\nAverage return\nH=1\nH=2\nH=5\n(c) HalfCheetah (value)\nFigure 2: Ablation study. In (a) we do the ablation study on the eﬀect of the imaginary\ndata on policy learning. In (b) and (c) we do ablation study on the value function learning\nwith diﬀerent length of rollout H. The x-axis is the training step and the y-axis is the\nreward.\nwe vary α (the regularizer parameter of entropy) in MEMB. When α = 0, it reduces to the\nnon-regularized formulation. In that case, we add the noise in policy for the exploration.\nWe report the result in panel (a) of Fig 2. It is clear that using imaginary data in policy\nupdate improves the learning with a wide margin.\nIn Section 3.2, we train Q and V with the true data set. In the experiment, we also\ntry the value expansion introduced in (Feinberg et al., 2018). We test the algorithm with\nvalue expansion, particularly with horizon H = 2 and H = 5. Our conclusion is that\nincluding the imaginary data to train the value function in our algorithm would hurt the\nperformance, especially in the complex tasks. We demonstrate the performance of MEMB\nwith value expansion in panel (b) and (c) of Figure 2. We ﬁrst test the algorithm on a\nsimple task Pendulum from OpenAI gym (Brockman et al., 2016) and show the result\nin Panel (b) of Figure 2. MEMB with H = 1 converges to the optimal policy within\nseveral epochs. When we increase the value of H, the performance decreases. Then we\nevaluate the performance of value expansion in a complex task HalfCheetah from the\nMujoco environment (Todorov et al., 2012) in panel (c) of Figure 2. In this task, value\nexpansion with H = 2 and H = 5 does not work at all. The reason would be that the\ndynamics model of HalfCheetah introduces more signiﬁcant model bias comparing to the\nsimple task Pendulum. Thus training both policy and value function in the imaginary\ndata set may cause a large error in policy gradient.\nB.2\nModel Error\nWe test the diﬀerence between the true model and learned model (using Wasserstein\ndistance), i.e., model error. In particular, we record the learned model by MEMB every\n10 epochs and then randomly sample the state action pair (s, a). Feed this pair to the\nlearn model we can obtain the predicted next state ˆs′, which is used to compared with the\ntrue next state s′. The error is averaged over state action pair. We do similar things on\nthe reward model. Result are tested on ﬁve trials using ﬁve diﬀerent random seeds. They\nare reported in Figure 3.\nB.3\nPlug The True Model Into MEMB\nIt is interesting to see the performance of MEMB if we plugin the true model in the algo-\nrithm. Since the dynamic in Mujoco is complicated, we just test a simple task pendulum.\n15\nIntuitively, MEMB with true model should have better performance than the MEMB with\nlearned one. We verify this in the Figure 4.\nB.4\nAsymptotic Performance\nWe evaluate the asymptotic behavior of model-free RL (particularly SAC) and MEMB\nagents through 2000 epoch of training (20M steps) on four simulation environments to\nsee the asymptotic result.\nThe SAC agent achieves 9453.32 ± 219.34 in HalfCheetah,\n4182.07 ± 63.23 in Walker2d, 3433.03 ± 18.05 in Hopper, 310.3 ± 4.60 in Swimmer. The\nMEMB agent achieves 8813.14±53.04 in HalfCheetah, 3544.3±252.3 in Walker2d, 3265±\n12.65 in Hopper, 307.57 ± 8.97 in Swimmer. In general, there is a small gap between the\nasymptotic performance between MBRL and MFRL. It is well expected since the learned\nmodel is not accurate.\nC\nEnvironment Overview and Hyperparameter Setting\nIn this section, we provide an overview of simulation environment in Table 1. The hyper-\nparameter setting for each environment is shown in Table 2.\nEnvironment Name\nObservation Space Dimension\nAction Space Dimension\nHorizon\nPendulum\n3\n1\n200\nInvertedPendulum\n4\n1\n1000\nHalfCheetah\n17\n6\n1000\nHopper\n11\n3\n1000\nWalker2D\n17\n6\n1000\nSwimmer\n8\n2\n1000\nReacher\n11\n2\n50\nTable 1: The observation space dimension, action space dimension, and horizon for each\nsimulation environment implemented in the experiment and ablation study.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nsteps (1e3)\n0\n5\n10\n15\n20\n25\ntransition error\ntransition_error\n(a) Transition model error\n0\n25\n50\n75\n100\n125\n150\n175\n200\nsteps (1e3)\n0\n1\n2\n3\n4\n5\nreward error\nreward_error\n(b) Reward model error\nFigure 3: Model error. We calculate the model error on the environment of HalfCheetah.\n16\n0\n2\n4\n6\n8\n10\n12\n14\nepochs\n1800\n1600\n1400\n1200\n1000\n800\n600\n400\n200\n0\nAverage return\nwith learned model\nwith true model\nFigure 4: MEMB with the true model vs MEMB with the learned model in pendulum\nPendulum\nInvertedPendulum\nHalfCheetah\nHopper\nWalker2D\nSwimmer\nReacher\nEpoch\n50\n40\n200\n300\n180\n50\nPolicy Learning Rate\n0.0003\nValue Learning Rate\n0.0003\n0.001\n0.001\n0.0003\n0.0003\n0.0003\nModel\nLearning Rate\n0.0003\n0.0001\n0.0001\n0.0001\nAlpha value\n(in entropy term)\n0.2\n0.1\n0.4\n0.2\n0.2\nenvironment steps\nper epoch\n1000\n50\nValue and Policy\nNetwork Architecture\n(256,256)\nModel\nNetwork Architecture\n(32,16)\n(256,128)\n(256,256)\n(256,128)\nTrain Actor-critic\nTimes (m)\n5\n1\n5\n3\n5\n5\nTable 2: The hyper-parameter used in training MEMB algorithm for each simulation\nenvironment. The number in policy, value, and model network architecture indicate the\nsize of hidden units in each layer of MLP. The ReLu activation function is implemented\nin all architecture.\nD\nProof\nIn this section, we give the proof of the theorem in the main paper. To start with, we give\nthe deﬁnition of the Wasserstein distance and its dual form, since we will use it frequently\nin the following discussion.\nDeﬁnition: Give a meritc space (M, d) and the set P(M) of probability measures on\nM, the Wasserstein metric between two probability distributions µ1 and µ2 in P(M) is\ndeﬁned as\nW(µ1, µ2) := inf\nj∈Σ\nZ Z\np(x, y)d(x, y)dxdy,\n(10)\nwhere Σ denotes the collection of all joint distributions p with marginal µ1 and µ2.\nThe dual presentation is a special case of the duality theorem of Kantorovich and\nRubinstein Villani (2010).\n17\nW(µ1, µ2) = sup\n∥f∥≤1\nZ\nf(s)(µ1(s) −µ2(s))ds\n(11)\nwhere ∥f∥≤1 means the function f is 1-Lipschitz.\nThe ﬁrst lemma is well known. It says the Lipschitz constant of a composition function\nis the product of Lipschitz constants of two functions.\nLemma 1. Deﬁne three metric spaces (M1, d1), (M2, d2), (M3, d3). Deﬁne Lipschitz func-\ntion f : M2 →M3 and g : M1 →M2 with constant Kf, Kg. Then h : f ◦g is Lipschitz\nwith constant Kh ≤KfKg\nProof.\nKh = sup\nx1,x2\nd3\n\u0000f(g(x1), f(g(x2)))\n\u0001\n/d(x1, x2)\n= sup\nx1,x2\nd2(g(x1), g(x2))\nd1(x1, x2)\nd3\n\u0000f(g(x1), f(g(x2))\n\u0001\nd2(g(x1), g(x2))\n≤sup\nx1,x2\nd2(g(x1), g(x2))\nd1(x1, x2)\nsup\ny1,y2\nd3(y1, y2)\nd2(y1, y2) ≤KgKf\n(12)\nLemma 2. Suppose we have two joint distribution p1(s, a) = p1(s)π1(a|s), p2(s, a) =\np2(s)π2(a|s). We further assume that W(p(s1), p(s2)) ≤ϵm and W(π1(a|s), π2(a|s)) ≤ϵπ.\nThen we have W(p1(s, a), p2(s, a)) ≤ϵπ + Kπϵm.\nProof. Using the triangle inequality, we have\nW(p1(s, a), p2(s, a)) ≤W(p(s1)π1(a|s), p1(s)π2(a|s)) + W(p1(s)π2(a|s), p2(s)π2(a|s)).\nNow we bound the ﬁrst term and second term respectively. For the ﬁrst term, according\nto the dual form of the Wasserstein distance, we have\nW(p1(s)π1(a|s), p1(s)π2(a|s)) = sup\n∥f∥≤1\nZ Z\nf(s, a)p1(s)(π1(a|s) −π2(a|s))dads\n.\n(13)\nNotice it is easy to verify that if f(s, a) is a 1-Lipschitz function w.r.t. (s, a), then for\na ﬁxed a, f(s, a) (we denote it as fs(a)) is also a 1-Lipschitz function w.r.t a.\nThus\n∀f ∈{f : ∥f∥≤1}, we have\nZ Z\nf(s, a)p1(s)(π1(a|s) −π2(a|s))dads\n=\nZ\np1(s)\nZ\nfs(a)(π1(a|s) −π2(a|s))dads\n≤\nZ\np1(s)W(π1(a|s), π2(a|s))ds = ϵπ.\n(14)\n18\nWe then bound the second term in the following way.\nW(p1(s)π2(a|s), p2(s)π2(a|s)) = sup\n∥f∥≤1\nZ Z\nf(s, a)(p1(s) −p2(s))π2(a|s)dads\n(1)\n= sup\n∥f∥≤1\nZ Z\nf(s, a)\nX\ngπ(fπ)1(fπ(s) = a)\n\u0000p1(s) −p2(s)\n\u0001\ndsda\n= sup\n∥f∥≤1\nZ X\nfπ\ngπ(fπ)f(s, fπ(s))(p1(s) −p2(s))ds\n≤\nX\nfπ\ngπ(fπ) sup\n∥f∥≤1\nZ\nf(s, fπ(s))(p1(s) −p2(s))ds\n=\nX\nfπ\ngπ(fπ)Kπ sup\n∥f∥≤1\nZ f(s, fπ(s))\nKπ\n(p1(s) −p2(s))ds\n(2)\n=\nX\nfπ\ngπ(fπ)KπW(p1(s), p2(s))\n≤Kπϵm\n(15)\nwhere (1) holds using the assumption π is in the Lipschitz class. (2) uses the fact that\nf(s,fπ(s))\nKπ\nis 1 Lipschitz, which holds using the similar argument in Lemma 1.\nCombine two pieces together, we obtain the result.\nLemma 3. Deﬁne p1,π1(s′|s) =\nR\np1(s′|s, a)π1(a|s)da and p2,π2(s′|s) =\nR\np2(s′|s, a)π2(a|s)da.\nSuppose W(p1(s′|s, a), p2(s′|s, a)) ≤ϵm, W(π1(a|s), W(π2(a|s))) ≤ϵπ, then we have\nW(p1,π1(s′|s), p2,π2(s′|s)) ≤Kmϵπ + ϵm\n.\nProof. We deﬁne a reference probability distribution p1,π2(s′|s) =\nR\np1(s′|s, a)π2(a|s)da.\nUsing the triangle inequaity, we have\nW(p1,π1(s′|s)), p2,π2(s′|s)) ≤W(p1,π1(s′|s), p1,π2(s′|s)) + W(p1,π2(s′|s), p2,π2(s′|s)).\nThus we just need to bound the two terms on the right hand side.\nFor the ﬁrst term, according to the deﬁnition of the Wasserstein distance, we have\nW(p1,π1(s′|s), p1,π2(s′|s))\n≤sup\n∥b∥≤1\nZ \u0000 Z\np1(s′|s, a)π1(a|s) −p1(s′|s, a)π2(a|s)da\n\u0001\nb(s′)ds′da\n(1)\n= sup\n∥b∥≤1\nZ Z X\nfm\ngm(fm)(π1(a|s) −π2(a|s))1(fm(a, s) = s′)b(s′)ds′da\n= sup\n∥b∥≤1\nX\nfm\ngm(fm)\nZ\n(π1(a|s) −π2(a|s))b(fm(s, a))da\n≤\nX\nfm\ngm(fm)Km sup\n∥b∥≤1\nZ\n(π1(a|s) −π2(a|s))b(fm(a, s))\nKm\nda\n(2)\n≤\nX\nfm\ngm(fm)KmW(π1(a|s), π2(a|s))\n≤Kmϵπ.\n(16)\n19\nwhere (1) holds using the assumption that the transition model is Lipschitz. (2) holds\nfrom the fact that b(fm(a, s))/Km is 1-Lipschitz w.r.t. a. Then we bound the second\nterm. Again according to the deﬁnition of the Wasserstein distance, we have\nW(p1,π2(s′|s), p2,π2(s′|s))\n= sup\n∥f∥≤1\nZ Z\n(p1(s′|s, a) −p2(s′|s, a))π2(a|s)f(s′)dads′\n≤\nZ\nπ2(a|s) sup\n∥f∥≤1\nZ\n(p1(s′|s, a) −p2(s′|s, a))f(s′)ds′da\n≤\nZ\nπ2(a|s)ϵmda\n=ϵm\n(17)\ncombine above two pieces together, we obtain the result.\nIn the next lemma, we would like to bound the Wasserstein distance between distri-\nbution W(pn\n1,π1(s′|s0), pn\n2,π2(s′|s0)), where s0 is the initial state,\npn\n1,π1(s′|s0) =\nR R\npn−1\n1,π1(s|s0)π(a|s)p1(s′|s, a)dads.\nLemma 4. Denote ∆= ϵm +Kmϵπ and ¯K = KmKπ. Then W(pn\n1,π1(s′|s0), pn\n2,π2(s′|s0)) ≤\n∆Pn−1\ni=0 ¯Ki = ∆1−¯\nKn\n1−¯\nK\nProof. We prove the result by induction. Denote δ(n) = W(pn\n1,π1(s′|s0), pn\n2,π2(s′|s0)). Thus\nδ(1) = W(p1,π1(s′|s0), p2,π2(s′|s0)). Using lemma 3, we have δ(1) ≤ϵm +kmϵπ = ∆. Using\nthe triangle inequality, we obtain\nδ(n) =W(pn\n1,π1(s′|s0), pn\n2,π2(s′|s0))\n≤W(pn\n1,π1(s′|s0), p1,π1(s′|pn−1\n2,π2(·|s0))) + W(p1,π1(s′|pn−1\n2,π2(·|s0)), P2,π2(s′|pn−1\n2,π2(·|s0)))\n=W(pn\n1,π1(s′|pn−1\n1,π1(·|s0)), p1,π1(s′|pn−1\n2,π2(·|s0))) + W(p1,π1(s′|pn−1\n2,π2(·|s0)), P2,π2(s′|pn−1\n2,π2(·|s0))).\n(18)\nWe bound two terms on the right hand side respectively. For the ﬁrst term, we denote\nµ1 := pn−1\n1,π1(·|s0) and µ2 := pn−1\n2,π2(·|s0) for short.\nThus, we need to bound W(pn\n1,π1(s′|µ1), pn\n1,π1(s′|µ2)). According to the deﬁnition of the\n20\nWasserstein distance, we have\nW(pn\n1,π1(s′|µ1), pn\n1,π1(s′|µ2))\n= sup\n∥f∥≤1\nZ\n(p1,π1(s′|µ1) −p1,π1(s′|µ2))f(s′)ds′\n= sup\n∥f∥≤1\nZ Z Z\np1(s′|s, a)π(a|s)(u1(s) −u2(s))f(s′)ds′dads\n= sup\n∥f∥≤1\nZ Z Z\np1(s′|s, a)\nX\nfπ\ngπ(fπ)1(fπ(s) = a)(u1(s) −u2(s))f(s′)ds′dads\n= sup\n∥f∥≤1\nZ Z X\nfπ\ngπ(fπ)p1(s′|s, fπ(s))(µ1(s) −µ2(s))f(s′)ds′ds\n= sup\n∥f∥≤1\nZ Z X\nfπ\ngπ(fπ)\nX\nfm\ngm(fm)(µ1(s) −µ2(s))1(fm(s, fπ(s)))f(s′)ds′ds\n≤\nX\nfπ\ngπ(fπ)\nX\nfm\ngm(fm) sup\n∥f∥≤1\nZ Z\n(µ1(s) −µ2(s))f(fm(s, fπ(s)))ds\n=\nX\nfπ\ngπ(fπ)\nX\nfm\ngm(fm)KπKm sup\n∥f∥≤1\nZ Z\n(µ1(s) −µ2(s))f(fm(s, fπ(s)))/(KπKm)ds\n=KπKmW(µ1, µ2)\n= ¯Kδ(n −1)\n(19)\nThen we bound the second term W(p1,π1(·|µ2), p2,π2(·|µ2)). Using the same argument with\nLemma 3, we have W(p1,π1(·|µ2), p2,π2(·|µ2)) ≤ϵm + Kmϵπ. We denote ∆:= ϵm + Kmϵπ.\nCombine all pieces together, we have\nδ(n) = ¯Kδ(n −1) + ∆\n(20)\nSuppose δ(n −1) ≤δ Pn−2\ni=0 ¯Ki, we have δ(n) ≤¯Kδ(n −1) + ∆≤∆Pn−1\ni=0 ¯Ki.\nIn the next lemma, we bound the long term reward with diﬀerent model and policy.\nLemma 5. let η1 be the long term reward induced by the policy π1(a|s) and model p1(s′|s, a).\nη2 is the long term reward induced by the policy π2(a|s) and model p2(s′|s, a). Suppose\nr(s, a) a Kr-Lipschitz function w.r.t (s, a). Then we have |η1 −η2| ≤Kr(γKπ(ϵm+Kmϵπ)\n(1−γ)(1−γ ¯\nK)\n+\n1\n1−γ ϵπ)\n21\nProof.\nη1 −η2 =\n∞\nX\nn=0\nZ\nγn(pn\n1,π1(s, a) −pn\n2,π2(s, a))r(s, a)dsda\n=Kr\n∞\nX\nn=0\nγn\nZ\n(pn\n1,π1(s, a) −pn\n2,π2(s, a))r(s, a)\nKr\ndsda\n(1)\n≤Kr\n∞\nX\nn=0\nγnW(pn\n1,π1(s, a), pn\n2,π2(s, a))\n(2)\n≤Kr\n∞\nX\nn=0\nγn(Kπ∆1 −¯Kn\n1 −¯K + ϵπ)\n=Kr\n\u0000γKπ∆\n(1 −γ)(1 −γ ¯K) +\n1\n1 −γ ϵπ\n\u0001\n=Kr(γKπ(ϵm + Kmϵπ)\n(1 −γ)(1 −γ ¯K) +\n1\n1 −γ ϵπ),\n(21)\nwhere (1) uses the fact that r(s, a)/Kr is 1-Lipschitz, (2) uses Lemma 3 and we have\nW(pn\n1,π1(s, a), pn\n2,π2(s, a)) ≤KπW(pn\n1,π1(s), pn\n2,π2(s)) + ϵπ.\nWe can derive the same bound for η2 −η1. Thus the lemma holds.\nLemma 6. Assume we run a branched rollout of length m. Before the branch we as-\nsume that the Wasserstein distance between ppre\n1 (s′|a, s), ppre\n2 (s′|a, s) is bounded by ϵpre\nm\nW(ppre\n1 (s′|a, s), ppre\n2 (s′|a, s)) ≤ϵpre\nm and similarly after the branch W(ppost\n1\n(s′|a, s), ppost\n2\n(s′|a, s)) ≤\nϵpost\nm . The policy diﬀerence (w.r.t the Wasserstein distance) is bounded by ϵpre\nπ\nand ϵpost\nπ\nrespectively .\nThen the m-step returns are bounded as |η1 −η2| ≤Kr\nKπ∆post(1−γm)\n(1−¯\nK)(1−γ)\n−\nKr\nKπ∆post(1−(γ ¯\nK)m)\n(1−γ ¯\nK)(1−¯\nK)\n+Kr\n1−γm\n1−γ ϵπ,post+γmKr[Kπ∆m,post\n1\n1−γ ¯\nK +Kπ∆pre\nγ\n(1−γ)(1−¯\nKγ) + ϵπ,pre\n1−γ ]\nProof. We want to bound W(pt\n1,π1(s, a), pt\n2,π2(s, a)). Depending on whether t is larger than\nm or not, we have two cases.\nFor t ≤m, using Lemma 4, we have\nW(pt\n1,π1(s), pt\n2,π2(s))) ≤∆post\n1 −¯Kt\n1 −¯K ,\nwhere ∆post = ϵpost\nm\n+ Kmϵpost\nπ\n.\nThen we use Lemma 1 and obtain\nW(pt\n1,π1(s, a), pt\n2,π2(s, a)) ≤Kπ∆post\n1 −¯Kt\n1 −¯K + ϵpost\nπ\nFor t > m, we denote ∆m,post = W(pm\n1,π1(s, a), pm\n2,π2(s, a)) ≤Kπ∆post 1−¯\nKm\n1−¯\nK + ϵpost\nπ\n.\nUsing (20) in the proof of lemma 4, we have\nW(pt\n1,π1(s, a), p2,π2(s, a)) ≤¯Kt−m∆m,post+\nt−m−1\nX\ni=0\n¯Ki∆pre = ¯Kt−m∆m,post+1 −¯Kt−m\n1 −¯K\n∆pre.\nUsing Lemma2 again, we have\n22\nW(pt\n1,π1(s, a), pt\n2,π2(s, a)) ≤Kπ\n\u0000 ¯Kt−m∆m,post + 1 −¯Kt−m\n1 −¯K\n∆pre) + ϵpre\nπ\nNow we are ready to bound the diﬀerence between η1 and η2. We ﬁrst bound the term\nfrom 0 to m-1.\nm−1\nX\nn=0\nZ\nγn(pn\n1,π1(s, a) −pn\n2,π2(s, a))r(s, a)dsda\n=Kr\nm−1\nX\nn=0\nγn\nZ\n(pn\n1,π1(s, a) −pn\n2,π2(s, a))r(s, a)\nKr\ndsda\n≤Kr\nm−1\nX\nn=0\nγnW(pn\n1,π1(s, a), pn\n2,π2(s, a))\n(a)\n≤Kr\nm−1\nX\nn=0\nγn(Kπ∆post\n1 −¯Kn\n1 −¯K + ϵπ,post)\n=Kr\nKπ∆post(1 −γm)\n(1 −¯K)(1 −γ)\n−Kr\nKπ∆post(1 −(γ ¯K)m)\n(1 −γ ¯K)(1 −¯K)\n+ Kr\n1 −γm\n1 −γ ϵπ,post\n(22)\nwhere (a) uses Lemma 4 and Lemma 2.\nThen we bound the term from m to inﬁnity.\nDenote the error term caused by step 0 to m as ∆m,post and ∆m,post = ∆post 1−¯\nKm\n1−¯\nK .\nFollowing similar step as that in t < m, we obtain.\n∞\nX\nn=m\nZ\nγn(pn\n1,π1(s, a) −pn\n2,π2(s, a))r(s, a)dsda\n≤Kr\n∞\nX\nn=m\nγn(Kπ ¯Kn−m∆m,post + Kπ\n1 −¯Kn−m\n1 −¯K\n∆pre + ϵπ,pre)\n=γm[Kπ∆m,post\n1\n1 −γ ¯K + Kπ∆pre\nγ\n(1 −γ)(1 −¯Kγ) + ϵπ,pre\n1 −γ ]\n(23)\nProof of theorem 1. Let πD be the data collecting policy. η(π) −ˆη(π) = η(π) −η(πD) +\nη(πD) −ˆη(π).\nUse Lemma 5 by setting ϵm = 0 we have\n|η(π) −η(πD)| ≤\nγKr ¯Kϵπ\n(1 −γ)(1 −γ ¯K) +\n1\n1 −γ Krϵπ.\nUse Lemma 5 again, we have\n|η(πD) −ˆη(π)| ≤\nγKr ¯Kϵπ\n(1 −γ)(1 −γ ¯K) +\n1\n1 −γ Krϵπ +\nγKrKπϵm\n(1 −γ)(1 −γ ¯K).\nCombine above two results, we have the theorem.\nProof of theorem 2. We deﬁne a reference process, where it executes the policy πD under\nthe true dynamics until the branch point and then executes the new policy π with true\ntransition model. We denote the return under this scheme as ηπD,π. Then we have\n23\n|η(π) −ηbranch(π)| ≤|η(π) −ηπD,π| + |ηπD,π −ηbranch(π)|. Now we bound two terms\non the right hand side respectively.\nFor the ﬁrst term η(π) −ηπD,π, the error just come from ϵpre\nπ . Plug ϵpre\nπ\n= ϵπ into\nlemma 6 and set all the other error to zeros and we have\n|η(π) −ηπD,π| ≤[\nγk+1 ¯K\n(1 −γ)(1 −¯Kγ) +\nγk\n1 −γ ]ϵπ.\nFor the second term ηπD,π −ηbranch(π), the error comes from ϵpost\nm .\nPlug ϵpost\nm\n=\nϵk into Lemma 6, we obtain |ηπD,π −ηbranch(π)| ≤\nKrKπϵm(1−γk)\n(1−¯\nK)(1−γ)\n−KrKπϵm(1−(γ ¯\nK)k)\n(1−γ ¯\nK)(1−¯\nK)\n+\nγk KrKπϵm(1−¯\nKk)\n(1−¯\nK)(1−γ)\n.\nCombine all pieces together, we have the result.\nE\nDerivation\nWe start the derivation with minimization of the KL divergence KL(˜p(τ)||p(τ)), where\np(τ) = [p(s0) QT\nt=0 ˆp(st+1|st, at)] exp\n\u0000 PT\nt=0 ˆr(st, at)\n\u0001\n, ˜p(τ) = p(s0) QT\nt=1 ˆp(st+1|st, at)π(at|st).\nKL(˜p(τ)||p(τ)) =Eτ∼˜p(τ)\nT\nX\nt=1\n\u0000ˆr(st, at) −log π(at|st)\n\u0001\n=\nX\nτ\np(s0)\nT\nY\nt=0\nˆp(st+1|st, at)π(at|st)\nT\nX\nt=0\n\u0000ˆr(st, at) −log π(at|st)\n\u0001\n(24)\nNow we optimize KL divergence w.r.t π(·|st). Considering the constraint P\nj π(j|st) =\n1, we introduce a Lagrangian multiplier λ(P|A|\nj=1 π(j|st)−1) (Rigorously speaking, we need\nto consider another constraint that each element of π is larger than 0, but later we will\nsee the optimal value satisﬁes this constraint automatically). Now we take gradient of\nKL(˜p(τ)||p(τ)) + λ(P|A|\nj=1 π(j|st) −1) w.r.t π(·|s), and obtain\nlog π∗(at|st) = Eˆp(st+1:T ,at+1:T |st,at)[\nT\nX\nt′=t\nˆr(st′, at′) −\nT\nX\nt′=t+1\nlog π(at′|st′)] −1 + λ.\nTherefore\nπ∗(at|st) ∝exp\n\u0000Eˆp(st+1:T ,at+1:T |st,at)[\nT\nX\nt′=t\nˆr(st′, at′) −\nT\nX\nt′=t+1\nlog π(at′|st′)]\n\u0001\n.\nSince we know P\nj π(j|st) = 1, thus we have\nπ∗(at|st) = 1\nZ exp\n\u0000Eˆp(st+1:T ,at+1:T |st,at)[\nT\nX\nt′=t\nˆr(st′, at′) −\nT\nX\nt′=t+1\nlog π(at′|st′)]\n\u0001\n.\nFor convenience, we deﬁne the soft V function and Q function as that in (Levine,\n2018).\n24\nV (st+1) := ˆE\n\u0002\nT\nX\nt′=t+1\nˆr(st′, at′) −log π(at|st)|st+1\n\u0003\n,\nQ(st, at) := ˆr(st, at) + Eˆp(st+1|st,at)[V (st+1)].\n(25)\nNotice it is easy to incorporate the discount factor by deﬁning a absorbing state where\neach transition have (1 −γ) probability to go to that state. Thus we have\nQ(st, at) = ˆr(st, at) + γEst+1∼ˆp(st+1|st,at)[V (st+1)],\n(26)\nV (st) = Eπ(at|st)[Q(st, at) −log π(at|st)].\n(27)\n25\n",
  "categories": [
    "cs.LG",
    "cs.AI"
  ],
  "published": "2020-06-16",
  "updated": "2020-06-16"
}