{
  "id": "http://arxiv.org/abs/2006.10027v2",
  "title": "Deep Learning Meets SAR",
  "authors": [
    "Xiao Xiang Zhu",
    "Sina Montazeri",
    "Mohsin Ali",
    "Yuansheng Hua",
    "Yuanyuan Wang",
    "Lichao Mou",
    "Yilei Shi",
    "Feng Xu",
    "Richard Bamler"
  ],
  "abstract": "Deep learning in remote sensing has become an international hype, but it is\nmostly limited to the evaluation of optical data. Although deep learning has\nbeen introduced in Synthetic Aperture Radar (SAR) data processing, despite\nsuccessful first attempts, its huge potential remains locked. In this paper, we\nprovide an introduction to the most relevant deep learning models and concepts,\npoint out possible pitfalls by analyzing special characteristics of SAR data,\nreview the state-of-the-art of deep learning applied to SAR in depth, summarize\navailable benchmarks, and recommend some important future research directions.\nWith this effort, we hope to stimulate more research in this interesting yet\nunder-exploited research field and to pave the way for use of deep learning in\nbig SAR data processing workflows.",
  "text": "ACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n1\nDeep Learning Meets SAR\nXiao Xiang Zhu, Fellow, IEEE, Sina Montazeri, Mohsin Ali, Yuansheng Hua, Member, IEEE, Yuanyuan\nWang, Member, IEEE, Lichao Mou, Member, IEEE, Yilei Shi, Member, IEEE, Feng Xu, Senior Member, IEEE,\nRichard Bamler, Fellow, IEEE\nAbstract—This is the pre-acceptance version, to read the ﬁnal\nversion please go to IEEE Geoscience and Remote Sensing\nMagazine on IEEE XPlore.\nDeep learning in remote sensing has become an international\nhype, but it is mostly limited to the evaluation of optical\ndata. Although deep learning has been introduced in Synthetic\nAperture Radar (SAR) data processing, despite successful ﬁrst\nattempts, its huge potential remains locked. In this paper, we\nprovide an introduction to the most relevant deep learning\nmodels and concepts, point out possible pitfalls by analyzing\nspecial characteristics of SAR data, review the state-of-the-art\nof deep learning applied to SAR in depth, summarize available\nbenchmarks, and recommend some important future research\ndirections. With this effort, we hope to stimulate more research\nin this interesting yet under-exploited research ﬁeld and to pave\nthe way for use of deep learning in big SAR data processing\nworkﬂows.\nIndex Terms—Benchmarks, deep learning, despeckling, Inter-\nferometric Synthetic Aperture Radar (InSAR), object detection,\nparameter inversion, Synthetic Aperture Radar (SAR), SAR-\noptical data fusion, terrain surface classiﬁcation.\nI. MOTIVATION\nIn recent years, deep learning [1] has been developed at a\ndramatic pace, achieving great success in many ﬁelds. Unlike\nconventional algorithms, deep learning-based methods com-\nmonly employ hierarchical architectures, such as deep neural\nnetworks, to extract feature representations of raw data for\nnumerous tasks. For instance, convolutional neural networks\n(CNNs) are capable of learning low- and high-level features\nfrom raw images with stacks of convolutional and pooling\nlayers, and then applying the extracted features to various\ncomputer vision tasks, such as large-scale image recognition\n[2], object detection [3], and semantic segmentation [4].\nThe work of X. Zhu is jointly supported by the European Research Council\n(ERC) under the European Union’s Horizon 2020 research and innovation pro-\ngramme (grant agreement No. [ERC-2016-StG-714087], Acronym: So2Sat),\nby the Helmholtz Association through the Framework of Helmholtz AI -\nLocal Unit “Munich Unit @Aeronautics, Space and Transport (MASTr)” and\nHelmholtz Excellent Professorship “Data Science in Earth Observation - Big\nData Fusion for Urban Research” and by the German Federal Ministry of\nEducation and Research (BMBF) in the framework of the international future\nAI lab ”AI4EO” (Grant number: 01DD20001). (Corresponding author: Xiao\nXiang Zhu).\nX. Zhu, M. Ali, Y. Hua, and L. Mou are with the Remote Sensing\nTechnology Institute (IMF), German Aerospace Center (DLR), Germany and\nwith Data Science in Earth Observation (SiPEO, former: Signal Processing\nin Earth Observation), Technical University of Munich (TUM), Germany (e-\nmails: xiaoxiang.zhu@dlr.de).\nS. Montazeri and Y. Wang are with DLR-IMF, Wessling, Germany.\nY. Shi is with the Chair of Remote Sensing Technology (LMF), TUM,\n80333 Munich, Germany.\nF. Xu is with the Key Laboratory for Information Science of Electromag-\nnetic Waves (MoE), Fudan Univeristy, Shanghai, China.\nR. Bamler is with DLR-IMF, 82234 Wessling, Germany and TUM-LMF,\n80333 Munich, Germany.\nInspired by numerous successful applications in the com-\nputer vision community, the use of deep learning in remote\nsensing is now obtaining wide attention [5]. As ﬁrst attempts in\nSynthetic Aperture Radar (SAR), deep learning-based methods\nhave been adopted for a variety of tasks, including terrain sur-\nface classiﬁcation [6], object detection [7], parameter inversion\n[8], despeckling [9], speciﬁc applications in Interferometric\nSAR (InSAR) [10], and SAR-optical data fusion [11].\nFor terrain surface classiﬁcation from SAR and Polarimetric\nSAR (PolSAR) images, effective feature extraction is essential.\nThese features are extracted based on expert domain knowl-\nedge and are usually applicable to a small number of cases and\ndata sets. Deep learning feature extraction has however proved\nto overcome, to some degrees, both of the aforementioned\nissues [6]. For SAR target detection, conventional approaches\nmainly rely on template matching, where speciﬁc templates\nare created manually [12] to classify different categories, or\nthrough the use of traditional machine learning approaches,\nsuch as Support Vector Machines (SVMs) [13], [14]; in\ncontrast, modern deep learning algorithms aim at applying\ndeep CNNs to extract discriminative features automatically for\ntarget recognition [7]. For parameter inversion, deep learning\nmodels are employed to learn the latent mapping function from\nSAR images to estimated parameters, e.g., sea ice concentra-\ntion [8]. Regarding despeckling, conventional methods often\nrely on artiﬁcial ﬁlters and may suffer from mis-eliminating\nsharp features when denoising. Furthermore, the development\nof joint analysis of SAR and optical images has been motivated\nby the capacities of extracting features from both types of\nimages. For applications in InSAR, only a few studies have\nbeen carried out such as the work described in [10]. However,\nthese algorithms neglect the special characteristics of phase\nand simply use an out-of-the-box deep learning-based model.\nDespite the ﬁrst successes, and unlike the evaluation of\noptical data, the huge potential of deep learning in SAR and\nInSAR remains locked. For example, to the best knowledge\nof the authors, there is no single example of deep learning in\nSAR that has been developed up to operational processing of\nbig data or integrated into the production chain of any satellite\nmission. This paper aims at stimulating more research in this\ninteresting yet under-exploited research ﬁeld.\nIn the remainder of this paper, Section II ﬁrst introduces\nthe most commonly used deep learning models in remote\nsensing. Section III describes the speciﬁc characteristics of\nSAR data that have to be taken into account to exploit the full\npotential of SAR combined with deep learning. Section IV\ndetails recent advances in the utilization of deep learning on\ndifferent SAR applications, which were outlined earlier in the\nsection. Section V reviews the existing benchmark data sets\narXiv:2006.10027v2  [eess.IV]  5 Jan 2021\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n2\nfor different applications of SAR and their limitations. Finally,\nSection VI concludes current research, and gives an overview\nof promising future directions.\nII. INTRODUCTION TO RELEVANT DEEP LEARNING\nMODELS AND CONCEPTS\nIn this section, we brieﬂy review relevant deep learning al-\ngorithms originally proposed for visual data processing that are\nwidely used for the state-of-the-art research of deep learning\nin SAR. In addition, we mention the latest developments of\ndeep learning, which are not yet widely applied to SAR but\nmay help create next generation of its algorithms. Fig. 1 gives\nan overview of the deep learning models we discuss in this\nsection.\nBefore discussing deep learning algorithms, we would\nlike to stress that the importance of high-quality benchmark\ndatasets in deep learning research cannot be overstated. Es-\npecially in supervised learning, the knowledge that can be\nlearned by the model is bounded by the information present\nin the training dataset. For example, the MNIST [25] dataset\nplayed a key role in Yann LeCun’s seminal paper about con-\nvolutional neural networks and gradient-based learning [26].\nSimilarly, there would be no AlexNet [27], the network that\nkick-started the current deep learning renaissance, without the\nImageNet [28] dataset, which contains over 14 million images\nand 22,000 classes. ImageNet has been such an important part\nof deep learning research that, even after over 10 years of\nbeing published, it is still used as a standard benchmark to\nevaluate the performance of CNNs for image classiﬁcation.\nA. Deep Learning Models\nThe main principle of deep learning models is to encode\ninput data into effective feature representations for target tasks.\nTo examplify how a deep learning framework works, we\ntake autoencoder as an example: it ﬁrst maps an input data\nto a latent representation via a trainable nonlinear mapping\nand then reconstructs inputs through reverse mapping. The\nreconstruction error is usually deﬁned as the Euclidian distance\nbetween inputs and reconstructed inputs. Parameters of autoen-\ncoders are optimized by gradient descent based optimizers, like\nstochastic gradient descent (SGD), RMSProp [29] and ADAM\n[30], during the backpropagation step.\n1) Convolutional Neural Networks (CNN): With the suc-\ncess of AlexNet in the ImageNet Large Scale Visual Recog-\nnition Challenge (ILSVRC-2012), where it scored a top-5\ntest error of 15.3% compared to 26.2% of the second best,\nCNNs have attracted worldwide attention and are now used for\nmany image understanding tasks, such as image classiﬁcation,\nobject detection, and semantic segmentation. AlexNet consists\nof ﬁve convolutional layers, three max-pooling layers, and\nthree fully-connected layers. One of the key innovations of\nthe AlexNet was the use of GPUs, which made it possible to\ntrain such large networks with huge datasets without using\nsupercomputers. In just two years, VGGNet [2] overtook\nAlexNet in performance by achieving a 6.8% top-5 test error\nin ILSVRC-2014; the main difference was that it only used\n3x3-sized convolutional kernels, which enabled it to have more\nnumber of channels and in turn capture more diverse features.\nResNet [31], U-Net [32], and DenseNet [33] were the\nnext major CNN architectures. The main feature of all these\narchitectures was the idea of connecting, not only neighboring\nlayers but any two layers in the network, by using skip\nconnections. This helped reduce loss of information across\nnetworks, mitigated the problem of vanishing gradients and\nallowed the design of deeper networks. U-Net is one of the\nmost commonly used image segmentation networks. It has\nautoencoder based architecture where it uses skip connections\nto concatenate features from the ﬁrst layer to last, second\nto second last, and so on: this way it can get ﬁne-grained\ninformation from initial layers to the end layers. U-Net was\ninitially proposed for medical image segmentation, where\ndata labeling is a big problem. The authors used heavy data\naugmentation techniques on input data, making it possible to\nlearn from only a few hundred annotated samples. In ResNet\nskip connections were used within individual blocks and not\nacross the whole network. Since its initial proposal, it has seen\nmany architectural tweaks, and even after 4-5 years its variants\nare always among the top scorers on ImageNet. In DenseNet\nall the layers were attached to all preceding layers, reducing\nthe size of the network, albeit at the cost of memory usage.\nFor a more detailed explanations of different CNN models,\ninterested readers are referred to [34]. These CNN models\nhave also proved their worth in SAR processing tasks e.g. see\n[35], [36], [37]. For more examples and details of CNNs in\nSAR we refer our readers to Section IV.\n2) Recurrent Neural Networks (RNN):\nBesides CNNs,\nRNNs [38] are another major class of deep networks. Their\nmain building blocks are recurrent units, which take the current\ninput and output of the previous state as input. They provide\nstate-of-the-art results for processing data of variable lengths\nlike text and time series data. Their weights can be replaced\nwith convolutional kernels for visual processing tasks such as\nimage captioning and predicting future frames/points in visual\ntime-series data. Long short term memory (LSTM) [39] is\none of the most popular architectures of RNN: its cells can\nstore values from any past instances while not being severely\naffected by the problem of gradient diminishing. Just like in\nany other time series data from deep learning toolkit the RNNs\nare natural choice to process SAR time series data, e.g. see\n[40].\n3) GANs: Proposed by Ian Goodfellow et al. [41], GANs\nare among the most popular and exciting inventions in the ﬁeld\nof deep learning. Based on game-theoretic principles, they\nconsist of two networks called a generator and a discriminator.\nThe generator’s objective is to learn a latent space, through\nwhich it can generate samples from the same distribution\nas the training data, while the discriminator tries to learn\nto distinguish if a sample is from the generator or training\ndata. This very simple mechanism is responsible for most\ncutting-edge algorithms of various applications, e.g., gener-\nating artiﬁcial photo-realistic images/videos, super-resolution,\nand text to image synthesis. For example in the SAR domain\nGANs have already been successfully used in cloud removal\napplications [42], [43]. The reader is referred to Section IV\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n3\nDeep\nLearning\nResNet Block\nGNN\nVGG Net\nVariational Auto Encoder\nGenerative Adversarial Network\nLSTM Unit\nRNN Unit Unfold\nConvolutional GNN\nRecurrent GNN\nGenerative \nmodels\nDeep RL\nRNN\nCNN\nUNet\nNeural Architecture Search \nUsing DeepRL\nFig. 1: A Selection of relevant deep learning models. Sources of the images: VGG [15], ResNet [16], U-Net [17], LSTM [18],\nRNN [19], VAE [20], GAN [21], CGNN [22], RGNN [23], and DeepRL [24].\nfor more examples.\nB. Supervised, Unsupervised and Reinforcement Learning\n1) Supervised Learning: Most of popular deep learning\nmodels fall under the category of supervised deep learning,\ni.e. they need labelled datasets to learn the objective functions.\nOne of big challenges of supervised learning is generalization,\ni.e. how well a trained model performs on test data. Therefore\nit is vital that training data truly represents the true distribution\nof data so it can handle all the unseen data. If the model\nﬁts well on training data and fails on test data then it is\ncalled overﬁtting, in deep learning literature there are several\ntechniques that can be used to avoid it, e.g. Dropout[44].\n2) Unsupervised Learning: Unsupervised learning refers\nto the class of algorithms where the training data do not\ncontain labels. For instance, in classical data analysis, principal\ncomponent analysis (PCA) [45] can be used to reduce the\ndata dimension followed by a clustering algorithm to group\nsimilar data points. In deep learning generative models like\nautoencoders and variational autoencoders (VAEs) [46] and\nGenerative Adversarial Networks (GANs) [41] are some of\npopular techniques that can be used for unsupervised learning.\nTheir primary goal is to generate output data from the same\ndistribution as input data. Autoencoders consists of an encoder\npart which ﬁnds compressed latent representation of input\nand a decoder part which decodes that representation back\nto the original input. VAEs take autoencoders to the next level\nby learning the whole distribution instead of just a single\nrepresentation at the end of the encoder part, which in turn\ncan be used by the decoder to generate the whole distribution\nof outputs. The trick to learning this distribution is to also\nlearn variance along with mean of latent representation at\nthe encoder-decoder meeting point and add a KL-divergence-\nbased loss term to the standard reconstruction loss function of\nthe autoencoders.\n3) Deep Reinforcement Learning (DeepRL):\nReinforce-\nment Learning (RL) tries to mimic the human learning behav-\nior, i.e., taking actions and then adjusting them for the future\naccording to feedback from the environment. For example,\nyoung children learn to repeat or not repeat their actions based\non the reaction of their parents. The RL model consists of an\nenvironment with states, actions to transition between those\nstates, and a reward system for ending up in different states.\nThe objective of the algorithm is to learn the best actions for\ngiven states using a feedback reward system. In a classical RL\nalgorithms function, approximators are used to calculate the\nprobability of different actions in different states. DeepRL uses\ndifferent types of neural networks to create these functions\n[47][48]. Recently DeepRL received particular attention and\npopularity due to the success of Google Deep Mind’s AlphaGo\n[49], which defeated the Go board game world champion. This\ntask was considered impossible by computers just until a few\nyears ago.\nC. Relevant Deep Learning Concepts\n1) Automatic Machine Learning (AutoML): Deep networks\nhave many hyperparameters to choose from, for example,\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n4\nnumber of layers, kernel sizes, type of optimizer, skip connec-\ntions, and the like. There are billions of possible combinations\nof these parameters and given high computational cost, time,\nand energy costs it is hard to ﬁnd the best performing network\neven from among a few hundred candidates. In the case of\ndeep learning, the objective of AutoML is mainly to ﬁnd the\nmost efﬁcient and high performing deep network for a given\ndataset and task. The ﬁrst major attempt in this ﬁeld was by\nZoph et al. [50], who used DeepRL to ﬁnd the optimum CNN\nfor image classiﬁcation. In their system an RNN creates CNN\narchitectures and, based on their classiﬁcation results, proposes\nchanges to them. This process continues to loop until the\noptimum architecture is found. This algorithm was able to ﬁnd\ncompeting networks compared to the state-of-the-art but took\nover 800 GPUs, which was unrealistic for practical application.\nRecently, there have been many new developments in the\nAutoML ﬁeld, which have made it possible to perform such\ntasks in more intelligent and efﬁcient ways. More details about\nthe ﬁeld of network architectural search can be found in [51].\nFurthermore AutoML have also already been successfully\napplied to SAR for PolSAR classiﬁcation [52]. The method\nshows great potential for segmentation and classiﬁcation tasks\nin particular.\n2) Geometric Deep Learning – Graph Neural Networks\n(GNNs): Except for well-structured image data, there is a large\namount of unstructured data, e.g., knowledge graphs and social\nnetworks, in real life that cannot be directly processed by a\ndeep CNN. Usually, these data are represented in the form\nof graphs, where each node represents an entity and edges\ndelineate their mutual relations. To learn from unstructured\ndata, geometric deep learning has been attracting an increasing\nattention, and a most-commonly used architecture is GNN,\nwhich is also proven successful in dealing with structured\ndata. Speciﬁcally, Using the terminology of graphs, nodes of\na graph can be regarded as feature descriptions of entities,\nand their edges are established by measuring their relations\nor distances and encoded in an adjacency matrix. Once a\ngraph is constructed, messages can be propagated among each\nnode by simply performing matrix multiplication. Follow-\ningly, [53] proposed Graph Convolutional Networks (GCNs)\ncharacterized by utilizing graph convolutions, and [45] fasten\nthe process. Moreover recurrent units in RGNNs (Recurrent\nGraph Neural Network) [54] [55] have also been proven to\nobtain achievements in learning from graphs. The usefulness\nof GNNs in SAR is still to be properly explored as [56] is one\nof the only attempts in trying to do so.\nIII. POSSIBLE PITFALLS\nTo develop tailored deep learning architectures and prepare\nsuitable training datasets for SAR or InSAR tasks, it is\nimportant to understand that SAR data is different from optical\nremote sensing data, not to mention images downloaded from\nthe internet. In this section, we discuss the special character-\nistics (and possible pitfalls) encountered while applying deep\nlearning to SAR.\nWhat makes SAR data and SAR data processing by neural\nnetworks unique? SAR data are substantially different from\noptical imagery in many respects. These are a few points to\nbe considered when transferring CNN experience and expertise\nfrom optical to SAR data:\n• Dynamic Range. Depending on their spatial resolution,\nthe dynamic range of SAR images can be up to 90\ndB (TerraSAR-X high resolution spotlight data with a\nresolution of about 1 m). Moreover, the distribution is\nextremely asymmetric, with the majority of pixels in the\nlow amplitude range (distributed scatterers) and a long\ntail representing bright discrete scatterers, in particular\nin urban areas. Standard CNNs are not able to handle\nsuch dynamic ranges and, hence, most approaches feature\ndynamic compression as a preprocessing step. In [57], the\nauthors ﬁrst take only amplitude values from 0 to 255\nand then subtract mean values of each image. In [11],\n[58], normalization is performed as a pre-processing step,\nwhich compresses the dynamic range signiﬁcantly.\n• Signal Statistics. In order to retrieve features from SAR\n(amplitude or intensity) images the speckle statistics must\nbe considered. Speckle is a multiplicative, rather than an\nadditive, phenomenon. This has consequences: While the\noptimum estimator of radar brightness of a homogeneous\nimage patch under speckle is a simple moving averaging\noperation (i.e., a convolution, like in the additive noise\ncase), other optimum detectors of edges and low-level\nfeatures under additive Gaussian noise may no longer\nbe optimum in the case of SAR. A popular example\nis Touzi’s CFAR edge detector [59] for SAR images,\nwhich uses the ratio of two spatial averages over adjacent\nwindows. This operation cannot be emulated by the ﬁrst\nlayer of a standard CNN.\nSome studies use a logarithmic mapping of the SAR\nimages prior to feeding them into a CNN [60], [9]. This\nturns speckle into an additive random variable and —as\na side effect —reduces dynamic range. But still, a single\nconvolutional layer can only emulate approximations to\noptimum SAR feature estimators. It could be valuable to\nsupplement the original log-SAR image by a few lowpass\nﬁltered and logarithmized versions as input to the CNN.\nAnother approach is to apply some sophisticated speckle\nreduction ﬁlter before entering the CNN, e.g., non-local\naveraging [61], [62], [63].\n• Imaging Geometry. The SAR image coordinates range\nand azimuth are not arbitrary coordinates like East and\nNorth or x and y, but rather reﬂect the peculiarities of\nthe image generation process. Layover always occurs at\nnear range shadow always at far range of an object. That\nmeans, that data augmentation by rotation of SAR images\nwould lead to nonsense imagery that would never be\ngenerated by a SAR.\n• The Complex Nature of SAR Data. The most valuable\ninformation of SAR data lies in its phase. This applies for\nSAR image formation, which takes place in the complex\nsignal domain, as well as for polarimetric, interferometric,\nand tomographic SAR data processing. This means that\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n5\nthe entire CNN must be able to handle complex numbers.\nFor the convolution operation this is trivial. The nonlinear\nactivation function and the loss function, however, require\nthorough consideration. Depending on whether the acti-\nvation function acts on the real and imaginary parts of\nthe signal independently, or only on its magnitude, and\nwhere bias is added, phase will be distorted to different\ndegrees.\nIf we use polarimetric SAR data for land cover or\ntarget classiﬁcation, a nonlinear processing of the phase\nis even desirable, because the phase between different\npolarimetric channels has physical meaning and, hence,\ncontributes to the classiﬁcation process.\nIn SAR interferometry and tomography, however, the\nabsolute phase has no meaning, i.e., the CNN must\nbe invariant to an arbitrary phase offset. Assume some\ninterferometric input signal x to a CNN and the output\nsignal CNN(x) with phase\nˆφ = ̸ CNN(x).\n(1)\nAny constant phase offset φ0 does not change the mean-\ning of the interferogram. Hence, we require an invariance\nthat we refer to as ”phase linearity” (valid at least in the\nexpectation):\nCNN(xejφ0) = CNN(x)ejφ0.\n(2)\nThis linearity is violated, for example, if the activation\nfunction is applied to real and imaginary parts separately,\nor if a bias is added to the complex numbers.\nAnother point to consider in regression-type InSAR\nCNN processing (e.g., for noise reduction) is the loss\nfunction. If the quantity of interest is not the complex\nnumber itself, but its phase, the loss function must be\nable to handle the cyclic nature of phases. It may also be\nadvantageous that the loss function is independent—at\nleast to a certain degree —of the signal magnitude to\nrelieve the CNN from modelling the magnitude. A loss\nfunction that meets these requirements is, for example,\nL = |E[ej(̸\nCNN(x)−̸\ny)]|,\n(3)\nwhere y is the reference signal.\nSome authors use magnitude and phase, rather than\nreal and imaginary parts, as input to the CNN. This\napproach is not invariant to phase offset, either. The in-\nterpretation of a phase function as a real-valued function\nforces the CNN to disregard the sharp discontinuities at\nthe ±π-transitions, whose positions are inconsequential.\nA standard CNN would pounce on these, interpreting\nthem as edges.\n• Simulation-based Training and Validation Data? The\nprevailing lack of ground-truth for regression-type tasks,\nlike speckle reduction or InSAR denoising, might tempt\nus to use simulated SAR data for training and validation\nof neural networks. However, this bears the risk that our\nnetworks will learn models that are far too simpliﬁed.\nUnlike in the optical imaging ﬁeld, where highly realistic\nscenes can be simulated, e.g. by PC games, the simulation\nof SAR data is more a scientiﬁc topic without the power\nof commercial companies and a huge market. SAR sim-\nulators focus on speciﬁc scenarios, e.g. vegetation (only\ndistributed scatterers considered) or persistent (point)\nscatterers. The most advanced simulators are probably the\nones for computing radar backscatter signatures of single\nmilitary objects, like vessels. To our knowledge though\nthere is no simulator available that can , e.g., generate\nrealistic interferometric data of rugged terrain with lay-\nover, spatially varying coherence, and diverse scattering\nmechanisms. Often simpliﬁed scattering assumptions are\nmade, e.g. that speckle is multiplicative. Even this is not\ntrue; pure Gaussian scattering can only be found for quite\nhomogeneous surfaces and low resolution SARs. As soon\nas the resolution increases chances for a few dominating\nscatterers in a resolution cell increase as well and the\nstatistics become substantially different from the one of\nfully developed speckle\nIV. RECENT ADVANCES IN DEEP LEARNING APPLIED TO\nSAR\nIn this section, we provide an in-depth review of deep\nlearning methods applied to SAR data from six perspectives:\nterrain surface classiﬁcation, object detection, parameter in-\nversion, despeckling, SAR Interferometry (InSAR), and SAR-\noptical data fusion. For each of these six applications, no-\ntable developments are stated in the chronological order, and\ntheir advantages and disadvantages are reported. Finally, each\nsubsection is concluded with a brief summary. It is worth\nmentioning that the application of deep learning to SAR image\nformation is not explicitly treated here. For SAR focusing\nwe have to distinguish between general-purpose focusing and\nimaging of objects with a priori known properties, like spar-\nsity. General-purpose algorithms produce data for applications\nlike Land use and Land cover (LULC) classiﬁcation, glacier\nmonitoring, biomass estimation or interferometry. These are\ncomplex-valued focused data that retain all the information\ncontained in the raw data. General-purpose focusing has\na well-deﬁned system model and requires a sequence of\nFast Fourier Transform (FFT)s and phasor multiplications,\ni.e. linear operations like matrix-vector multiplications. For\ndecades optimal algorithms have been developed to perform\nthese operations at highest speed and with diffraction limited\naccuracy. There is no reason why deep neural networks should\nperform better or faster that this gold standard. If we want to\nintroduce prior knowledge about the imaged objects, however,\nspecialized focusing algorithms may be beneﬁcially learned by\nneural networks. But even then it might make sense to focus\nraw data ﬁrst by a standard algorithm and apply deep learning\nfor post-processing. In [64] a CNN is trained to focus sparse\nmilitary targets. But even in this approach the raw data are\npartially focused by FFT, before entering to the CNN.\nA. Terrain Surface Classiﬁcation\nAs an important direction of SAR applications, terrain\nsurface classiﬁcation using PolSAR images is rapidly ad-\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n6\nvancing with the help of deep learning. Regarding feature\nextraction, most conventional methods rely on exploring phys-\nical scattering properties [65] and texture information [66]\nin SAR images. However, these features are mainly human-\ndesigned based on speciﬁc problems and characteristics of data\nsources. Compared to conventional methods, deep learning is\nsuperior in terrain surface classiﬁcation due to its capability of\nautomatically learning discriminative features. Moreover, deep\nlearning approaches, such as CNNs, can effectively extract not\nonly polarimetric characteristics but also spatial patterns of\nPolSAR images [6]. Some of the most notable deep learning\ntechniques for PolSAR image classiﬁcation are reviewed in\nthe following.\nXie et al. [67] ﬁrst applied deep learning to terrain surface\nclassiﬁcation using PolSAR images. They employed a stacked\nauto encoder (SAE) to automatically learn deep features from\nPolSAR data and then fed them to a softmax classiﬁer.\nRemarkable improvements in both classiﬁcation accuracy and\nvisual effect proved that this method can effectively learn\na comprehensive feature representation for classiﬁcation pur-\nposes.\nInstead of simply applying SAE, Geng et al. [70] proposed\na deep convolutional autoencoder (DCAE) for automatically\nextracting features and performing classiﬁcation. The ﬁrst\nlayer of DCAE is a hand-crafted convolutional layer, where\nﬁlters are pre-deﬁned, such as gray-level co-occurrence ma-\ntrices and Gabor ﬁlters. The second layer of DCAE performs\na scale transformation, which integrates correlated neighbor\npixels to reduce speckle. Following these two hand-crafted\nlayers, a trained SAE, which is similar to [67], is attached\nfor learning more abstract features. Tested on high-resolution\nsingle-polarization TerraSAR-X images, the method achieved\nremarkable classiﬁcation accuracy.\nBased on DCAE, Geng et al. [68] proposed a frame-\nwork, called deep supervised and contractive neural network\n(DSCNN), for SAR image classiﬁcation, which introduces\nhistogram of oriented gradient (HOG) descriptors. In addi-\ntion, a supervised penalty is designed to capture relevant\ninformation between features and labels, and a contractive\nrestriction, which can enhance local invariance, is employed\nin the following trainable autoencoder layers. An example of\napplying DSCNN on TerraSAR-X data from a small area in\nNorway is seen in Fig. 2. Compared to other algorithms, the\ncapability of DSCNN to achieve a highly accurate and noise\nfree classiﬁcation map is observed.\nIn addition to the aforementioned methods, many stud-\nies integrate SAE models with conventional classiﬁcation\nalgorithms for terrain surface classiﬁcation. Hou et al. [73]\nproposed an SAE combined with superpixel for PolSAR image\nclassiﬁcation. Multiple layers of the SAE are trained on a\npixel-by-pixel basis. Superpixels are formed based on Pauli-\ndecomposed pseudo-color images. Outputs of the SAE are\nused as features in the ﬁnal step of k-nearest neighbor clus-\ntering of superpixels. Zhang et al. [74] applied stacked sparse\nAE to PolSAR image classiﬁcation by taking into account\nlocal spatial information. Qin et al. [75] applied adaptive\nboosting of RBMs to PolSAR image classiﬁcation. Zhao et al.\n[76] proposed a discriminant DBN (DisDBN) for SAR image\nclassiﬁcation, in which discriminant features are learned by\ncombining ensemble learning with a deep belief network in\nan unsupervised manner. Moreover, taking into account that\nmost current deep learning methods aim at exploiting features\neither from polarization information or spatial information\nof PolSAR images, Gao et al. [72] proposed a dual-branch\nCNN to learn features from both perspectives for terrain\nsurface classiﬁcation. This method is built on two feature\nextraction channels: one to extract polarization features from\nthe 6-channel real matrix, and the other to extract spatial\nfeatures of a Pauli decomposition. Next the extracted features\nare combined using two parallel fully connected layers, and\nﬁnally fed to a softmax layer for classiﬁcation. The detailed\narchitecture of this network is illustrated in Fig. 3.\nDifferent variations of CNNs have been used for terrain\nsurface classiﬁcation as well. In [77], Zhou et al. ﬁrst extracted\na 6-channel covariance matrix and then fed it to a trainable\nCNN for PolSAR image classiﬁcation. Wang et al. [78]\nproposed a fully convolutional network integrated with sparse\nand low-rank subspace representations for classifying PolSAR\nimages. Chen et al. [79] improved CNN performances by\nincorporating expert knowledge of target scattering mechanism\ninterpretation and polarimetric feature mining. In a more recent\nwork [80], He et al. proposed the combination of features\nlearned from nonlinear manifold embedding and applying a\nfully convolutional network (FCN) on input PolSAR images;\nthe ﬁnal classiﬁcation was carried out in an ensemble approach\nby SVM. In [81], the authors focused on the computational\nefﬁciency of deep learning methods, proposing the use of\nlightweight 3D CNNs. They showed that classiﬁcation accu-\nracy comparable to other CNN methods was achievable while\nsigniﬁcantly reducing the number of learned parameters and\ntherefore gaining computational efﬁciency.\nApart from these single-image classiﬁcation schemes using\nCNN, the use of time series of SAR images for crop classiﬁca-\ntion has been shown in [40], [82]. The authors of both papers\nexperimented with using Recurrent Neural Network (RNN)-\nbased architectures to exploit the temporal dependency of\nmulti-temporal SAR images to improve classiﬁcation accuracy.\nA unique approach for tackling PolSAR classiﬁcation was\nrecently proposed in [52], where for the ﬁrst time the authors\nutilized an AutoML technique to ﬁnd the optimum CNN\narchitecture for each dataset. The approach takes into account\nthe complex nature of PolSAR images, is cost effective, and\nachieves high classiﬁcation accuracy [52].\nMost of the aforementioned methods rely primarily on\npreprocessing or transforming raw complex-valued data into\nfeatures in the real domain and then inputting them in a\ncommon CNN, which constrains the possibility of directly\nlearning features from raw data. To tackle this problem,\nZhang et al. [83] proposed a novel complex-valued CNN\n(CV-CNN) speciﬁcally designed to process complex values\nin PolSAR data, i.e., the off-diagonal elements of a coherency\nor covariance matrix. The CV-CNN not only takes complex\nnumbers as input but also employs complex weights and\ncomplex operations throughout different layers. A complex-\nvalued backpropagation algorithm is also developed for CV-\nCNN training. Other notable complex-valued deep learning\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n7\nFig. 2: Classiﬁcation maps obtained from a TerraSAR-X image of a small area in Norway [68]. Subﬁgures (a)-(f) depict\nthe results of classiﬁcation using SVM (accuracy = 78.42%), sparse representation classiﬁer (SRC) (accuracy = 85.61%),\nrandom forest (accuracy = 82.20%) [69], SAE (accuracy = 87.26%) [67], DCAE (accuracy = 94.57%) [70], contractive AE\n(accuracy = 88.74). Subﬁgures (g)-(i) show the combination of DSCNN with SVM (accuracy = 96.98%), with SRC (accuracy\n= 92.51%) [71], and with random forest (accuracy = 96.87%). Subﬁgures (j) and (k) represent the classiﬁcation results of\nDSCNN (accuracy = 97.09%) and DSCNN followed by spatial regularization (accuracy = 97.53%), which achieve higher\naccuracy than the other methods.\nFig. 3: The architecture of the dual-branch deep convolution neural network (Dual-CNN) for PolSAR image classiﬁcation,\nproposed in [72].\napproaches for classiﬁcation using PolSAR images can be\nfound in [84], [85], [86]. Different from the previously men-\ntioned works, which exploit the complex-valued nature of SAR\nimages in PolSAR image classiﬁcation, Huang et al. [87] has\nrecently proposed a novel deep learning framework called\nDeep SAR-Net for land use classiﬁcation focusing on feature\nextraction from single-pol complex SAR images. The authors\nperform a feature fusion based on spatial features learned from\nintensity images and time-frequency features extracted from\nspectral analysis of complex SAR images. Since the time-\nfrequency features are highly relevant for distinguishing differ-\nent backscattering mechanisms within SAR images, they gain\naccuracy in classifying man-made objects compared to the use\nof typical CNNs, which only focus on spatial information.\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n8\nAlthough not completely related to terrain surface classi-\nﬁcation, it is also worth mentioning that the combination of\nSAR and PolSAR images with feed-forward neural networks\nhas been extensively used for sea ice classiﬁcation. This topic\nis not treated any further in this section and the interested\nreader is referred to consult [88], [89], [90], [91], [92] for\nmore information. Similar to the polarimetric signature, In-\nSAR coherence provides information about physical scattering\nproperties. In [35] interferometric volume decorrelation is used\nas a feature for forest/non-forest mapping together with radar\nbackscatter and incidence angle. The authors used bistatic\nTanDEM-X data where temporal decorrelation can be ne-\nglected. They compared different architectures and concluded\nthat CNNs outperform random forest and U-Net [32] proved\nbest for this segmentation task.\nTo summarize, it is apparent that deep learning-based SAR\nand PolSAR classiﬁcation algorithms have advanced consid-\nerably in the past few years. Although at ﬁrst the focus was\nbased on low-rank representation learning using SAE [67] and\nits modiﬁcations [70], later research focused on a multitude of\nissues relevant to SAR imagery, such as taking into account\nspeckle [70], [68] preserving spatial structures [72] and their\ncomplex nature [83], [84], [85], [87]. It can also be seen\nthat the challenge of the scarcity of labeled data has driven\nresearchers to use semi-supervised learning algorithms [86]\nalthough weakly supervised methods for semantic annotation,\nthat has been proposed for high resolution optical data [93],\nhas not been explicitly explored for classiﬁcation tasks using\nSAR data. Furthermore, speciﬁc metric learning approaches\nto enhance class separability [94] can be adopted for SAR\nimagery in order to improve the overall classiﬁcation accuracy.\nFinally, one of machine learning’s important ﬁelds, AutoML,\na ﬁeld that had not been exploited extensively by the remote\nsensing community, has found its application for PolSAR\nimage classiﬁcation [52].\nB. Object Detection\nAlthough various characteristics distinguish SAR images\nfrom optical RGB images, the SAR object detection problem is\nstill analogous to optical image classiﬁcation and segmentation\nin the sense that feature extraction from raw data is always\nthe prior and crucial step. Hence, given success in the optical\ndomain, there is no doubt that deep learning is one of the most\npromising ways to develop the state-of-the-art SAR object\ndetection algorithms.\nThe majority of earlier works on SAR object detection\nusing deep learning consists of taking successful deep learning\nmethods for optical object detection and applying them with\nminor tweaks to military vehicle detection (MSTAR dataset;\nsee subsection V-C) or ship detection on custom datasets. Even\nsmall-sized networks are easily able to achieve more than 90%\ntest accuracy on most of these tasks.\nThe ﬁrst attempt in military vehicle detection can be found\nin [7], where Chen et al. used an unsupervised sparse autoen-\ncoder to generate convolution kernels from random patches of\na given input for a single-layer CNN, which generated features\nto train a softmax classiﬁer for classifying military targets in\nthe MSTAR dataset [96]. The experiments in [7] showed great\npotential for applying CNNs to SAR target recognition. With\nthis discovery, Chen et al. [97] proposed A-ConvNets, a simple\n5-layer CNN that was able to achieve state-of-the-art accuracy\nof about 99% on the MSTAR dataset.\nFollowing this trend, more and more authors applied CNNs\nto the MSTAR dataset [37], [98], [99]. Morgan [37] success-\nfully applied a modestly sized 3-layered CNN on MSTAR\nand building upon it Wilmanski et al. [100] investigated the\neffects of initialization and optimizer selection on ﬁnal results.\nDing et al. [98] investigated the capabilities of a CNN model\ncombined with domain-speciﬁc data augmentation techniques\n(e.g., pose synthesis and speckle adding) in SAR object detec-\ntion. Furthermore, Du et al. [99] proposed a displacement- and\nrotation-insensitive CNN, and claimed that data augmentation\non training samples is necessary and critical in the pre-\nprocessing stage.\nOn the same dataset, instead of treating CNN as an end-to-\nend model, Wagner [101] and similarly Gao [102] integrated\nCNN and SVM, by ﬁrst using a CNN to extract features, and\nthen feeding them to an SVM for ﬁnal prediction. Speciﬁcally,\nGao et al. [103] added a class of separation information\nto the cross-entropy cost function as a regularization term,\nwhich they show explicitly facilitates intra-class compactness\nand separtability, in turn improving the quality of extracted\nfeatures. More recently, Furukawa [104] proposed VersNet,\nan encoder-decoder style segmentation network, to not only\nidentify but also localize multiple objects in an input SAR im-\nage. Moreover, Zhang et al. [95] proposed an approach based\non multi-aspect image sequences as a pre-processing step. In\nthe contribution, they are taking into account backscattering\nsignals from different viewing geometries, following feature\nextraction using Gabor ﬁlters, dimensionallity reduction and\neventually feeding the results to a Bidirectional LSTM model\nfor joint recognition of targets. The ﬂowchart of this SAR ATR\nframework is illustrated in Fig. 4.\nBesides truck detection, ship detection is another tackled\nSAR object detection task. Early studies on applying deep\nlearning models to ship detection [105], [106], [107], [108],\n[109] mainly consist of two stages: ﬁrst cropping patches from\nthe whole SAR image and then identifying whether cropped\npatches belong to target objects using a CNN. Because of ﬁxed\npatch sizes these methods were not robust enough to cater for\nvariations in ship geometry, like size and shape. This problem\nwas overcome by using region-based CNNs [110], [111], with\ncreative use of skip connections and feature fusion techniques\nin later literature. For example, Li et al. [112] fuses features\nof the last three convolution layers before feeding them to a\nregion proposal network (RPN). Kang et al. [113] proposed\na contextual region based network that fuses features from\ndifferent levels. Meanwhile, to make the most use of features\nof different resolution, Jiao et al. [114] densely connected each\nlayer to its subsequent layers and fed features from all layers to\nseparate RPN to generat proposals; in the end the best proposal\nwas chosen based on an intersection-over-union score.\nIn more recent works on SAR object detection, scientists\nhave tried to explore many other interesting ideas to comple-\nment current works. Dechesne et al. [115] proposed a multitask\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n9\nFig. 4: The ﬂowchart of the multi-aspect-aware bi-directional approach for SAR ATR proposed in [95].\nnetwork that simultaneously learned to detect, classify, and\nestimate the length of ships. Mullissa et al. [116] showed\nthat CNNs can be trained directly on Complex-Valued SAR\ndata; Kazemi et al. [117] performed object classiﬁcation using\nan RNN based architecture directly on received SAR signal\ninstead of processed SAR images; and Rostami et al. [118]\nand Huang et al. [119] explored knowledge transfer or transfer\nlearning from other domains to the SAR domain for SAR\nobject detection.\nPerhaps one of the more interesting recent works in this\napplication area is building detection by Shahzad et al. [120].\nThey tackle the problem of Very High Resolution (VHR)\nSAR building detection using a FCN [121] architecture for\nfeature extraction, followed by CRF-RNN [122], which helps\ngive similar weights to neighboring pixels. This architecture\nproduced building segmentation masks with up to 93% ac-\ncuracy. An example of the detected buildings can be seen\nin Fig. 5, where the left subﬁgure is the amplitude of the\ninput TerraSAR-X image of Berlin, and the right subﬁgure\nis the predicted building mask. Another major contribution\nmade in that paper addresses the problem of lack of training\ndata by introducing an automatic annotation technique, which\nannotates the TomoSAR data using Open Street Map (OSM)\ndata.\nAs an extension of the abovementioned work, Sun et al.\n[123] tackled the problem of individual building segmentation\nin large-scale urban areas. They propose a conditional GIS-\naware network (CG-Net) that learns multi-level visual features\nand employs building footprint data to normalize these features\nfor predicting building masks. Thanks to the novel network\narchitecture and the large amounts buildings labels automat-\nically generated from an accurate DEM and GIS building\nfootprints, this network achieves F1 score of 75.08% for\nindividual building segmentation. With the predicted building\nmasks, large-scaled levels-of-detail (LoD) 1 building models\nare reconstructed with mean height error of 2.39 m.\nOverall deep learning has shown very good performance\nin existing SAR object detection tasks. There are two main\nchallenges that the algorithm designer needs to keep in mind\nwhen tackling any SAR object detection tasks. The ﬁrst is\nthe challenge of identifying characteristics of SAR imagery\nlike imaging geometry, size of objects, and speckle noise. The\nsecond and bigger challenge is the lack of good quality stan-\ndardized datasets. As we observed, the most popular dataset,\nMSTAR, is too easy for deep nets, and for ship detection,\nthe majority of authors created their datasets, which makes it\nvery hard to judge the quality of the proposed algorithms and\neven harder to compare different algorithms. An example of\ndifﬁcult to create dataset is that of a dataset for global building\ndetection. The shape, size, and style of the buildings changes\nfrom region to region quite drastically, and so a good dataset\nfor this purpose requires training examples of buildings from\naround the world which needs quite a big effort to do high\nquality annotation of enough number of buildings such that\ndeep nets can learn something from them.\nC. Parameter Inversion\nParameter inversion from SAR images is a challenging\nﬁeld in SAR applications. As one important branch, ice\nconcentration estimation is now attracting great attention due\nto its importance to ice monitoring and climate research\n[124]. Since there are complex interactions between SAR\nsignals and sea ice [125], empirical algorithms face difﬁculties\nwith interpreting SAR images for accurate ice concentration\nestimation.\nWang et al. [8] resorted to a CNN for generating ice\nconcentration maps from dual polarized SAR images. Their\nmethod takes image patches of the intensity-scaled dual band\nSAR images as inputs, and outputs ice concentration directly.\nIn [126], [127], Wang et al. employed various CNN models\nto estimate ice concentration from SAR images during the\nmelt season. Labels are produced by ice experts via visual\ninterpretation. The algorithm was tested on dual-pol RadarSat-\n2 data. Since the problem considered is the regression of\na continuous value, mean squared error is selected as the\nloss function. Experimental results demonstrate that CNNs\ncan offer a more accurate result than comparative operational\nproducts.\nIn a different application, Song et al. [130] used a deep\nCNN, including ﬁve pairs of convolutional and max pooling\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n10\nFig. 5: Very high resolution TerraSAR-X image of Berlin (left), and the predicted building mask [120] (right).\nFig. 6: The Architecture of CNN for SAR image despeckling [60].\nlayers followed by two fully connected layers for inverting\nrough surface parameters from SAR images. The training of\nthe network was based on simulated data solely due to the\nscarcity of real training data. The method was able to invert\nthe desired parameters with a reasonable accuracy and the\nauthors showed that training a CNN for parameter inversion\npurposes could be done quite efﬁciently. Furthermore, Zhao\net al. [131] designed a complex-valued CNN to directly\nlearn physical scattering signatures from PolSAR images. The\nauthors have notably proposed a framework to automatically\ngenerate labeled data, which led to a supervised learning\nalgorithm for the aforementioned parameter inversion. The\napproach is similar to the study presented in [132], where the\nauthors used deep learning for SAR image colorization and\nlearning a full polarimteric SAR image from single-pol data.\nAnother interesting application of deep learning in parameter\ninversion has been recently published in [133]. The authors\npropose a deep neural network architecture containing a CNN\nand a GAN to automatically learn SAR image simulation\nparameters from a small number of real SAR images. They\nlater feed the learned parameters to a SAR simulator such\nas RaySAR [134] to generate a wide variety of simulated\nSAR images, which can increase training data production\nand improve the interpretation of SAR images with complex\nbackscattering scenarios.\nOn the whole, deep learning-based parameter estimation for\nSAR applications has not yet been fully exploited. Unfortu-\nnately, most of the focus of the remote sensing community\nhas been devoted to classical problems, which overlap with\ncomputer vision tasks such as classiﬁcation, object detection,\nsegmentation, and denoising. One reason for this might be\nsince parameter estimation usually requires the incorporation\nof appropriate physical models and tackles the problem at hand\nas regression rather than classiﬁcation, the domain knowledge\nis quite essential in order to apply deep learning for such\ntasks, especially for SAR images with their peculiar physi-\ncal characteristics. One interesting study [87] that has been\nalready described in detail in subsection IV-A, which designs\ndiscriminative features by spectral analysis of complex-valued\nSAR data is an important work toward including deep learning\nin parameter inversion studies using SAR data. We hope that\nin the future more studies will be carried out in this direction.\nD. Despeckling\nSpeckle, caused by the coherent interaction among scattered\nsignals from sub-resolution objects, often makes processing\nand interpretation of SAR images difﬁcult. Therefore, despeck-\nling is a crucial procedure before applying SAR images to\nvarious tasks. Conventional methods aim at removing speckle\neither spatially, where local spatial ﬁlters, such as the Lee ﬁlter\n[135], Kuan ﬁlter [136], and Frost ﬁlter [137], are employed,\nor using wavelet-based methods [138], [139], [140]. For a full\noverview of these techniques, the reader is referred to [141].\nIn the past decade, patch-based methods for speckle reduction\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n11\nFig. 7: The comparison of speckle reduction among SAR-BM3D [128], SAR-CNN [60], and CNN-NLM applied to a small strip\nof COSMO-SkyMed data over Caserta, Italy, where the reference clean image has been obtained by temporal multi-looking\napplied to a stack of SAR images [129].\nhave gained high popularity due to their ability to preserve\nspatial features while not sacriﬁcing image resolution [142].\nDeledalle et al. [143] proposed one of the ﬁrst nonlocal patch-\nbased methods applied to speckle reduction by taking into\naccount the statistical properties of speckle combined with\nthe original nonlocal image denoising algorithm introduced\nin [144]. A vast number of variations of the nonlocal method\nfor SAR despeckling has been proposed, with the most no-\ntable ones included in [145], [146]. However, on one hand,\nmanual selection of appropriate parameters for conventional\nalgorithms is not easy and is sensitive to reference images.\nOn the other hand, it is difﬁcult to achieve a balance between\npreserving distinct image features and removing artifacts with\nempirical despeckling methods. To solve these limitations,\nmethods based on deep learning have been developed.\nInspired by the success of image denoising using a residual\nlearning network architecture in the computer vision commu-\nnity [147], Chierchia et al. [60] ﬁrst introduced a residual\nlearning CNN for SAR image despeckling by presenting a\n17-layered CNN for learning to subtract speckle components\nfrom noisy images. Considering that speckle noise is assumed\nto be multiplicative, the homomorphic approach with coupled\nlog- and exp-transformations is performed before and after\nfeeding images to the network. In this case, multiplicative\nspeckle noise is transformed into an additive form and can\nbe recovered by residual learning, where log-speckle noise is\nregarded as residual. As shown in Fig. 6, an input log-noisy\nimage is mapped identically to a fusion layer via a shortcut\nconnection, and then added element-wise with the learned\nresidual image to produce a log-clean image. Afterwards,\ndenoised images can be obtained by an exp-transformation.\nWang et al. [9] proposed a CNN, called ID-CNN, for image\ndespeckling, which can directly learn denoised images via a\ncomponent-wise division-residual layer with skip connections.\nIn another words, homomorphic processing is not introduced\nfor transforming multiplicative noise into additive noise and\nat a ﬁnal stage the noisy image is divided by the learned noise\nto yield the clean image.\nAs a step forward with respect to the two aforementioned\nresidual-based learning methods, Zhang et al. [148] employed\na dilated residual network, SAR-DRN, instead of simply stack-\ning convolutional layers. Unlike [60] and similar to [9], SAR-\nDRN is trained in an end-to-end fashion using a combination\nof dilated convolutions and skip connections with a residual\nlearning structure, which indicates that prior knowledge such\nas a noise description model is not required in the workﬂow.\nIn [149], Yue et al. proposed a novel deep neural network\narchitecture speciﬁcally designed for SAR despeckling. It used\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n12\na convolutional neural network to extract image features and\nreconstruct a discrete RCS probability density function (PDF).\nIt is trained by a hybrid loss function which measures the\ndistance between the actual SAR image intensity PDF and the\nestimated one that is derived from convolution between the\nreconstructed RCS PDF and prior speckle PDF. Experimental\nresults demonstrated that the proposed despeckling neural\nnetwork can achieve comparable performance as non-learning\nstate-of-the-art methods.\nThe unique distribution of SAR intensity images was also\ntaken into account in [150]. It proposed a different loss\nfunction which contains three terms between the true and\nthe reconstructed image. They are the common L2 loss, the\nL2 difference between the gradient of the two images, and\nthe Kullback-Leibler divergence between the distribution of\nthe two images. The three terms are designed to emphasize\nthe spatial details, the identiﬁcation of strong scatterers, and\nthe speckle statistics, respectively. Experiments in [150] show\nimproved performance compared to SAR-BM3D [128] and\nSAR-DRN [148].\nIn [57], the problem of despeckling was tackled by a time\nseries of images. Using a stack of images for despeckling is not\nunique to deep learning-based methods, as has been recently\ndemonstrated in [151] as well. In [57] the authors utilized\na multi-layer perceptron with several hidden layers to learn\nnon-linear intensity characteristics of training image patches.\nThis approach has shown promising results and reported\ncomparative performance with the state-of-the-art despeckling\nalgorithms.\nAgain using single images instead of time series, in [36] the\nauthors proposed a deep encoder–decoder CNN architecture\nwith focus on feature preservation, which is a weakness of\nCNNs. They modiﬁed U-Net [32] in order to accommodate\nspeckle statistical features. Another notable CNN approach\nwas introduced in [129], where the authors used a nonlocal\nstructure, while the weights for pixel-wise similarity measures\nwere assigned using a CNN. The results of this approach,\ncalled CNN-NLM, are reported in Fig. 7, where the superiority\nof the method with respect to both feature preservation and\nspeckle reduction is clearly observed.\nOne of the drawbacks of the aforementioned algorithms\nis the requirement of noise-free and noisy image pairs for\ntraining. Often, those training data are simulated using optical\nimages with multiplicative noise. This is of course not ideal\nfor real SAR images. Therefore, one elegant solution is the\nnoise2noise framework [152], where the network only requires\ntwo noisy images of the same area. [152] proves that the\nnetwork is able to learn a clean representation of the image\ngiven the noise distributions of the two noisy images are\nindependent and identical. This idea has been employed in\nSAR despeckling in [153]. The authors make use of multi-\ntemporal SAR images of a same area as the input to the\nnoise2noise network. To mitigate the effect of the temporal\nchange between the input SAR image pairs, the authors\nmultiples a patch similarity term to the original loss function.\nFrom the deep learning-based despeckling methods re-\nviewed in this subsection, it can be observed that most methods\nemploy CNN-based architectures with single images of the\nscene for training; they either output the clean image in an\nend-to-end fashion or propose residual-based techniques to\nlearn the underlying noise model. With the availability of\nlarge archives of time series thanks to the Sentinel-1 mission,\nan interesting direction is to exploit the temporal correlation\nof speckle characteristics for despeckling applications. One\ncritical issue is over-smoothing in despeckling that needs to\nbe addressed. Many of the CNN-based methods perform well\nin terms of speckle removal but are not able to preserve sharp\nedges. This is quite problematic in despeckling of high resolu-\ntion SAR images of urban areas in particular. Another problem\nin supervised deep learning-based despeckling techniques is\nthe lack of ground truth data. In many studies, the training\ndata set is built by corrupting optical images by multiplicative\nnoise. This is far from realistic for despeckling applied to real\nSAR data. Therefore, despeckling in an unsupervised manner\nwould be highly desirable and worth attention.\nE. InSAR\nInterferometric SAR (InSAR) is one of the most important\nSAR techniques, and is widely used in reconstructing the\ntopography of the Earth’s surface, i.e., digital elevation model\n(DEM) generation [154], [155], [65], and detecting topograph-\nical displacements, e.g., monitoring volcanic eruptions [156],\n[157], [158], earthquakes [159], [160], land subsidence [161],\nand urban areas using time series methods [162], [163], [164].\nThe principle of InSAR is to ﬁrst measure the interferomet-\nric phase between signals received by two antennas located\nat different positions and then extract topographic information\nfrom the obtained interferogram by unwrapping and converting\nthe absolute phase to height. However, an actual interferogram\noften suffers from a large number of singular points, which\noriginate from the interference distortion and noise in radar\nmeasurements. These points result in unwrapping errors and\nconsequently low quality DEMs. To tackle this problem,\nIchikawa and Hirose [165] applied a complex-valued neural\nnetwork, CVNN, in the spectral domain to restore singular\npoints. With the help of the Complex Markov Random Field\n(CMRF) ﬁlter [166], they aimed at learning ideal relationships\nbetween the spectrum of neighboring pixels and that of center\npixels via a one-hidden-layer CVNN. Notably, center pixels of\neach training sample are supposed to be ideal points, which\nindicate that singular points are not fed to the network during\nthe training procedure. Similarly, Oyama and Hirose [167]\nrestored singular points with a CVNN in the spectrum domain.\nRelated to topography extraction, Costante et al. [169]\nproposed a fully CNN Encoder-Decoder architecture for es-\ntimating DEM from single-pass image acquisitions. It is\ndemonstrated that this model is capable of extracting high-\nlevel features from input radar images using an encoder\nsection and then reconstructing full resolution DEM via a\ndecoder section. Moreover, the network can potentially solve\nthe layover phenomenon in one single-look SAR image with\ncontextual features.\nIn addition to reconstructing DEMs, Schwegmann et al.\n[170] presented a CNN-based technique to detect subsidence\ndeformations from interferograms. They employed a 9-layer\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n13\nFig. 8: The workﬂow of volcano deformation detection proposed in [168]. The CNN is trained on simulated data and is later\nused to detect phase gradients and a decorrelation mask from input wrapped interferograms to locate ground deformation\ncaused by volcanoes.\nnetwork to extract salient information in interferograms and\ndisplacement maps for discriminating deformation targets from\ndeformation-like targets. Furthermore, Anantrasirichai et al.\n[10], [171], [172] used a pre-trained CNN to automatically\ndetect volcanic ground deformation from InSAR images. They\ndivided each image into patches, and relabeled them with\nbinary labels, i.e., ”background” and ”volcano”, and ﬁnally\nfed them to the network to predict volcano deformation. They\nfurther improved their method to be able to detect slow-\nmoving volcanoes by using a time series of interferograms in\n[173]. In another study related to automatic volcanic deforma-\ntion detection, Valade et al. [168] designed and trained a CNN\nfrom scratch to learn a decorrelation mask from input wrapped\ninterferograms, which then was used to detect volcanic ground\ndeformation. The ﬂowchart of this approach can be seen in\nFig. 8. The training in both of the aforementioned works [173],\n[168] was based on simulated data. Another geophysically\nmotivated example of using deep learning on InSAR data,\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n14\nwhich was actually proposed earlier than the above-mentioned\nCNN-based studies, was seen in [174], [175], [176], where the\nauthors used simple feed-forward shallow neural networks for\nseismic event characterization and automatic seismic source\nparameter inversion by exploiting the power of neural net-\nworks in solving non-linear problems.\nRecently, deep learning has been utilized for tomographic\nprocessing as well. An unfolded deep network which involves\nthe vector approximate message passing algorithms has been\nproposed in [177]. Experiments with simulated and real data\nhave been performed, which shows the spectral estimation\ngains speed up and achieves competitive performance. In\n[178], a real-valued deep neural network is applied for MIMO\nSAR 3-D imaging. It shows a better super-resolution power\ncompared with other compressive sensing-based methods.\nIn summary, it can be concluded that the use of deep\nlearning methods in InSAR is still at a very early stage.\nAlthough deep learning has been used in different applications\ncombined with InSAR, the full potential of interferograms is\nnot yet fully exploited except in the pioneering work of Hirose\n[179]. Many applications treat interferograms or deformation\nmaps obtained from interferograms as images similar to RGB\nor gray-scale ones and therefore the complex nature of in-\nterferograms has remained unnoticed. Apart from this issue,\nlike the SAR despeckling problem using deep learning, lack\nof ground truth data for either detection or image restora-\ntion problems is a motivation to focus on developing semi-\nsupervised and unsupervised algorithms that combine deep\nlearning and InSAR. Otherwise a training database consisting\nof interferograms for different scenarios and also for different\nphase contributions could be beneﬁcial for supervised learning\napplications. Simulation-based interferogram generation for\nthe latter has been recently proposed [180].\nF. SAR-Optical Data fusion\nThe fusion of SAR and optical images can provide comple-\nmentary information about targets. However, considering the\ntwo different sensing modalities, prior identiﬁcation and co-\nregistration of corresponding images are challenging [181], but\ncompulsory for joint applications of SAR and optical images.\nFor the purpose of identifying and matching SAR and optical\nimages, many current methods resort to deep learning, given\nits powerful capabilities of extracting effective features from\ncomplex images.\nIn [58], the authors proposed a CNN for identifying corre-\nsponding image patches of very high resolution (VHR) optical\nand SAR imagery of complex urban scenes. Their network\nconsists of two streams: one designed for extracting features\nfrom optical images, the other responsible for learning features\nfrom SAR images. Next the extracted features are fused via\na concatenation layer for further binary prediction of their\ncorrespondence. A selection of True Positives, False Positives,\nFalse Negatives, and True Negatives of SAR-optical image\npatches from [58] can be seen in Fig. 9. Similarly, Hughes\net al. [11] proposed a pseudo-Siamese CNN for learning a\nmulti-sensor correspondence predictor for SAR and optical\nimage patches. Notably, both networks in [58], [11] are trained\nand validated on the SARptical dataset [182], [183], which is\nspeciﬁcally built for joint analysis of VHR SAR and optical\nimages in dense urban areas.\nIn [184], the authors proposed a deep learning frame-\nwork that can learn an end-to-end mapping between image\npatch pairs and their matching labels. An image pair is ﬁrst\ntransformed into two 1-D vectors and then concatenated to\nbuild a large 1-D vector as the input of the network. Then\nhidden layers are stacked for learning the mapping between\ninput vectors and output binary labels, which indicate their\ncorrespondence.\nFor the purpose of matching SAR and optical images,\nMerkle et al. [185] presented a CNN that comprises of a\nfeature extraction stage (Siamese network) and a similarity\nmeasure stage (dot product layer). Speciﬁcally, features of\ninput optical and SAR images are extracted via two separate\n9-layer branches and then fed to a dot product layer for\npredicting the shift of the optical image within the large SAR\nreference patch. Experimental results indicate that this deep\nlearning-based method outperforms state-of-the-art matching\napproaches [186], [187]. Furthermore, Abulkhanov et al. [188]\nsuccessfully trained a neural network to build feature point\ndescriptors to identify corresponding patches among SAR and\noptical images and match the detected descriptors using the\nRANSAC algorithm [189].\nIn contrast to training a model to identify corresponding\nimage patches, Merkle et al. [190] ﬁrst employed a conditional\ngenerative adversarial network (cGAN) to generate artiﬁcial\nSAR-like images from optical images, then matched them with\nreal SAR images. The authors demonstrate that the matching\naccuracy and precision are both improved with the proposed\nstrategy. Inspired by their study, more researchers resorted to\nusing GANs for the purpose of SAR-optical image matching\n(see [191], [192] for a review).\nWith respect to applications of SAR and optical image\nmatching, Yao et al. [193] aimed at applying SAR and optical\nimages to semantic segmentation with deep neural networks.\nThey collected corresponding optical patches from Google\nEarth according to TerraSAR-X patches and built ground truths\nusing data from OpenStreetMap. Then SAR and optical images\nwere separately fed to different CNNs to predict semantic\nlabels (building, natural, land use, and water). Despite their\nexperimental results not outperforming the state of the art by\nthe time [194] likely because of network design or training\nstrategy, they deduced that introducing advanced models and\nsimultaneously using both data sources can greatly improve the\nperformance of semantic segmentation. Another application\nmentioned in [195] demonstrated that standard fusion tech-\nniques for SAR and optical images require data from both\nsources, which indicates that it is still not easy to interpret SAR\nimages without the support of optical images. To address this\nissue, Schmitt et al. [195] proposed an automatic colorization\nnetwork, composed of a VAE and a mixture density network\n(MDN) [196], to predict artiﬁcially colored SAR images (i.e.,\nSentinel-1 images). These images are proven to disclose more\ninformation to the human interpreter than the original SAR\ndata.\nIn [42], the authors tackled the problem of cloud removal\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n15\nFig. 9: Randomly selected patches obtained from the testing phase of the network for SAR-optical image patch correspondence\ndetection proposed in [11].\nfrom optical imagery. They introduced a cGAN architecture\nto fuse SAR and cloud-corrupted multi-spectral data for\ngenerating cloud- and haze-free multi-spectral optical data.\nExperiments proved the effectiveness of the proposed network\nfor removing cloud from multi-spectral data with auxiliary\nSAR data. Extending previous multi-modal networks for cloud\nremoval, [43] proposed a cycle-consistent GAN architecture\n[197] that utilizes a image forward-backward translation con-\nsistency loss. Cloud-covered optical information is recon-\nstructed via SAR data fusion, while changes to cloud-free\nareas are minimized through use of the cycle consistency loss.\nThe cycle-consistent architecture allows training without pixel-\nwise correspondences between cloudy input and cloud-free\ntarget optical imagery, relaxing requirements on the training\ndata set.\nIn summary, it can be seen that the utilization of deep\nlearning methods for SAR-optical data fusion has been a hot\ntopic in the remote sensing community. Although a handful\nof data sets consisting of optical and SAR corresponding\nimage patches are available for different terrain types and\napplications, one of the biggest problems in this task is still\nthe scarcity of high quality training data. Semi-supervised\nmethods, as proposed in [198], seems to be a viable option to\ntackle the problem. A great challenge in the SAR-optical im-\nage matching is the extreme difference in viewing geometries\nof the two sensors. For this it is important to exploit auxiliary\n3D data in order to assist the training data generation.\nV. EXISTING BENCHMARK DATASETS AND THEIR\nLIMITATIONS\nIn order to train and evaluate deep learning models, large\ndatasets are indispensable. Unlike RGB images in the com-\nputer vision community, which can be easily collected and\ninterpreted, SAR images are much more difﬁcult to annotate\ndue to their complex properties. Our research shows that big\nSAR datasets created for the primary purpose of deep learning\nresearch are nearly non-existent in the community. In recent\nyears, only a few SAR datasets have been made public for\ntraining and assessing deep learning models. In the following,\nwe categorize those datasets according to their best suited deep\nlearning problem and focus on openly accessible and well-\ncurated large datasets.\nIn particular, we consider the following categories of deep\nlearning problems in SAR.\n• Image classiﬁcation: each pixel or patch in one image\nis classiﬁed into a single label. This is often the case in\ntypical land use land cover classiﬁcation problems.\n• Scene classiﬁcation: similar to image classiﬁcation, one\nimage or patch is classiﬁed into a single label. However,\none scene is usually much larger than an image patch.\nHence, it requires a different network architecture.\n• Semantic segmentation: one image or patch is segmented\nto a classiﬁcation map of the same dimension. Training\nof such neural networks also requires densely annotated\ntraining data.\n• Object detection: similar to scene classiﬁcation. However,\ndetection often requires the estimation of the object\nlocation.\n• Registration/matching:\nprovide\nbinary\nclassiﬁcation\n(matched or unmatched), or estimate the translation\nbetween two image patches. This type of task requires\nmatching pairs of two different image patches as training\ndata.\nA. Image/Scene Classiﬁcation\n• So2Sat LCZ42 [200]: So2Sat LCZ42 follows the local\nclimate zones (LCZs) classiﬁcation scheme. The dataset\ncomprises 400,673 pairs of dual-pol Sentinel-1 and multi-\nspectral Sentinel-2 image patches from 42 urban ag-\nglomerations, plus 10 additional smaller areas, across\nﬁve continents. The image patches are hand-labelled\ninto one of the 17 LCZ classes [213]. The Sentinel-1\nimage patches in this dataset contain both the geocoded\nsingle look complex image, as well as a despeckled\nLee ﬁltered variant. In particular, it is the ﬁrst Earth\nobservation dataset that provides a quantitative measure\nof the label uncertainty, achieved by letting a group of\ndomain experts cast 10 independent votes on 19 cities\nin the dataset. The dataset therefore can be considered\na large-scale data fusion and classiﬁcation benchmark\ndataset for cutting-edge machine learning methodological\ndevelopments, such as automatic topology learning, data\nfusion, and quantiﬁcation of uncertainties.\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n16\nFig. 10: Samples of the OpenSARUrban [199]. Six classes are shown from the top to the bottom: dense and low-rise residential\nbuildings, general residential area, high-rise buildings, villas, industrial storage area, and vegetation.\n• OpenSARUrban [199]: OpenSARUrban consists of\n33,358 patches of Sentinel-1 dual-pol images covering 21\nmajor cities in China. The dataset was manually annotated\naccording to a hierarchical classiﬁcation scheme, with 10\nclasses of urban scenes at its ﬁnest level. Each image\npatch has a dimension of 100 by 100 pixels with a\npixel spacing of 10 m (Sentinel-1 GRD product). This\ndataset can support deep learning studies of urban target\ncharacterization, and content-based SAR image queries.\nFig. 10 shows some samples from the OpenSARUrban\ndataset.\nB. Semantic Segmentation/Classiﬁcation\n• SEN12MS [202]: SEN12MS was created based on its\nprevious version SEN1-2 [203]. SEN12MS consists of\n180,662 triplets of dual-pol Sentinel-1 image patches,\nmulti-spectral Sentinel-2 image patches, and MODIS land\ncover maps. The patches are georeferenced with a ground\nsampling distance of 10 m. Each image patch has a\ndimension of 256 by 256 pixels. We expect this dataset\nto support the community in developing sophisticated\ndeep learning-based approaches for common tasks such\nas scene classiﬁcation or semantic segmentation for land\ncover mapping.\n• MSAW [204]: The multi-sensor all-weather mapping\n(MSAW) dataset includes high-resolution SAR data,\nwhich covers 120 km2 in the area of Rotterdam, the\nNetherlands. The quad-polarized X-band SAR imagery\nfrom Capella Space with 0.5 m spatial resolution was\nused for the SpaceNet 6 Challenge. A total of 48,000\nunique building footprints have been labeled with addi-\ntional building heights.\n• PolSF [205]: This dataset consists of PolSAR im-\nages of San Francisco from eight different sensors,\nincluding AIRSAR, ALOS-1, ALOS-2, RADARSAT-\n2, SENTINEL-1A, SENTINEL-1B, GAOFEN-3, and\nRISAT (data compiled by E. Pottier of IETR). Five of\nthe eight images were densely labeled to ﬁve or six land\nuse land cover classes in [205]. These densely annotated\nimages correspond to roughly 3,000 training patches of\n128 by 128 pixels. Although the data volume is relatively\nlow for deep learning research, this dataset is the only\nannotated multi-sensory PolSAR dataset, to the best of\nour knowledge. Therefore, we suggest that the creator of\nthis dataset increase the number of annotated images to\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n17\nTABLE I: Summary of available open SAR datasets\nName\nDescription\nSuitable tasks\nRelated work\nSo2Sat LCZ421 [200],\nTensorFlow API2\n400,673 pairs of corresponding Sentinel-1 dual-pol image patch, Sentinel-2\nmultispectral image patch, and manually labeled local climate zones classes\nover 42 urban agglomerations (plus 10 additional smaller areas) across the\nglobe. It is the ﬁrst EO dataset that provides a quantitative measure of the label\nuncertainty, achieved by having a group of domain experts cast 10 independent\nvotes on 19 cities in the dataset.\nimage classiﬁcation,\ndata fusion,\nquantiﬁcation of uncer-\ntainties\n[201]\nOpenSARUrban3 [199]\n33,358 Sentinel-1 dual-pol images patches covering 21 major cities in China,\nlabeled with 10 classes of urban scenes.\nimage classiﬁcation\nSEN12MS4 [202]\n180,748 corresponding image triplets containing Sentinel-1 dual-pol SAR\ndata, Sentinel-2 multi-spectral imagery, and MODIS-derived land cover maps,\ncovering all inhabited continents during all meteorological seasons.\nimage classiﬁcation,\nsemantic segmentation,\ndata fusion\n[203]\nMSAW5 [204]\nquad-pol X-band SAR imagery from Capella Space with 0.5 m spatial reso-\nlution, which covers 120 km2 in the area of Rotterdam, the Netherlands. A\ntotal number of 48,000 unique building footprints are labeled with associated\nheight information curated from the 3D Basis registratie Adressen en Gebouwen\n(3DBAG) dataset.\nsemantic segmentation\nPolSF, Data6,\nLabel7 [205]\nThe dataset includes PolSAR images of San Francisco from ﬁve different\nsensors. Each image was densely labeled to ﬁve or six classes, such as\nmountain, water, high-density urban, low-density urban, vegetation, developed,\nand bare soil.\nimage classiﬁcation,\nsemantic segmentation\ndata fusion\n[206]\nMSTAR8 [207]\n17,658 X-band very high resolution SAR images chips (patches) of 10 classes\nof different vehicles plus one class of simple geometric shaped target. SAR\nimages of pure clutter are also included in the dataset.\nobject detection,\nscene classiﬁcation\n[97] [98] [208]\nOpenSARShip 2.09 [209]\n34,528 Sentinel-1 SAR image chips of ships with the ship geometric infor-\nmation, the ship type, and the corresponding automatic identiﬁcation system\n(AIS) information.\nobject detection,\nscene classiﬁcation\n[210]\nSAR-Ship-Dataset10 [211]\n43,819 Gaofen-3 or Sentinel-1 image chips of different ships. Each image chip\nhas a dimension of 256 by 256 pixels in range and azimuth.\nobject detection, scene\nclassiﬁcation\nSARptical11 [212]\n10,108 coregistered pairs of TerraSAR-X very high resolution spotlight image\npatch and UltraCAM aerial RGB image patch in Berlin, Germany. The\ncoregistration is deﬁned by the matching of the 3D position of the center of\nthe image pair.\nimage matching\n[11], [183]\nSEN1-212 [203]\n282,384 pairs of corresponding Sentinel-1 single polarization intensity, and\nSentinel-2 RGB image patches, collected across the globe. The patches are of\ndimension 256 by 256 pixels.\nimage matching\ndata fusion\n[202]\nenable greater potential use of this dataset.\nC. Object Detection\n• MSTAR [207]: The Moving and Stationary Target Ac-\nquisition and Recognition (MSTAR) dataset is one of\nthe earliest datasets for SAR target recognition. The\ndataset consists of total 17,658 X-band SAR image chips\n(patches) of 10 classes of vehicle plus one class of\nsimple geometric shaped target. The collected SAR image\npatches are 128 by 128 pixels with a resolution of one\nfoot in range and azimuth. In addition, 100 SAR images\nof clutter were also provided in the dataset.\nIn our opinion, the number of image patches in this\ndataset is relatively low for deep learning models, espe-\ncially considering the number of classes. In addition, this\ndataset represents a rather ideal and unrealistic scenario:\nvehicles in the dataset are centered in the patch, and\nthe clutter is quite homogeneous without disturbing sig-\nnals. However, considering the scarcity of such datasets,\nMSTAR is a valuable source for target recognition.\n• OpenSARShip 2.0 [209]: This dataset was built based\non its previous version, OpenSARShip [210]. It contains\n34,528 Sentinel-1 SAR image patches of different ships\nwith automatic identiﬁcation system (AIS) information.\nFor each SAR image patch, the creators manually ex-\ntracted the ship length, width, and direction, as well as its\ntype by verifying this data on the Marine Trafﬁc website\n[209]. Among all the patches, about one-third is extracted\nfrom Sentinel-1 GRD products, and the other two-thirds\nare from Sentinel-1 SLC products. OpenSARShip 2.0 is\none of the handful of SAR datasets suitable for object\ndetection.\n• SAR-Ship-Dataset [211]: This dataset was created using\n102 Gaofen-3 and 108 Sentinel-1 images. It consists\nof 43,819 ship chips of 256 pixels in both range and\nazimuth. These ships mainly have distinct scales and\nbackgrounds. Therefore, this dataset can be employed for\ndeveloping multi-scale object detection models.\n• FUSAR-Ship [214]: This dataset was created using\nspace-time matched-up datasets of Gaofen-3 SAR images\nand ship AIS messages. It consists of over 5000 ship chips\nwith corresponding ship information extracted from AIS\nmessages, which can be used to trace back to each unique\nship of any particular chip.\n• AIR-SARShip-1.0/2.0 [215]: This dataset comprises 31\n(300) SAR images from the Geofen-3 satellite, which\nincludes 1m and 3m resolution imagery with different\nimaging modes, such as spotlight and stripmap. There are\nmore than ten object categories including ships, tankers,\nﬁshing boats and others. The scene types in the dataset\ninclude ports, islands, reefs and sea surfaces of different\nlevels.\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n18\nD. Registration/Matching\n• SARptical [212], [183]: The SARptical dataset was de-\nsigned for interpreting VHR spaceborne SAR images of\ndense urban areas. This dataset consists of 10,108 pairs\nof corresponding very high resolution SAR and optical\nimage patches, whose location is precisely coregistered in\n3D. They are extracted from TerraSAR-X VHR spotlight\nimages with resolution better than 1 m and UltraCAM\naerial optical images of 20 cm pixel spacing, respec-\ntively. Unlike low and medium resolution images, high\nresolution SAR and optical images in dense urban areas\nhave very distinct geometries. Therefore, in the SARptical\ndataset, the center points of each image pair are matched\nin 3D space via sophisticated 3D reconstruction and\nmatching algorithms. The UTM coordinates of the center\npixel of each pair are also made available publicly in the\ndataset. This dataset contributes to applications of multi-\nmodal data classiﬁcation, and SAR optical images co-\nregistering. However, we believe more training samples\nare required for learning complicated SAR optical image\nto image mapping.\n• SEN1-2 [203]: The SEN1-2 dataset consists of 282,384\npairs of corresponding Sentinel-1 single polarization in-\ntensity and Sentinel-2 RGB image patches, collected\nfrom across the globe and throughout all meteorological\nseasons. The patches are of dimension 256 by 256 pixels.\nTheir distribution over the four seasons is roughly even.\nSEN1-2 is the ﬁrst large open dataset of this kind. We\nbelieve it will support further developments in the ﬁeld of\ndeep learning for remote sensing as well as multi-sensor\ndata fusion, such as SAR image colorization, and SAR-\noptical image matching.\nE. Other Datasets\n• Sample PolSAR images from ESA: https://earth.esa.\nint/web/polsarpro/data-sources/sample-datasets. For ex-\nample, the Flevoland PolSAR Dataset. Several works\nmake use of this dataset for agricultural land use land\ncover classiﬁcation. The authors of [216], [217], [218]\nhave manually labeled the dataset according to different\nclassiﬁcation schemes.\n• SAR Image Land Cover Datasets [219]: This dataset\nis not publicly available. Please contact the creator.\n• Airbus Ship Detection Challenge: https://www.kaggle.\ncom/c/airbus-ship-detection.\n1https://doi.org/10.14459/2018mp1483140\n2https://www.tensorﬂow.org/datasets/catalog/so2sat\n3https://doi.org/10.21227/3sz0-dp26\n4https://mediatum.ub.tum.de/1474000\n5https://spacenet.ai/sn6-challenge/\n6https://www.ietr.fr/polsarpro-bio/san-francisco/\n7https://github.com/liuxuvip/PolSF\n8https://www.sdms.afrl.af.mil/index.php?collection=mstar\n9http://opensar.sjtu.edu.cn/Data/Search\n10https://github.com/CAESAR-Radi/SAR-Ship-Dataset\n11https://www.sipeo.bgu.tum.de/downloads/SARptical data.zip\n12https://mediatum.ub.tum.de/1436631\nVI. CONCLUSION AND FUTURE TRENDS\nThis paper reviews the current state-of-the-art of an im-\nportant and under-exploited research ﬁeld — deep learning\nin SAR. Relevant deep learning models are introduced, and\ntheir applications in six application ﬁelds — terrain surface\nclassiﬁcation, object detection, parameter inversion, despeck-\nling, InSAR, and SAR-optical data fusion — are analyzed in\ndepth. Exisiting benchmark datasets and their limitations are\ndiscussed. In summary, despite early successes, full exploita-\ntion of deep learning in SAR is mostly limited by 1) the lack of\nlarge and representative benchmark datasets and 2) the defect\nof tailored deep learning models that make full consideration\nof SAR signal characteristics.\nLooking forward, the years ahead will be exciting. Next\ngeneration spaceborne SAR missions will simultaneously pro-\nvide high resolution and global coverage, which will enable\nnovel applications such as monitoring the dynamic Earth.\nTo retrieve geo-parameters from these data, development of\nnew analytics methods are warranted. Deep learning is among\nthe most promising methods. To fully unlock its potential in\nSAR/InSAR applications in this big SAR data era, there are\nseveral promising future directions:\n• Large and Representative Benchmark Datasets: As\nsummarized in this article, there is only a handful of SAR\nbenchmarks, in particular when excluding multi-modal\nones. For instance, in SAR target detection, methods are\nmainly tested on a single benchmark data set — the\nMSTAR dataset, where only several thousands of target\nsamples in total (several hundreds for each class) are\nprovided for training. With respect to InSAR, due to\nthe lack of ground truth, datasets are extremely deﬁcient\nor nearly nonexistent. Large and representative expert-\nannotated benchmark datasets are in high demand in the\nSAR community, and deserve more attention.\n• Unsupervised Deep Learning: To bypass the deﬁcien-\ncies in annotated data in SAR, unsupervised deep learning\nis a promising direction. These algorithms derive insights\ndirectly from the data itself, and work as feature learning,\nrepresentation learning, or clustering, which could be\nfurther used for data-driven analytics. Autoencoders and\ntheir extensions, such as variational autoencoders (VAEs)\nand deep embedded clustering algorithms, are popular\nchoices. With respect to denoising, in despeckling, the\nhigh complexity of SAR images and lack of ground truth\nmake it infeasible to produce appropriate benchmarks\nfrom real data. Noise2Noise [152] is an elegant exam-\nple of unsupervised denoising where the authors learn\ndenoised data without clean data. Despite the nice visual\nappearance of the results, preserving details is a must for\nSAR applications.\n• Interferometric Data Processing: Since deep learning\nmethods are initially applied to perception tasks in com-\nputer vision, many methods resort to transforming SAR\nimages, e.g., PolSAR images, into RGB-like images in\nadvance or focus only on intensities. In other words,\nthe most essential component of a SAR measurement —\nthe phase information — is not appropriately considered.\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n19\nAlthough CV-CNNs are capable of learning phase infor-\nmation and show great potential in processing CV-SAR\nimages, only a few such attempts have been made [83].\nExtending CNN to complex domain, while being able to\npreserve the precious phase information, would enable\nnetworks to directly learn features from raw data, and\nwould open up a wide range of SAR/InSAR applications.\n• Quantiﬁcation of Uncertainties: Generally speaking,\ngeo-parameter estimates without uncertainty measures\nare considered invalid in remote sensing. Appropriately\ntrained deep learning models can achieve highly accu-\nrate predictions. Yet, they fail in quantifying the un-\ncertainty of these predictions. Here, giving a statement\nabout the predictive uncertainty, while considering both\naleatoric uncertainty and epistemic uncertainty, is of cru-\ncial importance. The Bayesian deep learning community\nhas developed a model-agnostic and easy-to-implement\nmethodology to estimate both data and model uncertainty\nwithin deep learning models [220], which are awaiting\nexploration by the SAR community.\n• Large Scale Nonlinear Optimization Problems: The\ndevelopment of inversion algorithms should keep up the\npace of data growth. Fast solvers are demanded for\nmany advanced parameter inversion models, which often\ninvolve non-convex, nonlinear, and complex-valued op-\ntimization problems, such as compressive-sensing-based\ntomographic inversion, or low rank complex tensor de-\ncomposition for InSAR time series data analysis. In some\ncases, the iterations of the optimization algorithms per-\nform similar computations as layers in neural networks,\nthat is, a linear step followed by a non-linear activation\n(see for example, the iteratively reweighted least-squares\napproach). And it is thus meaningful to replace the\ncomputationally expensive optimization algorithms with\nunrolled deep architectures that could be trained from\nsimulated data [221].\n• Cognitive Sensors: Radars –– and SARs in particular –\n– are very complex and versatile imaging machines. A\nvariety of modes (stripmap, spotlight, ScanSAR, TOPS,\netc.), swath-widths, incidence angles and polarizations\ncan be programmed in near real-time. Cognitive radars go\na giant step further; they adapt their operational modes\nautonomously to the environment to be imaged by an in-\ntelligent interplay of transmit waveforms, adaptive signal\nprocessing on the receiver side and learning. Cognitive\nSARs are still in their conceptual and experimental phase\nand are often justiﬁed by the stunning capabilities of\nthe echo-location system of bats. In his early pioneering\narticle [222] Haykin deﬁnes three ingredients of a cogni-\ntive radar: “1) intelligent signal processing, which builds\non learning through interactions of the radar with the\nsurrounding environment; 2) feedback from the receiver\nto the transmitter, which is a facilitator of intelligence;\nand 3) preservation of the information content of radar\nreturns, which is realized by the Bayesian approach to\ntarget detection through tracking.” Such a SAR could,\ne.g., perform a low resolution, yet wide swath, surveil-\nlance of a coastal area and in a ﬁrst step detect objects of\ninterest, like ships, in real-time. Based on these detections\nthe transmit waveform can be modiﬁed such as to zoom\ninto the region of interest and allow for a close-up look\nof the object and possibly classify or even identify it.\nReinforcement (online) learning is part of the concept as\nwell as fast and reliable detectors or classiﬁers (trained\nofﬂine), e.g. based on deep learning. All this is edge\ncomputing; the learning algorithms have to perform in\nreal-time and with the limited compute resources onboard\nthe satellite or airplane.\nLast but not least, technology advances in deep learning in\nremote sensing would only be possible if experts in remote\nsensing and machine learning work closely together. This is\nparticularly true when it comes to SAR. Thus, we encourage\nmore joint initiatives working collaboratively toward deep\nlearning powered, explainable and reproducible big SAR data\nanalytics.\nREFERENCES\n[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.\n521, no. 7553, pp. 436–444, 2015.\n[2] K. Simonyan and A. Zisserman, “Very deep convolutional networks\nfor large-scale image recognition,” arXiv:1409.1556, 2014.\n[3] Z.-Q. Zhao, P. Zheng, S.-T. Xu, and X. Wu, “Object detection with\ndeep learning: A review,” IEEE Transactions on Neural Networks and\nLearning Systems, vol. 30, no. 11, pp. 3212–3232, 2019.\n[4] Y. Guo, Y. Liu, T. Georgiou, and M. S. Lew, “A review of semantic\nsegmentation using deep neural networks,” International Journal of\nMultimedia Information Retrieval, vol. 7, no. 2, pp. 87–93, 2018.\n[5] X. X. Zhu, D. Tuia, L. Mou, G. Xia, L. Zhang, F. Xu, and F. Fraundor-\nfer, “Deep learning in remote sensing: A comprehensive review and list\nof resources,” IEEE Geoscience and Remote Sensing Magazine, vol. 5,\nno. 4, pp. 8–36, 2017.\n[6] H. Parikh, S. Patel, and V. Patel, “Classiﬁcation of SAR and PolSAR\nimages using deep learning: a review,” International Journal of Image\nand Data Fusion, vol. 11, no. 1, pp. 1–32, 2020.\n[7] S. Chen and H. Wang, “SAR target recognition based on deep learning,”\nin International Conference on Data Science and Advanced Analytics\n(DSAA), 2014.\n[8] L. Wang, A. Scott, L. Xu, and D. Clausi, “Ice concentration estimation\nfrom dual-polarized SAR images using deep convolutional neural\nnetworks,” IEEE Transactions on Geoscience and Remote Sensing,\n2014.\n[9] P. Wang, H. Zhang, and V. Patel, “SAR image despeckling using a\nconvolutional neural network,” IEEE Signal Processing Letters, vol. 24,\nno. 12, pp. 1763–1767, 2017.\n[10] N. Anantrasirichai, J. Biggs, F. Albino, P. Hill, and D. Bull, “Appli-\ncation of machine learning to classiﬁcation of volcanic deformation\nin routinely generated InSAR data,” Journal of Geophysical Research:\nSolid Earth, 2018.\n[11] L. Hughes, M. Schmitt, L. Mou, Y. Wang, and X. X. Zhu, “Identifying\ncorresponding patches in SAR and optical images with a pseudo-\nsiamese CNN,” IEEE Geoscience and Remote Sensing Letters, vol. 15,\nno. 5, pp. 784–788, 2018.\n[12] K. Ikeuchi, T. Shakunaga, M. Wheeler, and T. Yamazaki, “Invariant\nhistograms and deformable template matching for SAR target recog-\nnition,” in Proceedings CVPR IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition.\nIEEE, 1996, pp. 100–105.\n[13] Q. Zhao and J. Principe, “Support vector machines for SAR automatic\ntarget recognition,” IEEE Transactions on Aerospace and Electronic\nSystems, vol. 37, no. 2, pp. 643–654, 2001.\n[14] M. Bryant and F. Garber, “SVM classiﬁer applied to the MSTAR public\ndata set,” in Algorithms for Synthetic Aperture Radar Imagery, 1999.\n[15] M. Ferguson, R. Ak, Y.-T. T. Lee, and K. H. Law, “Automatic\nlocalization of casting defects with convolutional neural networks,”\nin 2017 IEEE International Conference on Big Data (Big Data).\nBoston, MA: IEEE, Dec. 2017, pp. 1726–1735. [Online]. Available:\nhttp://ieeexplore.ieee.org/document/8258115/\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n20\n[16] C.\nBourez,\n“Deep\nlearning\ncourse,”\n[Accessed\nMay\n27,\n2020].\n[Online].\nAvailable:\nhttp://christopher5106.github.io/img/\ndeeplearningcourse/DL46.png\n[17] S. Panchal, “Cityscape image segmentation with tensorﬂow 2.0,”\n[Accessed May 27, 2020]. [Online]. Available: https://miro.medium.\ncom/max/2000/1*3FGS0kEAS55XmqxIXkp0mQ.png\n[18] Wikipedia, “Long short-term memory,” [Accessed May 27, 2020].\n[Online]. Available: https://upload.wikimedia.org/wikipedia/commons/\nthumb/3/3b/The LSTM cell.png/1280px-The LSTM cell.png\n[19] W. Feng, N. Guan, Y. Li, X. Zhang, and Z. Luo, “Audio visual\nspeech recognition with multimodal recurrent neural networks,” in\n2017 International Joint Conference on Neural Networks (IJCNN).\nAnchorage, AK, USA: IEEE, May 2017, pp. 681–688. [Online].\nAvailable: http://ieeexplore.ieee.org/document/7965918/\n[20] “Under the hood of the variational autoencoder (in prose and\ncode),”\n[Accessed\nMay\n27,\n2020].\n[Online].\nAvailable:\nhttp:\n//fastforwardlabs.github.io/blog-images/miriam/imgs code/vae.4.png\n[21] T.\nSilva,\n“An\nintuitive\nintroduction\nto\ngenerative\nad-\nversarial\nnetworks\n(gans),”\n[Accessed\nMay\n26,\n2020].\n[Online].\nAvailable:\nhttps://cdn-media-1.freecodecamp.org/images/\nm41LtQVUf3uk5IOYlHLpPazxI3pWDwG8VEvU\n[22] M. Zitnik, M. Agrawal, and J. Leskovec, “Modeling polypharmacy side\neffects with graph convolutional networks,” Bioinformatics, vol. 34,\nno. 13, p. 457–466, 2018.\n[23] B. Huang and K. M. Carley, “Residual or Gate? Towards Deeper\nGraph Neural Networks for Inductive Graph Representation Learning,”\narXiv:1904.08035 [cs, stat], Aug. 2019, arXiv: 1904.08035. [Online].\nAvailable: http://arxiv.org/abs/1904.08035\n[24] B. Zoph and Q. V. Le, “Neural Architecture Search with Reinforcement\nLearning,” arXiv:1611.01578 [cs], Feb. 2017, arXiv: 1611.01578.\n[Online]. Available: http://arxiv.org/abs/1611.01578\n[25] Y. LeCun, C. Cortes, and C. Burges, “Mnist handwritten digit\ndatabase,” IEEE, 2010.\n[26] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based\nlearning applied to document recognition,” Proceedings of the IEEE,\nvol. 86, no. 11, pp. 2278–2324, 1998.\n[27] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in Neural\nInformation Processing Systems, 2012.\n[28] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in 2009 IEEE conference\non computer vision and pattern recognition.\nIeee, 2009, pp. 248–255.\n[29] T. Tieleman and G. Hinton, “Lecture 6.5—RmsProp: Divide the\ngradient by a running average of its recent magnitude,” COURSERA:\nNeural Networks for Machine Learning, 2012.\n[30] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[31] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in IEEE International Conference on Computer\nVision and Pattern Recognition (CVPR), 2016.\n[32] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional net-\nworks for biomedical image segmentation,” in International Confer-\nence on Medical image computing and computer-assisted intervention.\nSpringer, 2015, pp. 234–241.\n[33] G. Huang, Z. Liu, K. Weinberger, and L. Maaten, “Densely connected\nconvolutional networks,” in IEEE International Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2017.\n[34] T. Hoeser and C. Kuenzer, “Object Detection and Image Segmentation\nwith Deep Learning on Earth Observation Data: A Review-Part I:\nEvolution and Recent Trends,” Remote Sensing, vol. 12, no. 10, p.\n1667, 2020.\n[35] A.\nMazza,\nF.\nSica,\nP.\nRizzoli,\nand\nG.\nScarpa,\n“TanDEM-X\nForest Mapping Using Convolutional Neural Networks,” Remote\nSensing, vol. 11, no. 24, p. 2980, Jan. 2019. [Online]. Available:\nhttps://www.mdpi.com/2072-4292/11/24/2980\n[36] F. Lattari, B. Gonzalez Leon, F. Asaro, A. Rucci, C. Prati, and\nM. Matteucci, “Deep learning for SAR image despeckling,” Remote\nSensing, vol. 11, no. 13, p. 1532, 2019.\n[37] D. Morgan, “Deep convolutional neural networks for ATR from SAR\nimagery,” in Algorithms for Synthetic Aperture Radar Imagery, 2015.\n[38] B. A. Pearlmutter, “Learning state space trajectories in recurrent neural\nnetworks,” Neural Computation, vol. 1, no. 2, pp. 263–269, 1989.\n[39] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[40] E. Ndikumana, D. Ho Tong Minh, N. Baghdadi, D. Courault, and\nL. Hossard, “Deep recurrent neural network for agricultural classiﬁca-\ntion using multitemporal SAR sentinel-1 for camargue, france,” Remote\nSensing, vol. 10, no. 8, p. 1217, 2018.\n[41] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in\nAdvances in neural information processing systems, 2014, pp. 2672–\n2680.\n[42] C. Grohnfeld, M. Schmitt, and X. X. Zhu, “A conditional generative ad-\nversarial network to fuse SAR and multispectral optical data for cloud\nremoval from Sentinel-2 images,” in IEEE International Geoscience\nand Remote Sensing Symposium (IGARSS), 2018.\n[43] P. Ebel, M. Schmitt, and X. Zhu, “Cloud removal in unpaired sentinel-\n2 imagery using cycle-consistent gan and sar-optical data fusion,”\nIGARSS 2020 IEEE International Geoscience and Remote Sensing\nSymposium, 2020.\n[44] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: a simple way to prevent neural networks from\noverﬁtting,” The journal of machine learning research, vol. 15, no. 1,\npp. 1929–1958, 2014.\n[45] K. Pearson, “Liii. on lines and planes of closest ﬁt to systems of points\nin space,” The London, Edinburgh, and Dublin Philosophical Magazine\nand Journal of Science, vol. 2, no. 11, pp. 559–572, 1901.\n[46] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\npreprint arXiv:1312.6114, 2013.\n[47] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski\net al., “Human-level control through deep reinforcement learning,”\nNature, vol. 518, no. 7540, pp. 529–533, 2015.\n[48] H. Mao, M. Alizadeh, I. Menache, and S. Kandula, “Resource man-\nagement with deep reinforcement learning,” in Proceedings of the 15th\nACM Workshop on Hot Topics in Networks, 2016, pp. 50–56.\n[49] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van\nDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam,\nM. Lanctot et al., “Mastering the game of go with deep neural networks\nand tree search,” nature, vol. 529, no. 7587, p. 484, 2016.\n[50] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement\nlearning,” arXiv preprint arXiv:1611.01578, 2016.\n[51] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A\nsurvey,” arXiv preprint arXiv:1808.05377, 2018.\n[52] H. Dong, B. Zou, L. Zhang, and S. Zhang, “Automatic design of\nCNNs via differentiable neural architecture search for PolSAR image\nclassiﬁcation,” IEEE Transactions on Geoscience and Remote Sensing,\npp. 1–14, 2020.\n[53] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\nconvolutional networks,” arXiv preprint arXiv:1609.02907, 2016.\n[54] B. Huang and K. M. Carley, “Residual or gate? towards deeper graph\nneural networks for inductive graph representation learning,” arXiv\npreprint arXiv, 2019.\n[55] Y. Shi, Q. Li, and X. X. Zhu, “Building segmentation through a\ngated graph convolutional neural network with deep structured feature\nembedding,” ISPRS Journal of Photogrammetry and Remote Sensing,\nvol. 159, pp. 184–197, 2020.\n[56] F. Ma, F. Gao, J. Sun, H. Zhou, and A. Hussain, “Attention graph\nconvolution network for image segmentation in big sar imagery data,”\nRemote Sensing, vol. 11, no. 21, p. 2586, 2019.\n[57] X. Tang, L. Zhang, and X. Ding, “SAR image despeckling with a\nmultilayer perceptron neural network,” International Journal of Digital\nEarth, pp. 1–21, 2018.\n[58] L. Mou, M. Schmitt, Y. Wang, and X. X. Zhu, “A CNN for the\nidentiﬁcation of corresponding patches in SAR and optical imagery\nof urban scenes,” in Urban Remote Sensing Event (JURSE), 2017.\n[59] R. Touzi, A. Lopes, and P. Bousquet, “A statistical and geometrical\nedge detector for SAR images,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 26, no. 6, pp. 764–773, 1988.\n[60] G. Chierchia, D. Cozzolino, G. Poggi, and L. Verdoliva, “SAR\nimage\ndespeckling\nthrough\nconvolutional\nneural\nnetworks,”\narXiv:1704.00275, 2017.\n[61] Y. Shi, X. X. Zhu, and R. Bamler, “Optimized parallelization of non-\nlocal means ﬁlter for image noise reduction of InSAR image,” in IEEE\nInternational Conference on Information and Automation, 2015.\n[62] X. X. Zhu, R. Bamler, M. Lachaise, F. Adam, Y. Shi, and M. Eineder,\n“Improving TanDEM-X DEMs by non-local InSAR ﬁltering,” in Eu-\nropean Conference on Synthetic Aperture Radar (EUSAR), 2014.\n[63] L. Denis, C.-A. Deledalle, and F. Tupin, “From patches to deep\nlearning: Combining self-similarity and neural networks for sar image\ndespeckling,” in IGARSS 2019 - 2019 IEEE International Geoscience\nand Remote Sensing Symposium.\nIEEE, 2019, pp. 5113–5116.\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n21\n[64] J. Gao, B. Deng, Y. Qin, H. Wang, and X. Li, “Enhanced Radar\nImaging Using a Complex-Valued Convolutional Neural Network,”\nIEEE Geoscience and Remote Sensing Letters, vol. 16, no. 1, pp. 35–\n39, 2019.\n[65] A. Moreira, P. Prats-Iraola, M. Younis, G. Krieger, I. Hajnsek, and\nK. P. Papathanassiou, “A tutorial on synthetic aperture radar,” IEEE\nGeoscience and Remote Sensing Magazine, vol. 1, no. 1, pp. 6–43,\n2013.\n[66] C. He, S. Li, Z. Liao, and M. Liao, “Texture classiﬁcation of PolSAR\ndata based on sparse coding of wavelet polarization textons,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 51, no. 8, pp.\n4576–4590, 2013.\n[67] H. Xie, S. Wang, K. Liu, S. Lin, and B. Hou, “Multilayer feature\nlearning for polarimetric synthetic radar data classiﬁcation,” in IEEE\nInternational Geoscience and Remote Sensing Symposium (IGARSS),\n2014.\n[68] J. Geng, H. Wang, J. Fan, and X. Ma, “Deep supervised and contractive\nneural network for SAR image classiﬁcation,” IEEE Transactions on\nGeoscience and Remote Sensing, vol. 55, no. 4, pp. 2442–2459, 2017.\n[69] S. Uhlmann and S. Kiranyaz, “Integrating color features in polarimetric\nSAR image classiﬁcation,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 52, no. 4, pp. 2197–2216, 2014.\n[70] J. Geng, J. Fan, H. Wang, X. Ma, B. Li, and F. Chen, “High-resolution\nSAR image classiﬁcation via deep convolutional autoencoders,” IEEE\nGeoscience and Remote Sensing Letters, vol. 12, no. 11, pp. 2351–\n2355, 2015.\n[71] B. Hou, B. Ren, G. Ju, H. Li, L. Jiao, and J. Zhao, “SAR image\nclassiﬁcation via hierarchical sparse representation and multisize patch\nfeatures,” IEEE Geoscience and Remote Sensing Letters, vol. 13, no. 1,\npp. 33–37, 2016.\n[72] F. Gao, T. Huang, J. Wang, J. Sun, A. Hussain, and E. Yang, “Dual-\nbranch deep convolution neural network for polarimetric SAR image\nclassiﬁcation,” Applied Sciences, vol. 7, no. 5, p. 447, 2017.\n[73] B. Hou, H. Kou, and L. Jiao, “Classiﬁcation of polarimetric SAR\nimages using multilayer autoencoders and superpixels,” IEEE Journal\nof Selected Topics in Applied Earth Observations and Remote Sensing,\nvol. 9, no. 7, pp. 3072–3081, 2016.\n[74] L. Zhang, W. Ma, and D. Zhang, “Stacked sparse autoencoder in\nPolSAR data classiﬁcation using local spatial information,” IEEE\nGeoscience and Remote Sensing Letters, vol. 13, no. 9, pp. 1359–1363,\n2016.\n[75] F. Qin, J. Guo, and W. Sun, “Object-oriented ensemble classiﬁcation\nfor polarimetric SAR imagery using restricted Boltzmann machines,”\nRemote Sensing Letters, vol. 8, no. 3, pp. 204–213, 2017.\n[76] Z. Zhao, L. Jiao, J. Zhao, J. Gu, and J. Zhao, “Discriminant deep\nbelief network for high-resolution SAR image classiﬁcation,” Pattern\nRecognition, vol. 61, pp. 686–701, 2017.\n[77] Y. Zhou, H. Wang, F. Xu, and Y. Jin, “Polarimetric SAR image classi-\nﬁcation using deep convolutional neural networks,” IEEE Geoscience\nand Remote Sensing Letters, vol. 13, no. 12, pp. 1935–1939, 2016.\n[78] Y. Wang, C. He, X. Liu, and M. Liao, “A hierarchical fully convolu-\ntional network integrated with sparse and low-rank subspace represen-\ntations for PolSAR imagery classiﬁcation,” Remote Sensing, vol. 10,\nno. 2, p. 342, 2018.\n[79] S. Chen and C. Tao, “PolSAR image classiﬁcation using polarimetric-\nfeature-driven deep convolutional neural network,” IEEE Geoscience\nand Remote Sensing Letters, vol. 15, no. 4, pp. 627–631, 2018.\n[80] C. He, M. Tu, D. Xiong, and M. Liao, “Nonlinear manifold learning\nintegrated with fully convolutional networks for PolSAR image classi-\nﬁcation,” Remote Sensing, vol. 12, no. 4, p. 655, 2020.\n[81] H. Dong, L. Zhang, and B. Zou, “PolSAR image classiﬁcation with\nlightweight 3d convolutional networks,” Remote Sensing, vol. 12, no. 3,\np. 396, 2020.\n[82] N. Teimouri, M. Dyrmann, and R. N. Jørgensen, “A novel spatio-\ntemporal FCN-LSTM network for recognizing various crop types using\nmulti-temporal radar images,” Remote Sensing, vol. 11, no. 8, p. 990,\n2019.\n[83] Z. Zhang, H. Wang, F. Xu, and Y. Jin, “Complex-valued convolutional\nneural network and its application in polarimetric SAR image classiﬁ-\ncation,” IEEE Transactions on Geoscience and Remote Sensing, vol. 55,\nno. 12, pp. 7177–7188, 2017.\n[84] A. G. Mullissa, C. Persello, and A. Stein, “PolSARNet: A deep\nfully convolutional network for polarimetric SAR image classiﬁcation,”\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing, vol. 12, no. 12, pp. 5300–5309, 2019.\n[85] L. Li, L. Ma, L. Jiao, F. Liu, Q. Sun, and J. Zhao, “Complex contourlet-\nCNN for polarimetric SAR image classiﬁcation,” Pattern Recognition,\np. 107110, 2019.\n[86] W. Xie, G. Ma, F. Zhao, H. Liu, and L. Zhang, “PolSAR image\nclassiﬁcation via a novel semi-supervised recurrent complex-valued\nconvolution neural network,” Neurocomputing, vol. 388, pp. 255–268,\n2020.\n[87] Z. Huang, M. Datcu, Z. Pan, and B. Lei, “Deep SAR-Net: Learning\nobjects from signals,” ISPRS Journal of Photogrammetry and Remote\nSensing, vol. 161, pp. 179–193, 2020.\n[88] R. Ressel, A. Frost, and S. Lehner, “A neural network-based classi-\nﬁcation for sea ice types on x-band SAR images,” IEEE Journal of\nSelected Topics in Applied Earth Observations and Remote Sensing,\nvol. 8, no. 7, pp. 3672–3680, 2015.\n[89] R. Ressel, S. Singha, and S. Lehner, “Neural network based automatic\nsea ice classiﬁcation for CL-pol RISAT-1 imagery,” in 2016 IEEE\nInternational Geoscience and Remote Sensing Symposium (IGARSS).\nIEEE, 2016, pp. 4835–4838.\n[90] R. Ressel, S. Singha, S. Lehner, A. Rosel, and G. Spreen, “Investigation\ninto different polarimetric features for sea ice classiﬁcation using x-\nband synthetic aperture radar,” IEEE Journal of Selected Topics in\nApplied Earth Observations and Remote Sensing, vol. 9, no. 7, pp.\n3131–3143, 2016.\n[91] S. Singha, M. Johansson, N. Hughes, S. M. Hvidegaard, and H. Skou-\nrup, “Arctic sea ice characterization using spaceborne fully polarimetric\nl-, c-, and x-band SAR with validation by airborne measurements,”\nIEEE Transactions on Geoscience and Remote Sensing, vol. 56, no. 7,\npp. 3715–3734, 2018.\n[92] N. Zakhvatkina, V. Smirnov, and I. Bychkova, “Satellite SAR data-\nbased sea ice classiﬁcation: An overview,” Geosciences, vol. 9, no. 4,\np. 152, 2019.\n[93] X. Yao, J. Han, G. Cheng, X. Qian, and L. Guo, “Semantic Annotation\nof High-Resolution Satellite Images via Weakly Supervised Learning,”\nIEEE Transactions on Geoscience and Remote Sensing, vol. 54, no. 6,\npp. 3660–3671, 2016.\n[94] G. Cheng, C. Yang, X. Yao, L. Guo, and J. Han, “When Deep Learning\nMeets Metric Learning: Remote Sensing Image Scene Classiﬁcation via\nLearning Discriminative CNNs,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 56, no. 5, pp. 2811–2821, 2018.\n[95] F. Zhang, C. Hu, Q. Yin, W. Li, H. Li, and W. Hong, “SAR target\nrecognition using the multi-aspect-aware bidirectional LSTM recurrent\nneural networks,” arXiv:1707.09875, 2017.\n[96] E. Keydel, S. Lee, and J. Moore, “MSTAR extended operating condi-\ntions: A tutorial,” in Algorithms for Synthetic Aperture Radar Imagery\nIII, 1996.\n[97] S. Chen, H. Wang, F. Xu, and Y. Jin, “Target classiﬁcation using the\ndeep convolutional networks for SAR images,” IEEE Transactions on\nGeoscience and Remote Sensing, vol. 54, no. 8, pp. 4806–4817, 2016.\n[98] J. Ding, B. Chen, H. Liu, and M. Huang, “Convolutional neural network\nwith data augmentation for SAR target recognition,” IEEE Geoscience\nand Remote Sensing Letters, vol. 13, no. 3, pp. 364–368, 2016.\n[99] K. Du, Y. Deng, R. Wang, T. Zhao, and N. Li, “SAR ATR based on\ndisplacement-and rotation-insensitive CNN,” Remote Sensing Letters,\nvol. 7, no. 9, pp. 895–904, 2016.\n[100] M. Wilmanski, C. Kreucher, and J. Lauer, “Modern approaches in deep\nlearning for SAR ATR,” in Algorithms for Synthetic Aperture Radar\nImagery, 2016.\n[101] S. Wagner, “SAR ATR by a combination of convolutional neural net-\nwork and support vector machines,” IEEE Transactions on Aerospace\nand Electronic Systems, vol. 52, no. 6, pp. 2861–2872, 2016.\n[102] F. Gao, T. Huang, J. Sun, J. Wang, A. Hussain, and E. Yang, “A new\nalgorithm for sar image target recognition based on an improved deep\nconvolutional neural network,” Cognitive Computation, vol. 11, no. 6,\npp. 809–824, 2019.\n[103] F. Gao, T. Huang, J. Wang, J. Sun, E. Yang, and A. Hussain,\n“Combining deep convolutional neural network and svm to sar image\ntarget recognition,” in IEEE International Conference on Internet of\nThings (iThings) and IEEE Green Computing and Communications\n(GreenCom) and IEEE Cyber, Physical and Social Computing (CP-\nSCom) and IEEE Smart Data (SmartData), 2017.\n[104] H. Furukawa, “Deep learning for end-to-end automatic target recogni-\ntion from synthetic aperture radar imagery,” arXiv:1801.08558, 2018.\n[105] D. Cozzolino, G. Di Martino, G. Poggi, and L. Verdoliva, “A fully\nconvolutional neural network for low-complexity single-stage ship\ndetection in Sentinel-1 SAR images,” in IEEE International Geoscience\nand Remote Sensing Symposium (IGARSS), 2017.\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n22\n[106] C. Schwegmann, W. Kleynhans, B. Salmon, L. Mdakane, and R. Meyer,\n“Very deep learning for ship discrimination in synthetic aperture\nradar imagery,” in IEEE International Geoscience and Remote Sensing\nSymposium (IGARSS), 2016.\n[107] C. Bentes, A. Frost, D. Velotto, and B. Tings, “Ship-iceberg dis-\ncrimination with convolutional neural networks in high resolution\nSAR images,” in European Conference on Synthetic Aperture Radar\n(EUSAR), 2016.\n[108] N. Ødegaard, A. Knapskog, C. Cochin, and J. Louvigne, “Classiﬁcation\nof ships using real and simulated data in a convolutional neural\nnetwork,” in IEEE Radar Conference (RadarConf), 2016.\n[109] Y. Liu, M. Zhang, P. Xu, and Z. Guo, “SAR ship detection using\nsea-land segmentation-based convolutional neural network,” in Interna-\ntional Workshop on Remote Sensing with Intelligent Processing (RSIP),\n2017.\n[110] R. Girshick, “Fast R-CNN,” arXiv:1504.08083, 2015.\n[111] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: towards real-\ntime object detection with region proposal networks,” IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, vol. 39, no. 6, pp.\n1137–1149, 2017.\n[112] J. Li, C. Qu, and J. Shao, “Ship detection in SAR images based on an\nimproved faster R-CNN,” in SAR in Big Data Era: Models, Methods\nand Applications (BIGSARDATA), 2017.\n[113] M. Kang, K. Ji, X. Leng, and Z. Lin, “Contextual region-based\nconvolutional neural network with multilayer fusion for SAR ship\ndetection,” Remote Sensing, vol. 9, no. 8, p. 860, 2017.\n[114] J. Jiao, Y. Zhang, H. Sun, X. Yang, X. Gao, W. Hong, K. Fu, and\nX. Sun, “A densely connected end-to-end neural network for multiscale\nand multiscene SAR ship detection,” IEEE Access, vol. 6, pp. 20 881–\n20 892, 2018.\n[115] C. Dechesne, S. Lef`evre, R. Vadaine, G. Hajduch, and R. Fablet,\n“Multi-task deep learning from sentinel-1 sar: ship detection, classiﬁ-\ncation and length estimation,” in Conference on Big Data from Space,\n2019.\n[116] A. G. Mullissa, C. Persello, and A. Stein, “Polsarnet: A deep fully\nconvolutional network for polarimetric sar image classiﬁcation,” IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote\nSensing, 2019.\n[117] S. Kazemi, B. Yonel, and B. Yazici, “Deep learning for direct automatic\ntarget recognition from sar data,” in 2019 IEEE Radar Conference\n(RadarConf).\nIEEE, 2019, pp. 1–6.\n[118] M. Rostami, S. Kolouri, E. Eaton, and K. Kim, “Deep transfer learning\nfor few-shot sar image classiﬁcation,” Remote Sensing, vol. 11, no. 11,\np. 1374, 2019.\n[119] Z. Huang, Z. Pan, and B. Lei, “What, where, and how to transfer\nin sar target recognition based on deep cnns,” IEEE Transactions on\nGeoscience and Remote Sensing, 2019.\n[120] M. Shahzad, M. Maurer, F. Fraundorfer, Y. Wang, and X. X. Zhu,\n“Buildings detection in VHR SAR images using fully convolution neu-\nral networks,” IEEE Transactions on Geoscience and Remote Sensing,\nvol. 57, no. 2, pp. 1100–1116, 2019.\n[121] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in IEEE International Conference on\nComputer Vision and Pattern Recognition (CVPR), 2015.\n[122] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du,\nC. Huang, and P. H. Torr, “Conditional random ﬁelds as recurrent\nneural networks,” in Proceedings of the IEEE international conference\non computer vision, 2015, pp. 1529–1537.\n[123] Y. Sun, Y. Hua, L. Mou, and X. X. Zhu, “Cg-net: Conditional gis-aware\nnetwork for individual building segmentation in vhr sar images,” arXiv\npreprint arXiv:2011.08362, 2020.\n[124] F.\nRADAR\nand\nJ.\nFALKINGHAM,\n“Global\nsatel-\nlite\nobservation\nrequirements\nfor\nﬂoating\nice.”\n[Online].\nAvailable:\nhttps://www.wmo.int/pages/prog/sat/meetings/documents/\nPSTG-4 Doc 08-04 GlobSatObsReq-FloatingIce.pdf\n[125] W. Dierking, “Sea ice monitoring by synthetic aperture radar,”\nOceanography, vol. 26, no. 2, pp. 100–111, 2013.\n[126] L. Wang, K. Scott, L. Xu, and D. Clausi, “Sea ice concentration estima-\ntion during melt from dual-pol SAR scenes using deep convolutional\nneural networks: A case study,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 54, no. 8, pp. 4524–4533, 2016.\n[127] L. Wang, “Learning to estimate sea ice concentration from SAR\nimagery,” Ph.D. dissertation, University of Waterloo, 2016. [Online].\nAvailable: http://hdl.handle.net/10012/10954\n[128] S. Parrilli, M. Poderico, C. V. Angelino, and L. Verdoliva, “A nonlocal\nSAR image denoising algorithm based on LLMMSE wavelet shrink-\nage,” IEEE Transactions on Geoscience and Remote Sensing, vol. 50,\nno. 2, pp. 606–616, 2012.\n[129] D. Cozzolino, L. Verdoliva, G. Scarpa, and G. Poggi, “Nonlocal CNN\nSAR image despeckling,” Remote Sensing, vol. 12, no. 6, p. 1006,\n2020.\n[130] T. Song, L. Kuang, L. Han, Y. Wang, and Q. H. Liu, “Inversion of\nrough surface parameters from SAR images using simulation-trained\nconvolutional neural networks,” IEEE Geoscience and Remote Sensing\nLetters, vol. 15, no. 7, pp. 1130–1134, 2018.\n[131] J. Zhao, M. Datcu, Z. Zhang, H. Xiong, and W. Yu, “Contrastive-\nregulated cnn in the complex domain: A method to learn physical\nscattering signatures from ﬂexible polsar images,” IEEE Transactions\non Geoscience and Remote Sensing, vol. 57, no. 12, pp. 10 116–10 135,\n2019.\n[132] Q. Song, F. Xu, and Y.-Q. Jin, “Radar Image Colorization: Con-\nverting Single-Polarization to Fully Polarimetric Using Deep Neural\nNetworks,” IEEE Access, vol. 6, pp. 1647–1661, 2018.\n[133] S. Niu, X. Qiu, B. Lei, C. Ding, and K. Fu, “Parameter Extraction\nBased on Deep Neural Network for SAR Target Simulation,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 58, no. 7, pp.\n4901–4914, 2020.\n[134] S. Auer, R. Bamler, and P. Reinartz, “RaySAR - 3D SAR simulator:\nNow open source,” in 2016 IEEE International Geoscience and Remote\nSensing Symposium (IGARSS). Beijing, China: IEEE, 2016, pp. 6730–\n6733.\n[135] J. Lee, “Digital image enhancement and noise ﬁltering by use of\nlocal statistics,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. PAMI-2, no. 2, pp. 165–168, 1980.\n[136] D. Kuan, A. Sawchuk, T. Strand, and P. Chavel, “Adaptive noise\nsmoothing ﬁlter for images with signal-dependent noise,” IEEE trans-\nactions on Pattern Analysis and Machine Intelligence, vol. PAMI-7,\nno. 2, pp. 165–177, 1985.\n[137] V. Frost, J. Stiles, K. Shanmugan, and J. Holtzman, “A model for radar\nimages and its application to adaptive digital ﬁltering of multiplicative\nnoise,” IEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, vol. PAMI-4, no. 2, pp. 157–166, 1982.\n[138] H. Xie, L. Pierce, and F. Ulaby, “SAR speckle reduction using wavelet\ndenoising and Markov random ﬁeld modeling,” IEEE Transactions on\nGeoscience and Remote Sensing, vol. 40, no. 10, pp. 2196–2212, 2002.\n[139] F. Argenti and L. Alparone, “Speckle removal from SAR images in the\nundecimated wavelet domain,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 40, no. 11, pp. 2363–2374, 2002.\n[140] A. Achim, P. Tsakalides, and A. Bezerianos, “SAR image denoising\nvia Bayesian wavelet shrinkage based on heavy-tailed modeling,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 41, no. 8, pp.\n1773–1784, 2003.\n[141] F. Argenti, A. Lapini, T. Bianchi, and L. Alparone, “A tutorial on\nspeckle reduction in synthetic aperture radar images,” IEEE Geoscience\nand Remote Sensing Magazine, vol. 1, no. 3, pp. 6–35, 2013.\n[142] F. Tupin, L. Denis, C.-A. Deledalle, and G. Ferraioli, “Ten years of\npatch-based approaches for sar imaging: A review,” in IGARSS 2019 -\n2019 IEEE International Geoscience and Remote Sensing Symposium.\nIEEE, 2019, pp. 5105–5108.\n[143] C.-A. Deledalle, L. Denis, and F. Tupin, “Iterative weighted maximum\nlikelihood denoising with probabilistic patch-based weights,” IEEE\nTransactions on Image Processing, vol. 18, no. 12, pp. 2661–2672,\n2009.\n[144] A. Buades, B. Coll, and J.-M. Morel, “A non-local algorithm for image\ndenoising,” in 2005 IEEE Computer Society Conference on Computer\nVision and Pattern Recognition (CVPR’05), vol. 2.\nIEEE, 2005, pp.\n60–65.\n[145] Xin Su, C.-A. Deledalle, F. Tupin, and Hong Sun, “Two-step multi-\ntemporal nonlocal means for synthetic aperture radar images,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 52, no. 10, pp.\n6181–6196, 2014.\n[146] C.-A. Deledalle, L. Denis, F. Tupin, A. Reigber, and M. Jager,\n“NL-SAR: A uniﬁed nonlocal framework for resolution-preserving\n(pol)(in)SAR denoising,” IEEE Transactions on Geoscience and Re-\nmote Sensing, vol. 53, no. 4, pp. 2021–2038, 2015.\n[147] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, “Beyond a Gaus-\nsian denoiser: Residual learning of deep CNN for image denoising,”\nIEEE Transactions on Image Processing, vol. 26, no. 7, pp. 3142–3155,\n2017.\n[148] Q. Zhang, Q. Yuan, J. Li, Z. Yang, and X. Ma, “Learning a dilated\nresidual network for SAR image despeckling,” Remote Sensing, vol. 10,\nno. 2, p. 196, 2018.\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n23\n[149] D.-X. Yue, F. Xu, and Y.-Q. Jin, “Sar despeckling neural network\nwith logarithmic convolutional product model,” International Journal\nof Remote Sensing, vol. 39, no. 21, pp. 7483–7505, 2018.\n[150] S. Vitale, G. Ferraioli, and V. Pascazio, “Multi-Objective CNN\nBased Algorithm for SAR Despeckling,” arXiv:2006.09050 [cs,\neess], Aug. 2020, arXiv: 2006.09050v4. [Online]. Available: http:\n//arxiv.org/abs/2006.09050\n[151] G. Baier, W. He, and N. Yokoya, “Robust nonlocal low-rank SAR time\nseries despeckling considering speckle correlation by total variation\nregularization,” IEEE Transactions on Geoscience and Remote Sensing,\npp. 1–13, 2020.\n[152] J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Karras, M. Aittala,\nand T. Aila, “Noise2noise: Learning image restoration without clean\ndata,” 2018.\n[153] X. Ma, C. Wang, Z. Yin, and P. Wu, “SAR Image Despeckling by\nNoisy Reference-Based Deep Learning Method,” IEEE Transactions\non Geoscience and Remote Sensing, pp. 1–12, 2020. [Online].\nAvailable: https://ieeexplore.ieee.org/document/9091002/\n[154] H. Zebker, C. Werner, P. Rosen, and S. Hensley, “Accuracy of\ntopographic maps derived from ERS-1 interferometric radar,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 32, no. 4, pp.\n823–836, 1994.\n[155] R. Abdelfattah and J. Nicolas, “Topographic SAR interferometry for-\nmulation for high-precision DEM generation,” IEEE Transactions on\nGeoscience and Remote Sensing, vol. 40, no. 11, pp. 2415–2426, 2002.\n[156] D. Massonnet, P. Briole, and A. Arnaud, “Deﬂation of mount Etna\nmonitored by spaceborne radar interferometry,” Nature, vol. 375, no.\n6532, p. 567, 1995.\n[157] J. Ruch, J. Anderssohn, T. Walter, and M. Motagh, “Caldera-scale\ninﬂation of the Lazufre volcanic area, south America: Evidence from\nInSAR,” Journal of Volcanology and Geothermal Research, vol. 174,\nno. 4, pp. 337–344, 2008.\n[158] E. Trasatti, F. Casu, C. Giunchi, S. Pepe, G. Solaro, S. Tagliaventi,\nP. Berardino, M. Manzo, A. Pepe, G. Ricciardi, E. Sansosti, P. Tizzani,\nG. Zeni, and R. Lanari, “The 2004–2006 uplift episode at Campi\nFlegrei caldera (Italy): Constraints from SBAS-DInSAR ENVISAT\ndata and Bayesian source inference,” Geophysical Research Letters,\nvol. 35, no. 7, pp. 1–6, 2008.\n[159] D. Massonnet, M. Rossi, C. Carmona, F. Adragna, G. Peltzer, K. Feigl,\nand T. Rabaute, “The displacement ﬁeld of the landers earthquake\nmapped by radar interferometry,” Nature, vol. 364, no. 6433, p. 138,\n1993.\n[160] G. Peltzer and P. Rosen, “Surface displacement of the 17 May 1993\nEureka valley, California, earthquake observed by SAR interferometry,”\nScience, vol. 268, no. 5215, pp. 1333–1336, 1995.\n[161] V. B. H. (Gini) Ketelaar, Satellite Radar Interferometry, ser. Remote\nSensing and Digital Image Processing.\nSpringer Netherlands, 2009,\nvol. 14.\n[162] X. X. Zhu and R. Bamler, “Let’s do the time warp: Multicomponent\nnonlinear motion estimation in differential SAR tomography,” IEEE\nGeoscience and Remote Sensing Letters, vol. 8, no. 4, pp. 735–739,\n2011.\n[163] S. Gernhardt and R. Bamler, “Deformation monitoring of single\nbuildings using meter-resolution SAR data in PSI,” ISPRS Journal of\nPhotogrammetry and Remote Sensing, vol. 73, pp. 68–79, 2012.\n[164] S. Montazeri, X. X. Zhu, M. Eineder, and R. Bamler, “Three-\ndimensional deformation monitoring of urban infrastructure by to-\nmographic SAR using multitrack TerraSAR-x data stacks,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 54, no. 12, pp.\n6868–6878, 2016.\n[165] K. Ichikawa and A. Hirose, “Singular unit restoration in InSAR\nusing complex-valued neural networks in the spectral domain,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 55, no. 3, pp.\n1717–1723, 2017.\n[166] R. Yamaki and A. Hirose, “Singular unit restoration in interferograms\nbased on complex-valued markov random ﬁeld model for phase un-\nwrapping,” IEEE Geoscience and Remote Sensing Letters, vol. 6, no. 1,\npp. 18–22, 2009.\n[167] K. Oyama and A. Hirose, “Adaptive phase-singular-unit restoration\nwith entire-spectrum-processing complex-valued neural networks in\ninterferometric SAR,” Electronics Letters, vol. 54, no. 1, pp. 43–44,\n2018.\n[168] S. Valade, A. Ley, F. Massimetti, O. D’Hondt, M. Laiolo, D. Coppola,\nD. Loibl, O. Hellwich, and T. R. Walter, “Towards global volcano mon-\nitoring using multisensor sentinel missions and artiﬁcial intelligence:\nThe MOUNTS monitoring system,” Remote Sensing, vol. 11, no. 13,\np. 1528, 2019.\n[169] G. Costante, T. Ciarfuglia, and F. Biondi, “Towards monocular digital\nelevation model (DEM) estimation by convolutional neural networks-\napplication on synthetic aperture radar images,” arXiv:1803.05387,\n2018.\n[170] C. Schwegmann, W. Kleynhans, J. Engelbrecht, L. Mdakane, and\nR. Meyer, “Subsidence feature discrimination using deep convolutional\nneural networks in synthetic aperture radar imagery,” in IEEE Interna-\ntional Geoscience and Remote Sensing Symposium (IGARSS), 2017.\n[171] N. Anantrasirichai, F. Albino, P. Hill, D. Bull, and J. Biggs,\n“Detecting volcano deformation in InSAR using deep learning,”\narXiv:1803.00380, 2018.\n[172] N. Anantrasirichai, J. Biggs, F. Albino, and D. Bull, “A deep learning\napproach to detecting volcano deformation from satellite imagery\nusing synthetic datasets,” Remote Sensing of Environment, vol. 230,\np. 111179, 2019.\n[173] ——, “The application of convolutional neural networks to detect slow,\nsustained deformation in InSAR time series,” Geophysical Research\nLetters, vol. 46, no. 21, pp. 11 850–11 858, 2019.\n[174] F. Del Frate, M. Picchiani, G. Schiavon, and S. Stramondo, “Neural\nnetworks and SAR interferometry for the characterization of seismic\nevents,” in Proc. SPIE, C. Notarnicola, Ed., 2010, p. 78290J.\n[175] M. Picchiani, F. Del Frate, G. Schiavon, S. Stramondo, M. Chini, and\nC. Bignami, “Neural networks for automatic seismic source analysis\nfrom DInSAR data,” in Proc. SPIE, 2011, p. 81790K.\n[176] S. Stramondo, F. Del Frate, M. Picchiani, and G. Schiavon, “Seismic\nsource quantitative parameters retrieval from InSAR data and neural\nnetworks,” IEEE Transactions on Geoscience and Remote Sensing,\nvol. 49, no. 1, pp. 96–104, 2011.\n[177] J. Gao, Y. Ye, S. Li, Y. Qin, X. Gao, and X. Li, “Fast super-resolution\n3d sar imaging using an unfolded deep network,” in 2019 IEEE\nInternational Conference on Signal, Information and Data Processing\n(ICSIDP).\nIEEE, 2019, pp. 1–5.\n[178] C. Wu, Z. Zhang, L. Chen, and W. Yu, “Super-resolution for mimo\narray sar 3-d imaging based on compressive sensing and deep neural\nnetwork,” IEEE Journal of Selected Topics in Applied Earth Observa-\ntions and Remote Sensing, vol. 13, pp. 3109–3124, 2020.\n[179] A. Hirose, Complex-Valued Neural Networks, ser. Studies in Compu-\ntational Intelligence.\nSpringer Berlin Heidelberg, 2012, vol. 400.\n[180] G. Rongier, C. Rude, T. Herring, and V. Pankratius, “Generative\nModeling of InSAR Interferograms,” Earth and Space Science, vol. 6,\nno. 12, pp. 2671–2683, 2019.\n[181] M. Schmitt and X. X. Zhu, “On the challenges in stereogrammetric\nfusion of SAR and optical imagery for urban areas,” the International\nArchives of the Photogrammetry, Remote Sensing and Spatial Informa-\ntion Sciences, vol. 41, no. B7, pp. 719–722, 2016.\n[182] Y. Wang, X. X. Zhu, S. Montazeri, J. Kang, L. Mou, and M. Schmitt,\n“Potential of the “SARptical” system,” in FRINGE, 2017.\n[183] Y. Wang and X. X. Zhu, “The SARptical dataset for joint analysis of\nSAR and optical image in dense urban area,” in IEEE International\nGeoscience and Remote Sensing Symposium (IGARSS), 2018.\n[184] S. Wang, D. Quan, X. Liang, M. Ning, Y. Guo, and L. Jiao, “A\ndeep learning framework for remote sensing image registration,” ISPRS\nJournal of Photogrammetry and Remote Sensing, 2018.\n[185] N. Merkle, W. Luo, S. Auer, R. M¨uller, and R. Urtasun, “Exploiting\ndeep matching and SAR data for the geo-localization accuracy im-\nprovement of optical satellite images,” Remote Sensing, vol. 9, no. 6,\np. 586, 2017.\n[186] S. Suri and P. Reinartz, “Mutual-information-based registration of\nTerraSAR-X and Ikonos imagery in urban areas,” IEEE Transactions\non Geoscience and Remote Sensing, vol. 48, no. 2, pp. 939–949, 2010.\n[187] F. Dellinger, J. Delon, Y. Gousseau, J. Michel, and F. Tupin, “SAR-\nSIFT: A SIFT-like algorithm for SAR images,” IEEE Transactions on\nGeoscience and Remote Sensing, vol. 53, no. 1, pp. 453–466, 2015.\n[188] D. Abulkhanov, I. Konovalenko, D. Nikolaev, A. Savchik, E. Shvets,\nand D. Sidorchuk, “Neural network-based feature point descriptors for\nregistration of optical and SAR images,” in International Conference\non Machine Vision (ICMV), 2018.\n[189] M. A. Fischler and R. C. Bolles, “Random sample consensus: a\nparadigm for model ﬁtting with applications to image analysis and\nautomated cartography,” Communications of the ACM, vol. 24, no. 6,\npp. 381–395, 1981.\n[190] N. Merkle, S. Auer, R. M¨uller, and P. Reinartz, “Exploring the\npotential of conditional adversarial networks for optical and SAR\nimage matching,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing, pp. 1–10, 2018.\n[191] L. H. Hughes, N. Merkle, T. Burgmann, S. Auer, and M. Schmitt,\n“Deep learning for SAR-optical image matching,” in IGARSS 2019 -\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n24\n2019 IEEE International Geoscience and Remote Sensing Symposium.\nIEEE, 2019, pp. 4877–4880.\n[192] M. Fuentes Reyes, S. Auer, N. Merkle, C. Henry, and M. Schmitt,\n“SAR-to-optical image translation based on conditional generative\nadversarial networks—optimization, opportunities and limits,” Remote\nSensing, vol. 11, no. 17, p. 2067, 2019.\n[193] W. Yao, D. Marmanis, and M. Datcu, “Semantic segmentation using\ndeep neural networks for SAR and optical image pairs,” 2017.\n[194] N. Audebert, B. Le Saux, and S. Lef`evre, “Semantic Segmentation\nof\nEarth\nObservation\nData\nUsing\nMultimodal\nand\nMulti-scale\nDeep Networks,” in Computer Vision – ACCV 2016, S.-H. Lai,\nV.\nLepetit,\nK.\nNishino,\nand\nY.\nSato,\nEds.\nCham:\nSpringer\nInternational Publishing, 2017, vol. 10111, pp. 180–196, series\nTitle: Lecture Notes in Computer Science. [Online]. Available:\nhttp://link.springer.com/10.1007/978-3-319-54181-5 12\n[195] M. Schmitt, L. Hughes, M. K¨orner, and X. X. Zhu, “Colorizing\nSentinel-1 SAR images using a variational autoencoder conditioned\non Sentinel-2 imagery,” International Archives of the Photogrammetry,\nRemote Sensing and Spatial Information Sciences, vol. 42, p. 2, 2018.\n[196] C. Bishop, “Mixture density networks,” Citeseer, Tech. Rep., 1994.\n[197] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-\nto-image translation using cycle-consistent adversarial networks,”\nin\n2017\nIEEE\nInternational\nConference\non\nComputer\nVision\n(ICCV).\nIEEE,\nOct\n2017,\np.\n2242–2251.\n[Online].\nAvailable:\nhttp://ieeexplore.ieee.org/document/8237506/\n[198] L. H. Hughes and M. Schmitt, “A SEMI-SUPERVISED APPROACH\nTO SAR-OPTICAL IMAGE MATCHING,” ISPRS Annals of Pho-\ntogrammetry, Remote Sensing and Spatial Information Sciences, vol.\nIV-2/W7, pp. 71–78, 2019.\n[199] J. Zhao, Z. Zhang, W. Yao, M. Datcu, H. Xiong, and W. Yu,\n“OpenSARUrban: A Sentinel-1 SAR Image Dataset for Urban\nInterpretation,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing, vol. 13, pp. 187–203, 2020.\n[Online]. Available: https://ieeexplore.ieee.org/document/8952866/\n[200] X. Zhu, J. Hu, C. Qiu, Y. Shi, J. Kang, L. Mou, H. Bagheri, M. H¨aberle,\nY. Hua, R. Huang, L. D. Hughes, H. Li, Y. Sun, G. Zhang, S. Han,\nM. Schmitt, and Y. Wang, “So2Sat LCZ42: A benchmark dataset for\nglobal local climate zones classiﬁcation,” IEEE Geoscience and Remote\nSensing Magazine, vol. in press, 2020.\n[201] M.\nNeumann,\nA.\nS.\nPinto,\nX.\nZhai,\nand\nN.\nHoulsby,\n“In-\ndomain representation learning for remote sensing,” arXiv:1911.06721\n[cs],\nNov.\n2019,\narXiv:\n1911.06721.\n[Online].\nAvailable:\nhttp:\n//arxiv.org/abs/1911.06721\n[202] M.\nSchmitt,\nL.\nH.\nHughes,\nC.\nQiu,\nand\nX.\nX.\nZhu,\n“SEN12MS - A CURATED DATASET OF GEOREFERENCED\nMULTI-SPECTRAL\nSENTINEL-1/2\nIMAGERY\nFOR\nDEEP\nLEARNING\nAND\nDATA\nFUSION,”\nISPRS\nAnnals\nof\nPhotogrammetry, Remote Sensing and Spatial Information Sciences,\nvol.\nIV-2/W7,\npp.\n153–160,\nSep.\n2019.\n[Online].\nAvail-\nable:\nhttps://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.\nnet/IV-2-W7/153/2019/\n[203] M. Schmitt, L. H. Hughes, and X. X. Zhu, “The SEN1-2 dataset for\ndeep learning in SAR-Optical data fusion,” in ISPRS Annals of the\nPhotogrammetry, Remote Sensing and Spatial Information Sciences,\n2018.\n[204] J. Shermeyer, D. Hogan, J. Brown, A. Van Etten, N. Weir, F. Paci-\nﬁci, R. Haensch, A. Bastidas, S. Soenen, T. Bacastow et al.,\n“Spacenet 6: Multi-sensor all weather mapping dataset,” arXiv preprint\narXiv:2004.06500, 2020.\n[205] X. Liu, L. Jiao, and F. Liu, “PolSF: Polsar image dataset on san\nfrancisco,” arXiv preprint arXiv:1912.07259, 2019.\n[206] Y. Cao, Y. Wu, P. Zhang, W. Liang, and M. Li, “Pixel-wise polsar image\nclassiﬁcation via a novel complex-valued deep fully convolutional\nnetwork,” Remote Sensing, vol. 11, no. 22, p. 2653, 2019.\n[207] T. Ross, S. Worrell, V. Velten, J. Mossing, and M. Bryant, “Standard\nSAR ATR evaluation experiments using the MSTAR public release data\nset,” in Algorithms for Synthetic Aperture Radar Imagery, 1998.\n[208] F. Gao, Y. Yang, J. Wang, J. Sun, E. Yang, and H. Zhou, “A deep\nconvolutional generative adversarial networks (dcgans)-based semi-\nsupervised method for object recognition in synthetic aperture radar\n(sar) images,” Remote Sensing, vol. 10, no. 6, p. 846, 2018.\n[209] B. Li, B. Liu, L. Huang, W. Guo, Z. Zhang, and W. Yu, “OpenSARShip\n2.0: A large-volume dataset for deeper interpretation of ship targets in\nSentinel-1 imagery,” in 2017 SAR in Big Data Era: Models, Methods\nand Applications (BIGSARDATA).\nBeijing: IEEE, Nov. 2017, pp.\n1–5. [Online]. Available: http://ieeexplore.ieee.org/document/8124929/\n[210] L. Huang, B. Liu, B. Li, W. Guo, W. Yu, Z. Zhang, and W. Yu, “Open-\nSARShip: A Dataset Dedicated to Sentinel-1 Ship Interpretation,” IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote\nSensing, vol. 11, no. 1, pp. 195–208, Jan. 2018, conference Name:\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing.\n[211] Y. Wang, C. Wang, H. Zhang, Y. Dong, and S. Wei, “A SAR Dataset\nof Ship Detection for Deep Learning under Complex Backgrounds,”\nRemote Sensing, vol. 11, no. 7, p. 765, Mar. 2019. [Online]. Available:\nhttps://www.mdpi.com/2072-4292/11/7/765\n[212] Y. Wang, X. X. Zhu, B. Zeisl, and M. Pollefeys, “Fusing Meter-\nResolution 4-D InSAR Point Clouds and Optical Images for Semantic\nUrban Infrastructure Monitoring,” IEEE Transactions on Geoscience\nand Remote Sensing, vol. 55, no. 1, pp. 14–26, Jan. 2017, 00002.\n[213] I. D. Stewart and T. R. Oke, “Local climate zones for urban\ntemperature\nstudies,”\nBulletin\nof\nthe\nAmerican\nMeteorological\nSociety, vol. 93, no. 12, pp. 1879–1900, 2012. [Online]. Available:\nhttp://journals.ametsoc.org/doi/abs/10.1175/BAMS-D-11-00019.1\n[214] H. Xiyue, W. Ao, Q. Song, J. Lai, H. Wang, and F. Xu, “Fusar-ship: a\nhigh-resolution sar-ais matchup dataset of gaofen-3 for ship detection\nand recognition,” SCIENCE CHINA Information Sciences, 2020.\n[215] S. Xian, W. Zhirui, S. Yuanrui, D. Wenhui, Z. Yue, and F. Kun, “Air-\nsarship–1.0: High resolution sar ship detection dataset,” J. Radars,\nvol. 8, no. 6, pp. 852–862, 2019.\n[216] P. Yu, A. Qin, and D. Clausi, “Unsupervised polarimetric SAR im-\nage segmentation and classiﬁcation using region growing with edge\npenalty,” IEEE Transactions on Geoscience and Remote Sensing,\nvol. 50, no. 4, pp. 1302–1317, 2012.\n[217] D. Hoekman and M. Vissers, “A new polarimetric classiﬁcation ap-\nproach evaluated for agricultural crops,” IEEE Transactions on Geo-\nscience and Remote Sensing, vol. 41, no. 12, pp. 2881–2889, 2003.\n[218] W. Yang, D. Dai, J. Wu, and C. He, Weakly supervised polarimetric\nSAR image classiﬁcation with multi-modal Markov aspect model.\nISPRS, 2010.\n[219] C. O. Dumitru, G. Schwarz, and M. Datcu, “SAR Image Land Cover\nDatasets for Classiﬁcation Benchmarking of Temporal Changes,” IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote\nSensing, vol. 11, no. 5, pp. 1571–1592, May 2018, conference Name:\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing.\n[220] A. Kendall and Y. Gal, “What uncertainties do we need in bayesian\ndeep learning for computer vision?” 2017.\n[221] X. Chen, J. Liu, Z. Wang, and W. Yin, “Theoretical linear convergence\nof unfolded ista and its practical weights and thresholds,” 2018.\n[222] S. Haykin, “Cognitive radar: a way of the future,” IEEE Signal\nProcessing Magazine, vol. 23, no. 1, pp. 30–40, 2006.\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n25\nXiao Xiang Zhu (S’10–M’12–SM’14–F’21) re-\nceived the Master (M.Sc.) degree, her doctor of\nengineering (Dr.-Ing.) degree and her “Habilitation”\nin the ﬁeld of signal processing from Technical\nUniversity of Munich (TUM), Munich, Germany, in\n2008, 2011 and 2013, respectively.\nShe is currently the Professor for Data Science\nin Earth Observation (former: Signal Processing\nin Earth Observation) at Technical University of\nMunich (TUM) and the Head of the Department\n“EO Data Science” at the Remote Sensing Tech-\nnology Institute, German Aerospace Center (DLR). Since 2019, Zhu is a\nco-coordinator of the Munich Data Science Research School (www.mu-\nds.de). Since 2019 She also heads the Helmholtz Artiﬁcial Intelligence –\nResearch Field “Aeronautics, Space and Transport”. Since May 2020, she is\nthe director of the international future AI lab ”AI4EO – Artiﬁcial Intelligence\nfor Earth Observation: Reasoning, Uncertainties, Ethics and Beyond”, Munich,\nGermany. Since October 2020, she also serves in the board of directors of\nthe Munich Data Science Institute (MDSI), TUM. Prof. Zhu was a guest\nscientist or visiting professor at the Italian National Research Council (CNR-\nIREA), Naples, Italy, Fudan University, Shanghai, China, the University of\nTokyo, Tokyo, Japan and University of California, Los Angeles, United States\nin 2009, 2014, 2015 and 2016, respectively. Her main research interests are\nremote sensing and Earth observation, signal processing, machine learning\nand data science, with a special application focus on global urban mapping.\nDr. Zhu is a member of young academy (Junge Akademie/Junges Kolleg) at\nthe Berlin-Brandenburg Academy of Sciences and Humanities and the German\nNational Academy of Sciences Leopoldina and the Bavarian Academy of\nSciences and Humanities. She is an associate Editor of IEEE Transactions on\nGeoscience and Remote Sensing.\nSina Montazeri received the B.Sc. degree in geode-\ntic engineering from the University of Isfahan, Isfa-\nhan, Iran, in 2011, the M.Sc. degree in geomatics\nfrom Delft University of Technology (TU Delft),\nDelft, The Netherlands in 2014, and the Ph.D. degree\nin radar remote sensing from the Technical Univer-\nsity of Munich (TUM), Munich, Germany, in 2019\nwith a dissertation on Geodetic SAR Interferometry.\nIn 2012, he spent two weeks with the Laboratoire\ndes Sciences de l’Image, de l’Informatique et de la\nT´el´ed´etection, University of Strasbourg, Strasbourg,\nFrance, as a Junior Researcher working on thermal remote sensing. From 2013\nto 2015, he was a Research Assistant with the Remote Sensing Technology\nInstitute (IMF), German Aerospace Center (DLR), where he was involved in\nabsolute localization of point clouds obtained from SAR tomography. From\n2015 to 2019, he was a research associate with TUM-SiPEO and DLR-IMF\nworking on automatic positioning of ground control points from multi-view\nradar images. He is currently a Senior Researcher with the department of\nEO Data Science of DLR-IMF focused on developing Machine Learning\nalgorithms applied to radar imagery. His research interests include advanced\nInSAR techniques for deformation monitoring of urban infrastructure, image\nand signal processing relevant to radar imagery and applied machine learning.\nDr. Montazeri was the recipient of the DLR Science Award and the IEEE\nGeoscience and Remote Sensing Society Transactions Prize Paper Award, in\n2016 and 2017, respectively for his work on Geodetic SAR Tomography.\nMohsin Ali eceived his Bachelors in Computer\nEngineering degree from National University of\nScience and Technology(NUST), Islamabad, Pak-\nistan in 2013 and Masters in Computer Science\ndegree from University of Freiburg, Germany in\n2018. Since April 2019 he is a PhD candidate at\nEarth Observation Center, DLR supervised by Prof.\nDr. Xiaoxiang Zhu. His main research interest are\nuncertainty estimation in deep learning models for\nremote sensing applications.\nYuansheng Hua (S’18) received the Bachelor’s\ndegree in remote sensing science and technology\nfrom the Wuhan University, Wuhan, China, in 2014,\nand the Master’s degree in Earth Oriented Space Sci-\nence and Technology (ESPACE) from the Technical\nUniversity of Munich (TUM), Munich, Germany, in\n2018.\nHe is currently pursuing the Ph.D. degree with\nthe German Aerospace Center (DLR), Wessling,\nGermany and the Technical University of Munich\n(TUM), Munich, Germany.\nIn 2019, he was a visiting researcher with the Wageningen University &\nResearch, Wageningen, Netherlands. His research interests include remote\nsensing, computer vision, and deep learning, especially their applications in\nremote sensing.\nYuanyuan Wang (S’08-M’11) received the B.Eng.\ndegree (Hons.) in electrical engineering from The\nHong Kong Polytechnic University, Hong Kong, in\n2008, and the M.Sc. and Dr. Ing. degree from the\nTechnical University of Munich (TUM), Munich,\nGermany, in 2010 and 2015, respectively. In June\nand July 2014, he was a Guest Scientist with the\nInstitute of Visual Computing, ETH Z¨urich, Z¨urich,\nSwitzerland. He is currently with the Department\nof EO Data Science, Remote Sensing Technology\nInstitute of the German Aerospace Center, Weßling,\nGermany, where he leads the working group Big SAR Data. He is also a\nguest member of the Professorship of Data Science in Earth Observation,\nTechnical University of Munich, Munich, Germany, where he supports the\nscientiﬁc management of ERC projects So2Sat (so2sat.eu) and AI4SmartCities\n(cordis.europa.eu/project/id/957467). His research interests include optimal\nand robust parameters estimation in multibaseline InSAR techniques, mul-\ntisensor fusion algorithms of synthetic aperture radar (SAR) and optical data,\nnonlinear optimization with complex numbers, machine learning in SAR, and\nhigh-performance computing for big data.\nDr. Wang serves as the reviewer for multiple IEEE GRSS and other remote\nsensing journals. He was one of the best reviewers of the IEEE Transactions\non Geoscience and Remote Sensing in 2016. He is also an associate editor\nof the Geoscience Data Journal of the UK Royal Meteorological Society.\nLichao Mou received the Bachelor’s degree in\nautomation from the Xi’an University of Posts and\nTelecommunications, Xi’an, China, in 2012, the\nMaster’s degree in signal and information processing\nfrom the University of Chinese Academy of Sciences\n(UCAS), China, in 2015, and the Dr.-Ing. degree\nfrom the Technical University of Munich (TUM),\nMunich, Germany, in 2020.\nHe is currently a Guest Professor at the Munich\nAI Future Lab AI4EO, TUM and the Head of Visual\nLearning and Reasoning team at the Department\n“EO Data Science”, Remote Sensing Technology Institute (IMF), German\nAerospace Center (DLR), Wessling, Germany. Since 2019, he is an AI Con-\nsultant for the Helmholtz Artiﬁcial Intelligence Cooperation Unit (HAICU).\nIn 2015 he spent six months at the Computer Vision Group at the University\nof Freiburg in Germany. In 2019 he was a Visiting Researcher with the\nCambridge Image Analysis Group (CIA), University of Cambridge, UK. From\n2019 to 2020, he was a Research Scientist at DLR-IMF.\nHe was the recipient of the ﬁrst place in the 2016 IEEE GRSS Data Fusion\nContest and ﬁnalists for the Best Student Paper Award at the 2017 Joint Urban\nRemote Sensing Event and 2019 Joint Urban Remote Sensing Event.\nACCEPTED BY IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE, 2021\n26\nYilei Shi (M’18) received his Diploma (Dipl.-Ing.)\ndegree in Mechanical Engineering, his Doctorate\n(Dr.-Ing.) degree in Engineering from Technical\nUniversity of Munich (TUM), Germany. In April\nand May 2019, he was a guest scientist with the\ndepartment of applied mathematics and theoretical\nphysics, University of Cambridge, United Kingdom.\nHe is currently a senior scientist with the Chair of\nRemote Sensing Technology, Technical University\nof Munich.\nHis research interests include computational intel-\nligence, fast solver and parallel computing for large-scale problems, advanced\nmethods on SAR and InSAR processing, machine learning and deep learning\nfor variety data sources, such as SAR, optical images, medical images and so\non; PDE related numerical modeling and computing.\nFeng Xu (S’06-M’08-SM’14) received the B.E.\n(Hons.) degree in information engineering from\nSoutheast University, Nanjing, China, in 2003, and\nthe Ph.D. (Hons.) degree in electronic engineering\nfrom Fudan University, Shanghai, China, in 2008.\nFrom 2008 to 2010, he was a Post-Doctoral Fellow\nwith the NOAA Center for Satellite Application and\nResearch, Camp Springs, MD, USA. From 2010\nto 2013, he was with Intelligent Automation Inc.,\nRockville, MD, USA, while he was partly with\nthe NASA Goddard Space Flight Center, Greenbelt,\nMD, USA, as a Research Scientist. In 2012, he was selected into China’s\nGlobal Experts Recruitment Program, and subsequently returned to Fudan\nUniversity, Shanghai, China, in 2013, where he is currently a Professor with\nthe School of Information Science and Technology and the Vice Director\nof the MoE Key Laboratory for Information Science of Electromagnetic\nWaves. He has authored more than 30 papers in peer-reviewed journals,\nco-authored two books, and holds two patents, among many conference\npapers. His research interests include electromagnetic scattering modeling,\nSAR information retrieval, and radar system development.\nDr. Xu was a recipient of the second-class National Nature Science Award\nthe IEEE Geoscience and Remote Sensing Society and the 2014 SUMMA\nGraduate Fellowship in the advanced electromagnetics area. He currently\nerves as the Associate Editor of the IEEE GEOSCIENCE AND REMOTE\nSENSING LETTERS. He is the Founding Chair of the IEEE GRSS Shanghai\nChapter.\nRichard Bamler (M’95–SM’00–F’05) received his\nDiploma degree in Electrical Engineering, his Doc-\ntorate in Engineering, and his “Habilitation” in the\nﬁeld of signal and systems theory in 1980, 1986, and\n1988, respectively, from the Technical University of\nMunich, Germany.\nHe worked at the university from 1981 to 1989\non optical signal processing, holography, wave\npropagation, and tomography. He joined the Ger-\nman Aerospace Center (DLR), Oberpfaffenhofen,\nin 1989, where he is currently the Director of the\nRemote Sensing Technology Institute.\nIn early 1994, Richard Bamler was a visiting scientist at Jet Propulsion\nLaboratory (JPL) in preparation of the SIC-C/X-SAR missions, and in 1996\nhe was guest professor at the University of Innsbruck. Since 2003 he has held a\nfull professorship in remote sensing technology at the Technical University of\nMunich as a double appointment with his DLR position. His teaching activities\ninclude university lectures and courses on signal processing, estimation theory,\nand SAR. Since he joined DLR Richard Bamler, his team, and his institute\nhave been working on SAR and optical remote sensing, image analysis and\nunderstanding, stereo reconstruction, computer vision, ocean color, passive\nand active atmospheric sounding, and laboratory spectrometry. They were\nand are responsible for the development of the operational processors for\nSIR-C/X-SAR, SRTM, TerraSAR-X, TanDEM-X, Tandem-L, ERS-2/GOME,\nENVISAT/SCIAMACHY, MetOp/GOME-2, Sentinel-5P, Sentinel-4, DESIS,\nEnMAP, etc.\nRichard Bamler’s research interests are in algorithms for optimum infor-\nmation extraction from remote sensing data with emphasis on SAR. This\ninvolves new estimation algorithms, like sparse reconstruction, compressive\nsensing and deep learning.\n",
  "categories": [
    "eess.IV",
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-06-17",
  "updated": "2021-01-05"
}