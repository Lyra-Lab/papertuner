{
  "id": "http://arxiv.org/abs/2006.10236v1",
  "title": "Unsupervised Meta-Learning through Latent-Space Interpolation in Generative Models",
  "authors": [
    "Siavash Khodadadeh",
    "Sharare Zehtabian",
    "Saeed Vahidian",
    "Weijia Wang",
    "Bill Lin",
    "Ladislau Bölöni"
  ],
  "abstract": "Unsupervised meta-learning approaches rely on synthetic meta-tasks that are\ncreated using techniques such as random selection, clustering and/or\naugmentation. Unfortunately, clustering and augmentation are domain-dependent,\nand thus they require either manual tweaking or expensive learning. In this\nwork, we describe an approach that generates meta-tasks using generative\nmodels. A critical component is a novel approach of sampling from the latent\nspace that generates objects grouped into synthetic classes forming the\ntraining and validation data of a meta-task. We find that the proposed\napproach, LAtent Space Interpolation Unsupervised Meta-learning (LASIUM),\noutperforms or is competitive with current unsupervised learning baselines on\nfew-shot classification tasks on the most widely used benchmark datasets. In\naddition, the approach promises to be applicable without manual tweaking over a\nwider range of domains than previous approaches.",
  "text": "Unsupervised Meta-Learning through Latent-Space\nInterpolation in Generative Models\nSiavash Khodadadeh1∗\nSharare Zehtabian1∗\nSaeed Vahidian2\nWeijia Wang2\nBill Lin2\nLadislau Bölöni1\n1 Dept. of Computer Science\nUniversity of Central Florida\nOrlando, FL 32816\n2 Dept. of Electrical Engineering and Computer Science\nUniversity of California San Diego\nSan Diego, CA 92161\n{siavash.khodadadeh, sharare.zehtabian}@knights.ucf.edu\nAbstract\nUnsupervised meta-learning approaches rely on synthetic meta-tasks that are cre-\nated using techniques such as random selection, clustering and/or augmentation.\nUnfortunately, clustering and augmentation are domain-dependent, and thus they\nrequire either manual tweaking or expensive learning. In this work, we describe\nan approach that generates meta-tasks using generative models. A critical compo-\nnent is a novel approach of sampling from the latent space that generates objects\ngrouped into synthetic classes forming the training and validation data of a meta-\ntask. We ﬁnd that the proposed approach, LAtent Space Interpolation Unsupervised\nMeta-learning (LASIUM), outperforms or is competitive with current unsupervised\nlearning baselines on few-shot classiﬁcation tasks on the most widely used bench-\nmark datasets. In addition, the approach promises to be applicable without manual\ntweaking over a wider range of domains than previous approaches.\n1\nIntroduction\nMeta-learning algorithms for neural networks (1; 2; 3) prepare networks to quickly adapt to unseen\ntasks. This is done in a meta-training phase that typically involves a large number of supervised\nlearning tasks. Very recently, several approaches had been proposed that perform the meta-training\nby generating synthetic training tasks from an unsupervised dataset. This requires us to generate\nsamples with speciﬁc pairwise information: in-class pairs of samples that are with high likelihood in\nthe same class, and out-of-class pairs that are with high likelihood not in the same class. For instance,\nUMTRA (4) and AAL (5) achieve this through random selection from a domain with many classes\nfor out-of-class pairs and by augmentation for in-class pairs. CACTUs (6) creates synthetic labels\nthrough unsupervised clustering of the domain. Unfortunately, these algorithms depend on domain\nspeciﬁc expertise for the appropriate clustering and augmentation techniques.\nIn this paper, we rely on recent advances in the ﬁeld of generative models, such as the variants of\ngenerative adversarial networks (GANs) and variational autoencoders (VAEs), to generate the in-class\nand out-of-class pairs of meta-training data. The fundamental idea of our approach is that in-class\npairs are close while out-of-class pairs are far away in the latent space representation of the generative\nmodel. Thus, we can generate in-class pairs by interpolating between two out-of-class samples\n∗These authors contributed equally.\nPreprint. Under review.\narXiv:2006.10236v1  [cs.LG]  18 Jun 2020\nin the latent space and choosing interpolation ratios that put the new sample close to one of the\nobjects. From this latent sample, the generative model creates the new in-class object. Our approach\nrequires minimal domain-speciﬁc tweaking, and the necessary tweaks are human-comprehensible.\nFor instance, we need to choose thresholds for latent space distance that ensure that classes are\nin different domains, as well as interpolation ratio thresholds that ensure that the sample is in the\nsame class as the nearest edge. Another advantage of the approach is that we can take advantage of\noff-the-shelf, pre-trained generative models.\nThe main contributions of this paper can be summarized as follows:\n• We describe an algorithm, LAtent Space Interpolation Unsupervised Meta-learning (LA-\nSIUM), that creates training data for a downstream meta-learning algorithm starting from an\nunlabeled dataset by taking advantage of interpolation in the latent space of a generative\nmodel.\n• We show that on the most widely used few-shot learning datasets, LASIUM outperforms or\nperforms competitively with other unsupervised meta-learning algorithms, signiﬁcantly out-\nperforms transfer learning in all cases, and in a number of cases approaches the performance\nof supervised meta-learning algorithms.\n2\nRelated Work\nMeta-learning or “learning to learn” in the ﬁeld of neural networks is an umbrella term that covers a\nvariety of techniques that involve training a neural network over the course of a meta-training phase,\nsuch that when presented with the target task, the network is able to learn it much more efﬁciently than\nan unprepared network would. Such techniques had been proposed since the 1980s (7; 8; 9; 10). In\nrecent years, meta-learning has gained a resurgence, through approaches that either “learn to optimize”\n(2; 11; 12; 13; 14; 15) or learn embedding functions in a non-parametric setting (3; 16; 17; 18).\nHybrids between these two approaches had also been proposed (19; 20).\nMost approaches use labeled data during the meta-learning phase. While in some domains there is an\nabundance of labeled datasets, in many domains such labeled data is difﬁcult to acquire. Unsupervised\nmeta-learning approaches aim to learn from an unsupervised dataset from a domain similar from\nthat of the target task. Typically these approaches generate synthetic few-shot learning tasks for\nthe meta-learning phase through a variety of techniques. CACTUs (6) uses a progressive clustering\nmethod. UMTRA (4) utilizes the statistical diversity properties and domain-speciﬁc augmentations\nto generate synthetic training and validation data. AAL (5) uses augmentation of the unlabeled\ntraining set to generate the validation data. The accuracy of these approaches was shown to be\ncomparable with but lower than supervised meta-learning approaches, but with the advantage of\nrequiring orders of magnitude less labeled training data. A common weakness of these approaches is\nthat the techniques used to generate the synthetic tasks (clustering, augmentation, random sampling)\nare highly domain dependent.\nOur proposed approach, LASIUM, takes advantage of generative models trained on the speciﬁc\ndomain to create the in-class and out-of-class pairs of meta-training data. The most successful\nneural-network based generative models in recent years are variational autoencoders (VAE) (21)\nand generative adversarial networks (GANs) (22). The implementation variants of the LASIUM\nalgorithm described in this paper rely on the original VAE model and on two speciﬁc variations of\nthe GAN concept, respectively. MSGAN (aka Miss-GAN) (23) aims to solve the missing mode\nproblem of conditional GANs through a regularization term that maximizes the distance between\nthe generated images with respect to the distance between their corresponding input latent codes.\nProgressive GANs (24) are growing both the generator and discriminator progressively, and approach\nresembling the layer-wise training of autoencoders.\n3\nMethod\n3.1\nPreliminaries\nWe deﬁne an N-way, K(tr)-shot supervised classiﬁcation task, T , as a set D(tr)\nT\ncomposed of\ni ∈{1, . . . , N × K(tr)} data points (xi, yi) such that there are exactly K(tr) samples for each\n2\ncategorical label yi ∈{1, . . . , N}. During meta-learning, an additional set ,D(val)\nT\n, is attached to\neach task that contains another N × K(val) data points separate from the ones in D(tr)\nT\n. We have\nexactly K(val) samples for each class in D(val)\nT\nas well.\nIt is straightforward to package N-way, K(tr)-shot tasks with D(tr)\nT\nand D(val)\nT\nfrom a labeled dataset.\nHowever, in unsupervised meta-learning setting, a key challenge is how to automatically construct\ntasks from the unlabeled dataset U = {. . . xi . . .}.\n3.2\nGenerating meta-tasks using generative models\nWe have seen that in order to generate the training data for the meta-learning phase, we need\nto generate N-way training tasks with K(tr) training and K(val) validation samples. The label\nassociated with the classes in these tasks is not relevant, as it will be discarded after the meta-\nlearning phase. Our objective is simply to generate samples of the type xi,j with i ∈{1 . . . N} and\nj ∈{1 . . . K(tr) + K(val)} with the following properties: (a) all the samples xi,j are different (b) any\ntwo samples with the same i index are in-class samples and (c) any two samples with different i index\nare out-of-class samples. In the absence of human provided labels, the class structure of the domain\nis deﬁned only implicitly by the sample selection procedure. Previous approaches to unsupervised\nmeta-learning chose samples directly from the training data xi,j ∈U, or created new samples through\naugmentation. For instance, we can deﬁne the class structure of the domain by assuming that certain\ntypes of augmentations keep the samples in-class with the original sample. One challenge of such\napproaches is that the choice of the augmentation is domain dependent, and the augmentation itself\ncan be a complex mathematical operation.\nIn this paper we approach the sample selection problem differently. Instead of sampling xi,j from U,\nwe use the unsupervised dataset to train a generative model p(x). Generative models represent the\nfull probability distribution of a model, and allow us to sample new instances from the distribution.\nFor many models, this sampling process can be computationally expensive iterative process. Many\nsuccessful neural network based generative models use the reparametrization trick for the training\nand sampling which concentrate the random component of the model in a latent representation z.\nBy choosing the latent representation z from a simple (uniform or normal) distribution, we can\nobtain a sample from the complex distribution p(x) by passing z through a deterministic generator\nG(z) →x. Two of the most popular generative models, variational autoencoders (VAEs) and\ngenerative adversarial networks (GANs) follow this model.\nThe idea of the LASIUM algorithm is that given a generator component G(.), nearby latent space\nvalues z1 and z2 map to in-class samples x1 and x2. Conversely, z1 and z2 values that are far away\nfrom each other, map to out of class samples. Naturally, we still need to deﬁne what we mean by\n“near” and “far” in the latent space and how to choose the corresponding z values. However, this is a\nsigniﬁcantly simpler task than, for instance, deﬁning the set of complex augmentations that might\nretain class membership.\n[ht]\nTraining a generative model Our method for generating meta-tasks is agnostic to the choice of\ntraining algorithm for the generative model and can use either a VAE or a GAN with minimal\nadjustments. In our VAE experiments, we used a network trained with the standard VAE training\nalgorithm (21). For the experiments with GANs we used two different methods mode seeking GANs\n(MSGAN) (23) and progressive growing of GANs (proGAN) (24).\nAlgorithm 1 describes the steps of our method. We will delve into each step in the following parts of\nthis section.\nSampling out of class instances from the latent space representation: Our sampling techniques\ndiffer slightly whether we are using a GAN or VAE. For GAN, we use rejection sampling to ﬁnd\nN latent space vectors that are at a pairwise distance of at least threshold ϵ - see Figure 1(a). When\nusing a VAE, we also have an encoder network that allows us to map from the domain to the latent\nspace. Taking advantage of this, we can additionally sample data points from our unlabeled dataset\nU and embed them into a latent space. If the latent space representation of these N images are too\nclose to each other, we re-sample, otherwise we can use the N images and their representations and\n3\nz1\nz2\n(a) Sampling\n(c) Task generation \n𝒢 \n...\nK(tr) \n...\n K(val)\n(b) In-class sampling\nz3\nz′\n1\nz′\n2\nz′\n3\n𝓖 (zi)\n𝓖 (z′\ni)\n𝓖 (z″\ni)\nz″\n1\nz″\n2\nz″\n3\n...\nN\nFigure 1: 3-way, K(tr)-shot task generation with K(val) images for validation by a pre-trained GAN\ngenerator G. a) Sample 3 random vectors. b) Generate new vectors by one of the proposed in-class\nsampling strategies. c) Generate images from all of the latent vectors and put them into train and\nvalidation set to construct a task. The images in this ﬁgure have been generated by our algorithm.\nThe colored edge of each image indicates that it was generated from its corresponding latent vector.\n(b) Encoding\n(c) In-class sampling\n(a) Sampling\nK(tr) \n K(val)\n𝓭 (z′\ni)\n𝓭 (z″\ni)\n...\nN\n(d) Task generation\n...\n...\n...\nz1\nz2\nz3\nz′\n1\nz′\n2\nz′\n3\nz″\n1 z″\n2\nz″\n3\nFigure 2: 3-way, K(tr)-shot task generation by VAE on Omniglot dataset with K(val) images for\nvalidation set of each task. a) Sample 3 images from dataset. b) Encode the images into latent space\nand check if they are distanced. c) Use proposed in-class sampling techniques to generate new latent\nvectors. d) Generate images from the latent vectors and put them alongside with sampled images\nfrom step a into train and validation set to construct a task.\ncontinue the following steps exactly the same as GANs - see Figure 2(a) and (b). We will refer to the\nvectors selected here as anchor vectors.\nGenerating in-class latent space vectors Next, having N sampled anchor vectors {z1, . . . , zN}\nfrom the latent space representation, we aim to generate N new vectors {z′\n1, . . . , z′\nN} from the latent\nspace representation such that the generated image G(zi) belongs to the same class as the one of\nG(z′\ni) for i ∈1, . . . , N. This process needs to be repeated P for K(tr) + K(val) −1 times.\nThe sampling strategy takes as input the sampled vectors and a number ω ∈{1 . . . K(tr)+K(val)−1}\nand returns N new vectors such that zi and z′\ni are an in-class pair for i ∈{1 . . . N}. This ensures that\nno two z′\ni belong to the same class and creates N groups of (K(tr) + K(val)) vectors in our latent\nspace. We feed these vectors to our generator to get N groups of (K(tr) + K(val)) images. From\neach group we pick the ﬁrst K(tr) for D(tr)\nT\nand the last K(val) for D(val)\nT\n.\nWhat remains is to deﬁne the strategy to sample the individual in-class vectors. We propose three\ndifferent sampling strategies, all of which can be seen as variations of the idea of latent space\ninterpolation sampling. This motivates the name of the algorithm LAtent Space Interpolation\nUnsupervised Meta-learning (LASIUM).\nLASIUM-N (adding Noise): This technique generates in-class samples by adding Gaussian noise to\nthe anchor vector z′\ni = zi +ϵ where ϵ ∼N(0, σ2) (see Figure 3-Left). In the context of LASIUM, we\n4\nAlgorithm 1: LASIUM for unsupervised meta-learning task generation\nrequire :Unlabeled dataset U = {. . . xi . . .}, Pre-trained generator G, Policy P\nrequire :K(tr), K(val): number of samples for train and validation during meta-learning\nrequire :N: class-count, NMB: meta-batch size\n1 B = {} ; // meta-batch of tasks\n2 for i in 1, . . . , NMB do\n3\nSample N class-vectors in latent space of G and add them to task-vectors\n4\nfor ω in 1, . . . , K(tr) + K(val) −1 do\n5\ngenerate new-vectors = P(class-vectors, ω) and add them to task-vectors\n6\nend\n7\nGenerate N × (K(tr) + K(val)) images by feeding task-vectors to generator G\n8\nConstruct task Ti by putting the ﬁrst N × K(tr) images in task train set and the last N × K(val)\nimages in task validation set\n9\nB ←B ∪Ti\n10 end\n11 return B\nvalidation\ntrain\nin-class\nin-class\nin-class\nout-of-class\nout-of-class\nout-of-class\nLASIUM-N\nLASIUM-RO\nvalidation\ntrain\nLASIUM-OC\ntrain\nvalidation\nFigure 3: Latent space representation visualization of proposed strategies for generating in-class\ncandidates. Left: LASIUM-N, adding random noise to the sample vector. Middle: LASIUM-RO,\ninterpolate with random out-of-class samples. Right: LASIUM-OC, interpolate with other classes’\nsamples.\ncan see this as an interpolation between the anchor vector and a noise vector, with the interpolation\nfactor determined by σ. For the impact of different choices of σ see the ablation study in section 4.6.\nLASIUM-RO (with Random Out-of-class samples) To generate a new in-class sample to anchor\nvector zi we ﬁrst ﬁnd a random out-of-class sample vi, and choose an interpolated version closer\nto the anchor: z′\ni = zi + α × (vi −zi) (see Figure 3-Middle). Here, α is a hyperparameter, which\ncan be tuned to deﬁne the size of the class. As we are in a comparatively high-dimensional latent\nspace (in our case, 512 dimensions), we need relatively large values of α, such as α = 0.4 to deﬁne\nclasses of reasonable size. This model effectively allows us to deﬁne complex augmentations (such\nas a person seen without glasses, or in a changed lighting) with only one scalar hyperparameter to\ntune. By interpolating towards another sample we ensure that we are staying on the manifold that\ndeﬁnes the dataset (in the case of Figure 3, this being human faces).\nLASIUM-OC (with Other Classes’ samples) This technique is similar to LASIUM-RO, but instead\nof using a randomly generated out-of-class vector, we are interpolating towards vectors already chosen\n5\nfrom the other classes in the same task (see Figure 3-Right). This limits the selection of the samples to\nbe conﬁned to the convex hull deﬁned by the initial anchor points. The intuition behind this approach\nis that choosing the samples this way focuses the attention of the meta-learner towards the hard to\ndistinguish samples that are between the classes in the few shot learning class (eg. they share certain\nattributes).\n4\nExperiments\nWe tested the proposed algorithms on three few-shot learning benchmarks: (a) the 5-way Om-\nniglot (25), a benchmark for few-shot handwritten character recognition, (b) the 5-way CelebA\nfew-shot identity recognition, and (c) the CelebA attributes dataset (26) proposed as a few-shot\nlearning benchmark by (2) that comprises binary classiﬁcation (2-way) tasks in which each task is\ndeﬁned by selecting 3 different attributes and 3 boolean values corresponding to each attribute. Every\nimage in a certain task-speciﬁc class has the same attributes with each other while does not share any\nof these attributes with images in the other class. Last but not least we evaluate our results on (d) the\nmini-ImageNet (27) few-shot learning benchmark.\nWe partition each dataset into meta-training, meta-validation, and meta-testing splits between classes.\nTo evaluate our method, we use the classes in the test set to generate 1000 tasks as described\nin section 3.2. We set K(val) to be 15. We average the accuracy on all tasks and report a 95%\nconﬁdence interval. To ensure that comparisons are fair, we use the same random seed in the whole\ntask generation process. For the Omniglot dataset, we report the results for K(tr) ∈{1, 5}, and\nK(val) = 15. For CelebA identity recognition, we report our results for K(tr) ∈{1, 5, 15} and\nK(val) = 15. For CelebA attributes, we follow the K(tr) = 5 and K(val) = 5 tasks as proposed\nby (6).\n4.1\nBaselines\nAs baseline algorithms for our approach we follow the practice of recent papers in the unsupervised\nmeta-learning literature. The simplest baseline is to train the same network architecture from scratch\nwith N × K(tr) images. More advanced baselines can be obtained by learning an unsupervised\nembedding on U and use it for downstream task training. We used the ACAI (28), BiGAN (29; 30),\nand DeepCluster (31) as representative of the unsupervised learning literature. On top of these\nembeddings, we report accuracy for Knn-nearest neighbors, linear classiﬁer, multi layer perceptron\n(MLP) with dropout, and cluster matching.\nThe direct competition for our approach are the current state-of-the-art algorithms in unsupervised\nmeta-learning. We compare our results with CACTUs-MAML (6), CACTUs-ProtoNets (6) and\nUMTRA (4). Finally, it is useful to compare our approach with algorithms that require supervised\ndata. We include results for supervised standard transfer learning from VGG19 pre-trained on\nImageNet (32) and two supervised meta-learning algorithms, MAML (6), and ProtoNets (6).\n4.2\nNeural network architectures\nSince excessive tuning of hyperparameters can lead to the overestimation of the performance of a\nmodel (33), we keep the hyperparameters of the unsupervised meta-learning as constant as possible\n(including the MAML, and ProtoNets model architectures) in all experiments. Our model architecture\nconsists of four stacked convolutional blocks. Each block comprises 64 ﬁlters that carry out 3 × 3\nconvolutions, followed by batch normalization, a ReLU non-linearity, and 2 × 2 max-pooling. For the\nMAML experiments, classiﬁcation is performed by a fully connected layer, whereas for the ProtoNets\nmodel we compute distances based on the feature vectors produced by the last convolution module\nwithout any dense layers. The input size to our model is 84 × 84 × 3 for CelebA and 28 × 28 × 1 for\nOmniglot.\nFor Omniglot, our VAE model is constructed symmetrically. The encoder is composed of four\nconvolutional blocks, with batch normalization and ReLU activation following each of them. A\ndense layer is connected to the end such that given an input image of shape 28 × 28, the encoder\nproduces a latent vector of length 20. On the other side, the decoder starts from a dense layer whose\noutput has length 7 × 7 × 64 = 3136. It is then fed into four modules each of which consists of\n6\na transposed convolutional layer, batch normalization and the ReLU non-linearity. We use 3 × 3\nkernels, 64 channels and a stride of 2 for all the convolutional and transposed convolutional layers.\nHence, the generated image has the size of 28 × 28 that is identical to the input images. This VAE\nmodel is trained for 1000 epochs with a learning rate of 0.001.\nOur GAN generator gets an input of size l which is the dimensionality of the latent space and feeds it\ninto a dense layer of size 7 × 7 × 128. After applying a Leaky ReLU with α = 0.2, we reshape the\noutput of dense layer to 128 channels of shape 7 × 7. Then we feed it into two upsampling blocks,\nwhere each block has a transposed convolution with 128 channels, 4 × 4 kernels and 2 × 2 strides.\nFinally, we feed the outcome of the upsampling blocks into a convolution layer with 1 channel and a\n7 × 7 kernel with sigmoid activaiton. The discriminator takes a 28 × 28 × 1 input and feeds it into\nthree 3×3 convolution layers with 64, 128 and 128 channels and 2×2 strides. We apply leaky ReLU\nactivation after each convolution layer with α = 0.2. Finally we apply a global 2D max pooling layer\nand feed it into a dense layer with 1 neuron to classify the output as real or fake. We use the same\nloss function for training as described in (23).\nFor the CelebA GAN experiments, we use the pre-trained network architecture described in (24). For\nVAE, we use the same architecture as we described for Omniglot VAE with one more convolution\nblock and more channels to handle the larger input size of 84 × 84 × 3. The exact architecture is\ndescribed in section 4.6.\n4.3\nResults on Omniglot\nTable 1 shows the results on the Omniglot dataset. We ﬁnd that the LASIUM-RO-GAN-MAML\nconﬁguration outperforms all the unsupervised approaches, including the meta-learning based ones\nlike CACTUs (6) and UMTRA (4). Beyond the increase in performance, we must note that the\ncompeting approaches use more domain speciﬁc knowledge (in case of UMTRA augmentations, in\ncase of CACTUs, learned clustering). We also ﬁnd that on this benchmark, LASIUM outperforms\ntransfer learning using the much larger VGG-19 network.\nAs expected even the best LASIUM result is worse than the supervised meta-learning models.\nHowever, we need to consider that the unsupervised meta-learning approaches use several orders\nof magnitude less labels. For instance, the 95.29% accuracy of LASIUM-RO-GAN-MAML was\nobtained with only 25 labels, while the supervised approaches used 25,000.\n4.4\nResults on CelebA\nTable 2 shows our results on the CelebA identity recognition tasks where the objective is to recognize\nN different people given K(tr) images for each. We ﬁnd that on this benchmark as well, the LASIUM-\nRO-GAN-MAML conﬁguration performs better than other unsupervised meta-learning models as\nwell as transfer learning with VGG-19 - it only falls slightly behind LASIUM-RO-GAN-ProtoNets\non the one-shot case. As we have discussed in the case of Omniglot results, the performance remains\nlower then the supervised meta-learning approaches which use several orders of magnitude more\nlabeled data.\nFinally, Table 3 shows our results for CelebA attributes benchmark introduced in (6). A peculiarity\nof this dataset is that the way in which classes are deﬁned based on the attributes, the classes are\nunbalanced in the dataset, making the job of synthetic task selection more difﬁcult. We ﬁnd that\nLASIUM-N-GAN-MAML obtains the second best on this test with a performance of 74.79 ± 1.01,\nwithin the conﬁdence interval of the winner, CACTUs MAML with BiGAN 74.98 ± 1.02. In\nthis benchmark, transfer learning with the VGG-19 network performed better than all unsupervised\nmeta-learning approaches, possibly due to existing representations of the discriminating attributes in\nthat much more complex network.\n4.5\nResults on mini-ImageNet\nIn this section, we evaluate our algorithm on mini-ImageNet benchmark. Its complexity is high due\nto the use of ImageNet images. In total, there are 100 classes with 600 samples of 84 × 84 color\nimages per class. These 100 classes are divided into 64, 16, and 20 classes respectively for sampling\ntasks for meta-training, meta-validation, and meta-test. A big difference between mini-ImageNet and\nCelebA is that we have to classify a group of concepts instead of just the identity of a subject. This\n7\nTable 1: Accuracy results on the Omniglot dataset averaged over 1000, 5-way, K(tr)-shot downstream\ntasks with K(val) = 15 for each task. ± indicates the 95% conﬁdence interval. The top three\nunsupervised results are reported in bold.\nAlgorithm\nFeature Extractor\nK(tr) = 1\nK(tr) = 5\nTraining from scratch\nN/A\n51.64 ± 0.65\n71.44 ± 0.53\nK-nearest neighbors\nACAI\n57.46 ± 1.35\n81.16 ± 0.57\nLinear Classiﬁer\nACAI\n61.08 ± 1.32\n81.82 ± 0.58\nMLP with dropout\nACAI\n51.95 ± 0.82\n77.20 ± 0.65\nCluster matching\nACAI\n54.94 ± 0.85\n71.09 ± 0.77\nK-nearest neighbors\nBiGAN\n49.55 ± 1.27\n68.06 ± 0.71\nLinear Classiﬁer\nBiGAN\n48.28 ± 1.25\n68.72 ± 0.66\nMLP with dropout\nBiGAN\n40.54 ± 0.79\n62.56 ± 0.79\nCluster matching\nBiGAN\n43.96 ± 0.80\n58.62 ± 0.78\nCACTUs-MAML\nBiGAN\n58.18 ± 0.81\n78.66 ± 0.65\nCACTUs-MAML\nACAI\n68.84 ± 0.80\n87.78 ± 0.50\nUMTRA-MAML\nN/A\n81.91 ± 0.58\n94.58 ± 0.25\nLASIUM-RO-GAN-MAML\nN/A\n83.26 ± 0.55\n95.29 ± 0.22\nLASIUM-N-VAE-MAML\nN/A\n76.11 ± 0.64\n94.42 ± 0.26\nCACTUs-ProtoNets\nBiGAN\n54.74 ± 0.82\n71.69 ± 0.73\nCACTUs-ProtoNets\nACAI\n68.12 ± 0.84\n83.58 ± 0.61\nLASIUM-RO-GAN-ProtoNets\nN/A\n80.15 ± 0.64\n91.10 ± 0.35\nLASIUM-OC-VAE-ProtoNets\nN/A\n73.22 ± 0.73\n85.05 ± 0.46\nTransfer Learning (VGG-19)\nN/A\n54.49 ± 0.90\n89.57 ± 0.44\nSupervised MAML\nN/A\n94.46 ± 0.35\n98.83 ± 0.12\nSupervised ProtoNets\nN/A\n98.35 ± 0.22\n99.58 ± 0.09\nTable 2: Accuracy results of unsupervised learning on CelebA for different unsupervised methods.\nThe results are averaged over 1000, 5-way, K(tr)-shot downstream tasks with K(val) = 15 for each\ntask. ± indicates the 95% conﬁdence interval. The top three unsupervised results are reported in\nbold.\nAlgorithm\nK(tr) = 1\nK(tr) = 5\nK(tr) = 15\nTraining from scratch\n34.69 ± 0.50\n56.50 ± 0.55\n70.56 ± 0.49\nCACTUs\n41.42 ± 0.64\n62.71 ± 0.57\n74.18 ± 0.68\nUMTRA\n39.30 ± 0.59\n60.44 ± 0.56\n72.41 ± 0.48\nLASIUM-RO-GAN-MAML\n43.88 ± 0.57\n66.98 ± 0.53\n78.13 ± 0.44\nLASIUM-RO-VAE-MAML\n41.25 ± 0.57\n58.22 ± 0.54\n71.05 ± 0.49\nLASIUM-RO-GAN-ProtoNets\n44.39 ± 0.61\n60.83 ± 0.58\n66.66 ± 0.53\nLASIUM-RO-VAE-ProtoNets\n43.22 ± 0.58\n61.12 ± 0.54\n68.51 ± 0.51\nTransfer Learning (VGG-19)\n33.28 ± 0.57\n58.74 ± 0.62\n74.04 ± 0.49\nSupervised MAML\n85.46 ± 0.55\n94.98 ± 0.25\n96.18 ± 0.19\nSupervised ProtoNets\n84.17 ± 0.61\n90.84 ± 0.38\n90.85 ± 0.36\n8\nTable 3: Results on CelebA attributes benchmark 2-way, 5-shot tasks with K(val) = 5. The results\nare averaged over 1000 downstream tasks and ± indicates 95% conﬁdence interval. The top three\nunsupervised results are reported in bold.\nAlgorithm\nFeature Extractor\nAccuracy\nTraining from scratch\nN/A\n63.19 ± 1.06\nK-nearest neighbors\nBiGAN\n56.15 ± 0.89\nLinear Classiﬁer\nBiGAN\n58.44 ± 0.90\nMLP with dropout\nBiGAN\n56.26 ± 0.94\nCluster matching\nBiGAN\n56.20 ± 1.00\nK-nearest neighbors\nDeepCluster\n61.47 ± 0.99\nLinear Classiﬁer\nDeepCluster\n59.57 ± 0.98\nMLP with dropout\nDeepCluster\n60.65 ± 0.9\nCluster matching\nDeepCluster\n51.51 ± 0.89\nCACTUs MAML\nBiGAN\n74.98 ± 1.02\nCACTUs MAML\nDeepCluster\n73.79 ± 1.01\nLASIUM-N-GAN-MAML\nN/A\n74.79 ± 1.01\nCACTUs ProtoNets\nBiGAN\n65.58 ± 1.04\nCACTUs ProtoNets\nDeepCluster\n74.15 ± 1.02\nLASIUM-N-GAN-ProtoNets\nN/A\n73.41 ± 1.10\nTransfer Learning (VGG-19)\nN/A\n79.76 ± 1.03\nSupervised MAML\nN/A\n87.10 ± 0.85\nSupervised ProtoNets\nN/A\n85.13 ± 0.92\nmakes interpreting the latent space a bit trickier. For example, it is not rational to interpolate between\na bird and a piano. However, the assumption that nearby latent vectors belong to nearby instances is\nstill valid. Thereby, we could be conﬁdent by not getting too far from the current latent vector, we\ngenerate something which belongs to the same class (identity).\nFor mini-ImageNet we use a pre-trained network BigBiGAN2. Our experiments show that our method\nis very effective and can outperform state-of-the-art algorithms. See Table 4 for the results on mini-\nImageNet benchmark. Figure 4 demonstrates tasks constructed for mini-ImageNet by LASIUM-N\nwith σ2 = 1.0.\nTrain\nValidation\n...\n...\nFigure 4: Train and validation tasks for mini-ImageNet constructed by LASIUM-N with σ2 = 1.0\n4.6\nHyperparameters and ablation studies\nIn this section, we report the hyperparameters of LASIUM-MAML in Table 5 and LASIUM-ProtoNets\nin Table 6 for Omniglot, CelebA, CelebA attributes and mini-ImageNet datasets.\n2https://tfhub.dev/deepmind/bigbigan-resnet50/1\n9\nTable 4: Results on mini-ImageNet benchmark for 5-way, K(tr)-shot tasks with K(val) = 15. The\nresults are averaged over 1000 downstream tasks and ± indicates 95% conﬁdence interval. The top\nthree unsupervised results are reported in bold.\nAlgorithm\nEmbedding\nK(tr) = 1\nK(tr) = 5\nK(tr) = 20\nK(tr) = 50\nTraining from scratch\nN/A\n27.59 ± 0.59\n38.48 ± 0.66\n51.53 ± 0.72\n59.63 ± 0.74\nK-nearest neighbors\nBiGAN\n25.56 ± 1.08\n31.10 ± 0.63\n37.31 ± 0.40\n43.60 ± 0.37\nLinear Classiﬁer\nBiGAN\n27.08 ± 1.24\n33.91 ± 0.64\n44.00 ± 0.45\n50.41 ± 0.37\nMLP with dropout\nBiGAN\n22.91 ± 0.54\n29.06 ± 0.63\n40.06 ± 0.72\n48.36 ± 0.71\nCluster matching\nBiGAN\n24.63 ± 0.56\n29.49 ± 0.58\n33.89 ± 0.63\n36.13 ± 0.64\nK-nearest neighbors\nDeepCluster\n28.90 ± 1.25\n42.25 ± 0.67\n56.44 ± 0.43\n63.90 ± 0.38\nLinear Classiﬁer\nDeepCluster\n29.44 ± 1.22\n39.79 ± 0.64\n56.19 ± 0.43\n65.28 ± 0.34\nMLP with dropout\nDeepCluster\n29.03 ± 0.61\n39.67 ± 0.69\n52.71 ± 0.62\n60.95 ± 0.63\nCluster matching\nDeepCluster\n22.20 ± 0.50\n23.50 ± 0.52\n24.97 ± 0.54\n26.87 ± 0.55\nCACTUs MAML\nBiGAN\n36.24 ± 0.74\n51.28 ± 0.68\n61.33 ± 0.67\n66.91 ± 0.68\nCACTUs MAML\nDeepCluster\n39.90 ± 0.74\n53.97 ± 0.70\n63.84 ± 0.70\n69.64 ± 0.63\nUMTRA MAML\nN/A\n39.93\n50.73\n61.11\n67.15\nLASIUM-N-GAN-\nMAML\nN/A\n40.19 ± 0.58\n54.56 ± 0.55\n65.17 ± 0.49\n69.13 ± 0.49\nCACTUs ProtoNets\nBiGAN\n36.62 ± 0.70\n50.16 ± 0.73\n59.56 ± 0.68\n63.27 ± 0.67\nCACTUs ProtoNets\nDeepCluster\n39.18 ± 0.71\n53.36±0.70\n61.54±0.68\n63.55 ± 0.64\nLASIUM-N-GAN-\nProtoNets\nN/A\n40.05±0.60\n52.53 ± 0.51\n59.45 ± 0.48\n61.43 ± 0.45\nSupervised MAML\nN/A\n46.81 ± 0.77\n62.13 ± 0.72\n71.03 ± 0.69\n75.54 ± 0.62\nSupervised ProtoNets\nN/A\n46.56 ± 0.76\n62.29 ± 0.71\n70.05 ± 0.65\n72.04 ± 0.60\nWe also report the ablation studies on different strategies for task construction in Table 7. We run all\nthe algorithm for just 1000 iterations and compared between them. We also apply a small shift to\nOmniglot images.\nTable 5: LASIUM-MAML hyperparameters summary\nHyperparameter\nOmniglot\nCelebA\nCelebA attributes\nmini-ImageNet\nNumber of classes\n5\n5\n2\n5\nInput size\n28 × 28 × 1\n84 × 84 × 3\n84 × 84 × 3\n84 × 84 × 3\nInner learning rate\n0.4\n0.05\n0.05\n0.05\nMeta learning rate\n0.001\n0.001\n0.001\n0.001\nMeta-batch size\n4\n4\n4\n4\nK(tr) meta-learning\n1\n1\n5\n1\nK(val) meta-learning\n5\n5\n5\n5\nK(val) evaluation\n15\n15\n5\n15\nMeta-adaptation steps\n5\n5\n5\n5\nEvaluation adaptation steps\n50\n50\n50\n50\nTable 6: LASIUM-ProtoNets hyperparameters summary\nHyperparameter\nOmniglot\nCelebA\nCelebA attributes\nmini-ImageNet\nNumber of classes\n5\n5\n2\n5\nInput size\n28 × 28 × 1\n84 × 84 × 3\n84 × 84 × 3\n84 × 84 × 3\nMeta learning rate\n0.001\n0.001\n0.001\n0.001\nMeta-batch size\n4\n4\n4\n4\nK(tr) meta-learning\n1\n1\n5\n1\nK(val) meta-learning\n5\n5\n5\n5\nK(val) evaluation\n15\n15\n5\n15\n10\nTable 7: Accuracy of different proposed strategies on Omniglot. For the sake of comparison, we\nstop meta-learning after 1000 iterations. Results are reported on 1000 tasks with a 95% conﬁdence\ninterval.\nSampling Strategy\nHyperparameters GAN-MAML\nVAE-MAML\nGAN-Proto\nVAE-Proto\nLASIUM-N\nσ2=0.5\n77.16±0.65\n70.41 ± 0.71\n62.16 ± 0.79\n61.57 ± 0.80\nLASIUM-N\nσ2=1.0\n71.10 ± 0.70\n68.26 ± 0.71\n60.95 ± 0.78\n62.17 ± 0.80\nLASIUM-N\nσ2=2.0\n63.18 ± 0.71\n65.18 ± 0.71\n59.81 ± 0.78\n64.88±0.78\nLASIUM-RO\nα=0.2\n77.62±0.64\n75.02±0.66\n62.24±0.79\n62.17 ± 0.80\nLASIUM-RO\nα=0.4\n75.79±0.65\n71.31±0.70\n64.19±0.76\n62.20±0.80\nLASIUM-OC\nα=0.2\n74.70 ± 0.68\n74.98±0.67\n61.79 ± 0.79\n62.16 ± 0.78\nLASIUM-OC\nα=0.4\n73.40 ± 0.68\n68.79 ± 0.73\n64.59±0.76\n63.08±0.79\n5\nConclusion\nWe described LASIUM, an unsupervised meta-learning algorithm for few-shot classiﬁcation. The\nalgorithm is based on interpolation in the latent space of a generative model to create synthetic\nmeta-tasks. In contrast to other approaches, LASIUM requires minimal domain speciﬁc knowledge.\nWe found that LASIUM outperforms state-of-the-art unsupervised algorithms on the Omniglot and\nCelebA identity recognition benchmarks and competes very closely with CACTUs on the CelebA\nattributes learning benchmark.\n6\nAcknowledgements\nThis work had been in part supported by the National Science Foundation under Grant Number\nIIS-1409823.\nReferences\n[1] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A Simple Neural Attentive\nMeta-Learner. In Int’l Conf. on Learning Representations (ICLR), 2018.\n[2] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adapta-\ntion of deep networks. In Proc. of Int’l Conf. on Machine Learning (ICML), pages 1126–1135,\n2017.\n[3] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nAdvances in Neural Information Processing Systems (NeurIPS), pages 4077–4087, 2017.\n[4] Siavash Khodadadeh, Ladislau Bölöni, and Mubarak Shah. Unsupervised meta-learning for\nfew-shot image classiﬁcation. In Advances in Neural Information Processing Systems (NeurIPS),\npages 10132–10142, 2019.\n[5] Antreas Antoniou and Amos Storkey. Assume, augment and learn: Unsupervised few-shot\nmeta-learning via random labels and data augmentation. arXiv preprint arXiv:1902.09884,\n2019.\n[6] Kyle Hsu, Sergey Levine, and Chelsea Finn. Unsupervised learning via meta-learning. In Int’l\nConf. on Learning Representations (ICLR), 2019.\n[7] Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to\nlearn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.\n[8] Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier.\nLearning a synaptic learning rule.\nUniversité de Montréal, Département d’Informatique et de Recherche Opérationelle, 1990.\n[9] Devang K Naik and Richard J Mammone. Meta-neural networks that learn by learning. In\n[Proc. 1992] IJCNN Int’l Joint Conf. on Neural Networks, volume 1, pages 437–442, 1992.\n[10] Sebastian Thrun and Lorien Pratt. Learning to learn. Kluwer Academic Publishers, 1998.\n11\n[11] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. Int’l Conf.\non Learning Representations (ICLR), 2016.\n[12] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal\nconvolutions. arXiv preprint arXiv:1707.03141, 2017.\n[13] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-learning algorithms.\narXiv preprint arXiv:1803.02999, 2018.\n[14] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon\nOsindero, and Raia Hadsell. Meta-Learning with Latent Embedding Optimization. In Int’l Conf.\non Learning Representations (ICLR), 2019.\n[15] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with\nimplicit gradients. In Advances in Neural Information Processing Systems (NeurIPS), pages\n113–124, 2019.\n[16] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks\nfor one shot learning. In Advances in Neural Information Processing Systems (NeurIPS), pages\n3630–3638, 2016.\n[17] Mengye Ren, Sachin Ravi, Eleni Triantaﬁllou, Jake Snell, Kevin Swersky, Josh B. Tenen-\nbaum, Hugo Larochelle, and Richard S. Zemel. Meta-Learning for Semi-Supervised Few-Shot\nClassiﬁcation. In Int’l Conf. on Learning Representations (ICLR), 2018.\n[18] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sungju Hwang, and Yi Yang.\nLearning to propagate labels: Transductive propagation network for few-shot learning. In Int’l\nConf. on Learning Representations (ICLR), 2019.\n[19] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross\nGoroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle.\nMeta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. In Int’l Conf.\non Learning Representations (ICLR), 2020.\n[20] Duo Wang, Yu Cheng, Mo Yu, Xiaoxiao Guo, and Tao Zhang. A hybrid approach with\noptimization-based and metric-based meta-learner for few-shot learning. Neurocomputing,\n349:202–211, 2019.\n[21] P Kingma Diederik and Max Welling. Auto-encoding variational bayes. In Proc. of the Int’l\nConf. on Learning Representations (ICLR), volume 1, 2014.\n[22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural\nInformation Processing Systems (NeurIPS), pages 2672–2680, 2014.\n[23] Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking\ngenerative adversarial networks for diverse image synthesis. Proc. of the IEEE Conf. on\nComputer Vision and Pattern Recognition, pages 1429–1437, 2019.\n[24] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for\nimproved quality, stability, and variation. Proc. of the Int’l Conf. on Learning Representations\n(ICLR), 2018.\n[25] Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning\nof simple visual concepts. In Proc. of the Annual Meeting of the Cognitive Science Society,\nvolume 33, 2011.\n[26] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the\nwild. In Proc. of Int’l Conf. on Computer Vision (ICCV), December 2015.\n[27] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. Proc. of Int’l\nConf. on Learning Representations (ICLR), 2016.\n12\n[28] David Berthelot*, Colin Raffel*, Aurko Roy, and Ian Goodfellow. Understanding and improv-\ning interpolation in autoencoders via an adversarial regularizer. In Int’l Conf. on Learning\nRepresentations (ICLR), 2019.\n[29] Jeff Donahue, Philipp Krähenbühl, and Trevor Darrell. Adversarial feature learning. In Int’l\nConf. on Learning Representations (ICLR), 2017.\n[30] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin\nArjovsky, and Aaron Courville. Adversarially learned inference. In Int’l Conf. on Learning\nRepresentations (ICLR), 2017.\n[31] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for\nunsupervised learning of visual features. In Proc. of the European Conf. on Computer Vision\n(ECCV), pages 132–149, 2018.\n[32] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\nimage recognition. Int’l Conf. on Learning Representations (ICLR), 2015.\n[33] Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow.\nRealistic evaluation of deep semi-supervised learning algorithms. In Advances in Neural\nInformation Processing Systems (NeurIPS), pages 3235–3246, 2018.\n13\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-06-18",
  "updated": "2020-06-18"
}