{
  "id": "http://arxiv.org/abs/2006.10241v1",
  "title": "Robust Unsupervised Learning of Temporal Dynamic Interactions",
  "authors": [
    "Aritra Guha",
    "Rayleigh Lei",
    "Jiacheng Zhu",
    "XuanLong Nguyen",
    "Ding Zhao"
  ],
  "abstract": "Robust representation learning of temporal dynamic interactions is an\nimportant problem in robotic learning in general and automated unsupervised\nlearning in particular. Temporal dynamic interactions can be described by\n(multiple) geometric trajectories in a suitable space over which unsupervised\nlearning techniques may be applied to extract useful features from raw and\nhigh-dimensional data measurements. Taking a geometric approach to robust\nrepresentation learning for temporal dynamic interactions, it is necessary to\ndevelop suitable metrics and a systematic methodology for comparison and for\nassessing the stability of an unsupervised learning method with respect to its\ntuning parameters. Such metrics must account for the (geometric) constraints in\nthe physical world as well as the uncertainty associated with the learned\npatterns. In this paper we introduce a model-free metric based on the\nProcrustes distance for robust representation learning of interactions, and an\noptimal transport based distance metric for comparing between distributions of\ninteraction primitives. These distance metrics can serve as an objective for\nassessing the stability of an interaction learning algorithm. They are also\nused for comparing the outcomes produced by different algorithms. Moreover,\nthey may also be adopted as an objective function to obtain clusters and\nrepresentative interaction primitives. These concepts and techniques will be\nintroduced, along with mathematical properties, while their usefulness will be\ndemonstrated in unsupervised learning of vehicle-to-vechicle interactions\nextracted from the Safety Pilot database, the world's largest database for\nconnected vehicles.",
  "text": "Robust Unsupervised Learning of Temporal\nDynamic Interactions\nAritra Guha‡\nRayleigh Lei‡\nJiacheng Zhu†\nXuanLong Nguyen‡\nDing Zhao†\nDepartment of Mechanical Engineering, Carnegie Mellon University†\nDepartment of Statistics, University of Michigan‡\nJune 19, 2020\nAbstract\nRobust representation learning of temporal dynamic interactions is an important problem\nin robotic learning in general and automated unsupervised learning in particular. Tempo-\nral dynamic interactions can be described by (multiple) geometric trajectories in a suitable\nspace over which unsupervised learning techniques may be applied to extract useful features\nfrom raw and high-dimensional data measurements. Taking a geometric approach to robust\nrepresentation learning for temporal dynamic interactions, it is necessary to develop suitable\nmetrics and a systematic methodology for comparison and for assessing the stability of an\nunsupervised learning method with respect to its tuning parameters. Such metrics must ac-\ncount for the (geometric) constraints in the physical world as well as the uncertainty associated\nwith the learned patterns. In this paper we introduce a model-free metric based on the Pro-\ncrustes distance for robust representation learning of interactions, and an optimal transport\nbased distance metric for comparing between distributions of interaction primitives. These\ndistance metrics can serve as an objective for assessing the stability of an interaction learning\nalgorithm. They are also used for comparing the outcomes produced by diﬀerent algorithms.\nMoreover, they may also be adopted as an objective function to obtain clusters and represen-\ntative interaction primitives. These concepts and techniques will be introduced, along with\nmathematical properties, while their usefulness will be demonstrated in unsupervised learning\nof vehicle-to-vechicle interactions extracted from the Safety Pilot database, the world’s largest\ndatabase for connected vehicles.\n1\nIntroduction\nAdvances in large scale data processing and computation enables the application of sophisticated\nlearning algorithms to robotic design in complex and dynamic environments. In many applications\na fundamental challenge lies not only in learning about the interaction between the ego agent and\nthe environment, but also interactions between multiple agents. Due to the high dimensionality\nand typically noisy nature of the data required for such learning tasks, a standard approach is to\nutilize strong modeling assumptions on the interactions. For example, the interaction between a\nrobotic agent and the environment can be represented by instantaneous physical variables such\nas positions, velocities, a time series of which are then endowed with a stationary distribution for\nmathematical convenience and interpretability (e.g., via a Markov process framework). While such\napproach is useful in highly controlled environments, the strong modeling assumption are usually\nviolated in domains where the interactions among agents and with the environment are highly\ndynamic [8]. Such domains require the development of more robust and data-driven representation\nlearning approaches.\nAs a concrete example which serves as a primary motivation for this work, take the interaction\nbetween two intelligent vehicles that approach each other in a typical intersection. What the two\nvehicles proceed to do next depend on what they can learn of their encounter in real-time. The\ntwo cars may come toward the intersection in varying speeds at perhaps slightly diﬀerent time\npoints. They may or may not signal their intention. For example, one plans to go straight while\nthe other plans to take a turn cutting through the other’s path. Not only do the two agents have\nto learn their temporally varying interaction, they have to do so quickly and accurately while\ncontinually negotiating the traﬃc.\nIn this type of applications where the interaction is highly\ndynamic, a promising approach to robust interaction learning is by decomposing the interaction\nin terms of simpler elements [10, 38]. For traﬃc applications, such interaction elements are called\n1\narXiv:2006.10241v1  [cs.LG]  18 Jun 2020\ntraﬃc primitives. These primitives can be learned, labeled, and eﬀectively utilized for subsequent\ntasks such as vehicle trajectory prediction [45], traﬃc data generation [7, 43], or anomaly detection\n[42].\nStripping away the language of vehicle-to-vehicle (V2V) interactions, the temporal dynamic\ninteraction between two agents comprises of a pair of well-aligned trajectories deﬁned on a suit-\nable space that satisfy constraints presented by the environment and agents’ behaviors. Thus, the\ngoal of robust representation learning of a pairwise interaction between the two dynamic agents\nboils down to the learning of pairs of functions or curves which describe the aligned car physical\nmovements and/or driving behaviors. Such a mathematical viewpoint can be generalized to inter-\nactions among three or more vehicles. In this paper we will focus on the learning of interactions\nin two-agent dynamic scenarios. Although our work is motivated by the learning of multi-agent\ntraﬃc interaction’s primitives, we believe that the techniques developed here can be utilized to\nother settings of multi-agent temporal dynamic interaction learning.\nWithin the context of real-world traﬃc learning, both rule-based methods [11], supervised\nlearning [25], and unsupervised learning [38] have been applied to identify the interaction primitives.\nDue to the heterogeneity and complexity of traﬃc scenarios, unsupervised learning is a powerful\ntool to identify latent structures in unlabeled traﬃc scenario time series data; the goal is to\norganize the data into homogeneous groups/ clusters [2, 16, 34, 37, 22]. Within automatically\nlearned clusters, interpretable and typical driving behaviors can be obtained and analyzed, e.g.,\nleft/right turns along with multiple attributes including speed, acceleration, yaw rate and side-slip\nangle using Dynamic Time Wrapping (DTW) as a similarity measure [41]. Statistical model-based\napproaches that can learn complex driving behaviors while allowing for encoding domain-knowledge\nare also available. For instance, primitive segments extracted from time series traﬃc data can be\nobtained without specifying the number of categories via Bayesian nonparametric methods based\non Dirichlet processes. They include hierarchical Dirichlet Process Hidden Markov Model (HDP-\nHMM) [33, 37]. Dirichlet process mixtures of Gaussian processes were also successfully employed\nto identify complex multi-vehicle velocity ﬁelds [14, 19].\nGiven the plethora of methods and the need for learning complex interaction patterns in dy-\nnamic domains, it is natural to ask which method one should use. For unsupervised learning, this\nquestion is particularly challenging because one typically works with unlabelled data and without\nimmediately available objective functions for the quality of learned clusters of interactions, espe-\ncially ones which are mathematically represented as a collection of two or more curves taking values\nin a suitable space, as discussed above. In addition, while the problem of devising techniques which\nare free of any tuning parameters is an important one, parameter-free algorithms tend to be not\nrobust. A typical unsupervised learning method still requires some prior knowledge or pre-deﬁned\nparameters (tuning knobs). As a result, clustering results may still be sensitive to these choices.\nThus, even when a method is settled on, it is still an important issue how to handle the various\ntuning knobs and to assess their sensitivity, or stability with respect to changes in the tuning\nparameters.\nIdentifying suitable clustering criteria and analyzing learning stability/sensitivity have received\nmuch attention in data mining and statistical learning literatures. For clustering criteria, there are\nbroadly two categories: internal and external criteria [40]. Internal criteria relies on a similarity\nor dissimilarity measure that may be applied to the data samples. Such measures evaluate how\nalike the members of the same cluster are, how diﬀerent the members of diﬀerent clusters are,\nor some combination of thereof [27, 39]. On the other hand, there is a priori structure how the\ndata should be partitioned, external criteria allow one to compare the clustering results against this\nstructure [27]. Examples include Rand index, mutual information and model-based likelihood-type\nobjectives [39].\nMeanwhile, there is a rich literature on sensitivity analysis that focuses on the impacts of\nchanges in model/method speciﬁcation on the learning outcomes, see, e.g. textbooks [4, 29, 28].\nIf we focus on Bayesian methods or model-based methods, the key issue is on the eﬀect of the\nprior/ model speciﬁcation. While there are a number of variations, most sensitivity analysis tech-\nniques involve model ﬁtting with varying prior/ model speciﬁcations, and assessing the impacts on\nposterior distributions or estimates of parameters of interest. A model is said to be robust if the\nestimates are relatively insensitive to such varying speciﬁcations [15, 30]. Alternatively, instead\nof varying the model parameters one may consider perturbing data: a geometric framework was\ndeveloped to conduct sensitivity analysis with respect to the perturbation of the data, the prior\nand the sampling distribution for a class of statistical models. Within this framework, various geo-\n2\nmetric quantities were studied to characterize the intrinsic structure and eﬀect of the perturbation\n[44].\nTo assess the quality of unsupervised learning methods for temporal dynamic interactions,\nat a high level one may consider the aforementioned methods and frameworks. Moreover, it is\nnecessary to develop a set of suitable metrics for interaction comparison and for assessing the\nstability of an unsupervised learning method with respect to its tuning parameters. Motivated\nby the representation of dynamic vehicle-to-vehicle interactions that arise in the traﬃc learning\ndomain, one has to eﬀectively deal with pairs of aligned functions, i.e., trajectories taking values\nin a suitable space, which is typically non-Euclidean and has high or inﬁnite dimensions. Such\nmetrics must account for the geometric constraints in the physical world as well as the uncertainty\nassociated with the learned patterns.\nTo this end, we introduce a model-free metric on pairs of functions based on a Procrustes-\ntype distance, and an optimal transport based Wasserstein distance metric for comparing between\ndistributions of such pairs of functions. The former metric is critical because it preserves translation\nand rotation invariance, key properties required for capturing the essence of the temporal dynamic\nbetween two autonomous or semi-autonomous agents (e.g., vehicles or robots). The latter metric\nis also appropriate because the result of a clustering algorithm can be mathematically represented\nas the solution of an optimal transport problem [13, 17].\nIn addition to some connection to\noptimal transport based clustering, it is worth noting how our technical contributions are also\ninspired by several other prior lines of work. In particular, Procrustes-type metrics have been\nemployed in generalized Procrustes analysis which solves the problem of reorienting points to a\nﬁxed conﬁguration [12]. Similar metrics have also been successfully used in literature to study such\nproblems of shape preservation [31] as well for alignment of manifolds [36, 23]. In our work, we\nuse it to solve the clustering problem by comparing pairs of curves, each of which may be viewed\nas manifolds on R2.\nFinally, we note that the introduced distance metrics can serve as an objective for assessing\nthe stability of an interaction primitive learning algorithm.\nThey are also used for comparing\nthe outcomes produced by diﬀerent algorithms. Furthermore, they may also be adopted as an\nobjective function to obtain clusters of interactions, and the representative interactions. These\nconcepts and techniques will be introduced in this paper, along with mathematical properties,\nwhile their usefulness will be demonstrated in the analysis of vehicle-to-vehicle interactions that\narise in the Safety Pilot database [3], the world’s largest database for connected vehicles.\nThe paper is organized as follows. In Section 2, we describe a distance metric for pairs of\ntrajectories and explicate its useful mathematical properties. Building on this, Section 3 studies\ndistributions of trajectory pairs, which lead to methods for obtaining and assessing clusters of\ninteractions. Finally, Section 4 illustrates our methods on the clustering analysis of vehicle-to-\nvehicle interactions data.\n2\nA distance metric on temporal interactions\nBecause a temporal interaction between two agents is composed of trajectories, we need to ﬁrst\nformally deﬁne a trajectory.\nLet f : R →R2 denote a trajectory of an object (e.g., vehicles,\nrobots). In particular, f(t) represents the location of the object at time-point t. It suﬃces for our\npurpose to restrict to t ≥0.\nWe can consider all possible trajectories in a similar manner. Deﬁne the set of all possible\ntrajectories as F = {f : [0, ∞) →R2 : f is continuous}.\nThe set of all possible trajectories\nup to time-point t starting from time-point s is denoted by F[s,t) = {f : [s, t) →R2|f ∈F}.\nSimilarly for (t1, . . . , tk) ∈Rk\n+ we will use Ft1,...,tk := {(f(t1), . . . , f(tk)) : f ∈F}. Also, we deﬁne\nF := ∪s,t∈R+F[s,t).\nNext, operations can be deﬁned on these trajectories. For any c ∈R2, and f ∈F we deﬁne\nf +c ∈F as (f +c)(x) = f(x)+c for all x ∈[0, ∞). Similarly, for any orthogonal matrix O ∈R2×2,\ndeﬁne the function O ⊙f ∈F as (O ⊙f)(x) = O · f(x) for all x ∈[0, ∞), where O · f(x) is the\nusual matrix product between matrix O and vector f(x) which have matching dimensions.\nWith these deﬁnitions in place, we now deﬁne an interaction and operations on these interac-\ntions. An interaction is an ordered pair (f1, f2) such that f1, f2 ∈F. We also deﬁne operations\non interactions as well. Let SO(n) be the group of n × n orthogonal matrices with determinant\n+1. For a pair f1, f2 ∈F, we deﬁne O(f1,f2) = {(O ⊙f1, O ⊙f2) : O ∈SO(2)}. Similarly, deﬁne\nC(f1,f2) = {(f1 + c, f2 + c) : c ∈R2}.\n3\n2.1\nRotation and translation-invariant metrics on curves\nTo evaluate the stability and overall quality of clustering, we want a distance metric, d : F2 ×F2 →\nR+, where (F2 = F × F), that has the following properties:\n(a) Distance between two interactions is invariant with respect to the re-ordering of corresponding\ntrajectories, i.e., for f11, f12, f21, f22 ∈F, the following holds:\nd((f11, f12), (f21, f22)) = d((f12, f11), (f21, f22)).\n(b) Distance between a pair of interactions is invariant of starting points of the trajectories\ncomposing the interactions, given the knowledge of the relative distance of the starting points\nof trajectories comprising each interaction. Speciﬁcally, if (f ′\n1, f ′\n2) ∈C(f1,f2) ∪O(f1,f2), then,\nd((f ′\n1, f ′\n2), (f1, f2)) = 0.\nCondition (a) enables the removal of order in a pair of curves in an interaction, while condition\n(b) in essence characterizes rotational and translational invariance of interactions. We will\nhenceforth use (O, C)(f1,f2) to denote the set {(O⊙f1+c, O⊙f2+c) : O ∈SO(2), c ∈R2}. As shown\nin Lemma 1, condition (b) implies that d((f11, f12), (f21, f22)) = d((O⊙f11+c, O⊙f12+c), (f21, f22))\nfor all c ∈R2, O ∈SO(2). This appears to be a reasonable requirement since the exact location and\norientation of interactions should not aﬀect the classiﬁcation of diﬀerent interactions into clusters\ncharacterized by \"primitives\". Note that throughout this paper we only consider non-reﬂective\nrotational transforms, i.e., transforms involving orthogonal matrices, O, such that det(O) = +1.\nLet ρ be a distance metric for F2. We will then construct a metric d satisfying (a) and (b) from\nρ. Deﬁnition 2.1 shows how we can deﬁne d in terms of ρ.\nDeﬁnition 2.1. Deﬁne Procrustes distance\nd((f11, f12), (f21, f22))\n(1)\n:=\ninf\n(f ′\n1,f ′\n2)∈(O,C)(f21,f22)\n\u001a\nmin\n\u001a\nρ((f11, f12), (f ′\n1, f ′\n2)), ρ((f12, f11), (f ′\n1, f ′\n2))\n\u001b\u001b\n.\nFrom the deﬁnition of metric d above, it is clear that (f21, f22) ∈(O, C)(f11,f12)∪(O, C)(f12,f11) ⇐⇒\nd((f11, f12), (f21, f22)) = 0. With that knowledge, we can deﬁne an equivalence relation, ∼, as\n(f11, f12) ∼(f21, f22) ⇐⇒d((f11, f12), (f21, f22)) = 0.\n(2)\nAlthough d is not a proper metric on F2, as Proposition 2.1 shows, d does deﬁne a metric on\nthe quotient space relative to the equivalence relation.\nProposition 2.1. Let ρ be a distance metric on F2 such that for all f11, f12, f21, f22 ∈F,\n(i) ρ satisﬁes, for some function h,\nρ((f11, f12), (f21, f22)) = h((f11, f12) −(f21, f22)).\n(ii) ρ is an inner-product norm.\nThen d given by Eq. (1) is a distance metric on the quotient space F2/ ∼.\nProposition 2.1 also provides a method to build a metric satisfying conditions (a) and (b) above.\nOne way to do so is from a probability measure perspective. In fact, let µ be a probability measure\non [0, ∞). We consider the set of trajectories with integrable Euclidean norm on [0, ∞), i.e., we\nrestrict attention to the following set of trajectories:\nF2(µ) =\n\u001a\nf : [0, ∞) →R2\n\f\f\f\ff is continuous,\nZ ∞\n0\n∥f(x)∥2\n2µ(dx) < ∞,\n\u001b\nwhere ∥· ∥2 is the Euclidean norm in R2. For our purposes, we use ρ as the usual Euclidean metric\non F2\n2(µ) := F2(µ) × F2(µ). Namely, for (f11, f12), (f21, f22) ∈F2\n2(µ), we use\nρ((f11, f12), (f21, f22))2 := ∥f11 −f21∥2\n2 + ∥f12 −f22∥2\n2,\n(3)\n4\nwhere ∥f1i −f2i∥2\n2 =\nR ∞\n0\n∥f1i(x) −f2i(x)∥2\n2µ(dx), i = 1, 2. Here and henceforth we assume that\nthe trajectories f11, f12, f21, f22 all span across the same length of time. Note that this choice of\nmetric satisﬁes the criteria in Proposition 2.1. Also, equivalently, to deﬁne similar rotation and\ntranslation invariant metrics on F[s,t), for any s < t ̸= ∞, we can simply choose any probability\nmeasure µ with support on [s, t).\nProposition 2.2 below provides a simple method to explicitly compute the metric d between\ninteractions, when ρ is given by (3). We will need the following notation:\n(A1) For (f11, f12), (f21, f22) ∈F2\n2(µ), let UDV T be the singular value decomposition for the\nmatrix given by\n2\nX\ni=1\nZ ∞\n0\n\u0000f2i(x) −¯\nf2·(x)\n\u0001 \u0000f1i(x) −¯\nf1·(x)\n\u0001T µ(dx),\nwhere ¯\nf2·(x) =\nR ∞\n0 (f21(x) + f22(x))/2 µ(dx) and\n¯\nf1·(x) =\nR ∞\n0 (f11(x) + f12(x))/2 µ(dx).\nEach of the summands in (A1) form a 2 × 2 dimensional matrix.\nHere, ¯f1(x) denotes the\nelementwise integration of the 2 × 1 vector (f11(x) + f12(x))/2. Moreover, the outer-integral in\neach of the summands in (A1) is an elementwise integral of the 2 × 2 matrix integrand formed by\nmatrix multiplication of the 2 × 1 vector\n\u0000f21(x) −¯\nf2·(x)\n\u0001\nand the 1 × 2 vector\n\u0000f11(x) −¯\nf1·(x)\n\u0001T .\nProposition 2.2. Assume f11, f12, f21, f22 ∈F2(µ). Let UDV T be the singular value decomposi-\ntion as in (A1). Then,\ninf\n(f ′\n1,f ′\n2)∈(O,C)(f21,f22)\n(ρ((f11, f12), (f ′\n1, f ′\n2)))2 = −2 trace\n\u0012\nD\n\u00141\n0\n0\ndet(V T U)\n\u0015\u0013\n+\n2\nX\ni=1\nZ ∞\n0\n\r\r\r\rf2i(x) −¯\nf2·(x)\n\r\r\r\r\n2\n2\n+\n\r\r\r\rf1i(x) −¯\nf1·(x)\n\r\r\r\r\n2\n2\nµ(dx).\nThe optimal O, C that deﬁne the inﬁmum are given by :\n˜O = V T\n\u00141\n0\n0\ndet(V T U)\n\u0015\nU,\n˜C = ¯\nf1·(x) −˜O ·\n\u0000 ¯\nf2·(x)\n\u0001\n.\n(4)\nThe proof of the proposition is discussed in Section A.2. The problem discussed in Propos-\ntion 2.2 is a version of the well-known least root mean square deviation problem. It was ﬁrst\nsolved by the Kabsch algorithm [20, 21]. A more computationally eﬃcient method to compute the\noptimal O, C was later obtained using the theory of quarternions [18, 5].\n3\nQuantifying distributions of primitives\nThe metric deﬁned above can be used to obtain clusters of interactions, in addition to evaluating\nthe overall quality and stability of a particular clustering method. Our starting point is to note that\nthe problem of clustering or summarizing interactions can be formalized as ﬁnding a discrete dis-\ntribution on the space of interactions. More speciﬁcally, one needs to obtain a discrete probability\ndistribution on interactions, where each supporting atom represents a typical interaction (namely,\nan interaction primitive) and the mass associated with each atom represents the proportion of a\ncluster. From this perspective, an objective that naturally arises is to minimize a distance from\nthe empirical distribution of interactions to a discrete probability measures with a ﬁxed number,\nsay k, of supporting atoms, which represent the primitives. An useful tool for deﬁning distance\nmetrics on the space of distributions arises from the theory of optimal transport [35].\nOptimal transport distances enable comparisons of distributions in arbitrary structured and\nmetric spaces by accounting for the underlying metric structure.\nThey have been increasingly\nadopted to address clustering in a number of contexts [26, 13, 17]. For instance, it is well-known that\nthe problem of determining an optimal ﬁnite discrete probability measure minimizing the second-\norder Wasserstein distance W2 to the empirical distribution of the data is directly connected to the\nk-means clustering problem (discussed in Section III in details). Inspired by this connection, we\n5\nwill seek to summarize the distribution of interactions appropriately. To this end, we will deﬁne\nWasserstein distances for distributions of interactions as follows, by accounting for the metric\nstructure developed in the previous section.\nLet d be a distance metric on F2\n2(µ)/ ∼, where ∼is the equivalence relation deﬁned in Eq. (2).\nFix [(f11, f12)] ∈F2\n2(µ)/ ∼. Here, [(f11, f12)] denotes the equivalence class corresponding to\ninteraction (f11, f12) relative to the equivalence relation ∼and F2\n2(µ)/ ∼denotes the collection of\nall such classes of interactions. Let P(F2\n2(µ)/ ∼) denote all probability measures on F2\n2(µ)/ ∼. For\na ﬁxed order r ≥1, deﬁne the following subset of P(F2\n2(µ)/ ∼) subject to a moment-type condition\nusing the metric d:\nPr(F2\n2(µ)/ ∼) :=\n\u001a\nG ∈P(F2\n2(µ)/ ∼)|\nZ\ndr([(f21, f22)], [(f11, f12)])dG([(f21, f22)]) < ∞\n\u001b\n.\nThis class of probability measures can be shown to be independent of the choice of [(f11, f12)] and\ntherefore the collection of order-r integrable probability measures on the quotient space F2\n2(µ)/ ∼\nis independent of the choice of the base class [(f11, f12)]. We arrive at the following distance metric\nto compare between probability measures on the quotient space F2\n2(µ)/ ∼. This is an instantiation\nof Wasserstein distances that arise in the theory of optimal transport in metric spaces [35].\nDeﬁnition 3.1 (Wasserstein distances). Let F, G ∈Pr(F2\n2(µ)/ ∼). The Wasserstein distance\nof order r between F and G is deﬁned as:\nWr(F, G) :=\n\u0012\ninf\nπ∈Π(F,G)\nZ\ndr([(f11, f12)], [(f21, f22)])dπ([(f11, f12)], [(f21, f22)])\n\u00131/r\n,\nwhere Π(F, G) is the collection of all joint distributions on F2\n2(µ)/ ∼×F2\n2(µ)/ ∼with marginals F\nand G.\n3.1\nWasserstein barycenter and k-means clustering\nIn this section, we present the Wasserstein barycenter problem and highlight its connection to the\nk-means formulation.\nWasserstein barycenter problem\nFixing the order r = 2, let P1, P2, . . . , PN\n∈P2(F2\n2(µ)/ ∼) be probability measures on F2\n2(µ)/ ∼. Their second-order Wasserstein barycenter\nis a probability measure ¯PN,λ such that\n¯PN,λ =\nargmin\nP ∈P2(F2\n2(µ)/∼)\nN\nX\ni=1\nλiW 2\n2 (P, Pi).\nThe Wasserstein barycenter problem was ﬁrst studied by [1]. When Pi are themselves ﬁnite discrete\nprobability measures on arbitrary metric spaces, eﬃcient algorithms are available for obtaining\nlocally optimal solutions to the above [6].\nk-means clustering problem\nThe k-means clustering problem, when adapted to obtaining\nclusters in a non-Euclidean space of interactions, can be viewed as solving for the set S of k elements\n[(g11, g12)], . . . , [(gk1, gk2)] ∈F2\n2(µ)/ ∼such that, given samples (f11, f12), . . . , (fn1, fn2) ∈F2(µ)\nS = argmin\nT :|T |≤k\nn\nX\ni=1\ninf\n[(f ′\n1,f ′\n2)]∈T d2([(fi1, fi2)], [(f ′\n1, f ′\n2)]).\n(5)\nIt can be shown that this is equivalent to ﬁnding a discrete measure P which solves the following\nfor the choice r = 2:\ninf\nP ∈Ok(F2\n2(µ)/∼) Wr(P, Pn),\n(6)\n6\nwhere Pn is the empirical measure on F2\n2(µ)/ ∼, i.e., Pn places mass 1/n on equivalence class\nsample [(fi1, fi2)] for all i = 1, . . . , n, and Ok(F2\n2(µ)/ ∼) is the set of all measures in F2\n2(µ)/ ∼with\nat most k support points. (It is interesting to note that Eq. (6) is a special case of the Wasserstein\nbarycenter problem for N = 1 and r = 2.)\nAt the high level our approach is simple: we seek to summarize the empirical data distribution\nof interactions using a k-means-like approach, but there are several challenges due to the complex\nmetric structure exhibited by the non-Euclidean space of interactions. Finding the exact solution\neven in the simplest cases is an NP-hard problem. The most common method to approximate\nthe solution is the use of iterative steps similar to Lloyd’s algorithm [24] for solving the Euclidean\nk-means problem.\nHowever, the computation of cluster centroids at each iteration of Lloyd’s\nalgorithm when applied to the non-Euclidean metric d is non-trivial. Moreover, the computation of\npairwise distances between equivalence classes of interactions is non-trivial. In the next subsection\nwe present some approximate solutions to Eq. (5).\n3.2\nApproximations for non-Euclidean k-means clustering\nThe primary objective for this section is to obtain a robust representation for the distribution over\ninteraction primitives. Although the empirical distribution of interactions provides an estimate\nof the distribution over primitives, it suﬀers from lack of robustness guarantees.\nA robust k-\napproximation for the empirical distribution is formalized by Eq. (6). For order r = 2 this is\nequivalent to solving the k-means problem given by Eq. (5) for the interaction scenarios. The\ncomputational problem for computing exact centroids of k-means clusters is cumbersome and\ngenerally not solvable for arbitrary distance metrics d. To overcome such challenges we propose\nthree separate methods to obtain approximate solutions to Eq. (5). The ﬁrst approach is a standard\napplication of multi-dimensional scaling technique. The second and third approaches are based on\nother geometric ideas to be described in the sequel.\n3.2.1\nMultidimensional Scaling\nMulti-dimensional scaling (MDS) provides a way to obtain a lower dimensional representation of\nhigh-dimensional and/or non-Euclidean space elements while approximately preserving some dis-\ntance measure among data points. Given a distance (a.k.a. dissimilarity) matrix D = (dij)1≤i,j≤n,\nwhich collects all pairwise distance among the n data points using a notion of distance such as\nmetric d described earlier, MDS ﬁnds points x1, . . . , xn ∈Rm, for some small dimension m, such\nthat\n{x1, . . . , xn} =\nargmin\ny1,...,yn∈Rm\nn\nX\ni,j=1\n(∥yi −yj∥−dij)2\n(7)\nIn order to apply the k-means clustering technique to our MDS representation, the following\nimplicit assumption is required:\n(C1) Each of the cluster centroids for the k-means problem corresponds to an interaction in the\ndata sample.\nGiven (C1), Eq. (5) can be reformulated as follows.\nApproximate k-means\nGiven interaction samples (f11, f12), . . . , (fn1, fn2) ∈F2(µ), ﬁnd a set\nS ⊂{1, . . . , n} such that,\nS = argmin\nT :|T |≤k\nn\nX\ni=1\nmin\nj∈T d2([(fi1, fi2)], [(fj1, fj2)]).\n(8)\nThe approximate k-means problem in Eq. (8) diﬀers from the k-means problem (5) in that instead\nof ﬁnding primitives that are the global minimizer (and hence correspond to the cluster means), we\nlook for the primitive that is closest to all other interactions in its cluster. The advantage of this\napproach is that we do not need explicitly the inverse map that goes from the MDS representation\nback to the interaction space. We summarize this approach as Algorithm 1 in the following.\n7\nAlgorithm 1 Clustering interactions\nInput: interaction sample {(fi1, fi2)}n\ni=1\nOutput: k interaction primitives\n1: Obtain x1, . . . , xn as solution of MDS Eq. (7) with dij = d([(fi1, fi2)], [(fj1, fj2)]).\n2: Perform k-means on x1, . . . , xn to obtain the centroids.\n3: Approximate the centroids with points xi ∈Rm which are closest in ∥· ∥distance to the centroids, Γ1, Γ2, . . . , Γk.\n4: Return as primitives the k interaction sample corresponding to these approximate centroids, {(gj1, gj2)}k\nj=1.\n3.2.2\nGeometric Approximations\nA major computational challenge to solving Eq. (8) lies in the SVD decomposition of the Procrustes\ndistances (Eq. (1)) relative to each pair of interactions. There require O(n2) such decomposition.\nTo avoid this, we instead consider a geometric approximation of the Procrustes distance, inspired\nby work from the ﬁeld of morphometrics [32].\nConsider two interactions (fi1, fi2) and (fj1, fj2). Then, by an application of triangle inequality,\ninf\n(f ′\n1,f ′\n2)∈(O,C)(fj1,fj2)\nρ((fi1, fi2), (f ′\n1, f ′\n2))\n(9)\n=\ninf\nO1∈SO(2),c1∈R2 ρ((fi1, fi2), O1 ⊙(fj1, fj2) + c1)\n≤\ninf\nO1∈SO(2),c1∈R2 ρ((f11, f12), O1 ⊙(fj1, fj2) + c1)\n+\ninf\nO2∈SO(2),c2∈R2 ρ((f11, f12), O2 ⊙(fi1, fi2) + c2).\nEq. (9) shows that knowledge of optimal rotational matrices and translation vectors for computing\nthe distances d([(fi1, fi2)], [(f11, f12)]) and\nd([(fj1, fj2)], [(f11, f12)]) can provide an upper bound for computing the distance between the ith\nand jth pair of interactions. Therefore, we can provide a reasonable upper bound for all the n2\npairwise distances by simply performing only O(n) SVD decompositions. This approach, which\nwe call the ﬁrst geometric approximation, is summarized in Algorithm 2.\nAlgorithm 2 First Geometric Approximation\nInput: {(fi1, fi2)}n\ni=1\nOutput: k centroids\n1: for i = 1, 2, . . . , n do\n2:\nCenter and reorient (fi1, fi2) to (f11, f12) using Algorithm 4.\n3: end for\n4: Perform k-means on the centered and oriented {(fi1, fi2)}n\ni=1 to obtain the centroids, {(Γj1, Γj2)}k\nj=1.\n5: Return the centroids, {(Γj1, Γj2)}k\nj=1.\nAlgorithm 3 Second Geometric Approximation\nInput: {(fi1, fi2)}n\ni=1\nOutput: k centroids\n1: Randomly assign interaction samples {(fi1, fi2)}n\ni=1 to k clusters. Let zi indicate the cluster assignment.\n2: while k-means convergence criterion has not been met do\n3:\nfor k′ = 1, 2, . . . , k do\n4:\nCenter and orient all interaction samples (fi1, fi2) to (fik′ 1, fik′ 2) using Algorithm 4 if (fik′ 1, fik′ 2) is the ﬁrst\ninteraction sample such that zi = k′ for i = 1, 2, . . . n. Denote these oriented and centered samples as (f ′\ni1, f ′\ni2)(t).\n5:\nCompute the centroid for cluster j, (Γj1, Γj2), such that for t = 1, 2, . . . , tm,\n(Γj1, Γj2)(t) =\n1\n# (zi = k)\nX\ni:zi=k\n(f ′\ni1, f ′\ni2)(t)\n6:\nend for\n7:\nfor i = 1, 2, . . . , N do\n8:\nfor j = 1, 2, . . . , k do\n9:\nCenter and orient (fi1, fi2) to (Γj1, Γj2).\n10:\nCompute the L2 distance between the centered and oriented (fi1, fi2) and (Γj1, Γj2).\n11:\nend for\n12:\nSet zi = j if the smallest computed distance is from the centroid of cluster j.\n13:\nend for\n14: end while\n15: Return the centroids, {(Γj1, Γj2)}k\nj=1.\nHowever, this gain in computation eﬃciency is also accompanied by a loss of statistical ef-\nﬁciency. To mitigate this tension between computational and statistical eﬃciency we propose a\n8\n(a)\nMultidim.\nScaling (cf.\nSec-\ntion 3.2.1)\n(b) First geometric\napprox.\n(cf.\nSec-\ntion 3.2.2)\n(c) Second geomet-\nric approx. (cf. Sec-\ntion 3.2.2)\n(d) Polynomial co-\neﬃcients (cf.\nSec-\ntion 4.2)\n(e) DTW cost ma-\ntrix (cf. [38])\nFigure 1: Silhouette plots for 5 clusters obtained under various approaches:\nTotal within\nSquare Distance\nCluster 1\nAverage within\nSquare Distance\nCluster 5\nAverage within\nSquare Distance\nCluster 1\nAverage between\nSquare Distance\nCluster 5\nAverage between\nSquare Distance\nMDS\n10.68\n1.74e-03 (1.53e-05)\n1.23e-02 (4.60e-04)\n1.55e-02 (4.57e-04)\n1.17e-01 (5.13e-03)\nFirst Geometric Approx.\n13.32\n9.50e-04 (2.96e-05)\n6.33e-03 (3.92e-04)\n5.01e-02 (4.97e-03))\n1.12e-02 (2.92e-04)\nSecond Geometric Approx.\n274.27\n3.97e-03 (1.42e-04)\n8.01e-02 (1.38e-03)\n1.86e-02 (8.43e-04)\n1.84e-02 (1.51e-03)\nSpline Coeﬃcients\n222.56\n5.02e-02 (3.91e-03)\n9.40e-02 (1.05e-02)\n5.95e-02 (4.27e-03)\n2.05e-01 (2.33e-02)\nDTW Matrices\n201.12\n3.12e-02 (4.51e-03)\n9.00e-03 (3.41e-04)\n5.01e-02 (6.30e-03)\n9.38e-03 (3.79e-04)\nTable 1: A table of the quantities from Eq. (8), Eq. (11), and Eq. (13)) for each method’s cluster\nwith the most interaction (Cluster 1) and cluster with the fewest (Cluster 5). Variance of these\ndistances are included in parentheses. Note that the Procrustes distances were normalized so that\nthe maximum distance between any interaction is 1.\nsecond geometric approximation which performs the approximation of Algorithm 2 in batch form,\nwhere the batches comprise of the respective clusters. This procedure is described in Algorithm 3.\n4\nExperimental Results\nIn this section we provide a demonstration of our methods for unsupervised learning of vehicle\ninteractions. In particular, we will evaluate the quality and stability of clustered primitives ex-\ntracted from vehicle-to-vehicle interactions based on real-world experiments conducted in Ann\nArbor, Michigan. In the literature for this application domain, a real-time interaction between\ntwo vehicles is also alternatively referred to as an encounter. In practice, the interactions between\nvehicles are represented by multi-dimensional time series of varying duration, which need to be\nfurther segmented into shorter time duration via suitable data processing techniques.\n4.1\nVehicle-to-vehicle (V2V) interaction data processing\nWe work with a real-world V2V interaction data set which is extracted from the naturalistic driving\ndatabase generated by experiments conducted as part of the University of Michigan Safety Pilot\nModel Development (SPMD) program. In these experiments, dedicated short range communica-\ntions (DSRC) technology was utilized for the communication between two vehicles. Approximately\n3,500 equipped vehicles have collected data for more than 3 years. Latitude and longitude data of\neach vehicle was recorded by the by-wire speed sensor. The on-board sensor records data in 10Hz.\nTo investigate basic V2V interaction behaviors, a subset of 1400 driving scenarios was further\nﬁltered out from the SPMD’s database. Each scenario consists of a time series of GPS locations\nand speeds of a pair of vehicles, which are mutually less than 100 metres apart. For our purposes,\nit is natural to posit that each scenario is inclusive of multiple shorter encounters through diﬀerent\ntime duration. Pre-processing of the data was therefore aimed at segmenting each scenario into\nmore basic driving segments.\nThese segments constitute basic building blocks from which we\ncan meaningfully learn interaction primitives using a variety of clustering algorithms. The issue\nof segmentation is akin to identifying change points on functional curves embedded in a higher\n9\n(a) Cluster 1\n(b) Cluster 2\n(c) Cluster 3\n(d) Cluster 4\n(e) Cluster 5\n(f) All clusters\nFigure 2: Plot of the three most typical interactions organized from the cluster with the most\ninteraction to the cluster with the fewest for clustering using Multidimensional scaling (cf. Section\n3.2). The interactions are centered and oriented using Algortihm 4 to (t, 2t −1, −t, 1 −2t) for\nt = 0, 0.01, . . . , 1. The solid shapes and shapes with a black interior indicate the starting location\nof each interaction. The dot with the black interior indicates the second trajectory. Midpoints are\nindicated by dots ﬁlled in with a grey interior. Diﬀerent shapes indicate diﬀerent V2V interactions.\nNote that the individual cluster interactions plots are placed on their own scales.\ndimensional space.\nWe consider two diﬀerent segmentation schemes for V2V interaction data\nprocessing.\nThe ﬁrst segmentation scheme is detailed in Appendix B. It will be called a two-step spline\napproach, which goes as follows. Given an encounter, we ﬁt it with cubic splines in two main\nsteps. Here, the change points act as the knots. The ﬁrst step involves identifying a large number\nof probable change points via a binary search approach to add change points if adding change points\nreduced the squared error between the ﬁtted values and the observed data. The next step involves\na single forward pass to remove excess change points from consideration in order to minimize the\nsquared error with a penalty for the number of change points. We then segment each interaction\nat the knots. This segmentation technique created a set of 5622 basic V2V interactions to work\nwith.\nThe second segmentation scheme is considerably more complex, as it is derived from a non-\nparametric Bayesian model for time series data, the sticky Hierarchical Dirichlet Process Hidden\nMarkov Model (HDP-HMM) [9]. This model extends the basic HMM by allowing the number of\nhidden Markov states to be unbounded, while encouraging the Markov process to be \"sticky\", that\nis, the state tends to be constant for a period of time (e.g., a car tends to go straight after a long\nperiod of time). For model selection, as we will elaborate later, one of the hyperparameters of\nsticky HDP-HMM is varied. Consequently, the number of basic V2V interactions varied from 8779\nto 8829 with an average of 8799 interactions.\n10\n(a) In-cluster distances\nfrom mean interactions\n(b) In-cluster distances\nfrom\ntypical\ninterac-\ntions\n(c) All distances from\nmean interactions\n(d) All distances from\ntypical interactions\n(e) In-cluster distances\nfrom the mean interac-\ntions\n(f) In-cluster distances\nfrom the typical inter-\nactions\n(g) All distances from\nmean interactions\n(h) All distances from\nthe typical interactions\n(i)\nIn-cluster\nfrom\nmean interactions\n(j) In-cluster distances\nfrom\ntypical\ninterac-\ntions\n(k) All distances from\nmean interactions\n(l) All distances from\ntypical interactions\nFigure 3: Line plots showing the distribution (frequency) of interaction distance to either the\ncluster mean or the typical interaction. Clusters are obtained by the ﬁrst geometric method in\nrow 1, the second geometric method in row 2, and the cubic spline coeﬃcients based method in\nrow 3 (cf. Section 4.2). The clusters are numbered according to the number of interactions so that\nCluster 1 has the most and Cluster 5 has the fewest. Note that the range for the y-axis are much\nlarger on the left plots compared to the right plots.\n4.2\nCluster analysis of V2V interactions\nWe evaluate the clustering of primitives qualitatively and quantitatively. For the former, silhouette\nplots are useful – the silhouette, s(i), for interaction i is deﬁned as following:\ns(i) =\nb(i) −a(i)\nmax(a(i), b(i)).\nHere, a(i) is the average Procrustes distance between interaction i and all other interactions in the\nsame cluster as interaction i, while b(i) is the average Procrustes distance from interaction i to\nthose in another cluster. The cluster used for b(i) is the one that minimizes this average distance.\nBy deﬁnition, the silhouette ranges from -1 to 1. It will be close to 1 if b(i) is signiﬁcantly larger\nthan a(i) and -1 if a(i) is signiﬁcantly larger than b(i). Thus, the quality of the clustering for\ninteraction i decreases as s(i) decreases. Plotting the silhouettes for all interactions provides a\nqualitative way to determine how the clustering is performing because if most silhouettes are close\nto 1, the clustering is performing well.\nforming because if most silhouettes are close to 1, the clustering is performing well.\nFor a more quantitative way to examine the clustering, we look at the quantity in Eq. (5) and\n11\nEq. (8). Naturally, the method that reduces that quantity the most should be selected. We can\nalso break down that quantity further by the contribution of each cluster. Speciﬁcally, suppose zi\nindicates the cluster membership. If (Γj1, Γj2) is the cluster’s mean, then we report the following:\n1\n# (zi = k)\nX\ni:zi=k\nd2([(fi1, fi2)], [(Γj1, Γj2)]).\n(10)\nAlternatively, if interaction j minimizes d2([(fi1, fi2)], [(gj1, gj2)]) for all zi, zj = k, the approximate\nversion is the following:\n1\n# (zi = k)\nX\ni:zi=k\nd2([(fi1, fi2)], [(gj1, gj2)]).\n(11)\nTo compare clusters with diﬀerent number of interactions, we choose to divide it by the size of the\ncluster. Finally, like the silhouette, it might also be helpful to compare this against the average\nsquare Procrustes distance of one cluster’s mean V2V interaction and the interactions of all other\nclusters. In other words, we report the following if the cluster’s mean interactions are recoverable:\n1\n# (zi ̸= k)\nX\ni:zi̸=k\nd2([(fi1, fi2)], [(Γj1, Γj2)]).\n(12)\nAgain, we can report the approximate version instead:\n1\n# (zi ̸= k)\nX\ni:zi̸=k\nd2([(fi1, fi2)], [(gj1, gj2)]).\n(13)\nNote that the silhouette is more stringent because for the silhouette, we average only the distance\nfrom interactions of the nearest cluster for an observation.\nWe then made these silhouette plots and calculated the quantity for ﬁve diﬀerent methods.\nFirst, we wanted to evaluate how well the three clustering approaches, namely the Multidimen-\nsional Scaling approximation and the ﬁrst and second geometric approximations of the Procrustes\ndistance, introduced in Section 3, performed. Next, because we segmented encounters using splines,\nwe wanted to examine the quality of k-means clustering based on the coeﬃcients of the cubic\nsplines ﬁtted to these interactions.\nIn other words, suppose for interaction i, we ﬁt the cubic\nspline c1\ni10 + c1\ni11t + c1\ni12t2 + c1\ni13t3 to (gi1)1. We do the same for (gi2)2, (gi2)1, and (gi2)2. Then,\nwe perform k-means on the vectors, {{c1\ni1ℓ}3\nℓ=0, {c2\ni1ℓ}3\nℓ=0, {c1\ni2ℓ}3\nℓ=0, {c2\ni2ℓ}3\nℓ=0}n\ni=1. We call this ap-\nproach spline coeﬃcient clustering. Finally, dynamic time warping (DTW) is a standard approach\nto match curves – in Wang et. al [38], k-means clustering is performed on the DTW matrices that\nmatch one trajectory to another for each V2V interaction. This is another approach we wish to\nevaluate.\nWe focus on reporting for the case k = 5 for the moment, while the analysis can be replicated\non other choices of k. The results for encounters segmented by the two step approach can be seen in\nFigure 1 and Table 1. Accordingly, the MDS approach outlined in Algorithm 1 appears to perform\nthe best whereas the spline coeﬃcients and DTW matrices perform the worst. The total within\nsquare distance from Eq. (8) for the MDS approach in Table 1 is smallest and the silhouette plots in\nFigure 1 look reasonable. Indeed, even though the ﬁrst geometric approximation’s average within\nsquare distance for the clusters with most and fewest interactions is smaller and the average between\nsquare distance is comparable or larger, the silhouette plot shows us that the MDS approach does\nsigniﬁcantly better with the cluster with the second and third largest cluster. The silhouette values\nare much higher for that cluster than the ﬁrst geometric approximation.\nFor interpretability, one may be interested in visualizing typical interactions from each clusters.\nTake the MDS method. There are various interesting observations in Fig. 2. For instance, the\ntwo clusters with most interactions are interactions in which the vehicles do not move far from\neach other.\nOn the other hand, the other clusters have interactions in which the opposite is\ntrue. Further, while only the cluster with the fewest interactions have vehicles going in the same\ndirection, there are variation in how the vehicles are moving in opposite directions. Figure 6 in the\nAppendix shows the three most typical encounters for all clustering methods.\nOne can look more deeply into the distribution of interactions in each cluster, which is revealed\nby Figure 3.\nThe left two plots show the proportion of interactions in each cluster a certain\n12\n(a) All k and β between\n2 and 20\n(b) All β between 10\nand 20 and k between\n10 and 20\n(c) All β between 2 and\n20 and k between 10\nand 20\n(d) All β between 2 and\n20 and k between 10\nand 20\nFigure 4: The left two plots show heatmaps of the statistic introduced in Eq. (15) for encounters\nsegmented by the two-step spline approach (cf. Appendix B) and then clustered via MDS (cf.\nSection 3.2). The right two show changes in this statistic.\n(a) MDS clustering (cf. Section\n3.2) applied to two-step spline\nsegmented encounters (cf. Ap-\npendix B).\n(b) DTW matrix clustering ap-\nplied to encounters segmented\nby BNP (cf. [38]).\n(c) Two step spline segmented\nencounters\nclustered\nusing\nprimitives extracted from BNP\nsegmented encounters clustered\nusing DTW matrices (cf. 4.3).\nFigure 5: Heatmaps of the statistic given by Eq. (15) for non-reﬂective Procrustes distance for\nk ≥10 across diﬀerent methods.\ndistance away from the mean or typical interaction for each cluster. On the other hand, the right\ntwo plots show the proportion of interactions from the entire data set a certain distance away\nfor each cluster’s mean or typical interaction. The ﬁrst geometric approximation plot is ideal.\nWhile the other methods have a cluster that peaks higher near zero, the left two plots show higher\npeaks near zero across all clusters, indicating that most interactions in the cluster are close to the\ntypical or mean interaction. On the other hand, the right two plots show a peak near zero and\nthen plateau for a bit before decreasing to zero. This supports what we see in the silhouette plot.\nThe plateau demonstrates that the clusters are well separated because interactions outside the\ncluster are further away. Because of the left two plots, the peak near zero likely comes from the\ninteractions assigned to that cluster. It is likely that the plots for the MDS will look similar to the\nﬁrst geometric approximation plots. For the second geometric approximation, the interaction plots\nfor the mean interaction are ideal. However, outside of the largest cluster, the typical interaction to\ncluster interaction plots peak at values not near zero or plateau for ranges of distances. Meanwhile,\nthe plots for polynomial coeﬃcient exhibit peculiar peaks or plateaus in the left plots. These peaks\nare slightly dampened when using the typical interaction in place of the mean interaction.\n4.3\nStability Evaluation\nWe had to develop a statistic for stability based on our distance metric. Consider the k-means\nproblem in a Euclidean space. Let x1, . . . , xn ∈Rd be points in an Euclidean space belonging to\nclusters {1, . . . , K}. The cost function relative to the k-means problem is given by\nmin\n{Γ1,...,Γk,z1,...,zn}\n1\nn\nk\nX\nj=1\nX\ni:zi=j\n∥xi −Γj∥2.\n13\nThe above cost function can also be written as:\nmin\n{z1,...,zn}\n1\n2n\nK\nX\nk=1\nX\ni,j:zi,zj=k\n∥xi −xj∥2.\n(14)\nEq. (14) provides a way to partition the dataset {x1, . . . , xn} so as to optimize the within cluster\ndistance. We then use a measure equivalent to (14) to evaluate the stability of algorithms. Namely,\nif we use the same notation as before, then the stability of the algorithm is measured by computing\n1\n2n\nK\nX\nk=1\nX\ni,j:zi,zj=k\nd2((fi1, fi2), (fj1, fj2))),\n(15)\nfor varying values of tuning parameters, where d((fi1, fi2), (fj1, fj2))) is the metric introduced in\nEq. (2).\nWe then calculated this statistic for the MDS approach outlined in Section 3.2. Applying k-\nmeans to the MDS projection of the (non-reﬂective) Procrustes distance requires the speciﬁcation\nof the following parameters: dimension of the projection, β, and the number of clusters, k. The\nresults for β ∈[2, 20] and k ∈[2, 20] can be seen in Figure 4. From the heatmap, the MDS approach\nis particularly stable for k ≥15 and β > 5. This is further supported by examining the change in\nthe statistic introduced in Eq. (15). This makes sense because increasing the dimension for the\nMDS representation provides a better representation of the pairwise distance. On the other hand,\nincreasing k also leads to greater stability. However, the scales in the ﬁgure suggests that most of\nthe instability occurs when k ≤10.\nAs before, we proceed to compare among methods and data sets. First, following Wang and\nZhou [38], we examined the stability of the DTW approach for the encounters segmented by\nsticky HMM-HDP. While there are more parameters to consider, we empirically investigated the\nresults with α and c ﬁxed to 2 and 100 respectively and allowed γ and k to vary between [2, 19]\nand [2, 20]. Because changing γ gives us new primitives, we had to interpolate and recalculate\nthe Procrustes distance for each set of primitives. Second, we wanted to inspect the stability of\n\"transferring\" primitives. In other words, let {(g′\nj1, g′\nj2)}k\nj=1 be the primitives derived from applying\nBNP to segment encounters and using the DTW matrices to cluster them and {(fi1, fi2)}n\ni=1 be\nthe interactions extracted from the encounters using our two-step approach outlined in Appendix\nB. We assign interaction i to cluster j if\nj = argminj′=1:kd((fi1, fi2), (g′\nj1, g′\nj2)).\nHere, d((fi1, fi2), (g′\nj1, g′\nj2)) is the distance introduced in Eq. (1). The results can be seen in Figure\n5. For DTW, the results are similar to the results before with respect to k and may even be better.\nOn the other hand, as seen in the scales in Figure 5, we see that there is greater instability in both\nthe range and the pattern when we \"transfer\" primitives. Further, unlike before, this instability\npersists even as k increases. This could be due to the more extreme values in the BNP primitive\ndata set. As a result, there might be primitives that do not exist in the data set segmented by\nthe two step spline approach. This could mean that as we increase k, we might not be adding\ncentroids used to cluster the data. In addition, the ones that do exist might be inﬂuenced by these\nmore extreme values. This might be why the values are unstable for lower values of k.\n5\nConclusion\nWe developed a distance metric for the space of trajectory pairs that is invariant under translation\nand rotation. By using it to measure the distance between distributions, we could also use this met-\nric for clustering and for evaluating a variety of unsupervised techniques for interaction learning.\nThe distance metric and geometric approximation methods that we introduced help to address the\nchallenges for robust learning of non-Euclidean quantities that represent temporally dynamic in-\nteractions. These techniques were demonstrated by the unsupervised learning of vehicle-to-vehicle\ninteractions. An interesting direction for our work is to extend the metric based representation and\ngeometric algorithms to the multiple-vehicle interaction setting, and general multi-agent settings.\nThe challenge is the ﬁnd a right metric or a family of metrics which are both meaningful and\ncomputationally tractable for a number of learning tasks of interests.\n14\nAcknowledgements\nToyota Research Institute (TRI) provided funds to assist the authors with their research but this\narticle solely reﬂects the opinions and conclusions of its authors and not TRI or any other Toyota\nentity. Dr. Nguyen is also partially supported by grants NSF CAREER DMS-1351362 and NSF\nCNS-1409303.\nReferences\n[1] M. Agueh and G. Carlier. Barycenters in the Wasserstein space. Journal on Mathematical\nAnalysis, 43(2):904–924, 2011. 6\n[2] A. Bender, G. Agamennoni, J. R. Ward, S. Worrall, and E. M. Nebot.\nAn unsupervised\napproach for inferring driver behavior from naturalistic driving data. IEEE Trans. Intell.\nTransport. Syst., 16(6):3325–3336, 2015. 2\n[3] D. Bezzina and J. Sayer. Safety pilot model deployment: Test conductor team report. Report\nNo. DOT HS, 812:171, 2014. 3\n[4] S. Chatterjee and A. S. Hadi, editors. Sensitivity Analysis in Linear Regression. Wiley Series\nin Probability and Statistics. John Wiley & Sons, Inc., Hoboken, NJ, USA, Mar. 1988. 2\n[5] E. A. Coutsias, C. Seok, and K. A. Dill. Using quaternions to calculate rmsd. Journal of\nComputational Chemistry, 25:1849–1857, 2004. 5\n[6] M. Cuturi and A. Doucet. Fast computation of wasserstein barycenters. In International\nConference on Machine Learning, 2014. 6\n[7] W. Ding, W. Wang, and D. Zhao. Multi-vehicle trajectories generation for vehicle-to-vehicle\nencounters. arXiv preprint arXiv:1809.05680, 2018. 2\n[8] J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. S. Torr, P. Kohli, and S. Whiteson.\nStabilising experience replay for deep multi-agent reinforcement learning, 2017. 1\n[9] E. Fox, E. Sudderth, M. I. Jordan, and A. Willsky. The sticky HDP-HMM: Bayesian nonpara-\nmetric hidden Markov models with persistent states. Technical Report P-2777, MIT LIDS,\n2009. 10\n[10] E. Frazzoli, M. A. Dahleh, and E. Feron.\nManeuver-based motion planning for nonlinear\nsystems with symmetries. IEEE Transactions on Robotics, 21(6):1077–1091, 2005. 1\n[11] E. Frazzoli, M. A. Dahleh, and E. Feron.\nManeuver-based motion planning for nonlinear\nsystems with symmetries. IEEE transactions on robotics, 21(6):1077–1091, 2005. 2\n[12] J. Gower. Generalized procrustes analysis. Psychometrika, 40(1):33–51, 1975. 3\n[13] S. Graf and H. Luschgy. Foundations of quantization for probability distributions. Springer-\nVerlag, New York, 2000. 3, 5\n[14] Y. Guo, V. V. Kalidindi, M. Arief, W. Wang, J. Zhu, H. Peng, and D. Zhao. Modeling multi-\nvehicle interaction scenarios using gaussian random ﬁeld. arXiv preprint arXiv:1906.10307,\n2019. 2\n[15] P. Gustafson. Local Robustness in Bayesian Analysis. In D. R. Insua and F. Ruggeri, editors,\nRobust Bayesian Analysis, Lecture Notes in Statistics, pages 71–88. Springer, New York, NY,\n2000. 2\n[16] R. Hamada, T. Kubo, K. Ikeda, Z. Zhang, T. Shibata, T. Bando, K. Hitomi, and M. Egawa.\nModeling and prediction of driving behaviors using a nonparametric bayesian method with ar\nmodels. IEEE Trans. Intell. Veh., 1(2):131–138, 2016. 2\n[17] N. Ho, X. L. Nguyen, M. Yurochkin, H. H. Bui, V. Huynh, and D. Phung. Multilevel cluster-\ning via wasserstein means. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, pages 1501–1509. JMLR. org, 2017. 3, 5\n15\n[18] B. K. P. Horn. Closed-form solution of absolute orientation using unit quaternions. Journal\nof the Optical Society of America, 4:629–642, 1986. 5\n[19] J. Joseph, F. Doshi-Velez, A. S. Huang, and N. Roy. A bayesian nonparametric approach to\nmodeling motion patterns. Autonomous Robots, 31(4):383, 2011. 2\n[20] W. Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallograph-\nica, 32:922–923, 1976. 5\n[21] W. Kabsch. A discussion of the solution for the best rotation to relate two sets of vectors.\nActa Crystallographica, 34:827–828, 1978. 5\n[22] T. W. Liao. Clustering of time series data—a survey. Pattern recognition, 38(11):1857–1874,\n2005. 2\n[23] Y. Lipman, R. Al-Aifari, and I. Daubechies. The continuous procrustes distance between two\nsurfaces. https://doi.org/10.1002/cpa.21444, 2013. 3\n[24] S. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28\n(2):129–137, 1982. 7\n[25] A. Pervez, Y. Mao, and D. Lee.\nLearning deep movement primitives using convolutional\nneural networks. In 2017 IEEE-RAS 17th International Conference on Humanoid Robotics\n(Humanoids), pages 191–197. IEEE, 2017. 2\n[26] D. Pollard. Quantization and the method of k-means. IEEE Transactions on Information\nTheory, 28(2):199–204, 1982. 5\n[27] L. Rokach and O. Maimon. Clustering Methods. In O. Maimon and L. Rokach, editors, Data\nMining and Knowledge Discovery Handbook, pages 321–352. Springer US, Boston, MA, 2005.\n2\n[28] A. Saltelli, M. Ratto, T. Andres, F. Campolongo, J. Cariboni, D. Gatelli, M. Saisana, and\nS. Tarantola. Global Sensitivity Analysis: The Primer. John Wiley & Sons, Feb. 2008. 2\n[29] A. Saltelli, S. Tarantola, F. Campolongo, and M. Ratto. Sensitivity Analysis in Practice: A\nGuide to Assessing Scientiﬁc Models. John Wiley & Sons, July 2004. 2\n[30] S. Sivaganesan. Global and Local Robustness Approaches: Uses and Limitations. In D. R.\nInsua and F. Ruggeri, editors, Robust Bayesian Analysis, Lecture Notes in Statistics, pages\n89–108. Springer, New York, NY, 2000. 2\n[31] A. Srivastava and E. Klassen. Functional and Shape Data Analysis. Springer Series in Statis-\ntics, 2016. 3\n[32] M. B. Stegmann and D. D. Gomez.\nA brief introduction to statistical shape analysis.\nAccessed from https://graphics.stanford.edu/courses/cs164-09-spring/Handouts/\npaper_shape_spaces_imm403.pdf. 8\n[33] T. Taniguchi, S. Nagasaka, K. Hitomi, N. P. Chandrasiri, T. Bando, and K. Takenaka. Se-\nquence prediction of driving behavior using double articulation analyzer. IEEE Trans. Syst.,\nMan, and Cyber.: Syst., 46(9):1300–1313, 2016. 2\n[34] T. Taniguchi, S. Nagasaka, K. Hitomi, K. Takenaka, and T. Bando. Unsupervised hierarchical\nmodeling of driving behavior and prediction of contextual changing points. IEEE Trans. Intell.\nTransport. Syst., 16(4):1746–1760, 2015. 2\n[35] C. Villani. Topics in Optimal Transportation. American Mathematical Society, 2003. 5, 6\n[36] C. Wang and S. Mahadevan. Manifold alignment using procrustes analysis. In ICML, 2008. 3\n[37] W. Wang, J. Xi, and D. Zhao. Driving style analysis using primitive driving patterns with\nbayesian nonparametric approaches, 2017. arXiv:1708.08986. 2\n16\n[38] W. Wang and D. Zhao. Extracting traﬃc primitives directly from naturalistically logged data\nfor self-driving applications. IEEE Robotics and Automation Letters, 2017. 1, 2, 9, 12, 13, 14,\n20\n[39] D. Xu and Y. Tian. A Comprehensive Survey of Clustering Algorithms. Annals of Data\nScience, 2(2):165–193, June 2015. 2\n[40] R. Xu and D. C. Wunsch. Clustering. IEEE Press Series on Computational Intelligence. Wiley\n; IEEE Press, Hoboken, N.J. : Piscataway, NJ, 2009. OCLC: ocn216937130. 2\n[41] H. Yao, Y. Liu, Y. Wei, X. Tang, and Z. Li. Learning from multiple cities: A meta-learning\napproach for spatial-temporal prediction. The World Wide Web Conference on - WWW ’19,\n2019. 2\n[42] W. Zhang and W. Wang. Learning v2v interactive driving patterns at signalized intersections.\nTransportation Research Part C: Emerging Technologies, 108:151–166, 2019. 2\n[43] W. Zhang, W. Wang, and D. Zhao. Multi-vehicle interaction scenarios generation with inter-\npretable traﬃc primitives and gaussian process regression. arXiv preprint arXiv:1910.03633,\n2019. 2\n[44] H. Zhu, J. G. Ibrahim, and N. Tang. Bayesian inﬂuence analysis: A geometric approach.\nBiometrika, 98(2):307–323, June 2011. 3\n[45] J. Zhu, S. Qin, W. Wang, and D. Zhao. Probabilistic trajectory prediction for autonomous\nvehicles with attentive recurrent neural process, 2019. 2\n17\nAppendix\nA\nProofs\nA.1\nProof of Proposition 2.1\nWe need to establish\n(a) For any f11, f12, f21, f22 ∈F,d((f11, f12), (f21, f22)) = 0 if and only if (f11, f12) ∼(f21, f22).\n(b) For any f11, f12, f21, f22 ∈F, d((f11, f12), (f21, f22)) = d((f21, f22), (f11, f12)).\n(c) For any f11, f12, f21, f22, f31, f32 ∈F, d((f11, f12), (f21, f22))\n≤d((f31, f32), (f21, f22)) + d((f11, f12), (f31, f32)).\nCondition (a) follows by deﬁnition. To establish (b), note ρ((f11, f12), (f ′\n1, f ′\n2)) = ρ((f ′\n1, f ′\n2), (f11, f12)),\nso\nρ((f11, f12), O1 ⊙(f21, f22) + c1)\n=\nρ(O1 ⊙(f21, f22) + c1, (f11, f12))\n(16)\n=\nρ((f21, f22), O∗\n1 ⊙(f11, f12) −O∗\n1 ⊙c1),\nwhere the second equality is due to property (i) and (ii) in the proposition, with O∗\n1 being the\nconjugate transpose of O1, which is also orthogonal when O1 is. Now taking inﬁmum over C1 and\nO1 the conclusion of part (b) is achieved.\nFor condition (c), notice that it is easy to see, following the argument similar to Eq. (16), that\ninf\nO1,O2∈SO(2);C1,C2∈R2 ρ(O2 ⊙(f11, f12) + C2, O1 ⊙(f21, f22) + C1)\n(17)\n=\ninf\nO1∈SO(2),C1∈R2 ρ((f11, f12), O1 ⊙(f21, f22) + C1).\nNow for any f31, f32 ∈F,\nρ(O2 ⊙(f11, f12) + C2, O1 ⊙(f21, f22) + C1)\n(18)\n≤\nρ(O2 ⊙(f11, f12) + C2, (f31, f32)) + ρ((f31, f32), O1 ⊙(f21, f22) + C1),\nby triangle inequality applied to ρ. Taking inﬁmum wrt O1, O2 ∈SO(2); C1, C2 ∈R2, the rest\nfollows immediately.\nA.2\nProof of Proposition 2.2\nNote that\nd((f11, f12), O ⊙(f21, f22) + c))2 :=\n(19)\nZ ∞\n0\n\u0012\n∥f11(x) −O · f21(x) −c∥2\n2 + ∥f12(x) −O · f22(x) −c∥2\n2\n\u0013\nµ(dx).\nMinimizing Eq. (19) with respect to c, for ﬁxed O, we get\nc\n=\nZ ∞\n0\nf11(x) + f12(x)\n2\nµ(dx) −O ·\n\u0012Z ∞\n0\nf21(x) + f22(x)\n2\nµ(dx)\n\u0013\n.\nSubstituting this value of c, we obtain Eq. (20).\ninf\nc∈R2(ρ((f11, f12), O ⊙(f21, f22) + c))2\n= −2\nZ ∞\n0\n\u0012\nf11(x) −\nZ ∞\n0\nf11(x) + f12(x)\n2\nµ(dx)\n\u0013T\n· O ·\n\u0012\nf21(x) −\nZ ∞\n0\nf21(x) + f22(x)\n2\nµ(dx)\n\u0013\nµ(dx)\n−2\nZ ∞\n0\n\u0012\nf12(x) −\nZ ∞\n0\nf11(x) + f12(x)\n2\nµ(dx)\n\u0013T\nO ·\n\u0012\nf22(x) −\nZ ∞\n0\nf21(x) + f22(x)\n2\nµ(dx)\n\u0013\nµ(dx)\n+\n2\nX\ni=1\nZ ∞\n0\n\r\r\r\rf2i(x) −\nZ ∞\n0\nf21(x) + f22(x)\n2\nµ(dx)\n\r\r\r\r\n2\n2\nµ(dx) +\n2\nX\ni=1\nZ ∞\n0\n\r\r\r\rf1i(x) −\nZ ∞\n0\nf11(x) + f12(x)\n2\nµ(dx)\n\r\r\r\r\n2\n2\nµ(dx).\n(20)\n18\nMinimizing Eq. (20) with respect to O is same as maximizing Eq. (21) with respect to O ∈\nSO(2).\n2trace\n\u0012 Z ∞\n0\n\u0012\nf11(x) −\nZ ∞\n0\nf11(x) + f12(x)\n2\nµ(dx)\n\u0013T\n· O ·\n\u0012\nf21(x) −\nZ ∞\n0\nf21(x) + f22(x)\n2\nµ(dx)\n\u0013\nµ(dx)\n\u0013\n+ 2trace\n\u0012 Z ∞\n0\n\u0012\nf12(x) −\nZ ∞\n0\nf11(x) + f12(x)\n2\nµ(dx)\n\u0013T\n· O·\n\u0012\nf22(x) −\nZ ∞\n0\nf21(x) + f22(x)\n2\nµ(dx)\n\u0013\nµ(dx)\n\u0013\n= 2trace(UDV T · O) = 2trace(D(U T · OT · V )T ).\n(21)\nNow, this is maximized for O ∈SO(2), when O = V T\n\u00141\n0\n0\ndet(V T U)\n\u0015\nU. Plugging in this mini-\nmizing value for O, we get the solution for\ninf(f ′\n1,f ′\n2)∈(O,C)(f21,f22)(ρ((f11, f12), (f ′\n1, f ′\n2)))2 as required.\nLemma 1. Assume that (f ′, g′) ∈C(f,g) ∪O(f,g)\n=⇒\nd((f ′, g′), (f, g)) = 0.\nThen, for all\nc ∈R2, O ∈SO(2),\nd((f11, f12), (f21, f22))\n= d((O ⊙f11 + c, O ⊙f12 + c), (f21, f22)).\n(22)\nProof. By triangle inequality, d((f11, f12), (f21, f22)) ≤d((O ⊙f11 + c, O ⊙f12 + c), (f21, f22)) +\nd((O ⊙f11 + c, O ⊙f12 + c), (f11, f12)), so by assumption d((f11, f12), (f21, f22)) ≤d((O ⊙f11 +\nc, O ⊙f12 + c), (f21, f22)). The lemma follows by considering the reverse inequality.\nB\nObtaining primitives via splines\nOur two-step procedure to extract primitives is as follows.\n1. Add change points for each trajectory via the following steps. (a) Test whether using the\nmidpoint as a change point reduces the squared error of the ﬁtted polynomial (b) If it does,\nreturn the midpoint. (c) Otherwise, test whether using the midpoint of the valid interval\nof the half with the larger square error as a change point reduces the squared error. Rule\nout the other half as a site for change points. (d) Repeat (b)-(c) until either a change point\nis found or no further candidates exist. (e) If a change point was added previously, repeat\n(a)-(d) for the two segments and any subsequent segments. Stop when no more change points\nare added.\n2. Combine the change points from all trajectories in the following manner. Remove change\npoints via a forward search in the following way. Suppose that we have a set, C, of L ordered\nchange points, c1, c2, ..., cL, across all trajectories. Let c0 denote the start point and cL+1\ndenote the end point. Deﬁne ϵ to be our tolerence. Proceed in these steps: (a) Set ℓ= 0,\nℓ′ = 1, and ℓ′′ = 2; (b) Fit a polynomial to each trajectory from cℓto cℓ′′; (c) If the sum of\nthe squared error of the ﬁtted polynomials is below ϵ or there are only 4 observations between\ncℓand cℓ′′, remove cℓ′ from the set of C. Otherwise, increment ℓ. Increase ℓ′ and ℓ′′ by one\nand go back to (b) if ℓ′′ ≤L + 1; (d) Set L to be the size of C. Re-index the change points\nin C from one to L and return C.\nTo select ϵ from a set of potential tolerances, we set it to be the value that after running (2),\nminimizes\nn\nX\ni=1\nL+1\nX\nℓ=1\n\u0010\nf(ti) −ˆfℓ(ti)\n\u00112\n1(ti≤cℓ) + L + 2.\nC\nAlgorithm for centering and reorienting primitives\nAlgorithm 4 provides a way to reorient one set of interactions to another and is embedded in\nAlgorithms 2 and 3.\n19\n(a) Multidim. scal-\ning\napproach\n(cf.\nSection 3.2.1)\n(b) First geometric\napprox.\n(cf.\nSec-\ntion 3.2.2)\n(c) Second geomet-\nric approx. (cf. Sec-\ntion 3.2.2)\n(d) Polynomial co-\neﬃcients (cf.\nSec-\ntion 4.2)\n(e) DTW cost ma-\ntrix (cf. [38])\nFigure 6: Plot of the three most typical interactions organized from the cluster with the most\ninteraction to the cluster with the fewest for various methods. See Figure 2 for the legend and for\nhow the interactions are oriented.\nAlgorithm 4 Centering and reorienting interactions\nInput: Two interaction samples, (fi1, fi2) and (fj1, fj2)\nOutput: A centered (fi1, fi2) and a centered (fj1, fj2) reoriented to the centered (fi1, fi2)\n1: Set e\nfi(t) ∈R2tm × R2 to be the concatenation of fi1 and fi2 such that e\nfi(t) = fi1(t) for t = 1, 2, . . . tm and\ne\nfi(t) = fi2(t) for t = tm + 1, tm + 2, . . . 2tm and e\nfj(t) ∈R2tm × R2 be the same concatenation of fi1 and fi2.\n2: For f i(t) ∈R2tm × R2 and f i(t) ∈R2tm × R2, set\nf i(t) = e\nfi(t) −\n1\n2tm\n2tm\nX\nt′=1\ne\nfi(t′)\nf j(t) = e\nfj(t) −\n1\n2tm\n2tm\nX\nt′=1\ne\nfj(t′).\n3: Perform singular value decomposition to get the matrices U, D, V such that UDV T = f j(t)T f i(t).\n4: From before, let\n˜\nO = V T\n\u00141\n0\n0\ndet(V T U)\n\u0015\nU.\nThen, set f\n′\ni1(t), f\n′\ni2(t) ∈RT × R2 to be the matrices such that for t = 1, 2, . . . , T ,\nf\n′\ni1(t) = f i(t)\nf\n′\ni2(t) = f i(t + tm).\nOn the other hand, set f\n′\nj1(t), f\n′\nj2(t) ∈RT × R2 to be the matrices such that for t = 1, 2, . . . , T ,\nf\n′\nj1(t) = ( ˜\nOf j)(t)\nf\n′\nj2(t) = ( ˜\nOf j)(t + tm).\n5: Return (f\n′\ni1, f\n′\ni2) and (f\n′\nj1, f\n′\nj2).\n20\n",
  "categories": [
    "cs.LG",
    "cs.MA",
    "cs.RO",
    "stat.AP",
    "stat.ML"
  ],
  "published": "2020-06-18",
  "updated": "2020-06-18"
}