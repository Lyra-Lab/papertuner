{
  "id": "http://arxiv.org/abs/2006.10875v2",
  "title": "Provably adaptive reinforcement learning in metric spaces",
  "authors": [
    "Tongyi Cao",
    "Akshay Krishnamurthy"
  ],
  "abstract": "We study reinforcement learning in continuous state and action spaces endowed\nwith a metric. We provide a refined analysis of a variant of the algorithm of\nSinclair, Banerjee, and Yu (2019) and show that its regret scales with the\n\\emph{zooming dimension} of the instance. This parameter, which originates in\nthe bandit literature, captures the size of the subsets of near optimal actions\nand is always smaller than the covering dimension used in previous analyses. As\nsuch, our results are the first provably adaptive guarantees for reinforcement\nlearning in metric spaces.",
  "text": "Provably adaptive reinforcement learning in metric spaces\nTongyi Cao*1 and Akshay Krishnamurthy†2\n1University of Massachusetts, Amherst, MA\n2Microsoft Research, New York, NY\nAbstract\nWe study reinforcement learning in continuous state and action spaces endowed with a metric. We\nprovide a reﬁned analysis of a variant of the algorithm of Sinclair, Banerjee, and Yu (2019) and show\nthat its regret scales with the zooming dimension of the instance. This parameter, which originates in\nthe bandit literature, captures the size of the subsets of near optimal actions and is always smaller than\nthe covering dimension used in previous analyses. As such, our results are the ﬁrst provably adaptive\nguarantees for reinforcement learning in metric spaces.\n1\nIntroduction\nIn reinforcement learning (RL), an agent learns to select actions to navigate a state space and accumulates\nreward. In terms of theoretical results, the majority of results address the tabular setting, where the number of\nstates and actions are ﬁnite and comparatively small. However, tabular problems are rarely encountered in\npractical applications, as state and action spaces are often large and may even be continuous. To address these\npractically relevant settings, a growing body of work has developed algorithmic principles and guarantees for\nreinforcement learning in continuous spaces.\nIn this paper, we contribute to this line of work on reinforcement learning in continuous spaces. We\nconsider episodic RL where the joint state-action space is endowed with a metric and we posit that the optimal\nQ⋆function is Lipschitz continuous with respect to this metric. This setup has been studied in several recent\nworks establishing worst case regret bounds that scale with the covering dimension of the metric space (Song\nand Sun, 2019; Sinclair et al., 2019; Touati et al., 2020). While these results are encouraging, the guarantees\nare overly pessimistic, and intuition from the special case of Lipschitz bandits suggests that much more\nadaptive guarantees are achievable. In particular, while the Lipschitz contextual bandits setting of Slivkins\n(2014) is a special case of this setup, no existing analysis recovers his adaptive guarantee that scales with the\nzooming dimension of the problem.\nOur contribution.\nWe give the ﬁrst analysis for reinforcement learning in metric spaces that scales with\nthe zooming dimension of the instance instead of the covering dimension of the metric space. The zooming\ndimension, originally deﬁned by Kleinberg et al. (2019) in the context of Lipschitz bandits, measures the\nsize of the set of near-optimal actions and can be much smaller than the covering dimension in favorable\ninstances. For reinforcement learning, the natural generalization is to measure near-optimality relative to\nthe Q⋆function; this recovers the deﬁnition of Kleinberg et al. (2019) and Slivkins (2014) for bandits and\ncontextual bandits, respectively as special cases. As a consequence, our guarantees also strictly generalize\ntheirs to the multi-step reinforcement learning setting. In addition, our guarantee addresses an open problem\nof Sinclair et al. (2019) by characterizing problems where reﬁned guarantees are possible.\n*tcao@cs.umass.edu\n†akshaykr@microsoft.com\n1\narXiv:2006.10875v2  [cs.LG]  20 Oct 2021\nOur result is based on a reﬁned analysis of a variant of the algorithm of Sinclair et al. (2019). This\nalgorithm uses optimism to select actions and an adaptive discretization scheme to carefully reﬁne a coarse\npartition of the state-action space to focus (“zoom in”) on promising regions. Adaptive discretization is\nessential for obtaining instance-dependent guarantees, but the bounds in Sinclair et al. (2019) do not reﬂect\nthis favorable behavior.\nAt a technical level, the main challenge is that, unlike in bandits, we cannot upper bound the number\nof times a highly suboptimal arm will be selected by the optimistic strategy. Analysis for the bandit setting\nuses these upper bounds to prove that the adaptive discretization scheme will not zoom in on suboptimal\nregions, which is crucial for the instance-dependent bounds. However, in RL, the algorithm actually can\nzoom in on and select actions in suboptimal regions, but only when there is signiﬁcant error at later time steps.\nThus, in the analysis, we credit error incurred from a highly suboptimal region to the later time steps, so we\ncan proceed as if we never zoomed in on this region at all. Formally, this analysis uses the clipped regret\ndecomposition of Simchowitz and Jamieson (2019) as well as a careful bookkeeping argument to obtain the\ninstance-dependent bound.\nChanges from the initial version.\nThe present version of the paper corrects an error in the version\npublished in NeurIPS 2020. The differences are both in the algorithm, which is no longer identical to that\nof Sinclair et al. (2019), and in the analysis, which is somewhat more involved. The changes address an issue\nthat arises when a child ball inherits updates from its parent, which results in each sample appearing many\ntimes with the same weight αi\nt in the recursive regret decomposition used in the tabular analysis of Jin et al.\n(2018), displayed in (3). This ultimately compromises the ﬁnal regret bound, which crucially uses that these\nweights form a convergent series.\nThe ﬁx is that child balls no longer inherit data from the parent so that every interaction tuple (of state,\naction, reward, next state) results in exactly one update. This ensures that the αi\nt weight sequences converge,\nbut is also problematic, as child balls are initialized with large bonuses, so the conﬁdence sum does not\ncapture the zooming property we hope to exhibit. We resolve this latter issue with a buffering phase where a\nchild ball is slowly updated but is never played. Speciﬁcally, once a parent ball has received enough samples,\nwe split it and mark the children as buffering. While the children are buffering, we continue to use the parent\nfor action selection and mostly continue to update the parent, but every H + 1st update is instead performed\non the child. Once the child has enough updates that the bonus is small, we move it out of the buffering phase\nand can safely use it for decision making.\nUnfortunately, the buffering approach means that the parent ball is periodically chosen but not updated,\nwhich again results in a weight sequence where some terms (speciﬁcally every Hth term) appears twice.\nHowever this sequence is much more benign than the one that arises if we re-use samples. Indeed, we can\nshow that this new sequence is convergent via a new amortizing argument that relates it to the original one\nin Jin et al. (2018).\nThe ﬁnal challenge is that now the parent ball remains active for much longer. This results in a ﬁnal\nregret bound that now scales polynomially with Λ, the maximum number of children that a parent can have\n(or the doubling constant of the metric space), and additionally is polynomially worse in its dependence on\nthe horizon H than the bound claimed in the NeurIPS 2020 version of the paper. On the other hand, the new\nbound still captures the adaptive and zooming nature of the algorithm.\n2\nPreliminaries\nWe consider a ﬁnite-horizon episodic reinforcement learning setting in which an agent interacts with an MDP,\ndeﬁned by a tuple (S, A, H, P, r). Here S the state space, A is the action space, H ∈N is the horizon, P is\n2\nthe transition operator and r is the reward function. Formally, P : S × A →∆(S) and r : S × A →[0, 1]\nwhere ∆(·) denotes the set of distributions over its argument.1\nA (nonstationary) policy π is a mapping from states to distributions over actions for each time. Every\npolicy has non-stationary value and action-value functions, deﬁned as\nV π\nh (x) := Eπ\n\" H\nX\nh′=h\nrh′(xh′, ah′) | xh = x\n#\n,\nQπ\nh(x, a) := rh(x, a) + E\n\u0002\nV π\nh+1(x′) | x, a\n\u0003\n.\nHere Eπ [·] denotes that all actions are chosen by policy π and transitions are given by P. The optimal policy\nπ⋆and optimal action-value function Q⋆are deﬁned recursively as\nQ⋆\nh(x, a) := rh(x, a) + E\n\u0014\nmax\na′\nQ⋆(x′, a′) | x, a\n\u0015\n,\nπ⋆\nh(x) = argmax\na\nQ⋆\nh(x, a).\nThe optimal value function V ⋆\nh is deﬁned analogously.\nThe agent interacts with the MDP for K episodes, where in episode k the agent picks a policy πk and we\ngenerate the trajectory τk = (xk\n1, ak\n1, rk\n1, xk\n2, ak\n2, rk\n2 . . . , xk\nH, ak\nH, rk\nH) where (1) xk\n1 is chosen adversarially, (2)\nak\nh = πk(xk\nh), (3) xk\nh+1 ∼P(· | xk\nh, ak\nh), (4) rk\nh = r(xk\nh, ak\nh). We would like to choose actions to maximize\nthe cumulative rewards PH\nh=1 rk\nh.\nEquipped with these deﬁnitions, we can state our performance criterion. Over the course of K episodes,\nwe would like to accumulate reward that is comparable to the optimal policy, formalized via the notion of\nregret:\nReg(K) :=\nK\nX\nk=1\n \nV ⋆\n1 (xk\n1) −\nH\nX\nh=1\nrk\nh\n!\n.\nIn particular, we seek algorithms with regret rate that is sublinear in K. Note that we have not assumed that\n|S| and |A| are ﬁnite, and we also allow for the starting state xk\n1 to be chosen adversarially in each episode.\n2.1\nMetric spaces.\nInstead of assuming that |S| and |A| are ﬁnite, we will posit a metric structure on these spaces. We recall\nthe key deﬁnitions for metric spaces. A space Y equipped with a function D : Y × Y →R+ is a metric\nspace if D satisﬁes (a) D(y, y′) = 0 iff y = y′ (b) D is symmetric, and (c) D satisﬁes the triangle inequality\nD(x, y) ≤D(x, z) + D(z, y). If these properties hold then D is called a metric. For a radius r > 0, we\nuse the notation B(y, r) := {y′ ∈Y : D(y, y′) < r} to denote the open ball centered at y with radius r.\nFor a subset Y ′ ⊆Y the diameter is deﬁned as diam(Y ′) := supy,y′∈Y ′ D(y, y′). We also use the standard\nnotions of covering and packing to measure the size of metric spaces.\nDeﬁnition 1 (Notions of size). We deﬁne the following notions of size for a metric space.\n• A covering of Y at scale r (also called an r-covering) is a collection of subsets of Y , each with\ndiameter at most r, whose union equals Y . The minimum number of subsets that form an r-covering is\nthe r-covering number, denoted Nr(Y ).\n• A packing of Y at scale r (also called an r-packing) is a collection of points Z ⊂Y such that\nminz̸=z′∈Z D(z, z′) ≥r. The maximum number of points that form an r-packing is the r-packing\nnumber, denoted Npack\nr\n(Y ).\n1Deterministic rewards simpliﬁes the presentation but has no bearing on the ﬁnal results. In particular, we can handle stochastic\nbounded rewards with minimal modiﬁcation to the proofs.\n3\n• An r-net of Y is an r-packing S ⊂Y for which {B(y, r)}y∈S covers Y .\n• Deﬁne the doubling constant Λ(Y ) := maxr>0,y∈Y Nr/2(B(y, r)), which is the maximum number of\nballs of radius r/2 required to cover some ball of radius r.\nThese deﬁnitions also apply to subsets of the metric space, which will be important for our development.\nAlso note that Npack\n2r (Y ) ≤Nr(Y ) ≤Npack\nr\n(Y ).\n2.2\nMain Assumptions\nWe now state the main assumptions that we adopt in our analysis. These or closely related assumptions\nare standard in the literature on bandits and reinforcement learning in metric spaces (Song and Sun, 2019;\nSinclair et al., 2019; Touati et al., 2020; Slivkins, 2014).\nAssumption 1. (S × A, D) is a metric space with ﬁnite diameter diam(S × A) = dmax < ∞.\nAssumption 2. For every h ∈[H], Q⋆\nh is L-Lipschitz continuous with respect to D:\n∀(x, a), (x′, a′) :\n\f\fQ⋆\nh(x, a) −Q⋆\nh(x′, a′)\n\f\f ≤L · D((x, a), (x′, a′)).\n(1)\nAdditionally V ⋆\nh is L-Lipschitz with respect to the metric DS : (x, x′) 7→mina,a′ D((x, a), (x′, a′)):\n∀x, x′ :\n\f\fV ⋆\nh (x) −V ⋆\nh (x′)\n\f\f ≤L · min\na,a′ D((x, a), (x′, a′)).\n(2)\nAssumption 1 is a basic regularity condition, while the ﬁrst part of Assumption 2 imposes continuity\nof the Q⋆function. In particular, Lipschitz-continuity characterizes how the metric structure inﬂuences the\nreinforcement learning problem. These assumptions appear in prior work, and we note that (1) is strictly\nweaker than assuming that P is Lipschitz continuous (Kakade et al., 2003; Ortner and Ryabko, 2012).\nThe second part of Assumption 2 reﬂects an additional structural assumption on the problem, which is a\ndeparture from previous work. In detail, (2) posits that the optimal value function V ⋆\nh is L-Lipschitz with\nrespect to a metric deﬁned only on the states that is derived from the original one. This metric is dominated\nby the original one since for each (x, x′, a) we have mina1,a2 D((x, a1), (x′, a2)) ≤D((x, a), (x′, a)), so\nthis assumption is not directly implied by (1). However, whenever D is sub-additive in the sense that\nD((x, a), (x′, a′)) ≤DS(x, x′) + DA(a, a′), then the assumption holds trivially. Sub-additivity holds for\nmost metrics of interest, including those induced by ℓp norms for p ≥1. As such, we do not view this\nassumption as particularly restrictive.\n2.3\nRelated work\nReinforcement learning in the tabular setting, where the state and action spaces are ﬁnite, is relatively\nwell-understood (Azar et al., 2017; Dann et al., 2017; Zanette and Brunskill, 2019). Of this line of work, the\ntwo most related papers are those of of Jin et al. (2018) and Simchowitz and Jamieson (2019). Our results\nbuild on the model-free/martingale analysis of Jin et al. (2018), which has been used in recent work on RL in\nmetric spaces (Song and Sun, 2019; Sinclair et al., 2019; Touati et al., 2020). We also employ techniques\nfrom the gap-dependent analysis of Simchowitz and Jamieson (2019). In particular, we use a version of their\n“clipping” argument, as we will explain in Section 5.\nMoving beyond the tabular setting, several papers study reinforcement learning in metric spaces, orig-\ninating with the results of Kakade et al. (2003) (c.f., Ortner and Ryabko (2012); Ortner (2013); Song and\nSun (2019); Ni et al. (2019); Sinclair et al. (2019); Touati et al. (2020)). Of these, the most related result\nis that of Sinclair et al. (2019) who study the adaptive discretization algorithm and give a worst-case regret\n4\nanalysis, showing that the algorithm has a regret rate of K\nd+1\nd+2 where d is the covering dimension of the\nmetric space. Essentially the same results appear in Touati et al. (2020), although the algorithm is slightly\ndifferent. However, none of these results give sharper instance-dependence guarantees that reﬂect benign\nproblem structure, as we will obtain.\nFor the special case of (contextual) bandits, several instance-dependent guarantees that yield improved\nregret rates exist (Auer et al., 2007; Valko et al., 2013; Kleinberg et al., 2019; Bubeck et al., 2011; Slivkins,\n2014; Krishnamurthy et al., 2019). For non-contextual bandits, the results and assumptions vary considerably,\nbut most results quantify a benign instance in terms of the size of the set of near-optimal actions. The\nformulation that we adopt is the notion of zooming dimension, which measures the growth rate of the\nr-packing number of the set of O(r)-suboptimal arms. This notion has been used in several works on bandits\nand contextual bandits in metric spaces, and we will recover some of these results as a special case of our\nmain theorem.\n3\nMain Results\nOur main result is a regret bound that scales with the zooming dimension. We introduce this parameter with a\nsequence of deﬁnitions. First, we deﬁne the gap function, which describes the sub-optimality of an action a\nfor state x.\nDeﬁnition 2 (Gap). For any (x, a) ∈S × A, for h ∈[H], the stage-dependent sub-optimality gap is\ngaph(x, a) := V ⋆\nh (x) −Q⋆\nh(x, a).\nWe use the gaps to deﬁne the subset of the metric space that is near-optimal.\nDeﬁnition 3 (Near-optimal set). We deﬁne near-optimal set as\nPQ⋆\nh,r :=\n\u001a\n(x, a) ∈S × A : gaph(x, a) ≤\n\u00122(H + 1)\ndmax\n+ 2L\n\u0013\nr\n\u001b\n.\nIntuitively, PQ⋆\nh,r is the set of state-action pairs with gap that is O(r) at stage h. The constant in the\ndeﬁnition is a consequence of our analysis, but it is quite similar to the constant in the deﬁnition of Slivkins\n(2014) for contextual bandits. In particular, he considers dmax = 1, H = 1, L = 1 and obtains a constant of\n12, while we obtain a constant of 6 in this case.\nFinally, we deﬁne the zooming number and the zooming dimension.\nDeﬁnition 4 (Zooming number and dimension). The r-zooming number is the r-packing number of the\nnear-optimal set PQ⋆\nh,r, that is Npack\nr\n(PQ⋆\nh,r). The stage-dependent zooming dimension is deﬁned as\nzh,c := inf\nn\nd > 0 : Npack\nr\n(PQ⋆\nh,r) ≤cr−d, ∀r ∈(0, dmax]\no\n.\nThe zooming dimension for the instance as the largest among all stages zc = maxh∈[H] zh,c.\nIntuitively, the zooming dimension measures how the near-optimal region grows as we change the\nsub-optimality level r. Importantly, we use r both to parametrize the radius in the packing number and the\nsub-optimality. Thus, the zooming number captures how many r-separated points can be packed into the\nO(r) sub-optimal region.\n5\nx\na\nnear optimal\nactions for x\nFigure 1: An example where the zooming\ndimension is 1 while the the covering di-\nmension is 2.\nThe more standard notion of complexity of a metric space\nis the covering dimension, deﬁned as\ndc := inf{d > 0, Npack\nr\n(S × A) ≤cr−d, ∀r ∈(0, dmax]}.\nExamining the deﬁnitions, it is clear that we have zc ≤dc,\nsince the packing numbers are only smaller. However, in benign\ninstances where the sub-optimal region concentrates to a low\ndimensional manifold, we may have zc < dc (and possibly\nmuch smaller), which will enable sharper regret bounds. An\nexample is illustrated in Figure 1, where the set of near-optimal\nactions concentrates on a narrow band for each x. Thus the entire space and hence the covering dimension is\n2-dimensional, but the zooming dimension is 1. More generally, if S is a dS dimensional space and A is a dA\ndimensional space, then the covering dimension could be Ω(dS + dA) while the zooming dimension could\nbe as small as O(dS).\nWith these deﬁnitions, we can now state the main theorem.\nTheorem 1. For any initial states {xk\n1 : k ∈[K]}, and any δ ∈(0, 1), with probability at least, 1 −δ\nAdaptive Q-learning has the following regret2\nReg(K) ≤˜O\n\n(H3/2 +\n√\nHΛ)\ninf\nr0∈(0,dmax]\n\n\nH\nX\nh=1\nX\nr=dmax2−i,r≥r0\nNpack\nr\n(PQ⋆\nh,r)dmax\n√\nHΛ\nr\n+ Kr0\ndmax\n\n\n\n\n+ ˜O\n\u0010\nH2 +\np\nH3K log(1/δ)\n\u0011\n.\nBefore turning to a discussion of the theorem, we state some corollaries. First, by optimizing r0, we\nobtain a regret bound in terms of the zooming dimension.\nCorollary 2. For any initial states {xk\n1 : k ∈[K]}, and any δ ∈(0, 1), with probability at least 1 −δ\nAdaptive Q-learning has Reg(K) ≤˜O\n\u0010\nH\n5\n2 +\n1\n2zc+4 K\nzc+1\nzc+2\n\u0011\n, for any constant c > 0.\nFinally, we recover the regret rate of Slivkins (2014) in the special case of contextual bandits.\nCorollary 3 (Contextual bandits). If H = 1, then Adaptive Q-learning has regret ˜O\n\u0010\nK\nzc+1\nzc+2\n\u0011\n, which\nrecovers the regret rate of Slivkins (2014).\nWe now turn to the remarks:\n• Theorem 1 gives a regret bound that depends on the packing numbers of the near-optimal set (Deﬁni-\ntion 3). This bound should be compared with the “metric-speciﬁc” regret guarantee of Sinclair et al.\n(2019) or the “reﬁned regret bound” of Touati et al. (2020). Both of these results have the same form as\nours, but with Npack\nr\n(S × A) in the place of Npack\nr\n(PQ⋆\nh,r). As PQ⋆\nh,r ⊂S × A, our bound improves on\ntheirs in this sense, at the cost of a\n√\nHΛ additional dependence.\n• The more-interpretable bound is in terms of the zooming dimension (Deﬁnition 4), which highlights\nthe dependence on the number of episodes K. We obtain a regret rate of K\nzc+1\nzc+2 for any constant\nc > 0, which should be compared with the non-adaptive rate K\ndc+1\ndc+2 that scales with the covering\ndimension (Song and Sun, 2019; Sinclair et al., 2019; Touati et al., 2020).3 As the zooming dimension\ncan be smaller than covering dimension (recall Figure 1), this bound demonstrates a polynomial\nimprovement over non-adaptive approaches.\n2Throughout the paper ˜O(·) suppresses logarithmic dependence in its argument.\n3We always treat c as a universal constant, so its dependence in the regret bounds is suppressed.\n6\n• Corollary 3 shows that our bound recovers the guarantee from Slivkins (2014), although his bound\ndoes not require that (2) holds. We give a more detailed explanation on the necessity of (2) in Section 5.\nNevertheless, the fact that we essentially recover his bound suggests that our results are the natural\ngeneralization to multi-step RL.\n• Finally, we remark that we can instantiate the result in the tabular setting with ﬁnite S, A by taking the\nmetric to be D((x, a), (x′, a′)) = 1{(x, a) ̸= (x′, a′)}. In this case we obtain a “partial” gap-dependent\nbound of the form:\npoly(H) ·\n\np\n|S|K +\nH\nX\nh=1\nX\nx∈S\nX\na:gaph(x,a)>0\nlog(K)\ngaph(x, a)\n\n.\nThis is not a fully gap-dependent bound because of the\np\n|S|K term, but it does recover an intermediate\nresult of Simchowitz and Jamieson (2019). In particular, this conﬁrms that the model-free methods can\nachieve a partial gap-dependent guarantee for the tabular setting.\n4\nAlgorithm\nAs we have mentioned, the algorithm is based on the Adaptive Q-learning algorithm of Sinclair et al. (2019).\nThe pseudocode is presented in Algorithm 1. The algorithm adaptively partitions the state-action space to\nfocus on the informative regions, and uses optimism to explore the space and drive the agent to regions with\nhigh reward. Compared to Sinclair et al. (2019), the main difference is that when a new partition is formed it\ndoes not inherit the value and sample count from its parent. Instead, it will go through an additional buffering\nphase before it is activated and used for action selection.\nDuring the execution, the algorithm creates many balls B ⊂S × A for each stage h. For stage h and\nepisode k, we use Pk\nh to denote the set of active balls, and Fk\nh to denote the set of buffering balls. When a set\nof balls are created, they are ﬁrst moved to the buffering set, and balls in this set will not be used for decision\nmaking, but may occasionally be updated.\nEach ball B is associated with (1) a radius, denoted r(B), (2) a domain, denoted domk\nh(B), (3) several\ncounters and thresholds related to the amount of data it has seen, and (4) an optimistic estimate of Q⋆\nh. The\nradius of a ball is r(B) := diam(B) and the domain domk\nh(B) is the set of points contained in this ball, but\nnot in any other active ball with a smaller radius. Formally,\ndomk\nh(B) := B \\ {∪B′∈Pk\nh:r(B′)<r(B)B′}.\nFor the counters, nk\nh(B) denotes the number of times ball B has been updated at stage h and episode k,\nwhile ˜nk\nh(B) denotes the number of times ball B has been “played” or used for decision making. These two\ncounters will not be equivalent in general. We also use two thresholds: Nsplit(B) is the number of updates we\nmust perform before we split B into smaller balls, and Nmin(B) is the number of updates we must perform\nbefore moving B from the buffering set Fk\nh to the active set Pk\nh. These latter two are deﬁned as:\nNsplit(B) := 4Nmin(B) :=\n\u0012dmax\nr(B)\n\u00132\n.\nWhen a ball is split in line 9 the resulting balls are called children and denoted C(B). Finally, each ball\nmaintains a scalar Qk\nh(B) which serves as an upper bound on max(x,a)∈B Q⋆\nh(x, a).\nIn stage h of episode k, we select the action for state xk\nh as follows: we consider all the smallest active\nballs that contains xk\nh, deﬁned as “relevant” balls\nrelk\nh(x) := {B ∈Pk\nh | ∃a, (x, a) ∈domk\nh(B)}.\n7\nAlgorithm 1 Adaptive Q–learning with zooming dimension\n1: For h ∈[H], initialize F1\nh = ∅, P1\nh to be a dmax\nH -net of S × A.\n2: For B ∈P1\nh, deﬁne Q1\nh(B) = H. ˜n1\nh(B) = n1\nh(B) = 0.\n3: for each episode k = 1, 2, . . . , K do\n4:\nReceive xk\n1.\n5:\nfor stage h = 1, 2, . . . , H do\n6:\nBk\nh = argmaxB∈relk\nh(xk\nh) Qk\nh(B), ˜nk+1\nh\n(Bk\nh) = ˜nk\nh(Bk\nh) + 1.\n7:\nPlay action ak\nh for some (xk\nh, ak\nh) ∈domk\nh(Bk\nh), receive rk\nh, xk\nh+1.\n8:\nif ˜nk+1\nh\n(Bk\nh) ≥Nsplit(Bk\nh) then\n9:\nif Bk\nh is not split then split Bk\nh.\n10:\nCreate a set of children C(Bk\nh) = 1\n2r(Bk\nh)-net of domk\nh(Bk\nh).\n11:\nSet Fk+1\nh\n= Fk\nh ∪C(Bk\nh).\n12:\nelse if ˜nk+1\nh\n(Bk\nh) mod (H + 1) = 0 then\n13:\nFind B′ ∈C(Bk\nh) such that (xk\nh, ak\nh) ∈B′ and set Bk\nh = B′.\n14:\nend if\n15:\nend if\n16:\nUpdate nk+1\nh\n(Bk\nh) = nk\nh(Bk\nh) + 1 and set t = nk+1\nh\n(Bk\nh).\n17:\nV k\nh+1(xk\nh+1) = min\nn\nH, maxB∈relk\nh+1(xk\nh+1) Qk\nh+1(B)\no\n.\n18:\nQk+1\nh\n(Bk\nh) = (1 −αt)Qk\nh(Bk\nh) + αt(rk\nh + bt + V k\nh+1(xk\nh+1)).\n19:\nif Bk\nh ∈Fk\nh and nk+1\nh\n(Bk\nh) ≥Nmin(Bk\nh) then\n20:\nMove Bk\nh from Fk\nh to Pk\nh, i.e. Fk+1\nh\n= Fk\nh \\ {Bk\nh}, Pk+1\nh\n= Pk\nh ∪{Bk\nh}.\n21:\nSet ˜nk+1\nh\n(Bk\nh) = nk\nh(Bk\nh)\n22:\nend if\n23:\nend for\n24:\nAdvance all other algorithm state (i.e., Pk+1\nh\n←Pk\nh, etc., if not explicitly updated above)\n25: end for\nAmong the relevant balls, the algorithm selects the ball Bk\nh with the highest Qk\nh(B) value and plays an\narbitrary action such that (xk\nh, a) ∈domk\nh(Bk\nh). We almost always update the ball that we play, except\nsometimes we invoke line 13 where we rebind Bk\nh to be one of the children. In this case, we play a certain ball\nbut then update its child. At the end of the episode, we update the estimated Q value Qk\nh(Bk\nh) and increment\nthe sample count t = nk\nh(Bk\nh) + 1. The update rule is a form of optimistic Q learning\nQk+1\nh\n(Bk\nh) = (1 −αt)Qk\nh(Bk\nh) + αt(rk\nh + bt + V k\nh+1(xk\nh+1)),\nV k\nh+1(x) = min\n(\nH,\nmax\nB∈relk\nh+1(x)\nQk\nh+1(B)\n)\n.\nwhere the αt is the learning rate and b(t) is the bonus added to ensure that Qk\nh is optimistic. Formally,\nαt := H + 1\nH + t ,\nbt := 2\nr\nH3 log(4HK/δ)\nt\n+ 4Ldmax\n√\nHΛ + Λ + 1\n√\nt\n.\nFor all other balls at stage h, we set Qk+1\nh\n(B) ←Qk\nh(B), with no update.\nWe split a ball B as soon as nk\nh(B) ≥Nsplit(B). When splitting, we create a set of new “children” balls\nwith radius r(B)/2 that forms an r(B)/2-net of domk\nh(B). These “children” are added to the buffering set\n8\nFk\nh. Once a ball B ∈Fk\nh receives Nmin(B) updates, we move it to the active set Pk\nh and we can use it for\naction selection. This splitting rule leads to the following invariant:\nLemma 4 (Lemma 5.3 in Sinclair et al. (2019)). For every (h, k) ∈[H] × [K], we have\n1. (Covering) The domains of balls in Pk\nh covers S × A.\n2. (Separation) For any two balls of radius r, their centers are at distance at least r.\nThe last component to describe is the warm-starting process for balls in the buffering phase, which is\nthe main difference compared with the algorithm of Sinclair et al. (2019). This process works as follows. A\nball B with nk\nh ≥Nsplit(B) updates may still be chosen for action selection if some of its children are still\nbuffering (if no child is buffering, then, by the deﬁnition of domk\nh, B cannot be selected). During this phase,\nevery H + 1 times that we play ball B, we instead use the sample to update one of the children, speciﬁcally\nthe one that contains (xk\nh, ak\nh). In this way, balls in the buffering set Fk\nh slowly accumulate samples and\neventually can be moved to the active set.\n5\nProof sketch\nIn this section we describe the main steps of the proof, with details deferred to the appendix.\nIt is worth reviewing prior regret analyses for episodic RL (Jin et al., 2018). The arguments establish a\nregret decomposition that relates the estimate V k\n1 to V πk\n1 , the expected reward collected in episode k. The\ndecomposition is recursive in nature, involving differences between Qk\nh and Q⋆\nh. These are controlled by\nthe update rule and the design of the learning rate. In particular, we can bound Qk\nh −Q⋆\nh by an immediate\n“surplus” βt and the downstream value function error. Formally for any ball B with (x, a) ∈domk\nh(B)\nQk\nh(B) −Q⋆\nh(x, a) ≤1[t=0]H +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1) + βt,\n(3)\nwhere t = nk\nh(B), αt\ni = αi\nQt\nj=i+1(1−αj) and βt = 2 Pt\ni=1 αt\nibi. Here ki is the index of the episode where\nB was updated for the ith time. Summing over all episodes and grouping terms appropriately (and ignoring\nthe buffering process for now), we obtain\nK\nX\nk=1\n(V k\nh −V πk\nh )(xk\nh) ≤\nK\nX\nk=1\n\u0010\nH1[nk\nh=0] + βnk\nh + ξk\nh\n\u0011\n+ (1 + 1/H)\nK\nX\nk=1\n\u0010\nV k\nh+1 −V πk\nh+1\n\u0011\n(xk\nh+1),\nwhere ξk\nh+1 is a stochastic term that can be ignored for this discussion. Note that, as long as V k\nh is optimistic\n(which we will verify), this also provides a bound on the regret.\nFor the tabular setting, Jin et al. (2018) use this regret decomposition to obtain a worst-case bound. The\nleading term arises from the “surplus” term βnk\nh, which leads to a poly(H)\n√\nSAK regret bound for the\ntabular setting. On the other hand for our setting, the splitting rule and the buffering scheme implies that,\nfor any ball B, we must have nk\nh ≤(HΛ + Λ + 1) (dmax/r(B))2, as we will show. We can use this to obtain\na bound that depends on the number of active balls at each scale r times dmax/r. If we could bound the\nnumber of active balls at scale r in terms of the packing number Npack\nr\n(PQ⋆\nh,r), then we would obtain the\ninstance-dependent bound.\nUnfortunately, this is not possible. In general, the algorithm will activate balls outside of the near-optimal\nregion, because we may have to select a highly suboptimal ball many times to reduce downstream over-\nestimation error. So indeed the number of active balls at scale r could be much larger than the packing of the\nnear-optimal set.\n9\nWe address this with the following key observation. If the surplus βnk\nh is small compared to gap, and we\nchoose this ball, it must be the case that the downstream regret is quite large, otherwise we would not have\nchosen this ball. If this is true, we can account for the surplus by adding a small constant fraction of the future\nregret. In otherwords, we can “clip” the surplus to zero once it is proportional to the gap, and we only pay a\nconstant factor in the recursive term. This is the clipping trick developed by Simchowitz and Jamieson (2019)\nto establish gap dependent bounds for tabular MDP. Formally instead of (3), we have the following lemma.\nLemma 5 (Clipped upper bound). For any δ ∈(0, 1) with probability at least 1 −δ/2, ∀h ∈[H],\nQk\nh(Bk\nh) −Q⋆\nh(xk\nh, ak\nh) ≤(1 + 1/H)\n \nH1[t=0] +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1)\n!\n+ clip\n\u0014\nβt | gaph(xk\nh, ak\nh)\nH + 1\n\u0015\n,\nwhere t = nk\nh(B), αt\ni = αi\nQt\nj=i+1(1 −αj) and βt = 2 Pt\ni=1 αt\nibi and clip[µ | ν] := µ1{µ ≥ν}.\nThis bound should be compared with (3). On one hand the recursive term is multiplied by 1 + 1/H, but,\non the other, we are able to clip the surpluses βt. The former will exponentiate but will asymptote to e, while\nthe latter is crucial for our instance dependent bounds.\nUsing this lemma, we can bound the difference between V k\nh and V πk\nh .\nLemma 6 (Clipped recursion, informal). For any δ ∈(0, 1), with probability at least 1 −δ/2, ∀h ∈[H],\nK\nX\nk=1\n(V k\nh −V πk\nh )(xk\nh) ≤\nK\nX\nk=1\n(1 + 1/H)\n\u0010\nH1[nk\nh(Bk\nh)=0] + clip\nh\nβnk\nh(Bk\nh) | gaph(xk\nh, ak\nh)/(H + 1)\ni\n+ ξk\nh+1\n\u0011\n+ (1 + 2/H)3\nK\nX\nk=1\n(V k\nh+1 −V πk\nh+1)(xk\nh+1),\nwhere ξk\nh+1 is conditionally centered random variable with range H.\nWe bound V k\n1 −V πk\n1 , and by optimism the regret, by applying Lemma 6 recursively.\nThe last step is to show that the sum of clipped surpluses can be related to the zooming dimension. First\nnote that for any ball B, the buffering process implies that it is updated at least 1/4 (dmax/r(B))2 times before\nit becomes activated. If it becomes activated but only contains points with large gap, we can always clip the\nsurplus term. Thus all active balls B that have r(B) ≪minx,a∈B gap(x, a) do not contribute to the regret.\nNext, if a ball with radius r contains a point where the gap is small, we cannot appeal to clipping.\nHowever, by Lipschitzness, all points in the ball must have small gaps, which means that this ball is contained\nin the near optimal set at scale r. As above, the surplus for each of these balls contributes at most dmax/r to\nthe regret. Then, since all balls with radius r are at least r apart and we only incur regret for those entirely\ncontained in the near-optimal region, we obtain the bound that depends on Npack\nr\n(PQ⋆\nh,r).\nRemarks on Assumption 2.\nWe give some intuition on why our proof requires (2), which is slightly\nstronger than what is required for the zooming dimension analysis of Slivkins (2014) for contextual bandits.\nIn Slivkins (2014), the optimistic selection rule ensures that the context-action pairs chosen by the algorithm\nhave small gap, but this is not true in the multi-step setting. In the RL setting, we might select an action (in a\nball) with a large gap because the downstream regret is large. In this case, we can clip the surplus, but we can\nonly clip at the minimum gap among all (x, a) pairs in the ball. To obtain a zooming dimension bound, we\nmust argue that this ball is contained in the near-optimal set, but this requires that the value functions, and\nhence the gaps, are Lipschitz. We recall that (2) is implied by (1) if the metric is sub-additive.\n10\n6\nDiscussion\nIn this paper, we give a reﬁned analysis of a variant of the Adaptive Q-learning algorithm of Sinclair, Banerjee\nand Yu (2019) for sample efﬁcient reinforcement learning in metric spaces. We show that the algorithm has\na regret bound that depends on the zooming dimension of the instance, with rate K\nz+1\nz+2 when the zooming\ndimension is z. This improves on the worst-case bound that depends on the covering dimension, and can\nbe much better when the Q⋆function concentrates quickly onto a low-dimensional set of actions. The\nbound also recovers that of Slivkins (2014) for contextual bandits in metric spaces, under a slightly stronger\nassumption. The key technique is the clipped regret decomposition of Simchowitz and Jamieson (2019),\nwhich we complement with a novel buffering process and a corresponding book-keeping argument. Our\nresults show that adaptivity to benign instances is possible in RL with metric spaces, and partially mitigate\nthe curse of dimensionality in such settings.\nAcknowledgements\nWe thank Wen Sun and Aleksandrs Slivkins for formative discussions during the conception of this paper.\nWe also thank Max Simchowitz for insightful discussions regarding the clipping technique. Finally, we thank\nChicheng Zhang for identifying the error in the previous version of the paper and for helpful discussions\nregarding the ﬁx.\nReferences\nPeter Auer, Ronald Ortner, and Csaba Szepesvári. Improved rates for the stochastic continuum-armed bandit\nproblem. In Conference on Learning Theory, 2007.\nMohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement\nlearning. In International Conference on Machine Learning, 2017.\nSébastien Bubeck, Rémi Munos, Gilles Stoltz, and Csaba Szepesvári. X-armed bandits. Journal of Machine\nLearning Research, 2011.\nChristoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform PAC bounds for\nepisodic reinforcement learning. In Advances in Neural Information Processing Systems, 2017.\nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efﬁcient? In\nAdvances in Neural Information Processing Systems, 2018.\nSham Kakade, Michael J Kearns, and John Langford. Exploration in metric state spaces. In International\nConference on Machine Learning, 2003.\nRobert Kleinberg, Aleksandrs Slivkins, and Eli Upfal. Bandits and experts in metric spaces. Journal of the\nACM, 2019.\nAkshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual bandits with\ncontinuous actions: Smoothing, zooming, and adapting. In Conference on Learning Theory, 2019.\nChengzhuo Ni, Lin F Yang, and Mengdi Wang. Learning to control in metric space with optimal regret. In\nAllerton Conference on Communication, Control, and Computing, 2019.\nRonald Ortner. Adaptive aggregation for reinforcement learning in average reward markov decision processes.\nAnnals of Operations Research, 2013.\n11\nRonald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforcement learning.\nIn Advances in Neural Information Processing Systems, 2012.\nMax Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular MDPs. In\nAdvances in Neural Information Processing Systems, 2019.\nSean R Sinclair, Siddhartha Banerjee, and Christina Lee Yu. Adaptive discretization for episodic reinforcement\nlearning in metric spaces. ACM Conference on Measurement and Analysis of Computing Systems, 2019.\nAleksandrs Slivkins. Contextual bandits with similarity information. Journal of Machine Learning Research,\n2014.\nZhao Song and Wen Sun. Efﬁcient model-free reinforcement learning in metric spaces. arXiv:1905.00475,\n2019.\nAhmed Touati, Adrien Ali Taiga, and Marc G Bellemare. Zooming for efﬁcient model-free reinforcement\nlearning in metric spaces. arXiv:2003.04069, 2020.\nMichal Valko, Alexandra Carpentier, and Rémi Munos. Stochastic simultaneous optimistic optimization. In\nInternational Conference on Machine Learning, 2013.\nAndrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning\nwithout domain knowledge using value function bounds. In International Conference on Machine Learning,\n2019.\n12\n10\n15\n20\n25\nTime step t\nlogarithmic scale\n≤1/H2\nNsplit\nThe amortizing scheme, H=3, i=4\nαi\nt\neαi\nt\nFigure 2: An illustration of the amortizing argument which relates the sequence ˜αi\nt to αi\nt.\nA\nAppendix\nIn this section we provide a detailed proof for the main theorem. First we state some facts about the learning\nrate and the algorithm. The ﬁrst lemma regarding the learning rate sequence is directly from Jin et al. (2018).\nLemma 7 (Lemma 4.1 from Jin et al. (2018)). Let αi\nt := αi\nQt\nj=i+1(1 −αj). Then for every i ≥1:\n∞\nX\nt=i\nαi\nt = 1 + 1\nH .\nThe next lemma, also regarding the learning rate sequence, is new. This lemma shows how the skipped\nupdates that arise due to our buffering scheme do not signiﬁcantly compromise the regret bound. The proof is\nbased on an amortizing argument, which is illustrated in Figure 2.\nLemma 8. Fix i ≥H2 and T0 ≥0. Consider the sequence {αi\nt}t≥i, and let {˜αi\nt}t≥i be the sequence formed\nby repeating every Hth term in {αi\nt}t≥i starting at t = T0. Then\n∞\nX\nt=i\n˜αi\nt ≤\n\u0012\n1 + 2\nH\n\u00132\n.\nProof. We can rewrite the sum of ˜αi\nt as\n∞\nX\nt=i\n˜αi\nt =\n∞\nX\nt=i\nαi\nt +\n∞\nX\nj=0\nαi\nT0+jH.\nWe will show how to absorb the second sum into the ﬁrst, and we will use a crediting scheme as in Figure 2.\nThe ﬁrst observation is that\nαi\nT0+jH ≤1\nH\nH−1\nX\nk=0\nαi\nT0+jH−k,\n13\nsince αi\nt is a decreasing sequence. This observation immediately addresses any term for which the H previous\nterms appear in the ﬁrst sequence. This is any term where T0 + jH −i ≥H.\nWe just have to handle the terms where j is such that T0 + jH −i < H. In this case we must have j = 0.\nUsing the fact that i ≥H2 we obtain\nαi\nT0 ≤αi\ni ≤\nH + 1\nH + H2 ≤1\nH .\nPutting these two observations together, we haven\n∞\nX\nt=i\n˜αi\nt =\n∞\nX\nt=i\nαi\nt +\n∞\nX\nj=0\nαi\nT0+jH =\n∞\nX\nt=i\nαi\nt +\n∞\nX\nj=1\nαi\nT0+jH + αi\nT0\n≤\n∞\nX\nt=i\nαi\nt + 1\nH\n∞\nX\nt=i\nαi\nt + 1\nH ≤\n\u0012\n1 + 1\nH\n\u00132\n+ 1\nH ≤\n\u0012\n1 + 2\nH\n\u00132\n.\nThe last step uses Lemma 7.\nThe next lemma establishes some basic facts on the counters used in the algorithm.\nLemma 9. For any k ∈[K], h ∈[H] and ball B ∈Fk\nh, B′ ∈Pk\nh we have\nnk\nh(B) ∈[0, Nmin(B) −1],\n˜nk\nh(B) = 0\nNmin(B′) ≤nk\nh(B′) ≤˜nk\nh(B′) ≤(HΛ + Λ + 1)\n\u0012dmax\nr(B)\n\u00132\n=: Nmax(B)\nProof. For B ∈Fk\nh, the upper bound on the number of updates comes directly from the rule to move B\ninto the active set. Additionally, the ball in the buffering set will not be played according to the deﬁnition\nof relk\nh(x), so ˜nk\nh(B) = 0. For B′, at the time it is added to Pk\nh we have nk\nh(B′) = ˜nk\nh(B′). It will only be\nupdated if it is played, but if it is played it is not necessarily updated, so we have nk\nh(B′) ≤˜nk\nh(B′).\nThe ﬁnal bound is less obvious. A ball B will no longer be played if all of its children are in the active\nset, by deﬁnition of relk\nh(x). Further, by the deﬁnition of domk\nh, when a ball “passes down” its update\nto a child, that child must be in the buffering set (otherwise the state action pair is not in the domain of\nB). Before splitting, B is played at most\n\u0010\ndmax\nr(B)\n\u00112\ntimes. After splitting, each child will be updated at\nmost 1\n4\n\u0010\ndmax\nr(B)/2\n\u00112\n=\n\u0010\ndmax\nr(B)\n\u00112\nbefore being moved to the active set. Since we have at most Λ children (by\ndeﬁnition of the doubling constant) and we play the parent ball (H + 1) times for each update to a child, we\nobtain the bound (HΛ + Λ + 1)\n\u0010\ndmax\nr(B)\n\u00112\n.\nNext we prove an elementary bound on the bias incurred by some ball.\nLemma 10. For any (x, a, h, k) ∈S × A × [H] × [k] and ball B ∈Pk\nh with (x, a) ∈domk\nh(B), if B is\nupdated at step h in episodes k1 < k2 < · · · < kt < k, where t = nk\nh(B), then\nt\nX\ni=1\nαi\nt|Q⋆\nh(xki\nh , aki\nh ) −Q⋆\nh(x, a)| ≤4Ldmax\np\n(HΛ + Λ + 1) 1\n√\nt.\nProof. By Lemma 9, we have nk\nh(B) ≤(HΛ + Λ + 1)\n\u0010\ndmax\nr(B)\n\u00112\n. Re-arranging, we ﬁnd that r(B) ≤\ndmax\nr\n(HΛ+Λ+1)\nnk\nh(B)\n. Of course we always have αi\nt ≤1, and so, by Lipschitzness we have\nt\nX\ni=1\nαi\nt|Q⋆\nh(xki\nh , aki\nh ) −Q⋆\nh(x, a)| ≤2Lr(B)\nt\nX\ni=1\nαi\nt ≤2Ldmax\np\n(HΛ + Λ + 1) 1\n√\nt.\n14\nTo bound the regret, our starting point is an upper bound on the difference between the optimistic\nQ–function and the optimal Q⋆function.\nLemma 11. For any δ ∈(0, 1) if βt := 2 Pt\ni=1 αi\ntb(i) then\nβt ≤8\nr\nH3 log(4HK/δ)\nt\n+ 16Ldmax\np\n(HΛ + Λ + 1)\n√\nt\n.\nAdditionally, with probability at least 1 −δ/2 the following holds simultaneously for all (x, a, h, k) ∈\nS × A × [H] × [K] and ball B such that (x, a) ∈domk\nh(B):\n0 ≤Qk\nh(B) −Q⋆\nh(x, a) ≤1[t=0]H + βt +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1),\nwhere t = nk\nh(B), and k1 < · · · < kt are the episodes where B was previously updated by the algorithm.\nProof. This is a modiﬁed version of Lemma E.7 from Sinclair et al. (2019). The proof is exactly the same as\nthe original, except that we use a larger bonus term b(i) to account for larger upper bound in Lemma 10.\nThis bound contains three parts. The ﬁrst is an upper bound for the ﬁrst step when there is no data. The\nsecond term, βt, is the surplus that we add to ensure optimism. The third part is an “average” of the estimated\nfuture regret. The key observation is that when βt is small, it can be absorbed into the future regret. In this\nway, we can clip βt proportionally to the future regret which enables a form of gap dependent regret bound.\nThis clipping feature is demonstrated in the next lemma. Recall the deﬁnition clip[µ | ν] := µ1{µ ≥ν}.\nLemma 12 (Clipped upper bound). For any δ ∈(0, 1) if βt := 2 Pt\ni=1 αi\ntb(i). With probability at least\n1 −δ/2, ∀h ∈[H], k ∈[K],\nQk\nh(Bk\nh) −Q⋆\nh(xk\nh, ak\nh) ≤\n\u0012\n1 + 1\nH\n\u0013  \n1[t=0]H +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1)\n!\n+ clip\nh\nβt | gaph(xk\nh, ak\nh)/(H + 1)\ni\n.\nProof. We use a⋆\nh : X →A to denote a mapping from the state to the optimal action at stage h. By the\ndeﬁnition of the gap\ngaph(xk\nh, ak\nh) = Q⋆\nh(xk\nh, a⋆\nh(xk\nh)) −Q⋆(xk\nh, ak\nh) ≤Qk\nh(Bk⋆\nh ) −Q⋆\nh(xk\nh, ak\nh)\n≤Qk\nh(Bk\nh) −Q⋆\nh(xk\nh, ak\nh) ≤1[t=0]H + βt +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1),\nwhere Bk⋆\nh is any ball in relk\nh(xk\nh) such that (xk\nh, a⋆\nh(xk\nh)) ∈domk\nh(Bk⋆\nh ) (note that such a ball must exist).\nThe ﬁrst inequality is by the lower bound of Lemma 11, namely the optimism of Qk\nh. The second uses the\nselection rule of choosing the ball with the largest Qk\nh(B) among those in relk\nh(xk\nh). The third inequality is\nby the upper bound of Lemma 11.\nNow we consider two cases, if βt > gaph(xk\nh, ak\nh)/(H + 1), the bound is trivially implied by Lemma 11.\nIf βt ≤gaph(xk\nh, ak\nh)/(H + 1), then\ngaph(xk\nh, ak\nh) ≤1[t=0]H + βt +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1)\n≤1[t=0]H +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1) + gaph(xk\nh, ak\nh)/(H + 1)\n15\nBy re-arranging to move all gap terms to the left hand side, we have\ngaph(xk\nh, ak\nh) ≤H + 1\nH\n \n1[t=0]H +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1)\n!\nBy Lemma 11 and our assumption\nQk\nh(Bk\nh) −Q⋆\nh(xk\nh, ak\nh) ≤1[t=0]H + βt +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1)\n< 1[t=0]H + gaph(xk\nh, ak\nh)/(H + 1) +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1)\n≤\n\u0012\n1 + 1\nH\n\u0013  \n1[t=0]H +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1)\n!\n.\nThe next step is to replace the future regret to V ⋆with the future regret of V πk, so that we can solve for\nthe h = 1 case recursively.\nLemma 13 (Clipped recursion). For any δ ∈(0, 1) if βt := 2 Pt\ni=1 αi\ntb(i). With probability at least 1 −δ/2,\n∀h ∈[H], k ∈[K],\nK\nX\nk=1\n(V k\nh −V πk\nh )(xk\nh) ≤\nK\nX\nk=1\n\u0012\n1 + 1\nH\n\u0013 \u0012\nH1[nk\nh(Bk\nh)=0] + ξk\nh+1 + clip\n\u0014\nβnk\nh(Bk\nh) | gaph(xk\nh, ak\nh)\nH + 1\n\u0015\u0013\n+\n\u0012\n1 + 2\nH\n\u00133 K\nX\nk=1\n(V k\nh+1 −V πk\nh+1)(xk\nh+1),\nwhere ξk\nh+1 = E\n\u0002\nV ⋆\nh+1(x) −V πk\nh+1(x) | xk\nh, ak\nh\n\u0003\n−(V ⋆\nh+1 −V πk\nh+1)(xk\nh+1).\nProof. First, consider stage h in episode k and let Bk\nh be the ball that is chosen. Deﬁne t = nk\nh(Bk\nh). Then\nby applying the previous lemma, we have\nV k\nh (xk\nh) −V πk\nh (xk\nh) =\nmax\nB∈relk\nh(xk\nh)\nQk\nh(B) −Qπk\nh (xk\nh, ak\nh) = Qk\nh(Bk\nh) −Qπk\nh (xk\nh, ak\nx)\n= Qk\nh(Bk\nh) −Q⋆\nh(xk\nh, ak\nh) + Q⋆\nh(xk\nh, ak\nh) −Qπk\nh (xk\nh, ak\nx)\n≤\n\u0012\n1 + 1\nH\n\u0013  \n1[t=0]H +\nt\nX\ni=1\nαi\nt(V ki\nh+1 −V ⋆\nh+1)(xki\nh+1)\n!\n+ clip\n\u0014\nβt | gaph(xk\nh, ak\nh)\nH + 1\n\u0015\n+ (V ⋆\nh+1 −V πk\nh+1)(xk\nh+1) + ξk\nh+1.\nSumming over episodes, let tk\nh = nk\nh(Bk\nh) and let ki(Bk\nh) be the episode where tk\nh is incremented for the ith\ntime.\nK\nX\nk=1\nV k\nh (xk\nh) −V πk\nh (xk\nh) ≤\nK\nX\nk=1\n\u0012\n1 + 1\nH\n\u0013 \u0012\n1[nk\nh(Bk\nh)=0]H + clip\n\u0014\nβnk\nh(Bk\nh), gaph(xk\nh, ak\nh)\nH + 1\n\u0015\u0013\n+\n\u0012\n1 + 1\nH\n\u0013 K\nX\nk=1\nnk\nh(Bk\nh)\nX\ni=1\nαi\nnk\nh(V\nki(Bk\nh)\nh+1\n−V ⋆\nh+1)(x\nki(Bk\nh)\nh+1\n)\n+\nK\nX\nk=1\n\u0010\n(V ⋆\nh+1 −V πk\nh+1)(xk\nh+1) + ξk\nh+1\n\u0011\n.\n16\nFor any ball B, let T0(B) be the ﬁrst time that it is played but not updated, i.e., the ﬁrst time that ˜nk\nh(B) >\nnk\nh(B). In the terminology of lemma 8, for any ball B, we deﬁne the sequence ˜αi\nt(T0(B)) with this value of\nT0. Now, using the observation in Jin et al. (2018); Song and Sun (2019), we can rearrange the second term\nand use lemma 8:\nK\nX\nk=1\nnk\nh\nX\ni=1\nαi\nnk\nh(V\nki(Bk\nh)\nh+1\n−V ⋆\nh+1)(x\nki(Bk\nh)\nh+1\n) ≤\nK\nX\nk=1\n(V k\nh+1 −V ⋆\nh+1)(xk\nh+1)\n∞\nX\nt=nk\nh\n˜α\nnk\nh\nt (T0(Bk\nh))\n≤\n\u0012\n1 + 2\nH\n\u00132 K\nX\nk=1\n(V k\nh+1 −V ⋆\nh+1)(xk\nh+1).\nThe ﬁrst inequality is based on the following reasoning: The left hand side is “backward” looking, in the\nsense that for each episode k the expression involves the previous updates to the ball played. On the other\nhand, the right hand side is “forward” looking, in that episode k also results in an update to some ball (which\nmay not be the one that is played), and so it appears every subsequent time the latter ball is played. Thus,\nrather than looking at the previous updates to the ball played in episode k, we can look at the future plays of\nthe ball updated in episode k. This is why we switch the weight sequence from αi\nnk\nh to ˜α\nnk\nh\nt , where recall that\nthe latter has every Hth term repeated, possibly after some initial burn-in phase.\nSince V πk\nh+1(xk\nh+1) ≤V ⋆\nh+1(xk\nh+1), we have\n\u0012\n1 + 1\nH\n\u0013 \u0012\n1 + 2\nH\n\u00132 K\nX\nk=1\n(V k\nh+1 −V ⋆\nh+1)(xk\nh+1) +\nK\nX\nk=1\n(V ⋆\nh+1 −V πk\nh+1)(xk\nh+1)\n≤\n\u0012\n1 + 2\nH\n\u00133  K\nX\nk=1\n(V k\nh+1 −V ⋆\nh+1)(xk\nh+1) +\nK\nX\nk=1\n(V ⋆\nh+1 −V πk\nh+1)(xk\nh+1)\n!\n=\n\u0012\n1 + 2\nH\n\u00133 K\nX\nk=1\n(V k\nh+1 −V πk\nh+1)(xk\nh+1).\nSo we have\nK\nX\nk=1\n(V k\nh −V πk\nh )(xk\nh) ≤\nK\nX\nk=1\n\u0012\n1 + 1\nH\n\u0013 \u0012\nH1[nk\nh(Bk\nh)=0] + ξk\nh+1 + clip\n\u0014\nβnk\nh(Bk\nh) | gaph(xk\nh, ak\nh)\nH + 1\n\u0015\u0013\n+\n\u0012\n1 + 2\nH\n\u00133 K\nX\nk=1\n(V k\nh+1 −V πk\nh+1)(xk\nh+1).\nThere are two terms that we need to bound. The ξk\nh+1 term can be bounded by a concentration argument\nas shown in Sinclair et al. (2019).\nLemma 14 (Azuma–Hoeffding bound, Lemma E.9 from Sinclair et al. (2019)). For any δ ∈(0, 1), with\nprobability at least 1 −δ/2\nH\nX\nh=1\nk\nX\nk=1\nξk\nh+1 ≤2\np\n2H3K log(4HK/δ).\nThe clipped βt term requires a more reﬁned treatment to relate it to the zooming number or zooming\ndimension. Recall our deﬁnition of the near-optimal space\nPQ⋆\nh,r = {(x, a) : gaph(x, a) ≤c1r},\n17\nwhere c1 = 2(H+1)\ndmax\n+ 2L. Deﬁne the stage-dependent zooming number as\nzh,c = inf{d > 0 : |PQ⋆\nh,r| ≤cr−d}.\nThe following is our key lemma that bounds surplus βt using the zooming number.\nLemma 15.\nH\nX\nh=1\nK\nX\nk=1\nclip[βnk\nh, gaph(xk\nh, ak\nh)\nH + 1\n] ≤\nH\nX\nh=1\n32(\np\nH3 log(4HK/δ) + Ldmax\n√\n2HΛ)\ninf\nr0∈(0,dmax]\n\n\nX\nr=dmax2−i,r≥r0\nNpack\nr\n(PQ⋆\nh,r)2dmax\n√\n2HΛ\nr\n+ 2Kr0\ndmax\n\n.\nProof. Let c2 = 16(\np\nH3 log(4HK/δ) + Ldmax\n√\n2HΛ). By Lemma 11 we have\nβnk\nh ≤16(\np\nH3 log(4HK/δ) + Ldmax\n√\nHΛ + Λ + 1)\n1\nq\nnk\nh\n≤\nc2\nq\nnk\nh\nLet Nmin(B) = 1\n4\n\u0010\ndmax\nr(B)\n\u00112\n, and Nmax(B) =\n\u0010\ndmax\nr(B)\n\u00112\n(HΛ + Λ + 1). Considering Lemma 9, we know that\nwhenever βnk\nh appears in our regret bound (which only happens when a ball is played), we have\nNmin(B) ≤nk\nh(B) ≤Nmax(B).\nLetting gaph(B) = min(x,a)∈B gaph(x, a) be the minimum gap B, we can rearrange the sum for each ball.\nK\nX\nk=1\nclip\n\u0014\nβnk\nh(Bk\nh) | gaph(xk\nh, ak\nh)\nH + 1\n\u0015\n≤\nX\nB∈PK\nh\nNmax(B)\nX\nn=Nmin(B)\nclip\n\u0014 c2\n√n | gaph(B)\nH + 1\n\u0015\n≤c2\nX\nB∈PK\nh\nNmax(B)\nX\nn=Nmin(B)\nclip\n\u0014 1\n√n, gaph(B)\nH + 1\n\u0015\nThe last step is due to the fact that c2 > 1 and if c2\n√n < gaph(B)\nH+1\nthen\n1\n√n < gaph(B)\nH+1 . Now, ignoring clipping,\nthe inner sum can be bounded by\nNmax(B)\nX\nn=Nmin(B)\n1\n√n ≤\nZ Nmax(B)\ni=0\n1\np\ni + Nmin(B)\n≤2dmax\n√\nHΛ + Λ + 1\nr(B)\n.\nWith clipping, we consider two cases.\nCase 1: If gaph(B) ≥2(H+1)r(B)\ndmax\n, then the regret on ball B will always be clipped:\n1\nq\nnk\nh(B)\n≤\n1\np\nNmin(B)\n= 2r(B)\ndmax\n≤gaph(B)\nH + 1 .\nCase 2: If gaph(B) < 2(H+1)r(B)\ndmax\n, then we will pay 2dmax\n√\n2HΛ/r(B) for this ball. However, we will\nshow that this ball also belongs to the near optimal set, so that we do not incur this term too many times.\n18\nLet (xc, ac) be the center of B and (xm, am) ∈B be the point that has the minimum gap, i.e. the point\nthat achieves gaph(B). Using the assumption that Q⋆and V ⋆are Lipschitz:\ngaph(xc, ac) −gaph(B) = Q⋆\nh(xc, a⋆\nh(xc)) −Q⋆\nh(xc, ac) −(Q⋆\nh(xm, a⋆\nh(xm)) −Q⋆\nh(xm, am))\n≤2Lr(B),\nso we know that all the points in B have small gaps relative to r. In particular,\ngaph(xc, ac) ≤gaph(B) + 2Lr(B) ≤2(H + 1)r(B)\ndmax\n+ 2Lr(B).\nThus, we have (xc, ac) ∈PQ⋆\nh,r(B). Now we are ready bound the sum. Note that for a ball B ∈PK\nh , either B\ngets clipped, or the center of B is in PQ⋆\nh,r(B). Since all the balls of radius r are at least r apart, we can have at\nmost Npack\nr\n(PQ⋆\nh,r) in the latter case.\nK\nX\nk=1\nclip\n\u0014\nβnk\nh | gaph(xk\nh, ak\nh)\nH + 1\n\u0015\n≤\nX\nB∈PK\nh\nNmax(B)\nX\nn=Nmin(B)\nclip\n\u0014 c2\n√n | gaph(B)\nH + 1\n\u0015\n≤c2\ninf\nr0∈(0,dmax]\n\n\nX\nr=dmax2−i,r≥r0\nNpack\nr\n(PQ⋆\nh,r)2dmax\n√\n2HΛ\nr\n+ 2Kr0\ndmax\n\n.\nThe second term uses the fact that for any ball B with r(B) ≤r0, we have Nmin(B) ≤1\n4\n\u0010\ndmax\nr0\n\u00112\n.\nNow we are ready to prove Theorem 1.\nProof of Theorem 1. We apply Lemma 13 recursively.\nK\nX\nk=1\n(V k\n1 −V πk\n1 )(xk\n1)\n≤(H + 1) +\nK\nX\nk=1\n\u0012\n1 + 1\nH\n\u0013 \u0012\nξk\n2 + clip\n\u0014\nβnk\n1(Bk\n1 ) | gap1(xk\n1, ak\n1)\nH + 1\n\u0015\u0013\n+\n\u0012\n1 + 2\nH\n\u00133 K\nX\nk=1\n(V k\n2 −V πk\n2 )(xk\n2)\n≤\nH\nX\nh=1\nH\n\u0012\n1 + 2\nH\n\u00133(h−1)\n+\nH\nX\nh=1\n\u0012\n1 + 2\nH\n\u00133h K\nX\nk=1\n\u0012\nξk\nh+1 + clip\n\u0014\nβnk\nh(Bk\nh) | gaph(xk\nh, ak\nh)\nH + 1\n\u0015\u0013\n≤404H2 + 404\nH\nX\nh=1\nK\nX\nk=1\n\u0012\nclip\n\u0014\nβnk\nh(Bk\nh) | gaph(xk\nh, ak\nh)\nH + 1\n\u0015\n+ ξk\nh+1\n\u0013\n.\nHere we are using that (1 + 2/H)3H ≤\n\u0000(1 + 2/H)H/2\u00016 ≤e6 ≤404. Next, we use the Azuma-Hoeffding\ninequality above to obtain:\n404\nH\nX\nh=1\nK\nX\nk=1\nξk\nh+1 ≤808\np\n2H3K log(4HK/δ).\n19\nFinally, we use Lemma 15 to bound the clipped surplus term:\n404\nH\nX\nh=1\nK\nX\nk=1\nclip\n\u0014\nβnk\nh(Bk\nh) | gap(xk\nh, ak\nh)\nH + 1\n\u0015\n≤12928\nH\nX\nh=1\n\u0010p\nH3 log(4HK/δ) + Ldmax\n√\n2HΛ\n\u0011\n×\ninf\nr0∈(0,dmax]\n\n\nX\nr=dmax2−i,r≥r0\nNpack\nr\n(PQ⋆\nh,r)2dmax\n√\n2HΛ\nr\n+ 2Kr0\ndmax\n\n.\nCombining the above bounds, we obtain the theorem.\n20\n",
  "categories": [
    "cs.LG",
    "stat.ML"
  ],
  "published": "2020-06-18",
  "updated": "2021-10-20"
}