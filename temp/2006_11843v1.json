{
  "id": "http://arxiv.org/abs/2006.11843v1",
  "title": "Unsupervised Learning of Deep-Learned Features from Breast Cancer Images",
  "authors": [
    "Sanghoon Lee",
    "Colton Farley",
    "Simon Shim",
    "Yanjun Zhao",
    "Wookjin Choi",
    "Wook-Sung Yoo"
  ],
  "abstract": "Detecting cancer manually in whole slide images requires significant time and\neffort on the laborious process. Recent advances in whole slide image analysis\nhave stimulated the growth and development of machine learning-based approaches\nthat improve the efficiency and effectiveness in the diagnosis of cancer\ndiseases. In this paper, we propose an unsupervised learning approach for\ndetecting cancer in breast invasive carcinoma (BRCA) whole slide images. The\nproposed method is fully automated and does not require human involvement\nduring the unsupervised learning procedure. We demonstrate the effectiveness of\nthe proposed approach for cancer detection in BRCA and show how the machine can\nchoose the most appropriate clusters during the unsupervised learning\nprocedure. Moreover, we present a prototype application that enables users to\nselect relevant groups mapping all regions related to the groups in whole slide\nimages.",
  "text": "XXX-X-XXXX-XXXX-X/XX/$XX.00 Â©20XX IEEE \nUnsupervised Learning of Deep-Learned Features \nfrom Breast Cancer Images. \n \nSanghoon Lee  \nDepartment of Computer Sciences and \nElectrical Engineering \nMarshall University  \nHuntington, WV \nleesan@marshall.edu \nYanjun Zhao \nDepartment of Computer Science \nTroy University \nTroy, AL \nyjzhao@troy.edu \nColton Farley \nDepartment of Computer Sciences and \nElectrical Engineering \nMarshall University  \nHuntington, WV \nfarley195@live.marshall.edu \nWookjin Choi \nDepartment of Engineering and \nComputer Science  \nVirginia State University \nPetersburg, VA \nwchoi@vsu.edu\nSimon Shim \nDepartment of Biomedical Engineering \nMarshall University  \nHuntington, WV \nshim@marshall.edu \nWook-Sung Yoo \nDepartment of Computer Sciences and \nElectrical Engineering \nMarshall University  \nHuntington, WV  \nyoow@marshall.edu  \nAbstractâ€” Detecting cancer manually in whole slide images \nrequires significant time and effort on the laborious process. \nRecent advances in whole slide image analysis have stimulated \nthe growth and development of machine learning-based \napproaches that improve the efficiency and effectiveness in the \ndiagnosis of cancer diseases. In this paper, we propose an \nunsupervised learning approach for detecting cancer in breast \ninvasive carcinoma (BRCA) whole slide images. The proposed \nmethod is fully automated and does not require human \ninvolvement during the unsupervised learning procedure. We \ndemonstrate the effectiveness of the proposed approach for \ncancer detection in BRCA and show how the machine can \nchoose the most appropriate clusters during the unsupervised \nlearning procedure. Moreover, we present a prototype \napplication that enables users to select relevant groups mapping \nall regions related to the groups in whole slide images.  \nKeywordsâ€”Breast cancer, whole slide images, machine \nlearning, unsupervised learning, k-means clustering \nI. INTRODUCTION \nThe advances in whole slide imaging technologies \ncombined with the modern microscope equipment have \ngenerated a large volume of high-resolution digital images \nfrom microscopic slides providing highly reliable and useful \nimage data in determining treatment outcome [1][2]. \nSeveral computerized approaches have been introduced \nto analyze whole slide images (WSIs) avoiding the tedious \nand error-prone manual process, and the digital image \nanalysis approach based on machine learning techniques has \nshown successful results in detecting and diagnosing human \ncancers [3]. Machine learning-based approaches to analyze \ntissue samples obtained for the microscopic examination can \nprovide satisfactory outcomes and validated clinical \nprediction, yielding software tools with onboard machine \nlearning can assure faster turnaround times in a quantitative \nanalysis of WSIs [4]. The machine learning-based \napproaches assist domain experts in providing definitive \ndiagnoses and effective treatments in digital histopathology, \npromoting the transition of modern pathology. \nInvolving domain experts in validating machine learning \nmodels is common and is associated with the prediction of \noutcome. However, they still leave the potential human error \nor bias [5, 6]. Furthermore, labeling sufficient training \nsamples to ensure accurate classification for hundreds-of-\nmillions of histologic structures is very time-consuming and \nrequires laborious effort, thereby causing a significant delay \nin the diagnostic process [7, 8]. The critical challenges in the \nWSI analysis using the machine learning-based approaches \nare 1) creating a methodology to reduce human efforts and 2) \nintegrating machine learning capabilities into applications. \nUnsupervised learning has been widely used in the field \nof machine learning, including clustering, density estimation, \nand pattern recognition. Unlike supervised learning where all \ntraining instances are required to be labeled, unsupervised \nlearning does not involve training instancesâ€™ labeling effort. \nThe main advantage of the unsupervised learning approaches \nis that they are engaged to explore unknown datasets, and to \ndiscover label patterns and relationships in a given dataset, \nthus avoiding potential human error in the process of machine \nlearning. However, unsupervised learning suffers from a few \nshortcomings: not very specific outcomes, existing outliers, \nand less accurate predictions. \nIn this paper, we propose an unsupervised learning \nmethod to detect cancer in breast invasive carcinoma (BRCA) \nimages. BRCA images obtained from The Cancer Genome \nAtlas at the Genomic Data (TCGA) [9] are pre-processed \nthrough color normalization and square tessellation, \ntransformed into deep-learned features by low-level feature \ncharacteristics. We used K-means clustering to assign the \ndeep-learned features to K clusters as an unsupervised \nlearning algorithm by measuring silhouette coefficients to \ndetermine the number of clusters for maximizing the \nqualifications of the neighboring clusters. The proposed \napproach is fully automated and does not require human \ninvolvement during the unsupervised learning procedure. \nMoreover, we present a prototype application for detecting \ncancer in whole slide images using the unsupervised learning \napproach. The contributions of our paper are that 1) a \nstratified unsupervised learning approach is proposed not \nonly to avoid human effort but also to provide accurate \nprediction outcomes, and 2) a prototype cancer detection tool \nusing the unsupervised learning approach is developed for \nwhole slide image analysis. \nThe rest of the paper is organized as follows. In section 2, \nwe explain well-rounded machine learning approaches to \nidentify breast cancer in histopathological images and \nspatialized \ntechniques \nadopted \nfor \nbreast \ncancer \nidentification. In section 3, we describe the proposed \nunsupervised learning approach using deep-learned features \nextracted from whole slide images for breast cancer detection. \nIn section 4, we present the preliminary results of the \nproposed method. Finally, conclusions and future works will \nbe described in section 5.   \nII. RELATED WORKS \nBoth supervised-learning and unsupervised learning have \nbeen successfully exploited in histopathological image \nanalysis over a few decades [10]. Supervised learning \nmethods aim to predict target sample images using labeled \nsample images, while unsupervised learning methods aim to \nfind hidden patterns from unlabeled sample images. Artificial \nNeural Network (ANN) is a commonly used supervised \nlearning method performing with three-layered neural \nnetworks: an input layer, a hidden layer, and an output layer. \nBased on the three-layered neural networks, unlabeled \nneurons in the output layer are determined. Even though \nANN is suitable for non-linear multivariate problems, it has \nbeen mainly used for survival analysis [11, 12]. Another well-\nknown supervised learning method is Support Vector \nMachine (SVM). SVM has been successfully adopted for \nbreast cancer diagnosis because of its generalization \ncapability avoiding an overfitting problem. However, SVM \nis infeasible for massive datasets and does not provide \nprobabilistic explanations for the classification [13]. On the \nother hand, unsupervised learning methods have been used to \nfind patterns on various types of datasets. K-means clustering \nis a well-known simple, yet powerful unsupervised learning \nmethod that partitions a collection of clusters into K cluster \ngroups. K-means clustering scales to large datasets and \ngeneralizes well to unlabeled clusters, but the manual \nselection of K is an issue to overcome [14]. \nMeanwhile, computer-based image analysis has been \nmore conveniently placed in the histopathological image \ninterpretation. Specifically, representing the digitized images \nhas led to substantial improvements and advances in image \nprocessing methodologies via pre-processing, segmentation, \nand feature extraction. Color normalization has been a critical \nstage \nin \nimproving \nhistopathology \nimage \nanalysis \nperformance because of color variation in histopathology \nimages [15]. Segmentation algorithms have been employed \nto obtain important prognostically related tissue specificity, \nand quantitative features have stimulated histological \nobservations on biopsy specimens, solving the problem of \nobserver variability and reducing the workload of \npathologists. However, hand-crafted features designed before \nimage segmentation require large computational tasks and \nconsideration of tissue appearance (i.e., epithelial cancerous \ncell shows the highly irregular shape and enlarged size, \nremaining a considerable difficulty in identifying cancers). \nConvolutional Neural Network (CNN) has received \nmuch attention with respect to its significant potential in the \nhistopathological image interpretation [16]. While hand-\ncrafted features rely on explicit algorithms requiring heavy \nefforts for novel datasets, features directly learned from raw \ndata are trainable within CNN and can be automatically \ntransformed in supervised-learning and unsupervised \nlearning [17-19]. CNN features have provided better \naccuracy through convolution and pooling layers, followed \nby a fully connected layer for the learning task. In recent \nyears, various innovative CNN models have been introduced \nin the field of machine learning communities. As a part of \nImageNet project, VGG16 [20], a well-known CNN \narchitecture consisting of 16 convolutional layers with lots of \nfilters, has been widely used in image classification tasks [21]. \nIn this paper, we utilize VGG16 to extract the deep-learned \nfeatures from square tessellation regions.  \nIII. UNSUPERVISED LEARNING OF DEEP LEARNED FEATURES \nFROM BREAST CANCER IMAGES \nIn this section, we describe the whole process of the \nproposed unsupervised learning of deep-learned features \nfrom breast cancer images. The overview of the whole slide \nimaging, segmentation, feature extraction, and clustering \nmethods are shown in Fig 1. \nA. Whole slide imaging \nA tissue section in histopathology image analysis is \nmounted on a glass slide through the tissue preparation. Then, \nit is stained based on tissue components (i.e., the nucleus is \ndyed with the blue or purple stain, while the cytoplasm is \ndyed with the pink stain by Hematoxylin & Eosin staining), \nand then converted to a digital image by a digital pathology \nslide scanner. Digitized histopathological images of tissues \nstained with different staining methods are finally prepared \nfor computer-based image analysis. In this paper, we obtained \nseven whole slide images from TCGA. They are all \nhematoxylin and eosin-stained formalin-fixed paraffin-\nembedded (FFPE) sections of histopathology whole slide \nimages of breast invasive carcinoma. \nB. Pre-processing \nColor normalization was performed in the pre-processing \nstep for whole slide images. First of all, we separated the \nforeground (tissue area) from the background using a two-\ncomponent Gaussian mixture model to achieve the accurate \ndifferentiation between the tissue areas and the background \nof each whole slide image. The tissue area was partitioned \ninto 2048 x 2048 x 3 sized tile images, and then normalized \nusing Reinhard color normalization [15] . This normalization \nprocess uses the color distribution of the tissue areas mapped \nto that of the standard color images with the mean and \nstandard deviation in LAB color space. The normalized tissue \nareas were segmented by square tessellation for 128 x 128 x \n3 sized image patches. The pre-processing uses tools from \nHistomicsTK project and HistomicsML2 pipeline and its \nsoftware [22, 23]. \nC. Pipeline for feature extraction \n128 x 128 x 3 sized image patches were resized to 224 x \n224 x 3 sized image patches using nearest neighbor \ninterpolation to represent input features for VGG16 [20]. \nVGG16 architecture comprises three types of layers: \nconvolutional layers, max pooling layers, and fully connected \nlayers. The convolutional layers consist of 13 layers with \ndifferent number of filters. The number of filters in the first \nconvolutional layer is 64, while the number of filters in the \nsecond convolutional layer is 128. VGG16 architecture was \ntrained on the ImageNet dataset with 10+ million and 1000 \nclasses. In this paper, we extracted the first fully connected \nlayer representing a 4096-dimensional feature vector from \nVGG16 and performed an orthogonal linear transformation \ncalled Principal Component Analysis (PCA) to obtain 48 \nfeatures [24]. We obtained 48 features from 4096 features to \navoid latency and system overload during the process of the \nproposed approach. \nD. K-means clustering for K square tessellation region \nidentification \n \nA K-means clustering method to identify the K square \ntessellation region of the whole slide images is presented in \nthis subsection. Forty-eight features extracted by the previous \nsection represent a square tessellation region of whole slide \nimages. In this paper, 100K+ square tessellation regions are \nfinally extracted from the feature extraction pipeline described \nin the previous section. We used K-means clustering to \npartition the square tessellation regions into K distinct groups. \nHowever, determining the optimal number of clusters in the \nwhole dataset is a typical problem for K-means clustering. In \nthis paper, we computed the mean of silhouette coefficient \n[25] of all square tessellation regions to determine the optimal \nnumber of clusters. The average intra-cluster distance is \ncomputed as below: \nğ‘(ğ‘¥) =\nâˆ‘\nğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘¥, ğ‘¦)\n!âˆˆ#!,%&!\n|ğ¾%| âˆ’1\n                    (1) \nFor each square tessellation region ğ‘¥âˆˆğ¾% in the cluster ğ¾%, \nthe mean of intra-cluster distance ğ‘(ğ‘¥) is computed as the \nsum of ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘¥, ğ‘¦), the distance between ğ‘¥ and ğ‘¦ tessellation \nregion excepting the case where ğ‘¥ and ğ‘¦ are same, divided by \n|ğ¾%| âˆ’1. The mean of the nearest cluster distance is computed \nas below:  \nğ‘€(ğ‘¥) = min\n'&%\nâˆ‘\nğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘¥, ğ‘¦)\n!âˆˆ#\"\n|ğ¾'|\n                    (2) \nFor each square tessellation region ğ‘¥âˆˆğ¾% in the cluster ğ¾%, \nthe mean of the nearest cluster distance ğ‘€(ğ‘¥) is computed as \nthe smallest value represented as the sum of ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘¥, ğ‘¦), the \ndistance between ğ‘¥ and ğ‘¦ square tessellation region where ğ‘¥ \nis not same with ğ‘§, divided by |ğ¾'| . Thus, a silhouette \ncoefficient of a square tessellation region is defined as below: \nsilhouette(ğ‘¥) =\nğ‘€(ğ‘¥) âˆ’ğ‘(ğ‘¥)\nmaxBğ‘€(ğ‘¥), ğ‘(ğ‘¥)C              (3) \n, where |ğ¾%| â‰¥2. \n \nK-means clustering as a centroid-based algorithm is \nperformed by calculating the distance between two square \ntessellation regions after obtaining the optimal number of \nclusters in the silhouette coefficient equation. K-means \nclustering is a simple unsupervised learning algorithm that \npartition a dataset into K pre-defined clusters. The algorithm \nis fast, relatively efficient, and even very easy to understand. \nThe assumption is that the dataset is well-separated from each \nother. In this paper, we use the K-means clustering algorithm \nto partition square tessellation regions into K groups. In \naddition to K-means clustering, we identify K representative \nsquare tessellation regions which are the closest to the centroid \nof the groups. In this paper, we use a typical squared error \nfunction as an objective function given by: \n \nFig. 1 Overview of the proposed approach and its application. Whole-slide imaging and segmentation steps are shown at the top. A pipeline for feature \nextraction through VGG16 architecture and a dimensional reduction technique are shown in the middle. A clustering procedure and its application are \nshown in the bottom. \nJ(ğ¶) = H H(||ğ‘ ( âˆ’ğ‘)||)*\n+#\n),-\n+\n(,-\n                        (4) \n, where ğ‘˜ is the number of clusters and ğ‘˜( is the number of \nregions that belong to the ğ‘–th cluster. Besides the clustering \nobjective function, K-means clustering uses an equation for \nidentifying each cluster center over the iterative process. A \ncluster center ğ‘( is computed as below: \nğ‘( =\nâˆ‘\nğ‘ (\n+#\n),-\nğ‘˜(\n                                        (5) \nK means clustering algorithm to find K representative square \ntessellation regions follows:  \n \nAlgorithm 1 K means clustering algorithm to identify the representative \nK square tessellation regions \nInput: S = {ğ‘ !, ğ‘ \", ğ‘ #, â€¦ , ğ‘ $} be the set of square tessellation regions and \nC = {ğ‘!, ğ‘\", ğ‘#, â€¦ , ğ‘%}be the set of centers \nOutput: K representative square tessellation regions \n1: Select k cluster centers randomly. \n2: For every region in S, compute the distance between the region and \ncluster centers using (4). \n3: Find the minimum distance and assign the region to the cluster center.  \n4: Compute cluster centers using (5). \n5: Repeat from Step 2 until no regions found to assign. \n6: For every region that belongs to each cluster in S, compute the \ndistance between the region and the cluster centroid. \n7: Find the minimum distance and assign the region to the representative \nregion for the cluster.  \n \n \n \nThe output of Algorithm 1 for identifying K representative \nsquare tessellation regions is shown in Fig. 2. â€˜TCGA-OL-\nA66I-01Z-00-DX1â€™ represents the name of the whole slide \nimage, and the group names are represented by the numbers, \n7, 14, and 12. Each group includes a representative square \ntessellation region directly linked to other square tessellation \nregions with the same cluster. The square box colored as blue \nat the top of the figure indicates the representative square \ntessellation region of group 7. The square box colored as red \nin the middle shows the representative square tessellation \nregion of group 14, while the square box colored as blue at the \nbottom of the figure indicates the representative square \ntessellation region of group 12. \n \nK-means clustering algorithm to the K representative \nsquare tessellation regions identification provides insights into \nan implementation of a prototype web-based application that \nenables domain experts to determine the K representative \nsquare tessellation regions so that clustered areas can be \ndetected without further consideration. The web application is \nbuilt upon HistomicsML2 to use the interactive learning \nprocess avoiding selection errors of the K-means clustering \nalgorithm. Users can choose the most appropriate samples \nwith the closest centroids applied to the entire dataset. The \ninteractive process is performed over several steps and \nclassifies all square tessellation regions of WSIs as positive or \nnegative. The web application includes a heatmap function for \nthe positive class density determined by the K-means \nclustering algorithm. \n \nThe web application follows a few-shot learning strategy, \na type of machine learning methodology that limits the \nnumber of training samples in a supervised manner but \ngeneralizes to new samples well. This machine learning \nmethodology can be categorized by augmenting training \nsamples, constraining hypothesis space, or altering search \nstrategy in hypothesis space [26], helping to relieve the burden \nof human efforts in a large amount of data. Because WSI \nanalysis requires domain experts to provide sufficient \nannotated samples and very laborious work, it is worth \nconsidering the few-shot learning strategy in the whole slide \nimage analysis. Furthermore, few-shot learning can minimize \nthe data collection effort for users to organize their classifiers \nso that the entire process of analyzing whole slide images via \napplications can be done as quickly as possible. Thus, the \ninteractive process includes a multi-sample selection to \n \nFig. 2 Three representative square tessellation regions that belong to each \ngroup 7, 14, and 12 respectively. Square tessellation regions linked to the \nregions colored as blue in the top and the bottom belong to the groups of 7 \nand 12 respectively, while other square tessellation regions linked to the \nregion colored as red in the middle belong to the groups of 14.  \ndetermine whether the square tessellation regions are positive \nor negative. \n \nAn example of our application following a few-shot \nlearning strategy is shown in Fig 3. 18 selected square \ntessellation regions nearest to each cluster centroids are shown \nin the left figure. The labels colored as green represent positive \ncancer, and others represent negative cancer. A heatmap for a \nwhole slide image is shown in the middle, and more details \nare shown in the right. The prototype of our web application \nis open source and prepared as a docker image. \n \nIV. EVALUATION \nIn this section, we present the experimental results of the \nproposed approach for detecting breast cancer in whole slide \nimages. We describe the dataset, evaluation metrics, and \nprediction results used for the experiments in the section. \nA. Dataset \nSeven Breast invasive carcinoma (BRCA) whole slide \nimages obtained from The Cancer Genome Atlas at the \nGenomic Data (TCGA) [9] were pre-processed, segmented, \nand generated 1,627,434 square tessellation regions. Regions \nof interests (ROIs) were annotated for each BRCA whole \nslide image by two experts. \nB. Evaluation Metrics \nThe evaluation for detecting the cancer regions in the \nBRCA images was performed based on the accuracy and F1-\nscore. We use the accuracy and F1-score metrics as below: \nğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦=\n(ğ‘‡ğ‘ƒ+ ğ‘‡ğ‘)\n(ğ‘‡ğ‘ƒ+ ğ‘‡ğ‘+ ğ¹ğ‘ƒ+ ğ¹ğ‘)                (6) \nğ¹- =\n2 âˆ™ğ‘‡ğ‘ƒ\n2 âˆ™ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ+ ğ¹ğ‘                           (7) \n, where TP represents the true positive, the number of positive \npredictions actually positive. TN represents the true negative, \nthe number of negative predictions correctly identified as \nnegative. FP represents the false positive, the number of \nnegative predictions incorrectly identified as positive. FN \nrepresents the false negative, the number of positive \npredictions incorrectly identified as negative. The results of \nthe evaluation are shown in Table 1. Each row represents the \nresults, including Silhouette optimal number, cluster groups, \naccuracy, and F1-score for each BRCA whole slide image. \nThe Silhouette optimal number was obtained by following (1), \n(2), and (3). These numbers were used as the number of \nclusters to perform K means clustering. The cluster set is the \nlist of clusters used for measuring the accuracy and F1-score. \nFor example, 26 clusters obtained by minimizing the \nSilhouette scores were set for the initial number of clusters \nwhen performing K-means clustering the BRCA whole slide \nimage named â€˜TCGA-E2-A1LS-01Z-00-DX1â€™. We finally \nselected two representative square tessellation regions \nbelonging to the 18th and the 5th clusters to measure the \naccuracy and F1-score. The average accuracy and the average \nf1-score on the dataset are 0.8454 and 0.8293 respectively \nC. Prediction results \nThe representative prediction results of whole slide \nimages (TCGA-OL-A66I-01Z-00-DX1, TCGA-A2-A0YM-\n01Z-00-DX1, TCGA-A2-A3XT-01Z-00-DX1, and TCGA-\nA7-A0DA-01Z-00-DX1) are shown in Fig 4. We selected 4 \nrepresentative prediction results among 7 slides because they \nprovide better performance than others. Some extensive \nresearch works are required to resolve the issue with low \nperformance. The first column represents the original image \nresized from 40x magnification to 5x magnification. The \npredicted regions of cancer colored as blue and tumor-\ninfiltrating lymphocytes colored as red are in the second \ncolumn. The third column represents the cancer heatmap \npredicted by the proposed method. The tumor-infiltrating \nlymphocytes' heatmaps predicted by the proposed method are \nin the last column. The grid regions of each heatmap are sized \nas 40 x 40. The results shown in Fig 4 indicate that TILs \nregions are more likely to be located around the corner from \n \nFig. 3 The prediction resutls on the web application using a few-shot leaning strategy. Eighteen square tessellation regions closest to the centrods of the \nclusters are shown in the left. A heatmap generated from the web application is shown in the middle. A higher zoom level view is shown in the right. \nthe whole slide images surrounding the cancer regions. \nAnalyzing spatial relationships between cancer and the \ntumor-infiltrating lymphocytes benefits the cancer research \ncommunity, but we leave it to future research to demonstrate \nthe effectiveness of the proposed approach. \nV. CONCLUSIONS AND FUGURE WORKS \nIn this paper, we proposed an unsupervised learning \napproach for breast cancer detection in whole slide images. \nWhole slide images were segmented into square tessellation \nregions after pre-processing, and deep-learned features were \nextracted from the square tessellation regions through a deep \nlearning model architecture with a dimensionality reduction \ntechnique. Silhouette coefficients were measured to determine \nthe number of clusters, and the K-means clustering algorithm \nis adopted to obtain K square tessellation regions in the whole \nslide images. In addition to the description of the proposed \napproach, the performance evaluation was conducted in two \nFig. 4 Prediction results for four whole slide images on 40x magnification. The first column represents the original whole slide image, the second column \nrepresents the prediction results (cancer colored as blue while tumor infilterating lymphocytes colored as red), the third column represents a heatmap for \ncancer regions, and the last column represents a heatmap for tumor infilterating lymhocytes regions. The whole slide images at the first two columns are \nresized to 5x magnfication, while the heatmap images at the last two columns are generated in 40 x 40 grids. \n \nwell-known metrics: accuracy and F1-score. The averages of \nthe accuracy and F1-score are 0.8454 and 0.8293 respectively \nover the given dataset. \nTABLE I.  \nCANCER DETECTION ACCURACY AND F1-SCORE \n \nMoreover, we implemented a prototype web-based \napplication enabling users to choose appropriate square \ntessellation regions without the laborious efforts. The \nproposed approach is fully automated and does not require \nfurther inputs during the unsupervised learning procedure. A \nheatmap is displayed with the corresponding clusters when the \nuser selects the square tessellation regions closest to the \ncentroids of the clusters.  \nRecent studies have shown that statistical approaches for \nanalyzing spatial characteristics in histopathological images \nplay an important role in the tumor microenvironment. These \nstudies suggest a new approach to the issues in spatial analysis \nin whole slide images. We plan to investigate and compare \ndifferent spatial analysis methods in the future.  \nREFERENCES \n[1] Pantanowitz, L., Sinard, J.H., Henricks, W.H., Fatheree, L.A., Carter, \nA.B., Contis, L., Beckwith, B.A., Evans, A.J., Lal, A., Parwani, A.V., \nCollege of American Pathologists, P., and Laboratory Quality, C.: \nâ€˜Validating whole slide imaging for diagnostic purposes in pathology: \nguideline from the College of American Pathologists Pathology and \nLaboratory Quality Centerâ€™, Arch Pathol Lab Med, 2013, 137, (12), pp. \n1710-1722 \n[2]  Abels, E., and Pantanowitz, L.: â€˜Current State of the Regulatory \nTrajectory for Whole Slide Imaging Devices in the USAâ€™, J Pathol \nInform, 2017, 8, pp. 23 \n[3] Komura, D., and Ishikawa, S.: â€˜Machine Learning Methods for \nHistopathological Image Analysisâ€™, Comput Struct Biotechnol J, 2018, \n16, pp. 34-42 \n[4] \nCruz-Roa, A., Gilmore, H., Basavanhally, A., Feldman, M., Ganesan, \nS., Shih, N.N.C., Tomaszewski, J., Gonzalez, F.A., and Madabhushi, \nA.: â€˜Accurate and reproducible invasive breast cancer detection in \nwhole-slide images: A Deep Learning approach for quantifying tumor \nextentâ€™, Sci Rep, 2017, 7, pp. 46450 \n[5] \nCampanella, G., Hanna, M.G., Geneslaw, L., Miraflor, A., Werneck \nKrauss Silva, V., Busam, K.J., Brogi, E., Reuter, V.E., Klimstra, D.S., \nand Fuchs, T.J.: â€˜Clinical-grade computational pathology using weakly \nsupervised deep learning on whole slide imagesâ€™, Nat Med, 2019, 25, \n(8), pp. 1301-1309 \n[6] \nKothari, S., Phan, J.H., Stokes, T.H., and Wang, M.D.: â€˜Pathology \nimaging informatics for quantitative analysis of whole-slide imagesâ€™, J \nAm Med Inform Assoc, 2013, 20, (6), pp. 1099-1108 \n[7] \nTopol, E.J.: â€˜High-performance medicine: the convergence of human \nand artificial intelligenceâ€™, Nat Med, 2019, 25, (1), pp. 44-56 \n[8] \nVandenberghe, M.E., Scott, M.L., Scorer, P.W., Soderberg, M., \nBalcerzak, D., and Barker, C.: â€˜Relevance of deep learning to facilitate \nthe diagnosis of HER2 status in breast cancerâ€™, Sci Rep, 2017, 7, pp. \n45938 \n[9] \nTomczak, K., CzerwiÅ„ska, P., and Wiznerowicz, M.: â€˜The Cancer \nGenome Atlas (TCGA): an immeasurable source of knowledgeâ€™, \nContemp Oncol (Pozn), 2015, 19, (1A), pp. A68-A77 \n[10] Arevalo, J., Cruz-Roa, A., Arias, V., Romero, E., and GonzÃ¡lez, F.A.: \nâ€˜An unsupervised feature learning framework for basal cell carcinoma \nimage analysisâ€™, Artificial intelligence in medicine, 2015, 64, (2), pp. \n131-145 \n[11] Janghel, R., Shukla, A., Tiwari, R., and Kala, R.: â€˜Breast cancer \ndiagnosis using artificial neural network modelsâ€™, in Editor \n(Ed.)^(Eds.): â€˜Book Breast cancer diagnosis using artificial neural \nnetwork modelsâ€™ (IEEE, 2010, edn.), pp. 89-94 \n[12] Ahmad, F., Isa, N.A.M., Hussain, Z., Osman, M.K., and Sulaiman, \nS.N.: â€˜A GA-based feature selection and parameter optimization of an \nANN in diagnosing breast cancerâ€™, Pattern Analysis and Applications, \n2015, 18, (4), pp. 861-870 \n[13] Acharya, U.R., Ng, E.Y.-K., Tan, J.-H., and Sree, S.V.: â€˜Thermography \nbased breast cancer detection using texture features and support vector \nmachineâ€™, Journal of medical systems, 2012, 36, (3), pp. 1503-1510 \n[14] Zheng, B., Yoon, S.W., and Lam, S.S.: â€˜Breast cancer diagnosis based \non feature extraction using a hybrid of K-means and support vector \nmachine algorithmsâ€™, Expert Systems with Applications, 2014, 41, (4), \npp. 1476-1482 \n[15] Sethi, A., Sha, L., Vahadane, A.R., Deaton, R.J., Kumar, N., Macias, \nV., and Gann, P.H.: â€˜Empirical comparison of color normalization \nmethods for epithelial-stromal classification in H and E imagesâ€™, \nJournal of pathology informatics, 2016, 7 \n[16] Rouhi, R., Jafari, M., Kasaei, S., and Keshavarzian, P.: â€˜Benign and \nmalignant breast tumors classification based on region growing and \nCNN segmentationâ€™, Expert Systems with Applications, 2015, 42, (3), \npp. 990-1002 \n[17] Lee, S., Amgad, M., Masoud, M., Subramanian, R., Gutman, D., and \nCooper, L.: â€˜An Ensemble-based Active Learning for Breast Cancer \nClassificationâ€™, in Editor (Ed.)^(Eds.): â€˜Book An Ensemble-based \nActive Learning for Breast Cancer Classificationâ€™ (2019, edn.), pp. \n2549-2553 \n[18] Xu, Y., Zhu, J.-Y., Eric, I., Chang, C., Lai, M., and Tu, Z.: â€˜Weakly \nsupervised \nhistopathology \ncancer \nimage \nsegmentation \nand \nclassificationâ€™, Medical image analysis, 2014, 18, (3), pp. 591-604 \n[19] Nayak, N., Chang, H., Borowsky, A., Spellman, P., and Parvin, B.: \nâ€˜Classification of tumor histopathology via sparse feature learningâ€™, in \nEditor (Ed.)^(Eds.): â€˜Book Classification of tumor histopathology via \nsparse feature learningâ€™ (IEEE, 2013, edn.), pp. 410-413 \n[20] Simonyan, K., and Zisserman, A.: â€˜Very deep convolutional networks \nfor large-scale image recognitionâ€™, arXiv preprint arXiv:1409.1556, \n2014 \n[21] Wang, D., Khosla, A., Gargeya, R., Irshad, H., and Beck, A.H.: â€˜Deep \nlearning for identifying metastatic breast cancerâ€™, arXiv preprint \narXiv:1606.05718, 2016 \n[22] Gutman, D.A., Khalilia, M., Lee, S., Nalisnik, M., Mullen, Z., Beezley, \nJ., Chittajallu, D.R., Manthey, D., and Cooper, L.A.: â€˜The digital slide \narchive: A software platform for management, integration, and analysis \nof histology for cancer researchâ€™, Cancer research, 2017, 77, (21), pp. \ne75-e78 \n[23] Lee, S., Amgad, M., Chittajallu, D.R., McCormick, M., Pollack, B.P., \nElfandy, H., Hussein, H., Gutman, D.A., and Cooper, L.A.: \nâ€˜HistomicsML2.0: Fast interactive machine learning for whole slide \nimaging dataâ€™, in Editor (Ed.)^(Eds.): â€˜Book HistomicsML2.0: Fast \ninteractive machine learning for whole slide imaging dataâ€™ (2020, \nedn.), pp. arXiv:2001.11547 \n[24] Wold, S., Esbensen, K., and Geladi, P.: â€˜Principal component analysisâ€™, \nChemometrics and intelligent laboratory systems, 1987, 2, (1-3), pp. \n37-52 \n[25] Rousseeuw, J.: â€˜A graphical aid to the interpretation and validation of \ncluster analysisâ€™, Journal of Computational Application Math, 1989 \nWang, Y., and Yao, Q.: â€˜Few-shot learning: A surveyâ€™, arXiv preprint \narXiv:1904.05046, 2019 \n \n \n \nSlide name \nSilhouette \noptimal \nnumber \nCluster \nset \nAccuracy \nF1-score \nTCGA-A7-A0DA \n29 \n[25, 22, 6, \n2, 24, 14, \n0, 20, 10] \n0.8829 \n0.8929 \nTCGA-A2-A0YM \n20 \n[7, 6, 1, 5] \n0.8360 \n0.8863 \nTCGA-A2-A3XT \n19 \n[13, 10, 5, \n1, 2] \n0.9316 \n0.9514 \nTCGA-BH-A0BG \n8 \n[1, 5] \n0.7828 \n0.6857 \nTCGA-E2-A1LS \n26 \n[18, 5] \n0.8495 \n0.8680 \nTCGA-OL-A66I \n25 \n[7, 18, 12, \n20, 1, 0] \n0.7761 \n0.7091 \nTCGA-C8-A26Y \n21 \n[9, 18, 12, \n16] \n0.8594 \n0.8122 \n",
  "categories": [
    "eess.IV",
    "cs.CV",
    "cs.LG",
    "q-bio.QM"
  ],
  "published": "2020-06-21",
  "updated": "2020-06-21"
}