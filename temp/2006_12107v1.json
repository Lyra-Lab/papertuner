{
  "id": "http://arxiv.org/abs/2006.12107v1",
  "title": "Hidden Markov Nonlinear ICA: Unsupervised Learning from Nonstationary Time Series",
  "authors": [
    "Hermanni Hälvä",
    "Aapo Hyvärinen"
  ],
  "abstract": "Recent advances in nonlinear Independent Component Analysis (ICA) provide a\nprincipled framework for unsupervised feature learning and disentanglement. The\ncentral idea in such works is that the latent components are assumed to be\nindependent conditional on some observed auxiliary variables, such as the\ntime-segment index. This requires manual segmentation of data into\nnon-stationary segments which is computationally expensive, inaccurate and\noften impossible. These models are thus not fully unsupervised. We remedy these\nlimitations by combining nonlinear ICA with a Hidden Markov Model, resulting in\na model where a latent state acts in place of the observed segment-index. We\nprove identifiability of the proposed model for a general mixing nonlinearity,\nsuch as a neural network. We also show how maximum likelihood estimation of the\nmodel can be done using the expectation-maximization algorithm. Thus, we\nachieve a new nonlinear ICA framework which is unsupervised, more efficient, as\nwell as able to model underlying temporal dynamics.",
  "text": "Hidden Markov Nonlinear ICA:\nUnsupervised Learning from Nonstationary Time Series\nHermanni H¨alv¨a\nUniversity of Helsinki\nAapo Hyv¨arinen\nUniversit´e Paris-Saclay, Inria\nUniversity of Helsinki\nAbstract\nRecent advances in nonlinear Independent\nComponent Analysis (ICA) provide a princi-\npled framework for unsupervised feature learn-\ning and disentanglement. The central idea in\nsuch works is that the latent components are as-\nsumed to be independent conditional on some\nobserved auxiliary variables, such as the time-\nsegment index. This requires manual segmenta-\ntion of data into non-stationary segments which\nis computationally expensive, inaccurate and of-\nten impossible. These models are thus not fully\nunsupervised.\nWe remedy these limitations\nby combining nonlinear ICA with a Hidden\nMarkov Model, resulting in a model where a la-\ntent state acts in place of the observed segment-\nindex. We prove identiﬁability of the proposed\nmodel for a general mixing nonlinearity, such\nas a neural network. We also show how max-\nimum likelihood estimation of the model can\nbe done using the expectation-maximization al-\ngorithm. Thus, we achieve a new nonlinear\nICA framework which is unsupervised, more\nefﬁcient, as well as able to model underlying\ntemporal dynamics.\n1\nINTRODUCTION\nRepresentation learning – the task of ﬁnding useful fea-\ntures from data – is one of the main challenges in unsuper-\nvised learning. Recent theoretical and practical advances\nin Nonlinear ICA provide a principled approach to this\nproblem (Hyv¨arinen and Morioka, 2016; Hyv¨arinen and\nMorioka, 2017; Hyv¨arinen et al., 2019; Khemakhem et al.,\n2020; Sorrenson et al., 2020). These works frame Non-\nlinear ICA as deep generative models, which allows them\nto harness deep neural networks to recover latent inde-\npendent components from observed data. Identiﬁability\nProceedings of the 36th Conference on Uncertainty in Artiﬁcial\nIntelligence (UAI), PMLR volume 124, 2020.\nof the latent components can be guaranteed by explicitly\ndeﬁning probabilistic generative models with appropriate\nconditional independence structures. A general frame-\nwork was proposed recently by Hyv¨arinen et al. (2019),\nwho assumed that the components are independent given\nsome other observed auxiliary variable. For example, in\ntime-series data this can be the time-index or segment-\nindex if the data is non-stationary, as was earlier assumed\nin Time-Contrastive Learning or TCL (Hyv¨arinen and\nMorioka, 2016). Non-stationarity is a fundamental prop-\nerty of many applications, since for example, video, audio,\nand most neuroscience data are non-stationary.\nA crucial limitation of all of the above nonlinear ICA\nmodels is that the conditioning auxiliary variable is al-\nways assumed observable. In some sense, these models\nare therefore not fully unsupervised. If, for instance, we\nwish to exploit the nonstationary temporal structure opti-\nmally in estimating independent components, TCL would\nrequire segment indices that correspond to the different\nlatent data generative states. In practice we don’t observe\nsuch states so the default approach is to manually segment\nthe data.\nIn general, it is unrealistic to assume that we can infer\nfrom observed data alone the exact time-points at which\nthe latent data distribution changes. In fact, such change-\npoints may not exist at all. Segmenting data manually is\nalso infeasible for large datasets. The default approach\nis therefore to segment data at equal intervals, however,\nthis is problematic for various reasons. Consider, for\nexample, a situation where the true latent state switches\nbetween ﬁve different states. By segmenting the data at\nequal intervals we will end up with an unnecessarily large\nnumber of states where just a few would have done the job.\nThis is computationally expensive, inaccurate and will\ncompletely miss out on temporal dynamics in situations\nwhere the latent states repeat over time.\nIn fact, often a reasonable assumption is that non-\nstationarity can be succinctly summarized using a limited\narXiv:2006.12107v1  [stat.ML]  22 Jun 2020\nnumber of segment indices or latent states, and prop-\nerly modelling such state switching is likely to improve\nlearning. Notice that even if ground-truth nonstationary\ninformation was available, the existing methods lack the\nmachinery to perform inference on latent temporal dy-\nnamics. In many applications, for example brain imaging,\ndescribing the dynamics in terms of latent states could be\nvery useful in its own right.\nThe points above highlight the need for a nonlinear ICA\nmethod that is able to cluster observations and learn latent\nstates and their temporal dynamics in an unsupervised\nfashion. A well-known approach to modelling hidden la-\ntent states in time series is to use a Hidden Markov Model\n(HMM). HMMs can be viewed as probabilistic mixture\nmodels where the discrete latent states, which specify\nthe data generating distribution, are time-dependent with\nMarkov dynamics. HMMs are especially well suited for\nmodelling non-stationary data as they automatically al-\nlow for a representation of the time series in terms of a\ndiscrete number of states.\nIn this work, we therefore resolve the above limitations\nby combining Nonlinear ICA with a HMM. This idea\nhas been proposed earlier for linear ICA (Penny et al.,\n2000; Zhou and Zhang, 2008) but their identiﬁability and\nestimation results do not directly extend to the nonlinear\ncase. In our model, we achieve this by having the latent\nstate act in place of the conditioning auxiliary variable in\nthe framework of Hyv¨arinen et al. (2019). Importantly,\nwe are able to prove that Hidden Markov Nonlinear ICAs\nare identiﬁable. Attaining identiﬁability has been a major\nresearch focus for both Nonlinear ICA (Hyv¨arinen and\nMorioka, 2017; Hyv¨arinen et al., 2019) and HMMs (All-\nman et al., 2009; Gassiat et al., 2016), and therefore much\nof our paper is devoted to combining these two research\nstrands. To the best of our knowledge, this is the ﬁrst\nfully unsupervised non-linear ICA, in the sense that the\nmodel’s identiﬁability comes from an unobserved condi-\ntioning variable which is inferred from the time series as\na part of learning. We further show how the structure of\nthe model allows us to use the Expectation-Maximization\n(EM) algorithm for parameter estimation. In practice\nthe Hidden Markov Nonlinear ICA is endowed with rich\nrepresentation learning capabilities that allow it to simul-\ntaneously extract independent components and to learn\nthe dynamics of the latent state that drives non-stationarity\ndata, as illustrated by our simulations.\n2\nBACKGROUND\nWe start by giving an overview of the problem of uniden-\ntiﬁability in both nonlinear ICA and HMMs, and recently\nproposed solutions. For both types of models, identiﬁ-\nability arises as a consequence of appropriate temporal\nstructures which suggests a natural synthesis between the\ntwo.\n2.1\nNONLINEAR ICA AND IDENTIFIABILITY\nConsider a parametric model of observed data x with\nmarginal likelihood pθ(x). This model is identiﬁable if it\nfulﬁls below:\npθ(x) = pθ′(x) ⇒θ = θ′ : ∀(θ, θ′)\n(1)\nIn the context of a latent variable model, this is closely\nconnected to the idea of being able to recover the original\nlatent variables, as discussed by Khemakhem et al. (2020).\nAssume we observe N-dimensional data at discrete time-\nsteps, x(t) = (x(t)\n1 , . . . , x(t)\nN ). Simple nonlinear ICA\ncan be deﬁned as the task of estimating an unobserved\nN-dimensional independent component vector s(t) =\n(s(t)\n1 , . . . , s(t)\nN ) such that p(s(t)) = QN\ni=1 p(si), as well as\nthe inverse of a mixing function f, that has generated the\nobserved data:\nx(t) = f(s(t))\n(2)\nUnfortunately, without a temporal structure, that is if x(t)\nare i.i.d over the time-index, and if there are no constraints\non f, then this model is unidentiﬁable (Hyv¨arinen and Pa-\njunen, 1999). In fact, the authors show that there are\ninﬁnite potential nonlinear transformations and indepen-\ndent components that would satisfy the model, with no\ncriterion for choosing one of them over the others.\nIn order to make the model identiﬁable, constraints are\nthus needed. For time-series data, this comes naturally\nby placing restrictions on the temporal structure of the\nmodel. For linear ICA this approach has been shown\nto yield identiﬁable models (Belouchrani et al., 1997;\nTong et al., 1991), and extensions to the nonlinear case\nhave been also been proposed in earlier work (Harmel-\ning et al., 2003; Sprekeler et al., 2014). The ﬁrst fully\nrigorous proof of an identiﬁable nonlinear ICA model,\nalong with an estimation algorithm (Time-Contrastive\nLearning or TCL), was given by Hyv¨arinen and Morioka\n(2016). The constraint imposed in that work is that of a\nnon-stationary data generative process such that indepen-\ndent component vectors within different time-segments\nhave different distributional parameters. Speciﬁcally, the\nmodel assumes that each independent component has an\nexponential family distribution, where the time segment\nindex τ modulates the natural parameters (denoted as λ):\npτ(si) = qi(si)\nZ(λi) exp{⟨λi(τ), T(si)⟩}\n(3)\nwhere qi is the base measure and T are the sufﬁcient\nstatistics. TCL then assumes that the independent compo-\nnents in all the segments are transformed into observed\nvariables by some mixing function (2). The authors prove\nidentiﬁability up to a linear transformation of pointwise\nfunctions of the components:\nT(s(t)) = Ah(x(t); θ) + b\n(4)\nBy learning to contrast between the different segments,\nthe TCL algorithm learns the inverse of the mixing func-\ntion and the independent components.\nThis seminal work has inspired other frameworks for\nidentiﬁable nonlinear ICA estimation. Permutation Con-\ntrastive Learning (Hyv¨arinen and Morioka, 2017), for\ninstance, exploits temporal dependencies, rather than non-\nstationarity, to identify independent components. The\nunifying tenet of these identiﬁable nonlinear ICA algo-\nrithms is that independent components are conditionally\nindependent given some observed auxiliary variable. This\ngeneral idea was formalized in Hyv¨arinen et al. (2019), of\nwhich both the TCL (segment index as auxiliary variable)\nand the PCL (past data as auxiliary variable) are special\ncases.\nThese identiﬁable nonlinear ICA models provide a princi-\npled approach to ﬁnding meaningful data representations.\nThis is in contrast to the majority of recent deep gener-\native models used for representation learning, such as\nVAEs (Kingma and Welling, 2014) and GANs (Good-\nfellow et al., 2014), which are all malaised by uniden-\ntiﬁability. In fact, any generative latent variable model\nwith an unconditional prior is unidentiﬁable. This issue\nis portrayed in depth by Khemakhem et al. (2020) who\nresolve it by introducing identiﬁable VAE (iVAE). Like\nregular VAE, this model estimates a full generative model\npθ(x, s), but with a factorial conditional prior pθ(s|u).\nAs in Hyv¨arinen et al. (2019), u is some auxiliary vari-\nable, and iVAE provides a novel algorithm to estimate\nnonlinear independent components in the same identiﬁ-\nable framework. iVAE however suffers from the same\nproblems as TCL, as its auxiliary variable u has to be\nobserved.\n2.2\nHIDDEN MARKOV MODELS AND\nIDENTIFIABILITY\nIn order to deﬁne HMMs, let x(t) ∈Rn be an observed\nrandom variable from a time series with a discrete time\nindex t ∈{1, . . . , T}. In a standard hidden Markov\nmodel, distribution of the observations depends condi-\ntionally on a discrete latent state random variable c(t) as\nper p(x(t)|c(t)); we refer to this as the emission distribu-\ntion. The latent state c(t) undergoes ﬁrst-order Markov\nprocess governed by a C×C transition-probability matrix\nA. Ai,j is used to denote the probability of transitioning\nfrom state c(t) = i to c(t+1) = j, and π(c(1)) the starting-\nstate probabilities. The likelihood of a typical HMM is\nhence given by:\np(x(1), . . . , x(T ); A) =\nX\nc(1),...,c(T )\nπ(c(1))p(x(1)|c(1))\nT\nY\nt=2\nAc(t−1),c(t)p(x(t)|c(t))\n(5)\nHMMs can be viewed as mixture models where the latent\nstate is coupled across time by a Markov process. This\nobservation raises the question of identiﬁability since mix-\nture models with non-parametric emission distributions\nare generally unidentiﬁable, though many commonly used\nparametric forms are identiﬁable (Allman et al., 2009).\nRecently, however, Gassiat et al. (2016) have proven a\nmajor result that nonparametric HMMs are in general\nidentiﬁable under some mild assumptions. We will use\nthis result later and thus reproduce it here (notice that\ntheir nonparametric result subsumes parametric HMMs):\nTheorem 1. (Gassiat et al., 2016) Assume the number\nof latent states, C , is known. Use µ1, . . . , µC ∈RN to\ndenote nonparametric probability distributions of the C\nemission distributions. Also assume that the transition-\nmatrix A is full rank.\nThen the parameters A and\nM = (µ1, . . . , µC) are identiﬁable given the distri-\nbution, P(3)\nA,M, of at least 3 consecutive observations\nx(t−1), x(t), x(t+1), up to label swapping of the hidden\nstates, that is: if bA is a C × C transition matrix, if\nbπ(c) is a stationary distribution of bA with bπ(c) > 0\n∀c ∈{1, . . . , C}, and if ˆ\nM = (ˆµ1, . . . , ˆµC) are C prob-\nability distributions on RN that verify the equality of the\nHMM distribution functions P(3)\nb\nA, ˆ\nM = P(3)\nA,M, then there\nexists a permutation σ of the set {1, . . . , C} such that\nfor all k, l = 1, . . . , C we have ˆAk,l = Aσ(k),σ(l) and\nˆµk = µσ(k).\nMuch like for TCL, identiﬁability in nonparametric\nHMMs is a result of temporal structure, namely obser-\nvations across time are independent conditionally on the\nlatent state—this is in contrast to simple (i.i.d.) mixture\nmodels for which such general identiﬁability results are\nnot available. We show below that this temporal structure\nof the HMM’s, together with nonstationarity similar to\nTCL, combine to identify the resulting Hidden Markov\nNonlinear ICA model.\n3\nIDENTIFIABLE NONLINEAR ICA\nFOR NONSTATIONARY DATA\nIn this section, we propose a combination of a hidden\nMarkov model and nonlinear ICA. Speciﬁcally, we pro-\npose an HMM which has nonlinear ICA as its emis-\nsion model, and show how to estimate it by Expectation-\nMaximization.\n3.1\nMODEL DEFINITION\nTo incorporate nonlinear ICA into the standard HMM of\n(5) we deﬁne the emission distribution p(x(t)|c(t)) as a\ndeep latent variable model. First, the latent independent\ncomponent variables s(t) ∈RN are generated from a\nfactorial exponential family prior, given the hidden state\nc(t), as\np(s(t)|c(t); λc(t)) =\nN\nY\ni=1\np(s(t)\ni |c(t); λi,c(t))\n=\nN\nY\ni=1\nh(s(t)\ni )\nZ(λi,c(t)) exp{⟨λi,c(t), Ti(si)⟩}\n(6)\nwhere h(.) are the base measures, Z(λi,c(t)) the normal-\nizing constants, and Ti : R →RV the sufﬁcient statistics.\nSecond, the observed data is generated by a nonlinear\nmixing function as in Eq. (2).\nFor remainder of the paper we assume that the exponen-\ntial family is in minimal representation form so that the\nsufﬁcient statistics are linearly independent. The corre-\nsponding V -dimensional parameter vectors are denoted\nby λi,c(t). The subscripts indicate that the parameters of\nthe N different components are modulated directly, and\nindependently, by the HMM latent state. Indeed, it is\nprecisely this conditional dependence of the parameters\non the discrete latent state that seeps through our model\nand generates non-stationary observed data. Note that\nthe parameters themselves are time-homogeneous, that\nis they are constant over time; instead, the latent state\nevolves over time and determines which set of parameters\nis active a point in time. In other words, non-stationary\narises purely from the dynamics of the latent state c(t).\nThe full set of parameters for the independent components\ncan hence be captured by a C × NV matrix L (plus the\ntransition probabilities of the hidden states).\nThe nonlinear mixing function f in Eq. (2) is assumed\nto be bijective with inverse given by s(t) = g(x(t)). It\nfollows that in our model the conditional emission distri-\nbution for observed data is:\np(x(t)|c(t); f, λc(t)) =\n|Jg(x(t))|H(g(x(t)))\nZ(λc(t))\nexp{⟨λc(t), T(g(x(t)))⟩}\n(7)\nwhere |Jg(x(t))| is short-hand notation for the ab-\nsolute value of the determinant of the Jacobian of\nthe inverse (demixing) function, and H(g(x(t))) =\nQN\ni=1 h(gi(x(t))).\nWe have also simpliﬁed notation\nby stacking the vectors for different components T =\n(T1, . . . , TN)T and λc(t) = (λ1,c(t), . . . , λN,c(t))T .\nWe allow f to be any arbitrary but bijective function. In\npractice, it can be learned as a neural network. The model\ncan therefore be viewed as a deep generative model for\nnon-stationary data. Finally, using θ = {f, L} and Θ =\n{θ, A} our hidden Markov nonlinear ICA model’s data-\nlikelihood is given as:\np(x(1), . . . , x(T ); Θ) =\nX\nc(1),...,c(T )\n\u0010\nπ(c(1))p(x(1)|c(1), θc(1))×\nT\nY\nt=2\nAc(t−1),c(t)p(x(t)|c(t); θc(t))\n\u0011\n(8)\nwhere the emission distributions in Eq. (7) should be\nplugged in.\n3.2\nESTIMATION\nAssume we have a sequence of observed data D =\n{x(1), x(2), . . . , x(T )} generated by (8). In order to esti-\nmate the model parameters in practice we will choose the\nfactorial prior in (6) from a well-known family such that\nthe normalizing constant is tractable, such as a Gaussian\nlocation-scale family. Intractable normalizing constant\nwould make estimation very difﬁcult, even by approx-\nimate inference methods such as Variational Bayes or\nVAEs. However, notice that the choice of distribution for\nthe latent prior does not severely limit the type of data\nthat can be modelled since the non-linear mixing function\ncan be any arbitrary function.\nTractable exponential families also make it easy to esti-\nmate the model parameters by maximizing the likelihood\nin (8) by the EM algorithm. The ”free-energy” EM lower\nbound for our model is given by:\nL(q(c), Θ) :=\nEq(c)\nh\nlog p(c, x(1), . . . , x(T ); Θ)\ni\n−Eq(c) [log q(c)]\n(9)\nwhere c = (c(1), . . . , c(T )), such that the ﬁrst RHS\nterms is the complete-data likelihood under some dis-\ntribution q(c).\nIn the E-step one ﬁnds q(c⋆)\n:=\narg maxq(c) L(q(c), Θ) = p(c|x(1), . . . , x(T ); Θ) which\nis the standard result for HMMs and can be easily\ncomputed using the forward-backward (Baum-Welch)\nalgorithm.\nIn the M-step we aim to ﬁnd Θ⋆\n=\narg maxΘ L(q(c⋆), Θ), which reduces to maximizing:\n˜L(q(c), Θ) := Eq(c⋆)\nh\nlog p(c, x(1), . . . , x(T ); Θ)\ni\n=\nT\nX\nt=1\nEq(c(t)\n⋆)\nh\nlog p(x(t)|c(t); θc(t))\ni\n+\nT\nX\nt=2\nEq(c(t−1)\n⋆\n,c(t)\n⋆)\n\u0002\nlog Ac(t−1),c(t)\n\u0003\n(10)\nwhere we have left out the initial-state probability term as\nwe can assume a stationary process and infer them directly\nfrom A as its left eigenvector. The M-step updates for A\nare standard:\nA⋆\ni,j ←\nPT\nt=2 q(c(t−1)\n⋆\n= i, c(t)\n⋆\n= j)\nPT\nt=1 q(c(t)\n⋆)\n(11)\nM-step updates for the parameters L also follow from\nstandard EM results for exponential families:\n∇λk\nT\nX\nt=1\nEq(c(t)\n⋆)\nh\nlog p(x(t)|c(t); θc(t))\ni\n= ∇λk\nT\nX\nt=1\nEq(c(t)\n⋆)\nh\n⟨λk, T(g(x(t)))⟩−log Z(λk)\ni\n=\nT\nX\nt=1\nq(c(t)\n⋆\n= k)\n\u0014\nT(g(x(t))) −∇λkZ(λk)\nZ(λk)\n\u0015\n= 0\n⇒∇λkZ(λk)\nZ(λk)\n=\nPT\nt=1 q(c(t)\n⋆\n= k)T(g(x(t)))\nPT\nt=1 q(c(t)\n⋆\n= k)\n(12)\nwhere LHS can be rewritten as:\n1\nZ(λk)∇λk\nZ \u0010\n|Jg(x(t))|H(g(x(t)))\n× exp{⟨λc(t), T(g(x(t)))⟩}\n\u0011\n= Ep(x(t)|c(t);θc(t))\nh\nT(g(x(t)))\ni\n(13)\nThus the M-step updates for λ⋆\nk are the ones that solve:\nEp(x(t)|k;λ⋆\nk,f)\nh\nT(g(x(t)))\ni\n=\nPT\nt=1 q(c(t)\n⋆\n= k)T(g(x(t)))\nPT\nt=1 q(c(t)\n⋆\n= k)\n(14)\nIn practice, (14) has closed-form updates for many usual\nexponential family members. As an example, if we were\nto use a Gaussian distribution, the updates for mean and\nvariance vectors would be:\nµ⋆\nk ←\nPT\nt=1 q(c(t)\n⋆\n= k)g(x(t))\nPT\nt=1 q(c(t)\n⋆\n= k)\nσ2⋆\nk ←diag\n PT\nt=1 q(c(t)\n⋆\n= k)y⋆\nky⋆,T\nk\nPT\nt=1 q(c(t)\n⋆\n= k)\n!\n(15)\nwhere y⋆\nk = g(x(t)) −µ⋆\nk\nNext, the demixing function is estimated by parameter-\nizing it as a deep neural network but for notational sim-\nplicity we will not write these parameters explicitly and\ninstead subsume them in g. Since an exact M-step is not\npossible, a gradient ascent step on the lower bound is\ntaken instead, where the gradient is given by:\n∇g ˜L(q(c), Θ) = ∇g\nT\nX\nt=1\nEq(c(t)\n⋆)\nh\nlog p(x(t)|c(t); θc(t))\ni\n= ∇g\nT\nX\nt=1\nlog |Jg|\n+ ∇g\nT\nX\nt=1\nEq(c(t)\n⋆) [log H(g) + ⟨λc(t), T(g)⟩]\n(16)\nwhere we have used g = g(x(t)) for brevity. The param-\neters are then updated as:\ngnew ←gold + η∇g ˜L(q(c), Θ)\n(17)\nSee Appendix A for a discussion on the convergence of\nour algorithm.\nThe gradient term with respect to the determinant of the\nJacobian log |Jg| deserves special attention. It is widely\nconsidered difﬁcult to compute, and therefore, normaliz-\ning ﬂows models are often used in literature in order to\nmake the Jacobians more tractable. The problem with this\napproach is that, to our best knowledge, none of such ﬂow\nmodels has universal function approximation capabilities\n(despite some being universal distribution approximators).\nThis would restrict the possible set of nonlinear mixing\nfunctions that can be estimated, and is thus not practical\nfor our purposes. Fortunately modern autograd packages\nsuch as JAX make it possible to calculate gradients of\nthe log determinant Jacobian efﬁciently up to moderate\ndimensions (see Appendix B) – this is the approach we\ntake. Very recent, promising, alternative for computing\nthe log-determinant is the relative gradient (Gresele et al.,\n2020) which could easily be implemented in our frame-\nwork. Finally, notice that the second term (16) is easy to\nevaluate since the expectation is just a discrete sum over\nthe posteriors that we get from the E-step.\n3.3\nCOMMENT ON ESTIMATION FOR LONG\nTIME SEQUENCES\nThe above estimation method may be impractical for very\nlong time sequences since the forward-backward algo-\nrithm has computational complexity of O(TC2). In such\nsituations we can adapt the subchain sampling approach\nof Foti et al. (2014). This involves splitting up the full\ndataset into shorter time sequences and then forming mini-\nbatches over time. The resulting gradient updates would\nbe biased and therefore a scaling term will be applied to\nthem. The forward-backward algorithm applied to the\nsubchains is also only approximate due to loss of infor-\nmation at the ends of the chains but the authors describe\na technique to buffer the chains with extra observations\nto reduce this effect.\n3.4\nCOMMENT ON DIMENSION REDUCTION\nAn important problem in applying our method on real\ndata is dimension reduction. While in the theory above,\nwe assumed that the number of independent components\nis equal to the number of observed variables, in many\npractical cases, we would like to have a smaller number\nof components than observed variables. We propose here\ntwo solutions for this problem.\nThe ﬁrst solution, which is widely used in the linear ICA\ncase, is to ﬁrst reduce the dimension of the data by PCA,\nand then do ICA in that reduced space with the same\ndimensions of components and observed variables. In the\nnonlinear case, a number of nonlinear PCA methods, also\ncalled manifold learning methods, has been proposed and\ncould be used for such a two-stage method. In particular,\ndimension reduction is achieved by even the very sim-\nplest autoencoders; recent work has developed the theory\nfurther in various directions (Maaten and Hinton, 2008;\nVincent et al., 2010). This approach has the advantage\nof reducing the noise in the data, which is a well-known\nproperty of PCA, and allows us to separate the problem\nof dimension reduction from the problem of developing\nICA algorithms. A possible drawback is that such dimen-\nsion reduction may not be optimal from the viewpoint of\nestimating independent components.\nThe second solution is to build an explicit noise model\ninto the nonlinear ICA model, following Khemakhem\net al. (2020). Denote by n a random vector of Gaussian\nnoise which is white both temporally and spatially and\nof variance σ2. Instead of the Eq. (2), we would deﬁne a\nmixing model as\nx(t) = f(s(t)) + n(t)\n(18)\nwhere the model of the components s(t) is unchanged.\nWe could then combine the variational estimation method\npresented by Khemakhem et al. (2020) with the HMM\ninference procedure presented here. However, we leave\nthe details for future work.\n4\nIDENTIFIABILITY THEORY\nIn this section we provide identiﬁability theory for the\nmodel discussed in the previous section. As was dis-\ncussed above, many deep latent variable models are non-\nidentiﬁable. In other words, an estimation method such\nas the EM proposed above might not have a unique so-\nlution, or even a small number of solutions which are\nindistinguishable for any practical purposes.\nFortunately, we are able to combine previous nonlinear\nICA theory with the identiﬁability of Hidden Markov\nModels to prove the identiﬁability of our combined model.\nAlbeit our model being different from (Hyv¨arinen and\nMorioka, 2017), (Hyv¨arinen et al., 2019) and (Khe-\nmakhem et al., 2020), the identiﬁability we reach is very\nsimilar. We also show that in the case of Gaussian inde-\npendent components we can get exact identiﬁability up to\nlinear transformation of the components.\n4.1\nDEFINITIONS\nIn order to illustrate the relationship of our model’s iden-\ntiﬁability to earlier works in the area, we introduce the\nfollowing deﬁnitions from Khemakhem et al. (2020)\nDeﬁnition 1. Let ∼be the equivalence relation on Θ. (8)\nis said to be identiﬁable up to ∼if\np(x(1), . . . , x(T ); Θ) = p(x(1), . . . , x(T ); ˆΘ) ⇒Θ ∼ˆΘ\n(19)\nDeﬁnition 2. Let ∼be the binary relation on Θ deﬁned\nby:\n(f, λ) ∼(ˆf, ˆλ) ↔\n∃W, b | T(g(x(t))) = WT(ˆg(x(t))) + b\n(20)\nwhere W is an NV × NV matrix and b is an NV × 1\nvector.\nIf W is invertible, the above relation is denote by ∼W ,\nand if W is a block permutation matrix, it is denoted by\n∼P. In block permutation, each block linearly transforms\nTi(gi(xi)) into Tj(ˆgj(xi)) with each j corresponding\nto one, and only one, i.\n4.2\nGENERAL RESULT\nNow we present our most general Theorem on identiﬁabil-\nity. It will be followed by stronger results in the Gaussian\ncase below.\nTheorem 2. Assume observed data is generated by a\nHidden Markov Nonlinear ICA according to (5) - (8).\nAlso, assume:\n(i) The time-homogeneous transition matrix A has full\nrank and induces an irreducible 1 Markov chain with\na unique stationary state distribution\n(ii) The number of latent states, C, is known and C ≥\nNV + 1\n1all states can be reached from every state\n(iii) There exists an NV square matrix of the different\nstates’ parameters with respect to a pivot state\neL =\n\n\n\n(λc=1 −λc=0)T\n...\n(λc=NV −λc=0)T\n\n\n\n(21)\nwhich is invertible.\n(iv) The emission distributions for the different latent\nstates p(x(t)|1; θ1), . . . , p(x(t)|C; θC) are linearly\nindependent functions of x(t)\n(v) The non-linear mixing function f is bijective\nThen the model parameters (f, λ) are ∼W identiﬁable.\nProof. Suppose we have\np(x(1), . . . , x(T ); Θ) = p(x(1), . . . , x(T ); ˆΘ)\n(22)\nUsing assumptions (i)-(iv), we can invoke Theorem 1 and\napply it to our model to get:\nˆAk,l = Aσ(k),σ(l)\n(23)\np(x|k; ˆθk) = p(x|σ(k); θσ(k))\n(24)\nwhere superscript t is dropped for convenience. For no-\ntational simplicity, and without loss of generality, we\nassume the components are ordered such that k = σ(k).\nSubstituting in (7) we have:\n|Jˆg(x)|H(ˆg(x))\nZ(ˆλk)\nexp{⟨ˆλk, T(ˆg(x))⟩}\n= |Jg(x)|H(g(x))\nZ(λk) exp{⟨λk, T(g(x))⟩}\n(25)\nfor some latent state k. Recall from assumption (iii) that\nC ≥NV + 1. We can thus take C + 1 states and assign\none of them, say c = 0 as a pivot states. Taking logs of\n(25) for all the other states with respect to the pivot state\ngives C equations of below form:\n⟨(λk −λ1), T(gi(x))⟩+ log Z(λ1) −log Z(λk)\n= ⟨(ˆλk −ˆλ1), T(ˆgi(x))⟩+ log Z(ˆλ1) −log Z(ˆλc)\n(26)\nCollecting all the C such equations, we can stack them\ninto a linear system :\neLT(g(x)) = beLT(ˆg(x)) + β\n(27)\nwhere eL is the invertible square matrix deﬁned in assump-\ntion (iii), and the elements of beL are deﬁned similarly, but\nno assumption about its invertibility is made. The con-\nstants that result from the sums of the log-normalizers are\nstacked to form C × 1 vector β. Multiplying both sides\nby eL−1 results in our desired form:\nT(g(x)) = eL−1beLT(ˆg(x)) + eL−1β\nT(s) = WT(ˆg(x)) + b\n(28)\nRecall that we deﬁned the exponential families to be in\nminimal representation in Section 3.1. It follows that we\ncan ﬁnd an arbitrary number of points such that the V\nvectors formed by the sufﬁcient statistic functions of each\nindependent component (Ti,1(si), . . . , Ti,V (si)), are lin-\nearly independent. This can be done separately for each si.\nAdditionally, as si and sj can be changed independently,\nwe can ﬁnd for i ̸= j then Tl(si) and Tm(sj) are linearly\nindependent for all l, m ∈(1, . . . , V ). Therefore, all ele-\nments of the vector T(s) are linearly independent which\nimplies that the square matrix W in (28) is invertible.\n4.2.1\nComments on the assumptions of Theorem 1\nThe assumptions (i), (ii) are standard HMM assumptions.\nThe assumption of a full rank transition matrix is non-\nstandard but crucial here. Intuitively speaking, it allows\nthe latent states to be distinguished from each other, while\nthe irreducibility assumptions ensures that there is a sin-\ngle unique stationary state distribution.2 Notice that these\nassumptions necessarily hold, for example, when the tran-\nsition matrix is close to identity, as in a case where the\nstates are strongly persistent.\nThe assumption that the real number of latent components\nis known, is valid in certain applications, and if not it\ncould be estimated for instance be increasing the number\nof latent states between each estimation and then detect-\ning the point at which increases in likelihood become\nmarginal (the elbow method). Assumption (iii) is valid in\npractice as long as the parameters are generated randomly\n- in that case it almost surely holds as singular solutions\nwill lie in a submanifold of lower dimension. The validity\nof assumption is less obvious (iv), however, we will below\nprove that it holds, for instance, in the case of Gaussian\nindependent components.\n4.3\nIDENTIFIABILITY WITH GAUSSIAN\nINDEPENDENT COMPONENTS\nIn this section, we ﬁrst provide two lemmas which we\nuse to prove the claim, already alluded to above, that\nassumption (iv) of Theorem 2 is satisﬁed for Gaussian\n2technically one aperiodic state is also required. An ape-\nriodic state is one which can be returned to after an irregular\nnumber of steps\ncomponents. Then, we prove that in this case a stronger\nform of identiﬁability can be reached as a special case of\nabove results, namely that we get exact identiﬁcation of\ncomponents up to linear transformation. Together these\nresults make a strong case for using Gaussian latent com-\nponents in practical applications.\nWe begin by stating two Lemmas (proofs in Appendix C):\nLemma 1. Assumption (iv) of Theorem 1 requires the\nC emission distributions deﬁned by (7) to be linearly\nindependent. A sufﬁcient, and necessary, condition is that\nthe C conditional source distributions deﬁned by (6) are\nlinearly independent.\nLemma 2. Assume K probability density functions of N\nrandom variables p1(z1, . . . , zN), . . . , pK(z1, . . . , zN),\nand\nthat\neach\nfactorizes\nacross\nthe\nvari-\nables:\npk(z1, . . . , zN)\n=\nQN\ni=1 p(i)\nk (zi) ∀k\n∈\n{1, . . . , K}.\nIf the K factorial density functions\np(i)\n1 (zi), . . . , p(i)\nK (zi)\nare\nlinearly\nindependent\nfor\nsome i ∈{1, . . . , N}, then the K joint-density func-\ntions p1(z1, . . . , zN), . . . , pK(z1, . . . , zN) are linearly\nindependent.\nBased on these Lemmas, we can prove the following\nTheorem (proof in Appendix C):\nTheorem 3. Assume that distributions of the independent\ncomponents conditional on the latent state, as deﬁned by\n(6), are Gaussian parameterised by mean and variance.\nAssume also that the means of the C density functions are\nall different. Then the emission distributions, deﬁned by\n(7), are linearly independent, thus satisfying assumption\n(iv) in Theorem 2.\nFinally, we have the following Theorem which proves a\nstronger form of identiﬁability—essentially recovering\nthe components with minimum indeterminacy—of our\nHidden Markov Nonlinear ICA model in the Gaussian\ncase (proof in Appendix C)\nTheorem 4. Assume that the latent independent compo-\nnents have a conditionally Gaussian distributions, and\nassume hypotheses (i), (ii),(iii) and (v) of Theorem 2 hold,\nas well as the assumptions of Theorem 3. Additionally as-\nsume that the mixing function f has all of it second-order\ncross derivatives, then the components in our Hidden\nMarkov Nonlinear ICA model are exactly identiﬁed up to\nlinear transformation.\nNotice that this proof and the identiﬁability result is simi-\nlar to that in (Sorrenson et al., 2020), although our models\nare entirely different. These authors also prove a general\nversion for other distributions with different sufﬁcient\nstatistics.\n5\nEXPERIMENTS\nIn this section we present results from our simulations\non artiﬁcial non-stationary data. Code, written in JAX, is\navailable at github.com/HHalva/hmnlica.\nDataset: We generated a synthetic dataset from the model\ndeﬁned in Section 3.1. More speciﬁcally, the independent\ncomponents are created from non-stationary Gaussian\nemission distributions of an HMM with C discrete states –\nthe latent state determines the means and the variances of\nthe independent components at each time point. The tran-\nsition matrix was deﬁned so that at each time-step there\nwas a 99% probability that the state didn’t change and a\n1% probability the latent state switches to another state.3\nIf it switches to another state, it will always go to the next\none ‘in line’, where we deﬁne a circular ordering for the\nstates. That is, we deﬁned a circular repeating path for the\nlatent state where transitions could only happen to two\nstates such that the transition matrix is close to identity\n(Figure 1 illustrates this). These settings were chosen to\nensure the HMM assumptions of Theorem 2 hold, as well\nas to reﬂect a situation where a relatively small number\nof states repeats over time with some interesting, non-\nrandom, temporal dynamics, including persistence to stay\nin the same state. The mean and variance parameters were\nchosen at random for each latent state before data genera-\ntion so that assumption (iii) of Theorem 2 holds. Similarly\nto Hyv¨arinen et al. (2019), the mixing function (2) was\nsimulated with a randomly initialized, invertible4 multi-\nlayer perceptron (MLP) – this produced the observed data\nfor our experiments. The sequences that were created\nare 100,000 time steps long. The number of latent states\nwas set such that C = 2N + 1, which ensures that the\nassumptions (ii) and (iii) were fulﬁlled.\nModel estimation: We estimate the (inverse) mixing\nfunction and distribution parameters of our Hidden\nMarkov nonlinear-ICA model using the EM algorithm\ndescribed in Section 3.2. Mean and variance parameter\nestimates for the independent components are initialized\nrandomly at the start of the EM algorithm. The inverse\nmixing function is parameterized with an MLP where\nthe number hidden layers is set to match the number of\ndata generating mixing layers. The gradient M-step are\ntaken with the Adam optimizer (Kingma and Ba, 2017).\nRandom restarts were used to avoid inferior local max-\nima. Further, we found that a stochastic version of our\nalgorithm (see Section 3.3) converged faster – thus, the\nexperiments here have been run with 100 time-step long\nsub-sequences in minibatches of 64.\n3for context, the probability of staying in the same state for\nover 100 time steps with these numbers is around 37%\n4invertibility achieved by having all layers N units wide and\nutilizing leaky ReLUs\nFigure 1: An example of the independent components’\ndistributions from a HMM where the number of com-\nponents N = 2 and the number of latent states C = 5.\nThe clusters are ordered to illustrated the dynamics of the\nhidden Markov model, in particular its circular property.\nThe transition probabilities are the same throughout the\ndata.\nResults – independent component recovery: After es-\ntimating the model parameters and independent compo-\nnents, a linear sum assignment problem is solved to op-\ntimally match each of the estimated components to one\nof the real ones. This is necessary as the ordering of the\ncomponents is arbitrary. Mean absolute correlation co-\nefﬁcients over the resulting pairs of true and estimated\ncomponents are then used to measure how well original\ncomponents were recovered. This is the methodology\ntaken in previous nonlinear ICA works (Hyv¨arinen and\nMorioka, 2016).\nFigure 2 shows the mean correlation between the esti-\nmated components for our model in comparison to TCL,\nwhich is the only other nonlinear ICA model for non-\nstationary data. For TCL, the data was split into 500\ntime-step long segments; 500 steps provided best perfor-\nmance relative to other computationally feasible options\n(100, 250, 750, 1000). We can see that our model outper-\nforms TCL for all levels of nonlinearity. This validates\nour theoretical arguments that the TCL framework strug-\ngles with non-stationary data in which latent states (often\na relatively small number) repeat over time since the seg-\nments it has access to don’t correspond well with the true\ndata generating process.\nResults – temporal dynamics Unlike previous models,\nHidden Markov Nonlinear ICA is able to perform unsuper-\nvised clustering of latent states and to take into account the\nlearned temporal dynamics in doing so. To estimate this\nability, we ran the well-know Viterbi algorithm (Viterbi,\n1967) which ﬁnds the most likely path of latent states\nbased on our estimated model. The results show that\non average for N = 5 and C = 11 our model reaches\nnear perfect classiﬁcation accuracy in the linear ICA case,\nmean accuracy of around 80% for L = 2, and 68% for\nL = 4, thus clearly outperforming random chance level\n(ﬁgure in Appendix E).\n1\n2\n4\nNumber of mixing layers\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nMean Correlation Coefficient\nIndependent component estimation (N=5)\nhm-nlica\nTCL\nFigure 2: Performance of our Hidden Markov nonlinear\nICA vs. TCL in recovering the true sources for N=5\nfor our synthetic dataset. The amount of nonlinearity is\ncontrolled by number of hidden layers in the mixing MLP,\nso that L ∈[1, 2, 4].\n6\nCONCLUSION\nWe proposed a framework nonlinear ICA based on a Hid-\nden Markov Model of the temporal dynamics. This im-\nproves on existing nonlinear ICA methods in several ways.\nFirst, it removes the need for any arbitrary segmentation\nof the data as in TCL, which is likely to improve the es-\ntimation of the demixing function. Second, it leverages\nthe fact that the nonstationary structure is often repeating\nwith a limited number of hidden states, which not only\nreduces the computation by limiting the number classes,\nbut again is likely to improve estimation of the demix-\ning function. Third, our method estimates the underlying\nlatent temporal dynamics, which are often interesting in\ntheir own right. We believe this in an important advance\nin order to apply nonlinear ICA methods on real data.\nAcknowledgements\nThe authors would like to thank Elisabeth Gassiat, Ilyes\nKhemakhem and Ricardo Pio Monti for helpful discus-\nsion. I.K.’s help with the experiments is also much appre-\nciated. A.H. was supported by a Fellowship from CIFAR,\nand from the DATAIA convergence institute as part of\nthe “Programme d’Investissement d’Avenir” (ANR-17-\nCONV-0003) operated by Inria.\nReferences\nAllman, E. S., Matias, C., and Rhodes, J. A. (2009). Iden-\ntiﬁability of parameters in latent structure models with\nmany observed variables.\nThe Annals of Statistics,\n37(6A):3099–3132. arXiv: 0809.5032.\nBelouchrani, A., Abed-Meraim, K., Cardoso, J.-F., and\nMoulines, E. (1997). A blind source separation tech-\nnique using second-order statistics. IEEE Transactions\non Signal Processing, 45(2):434–444.\nDempster, A. P., Laird, N. M., and Rubin, D. B. (1977).\nMaximum likelihood from incomplete data via the em\nalgorithm. Journal of the Royal Statistical Society.\nSeries B (Methodological), 39(1):1–38.\nFoti, N., Xu, J., Laird, D., and Fox, E. (2014). Stochastic\nvariational inference for hidden Markov models. In\nGhahramani, Z., Welling, M., Cortes, C., Lawrence,\nN. D., and Weinberger, K. Q., editors, Advances in\nNeural Information Processing Systems 27, pages 3599–\n3607. Curran Associates, Inc.\nGassiat, E., Cleynen, A., and Robin, S. (2016). Inference\nin ﬁnite state space non parametric Hidden Markov\nModels and applications. Statistics and Computing,\n26(1-2):61–71.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. (2014). Generative adversarial nets. In Advances\nin neural information processing systems, pages 2672–\n2680.\nGresele, L., Fissore, G., Javaloy, A., Sch¨olkopf, B., and\nHyv¨arinen, A. (2020). Relative gradient optimization\nof the jacobian term in unsupervised deep learning.\nSubmitted.\nHarmeling, S., Ziehe, A., Kawanabe, M., and M¨uller,\nK.-R. (2003). Kernel-Based Nonlinear Blind Source\nSeparation. Neural Computation, 15(5):1089–1124.\nHyv¨arinen, A. and Morioka, H. (2016). Unsupervised\nFeature Extraction by Time-Contrastive Learning and\nNonlinear ICA. arXiv:1605.06336 [cs, stat]. arXiv:\n1605.06336.\nHyv¨arinen, A. and Morioka, H. (2017). Nonlinear ICA of\nTemporally Dependent Stationary Sources. Proceed-\nings of the 20 th International Con- ference on Artiﬁcial\nIntelligence and Statistics (AISTATS), page 14.\nHyv¨arinen, A. and Pajunen, P. (1999). Nonlinear indepen-\ndent component analysis: Existence and uniqueness\nresults. Neural Networks, 12(3):429–439.\nHyv¨arinen, A., Sasaki, H., and Turner, R. E. (2019). Non-\nlinear ICA Using Auxiliary Variables and Generalized\nContrastive Learning.\narXiv:1805.08651 [cs, stat].\narXiv: 1805.08651.\nKhemakhem,\nI.,\nKingma,\nD. P.,\nMonti,\nR. P.,\nand Hyv¨arinen, A. (2020).\nVariational Autoen-\ncoders and Nonlinear ICA: A Unifying Framework.\narXiv:1907.04809 [cs, stat]. arXiv: 1907.04809.\nKingma, D. P. and Ba, J. (2017). Adam: A Method for\nStochastic Optimization. arXiv:1412.6980 [cs]. arXiv:\n1412.6980.\nKingma, D. P. and Welling, M. (2014). Auto-Encoding\nVariational Bayes. arXiv:1312.6114 [cs, stat]. arXiv:\n1312.6114.\nMaaten, L. v. d. and Hinton, G. (2008). Visualizing data\nusing t-SNE. Journal of machine learning research,\n9(Nov):2579–2605.\nPenny, W., R.M., E., and S.J., R. (2000). hidden markov\nindependent component analysis. In Advances in Inde-\npendent Component Analysis.\nSorrenson, P., Rother, C., and K¨othe, U. (2020).\nDisentanglement\nby\nNonlinear\nICA\nwith\nGen-\neral\nIncompressible-ﬂow\nNetworks\n(GIN).\narXiv:2001.04872 [cs, stat]. arXiv: 2001.04872.\nSprekeler, H., Zito, T., and Wiskott, L. (2014). An ex-\ntension of slow feature analysis for nonlinear blind\nsource separation. J.\\ of Machine Learning Research,\n15(1):921–947.\nTong, L., Liu, R.-w., Soon, V., and Huang, Y.-F. (1991).\nIndeterminacy and identiﬁability of blind identiﬁcation.\nIEEE Transactions on Circuits and Systems, 38(5):499–\n509.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and\nManzagol, P.-a. (2010).\nstacked denoising autoen-\ncoders: learning useful representations in a deep net-\nwork with a local denoising criterion. journal of ma-\nchine learning research, 11(dec):3371–3408.\nViterbi, A. J. (1967). Error bounds for convolutional\ncodes and an asymptotically optimum decoding al-\ngorithm. IEEE Transactions on Information Theory,\n13:260–267.\nZhou, J. and Zhang, X. (2008). An ica mixture hidden\nmarkov model for video content analysis. IEEE Trans-\nactions on Circuits and Systems for Video Technology,\n18(11):1576–1586.\nAppendix for\nHidden Markov Nonlinear ICA for Unsupervised Learning from Nonstationary Time\nSeries (published at UAI 2020)\nA\nNote on the convergence of our estimation algorithm\nStandard theory (Dempster et al., 1977) shows that each EM iteration increases the likelihood, unless parameters are\nalready at a zero-gradient point. Further, maxima of free-energy and likelihood coincide. This also holds under the\ngradient M-steps in our algorithm (with classical assumption of sufﬁciently small step size). Under suitable regularity\nconditions, theoretical limit of inﬁnite data and universal approximation of the nonlinear transformation, combined with\nour identiﬁability proof, MLE guarantees convergence to correct parameters up to the equivalence class identiﬁed in our\nTheorem 2. In practice, however, these assumptions may not be satisﬁed — for instance, parameters may approach a\nboundary point and likelihood tend to inﬁnity. Random restarts and regularisation are common strategies to avoid these\nproblems.\nB\nNote on the compute time of the gradients of the logdet Jacobian\nWe estimate the non-linear mixing function in our model using a multi-layer perceptron without any restrictions on it.\nAs a consequence of the change of variable formula for probability densities, we have to calculate the gradient of the\nlog-determinant of the Jacobian as part of our parameter updates. JAX, a new machine learning package that utilizes\nautograd, has the ability to calculate the Jacobian in just a single forward pass thus making the computations efﬁcient\nfor typical data dimensions. For our model, we can see that the compute time required for the log-determinant of the\nJacobian starts to dominate as we approach 100 dimensions and above.\n20\n30\n40\n50\n60\n70\n80\n90\n100\nN=data dimension\n0\n50\n100\n150\n200\nComputation time (sec)\nCompute time for training step\nvariable\nwith logdetJ\nexc. logdetJ\nFigure 3: Average computation time over 100 epochs for computing the gradients of the function estimator in our\nmodel, including and excluding the log-determinant Jacobian term, in JAX. The function estimator is a four layer deep\nneural network where the width of the hidden units is always equal to N.\nC\nProofs\nProof for Lemma 1\nProof. Assume that we have linear independence of the C conditional source distributions as deﬁned by (6). Then we\nhave that:\na1pS(s|1; λ1) + · · · + aCpS(s|C; λC) ≡0 ⇒a = 0\n(29)\nAbove holds if we multiply it through by the Jacobian determinant of the mixing function as we have assumed bijectivity,\nthat is:\na1|Jg(x)|pS(s|1; λ1) + . . .\n+ aC|Jg(x)|pS(s|C; λC) ≡0 ⇒a = 0\n(30)\nwhich is equivalent to:\na1|Jg(x)|pS(g(x)|1; λ1) + . . .\n+ aC|Jg(x)|pS(g(x)|C; λC) ≡0 ⇒a = 0\n(31)\nAnd therefore by (7) we have:\na1pX(x|1; f, λ1) + . . .\n+ aCpX(x|C; f, λC) ≡0 ⇒a = 0\n(32)\nSo the emission distributions are linearly independent if the densities for the independence components are linearly\nindependent across the C different latent states. Necessity follows easily by the reverse of above argumentation.\nProof for Lemma 2\nProof. Assume linear independence of the K joint-density functions for some subset of variables zi, . . . , zi+n, where\ni ∈{1, . . . , N} and 0 ≤n ≤N −i −1, that is:\nw1p1(zi, . . . , zi+n) + · · · + wKpK(zi, . . . , zi+n)\n= w1\ni+n\nY\nj=i\np(j)\n1 (zj) + · · · + wK\ni+n\nY\nj=i\np(j)\nK (zj) ≡0\n⇒w = 0\n(33)\nNow the linear independence for joint of pk(zi, . . . , zi+n, zi+n+1) requires:\nw1p1(zi, . . . , zi+n, zi+n+1)+\n· · · + wKpK(zi, . . . , zi+n, zi+n+1) ≡0 ⇒w = 0\nUsing the factorial form of the joint, we can rewrite this as:\nw1p(i+n+1)\n1\n(zi+n+1)p1(zi, . . . , zi+n) + . . .\n+ wKp(i+n+1)\nK\n(zi+n+1)pK(zi, . . . , zi+n)\n≡0 ⇒w = 0\n(34)\nIf this didn’t hold we could deﬁne K constants vk := wkp(i+n+1)\nk\n(zi+n+1) such that:\nv1p1(zi, . . . , zi+n) + · · · + vKpK(zi, . . . , zi+n) ≡0\n(35)\nwhere the constants are not all zero which would contradict our original assumption. Thus it is sufﬁcient to prove linear\nindependence of p(i)\n1 (zi), . . . , p(i)\nK (zi), say for i = 1, without loss of generality, and then apply the above induction step\nto guarantee linear independence of the K joint-density functions p1(z1, . . . , zN), . . . , pK(z1, . . . , zN) .\nProof for Theorem 3\nProof. By Lemma 1 it is sufﬁcient to prove the linear independence of the C different conditional independent\ncomponent density functions, rather than emission densities. And by Lemma 2 it sufﬁces to prove this only for any one\nof the N different independent components. In exponential family form, the density is written as (see Appendix D):\np(si|c) =\n1\n√\n2π\nexp{ηi,c,1si −ηi,c,2s2\ni }\nZi,c\n(36)\nwhere ηi,c,2 > 0 ∀c ∈{1, . . . , C}. We drop subscript i for convenience. Consider:\nw1\n1\n√\n2π\nexp{η1,1s −η1,2s2}\nZ1\n+ . . .\n+ wC\n1\n√\n2π\nexp{ηC,1s −ηC,2s2}\nZC\n= 0\n(37)\nFirst assume, all the ηc are distinct. Also we can assume, without loss of generality, that the C latent states are ordered\nsuch that η1,2 < η2,2 < · · · < ηC,2. We can divide all the terms with the ﬁrst density to give:\nw1 + w2\nZ1\nZ2\nexp{(η1,1 −η2,1)s + (η1,2 −η2,2)s2} + . . .\n+ wC\nZ1\nZC\nexp{(η1,1 −ηC,1)s + (η1,2 −ηC,2)s2} = 0\n(38)\ntaking lims→+∞of above gives w1 = 0. Repeatedly performing this process for remaining terms eventually gives\nw = 0. Consider now the opposite case in which all the ηc are equal. Then we have:\nw1\n1\n√\n2π\nexp{η1,1}\nZ1\n+ · · · + wC\n1\n√\n2π\nexp{ηC,1}\nZC\n= 0\n(39)\nIf we re-order the terms such that η1,1 is the largest (recall we assumed that the means are different). We can again\ndivide everything by this term, take lims→+∞, and establish w1 = 0 and repeat the process to get w = 0. In the the\nﬁnal case where more than one component has the highest variance, but rest are unequal, (only the equality of the\nlargest variances of the remaining terms matters), we can ﬁrst perform the variance division followed by division by the\nlargest mean, repeatedly until w = 0.\nProof for Theorem 4\nProof. By Theorem 3, the above assumptions sufﬁce for Theorem 2 to hold. Next, note that the sufﬁcient statistics\nof a Gaussian distribution are twice differentiable. This, combined with the assumption about the existence of f’s\ncross-derivatives fulﬁls the conditions of Theorem 2 of Khemakhem et al. (2020) and thus our model’s parameters are\n∼P identiﬁable (as per Deﬁnition 2). We therefore have:\n\u0012si\ns2\ni\n\u0013\n= Wj\n\u0012 gj(x)\ngj(x)2\n\u0013\n+ bi\n(40)\nfor some i, j. Hence, we have\n(w11gj(x) + w12gj(x)2 + b11)2\n= w21gj(x) + w22gj(x)2 + b21\nw2\n12z4 + 2w11w12z3 + (w2\n11 −w22)z2 −w21z + b = 0\n(41)\nAbove has to hold for all values of z = gj(x). The trivial solution of all wij = 0 is impossible as W would not be\ninvertible. Therefore, it must be that w12 = w21 = 0 and w2\n11 = w22. Thus we have exact identiﬁcation (up to linear\ntransformation) si = wijgj(x) + bi for some constants wij, bi.\nD\nModel with Gaussian independent components\np(si|c) =\n1\nq\n2πσ2\ni,c\nexp{−\n1\n2σ2\ni,c\n(si −µi,c)2}\n(42)\n=\n1\nq\n2πσ2\ni,c\nexp{−\n1\n2σ2\ni,c\n(s2\ni −2siµi,c + µ2\ni,c)}\n(43)\n=\n1\nq\n2πσ2\ni,c\nexp{si\nµi,c\nσ2\ni,c\n−s2\ni\n1\n2σ2\ni,c\n−µ2\ni,c\n2σ2\ni,c\n}\n(44)\n=Z−1\ni,c exp{si\nµi,c\nσ2\ni,c\n−s2\ni\n1\n2σ2\ni,c\n}\n(45)\nTherefore by independence of components:\np(s|c) = exp{\nN\nX\ni=1\n(si\nµi,c\nσ2\ni,c\n−s2\ni\n1\n2σ2\ni,c\n)}\nN\nY\ni=1\nZ−1\ni,c\n(46)\n= exp{\nN\nX\ni=1\n(si\nµi,c\nσ2\ni,c\n−s2\ni\n1\n2σ2\ni,c\n)}Z−1\nc\n(47)\nAnd change of variable gives:\np(x|c) =|Jg(x)| exp{\nN\nX\ni=1\n(gi(x)µi,c\nσ2\ni,c\n−gi(x)2\n1\n2σ2\ni,c\n)}Z−1\nc\n(48)\n=|Jg(x)| exp{⟨λc, T(g(x))⟩}Z−1\nc\n(49)\nwhere λc =\n\n\nµ1,c\nσ2\n1,c\n−\n1\n2σ2\n1,c\n...\nµN,c\nσ2\nN,c\n−\n1\n2σ2\nN,c\n\n\nand T(g(x)) =\n\n\ng1(x)\ng2\n1(x)\n...\ngN(x)\ng2\nN(x)\n\n\n.\nE\nLatent state prediction\n1\n2\n4\nNumber of mixing layers\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nLatent state prediction (N=5)\nhm-nlica\nFigure 4: Performance of our Hidden Markov nonlinear ICA vs. chance level (dotted line = 0.09) for different levels of\nnonlinearity in latent state prediction. The number of latent states is 11 = 2N + 1\n",
  "categories": [
    "stat.ML",
    "cs.LG"
  ],
  "published": "2020-06-22",
  "updated": "2020-06-22"
}