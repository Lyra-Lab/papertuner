{
  "id": "http://arxiv.org/abs/2006.12641v2",
  "title": "Exploring Software Naturalness through Neural Language Models",
  "authors": [
    "Luca Buratti",
    "Saurabh Pujar",
    "Mihaela Bornea",
    "Scott McCarley",
    "Yunhui Zheng",
    "Gaetano Rossiello",
    "Alessandro Morari",
    "Jim Laredo",
    "Veronika Thost",
    "Yufan Zhuang",
    "Giacomo Domeniconi"
  ],
  "abstract": "The Software Naturalness hypothesis argues that programming languages can be\nunderstood through the same techniques used in natural language processing. We\nexplore this hypothesis through the use of a pre-trained transformer-based\nlanguage model to perform code analysis tasks. Present approaches to code\nanalysis depend heavily on features derived from the Abstract Syntax Tree (AST)\nwhile our transformer-based language models work on raw source code. This work\nis the first to investigate whether such language models can discover AST\nfeatures automatically. To achieve this, we introduce a sequence labeling task\nthat directly probes the language models understanding of AST. Our results show\nthat transformer based language models achieve high accuracy in the AST tagging\ntask. Furthermore, we evaluate our model on a software vulnerability\nidentification task. Importantly, we show that our approach obtains\nvulnerability identification results comparable to graph based approaches that\nrely heavily on compilers for feature extraction.",
  "text": "arXiv:2006.12641v2  [cs.CL]  24 Jun 2020\nExploring Software Naturalness through\nNeural Language Models\nLuca Buratti∗\nIBM Research\nluca.buratti1@ibm.com\nSaurabh Pujar∗\nIBM Research\nsaurabh.pujar@ibm.com\nMihaela Bornea∗\nIBM Research\nmabornea@us.ibm.com\nScott McCarley∗\nIBM Research\njsmc@us.ibm.com\nYunhui Zheng\nIBM Research\nzhengyu@us.ibm.com\nGaetano Rossiello\nIBM Research\ngaetano.rossiello@ibm.com\nAlessandro Morari\nIBM Research\namorari@us.ibm.com\nJim Laredo\nIBM Research\nlaredoj@us.ibm.com\nVeronika Thost\nIBM Research\nveronika.thost@ibm.com\nYufan Zhuang\nIBM Research\nyufan.zhuang@ibm.com\nGiacomo Domeniconi\nIBM Research\ngiacomo.domeniconi1@ibm.com\nAbstract\nThe Software Naturalness hypothesis argues that programming languages can be\nunderstood through the same techniques used in natural language processing. We\nexplore this hypothesis through the use of a pre-trained transformer-based lan-\nguage model to perform code analysis tasks. Present approaches to code analysis\ndepend heavily on features derived from the Abstract Syntax Tree (AST) while\nour transformer-based language models work on raw source code. This work is\nthe ﬁrst to investigate whether such language models can discover AST features\nautomatically. To achieve this, we introduce a sequence labeling task that directly\nprobes the language model’s understanding of AST. Our results show that trans-\nformer based language models achieve high accuracy in the AST tagging task.\nFurthermore, we evaluate our model on a software vulnerability identiﬁcation task.\nImportantly, we show that our approach obtains vulnerability identiﬁcation results\ncomparable to graph based approaches that rely heavily on compilers for feature\nextraction.\n1\nIntroduction\nThe ﬁelds of Programming Languages (PL) and Natural Language Processing (NLP) have long re-\nlied on separate communities, approaches and techniques. Researchers in the Software Engineering\ncommunity have proposed the Software Naturalness hypothesis [12] which argues that programming\nlanguages can be understood and manipulated with the same approaches as natural languages.\nThe idea of transferring representations, models and techniques from natural languages to program-\nming languages has inspired interesting research. However, it raises the question of whether lan-\nguage model approaches based purely on source code can compensate for the lack of structure and\n∗Equal Contribution. In no particular order.\nPreprint. Under review.\nsemantics available to graph-based approaches which incorporate compiler-produced features. The\napplication of language models to represent the source code has shown convincing results on code\ncompletion and bug detection [11, 18]. The use of structured information, such as the Abstract Syn-\ntax Tree (AST), the Control Flow Graph (CFG) and the Data Flow Graph (DFG), has proven to be\nbeneﬁcial for vulnerability identiﬁcation in source code [45]. However, the extraction of structured\ninformation can be very costly, and requires the complete project. In the case of the C and C++ lan-\nguages, this can only be done through complete pre-processing and compilation that includes all the\nlibraries and source ﬁles. Because of these requirements, approaches based on structural informa-\ntion are not only computationally expensive but also inapplicable to incomplete code, for instance a\npull request.\nThis work explores the software naturalness hypothesis, employing the pre-training/ﬁne-tuning\nparadigm widely used with transformer-based [37] language models (LMs) [5, 38] to address tasks\ninvolving the analysis of both syntax and semantics of the source code in the C language. To in-\nvestigate syntax, we ﬁrst introduce a novel sequence labeling task that directly probes the language\nmodel’s understanding of AST, as produced by Clang [48]. Furthermore, we investigate the capa-\nbilities of LMs in handling complex semantics in the source code through the task of vulnerability\nidentiﬁcation (VI). All of our experiments involved LMs pre-trained from scratch on the C language\nsource code of 100 open source repositories. The use of language models with source code rather\nthan natural language leads to multiple challenges. Data sparsity is a major problem when building\nlanguage models, leading to out-of-vocabulary (OOV) terms and poor representations for rare words.\nThese issues are particularly severe for PL because variable and function names can be of almost\narbitrary length and complexity.\nThere is a tradeoff between the granularity of tokenization and availability of long-range context for\nthe LM. Fine-grained tokenizations break identiﬁers into many small common tokens, alleviating\nissues with rare tokens, but at the risk of spreading important context across too many tokens. We ad-\ndress the OOV and rare words problems by investigating three choices for tokenization, which span\nthe context/vocabulary size tradeoff from the extreme of character-based tokenization to subword\ntokenization styles familiar to the NLP community. We indicate that the choice of the pre-training\nobjective is closely connected to the choice of the vocabulary. In particular, with character based to-\nkenization the pre-training task seems too easy. We introduce a more difﬁcult, whole word masking\n(WWM) pre-training objective.\nIn our experiments, we show that our language model is able to effectively learn how to extract\nAST features from source code. Moreover, we obtain compelling results compared to graph-based\nmethods in the vulnerability identiﬁcation task. While current approaches to code analysis depend\nheavily on features derived from the AST [45, 43, 43], our approach works using raw source code\nwithout leveraging any kind of external features. This a major advantage, since it avoids a full\ncompilation phase to avoid the extraction of structural information. Indeed, our model can identify\nvulnerabilities during the software development stage or in artifacts with incomplete code, which is\na valuable feature to increase productivity. We show the merits of simple approaches to tokenization,\nobtaining the best results using the character based tokenization with WWM pre-training.\nThe contributions of this work are summarized as follows: 1) we investigate the application of\ntransformer-based language models for complex source code analysis tasks; 2) we demonstrate that\ncharacter-based tokenization and pre-training with WWM eliminate the OOV and rare words; 3) this\nwork is the ﬁrst to investigate whether such language models can discover AST features automat-\nically; 4) our language model outperforms graph-based methods that use the compiler to generate\nfeatures.\n2\nModel: C-BERT\nWe investigate the software naturalness hypothesis by pre-training from scratch a transformer-based\nLM, based on BERT [5], on a collection of repositories written in C language. Then, we ﬁne-tune it\non the AST node tagging (Section 3) and vulnerability identiﬁcation (Section 4) tasks.\nHowever, the application of BERT on source code requires re-thinking the tokenization strategies.\nIndeed, tokenizers based on language grammar are unsuitable for use with transformer language\nmodels because of the nearly unlimited resulting vocabulary size. (There are more than 4 million\nunique C tokens in our pre-training corpus.) This greatly increases the training time of the language\n2\nTable 1: Average number of tokens per ﬁle in pre-training dataset. For comparison there were 3,272\nC tokens per ﬁle in the pre-training data. Column 4 refers to the VI task (Section 4)\nVocab Size\nPre-training data\nVI data\nTokenizer\n(tokens)\n(tokens/ﬁle)\n(tokens/ﬁle)\nChar\n103\n15,594\n1789\nKeyChar\n135\n15,011\n1469\nSPE\n5,000\n5,500\n558\nmodel, and introduces unacceptably many OOV items in held-out data if truncated to typical vocab-\nulary sizes. Encouraged by recent work on subword tokenization for source code [18] we explore\nthree subword tokenizer choices that reduce the vocabulary size and the impact of OOV and rare\nwords.\n2.1\nTokenizers\nCharacter (Char) We investigate the extreme of subword tokenization: an ASCII vocabulary, an\nalternative dismissed by [18]. Since our datasets are almost entirely ASCII characters, we are able to\nreduce the total vocabulary size to 103 (including [CLS], [SEP], [MASK] and [UNK].) This choice\nminimizes vocabulary size, at the cost of limiting the size of context window available to the model.\nCharacter + Keyword (KeyChar) Most programming languages have a relatively small number\nof keywords. By augmenting the Char vocabulary with 32 C language keywords [19], we are able\nto treat each keyword as a token, rather than breaking it up into individual characters or subwords.\nThis reduces the number of tokens per document (and increases the available context window), as\ncan be seen in Table 1.\nSentencepiece (SPE) Neural NLP models almost always use a subword tokenizer such as Senten-\ncepiece [20] or Byte-Pair Encoding (BPE) [35]. We follow other transformer based models and use\na sentencepiece tokenizer, with vocabulary size chosen to be 5000. This includes all C keywords in\nthe vocabulary, along with many common library functions and variable names. We ﬁnd that this\nleaves an almost-negligible number of tokens (< 0.001%) out-of-vocabulary in held-out data. Table\n1 shows that SPE tokenization reduces the number of tokens per document by about a factor of 3\ncompared to Char and KeyChar.\n2.2\nTransformer Based Language Models.\nOur model architecture is a multi-layer bidirectional transformer [37] based on BERT [5], which\nwe refer to as C-BERT. As in [5], the language model component of our model is ﬁrst pre-trained\non a large unlabeled corpus. Task-speciﬁc training (\"ﬁne-tuning\") is then continued with different\ntask-speciﬁc objective functions on additional training data labeled for the tasks. In this section, we\nbrieﬂy review the four objective functions and associated classiﬁer layers involved in training our\nmodels. In all cases, our model is trained on ﬁxed-width windows of tokens, to which we prepend\na [CLS] token to indicate the beginning of the sequence and appennd [SEP] token to indicate the\nend of the sequence.\nFor every input sequence X = [x1 = CLS, x2, . . . , xT −1, xT = SEP], the language model outputs\na sequence of contextualized token representations H = [h1 = hCLS, h2, . . . , hT = hSEP ] ∈\nRT ×768 where ht ∈R768.\nMasked Language Model (MLM) Pre-training Objective A small percentage of tokens are se-\nlected for \"masking\", as described in [5]. A linear layer WLM ∈R768×|V | (V denotes the C-BERT\nvocabulary associated with a tokenization) followed by softmax is added on top of C-BERT represen-\ntation to compute the probability distribution over V for each masked token. The objective function\nis maximum likelihood of the true labels as computed from\npLM = softmax(HWLM) ∈R768×|V |\n(1)\nand evaluated over only the masked tokens. The model trained with this objective is used to initialize\nthe task-speciﬁc models.\n3\nWhole Word Masked (WWM) Pre-training Objective\nThe masked language model task used\nto pretrain C-BERT depends heavily on the tokenization. Particularly with Char tokenization, many\nmasked tokens are ASCII characters within a variable name that is repeated elsewhere in the context.\nWe suspect in this case that the MLM task is too easy to adequately pretrain C-BERT, and conjecture\nthat making the pre-training task more difﬁcult by masking longer spans of source code could result\nin stronger models on downstream tasks. Indeed, there is evidence that such techniques are beneﬁcial\nin NLP. [9, 16]\nAs an alternative, we choose types of strings (for example, text strings that are legal variable or\nfunction names) to mask from the pre-training dataset, and write regular expressions to extract them.\nThese matches are stored in a trie to create a dictionary. During pre-training, when a token is selected\nfor masking, we identify which dictionary strings contain the token and then add the entire span of\ntokens from that string to the set of tokens masked in the MLM objective. We change the masking\nrate from 15% in MLM, to 3% for Char and KeyChar tokenizers and 9% for SPE. The new\nprobabilities ensure that the average number of masked tokens remains roughly the same as MLM.\nAST Fine-tuning Objective.\nIn this sequence labeling task, we add a linear layer WA ∈\nR768×|VAST | followed by softmax. Here VAST represents the set of AST labels. We ﬁne tune the\nmodel using the cross entropy between the gold AST labels and\npAST = softmax(HWAST)\n(2)\nacross all tokens.\nVI Fine-tuning Objective. The VI is a binary classiﬁcation task and depends only on the hCLS\nembedding. We add a single linear layer w ∈R768. We ﬁne-tune using the cross entropy between\nthe true labels and\npV I = sigmoid(h⊤\nCLSw)\n(3)\nevaluated across all context windows.\n2.3\nPre-training Corpus\nFor the pre-training dataset we chose the top-100 starred (at least 5500 stars) GitHub C language\nrepositories. Forks and clones were excluded to avoid duplication. These include well-maintained,\nwidely-used repositories such as linux, git, postgres, and openssl. The total size of the pre-training\ncorpus is about 5.8 GB, much less than the corpus used to train BERT, or source code versions of\nBERT such as [17] and [7]. Comments were removed to keep only tokens related to code. We\nimplemented the de-duplication strategy in [1] and found only about 40 ﬁles out of 60,000+ to be\nduplicates.\n2.4\nPre-training Details\nAll of our models are built using the Huggingface Pytorch Transformers library [41]. Regardless\nof the type of tokenizer used, our model consists of 12 layers of transformers, 768-dimensional\nembeddings, 12 attention heads/layers, the same architectural size of BERTBASE [5]. We divide\nthe training data into train, dev and test sets. Training is stopped based on dev set accuracy and\nmodels are selected for the VI task based on test set accuracy. We train on 8 nodes, with 32 GPUs,\n512 maximum sequence length and batch size of 8, per GPU, for a total batch size of 256. Learning\nrate and number of epochs vary based on the tokenizer. We found that Char and KeyChar models\nlearn faster compared to the SPE models. In total we selected 6 pretrained models, based on a\ncombination of 3 tokenizers and 2 masking strategies. We experiment with different learning rates\n(LR). Char models with both MLM and WWM were trained with LR 10−4 and reach peak accuracy\nrelatively quickly compared to the respective KeyChar and SPE models. For KeyChar we used\nLR of 2 × 10−5 for MLM and 10−4 for WWM. For SPE we used LR of 2 × 10−5 for MLM and\n2 × 10−6 for WWM.\n3\nAST Node Tagging Task\nThe Abstract Syntax Tree (AST) is an important structure produced by compilers and it has been\nused in prior works to improve the performance of code analysis tasks, such as vulnerability de-\ntection [45]. As a step toward full language model understanding of the AST, in this section, we\n4\nTable 2: Example with ∗used as multiplication (on the left); Example with ∗used as pointer deref-\nerence (on the right)\ntoken\ncursor_kind\ntoken_kind\ntoken\ncursor_kind\ntoken_kind\nif\nIF_STMT\nKEYWORD\nsizeof\nCXX_UNARY_EXPR\nKEYWORD\n(\nIF_STMT\nPUNCTUATION\n(\nPAREN_EXPR\nPUNCTUATION\nlensum\nDECL_REF_EXPR\nIDENTIFIER\n∗\nUNARY_OPERATOR\nPUNCTUATION\n∗\nBINARY_OPERATOR\nPUNCTUATION\n∗\nUNARY_OPERATOR\nPUNCTUATION\n9\nINTEGER_LITERAL\nLITERAL\ns\nDECL_REF_EXPR\nIDENTIFIER\n/\nBINARY_OPERATOR\nPUNCTUATION\n->\nMEMBER_REF_EXPR\nPUNCTUATION\n10\nINTEGER_LITERAL\nLITERAL\ncoarse\nMEMBER_REF_EXPR\nIDENTIFIER\n>\nBINARY_OPERATOR\nPUNCTUATION\n)\nPAREN_EXPR\nPUNCTUATION\nmaxpos\nDECL_REF_EXPR\nIDENTIFIER\ndescribe a sequence labeling problem where the goal is to capture the syntactic role of each compo-\nnent of a linearized AST. Part-Of-Speech (POS) tagging is an analogous problem in NLP [25].\nIn detail, we propose two tasks to show that our models can discover the AST structure. For each task\nwe represent the source code as a sequence of programming tokens, where each token is labeled with\nattributes token_kind and a cursor_kind . Gold labels for token_kind are produced by the compiler’s\ntokenizer component, while gold labels for the cursor_kind are produced by the compiler’s parser\ncomponent which maps each token to its cursor node in the AST. The role of the language model\nis to predict the correct token_kind and cursor_kind for every programming token by examining the\nsource code only.\nTo generate the gold data for our AST node tagging task we used Clang [48], an open-source C/C++\ncompiler with a modular design. Clang2 has 5 token_kind labels and 209 cursor_kind labels. The\ncompiler handles the tokenization of the input source ﬁle and assigns the attribute token_kind to each\nsource code token.\nThe AST is represented as a directed acyclic graph of cursor nodes. The cursor_kind is an attribute\nof the cursor to indicate the type of the node in the AST. During parsing, every token in the source\ncode is mapped to the corresponding cursor node in the AST according the rules of the C language\ngrammar.\nThe BERT-based language models and the baseline model solved the token_kind task with ease.\nBoth accuracy and F1 were greater than 99% across all three tokenizations on our data sets. On\nthis basis we concluded that small differences in performance on token_kind tasks were unlikely to\nprovide useful information about the strengths and weakness of our model. Further discussion will\nfocus entirely on the cursor_kind task.\nPredicting Clang’s cursor_kind label is similar to the task of word sense disambiguation in NLP. Just\nas the English word ’bank’ may refer to a ﬁnancial institution, or to the edge of a river, many tokens\nproduced by Clang have ambiguous meanings. For example the * operator is used as a binary\noperator, for multiplication, and as a unary operator, for dereferencing a pointer variable (among\nother meanings.) We show examples of Clang output for two C expressions containing * in Table 2.\nIn\nif ( lensum * 9 / 10 > maxpos )\n(4)\nthe * is used for multiplication, and is labeled BINARY_OPERATOR by Clang. In\nsizeof(**s->coarse)\n(5)\nboth instances of the * are used for pointer dereference, and are labeled UNARY _OPERATOR. The\nparentheses also have different labels in the two examples, and two meaning of the character > are\nimplicitly distinguished by its incorporation in the -> token.\nDatasets\nWe created two annotated datasets using the code from the FFmpeg [46] and QEMU [47]\nGitHub repositories. FFmpeg is a collection of libraries and tools to process multimedia content\nsuch as audio, video, subtitles and related metadata. It contains 1, 751 ﬁles with over 6 million\nClang tokens. QEMU is a generic machine and user space emulator and virtualizer containing\n2We use libclang python bindings atop Clang 11.0\n5\n2, 152 source ﬁles and over 7 million Clang tokens. Both projects are open source. We split each\ndataset by randomly assigning the ﬁles in a 70/10/20 ratio between train, dev and test sets.\nWe use the libclang python bindings atop Clang 11.0 to parse the C/C++ source ﬁles and traverse\nthe AST. In order to get the precise context information (e.g. cursor_kind for tokens), we make\nsure the source ﬁles can be correctly compiled by intercepting the build process to obtain necessary\ncompiler options and headers. We apply the LM tokenizer to the Clang output on each dataset and\nretain a unique, deterministic alignment between the Clang tokens and the LM’s tokens to produce\npredictions using C-BERT, as explained in Section 2.2.\nFine-tuning Details\nWe initialize from pre-trained C-BERT models for the three different tok-\nenization strategies described in Section 2 after validating that these models yielded acceptable\nperformance on the VI task. The sequence classiﬁer head in Equation 2 was randomly initialized.\nSelected experiments were repeated with 5 random number generator seeds in order to gauge ﬂuc-\ntuations. Since BERT is conﬁgured to handle a ﬁnite context window, the training ﬁle was divided\ninto non-overlapping windows of maximum 250 tokens. For the dev and test data a sliding window\napproach is used to create chunks of 250 tokens with 32 tokens overlap. When a token appears in\nmultiple segments, its prediction is determined through voting. Models were trained using a batch\nsize of 16, and for a maximum of 10 epochs or 24 hours, with ﬁnal models selected on the basis of\ndev-set F1. We investigated learning rates from the set of {5 × 10−5, 2 × 10−5, 10−4} For most\nexperiments the learning rate of 2 × 10−5 was used - we selected 10−4 if it yielded notably better\naccuracy during the learning rate exploration. Most experiments were run on NVidia K-80s, with\nlearning rate exploration on NVidia V-100s.\nExperimental Results\nTest set results for the cursor_kind tasks are shown in Table 3. We compare\nour transformer LMs with a BiLSTM baseline that uses the same tokenization as C-BERT. Both\nsystems produce cursor_kind predictions for every LM token in the input. We use the alignment\nbetween C tokens and LM tokens to report C token level F1 and accuracy for each dataset.\nTwo trends are immediately apparent. First, BERT-based language models out-performed BiLSTM-\nbased language models across all three tokenizations, and on both the FFmpeg and QEMU data sets.\nSecond, performance on the FFmpeg set was considerably higher than for the QEMU dataset, for\nboth the BERT and the BiLSTM-based models.\nDifferences in tokenization style led to only small changes in performance for C-BERT. These dif-\nferences were not statistically signiﬁcant - the absolute differences between the min and max F1\nacross ﬁve initialization seeds ranged from 1% to 3% for the different tokenizations of QEMU, and\nranged from 0.2% to 0.4% for the different tokenizations of FFmpeg. It appears that neural language\nmodels are able to infer enough higher-level structures of programming languages to perform these\ntasks without additional assistance from the tokenizer. For the BiLSTM results the tokenization\nstyles show no impact on the FFmpeg dataset where both the accuracy and the F1 score are high.\nThe tokenization has an impact on the QEMU dataset where the SPE tokenizer performs the best.\nWe analyzed the confusion matrix for both FFmpeg and QEMU dev sets. In both, the top 2 errors\nwere system predictions of COMPOUND_STATEMENT or DECL_REF_EXPR when the reference\nindicated O (for outside of a cursor_kind ). Clang typically indicates O for #include statements,\nmacro deﬁnitions, and architecture-dependent conditional compilations that are skipped. In particu-\nlar, we note that both repositories contain long macro deﬁnitions which superﬁcially resemble active\ncode, but which require fairly long context to identify as a macro deﬁnition. The most common error\nnot involving ’O’ was predicting DECL_REF_EXPR for COMPOUND_STMT.\nWe also experimented with BERT models pre-trained with the whole-word masking objective. These\nmodels hurt performance here, whereas (we will see in Section 4) they improve VI performance.\nFor example, on the QEMU dev-set, F1 dropped from 93.3% to 82.4% for Char tokenization,\nfrom 93.3% to 88.2% for SPE, and from 94.1% to 90.8% for KeyChar tokenization. Further\ninvestigation is needed to determine whether this difference is due to the syntax/semantics focus of\nthe two tasks or whether the WWM is simply more beneﬁcial to a task such as VI with a lower\nbaseline performance.\n6\nTable 3: Test set accuracy and F1 for the AST cursor_kind tasks\nFFmpeg\nQEMU\nModel\nTokenizer\nAcc\nF1\nAcc\nF1\nChar\n94.96\n95.71\n71.68\n80.53\nBiLSTM\nSPE\n94.69\n95.52\n74.12\n81.58\nKeyChar\n95.68\n96.58\n66.20\n76.19\nChar\n97.10\n97.72\n81.06\n87.43\nC-BERT\nSPE\n97.72\n98.29\n81.11\n87.79\nKeyChar\n97.73\n98.31\n80.78\n87.49\n4\nVulnerability Identiﬁcation Task (VI)\nWhile VI has been investigated previously (e.g. Draper[23], Juliet[28], Devign[45]) there is no\nsatisfactory comparable baseline. The Draper dataset [23] has many false positives, and the Juliet\ndataset [28] consists of synthetic data. Devign [45] released appropriate natural data, but there are\nlimitations which make it impossible to compare with their results. Only 2 out of 4 projects from\nthe dataset were released, and the training/test split was not speciﬁed. Also, unspeciﬁed data was\nomitted from their published results because of computational complexity and limitations of Joern\n[42] preprocessing, a key step in their pipeline. Furthermore, lack of details about a key feature of\ntheir GGNN [21] network, the pooling layer, prevent exact replication of the \"Devign composite\"\nmodel.\nData Description\nOur dataset consists of functions from the FFmpeg and QEMU projects with\nnon-vulnerable/vulnerable labels as released by Devign, with the duplicates removed. We also use\nthese two datasets to create a combined dataset. The original FFmpeg dataset contains 9,683 exam-\nples and QEMU contains 17,515 examples. We call these three datasets full FFmpeg, full QEMU\nand full combined datasets. In order to implement the GGNN baseline, we use Joern to compute\nmultiple graph features, including AST, just like Devign. We skip some examples that yield a com-\npilation error with Joern. With the exclusion of problematic samples, we get reduced versions of the\nfull datasets, and we call the resulting datasets FFmpeg reduced, QEMU reduced and combined\nreduced. The reduced dataset contains 6169 FFmpeg and 14,896 QEMU examples, which is a total\nreduction of 6133 relatively large functions.\nFine-tuning Details\nWe initialize with each of six pre-trained models described in Section 2.4.\nThe Huggingface implementation of BERT truncates function tokens beyond the count of 512. We\ncall this default approach T runcate. We ﬁne-tune according to Eq. 3. As can be seen in Table 1,\ncolumn 4, using Char and KeyChar tokenizers often pushes the context token count beyond 512.\nWith Char tokenizer, about 80% of FFmpeg functions have more than 510 tokens. We aggregate\nsuch functions, similar to [29], by ﬁrst breaking the input example into N different segments of\nmaximum size 510 and then using a BiLSTM layer to combine the [CLS] output. This output is\npassed to the linear layer for classiﬁcation. We train for 10 Epochs, with learning rate of 2 × 10−5\nfor KeyChar, SPE and 8 × 10−6 for Char, max sequence length of 512 and batch size of 4.\nBaselines\nThe Naive baseline shows the accuracy if all instances are labeled vulnerable. Just like\nDevign, we use BiLSTM and CNN as baselines. Our strongest baseline, GGNN, is implemented as\ndescribed in the Devign paper (GGRN composite) using the same graph features. Our implementa-\ntion of the Devign composite model, which is GGNN with pooling, did not improve upon GGNN\nbecause we lacked adequate information on how to implement the pooling layer. We did not include\nDevign composite into our results. We train BiLSTM and CNN baselines on all the six datasets.\nWe train GGNN only on the reduced datasets due to Joern compilation errors. Both BiLSTM and\nGGNN baselines are initialized with Word2Vec [27] embeddings trained on source code from the\nFFmpeg and QEMU projects.\nExperiment Results\nWe report results with accuracy as the evaluation metric rather than F1 score\nbecause our datasets are well-balanced, with 40%-55% labeled as vulnerable. The experiment re-\nsults are in Table 4. C-BERT models, with aggregation outperform the strongest GGNN baseline by\n7\nTable 4: Test set accuracy for the VI task on the full and reduced (\"red\") datasets; C-BERT model id\nindicates tokenization (C|K|S), LM masking objective (M|W) and aggregation(T|B); Aggr indicates\nthe aggregation method\nFFmpeg\nQEMU\nCombined\nModel\nLM + Tokenizer\nMasking\nAggr.\nfull\nred\nfull\nred\nfull\nred\nNaive\n51.1\n46.5\n42.4\n41.1\n45.5\n42.7\nBiLSTM\n59.5\n58.3\n61.6\n64.0\n57.6\n61.5\nCNN\n57.3\n58.7\n60.5\n63.3\n56.9\n59.9\nGGNN\nNA\n61.1\nNA\n65.8\nNA\n63.2\nCMT\nC-BERT Char\nMLM\nTruncate\n52.7\n54.7\n57.3\n58.7\n55.5\n57.8\nKMT\nC-BERT KeyChar\nMLM\nTruncate\n55.5\n57.0\n57.5\n59.5\n56.2\n58.1\nSMT\nC-BERT SPE\nMLM\nTruncate\n57.7\n54.8\n59.3\n60.5\n57.4\n58.0\nCWB\nC-BERT Char\nWWM\nBiLSTM\n62.2\n65.5\n65.8\n68.1\n63.5\n66.2\nKWB\nC-BERT KeyChar\nWWM\nBiLSTM\n58.0\n61.9\n64.1\n67.7\n61.5\n64.3\nSMB\nC-BERT SPE\nMLM\nBiLSTM\n60.7\n62.8\n66.1\n66.4\n63.6\n65.4\na reasonable margin of 3-4 points across all datasets. They also perform better than BiLSTM and\nCNN baselines on both the full and reduced datasets.\nThe three lines CMT, KMT and SMT show the effect of varying the tokenization strategy when\ntrained with the MLM objective. KMT performs better than CMT across all datasets and SMT\nhas the best overall performance. As expected, tokenizations which enable the model to see longer\ncontext windows (KeyChar and then SPE) achieve generally better results. All of our C-BERT\nMLM models improve signiﬁcantly upon the naive baseline.\nCWB, KWB and SMB are the best results for each of the three tokenizers. The best model is CWB,\nshowing the highest accuracy on most datasets. On the QEMU full and combined full, the CWB\naccuracy is comparable to SMB. BiLSTM aggregation technique improves results across all datasets,\nfor all models. WWM improves the performance of Char and KeyChar tokenizer, as expected.\nIt does not improve the SPE tokenization. We found that WWM pre-training improves Char the\nmost, and mostly eliminates systematic differences in tokenization. Indeed, a Char model, CWB,\nhas the best results on 4 of the 6 datasets, while an SPE model with MLM pre-training, SMB, is\nbest on the other two.\n5\nRelated Work\nIn recent years, research at the intersection of NLP, Machine Learning and Software Engineering\nhas signiﬁcantly increased. Topics in this ﬁeld include code completion [33], [11], [13], program\nrepair [4], [34], [36], bug detection [31] and type inference [32], [10]. [24], [30]. A common factor\namong all these techniques is the use of an n-gram or RNN based LM with the tokenization deﬁned\nby the programming language. Thus they are severely affected by the OOV and rare words problems.\nSub-word tokenization (e.g. BPE) was ﬁrst proposed by [18] as a suitable alternative for source code\nand shows compelling results for code completion and bug detection. In our work, we show that ﬁner\ngrained tokenization techniques are beneﬁcial for source code LMs compared to subword tokenizers.\nCuBert [17] is a recently introduced LM for modeling code. In contrast to our approach, this work\ndoes not consider character based tokenization and uses a subword tokenizer. A Github corpus in\nPython is used for pre-training a BERT-like model which is ﬁne-tuned for tasks like variable misuse\nclassiﬁcation, wrong binary operator detection, swapped operands, function-docstring mismatch,\nand prediction of exception type. CodeBert [7] is another transformer LM specialized for code\ngeneration from textual descriptions. Their specialized pre-training uses bimodal examples of source\ncode paired with natural language documentation from Github repositories.\nPrior to the use of machine learning techniques, static code analysis was used to understand code\nstructures and apply rules handcrafted by programmers to identify potential vulnerabilities. Tools\nlike RATS [14], Flawﬁnder [40], and Infer [6] are of this type. However, they produce many false\npositives, a problem identiﬁed early on by [2, 8, 15], making these tools difﬁcult to use effectively\nas part of the developer tool chain.\n8\nCurrent machine learning approaches to code analysis depend heavily on features derived from\nthe compiler, such as the AST, or other derived structures that require the compilation of source\ncode [45]. These features are paired with complex GGNN [26] models. In addition to the VI task\n[45], combining the AST and GGNN have shown good results in type inference [39], code clone\ndetection [3, 44] and bug detection [22]. This evidence motivated our AST tagging task.\n6\nConclusions\nThis work explores the software naturalness hypothesis by using language models on multiple source\ncode tasks. Unlike graph based approaches that use structural features like the AST, CFG and\nDFG, our model is built on raw source code. Furthermore, we show that the AST token_kind and\ncursor_kind can be learned with the LM. LMs can work even better than graph based approaches for\nVI because they avoid the computational cost and requirements of full compilation. This makes our\napproach suitable for a wider range of scenarios, such as pull requests. We propose two character\nbased tokenization approaches that solve the OOV problem while having very small vocabularies.\nWe suggest ways to improve them with aggregation and WWM. These approaches work just as well\nand sometimes even better than a subword tokenizer like sentencepiece that has been previously\nexplored for source code. In future work we propose joint learning of the AST and VI tasks on top of\nLM to further improve code analysis, without using the compiler to extract structured information.\nBroader Impact\nOur research supports the software naturalness hypothesis. This means that it may be possible to\ntransfer many recent advances in natural language understanding into the software domain and im-\nprove source code understanding. The LM created by pre-training on source code can thus be used\nfor different tasks such as VI, code completion [33], code repair [36], as well as multi-modal tasks\ninvolving both natural language and source code [7]. These tasks have applications in the ﬁelds of\nsoftware security, developer tools and automation of software development and maintenance. Here\nwe focus on software security through the task of VI. Development and improvement of end-to-\nend systems that can identify software vulnerabilities will make it easier to do the same in open\nsource software. As these tools become more available to everyone, it becomes imperative that\ndevelopers protect their code from malicious agents by incorporating these tools in Continuous Inte-\ngration/Continuous Delivery pipelines to identify vulnerabilities before code is exposed to others.\nReferences\n[1] ALLAMANIS, M.\nThe adverse effects of code duplication in machine learning models of\ncode. In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas,\nNew Paradigms, and Reﬂections on Programming and Software (2019), pp. 143–153.\n[2] CHEIRDARI, F., AND KARABATIS, G. Analyzing false positive source code vulnerabilities\nusing static analysis tools. In 2018 IEEE International Conference on Big Data (Big Data)\n(2018).\n[3] CHEN, L., YE, W., AND ZHANG, S. Capturing source code semantics via tree-based con-\nvolution over api-enhanced ast. In Proceedings of the 16th ACM International Conference\non Computing Frontiers (New York, NY, USA, 2019), CF ’19, Association for Computing\nMachinery, p. 174–182.\n[4] CHEN, Z., KOMMRUSCH, S. J., TUFANO, M., POUCHET, L., POSHYVANYK, D., AND\nMONPERRUS, M. Sequencer: Sequence-to-sequence learning for end-to-end program repair.\nIEEE Transactions on Software Engineering (2019).\n[5] DEVLIN, J., CHANG, M.-W., LEE, K., AND TOUTANOVA, K. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers) (Minneapolis, Minnesota, June\n2019), Association for Computational Linguistics, pp. 4171–4186.\n[6] FACEBOOK. Infer. https://fbinfer.com/, 2016.\n9\n[7] FENG, Z., GUO, D., TANG, D., DUAN, N., FENG, X., GONG, M., SHOU, L., QIN, B., LIU,\nT., JIANG, D., ET AL. Codebert: A pre-trained model for programming and natural languages.\narXiv preprint arXiv:2002.08155 (2020).\n[8] GADELHA, M. R., STEFFINLONGO, E., CORDEIRO, L. C., FISCHER, B., AND NICOLE,\nD. A. Smt-based refutation of spurious bug reports in the clang static analyzer. In Proceed-\nings of the 41st International Conference on Software Engineering: Companion Proceedings\n(2019), ICSE ’19, p. 11–14.\n[9] GOOGLE. Bert. github.com/google-research/bert/blob/master/README.md, 2018.\n[10] HELLENDOORN, V. J., BIRD, C., BARR, E. T., AND ALLAMANIS, M. Deep learning type\ninference. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engi-\nneering Conference and Symposium on the Foundations of Software Engineering (New York,\nNY, USA, 2018), ESEC/FSE 2018, Association for Computing Machinery, p. 152–162.\n[11] HELLENDOORN, V. J., AND DEVANBU, P. Are deep neural networks the best choice for\nmodeling source code? In Proceedings of the 2017 11th Joint Meeting on Foundations of Soft-\nware Engineering (New York, NY, USA, 2017), ESEC/FSE 2017, Association for Computing\nMachinery, p. 763–773.\n[12] HINDLE, A., BARR, E. T., SU, Z., GABEL, M., AND DEVANBU, P. On the naturalness\nof software. In Proceedings of the 34th International Conference on Software Engineering\n(2012), ICSE ’12, IEEE Press, p. 837–847.\n[13] HUSSAIN, Y., HUANG, Z., ZHOU, Y., AND WANG, S. Deep transfer learning for source code\nmodeling. arXiv preprint arXiv:1910.05493 (2019).\n[14] INC., S. S. Rough Audit Tool for Security. https://github.com/stgnet/rats, 2013.\n[15] JOHNSON, B., SONG, Y., MURPHY-HILL, E., AND BOWDIDGE, R. Why don’t software\ndevelopers use static analysis tools to ﬁnd bugs?\nIn Proceedings of the 2013 International\nConference on Software Engineering (2013), ICSE ’13, p. 672–681.\n[16] JOSHI, M., CHEN, D., LIU, Y., WELD, D. S., ZETTLEMOYER, L., AND LEVY, O. Spanbert:\nImproving pre-training by representing and predicting spans. Transactions of the Association\nfor Computational Linguistics 8 (2020), 64–77.\n[17] KANADE, A., MANIATIS, P., BALAKRISHNAN, G., AND SHI, K. Pre-trained contextual\nembedding of source code. arXiv preprint arXiv:2001.00059 (2019).\n[18] KARAMPATSIS, R.-M., BABII, H., ROBBES, R., SUTTON, C., AND JANES, A. Big code !=\nbig vocabulary: Open-vocabulary models for source code. arXiv preprint arXiv:2003.07914\n(2020).\n[19] KERNIGHAN, B. W., AND RITCHIE, D. M. The C Programming Language, 2nd ed. Prentice\nHall Professional Technical Reference, 1988.\n[20] KUDO, T., AND RICHARDSON, J. SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In \"Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Processing: System Demonstrations\" (\"Brus-\nsels, Belgium\", nov 2018), \"Association for Computational Linguistics\", pp. \"66–71\".\n[21] LI, Y., TARLOW, D., BROCKSCHMIDT, M., AND ZEMEL, R. Gated graph sequence neural\nnetworks. ICLR (2016).\n[22] LIANG, H., SUN, L., WANG, M., AND XING YANG, Y. Deep learning with customized\nabstract syntax tree for bug localization. IEEE Access 7 (2019), 116309–116320.\n[23] LOUIS KIM, REBECCA RUSSELL. Draper VDISC Dataset - Vulnerability Detection in Source\nCode. https://osf.io/d45bw/, 2020.\n[24] MALIK, R. S., PATRA, J., AND PRADEL, M. Nl2type: Inferring javascript function types\nfrom natural language information. In 2019 IEEE/ACM 41st International Conference on\nSoftware Engineering (ICSE) (2019), pp. 304–315.\n[25] MANNING, C. D., AND SCHÜTZE, H. Foundations of Statistical Natural Language Process-\ning. MIT Press, Cambridge, MA, USA, 1999.\n[26] MICROSOFT. github.com/microsoft/gated-graph-neural-network-samples, 2015.\n10\n[27] MIKOLOV, T., SUTSKEVER, I., CHEN, K., CORRADO, G. S., AND DEAN, J. Distributed\nrepresentations of words and phrases and their compositionality. In Advances in Neural Infor-\nmation Processing Systems 26, C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and\nK. Q. Weinberger, Eds. Curran Associates, Inc., 2013, pp. 3111–3119.\n[28] NIST. Juliet test suite v1.3. https://samate.nist.gov/SRD/testsuite.php, 2017.\n[29] PAPPAGARI, R., ˙ZELASKO, P., VILLALBA, J., CARMIEL, Y., AND DEHAK, N. Hierarchical\ntransformers for long document classiﬁcation. arXiv preprint arXiv:1910.10781 (2019).\n[30] PRADEL, M., GOUSIOS, G., LIU, J., AND CHANDRA, S. Typewriter: Neural type prediction\nwith search-based validation. arXiv preprint arXiv:1912.03768 (2019).\n[31] RAY, B., HELLENDOORN, V., GODHANE, S., TU, Z., BACCHELLI, A., AND DEVANBU, P.\nOn the \"naturalness\" of buggy code. In 2016 IEEE/ACM 38th International Conference on\nSoftware Engineering (ICSE) (2016), pp. 428–439.\n[32] RAYCHEV, V., VECHEV, M., AND KRAUSE, A. Predicting program properties from “big\ncode”. In Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles\nof Programming Languages (New York, NY, USA, 2015), POPL ’15, Association for Comput-\ning Machinery, p. 111–124.\n[33] RAYCHEV, V., VECHEV, M., AND YAHAV, E. Code completion with statistical language\nmodels. In In Proceedings of the 35th ACM SIGPLAN Conference on Programming Language\nDesign and Implementation (2014), pp. 419–428.\n[34] SANTOS, E. A., CAMPBELL, J. C., PATEL, D., HINDLE, A., AND AMARAL, J. N. Syntax\nand sensibility: Using language models to detect and correct syntax errors. In 2018 IEEE 25th\nInternational Conference on Software Analysis, Evolution and Reengineering (SANER) (2018),\npp. 311–322.\n[35] \"SENNRICH, R., HADDOW, B., AND BIRCH, A. \"neural machine translation of rare words\nwith subword units\". In \"Proceedings of the 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers)\" (\"Berlin, Germany\", aug 2016), \"Association\nfor Computational Linguistics\", pp. \"1715–1725\".\n[36] VASIC, M., KANADE, A., MANIATIS, P., BIEBER, D., AND SINGH, R. Neural program\nrepair by jointly learning to localize and repair. arXiv preprint arXiv:1904.01720 (2019).\n[37] VASWANI, A., SHAZEER, N., PARMAR, N., USZKOREIT, J., JONES, L., GOMEZ, A. N.,\nKAISER, L. U., AND POLOSUKHIN, I. Attention is all you need. In Advances in Neural\nInformation Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 5998–6008.\n[38] WANG, A., PRUKSACHATKUN, Y., NANGIA, N., SINGH, A., MICHAEL, J., HILL, F., LEVY,\nO., AND BOWMAN, S. R. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. In NeurIPS (2019), pp. 3261–3275.\n[39] WEI, J., GOYAL, M., DURRETT, G., AND DILLIG, I. Lambdanet: Probabilistic type infer-\nence using graph neural networks. In International Conference on Learning Representations\n(2020).\n[40] WHEELER, D.A. Flawﬁnder. http://www.dwheeler.com/flawfinder, 2018.\n[41] WOLF, T., DEBUT, L., SANH, V., CHAUMOND, J., DELANGUE, C., MOI, A., CISTAC, P.,\nRAULT, T., LOUF, R., FUNTOWICZ, M., AND BREW, J. Huggingface’s transformers: State-\nof-the-art natural language processing. arXiv preprint arXiv:1910.03771 (2019).\n[42] YAMAGUCHI, F., GOLDE, N., ARP, D., AND RIECK, K. Modeling and discovering vulnera-\nbilities with code property graphs. In Proc. of IEEE Symposium on Security and Privacy (S&P)\n(2014).\n[43] ZHANG, J., WANG, X., ZHANG, H., SUN, H., WANG, K., AND LIU, X. A novel neural\nsource code representation based on abstract syntax tree. In Proceedings of the 41st Interna-\ntional Conference on Software Engineering (2019), ICSE ’19, IEEE Press, p. 783–794.\n[44] ZHANG, J., WANG, X., ZHANG, H., SUN, H., WANG, K., AND LIU, X. A novel neural\nsource code representation based on abstract syntax tree. In 2019 IEEE/ACM 41st International\nConference on Software Engineering (ICSE) (2019), pp. 783–794.\n11\n[45] ZHOU, Y., LIU, S., SIOW, J., DU, X., AND LIU, Y. Devign: Effective vulnerability identiﬁca-\ntion by learning comprehensive program semantics via graph neural networks. In Advances in\nNeural Information Processing Systems 32 (2019), Curran Associates, Inc., pp. 10197–10207.\n[46] FFMPeg. https://github.com/FFmpeg/FFmpeg, 2020.\n[47] QEMU. https://github.com/quemu/qemu, 2020.\n[48] Clang Static Analyzer. https://clang-analyzer.llvm.org, 2020.\n12\n",
  "categories": [
    "cs.CL",
    "cs.LG",
    "cs.PL"
  ],
  "published": "2020-06-22",
  "updated": "2020-06-24"
}