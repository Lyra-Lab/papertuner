{
  "id": "http://arxiv.org/abs/2006.14773v1",
  "title": "Pushing the Limit of Unsupervised Learning for Ultrasound Image Artifact Removal",
  "authors": [
    "Shujaat Khan",
    "Jaeyoung Huh",
    "Jong Chul Ye"
  ],
  "abstract": "Ultrasound (US) imaging is a fast and non-invasive imaging modality which is\nwidely used for real-time clinical imaging applications without concerning\nabout radiation hazard. Unfortunately, it often suffers from poor visual\nquality from various origins, such as speckle noises, blurring, multi-line\nacquisition (MLA), limited RF channels, small number of view angles for the\ncase of plane wave imaging, etc. Classical methods to deal with these problems\ninclude image-domain signal processing approaches using various adaptive\nfiltering and model-based approaches. Recently, deep learning approaches have\nbeen successfully used for ultrasound imaging field. However, one of the\nlimitations of these approaches is that paired high quality images for\nsupervised training are difficult to obtain in many practical applications. In\nthis paper, inspired by the recent theory of unsupervised learning using\noptimal transport driven cycleGAN (OT-cycleGAN), we investigate applicability\nof unsupervised deep learning for US artifact removal problems without matched\nreference data. Experimental results for various tasks such as deconvolution,\nspeckle removal, limited data artifact removal, etc. confirmed that our\nunsupervised learning method provides comparable results to supervised learning\nfor many practical applications.",
  "text": "1\nPushing the Limit of Unsupervised Learning for\nUltrasound Image Artifact Removal\nShujaat Khan, Jaeyoung Huh, and Jong Chul Ye, Fellow, IEEE\nAbstract—Ultrasound (US) imaging is a fast and non-invasive\nimaging modality which is widely used for real-time clinical\nimaging applications without concerning about radiation haz-\nard. Unfortunately, it often suffers from poor visual quality\nfrom various origins, such as speckle noises, blurring, multi-\nline acquisition (MLA), limited RF channels, small number of\nview angles for the case of plane wave imaging, etc. Classical\nmethods to deal with these problems include image-domain signal\nprocessing approaches using various adaptive ﬁltering and model-\nbased approaches. Recently, deep learning approaches have\nbeen successfully used for ultrasound imaging ﬁeld. However,\none of the limitations of these approaches is that paired high\nquality images for supervised training are difﬁcult to obtain\nin many practical applications. In this paper, inspired by the\nrecent theory of unsupervised learning using optimal transport\ndriven cycleGAN (OT-cycleGAN), we investigate applicability of\nunsupervised deep learning for US artifact removal problems\nwithout matched reference data. Experimental results for various\ntasks such as deconvolution, speckle removal, limited data artifact\nremoval, etc. conﬁrmed that our unsupervised learning method\nprovides comparable results to supervised learning for many\npractical applications.\nIndex Terms—Ultrasound imaging, Portable, 3D, low-powered,\nDeep Learning, Sparse Sampling, Inverse Problem, Deconvolu-\ntion.\nI. INTRODUCTION\nUltrasound (US) imaging is a safe imaging modality with\nhigh temporal resolution, so it is considered as a ﬁrst choice for\nvarious clinical applications such as echo-cardiography, fetal\nscan, etc.\nTo form an US image, individual channel RF measurements\nare back-propagated and accumulated after applying speciﬁc\ndelays [1]. Accordingly, the quality of US images is limited\nby number of factors such as inhomogeneous sound speed,\nlimited number of channels, frame rate, etc. For instance,\nin conventional focused B-mode ultrasound, lateral and axis\nresolutions depend on the number of scan-lines and axial sam-\npling frequency, whereas in planewave B-mode imaging the\nthe quality of image is deﬁned by the number of planewaves\nused in coherent-planewave compounding (CPC) [2]. In addi-\ntion, sonographic signals are inherently susceptible to speckle\nnoises, which appear as granule patterns in US images and\nreduce the contrast-to-noise ratio (CNR) signiﬁcantly [3].\nTo address these issue, model-based methods have been\ndeveloped to remove the noises. These methods include clas-\nsical methods such as adaptive ﬁltering, deconvolution, etc.\nThe authors are with the Department of Bio and Brain Engineering, Korea\nAdvanced Institute of Science and Technology (KAIST), Daejeon 34141,\nRepublic of Korea (e-mail:{shujaat,woori93,jong.ye}@kaist.ac.kr). This work\nwas supported by the National Research Foundation of Korea under Grant\nNRF-2020R1A2B5B03001980.\n[3]–[9]. However, these methods are usually associated with\nhigh computation cost and may be subject to the model\ninconsistency [6]–[9].\nA promising direction to mitigate these issues is a deep\nlearning approach. In the recent past, deep learning has\nemerged as a promising tool for variety of medical imaging\nrelated inverse problems [10]–[15]. In particular, for high-\nquality ultrasound imaging there are number of solutions\nproposed by various researchers, which can be categorized\ninto i) channel data based solutions [16], [17], and ii) image\ndomain based solutions [16], [18]–[26]. Typically, channel\ndomain approaches such as deep beamformers [17] show\nbetter generalization in diverse imaging conditions, but image\ndomain approaches are more easier to implement for various\napplications without accessing the channel data.\nThe existing deep learning strategies mostly rely on the\npaired dataset for supervised training. However, in many real\nworld imaging situations, access to paired images (input image\nand its desired output pair) is not possible. For example, to\nimprove the visual quality of US images acquired using a\nlow-cost imaging system we need to scan exactly the same\npart using a high-end machine, which is not possible. In\nanother example, the conversion of the plane wave imaging\nto the focused B-mode image quality is not applicable by\nsupervised learning as we should obtain the same image using\ntwo different acquisition modes. One could use simulation data\nfor supervised training, but it is prone to bias to simulation\nenvironment. Therefore, we are interested in developing an\nunsupervised learning strategy where low quality images from\none scan can be used as inputs, whereas high quality images\nfrom different anatomy and imaging conditions can be used as\ntarget images, by doing so we can successfully train an image\nenhancement model.\nWe are aware that there are recent approaches in US\nliterature that aim at similar unsupervised learning set-up [21],\n[27]. For example, the authors in [21] employed cycleGAN\nnetwork to improve the image quality from portable US image\nusing high-end unmatched image data to improve the accuracy\nof cardiac chamber segmentation. Similar image domain cy-\ncleGAN approach has been recently proposed [27]. However,\nit is not clear whether such quality improvement is real or\na cosmetic change. Moreover, although there are potentially\nmany applications beyond the conversion of the low-end image\nto high-end image, these additional applications have been\nnever investigated.\nTherefore, one of the most important contributions of this\nstudy is an application of recent theory of unsupervised\nlearning using the optimal transport driven cycleGAN (OT-\narXiv:2006.14773v1  [cs.CV]  26 Jun 2020\n2\ncycleGAN) [28] for unsupervised artifact removal. Unlike the\nblack-box application of cycleGAN, the OT-cycleGAN was\nderived using optimal transport theory [29], [30] that transports\na probability distribution of noisy images to clean image\ndistribution [28]. Therefore, if properly trained, the theory\nguarantees that the image improvement is not a cosmetic\nchanges, but a real improvement by learning the distribution of\nthe clean image data distributions. Another important contri-\nbution of this paper is the extension of unsupervised learning\nto various US artifact removal problems, such as speckle\nnoise removal, deconvolution, limited measurements artifact\nremoval, plane wave to focused B-mode imaging conversion,\netc, which verify that our method provides near comparable\nresults in both qualitative and quantitative manners. Further-\nmore, our framework is so general that it can be used for\nvarious applications of US image quality improvement without\nconcerning about collecting paired reference data.\nThe rest of the paper is organized as follows. In Section II,\nwe brieﬂy review the recent theory of OT-cycleGAN for unsu-\npervised deep learning applications. The detailed information\nof the proposed method are presented in Section III, which is\nfollowed by the results and discussion in Section IV. Finally\nthe paper is concluded in Section V.\nII. THEORY\nHere, we brieﬂy introduced OT-CycleGAN [28] to make\nthe paper self-contained. However, our derivation is different\nfrom [28], since here we focus on the general geometry of\nunsupervised learning and explain why cycleGAN is a natural\nway to address this problem.\nA. Wasserstein Metric and Optimal Transport\nOptimal transport (OT) provides a mathematical means to\ncompare two probability measures [29], [30]. Formally, we say\nthat T : X 7→Y transports the probability measure µ ∈P(X)\nto another measure ν ∈P(Y), if\nν(B) = µ\n\u0000T −1(B)\n\u0001\n,\nfor all ν-measurable sets B,\n(1)\nSuppose there is a cost function c : X × Y →R ∪{∞} such\nthat c(x, y) represents the cost of moving one unit of mass\nfrom x ∈X to y ∈Y. Monge’s original OT problem [29],\n[30] is then to ﬁnd a transport map T that transports µ to ν\nat the minimum total transportation cost. Kantorovich relaxed\nthe assumption to consider probabilistic transport that allows\nmass splitting from a source toward several targets:\nmin\nπ∈Π(µ,ν)\nZ\nX×Y\nc(x, y)dπ(x, y)\n(2)\nwhere Π(µ, ν) is the set of joint distributions whose marginal\ndistribution is µ and ν, respectively.\nIf we choose a metric d in X as a transportation cost c,\nthen the optimal transport cost in (2) becomes Wasserstein-1\ndistance between two probability measures µ and ν:\nW1(µ, ν) :=\ninf\nπ∈Π(µ,ν)\nZ\nX×Y\nd(x, y)dπ(x, y)\n(3)\n=\ninf\nπ∈Π(µ,ν) Eπ [d(X, Y )]\n(4)\nwhere X, Y are the random vectors with the joint distribution\nπ, and Eπ[·, ·] is the expectation with respect to the joint\nmeasure π. Therefore, the meaning of the Wasserstein-1 metric\nis that the minimum average distance between samples in two\nprobability distributions µ and ν, the optimal transport theory\nis concerned about minimizing the average distance.\nUnlike Kullback–Leibler (KL) divergence [31], Wasserstein\nmetric is a real metric that satisﬁes all properties of a metric in\nthe metric space [29], [30]. Therefore, it provides a powerful\nway of measuring distance in the probability space, which\nis useful for unsupervised learning as described in the next\nsection.\nB. Geometry of Unsupervised Learning\nOur geometric view of unsupervised learning is shown in\nFig. 1. Here, the target image space X is equipped with a\nprobability measure µ, whereas the original image space is Y\nwith a probability measure ν. Since there are no paired data,\nthe goal of unsupervised learning is to match the probability\ndistributions rather than each individual samples. This can be\ndone by ﬁnding transportation maps that transport the measure\nµ to ν, and vice versa.\nMore speciﬁcally, the transportation from a measure space\n(Y, ν) to another measure space (X, µ) is done by a generator\nGθ : Y 7→X, realized by a deep network parameterized with\nθ. Then, the generator Gθ “pushes forward” the measure ν in\nY to a meaure µθ in the target space X [29], [30]. Similarly,\nthe transport from (X, µ) to (Y, ν) is performed by another\nneural network generator Fφ, so that the generator Fφ pushes\nforward the measure µ in X to νφ in the original space Y.\nThen, the optimal transport map for unsupervised learning can\nbe achieved by minimizing the statistical distances between µ\nand µθ, and between ν and νφ, and our proposal is to use\nthe Wasserstein-1 metric as a means to measure the statistical\ndistance.\nFig. 1. Geometric view of unsupervised learning.\nMore speciﬁcally, for the choice of a metric d(x, x′) =\n∥x−x′∥in X, using the change of measure formula [29], [30],\nthe Wasserstein-1 metric between µ and µθ can be computed\nby\nW1(µ, µθ) =\ninf\nπ∈Π(µ,ν)\nZ\nX×Y\n∥x −Gθ(y)∥dπ(x, y)\n(5)\nSimilarly, the Wasserstein-1 distance between ν and νφ is\ngiven by\nW1(ν, νφ) =\ninf\nπ∈Π(µ,ν)\nZ\nX×Y\n∥Fφ(x) −y∥dπ(x, y)\n(6)\n3\nHere, care should be taken, since we should minimize\nthe two statistical distances simultaneously for unsupervised\nlearning. More speciﬁcally, rather than minimizing (5) and\n(6) separately with distinct joint distributions, a better way of\nﬁnding the transportation map is to minimize them together\nwith the same joint distribution π:\ninf\nπ∈Π(µ,ν)\nZ\nX×Y\n∥x −Gθ(y)∥+ ∥Fφ(x) −y∥dπ(x, y)\n(7)\nThis is our unsupervised learning formulation from optimal\ntransport perspective [28].\nC. Optimal transport driven cycleGAN (OT-CycleGAN)\nOne of the most important contributions of our companion\npaper [28] is to show that the primal formulation of the\nunsupervised learning in (7) can be represented by a dual\nformulation. More speciﬁcally, the following primal problem\nmin\nθ,φ\ninf\nπ∈Π(µ,ν)\nZ\nX×Y\n∥x −Gθ(y)∥+ ∥Fφ(x) −y∥dπ(x, y)\n(8)\nis equivalent to the following dual formulation which we call\nthe optimal transport driven CycleGAN (OT-cycleGAN):\nmin\nφ,θ max\nψ,ϕ ℓcycleGAN(θ, φ; ψ, ϕ)\n(9)\nwhere\nℓcycleGAN(θ, φ) := γℓcycle(θ, φ) + ℓDisc(θ, φ; ψ, ϕ)\n(10)\nwhere γ > 0 is the hyper-parameter, and the cycle-consistency\nterm is given by\nℓcycle(θ, φ) =\nZ\nX\n∥x −Gθ(Fφ(x))∥dµ(x)\n+\nZ\nY\n∥y −Fφ(Gθ(y))∥dν(y)\nwhereas the second term is\nℓDisc(θ, φ; ψ, ϕ)\n= max\nϕ\nZ\nX\nϕ(x)dµ(x) −\nZ\nY\nϕ(Gθ(y))dν(y)\n+ max\nψ\nZ\nY\nψ(y)dν(y) −\nZ\nX\nψ(Fφ(x))dµ(x)\nHere, ϕ, ψ are often called Kantorovich potentials and\nsatisfy 1-Lipschitz condition (i.e.\n|ϕ(x) −ϕ(x′)| ≤∥x −x′∥, ∀x, x′ ∈X\n|ψ(y) −ψ(y′)| ≤∥y −y′∥, ∀y, y′ ∈Y\nIn machine learning context, the 1-Lipschitz potentials ϕ and ψ\ncorrespond to the Wasserstein-GAN (W-GAN) discriminators\n[32]. Speciﬁcally, ϕ tries to ﬁnd the difference between the\ntrue image x and the generated image GΘ(y), whereas ψ\nattempts to ﬁnd the fake measurement data that are generated\nby the synthetic measurement procedure Fφ(x). In fact, this\nformulation is equivalent to the cycleGAN formulation [33]\nexcept for the use of 1-Lipschitz discriminators.\nHere, care should be taken to ensure that the Kantorovich\npotentials become 1-Lipschitz. There are many approaches to\naddress this. For example, in the original W-GAN paper [32],\nthe weight clipping was used to impose 1-Lipschitz condition.\nAnother method is to use the spectral normalization method\n[34], which utilizes the power iteration method to impose\nconstraint on the largest singular value of weight matrix in\neach layer. Yet another popular method is the WGAN with\nthe gradient penalty (WGAN-GP), where the gradient of the\nKantorovich potential is constrained to be 1 [35]. Finally, in\nour companion paper [36], we also showed that the popular\nLS-GAN approach [37], which is often used in combination of\nstandard cycleGAN [33], is also closely related to imposing the\n1-Lipschitz condition. In this paper, we therefore consider LS-\nGAN variation as our implementation for discriminator term\nwhere the discriminator loss is given by\nℓDisc(θ, φ; ψ, ϕ) =\n−\nZ\nX\n(ϕ(x) −1)2dµ(x) −\nZ\nY\n(ϕ(GΘ(y)) + 1)2 dν(y)\n−\nZ\nY\n(ψ(y) −1)2dν(y) −\nZ\nX\n(ϕ(Fθ(x)) + 1)2 dµ(x)\n(11)\nD. Unsupervised US Artifact Removal\nBased on the mathematical background of unsupervised\nlearning and its implementation using OT-cycleGAN, we are\ninterested in solving the following unsupervised learning prob-\nlems in US:\n1) Deconvolution.\n2) Speckle noise removal.\n3) Planewave image enhancement using high quality fo-\ncused B-mode target.\n4) Multi-line acquisition (MLA) block artifact and noise\nremoval in cardiac imaging.\n5) Missing channel artifact removal\nFor each type of enhancement, we generated target data\nstarting from the fully-sampled RX data i.e., 64 channels from\nunmatched data set. The details of each application is given\nbelow.\n1) Deconvolution Ultrasound: The axial resolution of ultra-\nsound imaging is limited by the bandwidth of the transducer.\nConventional beamforming methods such as delay-and-sum\n(DAS) are limited by the accuracy of ray approximation of\nthe wave propagation [7]. In order to overcome these issues,\nmany researchers have explored the deconvolution of US\nimages [6]–[8]. Deconvolution ultrasound may help in dealing\nwith modeling inaccuracies and ﬁnite bandwidth issues, which\nwill eventually improve the spatial resolution of an ultrasonic\nimaging system.\nSpeciﬁcally, the received signal is modeled as a convolution\nof tissue reﬂectivity function (TRF) x with a point spread\nfunction (PSF) h, where tissue reﬂectivity function represents\nscatter’s acoustic properties, while the impulse response of\nthe imaging system is modeled by point spread function.\nThe estimation of x from the DAS measurement is known\nas a deconvolution US problem. In most practical cases, the\ncomplete knowledge of h is not available, and therefore both\nunknown TRF x and the PSF h have to be estimated together,\n4\nwhich is called the blind deconvolution problem [38]. One\nstrategy is to estimate h and x jointly [39], and another strategy\nis to estimate them separately [40], i.e., ﬁrst h is estimated\nfrom y, and then x is estimated based on h [41].\nIn this paper, for unmatched target distribution data gener-\nation, the second strategy is used ﬁrst with small number of\nDAS images, which is followed by training the DeepBF [8],\n[17] to perform deconvolution-based beamforming. This way\na large number of target data can be generated easily without\nsolving deconvolution problems for large number of data set.\n2) Speckle-noise removal: The granular patterns appears in\nUS images due to constructive and destructive interference of\nultrasonic wave. These are called ‘speckle’ noise. The speckle\nnoise is a multiplicative impulse noise. It is a major reason of\nquality degradation and removal of it can improve the visual\nquality and subsequently enhance the structural details in US\nimages [4]. In recent past, a variety of reasonably good de-\nspeckling algorithms have been proposed for US imaging [3]–\n[5]. However, most of them are either too slow to use for\nrun-time application or require complicated conﬁgurations of\nparameter for each image. These issues hinder the utilization\nin real world scenarios.\nOne such algorithm is proposed by Zhu et al [4], which is\nbased on the principal of non-local low-rank (NLLR) ﬁltering.\nTo generate speckle free target data herein, we used NLLR\nmethod. In NLLR, the image is pre-processed to generate\na guidance map and later non-local ﬁltering operations are\nperformed on the candidate patches that are selected using\nthat guidance map. For further reﬁnement of ﬁltered patches,\na truncated nuclear norm (TWNN) and structured sparsity\ncriterion are used [42], [43]. This algorithm is used to generate\nour target samples for despeckle images.\n3) Planewave image enhancement using high quality fo-\ncused B-mode target: Planewave (PW) imaging is an emerging\nmode of US scanning. It offers ultra-fast scanning capabilities\nwith comparable image quality. In PW imaging the quality\nof the acquired scan depends on the number of planewaves\n(PWs) used to generate the ﬁnal image. For most of the\nclinical applications, multiple PWs are combined using the\nCPC method to produce a desired quality image. However,\nthere is a trade-off in the quality and speed of the scan as\neach PW scanning require additional scanning time, limiting\nthe application of PWI for high quality accelerated imaging\n[44].\nTo ﬁnd an optimal trade-off between speed and visual\nquality, there are number of deep learning based PW com-\npounding methods [20], [45]. However, these method require\naccess to fully-sampled (31 ∼75) planewaves data to train a\nsupervised model. Typical lower-end commercial systems are\nnot equipped with such hardware complexity to produce such\na high quality label dataset.\nTherefore, we propose to use an unsupervised learning in\nwhich high-quality label images are obtained using focused\nB-mode imaging. For further quality improvement, the target\nfocused B-mode images are processed using deconvolution\nand ﬁltering with NLLR [4] speckle denoising algorithm.\n4) MLA block artifact and noise removal in cardiac imag-\ning: Echocardiography (ECHO) require fast scan time, and\nit is typically performed by a phased array probe operating\nin focused scanning mode in which multiple scan-lines are\ncombined to form a complete image. Therefore, to scan a\nlarge region of interest, high number of scan-lines are required\nresulting in reduced temporal resolution.\nFor accelerated echocardiography, conventional acceleration\nmethods like multi-line acquisition (MLA) are used, where\neach transmit/receive event’s data is used to generate multiple\nscan-lines. The limitation of the MLA is that it works only\nfor limited acceleration factor and produces blocking artifacts\nfor high frame rate [22], [46]. In addition to measurement’s\nlimitation, sonographic signals from echocardiography are\nsusceptible of speckle noise which is also a major factor for\nthe degradation of visual quality. As such, this visual quality\nand temporal resolution trade-off is a bottle neck for many\nechocardiography applications.\nA variety of deep learning based block artifact removal\nmethods exist, but they are designed for supervised training\nand require access to high quality labelled channel data [16],\n[22], [46]. In this study we proposed an image domain unsu-\npervised MLA artefact and noise removal deep neural network\nmethod. Unpaired target image distributions are generated\nwith single-line-acquisition (SLA) and ﬁltered using NLLR\n[4] ﬁltering technique.\n5) Missing channel artifact removal: The power consump-\ntion, size, and cost of the US system are mainly dependent on\nthe number of measurement channels. Therefore, in portable\nand three dimension ultrasound imaging system, there is an\nincreasing demand for computational algorithms which can\nproduce high quality images using fewer receive channels.\nConventional beamforming methods are not designed for sub-\nsampled RF data and standard DAS is highly susceptible\nto sub-sampling in measurements. On the other hand, ad-\nvance compressive beamforming methods [26], [47], [48] are\ncomputationally expensive and require hardware modiﬁcations\nlimiting their use as generalized solution.\nRecently a deep learning based compressive beamformer\nwas proposed [17], which can help reconstruct high quality\nimages from limited measurements. However, the method in\n[17] requires an access to fully-sampled label data which is not\naccessible for low-cost imaging system. In this study, we pro-\nposed to design an image domain quality enhancement method\nthat can directly process corrupted images to remove missing\nchannel artefact, and improve the contrast and resolution of\nthe B-mode images.\nFor the generation of unpaired target data distribution,\nunmatched target images are generated using DeepBF [17].\nIII. METHOD\nA. Dataset\nIn this study, we used four different dataset, all were\nacquired using an E-CUBE 12R US system (Alpinion Co.,\nKorea). For data acquisition, we used a linear array (L3-12H),\nand phased array (SP1-5) transducers and their conﬁguration\nare given in Table I.\n5\nFig. 2. Proposed network architecture: (a) Generator network, (b) Discriminator network.\nTABLE I\nUS PROBES CONFIGURATION\nParameter\nLinear array\nPhased array\nProbe Model No.\nL3-12H\nSP1-5\nCarrier wave frequency\n8.48/10.0 MHz\n3.1 MHz\nSampling frequency\n40 MHz\n40 MHz\nScan wave mode\nFocused/ 31-Planewaves\nFocused\nNo. of probe elements\n192\n192\nNo. of Tx elements\n128\n128\nNo. of TE events\n96\n96\nNo. of Rx elements\n64 (from center of Tx)\n64 (from center of Tx)\nElements pitch\n0.2 mm\n0.3 mm\nElements width\n0.14 mm\n0.22 mm\nElevating length\n4.5 mm\n13.5 mm\nAxial depth range\n20∼80 mm\n75 mm\nLateral length\n38.4 mm\n57.6 mm\nFocal depth range\n10∼40 mm\n45 mm\n1) Linear array focused B-mode dataset: The ﬁrst data\nconsist of 400 in-vivo and 218 phantom frames scanned using\na center frequency of 8.48 MHz. The in-vivo dataset acquired\nfrom the carotid/thyroid area of 10 volunteers, 40 temporal\nframes were scanned from each subject. For phantom dataset,\nwe acquired 218 frames from ATS-539 multipurpose tissue\nmimicking phantom. The phantom was scanned from different\nviews angles. Second dataset was scanned from the calf and\nforearm regions of two volunteers using 10 MHz carrier\nfrequency. There are total 100 images were scanned 50 from\neach body part.\nFor deconvolution and denoising experiments, the training\nwas performed using only the ﬁrst data. In particular, for\ntraining purpose the dataset of 8 individuals consist of 320\nin-vivo and 192 phantom images were used, while remaining\n80 images from two different individuals and 26 images from\ncompletely different region of phantom were used for testing.\nFor additional validation, second independent dataset is used.\nNote that all models were trained on same 508 images and\nno additional training is performed on any of the testing or\nindependent dataset.\nFor missing channel artifact experiments the above men-\ntioned data set is expanded into six subsets of input data each\nconsist of 4, 8, 16, 24, 32, and 64 channels representing 16×,\n8×, 4×, 2.667×, 2×, and 1× sub-sampling rates respectively.\n2) Linear array planewave B-mode dataset: For planewave\nimaging experiments, we collected the third dataset. For this\ndataset, we used the same (L3-12) operating at center fre-\nquency of 8.48 MHz in a planewave mode. There are total\n309 scans acquired, 100 from ATS-539 phantom and 209\nfrom in-vivo carotid/thyroid area of 10 volunteers. The dataset\nis expanded by using different subsets of PWs to simulate\ndifferent imaging conﬁgurations. In particular, four subsets\nwere used each consist of 31PWs, 11PWs, 7PWs and 3PWs.\nFor planewave image enhancement experiments 508 = 127×4\nimages were used. All PW images were processed using\nstandard DAS and CPC method [2]. The training dataset is\ncomposed of 127 images 50 of which were acquired from\nATS-539 phantom and remaining 77 from in-vivo dataset of 4\nvolunteers. The remaining dataset of phantom and 6 volunteers\nwas used for testing purpose only.\n3) Phased array B-mode dataset: To design MLA artifact\nremoval experiment, we designed an additional dataset. This\ndataset was acquired using (SP1-5) phased array probe and it\nconsist of 105 scans of different regions of ATS-539 phantom\nand 489 scans from the cardiac region of 7 volunteers. Five\nsubsets of images are generated using 16, 24, 32, 48 and 96\ntransmit events representing 6-MLA, 4-MLA, 3-MLA, 2-MLA\nand a single line acquisition (SLA). The target images are\ngenerated from SLA images ﬁltered with NLLR [4] algorithm.\nThe training dataset used in this study consists of 55 phantom\nscans, and 297 in-vivo scans acquired from 5 individual, while\nremaining dataset were used for testing. Total number of\nimages in training and test datasets are 1760 = 5×(55+297),\nand 1210 = 5 × (50 + 192) respectively.\nB. Network speciﬁcation\n1) Generator Model: The generator model has a U-Net\narchitecture as shown in Fig. 2(a). The model comprises\nof 9 modules, which consists of 27 convolution layers with\nbatch-normalization, pooling, up-sampling and concatenation\nblocks for skip connections. For all convolution layers ReLu\nactivation function and 2D ﬁlters of kernel size (3 × 3) were\nused, except for the output layer, where (1 × 1) ﬁlter size is\nused. The number of ﬁlters Gf is doubled in every next module\nof encoder part and halve in every next decoder module, expect\nfor the output layer where only a single layer is used to\nproduce single channel output. For example, in deconvolution\nUS and despeckle (speckle noise removal) experiments the\nnumber of channels starts from 8 i.e., in the ﬁrst module there\nwere 8 channels (number of ﬁlters) and in the next module the\nnumber of ﬁlters increased to 16, 32 and so forth. For missing\n6\nchannel and MLA artifact removal experiments, the number\nof channels starts from 64 and 16 respectively.\n2) Discriminator Model: The discriminator model is a fully\nconvolution neural network model to implement PatchGAN\n[33]. Unlike conventional discriminator where mapping be-\ntween an input image to a single scalar vector is performed,\nPatchGAN learns the mapping of sub-array (Patch) represent-\ning individual elements and their relative position in an image.\nThe model comprises of 4 convolution blocks each consist\nof a set of two convolution layers having stride-size of 2\nwith batch normalization and Leaky ReLu activation function.\nThe number of ﬁlters Df are doubled in every next module,\nexpect for the output layer where only a single layer is used\nto produce single channel output. The ﬁlter size in all layers\nwas again (3×3) except for the last layer where (1×1) ﬁlter\nsize is used. For all experiments the number of ﬁlter was 256,\nexcept for the MLA artifact experiment where it was chosen to\nbe 512. A detailed schematic of discriminator model is shown\nin Fig. 2(b).\nC. Performance metrics\nFor quantitative evaluation of our proposed method, the\nstandard quality metrics of ultrasound imaging are used.\nSpeciﬁcally, as the local anatomical structure or region of\ninterest are important in US, we used contrast statistics. The\ncontrast between the two regions of interests (Ra) and (Rb)\nin the image is quantiﬁed in terms of contrast-recovery (CR),\ncontrast-to-noise ratio (CNR), and generalized CNR (GCNR)\n[49]. To select region (Ra) and (Rb), we manually generated\nseparate ROI masks for each image.\nMore speciﬁcally, the contrast recovery is quantiﬁed as\nCR(Ra, Rb) = |µRa −µRb|\n(12)\nwhere µRa, and µRb, are the local means of region (Ra)\nand (Rb) respectively. The CR measure is a standard measure\nfor contrast. However, it does not consider the SNR loss. In\ntypical contrast enhancement methods, the contrast is usually\nimproved at the cost of SNR; therefore, to estimate the overall\ngain in contrast with respect to noise level, we used CNR\nmeasure which is deﬁned as\nCNR(Ra, Rb) = |µRa −µRb|\nq\nσ2\nRa + σ2\nRb\n,\n(13)\nwhere σRa, and σRb are the standard deviations of region (Ra)\nand (Rb) respectively. Recently, a more reliable measure of\ncontrast is proposed called generalized-CNR (GCNR) [49].\nThe GCNR is supposed to be an unbiased measure of contrast\nin which the overlap between the intensity distributions of two\nregions are compared as\nGCNR(Ra, Rb) = 1 −\nZ\nmin{pRa(i), pRb(i)}di,\n(14)\nwhere i is the pixel intensity, and pRa and pRb are the\nprobability distributions region (Ra) and (Rb) respectively. If\nthe intensities of both regions are statistically independent,\nthen GCNR will be equals to one, whereas, if they completely\noverlap then GCNR will be zero [49].\nIn addition to image quality, we also compared the recon-\nstruction time of proposed method.\nD. Network training\nIn the supervised learning of the network for comparative\nstudy, match image pairs are used to minimize the l1 loss\nand SSIM loss between target and the network output. For\nunsupervised learning, the loss function deﬁned in (9) is\nminimized.\nBoth the supervised and unsupervised methods were im-\nplemented using Python on TensorFlow platform [50]. For\nparameter optimization the Adam optimizer is used [51]. For\nsupervised training the default values of adam were used,\nwhile for unsupervised case the learning rate is linearly\nchanged from 5 × 10−4 to 1 × 10−4 in 200 epochs.\nFig. 3.\nDeconvolution results on fully-sampled RF data. B-Mode images\nfrom tissue mimicking phantom (left), and from in-vivo data of carotid region\n(center & right).\nIV. RESULTS\nIn the following, we verify the performance of the algorithm\nfor the following experiments:\n1) Deconvolution ultrasound.\n2) Speckle noise removal.\n3) Planewave image enhancement using high quality fo-\ncused b-mode target.\n4) MLA block artifact and noise removal in cardiac imag-\ning.\n5) High quality ultrasound imaging from sub-sampled\nchannel data.\nFor the calculation of contrast metrics two regions are selected\nas highlighted with red and blue dotted lines in respective\nﬁgures. The same regions are magniﬁed as inset ﬁgure for\nbetter visualization.\n7\nFig. 4. Comparison of performance metrics by various methods.\n1) Deconvolution: Fig. 3 show example results from in-\nvivo and phantom scans. From the ﬁgures it can be easily\nseen that the deconvoluted target images have better contrast\nand anatomical structures are quite prominent compared to\nthe input DAS images. Furthermore, both supervised and\nunsupervised deep learning method successfully learn the\ndeconvolution ﬁltering and improve the visual quality of das\nimages.\nFor quantitative comparison, the improvement in visual\nquality is quantiﬁed and it is prominent in the contrast distri-\nbution plots shown in Fig. 4. In particular, using unsupervised\nlearning method, CR values are comparable to the supervised\nmethod. As expected, contrary to CR, the CNR and GCNR\nvalues are reduced in deconvolution targets, this is because the\ndeconvolution enhances resolution at the cost of noisy high-\nfrequency components. Therefore, the contrast to noise ratio\ndrops substantially, however it is worth noting that at the same\nCNR and GCNR the CR gain in unsupervised method is much\nhigher compared to label and supervised method. In particular,\ncompared to input (DAS) the proposed unsupervised method\nrecover 3.28 dB, 2.96 dB, and 1.66 dB better CR in Calf\nand Forearm, Phantom and Thyroid and Carotid regions scans\nrespectively, which is 71.60%, 162.02%, and 16.11% higher\nthan supervised method.\n2) Speckle removal: In this experiment, we perform the\nspeckle de-noising from DAS images using NLLR [4], su-\npervised, and unsupervised learning methods. In Fig. 5 one\nphantom and two in-vivo examples are shown. As for compar-\nison, DAS input images are ﬁltered using NNLR [4] method.\nCompared to the DAS images, the speckle noise in output\nimages is noticeably reduced. The granular patterns in output\nimages are well suppressed, and resultant images from both\nsupervised and unsupervised learning methods are similar to\nthe target speckle free images. Here it is noteworthy to point-\nout that the reconstruction time of deep learning methods\nis several magnitude lower than the NLLR [4] method, and\nunlike NLLR no parameter tuning is needed.\nIn order to quantify the performance gain, we utilized the\nsame performance metrics used in deconvolution experiments,\nand the results are shown in Fig. 6. Interestingly, the CNR\nvalues in despeckle methods are signiﬁcantly improved. The\nreason for high CNR and GCNR is that the despeckle methods\nsuppresses the noises while maintain the contrast and structural\ndetails.\nIn particular, compared to input DAS image, the proposed\nunsupervised method enhance the CNR by 0.89 units, 0.28\nunits, and 0.23 units in Calf and Forearm, Phantom and\nThyroid and Carotid regions scans respectively, which is\ncomparable to supervised methods which shows 1.22, 0.34,\nand 0.23 units gain, and NLLR [4] method which show 0.85,\n0.35, and 0.26 units gain in Calf and Forearm, Phantom and\nThyroid and Carotid regions scans, respectively.\n3) Planewave image enhancement using high quality fo-\ncused B-mode target.: In this experiment, we design a model\nto improve the quality of planewave images using high qual-\nity focused B-mode images as target images. In particular,\ndifferent sets of PWs were used to generate low quality\ninput images. In Fig. 7, three example results are shown.\n8\nFig. 5. Speckle removal results on fully-sampled RF data. B-Mode images\nfrom tissue mimicking phantom (left), and from in-vivo data of carotid region\n(center & right).\nFrom the ﬁgure it can be seen that the contrast of the input\nimages has been improved. It is remarkable that for each\ntype of enhancement, a single trained model is used for all\nacceleration factors (PWs combinations). In Table II, three\ndifferent measures of contrast are shown. From the results it\ncan be clearly seen that the statistics of the output images\nare substantially improved. When the deconvoluted B-mode\nimages are used as target distribution, on average there is a\n3.60 dB gain in CR; when deconvoluted and speckle removed\nB-mode images are used as targets, on average there is a 4.20\ndB, 0.565 units and 0.04 units gain in terms of CR, CNR and\nGCNR respectively.\nTABLE II\nCOMPARISON OF PERFORMANCE STATISTICS ON TEST DATA IN THE\nPLANEWAVE IMAGE ENHANCEMENT.\nnumber of\nCR (dB)\nCNR\nGCNR\nPWs\na\nb\nc\na\nb\nc\na\nb\nc\n3\n13.36\n17.42\n17.73\n2.05\n2.09\n2.55\n0.8405\n0.8434\n0.8904\n7\n15.24\n18.64\n19.81\n2.29\n2.29\n2.86\n0.8739\n0.8742\n0.9162\n11\n16.23\n19.65\n20.54\n2.48\n2.46\n3.11\n0.8962\n0.8949\n0.9312\n31\n17.17\n20.69\n20.73\n2.65\n2.62\n3.21\n0.9164\n0.9165\n0.9388\na Input, b Deconvolution targets, c Deconvolution + despeckle targets\n4) High quality accelerated Echocardiograph: In Fig. 8(a)\nreconstruction results of conventional MLA (referred to as\ninput) and the proposed methods are compared for different\nnumber of acceleration factors (transmit events), which are\nreferred as SLA, 2-MLA, 3-MLA, 4-MLA, and 6-MLA re-\nspectively. For better visualization of the noise suppression\neffect of our proposed method, a region selected in target\nimage is zoomed out and shown as an inset image. From\nthe example results, it is quite evident that the reconstructed\nimages are very much similar to the target image and have\nless noise/artifacts compared to input image. Our method\nsufﬁciently enhance the visual quality of the input images\nby eliminating both the speckle and block artifacts for all\nacceleration factors. It is noteworthy to point out that the\nproposed method is trained in an unsupervised fashion and\na single one-time trained model is used for all acceleration\nfactors. From reconstructions error statistics in Table III, it is\nevident that the quality degradation in input images is much\nhigher than the output images. In particular, on average there\nis 0.1372 gain in GCNR.\nHere we would like to emphasize that the proposed method\nis based on a single universal model which is one-time trained\nfor multi-tasks i.e., blocking artifact and speckle noise removal\nand it works for variable MLA schemes without retraining.\nSince in real in-vivo case it is difﬁcult to decide between\nanomalies and true structures, to ensure structural preservation\nwe provided additional results using tissue mimicking phan-\ntom in Fig. 8(b). The results conﬁrm that our method can\naccurately recover the phantom for most cases. However, with\nhiger acceleration factors e.g., 6-MLA the block artefacts are\nbecoming prominent and recovery to target quality is not ideal.\nApart from reconstruction quality improvement one major\nadvantage of our method is the fast reconstruction time. This\nis especially important for real time echocardiography that\nrequires fast image reconstruction. Once a model is success-\nfully trained, on average reconstruction time for a single\nimage is around 7.92 (milliseconds), which is same for all\nacceleration factors and it could further reduce by optimized\nimplementation.\nTABLE III\nCOMPARISON OF PERFORMANCE STATISTICS ON TEST DATA FOR NOISE\nAND BLOCK ARTIFACT REMOVAL FROM MLA.\nSLA/MLA\nGCNR\nfactor\nInput\nOutput\nSLA\n0.8221\n0.9422\n2-MLA\n0.8173\n0.9380\n3-MLA\n0.8072\n0.9361\n4-MLA\n0.7797\n0.9313\n6-MLA\n0.7263\n0.8911\n5) High Quality Ultrasound Imaging From Sub-Sampled\nChannel Data: In this experiment, six sets of RF data at\ndifferent down-sampling rates is generated. For each case\na separate image is generated, mimicking low-quality/low-\npowered imaging conditions. For all sub-sampling conﬁgura-\ntions a single universal model is used, i.e., no separate training\nis performed for individual sub-sampling case.\nFig. 9 shows the results on example images generated\nfor proposed unsupervised learning method and DAS method\nusing sub-sampled RF data. The trained models are evaluated\nfor standard quality measures. Overall a single model (one-\ntime trained using either supervised/unsupervised approach)\nproduces signiﬁcant performance gain in terms of contrast,\nand resolution for all RF sub-sampling conﬁgurations and the\nperformance is comparable with the supervised learning (see\nTable IV).\n9\nFig. 6. Comparison of performance metrics by various methods.\nFig. 7. Planewave ultrasound images enhancement. B-Mode images from in-vivo data of carotid regions.\nV. CONCLUSION\nMedical ultrasound imaging is prone to variety of artifacts\nsuch as resolution and contrast loss due to insufﬁcient mea-\nsurement, speckle noise, etc. These artifacts are the major\nreasons of quality degradation in ultrasound imaging. To\naddress this issue, we proposed a robust unsupervised deep\nlearning approach that can help generate high-quality US\nimages from low quality noisy images. Compared to the\n10\nFig. 8. Reconstruction of high quality accelerated imaging: (a) B-modes images from echocardiograph. (b) B-Mode images from a tissue mimicking phantom.\nTABLE IV\nCOMPARISON OF PERFORMANCE STATISTICS ON TEST DATA FOR\nCOMPRESSIVE DECONVOLUTION ULTRASOUND.\nsub-sampling\nCR (dB)\nfactor\nInput\nSupervised\nUnsupervised\n1\n8.32\n9.80\n11.06\n2\n7.10\n9.24\n9.83\n4\n6.39\n8.54\n8.64\n8\n6.13\n8.39\n7.87\n16\n6.09\n8.34\n7.35\nblack-box approaches, our approach was derived based on the\nrigorous formulation of unsupervised learning using optimal\ntransport theory, so with proper training, the method provided\nreliable reconstruction results without creating any artiﬁcial\nfeatures. Since our method does not require paired data for\ntraining, the method can be applied for various US image\nenhancement applications, providing an important platform for\nfurther investigation.\nREFERENCES\n[1] M. Fink, “Time reversal of ultrasonic ﬁelds. i. basic principles,” IEEE\nTransactions on Ultrasonics, Ferroelectrics, and Frequency Control,\nvol. 39, no. 5, pp. 555–566, Sep. 1992.\n[2] G. Montaldo, M. Tanter, J. Bercoff, N. Benech, and M. Fink, “Coherent\nplane-wave compounding for very high frame rate ultrasonography and\ntransient elastography,” IEEE transactions on ultrasonics, ferroelectrics,\nand frequency control, vol. 56, no. 3, pp. 489–506, 2009.\n[3] J. Zhang and Y. Cheng, “Despeckle ﬁlters for medical ultrasound\nimages,” in Despeckling Methods for Medical Ultrasound Images.\nSpringer, 2020, pp. 19–45.\n[4] L. Zhu, C. Fu, M. S. Brown, and P. Heng, “A non-local low-rank\nframework for ultrasound speckle reduction,” in 2017 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), July 2017, pp.\n493–501.\n[5] P. Coup´e, P. Hellier, C. Kervrann, and C. Barillot, “Nonlocal means-\nbased speckle ﬁltering for ultrasound images,” IEEE transactions on\nimage processing, vol. 18, no. 10, pp. 2221–2229, 2009.\n[6] Z. Chen, A. Basarab, and D. Kouam´e, “Compressive deconvolution in\nmedical ultrasound imaging,” IEEE transactions on medical imaging,\nvol. 35, no. 3, pp. 728–737, 2015.\n[7] J. A. Jensen, “Deconvolution of ultrasound images,” Ultrasonic imaging,\nvol. 14, no. 1, pp. 1–15, 1992.\n[8] J. Duan, H. Zhong, B. Jing, S. Zhang, and M. Wan, “Increasing\naxial resolution of ultrasonic imaging with a joint sparse represen-\ntation model,” IEEE Transactions on Ultrasonics, Ferroelectrics, and\nFrequency Control, vol. 63, no. 12, pp. 2045–2056, Dec 2016.\n[9] C. Schretter, S. Bundervoet, D. Blinder, A. Dooms, J. D’hooge, and\nP. Schelkens, “Ultrasound imaging from sparse rf samples using system\npoint spread functions,” IEEE transactions on ultrasonics, ferroelectrics,\nand frequency control, vol. 65, no. 3, pp. 316–326, 2017.\n[10] E. Kang, J. Min, and J. C. Ye, “A deep convolutional neural network\nusing directional wavelets for low-dose x-ray ct reconstruction,” Medical\nPhysics, vol. 44, no. 10, 2017.\n[11] J. C. Ye, Y. Han, and E. Cha, “Deep convolutional framelets: A\ngeneral deep learning framework for inverse problems,” SIAM Journal\non Imaging Sciences, vol. 11, no. 2, pp. 991–1048, 2018.\n[12] Y. S. Han, J. Yoo, and J. C. Ye, “Deep learning with domain adap-\ntation for accelerated projection reconstruction mr,” arXiv preprint\narXiv:1703.01135, 2017.\n[13] H. K. Aggarwal, M. P. Mani, and M. Jacob, “MoDL: Model\n11\nFig. 9.\nReconstruction of high quality ultrasound image from sub-sampled\nRF data. B-Mode images from in-vivo data of carotid artery region.\nbased deep learning architecture for inverse problems,” arXiv preprint\narXiv:1712.02862, 2017.\n[14] S. Khan, J. Huh, and J. C. Ye, “Deep learning-based universal beam-\nformer for ultrasound imaging,” in International Conference on Medical\nImage Computing and Computer-Assisted Intervention. Springer, Cham,\n2019, pp. 619–627.\n[15] T. W¨urﬂ, F. C. Ghesu, V. Christlein, and A. Maier, “Deep learning\ncomputed tomography,” in International Conference on Medical Image\nComputing and Computer-Assisted Intervention.\nSpringer, 2016, pp.\n432–440.\n[16] Y. H. Yoon, S. Khan, J. Huh, J. C. Ye et al., “Efﬁcient b-mode ultrasound\nimage reconstruction from sub-sampled rf data using deep learning,”\nIEEE transactions on medical imaging, 2018.\n[17] S. Khan, J. Huh, and J. C. Ye, “Adaptive and compressive beamforming\nusing deep learning for medical ultrasound,” IEEE Transactions on\nUltrasonics, Ferroelectrics, and Frequency Control, 2020.\n[18] M. Feigin, D. Freedman, and B. W. Anthony, “A deep learning frame-\nwork for single sided sound speed inversion in medical ultrasound,”\narXiv preprint arXiv:1810.00322, 2018.\n[19] A. A. Nair, M. R. Gubbi, T. D. Tran, A. Reiter, and M. A. L. Bell, “A\nfully convolutional neural network for beamforming ultrasound images,”\nin 2018 IEEE International Ultrasonics Symposium (IUS). IEEE, 2018,\npp. 1–4.\n[20] S. Khan, J. Huh, and J. C. Ye, “Universal plane-wave compounding for\nhigh quality us imaging using deep learning,” in 2019 IEEE International\nUltrasonics Symposium (IUS).\nIEEE, 2019, pp. 2345–2347.\n[21] M. H. Jafari, H. Girgis, N. Van Woudenberg, N. Moulson, C. Luong,\nA. Fung, S. Balthazaar, J. Jue, M. Tsang, P. Nair et al., “Cardiac point-\nof-care to cart-based ultrasound translation using constrained cyclegan,”\nInternational Journal of Computer Assisted Radiology and Surgery, pp.\n1–10, 2020.\n[22] S. Vedula, O. Senouf, G. Zurakhov, A. Bronstein, M. Zibulevsky,\nO. Michailovich, D. Adam, and D. Gaitini, “High quality ultrasonic\nmulti-line transmission through deep learning,” in International Work-\nshop on Machine Learning for Medical Image Reconstruction. Springer,\n2018, pp. 147–155.\n[23] A. A. Nair, T. D. Tran, A. Reiter, and M. A. L. Bell, “A generative\nadversarial neural network for beamforming ultrasound images: Invited\npresentation,” in 2019 53rd Annual Conference on Information Sciences\nand Systems (CISS).\nIEEE, 2019, pp. 1–6.\n[24] Z. Yu, E.-L. Tan, D. Ni, J. Qin, S. Chen, S. Li, B. Lei, and T. Wang,\n“A deep convolutional neural network-based framework for automatic\nfetal facial standard plane recognition,” IEEE journal of biomedical and\nhealth informatics, vol. 22, no. 3, pp. 874–885, 2018.\n[25] E. Ozkan, V. Vishnevsky, and O. Goksel, “Inverse problem of ultrasound\nbeamforming with sparsity constraints and regularization,” IEEE trans-\nactions on ultrasonics, ferroelectrics, and frequency control, vol. 65,\nno. 3, pp. 356–365, 2018.\n[26] R. Cohen and Y. C. Eldar, “Sparse convolutional beamforming for\nultrasound imaging,” IEEE transactions on ultrasonics, ferroelectrics,\nand frequency control, vol. 65, no. 12, pp. 2390–2406, 2018.\n[27] O. Huang, W. Long, N. Bottenus, G. E. Trahey, S. Farsiu, and M. L.\nPalmeri, “Mimicknet, matching clinical post-processing under realistic\nblack-box constraints,” in 2019 IEEE International Ultrasonics Sympo-\nsium (IUS).\nIEEE, 2019, pp. 1145–1151.\n[28] B. Sim, G. Oh, S. Lim, and J. C. Ye, “Optimal transport, cycleGAN,\nand penalized LS for unsupervised learning in inverse problems,” arXiv\npreprint arXiv:1909.12116, 2019.\n[29] C. Villani, Optimal transport: old and new.\nSpringer Science &\nBusiness Media, 2008, vol. 338.\n[30] G. Peyr´e, M. Cuturi et al., “Computational optimal transport,” Founda-\ntions and Trends in Machine Learning, vol. 11, no. 5-6, pp. 355–607,\n2019.\n[31] S. Kullback, Information theory and statistics.\nCourier Corporation,\n1997.\n[32] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” arXiv\npreprint arXiv:1701.07875, 2017.\n[33] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks,” in Proceedings\nof the IEEE international conference on computer vision, 2017, pp.\n2223–2232.\n[34] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “Spectral\nnormalization for generative adversarial networks,” arXiv preprint\narXiv:1802.05957, 2018.\n[35] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville,\n“Improved training of Wasserstein GANs,” in Advances in neural\ninformation processing systems, 2017, pp. 5767–5777.\n[36] S. Lim, S.-E. Lee, S. Chang, and J. C. Ye, “CycleGAN with a blur\nkernel for deconvolution microscopy: Optimal transport geometry,” IEEE\nTrans. on Computational Imaging (in press), also available as arXiv\npreprint arXiv:1908.09414, 2020.\n[37] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. Paul Smolley, “Least\nsquares generative adversarial networks,” in Proceedings of the IEEE\nInternational Conference on Computer Vision, 2017, pp. 2794–2802.\n[38] T. Taxt and G. V. Frolova, “Noise robust one-dimensional blind deconvo-\nlution of medical ultrasound images,” IEEE transactions on ultrasonics,\nferroelectrics, and frequency control, vol. 46, no. 2, pp. 291–299, 1999.\n[39] C. Yu, C. Zhang, and L. Xie, “A blind deconvolution approach to\nultrasound imaging,” IEEE transactions on ultrasonics, ferroelectrics,\nand frequency control, vol. 59, no. 2, pp. 271–280, 2012.\n[40] J. A. Jensen, “Estimation of pulses in ultrasound b-scan images,” IEEE\ntransactions on medical imaging, vol. 10, no. 2, pp. 164–172, 1991.\n[41] R. Jirik and T. Taxt, “Two-dimensional blind bayesian deconvolution\nof medical ultrasound images,” IEEE transactions on ultrasonics, ferro-\nelectrics, and frequency control, vol. 55, no. 10, pp. 2140–2153, 2008.\n[42] D. Zhang, Y. Hu, J. Ye, X. Li, and X. He, “Matrix completion by\ntruncated nuclear norm regularization,” in 2012 IEEE Conference on\nComputer Vision and Pattern Recognition. IEEE, 2012, pp. 2192–2199.\n[43] S. Gu, L. Zhang, W. Zuo, and X. Feng, “Weighted nuclear norm\nminimization with application to image denoising,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2014,\npp. 2862–2869.\n[44] M. Tanter and M. Fink, “Ultrafast imaging in biomedical ultrasound,”\nIEEE transactions on ultrasonics, ferroelectrics, and frequency control,\nvol. 61, no. 1, pp. 102–119, 2014.\n[45] M. Gasse, F. Millioz, E. Roux, D. Garcia, H. Liebgott, and D. Friboulet,\n“High-quality plane wave compounding using convolutional neural net-\nworks,” IEEE transactions on ultrasonics, ferroelectrics, and frequency\ncontrol, vol. 64, no. 10, pp. 1637–1639, 2017.\n12\n[46] G. Matrone, A. Ramalli, A. S. Savoia, P. Tortoli, and G. Magenes,\n“High frame-rate, high resolution ultrasound imaging with multi-line\ntransmission and ﬁltered-delay multiply and sum beamforming,” IEEE\ntransactions on medical imaging, vol. 36, no. 2, pp. 478–486, 2017.\n[47] N. Wagner, Y. C. Eldar, and Z. Friedman, “Compressed beamforming in\nultrasound imaging,” IEEE Transactions on Signal Processing, vol. 60,\nno. 9, pp. 4643–4657, 2012.\n[48] A. Burshtein, M. Birk, T. Chernyakova, A. Eilam, A. Kempinski, and\nY. C. Eldar, “Sub-nyquist sampling and fourier domain beamforming\nin volumetric ultrasound imaging,” IEEE Trans. Ultrason., Ferroelectr.,\nFreq. Control, vol. 63, no. 5, pp. 703–716, 2016.\n[49] A. Rodriguez-Molares, O. M. H. Rindal, J. D’hooge, S. M˚asøy,\nA. Austeng, M. A. Lediju Bell, and H. Torp, “The generalized contrast-\nto-noise ratio: A formal deﬁnition for lesion detectability,” IEEE Trans-\nactions on Ultrasonics, Ferroelectrics, and Frequency Control, vol. 67,\nno. 4, pp. 745–759, 2020.\n[50] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for\nlarge-scale machine learning.” in OSDI, vol. 16, 2016, pp. 265–283.\n[51] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n",
  "categories": [
    "cs.CV",
    "cs.LG",
    "eess.IV",
    "stat.ML"
  ],
  "published": "2020-06-26",
  "updated": "2020-06-26"
}