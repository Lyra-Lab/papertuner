{
  "id": "http://arxiv.org/abs/2006.15110v1",
  "title": "Learning predictive representations in autonomous driving to improve deep reinforcement learning",
  "authors": [
    "Daniel Graves",
    "Nhat M. Nguyen",
    "Kimia Hassanzadeh",
    "Jun Jin"
  ],
  "abstract": "Reinforcement learning using a novel predictive representation is applied to\nautonomous driving to accomplish the task of driving between lane markings\nwhere substantial benefits in performance and generalization are observed on\nunseen test roads in both simulation and on a real Jackal robot. The novel\npredictive representation is learned by general value functions (GVFs) to\nprovide out-of-policy, or counter-factual, predictions of future lane\ncenteredness and road angle that form a compact representation of the state of\nthe agent improving learning in both online and offline reinforcement learning\nto learn to drive an autonomous vehicle with methods that generalizes well to\nroads not in the training data. Experiments in both simulation and the\nreal-world demonstrate that predictive representations in reinforcement\nlearning improve learning efficiency, smoothness of control and generalization\nto roads that the agent was never shown during training, including damaged lane\nmarkings. It was found that learning a predictive representation that consists\nof several predictions over different time scales, or discount factors,\nimproves the performance and smoothness of the control substantially. The\nJackal robot was trained in a two step process where the predictive\nrepresentation is learned first followed by a batch reinforcement learning\nalgorithm (BCQ) from data collected through both automated and human-guided\nexploration in the environment. We conclude that out-of-policy predictive\nrepresentations with GVFs offer reinforcement learning many benefits in\nreal-world problems.",
  "text": "Learning predictive representations in autonomous\ndriving to improve deep reinforcement learning\nDaniel Graves\nNoah’s Ark Lab\nHuawei Technologies Canada, Ltd\nEdmonton, Canada\ndaniel.graves@huawei.com\nNhat M. Nguyen\nNoah’s Ark Lab\nHuawei Technologies Canada, Ltd\nEdmonton, Canada\nminh.nhat.nguyen@huawei.com\nKimia Hassanzadeh\nNoah’s Ark Lab\nHuawei Technologies Canada, Ltd\nEdmonton, Canada\nkimia.hassanzadeh@huawei.com\nJun Jin\nNoah’s Ark Lab\nHuawei Technologies Canada, Ltd\nEdmonton, Canada\njun.jin1@huawei.com\nAbstract\nReinforcement learning using a novel predictive representation is applied to au-\ntonomous driving to accomplish the task of driving between lane markings where\nsubstantial beneﬁts in performance and generalization are observed on unseen test\nroads in both simulation and on a real Jackal robot. The novel predictive represen-\ntation is learned by general value functions (GVFs) to provide out-of-policy, or\ncounter-factual, predictions of future lane centeredness and road angle that form a\ncompact representation of the state of the agent improving learning in both online\nand ofﬂine reinforcement learning to learn to drive an autonomous vehicle with\nmethods that generalizes well to roads not in the training data. Experiments in\nboth simulation and the real-world demonstrate that predictive representations in\nreinforcement learning improve learning efﬁciency, smoothness of control and\ngeneralization to roads that the agent was never shown during training, including\ndamaged lane markings. It was found that learning a predictive representation\nthat consists of several predictions over different time scales, or discount factors,\nimproves the performance and smoothness of the control substantially. The Jackal\nrobot was trained in a two step process where the predictive representation is\nlearned ﬁrst followed by a batch reinforcement learning algorithm (BCQ) from\ndata collected through both automated and human-guided exploration in the en-\nvironment. We conclude that out-of-policy predictive representations with GVFs\noffer reinforcement learning many beneﬁts in real-world problems.\n1\nIntroduction\nLearning good representations in deep reinforcement learning that generalize to the complexity\nencountered by agents in the real-world is a challenging problem [White, 2015]. Being able to\nlearn and adapt continually by collecting experience is a long standing goal of AI [Sutton et al.,\n2011][Modayil et al., 2012]. Agents need to learn and understand the patterns observed in their\nsensorimotor streams to understand the world [Sutton et al., 2011][Clark, 2013]. This kind of\nrepresentation is often not focused on solving a particular problem but rather expanding its knowledge\nand skills to enable the agent to adapt quickly to new problems [Schaul and Ring, 2013]. A framework\nof learning predictions was presented in [Sutton et al., 2011] including out-of-policy predictions\nPreprint. Under review.\narXiv:2006.15110v1  [cs.LG]  26 Jun 2020\nabout alternative policies different from the policy the agent is behaving under in the environment1.\nAn example of an out-of-policy question is \"if I keep going straight, will I drive off the road?\"\nThese kinds of questions are very useful for decision making; if the answer is yes, then it means the\nroad turns ahead and some corrective action is needed before the vehicle drives off the road. A key\nchallenge that has prevented the wide-spread adoption of predictive representations is how to learn\nthe right predictive questions to ask [White, 2015]. Despite this challenge, the study of the brain gives\nus clues that the brain could not only represent knowledge in the form of sensorimotor predictions\n[Clark, 2013] but may even learn them with similar temporal difference learning mechanisms found\nin many reinforcement learning algorithms today [Russek et al., 2017].\nLearning out-of-policy predictions could be promising in speeding up and improving the learning\nof policies especially in ofﬂine learning, cf [Levine et al., 2020]. Ofﬂine learning is about learning\nnew policies from data that was collected in the past to solve a new task. Ofﬂine learning requires\nlearning out-of-policy predictions about the environment that are counter-factual \"what if\" questions\nin order to predict how the environment might change under new policies[Levine et al., 2020]. This\nis a challenge that could be met by predictive representations of the form of GVFs as a framework\nfor learning the answer to counter-factual queries referenced in [Levine et al., 2020]. However, a\nmodel that is capable of producing all possible out-of-policy predictions is very difﬁcult to learn;\nwith GVFs, one could focus on learning a collection of out-of-policy predictions that are useful for\nlearning solutions to many tasks.\nPrior work in predictive representations is motivated by the predictive representation hypothesis\nstating that \"particularly good generalization will result from representing the state of the world in\nterms of predictions about possible future experience\" [Littman and Sutton, 2002] [Rafols et al.,\n2005]. Efforts since have focused on understanding how to learn and build an agent’s representation\nof the world from its observations and interactions with it [Sutton et al., 2011][White, 2015][Modayil\net al., 2012]. Two prominent directions for predictive representations exist in the literature: (1) using\nthe predictions directly to represent the state of the agent [Littman and Sutton, 2002][Schaul and\nRing, 2013], and (2) using predictions to learn a shared representation with the agent [Jaderberg\net al., 2016]. In [Schaul and Ring, 2013], general value functions (GVFs) are used as a representation\nof state where signiﬁcant beneﬁts are observed in partially observable grid worlds including better\ngeneralization when compared with predictive state representation (PSR) [Littman and Sutton, 2002].\nLearning a shared representation through auxiliary tasks [Jaderberg et al., 2016] is now a very popular\napproach since it is very challenging to learn predictions that are sufﬁcient to satisfy the Markov\nproperty of state. The challenge with shared representations is that it is often not clear what kind\nof auxiliary tasks speed up or slow down learning; furthermore, the representations learned lack a\ndegree of transparency in understanding how the learned representations impact control decisions.\nThe engineered predictive representation solution that we present provides some transparency as the\npredictions directly impact control decisions which is important for understanding, testing and trusting\nreal-world applications of reinforcement learning. Finally, only a few works have demonstrated how\nto learn and use predictive representations that are applied to real-world robots [Günther et al., 2016]\n[Edwards et al., 2016].\nOur primary contribution focuses on investigating how predictive representations impact learning\na policy with reinforcement learning and its generalization to roads and conditions not provided\nto the agent during training. We focus on the challenging problem of learning to drive a vehicle\nbetween two lane markings with a camera using reinforcement learning where the state of the\nagent is described by a vector of GVF predictions. The novel predictions learned are out-of-policy\npredictions; out-of-policy in the sense that they are ﬁxed before learning and are different from both\nthe optimal driving policy and the behavior policy used to learn them. However, in many real-world\napplications like autonomous driving, the importance sampling ratio often used to learn out-of-policy\npredictions [Schlegel et al., 2019] cannot be computed exactly since the behavior distributions of\nhuman drivers are unknown. Therefore, an algorithm is presented that demonstrates the efﬁcacy in\nlearning the predictive representations with deep learning where the importance sampling ratio is\napproximated since the behavior policy distribution that appears in the denominator of the importance\nsampling ratio is unknown. Finally, we aim to motivate further research into the area of predictive\nrepresentation learning, and its application to real-world problems such as autonomous driving. The\nnovel contributions of this work are summarized as follows:\n1these predictions are sometimes called off-policy predictions\n2\n• Demonstrate a novel predictive representation using GVFs that reveals substantial beneﬁts\nin learning policies for autonomous driving in the real-world\n• Investigate how predictive representations improve learning efﬁciency, substantially improve\nthe smoothness of the control decisions, and improve generalization to unseen test roads\nand conditions not included during training\n• Present an algorithm for learning predictive representations that approximate the importance\nsampling ratio for applications where the behavior policy is not known.\n2\nPredictive Learning\nLet us consider a partially observable MDP described by a set of observations O, a set of states S, a\nset of actions A, transition dynamics with probability P(s′|s, a) of transitioning to next state s′ after\ntaking action a from state s, and a reward r. Because this setting is partially observable, the true state\ns of the agent is hidden. The objective of a POMDP is the classical RL objective of learning a policy\nπ that maximizes the future discounted sum of rewards. A common approach to solving a POMDP\nis to approximate s with a history of observations and actions; however, when the observations are\nhigh dimensional, learning low dimensional features with deep reinforcement learning where the\npolicy is changing is challenging because the target is moving [Mnih et al., 2013]. Our objective is to\nachieve more stable representation learning using GVF predictions with ﬁxed pre-deﬁned policies\nwhich pushes the heavy burden of deep feature representation learning in RL to the easier problem of\nprediction learning [Schlegel et al., 2019][Ghiassian et al., 2018][Daniel Graves, 2019][Jaderberg\net al., 2016]. For the remainder of this section, state s is approximated with a history of observations\nand actions forming a high dimensional observation vector that contains an abundance of superﬂuous\ninformation.\nTo ask a predictive question, one must deﬁne a cumulant ct = c(st, at, st+1), aka pseudo-reward, a\npolicy τ(a|s) and continuation function γt = γ(st, at, st+1). The answer to the predictive question\nis the expectation of the return Gt of the cumulant ct when following policy τ deﬁned by\nV τ(s) = Eτ[Gt] = Eτ[\n∞\nX\nk=0\n(\nk\nY\nj=0\nγt+j+1)ct+k+1|st = s, at = a, at+1:T −1 ∼τ, T ∼γ]\n(1)\nwhere 0 ≤γt ≤1 [Sutton et al., 2011]. The agent usually collects experience under a different\nbehavior policy µ(a|s). When µ is different from τ, the predictive question is called an out-of-policy\nprediction2. Cumulants are commonly scaled by a factor of 1−γ when γ is a constant in non-episodic\npredictions. The GVF V τ(s) can be approximated with any function approximator, such as a neural\nnetwork, parameterized by θ to learn (1). The parameters θ are optimized with gradient descent\nminimizing the following loss function\nL(θ) = Es∼dµ,a∼µ[ρδ2] = Es∼dµ,a∼τ[δ2]\n(2)\nwhere δ = ˆvτ(s; θ) −y is the TD error and ρ = τ(a|s)\nµ(a|s) is the importance sampling ratio to correct\nfor the difference between the policy distribution τ and behavior distribution µ. dµ is the state\ndistribution of the behavior policy µ and the time subscript on c and γ has been dropped to simplify\nnotation. Note that only the behavior policy distribution is corrected rather than the state distribution\ndµ. The target y is produced by bootstrapping a prediction [Sutton, 1988] of the value of the next\nstate following policy τ given by\ny = Es′∼P [c + γˆvτ(s′; θ)|s, a]\n(3)\nwhere y is a bootstrapped prediction using recent parameters θ that are assumed constant in the\ngradient computation. Unlike in [Mnih et al., 2013] where target networks consisting of older\nparameters were needed to stabilize learning of the value function, a target network is typically\nnot needed when learning predictions that have a ﬁxed policy τ since learning is more stable. The\ngradient of the loss function (2) is given by\n∇θL(θ) = Es∼dµ,a∼µ[ρδ∇θˆvτ(s; θ)]\n(4)\n2Some literature call this an off-policy prediction which should be distinguished from off-policy RL\n3\nAn alternative approach to using importance sampling ratios ρ friendly to deep learning methods is to\napply importance resampling [Schlegel et al., 2019]. With importance resampling, a replay buffer D\nof size N is required and the gradient is estimated from a mini-batch and multiplied with the average\nimportance sampling ratio of the samples in the buffer ¯ρ =\nPN\ni=1 ρi\nN\n. The importance resampling\ngradient is given by\n∇θL(θ) = Es,a∼D[¯ρδ∇θˆvτ(s; θ)]\n(5)\nwhere the transitions in the replay buffer are sampled according to Di =\nρi\nPN\nj=1 ρj where ρi = τ(ai|si)\nµ(ai|si)\nfor transition i in the replay buffer D. This approach is proven to have lower variance than (4) with\nlinear function approximation [Schlegel et al., 2019]. An efﬁcient data structure for the replay buffer\nis the SumTree used in prioritized experience replay [Schaul et al., 2016].\n2.1\nEstimating behavior\nA behavior policy needs to be deﬁned to adequately explore the environment when learning GVFs.\nThis may be a policy deﬁned by an expert, an evolving policy that is learned by RL, a random policy\nfor exploring the environment, or a human driver collecting data safely. It is common, especially in\nthe case of human drivers, for the behavior policy distribution µ(a|s) of the agent to be unknown.\nWe present an algorithm to learn the behavior policy and combine it with prediction learning to\nachieve out-of-policy prediction learning with GVFs. The density ratio trick is used where the\nratio of two probability densities can be expressed as a ratio of discriminator class probabilities that\ndistinguish samples from the two distributions. Let us deﬁne a probability density function η(a|s) for\nthe distribution to compare to the behavior distribution µ(a|s) and class labels y = +1 and y = −1\nthat denote the class of the distribution that the state action pair was sampled from: µ(a|s) or η(a|s)\nrespectively. A discriminator g(a, s) is learned that distinguishes state action pairs from these two\ndistributions using the cross-entropy loss. The ratio of the densities can be computed using only the\ndiscriminator g(a, s).\nµ(a|s)\nη(a|s) = p(a|s, y = +1)\np(a|s, y = −1) = p(y = +1|a, s)p(a|s)/p(y = +1)\np(y = −1|a, s)p(a|s)/p(y = −1)\n= p(y = +1|a, s)\np(y = −1|a, s) =\ng(a, s)\n1 −g(a, s)\n(6)\nHere we assume that p(y = +1) = p(y = −1). From this result, we can estimate µ(a|s) with ˆµ(a|s)\nas follows\nˆµ(a|s) =\ng(a, s)\n1 −g(a, s)η(a|s)\n(7)\nwhere η(a|s) is a known distribution over action conditioned on state. The uniform distribution over\nthe action is independent of state and has the advantage of being effective and easy to implement.\nAlternatively, one can estimate the importance sampling ratio without deﬁning an additional distribu-\ntion η by replacing the distribution η with τ; however, deﬁning η to be a uniform distribution ensured\nthe discriminator was learned effectively across the entire action space. The combined algorithms for\ntraining GVFs out-of-policy with an unknown behavior distribution are given in the Appendix for\nboth the online and ofﬂine RL settings.\n3\nPredictive Control for Autonomous Driving\nOnce predictions have been deﬁned and learned, our objective is to approximate the state s of the\nagent with a low dimensional representation φ that consists of GVF predictions. The rationale is that\nφ is a very compact representation and in our autonomous driving problem. Learning a policy π(a|φ)\ncould provide substantial beneﬁts over learning π(a|s) directly from high dimensional observations\nsuch as (1) improving learning speed of deep policies, (2) learning φ from data collected either ofﬂine\nor online, and (3) predicting cumulants that may be too expensive to obtain, too noisy for stable and\nreliable control or prohibited in certain environments. These advantages are particularly important\nin autonomous driving. Firstly, autonomous vehicles cannot be permitted to explore due to safety\nconcerns and thus exploiting real-world data collected by both human drivers and autonomous vehicle\nsystems for ofﬂine or batch reinforcement learning could be very valuable in learning useful policies.\nSecondly, high accuracy sensors for localizing a vehicle on a map can be very expensive (e.g. DGPS)\n4\nand not always available - especially in GPS-denied locations [Bing-Fei Wu et al., 2007]. Hence,\nsolutions with such sensors are not easily scalable to large ﬂeets of autonomous vehicles in diverse\nlocations.\nSeveral kinds of off-the-shelf methods can be applied to learning the policy π(a|φ): (1) online\nreinforcement learning with methods like deterministic policy gradient (DDPG) [Silver et al., 2014],\nand (2) ofﬂine reinforcement learning with methods like batch-constrained Q-learning (BCQ) [Fu-\njimoto et al., 2018]. The key insight is the way the predictive representation is crafted and learned\nthat enables RL to learn to both steer an autonomous vehicle and modulate its speed from camera\nobservations. In Figure 1, the center of lane is depicted; the policy π to be learned should produce\nactions that cause the vehicle to follow the center of lane precisely. Out-of-policy predictions of\nfuture lane centeredness are depicted as the deviation (or error) between the path of π and another\nﬁxed policy τ that generates a new action that is the same (or close) to the last action taken. This is\ndistinctly different from prior architectures that investigate predictions where τ = π, ie. predicting\ncumulants about the policy being learned [Barreto et al., 2017][Russek et al., 2017] [Van Seijen\net al., 2017]. A simple thought experiment is needed to understand why predictions of future lane\ncenteredness must be out-of-policy (or counter factual) predictions to learn a useful control policy;\nif we consider τ = π then the optimal policy π will follow the center of line precisely resulting in\nall predictions being zero since there is no deviation from the lane centeredness under π which is\ninsufﬁcient information to learn the optimal policy. We postulate that out-of-policy predictions τ ̸= π\ncould be more important for control than on-policy predictions τ = π and could be what’s needed to\nconstruct predictive representations that describe the state of the agent. Out-of-policy predictions can\nbe thought of as anticipated future \"errors\" that allow controllers to take corrective actions before the\nerrors occur.\nTo drive an autonomous vehicle, we learn predictions of lane centeredness and road angle using\navailable (and potentially noisy) localization techniques and a map. Cumulants for lane centeredness\nα and road angle β are depicted in Figure 2. Multiple time scales are learned, as depicted in Figure 1,\nwhich allow the agent to predict how the road curves in the future in a very compact representation\nfrom just images. φ(s) is thus the concatenation of lane centeredness predictions over multiple time\nscales, road angle predictions over multiple time scales, current speed of the vehicle, and the last\naction taken. It is worth pointing out that φ(s) consists of both predictions and two pieces of low\ndimensional information that are difﬁcult to ascertain from predictions of lane centeredness and road\nangle; in particular the last action is crucial because the policy τ depends on the last action.\nFigure 1: Future predictions capturing information about the shape of the road ahead\n(a) Lane centeredness\n(b) Road angle\nFigure 2: (a) the lane centeredness position α is the distance from the center of the lane to the center\nof the vehicle. (b) the road angle β is the angle between the direction of the vehicle and the direction\nof the road.\n4\nExperiments\nThe policies using predictive representations that control the steering angle and vehicle speed based\non a front facing camera are learned and evaluated in two environments: the TORCS racing simulator\n[Wymann et al., 2013] and a real-world Jackal robot. Our objective is to understand the impact\npredictive representations have on (1) learning, (2) generalization and robustness, and (3) smoothness\nof control. The policies that are learned in TORCS are evaluated on unseen test road periodically\n5\nduring simultaneous training of predictions and RL driving policy. For experiments on the Jackal\nrobot, both representations and policies are trained ofﬂine from exploration data collected in the real\nworld; the policy was trained with BCQ [Fujimoto et al., 2018]. The Jackal robot was then evaluated\non test roads different and more complex from the training data including damaged and distracted\nroads to test the robustness of the policies in the real-world.\n4.1\nTORCS Simulator\nThe proposed solution has three variants: (1) myopic predictions called GVF-0.0-DDPG where\nγ = 0 which is similar to the method in [Chen et al., 2015], (2) future predictions called GVF-\n0.95-DDPG where γ = 0.95, and (3) multiple time scale predictions called GVF-DDPG with\nγ = [0.0, 0.5, 0.9, 0.95, 0.97]. For details on how the predictive representations are learned, refer\nto the Appendix. We compare with a kinematic-based steering approach [Paden et al., 2016] and\ntwo variants of DDPG. The kinematic-based steering approach is a classical controller that using\nlane centeredness α and road angle β as controlled process variables where steering and speed are\ncontrolled independently. The DDPG variants are (1) DDPG-Image where DDPG observations are the\nsame as the GVF-DDPG method, and (2) DDPG-LowDim where DDPG appends lane centeredness\nα and road angle β to the observation vector. All learned policies output steering angle and vehicle\nspeed commands and observe a history of two images, the current vehicle speed and the last action.\nThe agents were trained on 85% of 40 tracks available in TORCS. The rest of the tracks were used\nfor testing (6 in total) to measure the generalization performance of the policies. Results are repeated\nover 5 runs for each method. Only three of the tracks were successfully completed by at least one\nlearned agent and those are reported here; refer to the Appendix for more detailed results. The\nreward in the TORCS environment is given by rt = 0.0002vt(cos βt + |αt|) where vt is the speed of\nthe vehicle in km/h, βt is the angle between the road direction and the vehicle direction, and αt is\nthe current lane centeredness. The policies were evaluated on test roads at regular intervals during\ntraining as shown in Figures 3 and 4.\nFigure 3: Test scores (accumulated reward) evaluated every 1000 steps during training for dirt-dirt-4,\nevo-evo-2 and road-spring\nThe GVF-DDPG agent achieved the best solution of all the learned agents and approached the\nperformance of the baseline. The GVF-0.0-DDPG and GVF-0.95-DDPG variations initially learned\nvery good solutions but then diverged indicating that one prediction may not be enough to control both\nsteering angle and vehicle speed. This agrees with ﬁndings in psychological studies of how humans\ndrive a vehicle [Salvucci and Gray, 2004]. Despite an unfair advantage provided by DDPG-LowDim\nwith the inclusion of lane centeredness and road angle in the observation vector, GVF-DDPG still\noutperforms both variants of DDPG on many of the test roads. DDPG-Image was challenging to tune\nand train due to instability in learning that is widely known in RL literature. The angular jerkiness\nand longitudinal jerkiness are deﬁned as the standard deviation in the change in steering action\nand speed action respectively. Only GVF-DDPG with multiple time scale predictions is able to\nachieve extraordinarily smooth control that is only matched by the classical control baseline; both\n6\nFigure 4: Angular and longitudinal jerkiness evaluated every 1000 steps during training for dirt-dirt-4,\nevo-evo-2 and road-spring\nDDPG-Image and DDPG-LowDim fail to provide smooth control policies. Additional results are\nfound in the Appendix.\n4.2\nJackal Robot\nWe then applied the same methods to a Jackal robot using only data collected in the real-world.\nThe proposed solution, simply called GVF, is a combination of learning a predictive representation\nwith GVFs and ofﬂine reinforcement learning with BCQ [Fujimoto et al., 2018]. Two baselines are\ndesigned: (1) a classical controller using existing model predictive control (MPC) software available\nfor the robot in ROS packages that uses a map and laser scanner for localization, and (2) a ofﬂine\ndeep reinforcement learning solution called E2E learned end-to-end with BCQ directly from images.\nOfﬂine reinforcement learning was convenient since it minimized wear and tear on the robot over\nlong periods of interaction, reduced safety concerns due to exploration and reduced the need of\nhuman supervision during training since the robot’s battery needed to be charged very 4 hours. All of\nthese considerations made online learning with RL impractical. The GVF and E2E methods were\ntrained with the same data and observations for a total of 10M updates. For GVF, the ﬁrst half of the\nupdates were dedicated to learning the predictive representation and the second half of the updates\nwere dedicated to learning the policy with BCQ. The observations of the learned policies consisted of\na history of 2 images, current vehicle speed, and last action. The learned polices output the steering\nangle and desired speed. The reward is rt = vt(cos βt + |αt|) where vt is the speed of the vehicle in\nkm/h, βt is the angle between the road direction and the vehicle direction, and αt is the current lane\ncenteredness.\nThe training data consisted of 6 training roads and 3 test roads where each of the 3 test roads had\ndamaged variants. The test roads were (1) a rectangle-shaped road with rounded outer corners, (2)\nan oval-shaped road, and (3) a road with multiple turns with complexity not observed by the agent\nduring training. Refer to the Appendix for more details in the experimental setup and training.\nHighlights of the most interesting results are provided in Tables 1, 2 and 3. GVF was compared\nto E2E in Table 1 where GVF was found to be far superior to E2E; the E2E policy had an average\nspeed roughly half of that of the GVF and failed to generalize. E2E was able to drive almost an entire\nloop of the rectangle road in the counter clockwise (CCW) direction but went out of the lane and\nwas unable to recover; moreover, in the clockwise (CW) direction, E2E did not generalize in the\nreverse direction and failed immediately. In Table 2, GVF was compared to MPC where GVF was\nfound to produce superior performance in nearly all metrics at a high target speed of 0.4 m/s. MPC\nwas difﬁcult to tune for 0.4 m/s; At 0.25 m/s, the gap in performance between GVF and MPC was\nlessened. Table 3 highlights the similarity in performance when the track is damaged demonstrating\nhow well the GVF method generalizes. Refer to the Appendix for more detailed results and plots.\n7\nTable 1: Comparison of GVF and E2E on Rectangle test road with 0.4 m/s target speed\nMethod\nDirection\nReward\n/s ↑\nAverage\nspeed ↑\nOffcentered\n-ness ↓\nAbsolute\nroad angle ↓\nNear out\nof lane ↓\nGVF\nCCW\n2.6835\n0.3205\n0.1345\n0.1315\n0.0\nE2E\nCCW\n1.2578\n0.1816\n0.2558\n0.2414\n0.0376\nGVF\nCW\n2.2915\n0.3140\n0.2217\n0.1586\n0.0\nE2E\nCW\n-0.1302\n0.1710\n0.9927\n0.3034\n0.5418\nTable 2: Comparison of GVF and MPC in CCW direction with 0.4 m/s target speed\nMethod\nReward\n/s ↑\nOffcenter-\nedness ↓\nAbsolute\nroad angle ↓\nNear out\nof lane ↓\nSpeed\ncomfort ↑\nSteering\ncomfort ↑\nRect-\nangle\nGVF\n2.6835\n0.1345\n0.1315\n0.0\n-0.0356\n-0.2251\nMPC\n0.9700\n0.5252\n0.1943\n0.2042\n-0.0832\n-1.2542\nOval\nGVF\n2.4046\n0.2754\n0.2125\n0.0145\n-0.0348\n-0.2191\nMPC\n0.8928\n0.5293\n0.1963\n0.227\n-0.1026\n-1.4119\nComp-\nlex\nGVF\n2.3501\n0.2221\n0.1817\n0.0\n-0.0341\n-0.2272\nMPC\n0.7172\n0.6407\n0.2131\n0.3894\n-0.0625\n-1.2133\n5\nConclusions\nAn investigation into the problem of driving a vehicle between lane markings reveals that a small\ncollection of out-of-policy predictions of the future at different time scales provides many beneﬁts\nin learning general policies with reinforcement learning with applications to real-world robots and\nautonomous driving. By testing our policies on roads not shown to the agent during training, including\ndamaged roads and roads with distracted markings, we ﬁnd that our novel predictive representation\nimproves learning speed, generalization in the real-world, and much smoother control decisions. In\naddition, it was found that learning a predictive representation of future lane centeredness and road\nangle over multiple time scales was required to achieve better performance in agreement with the\ntwo-point model described in [Salvucci and Gray, 2004] based on psychological studies performed\ndecades ago on understanding how humans drive.\nLearning to drive from images end-to-end is not new [Bojarski et al., 2016][Garimella et al.,\n2017][Chen and Huang, 2017][Sallab et al., 2017][Chi and Mu, 2017]. However, a serious challenge\nhas been ensuring the robustness of the controller and its generalization to roads and conditions not\nseen in the training data [Bojarski et al., 2016]. In this work, we show that policies that generalize\ncan be learned from data collected ofﬂine where the learning of the predictive representation can\nbe decoupled from the learning of the policy. Our real-world experiments demonstrated signiﬁcant\nimprovements in learning good policies with ofﬂine reinforcement learning methods like BCQ [Fuji-\nmoto et al., 2018]. We conclude that predictive representations can be used to represent the agent’s\nknowledge of the world, supporting claims in [White, 2015] and [Sutton et al., 2011], where our\ncontributions are in presenting ways to learn and exploit that predictive knowledge in autonomous\ndriving. The improvements offered in our algorithm of learning predictive representations from\ndata where the behavior policy is unknown has applications for applying reinforcement learning\nto autonomous driving in the real-world, especially as large volumes of data could potentially be\ncaptured by human drivers of all skill levels at scale that include valuable rare events like colli-\nTable 3: Effect of damaged lanes on GVF performance in CCW direction with 0.4 m/s target speed\nDamage\nReward\n/s ↑\nOffcenter-\nedness ↓\nAbsolute\nroad angle ↓\nNear out\nof lane ↓\nSpeed\ncomfort ↑\nSteering\ncomfort ↑\nRecta-\nngle\nNo\n2.6835\n0.1345\n0.1315\n0.0\n-0.0356\n-0.2251\nYes\n2.7407\n0.1358\n0.1351\n0.0\n-0.0383\n-0.2303\nOval\nNo\n2.4046\n0.2754\n0.2125\n0.0145\n-0.0348\n-0.2191\nYes\n2.0728\n0.3285\n0.2089\n0.0719\n-0.0334\n-0.2094\nComp-\nlex\nNo\n2.3501\n0.2221\n0.1817\n0.0\n-0.0341\n-0.2272\nYes\n2.1059\n0.3125\n0.2365\n0.0942\n-0.0437\n-0.2897\n8\nsions. Challenging problems however include evaluating predictive representations and learning\nthe predictive questions. Future work includes building a general framework to learn the predictive\nquestions themselves and investigate how shared representations offered by auxiliary tasks compare\nto direct representations. One obviously advantage of direct representations is that the representation\nis signiﬁcantly smaller (8 predictions + 2) compared with potentially hundreds of features that are\ntypically learned for shared representations.\n9\nReferences\nAndre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van\nHasselt, and David Silver.\nSuccessor features for transfer in reinforcement learning.\nIn I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\nand R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages\n4055–4065. Curran Associates,\nInc.,\n2017.\nURL http://papers.nips.cc/paper/\n6994-successor-features-for-transfer-in-reinforcement-learning.pdf.\nBing-Fei Wu, Tsu-Tian Lee, Hsin-Han Chang, Jhong-Jie Jiang, Cheng-Nan Lien, Tien-Yu Liao, and\nJau-Woei Perng. Gps navigation based autonomous driving system design for intelligent vehicles.\nIn 2007 IEEE International Conference on Systems, Man and Cybernetics, pages 3294–3299,\n2007.\nMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon\nGoyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,\nand Karol Zieba. End to end learning for self-driving cars. CoRR, abs/1604.07316, 2016. URL\nhttp://arxiv.org/abs/1604.07316.\nC. Chen, A. Seff, A. Kornhauser, and J. Xiao. Deepdriving: Learning affordance for direct perception\nin autonomous driving. In 2015 IEEE International Conference on Computer Vision (ICCV), pages\n2722–2730, Dec 2015. doi: 10.1109/ICCV.2015.312.\nZ. Chen and X. Huang. End-to-end learning for lane keeping of self-driving cars. In 2017 IEEE\nIntelligent Vehicles Symposium (IV), pages 1856–1860, June 2017. doi: 10.1109/IVS.2017.\n7995975.\nLu Chi and Yadong Mu. Deep steering: Learning end-to-end driving model from spatial and temporal\nvisual cues. arXiv preprint arXiv:1708.03798, 2017.\nA. Clark. Whatever next? predictive brains, situated agents, and the future of cognitive science.\nBehavioral and Brain Science, 36(3):181–204, 2013. doi: 10.1017/S0140525X12000477.\nSean Scheideman Daniel Graves, Kasra Rezaee. Perception as prediction using general value\nfunctions in autonomous driving applications. In Proceedings of the IEEE/RSJ International\nConference on Intelligent Robots and Systems, IROS 2019, 2019.\nAnn L. Edwards, Michael R. Dawson, Jacqueline S. Hebert, Craig Sherstan, Richard S. Sutton,\nK. Ming Chan, and Patrick M. Pilarski. Application of real-time machine learning to myoelectric\nprosthesis control: A case series in adaptive switching. Prosthetics and orthotics international, 40\n(5):573–581, 2016.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without\nexploration. arXiv preprint arXiv:1812.02900, 2018.\nG. Garimella, J. Funke, C. Wang, and M. Kobilarov. Neural network modeling for steering control of\nan autonomous vehicle. In 2017 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS), pages 2609–2615, Sep. 2017. doi: 10.1109/IROS.2017.8206084.\nSina Ghiassian, Andrew Patterson, Martha White, Richard S. Sutton, and Adam White. Online off-\npolicy prediction. CoRR, abs/1811.02597, 2018. URL http://arxiv.org/abs/1811.02597.\nJohannes Günther, Patrick M. Pilarski, Gerhard Helfrich, Hao Shen, and Klaus Diepold. Intelligent\nlaser welding through representation, prediction, and control learning: An architecture with deep\nneural networks and reinforcement learning. Mechatronics, 34:1 – 11, 2016. ISSN 0957-4158.\ndoi: https://doi.org/10.1016/j.mechatronics.2015.09.004. URL http://www.sciencedirect.\ncom/science/article/pii/S0957415815001555. System-Integrated Intelligence: New Chal-\nlenges for Product and Production Engineering.\nMax Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z. Leibo, David\nSilver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. CoRR,\nabs/1611.05397, 2016. URL http://dblp.uni-trier.de/db/journals/corr/corr1611.\nhtml#JaderbergMCSLSK16.\n10\nS. Kohlbrecher, J. Meyer, O. von Stryk, and U. Klingauf. A ﬂexible and scalable slam system with\nfull 3d motion estimation. In Proc. IEEE International Symposium on Safety, Security and Rescue\nRobotics (SSRR). IEEE, November 2011.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: tutorial,\nreview and perspectives on open problems. CoRR, abs/2005.01643, 2020. URL http://arxiv.\norg/abs/2005.01643.\nTimothy Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David\nSilver, and Daan Wierstra. Continuous control with deep reinforcement learning. ICLR, 2016.\nMichael L. Littman and Richard S Sutton. Predictive representations of state. In T. G. Diet-\nterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing\nSystems 14, pages 1555–1561. MIT Press, 2002.\nURL http://papers.nips.cc/paper/\n1983-predictive-representations-of-state.pdf.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-\nstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602,\n2013. URL http://arxiv.org/abs/1312.5602.\nJoseph Modayil, Adam White, and Richard S. Sutton. Multi-timescale nexting in a reinforcement\nlearning robot. In Tom Ziemke, Christian Balkenius, and John Hallam, editors, From Animals\nto Animats 12, pages 299–309, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg. ISBN\n978-3-642-33093-3.\nBrian Paden, Michal Cáp, Sze Zheng Yong, Dmitry S. Yershov, and Emilio Frazzoli. A survey of\nmotion planning and control techniques for self-driving urban vehicles. CoRR, abs/1604.07446,\n2016. URL http://arxiv.org/abs/1604.07446.\nEddie J. Rafols, Mark B. Ring, Richard S. Sutton, and Brian Tanner. Using predictive representations\nto improve generalization in reinforcement learning. In Proceedings of the 19th International Joint\nConference on Artiﬁcial Intelligence, IJCAI’05, pages 835–840, San Francisco, CA, USA, 2005.\nMorgan Kaufmann Publishers Inc. URL http://dl.acm.org/citation.cfm?id=1642293.\n1642427.\nEvan M. Russek, Ida Momennejad, Matthew M. Botvinick, Samuel J. Gershman, and Nathaniel D.\nDaw. Predictive representations can link model-based reinforcement learning to model-free\nmechanisms. PLOS Computational Biology, 13(9):1–35, 09 2017. doi: 10.1371/journal.pcbi.\n1005768. URL https://doi.org/10.1371/journal.pcbi.1005768.\nAhmad Sallab, Mohammed Abdou, Etienne Perot, and Senthil Yogamani. Deep reinforcement\nlearning framework for autonomous driving. Electronic Imaging, 2017:70–76, 01 2017. doi:\n10.2352/ISSN.2470-1173.2017.19.AVM-023.\nDario D Salvucci and Rob Gray. A two-point visual control model of steering. Perception, 33\n(10):1233–1248, 2004. doi: 10.1068/p5343. URL https://doi.org/10.1068/p5343. PMID:\n15693668.\nTom Schaul and Mark Ring. Better generalization with forecasts. In Proceedings of the Twenty-\nThird International Joint Conference on Artiﬁcial Intelligence, IJCAI ’13, pages 1656–1662.\nAAAI Press, 2013. ISBN 978-1-57735-633-2. URL http://dl.acm.org/citation.cfm?id=\n2540128.2540366.\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In\nICLR, Puerto Rico, 2016.\nMatthew Schlegel, Wesley Chung, Daniel Graves Jian Qian, and Martha White. Importance resam-\npling off-policy prediction. CoRR, abs/1906.04328, 2019. URL http://arxiv.org/abs/1906.\n04328.\nDavid Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.\nDeterministic policy gradient algorithms. In Proceedings of the 31st International Conference on\nInternational Conference on Machine Learning - Volume 32, ICML’14, pages I–387–I–395, 2014.\n11\nRichard Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick Pilarski, Adam White, and\nDoina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised\nsensorimotor interaction. In The 10th International Conference on Autonomous Agents and\nMultiagent Systems - Volume 2, AAMAS ’11, pages 761–768, 2011.\nRichard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,\n3(1):9–44, Aug 1988. ISSN 1573-0565. doi: 10.1023/A:1022633531479. URL https://doi.\norg/10.1023/A:1022633531479.\nSebastian Thrun. Probabilistic robotics. Communications of the ACM, 45(3):52–57, 2002.\nGeorge Uhlenbeck and Leonard Ornstein. On the theory of the brownian motion. Physical review,\n36:823–841, 1930.\nHarm Van Seijen,\nMehdi Fatemi,\nJoshua Romoff,\nRomain Laroche,\nTavian Barnes,\nand Jeffrey Tsang.\nHybrid reward architecture for reinforcement learning.\nIn\nI. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\nR. Garnett,\neditors,\nAdvances in Neural Information Processing Systems 30,\npages\n5392–5402. Curran Associates,\nInc.,\n2017.\nURL http://papers.nips.cc/paper/\n7123-hybrid-reward-architecture-for-reinforcement-learning.pdf.\nAdam White. Developing a predictive approach to knowledge. PhD thesis, University of Alberta,\n2015.\nBernhard Wymann, Eric Espie, Christophe Guionneau, Christos Dimitrakakis, Remi Coulom, and\nAndrew Sumner. Torcs, the open racing car simulator, v1.3.5, 2013. URL http://www.torcs.\norg/.\n12\n6\nAppendix\n6.1\nPredictive Learning Algorithms\nThe algorithm used for learning GVF predictions online through interaction with an environment is\ngiven in Algorithm 1.\nAlgorithm 1 Online Out-of-policy GVF training algorithm with unknown µ(a|s)\n1: Initialize ˆvτ, g(a, s), η(a|s), and replay memory D\n2: Observe initial state s0\n3: for t = 0,T do\n4:\nSample action at from unknown µ(at|st)\n5:\nExecute action at and observe state st+1\n6:\nCompute cumulant ct+1 = c(st, at, st+1)\n7:\nCompute continuation γt+1 = γ(st, at, st+1)\n8:\nEstimate behavior density value ˆµ(at|st) =\ng(at,st)\n1−g(at,st)η(at|st)\n9:\nEstimate importance sampling ratio ρt = τ(at|st)\nˆµ(at|st)\n10:\nStore transition (st, at, ct+1, γt+1, st+1, ρt) in D\n11:\nCompute average importance sampling ratio in replay buffer D of size n with ¯ρ = 1\nn\nPn\nj=1 ρj\n12:\nSample random minibatch A of transitions (si, ai, ci+1, γi+1, si+1) from D according to\nprobability\nρi\nPn\nj=1 ρj\n13:\nCompute yi = ci+1 + γi+1ˆvτ(si+1; θ) for minibatch A\n14:\nUpdate parameters θ using gradient descent on (2) with gradient (5) over the minibatch A\n15:\nSample random minibatch B of state action pairs (si, ai) from D according to a uniform\nprobability and assign label z = 1 to each pair\n16:\nRandomly select half the samples in the minibatch B replacing the action with at ∼η(a|s)\nand label with z = 0 and storing the updated samples in ˆB\n17:\nUpdate behavior discriminator g(a, s) with labels z in the modiﬁed minibatch ¯B using binary\ncross-entropy loss\nWith minor modiﬁcations, an ofﬂine version of the algorithm can be derived. This algorithm learns\nby reading the data in sequence and populating a replay buffer just as it would in online learning;\nthe only difference is that the ofﬂine algorithm returns the action taken in the data. This allows the\nsame algorithm and code for learning GVF predictions to be used in either online or ofﬂine learning\nsettings.\n7\nTORCS Experiments\nTORCS is a racing simulator used for learning to drive. All opponent vehicles were removed for\nthese experiments as well as roads that were sloped. The goal of the agent is to maximize the future\naccumulation of the following reward: rt = 0.0002vt(cos βt + |αt|) where vt is the speed of the\nvehicle in km/h, βt is the angle between the road direction and the vehicle direction, and αt is the\ncurrent lane centeredness. Termination occurs when either the agent leaves the lane or the maximum\nnumber of steps has been reached (1200 steps = 120 seconds) triggering a reset of the environment.\nUpon reset, a priority sampling method is used during training to select the next road to train on. The\nprobability of sampling road i during a reset is given by\ne−ni\nκ\nPN\nj=1 e−\nnj\nκ\n(8)\nwhere ni is the number of steps that the agent was able to achieve the last time the road was sampled\nand κ controls the spread of the distribution. A value of κ = 1\nN\nPN\nj=1 nj was found to perform well.\nThe initial probabilities are equal for all roads. This improved the efﬁciency in learning for all learned\nmethods.\n13\nAlgorithm 2 Ofﬂine Out-of-policy GVF training algorithm with unknown µ(a|s)\n1: Initialize ˆvτ, g(a, s), η(a|s), and replay memory D,\n2: Obtain the ﬁrst state in the data ﬁle s0\n3: for t = 0,T do\n4:\nObtain action at recorded in the data ﬁle that sampled from an unknown µ(at|st)\n5:\nObtain next state st+1 from the data ﬁle\n6:\nCompute cumulant ct+1 = c(st, at, st+1)\n7:\nCompute continuation γt+1 = γ(st, at, st+1)\n8:\nEstimate behavior density value ˆµ(at|st) =\ng(at,st)\n1−g(at,st)η(at|st)\n9:\nEstimate importance sampling ratio ρt = τ(at|st)\nˆµ(at|st)\n10:\nStore transition (st, at, ct+1, γt+1, st+1, ρt) in D\n11:\nCompute average importance sampling ratio in replay buffer D of size n with ¯ρ = 1\nn\nPn\nj=1 ρj\n12:\nSample random minibatch A of transitions (si, ai, ci+1, γi+1, si+1) from D according to\nprobability\nρi\nPn\nj=1 ρj\n13:\nCompute yi = ci+1 + γi+1ˆvτ(si+1; θ) for minibatch A\n14:\nUpdate parameters θ using gradient descent on (2) with gradient (5) over the minibatch A\n15:\nSample random minibatch B of state action pairs (si, ai) from D according to a uniform\nprobability and assign label z = 1 to each pair\n16:\nRandomly select half the samples in the minibatch B replacing the action with at ∼η(a|s)\nand label with z = 0 and storing the updated samples in ˆB\n17:\nUpdate behavior discriminator g(a, s) with labels z in the modiﬁed minibatch ¯B using binary\ncross-entropy loss\nThe TORCS environment was modiﬁed to provide higher resolution images in grayscale rather than\nRGB with most of the image above the horizon cropped out of the image. The grayscale images were\n128 pixels wide by 64 pixels high. This allowed the agent to see more detail farther away which is\nvery helpful in making long term predictions and is beneﬁcial to both policy gradient methods and\npredictive learning.\n7.1\nTraining\nTwo main learning algorithms are compared, along with their variants: our proposed GVF-DDPG\n(general value function deep deterministic policy gradient) and DDPG. The parameters used to train\nthe methods will be described in more detail here.\n7.1.1\nGVF-DDPG Training\nThe predictive learning approach presented in this paper is called GVF-DDPG (general value function\ndeep deterministic policy gradient). Exploration followed the same approach as [Lillicrap et al.,\n2016] where an Ornstein Uhlenbeck process [Uhlenbeck and Ornstein, 1930] was used to explore\nthe road; the parameters of the process (θ = 1.0, σ = 0.1, dt = 0.01) were tuned to provide a\ngradual wandering behavior on the road without excessive oscillations in the action. This improved\nthe learning of the off-policy predictions for GVF-DDPG since the behavior policy µ(a|s) was closer\nto the target policy τ(a|s) of the predictions.\nThe GVF-DDPG approach learned 8 predictions: 4 predictions of lane centeredness α, and 4\npredictions of road angle β. Each of the 5 predictions had different values of γ for different temporal\nhorizons: 0.0, 0.5, 0.9, 0.95, 0.97. This allowed the agent to predict how the road will turn in the\nfuture providing the necessary look ahead information for the agent to control effectively. The GVF\npredictors shared the same deep convolutional neural network where the convolutional layers were\nidentical to the architecture in [Bojarski et al., 2016] followed by three fully connected layers of\n512, 384 and 8 outputs, respectively. The behavior estimator µ(a|s) also used a very similar neural\nnetwork with identical convolutional layers followed by three fully connected layers of 512, 256, and\n1 output, respectively. The GVF predictors and behavior estimator were both given the current image,\nprevious image, current speed and the last action taken.\n14\nThe policy network of the DDPG agent was a small neural network with two branches for each action\n– steering angle and target speed – where each branch received the 8 predictions and last action as\ninput that fed into three fully connected layers of 256, 128, and 1 output. ReLU activation was used\nfor the hidden layers while linear activation was used on the output layer followed by a clip to the\nrange [−1.0, 1.0]. The output target speed passed through a linear transformation to change the range\nof values from [−1.0, 1.0] to [0.5, 1.0]. The structure of the action-value network was the same as the\npolicy network, except it received the 8 predictions, last action, and current action as input as well as\nused a linear activation on the output layer. Additionally, the steering and target speed input into both\npolicy and action-value networks were normalized to range [−1.0, 1.0].\nA replay buffer of size 100000 was used with a warmup of 10000 samples. In order to not bias the\nreplay buffer, the last layers in each branch of the policy network were initialized with a uniform\ndistribution [−1e−3, 1e−3] for the weight and 0 for the bias. The learning rates for the policy network,\naction-value network, predictors and behavior policy network were 1e−6, 1e−4, 1e−4, and 1e−4\nrespectively. Target networks [Lillicrap et al., 2016] were used for the action-value and policy\nnetworks with τ = 0.001 in order to make the bootstrapped prediction of the action-values more\nstable. The reward for was scaled by 0.0002 to scale the action-values to fall within the range\n[−1.0, 1.0].\n7.1.2\nBaseline DDPG Training\nThe two DDPG (deep deterministic policy gradient) [Lillicrap et al., 2016] baselines were trained\nnearly identically where the only difference was the information provided in the observation. The\nﬁrst method called DDPG-Image is a vision-based approach where the image, current speed, and last\naction are provided to the agent; the only information available to the agent about the road is supplied\nvia images. The second agent called DDPG-LowDim added lane centeredness α and road angle β to\nthe observation used by DDPG-Image. It should be noted that DDG-LowDim was the only learned\napproach that had access to α and β during testing, whereas GVF-DDPG and DDPG-Image only\nneed the information to compute the cumulant and reward during training.\nTraining setup for DDPG was largely the same as GVF-DDPG. An Ornstein Uhlenbeck process\n[Uhlenbeck and Ornstein, 1930] was used to explore the road (θ = 1.0, σ = 0.1, and dt = 0.01).\nExperimentally, it was found that the exploration parameters did not affect the learning performance\nof DDPG that much. Target networks were used with τ = 0.001. The learning rates of the action-\nvalue and policy networks were the same as those of GVF-DDPG. The architecture of the behavior\nnetwork for GVF-DDPG was used for action-value and policy network of DDPG, except that two\nfully connected branches with layer sizes of 512, 256 and 1 output were used for each action of the\npolicy network. As with GVF-DDPG, linear activation followed by a clip to the range [−1.0, 1.0]\nwas used for each output of the policy network and ReLU activation was for all other layers. The\nlinear transformation of the target speed and normalization of the input action was accomplished in\nthe same way as GVF-DDPG.\nThe low dimensional state information was fed through a separate branch of 12 neurons that were\nthen merged with the input into the fully connected layer of 512 neurons. This method of merging\npresents challenges due to the potential mismatch of the statistical properties of the branches but that\ndid not seem to present a signiﬁcant challenge in learning. Future work would be to ﬁnd better ways\nto bridge these two different pieces of information.\n7.2\nExperimental Results\nThe experimental results were averaged over 5 runs and mean and standard deviations plotted based\non performance measured on the test roads during training. The learning curves for the action-value\nnetworks for each of the DDPG agents, as well as learning curves for GVF predictions and behavior\nestimation for GVF-DDPG are shown in Figure 5. The average episode length is shown in Figure\n6. The average lane centeredness and road angle during each episode are plotted in Figures 7 and 8.\nWe can see how the GVF-DDPG with predictions over multiple time scales is able to maintain lane\ncenteredness and road angle better than GVF-DDPG with myopic prediction (γ = 0.0) and future\npredictions with only γ = 0.95. The average lane centeredness and road angle is not substancially\ndifferent though amoung the learned methods; however, DDPG-Image struggles a bit largely due to\ninstability in learning as the high variance is due to some failed runs. Figure 9 shows the standard\ndeviation in the change in the target speed action at each time step across an episode on each test\n15\nroad; this measures the jerkiness of the speed controller. GVF-DDPG and DDPG-LowDim are both\nable to control speed comfortably. Finally, Figure 10 shows the lane centeredness on all six of the\ntest roads during a ﬁnal evaluation after training was completed. All six roads in the test set were\nchallenging but the test roads a-speedway, alpine-2, and wheel-2 were especially challenging because\nthe image of the roads were too different from the training roads. Nevertheless, on the dirt-4, evo-2-r,\nand spring roads, the lane centeredness of the the methods was quite good for all learned methods\nexcept for DDPG-Image. Note that while DDPG-LowDim performs well in learning to steer and\ncontrol speed with lane centeredness and road angle, it required that information to learn to steer\nthe vehicle which may be expensive or prohibitive to obtain in all situations such as in GPS-denied\nlocations or locations where there is no map or it is out of date.\n(a) Action-Value\n(b) Predictors\n(c) Behavior\nFigure 5: Learning curves for (a) Q-values of the DDPG agents, (b) mean squared TD (temporal\ndifference) errors of the GVF predictors, and (c) MSE of the behavior model estimator\n16\nFigure 6: Mean episode length during training for dirt-dirt-4, evo-evo-2 and road-spring\n17\nFigure 7: Mean lane centeredness during training for dirt-dirt-4, evo-evo-2 and road-spring\n18\nFigure 8: Mean road angle during training for dirt-dirt-4, evo-evo-2 and road-spring\n19\nFigure 9: Standard deviation of the change in target speed action during training for dirt-dirt-4,\nevo-evo-2 and road-spring\n20\nFigure 10: The lane centeredness position on the (a) alpine-2, (b) evo-2-r, (c) dirt-4, (d) wheel-2, (e)\nspring, and (f) a-speedway roads in TORCS.\n21\n8\nJackal Robot Experiments\nThe Jackal robot is equipped with a 5MP camera using a wide angle lens, an IMU sensor and an\nindoor Hokuyo UTM-30LX LIDAR sensor with a 270◦scanning range and 0.1 to 10 meters scanning\ndistance. The objective is to drive the robot using a camera in the center of a lane marked with\ntape using only data collected in the real-world. This is a challenging problem for reinforcement\nlearning algorithms as RL, especially model-free, usually takes a prohibitive amount of time and\nsamples to learn. Training directly from real world experience requires addressing multiple issues\nsuch as minimizing wear and tear on the robot over long period of interaction, and the need of human\nsupervision during training in order to prevent robot crashes and recharge the battery.\nThere are two learned controllers, called GVF and E2E respectively, and one classical baseline called\nMPC (model predictive control). The learned controllers outputs a steering angle asteer\nt\nand target\nspeed aspeed\nt\nbased on the image taken by the camera in order drive centered in a closed loop track\non a carpeted ﬂoor. The MPC outputs a steering angle asteer\nt\nand target speed aspeed\nt\nbased on\nlocalization of the robot in a map constructed of the environment to follow a sequence of waypoints.\nLocalization, map and waypoints are needed to train the GVF controller; however, this information is\nno longer used during testing.\n8.1\nTraining and testing environment\nThe environment used for collecting data and evaluating the agents included two types of carpet ﬂoor\n- each with a different amount of friction. The evaluation roads were done on one carpet only which\nwas included in only about 20% of the training data; the rest of the training data was on the other\ntype of carpet to provide a generalization challenge for our learned controllers. The friction was quite\nhigh on both carpets and it caused the agent to shake the camera violently while turning since the\nrobot employs differential steering; tape on the wheels helped reduce the friction a bit. Nevertheless,\nlocalization techniques using wheel odometry was deemed unsuitable and LIDAR-based localization\nwas used instead. LIDAR localization was not highly accurate but was sufﬁcient; preliminary tests\nshowed that it was repeatable to within roughly 5 centimeters.\nNine closed loop roads were created by placing left and right lanes markings on the carpeted ﬂoor\nseparated to form a consistent lane width of roughly 76 centimeters for all the roads. 6 of the roads\nwere selected for collecting data to train our learned agents. Data was collected in both directions.\nThe remaining 3 roads were reserved for testing the performance of the policies. The poses and\norientations of a sequence of waypoints were established to denote the center of lane which is the\ndesired path of the agent; this was used to train our agents on the training roads and evaluate them on\nthe test roads. The center waypoints were collected by an expert manually and carefully navigating\nthe Jackal robot on the road in the approximate center of lane; while this was inaccurate, it was\nnot found to harm learning since the GVF approach was able to generalize and learn features to\ndrive the vehicle centered in the lane. The LIDAR-based localization produced poses periodically\nto form the center waypoints; these were cleaned up by removing overlapping waypoints to form a\nclosed loop path. However, it did not produce poses at a consistent sampling frequency and thus a\nlinear interpolation method was used to ﬁll in the gaps and provide localization and center waypoint\ninformation at every time step. The purpose of the center waypoints was to compute the road angle\nand lane centeredness of the robot in the road at any given time which is needed to train the GVF\npredictions and evaluate all of our controllers.\nThe center waypoints for the training and testing roads are depicted in Figure 11 and Figure 12,\nrespectively. The ﬁrst three training roads were simple geometric shapes while the other three were a\nbit more complex. The ﬁrst test road was the most similar to the training data where the outer edge of\nthe four corners were rounded. The second test road was an oval shape to evaluate how well the agent\nmaintained a continuous turn. The third test road was a complex shape with multiple sudden turns\nnot in the training data. This tests generalization to new roads. All methods were evaluated at 0.25\nm/s and 0.4 m/s maximum speeds and clock-wise (CW) and counter clock-wise (CCW) directions.\nIn order to test robustness, some test roads were altered by degrading or damaging. An example of an\nimage received from the robot with and without damage to the lane markers is shown in Figure 13.\n22\nFigure 11: Six roads for training. The second row shows more complex road structure. The rectangle\nroad has rectangular edges at all corners.\nFigure 12: Three roads for testing. Left to right: (1) rectangle with rounded corners; (2) oval; (3)\ncomplex.\nFigure 13: Input images of normal lane markings (top row) and damaged lane markers (bottom row).\n23\n8.2\nData collection\nBoth learned agents – GVF and E2E – were trained with batch reinforcement learning [Fujimoto\net al., 2018] where the GVF method learned a predictive representation. Rosbags were collected\nwith the Jackal robot each containing around 30 minutes to 2 hours of data for a total of 40 hours\nof data containing approximately 1.5 million images. The camera image, localization pose, IMU\nmeasurement, and action taken by the controller was recorded at each time step. The Jackal robot\nwas controlled using a random walk controller that was conﬁned to the road area to provide sufﬁcient\nand safe exploration. The map was created with Hector SLAM [Kohlbrecher et al., 2011] and the\nlocalization produced by Adaptive Monte-Carlo Localization Thrun [2002]. This localization method\nwas prone to a signiﬁcant amount of noise.\nThe random walk controller was based on a pure pursuit controller, where action taken at each time\nstep is deﬁned by\nasteer\nt\n= clip(angle(pt, p∗\nk(t)) −θz\nt , −π/2, π/2)\naspeed\nt\n= clip(v∗\nk(t), 0.2, 0.5)\n(9)\nwhere θz\nt and pt were the yaw angle and the 2-dimensional position of the robot in the real world\n(obtained from localization), p∗\nk(t) and v∗\nk(t) were the target position and linear velocity at time t,\nclip(x, MinVal, MaxVal) is the clip function that operates in scalar and vector element-wise, and\nangle(pt, p∗\nk(t)) is the function that returns the yaw angle in the real world of a vector that points from\npt to p∗\nk(t). The target position p∗\nk(t) and linear velocity v∗\nk(t) were encapsulated in the next target\npose at index k(t) in the sequence of target poses:\nk(1) = 1\nk(t + 1) =\n(\nk(t) + 1 if ||pt −p∗\nk(t)||2 < 0.025\nk(t) otherwise\n(10)\nThus, the robot advanced to the next target position and linear velocity in the target pose sequence\nonce it arrived within 2.5 centimeters of the current target position. In order to provide efﬁcient and\nsafe exploration that can be conﬁned to the road area, the target position p∗\nj was based on the position\n˜pj of the center waypoints collected earlier with some noise added:\np∗\nj = ˜pj%N + εp\nj\nv∗\nj =\n\u001av∗\nj−1 + εv\nj if j > 1\n0.35 if j = 1\n(11)\nwhere N is the number of points that deﬁne the center waypoints of the closed loop road. εp\nj and εv\nj\nwere the noises added at each time step:\nεp\nj =\n\u001aclip(εp\nj−1 + N(0, 0.02 ∗1), −0.3, 0.3) if j > 1\n[0, 0]⊺if j = 1\nεv\nj = N(0, 0.02)\n(12)\nThe noises for the poses were clipped so that the robot would not explore too far outside the road\narea.\nThe rosbags were processed to synchronize the sensor data streams at a ﬁxed sample frequency of\n10Hz and compute the lane centeredness αt, road angle βt, and speed vt of the robot at each time\nstep:\nνt = knn(pt, St)\nαt = clip(||pt −νt||2\nH\n, −1.0, 1.0)\nβt = clip(angle(pt, νt) −θz\nt , −π/2, π/2)\n(13)\nwhere knn(x, S) returns ν as the closest point to x in S using k-nearest neighbor and H = 38\ncentimeters as the half lane width. St is a pruned set of center waypoints where St = {˜pt} for all\n24\nroads, except for the ﬁgure 8 road in the lower right of Figure 11 where St was based on a sliding\nwindow:\nSt =\n\u001a{˜pt} if j = 1\n{˜pt=It−1−w→It−1+w} if j > 1\n(14)\nwhere Ij−1 is the index of νt−1 in St−1 at the previous time step and w = 10 is the size of the sliding\nwindow. Negative indices are wrapped to the beginning of the road. The speed was estimated using\nthe change in position over a single time step which was quite noisy but more reliable than speed\nreturned from the robot’s odometry. Due to computation constraints on the robot, the localization\nmessages were output at less than 10Hz; thus, a linear interpolation was used to ﬁll in missing poses\nand orientations in the data and synchronize the data streams.\nImages from camera with original size 1920 × 1080 were cropped to 960 × 540 region in the center\nand then additionally top-cropped by 60. The images were then downsampled by a factor of 8 in\nboth spatial dimensions to give a ﬁnal size of 120 × 60 and then converted to gray scale. To improve\ngeneralization in deep learning and balance the left-right biases in the data, augmented data was\ncreated with horizontally ﬂipped images along with the corresponding signs of the lane centeredness\nαt, road angle βt, and steering action asteer\nt\n) ﬂipped.\n8.3\nGVF-BCQ Training\nThe predictive neural network used was identical to the one used in the GVF-DDPG TORCS\nexperiments. The ofﬂine version of predictive learning, cf. algorithm 2, was used to train the GVFs.\nThe transitions in the data were loaded into a replay buffer in the same order that the transitions\nwere observed in the rosbag where mini-batches of size 128 where sampled from the growing replay\nbuffer and used to update the GVFs. The GVFs were updated for 5 million steps followed by BCQ\nfor an additional 5 million steps. The order that the rosbags were loaded into the replay buffer was\nrandomized. The replay buffer had a maximum capacity of 0.5 million samples; once the replay\nbuffer was ﬁlled, the oldest samples were removed. Training began once the replay buffer reached 0.1\nmillion samples. While a better approach would have been to sample mini-batches from the entire\ndata set from the beginning, our approach was found to be effective and required minimal changes to\nthe data loader.\nEstimating the behavior distribution for the GVF predictions was done in the same manner as with\nTORCS. η(a|s) was a uniform distribution deﬁned on the interval [−π/2, π/2] for the steering action\nand a uniform distribution deﬁned on the interval [0, 1] for the target speed action. The BCQ VAE\nmodel was a fully connected network receiving (da + ds) inputs that fed into a hidden layer of 256\nand into an encoder output layer dz containing the latent variables. The decoder of the VAE received\nthe encoded latent variables dz that fed into a hidden layer of 256 and output the reconstructed action\nof size da. The actor network was fully connected and received a vector of size (da + ds), where s\nwas the predictive representation of size ds and the action at generated by the VAE, that fed into a\nhidden layer of size 256 and output a perturbation to the action of dimension da. The critic received\na vector of size (da + ds) that fed into a hidden layer of size 256 and output a scale representing\nthe value of the state and action. The action dimension was da = 2, the predictive representation\nwas of dimension ds = 11(consisting 8 predictions + last steering action + last target speed action\n+ last robot speed) and the latent vector dimension was dz = 4 which was a Normal distribution\nparameterised by mean and log standard deviation. All networks used ReLU activation for the hidden\nlayers and linear activation for the outputs. Just as with BCQ, the actor outputs an action of dimension\nda dimension that is a perturbation to the action reconstructed through the VAE; the two actions are\ncombined to produce the ﬁnal action of size da such that the batch-constrained property is satisﬁed.\nThe action output from the actor and VAE were clipped to [−π/2, π/2] for steering and [0.1, 0.6] for\nthe target speed. The weight of the KL divergence loss was 0.5. The learning rate was 10−4 for both\nGVF and BCQ model training. Figure 14 shows the training curves for the predictive representation\n(GVFs) including the temporal-difference loss, behavior model loss and mean importance sampling\nratio.\n8.4\nEnd-to-end BCQ Baseline Training\nNearly the same training setup used for the GVF method was applied to the E2E method. The\nhyperparameters, training settings, activation functions for the output and action clipping are exactly\n25\nFigure 14: TD loss, behavior loss and mean importance sampling ratio in the buffer over training\nsteps. Red vertical dash line is the point when the buffer is full.\nthe same as GVF, except for the network architectures. The encoding branch of the VAE, the actor\nand critic networks use the same architecture as the prediction model of GVF-DDPG. Last steering,\nlast target speed action and last robot speed are merged into ﬂattened output from the convolutional\nlayers the same way as in DDPG-LowDim.\nFor a fair comparison, E2E with BCQ was trained for 10 millions update steps - the same number\nas GVF and BCQ combined in our GVF method. The agent was then tested on the rectangle test\nroad and it was found that E2E performed very poorly. The agent was too slow reaching an average\nspeed of about 0.18 m/s whereas the GVF method was able to reach double that speed. In addition,\nE2E steered very poorly and was often not centered in the lane; unlike the GVF method which was\nobserved to be quite robust, the E2E method sometimes drove out of the lane where an emergency\nstop was needed to prevent collision. For this reason, E2E was only compared to GVF on the rectangle\ntest road; and we focused on comparisons against the MPC controller which was more robust than\nE2E. A detailed evaluation of E2E is shown in Figure 18 and Table 1.\nFigure 15: Distribution of lane centeredness, road angle, speed and action distribution of GVF and\nE2E on the rectangle test track at 0.4 speed, counterclockwise direction.\n8.5\nMPC Baseline\nAn MPC baseline using standard ros nodes for the Jackal robot were used for controlling the Jackal\nrobot. The baseline was tuned for 0.4 m/s; however, it was challenging to achieve good performance\ndue to limited computation power for the look ahead and inaccurate modeling of the steering\ncharacteristics on carpet ﬂoors. The best performance was achieved for 0.25 m/s but signiﬁcant\noscillation was observed for 0.4 m/s that was challenging to completely eliminate. The center\nwaypoints provided as input to the MPC controller were processed so that the minimum distance\nbetween two consecutive waypoints was 2.5cm; the waypoints were then downsampled by a factor of\n16 in order to increase their separation and increase the look ahead distance; different downsampling\nfactors was tested but oscillation was never completely eliminated. The MPC had an optimization\nwindow of 5 steps into the future. This ensured the MPC look ahead was far enough into the future\nwhile also maintaining a reasonable computation burden needed for real time control.\n26\n8.6\nTest Results\nA comparison of GVF and E2E is given in Tables 8 and 9. This section provides a detailed comparison\nof GVF and MPC methods at different speeds and directions. For the GVF method, the agent was\ntrained to drive as fast as possible (i.e. no target speed was given) and the speed command was clipped\nat the maximum target speed. In our evaluation at 0.25 m/s in the counterclockwise direction, the\ntwo controllers performed similarly; the primary differences in the methods were observed at higher\nspeeds demonstrating that the GVF method was able to scale better than the MPC. The controllers\nstarted at the same position and heading angle and they were allowed to run for exactly 300 seconds.\nThe agents were evaluated based on the following criteria:\n• Reward per second:\n1\nN\nPN\nt=1 rt\n• Average speed:\n1\nN\nPN\nt=1 vt\n• Average absolute lane centeredness:\n1\nN\nPN\nt=1 |αt|\n• Average absolute road angle:\n1\nN\nPN\nt=1 |βt|\n• Near out of lane3:\n1\nN\nPN\nt=1 1|αt|>0.754.\n• First Order Comfort5: −\n1\nN−1\nPN−1\nt=1 |at+1 −at|\n• Second Order Comfort6: −\n1\nN−2\nPN−2\nt=1 |(at+2 −at+1) −(at+1 −at)|\nIn order to provide more insight into the performance of the controllers, we also investigated the\ndistributions of αt, βt, vt and at. Details evaluations are shown in Tables 4 and 5. Experiments\nare named according to the method used, the selected target speed and the direction of the road\nloop (i.e. counter-clock-wise versus clock-wise). For example, GVF-0.4-CCW points to the test\nof the GVF controller with 0.4 m/s target speed in the counter-clock-wise direction. Additionally,\nexperiments on roads with damaged lane markings are denoted with sufﬁx -D. It can be seen from\nTables 4 and 5 that the GVF method beat the MPC in reward and was better on all tracks at both\nspeed values without access to localization information during testing. The reason for this can be\nexplained by looking at the average lane centeredness of the agent. The MPC performed as well as\nthe GVF method while maintaining good speed; however, it fails at keeping the vehicle in the center\nof the lane. The reason may be due to a number of different factors including possible inaccuracies\nin the MPC forward model resulting from the friction between the wheels and the carpet in the test\nruns, especially at higher speeds. The MPC suffered from many near out of lane events and had\ntrouble staying within the lane markings of the oval road. The GVF method was better at controlling\nsteering, leading to much higher average reward even though it had lower average speed at 0.4 m/s\nmax speed. Additionally, the GVF method was much better in achieving smooth control. These points\nare reﬂected in Figure 16 on the rectangle test track where the MPC lane centeredness distribution is\nskewed to one side and its steering action distribution has two modes that are far from zero while the\nGVF method distributions are more concentrated around zero. Finally, the GVF method generalized\nwell to damaged lane markings and distractions in the visual images as shown in the similar scores\nand similar distributions in Figure 17 and Table 6 and 7.\n3ratio of time steps where the agent’s absolute lane centeredness is greater than 0.75\n4Where 1 is the indicator function.\n5First order comfort is the negative absolute change of action taken by the agent in one time step. Higher\ncomfort scores are better. Both steering and speed actions considered separately.\n6Second order comfort is the negative absolute change of the ﬁrst order comfort in one time step. Higher\ncomfort scores are better. Both steering and speed actions considered separately.\n7Note that measured vehicle speed might not be equal to speed action from the agent due to physical\nconstraints of the environment and noises in measurement.\n27\nTable 4: Comparison of GVF method and MPC on all the test roads at different speeds and directions\nExperiment\nReward\nper\nsecond ↑\nAverage\nspeed ↑\nAverage\noffcentered\n-ness ↓\n(normalized)\nAverage\nabsolute\nroad angle ↓\nNear\nout\nof lane ↓\nRectangle\nshape\nGVF-0.4-CCW\n2.6835\n0.3205\n0.1345\n0.1315\n0.0\nMPC-0.4-CCW\n0.9700\n0.3833\n0.5252\n0.1943\n0.2042\nGVF-0.4-CW\n2.2915\n0.3140\n0.2217\n0.1586\n0.0\nMPC-0.4-CW\n0.1282\n0.3836\n0.9086\n0.1916\n0.6786\nGVF-0.25-CCW\n2.1442\n0.2467\n0.1098\n0.1181\n0.0\nMPC-0.25-CCW\n1.1971\n0.2412\n0.1218\n0.1308\n0.0\nOval\nshape\nGVF-0.4-CCW\n2.4046\n0.3501\n0.2754\n0.2125\n0.0145\nMPC-0.4-CCW\n0.8928\n0.3825\n0.5293\n0.1963\n0.2275\nGVF-0.4-CW\n2.4848\n0.3658\n0.2953\n0.1922\n0.0\nMPC-0.4-CW\n-0.7168\n0.3836\n1.3182\n0.2095\n0.9122\nGVF-0.25-CCW\n1.5112\n0.2473\n0.3645\n0.1466\n0.0332\nMPC-0.25-CCW\n0.0225\n0.2296\n0.9565\n0.1381\n0.8792\nComplex\nshape\nGVF-0.4-CCW\n2.3501\n0.3129\n0.2221\n0.1817\n0.0\nMPC-0.4-CCW\n0.7172\n0.3845\n0.6407\n0.2131\n0.3894\nGVF-0.4-CW\n2.3182\n0.3168\n0.2317\n0.2150\n0.0006\nMPC-0.4-CW\n0.4324\n0.3905\n0.7662\n0.2264\n0.5223\nGVF-0.25-CCW\n1.9326\n0.2472\n0.1890\n0.1509\n0.0\nMPC-0.25-CCW\n1.1559\n0.2435\n0.1664\n0.1720\n0.0\nTable 5: Comparison of comfort of GVF method and MPC on all the test roads at different speeds\nand directions\nExperiment\nFirst\norder\nspeed\ncomfort ↑\nSecond\norder\nspeed\ncomfort ↑\nFirst\norder\nsteering\ncomfort ↑\nSecond\norder\nsteering\ncomfort ↑\nRectangle\nshape\nGVF-0.4-CCW\n-0.0356\n-0.2532\n-0.2251\n-1.3403\nMPC-0.4-CCW\n-0.0832\n-0.7605\n-1.2542\n-8.1963\nGVF-0.4-CW\n-0.0311\n-0.2149\n-0.1995\n-1.1850\nMPC-0.4-CW\n-0.0944\n-0.8916\n-1.4328\n-10.9570\nGVF-0.25-CCW\n-0.0009\n-0.0112\n-0.1466\n-0.8890\nMPC-0.25-CCW\n-0.0570\n-0.5272\n-0.6384\n-3.5208\nOval\nshape\nGVF-0.4-CCW\n-0.0348\n-0.2423\n-0.2191\n-1.4632\nMPC-0.4-CCW\n-0.1026\n-0.9301\n-1.4119\n-8.9051\nGVF-0.4-CW\n-0.0241\n-0.1638\n-0.1674\n-1.1451\nMPC-0.4-CW\n-0.0847\n-0.7534\n-1.3957\n-9.0432\nGVF-0.25-CCW\n-0.0005\n-0.0061\n-0.0969\n-0.7614\nMPC-0.25-CCW\n-0.0657\n-0.6273\n-0.4830\n-3.0566\nComplex\nshape\nGVF-0.4-CCW\n-0.0341\n-0.2540\n-0.2272\n-1.5306\nMPC-0.4-CCW\n-0.0625\n-0.5846\n-1.2133\n-8.1747\nGVF-0.4-CW\n-0.0348\n-0.2339\n-0.2240\n-1.3911\nMPC-0.4-CW\n-0.0809\n-0.7521\n-1.2861\n-8.7905\nGVF-0.25-CCW\n-0.0006\n-0.0082\n-0.1696\n-1.0394\nMPC-0.25-CCW\n-0.0525\n-0.4932\n-0.6457\n-3.6786\n28\nTable 6: Evaluation of the robustness of GVF method on damaged lane markings on all the test roads\nExperiment\nReward\nper\nsecond ↑\nAverage\nspeed ↑\nAverage\noffcentered\n-ness ↓\n(normalized)\nAverage\nabsolute\nroad angle ↓\nNear\nout\nof lane ↓\nRectangle\nshape\nGVF-0.4-CCW\n2.6835\n0.3205\n0.1345\n0.1315\n0.0\nGVF-0.4-CCW-D\n2.7407\n0.3261\n0.1358\n0.1351\n0.0\nOval\nshape\nGVF-0.4-CCW\n2.4046\n0.3501\n0.2754\n0.2125\n0.0145\nGVF-0.4-CCW-D\n2.0728\n0.3279\n0.3285\n0.2089\n0.0719\nComplex\nshape\nGVF-0.4-CCW\n2.3501\n0.3129\n0.2221\n0.1817\n0.0\nGVF-0.4-CCW-D\n2.1059\n0.3284\n0.3125\n0.2365\n0.0942\nTable 7: Evaluation of the comfort of GVF method on damaged lane markings on all the test roads\nExperiment\nFirst\norder\nspeed\ncomfort ↑\nSecond\norder\nspeed\ncomfort ↑\nFirst\norder\nsteering\ncomfort ↑\nSecond\norder\nsteering\ncomfort ↑\nRectangle\nshape\nGVF-0.4-CCW\n-0.0356\n-0.2532\n-0.2251\n-1.3403\nGVF-0.4-CCW-D\n-0.0383\n-0.2715\n-0.2303\n-1.4620\nOval\nshape\nGVF-0.4-CCW\n-0.0348\n-0.2423\n-0.2191\n-1.4632\nGVF-0.4-CCW-D\n-0.0334\n-0.2953\n-0.2094\n-1.6612\nComplex\nshape\nGVF-0.4-CCW\n-0.0341\n-0.2540\n-0.2272\n-1.5306\nGVF-0.4-CCW-D\n-0.0437\n-0.3608\n-0.2897\n-2.0946\nTable 8: Comparison of GVF-BCQ and E2E-BCQ on the Rectangle test road at 0.4 m/s\nExperiment\nReward\nper\nsecond ↑\nAverage\nspeed ↑\nAverage\noffcentered\n-ness ↓\n(normalized)\nAverage\nabsolute\nroad angle ↓\nNear\nout\nof lane ↓\nRectangle\nshape\nGVF-0.4-CCW\n2.6835\n0.3205\n0.1345\n0.1315\n0.0\nE2E-0.4-CCW\n1.2578\n0.1816\n0.2558\n0.2414\n0.0376\nGVF-0.4-CW\n2.2915\n0.3140\n0.2217\n0.1586\n0.0\nE2E-0.4-CW\n-0.1302\n0.1710\n0.9927\n0.3034\n0.5418\nTable 9: Comparison of GVF-BCQ and E2E-BCQ comfort levels on the Rectangle test road at 0.4\nm/s\nExperiment\nFirst\norder\nspeed\ncomfort ↑\nSecond\norder\nspeed\ncomfort ↑\nFirst\norder\nsteering\ncomfort ↑\nSecond\norder\nsteering\ncomfort ↑\nRectangle\nshape\nGVF-0.4-CCW\n-0.0356\n-0.2532\n-0.2251\n-1.3403\nE2E-0.4-CCW\n-0.0154\n-0.2266\n-0.1109\n-1.4240\nGVF-0.4-CW\n-0.0311\n-0.2149\n-0.1995\n-1.1850\nE2E-0.4-CW\n-0.0148\n-0.1937\n-0.1174\n-1.3514\n29\nFigure 16: Distribution of lane centeredness, road angle, speed and action distribution of GVF-BCQ\nand MPC at 0.4 m/s and 0.25 m/s on the rectangle test track. From top to bottom: GVF-0.4-CCW,\nMPC-0.4-CCW, GVF-0.25-CCW, MPC-0.4-CCW7\nFigure 17: Distribution of lane centeredness, road angle, speed and action distribution on the rectangle\ntest road. From top to bottom: GVF-BCQ-0.4, GVF-BCQ-0.4 with lane marking damage on the\nrectangle road. The similarities highlight the robustness of GVF-BCQ to the introduction of damaged\nlanes.\n30\nFigure 18: Distribution of lane centeredness, road angle, speed and action distribution of E2E-BCQ\non the rectangle test road at 0.4 speed, counterclockwise direction\n31\n",
  "categories": [
    "cs.LG",
    "cs.RO"
  ],
  "published": "2020-06-26",
  "updated": "2020-06-26"
}