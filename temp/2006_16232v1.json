{
  "id": "http://arxiv.org/abs/2006.16232v1",
  "title": "Learning Robot Skills with Temporal Variational Inference",
  "authors": [
    "Tanmay Shankar",
    "Abhinav Gupta"
  ],
  "abstract": "In this paper, we address the discovery of robotic options from\ndemonstrations in an unsupervised manner. Specifically, we present a framework\nto jointly learn low-level control policies and higher-level policies of how to\nuse them from demonstrations of a robot performing various tasks. By\nrepresenting options as continuous latent variables, we frame the problem of\nlearning these options as latent variable inference. We then present a temporal\nformulation of variational inference based on a temporal factorization of\ntrajectory likelihoods,that allows us to infer options in an unsupervised\nmanner. We demonstrate the ability of our framework to learn such options\nacross three robotic demonstration datasets.",
  "text": "Learning Robot Skills with Temporal Variational Inference\nTanmay Shankar 1 Abhinav Gupta 1 2\nAbstract\nIn this paper, we address the discovery of robotic\noptions from demonstrations in an unsupervised\nmanner. Speciﬁcally, we present a framework to\njointly learn low-level control policies and higher-\nlevel policies of how to use them from demonstra-\ntions of a robot performing various tasks. By rep-\nresenting options as continuous latent variables,\nwe frame the problem of learning these options as\nlatent variable inference. We then present a tempo-\nral formulation of variational inference based on\na temporal factorization of trajectory likelihoods,\nthat allows us to infer options in an unsupervised\nmanner. We demonstrate the ability of our frame-\nwork to learn such options across three robotic\ndemonstration datasets.\n1. Introduction\nThe robotics community has long since sought to acquire\ngeneral purpose and reusable robotic skills, to enable robots\nto execute a wide range of tasks. The idea of such skills is\nattractive; by abstracting away the details of low-level con-\ntrol and reasoning over high-level skills instead, a robot can\naddress more complex and longer term tasks. Further, the\nability to compose skills leads to a combinatorial increase\nin the robot’s capabilities (Leslie Pack Kaelbling, 2017),\nideally spanning the abilities required for the robot to com-\nplete the desired tasks. For example, a robot equipped with\nreaching, grasping, and pouring skills could compose them\nto make a cup of tea as well as pour cereal into a bowl.\nIndeed, the promise of skills has been explored in several\ncontexts, be it options in reinforcement learning (RL) (Sut-\nton et al., 1999), operators in the planning (Fikes & Nilsson,\n1971), primitives and behaviours in robotics (Muelling et al.,\n2010), or abstractions (Kim et al., 2019). Nomenclature\naside, these ideas share the notion of eliciting a certain pat-\n1Facebook AI Research, Pittsburgh, PA, USA 2Carnegie Mellon\nUniversity, Pittsburgh, PA, USA. Correspondence to: Tanmay\nShankar <tanmayshankar@fb.com>.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\ntern or template of action in response to a particular situation,\ndiffering in their exact implementations and how these skills\nare obtained. For example, while previous works (Muelling\net al., 2010; Mlling et al., 2013; Konidaris & Barto, 2009)\nmanually deﬁned these skills, more recent approaches have\nsought to learn these skills, either from interaction (Kulka-\nrni et al., 2016) or from demonstrations (Xu et al., 2018;\nHuang et al., 2019; Fox et al., 2017; Krishnan et al., 2017;\nKipf et al., 2019).\nLearning skills from demonstrations is appealing, since it\nallows non-robotics-expert humans to demonstrate solving\nthe target tasks, bypassing the tedium of manually speci-\nfying these skills or carefully engineering solutions to the\ntasks (Argall et al., 2009). But even using demonstrations,\napproaches such as Xu et al. (2018); Huang et al. (2019) re-\nquire heavy supervision such as segmentation of demonstra-\ntion data. Instead, learning skills in an unsupervised, data-\ndriven manner not only avoids having to annotate demon-\nstrations; it also enables the use of large scale and diverse\ndemonstration data in robotics (Sharma et al., 2018; Man-\ndlekar et al., 2018). This in turn enables learning a correp-\nsondingly diverse set of skills, as well as how to use them\nto achieve a variety of tasks.\nAside from the issue of learning and representing individual\nskills, is the notion of composing them. The efﬁcacy of these\nskills is rather limited when used in isolation; by selecting\nand sequencing the appropriate skills however, a robot can\nachieve a variety of complex tasks, as illustrated by the tea\nand cereal example. A rich body of literature addresses\ncomposing skills, including sequencing skills (Neumann\net al., 2014; Peters et al., 2013), hierarchical approaches\n(Lioutikov et al., 2017; Xu et al., 2018; Huang et al., 2019;\nKonidaris et al., 2012; Konidaris & Barto, 2009; Shankar\net al., 2020), options (Sutton et al., 1999), etc. Some works\namong these (Fox et al., 2017; Krishnan et al., 2017; Kipf\net al., 2019) address jointly learning skills and how to com-\npose them. Jointly learning both levels of this hierarchy not\nonly addresses how to use these skills, but also allows for\nadapting the skills based on how useful they are to the task\nat hand.\nUnfortunately, these works have their own limitations.\nWhile Neumann et al. (2014); Niekum et al. (2012);\nKonidaris & Barto (2009) do learn to sequence skills, they\narXiv:2006.16232v1  [cs.LG]  29 Jun 2020\nLearning Robot Skills with Temporal Variational Inference\nassume restrictive primitive representations - such as DMPs\n(Ijspeert et al., 2013). While Shankar et al. (2020) learn\ncontinuous representations of primitives, it requires an ad-\nditional post hoc policy learning step. Fox et al. (2017);\nKrishnan et al. (2017) do afford directly usable policies, but\ncritically are restricted to a ﬁxed number of discrete options.\nIn this paper, we propose a framework to jointly learn op-\ntions and how to compose and use them from demonstra-\ntions in an unsupervised manner. At the heart of our frame-\nwork is a temporal variational inference (TVI) based on a\ncorresponding factorization of trajectory likelihoods. We\nadopt a latent variable representation of options; this allows\nus to treat the problem of inferring options as inferring latent\nvariables, which we may then accomplish via our temporal\nvariational inference. Speciﬁcally, optimizing the objective\nafforded by our temporal variational inference with respect\nto the distributions involved naturally gives us low and high-\nlevel policies of options.\nWe evaluate our approach’s ability to learn options\nacross three datasets, and demonstrate that our ap-\nproach can learn a meaningful space of options that\ncorrespond with traditional skills in manipulation, vi-\nsualized at https://sites.google.com/view/learning-causal-\nskills/home. We quantify the effectiveness of our policies in\nsolving downstream tasks, evaluated on a suite of tasks.\n2. Related Work\nLearning from Demonstrations: Learning from Demon-\nstrations (LfD) addresses solving tasks by learning from\ndemonstrations of the task being solved by an expert. This\nmay be accomplished by simply cloning the original demon-\nstration (Esmaili et al., 1995), or ﬁtting the demonstration\nto a trajectory representation (Kober & Peters, 2009; Pe-\nters et al., 2013) or a policy (Schaal, 1997). More recent\nefforts have sought to segment demonstrations into smaller\nbehaviors, and ﬁtting a model to the resulting segments\n(Niekum et al., 2012; Krishnan et al., 2018; Murali et al.,\n2016; Meier et al., 2011). Argall et al. (2009) presents a\nthorough review of these techniques. Our work falls into\nthe broad paradigm of LfD, but seeks to learn hierarchical\npolicies from demonstrations.\nSequencing Primitives: The concept of learning move-\nment primitives using predeﬁned representations of primi-\ntives (Kober & Peters, 2009; Peters et al., 2013) has been\na popular technique to capturing robotic behaviors. A nat-\nural next step is to sequence such primitives to perform\nlonger horizon downstream tasks, (Neumann et al., 2014;\nNiekum et al., 2012; Muelling et al., 2010). Konidaris &\nBarto (2009); Konidaris et al. (2012) address merging these\nskills into skill-trees. Lioutikov et al. (2017; 2020) address\nsequencing primitives using probabilistic segmentation and\nattribute grammars respectively. Our work differs from a\nmajority of these works in that we jointly learn both the\nrepresentation of primitives and how these primitives must\nbe sequenced.\nHierarchical Policy Learning: Fox et al. (2017); Krishnan\net al. (2017); Kipf et al. (2019); Sharma et al. (2019) also ad-\ndress the problem of learning options from demonstrations\nwithout supervision. However these works are restricted\nto a discrete set of options, which must be pre-speciﬁed.\nFurther, Fox et al. (2017); Krishnan et al. (2017) employ\na forward-backward algorithm for inference that requires\nan often intractable marginalization over latents. The Com-\npILE framework (Kipf et al., 2019) makes use of continuous\nlatent variables to parameterize options, but requires care-\nfully designed attention mechanisms, and is evaluated in\nrelatively low-dimensional domains. Smith et al. (2018)\nand Bacon et al. (2017) derive policy gradients to address\nhierarchical policy learning in the RL setting.\nLearning Trajectory Representations:\nShankar et al.\n(2020) learning representations of primitives, but need to\nadopt an additional phase of training to produce usable poli-\ncies. Co-Reyes et al. (2018) also approach hierarchical\nRL from a trajectory representation perspective. Our work\nshares this notion of implicitly learning a representation of\nprimitives, but differs in that we do not require an additional\nhigh-policy learning step to perform hierarchical control.\nLearning Temporal Abstractions: Kim et al. (2019) and\nGregor et al. (2019) both address learning temporal abstrac-\ntions in the form of ‘jumpy’ transitions. Kim et al. (2019)\nseeks to learn a generic partitioning of the input sequence\ninto temporal abstractions, while Gregor et al. (2019) learns\ntemporal abstractions with beliefs over state to capture un-\ncertainity about the world. While both these works adopt\nvariational bounds of a similar form to ours, our bound ob-\njective is derived in terms of usable option policies rather\nthan abstract or belief states.\nCompositional Policies: Both Xu et al. (2018) and Huang\net al. (2019) address unseen manipulation tasks by learning\ncompositional policies, but require heavy supervision to do\nso. Andreas et al. (2017) and Shiarlis et al. (2018) compose\nmodular policies in the RL and LfD settings respectively,\nusing policy sketches to select which policies to execute.\nWhile our work approaches doesn’t explicitly address com-\npositionality, we too seek to beneﬁt from the beneﬁts of\nsuch compositionality.\n3. Approach\n3.1. Preliminaries\nThroughout our paper, we adopt an undiscounted Markov\nDecision Process without rewards (denoted as MDP\\R). An\nLearning Robot Skills with Temporal Variational Inference\nAlgorithm 1 Trajectory Generation Process with Options\nInput: Low-level Policy π, High-level Policy η, Initial\nState Distribution d1(s),\nOutput: Trajectory τ\n1: s1 ∼d1, b1 ←1\n▷Initialize state.\n2: for t ∈[1, 2, .., T] do\n3:\nif bt = 1 then:\n4:\nzt ∼η(z|s1:t, a1:t−1, z1:t−1)\n▷Select option.\n5:\nelse:\n6:\nzt ←zt−1\n▷Continue previous option.\n7:\nat ∼π(a|s1:t, a1:t−1, z1:t)\n▷Select action.\n8:\nst+1 ∼p(st+1|st, at)\n▷Execute action.\n9:\nbt+1 ∼β(st+1)\n▷Decide whether to terminate.\n10: τ ←{st, at}T\nt=1, ζ ←{zt, bt}T\nt=1\nMDP\\R is a tuple M : ⟨S, A, P⟩, that consists of states s in\nstate space S, actions a in action space A, and a transition\nfunction between successive states P(st+1|st, at).\nOptions: An option ω ∈Ω(Sutton et al., 1999) formally\nconsist of three components - an initiation set I, a policy π :\nS →A, and a termination function β : S →[0, 1]. When\nan option is invoked in a state in I, policy π is executed\nuntil the termination function dictates the option should be\nterminated (i.e., β(s) = 1). As in Fox et al. (2017); Smith\net al. (2018), we assume options may be initiated in the\nentire state space.\nOptions as Latent Variables: We assume that the identity\nof an option being executed is speciﬁed by a latent variable z,\nthat may be either continuous or discrete. We also consider\nthat at every timestep, a high-level policy η : S →Ω×[0, 1]\nselects the identity zt of the option to be invoked, as well\nas a binary variable bt of whether to terminate the option\nor not. This option informs the low-level policy π’s choice\nof action, constructing a trajectory as per the generative\nprocess in algorithm 1. We denote the sequence of options\nexecuted during a trajectory as a sequence of these latent\nvariables, ζ = {zt, bt}T\nt=1.\n3.2. Decomposition of trajectory likelihood\nConsider a trajectory τ = {st, at}T\nt=1, and the sequence\noption sequence that generated it ζ = {zt, bt}T\nt=1. The\njoint likelihood p(τ, ζ) of the trajectory and these options\nunder the generative process described in algorithm 1 may\nbe expressed as follows:\np(τ, ζ) = p(s1)\nT\nY\nt=1\nη(ζt|s1:t, a1:t−1, ζ1:t−1)\nπ(at|s1:t, a1:t−1, ζ1:t)p(st+1|st, at)\n(1)\nThe\ndistributions\nη(ζt|s1:t, a1:t−1, ζ1:t−1)\nand\nπ(at|s1:t, a1:t−1, ζ1:t)\nimplicitly\ncapture\nthe\ncausal\nrestriction that future variables (ex. st+1:T ) do not inﬂuence\nearlier variables (ex. a1:t). Further, both π and η may be\nqueried at any arbitrary time t with the information available\ntill that time. Ziebart et al. (2013) formalized the notion\nof causally conditioned distributions to represent such\ndistributions, a notion that plays a part in the formulation of\nour approach. We refer the reader to Ziebart et al. (2013);\nKramer (1998) for a more thorough treatment of this\nconcept.\n3.3. Temporal Variational Inference\nTo reiterate, our goal is to learn options and how to use\nthem from demonstrations; this formally corresponds to\nlearning low and high level policies π and η, from a dataset\nof N demonstrations, D = {τi}N\ni=1. This is equivalent to\ninferring latent variables ζ from a trajectory, since policies π\nand η essentially reason about the choice of ζ and its effect\non the choice of actions. The representation of options as\nlatent variables ζ we adopt hence allows us to view the\nproblem of inferring options from a perspective of inference\nof latent variables.\nThis allows us to employ unsupervised latent variable infer-\nence techniques; we employ a variant of variational infer-\nence (VI) (Kingma & Welling, 2013) to infer latent options\nζ. Our choice of VI over the forward-backward style algo-\nrithm employed in Fox et al. (2017); Krishnan et al. (2017)\nis because VI is amenable to both continuous and discrete\nlatent variables z. Further, VI bypasses the often intractable\nmarginalization over latents in favor of sampling based ap-\nproach.\nIn standard VI, a variational distribution q(z|x) is used to\ninfer latents z from observed data x, approximating the\nunknown conditional p(z|x). One then optimizes the like-\nlihood of observations given the predicted latents under\na learned decoder p(x|z). In our case, we seek to infer\nthe sequence of options ζ = {zt, bt}T\nt=1 from a trajectory\nτ = {st, at}T\nt=1; we estimate the conditional p(ζ|τ) with a\nvariational approximation q(ζ|τ).\nTo retrieve usable policies that can be queried at inference\nor “test” time, we require policies that can reason about the\ncurrent choices of option ζt and action at given the obser-\nvations available so far, i.e. s1:t, a1:t−1, and ζ1:t−1. This\nprecludes optimizing the conditional p(τ|ζ) as would be\ndone in standard VI. Instead, we optimize the joint likeli-\nhood p(τ, ζ) of trajectories and latents with respect to the\ncausally conditioned π and η. Not only does this afford us\ndirectly usable policies as desired, but this objective natu-\nrally arises from the variational bound constructed below.\nWe now formally present temporal variational inference,\na variant of VI suitable for our sequential latent variables,\nLearning Robot Skills with Temporal Variational Inference\nFigure 1. Depiction of the key distributions q, π, and η, and the probabilistic graphical model underlying our approach. Note the\ndependence between trajectories and options. We also depict the variables that each of the three networks reason about, and the\ninformation they make use of to do so.\nthat accounts for the causal restriction of these latent vari-\nables based on the decomposition of trajectory likelihood in\nsection 3.2. We begin with the standard objective of maxi-\nmizing the log-likelihood of trajectories across the dataset,\nL = Eτ∼D\n\u0002\nlog p(τ)\n\u0003\n. L is lower bounded by J, where:\nJ = Eτ∼D\n\u0002\nlog p(τ)\n\u0003\n−DKL\n\u0002\nq(ζ|τ)||p(ζ|τ)\n\u0003\n= Eτ∼D\n\u0002\nlog p(τ)\n\u0003\n−Eτ∼D,ζ∼q(ζ|τ)\nh\nlog q(ζ|τ)\np(ζ|τ)\ni\n= Eτ∼D,ζ∼q(ζ|τ)\nh\nlog p(τ) + log p(ζ|τ) −log q(ζ|τ)\ni\n= Eτ∼D,ζ∼q(ζ|τ)\nh\nlog p(τ, ζ) −log q(ζ|τ)\ni\nSubstituting the joint likelihood decomposition from eq. (1)\nabove yields the following objective:\nJ = Eτ∼D,ζ∼q(ζ|τ)\nh X\nt\n{log η(ζt|s1:t, a1:t−1, ζ1:t−1)\n+ log π(at|s1:t, a1:t−1, ζ1:t) + log p(st+1|st, at)}\n+ log p(s1) −log q(ζ|τ)\ni\n(2)\nAssuming distributions π, η, and q are parameterized by θ, φ,\nand ω respectively, we may optimize these using standard\ngradient based optimization of J:\n∇J = ∇θ,φ,ωEτ∼D,ζ∼q(ζ|τ)\nh X\nt\n{log π(at|s1:t, a1:t−1, ζ1:t)\n+ log η(bt, zt|s1:t, a1:t−1, ζ1:t−1)} −log q(ζ|τ)\ni\n(3)\nNote that the dynamics p(st+1|st, at) and initial state dis-\ntribution p(s1) factor out of this gradient, as derived in the\nsupplementary material.\nParsing the objective J: We provide a brief analysis of\nhow objective J and its implied gradient update eq. (3)\njointly optimizes π, η, and q to be consistent with each\nother. Three interacting terms optimize q. The ﬁrst two\nterms encourage q to predict options ζ that result in high\nlikelihood of actions at under π, and that are likely under\nthe current estimate of the high level policy η. The ﬁnal\n−log q(ζ|τ) term encourages maximum entropy of q, dis-\ncouraging q from committing to an option unless it results\nin high likelihood under π and η. This entropy term also pre-\nvents q from trivially encoding all trajectories into a single\noption.\nLow-level policy π is trained to increase the likelihood of\nselecting actions at given the current option being executed\nζt. High-level policy η is trained to mimic the choices\nof options made by the variational network (i.e. options\nthat result in high likelihood of demonstrated actions), only\nusing the available information at time t to do so.\nReparameterization: While eq. (3) can be implemented\nvia REINFORCE (Williams, 1992), we do so only for infer-\nring discrete variables {bt}T\nt=1. As in standard VI, we ex-\nploit the continuous nature of z’s and employ the reparame-\nterization trick (Kingma & Welling, 2013) to enable efﬁcient\nlearning of {zt}T\nt=1. Rather than sample latents {zt}T\nt=1\nfrom a stochastic variational distribution q(ζ|τ), {zt}T\nt=1\nis parameterized as a differentiable and deterministic func-\ntion of the inputs τ and a noise vector ϵ, drawn from an\nappropriately scaled normal distribution. In practice, we pa-\nrameterize q as an LSTM that takes τ as input, and predicts\nLearning Robot Skills with Temporal Variational Inference\nAlgorithm 2 Temporal Variational Inference for Learning\nSkills\nInput: D\n▷Require a demonstration dataset\nOutput: π, η\n▷Output low and high-level policies\n1: Initialize πθ, ηφ, qω\n▷Initialize networks\n2: Pretrain π as VAE\n▷Pretrain latent representation\n3: for i ∈[1, 2, ..., Niterations] do\n4:\nτi ←D\n▷Retrieve trajectory from dataset\n5:\nζ ∼q(ζ|τi)\n▷Sample latent sequence from\nvariational network\n6:\nJ\n←\nP\nt log π(at|s1:t, a1:t−1, ζ1:t)\n+\nP\nt log η(ζt|s1:t, a1:t−1, ζ1:t−1) −log q(ζ|τ)\n▷Evaluate likelihood objective under current policy\nestimates\n7:\nUpdate πθ, ηφ, qω via ∇θ,φ,ωJ\nmean µt and variance σt of the distribution q({zt}T\nt=1|τ).\nWe then retrieve {zt}T\nt=1 as {zt = µt + σtϵt}T\nt=1.\nIn our case, gradients ﬂow from our objective J through\nboth the low and high-level policies π and η to the varia-\ntional network q. This in contrast with standard VI, where\ngradients pass through the decoder p(x|z). This reparame-\nterization enables the efﬁcient gradient based learning of q\nbased on signal from both the low and high-level policies.\nFeatures of objective: The temporal variational inference\nwe present has several desirable traits that we describe be-\nlow.\n(1) First and foremost, J provides us with causally condi-\ntioned low and high-level policies π and η. These policies\nare directly usable at inference time towards solving down-\nstream tasks, since they are only trained with information\nthat is also available at inference time.\n(2) With TVI, we can adopt a continuous parameterization\nof options, z ∈Rn. This eliminates the need to pre-specify\nthe number of options required; instead we may learn as\nmany options as are required to capture the behaviors ob-\nserved in the data, in a data driven manner. The continuous\nspace of options also allows us to reason about how similar\nthe various learned options are to one another (allowing\nsubstitution of options for one another).\n(3) The joint training of π, η, and q implied by J not only\nallows training the high-level policy η to based on the avail-\nable options, but also allows the adaptation of the low-level\noptions π based on how useful they are to reconstructing a\ndemonstration.\n(4) The objective J is also amenable to gradient based opti-\nmization, allowing us to learn options efﬁciently.\n3.4. Learning Skills with Temporal VI\nEquipped with this understanding of our temporal varia-\ntional inference (TVI), we may retrieve low and high-level\npolicies π and η by gradient based optimization of the ob-\njective J presented. We ﬁrst make note of some practical\nconsiderations required to learn skills with TVI, then present\na complete algorithm to learn skills using TVI.\nPolicy Parameterization: We parameterize each of the\npolicies π and η as LSTMs (Hochreiter & Schmidhuber,\n1997), with 8 layers and 128 hidden units per layer. The\nrecurrent nature of the LSTM naturally captures the causal\nnature of π and η. In contrast, q is parameterized as a\nbi-directional LSTM, since q reasons about the sequence\nof latents ζ given the entire trajectory τ. q, π and η all\ntake in a concatenation of trajectory states and actions as\ninput. π and η also take in the sequence of latents until the\ncurrent timestep as input. Since η reasons about the choice\nof latents to solve the task occurring in the demonstration, η\nalso takes in additional information about the task, such as\ntask ID and object-state information. π predicts the mean\nµa and variance σa of a Gaussian distribution from which\nactions a ∈A are drawn. η and q both predict mean µz and\nvariance σz of a Gaussian distribution from which latent\nvariables z are drawn. η and q also predict the probability\nof terminating a particular option p(b), from which binary\ntermination variables b are drawn. While q predicts the\nentire sequence of ζ’s, latents are retrieved from η during a\nrollout via the generative process in algorithm 1.\nPretraining the Low-level Policy: Optimizing our joint\nobjective J with randomly initialized low-level policies re-\nsults in our training procedure diverging. During initial\nphases of training, the random likelihoods of actions and\noptions under the random initial policies provide uninforma-\ntive gradients. To counteract this, we initialize the low-level\npolicy to capture some meaningful skills, by pretraining\nit to reconstruct demonstration segments in a VAE setting.\nSpeciﬁcally, we draw trajectory segments from the dataset\nand encode them as a single latent z (i.e. a single option).\nWe train the low-level policy (i.e. as a decoder) to maxi-\nmize the likelihood of the actions observed in this trajectory\nsegment given the latent z predicted by the encoder.\nAlgorithm: We present the full algorithm for learning skills\nvia temporal variational inference in algorithm 2. After the\npre-training step described above, there are three steps that\noccur every iteration in training. (1) For every trajectory,\na corresponding sequence of latent variables ζ is sampled\nfrom the varitional network q. (2) This likelihood of this es-\ntimated sequence of latents ζ is evaluated under the current\npolicy estimates, giving us objective J. (3) The gradients of\nJ are then used to update the three networks π, η, and q.\n4. Experiments\nWe would like to understand how well our approach can\ndiscover options from a set of demonstrations in an un-\nLearning Robot Skills with Temporal Variational Inference\n(a) Latent space of skills for MIME dataset.\nNote the clustering of skills into left and\nright handed reaching, returning and sliding\nskills, along with additional hybrid skills.\n(b) Latent space of skills for Roboturk\ndataset. Note the clustering of skills into\nsingle armed pushing, grasping, and placing\nin different locations and to different extents.\n(c) Latent space of skills for Mocap dataset.\nWhile less structured than (a) and (b), the\nspace consists of diverse skills ranging such\nas running, front ﬂips and punching.\nFigure 2. Visualization of the learned latent space of skills for the (a) MIME dataset, (b) Roboturk dataset, and (c) Mocap dataset. Note\nthe emergence of clusters of skills in each case.\nsupervised manner, and quantify how useful the learnt\npolicies are for solving a set of target tasks.\nTo this\nend, we evaluate our approach across the three datasets\ndescribed below, as well as a suite of simulated robotic\ntasks. We present visualizations of the results of our model\nat https://sites.google.com/view/learning-causal-skills. We\nﬁrst describe the datasets used below.\nMIME Dataset: The MIME Dataset (Sharma et al., 2018)\nconsists of 8000+ kinesthetic demonstrations across 20 tasks\n(such as pushing, bottle-opening, stacking, etc.) collected\non a Baxter robot. We use the 16 dimensional joint-angles\nof the Baxter (7 joints for each of the 2 arms, and a grip-\nper for each arm) as the input and prediction space for our\nmodel.\nRoboturk Dataset: The Roboturk Dataset (Mandlekar\net al., 2018) consists of 2000+ demonstrations collected\non a suite of tasks (such as bin picking, nut-and-peg assem-\nbly, etc.) by teleoperating a simulated Sawyer robot. These\nteleoperation demonstrations are hence more noisy than the\nkinesthetic MIME dataset. We use the 8 dimensional joint\nangles of the Sawyer (7 arm joints, and a single gripper), as\nwell as the robot-state and object-state vectors provided in\nMandlekar et al. (2018) to train our model.\nCMU Mocap Dataset: The CMU Mocap Dataset (CMU,\n2002) consists of 1953 motions collected by tracking visual\nmarkers placed on humans, while performing a variety of\ndifferent actions. These actions include punching, jumping,\nperforming ﬂips, running, etc. We use the local (i.e., relative\nto a root node on the agent itself) 3-D positions of each\nof the 22 joints recorded in the dataset, for a total of 66\ndimensions.\nPreprocessing and Train-Test Split: In both the MIME\nand Roboturk datasets, the gripper values are normalized\nto a range of {−1, 1}, while joint angles are unnormalized.\nThe trajectories across all datasets are downsampled (in\ntime) by a factor of 20. For each dataset, we set aside 500\nrandomly sampled trajectories that serve as our test set for\nour experiments in section 4.2. The remaining trajectories\nserve as the respective training sets.\n4.1. Qualitative Evaluation of Learned Space\nThe ﬁrst question we would like to answer is - “Is our ap-\nproach able to learn a diverse space of options?”. We answer\nthis by presenting a qualitative analysis of the space of op-\ntions learned by our model.\nFor a set of 100 demonstrations, we retrieve the se-\nquence of latent z’s and their corresponding trajectory seg-\nments executed over each demonstration from our model.\nWe embed these latent z’s in a 2-D space using T-SNE\n(van der Maaten & Hinton, 2008), and visualize the tra-\njectory segments at their corresponding position in this 2-\nD embedded space. We visualize the learned embedding\nspaces for the MIME dataset in ﬁg. 2a, for the Roboturk\ndataset in ﬁg. 2b, and for the Mocap dataset in ﬁg. 2c.\nDynamic visualizations of these ﬁgures are available at\nhttps://sites.google.com/view/learning-causal-skills.\nNote the emergence of clusters of skills on the basis of types\nof motions being executed across these datasets.\nIn the case of the MIME dataset ﬁg. 2a, we note that the\nemergent skills are separated on the basis of the nature of\nmotion being executed, as well as the arm of the robot\nbeing used. This is expected in a bimanual robot, as skills\nwith the left and right hands are to be treated differently, as\nare skills using both hands. The skills that our approach\nLearning Robot Skills with Temporal Variational Inference\nTable 1. Trajectory Reconstruction Error of our approach and base-\nlines across various datasets. The baselines were adapted from (1)\nKingma & Welling (2013), (2) Ijspeert et al. (2013), (3) Niekum\net al. (2012), (4) Shankar et al. (2020).\nMETHOD\nMIME\nROBOTURK\nMOCAP\nDATA\nDATA\nDATA\nFLAT VAE [1]\n0.14\n0.23\n0.08\nFLAT DMP [2]\n0.36\n0.54\n3.45\nH-DMP\n[3]\n0.02\n0.06\n0.01\nDISCO-MP [4]\n0.02\n0.03\n0.03\nOURS\n0.02\n0.04\n0.04\ncaptures correspond directly to traditional notions of skills\nin the manipulation community, such as reaching, returning,\nsliding / pushing, etc. Our approach further distinguishes\nbetween left handed reaching and right handed reaching,\netc., a useful ability in addressing downstream tasks.\nFor the Roboturk dataset ﬁg. 2b, the space is separated on\nthe basis of the nature, shape and direction of motion being\nexecuted. For example, placing motions to the left and\nright of the robot appear separately in the space. Additional\nplacing motions that move the arm to a lesser extent also\nappear separately. The space also captures ﬁner motions\nsuch as closing the grippers down (typically in a position\nabove the workspace).\nFor the Mocap dataset ﬁg. 2c, the learned space is not as\nclearly structured as in the case of MIME and Roboturk\ndatasets. We believe this is due to the much larger range\nof motions executed in the dataset, coupled with the high-\ndimensional nature of the humanoid agent. Despite this lack\nof structure, a diverse set of skills is still learned. For ex-\nample, the space consists of running, walking, and jumping\nskills, which constitute a majority of the dataset. More inter-\nesting skills such as fencing, performing ﬂips, and boxing\nskills were also present and captured well by our model.\nIn both the MIME and Roboturk datasets, the correspon-\ndence of many emergent skills with traditional notions of\nskills in the manipulation community is indicative that our\napproach can indeed discover diverse robotic skills from\ndemonstrations without supervision.\n4.2. Reconstruction of Demonstrations\nWe evaluate how well our approach can use the learned skills\nto reconstruct the demonstrated trajectories; i.e. whether it\nis able to capture the overall structure of the demonstrations\nin terms of the skills being executed. We do so quantitatively\nand qualitatively. Quantitatively, we measure the average\nstate distance (measured by the mean squared error) between\nthe reconstructed trajectory and the original demonstration,\nacross the 500 randomly sampled unseen demonstrations in\neach dataset. We compare our approach’s performance on\nthis metric against a set of baselines:\n• Flat-VAE: We train an LSTM VAE (Kingma &\nWelling, 2013) to reconstruct demonstrations. This rep-\nresents a ﬂat, non-hierarchical baseline with a learned\nrepresentation. The architecture used is an 8 layer\nLSTM with 128 hidden units, like our policies.\n• Flat-DMP: We ﬁt a single Dynamic Movement Prim-\nitive (Ijspeert et al., 2013) to each trajectory in the\ndataset. This represents a non-hierarchical baseline\nwith a predeﬁned trajectory representation.\n• H-DMP: We evaluate a hierarchical DMP based ap-\nproach similar to Niekum et al. (2012), that ﬁrst seg-\nments a trajectory by detecting changepoints in accel-\neration, and then ﬁts individual DMPs to each segment\nof the trajectory. This represents a hierarchical baseline\nwith a predeﬁned trajectory representation.\n• Disco-MP: We also evaluate the method of Shankar\net al. (2020), which is directly optimized for trajectory\nreconstruction. This approach represents a hierarchical\nbaseline with a learned trajectory representation.\n• Ours: We obtain predicted trajectories from our model\nobtaining the latent z’s that occur in a demonstration\nfrom our q network, and rolling out the low-level policy\nwith these z’s as input.\nWe present the average state distances obtained by our ap-\nproach against these baselines in table 1. The ﬂat VAE is\nable to achieve a reasonably low reconstruction error, indi-\ncating the beneﬁts of a learnt trajectory representation over\na predeﬁned representation such as DMPs. Combinining a\nlearnt trajectory representation with the ability to compose\nprimitives naturally leads to a further decrease in the tra-\njectory reconstruction error, as observed in the Disco-MP\nbaseline and our approach. Note that our approach is able to\nachieve a similar reconstruction error to that of Disco-MP,\nwhich is explicitly optimized to minimize (aligned) state\ndistances, as well as the H-DMP baseline, which heavily\noverﬁts to a single trajectory (thus promising low recon-\nstruction error). This demonstrates that our approach indeed\ncaptures the overall structure of demonstrations and is able\nto represent them faithfully. These trends are consistently\nobserved across all three datasets we evaluate on.\nTo qualitatively analyse how well our approach captures the\nstructure of demonstration, we visualize the reconstructed\ntrajectories predicted by our approach against the corre-\nsponding ground truth trajectory. These results are presented\nin our website:\nhttps://sites.google.com/view/learning-\ncausal-skills/home. We observe that our approach is indeed\nable to capture the rough sequence of skills that occur in\nthe demonstrated trajectories. In the case of the MIME\nand Roboturk datasets, our model predicts similar reaching,\nreturning, sliding etc. primitives when the ground truth tra-\nLearning Robot Skills with Temporal Variational Inference\nTable 2. Average Rewards of our approach and baselines on various RL environments, across 100 episodes. Baselines are adapted from\n(1) Lillicrap et al. (2015), (2) Esmaili et al. (1995), (3) Kulkarni et al. (2016).\nMethod\nSawyerPick-\nSawyerPick-\nSawyerPick-\nSawyerPick-\nSawyerNut-\nSawyerNut-\nPlaceBread\nPlaceCan\nPlaceCereal\nPlaceMilk\nAssemblyRound\nAssemblySquare\nFlat RL [1]\n0.11\n0.34\n0.18\n0.14\n0.44\n0.55\nFlat IL [2]\n0.60\n0.65\n0.29\n0.67\n0.49\n1.87\nHierarchical RL (No Init) [3]\n0.13\n0.11\n0.04\n0.15\n0.36\n1.16\nHierarchical RL (W/ Init) [3]\n0.41\n0.37\n0.22\n0.34\n0.54\n1.09\nOurs\n1.54\n1.22\n1.54\n0.48\n1.88\n3.62\njectory executes a corresponding primitives. Further, the\nrough shape of the arms during the predicted skills corre-\nlate strongly with the shapes observed in the ground truth\ntrajectories; this is consistent with the quantitative results\npresented above. Our approach also notably captures ﬁne\nmotions (such as opening and closing of the gripper) in\ntrajectories well. In case of the Mocap dataset, the overall\nshape and trend of rolled out trajectories align very closely\nwith the original demonstration, showing the use of our ap-\nproach in learning skills across widely differently structured\ndata and morphology of agents.\n4.3. Downstream Tasks\nWe would also like to evaluate how useful our learned poli-\ncies are for solving a set of target tasks. This is central to\nour motivation of jointly learning options and how to use\nthem. We test our learned policies on a suite of 6 simulated\nrobotic tasks from the Robosuite (Mandlekar et al., 2018)\nenvironment on the Sawyer robot. Since the MIME dataset\nlacks any object information, we disregard the Baxter tasks\nin Robosuite (Mandlekar et al., 2018), and instead use tasks\nfrom Robosuite for which the Roboturk dataset has demon-\nstrations. In the pick-place tasks, a reward of 1 is given to\nobjects successfully placed in the bin. In the nut-and-peg as-\nsembly tasks, a reward of 1 is given for successfully placing\na nut. Both tasks also have additional rewards based on con-\nditions of how the task was executed. We point the reader to\n(Mandlekar et al., 2018) for a full description of these tasks\nand their reward structure. We compare the performance of\nour method on these tasks against the following baselines:\n• Flat RL: We train ﬂat policies on each of the 6 tasks\nin the reinforcement learning setting, using DDPG\n(Lillicrap et al., 2015).\n• Flat IL: We train ﬂat policies to mimic the actions ob-\nserved in the demonstrations of each of the 6 tasks, and\nsubsequently ﬁnetune these policies in the RL setting.\n• Hierarchical RL: We train hierarchical policies as in\n(Kulkarni et al., 2016) in the pure RL setting, with (W/\nInit) and without (No Init) any prior initialization.\n• Ours: We ﬁne-tune the low and high-level policies\nobtained by our model in the RL setting.\nAll baseline policies are implemented as 8 layer LSTMs\nwith 128 hidden units, for direct comparison with our poli-\ncies. The RL based approaches are trained with DDPG\nwith the same exploration processes and hyperparameters\n(such as initializations of the networks, learning rates used,\netc.), as noted in the supplementary. We evaluate each of\nthese baselines along with our approach by testing out the\nlearned policies over 100 episodes in the 6 environments,\nand reporting the average rewards obtained in table 2.\nWe observe that the Flat RL and Hierarchical RL base-\nlines are unable to solve these tasks, and achieve low re-\nwards across all tasks. This is unsurprising given the difﬁ-\nculty of exploration problem underlying these tasks (Man-\ndlekar et al., 2018). Pretraining policies with imitation\nlearning somewhat allieviates this problem, as observed in\nthe slightly higher rewards of the Flat IL baseline. This\nis likely because the policies are biased towards actions\nsimilar to those seen in the demonstrations. Training hi-\nerarchical policies initialized with our approach is able to\nachieve signiﬁcantly higher rewards than both the IL and\nRL baselines consistently across most environments. By\nproviding these policies with suitable notions of skills that\nextend over several timesteps, we are able to bypass rea-\nsoning over low-level actions for 100’s of timesteps, thus\nguiding exploration in these tasks more efﬁciently.\n5. Conclusion\nIn this paper, we presented a framework for jointly learning\nrobotic skills and how to use them from demonstrations in\nan unsupervised manner. Our temporal variational inference\nallows us to construct an objective that directly affords us\nusable policies on optimization. We are able to learn se-\nmantically meaningful skills that correspond closely with\nthe traditional notions of skills observed in manipulation.\nFurther, our approach is able to capture the overall structure\nof demonstrations in terms of the learned skills. We hope\nthat these factors contribute towards accelerating research\nin robot learning for manipulation.\nLearning Robot Skills with Temporal Variational Inference\nAcknowledgements\nThe authors would like to thank Shubham Tulsiani for valu-\nable discussions on the formulation of the approach, and\nJungdam Won and Deepak Gopinath for help with data\nprocessing and visualization for the Mocap dataset.\nReferences\nAndreas, J., Klein, D., and Levine, S. Modular multitask\nreinforcement learning with policy sketches. In ICML,\n2017.\nArgall, B. D., Chernova, S., Veloso, M., and Browning, B.\nA survey of robot learning from demonstration. Robotics\nand autonomous systems, 2009.\nBacon, P.-L., Harb, J., and Precup, D. The option-critic\narchitecture. In AAAI, 2017.\nCMU. Cmu graphics lab motion capture database. 2002.\nURL http://mocap.cs.cmu.edu.\nCo-Reyes, J. D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel,\nP., and Levine, S. Self-consistent trajectory autoencoder:\nHierarchical reinforcement learning with trajectory em-\nbeddings. arXiv preprint arXiv:1806.02813, 2018.\nEsmaili, N., Sammut, C., and Shirazi, G.\nBehavioural\ncloning in control of a dynamic system. IEEE, 1995.\nFikes, R. E. and Nilsson, N. J. Strips: A new approach to\nthe application of theorem proving to problem solving.\nArtiﬁcial intelligence, 1971.\nFox, R., Krishnan, S., Stoica, I., and Goldberg, K.\nMulti-level discovery of deep options. arXiv preprint\narXiv:1703.08294, 2017.\nGregor, K., Papamakarios, G., Besse, F., Buesing, L., and\nWeber, T. Temporal difference variational auto-encoder.\nIn International Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=S1x4ghC9tQ.\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,\nBotvinick, M., Mohamed, S., and Lerchner, A. beta-\nvae: Learning basic visual concepts with a constrained\nvariational framework.\nHochreiter, S. and Schmidhuber, J. Long short-term mem-\nory. Neural Comput., 9(8):17351780, November 1997.\nISSN 0899-7667.\ndoi: 10.1162/neco.1997.9.8.1735.\nURL https://doi.org/10.1162/neco.1997.\n9.8.1735.\nHuang, D.-A., Nair, S., Xu, D., Zhu, Y., Garg, A., Fei-\nFei, L., Savarese, S., and Niebles, J. C. Neural task\ngraphs: Generalizing to unseen tasks from a single video\ndemonstration. In CVPR, 2019.\nIjspeert, A. J., Nakanishi, J., Hoffmann, H., Pastor, P., and\nSchaal, S. Dynamical movement primitives: learning at-\ntractor models for motor behaviors. Neural computation,\n25(2):328–373, 2013.\nKim, T., Ahn, S., and Bengio, Y. Variational temporal ab-\nstraction. In Advances in Neural Information Processing\nSystems 32, pp. 11566–11575. Curran Associates, Inc.,\n2019.\nURL http://papers.nips.cc/paper/\n9332-variational-temporal-abstraction.\npdf.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes, 2013. URL http://arxiv.org/abs/1312.\n6114. cite arxiv:1312.6114.\nKipf, T., Li, Y., Dai, H., Zambaldi, V., Sanchez-Gonzalez,\nA., Grefenstette, E., Kohli, P., and Battaglia, P. Compile:\nCompositional imitation learning and execution. In ICML,\n2019.\nKober, J. and Peters, J.\nLearning motor primitives for\nrobotics. In ICRA, 2009.\nKonidaris, G. and Barto, A. Skill chaining: Skill discovery\nin continuous domains. In the Multidisciplinary Sym-\nposium on Reinforcement Learning, Montreal, Canada,\n2009.\nKonidaris, G., Kuindersma, S., Grupen, R., and Barto, A.\nRobot learning from demonstration by constructing skill\ntrees. IJRR, 2012.\nKramer, G. Directed information for channels with feedback.\n1998.\nKrishnan, S., Fox, R., Stoica, I., and Goldberg, K. Ddco:\nDiscovery of deep continuous options for robot learning\nfrom demonstrations. arXiv preprint arXiv:1710.05421,\n2017.\nKrishnan, S., Garg, A., Patil, S., Lea, C., Hager, G., Abbeel,\nP., and Goldberg, K. Transition state clustering: Unsuper-\nvised surgical trajectory segmentation for robot learning.\nIn RR. 2018.\nKulkarni, T. D., Narasimhan, K. R., Saeedi, A., and Tenen-\nbaum, J. B. Hierarchical deep reinforcement learning:\nIntegrating temporal abstraction and intrinsic motiva-\ntion. In Proceedings of the 30th International Conference\non Neural Information Processing Systems, NIPS16, pp.\n36823690, Red Hook, NY, USA, 2016. Curran Associates\nInc. ISBN 9781510838819.\nLearning Robot Skills with Temporal Variational Inference\nLeslie Pack Kaelbling, T. L.-P. Learning composable models\nof parameterized skills. In IEEE Conference on Robotics\nand Automation (ICRA), 2017.\nURL http://lis.\ncsail.mit.edu/pubs/lpk/ICRA17.pdf.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nLioutikov, R., Neumann, G., Maeda, G., and Peters, J.\nLearning movement primitive libraries through prob-\nabilistic segmentation.\nThe International Journal of\nRobotics Research, 36(8):879–894, 2017. doi: 10.1177/\n0278364917713116. URL https://doi.org/10.\n1177/0278364917713116.\nLioutikov, R., Maeda, G., Veiga, F., Kersting, K., and\nPeters, J. Learning attribute grammars for movement\nprimitive sequencing.\nThe International Journal of\nRobotics Research, 39(1):21–38, 2020. doi: 10.1177/\n0278364919868279. URL https://doi.org/10.\n1177/0278364919868279.\nMandlekar, A., Zhu, Y., Garg, A., Booher, J., Spero, M.,\nTung, A., Gao, J., Emmons, J., Gupta, A., Orbay, E.,\nSavarese, S., and Fei-Fei, L. Roboturk: A crowdsourcing\nplatform for robotic skill learning through imitation. In\nConference on Robot Learning, 2018.\nMeier, F., Theodorou, E., Stulp, F., and Schaal, S. Move-\nment segmentation using a primitive library. In 2011\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems, 2011.\nMuelling, K., Kober, J., and Peters, J. Learning table tennis\nwith a mixture of motor primitives. In 2010 10th IEEE-\nRAS International Conference on Humanoid Robots, pp.\n411–416, Dec 2010. doi: 10.1109/ICHR.2010.5686298.\nMurali, A., Garg, A., Krishnan, S., Pokorny, F. T., Abbeel,\nP., Darrell, T., and Goldberg, K. Tsc-dl: Unsupervised\ntrajectory segmentation of multi-modal surgical demon-\nstrations with deep learning. In ICRA, 2016.\nMlling, K., Kober, J., Kroemer, O., and Peters, J.\nLearning to select and generalize striking movements\nin robot table tennis.\nThe International Journal of\nRobotics Research, 32(3):263–279, 2013. doi: 10.1177/\n0278364912472380. URL https://doi.org/10.\n1177/0278364912472380.\nNeumann, G., Daniel, C., Paraschos, A., Kupcsik, A., and\nPeters, J. Learning modular policies for robotics. Fron-\ntiers in computational neuroscience, 8:62, 2014.\nNiekum, S., Osentoski, S., Konidaris, G., and Barto, A. G.\nLearning and generalization of complex tasks from un-\nstructured demonstrations. IEEE, 2012.\nPeters, J., Kober, J., M¨ulling, K., Kr¨amer, O., and Neumann,\nG. Towards robot skill learning: From simple skills to\ntable tennis. In Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases, pp.\n627–631. Springer, 2013.\nSchaal, S. Learning from demonstration. In Advances\nin Neural Information Processing Systems 9,\npp.\n1040–1046, Cambridge, MA, 1997. MIT Press. URL\nhttp://www-clmc.usc.edu/publications/\nS/schaal-NIPS1997.pdf. clmc.\nShankar, T., Tulsiani, S., Pinto, L., and Gupta, A. Discover-\ning motor programs by recomposing demonstrations. In\nInternational Conference on Learning Representations,\n2020. URL https://openreview.net/forum?\nid=rkgHY0NYwr.\nSharma, M., Sharma, A., Rhinehart, N., and Kitani, K. M.\nDirected-info GAIL: learning hierarchical policies from\nunsegmented demonstrations using directed information.\nIn 7th International Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019,\n2019. URL https://openreview.net/forum?\nid=BJeWUs05KQ.\nSharma, P., Mohan, L., Pinto, L., and Gupta, A. Multiple in-\nteractions made easy (mime): Large scale demonstrations\ndata for imitation. In CoRL, 2018.\nShiarlis, K., Wulfmeier, M., Salter, S., Whiteson, S.,\nand Posner, I.\nTaco: Learning task decomposition\nvia temporal alignment for control.\narXiv preprint\narXiv:1803.01840, 2018.\nSmith, M., Hoof, H., and Pineau, J. An inference-based\npolicy gradient method for learning options. In ICML,\n2018.\nSutton, R. S., Precup, D., and Singh, S. Between mdps\nand semi-mdps: A framework for temporal abstraction in\nreinforcement learning. Artiﬁcial intelligence, 1999.\nvan der Maaten, L. and Hinton, G.\nVisualizing high-\ndimensional data using t-sne. 2008.\nWilliams, R. J. Simple statistical gradient-following algo-\nrithms for connectionist reinforcement learning. Machine\nlearning, 1992.\nXu, D., Nair, S., Zhu, Y., Gao, J., Garg, A., Fei-Fei, L.,\nand Savarese, S. Neural task programming: Learning to\ngeneralize across hierarchical tasks. In ICRA, 2018.\nZiebart, B. D., Bagnell, J. A., and Dey, A. K. The principle\nof maximum causal entropy for estimating interacting\nprocesses. IEEE Transactions on Information Theory,\n59(4):1966–1980, April 2013. ISSN 1557-9654. doi:\n10.1109/TIT.2012.2234824.\nLearning Robot Skills with Temporal Variational Inference\n6. Appendix\nWe provide additional details and insights into our approach below.\n6.1. Derivation of Temporal Variational Inference:\nWhile our main paper presents the speciﬁc temporal variational inference that we use to learn policies, the notion of temporal\nvariational inference is more generally applicable to sequential data with hidden states (such as in HMMs).\nWe present a detailed derivation of this general temporal variational inference objective, and contrast it with standard\nvariational inference below. We then provide a detailed derivation of the gradient of this objective in our case, explaining\nhow dependencies on system dynamics may be factored out.\nWe begin by considering observed sequential data x = {xt}t\nt=1 (in our case, this corresponds to state-action tuples, i.e.\n{xt = (st, at)}T\nt=1), interacting with a sequence of unobserved latent variables y = {yt}T\nt=1 (in our case, yt is simply ζt).\nFor these sequences of data, the true likelihood of observed data L = Ex∼p(x)\n\u0002\nlog p(x)\n\u0003\nis lower bounded by J, where:\nJ = Ex∼p(x)\n\u0002\nlog p(x)\n\u0003\n−DKL\n\u0002\nq(y|x)||p(y|x)\n\u0003\nwhere q(y|x) is a variational approximation to the true conditional p(y|x), and the lower bounded due to the non-negativity\nof KL divergence. Expanding the KL divergence term, and using the fact that p(x, y) = p(y|x)p(x), we have:\nJ = Ex∼p(x)\n\u0002\nlog p(x)\n\u0003\n−Ex∼p(x),y∼q(y|x)\nh\nlog q(y|x)\np(y|x)\ni\nJ = Ex∼p(x),y∼q(y|x)\nh\nlog p(x, y) −log q(y|x)\ni\nStandard variational inference then decomposes the joint likelihood p(x, y) into a “decoder” p(x|y) and a prior p(y):\nJStandardVI = Ex∼p(x),y∼q(y|x)\nh\nlog p(x|y) + log p(y) −log q(y|x)\ni\nGiven the sequential nature of x and y, inferring the conditional p(x|y) does not provide a useful insight into how the\nsequence of x and y will evolve, and is often difﬁcult to learn. In contrast, our temporal variational inference makes use of\nthe following decomposition of joint likelihood p(x, y):\np(x, y) =\nT\nY\nt=1\np(xt|x1:t−1, y1:t−1)\n|\n{z\n}\n= p(x||y)\nT\nY\nt=1\np(yt|x1:t, y1:t−1)\n|\n{z\n}\n= p(y||x)\np(x||y) and p(y||x) are causally conditioned distributions, i.e. they only depend on information available until the current\ntimestep. p(x||y) and p(y||x) are formally deﬁned as QT\nt=1 p(xt|x1:t−1, y1:t−1) and QT\nt=1 p(yt|x1:t, y1:t−1) respectively\n(Ziebart et al., 2013). We can plug this decomposition into the objective J to give us the temporal variational inference\nobjective:\nJTemporalVI = Ex∼p(x),y∼q(y|x)\nh\nlog p(x||y) + log p(y||x) −log q(y|x)\ni\n= Ex∼p(x),y∼q(y|x)\nh\nlog\nT\nY\nt=1\np(xt|x1:t−1, y1:t−1) + log\nT\nY\nt=1\np(yt|x1:t, y1:t−1) −log q(y|x)\ni\n= Ex∼p(x),y∼q(y|x)\nh\nT\nX\nt=1\nlog p(xt|x1:t−1, y1:t−1) +\nT\nX\nt=1\nlog p(yt|x1:t, y1:t−1) −log q(y|x)\ni\nLearning Robot Skills with Temporal Variational Inference\n6.2. Derivation of Gradient Update:\nNow that we have a better understanding of the origin of the temporal variational inference objective, we present a derivation\nof the gradient to this objective in our case. In our option learning setting, the TVIobjective is:\nJTemporalVI = Eτ∼D,ζ∼q(ζ|τ)\nh\nT\nX\nt=1\n\b\nlog η(ζt|s1:t, a1:t−1, ζ1:t−1) + log π(at|s1:t, a1:t−1, ζ1:t)\n+ log p(st+1|st, at)\n\t\n+ log p(s1) −log q(ζ|τ)\ni\nAssuming distributions π, η, and q are parameterized by θ, φ, and ω respectively, the gradient of this objective is:\n∇θ,φ,ωJTemporalVI = ∇θ,φ,ω Eτ∼D,ζ∼q(ζ|τ)\nh\nT\nX\nt=1\n\b\nlog η(ζt|s1:t, a1:t−1, ζ1:t−1) + log π(at|s1:t, a1:t−1, ζ1:t)\n+ log p(st+1|st, at)\n\t\n+ log p(s1) −log q(ζ|τ)\ni\nNote that the system dynamics p(st+1|st, at) and the initial state distribution p(s1) are both independent of the parameteri-\nzation of networks θ, φ, and ω. The gradient of the objective with respect to θ, φ, and ω can be separated into additive terms\nthat depend on the dynamics and initial state distributions, and terms that depend on networks π, η, and q. The expectation\nof the dynamics and initial state distribution terms are constant, and their gradient hence vanishes:\n∇θ,φ,ωJTemporalVI = ∇θ,φ,ω Eτ∼D,ζ∼q(ζ|τ)\nh\nT\nX\nt=1\nlog p(st+1|st, at) + log p(s1)\ni\n|\n{z\n}\nConstant\n= 0\nThe gradient update of temporal variational inference hence doesn’t depend on the dynamics and the initial state distribution,\nleading to the following gradient update, as presented in the main paper in equation 3:\n∇θ,φ,ωJ = ∇θ,φ,ωEτ∼D,ζ∼q(ζ|τ)\nh X\nt\nlog π(at|s1:t, a1:t−1, ζ1:t) +\nX\nt\nlog η(bt, zt|s1:t, a1:t−1, ζ1:t−1) −log q(ζ|τ)\ni\n6.3. Implementation Details:\nWe make note of several implementation details below, such as network architectures and hyperparameter settings.\n6.3.1. NETWORK ARCHITECTURE:\nWe describe the speciﬁc architectures of each of the networks q, π, and η in our approach. The base architecture for each\nof these three networks is an 8 layer LSTM with 128 hidden units in each layer. We found that an 8 layer LSTM was\nsufﬁciently expressive for represent the distributions q, π, and η.\nThe space of predictions for q and η are the binary termination variables bt at every timestep t, and the continuous\nparameterization of options z. q and η are thus implemented with two heads on top of the ﬁnal LSTM layer.\nThe ﬁrst head predicts termination probabilities (from which termination variables bt are sampled), and consists of a linear\nlayer of output size 2 followed by a softmax layer to predict probabilities.\nThe second head of the network predicts the latent z parameterization at each timestep. It consists of two separate linear\nlayers above the ﬁnal LSTM layer, that predict the mean and variance of a Gaussian distribution respectively. The mean\npredictions do not use an activation layer. The variance predictions employ a SoftPlus activation function to predict positive\nvariances. The dimensionality of latent z’s is 64 across all three datasets and across all networks.\nThe prediction space for π is the continuous low-level actions (i.e. joint velocities). Similar to the z prediction, this is\nimplemented by with 2 separate linear layers to predict the mean and variances of a Gaussian distribution, from which\nactions are drawn. As above, variances use a SoftPlus activation, while mean predictions are done without an activation.\nThe dimensions of the action space are 16 for the MIME dataset, 8 for the Roboturk dataset, and 66 for the Mocap dataset.\nLearning Robot Skills with Temporal Variational Inference\n6.3.2. HYPERPARAMETERS:\nWe provide a list of the hyperparameters and their values, and other training details used in our training procedure.\n1. Optimizer: We use the Adam optimizer (Kingma & Ba, 2014) to train all networks in our model.\n2. Learning Rate: We use a learning rate of 10−4 for our optimizer, as is standard.\n3. Epsilon: The exploration parameter ϵ is used in our training procedure to both scale perturbation of latent z’s sampled\nfrom our model, as well as to explore different latent b’s in an epsilon-greedy fashion. We use an initial ϵ value of 0.3,\nand linearly decay this value to 0.05 over 30 epochs of training, and found this works well. We considered a range of\n0.1 −0.3 for the initial value of epsilon, and a range of 0.05 −0.1 for the ﬁnal epsilon.\n4. Ornstein Uhlenbeck Noise Parameters: Our DDPG implementation for the RL training uses the Ornstein Uhlenbeck\nnoise process, with parameters identical to those used in the DDPG paper (Lillicrap et al., 2015).\n5. Loss Weights: In practice, the various terms in our objective are reweighted prior to gradient computation, to facilitate\nlearning the desired behaviors and to prevent particular terms from dominating others.\n(a) Option likelihood weight:\nDuring initial phases of training, we reweight the option likelihood term\nP\nt log η(ζt|s1:t, a1:t−1, ζ1:t−1) in our gradient update by a factor of 0.01, to prevent the variational network\nfrom getting inconsistent gradients from the randomly initialized η. Once the variational policy has been trained\nsufﬁciently, we set the weight of this option likelihood to 1.\n(b) KL Divergence weight: We reweight the KL divergence term, as done in the β-VAE paper (Higgins et al.), by a\nfactor of 0.01.\n6.3.3. RL DETAILS:\nFor our reinforcement learning experiments, we use variants of the following Robosuite (Mandlekar et al., 2018) environ-\nments to evaluate our approach:\n• SawyerPickPlace - An environment where a sawyer robot grasps and places objects in speciﬁc positions in respec-\ntive bins. We use 4 variants of this task, SawyerPickPlaceBread, SawyerPickPlaceCereal, SawyerPickPlaceCan,\nSawyerPickPlaceMilk, where the objective is to place the corresponding object into the correct bin.\n• SawyerNutAssembly - An environment where a sawyer robot must pick a nut up and place it around an appropriately\nshaped peg. We use 2 variants of this task, SawyerNutAssemblySquare and SawyerNutAssemblyRound, where the\nshape of the nut and peg are varied.\nThe 3 baseline algorithms speciﬁed in the main paper and our approach all share the same policy architectures (i.e., an 8\nlayer LSTM with 128 hidden units) for both low-level policies (in all baselines and our approach) and high-level policies\n(our approach and the hierarchical RL baseline).\nFor these pick-place and nut-assembly tasks, the information available to the policies are the sequence of joint states of the\nrobot, previous joint velocities executed, the robot-state provided by Robosuite (consists of sin and cos of the joint\nangles, gripper positions, etc.), and the object-state provided by Robosuite (consisting of absolute positions of the\ntarget objects, object positions relative to the robot end-effector, etc.). The output space for the policies is always the joint\nvelocities (including the gripper).\n6.3.4. DATASET DETAILS:\nRegarding the CMU Mocap dataset, the data used in this project was obtained from mocap.cs.cmu.edu, the database was\ncreated with funding from NSF EIA-0196217.\n",
  "categories": [
    "cs.LG",
    "cs.RO",
    "stat.ML"
  ],
  "published": "2020-06-29",
  "updated": "2020-06-29"
}