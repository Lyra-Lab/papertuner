{
  "id": "http://arxiv.org/abs/2007.01544v2",
  "title": "A Conceptual Framework for Externally-influenced Agents: An Assisted Reinforcement Learning Review",
  "authors": [
    "Adam Bignold",
    "Francisco Cruz",
    "Matthew E. Taylor",
    "Tim Brys",
    "Richard Dazeley",
    "Peter Vamplew",
    "Cameron Foale"
  ],
  "abstract": "A long-term goal of reinforcement learning agents is to be able to perform\ntasks in complex real-world scenarios. The use of external information is one\nway of scaling agents to more complex problems. However, there is a general\nlack of collaboration or interoperability between different approaches using\nexternal information. In this work, while reviewing externally-influenced\nmethods, we propose a conceptual framework and taxonomy for assisted\nreinforcement learning, aimed at fostering collaboration by classifying and\ncomparing various methods that use external information in the learning\nprocess. The proposed taxonomy details the relationship between the external\ninformation source and the learner agent, highlighting the process of\ninformation decomposition, structure, retention, and how it can be used to\ninfluence agent learning. As well as reviewing state-of-the-art methods, we\nidentify current streams of reinforcement learning that use external\ninformation in order to improve the agent's performance and its decision-making\nprocess. These include heuristic reinforcement learning, interactive\nreinforcement learning, learning from demonstration, transfer learning, and\nlearning from multiple sources, among others. These streams of reinforcement\nlearning operate with the shared objective of scaffolding the learner agent.\nLastly, we discuss further possibilities for future work in the field of\nassisted reinforcement learning systems.",
  "text": "Cite as: Bignold, A., Cruz, F., Taylor, M., Brys, T., Dazeley, R., Vamplew, P., Foale, C. (2021).\nA Conceptual Framework for Externally-inﬂuenced Agents:\nAn Assisted Reinforcement Learning\nReview. Journal of Ambient Intelligence and Humanized Computing.\nA Conceptual Framework for Externally-inﬂuenced Agents:\nAn Assisted Reinforcement Learning Review\nAdam Bignold1,∗\nFrancisco Cruz2,3,∗\nMatthew E. Taylor4\nTim Brys5\nRichard Dazeley2\nPeter Vamplew1\nCameron Foale1\n1 School of Engineering, IT and Physical Sciences, Federation University, Ballarat, Australia.\n2 School of Information Technology, Deakin University, Geelong, Australia.\n3 Escuela de Ingenier´ıa, Universidad Central de Chile, Santiago, Chile.\n4 Department of Computing Science and The Alberta Machine Intelligence Institute (Amii), University of\nAlberta, Edmonton, AB, Canada.\n5 Action Research Associates, Beirut, Lebanon.\n∗Both authors contributed equally to this manuscript.\nCorresponding e-mails: {a.bignold, p.vamplew, c.foale}@federation.edu.au,\n{francisco.cruz, richard.dazeley}@deakin.edu.au, matthew.e.taylor@ualberta.ca,\ntbrys@actionresearchassociates.org\nAbstract\nA long-term goal of reinforcement learning agents is to\nbe able to perform tasks in complex real-world scenar-\nios. The use of external information is one way of scal-\ning agents to more complex problems. However, there\nis a general lack of collaboration or interoperability be-\ntween diﬀerent approaches using external information.\nIn this work, while reviewing externally-inﬂuenced\nmethods, we propose a conceptual framework and\ntaxonomy for assisted reinforcement learning, aimed\nat fostering collaboration by classifying and comparing\nvarious methods that use external information in the\nlearning process. The proposed taxonomy details the\nrelationship between the external information source\nand the learner agent, highlighting the process of in-\nformation decomposition, structure, retention, and\nhow it can be used to inﬂuence agent learning. As\nwell as reviewing state-of-the-art methods, we identify\ncurrent streams of reinforcement learning that use\nexternal information in order to improve the agent’s\nperformance and its decision-making process. These\ninclude heuristic reinforcement learning, interactive\nreinforcement learning, learning from demonstration,\ntransfer learning, and learning from multiple sources,\namong others. These streams of reinforcement learn-\ning operate with the shared objective of scaﬀolding the\nlearner agent. Lastly, we discuss further possibilities\nfor future work in the ﬁeld of assisted reinforcement\nlearning systems.\nKeywords:\nAssisted\nreinforcement\nlearning,\nExternally-inﬂuenced agents, Assistance taxonomy.\n1\nIntroduction\nReinforcement learning (RL) [1] is a learning approach\nin which an agent uses sequential decisions to interact\nwith its environment trying to ﬁnd a (near-) opti-\nmal policy to perform an intended task. RL agents\nhave the ability to improve while operating, to learn\nwithout supervision, and to adapt to changing cir-\ncumstances [2]. By exploring, a standard agent learns\nsolely from the signals it receives from the environ-\nment. The RL approach has shown success in domains\nsuch as robotics [3, 4, 5, 6], game-playing [7, 8], inven-\ntory management [9], and cloud computing [10, 11, 12],\namong others.\nThis preprint has not undergone peer review or any post-submission improvements or corrections. The Version of\nRecord of this article is published in the Journal of Ambient Intelligence and Humanized Computing (JAIHC), and is\navailable online at https://doi.org/10.1007/s12652-021-03489-y.\narXiv:2007.01544v2  [cs.AI]  20 Sep 2021\nLike many machine learning techniques, RL faces\nthe problem of high-dimensionality spaces. As envi-\nronments become larger, the agent’s learning time\nincreases and ﬁnding the optimal solution becomes\nimpractical [13]. Early research on this topic [2, 14]\nargued that for RL to successfully scale into real-world\nscenarios, then the use of information external to the\nenvironment would be needed. Diﬀerent RL strategies\nusing this approach have emerged in order to speed\nup the learning process. They use external informa-\ntion to assist either the process of generalising the\nenvironment representation [15], the agent’s decision-\nmaking process [16], or in providing more focused\nexploration [17].\nIn this article, we refer to external information as\nany kind of information provided to the agent orig-\ninating from outside of the agent’s representation\nof the environment. This may include demonstra-\ntions [18, 19, 20], advice and critiques [21, 16], ini-\ntial bias based on previously gathered data [22], or\nhighly-detailed domain-speciﬁc shaping functions [23].\nAdditionally, in this work, we use independently the\nconcepts of RL approach, method, and technique to\nrefer to the underlying learning algorithm.\nThese\nconcepts have been previously used mostly equally by\nthe RL research community.\nIn this regard, we deﬁne Assisted reinforcement\nlearning (ARL) as a range of techniques that use\nexternal information, either before, during, or after\ntraining, to improve the performance of the learner\nagent, as well as to scale RL to larger and more com-\nplex scenarios. While a relevant characteristic of RL\nis its ability to endow agents with new skills from\nthe ground up, ARL also makes use of existing infor-\nmation and/or previously learned behaviour. Some\nmethods for improving the agent’s performance using\nexternal information include: directly altering weights\nfor actions and states (biasing) [24]; altering the state\nor action space [25]; critiquing past or advising on\nfuture decision-making [26]; dynamically altering re-\nward functions [21]; directly modifying the policy [16];\nguiding exploration and action selection [17]; and,\ncreating information repositories/models to supple-\nment the environmental information [15]. Figure 1\ncaptures all of these methods in a basic view of the\nARL conceptual framework used in this work. The\nEnvironment\nAgent\nrt+1\nst+1\naction at\nstate st , reward rt\nExternal\ninformation source\nadvice\nReinforcement Learning\nAssisted Reinforcement Learning\nmodiﬁcation\nFigure 1: Assisted reinforcement learning simpliﬁed\nframework. In autonomous reinforcement learning, an\nagent performs an action at from a state st and the\nenvironment produces an answer leading the agent to\na new state st+1 and receiving a reward rt+1. Assisted\nreinforcement learning adds an external information\nsource, referred to as a trainer, teacher, advisor or\nassistant, that observes the environment and the agent\nin order to generate advice. The trainer may advise\nthe learner agent or sometimes directly modify the\nenvironment. Moreover, the agent may also actively\nask advice to the external information source.\nclassic RL approach is shown within the ﬁgure where\nan agent performs an action on the environment reach-\ning a new state and obtaining a reward. In ARL, the\nresponse of the environment is also shared with the\nexternal information source from where advice is given\nto the agent or changes sometimes made directly to\nthe environment [27].\nTo date, many methods using external information\nhave been proposed aiming to speed up the learn-\ning process for an autonomous agent [28, 29, 30, 31].\nUsually, they have been organized according to the\ntechnique employed, e.g., heuristic, interactive, or\ntransfer learning, among others. Nevertheless, there\n2\nis an important lack of understanding of how these\ntechniques are related and what characteristics they\nshare. Therefore, in this review, we present a con-\nceptual framework and a taxonomy to be used to\ndescribe the practice of using external information. A\nstandardised ARL taxonomy will foster collaboration\nbetween diﬀerent RL communities, improve compara-\nbility, allow a precise description of new approaches,\nand assist in identifying and addressing key questions\nfor further research.\n2\nA Conceptual Framework\nfor Assisted Reinforcement\nLearning\nIn this section, we give more details about the ARL ap-\nproach including some introductory examples of works\nin which external information sources have been used.\nMoreover, we deﬁne a conceptual framework identify-\ning the diﬀerent parts that comprise the underlying\nprocess used in ARL techniques. Based on this con-\nceptual framework, in the following section, we deﬁne\na more detailed taxonomy for ARL approaches.\n2.1\nAssisted Reinforcement Learning\nThe main strength of RL is its ability for endowing\nan agent with new skills given no initial knowledge\nabout the environment. With an appropriate reward\nfunction and enough interaction with its environment,\nan RL agent can learn (near-) optimal behaviour [1].\nThe agent’s behaviour at every step is deﬁned by\nits policy. The reward function promotes desirable\nbehaviour and sometimes penalises undesirable be-\nhaviour. In the traditional view of RL, the reward\nfunction, and the rewards it produces, are internal\nto the environment [2]. Traditional RL, in which the\nenvironment is the sole provider of information to\nthe agent, has been demonstrated to perform well in\nmany diﬀerent domains, especially when facing small\nand bounded problems [1]. However, RL has some\ndiﬃculties when scaling up to large, unbounded envi-\nronments, particularly regarding the time needed for\nthe agent to learn the optimal policy [32, 33]. In RL,\none approach to tackling this issue is to use external\ninformation to supplement the information that the\nenvironment provides [34, 35].\nInformation is considered external if it originates\nfrom outside of the agent’s interactions with the en-\nvironment. In this regard, internal information is de-\ntermined solely through interactions and observations\nwith the environment. For example, in the case of a\nhuman the internal information would be anything\nthe person can observe from the environment using\ntheir senses [36]. The external information would be\nany information provided by peers, advisors, the in-\nternet, books, maps, and tutelage. In RL, anything\nexternal to the agent is usually considered part of the\nenvironment. In this regard, if an agent is learning in\nan environment, a person can be considered as part\nof it, therefore, the agent could model that person\nor communicate with them [37]. Although it is pos-\nsible that external sources of information could be\njust treated as part of the environment, this is handi-\ncapping the agent in an unnecessary way. There are\nexternal sources of information that might not neces-\nsarily be treated as part of the environment because\nthey are socially advantaged. For instance, if an exter-\nnal source is providing action advice using directions\nas ‘left’ and ‘right’, the agent does not have to learn\nthe meaning of these words from the ground up, or\nlearn how to react to these instructions. Instead, we\nassume the agent knows that advice is coming, what\nit means, and how to use it. For example, if a person\neats some berries and later becomes sick, the person\nmay determine that those berries are poisonous. In\nthis case, this would be internal information obtained\nby interaction with the environment. If instead, a peer\nhad previously advised the person that eating those\nberries will make them sick, that would be external\ninformation provided by an extrinsic source.\nIn this work, we refer to methods using externally-\ninﬂuenced agent learning as as assisted reinforcement\nlearning. The ARL framework is deﬁned to include\nany type of RL that uses external information to\nsupplement agent learning and the decision-making\nprocess. Some common practices include the direct\nalteration of the agent’s understanding of the envi-\nronment [15], focusing exploration eﬀorts through\ncritique and advice [26], or assisting the agent in\n3\nthe decision-making process [17]. For instance, exist-\ning ARL techniques include interactive reinforcement\nlearning [38, 39], learning from demonstration [40, 41],\nand transfer learning [22, 42], among others.\nThe previously mentioned RL approaches are just\nexamples of ARL methods that use external infor-\nmation to supplement the agent’s decision-making\nprocess and learning. Additional details of these and\nother approaches and how they use an external in-\nformation source to assist the agent (in terms of our\nARL framework) are addressed in Section 4. The ex-\nternal information source is most commonly a human\nor another artiﬁcial agent. Regardless of the source,\nthe use of external information has often been shown\nto improve an agent’s ability and learning speed. In\nthe next section, we present a more detailed concep-\ntual framework for ARL which is the base for the\ntaxonomy we propose subsequently.\n2.2\nConceptual Framework\nThe proposed ARL framework is built to improve the\nclassiﬁcation, the comparability, and the discussion\non diﬀerent externally-inﬂuenced RL methods. To\nachieve this aim, the framework has been designed\nusing insights and observations drawn from many dif-\nferent ARL approaches. The result is a framework\nthat can describe existing methods while also being\nﬂexible enough to include future research. The frame-\nwork details are shown in Figure 2.\nThe proposed ARL framework comprises four pro-\ncessing components shown using red boxes in the\ndiagram, i.e., information source, advice interpreta-\ntion, external model, and the assisted agent itself.\nThe external information source may not have perfect\nobservability and also may not know details about\nthe RL agent (algorithms, weights, hyperparameters,\netc.), or make assumptions, e.g., value-based learn-\ners [43]. The processing components are responsible\nfor providing, transforming, and storing information.\nWe do include the agent as part of the processing\ncomponents since it is part of the RL process as well.\nHowever, an agent using ARL generally behaves as a\ntraditional RL agent, i.e., it interacts with the envi-\nronment by exploring/exploiting actions. Inside the\nagent, there are three diﬀerent stages: reward update,\ninternal processing, and action selection.\nEach of\nthose stages may be altered by the external model us-\ning reward/state modiﬁcations, internal modiﬁcations,\nor action modiﬁcations respectively. Moreover, the\nARL framework also comprises three communication\nlinks that connect the four processing components\nand are labelled: temporality, advice structure, and\nagent modiﬁcation. These links are shown between\nthe processing components and represent the commu-\nnication lines in Figure 2 that connect the processing\ncomponents together. The communication links con-\nvey information or denote constraints on the data\nsuch as where or when to provide information.\nThe ARL framework describes the transmission,\nmodiﬁcation, and modality of sourced information.\nIn this regard, we consider the ARL framework as a\nwhole unit, comprising traditional autonomous RL\nplus the components and links for assistance. Thus,\nthe taxonomy is a part of the framework and oriented\nto describe the assisted learning section. Although the\nframework has been developed on how ARL is usually\nbuilt, not all ARL approaches use all the proposed\ncomponents and links. Below, we brieﬂy describe each\nof the components and links of the framework. They\nare subsequently used in the next section to describe\nin detail the proposed taxonomy.\n• Information source: is the origin of the assis-\ntance being provided to the agent. The source\nmay be a human, a repository, or another agent.\nThere may be multiple information sources pro-\nviding assistance to an agent.\n• Temporality:\ndetermines both the time at\nwhich information is provided to the agent, and\nthe frequency with which it is provided. Infor-\nmation may be provided, before, during, or after\nagent training, and occur multiple times through\nthe learning process. Therefore, it is also responsi-\nble for how the information source communicates\ntemporal issues to the advice interpreter.\n• Advice interpretation: denotes the process of\ntransforming incoming information into a format\nbetter suited for the agent. This may involve ex-\ntracting key frames from video, converting audio\n4\nEnvironment\nrt+1\nInternal \nprocessing\nst+1\naction at\nstate st , reward rt\nreward/state \nmodiﬁcation\nReinforcement \nLearning\nenvironment \nmodiﬁcation\nAction \nselection\nReward \nupdate\nInformation source\nAdvice \ninterpretation\nExternal model\nAssisted agent\ninternal \nmodiﬁcation\naction \nmodiﬁcation\ntemporality\nadvice \nstructure\nagent \nmodiﬁcation\nAssisted\nReinforcement Learning\nFigure 2: Detailed view of the assisted reinforcement learning framework.\nThe diagram includes four\nprocessing components shown as dashed red boxes. Inside the assisted agent, one can observe three diﬀerent\npoints where it can receive possible modiﬁcations from the external model. Additionally, three communication\nlinks are shown with underlined text. This framework is subsequently used to further discuss the proposed\nARL taxonomy.\nsamples to rewards, or mapping information to\nstates.\n• Advice structure: represents the structure of\nthe advice after translation in a form suitable\nfor the external model. Some approaches may\nnot have an explicit external model, therefore,\nthis structure might instead be directly used to\nmodify the agent.\n• External model: is responsible for retaining\nand relaying the information between the source\nand the agent. The model may retain the received\ninformation in the learning model, using it for\n5\nInformation\nSource\nAdvice\nInterpretation\nExternal\nModel\nAssisted\nAgent\nTemporality\nAdvice structure\nAgent modification\nFigure 3:\nRelation between the processing compo-\nnents and the communication links as a UML sequence\ndiagram.\nlater decisions, or it may discard the received\ninformation as soon as it has been used.\n• Agent modiﬁcation:\ndenotes the approach\nthat the agent uses to beneﬁt from the incom-\ning information. The most common modiﬁcation\napproaches may use information to alter the en-\nvironmental reward signal or modify the agent’s\nbehaviour or the decision-making process directly.\n• Assisted Agent: is the RL agent receiving the\nexternal information or advice while learning a\nnew task. The agent needs to work out how to\nincorporate the provided information with its\nown learning. If a diﬀerent action is suggested by\nthe trainer then the agent may decide if it should\nfollow to that advice or not.\nFigure 3 shows in a UML sequence diagram the\ninteraction between the processing components and\ncommunication links according to Figure 2.\n3\nAssisted Reinforcement\nLearning Taxonomy\nIn this section, we describe the processing components\nand communication links included in the proposed\nframework within an ARL taxonomy1 and give more\n1In this context, we refer the taxonomy as a classiﬁcation of\nthe diﬀerent elements of the ARL framework, i.e., processing\ncomponents and communication links, and not as a way to\nclassify each ARL method.\ndetails of each of them. Figure 4 shows all the elements\nof the proposed ARL taxonomy including examples\nfor each processing component and communication\nlink. In the taxonomy, we include the agent as a com-\nponent being the one that receives the advice. Each\nof the seven elements, i.e., processing components\nand communication links, is described in detail in the\nfollowing subsections. In our work, the concept of\ntaxonomy is used to classify the diﬀerent elements\nwithin a class of problems, i.e., ARL problems. In\nthis regard, our proposal is represented by a general\nontology where the class is ARL, the properties are\nthe processing components and the communication\nlinks, and the relations between the properties are as\nshown in Figure 4.\n3.1\nInformation Source\nThe external information source is the main factor\nthat sets ARL apart from traditional RL approaches.\nIt is responsible for introducing new information about\nthe task to the agent, supplementing or replacing the\ninformation the agent receives from the environment.\nThe source is external to the agent and the environ-\nment, providing information that either the agent\nmay not have had access to, or would have eventually\nlearned itself. The information source may be able\nto observe the environment, the agent, or the agent’s\ndecision-making process. The objective of the infor-\nmation source is to assist the agent in achieving its\ngoal faster.\nThere may be multiple information sources com-\nmunicating with an agent.\nThis may be humans,\nagents, other digital sources, or any combination of\nthe three [44].\nThe use of multiple sources oﬀers\na wider range of available information to the agent.\nHowever, more complex modiﬁcation methods may\nbe required to manage the information and handle\nconﬂicting advice [45].\nThere are many examples of external information\nsources in current ARL literature, the most common\nof which are humans and additional reward func-\ntions [46, 47, 35].\nFor instance, RLfD and IntRL\nuse human guidance to provide the agent with a gen-\neralised view of the solution [48, 49]. Moreover, the\nuse of additional reward functions is one of the earliest\n6\nInformation source\nAdvice \ninterpretation\nExternal model\nAssisted Reinforcement Learning Taxonomy\nTemporality\nAdvice \nstructure\nAssisted agent\nAgent \nmodiﬁcation\nHuman\nAgent\nNumber of advisors\nOther\nSpeech-to-text\nFeature identiﬁcation\nKey-frame identiﬁcation\nBinary / scalar\nVector\nRule / Tree\nImmediate\nRetained\nCombined\nPlanned\nInteractive\nReward / policy shaping\nInternal modiﬁcation\nAction biasing\nState shaping\nNormal agent\nCuriosity-driven agent\nMulti-policy agent\nFigure 4: The assisted reinforcement learning taxon-\nomy. This ﬁgure shows the four processing compo-\nnents as dashed red boxes and the communication\nlinks as green parallelograms using underlined text.\nExamples for each component and method are in-\ncluded at the right.\nexamples of ARL. In such cases, the designer of the\nagent encodes some further information about the\nenvironment or goal as an additional reward, supple-\nmenting the original reward given by the environment.\nAn example of the use of additional reward func-\ntions can be found in Randløv and Alstrøm’s bicycle\nexperiment [23], in which, they teach an agent to ride\na bicycle towards a goal point. Without additional\nassistance, the RL agent would only receive a reward\nupon reaching the termination state. Randløv and\nAlstrøm encoded some of their knowledge as a shaping\nreward signal external to the environment, providing\nthe agent with additional rewards if it is cycling to-\nwards the goal point. In this scenario, the system\ndesigners acted as an external information source,\nproviding extra information to the RL agent. The\nuse of this external information results in the agent\nlearning the solution faster than using the traditional\nRL approach.\nSome other information sources include behaviours\nfrom past experiences or other agents, repositories\nof labelled data or examples, or distribution tables\nfor initialising/biasing agent behaviour [39]. Video,\naudio, and text sources may be used as well [50]. How-\never, these sources may require substantial amounts\nof interpretation and preprocessing to be of use.\nThe accuracy, availability, or consistency of the in-\nformation source can aﬀect the maximum utility of\nthe information [51, 52]. Identifying in advance inac-\ncurate information given to the agent can signiﬁcantly\nimprove performance [32, 53]. While the information\nsource may perform the validation and the veriﬁcation\nof the given advice, the primary duty remains simply\nto act as a supplementary source of information. In\nthis regard, both validation and veriﬁcation of infor-\nmation are functions better suited for the external\nmodel or the assisted agent.\n3.2\nTemporality\nThe temporal component, or temporality, refers to the\ntime at which information is communicated by the\ninformation source. The information may be provided\nin full to the agent at a set time (either before, during,\nor after training).\nThis is referred to as planned\nassistance [54, 55]. Alternatively, the information may\nbe provided at any time during the agent’s operation,\nreferred to as interactive assistance [56, 57].\nPlanned assistance, on the one hand, is common\nin ARL methods. Some examples are predeﬁned ad-\nditional shaping functions, agent policy initialisation\nbased on either prior experience or a known distri-\nbution, and the creation of subgoals that lead the\nway to a ﬁnal solution [54]. These methods let the\nexperiment designer endow the agent with initial in-\nformation about the environment or the goal to be\n7\nachieved. By providing this initial knowledge, the\ndesigner can reduce the agent’s need for exploration.\nThe bicycle experiment discussed in the previous\nsection is an example of planned assistance. As men-\ntioned, the agent is learning to control a bicycle and\nmust learn to steer it towards a goal [23]. Before the\nexperiment, the designers give the agent additional\ninformation in the form of a reward signal that corre-\nlates to the direction of the goal state. This planned\nassistance approach helps the agent to narrow the\nsearch space by giving it extra information about the\nenvironment. This small yet beneﬁcial initial infor-\nmation results in a signiﬁcant improvement in the\nagent’s learning speed.\nAnother example of planned assistance is found in\nheuristic RL. Heuristic RL is a method of applying\nadvice to agent decision-making.\nOne example is\nan experiment which implements heuristic RL in the\nRoboCup soccer domain [58], a domain known for\nits large state space and continuous state range. In\nthis environment, one team attempts to score a goal,\nwhile the other team tries to block the ﬁrst team\nfrom scoring, such as in half-ﬁeld oﬀence [59, 60]. In\nthis experiment using heuristic RL, the defending\nteam is given initial advice before training.\nThis\nadvice consists of two rules: if the agent is not near\nthe ball then move closer, and if the agent is near\nthe ball then do something with it. The experiment\nresults show that a team that uses planned assistance\nperforms better than a team that is given no initial\nknowledge [58].\nInteractive assistance, on the other hand, refers\nto information provided by the source repeatedly\nthroughout the agent’s learning. Information sources\nthat assist interactively often can observe the agent’s\ncurrent state, or the environment the agent is op-\nerating in. In current literature, humans are more\ncommonly used as information sources for interactive\nassistance [61, 62]. The human can observe how the\nagent is performing and its current state in the en-\nvironment, and provides guidance or critiques of the\nagent’s behaviour [63].\nFor example, Sophie’s Kitchen [26] presents an In-\ntRL based agent, called Sophie, which attempts to\nbake a cake by interacting with the items and ingre-\ndients found in a kitchen. In this experiment, the\nagent will receive a reward if it successfully bakes\nthe cake. At any point during the agent’s training,\nan observing human can provide the agent with an\nadditional reward to supplement the reward signal\ngiven by the environment. If the agent performs an\nundesirable action, such as forgetting to add eggs to\nthe cake, the human can punish the agent by provid-\ning an immediate negative reward. The human can\nalso reward the agent for performing desirable actions,\nsuch as adding ingredients in the correct order. In\nthis experiment, the human advisor is acting as an\ninteractive information source.\nAlthough the agent could learn the task without\nany assistance, the addition of the human advisor\nand interactive feedback allows the agent to learn the\ndesired behaviour faster in comparison to autonomous\nRL [26]. The beneﬁt of using interactive advice rather\nthan planned advice is that the information source can\nreact to the current state of the agent. Additionally,\nan interactive information source does not need to\nencode all possibly useful advice up front. Instead,\nit can choose to provide relevant information only\nwhen required. This approach does have a signiﬁcant\ncost; the information source needs to be constantly\nobserving the agent and determining what information\nis relevant. For instance, an approach using inverse RL\nthrough demonstrations may also consider providing\nfailed examples to show the agent what not to do [64].\n3.3\nAdvice Interpretation\nThe advice interpretation stage of the taxonomy de-\nnotes what transformations need to occur on the in-\ncoming information. The source provides information\nfor the agent to use that may need to be translated\ninto a format that the agent can understand. The\ninformation source may provide their assistance in\nmany diﬀerent forms.\nSome examples include au-\ndio [65], video [50], text [66], distributions and proba-\nbilities [35], or prior learned behaviour from a diﬀer-\nent task or agent [30]. This information needs to be\nadapted for use by the agent for the current task. The\nproduct of the advice interpretation stage depends\non the structure that the agent or external model\nrequires.\nA ﬁeld where the interpretation of incoming advice\n8\nis crucial is Transfer Learning (TL). The goal of TL\nis to use behaviour learned in a prior task to improve\nperformance in a new, previously unseen task [67].\nA critical step in TL is the mapping of states and\nobservations between the old and new domains. The\ninformation source provides information to the agent\nthat does not fully align with its current task. There-\nfore, it is crucial that the information provided can be\ncorrectly interpreted, so as to be useful to the current\ndomain. More commonly, this interpretation stage in\nTL is performed by hand. However, there has also\nbeen eﬀort attempting to automate this stage [68, 69].\nAnother example of the use of the advice interpre-\ntation stage is with the sourcing of feedback for RL\nagents. In the Sophie’s Kitchen experiment [26], dis-\ncussed in the previous section, the agent can be given\npositive or negative feedback by a human regarding\nits choice of actions. In this experiment, the human\ncreates either a green (positive) or a red (negative) bar\nto represent the desired feedback to be given to the\nagent. This bar is used to interpret the reward signal\nto give to the agent, with the colour of the bar desig-\nnating whether the reward is positive or negative, and\nthe size of the bar designating the magnitude of the\nreward. This type of feedback can also be extended to\naudio, where recording phrases such as ‘Good’ or ‘Well\nDone’ are interpreted as positive rewards and ‘Bad’ or\n‘Try Again’ are interpreted as negative rewards [70].\nThese methods can also be combined into a multi-\nmodel architecture to provide advice to an RL robotic\nagent using audiovisual sensory inputs, such as work\nby Cruz et al. [50]. In this experiment, a simulated\nrobot learns how to clean a table using a multi-modal\nassociative function to integrate auditory and visual\ncues into a single piece of advice which is used by the\nRL algorithm. In this scenario, the external informa-\ntion source is a human trainer and the RL algorithm\nrepresents the integrated advice as a state-action pair.\n3.4\nAdvice Structure\nThe advice structure component refers to the form\nthat the agent or external model requires incoming\ninformation to take. The information that the agent\nuses can be represented in a number of ways. Some\nexamples of advice structures include: Boolean values\ndenoting positive or negative feedback; rules deter-\nmining action selection; matrices for mapping prior\nexperiences to new states; case-based reasoning struc-\ntures for the agent to consult with; or, hierarchical\ndecision trees to represent options for the agent to\ntake [62, 71].\nThe simplest form of structure is binary, in which\nthe information takes only one from two options, such\nas ‘Good’ or ‘Bad’. An example of the use of a binary\nstructure is the TAMER-RL agent [72]. TAMER-RL\nis an IntRL agent that uses binary feedback from\nan observing human. At any time step, the human\ncan agree or disagree with the agent about its last\naction. In this case, the feedback is a binary structure\nindicating agree or disagree.\nA more complex advice structure is used in case-\nbased RL agents [73]. A case in this context represents\na generalised area of the state space and provides\ninformation about which actions to take in that state.\nThe use of a case-based structure allows the agent to\ngain more information from the information source\ncompared to a binary structure, at a cost of more\ncomplex sourcing and interpretation approaches.\nOne of the more common advice structures is a\nsimple state-action pair. A state-action pair consists\nof a single state and an associated piece of advice.\nThe associated advice may be an additional scalar re-\nward or a recommended action. Using a state-action\npair, sourced information is interpreted to provide\nadvice for a given state. In the cleaning-table robot\ntask [50], discussed in the previous section, the ex-\nternal trainer using multi-modal advice provides an\naction to be performed in speciﬁc states. Once the\nadvice is processed using the multi-modal integration\nfunction, the proposed action is given to the RL agent\nto be executed as a state-action pair considering the\nagent’s current state. This state-action structure has\nalso been used for other methods including TAMER-\nRL [72], Sophie’s Kitchen [26], and policy-shaping\napproaches [16].\nA novel rule-based interactive advice structure is\nintroduced in [74]. Interactive RL methods rely on\nconstant human supervision and evaluation, requir-\ning a substantial commitment from the advice-giver.\nThis constraint restricts the user to providing advice\nrelevant to the current state and no other, even when\n9\nsuch advice may be applicable to multiple states. Al-\nlowing users to provide information in the form of\nrules, rather than per-state action recommendations,\nincreases the information per interaction, and does not\nlimit the information to the current state. Rules can\nbe interactively created during the agent’s operation\nand be generalised over the state space while remain-\ning ﬂexible enough to handle potentially inaccurate\nor irrelevant information. The learner agent uses the\nrules as persistent advice allowing the retention and\nreuse of the information in the future. Rule-based\nadvice signiﬁcantly reduces human guidance require-\nments while improving agent performance.\n3.5\nExternal Model\nThe external model is responsible for retaining and\nrelaying information between the information source\nand the agent. The external model receives inter-\npreted information from the information source and\nmay either retain the information for use by the agent\nwhen required or pass it to the agent immediately.\nA retained model is an external model that stores all\ninformation provided by the information source [17].\nA retained model may be used if the cost of acquiring\ninformation is greater than the cost of storing it, if the\ninformation provided is general or applies to multiple\nstates, or if the information is gathered incrementally.\nIn instances where information is gathered incremen-\ntally, using a retained model allows the agent to build\nup a knowledge base over time. The agent may con-\nsult with the model at any time to determine if a\nreward signal is to be altered, or if there is any extra\ninformation that may assist with decision-making.\nAn immediate model passes the information directly\nto the agent [75]. In this case, the information received\nis only relevant to the current time step, or the cost\nof reacquiring the information from the source is less\nthan that of retaining the information.\nApproaches can also combine this by incorporat-\ning both a retained model as well as passing some\ninformation through directly, such as [32]. In this\nwork, an RL agent uses a combination of interactive\nfeedback and contextual aﬀordances [76] to speed up\nthe learning process of a robot performing a domestic\ntask. On the one hand, contextual aﬀordances are\nlearned at the beginning of autonomous RL and are\nreadily available from there on to avoid the so-called\nfailed-states, which are states from where the robot is\nnot able to ﬁnish the task successfully anymore. On\nthe other hand, interactive feedback is provided by\nan external advisor and used to suggest actions to\nperform when the robot is learning the task. This\nadvice is given to the robot to be used in the current\nstate and it is discarded immediately after.\nThe external model may have diﬀerent functions de-\npending on its implementation. For instance, heuristic\nRL hosts a model that stores rules and advice that\ngeneralise over sections of the state space [77]. In TL,\nthe external model may hold information regarding\npast experiences and policies from problems similar\nto the current domain [22, 78], or in inverse RL, the\nexternal model is a substitute for the reward func-\ntion [79].\n3.6\nAgent Modiﬁcation\nThe modiﬁcation stage of the framework denotes how\nthe information that the external model contains is\nused to assist the agent in achieving its goal. It is\nresponsible for supplementing the agent’s reward, al-\ntering the agent’s policy, or helping with the decision-\nmaking process. A popular method for injecting ex-\nternal information into agent learning is shaping [80].\nShaping is a common method for altering agent per-\nformance by modifying parameters in the learning\nprocess. Erez and Smart [25] propose a list of tech-\nniques in which shaping can be applied to RL agents.\nThese include altering the reward, the agent’s pol-\nicy, agent learning parameters, and environmental\ndynamics [27].\nAltering the reward the agent receives is a straight-\nforward method for inﬂuencing an agent’s learning [81].\nIt is known as reward-shaping, in which the external\ninformation is used to bias the agent’s learning [46].\nSpecial care must be taken to ensure that any mod-\niﬁcation of the reward signal remains zero-sum to\navoid the agent exploiting the shaped reward in ways\nthat do not align with the desired goal. This can\nbe achieved by ensuring that additional rewards are\npotential-based, meaning that they are derived from\nthe diﬀerence in the values of a potential function at\n10\nthe current and successor states [82]. However, recent\nwork by [83] shows a ﬂaw in the previous method when\ntransforming non-potential-based reward-shaping into\npotential-based. Alternatively, the authors introduce\na policy invariant explicit shaping algorithm allow-\ning for arbitrary advice, conﬁrming that it ensures\nconvergence to the optimal policy when the advice\nis misleading and also accelerates learning when the\nadvice is useful [83]. Shaping techniques have also\nbeen used to alter state-action pairs [84], for dynamic\nsituations [82, 85], and for multi-agent systems [86].\nPolicy-shaping is the modiﬁcation of the agent’s\nbehaviour [16]. This modiﬁcation can be done either\nby inﬂuencing how the agent makes decisions or by di-\nrectly altering the agent’s learned behaviour. A simple\nmethod of policy-shaping involves forcing it to take\ncertain actions if advice from the information source\nhas recommended them [87, 88]. Human-in-the-loop\ntechniques may be beneﬁcial to address complex RL\nproblems with the help of domain experts, e.g., in\nhealth informatics [89]. This allows the external in-\nformation source to guide the agent and take direct\ncontrol over exploration/exploitation. Alternatively,\nthe information source can choose to alter the agent’s\nbehaviour directly by changing Q-values or installing\nrules that override the actions for chosen states [90].\nThis method of modiﬁcation can improve agent per-\nformance rapidly, as it can give the agent partial\nsolutions.\nInternal modiﬁcation is a method of altering the pa-\nrameters of the agent that are essential to its learning.\nParameters such as the learning rate (α), discount\nfactor (γ), and exploration percentage (ϵ), are all in-\nternal to the RL agent and may be altered to aﬀect its\nperformance [91]. For example, if an advisor observes\nthat an agent is repeating actions and not exploring\nenough then the exploration percentage or learning\nrate may be temporarily increased. Internal modiﬁ-\ncation is a simple method to implement. However, it\ncan be diﬃcult at times to know which parameters to\nadjust, and to what degree they are to be adjusted.\nEnvironmental modiﬁcation is an indirect method\nfor inﬂuencing an RL agent. Altering the environ-\nment is not always achievable and may be a technique\nbetter suited for digital or simulated environments.\nSome examples of modifying the environment include\naltering or reducing the state space and observable\ninformation [92, 93], reducing the action space [94],\nmodifying the agent’s starting state [95], or altering\nthe dynamics of the environment to make the task\neasier to solve [96] Below, we further describe these\nenvironmental modiﬁcations.\nReducing the state space can speed up the agent’s\nlearning as there is less of the environment to search.\nWhile the agent cannot fully solve the task with an\nincomplete environment representation, it allows the\nagent to learn the basic behaviour. The level of de-\ntail in the state representation can then be increased,\nallowing the agent to reﬁne its policy towards the\ncorrect behaviour [92, 93]. Reducing the action space\nis similar to the previous. The agent’s available ac-\ntions are limited, and the agent attempts to learn the\nbest behaviour it can with the actions it has avail-\nable. Once a suitable behaviour has been achieved,\nnew actions can be provided, and the agent can begin\nto learn more complex solutions [94]. Modifying the\nagent’s starting space alters where in the environment\nthe agent begins learning. Using this approach, the\nagent can begin training close to the goal. As the\nagent learns how to navigate to the goal, the start-\ning state is incrementally moved further away. This\nallows the agent to build upon its past knowledge\nof the environment [95]. Altering the dynamics of\nthe environment involves changing how the environ-\nment operates to make the task easier for the agent\nto learn [27]. By altering attributes of the environ-\nment such as reducing gravity, lowering maximum\ndriving speed, or reducing noise, the agent may learn\nthe desired behaviour faster or more safely. After the\nagent learns a satisfactory behaviour, the environment\ndynamics can be changed to more typical levels [97].\n3.7\nAssisted Agent\nThe ﬁnal component of the proposed ARL taxonomy\nis the RL agent. A key aspect of the taxonomy is\nthat the agent, in the absence of any external informa-\ntion, should operate the same as any RL agent would.\nGiven no external information, the agent should con-\ntinue to explore and interact autonomously with its\nenvironment and attempt to achieve its goal.\nIn the next section, we present an in-depth look at\n11\nsome ARL techniques and describe them in terms of\nthe taxonomy that has been presented in this section.\n4\nIllustrative Approaches with\nComponents and Links from\nthe Taxonomy\nThis section presents an in-depth analysis of some\npopular and well-known ARL approaches. Each il-\nlustrative approach is described as an instance of the\nproposed taxonomy shown in Section 3, in some cases\nusing a speciﬁc approach and in other cases a set of\nthem. Therefore, for each presented ARL approach,\nwe show how each processing component and each\ncommunication link particularly adapts to the ARL\ntaxonomy using current literature in the respective\nﬁeld for concrete examples.\n4.1\nHeuristic Reinforcement Learning\nHeuristic RL uses pieces of information that generalise\nover an area of the state space. The information is\nused to assist the agent in decision-making and reduce\nthe searchable state space [98, 99]. An example of a\nheuristic is a rule. A rule can cover multiple states,\nmaking its use eﬃcient at delivering advice to an\nagent. In Section 3.2, we have introduced a heuristic\nRL experiment applied to the RoboCup soccer do-\nmain [58]. In the RoboCup soccer domain, one team\nactively tries to score a goal, while the other team\ntries to block it. As mentioned, the defending team is\ngiven initial advice before training, consisting of two\npredeﬁned rules. The following is an analysis of this\nheuristic RL example applied as the ARL taxonomy.\n• Information source: The information source\nfor the RoboCup experiment is a person. In this\ncase, the person has previously experimented\nwith the robot soccer domain and can advise the\nagent with some rules that will speed up learning.\n• Temporality: The advice for the agent is given\nbefore training begins. Once training has begun\nthe person does not interact with the agent again.\nThis is an example of planned assistance, where\nHuman-domain \nexpert\nConvert rule to \nmachine language\nRetained\nrule-set\nHeuristic Reinforcement Learning\nPlanned\nMachine \nrule\nNormal agent\nPolicy \nshaping\nInformation source\nTemporality\nAdvice interpretation\nAdvice structure\nExternal model\nAgent modiﬁcation\nAssisted agent\nFigure 5: Heuristic RL components according the\nproposed ARL taxonomy.\nThe particular process-\ning components and communication links illustrate a\ntechnique used in the RoboCup soccer domain [58].\ninformation is given to the agent at a ﬁxed time,\nand the information is known by the information\nsource in advance.\n• Advice interpretation: The information needs\nto be understandable by the agent. In the robot\nsoccer domain, the person gives two rules; (i) if\nnot near the ball then move towards the ball, and\n(ii) if near the ball do something with the ball.\nThese rules are understandable by the human but\nneed to be translated into machine code so that\nagent can use them. This is usually a task easily\nperformed by a knowledgeable human operator.\n12\nThe result is conditional-like rules as: (i) IF\nNOT close to ball() THEN target and move(),\nand (ii) IF close to ball() THEN kick ball().\n• Advice structure: The structure of the advice\nafter being interpreted is a new rule. The rule\nneeds to be compatible with the agent, including\nthe ability to substitute variables and evaluate\nexpressions.\n• External model: The external model used by\nthe heuristic RL agent is a rule set. The external\nmodel retains all rules given to it. The model\nmay also retain statistics about the rule relating\nto conﬁdence, number of uses, and state space\ncovered.\n• Agent modiﬁcation: Heuristic RL uses the\nrule set to assist the agent in its decision-making.\nIf a rule applies to the current state, then the\naction that the rule recommends is taken by the\nagent. This is a form of policy-shaping as the\nagent’s decision-making is directly manipulated\nby the external information.\n• Assisted Agent: The RL agent operates as\nusual. When it is time to decide on an action to\ntake it consults the external model. The external\nmodel tests all the rules it has and checks to\nsee if any applies to the current state, otherwise,\nthe agent’s default decision-making mechanism\nis used.\nFigure 5 shows how the heuristic RL approach ﬁts\ninto the proposed ARL taxonomy taking into consid-\neration the previous deﬁnitions of processing compo-\nnents and communication links from the RoboCup\nsoccer domain.\n4.2\nInteractive Reinforcement Learn-\ning\nIntRL is another application of ARL. Most commonly,\nthe information source is an observing human or a\nsubstitute for a human, such as an oracle, a simulated\nuser, or another agent [100]. The human provides\nassessment and advice to the agent, reinforcing the\nagent’s past actions and guiding future decisions. The\nhuman can assess past actions in two ways, by stating\nthat the agent’s chosen action is somehow correct or\nincorrect, or by telling the agent what the correct\naction to take is in that instance. Alternatively, the\nhuman can advise the agent on what actions to take in\nthe future [101]. The human can recommend actions\nto take or to avoid, or provide more information about\nthe current state to assist the agent in its decision-\nmaking [33].\nIntRL applications include having a human to pro-\nvide additional reward information [102, 103], and hav-\ning a human or agent provide action advice [104, 105].\nAll of these methods work in real-time and similarly,\ndiﬀering mainly in the agent modiﬁcation stage. The\nfollowing is an analysis of these IntRL approaches\napplied as the ARL taxonomy.\n• Information source: The information source\nis a human or simulated user. A simulated user\nis a program, analogous to a human, that acts\nhow a human would in a given situation. The\nhuman can observe the agent’s current and past\nstates, past actions taken, and what action the\nagent recommends it takes [106].\n• Temporality:\nIntRL agents operate interac-\ntively. The advisor can provide information to\nthe agent before, during, or after learning, and\nrepeatedly throughout the learning process. This\nallows the advisor to react to current information\nand supply the agent with relevant advice.\n• Advice interpretation: The advisor provides\neither an assessment of past actions taken, rec-\nommendations about actions to take, or a reward\nsignal. Computer simulated agents can receive\nthis information as key presses. However, physi-\ncal agents may receive this information through\naudio or video inputs [50]. In the case of audio\ninputs, these may be simple commands such as\n’Correct’ or ’Go Right’, which can be translated\nto a form the agent can understand [65]. Sup-\nporting input modalities such as natural language\nmakes systems based on IntRL more accessible to\nusers who are not themselves familiar with RL.\n13\n• Advice structure: A common structure of ad-\nvice the agent requires is simply a state-action\npair. Using this structure the human can assign\nadvice to a state for the agent to use, such as: In\nthis state, do this [107].\n• External model: Either retained or immediate\nmodels are commonly used [17, 108]. A retained\nmodel tracks what advice/feedback has been re-\nceived for each state [17]. The agent can use\nthis model to determine the human’s accuracy,\nconsistency, and discount for each piece of advice\nreceived. The model acts as a lookup table for\nthe agent, if advice exists for the current state,\nthen the agent can use it. Alternative methods\nmay not retain information given by the human\nand only use it for the current state [108].\n• Agent modiﬁcation: The most common meth-\nods of using the advice to modify the agents learn-\ning process are reward- and policy-shaping [101].\nReward-shaping uses assessment/critique gath-\nered from the advisor to alter the reward given\nto the agent.\nIf the advisor disagrees with a\npast action, then the reward received for that\nstate-action pair is decreased. If the advisor rec-\nommends an action to take in the future, then\npolicy-shaping can be used to override the agent’s\nusual action selection mechanism. One method\nof implementing policy-shaping for interactive\nadvice is probabilistic policy reuse [17].\n• Assisted Agent: Most of the time, the RL agent\noperates as any other RL agent would, i.e., it per-\nforms actions in the environment by exploiting/\nexploring. The agent should continue to do so\neven if no advice from the trainer is given. Al-\nthough a trainer could proactively provide advice\nto the learner, sometimes the student could de-\ncide to request such advice, and the trainer may\nor may not respond to that request. For instance,\nheuristics have been used to decide if the trainer\nshould provide advice and/or if the learner should\nask for it [105]. In contrast, recent work estimates\nthe learner’s uncertainty in its current state, ask-\ning for advice in case the level of uncertainty is\nabove a predeﬁned threshold [109].\nHuman / simulated \nuser\nConvert modal cue \nto signal\nRetained / \nimmediate\nInteractive Reinforcement Learning\nInteractive\nState-action \npair\nCuriosity-driven \nagent\nPolicy / reward \nshaping\nInformation source\nTemporality\nAdvice interpretation\nAdvice structure\nExternal model\nAgent modiﬁcation\nAssisted agent\nFigure 6: Interactive RL as the proposed ARL tax-\nonomy. In this approach, interactive advice is given\nby the user and more commonly used as policy and\nreward shaping.\nFigure 6 shows how the IntRL approach is adapted\nto the proposed ARL taxonomy taking into account\nthe previous deﬁnitions of processing components and\ncommunication links.\n4.3\nReinforcement\nLearning\nfrom\nDemonstration\nRLfD is a term coined by Schaal [110]. It refers to the\nsetting where both a reward signal and demonstra-\ntions are available to learn from, combining the best\nof the ﬁelds of RL and Learning from Demonstration\n14\n(LfD). Since RL presents an objective evaluation of\nbehaviour, optimal behaviour can be achieved. Such\nan objective evaluation of behaviour is not present\nin LfD [111], where only expert demonstrations are\navailable to be mimicked and generalised. The stu-\ndent can thus not surpass its master. Nevertheless,\nLfD is typically much more sample eﬃcient than RL.\nTherefore, the aim is to combine the fast LfD method\nwith objective behaviour evaluation and theoretical\nguarantees from RL.\nTwo diﬀerent approaches have been proposed to\nuse demonstrations in an RL setting. The ﬁrst is the\ngeneration of an initial value-function for temporal-\ndiﬀerence learning by using the demonstrations as\npassive learning experiences for the RL agent [112].\nThe second approach derives an initial policy from\nthe demonstrations and uses that to kickstart the RL\nagent [113, 114]. In this regard, Taylor et al. propose\nthe Human-Agent Transfer (HAT) algorithm [115],\nwhich consists of three steps: (i) demonstration: the\nagent performs the task teleoperated and records all\nstate-action transitions, (ii) policy summarising: in\norder to bootstrap autonomous learning, policy rules\nare derived from the recorded state-action transitions,\nand (iii) independent learning: autonomous reinforce-\nment learning using the policy summary to bias the\nlearning. Below we use the HAT algorithm to describe\nhow RLfD ﬁts into the ARL taxonomy.\n• Information source: An expert of the task\n(human or otherwise) can provide sample be-\nhaviour by demonstrating its execution of the\ntask. Preferably these demonstrations are eﬃ-\ncient and successful executions of the task.\n• Temporality:\nIt\nuses\nplanned\nassistance.\nDemonstrations are recorded and given to the\nlearning agent before it starts training.\n• Advice interpretation: The received demon-\nstrations must be ﬁrst transformed into the\nagent’s perspective by encoding them as se-\nquences of state-action pairs. These are then\nprocessed using a classiﬁer, which serves as the\nLfD component, creating an approximation of\nthe demonstrator’s policy using rules.\nDomain expert\nConvert demonstration \nto agent’s perspective\nRetained rule \nsystem\nReinforcement Learning from Demonstration\nPlanned\nRule system\nCuriosity-driven \nagent\nAction \nbiasing\nInformation source\nTemporality\nAdvice interpretation\nAdvice structure\nExternal model\nAgent modiﬁcation\nAssisted agent\nFigure 7: RL from demonstration as the proposed\nARL taxonomy. In this case, the processing compo-\nnents and communication links are deﬁned from the\nHAT algorithm [115], which combines RL and LfD.\n• Advice structure: The information is encoded\nas a classiﬁer that maps states to the actions\nwhich the demonstrator is hypothesised to exe-\ncute in those states.\n• External model:\nThe generated rules are\nstored in the external model and not modiﬁed\nanymore. The external model can be queried\nwith a state and responds with the hypothesised\ndemonstrator action in that state.\n• Agent modiﬁcation: The action proposed by\nthe demonstrator can be integrated into the agent\n15\nthrough three action biasing methods: (i) at-\ntributing a value bonus to the Q-value for that\nstate-action pair, (ii) extending the agent’s action\nset with an action that executes the hypothesised\ndemonstrator action, and (iii) probabilistically\nchoosing to execute the action suggested by the\nmodel.\n• Assisted agent:\nDuring its decision-making\n(when and how depends on the implemented mod-\niﬁcation method) the agent has the option to\nconsult the external model to obtain the action\nthat the demonstrator is assumed to take. This\nkind of agent is sometimes referred to as curiosity-\ndriven agent [116]. Otherwise, the agent acts as\na usual RL agent.\nFigure 7 shows how the RLfD approach is adapted\nto the proposed ARL taxonomy taking into account\nthe previous deﬁnitions of processing components and\ncommunication links for the HAT algorithm.\n4.4\nTransfer Learning\nThe idea of transferring information between tasks\n(or between agents), rather than learning every task\nfrom the ground up seems to be obvious in retrospect.\nWhile transfer between diﬀerent tasks has long been\nstudied in humans, it has only gained popularity in\nRL settings in the last decade [22]. We consider three\ndistinct settings where TL can be useful.\nFirst, an agent may have learned how to perform\na task and a new agent must learn to perform that\nsame task or a variation on the task under diﬀerent\ncircumstances. Let us consider two agents with diﬀer-\nent state features, i.e., diﬀerent sensors, or diﬀerent\naction spaces (or diﬀerent actuators). In this case, an\ninter-task mapping [117, 118] can be hand speciﬁed or\nlearned from data [119, 120] to relate the new target\nagent to the existing source agent. One of the simplest\nways to reuse such knowledge is to embed it into the\ntarget task agent, e.g., directly reuse the Q-values\nthat the source agent had learned [118].\nSecond, let us now consider that the world may\nbe non-stationary. In TL settings, it is common to\nassume that the agent is notiﬁed when the world (or\nAgent with diﬀerent \ncapabilities\nQ-values, rules, \nor models\nRetained\nsource model\nTransfer Learning\nPlanned\nValue, rule, \nor model\nNormal agent\nAction \nbiasing\nInformation source\nTemporality\nAdvice interpretation\nAdvice structure\nExternal model\nAgent modiﬁcation\nAssisted agent\nFigure 8: Transfer learning as the proposed ARL\ntaxonomy. In this case, an agent with diﬀerent capa-\nbilities (or the same agent) provides the model of a\nsource task which is transferred to a target task.\ntask in that world) changes. However, a TL agent\nsometimes does not need to detect changes [121] or\nworry about the slow world changes over time [122].\nAs in the previous setting, the agent may want to\nmodify the information, e.g., by using an inter-task\nmapping, to relate the two tasks. In addition, the\nagent may decide not to use its prior knowledge at\nall, e.g., to avoid negative transfer because the tasks\nare too dissimilar [118].\nThird, TL could be a critical step within a cur-\nriculum learning approach [123, 124]. For example,\nprevious work has shown that learning a sequence\n16\nof tasks that gradually increase in diﬃculty can be\nfaster than directly training on the ﬁnal (diﬃcult)\ntask [119, 125].\nIn addition to curricula that are\ncreated by machine learning experts, curricula con-\nstructed by naive human participants have also been\nconsidered [126]. Others have considered as a com-\nplementary problem a learning agent autonomously\ncreating a curriculum [127, 128]. In all cases, the\ndiﬃculty is scaﬀolding correctly so that the agent can\nlearn quickly on a sequence of tasks. These approaches\nare distinct from multi-task learning [17], where the\nagent wants to learn over a distribution of tasks, and\nlifelong learning [129, 130], where learning a new task\nshould also improve performance on previous tasks.\nThe following is an analysis of TL methods in terms\nof the ARL taxonomy.\n• Information source: The information comes\nfrom an agent with diﬀerent capabilities or the\nsame agent that has trained on a diﬀerent task.\n• Temporality: Transfer typically occurs when a\ntask changes or when an agent ﬁrst faces a novel\ntask. In both cases, it is planned assistance, i.e.,\nthe source agent transfers knowledge to the target\nagent before the target agent begins learning. If\nthe inter-task mapping is initially unknown, some\ntime may be spent trying to learn an inter-task\nmapping or estimate task similarity to previous\ntasks. However, the more time spent before the\ntransfer, the less impact transfer can have.\n• Advice interpretation: There are many types\nof information that can be transferred, including\nQ-values, rules, a model, etc. [118]. TL methods\nassume the target agent has access to the source\nagent’s ‘brain’, an assumption that may not al-\nways be true, e.g., if the designer of the source\nagent has not provided an API or if the source\nagent is a human.\n• Advice structure: The structure of the trans-\nferred knowledge is as varied as the types of\ninformation that can be provided. This variety of\ninformation includes Q-values, rules, or a model,\namong others.\n• External model: The source model is normally\nretained. Because the source task knowledge is\nnot necessarily suﬃcient for optimal performance\nin the target task, it is important for the tar-\nget agent to be able to learn to outperform the\ntransferred information.\n• Agent modiﬁcation: The target task agent\nuses the transferred information to bias its learn-\ning. The transferred knowledge is not typically\nmodiﬁed. Instead, the target task agent builds\non top of the knowledge, learning when to ignore\nit and instead follow the knowledge it has learned\nfrom the environment.\n• Assisted Agent: The agent is a typical RL\nagent that can take advantage of one or more\ntypes of prior knowledge.\nFigure 8 shows how the TL approach can be rep-\nresented within the proposed ARL taxonomy taking\ninto account the previous deﬁnitions of processing\ncomponents and communication links.\n4.5\nMultiple Information Sources\nWhile the majority of work in ARL is based on a\nsingle source of advice, several researchers have con-\nsidered scenarios where multiple sources of advice\nmay exist [131, 132, 133, 134]. Although the use of\nmultiple information sources is not an ARL approach\nby itself and could comprise sources utilising any of\nthe previously mentioned approaches, we include it\nhere to highlight how this multiple sources can be\nframed within the proposed taxonomy. The introduc-\ntion of multiple advisors may have beneﬁts for ARL\nagents, particularly in scenarios where each individ-\nual advisor has knowledge which is limited in some\nway [135], e.g., individual advisors may have expertise\ncovering diﬀerent sub-areas of the problem domain.\nHowever, it also introduces additional problems for\nthe agent, such as handling inconsistencies or direct\nconﬂicts between the guidance provided by diﬀerent\nadvisors, or learning to judge the reliability of each\nadvisor, possibly in a state-sensitive manner [104].\nIn the extreme case, an agent may even need to be\nable to identify and ignore the advice provided by\n17\ndeliberately malicious advisors [136]. The following is\nan analysis of approaches using multiple information\nsources with respect to the proposed ARL taxonomy.\n• Information source: Prior research has identi-\nﬁed several scenarios in which an agent may have\naccess to multiple sources of external informa-\ntion. Argall et al. [137] argue that when robots\nare applied to tasks within society in general,\nit is very likely that multiple users will interact\nwith and guide the behaviour of a robot.\nIn\nthe context of TL, multiple sources of informa-\ntion may be derived either from experience on\nvarying MDPs [138], or on alternative mappings\nfrom a single prior MDP to the current environ-\nment [139]. In multi-agent systems, each agent\nmay serve as a potential source of information\nfor every other agent [140, 141].\n• Temporality: Assistance may be planned or in-\nteractive. For instance, Argall et al. [137] have\nconsidered two diﬀerent sources of information,\nin the form of teacher demonstrations and teacher\nfeedback on trajectories generated by the learner.\nThe former may be provided in advance of learn-\ning consisting of complete state-action trajecto-\nries, i.e., planned assistance, while the latter oc-\ncurs on an interactive basis during learning, and\nstructurally consists of a subset of the learner’s\nactions being ﬂagged as correct by the teacher,\ni.e., interactive assistance.\n• Advice interpretation: The majority of work\nso far on ARL from multiple information sources\nhas assumed that these sources are homogeneous\nin terms of the timing and nature of the infor-\nmation provided. However, this need not be the\ncase, and for heterogeneous information sources,\nsome aspects of the advice may diﬀer in terms of\ninterpretation and structure. In this regard, the\nadvice needs to be integrated considering either\nall possible sources (equally or non-equally con-\ntributing), some sources (with the information\nprovided partially or fully considered), or only\nfrom one source at a time [135].\n• Advice structure: Each information source\nmay use a diﬀerent structure of advice. Therefore,\nMulti-users or \nmulti-agent system\nMulti-source \nintegration\nSeparated or \ncombined model\nMultiple Information Sources\nPlanned or \ninteractive\nIntegrated \nadvice\nMulti-policy \nagent\nWeighted \ncombination\nInformation source\nTemporality\nAdvice interpretation\nAdvice structure\nExternal model\nAgent modiﬁcation\nAssisted agent\nFigure 9: Multiple information sources as the pro-\nposed ARL taxonomy. In this case, there could be\nmultiple humans or multiple agents. One important\naspect is to integrate the diﬀerent pieces of advice.\nThe agent may also learn multiple policies as in multi-\nobjective RL.\nindividually all the aforementioned structures in\nprevious sections are possible to be used, e.g., ma-\nchine rule, state-action pair, rule system, value,\nor model. The ﬁnal structure into a single piece\nof advice may be done by integrating the mul-\ntiple information sources, for instance using a\nmulti-modal integration function [50] or using\ngraph structures (e.g., graph neural networks) us-\ning causal links between features for multi-modal\ncausability [142].\n18\n• External model: An ARL agent must choose\nwhether (i) to maintain a separate model for each\ninformation source, (ii) to combine the informa-\ntion from all sources into a single model, or (iii)\na combination of both. An example of the lat-\nter approach is the inverse RL system presented\nin [143], which learns a model of each information\nsource in the form of a feature-weighting function\nand then forms a combined feature-weighting via\naveraging. As noted by Karlsson [143], single-\nmodel approaches may encounter diﬃculties if\ndealing with information sources which are fun-\ndamentally incompatible with each other. An\nadditional beneﬁt of maintaining independent\nmodels is that these can also be augmented by\nadditional data on characteristics of each informa-\ntion source, such as the reliability or consistency\nof its advice [137, 139].\n• Agent modiﬁcation: Any of the modiﬁcation\napproaches discussed in the earlier sections of\nthis paper may also be applied in the context of\nmultiple information sources. For example, agent\nmodiﬁcation methods from LfD [137], TL [139,\n138], reward-shaping [144, 145] as well as inverse\nRL [143, 146]. The main additional consideration\nis how these methods may be aﬀected by the\npresence of multiple external models. The main\nmethods examined so far use a combination of the\nmodels, either weighted or unweighted [137, 143]\nor select a single best model to use [139].\n• Assisted Agent: In most circumstances, the\noperation of the agent itself is largely unaﬀected\nby the presence of more than one information\nsource. However, Tanwani and Billard [146] con-\nsider the task of performing inverse RL from\nmultiple demonstrations provided by multiple ex-\nperts, operating according to diﬀerent strategies\nor preferences. To address the potential incom-\npatibilities between these strategies, the agent\nattempts to learn a set of multiple policies, so\nas to be able to satisfy any policy expert strat-\negy, including those not provided to the agent.\nThis approach is closely related to multi-policy\nalgorithms developed for multiobjective RL [147].\nFigure 9 shows how an approach using multiple\ninformation sources is adapted to the proposed ARL\ntaxonomy taking into account the previous deﬁnitions\nof processing components and communication links.\nMoreover, Table 1 summarises how each of the ARL\napproaches and examples reviewed in this section is\nadapted to the proposed taxonomy.\n5\nFuture Directions and Open\nChallenges\nIn this section, we discuss open issues and propose\nfurther possibilities for future work in the ﬁeld of ARL.\nThese open questions have been identiﬁed from the\ncurrent literature in the ﬁeld. Many of these issues\nare shared with autonomous RL but it still remains\nopen how they could be addressed within the ARL\nframework.\n5.1\nIncorrect Assistance\nA common assumption that ARL methods make is\nthat all external information that the agent receives\nis accurate [148].\nAccurate information is correct\nadvice that assists the agent in completing its goal.\nHowever, the assumption that information will always\nbe of use to the agent is wrong, especially when the\ninformation source is an observing human, as in RL\nfrom imperfect demonstrations [149, 150]. Humans\nmay deliver advice late, and therefore the agent may\nrelate it to a wrong state. The advice may be of short-\nterm use to the agent but prevent it from achieving\noptimal performance. Moreover, the human trainer\nmay even be malicious and actively attempting to\nsabotage the agent’s performance.\nIncorrect information can be introduced by other\nsources as well. Some examples for non-human incor-\nrect advice include behaviour transferred from another\ndomain that does not align correctly, rules that gener-\nalise over multiple states which may cover exception\nstates, and noisy or missing information from audio-\nvisual sources [50].\nInformation given to agents may be correct initially,\nbut over time no longer be the optimal solution [122].\nOther advice may be mostly accurate or correct for\n19\nTable 1: Summary of the reviewed assisted reinforcement learning approaches adapted to the proposed\ntaxonomy.\nApproach\nInformation\nsource\nTempora-\nlity\nAdvice\ninterpre-\ntation\nAdvice\nstructure\nExternal\nmodel\nAgent\nmodiﬁ-\ncation\nAssisted\nagent\nHeuristic\nreinforcement\nlearning\nHuman-\ndomain\nexpert\nPlanned\nConvert\nrule to\nmachine\nlanguage\nMachine\nrule\nRetained\nrule-set\nPolicy\nshaping\nNormal\nagent\nInteractive\nreinforcement\nlearning\nHuman /\nsimulated\nuser\nInterac-\ntive\nConvert\nmodal cue\nto signal\nState-\naction\npair\nImmediate\nPolicy /\nreward\nshaping\nCuriosity-\ndriven\nagent\nReinforce-\nment learning\nfrom demon-\nstration\nDomain\nexpert\nPlanned\nConvert\ndemon-\nstration to\nagent’s\nperspec-\ntive\nRule\nsystem\nRetained\nrule\nsystem\nAction\nbiasing\nCuriosity-\ndriven\nagent\nTransfer\nlearning\nAgent with\ndiﬀerent\ncapabilities\nPlanned\nQ-values,\nrules, or\nmodels\nValue,\nrule, or\nmodel\nRetained\nsource\nmodel\nAction\nbiasing\nNormal\nagent\nMultiple\ninformation\nsources\nMulti-users\nor\nmulti-agent\nsystem\nPlanned\nor interac-\ntive\nMulti-\nsource\nintegra-\ntion\nIntegrated\nadvice\nSeparated\nor\ncombined\nmodel\nWeighted\nor\nunweighted\ncombina-\ntion\nMulti-\npolicy\nagent\nmost states, however, there can exist states of excep-\ntion to the advice. These exception states can be the\ncritical diﬀerence between an ordinary solution and\nthe optimal solution. There is a need for research on\nhow to identify and mitigate incorrect information\nin these scenarios, especially considering that even a\nvery small amount of incorrect advice may be really\ndetrimental for the learning process [53].\n5.2\nMultiple Information Sources\nAs reviewed in the previous section, the use of multi-\nple information sources may naturally arise on some\napplication scenarios, and can increase the agent’s\nknowledge of the environment, and increase conﬁ-\ndence in decision-making if the diﬀerent sources agree\non an action. However, the use of multiple sources\nraises additional questions:\n• What if the diﬀerent sources disagree on the best\naction to take?\n• How can the agent identify the best information\nsource to listen to?\n• How can the agent manage conﬂicting informa-\ntion?\n• How can the agent measure trust in the diﬀerent\ninformation sources?\n20\nAdditionally, the use of multiple sources may be ex-\ntended to crowdsourcing [45]. In this context, crowd-\nsourcing refers to the enlistment and use of a large\nnumber of people, either paid or unpaid and can range\nin size from tens to tens of thousands.\nTypically,\ncrowdsourcing is performed via the internet. This can\nraise challenges of malicious users, anonymity, and\nlarge uncertainty in the value and reliability of the\ninformation.\n5.3\nExplainability\nExplainability refers to translating the agent’s infor-\nmation into a form the human can understand [151,\n152]. The reasons why an agent develops certain be-\nhaviours can sometimes be diﬃcult to understand for\nnon-expert end-users. Systems to measure the quality\nof explanations generated by AI-based systems have\nbeen previously introduced in order to build eﬀec-\ntive and eﬃcient human-AI interaction [153]. When\ncombining the RL method with policy modiﬁcation\nmethods such as rules, expert assistance, external\nmodels, and policy-shaping, understanding why an\nagent chooses to take an action becomes even more\ndiﬃcult. Developing methods for understanding agent\nlearning and its decision-making is important as it\nallows the human to remain informed of the agent’s\nmotivations and decisions, and keep track of the ac-\ncountability of the actions taken [154]. This can be\nbeneﬁcial for artiﬁcial intelligence ethics, and human-\ncomputer teaching, among other ﬁelds.\n5.4\nTwo-Way Communication\nTwo-way communication refers to the ability for the\ninformation source and the agent to converse with\neach other, perhaps multiple times before making a\ndecision [155]. Two-way communication can allow the\ninformation source, presumably human, and the agent\nto ask questions to each other, request more informa-\ntion, and to clarify decision-making and its reasoning.\nAlthough the proposed framework includes two-way\ncommunication, as shown in Figure 1, most current\nARL methods do not have two-way communication\nto the extent that non-expert human advisors can\ninteract with the agent freely. For two-way communi-\ncation to apply to non-expert human advisors issues\nof explainability (as shown in the previous section),\ntiming, and agent initiation need to be addressed.\nTiming refers to the time it takes to communicate\nback and forth. Agents sometimes have a ﬁxed time\nlimit, during which they need to learn, communicate,\nand decide on the next action. Methods for reducing\nthe time it takes to interact with the human and reduc-\ning the number of interactions needed with the human\nare two areas open for research. Agent initiation refers\nto the ability for the agent to initiate communication\nwith the human source itself. The agent may choose\nto do this so to request clariﬁcation on information,\nor request assistance for decision-making. A challenge\nfor agent initiation is to determine when and how of-\nten the agent should request assistance. The requests\nfor assistance should be frequent enough to make use\nof the information source while not becoming a nui-\nsance to the human, or detracting from learning time,\nand should consider the cost of the request, e.g., in\npaid crowdsourcing.\n5.5\nOther Challenges\nThere are also other challenges to be considered for\nfuture possibilities of ARL systems. Although many\nof the issues described in this section are also shared\nwith autonomous RL [156], we focus the discussion on\nhow particularly externally-inﬂuenced agents may be\naﬀected in the context of the ARL framework. While\nwe describe the essential implications on ARL systems\nfor each of the following areas, we note that further\nand deeper discussion may be addressed for each of\nthem.\n• Real-time policy inference: Many RL sys-\ntems need to be deployed in real-world scenarios\nand, therefore, policy inference must happen in\nreal-time [157]. Using ARL frameworks may lead\nto additional issues since the external informa-\ntion source should observe and react to the RL\nagent’s state as fast as possible, otherwise the\nassistance may become unnecessary or incorrect\nfor the new reached state.\n• Assistance delay: There are RL systems where\ndetermining the state or receiving the reward sig-\n21\nnal may take even weeks, such as a recommender\nsystem where the reward is based on user in-\nteraction [158]. In these contexts, the external\ninformation source may also lead to unknown\ndelays in the system actuators, sensors, or re-\nwards, making the assistance atemporal, either\ndelayed or ahead, or even in some cases being con-\nﬂicting or redundant considering the RL agent’s\nautonomous operation.\n• Continuous states and actions: When an RL\nagent works in high-dimensional continuous state\nand action spaces [35, 107] there could be is-\nsues for learning even in traditional RL [159].\nIn an ARL framework, additional problems may\nbe present as the agent uses external informa-\ntion which may be not accurate enough given\nthe high dimensionality. In the presence of high-\ndimensional states and actions, even small diﬀer-\nences in the received assistance may substantially\nslow the learning process since these diﬀerences\nmay represent in essence a very diﬀerent state or\naction.\n• Safety constraints: In RL environments, there\nare safety constraints that should never or at\nleast rarely be violated [160].\nSpecial care is\nneeded when receiving information from an ex-\nternal source since there could be situations that\nthe advisor may repeatedly direct the agent to\nunsafe states and, in turn, lead to an increase in\nthe time needed for learning.\n• Partially observable environments: In prac-\ntice, many RL problems are partially observ-\nable [161]. For instance, partial observabilities\nmay occur in non-stationary environments [35]\nor in presence of stochastic transitions [162]. If\nthe external information source does not have\nobservations to clearly infer the current state in\nthe environment may lead to giving incorrect\nassistance to the learner agent.\n• Multi-objective reward: In many cases, RL\nagents need to balance multiple and conﬂict-\ning subgoals, therefore, they may use multi-\ndimensional reward functions [163]. In this re-\ngard, an external information source may give\npriority to a particular subgoal over the others,\nunbalancing the global reward function. There\ncould be also issues when multiple information\nsources are used covering or favouring diﬀerent\nsubgoals. Moreover, when using a multi-objective\nreward in TL, there could only be some subgoals\nfrom the source task which are relevant in the\ntarget task, therefore, the RL agent should also\ncoordinate and ﬁlter relevant information.\n• Multi-agent systems: There could be multiple\nagents learning a task and multiple external in-\nformation sources. In this case, if an information\nsource provides advice it could be generalised to\nall of them or it could be pointed speciﬁcally to\nan agent. Moreover, advice useful for one agent\nmay be detrimental to another, depending on the\nstate, the agent’s current knowledge, or its partic-\nular reward function. Using multiple information\nsources, if an agent consults an external source,\nit may be necessary to discriminate which one\nis the best for the particular state. Additionally,\nthe teacher-student approach usually integrated\ninto ARL requires the teacher to be an expert\nin the learning domain. In this regard, multiple\nlearning agents may also advise each other while\nlearning in a common environment [140].\n6\nConclusions\nIn this article, we have reviewed ARL methods and\npresented an ARL framework, comprising all RL tech-\nniques that use external information. ARL methods\nuse external information to supplement the informa-\ntion the agent receives from the environment to im-\nprove performance and decision-making.\nTo describe the diﬀerent ARL methods, we propose\na taxonomy to classify the diﬀerent functions of an\nexternally-inﬂuenced RL agent. Through the analysis\nof the current literature, we have found seven key\nfeatures that make up an ARL technique. They are\ndivided into four processing components and three\ncommunication links. A deﬁnition and examples of\neach of these seven features have been presented.\nThe contribution of this paper is twofold: the review\n22\nof state-of-the-art ARL methods and the ARL taxon-\nomy as an additional level of abstraction. However,\nfuture work framed into our proposed ARL taxonomy\ncan also make use of the diﬀerent concepts here de-\nﬁned, either processing components or communication\nlinks. In this regard, it is essential to understand that\nnot each ARL method must necessarily use all the\nproposed concepts. In some cases, simpliﬁed models\nmay also be a representation of the ARL framework.\nAdditionally, we demonstrated the applicability of\nthe framework on diﬀerent ARL ﬁelds. These areas\ninclude heuristic RL, IntRL, RLfD, TL, and multiple\ninformation sources. Each of these ﬁelds has been\nanalysed and described as applied to the presented\ntaxonomy. Finally, we also present some ideas about\nareas for future research in order to extend the ARL\nﬁeld.\nAcknowledgments\nThis work has been partially supported by the\nAustralian Government Research Training Program\n(RTP) and the RTP Fee-Oﬀset Scholarship through\nFederation University Australia.\nReferences\n[1] R. S. Sutton and A. G. Barto, Reinforcement\nlearning: An introduction. MIT press, 2018.\n[2] L. P. Kaelbling, M. L. Littman, and A. W.\nMoore,\n“Reinforcement\nlearning:\nA\nsur-\nvey,” Journal of artiﬁcial intelligence research,\npp. 237–285, 1996.\n[3] H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda,\nE. Osawa, and H. Matsubara, “RoboCup: A\nchallenge problem for AI,” AI magazine, vol. 18,\nno. 1, p. 73, 1997.\n[4] J. Kober, J. A. Bagnell, and J. Peters, “Re-\ninforcement learning in robotics:\nA survey,”\nThe International Journal of Robotics Research,\nvol. 32, no. 11, pp. 1238–1274, 2013.\n[5] F. Cruz, P. W¨uppen, A. Fazrie, C. Weber,\nand S. Wermter, “Action selection methods\nin a robotic reinforcement learning scenario,”\nin 2018 IEEE Latin American Conference on\nComputational Intelligence (LA-CCI), pp. 1–6,\nIEEE, 2018.\n[6] R. Contreras, A. Ayala, and F. Cruz, “Un-\nmanned aerial vehicle control through domain-\nbased automatic speech recognition,” Comput-\ners, vol. 9, no. 3, p. 75, 2020.\n[7] G. Tesauro, “TD-Gammon, a self-teaching\nbackgammon program, achieves master-level\nplay,” Neural computation, vol. 6, no. 2, pp. 215–\n219, 1994.\n[8] P. Barros, A. Tanevska, F. Cruz, and A. Sci-\nutti, “Moody learners-explaining competitive\nbehaviour of reinforcement learning agents,” in\n2020 Joint IEEE 10th International Conference\non Development and Learning and Epigenetic\nRobotics (ICDL-EpiRob), pp. 1–8, IEEE, 2020.\n[9] I. Giannoccaro and P. Pontrandolfo, “Inventory\nmanagement in supply chains: a reinforcement\nlearning approach,” International Journal of\nProduction Economics, vol. 78, no. 2, pp. 153–\n161, 2002.\n[10] A. Shakarami, M. Ghobaei-Arani, M. Mas-\ndari, and M. Hosseinzadeh, “A survey on\nthe computation oﬄoading approaches in mo-\nbile edge/cloud computing environment:\na\nstochastic-based perspective,” Journal of Grid\nComputing, pp. 1–33, 2020.\n[11] A. Shahidinejad and M. Ghobaei-Arani, “Joint\ncomputation oﬄoading and resource provision-\ning for edge-cloud computing environment:\nA machine learning-based approach,” Soft-\nware: Practice and Experience, vol. 50, no. 12,\npp. 2212–2230, 2020.\n[12] M.\nGhobaei-Arani,\nA.\nA.\nRahmanian,\nM. Shamsi, and A. Rasouli-Kenari, “A learning-\nbased approach for virtual machine placement\nin cloud data centers,” International Journal\nof Communication Systems, vol. 31, no. 8,\np. e3537, 2018.\n23\n[13] A. R. Cassandra and L. P. Kaelbling, “Learning\npolicies for partially observable environments:\nScaling up,” in Proceedings of the International\nConference on Machine Learning ICML, p. 362,\nMorgan Kaufmann, 2016.\n[14] L. J. Lin, “Programming robots using reinforce-\nment learning and teaching.,” in Proceedings of\nthe Association for the Advancement of Artiﬁ-\ncial Intelligence conference AAAI, pp. 781–786,\n1991.\n[15] B. Price and C. Boutilier, “Accelerating re-\ninforcement learning through implicit imita-\ntion,” Journal of Artiﬁcial Intelligence Research,\nvol. 19, pp. 569–629, 2003.\n[16] S. Griﬃth, K. Subramanian, J. Scholz, C. Isbell,\nand A. L. Thomaz, “Policy shaping: Integrating\nhuman feedback with reinforcement learning,”\nin Advances in Neural Information Processing\nSystems, pp. 2625–2633, 2013.\n[17] F. Fern´andez and M. Veloso, “Probabilistic pol-\nicy reuse in a reinforcement learning agent,” in\nProceedings of the International Conference on\nAutonomous Agents and Multiagent Systems\nAAMAS, pp. 720–727, ACM, 2006.\n[18] G. Konidaris, S. Kuindersma, R. Grupen, and\nA. Barto, “Robot learning from demonstration\nby constructing skill trees,” The International\nJournal of Robotics Research, vol. 31, no. 3,\npp. 360–375, 2012.\n[19] L. Rozo, P. Jim´enez, and C. Torras, “A robot\nlearning from demonstration framework to per-\nform force-based manipulation tasks,” Intelli-\ngent service robotics, vol. 6, no. 1, pp. 33–51,\n2013.\n[20] S.-A. Chen, V. Tangkaratt, H.-T. Lin, and\nM. Sugiyama, “Active deep Q-learning with\ndemonstration,” Machine Learning, pp. 1–27,\n2019.\n[21] W. B. Knox and P. Stone, “Combining manual\nfeedback with subsequent MDP reward signals\nfor reinforcement learning,” in Proceedings of\nthe International Conference on Autonomous\nAgents and Multiagent Systems AAMAS, pp. 5–\n12, International Foundation for Autonomous\nAgents and Multiagent Systems, 2010.\n[22] M. E. Taylor and P. Stone, “Transfer learning\nfor reinforcement learning domains: A survey,”\nJournal of Machine Learning Research, vol. 10,\nno. 7, pp. 1633–1685, 2009.\n[23] J. Randløv and P. Alstrøm, “Learning to drive\na bicycle using reinforcement learning and shap-\ning,” in Proceedings of the International Confer-\nence on Machine Learning ICML, pp. 463–471,\n1998.\n[24] N. Vlassis, M. Ghavamzadeh, S. Mannor, and\nP. Poupart, “Bayesian reinforcement learning,”\nReinforcement Learning, pp. 359–386, 2012.\n[25] T. Erez and W. D. Smart, “What does shaping\nmean for computational reinforcement learn-\ning?,” in Proceedings of the IEEE International\nConference on Development and Learning ICDL,\npp. 215–219, IEEE, 2008.\n[26] A. L. Thomaz and C. Breazeal, “Asymmetric\ninterpretations of positive and negative human\nfeedback for a social learning agent,” in Proceed-\nings of the IEEE International Symposium on\nRobot and Human Interactive Communication\nRO-MAN, pp. 720–725, IEEE, 2007.\n[27] H. Xu, R. Bector, and Z. Rabinovich, “Teach-\ning multiple learning agents by environment-\ndynamics tweaks,” in AAMAS Adaptive and\nLearning Agents Workshop ALA 2020, p. 8,\n2020.\n[28] C. Arzate Cruz and T. Igarashi, “A survey on\ninteractive reinforcement learning: Design prin-\nciples and open challenges,” in Proceedings of\nthe 2020 ACM Designing Interactive Systems\nConference, pp. 1195–1209, 2020.\n[29] J. Lin, Z. Ma, R. Gomez, K. Nakamura, B. He,\nand G. Li, “A review on interactive reinforce-\nment learning from human social feedback,”\nIEEE Access, vol. 8, pp. 120757–120765, 2020.\n24\n[30] F. L. Da Silva, G. Warnell, A. H. R. Costa, and\nP. Stone, “Agents teaching agents: a survey\non inter-agent transfer learning,” Autonomous\nAgents and Multi-Agent Systems, vol. 34, no. 1,\np. 9, 2020.\n[31] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu,\nH. Zhu, H. Xiong, and Q. He, “A comprehensive\nsurvey on transfer learning,” Proceedings of the\nIEEE, vol. 109, no. 1, pp. 43–76, 2020.\n[32] F. Cruz, S. Magg, C. Weber, and S. Wermter,\n“Training agents with interactive reinforcement\nlearning and contextual aﬀordances,” IEEE\nTransactions on Cognitive and Developmental\nSystems, vol. 8, no. 4, pp. 271–284, 2016.\n[33] F. Cruz, G. I. Parisi, and S. Wermter, “Multi-\nmodal feedback for aﬀordance-driven interactive\nreinforcement learning,” in Proceedings of the\nInternational Joint Conference on Neural Net-\nworks IJCNN, pp. 5515–5122, IEEE, 2018.\n[34] H. B. Suay and S. Chernova, “Eﬀect of human\nguidance and state space size on interactive\nreinforcement learning,” in Proceedings of the\nIEEE International Symposium on Robot and\nHuman Interactive Communication RO-MAN,\npp. 1–6, IEEE, 2011.\n[35] C. Mill´an, B. Fernandes, and F. Cruz, “Hu-\nman feedback in continuous actor-critic rein-\nforcement learning,” in Proceedings of the Euro-\npean Symposium on Artiﬁcial Neural Networks,\nComputational Intelligence and Machine Learn-\ning ESANN, pp. 661–666, ESANN, 2019.\n[36] Y. Niv, “Reinforcement learning in the brain,”\nJournal of Mathematical Psychology, vol. 53,\nno. 3, pp. 139–154, 2009.\n[37] E. Sert, Y. Bar-Yam, and A. J. Morales, “Seg-\nregation dynamics with reinforcement learning\nand agent based modeling,” Scientiﬁc reports,\nvol. 10, no. 1, pp. 1–12, 2020.\n[38] S. Amershi, M. Cakmak, W. B. Knox, and\nT. Kulesza, “Power to the people: The role\nof humans in interactive machine learning,” AI\nMagazine, vol. 35, no. 4, pp. 105–120, 2014.\n[39] F. Cruz, P. W¨uppen, S. Magg, A. Fazrie, and\nS. Wermter, “Agent-advising approaches in an\ninteractive reinforcement learning scenario,” in\nProceedings of the Joint IEEE International\nConference on Development and Learning and\nEpigenetic Robotics ICDL-EpiRob, pp. 209–214,\nIEEE, 2017.\n[40] B. Argall, B. Browning, and M. Veloso, “Learn-\ning by demonstration with critique from a hu-\nman teacher,” in Proceedings of the ACM/IEEE\nInternational Conference on Human-Robot In-\nteraction HRI, pp. 57–64, ACM, 2007.\n[41] A.\nNair,\nB.\nMcGrew,\nM.\nAndrychowicz,\nW. Zaremba, and P. Abbeel, “Overcoming explo-\nration in reinforcement learning with demonstra-\ntions,” in Proceedings of the IEEE International\nConference on Robotics and Automation ICRA,\npp. 6292–6299, IEEE, 2018.\n[42] K. Shao, Y. Zhu, and D. Zhao, “Starcraft micro-\nmanagement with reinforcement learning and\ncurriculum transfer learning,” IEEE Transac-\ntions on Emerging Topics in Computational In-\ntelligence, vol. 3, no. 1, pp. 73–84, 2018.\n[43] M. E. Taylor, P. Stone, and Y. Liu, “Value func-\ntions for rl-based behavior transfer: A compar-\native study,” in Proceedings of the Association\nfor the Advancement of Artiﬁcial Intelligence\nconference AAAI, vol. 5, pp. 880–885, 2005.\n[44] C. L. Isbell, M. Kearns, D. Kormann, S. Singh,\nand P. Stone, “Cobot in LambdaMOO: A social\nstatistics agent,” in Proceedings of the Associ-\nation for the Advancement of Artiﬁcial Intelli-\ngence conference AAAI, pp. 36–41, 2000.\n[45] E. Kamar, S. Hacker, and E. Horvitz, “Com-\nbining human and machine intelligence in large-\nscale crowdsourcing,” in Proceedings of the In-\nternational Conference on Autonomous Agents\nand Multiagent Systems AAMAS, pp. 467–\n474, International Foundation for Autonomous\nAgents and Multiagent Systems, 2012.\n25\n[46] A. Y. Ng, D. Harada, and S. Russell, “Policy in-\nvariance under reward transformations: Theory\nand application to reward shaping,” in Proceed-\nings of the International Conference on Machine\nLearning ICML, vol. 99, pp. 278–287, 1999.\n[47] A. L. Thomaz, C. Breazeal, et al., “Reinforce-\nment learning with human teachers: Evidence\nof feedback and guidance with implications for\nlearning performance,” in Proceedings of the\nAssociation for the Advancement of Artiﬁcial\nIntelligence conference AAAI, vol. 6, pp. 1000–\n1005, Boston, MA, 2006.\n[48] L. C. Cobo, K. Subramanian, C. L. Isbell Jr,\nA. D. Lanterman, and A. L. Thomaz, “Abstrac-\ntion from demonstration for eﬃcient reinforce-\nment learning in high-dimensional domains,”\nArtiﬁcial Intelligence, vol. 216, pp. 103–128,\n2014.\n[49] K. Subramanian, C. L. Isbell Jr, and A. L.\nThomaz, “Exploration from demonstration for\ninteractive reinforcement learning.,” in Proceed-\nings of the International Conference on Au-\ntonomous Agents and MultiAgent Systems AA-\nMAS, pp. 447–456, 2016.\n[50] F. Cruz, G. I. Parisi, J. Twiefel, and S. Wermter,\n“Multi-modal integration of dynamic audiovisual\npatterns for an interactive reinforcement learn-\ning scenario,” in Proceedings fo the IEEE/RSJ\nInternational Conference on Intelligent Robots\nand Systems IROS, pp. 759–766, IEEE, 2016.\n[51] L. Torrey and M. E. Taylor, “Teaching on a Bud-\nget: Agents Advising Agents in Reinforcement\nLearning,” in Proceedings of the International\nConference on Autonomous Agents and Multia-\ngent Systems AAMAS, 2013.\n[52] M. E. Taylor, N. Carboni, A. Fachantidis, I. Vla-\nhavas, and L. Torrey, “Reinforcement learning\nagents providing advice in complex video games,”\nConnection Science, vol. 26, no. 1, pp. 45–63,\n2014.\n[53] F. Cruz, S. Magg, Y. Nagai, and S. Wermter,\n“Improving interactive reinforcement learning:\nWhat makes a good teacher?,” Connection Sci-\nence, vol. 30, no. 3, pp. 306–325, 2018.\n[54] I. Partalas, D. Vrakas, and I. Vlahavas, “Rein-\nforcement learning and automated planning: A\nsurvey,” in Artiﬁcial Intelligence for Advanced\nProblem Solving Techniques, pp. 148–165, IGI\nGlobal, 2008.\n[55] S.-T. Cheng, T.-Y. Chang, and C.-W. Hsu, “A\nframework of an agent planning with reinforce-\nment learning for e-pet,” in Proceedings of the\nInternational Conference on Orange Technolo-\ngies ICOT, pp. 310–313, IEEE, 2013.\n[56] P. M. Pilarski and R. S. Sutton, “Between in-\nstruction and reward: human-prompted switch-\ning,” in AAAI Fall Symposium Series: Robots\nLearning Interactively from Human Teachers,\npp. 45–52, 2012.\n[57] C. Stahlhut, N. Navarro-Guerrero, C. Weber,\nS. Wermter, and V.-K.-S. WTM, “Interaction is\nmore beneﬁcial in complex reinforcement learn-\ning problems than in simple ones,” in Proceed-\nings of the Interdisziplin¨arer Workshop Kogni-\ntive Systeme (KogSys), pp. 142–150, 2015.\n[58] L. A. Celiberto Jr, C. H. Ribeiro, A. H. Costa,\nand R. A. Bianchi, Heuristic reinforcement\nlearning applied to robocup simulation agents,\npp. 220–227. Springer, 2007.\n[59] S. Kalyanakrishnan, Y. Liu, and P. Stone, “Half\nﬁeld oﬀense in RoboCup soccer: A multiagent\nreinforcement learning case study,” in Robot\nSoccer World Cup, pp. 72–85, Springer, 2006.\n[60] M. Hausknecht, P. Mupparaju, S. Subramanian,\nS. Kalyanakrishnan, and P. Stone, “Half ﬁeld\noﬀense: An environment for multiagent learn-\ning and ad hoc teamwork,” in AAMAS Adap-\ntive and Learning Agents Workshop ALA 2016,\n2016.\n26\n[61] A. L. Thomaz, G. Hoﬀman, and C. Breazeal,\n“Reinforcement learning with human teachers:\nUnderstanding how people want to teach robots,”\nin Proceedings of the IEEE International Sym-\nposium on Robot and Human Interactive Com-\nmunication RO-MAN, pp. 352–357, IEEE, 2006.\n[62] K. Subramanian, C. Isbell, and A. Thomaz,\n“Learning options through human interaction,”\nin IJCAI Workshop on Agents Learning Interac-\ntively from Human Teachers (ALIHT), Citeseer,\n2011.\n[63] A. Bignold, F. Cruz, R. Dazeley, P. Vamplew,\nand C. Foale, “Human engagement providing\nevaluative and informative advice for interac-\ntive reinforcement learning,” arXiv preprint\narXiv:2009.09575, 2020.\n[64] K. Shiarlis, J. ao Messias, and S. Whiteson,\n“Inverse reinforcement learning from failure,” in\nProceedings of the International Conference on\nAutonomous Agents and Multiagent Systems\nAAMAS, pp. 1060–1068, 2016.\n[65] F. Cruz, J. Twiefel, S. Magg, C. Weber, and\nS. Wermter, “Interactive reinforcement learning\nthrough speech guidance in a domestic scenario,”\nin Proceedings of the International Joint Confer-\nence on Neural Networks IJCNN, pp. 1341–1348,\nIEEE, 2015.\n[66] X. Liu, R. Deng, K.-K. R. Choo, and Y. Yang,\n“Privacy-preserving reinforcement learning de-\nsign for patient-centric dynamic treatment\nregimes,” IEEE Transactions on Emerging Top-\nics in Computing, 2019.\n[67] F. L. Da Silva and A. H. R. Costa, “A survey on\ntransfer learning for multiagent reinforcement\nlearning systems,” Journal of Artiﬁcial Intelli-\ngence Research, vol. 64, pp. 645–703, 2019.\n[68] M. E. Taylor, G. Kuhlmann, and P. Stone, “Au-\ntonomous transfer for reinforcement learning,”\nin Proceedings of the International Conference\non Autonomous Agents and Multiagent Systems\nAAMAS, pp. 283–290, International Founda-\ntion for Autonomous Agents and Multiagent\nSystems, 2008.\n[69] S. Narvekar, J. Sinapov, M. Leonetti, and\nP. Stone, “Source task creation for curriculum\nlearning,” in Proceedings of the International\nConference on Autonomous Agents and Multia-\ngent Systems AAMAS, pp. 566–574, 2016.\n[70] A. C. Tenorio-Gonzalez, E. F. Morales, and\nL. Villase˜nor-Pineda, “Dynamic reward shaping:\ntraining a robot by voice,” in Advances in Artiﬁ-\ncial Intelligence–IBERAMIA 2010, pp. 483–492,\nSpringer, 2010.\n[71] F. Kaplan, P.-Y. Oudeyer, E. Kubinyi, and\nA. Mikl´osi, “Robotic clicker training,” Robotics\nand Autonomous Systems, vol. 38, no. 3, pp. 197–\n206, 2002.\n[72] W. B. Knox and P. Stone, “Interactively shaping\nagents via human reinforcement: The TAMER\nframework,” in Proceedings of the International\nConference on Knowledge Capture, pp. 9–16,\nACM, 2009.\n[73] M. Sharma, M. P. Holmes, J. C. Santamar´ıa,\nA. Irani, C. L. Isbell Jr, and A. Ram, “Trans-\nfer learning in real-time strategy games using\nhybrid cbr/rl.,” in Proceedings of the Interna-\ntional Joint Conference on Artiﬁcial Intelligence\nIJCAI, vol. 7, pp. 1041–1046, 2007.\n[74] A. Bignold, F. Cruz, R. Dazeley, P. Vamplew,\nand C. Foale, “Persistent rule-based interactive\nreinforcement learning,” Neural Computing and\nApplications, pp. 1–18, 2021.\n[75] I. Moreira, J. Rivas, F. Cruz, R. Dazeley, A. Ay-\nala, and B. Fernandes, “Deep reinforcement\nlearning with interactive feedback in a human–\nrobot environment,” Applied Sciences, vol. 10,\nno. 16, p. 5574, 2020.\n[76] F. Cruz, G. I. Parisi, and S. Wermter, “Learning\ncontextual aﬀordances with an associative neu-\nral architecture,” in Proceedings of the European\nSymposium on Artiﬁcial Neural Network. Com-\nputational Intelligence and Machine Learning\nESANN, pp. 665–670, UCLouvain, 2016.\n27\n[77] M. Dorigo and L. Gambardella, “Ant-Q: A re-\ninforcement learning approach to the traveling\nsalesman problem,” in Proceedings of Interna-\ntional Conference on Machine Learning ICML,\npp. 252–260, 2014.\n[78] B. Banerjee, “General game learning using\nknowledge transfer,” in Proceedings of the In-\nternational Joint Conference on Artiﬁcial Intel-\nligence IJCAI, pp. 672–677, 2007.\n[79] P. Abbeel and A. Y. Ng, “Apprenticeship learn-\ning via inverse reinforcement learning,” in Pro-\nceedings of the International Conference on Ma-\nchine learning ICML, pp. 1–8, ACM, 2004.\n[80] B. F. Skinner, “The shaping of phylogenic be-\nhavior,” Journal of the Experimental Analysis\nof Behavior, vol. 24, no. 1, pp. 117–120, 1975.\n[81] N. Churamani, F. Cruz, S. Griﬃths, and P. Bar-\nros, “iCub: learning emotion expressions using\nhuman reward,” in Proceedings of the Workshop\non Bio-inspired Social Robot Learning in Home\nScenarios, IEEE/RSJ IROS, p. 2, 2016.\n[82] A. Harutyunyan, S. Devlin, P. Vrancx, and\nA. Now´e, “Expressing arbitrary reward func-\ntions as potential-based advice.,” in Proceedings\nof the Association for the Advancement of Ar-\ntiﬁcial Intelligence conference AAAI, pp. 2652–\n2658, 2015.\n[83] P. Behboudian, Y. Satsangi, M. E. Taylor,\nA. Harutyunyan, and M. Bowling, “Useful pol-\nicy invariant shaping from arbitrary advice,” in\nAAMAS Adaptive and Learning Agents Work-\nshop ALA 2020, p. 9, 2020.\n[84] E. Wiewiora, G. Cottrell, and C. Elkan, “Prin-\ncipled methods for advising reinforcement learn-\ning agents,” in Proceedings of the International\nConference on Machine learning ICML, pp. 792–\n799, 2003.\n[85] S. Devlin and D. Kudenko, “Dynamic potential-\nbased reward shaping,” in Proceedings of the In-\nternational Conference on Autonomous Agents\nand Multiagent Systems AAMAS, pp. 433–\n440, International Foundation for Autonomous\nAgents and Multiagent Systems, 2012.\n[86] S. Devlin and D. Kudenko, “Theoretical consid-\nerations of potential-based reward shaping for\nmulti-agent systems,” in Proceedings of the In-\nternational Conference on Autonomous Agents\nand Multiagent Systems AAMAS, pp. 225–\n232, International Foundation for Autonomous\nAgents and Multiagent Systems, 2011.\n[87] J. Grizou, M. Lopes, and P.-Y. Oudeyer, “Robot\nlearning simultaneously a task and how to inter-\npret human instructions,” in Proceedings of the\nJoint IEEE International Conference on Devel-\nopment and Learning and Epigenetic Robotics\nICDL-EpiRob, pp. 1–8, IEEE, 2013.\n[88] N. Navidi, “Human AI interaction loop train-\ning: New approach for interactive reinforcement\nlearning,” arXiv preprint arXiv:2003.04203,\n2020.\n[89] A. Holzinger, “Interactive machine learning for\nhealth informatics: when do we need the human-\nin-the-loop?,” Brain Informatics, vol. 3, no. 2,\npp. 119–131, 2016.\n[90] M. J. Knowles and S. Wermter, “The hybrid\nintegration of perceptual symbol systems and\ninteractive reinforcement learning,” in Proceed-\nings of the International Conference on Hybrid\nIntelligent Systems, pp. 404–409, IEEE, 2008.\n[91] G. Tesauro, “Extending Q-learning to general\nadaptive multi-agent systems,” in Advances in\nneural information processing systems, pp. 871–\n878, 2004.\n[92] M. Kerzel, H. B. Mohammadi, M. A. Zamani,\nand S. Wermter, “Accelerating deep continuous\nreinforcement learning through task simpliﬁca-\ntion,” in Proceedings of the International Joint\nConference on Neural Networks IJCNN, pp. 1–6,\nIEEE, 2018.\n28\n[93] M. Breyer, F. Furrer, T. Novkovic, R. Siegwart,\nand J. Nieto, “Comparing task simpliﬁcations\nto learn closed-loop object picking using deep\nreinforcement learning,” IEEE Robotics and Au-\ntomation Letters, vol. 4, no. 2, pp. 1549–1556,\n2019.\n[94] M. Sridharan, B. Meadows, and R. Gomez,\n“What can I not do? towards an architecture for\nreasoning about and learning aﬀordances,” in\nProceedings of the International Conference on\nAutomated Planning and Scheduling, pp. 461–\n469, 2017.\n[95] K. Dixon, R. J. Malak, and P. K. Khosla, Incor-\nporating prior knowledge and previously learned\ninformation into reinforcement learning agents.\nCarnegie Mellon University, Institute for Com-\nplex Engineered Systems, 2000.\n[96] C. Mill´an-Arias, B. Fernandes, F. Cruz, R. Daze-\nley, and S. Fernandes, “A robust approach for\ncontinuous interactive actor-critic algorithms,”\nIEEE Access, pp. 104242–104260, 2021.\n[97] C. Mill´an-Arias, B. Fernandes, F. Cruz, R. Daze-\nley, and S. Fernandes, “A robust approach for\ncontinuous interactive reinforcement learning,”\nin Proceedings of the 8th International Confer-\nence on Human-Agent Interaction, pp. 278–280,\n2020.\n[98] R. A. Bianchi, L. A. Celiberto Jr, P. E. Santos,\nJ. P. Matsuura, and R. L. de Mantaras, “Trans-\nferring knowledge as heuristics in reinforcement\nlearning: A case-based approach,” Artiﬁcial In-\ntelligence, vol. 226, pp. 102–121, 2015.\n[99] M.-C.\nYang,\nH.\nSamani,\nand\nK.\nZhu,\n“Emergency-response locomotion of hexapod\nrobot with heuristic reinforcement learning us-\ning q-learning,” in Proceedings of the Interna-\ntional Conference on Interactive Collaborative\nRobotics, pp. 320–329, Springer, 2019.\n[100] A. L. Thomaz, G. Hoﬀman, and C. Breazeal,\n“Real-time interactive reinforcement learning for\nrobots,” in AAAI 2005 Workshop on Human\nComprehensible Machine Learning, 2005.\n[101] G. Li, R. Gomez, K. Nakamura, and B. He,\n“Human-centered reinforcement learning: A sur-\nvey,” IEEE Transactions on Human-Machine\nSystems, vol. 49, no. 4, pp. 337–349, 2019.\n[102] W. B. Knox and P. Stone, “Reinforcement learn-\ning from simultaneous human and MDP re-\nward.,” in Proceedings of the International Con-\nference on Autonomous Agents and Multiagent\nSystems AAMAS, pp. 475–482, 2012.\n[103] W. B. Knox and P. Stone, “Reinforcement learn-\ning from human reward: Discounting in episodic\ntasks,” in Proceedings of the IEEE International\nSymposium on Robot and Human Interactive\nCommunication RO-MAN, pp. 878–885, IEEE,\n2012.\n[104] Y. Zhan, H. B. Ammar, and M. E. Taylor,\n“Theoretically-Grounded Policy Advice from\nMultiple Teachers in Reinforcement Learning\nSettings with Applications to Negative Trans-\nfer,” in Proceedings of the International Joint\nConference on Artiﬁcial Intelligence IJCAI, July\n2016.\n[105] O. Amir, E. Kamar, A. Kolobov, and B. Grosz,\n“Interactive teaching strategies for agent train-\ning,” in Proceedings of the International Joint\nConference on Artiﬁcial Intelligence IJCAI,\npp. 804–811, 2016.\n[106] A. Bignold, F. Cruz, R. Dazeley, P. Vamplew,\nand C. Foale, “An evaluation methodology for\ninteractive reinforcement learning with simu-\nlated users,” Biomimetics, vol. 6, no. 1, p. 13,\n2021.\n[107] A. Ayala, C. Henr´ıquez, and F. Cruz, “Rein-\nforcement learning using continuous states and\ninteractive feedback,” in Proceedings of the In-\nternational Conference on Applications of Intel-\nligent Systems, pp. 1–5, 2019.\n[108] W. B. Knox,\nB. D. Glass,\nB. C. Love,\nW. T. Maddox, and P. Stone, “How humans\nteach agents,” International Journal of Social\nRobotics, vol. 4, no. 4, pp. 409–421, 2012.\n29\n[109] F. L. Da Silva, P. Hernandez-Leal, B. Kartal,\nand M. E. Taylor, “Uncertainty-aware action\nadvising for deep reinforcement learning agents,”\nin Proceedings of the Association for the Ad-\nvancement of Artiﬁcial Intelligence conference\nAAAI, pp. 5792–5799, 2020.\n[110] S. Schaal, “Learning from demonstration,” Ad-\nvances in neural information processing systems,\nvol. 9, pp. 1040–1046, 1997.\n[111] B. D. Argall, S. Chernova, M. Veloso, and\nB. Browning, “A survey of robot learning from\ndemonstration,” Robotics and autonomous sys-\ntems, vol. 57, no. 5, pp. 469–483, 2009.\n[112] W. D. Smart and L. P. Kaelbling, “Eﬀective\nreinforcement learning for mobile robots,” in\nProceedings of the IEEE International Confer-\nence on Robotics and Automation ICRA, vol. 4,\npp. 3404–3410, IEEE, 2002.\n[113] T. Brys, A. Harutyunyan, H. B. Suay, S. Cher-\nnova, M. E. Taylor, and A. Now´e, “Rein-\nforcement learning from demonstration through\nshaping,” in Proceedings of the International\nJoint Conference on Artiﬁcial Intelligence IJ-\nCAI, p. 26, 2015.\n[114] H. B. Suay, T. Brys, M. E. Taylor, and S. Cher-\nnova, “Learning from demonstration for shaping\nthrough inverse reinforcement learning,” in Pro-\nceedings of the International Conference on Au-\ntonomous Agents and Multiagent Systems AA-\nMAS, pp. 429–437, International Foundation for\nAutonomous Agents and Multiagent Systems,\n2016.\n[115] M. E. Taylor, H. B. Suay, and S. Chernova, “In-\ntegrating reinforcement learning with human\ndemonstrations of varying ability,” in Proceed-\nings of the International Conference on Au-\ntonomous Agents and Multiagent Systems AA-\nMAS, pp. 617–624, International Foundation for\nAutonomous Agents and Multiagent Systems,\n2011.\n[116] D. Pathak, P. Agrawal, A. A. Efros, and\nT. Darrell, “Curiosity-driven exploration by self-\nsupervised prediction,” in Proceedings of the\nIEEE Conference on Computer Vision and Pat-\ntern Recognition Workshops, pp. 16–17, 2017.\n[117] H. Bou Ammar, M. E. Taylor, K. Tuyls, and\nG. Weiss, “Reinforcement learning transfer us-\ning a sparse coded inter-task mapping,” in Euro-\npean Workshop on Multi-Agent Systems, pp. 1–\n16, Springer, 2011.\n[118] M. E. Taylor, P. Stone, and Y. Liu, “Transfer\nlearning via inter-task mappings for temporal\ndiﬀerence learning,” Journal of Machine Learn-\ning Research, vol. 8, no. 1, pp. 2125–2167, 2007.\n[119] M. E. Taylor, S. Whiteson, and P. Stone, “Trans-\nfer via Inter-Task Mappings in Policy Search Re-\ninforcement Learning,” in Proceedings of the In-\nternational Conference on Autonomous Agents\nand Multiagent Systems AAMAS, pp. 156–163,\n2007.\n[120] H. B. Ammar, E. Eaton, P. Ruvolo, and M. E.\nTaylor, “Unsupervised Cross-Domain Transfer\nin Policy Gradient Reinforcement Learning via\nManifold Alignment,” in Proceedings of the As-\nsociation for the Advancement of Artiﬁcial In-\ntelligence conference AAAI, 2015.\n[121] P. Hernandez-Leal, Y. Zhan, M. E. Taylor, L. E.\nSucar, and E. Munoz de Cote, “Eﬃciently de-\ntecting switches against non-stationary oppo-\nnents,” Autonomous Agents and Multi-Agent\nSystems, pp. 1–23, November 2016.\n[122] V. Akila and G. Zayaraz, “A brief survey on\nconcept drift,” in Intelligent Computing, Com-\nmunication and Devices, pp. 293–302, Springer,\n2015.\n[123] M. E. Taylor, “Assisting Transfer-Enabled Ma-\nchine Learning Algorithms: Leveraging Human\nKnowledge for Curriculum Design,” in The\nAAAI 2009 Spring Symposium on Agents that\nLearn from Human Teachers, March 2009.\n30\n[124] Y. Bengio, J. Louradour, R. Collobert, and\nJ. Weston, “Curriculum learning,” in Proceed-\nings of the International Conference on Machine\nlearning ICML, (New York, NY, USA), pp. 41–\n48, ACM, 2009.\n[125] M. Eppe, S. Magg, and S. Wermter, “Cur-\nriculum goal masking for continuous deep rein-\nforcement learning,” in Proceedings of the Joint\nIEEE International Conference on Development\nand Learning and Epigenetic Robotics ICDL-\nEpiRob, pp. 183–188, IEEE, 2019.\n[126] B. Peng, J. MacGlashan, R. Loftin, M. L.\nLittman, D. L. Roberts, and M. E. Taylor, “Cur-\nriculum Design for Machine Learners in Sequen-\ntial Decision Tasks (Extended Abstract),” in\nProceedings of the International Conference on\nAutonomous Agents and Multiagent Systems\nAAMAS, May 2017.\n[127] S. Narvekar, J. Sinapov, and P. Stone, “Au-\ntonomous task sequencing for customized cur-\nriculum design in reinforcement learning,” in\nProceedings of the International Joint Confer-\nence on Artiﬁcial Intelligence IJCAI, August\n2017.\n[128] F. L. Da Silva and A. H. R. Costa, “Object-\noriented curriculum generation for reinforce-\nment learning,” in Proceedings of the Interna-\ntional Conference on Autonomous Agents and\nMultiagent Systems AAMAS, pp. 1026–1034, In-\nternational Foundation for Autonomous Agents\nand Multiagent Systems, 2018.\n[129] Z. Chen and B. Liu, Lifelong Machine Learning.\nSynthesis Lectures on Artiﬁcial Intelligence and\nMachine Learning, Morgan & Claypool Publish-\ners, 2016.\n[130] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan,\nand S. Wermter, “Continual lifelong learning\nwith neural networks: A review,” Neural Net-\nworks, 2019.\n[131] T. Brys, A. Harutyunyan, P. Vrancx, A. Now´e,\nand M. E. Taylor, “Multi-objectivization and\nensembles of shapings in reinforcement learning,”\nNeurocomputing, vol. 263, pp. 48–59, 2017.\n[132] F. L. Da Silva, “Integrating agent advice and\nprevious task solutions in multiagent reinforce-\nment learning,” in Proceedings of the Interna-\ntional Conference on Autonomous Agents and\nMultiagent Systems AAMAS, pp. 2447–2448, In-\nternational Foundation for Autonomous Agents\nand Multiagent Systems, 2019.\n[133] M. Gimelfarb, S. Sanner, and C.-G. Lee, “Re-\ninforcement learning with multiple experts: A\nBayesian model combination approach,” in Ad-\nvances in Neural Information Processing Sys-\ntems, pp. 9528–9538, 2018.\n[134] T. Yamagata, R. Santos-Rodr´ıguez, R. Mc-\nConville, and A. Elsts, “Online feature selec-\ntion for activity recognition using reinforcement\nlearning with multiple feedback,” arXiv preprint\narXiv:1908.06134, 2019.\n[135] C. R. Shelton, “Balancing multiple sources\nof reward in reinforcement learning,” in Ad-\nvances in Neural Information Processing Sys-\ntems, pp. 1082–1088, 2001.\n[136] L. Nunes and E. Oliveira, “Exchanging advice\nand learning to trust,” Cooperative Information\nAgents VII, pp. 250–265, 2003.\n[137] B. D. Argall, B. Browning, and M. Veloso,\n“Automatic weight learning for multiple data\nsources when learning from demonstration,” in\nProceedings of the IEEE International Con-\nference on Robotics and Automation ICRA,\npp. 226–231, IEEE, 2009.\n[138] E. Parisotto, J. L. Ba, and R. Salakhutdi-\nnov, “Actor-mimic: Deep multitask and trans-\nfer reinforcement learning,” arXiv preprint\narXiv:1511.06342, 2015.\n[139] E. Talvitie and S. P. Singh, “An experts algo-\nrithm for transfer learning.,” in Proceedings of\nthe International Joint Conference on Artiﬁcial\nIntelligence IJCAI, pp. 1065–1070, 2007.\n31\n[140] F. L. Da Silva, R. Glatt, and A. H. R. Costa,\n“Simultaneously learning and advising in mul-\ntiagent reinforcement learning,” in Proceed-\nings of the International Conference on Au-\ntonomous Agents and Multiagent Systems AA-\nMAS, pp. 1100–1108, 2017.\n[141] A. Fachantidis, M. E. Taylor, and I. Vlahavas,\n“Learning to teach reinforcement learning agents,”\nMachine Learning and Knowledge Extraction,\nvol. 1, no. 1, pp. 21–42, 2019.\n[142] A. Holzinger,\nB. Malle,\nA. Saranti,\nand\nB. Pfeifer, “Towards multi-modal causability\nwith graph neural networks enabling informa-\ntion fusion for explainable ai,” Information Fu-\nsion, vol. 71, pp. 28–37, 2021.\n[143] J. Karlsson, “Learning to play games from\nmultiple imperfect teachers,” Master’s thesis,\nChalmers University of Technology, Gothenburg,\nSweden, 2014.\n[144] T. Brys, A. Now´e, D. Kudenko, and M. E. Tay-\nlor, “Combining multiple correlated reward and\nshaping signals by measuring conﬁdence.,” in\nProceedings of the Association for the Advance-\nment of Artiﬁcial Intelligence conference AAAI,\npp. 1687–1693, 2014.\n[145] W. B. Knox, P. Stone, and C. Breazeal, “Train-\ning a robot via human feedback: A case study,”\nin Proceedings of the International Conference\non Social Robotics, pp. 460–470, Springer, 2013.\n[146] A. K. Tanwani and A. Billard, “Transfer in in-\nverse reinforcement learning for multiple strate-\ngies,” in Proceedings of the IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and\nSystems IROS, pp. 3244–3250, IEEE, 2013.\n[147] D. M. Roijers, P. Vamplew, S. Whiteson, and\nR. Dazeley, “A survey of multi-objective se-\nquential decision-making,” Journal of Artiﬁcial\nIntelligence Research, vol. 48, pp. 67–113, 2013.\n[148] K.\nEfthymiadis,\nS.\nDevlin,\nand\nD.\nKu-\ndenko, “Overcoming erroneous domain knowl-\nedge in plan-based reward shaping,” in Pro-\nceedings of the International Conference on Au-\ntonomous Agents and Multiagent Systems AA-\nMAS, pp. 1245–1246, International Foundation\nfor Autonomous Agents and Multiagent Sys-\ntems, 2013.\n[149] Y. Gao, H. Xu, J. Lin, F. Yu, S. Levine,\nand T. Darrell, “Reinforcement learning from\nimperfect\ndemonstrations,”\narXiv preprint\narXiv:1802.05313, 2018.\n[150] M. Jing, X. Ma, W. Huang, F. Sun, C. Yang,\nB. Fang, and H. Liu, “Reinforcement learning\nfrom imperfect demonstrations under soft expert\nguidance,” in Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, pp. 5109–5116,\n2020.\n[151] F. Cruz, R. Dazeley, and P. Vamplew, “Memory-\nbased explainable reinforcement learning,” in\nProceedings of the Australasian Joint Con-\nference on Artiﬁcial Intelligence, pp. 66–77,\nSpringer, 2019.\n[152] R. Dazeley, P. Vamplew, C. Foale, C. Young,\nS. Aryal, and F. Cruz, “Levels of explainable\nartiﬁcial intelligence for human-aligned conver-\nsational explanations,” Artiﬁcial Intelligence,\np. 103525, 2021.\n[153] A. Holzinger, A. Carrington, and H. M¨uller,\n“Measuring the quality of explanations: the sys-\ntem causability scale (scs),” KI-K¨unstliche In-\ntelligenz, pp. 1–6, 2020.\n[154] R. Dazeley, P. Vamplew, and F. Cruz, “Ex-\nplainable reinforcement learning for broad-xai:\nA conceptual framework and survey,” arXiv\npreprint arXiv:2108.09003, 2021.\n[155] T. Kessler Faulkner, R. A. Gutierrez, E. S.\nShort, G. Hoﬀman, and A. L. Thomaz, “Active\nattention-modiﬁed policy shaping: socially inter-\nactive agents track,” in Proceedings of the Inter-\nnational Conference on Autonomous Agents and\nMultiagent Systems AAMAS, pp. 728–736, In-\nternational Foundation for Autonomous Agents\nand Multiagent Systems, 2019.\n32\n[156] D. J. Mankowitz, G. Dulac-Arnold, and T. Hes-\nter, “Challenges of real-world reinforcement\nlearning,” in ICML Workshop on Real-Life Re-\ninforcement Learning, p. 14, 2019.\n[157] S. Koenig and R. G. Simmons, “Complexity\nanalysis of real-time reinforcement learning,” in\nProceedings of the Association for the Advance-\nment of Artiﬁcial Intelligence conference AAAI,\npp. 99–107, 1993.\n[158] T. A. Mann, S. Gowal, R. Jiang, H. Hu, B. Lak-\nshminarayanan, and A. Gyorgy, “Learning from\ndelayed outcomes with intermediate observa-\ntions,” arXiv preprint arXiv:1807.09387, 2018.\n[159] G. Dulac-Arnold, R. Evans, H. van Hasselt,\nP. Sunehag, T. Lillicrap, J. Hunt, T. Mann,\nT. Weber, T. Degris, and B. Coppin, “Deep\nreinforcement learning in large discrete action\nspaces,” arXiv preprint arXiv:1512.07679, 2015.\n[160] T. G. Karimpanal, S. Rana, S. Gupta, T. Tran,\nand S. Venkatesh, “Learning transferable do-\nmain priors for safe exploration in reinforcement\nlearning,” in Proceedings of the International\nJoint Conference on Neural Networks IJCNN,\npp. 1–8, 2019.\n[161] H. Chen, B. Yang, and J. Liu, “Partially observ-\nable reinforcement learning for sustainable ac-\ntive surveillance,” in Proceedings of the Interna-\ntional Conference on Knowledge Science, Engi-\nneering and Management, pp. 425–437, Springer,\n2018.\n[162] F. Cruz, R. Dazeley, P. Vamplew, and M. Ithan,\n“Explainable robotic systems: Understanding\ngoal-driven actions in a reinforcement learning\nscenario,” Neural Computing and Applications,\npp. 1–17, 2021.\n[163] P.\nVamplew,\nC.\nFoale,\nand\nR.\nDazeley,\n“A demonstration of issues with value-based\nmultiobjective reinforcement learning under\nstochastic state transitions,” arXiv preprint\narXiv:2004.06277, 2020.\n33\n",
  "categories": [
    "cs.AI",
    "cs.LG",
    "cs.MA"
  ],
  "published": "2020-07-03",
  "updated": "2021-09-20"
}