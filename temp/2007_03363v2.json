{
  "id": "http://arxiv.org/abs/2007.03363v2",
  "title": "Deep Reinforcement Learning with Interactive Feedback in a Human-Robot Environment",
  "authors": [
    "Ithan Moreira",
    "Javier Rivas",
    "Francisco Cruz",
    "Richard Dazeley",
    "Angel Ayala",
    "Bruno Fernandes"
  ],
  "abstract": "Robots are extending their presence in domestic environments every day, being\nmore common to see them carrying out tasks in home scenarios. In the future,\nrobots are expected to increasingly perform more complex tasks and, therefore,\nbe able to acquire experience from different sources as quickly as possible. A\nplausible approach to address this issue is interactive feedback, where a\ntrainer advises a learner on which actions should be taken from specific states\nto speed up the learning process. Moreover, deep reinforcement learning has\nbeen recently widely utilized in robotics to learn the environment and acquire\nnew skills autonomously. However, an open issue when using deep reinforcement\nlearning is the excessive time needed to learn a task from raw input images. In\nthis work, we propose a deep reinforcement learning approach with interactive\nfeedback to learn a domestic task in a human-robot scenario. We compare three\ndifferent learning methods using a simulated robotic arm for the task of\norganizing different objects; the proposed methods are (i) deep reinforcement\nlearning (DeepRL); (ii) interactive deep reinforcement learning using a\npreviously trained artificial agent as an advisor (agent-IDeepRL); and (iii)\ninteractive deep reinforcement learning using a human advisor (human-IDeepRL).\nWe demonstrate that interactive approaches provide advantages for the learning\nprocess. The obtained results show that a learner agent, using either\nagent-IDeepRL or human-IDeepRL, completes the given task earlier and has fewer\nmistakes compared to the autonomous DeepRL approach.",
  "text": "In press Appl. Sci. 2020\nPage 1\nDeep Reinforcement Learning with Interactive\nFeedback in a Human-Robot Environment\nIthan Moreira1,‡\nJavier Rivas1,‡\nFrancisco Cruz1,2\nRichard Dazeley2\nAngel Ayala3\nBruno Fernandes3\n1 Escuela de Ingenier´ıa, Universidad Central de Chile, Santiago, Chile.\n2 School of Information Technology, Deakin University, Geelong, Australia.\n3 Escola Polit´ecnica de Pernambuco, Universidade de Pernambuco,\nRecife, Brasil.\nCorresponding e-mails: {ithan.moreira, javier.rivas}@alumnos.ucentral.cl,\n{francisco.cruz, richard.dazeley}@deakin.edu.au, {aaam, bjtf}@ecomp.poli.br\n‡ These authors contributed equally to this work.\nAbstract\nRobots are extending their presence in domestic environments every\nday, being more common to see them carrying out tasks in home sce-\nnarios. In the future, robots are expected to increasingly perform more\ncomplex tasks and, therefore, be able to acquire experience from diﬀerent\nsources as quickly as possible. A plausible approach to address this issue\nis interactive feedback, where a trainer advises a learner on which actions\nshould be taken from speciﬁc states to speed up the learning process.\nMoreover, deep reinforcement learning has been recently widely used in\nrobotics to learn the environment and acquire new skills autonomously.\nHowever, an open issue when using deep reinforcement learning is the ex-\ncessive time needed to learn a task from raw input images. In this work,\nwe propose a deep reinforcement learning approach with interactive feed-\nback to learn a domestic task in a human-robot scenario. We compare\nthree diﬀerent learning methods using a simulated robotic arm for the task\nof organizing diﬀerent objects; the proposed methods are (i) deep rein-\nforcement learning (DeepRL); (ii) interactive deep reinforcement learning\nusing a previously trained artiﬁcial agent as an advisor (agent-IDeepRL);\nand (iii) interactive deep reinforcement learning using a human advisor\n(human-IDeepRL). We demonstrate that interactive approaches provide\nadvantages for the learning process. The obtained results show that a\nlearner agent, using either agent-IDeepRL or human-IDeepRL, completes\nthe given task earlier and has fewer mistakes compared to the autonomous\nDeepRL approach.\nKeywords: Robotics, interactive deep reinforcement learning, deep rein-\nforcement learning, domestic scenario.\narXiv:2007.03363v2  [cs.AI]  11 Aug 2020\nIn press Appl. Sci. 2020\nPage 2\n1\nIntroduction\nRobotics has been getting more attention since new researched advances have in-\ntroduced signiﬁcant improvements to our society. For instance, for many years,\nrobots have been installed in the automotive industrial area [1]. However, the\ncurrent technological progress has allowed expanding the robot’s applications\ndomain in areas such as medicine, military, search and rescue, and entertain-\nment. In this regard, under current research, another challenging application of\nrobotics is its integration to domestic environments, mainly due to the presence\nof many dynamic variables in comparison to industrial contexts [2]. Moreover,\nin domestic environments, it is expected that humans regularly interact with\nrobots and that the robots can understand and respond accordingly to the in-\nteractions [3, 4].\nAlgorithms such as reinforcement learning (RL) [5] allow a robotic agent\nto autonomously learn new skills, in order to solve complex tasks inspired by\nthe way as humans do, through trial and error [6]. RL agents interact with\nthe environment in order to ﬁnd an appropriate policy that meets the problem\naims. To ﬁnd the appropriate policy, the agent interacts with the environment\nby performing an action at and, in turn, the environment returns a new state\nst+1 with a reward rt+1 for the performed action to adjust the policy. How-\never, an open issue in RL algorithms is the time and the resources required\nto achieve good learning outcomes [7, 8], which is especially critical in online\nenvironments [9, 10]. One of the reasons for this problem is that the agent, at\nthe beginning of the learning process, does not know the environment and the\ninteractions responses. Thus, to address this problem, the agent must explore\nmultiple paths to reﬁne its knowledge about the environment.\nIn continuous spaces, an alternative is to recognize the agent’s state directly\nfrom raw inputs. Deep reinforcement learning (DeepRL) [11] is based on the\nsame RL structure but also adds deep learning to process the function approx-\nimation for the state in multiple abstraction levels. An example of DeepRL\nimplementations is by convolutional neural networks (CNN) [12] which can be\nmodiﬁed to be used for DeepRL, e.g., DQN [13]. CNNs have brought signiﬁ-\ncant progress in the last years in diﬀerent areas such as image, video, audio and\nspeech processing, among others [14]. Nevertheless, for a robotic agent working\nin highly dynamic environments, DeepRL still needs excessive time to learn a\nnew task properly.\n2\nRelated works\nIn this section, we review previously developed works considering two main\nareas. First, we address the deep reinforcement learning approach and the use\nof interactive feedback. Following, we discuss the problem of vision-based object\nsorting using robots in order to contextualize our approach properly. Finally,\nwe describe the scientiﬁc contributions of this work.\nIn press Appl. Sci. 2020\nPage 3\n2.1\nDeep reinforcement learning and interactive feedback\nDeep reinforcement learning (DeepRL) combines reinforcement Learning (RL)\nand deep neural networks (DNN). This combination has allowed the enhance-\nment of RL agents when autonomously exploring a given scenario [15]. If an\nRL agent is learning a task, the environment gives the agent the necessary in-\nformation on how good or bad the taken actions are. With this information,\nthe agent must diﬀerentiate which actions lead to a better accomplishment of\nthe task aims [5].\nAims may be expressed by a reward function that assigns a numerical value\nto each performed action from a given state. Additionally, after performing an\naction, the agent reaches a new state. Therefore, the agent associates states\nwith actions to maximize r0 +γ ·r1 +γ2 ·r2 +..., where ri is the obtained reward\nin the i-th episode and γ is the discount factor parameter that indicates how\ninﬂuential future actions are.\nThe fundamental base of RL tasks is the Markov decision process (MDP),\nin which future transitions and rewards are aﬀected only by the current state\nand the selected action [16]. Therefore, if the Markovian property is present for\na state, this state contains all the information needed about the dynamics of a\ntask. For instance, chess is a classic example of the Markov property. In this\ngame, it does not matter the history of plays in order to make a decision about\nthe next movement.\nAll the information is already described in the current\ndistribution of pieces over the board. In other words, if the current state is\nknown, the previous transitions that led the agent to that situation become\nirrelevant in terms of the decision-making problem.\nFormally, an MDP is deﬁned by a 4-tuple < S, A, δ, r > where:\n– S is a ﬁnite set of system states, s ∈S;\n– A is a ﬁnite set of actions, a ∈A, and Ast ∈A is a ﬁnite set of actions\navailable in st ∈S at time t;\n– δ is the transition function δ : S × A →S;\n– r is the immediate reward (or reinforcement) function r : S × A →R.\nEach time-step t, the agent perceives the current state st ∈S and chooses an\naction at ∈A to be carried out. The environment gives a reward rt = r(st, at)\nand the agent moves to the state st+1 = δ(st, at). The functions r and δ depend\nonly on the current state st and action at, therefore, it is a no memory process.\nOver time, the agent tries to learn a policy π : S →A which, from a state st,\nyields the greatest value or discounted reward [5], as shown in Eq. (1).\nQπ(st, at) = rt + γ · rt+1 + γ2 · rt+2 + ... =\n∞\nX\ni=0\nγi · rt+1\n(1)\nwhere Qπ(st, at) is the action-value function following the policy π (e.g., choos-\ning action at) from a given state st.\nThe discount factor γ is a constant\nIn press Appl. Sci. 2020\nPage 4\n(0 ≤γ < 1) which determines the relative importance of immediate and fu-\nture rewards. For instance, in case γ = 0, then the agent is short-sighted and\nmaximizes only the immediate rewards, or in case γ →1 the agent is more\nforesighted in terms of future reward.\nThe ﬁnal goal of RL is to ﬁnd an optimal policy (π∗) mapping states to\nactions in order to maximize the future reward (r) over a time (t) with a discount\nrate (γ ∈[0, 1]), as shown in Eq. (2). In the equation, Eπ[] denotes the expected\nvalue given that the agent follows policy π and Q∗(st, at) denotes the optimal\naction-value function [5]. Table 1 summarizes all the elements shown within Eq.\n(2). In DeepRL, an approximation function, implemented by DNN, allows an\nagent to work with high-dimensional observation spaces, such as pixels of an\nimage [13].\nQ∗(st, at) = max\n\u0000Eπ[rt + γrt+1 + γ2rt+2 + ...|st = s, at = a, π]\n\u0001\n(2)\nTable 1: Elements to compute the optimal action-value function.\nSymbol\nMeaning\nQ∗(s, a)\nOptimal action-value function\nE\nExpected value following policy π\nπ\nPolicy to map states to actions\nγ\nDiscount factor\nrt\nReward received at time step t\nst\nAgent’s state at time step t\nat\nAction taken at time step t\nInteractive feedback is a method that improves the learning time of an RL\nagent [17]. In this method, an external trainer can guide the agent’s appren-\nticeship to explore more promising areas at early learning stages. The external\ntrainer is an agent that can be a human, a robot, or another artiﬁcial agent.\nThere are two principal strategies for providing interactive feedback in RL\nscenarios, i.e., evaluative and corrective feedback [18]. In the ﬁrst one, called\nreward-shaping, the trainer modiﬁes or accepts the reward given by the envi-\nronment in order to bias the agent’s learning [19, 20]. In the second one, called\npolicy-shaping, the trainer may suggest a diﬀerent action to perform, by replac-\ning the one proposes by the policy [21, 22]. A simple policy-shaping method\ninvolves forcing the agent to take certain actions that are recommended by the\ntrainer [23, 24]. For instance, a similar approach is used when a teacher is guid-\ning a child’s hand to learn how to draw a geometric ﬁgure. In this work, we\nuse the policy-shaping approach since it has been shown that humans using this\ntechnique to instruct an agent provide advice that is more accurate, are able to\nassist the learner agent for a longer time, and provide more advice per episode.\nMoreover, people using policy-shaping have reported that the agent’s ability to\nfollow the advice is higher, and therefore, felt their own advice to be of higher\nIn press Appl. Sci. 2020\nPage 5\n\u001f\u001e\u001d\u001c\u001b\u001a\n\u001b\u0019\u0018\u0017\n\u0016\u0019\u001c\u0019\u001e\n\u0015\u0019\u0018\u0017\n\u0014\u0013\u0019\u0012\u0011\u0010\n\u001c\u0019\n\u000f\u0011\u0010\u000e\u0012\u001b\r\u0015\f\u001c\u0013\u0019\u0012\u0011\u0010\f\u001c\u0019\f\u0011\u001b\f\n\u0015\u001e\u000b\u001e\u0013\u0019\u0015\f\u001c\u0010\f\u001c\u000b\u0019\u001e\u001b\u0010\u001c\u0019\u0012\n\u001e\f\n\u001c\u0013\u0019\u0012\u0011\u0010\nAGENT\nEXTERNAL\nTRAINER\nENVIRONMENT\nFigure 1: Policy-shaping interactive feedback approach. In this approach, the\ntrainer may advise the agent on what actions to take in a particular given state.\naccuracy when compared to people providing advice via reward-shaping [25].\nThe policy-shaping approach is depicted in Figure 1.\nThere are diﬀerent ways to support the agent’s learning, which in turn may\nlead to other problems. For instance, if the trainer delivers too much advice, the\nlearner never gets to know other alternatives because most of the decisions taken\nare given from the external trainer [26]. The quality of the given advice by the\nexternal trainer must also be considered to improve the learning. It has been\nshown that inconsistent advice may be very detrimental during the learning\nprocess, so that in case of low consistency of advice, autonomous learning may\nlead to better performance [27].\nOne additional strategy to better distribute the given advice is to use a\nbudget [26]. In this strategy, the trainer has a limited amount of interaction\nwith the learner agent, similar to the limited patience of a person for teaching.\nThere are diﬀerent ways of using the budget, in terms of when to interact or give\nadvice, namely, early advising, alternating advice, importance advising, mistake\ncorrecting, and predictive advising. In this work, we use early advising allowing\nus to fairly compare interactive approaches using the diﬀerent kinds of trainers\nused in the proposed methods, i.e., humans or artiﬁcial agents as trainers.\nAlthough there have been some approaches addressing the interactive deep\nreinforcement learning problem, they have been mostly used in other scenarios.\nFor instance, in [28] is presented an application to serious games, and in [29] is\npresented a dexterous robotic manipulation approach using demonstrations. In\nthe game scenario [28], the addressed task presents diﬀerent environmental dy-\nnamics compared to human-robot environments. Moreover, the authors propose\nundoing an action by the advisor, which is not always feasible. For example,\nin a human-robot environment, a robot might break an object as a result of a\nperformed action, which is impossible to undo.\nIn press Appl. Sci. 2020\nPage 6\n2.2\nVision-based object sorting with robots\nThe automation of sorting object tasks has been previously addressed using\nmachine learning techniques. For instance, Lukka et al. [30] implemented a\nrecycling robot for construction and demolition waste. In this work, the robot\nsorts the waste using a vision-based system for object recognition and object ma-\nnipulation to control the movement of the robot in order to classify the objects\npresented on a moving belt properly. The authors did not present performance\nresults since the approach is presented as a functional industrial prototype for\nsorting objects through images.\nThe object recognition problem is an extended research area that has been\naddressed by diﬀerent techniques, including deep learning, as presented in [31].\nThis approach is a similar system to [30] in terms of proposing to sort garbage\nfrom a moving belt. The authors used a convolutional neural network, called\nFast R-CNN, to obtain the moving object’s class and location, and send the\ninformation to the robotic grasping control unit to grasp the object and move it.\nAs the authors point out, the key problem the deep learning method tries to solve\nis the object identiﬁcation. Moreover, another approach to improve the object\nrecognition task is presented in [32], where the authors implemented a stereo\nvision system to recognize the material and the clothes categories. The stereo\nvision system creates a 3D reconstruction of the image to process and obtains\nlocal and global features to predict the clothing category class and manipulate\na robot that must grasp and sort the clothing in a preestablished box. These\ntwo systems, presented in [31] and [32], use a supervised learning method that\nrequires prior training of items to be sorted, leading to low generalization for\nnew objects.\nUsing RL to sort objects has also been previously addressed. For instance, in\n[33] a cleaning-table task in a simulated robotic scenario is presented. In order\nto complete the task, the robot needs to deal with objects such as a cup and a\nsponge. In this work, the RL agent used a discrete tabular RL approach com-\nplemented by interactive feedback and aﬀordances [34]. Therefore, the agent\ndid not deal with the problem of continuous visual inputs for state represen-\ntation. Furthermore, in [35] an approach for robotic control using DeepRL is\npresented. In this work, a simulated Baxter robot learned autonomous control\nusing a DQN-based system. When transferring the system to a real-world sce-\nnario, the approach failed. To ﬁx this, the system ran replacing the camera\nimages with synthetic images in order for the agent to acquire the state and\ndecide which action to take in real-world.\n2.3\nScientiﬁc contribution\nAlthough a robot may be capable of learning autonomously to sort objects in dif-\nferent contexts, current approaches address the problem using supervised deep\nlearning methods with previously labeled data to recognize the objects, e.g., [31]\nand [32]. In this regard, using the DeepRL approach allows classifying objects\nas well as deciding how to act with them. Additionally, if prior knowledge of\nIn press Appl. Sci. 2020\nPage 7\nthe problem is transferred to the DeepRL agent, e.g., using demonstrations [36],\nthe learning speed may also be improved. Therefore, using interactive feedback\nas an assistance method, we will be able to advise the learner agent during the\nlearning process, using both artiﬁcial and human trainers, to evaluate how the\nDeepRL method responds to the given advice.\nIn this work, we present an interactive-shaping vision-based algorithm de-\nrived from DeepRL, referred to here as interactive DeepRL or IDeepRL. Our\nalgorithm allows us to speed up the required learning time through strategic in-\nteraction with either a human or an artiﬁcial advisor. The information exchange\nbetween the learner and the advisor gives the learner a better understanding of\nthe environment by reducing the search space.\nWe have implemented a simulated domestic scenario, in which a robot has to\norganize objects considering color and shape. The RL agent perceives the world\nthrough RGB images and interacts through a robotic arm while an external\ntrainer may advise the agent a diﬀerent action to perform during the ﬁrst train-\ning steps. The implemented scenario is used for test and comparison between\nthe DeepRL and IDeepRL algorithms, as well as the evaluation of IDeepRL\nusing two diﬀerent types of trainers, namely, another artiﬁcial agent previously\ntrained and a human advisor.\nTherefore, the contribution of the proposed method is to demonstrate that\ninteractive-shaping advice can be eﬃciently integrated into vision-based deep\nlearning algorithms. The interactive learning methodologies proposed in this\nwork outperform current autonomous DeepRL approaches allowing to collect\nmore and faster reward using both artiﬁcial and human trainers.\n3\nMaterial and methods\n3.1\nMethodology and implementation of the agents\nIn this work, our focus is on assessing how interactive feedback, used as an\nassistance method, may aﬀect the performance of a DeepRL agent.\nTo this\naim, we implement three diﬀerent approaches for the RL agents:\ni. DeepRL: where the agent interacts autonomously with the environment;\nii. agent-IDeepRL: where the DeepRL approach is complemented with a pre-\nviously trained artiﬁcial agent to give advice; and\niii. human-IDeepRL: where the DeepRL approach is complemented with a\nhuman trainer.\nThe ﬁrst approach includes a standard deep reinforcement learning agent,\nreferred to here as DeepRL, and is the basis of both of the interactive agents\ndiscussed subsequently. The DeepRL agent perceives the environment informa-\ntion through a visual representation [37], which is processed by a convolutional\nneural network (CNN) that estimates the Q-values. The deep Q-learning algo-\nrithm allows the agents to learn through actions previously experienced, using\nIn press Appl. Sci. 2020\nPage 8\nPopulate initial \nmemory\nAgent’s training\nExternal \nadvice\nPretraining\n900 time-steps\n100 time-steps\nPopulate initial memory\nAgent’s training\nPretraining\n1000 time-steps\nDeepRL agent\nIDeepRL agent\nFigure 2: The learning process for autonomous and interactive agents. Both\napproaches include a pretraining stage comprising 1000 actions. For interac-\ntive agents, the ﬁnal part of the pretraining is performed using external advice\ninstead of random actions.\nthe CNN as a function approximator, allowing them to generalize states and\napply Q-learning in continuous state spaces.\nTo save past experiences, the experience replay [38] technique is imple-\nmented. This technique saves the most useful information (experience) in mem-\nory, which is used afterward to train the RL agent.\nThe neural network is\nresponsible for processing the visual representation and gives the Q-value of\neach action to the agent, which decides what action to take. In order for the\nagent to balance exploration and exploitation of actions, the ϵ-greedy method is\nused. This method includes an ϵ parameter, which allows the agent to performs\neither a random exploratory action or the best known action proposed by the\npolicy.\nThe learning process for the autonomous agent, i.e., DeepRL agent, is sep-\narated into two stages.\nThe ﬁrst pretraining stage consists of 1000 random\nactions that the agent must perform to populate the initial memory. In the\nsecond stage, the agent’s training is carried out using the ϵ-greedy policy and,\nafter each performed action, the agent is trained using 128 tuples considering\nstate, action, reward, and next state, as < st, at, rt, st+1 > extracted from the\nmemory.\nBoth IDeepRL approaches are based on autonomous DeepRL and include the\ninteractive feedback strategy from an external trainer to improve the DeepRL\nperformance. Therefore, the agents have the same base algorithm, adding an\nextra interactive stage. For the interactive agents, the learning process is sepa-\nrated into three stages. The ﬁrst pretraining stage corresponds to 900 random\nactions that the agent must perform in order to populate the initial memory.\nIn the second stage, the external trainer participates giving early advice about\nthe environment dynamics and the intended task during 100 consecutive time-\nsteps. In the third stage, the agent starts training using the ϵ-greedy policy,\nand, following each action selected, the agent is trained with 128 tuples as\n< st, at, rt, st+1 > saved previously in the batch memory. The learning process\nIn press Appl. Sci. 2020\nPage 9\nAlgorithm 1 Pretraining algorithm to populate the batch memory including\ninteractive feedback.\n1: Initialize memory M\n2: Observe agent’s initial state s0\n3: while len(M) ≤1000 do\n4:\nif interaction is used AND length of M > 900 then\n5:\nGet action at from advisor\n6:\nelse\n7:\nChoose a random action at\n8:\nend if\n9:\nPerform action at\n10:\nObserve rt and next state st+1\n11:\nAdd (< st, at, rt, st+1 >) to M\n12:\nif st is terminal OR time-steps > 250 then\n13:\nReset episode\n14:\nend if\n15: end while\nfor both autonomous and interactive agents is depicted in Figure 2.\nAlgorithm 2 Training algorithm to populate and extract information from the\nbatch memory.\n1: Perform the pretraining Algorithm 1\n2: for each episode do\n3:\nObserve state st\n4:\nrepeat\n5:\nChoose an action at using ϵ-greedy\n6:\nPerform action at\n7:\nObserve rt and next state st+1\n8:\nAdd (< st, at, rt, st+1 >) to M\n9:\nPopulate randomly batch B from M\n10:\nTrain CNN using data in B\n11:\nst ←st+1\n12:\nϵ ←ϵ ∗ϵ decay\n13:\nuntil st is terminal OR time-steps > 250\n14: end for\nIn the second stage of the IDeepRL approach, the learner agent receives\nadvice either from a previously trained artiﬁcial agent or from a human trainer.\nThe artiﬁcial trainer agent used in agent-IDeepRL is an RL agent that collected\nprevious experience by performing the autonomous DeepRL approach using the\nsame hyperparameters. Therefore, the knowledge is acquired by interacting with\nthe environment in the same manner as the learner agent does. Once the trainer\nagent has learned the environment, it is used to then provide advice in agent-\nIDeepRL over 100 consecutive time-steps. Both the trainer agent and the learner\nIn press Appl. Sci. 2020\nPage 10\nagent perceived the environmental information through a visual representation\nafter an action is performed.\nAlgorithm 1 shows the ﬁrst stage for DeepRL and IDeepRL approaches,\nwhich corresponds to the pretraining stages to populate the batch memory. This\nalgorithm also contains the second stage for interactive agents using IDeepRL\nrepresented with a conditional in line 4. Moreover, in Algorithm 2 is observed\nthe second stage for DeepRL, which also corresponds to the third stage for\nIDeepRL, the training stage.\n3.1.1\nInteractive approach\nAs previously discuss, the IDeepRL methods include an external advisor, which\ncan be another already trained agent or human. In our scenario, the advisor uses\na policy-shaping approach during the decision-making process, as previously\nshown in Figure 1. Moreover, between the diﬀerent alternatives for interactive\nfeedback, we use teaching on a budget with early advising [39]. This technique\nattempts to reduce the time required for an RL agent to understand better\nthe environment, achieved by 100 early consecutive pieces of advice from the\ntrainer, trying to transfer the trainer’s knowledge of the environment as quickly\nas possible. For the diﬀerent ways to implement the interactive approach, we use\nearly advising for the training of the learner agent, using a limited consecutive\namount of advice to be used by the trainer to help the agent.\n3.1.2\nVisual representation\nA visual representation for the deep Q-learning algorithm is used, which consists\nof Q-learning using a function approximator for the Q-values with a CNN. Ad-\nditionally, it uses a memory with past experiences from where are taken batches\nfor the network training.\nOur architecture is capable of processing input images of 64×64 pixels used\nin RGB channels for learning the image features. The architecture is inspired by\nsimilar networks used in other DeepRL works [13, 14]. Particularly, in the ﬁrst\nlayer, an 8×8 convolution with four ﬁlters is used, then a 2×2 max-pooling layer\nfollowed by a 4 × 4 convolution layer with eight ﬁlters, and followed by another\nmax-pooling with the same speciﬁcation as the previous one. The network has\na last 2 × 2 convolution layer with 16 ﬁlters. After the last pooling, a ﬂatten\nlayer is applied, which is fully connected to a layer with 256 neurons. Finally,\nthe 256 neurons are also fully connected with the output layer, which uses a\nsoftmax function, including four neurons to represent the possible actions. The\nfull network architecture can be seen in Figure 3. Since this work is oriented to\ncompare diﬀerent learning methodologies, all agents were trained with the same\narchitecture to compare them fairly.\n3.1.3\nContinuous representation\nGiven the task characteristics, considering images as inputs to recognize diﬀer-\nent objects in a dynamic environment, it is impractical to generate a table with\nIn press Appl. Sci. 2020\nPage 11\n\u001f\u001e\u001d\u001c\u001b\n\u001a\u0019\u0018\u0017\u0018\u001a\u0019\n\u0016\u0015\u0014\u0013\u0012\u0011\u0015\u0018\u0010\u0014\u000f\u000e\n\u0019\r\u0018\f\u0018\u000b\u0018\f\n\u0016\u0015\u0014\u0013\u0012\u0011\u0015\u0018\u0010\u0014\u000f\u000e\n\n\r\u0018\n\u0018\u000b\u0018\n\u0016\u0015\u0014\u0013\u0012\u0011\u0015\u0018\u0010\u0014\u000f\u000e\n\f\r\u0018\u0019\u0018\u000b\u0018\u0019\n\u0016\u0015\u0014\u0013\u0012\u0011\u0015\u0018\u0010\u0014\u000f\u000e\n\n\r\u0018\n\u0018\u000b\u0018\n\t\u001c\u001d\u001c\u001b\n\u0019\n\b\u0007\u0006\u0005\u0007\u0004\u0012\u0013\u0003\u0007\u0006\n\u001d\u0007\u0007\u0004\u0003\u0006\u0002\n\u001d\u0007\u0007\u0004\u0003\u0006\u0002\n\b\u0007\u0006\u0005\u0007\u0004\u0012\u0013\u0003\u0007\u0006\n\b\u0007\u0006\u0005\u0007\u0004\u0012\u0013\u0003\u0007\u0006\n\u001d\u0007\u0007\u0004\u0003\u0006\u0002\n\u0016\u0015\u0014\u0013\u0012\u0011\u0015\u0018\u0010\u0014\u000f\u000e\n\n\r\u0018\n\u0018\u000b\u0018\n\u0016\u0015\u0014\u0013\u0012\u0011\u0015\u0018\u0010\u0014\u000f\u000e\n\u0001\u001a\r\u0018\n\u0018\u000b\u0018\n\u0014\u0015\u0011\n\u0014\u0013\u0013\u0015\u0006\n\u0014\u0015\u0011\n\n\u001a\n\u0012\u0004\u0004\u0018\u0018\u0007\u0006\u0006\u0015\u0013\u0015 \n\u0012\u0004\u0004\u0018\u0018\u0007\u0006\u0006\u0015\u0013\u0015 \nFigure 3: Neural network architecture, with an input of a 64 × 64 RGB image,\nand composed of three convolution layers, three max-pooling layers, and two\nfully connected layers, including a softmax function for the output.\nall the possible state-action combinations. Therefore, we have used a continuous\nrepresentation combining two methods. The ﬁrst method is a function approx-\nimator through a neuronal network for Q(st, at), which allows us to generalize\nthe states, in order to use Q-learning in continuous spaces and select which\naction is carried out. The second method is the experience replay technique,\nwhich saves on memory a tuple of an experience given by < st, at, rt, st+1 >.\nThese data saved in memory are used afterward to train the RL agent.\n3.2\nExperimental setup\nWe have designed a simulated domestic scenario focused on organizing objects.\nThe agent aims to classify geometric ﬁgures with diﬀerent shapes and colors\nand organize them in designated locations to optimize the collected reward.\nClassiﬁcation tasks are widespread in domestic scenarios, e.g., organizing cloth.\nThe object shape might represent diﬀerent cloth types, while the color might\nrepresent whether it is clean or dirty.\nIn order to compare DeepRL and IDeepRL algorithms, three diﬀerent agents\nare trained in this scenario in terms of collected reward and learning time. The\nexperimental scenario is developed in the simulator CoppeliaSim developed by\nCoppelia Robotics [40].\nThree tables are used in the scenario; the ﬁrst contains all the unsorted\nobjects initially placed randomly on the table within nine preestablished po-\nsitions. This represents the initial dynamic state. The second two tables are\nused to place the objects once the agent determines which table objects belong.\nTo perform this sort a robotic manipulator arm with 7 degrees of freedom, six\naxes, and a suction cup grip is used. The robotic arm is placed on another table\nalong with a camera from where we obtain RGB images. The objects to be\norganized are cubes, cylinders, and disks in two diﬀerent colors, red and blue,\nas are presented in Figure 4.\nIn press Appl. Sci. 2020\nPage 12\nFigure 4: The simulated domestic scenario presenting 6 objects in diﬀerent\ncolors and the robotic arm. In the upper left corner is shown the camera signal,\nwhich is a 64 × 64 pixels RGB image for the state representation of the agent.\n3.2.1\nActions and state representation\nThe available actions for the agent are four and can be taken in an autonomous\nway or through given advice from the external trainer. The actions are the\nfollowing:\ni. Grab object: the agent grabs randomly one of the objects with the suction\ncup grip.\nii. Move right: the robotic arm is moved to the table on the right side of the\nscenario; if the arm is already there, do nothing.\niii. Move left: the robotic arm is moved to the table on the left side of the\nscenario; if the arm is already there, do nothing.\niv. Drop: if the robotic arm has an object in the grip and is located in one\nof the side tables, the arm goes down and releases the object; in case the\narm is positioned in the center, the arm keeps the position.\nFor example, the actions required to correctly organize a blue cube from\nthe central table consist of (i) grab object, (ii) move right, and (iii) drop. The\nrobot low-level control to reach the diﬀerent positions within the scenario is\nperformed using inverse kinematics. Although to reach an object we use inverse\nkinematics, the CNN is responsible for deciding to perform the action grab an\nobject through the Q-values, and if so, to decide where to place the object, based\non the classiﬁcation.\nThe state comprises a high-dimensional space, represented by a raw image\ncaptured by an RGB camera. The image presents a dimension of 64 × 64 pixels\nIn press Appl. Sci. 2020\nPage 13\nfrom where the agent perceives the environment and chooses what action to\ntake according to the network output. The input image is normalized to values\n∈[0, 1] to be presented to the convolutional neural network.\n3.2.2\nReward\nIn terms of the reward function, there are diﬀerent subtasks to complete an\nepisode. To complete the task successfully, all the objects must be correctly\norganized. To organize one object is considered a partial goal of the task. All\nthe objects are initially located in the central table to be sorted, and once placed\nin the side tables, they cannot be grasped again. If all the objects are correctly\nsorted, the reward is equal to 1, and for correctly organizing a single object,\nthe reward is equal to 0.4. For example, if the classiﬁcation of the six objects is\ncorrect, each of the ﬁrst ﬁve organized objects leads to a reward of 0.4, and the\nlast one obtains a reward of 1, summarizing a total reward of 3. Furthermore, to\nencourage the agent to accomplish the classiﬁcation task in fewer steps, a small\nnegative reward of -0.01 per step is considered when the steps are more than 18,\nwhich is the minimal time-steps needed to complete the task satisfactorily. If\nan object is misplaced, the current training episode ends, and the agent receives\na negative reward of -1. The complete reward function is shown in Eq. (3).\nr(s) =\n\n\n\n\n\n\n\n1\nif all the objects are organized\n0.4\nif a single object is organized\n−1\nif an object is incorrectly organized\n−0.01\nif steps > 18\n(3)\n3.2.3\nHuman interaction\nIn the case of human trainers giving advice during the second stage of the\nIDeepRL approach, a brief three-step induction is carried out for each partici-\npant:\ni. The user reads an introduction to the scenario and the experiment, as well\nas the expected results.\nii. The user is given an explanation about the problem and how it can be\nsolved.\niii. The user is taught how to use the computer interface to advise actions to\nthe learner agent.\nIn general terms, the participants chosen for the experiment have not had\nsigniﬁcant exposure to artiﬁcial intelligence, and are not familiar with simulated\nrobotic environments. The solution explanation is given to the participants in\norder to give to all of them an equal understanding of the problem and thus\nto reduce the time that they spend exploring the environment and focus on\nadvising the agent. Each participant communicates with the learner agent using\na computer interface while observing the current state and performance in the\nIn press Appl. Sci. 2020\nPage 14\nrobot simulator. The user interface contains in a menu all possible actions that\ncan be advised.\nThese action possibilities are shown at the screen, and the\ntrainer may choose any of them to be performed by the learner agent. There is\nno time limit to advise each action, but, as mentioned, during the second stage\nof IDeepRL, the trainer has a maximum of 100 consecutive time-steps available\nfor advice.\n4\nResults\nIn this section, we show the experimental results obtained during the training\nof three diﬀerent agents implemented with the three proposed methodologies,\ni.e., an autonomous agent scenario, a human-agent scenario, and an agent-agent\nscenario, namely, DeepRL, human-IDeepRL, and agent-IDeepRL. The method-\nologies are tested with the same hyperparameters, which have been experimen-\ntally determined concerning our scenario, as follows: initial value of ϵ = 1, ϵ\ndecay rate of 0.9995, learning rate α = 10−3, and discount factor γ = 0.9 during\n300 episodes.\nAs discussed in section 3.1, the ﬁrst methodology is an autonomous agent\nusing DeepRL, who must learn how the environment works and how to complete\nthe task. Given the complexity of learning the task autonomously, the time\nrequired for the learning process is rather high. The average collected reward\nfor ten autonomous agents is shown in Figure 5 represented by the black line.\nMoreover, this complexity also increases the error rate or misclassiﬁcation of\nthe objects located in the central table.\nNext, we perform the interactive learning approaches by using agent-IDeepRL\nand human-IDeepRL. The average obtained results for ten interactive agents are\nshown in Figure 5 represented by the blue line and the red line, respectively. The\nagent-IDeepRL approach performs slightly better than the human-IDeepRL ap-\nproach, mainly because people needed more time to understand the setup and\nto react during the experiments. However, both interactive approaches obtain\nvery similar results, achieving much faster convergence when comparing to the\nautonomous DeepRL approach.\nFurthermore, the learner agents getting ad-\nvice from external trainers make fewer mistakes, especially at the beginning\nof the learning process, and are able to learn the task in fewer episodes. On\nthe other hand, the autonomous agent makes more mistakes at the beginning\nsince it is trying to learn how the environment works and the aim that it has\nto be accomplished. This is not the case for the interactive agents since the\nadvisors help them during this critical part of the learning. For a qualitative\nanalysis of the obtained results, we have computed the total collected reward\nRT deﬁned as the sum of all individual rewards ri received during the learning\nprocess, i.e., N = 300 for this case (see Eq. (4)). The total collected rewards for\nthe three methods are: autonomous DeepRL RT = 369.8788, agent-IDeepRL\nRT = 606.6995, and human-IDeepRL RT = 589.5974. Therefore, the meth-\nIn press Appl. Sci. 2020\nPage 15\n0\n50\n100\n150\n200\n250\n300\nEpisodes\n3\n2\n1\n0\n1\n2\n3\nReward\nautonomous DeepRL\nagent-IDeepRL\nhuman-IDeepRL\nFigure 5: Average collected reward for the three proposed methods. The black\nline represents the autonomous (DeepRL) agent, which has to discover the en-\nvironment without any help. The blue and red lines are the agents with an\nexternal trainer, namely an artiﬁcial advisor (agent-IDeepRL) and a human ad-\nvisor (human-IDeepRL), respectively. The shadowed area around the curves\nshows the standard deviation for ten agents. The methods agent-IDeepRL and\nhuman-IDeepRL collect 64.03% and 59.40% more reward RT when compared\nto the autonomous DeepRL baseline.\nods agent-IDeepRL and human-IDeepRL present an improvement, in terms of\ncollected reward, of 64.03% and 59.40% respectively, in comparison to the au-\ntonomous DeepRL used as a baseline.\nRT =\nN\nX\ni\nri\n(4)\nDue to the trainer has a budget of 100 actions to advise, the interactive\nfeedback is consumed within the ﬁrst six episodes, taking into account that the\nminimal amount of actions to complete an episode are 18 actions. Even with\nsuch a small amount of feedback, the learner agent receives an important knowl-\nedge from the advisor that is complemented with the experience replay method.\nDuring the human-IDeepRL approach, to give demonstrative advice, 11 people\nparticipated in the experiments, four males and seven females, with ages be-\ntween 16 and 24 (M = 21.63, SD = 2.16). The participants were explained how\nto help the robot to complete the task giving advice using the same script for\nIn press Appl. Sci. 2020\nPage 16\nall of them (see Section 3.1).\nIn Figure 6 are shown the collected rewards by an autonomous DeepRL\nagent and three learner agents trained by diﬀerent people as examples.\nAll\nhuman-IDeepRL approaches work much better than the autonomous DeepRL,\nmaking fewer mistakes and, therefore, collecting faster rewards. Between the\ninteractive agents, there are some diﬀerences, especially at the beginning of\nthe learning process, within the ﬁrst 50 episodes.\nIt is possible to observe\nthe diﬀerent strategies followed by the human trainers, for instance, the sixth\ntrainer (yellow line, labeled as human-IDeepRL6) started giving wrong advice,\nleading to less reward at the beginning, while the eighth trainer (green line,\nlabeled as human-IDeepRL8) started giving almost perfect advice, experiencing\na drop in the collected reward some episodes later. Nevertheless, all the agents,\neven the autonomous, managed well the task reaching a similar reward. As\nin the previous case, we have computed the total collected reward RT (see\nEq. (4)) for each agent shown in Figure 6. The autonomous DeepRL agent\ncollected 266.0905 of total reward, whereas the agents with the human-IDeepRL\nmethod using as trainers subjects 2, 6, and 8 collected 603.6375, 561.4384,\nand 630.4684 respectively. Although there are diﬀerences in the way that each\ntrainer instructs the learner agent, yet they have accomplished an improvement\nin the total accumulated reward of 126.85%, 110.99%, and 136.94%.\nFigure 7 shows Pearson’s correlation of the collected rewards for all the\ninteractive agents trained by the participants in the experiment. Moreover, we\ninclude an autonomous agent (AAu) and an interactive agent trained by an\nartiﬁcial trainer agent (AAT ) as a reference. It is possible to observe that all\ninteractive agents, including the one using an artiﬁcial trainer agent, have a high\ncorrelation in terms of the collected reward. However, the autonomous agent\nshows a lower correlation in comparison to the interactive approaches.\nAdditionally, we have computed the Student’s t-test to test the statisti-\ncal diﬀerence between the obtained results.\nWhen the autonomous DeepRL\napproach is compared to agent-IDeepRL and human-IDeepRL, it obtains a\nt-score t=7.6829 (p-value p=6.3947 × 10−14) and a t-score t=7.0192 (p-value\np=6.0755 × 10−12) respectively, indicating that there is a statistically signif-\nicant diﬀerence between the two approaches.\nOn the other hand, compar-\ning both interactive approaches between each other, i.e., agent-IDeepRL and\nhuman-IDeepRL, a t-score t=0.8461 (p-value p=0.3978) is obtained, showing\nthat there is no statistical diﬀerence between the interactive methods. Table 2\nshows all the obtained t-scores along with the p-values for each of them.\nTable 2:\nStudent’s t-test for comparison of autonomous DeepRL, agent-\nIDeepRL, and human-IDeepRL.\nDeepRL vs.\nDeepRL vs.\nagent-IDeepRL vs.\nagent-IDeepRL\nhuman-IDeepRL\nhuman-IDeepRL\nt = 7.6829\nt = 7.0192\nt = 0.8461\np = 6.3947 × 10−14\np = 6.0755 × 10−12\np = 0.3978\nIn press Appl. Sci. 2020\nPage 17\n0\n50\n100\n150\n200\n250\n300\nEpisodes\n3\n2\n1\n0\n1\n2\n3\nReward\nautonomous DeepRL\nhuman-IDeepRL2\nhuman-IDeepRL6\nhuman-IDeepRL8\nFigure 6:\nCollected rewards for a selection of example interactive agents. The\nﬁgure compares the learning process of agents trained by diﬀerent people us-\ning human-IDeepRL (the black line is an autonomous agent that is included\nas a reference). The three human examples diﬀer from each other by following\ndiﬀerent strategies to teach the learner agent, especially at the beginning. For\nexample, the sixth trainer (yellow) started giving wrong advice, leading to less\nreward collected initially, while the eighth trainer (green) started giving much\nbetter advice, which is reﬂected in the accumulated reward at the beginning,\nhowever, it experienced a drop in the collected reward some episodes later. Al-\nthough each person has initially a diﬀerent understanding of the environment\nconsidering objectives and possible movements, all the interactive agents con-\nverge to similar behavior at the end of the learning process. Quantitatively, the\ninteractive agents collected 126.85%, 110.99%, and 136.94% more reward RT\nwhen compared to the autonomous DeepRL agent.\nIn press Appl. Sci. 2020\nPage 18\nAAu\nAAT\nAH1 AH2 AH3 AH4 AH5 AH6 AH7 AH8 AH9 AH10 AH11\nReward\nAAu\nAAT\nAH1\nAH2\nAH3\nAH4\nAH5\nAH6\nAH7\nAH8\nAH9\nAH10\nAH11\nReward\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nFigure 7:\nPearson’s correlation between the collected rewards for diﬀerent\nagents. The shown agents include an autonomous agent (AAu), an interactive\nagent trained by an artiﬁcial trainer (AAT ), and the interactive agents trained\nby humans (from AH1 to AH11). The collected reward for all the interactive\napproaches, including the one using the artiﬁcial trainer, presents a similar be-\nhavior showing a high correlation. On the contrary, the collected reward by the\nautonomous agent shows a lower correlation in comparison to the interactive\nagents.\nIn all the tested approaches, approximately since episode 150, the agent\nperforms actions mainly based on its learning or training. In that episode, the\nvalue of ϵ in the ϵ-greedy policy decay to 1% of exploratory actions. Moreover, in\nall the approaches, the maximal collected reward ﬂuctuates between 2.5 and 3.\nThis is because the robot, with its movements, sometimes throws away another\nobject from the table, diﬀerent from the one being manipulated.\n5\nConclusions\nWe have presented an interactive deep reinforcement learning approach to train\nan agent in a human-robot environment. We have also performed a comparison\nbetween three diﬀerent methods for learning agents. First, we implemented an\nautonomous version of DeepRL, which had to interact and learn the environment\nIn press Appl. Sci. 2020\nPage 19\nby itself. Next, we proposed an interactive version called IDeepRL, which used\nan external trainer to give useful advice during the decision-making process\nthrough interactive feedback delivered through early advising.\nWe have implemented two variations of IDeepRL by using previously trained\nartiﬁcial agents and humans as trainers.\nWe called these approaches agent-\nIDeepRL and human-IDeepRL, respectively. Our proposed IDeepRL methods\nconsiderably outperform the autonomous DeepRL version in the implemented\nrobotic scenario. Moreover, in complex tasks, which often require more training\ntime, to have an external trainer giving supportive feedback, leads to great\nbeneﬁts in terms of time and collected reward.\nOverall, the interactive deep reinforcement learning approach introduces an\nadvantage in domestic-like environments. It allows speeding up the learning pro-\ncess of a robotic agent interacting with the environment and allows people to\ntransfer prior knowledge about a speciﬁc task. Furthermore, using a reinforce-\nment learning approach allows the agent to learn the task without the necessity\nof previously labeled data, such as the case for supervised learning methods. In\nthis regard, the task is learned in such a way that the agent learns to recognize\nthe state of the environment as well as to behave on it, deciding where to place\nthe diﬀerent objects. Our novel interactive-shaping vision-based approach out-\nperforms the current autonomous DeepRL method, used as a baseline in this\nwork. The introduced approaches demonstrate that the use of external train-\ners, either artiﬁcial or human, lead to more and faster reward in comparison to\ntraditional the DeepRL approach.\nAs future work, we consider the use of diﬀerent kinds of artiﬁcial trainers\nto select possible advisors better. A bad teacher can negatively inﬂuence the\nlearning process and somehow limit the learner by teaching a strategy that is not\nnecessarily optimal. To select a good teacher, it is necessary to take into account\nthat an agent that obtains the best results for the task, in terms of accumulated\nreward, is not necessarily the best teacher [27]. Rather a good teacher could be\none with a small standard deviation over the visited states. This would allow\nadvising the learner agent in more speciﬁc situations. Additionally, we plan to\ntransfer and test the proposed approach in a real-world human-robot interaction\nscenario. In such a case, it is necessary to deal with additional environmental\ndynamics and variables that in simulated scenarios may be easier to control.\nAcknowledgments\nThis research was partially funded by Universidad Central de Chile under the\nresearch project CIP2018009, the Coordena¸c˜ao de Aperfei¸coamento de Pessoal\nde N´ıvel Superior - Brasil (CAPES) - Finance Code 001, the Brazilian agencies\nFACEPE, and CNPq.\nIn press Appl. Sci. 2020\nPage 20\nReferences\n[1] S. Shepherd and A. Buchstab, “Kuka robots on-site,” in Robotic Fabrication\nin Architecture, Art and Design 2014, pp. 373–380, Springer, 2014.\n[2] F. Cruz, P. W¨uppen, A. Fazrie, C. Weber, and S. Wermter, “Action se-\nlection methods in a robotic reinforcement learning scenario,” in 2018\nIEEE Latin American Conference on Computational Intelligence (LA-\nCCI), pp. 1–6, IEEE, 2018.\n[3] M. A. Goodrich, A. C. Schultz, et al., “Human–robot interaction: a survey,”\nFoundations and Trends R⃝in Human–Computer Interaction, vol. 1, no. 3,\npp. 203–275, 2008.\n[4] N. Churamani, F. Cruz, S. Griﬃths, and P. Barros, “icub: learning emotion\nexpressions using human reward,” arXiv preprint arXiv:2003.13483, 2020.\n[5] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[6] Y. Niv, “Reinforcement learning in the brain,” Journal of Mathematical\nPsychology, vol. 53, no. 3, pp. 139–154, 2009.\n[7] F. Cruz, G. I. Parisi, and S. Wermter, “Multi-modal feedback for\naﬀordance-driven interactive reinforcement learning,” in 2018 International\nJoint Conference on Neural Networks (IJCNN), pp. 1–8, IEEE, 2018.\n[8] A. Bignold, F. Cruz, M. E. Taylor, T. Brys, R. Dazeley, P. Vamplew, and\nC. Foale, “A conceptual framework for externally-inﬂuenced agents: An\nassisted reinforcement learning review,” arXiv preprint arXiv:2007.01544,\n2020.\n[9] A. Ayala, C. Henr´ıquez, and F. Cruz, “Reinforcement learning using con-\ntinuous states and interactive feedback,” in Proceedings of the International\nConference on Applications of Intelligent Systems, pp. 1–5, 2019.\n[10] C. Mill´an, B. Fernandes, and F. Cruz, “Human feedback in continuous\nactor-critic reinforcement learning,” in Proceedings of the European Sym-\nposium on Artiﬁcial Neural Networks, Computational Intelligence and Ma-\nchine Learning ESANN, pp. 661–666, ESANN, 2019.\n[11] P. Barros, A. Tanevska, F. Cruz, and A. Sciutti, “Moody learners – explain-\ning competitive behaviour of reinforcement learning agents,” in Proceedings\nof IEEE International Conference on Development and Learning and Epi-\ngenetic Robotics (ICDL-EpiRob), 2020.\n[12] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\nIn press Appl. Sci. 2020\nPage 21\n[13] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,\net al., “Human-level control through deep reinforcement learning,” Nature,\nvol. 518, no. 7540, p. 529, 2015.\n[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with\ndeep convolutional neural networks,” in Advances in neural information\nprocessing systems, pp. 1097–1105, 2012.\n[15] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with\ndouble q-learning,” in Thirtieth AAAI conference on artiﬁcial intelligence,\npp. 2094–2100, 2016.\n[16] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic\nProgramming. Wiley, 1994.\n[17] H. B. Suay and S. Chernova, “Eﬀect of human guidance and state space\nsize on interactive reinforcement learning,” in 2011 Ro-Man, pp. 1–6, IEEE,\n2011.\n[18] A. Najar and M. Chetouani, “Reinforcement learning with human advice.\na survey,” arXiv preprint arXiv:2005.11016, 2020.\n[19] A. Y. Ng, D. Harada, and S. Russell, “Policy invariance under reward\ntransformations: Theory and application to reward shaping,” in ICML,\nvol. 99, pp. 278–287, 1999.\n[20] T. Brys, A. Now´e, D. Kudenko, and M. E. Taylor, “Combining multiple\ncorrelated reward and shaping signals by measuring conﬁdence.,” in AAAI,\npp. 1687–1693, 2014.\n[21] S. Griﬃth, K. Subramanian, J. Scholz, C. Isbell, and A. L. Thomaz, “Pol-\nicy shaping: Integrating human feedback with reinforcement learning,” in\nAdvances in Neural Information Processing Systems, pp. 2625–2633, 2013.\n[22] G. Li, R. Gomez, K. Nakamura, and B. He, “Human-centered reinforce-\nment learning: A survey,” IEEE Transactions on Human-Machine Systems,\nvol. 49, no. 4, pp. 337–349, 2019.\n[23] J. Grizou, M. Lopes, and P.-Y. Oudeyer, “Robot learning simultaneously\na task and how to interpret human instructions,” in 2013 IEEE Third\nJoint International Conference on Development and Learning and Epige-\nnetic Robotics (ICDL), pp. 1–8, IEEE, 2013.\n[24] N. Navidi, “Human ai interaction loop training: New approach for interac-\ntive reinforcement learning,” arXiv preprint arXiv:2003.04203, 2020.\n[25] A. Bignold, Rule-based interactive assisted reinforcement learning.\nPhD\nthesis, Federation University Australia, 2019.\nIn press Appl. Sci. 2020\nPage 22\n[26] M. E. Taylor, N. Carboni, A. Fachantidis, I. Vlahavas, and L. Torrey,\n“Reinforcement learning agents providing advice in complex video games,”\nConnection Science, vol. 26, no. 1, pp. 45–63, 2014.\n[27] F. Cruz, S. Magg, Y. Nagai, and S. Wermter, “Improving interactive re-\ninforcement learning: What makes a good teacher?,” Connection Science,\nvol. 30, no. 3, pp. 306–325, 2018.\n[28] A. Dobrovsky, U. M. Borghoﬀ, and M. Hofmann, “Improving adaptive\ngameplay in serious games through interactive deep reinforcement learn-\ning,” in Cognitive infocommunications, theory and applications, pp. 411–\n432, Springer, 2019.\n[29] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov,\nand S. Levine, “Learning complex dexterous manipulation with deep rein-\nforcement learning and demonstrations,” arXiv preprint arXiv:1709.10087,\n2017.\n[30] T. J. Lukka, T. Tossavainen, J. V. Kujala, and T. Raiko, “Zenrobotics\nrecycler–robotic sorting using machine learning,” in Proceedings of the In-\nternational Conference on Sensor-Based Sorting (SBS), 2014.\n[31] C. Zhihong, Z. Hebin, W. Yanbo, L. Binyan, and L. Yu, “A vision-based\nrobotic grasping system using deep learning for garbage sorting,” in 2017\n36th Chinese Control Conference (CCC), pp. 11223–11226, 2017.\n[32] L. Sun, G. Aragon-Camarasa, S. Rogers, R. Stolkin, and J. P. Siebert,\n“Single-shot clothing category recognition in free-conﬁgurations with ap-\nplication to autonomous clothes sorting,” in 2017 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pp. 6699–6706, 2017.\n[33] F. Cruz, S. Magg, C. Weber, and S. Wermter, “Improving reinforcement\nlearning with interactive feedback and aﬀordances,” in 4th International\nConference on Development and Learning and on Epigenetic Robotics,\npp. 165–170, IEEE, 2014.\n[34] F. Cruz, G. I. Parisi, and S. Wermter, “Learning contextual aﬀordances\nwith an associative neural architecture,” in Proceedings of the European\nSymposium on Artiﬁcial Neural Network. Computational Intelligence and\nMachine Learning ESANN, pp. 665–670, UCLouvain, 2016.\n[35] F. Zhang, J. Leitner, M. Milford, B. Upcroft, and P. Corke, “Towards\nvision-based deep reinforcement learning for robotic motion control,” arXiv\npreprint arXiv:1511.03791, 2015.\n[36] M. Vecerik, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess,\nT. Roth¨orl, T. Lampe, and M. Riedmiller, “Leveraging demonstrations for\ndeep reinforcement learning on robotics problems with sparse rewards,”\narXiv preprint arXiv:1707.08817, 2017.\nIn press Appl. Sci. 2020\nPage 23\n[37] N. Desai and A. Banerjee, “Deep reinforcement learning to play space in-\nvaders,” tech. rep., Stanford University, 2017.\n[38] S. Adam, L. Busoniu, and R. Babuska, “Experience replay for real-time\nreinforcement learning control,” Systems, Man, and Cybernetics, Part C:\nApplications and Reviews, IEEE Transactions on, vol. 42, pp. 201 – 212,\n04 2012.\n[39] F. Cruz, P. W¨uppen, S. Magg, A. Fazrie, and S. Wermter, “Agent-advising\napproaches in an interactive reinforcement learning scenario,” in 2017 Joint\nIEEE International Conference on Development and Learning and Epige-\nnetic Robotics (ICDL-EpiRob), pp. 209–214, IEEE, 2017.\n[40] E. Rohmer, S. P. Singh, and M. Freese, “V-rep: A versatile and scalable\nrobot simulation framework,” in IEEE/RSJ International Conference on\nIntelligent Robots and Systems, pp. 1321–1326, IEEE, 2013.\n",
  "categories": [
    "cs.AI",
    "cs.MA",
    "cs.RO"
  ],
  "published": "2020-07-07",
  "updated": "2020-08-11"
}