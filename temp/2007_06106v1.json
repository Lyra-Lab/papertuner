{
  "id": "http://arxiv.org/abs/2007.06106v1",
  "title": "Unsupervised Feature Selection for Tumor Profiles using Autoencoders and Kernel Methods",
  "authors": [
    "Martin Palazzo",
    "Pierre Beauseroy",
    "Patricio Yankilevich"
  ],
  "abstract": "Molecular data from tumor profiles is high dimensional. Tumor profiles can be\ncharacterized by tens of thousands of gene expression features. Due to the size\nof the gene expression feature set machine learning methods are exposed to\nnoisy variables and complexity. Tumor types present heterogeneity and can be\nsubdivided in tumor subtypes. In many cases tumor data does not include tumor\nsubtype labeling thus unsupervised learning methods are necessary for tumor\nsubtype discovery. This work aims to learn meaningful and low dimensional\nrepresentations of tumor samples and find tumor subtype clusters while keeping\nbiological signatures without using tumor labels. The proposed method named\nLatent Kernel Feature Selection (LKFS) is an unsupervised approach for gene\nselection in tumor gene expression profiles. By using Autoencoders a low\ndimensional and denoised latent space is learned as a target representation to\nguide a Multiple Kernel Learning model that selects a subset of genes. By using\nthe selected genes a clustering method is used to group samples. In order to\nevaluate the performance of the proposed unsupervised feature selection method\nthe obtained features and clusters are analyzed by clinical significance. The\nproposed method has been applied on three tumor datasets which are Brain, Renal\nand Lung, each one composed by two tumor subtypes. When compared with benchmark\nunsupervised feature selection methods the results obtained by the proposed\nmethod reveal lower redundancy in the selected features and a better clustering\nperformance.",
  "text": "Unsupervised feature selection for tumor proﬁles\nusing autoencoders and kernel methods\n1st Martin Palazzo\nUniversite de Technologie de Troyes &\nInstituto de Investigacion en Biomedicina\nde Buenos Aires (IBioBA)CONICET\nPartner Institute of the Max Planck Society\nTroyes, France\nmartin.palazzo@utt.fr\n2rd Pierre Beauseroy\nUniversite de Technologie de Troyes\nTroyes, France\npierre.beauseroy@utt.fr\n3nd Patricio Yankilevich\nInstituto de Investigacion en Biomedicina\nde Buenos Aires (IBioBA)CONICET\nPartner Institute of the Max Planck Society\nBuenos Aires, Argentina\npyankilevich@ibioba-mpsp-conicet-gov.ar\nAbstract\nMolecular data from tumor proﬁles is high dimensional. Tumor proﬁles can be characterized by tens of thousands of gene\nexpression features. Due to the size of the gene expression feature set machine learning methods are exposed to noisy variables\nand complexity. Tumor types present heterogeneity and can be subdivided in tumor subtypes. In many cases tumor data does not\ninclude tumor subtype labeling thus unsupervised learning methods are necessary for tumor subtype discovery. This work aims to\nlearn meaningful and low dimensional representations of tumor samples and ﬁnd tumor subtype clusters while keeping biological\nsignatures without using tumor labels. The proposed method named Latent Kernel Feature Selection (LKFS) is an unsupervised\napproach for gene selection in tumor gene expression proﬁles. By using Autoencoders a low dimensional and denoised latent\nspace is learned as a target representation to guide a Multiple Kernel Learning model that selects a subset of genes. By using the\nselected genes a clustering method is used to group samples. In order to evaluate the performance of the proposed unsupervised\nfeature selection method the obtained features and clusters are analyzed by clinical signiﬁcance. The proposed method has been\napplied on three tumor datasets which are Brain, Renal and Lung, each one composed by two tumor subtypes. When compared\nwith benchmark unsupervised feature selection methods the results obtained by the proposed method reveal lower redundancy in\nthe selected features and a better clustering performance.\nIndex Terms\nUnsupervised Feature Selection, Gene Expression, Kernel Methods, Autoencoders\nI. INTRODUCTION\nCancer informatics has been characterized by the increasing availability of large data repositories like The Cancer Genome\nAtlas [Tomczak et al., 2015] and the International Cancer Genome Consortium [Zhang et al., 2011]. One of the most used\ngenome technologies to characterize tumor sub-types is Transcriptomics, which measures gene expression [Lu and Han, 2003].\nEach tumor proﬁle is described by more than 20.000 gene expression features which deﬁnes a high dimensional input space\nand further complexity in data analysis. Tumor types presents inner heterogeneity that can be sub-divided in tumor sub-types\n[Chen et al., 2017] [Xiao et al., 2017] [Chen et al., 2016]. Given the high dimensional space from the input data it is necessary\nto reduce the dimensionality while preserving the biological interpretation of the system. Moreover, tumor labels like tumor\nsubtype or tumor stage are not always available therefore unsupervised approaches that do not need labeled data are necessary\nin these cases [Ang et al., 2015]. These reasons motivate us to propose an unsupervised feature selection method for tumor\nclustering.\nFeature selection methods are necessary in cancer genomics since they provide a low dimensional representation of the input\ndata. This representation is characterized by a selected subset of the input genes providing interpretability of the results and\ndiscarding the rest by following an objective function related to improve a learning task [Li et al., 2018]. In addition, the\nselected genes can be used to guide biomarker discovery strategies [He and Yu, 2010]. The resulting subspace obtained by a\nfeature selection method is described explicitly by a subset of biological features assuming that the initial feature set contains\nnoisy features that can be discarded. A reduced feature subset has beneﬁts in reducing model complexity and in measuring\nonly a reduced set of biological features [Ang et al., 2015].\nIn an unsupervised problem it is expected that the selected genes may improve the clustering of tumor proﬁles. To guide the\nfeature selection process this work proposes a low dimensional and denoised representation of the input data known as latent\nspace which is learned by an unsupervised neural network known as Autoencoder. Then by a Multiple Kernel Learning process\na subset of gene features is selected with the objective to approach as much as possible to the learned representation from the\nautoencoder. Finally by doing clustering using just the selected features it is expected to observe signiﬁcant clinical attributes\nassociated to each cluster. Although the latent features learned by autoencoders are useful for further clustering of tumors and\nsubtype discovery, these features are a nonlinear combination of the original ones. For this reason these latent features are not\nuseful for biological interpretation since they are not explicit.\narXiv:2007.06106v1  [cs.LG]  12 Jul 2020\nThe main contribution of this work is an unsupervised method able to select genes with clinical relevance from high dimensional\ngene expression data without the need of having tumor labels. The proposed unsupervised feature selection method is applied\nin tumor data from Lung cancer, Renal cancer and Brain cancer. The performance of the proposed method is compared with\ntwo unsupervised feature selection methods.\nII. RELATED WORK\nReducing the dimensionality of gene expression data can be achieved by feature selection [Ang et al., 2015] and feature\nextraction methods [Hira and Gillies, 2015].\nFeature extraction is the construction of a reduced subset of new features obtained from a linear or nonlinear combination\nof the initial set of features. Neural Networks have gained popularity for feature extraction and dimensionality reduction\nlead by the Autoencoder model which is based on nonlinear transformations [Hinton and Salakhutdinov, 2006]. The reduced\ndimensional space and the extracted features are known as latent space and latent features respectively. Latent features retain\nsalient characteristics from the input data in a low dimensional space [Goodfellow et al., 2016]. Autoencoders have been used\nin biomedical problems to integrate multi-omic data like gene expression, methylation and microRNA to predict Liver cancer\nprognosis [Chaudhary et al., 2018]. In a similar way autoencoders have been used to learn meaningful representations from\ngene expression data of Breast cancer patients and then identify tumor subtypes [Guo et al., 2019]. In addition, Autoencoders\nhave been applied to learn a latent space from somatic mutation data of the pan-cancer landscape showing improvements in\nthe clustering performance [Way and Greene, 2017] [Palazzo et al., 2019]. Moreover, Variational Autoencoders (VAE) have\nbeen trained on DNA Methylation data from Lung Cancer patients [Wang and Wang, 2018] and on gene expression data of\npan-cancer tumor samples to learn meaningful representations for supervised and unsupervised tasks [Way and Greene, 2017]\n[Grønbech et al., 2018]. It is clear the important role and capacity of Autoencoders for feature extraction and dimensionality\nreduction on molecular data from tumors.\nMultiple Kernel Learning (MKL) have been used for gene selection in supervised problems with the objective to improve the\nclassiﬁcation between tumor types [Rakotomamonjy et al., 2008] [Du et al., 2017]. Feature selection has been applied also\non multi-omic data like Gene Expression, Methylation and miRNA using the Minimum redundancy - maximum relevance\n(mRMR) [Ding and Peng, 2005] method to predict survival rate on Glioblastoma Multiforme patients [Zhang et al., 2016].\nAnother study proposes a stable feature selection method for high-dimensional RNA-seq data while applying an ensemble\nL1-norm support vector machines to reduce irrelevant features [Moon and Nakai, 2016] and classify tumor stages of renal\nclear cell carcinoma. In addition, feature selection by Elastic Net [Zou and Hastie, 2005] has been proposed to select genes\nlinked to the Triple Negative Breast Cancer subtype [Lopes et al., 2018]. The papers described above show the potential and\nnecessity of supervised feature selection methods for gene selection on cancer molecular data.\nNevertheless, labeled data is not always available and the selection of genes is needed for unsupervised tasks such as clustering\nsince tumor types may present heterogeneity and each cluster can present different clinical properties. This problem is faced\nby Unsupervised Feature Selection methods for Clustering [Alelyani et al., 2018]. Multi-Cluster Feature Selection (MCFS)\n[Cai et al., 2010] is an unsupervised model proposed to select the features that preserves the cluster structure of the original\ndata and has been applied on micro-RNA data [Bandyopadhyay et al., 2015]. Also the Sparse k-Means (SKM) method [Witten\nand Tibshirani, 2010] has been proposed to weight each feature based on the partition of data and by this way a subset of\nfeatures is selected by penalizing weights with the L1 norm. Moreover, an unsupervised spectral method (SPEC) [Zhao and\nLiu, 2007] has been proposed to determine relevant genes on acute lymphoblastic leukemia [Zhao et al., 2010a].\nFeature selection and feature extraction methods both have shown potential to reduce the dimensionality of tumor data. In this\nwork we propose a method that combines both strategies by learning a low dimensional latent space by extracting features\nfrom the input data and then selecting the gene expression features that approach to the resulting learned latent representation.\nIII. MATERIALS AND METHODS\nThis work proposes an unsupervised feature selection method based on Kernel Methods guided by the latent structure\nobtained from an Autoencoder. The method is divided in tree main steps. First build a target kernel matrix composed by the\ntraining samples lying on the latent space of an Autoencoder. Then select features by a multiple kernel learning strategy guided\nby the kernel built in the previous step as the target representation. Finally with the selected features perform clustering and\nmeasure the cluster quality by comparing the enrichment of tumor subtypes on each cluster.\nA. Datasets\nThe proposed method is designed to be used on high dimensional gene expression data from tumor proﬁles. To evaluate\nthe method three tumor datasets are used: Lung, Renal and Brain cancer. Each type is a separated dataset and is composed by\ntwo tumor subtypes samples from the International Cancer Genome Consortium (ICGC) data portal. The Lung Cancer dataset\nis composed by the projects LUSC-US (Squamus cell subtype) and LUAD-US (Adenocarcinoma subtype) with 478 and 428\ntumor samples respectively. The Brain cancer dataset is composed by the projects GBM-US (Glioblastoma) and LGG-US\n(Lower Grade Glioma) with 159 and 439 tumor samples respectively. The Renal cancer data is composed by the projects\nKIRP-US (Papillary) and KIRC-US (Clear cell) with 222 and 518 tumor samples respectively. The data matrix for each dataset\nis Xn,d0 with n tumor samples and characterized initially by d0 = 17640 protein coding genes. The objective is to select a\nsubset of genes p << d0 to reduce the dimensionality.\nType\nSubtype\nProject\nPatients (n)\nLung\nDataset\nSquamus cell\nLUSC-US\n478\nAdenocarcinoma\nLUAD-US\n428\nRenal\nDataset\nPapillary\nKIRP-US\n222\nClear cell\nKIRC-US\n518\nBrain\nDataset\nLower Grade Glioma\nLGG-US\n439\nGlioblastoma\nGBM-US\n159\nTABLE I\nNUMBER OF PATIENTS BY TUMOR TYPE AND SUBTYPE.\n1) Pre-processing: To estimate statistically the performance of the proposed method ten independent times 80% of the\nsamples have been randomly selected from the input dataset. The subset of randomly selected samples are used to train the\nautoencoder and to select the gene features. Each feature has been min-max scaled between 0 and 1 [van den Berg et al.,\n2006]. Then an univariate ﬁlter is applied to reduce the initial number of features from d0 = 17640 to d = 8820 by ranking\nthe features by variance and only keeping the 50% best ranked. Univariate ﬁlter is used to discard low variance features and\nperform an initial reduction of the high dimensional space. Nevertheless, the subset of d variables preserved remains large and\nthe sample to feature ratio is (n/d) = 0.088 thus the proposed feature selection method is applied at this point.\nB. Autoencoders\nAutoencoders (AEs) are feed forward artiﬁcial neural networks (ANNs). AEs have the objective to learn two functions, an\nencoder and a decoder. The encoder is a non-linear function that maps the input domain X of n samples and d dimensions\nto a latent space Z of lower dimension l. On the other side, the decoder is a function designed to reconstruct the samples\nfrom the latent space z to the input space. The encoder is forced to learn a function that captures the salient features from X\nand maps to Z [Goodfellow et al., 2016]. The encoder function is deﬁned as z = f (x) and the decoder as ˜x = d (z). The\nsamples at the latent space are expressed by z while ˜x represents the reconstructed samples by the decoder function lying on\nX. During training the autoencoder has to minimize following loss function\nL (x, ˜x) = L (x, d (f (x)))\n(1)\nwhere L penalizes d (f (x)) when it is different from x. The loss function is computed by the Mean Squared Error (MSE)\nexpressed as\nLMSE(X, ˜X)) =\nn\nX\ni=1\n||xi −˜xi||2\nThen the encoder F and decoder D functions are expressed as [Kampffmeyer et al., 2017]\nz = F (x, WF ) =\nσ (WF x + bF )\n˜x = D (z, WD) =\nσ (WDz + bD)\nThe encoding function is F (·, WF ) and the decoding D (·, WD). The expression σ (·) is the activation function of the network.\nThe vectors W and b are the network parameters to learn with the objective to minimize the loss function and represent the\nweights and biases of the encoder and decoder functions respectively. The optimizer used to learn the parameters of the\nnetwork is the Adaptive Moment Estimation (Adam) [Kingma and Ba, 2014] which computes depending on the gradient mean\nan adaptive learning rate to speed up the learning process.\nIn order to force the encoder to learn a useful representation on the latent space and to avoid the AE to just copy ˜x from x\ndifferent regularization strategies are implemented. First, a regularization term using the L2 norm is imposed on the weights\nWF and added to the loss function L. The regularization hyper-parameter is β as follows\nLr = L (x, d (f (x))) + β\nX\ni\n||wi| |2\n(2)\nThe regularization term avoid both f and d to have large weights and leads to learn a simpler model, in consequence this\nreduces the overﬁtting of the trained model. A second regularization strategy that helps to improve even more the generalization\ncapacity of the model is Batch Normalization (BN) [Ioffe and Szegedy, 2015] which consists to perform normalization at each\nmini-batch iteration during training.\nIn this work autoencoders are used to learn meaningful representations from gene expression data from cancer patients. We\npropose also an architecture for both the Encoder and the Decoder functions. Starting from the Encoder the input layer of\ndimension d is fully connected to a hidden layer HL1 of 200 neurons. Then HL1 is fully connected to a second hidden layer\nHL2 with 100 neurons. Finally HL2 is connected to the latent hidden layer HLz. The latent representation Z has a lower\ndimension l = 50 in comparison to the input dimension d and describes with less noise the original data. Symmetrically and\nstarting from the latent layer HLz the Decoder Function has one hidden layer HL3 of 100 neurons followed by one HL4 of\n200 neurons and ﬁnally the output layer of dimension d. Using the sample set {zi} in Z a Kernel Kz is built and used as\ntarget kernel by the following Multiple Kernel Learning step. In the next section Kernel Methods are deﬁned and is explained\nhow features are selected to approach the target representation obtained from the autoencoder.\nC. Multiple Kernel Learning\nA kernel k is a symmetric function k : X × X →R that meets the condition\nk (xi, xj) = ⟨φ(xi), φ(xj)⟩H\n(3)\nwhere X ⊆Rd is a d-dimensional space X, and φ is a mapping function from X to a high dimensional feature Hilbert\nspace H with a dot product such that\nφ : X 7→φ (X) ∈H\n(4)\nand H is a Reproducing Kernel Hilbert Space (RKHS). Considering a set of n samples such that (x1, x2, ..., xn) from X\nthe Kernel or Gram Matrix K is a n × n matrix with entries deﬁned as Kij = ⟨xi, xj⟩. Each position of the Kernel Matrix is\nexpressed as\nKij = ⟨φ (xi) , φ (xj)⟩= k (xi, xj)\n(5)\nand the kernel function is symmetric and positive semi-deﬁnite. By using Kernel functions is not necessary to compute\nexplicitly the mapping φ and the dot products between a pair of samples in the RKHS are computed directly with k (xi, xj). A\nKernel can be thought as a similarity function between vectors samples where at the presence of a pair of orthogonal vectors\noutputs 0 and at the presence of similar or equal vectors outputs a positive value thus Kij ∈[0, 1]. This work uses the Gaussian\nKernel deﬁned as\nk(xi, xj) = exp\n \n∥xi −xj∥2\n2σ2\n!\n; σ > 0\n(6)\nwhere σ deﬁnes the bandwith of the kernel function. The value of σ parameter is obtained by computing the median of the\npairwise distances of the tumor data [Gretton et al., 2005].\nThe Alignment A(K1, K2) between two valid kernels K1 and K2 built from a set of samples n is deﬁned as\nA (K1, K2) =\n⟨K1, K2⟩F\np\n⟨K1, K1⟩F ⟨K2, K2⟩F\n(7)\nand measures how close both kernels are in terms of ﬁnding similarities between pair of samples from n [Cristianini et al.,\n2002] [Kandola et al., 2002]. The operation ⟨K1, K1⟩F means the Frobenious inner product between the two kernel matrices.\nKernels can be combined to form more complex functions capable of handling the biological data. Multiple Kernel Learning\n(MKL) builds a kernel kµ from a linear combination of single kernels ki [G¨onen and Alpaydın, 2011] and is expressed as\nKµ (x, x′) =\nn\nX\ni=1\nµiKi (x, x′) , µi ≥0\n(8)\nThe vector µ represents the weight µi of each kernel ki and reﬂects the relative importance of each kernel in the ﬁnal\nsolution Kµ. There are many objective functions to optimize while learning the MKL task, nevertheless in this work the MKL\nmodel is built with the objective to maximize the alignment of the resulting kernel Kµ and a target kernel Kt. Particularly, the\ntarget kernel proposed in this method is the one Kz built from the samples zi lying at the latent space Z of the Autoencoder\nand the resulting alignment to optimize is A(Kµ, Kz).\nTo compute the vector of weights µ and the resulting Kµ a greedy strategy is used [Pothin and Richard, 2006]. This approach\ncombines just two kernels k1 and k2 at each iteration while maximizing the aligment A(Kµ, Kz) of the resulting kernel. At\neach iteration the weights µ1 and µ2 of the resulting kernel Kµ are obtained by computing the derivative of the alignment\nand ﬁnd where it becomes = 0 as optimal condition for each partial derivative. If µ = [µ1, µ2] at each iteration then µ1 and\nµ2 are obtained from\n∂A(µ1,µ2)\n∂µ1\n= 0\n∂A(µ1,µ2)\n∂µ2\n= 0\nThe condition µ1, µ2 ≥0 is needed since only positive linear combination of kernels are valid kernels. Starting from a set of\nkernels D = [K1, K2, ..., Kd] the greedy MKL algorithm chooses at the ﬁrst iteration the kernel Ki with highest A(Ki, Kz).\nThen it will start adding at each iteration a new kernel to the solution that improves as much as possible the current alignment\nuntil A(Ki, Kz) stop increasing. The ﬁnal vector µ is sparse with only µi > 0 for every Ki selected during the MKL process.\nD. Proposed Latent Kernel Feature Selection Method\nGiven a set of n tumor samples characterized by d gene expression features the data is contained in a Xnd expression matrix.\nIt is desired to select a subset of p features from d. To achieve this goal ﬁrst an autoencoder model is trained and a latent\nspace Z of dimension l is obtained where l << d. Then using the set of samples n projected in the latent space a gaussian\nkernel Kz is built and used as the target kernel. Secondly from Xnd a set of d feature-wise kernels are built producing one\nkernel per feature. Finally by using the MKL model described previously a reduced subset of p kernels is iterativelly selected\nand combined to build a Kµ kernel that increases the alignment A(Kµ, Kz).\nFig. 1.\nPipeline of the proposed method. First starting from the raw data (1) an autoencoder is trained (2) and a latent space learned. Then a Kz kernel\nbuilt (3) using the sample set projected on the latent space. Finally feature-wise kernels are built (4) and combined by MKL (5) to obtain a Kµ kernel by\nimproving the aligment A(Kµ, Kz). The result is a Xnp matrix characterized by a subset of p features associated to the feature-wise kernels selected by\nMKL.\nOnly the feature-wise kernels that increases the alignment A(Kµ, Kz) are included in the ﬁnal kernel Kµ. This approach\nleads to a sparse solution where the non-zero values of the µ vector indicates the feature importance on the result. Features\nare selected by an unsupervised strategy that best align the representation learned from the autoencoder. We name this method\nLatent Kernel Feature Selection (LKFS). Figure 1 shows a diagram of the proposed method.\nE. Evaluation of selected features\nTo evaluate the quality of the selected features the Redundancy Rate (RED) [Zhao et al., 2010b] [Yamada et al., 2014]\nmetric is used in this work. This metric measures between the selected features the mean value of absolute correlation and is\ncomputed as\nRED =\n1\np(p −1)\nX\nfi,fj∈P\n|ρij|\n(9)\nwhere ρij is the correlation index between the ith and the jth selected features. The RED score takes values between 0 and\n1. A RED value close to 0 means low linear correlation between the selected features thus a low redundancy which is a desired\nresult. On the other side when the values of RED are close to 1 means that the selected features are highly redundant which\nis a non desired output.\nTo evaluate visually the quality of the selected features it is possible to use a non-linear dimensionality reduction method\nthat projects the tumor samples characterized by the selected p features in a two-dimensional representation used only for\nvisualization purposes. Then it is possible to evaluate how the tumor samples are distributed in the new feature subset. The\nt-distributed stochastic neighbor embedding (t-SNE) [Maaten and Hinton, 2008] is used in this work for this purposes.\nAnother way to evaluate the quality of the selected features is to perform a unsupervised learning task like clustering after\nfeature selection and measure the quality of the obtained clusters. K-means clustering method [Jain, 2010] is applied on the\ntumor samples characterized by the selected p gene features and each tumor sample xi is assigned to a cluster which implies\nto assign a cluster label ci. The quality of clusters is evaluated by the Adjusted Rand Index [Rand, 1971] and is done by\ncomparing the cluster label ci obtained with the ground truth yi labels related to the tumor subtype provided as clinical data.\nIt is important to remark that the ground truth labels are never used to select the features and are only used to measure how\nwell the k-means groups the tumor samples. It is computed as\nRand Index =\nA + B\nA + B + C + D\n(10)\nwhere A is the number of tumor sample-pairs assigned to the same cluster and belonging simultaneously to the same tumor\nsubtype, B is the number of tumor sample pairs assigned to different clusters and simultaneously belonging to different tumor\nsubtypes, C is the number of tumor sample pairs assigned to the same cluster but belonging to different tumor subtypes and\nD is the number of tumor sample-pairs assigned to different clusters and belonging to the same tumor subtype. The Rand\nIndex can be thought as a clustering accuracy and takes values from 0 to 1 where a value close to 0 means a random and\nnon informative clustering results regarding the ground truth clinical labels. When the Rand Index is close to 1 it means that\nalmost every cluster is populated with tumor samples of the same subtype which is a desired score.\nF. Baseline methods\nTo evaluate the performance of LKFS ﬁrst we stablish a baseline from existing methods. In this work LKFS is compared\nwith the Sparse K-Means (SKM) [Witten and Tibshirani, 2010] and the Spectral Feature Selection (SPEC) [Zhao and Liu,\n2007]. Both have been designed to perform unsupervised feature selection for clustering.\nThe SKM method computes via an optimization problem feature weights w = [w1, ..., wd] and applies a lasso-type penalty\n||w||1 < α to select the most important features while doing k-means clustering. The feature weights are a measurement of the\nvariable importance in clustering. The SKM method is based on the K-means type family of algorithms and assigns a larger\nweight to the features that have a smaller sum of intra-cluster distances and a smaller or zero weight to the features which a\nhigh intra-cluster distance.\nThe other benchmark method is the Spectral Feature Selection for unsupervised learning (SPEC) which is based on spectral\ngraph theory. SPEC uses a pairwise similarity matrix S between samples to build a graph G where each node is a sample\nand each edge is the similarity measurement. The idea with SPEC is to select the features that are consistent with the graph\nstructure. The objective of SPEC is to select features that gives similar values to samples that are near each other in the graph.\nA graph G can be built from the pairwise similarity obtained from X. Then the SPEC method makes a feature ranking based\non the Normalized Cut of the graph G by using the corresponding Laplacian matrix from the graph.\nAll the presented methods have the number of features to select as an hyperparameter p.\nIV. RESULTS\nExperiments on each dataset have been conducted ten times by randomly selecting 80% of the data. At each random iteration\nand in the following order data pre-processing, feature selection and clustering are performed. Then the evaluation by RED\nand Rand Index is averaged among all random iterations. A set of different number of selected p features is used where\np = [10, 20, 30, 40, 50]. A set of k clusters are obtained where k = [2, 3, 4, 5].\nThe ﬁrst evaluation to be done is the RED score for each subset of selected features p of each method on each dataset.\nFig. 2. RED index of the selected features on each dataset by each method.\nFigure 2 shows the results of the RED score. It is observed that LKFS has the lowest RED score in all the experiments\nfollowed by the SPEC method while SKM has the highest RED. This evidences that the features selected by the LKFS have\nthe lowest redundancy which is a desired result since these are more informative and less redundant.\nFig. 3. Two dimensional t-SNE scatter plot for by dataset and method. Each tumor subtype on each dataset is highlighted in blue and orange respectively.\nFigure 3 shows a two dimension t-SNE scatter plot of each dataset by each method whith p = 50. For the Brain, Lung and\nRenal datasets LKFS shows how clusters tend to group tumors of the same subtype together in two main clusters. The SKM\ntend to polarize different tumor subtypes within the same cluster structure and ﬁnally SPEC fails to separate effectively the\ntwo tumor subtypes of each dataset.\nFig. 4. Rand Index on each method and dataset for different values of k clusters and p features.\nThen K-means is applied on the selected features and the quality of clusters measured by the Rand-Index. Figure 4 shows\nthe Rand-Index for different number of k clusters on different number of p selected features for each dataset and for each\nmethod. In the Brain dataset the LKFS method clearly outperforms the SPEC and SKM methods for every value of p and\nk. In both the Renal and Lung datasets the LKFS outperforms the benchmark methods from k = 3 to k = 5. For these two\ndatasets the three methods have a considerably low rand-index with k = 2 since in every case a small and isolated cluster\ncomposed by the two subtypes affects the performance and can be observed in Figure 3.\nA. Discussion\nThis work proposes the LKFS method which selects by an unsupervised approach a reduced gene subset from more than\n8.000 genes to less than 50 in three different types of cancer datasets. The selection of features is done by improving the\nalignment between a kernel Kz obtained from the latent space Z learned via an autoencoder and by the resulting one Kµ\nafter multiple kernel learning on feature-wise kernels. The approach is based on building a target representation of the input\ndata with the latent space of an autoencoder. This target representation has a reduced level of noise and is low dimensional.\nLKFS selects only the features that align the most to the target representation by kernel alignment. The target representation\ncan be interpreted as a prior distribution that guides the selection process.\nTo measure the quality of the unsupervised feature selection process the Redundancy score RED is computed on the selected\nfeatures. In addition a K-means clustering method is applied and the cluster performance is evaluated by the homogeneity of\nground truth labels across clusters by the Rand Index.\nFrom the experimental results it is observed that LKFS outperforms the SKM and SPEC methods by Rand Index and RED\nscore for almost all the datasets and number of selected features. This suggest that LKFS is able to selects low redundancy\nfeatures from high dimensional input space that contributes to ﬁnd well deﬁned clusters composed mainly from one tumor\nsubtype. In contrast, SKM and SPEC select features with higher redundancy which do not contribute enough to build separated\nclusters neither to group samples from the same tumor subtype together.\nOne of the advantages of LKFS comes from the target representation of the Autoencoder. This representation captures the\nsalient features of the input dataset and then by MKL the selected features will capture approximately the same data structure.\nFinally, LKFS provides three outputs. The ﬁrst is the subset of selected features which is considerably reduced in comparison\nwith the original feature set. The second one is the latent space provided by the autoencoder. The latent space serves not only\nas a target representation for the MKL process but also as a tool for data exploration since it can summarizes in a lower\ndimensional space the salient features of the original data. The third output is a set of tumor clusters for further subtype\ndiscovery.\nOne limitation of LKFS relies in the need to train two models, the AE and the MKL. In this architecture the AE conditions\nthe quality of the selected features.\nV. CONCLUSIONS\nThis work proposes an unsupervised feature selection method named LKFS which can select a considerably reduced subset\nof meaningful and low redundant features from high dimensional gene expression data. Experimental results show that the\nproposed method outperforms two unsupervised feature selection algorithms by analyzing the quality of the clusters built\non the selected features. LKFS has been evaluated on tumor gene expression datasets from Lung, Renal and Brain Cancer\npatients and select features that help to identify tumor subtypes without any supervised approach. For this reason LKFS is a\nuseful model for pattern recognition and data mining in a variety of cancer types and high dimensional biological applications.\nFurther work will include multi-omics data fusion approaches in order to consider genomic, proteomic or metabolomic features\nsimultaneously.\nACKNOWLEDGMENT\nThis work was supported by the doctoral program of the Universidad Tecnologica Nacional in Argentina, the UTN FRBA\ndoctoral school of Signal Processing and the Universite de Technologie de Troyes. Also it was funded by grants from CONICET,\nANPCyT and FOCEM-Mercosur.\nREFERENCES\n[Alelyani et al., 2018] Alelyani, S., Tang, J., and Liu, H. (2018). Feature selection for clustering: A review. In Data Clustering, pages 29–60. Chapman and\nHall/CRC.\n[Ang et al., 2015] Ang, J. C., Mirzal, A., Haron, H., and Hamed, H. N. A. (2015). Supervised, unsupervised, and semi-supervised feature selection: a review\non gene selection. IEEE/ACM transactions on computational biology and bioinformatics, 13(5):971–989.\n[Bandyopadhyay et al., 2015] Bandyopadhyay, S., Ghosh, D., Mitra, R., and Zhao, Z. (2015). Mbstar: multiple instance learning for predicting speciﬁc\nfunctional binding sites in microrna targets. Scientiﬁc reports, 5:8004.\n[Cai et al., 2010] Cai, D., Zhang, C., and He, X. (2010). Unsupervised feature selection for multi-cluster data. In Proceedings of the 16th ACM SIGKDD\ninternational conference on Knowledge discovery and data mining, pages 333–342. ACM.\n[Chaudhary et al., 2018] Chaudhary, K., Poirion, O. B., Lu, L., and Garmire, L. X. (2018). Deep learning–based multi-omics integration robustly predicts\nsurvival in liver cancer. Clinical Cancer Research, 24(6):1248–1259.\n[Chen et al., 2016] Chen, F., Zhang, Y., S¸enbabao˘glu, Y., Ciriello, G., Yang, L., Reznik, E., Shuch, B., Micevic, G., De Velasco, G., Shinbrot, E., et al.\n(2016). Multilevel genomics-based taxonomy of renal cell carcinoma. Cell reports, 14(10):2476–2489.\n[Chen et al., 2017] Chen, R., Smith-Cohn, M., Cohen, A. L., and Colman, H. (2017). Glioma subclassiﬁcations and their clinical signiﬁcance. Neurothera-\npeutics, 14(2):284–297.\n[Cristianini et al., 2002] Cristianini, N., Shawe-Taylor, J., Elisseeff, A., and Kandola, J. S. (2002).\nOn kernel-target alignment.\nIn Advances in neural\ninformation processing systems, pages 367–373.\n[Ding and Peng, 2005] Ding, C. and Peng, H. (2005). Minimum redundancy feature selection from microarray gene expression data. Journal of bioinformatics\nand computational biology, 3(02):185–205.\n[Du et al., 2017] Du, W., Cao, Z., Song, T., Li, Y., and Liang, Y. (2017). A feature selection method based on multiple kernel learning with expression\nproﬁles of different types. BioData mining, 10(1):4.\n[G¨onen and Alpaydın, 2011] G¨onen, M. and Alpaydın, E. (2011). Multiple kernel learning algorithms. Journal of machine learning research, 12(Jul):2211–\n2268.\n[Goodfellow et al., 2016] Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep learning. MIT press.\n[Gretton et al., 2005] Gretton, A., Bousquet, O., Smola, A., and Sch¨olkopf, B. (2005). Measuring statistical dependence with hilbert-schmidt norms. In\nInternational conference on algorithmic learning theory, pages 63–77. Springer.\n[Grønbech et al., 2018] Grønbech, C. H., Vording, M. F., Timshel, P. N., Sønderby, C. K., Pers, T. H., and Winther, O. (2018). scvae: Variational auto-encoders\nfor single-cell gene expression data. bioRxiv, page 318295.\n[Guo et al., 2019] Guo, Y., Shang, X., and Li, Z. (2019). Identiﬁcation of cancer subtypes by integrating multiple types of transcriptomics data with deep\nlearning in breast cancer. Neurocomputing, 324:20–30.\n[He and Yu, 2010] He, Z. and Yu, W. (2010). Stable feature selection for biomarker discovery. Computational biology and chemistry, 34(4):215–225.\n[Hinton and Salakhutdinov, 2006] Hinton, G. E. and Salakhutdinov, R. R. (2006).\nReducing the dimensionality of data with neural networks.\nscience,\n313(5786):504–507.\n[Hira and Gillies, 2015] Hira, Z. M. and Gillies, D. F. (2015). A review of feature selection and feature extraction methods applied on microarray data.\nAdvances in bioinformatics, 2015.\n[Ioffe and Szegedy, 2015] Ioffe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift.\narXiv preprint arXiv:1502.03167.\n[Jain, 2010] Jain, A. K. (2010). Data clustering: 50 years beyond k-means. Pattern recognition letters, 31(8):651–666.\n[Kampffmeyer et al., 2017] Kampffmeyer, M., Løkse, S., Bianchi, F. M., Jenssen, R., and Livi, L. (2017). Deep kernelized autoencoders. In Scandinavian\nConference on Image Analysis, pages 419–430. Springer.\n[Kandola et al., 2002] Kandola, J., Shawe-Taylor, J., and Cristianini, N. (2002). On the extensions of kernel alignment.\n[Kingma and Ba, 2014] Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n[Li et al., 2018] Li, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R. P., Tang, J., and Liu, H. (2018). Feature selection: A data perspective. ACM Computing\nSurveys (CSUR), 50(6):94.\n[Lopes et al., 2018] Lopes, M. B., Ver´ıssimo, A., Carrasquinha, E., Casimiro, S., Beerenwinkel, N., and Vinga, S. (2018). Ensemble outlier detection and\ngene selection in triple-negative breast cancer data. BMC bioinformatics, 19(1):168.\n[Lu and Han, 2003] Lu, Y. and Han, J. (2003). Cancer classiﬁcation using gene expression data. Information Systems, 28(4):243–268.\n[Maaten and Hinton, 2008] Maaten, L. v. d. and Hinton, G. (2008). Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579–2605.\n[Moon and Nakai, 2016] Moon, M. and Nakai, K. (2016). Stable feature selection based on the ensemble l 1-norm support vector machine for biomarker\ndiscovery. BMC genomics, 17(13):1026.\n[Palazzo et al., 2019] Palazzo, M., Beauseroy, P., and Yankilevich, P. (2019).\nA pan-cancer somatic mutation embedding using autoencoders.\nBMC\nbioinformatics, 20(1):655.\n[Pothin and Richard, 2006] Pothin, J.-B. and Richard, C. (2006). A greedy algorithm for optimizing the kernel alignment and the performance of kernel\nmachines. In 2006 14th European Signal Processing Conference, pages 1–4. IEEE.\n[Rakotomamonjy et al., 2008] Rakotomamonjy, A., Bach, F. R., Canu, S., and Grandvalet, Y. (2008). Simplemkl. Journal of Machine Learning Research,\n9(Nov):2491–2521.\n[Rand, 1971] Rand, W. M. (1971). Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association, 66(336):846–\n850.\n[Tomczak et al., 2015] Tomczak, K., Czerwi´nska, P., and Wiznerowicz, M. (2015). The cancer genome atlas (tcga): an immeasurable source of knowledge.\nContemporary oncology, 19(1A):A68.\n[van den Berg et al., 2006] van den Berg, R. A., Hoefsloot, H. C., Westerhuis, J. A., Smilde, A. K., and van der Werf, M. J. (2006). Centering, scaling, and\ntransformations: improving the biological information content of metabolomics data. BMC genomics, 7(1):142.\n[Wang and Wang, 2018] Wang, Z. and Wang, Y. (2018). Exploring dna methylation data of lung cancer samples with variational autoencoders. In 2018\nIEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 1286–1289. IEEE.\n[Way and Greene, 2017] Way, G. P. and Greene, C. S. (2017). Extracting a biologically relevant latent space from cancer transcriptomes with variational\nautoencoders. BioRxiv, page 174474.\n[Witten and Tibshirani, 2010] Witten, D. M. and Tibshirani, R. (2010). A framework for feature selection in clustering. Journal of the American Statistical\nAssociation, 105(490):713–726.\n[Xiao et al., 2017] Xiao, J., Lu, X., Chen, X., Zou, Y., Liu, A., Li, W., He, B., He, S., and Chen, Q. (2017). Eight potential biomarkers for distinguishing\nbetween lung adenocarcinoma and squamous cell carcinoma. Oncotarget, 8(42):71759.\n[Yamada et al., 2014] Yamada, M., Jitkrittum, W., Sigal, L., Xing, E. P., and Sugiyama, M. (2014). High-dimensional feature selection by feature-wise\nkernelized lasso. Neural computation, 26(1):185–207.\n[Zhang et al., 2011] Zhang, J., Baran, J., Cros, A., Guberman, J. M., Haider, S., Hsu, J., Liang, Y., Rivkin, E., Wang, J., Whitty, B., et al. (2011). International\ncancer genome consortium data portala one-stop shop for cancer genomics data. Database, 2011.\n[Zhang et al., 2016] Zhang, Y., Li, A., Peng, C., and Wang, M. (2016). Improve glioblastoma multiforme prognosis prediction by using feature selection and\nmultiple kernel learning. IEEE/ACM transactions on computational biology and bioinformatics, 13(5):825–835.\n[Zhao and Liu, 2007] Zhao, Z. and Liu, H. (2007). Spectral feature selection for supervised and unsupervised learning. In Proceedings of the 24th international\nconference on Machine learning, pages 1151–1157. ACM.\n[Zhao et al., 2010a] Zhao, Z., Wang, J., Sharma, S., Agarwal, N., Liu, H., and Chang, Y. (2010a). An integrative approach to identifying biologically relevant\ngenes. In Proceedings of the 2010 SIAM International Conference on Data Mining, pages 838–849. SIAM.\n[Zhao et al., 2010b] Zhao, Z., Wang, L., and Liu, H. (2010b). Efﬁcient spectral feature selection with minimum redundancy. In Twenty-fourth AAAI conference\non artiﬁcial intelligence.\n[Zou and Hastie, 2005] Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. Journal of the royal statistical society: series\nB (statistical methodology), 67(2):301–320.\n",
  "categories": [
    "cs.LG",
    "q-bio.GN",
    "q-bio.QM",
    "stat.ML"
  ],
  "published": "2020-07-12",
  "updated": "2020-07-12"
}