{
  "id": "http://arxiv.org/abs/2007.06183v2",
  "title": "Data-driven geophysics: from dictionary learning to deep learning",
  "authors": [
    "Siwei Yu",
    "Jianwei Ma"
  ],
  "abstract": "Understanding the principles of geophysical phenomena is an essential and\nchallenging task. \"Model-driven\" approaches have supported the development of\ngeophysics for a long time; however, such methods suffer from the curse of\ndimensionality and may inaccurately model the subsurface. \"Data-driven\"\ntechniques may overcome these issues with increasingly available geophysical\ndata. In this article, we review the basic concepts of and recent advances in\ndata-driven approaches from dictionary learning to deep learning in a variety\nof geophysical scenarios. Explorational geophysics including data processing,\ninversion and interpretation will be mainly focused. Artificial intelligence\napplications on geoscience involving deep Earth, earthquake, water resource,\natmospheric science, satellite remoe sensing and space sciences are also\nreviewed. We present a coding tutorial and a summary of tips for beginners and\ninterested geophysical readers to rapidly explore deep learning. Some promising\ndirections are provided for future research involving deep learning in\ngeophysics, such as unsupervised learning, transfer learning, multimodal deep\nlearning, federated learning, uncertainty estimation, and activate learning.",
  "text": "manuscript submitted to Reviews of Geophysics \n 1\nData-driven Geophysics: from Dictionary Learning to Deep Learning \nSiwei Yu1 and Jianwei Ma2,1 \n1 Center of Geophysics, Artificial Intelligence Laboratory, and School of Mathematics, Harbin \nInstitute of Technology, Harbin, China. \n2 School of Earth and Space Sciences, Peking University, Beijing, China. \nCorresponding author: Jianwei Ma (jwm@pku.edu.cn) \nKey Points: \n \nA review of state-of-the art artificial intelligence methods in geophysical applications is \nprovided. \n \nThe relationship between traditional dictionary learning methods and deep learning \nmethods is discussed. \n A tutorial for beginners and a discussion of future directions are given. \n \nmanuscript submitted to Reviews of Geophysics \n 2\nAbstract \nUnderstanding the principles of geophysical phenomena is an essential and challenging task. \n“Model-driven” approaches have supported the development of geophysics for a long time; \nhowever, such methods suffer from the curse of dimensionality and may inaccurately model the \nsubsurface. “Data-driven” techniques may overcome these issues with increasingly available \ngeophysical data. In this article, we review the basic concepts of and recent advances in data-\ndriven approaches from dictionary learning to deep learning in a variety of geophysical scenarios. \nExplorational geophysics including data processing, inversion and interpretation will be mainly \nfocused. Artificial intelligence applications on geoscience involving deep Earth, earthquake, \nwater resource, atmospheric science, satellite remoe sensing and space sciences are also \nreviewed. We present a coding tutorial and a summary of tips for beginners and interested \ngeophysical readers to rapidly explore deep learning. Some promising directions are provided for \nfuture research involving deep learning in geophysics, such as unsupervised learning, transfer \nlearning, multimodal deep learning, federated learning, uncertainty estimation, and activate \nlearning. \nPlain Language Summary \nWith the fast development of artificial intelligence (AI), innumerable students and researchers in \nthe geophysical community would like to know the AI, to learn how to use AI for geophysics, \nhow and what the AI can impact the traditional geophysical methods. We present a review \nwritten in the geophysical language, for readers to get the picture for history, recent advances, \nopen problems, and future directions. This review aims to pave the way for more geophysical \nresearchers, students, and teachers to use data-driven AI techniques.  \n1 \nIntroduction \nObservation is an important means by which humans come to understand unknown \nnatural phenomena. Geophysics involves the physical mechanics and properties of the earth and \nspace environments at various temporal and spatial scales, and the subject encompasses \nseismology, gravity fields, magnetic fields, electric fields, the atmosphere, and the internal \nstructures of Earth and other planets. With state-of-the-art observation equipment, the amount of \nmanuscript submitted to Reviews of Geophysics \n 3\nobserved data is increasing at an impressive speed. How to process such a large amount of \nobserved data and obtain useful information is a significant problem for further understanding \nthe laws of geophysics. Model- and data-driven methods are two important ways to process \ngeophysical data (Figure 1). \nModel-driven geophysics is based on induction and deduction from the perspective of \nphilosophy of science. First, the principles of geophysical phenomena are induced from a large \namount of observed data. Mathematical or modeling methods are established based on physical \ncausality and laws, such as the wave equation, the diffusion equation, the heat transfer equation, \nMaxwell’s electromagnetic equation, Newton’s law of universal gravitation, Newton’s law of \nmotion, and many empirical formulas. Then, the models are used to deduce future or past \ngeophysical phenomena. Model-driven methods have played a vital role in the evolution of \ngeophysical methods. For example, Newton’s law of universal gravitation helps find Neptune \nand Pluto without direct observation at these two planets. Despite the success of model-driven \nmethods, they have limitations in the accurate prediction of complex system states at large \nspatial and temporal scales, such as in global climate estimation and earthquake prediction. We \nlist several difficult tasks in geophysics in Table 1, in which data-driven methods may be more \nadvantageous than model-driven methods. \nTo show the bottlenecks of model-driven methods in detail, we use exploration \ngeophysics as an example. Exploration geophysics aims to observe the subsurface of Earth or \nother planets with physical fields collected at the surface, such as seismic fields and gravity \nfields. The process of exploration geophysics includes signal processing, modeling, inversion, \nand interpretation. In the geophysical signal processing stage, the simplest model assumption \nregarding the shape of underground layers is the linear assumption in small windows (Spitz \n1991). Further assumptions include the sparsity (Herrmann and Hennenfent 2008) and low-rank \n(Oropeza and Sacchi 2011) assumptions, among others. However, the predesigned linear event \nassumption or sparse transform assumption is not adaptive to seismic data and may lead to low \ndenoising or interpolation quality for data with complex structures. In the seismic modeling and \ninversion stages, wave equations govern the kinematics and dynamics of seismic wave \npropagation. Acoustic, elastic, or viscoelastic wave equations introduce an increasing number of \nmanuscript submitted to Reviews of Geophysics \n 4\nfactors into the wave equations, and the generated wave field records can precisely estimate real \nscenarios. However, as the wave equation becomes increasingly complex, the numerical \nimplementation of the equation becomes nontrivial, and the computational cost increases \nconsiderably for large-scale scenarios. In seismic interpretation tasks, traditional methods rely on \nthe experience of interpreters, leading to low efficiency and subjective bias. Therefore, an \nadaptive and automated process in geophysical fields is needed. Figure 2 illustrates model- and \ndata-driven methods in exploration geophysics. \n“Data-driven” methods were proposed to overcome the bottlenecks of model-driven \napproaches. In data-driven geophysics, given a large amount of observed data, the computer first \nbuilds a regression or classification model without considering physical causality. This process is \nalso called training. Then, this model performs tasks such as prediction, classification, and \nrecognition on incoming datasets. Dictionary learning (Aharon et al. 2006) and deep learning \n(DL) (LeCun et al. 2015) are two widely adopted data-driven techniques. In dictionary learning, \nan adaptive dictionary is learned as a representation of the target data rather than utilizing a \npredefined dictionary. Dictionary learning is widely applied in seismic data denoising, \ninterpolation, and inversion based on compressive sensing (Candes and Wakin 2008). The key \nfeatures of dictionary learning are single-level decomposition, unsupervised learning, and \nlinearity. Single-level decomposition means that one dictionary is used to represent a signal. \nUnsupervised learning means no labels are provided during dictionary learning. In addition, only \nthe target data are used without an extensive training set. Linearity implies that the data \ndecomposition on the dictionary is linear. The above features make the theory of dictionary \nlearning simple; however, they limit the data representation ability of a dictionary. Unlike \ndictionary learning, DL decomposes data at a deep level and has an excellent representation \nability. \nDL and artificial intelligence (AI) are very hot topics in recent years. DL is a \nrepresentative category of AI methods. DL methods train a deep neural network (DNN) through \na complex nonlinear mapping process with adjustable parameters based on a large dataset. The \ntrained DNN is used to predict the desired output of the target data for a specific task, such as \nprediction, detection, and classification. Deep learning encompasses both supervised and \nmanuscript submitted to Reviews of Geophysics \n 5\nunsupervised data-driven approaches depending on whether labels are available. Recently, deep \nlearning methods have been widely adopted in various geophysical applications, such as solid \nEarth geoscience (Bergen et al. 2019), aftershock pattern analysis (DeVries et al. 2018), and \nEarth system analysis (Reichstein et al. 2019). \nA review article about DL in solid Earth geoscience was recently published in Science \n(Bergen et al. 2019). The topic includes a variety of machine learning techniques, from \ntraditional methods, such as logistic regression, support vector machines, random forests and \nneural networks, to modern methods, such as deep neural networks and deep generative models. \nThe article stresses that machine learning will play a key role in accelerating the understanding \nof the complex, interacting and multiscale processes of Earth’s behavior. Since 2019, AI \ngeophysics has made rapid progress. Our review will introduce AI geophysics at the leading \nedge, mainly focusing on DL, will cover a variety of geophysical applications, from deep to the \nearth’s core and far into outer space, and will mainly focus on exploration geophysics. \nFurthermore, this review will help readers transfer existing knowledge on dictionary learning and \ncompressive sensing to DL. This review intends not only to provide a glance at the most recent \nDL research related to geophysics for geophysical readers but also to provide a cookbook for \nbeginners who are interested in DL, from geophysical students to young researchers. \nThis review is organized as follows. First, the background, target, and outline of this \nreview are given in the introduction (S1). The review part consists of three layers. The first layer \ncontains concepts, and we introduce the basic idea of dictionary learning and DL (S2). The \nsecond layer contains detailed techniques (S3, S4). Applications in exploration geophysics are \nintroduced following each method to better explain the concepts of dictionary learning and DL. \nThe third layer presents AI applications in other geophysical areas (S5). The relationship \nbetween traditional methods and DL is further discussed in S6. A tutorial section for beginners \n(S7) and a discussion of future directions (S8) are given as extensions of this review. S9 \nsummarizes this review. \nmanuscript submitted to Reviews of Geophysics \n 6\n2 \nGeneral theory \nReaders who are already familiar with general theory in dictionary learning, compressive \nsensing and DL may wish to skip to Section 3. We denote scalars by italic letters, vectors by bold \nlowercase letters and matrices by bold uppercase letters. In geophysics, the goal is to invert \nunknown parameters x from an available dataset y=Lx. L is a forward or degraded operator in \ngeophysical applications, such as denoising, reconstruction, or full-waveform inversion. \nHowever, L is usually ill-conditioned or not invertible. Compressive sensing approximates x by \nforming an optimization objective loss function E(x) with an additional constraint R: \n\n\n\n( )\n,\nE\nD\nR\n\n\nx\nLx y\nx  \nwhere D is a similarity measurement function. Typically, the L2-norm \n2\n\nLx\ny\n is used for \nsmooth measurement. R is a regularization term. Sparsity is a popular regularization term \nadopted in compressive sensing, where \n\n1\nR\n\nx\nWx . W is a sparse transform with several \nvectorized basis. W is also termed as dictionary. The goal of dictionary learning is to train an \noptimized sparse transform W, which is used for the sparse representation of x. Dictionary \nlearning involves learning W via matrix decomposition with constraints Rw and Rc on the \ndictionary W and coefficient v. \n \n\n\n\n\nT\n(\n, )\n,\n( )\nw\nc\nE\nD\nR\nR\n\n\n\nW v\nW v x\nW\nv  \n(1) \n \nUnlike dictionary learning, deep learning treats geophysical problems as regression or \nclassification problems. A DNN F is used to approximate x, \n\n\n;\nF\n\nx\ny Θ  \nwhere Θ is the parameter set of the DNN. From the view of mathematics, a DNN provides a \nhigh-dimensional and nonlinear mapping from y to x. From the perspective of biology, the \narchitecture of a DNN is bionic and includes biological nerve cells in a multilayer structure. Each \nlayer contains several neural units, with input, output, and nonlinear activation features that are \nanalogous to axons, dendrites, and the cell unit. The input and output of a neuron are connected \nto the neurons in the neighboring layers, and Θ represents the connection weights. A particular \nmanuscript submitted to Reviews of Geophysics \n 7\nconnection format, the convolutional filter, is often used in modern DNNs to share the \nparameters among different neurons. \n \nDeep learning aims to build a high-dimension approximation between two sets \n\n\n,\n1\ni i\nN\n\n\nX\nx\n and \n\n\n,\n1\ni i\nN\n\n\nY\ny\n, i.e., the inputs and labels, with a DNN. The \napproximation is achieved by minimizing the following loss function to obtain an optimized Θ: \n\n\n2\n2\n1\n( ;\n,\n)\n;\nN\ni\ni\ni\nE\nF\n\n\n\n\nΘ X Y\nx\ny Θ\n \nIf F is differentiable, a gradient-based method can be used to optimize Θ. However, a \nlarge Jacobi matrix is involved when calculating \nE\nΘ\n, making it infeasible for large-scale \ndatasets. A back-propagation method (LeCun et al. 1997) is proposed to compute \nE\nΘ\n and \navoid calculating the Jacobi matrix. In the following sections, we first introduce dictionary \nlearning from the traditional K-means method and the widely used K-SVD approach; a recent \nfast algorithm with a tight data-driven framework and other deep learning methods are also \npresented. Each subject is introduced along with applications in exploration geophysics. A \nmapping diagram of the primary references used in this paper is given in Figure 3.  \n3 \nDictionary learning \n3.1 \nK-means \nClustering is used to group geophysical attributes into several classifications. For \nexample, we need to decide whether a region contains fluvial facies or faults based on stacked \nsections. K-means (Hartigan and Wong 1979), which is a classical clustering algorithm, can be \ntreated as a dictionary learning method with an extremely sparse representation, where only one \ndictionary component is allowed, and the representation coefficient must be one. Though simple, \nwe can briefly explore the basic steps in dictionary learning with this approach. \nK-means aims to cluster N given samples with M features into K groups. K-means applies \ntwo steps per iteration with K randomly initialized cluster centers. i) Assign the training samples \nto the nearest cluster center. ii) Update each cluster center based on the weighted center of the \nmanuscript submitted to Reviews of Geophysics \n 8\nattached samples. Figure 4 shows an example of how K-means splits a dataset into two classes \nbased on two selected features. \n \nK-means, and the corresponding improved methods, is used for signal classification in \ngeophysics. Because K-means is sensitive to feature selection, Galvis et al. (2017) suggested that \nseismic attributes be selected based on the notion of similarity and that these attributes can be \nused in the classification of surface waves with the K-means approach. K-means is also sensitive \nto outliers. Song et al. (2018) propose an adaptive-phase K-means method. The advantage of \nusing the phase distance as a similarity measure is that it provides robustness in the presence of \nhorizon error. The classification result of K-means depends on the user-specified number of \nclusters. Waheed et al. (2019) showed that the density-based spatial clustering method does not \nrequire a specification for the number of clusters and reduces the cost of automatic velocity \nselection compared to that in the traditional K-means approach. De Lima and Marfurt (2018) \nproposed a combination of principal component analysis and K-means for the classification of \nairborne gamma ray spectrometry. \n3.2 \nK-SVD \nSimilar to K-means, most dictionary learning algorithms consist of two steps: i) sparse \ncoding and ii) dictionary updating. Unlike K-means, the sparse representation is not limited to \none component, and the number of representation coefficients is also not limited. The method of \noptimal directions (MOD) (Engan et al. 2002) uses orthogonal matching pursuit for sparse \ncoding and a second-order Newtonian method for dictionary updating. Beckouche and Ma (2014) \nuse the MOD method for dictionary learning and sparse approximation in seismic denoising. The \ndictionary updating approach in MOD has favorable flexibility and simplicity; however, it is \nrelatively impractical for large dictionaries since matrix inversion is involved. \n \nK-SVD (where SVD is singular value decomposition) (Aharon et al. 2006) shares the \nsame sparse coding structure as MOD, but several improvements in dictionary updating are \ngiven. First, one component of the dictionary is updated at a given time, and the remaining terms \nare fixed. Second, a rank-1 approximation SVD algorithm is used to obtain the updated \ndictionary and coefficients simultaneously, thereby accelerating convergence and reducing \nmanuscript submitted to Reviews of Geophysics \n 9\ncomputational memory use compared to those in the MOD. K-SVD is applied in geophysics with \npreferred extensions. Nazari Siahsar et al. (2017) split the training data into different slices and \ntrained different dictionaries with a shared sparse coefficient matrix. Such a strategy allows 3-D \ndatasets to processed with a reasonable time cost for all training patches. \n3.3 \nData-driven tight frame \nDespite the success of K-SVD in signal enhancement and compression, dictionary \nupdating is still time consuming in regard to high-dimensional and large-scale datasets, such as \n3-D prestacked data in seismic exploration. K-SVD includes one SVD step to update one \ndictionary term. Can the entire dictionary be updated by one SVD for efficient improvement? \nCai et al. (2014) proposed a data-driven tight frame (DDTF) by enforcing a tight frame constraint \non the dictionary. The tight frame condition is a slightly weaker condition than orthogonality, for \nwhich the perfect reconstruction property holds. With the tight frame property, dictionary \nupdating in DDTF is achieved with one SVD, which is hundreds of times faster than K-SVD. \n \nLiang et al. (2014) first utilized 2-D DDTF in seismic data interpolation. The extension of \ndictionary learning to high dimensions is straightforward since the data are vectorized at the \npatch scale. Patches are blocks generated from original data division into training samples. Yu et \nal. (2015) extend DDTF to 3-D and 5-D with applications in seismic data interpolation. The \ntraining patches for dictionary learning are a random subset of all patches. An example of a \nlearned dictionary with 3-D DDTF for a seismic volume is shown in Figure 5. Yu et al. (2016) \ndesigned a Monte Carlo selection method based on a training set. The patches with high variance \nwere selected with high probability to further improve efficiency. Liu et al. (2017) proposed \ntensor DDTF, in which high-dimensional data are obtained by tensor products, to save \ncomputational resources and constrain data structures. Liu et al. (2018) and Liu and Ma (2019) \nproposed graph DDTF, in which a binary tree is used to cluster training patches. DDTF is \nimplemented for each cluster to obtain a sparse dictionary with similar patches as the original \ndictionary. Wang and Ma (2019) and Wang et al. (2019) proposed adaptive DDTF and group-\nsparsity DDTF for the preservation of weak signals by considering the similarity among different \npatches. \nmanuscript submitted to Reviews of Geophysics \n 10 \n4 \nFrom dictionary learning to deep learning \nThough both are data-driven methods, deep learning differs from dictionary learning in \nthree aspects: the depth of decomposition, the amount of training data, and the nonlinear \noperators. Dictionary learning is usually a single-level matrix decomposition problem. \nRubinstein et al. (2010) proposed double sparsity (DS) dictionary learning to explore deep \ndecomposition. The motivation of DS is that the learned dictionary atoms still share some \nunderlying sparse pattern for a generic dictionary. In other words, the dictionary is represented \nwith a sparse coefficient matrix multiplied by a fixed dictionary, as in discrete cosine transform. \nInspired by DS dictionary learning, can we propose triple, quadruple or even centuple dictionary \nlearning? We know cascading linear operators are equivalent to a single linear operator. \nTherefore, using more than one fixed dictionary does not improve the signal representation \nability compared to that ability of one fixed dictionary if no additional constraints are provided. \nIn deep learning, nonlinear operators are combined in such a deep structure. A neural network \nwith one hidden layer and nonlinear operators can represent any complex function with a \nsufficient number of hidden neurons. To fit a neural network with many hidden neurons, we need \nan extensive training set, while dictionary learning involves only one target data. A comparison \nof the learned features in dictionary learning and deep learning is shown in Figure 6. \n \nDeep learning is a machine learning method based on a DNN, which is an artificial neural \nnetwork (ANN) with many layers. ANNs are widely in machine learning and date from the late \n1940s. In a multilayer perceptron (MLP), a specific type of ANN, neurons are organized into \ndifferent groups called layers. Neurons in adjacent layers are connected by connection weights. \nThe output of a neuron is the weighted summation of the output of the neurons from the previous \nlayers, and each output is then input into a nonlinear activation function. The simplest MLP \nincludes an input layer, a hidden layer, and an output layer.  \n \nPoulton (2002) published a review article on ANN methods in geophysics in 2000. Since \n2000, many pioneers have applied machine learning methods in geophysics, and deep learning \nhas slowly become popular. Limited by the length of this review, we only recall some such \nstudies. Lim (2005) characterized reservoir properties using fuzzy logic and an ANN for well \nmanuscript submitted to Reviews of Geophysics \n 11 \ndata. Huang et al. (2006) explored seismic data parameter determination and pattern detection \nwith an ANN. Helmy et al. (2010) applied hybrid computational models to characterize oil and \ngas reservoirs. Zhang et al. (2014) proposed using a kernel-regularized least-squares (Evgeniou \net al. 2000) method for fault detection from seismic records. Jia and Ma (2017) suggest using \nsupported vector regression (Cortes and Vapnik 1995) for seismic interpolation. The authors \nused linearly interpolated data and original data as the inputs and outputs, respectively, and \nsupported vector regression was used to obtain the relation between the inputs and outputs. They \nclaimed that no assumptions were imposed on the data and that no parameter tuning was required \nfor interpolation. \n \nOur review will focus on DNNs. The number of layers in an ANN has a significant effect \non the fitting and generalization abilities of an algorithm. Early ANNs were restricted to a few \nlayers due to the computational capacity of the available hardware. With the development of \nhardware and optimization algorithms, ANNs are expanding towards deep layers. However, \nMLPs encounter computational and storage problems due to the massive number of parameters \nrequired when the network deepens or the size of the inputs increases in practical applications. In \naddition, an MLP requires preselected features as inputs into the neural network and ignores the \nstructure of the input entirely, with full reliance on experience. Qi et al. (2020) proposed feature \nselection for machine learning facies analysis. An exhaustive search method, which took 638 \nhours for the candidate attributes, was used to determine both the optimal number and \ncombination of parameters. To reduce the number of parameters in an MLP, convolutional \nneural networks (CNN) (McCann et al. 2017) were proposed to share parameters with \nconvolutional filters. \n4.1 \nConvolutional neural networks \nCNNs were proposed to consider local coherency and reduce the number of weight \nparameters. CNN uses convolutional filters to restrict the inputs of a neural network to within a \nlocal range. The convolutional filters are shared by different neurons in the same layer. A CNN \nuses original data rather than selected features as an input set. Pooling layers are used in CNNs to \nextract key features by subsampling the input set. CNNs have developed rapidly since 2010 for \nmanuscript submitted to Reviews of Geophysics \n 12 \nimage classification and segmentation, and some popular CNNs include VGGNet (Simonyan and \nZisserman 2015) and AlexNet (Krizhevsky et al. 2017). CNNs are also used in image denoising \n(Zhang et al. 2017) and super-resolution tasks (Dong et al. 2014). The above CNNs are named \nvanilla CNNs, which are CNNs with simple sequential structures. Vanilla CNNs are used for \nregression and classification tasks. In regression tasks, the outputs are continuous variables; in \nclassification tasks, the outputs are discrete variables. Compared to K-means, deep learning \nachieves complex classification with a multilayer feature extractor and simple classifiers. \n \nAdditional deep learning network architectures have been proposed for specific tasks \nbased on MLPs or vanilla CNNs (Figure 7a,b). An autoencoder (AE) is a network in which the \ninputs and outputs are the same. Hidden layers in AEs extract deep features that are typically \nused for unsupervised classification with the help of K-means. A deep convolutional autoencoder \n(CAE, Figure 7c) is an AE with convolutional layers that acts as a feature extractor. U-Net \n(Ronneberger et al. 2015) (Figure 7d) uses skip connections to bring low-level features to a high \nlevel. The generative adversarial network (Goodfellow et al. 2014, Creswell et al. 2018) (GAN, \nFigure 7e) aims to reproduce data examples with the same distribution as the training set. A \nGAN contains a generative network and a discriminative network. The generative network tries \nto produce a nearly real image. The discriminative network tries to distinguish whether the input \nimage is real or generated. Therefore, such a game will finally allow the generative network to \nproduce fake images that the discriminative network cannot distinguish from real images. \nCycleGAN (Zhu et al. 2017) is a GAN with two generative networks and two discriminative \nnetworks, such that a cycle mapping between two datasets is trained. Recurrent neural networks \n(RNNs, Figure 7f) are commonly used for prediction tasks based on sequential data, and the \nprediction for the current input depends on the history of inputs fed into the neural network. \nLong short-term memory (LSTM) (Hochreiter and Schmidhuber 1997) is a widely used RNN \nthat considers how much historical information is forgotten or remembered. We introduce the \nconcepts and applications of the above DNNs in detail in the following sections. \nmanuscript submitted to Reviews of Geophysics \n 13 \n4.2 \nVanilla convolutional neural networks \nVanilla CNNs are the most popular CNNs if many training samples and labels are \navailable. Cascading convolutional layers, nonlinear layers, and data regularization layers \nprovide a remarkable fit for the training samples in regression tasks. Pooling layers are used for \nfeature extraction in classification tasks. Vanilla CNNs are reliable for most applications in \ngeophysics, such as denoising, interpolation, velocity modeling, and data interpretation. \nIn the seismic denoising area, Yu et al. (2019) proposed a denoising CNN (DnCNN) \n(Zhang, Zuo et al. 2017) based method for three kinds of seismic noise. The DnCNN developed \nwas composed of convolutional, batch normalization, and rectified linear unit layers. In this \napproach, the final output is equal to the network output plus the input, which is called residual \nlearning, i.e., the output of the network represents noise. The concept of residual learning is \nsimilar to that used in a residual network (He et al. 2016) to avoid vanishing gradients. Random \nand linear noise are manually added to synthetic datasets, and multiple data sets are generated \nwith the acoustic wave equation. In this case, transfer learning (Donahue et al. 2014) is used for \nfield data denoising. Different kinds of noise are processed with the same network architecture \nbut different training sets. An example of scattered ground-roll attenuation is shown in Figure 8. \nScattered ground roll is mainly observed in desert area, caused by the scattering of ground roll \nwhen the near surface is laterally heterogeneous. Scattered ground roll is difficult to remove \nbecause it occupies the same F-K domain as reflected signals. DnCNN was used to remove \nscattered ground roll successfully. Wu et al. (2019) claimed that in a traditional CNN, the labels \nof clean data are difficult to obtain. They used multiple trials involving user-generated white \nnoise to simulate real white noise. Additionally, the inputs were decomposed with the variational \nmode decomposition method (Dragomiretskiy and Zosso 2014) to obtain a few modes with \ndifferent frequency supports, which were then fed into a CNN. \n \nIn the seismic interpolation field, Wang et al. (2019) proposed the use of ResNet for the \nreconstruction of regularly missing data. The training set consisted of synthetic and field samples. \nThe input of the network was preprocessed with a bicubic interpolation algorithm. Zhang et al. \n(2020) trained a denoised neural network with a natural image dataset and used the trained \nmanuscript submitted to Reviews of Geophysics \n 14 \nnetwork in the project onto a convex set (Abma and Kabir 2006) framework for seismic data \ninterpolation. Therefore, no new networks were required for the interpolation of other datasets or \nother tasks. Figure 9 gives the training set and a simple interpolation result (Zhang, Yang et al. \n2020). \n \nIn seismic deblending, Zu et al. (2020) constructed an end-to-end deblending CNN. The \ntrained network was iteratively applied to blended data. The authors claimed that networks \ntrained with both synthetic and field datasets perform well with real input datasets. Sun et al. \n(2020) also used an end-to-end CNN for deblending. Different hyper-parameters, such as the \nnumber of layers, number of filters, and size of the filters, were used to construct a network that \nwas optimal for seismic data. They used field datasets to construct the training set, which \ninevitably contained some noise contamination associated with the labels. Nakayama et al. (2019) \npresented a method for designing acquisition parameters, including blending, source, and \nreceiver positions, based on a genetic algorithm and CNN. The genetic algorithm was used to \nproduce combinations of acquisition parameters, and the CNN was used to classify the \ncombinations. Finally, a deblending algorithm was used to obtain a clean signal. If this signal \nwas close to the original signal, the iteration stopped; if not, the algorithm proceeded with new \nparameters. \n \nIn velocity analysis and inversion, Araya-Polo et al. (2018) used a CNN for seismic \ntomography and obtained a promising result for synthetic 2D data. Wang and Ma (2020) \nproposed a network combined with fully connected layers and fully convolutional layers (FCN). \nThe FCN used a contracting encoder and an expansive decoder corresponding to feature \nextraction and function fitting, respectively. The FCN yielded outputs with the same size as the \ninputs. The input was a seismology data set with a cross-well geometry, and the output was a \nvelocity model. Notably, the authors used smoothed natural images as seismic models, thus \nproducing a large number of models to construct the training set. Figure 10 shows how Wang \nand Ma (2020) converted a three-channel color image to a velocity model. Park and Sacchi (2019) \ndeveloped a CNN to directly estimate stacking velocities. The portions with different time slices \nas channels were used as inputs. The root square velocity was the output. For a much different \ndataset, the authors used transfer learning instead of network training from random initialization. \nmanuscript submitted to Reviews of Geophysics \n 15 \nOvcharenko et al. (2019) proposed a DL framework for extrapolating the frequency range of \nseismic data from high to low frequencies. The inputs and outputs of the network were multiple \nhigh-frequency and single low-frequency representations of a shot gather. Moreover, 0.25 Hz \ndata were obtained from 2 to 4.5 Hz frequencies. Low-frequency information was used for the \ninitialization of full-waveform inversion (FWI). \n \nIn the attribute inversion area, Das et al. (2019) proposed a 1-D CNN for seismic \nimpedance inversion. The training set consisted of synthetic datasets and contained six output \nfeatures, such as the spherical variogram ranges of facies, phases, and the central frequency. The \nuncertainty was computed with an approximate Bayesian computational method. You et al. \n(2020) predicted anisotropy information from conventional well logs based on a DNN, and the \nmethod was generalized for use with field data. \n4.3 \nConvolutional autoencoder \nA CAE is a type of CNN consisting of an encoder and a decoder. The encoder uses \nconvolutional layers and pooling layers to extract critical features in a latent space from the \ninputs, resulting in a contracting path. The decoder uses deconvolutional layers and unpooling \nlayers to decode the features into the original data space, resulting in an expanding path. A CAE \nworks in both supervised and unsupervised ways. If labels are provided as outputs, a CAE is a \nsupervised regression network. If the outputs are the same as the inputs, a CAE works in an \nunsupervised way, and the latent features are used for other tasks, such as clustering. The learned \nlatent features can also be used for dimension reduction in large-scale tasks. \n \nIn seismic data processing, Wang et al. (2020) proposed a CAE-based interpolation \nmethod for irregular sampling. The subsampled dataset is the input, and the complete dataset is \nthe output. Transfer learning is used when the method is applied to field data. Wu et al. (2019) \ntreated first-arrival selection as an image segmentation problem with a CAE. Anything prior to \nthe first arrival is set to zero, and all instances after the first arrival are set to one. This method \nworks well for noisy situations and field datasets. For the training set, a subset of traces \ngenerated with a simple model is used. Gao et al. (2019) used CAE for dimension reduction in \nmanuscript submitted to Reviews of Geophysics \n 16 \nFWI to estimate longwave information, where the latent parameters were optimized instead of \nthe whole dataset. \n \nIn attribute analysis, Duan et al. (2019) used a CAE to extract the features of 1D data and \nK-means for clustering. They used Kullback-Leibler divergence to measure the similarities \nbetween the two distributions. He et al. (2018) and Qian et al. (2018) used an AE to extract \nseismic features in an unsupervised way, and then a K-means clustering method was used to \nclassify the seismic facies. Qian, Yin et al. (2018) built a physical model and used field data from \nthe Liziba survey to test the proposed method. \n4.4 \nU-Net \n \nU-Nets have U-shaped structures and skip connections. The skip connections bring low-\nlevel features to high levels. U-Net was first proposed for image segmentation and has been \napplied in seismic data processing, inversion, and interpretation. The U-structure with a \ncontracting path and expanding path makes every data point in the output contain all information \nfrom the input, such that the approach is suitable for mapping data in different domains, such as \ninverting velocity from seismic records in FWI. The input size of the test set must be the same as \nthat in the training set for a trained U-Net. \n \nIn seismic data processing and inversion, Mandelli et al. (2018) used a U-Net for the \ninterpolation of seismic data, and prestack images with and without missing traces were used as \ninputs and outputs. Hu et al. (2019) proposed a U-Net-based first-arrival selection method by \nformulating a binary segmentation problem. Yang and Ma (2019) proposed a new general \nvelocity model construction method based on U-Net. The inputs were seismology data sets \ngenerated by the acoustic wave equation from surface survey, and labels were the velocity \nmodels. This method is useful for generating low-frequency models for the initialization of \ntraditional FWI. Low-frequency information helps FWI converge. Figure 11 shows the velocity \nprediction results from Yang and Ma (2019). Unlike the conventional inversion method based on \nphysical models, supervised deep learning methods are based on big-data training rather than \nprior-knowledge assumptions. Unlike in FWI, after network training is completed, the \nreconstruction costs are negligible. Moreover, little human intervention is needed, no initial \nmanuscript submitted to Reviews of Geophysics \n 17 \nvelocities are involved, and no cycle-skipping problem exists. Instead of regression from seismic \nrecords to velocity models, Zhang and Alkhalifah (2019) used deep learning to estimate the \ndistribution of facies from the results of conventional FWI, and the facies were used to constrain \na new iteration of the FWI approach several times. In this method, deep learning was integrated \ninto FWI as part of a model constraint. \n \nIn seismic interpretation, Wu et al. (2020) built an approximately realistic 3-D training \ndataset by randomly choosing folding and faulting parameters in a reasonable range. Then, the \ndataset was used to train a 3D U-Net for seismic structural interpretation for features such as \nfaults, layers, and dips in field datasets. Building realistic synthetic datasets rather than \nhandcrafted field datasets is more efficient and can produce similar results. Wu et al. (2019) used \nan end-to-end U-Net for 3D seismic fault segmentation. The inputs were seismic images, and the \noutputs were ones, indicating faults, and zeros, indicating nonfaults. A class-balanced binary \ncross-entropy loss function was used to adjust the data imbalance so that the network was not \ntrained to predict only zeros. A network trained only on synthetic data worked well when field \ndatasets were considered. Wu et al. (2019) treated the horizon interpretation problem as an image \nclassification task; they used U-Net and post-stack traces located on a user-defined coarse grid as \nthe inputs and manually picked horizons as labels. This approach is semi-automated because the \nhorizons must be labeled for the coarse grid. The traces were processed individually. An example \nof synthetic post-stack image and field data fault analysis is shown in Figure 12, as published by \nWu, Geng et al. (2020). \n \nIt is convenient to apply U-Net in various geophysical applications. Here, we give two \nexamples: velocity picking and first-arrival picking. Figure 13 shows the results of using U-Net \nfor velocity picking. The inputs are seismological data, and the outputs are ones where the picks \nare located and zeros elsewhere. Figure 14 shows the results of the phase picking based on U-Net. \nWe used 8000 synthetic samples. A gradient constraint was added in the loss function to enhance \nthe continuity of the picked positions. Seismological data sets were used as inputs. For the output, \nthree classifications were set: zeros above the first arrival, ones below the first arrival, and twos \nfor the first arrival. The training dataset was contaminated with strong noise and had missing \ntraces. The predicted picking were close to the labels. First-arrival picking based on deep \nmanuscript submitted to Reviews of Geophysics \n 18 \nlearning has been applied for realistic seismic data processing by using different neural networks \n(Hu, Zheng et al. 2019, Wu, Zhang et al. 2019). \n4.5 \nGenerative adversarial networks \nGANs can be applied in adversarial training for two CNNs: one generator to produce a \nfake image and one discriminator to distinguish the produced image from real images. When \ntraining the discriminator, the real dataset and generated dataset correspond to labels one and \nzero, respectively. Additionally, when the generator is trained, all datasets correspond to the \nlabel one. A GAN is used to generate samples with similar distributions as the training set. The \ngenerated samples are used for simulating realistic scenarios or expanding the training set. Zhu, \nPark et al. (2017) proposed an extended GAN, named CycleGAN, with two generators and two \ndiscriminators for signal processing. In CycleGAN, a two-way mapping is trained for mapping \ntwo datasets from one to the other. The training set CycleGAN is not necessarily paired, as in a \nvanilla CNN, which makes it relatively easy to construct training sets in geophysical applications \nwith few labels. \n \nTo artificially expand labeled data sets, Wang et al. (2019) proposed the GAN-based \nmodel EarthquakeGen. The detection accuracy was greatly improved by performing artificial \nsampling for the training set. Si et al. (2020) proposed the use of CycleGAN for ground-roll \nattenuation. The training set consisted of synthetic and field-derived seismic data. Zhang et al. \n(2019) proposed a seismic enhancement algorithm based on a GAN with time resolution \nimprovements. Lipari et al. (2018) used a GAN to map low-quality migrated images to high-\nquality images and the corresponding reflectivity images. Siahkoohi et al. (2019) proposed a \nGAN to produce a high-quality wavefield from a low-quality wavefield in the context of surface-\nrelated multiples, ghosts, and dispersion. The training procedure consisted of initial training and \ntransfer learning. The initial training was performed with datasets from nearby surveys. Transfer \nlearning was performed with a small training set with high-fidelity data from the current dataset. \nThe GAN used did not require training set pairing. Only two sets with and without high fidelity \nare needed. Wang et al. (2019) proposed a 1D CycleGAN-based impedance inversion algorithm \nto mitigate the dependence of vanilla CNNs on the amount of labeled seismic data available. \nmanuscript submitted to Reviews of Geophysics \n 19 \n4.6 \nRecurrent neural networks \nIn time-sequenced data processing applications, RNNs use the output of a network as the \ninput of the subsequent process to consider the historical influence. RNNs are used for the \nprediction of new outputs from a sequential input, such as predicting new words from an input \nsentence. The prediction accuracy of LSTM increases with the amount of historical information \nconsidered. In geophysical applications, RNNs are used for predicting the next sample of a time-\nsequenced or spatially sequenced dataset. RNNs are also used for wavefield simulation based on \na time-dependent network form. \n \nIn seismic data processing, Payani et al. (2019) used an RNN to estimate the relationships \namong samples in a seismic trace; they found that 16 bits are needed for lossless representation \ninstead of 32 bits per sample. Chen et al. (2020) applied LSTM for the denoising of \nmagnetotelluric data with prediction data samples in a trace. Li et al. (2019) utilized an RNN to \nconsider the spatial continuity and similarity of adjacent traces in facies analysis. \n \nIn seismic modeling and inversion, Sun et al. (2020) constructed an RNN for wave \nmodeling and inversion, and the network parameters corresponded to the selected velocity model. \nThe structure of an RNN is similar to finite different time evolution. Therefore, optimizing an \nRNN is equivalent to seismic waveform inversion. Experiments with various optimization \nalgorithms, including gradient descent, conjugate gradient, adaptive moment, and limited-\nmemory Broyden-Fletcher-Goldfarb-Shanno algorithms, have been performed. The results have \nindicated that first-order methods perform better than second-order methods. Liu (2020) extends \nRNN for simultaneous inversion of velocity and density. Figure 15 shows the structure of a \nmodified RNN based on the acoustic wave equation used in Liu (2020). The diagram represents \nthe discretized wave equation with a flow chart implemented in an RNN. The inversion process \nin full waveform inversion is the training process of RNN. Fabien-Ouellet and Sarkar (2019) \ncombined three networks: a CNN with a CMP gather input and a semblance output, an RNN for \ndata reduction, and LSTM for velocity decoding. This design was inspired by the information \nflow in semblance analysis. The proposed method works well for 1D layered velocity models. \nWe give an example of using an RNN for simultaneous velocity and density inversion.  \nmanuscript submitted to Reviews of Geophysics \n 20 \n5 \nAI geophysical applications \nWe investigate more AI geophysical applications apart from exploration geophysics in \nthis section. The topics are roughly arranged by the order from the earth’s core to outer space. \n5.1 \nThe earth’s core, mantle and crust \nUnderstanding the interior of the earth is a challenging task since observations are mainly \nlimited on the earth’s surface. The earth is roughly divided into core, mantle and crustal layers \nfrom inside to the surface; however, the detailed structures and properties of the deep side of the \nearth are not clear. Kim et al. (2020) discovered a previously unrecognized ultra-low-velocity \nzone at the core-mantle boundary beneath the Marquesas Islands with a manifold learning \nmethod called the Sequencer. Thousands of diffracted waveforms are arranged in a sequential \norder on the manifold, where half reveal loud signals indicating the ultra-low-velocity zone \n(Figure 16). Shahnas et al. (2018) predicted mantle flow processes by employing support vector \nmachines and training samples from numerical convection models. Shahnas and Pysklywec \n(2020) used deep learning to predict the mantle thermal state of simplified model planets. They \nachieved an accuracy of 99% for both the mean mantle temperature and the mean surface heat \nflux compared to the calculated values. They claimed that deep learning can be employed in \nmore complex mantle states. Cheng et al. (2019) used DNN to map Rayleigh surface wave \nvelocities to crustal thickness in eastern Tibet and the western Yangtze craton. The training \ndataset is generated based on seismic wave forward modeling theories. \n \nThe earth is covered by soil and rocks. Fang et al. (2017) used LSTM to predict historical \nsoil moisture with high fidelity from two recent years of satellite data, showing LSTM’s \npotential for hindcasting, data assimilation, and weather forecasting. Anantrasirichai et al. (2018) \nused a CNN to classify interferometric fringes in wrapped interferograms with no atmospheric \ncorrections for detecting volcanic deformation. \n5.2 \nEarthquake \nThe goal of earthquake data processing is quite different from that of exploration \ngeophysics; therefore, this section focuses on deep learning-based earthquake signal processing. \nmanuscript submitted to Reviews of Geophysics \n 21 \nThe preliminary processing of earthquake signals includes classification to distinguish real \nearthquakes from noise and arrival picking to identify the arrival times of primary and secondary \nwaves. Further applications involve earthquake early warning (EEW) analysis and Earth \ntomography. Deep learning has shown promising results in these applications. \n5.2.1 Classification \n \nMeier et al. (2019) trained five DNNs for seismic signal and noise discrimination. The \ntraining set contained 374,000 earthquake and 946,000  noise records from three channels. Li et \nal. (2018) trained machine learning algorithms on an extensive dataset to discriminate earthquake \nP waves from local impulsive noise. Linville et al. (2019) used an RNN and a CNN to identify \nevents as either quarry blasts or earthquakes. The purpose of volcano seismic detection is similar \nto that of EEW, i.e., determining whether an event is dangerous. Malfante et al. (2018) extracted \n102 features from acoustic and seismic fields. Six classes were considered: long-period events, \nvolcanic tremors, volcano-tectonic events, explosions, hybrid events, and tornados. \n \nWe provide an example of using the wavelet scattering transform (WST) (Mallat 2012) \nand a support vector machine for earthquake classification with a limited number of training \nsamples. The WST involves a cascade of wavelet transforms, a module operator, and an \naveraging operator, corresponding to convolutional filters, a nonlinear operator, and a pooling \noperator, respectively, in a CNN. The critical difference between the WST and a CNN is that the \nfilters are predesigned with the wavelet transform in the WST. In our case, only 100 records \nwere used for training, and 2000 records were used for testing. We obtained a classification \naccuracy as high as 93% with the WST method. Figure 17 shows the architecture of the WST \nalgorithm. \n5.2.2 Arrival picking \n \nWang et al. (2019) proposed PickNet to choose natural seismic arrivals based on a deep \nresidual network. The selected arrivals were used for seismic tomography, and the underground \nstructure of Japan was reconstructed. Ross et al. (2018) trained a CNN for arrival picking and \npolarity classification. The training set contained 19.4 million seismograms. They achieved \nmanuscript submitted to Reviews of Geophysics \n 22 \nremarkably high picking and classification accuracies close to or better than those obtained by \nhuman experts. Zhao et al. (2019) proposed using U-Net for P- and S-phase arrival picking and \nachieved superior results compared to those of the short-time average over the long-time average \n(STA/LTA) method. Zhou et al. (2019) developed a hybrid event detection and phase-picking \nalgorithm with both CNNs and RNNs. \n5.2.3 Earthquake early warning, prediction, and Earth tomography \n \nZhang et al. (2020) used a CNN to locate seismic sources from received waveforms at \nseveral stations. This method worked well for small earthquakes (ML<3.0) with low SNRs, for \nwhich traditional methods fail. The prediction results and errors of earthquake source locations \nare indicated in Figure 18. Ross et al. (2019) generated millions of synthetic sequences to train \nthe PhaseLink network. Associating seismic phases involves classifying seismological records \nfrom the same source into one set. This concept is similar to seismic registration. Zhang and \nCurtis (2020) used variational inference for seismic tomography. This method obtains the mean \nand variance as outputs. Yamaga and Mitsui (2019) analyzed the relationship between a strong \nearthquake and postseismic deformation. The dataset was obtained with the Global Navigation \nSatellite System and was relatively small, with 153 training points and 38 testing points. An \nRNN was used to learn the corresponding relationships, and the results were far more accurate \nthan those of traditional regression methods. Rouet-Leduc et al. (2017) used a random forest \nmodel to predict earthquakes in the laboratory. They used acoustical signals as inputs and \npredicted laboratory fault failures. The machine learning method identified a signal emitted from \nthe fault zone previously thought to be low-amplitude noise. \n5.3 \nWater resources \nWater on Earth has a great impact on ecosystems and natural disasters. The oceans \ncontain most of the water on Earth. Estimating the dynamic parameters of oceans is a \nchallenging and important task. Machine learning can predict ocean parameters, such as \nsubsurface temperatures (Hua et al. 2018) and oxygen levels (Giglio et al. 2018). Hua et al. \n(2018) retrieved anomaly temperatures on the subsurface of the ocean from satellite observations \nbased on the random forest method. They used surface height, temperature, salinity, and wind as \nmanuscript submitted to Reviews of Geophysics \n 23 \ninputs. Giglio et al. (2018) estimated the oxygen level in the Southern Ocean. The authors used \ntemperature and salinity as the input for the random forest model. Wang et al. (2019) used LSTM \nto predict the Loop Current in the Gulf of Mexico. They predicted the Loop Current evolution \nwithin 40 kilometers nine weeks in advance. \nThe sea level is increasing due to global warming. In summer, the meltwater on Arctic \nice will abord more solar radiation and accelerate  ice melt. Wright and Polashenski (2020) \nmeasured the percentage of pond coverage from Arctic sea ice based on a random forest model. \nThey use low-resolution satellite imagery as input to cover a larger spatial range. Similarly, \nBarbat et al. (2019) estimated the size of icebergs in the pan-Antarctic near-coastal zone. To \ncover the whole Antarctic continent, they used a machine learning method that operates on low-\nresolution synthetic aperture radar (SAR) imagery. Liu et al. (2019) used U-Net to predict coastal \ninundation mapping from synthetic aperture radar imagery information, providing a better \nunderstanding of the geospatial and temporal characteristics of coastal flooding. \nIn addition to oceans, water is stored in different forms, such as rivers, lakes, rain and \nsnow. Sun et al. (2019) used a CNN to estimate groundwater storage in India. The CNN was \ntrained to compensate for missing components between the satellite data and NOAH-simulated \ndata. Once trained, the CNN was used to correct the NOAH-simulated data without using the \nsatellite data. Limited by the resolution of the remote sensing imagery, Ling et al. (2019) \nmeasured river widths on a subpixel scale. With a superresolution CNN, they obtained highly \naccurate results. Guillon et al. (2020) predicted river channel types from geospatial data in a \nlarge river basin using a random forest model (Figure 19). They operated on a coarse scale to \ncover a large area and achieved an accuracy of 61% for ten types of channels. Read et al. (2019) \nused an LSTM-based machine learning method to predict the temperature of lake water. They \nachieved an RMSE reduction of 0.5°C relative to traditional methods. \n \nAkbari Asanjan et al. (2018) also proposed an LSTM-based learning method to predict \nrainfall runoff. They predicted hourly runoff for a twenty-four-hour period using observations \nsuch as those of rainfall and runoff. Tang et al. (2018) used a DNN to estimate rain and snow at \nhigh latitudes. They used passive microwave data, infrared data and environmental data from \nmanuscript submitted to Reviews of Geophysics \n 24 \nspaceborne satellite radar data to improve precipitation predictions. A review paper of water \nscience based on deep learning is provided in Shen (2018). \n5.4 \nClimatology and atmospheric science \nAtmospheric science observes and predicts climate, weather and atmospheric \nphenomena. A complete observation of global atmospheric parameters is difficult since the earth \nis extremely large and sensor locations are limited. Kadow et al. (2020) chose a CNN-based \ninpainting algorithm to reconstruct missing values in global climate datasets such as HadCRUT4 \n(Figure 20). Minnis et al. (2016) used a neural network to estimate the optical depth of ice clouds \nfor the prediction of future water paths. Multispectral infrared radiances were used to reduce the \ndependence on solar illumination conditions. \nPrecipitation observed from satellites is on a coarse scale, limiting its resolution in small \nregions. Sharifi et al. (2019) proposed a downscaling algorithm based on machine learning to \nmap satellite observations to a fine spatial resolution. Whitburn et al. (2016) estimated the global \nNH3 level from measurements of an infrared atmospheric sounding interferometer. A neural \nnetwork was used to convert the spectral hyperspectral range index to the NH3 level. Xu et al. \n(2018) proposed using machine learning to reconstruct evapotranspiration in land-atmosphere \ninteractions. The method could overcome the spatial and temporal coverage limitations of in situ \ntechniques and the inaccuracy of modeling approaches at regional scales. Zhu et al. (2019) \npredicted global radiative flux and feedbacks from atmospheric and surface variables based on a \nneural network. \nAir pollution is damaging both the earth’s environment and human health. Li et al. (2017) \nused a DNN to fuse satellite observations and station measurements for estimating ground-level \nPM2.5 levels. Shen et al. (2018) directly estimated the PM2.5 level in Wuhan from the top-of-\natmosphere reflectance measured from satellites based on a DNN. Similarly, Tang et al. (2018) \nused machine learning to estimate the PM10 level from satellite data. They obtained over all \naccuracy R2=0.77. Weather forecasting is a long-standing challenge in atmospheric science. Han \net al. (2017) used machine learning for precipitation nowcasting with radar reflectivity data. This \nsystem is superior to traditional expert systems, such as small and easy-to-maintain systems. \nmanuscript submitted to Reviews of Geophysics \n 25 \nRüttgers et al. (2019) predicted the tracks of typhoons with a GAN based on satellite images. \nThey produced a 6-hour-advance track with an average error of 95.6 km. Jiang et al. (2018) \npredicted both typhoon track and intensity based on a DNN (Figure 21). Flow-dependent \ntyphoon-induced sea surface temperature cooling was estimated by a DNN and used for \nimproving typhoon predictions. \n5.5 \nRemote sensing \nRemote sensing uses sensors in satellites or aerial crafts to image geophysical parameters. \nRemote sensing imagery mainly includes hyperspectral images, SAR images, and optical images. \nImages obtained by hyperspectral sensors have rich spectral information, such that different land \ncover categories can potentially be precisely differentiated. In recent years, numerous works \nhave explored deep learning methods for hyperspectral image classification (Li et al. 2019). \nChen et al. (2016) used a 3-D CNN to extract the effective features of hyperspectral imagery by \nconsidering the spectral-spatial structure simultaneously. The extracted features are useful for \nimage classification and target detection. Mou et al. (2017) first proposed using an RNN for \nhyperspectral image classification. The authors regarded hyperspectral pixels as sequential data \nto explore the relationships among different spectrum channels. \nSAR systems can operate in all-weather and day-and-night conditions to produce high-\nresolution images. Chen et al. (2016) used a CNN for target classification in SAR images. The \nproposed method avoided handcrafted features and achieved an accuracy of 99% in a ten-class \nclassification scheme for ground targets. Zhang et al. (2017) proposed a complex-valued CNN \nfor polarimetric SAR image classification. The authors took both the amplitude and phase \ninformation of complex SAR imagery into consideration. The layers in the CNN were extended \nto process complex-valued inputs. Saha et al. (2020) detect changes in buildings in SAR images \nvia unsupervised learning. A CycleGAN was trained to map SAR images to optical images with \nno requirement of a paired training set. Then, traditional methods were used to detect building \nchanges. \nLarge-scale and high-resolution satellite optical color imagery can be used for precision \nagriculture and urban planning. Maggiori et al. (2017) proposed a CNN-based pixelwise \nmanuscript submitted to Reviews of Geophysics \n 26 \nclassification of large-scale satellite imagery. Inaccurate data were considered by a two-step \ntraining approach. First, the CNN was initialized by numerous inaccurate reference data and then \nrefined on a small amount of correctly labeled data. Cheng et al. (2016) proposed a rotation-\ninvariant CNN for object detection in very high-resolution optical remote sensing images. A \nrotation-invariant layer was introduced by enforcing the training samples before and after \nrotation to share the same features. Jiang et al. (2019) constructed an edge-enhancement GAN \nfor remote sensing image superresolution. The image contours were extracted to remove the \nartifacts and noise in superresolution. \nMore advanced remote sensing techniques, such as LiDAR, have become popular in \nrecent years. Moorthy et al. (2020) proposed a data-driven method for wood and leaf \nclassification from LiDAR point clouds of forests. Wood and leaf classification in forestry and \necology provides a better understanding of radiation transfer between the canopy and \natmosphere. Three tree-based machine learning methods are tested, and they achieved a \nclassification accuracy of up to 94.2%. \n5.6 \nSpace science \nThe planets in outer space have attracted researchers’ attention for a long time. Camporeale \net al. (2017) used machine learning to classify solar wind. The classification results were used to \nobtain the transition probabilities between different solar wind categories for the first time. \nRuhunusiri et al. (2018) developed a DNN to infer solar wind proxies at Mars using sheath \nmeasurements. Seven solar wind parameters were inferred simultaneously using spacecraft \nmeasurements. Montavon et al. (2020) used neural networks to predict the entire evolution (0-4.5 \nbillion years) of the temperature profile of a Mars-like planet. They used a simple MLP with six \nparameters as inputs: the reference viscosity, activation energy, activation volume of diffusion \ncreep, enrichment factor of heat-producing elements in the crust, initial temperature of the \nmantle, and the reference time. \nGlobal space parameter estimation and prediction are long-standing tasks in space science. \nChu et al. (2017) used a neural network to predict short-term and long-term 3-D dynamic \nelectron densities in the inner magnetosphere. This network can obtain the magnetospheric \nmanuscript submitted to Reviews of Geophysics \n 27 \nplasma density at any time and for any location. Chen et al. (2019) reconstructed dynamic total \nelectron content maps with a regularized GAN. Some existing maps were used as references to \ninterpolate missing values in some regions, such as the oceans. The topside electron temperature \nis an important parameter of the ionosphere; however, its measurement is limited by the number \nof incoherent scatter radar stations. Hu et al. (2020) used a DNN to estimate the relationship \nbetween electron temperature and electron density in small regions. Therefore, the global \nelectron density is easily measured and used to predict the global electron temperature. Gowtam \net al. (2019) modeled a global 3-D ionosphere based on a neural network. Nearly two decades of \nmeasurements from the ground and satellites were used to train the neural network. They \nsuccessfully predicted large-scale ionospheric phenomena, such as annual anomalies. \nAn aurora is an astronomical phenomenon commonly observed in polar areas. Auroras are \ncaused by disturbances in the magnetosphere caused by solar wind. Auroral classification is \nimportant for polar and solar wind research. Clausen and Nickisch (2018) proposed classifying \nauroral images with a DNN (Figure 22). The authors used images manually labeled with six \nclasses. Zhong et al. (2020) automatically classified all-sky auroral images with three CNNs. The \nclassification results were used to produce an auroral occurrence distribution. Nichols et al. \n(2019) analyzed Jupiter’s auroral morphology and its response to magnetospheric drivers with \nmachine learning. Yang et al. (2019) used a CycleGAN model to extract key local structures \nfrom all-sky auroral images. The unpaired training set consisted of 2508 auroral images, of \nwhich only 200 images were annotated. \n6 \nRevisiting the relationship between dictionary learning and deep learning \nIn this section, we discuss the relationship between dictionary learning and two specific \ndeep learning methods, the deep image prior (DIP) (Lempitsky et al. 2018) and AE methods. The \ndictionary learning model in equation (1) is written in a specific form with a sparse constraint on \nthe coefficient: \nT\n1\nF\n(\n, )\nE\n\n\n\n\nW C\nW C\nX\nC  \nwhere 𝛼 is a weight parameter. Lempitsky, Vedaldi et al. (2018) proposed DIP, which uses a \nDNN with random inputs for regularization compared to traditional regularization. DIP uses the \nmanuscript submitted to Reviews of Geophysics \n 28 \ntarget data as the only training samples. Mathematically, DIP replaces the dictionary and the \nsparse constraint with a single, deep U-Net-based generator. \nTand sparsity\nF\n( )\nGenerator( ;\n)\nE\n\n\nW\nΘ\nv Θ\nX\n \nAfter obtaining an optimized ˆΘ , another round of forwarding generator propagation will \nproduce a regularized result. In the AE, sparsity is achieved with an encoder that outputs a low-\ndimensional vector. A decoder corresponds to WT in dictionary learning, \nT\n2\nE\nD\nE\nD\n with sparsity\n2\n(\n,\n)\nDecoder Encoder(\n;\n);\ni\ni\ni\nE\n\n\n\n\n\n\n\n\n\n\n\nW\nC\nΘ\nΘ\nX Θ\nΘ\nX\n \nwhere \nE\nΘ  and \nD\nΘ  are the parameters of the encoder and decoder, respectively. Figure 23 shows \ndiagrams of the use of dictionary learning, DIP, and an AE for geophysical signal processing. \n7 \nA deep learning tutorial for beginners \n7.1 \nA coding example of a DnCNN \nThe implementation of deep learning algorithms in geophysical data processing is quite \nsimple based on existing frameworks, such as Caffe, Pytorch, Keras, and TensorFlow. Here, we \nprovide an example of how to use Python and Keras to construct a DnCNN for seismic denoising. \nThe code requires 12 lines for dataset loading, model construction, training, and testing. The \ndataset is preconstructed and includes a clean subset and a noisy subset; the overall dataset \nincludes 12800 samples with size 64 × 64 (available at https://bit.ly/33SyXPO). \n1. import h5py   \n2. from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,ReLU,Subtract   \n3. from tensorflow.keras.models import Model   \n4. ftrain = h5py.File('noise_dataset.h5','r')   \n5. X, Y = ftrain['/X'][()] , ftrain['/Y'][()]   \n6. input = Input(shape=(None,None,1))   \n7. x = Conv2D(64, 3, padding='same',activation='relu')(input)   \n8. for i in range(15):   \nmanuscript submitted to Reviews of Geophysics \n 29 \n9.     x = Conv2D(64, 3, padding='same',use_bias = False)(x)   \n10.     x = ReLU()(BatchNormalization(axis=3, momentum=0.0,epsilon=0.0001)(x))   \n11. x = Conv2D(1, 3, padding='same',use_bias = False)(x)   \n12. model = Model(inputs=input, outputs=Subtract()([input, x]))   \n13. model.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\")   \n14. model.fit(X[:-1000], Y[:-1000], batch_size=32, epochs=50, shuffle=True)   \n15. Y_ = model.predict(X[-1000:]) \nAny appropriate plotting tool can be used for data visualization. The training takes less \nthan one hour on an NVidia 2080ti graphics processing unit. For further implementations, we \nsuggest some public repositories of dictionary learning and deep learning information for \ninterested readers, as listed in Table 2. \n7.2 \nTips for beginners \nWe introduce some practical tips for beginners who want to explore deep learning in \ngeophysics from the perspective of the three most critical steps in deep learning: data generation, \nnetwork construction, and training. \n7.2.1 Data generation \nAs noted by Poulton 2002, “training a feed-forward neural network is approximately 10% \nof the effort involved in an application; deciding on the input and output data coding and creating \ngood training and testing sets is 90% of the work”. In deep learning, we advise that the \npercentages of the effort for network construction and dataset preparation should be \napproximately 40% and 60%. First, most deep learning approaches use an original data set as the \ninput, thus reducing coding decision efforts. Second, a wider variety of network architectures and \nparameters can be used in deep learning compared to those in traditional neural networks. \nOverall, constructing a proper training set plays a more prominent role in deep learning. \nSynthetic datasets can be effectively used in deep learning, which is advantageous since \nrealistic datasets with labels are difficult to obtain. First, to assess the availability of deep \nlearning in a specific geophysical application, using synthetic datasets is the most convenient \nmethod. Second, if a satisfactory result is obtained with synthetic datasets, realistic datasets can \nmanuscript submitted to Reviews of Geophysics \n 30 \nbe used for network analysis via transfer learning with a few annotated realistic datasets. Third, \nif the synthetic datasets are sufficiently complicated, i.e., if the most important factors are \nconsidered when generating the datasets, the trained network may be able to process realistic \ndatasets directly (Wu, Geng et al. 2020 and Wu, Liang et al. 2019). \nA synthetic training set should be diverse. First, we suggest using an existing synthetic \ndataset with an open license, such as SEG open data, instead of generating a dataset. For specific \ntasks, such as FWI, a dataset may need to be generated based on a wave equation. Second, a \ndataset can be modified to increase the degrees of freedom. For example, noise, missing traces, \nand faults can be added to clean datasets depending on the considered task. Third, data \naugmentation can be used to expand a training set, such as via rotation, symmetry, scaling, \ntranslation, and other processes. The goal is to generate extremely large synthetic datasets that \nare as close to realistic datasets as possible. \nTo generate realistic datasets, we suggest using existing methods to generate labels that \nshould then be checked by a human. For example, in first-arrival picking, an automatic picking \nalgorithm is used to preprocess the datasets, and the results are then provided to an expert who \nidentifies the outliers. Such a procedure requires a human to check very notation. We also \nsuggest using activate learning (Yoo and Kweon 2019) to provide a semiautomated labeling \nprocedure. First, all datasets with machine annotation are used to train a DNN, and the samples \nwith high predicted uncertainty are required to be manually annotated.  \n \n7.2.2 Network construction for different tasks \nBeginners are suggested to use a DnCNN or U-Net for testing. DnCNNs are available for \nmost tasks in which the input and output share the same domain, such as denoising, interpolation, \nand attribute analysis. The input size of a DnCNN can vary since there are no pooling layers \ninvolved. However, each output data point is determined by a local field from the input rather \nthan from the entire input set. Additionally, U-Net contains pooling layers, and all input points \nare used to determine an output point. U-Nets are available for tasks even when the inputs and \noutputs are in different domains, such as in FWI. However, the input size of U-Net is fixed once \ntrained. \nmanuscript submitted to Reviews of Geophysics \n 31 \nCombining a CAE and K-means is suggested for unsupervised clustering tasks, such as \nattribute classification. We do not suggest CycleGAN for geophysical tasks since the training \nprocess is extremely time consuming and the results are not stable. An RNN provides a high-\nperformance framework for time-dependent tasks, such as forward wave modeling and FWI. \nRNNs are also used for regression and classification tasks involving temporal or spatial \nsequential datasets, such as in the denoising of a single trace. \nTo adjust the hyperparameters of a DNN and optimization algorithms, we suggest using \nan autoML toolbox, such as Autokeras, instead of manually adjusting the values. The basic \nobjective is to search for the best parameter combination within a given sampling range. Such a \nsearch is exceptionally time consuming, and a random search strategy may accelerate the tuning \nprocess. Moreover, for most applications, the default architecture gives reasonable results. \n7.2.3 Training, validation, and testing \nThe available dataset should be split into three subsets: one training set, one validation set, \nand one test set to optimize the network parameters. The proportions are suggested as 60%, 20%, \nand 20% based on experience. In a classification task, we suggest using one-hot coding in \ntraining. The validation set is used to test the network during training. Then, the model with the \nbest validation accuracy is selected rather than the final trained model. If the validation accuracy \ndoes not improve during training, an early stopping strategy is suggested to avoid wasted time. \nNetwork hyperparameters should be tuned according to the validation accuracy. The validation \nset is used to guide training, and the test set is used to test the model based on unseen datasets; \nhowever, this set should not be used for hyperparameter tuning. \nTwo commonly seen issues during training are as follows: the validation loss is less than \nthe training loss, and the loss is not a number. Intuitively, the training loss should be less than the \nvalidation loss since the model is trained with a training dataset. Some potential reasons for this \nissue are as follows: 1. regularization occurs during training but is ignored during validation, \nsuch as in the dropout layer; 2. the training loss is obtained by averaging the loss of each batch \nduring an iteration, and the validation loss is obtained based on the loss after one iteration; and 3. \nthe validation set may be less complicated than the training set. The potential reasons for NaN \nmanuscript submitted to Reviews of Geophysics \n 32 \nloss are as follows: 1. the learning rate is too high; 2. in an RNN, one should clip the gradient to \navoid gradient explosion and 3. zero is used as a divisor, negative values are used in logarithm, \nor an exponent is assigned too large of a value. \n8 \nFuture directions for deep learning in geophysics \nDeep learning, as an efficient artificial intelligence technique, is expected to discover \ngeophysical concepts and inherit expert knowledge through machine-assisted mathematical \nalgorithms. Despite the success of neural networks, their use as a tool for practical geophysics is \nstill in its infancy. The main problems include a shortage of training samples, low signal-to-noise \nratios, and strong nonlinearity. Among these issues, the critical challenge is the lack of training \nsamples in geophysical applications compared to those in other industries. Several advanced \ndeep learning methods have been proposed related to this challenge, such as semisupervised and \nunsupervised learning, transfer learning, multimodal deep learning, federated learning, and active \nlearning. We suggest that a focused be placed on the subjects below for future research in the \ncoming decade. \n8.1 \nSemisupervised and unsupervised learning \nIn practical geophysical applications, obtaining labels for a large dataset is time \nconsuming and can even be infeasible. Therefore, semisupervised or unsupervised learning is \nrequired to limit the dependence on labels. Dunham et al. (2019) focused on the application of \nsemisupervised learning in a situation in which the available labels were scarce. A self-training-\nbased label propagation method was proposed, and it outperformed supervised learning methods \nin which unlabeled samples were neglected. Semisupervised learning takes advantage of both \nlabeled and unlabeled datasets. The combination of AE and K-means is an efficient unsupervised \nlearning method (He, Cao et al. 2018 and Qian, Yin et al. 2018). An autoencoder is used to learn \nlow-dimensional latent features in an unsupervised way, and then K-means is used to cluster the \nlatent features. \nmanuscript submitted to Reviews of Geophysics \n 33 \n8.2 \nTransfer learning \nUsually, we must train one DNN for a specific dataset and a specific task. For example, a \nDNN may effectively process land data but not marine data, or a DNN may be effective in fault \ndetection but not in facies classification. To increase the reusability of a trained network for \ndifferent datasets or different tasks, transfer learning (Donahue, Jia et al. 2014) is suggested. \nIn transfer learning with different datasets, the optimized parameters for one dataset can \nbe used as initialization values for learning a new network with another dataset; this process is \ncalled fine tuning. Fine tuning is typically much faster and easier than training a network with \nrandomly initialized weights from scratch. In transfer learning involving different tasks, we \nassume that the extracted features should be the same in different tasks. Therefore, the first \nlayers in a model trained for one task are copied to the new model for another task to reduce the \ntraining time. Another benefit of transfer learning is that with a small number of training samples, \nwe can promptly transfer the learned features to a new task or a new dataset. Diagrams of these \ntwo transfer learning methods are shown in Figure 24. Further topics in transfer learning include \nthe relationship between the transferability of features (Yosinski et al. 2014) and the distance \nbetween different tasks and different data sets (Oquab et al. 2014). \n8.3 \nCombination of data-driven and model-driven methods \nTo combine geophysical mechanics and deep learning, can we combine model-driven and \ndata-driven approaches? Intuitively, such a combination will produce a more precise result than \nmodel-driven methods and a more reliable result than data-driven methods. In addition, with an \nadditional physical constraint on deep learning methods, fewer training samples are required to \nobtain a more generalized prediction than those of traditional methods. Zhang et al. (2017) \nproposed learning a denoising prior with a DNN and replacing the denoiser in the iteration \noptimization algorithm, such that different tasks use the same denoiser but different models. \nRaissi et al. (2019) proposed a physical informed neural network (PINN) that combines training \ndata and physical equation constraints for training. Taking wave modeling as an example, the \nwavefield was represented with a DNN,\n\n\n( , )\n, ;\nu x t\nF x t\n\nΘ , such that the acoustic wave equation \nwas: \nmanuscript submitted to Reviews of Geophysics \n 34 \n\n\n\n\n\n\n( , )\n, ;\n2\n2\n, ;\n, ;\nu x t\nF x t\ntt\ntt\nu\nc\nu\nF\nx t\nc\nF x t\n\n\n\n\n\n\nΘ\nΘ\nΘ  \nThe above equation can serve as a constraint while training the DNN. Tartakovsky et al. \n(2020) used PINN to learn parameters and constitutive relationships in subsurface flow problems. \nAnother discussed deep learning technique, DIP, can be applied in different tasks with physical \nmodels. Similar to the idea of DIP, Wu and McMechan (2019) showed that a DNN generator can \nbe added to an FWI framework. First, a U-Net-based generator \n( ;\n)\nF v Θ  with random input v was \nused to approximate a velocity model m with high accuracy. Then, \n( ;\n)\nF\n\nm\nv Θ  was inserted into \nthe FWI objective function: \n2\nFWI\n2\n1\nE\n( )\n( ( ;\n))\n2\nr\nP F\n\n\nΘ\nv Θ\nd\n \nwhere dr is the seismic record and P is the forward wavefield propagator. The gradient of EFWI \nwith respect to network parameters Θ is calculated with the chain rule. U-Net is only used for \nregularizing the velocity model. After training, one forward propagation of the network will \nproduce a regularized result. Data-driven and model-driven methods are not independent; data-\ndriven methods are also used for discovering physical concepts (Iten et al. 2020). \n8.4 \nMultimodal deep learning \nTo improve the resolution of inversion, the joint inversion of data from different sources \nhas been a popular topic in recent years (Garofalo et al. 2015). One of the advantages of DNNs is \nthat they can fuse information from multiple inputs. In multimodal deep learning (Ngiam et al. \n2011, Ramachandram and Taylor 2017), inputs are from different sources, such as seismic data \nand gravity data. Collecting data from different sources can help relieve the bottleneck of a \nlimited number of training samples. In addition, using multimodal datasets can increase the \naccuracy and reliability of deep learning methods. Feng et al. (2020) used data integration to \nforcast streamflow. 23 variables were used intergrated, such as precipitation, solar radiation, and \ntemperature. Figure 25 shows an illustration of multimodal deep learning. \nmanuscript submitted to Reviews of Geophysics \n 35 \n8.5 \nFederated learning \nTo provide a practical training set in deep learning for geophysical applications, \ncollecting available datasets from different institutes or corporations might be a possible solution. \nHowever, data transfer via the internet is time consuming and expensive for large-scale \ngeophysical datasets. In addition, most datasets are protected and cannot be shared. Federated \nlearning was first proposed by Google (Mcmahan et al. 2017, Li et al. 2020) to train a DNN with \nuser data from millions of cellphones without privacy or security issues. The encrypted gradients \nfrom different clients are assembled in a central server, thus avoiding data transfer. The server \nupdates the model and distributes information to all clients (Figure 26). In a simple federated \nlearning setting, the clients and the server share the same network architecture. We give a \npossible example of federated learning in geophysics based on the concept that some \ncorporations do not share the annotations of first arrivals; however, they can benefit from \nfederated learning by training a DNN together for first arrival picking. \n8.6 \nUncertainty estimation \nOne of the remaining questions associated with applying deep learning in geophysics is \nrelated to whether the results of deep learning-based model-driven methods with a solid \ntheoretical foundation can be trusted. One trial in drilling may cost millions of dollars. What if a \nneural network can report high confidence in a prediction? Deep learning with uncertainty \nanalysis was proposed to assess reliability, such as through Markov chain Monte Carlo (MCMC) \n(de Figueiredo et al. 2019), variational inference (Subedar et al. 2019), and Monte Carlo dropout \n(Gal and Ghahramani 2016) methods. For example, in Monte Carlo dropout, dropout layers are \nadded to each original layer to simulate a Bernoulli distribution. With multiple realizations of \ndropout, the results are collected, and the variance is computed as the uncertainty. \nGrana et al. (2020) assessed the classification accuracy and uncertainty of RNN and \nMCMC methods. The RNN method yielded higher accuracy but relatively high uncertainty. The \nMCMC method provided similar accuracy and was robust to uncertainty through the use of prior \nspatial correlation models. In the RNN, the uncertainty was obtained through multiple runs of the \nsame procedure with different training subsets, but the results were similar in each case. Maiti \nmanuscript submitted to Reviews of Geophysics \n 36 \nand Tiwari (2010) used a Bayesian neural network (BNN) to predict the boundaries of lithofacies. \nBNNs provide low uncertainty compared to traditional deep learning methods. Cao et al. (2020) \nproposed a sequence of fast seismic acquisitions for dispersion curve extraction and inversion for \n3-D seismic models with uncertainty estimates using pretrained mixture density networks. \n8.7 \nActive learning \nTo train a high-precision model using a small amount of labeled data, active learning is \nproposed to imitate the self-learning ability of human beings (Yoo and Kweon 2019). An active \nlearning model selects the most useful data based on a sampling strategy for manual annotation \nand adds this data to the training set; then, the updated dataset is used for the next round of \ntraining (Figure 27). One of the sampling strategies is based on the uncertainty principle, i.e., the \nsamples with high uncertainty are selected. Taking fault detection as an example, if a trained \nnetwork is not sure whether a fault exists at a given location, we can annotate the fault manually \nand add the sample to the training set. \n9 \nSummary \nData-driven methods, especially deep learning methods, have created both opportunities \nand challenges in geophysical fields. Pioneer researchers have provided a basis for deep learning \nin geophysics with promising results; more advanced deep learning technologies and more \npractical problems must now be explored. To close this paper, we summarize a roadmap for \napplying deep learning in different geophysical tasks based on a three-level approach. \n \nTraditional methods are time consuming and require intensive human labor and \nexpert knowledge, such as in first-arrival selection and velocity selection in \nexplorational geophysics. \n \nTraditional methods have difficulties and bottlenecks. For example, geophysical \ninversion requires good initial values and high accuracy modeling and suffers from \nlocal minimization. \n \nTraditional methods cannot handle some cases, such as multimodal data fusion and \ninversion. \nmanuscript submitted to Reviews of Geophysics \n 37 \nWith the development of new artificial intelligence models beyond deep learning and \nadvances in research into the infinite possibilities of applying deep learning in geophysics, we \ncan expect intelligent and automatic discovering of unknown geophysical principles soon. \nGlossary \nAE: Autoencoder; an ANN with the same inputs and outputs. \nAI: Artificial Intelligence; Machines are taught to think as humans. \nANN: Artificial neural network; a computing system inspired by biological neural networks \nthat constitute animal brains. \nAurora: A natural light display in the earth's sky; disturbances in the magnetosphere caused \nby solar wind. \nBNN: Bayesian neural network; the network parameters are random variables instead of \nregular variables. \nCAE: Convolutional autoencoder; an AE with shared weights. \nCNN: Convolutional neural network; a DNN with shared weights. \nCompressive sensing: A sampling technique for reconstructing a signal with a sample rate \nlower than the requirement of Shannon sampling theory. The mathematical principle of \ncompressive sensing is that an underdetermined linear system can be solved with a sparse \nprior on the signal. \nDDTF: Data-driven tight frame; A dictionary learning method using a tight frame constraint \nfor the dictionary. \nDeblending: In seismic exploration, several explosion sources are shot very close in time to \nimprove efficiency. Then, the seismic waves from different sources are blended. The \nrecorded dataset first needs to be deblended before further processing. \nDeduction: The principles used for the prediction of unknown facts. \nDictionary: A set of vectors used to represent signals as a linear combination. \nDIP: Deep image prior; the architecture of a DNN is used as a prior constraint for an image. \nDL: Deep learning; a machine learning technology based on a deep neural network. \nDnCNN: Denoised convolutional neural network. \nDNN: Deep neural network; an ANN with many layers between the input and output layers. \nmanuscript submitted to Reviews of Geophysics \n 38 \nDS: Double sparsity; the data are represented with a sparse coefficient matrix multiplied by \nan adaptive dictionary. The adaptive dictionary is represented by a sparse coefficient matrix \nmultiplied by a fixed dictionary. \nEEW: Earthquake early warning; earthquake alerts are sent to people seconds to tens of \nseconds in advance of when shaking waves are expected to arrive. EEW systems make use \nof the speed difference between propagation of the primary and secondary waves. \nEvent: In exploration geophysics, a seismic event means reflected waves with the same \nphase. In seismology, an event means a happened earthquake. \nFacies: A seismic facies unit is a mapped, three-dimensional seismic unit composed of \ngroups of reflections whose parameters differ from adjacent facies units. \nFault: a discontinuity in a volume of rock across which there has been significant \ndisplacement as a result of rock-mass movement. \nFCN: Fully convolutional network; an FCN is a network that contains no fully connected \nlayers. Fully connected layers do not share weights. \nFWI: Full waveform inversion; full waveform information is used to obtain subsurface \nparameters. FWI is achieved based on the wave equation and inversion theory. \nGAN: Generative adversarial network; GANs are used to generate fake images. A GAN \ncontains a generative network and a discriminative network. The generative network tries to \nproduce a nearly real image. The discriminative network tries to distinguish whether the \ninput image is real or generated. Therefore, such a game will eventually allow the generative \nnetwork to produce fake images that the discriminative network cannot distinguish from real \nimages. \nGraphics processing unit (GPU): A parallel computing device. GPUs are widely used for \ntraining neural works in deep learning. \nHadCRUT4: Temperature records from Hadley Centre (sea surface temperature) and the \nClimatic Research Unit (land surface air temperature). \nInduction: Principles are inferred from observations. \nIonosphere: The ionized part of the earth's upper atmosphere. \nK-means: A classical clustering algorithm, where K is the number of clusters. \nK-SVD: A dictionary learning method using SVD for dictionary updating. \nKullback-Leibler divergence: A measurement of the distance between two probability \ndistributions \nLiDAR: A measurement device using laser light to produce high-resolution images. \nmanuscript submitted to Reviews of Geophysics \n 39 \nLSTM: long short-term memory; LSTM considers how much historical information is \nforgotten or remembered with adaptive switches. \nMagnetosphere: Range of the magnetic field surrounding an astronomical object where \ncharged particles are affected. \nML: Earthquake local magnitude; a method for measuring earthquake scale. \nMLP: Multilayer perceptron; A feedforward ANN in which only adjacent  layers are \nconnected. \nMOD: Method of optimal directions; a dictionary learning method using orthogonal \nmatching pursuit for sparse coding. \nPatch: In dictionary learning, an image is divided into many patches (blocks) that are the \nsame size as the atoms in a dictionary. \nPINN: Physical informed neural network; A physical equation is used to constrain the neural \nnetwork. \nPM: Particulate matter. PM10 are coarse particles with a diameter of 10 micrometers or less; \nPM2.5 are fine particles with a diameter of 2.5 micrometers or less. \nR2: Coefficient of determination; A statistical parameter range from zero to one indicates \nhow strong the linear relationship is between two variables. \nResNet: Residual neural network; ResNets contain skip connections to jump over some \nlayers. The output of a residual block is the residual between the input and the direct output. \nRNN: Recurrent neural network; in time-sequenced data processing applications, RNNs use \nthe output of a network as the input of the subsequent process to consider the historical \ncontext. \nSAR: Synthetic aperture radar; the motion of a radar antenna over a target is treated as an \nantenna with a large aperture. The larger the aperture is, the higher the image resolution will \nbe. \nSequencer: A machine learning-based ordering algorithm in which similar waveforms are \nclose to each other. \nSolar wind: A stream of charged particles released from the upper atmosphere of the Sun. \nSparse coding: Input data are represented in the form of a linear combination of a dictionary \nwhere the coefficients are sparse. \nSparsity: The number of nonzero values in a vector. \nSVD: Singular value decomposition; a matrix factorization method. A=USV, where U and V \nare two orthogonal matrices, S is a diagonal matrix whose elements are the singular values of \nmanuscript submitted to Reviews of Geophysics \n 40 \nA. SVD is used for dimension reduction by removing the smaller singular values. SVD is \nalso used for recommendation systems and natural language processing. \nTight frame: A frame provides a redundant, stable way of representing a signal, similar to \ndictionary. A tight frame is a frame with the perfect reconstruction property; i.e., WTW=I. \nTomography: Inversion of the subsurface velocity based on travel time information. \nU-Net: U-shaped network; U-Nets have U-shaped structures and skip connections. The skip \nconnections bring low-level features to high levels. \nVelocity analysis: Analysis of a  velocity distribution along the depth based on signal \nsemblance. \nWave equation: A partial differential equation that controls wave propagation. \nWST: Wavelet scattering transform; a transform involves a cascade of wavelet transforms, a \nmodule operator, and an averaging operator. \nAcknowledgments \nThe work was supported in part by the National Key Research and Development Program \nof China under grant nos. 2017YFB0202902 and 2018YFC1503705 and NSFC under grant nos. \n41625017 and 41804102. We thank Society of Exploration Geophysicists, Nature Research, and \nAmerican Association for the Advancement of Science for allowing us to reuse the original \nfigures from their journals.  \nData Availability Statement \nData were not used, nor created for this research. \nReferences  \nAbma, R. and N. Kabir (2006). 3D interpolation of irregular data with a POCS algorithm. Geophysics. 71(6): 91-97. \nAharon, M., M. Elad and A. Bruckstein (2006). K-SVD: An algorithm for designing overcomplete dictionaries for \nsparse representation. IEEE Transactions on Signal Processing. 54(11): 4311-4322. \nAraya-Polo, M., J. Jennings, A. Adler and T. Dahlke (2018). Deep-learning tomography. The Leading Edge. 37(1): \n58-66. \nBeckouche, S. and J. Ma (2014). Simultaneous dictionary learning and denoising for seismic data. Geophysics. \n79(3): A27-A31. \nCai, J., H. Ji, Z. Shen and G. Ye (2014). Data-driven tight frame construction and image denoising. Applied and \nComputational Harmonic Analysis. 37(1): 89-105. \nCao, R., S. Earp, S. A. L. de Ridder, A. Curtis and E. Galetti (2020). Near-real-time near-surface 3D seismic \nvelocity and uncertainty models by wavefield gradiometry and neural network inversion of ambient seismic noise. \nGeophysics. 85(1): KS13-KS27. \nmanuscript submitted to Reviews of Geophysics \n 41 \nChen, H., R. Guo, J. Liu, Y. Wang and R. Lin (2020). Magnetotelluric data denoising with recurrent neural network. \nSEG 2019 Workshop: Mathematical Geophysics: Traditional vs Learning. \nClausen, L. B. N. and H. Nickisch (2018). Automatic classification of auroral images from the oslo auroral themis \n(OATH) data set using machine learning. Journal of Geophysical Research-Space Physics. 123(7): 5640-5647. \nCortes, C. and V. Vapnik (1995). Support-vector networks. Machine learning. 20(3): 273-297. \nCreswell, A., T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta and A. A. Bharath (2018). Generative \nadversarial networks: An overview. IEEE Signal Processing Magazine. 35(1): 53-65. \nDas, V., A. Pollack, U. Wollner and T. Mukerji (2019). Convolutional neural network for seismic impedance \ninversion. Geophysics. 84(6): R869-R880. \nde Figueiredo, L. P., D. Grana, M. Roisenberg and B. B. Rodrigues (2019). Gaussian mixture markov chain Monte \nCarlo method for linear seismic inversion. Geophysics. 84(3): R463-R476. \nDe Lima, R. P. and K. J. Marfurt (2018). Principal component analysis and K-means analysis of airborne gamma-\nray spectrometry surveys. SEG Technical Program Expanded Abstracts: 2277-2281. \nDonahue, J., Y. Jia, O. Vinyals, J. Hoffman and T. Darrell (2014). Decaf: A deep convolutional activation feature \nfor generic visual recognition. International Conference on Machine Learning: 647-655. \nDong, C., C. C. Loy, K. He and X. Tang (2014). Learning a deep convolutional network for image super-resolution. \nEuropean Conference on Computer Vision: 184-199. \nDragomiretskiy, K. and D. Zosso (2014). Variational mode decomposition. IEEE Transactions on Signal Processing. \n62(3): 531-544. \nDuan, Y., X. Zheng, L. Hu and L. Sun (2019). Seismic facies analysis based on deep convolutional embedded \nclustering. Geophysics. 84(6): IM87-IM97. \nDunham, M. W., A. Malcolm and J. Kim Welford (2019). Improved well-log classification using semisupervised \nlabel propagation and self-training, with comparisons to popular supervised algorithms. Geophysics. 85(1): O1-O15. \nEngan, K., S. O. Aase and J. H. Husoy (2002). Method of optimal directions for frame design. 1999 IEEE \nInternational Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. \nNo.99CH36258). \nEvgeniou, T., M. Pontil and T. Poggio (2000). Regularization networks and support vector machines. Advances in \nComputational Mathematics. 13(1): 1-50. \nFabien-Ouellet, G. and R. Sarkar (2019). Seismic velocity estimation: A deep recurrent neural-network approach. \nGeophysics. 85(1): U21-U29. \nFeng, D., K. Fang and C. Shen (2020). Enhancing streamflow forecast and extracting insights using long-short term \nmemory networks with data integration at continental scales. Water Resources Research. \nGal, Y. and Z. Ghahramani (2016). Dropout as a Bayesian approximation: Representing model uncertainty in deep \nlearning. International Conference on Machine Learning. \nGalvis, I. S., Y. Villa, C. Duarte, D. A. Sierra and W. Agudelo (2017). Seismic attribute selection and clustering to \ndetect and classify surface waves in multicomponent seismic data by using K-means algorithm. Geophysics. 36(3): \n239-248. \nGao, Z., Z. Pan, J. Gao and Z. Xu (2019). Building long-wavelength velocity for salt structure using stochastic full \nwaveform inversion with deep autoencoder based model reduction. SEG Technical Program Expanded Abstracts: \n1680-1684. \nGarofalo, F., G. Sauvin, L. V. Socco and I. Lecomte (2015). Joint inversion of seismic and electric data applied to \n2D media. Geophysics. 80(4): EN93-EN104. \nGoodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville and Y. Bengio (2014). \nGenerative adversarial nets. Neural Information Processing Systems: 2672-2680. \nGrana, D., L. Azevedo and M. Liu (2020). A comparison of deep machine learning and Monte Carlo methods for \nfacies classification from seismic data. Geophysics. 85(4): WA41-WA52. \nGuillon, H., C. F. Byrne, B. A. Lane, S. S. Solis and G. B. Pasternack (2020). Machine learning predicts reach-scale \nchannel types from coarse-scale geospatial data in a large river basin. Water Resources Research. 56(3). \nHartigan, J. A. and M. A. Wong (1979). A k-means clustering algorithm. Journal of The Royal Statistical Society \nSeries C-applied Statistics. 28(1): 100-108. \nHe, K., X. Zhang, S. Ren and J. Sun (2016). Deep residual learning for image recognition. IEEE Conference on \nComputer Vision and Pattern Recognition: 770-778. \nHe, Y., J. Cao, Y. Lu, Y. Gan and S. Lv (2018). Shale seismic facies recognition technology based on sparse \nautoencoder. International Geophysical Conference. \nHelmy, T., A. Fatai and K. Faisal (2010). Hybrid computational models for the characterization of oil and gas \nreservoirs. Expert Systems with Applications. 37(7): 5353-5363. \nmanuscript submitted to Reviews of Geophysics \n 42 \nHerrmann, F. J. and G. Hennenfent (2008). Non-parametric seismic data recovery with curvelet frames. Geophysical \nJournal International. 173(1): 233-248. \nHochreiter, S. and J. Schmidhuber (1997). Long short-term memory. Neural Comput. 9(8): 1735-1780. \nHu, L., X. Zheng, Y. Duan, X. Yan, Y. Hu and X. Zhang (2019). First-arrival picking with a U-net convolutional \nnetwork. Geophysics. 84(6): U45-U57. \nHuang, K., J. You, K. Chen, H. Lai and A. Don (2006). Neural network for parameters determination and seismic \npattern detection. SEG Technical Program Expanded Abstracts: 2285-2289. \nIten, R., T. Metger, H. Wilming, L. Del Rio and R. Renner (2020). Discovering physical concepts with neural \nnetworks. Phys Rev Lett. 124(1): 010508. \nJia, Y. and J. Ma (2017). What can machine learning do for seismic data processing? An interpolation application. \nGeophysics. 82(3): V163-V177. \nJiang, G.-q., J. Xu and J. Wei (2018). A deep learning algorithm of neural network for the parameterization of \ntyphoon-ocean feedback in typhoon forecast models. Geophysical Research Letters. 45(8): 3706-3716. \nKadow, C., D. M. Hall and U. Ulbrich (2020). Artificial intelligence reconstructs missing climate information. \nNature Geoscience. 13(6): 408-413. \nKim, D., V. Lekic, B. Menard, D. Baron and M. Taghizadeh-Popp (2020). Sequencing seismograms: A panoptic \nview of scattering in the core-mantle boundary region. Science. 368(6496): 1223-1228. \nKrizhevsky, A., I. Sutskever and G. E. Hinton (2017). Imagenet classification with deep convolutional neural \nnetworks. Communications of the Acm. 60(6): 84-90. \nLeCun, Y., B. Boser, J. S. Denker, D. Henderson and L. D. Jackel (1997). Handwritten digit recognition with a \nback-propagation network. Advances in Neural Information Processing Systems. 2(2): 396-404. \nLempitsky, V., A. Vedaldi and D. Ulyanov (2018). Deep image prior. IEEE Conference on Computer Vision and \nPattern Recognition: 9446-9454. \nLi, L., Y. Lin, X. Zhang, H. Liang, W. Xiong and S. Zhan (2019). Convolutional recurrent neural networks based \nwaveform classification in seismic facies analysis. SEG Technical Program Expanded Abstracts: 2599-2603. \nLi, T., A. K. Sahu, A. Talwalkar and V. Smith (2020). Federated learning: Challenges, methods, and future \ndirections. IEEE Signal Processing Magazine. 37(3): 50-60. \nLiang, J., J. Ma and X. Zhang (2014). Seismic data restoration via data-driven tight frame. Geophysics. 79(3): V65-\nV74. \nLim, J. S. (2005). Reservoir properties determination using fuzzy logic and neural networks from well data in \noffshore korea. Journal of Petroleum Science and Engineering. 49(3-4): 182-192. \nLipari, V., F. Picetti, P. Bestagini and S. Tubaro (2018). A generative adversarial network for seismic imaging \napplications. SEG Technical Program Expanded Abstracts: 2231-2235. \nLiu, L. and J. Ma (2019). Structured graph dictionary learning and application on the seismic denoising. IEEE \nTransactions on Geoscience and Remote Sensing. 57(4): 1883-1893. \nLiu, L., J. Ma and G. Plonka (2018). Sparse graph-regularized dictionary learning for suppressing random seismic \nnoise. Geophysics. 83(3): V215-V231. \nLiu, L., G. Plonka and J. Ma (2017). Seismic data interpolation and denoising by learning a tensor tight frame. \nInverse Problems. 33(10): 105011. \nLiu, S. (2020). Multi-parameter full waveform inversions based on recurrent neural networks. Dissertation for the \nMaster Degree in Science, Harbin Institute of Technology. \nMaiti, S. and R. K. Tiwari (2010). Neural network modeling and an uncertainty analysis in Bayesian framework: A \ncase study from the KTB borehole site. Journal of Geophysical Research-Solid Earth. 115: 1-28. \nMandelli, S., F. Borra, V. Lipari, P. Bestagini, A. Sarti and S. Tubaro (2018). Seismic data interpolation through \nconvolutional autoencoder. SEG Technical Program Expanded Abstracts: 4101-4105. \nMcCann, M. T., K. H. Jin and M. Unser (2017). Convolutional neural networks for inverse problems in imaging: A \nreview. IEEE Signal Processing Magazine. 34(6): 85-95. \nMcmahan, H. B., E. Moore, D. Ramage, S. Hampson and B. A. Y. Arcas (2017). Communication-efficient learning \nof deep networks from decentralized data. International Conference on Artificial Intelligence and Statistics. \nNakayama, S., G. Blacquiere and T. Ishiyama (2019). Automated survey design for blended acquisition with \nirregular spatial sampling via the integration of a metaheuristic and deep learning. Geophysics. 84(4): P47-P60. \nNazari Siahsar, M. A., S. Gholtashi, A. R. Kahoo, W. Chen and Y. Chen (2017). Data-driven multitask sparse \ndictionary learning for noise attenuation of 3D seismic data. Geophysics. 82(6): V385-V396. \nNgiam, J., A. Khosla, M. Kim, J. Nam, H. Lee and A. Y. Ng (2011). Multimodal deep learning. International \nConference on Machine Learning. \nmanuscript submitted to Reviews of Geophysics \n 43 \nOquab, M., L. Bottou, I. Laptev and J. Sivic (2014). Learning and transferring mid-level image representations using \nconvolutional neural networks. IEEE Conference on Computer Vision and Pattern Recognition. \nOvcharenko, O., V. Kazei, M. Kalita, D. Peter and T. Alkhalifah (2019). Deep learning for low-frequency \nextrapolation from multioffset seismic data. Geophysics. 84(6): R989-R1001. \nPark, M. J. and M. D. Sacchi (2019). Automatic velocity analysis using convolutional neural network and transfer \nlearning. Geophysics. 85(1): V33-V43. \nPayani, A., F. Fekri, G. Alregib, M. Mohandes and M. Deriche (2019). Compression of seismic signals via recurrent \nneural networks: Lossy and lossless algorithms. SEG Technical Program Expanded Abstracts 2019: 4082-4086. \nPoulton, M. M. (2002). Neural networks as an intelligence amplification tool: A review of applications. Geophysics. \n67(3): 979-993. \nQi, J., B. Zhang, B. Lyu and K. Marfurt (2020). Seismic attribute selection for machine-learning-based facies \nanalysis. Geophysics. 85(2): O17-O35. \nQian, F., M. Yin, X. Liu, Y. Wang, C. Lu and G. Hu (2018). Unsupervised seismic facies analysis via deep \nconvolutional autoencoders. Geophysics. 83(3): A39-A43. \nRaissi, M., P. Perdikaris and G. E. Karniadakis (2019). Physics-informed neural networks: A deep learning \nframework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of \nComputational Physics. 378: 686-707. \nRamachandram, D. and G. W. Taylor (2017). Deep multimodal learning: A survey on recent advances and trends. \nIEEE Signal Processing Magazine. 34(6): 96-108. \nRonneberger, O., P. Fischer and T. Brox (2015). U-net: Convolutional networks for biomedical image segmentation. \nMedical Image Computing and Computer Assisted Intervention: 234-241. \nRubinstein, R., M. Zibulevsky and M. Elad (2010). Double sparsity: Learning sparse dictionaries for sparse signal \napproximation. IEEE Transactions on Signal Processing. 58(3): 1553-1564. \nSi, X., Y. Yuan, F. Ping, Y. Zheng and L. Feng (2020). Ground roll attenuation based on conditional and cycle \ngenerative adversarial networks. SEG 2019 Workshop: Mathematical Geophysics: Traditional vs Learning. \nSiahkoohi, A., M. Louboutin and F. J. Herrmann (2019). The importance of transfer learning in seismic modeling \nand imaging. Geophysics. 84(6): A47-A52. \nSimonyan, K. and A. Zisserman (2015). Very deep convolutional networks for large-scale image recognition. \nInternational Conference on Learning Representations. \nSong, C., Z. Liu, Y. Wang, F. Xu, X. Li and G. Hu (2018). Adaptive phase K-means algorithm for waveform \nclassification. Exploration Geophysics. 49(2): 213-219. \nSubedar, M., R. Krishnan, P. L. Meyer, O. Tickoo and J. Huang (2019). Uncertainty-aware audiovisual activity \nrecognition using deep Bayesian variational inference. International Conference on Computer Vision: 6300-6309. \nSun, J., Z. Niu, K. A. Innanen, J. Li and D. O. Trad (2020). A theory-guided deep-learning formulation and \noptimization of seismic waveform inversion. Geophysics. 85(2): R87-R99. \nSun, J., S. Slang, T. Elboth, T. Larsen Greiner, S. McDonald and L.-J. Gelius (2020). A convolutional neural \nnetwork approach to deblending seismic data. Geophysics. 85(4): WA13-WA26. \nTartakovsky, A. M., C. O. Marrero, P. Perdikaris, G. D. Tartakovsky and D. Barajas-Solano (2020). Physics-\ninformed deep neural networks for learning parameters and constitutive relationships in subsurface flow problems. \nWater Resources Research. 56(5). \nWaheed, U. B., S. Alzahrani and S. M. Hanafy (2019). Machine learning algorithms for automatic velocity picking: \nK-means vs. DBSCAN. SEG Technical Program Expanded Abstracts: 5510-5114. \nWang, B., N. Zhang, W. Lu and J. Wang (2019). Deep-learning-based seismic data interpolation: A preliminary \nresult. Geophysics. 84(1): V11-V20. \nWang, T., Z. Zhang and Y. Li (2019). Earthquakegen: Earthquake generator using generative adversarial networks. \nSEG Technical Program Expanded Abstracts: 2674-2678. \nWang, W. and J. Ma (2020). Velocity model building in a crosswell acquisition geometry with image-trained \nartificial neural network. geophysics. 85(2): U31-U46. \nWang, X. and J. Ma (2019). Adaptive dictionary learning for blind seismic data denoising. IEEE Geoscience and \nRemote Sensing Letters: 1-5. \nWang, X., B. Wen and J. Ma (2019). Denoising with weak signal preservation by group-sparsity transform learning. \nGeophysics. 84(6): V351-V368. \nWang, Y., Q. Ge, W. Lu and X. Yan (2019). Seismic impedance inversion based on cycle-consistent generative \nadversarial network. SEG Technical Program Expanded Abstracts: 2498-2502. \nWang, Y., B. Wang, N. Tu and J. Geng (2020). Seismic trace interpolation for irregularly spatial sampled data using \nconvolutional autoencoder. Geophysics. 85(2): V119-V130. \nmanuscript submitted to Reviews of Geophysics \n 44 \nWu, H., B. Zhang, F. Li and N. Liu (2019). Semiautomatic first-arrival picking of microseismic events by using the \npixel-wise convolutional image segmentation method. Geophysics. 84(3): V143-V155. \nWu, H., B. Zhang, T. Lin, D. Cao and Y. Lou (2019). Semiautomated seismic horizon interpretation using the \nencoder-decoder convolutional neural network. Geophysics. 84(6): B403-B417. \nWu, H., B. Zhang, T. Lin, F. Li and N. Liu (2019). White noise attenuation of seismic trace by integrating \nvariational mode decomposition with convolutional neural network. Geophysics. 84(5): V307-V317. \nWu, X., Z. Geng, Y. Shi, N. Pham, S. Fomel and G. Caumon (2020). Building realistic structure models to train \nconvolutional neural networks for seismic structural interpretation. Geophysics. 85(4): WA27-WA39. \nWu, X., L. Liang, Y. Shi and S. Fomel (2019). FaultSeg3D: Using synthetic data sets to train an end-to-end \nconvolutional neural network for 3D seismic fault segmentation. Geophysics. 84(3): IM35-IM45. \nWu, Y. and G. A. McMechan (2019). Parametric convolutional neural network-domain full-waveform inversion. \nGeophysics. 84(6): R881-R896. \nYang, F. and J. Ma (2019). Deep-learning inversion: A next-generation seismic velocity model building method. \nGeophysics. 84(4): R585-R584. \nYoo, D. and I. S. Kweon (2019). Learning loss for active learning. IEEE Conference on Computer Vision and \nPattern Recognition. \nYosinski, J., J. Clune, Y. Bengio and H. Lipson (2014). How transferable are features in deep neural networks. \nNeural Information Processing Systems. \nYou, N., Y. E. Li and A. Cheng (2020). Shale anisotropy model building based on deep neural networks. Journal of \nGeophysical Research: Solid Earth. 125(2): e2019JB019042. \nYu, S., J. Ma and S. Osher (2016). Monte Carlo data-driven tight frame for seismic data recovery. Geophysics. 81(4): \nV327-V340. \nYu, S., J. Ma and W. Wang (2019). Deep learning for denoising. Geophysics. 84(6): V333-V350. \nYu, S., J. Ma, X. Zhang and M. Sacchi (2015). Interpolation and denoising of high-dimensional seismic data by \nlearning a tight frame. Geophysics. 80(5): V119-V132. \nZhang, C., C. Frogner, M. Araya-Polo and D. Hohl (2014). Machine-learning based automated fault detection in \nseismic traces. 76th EAGE Conference and Exhibition 2014. 2014(1): 1-5. \nZhang, H., W. Wang, X. Wang, W. Chen and Z. Zhao (2019). An implementation of the seismic resolution \nenhancing network based on GAN. SEG Technical Program Expanded Abstracts 2019: 2478-2482. \nZhang, H., X. Yang and J. Ma (2020). Can learning from natural image denoising be used for seismic data \ninterpolation? Geophysics. 85(4): WA115-WA136. \nZhang, K., W. Zuo, Y. Chen, D. Meng and L. Zhang (2017). Beyond a Gaussian denoiser: Residual learning of deep \nCNN for image denoising. IEEE Transactions on Image Processing. 26(7): 3142-3155. \nZhang, K., W. Zuo, S. Gu and L. Zhang (2017). Learning deep CNN denoiser prior for image restoration. IEEE \nConference on Computer Vision and Pattern Recognition: 2808-2817. \nZhang, X., J. Zhang, C. Yuan, S. Liu, Z. Chen and W. Li (2020). Locating induced earthquakes with a network of \nseismic stations in oklahoma via a deep learning method. Scientific Reports. 10(1): 1941. \nZhang, Z. and T. Alkhalifah (2019). Regularized elastic full-waveform inversion using deep learning. Geophysics. \n84(5): R741-R751. \nZhu, J., T. Park, P. Isola and A. A. Efros (2017). Unpaired image-to-image translation using cycle-consistent \nadversarial networks. International Conference on Computer Vision: 2242-2251. \nZu, S., J. Cao, S. Qu and Y. Chen (2020). Iterative deblending for simultaneous source data using the deep neural \nnetwork. Geophysics. 85(2): V131-V141. \n \n \n \nmanuscript submitted to Reviews of Geophysics \n 45 \nTables \nTable 1 Data-driven tasks in Geophysics \n \n \n \nmanuscript submitted to Reviews of Geophysics \n 46 \nTable 2 Open sources of dictionary learning and deep learning for geophysical applications \n \n \nmanuscript submitted to Reviews of Geophysics \n 47 \nFigures \n \nFigure 1 An illustration of model-driven and data-driven methods. On the left are the research topics in geophysics \nranging from the Earth’s core to the outer space. One the right are the observation means used in model technology. \nIn the middle are examples of model-driven and data-driven methods. In model-driven methods, the principles of \ngeophysical phenomena are induced from a large amount of observed data based on physical causality, then the \nmodels are used to deduct the geophyscial phanomena in the future or in the past. In data-driven methods, the \ncomputer first induct a regression or classification model without considering physical causality. Then, this model \nwill perform tasks such as prediction, classification, and recognition on incoming datasets. \n \n \nmanuscript submitted to Reviews of Geophysics \n 48 \n \n(a) Model-driven methods \n \n(b) Data-driven methods \nFigure 2. Examples of model-driven and data-driven methods in exploration geophysics. (a) Model-driven methods. \nIn random denoising tasks, the curvelet denoising method (Herrmann and Hennenfent 2008) assumes that the signal \nis sparse under curvelet transform, and a matching method is used for denoising. In velocity inversion tasks, full-\nwaveform inversion based on the wave equation is used for forward and adjoint modeling in the optimization \nalgorithm. In fault interpretation tasks, faults are picked by interpreters. (b) Data-driven methods. The mentioned \ntasks are treated as regression problems that are optimized with neural networks. Different tasks may require \ndifferent neural network architectures. \nmanuscript submitted to Reviews of Geophysics \n 49 \n \nFigure 3. Conceptual map of the data-driven methods in exploration geophysics included in this paper\nmanuscript submitted to Review of Geophysics \n 50 \n \nFigure 4. Illustration of the K-means method. Left: A randomly generated dataset with 300 samples (N=300) \nand two features (M=2). Right: The classification result (K=2).  \n \n \nmanuscript submitted to Review of Geophysics \n 51 \n \nFigure 5. An illustration of DDTF. The dictionary is initialized with a spline framelet. After training based on a \npost-stack seismic dataset, the trained dictionary exhibits apparent structures. \n \n \nmanuscript submitted to Review of Geophysics \n 52 \n \n \n \nFigure 6. Comparison of the learned features in dictionary learning and deep learning. Dictionary learning obtains \nsingle-level decomposed features. Deep learning captures multilevel decomposed features. \n \n \nmanuscript submitted to Review of Geophysics \n 53 \n \n \n \n(a) Vanilla regression CNN \n(b) Vanilla classification CNN \n(c) CAE \n \n \n \n(d) U-Net \n(e) GAN \n(f) RNN \nFigure 7. Sketches of DNNs. We omit the details of the layers and maintain the shape of each \nnetwork architecture. The blue lines indicate inputs, and the orange lines indicate outputs, the length \nof which represents the data dimension. The green lines indicate intermedia connections. \nConvolutional layers usually have the same size as the input and output. Pooling layers will reduce \nthe data set size. Unpooling layers will expand the data set size. (a) In regression tasks, such as \ndenoising or interpolation, the output often has the same dimension as the input. (b) In classification \ntasks, the outputs are labels with a relatively small dimension. (c) The dimension of the latent feature \nspace in the CAE is lower than that of the data space. (d) Skip connections are used to bring the low-\nlevel features to a high level in U-Net. (e) In a GAN, low-dimensional random vectors are used to \ngenerate a sample from the generator, and then the sample is classified as true or false by the \ndiscriminator. (f) In an RNN, the output of the network is used as input in a cycle.  \n \n \n \nmanuscript submitted to Review of Geophysics \n 54 \n \nFigure 8. Deep learning for scattered ground-roll attenuation. On the left is the original noisy \ndataset. On the right is the denoised dataset. The scattered ground roll marked by the green arrows \nare removed. \n \n \n \nmanuscript submitted to Review of Geophysics \n 55 \n \n \n \n(a) \n(b) \n(c) \nFigure 9. The training set and seismic interpolation result (Zhang, Yang et al. 2020). (a) A subset of the natural \nimage dataset. The natural image dataset was used to train a network for seismic data interpolation. (b) An under-\nsampled seismic record. (c) The interpolated record corresponding to (b). The regions 1.6-1.88 s and 1.0-1.375 km \nare enlarged at the top-right corner.  \n \n \n \n \nmanuscript submitted to Review of Geophysics \n 56 \n \nFigure 10. Converting a three-channel color image into a velocity model (Wang and Ma 2020). (a)-\n(c) are original color image, gray scale image, and corresponding velocity model. (d) is the seismic \nrecord generated from a cross-well geometry on (c). \n \n \n \nmanuscript submitted to Review of Geophysics \n 57 \n \nFigure 11. Predicting the velocity model with U-Net from raw seismological data (Yang and Ma \n2019). The columns indicate different velocity models. From top to bottom are the ground truth \nvelocity models, generated seismic records from one shot, and the predicted velocity models.  \n \n \n \n \nmanuscript submitted to Review of Geophysics \n 58 \n \nFigure 12. (a) A post-stack dataset. (b) Prediction result of (a). (c) A synthetic dataset (Wu, Geng et \nal. 2020). \n \n \n \nmanuscript submitted to Review of Geophysics \n 59 \n \nFigure 13. Velocity picking based on U-Net. The inputs are seismological data on the left. The \noutputs are the picking positions on the right. GT means ground truth. PD_REG and PD_CLS \nrepresent the predictions of the regression network and classification network, respectively. \n \n \n \nmanuscript submitted to Review of Geophysics \n 60 \n \n \nFigure 14. Phase picking based on U-Net. The inputs are seismological data. The outputs are zeros \nabove the first arrival in the green area, ones below the first arrival in the yellow area, and twos for \nthe first arrival on the blue line. The green line indicates the predicted first arrival. This experiment \nwas performed based on the modified code from https://github.com/DaloroAT/first_break_picking. \n  \n \n \nmanuscript submitted to Review of Geophysics \n 61 \n \nFigure 15. Modified RNN based on the acoustic wave equation for wave modeling (Liu 2020). The \ndiagram represents the discretized wave equation implemented in an RNN. The auto-differential \nmechanics of a DNN help to efficiently optimize the velocity and density. \n \n \n \nmanuscript submitted to Review of Geophysics \n 62 \n \nFigure 16 (a) Sequencer ordering enables the identification of a substantial (~40% of all waveforms) \nsubpopulation of Sdiff postcursors (red box). (b) Stacks of postcursor amplitude relative to main Sdiff \narrival averaged in 1° bins (Kim et al. 2020).  \n \n \n \n \nmanuscript submitted to Review of Geophysics \n 63 \n \nFigure 17. (a) The architecture of WST. Unlike in a CNN, the outputs of WST are combined with the \noutputs of each layer. Then, the outputs of WST serve as features for a classifier.  \n \n \n \nmanuscript submitted to Review of Geophysics \n 64 \n \n \n \nFigure 18. Locating earthquake sources with deep learning. The black triangles are stations. Left: the \nblue dots are the actual locations. Right: the red circles are the predicted locations. The radius of a \ncircle represents the predicted epicenter error (Zhang et al. 2020). \n  \n \n \nmanuscript submitted to Review of Geophysics \n 65 \n \nFigure 19. Machine learning predictions of stream channel type for the Sacramento Basin in \nCalifornia, USA (Guillon et al. 2020). \n \n \n \nmanuscript submitted to Review of Geophysics \n 66 \n \nFigure 20 AI models reconstruct temperature anomalies with many missing values (Kadow et al. 2020). \n \n \nmanuscript submitted to Review of Geophysics \n 67 \nFigure 21 Typhoon track prediction with (a) shallow learning algorithm and (b) deep learning algorithm (Jiang et al. \n2018). \n \n \n \nmanuscript submitted to Review of Geophysics \n 68 \n \nFigure 22 The bottom panel shows a keogram from auroral data collected on 21 January 2006 at Rankin Inlet. The \nkeogram consists of single column from the auroral images at different time. The middle panel shows the \nprobabilities for the six categories as predicted by the ridge classiffier trained with the entire training dataset. At the \ntop are auroral images at different times. (Clausen and Nickisch 2018) \n \n \nmanuscript submitted to Review of Geophysics \n 69 \n \n(a) Diagram of dictionary learning \n \n(b) Diagram of DIP \n \n(c) Diagram of an autoencoder \nFigure 23. Comparison of dictionary learning and deep learning. (a) Dictionary learning has a \nshallow structure and is unsupervised and linear. A sparsity constraint is placed on the coefficients. \n(b) In DIP, the network architecture constrains the produced image. (c) An autoencoder has a deep \nstructure, an extensive training set, and nonlinear operators. \n \n \n \nmanuscript submitted to Review of Geophysics \n 70 \n \n \n(a) Datasets transfer learning \n(b) Tasks transfer learning \nFigure 24. Diagrams of transfer learning. (a) Transfer learning between different datasets. The \nparameters of one trained model can be moved to another model as initialization conditions. (b) \nTransfer learning between different tasks. The first layers of one trained model can be copied to \nanother model. \n \n \n \nmanuscript submitted to Review of Geophysics \n 71 \n \n \n \nFigure 25. An illustration of multimodal deep learning \n \n \n \nmanuscript submitted to Review of Geophysics \n 72 \n \n \n \nFigure 26. Federated learning. The clients train the DNN with local datasets and uploads the model gradient to the \nserver. The server aggregates the gradients and updates the global model. Then, the updated model is distributed to \nall the local clients. Many rounds of training are performed until the model meets a certain accuracy requirement. \n \n \n \nmanuscript submitted to Review of Geophysics \n 73 \n \n \n \nFigure 27. An illustration of active learning. We choose samples with high uncertainty and manually annotate them \nto serve as training samples.  \n \n \n",
  "categories": [
    "physics.geo-ph",
    "cs.LG",
    "eess.IV"
  ],
  "published": "2020-07-13",
  "updated": "2020-09-29"
}