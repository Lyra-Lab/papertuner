{
  "id": "http://arxiv.org/abs/2007.09637v1",
  "title": "Survey on Deep Learning-based Kuzushiji Recognition",
  "authors": [
    "Kazuya Ueki",
    "Tomoka Kojima"
  ],
  "abstract": "Owing to the overwhelming accuracy of the deep learning method demonstrated\nat the 2012 image classification competition, deep learning has been\nsuccessfully applied to a variety of other tasks. The high-precision detection\nand recognition of Kuzushiji, a Japanese cursive script used for transcribing\nhistorical documents, has been made possible through the use of deep learning.\nIn recent years, competitions on Kuzushiji recognition have been held, and many\nresearchers have proposed various recognition methods. This study examines\nrecent research trends, current problems, and future prospects in Kuzushiji\nrecognition using deep learning.",
  "text": "arXiv:2007.09637v1  [cs.CV]  19 Jul 2020\nSurvey on Deep Learning-based\nKuzushiji Recognition\nKazuya Ueki\nSchool of Information Science\nMeisei University\nEmail: kazuya.ueki@meisei-u.ac.jp\nTomoka Kojima\nSchool of Information Science\nMeisei University\nEmail: 18j5061@stu.meisei-u.ac.jp\nAbstract—Owing to the overwhelming accuracy of the deep\nlearning method demonstrated at the 2012 image classiﬁcation\ncompetition, deep learning has been successfully applied to a\nvariety of other tasks. The high-precision detection and recogni-\ntion of Kuzushiji, a Japanese cursive script used for transcribing\nhistorical documents, has been made possible through the use\nof deep learning. In recent years, competitions on Kuzushiji\nrecognition have been held, and many researchers have proposed\nvarious recognition methods. This study examines recent research\ntrends, current problems, and future prospects in Kuzushiji\nrecognition using deep learning.\nI. INTRODUCTION\nKuzushiji has been commonly used in Japan for more than\na thousand years. However, since the 1900s, schools have\nno longer been teaching Kuzushiji, and only a few thousand\npeople in Japan can currently read and understand it. Hiragana\ncharacters1 have a root Kanji2 called a Jibo3, leading to various\nshapes for a single character; training is required to read char-\nacters that differ from modern Hiragana. For this reason, many\nresearchers have been working on Kuzushiji recognition using\nmachine learning techniques. Recently, with the advent of deep\nlearning, research on Kuzushiji recognition has accelerated and\nthe accuracy of the methods has signiﬁcantly improved. In this\npaper, we present a survey and analysis of recent methods of\nKuzushiji recognition based on deep learning.\nII. REPRESENTATIVE RESEARCH ON KUZUSHIJI\nRECOGNITION\nMany studies on Kuzushiji recognition were conducted prior\nto the introduction of deep learning. The “Historical Character\nRecognition Project” [1], which was initiated in 1999, reported\nthe development of a system to support the transcription of\nancient documents. In this project, to develop a historical\ndocument research support system, the authors studied a\n1Hiragana is one of the three different character sets used in Japanese\nwriting. Each Hiragana character represents a particular syllable. There are\n46 basic characters.\n2Kanji is another one of the three character sets used in the Japanese\nlanguage. Along with syllabaries, Kanji is made up of ideographic characters,\nand each letter symbolizes a speciﬁc meaning. Most Kanji characters were\nimported from China, although some were developed in Japan. Although\nthere are approximately 50,000 Kanji characters, only approximately 2,500\nare actually used in daily life in Japan.\n3A Jibo is a root Kanji character of Hiragana. For example, the character\n“あ” is derived from different Jibos including “安” and “阿.”\ncharacter database, corpus, character segmentation, character\nrecognition, intellectual transcription support system, and a\ndigital dictionary. Speciﬁcally, they developed a computerized\nhistorical character dictionary using stroke information [2]\nas well as Japanese off-line hand-written optical character\nrecognition (OCR) technology, and implemented a Kuzushiji\nrecognition system for 67,739 categories by combining on-\nline and off-line recognition methods [3]. Other methods,\nsuch as [4][5], which is a recognition method using self-\norganizing maps, and [6], which is a recognition method using\na neocognitron, have also been proposed.\nSince the introduction of deep learning, further research\non Kuzushiji recognition has become increasingly active,\nand various methods have been proposed. During the early\nintroductory stage of deep learning, most recognition methods\n[7][8] were based on approximately 50 different Hiragana\nimages with recognition rates of 75% to 90%. There were\nalso studies in which more than 1,000 characters including\nHiragana, and Katakana4 and Kanji, were recognized [9],\nalong with the results of character recognition in documents\nof the Taiwan Governor’s Ofﬁce, which dealt with more than\n3,000 characters [10]. In these studies, the problems of large\nnumbers of classes, an unbalanced number of data between\nclasses, and a variation of characters were solved through a\ndata augmentation commonly used in deep learning training.\nIn addition, a network that outputs a three-character string\nhas also been proposed as a method for recognizing con-\nsecutive characters [11]. This method uses single-character\nand binary classiﬁers to distinguish between characters; the\ncharacter strings are then recognized using bidirectional long\nshort-term memory (BLSTM). The authors reported that the\nrecognition rate of a single character was approximately 92%;\nhowever, the recognition rate of three characters was only\napproximately 76%. Similarly, a method for recognizing a\nstring of three consecutive characters using a sliding window\nand BLSTM was proposed [12]. The authors used the tendency\nin which the maximum output probability of a neural network\nis not particularly high for a misaligned character image but\nis high for an accurately aligned image, and increased the\n4In the same way as Hiragana, Katakana is one of the three different\ncharacter sets used in Japanese. Katakana is also a phonetic syllabary, in\nwhich each letter represents the sound of a syllable. There are also 46 basic\ncharacters.\nrecognition rate to 86% by integrating multiple results. In\naddition, a deep learning method for recognizing a series of\nKuzushiji phrases using an image from “The Tale of Genji”\nwas proposed [13]. An end-to-end method with an attention\nmechanism was applied to recognize consecutive Kuzushiji\ncharacters within phrases. This method can recognize phrases\nwritten in Hiragana (47 different characters) with 78.92%\naccuracy, and phrases containing both Kanji (63 different\ncharacters) and Hiragana with 59.80% accuracy.\nIn recent years, research on Kuzushiji recognition has\nbecome increasingly active since the Kuzushiji dataset ﬁrst\nbecame publicly available [14]. With the development of this\ndatabase, a PRMU algorithm contest described in IV-A and a\nKaggle competition introduced in IV-B were held, and many\nresearchers have started to work on Kuzushiji recognition. The\npreparation, progress, and results of a Kuzushiji recognition\ncompetition, knowledge obtained from the competition, and\nthe value of utilizing machine learning competitions have also\nbeen reported [15][16]. In these reports, the results of the\nKuzushiji recognition competition showed that existing object\ndetection algorithms such as a Faster R-CNN [17] and cascade\nR-CNN [18] are also effective for Kuzushiji detection. At\nthe forefront of Kuzushiji recognition, end-to-end approaches\nfor actual transcriptions are becoming the mainstream. As a\nrepresentative method, an end-to-end method, KuroNet, was\nproposed to recognize whole pages of text using the U-Net\narchitecture [19][20]. The authors demonstrated that KuroNet\ncan handle long-range contexts, large vocabularies, and non-\nstandardized character layouts by predicting the location and\nidentity of all characters given a page of text without any\npreprocessing. To recognize multiple lines of historical doc-\numents, a document reading system inspired by human eye\nmovements was proposed, and the results of evaluations of the\nPRMU algorithm contest database described in IV-A [21] and\nthe Kaggle competition database described in IV-B [22] were\nreported. In addition, a two-dimensional context box proposal\nnetwork used to detect Kuzushiji in historical documents\nwas proposed [23]. The authors employed VGG16 to extract\nfeatures from an input image and BLSTM [24] for exploring\nthe vertical and horizontal dimensions, and then predicted\nthe bounding boxes from the output of the two-dimensional\ncontext.\nIn a study on the practical aspects of transcription, a new\ntype of OCR technology was proposed to reduce the labor of\na high-load transcription [25]. This technology is not a fully\nautomated process but aims to save labor by dividing tasks\nbetween experts and non-experts and applying an automatic\nprocessing. The authors stated that a translation can be made\nquickly and accurately by not aiming at a decoding accuracy\nof 100% using only the OCR of automatic processing, but\nby leaving the characters with a low degree of certainty as\na “〓(geta)” and entrusting the evaluation to experts in the\nfollowing process. A method for automatically determining\nwhich characters should be held for evaluation using a machine\nlearning technique was also proposed [26][27]. This method\ncan automatically identify difﬁcult-to-recognize characters or\nFig. 1. Example images of Kuzushiji dataset\ncharacters that were not used during training based on the\nconﬁdence level obtained from the neural network.\nA study on an interface of a Kuzushiji recognition system\nalso introduced a system that can operate on a Raspberry Pi\nwithout problems and with almost the same processing time\nand accuracy as in previous studies; there was also no need\nfor a high-performance computer [28].\nAs another area of focus, a framework for assisting humans\nin reading Japanese historical manuscripts, formulated as a\nconstraint satisfaction problem, and a system for transcribing\nKuzushiji and its graphical user interface have also been\nintroduced [29][30].\nAn interactive system was also been proposed to assist in\nthe transcription of digitized Japanese historical woodblock-\nprinted books [31]. This system includes a layout analysis,\ncharacter segmentation, transcription, and the generation of\na character image database. The procedures for applying the\nsystem consist of two major phases. During the ﬁrst phase, the\nsystem automatically produces provisional character segmen-\ntation data, and users interactively edit and transcribe the data\ninto text data for storage in the character image database. Dur-\ning the second phase, the system conducts automatic character\nsegmentation and transcription using the database generated\nduring the ﬁrst phase. By repeating the ﬁrst and second phases\nwith a variety of materials, the contents of the character\nimage database can be enhanced and the performance of the\nsystem in terms of character segmentation and transcription\nwill increase.\nIII. DATASETS\nA. Kuzushiji Dataset\nThe Kuzushiji dataset5 consists of 6,151 pages of image data\nof 44 classical books held by National Institute of Japanese\nLiterature and published by ROIS-DS Center for Open Data\nin the Humanities (CODH). Example images included in\nthe Kuzushiji dataset are shown in Fig. 1. This Kuzushiji\n5http://codh.rois.ac.jp/char-shape/\nFig. 2.\n“Electronic Kuzushiji dictionary database” and “wooden tablet\ndatabase” collaborative search function\ndatabase comprises bounding boxes for characters, including\n4,328 character types and 1,086,326 characters. There is a\nlarge bias in the number of data depending on the class: The\nclass with the largest number of images is “の” (character\ncode, U+306B), which has 41,293 images; many classes have\nextremely few images, and 790 classes only have 1 image.\nKuzushiji-MNIST, Kuzushiji-49, and Kuzushiji-Kanji were\nalso provided as a subset of the above dataset6 [32]. These\ndatasets not only serve as a benchmark for advanced classiﬁ-\ncation algorithms, they can also be used in more creative areas\nsuch as generative modeling, adversarial examples, few-shot\nlearning, transfer learning, and domain adaptation. Kuzushiji-\nMNIST has 70,000 28×28 grayscale images with 10 Hiragana\ncharacter classes. Kuzushiji-49 is an imbalanced dataset that\nhas 49 classes (28×28 grayscale, 270,912 images) containing\n48 Hiragana characters and one Hiragana iteration mark.\nKuzushiji-Kanji is an imbalanced dataset with a total of 3,832\nKanji characters (64×64 grayscale, 140,426 images), ranging\nfrom 1,766 examples to only a single example per class.\nB. Electronic Kuzushiji Dictionary Database\nThe Electronic Kuzushiji Dictionary Database7 is a database\nof glyphs and fonts collected from ancient documents and\nrecords in the Historiographical Institute of the University of\nTokyo; it contains approximately 6,000 different characters,\n2,600 different vocabularies, and 280,000 character image\nﬁles. This database contains character forms from various\nperiods from the Nara period (8th century) to the Edo period\n(18th century).\nC. Wooden Tablet Database\nThe Nara National Research Institute for Cultural Properties\nhas developed and published a database that collects images\nof glyphs and fonts allowing the recognition of inscriptions\n6https://github.com/rois-codh/kmnist\n7https://wwwap.hi.u-tokyo.ac.jp/ships/shipscontroller\nwritten on wooden blocks excavated from underground sites.\nThe database contains approximately 24,000 characters, 1,500\ncharacter types, and 35,000 character images. It contains\ninformation from the Asuka-Nara period, which is not often\nincluded in the Electronic Kuzushiji Dictionary Database\ndescribed in III-B. For this reason, the “Electronic Kuzushiji\nDictionary Database” and “wooden tablet database” collabo-\nrative search function8 shown in Fig. 2, which integrates the\ntwo databases, was provided for convenience.\nIV. BENCHMARKS\nA. PRMU Algorithm Contest\nThe tasks used in the 2017 and 2019 PRMU Algorithm\nContest9 required recognizing Kuzushiji contained in the des-\nignated region of an image of a classical Japanese book and\noutputting the Unicode of each character. In this contest, a\ntotal of 46 types of Hiragana characters that do not include\nKatakana or Kanji needed to be recognized. The 2017 contest\nhad three tasks for three different difﬁculty levels, levels 1, 2,\nand 3, depending on the number of characters contained in the\nrectangle. The participants were required to recognize single\nsegmented characters in level 1, three consecutive characters\nin the vertical direction in level 2, and three or more characters\nin the vertical and horizontal directions in level 3. In the 2019\ncontest, the task was to recognize three consecutive characters\nin the vertical direction as in level 2 in 2017.\nHerein, we introduce the tasks of the 2017 contest and the\nmethods used by the best performing teams [33]. The dataset\napplied in 2017 was constructed from 2,222 scanned pages of\n15 historical books provided by CODH. One of the 15 books\nwas used as the test data because it contained many fragmented\nand noisy patterns, as well as various backgrounds. The dataset\nfor level 1 consisted of 228,334 single Hiragana images, and\ntest data of 47,950 were selected from the dataset. To improve\nthe accuracy, multiple models were trained and a voting-based\nensemble method was employed to integrate the results of\nmany different models. The level 2 dataset consists of 92,813\nimages of three consecutive characters, and a test set of 13,648\nimages was selected from the dataset. A combined architecture\nof a CNN , BLSTM, and connectionist temporal classiﬁcation\n(CTC) was employed [34], and a sequence error rate (SER)\nof 31.60% was achieved. In the level 3 dataset, there are\n12,583 images from which a test set of 1,340 images were\nselected. The authors employed a combination of vertical line\nsegmentation and multiple line concatenation before applying\na deep convolutional recurrent network. The SER is 82.57%,\nand there is still a signiﬁcant need for improvement.\nAnother report evaluated a historical document recognition\nsystem inspired by human eye movements using the dataset\nfrom the 2017 PRMU algorithm contest [21]. This system\nincludes two modules: a CNN for feature extraction and an\nLSTM decoder with an attention model for generating the\n8http://clioapi.hi.u-tokyo.ac.jp/ships/ZClient/W34/z srch.php\n9A contest held annually by the Pattern Recognition and Media Under-\nstanding (PRMU) for the purpose of revitalizing research group activities.\ntarget characters. The authors achieved SERs of 9.87% and\n53.81% at levels 2 and 3 of the dataset, respectively.\nNow, we introduce the methods of the ﬁrst through third\nplace teams in the 2019 contest. The dataset used in 2019 was\nalso composed of 48 Hiragana character images cropped from\nthe books in the Kuzushiji dataset provided by CODH. Single-\ncharacter images and images containing three consecutive\ncharacters in the vertical direction were provided as data\nfor training. A total of 388,146 single-character images were\napplied, and 119,997 images with three consecutive characters\nwere used for training and 16,387 were used for testing.\nThe ﬁrst place team adopted a method dividing the charac-\nters into three images through a preprocessing, inputting each\nimage into the CNN to extract the features, and recognizing\nthree consecutive characters using two layers of a bidirectional\ngated recurrent unit (GRU) [35]10. They achieved a rate of\n90.63% through a combination of three backbone models (SE-\nResNeXt, DenseNet, and Inception-v4) . As data augmentation\nmethods, in addition to a random crop and a random shift, the\ndivision position was randomly shifted up and down slightly\nduring training for robustness to the division position.\nThe second place team used a CNN, BLSTM, and CTC\nin the ﬁrst step and output three characters by majority\nvoting during the second step. In the ﬁrst step, a CNN (six\nlayers) was used to extract the features, BLSTM (two layers)\nwas used to convert the features into sequential data, and\nCTC was used to output the text. To improve the accuracy,\ndata augmentation such as a random rotation, random zoom,\nparallel shift, random noise, and random erasing [36] were\nused.\nThe third place team employed an algorithm that applies\nmulti-label image classiﬁcation. In the ﬁrst step, a multi-label\nestimation was conducted using an image classiﬁcation model,\nand three characters were estimated in no particular order.\nDuring the second step, Grad-CAM [37] identiﬁed and aligned\nthe region of interest for each candidate character and output\nthree consecutive characters.\nB. Kaggle Competition\nA Kaggle competition called “Opening the door to a thou-\nsand years of Japanese culture” was held from July 19 to\nOctober 14, 2019. Whereas the PRMU algorithm competition\ninvolved a recognition of single-character images or images\ncontaining a few characters, the Kaggle competition tackled\nthe more challenging task of automatically detecting the posi-\ntion of characters on a page of a classical book and correctly\nrecognizing the type of characters. Of the 44 books in the\nKuzushiji dataset described in III-A, 28 books released before\nthe competition were used as training data, and 15 books\nreleased after the competition were used as the test data11. The\nF value, which is the harmonic mean of the precision (the per-\ncentage of correct responses among the characters output by\nthe system) and the recall (the percentage of correct responses\n10https://github.com/katsura-jp/alcon23\n11One book was eliminated from the competition.\namong the characters in the test data), was used for evaluation.\nFor approximately 3 months, many international researchers\nworked on this competition and achieved a practical level of\naccuracy (F value of greater than 0.9). There were two typical\nmethods, namely, a single-stage method that applies detection\nand recognition simultaneously, and a two-stage method that\nconducts character detection and recognition in stages. Most\nof the top teams adopted the two-stage method. The two-\nstage method applied detectors such as a Faster R-CNN and\nCenterNet [38] to detect character regions, and models such as\nResNet [39] to recognize individual characters. As shown in\nTable I, the method descriptions and implementations of the\ntop teams were published. We now describe the methods of\nthe top winning teams.\nThe Chinese team took ﬁrst place using a straightforward\nmethod with a Cascade R-CNN. Cascade R-CNN can improve\nthe accuracy of object detection by connecting a Faster R-CNN\nin multiple stages. High-Resolution Net (HRNet) [40] was\nused as the backbone network of the Cascade R-CNN. HRNet\nutilizes multi-resolution feature maps and can retain high-\nresolution feature representations without a loss. The team\nwas able to achieve a high accuracy while maintaining greater\nsimplicity than the methods used by the other teams because\nthe latest techniques were applied, including a Cascade R-\nCNN and HRNet, which showed the highest levels of accuracy.\nThe second place team, from Russia, used a two-stage\nmethod of detection and classiﬁcation. A Faster R-CNN with\na ResNet152 backbone was used for detection. ResNet and\nResNeXt [41] were used to estimate the type of characters.\nVarious efforts have been made to improve the accuracy of\nrecognition. For example, because books of test data are\ndifferent from books of training data, pseudo labels have also\nbeen used to adapt to the environment of an unknown book\n(author). Moreover, a new character class, called a detection\nerror character class, was added to eliminate the detection\nerror at the classiﬁcation stage. Finally, the gradient boosting\nmethods LightGBM [42] and XGBoost [43] were also used to\nfurther improve the accuracy.\nThe third place team, from Japan, adopted a two-stage\nmethod of character detection using a Faster R-CNN and\ncharacter-type classiﬁcation using EfﬁcientNet [44]. The team\nemployed several types of data augmentation to increase the\nnumber of training data, as shown below. First, because color\nand grayscale images were mixed in the training data, they\nused a random grayscale, which randomly converts images\ninto monochrome during training. In addition, the training\ndata were augmented using techniques such as combining\nmultiple images by applying mixup [45] and random image\ncropping and patching (RICAP) [46], and adding some noise\nto the image by random erasing. Because Furigana12 is not a\nrecognition target, post-processing such as the creation of a\nfalse positive predictor is used for its removal.\nThe fourth place team, from China, adopted a different\n12Furigana is made up of phonetic symbols occasionally written next to\ndifﬁcult or rare Kanji to show their pronunciation\nTABLE I\nEXPLANATION AND PROGRAM IMPLEMENTATION OF THE WINNING METHOD IN THE KAGGLE COMPETITION\nRank\nF value\nURL\n1\n0.950\nExplanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112788\nImplementation: https://github.com/tascj/kaggle-kuzushiji-recognition\n2\n0.950\nExplanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112712\nImplementation: https://github.com/lopuhin/kaggle-kuzushiji-2019\n3\n0.944\nExplanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/113049\nImplementation: https://github.com/knjcode/kaggle-kuzushiji-recognition-2019\n4\n0.942\nExplanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/114764\nImplementation: https://github.com/linhuifj/kaggle-kuzushiji-recognition\n5\n0.940\nExplanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112771\nImplementation: https://github.com/see–/kuzushiji-recognition\n7\n0.934\nExplanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112899\nImplementation: https://www.kaggle.com/kmat2019/centernet-keypoint-detector\n8\n0.920\nExplanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/113419\nImplementation: https://github.com/t-hanya/kuzushiji-recognition\n9\n0.910\nExplanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/112807\nImplementation: https://github.com/mv-lab/kuzushiji-recognition\n13\n0.901\nExplanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/113518\nImplementation: https://github.com/jday96314/Kuzushiji\n15\n0.900\nExplanation: https://www.kaggle.com/c/kuzushiji-recognition/discussion/114120\nImplementation: https://github.com/statsu1990/kuzushiji-recognition\nmethod than the other groups; they used a hybrid task cascade\n(HTC) [47] for character detection followed by a connectionist\ntext proposal network (CTPN) [48] for line-by-line character\nrecognition. One-line images were resized to 32×800, and\nthen fed to a model that recognizes a single line of text. A\nconvolutional recurrent neural network (CRNN) model was\nused for line recognition and had a structure with 200 outputs.\nA six-gram language model was trained using the KenLM\ntoolkit [49], and a beam search was applied to decode the CTC\noutput of the model. The positional accuracy of the CTC out-\nput was improved using multitask learning [50], which added\nan attention loss. For the data augmentation methods, contrast\nlimited adaptive histogram equalization, random brightness,\nrandom contrast, random scale, and random distortion were\nused. A dropout and cutout were applied as the regularization\nmethods. Although many other teams reported that language\nmodels are ineffective, this team reported a slight increase in\naccuracy.\nThe ﬁfth place team, from Germany, reported that the one-\nstage method using CenterNet is consistently more accurate,\nunlike the two-stage method of the other top teams. Although,\nit is common for object detection tasks to deal with approxi-\nmately 80 classes of objects, such as the MS COCO dataset,\nthis team showed that detection can be achieved without\nproblems even if the number of classes is large. They made\nseveral modiﬁcations to CenterNet, such as creating it from\nscratch, avoiding the use of an HourglassNet, and using the\nResNet50 and ResNet101 structures with a feature pyramid\nnetwork. They also reported that the use of high-resolution\nimages such as 1536×1536 did not show any improvement.\nThe seventh place team, from Japan, adopted a two-stage\nmethod of character region detection using CenterNet followed\nby ResNet-based character recognition. For the data augmen-\ntation, ﬂipping, cropping, brightness, and contrast were used\nto create the detector, and cropping, brightness, contrast, and\na pseudo-label were used to create a classiﬁer. The authors at-\ntempted to build their own model instead of using a predeﬁned\napproach. In general, although predeﬁned models have been\ndesigned to provide local features at a high resolution and a\nwide ﬁeld of view at a low resolution, it was not necessary to\nprovide a wide ﬁeld of view at a signiﬁcantly low resolution\nfor the Kuzushiji detection task. The team reported that the\nsuccess of this model was due to the fact that they built their\nown task-speciﬁc model with a high degree of freedom.\nThe eighth place team, also from Japan, adopted a two-stage\nmethod of character detection using CenterNet (ResNet18,\nUNet) and character classiﬁcation using MobileNetV3 [51].\nThe character detection was made by maintaining the aspect\nratio to prevent a separation of characters, and bounding box\nvoting [52] was used to reduce the number of undetected\ncharacters. Because the appearance of the characters varies sig-\nniﬁcantly depending on the book applied, the team augmented\nthe data, such as through a grid distortion, elastic transform,\nand random erasing, to increase the visual variation.\nThe nineth place team adopted a simple two-stage method\nusing CenterNet with HourglassNet as the backbone for de-\ntection and ResNet18 for classiﬁcation. Because the detection\naccuracy was insufﬁcient, multiple results were combined.\nFor character classiﬁcation, the team treated characters with\nless than 5 data as pseudo-labels, NaN, because 276 of the\ncharacters were not in the training data.\nThe 13th place team used a Faster R-CNN to detect and\nclassify the character regions, but did not share the network\nfor the detection and classiﬁcation. A wide ResNet-34 was\nused as the backbone of the network for both character\ndetection and character recognition. The team tried to apply\ndeeper networks, such as ResNet-50 and ResNet-101, but\nreported that the wider and shallower networks achieve a better\naccuracy. They also reported that the use of color images\nslightly improves the accuracy of detection but worsens the\naccuracy of the character recognition. Although they attempted\nto use a language model to correct incorrect labels, it was\nineffective.\nThe 15th place team, from Japan, used a two-stage method\nof detection and classiﬁcation. First, they applied a grayscale\nconversion, Gaussian ﬁlter, gamma correction, and Ben’s pre-\nprocessing of the images. A two-stage CenterNet with Hour-\nglassNet as a backbone was applied for character detection. In\nthe ﬁrst stage of CenterNet, the bounding box was estimated,\nand the outside of the outermost bounding box was removed.\nIn the second stage of CenterNet, the bounding box was\nestimated again, and the results of the ﬁrst and second stages\nwere combined. Random erasing, horizontal movement, and\na brightness adjustment were used to augment the data when\ncreating the detection model. For character classiﬁcation, the\nresults of three types of ResNet-based models also trained\nusing pseudo labels were combined and output. Horizontal\nmovement, rotation, zoom, and random erasing were used for\ndata augmentation when creating the character classiﬁcation\nmodel.\nSummarizing the methods of the top teams explained here,\nwe can see that it is important to make use of recently\nproposed detection and classiﬁcation models. However, it is\nimpossible to determine whether a one-stage or two-stage\nmethod achieves better results. For the detection method,\nmodels such as YOLO [53], which are frequently used in\nobject detection, did not perform well, whereas Faster R-CNN\nand CenterNet were successful. As the reason for this, there\nis almost no overlap of characters in the image, which differs\nfrom conventional object detection. Some teams reported an\nimproved accuracy using the latest models with strong back-\nbones, whereas others reported that their own methods were\nmore successful.\nV. ACTIVITIES RELATED TO KUZUSHIJI RECOGNITION\nThe “Japanese Culture and AI Symposium 2019”13 was\nheld in November 2019. This symposium introduced lead-\ning research being conducted on Kuzushiji globally, and the\nparticipants discussed the past and present studies with an\naim toward future research using AI for the reading and\nunderstanding of Kuzushiji.\nIn the “Cloud Honkoku (Cloud Transcription)” project, a\nsystem that allows correcting the transcriptions of other par-\nticipants was implemented. By cooperating with the Kuzushiji\nlearning application using KuLA and AI technology, the\nmethods developed by the participants were simpliﬁed and\nmade more efﬁcient. The transcribed results can also be used\nas training data.\nCODH provides several online services such as the KuroNet\nKuzushiji character recognition service, KogumaNet Kuzushiji\n13http://codh.rois.ac.jp/symposium/japanese-culture-ai-2019/\ncharacter recognition service, and International Image Interop-\nerability Framework (IIIF)14 compatible Kuzushiji character\nrecognition viewer. The KuroNet Kuzushiji recognition ser-\nvice15 provides a multi-character Kuzushiji OCR function for\nIIIF-compliant images. With this service, we do not need to\nupload images to the server, but can test our own images.\nThe IIIF-compatible character recognition viewer provides\na single-character OCR function for IIIF-compliant images.\nAs an advantage of using this viewer, it can be applied\nimmediately while viewing an image, allowing not only the\ntop candidate but also other candidates to be viewed.\nIn the future, it is expected that Kuzushiji transcription will\nbe further enabled by individuals in various ﬁelds, such as\nthose involved in machine learning, the humanities, and the\nactual application of this historical script.\nVI. FUTURE STUDIES ON KUZUSHIJI RECOGNITION\nIn Japan, historical materials have been preserved for more\nthan a thousand years; pre-modern books and historical doc-\numents and records have been estimated to number approx-\nimately three million and one billion, respectively. However,\nmost historical materials have not been transcribed. Currently,\nbecause the Kuzushiji database provided by CODH is mainly\nlimited to documents after the Edo period, it is difﬁcult to\nautomatically transcribe books written prior to this period. To\naddress this issue, it will be necessary to combine multiple\nbooks of various periods owned by multiple institutions and\nrebuild a large-scale Kuzushiji database. Looking back further\ninto history, the rules for Kuzushiji increase, and thus it is\nexpected that there will be many character types and character\nforms that cannot be found in the database. Therefore, we\nneed a framework for handling unknown character types and\ncharacter forms that do not exist in the training data. In\naddition, as one of the problems in character classiﬁcation,\nthe number of data for each character class is imbalanced,\nand it is impossible to accurately recognize characters in a\nclass with an extremely small number of data.\nIn addition, the quality of the images differs signiﬁcantly\nfrom one book to the next, and the writing styles are com-\npletely different depending on the author, which also makes\ncharacter recognition difﬁcult to achieve. To this end, it is\nimportant to increase the number and variation of training\ndata, to incorporate techniques such as domain adaptation, and\nimprove the database itself.\nThere\nis\nroom\nfor\nfurther\nstudies\non\nnot\nonly\nindividual\ncharacter-by-character\nrecognition,\nbut\nalso\nword/phrase/sentence-level\nrecognition\nbased\non\nthe\nsurrounding characters and context. In the Kaggle competition,\nalthough many teams added contextual information, such as\nthe use of language models, the accuracy did not signiﬁcantly\nimprove because there are problems speciﬁc to Kuzushiji that\ndiffer from those of modern languages. Therefore, we believe\n14A set of technology standards intended to make it easier for researchers,\nstudents, and the public at large to view, manipulate, compare, and annotate\ndigital images on the web. https://iiif.io/\n15http://codh.rois.ac.jp/kuronet/\nit will be necessary to not only improve machine learning\ntechniques such as deep learning, but also learn rules based\non specialized knowledge regarding Kuzushiji.\nBased on the current state of transcription, it is important\nto design an interface that not only allows a complete and\nautomatic recognition of characters, but also allows the user\nto work effortlessly. It is desirable to have various functions,\nsuch as a visualization of characters with uncertain estimation\nresults, and output alternative candidate characters.\nVII. CONCLUSION\nIn this paper, we introduced recent techniques and problems\nin Kuzushiji recognition using deep learning. The introduction\nof deep learning has dramatically improved the detection and\nrecognition rate of Kuzushiji. In particular, the inclusion of\nKuzushiji recognition in the PRMU algorithm contest and\nKaggle competition has attracted the attention of numerous\nresearchers, who have contributed to signiﬁcant improvements\nin accuracy. However, there are still many old manuscripts\nthat need to be transcribed, and there are still many issues\nto be addressed. To solve these problems, in addition to\nimproving algorithms such as machine learning, it is necessary\nto further promote the development of a Kuzushiji database\nand cooperation among individuals in different ﬁelds.\nREFERENCES\n[1] S. Yamada, N. Kato, M. Namiki, H. Kawaguchi, S. Hara, Y. Ishitani,\nK. Kasaya, M.Kojima, M. Umeda, K. Yamamoto, M. Shibayama,\n“Historical Character Recognition (HCR) Project Report (2),” IPSJ SIG\nComputers and the Humanities (CH), vol.50, no.2, pp.9–16, 2001. (in\nJapanese)\n[2] S. Yamada, Y. Waizumi, n. Kato, M. Shibayama, “Development of a\ndigital dictionary of historical characters with search function of similar\ncharacters,” IPSJ SIG Computers and the Humanities (CH), vol.54, no.7,\npp.43-50, 2002. (in Japanese)\n[3] M. Onuma, B. Zhu, S. Yamada, M. Shibayama, M. Nakagawa, “De-\nvelopment of cursive character pattern recognition for accessing a\ndigital dictionary to support decoding of historical documents,” IEICE\nTechnical Report, vol.106, no.606, PRMU2006-270, pp.91–96, 2007. (in\nJapanese)\n[4] T. Horiuchi, S. Kato, “A Study on Japanese Historical Character\nRecognition Using Modular Neural Networks,” International Journal of\nInnovative Computing, Information and Control, vol.7, no.8, pp.5003–\n5014, 2011.\n[5] S. Kato, R. Asano, “A Study on Historical Character Recognition by\nusing SOM Template,” In Proc. of 30th Fuzzy System Symposium,\npp.242–245, 2014. (in Japanese)\n[6] T. Hayasaka, W. Ohno, Y. Kato, “Recognition of obsolete script in pre-\nmodern Japanese texts by Neocognitron,” Journal of Toyota College of\nTechnology, vol.48, pp.5–12, 2015. (in Japanese)\n[7] T. Hayasaka, W. Ohno, Y. Kato, K. Yamamoto, “Recognition of Hen-\ntaigana by Deep Learning and Trial Production of WWW Application,”\nIn Proc. of IPSJ Symposium of Humanities and Computer Symposium,\npp.7–12, 2016. (in Japanese)\n[8] K. Ueda, M. Sonogashira, M. Iiyama, “Old Japanese Character Recogni-\ntion by Convolutional Neural Net and Character Aspect Ratio,” ELCAS\nJournal, vol.3, pp.88–90, 2018. (in Japanese)\n[9] T. Kojima, K. Ueki, “Utilization and Analysis of Deep Learning for\nKuzushiji Translation,” Journal of the Japan Society for Precision\nEngineering, vol.85, no.12, pp.1081–1086, 2019. (in Japanese)\n[10] Z. Yang, K. Doman, M. Yamada, Y. Mekada, “Character recognition of\nmodern Japanese ofﬁcial documents using CNN for imblanced learning\ndata,” In Proc. of 2019 Int. Workshop on Advanced Image Technology\n(IWAIT), no.74, 2019.\n[11] A. Nagai, “Recognizing Three Character String of Old Japanese Cursive\nby Convolutional Neural Networks,” In Proc. of Information Processing\nSociety of Japan (IPSJ) Symposium, pp.213–218, 2017. (in Japanese)\n[12] K.Ueki, T. Kojima, R. Mutou, R. S. Nezhad, Y. Hagiwara, “Recognition\nof Japanese Connected Cursive Characters Using Multiple Softmax Out-\nputs,” In Proc. of International Conference on Multimedia Information\nProcessing and Retrieval, 2020.\n[13] X. Hu, M. Inamoto, A. Konagaya, “Recognition of Kuzushi-ji with\nDeep Learning Method: A Case Study of Kiritsubo Chapter in the Tale\nof Genji,” The 33rd Annual Conference of the Japanese Society for\nArtiﬁcial Intelligence, 2019.\n[14] A. Kitamoto, T. Clanuwat, T. Miyazaki, K. Yayamoto, “Analysis of\nCharacter Data: Potential and Impact of Kuzushiji Recognition by\nMachine Learning,” Journal of Institute of Electronics, Information,\nand Communication Engineers, vol.102, no.6, pp.563–568, 2019. (in\nJapanese)\n[15] A. Kitamoto, T. Clanuwat, A. Lamb, M. Bober-Irizar, “Progress and\nResults of Kaggle Machine Learning Competition for Kuzushiji Recog-\nnition,” In Proc. of the Computers and the Humanities Symposium,\npp.223–230, 2019. (in Japanese)\n[16] A. Kitamoto, T. Clanuwat, M. Bober-Irizar, “Kaggle Kuzushiji Recogni-\ntion Competition – Challenges of Hosting a World-Wide Competition in\nthe Digital Humanities –,” Journal of the Japanese Society for Artiﬁcial\nIntelligence, vol.35, no.3, pp.366–376, 2020. (in Japanese)\n[17] S. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: Towards Real-Time\nObject Detection with Region Proposal Networks,” arXiv:1506.01497,\n2015.\n[18] Z. Cai, N. Vasconcelos, “Cascade R-CNN: High Quality Object Detec-\ntion and Instance Segmentation,” arXiv:1906.09756, 2019.\n[19] T. Clanuwat, A. Lamb, A. Kitamoto, “KuroNet: Pre-Modern Japanese\nKuzushiji Character Recognition with Deep Learning,” In Proc. of\nInternational Conference on Document Analysis and Recognition (IC-\nDAR2019), 2019.\n[20] A. Lamb, T. Clanuwat, A. Kitamoto, “KuroNet: Regularized Residual\nU-Nets for End-to-End Kuzushiji Character Recognition,” In Proc. of\nSN Computer Science (2020), 2020.\n[21] A. D. Le, T. Clanuwat, A. Kitamoto, “A human-inspired recognition sys-\ntem for premodern Japanese historical documents,” arXiv:1905.05377,\n2019.\n[22] A. D. Le, “Automated Transcription for Pre-Modern Japanese Kuzushiji\nDocuments by Random Lines Erasure and Curriculum Learning,”\narXiv:2005.02669, 2020.\n[23] A. D. Le, “Detecting Kuzushiji Characters from Historical Documents\nby Two-Dimensional Context Box Proposal Network,” Future Data and\nSecurity Engineering, pp.731–738.\n[24] A. Graves, J. Schmidhuber, ”Framewise phoneme classiﬁcation with\nbidirectional LSTM and other neural network architectures,” Neural\nNetworks, vol.18, no.5–6, pp.602–610, 2005.\n[25] S. Yamamoto, O. Tomejiro, “Labor saving for reprinting Japanese rare\nclassical books,” Journal of Information Processing and Management,\nvol.58, no.11, pp.819–827, 2016. (in Japanese)\n[26] K. Ueki, T. Kojima, “Feasibility Study of Deep Learning Based Japanese\nCursive Character Recognition,” IIEEJ Transactions on Image Electron-\nics and Visual Computing, vol.8, no.1, pp.10–16, 2020.\n[27] K. Ueki, T. Kojima, “Japanese Cursive Character Recognition for\nEfﬁcient Transcription,” In Proc. of the International Conference on\nPattern Recognition Applications and Methods, 2020.\n[28] M. Takeuchi, T. Hayasaka, W. Ohone, Y. Kato, K. Yamamoto, M. Ishima,\nT. Ishikawa, “Development of Embedded System for Recognizing\nKuzushiji by Deep Learning,” In Proc. of the 33rd Annual Conference\nof the Japanese Society for Artiﬁcial Intelligence, 2019. (in Japanese)\n[29] K. Sando, T. Suzuki, A. Aiba, “A Constraint Solving Web Service for\nRecognizing Historical Japanese KANA Texts,” In Proc. the 10th In-\nternational Conference on Agents and Artiﬁcial Intelligence (ICAART),\n2018.\n[30] Atsushi Yamazaki, Tetsuya Suzuki, Kazuki Sando, Akira Aiba, “A\nHandwritten Japanese Historical Kana Reprint Support System,” In Proc.\nthe 18th ACM Symposium on Document Engineering, 2018.\n[31] C. Panichkriangkrai, L. Li, T. Kaneko, R. Akama, K. Hachimura, “Char-\nacter segmentation and transcription system for historical Japanese books\nwith a self-proliferating character image database,” International Journal\non Document Analysis and Recognition (IJDAR), vol.20, pp.241–257,\n2017.\n[32] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb,\nKazuaki Yamamoto, David Ha, “Deep Learning for Classical Japanese\nLiterature,” arXiv:1812.01718, 2018.\n[33] H. T. Nguyen, N. T. Ly, K. C. Nguyen, C. T. Nguyen, M. Nakagawa,\n“Attempts to recognize anomalously deformed Kana in Japanese histor-\nical documents,” In Proc. of the International Workshop on Historical\nDocument Imaging and Processing (HIP 2017), 2017.\n[34] A. Graves, S. Fernandez, F. Gomez, J. Schmidhuber, “Connectionist\nTemporal Classiﬁcation: Labelling Unsegmented Sequence Data with\nRecurrent Neural Networks,” In Proc. of the International Conference\non Machine Learning, pp.369–376, 2006.\n[35] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, Y. Bengio, “Learning Phrase Representations using RNN\nEncoder–Decoder for Statistical Machine Translation,” In Proc. of the\nConference on Empirical Methods in Natural Language Processing\n(EMNLP), pp.1724–1734, 2014.\n[36] Z. Zhong, L. Zheng, G. Kang, S. Li, Y. Yang, “Random Erasing Data\nAugmentation,” arXiv:1708.04896, 2017.\n[37] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D.\nBatra, “Grad-CAM: Visual Explanations from Deep Networks via Gra-\ndientbased Localization,” arXiv:1610.02391, 2016.\n[38] K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, Q. Tian, “CenterNet: Keypoint\nTriplets for Object Detection,” arXiv:1904.08189, 2019.\n[39] K. He, X. Zhang, S. Ren, and Jian Sun, “Deep Residual Learning for\nImage Recognition,” arXiv:1512.03385, 2015.\n[40] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu,\nY. Mu, M. Tan, X. Wang, W. Liu, B. Xiao, “Deep High-Resolution\nRepresentation Learning for Visual Recognition,” IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2020.\n[41] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, K. He, “Aggregated Residual Trans-\nformation for Deep Neural Networks,” In Proc. of IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017.\n[42] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T.-\nY. Liu, “LightGBM: A Highly Efﬁcient Gradient Boosting Decision\nTree,” Advances in Neural Information Processing Systems (NIPS) 30,\npp.3148–3156, 2017.\n[43] T. Chen, C. Guestrin, ”XGBoost: A Scalable Tree Boosting System,”\narXiv:1603.02754, 2016.\n[44] M. Tan, Q. V. Le, “EfﬁcientNet: Rethinking Model Scaling for Convo-\nlutional Neural Networks,” arXiv:1905.11946, 2019.\n[45] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, “mixup: Beyond\nEmpirical Risk Minimization,” arXiv:1710.09412, 2017.\n[46] R. Takahashi, T. Matsubara, K. Uehara, “Data Augmentation using Ran-\ndom Image Cropping and Patching for Deep CNNs,” arXiv:1811.09030,\n2018.\n[47] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J.\nShi, W. Ouyang, C. C. Loy, D. Lin, “Hybrid Task Cascade for Instance\nSegmentation,” In Proc. of IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pp.4974–4983, 2019.\n[48] Z. Tian, W. Huang, T. He, P. He, Y. Qiao, “Detecting Text in Natural\nImage with Connectionist Text Proposal Network,” arXiv:1609.03605,\n2016.\n[49] K. Heaﬁeld, “KenLM: Faster and Smaller Language Model Queries,” In\nProc. of the Sixth Workshop on Statistical Machine Translation, pp.187–\n197, 2011.\n[50] S. Kim, T. Hori, S. Watanabe, “Joint CTC-attention based end-to-end\nspeech recognition using multi-task learning,” in Proc. of the IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2017.\n[51] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, Weijun\nWang, Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, H. Adam, “Searching\nfor MobileNetV3,” arXiv:1905.02244, 2019.\n[52] S. Gidaris, N. Komodakis, Object detection via a multi-region &\nsemantic segmentation-aware CNN model arXiv:1505.01749, 2015.\n[53] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, “You Only Look Once:\nUniﬁed, Real-Time Object Detection,” arXiv:1506.02640, 2015.\n",
  "categories": [
    "cs.CV"
  ],
  "published": "2020-07-19",
  "updated": "2020-07-19"
}